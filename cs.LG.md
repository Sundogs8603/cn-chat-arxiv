# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [D4: Improving LLM Pretraining via Document De-Duplication and Diversification.](http://arxiv.org/abs/2308.12284) | 通过预训练模型嵌入和数据重复方法，我们展示了D4算法可以在LLM预训练中加速训练并提高下游任务准确率。 |
| [^2] | [Extended Linear Regression: A Kalman Filter Approach for Minimizing Loss via Area Under the Curve.](http://arxiv.org/abs/2308.12280) | 本研究介绍了一种通过整合卡尔曼滤波器和分析曲线面积的方法来增强线性回归模型，以最小化损失。该方法通过使用随机梯度下降来开发最优的线性回归方程进行权重更新，避免了常量权重更新和处理部分数据集的限制。 |
| [^3] | [On-Manifold Projected Gradient Descent.](http://arxiv.org/abs/2308.12279) | 本研究提出了一种张量梯度下降的流形投影方法，用于解决高维数据中神经网络分类器的对抗攻击问题，并生成了新颖的流形数据样本。 |
| [^4] | [Language Reward Modulation for Pretraining Reinforcement Learning.](http://arxiv.org/abs/2308.12270) | 这项工作提出了一种名为LAMP的方法，通过利用语言奖励函数作为强化学习的预训练信号，代替传统的任务奖励，以解决稀疏奖励问题，并在实验中取得了稳定的进展。 |
| [^5] | [FECoM: A Step towards Fine-Grained Energy Measurement for Deep Learning.](http://arxiv.org/abs/2308.12264) | FECoM是一个用于细粒度深度学习能耗测量的框架，通过静态仪器分析和考虑计算负载和温度稳定性等因素，为研究人员和开发人员提供了对深度学习API进行概要分析的机制。 |
| [^6] | [Learning from Negative User Feedback and Measuring Responsiveness for Sequential Recommenders.](http://arxiv.org/abs/2308.12256) | 该论文研究了在顺序推荐系统中如何从负面用户反馈中学习以及如何度量响应能力。通过采用“不推荐”损失函数，将显式和隐式的负面用户反馈纳入推荐系统的训练目标，该方法有效提升了推荐系统的性能。 |
| [^7] | [How Safe Am I Given What I See? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy.](http://arxiv.org/abs/2308.12252) | 本文提出了一种基于生成世界模型的学习流水线族，通过克服学习安全知情表示和分布漂移下缺失安全标签的挑战，实现了在线安全预测。这些流水线具有统计校准保证的安全机会预测能力。 |
| [^8] | [How to Protect Copyright Data in Optimization of Large Language Models?.](http://arxiv.org/abs/2308.12247) | 本文提出了一种方法来在优化大型语言模型时避免生成版权数据，通过将大型语言模型训练和优化视为softmax回归问题，并建立一种高效进行softmax回归的方法。 |
| [^9] | [Multi-Objective Optimization for Sparse Deep Neural Network Training.](http://arxiv.org/abs/2308.12243) | 这项工作提出了一种用于稀疏深度神经网络训练的多目标优化算法，通过加权Chebyshev标量化和增广Lagrangian方法，解决了同时优化多个任务时的挑战。 |
| [^10] | [Critical Learning Periods Emerge Even in Deep Linear Networks.](http://arxiv.org/abs/2308.12221) | 即使在深度线性网络中也存在关键学习期，这些关键学习期取决于模型的深度和数据分布的结构。 |
| [^11] | [Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning.](http://arxiv.org/abs/2308.12219) | 本文研究表明，通过扩展扩散语言模型的数据、规模和任务，可以有效使其成为强大的语言学习者。实验证明，扩展扩散语言模型在解决通用语言任务方面能够持续提高性能。 |
| [^12] | [The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection.](http://arxiv.org/abs/2308.12215) | 本研究通过虚假信息检测为例，检查了机器学习在信任与安全问题中学术与实践之间的脱节，并发现了文献中存在的严重不足之处，包括任务不符合在线服务面临的挑战、数据集和模型评估不真实、评估不独立于模型训练等。在此基础上，提出了评估机器学习应用于信任与安全问题的建议。 |
| [^13] | [Learning to Learn Financial Networks for Optimising Momentum Strategies.](http://arxiv.org/abs/2308.12212) | 这篇论文提出了一个端到端的机器学习框架L2GMOM，它可以同时学习金融网络和优化网络动量策略的交易信号，解决了传统方法依赖昂贵数据库和金融专业知识的问题，并提供了高度可解释的前向传播架构。 |
| [^14] | [ULDP-FL: Federated Learning with Across Silo User-Level Differential Privacy.](http://arxiv.org/abs/2308.12210) | ULDP-FL是一种新颖的联邦学习框架，设计用于跨边界的联邦学习中确保用户级差分隐私。算法通过每个用户的加权剪裁直接确保用户级差分隐私，并通过密码学构件增强了其效用。实证实验表明，该方法在隐私和效用的权衡方面取得了显著改进。 |
| [^15] | [Curriculum Learning with Adam: The Devil Is in the Wrong Details.](http://arxiv.org/abs/2308.12202) | 本文研究了为何课程学习在自然语言处理领域取得有限的成功。研究发现，当将课程与Adam优化算法结合使用时，它们学习适应了次优化参数，导致其脆弱性增加 |
| [^16] | [Self-Supervised Knowledge-Driven Deep Learning for 3D Magnetic Inversion.](http://arxiv.org/abs/2308.12193) | 提出了一种基于自监督深度学习方法的3D磁性反演方法，通过闭环学习目标场地数据的反演和正演模型来实现磁性反演。在预设的正演模型参数的条件下，该方法通过优化反演模型来实现最小化观测和重新估计的地表磁异常之间的绝对误差。 |
| [^17] | [Robustness Analysis of Continuous-Depth Models with Lagrangian Techniques.](http://arxiv.org/abs/2308.12192) | 该论文介绍了一种统一的方法，用于确定性和统计性地量化连续深度模型的行为鲁棒性。研究人员提出了基于Lagrangian技术的算法，构造了紧致的ReachTube，并给出了相应的保证。通过比较不同方法中的变分方程、均值定理和Lipschitz常数的使用，实现了确定性和统计性的保证。 |
| [^18] | [Development and external validation of a lung cancer risk estimation tool using gradient-boosting.](http://arxiv.org/abs/2308.12188) | 本研究开发了一个使用梯度提升的肺癌风险估计工具，并通过使用两个数据集进行外部验证。这个工具可以估计五年内发生肺癌的可能性，并通过去除非吸烟者和与肺癌无关的死因的患者进行数据预处理，以减轻偏差。通过特征选择、超参数优化和模型校准，该工具利用XGBoost算法进行训练。 |
| [^19] | [Unsupervised anomalies detection in IIoT edge devices networks using federated learning.](http://arxiv.org/abs/2308.12175) | 本论文提出了一种使用联合学习在工业物联网边缘设备网络中进行无监督异常检测的方法。通过在收集数据的设备上进行模型训练，避免了数据传输到中央服务器的隐私问题。 |
| [^20] | [Data-driven decision-focused surrogate modeling.](http://arxiv.org/abs/2308.12161) | 这项研究提出了决策聚焦代理建模的概念，通过学习一个简化的凸优化模型来最小化原始和代理优化模型之间的决策预测误差，并在实时环境中解决计算上具有挑战性的非线性优化问题。 |
| [^21] | [A Probabilistic Fluctuation based Membership Inference Attack for Generative Models.](http://arxiv.org/abs/2308.12143) | 本研究针对生成模型提出了一种概率波动评估成员推断攻击方法(PFAMI)，通过检测概率分布的波动性来推断模型中是否存在某条训练记录的成员身份。 |
| [^22] | [Masking Strategies for Background Bias Removal in Computer Vision Models.](http://arxiv.org/abs/2308.12127) | 本研究研究了背景引起的偏差对细粒度图像分类的影响，并提出了早期和晚期掩蔽两种策略来减轻该偏差。通过评估标准骨干模型（CNN和ViT）在不同掩蔽策略下的行为，重点关注它们对非分布背景的泛化能力。 |
| [^23] | [An Accelerated Block Proximal Framework with Adaptive Momentum for Nonconvex and Nonsmooth Optimization.](http://arxiv.org/abs/2308.12126) | 本论文提出一种使用自适应动量的加速分块近端框架(ABPL+)来解决非凸和非光滑优化问题，并通过增强比较过程解决外推步骤失败的问题。同时，扩展算法适用于更新块变量的任何情况，并证明了算法的可行性和有效性。通过展示序列的导数集为关键点的性质，更明显地证明了算法的优势。 |
| [^24] | [An Open-Source ML-Based Full-Stack Optimization Framework for Machine Learning Accelerators.](http://arxiv.org/abs/2308.12120) | 本论文提出了一个开源的机器学习加速器全栈优化框架，结合了后端和前端模拟，可以实现准确的系统指标估计和架构参数的自动优化。 |
| [^25] | [Less is More -- Towards parsimonious multi-task models using structured sparsity.](http://arxiv.org/abs/2308.12114) | 该论文研究了将结构稀疏性引入多任务学习框架，开发了一种简洁的模型，可以用较少的参数有效地处理多个任务，并在性能上与密集模型相当或更优。通过在训练期间对模型进行稀疏化，可以减少内存占用、计算需求和预测时间。具体而言，该研究通过在卷积神经网络的共享层中使用通道级l1/l2组稀疏，同时消除多余的组（通道）并对权重施加惩罚，提高了所有任务的学习能力。 |
| [^26] | [Generalized Continual Category Discovery.](http://arxiv.org/abs/2308.12112) | 本研究提出了一种广义持续类别发现（GCCD）的框架，用于在现实生活场景中同时处理新的和已知的类别，并且利用持续的无监督学习方法来发现它们。通过实验证明现有方法无法处理后续任务中的无标记样本。 |
| [^27] | [Quantifying degeneracy in singular models via the learning coefficient.](http://arxiv.org/abs/2308.12108) | 这项工作介绍了一种称为学习系数的量，用于精确量化深度神经网络中的退化程度，该方法能够区分不同参数区域的退化顺序，并揭示了随机优化器对临界点的归纳偏好。 |
| [^28] | [Cached Operator Reordering: A Unified View for Fast GNN Training.](http://arxiv.org/abs/2308.12093) | 本文通过统一视图提供了一种解决图神经网络计算稀疏性挑战的方法，并通过缓存和自适应操作重排实现了GCN和GAT的高速运行，有效节省内存并缓解性能瓶颈。 |
| [^29] | [Stabilizing RNN Gradients through Pre-training.](http://arxiv.org/abs/2308.12075) | 该研究通过预训练网络实现局部稳定性，拓展了已有的稳定性理论，可以应用于复杂结构的循环神经网络。 |
| [^30] | [Identifying Reaction-Aware Driving Styles of Stochastic Model Predictive Controlled Vehicles by Inverse Reinforcement Learning.](http://arxiv.org/abs/2308.12069) | 本文通过逆向强化学习方法识别了随机模型预测控制车辆的反应感知驾驶风格。 |
| [^31] | [InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4.](http://arxiv.org/abs/2308.12067) | InstructionGPT-4通过仅使用200个例子进行微调，在多模式指令数据质量度量和选择器的帮助下，在各种评估任务中优于原始的MiniGPT-4。 |
| [^32] | [Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference.](http://arxiv.org/abs/2308.12066) | 预门控MoE系统通过算法和系统的共同设计，有效解决了传统MoE架构的计算和存储挑战。 |
| [^33] | [Ensembling Uncertainty Measures to Improve Safety of Black-Box Classifiers.](http://arxiv.org/abs/2308.12065) | 本文提出了一种叫做SPROUT的方法，通过对黑盒分类器的输入和输出进行不确定性度量来提高系统的安全性。实验证明，SPROUT可以识别出绝大部分的错分情况。 |
| [^34] | [HarvestNet: A Dataset for Detecting Smallholder Farming Activity Using Harvest Piles and Remote Sensing.](http://arxiv.org/abs/2308.12061) | 该论文介绍了一种新的方法，通过检测全球小农户系统中常见的收获堆来映射农田的存在。作者提供了HarvestNet数据集，该数据集由专家知识和卫星图像收集而来，可用于在埃塞俄比亚的特定地区进行农田映射。实验结果表明，作者的最佳模型在手动标记数据上具有约80%的分类性能。 |
| [^35] | [Manipulating Embeddings of Stable Diffusion Prompts.](http://arxiv.org/abs/2308.12059) | 本论文提出了一种直接改变提示嵌入的方法，促进了用户对生成图像细粒度和目标化的控制。这种方法将生成模型视为连续函数，并在图像空间和提示嵌入空间之间传递梯度。 |
| [^36] | [Sample Complexity of Robust Learning against Evasion Attacks.](http://arxiv.org/abs/2308.12054) | 本论文研究了鲁棒学习在面对规避攻击时的样本复杂度。首先，我们发现分布假设在只有随机示例的情况下至关重要。接着，我们研究了满足Lipschitz条件的输入数据分布下，鲁棒学习单调并联所需的样本复杂度至少是指数级的。如果攻击者受限于扰动$O(\log n)$位，则样本复杂度会更高。 |
| [^37] | [Layer-wise Feedback Propagation.](http://arxiv.org/abs/2308.12053) | 本文提出了一种名为“层级反馈传播（LFP）”的新型神经网络预测器训练方法，通过利用可解释性细化与层级相关性传播（LRP）相结合，根据每个连接对任务的贡献分配奖励，该方法克服了传统梯度下降方法存在的问题。对于各种模型和数据集，LFP取得了与梯度下降相当的性能。 |
| [^38] | [A multiobjective continuation method to compute the regularization path of deep neural networks.](http://arxiv.org/abs/2308.12044) | 本文提出了一种多目标延续方法，用于计算深度神经网络的正则化路径，以解决DNNs中稀疏性和数值效率之间的冲突。 |
| [^39] | [IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning.](http://arxiv.org/abs/2308.12043) | IncreLoRA是一种增量参数分配方法，根据模块的重要性分数在训练过程中自适应地添加可训练参数，以实现参数高效调优。 |
| [^40] | [CACTUS: a Comprehensive Abstraction and Classification Tool for Uncovering Structures.](http://arxiv.org/abs/2308.12031) | CACTUS是一个用于发现结构的全面的抽象和分类工具，通过可解释的人工智能支持安全分析，提供对分类属性的额外支持，并通过并行化优化内存使用和加快计算速度。 |
| [^41] | [Prompt-Based Length Controlled Generation with Reinforcement Learning.](http://arxiv.org/abs/2308.12030) | 提出了一种基于提示的长度控制方法，利用强化学习和奖励模型来实现大型语言模型（LLM）的长度受控生成。该方法可以有效减少推理成本并满足不同需求。 |
| [^42] | [A Scale-Invariant Task Balancing Approach for Multi-Task Learning.](http://arxiv.org/abs/2308.12029) | 这篇论文提出了一种尺度不变的多任务学习方法（SI-MTL），通过对任务损失进行对数变换和对任务梯度进行归一化，解决了多任务学习中的任务平衡问题，并在多个基准数据集上取得了领先的性能。 |
| [^43] | [Bias-Aware Minimisation: Understanding and Mitigating Estimator Bias in Private SGD.](http://arxiv.org/abs/2308.12018) | 本文提出了有偏差感知的最小化（BAM）方法，可以证明降低私有梯度估计偏差，并在实验中证明BAM不仅可以减少偏差，还可以在多个数据集上改善隐私-效用的权衡。 |
| [^44] | [MKL-$L_{0/1}$-SVM.](http://arxiv.org/abs/2308.12016) | 本文提出了一种多核学习的支持向量机框架(MKL-$L_{0/1}$-SVM)，通过开发快速的ADMM求解器处理非凸非光滑的优化问题，并在实验中展示了与领先方法相当的性能。 |
| [^45] | [Quantum-Noise-driven Generative Diffusion Models.](http://arxiv.org/abs/2308.12013) | 该论文提出了三种量子噪声驱动的生成扩散模型，利用了量子特性以克服传统模型的主要计算困难，并建议将量子噪声视为可利用的特性而非问题。 |
| [^46] | [Neural oscillators for magnetic hysteresis modeling.](http://arxiv.org/abs/2308.12002) | 本研究开发了一种名为HystRNN的神经振荡器，通过更新隐藏状态来建模和量化磁滞现象，并且在预测广义场景中表现出了良好的泛化能力。这项研究突显了神经振荡器在捕捉磁性材料的复杂磁滞模式方面相对于传统基于RNN的方法的优势。 |
| [^47] | [On Uniformly Optimal Algorithms for Best Arm Identification in Two-Armed Bandits with Fixed Budget.](http://arxiv.org/abs/2308.12000) | 本文研究了在有限预算的随机二臂赌博机中进行最佳臂选择的问题，并证明不存在比等概率采样算法更好的算法。我们引入了一致稳定算法的概念，并证明任何在所有情况下与等概率采样算法表现一样好的算法必须属于这个类别。这一结果解决了之前的两个未解之谜。 |
| [^48] | [Relational Concept Based Models.](http://arxiv.org/abs/2308.11991) | 关系概念模型是一种关系深度学习方法家族，用于在关系领域提供可解释的任务预测，相比非关系的基于概念的模型，它在泛化性能上与现有的关系模型相匹配，并支持生成量化的基于概念的解释，同时在测试时干预、超出分布情景、有限的训练数据范围和稀缺的概念监督等苛刻条件下也能有效应对。 |
| [^49] | [Will More Expressive Graph Neural Networks do Better on Generative Tasks?.](http://arxiv.org/abs/2308.11978) | 本论文调查了更具表现力的图神经网络在分子图生成任务中的表现能力，并通过替换图生成模型的基础GNN来进行实验。研究发现，使用更具表现力的GNN可以改善生成任务的性能。 |
| [^50] | [Approximating Score-based Explanation Techniques Using Conformal Regression.](http://arxiv.org/abs/2308.11975) | 本研究提出了使用计算成本较低的回归模型来近似评分解释技术，并采用归纳式符合预测框架提供近似值的有效性保证。实证研究结果显示，该方法能显著提升执行时间。 |
| [^51] | [EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE.](http://arxiv.org/abs/2308.11971) | 本文引入了一种名为EVE的高效视觉-语言预训练模型，通过遮蔽信号建模和模态感知的方式，实现了统一的多模态Transformer网络，加速了训练进程，并取得了良好的效果。 |
| [^52] | [Anisotropic Hybrid Networks for liver tumor segmentation with uncertainty quantification.](http://arxiv.org/abs/2308.11969) | 本研究提出了两种不同的基于不均匀模型的流程，用于带有不确定性评估的肝脏肿瘤分割。第一个流程使用多类模型同时分割肝脏和肿瘤，而第二个流程使用两个二进制模型分别分割肝脏和肿瘤。结果表明这两种流程在分割肝脏和肿瘤方面都取得了良好效果。 |
| [^53] | [Maintaining Plasticity via Regenerative Regularization.](http://arxiv.org/abs/2308.11958) | 本文提出了一种名为L2 Init的简单方法，通过将L2正则化应用于初始参数，来维持神经网络在处理非平稳数据流时的可塑性且易于实施。该方法使得参数能够迅速适应新任务并减轻可塑性的丢失。 |
| [^54] | [When MiniBatch SGD Meets SplitFed Learning:Convergence Analysis and Performance Evaluation.](http://arxiv.org/abs/2308.11953) | 当MiniBatch SGD遇上SplitFed Learning：本文提出了MiniBatch-SFL算法，通过引入MiniBatch SGD到SplitFed Learning中，解决了非均衡数据导致的客户端漂移问题。 |
| [^55] | [Multi-scale Transformer Pyramid Networks for Multivariate Time Series Forecasting.](http://arxiv.org/abs/2308.11946) | 本论文提出了一种多尺度Transformer金字塔网络(MTPNet)，通过引入维度不变的嵌入技术和捕捉不受限制尺度上的时间依赖性，解决了在多元时间序列预测中捕捉多样的季节性的问题。 |
| [^56] | [RamseyRL: A Framework for Intelligent Ramsey Number Counterexample Searching.](http://arxiv.org/abs/2308.11943) | 本文提出了一种智能化的Ramsey数反例搜索框架，通过引入图向量化和基于深度神经网络（DNN）的启发式方法，使用最佳优先搜索算法和强化学习（RL）技术，以寻找特定Ramsey数的反例，并提出了算法优化以限制多项式搜索运行时间。 |
| [^57] | [Audio Generation with Multiple Conditional Diffusion Model.](http://arxiv.org/abs/2308.11940) | 本论文提出了一种使用多条件扩散模型进行音频生成的方法。通过引入内容和风格等额外条件，增强了现有模型的可控性。这种方法可以精确控制生成音频的时间顺序、音高和能量。由于缺乏合适的数据集和评估指标，作者整合了现有数据集并进行了实验验证。 |
| [^58] | [Retail Demand Forecasting: A Comparative Study for Multivariate Time Series.](http://arxiv.org/abs/2308.11939) | 本研究通过将客户需求的时间序列数据与宏观经济变量相结合，开发并比较了各种回归和机器学习模型，以准确预测零售需求。 |
| [^59] | [System Identification for Continuous-time Linear Dynamical Systems.](http://arxiv.org/abs/2308.11933) | 本文解决了在连续时间下观测不规则采样的情况下，Kalman滤波器的系统识别问题。通过引入连续时间Ito随机微分方程来推广Kalman滤波器的学习，并提供一个新颖的两滤波器的后验计算方法，通过贝叶斯派生获得的解析形式的后验计算方法可以高效地估计SDE的参数。 |
| [^60] | [Dynamic landslide susceptibility mapping over recent three decades to uncover variations in landslide causes in subtropical urban mountainous areas.](http://arxiv.org/abs/2308.11929) | 该研究提出了一种动态滑坡易发性制图方法，通过多个预测模型进行年度评估，并解决了小样本和泛化问题。 |
| [^61] | [Solving Elliptic Optimal Control Problems using Physics Informed Neural Networks.](http://arxiv.org/abs/2308.11925) | 本论文提出了一种使用物理信息神经网络（PINNs）求解椭圆型最优控制问题的数值方法，并进行了误差分析和数值实验验证。 |
| [^62] | [Diverse Policies Converge in Reward-free Markov Decision Processe.](http://arxiv.org/abs/2308.11924) | 本文提出了一个统一的多样化强化学习框架，并研究了训练多样化策略的收敛性，同时提出了一个经过验证的高效性多样化强化学习算法。 |
| [^63] | [Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement.](http://arxiv.org/abs/2308.11923) | 该论文提出了音频差异字幕生成（ADC）作为一种新的音频字幕生成扩展任务，用于描述类似但略有差异的音频片段之间的语义差异。通过引入交叉注意力集中的Transformer编码器和相似性-差异解缠，该方法有效解决了传统音频字幕生成中的差异描述问题，并利用可视化来改善注意力权重以提取差异。 |
| [^64] | [Addressing Selection Bias in Computerized Adaptive Testing: A User-Wise Aggregate Influence Function Approach.](http://arxiv.org/abs/2308.11912) | 本文研究了计算机自适应测试中存在的选择偏差问题，并提出了一种基于用户的聚合影响函数方法来解决该问题。 |
| [^65] | [Utilizing Admissible Bounds for Heuristic Learning.](http://arxiv.org/abs/2308.11905) | 本文通过将可接受启发式作为截断高斯分布的参数，明确了在监督启发式学习中可接受启发式的作用，紧缩了假设空间。 |
| [^66] | [Rethinking Data Perturbation and Model Stabilization for Semi-supervised Medical Image Segmentation.](http://arxiv.org/abs/2308.11903) | 该论文重新思考了半监督医学图像分割中的数据扰动和模型稳定化的重要性，并提出了名为DPMS的简单而有效的方法来提高半监督分割性能。 |
| [^67] | [Shape-conditioned 3D Molecule Generation via Equivariant Diffusion Models.](http://arxiv.org/abs/2308.11890) | 本文提出了一个基于形状的分子生成问题，通过等变形状引导的生成模型ShapeMol成功生成了新颖、多样且类似给定形状条件的药物样分子。 |
| [^68] | [Adversarial Training Using Feedback Loops.](http://arxiv.org/abs/2308.11881) | 本文提出了一种基于反馈控制的新型对抗训练方法，通过将反馈控制纳入神经网络架构中进行训练，以增强DNN对对抗性攻击的鲁棒性。 |
| [^69] | [SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets.](http://arxiv.org/abs/2308.11880) | SUMMIT方法放宽了传统模型自适应方法的两个假设，通过解决无源数据、无配对数据的情况将独立训练的单模态模型适应到多模态目标领域。 |
| [^70] | [Cabrita: closing the gap for foreign languages.](http://arxiv.org/abs/2308.11878) | Cabrita是一种解决性能和高效标记化问题的方法，以可承受的成本解决了从头训练模型的限制。 |
| [^71] | [Integrating Large Language Models into the Debugging C Compiler for generating contextual error explanations.](http://arxiv.org/abs/2308.11873) | 本文介绍了一种利用大型语言模型将调试C编译器的错误解释改进的方法，并通过专家评估证明其在编译时和运行时错误解释的准确性方面的有效性。 |
| [^72] | [KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods.](http://arxiv.org/abs/2308.11863) | 通过自监督预训练、课程进度微调和半监督学习利用无标签语音数据，该论文提出了一种改善基尼亚兰达语音识别的方法，实现了最先进的结果。 |
| [^73] | [Finding the Perfect Fit: Applying Regression Models to ClimateBench v1.0.](http://arxiv.org/abs/2308.11854) | 本文研究了在ClimateBench数据集上应用非线性回归模型进行气候模拟的能力，重点比较了三种非线性回归模型的性能，结果表明高斯过程回归器在模拟能力方面表现出优势。 |
| [^74] | [A deep reinforcement learning approach for real-time demand-responsive railway rescheduling to mitigate station overcrowding using mobile data.](http://arxiv.org/abs/2308.11849) | 本文提出了一种利用移动数据进行实时需求响应的深度强化学习方法来缓解车站拥挤的铁路重新调度。该方法通过推断真实世界的乘客流动性来实现实时调度，并着重考虑高需求车站的重新安排。 |
| [^75] | [SEA: Shareable and Explainable Attribution for Query-based Black-box Attacks.](http://arxiv.org/abs/2308.11845) | 本论文提出了SEA，一种用于归因基于查询的黑盒攻击的机器学习安全系统，通过利用隐藏马尔可夫模型框架来理解攻击的演变过程，并有效归因攻击，即使是对于第二次出现的攻击，具有鲁棒性，旨在实现取证和人类可解释的情报共享。 |
| [^76] | [${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2308.11842) | 本文利用欧几里德对称性，设计了一种基于${\rm E}(3)$等变性的协作多智体强化学习的演员-评论家方法，通过对称约束的神经网络结构，实现了在各种协作MARL基准测试中的卓越性能和令人印象深刻的泛化能力。 |
| [^77] | [A Survey for Federated Learning Evaluations: Goals and Measures.](http://arxiv.org/abs/2308.11841) | 本研究调查了联邦学习的评估目标和指标，介绍了一个开源平台FedEval，提供了标准化的评估框架，为联邦学习算法的效用、效率和安全性方面进行评估，并讨论了该领域面临的挑战和未来研究方向。 |
| [^78] | [A Benchmark Study on Calibration.](http://arxiv.org/abs/2308.11838) | 这项研究提出了一个模型校准的基准研究，利用神经架构搜索空间探索了模型校准属性。研究结果显示，模型校准可以在不同任务中泛化，并可以同时兼顾模型的准确性和校准性能。 |
| [^79] | [Characterizing normal perinatal development of the human brain structural connectivity.](http://arxiv.org/abs/2308.11836) | 本研究开发了一个计算框架，通过时空平均方法来研究围产期人类大脑结构连接的正常发展。 |
| [^80] | [Performance Comparison and Implementation of Bayesian Variants for Network Intrusion Detection.](http://arxiv.org/abs/2308.11834) | 本研究实现并比较了贝叶斯分类器的各个变体在网络入侵检测中的性能，并发现假设是影响准确性的最重要因素。 |
| [^81] | [Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver's License Knowledge Test.](http://arxiv.org/abs/2308.11827) | 本研究提出了一种方法，通过使用新的信息源的上下文，让GPT模型能够回答考试题目。在使用加利福尼亚驾驶手册作为信息源的测试中，GPT-3模型取得了96%的及格分数。 |
| [^82] | [Accel-GCN: High-Performance GPU Accelerator Design for Graph Convolution Networks.](http://arxiv.org/abs/2308.11825) | Accel-GCN使用轻量级度排序、块级分区和综合warp策略的GPU加速器架构，提高了图卷积网络（GCNs）在主流GPU上的性能。 |
| [^83] | [PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model Modification.](http://arxiv.org/abs/2308.11822) | 本论文提出了一种无需修改模型的深度神经网络后门攻击方法，通过在摄像头前放置一个精心设计的补丁，即可触发模型的错误行为，而无需修改模型。 |
| [^84] | [Mitigating Health Disparity on Biased Electronic Health Records via Deconfounder.](http://arxiv.org/abs/2308.11819) | 通过使用去混淆器模型FLMD，在电子健康记录建模中实现了公平性和准确性，解决了有偏见的EHR中的健康差异问题。 |
| [^85] | [Incorporating Nonlocal Traffic Flow Model in Physics-informed Neural Networks.](http://arxiv.org/abs/2308.11818) | 本研究在物理信息感知深度学习框架中引入非局部交通流模型，以提高交通状态估计的准确性和可靠性。 |
| [^86] | [Evaluation of Deep Neural Operator Models toward Ocean Forecasting.](http://arxiv.org/abs/2308.11814) | 深度神经操作模型在海洋预测中表现良好，可以对经典流体流动和现实海洋动力学进行有效预测。 |
| [^87] | [Ceci n'est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings.](http://arxiv.org/abs/2308.11804) | 该论文研究了多模态嵌入中的对抗幻觉问题。对手可以扰动输入的任意模态，使其嵌入与其他模态的任意输入接近，从而实现任意图像与任意文本、任意文本与任意声音的对齐。该问题与下游任务无关，对生成和分类任务会产生误导。 |
| [^88] | [Variational Density Propagation Continual Learning.](http://arxiv.org/abs/2308.11801) | 本文提出了一个用于适应数据分布漂移的连续学习框架，并通过利用贝叶斯推断中的不确定性量化来减轻灾难性遗忘。通过传播分布的前两个矩来优化闭式ELBO目标，从而近似预测分布。这种方法消除了Monte Carlo采样的需要，有效惩罚模型似然性的变化。 |
| [^89] | [Complex-valued neural networks for voice anti-spoofing.](http://arxiv.org/abs/2308.11800) | 本文提出了一种使用复数值神经网络处理音频的新方法，该方法结合了幅度谱图和原始特征的优点，保留了相位信息，并且允许使用可解释性AI方法。实验结果表明，在反欺诈数据集上，该方法优于先前的方法，并且可以通过解释性AI解释结果。 |
| [^90] | [Karasu: A Collaborative Approach to Efficient Cluster Configuration for Big Data Analytics.](http://arxiv.org/abs/2308.11792) | Karasu是一种通过促进类似基础设施、框架、算法或数据集上工作的用户之间的数据共享，实现更高效的资源配置配置文件分析的方法。 |
| [^91] | [HypBO: Expert-Guided Chemist-in-the-Loop Bayesian Search for New Materials.](http://arxiv.org/abs/2308.11787) | HypBO是一种利用专家人类知识引导贝叶斯搜索的方法，通过生成改进的样本种子来更快地找到有希望的化学空间区域。 |
| [^92] | [Coarse-to-Fine Multi-Scene Pose Regression with Transformers.](http://arxiv.org/abs/2308.11783) | 本文提出了一种使用Transformers进行多场景精确相机姿态回归的方法，通过自注意力聚合激活图和解码潜在特征来实现对通用特征的定位，并在多个场景中并行嵌入。 |
| [^93] | [Addressing Dynamic and Sparse Qualitative Data: A Hilbert Space Embedding of Categorical Variables.](http://arxiv.org/abs/2308.11781) | 我们提出了一个新的框架，将定性数据融入到因果估计的定量模型中，通过将观察到的范畴嵌入到潜在的Baire空间，并引入一个连续的线性映射，转化定性变量的处理为RKHS中的识别结构，从而简化了估计的过程 |
| [^94] | [Few-shot Anomaly Detection in Text with Deviation Learning.](http://arxiv.org/abs/2308.11780) | 本论文介绍了一种基于深度少样本学习的框架FATE，它通过离群学习明确地学习文本中的异常得分，并利用先前已知的少量异常示例，从而克服了传统方法中对无标签数据的依赖，并优化了异常得分的精确度和数据利用效率。 |
| [^95] | [Understanding Hessian Alignment for Domain Generalization.](http://arxiv.org/abs/2308.11778) | 本论文通过分析分类器的Hessian矩阵和梯度在领域泛化中的作用，发现了跨领域的Hessian矩阵之间的谱范数是传输度量的上界，并分析了在鼓励Hessian和梯度之间的相似性时的所有对齐属性。 |
| [^96] | [3ET: Efficient Event-based Eye Tracking using a Change-Based ConvLSTM Network.](http://arxiv.org/abs/2308.11771) | 本文介绍了一种3ET方法，即使用基于变化的ConvLSTM网络的高效事件驱动眼球追踪的模型。该模型利用事件相机的优势，在标记的瞳孔数据集上实现了高效的时空特征提取和准确的瞳孔追踪，适用于资源有限的设备。 |
| [^97] | [Patient Clustering via Integrated Profiling of Clinical and Digital Data.](http://arxiv.org/abs/2308.11748) | 本研究引入了一种基于个人配置的患者聚类模型，利用临床数据和数字交互数据构建患者配置，通过综合评估显示出卓越的聚类性能和推荐准确性。 |
| [^98] | [Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape.](http://arxiv.org/abs/2308.11737) | Animal3D是一个全面的哺乳动物3D姿态和形状数据集，通过手工标注和检查确保了高质量的结果。通过Animal3D数据集，我们对形状和姿态估计模型进行了基准测试，包括使用只有Animal3D数据的监督学习、合成到真实的转移和微调人体姿势和形状估计模型。 |
| [^99] | [Knowledge Graph Prompting for Multi-Document Question Answering.](http://arxiv.org/abs/2308.11730) | 这篇论文提出了一种知识图谱引导的方法，用于在多文档问答任务中为大型语言模型（LLMs）提示正确的上下文。通过构建多个文档上的知识图谱，并设计基于语言模型的图遍历器，该方法能够帮助LLMs在MD-QA中进行答案预测。 |
| [^100] | [When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-making.](http://arxiv.org/abs/2308.11721) | 这项研究分析了一种特定类型的人工和算法合作，对于多个噪音模型来说，将选择项目的子集大小$k$设置在$[2, n-1]$范围内能够最大化最终选择最佳项目的概率。 |
| [^101] | [Advancing Relation Extraction through Language Probing with Exemplars from Set Co-Expansion.](http://arxiv.org/abs/2308.11720) | 本文提出了一种通过集合扩展和代表性示例的语言探测方法来推进关系提取。该方法通过整合相似度度量和类别排序，提高了关系分类准确性并减少对比类之间的混淆。经验证明该方法有效提高了关系提取的性能。 |
| [^102] | [SuperCalo: Calorimeter shower super-resolution.](http://arxiv.org/abs/2308.11700) | 本文介绍了一种名为SuperCalo的能量沉积模拟超分辨率模型，能够快速上采样高维细粒度的能量沉积模拟，从而降低计算成本和生成时间。 |
| [^103] | [Efficient Benchmarking (of Language Models).](http://arxiv.org/abs/2308.11696) | 本研究提出了一种名为"Efficient Benchmarking"的问题，旨在智能地减少语言模型评估的计算成本而不降低可靠性，并使用一种名为Decision Impact on Reliability（DIoR）的新度量来评估决策的可靠性。通过HELM基准测试的案例研究，发现只需删除一个低排名模型即可改变领先者，并仅需少量示例即可得到正确的基准测试排名。 |
| [^104] | [Practical Insights on Incremental Learning of New Human Physical Activity on the Edge.](http://arxiv.org/abs/2308.11691) | 本文探讨了边缘机器学习中的重要挑战，包括边缘设备上的数据存储限制、有限的训练计算能力以及学习类别数量的影响。研究通过移动传感器收集的数据学习人体活动，从而为边缘机器学习提供了有价值的见解。 |
| [^105] | [An Analysis of Initial Training Strategies for Exemplar-Free Class-Incremental Learning.](http://arxiv.org/abs/2308.11677) | 本文分析了无范例类增量学习过程中的初始训练策略。研究发现，初始学习策略的选择会显著影响增量学习模型的性能，但目前还没有进行深入研究。 |
| [^106] | [A Study on the Impact of Non-confounding Covariates on the Inferential Performance of Methods based on the Potential Outcome Framework.](http://arxiv.org/abs/2308.11676) | "本文研究了非混淆协变量对基于潜在结果框架的方法推断性能的影响，通过提供统一的图形框架来增强对这些模型基本原理的理解，为实际场景中应用这些模型带来了潜在价值。" |
| [^107] | [WEARS: Wearable Emotion AI with Real-time Sensor data.](http://arxiv.org/abs/2308.11673) | WEARS是一个基于智能手表传感器的情绪预测系统，通过收集真实数据和多种机器学习模型实现用户情绪的预测，并通过消融研究分析了心率、加速度计和陀螺仪传感器数据对情绪的影响。 |
| [^108] | [Generalising sequence models for epigenome predictions with tissue and assay embeddings.](http://arxiv.org/abs/2308.11671) | 本文提出了一种将组织和实验嵌入到上下文化的基因组网络（CGN）中的方法，通过在输入空间中增强长程序列嵌入来提供上下文信息，实现了广泛的实验条件下的强相关性，并在多种设置中超越了现有技术水平。 |
| [^109] | [Machine Learning-based Positioning using Multivariate Time Series Classification for Factory Environments.](http://arxiv.org/abs/2308.11670) | 本文提出了一种基于机器学习的室内定位系统，使用多元时间序列分类，通过IoT设备的内部传感器数据实现在工厂环境中定位移动实体。 |
| [^110] | [Class Label-aware Graph Anomaly Detection.](http://arxiv.org/abs/2308.11669) | 本研究提出了一种基于类标签的图异常检测框架（CLAD），利用少量标记节点来增强无监督图异常检测的性能，在十个数据集上的实验证明了其优越性能。 |
| [^111] | [An engine to simulate insurance fraud network data.](http://arxiv.org/abs/2308.11659) | 本论文介绍了一种模拟保险欺诈网络数据的引擎，利用索赔涉及方的社交网络特征进行学习方法，旨在开发高效准确的欺诈检测模型。但面临类别不平衡、大量未标记数据和缺乏公开数据等挑战。 |
| [^112] | [Pseudo-online framework for BCI evaluation: A MOABB perspective.](http://arxiv.org/abs/2308.11656) | 这篇论文提出了一种基于伪在线模式的BCI评估框架，可以更准确地模拟在线处理的特性，具有较好的治疗应用。 |
| [^113] | [Large Transformers are Better EEG Learners.](http://arxiv.org/abs/2308.11654) | 本研究表明从图像和文本预训练的大型变压器模型可以直接应用于脑电图预测任务的微调，通过设计AdaCE模块在多个EEG基于预测任务上取得了最新的性能。 |
| [^114] | [Accelerating Exact Combinatorial Optimization via RL-based Initialization -- A Case Study in Scheduling.](http://arxiv.org/abs/2308.11652) | 本研究提出了一种基于强化学习的初始化加速精确组合优化的方法，以调度为案例研究。该方法在保持启发式方法运行时间成本的同时，提供了优化性和确定性的保证，并在实际的EdgeTPU平台上实现了128倍的速度提升。 |
| [^115] | [Distributionally Robust Cross Subject EEG Decoding.](http://arxiv.org/abs/2308.11651) | 本文提出了一种基于分布鲁棒优化的方法，通过对数据的动态演化来提高脑电解码的稳健性。 |
| [^116] | [Joint Local Relational Augmentation and Global Nash Equilibrium for Federated Learning with Non-IID Data.](http://arxiv.org/abs/2308.11646) | 本论文中提出的FedRANE方法结合了局部关系增强和全局纳什均衡，可以解决分布式机器学习中的非独立非同分布数据问题。 |
| [^117] | [Neurological Prognostication of Post-Cardiac-Arrest Coma Patients Using EEG Data: A Dynamic Survival Analysis Framework with Competing Risks.](http://arxiv.org/abs/2308.11645) | 本研究提出了一种基于脑电图数据的动态生存分析框架，用于预测心搏骤停后昏迷患者的神经学预后。该框架可以根据不同时点患者的脑电图数据进行预测，并提供苏醒或死亡的概率。这是目前已知的第一个关于这一领域的动态框架。 |
| [^118] | [Synergistic Signal Denoising for Multimodal Time Series of Structure Vibration.](http://arxiv.org/abs/2308.11644) | 这种论文提出了一种深度学习算法，通过融合卷积和循环结构以及关键的注意力机制，协同去噪多模态振动信号，从而在结构健康监测中提升了预测准确性、早期损伤检测和适应性。 |
| [^119] | [In situ Fault Diagnosis of Indium Tin Oxide Electrodes by Processing S-Parameter Patterns.](http://arxiv.org/abs/2308.11639) | 本研究提出了一种利用散射参数（S参数）信号处理的现场故障诊断方法，对氧化铟锡（ITO）电极进行故障检测和诊断。这种方法具有早期检测、高诊断精度、噪声鲁棒性和根本原因分析的优势。 |
| [^120] | [IoT Data Trust Evaluation via Machine Learning.](http://arxiv.org/abs/2308.11638) | 通过随机游走补全（RWI）方法合成可信和不可信的物联网时间序列数据集，并提取特征用于开发和验证物联网数据信任评估的机器学习模型。 |
| [^121] | [Aggregating Intrinsic Information to Enhance BCI Performance through Federated Learning.](http://arxiv.org/abs/2308.11636) | 提出了一个层次化的个性化联邦学习脑电图解码框架（FLEEG），通过在模型训练过程中使具有不同数据格式的数据集进行合作，克服了脑机接口（BCI）面临的数据不足挑战。 |
| [^122] | [Semi-Supervised Dual-Stream Self-Attentive Adversarial Graph Contrastive Learning for Cross-Subject EEG-based Emotion Recognition.](http://arxiv.org/abs/2308.11635) | 本论文提出了一种半监督双流自注意对抗图对比学习框架 DS-AGC，用于跨主体脑电情绪识别中的标记数据不足问题。该框架利用两个并行流提取非结构化和结构化脑电特征，并通过半监督方法解决分布差异和提取有效的基于图的特征表示。 |
| [^123] | [Advances in Self-Supervised Learning for Synthetic Aperture Sonar Data Processing, Classification, and Pattern Recognition.](http://arxiv.org/abs/2308.11633) | 本文提出了MoCo-SAS，利用自监督学习（SSL）进行合成孔径声纳（SAS）数据处理、分类和模式识别。实验证明MoCo-SAS显著优于传统有监督学习方法，为增强水下物体检测和分类提供了有希望的途径。 |
| [^124] | [Deep learning-based flow disaggregation for hydropower plant management.](http://arxiv.org/abs/2308.11631) | 本研究提出了一种基于深度学习的时间序列分解模型，用于将每日流量分解为每小时流量，并在挪威某流量测站的数据上进行了测试，初步结果显示了该模型的一些有希望的方面。 |
| [^125] | [Addressing Data Scarcity in Optical Matrix Multiplier Modeling Using Transfer Learning.](http://arxiv.org/abs/2308.11630) | 本研究使用迁移学习解决了光学矩阵乘法模型训练中的数据稀缺问题，并通过利用合成数据进行预训练和实验数据进行微调，成功减小了建模误差，在仅使用25%数据的情况下实现对矩阵权重的小于1 dB的均方根误差。 |
| [^126] | [Revolutionizing TCAD Simulations with Universal Device Encoding and Graph Attention Networks.](http://arxiv.org/abs/2308.11624) | 本论文提出了一种利用人工智能和图表示技术对TCAD器件模拟中的半导体器件进行编码的创新方法，通过引入图注意力网络和通用编码方案，实现了全面的数据驱动建模，为研究人员提供了在设备级上应用基于人工智能的电子设计自动化解决方案的可能性。 |
| [^127] | [Tryage: Real-time, intelligent Routing of User Prompts to Large Language Model.](http://arxiv.org/abs/2308.11601) | Tryage是一个上下文感知的路由系统，能够根据对个体输入提示的分析，从模型库中选择最佳的专家模型，以消除模型选择和定制化的负担，释放庞大的新兴模型库的巨大威力给最终用户。 |
| [^128] | [Designing an attack-defense game: how to increase robustness of financial transaction models via a competition.](http://arxiv.org/abs/2308.11406) | 通过设计一款攻防游戏，我们研究了使用序列金融数据的神经网络模型的对抗攻击和防御的现状和动态，并且通过分析比赛动态，回答了隐藏模型免受恶意用户攻击的重要性以及需要多长时间才能破解模型的问题。 |
| [^129] | [An Effective Transformer-based Contextual Model and Temporal Gate Pooling for Speaker Identification.](http://arxiv.org/abs/2308.11241) | 本文介绍了一种基于Transformer的上下文模型和时间门池化的有效方法，应用于说话人识别，并在准确率85.9%的情况下比较了其性能与wav2vec2方法。 |
| [^130] | [Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models.](http://arxiv.org/abs/2308.11217) | 本论文提出了一种多模态联邦学习框架，利用私有领域数据协同训练大型模型，以实现跨场景的智能服务。在大模型时代，该框架解决了异构数据、模型聚合、性能和成本权衡、数据隐私以及激励机制等方面的挑战。 |
| [^131] | [Foundation Model-oriented Robustness: Robust Image Model Evaluation with Pretrained Models.](http://arxiv.org/abs/2308.10632) | 本文提出了一种通过预训练模型对稳健性图像模型进行评估的新方法，通过与基础模型进行比较，直接测量图像分类模型的性能，扩展图像数据集以完成超出基准范围的评估。 |
| [^132] | [Information Theory-Guided Heuristic Progressive Multi-View Coding.](http://arxiv.org/abs/2308.10522) | 通过信息理论提出了一个新的多视图学习框架，并基于此框架构建了一种渐进式多视图编码方法。 |
| [^133] | [Minimalist Traffic Prediction: Linear Layer Is All You Need.](http://arxiv.org/abs/2308.10276) | 本文提出了一种极简的交通预测模型STLinear，通过节点嵌入方法、时间序列分解和周期性学习解决了空间-时间图神经网络（STGNN）的计算复杂性和资源密集性问题，该方法只使用线性层，在准确度上与领先的STGNN相匹敌甚至超过，计算需求显著降低。 |
| [^134] | [Backward Reasoning in Large Language Models for Verification.](http://arxiv.org/abs/2308.07758) | 本文研究了在大型语言模型中使用反向推理进行验证的方法。作者提出了一种新颖的技术，通过屏蔽问题中的一个标记，并要求语言模型预测被屏蔽的标记来验证候选答案。同时，作者还提出了一种结合正向和反向推理的方法来估计候选答案的概率。 |
| [^135] | [AudioFormer: Audio Transformer learns audio feature representations from discrete acoustic codes.](http://arxiv.org/abs/2308.07221) | AudioFormer是一种学习音频特征表示的方法，通过生成离散的声学代码并利用它们来训练掩码语言模型，从而将音频分类任务视为自然语言理解的形式。此外，引入了多正样本对比学习方法，通过学习联合表示来捕捉音频中的相关性。 |
| [^136] | [Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation.](http://arxiv.org/abs/2308.06644) | 本文提出使用逐步蒸馏来加速基于扩散的组合优化求解器，并在TSP-50数据集上展示了16倍的推理速度提升，仅有0.019%的性能降级。 |
| [^137] | [On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey.](http://arxiv.org/abs/2307.16680) | 本文综合调查了大规模生成模型的可信度问题，涵盖了隐私、安全、公平性和责任等多个维度，并提出了实际建议和未来发展方向。 |
| [^138] | [Self-consistency for open-ended generations.](http://arxiv.org/abs/2307.06857) | 本论文提出了一种改进大规模预训练语言模型生成输出质量和一致性的新方法，通过扩展自洽性框架的适用性，实现了从一个候选集中恢复最优或接近最优的生成结果，并提出了一种轻量级无参数相似性函数来改进代码生成、自动形式化和摘要任务的效果。 |
| [^139] | [An ML approach to resolution of singularities.](http://arxiv.org/abs/2307.00252) | 该论文介绍了一种新的机器学习方法，使用强化学习代理来解决奇点问题中的最优解。实验证明在多项式相加的总数方面，该方法超过了当前最先进的选择启发式算法，展示了近期研究的潜力。 |
| [^140] | [Automated Assignment and Classification of Software Issues.](http://arxiv.org/abs/2307.00009) | 本论文提出了一种自动分配和分类软件问题的方法。通过使用经过精心策划的语言特征和不同的机器学习方法，将问题分配给最相关的团队成员，并将其分类为不同的类别，以提高工作效率和准确性。 |
| [^141] | [UTRNet: High-Resolution Urdu Text Recognition In Printed Documents.](http://arxiv.org/abs/2306.15782) | 本文提出了一种解决印刷乌尔都文本识别挑战的新方法，并引入了大规模实际标记数据集和合成数据集，提供了乌尔都文本行检测的基准数据集，同时开发了一个在线工具，实现了印刷文档中乌尔都OCR的端到端识别。 |
| [^142] | [U-TOE: Universal TinyML On-board Evaluation Toolkit for Low-Power IoT.](http://arxiv.org/abs/2306.14574) | U-TOE是一个通用的低功耗物联网TinyML局部评估工具包，结合了低功耗嵌入式操作系统、模型转换器和编译器、性能测量模块和远程物联网测试平台的功能。能够帮助物联网设计师和研究人员评估在低功耗物联网硬件上执行各种模型的可行性。 |
| [^143] | [An Overview of Catastrophic AI Risks.](http://arxiv.org/abs/2306.12001) | 本文综述了人工智能灾难性风险的四个主要来源，包括恶意使用、人工智能竞赛、组织风险和流氓人工智能。 |
| [^144] | [Compressed Sensing: A Discrete Optimization Approach.](http://arxiv.org/abs/2306.04647) | 本文中提出了一种离散优化方法来解决压缩感知问题，该方法在二次锥松弛下，可以找到最稀疏的向量，得到了可靠的最优解。 |
| [^145] | [Task Relation-aware Continual User Representation Learning.](http://arxiv.org/abs/2306.01792) | 本文提出了一种新的持续用户表示学习方法TERACON，它能够学习通用的用户表示，而不是为每个任务学习任务特定的用户表示，具有很强的实用性和学习能力。 |
| [^146] | [On the Choice of Perception Loss Function for Learned Video Compression.](http://arxiv.org/abs/2305.19301) | 本文研究了在学习视频压缩时，感知损失函数的选择对于重建效果的影响，发现选择 PLF-JD 可以更好地保留跨帧时序相关性，但同时会带来更大的失真惩罚和更难以纠正早期输出帧中的错误。 |
| [^147] | [Traffic Forecasting on New Roads Unseen in the Training Data Using Spatial Contrastive Pre-Training.](http://arxiv.org/abs/2305.05237) | 本文提出一种名为SCPT的框架，利用对比学习进行空间预训练，并引入一个空间编码器模块，用于从未见数据中提取特征。该方法可以用于进行新道路的交通预测，无需重新训练模型。 |
| [^148] | [A Survey on Dataset Distillation: Approaches, Applications and Future Directions.](http://arxiv.org/abs/2305.01975) | 数据集蒸馏在机器学习中越来越重要。该方法可以通过合成高信息密度的数据集来支持持续学习、神经架构搜索和隐私保护。这篇综述性调查论文提出了一种分类方法，对现有方法进行了特征化，并系统回顾了数据模态和相关应用，同时总结了挑战并讨论了未来方向。 |
| [^149] | [Data-driven abstractions via adaptive refinements and a Kantorovich metric [extended version].](http://arxiv.org/abs/2303.17618) | 我们提出了一种基于自适应细化和康托洛维奇度量的智能且可扩展的动态系统抽象技术，并且定义了一种马尔可夫链之间的度量用作损失函数。我们的方法具有更好的计算复杂度。 |
| [^150] | [Linking generative semi-supervised learning and generative open-set recognition.](http://arxiv.org/abs/2303.11702) | 本研究旨在探究生成半监督学习和生成开放集识别之间的关系。SSL-GANs和OSR-GANs方法的相似性在于都要求生成器在互补空间中产生样本，并通过正则化来推广开放空间。研究结果表明SSL优化边缘-GAN在结合SSL-OSR任务方面树立新的标准，但在某些OSR任务中OSR优化的ARP-GAN仍然略优于SSL-GAN。 |
| [^151] | [Self-supervised based general laboratory progress pretrained model for cardiovascular event detection.](http://arxiv.org/abs/2303.06980) | 研究利用自监督学习和迁移学习，将心血管实验室指标的患者进展趋势从常见情况转移到罕见或特定心血管事件检测中，以协助检测经皮冠状动脉介入治疗患者的靶血管重建。 |
| [^152] | [Domain Specific Question Answering Over Knowledge Graphs Using Logical Programming and Large Language Models.](http://arxiv.org/abs/2303.02206) | 该论文提出了一种在知识图谱上回答领域特定问题的方法，通过将经典的逻辑编程语言与大型语言模型整合，实现逻辑推理能力来解决KGQA任务。实验证明该方法在准确识别所有测试问题的正确答案实体方面表现出较高效果，即使仅使用少量标注数据进行训练。这种方法为解决领域特定图谱上的问题回答提供了有前景的解决方案，具有可解释性和稳健性。 |
| [^153] | [Learning to Generalize towards Unseen Domains via a Content-Aware Style Invariant Model for Disease Detection from Chest X-rays.](http://arxiv.org/abs/2302.13991) | 通过内容感知的风格不变模型，我们提出了一种解决深度学习医学图像分析中源领域不匹配挑战的方法。我们采用了风格随机化模块来提取既是风格不变又是内容偏好的领域不变特征，在胸部X射线疾病检测中取得了良好的性能。 |
| [^154] | [Estimating Driver Personality Traits from On-Road Driving Data.](http://arxiv.org/abs/2302.10898) | 本研究旨在从驾驶行为数据中利用机器学习和深度学习技术开发一个模型，以估计驾驶员的心理特征，为驾驶辅助系统提供适应性反馈和预防交通事故。 |
| [^155] | [Measuring Equality in Machine Learning Security Defenses.](http://arxiv.org/abs/2302.08973) | 本文研究了机器学习安全防御方法的平等性能问题，提出了一种简单的平等度量和分析框架，鼓励进一步探索公平性在该领域的应用。 |
| [^156] | [Regret-Based Optimization for Robust Reinforcement Learning.](http://arxiv.org/abs/2302.06912) | 本论文提出了一种基于后悔的优化方法，用于使强化学习算法更加鲁棒，以应对观测中的对抗性噪声。 |
| [^157] | [Pruning Deep Neural Networks from a Sparsity Perspective.](http://arxiv.org/abs/2302.05601) | 本文提出了一种稀疏感知自适应剪枝算法，通过利用PQ指数来衡量深度神经网络的潜在压缩性，可以有效地确定模型的剪枝程度，确保不会过度或欠剪枝。 |
| [^158] | [Dirac signal processing of higher-order topological signals.](http://arxiv.org/abs/2301.10137) | 本文提出了一种狄拉克信号处理算法，用于改善高阶拓扑信号的信噪比，并能够联合处理节点和链接上的拓扑信号。 |
| [^159] | [BallGAN: 3D-aware Image Synthesis with a Spherical Background.](http://arxiv.org/abs/2301.09091) | BallGAN是一个新颖的3D感知GAN框架，通过将背景近似为球形表面，并使用特定约束来减少背景自由度，可以产生更合理的3D几何。 |
| [^160] | [A Rigorous Uncertainty-Aware Quantification Framework Is Essential for Reproducible and Replicable Machine Learning Workflows.](http://arxiv.org/abs/2301.05763) | 这篇论文讨论了一个基于贝叶斯范式的不确定性量化框架，可以提供一个广泛而严格的方法来评估机器学习和人工智能在科学工作流程中的可重复性和可信度。 |
| [^161] | [Phase-shifted Adversarial Training.](http://arxiv.org/abs/2301.04785) | 本论文通过分析响应频率的视角，发现对抗训练导致神经网络收敛性较低，从而在每个数据附近产生高度振荡的预测。为了有效地学习高频内容，提出了相位偏移对抗训练(PhaseAT)方法。 |
| [^162] | [Graphon Pooling for Reducing Dimensionality of Signals and Convolutional Operators on Graphs.](http://arxiv.org/abs/2212.08171) | 本文提出了一种在图上进行卷积信息处理的池化方法，通过利用图的图象理论和密集图序列的极限来实现，得到卷积运算符的低维表示，并通过简单的局部插值函数实现信号的降维，具有较好的效果。 |
| [^163] | [Temporal Saliency Detection Towards Explainable Transformer-based Timeseries Forecasting.](http://arxiv.org/abs/2212.07771) | 这项研究提出了一种名为Temporal Saliency Detection (TSD)的方法，利用基于注意力机制的架构实现了多步时间序列预测，并通过压缩多头注意力进行显著性模式的多分辨率分析。 |
| [^164] | [Spurious Features Everywhere -- Large-Scale Detection of Harmful Spurious Features in ImageNet.](http://arxiv.org/abs/2212.04871) | 本文提出了一个框架来系统地识别ImageNet中的有害伪特征，并引入了一个新的数据集"Spurious ImageNet"来衡量分类器对这些特征的依赖性。同时，介绍了一个简单的缓解方法SpuFix来减少分类器对有害伪特征的依赖性。 |
| [^165] | [Physics-informed neural networks with unknown measurement noise.](http://arxiv.org/abs/2211.15498) | 这篇论文提出了一种解决物理信息神经网络在存在非高斯噪声情况下失效的问题的方法，即通过同时训练一个能量模型来学习正确的噪声分布。通过多个例子的实验证明了该方法的改进性能。 |
| [^166] | [Knowledge-Aware Federated Active Learning with Non-IID Data.](http://arxiv.org/abs/2211.13579) | 本文提出了一种知识感知的非IID数据联邦主动学习方法，通过知识专业化主动抽样和知识补偿联邦更新来解决联邦主动学习中的目标不匹配问题。 |
| [^167] | [Exact Manifold Gaussian Variational Bayes.](http://arxiv.org/abs/2210.14598) | 我们提出了一种在复杂模型中进行变分推断的优化算法，通过使用自然梯度更新和黎曼流形，我们开发了一种高效的高斯变分推断算法，并验证了其在多个数据集上的性能。 |
| [^168] | [MARLlib: A Scalable Multi-agent Reinforcement Learning Library.](http://arxiv.org/abs/2210.13708) | 本文提出了MARLlib，这是一个全面的MARL算法库，可统一数十种算法。它还超越了当前工作，集成了各种环境接口和提供灵活的参数共享策略。 |
| [^169] | [ProtoBandit: Efficient Prototype Selection via Multi-Armed Bandits.](http://arxiv.org/abs/2210.01860) | 本文提出的ProtoBandit算法通过多臂赌博机方法实现高效的原型选择，避免了在大规模设置下进行相似性比较的昂贵性，能够识别出一组紧凑的原型实例，有效代表给定的目标集。 |
| [^170] | [Learning to predict 3D rotational dynamics from images of a rigid body with unknown mass distribution.](http://arxiv.org/abs/2209.11355) | 该研究提出了一种基于物理知识的神经网络模型，通过多级预测流程，从刚体图像序列中预测3D旋转动力学，解决了标准深度学习方法无法揭示体内质量分布影响的问题。 |
| [^171] | [Interpreting the Mechanism of Synergism for Drug Combinations Using Attention-Based Hierarchical Graph Pooling.](http://arxiv.org/abs/2209.09245) | 这项研究开发了一种可解释的图神经网络模型，通过挖掘重要的亚分子网络，揭示了药物组合的协同机制。 |
| [^172] | [Towards Top-Down Automated Development in Limited Scopes: A Neuro-Symbolic Framework from Expressibles to Executables.](http://arxiv.org/abs/2209.01566) | 本研究提出了一个神经符号框架，从可表达性到可执行性实现了自顶向下的自动化开发。通过建立代码分类法和使用语义金字塔来关联文本数据和代码数据，我们可以改进深度代码生成的效果。 |
| [^173] | [Neural Networks for Scalar Input and Functional Output.](http://arxiv.org/abs/2208.05776) | 该论文提出了一种解决标量输入和函数输出之间回归问题的方法，使用前馈神经网络预测函数响应。该方法适用于大量预测变量或非线性关系，并可以控制预测曲线的平滑程度。在实验中验证了方法的有效性。 |
| [^174] | [Stability of Aggregation Graph Neural Networks.](http://arxiv.org/abs/2207.03678) | 本文研究了聚合图神经网络（Agg-GNNs）的稳定性特性，并推导了稳定性界限与第一层滤波器属性之间的关系。 |
| [^175] | [Group Equality in Adaptive Submodular Maximization.](http://arxiv.org/abs/2207.03364) | 本论文研究了非自适应和自适应情况下，受群体平等约束的经典子模块最大化问题。研究发现，现有算法没有考虑公平性约束，导致一些特定群体的欠代表或过代表。因此，我们研究了在群体平等约束下选择一组项以最大化子模块效用函数。 |
| [^176] | [Randomized Coordinate Subgradient Method for Nonsmooth Optimization.](http://arxiv.org/abs/2206.14981) | 本文提出了适用于非光滑优化问题的随机坐标半梯度法，该方法在每次迭代只更新一个坐标块，考虑了目标函数的线性有界次梯度假设，并在凸和非凸情况下建立了全面的收敛性分析，收敛速率为$\widetilde{\mathcal{O}}(1/\sqrt{k})$和$\mathcal{O}(1/k)$。 |
| [^177] | [Emergent segmentation from participation dynamics and multi-learner retraining.](http://arxiv.org/abs/2206.02667) | 该论文通过研究基于数据驱动服务的参与和重新训练动态，发现当学习者和用户子群具有风险减少性质时，唯一的稳定均衡是细分的，将子群分配给单个学习者。功利主义社会最优是一个稳定均衡。 |
| [^178] | [Deletion and Insertion Tests in Regression Models.](http://arxiv.org/abs/2205.12423) | 本研究提出了一种在回归模型中评估删除和插入测试的方法，用于确定解释性人工智能中最重要的特征。我们通过比较不同算法计算的特征重要性，发现Kernel SHAP在综合性能方面表现最佳。我们还提出了一种计算更快速的替代指标，适用于回归设置。 |
| [^179] | [A Structured Span Selector.](http://arxiv.org/abs/2205.03977) | 提出了一种新颖的基于语法的结构化跨度选择模型，通过利用部分跨度级注释，摒弃了贪心跨度选择方案，为共指消解和语义角色标注任务带来了实证改进。 |
| [^180] | [Riemannian Hamiltonian methods for min-max optimization on manifolds.](http://arxiv.org/abs/2204.11418) | 本文研究了流形上的min-max优化问题，并引入了Riemannian Hamiltonian方法作为其代理方法。通过最小化Hamiltonian函数，可以得到所需的min-max鞍点。该方法在geodesic-bilinear优化问题中具有挑战性，但通过解决代理问题可以得到全局最优搜索方向。该方法在多个应用中展示了其有效性。 |
| [^181] | [ConceptEvo: Interpreting Concept Evolution in Deep Learning Training.](http://arxiv.org/abs/2203.16475) | ConceptEvo是一个统一的深度神经网络解释框架，可以在训练过程中揭示概念的产生和演变，并通过人机评估和实验证明其发现对模型和预测具有重要意义。 |
| [^182] | [Deep Residual Error and Bag-of-Tricks Learning for Gravitational Wave Surrogate Modeling.](http://arxiv.org/abs/2203.08434) | 利用深度学习方法加速引力波代理波形的建立，通过添加第二个网络模型，减少了波形的不匹配程度，并探索了其他方法来提高代理模型的准确性。 |
| [^183] | [AdaTerm: Adaptive T-Distribution Estimated Robust Moments for Noise-Robust Stochastic Gradient Optimization.](http://arxiv.org/abs/2201.06714) | AdaTerm是一种自适应T分布估计稳健矩的方法，提供了对优化算法的统一处理。 |
| [^184] | [Towards Interactive Reinforcement Learning with Intrinsic Feedback.](http://arxiv.org/abs/2112.01575) | 这篇论文综述了交互式强化学习与内在反馈的关系，强调了将人类输入与RL算法结合的重要性，并指出在这一关键联系上的探索仍有待加强。 |
| [^185] | [Unsupervised Selective Labeling for More Effective Semi-Supervised Learning.](http://arxiv.org/abs/2110.03006) | 本文研究了在半监督学习中如何选择性地标注无标签数据，以提高学习效果。作者提出了一种无监督的选择性标注方法，通过选择聚类原型来获得代表性和多样性的标注数据，证明了该方法在性能上优于传统的主动学习方法。 |
| [^186] | [How much pre-training is enough to discover a good subnetwork?.](http://arxiv.org/abs/2108.00259) | 本论文研究了神经网络剪枝中预训练的数量对剪枝后网络性能的影响，并提出了一个简单的理论界限，该界限以数据集大小的对数关系决定了预训练的迭代次数。 |
| [^187] | [Dive into Deep Learning.](http://arxiv.org/abs/2106.11342) | 《深入深度学习》是一本旨在使深度学习易于理解的开源书籍，提供从概念到代码的教学资源，旨在成为成为应用机器学习科学家的起点，并允许社区快速更新和互动讨论。 |
| [^188] | [The Common Intuition to Transfer Learning Can Win or Lose: Case Studies for Linear Regression.](http://arxiv.org/abs/2103.05621) | 本论文研究了传输学习的基本过程，针对线性回归任务，通过利用源任务参数和目标任务训练数据，提出了一种传输学习方法。我们分析了该方法的泛化性能，并展示了其在解决线性回归中的泛化误差峰值方面的能力。此外，我们证明了在足够相关的任务中，该传输学习方法可以优于岭回归方法。 |

# 详细

[^1]: D4：通过文档去重与多样化改进LLM预训练

    D4: Improving LLM Pretraining via Document De-Duplication and Diversification. (arXiv:2308.12284v1 [cs.CL])

    [http://arxiv.org/abs/2308.12284](http://arxiv.org/abs/2308.12284)

    通过预训练模型嵌入和数据重复方法，我们展示了D4算法可以在LLM预训练中加速训练并提高下游任务准确率。

    

    近年来，越来越多的计算资源和数据被用于训练大规模语言模型(LLM)，通常是通过对来自大规模网络语料库中随机选择的尽可能多的标记进行一次学习。虽然在越来越大的互联网局部上进行训练会导致不断改进的性能，但是这些改进的规模随着规模的增加而减小，目前很少有研究探索数据选择对预训练和下游性能的影响，除了MinHash等简单的去重方法。在这里，我们通过预训练模型嵌入展示了通过谨慎的数据选择(在去重数据的基础上)可以加快训练(提高了20%的效率)并且在16个自然语言处理任务的平均下游准确率上有所提升(高达2%)，在6.7B模型规模上。此外，我们还展示了智能重复数据的表现总是优于基线训练(而重复随机数据的表现比基线训练更差)。我们的结果表明，聪明的数据处理能够显著提升LLM的训练效果和下游任务的准确率。

    Over recent years, an increasing amount of compute and data has been poured into training large language models (LLMs), usually by doing one-pass learning on as many tokens as possible randomly selected from large-scale web corpora. While training on ever-larger portions of the internet leads to consistent performance improvements, the size of these improvements diminishes with scale, and there has been little work exploring the effect of data selection on pre-training and downstream performance beyond simple de-duplication methods such as MinHash. Here, we show that careful data selection (on top of de-duplicated data) via pre-trained model embeddings can speed up training (20% efficiency gains) and improves average downstream accuracy on 16 NLP tasks (up to 2%) at the 6.7B model scale. Furthermore, we show that repeating data intelligently consistently outperforms baseline training (while repeating random data performs worse than baseline training). Our results indicate that clever d
    
[^2]: 扩展线性回归：一种通过曲线下面积来减小损失的卡尔曼滤波方法

    Extended Linear Regression: A Kalman Filter Approach for Minimizing Loss via Area Under the Curve. (arXiv:2308.12280v1 [cs.LG])

    [http://arxiv.org/abs/2308.12280](http://arxiv.org/abs/2308.12280)

    本研究介绍了一种通过整合卡尔曼滤波器和分析曲线面积的方法来增强线性回归模型，以最小化损失。该方法通过使用随机梯度下降来开发最优的线性回归方程进行权重更新，避免了常量权重更新和处理部分数据集的限制。

    

    本研究通过整合卡尔曼滤波器和分析曲线面积来增强线性回归模型，以最小化损失。目标是使用随机梯度下降（SGD）来开发最优的线性回归方程进行权重更新。我们的方法涉及一个逐步过程，从用户定义的参数开始。线性回归模型使用SGD进行训练，分别跟踪权重和损失，并最终进行压缩。然后，基于权重和损失数组训练卡尔曼滤波器以预测下一个合并后的权重。预测结果是将输入均值与权重相乘，经过损失评估后形成权重与损失的曲线。使用两点公式推导出曲线的方程，并通过积分计算曲线下面积。具有最小面积的线性回归方程成为最佳预测曲线。优点包括避免通过梯度下降进行常量权重更新，并且可以处理部分数据集，不像其他需要完整数据集的方法。

    This research enhances linear regression models by integrating a Kalman filter and analysing curve areas to minimize loss. The goal is to develop an optimal linear regression equation using stochastic gradient descent (SGD) for weight updating. Our approach involves a stepwise process, starting with user-defined parameters. The linear regression model is trained using SGD, tracking weights and loss separately and zipping them finally. A Kalman filter is then trained based on weight and loss arrays to predict the next consolidated weights. Predictions result from multiplying input averages with weights, evaluated for loss to form a weight-versus-loss curve. The curve's equation is derived using the two-point formula, and area under the curve is calculated via integration. The linear regression equation with minimum area becomes the optimal curve for prediction. Benefits include avoiding constant weight updates via gradient descent and working with partial datasets, unlike methods needin
    
[^3]: 张量梯度下降的流形投影方法

    On-Manifold Projected Gradient Descent. (arXiv:2308.12279v1 [cs.LG])

    [http://arxiv.org/abs/2308.12279](http://arxiv.org/abs/2308.12279)

    本研究提出了一种张量梯度下降的流形投影方法，用于解决高维数据中神经网络分类器的对抗攻击问题，并生成了新颖的流形数据样本。

    

    本研究提供了一种可计算、直接且数学严谨的逼近高维数据类流形的微分几何方法，以及从输入空间映射到这些类流形上的非线性投影方法。这些工具应用于神经网络图像分类器的设置中，我们生成了新颖的流形数据样本，并实现了一种用于流形对抗训练的投影梯度下降算法。神经网络(NN)对对抗攻击的敏感性突出了神经网络决策边界在输入空间中的脆弱性。在训练过程中引入对抗样本已被证明能减少神经网络对对抗攻击的敏感性；然而，如果这些样本对于该类别无效，也会降低分类器的准确性。在自动编码器的潜空间中，以前已经生成了真实的“流形上”的例子。我们的工作在地进行了探索。

    This work provides a computable, direct, and mathematically rigorous approximation to the differential geometry of class manifolds for high-dimensional data, along with nonlinear projections from input space onto these class manifolds. The tools are applied to the setting of neural network image classifiers, where we generate novel, on-manifold data samples, and implement a projected gradient descent algorithm for on-manifold adversarial training. The susceptibility of neural networks (NNs) to adversarial attack highlights the brittle nature of NN decision boundaries in input space. Introducing adversarial examples during training has been shown to reduce the susceptibility of NNs to adversarial attack; however, it has also been shown to reduce the accuracy of the classifier if the examples are not valid examples for that class. Realistic "on-manifold" examples have been previously generated from class manifolds in the latent of an autoencoder. Our work explores these phenomena in a ge
    
[^4]: 使用语言奖励调制预训练强化学习

    Language Reward Modulation for Pretraining Reinforcement Learning. (arXiv:2308.12270v1 [cs.LG])

    [http://arxiv.org/abs/2308.12270](http://arxiv.org/abs/2308.12270)

    这项工作提出了一种名为LAMP的方法，通过利用语言奖励函数作为强化学习的预训练信号，代替传统的任务奖励，以解决稀疏奖励问题，并在实验中取得了稳定的进展。

    

    通过使用学习到的奖励函数（LRF）来解决稀疏奖励强化学习任务，近年来在任务复杂性方面取得了一些稳定的进展。在这项工作中，我们质疑当今的LRF是否最适合作为任务奖励的直接替代。相反，我们提出利用LRF作为RL的预训练信号。具体来说，我们提出了Language Reward Modulated Pretraining（LAMP），它利用视觉-语言模型（VLM）的零射能力作为预训练的效用来代替下游任务奖励。LAMP使用一个冻结的、预训练的VLM来可扩展地生成噪声的探索奖励，通过计算一个高度多样化的语言指令集和一个代理在预训练环境中的图像观测之间的对比对齐。LAMP与标准的探索奖励一起优化这些奖励。

    Using learned reward functions (LRFs) as a means to solve sparse-reward reinforcement learning (RL) tasks has yielded some steady progress in task-complexity through the years. In this work, we question whether today's LRFs are best-suited as a direct replacement for task rewards. Instead, we propose leveraging the capabilities of LRFs as a pretraining signal for RL. Concretely, we propose $\textbf{LA}$nguage Reward $\textbf{M}$odulated $\textbf{P}$retraining (LAMP) which leverages the zero-shot capabilities of Vision-Language Models (VLMs) as a $\textit{pretraining}$ utility for RL as opposed to a downstream task reward. LAMP uses a frozen, pretrained VLM to scalably generate noisy, albeit shaped exploration rewards by computing the contrastive alignment between a highly diverse collection of language instructions and the image observations of an agent in its pretraining environment. LAMP optimizes these rewards in conjunction with standard novelty-seeking exploration rewards with rei
    
[^5]: FECoM: 朝着深度学习的细粒度能耗测量迈进

    FECoM: A Step towards Fine-Grained Energy Measurement for Deep Learning. (arXiv:2308.12264v1 [cs.LG])

    [http://arxiv.org/abs/2308.12264](http://arxiv.org/abs/2308.12264)

    FECoM是一个用于细粒度深度学习能耗测量的框架，通过静态仪器分析和考虑计算负载和温度稳定性等因素，为研究人员和开发人员提供了对深度学习API进行概要分析的机制。

    

    随着深度学习模型的使用、规模和复杂性增加，其能源消耗迅速增长已成为一个关键问题。促进绿色发展和不同粒度的能源意识，以限制深度学习系统的碳排放是当务之急。然而，缺乏准确测量和优化细粒度（例如方法级别）能耗的标准和可重复工具阻碍了该领域的进展。在本文中，我们介绍了FECoM（细粒度能耗测量仪），这是一个用于细粒度深度学习能耗测量的框架。具体而言，FECoM为研究人员和开发人员提供了一种对深度学习API进行概要分析的机制。FECoM通过使用静态仪器分析和考虑计算负载和温度稳定性等各种因素来解决细粒度能耗测量的挑战。我们评估了FECoM在最常用的深度学习模型之一上测量细粒度能耗的能力。

    With the increasing usage, scale, and complexity of Deep Learning (DL) models, their rapidly growing energy consumption has become a critical concern. Promoting green development and energy awareness at different granularities is the need of the hour to limit carbon emissions of DL systems. However, the lack of standard and repeatable tools to accurately measure and optimize energy consumption at a fine granularity (e.g., at method level) hinders progress in this area. In this paper, we introduce FECoM (Fine-grained Energy Consumption Meter), a framework for fine-grained DL energy consumption measurement. Specifically, FECoM provides researchers and developers a mechanism to profile DL APIs. FECoM addresses the challenges of measuring energy consumption at fine-grained level by using static instrumentation and considering various factors, including computational load and temperature stability. We assess FECoM's capability to measure fine-grained energy consumption for one of the most p
    
[^6]: 从负面用户反馈中学习和度量顺序推荐系统的响应能力

    Learning from Negative User Feedback and Measuring Responsiveness for Sequential Recommenders. (arXiv:2308.12256v1 [cs.IR])

    [http://arxiv.org/abs/2308.12256](http://arxiv.org/abs/2308.12256)

    该论文研究了在顺序推荐系统中如何从负面用户反馈中学习以及如何度量响应能力。通过采用“不推荐”损失函数，将显式和隐式的负面用户反馈纳入推荐系统的训练目标，该方法有效提升了推荐系统的性能。

    

    由于其在建模用户偏好方面的优势，顺序推荐系统在工业中被广泛使用。尽管这些模型擅长学习用户的正面兴趣，但对于从负面用户反馈中学习却付出了较少的关注。负面用户反馈是用户控制的重要手段，并伴随着对推荐系统应该快速响应和减少类似推荐的期望。然而，负面反馈信号在顺序检索模型的训练目标中经常被忽视，这些模型主要旨在预测用户的正面交互。在这项工作中，我们使用“不推荐”损失函数将显式和隐式的负面用户反馈纳入顺序推荐系统在检索阶段的训练目标，优化不推荐带有负面反馈的项目的对数似然。我们通过对大规模工业推荐系统进行实时实验来证明这种方法的有效性。

    Sequential recommenders have been widely used in industry due to their strength in modeling user preferences. While these models excel at learning a user's positive interests, less attention has been paid to learning from negative user feedback. Negative user feedback is an important lever of user control, and comes with an expectation that recommenders should respond quickly and reduce similar recommendations to the user. However, negative feedback signals are often ignored in the training objective of sequential retrieval models, which primarily aim at predicting positive user interactions. In this work, we incorporate explicit and implicit negative user feedback into the training objective of sequential recommenders in the retrieval stage using a "not-to-recommend" loss function that optimizes for the log-likelihood of not recommending items with negative feedback. We demonstrate the effectiveness of this approach using live experiments on a large-scale industrial recommender system
    
[^7]: 我看到的东西有多安全？基于图像控制的自治安全性预测的校准预测

    How Safe Am I Given What I See? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy. (arXiv:2308.12252v1 [cs.LG])

    [http://arxiv.org/abs/2308.12252](http://arxiv.org/abs/2308.12252)

    本文提出了一种基于生成世界模型的学习流水线族，通过克服学习安全知情表示和分布漂移下缺失安全标签的挑战，实现了在线安全预测。这些流水线具有统计校准保证的安全机会预测能力。

    

    端到端学习已经成为开发自治系统的主要范 paradigm。不幸的是，随着其性能和便利性，安全保证面临着更大的挑战。挑战的一个关键因素是缺乏低维可解释动态状态的概念，传统的保证方法都围绕这一概念展开。本文针对在线安全预测问题，提出了一种基于生成世界模型的可配置学习流水线族，不需要低维状态。为了实现这些流水线，我们克服了学习安全知情潜在表示和预测引起的分布漂移下的缺失安全标签的挑战。这些流水线基于符合性预测，对其安全机会预测提供了统计校准保证。我们对提出的学习流水线在两个图像控制系统的案例研究上进行了广泛评估：赛车和汽车。

    End-to-end learning has emerged as a major paradigm for developing autonomous systems. Unfortunately, with its performance and convenience comes an even greater challenge of safety assurance. A key factor of this challenge is the absence of the notion of a low-dimensional and interpretable dynamical state, around which traditional assurance methods revolve. Focusing on the online safety prediction problem, this paper proposes a configurable family of learning pipelines based on generative world models, which do not require low-dimensional states. To implement these pipelines, we overcome the challenges of learning safety-informed latent representations and missing safety labels under prediction-induced distribution shift. These pipelines come with statistical calibration guarantees on their safety chance predictions based on conformal prediction. We perform an extensive evaluation of the proposed learning pipelines on two case studies of image-controlled systems: a racing car and a car
    
[^8]: 如何在优化大型语言模型时保护版权数据？

    How to Protect Copyright Data in Optimization of Large Language Models?. (arXiv:2308.12247v1 [cs.LG])

    [http://arxiv.org/abs/2308.12247](http://arxiv.org/abs/2308.12247)

    本文提出了一种方法来在优化大型语言模型时避免生成版权数据，通过将大型语言模型训练和优化视为softmax回归问题，并建立一种高效进行softmax回归的方法。

    

    大型语言模型（LLM）和生成式人工智能在计算机研究和应用中发挥了变革性的作用。关于这些模型是否输出受版权保护的数据引发了争议，这可能发生在模型训练的数据本身受版权保护的情况下。LLM是建立在Transformer神经网络架构上的，而Transformer依赖一种称为Attention的数学计算，其中使用了softmax函数。在本文中，我们展示了大型语言模型的训练和优化可以被看作是一个softmax回归问题。然后，我们建立了一种高效进行softmax回归的方法，以防止回归函数生成版权数据。这为在训练大型语言模型时避免生成版权数据建立了一个理论方法。

    Large language models (LLMs) and generative AI have played a transformative role in computer research and applications. Controversy has arisen as to whether these models output copyrighted data, which can occur if the data the models are trained on is copyrighted. LLMs are built on the transformer neural network architecture, which in turn relies on a mathematical computation called Attention that uses the softmax function.  In this paper, we show that large language model training and optimization can be seen as a softmax regression problem. We then establish a method of efficiently performing softmax regression, in a way that prevents the regression function from generating copyright data. This establishes a theoretical method of training large language models in a way that avoids generating copyright data.
    
[^9]: 用于稀疏深度神经网络训练的多目标优化

    Multi-Objective Optimization for Sparse Deep Neural Network Training. (arXiv:2308.12243v1 [cs.LG])

    [http://arxiv.org/abs/2308.12243](http://arxiv.org/abs/2308.12243)

    这项工作提出了一种用于稀疏深度神经网络训练的多目标优化算法，通过加权Chebyshev标量化和增广Lagrangian方法，解决了同时优化多个任务时的挑战。

    

    在深度学习的各种场景中，会自然地出现不同的冲突优化准则。这些准则可以解决不同的主任务（如多任务学习设置），也可以解决主要任务和次要任务，例如损失最小化与稀疏性。通常的方法是简单地加权准则，但在凸设置中才有效。本文提出了一种多目标优化算法，对多任务深度神经网络（DNNs）进行训练，使用改进的加权Chebyshev标量化方法。通过使用这种标量化技术，算法可以识别原始问题的所有最优解，同时将其复杂性降低为一系列单目标问题。然后，使用增广Lagrangian方法来解决简化后的问题，从而可以使用常见的优化技术，如Adam和随机梯度下降，同时有效地处理约束条件。我们的工作旨在解决经济化问题。

    Different conflicting optimization criteria arise naturally in various Deep Learning scenarios. These can address different main tasks (i.e., in the setting of Multi-Task Learning), but also main and secondary tasks such as loss minimization versus sparsity. The usual approach is a simple weighting of the criteria, which formally only works in the convex setting. In this paper, we present a Multi-Objective Optimization algorithm using a modified Weighted Chebyshev scalarization for training Deep Neural Networks (DNNs) with respect to several tasks. By employing this scalarization technique, the algorithm can identify all optimal solutions of the original problem while reducing its complexity to a sequence of single-objective problems. The simplified problems are then solved using an Augmented Lagrangian method, enabling the use of popular optimization techniques such as Adam and Stochastic Gradient Descent, while efficaciously handling constraints. Our work aims to address the (economi
    
[^10]: 即使在深度线性网络中也存在关键学习期

    Critical Learning Periods Emerge Even in Deep Linear Networks. (arXiv:2308.12221v1 [cs.LG])

    [http://arxiv.org/abs/2308.12221](http://arxiv.org/abs/2308.12221)

    即使在深度线性网络中也存在关键学习期，这些关键学习期取决于模型的深度和数据分布的结构。

    

    关键学习期是指在发育早期，暂时的感知缺陷会对行为和学习表示产生永久影响的时间段。尽管生物网络和人工网络之间存在根本性的差异，但关键学习期在两个系统中都有经验观察到。这表明关键学习期可能是学习的基本要素，而不是生物学上的偶然现象。然而，为什么关键学习期会在深度网络中出现仍然是一个未解之谜，尤其是不清楚在两个系统中观察到的关键学习期是否依赖于特定的架构或优化细节。为了确定关键的基本因素，我们专注于深度线性网络模型，并展示了令人惊讶的是，这样的网络也显示出生物学和人工网络中观察到的许多行为，同时还可以进行分析处理。我们展示了关键学习期取决于模型的深度和数据分布的结构。

    Critical learning periods are periods early in development where temporary sensory deficits can have a permanent effect on behavior and learned representations. Despite the radical differences between biological and artificial networks, critical learning periods have been empirically observed in both systems. This suggests that critical periods may be fundamental to learning and not an accident of biology. Yet, why exactly critical periods emerge in deep networks is still an open question, and in particular it is unclear whether the critical periods observed in both systems depend on particular architectural or optimization details. To isolate the key underlying factors, we focus on deep linear network models, and show that, surprisingly, such networks also display much of the behavior seen in biology and artificial networks, while being amenable to analytical treatment. We show that critical periods depend on the depth of the model and structure of the data distribution. We also show 
    
[^11]: 扩展性和指导调优的扩散语言模型能够完成多种任务

    Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning. (arXiv:2308.12219v1 [cs.CL])

    [http://arxiv.org/abs/2308.12219](http://arxiv.org/abs/2308.12219)

    本文研究表明，通过扩展扩散语言模型的数据、规模和任务，可以有效使其成为强大的语言学习者。实验证明，扩展扩散语言模型在解决通用语言任务方面能够持续提高性能。

    

    最近生成式人工智能的兴起得益于扩散概率模型的生成能力和大规模语言模型的可扩展性。尽管具有潜力，但扩散语言模型是否能够解决与自回归模型相媲美的通用语言任务仍然不明确。本文证明了在数据、规模和任务方面扩展扩散模型能够有效使其成为强大的语言学习者。我们通过先通过掩码语言建模预训练从大规模数据中获取知识，再通过扩散适应将预训练的掩码语言模型改进为扩散语言模型，通过任务特定的微调和指导调优来发掘其在解决通用语言任务方面的多样性。实验证明，扩展扩散语言模型能够在下游语言任务中持续提高性能。

    The recent surge of generative AI has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models. Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream langua
    
[^12]: 机器学习在信任与安全方面的挑战：一个针对虚假信息检测的案例研究

    The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection. (arXiv:2308.12215v1 [cs.LG])

    [http://arxiv.org/abs/2308.12215](http://arxiv.org/abs/2308.12215)

    本研究通过虚假信息检测为例，检查了机器学习在信任与安全问题中学术与实践之间的脱节，并发现了文献中存在的严重不足之处，包括任务不符合在线服务面临的挑战、数据集和模型评估不真实、评估不独立于模型训练等。在此基础上，提出了评估机器学习应用于信任与安全问题的建议。

    

    我们使用虚假信息检测作为案例研究，检查了在将机器学习应用于信任与安全问题上学术和实践之间的脱节。我们对该领域中270篇广受引用的论文进行了自动检测虚假信息的文献系统化，并对子集中的论文进行了数据和代码的可用性、设计失误、可复现性和泛化性等方面的研究。我们发现文献中存在严重的不足之处，这对所声称的性能和实用性提出了质疑。检测任务通常与在线服务真正面临的挑战有本质上的区别。数据集和模型评估通常不代表现实世界的情景，而且评估往往不独立于模型训练。数据和代码的可用性很差。模型在领域外的数据上泛化能力不强。基于这些结果，我们提出了评估机器学习应用于信任与安全问题的建议。

    We examine the disconnect between scholarship and practice in applying machine learning to trust and safety problems, using misinformation detection as a case study. We systematize literature on automated detection of misinformation across a corpus of 270 well-cited papers in the field. We then examine subsets of papers for data and code availability, design missteps, reproducibility, and generalizability. We find significant shortcomings in the literature that call into question claimed performance and practicality. Detection tasks are often meaningfully distinct from the challenges that online services actually face. Datasets and model evaluation are often non-representative of real-world contexts, and evaluation frequently is not independent of model training. Data and code availability is poor. Models do not generalize well to out-of-domain data. Based on these results, we offer recommendations for evaluating machine learning applications to trust and safety problems. Our aim is fo
    
[^13]: 学习学习金融网络以优化动力策略

    Learning to Learn Financial Networks for Optimising Momentum Strategies. (arXiv:2308.12212v1 [q-fin.PM])

    [http://arxiv.org/abs/2308.12212](http://arxiv.org/abs/2308.12212)

    这篇论文提出了一个端到端的机器学习框架L2GMOM，它可以同时学习金融网络和优化网络动量策略的交易信号，解决了传统方法依赖昂贵数据库和金融专业知识的问题，并提供了高度可解释的前向传播架构。

    

    网络动量提供了一种新型的风险溢价，它利用金融网络中资产之间的相互关联来预测未来的回报。然而，目前构建金融网络的过程依赖于昂贵的数据库和金融专业知识，限制了小型和学术机构的可访问性。此外，传统方法将网络构建和投资组合优化视为单独的任务，可能会影响最优投资组合的表现。为了解决这些挑战，我们提出了L2GMOM，一个端到端的机器学习框架，可同时学习金融网络和优化网络动量策略的交易信号。L2GMOM模型是一个具有高度可解释前向传播架构的神经网络，它是从算法展开中推导出来的。L2GMOM具有灵活性，并可以使用不同的投资组合绩效损失函数进行训练，例如负夏普比率。在回测中。

    Network momentum provides a novel type of risk premium, which exploits the interconnections among assets in a financial network to predict future returns. However, the current process of constructing financial networks relies heavily on expensive databases and financial expertise, limiting accessibility for small-sized and academic institutions. Furthermore, the traditional approach treats network construction and portfolio optimisation as separate tasks, potentially hindering optimal portfolio performance. To address these challenges, we propose L2GMOM, an end-to-end machine learning framework that simultaneously learns financial networks and optimises trading signals for network momentum strategies. The model of L2GMOM is a neural network with a highly interpretable forward propagation architecture, which is derived from algorithm unrolling. The L2GMOM is flexible and can be trained with diverse loss functions for portfolio performance, e.g. the negative Sharpe ratio. Backtesting on 
    
[^14]: ULDP-FL:具有跨边界用户级差分隐私的联邦学习

    ULDP-FL: Federated Learning with Across Silo User-Level Differential Privacy. (arXiv:2308.12210v1 [cs.LG])

    [http://arxiv.org/abs/2308.12210](http://arxiv.org/abs/2308.12210)

    ULDP-FL是一种新颖的联邦学习框架，设计用于跨边界的联邦学习中确保用户级差分隐私。算法通过每个用户的加权剪裁直接确保用户级差分隐私，并通过密码学构件增强了其效用。实证实验表明，该方法在隐私和效用的权衡方面取得了显著改进。

    

    差分隐私联邦学习（DP-FL）作为一种确保形式隐私的协同机器学习方法，已经引起了人们的关注。大多数DP-FL方法确保在每个边界内以记录级别的DP进行跨边界FL。然而，单个用户的数据可能延伸到多个边界，对于这种情况下的期望用户级DP保证仍然未知。在本研究中，我们提出了ULDP-FL，这是一个新颖的FL框架，旨在在单个用户的数据可能属于多个边界的跨边界FL中保证用户级DP。我们的算法通过每个用户的加权剪裁直接确保用户级DP，而不是采用组隐私方法。我们对算法的隐私和效用进行了理论分析。另外，我们通过使用密码学构件来增强算法的效用并展示了其私密实现。在真实数据集上的实证实验表明，我们的方法在隐私和效用的权衡方面取得了显著的改进。

    Differentially Private Federated Learning (DP-FL) has garnered attention as a collaborative machine learning approach that ensures formal privacy. Most DP-FL approaches ensure DP at the record-level within each silo for cross-silo FL. However, a single user's data may extend across multiple silos, and the desired user-level DP guarantee for such a setting remains unknown. In this study, we present ULDP-FL, a novel FL framework designed to guarantee user-level DP in cross-silo FL where a single user's data may belong to multiple silos. Our proposed algorithm directly ensures user-level DP through per-user weighted clipping, departing from group-privacy approaches. We provide a theoretical analysis of the algorithm's privacy and utility. Additionally, we enhance the algorithm's utility and showcase its private implementation using cryptographic building blocks. Empirical experiments on real-world datasets show substantial improvements in our methods in privacy-utility trade-offs under us
    
[^15]: 使用Adam进行课程学习：魔鬼在于错误的细节

    Curriculum Learning with Adam: The Devil Is in the Wrong Details. (arXiv:2308.12202v1 [cs.LG])

    [http://arxiv.org/abs/2308.12202](http://arxiv.org/abs/2308.12202)

    本文研究了为何课程学习在自然语言处理领域取得有限的成功。研究发现，当将课程与Adam优化算法结合使用时，它们学习适应了次优化参数，导致其脆弱性增加

    

    课程学习（CL）认为，与人类类似，机器学习模型可能更有效地从与其当前学习进展相匹配的数据中学习。然而，CL方法仍然被很少了解，在自然语言处理（NLP）领域尤其如此，其取得的成果也有限。本文探讨了其中的原因。我们尝试复现和扩展一些最近的课程方法，但发现当应用于NLP时，这些方法的结果出奇地脆弱。对某些情况下课程效果的深入研究向我们展示了原因：当将课程与广受欢迎的Adam优化算法结合使用时，它们往往会学习适应此算法的次优化参数。我们提供了几个不同的案例研究，涉及常见的手工制作和自动化CL方法，以说明这种现象，并发现其中没有一个能够胜过仅使用精心选择的Adam进行优化

    Curriculum learning (CL) posits that machine learning models -- similar to humans -- may learn more efficiently from data that match their current learning progress. However, CL methods are still poorly understood and, in particular for natural language processing (NLP), have achieved only limited success. In this paper, we explore why. Starting from an attempt to replicate and extend a number of recent curriculum methods, we find that their results are surprisingly brittle when applied to NLP. A deep dive into the (in)effectiveness of the curricula in some scenarios shows us why: when curricula are employed in combination with the popular Adam optimisation algorithm, they oftentimes learn to adapt to suboptimally chosen optimisation parameters for this algorithm. We present a number of different case studies with different common hand-crafted and automated CL approaches to illustrate this phenomenon, and we find that none of them outperforms optimisation with only Adam with well-chose
    
[^16]: 3D磁性反演的自监督知识驱动深度学习方法

    Self-Supervised Knowledge-Driven Deep Learning for 3D Magnetic Inversion. (arXiv:2308.12193v1 [physics.geo-ph])

    [http://arxiv.org/abs/2308.12193](http://arxiv.org/abs/2308.12193)

    提出了一种基于自监督深度学习方法的3D磁性反演方法，通过闭环学习目标场地数据的反演和正演模型来实现磁性反演。在预设的正演模型参数的条件下，该方法通过优化反演模型来实现最小化观测和重新估计的地表磁异常之间的绝对误差。

    

    磁性反演方法是一种非破坏性地球物理方法，旨在通过地表磁异常数据估计地下磁化率分布。最近，监督式深度学习方法在包括磁性反演在内的地球物理领域得到了广泛应用。然而，这些方法严重依赖于合成训练数据，其性能受限于合成数据与实际场数据的分布不独立且不相同。因此，我们提出了一种基于自监督深度学习的磁性反演方法。该方法通过反演和正演模型的闭环学习目标场地数据，实现磁性反演。在预设的正演模型参数的情况下，自监督知识驱动3D磁性反演方法(SSKMI)通过最小化观测和重新估计的地表磁异常之间的绝对误差来优化反演模型。此外，在该方法中还有一个知识驱动模块。

    The magnetic inversion method is one of the non-destructive geophysical methods, which aims to estimate the subsurface susceptibility distribution from surface magnetic anomaly data. Recently, supervised deep learning methods have been widely utilized in lots of geophysical fields including magnetic inversion. However, these methods rely heavily on synthetic training data, whose performance is limited since the synthetic data is not independently and identically distributed with the field data. Thus, we proposed to realize magnetic inversion by self-supervised deep learning. The proposed self-supervised knowledge-driven 3D magnetic inversion method (SSKMI) learns on the target field data by a closed loop of the inversion and forward models. Given that the parameters of the forward model are preset, SSKMI can optimize the inversion model by minimizing the mean absolute error between observed and re-estimated surface magnetic anomalies. Besides, there is a knowledge-driven module in the 
    
[^17]: 基于Lagrangian技术的连续深度模型的鲁棒性分析

    Robustness Analysis of Continuous-Depth Models with Lagrangian Techniques. (arXiv:2308.12192v1 [cs.LG])

    [http://arxiv.org/abs/2308.12192](http://arxiv.org/abs/2308.12192)

    该论文介绍了一种统一的方法，用于确定性和统计性地量化连续深度模型的行为鲁棒性。研究人员提出了基于Lagrangian技术的算法，构造了紧致的ReachTube，并给出了相应的保证。通过比较不同方法中的变分方程、均值定理和Lipschitz常数的使用，实现了确定性和统计性的保证。

    

    本文以统一的方式介绍了确定性和统计性Lagrangian验证技术。它们形式化地量化了任何以连续深度模型形式呈现的时间连续过程的行为鲁棒性。为此，我们回顾了LRT-NG，SLR和GoTube算法，用于构造紧致ReachTube，即在给定时间范围内到达的状态集的过度估计，并为ReachTube边界提供了保证。我们比较了使用与系统方程相关的变分方程、均值定理和Lipschitz常数在实现确定性和统计性保证方面的效果。在LRT-NG中，Lipschitz常数被用作初始扰动的膨胀因子，以计算椭圆体中的半径，该椭圆体以最佳度量方式过度估计了可达状态集。在SLR和GoTube中，我们通过使用Lipschitz常数在样本周围计算局部球来获得统计保证。

    This paper presents, in a unified fashion, deterministic as well as statistical Lagrangian-verification techniques. They formally quantify the behavioral robustness of any time-continuous process, formulated as a continuous-depth model. To this end, we review LRT-NG, SLR, and GoTube, algorithms for constructing a tight reachtube, that is, an over-approximation of the set of states reachable within a given time-horizon, and provide guarantees for the reachtube bounds. We compare the usage of the variational equations, associated to the system equations, the mean value theorem, and the Lipschitz constants, in achieving deterministic and statistical guarantees. In LRT-NG, the Lipschitz constant is used as a bloating factor of the initial perturbation, to compute the radius of an ellipsoid in an optimal metric, which over-approximates the set of reachable states. In SLR and GoTube, we get statistical guarantees, by using the Lipschitz constants to compute local balls around samples. These 
    
[^18]: 使用梯度提升的肺癌风险估计工具的开发和外部验证

    Development and external validation of a lung cancer risk estimation tool using gradient-boosting. (arXiv:2308.12188v1 [cs.LG])

    [http://arxiv.org/abs/2308.12188](http://arxiv.org/abs/2308.12188)

    本研究开发了一个使用梯度提升的肺癌风险估计工具，并通过使用两个数据集进行外部验证。这个工具可以估计五年内发生肺癌的可能性，并通过去除非吸烟者和与肺癌无关的死因的患者进行数据预处理，以减轻偏差。通过特征选择、超参数优化和模型校准，该工具利用XGBoost算法进行训练。

    

    肺癌是全球致死的重要原因，强调早期发现对提高生存率的重要性。本研究提出了一个基于机器学习(ML)的工具，使用PLCO癌症筛查试验数据进行训练，并在NLST上进行验证，以估计五年内发生肺癌的可能性。该研究利用了两个数据集，PLCO (n=55,161)和NLST (n=48,595)，其中包含有关肺癌风险因素、临床测量和结局的全面信息。数据预处理包括删除不是目前或曾经吸烟者以及死于与肺癌无关原因的患者。此外，重点放在减轻被截断数据引起的偏差上。特征选择、超参数优化和模型校准使用XGBoost进行，该算法结合了梯度提升和决策树的集成学习算法。ML模型在预处理的PLCO数据集上进行训练。

    Lung cancer is a significant cause of mortality worldwide, emphasizing the importance of early detection for improved survival rates. In this study, we propose a machine learning (ML) tool trained on data from the PLCO Cancer Screening Trial and validated on the NLST to estimate the likelihood of lung cancer occurrence within five years. The study utilized two datasets, the PLCO (n=55,161) and NLST (n=48,595), consisting of comprehensive information on risk factors, clinical measurements, and outcomes related to lung cancer. Data preprocessing involved removing patients who were not current or former smokers and those who had died of causes unrelated to lung cancer. Additionally, a focus was placed on mitigating bias caused by censored data. Feature selection, hyper-parameter optimization, and model calibration were performed using XGBoost, an ensemble learning algorithm that combines gradient boosting and decision trees. The ML model was trained on the pre-processed PLCO dataset and t
    
[^19]: 使用联合学习在工业物联网边缘设备网络中进行无监督异常检测

    Unsupervised anomalies detection in IIoT edge devices networks using federated learning. (arXiv:2308.12175v1 [cs.LG])

    [http://arxiv.org/abs/2308.12175](http://arxiv.org/abs/2308.12175)

    本论文提出了一种使用联合学习在工业物联网边缘设备网络中进行无监督异常检测的方法。通过在收集数据的设备上进行模型训练，避免了数据传输到中央服务器的隐私问题。

    

    在连接许多物联网设备的情况下，通常训练机器学习模型需要将数据传输到中央服务器，这要求严格的隐私规则。然而，由于数据安全问题，一些业主不愿将其数据提供给公司之外的人。联合学习作为一种分布式机器学习方法，通过在收集数据的设备上进行模型训练。在这种情况下，数据不会在网络上共享用于训练。Fedavg作为联合学习算法之一，允许在训练会话期间将模型复制到参与设备上。设备可以随机选择，也可以中断。生成的模型将发送到协调服务器，然后计算来自完成训练的设备的平均模型。这个过程循环重复，直到达到所需的模型准确性。通过这种方式，联合学习方法解决了物联网/工业物联网设备的隐私问题。

    In a connection of many IoT devices that each collect data, normally training a machine learning model would involve transmitting the data to a central server which requires strict privacy rules. However, some owners are reluctant of availing their data out of the company due to data security concerns. Federated learning(FL) as a distributed machine learning approach performs training of a machine learning model on the device that gathered the data itself. In this scenario, data is not share over the network for training purpose. Fedavg as one of FL algorithms permits a model to be copied to participating devices during a training session. The devices could be chosen at random, and a device can be aborted. The resulting models are sent to the coordinating server and then average models from the devices that finished training. The process is repeated until a desired model accuracy is achieved. By doing this, FL approach solves the privacy problem for IoT/ IIoT devices that held sensitiv
    
[^20]: 数据驱动的决策聚焦代理建模

    Data-driven decision-focused surrogate modeling. (arXiv:2308.12161v1 [math.OC])

    [http://arxiv.org/abs/2308.12161](http://arxiv.org/abs/2308.12161)

    这项研究提出了决策聚焦代理建模的概念，通过学习一个简化的凸优化模型来最小化原始和代理优化模型之间的决策预测误差，并在实时环境中解决计算上具有挑战性的非线性优化问题。

    

    我们引入了决策聚焦代理建模的概念，用于在实时环境中解决计算上具有挑战性的非线性优化问题。提出的数据驱动框架旨在学习一个更简单的、例如凸优化模型，该模型经过训练以最小化决策预测误差，决策预测误差定义为原始和代理优化模型的最优解之间的差异。学习问题被构建为一个双层规划问题，可以看作是一个数据驱动的逆优化问题，我们采用了以前工作中的基于分解的求解算法。通过涉及常见非线性化学过程（如化学反应器、换热器网络和材料混合系统）的数值实验验证了我们的框架。我们还将决策聚焦代理建模与标准的数据驱动代理建模方法进行了详细比较，并证明了我们的方法的优越性。

    We introduce the concept of decision-focused surrogate modeling for solving computationally challenging nonlinear optimization problems in real-time settings. The proposed data-driven framework seeks to learn a simpler, e.g. convex, surrogate optimization model that is trained to minimize the decision prediction error, which is defined as the difference between the optimal solutions of the original and the surrogate optimization models. The learning problem, formulated as a bilevel program, can be viewed as a data-driven inverse optimization problem to which we apply a decomposition-based solution algorithm from previous work. We validate our framework through numerical experiments involving the optimization of common nonlinear chemical processes such as chemical reactors, heat exchanger networks, and material blending systems. We also present a detailed comparison of decision-focused surrogate modeling with standard data-driven surrogate modeling methods and demonstrate that our appro
    
[^21]: 一种基于概率波动的生成模型成员推断攻击方法

    A Probabilistic Fluctuation based Membership Inference Attack for Generative Models. (arXiv:2308.12143v1 [cs.LG])

    [http://arxiv.org/abs/2308.12143](http://arxiv.org/abs/2308.12143)

    本研究针对生成模型提出了一种概率波动评估成员推断攻击方法(PFAMI)，通过检测概率分布的波动性来推断模型中是否存在某条训练记录的成员身份。

    

    成员推断攻击(MIA)通过查询模型来识别机器学习模型的训练集中是否存在某条记录。对经典分类模型的MIA已有很多研究，最近的工作开始探索如何将MIA应用到生成模型上。我们的研究表明，现有的面向生成模型的MIA主要依赖于目标模型的过拟合现象。然而，过拟合可以通过采用各种正则化技术来避免，而现有的MIA在实践中表现不佳。与过拟合不同，记忆对于深度学习模型实现最佳性能是至关重要的，使其成为一种更为普遍的现象。生成模型中的记忆导致生成记录的概率分布呈现出增长的趋势。因此，我们提出了一种基于概率波动的成员推断攻击方法(PFAMI)，它是一种黑盒MIA，通过检测概率波动来推断成员身份。

    Membership Inference Attack (MIA) identifies whether a record exists in a machine learning model's training set by querying the model. MIAs on the classic classification models have been well-studied, and recent works have started to explore how to transplant MIA onto generative models. Our investigation indicates that existing MIAs designed for generative models mainly depend on the overfitting in target models. However, overfitting can be avoided by employing various regularization techniques, whereas existing MIAs demonstrate poor performance in practice. Unlike overfitting, memorization is essential for deep learning models to attain optimal performance, making it a more prevalent phenomenon. Memorization in generative models leads to an increasing trend in the probability distribution of generating records around the member record. Therefore, we propose a Probabilistic Fluctuation Assessing Membership Inference Attack (PFAMI), a black-box MIA that infers memberships by detecting t
    
[^22]: 在计算机视觉模型中去除背景偏差的掩蔽策略

    Masking Strategies for Background Bias Removal in Computer Vision Models. (arXiv:2308.12127v1 [cs.CV])

    [http://arxiv.org/abs/2308.12127](http://arxiv.org/abs/2308.12127)

    本研究研究了背景引起的偏差对细粒度图像分类的影响，并提出了早期和晚期掩蔽两种策略来减轻该偏差。通过评估标准骨干模型（CNN和ViT）在不同掩蔽策略下的行为，重点关注它们对非分布背景的泛化能力。

    

    针对细粒度图像分类任务的模型，由于类别之间的差异极为微妙且每类样本数量较少，很容易受到背景相关偏差的影响，需要使用稳健的方法来处理具有非分布背景的潜在样本。为了更深入地了解这个关键问题，我们的研究调查了背景引起的偏差对细粒度图像分类的影响，评估了诸如卷积神经网络（CNN）和视觉转换器（ViT）等标准骨干模型。我们探索了两种掩蔽策略来减轻背景引起的偏差：早期掩蔽，即在（输入）图像级别上去除背景信息，以及晚期掩蔽，即选择性地掩蔽与背景相对应的高层空间特征。通过大量实验评估了CNN和ViT模型在不同掩蔽策略下的行为，重点关注它们对非分布背景的泛化能力。

    Models for fine-grained image classification tasks, where the difference between some classes can be extremely subtle and the number of samples per class tends to be low, are particularly prone to picking up background-related biases and demand robust methods to handle potential examples with out-of-distribution (OOD) backgrounds. To gain deeper insights into this critical problem, our research investigates the impact of background-induced bias on fine-grained image classification, evaluating standard backbone models such as Convolutional Neural Network (CNN) and Vision Transformers (ViT). We explore two masking strategies to mitigate background-induced bias: Early masking, which removes background information at the (input) image level, and late masking, which selectively masks high-level spatial features corresponding to the background. Extensive experiments assess the behavior of CNN and ViT models under different masking strategies, with a focus on their generalization to OOD backg
    
[^23]: 使用自适应动量的加速分块近端框架用于非凸和非光滑优化

    An Accelerated Block Proximal Framework with Adaptive Momentum for Nonconvex and Nonsmooth Optimization. (arXiv:2308.12126v1 [math.OC])

    [http://arxiv.org/abs/2308.12126](http://arxiv.org/abs/2308.12126)

    本论文提出一种使用自适应动量的加速分块近端框架(ABPL+)来解决非凸和非光滑优化问题，并通过增强比较过程解决外推步骤失败的问题。同时，扩展算法适用于更新块变量的任何情况，并证明了算法的可行性和有效性。通过展示序列的导数集为关键点的性质，更明显地证明了算法的优势。

    

    我们提出了一种使用自适应动量的加速分块近端线性框架（ABPL+）用于非凸和非光滑优化。我们分析了一些算法中的外推步骤失败的潜在原因，并通过增强比较过程来解决这个问题，该过程评估了我们算法中近端梯度步骤与线性外推步骤之间的权衡。此外，我们将算法扩展到涉及使用正整数更新块变量的任何情况，允许每个周期随机重新排列变量块的更新顺序。此外，在一些温和假设下，我们证明了ABPL+可以在严格限定外推参数和步长的情况下单调地减小函数值，并展示了以随机顺序更新这些块的可行性和有效性。我们还通过更明显直观地展示由我们算法生成的序列的导数集是关键点的性质。

    We propose an accelerated block proximal linear framework with adaptive momentum (ABPL$^+$) for nonconvex and nonsmooth optimization. We analyze the potential causes of the extrapolation step failing in some algorithms, and resolve this issue by enhancing the comparison process that evaluates the trade-off between the proximal gradient step and the linear extrapolation step in our algorithm. Furthermore, we extends our algorithm to any scenario involving updating block variables with positive integers, allowing each cycle to randomly shuffle the update order of the variable blocks. Additionally, under mild assumptions, we prove that ABPL$^+$ can monotonically decrease the function value without strictly restricting the extrapolation parameters and step size, demonstrates the viability and effectiveness of updating these blocks in a random order, and we also more obviously and intuitively demonstrate that the derivative set of the sequence generated by our algorithm is a critical point 
    
[^24]: 一个开源的基于机器学习的全栈优化框架用于机器学习加速器

    An Open-Source ML-Based Full-Stack Optimization Framework for Machine Learning Accelerators. (arXiv:2308.12120v1 [cs.LG])

    [http://arxiv.org/abs/2308.12120](http://arxiv.org/abs/2308.12120)

    本论文提出了一个开源的机器学习加速器全栈优化框架，结合了后端和前端模拟，可以实现准确的系统指标估计和架构参数的自动优化。

    

    可参数化的机器学习（ML）加速器是近期ML领域的突破成果。为了完全实现其设计空间探索（DSE），我们提出了一个物理设计驱动的、基于学习的预测框架，用于深度神经网络（DNN）和非DNN ML算法的硬件加速。该框架采用了统一的方法，将后端功耗、性能和面积（PPA）分析与前端性能模拟结合起来，从而实现对后端PPA和系统指标（如运行时间和能量）的真实估计。此外，我们的框架还包括了完全自动化的DSE技术，通过对架构和后端参数的自动搜索来优化后端和系统指标。实验研究表明，我们的方法始终以平均7%或更低的预测误差准确预测了两个深度学习加速器平台（VTA和VeriGOOD-ML）在商业12纳米工艺和。

    Parameterizable machine learning (ML) accelerators are the product of recent breakthroughs in ML. To fully enable their design space exploration (DSE), we propose a physical-design-driven, learning-based prediction framework for hardware-accelerated deep neural network (DNN) and non-DNN ML algorithms. It adopts a unified approach that combines backend power, performance, and area (PPA) analysis with frontend performance simulation, thereby achieving a realistic estimation of both backend PPA and system metrics such as runtime and energy. In addition, our framework includes a fully automated DSE technique, which optimizes backend and system metrics through an automated search of architectural and backend parameters. Experimental studies show that our approach consistently predicts backend PPA and system metrics with an average 7% or less prediction error for the ASIC implementation of two deep learning accelerator platforms, VTA and VeriGOOD-ML, in both a commercial 12 nm process and a 
    
[^25]: 简约多任务模型-使用结构稀疏性实现简洁的多任务模型

    Less is More -- Towards parsimonious multi-task models using structured sparsity. (arXiv:2308.12114v1 [cs.CV])

    [http://arxiv.org/abs/2308.12114](http://arxiv.org/abs/2308.12114)

    该论文研究了将结构稀疏性引入多任务学习框架，开发了一种简洁的模型，可以用较少的参数有效地处理多个任务，并在性能上与密集模型相当或更优。通过在训练期间对模型进行稀疏化，可以减少内存占用、计算需求和预测时间。具体而言，该研究通过在卷积神经网络的共享层中使用通道级l1/l2组稀疏，同时消除多余的组（通道）并对权重施加惩罚，提高了所有任务的学习能力。

    

    机器学习中的组稀疏性鼓励更简单、更可解释的模型，具有较少的活跃参数组。本研究旨在将结构化组稀疏性纳入多任务学习框架的共享参数，开发简洁的模型，它可以有效地处理多个任务，同时减少参数，而保持与密集模型相当或更高的性能。在训练过程中对模型进行稀疏化有助于减少模型的内存占用、计算需求和预测时间。我们在卷积神经网络的共享层中使用通道级l1/l2组稀疏。此方法不仅有助于消除多余的组（通道），还对权重施加惩罚，从而增强所有任务的学习能力。我们在两个公开可用的多任务学习数据集NYU-v2和CelebAMask-HQ上比较了组稀疏性下单任务和多任务实验的结果。

    Group sparsity in Machine Learning (ML) encourages simpler, more interpretable models with fewer active parameter groups. This work aims to incorporate structured group sparsity into the shared parameters of a Multi-Task Learning (MTL) framework, to develop parsimonious models that can effectively address multiple tasks with fewer parameters while maintaining comparable or superior performance to a dense model. Sparsifying the model during training helps decrease the model's memory footprint, computation requirements, and prediction time during inference. We use channel-wise l1/l2 group sparsity in the shared layers of the Convolutional Neural Network (CNN). This approach not only facilitates the elimination of extraneous groups (channels) but also imposes a penalty on the weights, thereby enhancing the learning of all tasks. We compare the outcomes of single-task and multi-task experiments under group sparsity on two publicly available MTL datasets, NYU-v2 and CelebAMask-HQ. We also i
    
[^26]: 广义持续类别发现

    Generalized Continual Category Discovery. (arXiv:2308.12112v1 [cs.LG])

    [http://arxiv.org/abs/2308.12112](http://arxiv.org/abs/2308.12112)

    本研究提出了一种广义持续类别发现（GCCD）的框架，用于在现实生活场景中同时处理新的和已知的类别，并且利用持续的无监督学习方法来发现它们。通过实验证明现有方法无法处理后续任务中的无标记样本。

    

    大多数持续学习（CL）方法推动着监督学习设置的极限，其中一个智能体期望学习新的标记任务而不会忘记先前的知识。然而，这些设置与现实生活场景不太吻合，其中学习智能体可以访问大量的无标记数据，包括全新（完全无标记）类别和已知类别的示例。受到广义类别发现（GCD）的启发，我们引入了一个新的框架来放松这个假设。确切地说，在任何任务中，我们允许存在新的和已知的类别，并且必须使用持续版本的无监督学习方法来发现它们。我们称这种设置为广义持续类别发现（GCCD）。它统一了CL和GCD，弥合了合成基准和现实生活场景之间的差距。通过一系列实验，我们发现现有的方法无法从后续任务中积累知识，其中包含无标记样本。

    Most of Continual Learning (CL) methods push the limit of supervised learning settings, where an agent is expected to learn new labeled tasks and not forget previous knowledge. However, these settings are not well aligned with real-life scenarios, where a learning agent has access to a vast amount of unlabeled data encompassing both novel (entirely unlabeled) classes and examples from known classes. Drawing inspiration from Generalized Category Discovery (GCD), we introduce a novel framework that relaxes this assumption. Precisely, in any task, we allow for the existence of novel and known classes, and one must use continual version of unsupervised learning methods to discover them. We call this setting Generalized Continual Category Discovery (GCCD). It unifies CL and GCD, bridging the gap between synthetic benchmarks and real-life scenarios. With a series of experiments, we present that existing methods fail to accumulate knowledge from subsequent tasks in which unlabeled samples of 
    
[^27]: 通过学习系数量化奇异模型中的退化情况

    Quantifying degeneracy in singular models via the learning coefficient. (arXiv:2308.12108v1 [stat.ML])

    [http://arxiv.org/abs/2308.12108](http://arxiv.org/abs/2308.12108)

    这项工作介绍了一种称为学习系数的量，用于精确量化深度神经网络中的退化程度，该方法能够区分不同参数区域的退化顺序，并揭示了随机优化器对临界点的归纳偏好。

    

    深度神经网络(DNN)是具有复杂退化的奇异统计模型。本文阐述了一种称为学习系数的量，它在奇异学习理论中精确地量化了深度神经网络的退化程度。重要的是，我们将证明DNN中的退化不能仅通过计算“平坦”方向的数量来解释。我们提出了一种基于随机梯度 Langevin 动力学的局部学习系数的计算可扩展近似方法。为了验证我们的方法，我们在已知理论值的低维模型上演示了其准确性。重要的是，局部学习系数能够正确恢复感兴趣参数区域之间退化的顺序。对MNIST的实验证明，局部学习系数可以揭示随机优化器对更退化或不太退化的临界点的归纳偏好。

    Deep neural networks (DNN) are singular statistical models which exhibit complex degeneracies. In this work, we illustrate how a quantity known as the \emph{learning coefficient} introduced in singular learning theory quantifies precisely the degree of degeneracy in deep neural networks. Importantly, we will demonstrate that degeneracy in DNN cannot be accounted for by simply counting the number of "flat" directions. We propose a computationally scalable approximation of a localized version of the learning coefficient using stochastic gradient Langevin dynamics. To validate our approach, we demonstrate its accuracy in low-dimensional models with known theoretical values. Importantly, the local learning coefficient can correctly recover the ordering of degeneracy between various parameter regions of interest. An experiment on MNIST shows the local learning coefficient can reveal the inductive bias of stochastic opitmizers for more or less degenerate critical points.
    
[^28]: 缓存操作重排：用于快速GNN训练的统一视图

    Cached Operator Reordering: A Unified View for Fast GNN Training. (arXiv:2308.12093v1 [cs.LG])

    [http://arxiv.org/abs/2308.12093](http://arxiv.org/abs/2308.12093)

    本文通过统一视图提供了一种解决图神经网络计算稀疏性挑战的方法，并通过缓存和自适应操作重排实现了GCN和GAT的高速运行，有效节省内存并缓解性能瓶颈。

    

    图神经网络（GNN）是处理结构化图数据和解决节点分类、图分类和聚类等任务的强大工具。然而，与传统深度神经网络相比，GNN计算的稀疏性提出了性能优化方面的新挑战。我们通过对GNN计算、输入/输出和内存进行统一视图来解决这些挑战。通过分析图卷积网络（GCN）和图注意力（GAT）两种广泛使用的GNN层的计算图，我们提出了替代的计算策略。我们提出了自适应操作重排和缓存，使GCN的速度提高了最多2.43倍，超过了当前最先进的技术。此外，对GAT的不同缓存方案的探索使速度提高了最多1.94倍。所提出的优化方法节省了内存，易于在各种硬件平台上实现，并有望缓解性能瓶颈。

    Graph Neural Networks (GNNs) are a powerful tool for handling structured graph data and addressing tasks such as node classification, graph classification, and clustering. However, the sparse nature of GNN computation poses new challenges for performance optimization compared to traditional deep neural networks. We address these challenges by providing a unified view of GNN computation, I/O, and memory. By analyzing the computational graphs of the Graph Convolutional Network (GCN) and Graph Attention (GAT) layers -- two widely used GNN layers -- we propose alternative computation strategies. We present adaptive operator reordering with caching, which achieves a speedup of up to 2.43x for GCN compared to the current state-of-the-art. Furthermore, an exploration of different caching schemes for GAT yields a speedup of up to 1.94x. The proposed optimizations save memory, are easily implemented across various hardware platforms, and have the potential to alleviate performance bottlenecks i
    
[^29]: 通过预训练稳定RNN的梯度

    Stabilizing RNN Gradients through Pre-training. (arXiv:2308.12075v1 [cs.LG])

    [http://arxiv.org/abs/2308.12075](http://arxiv.org/abs/2308.12075)

    该研究通过预训练网络实现局部稳定性，拓展了已有的稳定性理论，可以应用于复杂结构的循环神经网络。

    

    学习的众多理论都建议通过防止梯度的方差以指数形式随深度或时间增长来稳定和改善训练。通常情况下，这些分析是在前馈全连接神经网络或单层循环神经网络上进行的，因为它们具有数学的可解性。与此相反，本研究表明，当体系结构过于复杂以至于无法进行解析初始化时，通过预训练网络实现局部稳定性是有效的。此外，我们扩展了已知的稳定性理论，涵盖了一个更广泛的深层循环网络家族，对数据和参数分布的要求较少，这个理论被称为局部稳定性条件（LSC）。我们的调查发现，经典的Glorot、He和正交初始化方案在应用于前馈全连接神经网络时可以满足LSC。然而，在对深层循环网络进行分析时，我们发现了一种新的指数增长来源。

    Numerous theories of learning suggest to prevent the gradient variance from exponential growth with depth or time, to stabilize and improve training. Typically, these analyses are conducted on feed-forward fully-connected neural networks or single-layer recurrent neural networks, given their mathematical tractability. In contrast, this study demonstrates that pre-training the network to local stability can be effective whenever the architectures are too complex for an analytical initialization. Furthermore, we extend known stability theories to encompass a broader family of deep recurrent networks, requiring minimal assumptions on data and parameter distribution, a theory that we refer to as the Local Stability Condition (LSC). Our investigation reveals that the classical Glorot, He, and Orthogonal initialization schemes satisfy the LSC when applied to feed-forward fully-connected neural networks. However, analysing deep recurrent networks, we identify a new additive source of exponent
    
[^30]: 靠逆向强化学习识别随机模型预测控制车辆的反应感知驾驶风格

    Identifying Reaction-Aware Driving Styles of Stochastic Model Predictive Controlled Vehicles by Inverse Reinforcement Learning. (arXiv:2308.12069v1 [cs.RO])

    [http://arxiv.org/abs/2308.12069](http://arxiv.org/abs/2308.12069)

    本文通过逆向强化学习方法识别了随机模型预测控制车辆的反应感知驾驶风格。

    

    自动驾驶车辆（AV）的驾驶风格指的是其与其他AV的行为和相互作用方式。在多车辆自动驾驶系统中，能够识别附近AV的驾驶风格的AV可以可靠评估碰撞风险并做出更合理的驾驶决策。然而，在文献中对于AV驾驶风格的定义并不一致，尽管通常认为驾驶风格通过AV的轨迹进行编码，并可以使用最大熵逆向强化学习（ME-IRL）方法作为成本函数来识别。然而，驾驶风格的一个重要指标，即AV对附近AV的反应方式，在先前的ME-IRL方法的特征设计中并未充分考虑。本文将驾驶风格描述为一系列加权特征的成本函数。我们设计了额外的新特征来捕捉AV的反应感知特性。然后，我们进行驾驶风格的识别。

    The driving style of an Autonomous Vehicle (AV) refers to how it behaves and interacts with other AVs. In a multi-vehicle autonomous driving system, an AV capable of identifying the driving styles of its nearby AVs can reliably evaluate the risk of collisions and make more reasonable driving decisions. However, there has not been a consistent definition of driving styles for an AV in the literature, although it is considered that the driving style is encoded in the AV's trajectories and can be identified using Maximum Entropy Inverse Reinforcement Learning (ME-IRL) methods as a cost function. Nevertheless, an important indicator of the driving style, i.e., how an AV reacts to its nearby AVs, is not fully incorporated in the feature design of previous ME-IRL methods. In this paper, we describe the driving style as a cost function of a series of weighted features. We design additional novel features to capture the AV's reaction-aware characteristics. Then, we identify the driving styles 
    
[^31]: InstructionGPT-4: 一个200指令范式用于微调MiniGPT-4

    InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4. (arXiv:2308.12067v1 [cs.LG])

    [http://arxiv.org/abs/2308.12067](http://arxiv.org/abs/2308.12067)

    InstructionGPT-4通过仅使用200个例子进行微调，在多模式指令数据质量度量和选择器的帮助下，在各种评估任务中优于原始的MiniGPT-4。

    

    多模式大型语言模型通过两阶段的训练过程获取其遵循指令的能力：在图像-文本对上进行预训练，然后在监督式视觉-语言指令数据上进行微调。最近的研究表明，即使只有有限量的高质量遵循指令数据，大型语言模型也能取得令人满意的结果。在本文中，我们介绍了InstructionGPT-4，它经过微调的数据集只包含200个例子，约占MiniGPT-4对齐数据集中使用的遵循指令数据的6%。我们首先提出了几个用于评估多模式指令数据质量的度量指标。基于这些度量指标，我们提出了一个简单而有效的数据选择器，自动识别和过滤低质量的视觉-语言数据。通过采用这种方法，InstructionGPT-4在各种评估（如视觉问答、GPT-4偏好）上优于原始的MiniGPT-4。总体而言，我们的研究发现...

    Multimodal large language models acquire their instruction-following capabilities through a two-stage training process: pre-training on image-text pairs and fine-tuning on supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6% of the instruction-following data used in the alignment dataset for MiniGPT-4. We first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present a simple and effective data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations (e.g., visual question answering, GPT-4 preference). Overall, our findi
    
[^32]: 预门控MoE：快速且可扩展混合专家推理的算法和系统共同设计

    Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference. (arXiv:2308.12066v1 [cs.LG])

    [http://arxiv.org/abs/2308.12066](http://arxiv.org/abs/2308.12066)

    预门控MoE系统通过算法和系统的共同设计，有效解决了传统MoE架构的计算和存储挑战。

    

    在最近几年中，基于transformers的大型语言模型（LLMs）取得了重大进展，其成功源于模型规模的扩大。尽管算法性能很高，但LLMs的计算和存储需求带来了前所未有的挑战。为了解决LLMs的高计算需求，引入了混合专家（MoE）架构，能够在不成比例地扩大计算需求的情况下扩展模型大小。然而，MoE的高存储需求和稀疏专家的动态激活限制了其在实际问题中的适用性。之前的解决方案将MoE的内存占用高的专家参数转移到CPU内存上，但是从CPU迁移已激活的专家到GPU的延迟导致了高性能开销。我们提出的预门控MoE系统通过算法和系统的共同设计，有效解决了传统MoE架构的计算和存储挑战。

    Large language models (LLMs) based on transformers have made significant strides in recent years, the success of which is driven by scaling up their model size. Despite their high algorithmic performance, the computational and memory requirements of LLMs present unprecedented challenges. To tackle the high compute requirements of LLMs, the Mixture-of-Experts (MoE) architecture was introduced which is able to scale its model size without proportionally scaling up its computational requirements. Unfortunately, MoE's high memory demands and dynamic activation of sparse experts restrict its applicability to real-world problems. Previous solutions that offload MoE's memory-hungry expert parameters to CPU memory fall short because the latency to migrate activated experts from CPU to GPU incurs high performance overhead. Our proposed Pre-gated MoE system effectively tackles the compute and memory challenges of conventional MoE architectures using our algorithm-system co-design. Pre-gated MoE 
    
[^33]: 将不确定性度量集合起来以提高黑盒分类器安全性

    Ensembling Uncertainty Measures to Improve Safety of Black-Box Classifiers. (arXiv:2308.12065v1 [cs.SE])

    [http://arxiv.org/abs/2308.12065](http://arxiv.org/abs/2308.12065)

    本文提出了一种叫做SPROUT的方法，通过对黑盒分类器的输入和输出进行不确定性度量来提高系统的安全性。实验证明，SPROUT可以识别出绝大部分的错分情况。

    

    机器学习算法中的分类器可能会预测错误的类别，从而出现错分。众所周知，错分可能对系统产生连锁效应，可能导致关键故障。本文提出了SPROUT，一种通过对黑盒分类器的输入和输出进行不确定性度量来怀疑错分的安全性封装器。如果检测到错分，SPROUT会阻止将分类器的输出传播到系统中。对安全性的影响是，SPROUT将不稳定的输出（错分）转化为数据遗漏故障，在系统层面上可以很容易地进行管理。SPROUT具有广泛的应用范围，适用于二分类和多分类问题，包括图像和表格数据集。我们通过实验证明，SPROUT总是能够识别出大部分超级错分情况。

    Machine Learning (ML) algorithms that perform classification may predict the wrong class, experiencing misclassifications. It is well-known that misclassifications may have cascading effects on the encompassing system, possibly resulting in critical failures. This paper proposes SPROUT, a Safety wraPper thROugh ensembles of UncertainTy measures, which suspects misclassifications by computing uncertainty measures on the inputs and outputs of a black-box classifier. If a misclassification is detected, SPROUT blocks the propagation of the output of the classifier to the encompassing system. The resulting impact on safety is that SPROUT transforms erratic outputs (misclassifications) into data omission failures, which can be easily managed at the system level. SPROUT has a broad range of applications as it fits binary and multi-class classification, comprising image and tabular datasets. We experimentally show that SPROUT always identifies a huge fraction of the misclassifications of super
    
[^34]: HarvestNet：利用收获堆和遥感技术检测小农户农业活动的数据集

    HarvestNet: A Dataset for Detecting Smallholder Farming Activity Using Harvest Piles and Remote Sensing. (arXiv:2308.12061v1 [cs.CV])

    [http://arxiv.org/abs/2308.12061](http://arxiv.org/abs/2308.12061)

    该论文介绍了一种新的方法，通过检测全球小农户系统中常见的收获堆来映射农田的存在。作者提供了HarvestNet数据集，该数据集由专家知识和卫星图像收集而来，可用于在埃塞俄比亚的特定地区进行农田映射。实验结果表明，作者的最佳模型在手动标记数据上具有约80%的分类性能。

    

    小型农场在发展中国家的生产土地中所占比例很大。在撒哈拉以南非洲等地区，80%的农场都很小（面积小于2公顷），映射小农户的农田是追踪作物生产力等可持续发展措施的重要组成部分。然而，小农场的外观多样且微妙，传统的农田映射方法的效果受到限制。在这里，我们引入了一个基于检测遍布全球许多小农户系统的收获堆的新方法。我们提供了HarvestNet数据集，用于在埃塞俄比亚的提格雷和阿姆哈拉地区的2020-2023年间映射农田的存在，该数据集利用专家知识和卫星图像收集，共有7k个手动标记的图像和2k个地面收集标签。我们还针对现有技术在遥感领域进行了基准测试，我们的最佳模型在手动标记数据上具有约80%的分类性能。

    Small farms contribute to a large share of the productive land in developing countries. In regions such as sub-Saharan Africa, where 80% of farms are small (under 2 ha in size), the task of mapping smallholder cropland is an important part of tracking sustainability measures such as crop productivity. However, the visually diverse and nuanced appearance of small farms has limited the effectiveness of traditional approaches to cropland mapping. Here we introduce a new approach based on the detection of harvest piles characteristic of many smallholder systems throughout the world. We present HarvestNet, a dataset for mapping the presence of farms in the Ethiopian regions of Tigray and Amhara during 2020-2023, collected using expert knowledge and satellite images, totaling 7k hand-labeled images and 2k ground collected labels. We also benchmark a set of baselines including SOTA models in remote sensing with our best models having around 80% classification performance on hand labelled data
    
[^35]: 操作稳定扩散提示的嵌入

    Manipulating Embeddings of Stable Diffusion Prompts. (arXiv:2308.12059v1 [cs.CV])

    [http://arxiv.org/abs/2308.12059](http://arxiv.org/abs/2308.12059)

    本论文提出了一种直接改变提示嵌入的方法，促进了用户对生成图像细粒度和目标化的控制。这种方法将生成模型视为连续函数，并在图像空间和提示嵌入空间之间传递梯度。

    

    生成文本到图像的模型，如稳定扩散，允许用户根据文本描述生成图像。改变提示仍然是用户想要改变生成图像的主要方式。然而，通过重新构思提示来改变图像仍然是一个困难的试错过程，这导致了提示工程作为一个新的研究领域的出现。我们提出并分析了直接改变提示嵌入的方法，而不是提示文本。它允许更精细和有针对性的控制，考虑到用户的意图。我们的方法将生成的文本到图像模型视为一个连续函数，并在图像空间和提示嵌入空间之间传递梯度。通过解决不同的用户交互问题，我们可以在三个场景中应用这个想法：（1）优化图像空间中定义的度量，可以测量图像风格等。（2）帮助用户进行创造性的图像编辑。

    Generative text-to-image models such as Stable Diffusion allow users to generate images based on a textual description, the prompt. Changing the prompt is still the primary means for the user to change a generated image as desired. However, changing the image by reformulating the prompt remains a difficult process of trial and error, which has led to the emergence of prompt engineering as a new field of research. We propose and analyze methods to change the embedding of a prompt directly instead of the prompt text. It allows for more fine-grained and targeted control that takes into account user intentions. Our approach treats the generative text-to-image model as a continuous function and passes gradients between the image space and the prompt embedding space. By addressing different user interaction problems, we can apply this idea in three scenarios: (1) Optimization of a metric defined in image space that could measure, for example, image style. (2) Assistance of users in creative 
    
[^36]: 鲁棒学习抵御规避攻击的样本复杂度研究

    Sample Complexity of Robust Learning against Evasion Attacks. (arXiv:2308.12054v1 [cs.LG])

    [http://arxiv.org/abs/2308.12054](http://arxiv.org/abs/2308.12054)

    本论文研究了鲁棒学习在面对规避攻击时的样本复杂度。首先，我们发现分布假设在只有随机示例的情况下至关重要。接着，我们研究了满足Lipschitz条件的输入数据分布下，鲁棒学习单调并联所需的样本复杂度至少是指数级的。如果攻击者受限于扰动$O(\log n)$位，则样本复杂度会更高。

    

    理解机器学习模型对抗性攻击的弱点变得越来越重要。在对抗性机器学习中，一个基本问题是在规避攻击存在的情况下，需要多少训练数据量来量化。在本论文中，我们以准确在球内的鲁棒性为基础，从学习理论的角度研究了对抗攻击情况下的鲁棒学习可行性，考虑样本复杂度。首先，我们探讨了仅有随机示例的情况，并证明了分布假设的重要性。然后，我们着重研究具有满足Lipschitz条件的输入数据分布的学习问题，证明了鲁棒学习单调并联具有指数级的样本复杂度，对应于攻击者的预算（每个输入可以扰动的最大位数）的增长。但是，如果攻击者被限制在扰动$O(\log n)$位。

    It is becoming increasingly important to understand the vulnerability of machine learning models to adversarial attacks. One of the fundamental problems in adversarial machine learning is to quantify how much training data is needed in the presence of evasion attacks, where data is corrupted at test time. In this thesis, we work with the exact-in-the-ball notion of robustness and study the feasibility of adversarially robust learning from the perspective of learning theory, considering sample complexity.  We first explore the setting where the learner has access to random examples only, and show that distributional assumptions are essential. We then focus on learning problems with distributions on the input data that satisfy a Lipschitz condition and show that robustly learning monotone conjunctions has sample complexity at least exponential in the adversary's budget (the maximum number of bits it can perturb on each input). However, if the adversary is restricted to perturbing $O(\log
    
[^37]: 层级反馈传播

    Layer-wise Feedback Propagation. (arXiv:2308.12053v1 [cs.LG])

    [http://arxiv.org/abs/2308.12053](http://arxiv.org/abs/2308.12053)

    本文提出了一种名为“层级反馈传播（LFP）”的新型神经网络预测器训练方法，通过利用可解释性细化与层级相关性传播（LRP）相结合，根据每个连接对任务的贡献分配奖励，该方法克服了传统梯度下降方法存在的问题。对于各种模型和数据集，LFP取得了与梯度下降相当的性能。

    

    本文提出了一种称为“层级反馈传播（LFP）”的新型神经网络预测器训练方法，该方法利用可解释性，具体而言是层级相关性传播（LRP），根据每个连接对解决给定任务的贡献独立分配奖励。这与传统的梯度下降方法不同，梯度下降方法是朝向估计的损失最小值更新参数。LFP在模型中传播奖励信号，而无需梯度计算。它增强接收到正反馈的结构，同时降低接收到负反馈的结构的影响。我们从理论和实证的角度证明了LFP的收敛性，并展示了它在各种模型和数据集上实现与梯度下降相当的性能。值得注意的是，LFP克服了梯度方法的某些局限性，例如对有意义的导数的依赖。我们进一步研究了LFP如何解决梯度方法相关问题的限制。

    In this paper, we present Layer-wise Feedback Propagation (LFP), a novel training approach for neural-network-like predictors that utilizes explainability, specifically Layer-wise Relevance Propagation(LRP), to assign rewards to individual connections based on their respective contributions to solving a given task. This differs from traditional gradient descent, which updates parameters towards anestimated loss minimum. LFP distributes a reward signal throughout the model without the need for gradient computations. It then strengthens structures that receive positive feedback while reducingthe influence of structures that receive negative feedback. We establish the convergence of LFP theoretically and empirically, and demonstrate its effectiveness in achieving comparable performance to gradient descent on various models and datasets. Notably, LFP overcomes certain limitations associated with gradient-based methods, such as reliance on meaningful derivatives. We further investigate how 
    
[^38]: 用于计算深度神经网络正则化路径的多目标延续方法

    A multiobjective continuation method to compute the regularization path of deep neural networks. (arXiv:2308.12044v1 [cs.LG])

    [http://arxiv.org/abs/2308.12044](http://arxiv.org/abs/2308.12044)

    本文提出了一种多目标延续方法，用于计算深度神经网络的正则化路径，以解决DNNs中稀疏性和数值效率之间的冲突。

    

    稀疏性是深度神经网络(DNNs)中非常理想的特征，因为它确保了数值效率，提高了模型的可解释性(由于相关特征的数量较少)和鲁棒性。在基于线性模型的机器学习方法中，众所周知在$\ell^1$范数(即零权重)的最稀疏解和非正则化解之间存在一条连接路径，这条路径被称为正则化路径。最近，通过将经验损失和稀疏性($\ell^1$范数)作为两个冲突的标准，并解决由此产生的多目标优化问题，首次尝试将正则化路径的概念扩展到DNNs。然而，由于$\ell^1$范数的不光滑性和参数数量的高度，从计算的角度来看，这种方法并不是很有效。为了克服这个限制，我们提出了一种算法，可以近似计算整个帕累托曲线

    Sparsity is a highly desired feature in deep neural networks (DNNs) since it ensures numerical efficiency, improves the interpretability of models (due to the smaller number of relevant features), and robustness. In machine learning approaches based on linear models, it is well known that there exists a connecting path between the sparsest solution in terms of the $\ell^1$ norm (i.e., zero weights) and the non-regularized solution, which is called the regularization path. Very recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the resulting multiobjective optimization problem. However, due to the non-smoothness of the $\ell^1$ norm and the high number of parameters, this approach is not very efficient from a computational perspective. To overcome this limitation, we present an algorithm that allows for the approximation of the entire Pareto
    
[^39]: IncreLoRA: 增量参数分配方法用于参数高效调优

    IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning. (arXiv:2308.12043v1 [cs.CL])

    [http://arxiv.org/abs/2308.12043](http://arxiv.org/abs/2308.12043)

    IncreLoRA是一种增量参数分配方法，根据模块的重要性分数在训练过程中自适应地添加可训练参数，以实现参数高效调优。

    

    随着预训练语言模型（PLM）的大小不断增加，对模型中的所有参数进行微调并不高效，特别是当存在大量的下游任务时，这会导致显著的训练和存储成本。已有许多参数高效调优（PEFT）方法被提出，其中，低秩自适应（LoRA）是一种将可训练的秩分解矩阵注入每个目标模块的典型方法。然而，LoRA忽视了不同模块中参数的重要性。为了解决这个问题，已经提出了许多方法来剪枝LoRA的参数。然而，在有限的训练条件下，修剪参数矩阵的秩的上界仍然受到预设值的影响。因此，我们提出了IncreLoRA，一种增量参数分配方法，根据每个模块的重要性分数在训练过程中自适应地添加可训练参数。这种方法与修剪方法不同，因为它从增加参数的角度来优化调优过程。

    With the increasing size of pre-trained language models (PLMs), fine-tuning all the parameters in the model is not efficient, especially when there are a large number of downstream tasks, which incur significant training and storage costs. Many parameter-efficient fine-tuning (PEFT) approaches have been proposed, among which, Low-Rank Adaptation (LoRA) is a representative approach that injects trainable rank decomposition matrices into every target module. Yet LoRA ignores the importance of parameters in different modules. To address this problem, many works have been proposed to prune the parameters of LoRA. However, under limited training conditions, the upper bound of the rank of the pruned parameter matrix is still affected by the preset values. We, therefore, propose IncreLoRA, an incremental parameter allocation method that adaptively adds trainable parameters during training based on the importance scores of each module. This approach is different from the pruning method as it i
    
[^40]: CACTUS: 一个全面的抽象和分类工具，用于发现结构

    CACTUS: a Comprehensive Abstraction and Classification Tool for Uncovering Structures. (arXiv:2308.12031v1 [cs.LG])

    [http://arxiv.org/abs/2308.12031](http://arxiv.org/abs/2308.12031)

    CACTUS是一个用于发现结构的全面的抽象和分类工具，通过可解释的人工智能支持安全分析，提供对分类属性的额外支持，并通过并行化优化内存使用和加快计算速度。

    

    大规模数据集的可用性推动了当前人工智能发展的动力。然而，由于实际和成本效益的部署以及深度学习模型的不透明性，对于使用小数据集开发解决方案存在挑战。CACTUS是一个全面的抽象和分类工具，用于通过可解释的人工智能有效地支持安全分析。它提供对分类属性的额外支持，保持其原始含义，通过并行化优化内存使用和加快计算速度。它向用户显示每个类别的属性频率并根据其区分能力对其进行排序。通过应用于威斯康星诊断乳腺癌和甲状腺0387数据集对其性能进行评估。

    The availability of large data sets is providing an impetus for driving current artificial intelligent developments. There are, however, challenges for developing solutions with small data sets due to practical and cost-effective deployment and the opacity of deep learning models. The Comprehensive Abstraction and Classification Tool for Uncovering Structures called CACTUS is presented for improved secure analytics by effectively employing explainable artificial intelligence. It provides additional support for categorical attributes, preserving their original meaning, optimising memory usage, and speeding up the computation through parallelisation. It shows to the user the frequency of the attributes in each class and ranks them by their discriminative power. Its performance is assessed by application to the Wisconsin diagnostic breast cancer and Thyroid0387 data sets.
    
[^41]: 基于提示的长度受控生成与强化学习

    Prompt-Based Length Controlled Generation with Reinforcement Learning. (arXiv:2308.12030v1 [cs.CL])

    [http://arxiv.org/abs/2308.12030](http://arxiv.org/abs/2308.12030)

    提出了一种基于提示的长度控制方法，利用强化学习和奖励模型来实现大型语言模型（LLM）的长度受控生成。该方法可以有效减少推理成本并满足不同需求。

    

    最近，大型语言模型（LLM）如ChatGPT和GPT-4因其惊人的改进和性能而受到广泛关注。长度受控生成成为LLM中的一个重要话题，它还使用户能够充分利用LLM的能力在更多实际场景中生成所需长度的合适答案或文章。此外，LLM中的自回归生成非常耗时，而控制生成长度的能力可以通过限制长度任意降低推理成本，从而满足不同需求。因此，我们旨在提出一种基于提示的长度控制方法来实现长度受控生成，这种方法也可以广泛应用于类似GPT的LLM中。具体而言，我们采用强化学习，使用可训练或基于规则的奖励模型提供奖励信号，进一步通过对预定义目标长度进行奖励来影响LLM的生成。实验证明...

    Recently, large language models (LLMs) like ChatGPT and GPT-4 have attracted great attention given their surprising improvement and performance. Length controlled generation of LLMs emerges as an important topic, which also enables users to fully leverage the capability of LLMs in more real-world scenarios like generating a proper answer or essay of a desired length. In addition, the autoregressive generation in LLMs is extremely time-consuming, while the ability of controlling this generated length can arbitrarily reduce the inference cost by limiting the length, and thus satisfy different needs. Therefore, we aim to propose a prompt-based length control method to achieve this length controlled generation, which can also be widely applied in GPT-style LLMs. In particular, we adopt reinforcement learning with the reward signal given by either trainable or rule-based reward model, which further affects the generation of LLMs via rewarding a pre-defined target length. Experiments show th
    
[^42]: 一种针对多任务学习的尺度不变任务平衡方法

    A Scale-Invariant Task Balancing Approach for Multi-Task Learning. (arXiv:2308.12029v1 [cs.LG])

    [http://arxiv.org/abs/2308.12029](http://arxiv.org/abs/2308.12029)

    这篇论文提出了一种尺度不变的多任务学习方法（SI-MTL），通过对任务损失进行对数变换和对任务梯度进行归一化，解决了多任务学习中的任务平衡问题，并在多个基准数据集上取得了领先的性能。

    

    多任务学习（MTL）是一种同时学习多个相关任务的学习范式，在各个领域取得了巨大的成功。然而，任务平衡仍然是MTL中的一个重要挑战，损失/梯度尺度的不平衡经常导致性能折中。本文提出了一种尺度不变的多任务学习（SI-MTL）方法，从损失和梯度角度缓解了任务平衡问题。具体来说，SI-MTL包含对所有任务损失进行的对数变换，以确保在损失水平上具有尺度不变性，以及一种梯度平衡方法SI-G，它将所有任务的梯度归一化为与最大梯度范数相同的大小。在几个基准数据集上进行的大量实验一致证明了SI-G的有效性和SI-MTL的最先进性能。

    Multi-task learning (MTL), a learning paradigm to learn multiple related tasks simultaneously, has achieved great success in various fields. However, task-balancing remains a significant challenge in MTL, with the disparity in loss/gradient scales often leading to performance compromises. In this paper, we propose a Scale-Invariant Multi-Task Learning (SI-MTL) method to alleviate the task-balancing problem from both loss and gradient perspectives. Specifically, SI-MTL contains a logarithm transformation which is performed on all task losses to ensure scale-invariant at the loss level, and a gradient balancing method, SI-G, which normalizes all task gradients to the same magnitude as the maximum gradient norm. Extensive experiments conducted on several benchmark datasets consistently demonstrate the effectiveness of SI-G and the state-of-the-art performance of SI-MTL.
    
[^43]: 有偏差感知的最小化：理解和减轻私有SGD中的估计偏差

    Bias-Aware Minimisation: Understanding and Mitigating Estimator Bias in Private SGD. (arXiv:2308.12018v1 [cs.LG])

    [http://arxiv.org/abs/2308.12018](http://arxiv.org/abs/2308.12018)

    本文提出了有偏差感知的最小化（BAM）方法，可以证明降低私有梯度估计偏差，并在实验中证明BAM不仅可以减少偏差，还可以在多个数据集上改善隐私-效用的权衡。

    

    差分隐私SGD（DP-SGD）承诺能够安全和负责任地将机器学习应用于敏感数据集。然而，DP-SGD仅提供有偏差、噪声较大的小批量梯度估计。这使得优化步骤变得不太有效，并因此限制了模型的效用。本文通过展示每个样本梯度范数与DP-SGD中使用的私有梯度预测的估计偏差之间的关系，提出了有偏差感知的最小化（BAM）方法，可以证明降低私有梯度估计偏差。我们展示了如何高效计算BAM所需的量，以适用于大规模神经网络，并突出了与相关方法（如Sharpness-Aware Minimisation）之间的相似之处。最后，我们提供实证证据表明BAM不仅可以减少偏差，还可以在CIFAR-10、CIFAR-100和ImageNet-32数据集上显著改善隐私-效用的权衡。

    Differentially private SGD (DP-SGD) holds the promise of enabling the safe and responsible application of machine learning to sensitive datasets. However, DP-SGD only provides a biased, noisy estimate of a mini-batch gradient. This renders optimisation steps less effective and limits model utility as a result. With this work, we show a connection between per-sample gradient norms and the estimation bias of the private gradient oracle used in DP-SGD. Here, we propose Bias-Aware Minimisation (BAM) that allows for the provable reduction of private gradient estimator bias. We show how to efficiently compute quantities needed for BAM to scale to large neural networks and highlight similarities to closely related methods such as Sharpness-Aware Minimisation. Finally, we provide empirical evidence that BAM not only reduces bias but also substantially improves privacy-utility trade-offs on the CIFAR-10, CIFAR-100, and ImageNet-32 datasets.
    
[^44]: MKL-$L_{0/1}$-SVM: 一种多核学习的支持向量机框架

    MKL-$L_{0/1}$-SVM. (arXiv:2308.12016v1 [stat.ML])

    [http://arxiv.org/abs/2308.12016](http://arxiv.org/abs/2308.12016)

    本文提出了一种多核学习的支持向量机框架(MKL-$L_{0/1}$-SVM)，通过开发快速的ADMM求解器处理非凸非光滑的优化问题，并在实验中展示了与领先方法相当的性能。

    

    本文提出了一种适用于$(0, 1)$损失函数的支持向量机的多核学习（MKL）框架。首先给出了一阶最优性条件，然后利用它们开发了一个快速的ADMM求解器来处理非凸非光滑的优化问题。详细的合成和真实数据集上的实验表明，我们的MKL-$L_{0/1}$-SVM的性能与一种名为SimpleMKL的领先方法相当。

    This paper presents a Multiple Kernel Learning (abbreviated as MKL) framework for the Support Vector Machine (SVM) with the $(0, 1)$ loss function. Some first-order optimality conditions are given and then exploited to develop a fast ADMM solver to deal with the nonconvex and nonsmooth optimization problem. Extensive numerical experiments on synthetic and real datasets show that the performance of our MKL-$L_{0/1}$-SVM is comparable with the one of the leading approaches called SimpleMKL developed by Rakotomamonjy, Bach, Canu, and Grandvalet [Journal of Machine Learning Research, vol. 9, pp. 2491-2521, 2008].
    
[^45]: 量子噪声驱动的生成扩散模型

    Quantum-Noise-driven Generative Diffusion Models. (arXiv:2308.12013v1 [quant-ph])

    [http://arxiv.org/abs/2308.12013](http://arxiv.org/abs/2308.12013)

    该论文提出了三种量子噪声驱动的生成扩散模型，利用了量子特性以克服传统模型的主要计算困难，并建议将量子噪声视为可利用的特性而非问题。

    

    通过机器学习技术实现的生成模型是从有限的训练样本中推断出复杂和未知数据分布并产生新的合成数据的强大工具。扩散模型是一种新兴的框架，最近在创建合成文本和高质量图像方面已经超越了生成对抗性网络的性能。在这里，我们提出并讨论了扩散模型的量子推generalization，即三种可能在实际量子系统上进行实验的量子噪声驱动的生成扩散模型。我们的想法是利用独特的量子特性，特别是目前可用的有噪声量子处理器不可避免地受到的相干性、纠缠性和噪声之间的非平凡相互作用，以克服传统扩散模型在推断过程中的主要计算负担。因此，我们建议将量子噪声不作为需要检测和解决的问题，而是作为一种可利用的特性，使得扩散模型能够更好地工作。

    Generative models realized with machine learning techniques are powerful tools to infer complex and unknown data distributions from a finite number of training samples in order to produce new synthetic data. Diffusion models are an emerging framework that have recently overcome the performance of the generative adversarial networks in creating synthetic text and high-quality images. Here, we propose and discuss the quantum generalization of diffusion models, i.e., three quantum-noise-driven generative diffusion models that could be experimentally tested on real quantum systems. The idea is to harness unique quantum features, in particular the non-trivial interplay among coherence, entanglement and noise that the currently available noisy quantum processors do unavoidably suffer from, in order to overcome the main computational burdens of classical diffusion models during inference. Hence, we suggest to exploit quantum noise not as an issue to be detected and solved but instead as a ver
    
[^46]: 磁滞建模的神经振荡器

    Neural oscillators for magnetic hysteresis modeling. (arXiv:2308.12002v1 [cs.LG])

    [http://arxiv.org/abs/2308.12002](http://arxiv.org/abs/2308.12002)

    本研究开发了一种名为HystRNN的神经振荡器，通过更新隐藏状态来建模和量化磁滞现象，并且在预测广义场景中表现出了良好的泛化能力。这项研究突显了神经振荡器在捕捉磁性材料的复杂磁滞模式方面相对于传统基于RNN的方法的优势。

    

    磁滞是科学和工程中普遍存在的现象；其建模和识别对于理解和优化各种系统的行为至关重要。我们开发了一种基于常微分方程循环神经网络（RNN）的方法来建模和量化磁滞，它表现为时序性和历史依赖性。我们的神经振荡器，HystRNN，受耦合振荡RNN和现象学磁滞模型的启发，通过更新隐藏状态来描述磁滞。评估了HystRNN在预测广义场景（包括一阶反转曲线和小环）中的性能。结果表明，HystRNN具有将其行为推广到以前未经训练区域的能力，这是磁滞模型必须具备的重要特征。这项研究突显了相对于传统基于RNN的方法，在捕捉磁性材料中复杂磁滞模式方面，神经振荡器的优势。

    Hysteresis is a ubiquitous phenomenon in science and engineering; its modeling and identification are crucial for understanding and optimizing the behavior of various systems. We develop an ordinary differential equation-based recurrent neural network (RNN) approach to model and quantify the hysteresis, which manifests itself in sequentiality and history-dependence. Our neural oscillator, HystRNN, draws inspiration from coupled-oscillatory RNN and phenomenological hysteresis models to update the hidden states. The performance of HystRNN is evaluated to predict generalized scenarios, involving first-order reversal curves and minor loops. The findings show the ability of HystRNN to generalize its behavior to previously untrained regions, an essential feature that hysteresis models must have. This research highlights the advantage of neural oscillators over the traditional RNN-based methods in capturing complex hysteresis patterns in magnetic materials, where traditional rate-dependent me
    
[^47]: 有关在有限预算二臂赌博机中进行最佳臂选择的统一最优算法研究

    On Uniformly Optimal Algorithms for Best Arm Identification in Two-Armed Bandits with Fixed Budget. (arXiv:2308.12000v1 [stat.ML])

    [http://arxiv.org/abs/2308.12000](http://arxiv.org/abs/2308.12000)

    本文研究了在有限预算的随机二臂赌博机中进行最佳臂选择的问题，并证明不存在比等概率采样算法更好的算法。我们引入了一致稳定算法的概念，并证明任何在所有情况下与等概率采样算法表现一样好的算法必须属于这个类别。这一结果解决了之前的两个未解之谜。

    

    本文研究了在具有伯努利奖励的随机二臂赌博机中，使用有限预算进行最佳臂选择的问题。我们证明令人惊讶的是，不存在一个算法可以在所有情况下与等概率采样算法表现一样好（该算法被称为“均匀采样”算法），并且在至少一个情况下明显优于该算法。简而言之，不存在比均匀采样算法更好的算法。为了证明这一结果，我们引入了“一致”和“稳定”算法的自然类，并且证明了任何算法要在所有情况下与均匀采样算法表现一样好，必须属于这个类别。通过导出满足任何一致且稳定算法的错误率的下界，并证明均匀采样算法与此下界相匹配，我们完成了证明过程。我们的结果解决了\cite{qin2022open}中提出的两个未解之谜。

    We study the problem of best-arm identification with fixed budget in stochastic two-arm bandits with Bernoulli rewards. We prove that surprisingly, there is no algorithm that (i) performs as well as the algorithm sampling each arm equally (this algorithm is referred to as the {\it uniform sampling} algorithm) on all instances, and that (ii) strictly outperforms this algorithm on at least one instance. In short, there is no algorithm better than the uniform sampling algorithm. Towards this result, we introduce the natural class of {\it consistent} and {\it stable} algorithms, and show that any algorithm that performs as well as the uniform sampling algorithm on all instances belongs to this class. The proof is completed by deriving a lower bound on the error rate satisfied by any consistent and stable algorithm, and by showing that the uniform sampling algorithm matches this lower bound. Our results provide a solution to the two open problems presented in \cite{qin2022open}.
    
[^48]: 关于关系概念模型的研究

    Relational Concept Based Models. (arXiv:2308.11991v1 [cs.LG])

    [http://arxiv.org/abs/2308.11991](http://arxiv.org/abs/2308.11991)

    关系概念模型是一种关系深度学习方法家族，用于在关系领域提供可解释的任务预测，相比非关系的基于概念的模型，它在泛化性能上与现有的关系模型相匹配，并支持生成量化的基于概念的解释，同时在测试时干预、超出分布情景、有限的训练数据范围和稀缺的概念监督等苛刻条件下也能有效应对。

    

    在关系领域中设计可解释的深度学习模型是一个开放性挑战：可解释的深度学习方法，如基于概念的模型（CBMs），并没有设计来解决关系问题，而关系模型也没有像CBMs那样可解释。为了解决这个问题，我们提出了关系概念模型，这是一种提供可解释任务预测的关系深度学习方法家族。我们的实验从图像分类到知识图谱中的链接预测，表明关系CBMs：（i）与现有的关系黑盒的泛化性能相匹配（不同于非关系的CBMs），（ii）支持生成量化的基于概念的解释，（iii）有效应对测试时的干预，以及（iv）经受住包括超出分布情景、有限的训练数据范围和稀缺的概念监督等苛刻条件。

    The design of interpretable deep learning models working in relational domains poses an open challenge: interpretable deep learning methods, such as Concept-Based Models (CBMs), are not designed to solve relational problems, while relational models are not as interpretable as CBMs. To address this problem, we propose Relational Concept-Based Models, a family of relational deep learning methods providing interpretable task predictions. Our experiments, ranging from image classification to link prediction in knowledge graphs, show that relational CBMs (i) match generalization performance of existing relational black-boxes (as opposed to non-relational CBMs), (ii) support the generation of quantified concept-based explanations, (iii) effectively respond to test-time interventions, and (iv) withstand demanding settings including out-of-distribution scenarios, limited training data regimes, and scarce concept supervisions.
    
[^49]: 更具表现力的图神经网络在生成任务中是否更好？

    Will More Expressive Graph Neural Networks do Better on Generative Tasks?. (arXiv:2308.11978v1 [cs.LG])

    [http://arxiv.org/abs/2308.11978](http://arxiv.org/abs/2308.11978)

    本论文调查了更具表现力的图神经网络在分子图生成任务中的表现能力，并通过替换图生成模型的基础GNN来进行实验。研究发现，使用更具表现力的GNN可以改善生成任务的性能。

    

    图生成是一个重要的挑战，它涉及根据给定的标签预测一个完整的具有多个节点和边的图。这个任务对许多实际应用非常重要，包括药物和分子设计。近年来，在图生成领域出现了几种成功的方法。然而，这些方法存在两个重大问题：(1) 这些方法中使用的基础图神经网络（GNN）架构往往未经深入探索；(2) 这些方法往往只在有限的指标上进行评估。为填补这个空白，我们通过将图生成模型的基础GNN替换为更具表现力的GNN，研究了GNN在分子图生成任务中的表现能力。具体而言，我们分析了两种不同生成框架（GCPN和GraphAF）中六种GNN在六个不同的分子生成目标上的性能。

    Graph generation poses a significant challenge as it involves predicting a complete graph with multiple nodes and edges based on simply a given label. This task also carries fundamental importance to numerous real-world applications, including de-novo drug and molecular design. In recent years, several successful methods have emerged in the field of graph generation. However, these approaches suffer from two significant shortcomings: (1) the underlying Graph Neural Network (GNN) architectures used in these methods are often underexplored; and (2) these methods are often evaluated on only a limited number of metrics. To fill this gap, we investigate the expressiveness of GNNs under the context of the molecular graph generation task, by replacing the underlying GNNs of graph generative models with more expressive GNNs. Specifically, we analyse the performance of six GNNs in two different generative frameworks (GCPN and GraphAF), on six different molecular generative objectives on the ZIN
    
[^50]: 使用符合回归方法近似评分解释技术

    Approximating Score-based Explanation Techniques Using Conformal Regression. (arXiv:2308.11975v1 [cs.LG])

    [http://arxiv.org/abs/2308.11975](http://arxiv.org/abs/2308.11975)

    本研究提出了使用计算成本较低的回归模型来近似评分解释技术，并采用归纳式符合预测框架提供近似值的有效性保证。实证研究结果显示，该方法能显著提升执行时间。

    

    基于评分的可解释机器学习技术经常被用来理解黑盒模型背后的逻辑。然而，这些解释技术通常计算成本高昂，限制了它们在时间关键环境中的应用。因此，我们提出并研究了使用计算成本较低的回归模型来近似评分解释技术（如SHAP）的输出。此外，我们采用归纳式符合预测框架提供了近似值的有效性保证。我们提出了几个非符合度度量，旨在同时考虑近似解释的难度和计算成本的低廉。我们进行了大规模的实证研究，评估了我们提出的模型生成的近似解释的效率（区间大小）。结果表明，所提出的方法可以显著改善执行时间。

    Score-based explainable machine-learning techniques are often used to understand the logic behind black-box models. However, such explanation techniques are often computationally expensive, which limits their application in time-critical contexts. Therefore, we propose and investigate the use of computationally less costly regression models for approximating the output of score-based explanation techniques, such as SHAP. Moreover, validity guarantees for the approximated values are provided by the employed inductive conformal prediction framework. We propose several non-conformity measures designed to take the difficulty of approximating the explanations into account while keeping the computational cost low. We present results from a large-scale empirical investigation, in which the approximate explanations generated by our proposed models are evaluated with respect to efficiency (interval size). The results indicate that the proposed method can significantly improve execution time com
    
[^51]: EVE: 使用遮蔽预测和模态感知的高效视觉-语言预训练

    EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE. (arXiv:2308.11971v1 [cs.CV])

    [http://arxiv.org/abs/2308.11971](http://arxiv.org/abs/2308.11971)

    本文引入了一种名为EVE的高效视觉-语言预训练模型，通过遮蔽信号建模和模态感知的方式，实现了统一的多模态Transformer网络，加速了训练进程，并取得了良好的效果。

    

    在本文中，我们介绍了一种名为EVE的高效视觉-语言基础模型，它是由一种统一的Transformer进行预训练的统一多模态模型。EVE通过在图像-文本对上进行遮蔽信号建模来统一视觉和语言的预训练任务，以重建可见信号，即图像像素和文本标记。通过集成模态感知的稀疏专家混合模块，EVE在一个共享的Transformer网络中编码了视觉和语言，并通过选择性地切换到不同的专家来捕捉模态特定信息。

    Building scalable vision-language models to learn from diverse, multimodal data remains an open challenge. In this paper, we introduce an Efficient Vision-languagE foundation model, namely EVE, which is one unified multimodal Transformer pre-trained solely by one unified pre-training task. Specifically, EVE encodes both vision and language within a shared Transformer network integrated with modality-aware sparse Mixture-of-Experts (MoE) modules, which capture modality-specific information by selectively switching to different experts. To unify pre-training tasks of vision and language, EVE performs masked signal modeling on image-text pairs to reconstruct masked signals, i.e., image pixels and text tokens, given visible signals. This simple yet effective pre-training objective accelerates training by 3.5x compared to the model pre-trained with Image-Text Contrastive and Image-Text Matching losses. Owing to the combination of the unified architecture and pre-training task, EVE is easy t
    
[^52]: 不均匀混合网络用于带不确定性评估的肝脏肿瘤分割

    Anisotropic Hybrid Networks for liver tumor segmentation with uncertainty quantification. (arXiv:2308.11969v1 [eess.IV])

    [http://arxiv.org/abs/2308.11969](http://arxiv.org/abs/2308.11969)

    本研究提出了两种不同的基于不均匀模型的流程，用于带有不确定性评估的肝脏肿瘤分割。第一个流程使用多类模型同时分割肝脏和肿瘤，而第二个流程使用两个二进制模型分别分割肝脏和肿瘤。结果表明这两种流程在分割肝脏和肿瘤方面都取得了良好效果。

    

    肝脏肿瘤的负担很重要，是癌症死亡的第四大原因。对于肝细胞癌（HCC），在对比增强磁共振成像（CE-MRI）上的肝脏和肿瘤进行界定，以指导治疗策略。由于这个任务耗时，需要高度的专业知识，且可能存在观察者之间的差异，因此迫切需要自动化工具。然而，缺乏可用的训练数据以及图像分辨率和MRI序列的高度变异性都带来了挑战。在本研究中，我们提出了两种基于不均匀模型的不同流程，用于获取肝脏和肿瘤的分割。第一个流程是基于多类模型的基线模型，可以同时分割肝脏和肿瘤类别。在第二种方法中，我们训练了两个不同的二进制模型，一个用于分割肝脏，另一个用于分割肿瘤。我们的结果表明，这两种流程都可以取得良好的分割效果。

    The burden of liver tumors is important, ranking as the fourth leading cause of cancer mortality. In case of hepatocellular carcinoma (HCC), the delineation of liver and tumor on contrast-enhanced magnetic resonance imaging (CE-MRI) is performed to guide the treatment strategy. As this task is time-consuming, needs high expertise and could be subject to inter-observer variability there is a strong need for automatic tools. However, challenges arise from the lack of available training data, as well as the high variability in terms of image resolution and MRI sequence. In this work we propose to compare two different pipelines based on anisotropic models to obtain the segmentation of the liver and tumors. The first pipeline corresponds to a baseline multi-class model that performs the simultaneous segmentation of the liver and tumor classes. In the second approach, we train two distinct binary models, one segmenting the liver only and the other the tumors. Our results show that both pipe
    
[^53]: 通过再生性正则化维持可塑性

    Maintaining Plasticity via Regenerative Regularization. (arXiv:2308.11958v1 [cs.LG])

    [http://arxiv.org/abs/2308.11958](http://arxiv.org/abs/2308.11958)

    本文提出了一种名为L2 Init的简单方法，通过将L2正则化应用于初始参数，来维持神经网络在处理非平稳数据流时的可塑性且易于实施。该方法使得参数能够迅速适应新任务并减轻可塑性的丢失。

    

    在连续学习中，可塑性指的是代理快速适应新信息的能力。已知神经网络在处理非平稳数据流时会失去可塑性。本文提出了一种名为L2 Init的非常简单的方法，通过将L2正则化应用于初始参数，来维持可塑性。这与标准的L2正则化非常相似，唯一的区别在于L2 Init正则化朝向原点。L2 Init易于实施，只需要选择一个超参数。这个方法的动机与重置神经元或参数值的方法相同。直观上讲，当最近的损失对特定参数不敏感时，这些参数会向它们的初始值漂移。这使得参数能够迅速适应新任务。在代表连续学习中不同类型非平稳性的简单问题上，我们证明了L2 Init能够一致地减轻可塑性的丢失。

    In continual learning, plasticity refers to the ability of an agent to quickly adapt to new information. Neural networks are known to lose plasticity when processing non-stationary data streams. In this paper, we propose L2 Init, a very simple approach for maintaining plasticity by incorporating in the loss function L2 regularization toward initial parameters. This is very similar to standard L2 regularization (L2), the only difference being that L2 regularizes toward the origin. L2 Init is simple to implement and requires selecting only a single hyper-parameter. The motivation for this method is the same as that of methods that reset neurons or parameter values. Intuitively, when recent losses are insensitive to particular parameters, these parameters drift toward their initial values. This prepares parameters to adapt quickly to new tasks. On simple problems representative of different types of nonstationarity in continual learning, we demonstrate that L2 Init consistently mitigates 
    
[^54]: 当MiniBatch SGD遇上SplitFed Learning: 收敛性分析和性能评估

    When MiniBatch SGD Meets SplitFed Learning:Convergence Analysis and Performance Evaluation. (arXiv:2308.11953v1 [cs.LG])

    [http://arxiv.org/abs/2308.11953](http://arxiv.org/abs/2308.11953)

    当MiniBatch SGD遇上SplitFed Learning：本文提出了MiniBatch-SFL算法，通过引入MiniBatch SGD到SplitFed Learning中，解决了非均衡数据导致的客户端漂移问题。

    

    联邦学习（FL）使得分布式客户端（例如边缘设备）能够协同进行模型训练，而不需要共享原始数据。然而，FL在计算上可能会很昂贵，因为客户端需要训练整个模型多次。SplitFed学习（SFL）是一种最近的分布式方法，它通过在一个切割层将模型分成两部分来减轻客户设备上的计算负载，其中客户端只需要训练模型的一部分。然而，当客户端数据高度不均衡时，SFL仍然面临着“客户端漂移”问题。为了解决这个问题，我们提出了MiniBatch-SFL。该算法将MiniBatch SGD引入SFL中，其中客户端以FL方式训练客户端模型，而服务器则类似于MiniBatch SGD训练服务器端模型。我们分析了MiniBatch-SFL的收敛性，并通过分析预期的服务器端和客户端模型更新来得到期望损失的界限。

    Federated learning (FL) enables collaborative model training across distributed clients (e.g., edge devices) without sharing raw data. Yet, FL can be computationally expensive as the clients need to train the entire model multiple times. SplitFed learning (SFL) is a recent distributed approach that alleviates computation workload at the client device by splitting the model at a cut layer into two parts, where clients only need to train part of the model. However, SFL still suffers from the \textit{client drift} problem when clients' data are highly non-IID. To address this issue, we propose MiniBatch-SFL. This algorithm incorporates MiniBatch SGD into SFL, where the clients train the client-side model in an FL fashion while the server trains the server-side model similar to MiniBatch SGD. We analyze the convergence of MiniBatch-SFL and show that the bound of the expected loss can be obtained by analyzing the expected server-side and client-side model updates, respectively. The server-s
    
[^55]: 多尺度Transformer金字塔网络用于多元时间序列预测

    Multi-scale Transformer Pyramid Networks for Multivariate Time Series Forecasting. (arXiv:2308.11946v1 [cs.LG])

    [http://arxiv.org/abs/2308.11946](http://arxiv.org/abs/2308.11946)

    本论文提出了一种多尺度Transformer金字塔网络(MTPNet)，通过引入维度不变的嵌入技术和捕捉不受限制尺度上的时间依赖性，解决了在多元时间序列预测中捕捉多样的季节性的问题。

    

    多元时间序列（MTS）预测涉及对历史记录中的时间依赖性进行建模。由于能够捕捉长期依赖关系，Transformer在MTS预测中表现出了显著的性能。然而，先前的工作仅局限于在固定尺度或指数级增加的多个尺度上建模时间依赖性（大部分以2为基数）。这种限制阻碍了其在捕捉多样的季节性（例如小时和日常模式）方面的有效性。本文介绍了一种维度不变的嵌入技术，用于捕捉短期时间依赖性并将MTS数据投影到更高维度的空间中，同时保留MTS数据中的时间步骤和变量维度。此外，我们提出了一种新颖的多尺度Transformer金字塔网络（MTPNet），专门设计用于有效地捕捉多个不受限制尺度上的时间依赖性。预测是从多尺度潜变量中推断得出的。

    Multivariate Time Series (MTS) forecasting involves modeling temporal dependencies within historical records. Transformers have demonstrated remarkable performance in MTS forecasting due to their capability to capture long-term dependencies. However, prior work has been confined to modeling temporal dependencies at either a fixed scale or multiple scales that exponentially increase (most with base 2). This limitation hinders their effectiveness in capturing diverse seasonalities, such as hourly and daily patterns. In this paper, we introduce a dimension invariant embedding technique that captures short-term temporal dependencies and projects MTS data into a higher-dimensional space, while preserving the dimensions of time steps and variables in MTS data. Furthermore, we present a novel Multi-scale Transformer Pyramid Network (MTPNet), specifically designed to effectively capture temporal dependencies at multiple unconstrained scales. The predictions are inferred from multi-scale latent
    
[^56]: RamseyRL:一种智能化的Ramsey数反例搜索框架。

    RamseyRL: A Framework for Intelligent Ramsey Number Counterexample Searching. (arXiv:2308.11943v1 [cs.LG])

    [http://arxiv.org/abs/2308.11943](http://arxiv.org/abs/2308.11943)

    本文提出了一种智能化的Ramsey数反例搜索框架，通过引入图向量化和基于深度神经网络（DNN）的启发式方法，使用最佳优先搜索算法和强化学习（RL）技术，以寻找特定Ramsey数的反例，并提出了算法优化以限制多项式搜索运行时间。

    

    Ramsey数是使得所有节点数为$n$的简单无向图包含一个顺序为$s$的团或者一个顺序为$t$的独立集的最小数$n = R(s, t)$。本文探索了一种最佳优先搜索算法和强化学习（RL）技术的应用，以寻找特定Ramsey数的反例。我们通过引入图向量化和基于深度神经网络（DNN）的启发式方法，逐步改进了之前的搜索方法（如随机搜索），通过衡量图形成为反例的可能性来测算。文章还提出了算法优化以限制多项式搜索运行时间。本文的目标不是提供新的反例，而是介绍和评估支持Ramsey反例探索的框架，使用其他的启发式方法。代码和方法通过PyPI软件包和GitHub存储库提供。

    The Ramsey number is the minimum number of nodes, $n = R(s, t)$, such that all undirected simple graphs of order $n$, contain a clique of order $s$, or an independent set of order $t$. This paper explores the application of a best first search algorithm and reinforcement learning (RL) techniques to find counterexamples to specific Ramsey numbers. We incrementally improve over prior search methods such as random search by introducing a graph vectorization and deep neural network (DNN)-based heuristic, which gauge the likelihood of a graph being a counterexample. The paper also proposes algorithmic optimizations to confine a polynomial search runtime. This paper does not aim to present new counterexamples but rather introduces and evaluates a framework supporting Ramsey counterexample exploration using other heuristics. Code and methods are made available through a PyPI package and GitHub repository.
    
[^57]: 使用多条件扩散模型进行音频生成

    Audio Generation with Multiple Conditional Diffusion Model. (arXiv:2308.11940v1 [cs.SD])

    [http://arxiv.org/abs/2308.11940](http://arxiv.org/abs/2308.11940)

    本论文提出了一种使用多条件扩散模型进行音频生成的方法。通过引入内容和风格等额外条件，增强了现有模型的可控性。这种方法可以精确控制生成音频的时间顺序、音高和能量。由于缺乏合适的数据集和评估指标，作者整合了现有数据集并进行了实验验证。

    

    基于文本的音频生成模型有其局限性，因为它们无法包含音频中的所有信息，仅依靠文本会导致受控性受限。为了解决这个问题，我们提出了一种新颖的模型，通过引入额外的条件（包括内容（时间戳）和风格（音高曲线和能量曲线））作为文本的补充，增强了现有预训练文本到音频模型的可控性。这种方法实现了对生成音频的时间顺序、音高和能量的精细控制。为了保持生成的多样性，我们使用一个可训练的控制条件编码器，该编码器由一个大型语言模型增强，并使用一个可训练的融合网络来编码和融合额外的条件，同时保持预训练文本到音频模型的权重不变。由于缺乏合适的数据集和评估指标，我们将现有数据集整合为一个新的数据集，包括音频和相应的条件，并使用一个可训练的控制条件编码器，该编码器由一个大型语言模型增强，并使用一个可训练的融合网络来编码和融合额外的条件，同时保持预训练文本到音频模型的权重不变。

    Text-based audio generation models have limitations as they cannot encompass all the information in audio, leading to restricted controllability when relying solely on text. To address this issue, we propose a novel model that enhances the controllability of existing pre-trained text-to-audio models by incorporating additional conditions including content (timestamp) and style (pitch contour and energy contour) as supplements to the text. This approach achieves fine-grained control over the temporal order, pitch, and energy of generated audio. To preserve the diversity of generation, we employ a trainable control condition encoder that is enhanced by a large language model and a trainable Fusion-Net to encode and fuse the additional conditions while keeping the weights of the pre-trained text-to-audio model frozen. Due to the lack of suitable datasets and evaluation metrics, we consolidate existing datasets into a new dataset comprising the audio and corresponding conditions and use a 
    
[^58]: 零售需求预测：多元时间序列的比较研究

    Retail Demand Forecasting: A Comparative Study for Multivariate Time Series. (arXiv:2308.11939v1 [cs.LG])

    [http://arxiv.org/abs/2308.11939](http://arxiv.org/abs/2308.11939)

    本研究通过将客户需求的时间序列数据与宏观经济变量相结合，开发并比较了各种回归和机器学习模型，以准确预测零售需求。

    

    在零售行业中，准确的需求预测是财务绩效和供应链效率的重要决定因素。随着全球市场日益互联互通，企业开始采用先进的预测模型来获取竞争优势。然而，现有文献主要关注历史销售数据，忽略了宏观经济条件对消费者支出行为的重要影响。在本研究中，我们通过将客户需求的时间序列数据与消费者物价指数（CPI）、消费者信心指数（ICS）和失业率等宏观经济变量相结合，弥补了这一差距。利用这个综合数据集，我们开发并比较了各种回归和机器学习模型，以准确预测零售需求。

    Accurate demand forecasting in the retail industry is a critical determinant of financial performance and supply chain efficiency. As global markets become increasingly interconnected, businesses are turning towards advanced prediction models to gain a competitive edge. However, existing literature mostly focuses on historical sales data and ignores the vital influence of macroeconomic conditions on consumer spending behavior. In this study, we bridge this gap by enriching time series data of customer demand with macroeconomic variables, such as the Consumer Price Index (CPI), Index of Consumer Sentiment (ICS), and unemployment rates. Leveraging this comprehensive dataset, we develop and compare various regression and machine learning models to predict retail demand accurately.
    
[^59]: 连续时间线性动态系统的系统识别

    System Identification for Continuous-time Linear Dynamical Systems. (arXiv:2308.11933v1 [cs.LG])

    [http://arxiv.org/abs/2308.11933](http://arxiv.org/abs/2308.11933)

    本文解决了在连续时间下观测不规则采样的情况下，Kalman滤波器的系统识别问题。通过引入连续时间Ito随机微分方程来推广Kalman滤波器的学习，并提供一个新颖的两滤波器的后验计算方法，通过贝叶斯派生获得的解析形式的后验计算方法可以高效地估计SDE的参数。

    

    Kalman滤波器的系统识别问题在学习动态系统的基础参数时，通常假设观测值在等间隔的时间点采样。然而，在许多应用中，这个假设是有限制和不切实际的。本文针对连续离散滤波器的系统识别问题，通过求解连续时间Ito随机微分方程（SDE）来推广Kalman滤波器的学习。我们引入了一个新颖的两滤波器，具有贝叶斯派生的解析形式的后验，这样可以得到不需要预先计算的正向传递的解析更新。利用这种解析的高效计算后验的方法，我们提供了一种EM过程，用于估计SDE的参数，自然地纳入了不规则采样的测量。

    The problem of system identification for the Kalman filter, relying on the expectation-maximization (EM) procedure to learn the underlying parameters of a dynamical system, has largely been studied assuming that observations are sampled at equally-spaced time points. However, in many applications this is a restrictive and unrealistic assumption. This paper addresses system identification for the continuous-discrete filter, with the aim of generalizing learning for the Kalman filter by relying on a solution to a continuous-time It\^o stochastic differential equation (SDE) for the latent state and covariance dynamics. We introduce a novel two-filter, analytical form for the posterior with a Bayesian derivation, which yields analytical updates which do not require the forward-pass to be pre-computed. Using this analytical and efficient computation of the posterior, we provide an EM procedure which estimates the parameters of the SDE, naturally incorporating irregularly sampled measurement
    
[^60]: 近30年来动态山地城市地区滑坡易发性制图揭示滑坡成因的变化情况

    Dynamic landslide susceptibility mapping over recent three decades to uncover variations in landslide causes in subtropical urban mountainous areas. (arXiv:2308.11929v1 [cs.LG])

    [http://arxiv.org/abs/2308.11929](http://arxiv.org/abs/2308.11929)

    该研究提出了一种动态滑坡易发性制图方法，通过多个预测模型进行年度评估，并解决了小样本和泛化问题。

    

    滑坡易发性评估对于减轻滑坡风险至关重要。随着空中和卫星数据的日益丰富，基于数据驱动方法预测滑坡易发性的应用不断增加。然而，滑坡诱发环境（LIE）内的快速波动，主要是由于降雨等外部触发因素的显著变化，给当前数据驱动的LSA方法在不同时间段内适应LIE带来困难。本研究提出了动态滑坡易发性制图，简单地利用多种预测模型进行年度LSA。实践中，由于某些年份滑坡样本数量有限，不可避免地会遇到小样本问题。另一个问题是，现有的LSA方法大多采用训练黑盒模型以适应不同数据集，但往往在泛化和提供全面解释方面存在问题。

    Landslide susceptibility assessment (LSA) is of paramount importance in mitigating landslide risks. Recently, there has been a surge in the utilization of data-driven methods for predicting landslide susceptibility due to the growing availability of aerial and satellite data. Nonetheless, the rapid oscillations within the landslide-inducing environment (LIE), primarily due to significant changes in external triggers such as rainfall, pose difficulties for contemporary data-driven LSA methodologies to accommodate LIEs over diverse timespans. This study presents dynamic landslide susceptibility mapping that simply employs multiple predictive models for annual LSA. In practice, this will inevitably encounter small sample problems due to the limited number of landslide samples in certain years. Another concern arises owing to the majority of the existing LSA approaches train black-box models to fit distinct datasets, yet often failing in generalization and providing comprehensive explanati
    
[^61]: 使用物理信息神经网络解决椭圆型最优控制问题

    Solving Elliptic Optimal Control Problems using Physics Informed Neural Networks. (arXiv:2308.11925v1 [math.OC])

    [http://arxiv.org/abs/2308.11925](http://arxiv.org/abs/2308.11925)

    本论文提出了一种使用物理信息神经网络（PINNs）求解椭圆型最优控制问题的数值方法，并进行了误差分析和数值实验验证。

    

    在这项工作中，我们提出并分析了一个用于线性和半线性二阶椭圆问题的最优控制问题（有/无盒约束）的数值求解器。该方法基于从最优控制问题的一阶最优性系统推导出的耦合系统，利用物理信息神经网络（PINNs）来解决耦合系统。我们对数值方案进行误差分析，并针对深度神经网络参数（如深度、宽度和参数范围）以及域内和边界上的采样点数，给出了关于状态、控制和伴随状态的$L^2(\Omega)$误差界。分析中的主要工具包括偏移Rademacher复杂度以及神经网络函数的有界性和Lipschitz连续性。我们提供了几个数值示例来说明该方法，并与三种现有方法进行比较。

    In this work, we present and analyze a numerical solver for optimal control problems (without / with box constraint) for linear and semilinear second-order elliptic problems. The approach is based on a coupled system derived from the first-order optimality system of the optimal control problem, and applies physics informed neural networks (PINNs) to solve the coupled system. We present an error analysis of the numerical scheme, and provide $L^2(\Omega)$ error bounds on the state, control and adjoint state in terms of deep neural network parameters (e.g., depth, width, and parameter bounds) and the number of sampling points in the domain and on the boundary. The main tools in the analysis include offset Rademacher complexity and boundedness and Lipschitz continuity of neural network functions. We present several numerical examples to illustrate the approach and compare it with three existing approaches.
    
[^62]: 不同政策在无奖励马尔可夫决策过程中的收敛性研究

    Diverse Policies Converge in Reward-free Markov Decision Processe. (arXiv:2308.11924v1 [cs.LG])

    [http://arxiv.org/abs/2308.11924](http://arxiv.org/abs/2308.11924)

    本文提出了一个统一的多样化强化学习框架，并研究了训练多样化策略的收敛性，同时提出了一个经过验证的高效性多样化强化学习算法。

    

    强化学习在许多决策任务中取得了巨大的成功，传统的强化学习算法主要是为了获得一个单一的最优解。然而，最近的研究表明发展多样化的策略的重要性，这已成为一个新兴的研究课题。尽管出现了多种多样化强化学习算法，但它们中没有一个在理论上回答了算法如何收敛以及算法的效率如何的问题。在本文中，我们提供了一个统一的多样化强化学习框架，并研究了训练多样化策略的收敛性。在这样的框架下，我们还提出了一个经过验证的高效性多样化强化学习算法。最后，我们通过数值实验验证了我们方法的有效性。

    Reinforcement learning has achieved great success in many decision-making tasks, and traditional reinforcement learning algorithms are mainly designed for obtaining a single optimal solution. However, recent works show the importance of developing diverse policies, which makes it an emerging research topic. Despite the variety of diversity reinforcement learning algorithms that have emerged, none of them theoretically answer the question of how the algorithm converges and how efficient the algorithm is. In this paper, we provide a unified diversity reinforcement learning framework and investigate the convergence of training diverse policies. Under such a framework, we also propose a provably efficient diversity reinforcement learning algorithm. Finally, we verify the effectiveness of our method through numerical experiments.
    
[^63]: 利用相似性-差异解缠来进行音频差异字幕生成

    Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement. (arXiv:2308.11923v1 [eess.AS])

    [http://arxiv.org/abs/2308.11923](http://arxiv.org/abs/2308.11923)

    该论文提出了音频差异字幕生成（ADC）作为一种新的音频字幕生成扩展任务，用于描述类似但略有差异的音频片段之间的语义差异。通过引入交叉注意力集中的Transformer编码器和相似性-差异解缠，该方法有效解决了传统音频字幕生成中的差异描述问题，并利用可视化来改善注意力权重以提取差异。

    

    我们提出了音频差异字幕生成（ADC）作为音频字幕生成的新扩展任务，用于描述类似但略有差异的音频片段之间的语义差异。ADC解决了传统音频字幕生成中，对于相似音频片段生成类似字幕的问题，无法描述内容差异的情况。我们还提出了一种交叉注意力集中的Transformer编码器，通过比较一对音频片段和一种相似性-差异解缠来强调潜在空间中的差异。为了评估所提出的方法，我们构建了一个AudioDiffCaps数据集，其中包含了类似但略有差异的音频片段对以及人工标注的它们之间差异的描述。使用该数据集进行的实验证明了所提出的方法能够有效解决ADC任务，并通过在Transformer编码器中对其进行可视化来改善注意力权重以提取差异。

    We proposed Audio Difference Captioning (ADC) as a new extension task of audio captioning for describing the semantic differences between input pairs of similar but slightly different audio clips. The ADC solves the problem that conventional audio captioning sometimes generates similar captions for similar audio clips, failing to describe the difference in content. We also propose a cross-attention-concentrated transformer encoder to extract differences by comparing a pair of audio clips and a similarity-discrepancy disentanglement to emphasize the difference in the latent space. To evaluate the proposed methods, we built an AudioDiffCaps dataset consisting of pairs of similar but slightly different audio clips with human-annotated descriptions of their differences. The experiment with the AudioDiffCaps dataset showed that the proposed methods solve the ADC task effectively and improve the attention weights to extract the difference by visualizing them in the transformer encoder.
    
[^64]: 解决计算机自适应测试中的选择偏差问题：一种基于用户的聚合影响函数方法

    Addressing Selection Bias in Computerized Adaptive Testing: A User-Wise Aggregate Influence Function Approach. (arXiv:2308.11912v1 [cs.LG])

    [http://arxiv.org/abs/2308.11912](http://arxiv.org/abs/2308.11912)

    本文研究了计算机自适应测试中存在的选择偏差问题，并提出了一种基于用户的聚合影响函数方法来解决该问题。

    

    计算机自适应测试（CAT）是一种广泛使用的高效测试模式，可以根据受试者在测试领域的熟练程度进行适应。CAT需要预先训练的项目简介，因为CAT根据已注册项目的简介实时评估学生，并使用候选项目的简介选择下一个要指导的项目。然而，获取这样的项目简介是一个昂贵的过程，涉及收集大量密集的项目响应数据，然后在收集的数据上训练诊断模型。在本文中，我们探讨了利用CAT服务中收集的响应数据的可能性。我们首先展示了这带来的独特挑战，原因是CAT引入了固有的选择偏差，即熟练程度更高的学生会收到更难的问题。实际上，当使用CAT响应数据进行简单训练诊断模型时，我们观察到项目简介与实际情况显著偏离。为了解决选择偏差问题，我们提出了基于用户的聚合影响函数方法。

    Computerized Adaptive Testing (CAT) is a widely used, efficient test mode that adapts to the examinee's proficiency level in the test domain. CAT requires pre-trained item profiles, for CAT iteratively assesses the student real-time based on the registered items' profiles, and selects the next item to administer using candidate items' profiles. However, obtaining such item profiles is a costly process that involves gathering a large, dense item-response data, then training a diagnostic model on the collected data. In this paper, we explore the possibility of leveraging response data collected in the CAT service. We first show that this poses a unique challenge due to the inherent selection bias introduced by CAT, i.e., more proficient students will receive harder questions. Indeed, when naively training the diagnostic model using CAT response data, we observe that item profiles deviate significantly from the ground-truth. To tackle the selection bias issue, we propose the user-wise agg
    
[^65]: 利用可接受边界进行启发式学习

    Utilizing Admissible Bounds for Heuristic Learning. (arXiv:2308.11905v1 [cs.AI])

    [http://arxiv.org/abs/2308.11905](http://arxiv.org/abs/2308.11905)

    本文通过将可接受启发式作为截断高斯分布的参数，明确了在监督启发式学习中可接受启发式的作用，紧缩了假设空间。

    

    虽然利用现代机器学习技术学习前向搜索算法的启发式函数近年来受到了关注，但对于它们应该学习的内容、如何训练以及为什么这样做的理论认识还很少。这种理解的不足导致文献中进行数据集选择（次优成本对最优成本或可接受对不可接受启发式）和优化指标（例如平方误差和绝对误差）时进行了临时选择。此外，由于所得到的训练启发式函数缺乏可接受性，对于学习过程中可接受性的重要性也缺乏关注。本文通过将可接受启发式作为截断高斯分布的参数，明确了在监督启发式学习中可接受启发式的作用，相比普通高斯分布，紧缩了假设空间。我们认为这个数学模型忠实地遵循了最大熵原则。

    While learning a heuristic function for forward search algorithms with modern machine learning techniques has been gaining interest in recent years, there has been little theoretical understanding of \emph{what} they should learn, \emph{how} to train them, and \emph{why} we do so. This lack of understanding leads to various literature performing an ad-hoc selection of datasets (suboptimal vs optimal costs or admissible vs inadmissible heuristics) and optimization metrics (e.g., squared vs absolute errors). Moreover, due to the lack of admissibility of the resulting trained heuristics, little focus has been put on the role of admissibility \emph{during} learning. This paper articulates the role of admissible heuristics in supervised heuristic learning using them as parameters of Truncated Gaussian distributions, which tightens the hypothesis space compared to ordinary Gaussian distributions. We argue that this mathematical model faithfully follows the principle of maximum entropy and em
    
[^66]: 重新思考半监督医学图像分割中的数据扰动和模型稳定化

    Rethinking Data Perturbation and Model Stabilization for Semi-supervised Medical Image Segmentation. (arXiv:2308.11903v1 [cs.CV])

    [http://arxiv.org/abs/2308.11903](http://arxiv.org/abs/2308.11903)

    该论文重新思考了半监督医学图像分割中的数据扰动和模型稳定化的重要性，并提出了名为DPMS的简单而有效的方法来提高半监督分割性能。

    

    近年来，半监督医学图像分割（SSMIS）的研究取得了快速进展。由于有限的标记数据，SSMIS方法主要关注如何有效利用未标记数据来提高分割性能。然而，尽管当前最先进的方法表现出色，但它们往往优先考虑集成复杂技术和损失项，而不是直接解决半监督场景的核心挑战。我们认为，SSMIS的关键在于在未标记数据上生成充分且合适的预测差异。为此，我们强调了在半监督分割中数据扰动和模型稳定化的重要性，并提出了一种简单而有效的方法来显著提高SSMIS性能，称为DPMS。具体而言，我们首先从数据、模型和损失三个不同角度重新审视了SSMIS，并对相应策略进行了全面研究以检验其有效性。

    Studies on semi-supervised medical image segmentation (SSMIS) have seen fast progress recently. Due to the limited labelled data, SSMIS methods mainly focus on effectively leveraging unlabeled data to enhance the segmentation performance. However, despite their promising performance, current state-of-the-art methods often prioritize integrating complex techniques and loss terms rather than addressing the core challenges of semi-supervised scenarios directly. We argue that the key to SSMIS lies in generating substantial and appropriate prediction disagreement on unlabeled data. To this end, we emphasize the crutiality of data perturbation and model stabilization in semi-supervised segmentation, and propose a simple yet effective approach to boost SSMIS performance significantly, dubbed DPMS. Specifically, we first revisit SSMIS from three distinct perspectives: the data, the model, and the loss, and conduct a comprehensive study of corresponding strategies to examine their effectiveness
    
[^67]: 通过等变扩散模型进行基于形状的3D分子生成

    Shape-conditioned 3D Molecule Generation via Equivariant Diffusion Models. (arXiv:2308.11890v1 [cs.LG])

    [http://arxiv.org/abs/2308.11890](http://arxiv.org/abs/2308.11890)

    本文提出了一个基于形状的分子生成问题，通过等变形状引导的生成模型ShapeMol成功生成了新颖、多样且类似给定形状条件的药物样分子。

    

    配体基药物设计旨在识别与已知活性分子形状相似的新型药物候选物。本文提出了一个基于形状的分子生成问题，即在给定分子的形状条件下生成3D分子结构。为了解决这个问题，我们开发了一个等变形状引导的生成模型ShapeMol。ShapeMol由一个等变形状编码器和一个基于这些编码生成3D分子的等变扩散模型组成。实验结果表明，ShapeMol能够生成新颖、多样且类似给定形状条件的药物样分子。这些结果展示了ShapeMol在设计具有所需3D形状并与蛋白靶点结合的药物候选物方面的潜力。

    Ligand-based drug design aims to identify novel drug candidates of similar shapes with known active molecules. In this paper, we formulated an in silico shape-conditioned molecule generation problem to generate 3D molecule structures conditioned on the shape of a given molecule. To address this problem, we developed a translation- and rotation-equivariant shape-guided generative model ShapeMol. ShapeMol consists of an equivariant shape encoder that maps molecular surface shapes into latent embeddings, and an equivariant diffusion model that generates 3D molecules based on these embeddings. Experimental results show that ShapeMol can generate novel, diverse, drug-like molecules that retain 3D molecular shapes similar to the given shape condition. These results demonstrate the potential of ShapeMol in designing drug candidates of desired 3D shapes binding to protein target pockets.
    
[^68]: 使用反馈循环的对抗训练

    Adversarial Training Using Feedback Loops. (arXiv:2308.11881v1 [cs.LG])

    [http://arxiv.org/abs/2308.11881](http://arxiv.org/abs/2308.11881)

    本文提出了一种基于反馈控制的新型对抗训练方法，通过将反馈控制纳入神经网络架构中进行训练，以增强DNN对对抗性攻击的鲁棒性。

    

    深度神经网络(DNN)由于能够准确地学习非常复杂的输入-输出关系而在许多领域得到了广泛应用。尽管DNN准确性高且使用广泛，但由于泛化能力有限，它们对对抗性攻击非常敏感。为了未来在这个领域取得进展，构建对任何数据点的扰动都具有抵抗能力的DNN是至关重要的。过去提出了许多使用网络的一阶导数信息来增强DNN的技术。本文提出了一种基于控制论的新型增强方法。提出了一种将反馈控制纳入神经网络架构的神经网络架构，称为反馈神经网络。控制器本身是一个神经网络，通过使用常规和对抗性数据进行训练，以稳定系统的输出。基于反馈控制架构的新型对抗训练方法称为反馈循环对抗训练(FLAT)。

    Deep neural networks (DNN) have found wide applicability in numerous fields due to their ability to accurately learn very complex input-output relations. Despite their accuracy and extensive use, DNNs are highly susceptible to adversarial attacks due to limited generalizability. For future progress in the field, it is essential to build DNNs that are robust to any kind of perturbations to the data points. In the past, many techniques have been proposed to robustify DNNs using first-order derivative information of the network.  This paper proposes a new robustification approach based on control theory. A neural network architecture that incorporates feedback control, named Feedback Neural Networks, is proposed. The controller is itself a neural network, which is trained using regular and adversarial data such as to stabilize the system outputs. The novel adversarial training approach based on the feedback control architecture is called Feedback Looped Adversarial Training (FLAT). Numeri
    
[^69]: SUMMIT: 无源自适应单模型到多模态目标的方法

    SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets. (arXiv:2308.11880v1 [cs.CV])

    [http://arxiv.org/abs/2308.11880](http://arxiv.org/abs/2308.11880)

    SUMMIT方法放宽了传统模型自适应方法的两个假设，通过解决无源数据、无配对数据的情况将独立训练的单模态模型适应到多模态目标领域。

    

    在许多应用中，使用多模态数据进行场景理解是必要的，例如自主导航。为了在各种情况下实现这一目标，现有的模型必须能够适应不断变化的数据分布，而无需繁琐的数据标注。当前的方法假设在自适应过程中存在源数据，并且源数据是成对的多模态数据。然而，这些假设对许多应用来说可能存在问题。源数据可能由于隐私、安全或经济方面的考虑而不可用。在训练过程中假设存在成对的多模态数据还会导致显著的数据收集成本，并且无法充分利用广泛可用的免费分发的单模态模型。在本文中，我们通过解决以下问题来放宽这两个假设：如何将独立训练在单模态数据上的一组模型自适应到由无标签的多模态数据组成的目标域，而无需访问原始源数据集。

    Scene understanding using multi-modal data is necessary in many applications, e.g., autonomous navigation. To achieve this in a variety of situations, existing models must be able to adapt to shifting data distributions without arduous data annotation. Current approaches assume that the source data is available during adaptation and that the source consists of paired multi-modal data. Both these assumptions may be problematic for many applications. Source data may not be available due to privacy, security, or economic concerns. Assuming the existence of paired multi-modal data for training also entails significant data collection costs and fails to take advantage of widely available freely distributed pre-trained uni-modal models. In this work, we relax both of these assumptions by addressing the problem of adapting a set of models trained independently on uni-modal data to a target domain consisting of unlabeled multi-modal data, without having access to the original source dataset. O
    
[^70]: Cabrita: 弥合外语差距

    Cabrita: closing the gap for foreign languages. (arXiv:2308.11878v1 [cs.CL])

    [http://arxiv.org/abs/2308.11878](http://arxiv.org/abs/2308.11878)

    Cabrita是一种解决性能和高效标记化问题的方法，以可承受的成本解决了从头训练模型的限制。

    

    从头训练模型在特定语言或领域中有两个重要目的：i)增强在特定语言或领域背景下的性能，ii)确保有效的标记化。然而，这种方法的主要限制在于相关成本，这些成本可能达到六位数甚至七位数的美元金额，这取决于模型大小和涉及的参数数量。为了克服这个成本挑战，主要解决方案是依赖可用的预训练模型，尽管最近出现了像LLaMA和LLaMA-2模型这样的进展，但对于某些特定领域问题仍然表现低效，或者在涉及对话式记忆资源的场景中无效，因为表示文本所需的标记数量巨大。为了解决这个问题，我们提出了一种名为Cabrita的方法，我们的研究证明，它成功解决了性能和高效标记化的问题，而且成本可承受。

    The strategy of training the model from scratch in a specific language or domain serves two essential purposes: i) enhancing performance in the particular linguistic or domain context, and ii) ensuring effective tokenization. The main limitation inherent to this approach lies in the associated cost, which can reach six to seven-digit dollar values, depending on the model size and the number of parameters involved.  The main solution to overcome the cost challenge is to rely on available pre-trained models, which, despite recent advancements such as the LLaMA and LLaMA-2 models, still demonstrate inefficiency for certain specific domain problems or prove ineffective in scenarios involving conversational memory resources, given the large number of tokens required to represent text.  To overcome this issue, we present a methodology named Cabrita, which, as our research demonstrates, successfully addresses the performance and efficient tokenization problem, all at an affordable cost. We be
    
[^71]: 将大型语言模型集成到调试C编译器中以生成上下文错误解释

    Integrating Large Language Models into the Debugging C Compiler for generating contextual error explanations. (arXiv:2308.11873v1 [cs.SE])

    [http://arxiv.org/abs/2308.11873](http://arxiv.org/abs/2308.11873)

    本文介绍了一种利用大型语言模型将调试C编译器的错误解释改进的方法，并通过专家评估证明其在编译时和运行时错误解释的准确性方面的有效性。

    

    本文介绍了一种使用大型语言模型（LLM）在我们的调试C编译器（DCC）中生成增强型编译器错误解释的方法。众所周知，编译器错误消息对于初学者学习如何编程是一个障碍。虽然我们最初在入门编程（CS1）中使用DCC已经通过提供常见错误的保护机制和翻译通常含义隐晦的编译器错误消息，有助于教授C语言给初学者，但我们提出将LLM生成的解释纳入进来会进一步增强初学者的学习体验。通过专家评估，我们观察到LLM生成的编译器错误解释在90%的编译时错误和75%的运行时错误中在概念上是准确的。此外，新的DCC帮助工具已经越来越多被学生使用，每周平均有1047次独特运行。

    This paper introduces a method for Large Language Models (LLM) to produce enhanced compiler error explanations, in simple language, within our Debugging C Compiler (DCC). It is well documented that compiler error messages have been known to present a barrier for novices learning how to program. Although our initial use of DCC in introductory programming (CS1) has been instrumental in teaching C to novice programmers by providing safeguards to commonly occurring errors and translating the usually cryptic compiler error messages at both compile- and run-time, we proposed that incorporating LLM-generated explanations would further enhance the learning experience for novice programmers. Through an expert evaluation, we observed that LLM-generated explanations for compiler errors were conceptually accurate in 90% of compile-time errors, and 75% of run-time errors. Additionally, the new DCC-help tool has been increasingly adopted by students, with an average of 1047 unique runs per week, dem
    
[^72]: KinSPEAK: 通过半监督学习方法提高基尼亚兰达语语音识别的准确性

    KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods. (arXiv:2308.11863v1 [eess.AS])

    [http://arxiv.org/abs/2308.11863](http://arxiv.org/abs/2308.11863)

    通过自监督预训练、课程进度微调和半监督学习利用无标签语音数据，该论文提出了一种改善基尼亚兰达语音识别的方法，实现了最先进的结果。

    

    尽管最近具备了大规模记录的基尼亚兰达语语音数据，但是实现基尼亚兰达语的强大语音识别仍然具有挑战性。本研究表明，使用自监督预训练，遵循简单的课程进度进行微调，以及使用半监督学习来利用大规模无标签语音数据，显著提高了基尼亚兰达语的语音识别性能。我们的方法仅关注使用公共领域数据。我们从公共网站收集了一个新的制作室级别的语音数据集，然后使用该数据集训练一个干净的基准模型。然后，使用该干净的基准模型对来自更多多样和嘈杂的公共数据集的样本进行排序，定义一个简单的课程训练进度。最后，我们将半监督学习应用于连续四代对大规模无标签数据进行标记和学习。我们的最终模型在新数据集上实现了3.2％的字错误率（WER），在Mozilla Common Voice基准测试中实现了15.9％的WER，达到了最先进的水平。

    Despite recent availability of large transcribed Kinyarwanda speech data, achieving robust speech recognition for Kinyarwanda is still challenging. In this work, we show that using self-supervised pre-training, following a simple curriculum schedule during fine-tuning and using semi-supervised learning to leverage large unlabelled speech data significantly improve speech recognition performance for Kinyarwanda. Our approach focuses on using public domain data only. A new studio-quality speech dataset is collected from a public website, then used to train a clean baseline model. The clean baseline model is then used to rank examples from a more diverse and noisy public dataset, defining a simple curriculum training schedule. Finally, we apply semi-supervised learning to label and learn from large unlabelled data in four successive generations. Our final model achieves 3.2% word error rate (WER) on the new dataset and 15.9% WER on Mozilla Common Voice benchmark, which is state-of-the-art
    
[^73]: 寻找完美拟合: 应用回归模型到ClimateBench v1.0

    Finding the Perfect Fit: Applying Regression Models to ClimateBench v1.0. (arXiv:2308.11854v1 [cs.LG])

    [http://arxiv.org/abs/2308.11854](http://arxiv.org/abs/2308.11854)

    本文研究了在ClimateBench数据集上应用非线性回归模型进行气候模拟的能力，重点比较了三种非线性回归模型的性能，结果表明高斯过程回归器在模拟能力方面表现出优势。

    

    数据驱动的机器学习模型作为模拟器的气候预测是研究的重点之一，以使决策者能够做出明智的决策。使用机器学习模拟器作为计算复杂的GCM模拟器的替代，可以减少时间和碳足迹。在这方面，ClimateBench是一个最近为评估气候数据的机器学习模拟器的性能而设计的基准数据集。最近的研究显示，尽管被认为是基础，回归模型在气候模拟中具有一些优势。特别是通过利用核技巧，回归模型能够捕捉复杂的关系并提高预测能力。本研究重点评估在上述数据集上使用非线性回归模型。具体而言，我们比较了三种非线性回归模型的模拟能力。其中，高斯过程回归器证明了它的性能优势。

    Climate projections using data driven machine learning models acting as emulators, is one of the prevailing areas of research to enable policy makers make informed decisions. Use of machine learning emulators as surrogates for computationally heavy GCM simulators reduces time and carbon footprints. In this direction, ClimateBench [1] is a recently curated benchmarking dataset for evaluating the performance of machine learning emulators designed for climate data. Recent studies have reported that despite being considered fundamental, regression models offer several advantages pertaining to climate emulations. In particular, by leveraging the kernel trick, regression models can capture complex relationships and improve their predictive capabilities. This study focuses on evaluating non-linear regression models using the aforementioned dataset. Specifically, we compare the emulation capabilities of three non-linear regression models. Among them, Gaussian Process Regressor demonstrates the
    
[^74]: 一种利用移动数据进行实时需求响应的深度强化学习方法来缓解车站拥挤的铁路重新调度

    A deep reinforcement learning approach for real-time demand-responsive railway rescheduling to mitigate station overcrowding using mobile data. (arXiv:2308.11849v1 [eess.SY])

    [http://arxiv.org/abs/2308.11849](http://arxiv.org/abs/2308.11849)

    本文提出了一种利用移动数据进行实时需求响应的深度强化学习方法来缓解车站拥挤的铁路重新调度。该方法通过推断真实世界的乘客流动性来实现实时调度，并着重考虑高需求车站的重新安排。

    

    实时铁路重新调度是一种及时灵活的技术，可以根据时变条件自动改变运营计划。目前的研究缺乏基于数据驱动的方法，能够捕捉铁路中乘客在紧急情况下的实时流动性，主要依赖基于OD的数据和基于模型的方法来估计列车的需求。与此同时，现有的长期紧急情况下的调度更新原则忽视了需求在时间上的不均匀分布。为填补这一空白，本文提出了一种需求响应的方法，通过从移动数据中推断出真实世界的乘客流动性来促进实时调度。与网络层面的方法不同，本文着重关注紧急区域上游的高需求车站。目标是重新安排通过该目标站的多条线路上受到严重突发事件（如自然灾害）影响的所有列车。需要特别注意避免积累。

    Real-time railway rescheduling is a timely and flexible technique to automatically alter the operation schedule in response to time-varying conditions. Current research lacks data-driven approaches that capture real-time passenger mobility during railway disruptions, relying mostly on OD-based data and model-based methods for estimating demands of trains. Meanwhile, the schedule-updating principles for a long-term disruption overlook the uneven distribution of demand over time. To fill this gap, this paper proposes a demand-responsive approach by inferring real-world passenger mobility from mobile data (MD) to facilitate real-time rescheduling. Unlike network-level approaches, this paper focuses on a heavy-demand station upstream of the disrupted area. The objective is to reschedule all trains on multiple routes passing through this target station, which have been affected by a severe emergency event such as a natural disaster. Particular attention should be given to avoiding the accum
    
[^75]: SEA：可共享和可解释的基于查询的黑盒攻击归因

    SEA: Shareable and Explainable Attribution for Query-based Black-box Attacks. (arXiv:2308.11845v1 [cs.LG])

    [http://arxiv.org/abs/2308.11845](http://arxiv.org/abs/2308.11845)

    本论文提出了SEA，一种用于归因基于查询的黑盒攻击的机器学习安全系统，通过利用隐藏马尔可夫模型框架来理解攻击的演变过程，并有效归因攻击，即使是对于第二次出现的攻击，具有鲁棒性，旨在实现取证和人类可解释的情报共享。

    

    机器学习系统容易受到来自基于查询的黑盒攻击的敌对样本的攻击。尽管有各种努力来检测和防止这些攻击，但仍然需要一种更全面的方法来记录、分析和分享攻击证据。虽然经典安全领域受益于成熟的取证和情报共享技术，但机器学习领域尚未找到一种方式来对攻击者进行画像，并分享关于他们的信息。为此，本论文引入了SEA，一种新颖的机器学习安全系统，用于为取证目的表征对机器学习系统的黑盒攻击，并促进可解释的情报共享。SEA利用隐藏马尔可夫模型框架将观察到的查询序列归因于已知的攻击，因此它能够理解攻击的演变过程而不仅仅关注最终的敌对样本。我们的评估结果显示，SEA能够有效进行攻击归因，即使是对于第二次出现的攻击，也具有鲁棒性。

    Machine Learning (ML) systems are vulnerable to adversarial examples, particularly those from query-based black-box attacks. Despite various efforts to detect and prevent such attacks, there is a need for a more comprehensive approach to logging, analyzing, and sharing evidence of attacks. While classic security benefits from well-established forensics and intelligence sharing, Machine Learning is yet to find a way to profile its attackers and share information about them. In response, this paper introduces SEA, a novel ML security system to characterize black-box attacks on ML systems for forensic purposes and to facilitate human-explainable intelligence sharing. SEA leverages the Hidden Markov Models framework to attribute the observed query sequence to known attacks. It thus understands the attack's progression rather than just focusing on the final adversarial examples. Our evaluations reveal that SEA is effective at attack attribution, even on their second occurrence, and is robus
    
[^76]: 基于${\rm E}(3)$等变性的协作多智体强化学习的演员-评论家方法

    ${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2308.11842v1 [cs.MA])

    [http://arxiv.org/abs/2308.11842](http://arxiv.org/abs/2308.11842)

    本文利用欧几里德对称性，设计了一种基于${\rm E}(3)$等变性的协作多智体强化学习的演员-评论家方法，通过对称约束的神经网络结构，实现了在各种协作MARL基准测试中的卓越性能和令人印象深刻的泛化能力。

    

    在自然界中识别和分析对称模式已经在各个科学领域取得了重要的发现，例如物理学中的引力定律的制定和化学结构研究的进展。本文着重于利用在某些协作多智体强化学习（MARL）问题中固有的欧几里德对称性，以及在许多应用中普遍存在的对称性。我们首先通过形式化地描述一类具有一般对称性概念的马尔可夫博弈，该概念允许存在对称的最优值和策略。受到这些性质的启发，我们设计了具有对称约束的神经网络结构，作为多智体演员-评论家方法的归纳偏差。这种归纳偏差在各种协作MARL基准测试中表现出卓越的性能，并具有零样本学习和在具有重复对称模式的未见场景中的迁移学习等令人印象深刻的泛化能力。

    Identification and analysis of symmetrical patterns in the natural world have led to significant discoveries across various scientific fields, such as the formulation of gravitational laws in physics and advancements in the study of chemical structures. In this paper, we focus on exploiting Euclidean symmetries inherent in certain cooperative multi-agent reinforcement learning (MARL) problems and prevalent in many applications. We begin by formally characterizing a subclass of Markov games with a general notion of symmetries that admits the existence of symmetric optimal values and policies. Motivated by these properties, we design neural network architectures with symmetric constraints embedded as an inductive bias for multi-agent actor-critic methods. This inductive bias results in superior performance in various cooperative MARL benchmarks and impressive generalization capabilities such as zero-shot learning and transfer learning in unseen scenarios with repeated symmetric patterns.
    
[^77]: 《联邦学习评估：目标和指标的调查》

    A Survey for Federated Learning Evaluations: Goals and Measures. (arXiv:2308.11841v1 [cs.LG])

    [http://arxiv.org/abs/2308.11841](http://arxiv.org/abs/2308.11841)

    本研究调查了联邦学习的评估目标和指标，介绍了一个开源平台FedEval，提供了标准化的评估框架，为联邦学习算法的效用、效率和安全性方面进行评估，并讨论了该领域面临的挑战和未来研究方向。

    

    评估是一种系统评估一个系统如何实现其预期目标的方法。联邦学习（FL）是一种保护隐私的机器学习新范式，允许多个参与方在不共享敏感数据的情况下共同训练模型。然而，评估FL具有跨学科性和多样化的目标（如效用、效率和安全性），因此具有挑战性。在本调查中，我们首先回顾了现有研究中采用的主要评估目标，然后探讨了每个目标使用的评估指标。我们还介绍了FedEval，一个开源平台，它提供了一个标准化和全面的联邦学习算法评估框架，涵盖了其效用、效率和安全性。最后，我们讨论了联邦学习评估面临的几个挑战和未来研究方向。

    Evaluation is a systematic approach to assessing how well a system achieves its intended purpose. Federated learning (FL) is a novel paradigm for privacy-preserving machine learning that allows multiple parties to collaboratively train models without sharing sensitive data. However, evaluating FL is challenging due to its interdisciplinary nature and diverse goals, such as utility, efficiency, and security. In this survey, we first review the major evaluation goals adopted in the existing studies and then explore the evaluation metrics used for each goal. We also introduce FedEval, an open-source platform that provides a standardized and comprehensive evaluation framework for FL algorithms in terms of their utility, efficiency, and security. Finally, we discuss several challenges and future research directions for FL evaluation.
    
[^78]: 一个关于校准的基准研究

    A Benchmark Study on Calibration. (arXiv:2308.11838v1 [cs.LG])

    [http://arxiv.org/abs/2308.11838](http://arxiv.org/abs/2308.11838)

    这项研究提出了一个模型校准的基准研究，利用神经架构搜索空间探索了模型校准属性。研究结果显示，模型校准可以在不同任务中泛化，并可以同时兼顾模型的准确性和校准性能。

    

    深度神经网络在各种机器学习任务中的应用越来越广泛。然而，随着这些模型复杂性的增加，它们往往面临校准问题，尽管预测准确性有所提高。许多研究通过数据预处理、使用特定损失函数和训练框架来改善校准性能。然而，对校准属性的研究有点被忽视了。我们的研究利用神经架构搜索（NAS）搜索空间，在全面探索校准属性的模型架构空间中提供了一个详尽的模型架构空间。我们特别创建了一个模型校准数据集。该数据集在广泛使用的NATS-Bench搜索空间中评估了90个基于区间的校准度量和12个其他校准度量，涵盖了117,702个独特的神经网络。我们的分析旨在通过我们提出的数据集回答该领域一些长期存在的问题：（i）模型校准能否在不同任务中泛化？（ii）能否同时兼顾模型的准确性和校准性能？

    Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through data preprocessing, the use of specific loss functions, and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. Our study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We specifically create a model calibration dataset. This dataset evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. Our analysis aims to answer several longstanding questions in the field, using our proposed dataset: (i) Can model calibration be generalized across different tasks? (ii) Can rob
    
[^79]: 描述人类脑结构连接的正常围产期发展

    Characterizing normal perinatal development of the human brain structural connectivity. (arXiv:2308.11836v1 [q-bio.NC])

    [http://arxiv.org/abs/2308.11836](http://arxiv.org/abs/2308.11836)

    本研究开发了一个计算框架，通过时空平均方法来研究围产期人类大脑结构连接的正常发展。

    

    早期大脑发育特点是高度有序的结构连接形成。这种连接的互相关性是大脑认知能力的基础，影响其对疾病和环境因素的反应。因此，在围产期阶段量化评估结构连接对研究正常和异常神经发育很有用。然而，从扩散磁共振成像数据中估计连接组需要复杂的计算。对于围产期，这些计算面临快速脑发展和成像困难的挑战。加上高个体间变异性，这些因素使得描述结构连接的正常发展变得困难。因此，目前缺乏在这一关键脑发育阶段的结构连接度量的可靠基线。在本研究中，我们开发了一个基于时空平均的计算框架来确定结构连接。

    Early brain development is characterized by the formation of a highly organized structural connectome. The interconnected nature of this connectome underlies the brain's cognitive abilities and influences its response to diseases and environmental factors. Hence, quantitative assessment of structural connectivity in the perinatal stage is useful for studying normal and abnormal neurodevelopment. However, estimation of the connectome from diffusion MRI data involves complex computations. For the perinatal period, these computations are further challenged by the rapid brain development and imaging difficulties. Combined with high inter-subject variability, these factors make it difficult to chart the normal development of the structural connectome. As a result, there is a lack of reliable normative baselines of structural connectivity metrics at this critical stage in brain development. In this study, we developed a computational framework, based on spatio-temporal averaging, for determi
    
[^80]: 基于贝叶斯算法变种的网络入侵检测性能比较与实现

    Performance Comparison and Implementation of Bayesian Variants for Network Intrusion Detection. (arXiv:2308.11834v1 [cs.LG])

    [http://arxiv.org/abs/2308.11834](http://arxiv.org/abs/2308.11834)

    本研究实现并比较了贝叶斯分类器的各个变体在网络入侵检测中的性能，并发现假设是影响准确性的最重要因素。

    

    在真实世界应用中，贝叶斯分类器在每个特征完全独立于其他特征时表现良好，但这种情况并不总是成立。本研究旨在实现并比较贝叶斯分类器的各个变体（多项式、伯努利和高斯）在网络入侵异常检测中的性能，并研究每个变体的假设与性能之间是否存在关联。我们的研究发现，贝叶斯算法的每个变体都盲目地遵循其假设，而不考虑特征的属性，假设是影响准确性的最重要因素。实验结果显示，伯努利在测试集上的准确率为69.9%（训练集为71%），多项式在测试集上的准确率为31.2%（训练集为31.2%），而高斯在测试集上的准确率为81.69%（训练集为82.84%）。进一步研究发现，每个朴素贝叶斯变体的性能和准确性主要取决于每个分类器的假设。

    Bayesian classifiers perform well when each of the features is completely independent of the other which is not always valid in real world application. The aim of this study is to implement and compare the performances of each variant of Bayesian classifier (Multinomial, Bernoulli, and Gaussian) on anomaly detection in network intrusion, and to investigate whether there is any association between each variant assumption and their performance. Our investigation showed that each variant of Bayesian algorithm blindly follows its assumption regardless of feature property, and that the assumption is the single most important factor that influences their accuracy. Experimental results show that Bernoulli has accuracy of 69.9% test (71% train), Multinomial has accuracy of 31.2% test (31.2% train), while Gaussian has accuracy of 81.69% test (82.84% train). Going deeper, we investigated and found that each Naive Bayes variants performances and accuracy is largely due to each classifier assumpti
    
[^81]: 探索GPT模型在考试中的效果：驾驶执照知识测试案例研究

    Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver's License Knowledge Test. (arXiv:2308.11827v1 [cs.CL])

    [http://arxiv.org/abs/2308.11827](http://arxiv.org/abs/2308.11827)

    本研究提出了一种方法，通过使用新的信息源的上下文，让GPT模型能够回答考试题目。在使用加利福尼亚驾驶手册作为信息源的测试中，GPT-3模型取得了96%的及格分数。

    

    大型语言模型，如Open AI的生成式预训练变压器（GPT）模型，擅长回答问题，但其知识仅限于其训练数据中的信息。这种限制使得当面临有关最新发展或非公开文件的问题时，它们变得无效。我们的研究提出了一种方法，通过使用之前未包含在其训练数据中的信息源的上下文来使GPT模型能够回答问题。该方法包括上下文信息的预处理、上下文和查询的嵌入、通过整合上下文嵌入构建提示以及使用GPT模型生成答案。我们将此方法应用于一个受控测试场景，使用加利福尼亚驾驶手册作为信息源。GPT-3模型在一套50道样本驾驶知识测试题上取得了96%的及格分数。相比之下，在没有上下文的情况下，模型的及格分数下降到了...

    Large language models such as Open AI's Generative Pre-trained Transformer (GPT) models are proficient at answering questions, but their knowledge is confined to the information present in their training data. This limitation renders them ineffective when confronted with questions about recent developments or non-public documents. Our research proposes a method that enables GPT models to answer questions by employing context from an information source not previously included in their training data. The methodology includes preprocessing of contextual information, the embedding of contexts and queries, constructing prompt through the integration of context embeddings, and generating answers using GPT models. We applied this method in a controlled test scenario using the California Driver's Handbook as the information source. The GPT-3 model achieved a 96% passing score on a set of 50 sample driving knowledge test questions. In contrast, without context, the model's passing score fell to
    
[^82]: Accel-GCN: 高性能图卷积网络的GPU加速器设计

    Accel-GCN: High-Performance GPU Accelerator Design for Graph Convolution Networks. (arXiv:2308.11825v1 [cs.AR])

    [http://arxiv.org/abs/2308.11825](http://arxiv.org/abs/2308.11825)

    Accel-GCN使用轻量级度排序、块级分区和综合warp策略的GPU加速器架构，提高了图卷积网络（GCNs）在主流GPU上的性能。

    

    图卷积网络（GCNs）在从各个领域的图数据中提取潜在信息方面起到了关键作用，但它们在主流GPU上的加速受到了工作负载不平衡和内存访问不规则性的挑战。为了解决这些挑战，我们提出了Accel-GCN，一种针对GCNs的GPU加速器架构。Accel-GCN的设计包括：（i）一个轻量级的度排序阶段，以组合具有相似度的节点；（ii）一个块级分区策略，动态调整warp工作负载大小，增强共享内存本地性和工作负载平衡，并减少与GNNAdvisor等设计相比的元数据开销；（iii）一种综合warp策略，提高在稠密矩阵的列维度上的内存合并和计算并行性。利用这些原则，我们针对GCNs在稀疏矩阵乘法（SpMM）中的核心进行了设计，采用块级分区和综合warp策略。这种方法提高了性能和多层记忆。

    Graph Convolutional Networks (GCNs) are pivotal in extracting latent information from graph data across various domains, yet their acceleration on mainstream GPUs is challenged by workload imbalance and memory access irregularity. To address these challenges, we present Accel-GCN, a GPU accelerator architecture for GCNs. The design of Accel-GCN encompasses: (i) a lightweight degree sorting stage to group nodes with similar degree; (ii) a block-level partition strategy that dynamically adjusts warp workload sizes, enhancing shared memory locality and workload balance, and reducing metadata overhead compared to designs like GNNAdvisor; (iii) a combined warp strategy that improves memory coalescing and computational parallelism in the column dimension of dense matrices.  Utilizing these principles, we formulated a kernel for sparse matrix multiplication (SpMM) in GCNs that employs block-level partitioning and combined warp strategy. This approach augments performance and multi-level memor
    
[^83]: PatchBackdoor: 无需修改模型的深度神经网络后门攻击

    PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model Modification. (arXiv:2308.11822v1 [cs.LG])

    [http://arxiv.org/abs/2308.11822](http://arxiv.org/abs/2308.11822)

    本论文提出了一种无需修改模型的深度神经网络后门攻击方法，通过在摄像头前放置一个精心设计的补丁，即可触发模型的错误行为，而无需修改模型。

    

    后门攻击是对安全关键场景下深度学习系统的主要威胁，旨在在受攻击者控制的条件下触发神经网络模型的错误行为。然而，大多数后门攻击需要通过训练带有有毒数据和/或直接修改模型来修改神经网络模型，这导致了一种普遍但错误的信念，即通过正确保护模型可以轻松避免后门攻击。在本文中，我们展示了无需任何模型修改即可实现后门攻击。我们提出将一个精心设计的补丁（称为后门补丁）放置在摄像头前面，与输入图像一起输入模型。该补丁可以在大多数时候表现正常，但当输入图像包含受攻击者控制的触发物体时会产生错误预测。我们的主要技术包括一种有效的训练方法来生成补丁。

    Backdoor attack is a major threat to deep learning systems in safety-critical scenarios, which aims to trigger misbehavior of neural network models under attacker-controlled conditions. However, most backdoor attacks have to modify the neural network models through training with poisoned data and/or direct model editing, which leads to a common but false belief that backdoor attack can be easily avoided by properly protecting the model. In this paper, we show that backdoor attacks can be achieved without any model modification. Instead of injecting backdoor logic into the training data or the model, we propose to place a carefully-designed patch (namely backdoor patch) in front of the camera, which is fed into the model together with the input images. The patch can be trained to behave normally at most of the time, while producing wrong prediction when the input image contains an attacker-controlled trigger object. Our main techniques include an effective training method to generate th
    
[^84]: 通过去混淆器解决有偏见的电子健康记录中的健康差异问题

    Mitigating Health Disparity on Biased Electronic Health Records via Deconfounder. (arXiv:2308.11819v1 [cs.LG])

    [http://arxiv.org/abs/2308.11819](http://arxiv.org/abs/2308.11819)

    通过使用去混淆器模型FLMD，在电子健康记录建模中实现了公平性和准确性，解决了有偏见的EHR中的健康差异问题。

    

    临床数据建模的公平性问题，尤其是在电子健康记录（EHRs）上，由于EHR的复杂潜在结构和潜在选择偏差问题而至关重要。在实践中，通常需要在保持模型整体准确性的同时减少健康差异。然而，传统方法往往在准确性和公平性之间存在权衡，因为它们无法捕捉到观察数据之外的潜在因素。为了解决这个挑战，我们提出了一个名为公平纵向医疗去混淆器（FLMD）的新模型，旨在实现纵向电子健康记录（EHR）的公平性和准确性建模。受到去混淆器理论的启发，FLMD采用了一个两阶段的训练过程。在第一阶段，FLMD捕捉了每次接触的未观察到的混淆因子，这有效地表示了超出观察到的EHR之外的潜在医疗因素，如患者基因型和生活习惯。这个未观察到的混淆因子是至关重要的。

    The fairness issue of clinical data modeling, especially on Electronic Health Records (EHRs), is of utmost importance due to EHR's complex latent structure and potential selection bias. It is frequently necessary to mitigate health disparity while keeping the model's overall accuracy in practice. However, traditional methods often encounter the trade-off between accuracy and fairness, as they fail to capture the underlying factors beyond observed data. To tackle this challenge, we propose a novel model called Fair Longitudinal Medical Deconfounder (FLMD) that aims to achieve both fairness and accuracy in longitudinal Electronic Health Records (EHR) modeling. Drawing inspiration from the deconfounder theory, FLMD employs a two-stage training process. In the first stage, FLMD captures unobserved confounders for each encounter, which effectively represents underlying medical factors beyond observed EHR, such as patient genotypes and lifestyle habits. This unobserved confounder is crucial 
    
[^85]: 将非局部交通流模型引入物理信息感知神经网络

    Incorporating Nonlocal Traffic Flow Model in Physics-informed Neural Networks. (arXiv:2308.11818v1 [cs.LG])

    [http://arxiv.org/abs/2308.11818](http://arxiv.org/abs/2308.11818)

    本研究在物理信息感知深度学习框架中引入非局部交通流模型，以提高交通状态估计的准确性和可靠性。

    

    本研究利用非局部LWR模型在物理信息感知深度学习框架中的优势，为交通状态估计方法的进展做出了贡献。经典的LWR模型在准确表示实际交通流方面存在局限。非局部LWR模型通过将速度视为下游交通密度的加权均值来克服这一限制。本文提出了一种新颖的PIDL框架，将非局部LWR模型纳入其中。我们引入了固定长度和可变长度的卷积核，并开发了所需的数学方法。通过使用NGSIM和CitySim数据集的数据，对所提出的PIDL框架进行了全面评估，包括各种卷积核和前瞻窗口。结果显示，相对于使用局部LWR模型的基线PIDL方法，所提出的方法取得了改进。研究结果突出了所提出方法提高交通状态估计准确性和可靠性的潜力。

    This research contributes to the advancement of traffic state estimation methods by leveraging the benefits of the nonlocal LWR model within a physics-informed deep learning framework. The classical LWR model, while useful, falls short of accurately representing real-world traffic flows. The nonlocal LWR model addresses this limitation by considering the speed as a weighted mean of the downstream traffic density. In this paper, we propose a novel PIDL framework that incorporates the nonlocal LWR model. We introduce both fixed-length and variable-length kernels and develop the required mathematics. The proposed PIDL framework undergoes a comprehensive evaluation, including various convolutional kernels and look-ahead windows, using data from the NGSIM and CitySim datasets. The results demonstrate improvements over the baseline PIDL approach using the local LWR model. The findings highlight the potential of the proposed approach to enhance the accuracy and reliability of traffic state es
    
[^86]: 对深度神经操作模型在海洋预测中的评估

    Evaluation of Deep Neural Operator Models toward Ocean Forecasting. (arXiv:2308.11814v1 [cs.LG])

    [http://arxiv.org/abs/2308.11814](http://arxiv.org/abs/2308.11814)

    深度神经操作模型在海洋预测中表现良好，可以对经典流体流动和现实海洋动力学进行有效预测。

    

    最近，数据驱动的深度学习建模框架已经用于预测时间序列数据。这样的机器学习模型在多个领域中都可能有用，包括大气和海洋以及流体学领域。本研究调查了这种深度神经操作模型在重现和预测经典流体流动和现实海洋动力学模拟中的可能有效性。我们首先简要评估了这种深度神经操作模型在训练了一个模拟的二维流体流经圆柱体的情况下的能力。然后，我们研究了它们在预测大西洋中部和马萨诸塞湾海洋表层环流的应用，利用了用于实际海洋实验的高分辨率数据同化模拟进行学习。我们确认训练的深度神经操作模型能够预测理想化的周期性涡脱落。对于现实海洋表层流动和我们的初步研究，

    Data-driven, deep-learning modeling frameworks have been recently developed for forecasting time series data. Such machine learning models may be useful in multiple domains including the atmospheric and oceanic ones, and in general, the larger fluids community. The present work investigates the possible effectiveness of such deep neural operator models for reproducing and predicting classic fluid flows and simulations of realistic ocean dynamics. We first briefly evaluate the capabilities of such deep neural operator models when trained on a simulated two-dimensional fluid flow past a cylinder. We then investigate their application to forecasting ocean surface circulation in the Middle Atlantic Bight and Massachusetts Bay, learning from high-resolution data-assimilative simulations employed for real sea experiments. We confirm that trained deep neural operator models are capable of predicting idealized periodic eddy shedding. For realistic ocean surface flows and our preliminary study,
    
[^87]: 这不是一个苹果：多模态嵌入中的对抗幻觉

    Ceci n'est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings. (arXiv:2308.11804v1 [cs.CR])

    [http://arxiv.org/abs/2308.11804](http://arxiv.org/abs/2308.11804)

    该论文研究了多模态嵌入中的对抗幻觉问题。对手可以扰动输入的任意模态，使其嵌入与其他模态的任意输入接近，从而实现任意图像与任意文本、任意文本与任意声音的对齐。该问题与下游任务无关，对生成和分类任务会产生误导。

    

    多模态编码器将图像、声音、文本、视频等映射到一个单一的嵌入空间中，通过对齐不同模态的表示（例如将一张狗的图像与一种叫声相关联）。我们展示了多模态嵌入可以受到一种我们称之为“对抗幻觉”的攻击。给定任意模态的输入，对手可以扰动它，使其嵌入接近于另一模态中任意对手选择的输入的嵌入。幻觉使对手能够将任意图像与任意文本、任意文本与任意声音等进行对齐。对抗幻觉利用了嵌入空间中的接近性，因此与下游任务无关。使用ImageBind嵌入，我们演示了在没有具体下游任务知识的情况下，通过对抗性对齐的输入如何误导图像生成、文本生成和零样例分类。

    Multi-modal encoders map images, sounds, texts, videos, etc. into a single embedding space, aligning representations across modalities (e.g., associate an image of a dog with a barking sound). We show that multi-modal embeddings can be vulnerable to an attack we call "adversarial illusions." Given an input in any modality, an adversary can perturb it so as to make its embedding close to that of an arbitrary, adversary-chosen input in another modality. Illusions thus enable the adversary to align any image with any text, any text with any sound, etc.  Adversarial illusions exploit proximity in the embedding space and are thus agnostic to downstream tasks. Using ImageBind embeddings, we demonstrate how adversarially aligned inputs, generated without knowledge of specific downstream tasks, mislead image generation, text generation, and zero-shot classification.
    
[^88]: 变分密度传播连续学习

    Variational Density Propagation Continual Learning. (arXiv:2308.11801v1 [cs.LG])

    [http://arxiv.org/abs/2308.11801](http://arxiv.org/abs/2308.11801)

    本文提出了一个用于适应数据分布漂移的连续学习框架，并通过利用贝叶斯推断中的不确定性量化来减轻灾难性遗忘。通过传播分布的前两个矩来优化闭式ELBO目标，从而近似预测分布。这种方法消除了Monte Carlo采样的需要，有效惩罚模型似然性的变化。

    

    在现实世界中部署的深度神经网络（DNN）经常面临来自分布外（OoD）数据、各种类型的噪声和概念目标的变化。本文提出了一个适应基准连续学习数据集建模的数据分布漂移的框架。我们开发和评估了一种利用贝叶斯推断中的不确定性量化来减轻灾难性遗忘的连续学习方法。我们通过消除模型权重的Monte Carlo采样来采样预测分布，扩展了之前的方法。我们通过在所有网络层中传播分布的前两个矩（即均值和协方差）来优化闭式证据下界（ELBO）目标，从而近似预测分布。通过使用闭式ELBO来近似最小描述长度（MDL）原则来缓解灾难性遗忘，从而惩罚模型似然性的变化。

    Deep Neural Networks (DNNs) deployed to the real world are regularly subject to out-of-distribution (OoD) data, various types of noise, and shifting conceptual objectives. This paper proposes a framework for adapting to data distribution drift modeled by benchmark Continual Learning datasets. We develop and evaluate a method of Continual Learning that leverages uncertainty quantification from Bayesian Inference to mitigate catastrophic forgetting. We expand on previous approaches by removing the need for Monte Carlo sampling of the model weights to sample the predictive distribution. We optimize a closed-form Evidence Lower Bound (ELBO) objective approximating the predictive distribution by propagating the first two moments of a distribution, i.e. mean and covariance, through all network layers. Catastrophic forgetting is mitigated by using the closed-form ELBO to approximate the Minimum Description Length (MDL) Principle, inherently penalizing changes in the model likelihood by minimi
    
[^89]: 基于复数值神经网络的声音反欺诈系统

    Complex-valued neural networks for voice anti-spoofing. (arXiv:2308.11800v1 [cs.SD])

    [http://arxiv.org/abs/2308.11800](http://arxiv.org/abs/2308.11800)

    本文提出了一种使用复数值神经网络处理音频的新方法，该方法结合了幅度谱图和原始特征的优点，保留了相位信息，并且允许使用可解释性AI方法。实验结果表明，在反欺诈数据集上，该方法优于先前的方法，并且可以通过解释性AI解释结果。

    

    当前的反欺诈和音频深度伪造检测系统使用基于幅度谱图的特征（如CQT或Melspectrograms）或经过卷积或sinc层处理的原始音频。这两种方法都存在缺点：幅度谱图丢失相位信息，影响音频的自然性，而基于原始特征的模型无法使用传统的可解释性AI方法。本文提出了一种新方法，通过使用复数值神经网络处理输入音频的复数值CQT频域表示，结合了两种方法的优点。这种方法保留了相位信息，并允许使用可解释性AI方法。结果表明，这种方法在“In-the-Wild”反欺诈数据集上优于先前的方法，并且通过可解释性AI可以解释结果。消融研究证实了该模型已经学会了使用相位信息来检测声音伪造。

    Current anti-spoofing and audio deepfake detection systems use either magnitude spectrogram-based features (such as CQT or Melspectrograms) or raw audio processed through convolution or sinc-layers. Both methods have drawbacks: magnitude spectrograms discard phase information, which affects audio naturalness, and raw-feature-based models cannot use traditional explainable AI methods. This paper proposes a new approach that combines the benefits of both methods by using complex-valued neural networks to process the complex-valued, CQT frequency-domain representation of the input audio. This method retains phase information and allows for explainable AI methods. Results show that this approach outperforms previous methods on the "In-the-Wild" anti-spoofing dataset and enables interpretation of the results through explainable AI. Ablation studies confirm that the model has learned to use phase information to detect voice spoofing.
    
[^90]: Karasu:一种用于大数据分析的高效集群配置的协作方法

    Karasu: A Collaborative Approach to Efficient Cluster Configuration for Big Data Analytics. (arXiv:2308.11792v1 [cs.DC])

    [http://arxiv.org/abs/2308.11792](http://arxiv.org/abs/2308.11792)

    Karasu是一种通过促进类似基础设施、框架、算法或数据集上工作的用户之间的数据共享，实现更高效的资源配置配置文件分析的方法。

    

    由于机器类型和集群规模等配置选项的广泛多样性，选择适合大数据分析作业的正确资源是困难的。由于糟糕的选择可能对资源效率、成本和能源使用产生重大影响，自动化方法越来越受欢迎。大多数现有方法依赖于对重复工作负载进行配置文件分析，以寻找接近最优解的解决方案。由于冷启动问题，这通常导致费时且昂贵的配置文件分析阶段。然而，不同用户的大数据分析作业可以共享许多共同特性：它们通常在类似的基础设施上操作，使用类似的算法在类似的框架中实现。共享聚合配置文件分析运行以协作解决冷启动问题的潜力还没有得到充分探索。我们提出了Karasu，一种促进在类似基础设施、框架、算法或数据集上工作的用户之间共享数据的更高效的资源配置配置文件分析方法。

    Selecting the right resources for big data analytics jobs is hard because of the wide variety of configuration options like machine type and cluster size. As poor choices can have a significant impact on resource efficiency, cost, and energy usage, automated approaches are gaining popularity. Most existing methods rely on profiling recurring workloads to find near-optimal solutions over time. Due to the cold-start problem, this often leads to lengthy and costly profiling phases. However, big data analytics jobs across users can share many common properties: they often operate on similar infrastructure, using similar algorithms implemented in similar frameworks. The potential in sharing aggregated profiling runs to collaboratively address the cold start problem is largely unexplored.  We present Karasu, an approach to more efficient resource configuration profiling that promotes data sharing among users working with similar infrastructures, frameworks, algorithms, or datasets. Karasu tr
    
[^91]: HypBO: 专家引导下的化学家参与的贝叶斯搜索新材料论文

    HypBO: Expert-Guided Chemist-in-the-Loop Bayesian Search for New Materials. (arXiv:2308.11787v1 [cs.LG])

    [http://arxiv.org/abs/2308.11787](http://arxiv.org/abs/2308.11787)

    HypBO是一种利用专家人类知识引导贝叶斯搜索的方法，通过生成改进的样本种子来更快地找到有希望的化学空间区域。

    

    机器人和自动化可以大大加速解决材料发现等难以解决的多变量科学问题，但可用的搜索空间可能非常庞大。贝叶斯优化已经成为一种受欢迎的样本高效优化引擎，在没有目标函数或属性的解析形式被知道的任务中获得了成功。在这里，我们利用专家人类知识以假设的形式，更快地将贝叶斯搜索引导到有希望的化学空间区域。先前的方法使用从现有实验测量得到的潜在分布，这对于新的未开发的科学任务是不可行的。此外，这样的分布无法捕捉精细的假设。我们提出的方法，称为HypBO，利用专家人类假设生成改进的样本种子。不太有希望的种子自动折扣，而有希望的种子用于增加代理模型数据，从而实现更具信息的采样。

    Robotics and automation offer massive accelerations for solving intractable, multivariate scientific problems such as materials discovery, but the available search spaces can be dauntingly large. Bayesian optimization (BO) has emerged as a popular sample-efficient optimization engine, thriving in tasks where no analytic form of the target function/property is known. Here we exploit expert human knowledge in the form of hypotheses to direct Bayesian searches more quickly to promising regions of chemical space. Previous methods have used underlying distributions derived from existing experimental measurements, which is unfeasible for new, unexplored scientific tasks. Also, such distributions cannot capture intricate hypotheses. Our proposed method, which we call HypBO, uses expert human hypotheses to generate an improved seed of samples. Unpromising seeds are automatically discounted, while promising seeds are used to augment the surrogate model data, thus achieving better-informed sampl
    
[^92]: 用Transformers进行粗到精多场景姿态回归

    Coarse-to-Fine Multi-Scene Pose Regression with Transformers. (arXiv:2308.11783v1 [cs.CV])

    [http://arxiv.org/abs/2308.11783](http://arxiv.org/abs/2308.11783)

    本文提出了一种使用Transformers进行多场景精确相机姿态回归的方法，通过自注意力聚合激活图和解码潜在特征来实现对通用特征的定位，并在多个场景中并行嵌入。

    

    绝对相机姿态回归器根据仅靠图像进行估计，给出相机的位置和方向。通常，使用卷积骨干网络和多层感知机（MLP）头部，通过图像和姿态标签进行训练，一次仅嵌入一个参考场景。最近，通过用一组全连接层替换MLP头部，将此方案扩展到学习多个场景。在本文中，我们提出了用Transformers学习多场景绝对相机姿态回归的方法，其中编码器用于通过自注意力聚合激活图，并且解码器将潜在特征和场景编码转化为姿态预测。这使得我们的模型能够专注于对定位有信息量的通用特征，同时并行嵌入多个场景。我们通过引入混合分类-回归架构改进了我们先前的MS-Transformer方法\cite{shavit2021learning}，提高了定位准确性。我们的方法在常见的数据集上进行了评估。

    Absolute camera pose regressors estimate the position and orientation of a camera given the captured image alone. Typically, a convolutional backbone with a multi-layer perceptron (MLP) head is trained using images and pose labels to embed a single reference scene at a time. Recently, this scheme was extended to learn multiple scenes by replacing the MLP head with a set of fully connected layers. In this work, we propose to learn multi-scene absolute camera pose regression with Transformers, where encoders are used to aggregate activation maps with self-attention and decoders transform latent features and scenes encoding into pose predictions. This allows our model to focus on general features that are informative for localization, while embedding multiple scenes in parallel. We extend our previous MS-Transformer approach \cite{shavit2021learning} by introducing a mixed classification-regression architecture that improves the localization accuracy. Our method is evaluated on commonly b
    
[^93]: 处理动态和稀疏的定性数据：范畴变量的希尔伯特空间嵌入

    Addressing Dynamic and Sparse Qualitative Data: A Hilbert Space Embedding of Categorical Variables. (arXiv:2308.11781v1 [cs.LG])

    [http://arxiv.org/abs/2308.11781](http://arxiv.org/abs/2308.11781)

    我们提出了一个新的框架，将定性数据融入到因果估计的定量模型中，通过将观察到的范畴嵌入到潜在的Baire空间，并引入一个连续的线性映射，转化定性变量的处理为RKHS中的识别结构，从而简化了估计的过程

    

    我们提出了一个新的框架，将定性数据融入到因果估计的定量模型中。以往的方法使用从定性数据导出的范畴变量来构建定量模型。然而，如果定性信息是动态和复杂的，这种方法可能导致数据稀疏的范畴，并产生不一致（渐近偏置）和不精确（有限样本偏置）的估计。我们使用函数分析创建一个更细致和灵活的框架。我们将观察到的范畴嵌入到潜在的Baire空间中，并引入一个连续的线性映射——希尔伯特空间嵌入——从范畴的Baire空间到表示函数的再生核希尔伯特空间（RKHS）。通过Riesz表示定理，我们证明因果模型中的范畴变量的经典处理可以转化为RKHS中的一个已识别结构。转移学习作为一个催化剂，简化了估计——嵌入

    We propose a novel framework for incorporating qualitative data into quantitative models for causal estimation. Previous methods use categorical variables derived from qualitative data to build quantitative models. However, this approach can lead to data-sparse categories and yield inconsistent (asymptotically biased) and imprecise (finite sample biased) estimates if the qualitative information is dynamic and intricate. We use functional analysis to create a more nuanced and flexible framework. We embed the observed categories into a latent Baire space and introduce a continuous linear map -- a Hilbert space embedding -- from the Baire space of categories to a Reproducing Kernel Hilbert Space (RKHS) of representation functions. Through the Riesz representation theorem, we establish that the canonical treatment of categorical variables in causal models can be transformed into an identified structure in the RKHS. Transfer learning acts as a catalyst to streamline estimation -- embeddings
    
[^94]: 在文本中使用离群学习进行少样本异常检测

    Few-shot Anomaly Detection in Text with Deviation Learning. (arXiv:2308.11780v1 [cs.LG])

    [http://arxiv.org/abs/2308.11780](http://arxiv.org/abs/2308.11780)

    本论文介绍了一种基于深度少样本学习的框架FATE，它通过离群学习明确地学习文本中的异常得分，并利用先前已知的少量异常示例，从而克服了传统方法中对无标签数据的依赖，并优化了异常得分的精确度和数据利用效率。

    

    目前大多数文本异常检测方法都集中在构建仅依赖无标签数据的模型上。这些模型基于没有可用的标记异常示例的假设运行，在许多实际应用中，这些异常通常以小数量存在，这阻碍了它们利用先前已知的异常知识。此外，这些模型更注重学习特征嵌入而不是直接优化异常得分，这可能会导致次优的异常得分和学习过程中数据的低效利用。在本文中，我们介绍了FATE，一种基于深度少样本学习的框架，它利用有限的异常示例，并使用离群学习的端到端方法明确地学习异常得分。在这种方法中，将正常示例的异常得分调整为与先前分布获得的参考得分相似。相反，异常样本被迫具有明显偏离的异常得分。

    Most current methods for detecting anomalies in text concentrate on constructing models solely relying on unlabeled data. These models operate on the presumption that no labeled anomalous examples are available, which prevents them from utilizing prior knowledge of anomalies that are typically present in small numbers in many real-world applications. Furthermore, these models prioritize learning feature embeddings rather than optimizing anomaly scores directly, which could lead to suboptimal anomaly scoring and inefficient use of data during the learning process. In this paper, we introduce FATE, a deep few-shot learning-based framework that leverages limited anomaly examples and learns anomaly scores explicitly in an end-to-end method using deviation learning. In this approach, the anomaly scores of normal examples are adjusted to closely resemble reference scores obtained from a prior distribution. Conversely, anomaly samples are forced to have anomalous scores that considerably devi
    
[^95]: 理解Hessian对领域泛化的对齐

    Understanding Hessian Alignment for Domain Generalization. (arXiv:2308.11778v1 [cs.LG])

    [http://arxiv.org/abs/2308.11778](http://arxiv.org/abs/2308.11778)

    本论文通过分析分类器的Hessian矩阵和梯度在领域泛化中的作用，发现了跨领域的Hessian矩阵之间的谱范数是传输度量的上界，并分析了在鼓励Hessian和梯度之间的相似性时的所有对齐属性。

    

    在许多实际场景中，包括医疗保健和自动驾驶，超出分布（OOD）泛化是深度学习模型的关键能力。最近，已经提出了不同的技术来改进OOD泛化。在这些方法中，基于梯度的正则化器与其他竞争对手相比显示出有希望的性能。尽管取得了这样的成功，我们对Hessian和梯度对领域泛化的作用的认识仍然有限。为了解决这个缺点，我们使用最近的OOD转移性理论分析了分类器头部Hessian矩阵和梯度在领域泛化中的作用。从理论上讲，我们表明，跨领域的分类器头部Hessian矩阵之间的谱范数是传输度量的上界，传输度量是目标领域和源领域之间的距离的概念。此外，我们分析了在鼓励Hessian和梯度之间的相似性时所有的对齐属性。

    Out-of-distribution (OOD) generalization is a critical ability for deep learning models in many real-world scenarios including healthcare and autonomous vehicles. Recently, different techniques have been proposed to improve OOD generalization. Among these methods, gradient-based regularizers have shown promising performance compared with other competitors. Despite this success, our understanding of the role of Hessian and gradient alignment in domain generalization is still limited. To address this shortcoming, we analyze the role of the classifier's head Hessian matrix and gradient in domain generalization using recent OOD theory of transferability. Theoretically, we show that spectral norm between the classifier's head Hessian matrices across domains is an upper bound of the transfer measure, a notion of distance between target and source domains. Furthermore, we analyze all the attributes that get aligned when we encourage similarity between Hessians and gradients. Our analysis expl
    
[^96]: 3ET: 使用基于变化的ConvLSTM网络的高效事件驱动眼球追踪

    3ET: Efficient Event-based Eye Tracking using a Change-Based ConvLSTM Network. (arXiv:2308.11771v1 [cs.CV])

    [http://arxiv.org/abs/2308.11771](http://arxiv.org/abs/2308.11771)

    本文介绍了一种3ET方法，即使用基于变化的ConvLSTM网络的高效事件驱动眼球追踪的模型。该模型利用事件相机的优势，在标记的瞳孔数据集上实现了高效的时空特征提取和准确的瞳孔追踪，适用于资源有限的设备。

    

    本文提出了一种稀疏的基于变化的Convolutional Long Short-Term Memory (CB-ConvLSTM) 模型，用于事件驱动眼球追踪，这是下一代可穿戴医疗技术（如AR/VR头盔）的关键。我们利用视网膜灵感的事件相机的低延迟响应和稀疏输出事件流的优势，超过传统的基于帧的相机。我们的CB-ConvLSTM架构能够高效地从事件流中提取时空特征，用于瞳孔追踪，优于传统的CNN结构。通过利用增强激活稀疏性的增量编码循环路径，CB-ConvLSTM在经过标记的瞳孔数据集上的测试中，在不损失准确性的情况下，将算术操作减少了约4.7倍。这种提高效率的增加使其非常适合在资源有限的设备上进行实时眼球追踪。项目代码和数据集可在\url{https://github.com/qinche106/cb-convlstm-eyetracking}中公开获取。

    This paper presents a sparse Change-Based Convolutional Long Short-Term Memory (CB-ConvLSTM) model for event-based eye tracking, key for next-generation wearable healthcare technology such as AR/VR headsets. We leverage the benefits of retina-inspired event cameras, namely their low-latency response and sparse output event stream, over traditional frame-based cameras. Our CB-ConvLSTM architecture efficiently extracts spatio-temporal features for pupil tracking from the event stream, outperforming conventional CNN structures. Utilizing a delta-encoded recurrent path enhancing activation sparsity, CB-ConvLSTM reduces arithmetic operations by approximately 4.7$\times$ without losing accuracy when tested on a \texttt{v2e}-generated event dataset of labeled pupils. This increase in efficiency makes it ideal for real-time eye tracking in resource-constrained devices. The project code and dataset are openly available at \url{https://github.com/qinche106/cb-convlstm-eyetracking}.
    
[^97]: 通过综合分析临床和数字数据进行患者聚类

    Patient Clustering via Integrated Profiling of Clinical and Digital Data. (arXiv:2308.11748v1 [cs.LG])

    [http://arxiv.org/abs/2308.11748](http://arxiv.org/abs/2308.11748)

    本研究引入了一种基于个人配置的患者聚类模型，利用临床数据和数字交互数据构建患者配置，通过综合评估显示出卓越的聚类性能和推荐准确性。

    

    我们引入了一种新型的基于个人配置的患者聚类模型，该模型针对医疗保健中的临床数据设计。通过利用基于约束的低秩近似方法，我们的模型利用患者的临床数据和数字交互数据（包括浏览和搜索）来构建患者配置。作为该方法的结果，生成了非负的嵌入向量，作为患者的低维表示。我们使用来自医疗保健网站的真实患者数据对我们的模型进行了评估，采用了综合评估方法，考虑了聚类和推荐能力。与其他基线相比，我们的方法在聚类一致性和推荐准确性方面表现出卓越的性能。

    We introduce a novel profile-based patient clustering model designed for clinical data in healthcare. By utilizing a method grounded on constrained low-rank approximation, our model takes advantage of patients' clinical data and digital interaction data, including browsing and search, to construct patient profiles. As a result of the method, nonnegative embedding vectors are generated, serving as a low-dimensional representation of the patients. Our model was assessed using real-world patient data from a healthcare web portal, with a comprehensive evaluation approach which considered clustering and recommendation capabilities. In comparison to other baselines, our approach demonstrated superior performance in terms of clustering coherence and recommendation accuracy.
    
[^98]: Animal3D:一份全面的3D动物姿态和形状数据集

    Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape. (arXiv:2308.11737v1 [cs.CV])

    [http://arxiv.org/abs/2308.11737](http://arxiv.org/abs/2308.11737)

    Animal3D是一个全面的哺乳动物3D姿态和形状数据集，通过手工标注和检查确保了高质量的结果。通过Animal3D数据集，我们对形状和姿态估计模型进行了基准测试，包括使用只有Animal3D数据的监督学习、合成到真实的转移和微调人体姿势和形状估计模型。

    

    准确估计3D姿态和形状是理解动物行为的重要一步，对于野生动物保护等许多下游应用有潜在好处。然而，这个领域的研究受到缺乏包含高质量3D姿态和形状注释的全面多样数据集的限制。在本文中，我们提出了Animal3D，这是一个用于哺乳动物动物3D姿态和形状估计的首个全面数据集。Animal3D由来自40个哺乳动物物种的3379个图像组成，包含26个关键点的高质量注释，以及SMAL模型的姿态和形状参数。所有注释都经过多阶段手工标注和检查，以确保最高质量的结果。基于Animal3D数据集，我们对代表性的形状和姿态估计模型进行了以下基准测试：（1）只使用Animal3D数据的监督学习，（2）从合成生成的图像进行合成到真实的转移，（3）微调人体姿势和形状估计模型。

    Accurately estimating the 3D pose and shape is an essential step towards understanding animal behavior, and can potentially benefit many downstream applications, such as wildlife conservation. However, research in this area is held back by the lack of a comprehensive and diverse dataset with high-quality 3D pose and shape annotations. In this paper, we propose Animal3D, the first comprehensive dataset for mammal animal 3D pose and shape estimation. Animal3D consists of 3379 images collected from 40 mammal species, high-quality annotations of 26 keypoints, and importantly the pose and shape parameters of the SMAL model. All annotations were labeled and checked manually in a multi-stage process to ensure highest quality results. Based on the Animal3D dataset, we benchmark representative shape and pose estimation models at: (1) supervised learning from only the Animal3D data, (2) synthetic to real transfer from synthetically generated images, and (3) fine-tuning human pose and shape estim
    
[^99]: 多文档问答中的知识图谱引导

    Knowledge Graph Prompting for Multi-Document Question Answering. (arXiv:2308.11730v1 [cs.CL])

    [http://arxiv.org/abs/2308.11730](http://arxiv.org/abs/2308.11730)

    这篇论文提出了一种知识图谱引导的方法，用于在多文档问答任务中为大型语言模型（LLMs）提示正确的上下文。通过构建多个文档上的知识图谱，并设计基于语言模型的图遍历器，该方法能够帮助LLMs在MD-QA中进行答案预测。

    

    大型语言模型（LLMs）的“预训练、提示、预测”范式在开放域问答（OD-QA）中取得了显著的成功。然而，很少有工作在多文档问答（MD-QA）场景下探索这个范式，这是一个要求对不同文档的内容和结构之间的逻辑关联有深入理解的任务。为了填补这一重要的空白，我们提出了一种知识图谱引导（KGP）方法，用于在MD-QA中为LLMs提示正确的上下文，该方法包括图构建模块和图遍历模块。对于图构建，我们使用节点来表示文段或文档结构（例如，页面/表格），而使用边来表示文段之间的语义/词汇相似性或者文档内的结构关系。对于图遍历，我们设计了一个基于LM的图遍历器，它在节点之间导航并收集支持性的文段，以帮助LLMs在MD-QA中进行答案预测。

    The 'pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in the scenario of multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of different documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or intra-document structural relations. For graph traversal, we design an LM-guided graph traverser that navigates across nodes and gathers supporting passages assisting LLMs in MD
    
[^100]: 两个列表什么时候比一个列表更好？合作决策中的益处和伤害

    When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-making. (arXiv:2308.11721v1 [cs.LG])

    [http://arxiv.org/abs/2308.11721](http://arxiv.org/abs/2308.11721)

    这项研究分析了一种特定类型的人工和算法合作，对于多个噪音模型来说，将选择项目的子集大小$k$设置在$[2, n-1]$范围内能够最大化最终选择最佳项目的概率。

    

    在过去的机器学习研究中，很大一部分关注的是算法的性能，但最近更多地关注于优化人工和算法的联合性能。在这里，我们分析了一种特定类型的人工和算法合作，在这种合作中，算法可以访问一组n个项目，并将大小为k的一个子集呈现给人类，然后人类从这些k个项目中选择一个最终项目。这种情况可以模拟内容推荐、路径规划或任何类型的标注任务。由于人类和算法都对项目的真实排序有着不完美、有噪音的信息，关键问题是：哪个$k$值能最大化最终选择最佳项目的概率？对于$k=1$，算法单独行动时性能最优，而对于$k=n$，人类单独行动时性能最优。令人惊讶的是，我们发现对于多个噪音模型，将$k$设置在$[2, n-1]$范围内是最优的，也就是说，合作有明显的益处。

    Historically, much of machine learning research has focused on the performance of the algorithm alone, but recently more attention has been focused on optimizing joint human-algorithm performance. Here, we analyze a specific type of human-algorithm collaboration where the algorithm has access to a set of $n$ items, and presents a subset of size $k$ to the human, who selects a final item from among those $k$. This scenario could model content recommendation, route planning, or any type of labeling task. Because both the human and algorithm have imperfect, noisy information about the true ordering of items, the key question is: which value of $k$ maximizes the probability that the best item will be ultimately selected? For $k=1$, performance is optimized by the algorithm acting alone, and for $k=n$ it is optimized by the human acting alone. Surprisingly, we show that for multiple of noise models, it is optimal to set $k \in [2, n-1]$ - that is, there are strict benefits to collaborating,
    
[^101]: 通过使用来自集合扩展的示例进行语言探测来推进关系提取

    Advancing Relation Extraction through Language Probing with Exemplars from Set Co-Expansion. (arXiv:2308.11720v1 [cs.CL])

    [http://arxiv.org/abs/2308.11720](http://arxiv.org/abs/2308.11720)

    本文提出了一种通过集合扩展和代表性示例的语言探测方法来推进关系提取。该方法通过整合相似度度量和类别排序，提高了关系分类准确性并减少对比类之间的混淆。经验证明该方法有效提高了关系提取的性能。

    

    关系提取是从非结构化文本中自动提取结构化信息的关键任务。本文提出了一种多方面的方法，通过整合代表性示例和集合扩展来提高关系分类准确性并减少对比类之间的混淆。我们的方法首先通过代表性示例为每个关系类提供种子。随后，我们的集合扩展算法通过将目标对和目标类的代表对之间的相似度度量纳入训练目标来丰富训练目标。此外，集合扩展过程还涉及一个考虑对比类示例的类别排序过程。利用无上下文的Hearst模式利用关系提及的上下文细节来确定上下文相似性。经验证明我们的集合扩展方法的有效性，提高了关系提取的性能。

    Relation Extraction (RE) is a pivotal task in automatically extracting structured information from unstructured text. In this paper, we present a multi-faceted approach that integrates representative examples and through co-set expansion. The primary goal of our method is to enhance relation classification accuracy and mitigating confusion between contrastive classes.  Our approach begins by seeding each relationship class with representative examples. Subsequently, our co-set expansion algorithm enriches training objectives by incorporating similarity measures between target pairs and representative pairs from the target class. Moreover, the co-set expansion process involves a class ranking procedure that takes into account exemplars from contrastive classes. Contextual details encompassing relation mentions are harnessed via context-free Hearst patterns to ascertain contextual similarity.  Empirical evaluation demonstrates the efficacy of our co-set expansion approach, resulting in a
    
[^102]: SuperCalo: 能量沉积量模拟的超分辨率技术

    SuperCalo: Calorimeter shower super-resolution. (arXiv:2308.11700v1 [physics.ins-det])

    [http://arxiv.org/abs/2308.11700](http://arxiv.org/abs/2308.11700)

    本文介绍了一种名为SuperCalo的能量沉积模拟超分辨率模型，能够快速上采样高维细粒度的能量沉积模拟，从而降低计算成本和生成时间。

    

    能量沉积模拟是大型强子对撞机计算流程中的主要瓶颈。最近有一些工作使用深度生成模型来克服这个挑战，但许多表现最佳的模型在训练和生成时间上无法很好地适应高维能量沉积模拟。在本文中，我们介绍了一种名为SuperCalo的基于流的超分辨率模型，并证明了高维细粒度的能量沉积模拟可以从粗粒度模拟中快速上采样。这种新颖的方法可以降低与快速能量沉积模拟模型相关的计算成本、内存需求和生成时间。另外，我们还展示了由SuperCalo上采样得到的能量沉积模拟具有高度变化的特点。这使得可以从较少的粗粒度模拟中上采样出多个高维能量沉积模拟，以高保真度的方式进一步减少生成时间。

    Calorimeter shower simulation is a major bottleneck in the Large Hadron Collider computational pipeline. There have been recent efforts to employ deep-generative surrogate models to overcome this challenge. However, many of best performing models have training and generation times that do not scale well to high-dimensional calorimeter showers. In this work, we introduce SuperCalo, a flow-based super-resolution model, and demonstrate that high-dimensional fine-grained calorimeter showers can be quickly upsampled from coarse-grained showers. This novel approach presents a way to reduce computational cost, memory requirements and generation time associated with fast calorimeter simulation models. Additionally, we show that the showers upsampled by SuperCalo possess a high degree of variation. This allows a large number of high-dimensional calorimeter showers to be upsampled from much fewer coarse showers with high-fidelity, which results in additional reduction in generation time.
    
[^103]: 有效的语言模型基准测试

    Efficient Benchmarking (of Language Models). (arXiv:2308.11696v1 [cs.CL])

    [http://arxiv.org/abs/2308.11696](http://arxiv.org/abs/2308.11696)

    本研究提出了一种名为"Efficient Benchmarking"的问题，旨在智能地减少语言模型评估的计算成本而不降低可靠性，并使用一种名为Decision Impact on Reliability（DIoR）的新度量来评估决策的可靠性。通过HELM基准测试的案例研究，发现只需删除一个低排名模型即可改变领先者，并仅需少量示例即可得到正确的基准测试排名。

    

    语言模型的多功能性增加导致了一类全面评估广泛能力的基准测试的出现。这些基准测试与大规模计算成本相关，每个模型需要数千个GPU小时。然而，关于评估效率方面的问题在文献中讨论较少。本文提出了一种名为"Efficient Benchmarking"的问题，即在不损害可靠性的情况下智能地减少语言模型评估的计算成本。通过使用HELM基准测试作为示例，我们研究了不同基准测试设计选择如何影响计算-可靠性权衡。我们提出使用一种名为Decision Impact on Reliability（DIoR）的新度量来评估这些决策的可靠性。例如，我们发现仅通过从基准测试中删除一个低排名模型，当前在HELM上的领先者可能会改变，并且观察到只需一小部分示例即可获得正确的基准测试排名。

    The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark rank
    
[^104]: 边缘上对新的人体活动进行增量学习的实用见解

    Practical Insights on Incremental Learning of New Human Physical Activity on the Edge. (arXiv:2308.11691v1 [eess.SP])

    [http://arxiv.org/abs/2308.11691](http://arxiv.org/abs/2308.11691)

    本文探讨了边缘机器学习中的重要挑战，包括边缘设备上的数据存储限制、有限的训练计算能力以及学习类别数量的影响。研究通过移动传感器收集的数据学习人体活动，从而为边缘机器学习提供了有价值的见解。

    

    边缘机器学习（Edge ML）将计算智能从基于云的系统转移到边缘设备，由于其明显的优势，如降低延迟、增强数据隐私和减少连接依赖，吸引了极大的关注。尽管这些优势具有说服力，但它们引入了传统基于云的方法中缺失的独特挑战。本文深入探讨了边缘学习的复杂性，研究了边缘设备上的受限数据存储、有限的训练计算能力以及学习类别数量之间的相互依赖关系。通过使用我们的MAGNETO系统进行的实验，重点关注通过移动传感器收集的数据学习人体活动，我们突出了这些挑战，并提供了有价值的边缘机器学习观点。

    Edge Machine Learning (Edge ML), which shifts computational intelligence from cloud-based systems to edge devices, is attracting significant interest due to its evident benefits including reduced latency, enhanced data privacy, and decreased connectivity reliance. While these advantages are compelling, they introduce unique challenges absent in traditional cloud-based approaches. In this paper, we delve into the intricacies of Edge-based learning, examining the interdependencies among: (i) constrained data storage on Edge devices, (ii) limited computational power for training, and (iii) the number of learning classes. Through experiments conducted using our MAGNETO system, that focused on learning human activities via data collected from mobile sensors, we highlight these challenges and offer valuable perspectives on Edge ML.
    
[^105]: 无范例类增量学习的初始训练策略分析

    An Analysis of Initial Training Strategies for Exemplar-Free Class-Incremental Learning. (arXiv:2308.11677v1 [cs.LG])

    [http://arxiv.org/abs/2308.11677](http://arxiv.org/abs/2308.11677)

    本文分析了无范例类增量学习过程中的初始训练策略。研究发现，初始学习策略的选择会显著影响增量学习模型的性能，但目前还没有进行深入研究。

    

    类增量学习旨在从数据流中构建分类模型。在类增量学习过程的每一步中，新的类别必须被整合到模型中。由于灾难性遗忘，当无法存储过去类别的样本时，类增量学习变得尤为具有挑战性，这正是我们在此研究的对象。迄今为止，大多数方法仅基于类增量学习过程的目标数据集。然而，最近在大量数据上通过自监督方式预训练模型的使用已经逐渐增多。类增量学习过程的初始模型可能仅使用目标数据集的第一批数据，或者还可以使用在辅助数据集上获得的预训练权重。这两种初始学习策略的选择可以极大地影响增量学习模型的性能，但目前还没有进行深入研究。性能还受到类增量学习算法的选择、神经网络结构、目标任务的性质、类别分布的影响。

    Class-Incremental Learning (CIL) aims to build classification models from data streams. At each step of the CIL process, new classes must be integrated into the model. Due to catastrophic forgetting, CIL is particularly challenging when examples from past classes cannot be stored, the case on which we focus here. To date, most approaches are based exclusively on the target dataset of the CIL process. However, the use of models pre-trained in a self-supervised way on large amounts of data has recently gained momentum. The initial model of the CIL process may only use the first batch of the target dataset, or also use pre-trained weights obtained on an auxiliary dataset. The choice between these two initial learning strategies can significantly influence the performance of the incremental learning model, but has not yet been studied in depth. Performance is also influenced by the choice of the CIL algorithm, the neural architecture, the nature of the target task, the distribution of clas
    
[^106]: "非混淆协变量对基于潜在结果框架的方法推断性能的影响研究"

    A Study on the Impact of Non-confounding Covariates on the Inferential Performance of Methods based on the Potential Outcome Framework. (arXiv:2308.11676v1 [stat.ME])

    [http://arxiv.org/abs/2308.11676](http://arxiv.org/abs/2308.11676)

    "本文研究了非混淆协变量对基于潜在结果框架的方法推断性能的影响，通过提供统一的图形框架来增强对这些模型基本原理的理解，为实际场景中应用这些模型带来了潜在价值。"

    

    "潜在结果框架（POF）在因果推断领域中起着重要作用。大多数基于POF的因果推断模型（CIMs-B-POF）旨在消除混淆偏差，并默认存在混淆协变量的基本假设。这一假设认为协变量仅由混淆变量组成。然而，在实践中保持混淆协变量的假设是具有挑战性的，特别是在处理高维协变量时。虽然已经提出了一些方法在进行因果推断之前区分协变量的不同组成部分，但将非混淆的协变量视为混淆变量的后果仍不清楚。这种不确定性在实际场景中应用CIMs-B-POF时存在潜在风险。在本文中，我们提出了一个统一的图形框架，用于理解CIMs-B-POF模型的基本原理。利用这个图形框架，我们对CIMs-B-POF的性能进行了量化分析。"

    The Potential Outcome Framework (POF) plays a prominent role in the field of causal inference. Most causal inference models based on the POF (CIMs-B-POF) are designed for eliminating confounding bias and default to an underlying assumption of Confounding Covariates. This assumption posits that the covariates consist solely of confounders. However, the assumption of Confounding Covariates is challenging to maintain in practice, particularly when dealing with high-dimensional covariates. While certain methods have been proposed to differentiate the distinct components of covariates prior to conducting causal inference, the consequences of treating non-confounding covariates as confounders remain unclear. This ambiguity poses a potential risk when applying the CIMs-B-POF in practical scenarios. In this paper, we present a unified graphical framework for the CIMs-B-POF, which greatly enhances the comprehension of these models' underlying principles. Using this graphical framework, we quant
    
[^107]: WEARS: 基于实时传感器数据的可穿戴情绪人工智能

    WEARS: Wearable Emotion AI with Real-time Sensor data. (arXiv:2308.11673v1 [eess.SP])

    [http://arxiv.org/abs/2308.11673](http://arxiv.org/abs/2308.11673)

    WEARS是一个基于智能手表传感器的情绪预测系统，通过收集真实数据和多种机器学习模型实现用户情绪的预测，并通过消融研究分析了心率、加速度计和陀螺仪传感器数据对情绪的影响。

    

    情绪预测是研究理解人类情绪的领域。现有方法主要集中在文本、音频、面部表情等特征上，但这些特征可能对用户来说是私密的。情绪也可以从主题的心理数据中推导得到。已经提出了使用生理传感器组合进行情绪识别的各种方法。然而，并非所有传感器对用户在日常生活中使用都方便。因此，我们提出了使用智能手表传感器来预测用户情绪的系统。我们设计了一个框架，利用基于英语和区域语言的视频来引起参与者的情绪并收集数据，以实时采集真实数据。此外，由于数据集有限，我们将问题建模为二分类，并尝试了多种机器学习模型。我们还进行了消融研究，以了解心率、加速度计和陀螺仪传感器数据对心情的影响。

    Emotion prediction is the field of study to understand human emotions. Existing methods focus on modalities like text, audio, facial expressions, etc., which could be private to the user. Emotion can be derived from the subject's psychological data as well. Various approaches that employ combinations of physiological sensors for emotion recognition have been proposed. Yet, not all sensors are simple to use and handy for individuals in their daily lives. Thus, we propose a system to predict user emotion using smartwatch sensors. We design a framework to collect ground truth in real-time utilizing a mix of English and regional language-based videos to invoke emotions in participants and collect the data. Further, we modeled the problem as binary classification due to the limited dataset size and experimented with multiple machine-learning models. We also did an ablation study to understand the impact of features including Heart Rate, Accelerometer, and Gyroscope sensor data on mood. From
    
[^108]: 使用组织和实验嵌入推广序列模型进行表观基因组预测

    Generalising sequence models for epigenome predictions with tissue and assay embeddings. (arXiv:2308.11671v1 [q-bio.GN])

    [http://arxiv.org/abs/2308.11671](http://arxiv.org/abs/2308.11671)

    本文提出了一种将组织和实验嵌入到上下文化的基因组网络（CGN）中的方法，通过在输入空间中增强长程序列嵌入来提供上下文信息，实现了广泛的实验条件下的强相关性，并在多种设置中超越了现有技术水平。

    

    近年来，用于表观遗传图谱预测的序列建模方法在序列长度、模型规模和图谱多样性方面有了进一步发展。然而，由于对上下文信息的利用不足，当前模型无法对许多实验可行的组织和实验组合进行推理，限制了对调控基因组的$\textit{in silico}$理解。我们演示了通过将组织和实验嵌入纳入到上下文化的基因组网络（CGN）中，可以在广泛的实验条件下实现强相关性。与以前的方法不同，我们通过在输入空间中增强长程序列嵌入来提供上下文信息，而不是扩展输出空间。我们展示了我们方法在广泛的表观遗传图谱中的有效性，并首次揭示了遗传变异对表观序列模型训练的影响。我们的上下文集成方法在多种设置中超越了现有技术水平。

    Sequence modelling approaches for epigenetic profile prediction have recently expanded in terms of sequence length, model size, and profile diversity. However, current models cannot infer on many experimentally feasible tissue and assay pairs due to poor usage of contextual information, limiting $\textit{in silico}$ understanding of regulatory genomics. We demonstrate that strong correlation can be achieved across a large range of experimental conditions by integrating tissue and assay embeddings into a Contextualised Genomic Network (CGN). In contrast to previous approaches, we enhance long-range sequence embeddings with contextual information in the input space, rather than expanding the output space. We exhibit the efficacy of our approach across a broad set of epigenetic profiles and provide the first insights into the effect of genetic variants on epigenetic sequence model training. Our general approach to context integration exceeds state of the art in multiple settings while emp
    
[^109]: 使用多元时间序列分类的基于机器学习的工厂环境定位系统

    Machine Learning-based Positioning using Multivariate Time Series Classification for Factory Environments. (arXiv:2308.11670v1 [eess.SP])

    [http://arxiv.org/abs/2308.11670](http://arxiv.org/abs/2308.11670)

    本文提出了一种基于机器学习的室内定位系统，使用多元时间序列分类，通过IoT设备的内部传感器数据实现在工厂环境中定位移动实体。

    

    室内定位系统在许多工业应用中变得越来越重要。现有解决方案严重依赖于外部基础设施，可能涉及潜在的隐私妥协、外部信息要求和假设，这使得这些解决方案对于需要保护隐私和延长功能的环境不利。在某些环境中，部署辅助基础设施进行室内定位可能是不可行和昂贵的。机器学习的最新发展提供了解决这些限制的方案，仅依靠物联网设备的内部传感器数据。然而，针对物联网设备的资源限制，目前还不清楚哪种模型最合适。本文提出了一种基于机器学习的室内定位系统，使用运动和环境传感器，在关注隐私的工厂环境中定位移动实体。该问题被构建为多元时间序列分类 (MTSC)，并进行了比较分析。

    Indoor Positioning Systems (IPS) gained importance in many industrial applications. State-of-the-art solutions heavily rely on external infrastructures and are subject to potential privacy compromises, external information requirements, and assumptions, that make it unfavorable for environments demanding privacy and prolonged functionality. In certain environments deploying supplementary infrastructures for indoor positioning could be infeasible and expensive. Recent developments in machine learning (ML) offer solutions to address these limitations relying only on the data from onboard sensors of IoT devices. However, it is unclear which model fits best considering the resource constraints of IoT devices. This paper presents a machine learning-based indoor positioning system, using motion and ambient sensors, to localize a moving entity in privacy concerned factory environments. The problem is formulated as a multivariate time series classification (MTSC) and a comparative analysis of 
    
[^110]: 基于类标签的图异常检测

    Class Label-aware Graph Anomaly Detection. (arXiv:2308.11669v1 [cs.LG])

    [http://arxiv.org/abs/2308.11669](http://arxiv.org/abs/2308.11669)

    本研究提出了一种基于类标签的图异常检测框架（CLAD），利用少量标记节点来增强无监督图异常检测的性能，在十个数据集上的实验证明了其优越性能。

    

    无监督的图异常检测方法假设缺乏异常标签，即节点是否异常。我们从以往的无监督方法中观察到，它们不仅假设缺乏异常标签，还假设缺乏类标签（节点在一般节点分类任务中所属的类）。在这项工作中，我们研究了类标签对无监督图异常检测的效用，特别是它们如何增强对结构异常的检测。为此，我们提出了一种基于类标签的图异常检测框架（CLAD），利用有限数量的标记节点来提高无监督图异常检测的性能。在十个数据集上进行的大量实验证明了CLAD相对于现有的无监督图异常检测方法的优越性能，即使在没有地面真实类标签信息的情况下。CLAD的源代码可在\url{https://github.com/jhkim611/CLAD}上找到。

    Unsupervised GAD methods assume the lack of anomaly labels, i.e., whether a node is anomalous or not. One common observation we made from previous unsupervised methods is that they not only assume the absence of such anomaly labels, but also the absence of class labels (the class a node belongs to used in a general node classification task). In this work, we study the utility of class labels for unsupervised GAD; in particular, how they enhance the detection of structural anomalies. To this end, we propose a Class Label-aware Graph Anomaly Detection framework (CLAD) that utilizes a limited amount of labeled nodes to enhance the performance of unsupervised GAD. Extensive experiments on ten datasets demonstrate the superior performance of CLAD in comparison to existing unsupervised GAD methods, even in the absence of ground-truth class label information. The source code for CLAD is available at \url{https://github.com/jhkim611/CLAD}.
    
[^111]: 一种模拟保险欺诈网络数据的引擎

    An engine to simulate insurance fraud network data. (arXiv:2308.11659v1 [cs.LG])

    [http://arxiv.org/abs/2308.11659](http://arxiv.org/abs/2308.11659)

    本论文介绍了一种模拟保险欺诈网络数据的引擎，利用索赔涉及方的社交网络特征进行学习方法，旨在开发高效准确的欺诈检测模型。但面临类别不平衡、大量未标记数据和缺乏公开数据等挑战。

    

    传统上，检测保险欺诈索赔依赖于业务规则和专家判断，这使得这一过程耗时且昂贵。因此，研究人员一直在探索开发高效准确的分析策略来标记可疑索赔。从索赔涉及方的社交网络中提取特征并将其馈送给学习方法是一种特别有潜力的策略。然而，在开发欺诈检测模型时，我们面临着几个挑战。例如，欺诈的非常规性质导致了高度的类别不平衡，这增加了开发性能良好的分析分类模型的难度。此外，只有少数索赔得到调查和标签，从而产生了大量未标记的数据。另一个挑战是缺乏公开可用的数据，这妨碍了研究和模型验证。

    Traditionally, the detection of fraudulent insurance claims relies on business rules and expert judgement which makes it a time-consuming and expensive process (\'Oskarsd\'ottir et al., 2022). Consequently, researchers have been examining ways to develop efficient and accurate analytic strategies to flag suspicious claims. Feeding learning methods with features engineered from the social network of parties involved in a claim is a particularly promising strategy (see for example Van Vlasselaer et al. (2016); Tumminello et al. (2023)). When developing a fraud detection model, however, we are confronted with several challenges. The uncommon nature of fraud, for example, creates a high class imbalance which complicates the development of well performing analytic classification models. In addition, only a small number of claims are investigated and get a label, which results in a large corpus of unlabeled data. Yet another challenge is the lack of publicly available data. This hinders not 
    
[^112]: BCI评估的伪在线框架：MOABB视角

    Pseudo-online framework for BCI evaluation: A MOABB perspective. (arXiv:2308.11656v1 [cs.HC])

    [http://arxiv.org/abs/2308.11656](http://arxiv.org/abs/2308.11656)

    这篇论文提出了一种基于伪在线模式的BCI评估框架，可以更准确地模拟在线处理的特性，具有较好的治疗应用。

    

    BCI（脑机接口）技术有三种操作模式：在线、离线和伪在线。在线模式下，实时的脑电数据被持续分析。离线模式下，信号在采集后进行处理。伪在线模式下，采集到的数据被处理成仿真实时接收的形式。主要区别在于离线模式经常分析整个数据，而在线和伪在线模式只分析短时间窗口的数据。离线分析通常与异步BCI一起使用，这限制了分析到预定义的时间窗口。与在线和伪在线模式兼容的异步BCI允许灵活的思维活动持续时间。离线处理往往更准确，而在线分析则更适用于治疗应用。伪在线实现近似于无实时约束的在线处理。许多离线的BCI研究相对于真实环境中的情景引入了偏差，影响了分类结果。

    Objective: BCI (Brain-Computer Interface) technology operates in three modes: online, offline, and pseudo-online. In the online mode, real-time EEG data is constantly analyzed. In offline mode, the signal is acquired and processed afterwards. The pseudo-online mode processes collected data as if they were received in real-time. The main difference is that the offline mode often analyzes the whole data, while the online and pseudo-online modes only analyze data in short time windows. Offline analysis is usually done with asynchronous BCIs, which restricts analysis to predefined time windows. Asynchronous BCI, compatible with online and pseudo-online modes, allows flexible mental activity duration. Offline processing tends to be more accurate, while online analysis is better for therapeutic applications. Pseudo-online implementation approximates online processing without real-time constraints. Many BCI studies being offline introduce biases compared to real-life scenarios, impacting clas
    
[^113]: 大型变压器是更好的脑电图学习器

    Large Transformers are Better EEG Learners. (arXiv:2308.11654v1 [eess.SP])

    [http://arxiv.org/abs/2308.11654](http://arxiv.org/abs/2308.11654)

    本研究表明从图像和文本预训练的大型变压器模型可以直接应用于脑电图预测任务的微调，通过设计AdaCE模块在多个EEG基于预测任务上取得了最新的性能。

    

    预训练的大型变压器模型在自然语言处理和计算机视觉领域取得了显著的性能。由于可用的标记脑电图（EEG）数据的规模远远低于文本和图像数据，因此很难将从EEG预训练的变压器模型开发到像GPT-4 100T这样的规模，从而完全发挥该架构的潜力。在本文中，我们展示了从图像和文本预训练的变压器模型可以直接用于EEG基于预测任务的微调。我们设计了AdaCE，即将EEG数据转换为图像和文本形式的插拔式适配器，用于微调预训练的视觉和语言变压器。提出的AdaCE模块在微调预训练的变压器模型时非常有效，同时在多种基于EEG的预测任务上实现了最新的性能。例如，预训练的Swin-Transformer上的AdaCE达到了99.6％的精度，绝对改善了9.2％。

    Pre-trained large transformer models have achieved remarkable performance in the fields of natural language processing and computer vision. Since the magnitude of available labeled electroencephalogram (EEG) data is much lower than that of text and image data, it is difficult for transformer models pre-trained from EEG to be developed as large as GPT-4 100T to fully unleash the potential of this architecture. In this paper, we show that transformers pre-trained from images as well as text can be directly fine-tuned for EEG-based prediction tasks. We design AdaCE, plug-and-play Adapters for Converting EEG data into image as well as text forms, to fine-tune pre-trained vision and language transformers. The proposed AdaCE module is highly effective for fine-tuning pre-trained transformers while achieving state-of-the-art performance on diverse EEG-based prediction tasks. For example, AdaCE on the pre-trained Swin-Transformer achieves 99.6%, an absolute improvement of 9.2%, on the EEG-deco
    
[^114]: 使用基于强化学习的初始化加速精确组合优化--调度案例研究

    Accelerating Exact Combinatorial Optimization via RL-based Initialization -- A Case Study in Scheduling. (arXiv:2308.11652v1 [cs.LG])

    [http://arxiv.org/abs/2308.11652](http://arxiv.org/abs/2308.11652)

    本研究提出了一种基于强化学习的初始化加速精确组合优化的方法，以调度为案例研究。该方法在保持启发式方法运行时间成本的同时，提供了优化性和确定性的保证，并在实际的EdgeTPU平台上实现了128倍的速度提升。

    

    数据流图（也称为计算图）上的调度是一个NP难的问题。传统的精确方法受到运行时复杂度的限制，而强化学习（RL）和启发式方法在确定性和解决方案质量方面存在困难。本研究旨在开发一种创新方法，利用机器学习（ML）解决组合优化问题，以调度为案例研究。目标是在保持启发式方法运行时间成本的同时，提供优化性和确定性的保证。具体而言，我们引入了一种新颖的两阶段RL-ILP调度框架，包括三个步骤：1）RL求解器作为粗粒度调度器，2）解决方案放松和3）通过ILP进行精确求解。我们的框架表明，在实际的EdgeTPU平台上，利用ImageNet DNN计算，与使用精确调度方法相比，具有相同的调度性能并实现高达128倍的速度提升。

    Scheduling on dataflow graphs (also known as computation graphs) is an NP-hard problem. The traditional exact methods are limited by runtime complexity, while reinforcement learning (RL) and heuristic-based approaches struggle with determinism and solution quality. This research aims to develop an innovative approach that employs machine learning (ML) for addressing combinatorial optimization problems, using scheduling as a case study. The goal is to provide guarantees in optimality and determinism while maintaining the runtime cost of heuristic methods. Specifically, we introduce a novel two-phase RL-to-ILP scheduling framework, which includes three steps: 1) RL solver acts as coarse-grain scheduler, 2) solution relaxation and 3) exact solving via ILP. Our framework demonstrates the same scheduling performance compared with using exact scheduling methods while achieving up to 128 $\times$ speed improvements. This was conducted on actual EdgeTPU platforms, utilizing ImageNet DNN comput
    
[^115]: 分布鲁棒跨主题脑电解码

    Distributionally Robust Cross Subject EEG Decoding. (arXiv:2308.11651v1 [eess.SP])

    [http://arxiv.org/abs/2308.11651](http://arxiv.org/abs/2308.11651)

    本文提出了一种基于分布鲁棒优化的方法，通过对数据的动态演化来提高脑电解码的稳健性。

    

    最近，深度学习已经被证明对于脑电解码任务非常有效。然而，其性能可能会受到两个关键因素的消极影响：1）信号中固有的高方差和不同类型的污染，2）脑电数据集通常相对较小，给定了采集成本、注释成本和所需的努力量。数据增强方法已经在实践中进行了研究，通过在空间域、时间域或频率域上进行数据增强操作，手工基于领域知识的专家知识。在这项工作中，我们提出了一种基于分布鲁棒优化的原则性方法，通过对数据的动态演化来提高解码的稳健性。该方法通过优化一族演化数据分布来实现鲁棒性，而不是单一训练数据分布。我们基于Wasserstein梯度推导出了一个通用的数据演化框架。

    Recently, deep learning has shown to be effective for Electroencephalography (EEG) decoding tasks. Yet, its performance can be negatively influenced by two key factors: 1) the high variance and different types of corruption that are inherent in the signal, 2) the EEG datasets are usually relatively small given the acquisition cost, annotation cost and amount of effort needed. Data augmentation approaches for alleviation of this problem have been empirically studied, with augmentation operations on spatial domain, time domain or frequency domain handcrafted based on expertise of domain knowledge. In this work, we propose a principled approach to perform dynamic evolution on the data for improvement of decoding robustness. The approach is based on distributionally robust optimization and achieves robustness by optimizing on a family of evolved data distributions instead of the single training data distribution. We derived a general data evolution framework based on Wasserstein gradient f
    
[^116]: 联邦学习中非独立和同分布数据的联合局部关系增强和全局纳什均衡

    Joint Local Relational Augmentation and Global Nash Equilibrium for Federated Learning with Non-IID Data. (arXiv:2308.11646v1 [cs.LG])

    [http://arxiv.org/abs/2308.11646](http://arxiv.org/abs/2308.11646)

    本论文中提出的FedRANE方法结合了局部关系增强和全局纳什均衡，可以解决分布式机器学习中的非独立非同分布数据问题。

    

    联邦学习是一种需要服务器和一系列具有分散数据的客户端之间合作的分布式机器学习范 paradigm。为了使联邦学习在实际应用中有效，现有的研究致力于改进非独立相同分布(non-IID)的分散数据的建模。在非IID环境中，在数据建模中存在来自不平衡数据的客户端内一致性和异构客户端分布之间的客户端间不一致性，这不仅阻碍了少数数据的充分表示，还带来了不一致的模型偏差。然而，以往的工作忽视了同时处理上述两种耦合不一致性。在这项工作中，我们提出了FedRANE，它由两个主要模块组成，即局部关系增强(LRA)和全局纳什均衡(GNE)，以同时解决客户端内和客户端间的不一致性问题。

    Federated learning (FL) is a distributed machine learning paradigm that needs collaboration between a server and a series of clients with decentralized data. To make FL effective in real-world applications, existing work devotes to improving the modeling of decentralized data with non-independent and identical distributions (non-IID). In non-IID settings, there are intra-client inconsistency that comes from the imbalanced data modeling, and inter-client inconsistency among heterogeneous client distributions, which not only hinders sufficient representation of the minority data, but also brings discrepant model deviations. However, previous work overlooks to tackle the above two coupling inconsistencies together. In this work, we propose FedRANE, which consists of two main modules, i.e., local relational augmentation (LRA) and global Nash equilibrium (GNE), to resolve intra- and inter-client inconsistency simultaneously. Specifically, in each client, LRA mines the similarity relations a
    
[^117]: 使用脑电图数据的心搏骤停后昏迷患者的神经学预后:一种具有竞争风险的动态生存分析框架

    Neurological Prognostication of Post-Cardiac-Arrest Coma Patients Using EEG Data: A Dynamic Survival Analysis Framework with Competing Risks. (arXiv:2308.11645v1 [eess.SP])

    [http://arxiv.org/abs/2308.11645](http://arxiv.org/abs/2308.11645)

    本研究提出了一种基于脑电图数据的动态生存分析框架，用于预测心搏骤停后昏迷患者的神经学预后。该框架可以根据不同时点患者的脑电图数据进行预测，并提供苏醒或死亡的概率。这是目前已知的第一个关于这一领域的动态框架。

    

    从心搏骤停复苏进入昏迷状态的患者面临着较高的死亡风险。预测这些患者的神经学结局（神经学预后任务）可以帮助决策治疗。在本文中，我们提出了首个动态框架，使用脑电图数据对心搏骤停后昏迷患者进行神经学预后：我们的框架根据随着更多脑电图数据的获得，随时间为患者做出预测，并且不同的训练患者可用的脑电图时间序列的长度可能存在差异。预测可通过时间-事件结果（苏醒时间或死亡时间）或患者在多个时间段苏醒或死亡的概率来表述。我们的框架使用任何支持竞争风险的动态生存分析模型，以估计患者水平的累积发生函数。我们考虑患者首先出现的三种竞争风险：苏醒、死亡或发作。

    Patients resuscitated from cardiac arrest who enter a coma are at high risk of death. Forecasting neurological outcomes of these patients (the task of neurological prognostication) could help with treatment decisions. In this paper, we propose, to the best of our knowledge, the first dynamic framework for neurological prognostication of post-cardiac-arrest comatose patients using EEG data: our framework makes predictions for a patient over time as more EEG data become available, and different training patients' available EEG time series could vary in length. Predictions are phrased in terms of either time-to-event outcomes (time-to-awakening or time-to-death) or as the patient's probability of awakening or of dying across multiple time horizons. Our framework uses any dynamic survival analysis model that supports competing risks in the form of estimating patient-level cumulative incidence functions. We consider three competing risks as to what happens first to a patient: awakening, bei
    
[^118]: 结构振动多模态时间序列的协同信号去噪

    Synergistic Signal Denoising for Multimodal Time Series of Structure Vibration. (arXiv:2308.11644v1 [eess.SP])

    [http://arxiv.org/abs/2308.11644](http://arxiv.org/abs/2308.11644)

    这种论文提出了一种深度学习算法，通过融合卷积和循环结构以及关键的注意力机制，协同去噪多模态振动信号，从而在结构健康监测中提升了预测准确性、早期损伤检测和适应性。

    

    结构健康监测（SHM）在确保基础设施的长久性和安全性方面扮演着不可或缺的角色。随着传感器技术的快速发展，从各种结构中产生的数据量呈现出前所未有的激增，给有效分析和解释带来了挑战。本文介绍了一种新颖的深度学习算法，针对SHM中普遍存在的多模态振动信号的复杂性进行了定制。通过融合卷积和循环的结构，该算法灵活地捕捉到了局部和持久的结构行为。关键的注意力机制的整合进一步增强了模型的能力，使其能够区分并优先处理显著的结构响应和外部噪声。我们的结果展示了在预测准确性、早期损伤检测和适应多个SHM场景方面的显著改进。鉴于SHM的重要性，所提出的方法不仅提供了

    Structural Health Monitoring (SHM) plays an indispensable role in ensuring the longevity and safety of infrastructure. With the rapid growth of sensor technology, the volume of data generated from various structures has seen an unprecedented surge, bringing forth challenges in efficient analysis and interpretation. This paper introduces a novel deep learning algorithm tailored for the complexities inherent in multimodal vibration signals prevalent in SHM. By amalgamating convolutional and recurrent architectures, the algorithm adeptly captures both localized and prolonged structural behaviors. The pivotal integration of attention mechanisms further enhances the model's capability, allowing it to discern and prioritize salient structural responses from extraneous noise. Our results showcase significant improvements in predictive accuracy, early damage detection, and adaptability across multiple SHM scenarios. In light of the critical nature of SHM, the proposed approach not only offers 
    
[^119]: 通过处理S参数模式对氧化铟锡电极进行现场故障诊断

    In situ Fault Diagnosis of Indium Tin Oxide Electrodes by Processing S-Parameter Patterns. (arXiv:2308.11639v1 [eess.SP])

    [http://arxiv.org/abs/2308.11639](http://arxiv.org/abs/2308.11639)

    本研究提出了一种利用散射参数（S参数）信号处理的现场故障诊断方法，对氧化铟锡（ITO）电极进行故障检测和诊断。这种方法具有早期检测、高诊断精度、噪声鲁棒性和根本原因分析的优势。

    

    在光电子领域，氧化铟锡（ITO）电极在显示器、传感器和太阳能电池等各种应用中起着关键作用。有效的故障检测和诊断是确保设备性能和可靠性的关键。然而，传统的视觉检查对于透明的ITO电极来说具有挑战性，而现有的故障检测方法在确定缺陷根本原因方面存在局限性，通常需要破坏性评估。本研究提出了一种利用散射参数（S参数）信号处理的现场故障诊断方法，提供了早期检测、高诊断精度、噪声鲁棒性和根本原因分析。根据缺陷状态获取了全面的S参数模式数据库。然后使用深度学习（DL）方法，包括多层感知器（MLP）、卷积神经网络（CNN）和Transformer，同时分析故障原因和严重性。

    In the field of optoelectronics, indium tin oxide (ITO) electrodes play a crucial role in various applications, such as displays, sensors, and solar cells. Effective fault detection and diagnosis of the ITO electrodes are essential to ensure the performance and reliability of the devices. However, traditional visual inspection is challenging with transparent ITO electrodes, and existing fault detection methods have limitations in determining the root causes of the defects, often requiring destructive evaluations. In this study, an in situ fault diagnosis method is proposed using scattering parameter (S-parameter) signal processing, offering early detection, high diagnostic accuracy, noise robustness, and root cause analysis. A comprehensive S-parameter pattern database is obtained according to defect states. Deep learning (DL) approaches, including multilayer perceptron (MLP), convolutional neural network (CNN), and transformer, are then used to simultaneously analyze the cause and sev
    
[^120]: 通过机器学习评估物联网数据信任

    IoT Data Trust Evaluation via Machine Learning. (arXiv:2308.11638v1 [eess.SP])

    [http://arxiv.org/abs/2308.11638](http://arxiv.org/abs/2308.11638)

    通过随机游走补全（RWI）方法合成可信和不可信的物联网时间序列数据集，并提取特征用于开发和验证物联网数据信任评估的机器学习模型。

    

    针对评估物联网数据信任，提出了基于监督或无监督机器学习的各种方法。但是，由于缺乏相关的公开可用数据集，很难评估这些方法的实际有效性。因此，提出了一种数据合成方法称为随机游走补全（RWI），通过从现有的可信数据合成不可信数据，以增强物联网时间序列数据集。同时，从物联网时间序列传感器数据中提取新特征，有效捕捉其自相关性以及与邻近（对等）传感器数据的交叉相关性。这些特征可以用于学习机器学习模型，以识别物联网传感器数据的可信度。通过合成的基准标记数据集和信息丰富的相关特征，开发和验证物联网数据信任评估的机器学习模型。

    Various approaches based on supervised or unsupervised machine learning (ML) have been proposed for evaluating IoT data trust. However, assessing their real-world efficacy is hard mainly due to the lack of related publicly-available datasets that can be used for benchmarking. Since obtaining such datasets is challenging, we propose a data synthesis method, called random walk infilling (RWI), to augment IoT time-series datasets by synthesizing untrustworthy data from existing trustworthy data. Thus, RWI enables us to create labeled datasets that can be used to develop and validate ML models for IoT data trust evaluation. We also extract new features from IoT time-series sensor data that effectively capture its auto-correlation as well as its cross-correlation with the data of the neighboring (peer) sensors. These features can be used to learn ML models for recognizing the trustworthiness of IoT sensor data. Equipped with our synthesized ground-truth-labeled datasets and informative corr
    
[^121]: 通过联邦学习聚合内在信息提升BCI性能

    Aggregating Intrinsic Information to Enhance BCI Performance through Federated Learning. (arXiv:2308.11636v1 [eess.SP])

    [http://arxiv.org/abs/2308.11636](http://arxiv.org/abs/2308.11636)

    提出了一个层次化的个性化联邦学习脑电图解码框架（FLEEG），通过在模型训练过程中使具有不同数据格式的数据集进行合作，克服了脑机接口（BCI）面临的数据不足挑战。

    

    对于构建高性能深度学习模型，数据不足是脑机接口（BCI）面临的长期挑战。尽管许多研究团队和机构为同一个BCI任务收集了大量的脑电图数据集，但由于设备的异质性，共享来自多个站点的脑电图数据仍然具有挑战性。数据多样性在促进模型的鲁棒性方面具有重要作用，因此这个挑战的重要性不可低估。然而，现有的研究很少讨论这个问题，主要关注单个数据集内模型训练，通常是在不同受试者或不同会话设置下。本文提出了一个层次化的个性化联邦学习脑电图解码框架（FLEEG），以克服这个挑战。这种创新的框架为BCI带来了一种新的学习范式，使具有不同数据格式的数据集可以在模型训练过程中合作。每个客户端被分配一个特定的数据集和tr

    Insufficient data is a long-standing challenge for Brain-Computer Interface (BCI) to build a high-performance deep learning model. Though numerous research groups and institutes collect a multitude of EEG datasets for the same BCI task, sharing EEG data from multiple sites is still challenging due to the heterogeneity of devices. The significance of this challenge cannot be overstated, given the critical role of data diversity in fostering model robustness. However, existing works rarely discuss this issue, predominantly centering their attention on model training within a single dataset, often in the context of inter-subject or inter-session settings. In this work, we propose a hierarchical personalized Federated Learning EEG decoding (FLEEG) framework to surmount this challenge. This innovative framework heralds a new learning paradigm for BCI, enabling datasets with disparate data formats to collaborate in the model training process. Each client is assigned a specific dataset and tr
    
[^122]: 半监督双流自注意对抗图对比学习在基于跨主体脑电情绪识别中的应用

    Semi-Supervised Dual-Stream Self-Attentive Adversarial Graph Contrastive Learning for Cross-Subject EEG-based Emotion Recognition. (arXiv:2308.11635v1 [eess.SP])

    [http://arxiv.org/abs/2308.11635](http://arxiv.org/abs/2308.11635)

    本论文提出了一种半监督双流自注意对抗图对比学习框架 DS-AGC，用于跨主体脑电情绪识别中的标记数据不足问题。该框架利用两个并行流提取非结构化和结构化脑电特征，并通过半监督方法解决分布差异和提取有效的基于图的特征表示。

    

    脑电图 (EEG) 是一种有着广泛应用前景的客观情绪识别工具。然而，标记数据的稀缺性仍然是该领域的主要挑战，限制了基于脑电的情绪识别的广泛应用。本文提出了一种半监督双流自注意对抗图对比学习框架 (简称为 DS-AGC)，用于解决基于跨主体脑电情绪识别中有限标记数据的挑战。DS-AGC 框架包括两个并行流，用于提取非结构化和结构化脑电特征。非结构化流采用半监督多领域适应方法，以缓解标记源域、未标记源域和未知目标域之间的分布差异。结构化流则开发了一种图对比学习方法，以半监督方式从多个脑电通道中提取有效的基于图的特征表示。此外，一种自注意

    Electroencephalography (EEG) is an objective tool for emotion recognition with promising applications. However, the scarcity of labeled data remains a major challenge in this field, limiting the widespread use of EEG-based emotion recognition. In this paper, a semi-supervised Dual-stream Self-Attentive Adversarial Graph Contrastive learning framework (termed as DS-AGC) is proposed to tackle the challenge of limited labeled data in cross-subject EEG-based emotion recognition. The DS-AGC framework includes two parallel streams for extracting non-structural and structural EEG features. The non-structural stream incorporates a semi-supervised multi-domain adaptation method to alleviate distribution discrepancy among labeled source domain, unlabeled source domain, and unknown target domain. The structural stream develops a graph contrastive learning method to extract effective graph-based feature representation from multiple EEG channels in a semi-supervised manner. Further, a self-attentiv
    
[^123]: 自监督学习在合成孔径声纳数据处理、分类和模式识别中的进展

    Advances in Self-Supervised Learning for Synthetic Aperture Sonar Data Processing, Classification, and Pattern Recognition. (arXiv:2308.11633v1 [eess.SP])

    [http://arxiv.org/abs/2308.11633](http://arxiv.org/abs/2308.11633)

    本文提出了MoCo-SAS，利用自监督学习（SSL）进行合成孔径声纳（SAS）数据处理、分类和模式识别。实验证明MoCo-SAS显著优于传统有监督学习方法，为增强水下物体检测和分类提供了有希望的途径。

    

    合成孔径声纳（SAS）成像因其在增加距离时保持分辨率的独特能力而成为水下探测的关键技术，这是传统声纳技术所没有的特征。然而，由于标记数据的稀缺性，深度学习在SAS数据处理中的有效应用通常受到限制。为了应对这一挑战，本文提出了MoCo-SAS，利用自监督学习（SSL）进行SAS数据处理、分类和模式识别。实验结果表明，MoCo-SAS在F1-score方面显著优于传统的有监督学习方法，这些发现凸显了SSL在推动SAS数据处理的最新进展方面的潜力，为增强水下物体检测和分类提供了有希望的途径。

    Synthetic Aperture Sonar (SAS) imaging has become a crucial technology for underwater exploration because of its unique ability to maintain resolution at increasing ranges, a characteristic absent in conventional sonar techniques. However, the effective application of deep learning to SAS data processing is often limited due to the scarcity of labeled data. To address this challenge, this paper proposes MoCo-SAS that leverages self-supervised learning (SSL) for SAS data processing, classification, and pattern recognition. The experimental results demonstrate that MoCo-SAS significantly outperforms traditional supervised learning methods, as evidenced by significant improvements observed in terms of the F1-score. These findings highlight the potential of SSL in advancing the state-of-the-art in SAS data processing, offering promising avenues for enhanced underwater object detection and classification.
    
[^124]: 基于深度学习的水电站管理流量分解

    Deep learning-based flow disaggregation for hydropower plant management. (arXiv:2308.11631v1 [eess.SP])

    [http://arxiv.org/abs/2308.11631](http://arxiv.org/abs/2308.11631)

    本研究提出了一种基于深度学习的时间序列分解模型，用于将每日流量分解为每小时流量，并在挪威某流量测站的数据上进行了测试，初步结果显示了该模型的一些有希望的方面。

    

    高时空分辨率数据对于水电站管理至关重要。目前，大部分挪威水电站只有每日分辨率的数据，然而，为了实现更精确的管理，通常需要亚日分辨率的数据。为了解决亚日数据的普遍缺失，时间序列分解是一种潜在的工具。在本研究中，我们提出了一个基于深度学习的时间序列分解模型，该模型使用挪威某流量测站的数据进行测试，将每日流量分解为每小时流量。初步结果显示了该模型的一些有希望的方面。

    High temporal resolution data is a vital resource for hydropower plant management. Currently, only daily resolution data are available for most of Norwegian hydropower plant, however, to achieve more accurate management, sub-daily resolution data are often required. To deal with the wide absence of sub-daily data, time series disaggregation is a potential tool. In this study, we proposed a time series disaggregation model based on deep learning, the model is tested using flow data from a Norwegian flow station, to disaggregate the daily flow into hourly flow. Preliminary results show some promising aspects for the proposed model.
    
[^125]: 使用迁移学习解决光学矩阵乘法模型中的数据稀缺问题

    Addressing Data Scarcity in Optical Matrix Multiplier Modeling Using Transfer Learning. (arXiv:2308.11630v1 [cs.LG])

    [http://arxiv.org/abs/2308.11630](http://arxiv.org/abs/2308.11630)

    本研究使用迁移学习解决了光学矩阵乘法模型训练中的数据稀缺问题，并通过利用合成数据进行预训练和实验数据进行微调，成功减小了建模误差，在仅使用25%数据的情况下实现对矩阵权重的小于1 dB的均方根误差。

    

    我们提出并通过实验证明使用迁移学习可以解决训练Mach-Zehnder干涉仪网状光学矩阵乘法器的神经网络模型时面临的实验数据稀缺问题。我们的方法包括使用从不太准确的解析模型生成的合成数据进行预训练，并使用实验数据进行微调。我们的研究表明，与使用解析模型或独立神经网络模型相比，这种方法可以显著减小建模误差。通过利用正则化技术和集成平均，我们在仅使用25%可用数据的情况下，实现了对由光子芯片实现的矩阵权重的小于1 dB的均方根误差。

    We present and experimentally evaluate using transfer learning to address experimental data scarcity when training neural network (NN) models for Mach-Zehnder interferometer mesh-based optical matrix multipliers. Our approach involves pre-training the model using synthetic data generated from a less accurate analytical model and fine-tuning with experimental data. Our investigation demonstrates that this method yields significant reductions in modeling errors compared to using an analytical model, or a standalone NN model when training data is limited. Utilizing regularization techniques and ensemble averaging, we achieve < 1 dB root-mean-square error on the matrix weights implemented by a photonic chip while using only 25% of the available data.
    
[^126]: 用通用设备编码和图注意力网络革新TCAD模拟

    Revolutionizing TCAD Simulations with Universal Device Encoding and Graph Attention Networks. (arXiv:2308.11624v1 [cs.LG])

    [http://arxiv.org/abs/2308.11624](http://arxiv.org/abs/2308.11624)

    本论文提出了一种利用人工智能和图表示技术对TCAD器件模拟中的半导体器件进行编码的创新方法，通过引入图注意力网络和通用编码方案，实现了全面的数据驱动建模，为研究人员提供了在设备级上应用基于人工智能的电子设计自动化解决方案的可能性。

    

    本论文提出了一种创新方法，利用人工智能和图表示技术来对TCAD器件模拟中的半导体器件进行编码。提出了一种基于图的通用编码方案，不仅考虑了材料级和器件级嵌入，还引入了一种新颖的基于空间关系的嵌入，受有限元网格中常用的插值操作启发而来。利用器件模拟的通用物理定律进行全面的数据驱动建模，包括基于泊松仿真的替代和基于漂移扩散模型的电流-电压（IV）预测。这两者都是使用一种新颖的图注意力网络（称为RelGAT）实现的。论文还提供了基于Sentaurus TCAD器件模拟器的详细技术细节，使研究人员可以在设备级上采用提出的基于人工智能的电子设计自动化解决方案。

    An innovative methodology that leverages artificial intelligence (AI) and graph representation for semiconductor device encoding in TCAD device simulation is proposed. A graph-based universal encoding scheme is presented that not only considers material-level and device-level embeddings, but also introduces a novel spatial relationship embedding inspired by interpolation operations typically used in finite element meshing. Universal physical laws from device simulations are leveraged for comprehensive data-driven modeling, which encompasses surrogate Poisson emulation and current-voltage (IV) prediction based on drift-diffusion model. Both are achieved using a novel graph attention network, referred to as RelGAT. Comprehensive technical details based on the device simulator Sentaurus TCAD are presented, empowering researchers to adopt the proposed AI-driven Electronic Design Automation (EDA) solution at the device level.
    
[^127]: Tryage: 实时智能路由用户提示到大型语言模型

    Tryage: Real-time, intelligent Routing of User Prompts to Large Language Model. (arXiv:2308.11601v1 [cs.LG])

    [http://arxiv.org/abs/2308.11601](http://arxiv.org/abs/2308.11601)

    Tryage是一个上下文感知的路由系统，能够根据对个体输入提示的分析，从模型库中选择最佳的专家模型，以消除模型选择和定制化的负担，释放庞大的新兴模型库的巨大威力给最终用户。

    

    变压器架构和自注意机制的引入导致了在特定下游任务和数据领域训练的语言模型的爆炸性增长。在Hugging Face生态系统中有超过200,000个模型，用户在选择和优化模型以适应多方面的工作流程和数据领域的同时，还要解决计算、安全和时效性等问题。迫切需要机器学习框架来消除模型选择和定制化的负担，并释放庞大的新兴模型库的巨大威力给最终用户。在这里，我们提出了一个上下文感知的路由系统Tryage，它利用语言模型路由器根据对个体输入提示的分析，从模型库中选择最佳的专家模型。受大脑中的丘脑路由器启发，Tryage采用感知路由器来预测下游模型在提示上的性能，并根据目标做出路由决策。

    The introduction of the transformer architecture and the self-attention mechanism has led to an explosive production of language models trained on specific downstream tasks and data domains. With over 200, 000 models in the Hugging Face ecosystem, users grapple with selecting and optimizing models to suit multifaceted workflows and data domains while addressing computational, security, and recency concerns. There is an urgent need for machine learning frameworks that can eliminate the burden of model selection and customization and unleash the incredible power of the vast emerging model library for end users. Here, we propose a context-aware routing system, Tryage, that leverages a language model router for optimal selection of expert models from a model library based on analysis of individual input prompts. Inspired by the thalamic router in the brain, Tryage employs a perceptive router to predict down-stream model performance on prompts and, then, makes a routing decision using an ob
    
[^128]: 设计一款攻防游戏：通过竞争来增加金融交易模型的鲁棒性

    Designing an attack-defense game: how to increase robustness of financial transaction models via a competition. (arXiv:2308.11406v1 [cs.LG])

    [http://arxiv.org/abs/2308.11406](http://arxiv.org/abs/2308.11406)

    通过设计一款攻防游戏，我们研究了使用序列金融数据的神经网络模型的对抗攻击和防御的现状和动态，并且通过分析比赛动态，回答了隐藏模型免受恶意用户攻击的重要性以及需要多长时间才能破解模型的问题。

    

    鉴于金融领域恶意攻击风险不断升级和由此引发的严重损害，对机器学习模型的对抗策略和鲁棒的防御机制有深入的理解至关重要。随着银行日益广泛采用更精确但潜在脆弱的神经网络，这一威胁变得更加严重。我们旨在调查使用序列金融数据作为输入的神经网络模型的对抗攻击和防御的当前状态和动态。为了实现这一目标，我们设计了一个比赛，允许对现代金融交易数据中的问题进行逼真而详细的研究。参与者直接竞争，因此可能的攻击和防御在接近真实条件下进行了检验。我们的主要贡献是分析比赛动态，回答了隐藏模型免受恶意用户攻击的重要性以及需要多长时间才能破解模型的问题。

    Given the escalating risks of malicious attacks in the finance sector and the consequential severe damage, a thorough understanding of adversarial strategies and robust defense mechanisms for machine learning models is critical. The threat becomes even more severe with the increased adoption in banks more accurate, but potentially fragile neural networks. We aim to investigate the current state and dynamics of adversarial attacks and defenses for neural network models that use sequential financial data as the input.  To achieve this goal, we have designed a competition that allows realistic and detailed investigation of problems in modern financial transaction data. The participants compete directly against each other, so possible attacks and defenses are examined in close-to-real-life conditions. Our main contributions are the analysis of the competition dynamics that answers the questions on how important it is to conceal a model from malicious users, how long does it take to break i
    
[^129]: 一个有效的基于Transformer的上下文模型和时间门池化用于说话人识别

    An Effective Transformer-based Contextual Model and Temporal Gate Pooling for Speaker Identification. (arXiv:2308.11241v1 [cs.SD])

    [http://arxiv.org/abs/2308.11241](http://arxiv.org/abs/2308.11241)

    本文介绍了一种基于Transformer的上下文模型和时间门池化的有效方法，应用于说话人识别，并在准确率85.9%的情况下比较了其性能与wav2vec2方法。

    

    Wav2vec2在语音识别中应用Transformer架构和自监督学习取得了成功。最近，这些方法不仅用于语音识别，还用于整个语音处理。本文介绍了一种应用了基于Transformer的上下文模型的有效端到端说话人识别模型。我们探索了参数与性能之间的关系，以确定一个有效模型的结构。此外，我们提出了一种具有强大学习能力的池化方法，称为时间门池化(Temporal Gate Pooling)，用于说话人识别。我们将Conformer作为编码器，并利用BEST-RQ进行预训练，并使用VoxCeleb1的说话人识别进行了评估。该方法在仅有28.5M个参数的情况下，实现了85.9%的准确率，与具有317.7M个参数的wav2vec2相当。代码可在https://github.com/HarunoriKawano/speaker-identification-with-tgp获得。

    Wav2vec2 has achieved success in applying Transformer architecture and self-supervised learning to speech recognition. Recently, these have come to be used not only for speech recognition but also for the entire speech processing. This paper introduces an effective end-to-end speaker identification model applied Transformer-based contextual model. We explored the relationship between the parameters and the performance in order to discern the structure of an effective model. Furthermore, we propose a pooling method, Temporal Gate Pooling, with powerful learning ability for speaker identification. We applied Conformer as encoder and BEST-RQ for pre-training and conducted an evaluation utilizing the speaker identification of VoxCeleb1. The proposed method has achieved an accuracy of 85.9% with 28.5M parameters, demonstrating comparable precision to wav2vec2 with 317.7M parameters. Code is available at https://github.com/HarunoriKawano/speaker-identification-with-tgp.
    
[^130]: 大模型时代中的联邦学习：针对特定领域的多模态大模型

    Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models. (arXiv:2308.11217v1 [cs.LG])

    [http://arxiv.org/abs/2308.11217](http://arxiv.org/abs/2308.11217)

    本论文提出了一种多模态联邦学习框架，利用私有领域数据协同训练大型模型，以实现跨场景的智能服务。在大模型时代，该框架解决了异构数据、模型聚合、性能和成本权衡、数据隐私以及激励机制等方面的挑战。

    

    多模态数据能够全面感知和识别物理世界，已成为通往通用人工智能的重要路径。然而，在公共数据集上训练的多模态大模型在特定工业领域的性能往往不理想。本文提出了一种多模态联邦学习框架，可以使多个企业利用私有领域数据协同训练大型模型，实现跨场景的智能服务。作者深入探讨了大模型时代联邦学习的智能基础和目标的战略转变，以及在异构数据、模型聚合、性能和成本权衡、数据隐私和激励机制方面面临的新挑战。本文详细介绍了领先企业在城市安全运营管理方面贡献多模态数据和专家知识的案例研究，包括分布式部署和高效性能的实现。

    Multimodal data, which can comprehensively perceive and recognize the physical world, has become an essential path towards general artificial intelligence. However, multimodal large models trained on public datasets often underperform in specific industrial domains. This paper proposes a multimodal federated learning framework that enables multiple enterprises to utilize private domain data to collaboratively train large models for vertical domains, achieving intelligent services across scenarios. The authors discuss in-depth the strategic transformation of federated learning in terms of intelligence foundation and objectives in the era of big model, as well as the new challenges faced in heterogeneous data, model aggregation, performance and cost trade-off, data privacy, and incentive mechanism. The paper elaborates a case study of leading enterprises contributing multimodal data and expert knowledge to city safety operation management , including distributed deployment and efficient 
    
[^131]: 基础模型导向的稳健性: 通过预训练模型对稳健性图像模型进行评估

    Foundation Model-oriented Robustness: Robust Image Model Evaluation with Pretrained Models. (arXiv:2308.10632v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2308.10632](http://arxiv.org/abs/2308.10632)

    本文提出了一种通过预训练模型对稳健性图像模型进行评估的新方法，通过与基础模型进行比较，直接测量图像分类模型的性能，扩展图像数据集以完成超出基准范围的评估。

    

    机器学习在有限数据集上表现出了出色的性能，然而，固定基准上的分数是否足以充分体现模型在真实世界中的性能仍有讨论。实际上，理想的稳健模型可能与神谕（例如，人类用户）表现类似，因此一个好的评估协议可能是评估模型相对于神谕的行为。本文介绍了一种新的稳健性测量方法，直接测量图像分类模型相对于替代神谕（即基础模型）的性能。此外，我们设计了一种简单的方法，可以在基准范围之外完成评估。我们的方法通过添加具有足够扰动的新样本来扩展图像数据集，这些样本与原始集合中的样本有所不同，但仍限制在原始测试图像所代表的相同图像-标签结构内，由预训练的基础模型限定。

    Machine learning has demonstrated remarkable performance over finite datasets, yet whether the scores over the fixed benchmarks can sufficiently indicate the model's performance in the real world is still in discussion. In reality, an ideal robust model will probably behave similarly to the oracle (e.g., the human users), thus a good evaluation protocol is probably to evaluate the models' behaviors in comparison to the oracle. In this paper, we introduce a new robustness measurement that directly measures the image classification model's performance compared with a surrogate oracle (i.e., a foundation model). Besides, we design a simple method that can accomplish the evaluation beyond the scope of the benchmarks. Our method extends the image datasets with new samples that are sufficiently perturbed to be distinct from the ones in the original sets, but are still bounded within the same image-label structure the original test image represents, constrained by a foundation model pretraine
    
[^132]: 信息理论引导的启发式渐进式多视图编码

    Information Theory-Guided Heuristic Progressive Multi-View Coding. (arXiv:2308.10522v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2308.10522](http://arxiv.org/abs/2308.10522)

    通过信息理论提出了一个新的多视图学习框架，并基于此框架构建了一种渐进式多视图编码方法。

    

    多视图表示学习旨在从多个视图中捕获共享上下文的全面信息。最近的研究直观地将对比学习应用于不同视图的成对方式，这仍然可扩展：学习视图共享表示时未过滤视图特定的噪声；虚假的负对中，负项实际上在同一类别中与正项相同，并且真正的负对被同等对待；均匀地衡量术语之间的相似性可能干扰优化。重要的是，很少有工作研究广义自监督多视图学习的理论框架，特别是针对多于两个视图的情况。为此，我们从信息理论的视角重新思考了现有的多视图学习范式，然后提出了一个新的信息理论框架，用于广义多视图学习。在其指导下，我们构建了一个具有三层渐进式结构的多视图编码方法，即In

    Multi-view representation learning aims to capture comprehensive information from multiple views of a shared context. Recent works intuitively apply contrastive learning to different views in a pairwise manner, which is still scalable: view-specific noise is not filtered in learning view-shared representations; the fake negative pairs, where the negative terms are actually within the same class as the positive, and the real negative pairs are coequally treated; evenly measuring the similarities between terms might interfere with optimization. Importantly, few works study the theoretical framework of generalized self-supervised multi-view learning, especially for more than two views. To this end, we rethink the existing multi-view learning paradigm from the perspective of information theory and then propose a novel information theoretical framework for generalized multi-view learning. Guided by it, we build a multi-view coding method with a three-tier progressive architecture, namely In
    
[^133]: 极简交通预测：只需线性层。

    Minimalist Traffic Prediction: Linear Layer Is All You Need. (arXiv:2308.10276v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.10276](http://arxiv.org/abs/2308.10276)

    本文提出了一种极简的交通预测模型STLinear，通过节点嵌入方法、时间序列分解和周期性学习解决了空间-时间图神经网络（STGNN）的计算复杂性和资源密集性问题，该方法只使用线性层，在准确度上与领先的STGNN相匹敌甚至超过，计算需求显著降低。

    

    交通预测对于智能交通系统（ITS）和智能城市的发展至关重要。虽然空间-时间图神经网络（STGNN）通过融合循环神经网络（RNN）或Transformer与图神经网络（GNN）展示了在这个领域的潜力，但面临着计算复杂性、梯度问题和资源密集性的挑战。本文通过提出节点嵌入方法、时间序列分解和周期性学习来应对这些挑战。我们引入了STLinear，这是一种旨在优化效率和性能的极简模型结构。与传统的STGNN不同，STLinear完全在本地运行，避免了节点间的数据交换，仅依赖线性层，从而大大降低了计算需求。我们在真实数据集上的实证研究证实了STLinear的卓越性能，达到或超过了领先的STGNN的准确度，但计算需求显著减少。

    Traffic prediction is essential for the progression of Intelligent Transportation Systems (ITS) and the vision of smart cities. While Spatial-Temporal Graph Neural Networks (STGNNs) have shown promise in this domain by leveraging Graph Neural Networks (GNNs) integrated with either RNNs or Transformers, they present challenges such as computational complexity, gradient issues, and resource-intensiveness. This paper addresses these challenges, advocating for three main solutions: a node-embedding approach, time series decomposition, and periodicity learning. We introduce STLinear, a minimalist model architecture designed for optimized efficiency and performance. Unlike traditional STGNNs, STlinear operates fully locally, avoiding inter-node data exchanges, and relies exclusively on linear layers, drastically cutting computational demands. Our empirical studies on real-world datasets confirm STLinear's prowess, matching or exceeding the accuracy of leading STGNNs, but with significantly r
    
[^134]: 在大型语言模型中使用反向推理进行验证

    Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])

    [http://arxiv.org/abs/2308.07758](http://arxiv.org/abs/2308.07758)

    本文研究了在大型语言模型中使用反向推理进行验证的方法。作者提出了一种新颖的技术，通过屏蔽问题中的一个标记，并要求语言模型预测被屏蔽的标记来验证候选答案。同时，作者还提出了一种结合正向和反向推理的方法来估计候选答案的概率。

    

    链式思考（Chain-of-Though, CoT）提示在各种推理任务中表现出了很好的性能。最近，Self-Consistency提出了一种方法，即通过采样一组不同的推理链，这些链可能导致不同的答案，然后选择得票最多的答案。本文提出了一种新颖的方法，即在验证候选答案时使用反向推理。我们使用一个简单的模板，即``如果我们知道上述问题的答案是候选答案，那么未知变量x的值是多少？''，将问题中的一个标记屏蔽，并要求语言模型预测被屏蔽的标记。直观上讲，如果提供的候选答案是正确的，语言模型应该能够成功预测被屏蔽的标记。我们进一步提出了FOBAR方法，将正向和反向推理结合起来估计候选答案的概率。我们在六个数据集和三个实验中进行了广泛的实验。

    Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., ``\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three
    
[^135]: AudioFormer: 通过离散的声学代码学习音频特征表示的音频变换器

    AudioFormer: Audio Transformer learns audio feature representations from discrete acoustic codes. (arXiv:2308.07221v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2308.07221](http://arxiv.org/abs/2308.07221)

    AudioFormer是一种学习音频特征表示的方法，通过生成离散的声学代码并利用它们来训练掩码语言模型，从而将音频分类任务视为自然语言理解的形式。此外，引入了多正样本对比学习方法，通过学习联合表示来捕捉音频中的相关性。

    

    我们提出了一种名为AudioFormer的方法，通过获取离散的声学代码来学习音频特征表示，并随后对其进行微调以用于音频分类任务。我们首先将音频分类任务视为一种自然语言理解 (NLU) 的形式，借助现有的神经音频编解码模型，我们生成了离散的声学代码，并利用它们来训练一个掩码语言模型 (MLM)，从而获得音频特征表示。此外，我们首创了一种多正样本对比 (MPC) 学习方法的整合，该方法能够学习同一音频输入中多个离散声学代码间的联合表示。在实验中，我们将离散的声学代码视为文本数据，并使用类似填空题的方法训练一个掩码语言模型，最终得到高质量的音频表示。值得注意的是，MPC学习技术能够有效捕捉到音频中的相关性。

    We propose a method named AudioFormer,which learns audio feature representations through the acquisition of discrete acoustic codes and subsequently fine-tunes them for audio classification tasks. Initially,we introduce a novel perspective by considering the audio classification task as a form of natural language understanding (NLU). Leveraging an existing neural audio codec model,we generate discrete acoustic codes and utilize them to train a masked language model (MLM),thereby obtaining audio feature representations. Furthermore,we pioneer the integration of a Multi-Positive sample Contrastive (MPC) learning approach. This method enables the learning of joint representations among multiple discrete acoustic codes within the same audio input. In our experiments,we treat discrete acoustic codes as textual data and train a masked language model using a cloze-like methodology,ultimately deriving high-quality audio representations. Notably,the MPC learning technique effectively captures c
    
[^136]: 通过逐步蒸馏加速基于扩散的组合优化求解器

    Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation. (arXiv:2308.06644v1 [cs.LG])

    [http://arxiv.org/abs/2308.06644](http://arxiv.org/abs/2308.06644)

    本文提出使用逐步蒸馏来加速基于扩散的组合优化求解器，并在TSP-50数据集上展示了16倍的推理速度提升，仅有0.019%的性能降级。

    

    基于图扩散模型在生成高质量解决方案的NP完全组合优化问题方面表现出了良好的结果。然而，由于去噪扩散过程的迭代评估特性，这些模型在推理上常常效率低下。本文提出使用逐步蒸馏来加速推理过程，仅在单步内预测两个步骤之前的情况。我们的实验结果表明，经过逐步蒸馏的模型可以在TSP-50数据集上进行推理，速度快了16倍，性能仅有0.019%的降级。

    Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset.
    
[^137]: 关于最先进生成模型的可信度景观：一项综合调查

    On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey. (arXiv:2307.16680v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.16680](http://arxiv.org/abs/2307.16680)

    本文综合调查了大规模生成模型的可信度问题，涵盖了隐私、安全、公平性和责任等多个维度，并提出了实际建议和未来发展方向。

    

    扩散模型和大规模语言模型已经成为领先的生成模型，并对人类生活的各个方面产生了革命性的影响。然而，这些模型的实际应用也暴露出固有的风险，突显了它们的双重性质，并引发了对它们可信度的担忧。尽管有大量关于这个主题的文献，但针对大规模生成模型及其可信度的综合调查仍然很少见。为了弥补这一空白，本文调查了涉及这些模型的长期和新兴威胁，涵盖了隐私、安全、公平和责任这四个基本维度。通过这种方式，我们构建了一张详尽的地图，概述了这些模型的可信度，并提供了实际建议和未来的发展方向。这些努力对于促进这些模型的可信度部署至关重要。

    Diffusion models and large language models have emerged as leading-edge generative models and have sparked a revolutionary impact on various aspects of human life. However, the practical implementation of these models has also exposed inherent risks, highlighting their dual nature and raising concerns regarding their trustworthiness. Despite the abundance of literature on this subject, a comprehensive survey specifically delving into the intersection of large-scale generative models and their trustworthiness remains largely absent. To bridge this gap, This paper investigates both the long-standing and emerging threats associated with these models across four fundamental dimensions: privacy, security, fairness, and responsibility. In this way, we construct an extensive map outlining the trustworthiness of these models, while also providing practical recommendations and identifying future directions. These efforts are crucial for promoting the trustworthy deployment of these models, ulti
    
[^138]: 自洽性方法用于无限生成问题的改进

    Self-consistency for open-ended generations. (arXiv:2307.06857v1 [cs.AI])

    [http://arxiv.org/abs/2307.06857](http://arxiv.org/abs/2307.06857)

    本论文提出了一种改进大规模预训练语言模型生成输出质量和一致性的新方法，通过扩展自洽性框架的适用性，实现了从一个候选集中恢复最优或接近最优的生成结果，并提出了一种轻量级无参数相似性函数来改进代码生成、自动形式化和摘要任务的效果。

    

    在这篇论文中，我们提出了一种改进大规模预训练语言模型生成输出的质量和一致性的新方法。自洽性已经被证明是一种有效的方法，对于具有固定答案的提示，选择得票最多的答案。我们引入了一个推广的自洽性框架，扩展了其适用性，超越了固定答案问题的范围。通过大量的模拟实验，我们证明了我们的方法能够从候选集中恢复最优或接近最优的生成结果。我们还提出了一种轻量级无参数相似性函数，即使没有访问到标记的概率，也能在代码生成、自动形式化和摘要任务中显著和一致地改进效果。我们的方法几乎没有计算开销，不需要额外的再排序模型或对现有模型的修改。

    In this paper, we present a novel approach for improving the quality and consistency of generated outputs from large-scale pre-trained language models (LLMs). Self-consistency has emerged as an effective approach for prompts with fixed answers, selecting the answer with the highest number of votes. In this paper, we introduce a generalized framework for self-consistency that extends its applicability beyond problems that have fixed-answer answers. Through extensive simulations, we demonstrate that our approach consistently recovers the optimal or near-optimal generation from a set of candidates. We also propose lightweight parameter-free similarity functions that show significant and consistent improvements across code generation, autoformalization, and summarization tasks, even without access to token log probabilities. Our method incurs minimal computational overhead, requiring no auxiliary reranker models or modifications to the existing model.
    
[^139]: 一种解决奇点问题的机器学习方法

    An ML approach to resolution of singularities. (arXiv:2307.00252v1 [cs.LG])

    [http://arxiv.org/abs/2307.00252](http://arxiv.org/abs/2307.00252)

    该论文介绍了一种新的机器学习方法，使用强化学习代理来解决奇点问题中的最优解。实验证明在多项式相加的总数方面，该方法超过了当前最先进的选择启发式算法，展示了近期研究的潜力。

    

    多项式方程组的解集通常包含不光滑、奇异的点。解决奇点是几何中的基本过程，我们将奇点替换为光滑点，同时保持解集的剩余部分不变。解决奇点并不是唯一的：通常的方法是反复进行被称为“blowing-up”的基本操作，解决的复杂性高度依赖于某些选择。这个过程可以转化成不同版本的两人博弈，即所谓的Hironaka游戏，而第一位玩家的获胜策略提供了解决奇点问题的解。本文介绍了一种新的Hironaka游戏方法，使用强化学习代理来寻找奇点的最优解。在某些领域中，训练模型在多项式相加的总数方面优于最先进的选择启发式算法，这证明了最近发展的潜力。

    The solution set of a system of polynomial equations typically contains ill-behaved, singular points. Resolution is a fundamental process in geometry in which we replace singular points with smooth points, while keeping the rest of the solution set unchanged. Resolutions are not unique: the usual way to describe them involves repeatedly performing a fundamental operation known as "blowing-up", and the complexity of the resolution highly depends on certain choices. The process can be translated into various versions of a 2-player game, the so-called Hironaka game, and a winning strategy for the first player provides a solution to the resolution problem. In this paper we introduce a new approach to the Hironaka game that uses reinforcement learning agents to find optimal resolutions of singularities. In certain domains, the trained model outperforms state-of-the-art selection heuristics in total number of polynomial additions performed, which provides a proof-of-concept that recent devel
    
[^140]: 自动分配和分类软件问题

    Automated Assignment and Classification of Software Issues. (arXiv:2307.00009v1 [cs.CL])

    [http://arxiv.org/abs/2307.00009](http://arxiv.org/abs/2307.00009)

    本论文提出了一种自动分配和分类软件问题的方法。通过使用经过精心策划的语言特征和不同的机器学习方法，将问题分配给最相关的团队成员，并将其分类为不同的类别，以提高工作效率和准确性。

    

    软件问题包含修复、改进或创建新线程的工作单元，在开发过程中促进团队成员之间的沟通。将问题分配给最相关的团队成员并确定问题的类别是一项繁琐且具有挑战性的任务。错误的分类会导致项目延迟和重新工作，给团队成员带来麻烦。本文提出了一组经过精心策划的用于浅层机器学习方法的语言特征，并将浅层方法和集成方法与深度语言模型的性能进行了比较。与现有技术不同的是，我们将问题分配给四种角色（设计师、开发人员、测试人员和领导者），而不是特定的个人或团队，以促进我们解决方案的普遍性。我们还考虑开发人员的经验水平，以反映我们解决方案的工业实践。我们采用分类方法将问题分类为不同的类别，包括错误、新功能、改进等。

    Software issues contain units of work to fix, improve or create new threads during the development and facilitate communication among the team members. Assigning an issue to the most relevant team member and determining a category of an issue is a tedious and challenging task. Wrong classifications cause delays and rework in the project and trouble among the team members. This thesis proposes a set of carefully curated linguistic features for shallow machine learning methods and compares the performance of shallow and ensemble methods with deep language models. Unlike the state-of-the-art, we assign issues to four roles (designer, developer, tester, and leader) rather than to specific individuals or teams to contribute to the generality of our solution. We also consider the level of experience of the developers to reflect the industrial practices in our solution formulation. We employ a classification approach to categorize issues into distinct classes, namely bug, new feature, improve
    
[^141]: UTRNet: 印刷文档中高分辨率乌尔都文本识别

    UTRNet: High-Resolution Urdu Text Recognition In Printed Documents. (arXiv:2306.15782v1 [cs.CV])

    [http://arxiv.org/abs/2306.15782](http://arxiv.org/abs/2306.15782)

    本文提出了一种解决印刷乌尔都文本识别挑战的新方法，并引入了大规模实际标记数据集和合成数据集，提供了乌尔都文本行检测的基准数据集，同时开发了一个在线工具，实现了印刷文档中乌尔都OCR的端到端识别。

    

    本文提出了一种新颖方法来解决印刷乌尔都文本识别的挑战，使用高分辨率、多尺度的语义特征提取。我们提出的UTRNet架构，一个混合CNN-RNN模型，在基准数据集上展示了最先进的性能。为了解决以前工作的局限性，这些工作很难推广到乌尔都文本的复杂性和缺乏足够的实际标记数据，我们引入了UTRSet-Real，一个包含超过11,000行的大规模实际标记数据集和UTRSet-Synth，一个与实际世界非常相似的含有20,000行的合成数据集，并对现有的IIITH数据集的基准真实性进行了修正，使其成为未来研究的更可靠的资源。我们还提供了UrduDoc，一种用于扫描文档中乌尔都文本行检测的基准数据集。此外，我们还开发了一种在线工具，通过将UTRNet与文本的端到端乌尔都OCR集成在印刷文档中。

    In this paper, we propose a novel approach to address the challenges of printed Urdu text recognition using high-resolution, multi-scale semantic feature extraction. Our proposed UTRNet architecture, a hybrid CNN-RNN model, demonstrates state-of-the-art performance on benchmark datasets. To address the limitations of previous works, which struggle to generalize to the intricacies of the Urdu script and the lack of sufficient annotated real-world data, we have introduced the UTRSet-Real, a large-scale annotated real-world dataset comprising over 11,000 lines and UTRSet-Synth, a synthetic dataset with 20,000 lines closely resembling real-world and made corrections to the ground truth of the existing IIITH dataset, making it a more reliable resource for future research. We also provide UrduDoc, a benchmark dataset for Urdu text line detection in scanned documents. Additionally, we have developed an online tool for end-to-end Urdu OCR from printed documents by integrating UTRNet with a tex
    
[^142]: U-TOE: 低功耗物联网通用TinyML局部评估工具包

    U-TOE: Universal TinyML On-board Evaluation Toolkit for Low-Power IoT. (arXiv:2306.14574v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.14574](http://arxiv.org/abs/2306.14574)

    U-TOE是一个通用的低功耗物联网TinyML局部评估工具包，结合了低功耗嵌入式操作系统、模型转换器和编译器、性能测量模块和远程物联网测试平台的功能。能够帮助物联网设计师和研究人员评估在低功耗物联网硬件上执行各种模型的可行性。

    

    TinyML社区的结果表明，即使是小型基于微控制器的设备，也可以直接在终端上执行机器学习模型。然而，到目前为止，该领域的从业者缺乏便捷的一体化工具包，帮助他们评估在任意低功耗物联网硬件上执行任意模型的可行性。为此，我们在本文中提出了U-TOE，这是一个通用工具包，我们设计它来简化物联网设计师和研究人员的工作，它结合了低功耗嵌入式操作系统、通用模型转换器和编译器、集成性能测量模块和开放式远程物联网测试平台的功能。我们提供了U-TOE的开源实现，并演示了其在各种基于流行微控制器架构的低功耗物联网板上实验评估各种模型性能的用途。U-TOE可实现易于复现和可定制的比较评估。

    Results from the TinyML community demonstrate that, it is possible to execute machine learning models directly on the terminals themselves, even if these are small microcontroller-based devices. However, to date, practitioners in the domain lack convenient all-in-one toolkits to help them evaluate the feasibility of executing arbitrary models on arbitrary low-power IoT hardware. To this effect, we present in this paper U-TOE, a universal toolkit we designed to facilitate the task of IoT designers and researchers, by combining functionalities from a low-power embedded OS, a generic model transpiler and compiler, an integrated performance measurement module, and an open-access remote IoT testbed. We provide an open source implementation of U-TOE and we demonstrate its use to experimentally evaluate the performance of various models, on a wide variety of low-power IoT boards, based on popular microcontroller architectures. U-TOE allows easily reproducible and customizable comparative eval
    
[^143]: 人工智能灾难性风险综述

    An Overview of Catastrophic AI Risks. (arXiv:2306.12001v1 [cs.CY])

    [http://arxiv.org/abs/2306.12001](http://arxiv.org/abs/2306.12001)

    本文综述了人工智能灾难性风险的四个主要来源，包括恶意使用、人工智能竞赛、组织风险和流氓人工智能。

    

    人工智能的快速发展引起了专家、政策制定者和世界各国领导人对越来越先进的人工智能系统可能带来灾难性风险的担忧。虽然已经有很多风险被单独详细介绍过，但迫切需要系统地讨论和说明潜在危险，以更好地支持减轻这些风险的努力。本文概述了人工智能灾难性风险的主要来源，我们将其分为四个类别：恶意使用，即个人或团体有意使用人工智能造成伤害；人工智能竞赛，即竞争环境促使行动者部署不安全的人工智能或放弃控制权交给人工智能；组织风险，突出人为和复杂系统如何增加灾难性事故发生的可能性；以及流氓人工智能，描述了控制比人类智能更高的代理程序困难的固有难题。对于每个风险类别，我们描述了具体的危害。

    Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards,
    
[^144]: 压缩感知：离散优化方法

    Compressed Sensing: A Discrete Optimization Approach. (arXiv:2306.04647v1 [eess.SP])

    [http://arxiv.org/abs/2306.04647](http://arxiv.org/abs/2306.04647)

    本文中提出了一种离散优化方法来解决压缩感知问题，该方法在二次锥松弛下，可以找到最稀疏的向量，得到了可靠的最优解。

    

    本文研究了压缩感知问题，即找到最稀疏的向量，该向量满足一组线性测量，同时达到一定的数值容限。压缩感知是统计学、运筹学和机器学习中的核心问题，应用于信号处理、数据压缩和图像重建等领域。我们引入了一个带有$\ell_2$正则化的压缩感知问题，将其作为混合整数二次锥规划来重新定义。我们推导出此问题的二次锥松弛，并展示了在正则化参数的温和限制下，得到的松弛等价于深入研究的基础追踪去噪问题。我们提出了一个半定松弛来加强二次锥松弛，开发了一种定制的分支定界算法，利用我们的二次锥松弛来解决压缩感知问题的实例，以确证的最优解。我们的数值结果表明，我们的方法产生的解决方案是精确的，并且优于其他方法。

    We study the Compressed Sensing (CS) problem, which is the problem of finding the most sparse vector that satisfies a set of linear measurements up to some numerical tolerance. CS is a central problem in Statistics, Operations Research and Machine Learning which arises in applications such as signal processing, data compression and image reconstruction. We introduce an $\ell_2$ regularized formulation of CS which we reformulate as a mixed integer second order cone program. We derive a second order cone relaxation of this problem and show that under mild conditions on the regularization parameter, the resulting relaxation is equivalent to the well studied basis pursuit denoising problem. We present a semidefinite relaxation that strengthens the second order cone relaxation and develop a custom branch-and-bound algorithm that leverages our second order cone relaxation to solve instances of CS to certifiable optimality. Our numerical results show that our approach produces solutions that 
    
[^145]: 任务关系感知的持续用户表示学习

    Task Relation-aware Continual User Representation Learning. (arXiv:2306.01792v1 [cs.IR])

    [http://arxiv.org/abs/2306.01792](http://arxiv.org/abs/2306.01792)

    本文提出了一种新的持续用户表示学习方法TERACON，它能够学习通用的用户表示，而不是为每个任务学习任务特定的用户表示，具有很强的实用性和学习能力。

    

    用户建模是基于其过去行为学习将用户表示为低维表示空间的方法，它受到了工业界提供个性化服务的兴趣激增。以往的用户建模工作主要集中在学习为单一任务而设计的任务特定用户表示上。然而，由于为每个任务学习任务特定用户表示是不可行的，因此最近的研究引入了通用用户表示的概念，即与多种任务相关的更广义用户表示。尽管这些方法非常有效，但由于数据需求、灾难性遗忘以及为持续添加的任务提供有限的学习能力，现有的学习通用用户表示的方法在实际应用中是不切实际的。本文提出了一种新颖的持续用户表示学习方法TERACON，其学习能力不受任务数量限制。

    User modeling, which learns to represent users into a low-dimensional representation space based on their past behaviors, got a surge of interest from the industry for providing personalized services to users. Previous efforts in user modeling mainly focus on learning a task-specific user representation that is designed for a single task. However, since learning task-specific user representations for every task is infeasible, recent studies introduce the concept of universal user representation, which is a more generalized representation of a user that is relevant to a variety of tasks. Despite their effectiveness, existing approaches for learning universal user representations are impractical in real-world applications due to the data requirement, catastrophic forgetting and the limited learning capability for continually added tasks. In this paper, we propose a novel continual user representation learning method, called TERACON, whose learning capability is not limited as the number 
    
[^146]: 关于学习视频压缩中感知损失函数的选择

    On the Choice of Perception Loss Function for Learned Video Compression. (arXiv:2305.19301v1 [eess.IV])

    [http://arxiv.org/abs/2305.19301](http://arxiv.org/abs/2305.19301)

    本文研究了在学习视频压缩时，感知损失函数的选择对于重建效果的影响，发现选择 PLF-JD 可以更好地保留跨帧时序相关性，但同时会带来更大的失真惩罚和更难以纠正早期输出帧中的错误。

    

    本文针对低延迟、顺序视频压缩下受到均方误差（MSE）失真损失和感知损失（以实现真实感为目标）的输出设计了研究，并考虑了两种不同的感知损失函数（PLFs），分别是PLF-JD和PLF-FMD。通过信息论分析和基于深度学习的实验，我们演示了 PLF 的选择可能对重建效果产生显著影响，特别是在低比特率的情况下。具体而言，在保留跨帧时序相关性方面，基于PLF-JD 的重建效果会更好，但与 PLF-FMD 相比，也会带来更大的失真惩罚，并使其更难以恢复早期输出帧中出现的错误。

    We study causal, low-latency, sequential video compression when the output is subjected to both a mean squared-error (MSE) distortion loss as well as a perception loss to target realism. Motivated by prior approaches, we consider two different perception loss functions (PLFs). The first, PLF-JD, considers the joint distribution (JD) of all the video frames up to the current one, while the second metric, PLF-FMD, considers the framewise marginal distributions (FMD) between the source and reconstruction. Using information theoretic analysis and deep-learning based experiments, we demonstrate that the choice of PLF can have a significant effect on the reconstruction, especially at low-bit rates. In particular, while the reconstruction based on PLF-JD can better preserve the temporal correlation across frames, it also imposes a significant penalty in distortion compared to PLF-FMD and further makes it more difficult to recover from errors made in the earlier output frames. Although the cho
    
[^147]: 利用空间对比预训练进行未见训练数据的新道路交通预测

    Traffic Forecasting on New Roads Unseen in the Training Data Using Spatial Contrastive Pre-Training. (arXiv:2305.05237v1 [cs.LG])

    [http://arxiv.org/abs/2305.05237](http://arxiv.org/abs/2305.05237)

    本文提出一种名为SCPT的框架，利用对比学习进行空间预训练，并引入一个空间编码器模块，用于从未见数据中提取特征。该方法可以用于进行新道路的交通预测，无需重新训练模型。

    

    随着时间推移会不断建设新的道路，但是之前的深度预测模型对于新道路（未见数据）的泛化能力很少被探索。本文引入了一个被称为时空（ST）分割的新设置，以评估模型对未见数据的泛化能力。在这个设置中，模型训练时使用一部分的道路数据，但测试时使用未见数据的道路。我们还提出了一种新的框架，称之为空间对比预训练（SCPT），其中引入了一个空间编码器模块来提取推理时未见道路的潜在特征。这个空间编码器是使用对比学习预训练的。在推理时，空间编码器仅需要新道路的两天交通数据，而不需要任何重新训练。我们还展示了空间编码器的输出可以有效地用于推断未见道路上的潜在节点嵌入。

    New roads are being constructed all the time. However, the capabilities of previous deep forecasting models to generalize to new roads not seen in the training data (unseen roads) are rarely explored. In this paper, we introduce a novel setup called a spatio-temporal (ST) split to evaluate the models' capabilities to generalize to unseen roads. In this setup, the models are trained on data from a sample of roads, but tested on roads not seen in the training data. Moreover, we also present a novel framework called Spatial Contrastive Pre-Training (SCPT) where we introduce a spatial encoder module to extract latent features from unseen roads during inference time. This spatial encoder is pre-trained using contrastive learning. During inference, the spatial encoder only requires two days of traffic data on the new roads and does not require any re-training. We also show that the output from the spatial encoder can be used effectively to infer latent node embeddings on unseen roads during 
    
[^148]: 数据集蒸馏研究综述: 方法、应用和未来方向

    A Survey on Dataset Distillation: Approaches, Applications and Future Directions. (arXiv:2305.01975v1 [cs.LG])

    [http://arxiv.org/abs/2305.01975](http://arxiv.org/abs/2305.01975)

    数据集蒸馏在机器学习中越来越重要。该方法可以通过合成高信息密度的数据集来支持持续学习、神经架构搜索和隐私保护。这篇综述性调查论文提出了一种分类方法，对现有方法进行了特征化，并系统回顾了数据模态和相关应用，同时总结了挑战并讨论了未来方向。

    

    随着训练集的不断增长和训练最先进的模型成本越来越高，数据集蒸馏在机器学习中越来越受到关注。通过合成高信息密度的数据集，数据集蒸馏提供了一系列潜在应用，包括支持持续学习、神经架构搜索和隐私保护。尽管近年来有了一些进展，但我们缺乏对方法和应用的全面理解。我们的调查旨在填补这一空白，首先提出一种数据集蒸馏的分类方法，对现有方法进行特征化，然后系统地回顾数据模态和相关应用。此外，我们总结了挑战并讨论了这一研究领域的未来方向。

    Dataset distillation is attracting more attention in machine learning as training sets continue to grow and the cost of training state-of-the-art models becomes increasingly high. By synthesizing datasets with high information density, dataset distillation offers a range of potential applications, including support for continual learning, neural architecture search, and privacy protection. Despite recent advances, we lack a holistic understanding of the approaches and applications. Our survey aims to bridge this gap by first proposing a taxonomy of dataset distillation, characterizing existing approaches, and then systematically reviewing the data modalities, and related applications. In addition, we summarize the challenges and discuss future directions for this field of research.
    
[^149]: 基于自适应细化和康托洛维奇度量的数据驱动抽象（扩展版）

    Data-driven abstractions via adaptive refinements and a Kantorovich metric [extended version]. (arXiv:2303.17618v1 [cs.LG])

    [http://arxiv.org/abs/2303.17618](http://arxiv.org/abs/2303.17618)

    我们提出了一种基于自适应细化和康托洛维奇度量的智能且可扩展的动态系统抽象技术，并且定义了一种马尔可夫链之间的度量用作损失函数。我们的方法具有更好的计算复杂度。

    

    我们介绍了一种智能且可扩展的动态系统抽象自适应细化技术。我们的技术依赖于根据未来输出的观察将状态空间划分。然而，这种知识是动态地以不对称的方式构建的。为了学习最优结构，我们定义了马尔可夫链之间的康托洛维奇度量，并将其用作损失函数。我们的技术适用于数据驱动的框架，但不受限于此。我们还研究了马尔可夫链之间上述度量的性质，我们认为这可能具有更广泛的应用。我们提出了一种近似计算该度量的算法，并且我们展示了我们的方法比使用传统的线性规划技术具有更好的计算复杂度。

    We introduce an adaptive refinement procedure for smart, and scalable abstraction of dynamical systems. Our technique relies on partitioning the state space depending on the observation of future outputs. However, this knowledge is dynamically constructed in an adaptive, asymmetric way. In order to learn the optimal structure, we define a Kantorovich-inspired metric between Markov chains, and we use it as a loss function. Our technique is prone to data-driven frameworks, but not restricted to.  We also study properties of the above mentioned metric between Markov chains, which we believe could be of application for wider purpose. We propose an algorithm to approximate it, and we show that our method yields a much better computational complexity than using classical linear programming techniques.
    
[^150]: 连接生成半监督学习和生成开放集识别

    Linking generative semi-supervised learning and generative open-set recognition. (arXiv:2303.11702v1 [cs.CV])

    [http://arxiv.org/abs/2303.11702](http://arxiv.org/abs/2303.11702)

    本研究旨在探究生成半监督学习和生成开放集识别之间的关系。SSL-GANs和OSR-GANs方法的相似性在于都要求生成器在互补空间中产生样本，并通过正则化来推广开放空间。研究结果表明SSL优化边缘-GAN在结合SSL-OSR任务方面树立新的标准，但在某些OSR任务中OSR优化的ARP-GAN仍然略优于SSL-GAN。

    

    本研究在生成对抗网络（GANs）的背景下，探究了半监督学习（SSL）和开放集识别（OSR）之间的关系。尽管以前没有正式将SSL和OSR联系起来的研究，但它们各自的方法有惊人的相似之处。具体而言，SSL-GAN和OSR-GAN要求生成器在互补空间中产生样本。随后，通过对生成样本进行正则化，SSL和OSR分类器都可以完全识别开放空间。为了证明SSL和OSR之间的关联，我们在理论上和实验上比较了最先进的SSL-GAN方法和最先进的OSR-GAN方法。结果表明，文献基础更加牢固的SSL优化边缘-GAN在结合SSL-OSR任务方面树立新的标准，并在某些一般的OSR实验中取得了新的最先进的结果。然而，OSR优化的对抗性互惠点（ARP）-GAN在一些OSR任务中仍然略优于SSL-GAN。

    This study investigates the relationship between semi-supervised learning (SSL) and open-set recognition (OSR) in the context of generative adversarial networks (GANs). Although no previous study has formally linked SSL and OSR, their respective methods share striking similarities. Specifically, SSL-GANs and OSR-GANs require generator to produce samples in the complementary space. Subsequently, by regularising networks with generated samples, both SSL and OSR classifiers generalize the open space. To demonstrate the connection between SSL and OSR, we theoretically and experimentally compare state-of-the-art SSL-GAN methods with state-of-the-art OSR-GAN methods. Our results indicate that the SSL optimised margin-GANs, which have a stronger foundation in literature, set the new standard for the combined SSL-OSR task and achieves new state-of-other art results in certain general OSR experiments. However, the OSR optimised adversarial reciprocal point (ARP)-GANs still slightly out-performe
    
[^151]: 一种基于自监督的心血管事件检测通用实验室进展预训练模型

    Self-supervised based general laboratory progress pretrained model for cardiovascular event detection. (arXiv:2303.06980v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06980](http://arxiv.org/abs/2303.06980)

    研究利用自监督学习和迁移学习，将心血管实验室指标的患者进展趋势从常见情况转移到罕见或特定心血管事件检测中，以协助检测经皮冠状动脉介入治疗患者的靶血管重建。

    

    定期监测是管理心血管疾病的必要方面。由于罕见或特定疾病的患者规模较小，观察也是间歇性的，因此其招募常常受到限制，而常见情况由于定期随访而更容易累积纵向数据。然而，这些数据以其无规律性、时间性、缺席性和稀疏性而闻名。本研究利用自监督学习和迁移学习来克服上述障碍，将心血管实验室指标的患者进展趋势从常见情况转移到罕见或特定心血管事件检测中。我们使用高血压患者（尚未患糖尿病）进行了一般实验室进展（GLP）预训练模型的预训练，并将其实验室进展趋势转移，以协助检测经皮冠状动脉介入治疗患者的靶血管重建（TVR）。GLP采用了两个阶段的训练过程，包括预训练和微调。

    Regular surveillance is an indispensable aspect of managing cardiovascular disorders. Patient recruitment for rare or specific diseases is often limited due to their small patient size and episodic observations, whereas prevalent cases accumulate longitudinal data easily due to regular follow-ups. These data, however, are notorious for their irregularity, temporality, absenteeism, and sparsity. In this study, we leveraged self-supervised learning (SSL) and transfer learning to overcome the above-mentioned barriers, transferring patient progress trends in cardiovascular laboratory parameters from prevalent cases to rare or specific cardiovascular events detection. We pretrained a general laboratory progress (GLP) pretrain model using hypertension patients (who were yet to be diabetic), and transferred their laboratory progress trend to assist in detecting target vessel revascularization (TVR) in percutaneous coronary intervention patients. GLP adopted a two-stage training process that u
    
[^152]: 使用逻辑编程和大型语言模型的领域特定问题回答在知识图谱上。 (arXiv:2303.02206v2 [cs.LG] UPDATED)

    Domain Specific Question Answering Over Knowledge Graphs Using Logical Programming and Large Language Models. (arXiv:2303.02206v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02206](http://arxiv.org/abs/2303.02206)

    该论文提出了一种在知识图谱上回答领域特定问题的方法，通过将经典的逻辑编程语言与大型语言模型整合，实现逻辑推理能力来解决KGQA任务。实验证明该方法在准确识别所有测试问题的正确答案实体方面表现出较高效果，即使仅使用少量标注数据进行训练。这种方法为解决领域特定图谱上的问题回答提供了有前景的解决方案，具有可解释性和稳健性。

    

    在领域特定的图谱上回答问题需要一种定制的方法，因为关系的数量有限，并且领域的特定性。我们的方法将经典的逻辑编程语言整合到大型语言模型中，使其能够利用逻辑推理能力来解决KGQA任务。通过将问题表示为Prolog查询，我们可以容易地生成程序化生成的答案。为了验证我们的方法的有效性，我们使用一个著名的基准数据集MetaQA进行评估。实验结果表明，即使在训练了一小部分注释数据的情况下，我们的方法也能准确地识别出所有测试问题的正确答案实体。总体而言，我们的工作提出了一种有前景的方法来解决领域特定图谱上的问题回答，通过整合逻辑编程提供了一个可解释和健壮的解决方案。

    Answering questions over domain-specific graphs requires a tailored approach due to the limited number of relations and the specific nature of the domain. Our approach integrates classic logical programming languages into large language models (LLMs), enabling the utilization of logical reasoning capabilities to tackle the KGQA task. By representing the questions as Prolog queries, which are readable and near close to natural language in representation, we facilitate the generation of programmatically derived answers. To validate the effectiveness of our approach, we evaluate it using a well-known benchmark dataset, MetaQA. Our experimental results demonstrate that our method achieves accurate identification of correct answer entities for all test questions, even when trained on a small fraction of annotated data. Overall, our work presents a promising approach to addressing question answering over domain-specific graphs, offering an explainable and robust solution by incorporating log
    
[^153]: 通过内容感知的风格不变模型学习对未知领域进行泛化：用于胸部X射线疾病检测的翻译摘要

    Learning to Generalize towards Unseen Domains via a Content-Aware Style Invariant Model for Disease Detection from Chest X-rays. (arXiv:2302.13991v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.13991](http://arxiv.org/abs/2302.13991)

    通过内容感知的风格不变模型，我们提出了一种解决深度学习医学图像分析中源领域不匹配挑战的方法。我们采用了风格随机化模块来提取既是风格不变又是内容偏好的领域不变特征，在胸部X射线疾病检测中取得了良好的性能。

    

    在基于深度学习的医学图像分析中，由于源领域不匹配而导致性能降低一直是一个长期存在的挑战，特别是在胸部X射线（CXR）领域。为了解决这种领域转移问题，已经提出了一些方法（如对抗训练，多领域混合），用于提取领域不变的高级特征。然而，这些方法并没有明确规范提取的领域不变特征的内容和风格特征。最近的研究表明，CNN模型对风格（例如，无信息的纹理）有很强的偏好，而不是对内容（例如，形状）的偏好，这与人类视觉系统形成鲜明对比。放射科医师倾向于从CXR图像中学习视觉线索，并因此在多个领域中表现良好。因此，在从CXR图像进行病理诊断的医学成像中，模型应该提取既是风格不变又是内容偏好的领域不变特征。受此启发，我们在实验中使用了新颖的风格随机化模块（SRMs）。

    Performance degradation due to source domain mismatch is a longstanding challenge in deep learning-based medical image analysis, particularly for chest X-rays (CXRs). Several methods (e.g., adversarial training, multi-domain mixups) have been proposed to extract domain-invariant high-level features to address this domain shift. However, these methods do not explicitly regularize the content and style characteristics of the extracted domain-invariant features. Recent studies have demonstrated that CNN models exhibit a strong bias toward styles (e.g., uninformative textures) rather than content (e.g., shape), in stark contrast to the human-vision system. Radiologists tend to learn visual cues from CXRs and thus perform well across multiple domains. Therefore, in medical imaging for pathology diagnosis from CXR images, models should extract domain-invariant features that are style-invariant and content-biased. Motivated by this, we employ the novel style randomization modules (SRMs) at bo
    
[^154]: 从道路行驶数据中估计驾驶员个性特质

    Estimating Driver Personality Traits from On-Road Driving Data. (arXiv:2302.10898v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10898](http://arxiv.org/abs/2302.10898)

    本研究旨在从驾驶行为数据中利用机器学习和深度学习技术开发一个模型，以估计驾驶员的心理特征，为驾驶辅助系统提供适应性反馈和预防交通事故。

    

    本文着重于利用驾驶数据来估计驾驶员的心理特征，以用于驾驶辅助系统。适应个体心理特征的驾驶辅助系统能够提供适当的反馈并预防交通事故。作为实现这样自适应辅助系统的第一步，本研究旨在利用机器学习和深度学习技术，从在路行驶的行为数据中开发一个估计驾驶员心理特征的模型，例如认知功能、心理驾驶风格和工作负荷敏感度。我们还通过回归建模研究了驾驶行为与各种认知功能之间的关系，包括追踪制图测试（TMT）和有用视野（UFOV）测试。所提出的方法关注道路类型信息，并捕捉从驾驶行为中观察到的各种时间序列数据的持续时间。首先，我们对驾驶时间进行分段处理。

    This paper focuses on the estimation of a driver's psychological characteristics using driving data for driving assistance systems. Driving assistance systems that support drivers by adapting individual psychological characteristics can provide appropriate feedback and prevent traffic accidents. As a first step toward implementing such adaptive assistance systems, this research aims to develop a model to estimate drivers' psychological characteristics, such as cognitive function, psychological driving style, and workload sensitivity, from on-road driving behavioral data using machine learning and deep learning techniques. We also investigated the relationship between driving behavior and various cognitive functions, including the Trail Making Test (TMT) and Useful Field of View (UFOV) test, through regression modeling. The proposed method focuses on road type information and captures various durations of time-series data observed from driving behaviors. First, we segment the driving ti
    
[^155]: 机器学习安全防御中的平等度量

    Measuring Equality in Machine Learning Security Defenses. (arXiv:2302.08973v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08973](http://arxiv.org/abs/2302.08973)

    本文研究了机器学习安全防御方法的平等性能问题，提出了一种简单的平等度量和分析框架，鼓励进一步探索公平性在该领域的应用。

    

    在过去的十年中，机器学习安全社区已经发展了许多对抗攻击的防御方法。但这个社区中鲜有研究一个问题：这些防御方法为谁提供保护呢？本文考虑了一些常见的防御方法，并研究了当这些防御方法被不同的子群体使用时，它们是否会产生意想不到的平等性能问题。我们提出了一种简单的平等度量和分析框架，通过机器学习安全方法的公平性实证结果来回答这个问题。许多方法可能会直接造成伤害，我们称之为有偏漏洞和有偏排斥。我们的框架和度量方法可以应用于强化训练模型、基于预处理的方法和拒绝方法，以捕捉在安全预算上的行为。我们确定了一个实际的数据集，具有合理的计算成本，适合于测量防御的平等性。通过一个案例研究，我们展示了现代防御方法的准确性和平等性性能的衡量价值。我们希望我们提出的指标和方法能够鼓励和促进机器学习安全和防御领域的公平性探索。

    The machine learning security community has developed myriad defenses for evasion attacks over the past decade. An understudied question in that community is: for whom do these defenses defend? In this work, we consider some common approaches to defending learned systems and whether those approaches may offer unexpected performance inequities when used by different sub-populations. We outline simple parity metrics and a framework for analysis that can begin to answer this question through empirical results of the fairness implications of machine learning security methods. Many methods have been proposed that can cause direct harm, which we describe as biased vulnerability and biased rejection. Our framework and metric can be applied to robustly trained models, preprocessing-based methods, and rejection methods to capture behavior over security budgets. We identify a realistic dataset with a reasonable computational cost suitable for measuring the equality of defenses. Through a case st
    
[^156]: 强化学习的后悔优化方法

    Regret-Based Optimization for Robust Reinforcement Learning. (arXiv:2302.06912v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06912](http://arxiv.org/abs/2302.06912)

    本论文提出了一种基于后悔的优化方法，用于使强化学习算法更加鲁棒，以应对观测中的对抗性噪声。

    

    深度强化学习策略对观测中的微小对抗性噪声容易受到攻击。这种对抗性噪声在安全关键环境中可能造成灾难性后果。现有的使强化学习算法对观测扰动的对抗策略主要集中在迭代改进每个迭代中生成的对抗示例。虽然这些方法已经显示出对普通强化学习方法的改进，但它们是被动性的，如果某些类别的对抗性示例在训练中没有产生，它们可能会表现得更差。因此，我们追求一种更积极的方法，依赖于直接优化一个经过充分研究的鲁棒指标。

    Deep Reinforcement Learning (DRL) policies have been shown to be vulnerable to small adversarial noise in observations. Such adversarial noise can have disastrous consequences in safety-critical environments. For instance, a self-driving car receiving adversarially perturbed sensory observations about nearby signs (e.g., a stop sign physically altered to be perceived as a speed limit sign) or objects (e.g., cars altered to be recognized as trees) can be fatal. Existing approaches for making RL algorithms robust to an observation-perturbing adversary have focused on reactive approaches that iteratively improve against adversarial examples generated at each iteration. While such approaches have been shown to provide improvements over regular RL methods, they are reactive and can fare significantly worse if certain categories of adversarial examples are not generated during training. To that end, we pursue a more proactive approach that relies on directly optimizing a well-studied robustn
    
[^157]: 基于稀疏性的深度神经网络剪枝技术

    Pruning Deep Neural Networks from a Sparsity Perspective. (arXiv:2302.05601v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05601](http://arxiv.org/abs/2302.05601)

    本文提出了一种稀疏感知自适应剪枝算法，通过利用PQ指数来衡量深度神经网络的潜在压缩性，可以有效地确定模型的剪枝程度，确保不会过度或欠剪枝。

    

    近年来，深度网络剪枝技术受到了重视，旨在将人工智能快速部署到计算和内存受限的小型设备上。这种剪枝通常通过丢弃深度网络中的冗余权重、神经元或层来实现，同时努力保持可比的测试性能。已经提出了许多深层剪枝算法，取得了令人瞩目的实证成果。然而，现有方法缺乏可量化的措施来估算每个剪枝迭代中子网络的可压缩性，因此可能会对模型进行过剪枝或欠剪枝。在本文中，我们提出了PQ指数（PQI）来衡量深度神经网络的潜在压缩性，并利用此指数开发出了一种稀疏感知自适应剪枝（SAP）算法。我们的广泛实验支持假设，对于通用剪枝程序，当大型模型被有效地正则化时，PQI首先减小，然后在其可压缩性达到最小值时增加。

    In recent years, deep network pruning has attracted significant attention in order to enable the rapid deployment of AI into small devices with computation and memory constraints. Pruning is often achieved by dropping redundant weights, neurons, or layers of a deep network while attempting to retain a comparable test performance. Many deep pruning algorithms have been proposed with impressive empirical success. However, existing approaches lack a quantifiable measure to estimate the compressibility of a sub-network during each pruning iteration and thus may under-prune or over-prune the model. In this work, we propose PQ Index (PQI) to measure the potential compressibility of deep neural networks and use this to develop a Sparsity-informed Adaptive Pruning (SAP) algorithm. Our extensive experiments corroborate the hypothesis that for a generic pruning procedure, PQI decreases first when a large model is being effectively regularized and then increases when its compressibility reaches a
    
[^158]: 高阶拓扑信号的狄拉克信号处理

    Dirac signal processing of higher-order topological signals. (arXiv:2301.10137v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2301.10137](http://arxiv.org/abs/2301.10137)

    本文提出了一种狄拉克信号处理算法，用于改善高阶拓扑信号的信噪比，并能够联合处理节点和链接上的拓扑信号。

    

    高阶网络可以维持与节点、链路、三角形和更高维的单纯复合体相关的拓扑信号，这些拓扑信号可以描述包括海洋电流、神经元之间的突触电流和生物运输网络在内的各种真实系统。在真实场景中，拓扑信号数据可能存在噪声，一个重要的任务是通过提高信噪比来处理这些信号。迄今为止，拓扑信号通常是独立处理的。例如，节点信号独立于链路信号进行处理，而可以在不同维度上强制一致处理拓扑信号的算法在很大程度上缺乏。在这里，我们提出了狄拉克信号处理，一种自适应的无监督信号处理算法，它学习联合过滤支持在节点、链接上的拓扑信号。

    Higher-order networks can sustain topological signals which are variables associated not only to the nodes, but also to the links, to the triangles and in general to the higher dimensional simplices of simplicial complexes. These topological signals can describe a large variety of real systems including currents in the ocean, synaptic currents between neurons and biological transportation networks. In real scenarios topological signal data might be noisy and an important task is to process these signals by improving their signal to noise ratio. So far topological signals are typically processed independently of each other. For instance, node signals are processed independently of link signals, and algorithms that can enforce a consistent processing of topological signals across different dimensions are largely lacking. Here we propose Dirac signal processing, an adaptive, unsupervised signal processing algorithm that learns to jointly filter topological signals supported on nodes, link
    
[^159]: BallGAN: 带有球形背景的3D感知图像合成

    BallGAN: 3D-aware Image Synthesis with a Spherical Background. (arXiv:2301.09091v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.09091](http://arxiv.org/abs/2301.09091)

    BallGAN是一个新颖的3D感知GAN框架，通过将背景近似为球形表面，并使用特定约束来减少背景自由度，可以产生更合理的3D几何。

    

    3D感知的生成对抗网络（GAN）旨在合成逼真的3D场景，以便可以以任意角度进行渲染以产生图像。尽管以前的方法可以产生逼真的图像，但它们在训练不稳定或存在不自然的3D几何解决方案方面存在问题。我们假设3D几何在约束不足的情况下是不确定的，即仅将其分类为真实图像对于鉴别器来说是不够的。为了解决这个问题，我们提出将背景近似为球形表面，并将场景表示为放置在球体中的前景和薄球形背景的联合。这样可以减少背景场的自由度。因此，我们修改了体渲染方程，并加入了专用的约束，设计了一种名为BallGAN的新型3D感知GAN框架。 BallGAN具有以下多个优点。1）它产生了更合理的3D几何；场景在不同视角下的图像具有更好的光度。

    3D-aware GANs aim to synthesize realistic 3D scenes such that they can be rendered in arbitrary perspectives to produce images. Although previous methods produce realistic images, they suffer from unstable training or degenerate solutions where the 3D geometry is unnatural. We hypothesize that the 3D geometry is underdetermined due to the insufficient constraint, i.e., being classified as real image to the discriminator is not enough. To solve this problem, we propose to approximate the background as a spherical surface and represent a scene as a union of the foreground placed in the sphere and the thin spherical background. It reduces the degree of freedom in the background field. Accordingly, we modify the volume rendering equation and incorporate dedicated constraints to design a novel 3D-aware GAN framework named BallGAN. BallGAN has multiple advantages as follows. 1) It produces more reasonable 3D geometry; the images of a scene across different viewpoints have better photometric 
    
[^160]: 一种严格的不确定性感知量化框架对机器学习工作流之可重复再现性至关重要

    A Rigorous Uncertainty-Aware Quantification Framework Is Essential for Reproducible and Replicable Machine Learning Workflows. (arXiv:2301.05763v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05763](http://arxiv.org/abs/2301.05763)

    这篇论文讨论了一个基于贝叶斯范式的不确定性量化框架，可以提供一个广泛而严格的方法来评估机器学习和人工智能在科学工作流程中的可重复性和可信度。

    

    机器学习或人工智能模型的预测和科学工作流程中包含的结果复现能力受多种因素影响。一种能够定量评估感兴趣量（QoI）复制性的不确定度感知度量标准，将有助于科学工作流程中应用机器学习和人工智能获得的结果具有更好的可信度。本文讨论了贝叶斯范式中的不确定性量化如何提供一种广泛和严格的框架，定量地评估复杂科学工作流程的可重复性。这样的框架将填补机器学习或人工智能在科学工作流程中的关键缺陷，因为它将使研究人员能够确定机器学习或人工智能模型预测变异对工作流程中预测的影响。我们期望这个框架将有助于设计更具重复性和可信度的工作流程。

    The ability to replicate predictions by machine learning (ML) or artificial intelligence (AI) models and results in scientific workflows that incorporate such ML/AI predictions is driven by numerous factors. An uncertainty-aware metric that can quantitatively assess the reproducibility of quantities of interest (QoI) would contribute to the trustworthiness of results obtained from scientific workflows involving ML/AI models. In this article, we discuss how uncertainty quantification (UQ) in a Bayesian paradigm can provide a general and rigorous framework for quantifying reproducibility for complex scientific workflows. Such as framework has the potential to fill a critical gap that currently exists in ML/AI for scientific workflows, as it will enable researchers to determine the impact of ML/AI model prediction variability on the predictive outcomes of ML/AI-powered workflows. We expect that the envisioned framework will contribute to the design of more reproducible and trustworthy wor
    
[^161]: 相位偏移对抗训练

    Phase-shifted Adversarial Training. (arXiv:2301.04785v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.04785](http://arxiv.org/abs/2301.04785)

    本论文通过分析响应频率的视角，发现对抗训练导致神经网络收敛性较低，从而在每个数据附近产生高度振荡的预测。为了有效地学习高频内容，提出了相位偏移对抗训练(PhaseAT)方法。

    

    对抗训练被认为是确保神经网络应用程序安全部署到现实世界的关键组成部分。现有方法主要集中在如何通过增加更新步骤的数量、使用平滑的损失函数对模型进行正则化以及将随机性注入到攻击中来生成强有力的攻击。然而，我们通过响应频率的视角分析了对抗训练的行为。我们经验性地发现，对抗训练导致神经网络对高频信息的收敛性较低，从而在每个数据附近产生高度振荡的预测。为了高效而有效地学习高频内容，我们首先证明了一个频率原理的普遍现象，即\textit{较低的频率先学习}在对抗训练中仍然成立。基于此，我们提出了相位偏移对抗训练(PhaseAT)，模型通过学习高频内容来改善对抗训练的收敛性问题。

    Adversarial training has been considered an imperative component for safely deploying neural network-based applications to the real world. To achieve stronger robustness, existing methods primarily focus on how to generate strong attacks by increasing the number of update steps, regularizing the models with the smoothed loss function, and injecting the randomness into the attack. Instead, we analyze the behavior of adversarial training through the lens of response frequency. We empirically discover that adversarial training causes neural networks to have low convergence to high-frequency information, resulting in highly oscillated predictions near each data. To learn high-frequency contents efficiently and effectively, we first prove that a universal phenomenon of frequency principle, i.e., \textit{lower frequencies are learned first}, still holds in adversarial training. Based on that, we propose phase-shifted adversarial training (PhaseAT) in which the model learns high-frequency com
    
[^162]: 图形池化减小信号和图上卷积运算的维度

    Graphon Pooling for Reducing Dimensionality of Signals and Convolutional Operators on Graphs. (arXiv:2212.08171v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.08171](http://arxiv.org/abs/2212.08171)

    本文提出了一种在图上进行卷积信息处理的池化方法，通过利用图的图象理论和密集图序列的极限来实现，得到卷积运算符的低维表示，并通过简单的局部插值函数实现信号的降维，具有较好的效果。

    

    本文提出了一种在图上进行卷积信息处理的池化方法，该方法基于图的图象理论和密集图序列的极限。我们提出了三种方法，利用图的图握表示和图象在[0, 1]2上的分区进行计算。结果我们得到卷积运算符的低维表示，信号的降维是通过L2([0, 1])中的简单局部插值函数实现的。我们证明了这些低维表示分别构成了图和图象的收敛序列。我们提出的方法和提供的理论保证表明降低后的图和信号继承了原始数量的光谱结构特性。我们通过在依赖图形池化的图神经网络(GNNs)上进行一系列数值实验来评估我们的方法。观察到图形池化的效果很好。

    In this paper we propose a pooling approach for convolutional information processing on graphs relying on the theory of graphons and limits of dense graph sequences. We present three methods that exploit the induced graphon representation of graphs and graph signals on partitions of [0, 1]2 in the graphon space. As a result we derive low dimensional representations of the convolutional operators, while a dimensionality reduction of the signals is achieved by simple local interpolation of functions in L2([0, 1]). We prove that those low dimensional representations constitute a convergent sequence of graphs and graph signals, respectively. The methods proposed and the theoretical guarantees that we provide show that the reduced graphs and signals inherit spectral-structural properties of the original quantities. We evaluate our approach with a set of numerical experiments performed on graph neural networks (GNNs) that rely on graphon pooling. We observe that graphon pooling performs sign
    
[^163]: 面向可解释的基于Transformer的时间序列预测的时间显著性检测

    Temporal Saliency Detection Towards Explainable Transformer-based Timeseries Forecasting. (arXiv:2212.07771v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.07771](http://arxiv.org/abs/2212.07771)

    这项研究提出了一种名为Temporal Saliency Detection (TSD)的方法，利用基于注意力机制的架构实现了多步时间序列预测，并通过压缩多头注意力进行显著性模式的多分辨率分析。

    

    尽管在许多基于Transformer的模型中取得了显著的进展，但长期多步预测的时间序列预测任务仍然是一个持续的挑战，特别是在可解释性方面。本文的目标是基于常用的显著性图解释DNN的思想，构建一种基于注意力机制的架构，能够通过与适当的注意力头建立连接，自动编码与显著性相关的时间模式。因此，本文介绍了一种名为Temporal Saliency Detection (TSD) 的有效方法，它在多步时间序列预测中利用了注意力机制。虽然我们提出的架构遵循常规的编码器-解码器结构，但在编码器组件中经历了重大的改进，其中我们采用了受U-Net风格架构启发的一系列信息收缩和扩展模块。TSD方法通过压缩多头注意力实现了显著性模式的多分辨率分析。

    Despite the notable advancements in numerous Transformer-based models, the task of long multi-horizon time series forecasting remains a persistent challenge, especially towards explainability. Focusing on commonly used saliency maps in explaining DNN in general, our quest is to build attention-based architecture that can automatically encode saliency-related temporal patterns by establishing connections with appropriate attention heads. Hence, this paper introduces Temporal Saliency Detection (TSD), an effective approach that builds upon the attention mechanism and applies it to multi-horizon time series prediction. While our proposed architecture adheres to the general encoder-decoder structure, it undergoes a significant renovation in the encoder component, wherein we incorporate a series of information contracting and expanding blocks inspired by the U-Net style architecture. The TSD approach facilitates the multiresolution analysis of saliency patterns by condensing multi-heads, th
    
[^164]: Spurious Features Everywhere -- 在ImageNet中大规模检测有害的伪特征

    Spurious Features Everywhere -- Large-Scale Detection of Harmful Spurious Features in ImageNet. (arXiv:2212.04871v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.04871](http://arxiv.org/abs/2212.04871)

    本文提出了一个框架来系统地识别ImageNet中的有害伪特征，并引入了一个新的数据集"Spurious ImageNet"来衡量分类器对这些特征的依赖性。同时，介绍了一个简单的缓解方法SpuFix来减少分类器对有害伪特征的依赖性。

    

    光深度学习分类器的基准性能并不可靠来预测部署模型的性能。特别是，如果图像分类器在训练数据中捕捉到了伪特征，其预测结果可能会以意想不到的方式失败。本文提出了一个框架，可以系统地识别像ImageNet这样的大规模数据集中的伪特征。它基于我们的神经PCA组件及其可视化。之前对伪特征的相关工作通常在玩具场景中进行，或者需要昂贵的像素级注释。相比之下，我们使用ImageNet进行研究，并通过展示某个类别中存在有害伪特征就足以触发该类别的预测来验证我们的结果。我们引入了一个新的数据集"Spurious ImageNet"，可以衡量任何ImageNet分类器对有害伪特征的依赖性。此外，我们还引入了一种名为SpuFix的简单缓解方法来减少任何ImageNet分类器对有害伪特征的依赖性。

    Benchmark performance of deep learning classifiers alone is not a reliable predictor for the performance of a deployed model. In particular, if the image classifier has picked up spurious features in the training data, its predictions can fail in unexpected ways. In this paper, we develop a framework that allows us to systematically identify spurious features in large datasets like ImageNet. It is based on our neural PCA components and their visualization. Previous work on spurious features often operates in toy settings or requires costly pixel-wise annotations. In contrast, we work with ImageNet and validate our results by showing that presence of the harmful spurious feature of a class alone is sufficient to trigger the prediction of that class. We introduce the novel dataset "Spurious ImageNet" which allows to measure the reliance of any ImageNet classifier on harmful spurious features. Moreover, we introduce SpuFix as a simple mitigation method to reduce the dependence of any Imag
    
[^165]: 具有未知测量噪声的物理信息神经网络

    Physics-informed neural networks with unknown measurement noise. (arXiv:2211.15498v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.15498](http://arxiv.org/abs/2211.15498)

    这篇论文提出了一种解决物理信息神经网络在存在非高斯噪声情况下失效的问题的方法，即通过同时训练一个能量模型来学习正确的噪声分布。通过多个例子的实验证明了该方法的改进性能。

    

    物理信息神经网络(PINNs)是一种既能找到解决方案又能识别偏微分方程参数的灵活方法。大多数相关的研究都假设数据是无噪声的，或者是受弱高斯噪声污染的。我们展示了标准PINN框架在非高斯噪声情况下失效的问题，并提出了一种解决这个根本性问题的方法，即同时训练一个能量模型(Energy-Based Model, EBM)来学习正确的噪声分布。我们通过多个例子展示了我们方法的改进性能。

    Physics-informed neural networks (PINNs) constitute a flexible approach to both finding solutions and identifying parameters of partial differential equations. Most works on the topic assume noiseless data, or data contaminated by weak Gaussian noise. We show that the standard PINN framework breaks down in case of non-Gaussian noise. We give a way of resolving this fundamental issue and we propose to jointly train an energy-based model (EBM) to learn the correct noise distribution. We illustrate the improved performance of our approach using multiple examples.
    
[^166]: 知识感知的非IID数据联邦主动学习

    Knowledge-Aware Federated Active Learning with Non-IID Data. (arXiv:2211.13579v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.13579](http://arxiv.org/abs/2211.13579)

    本文提出了一种知识感知的非IID数据联邦主动学习方法，通过知识专业化主动抽样和知识补偿联邦更新来解决联邦主动学习中的目标不匹配问题。

    

    联邦学习使得多个分散的客户端能够在不共享本地训练数据的情况下进行协作学习。然而，获取本地数据标签的昂贵注释成本仍然是利用本地数据的障碍。在本文中，我们提出了一种联邦主动学习范式，以在有限注释预算的同时保护数据隐私，以分散化学习的方式高效地学习全局模型。联邦主动学习面临的主要挑战是服务器端全局模型的主动抽样目标与异步本地客户端的目标不匹配。当数据在本地客户端之间分布非IID时，这一挑战变得更加重要。为了解决上述挑战，我们提出了一种知识感知的联邦主动学习(KAFAL)方法，它包括知识专业化主动抽样(KSAS)和知识补偿联邦更新(KCFU)。

    Federated learning enables multiple decentralized clients to learn collaboratively without sharing the local training data. However, the expensive annotation cost to acquire data labels on local clients remains an obstacle in utilizing local data. In this paper, we propose a federated active learning paradigm to efficiently learn a global model with limited annotation budget while protecting data privacy in a decentralized learning way. The main challenge faced by federated active learning is the mismatch between the active sampling goal of the global model on the server and that of the asynchronous local clients. This becomes even more significant when data is distributed non-IID across local clients. To address the aforementioned challenge, we propose Knowledge-Aware Federated Active Learning (KAFAL), which consists of Knowledge-Specialized Active Sampling (KSAS) and Knowledge-Compensatory Federated Update (KCFU). KSAS is a novel active sampling method tailored for the federated acti
    
[^167]: 确切的流形高斯变分贝叶斯

    Exact Manifold Gaussian Variational Bayes. (arXiv:2210.14598v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.14598](http://arxiv.org/abs/2210.14598)

    我们提出了一种在复杂模型中进行变分推断的优化算法，通过使用自然梯度更新和黎曼流形，我们开发了一种高效的高斯变分推断算法，并验证了其在多个数据集上的性能。

    

    我们提出了一种用于复杂模型中变分推断（VI）的优化算法。我们的方法依赖于自然梯度更新，其中变分空间是一个黎曼流形。我们开发了一个高效的高斯变分推断算法，以隐式满足变分协方差矩阵的正定约束。我们的确切流形高斯变分贝叶斯（EMGVB）提供了精确但简单的更新规则，并且易于实现。由于其黑盒性质，EMGVB成为复杂模型中即插即用的解决方案。通过在不同统计、计量和深度学习模型上使用五个数据集，我们对我们的可行性方法进行了实证验证，并与基准方法进行了性能讨论。

    We propose an optimization algorithm for Variational Inference (VI) in complex models. Our approach relies on natural gradient updates where the variational space is a Riemann manifold. We develop an efficient algorithm for Gaussian Variational Inference that implicitly satisfies the positive definite constraint on the variational covariance matrix. Our Exact manifold Gaussian Variational Bayes (EMGVB) provides exact but simple update rules and is straightforward to implement. Due to its black-box nature, EMGVB stands as a ready-to-use solution for VI in complex models. Over five datasets, we empirically validate our feasible approach on different statistical, econometric, and deep learning models, discussing its performance with respect to baseline methods.
    
[^168]: MARLlib: 一个可扩展的多智能体强化学习库

    MARLlib: A Scalable Multi-agent Reinforcement Learning Library. (arXiv:2210.13708v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13708](http://arxiv.org/abs/2210.13708)

    本文提出了MARLlib，这是一个全面的MARL算法库，可统一数十种算法。它还超越了当前工作，集成了各种环境接口和提供灵活的参数共享策略。

    

    尽管多智能体系统和多智能体强化学习算法得到了快速发展，但缺乏统一的评估平台和公认的基准实现。因此，迫切需要开发一个集成库套件，以在各种基准测试中提供可靠的MARL实现和可复制的评估。本文提出了MARLlib，这是一个全面的MARL算法库，用于解决多智能体问题。MARLlib通过新颖的基于代理的分布式数据流设计，在高度可组合的集成风格中统一了数十种算法。此外，MARLlib通过集成各种环境接口和提供灵活的参数共享策略，超越了当前工作；这允许最终用户在最小的代码修改下实现协作、竞争和混合任务的多种解决方案。最后，MARLlib提供易于使用的API和完全解耦合的配置。

    Despite the fast development of multi-agent systems (MAS) and multi-agent reinforcement learning (MARL) algorithms, there is a lack of unified evaluation platforms and commonly-acknowledged baseline implementation. Therefore, an urgent need is to develop an integrated library suite that delivers reliable MARL implementation and replicable evaluation in various benchmarks. To fill such a research gap, in this paper, we propose MARLlib, a comprehensive MARL algorithm library for solving multi-agent problems. With a novel design of agent-level distributed dataflow, MARLlib manages to unify tens of algorithms in a highly composable integration style. Moreover, MARLlib goes beyond current work by integrating diverse environment interfaces and providing flexible parameter sharing strategies; this allows for versatile solutions to cooperative, competitive, and mixed tasks with minimal code modifications for end users. Finally, MARLlib provides easy-to-use APIs and a fully decoupled configurat
    
[^169]: ProtoBandit: 通过多臂赌博机实现高效原型选择

    ProtoBandit: Efficient Prototype Selection via Multi-Armed Bandits. (arXiv:2210.01860v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01860](http://arxiv.org/abs/2210.01860)

    本文提出的ProtoBandit算法通过多臂赌博机方法实现高效的原型选择，避免了在大规模设置下进行相似性比较的昂贵性，能够识别出一组紧凑的原型实例，有效代表给定的目标集。

    

    本文提出了一种基于多臂赌博机的框架，用于从源数据集S中识别一组紧凑的信息数据实例（即原型），以最好地代表给定的目标集T。给定数据集的原型示例提供了对底层数据分布的可解释性洞察，并在基于实例的推理中起到辅助作用，从而影响人类决策的各个领域。当前最先进的原型选择方法需要在源数据点和目标数据点之间进行O（|S| |T|）的相似性比较，对于大规模设置来说显得难以承受。我们提出通过在原型示例空间中采用随机贪婪搜索和多臂赌博机来减少相似性比较的数量来缓解这个局限性。我们的随机算法ProtoBandit能够在产生O（k^3 |S|）的相似性比较的情况下识别出一组k个原型，这与目标集的大小无关。

    In this work, we propose a multi-armed bandit-based framework for identifying a compact set of informative data instances (i.e., the prototypes) from a source dataset $S$ that best represents a given target set $T$. Prototypical examples of a given dataset offer interpretable insights into the underlying data distribution and assist in example-based reasoning, thereby influencing every sphere of human decision-making. Current state-of-the-art prototype selection approaches require $O(|S||T|)$ similarity comparisons between source and target data points, which becomes prohibitively expensive for large-scale settings. We propose to mitigate this limitation by employing stochastic greedy search in the space of prototypical examples and multi-armed bandits for reducing the number of similarity comparisons. Our randomized algorithm, ProtoBandit, identifies a set of $k$ prototypes incurring $O(k^3|S|)$ similarity comparisons, which is independent of the size of the target set. An interesting
    
[^170]: 从刚体图像中预测3D旋转动力学（未知质量分布）

    Learning to predict 3D rotational dynamics from images of a rigid body with unknown mass distribution. (arXiv:2209.11355v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.11355](http://arxiv.org/abs/2209.11355)

    该研究提出了一种基于物理知识的神经网络模型，通过多级预测流程，从刚体图像序列中预测3D旋转动力学，解决了标准深度学习方法无法揭示体内质量分布影响的问题。

    

    在许多实际情况下，当低维度测量不可用时，会有自由旋转的3D刚体的图像观察。然而，图像数据的高维数阻止了使用经典估计技术来学习动态。标准深度学习方法的有用性也受限于一个刚体图像无法揭示体内质量分布，而质量分布与初始角速度一起决定刚体旋转方式。我们提出了一种基于物理知识的神经网络模型，从图像序列中估计和预测3D旋转动力学。我们使用多级预测流程实现了这一目标，该流程将单个图像映射到与 $\mathbf{SO}(3)$ 同胚的潜在表示中，从潜在对中计算角速度，并使用Hamilton运动方程预测未来的潜在状态。我们在新的旋转刚体数据集上展示了我们方法的有效性。

    In many real-world settings, image observations of freely rotating 3D rigid bodies, may be available when low-dimensional measurements are not. However, the high-dimensionality of image data precludes the use of classical estimation techniques to learn the dynamics. The usefulness of standard deep learning methods is also limited because an image of a rigid body reveals nothing about the distribution of mass inside the body, which, together with initial angular velocity, is what determines how the body will rotate. We present a physics-informed neural network model to estimate and predict 3D rotational dynamics from image sequences. We achieve this using a multi-stage prediction pipeline that maps individual images to a latent representation homeomorphic to $\mathbf{SO}(3)$, computes angular velocities from latent pairs, and predicts future latent states using the Hamiltonian equations of motion. We demonstrate the efficacy of our approach on new rotating rigid-body datasets of sequenc
    
[^171]: 使用基于注意力的分层图池化解释药物组合的协同机制

    Interpreting the Mechanism of Synergism for Drug Combinations Using Attention-Based Hierarchical Graph Pooling. (arXiv:2209.09245v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2209.09245](http://arxiv.org/abs/2209.09245)

    这项研究开发了一种可解释的图神经网络模型，通过挖掘重要的亚分子网络，揭示了药物组合的协同机制。

    

    协同药物组合为增强治疗效果和减少不良反应提供了巨大潜力。然而，由于未知的疾病信号传导途径，有效且协同的药物组合预测仍然是一个未解决的问题。尽管提出了多种深度学习（AI）模型来定量预测药物组合的协同作用，但现有深度学习方法的主要局限性是它们本质上不可解释，这使得AI模型的结论对人类专家不透明，从而限制了模型的鲁棒性和在现实世界中人工智能医疗中的实施能力。本文提出了一种可解释的图神经网络（GNN），通过挖掘重要性较大的亚分子网络，揭示了潜在的关键治疗靶点和协同机制（MoS）。可解释的GNN预测模型的关键点是一种新颖的图池化方法。

    Synergistic drug combinations provide huge potentials to enhance therapeutic efficacy and to reduce adverse reactions. However, effective and synergistic drug combination prediction remains an open question because of the unknown causal disease signaling pathways. Though various deep learning (AI) models have been proposed to quantitatively predict the synergism of drug combinations, the major limitation of existing deep learning methods is that they are inherently not interpretable, which makes the conclusions of AI models untransparent to human experts, henceforth limiting the robustness of the model conclusion and the implementation ability of these models in real-world human--AI healthcare. In this paper, we develop an interpretable graph neural network (GNN) that reveals the underlying essential therapeutic targets and the mechanism of the synergy (MoS) by mining the sub-molecular network of great importance. The key point of the interpretable GNN prediction model is a novel graph
    
[^172]: 从可表达性到可执行性：在有限范围内实现自顶向下的自动化开发的神经符号框架

    Towards Top-Down Automated Development in Limited Scopes: A Neuro-Symbolic Framework from Expressibles to Executables. (arXiv:2209.01566v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2209.01566](http://arxiv.org/abs/2209.01566)

    本研究提出了一个神经符号框架，从可表达性到可执行性实现了自顶向下的自动化开发。通过建立代码分类法和使用语义金字塔来关联文本数据和代码数据，我们可以改进深度代码生成的效果。

    

    深度代码生成是软件工程中深度学习的一个主题，采用神经模型为预期功能生成代码。由于端到端神经方法缺乏领域知识和软件层次意识，它们在项目级任务上表现不佳。为了系统地探索代码生成的潜在改进，我们让其参与从“可表达性”到“可执行性”的自顶向下开发，这在有限的范围内是可能的。在这个过程中，它从大量的样本、特征和知识中受益。作为基础，我们建议在代码数据上建立一个分类法，即代码分类法，利用代码信息的分类。此外，我们引入了一个三层语义金字塔(SP)来关联文本数据和代码数据。它识别不同抽象层次的信息，从而引入了关于开发的领域知识，并揭示了软件的层次结构。

    Deep code generation is a topic of deep learning for software engineering (DL4SE), which adopts neural models to generate code for the intended functions. Since end-to-end neural methods lack domain knowledge and software hierarchy awareness, they tend to perform poorly w.r.t project-level tasks. To systematically explore the potential improvements of code generation, we let it participate in the whole top-down development from \emph{expressibles} to \emph{executables}, which is possible in limited scopes. In the process, it benefits from massive samples, features, and knowledge. As the foundation, we suggest building a taxonomy on code data, namely code taxonomy, leveraging the categorization of code information. Moreover, we introduce a three-layer semantic pyramid (SP) to associate text data and code data. It identifies the information of different abstraction levels, and thus introduces the domain knowledge on development and reveals the hierarchy of software. Furthermore, we propo
    
[^173]: 标量输入和函数输出的神经网络

    Neural Networks for Scalar Input and Functional Output. (arXiv:2208.05776v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2208.05776](http://arxiv.org/abs/2208.05776)

    该论文提出了一种解决标量输入和函数输出之间回归问题的方法，使用前馈神经网络预测函数响应。该方法适用于大量预测变量或非线性关系，并可以控制预测曲线的平滑程度。在实验中验证了方法的有效性。

    

    在一组标量预测变量上回归函数响应可以是一项具有挑战性的任务，特别是当有大量预测变量或者预测变量与响应之间的关系是非线性的时候。在这项工作中，我们提出了一个解决方案：使用前馈神经网络（NN）预测标量输入下的函数响应。首先，我们将函数响应转化为有限维度表示，并构建一个输出该表示的神经网络。然后，我们提出通过目标函数修改神经网络的输出，并引入不同的目标函数来进行网络训练。所提出的模型适用于均匀和不均匀间隔的数据，并可以进一步应用平滑惩罚项来控制预测曲线的平滑程度。实现这些特性的困难在于定义可以进行反向传播的目标函数。在我们的实验中，我们展示了我们的方法在多个数据集上的有效性。

    The regression of a functional response on a set of scalar predictors can be a challenging task, especially if there is a large number of predictors, or the relationship between those predictors and the response is nonlinear. In this work, we propose a solution to this problem: a feed-forward neural network (NN) designed to predict a functional response using scalar inputs. First, we transform the functional response to a finite-dimensional representation and construct an NN that outputs this representation. Then, we propose to modify the output of an NN via the objective function and introduce different objective functions for network training. The proposed models are suited for both regularly and irregularly spaced data, and a roughness penalty can be further applied to control the smoothness of the predicted curve. The difficulty in implementing both those features lies in the definition of objective functions that can be back-propagated. In our experiments, we demonstrate that our 
    
[^174]: Aggregation图神经网络的稳定性研究

    Stability of Aggregation Graph Neural Networks. (arXiv:2207.03678v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.03678](http://arxiv.org/abs/2207.03678)

    本文研究了聚合图神经网络（Agg-GNNs）的稳定性特性，并推导了稳定性界限与第一层滤波器属性之间的关系。

    

    本文研究了聚合图神经网络（Agg-GNNs）在考虑底层图变化时的稳定性特性。Agg-GNN是一种混合架构，其中将信息定义在图的节点上，但在经过多次图移操作后，通过欧几里得CNN在节点上进行逐块处理。我们推导了与通用Agg-GNN关联的映射算子的稳定性界限，并指定了这种算子在何种条件下可以对变形保持稳定。我们证明了稳定性界限由作用于每个节点的CNN的第一层滤波器的属性定义。此外，我们还展示了聚合数量、滤波器的选择性以及稳定性常数的大小之间存在紧密关系。我们还得出了结论：在Agg-GNNs中，映射算子的选择性仅与CNN阶段的第一层滤波器的属性有关。这显示了一个重要的发现。

    In this paper we study the stability properties of aggregation graph neural networks (Agg-GNNs) considering perturbations of the underlying graph. An Agg-GNN is a hybrid architecture where information is defined on the nodes of a graph, but it is processed block-wise by Euclidean CNNs on the nodes after several diffusions on the graph shift operator. We derive stability bounds for the mapping operator associated to a generic Agg-GNN, and we specify conditions under which such operators can be stable to deformations. We prove that the stability bounds are defined by the properties of the filters in the first layer of the CNN that acts on each node. Additionally, we show that there is a close relationship between the number of aggregations, the filter's selectivity, and the size of the stability constants. We also conclude that in Agg-GNNs the selectivity of the mapping operators is tied to the properties of the filters only in the first layer of the CNN stage. This shows a substantial d
    
[^175]: 在自适应子模块最大化中的群体平等

    Group Equality in Adaptive Submodular Maximization. (arXiv:2207.03364v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.03364](http://arxiv.org/abs/2207.03364)

    本论文研究了非自适应和自适应情况下，受群体平等约束的经典子模块最大化问题。研究发现，现有算法没有考虑公平性约束，导致一些特定群体的欠代表或过代表。因此，我们研究了在群体平等约束下选择一组项以最大化子模块效用函数。

    

    本论文研究了在非自适应和自适应设置下，受群体平等约束的经典子模块最大化问题。已经证明，许多机器学习应用的效用函数，包括数据汇总、社交网络中的影响最大化和个性化推荐，满足子模块性质。因此，在许多应用中，子模块最大化的核心是在各种约束下选择最具代表性的项（例如，数据点）。然而，大多数现有算法的设计没有考虑公平性约束，导致某些特定群体的欠代表或过代表。这激发了我们研究具有群体平等的子模块最大化问题，即在群体平等约束下选择一组项以最大化一个（可能非单调）的子模块效用函数。

    In this paper, we study the classic submodular maximization problem subject to a group equality constraint under both non-adaptive and adaptive settings. It has been shown that the utility function of many machine learning applications, including data summarization, influence maximization in social networks, and personalized recommendation, satisfies the property of submodularity. Hence, maximizing a submodular function subject to various constraints can be found at the heart of many of those applications. On a high level, submodular maximization aims to select a group of most representative items (e.g., data points). However, the design of most existing algorithms does not incorporate the fairness constraint, leading to under- or over-representation of some particular groups. This motivates us to study the submodular maximization problem with group equality, where we aim to select a group of items to maximize a (possibly non-monotone) submodular utility function subject to a group equ
    
[^176]: 非光滑优化的随机坐标半梯度法

    Randomized Coordinate Subgradient Method for Nonsmooth Optimization. (arXiv:2206.14981v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2206.14981](http://arxiv.org/abs/2206.14981)

    本文提出了适用于非光滑优化问题的随机坐标半梯度法，该方法在每次迭代只更新一个坐标块，考虑了目标函数的线性有界次梯度假设，并在凸和非凸情况下建立了全面的收敛性分析，收敛速率为$\widetilde{\mathcal{O}}(1/\sqrt{k})$和$\mathcal{O}(1/k)$。

    

    本文提出了适用于解决非光滑凸性和非光滑非凸性（非光滑弱凸性）优化问题的“随机坐标半梯度法”（RCS）。RCS在每次迭代中随机选择一个坐标块进行更新，比更新所有坐标更实用。我们考虑了目标函数的线性有界次梯度假设，该假设比传统的Lipschitz连续性假设更为通用，以适应实际情况。此外，我们基于这种广义的Lipschitz类型假设对RCS在凸和非凸情况下进行了全面的收敛性分析。具体来说，当$f$为非光滑凸函数时，我们在期望上建立了$\widetilde{\mathcal{O}}(1/\sqrt{k})$收敛速率，以及在次优间隙方面的$\tilde o(1/\sqrt{k})$几乎肯定渐近收敛速率。如果$f$进一步满足全局二次增长条件，则显示了改进的$\mathcal{O}(1/k)$速率。

    In this work, we propose the {Randomized Coordinate Subgradient method} (RCS) for solving nonsmooth convex and nonsmooth nonconvex (nonsmooth weakly convex) optimization problems. RCS randomly selects one block coordinate to update at each iteration, making it more practical than updating all coordinates. We consider the linearly bounded subgradients assumption for the objective function, which is more general than the traditional Lipschitz continuity assumption, to account for practical scenarios. We then conduct thorough convergence analysis for RCS in both convex and nonconvex cases based on this generalized Lipschitz-type assumption. Specifically, we establish the $\widetilde{\mathcal{O}}(1/\sqrt{k})$ convergence rate in expectation and the $\tilde o(1/\sqrt{k})$ almost sure asymptotic convergence rate in terms of suboptimality gap when $f$ is nonsmooth convex. If $f$ further satisfies the global quadratic growth condition, the improved $\mathcal{O}(1/k)$ rate is shown in terms of 
    
[^177]: 从参与度动态和多学习者重新训练中产生的紧急细分

    Emergent segmentation from participation dynamics and multi-learner retraining. (arXiv:2206.02667v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.02667](http://arxiv.org/abs/2206.02667)

    该论文通过研究基于数据驱动服务的参与和重新训练动态，发现当学习者和用户子群具有风险减少性质时，唯一的稳定均衡是细分的，将子群分配给单个学习者。功利主义社会最优是一个稳定均衡。

    

    在基于数据驱动的服务中选择参与，往往基于该服务的质量，影响了服务学习和改进的能力。我们研究了当学习者和用户子群都具有风险减少性质时，参与和重新训练的动态生成的情况，其中包括了梯度下降、乘法权重等广泛的更新方法。举个例子，假设个体选择在社交媒体平台上花费时间的比例与每个平台对他们的工作效果成比例。每个平台还会收集其活跃用户的数据，并用梯度步骤更新参数。对于这个例子和我们的一般动态类别，我们展示了唯一的渐近稳定均衡是细分的，将子群分配给单个学习者。在温和的假设下，功利主义社会最优是一个稳定均衡。与先前的工作相反，先前的工作显示重复的风险最小化可能不会对韧性和利益进行任何保证。

    The choice to participate in a data-driven service, often made on the basis of quality of that service, influences the ability of the service to learn and improve. We study the participation and retraining dynamics that arise when both the learners and sub-populations of users are \emph{risk-reducing}, which cover a broad class of updates including gradient descent, multiplicative weights, etc. Suppose, for example, that individuals choose to spend their time amongst social media platforms proportionally to how well each platform works for them. Each platform also gathers data about its active users, which it uses to update parameters with a gradient step. For this example and for our general class of dynamics, we show that the only asymptotically stable equilibria are segmented, with sub-populations allocated to a single learner. Under mild assumptions, the utilitarian social optimum is a stable equilibrium. In contrast to previous work, which shows that repeated risk minimization can
    
[^178]: 回归模型中的删除和插入测试

    Deletion and Insertion Tests in Regression Models. (arXiv:2205.12423v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.12423](http://arxiv.org/abs/2205.12423)

    本研究提出了一种在回归模型中评估删除和插入测试的方法，用于确定解释性人工智能中最重要的特征。我们通过比较不同算法计算的特征重要性，发现Kernel SHAP在综合性能方面表现最佳。我们还提出了一种计算更快速的替代指标，适用于回归设置。

    

    解释性人工智能（XAI）中的一个基本任务是确定黑盒函数$f$预测背后最重要的特征。Petsiuk等人（2018）的插入和删除测试可以用来评判对于分类中像素从重要到不重要进行排序的算法的质量。在回归问题上，我们建立了一个公式，以$f$的主效应和交互作用来衡量其曲线下面积（AUC）的标准。我们找到了在输入随机顺序下AUC的期望值的表达式，并提出了一个适用于回归设置的直线上方面积的替代指标。我们使用这个指标将集成梯度（IG）计算的特征重要性与Kernel SHAP（KS）、LIME、DeepLIFT、vanilla gradient和input$\times$gradient方法计算的特征重要性进行比较。在我们考虑的两个数据集中，KS的整体表现最好，但计算代价很高。我们发现IG在一些数据集上和KS表现相近，但计算更快速。

    A basic task in explainable AI (XAI) is to identify the most important features behind a prediction made by a black box function $f$. The insertion and deletion tests of Petsiuk et al. (2018) can be used to judge the quality of algorithms that rank pixels from most to least important for a classification. Motivated by regression problems we establish a formula for their area under the curve (AUC) criteria in terms of certain main effects and interactions in an anchored decomposition of $f$. We find an expression for the expected value of the AUC under a random ordering of inputs to $f$ and propose an alternative area above a straight line for the regression setting. We use this criterion to compare feature importances computed by integrated gradients (IG) to those computed by Kernel SHAP (KS) as well as LIME, DeepLIFT, vanilla gradient and input$\times$gradient methods. KS has the best overall performance in two datasets we consider but it is very expensive to compute. We find that IG 
    
[^179]: 结构化跨度选择器

    A Structured Span Selector. (arXiv:2205.03977v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.03977](http://arxiv.org/abs/2205.03977)

    提出了一种新颖的基于语法的结构化跨度选择模型，通过利用部分跨度级注释，摒弃了贪心跨度选择方案，为共指消解和语义角色标注任务带来了实证改进。

    

    许多自然语言处理任务，例如共指消解和语义角色标注，需要选择文本跨度并对其进行决策。这些任务的一种典型方法是对所有可能的跨度进行评分，并贪婪地选择跨度进行特定任务的下游处理。然而，这种方法并没有结合任何归纳偏置来确定应该选择哪种跨度，例如选择的跨度往往是句法成分。在本文中，我们提出了一种新颖的基于语法的结构化跨度选择模型，该模型学习利用为这些问题提供的部分跨度级注释。与先前的方法相比，我们的方法摒弃了启发式的贪婪跨度选择方案，使我们能够在最佳一组跨度上对下游任务进行建模。我们在两个常见的跨度预测任务：共指消解和语义角色标注上评估了我们的模型，并展示了实证改进。

    Many natural language processing tasks, e.g., coreference resolution and semantic role labeling, require selecting text spans and making decisions about them. A typical approach to such tasks is to score all possible spans and greedily select spans for task-specific downstream processing. This approach, however, does not incorporate any inductive bias about what sort of spans ought to be selected, e.g., that selected spans tend to be syntactic constituents. In this paper, we propose a novel grammar-based structured span selection model which learns to make use of the partial span-level annotation provided for such problems. Compared to previous approaches, our approach gets rid of the heuristic greedy span selection scheme, allowing us to model the downstream task on an optimal set of spans. We evaluate our model on two popular span prediction tasks: coreference resolution and semantic role labeling. We show empirical improvements on both.
    
[^180]: 流形上的Riemannian Hamiltonian方法用于min-max优化问题

    Riemannian Hamiltonian methods for min-max optimization on manifolds. (arXiv:2204.11418v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2204.11418](http://arxiv.org/abs/2204.11418)

    本文研究了流形上的min-max优化问题，并引入了Riemannian Hamiltonian方法作为其代理方法。通过最小化Hamiltonian函数，可以得到所需的min-max鞍点。该方法在geodesic-bilinear优化问题中具有挑战性，但通过解决代理问题可以得到全局最优搜索方向。该方法在多个应用中展示了其有效性。

    

    本文研究了流形上的min-max优化问题。我们引入了一个Riemannian Hamiltonian函数，其最小化作为解决原始min-max问题的代理。在Riemannian Polyak-{\L}ojasiewicz条件下，其最小值对应于所需的min-max鞍点。我们还提供了满足此条件的情况。特别是对于geodesic-bilinear优化，在解决代理问题的情况下，可以得到正确的全局最优搜索方向，而在min-max形式化中变得具有挑战性。为了最小化Hamiltonian函数，我们提出了Riemannian Hamiltonian方法（RHM）并提出了它们的收敛性分析。我们将RHM扩展到包括共识正则化和随机设置。我们通过应用如子空间鲁棒Wasserstein距离、神经网络的鲁棒训练和生成对抗网络等来说明所提出的RHM的有效性。

    In this paper, we study min-max optimization problems on Riemannian manifolds. We introduce a Riemannian Hamiltonian function, minimization of which serves as a proxy for solving the original min-max problems. Under the Riemannian Polyak--{\L}ojasiewicz condition on the Hamiltonian function, its minimizer corresponds to the desired min-max saddle point. We also provide cases where this condition is satisfied. For geodesic-bilinear optimization in particular, solving the proxy problem leads to the correct search direction towards global optimality, which becomes challenging with the min-max formulation. To minimize the Hamiltonian function, we propose Riemannian Hamiltonian methods (RHM) and present their convergence analyses. We extend RHM to include consensus regularization and to the stochastic setting. We illustrate the efficacy of the proposed RHM in applications such as subspace robust Wasserstein distance, robust training of neural networks, and generative adversarial networks.
    
[^181]: ConceptEvo：解读深度学习训练中的概念演变

    ConceptEvo: Interpreting Concept Evolution in Deep Learning Training. (arXiv:2203.16475v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.16475](http://arxiv.org/abs/2203.16475)

    ConceptEvo是一个统一的深度神经网络解释框架，可以在训练过程中揭示概念的产生和演变，并通过人机评估和实验证明其发现对模型和预测具有重要意义。

    

    本文提出了ConceptEvo，一个用于深度神经网络（DNNs）的统一解释框架，可以揭示训练过程中学习概念的产生和演变。我们的工作填补了DNN解释研究中的一个关键空白，因为现有方法仅关注训练后的解释。ConceptEvo提出了两个新颖的技术贡献：（1）一种生成统一语义空间的算法，可以在训练过程中进行不同模型的并行比较；（2）一种发现和量化类别预测中重要概念演变的算法。通过与260名参与者进行大规模人机评估和定量实验，我们展示了ConceptEvo可以发现不同模型之间有意义且对预测重要的演变。ConceptEvo适用于现代（ConvNeXt）和经典的DNNs（例如VGGs，InceptionV3）。

    We present ConceptEvo, a unified interpretation framework for deep neural networks (DNNs) that reveals the inception and evolution of learned concepts during training. Our work fills a critical gap in DNN interpretation research, as existing methods focus on post-hoc interpretation after training. ConceptEvo presents two novel technical contributions: (1) an algorithm that generates a unified semantic space that enables side-by-side comparison of different models during training; and (2) an algorithm that discovers and quantifies important concept evolutions for class predictions. Through a large-scale human evaluation with 260 participants and quantitative experiments, we show that ConceptEvo discovers evolutions across different models that are meaningful to humans and important for predictions. ConceptEvo works for both modern (ConvNeXt) and classic DNNs (e.g., VGGs, InceptionV3).
    
[^182]: 深度残差误差和“学霸技巧”用于引力波模拟建模

    Deep Residual Error and Bag-of-Tricks Learning for Gravitational Wave Surrogate Modeling. (arXiv:2203.08434v2 [astro-ph.IM] UPDATED)

    [http://arxiv.org/abs/2203.08434](http://arxiv.org/abs/2203.08434)

    利用深度学习方法加速引力波代理波形的建立，通过添加第二个网络模型，减少了波形的不匹配程度，并探索了其他方法来提高代理模型的准确性。

    

    在引力波天文学中，已经使用深度学习方法加快了旋转对齐黑洞二进制系统的代理波形构建，以及其他应用。我们面临着对人工神经网络模型的残差误差建模的挑战，该模型模拟代理波形展开系数的误差（特别是波形的相位），我们证明这些误差具有足够的结构可以被第二个网络学习到。通过添加第二个网络，我们能够将验证集中波形的最大不匹配程度减少了13.4倍。我们还尝试了几种改进代理模型准确性的方法，如利用波形之间的相似性、训练集的增强、输入空间的解剖、使用专用网络预测输出的系数以及输出增强。在几种情况下，可以观察到一些小的改进，但最显著的改进仍然是...

    Deep learning methods have been employed in gravitational-wave astronomy to accelerate the construction of surrogate waveforms for the inspiral of spin-aligned black hole binaries, among other applications. We face the challenge of modeling the residual error of an artificial neural network that models the coefficients of the surrogate waveform expansion (especially those of the phase of the waveform) which we demonstrate has sufficient structure to be learnable by a second network. Adding this second network, we were able to reduce the maximum mismatch for waveforms in a validation set by 13.4 times. We also explored several other ideas for improving the accuracy of the surrogate model, such as the exploitation of similarities between waveforms, the augmentation of the training set, the dissection of the input space, using dedicated networks per output coefficient and output augmentation. In several cases, small improvements can be observed, but the most significant improvement still 
    
[^183]: AdaTerm: 自适应T分布估计稳健矩用于噪声健壮随机梯度优化

    AdaTerm: Adaptive T-Distribution Estimated Robust Moments for Noise-Robust Stochastic Gradient Optimization. (arXiv:2201.06714v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.06714](http://arxiv.org/abs/2201.06714)

    AdaTerm是一种自适应T分布估计稳健矩的方法，提供了对优化算法的统一处理。

    

    随着深度学习应用的增加，从各种来源如测量误差、错误标记和估计代理输入/输出中受损的数据集不可避免地影响了优化结果，提高优化算法对噪声的稳健性已成为常见做法。之前的研究发现，Adam-like随机梯度下降优化器中使用的一阶矩可以基于学生t分布进行修改。然而，这种修改只影响了一阶矩，其他关联的统计量保持不变，导致了所假设模型的一致性问题。本文提出了AdaTerm，一种新颖方法，将学生t分布用于推导一阶矩和所有关联的统计量，从而提供了对优化算法的统一处理。

    With the increasing practicality of deep learning applications, practitioners are inevitably faced with datasets corrupted by noise from various sources such as measurement errors, mislabeling, and estimated surrogate inputs/outputs that can adversely impact the optimization results. It is a common practice to improve the optimization algorithm's robustness to noise, since this algorithm is ultimately in charge of updating the network parameters. Previous studies revealed that the first-order moment used in Adam-like stochastic gradient descent optimizers can be modified based on the Student's t-distribution. While this modification led to noise-resistant updates, the other associated statistics remained unchanged, resulting in inconsistencies in the assumed models. In this paper, we propose AdaTerm, a novel approach that incorporates the Student's t-distribution to derive not only the first-order moment but also all the associated statistics. This provides a unified treatment of the o
    
[^184]: 向具有内在反馈的交互式强化学习迈进

    Towards Interactive Reinforcement Learning with Intrinsic Feedback. (arXiv:2112.01575v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2112.01575](http://arxiv.org/abs/2112.01575)

    这篇论文综述了交互式强化学习与内在反馈的关系，强调了将人类输入与RL算法结合的重要性，并指出在这一关键联系上的探索仍有待加强。

    

    过去十年，强化学习（RL）和脑机接口（BCI）取得了显著的发展。随着人们对人机协同（HITL）的兴趣日益增长，将人类输入与RL算法相结合已经催生了交互式RL的子领域。同时，BCI领域长期以来一直致力于从神经活动中提取有关人机交互的信息性脑信号。这两个领域之间的关键联系在于将神经活动解释为反馈，以便可以应用交互式RL方法。我们将这种新兴的反馈介质称为内在反馈。尽管内在反馈能够自动传达甚至无意识地传达，但对于这个关键联系的适当探索几乎未受到两个领域的关注。因此，为了促进深入理解和更有效的利用，我们提供了一个教程风格的综述，涵盖了动机、方法和开放问题。

    Reinforcement learning (RL) and brain-computer interfaces (BCI) have experienced significant growth over the past decade. With rising interest in human-in-the-loop (HITL), incorporating human input with RL algorithms has given rise to the sub-field of interactive RL. Adjacently, the field of BCI has long been interested in extracting informative brain signals from neural activity for use in human-computer interactions. A key link between these fields lies in the interpretation of neural activity as feedback such that interactive RL approaches can be employed. We denote this new and emerging medium of feedback as intrinsic feedback. Despite intrinsic feedback's ability to be conveyed automatically and even unconsciously, proper exploration surrounding this key link has largely gone unaddressed by both communities. Thus, to help facilitate a deeper understanding and a more effective utilization, we provide a tutorial-style review covering the motivations, approaches, and open problems of
    
[^185]: 无监督选择性标注以实现更有效的半监督学习

    Unsupervised Selective Labeling for More Effective Semi-Supervised Learning. (arXiv:2110.03006v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.03006](http://arxiv.org/abs/2110.03006)

    本文研究了在半监督学习中如何选择性地标注无标签数据，以提高学习效果。作者提出了一种无监督的选择性标注方法，通过选择聚类原型来获得代表性和多样性的标注数据，证明了该方法在性能上优于传统的主动学习方法。

    

    针对一个无标签数据集和一个注释预算，我们研究了如何有选择地标注固定数量的实例，以便在这样一个部分标记的数据集上进行半监督学习（SSL）更加有效。我们关注选择适当的数据进行标注，除了通常的SSL将标签从有标记数据传播到其余未标记数据的过程。这个实例选择任务很有挑战性，因为在没有任何有标记数据的情况下，我们不知道学习的目标应该是什么。直观地说，无论下游任务是什么，要标注的实例必须是具有代表性和多样性的：前者有助于将标签传播到未标记数据，而后者保证了整个数据集的覆盖范围。我们通过在预训练特征空间中选择聚类原型或在特征优化过程中选择聚类原型来捕捉这个想法，而都不需要标签。我们的无监督选择性标注方法在有标记数据的情况下始终比最先进的主动学习方法提高了8％。

    Given an unlabeled dataset and an annotation budget, we study how to selectively label a fixed number of instances so that semi-supervised learning (SSL) on such a partially labeled dataset is most effective. We focus on selecting the right data to label, in addition to usual SSL's propagating labels from labeled data to the rest unlabeled data. This instance selection task is challenging, as without any labeled data we do not know what the objective of learning should be. Intuitively, no matter what the downstream task is, instances to be labeled must be representative and diverse: The former would facilitate label propagation to unlabeled data, whereas the latter would ensure coverage of the entire dataset. We capture this idea by selecting cluster prototypes, either in a pretrained feature space, or along with feature optimization, both without labels. Our unsupervised selective labeling consistently improves SSL methods over state-of-the-art active learning given labeled data, by 8
    
[^186]: 用多少预训练足以发现一个好的子网络？

    How much pre-training is enough to discover a good subnetwork?. (arXiv:2108.00259v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2108.00259](http://arxiv.org/abs/2108.00259)

    本论文研究了神经网络剪枝中预训练的数量对剪枝后网络性能的影响，并提出了一个简单的理论界限，该界限以数据集大小的对数关系决定了预训练的迭代次数。

    

    神经网络剪枝对于在预训练的密集网络结构中发现高效、高性能的子网络非常有用。通常情况下，它涉及到一个三步过程——预训练、剪枝和重新训练，这在计算上是昂贵的，因为密集模型必须完全预训练。虽然先前的工作通过实验证明了预训练的数量与剪枝网络性能之间的关系，但对于这种依赖关系的理论描述仍然缺失。为了数学分析密集网络预训练所需的数量，以便剪枝后的网络能够表现良好，我们在二层全连接网络上发现了一个简单的理论界限，超过这个界限，通过贪婪前向选择的剪枝可以达到良好的训练误差。有趣的是，这个阈值被证明与数据集的大小呈对数依赖关系。

    Neural network pruning is useful for discovering efficient, high-performing subnetworks within pre-trained, dense network architectures. More often than not, it involves a three-step process -- pre-training, pruning, and re-training -- that is computationally expensive, as the dense model must be fully pre-trained. While previous work has revealed through experiments the relationship between the amount of pre-training and the performance of the pruned network, a theoretical characterization of such dependency is still missing. Aiming to mathematically analyze the amount of dense network pre-training needed for a pruned network to perform well, we discover a simple theoretical bound in the number of gradient descent pre-training iterations on a two-layer, fully-connected network, beyond which pruning via greedy forward selection [61] yields a subnetwork that achieves good training error. Interestingly, this threshold is shown to be logarithmically dependent upon the size of the dataset,
    
[^187]: 深入深度学习

    Dive into Deep Learning. (arXiv:2106.11342v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.11342](http://arxiv.org/abs/2106.11342)

    《深入深度学习》是一本旨在使深度学习易于理解的开源书籍，提供从概念到代码的教学资源，旨在成为成为应用机器学习科学家的起点，并允许社区快速更新和互动讨论。

    

    这本开源书是我们的努力，让深度学习变得易于理解，教读者概念、背景和代码。整本书都是在Jupyter笔记本中起草的，与独立的代码无缝集成了说明图、数学和互动示例。我们的目标是提供一个资源，既可以自由使用，又可以提供足够的技术深度，为成为应用机器学习科学家的起点; 包括可运行的代码，向读者展示如何实践解决问题; 允许快速更新，不仅由我们，还由整个社区更新; 接受技术细节的互动讨论和解答问题的论坛。

    This open-source book represents our attempt to make deep learning approachable, teaching readers the concepts, the context, and the code. The entire book is drafted in Jupyter notebooks, seamlessly integrating exposition figures, math, and interactive examples with self-contained code. Our goal is to offer a resource that could (i) be freely available for everyone; (ii) offer sufficient technical depth to provide a starting point on the path to actually becoming an applied machine learning scientist; (iii) include runnable code, showing readers how to solve problems in practice; (iv) allow for rapid updates, both by us and also by the community at large; (v) be complemented by a forum for interactive discussion of technical details and to answer questions.
    
[^188]: 传输学习的共同直觉可以带来胜利或失败: 线性回归案例研究

    The Common Intuition to Transfer Learning Can Win or Lose: Case Studies for Linear Regression. (arXiv:2103.05621v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.05621](http://arxiv.org/abs/2103.05621)

    本论文研究了传输学习的基本过程，针对线性回归任务，通过利用源任务参数和目标任务训练数据，提出了一种传输学习方法。我们分析了该方法的泛化性能，并展示了其在解决线性回归中的泛化误差峰值方面的能力。此外，我们证明了在足够相关的任务中，该传输学习方法可以优于岭回归方法。

    

    我们研究了从源回归任务到目标回归任务的基本传输学习过程，包括在有比数据样本更多的学习参数的过参数化设置中。目标任务的学习通过使用其训练数据和先前计算的源任务参数来解决。我们将目标任务的传输学习方法定义为带有正则化的线性回归优化，其中正则化项是待学习的目标参数与已学习的源参数之间的距离。我们分析地表征了我们的传输学习方法的泛化性能，并展示了它解决线性回归的最小L2范数解中的泛化误差峰值的能力。此外，我们证明了对于足够相关的任务来说，经过最佳调优的传输学习方法可以胜过最佳调优的岭回归方法，即使真实参数向量符合最小L2范数解。

    We study a fundamental transfer learning process from source to target linear regression tasks, including overparameterized settings where there are more learned parameters than data samples. The target task learning is addressed by using its training data together with the parameters previously computed for the source task. We define a transfer learning approach to the target task as a linear regression optimization with a regularization on the distance between the to-be-learned target parameters and the already-learned source parameters. We analytically characterize the generalization performance of our transfer learning approach and demonstrate its ability to resolve the peak in generalization errors in double descent phenomena of the minimum L2-norm solution to linear regression. Moreover, we show that for sufficiently related tasks, the optimally tuned transfer learning approach can outperform the optimally tuned ridge regression method, even when the true parameter vector conform
    

