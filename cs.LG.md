# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [NLP-based Cross-Layer 5G Vulnerabilities Detection via Fuzzing Generated Run-Time Profiling.](http://arxiv.org/abs/2305.08226) | 本文提出一种基于NLP技术和自动生成运行时分析文件的创新方法，用于自动检测5G软件栈中的漏洞、意外行为和性能下降，试验表明其具有良好的效果和可扩展性。 |
| [^2] | [A Dataset Fusion Algorithm for Generalised Anomaly Detection in Homogeneous Periodic Time Series Datasets.](http://arxiv.org/abs/2305.08197) | 该论文介绍了一种名为“数据集融合”的新型数据集合成算法，可将来自多个同质数据集的周期信号融合为单个数据集，同时保留了通用异常检测的独特特征。在两个同质感应电动机（IM）故障数据集的3相电流数据的案例研究中表现出显著优势，有潜力在跨多个来源利用可用数据方面发挥作用。 |
| [^3] | [Is end-to-end learning enough for fitness activity recognition?.](http://arxiv.org/abs/2305.08191) | 该研究使用全面注释的健身活动视频数据集，表明基于像素的端到端学习可以与基于姿势估计的最先进的动作识别流程相竞争，同时还可以支持时间上的细粒度任务。 |
| [^4] | [An Optimal and Scalable Matrix Mechanism for Noisy Marginals under Convex Loss Functions.](http://arxiv.org/abs/2305.08175) | ResidualPlanner是一种用于带有高斯噪声的边缘的矩阵机制，既优化又可扩展，可以优化许多可以写成边际方差的凸函数的损失函数。 |
| [^5] | [Can Learning Deteriorate Control? Analyzing Computational Delays in Gaussian Process-Based Event-Triggered Online Learning.](http://arxiv.org/abs/2305.08169) | 该论文提出了一种适用于具有计算延迟的基于高斯过程的在线学习的新型事件触发器, 可以有效降低计算延迟。 |
| [^6] | [Latent Processes Identification From Multi-View Time Series.](http://arxiv.org/abs/2305.08164) | 该论文提出了一个名为MuLTI的新框架，采用了对比学习技术和排列机制来从多视角时间序列数据中识别潜在过程。 |
| [^7] | [Inverse Reinforcement Learning With Constraint Recovery.](http://arxiv.org/abs/2305.08130) | 本论文提出了一种带有约束恢复的逆强化学习算法，以针对约束马尔可夫决策过程问题。通过最大熵原理，将其视为一个受限制的非凸优化问题，并使用指数梯度下降算法来解决它。 |
| [^8] | [Unraveling Cold Start Enigmas in Predictive Analytics for OTT Media: Synergistic Meta-Insights and Multimodal Ensemble Mastery.](http://arxiv.org/abs/2305.08120) | 本研究提出了一种通用方法，通过利用元数据和采用多模型集成技术来解决OTT媒体预测分析中的冷启动问题，结果表明多模型集成方法显著提高了预测准确性。 |
| [^9] | [Automatic Generation of Attention Rules For Containment of Machine Learning Model Errors.](http://arxiv.org/abs/2305.08115) | 本文提出了一种将最可能出错的观察结果分离成“注意集”的自动化方法，以帮助机器学习模型的诊断和改进。采用基于特征切片的策略，具有人类可解释性、模型无关性和较少的辅助输入或知识，而且相较于其他方法，该方法的性能表现更佳。 |
| [^10] | [Balancing Privacy and Utility of Spatio-Temporal Data for Taxi-Demand Prediction.](http://arxiv.org/abs/2305.08107) | 本文提出了使用联合学习进行出租车需求预测的方法，该方法允许多个参与方训练机器学习模型并保持数据私密和安全。文章对于类别不平衡、数据稀缺和模型泛化等技术挑战提出了解决方案，最终在实际数据集上展示了具有竞争力的表现。 |
| [^11] | [Blockchain Transaction Fee Forecasting: A Comparison of Machine Learning Methods.](http://arxiv.org/abs/2305.08105) | 本文研究区块链交易费用预测，比较了多种机器学习方法，结合小波阈值去噪和矩阵剖面数据处理，预测多个时间跨度上的5分钟内最低煤气价格，为交易费用预测提供了一种可行的方法。 |
| [^12] | [Federated TD Learning over Finite-Rate Erasure Channels: Linear Speedup under Markovian Sampling.](http://arxiv.org/abs/2305.08104) | 该论文提出了一种适用于联邦学习的算法 QFedTD，在有限速抹通道下使用线性函数逼近以达到线性加速的效果，在强化学习方面具有实际应用价值。 |
| [^13] | [A machine learning-based viscoelastic-viscoplastic model for epoxy nanocomposites with moisture content.](http://arxiv.org/abs/2305.08102) | 本文提出了一种运用深度学习的本构模型，能够准确的捕捉与速率相关的应力-应变关系和一致的切线模量，以研究含湿度纳米颗粒/环氧纳米复合材料的循环粘弹-粘塑性-破坏行为。 |
| [^14] | [Conditional mean embeddings and optimal feature selection via positive definite kernels.](http://arxiv.org/abs/2305.08100) | 本文提出一种新的算子理论方法来解决条件平均嵌入问题，构造了非线性数据的基于优化的特征选择。通过使用正定核的凸集，得到多种希尔伯特空间和特征的实现方式。 |
| [^15] | [Self-supervised Neural Factor Analysis for Disentangling Utterance-level Speech Representations.](http://arxiv.org/abs/2305.08099) | 本文提出了一种自监督神经因子分析模型，使用HuBERT中的聚类方法来发现隐藏的声学单元，并使用这些单元对齐SSL模型的特征，从而产生解耦后的语音表示，从而为专门任务提供了一种基于Utterance水平的无监督学习目标。实验结果表明，SSNFA模型在说话人识别、语言识别和情感识别等各种任务中均显著优于现有的SSL模型，并且没有任何特定任务的微调或监督。 |
| [^16] | [Meta-DM: Applications of Diffusion Models on Few-Shot Learning.](http://arxiv.org/abs/2305.08092) | 本文提出了一个基于扩散模型的数据处理模块Meta-DM，可以轻松地集成到现有的少样本学习方法中，在监督和非监督设置中都能带来显著的性能提升。 |
| [^17] | [Optimization of Residential Demand Response Program Cost with Consideration for Occupants Thermal Comfort and Privacy.](http://arxiv.org/abs/2305.08077) | 本文提出一种非侵入性、准确且经济有效的方法，开发一个多目标仿真模型，用于智能住宅中的需求响应计划（DRP），以提高居民的经济效益和热舒适。 |
| [^18] | [Improving Defensive Distillation using Teacher Assistant.](http://arxiv.org/abs/2305.08076) | 该论文阐述了对抗攻击对深度神经网络的威胁，介绍了防御性蒸馏的方法并提出了利用教师助手来提高网络的鲁棒性。 |
| [^19] | [HiPerformer: Hierarchically Permutation-Equivariant Transformer for Time Series Forecasting.](http://arxiv.org/abs/2305.08073) | 本文提出了一种考虑股票组内和组间结构的预测模型，通过分层排列等变性的概念进行设计。实验结果表明该方法在预测上表现出色。 |
| [^20] | [A Survey of Federated Evaluation in Federated Learning.](http://arxiv.org/abs/2305.08070) | 本篇论文对现有联邦评估方法进行全面综述，阐述了联邦评估在客户端选择、激励机制设计、恶意攻击检测等方面的重要作用，探讨了联邦评估在增强FL性能方面的各种应用，并提出了未来的研究方向。 |
| [^21] | [Instance-Aware Repeat Factor Sampling for Long-Tailed Object Detection.](http://arxiv.org/abs/2305.08069) | 提出一个能解决长尾目标检测中样本分布不平衡的方法IRFS，将实例和图像计数结合应用到重新采样过程中，结果表明IRFS优于现有采样方法并取得了最新的结果。 |
| [^22] | [Improving End-to-End SLU performance with Prosodic Attention and Distillation.](http://arxiv.org/abs/2305.08067) | 本文提出了韵律关注和韵律蒸馏方法来利用韵律特征提高端到端语义理解（SLU）的性能，其中韵律蒸馏方法相对于基线方法在SLURP和STOP数据集上提高了8％和2％的意图分类准确性。 |
| [^23] | [Off-Policy Evaluation for Large Action Spaces via Conjunct Effect Modeling.](http://arxiv.org/abs/2305.08062) | 本研究提出了一个称为OffCEM的估计器，用于对大离散动作空间下上下文匹配策略进行离线策略评估。该估计器通过基于模型的奖励估计来处理残余因果效应，并在新的本地正确性条件下保持无偏性。结果表明，OffCEM在合成和实际大动作空间数据集上优于现有方法。 |
| [^24] | [CREMP: Conformer-Rotamer Ensembles of Macrocyclic Peptides for Machine Learning.](http://arxiv.org/abs/2305.08057) | CREMP是一个资源，包含超过3千万个大环肽构象形状，旨在快速开发和评估机器学习模型，以便更好地模拟大环肽的构象。 |
| [^25] | [Towards Understanding the Generalization of Graph Neural Networks.](http://arxiv.org/abs/2305.08048) | 本文探索了图神经网络的泛化；通过理论分析和实验结果发现了影响泛化间隙的体系结构因素。 |
| [^26] | [Using EEG Signals to Assess Workload during Memory Retrieval in a Real-world Scenario.](http://arxiv.org/abs/2305.08044) | 本研究使用EEG信号作为神经工效学的生理测量员评估了参与者在典型办公任务期间的记忆负荷。结果表明单显示器和双显示器设置下的记忆工作负荷之间存在显著差异，并且机器学习模型能够明确高负荷和低负荷的状态。 |
| [^27] | [Provable Multi-instance Deep AUC Maximization with Stochastic Pooling.](http://arxiv.org/abs/2305.08040) | 本文提出了在多实例学习中使用深度AUC最大化（DAM）的方法，并根据包含大量实例的情况下训练的计算挑战，提出了一种基于方差减少的随机池化方法，使得只需对每个包进行少量采样即可计算MIDAM模型，提高了效率和准确性。 |
| [^28] | [Small-data Reduced Order Modeling of Chaotic Dynamics through SyCo-AE: Synthetically Constrained Autoencoders.](http://arxiv.org/abs/2305.08036) | 本文利用自编码器对混沌动力学进行小数据降阶建模，通过在降阶空间中施加合成约束来保持完全非线性和高度不稳定的自由度的同时，防止了发散。使用这种方法，即便是使用更少的数据，也可以产生低误差的中长期预测。 |
| [^29] | [Grasping Extreme Aerodynamics on a Low-Dimensional Manifold.](http://arxiv.org/abs/2305.08024) | 研究探索处理极端气动学问题的基本物理机制方法。 |
| [^30] | [TIPS: Topologically Important Path Sampling for Anytime Neural Networks.](http://arxiv.org/abs/2305.08021) | TIPS是一种自动设计AnytimeNNs框架，通过识别贡献最大的路径来提高收敛速度和测试准确率，比现有方法提高了2%-6.6%的准确率，在准确率-FLOPs之间取得了最佳平衡。 |
| [^31] | [DRew: Dynamically Rewired Message Passing with Delay.](http://arxiv.org/abs/2305.08018) | 本文提出了一种能够应用于任何MPNN结构的框架，执行基于层的动态重连来确保逐渐密集化的图形。同时引入了一种延迟机制，允许跨层节点之间的跳跃连接。 |
| [^32] | [Surface EMG-Based Inter-Session/Inter-Subject Gesture Recognition by Leveraging Lightweight All-ConvNet and Transfer Learning.](http://arxiv.org/abs/2305.08014) | 本论文提出了一种轻量级全卷积神经网络+迁移学习方法，能够在跨场景手势识别任务中有效解决数据变异性问题。 |
| [^33] | [Information Bottleneck Analysis of Deep Neural Networks via Lossy Compression.](http://arxiv.org/abs/2305.08013) | 本文提出了一种综合框架，用于对一般神经网络进行信息瓶颈分析，以研究训练过程中的拟合和压缩阶段。通过该分析，可以更好地理解深度神经网络的泛化能力。 |
| [^34] | [Successive Affine Learning for Deep Neural Networks.](http://arxiv.org/abs/2305.07996) | 本文提出了一种连续仿射学习（SAL）模型，用于构建深度神经网络(DNNs)。 该模型通过解决涉及激活函数的二次/凸优化问题来学习仿射映射，但是在权重矩阵和偏差之后。 |
| [^35] | [Nonnegative Low-Rank Tensor Completion via Dual Formulation with Applications to Image and Video Completion.](http://arxiv.org/abs/2305.07976) | 本文提出了一种对偶形式的非负低秩张量分解方法，解决了张量补全中忽视数据非负结构的问题，并在多个任务上优于最先进的算法。 |
| [^36] | [On the Computational Cost of Stochastic Security.](http://arxiv.org/abs/2305.07973) | 本文探究了使用长期持续蒙特卡罗模拟是否能提高能量模型的质量，并通过增加计算预算改进了模型的校准性和对抗鲁棒性。 |
| [^37] | [Tight and fast generalization error bound of graph embedding in metric space.](http://arxiv.org/abs/2305.07971) | 本文提供了一个紧致快速的泛化误差上限，可以保证非欧几里得度量空间中的图嵌入在实际训练数据规模下的成功。 |
| [^38] | [Structured Low-Rank Tensor Learning.](http://arxiv.org/abs/2305.07967) | 本文提出了一种结构化低秩张量学习的解决方案，并通过优化算法在流形上求解，解决了部分观测和结构约束下的学习问题。 |
| [^39] | [Leveraging Large Language Models in Conversational Recommender Systems.](http://arxiv.org/abs/2305.07961) | 本文提出了一种使用大型语言模型构建端到端大规模对话推荐系统的路线图，解决在该系统中有效利用大型语言模型所面临的技术挑战。 |
| [^40] | [A Novel Memetic Strategy for Optimized Learning of Classification Trees.](http://arxiv.org/abs/2305.07959) | 本论文提出了一种新的演化算法，用于诱导分类树，利用记忆学习方法处理大型数据集，结构具有泛化能力且与最先进的方法相当竞争力。 |
| [^41] | [More for Less: Safe Policy Improvement With Stronger Performance Guarantees.](http://arxiv.org/abs/2305.07958) | 本论文提出了一种新的方法，可以用较少的数据保证安全策略优化(SPI)的性能，并降低了SPIBB算法的样本复杂度。 |
| [^42] | [CodeT5+: Open Code Large Language Models for Code Understanding and Generation.](http://arxiv.org/abs/2305.07922) | CodeT5+是一组灵活组合的编码器-解码器LLM族，用于代码，混合了多种不同的预训练目标，包括代码生成、自然语言处理和程序合成，可以适应多种不同的下游代码任务，并且在实验中比现有代码-specific LLMs实现了最先进的性能。 |
| [^43] | [Convergence and scaling of Boolean-weight optimization for hardware reservoirs.](http://arxiv.org/abs/2305.07908) | 给出了在随机水库模型上使用坐标下降法进行优化的收敛性分析和尺度定律，为硬件网络优化提供了坚实的基础。 |
| [^44] | [Network-GIANT: Fully distributed Newton-type optimization via harmonic Hessian consensus.](http://arxiv.org/abs/2305.07898) | 本文提出了一种基于哈蒙莫特 Hessian 一致性的全分布式牛顿型优化算法 Network-GIANT，将梯度跟踪和牛顿型迭代算法相结合，经证明对严格凸和光滑损失函数有半全局和指数收敛到精确解的保证，实验证明 Network-GIANT 优于其他分布式学习算法（如 Network-DANE 和 Newton-Raphson Consensus）的收敛性能。 |
| [^45] | [Voxel-wise classification for porosity investigation of additive manufactured parts with 3D unsupervised and (deeply) supervised neural networks.](http://arxiv.org/abs/2305.07894) | 本研究使用三维无监督和(深度)监督神经网络进行增材制造件孔隙率检测的体素级分类，得出使用体素级分类的三维DL模型在AM零件的孔隙率检测方面具有巨大潜力的结论。 |
| [^46] | [DAC-MR: Data Augmentation Consistency Based Meta-Regularization for Meta-Learning.](http://arxiv.org/abs/2305.07892) | 为了提高元学习的性能，我们提出了一个基于元知识信息增强的元学习框架。我们通过使用适当的MR目标将元知识初步集成到元目标中，来规范元模型函数类的容量复杂度，有助于在未知任务上实现更好的泛化性能。 |
| [^47] | [Neural operator for structural simulation and bridge health monitoring.](http://arxiv.org/abs/2305.07889) | 本论文提出了结构模拟和桥梁健康监测的神经运算器VINO，通过学习结构响应场和损伤场之间的映射，在前向预测和反向确定损伤区域和程度方面可以比传统有限元模型更准确地预测和判断。 |
| [^48] | [Contrastive Domain Generalization via Logit Attribution Matching.](http://arxiv.org/abs/2305.07888) | 本论文提出了一种名为对比领域泛化（CDG）的新方法，通过强烈对比的数据对所展示的语义不变性进行利用。同时，提出了一种正则化技术——Logit Attribution Matching (LAM)，以实现CDG。实验结果表明，LAM仅使用少量配对数据就能胜过最先进的DG方法，且有助于模型更好地关注对领域泛化至关重要的语义特征。 |
| [^49] | [Differentiating Viral and Bacterial Infections: A Machine Learning Model Based on Routine Blood Test Values.](http://arxiv.org/abs/2305.07877) | 本研究开发了一种基于血液检查数值的病毒与细菌机器学习模型，用于准确识别感染类型。该模型在CRP水平10-40 mg/L范围内表现出更好的区分细菌和病毒感染的准确性，证明了多种血液参数对于诊断决策的重要性。 |
| [^50] | [SPP-CNN: An Efficient Framework for Network Robustness Prediction.](http://arxiv.org/abs/2305.07872) | 本文提出了一种名为SPP-CNN的高效框架，可用于网络鲁棒性预测。该框架通过在卷积和全连接层之间添加空间金字塔池化层，克服了CNN预测方法中常见的不匹配问题，并在综合实验中表现出优越性能。 |
| [^51] | [Scalable Educational Question Generation with Pre-trained Language Models.](http://arxiv.org/abs/2305.07871) | 这项研究开发了一种新的教育问题生成模型，能够通过在科学文本和科学问题数据上进行预训练和微调预训练语言模型，实现优秀的教育问题自动生成。 |
| [^52] | [A Flow-Based Generative Model for Rare-Event Simulation.](http://arxiv.org/abs/2305.07863) | 本文提出了一种使用基于流的生成模型直接模拟罕见事件分布的方法，该方法可以结合重要性采样获得高精度的复杂积分和期望估计，有效地提高采样效率并为罕见事件分布提供致命见解。 |
| [^53] | [HAiVA: Hybrid AI-assisted Visual Analysis Framework to Study the Effects of Cloud Properties on Climate Patterns.](http://arxiv.org/abs/2305.07859) | 本研究提出了一种混合 AI 辅助可视化分析框架（HAiVA），可以用于探索云属性和气候模式之间的复杂交互作用，以及设计和测试海洋云增白（MCB）的干预方案，以评估它们对气候模式的预期和意外影响。 |
| [^54] | [A Federated Learning-based Industrial Health Prognostics for Heterogeneous Edge Devices using Matched Feature Extraction.](http://arxiv.org/abs/2305.07854) | 提出了一种基于联邦学习的健康预测模型，该模型具有特征相似性匹配算法来区分学习来自异构边缘设备的数据，以便开发出更准确的预测模型。 |
| [^55] | [Meta-Polyp: a baseline for efficient Polyp segmentation.](http://arxiv.org/abs/2305.07848) | 本研究提出Meta-Polyp，将Meta-Former与UNet融合并引入多尺度上采样块和Convformer块，解决了CNN和Vision Transformer在处理分布外数据集、缺失边界和小息肉时遇到的困难，提高了息肉分割的效率。 |
| [^56] | [Understanding Model Averaging in Federated Learning on Heterogeneous Data.](http://arxiv.org/abs/2305.07845) | 本文研究了异构数据联邦学习中的模型平均技术，通过可视化损失/错误景观揭示了客户端模型环绕全局模型在一个共同的盆地内，并且发现全局模型在早期训练后的误差主要来自客户端数据集和全局数据集之间非重叠的数据及全局模型与客户端模型之间的最大距离两个因素。 |
| [^57] | [Thompson Sampling for Parameterized Markov Decision Processes with Uninformative Actions.](http://arxiv.org/abs/2305.07844) | 研究了带有未知参数和无信息动作的参数化马尔可夫决策过程，提出了关于PMDP的假设，并使用汤普森抽样保证了其渐近最优的期望遗憾界。 |
| [^58] | [No-Reference Point Cloud Quality Assessment via Weighted Patch Quality Prediction.](http://arxiv.org/abs/2305.07829) | COPP-Net是一种利用加权补丁质量预测进行局部相关性分析的无参考点云质量评估方法，优于现有的基准NR-PCQA方法。 |
| [^59] | [Description and Discussion on DCASE 2023 Challenge Task 2: First-Shot Unsupervised Anomalous Sound Detection for Machine Condition Monitoring.](http://arxiv.org/abs/2305.07828) | DCASE 2023 挑战任务2旨在解决机器状态监测中，部署新型机器的无监督异常声音检测，仅使用极少量的正常数据进行训练，且无需超参数调整。 |
| [^60] | [An Active Learning-based Approach for Hosting Capacity Analysis in Distribution Systems.](http://arxiv.org/abs/2305.07818) | 本文提出基于主动学习的配电系统承载能力分析方法，通过选择最具信息价值的分布式能源集成场景来构建HC替代模型，大大降低了成本，并确保了结果的可靠性。 |
| [^61] | [Depth Dependence of $\mu$P Learning Rates in ReLU MLPs.](http://arxiv.org/abs/2305.07810) | 本文研究了宽度为 $n$，深度为 $L$ 的随机全连接 ReLU 网络中 $\mu$P 学习率对 $n$ 和 $L$ 的依赖性，发现除第一层和最后一层以外，最大学习率与 $n$ 无关，但与 $L$ 按 $L^{-3/2}$ 缩放有关。 |
| [^62] | [Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy.](http://arxiv.org/abs/2305.07805) | Mesh2SSM是一种基于无监督排列不变表示学习的方法，可以将模板点云变形为特定主体的网格，形成基于对应关系的解剖学统计形态模型。 |
| [^63] | [Dr. LLaMA: Improving Small Language Models in Domain-Specific QA via Generative Data Augmentation.](http://arxiv.org/abs/2305.07804) | 本论文介绍了一种名为Dr. LLaMA的方法，通过使用大型语言模型进行生成式数据增强，以改善小语言模型的性能，特别是在医学问答任务中。这种方法在微调后使模型性能提高，并提出了在特定领域问答任务中使用LLM所面临的挑战和潜在的研究方向。 |
| [^64] | [Using Deepfake Technologies for Word Emphasis Detection.](http://arxiv.org/abs/2305.07791) | 本研究利用Deepfake技术产生没有重音的语音来解决自动语音重音检测任务，通过比较生成的语音和口述语音，能够分离出相对容易检测到的重音模式。 |
| [^65] | [Revisiting Matching Pursuit: Beyond Approximate Submodularity.](http://arxiv.org/abs/2305.07782) | 本文提出了一种期望次模性的函数，并通过贪心算法在期望意义下保证了选取近似最优向量的有效性，同时缓解了常用匹配追踪（MP）算法中的缺陷。 |
| [^66] | [Accelerator-Aware Training for Transducer-Based Speech Recognition.](http://arxiv.org/abs/2305.07778) | 本文提出的加速器感知训练方法应用于RNN-T模型，有效地模拟NNA操作，使用户感知延迟（UPL）减少，同时在引擎延迟方面有5-7％的改进，节省高达10％的WER相对下降。 |
| [^67] | [Monitoring and Adapting ML Models on Mobile Devices.](http://arxiv.org/abs/2305.07772) | 这篇论文介绍了Nazr，这是一个能够在移动设备上连续监测和调整机器学习模型，以提高模型准确性的端到端系统。 |
| [^68] | [TinyStories: How Small Can Language Models Be and Still Speak Coherent English?.](http://arxiv.org/abs/2305.07759) | 本文针对小型语言模型生成连贯的英文文本难题，引入了一个合成故事数据集 TinyStories，并探索小型模型规模、结构复杂度和训练数据规模对于语言模型表现的影响，证明了仅含 200 万参数的简单语言模型也能产生连贯的短故事。 |
| [^69] | [Private and Communication-Efficient Algorithms for Entropy Estimation.](http://arxiv.org/abs/2305.07751) | 本文提出了用于熵估计的私有且通信高效的算法，对于多变量联合分布，其样本数与变量数量呈线性关系；算法可以在保证隐私的同时最小化通信成本。 |
| [^70] | [To transfer or not transfer: Unified transferability metric and analysis.](http://arxiv.org/abs/2305.07741) | 该论文提出了基于Wasserstein距离的联合估计（WDJE）的分析方法和度量标准，用于转移能力评估和决策，覆盖了领域和任务差异，以及分类和回归问题。从比较目标风险的角度促进决策，提出了一个易于计算的目标风险边界。 |
| [^71] | [Measuring Surprise in the Wild.](http://arxiv.org/abs/2305.07733) | 本文首次展示了如何将根植于认知科学和神经科学的惊奇计算模型与机器学习生成模型相结合，用于复杂动态环境中的惊奇行为检测，并可用于交通安全中的冲突识别和驾驶行为评估。 |
| [^72] | [Predicting COVID-19 pandemic by spatio-temporal graph neural networks: A New Zealand's study.](http://arxiv.org/abs/2305.07731) | ATMGNN是一种基于时空图神经网络结构的新型深度学习架构，它能够结合空间和时间信息预测未来疫情趋势。通过学习聚类算法，该方法能够从多尺度的空间图中捕捉局部和全局信号，并建模长程的空间和时间依赖关系。实验结果表明ATMGNN在新西兰的COVID-19疫情预测中表现比其他方法更好，达到了最先进水平。 |
| [^73] | [Designing Optimal Behavioral Experiments Using Machine Learning.](http://arxiv.org/abs/2305.07721) | 本文介绍了利用BOED和机器学习的最新进展，为我们能够从中模拟数据的任何类型的模型找到最优实验，以获得更深入的了解人类行为和认知。 |
| [^74] | [Intercomparison of Brown Dwarf Model Grids and Atmospheric Retrieval Using Machine Learning.](http://arxiv.org/abs/2305.07719) | 本文通过机器学习方法分析了14个棕矮星模型网格的预测能力，发现棕矮星的有效温度可以被预测，但推断表面重力加速度和金属丰度与模型网格有关。 |
| [^75] | [Optimal signal propagation in ResNets through residual scaling.](http://arxiv.org/abs/2305.07715) | 本文为ResNets导出系统的有限尺寸理论，指出对于深层网络架构，缩放参数是优化信号传播和确保有效利用网络深度方面的关键。 |
| [^76] | [Using Language Models to Detect Alarming Student Responses.](http://arxiv.org/abs/2305.07709) | 本文介绍了一种利用自然语言处理技术识别危险学生回复的系统，该系统采用经过微调的语言模型进行训练，能够显著提高准确性。 |
| [^77] | [Mastering Percolation-like Games with Deep Learning.](http://arxiv.org/abs/2305.07687) | 研究使用单人游戏和深度学习在网络攻击中的应用，利用训练的代理人和不同的鲁棒性定义，发现优化攻击或防御网络对特定目标非常敏感。 |
| [^78] | [Synthetic data generation for a longitudinal cohort study -- Evaluation, method extension and reproduction of published data analysis results.](http://arxiv.org/abs/2305.07685) | 本研究使用先进的合成数据生成方法生成营养领域中的数据，并对其进行深入的质量分析，展示了细致分析合成数据的必要性和保护机密性的重要性，从而确保合成数据的可重复使用性。 |
| [^79] | [ML-Based Teaching Systems: A Conceptual Framework.](http://arxiv.org/abs/2305.07681) | 本文研究了机器学习模型在基于信息技术的教学系统中的应用，旨在解决组织如何更好地传播即将退休专家的知识并传授给新手的难题。 |
| [^80] | [Exploring the Rate-Distortion-Complexity Optimization in Neural Image Compression.](http://arxiv.org/abs/2305.07678) | 本文系统地研究了神经图像压缩中的速率失真复杂度(RDC)优化，提出了一种新的RDC感知神经图像编解码器RDCAuto，其能够动态调整压缩配置以实现目标速率、失真和编码复杂度，且在性能方面优于现有的神经图像编解码器。 |
| [^81] | [Masked Audio Text Encoders are Effective Multi-Modal Rescorers.](http://arxiv.org/abs/2305.07677) | 本文提出了Masked Audio Text Encoders（MATE），一个多模态掩码语言模型重新打分器，将声学表示形式并入到MLM的输入空间中。使用MATE对自动语音识别（ASR）系统进行多模态打分，即使在目标域数据不足的情况下，也可以提高系统的领域泛化能力，并且可以在非常有限的训练数据量下就将单词错误率（WER）降低。 |
| [^82] | [LatentPINNs: Generative physics-informed neural networks via a latent representation learning.](http://arxiv.org/abs/2305.07671) | LatentPINNs是一个利用潜在表示学习实现的物理学约束神经网络，通过潜在扩散模型的压缩表示，可以更快速、更有效地求解偏微分方程(PDE)。 |
| [^83] | [Liver Infection Prediction Analysis using Machine Learning to Evaluate Analytical Performance in Neural Networks by Optimization Techniques.](http://arxiv.org/abs/2305.07670) | 本研究利用机器学习算法分析不同肝脏疾病数据集，评估分析性能并通过优化技术找出最佳分类模型。 |
| [^84] | [Quantified Semantic Comparison of Convolutional Neural Networks.](http://arxiv.org/abs/2305.07663) | 本研究提出了两种方法来量化卷积神经网络潜在空间中语义信息之间的相似性，从而揭示CNN层内语义信息的流动和相似性，以及不同网络之间的相似度程度。 |
| [^85] | [Self-information Domain-based Neural CSI Compression with Feature Coupling.](http://arxiv.org/abs/2305.07662) | 本文提出了一种基于自信息域的神经CSI压缩与特征耦合方法，通过引入自信息作为CSI的表征，在新定义的自信息域中提取时间和空间特征进行有效压缩，并在压缩CSI反馈上实现了高达3.22dB的性能提升。 |
| [^86] | [RHINO: Rotated DETR with Dynamic Denoising via Hungarian Matching for Oriented Object Detection.](http://arxiv.org/abs/2305.07598) | 本文提出了一种面向定向目标检测的DINO基线模型RHINO。并通过匈牙利匹配和查询对齐的方式实现动态降噪，解决了重复预测的问题，从而在公共基准测试中达到最先进的性能水平。 |
| [^87] | [Continual Vision-Language Representaion Learning with Off-Diagonal Information.](http://arxiv.org/abs/2305.07437) | 本文探讨了通过流数据持续训练CLIP模型的可行性，提出了一种有效的连续学习框架Mod-X，并证明内部旋转和跨模态偏差导致了CLIP在跨模态检索任务中性能下降。 |
| [^88] | [A Survey on Intersectional Fairness in Machine Learning: Notions, Mitigation, and Challenges.](http://arxiv.org/abs/2305.06969) | 本篇论文调查了机器学习中交叉公平性的最新进展，提出了分类法和缓解方法，并探讨了未来研究的挑战与方向。 |
| [^89] | [Towards Theoretical Understanding of Data-Driven Policy Refinement.](http://arxiv.org/abs/2305.06796) | 本文介绍了一种数据驱动的强化学习政策细化方法，用于改进安全关键应用的策略，并提出了一系列定理验证其收敛性、鲁棒性界限、泛化误差和对模型错误的弹性。 |
| [^90] | [Utility-Maximizing Bidding Strategy for Data Consumers in Auction-based Federated Learning.](http://arxiv.org/abs/2305.06784) | 本文提出了一种首个效用最大化竞标策略（Fed-Bidder），使多个FL数据消费者可以通过AFL有效而高效地竞争数据拥有者。 |
| [^91] | [On practical robust reinforcement learning: adjacent uncertainty set and double-agent algorithm.](http://arxiv.org/abs/2305.06657) | 本文提出了一个名为ARQ-Learning的鲁棒性强化学习算法，采用了一个更加实际的不确定性集，并提出了一种称为“悲观代理”的方法。该算法在表格化的情况下获得了与现有算法相同的快速收敛速度，并为实际应用提供了更好的鲁棒性。而且，本文还首次提出了用于深度Q网络和深度确定策略梯度的鲁棒RL算法PR-DQN和PR-DDPG。 |
| [^92] | [HAHE: Hierarchical Attention for Hyper-Relational Knowledge Graphs in Global and Local Level.](http://arxiv.org/abs/2305.06588) | 提出了HAHE模型，使用全局和局部水平的注意力学习了超关系知识图谱的图形结构和顺序结构，并在多个基准数据集上实现了最先进的表现。 |
| [^93] | [How to Index Item IDs for Recommendation Foundation Models.](http://arxiv.org/abs/2305.06569) | 本研究对推荐基础模型的项目索引问题进行了系统检查，提出了一种新的上下文感知索引方法，该方法在项目推荐准确性和文本生成质量方面具有优势。 |
| [^94] | [Exploring the Landscape of Machine Unlearning: A Survey and Taxonomy.](http://arxiv.org/abs/2305.06360) | 本文综述了机器遗忘的现状和技术应用，包括数据删除、扰动和模型更新，讨论了MU在隐私、安全和公正性等领域的潜在益处，以及它在自然语言处理、计算机视觉和推荐系统中的未来发展方向。 |
| [^95] | [Rethinking the Value of Labels for Instance-Dependent Label Noise Learning.](http://arxiv.org/abs/2305.06247) | 本文提出了一种利用深度生成模型和因果表征学习处理实例相关标签噪声问题的新算法，能很好地识别高层次的内容和风格潜在因素，并在合成和真实数据集上验证了其有效性。 |
| [^96] | [Patchwork Learning: A Paradigm Towards Integrative Analysis across Diverse Biomedical Data Sources.](http://arxiv.org/abs/2305.06217) | 补丁学习是一种新的范式，通过整合来自不同数据源的信息，解决了数据隐私、异构数据来源和无法充分利用多个数据模态的挑战。它可以同时利用互补的数据来源，同时保护数据隐私，从而实现开发更全面和具有普适性的ML模型。 |
| [^97] | [A Neural Emulator for Uncertainty Estimation of Fire Propagation.](http://arxiv.org/abs/2305.06139) | 本论文提出了一种新的神经网络模型，直接估计给定输入参数不确定性的火灾传播概率，省去了繁重的模拟集合，能够更有效地对火灾传播的不确定性进行分析。 |
| [^98] | [All models are local: time to replace external validation with recurrent local validation.](http://arxiv.org/abs/2305.03219) | 本文认为外部验证无法确保机器学习模型的安全性或实用性，提出了循环本地验证的MLOps启发式范式作为新的黄金标准，强调对各个本地部署的模型进行监测和更新，从而更好地对齐临床和医疗特定需求与机器学习模型验证策略，提高临床决策支持工具的安全性和实用性。 |
| [^99] | [Explainable Reinforcement Learning via a Causal World Model.](http://arxiv.org/abs/2305.02749) | 本文提出了一种新的可解释强化学习框架，通过学习因果世界模型来解释行动的长期影响以及教学习者如何影响环境变量并最终导致奖励。 |
| [^100] | [Differentiable Neural Networks with RePU Activation: with Applications to Score Estimation and Isotonic Regression.](http://arxiv.org/abs/2305.00608) | 该论文介绍了使用RePU激活函数的可微分神经网络，在近似$C^s$平滑函数及其导数的同时建立了下限误差界，并证明了其在降低维度灾难方面的能力，此外还提出了一种使用RePU网络的惩罚保序回归(PDIR)方法。 |
| [^101] | [Deep Neural-network Prior for Orbit Recovery from Method of Moments.](http://arxiv.org/abs/2304.14604) | 该论文提出了一种基于深度神经网络先验的矩法轨道恢复方法，可用于解决多参照面对齐和单颗粒冷冻电镜建模等问题，具有抑制噪声的优势. |
| [^102] | [Generating Post-hoc Explanations for Skip-gram-based Node Embeddings by Identifying Important Nodes with Bridgeness.](http://arxiv.org/abs/2304.12036) | 本文提出了一种解释Skip-gram节点嵌入的方法，即通过计算桥接度识别重要节点，并提出了一种新型基于梯度的解释方法GRAPH-wGD，有效地提供全局性解释。 |
| [^103] | [Tetra-NeRF: Representing Neural Radiance Fields Using Tetrahedra.](http://arxiv.org/abs/2304.09987) | 本文提出了一种基于四面体和 Delaunay 表示的自适应表示方法，用于神经辐射场，可实现高效的训练和最先进的结果，同时提供了更多接近表面的细节，并且实现了比基于点的表示更好的性能。 |
| [^104] | [Accelerate Support Vector Clustering via Spectrum-Preserving Data Compression?.](http://arxiv.org/abs/2304.09868) | 本文介绍了一种通过保留谱的数据压缩来加速支持向量聚类的方法，可以显著提高聚类速度而不牺牲聚类质量。 |
| [^105] | [Behavior Retrieval: Few-Shot Imitation Learning by Querying Unlabeled Datasets.](http://arxiv.org/abs/2304.08742) | 本论文提出了一种简单的方法，利用少量下游专家数据从离线未标记数据集中选择性地查询相关行为（包括许多次优行为），实现了行为检索，进而实现了少样本学习。 |
| [^106] | [Learning Empirical Bregman Divergence for Uncertain Distance Representation.](http://arxiv.org/abs/2304.07689) | 本文介绍了一种新的基于Deep Metric Learning的方法，通过学习经验Bregman散度直接从数据中进行不确定距离表示，能够有效的在模式识别和聚类任务上提高准确性。 |
| [^107] | [Interpretable statistical representations of neural population dynamics and geometry.](http://arxiv.org/abs/2304.03376) | 该论文提出了一种基于统计分布的几何深度学习框架，用于表示非线性动态系统的几何感知或几何无感知表示，以对已测量轨迹进行无偏比较。利用该方法，能够解释神经动力学的嵌入，在灵长类似任务中取得了最先进的准确性。 |
| [^108] | [Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures From Routine EHRs for Pulmonary Nodule Classification.](http://arxiv.org/abs/2304.02836) | 本文提出了一种新的肺部结节分类方法，使用变压器模型整合了EHR中的成像和临床特征。 |
| [^109] | [Predictive Context-Awareness for Full-Immersive Multiuser Virtual Reality with Redirected Walking.](http://arxiv.org/abs/2303.17907) | 本文提出了利用预测性上下文感知来优化发射端和接收端的波束成形和波束导向，实现面向全沉浸多用户虚拟现实技术的高效通信。 |
| [^110] | [Hyperbolic Geometry in Computer Vision: A Novel Framework for Convolutional Neural Networks.](http://arxiv.org/abs/2303.15919) | 本文提出了HCNN，是第一个专为计算机视觉任务设计的完全双曲卷积神经网络。在洛伦兹模型的基础上，我们推广了CNN的基本组件，并通过在完全双曲设置中的实验证明了HCNN框架和洛伦兹模型的有效性。 |
| [^111] | [Logic of Differentiable Logics: Towards a Uniform Semantics of DL.](http://arxiv.org/abs/2303.10650) | 该论文介绍了一个元语言——LDL，用于定义DL，该元语言从语法和语义两方面上提高DL的形式化程度，使得对DL的性质和实现进行系统比较研究成为了可能。 |
| [^112] | [Generating symbolic music using diffusion models.](http://arxiv.org/abs/2303.08385) | 本文提出了一种使用扩散模型生成钢琴卷帘的方法，可以协调、生成、完善音乐；代码已公开共享。 |
| [^113] | [A Novel Tensor-Expert Hybrid Parallelism Approach to Scale Mixture-of-Experts Training.](http://arxiv.org/abs/2303.06318) | 本文提出了一种新的混合并行算法，结合了张量、专家和数据并行，以实现MoE模型的训练，其基本模型比当前最先进的DeepSpeed-MoE大4-8倍。 |
| [^114] | [Restoration based Generative Models.](http://arxiv.org/abs/2303.05456) | 本文提出了一种基于图像修复的生成模型，即DDMs。通过整合IR文献，我们使用替代目标和多样的前向过程，提供了一个MAP先验损失函数的基础，消除了DDMs中昂贵的采样的需求。此外，我们提出了多尺度训练，提高了性能。我们展示了我们的模型在反问题上的适用性，并相信我们的框架可以铺平快速图像合成和修复的道路。 |
| [^115] | [CoolPINNs: A Physics-informed Neural Network Modeling of Active Cooling in Vascular Systems.](http://arxiv.org/abs/2303.05300) | 本文提出了一种基于物理学引导的神经网络的建模框架——CoolPINNs，用于旨在高效热调节的血管系统。它能够准确捕捉复杂血管布局中的热流急剧跳变，处理包括切向和法向分量的斜率导数，解决由于辐射热传递引起的非线性，并为实时监测提供高速预测，更便于稳健的反演建模。 |
| [^116] | [QuickSRNet: Plain Single-Image Super-Resolution Architecture for Faster Inference on Mobile Platforms.](http://arxiv.org/abs/2303.04336) | QuickSRNet 是一种适用于移动平台上的实时应用的高效超分辨率架构，它解决了移动设备上进行实时的深度学习超分辨率的挑战，并提供了比现有神经架构更好的准确性与延迟权衡。 |
| [^117] | [Arbitrary Decisions are a Hidden Cost of Differentially Private Training.](http://arxiv.org/abs/2302.14517) | 差分隐私训练会产生预测多样性，即使对于相同输入，使用不同的随机性也会得到不同的输出，这一成本不仅未被研究还未被审核或传达给模型设计者和利益相关者。 |
| [^118] | [Agile Modeling: From Concept to Classifier in Minutes.](http://arxiv.org/abs/2302.12948) | 本文介绍了敏捷建模的概念，即将任何主观视觉概念转化为计算机视觉模型的过程，并通过用户研究表明，用户可以在30分钟内轻松创建分类器。 |
| [^119] | [Efficiently handling constraints with Metropolis-adjusted Langevin algorithm.](http://arxiv.org/abs/2302.11971) | 本研究提出了Metropolis-adjusted Langevin算法来处理带有约束条件的目标分布，理论和实验结果表明该算法在处理这种情况时效果优于竞争算法。 |
| [^120] | [Guided Deep Kernel Learning.](http://arxiv.org/abs/2302.09574) | 本文提出了一种引导深度核学习的方法，利用无限宽度神经网络学习深度核，通过神经网络高斯过程模型在优化中指导深度核学习模型，在遇到新数据点时能够适应目标置信度，既利用了贝叶斯行为，又保持了深度核的泛化能力、可扩展性和灵活性。 |
| [^121] | [Sanity checks and improvements for patch visualisation in prototype-based image classification.](http://arxiv.org/abs/2302.08508) | 本文通过精细的数据集，发现了基于原型的视觉分类中可视化方法的局限性，并提出了使用更忠实方法的必要性。 |
| [^122] | [Discovering sparse hysteresis models for smart materials.](http://arxiv.org/abs/2302.05313) | 本文提出了一种利用机器学习中的稀疏回归技术建模智能材料滞后的方法，并成功对压电材料的滞后现象进行建模和预测。同时在磁性材料方面提供了稀疏白盒建模滞后的见解。 |
| [^123] | [Exploiting Sparsity in Pruned Neural Networks to Optimize Large Model Training.](http://arxiv.org/abs/2302.05045) | 本文提出了一种新的方法，利用稀疏子网络来优化两种流行的深度学习并行算法 - 数据并行和层间并行的内存利用和通信。在512个NVIDIA V100 GPU上，我们的优化将27亿参数模型的内存消耗减少了74％，总通信时间减少了40％。 |
| [^124] | [On the Richness of Calibration.](http://arxiv.org/abs/2302.04118) | 本文提出了一个新的校准评估框架，探索了校准分数设计中的不同选择，并研究了根据输入特征而不是预测结果对数据点进行分组的优势，从而帮助制定具有理想数学特性的新方法。 |
| [^125] | [State-wise Safe Reinforcement Learning: A Survey.](http://arxiv.org/abs/2302.03122) | 本文综合回顾了强化学习中解决基于状态约束的方法，讨论了它们在安全保障和可扩展性、安全性和奖励表现、收敛后和训练过程中的安全性等方面的联系、差异和权衡，并讨论了未来发展方向。 |
| [^126] | [Evaluating Self-Supervised Learning via Risk Decomposition.](http://arxiv.org/abs/2302.03068) | 通过风险分解，提出四个误差部分评估自监督学习对169个视觉模型的影响，为SSL的设计和使用提供宝贵的见解。 |
| [^127] | [Double Permutation Equivariance for Knowledge Graph Completion.](http://arxiv.org/abs/2302.01313) | 本研究提出了双排列等变性的KG表示方法，可以使神经网络在KG中执行复杂的逻辑推理任务，并在多个归纳KG完成任务中实现了最先进的Hits@10测试准确率。双排列等变性在KG中开辟了新的研究方向。 |
| [^128] | [Physics Constrained Motion Prediction with Uncertainty Quantification.](http://arxiv.org/abs/2302.01060) | 该论文提出了一种物理学约束的运动预测方法，使用替代动力学模型来确保预测的轨迹在动力学上是可行的，同时通过构建适用于自动驾驶的预测区域，量化了不确定性，实验结果表明，在自主赛车数据集上实现了显著的预测精度提升。 |
| [^129] | [Debiasing Vision-Language Models via Biased Prompts.](http://arxiv.org/abs/2302.00070) | 本研究提出了一种通用方法，通过在文本嵌入中投影出偏向方向来校准视觉语言基础模型的偏差，仅进行文本嵌入去偏执就足以生成严谨的分类器和公正的生成模型，有效减少了社交偏见和虚假相关，无需额外数据或培训。 |
| [^130] | [LegendreTron: Uprising Proper Multiclass Loss Learning.](http://arxiv.org/abs/2301.11695) | 本文提出了一种新颖和实用的方法{\sc LegendreTron}，用于联合学习多类别问题的正确标准损失和概率。这种方法在基准测试中经常优于其他方法。 |
| [^131] | [Context-specific kernel-based hidden Markov model for time series analysis.](http://arxiv.org/abs/2301.09870) | 本文提出了一种可以捕获核依赖关系的基于核密度估计的隐马尔可夫模型。与传统模型和基于核密度估计的模型相比，该模型在具有依赖性的数据上具有更好的性能。 |
| [^132] | [Dual Personalization on Federated Recommendation.](http://arxiv.org/abs/2301.08143) | 本研究提出了一种新的个性化联邦推荐框架，可以学习轻量级模型并在智能设备上部署，同时实现对用户和物品的精细个性化。 |
| [^133] | [Negative Flux Aggregation to Estimate Feature Attributions.](http://arxiv.org/abs/2301.06989) | 该论文提出了一种新的算法，名为负通量聚合（NeFLAG），可用于估计深度神经网络中输入特征对预测的影响，该方法不需要拟合替代模型或路径积分梯度。 |
| [^134] | [Dynamic Data Assimilation of MPAS-O and the Global Drifter Dataset.](http://arxiv.org/abs/2301.05551) | 本研究提出了一种新的动态数据同化方法，将全球漂流器数据集与海洋预测模型相结合，通过利用地球系统模型的动态和模式，提高了海洋温度预测的准确性。 |
| [^135] | [gRoMA: a Tool for Measuring Deep Neural Networks Global Robustness.](http://arxiv.org/abs/2301.02288) | gRoMA是一种衡量DNN全局鲁棒性的创新工具，采用概率验证方法评估特定输出类别遭受到对抗性输入的概率。该工具可运行于预训练的黑盒分类模型上，并对整个模型和每个输入样本产生鲁棒性测量结果。 |
| [^136] | [On the Interpretability of Attention Networks.](http://arxiv.org/abs/2212.14776) | 本文研究注意力网络的可解释性，提出了选择依赖分类（SDC）变体分类问题，并演示了注意力模型可以准确无误但不具有可解释性的多种错误模式。该研究为评估SDC模型及其解释性提供了一种评估指标，并评估了不同架构的模型的解释性。 |
| [^137] | [Bayesian Interpolation with Deep Linear Networks.](http://arxiv.org/abs/2212.14457) | 本文在线性网络的情况下，使用贝叶斯推理找到了预测后验和贝叶斯模型证据的非渐近表达，并通过这些表达式得到深度、宽度和数据集大小的联合作用的新图像，同时证明了线性网络在无限深度时提供了可证明的最优预测，并推导了有限网络的尖锐大偏差边界。 |
| [^138] | [On Noisy Evaluation in Federated Hyperparameter Tuning.](http://arxiv.org/abs/2212.08930) | 本论文为联邦超参数调整中的噪声评估问题提供了第一次系统研究，发现即使是小量的噪声也会显著影响现有的方法，提出了一种利用公共代理数据来提高评估信号的简单有效方法。 |
| [^139] | [Causes and Cures for Interference in Multilingual Translation.](http://arxiv.org/abs/2212.07530) | 研究探究了多语言机器翻译中干扰的主要因素，通过系统化试验发现使用不到10亿参数的标准Transformer配置可以在很大程度上缓解干扰并促进协同，同时发现调整采样温度以控制数据中每个语言对所占比例的方法是平衡语言对之间关系的关键。 |
| [^140] | [AI Ethics on Blockchain: Topic Analysis on Twitter Data for Blockchain Security.](http://arxiv.org/abs/2212.06951) | 本研究分析了Twitter上与MEV相关的话题，结果表明推文讨论了深刻的伦理问题，包括安全、公平、情感情绪和对MEV解决方案的渴望。 |
| [^141] | [On the Relationship Between Explanation and Prediction: A Causal View.](http://arxiv.org/abs/2212.06925) | 本篇论文用因果推断的方法系统地评估了解释与预测的关系，结果表明这种关系远不如理想情况。 |
| [^142] | [Scalable and Sample Efficient Distributed Policy Gradient Algorithms in Multi-Agent Networked Systems.](http://arxiv.org/abs/2212.06357) | 本文介绍了一种分布式政策梯度算法，用于解决具有奖励耦合的多智能体强化学习问题，具有广泛的应用价值。 |
| [^143] | [Uncertainty Estimation in Deep Speech Enhancement Using Complex Gaussian Mixture Models.](http://arxiv.org/abs/2212.04831) | 本研究提出了一种基于复高斯混合模型的深度语音增强方法，可以估计干净语音的完整后验分布，而不是单一的乘性掩码。这种方法在准确性和预测不确定性的量化方面都表现优异，超越了现有的单通道语音增强方法。 |
| [^144] | [Statistical mechanics of continual learning: variational principle and mean-field potential.](http://arxiv.org/abs/2212.02846) | 从物理学的角度将连续学习的问题转化为Franz-Parisi热力学势的框架，将之前学习到的任务作为先验和参考，提出了一个在场空间中训练神经网络的变分贝叶斯学习设置，用于调节任务间的突触资源。 |
| [^145] | [Curriculum Learning for Relative Overgeneralization.](http://arxiv.org/abs/2212.02733) | 本论文提出了一种名为相对过度泛化的课程学习（CURO）的新算法来解决多智能体强化学习中存在的相对过度泛化 (RO) 问题，该方法在解决展示强RO的合作任务方面具有很好的表现。 |
| [^146] | [Physics-Informed Model-Based Reinforcement Learning.](http://arxiv.org/abs/2212.02179) | 本文介绍了一种应用于机器人任务的基于模型的强化学习算法，利用环境模型生成轨迹来更新策略，其中使用深度神经网络动态模型的性能比使用更准确的p型混合自动编码器动态模型的性能要差。 |
| [^147] | [Compositional Learning of Dynamical System Models Using Port-Hamiltonian Neural Networks.](http://arxiv.org/abs/2212.00893) | 本论文提出了一种组合学习动态系统模型的方法，使用端口-哈密顿神经网络来训练和组合子模型，从而实现模块化学习，避免需要更多复合系统数据的问题。 |
| [^148] | [Lie Group Forced Variational Integrator Networks for Learning and Control of Robot Systems.](http://arxiv.org/abs/2211.16006) | 本文提出了一种新的深度学习架构，LieFVIN，通过对动力学模型的结构特性进行建模，可以学习机器人系统的控制Lagrangian或Hamiltonian动力学，并在各种模拟机器人上得到了验证。 |
| [^149] | [PipeFisher: Efficient Training of Large Language Models Using Pipelining and Fisher Information Matrices.](http://arxiv.org/abs/2211.14133) | PipeFisher提出了一种新的方法，将二阶优化方法K-FAC的工作分配给管道间隙，以加速收敛和提高大语言模型训练的有效性。 |
| [^150] | [The Past Does Matter: Correlation of Subsequent States in Trajectory Predictions of Gaussian Process Models.](http://arxiv.org/abs/2211.11103) | 高斯过程模型中，对预测轨迹的后续状态之间独立性的假设是错误的，本文提出了一种新的高斯过程分段线性近似方法来缓解这个问题。 |
| [^151] | [Environmental Sensor Placement with Convolutional Gaussian Neural Processes.](http://arxiv.org/abs/2211.10381) | 本论文提出了一种新的方式——卷积高斯神经过程（ConvGNP），用于提高环境传感器的放置效率。ConvGNP使用神经网络来参数化联合高斯分布，通过学习空间和季节性非平稳性，优于传统的非平稳高斯过程模型。 |
| [^152] | [Spatial-temporal recurrent reinforcement learning for autonomous ships.](http://arxiv.org/abs/2211.01004) | 本文提出了一个时空循环神经网络架构，通过考虑 COLREG 规则和引入最先进的碰撞风险度量方法，实现了针对自主船舶的深度 Q 网络。在真实环境和模拟环境中验证后，所提出的方法表现出在海上路径规划中的潜力并具有鲁棒性。 |
| [^153] | [Online Control of Adaptive Large Neighborhood Search using Deep Reinforcement Learning.](http://arxiv.org/abs/2211.00759) | 本研究提出一种基于深度强化学习的方法来在线控制自适应大邻域搜索算法，该方法能够自适应选择启发式策略、调整参数和控制接受标准，以获得优化问题的良好解，对应用组合优化问题中的实际问题具有重要意义。 |
| [^154] | [Analyzing Deep Learning Representations of Point Clouds for Real-Time In-Vehicle LiDAR Perception.](http://arxiv.org/abs/2210.14612) | 本文提出了一种新的LiDAR点云表示的计算分类方法，通过迁移学习分析了不同网络架构在LiDAR点云分类和分割任务上的性能。实验结果表明，它可以帮助更好地理解和设计用于LiDAR点云的深度学习表示，提高自动驾驶的实时感知性能。 |
| [^155] | [Tighter Abstract Queries in Neural Network Verification.](http://arxiv.org/abs/2210.12871) | CEGARETTE是一种新的验证机制，可以同时对系统和属性进行抽象和细化，从而产生大小适中且足够准确的抽象网络，使得神经网络验证更加快速高效。 |
| [^156] | [Removing grid structure in angle-resolved photoemission spectra via deep learning method.](http://arxiv.org/abs/2210.11200) | 本文提出了一种基于深度学习的方法，可以通过利用光谱本身的自相关信息来消除光谱中的网格结构和噪声，从而优化光谱质量，并且该方法有潜力扩展到其他光谱测量中以消除外部信号。 |
| [^157] | [Spectroscopic data de-noising via training-set-free deep learning method.](http://arxiv.org/abs/2210.10494) | 本文提出了一种基于光谱自身信息的去噪方法，无需训练集，能够提取光谱的内在信息，保留了能带特征，可扩展性强。 |
| [^158] | [Accelerated Single-Call Methods for Constrained Min-Max Optimization.](http://arxiv.org/abs/2210.03096) | 本文提出了两种单调用单投影算法，分别是优化梯度方法和加速反射梯度方法，分别适用于满足弱Minty变分不等式和负共同单调性的包含问题，可以加速最小-最大优化，并分别取得了$O(\frac{1}{\sqrt{T}})$和$O(\frac{1}{T})$的收敛率。 |
| [^159] | [Training Diverse High-Dimensional Controllers by Scaling Covariance Matrix Adaptation MAP-Annealing.](http://arxiv.org/abs/2210.02622) | 本论文提出了三种新的CMA-MAE变体，利用高效近似方法提高了其可扩展性，相较于ES基线在基准测试中表现更好，并且达到或超过最先进的深度强化学习算法的性能。 |
| [^160] | [Exact conservation laws for neural network integrators of dynamical systems.](http://arxiv.org/abs/2209.11661) | 本文研究了使用神经网络积分器精确保守律的方法。相对于从数据中学习这些保守律，本文利用诺特定理将其固有地结合到了神经网络的结构中，并证明了其预测效果更好。 |
| [^161] | [Adaptive Bias Correction for Improved Subseasonal Forecasting.](http://arxiv.org/abs/2209.10666) | 本研究提出一种自适应偏差校正方法，应用于欧洲中期天气预报中心的亚季节模型，可以显著提高温度和降水预测精度。 |
| [^162] | [On the Reuse Bias in Off-Policy Reinforcement Learning.](http://arxiv.org/abs/2209.07074) | 本文揭示了离线强化学习中一个新的偏见问题：重复使用偏见，提出了一种简单有效的方法——重复使用感知重要性加权（RAW）来解决这个问题，并证明RAW显著提高了离线方法的样本效率和鲁棒性。 |
| [^163] | [Pre-trained Language Models for the Legal Domain: A Case Study on Indian Law.](http://arxiv.org/abs/2209.06049) | 本研究针对印度法律文本，重新训练和从零开始训练了两个PLMs，即LegalBERT和CaseLawBERT，并采用基于印度法律文本的词汇表训练了一个模型。我们在几项基准法律NLP任务中，对印度和非印度的法律文本进行了应用。 |
| [^164] | [Graph Embeddings via Tensor Products and Approximately Orthonormal Codes.](http://arxiv.org/abs/2208.10917) | 本文介绍了一种嵌入图形到向量空间的方法，使用张量积以及球形码实现高效压缩和表征，在稀疏图表示和其他应用中具有潜在技术优势。 |
| [^165] | [Detection and Mitigation of Byzantine Attacks in Distributed Training.](http://arxiv.org/abs/2208.08085) | 本文探讨了分布式训练中的拜占庭攻击及其检测和缓解方法。 |
| [^166] | [Do Quantum Circuit Born Machines Generalize?.](http://arxiv.org/abs/2207.13645) | 本文研究了量子电路出生机器对基数受限分布的学习过程，提高了模型的泛化性能，并探究了此能力的资源需求，并提出了一种改进的训练框架，可以学习具有高准确性和泛化性能的复杂分布. |
| [^167] | [Unveiling the Latent Space Geometry of Push-Forward Generative Models.](http://arxiv.org/abs/2207.10541) | 本文研究了深度生成模型的潜在空间及其与模型性能之间的关系。借助几何测量理论，我们发现了优化的充分条件。我们提出了一种截断方法，可以在潜在空间中强制实行一个简单的簇结构，并提高了模型的性能。 |
| [^168] | [Deep Learning for Anomaly Detection in Log Data: A Survey.](http://arxiv.org/abs/2207.03820) | 该综述对深度学习在日志文件异常检测方面的应用进行了系统的文献综述，分析了现有模型、数据预处理机制、异常检测技术和评估方法，以及深度学习相对于传统机器学习技术的优点。 |
| [^169] | [Federated X-Armed Bandit.](http://arxiv.org/abs/2205.15268) | 本文提出了第一个联邦多臂赌博机算法，通过利用全局目标的拓扑结构以及层次分割和弱平滑特性，实现了与客户端数量和评估预算相关的次线性累计遗憾度，对数通信只在中央服务器和客户端之间进行，保护了客户端的隐私。 |
| [^170] | [Intelligent Spatial Interpolation-based Frost Prediction Methodology using Artificial Neural Networks with Limited Local Data.](http://arxiv.org/abs/2204.08465) | 本文提出了一种基于空间插值的冻霜预测方法，利用气象站的气候数据、数字高程模型和植被指数数据，预测目标场地的下一小时最低温度，达到了92.55%的探测率。 |
| [^171] | [PADA: Pruning Assisted Domain Adaptation for Self-Supervised Speech Representations.](http://arxiv.org/abs/2203.16965) | 本文提出了PADA方法，在自监督语音表示学习领域自适应方面加入剪枝策略，使用CD-TAW方法从精细调整的OOT模型中获得初始剪枝掩码，并取得良好效果。 |
| [^172] | [Quantum compiling with variational instruction set for accurate and fast quantum computing.](http://arxiv.org/abs/2203.15574) | 本文提出了基于灵活设计的多量子比特门操作的量子变分指令集(QuVIS)，通过细粒度的时间优化算法，实现了快速和精确的量子计算，并且通过实验展示出了更低的误差积累和时间成本。 |
| [^173] | [CD-GAN: a robust fusion-based generative adversarial network for unsupervised remote sensing change detection with heterogeneous sensors.](http://arxiv.org/abs/2203.00948) | CD-GAN是一种针对具有异构传感器的遥感图像的无监督变化检测方法，该方法利用了最近的融合技术进展和对抗性网络，能够有效解决传统方法中传感器空间和/或光谱分辨率不同造成的挑战。 |
| [^174] | [On the Convergence of SARSA with Linear Function Approximation.](http://arxiv.org/abs/2202.06828) | 本文通过对投影SARSA到有限区域的收敛速度的探究，取得了在带有线性函数逼近的SARSA算法收敛性方面的进展，发现收敛区域比想象的要小得多。 |
| [^175] | [Existence and Estimation of Critical Batch Size for Training Generative Adversarial Networks with Two Time-Scale Update Rule.](http://arxiv.org/abs/2201.11989) | 本文研究了使用两时间尺度更新规则（TTUR）训练生成式对抗网络（GAN）时批次大小与训练所需步骤数量之间的关系，理论上证明了为了找到稳定点，随着批次大小的增加所需步骤数量会减少并且存在一个最小化随机一阶预言机（SFO）复杂度的关键批次大小。 |
| [^176] | [On the Existence of the Adversarial Bayes Classifier (Extended Version).](http://arxiv.org/abs/2112.01694) | 本篇论文研究了对抗训练健壮性下Bayes最优分类器的存在性问题，提出了一般性的充分条件，并可以为研究对抗性代理损失和其一致性属性提供有用的工具。 |
| [^177] | [PARIS: Personalized Activity Recommendation for Improving Sleep Quality.](http://arxiv.org/abs/2110.13745) | 该论文利用机器学习技术，结合可穿戴设备监测的数据，通过时间序列聚类找到与指定主题相关的行为模型并生成相应的睡眠质量活动建议，为提高睡眠质量提供了一种个性化解决方案。 |
| [^178] | [AxoNN: An asynchronous, message-driven parallel framework for extreme-scale deep learning.](http://arxiv.org/abs/2110.13005) | AxoNN是一种利用异步性和消息驱动执行调度每个GPU上神经网络操作的并行深度学习框架，将CPU内存作为冗余空间，降低GPU内存消耗，同时将每个GPU的参数数量增加4倍，从而减少了必需的GPU数量和训练时间。 |
| [^179] | [Trade When Opportunity Comes: Price Movement Forecasting via Locality-Aware Attention and Iterative Refinement Labeling.](http://arxiv.org/abs/2107.11972) | 本论文提出了一种名为LARA的新型价格变动预测框架，包括两个主要部分：局部感知注意力和迭代细化标注。在真实世界的金融数据集上的实验结果表明，LARA在准确性和盈利能力方面优于现有的最先进方法。 |
| [^180] | [Learning an Explicit Hyperparameter Prediction Function Conditioned on Tasks.](http://arxiv.org/abs/2107.02378) | 本研究探索了一种基于任务的学习方法，学习一个显式超参数预测函数以适应不同的查询任务。 |
| [^181] | [Detecting Errors and Estimating Accuracy on Unlabeled Data with Self-training Ensembles.](http://arxiv.org/abs/2106.15728) | 本文提出了一种利用自学习集合框架，同时解决无标签数据上的误差检测和准确性估计的方法。 |
| [^182] | [A hybrid ensemble method with negative correlation learning for regression.](http://arxiv.org/abs/2104.02317) | 该论文提出了一种带有负相关学习的混合集成算法，通过自动选择和加权子模型来解决在回归任务中模型不确定性问题，并在实验中表现出较好的性能。 |
| [^183] | [Controlled Gaussian Process Dynamical Models with Application to Robotic Cloth Manipulation.](http://arxiv.org/abs/2103.06615) | 本文提出了一种称为受控高斯过程动力学模型（CGPDM）的方法，用于学习高维非线性动态。该模型将高维状态空间投影到较小的维度潜在空间中。CGPDM由一个低维潜在空间组成，具有相关的动态，外部控制变量可以作用于该空间，并映射到观测空间。 |
| [^184] | [Multi-Task Attentive Residual Networks for Argument Mining.](http://arxiv.org/abs/2102.12227) | 本文提出了一种多任务注意力残差网络架构，通过利用集成方法、注意力机制和多任务学习，无需假设文档或论据结构，成功应用于多个论述挖掘任务中，成为了一种既通用又高性能的架构。 |
| [^185] | [On Low Rank Directed Acyclic Graphs and Causal Structure Learning.](http://arxiv.org/abs/2006.05691) | 本文提出了利用DAG因果模型的低秩假设来解决高维情况下学习因果结构的难题，并成功地将现有的低秩技术应用到了因果结构学习中，实验表明这种方法对于稠密图的数据模型具有实用性。 |
| [^186] | [A Unified Analysis of AdaGrad with Weighted Aggregation and Momentum Acceleration.](http://arxiv.org/abs/1808.03408) | 本论文提出了一种名为AdaUSM的AdaGrad变体，它采用了一种新的加权自适应学习率，可以统一AdaGrad、AccAdaGrad、Adam和RMSProp的学习率，同时通过使用统一动量方案，覆盖了重球动量和Nesterov加速梯度动量；在非凸随机设置中的收敛率为$\mathcal{O}(\log(T)/\sqrt{T})$。 |

# 详细

[^1]: 基于自动生成运行时分析文件的NLP技术的跨层5G漏洞检测

    NLP-based Cross-Layer 5G Vulnerabilities Detection via Fuzzing Generated Run-Time Profiling. (arXiv:2305.08226v1 [cs.CR])

    [http://arxiv.org/abs/2305.08226](http://arxiv.org/abs/2305.08226)

    本文提出一种基于NLP技术和自动生成运行时分析文件的创新方法，用于自动检测5G软件栈中的漏洞、意外行为和性能下降，试验表明其具有良好的效果和可扩展性。

    

    5G软件栈漏洞和意外行为的有效性和效率对于5G的保障至关重要，特别是对于它在关键基础设施中的应用。测试方法和网络安全研究中的可伸缩性和自动化是主要挑战。本文提出了一种创新的方法，通过与代码库中的fuzz测试相对应的运行时分析文件自动检测5G软件栈中的漏洞、意外行为和性能下降，在srsRAN上进行了试验。我们首先通过fuzz测试生成的Logging Information (LogInfo)将运行时分析映射到高维度度量空间，然后基于它们的时间戳信息构建特征空间，最后进一步利用基于机器学习的分类算法，包括Logistic回归，K-近邻和随机森林对其对性能和安全属性的影响进行分类。

    The effectiveness and efficiency of 5G software stack vulnerability and unintended behavior detection are essential for 5G assurance, especially for its applications in critical infrastructures. Scalability and automation are the main challenges in testing approaches and cybersecurity research. In this paper, we propose an innovative approach for automatically detecting vulnerabilities, unintended emergent behaviors, and performance degradation in 5G stacks via run-time profiling documents corresponding to fuzz testing in code repositories. Piloting on srsRAN, we map the run-time profiling via Logging Information (LogInfo) generated by fuzzing test to a high dimensional metric space first and then construct feature spaces based on their timestamp information. Lastly, we further leverage machine learning-based classification algorithms, including Logistic Regression, K-Nearest Neighbors, and Random Forest to categorize the impacts on performance and security attributes. The performance 
    
[^2]: 一种用于同质周期时间序列数据的异常检测的数据集融合算法

    A Dataset Fusion Algorithm for Generalised Anomaly Detection in Homogeneous Periodic Time Series Datasets. (arXiv:2305.08197v1 [cs.LG])

    [http://arxiv.org/abs/2305.08197](http://arxiv.org/abs/2305.08197)

    该论文介绍了一种名为“数据集融合”的新型数据集合成算法，可将来自多个同质数据集的周期信号融合为单个数据集，同时保留了通用异常检测的独特特征。在两个同质感应电动机（IM）故障数据集的3相电流数据的案例研究中表现出显著优势，有潜力在跨多个来源利用可用数据方面发挥作用。

    

    神经网络在多个数据集上的推广往往被文献忽略，因为NN通常针对特定的数据源进行优化。在基于时间序列的多数据集模型中，由于来自不同传感器和采集规范的连续数据的融合困难，这变得尤其具有挑战性。然而，在商业环境中，通用性可以有效利用可用数据和计算能力，在AI模型的可持续发展之中非常重要。本文介绍了“数据集融合”算法，这是一种将来自多个同质数据集的周期信号融合为单个数据集的新型数据集合成算法，同时保留了通用异常检测的独特特征。经过实验，在使用无监督LSTMCaps NN对来自2个不同同质感应电动机（IM）故障数据集的3相电流数据进行案例研究时，所提出的方法显著优于常规训练方法，显示出在跨多个来源利用可用数据上的潜力。

    The generalisation of Neural Networks (NN) to multiple datasets is often overlooked in literature due to NNs typically being optimised for specific data sources. This becomes especially challenging in time-series-based multi-dataset models due to difficulties in fusing sequential data from different sensors and collection specifications. In a commercial environment, however, generalisation can effectively utilise available data and computational power, which is essential in the context of Green AI, the sustainable development of AI models. This paper introduces "Dataset Fusion," a novel dataset composition algorithm for fusing periodic signals from multiple homogeneous datasets into a single dataset while retaining unique features for generalised anomaly detection. The proposed approach, tested on a case study of 3-phase current data from 2 different homogeneous Induction Motor (IM) fault datasets using an unsupervised LSTMCaps NN, significantly outperforms conventional training approa
    
[^3]: 端到端的学习是否足以支持健身活动识别？

    Is end-to-end learning enough for fitness activity recognition?. (arXiv:2305.08191v1 [cs.CV])

    [http://arxiv.org/abs/2305.08191](http://arxiv.org/abs/2305.08191)

    该研究使用全面注释的健身活动视频数据集，表明基于像素的端到端学习可以与基于姿势估计的最先进的动作识别流程相竞争，同时还可以支持时间上的细粒度任务。

    

    端到端学习已经被广泛应用于许多计算机视觉任务中，尤其是与静止图像相关的任务，由于任务特定的优化产生了非常强大的性能。然而，以人为中心的行为识别仍然主要由手工设计的流程控制着，只有个别组件被神经网络替换，这些网络通常只在单个帧上运行。作为对这些流程的相关性进行研究的实验，我们提供了一个新的全面注释的健身活动视频数据集。任何在这个领域中的识别能力几乎都是由人体姿势及其时间动态的函数，因此基于姿势的解决方案应该会表现得很好。我们展示了，在这些标记数据的基础上，基于原始像素的端到端学习可以与基于姿势估计的最先进的动作识别流程相竞争。我们还展示了，端到端学习可以支持实时的重复计数等时间上的细粒度任务。

    End-to-end learning has taken hold of many computer vision tasks, in particular, related to still images, with task-specific optimization yielding very strong performance. Nevertheless, human-centric action recognition is still largely dominated by hand-crafted pipelines, and only individual components are replaced by neural networks that typically operate on individual frames. As a testbed to study the relevance of such pipelines, we present a new fully annotated video dataset of fitness activities. Any recognition capabilities in this domain are almost exclusively a function of human poses and their temporal dynamics, so pose-based solutions should perform well. We show that, with this labelled data, end-to-end learning on raw pixels can compete with state-of-the-art action recognition pipelines based on pose estimation. We also show that end-to-end learning can support temporally fine-grained tasks such as real-time repetition counting.
    
[^4]: 一种优化且可扩展的矩阵机制用于扰动边缘数据下凸损失函数。

    An Optimal and Scalable Matrix Mechanism for Noisy Marginals under Convex Loss Functions. (arXiv:2305.08175v1 [cs.DB])

    [http://arxiv.org/abs/2305.08175](http://arxiv.org/abs/2305.08175)

    ResidualPlanner是一种用于带有高斯噪声的边缘的矩阵机制，既优化又可扩展，可以优化许多可以写成边际方差的凸函数的损失函数。

    

    扰动的边缘数据是一种常见的保护数据隐私的形式，可用于诸如列联表分析、贝叶斯网络构建和合成数据生成等下游任务。我们提出了ResidualPlanner，这是一种用于带有高斯噪声的边缘的矩阵机制，既优化又可扩展。ResidualPlanner可以优化许多可以写成边际方差的凸函数的损失函数。此外，ResidualPlanner可以在几秒钟内优化大规模设置中的边缘准确性，即使之前的最先进技术（HDMM）也会占用过多的内存。甚至在具有100个属性的数据集上也可以在几分钟内运行。此外，ResidualPlanner还可以有效地计算每个边缘的方差/协方差值（之前的方法会很快失败）。

    Noisy marginals are a common form of confidentiality-protecting data release and are useful for many downstream tasks such as contingency table analysis, construction of Bayesian networks, and even synthetic data generation. Privacy mechanisms that provide unbiased noisy answers to linear queries (such as marginals) are known as matrix mechanisms.  We propose ResidualPlanner, a matrix mechanism for marginals with Gaussian noise that is both optimal and scalable. ResidualPlanner can optimize for many loss functions that can be written as a convex function of marginal variances (prior work was restricted to just one predefined objective function). ResidualPlanner can optimize the accuracy of marginals in large scale settings in seconds, even when the previous state of the art (HDMM) runs out of memory. It even runs on datasets with 100 attributes in a couple of minutes. Furthermore ResidualPlanner can efficiently compute variance/covariance values for each marginal (prior methods quickly
    
[^5]: 学习能否降低控制？分析高斯过程事件触发在线学习中的计算延迟

    Can Learning Deteriorate Control? Analyzing Computational Delays in Gaussian Process-Based Event-Triggered Online Learning. (arXiv:2305.08169v1 [eess.SY])

    [http://arxiv.org/abs/2305.08169](http://arxiv.org/abs/2305.08169)

    该论文提出了一种适用于具有计算延迟的基于高斯过程的在线学习的新型事件触发器, 可以有效降低计算延迟。

    

    当系统的动态未知时，通常采用监督学习技术从数据推断模型。高斯过程回归是一种特别流行的学习方法，因为存在预测误差界限。此外，GP模型可以进行高效的在线更新，从而可以采用事件触发在线学习策略以确保特定的跟踪精度。然而，由于非常规计算时间的存在，必须能够在任意时间评估现有的触发条件，而这在实践中是无法实现的。因此，我们首先推导了一个延迟感知的跟踪误差界限，揭示了精度-时延的折衷。基于此结果，我们提出了一种适用于具有计算延迟的基于高斯过程的在线学习的新型事件触发器，我们证明了该触发器对于足够小的计算时间而言具有优势。最后，我们展示了所提出的事件触发器的有效性。

    When the dynamics of systems are unknown, supervised machine learning techniques are commonly employed to infer models from data. Gaussian process (GP) regression is a particularly popular learning method for this purpose due to the existence of prediction error bounds. Moreover, GP models can be efficiently updated online, such that event-triggered online learning strategies can be pursued to ensure specified tracking accuracies. However, existing trigger conditions must be able to be evaluated at arbitrary times, which cannot be achieved in practice due to non-negligible computation times. Therefore, we first derive a delay-aware tracking error bound, which reveals an accuracy-delay trade-off. Based on this result, we propose a novel event trigger for GP-based online learning with computational delays, which we show to offer advantages over offline trained GP models for sufficiently small computation times. Finally, we demonstrate the effectiveness of the proposed event trigger for o
    
[^6]: 从多视角时间序列中识别潜在过程

    Latent Processes Identification From Multi-View Time Series. (arXiv:2305.08164v1 [cs.LG])

    [http://arxiv.org/abs/2305.08164](http://arxiv.org/abs/2305.08164)

    该论文提出了一个名为MuLTI的新框架，采用了对比学习技术和排列机制来从多视角时间序列数据中识别潜在过程。

    

    理解时间序列数据的动态通常需要识别数据生成的唯一潜在因素，即潜在过程识别。由于两个主要挑战，即（i）复杂的数据结构（如时间依赖性）可能导致独立假设的违反，（ii）来自不同视角的因素通常是重叠的且难以聚合为完整的集合，因此，将现有工作扩展到多视角时间序列数据是一个非平凡的问题。在这项工作中，我们提出了一个新的框架MuLTI，它采用了对比学习技术来反演数据生成过程以增强可识别性。此外，MuLTI集成了一个排列机制，通过建立最优传输公式来合并相应的重叠变量。对合成和真实数据集的广泛实验结果表明，MuLTI在从多视角时间序列数据中识别潜在过程方面具有优越性能。

    Understanding the dynamics of time series data typically requires identifying the unique latent factors for data generation, \textit{a.k.a.}, latent processes identification. Driven by the independent assumption, existing works have made great progress in handling single-view data. However, it is a non-trivial problem that extends them to multi-view time series data because of two main challenges: (i) the complex data structure, such as temporal dependency, can result in violation of the independent assumption; (ii) the factors from different views are generally overlapped and are hard to be aggregated to a complete set. In this work, we propose a novel framework MuLTI that employs the contrastive learning technique to invert the data generative process for enhanced identifiability. Additionally, MuLTI integrates a permutation mechanism that merges corresponding overlapped variables by the establishment of an optimal transport formula. Extensive experimental results on synthetic and re
    
[^7]: 带有约束恢复的逆强化学习算法

    Inverse Reinforcement Learning With Constraint Recovery. (arXiv:2305.08130v1 [cs.LG])

    [http://arxiv.org/abs/2305.08130](http://arxiv.org/abs/2305.08130)

    本论文提出了一种带有约束恢复的逆强化学习算法，以针对约束马尔可夫决策过程问题。通过最大熵原理，将其视为一个受限制的非凸优化问题，并使用指数梯度下降算法来解决它。

    

    本文提出了一种针对约束马尔可夫决策过程（CMDP）问题的新型逆强化学习（IRL）算法。在标准的IRL问题中，逆学习者或代理人寻求恢复MDP的奖励函数，给定最优策略的轨迹演示集。在本文中，我们不仅寻求推断CMDP的奖励函数，还要推断约束。利用最大熵原理，我们表明IRL与约束恢复（IRL-CR）问题可以被视为一个受限制的非凸优化问题。我们将其减少为交替受限制的优化问题，这些子问题是凸的。我们使用指数梯度下降算法来解决它。 最后，我们证明了我们的算法在网格世界环境中的有效性。

    In this work, we propose a novel inverse reinforcement learning (IRL) algorithm for constrained Markov decision process (CMDP) problems. In standard IRL problems, the inverse learner or agent seeks to recover the reward function of the MDP, given a set of trajectory demonstrations for the optimal policy. In this work, we seek to infer not only the reward functions of the CMDP, but also the constraints. Using the principle of maximum entropy, we show that the IRL with constraint recovery (IRL-CR) problem can be cast as a constrained non-convex optimization problem. We reduce it to an alternating constrained optimization problem whose sub-problems are convex. We use exponentiated gradient descent algorithm to solve it. Finally, we demonstrate the efficacy of our algorithm for the grid world environment.
    
[^8]: 针对OTT媒体预测分析中的冷启动难题：元洞察和多模态集成掌握的揭示

    Unraveling Cold Start Enigmas in Predictive Analytics for OTT Media: Synergistic Meta-Insights and Multimodal Ensemble Mastery. (arXiv:2305.08120v1 [cs.LG])

    [http://arxiv.org/abs/2305.08120](http://arxiv.org/abs/2305.08120)

    本研究提出了一种通用方法，通过利用元数据和采用多模型集成技术来解决OTT媒体预测分析中的冷启动问题，结果表明多模型集成方法显著提高了预测准确性。

    

    冷启动问题是各个领域常见的挑战，包括媒体应用案例，比如预测在Over-The-Top（OTT）平台上新推出节目的收视率。本研究提出了一种通用方法，通过利用元数据和采用多模型集成技术来解决冷启动问题。我们的方法包括特征工程、模型选择和基于预测加权平均值的集成方法。我们使用各种性能度量来评估我们提出的方法的性能。我们的结果表明，与单个模型相比，多模型集成方法显著提高了预测准确性。

    The cold start problem is a common challenge in various domains, including media use cases such as predicting viewership for newly launched shows on Over-The-Top (OTT) platforms. In this study, we propose a generic approach to tackle cold start problems by leveraging metadata and employing multi-model ensemble techniques. Our methodology includes feature engineering, model selection, and an ensemble approach based on a weighted average of predictions. The performance of our proposed method is evaluated using various performance metrics. Our results indicate that the multi-model ensemble approach significantly improves prediction accuracy compared to individual models.
    
[^9]: 自动产生注意规则以限制机器学习模型错误

    Automatic Generation of Attention Rules For Containment of Machine Learning Model Errors. (arXiv:2305.08115v1 [cs.LG])

    [http://arxiv.org/abs/2305.08115](http://arxiv.org/abs/2305.08115)

    本文提出了一种将最可能出错的观察结果分离成“注意集”的自动化方法，以帮助机器学习模型的诊断和改进。采用基于特征切片的策略，具有人类可解释性、模型无关性和较少的辅助输入或知识，而且相较于其他方法，该方法的性能表现更佳。

    

    机器学习解决方案在许多应用中得到了广泛应用，但在商业级别上使用仍然存在许多挑战，其中一个主要问题是将模型的误差率维持在可接受的水平。本文提出一种方法来将最有可能出现错误预测的观察结果分离为“注意集”，这些集合可以直接帮助模型诊断和改进，并用于决定这些问题观察结果的替代方案。作者提供了几种用于确定最佳规则以分离这些观察结果的算法（“策略”）。特别地，采用基于特征切片的策略，因为它们具有人类可解释性、模型无关性，并需要最少的辅助输入或知识。此外，作者还表明这些策略的性能优于其他方法。

    Machine learning (ML) solutions are prevalent in many applications. However, many challenges exist in making these solutions business-grade. For instance, maintaining the error rate of the underlying ML models at an acceptably low level. Typically, the true relationship between feature inputs and the target feature to be predicted is uncertain, and hence statistical in nature. The approach we propose is to separate the observations that are the most likely to be predicted incorrectly into 'attention sets'. These can directly aid model diagnosis and improvement, and be used to decide on alternative courses of action for these problematic observations. We present several algorithms (`strategies') for determining optimal rules to separate these observations. In particular, we prefer strategies that use feature-based slicing because they are human-interpretable, model-agnostic, and require minimal supplementary inputs or knowledge. In addition, we show that these strategies outperform seve
    
[^10]: 平衡出租车时空数据的隐私和效用用于需求预测

    Balancing Privacy and Utility of Spatio-Temporal Data for Taxi-Demand Prediction. (arXiv:2305.08107v1 [cs.LG])

    [http://arxiv.org/abs/2305.08107](http://arxiv.org/abs/2305.08107)

    本文提出了使用联合学习进行出租车需求预测的方法，该方法允许多个参与方训练机器学习模型并保持数据私密和安全。文章对于类别不平衡、数据稀缺和模型泛化等技术挑战提出了解决方案，最终在实际数据集上展示了具有竞争力的表现。

    

    出租车需求预测是机器学习的重要应用，它可以使出租车提供设施优化其运营，城市规划者改善交通基础设施和服务。然而，这些系统中使用敏感数据引发了隐私和安全方面的担忧。在本文中，我们提出了使用联合学习进行出租车需求预测，允许多个参与方在保持数据私密和安全的同时训练机器学习模型。这可以使组织在不得以获取数据的情况下构建模型。尽管联合学习为出租车需求预测带来了潜在的益处，但它也面临着一些技术挑战，例如类别不平衡，某些参与方的数据稀缺以及需要确保模型泛化以适应不同的设施和地理区域。为了有效地应对这些挑战，我们提出了一个系统，利用区域无关的编码进行地理特征处理，自适应正则化来平衡数据稀缺，以及数据增强来解决类别不平衡问题。我们在真实的时空出租车需求数据集上评估了我们的方法，并展示了它在保护隐私和保持数据效用方面达到了有竞争力的表现。

    Taxi-demand prediction is an important application of machine learning that enables taxi-providing facilities to optimize their operations and city planners to improve transportation infrastructure and services. However, the use of sensitive data in these systems raises concerns about privacy and security. In this paper, we propose the use of federated learning for taxi-demand prediction that allows multiple parties to train a machine learning model on their own data while keeping the data private and secure. This can enable organizations to build models on data they otherwise would not be able to access. Despite its potential benefits, federated learning for taxi-demand prediction poses several technical challenges, such as class imbalance, data scarcity among some parties, and the need to ensure model generalization to accommodate diverse facilities and geographic regions. To effectively address these challenges, we propose a system that utilizes region-independent encoding for geogr
    
[^11]: 区块链交易费用预测：机器学习方法的比较

    Blockchain Transaction Fee Forecasting: A Comparison of Machine Learning Methods. (arXiv:2305.08105v1 [cs.LG])

    [http://arxiv.org/abs/2305.08105](http://arxiv.org/abs/2305.08105)

    本文研究区块链交易费用预测，比较了多种机器学习方法，结合小波阈值去噪和矩阵剖面数据处理，预测多个时间跨度上的5分钟内最低煤气价格，为交易费用预测提供了一种可行的方法。

    

    煤气是以太坊网络的交易费计量系统。网络的用户需要选择一个煤气价格来提交他们的交易，这会增加过高支付的风险，或者导致选择延迟/未处理的交易。本文研究了伦敦硬分叉后的数据，深入探讨了该分叉后网络的交易动态。因此，本文对2019年之前关于EthUSD BitUSD和煤气价格之间链接的工作进行了更新。为预测煤气价格，我们比较了一些机器学习方法，如直接递归混合LSTM、CNNLSTM和注意力LSTM等，结合小波阈值去噪和矩阵剖面数据处理，预测多个时间跨度上的5分钟内最低煤气价格。作为我们所知道的第一次将矩阵剖面应用于煤气价格数据和预测的研究，本研究证明矩阵配置是一种可行的交易费用预测方法。

    Gas is the transaction-fee metering system of the Ethereum network. Users of the network are required to select a gas price for submission with their transaction, creating a risk of overpaying or delayed/unprocessed transactions in this selection. In this work, we investigate data in the aftermath of the London Hard Fork and shed insight into the transaction dynamics of the net-work after this major fork. As such, this paper provides an update on work previous to 2019 on the link between EthUSD BitUSD and gas price. For forecasting, we compare a novel combination of machine learning methods such as Direct Recursive Hybrid LSTM, CNNLSTM, and Attention LSTM. These are combined with wavelet threshold denoising and matrix profile data processing toward the forecasting of block minimum gas price, on a 5-min timescale, over multiple lookaheads. As the first application of the matrix profile being applied to gas price data and forecasting we are aware of, this study demonstrates that matrix p
    
[^12]: 基于有限速抹通道的联邦 TD 学习：马尔可夫采样下的线性加速

    Federated TD Learning over Finite-Rate Erasure Channels: Linear Speedup under Markovian Sampling. (arXiv:2305.08104v1 [cs.LG])

    [http://arxiv.org/abs/2305.08104](http://arxiv.org/abs/2305.08104)

    该论文提出了一种适用于联邦学习的算法 QFedTD，在有限速抹通道下使用线性函数逼近以达到线性加速的效果，在强化学习方面具有实际应用价值。

    

    近年来，由于其在通信和隐私约束下加速监督学习任务的有效性，联邦学习 (FL)引起了广泛关注。然而，关于强化学习是否可以实现类似的加速，在理论上仍然不太清楚。针对这个方向，我们研究了一种联邦策略评估问题，在其中代理通过中央聚合器进行通信，以加快共同策略的评估。为了捕捉 FL 中的典型通信约束，我们考虑可以根据伯努利擦拭模型丢弃数据包的有限容量上行链路通道。在这种情况下，我们提出并分析了 QFedTD-一种带有线性函数逼近的量化联邦时序差分学习算法。我们的主要技术贡献是提供 QFedTD 的有限样本分析，该分析突出了量化和抹除对收敛速率的影响；并建立了与代理数量成线性的加速度。(翻译仅供参考，不代表达意完全正确)

    Federated learning (FL) has recently gained much attention due to its effectiveness in speeding up supervised learning tasks under communication and privacy constraints. However, whether similar speedups can be established for reinforcement learning remains much less understood theoretically. Towards this direction, we study a federated policy evaluation problem where agents communicate via a central aggregator to expedite the evaluation of a common policy. To capture typical communication constraints in FL, we consider finite capacity up-link channels that can drop packets based on a Bernoulli erasure model. Given this setting, we propose and analyze QFedTD - a quantized federated temporal difference learning algorithm with linear function approximation. Our main technical contribution is to provide a finite-sample analysis of QFedTD that (i) highlights the effect of quantization and erasures on the convergence rate; and (ii) establishes a linear speedup w.r.t. the number of agents un
    
[^13]: 一种基于机器学习的含湿度环氧纳米复合材料粘弹-粘塑性模型

    A machine learning-based viscoelastic-viscoplastic model for epoxy nanocomposites with moisture content. (arXiv:2305.08102v1 [cs.LG])

    [http://arxiv.org/abs/2305.08102](http://arxiv.org/abs/2305.08102)

    本文提出了一种运用深度学习的本构模型，能够准确的捕捉与速率相关的应力-应变关系和一致的切线模量，以研究含湿度纳米颗粒/环氧纳米复合材料的循环粘弹-粘塑性-破坏行为。

    

    本文提出了一种基于深度学习的本构模型，用于研究含湿度纳米颗粒/环氧纳米复合材料的循环粘弹-粘塑性-破坏行为。采用一种采样技术和扰动方法的组合框架训练长短期记忆网络，将实验验证的粘弹-粘塑性模型生成的训练数据用于训练模型，使得深度学习模型能够准确捕捉到与加载速率相关的应力-应变关系和一致的切线模量。此外，将基于深度学习的本构模型实现到有限元分析中。通过有限元模拟研究了加载速率和湿度含量对纳米颗粒/环氧样品的力-位移响应的影响。数值实例表明，基于深度学习的模型的计算效率取决于加载条件，并且明显优于传统本构模型。

    In this work, we propose a deep learning (DL)-based constitutive model for investigating the cyclic viscoelastic-viscoplastic-damage behavior of nanoparticle/epoxy nanocomposites with moisture content. For this, a long short-term memory network is trained using a combined framework of a sampling technique and a perturbation method. The training framework, along with the training data generated by an experimentally validated viscoelastic-viscoplastic model, enables the DL model to accurately capture the rate-dependent stress-strain relationship and consistent tangent moduli. In addition, the DL-based constitutive model is implemented into finite element analysis. Finite element simulations are performed to study the effect of load rate and moisture content on the force-displacement response of nanoparticle/ epoxy samples. Numerical examples show that the computational efficiency of the DL model depends on the loading condition and is significantly higher than the conventional constituti
    
[^14]: 条件平均嵌入和通过正定核的最优特征选择

    Conditional mean embeddings and optimal feature selection via positive definite kernels. (arXiv:2305.08100v1 [cs.LG])

    [http://arxiv.org/abs/2305.08100](http://arxiv.org/abs/2305.08100)

    本文提出一种新的算子理论方法来解决条件平均嵌入问题，构造了非线性数据的基于优化的特征选择。通过使用正定核的凸集，得到多种希尔伯特空间和特征的实现方式。

    

    本文提出一种新的算子理论方法来解决条件平均嵌入问题。基于谱分析优化算法和核、随机过程以及建设性学习算法的组合结果，我们构造了对于非线性数据的基于优化的特征选择。通过使用正定核的凸集，我们可以选择最优的特征表示，从而得到多种希尔伯特空间和特征的实现方式。我们的一个新想法是，我们允许从正定核的凸集中选择一组核，并在此基础上进行二次优化，以获得“最佳”特征表示。

    Motivated by applications, we consider here new operator theoretic approaches to Conditional mean embeddings (CME). Our present results combine a spectral analysis-based optimization scheme with the use of kernels, stochastic processes, and constructive learning algorithms. For initially given non-linear data, we consider optimization-based feature selections. This entails the use of convex sets of positive definite (p.d.) kernels in a construction of optimal feature selection via regression algorithms from learning models. Thus, with initial inputs of training data (for a suitable learning algorithm,) each choice of p.d. kernel $K$ in turn yields a variety of Hilbert spaces and realizations of features. A novel idea here is that we shall allow an optimization over selected sets of kernels $K$ from a convex set $C$ of positive definite kernels $K$. Hence our \textquotedblleft optimal\textquotedblright{} choices of feature representations will depend on a secondary optimization over p.d
    
[^15]: 自监督神经因子分析解耦语音表示

    Self-supervised Neural Factor Analysis for Disentangling Utterance-level Speech Representations. (arXiv:2305.08099v1 [cs.SD])

    [http://arxiv.org/abs/2305.08099](http://arxiv.org/abs/2305.08099)

    本文提出了一种自监督神经因子分析模型，使用HuBERT中的聚类方法来发现隐藏的声学单元，并使用这些单元对齐SSL模型的特征，从而产生解耦后的语音表示，从而为专门任务提供了一种基于Utterance水平的无监督学习目标。实验结果表明，SSNFA模型在说话人识别、语言识别和情感识别等各种任务中均显著优于现有的SSL模型，并且没有任何特定任务的微调或监督。

    

    自监督学习技术在自动语音识别方面已经展示了出色的性能，在低标注资源情况下证明非常有用，本文针对该技术在说话人、情感和语言识别等任务中的性能问题进行了探究。本文提出了一种因子分析模型，使用HuBERT中的聚类方法来发现隐藏的声学单元，并使用这些单元对齐SSL模型的特征，从而产生解耦后的语音表示，从而为专门任务提供了一种基于Utterance水平的无监督学习目标。实验结果表明，SSNFA模型在说话人识别、语言识别和情感识别等各种任务中均显著优于现有的SSL模型，并且没有任何特定任务的微调或监督。

    Self-supervised learning (SSL) speech models such as wav2vec and HuBERT have demonstrated state-of-the-art performance on automatic speech recognition (ASR) and proved to be extremely useful in low label-resource settings. However, the success of SSL models has yet to transfer to utterance-level tasks such as speaker, emotion, and language recognition, which still require supervised fine-tuning of the SSL models to obtain good performance. We argue that the problem is caused by the lack of disentangled representations and an utterance-level learning objective for these tasks. Inspired by how HuBERT uses clustering to discover hidden acoustic units, we formulate a factor analysis (FA) model that uses the discovered hidden acoustic units to align the SSL features. The underlying utterance-level representations are disentangled from the content of speech using probabilistic inference on the aligned features. Furthermore, the variational lower bound derived from the FA model provides an ut
    
[^16]: Meta-DM：扩散模型在少样本学习上的应用

    Meta-DM: Applications of Diffusion Models on Few-Shot Learning. (arXiv:2305.08092v1 [cs.LG])

    [http://arxiv.org/abs/2305.08092](http://arxiv.org/abs/2305.08092)

    本文提出了一个基于扩散模型的数据处理模块Meta-DM，可以轻松地集成到现有的少样本学习方法中，在监督和非监督设置中都能带来显著的性能提升。

    

    在少样本学习领域，广泛的研究关注于改进网络结构和训练策略。然而，数据处理模块的作用尚未得到充分探索。因此，在本文中，我们提出了基于扩散模型的通用数据处理模块Meta-DM，用于少样本学习问题。Meta-DM是一个简单而有效的模块，可以轻松地集成到现有的少样本学习方法中，在监督和非监督设置中都能带来显著的性能提升。我们对Meta-DM进行了理论分析，并在几种算法上进行了性能评估。实验表明，将Meta-DM与某些方法相结合可以实现最先进的结果。

    In the field of few-shot learning (FSL), extensive research has focused on improving network structures and training strategies. However, the role of data processing modules has not been fully explored. Therefore, in this paper, we propose Meta-DM, a generalized data processing module for FSL problems based on diffusion models. Meta-DM is a simple yet effective module that can be easily integrated with existing FSL methods, leading to significant performance improvements in both supervised and unsupervised settings. We provide a theoretical analysis of Meta-DM and evaluate its performance on several algorithms. Our experiments show that combining Meta-DM with certain methods achieves state-of-the-art results.
    
[^17]: 考虑住户热舒适和隐私的住宅需求响应计划成本优化

    Optimization of Residential Demand Response Program Cost with Consideration for Occupants Thermal Comfort and Privacy. (arXiv:2305.08077v1 [eess.SY])

    [http://arxiv.org/abs/2305.08077](http://arxiv.org/abs/2305.08077)

    本文提出一种非侵入性、准确且经济有效的方法，开发一个多目标仿真模型，用于智能住宅中的需求响应计划（DRP），以提高居民的经济效益和热舒适。

    

    当居民使用家庭能源管理系统（HEMS）时，可以利用需求响应计划（DRP）来降低能源成本，通过自动调整空调设定温度和将某些电器转移到低峰时段。如果HEMS了解居住状态，居民可以获得更多经济效益和热舒适。然而，对于建筑物的居住状态，直接传感成本高、准确性低且对居民具有侵入性。因此，预测算法可以作为一种有效的替代方法。本研究的目标是提出一种非侵入性、准确且经济有效的方法，开发一个多目标仿真模型，应用于智能住宅中的DRP，其中包括（a）电力负荷需求减少，（b）调整热舒适（空调）温度设定点，以及（c）最坏情况下的保守方法。

    Residential consumers can use the demand response program (DRP) if they can utilize the home energy management system (HEMS), which reduces consumer costs by automatically adjusting air conditioning (AC) setpoints and shifting some appliances to off-peak hours. If HEMS knows occupancy status, consumers can gain more economic benefits and thermal comfort. However, for the building occupancy status, direct sensing is costly, inaccurate, and intrusive for residents. So, forecasting algorithms could serve as an effective alternative. The goal of this study is to present a non-intrusive, accurate, and cost-effective approach, to develop a multi-objective simulation model for the application of DRPs in a smart residential house, where (a) electrical load demand reduction, (b) adjustment in thermal comfort (AC) temperature setpoints, and (c) , worst cases scenario approach is very conservative. Because that is unlikely all uncertain parameters take their worst values at all times. So, the fle
    
[^18]: 利用教师助手提高防御性蒸馏

    Improving Defensive Distillation using Teacher Assistant. (arXiv:2305.08076v1 [cs.CV])

    [http://arxiv.org/abs/2305.08076](http://arxiv.org/abs/2305.08076)

    该论文阐述了对抗攻击对深度神经网络的威胁，介绍了防御性蒸馏的方法并提出了利用教师助手来提高网络的鲁棒性。

    

    对抗攻击对现代应用中应用的深度神经网络的安全性和稳定性构成重大威胁。在基于计算机视觉的任务中，专家可以利用模型结构的知识来创建人类视觉无法察觉的对抗样本。这些攻击可能会导致流行的应用程序（如自动驾驶汽车、人脸识别等）存在安全问题。因此，建立对此类攻击具有鲁棒性的网络是非常必要和重要的。在文献中，防御性蒸馏是最有前途的方法之一。使用知识蒸馏，研究人员已经能够创建对一些攻击具有鲁棒性的模型。但是，越来越多的攻击被开发出来，暴露了防御性蒸馏的弱点。在这个项目中，我们从教师助手知识蒸馏中获得灵感，并提出引入辅助网络可以提高蒸馏模型的鲁棒性。

    Adversarial attacks pose a significant threat to the security and safety of deep neural networks being applied to modern applications. More specifically, in computer vision-based tasks, experts can use the knowledge of model architecture to create adversarial samples imperceptible to the human eye. These attacks can lead to security problems in popular applications such as self-driving cars, face recognition, etc. Hence, building networks which are robust to such attacks is highly desirable and essential. Among the various methods present in literature, defensive distillation has shown promise in recent years. Using knowledge distillation, researchers have been able to create models robust against some of those attacks. However, more attacks have been developed exposing weakness in defensive distillation. In this project, we derive inspiration from teacher assistant knowledge distillation and propose that introducing an assistant network can improve the robustness of the distilled mode
    
[^19]: HiPerformer：层级排列等变换器用于时间序列预测

    HiPerformer: Hierarchically Permutation-Equivariant Transformer for Time Series Forecasting. (arXiv:2305.08073v1 [cs.LG])

    [http://arxiv.org/abs/2305.08073](http://arxiv.org/abs/2305.08073)

    本文提出了一种考虑股票组内和组间结构的预测模型，通过分层排列等变性的概念进行设计。实验结果表明该方法在预测上表现出色。

    

    为了准确预测多个时间序列之间的关系，需要进行区分。特别是对于股票价格，往往将股票分为具有相同特征的组，具有与该组结构一致的关系的模型应该是有效的。因此，我们提出了分层排列等变性的概念，专注于组内和组间成分的指数交换，以设计考虑这种组结构的模型。当预测模型具有分层排列等变性时，预测结果符合成分的组之间的关系。因此，我们提出了一种层级排列等变性模型，考虑了同一组中成分之间和组之间的关系。基于真实数据开展的实验表明，该方法优于现有的最先进方法。

    It is imperative to discern the relationships between multiple time series for accurate forecasting. In particular, for stock prices, components are often divided into groups with the same characteristics, and a model that extracts relationships consistent with this group structure should be effective. Thus, we propose the concept of hierarchical permutation-equivariance, focusing on index swapping of components within and among groups, to design a model that considers this group structure. When the prediction model has hierarchical permutation-equivariance, the prediction is consistent with the group relationships of the components. Therefore, we propose a hierarchically permutation-equivariant model that considers both the relationship among components in the same group and the relationship among groups. The experiments conducted on real-world data demonstrate that the proposed method outperforms existing state-of-the-art methods.
    
[^20]: 联邦学习中的联邦评估综述

    A Survey of Federated Evaluation in Federated Learning. (arXiv:2305.08070v1 [cs.LG])

    [http://arxiv.org/abs/2305.08070](http://arxiv.org/abs/2305.08070)

    本篇论文对现有联邦评估方法进行全面综述，阐述了联邦评估在客户端选择、激励机制设计、恶意攻击检测等方面的重要作用，探讨了联邦评估在增强FL性能方面的各种应用，并提出了未来的研究方向。

    

    在传统机器学习中，由于所有数据样本都由服务器进行集中管理，因此进行模型评估非常简单。然而，在联邦学习（FL）中，模型评估变得更加具有挑战性，这被称为本文中的联邦评估。这是因为客户端不会公开其原始数据以保护数据隐私。联邦评估在客户端选择、激励机制设计、恶意攻击检测等方面发挥着重要作用。在本文中，我们首次全面调查了现有的联邦评估方法。此外，我们探讨了联邦评估在增强FL性能方面的各种应用，并最终通过展望一些挑战提出了未来的研究方向。

    In traditional machine learning, it is trivial to conduct model evaluation since all data samples are managed centrally by a server. However, model evaluation becomes a challenging problem in federated learning (FL), which is called federated evaluation in this work. This is because clients do not expose their original data to preserve data privacy. Federated evaluation plays a vital role in client selection, incentive mechanism design, malicious attack detection, etc. In this paper, we provide the first comprehensive survey of existing federated evaluation methods. Moreover, we explore various applications of federated evaluation for enhancing FL performance and finally present future research directions by envisioning some challenges.
    
[^21]: 基于实例感知重复因子采样的长尾目标检测

    Instance-Aware Repeat Factor Sampling for Long-Tailed Object Detection. (arXiv:2305.08069v1 [cs.CV])

    [http://arxiv.org/abs/2305.08069](http://arxiv.org/abs/2305.08069)

    提出一个能解决长尾目标检测中样本分布不平衡的方法IRFS，将实例和图像计数结合应用到重新采样过程中，结果表明IRFS优于现有采样方法并取得了最新的结果。

    

    我们提出了一种非常简单的方法——实例感知重复因子采样（IRFS），用于解决长尾目标检测中不平衡数据的问题。在现实世界的目标检测中，不平衡数据集通常会受到每个类别实例数量的巨大差异的困扰。为了提高目标检测模型在罕见类别上的泛化性能，已经提出了各种数据采样技术。由于其简洁和有效性，重复因子采样（RFS）已经显示出了潜力。尽管RFS效率很高，但它完全忽略了实例数量，仅依赖于图像计数来进行重新采样。然而，对于不同类别的实例计数可能巨大不同，虽然其图像计数相似。这种差异强调了解决长尾分布的不同方面的图像和实例的重要性。因此，我们提出了IRFS，它将实例和图像计数统一为重新采样过程的一部分，以便知道不平衡性问题的不同视角。我们在若干个长尾检测基准上评估了我们的方法，并表明IRFS优于现有采样方法并取得了最新的结果。

    We propose an embarrassingly simple method -- instance-aware repeat factor sampling (IRFS) to address the problem of imbalanced data in long-tailed object detection. Imbalanced datasets in real-world object detection often suffer from a large disparity in the number of instances for each class. To improve the generalization performance of object detection models on rare classes, various data sampling techniques have been proposed. Repeat factor sampling (RFS) has shown promise due to its simplicity and effectiveness. Despite its efficiency, RFS completely neglects the instance counts and solely relies on the image count during re-sampling process. However, instance count may immensely vary for different classes with similar image counts. Such variation highlights the importance of both image and instance for addressing the long-tail distributions. Thus, we propose IRFS which unifies instance and image counts for the re-sampling process to be aware of different perspectives of the imbal
    
[^22]: 利用韵律关注和蒸馏来提高端到端语义理解性能

    Improving End-to-End SLU performance with Prosodic Attention and Distillation. (arXiv:2305.08067v1 [cs.CL])

    [http://arxiv.org/abs/2305.08067](http://arxiv.org/abs/2305.08067)

    本文提出了韵律关注和韵律蒸馏方法来利用韵律特征提高端到端语义理解（SLU）的性能，其中韵律蒸馏方法相对于基线方法在SLURP和STOP数据集上提高了8％和2％的意图分类准确性。

    

    大多数端到端语义理解（SLU）方法依赖于预训练的自动语音识别或语言模型功能来进行意图预测。然而，语音中的其他重要信息，如韵律，经常被忽视。最近的研究表明，将韵律信息合并到对话行为分类中可以获得改进的结果。本文提出了韵律关注，它使用不同的方式利用韵律特征来生成跨时间帧的注意力图。然后，我们提出了韵律蒸馏，来明确地学习声学编码器中的韵律信息，而不是连接隐含的韵律特征。两种方法都提高了基线结果，其中韵律蒸馏方法在SLURP和STOP数据集上相对于韵律基线提高了8％和2％的意图分类准确性。

    Most End-to-End SLU methods depend on the pretrained ASR or language model features for intent prediction. However, other essential information in speech, such as prosody, is often ignored. Recent research has shown improved results in classifying dialogue acts by incorporating prosodic information. The margins of improvement in these methods are minimal as the neural models ignore prosodic features. In this work, we propose prosody-attention, which uses the prosodic features differently to generate attention maps across time frames of the utterance. Then we propose prosody-distillation to explicitly learn the prosodic information in the acoustic encoder rather than concatenating the implicit prosodic features. Both the proposed methods improve the baseline results, and the prosody-distillation method gives an intent classification accuracy improvement of 8\% and 2\% on SLURP and STOP datasets over the prosody baseline.
    
[^23]: 基于连词效应建模的大动作空间离线策略评估

    Off-Policy Evaluation for Large Action Spaces via Conjunct Effect Modeling. (arXiv:2305.08062v1 [stat.ML])

    [http://arxiv.org/abs/2305.08062](http://arxiv.org/abs/2305.08062)

    本研究提出了一个称为OffCEM的估计器，用于对大离散动作空间下上下文匹配策略进行离线策略评估。该估计器通过基于模型的奖励估计来处理残余因果效应，并在新的本地正确性条件下保持无偏性。结果表明，OffCEM在合成和实际大动作空间数据集上优于现有方法。

    

    本文讨论了对于传统重要性加权方法方巨的大离散动作空间下的上下文匹配策略的离线策略评估（OPE）问题。为了解决方巨问题，我们提出了一个新的估计器OffCEM，该方法基于连词效应模型（CEM），这是一种新的因果效应分解方法，可以将效应分为群集效应和残差效应。OffCEM仅对行动群集应用重要性加权，通过基于模型的奖励估计来处理残余因果效应。我们表明，在新的本地正确性条件下，该估计器是无偏的，该条件仅要求残差效应模型保留每个群集中行动的相对期望奖励差异。为了充分利用CEM和本地正确性，我们还提出了一种新的两步过程，用于执行基于模型的估计，第一步最小化偏差，第二步最小化方差。我们发现，所得到的OPE估计器OffCEM在合成和实际大动作空间数据集上都明显优于现有的最先进方法。

    We study off-policy evaluation (OPE) of contextual bandit policies for large discrete action spaces where conventional importance-weighting approaches suffer from excessive variance. To circumvent this variance issue, we propose a new estimator, called OffCEM, that is based on the conjunct effect model (CEM), a novel decomposition of the causal effect into a cluster effect and a residual effect. OffCEM applies importance weighting only to action clusters and addresses the residual causal effect through model-based reward estimation. We show that the proposed estimator is unbiased under a new condition, called local correctness, which only requires that the residual-effect model preserves the relative expected reward differences of the actions within each cluster. To best leverage the CEM and local correctness, we also propose a new two-step procedure for performing model-based estimation that minimizes bias in the first step and variance in the second step. We find that the resulting O
    
[^24]: CREMP: 大环肽的构象旋转组合集，用于机器学习

    CREMP: Conformer-Rotamer Ensembles of Macrocyclic Peptides for Machine Learning. (arXiv:2305.08057v1 [q-bio.BM])

    [http://arxiv.org/abs/2305.08057](http://arxiv.org/abs/2305.08057)

    CREMP是一个资源，包含超过3千万个大环肽构象形状，旨在快速开发和评估机器学习模型，以便更好地模拟大环肽的构象。

    

    计算和机器学习方法对于模拟大环肽的构象具有潜在的启示性作用，能够实现合理的设计和优化。然而，准确、快速、可伸缩的建模大环肽几何形状的方法仍然很难得到。近期的深度学习方法显著加速了蛋白质结构预测和小分子构象集合的生成，但由于大环肽的独特特性，在大环肽的建模方面仍未取得类似的进展。在这里，我们引入了CREMP，这是一个用来快速开发和评估大环肽机器学习模型的资源。CREMP包含36,198个独特的大环肽及其利用构象-旋转组合采样工具（CREST）生成的高质量构象集合。总之，这个新的数据集包含近3130万个独特的大环肽几何形状，每个形状都用半经验扩展紧束缚（xTB）DF导出的能量进行注释。

    Computational and machine learning approaches to model the conformational landscape of macrocyclic peptides have the potential to enable rational design and optimization. However, accurate, fast, and scalable methods for modeling macrocycle geometries remain elusive. Recent deep learning approaches have significantly accelerated protein structure prediction and the generation of small-molecule conformational ensembles, yet similar progress has not been made for macrocyclic peptides due to their unique properties. Here, we introduce CREMP, a resource generated for the rapid development and evaluation of machine learning models for macrocyclic peptides. CREMP contains 36,198 unique macrocyclic peptides and their high-quality structural ensembles generated using the Conformer-Rotamer Ensemble Sampling Tool (CREST). Altogether, this new dataset contains nearly 31.3 million unique macrocycle geometries, each annotated with energies derived from semi-empirical extended tight-binding (xTB) DF
    
[^25]: 探索图神经网络泛化的理解

    Towards Understanding the Generalization of Graph Neural Networks. (arXiv:2305.08048v1 [cs.LG])

    [http://arxiv.org/abs/2305.08048](http://arxiv.org/abs/2305.08048)

    本文探索了图神经网络的泛化；通过理论分析和实验结果发现了影响泛化间隙的体系结构因素。

    

    图神经网络是应用于图结构数据的学习和表示中最广泛使用的模型。尽管它们在实际应用中取得了非凡的成功，但从理论上理解它们的工作机制仍处于初级阶段。本文从泛化的角度出发，首先考虑了随机优化的情况下传递学习的泛化间隙和梯度的高概率界限。其次，为流行的GNN提供了泛化间隙的高概率上限。理论结果揭示了影响泛化间隙的体系结构特定因素。基准数据集上的实验结果显示了理论结果和经验证据之间的一致性。我们的结果提供了理解GNN泛化的新思路。

    Graph neural networks (GNNs) are the most widely adopted model in graph-structured data oriented learning and representation. Despite their extraordinary success in real-world applications, understanding their working mechanism by theory is still on primary stage. In this paper, we move towards this goal from the perspective of generalization. To be specific, we first establish high probability bounds of generalization gap and gradients in transductive learning with consideration of stochastic optimization. After that, we provide high probability bounds of generalization gap for popular GNNs. The theoretical results reveal the architecture specific factors affecting the generalization gap. Experimental results on benchmark datasets show the consistency between theoretical results and empirical evidence. Our results provide new insights in understanding the generalization of GNNs.
    
[^26]: 使用EEG信号在真实场景中评估记忆检索工作负荷

    Using EEG Signals to Assess Workload during Memory Retrieval in a Real-world Scenario. (arXiv:2305.08044v1 [cs.HC])

    [http://arxiv.org/abs/2305.08044](http://arxiv.org/abs/2305.08044)

    本研究使用EEG信号作为神经工效学的生理测量员评估了参与者在典型办公任务期间的记忆负荷。结果表明单显示器和双显示器设置下的记忆工作负荷之间存在显著差异，并且机器学习模型能够明确高负荷和低负荷的状态。

    

    目的：脑电图(electroencephalogram, EEG)作为神经工效学的一种生理测量，因为具有客观性、不易受到偏见的影响，能够评估认知状态的动态变化，在人类因素研究中越来越受欢迎。本研究研究了记忆负荷与EEG之间的关联，在参与者在单显示器和双显示器设置下进行典型办公任务期间，我们预计单显示器的情况下将具有更高的记忆负荷。方法：我们设计了一个实验，模拟了被试执行一些办公工作的场景，并检查了被试在两种不同的办公桌面设置（单显示器设置和双显示器设置）下是否经历了不同水平的记忆工作负荷。我们使用EEG带功率、互信息和相干性作为特征，训练机器学习模型来分类高负荷和低负荷状态。主要结果：研究结果显示这些特征表现出显著的差异...

    Objective: The Electroencephalogram (EEG) is gaining popularity as a physiological measure for neuroergonomics in human factor studies because it is objective, less prone to bias, and capable of assessing the dynamics of cognitive states. This study investigated the associations between memory workload and EEG during participants' typical office tasks on a single-monitor and dual-monitor arrangement. We expect a higher memory workload for the single-monitor arrangement. Approach: We designed an experiment that mimics the scenario of a subject performing some office work and examined whether the subjects experienced various levels of memory workload in two different office setups: 1) a single-monitor setup and 2) a dual-monitor setup. We used EEG band power, mutual information, and coherence as features to train machine learning models to classify high versus low memory workload states. Main results: The study results showed that these characteristics exhibited significant differences t
    
[^27]: 基于随机池化的可证明多实例深度AUC最大化方法

    Provable Multi-instance Deep AUC Maximization with Stochastic Pooling. (arXiv:2305.08040v1 [cs.LG])

    [http://arxiv.org/abs/2305.08040](http://arxiv.org/abs/2305.08040)

    本文提出了在多实例学习中使用深度AUC最大化（DAM）的方法，并根据包含大量实例的情况下训练的计算挑战，提出了一种基于方差减少的随机池化方法，使得只需对每个包进行少量采样即可计算MIDAM模型，提高了效率和准确性。

    

    本文提出了一种深度AUC最大化（DAM）的新型应用，用于多实例学习（MIL），其中将单个类标签分配给一组实例（例如，患者的多个CT扫描的多个2D切片）。我们在DAM的背景下解决了MIL中被忽略但非常重要的计算挑战，即包大小过大，无法在反向传播时加载到GPU内存中，这是MIL标准池化方法所必需的。为了解决这个问题，我们提出了一种基于方差减少的随机池化方法，这种方法可以将关于汇聚预测的损失函数构造为多级组合函数。通过综合随机组合优化和非凸极小最大优化技术，我们提出了一种统一且可证明的多实例DAM（MIDAM）算法，其使用随机平滑最大池化或随机注意力池化，仅对每个包对应的实例进行少量采样来计算 sto。

    This paper considers a novel application of deep AUC maximization (DAM) for multi-instance learning (MIL), in which a single class label is assigned to a bag of instances (e.g., multiple 2D slices of a CT scan for a patient). We address a neglected yet non-negligible computational challenge of MIL in the context of DAM, i.e., bag size is too large to be loaded into {GPU} memory for backpropagation, which is required by the standard pooling methods of MIL. To tackle this challenge, we propose variance-reduced stochastic pooling methods in the spirit of stochastic optimization by formulating the loss function over the pooled prediction as a multi-level compositional function. By synthesizing techniques from stochastic compositional optimization and non-convex min-max optimization, we propose a unified and provable muli-instance DAM (MIDAM) algorithm with stochastic smoothed-max pooling or stochastic attention-based pooling, which only samples a few instances for each bag to compute a sto
    
[^28]: 通过SyCo-AE: Synthetically Constrained Autoencoders对混沌动力学进行小数据降阶建模

    Small-data Reduced Order Modeling of Chaotic Dynamics through SyCo-AE: Synthetically Constrained Autoencoders. (arXiv:2305.08036v1 [cs.LG])

    [http://arxiv.org/abs/2305.08036](http://arxiv.org/abs/2305.08036)

    本文利用自编码器对混沌动力学进行小数据降阶建模，通过在降阶空间中施加合成约束来保持完全非线性和高度不稳定的自由度的同时，防止了发散。使用这种方法，即便是使用更少的数据，也可以产生低误差的中长期预测。

    

    基于数据的混沌动力学降阶建模可能会导致系统发散或灭绝。本文利用自编码器的非线性降维和神经网络的非线性运算推断自由，通过在降阶空间中施加合成约束来解决这个问题。合成约束使我们的降阶模型在保持完全非线性和高度不稳定的自由度的同时，防止了发散。我们利用经典的40变量Lorenz '96方程说明了方法的实用性，结果表明我们的方法使用更少的数据可以产生低误差的中长期预测。

    Data-driven reduced order modeling of chaotic dynamics can result in systems that either dissipate or diverge catastrophically. Leveraging non-linear dimensionality reduction of autoencoders and the freedom of non-linear operator inference with neural-networks, we aim to solve this problem by imposing a synthetic constraint in the reduced order space. The synthetic constraint allows our reduced order model both the freedom to remain fully non-linear and highly unstable while preventing divergence. We illustrate the methodology with the classical 40-variable Lorenz '96 equations, showing that our methodology is capable of producing medium-to-long range forecasts with lower error using less data.
    
[^29]: 降维处理极端气动学问题

    Grasping Extreme Aerodynamics on a Low-Dimensional Manifold. (arXiv:2305.08024v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2305.08024](http://arxiv.org/abs/2305.08024)

    研究探索处理极端气动学问题的基本物理机制方法。

    

    现代航空器执行广泛的任务，包括运输、国防、监视和救援。这些飞机可以在平静的条件下飞行，但避免在窄谷，山区和船舶尾迹等多风区操作。其中，小型飞机尤其容易受到这些气流干扰。随着全球变暖使极端天气越来越频繁，预计飞机，尤其是那些较小的飞机，会遇到大规模的大气干扰，但仍应执行稳定的飞行。然而，目前几乎没有理论来描述强烈的气流对飞行器的影响。更为复杂的是，翼面遇到气流的参数空间非常大，而不同参数组合之间的涡旋气流和翼的相互作用似乎具有不同的特点。本研究展示了一种处理这一问题的基本物理机制的方法。

    Modern air vehicles perform a wide range of operations, including transportation, defense, surveillance, and rescue. These aircraft can fly in calm conditions but avoid operations in gusty environments, which are seen in urban canyons, over mountainous terrains, and in ship wakes. Smaller aircraft are especially prone to such gust disturbances. With extreme weather becoming ever more frequent due to global warming, it is anticipated that aircraft, especially those that are smaller in size, encounter large-scale atmospheric disturbances and still be expected to manage stable flight. However, there exists virtually no foundation to describe the influence of extreme vortical gusts on flying bodies. To compound on this difficult problem, there is an enormous parameter space for gusty conditions wings encounter. While the interaction between the vortical gusts and wings is seemingly complex and different for each combination of gust parameters, we show in this study that the fundamental phy
    
[^30]: TIPS：任何时候神经网络的拓扑重要路径采样

    TIPS: Topologically Important Path Sampling for Anytime Neural Networks. (arXiv:2305.08021v1 [cs.LG])

    [http://arxiv.org/abs/2305.08021](http://arxiv.org/abs/2305.08021)

    TIPS是一种自动设计AnytimeNNs框架，通过识别贡献最大的路径来提高收敛速度和测试准确率，比现有方法提高了2%-6.6%的准确率，在准确率-FLOPs之间取得了最佳平衡。

    

    任何时候神经网络(AnytimeNNs) 是一种能够在各种硬件资源约束下适应性调整模型复杂度的有前途的解决方案。然而，手动设计的 AnytimeNNs 往往会受到设计师先前经验的影响，从而提供次优解。为了解决现有手工方法的限制，我们首先将 AnytimeNNs 的训练过程建模为离散时间马尔可夫链(DTMC)，并利用它来识别对 AnytimeNNs 训练贡献最大的路径。基于这种新的 DTMC 基础分析，我们进一步提出了 TIPS (Topologically Important Path Sampling) 框架，以自动设计适应各种硬件约束的 AnytimeNNs。我们的实验结果表明，TIPS 能够提高 AnytimeNNs 的收敛速度和测试准确率。与现有 AnytimeNNs 方法相比，TIPS 在多个数据集上将准确率提高了 2%-6.6%，并实现了 SOTA 的准确率-FLOPs 折衷。

    Anytime neural networks (AnytimeNNs) are a promising solution to adaptively adjust the model complexity at runtime under various hardware resource constraints. However, the manually-designed AnytimeNNs are biased by designers' prior experience and thus provide sub-optimal solutions. To address the limitations of existing hand-crafted approaches, we first model the training process of AnytimeNNs as a discrete-time Markov chain (DTMC) and use it to identify the paths that contribute the most to the training of AnytimeNNs. Based on this new DTMC-based analysis, we further propose TIPS, a framework to automatically design AnytimeNNs under various hardware constraints. Our experimental results show that TIPS can improve the convergence rate and test accuracy of AnytimeNNs. Compared to the existing AnytimeNNs approaches, TIPS improves the accuracy by 2%-6.6% on multiple datasets and achieves SOTA accuracy-FLOPs tradeoffs.
    
[^31]: DRew：带延迟的动态重连消息传递

    DRew: Dynamically Rewired Message Passing with Delay. (arXiv:2305.08018v1 [cs.LG])

    [http://arxiv.org/abs/2305.08018](http://arxiv.org/abs/2305.08018)

    本文提出了一种能够应用于任何MPNN结构的框架，执行基于层的动态重连来确保逐渐密集化的图形。同时引入了一种延迟机制，允许跨层节点之间的跳跃连接。

    

    已经证明，消息传递神经网络（MPNN）存在过度压缩现象，导致长程相互作用任务表现不佳。这主要归因于只在节点的相邻居之间进行局部消息传递。试图使图形“更连通”并且更适合长程任务的重连方法通常会失去基于图形距离提供的归纳偏差，因为它们会使远程节点在每一层中立即通信。在本文中，我们提出了一个框架，可应用于任何MPNN架构，以执行基于层的重连，以确保逐渐加密图形。我们还提出了一种延迟机制，它允许根据层和它们的相互距离在节点之间进行跳跃连接。我们在几个长程任务上验证了我们的方法，并表明其优于图形变换器和多跳MPNN。

    Message passing neural networks (MPNNs) have been shown to suffer from the phenomenon of over-squashing that causes poor performance for tasks relying on long-range interactions. This can be largely attributed to message passing only occurring locally, over a node's immediate neighbours. Rewiring approaches attempting to make graphs `more connected', and supposedly better suited to long-range tasks, often lose the inductive bias provided by distance on the graph since they make distant nodes communicate instantly at every layer. In this paper we propose a framework, applicable to any MPNN architecture, that performs a layer-dependent rewiring to ensure gradual densification of the graph. We also propose a delay mechanism that permits skip connections between nodes depending on the layer and their mutual distance. We validate our approach on several long-range tasks and show that it outperforms graph Transformers and multi-hop MPNNs.
    
[^32]: 基于表面肌电图像的轻量级全卷积神经网络和迁移学习的跨场景手势识别

    Surface EMG-Based Inter-Session/Inter-Subject Gesture Recognition by Leveraging Lightweight All-ConvNet and Transfer Learning. (arXiv:2305.08014v1 [cs.CV])

    [http://arxiv.org/abs/2305.08014](http://arxiv.org/abs/2305.08014)

    本论文提出了一种轻量级全卷积神经网络+迁移学习方法，能够在跨场景手势识别任务中有效解决数据变异性问题。

    

    利用低分辨率瞬时高清肌电图像进行手势识别可以开辟发展更流畅、更自然的肌肉-计算机界面的新途径。然而，跨场景数据的变异性存在极大的挑战。现有的方法采用非常大且复杂的深度卷积神经网络或基于2SRNN的领域适应方法，来逼近由这些跨场景数据变异性引起的分布偏移。因此，这些方法也需要在预训练和适应阶段中在数百万个训练参数和大规模预训练数据集上进行学习。结果，这使得在实时应用中进行高端资源约束和计算非常昂贵的部署。为了解决这个问题，我们提出了一种轻量级的全卷积神经网络+迁移学习模型，利用轻量级全卷积神经网络和迁移学习(TL)来增强跨场景手势识别。

    Gesture recognition using low-resolution instantaneous HD-sEMG images opens up new avenues for the development of more fluid and natural muscle-computer interfaces. However, the data variability between inter-session and inter-subject scenarios presents a great challenge. The existing approaches employed very large and complex deep ConvNet or 2SRNN-based domain adaptation methods to approximate the distribution shift caused by these inter-session and inter-subject data variability. Hence, these methods also require learning over millions of training parameters and a large pre-trained and target domain dataset in both the pre-training and adaptation stages. As a result, it makes high-end resource-bounded and computationally very expensive for deployment in real-time applications. To overcome this problem, we propose a lightweight All-ConvNet+TL model that leverages lightweight All-ConvNet and transfer learning (TL) for the enhancement of inter-session and inter-subject gesture recogniti
    
[^33]: 通过损失压缩分析深度神经网络的信息瓶颈(arXiv:2305.08013v1 [cs.LG])

    Information Bottleneck Analysis of Deep Neural Networks via Lossy Compression. (arXiv:2305.08013v1 [cs.LG])

    [http://arxiv.org/abs/2305.08013](http://arxiv.org/abs/2305.08013)

    本文提出了一种综合框架，用于对一般神经网络进行信息瓶颈分析，以研究训练过程中的拟合和压缩阶段。通过该分析，可以更好地理解深度神经网络的泛化能力。

    

    信息瓶颈原理提供了一个信息论框架，用于分析深度神经网络(DNNs)的训练过程。其核心在于跟踪隐藏层与类标签之间的互信息值和隐藏层与DNN输入之间的互信息值的动态变化。据Shwartz-Ziv和Tishby(2017)提出的假说，训练过程由两个不同的阶段组成:拟合和压缩。后者被认为是DNNs表现良好的泛化能力的原因。本文引入了一个综合框架，用于进行对一般NNs的IB分析。我们的方法利用了Goldfeld等人(2019)提出的随机神经网络方法，并结合了一种压缩方法。

    The Information Bottleneck (IB) principle offers an information-theoretic framework for analyzing the training process of deep neural networks (DNNs). Its essence lies in tracking the dynamics of two mutual information (MI) values: one between the hidden layer and the class label, and the other between the hidden layer and the DNN input. According to the hypothesis put forth by Shwartz-Ziv and Tishby (2017), the training process consists of two distinct phases: fitting and compression. The latter phase is believed to account for the good generalization performance exhibited by DNNs. Due to the challenging nature of estimating MI between high-dimensional random vectors, this hypothesis has only been verified for toy NNs or specific types of NNs, such as quantized NNs and dropout NNs. In this paper, we introduce a comprehensive framework for conducting IB analysis of general NNs. Our approach leverages the stochastic NN method proposed by Goldfeld et al. (2019) and incorporates a compres
    
[^34]: 深度神经网络的连续仿射学习

    Successive Affine Learning for Deep Neural Networks. (arXiv:2305.07996v1 [cs.LG])

    [http://arxiv.org/abs/2305.07996](http://arxiv.org/abs/2305.07996)

    本文提出了一种连续仿射学习（SAL）模型，用于构建深度神经网络(DNNs)。 该模型通过解决涉及激活函数的二次/凸优化问题来学习仿射映射，但是在权重矩阵和偏差之后。

    

    本文介绍了一种用于构建深度神经网络(DNNs)的连续仿射学习(SAL)模型。传统上，DNN是通过解决非凸优化问题来构建的。由于其非凸性和层数众多，通常难以在数值上解决这种问题。为了解决这一挑战，本文作者启发于人类教育系统，最近提出了多级深度学习(MGDL)模型。MGDL模型以多个年级的形式学习DNN，在每个年级中构建由少量层数组成的浅层DNN。MGDL模型仍需要解决几个非凸优化问题。提出的SAL模型是在MGDL模型基础上演变而来。发现DNN的每层都由仿射映射和激活函数组成，我们建议通过解决涉及激活函数的二次/凸优化问题来学习仿射映射，但是在权重矩阵和偏差之后。

    This paper introduces a successive affine learning (SAL) model for constructing deep neural networks (DNNs). Traditionally, a DNN is built by solving a non-convex optimization problem. It is often challenging to solve such a problem numerically due to its non-convexity and having a large number of layers. To address this challenge, inspired by the human education system, the multi-grade deep learning (MGDL) model was recently initiated by the author of this paper. The MGDL model learns a DNN in several grades, in each of which one constructs a shallow DNN consisting of a small number of layers. The MGDL model still requires solving several non-convex optimization problems. The proposed SAL model mutates from the MGDL model. Noting that each layer of a DNN consists of an affine map followed by an activation function, we propose to learn the affine map by solving a quadratic/convex optimization problem which involves the activation function only {\it after} the weight matrix and the bias
    
[^35]: 通过对偶形式学习非负低秩张量补全及其在图像和视频修复中的应用

    Nonnegative Low-Rank Tensor Completion via Dual Formulation with Applications to Image and Video Completion. (arXiv:2305.07976v1 [cs.CV])

    [http://arxiv.org/abs/2305.07976](http://arxiv.org/abs/2305.07976)

    本文提出了一种对偶形式的非负低秩张量分解方法，解决了张量补全中忽视数据非负结构的问题，并在多个任务上优于最先进的算法。

    

    最近的张量补全方法往往忽视了数据的非负结构。本文考虑学习一个非负低秩张量，并利用对偶理论，提出了一种新的张量分解方法。这种分解将非负约束从低秩约束中分离出来。由此得到的问题是流形上的优化问题，本文提出了一种变种的黎曼共轭梯度算法来解决它。我们在颜色图像修补、视频补全和高光谱图像补全等多个任务上测试了所提出的算法。实验结果表明，该方法优于许多最先进的张量补全算法。

    Recent approaches to the tensor completion problem have often overlooked the nonnegative structure of the data. We consider the problem of learning a nonnegative low-rank tensor, and using duality theory, we propose a novel factorization of such tensors. The factorization decouples the nonnegative constraints from the low-rank constraints. The resulting problem is an optimization problem on manifolds, and we propose a variant of Riemannian conjugate gradients to solve it. We test the proposed algorithm across various tasks such as colour image inpainting, video completion, and hyperspectral image completion. Experimental results show that the proposed method outperforms many state-of-the-art tensor completion algorithms.
    
[^36]: 论随机安全性的计算成本

    On the Computational Cost of Stochastic Security. (arXiv:2305.07973v1 [cs.LG])

    [http://arxiv.org/abs/2305.07973](http://arxiv.org/abs/2305.07973)

    本文探究了使用长期持续蒙特卡罗模拟是否能提高能量模型的质量，并通过增加计算预算改进了模型的校准性和对抗鲁棒性。

    

    我们探讨了使用朗之万动力学的长期持续蒙特卡罗模拟是否会提高基于能量的模型（EBM）所达到的表征质量。我们考虑一种方案，其中使用训练过的EBM的扩散过程的蒙特卡罗模拟，用于提高独立分类器网络的对抗鲁棒性和校准分数。我们的结果表明，在持续对比散度的计算预算增加吉布斯采样的情况下，改进了模型的校准性和对抗鲁棒性，澄清了实现有效从连续能量势中进行吉布斯采样的新量子和经典硬件和软件的实际价值。

    We investigate whether long-run persistent chain Monte Carlo simulation of Langevin dynamics improves the quality of the representations achieved by energy-based models (EBM). We consider a scheme wherein Monte Carlo simulation of a diffusion process using a trained EBM is used to improve the adversarial robustness and the calibration score of an independent classifier network. Our results show that increasing the computational budget of Gibbs sampling in persistent contrastive divergence improves the calibration and adversarial robustness of the model, elucidating the practical merit of realizing new quantum and classical hardware and software for efficient Gibbs sampling from continuous energy potentials.
    
[^37]: 度量空间中图嵌入的紧致快速泛化误差界限

    Tight and fast generalization error bound of graph embedding in metric space. (arXiv:2305.07971v1 [stat.ML])

    [http://arxiv.org/abs/2305.07971](http://arxiv.org/abs/2305.07971)

    本文提供了一个紧致快速的泛化误差上限，可以保证非欧几里得度量空间中的图嵌入在实际训练数据规模下的成功。

    

    近期的研究实验证明，我们可以在非欧几里得度量空间中实现有效且高效的图嵌入，旨在获得在度量空间中反映图结构的顶点表示。特别地，在双曲空间中的图嵌入在嵌入具有分层树结构的图上，例如自然语言、社交网络和知识库中的数据，已经获得了实验成功。然而，最近的理论分析显示，非欧几里得图嵌入的泛化误差比欧几里得图嵌入的误差上限要高得多，高泛化误差表明数据中的不完整性和噪声可能会显著损坏学习性能。这意味着现有的界限不能保证实际训练数据规模下在非欧几里得度量空间中的图嵌入成功，这可能会阻止非欧几里得图嵌入在实际问题中的应用。本文提供了一个新的非欧几里得度量空间中图嵌入的泛化误差上限，该上限紧凑且快速，可以保证在实际训练数据规模下非欧几里得图嵌入的成功。

    Recent studies have experimentally shown that we can achieve in non-Euclidean metric space effective and efficient graph embedding, which aims to obtain the vertices' representations reflecting the graph's structure in the metric space. Specifically, graph embedding in hyperbolic space has experimentally succeeded in embedding graphs with hierarchical-tree structure, e.g., data in natural languages, social networks, and knowledge bases. However, recent theoretical analyses have shown a much higher upper bound on non-Euclidean graph embedding's generalization error than Euclidean one's, where a high generalization error indicates that the incompleteness and noise in the data can significantly damage learning performance. It implies that the existing bound cannot guarantee the success of graph embedding in non-Euclidean metric space in a practical training data size, which can prevent non-Euclidean graph embedding's application in real problems. This paper provides a novel upper bound of
    
[^38]: 结构化低秩张量学习

    Structured Low-Rank Tensor Learning. (arXiv:2305.07967v1 [cs.LG])

    [http://arxiv.org/abs/2305.07967](http://arxiv.org/abs/2305.07967)

    本文提出了一种结构化低秩张量学习的解决方案，并通过优化算法在流形上求解，解决了部分观测和结构约束下的学习问题。

    

    本文研究在具有结构约束的部分观察情况下学习低秩张量的问题，并提出了一种新颖的张量分解方法，从而导致了一个更简单的优化问题。由此产生的问题是在流形上的优化问题。我们开发了一阶和二阶黎曼优化算法来解决它。已推导出所得问题的对偶间隙，并通过实验证明了所提出算法的正确性。我们演示了在非负约束和Hankel约束条件下的算法。

    We consider the problem of learning low-rank tensors from partial observations with structural constraints, and propose a novel factorization of such tensors, which leads to a simpler optimization problem. The resulting problem is an optimization problem on manifolds. We develop first-order and second-order Riemannian optimization algorithms to solve it. The duality gap for the resulting problem is derived, and we experimentally verify the correctness of the proposed algorithm. We demonstrate the algorithm on nonnegative constraints and Hankel constraints.
    
[^39]: 在对话推荐系统中利用大型语言模型

    Leveraging Large Language Models in Conversational Recommender Systems. (arXiv:2305.07961v1 [cs.IR])

    [http://arxiv.org/abs/2305.07961](http://arxiv.org/abs/2305.07961)

    本文提出了一种使用大型语言模型构建端到端大规模对话推荐系统的路线图，解决在该系统中有效利用大型语言模型所面临的技术挑战。

    

    对话推荐系统通过启用实时的多轮对话使用户更加透明和掌控。最近，大型语言模型展现了与人类对话自然的能力，并将世界知识和常识推理融入到语言理解中，进一步释放了这一范式的潜力。然而，在对话推荐系统中有效利用大型语言模型引入了新的技术挑战，包括适当地理解和控制复杂的对话和从外部信息源检索。由于大而不断增长的项目语料库和缺乏对话数据进行训练，这些问题加剧了。在本文中，我们提供了使用大型语言模型构建端到端大规模对话推荐系统的路线图。特别地，我们提出了用户偏好理解、灵活的对话管理和可解释的推荐作为整个系统的一部分的新实现方式。

    A Conversational Recommender System (CRS) offers increased transparency and control to users by enabling them to engage with the system through a real-time multi-turn dialogue. Recently, Large Language Models (LLMs) have exhibited an unprecedented ability to converse naturally and incorporate world knowledge and common-sense reasoning into language understanding, unlocking the potential of this paradigm. However, effectively leveraging LLMs within a CRS introduces new technical challenges, including properly understanding and controlling a complex conversation and retrieving from external sources of information. These issues are exacerbated by a large, evolving item corpus and a lack of conversational data for training. In this paper, we provide a roadmap for building an end-to-end large-scale CRS using LLMs. In particular, we propose new implementations for user preference understanding, flexible dialogue management and explainable recommendations as part of an integrated architecture
    
[^40]: 一种用于分类树优化学习的新型记忆策略

    A Novel Memetic Strategy for Optimized Learning of Classification Trees. (arXiv:2305.07959v1 [cs.LG])

    [http://arxiv.org/abs/2305.07959](http://arxiv.org/abs/2305.07959)

    本论文提出了一种新的演化算法，用于诱导分类树，利用记忆学习方法处理大型数据集，结构具有泛化能力且与最先进的方法相当竞争力。

    

    鉴于可解释机器学习的越来越多的关注，分类树因其玻璃盒结构再次引起了科学界的关注。这些模型通常使用贪心算法构建，通过解决问题来找到最小化某些不纯度度量的特征空间中的切割点。与这种标准的贪心方法和最近通过基于MILP的精确公式来定义学习问题的进展相反，在本文中，我们提出了一种新的演化算法，用于诱导分类树，利用一种记忆方法处理数以千计的数据集。我们的过程将可行解空间的探索与局部搜索相结合，以获得具有泛化能力且与最先进的方法竞争力相当的结构。

    Given the increasing interest in interpretable machine learning, classification trees have again attracted the attention of the scientific community because of their glass-box structure. These models are usually built using greedy procedures, solving subproblems to find cuts in the feature space that minimize some impurity measures. In contrast to this standard greedy approach and to the recent advances in the definition of the learning problem through MILP-based exact formulations, in this paper we propose a novel evolutionary algorithm for the induction of classification trees that exploits a memetic approach that is able to handle datasets with thousands of points. Our procedure combines the exploration of the feasible space of solutions with local searches to obtain structures with generalization capabilities that are competitive with the state-of-the-art methods.
    
[^41]: 更少的数据更强的安全策略优化性能保证

    More for Less: Safe Policy Improvement With Stronger Performance Guarantees. (arXiv:2305.07958v1 [cs.LG])

    [http://arxiv.org/abs/2305.07958](http://arxiv.org/abs/2305.07958)

    本论文提出了一种新的方法，可以用较少的数据保证安全策略优化(SPI)的性能，并降低了SPIBB算法的样本复杂度。

    

    在离线强化学习环境中，安全策略优化(SPI)问题旨在根据生成样本数据的行为策略，提高其性能。现有的解决SPI问题的方法需要较高数量的样本，以提供对改进策略性能的实际概率保证。我们提出了一种新的方法来解决SPI问题，用较少的数据即可获得这样的保证。具体来说，为了证明这些保证的正确性，我们设计了数据集和基础环境模型上的隐式转换，这些转换作为推导更紧密的SPI改进界限的理论基础。我们使用专业的SPI与基线引导(SPIBB)算法在标准基准测试中进行实证评估，结果表明我们的方法确实显著降低了SPIBB算法的样本复杂度。

    In an offline reinforcement learning setting, the safe policy improvement (SPI) problem aims to improve the performance of a behavior policy according to which sample data has been generated. State-of-the-art approaches to SPI require a high number of samples to provide practical probabilistic guarantees on the improved policy's performance. We present a novel approach to the SPI problem that provides the means to require less data for such guarantees. Specifically, to prove the correctness of these guarantees, we devise implicit transformations on the data set and the underlying environment model that serve as theoretical foundations to derive tighter improvement bounds for SPI. Our empirical evaluation, using the well-established SPI with baseline bootstrapping (SPIBB) algorithm, on standard benchmarks shows that our method indeed significantly reduces the sample complexity of the SPIBB algorithm.
    
[^42]: CodeT5+: 用于代码理解和生成的开放代码大型语言模型

    CodeT5+: Open Code Large Language Models for Code Understanding and Generation. (arXiv:2305.07922v1 [cs.CL])

    [http://arxiv.org/abs/2305.07922](http://arxiv.org/abs/2305.07922)

    CodeT5+是一组灵活组合的编码器-解码器LLM族，用于代码，混合了多种不同的预训练目标，包括代码生成、自然语言处理和程序合成，可以适应多种不同的下游代码任务，并且在实验中比现有代码-specific LLMs实现了最先进的性能。

    

    预训练在大量源代码上的大型语言模型(LLMs)在代码智能方面取得了显著进展。然而，现有的代码LLM在架构和预训练任务方面有两个主要限制。首先，它们通常采用特定的架构(仅编码器或仅解码器)或依赖于不同下游任务的统一编码器-解码器网络。前一种范式受到应用灵活性的限制，而在后一种范式中，模型被视为所有任务的单一系统，导致在某些任务的子集上性能不优。其次，它们通常采用有限的预训练目标，这些目标可能与某些下游任务不相关，因此会导致性能显著下降。为了解决这些限制，我们提出了“CodeT5+”，这是一组编码器-解码器LLM族，用于代码，其中组件模块可以灵活组合以适应各种下游代码任务。这种灵活性是通过我们提出的混合预训练目标实现的，包括代码生成，自然语言处理和程序合成。我们在几个与代码相关的下游任务上进行了广泛实验，证明CodeT5+相对于现有的代码特定LLM实现了最先进的性能。

    Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading to suboptimal performance on a subset of tasks. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some downstream tasks and hence result in substantial performance degrade. To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks. Such flexibility is enabled by our proposed mixture of pretrai
    
[^43]: 布尔权重优化的收敛性和尺度在硬件水库中的应用

    Convergence and scaling of Boolean-weight optimization for hardware reservoirs. (arXiv:2305.07908v1 [stat.ML])

    [http://arxiv.org/abs/2305.07908](http://arxiv.org/abs/2305.07908)

    给出了在随机水库模型上使用坐标下降法进行优化的收敛性分析和尺度定律，为硬件网络优化提供了坚实的基础。

    

    实现神经网络的硬件化是实现下一代高效和强大人工智能解决方案的重要一步。除了实现并行、高效和可扩展的硬件架构外，用抽样有效的方法优化系统极大的参数空间也至关重要。本研究分析地导出了高效坐标下降法在优化随机复杂神经网络--水库的读出层所需的尺度定律。我们证明了收敛是指数级的，并且随着网络神经元数量的线性缩放。我们的研究完全复现了一个大规模的光子水库实验中的收敛性和尺度。因此，我们的工作为硬件网络优化提供了坚实的基础，并确定了有前途的优化收束速度的未来方向。

    Hardware implementation of neural network are an essential step to implement next generation efficient and powerful artificial intelligence solutions.  Besides the realization of a parallel, efficient and scalable hardware architecture, the optimization of the system's extremely large parameter space with sampling-efficient approaches is essential.  Here, we analytically derive the scaling laws for highly efficient Coordinate Descent applied to optimizing the readout layer of a random recurrently connection neural network, a reservoir.  We demonstrate that the convergence is exponential and scales linear with the network's number of neurons.  Our results perfectly reproduce the convergence and scaling of a large-scale photonic reservoir implemented in a proof-of-concept experiment.  Our work therefore provides a solid foundation for such optimization in hardware networks, and identifies future directions that are promising for optimizing convergence speed during learning leveraging mea
    
[^44]: Network-GIANT: 基于哈蒙莫特 Hessian 一致性的全分布式牛顿型优化

    Network-GIANT: Fully distributed Newton-type optimization via harmonic Hessian consensus. (arXiv:2305.07898v1 [math.OC])

    [http://arxiv.org/abs/2305.07898](http://arxiv.org/abs/2305.07898)

    本文提出了一种基于哈蒙莫特 Hessian 一致性的全分布式牛顿型优化算法 Network-GIANT，将梯度跟踪和牛顿型迭代算法相结合，经证明对严格凸和光滑损失函数有半全局和指数收敛到精确解的保证，实验证明 Network-GIANT 优于其他分布式学习算法（如 Network-DANE 和 Newton-Raphson Consensus）的收敛性能。

    

    本文考虑了分布式多代理学习的问题，其中全局目标是通过本地优化和节点之间的信息交换来最小化本地目标（经验损失）函数的总和。 我们介绍了一种新的牛顿型完全分布式优化算法，Network-GIANT，它基于 GIANT，这是一种依赖于集中式参数服务器的联邦学习算法。 Network-GIANT 算法是通过在每个节点上使用梯度跟踪和牛顿型迭代算法的组合以及本地梯度和牛顿更新的共识平均来设计的。我们证明了我们的算法保证了对网络上的严格凸和光滑损失函数的半全局和指数收敛到精确解。我们提供了 Network-GIANT 优于其他最先进的分布式学习算法（如 Network-DANE 和 Newton-Raphson Consensus）的收敛性能的实证证据。

    This paper considers the problem of distributed multi-agent learning, where the global aim is to minimize a sum of local objective (empirical loss) functions through local optimization and information exchange between neighbouring nodes. We introduce a Newton-type fully distributed optimization algorithm, Network-GIANT, which is based on GIANT, a Federated learning algorithm that relies on a centralized parameter server. The Network-GIANT algorithm is designed via a combination of gradient-tracking and a Newton-type iterative algorithm at each node with consensus based averaging of local gradient and Newton updates. We prove that our algorithm guarantees semi-global and exponential convergence to the exact solution over the network assuming strongly convex and smooth loss functions. We provide empirical evidence of the superior convergence performance of Network-GIANT over other state-of-art distributed learning algorithms such as Network-DANE and Newton-Raphson Consensus.
    
[^45]: 利用三维无监督和(深度)监督神经网络进行增材制造件孔隙率检测的体素级分类

    Voxel-wise classification for porosity investigation of additive manufactured parts with 3D unsupervised and (deeply) supervised neural networks. (arXiv:2305.07894v1 [cs.CE])

    [http://arxiv.org/abs/2305.07894](http://arxiv.org/abs/2305.07894)

    本研究使用三维无监督和(深度)监督神经网络进行增材制造件孔隙率检测的体素级分类，得出使用体素级分类的三维DL模型在AM零件的孔隙率检测方面具有巨大潜力的结论。

    

    增材制造已成为一种生产进程，可以直接从数字模型中生产样本。为了确保在整个制造批次中的所有产品样本都符合质量标准，通常使用X射线计算机断层扫描（X-CT）与自动化异常检测相结合。本研究重新审视了最近的监督和无监督深度学习（DL）模型，对来自X-CT图像的AM样品中的孔隙分析进行了扩展，以接受3D输入数据进行体素级分类，从而降低计算要求，提高效率和通用性。结果表明，使用体素级分类的三维DL模型在AM零件的孔隙率检测方面具有巨大的潜力。

    Additive Manufacturing (AM) has emerged as a manufacturing process that allows the direct production of samples from digital models. To ensure that quality standards are met in all manufactured samples of a batch, X-ray computed tomography (X-CT) is often used combined with automated anomaly detection. For the latter, deep learning (DL) anomaly detection techniques are increasingly, as they can be trained to be robust to the material being analysed and resilient towards poor image quality. Unfortunately, most recent and popular DL models have been developed for 2D image processing, thereby disregarding valuable volumetric information.  This study revisits recent supervised (UNet, UNet++, UNet 3+, MSS-UNet) and unsupervised (VAE, ceVAE, gmVAE, vqVAE) DL models for porosity analysis of AM samples from X-CT images and extends them to accept 3D input data with a 3D-patch pipeline for lower computational requirements, improved efficiency and generalisability. The supervised models were trai
    
[^46]: DAC-MR: 基于数据增强一致性的元学习元正则化

    DAC-MR: Data Augmentation Consistency Based Meta-Regularization for Meta-Learning. (arXiv:2305.07892v1 [cs.LG])

    [http://arxiv.org/abs/2305.07892](http://arxiv.org/abs/2305.07892)

    为了提高元学习的性能，我们提出了一个基于元知识信息增强的元学习框架。我们通过使用适当的MR目标将元知识初步集成到元目标中，来规范元模型函数类的容量复杂度，有助于在未知任务上实现更好的泛化性能。

    

    近年来，元学习在机器学习领域内备受关注并推动了现代机器学习的发展。然而，要实现表现良好的元学习模型需要大量具有高质量元数据的训练任务，以表示底层任务泛化目标，有时对于真实应用而言难以获得。当前基于元数据的元学习方法，难以使用不完美的训练任务训练令人满意的元模型。为了解决这个问题，我们提出了一个元知识信息增强的元学习框架（MKIML），通过将补偿的元知识集成到元学习过程中，全面提高元学习的性能。我们通过使用适当的元正则化（MR）目标将元知识初步集成到元目标中，来规范元模型函数类的容量复杂度，有助于在未知任务上实现更好的泛化性能。作为一种实用化实现，我们提出数据增强。

    Meta learning recently has been heavily researched and helped advance the contemporary machine learning. However, achieving well-performing meta-learning model requires a large amount of training tasks with high-quality meta-data representing the underlying task generalization goal, which is sometimes difficult and expensive to obtain for real applications. Current meta-data-driven meta-learning approaches, however, are fairly hard to train satisfactory meta-models with imperfect training tasks. To address this issue, we suggest a meta-knowledge informed meta-learning (MKIML) framework to improve meta-learning by additionally integrating compensated meta-knowledge into meta-learning process. We preliminarily integrate meta-knowledge into meta-objective via using an appropriate meta-regularization (MR) objective to regularize capacity complexity of the meta-model function class to facilitate better generalization on unseen tasks. As a practical implementation, we introduce data augmenta
    
[^47]: 结构模拟和桥梁健康监测的神经运算器

    Neural operator for structural simulation and bridge health monitoring. (arXiv:2305.07889v1 [cs.LG])

    [http://arxiv.org/abs/2305.07889](http://arxiv.org/abs/2305.07889)

    本论文提出了结构模拟和桥梁健康监测的神经运算器VINO，通过学习结构响应场和损伤场之间的映射，在前向预测和反向确定损伤区域和程度方面可以比传统有限元模型更准确地预测和判断。

    

    将深度学习与结构工程相结合已经受到了广泛关注，用于前向问题（结构模拟）和反向问题（结构健康监测）。本研究基于傅里叶神经运算器，提出了VINO（车辆-桥梁相互作用神经运算器），作为桥梁结构的数字孪生。VINO学习结构响应场和损伤场之间的映射。本研究通过运行参数有限元（FE）模拟，考虑结构初始损伤场的随机分布，建立了VBI-FE数据集。随后，在四种损伤情况下进行了实验研究，产生了VBI-EXP数据集。在VINO通过VBI-FE预训练并在健康状态下通过VBI-EXP微调后，模型实现了以下两个改进。首先，前向的VINO比FE模型更准确地从损伤场输入预测结构响应。其次，反向的VINO可以确定损伤区域和程度。

    Infusing deep learning with structural engineering has received widespread attention for both forward problems (structural simulation) and inverse problems (structural health monitoring). Based on Fourier Neural Operator, this study proposes VINO (Vehicle-bridge Interaction Neural Operator) to serve as the digital twin of bridge structures. VINO learns mappings between structural response fields and damage fields. In this study, VBI-FE dataset was established by running parametric finite element (FE) simulations considering a random distribution of structural initial damage field. Subsequently, VBI-EXP dataset was produced by conducting an experimental study under four damage scenarios. After VINO was pre-trained by VBI-FE and fine-tuned by VBI-EXP from the bridge at the healthy state, the model achieved the following two improvements. First, forward VINO can predict structural responses from damage field inputs more accurately than the FE model. Second, inverse VINO can determine, loc
    
[^48]: 通过Logit Attribution匹配实现对比度领域泛化

    Contrastive Domain Generalization via Logit Attribution Matching. (arXiv:2305.07888v1 [cs.LG])

    [http://arxiv.org/abs/2305.07888](http://arxiv.org/abs/2305.07888)

    本论文提出了一种名为对比领域泛化（CDG）的新方法，通过强烈对比的数据对所展示的语义不变性进行利用。同时，提出了一种正则化技术——Logit Attribution Matching (LAM)，以实现CDG。实验结果表明，LAM仅使用少量配对数据就能胜过最先进的DG方法，且有助于模型更好地关注对领域泛化至关重要的语义特征。

    

    领域泛化是机器学习中一个重要的开放性问题。深度模型容易受到甚至微小程度的领域偏移的影响，在实际应用中严重损害其可靠性。为了缓解这个问题，大多数现有的方法在多个训练领域上加强各种不变量限制。然而，这种方法通常不能为新的测试领域提供很好的性能保证。在本文中，我们研究了一种不同的方法，名为对比领域泛化（CDG），它利用强烈对比的数据对所展示的语义不变性，而不是多个域。我们提出了一个因果领域泛化理论，展示了CDG的潜在能力；同时，我们还提出了一种正则化技术——Logit Attribution Matching (LAM)，以实现CDG。我们在实证上展示了，LAM仅使用少量配对数据就能胜过最先进的DG方法，而且LAM有助于模型更好地关注对领域泛化至关重要的语义特征。

    Domain Generalization (DG) is an important open problem in machine learning. Deep models are susceptible to domain shifts of even minute degrees, which severely compromises their reliability in real applications. To alleviate the issue, most existing methods enforce various invariant constraints across multiple training domains. However,such an approach provides little performance guarantee for novel test domains in general. In this paper, we investigate a different approach named Contrastive Domain Generalization (CDG), which exploits semantic invariance exhibited by strongly contrastive data pairs in lieu of multiple domains. We present a causal DG theory that shows the potential capability of CDG; together with a regularization technique, Logit Attribution Matching (LAM), for realizing CDG. We empirically show that LAM outperforms state-of-the-art DG methods with only a small portion of paired data and that LAM helps models better focus on semantic features which are crucial to DG.
    
[^49]: 基于例行血液检查数值的机器学习模型鉴别病毒和细菌感染。

    Differentiating Viral and Bacterial Infections: A Machine Learning Model Based on Routine Blood Test Values. (arXiv:2305.07877v1 [cs.LG])

    [http://arxiv.org/abs/2305.07877](http://arxiv.org/abs/2305.07877)

    本研究开发了一种基于血液检查数值的病毒与细菌机器学习模型，用于准确识别感染类型。该模型在CRP水平10-40 mg/L范围内表现出更好的区分细菌和病毒感染的准确性，证明了多种血液参数对于诊断决策的重要性。

    

    随着抗生素耐药性日益威胁，正确区分细菌和病毒感染以进行正确的抗生素使用变得越来越重要。本研究开发了一种基于16个例行血液检查结果、C-反应蛋白水平、生物性别和年龄的病毒与细菌机器学习模型，用于区分这些感染类型。使用单个医疗中心的44,120个案例数据集，"病毒 vs. 细菌"模型表现出令人瞩目的82.2%的准确率，0.129的Brier得分和0.91的ROC曲线下面积，超越了传统CRP决策规则模型的性能。该模型在CRP范围为10-40 mg/L时表现出显著的改进准确性，这个范围内仅靠CRP无法为细菌和病毒感染进行区分的诊断价值有限。这些发现强调了在诊断决策中考虑多种血液参数的重要性，并建议病毒 vs. 细菌模型使得应用于临床决策成为可能。

    The growing threat of antibiotic resistance necessitates accurate differentiation between bacterial and viral infections for proper antibiotic administration. In this study, a Virus vs. Bacteria machine learning model was developed to discern between these infection types using 16 routine blood test results, C-reactive protein levels, biological sex, and age. With a dataset of 44,120 cases from a single medical center, the Virus vs. Bacteria model demonstrated remarkable accuracy of 82.2%, a Brier score of 0.129, and an area under the ROC curve of 0.91, surpassing the performance of traditional CRP decision rule models. The model demonstrates substantially improved accuracy within the CRP range of 10 40 mg/L, an interval in which CRP alone offers limited diagnostic value for distinguishing between bacterial and viral infections. These findings underscore the importance of considering multiple blood parameters for diagnostic decision-making and suggest that the Virus vs. Bacteria model 
    
[^50]: SPP-CNN：一种网络鲁棒性预测的高效框架

    SPP-CNN: An Efficient Framework for Network Robustness Prediction. (arXiv:2305.07872v1 [cs.LG])

    [http://arxiv.org/abs/2305.07872](http://arxiv.org/abs/2305.07872)

    本文提出了一种名为SPP-CNN的高效框架，可用于网络鲁棒性预测。该框架通过在卷积和全连接层之间添加空间金字塔池化层，克服了CNN预测方法中常见的不匹配问题，并在综合实验中表现出优越性能。

    

    本文解决了网络在遭受恶意攻击时维持连接性和可控性的鲁棒性问题。新提出的框架SPP-CNN在卷积和全连接层之间装载空间金字塔池化层，解决了基于CNN的预测方法中常见的不匹配问题，并提高了其泛化能力。

    This paper addresses the robustness of a network to sustain its connectivity and controllability against malicious attacks. This kind of network robustness is typically measured by the time-consuming attack simulation, which returns a sequence of values that record the remaining connectivity and controllability after a sequence of node- or edge-removal attacks. For improvement, this paper develops an efficient framework for network robustness prediction, the spatial pyramid pooling convolutional neural network (SPP-CNN). The new framework installs a spatial pyramid pooling layer between the convolutional and fully-connected layers, overcoming the common mismatch issue in the CNN-based prediction approaches and extending its generalizability. Extensive experiments are carried out by comparing SPP-CNN with three state-of-the-art robustness predictors, namely a CNN-based and two graph neural networks-based frameworks. Synthetic and real-world networks, both directed and undirected, are in
    
[^51]: 基于预训练语言模型的可扩展教学题生成

    Scalable Educational Question Generation with Pre-trained Language Models. (arXiv:2305.07871v1 [cs.AI])

    [http://arxiv.org/abs/2305.07871](http://arxiv.org/abs/2305.07871)

    这项研究开发了一种新的教育问题生成模型，能够通过在科学文本和科学问题数据上进行预训练和微调预训练语言模型，实现优秀的教育问题自动生成。

    

    在全球人口在探索个性化学习之旅时，教育问题的自动生成将在在线教育的扩展中发挥关键作用，实现大规模的自我评估。我们开发了一种新的教育问题生成模型EduQG，通过调整大型语言模型进行构建。我们广泛的实验表明，EduQG能够通过在科学文本和科学问题数据上进一步进行预训练和微调预训练语言模型，生成出更优秀的教育问题。

    The automatic generation of educational questions will play a key role in scaling online education, enabling self-assessment at scale when a global population is manoeuvring their personalised learning journeys. We develop \textit{EduQG}, a novel educational question generation model built by adapting a large language model. Our extensive experiments demonstrate that \textit{EduQG} can produce superior educational questions by further pre-training and fine-tuning a pre-trained language model on the scientific text and science question data.
    
[^52]: 一种用于罕见事件模拟的基于流的生成模型

    A Flow-Based Generative Model for Rare-Event Simulation. (arXiv:2305.07863v1 [stat.ML])

    [http://arxiv.org/abs/2305.07863](http://arxiv.org/abs/2305.07863)

    本文提出了一种使用基于流的生成模型直接模拟罕见事件分布的方法，该方法可以结合重要性采样获得高精度的复杂积分和期望估计，有效地提高采样效率并为罕见事件分布提供致命见解。

    

    在复杂的随机环境中解决决策问题通常通过蒙特卡罗采样估计决策的期望结果来实现。然而，采样可能会忽略罕见但重要的事件，这可能会严重影响决策过程。我们提出一种方法，其中训练了一个归一化流生成模型，以直接从条件分布中模拟样本，前提是发生罕见事件。通过利用耦合流，我们的模型可以在原则上任意好地逼近任何采样分布。通过将逼近方法与重要性采样相结合，可以获得高精度的复杂积分和期望估计。我们包括了几个示例来演示如何在高维和罕见事件设置中使用该方法进行有效采样和估计。我们证明了通过直接从罕见事件分布中模拟可以获得关于罕见事件发生方式的重要见解。

    Solving decision problems in complex, stochastic environments is often achieved by estimating the expected outcome of decisions via Monte Carlo sampling. However, sampling may overlook rare, but important events, which can severely impact the decision making process. We present a method in which a Normalizing Flow generative model is trained to simulate samples directly from a conditional distribution given that a rare event occurs. By utilizing Coupling Flows, our model can, in principle, approximate any sampling distribution arbitrarily well. By combining the approximation method with Importance Sampling, highly accurate estimates of complicated integrals and expectations can be obtained. We include several examples to demonstrate how the method can be used for efficient sampling and estimation, even in high-dimensional and rare-event settings. We illustrate that by simulating directly from a rare-event distribution significant insight can be gained into the way rare events happen.
    
[^53]: HAiVA：混合 AI 辅助可视化分析框架研究云属性对气候模式的影响

    HAiVA: Hybrid AI-assisted Visual Analysis Framework to Study the Effects of Cloud Properties on Climate Patterns. (arXiv:2305.07859v1 [cs.LG])

    [http://arxiv.org/abs/2305.07859](http://arxiv.org/abs/2305.07859)

    本研究提出了一种混合 AI 辅助可视化分析框架（HAiVA），可以用于探索云属性和气候模式之间的复杂交互作用，以及设计和测试海洋云增白（MCB）的干预方案，以评估它们对气候模式的预期和意外影响。

    

    云对地球气候系统有着重要的影响，可以通过海洋云增白（Marine Cloud Brightening，MCB）等气候干预技术来调节地球辐射平衡和驱动区域性温度和降水变化。然而，为了避免 MCB 的意外影响，我们需要更好地理解复杂的云到气候响应函数。使用传统的地球系统模型来设计和测试这样的干预方案需要消耗大量计算资源。因此，我们提出了一种混合 AI 辅助可视化分析框架（HAiVA）来驱动这些科学研究，并促进不同 MCB 干预方案的交互式 what-if 探索，以评估它们对气候模式的预期和意外影响。我们与气候科学家团队合作，开发了一套混合 AI 模型来模拟基于历史观测和地球系统模型模拟的云-气候响应函数。HAiVA 将统计和机器学习模型与交互式数据可视化相结合，以探索云属性和气候模式之间的复杂相互作用，以及设计和测试 MCB 情景。

    Clouds have a significant impact on the Earth's climate system. They play a vital role in modulating Earth's radiation budget and driving regional changes in temperature and precipitation. This makes clouds ideal for climate intervention techniques like Marine Cloud Brightening (MCB) which refers to modification in cloud reflectivity, thereby cooling the surrounding region. However, to avoid unintended effects of MCB, we need a better understanding of the complex cloud to climate response function. Designing and testing such interventions scenarios with conventional Earth System Models is computationally expensive. Therefore, we propose a hybrid AI-assisted visual analysis framework to drive such scientific studies and facilitate interactive what-if investigation of different MCB intervention scenarios to assess their intended and unintended impacts on climate patterns. We work with a team of climate scientists to develop a suite of hybrid AI models emulating cloud-climate response fun
    
[^54]: 基于匹配特征提取的异构边缘设备工业健康预测的联邦学习

    A Federated Learning-based Industrial Health Prognostics for Heterogeneous Edge Devices using Matched Feature Extraction. (arXiv:2305.07854v1 [cs.LG])

    [http://arxiv.org/abs/2305.07854](http://arxiv.org/abs/2305.07854)

    提出了一种基于联邦学习的健康预测模型，该模型具有特征相似性匹配算法来区分学习来自异构边缘设备的数据，以便开发出更准确的预测模型。

    

    数据驱动的工业健康预测需要丰富的训练数据才能开发准确可靠的预测模型。然而，严格的数据隐私法律和丰富的边缘工业数据需要分散式数据利用。因此，联邦学习（FL）是一个分散式和隐私保护的学习技术，非常适用于工业健康预测领域。然而，由于异构数据的数据异质性，以及由于不同的退化机制和不平等的数据集大小所导致的数据异构性，在联邦学习的基础上开发精度高的训练模型是一个关键的统计挑战。因此，FL在健康预测任务中的应用尚未充分研究。本文提出了一种具有特征相似性匹配的参数聚合算法的 FL 健康预测模型。

    Data-driven industrial health prognostics require rich training data to develop accurate and reliable predictive models. However, stringent data privacy laws and the abundance of edge industrial data necessitate decentralized data utilization. Thus, the industrial health prognostics field is well suited to significantly benefit from federated learning (FL), a decentralized and privacy-preserving learning technique. However, FL-based health prognostics tasks have hardly been investigated due to the complexities of meaningfully aggregating model parameters trained from heterogeneous data to form a high performing federated model. Specifically, data heterogeneity among edge devices, stemming from dissimilar degradation mechanisms and unequal dataset sizes, poses a critical statistical challenge for developing accurate federated models. We propose a pioneering FL-based health prognostic model with a feature similarity-matched parameter aggregation algorithm to discriminatingly learn from h
    
[^55]: Meta-Polyp：高效息肉分割的基准线。

    Meta-Polyp: a baseline for efficient Polyp segmentation. (arXiv:2305.07848v1 [eess.IV])

    [http://arxiv.org/abs/2305.07848](http://arxiv.org/abs/2305.07848)

    本研究提出Meta-Polyp，将Meta-Former与UNet融合并引入多尺度上采样块和Convformer块，解决了CNN和Vision Transformer在处理分布外数据集、缺失边界和小息肉时遇到的困难，提高了息肉分割的效率。

    

    近年来，息肉分割变得越来越重要，并且许多方法利用CNN、Vision Transformer和Transformer技术开发以实现竞争性结果。然而，这些方法在处理分布外数据集、缺失边界和小息肉时经常遇到困难。在2022年，Meta-Former作为一种新的视觉基准线被引入，它不仅提高了多任务计算机视觉的性能，而且解决了Vision Transformer和CNN家族骨架的局限性。为了进一步增强分割，我们提出了Meta-Former与UNet的融合，同时在解码器阶段引入了多尺度上采样块与级联组合，以增强纹理；此外，我们提出了Convformer块，基于Meta-Former的思想，以加强局部特征的关键信息。这些块将全局信息（例如息肉的整体形状）与局部信息相结合，提高了进行息肉分割的效率。

    In recent years, polyp segmentation has gained significant importance, and many methods have been developed using CNN, Vision Transformer, and Transformer techniques to achieve competitive results. However, these methods often face difficulties when dealing with out-of-distribution datasets, missing boundaries, and small polyps. In 2022, Meta-Former was introduced as a new baseline for vision, which not only improved the performance of multi-task computer vision but also addressed the limitations of the Vision Transformer and CNN family backbones. To further enhance segmentation, we propose a fusion of Meta-Former with UNet, along with the introduction of a Multi-scale Upsampling block with a level-up combination in the decoder stage to enhance the texture, also we propose the Convformer block base on the idea of the Meta-former to enhance the crucial information of the local feature. These blocks enable the combination of global information, such as the overall shape of the polyp, wit
    
[^56]: 理解异构数据联邦学习中的模型平均

    Understanding Model Averaging in Federated Learning on Heterogeneous Data. (arXiv:2305.07845v1 [cs.LG])

    [http://arxiv.org/abs/2305.07845](http://arxiv.org/abs/2305.07845)

    本文研究了异构数据联邦学习中的模型平均技术，通过可视化损失/错误景观揭示了客户端模型环绕全局模型在一个共同的盆地内，并且发现全局模型在早期训练后的误差主要来自客户端数据集和全局数据集之间非重叠的数据及全局模型与客户端模型之间的最大距离两个因素。

    

    模型平均是联邦学习中广泛采用的一种技术，它会聚集训练于异构数据上的多个客户端模型以获得表现良好的全局模型。然而，其成功背后的原理尚不是很清楚。本文通过可视化损失/错误景观来研究模型平均的几何特性，揭示了客户端模型环绕全局模型在一个共同的盆地内，并且即使全局模型表现优异，也可能偏离盆地底部。进一步的分析表明，全局模型在早期训练后的误差主要来自客户端数据集和全局数据集之间非重叠的数据及全局模型与客户端模型之间的最大距离两个因素。

    Model averaging, a widely adopted technique in federated learning (FL), aggregates multiple client models trained on heterogeneous data to obtain a well-performed global model. However, the rationale behind its success is not well understood. To shed light on this issue, we investigate the geometric properties of model averaging by visualizing the loss/error landscape. The geometrical visualization shows that the client models surround the global model within a common basin, and the global model may deviate from the bottom of the basin even though it performs better than the client models. To further understand this phenomenon, we decompose the expected prediction error of the global model into five factors related to client models. Specifically, we find that the global-model error after early training mainly comes from i) the client-model error on non-overlapping data between client datasets and the global dataset and ii) the maximal distance between the global and client models. Insp
    
[^57]: 带有无信息动作的参数化马尔可夫决策过程的汤普森抽样

    Thompson Sampling for Parameterized Markov Decision Processes with Uninformative Actions. (arXiv:2305.07844v1 [eess.SY])

    [http://arxiv.org/abs/2305.07844](http://arxiv.org/abs/2305.07844)

    研究了带有未知参数和无信息动作的参数化马尔可夫决策过程，提出了关于PMDP的假设，并使用汤普森抽样保证了其渐近最优的期望遗憾界。

    

    我们研究了带有未知关键参数的参数化马尔可夫决策过程（PMDP），必须使用贝叶斯推断进行学习。这些模型的一个关键定义特征是具有“无信息”动作，这些动作不提供关于未知参数的信息。我们提出了一组关于PMDP的假设，根据这些假设，汤普森抽样可保证渐近最优的期望遗憾界为$O(T^{-1})$，这些假设易于验证，适用于许多问题类别，例如排队，库存控制和动态定价。

    We study parameterized MDPs (PMDPs) in which the key parameters of interest are unknown and must be learned using Bayesian inference. One key defining feature of such models is the presence of "uninformative" actions that provide no information about the unknown parameters. We contribute a set of assumptions for PMDPs under which Thompson sampling guarantees an asymptotically optimal expected regret bound of $O(T^{-1})$, which are easily verified for many classes of problems such as queuing, inventory control, and dynamic pricing.
    
[^58]: 基于加权补丁质量预测的无参考点云质量评估

    No-Reference Point Cloud Quality Assessment via Weighted Patch Quality Prediction. (arXiv:2305.07829v1 [cs.CV])

    [http://arxiv.org/abs/2305.07829](http://arxiv.org/abs/2305.07829)

    COPP-Net是一种利用加权补丁质量预测进行局部相关性分析的无参考点云质量评估方法，优于现有的基准NR-PCQA方法。

    

    随着基于点云的三维视觉应用的快速发展，点云质量评估成为一个重要的研究课题。本文提出了一种具有局部区域相关性分析能力的无参考点云质量评估（NR-PCQA）方法，称为COPP-Net。具体而言，我们将点云分为补丁，为每个补丁生成纹理和结构特征，并将它们融合成补丁特征来预测补丁质量。然后，我们收集一个点云中所有补丁的特征进行相关性分析，以获得相关性权重。最后，预测的质量和所有补丁的相关权重用于推导最终质量得分。实验结果表明，我们的方法优于现有的基准NR-PCQA方法。

    With the rapid development of 3D vision applications based on point clouds, point cloud quality assessment(PCQA) is becoming an important research topic. However, the prior PCQA methods ignore the effect of local quality variance across different areas of the point cloud. To take an advantage of the quality distribution imbalance, we propose a no-reference point cloud quality assessment (NR-PCQA) method with local area correlation analysis capability, denoted as COPP-Net. More specifically, we split a point cloud into patches, generate texture and structure features for each patch, and fuse them into patch features to predict patch quality. Then, we gather the features of all the patches of a point cloud for correlation analysis, to obtain the correlation weights. Finally, the predicted qualities and correlation weights for all the patches are used to derive the final quality score. Experimental results show that our method outperforms the state-of-the-art benchmark NR-PCQA methods. Th
    
[^59]: DCASE 2023 挑战任务2：面向机器状态监测的首次无监督异常声音检测的描述与讨论(arXiv:2305.07828v1 [cs.SD])

    Description and Discussion on DCASE 2023 Challenge Task 2: First-Shot Unsupervised Anomalous Sound Detection for Machine Condition Monitoring. (arXiv:2305.07828v1 [cs.SD])

    [http://arxiv.org/abs/2305.07828](http://arxiv.org/abs/2305.07828)

    DCASE 2023 挑战任务2旨在解决机器状态监测中，部署新型机器的无监督异常声音检测，仅使用极少量的正常数据进行训练，且无需超参数调整。

    

    本文介绍了Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 挑战任务 2:“面向机器状态监测的首次无监督异常声音检测”。该任务的主要目标是仅使用少数正常样本就能快速部署针对新型机器的 ASD 系统，无需超参数调整。在过去的 ASD 任务中，发展的方法为每种机器类型调整超参数，因为发展和评估数据集具有相同的机器类型。然而，在实践中收集正常和异常数据集可能是不可行的。在 2023 Task 2 中，我们专注于解决首次问题，即在完全新型的机器类型的少数机器上训练模型的挑战。具体来说，（i）每种机器类型仅有一个部分，（ii）发展和评估数据集中的机器类型完全不同。我们将添加子任务的挑战结果和分析。

    We present the task description of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 Challenge Task 2: "First-shot unsupervised anomalous sound detection (ASD) for machine condition monitoring". The main goal is to enable rapid deployment of ASD systems for new kinds of machines using only a few normal samples, without the need for hyperparameter tuning. In the past ASD tasks, developed methods tuned hyperparameters for each machine type, as the development and evaluation datasets had the same machine types. However, collecting normal and anomalous data as the development dataset can be infeasible in practice. In 2023 Task 2, we focus on solving first-shot problem, which is the challenge of training a model on a few machines of a completely novel machine type. Specifically, (i) each machine type has only one section, and (ii) machine types in the development and evaluation datasets are completely different. We will add challenge results and analysis of the sub
    
[^60]: 基于主动学习的配电系统承载能力分析方法

    An Active Learning-based Approach for Hosting Capacity Analysis in Distribution Systems. (arXiv:2305.07818v1 [eess.SY])

    [http://arxiv.org/abs/2305.07818](http://arxiv.org/abs/2305.07818)

    本文提出基于主动学习的配电系统承载能力分析方法，通过选择最具信息价值的分布式能源集成场景来构建HC替代模型，大大降低了成本，并确保了结果的可靠性。

    

    随着分布式能源的不断集成，模拟和分析未来电力分配网络的承载能力(HC)变得越来越重要。本文提出了一种基于主动学习(AL)的分析方法，该方法通过迭代地选择最具信息价值的DER集成场景，并将其与以前模拟的场景相结合构建HC替代模型，然后使用该替代模型来识别实际模拟中最有信息价值的场景，可以显著降低HCA的计算成本，同时确保HC结果的可靠性，并可处理DER集成问题的不确定性和复杂性。

    With the increasing amount of distributed energy resources (DERs) integration, there is a significant need to model and analyze hosting capacity (HC) for future electric distribution grids. Hosting capacity analysis (HCA) examines the amount of DERs that can be safely integrated into the grid and is a challenging task in full generality because there are many possible integration of DERs in foresight. That is, there are numerous extreme points between feasible and infeasible sets. Moreover, HC depends on multiple factors such as (a) adoption patterns of DERs that depend on socio-economic behaviors and (b) how DERs are controlled and managed. These two factors are intrinsic to the problem space because not all integration of DERs may be centrally planned, and could largely change our understanding about HC. This paper addresses the research gap by capturing the two factors (a) and (b) in HCA and by identifying a few most insightful HC scenarios at the cost of domain knowledge. We propos
    
[^61]: ReLU MLPs 中 $\mu$P 学习速率的深度依赖性。

    Depth Dependence of $\mu$P Learning Rates in ReLU MLPs. (arXiv:2305.07810v1 [cs.LG])

    [http://arxiv.org/abs/2305.07810](http://arxiv.org/abs/2305.07810)

    本文研究了宽度为 $n$，深度为 $L$ 的随机全连接 ReLU 网络中 $\mu$P 学习率对 $n$ 和 $L$ 的依赖性，发现除第一层和最后一层以外，最大学习率与 $n$ 无关，但与 $L$ 按 $L^{-3/2}$ 缩放有关。

    

    在这篇简短的论文中，我们考虑了宽度为 $n$，深度为 $L$ 的随机全连接 ReLU 网络，并配备了平均场权重初始化。我们的目的是研究 $\mu$P 学习率对 $n$ 和 $L$ 的依赖性——在 $n,L$ 很大时 ，经过梯度下降一步后的预激活均方差变化仍保持均匀有界的最大学习率。与 Yang 等人关于 $\mu$P 的先前工作一样，我们发现除第一层和最后一层的权重外，这个最大更新学习率与 $n$ 无关。然而，我们发现它对 $L$ 有一个非平凡的依赖性，按 $L^{-3/2}$ 缩放。

    In this short note we consider random fully connected ReLU networks of width $n$ and depth $L$ equipped with a mean-field weight initialization. Our purpose is to study the dependence on $n$ and $L$ of the maximal update ($\mu$P) learning rate, the largest learning rate for which the mean squared change in pre-activations after one step of gradient descent remains uniformly bounded at large $n,L$. As in prior work on $\mu$P of Yang et. al., we find that this maximal update learning rate is independent of $n$ for all but the first and last layer weights. However, we find that it has a non-trivial dependence of $L$, scaling like $L^{-3/2}.$
    
[^62]: Mesh2SSM: 从表面网格到解剖学统计形态模型

    Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy. (arXiv:2305.07805v1 [cs.CV])

    [http://arxiv.org/abs/2305.07805](http://arxiv.org/abs/2305.07805)

    Mesh2SSM是一种基于无监督排列不变表示学习的方法，可以将模板点云变形为特定主体的网格，形成基于对应关系的解剖学统计形态模型。

    

    统计形态建模是从医学图像（如MRI和CT扫描）中捕获的分割解剖结构中发现显著形态参数的计算过程，可全面描述种群中特定主体的解剖学。由于人体解剖结构中存在大量的非线性变异，传统的形态建模过程常常面临挑战。深度学习技术可以学习到复杂的非线性形态表示，并生成更忠实于基础种群变异的统计形态模型。然而，现有的深度学习模型仍然存在局限性，需要已建立/优化的形态模型进行训练。我们提出了Mesh2SSM，一种新的方法，利用无监督、排列不变的表示学习来估计如何将模板点云变形为特定主体的网格，形成基于对应关系的形态模型。Mesh2SSM还可以学习种群特定的模板，从而减少了数据对齐的需要。

    Statistical shape modeling is the computational process of discovering significant shape parameters from segmented anatomies captured by medical images (such as MRI and CT scans), which can fully describe subject-specific anatomy in the context of a population. The presence of substantial non-linear variability in human anatomy often makes the traditional shape modeling process challenging. Deep learning techniques can learn complex non-linear representations of shapes and generate statistical shape models that are more faithful to the underlying population-level variability. However, existing deep learning models still have limitations and require established/optimized shape models for training. We propose Mesh2SSM, a new approach that leverages unsupervised, permutation-invariant representation learning to estimate how to deform a template point cloud to subject-specific meshes, forming a correspondence-based shape model. Mesh2SSM can also learn a population-specific template, reduci
    
[^63]: Dr. LLaMA：通过生成式数据增强改善特定领域QA中的小语言模型

    Dr. LLaMA: Improving Small Language Models in Domain-Specific QA via Generative Data Augmentation. (arXiv:2305.07804v1 [cs.CL])

    [http://arxiv.org/abs/2305.07804](http://arxiv.org/abs/2305.07804)

    本论文介绍了一种名为Dr. LLaMA的方法，通过使用大型语言模型进行生成式数据增强，以改善小语言模型的性能，特别是在医学问答任务中。这种方法在微调后使模型性能提高，并提出了在特定领域问答任务中使用LLM所面临的挑战和潜在的研究方向。

    

    大型语言模型在自然语言处理方面取得了重大进展，但随着其规模的增长，也面临着计算开销和效率的挑战，特别是在特定领域的任务中。另一方面，小型语言模型由于容量和训练数据的限制，在这些任务中往往表现不佳。本文介绍了一种名为Dr. LLaMA的方法，通过使用大型语言模型进行生成式数据增强，聚焦医学问答任务和PubMedQA数据集，以改善小语言模型的性能。我们的发现表明，LLM有效地细化和扩展现有的问题-答案对，在微调后，使得小型模型在特定领域QA数据集上性能提高。本研究强调了在特定领域问答任务中使用LLM面临的挑战，并提出了潜在的研究方向，最终旨在为专业应用创建更高效和能力更强的模型。

    Large Language Models (LLMs) have made significant strides in natural language processing but face challenges in terms of computational expense and inefficiency as they grow in size, especially in domain-specific tasks. Small Language Models (SLMs), on the other hand, often struggle in these tasks due to limited capacity and training data. In this paper, we introduce Dr. LLaMA, a method for improving SLMs through generative data augmentation using LLMs, focusing on medical question-answering tasks and the PubMedQA dataset. Our findings indicate that LLMs effectively refine and diversify existing question-answer pairs, resulting in improved performance of a much smaller model on domain-specific QA datasets after fine-tuning. This study highlights the challenges of using LLMs for domain-specific question answering and suggests potential research directions to address these limitations, ultimately aiming to create more efficient and capable models for specialized applications. We have als
    
[^64]: 使用Deepfake技术实现语音重音检测

    Using Deepfake Technologies for Word Emphasis Detection. (arXiv:2305.07791v1 [cs.LG])

    [http://arxiv.org/abs/2305.07791](http://arxiv.org/abs/2305.07791)

    本研究利用Deepfake技术产生没有重音的语音来解决自动语音重音检测任务，通过比较生成的语音和口述语音，能够分离出相对容易检测到的重音模式。

    

    本研究考虑了自动语音重音检测任务。这个任务很具挑战性，因为重音受到讲话者语音特点的影响，比如口音、方言或声音。为了解决这个问题，我们提出利用Deepfake技术为这个说话者产生一个没有重音的语音。这需要提取口述语音的文本，然后使用来自同一演讲者的语音样本来为这个任务产生没有重音的语音。通过比较生成的语音和口述语音，我们能够分离出相对容易检测到的重音模式。

    In this work, we consider the task of automated emphasis detection for spoken language. This problem is challenging in that emphasis is affected by the particularities of speech of the subject, for example the subject accent, dialect or voice. To address this task, we propose to utilize deep fake technology to produce an emphasis devoid speech for this speaker. This requires extracting the text of the spoken voice, and then using a voice sample from the same speaker to produce emphasis devoid speech for this task. By comparing the generated speech with the spoken voice, we are able to isolate patterns of emphasis which are relatively easy to detect.
    
[^65]: 重新审视匹配追踪：超越近似次模性

    Revisiting Matching Pursuit: Beyond Approximate Submodularity. (arXiv:2305.07782v1 [eess.SP])

    [http://arxiv.org/abs/2305.07782](http://arxiv.org/abs/2305.07782)

    本文提出了一种期望次模性的函数，并通过贪心算法在期望意义下保证了选取近似最优向量的有效性，同时缓解了常用匹配追踪（MP）算法中的缺陷。

    

    我们研究了从大量向量中选择子集，以便在一组函数上获得最佳信号表示的问题。虽然贪心方法已广泛用于解决这一问题，并且许多这些算法已在（微弱的）次模性的视角之下进行了分析，但这些算法中没有一个明确使用这种函数属性。我们在这里重新审视向量选择问题，并介绍一个被证明具有期望次模性的函数。这个函数不仅通过贪心算法在期望意义下保证了近似最优性，而且还缓解了常用匹配追踪（MP）算法中的现有缺陷。我们进一步展示了所提出的贪心算法的单点估计版本与MP变量之间的关系。我们的理论结果得到了角度到达估计问题的数值实验的支持，这是一个典型的信号表示任务。实验展示了所提出算法在精度和效率方面的优势。

    We study the problem of selecting a subset of vectors from a large set, to obtain the best signal representation over a family of functions. Although greedy methods have been widely used for tackling this problem and many of those have been analyzed under the lens of (weak) submodularity, none of these algorithms are explicitly devised using such a functional property. Here, we revisit the vector-selection problem and introduce a function which is shown to be submodular in expectation. This function does not only guarantee near-optimality through a greedy algorithm in expectation, but also alleviates the existing deficiencies in commonly used matching pursuit (MP) algorithms. We further show the relation between the single-point-estimate version of the proposed greedy algorithm and MP variants. Our theoretical results are supported by numerical experiments for the angle of arrival estimation problem, a typical signal representation task; the experiments demonstrate the benefits of the 
    
[^66]: 基于加速器的训练：用于转换器语音识别的方法

    Accelerator-Aware Training for Transducer-Based Speech Recognition. (arXiv:2305.07778v1 [cs.LG])

    [http://arxiv.org/abs/2305.07778](http://arxiv.org/abs/2305.07778)

    本文提出的加速器感知训练方法应用于RNN-T模型，有效地模拟NNA操作，使用户感知延迟（UPL）减少，同时在引擎延迟方面有5-7％的改进，节省高达10％的WER相对下降。

    

    在训练过程中，机器学习模型的权重和激活状态以全精度表示。然而，当在神经网络加速器芯片上进行部署时，这会导致运行时性能下降。本文中，我们在训练阶段复制了NNA运算符来考虑低精度推理带来的后向传播中的性能下降。我们的方法有效地模拟了NNA操作，避免了将数据传输到中央处理单元（CPU）中的量化错误，从而最终减少了用户感知延迟（UPL）。我们将这种方法应用于循环神经网络转录器（RNN-T）中，这是一种适用于设备上流式语音识别任务的优秀架构。我们在270K小时的英语数据上训练和评估模型，并显示出在引擎延迟方面有5-7％的改进，同时节省高达10％的WER相对下降。

    Machine learning model weights and activations are represented in full-precision during training. This leads to performance degradation in runtime when deployed on neural network accelerator (NNA) chips, which leverage highly parallelized fixed-point arithmetic to improve runtime memory and latency. In this work, we replicate the NNA operators during the training phase, accounting for the degradation due to low-precision inference on the NNA in back-propagation. Our proposed method efficiently emulates NNA operations, thus foregoing the need to transfer quantization error-prone data to the Central Processing Unit (CPU), ultimately reducing the user perceived latency (UPL). We apply our approach to Recurrent Neural Network-Transducer (RNN-T), an attractive architecture for on-device streaming speech recognition tasks. We train and evaluate models on 270K hours of English data and show a 5-7% improvement in engine latency while saving up to 10% relative degradation in WER.
    
[^67]: 在移动设备上监测和调整机器学习模型

    Monitoring and Adapting ML Models on Mobile Devices. (arXiv:2305.07772v1 [cs.LG])

    [http://arxiv.org/abs/2305.07772](http://arxiv.org/abs/2305.07772)

    这篇论文介绍了Nazr，这是一个能够在移动设备上连续监测和调整机器学习模型，以提高模型准确性的端到端系统。

    

    为了实现低延迟推理和离线操作，机器学习模型越来越多地被部署到移动设备上。然而，一旦部署了模型，运营者难以追踪其精确度，可能会因为数据漂移等问题而不可预测地降低。我们设计了Nazr，这是第一个端到端的系统，可以在移动设备上连续监测和调整模型，无需用户反馈。我们的关键观察是，模型退化通常是由特定的根本原因造成的，这可能会影响大量设备。因此，一旦Nazr检测到大量设备上的一致性退化，它就会采用根本原因分析来确定问题的起源，并应用特定于原因的适应。我们在两个计算机视觉数据集上评估了Nazr，并展示了与现有方法相比，它在提高准确性方面始终表现出色。在一个包含从驾驶汽车中收集的照片的数据集上，Nazr的平均准确性提高了15％。

    ML models are increasingly being pushed to mobile devices, for low-latency inference and offline operation. However, once the models are deployed, it is hard for ML operators to track their accuracy, which can degrade unpredictably (e.g., due to data drift). We design Nazar, the first end-to-end system for continuously monitoring and adapting models on mobile devices without requiring feedback from users. Our key observation is that often model degradation is due to a specific root cause, which may affect a large group of devices. Therefore, once Nazar detects a consistent degradation across a large number of devices, it employs a root cause analysis to determine the origin of the problem and applies a cause-specific adaptation. We evaluate Nazar on two computer vision datasets, and show it consistently boosts accuracy compared to existing approaches. On a dataset containing photos collected from driving cars, Nazar improves the accuracy on average by 15%.
    
[^68]: TinyStories: 语言模型能简小到什么程度却依然能够讲述连贯的英文故事？

    TinyStories: How Small Can Language Models Be and Still Speak Coherent English?. (arXiv:2305.07759v1 [cs.CL])

    [http://arxiv.org/abs/2305.07759](http://arxiv.org/abs/2305.07759)

    本文针对小型语言模型生成连贯的英文文本难题，引入了一个合成故事数据集 TinyStories，并探索小型模型规模、结构复杂度和训练数据规模对于语言模型表现的影响，证明了仅含 200 万参数的简单语言模型也能产生连贯的短故事。

    

    语言模型是自然语言处理中强大的工具，但在小型化时经常难以产生连贯和流畅的文本。本文引入了一个名为 TinyStories 的合成故事数据集，用于训练和评估规模小、复杂度低的语言模型对于短故事的生成能力。

    Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention).  In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet stil
    
[^69]: 用于熵估计的私有且通信高效的算法

    Private and Communication-Efficient Algorithms for Entropy Estimation. (arXiv:2305.07751v1 [cs.LG])

    [http://arxiv.org/abs/2305.07751](http://arxiv.org/abs/2305.07751)

    本文提出了用于熵估计的私有且通信高效的算法，对于多变量联合分布，其样本数与变量数量呈线性关系；算法可以在保证隐私的同时最小化通信成本。

    

    现代统计估计通常在分布式环境中进行，其中每个样本属于单个用户，该用户与中央服务器共享其数据。用户通常关心保护其样本的隐私，并尽量减少其必须向服务器传输的数据量。我们提供了改进的私有和通信高效算法，用于估计分布熵的几个流行度量。我们所有的算法通信成本固定且符合局部差分隐私。对于由树给出条件独立性的多变量联合分布，我们描述了估计Shannon熵的算法，其样本数与变量数量呈线性关系，而先前工作的样本复杂度为二次。我们还描述了一种用于估计Gini熵的算法，其样本复杂度没有依赖于分布支持大小，并且可以使用noteq

    Modern statistical estimation is often performed in a distributed setting where each sample belongs to a single user who shares their data with a central server. Users are typically concerned with preserving the privacy of their samples, and also with minimizing the amount of data they must transmit to the server. We give improved private and communication-efficient algorithms for estimating several popular measures of the entropy of a distribution. All of our algorithms have constant communication cost and satisfy local differential privacy. For a joint distribution over many variables whose conditional independence is given by a tree, we describe algorithms for estimating Shannon entropy that require a number of samples that is linear in the number of variables, compared to the quadratic sample complexity of prior work. We also describe an algorithm for estimating Gini entropy whose sample complexity has no dependence on the support size of the distribution and can be implemented usi
    
[^70]: 转移还是不转移：统一的可转移度量与分析

    To transfer or not transfer: Unified transferability metric and analysis. (arXiv:2305.07741v1 [cs.LG])

    [http://arxiv.org/abs/2305.07741](http://arxiv.org/abs/2305.07741)

    该论文提出了基于Wasserstein距离的联合估计（WDJE）的分析方法和度量标准，用于转移能力评估和决策，覆盖了领域和任务差异，以及分类和回归问题。从比较目标风险的角度促进决策，提出了一个易于计算的目标风险边界。

    

    在转移学习中，可转移性是最基本的问题之一，旨在评估任意转移任务的有效性。现有研究关注分类任务，忽视了领域或任务差异。更重要的是，缺乏确定是否转移的研究。为了解决这些问题，我们提出了一种新的分析方法和度量标准-基于Wasserstein距离的联合估计（WDJE），用于转移能力评估和决策，并在统一的分类和回归问题中考虑了领域和任务差异。WDJE通过比较有无转移的目标风险来促进决策制定。为了使比较成立，我们提出了一个非对称，易于理解和易于计算的目标风险边界，以近似目标转移风险，即使有限的目标标签也可以胜任。所提出的边界将目标风险与源模型性能，领域和共享性联系起来。

    In transfer learning, transferability is one of the most fundamental problems, which aims to evaluate the effectiveness of arbitrary transfer tasks. Existing research focuses on classification tasks and neglects domain or task differences. More importantly, there is a lack of research to determine whether to transfer or not. To address these, we propose a new analytical approach and metric, Wasserstein Distance based Joint Estimation (WDJE), for transferability estimation and determination in a unified setting: classification and regression problems with domain and task differences. The WDJE facilitates decision-making on whether to transfer or not by comparing the target risk with and without transfer. To enable the comparison, we approximate the target transfer risk by proposing a non-symmetric, easy-to-understand and easy-to-calculate target risk bound that is workable even with limited target labels. The proposed bound relates the target risk to source model performance, domain and
    
[^71]: 野外惊奇度的测量

    Measuring Surprise in the Wild. (arXiv:2305.07733v1 [cs.LG])

    [http://arxiv.org/abs/2305.07733](http://arxiv.org/abs/2305.07733)

    本文首次展示了如何将根植于认知科学和神经科学的惊奇计算模型与机器学习生成模型相结合，用于复杂动态环境中的惊奇行为检测，并可用于交通安全中的冲突识别和驾驶行为评估。

    

    关于惊奇是如何被体验的以及何时被体验的定量测量在大多数情况下仅限于实验室研究，并且扩展到自然环境下是极具挑战性的。本文首次展示了根植于认知科学和神经科学的惊奇计算模型与最先进的机器学习生成模型相结合，可以用来检测复杂动态环境中的惊奇人类行为，如道路交通。在交通安全方面，这样的模型可以支持交通冲突的识别，道路用户响应时间的建模以及人类和自动驾驶员行为评估。我们还提供了量化惊奇的新方法，并使用自然驾驶场景展示了相对于现有文献中的惊奇度的优势。使用学习生成模型来建模惊奇行为是一种新颖的概念，可推广到任何动态现实世界。

    The quantitative measurement of how and when we experience surprise has mostly remained limited to laboratory studies, and its extension to naturalistic settings has been challenging. Here we demonstrate, for the first time, how computational models of surprise rooted in cognitive science and neuroscience combined with state-of-the-art machine learned generative models can be used to detect surprising human behavior in complex, dynamic environments like road traffic. In traffic safety, such models can support the identification of traffic conflicts, modeling of road user response time, and driving behavior evaluation for both human and autonomous drivers. We also present novel approaches to quantify surprise and use naturalistic driving scenarios to demonstrate a number of advantages over existing surprise measures from the literature. Modeling surprising behavior using learned generative models is a novel concept that can be generalized beyond traffic safety to any dynamic real-world 
    
[^72]: 基于时空图神经网络的新西兰COVID-19疫情预测研究

    Predicting COVID-19 pandemic by spatio-temporal graph neural networks: A New Zealand's study. (arXiv:2305.07731v1 [cs.LG])

    [http://arxiv.org/abs/2305.07731](http://arxiv.org/abs/2305.07731)

    ATMGNN是一种基于时空图神经网络结构的新型深度学习架构，它能够结合空间和时间信息预测未来疫情趋势。通过学习聚类算法，该方法能够从多尺度的空间图中捕捉局部和全局信号，并建模长程的空间和时间依赖关系。实验结果表明ATMGNN在新西兰的COVID-19疫情预测中表现比其他方法更好，达到了最先进水平。

    

    疫情动态建模和模拟在理解和应对COVID-19等高传染性疾病传播方面发挥着重要作用。本文提出了一种名为“基于注意力的多分辨率图神经网络”(ATMGNN)的新颖深度学习架构，该架构学习将空间图信息(即地理数据)与时间信息(即COVID-19病例数量的时间序列数据)相结合，预测疫情未来动态。关键创新在于，我们的方法可以通过基于数据驱动的学习聚类算法捕获空间图的多尺度结构。这使得我们的架构可以学习捕捉疫情的局部或全局信号，并建模长程的空间和时间依赖关系。重要的是，我们收集和组装了新西兰的新数据集。我们建立了统计方法、时间架构、图神经网络和我们提出的方法的全面基准。在预测新西兰COVID-19疫情趋势方面，实验结果表明，我们提出的ATMGNN显著优于其他方法，在性能方面达到了最先进水平。

    Modeling and simulations of pandemic dynamics play an essential role in understanding and addressing the spreading of highly infectious diseases such as COVID-19. In this work, we propose a novel deep learning architecture named Attention-based Multiresolution Graph Neural Networks (ATMGNN) that learns to combine the spatial graph information, i.e. geographical data, with the temporal information, i.e. timeseries data of number of COVID-19 cases, to predict the future dynamics of the pandemic. The key innovation is that our method can capture the multiscale structures of the spatial graph via a learning to cluster algorithm in a data-driven manner. This allows our architecture to learn to pick up either local or global signals of a pandemic, and model both the long-range spatial and temporal dependencies. Importantly, we collected and assembled a new dataset for New Zealand. We established a comprehensive benchmark of statistical methods, temporal architectures, graph neural networks a
    
[^73]: 利用机器学习设计最优行为实验

    Designing Optimal Behavioral Experiments Using Machine Learning. (arXiv:2305.07721v1 [cs.LG])

    [http://arxiv.org/abs/2305.07721](http://arxiv.org/abs/2305.07721)

    本文介绍了利用BOED和机器学习的最新进展，为我们能够从中模拟数据的任何类型的模型找到最优实验，以获得更深入的了解人类行为和认知。

    

    计算模型是理解人类认知和行为的强大工具。它们能够清晰、精确地表达我们的理论，并提供细微而常常出人意料的预测。然而，这种丰富性和惊喜的能力意味着我们的科学直觉和传统工具不适合设计实验来测试和比较这些模型。为了避免这些陷阱并实现计算建模的全部潜力，我们需要设计实验的工具，以清晰地回答模型解释人类行为的方式以及这些模型必须做出的辅助假设。贝叶斯最优实验设计（BOED）通过确定预期产生信息性数据的实验来正式化搜索最优实验设计。在这项工作中，我们提供了一个教程，介绍了如何利用BOED和机器学习的最新进展，为我们能够从中模拟数据的任何类型的模型找到最优实验，并展示如何通过这样做，我们可以更深入地了解人类行为和认知。

    Computational models are powerful tools for understanding human cognition and behavior. They let us express our theories clearly and precisely, and offer predictions that can be subtle and often counter-intuitive. However, this same richness and ability to surprise means our scientific intuitions and traditional tools are ill-suited to designing experiments to test and compare these models. To avoid these pitfalls and realize the full potential of computational modeling, we require tools to design experiments that provide clear answers about what models explain human behavior and the auxiliary assumptions those models must make. Bayesian optimal experimental design (BOED) formalizes the search for optimal experimental designs by identifying experiments that are expected to yield informative data. In this work, we provide a tutorial on leveraging recent advances in BOED and machine learning to find optimal experiments for any kind of model that we can simulate data from, and show how by
    
[^74]: 棕矮星模型网格与机器学习大气反演的比较研究

    Intercomparison of Brown Dwarf Model Grids and Atmospheric Retrieval Using Machine Learning. (arXiv:2305.07719v1 [astro-ph.SR])

    [http://arxiv.org/abs/2305.07719](http://arxiv.org/abs/2305.07719)

    本文通过机器学习方法分析了14个棕矮星模型网格的预测能力，发现棕矮星的有效温度可以被预测，但推断表面重力加速度和金属丰度与模型网格有关。

    

    理解次恒星光谱数据和模型之间的差异一直是个挑战，尤其对于自洽模型网格的全面研究。本文采用随机森林监督机器学习方法，研究了14个以前发表的棕矮星模型网格（从1997到2021年）。随机森林方法让我们能够分析这些模型网格的预测能力，并在近似贝叶斯计算框架下解释数据。我们的数据集包括3个基准棕矮星（Gl 570D，ε Indi Ba和Bb）以及19个L型和T型矮星的样本；这个样本之前曾使用传统贝叶斯方法（嵌套取样）在Lueber等人（2022）中进行过分析。我们发现，可以独立于所选择的模型网格，强有力地预测棕矮星的有效温度。然而，推断棕矮星表面重力加速度和金属丰度因所选择的模型网格而异。

    Understanding differences between sub-stellar spectral data and models has proven to be a major challenge, especially for self-consistent model grids that are necessary for a thorough investigation of brown dwarf atmospheres. Using the supervised machine learning method of the random forest, we study the information content of 14 previously published model grids of brown dwarfs (from 1997 to 2021). The random forest method allows us to analyze the predictive power of these model grids, as well as interpret data within the framework of Approximate Bayesian Computation (ABC). Our curated dataset includes 3 benchmark brown dwarfs (Gl 570D, {\epsilon} Indi Ba and Bb) as well as a sample of 19 L and T dwarfs; this sample was previously analyzed in Lueber et al. (2022) using traditional Bayesian methods (nested sampling). We find that the effective temperature of a brown dwarf can be robustly predicted independent of the model grid chosen for the interpretation. However, inference of the sur
    
[^75]: 通过残差缩放实现ResNets的信号最优传递

    Optimal signal propagation in ResNets through residual scaling. (arXiv:2305.07715v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2305.07715](http://arxiv.org/abs/2305.07715)

    本文为ResNets导出系统的有限尺寸理论，指出对于深层网络架构，缩放参数是优化信号传播和确保有效利用网络深度方面的关键。

    

    Residual网络（ResNets）在大深度上比前馈神经网络具有更好的训练能力和性能。引入跳过连接可以促进信号向更深层的传递。此外，先前的研究发现为残差分支添加缩放参数可以进一步提高泛化性能。尽管他们经验性地确定了这种缩放参数特别有利的取值范围，但其相关的性能提升及其在网络超参数上的普适性仍需要进一步理解。对于前馈神经网络（FFNets），有限尺寸理论在信号传播和超参数调节方面获得了重要洞见。我们在这里为ResNets导出了一个系统的有限尺寸理论，以研究信号传播及其对残差分支缩放的依赖性。我们导出响应函数的分析表达式，这是衡量网络对输入敏感性的一种指标，并表明对于深层网络架构，缩放参数在优化信号传播和确保有效利用网络深度方面发挥着至关重要的作用。

    Residual networks (ResNets) have significantly better trainability and thus performance than feed-forward networks at large depth. Introducing skip connections facilitates signal propagation to deeper layers. In addition, previous works found that adding a scaling parameter for the residual branch further improves generalization performance. While they empirically identified a particularly beneficial range of values for this scaling parameter, the associated performance improvement and its universality across network hyperparameters yet need to be understood. For feed-forward networks (FFNets), finite-size theories have led to important insights with regard to signal propagation and hyperparameter tuning. We here derive a systematic finite-size theory for ResNets to study signal propagation and its dependence on the scaling for the residual branch. We derive analytical expressions for the response function, a measure for the network's sensitivity to inputs, and show that for deep netwo
    
[^76]: 使用语言模型检测危险的学生回复

    Using Language Models to Detect Alarming Student Responses. (arXiv:2305.07709v1 [cs.CL])

    [http://arxiv.org/abs/2305.07709](http://arxiv.org/abs/2305.07709)

    本文介绍了一种利用自然语言处理技术识别危险学生回复的系统，该系统采用经过微调的语言模型进行训练，能够显著提高准确性。

    

    本文详细介绍了一种利用人工智能识别危险学生回复的系统的进展。该系统集成在我们的评估平台中，用于评估学生的回复是否表明他们对自己或他人构成威胁。这些回复可能包括关于暴力威胁、严重抑郁、自杀风险和虐待描述的细节。最新模型是一个经过微调的语言模型，它是在由学生回复和补充文本构成的大型语料库上训练而成。我们证明，使用语言模型比此前版本的系统能够大幅提高准确性。

    This article details the advances made to a system that uses artificial intelligence to identify alarming student responses. This system is built into our assessment platform to assess whether a student's response indicates they are a threat to themselves or others. Such responses may include details concerning threats of violence, severe depression, suicide risks, and descriptions of abuse. Driven by advances in natural language processing, the latest model is a fine-tuned language model trained on a large corpus consisting of student responses and supplementary texts. We demonstrate that the use of a language model delivers a substantial improvement in accuracy over the previous iterations of this system.
    
[^77]: 用深度学习掌握类渗透游戏

    Mastering Percolation-like Games with Deep Learning. (arXiv:2305.07687v1 [cs.LG])

    [http://arxiv.org/abs/2305.07687](http://arxiv.org/abs/2305.07687)

    研究使用单人游戏和深度学习在网络攻击中的应用，利用训练的代理人和不同的鲁棒性定义，发现优化攻击或防御网络对特定目标非常敏感。

    

    尽管网络对随机攻击的鲁棒性得到了广泛研究，但是智能代理的故意破坏并不适用于先前的方法。在这里，我们设计了一个在晶格上的单人游戏，模拟攻击者试图摧毁网络的逻辑。游戏的目标是在最少的步骤中禁用所有节点。我们使用深度Q学习开发了一种强化学习方法，能够成功地学习玩这个游戏，并通过这种方式最优地攻击网络。由于学习算法是通用的，我们训练代理人在不同的鲁棒性定义上并比较学习策略。我们发现，表面上相似的鲁棒性定义引导训练代理使用不同的策略，暗示着优化攻击或防御网络对特定目标非常敏感。我们的方法为理解网络稳健性提供了一种新的方法，可应用于其他离散过程。

    Though robustness of networks to random attacks has been widely studied, intentional destruction by an intelligent agent is not tractable with previous methods. Here we devise a single-player game on a lattice that mimics the logic of an attacker attempting to destroy a network. The objective of the game is to disable all nodes in the fewest number of steps. We develop a reinforcement learning approach using deep Q-learning that is capable of learning to play this game successfully, and in so doing, to optimally attack a network. Because the learning algorithm is universal, we train agents on different definitions of robustness and compare the learned strategies. We find that superficially similar definitions of robustness induce different strategies in the trained agent, implying that optimally attacking or defending a network is sensitive the particular objective. Our method provides a new approach to understand network robustness, with potential applications to other discrete proces
    
[^78]: 一项纵向队列研究的合成数据生成-- 对已发布数据分析结果的验证、方法扩展和再现

    Synthetic data generation for a longitudinal cohort study -- Evaluation, method extension and reproduction of published data analysis results. (arXiv:2305.07685v1 [stat.ME])

    [http://arxiv.org/abs/2305.07685](http://arxiv.org/abs/2305.07685)

    本研究使用先进的合成数据生成方法生成营养领域中的数据，并对其进行深入的质量分析，展示了细致分析合成数据的必要性和保护机密性的重要性，从而确保合成数据的可重复使用性。

    

    访问个人健康数据对于获得新的见解和推动科学进步至关重要。特别是，基于人工智能的现代方法依赖于大型数据集的可用性和可访问性。在健康领域中，由于隐私问题，访问个人级别的数据常常具有挑战性。一种有希望的替代方案是生成完全综合数据，即通过随机过程生成具有与原始数据相似统计特性的数据，但它们与原始个人级别记录没有一对一的对应关系。在本研究中，我们使用了一种最先进的合成数据生成方法，并对在营养领域中特定用例的生成数据进行深入的质量分析。我们展示了对合成数据进行细致分析的必要性，这超出了描绘性统计，并提供了有价值的见解，介绍如何实现综合数据集的全部潜力。通过方法的扩展和保护机密性，合成数据可以成为可重复研究的有力工具。

    Access to individual-level health data is essential for gaining new insights and advancing science. In particular, modern methods based on artificial intelligence rely on the availability of and access to large datasets. In the health sector, access to individual-level data is often challenging due to privacy concerns. A promising alternative is the generation of fully synthetic data, i.e. data generated through a randomised process that have similar statistical properties as the original data, but do not have a one-to-one correspondence with the original individual-level records. In this study, we use a state-of-the-art synthetic data generation method and perform in-depth quality analyses of the generated data for a specific use case in the field of nutrition. We demonstrate the need for careful analyses of synthetic data that go beyond descriptive statistics and provide valuable insights into how to realise the full potential of synthetic datasets. By extending the methods, but also
    
[^79]: 基于机器学习的教学系统：一个概念框架

    ML-Based Teaching Systems: A Conceptual Framework. (arXiv:2305.07681v1 [cs.HC])

    [http://arxiv.org/abs/2305.07681](http://arxiv.org/abs/2305.07681)

    本文研究了机器学习模型在基于信息技术的教学系统中的应用，旨在解决组织如何更好地传播即将退休专家的知识并传授给新手的难题。

    

    随着人口结构变化所带来的技能短缺问题不断恶化，组织机构如何保留即将退休专家的知识并传授给新手已成为一个关键性挑战。传统上这种知识传播是通过个人互动来完成的，但这种方式缺少可伸缩性且需要大量的资源与时间。基于信息技术的教学系统解决了可伸缩性问题，但它们的开发仍然耗费时间且繁琐。在本文中，我们研究了机器学习模型在组织背景下促进知识传播的潜力，从而实现更具成本效益的基于信息技术的教学系统。通过系统的文献综述，我们考察了核心概念、主题和维度，以更好地理解和设计基于机器学习的教学系统。为此，我们总结了机器学习模型在基于信息技术的教学系统中的应用，归纳地分析了相关领域的概念。

    As the shortage of skilled workers continues to be a pressing issue, exacerbated by demographic change, it is becoming a critical challenge for organizations to preserve the knowledge of retiring experts and to pass it on to novices. While this knowledge transfer has traditionally taken place through personal interaction, it lacks scalability and requires significant resources and time. IT-based teaching systems have addressed this scalability issue, but their development is still tedious and time-consuming. In this work, we investigate the potential of machine learning (ML) models to facilitate knowledge transfer in an organizational context, leading to more cost-effective IT-based teaching systems. Through a systematic literature review, we examine key concepts, themes, and dimensions to better understand and design ML-based teaching systems. To do so, we capture and consolidate the capabilities of ML models in IT-based teaching systems, inductively analyze relevant concepts in this 
    
[^80]: 探索神经图像压缩中的速率-失真-复杂度优化

    Exploring the Rate-Distortion-Complexity Optimization in Neural Image Compression. (arXiv:2305.07678v1 [eess.IV])

    [http://arxiv.org/abs/2305.07678](http://arxiv.org/abs/2305.07678)

    本文系统地研究了神经图像压缩中的速率失真复杂度(RDC)优化，提出了一种新的RDC感知神经图像编解码器RDCAuto，其能够动态调整压缩配置以实现目标速率、失真和编码复杂度，且在性能方面优于现有的神经图像编解码器。

    

    尽管有着短暂的历史，神经图像编解码器在速率失真性能方面已被证明优于传统图像编解码器。然而，它们中的大多数都遭受着显著较长的解码时间的困扰，这阻碍了神经图像编解码器的实际应用。特别是在使用有效但耗时的自回归上下文模型时，这个问题就更加突出了，因为它会将熵解码时间增加几个数量级。本文系统地研究了神经图像压缩中的速率失真复杂度(RDC)优化，通过将解码复杂度量化为优化目标的因素，我们现在能够精确地控制RDC权衡，然后证明神经图像编解码器的速率失真性能如何适应不同的复杂度要求。除了研究RDC优化，我们还提出了一种新的RDC感知神经图像编解码器RDCAuto，它动态调整其压缩配置来实现目标速率、失真和编码复杂度。我们评估了RDCAuto并证明它优于现有的神经图像编解码器，在Diverse Image Dataset (DID)基准测试和Kodak Lossless True Color Image Suite (Kodak)上实现了最先进的性能，同时能够快速生成压缩图像。

    Despite a short history, neural image codecs have been shown to surpass classical image codecs in terms of rate-distortion performance. However, most of them suffer from significantly longer decoding times, which hinders the practical applications of neural image codecs. This issue is especially pronounced when employing an effective yet time-consuming autoregressive context model since it would increase entropy decoding time by orders of magnitude. In this paper, unlike most previous works that pursue optimal RD performance while temporally overlooking the coding complexity, we make a systematical investigation on the rate-distortion-complexity (RDC) optimization in neural image compression. By quantifying the decoding complexity as a factor in the optimization goal, we are now able to precisely control the RDC trade-off and then demonstrate how the rate-distortion performance of neural image codecs could adapt to various complexity demands. Going beyond the investigation of RDC optim
    
[^81]: Masked Audio Text Encoders 在多模态重打分中是有效的。

    Masked Audio Text Encoders are Effective Multi-Modal Rescorers. (arXiv:2305.07677v1 [cs.SD])

    [http://arxiv.org/abs/2305.07677](http://arxiv.org/abs/2305.07677)

    本文提出了Masked Audio Text Encoders（MATE），一个多模态掩码语言模型重新打分器，将声学表示形式并入到MLM的输入空间中。使用MATE对自动语音识别（ASR）系统进行多模态打分，即使在目标域数据不足的情况下，也可以提高系统的领域泛化能力，并且可以在非常有限的训练数据量下就将单词错误率（WER）降低。

    

    掩码语言模型（MLM）已被证明对于自动语音识别（ASR）系统的二次打分非常有效。在这项工作中，我们提出 Masked Audio Text Encoder（MATE），它是一个多模态掩码语言模型重新打分器，将声学表示形式并入到MLM的输入空间中。我们采用对比学习来通过学习共享表示来有效地对齐各种模态。我们发现，当目标域数据不可用时，使用多模态重新打分器对ASR系统的领域泛化很有好处。与仅文本的基线相比，在域内数据组上，MATE 可以将单词错误率（WER）降低4％-16％，在域外数据组上可将WER降低3％-7％。此外，仅使用非常有限的训练数据（0.8小时），MATE就可以将WER比一次打分的基线降低8％-23％。

    Masked Language Models (MLMs) have proven to be effective for second-pass rescoring in Automatic Speech Recognition (ASR) systems. In this work, we propose Masked Audio Text Encoder (MATE), a multi-modal masked language model rescorer which incorporates acoustic representations into the input space of MLM. We adopt contrastive learning for effectively aligning the modalities by learning shared representations. We show that using a multi-modal rescorer is beneficial for domain generalization of the ASR system when target domain data is unavailable. MATE reduces word error rate (WER) by 4%-16% on in-domain, and 3%-7% on out-of-domain datasets, over the text-only baseline. Additionally, with very limited amount of training data (0.8 hours), MATE achieves a WER reduction of 8%-23% over the first-pass baseline.
    
[^82]: LatentPINNs：通过潜在表示学习实现的物理学约束神经网络

    LatentPINNs: Generative physics-informed neural networks via a latent representation learning. (arXiv:2305.07671v1 [cs.LG])

    [http://arxiv.org/abs/2305.07671](http://arxiv.org/abs/2305.07671)

    LatentPINNs是一个利用潜在表示学习实现的物理学约束神经网络，通过潜在扩散模型的压缩表示，可以更快速、更有效地求解偏微分方程(PDE)。

    

    物理学约束神经网络(PINNs)通过对偏微分方程(PDE)求解进行物理学约束，提供更加精确和灵活的PDE解决方案。然而，PINNs的训练速度相对较慢且需要对不同的PDE参数进行额外的、可能昂贵的训练。为了解决这个问题，我们引入了latentPINN，这是一个利用PDE参数潜在表示作为PINN附加输入的框架，并允许在这些参数的分布上进行训练。我们使用潜在扩散模型学习PDE参数分布的压缩潜在表示，并训练一个受物理学约束的神经网络，使其能够精确求解PDE。

    Physics-informed neural networks (PINNs) are promising to replace conventional partial differential equation (PDE) solvers by offering more accurate and flexible PDE solutions. However, they are hampered by the relatively slow convergence and the need to perform additional, potentially expensive, training for different PDE parameters. To solve this limitation, we introduce latentPINN, a framework that utilizes latent representations of the PDE parameters as additional (to the coordinates) inputs into PINNs and allows for training over the distribution of these parameters. Motivated by the recent progress on generative models, we promote the use of latent diffusion models to learn compressed latent representations of the PDE parameters distribution and act as input parameters to NN functional solutions. We use a two-stage training scheme in which the first stage, we learn the latent representations for the distribution of PDE parameters. In the second stage, we train a physics-informed 
    
[^83]: 基于机器学习的肝脏感染预测分析：通过优化技术评估神经网络的分析性能

    Liver Infection Prediction Analysis using Machine Learning to Evaluate Analytical Performance in Neural Networks by Optimization Techniques. (arXiv:2305.07670v1 [cs.LG])

    [http://arxiv.org/abs/2305.07670](http://arxiv.org/abs/2305.07670)

    本研究利用机器学习算法分析不同肝脏疾病数据集，评估分析性能并通过优化技术找出最佳分类模型。

    

    肝脏感染是一种常见疾病，对人类健康构成了巨大威胁，但目前还能够识别一种可用于大规模筛查的最佳技术。本文研究了使用不同数据集和预测分析的机器学习算法。因此，机器学习可用于不同疾病，以整合可视化模式。本文使用不同的肝脏疾病数据集进行了各种机器学习算法的分析，以评估使用不同类型参数和优化技术的分析性能。选择的分类算法分析结果的差异，并找出最优的肝病分类模型。机器学习优化是通过修改超参数来采用优化方法之一来最小化成本函数的过程。设置超参数，包括磷酸酶，直接胆红素，蛋白质，白蛋白和球蛋白数量。

    Liver infection is a common disease, which poses a great threat to human health, but there is still able to identify an optimal technique that can be used on large-level screening. This paper deals with ML algorithms using different data sets and predictive analyses. Therefore, machine ML can be utilized in different diseases for integrating a piece of pattern for visualization. This paper deals with various machine learning algorithms on different liver illness datasets to evaluate the analytical performance using different types of parameters and optimization techniques. The selected classification algorithms analyze the difference in results and find out the most excellent categorization models for liver disease. Machine learning optimization is the procedure of modifying hyperparameters in arrange to employ one of the optimization approaches to minimise the cost function. To set the hyperparameter, include a number of Phosphotase,Direct Billirubin, Protiens, Albumin and Albumin Glo
    
[^84]: 卷积神经网络的定量语义比较

    Quantified Semantic Comparison of Convolutional Neural Networks. (arXiv:2305.07663v1 [cs.CV])

    [http://arxiv.org/abs/2305.07663](http://arxiv.org/abs/2305.07663)

    本研究提出了两种方法来量化卷积神经网络潜在空间中语义信息之间的相似性，从而揭示CNN层内语义信息的流动和相似性，以及不同网络之间的相似度程度。

    

    卷积神经网络（CNN）在计算机视觉领域的应用处于领先地位，具有出色的性能，然而它们的工作原理却很难阐明。但是，对于自动驾驶这类安全关键应用，模型选择还应考虑候选模型在模型透明性方面如何表示语义信息。为了解决这一尚未解决的问题，我们的工作提出了两种方法来量化CNN潜在空间中语义信息之间的相似性，旨在揭示CNN层内语义信息的流动和相似性，以及不同网络之间的相似度程度。我们使用了可解释人工智能（XAI）领域的著名技术作为基础，这些技术用于获得每个潜在空间中语义概念的全局向量表示，并基于它们在测试输入上的激活进行比较。本工作在三个不同的目标检测器和两个不同范围的图像数据集上进行了评估。

    The state-of-the-art in convolutional neural networks (CNNs) for computer vision excels in performance, while remaining opaque. But due to safety regulations for safety-critical applications, like perception for automated driving, the choice of model should also take into account how candidate models represent semantic information for model transparency reasons. To tackle this yet unsolved problem, our work proposes two methods for quantifying the similarity between semantic information in CNN latent spaces. These allow insights into both the flow and similarity of semantic information within CNN layers, and into the degree of their similitude between different networks. As a basis, we use renown techniques from the field of explainable artificial intelligence (XAI), which are used to obtain global vector representations of semantic concepts in each latent space. These are compared with respect to their activation on test inputs. When applied to three diverse object detectors and two d
    
[^85]: 基于自信息域的神经CSI压缩与特征耦合

    Self-information Domain-based Neural CSI Compression with Feature Coupling. (arXiv:2305.07662v1 [cs.IT])

    [http://arxiv.org/abs/2305.07662](http://arxiv.org/abs/2305.07662)

    本文提出了一种基于自信息域的神经CSI压缩与特征耦合方法，通过引入自信息作为CSI的表征，在新定义的自信息域中提取时间和空间特征进行有效压缩，并在压缩CSI反馈上实现了高达3.22dB的性能提升。

    

    基于深度学习的信道状态信息(CSI)反馈方法通常通过延迟和角度特征直接压缩CSI矩阵，但很少考虑CSI矩阵中信息的量度。我们从信息论角度引入自信息作为CSI的表征，它以明确的方式反映了原始CSI矩阵中的信息量。然后，在自信息域中提出了一种新的DL-based网络SD-CsiNet，用于临时CSI压缩。SD-CsiNet将原始CSI投影到自信息矩阵上，在新定义的自信息域中提取自信息矩阵的时间和空间特征，然后耦合这两种特征进行有效压缩。实验结果通过利用CSI的自信息验证了所提出方法的有效性。特别是对于压缩CSI反馈，该方法与现有技术相比可以达到高达3.22dB的性能提升。

    Deep learning (DL)-based channel state information (CSI) feedback methods compressed the CSI matrix by exploiting its delay and angle features straightforwardly, while the measure in terms of information contained in the CSI matrix has rarely been considered. Based on this observation, we introduce self-information as an informative CSI representation from the perspective of information theory, which reflects the amount of information of the original CSI matrix in an explicit way. Then, a novel DL-based network is proposed for temporal CSI compression in the self-information domain, namely SD-CsiNet. The proposed SD-CsiNet projects the raw CSI onto a self-information matrix in the newly-defined self-information domain, extracts both temporal and spatial features of the self-information matrix, and then couples these two features for effective compression. Experimental results verify the effectiveness of the proposed SD-CsiNet by exploiting the self-information of CSI. Particularly for 
    
[^86]: RHINO：通过匈牙利匹配实现动态降噪的旋转目标检测的旋转DETR

    RHINO: Rotated DETR with Dynamic Denoising via Hungarian Matching for Oriented Object Detection. (arXiv:2305.07598v1 [cs.CV])

    [http://arxiv.org/abs/2305.07598](http://arxiv.org/abs/2305.07598)

    本文提出了一种面向定向目标检测的DINO基线模型RHINO。并通过匈牙利匹配和查询对齐的方式实现动态降噪，解决了重复预测的问题，从而在公共基准测试中达到最先进的性能水平。

    

    随着DINO的发布，一种DETR的变体，检测变压器正在通过其端到端设计和可扩展性在目标检测基准中刷新记录。然而，虽然预计从其端到端架构中获得更多的好处，如消除NMS和与锚相关的成本，但尚未彻底研究DETR在定向目标检测方面的扩展。本文提出了首个面向定向目标检测的DINO基线。我们发现，直接使用DETR进行定向目标检测并不能保证不重复预测，并提出了一种简单的成本来减轻这种情况。此外，我们介绍了一种新的去噪策略，该策略使用匈牙利匹配来过滤冗余的带噪声的查询，并使用查询对齐来保持Transformer解码器层之间的匹配一致性。我们提出的模型在公共基准测试中优于以前的旋转DETR和其他对手，实现了最先进的性能。

    With the publication of DINO, a variant of the Detection Transformer (DETR), Detection Transformers are breaking the record in the object detection benchmark with the merits of their end-to-end design and scalability. However, the extension of DETR to oriented object detection has not been thoroughly studied although more benefits from its end-to-end architecture are expected such as removing NMS and anchor-related costs. In this paper, we propose a first strong DINO-based baseline for oriented object detection. We found that straightforward employment of DETRs for oriented object detection does not guarantee non-duplicate prediction, and propose a simple cost to mitigate this. Furthermore, we introduce a novel denoising strategy that uses Hungarian matching to filter redundant noised queries and query alignment to preserve matching consistency between Transformer decoder layers. Our proposed model outperforms previous rotated DETRs and other counterparts, achieving state-of-the-art pe
    
[^87]: 带有非对角信息的视觉-语言连续表示学习

    Continual Vision-Language Representaion Learning with Off-Diagonal Information. (arXiv:2305.07437v1 [cs.LG])

    [http://arxiv.org/abs/2305.07437](http://arxiv.org/abs/2305.07437)

    本文探讨了通过流数据持续训练CLIP模型的可行性，提出了一种有效的连续学习框架Mod-X，并证明内部旋转和跨模态偏差导致了CLIP在跨模态检索任务中性能下降。

    

    本文讨论了通过流数据持续训练CLIP模型的可行性。通过追踪连续更新的CLIP模型中表示向量的方向变化，我们探索和总结了这些空间变化，称为空间混乱（SD），可以分为内部旋转和跨模态偏差。此外，我们从经验和理论上证明了内部旋转和跨模态偏差如何导致CLIP在跨模态检索任务中性能下降。为了缓解空间混乱，我们提出了一种简单而有效的连续学习框架Mod-X: 维护非对角信息矩阵。在各种不同规模和范围的常用数据集上的实验表明了我们方法的有效性。

    This paper discusses the feasibility of continuously training the CLIP model through streaming data. Then, by tracking the directional changes of the representation vectors in the continuously updated CLIP model, we explore and summarize these spatial variations as Spatial Disorder (SD), which can be divided into Intra-modal Rotation and Inter-modal Deviation. Moreover, we demonstrate how intra-modal rotation and inter-modal deviation lead to a performance decline for CLIP on cross-modal retrieval tasks in both empirically and theoretically. To alleviate the spatial disorder, we propose a simple yet effective continual learning framework Mod-X: Maintain off-diagonal information-matriX. The experiments (in Section \ref{method}, \ref{experiments} and Appendix \ref{Appendix_to_experiments}) on commonly used datasets with different scales and scopes have illustrated the effectiveness of our method.
    
[^88]: 机器学习中交叉公平性调查：概念、缓解和挑战

    A Survey on Intersectional Fairness in Machine Learning: Notions, Mitigation, and Challenges. (arXiv:2305.06969v1 [cs.LG])

    [http://arxiv.org/abs/2305.06969](http://arxiv.org/abs/2305.06969)

    本篇论文调查了机器学习中交叉公平性的最新进展，提出了分类法和缓解方法，并探讨了未来研究的挑战与方向。

    

    机器学习系统的广泛应用，特别是在更为决策至关重要的应用中，如刑事判决和银行贷款，引发了对公平性的更多关注。已经开发了算法和指标来缓解和衡量这些歧视。最近，一些研究发现了一种更具挑战性的偏见形式，称为交叉偏见，涵盖了多个敏感属性，例如种族和性别。在本调查中，我们回顾了交叉公平性的最新进展，提出了一个交叉公平性和缓解的分类法。最后，我们确定了关键挑战，并为未来的研究指导提供了指导方针。

    The widespread adoption of Machine Learning systems, especially in more decision-critical applications such as criminal sentencing and bank loans, has led to increased concerns about fairness implications. Algorithms and metrics have been developed to mitigate and measure these discriminations. More recently, works have identified a more challenging form of bias called intersectional bias, which encompasses multiple sensitive attributes, such as race and gender, together. In this survey, we review the state-of-the-art in intersectional fairness. We present a taxonomy for intersectional notions of fairness and mitigation. Finally, we identify the key challenges and provide researchers with guidelines for future directions.
    
[^89]: 数据驱动政策细化的理论研究

    Towards Theoretical Understanding of Data-Driven Policy Refinement. (arXiv:2305.06796v1 [cs.LG])

    [http://arxiv.org/abs/2305.06796](http://arxiv.org/abs/2305.06796)

    本文介绍了一种数据驱动的强化学习政策细化方法，用于改进安全关键应用的策略，并提出了一系列定理验证其收敛性、鲁棒性界限、泛化误差和对模型错误的弹性。

    

    本文介绍了一种为安全关键应用量身定制的强化学习数据驱动政策细化方法。我们的方法利用数据驱动优化和强化学习的优势，通过迭代改进来增强策略的安全性和优化性。我们的主要贡献在于这个数据驱动政策细化概念的数学表述。该方法通过从数据驱动验证中浮现的反例学习，系统地改进强化学习策略。此外，我们提出了一系列定理，阐明了我们方法的关键理论性质，包括收敛性、鲁棒性界限、泛化误差和对模型错误的弹性。这些结果不仅验证了我们方法的有效性，而且有助于更深入地理解这个方法在不同环境和情况下的行为。

    This paper presents an approach for data-driven policy refinement in reinforcement learning, specifically designed for safety-critical applications. Our methodology leverages the strengths of data-driven optimization and reinforcement learning to enhance policy safety and optimality through iterative refinement. Our principal contribution lies in the mathematical formulation of this data-driven policy refinement concept. This framework systematically improves reinforcement learning policies by learning from counterexamples surfaced during data-driven verification. Furthermore, we present a series of theorems elucidating key theoretical properties of our approach, including convergence, robustness bounds, generalization error, and resilience to model mismatch. These results not only validate the effectiveness of our methodology but also contribute to a deeper understanding of its behavior in different environments and scenarios.
    
[^90]: 基于拍卖的联邦学习中数据消费者的效用最大化竞标策略

    Utility-Maximizing Bidding Strategy for Data Consumers in Auction-based Federated Learning. (arXiv:2305.06784v1 [cs.LG])

    [http://arxiv.org/abs/2305.06784](http://arxiv.org/abs/2305.06784)

    本文提出了一种首个效用最大化竞标策略（Fed-Bidder），使多个FL数据消费者可以通过AFL有效而高效地竞争数据拥有者。

    

    基于拍卖的联邦学习（AFL）因通过经济手段激励数据拥有者加入FL而受到广泛的研究兴趣。现有工作假设在AFL市场上仅存在一个数据消费者和多个数据拥有者（即垄断市场）。因此，数据拥有者竞标加入数据消费者进行FL。但是，在实际的AFL市场中，多个数据消费者可能会竞争以吸引数据拥有者加入他们各自的FL任务，这种假设是不现实的。本文提出了一种首个效用最大化竞标策略（Fed-Bidder），使多个FL数据消费者可以通过AFL有效而高效地竞争数据拥有者，并提供了能够容纳不同市场动态的各种获胜函数的效用估计能力。基于六个常用基准数据集的广泛实验表明了策略的有效性和可扩展性。

    Auction-based Federated Learning (AFL) has attracted extensive research interest due to its ability to motivate data owners to join FL through economic means. Existing works assume that only one data consumer and multiple data owners exist in an AFL marketplace (i.e., a monopoly market). Therefore, data owners bid to join the data consumer for FL. However, this assumption is not realistic in practical AFL marketplaces in which multiple data consumers can compete to attract data owners to join their respective FL tasks. In this paper, we bridge this gap by proposing a first-of-its-kind utility-maximizing bidding strategy for data consumers in federated learning (Fed-Bidder). It enables multiple FL data consumers to compete for data owners via AFL effectively and efficiently by providing with utility estimation capabilities which can accommodate diverse forms of winning functions, each reflecting different market dynamics. Extensive experiments based on six commonly adopted benchmark dat
    
[^91]: 实用的鲁棒性强化学习：相邻不确定性集和双代理算法

    On practical robust reinforcement learning: adjacent uncertainty set and double-agent algorithm. (arXiv:2305.06657v1 [cs.LG])

    [http://arxiv.org/abs/2305.06657](http://arxiv.org/abs/2305.06657)

    本文提出了一个名为ARQ-Learning的鲁棒性强化学习算法，采用了一个更加实际的不确定性集，并提出了一种称为“悲观代理”的方法。该算法在表格化的情况下获得了与现有算法相同的快速收敛速度，并为实际应用提供了更好的鲁棒性。而且，本文还首次提出了用于深度Q网络和深度确定策略梯度的鲁棒RL算法PR-DQN和PR-DDPG。

    

    鲁棒性强化学习（RL）旨在学习一个策略，该策略在一个不确定性集上优化最差性能。给定一个产生训练样本的标准马尔可夫决策过程（N-MDP），该集合包含通过对N-MDP进行某些扰动而获得的MDP。本文引入了一个新的不确定性集，其中包含比现有集合更实际的MDP。使用这个不确定性集，我们提出了一个鲁棒RL算法，名为ARQ-Learning，用于表格化的情况。此外，我们表征了有限时间的误差界并证明它与Q-Learning和鲁棒Q-Learning（即现有的鲁棒RL方法）一样快地收敛，同时为实际应用提供更好的鲁棒性。我们提出了一种称为“悲观代理”的方法，有效地解决了将ARQ-Learning扩展到大型或连续状态空间的关键瓶颈。利用这一技术，我们首先提出了PRQ-Learning。接着，将其与DQN和DDPG相结合，我们分别开发了PR-DQN和PR-DDPG，这是首个用于深度Q网络和深度确定策略梯度的鲁棒RL算法。我们在基准领域上的实验验证了我们所提出算法的有效性。

    Robust reinforcement learning (RL) aims at learning a policy that optimizes the worst-case performance over an uncertainty set. Given nominal Markov decision process (N-MDP) that generates samples for training, the set contains MDPs obtained by some perturbations from N-MDP. In this paper, we introduce a new uncertainty set containing more realistic MDPs in practice than the existing sets. Using this uncertainty set, we present a robust RL, named ARQ-Learning, for tabular cases. Also, we characterize the finite-time error bounds and prove that it converges as fast as Q-Learning and robust Q-Learning (i.e., the state-of-the-art robust RL method) while providing better robustness for real applications. We propose {\em pessimistic agent} that efficiently tackles the key bottleneck for the extension of ARQ-Learning into large or continuous state spaces. Using this technique, we first propose PRQ-Learning. To the next, combining this with DQN and DDPG, we develop PR-DQN and PR-DDPG, respect
    
[^92]: HAHE: 基于全局和局部水平的分层注意力模型用于超关系知识图谱

    HAHE: Hierarchical Attention for Hyper-Relational Knowledge Graphs in Global and Local Level. (arXiv:2305.06588v1 [cs.AI])

    [http://arxiv.org/abs/2305.06588](http://arxiv.org/abs/2305.06588)

    提出了HAHE模型，使用全局和局部水平的注意力学习了超关系知识图谱的图形结构和顺序结构，并在多个基准数据集上实现了最先进的表现。

    

    在超关系知识图谱上进行链接预测是值得尝试的。该论文提出了一种新颖的基于分层注意力的模型——HAHE，包括全局和局部水平的注意力机制来表示超关系知识图谱中的结构。通过采用超图双重注意力层，全局级别的注意力可以建模超关系知识图谱的图形结构；而采用异质性自注意层，局部级别的注意力则可以学习H-Facts内部的顺序结构。实验结果表明，HAHE在多个基准数据集上的链接预测方面取得了最先进的性能。

    Link Prediction on Hyper-relational Knowledge Graphs (HKG) is a worthwhile endeavor. HKG consists of hyper-relational facts (H-Facts), composed of a main triple and several auxiliary attribute-value qualifiers, which can effectively represent factually comprehensive information. The internal structure of HKG can be represented as a hypergraph-based representation globally and a semantic sequence-based representation locally. However, existing research seldom simultaneously models the graphical and sequential structure of HKGs, limiting HKGs' representation. To overcome this limitation, we propose a novel Hierarchical Attention model for HKG Embedding (HAHE), including global-level and local-level attention. The global-level attention can model the graphical structure of HKG using hypergraph dual-attention layers, while the local-level attention can learn the sequential structure inside H-Facts via heterogeneous self-attention layers. Experiment results indicate that HAHE achieves state
    
[^93]: 如何为推荐基础模型索引项目ID

    How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v1 [cs.IR])

    [http://arxiv.org/abs/2305.06569](http://arxiv.org/abs/2305.06569)

    本研究对推荐基础模型的项目索引问题进行了系统检查，提出了一种新的上下文感知索引方法，该方法在项目推荐准确性和文本生成质量方面具有优势。

    

    推荐基础模型将推荐任务转换为自然语言任务，利用大型语言模型（LLM）进行推荐。它通过直接生成建议的项目而不是计算传统推荐模型中每个候选项目的排名得分，简化了推荐管道，避免了多段过滤的问题。为了避免在决定要推荐哪些项目时生成过长的文本，为推荐基础模型创建LLM兼容的项目ID是必要的。本研究系统地研究了推荐基础模型的项目索引问题，以P5为代表的主干模型，并使用各种索引方法复制其结果。我们首先讨论了几种微不足道的项目索引方法（如独立索引、标题索引和随机索引）的问题，并表明它们不适用于推荐基础模型，然后提出了一种新的索引方法，称为上下文感知索引。我们表明，这种索引方法在项目推荐准确性和文本生成质量方面优于其他索引方法。

    Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text when deciding which item(s) to recommend, creating LLM-compatible item IDs is essential for recommendation foundation models. In this study, we systematically examine the item indexing problem for recommendation foundation models, using P5 as the representative backbone model and replicating its results with various indexing methods. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as independent indexing, title indexing, and random inde
    
[^94]: 探索机器遗忘的领域：一篇综述与分类

    Exploring the Landscape of Machine Unlearning: A Survey and Taxonomy. (arXiv:2305.06360v1 [cs.LG])

    [http://arxiv.org/abs/2305.06360](http://arxiv.org/abs/2305.06360)

    本文综述了机器遗忘的现状和技术应用，包括数据删除、扰动和模型更新，讨论了MU在隐私、安全和公正性等领域的潜在益处，以及它在自然语言处理、计算机视觉和推荐系统中的未来发展方向。

    

    机器遗忘是一个越来越受关注的领域，因为需要删除或修改机器学习模型所做出的预测。虽然训练模型变得更加有效和准确，但在某些领域（如隐私、安全和公正性），遗忘先前学到的信息的重要性变得越来越显著。本文介绍了机器遗忘的综述，涵盖了当前最先进的技术和方法，包括数据删除、扰动和模型更新。此外，文中还介绍了常用的度量标准和数据集。文章还强调了需要解决的挑战，包括攻击复杂性、标准化、可转移性、可解释性、训练数据和资源限制。本文的贡献包括讨论MU的潜在益处以及它在自然语言处理、计算机视觉和推荐系统中的未来方向。

    Machine unlearning (MU) is a field that is gaining increasing attention due to the need to remove or modify predictions made by machine learning (ML) models. While training models have become more efficient and accurate, the importance of unlearning previously learned information has become increasingly significant in fields such as privacy, security, and fairness. This paper presents a comprehensive survey of MU, covering current state-of-the-art techniques and approaches, including data deletion, perturbation, and model updates. In addition, commonly used metrics and datasets are also presented. The paper also highlights the challenges that need to be addressed, including attack sophistication, standardization, transferability, interpretability, training data, and resource constraints. The contributions of this paper include discussions about the potential benefits of MU and its future directions in Natural Language Processing, Computer vision, and Recommender Systems. Additionally, 
    
[^95]: 关于实例相关标签噪声学习价值的反思

    Rethinking the Value of Labels for Instance-Dependent Label Noise Learning. (arXiv:2305.06247v1 [cs.LG])

    [http://arxiv.org/abs/2305.06247](http://arxiv.org/abs/2305.06247)

    本文提出了一种利用深度生成模型和因果表征学习处理实例相关标签噪声问题的新算法，能很好地识别高层次的内容和风格潜在因素，并在合成和真实数据集上验证了其有效性。

    

    在大规模数据集中，标签噪声普遍存在，大大降低了深度学习算法的性能。由于实例相关噪声转移矩阵的不可识别性，大部分现有算法通过假设噪声标签生成过程与实例特征无关来解决该问题。然而，在真实世界的应用中，噪声标签通常取决于真实标签和特征。本文提出了一种新的深度生成模型来处理实例相关标签噪声问题，避免了明确建模噪声转移矩阵。我们的算法利用因果表征学习，同时从数据中识别高层次的内容和风格潜在因素。通过利用结构因果模型中的噪声标签监督信息，我们在广泛的合成和真实世界的实例相关标签噪声数据集上进行了实证评估，证明了所提出算法的显著性能。

    Label noise widely exists in large-scale datasets and significantly degenerates the performances of deep learning algorithms. Due to the non-identifiability of the instance-dependent noise transition matrix, most existing algorithms address the problem by assuming the noisy label generation process to be independent of the instance features. Unfortunately, noisy labels in real-world applications often depend on both the true label and the features. In this work, we tackle instance-dependent label noise with a novel deep generative model that avoids explicitly modeling the noise transition matrix. Our algorithm leverages casual representation learning and simultaneously identifies the high-level content and style latent factors from the data. By exploiting the supervision information of noisy labels with structural causal models, our empirical evaluations on a wide range of synthetic and real-world instance-dependent label noise datasets demonstrate that the proposed algorithm significa
    
[^96]: 补丁学习：实现跨不同生物医学数据源的综合分析的范式。

    Patchwork Learning: A Paradigm Towards Integrative Analysis across Diverse Biomedical Data Sources. (arXiv:2305.06217v1 [cs.LG])

    [http://arxiv.org/abs/2305.06217](http://arxiv.org/abs/2305.06217)

    补丁学习是一种新的范式，通过整合来自不同数据源的信息，解决了数据隐私、异构数据来源和无法充分利用多个数据模态的挑战。它可以同时利用互补的数据来源，同时保护数据隐私，从而实现开发更全面和具有普适性的ML模型。

    

    在医疗保健领域中，机器学习（ML）提供了许多增强患者护理、人口健康和医疗保健提供者工作流程的机会。然而，由于数据隐私、异构数据来源和无法充分利用多个数据模态的挑战，现实中的临床和成本效益仍然有限。在这篇观点论文中，我们介绍了“补丁学习”（PL），这是一种新的范式，通过集成来自不同数据来源（例如，临床免费文本、医学图像、组学）和分布在不同安全站点上的不同数据模态的不同数据集的信息来解决这些限制。PL允许同时利用互补的数据来源，同时保护数据隐私，从而实现开发更全面和具有普适性的ML模型。我们介绍了补丁学习的概念以及其在医疗保健领域的当前实现，探讨了解决各种问题的潜在机会和适用数据来源。

    Machine learning (ML) in healthcare presents numerous opportunities for enhancing patient care, population health, and healthcare providers' workflows. However, the real-world clinical and cost benefits remain limited due to challenges in data privacy, heterogeneous data sources, and the inability to fully leverage multiple data modalities. In this perspective paper, we introduce "patchwork learning" (PL), a novel paradigm that addresses these limitations by integrating information from disparate datasets composed of different data modalities (e.g., clinical free-text, medical images, omics) and distributed across separate and secure sites. PL allows the simultaneous utilization of complementary data sources while preserving data privacy, enabling the development of more holistic and generalizable ML models. We present the concept of patchwork learning and its current implementations in healthcare, exploring the potential opportunities and applicable data sources for addressing various
    
[^97]: 火灾传播不确定性估计的神经仿真器

    A Neural Emulator for Uncertainty Estimation of Fire Propagation. (arXiv:2305.06139v1 [cs.LG])

    [http://arxiv.org/abs/2305.06139](http://arxiv.org/abs/2305.06139)

    本论文提出了一种新的神经网络模型，直接估计给定输入参数不确定性的火灾传播概率，省去了繁重的模拟集合，能够更有效地对火灾传播的不确定性进行分析。

    

    森林火灾的传播是一个高度随机的过程，环境条件的微小变化（例如风速和方向）会导致观察到的行为发生巨大变化。传统方法通过模拟集合生成概率图来量化火线传播的不确定性。但使用集合通常计算成本高昂，这会限制不确定性分析的范围。为解决这个问题，我们探索了一种基于时空神经网络建模方法，直接估计给定输入参数不确定性的火灾传播概率。通过在模型训练过程中有意试图扰动输入的天气预报来表示不确定性。计算负载集中于模型训练过程中，这允许在部署期间探索更大的概率空间。经验评估表明，所提出的模型实现了与传统 S*t模拟所产生的相当火灾边界。

    Wildfire propagation is a highly stochastic process where small changes in environmental conditions (such as wind speed and direction) can lead to large changes in observed behaviour. A traditional approach to quantify uncertainty in fire-front progression is to generate probability maps via ensembles of simulations. However, use of ensembles is typically computationally expensive, which can limit the scope of uncertainty analysis. To address this, we explore the use of a spatio-temporal neural-based modelling approach to directly estimate the likelihood of fire propagation given uncertainty in input parameters. The uncertainty is represented by deliberately perturbing the input weather forecast during model training. The computational load is concentrated in the model training process, which allows larger probability spaces to be explored during deployment. Empirical evaluations indicate that the proposed model achieves comparable fire boundaries to those produced by the traditional S
    
[^98]: 所有的模型都是局部的: 用循环本地验证取代外部验证

    All models are local: time to replace external validation with recurrent local validation. (arXiv:2305.03219v1 [cs.LG])

    [http://arxiv.org/abs/2305.03219](http://arxiv.org/abs/2305.03219)

    本文认为外部验证无法确保机器学习模型的安全性或实用性，提出了循环本地验证的MLOps启发式范式作为新的黄金标准，强调对各个本地部署的模型进行监测和更新，从而更好地对齐临床和医疗特定需求与机器学习模型验证策略，提高临床决策支持工具的安全性和实用性。

    

    外部验证经常被推荐用于确保机器学习模型的泛化能力。然而，它既不能保证泛化能力，也不能等价于模型的临床实用性（任何临床决策支持工具的最终目标）。外部验证与当前医疗保健机器学习的需要不一致。其次，新的机器学习技术、当前的市场力量和更新的监管框架正在促进对个体部署的模型实例的频繁更新和监控。我们认为，外部验证不足以确保机器学习模型的安全性或实用性。修复外部验证范式的建议不够彻底。继续依赖它作为最终测试很可能会使我们走上错误道路。我们提出了 MLOps 启发式范式的循环本地验证作为新的黄金标准，强调监测和更新各个本地部署的模型。采用这种范式将更好地对齐临床和医疗特定需求与机器学习模型验证策略，提高临床决策支持工具的安全性和实用性。

    External validation is often recommended to ensure the generalizability of ML models. However, it neither guarantees generalizability nor equates to a model's clinical usefulness (the ultimate goal of any clinical decision-support tool). External validation is misaligned with current healthcare ML needs. First, patient data changes across time, geography, and facilities. These changes create significant volatility in the performance of a single fixed model (especially for deep learning models, which dominate clinical ML). Second, newer ML techniques, current market forces, and updated regulatory frameworks are enabling frequent updating and monitoring of individual deployed model instances. We submit that external validation is insufficient to establish ML models' safety or utility. Proposals to fix the external validation paradigm do not go far enough. Continued reliance on it as the ultimate test is likely to lead us astray. We propose the MLOps-inspired paradigm of recurring local v
    
[^99]: 通过因果世界模型实现可解释强化学习

    Explainable Reinforcement Learning via a Causal World Model. (arXiv:2305.02749v1 [cs.LG])

    [http://arxiv.org/abs/2305.02749](http://arxiv.org/abs/2305.02749)

    本文提出了一种新的可解释强化学习框架，通过学习因果世界模型来解释行动的长期影响以及教学习者如何影响环境变量并最终导致奖励。

    

    给强化学习提供解释是一项挑战，因为行动可能对未来产生长期影响。本文提出了一种新的可解释强化学习框架：通过学习一个因果世界模型而不预先知道环境的因果结构。该模型捕捉到动作的影响，使我们能够通过因果链来解释行动的长期影响，从而揭示出行动是如何影响环境变量并最终导致奖励的。与大多数解释性模型的低准确性不同，我们的模型保持高准确性的同时提高了解释性，使其适用于基于模型的学习。因此，我们证明了我们的因果模型可以成为解释性和学习之间的桥梁。

    Generating explanations for reinforcement learning (RL) is challenging as actions may produce long-term effects on the future. In this paper, we develop a novel framework for explainable RL by learning a causal world model without prior knowledge of the causal structure of the environment. The model captures the influence of actions, allowing us to interpret the long-term effects of actions through causal chains, which present how actions influence environmental variables and finally lead to rewards. Different from most explanatory models which suffer from low accuracy, our model remains accurate while improving explainability, making it applicable in model-based learning. As a result, we demonstrate that our causal model can serve as the bridge between explainability and learning.
    
[^100]: 使用RePU激活函数的可微分神经网络：在得分估计和保序回归中的应用。

    Differentiable Neural Networks with RePU Activation: with Applications to Score Estimation and Isotonic Regression. (arXiv:2305.00608v1 [stat.ML])

    [http://arxiv.org/abs/2305.00608](http://arxiv.org/abs/2305.00608)

    该论文介绍了使用RePU激活函数的可微分神经网络，在近似$C^s$平滑函数及其导数的同时建立了下限误差界，并证明了其在降低维度灾难方面的能力，此外还提出了一种使用RePU网络的惩罚保序回归(PDIR)方法。

    

    我们研究了由修正后的幂单元（RePU）函数激活的可微分神经网络的属性。我们展示了RePU神经网络的偏导数可以由混合激活RePU网络来表示，并推导了导数RePU网络函数类的复杂度的上界。在使用RePU激活的深度神经网络中，我们建立了同时近似$C^s$平滑函数及其导数的误差界。此外，当数据具有近似低维支持时，我们推导出改进的逼近误差界，证明了RePU网络减缓维度灾难的能力。为了说明我们的结果的实用性，我们考虑了深度得分匹配估计器(DSME)，并提出了一种使用RePU网络的惩罚保序回归(PDIR)。我们在假定目标函数属于$C^s$平滑函数类的情况下为DSME和PDIR建立非渐近超额风险界。

    We study the properties of differentiable neural networks activated by rectified power unit (RePU) functions. We show that the partial derivatives of RePU neural networks can be represented by RePUs mixed-activated networks and derive upper bounds for the complexity of the function class of derivatives of RePUs networks. We establish error bounds for simultaneously approximating $C^s$ smooth functions and their derivatives using RePU-activated deep neural networks. Furthermore, we derive improved approximation error bounds when data has an approximate low-dimensional support, demonstrating the ability of RePU networks to mitigate the curse of dimensionality. To illustrate the usefulness of our results, we consider a deep score matching estimator (DSME) and propose a penalized deep isotonic regression (PDIR) using RePU networks. We establish non-asymptotic excess risk bounds for DSME and PDIR under the assumption that the target functions belong to a class of $C^s$ smooth functions. We 
    
[^101]: 基于深度神经网络先验的矩法轨道恢复

    Deep Neural-network Prior for Orbit Recovery from Method of Moments. (arXiv:2304.14604v1 [stat.ME] CROSS LISTED)

    [http://arxiv.org/abs/2304.14604](http://arxiv.org/abs/2304.14604)

    该论文提出了一种基于深度神经网络先验的矩法轨道恢复方法，可用于解决多参照面对齐和单颗粒冷冻电镜建模等问题，具有抑制噪声的优势.

    

    轨道恢复问题是一类经常出现的问题，有多种形式。在这些问题中，我们旨在在经过群作用扭曲并通过已知算子观察后，估计未知函数。通常情况下，观测值会受到非平凡水平的噪声污染。本文研究了两种特定的轨道恢复问题，即多参照面对齐和单颗粒冷冻电镜建模。为了抑制噪声，我们建议在两个问题中都使用矩法方法，并引入深度神经网络先验。特别地，我们的神经网络应输出信号和群元素的分布，而矩则为输入。在多参照面对齐的情况下，我们展示了使用神经网络提高从矩中重建信号的收敛速度的优势。最后，我们使用我们的方法重建了冷冻电镜模拟和生物体积。

    Orbit recovery problems are a class of problems that often arise in practice and in various forms. In these problems, we aim to estimate an unknown function after being distorted by a group action and observed via a known operator. Typically, the observations are contaminated with a non-trivial level of noise. Two particular orbit recovery problems of interest in this paper are multireference alignment and single-particle cryo-EM modeling. In order to suppress the noise, we suggest using the method of moments approach for both problems while introducing deep neural network priors. In particular, our neural networks should output the signals and the distribution of group elements, with moments being the input. In the multireference alignment case, we demonstrate the advantage of using the NN to accelerate the convergence for the reconstruction of signals from the moments. Finally, we use our method to reconstruct simulated and biological volumes in the cryo-EM setting.
    
[^102]: 通过识别桥接度重要节点生成Skip-gram节点嵌入的后续解释

    Generating Post-hoc Explanations for Skip-gram-based Node Embeddings by Identifying Important Nodes with Bridgeness. (arXiv:2304.12036v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.12036](http://arxiv.org/abs/2304.12036)

    本文提出了一种解释Skip-gram节点嵌入的方法，即通过计算桥接度识别重要节点，并提出了一种新型基于梯度的解释方法GRAPH-wGD，有效地提供全局性解释。

    

    网络中的节点表示学习是编码连续矢量空间中的关系信息同时保留网络固有属性和结构的重要机器学习技术。最近，DeepWalk、LINE、struc2vec、PTE、UserItem2vec和RWJBG等无监督节点嵌入方法从Skip-gram模型中出现，并在诸如节点分类和链接预测等下游任务中表现出更好的性能。然而，由于缺乏适用于嵌入的解释方法和理论研究，提供Skip-gram嵌入的后续解释仍然是一个具有挑战性的问题。本文首先表明可以通过在谱聚类感知局部扰动下计算桥接度来找到Skip-gram嵌入的全局解释。此外，还提出了一种名为GRAPH-wGD的新型基于梯度的解释方法，允许检索top-q全局性解释，并通过实验证明了其有效性。

    Node representation learning in a network is an important machine learning technique for encoding relational information in a continuous vector space while preserving the inherent properties and structures of the network. Recently, unsupervised node embedding methods such as DeepWalk, LINE, struc2vec, PTE, UserItem2vec, and RWJBG have emerged from the Skip-gram model and perform better performance in several downstream tasks such as node classification and link prediction than the existing relational models. However, providing post-hoc explanations of Skip-gram-based embeddings remains a challenging problem because of the lack of explanation methods and theoretical studies applicable for embeddings. In this paper, we first show that global explanations to the Skip-gram-based embeddings can be found by computing bridgeness under a spectral cluster-aware local perturbation. Moreover, a novel gradient-based explanation method, which we call GRAPH-wGD, is proposed that allows the top-q glo
    
[^103]: Tetra-NeRF：使用四面体表示的神经辐射场

    Tetra-NeRF: Representing Neural Radiance Fields Using Tetrahedra. (arXiv:2304.09987v1 [cs.CV])

    [http://arxiv.org/abs/2304.09987](http://arxiv.org/abs/2304.09987)

    本文提出了一种基于四面体和 Delaunay 表示的自适应表示方法，用于神经辐射场，可实现高效的训练和最先进的结果，同时提供了更多接近表面的细节，并且实现了比基于点的表示更好的性能。

    

    神经辐射场 (NeRF) 是一种非常流行的方法，用于新视角合成和三维重构问题。NeRF 常用的场景表示是将场景的一致的基于体素的细分与 MLP 结合起来。本文根据观察到的场景的（稀疏）点云提出了一种基于 Delaunay 表示的自适应表示，而非一致的细分或基于点的表示。我们证明了这种表示可以实现高效的训练，获得最先进的结果。我们的方法巧妙地结合了三维几何处理、三角形渲染和现代神经辐射场的概念。与基于体素的表示相比，我们的方法提供了更多接近表面的场景细节。与基于点的表示相比，我们的方法实现了更好的性能。

    Neural Radiance Fields (NeRFs) are a very recent and very popular approach for the problems of novel view synthesis and 3D reconstruction. A popular scene representation used by NeRFs is to combine a uniform, voxel-based subdivision of the scene with an MLP. Based on the observation that a (sparse) point cloud of the scene is often available, this paper proposes to use an adaptive representation based on tetrahedra and a Delaunay representation instead of the uniform subdivision or point-based representations. We show that such a representation enables efficient training and leads to state-of-the-art results. Our approach elegantly combines concepts from 3D geometry processing, triangle-based rendering, and modern neural radiance fields. Compared to voxel-based representations, ours provides more detail around parts of the scene likely to be close to the surface. Compared to point-based representations, our approach achieves better performance.
    
[^104]: 通过保留谱的数据压缩加速支持向量聚类

    Accelerate Support Vector Clustering via Spectrum-Preserving Data Compression?. (arXiv:2304.09868v1 [cs.LG])

    [http://arxiv.org/abs/2304.09868](http://arxiv.org/abs/2304.09868)

    本文介绍了一种通过保留谱的数据压缩来加速支持向量聚类的方法，可以显著提高聚类速度而不牺牲聚类质量。

    

    支持向量聚类是一种重要的聚类方法，但是由于其计算昂贵的簇分配步骤，它面临着可伸缩性问题。在本文中，我们通过保留谱的数据压缩来加速支持向量聚类。具体而言，我们将原始数据集压缩成少量谱表示的聚合数据点，然后在压缩后的数据集上执行标准的支持向量聚类，最后将压缩数据集的聚类结果映射回原始数据集以发现簇。我们在真实数据集上的大量实验结果表明，相较于标准支持向量聚类，我们的方法大大提高了速度，而不会损失聚类质量。

    Support vector clustering is an important clustering method. However, it suffers from a scalability issue due to its computational expensive cluster assignment step. In this paper we accelertate the support vector clustering via spectrum-preserving data compression. Specifically, we first compress the original data set into a small amount of spectrally representative aggregated data points. Then, we perform standard support vector clustering on the compressed data set. Finally, we map the clustering results of the compressed data set back to discover the clusters in the original data set. Our extensive experimental results on real-world data set demonstrate dramatically speedups over standard support vector clustering without sacrificing clustering quality.
    
[^105]: 行为检索：通过查询未标记数据集实现少样本模仿学习

    Behavior Retrieval: Few-Shot Imitation Learning by Querying Unlabeled Datasets. (arXiv:2304.08742v1 [cs.RO])

    [http://arxiv.org/abs/2304.08742](http://arxiv.org/abs/2304.08742)

    本论文提出了一种简单的方法，利用少量下游专家数据从离线未标记数据集中选择性地查询相关行为（包括许多次优行为），实现了行为检索，进而实现了少样本学习。

    

    在数据效率方面使机器人学习新的视觉动作技能仍然是一个难题，有许多挑战。解决这个问题的一种流行范式是利用大型未标记的数据集，其中包含许多行为，然后使用少量任务特定的人类监督（即介入或演示）来适应特定任务的策略。但是，如何最好地利用狭窄的任务特定监督并将其与离线数据平衡仍然是一个待解决的问题。我们的关键洞察力在于任务特定数据不仅为代理提供了新的训练数据，还可以为代理的学习提供有关先前数据类型的信息。具体来说，我们提出了一种简单的方法，利用少量下游专家数据从离线未标记数据集中选择性地查询相关行为（包括许多次优行为）。然后代理被联合训练在专家和查询数据上。我们观察到，我们的

    Enabling robots to learn novel visuomotor skills in a data-efficient manner remains an unsolved problem with myriad challenges. A popular paradigm for tackling this problem is through leveraging large unlabeled datasets that have many behaviors in them and then adapting a policy to a specific task using a small amount of task-specific human supervision (i.e. interventions or demonstrations). However, how best to leverage the narrow task-specific supervision and balance it with offline data remains an open question. Our key insight in this work is that task-specific data not only provides new data for an agent to train on but can also inform the type of prior data the agent should use for learning. Concretely, we propose a simple approach that uses a small amount of downstream expert data to selectively query relevant behaviors from an offline, unlabeled dataset (including many sub-optimal behaviors). The agent is then jointly trained on the expert and queried data. We observe that our 
    
[^106]: 学习经验Bregman散度用于不确定距离表示

    Learning Empirical Bregman Divergence for Uncertain Distance Representation. (arXiv:2304.07689v1 [cs.CV])

    [http://arxiv.org/abs/2304.07689](http://arxiv.org/abs/2304.07689)

    本文介绍了一种新的基于Deep Metric Learning的方法，通过学习经验Bregman散度直接从数据中进行不确定距离表示，能够有效的在模式识别和聚类任务上提高准确性。

    

    深度度量学习技术已应用于各种监督和无监督学习任务，通过深度网络学习样本嵌入来进行视觉表示。然而，经典方法采用固定距离度量作为两个嵌入之间的相似性函数，可能导致捕捉复杂数据分布的亚最优性能。Bregman散度概括了各种距离度量的度量，并在许多深度度量学习领域中产生。本文首先展示了如何从Bregman散度获得深度度量学习损失。然后，我们介绍了一种直接从数据中学习经验Bregman散度的新方法，通过使用深度学习设置对Bregman散度下的凸函数进行参数化。我们进一步实验证明，与其他SOTA深度度量学习方法相比，我们的方法在五个流行公共数据集上表现出色，特别是在模式识别和聚类任务上。

    Deep metric learning techniques have been used for visual representation in various supervised and unsupervised learning tasks through learning embeddings of samples with deep networks. However, classic approaches, which employ a fixed distance metric as a similarity function between two embeddings, may lead to suboptimal performance for capturing the complex data distribution. The Bregman divergence generalizes measures of various distance metrics and arises throughout many fields of deep metric learning. In this paper, we first show how deep metric learning loss can arise from the Bregman divergence. We then introduce a novel method for learning empirical Bregman divergence directly from data based on parameterizing the convex function underlying the Bregman divergence with a deep learning setting. We further experimentally show that our approach performs effectively on five popular public datasets compared to other SOTA deep metric learning methods, particularly for pattern recognit
    
[^107]: 神经群体动态和几何的可解释统计表示

    Interpretable statistical representations of neural population dynamics and geometry. (arXiv:2304.03376v1 [cs.LG])

    [http://arxiv.org/abs/2304.03376](http://arxiv.org/abs/2304.03376)

    该论文提出了一种基于统计分布的几何深度学习框架，用于表示非线性动态系统的几何感知或几何无感知表示，以对已测量轨迹进行无偏比较。利用该方法，能够解释神经动力学的嵌入，在灵长类似任务中取得了最先进的准确性。

    

    在各种任务中，神经元群体的动态通常在低维流形上演化。然而，区分几何和动态对编码相关行为变量的贡献仍然具有挑战性。在这里，我们引入了一种基于局部相轨特征的统计分布的非线性动态系统的几何深度学习框架，用于表示。我们的方法提供了对几何感知或几何无感知表示，以对已测量轨迹进行无偏比较。我们证明，我们的统计表示可以横跨神经网络实例进行推广，以区分计算机制，在具有几何对应的灵长类似任务中解释嵌入神经动力学，并开发具有最先进准确性的解码算法。我们的结果强调了使用内在流形结构优于时间信息的重要性。

    The dynamics of neuron populations during diverse tasks often evolve on low-dimensional manifolds. However, it remains challenging to discern the contributions of geometry and dynamics for encoding relevant behavioural variables. Here, we introduce an unsupervised geometric deep learning framework for representing non-linear dynamical systems based on statistical distributions of local phase portrait features. Our method provides robust geometry-aware or geometry-agnostic representations for the unbiased comparison of dynamics based on measured trajectories. We demonstrate that our statistical representation can generalise across neural network instances to discriminate computational mechanisms, obtain interpretable embeddings of neural dynamics in a primate reaching task with geometric correspondence to hand kinematics, and develop a decoding algorithm with state-of-the-art accuracy. Our results highlight the importance of using the intrinsic manifold structure over temporal informati
    
[^108]: 长期的多模式变压器整合EHR中成像和潜在临床特征，用于肺部结节分类。

    Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures From Routine EHRs for Pulmonary Nodule Classification. (arXiv:2304.02836v1 [eess.IV])

    [http://arxiv.org/abs/2304.02836](http://arxiv.org/abs/2304.02836)

    本文提出了一种新的肺部结节分类方法，使用变压器模型整合了EHR中的成像和临床特征。

    

    将重复成像和医疗背景（如电子健康记录）纳入预测性孤立性肺部结节（SPN）诊断模型可以极大增加准确性。然而，像成像和诊断代码这样的临床常规模式可能是异步的，并且在不同时间尺度上进行不规则采样，这是长期多模态学习的障碍。我们提出了一种基于变压器的多模态策略，将重复成像与日常收集的EHR中的长期临床特征相整合，以进行SPN分类。我们对潜在临床特征进行无监督的解缠缚，并利用时间距离缩放自注意力来联合学习临床特征表达和胸部计算机断层扫描（CT）。我们的分类器是在一个公共数据集的2,668个扫描和1,149名志愿者的长期胸部CT、账单代码、药物和实验室检查记录中进行预训练的。

    The accuracy of predictive models for solitary pulmonary nodule (SPN) diagnosis can be greatly increased by incorporating repeat imaging and medical context, such as electronic health records (EHRs). However, clinically routine modalities such as imaging and diagnostic codes can be asynchronous and irregularly sampled over different time scales which are obstacles to longitudinal multimodal learning. In this work, we propose a transformer-based multimodal strategy to integrate repeat imaging with longitudinal clinical signatures from routinely collected EHRs for SPN classification. We perform unsupervised disentanglement of latent clinical signatures and leverage time-distance scaled self-attention to jointly learn from clinical signatures expressions and chest computed tomography (CT) scans. Our classifier is pretrained on 2,668 scans from a public dataset and 1,149 subjects with longitudinal chest CTs, billing codes, medications, and laboratory tests from EHRs of our home institution
    
[^109]: 面向全沉浸多用户虚拟现实技术的预测上下文感知和重定向步行

    Predictive Context-Awareness for Full-Immersive Multiuser Virtual Reality with Redirected Walking. (arXiv:2303.17907v1 [cs.NI])

    [http://arxiv.org/abs/2303.17907](http://arxiv.org/abs/2303.17907)

    本文提出了利用预测性上下文感知来优化发射端和接收端的波束成形和波束导向，实现面向全沉浸多用户虚拟现实技术的高效通信。

    

    虚拟现实技术正朝着增强沉浸感、支持多用户体验和在虚拟体验中支持无限制的移动，而通过重定向步行将用户限制在专门的VR设置内。为了满足未来VR系统的极端数据速率和延迟要求，支持无线网络基础设施将在毫米波（mmWave）频率上运行，并通过波束成形和波束导向实现高度定向的通信。我们提出了利用预测性上下文感知来优化发射端和接收端的波束成形和波束导向。具体而言，我们认为通过短期预测多用户VR设置中用户的横向移动，可以利用用户方向上的直线视距（LoS）“跟踪”来优化发射端的波束成形和波束导向。

    Virtual Reality (VR) technology is being advanced along the lines of enhancing its immersiveness, enabling multiuser Virtual Experiences (VEs), and supporting unconstrained mobility of the users in their VEs, while constraining them within specialized VR setups through Redirected Walking (RDW). For meeting the extreme data-rate and latency requirements of future VR systems, supporting wireless networking infrastructures will operate in millimeter Wave (mmWave) frequencies and leverage highly directional communication in both transmission and reception through beamforming and beamsteering. We propose to leverage predictive context-awareness for optimizing transmitter and receiver-side beamforming and beamsteering. In particular, we argue that short-term prediction of users' lateral movements in multiuser VR setups with RDW can be utilized for optimizing transmitter-side beamforming and beamsteering through Line-of-Sight (LoS) "tracking" in the users' directions. At the same time, short-
    
[^110]: 计算机视觉中的双曲几何：卷积神经网络的新框架

    Hyperbolic Geometry in Computer Vision: A Novel Framework for Convolutional Neural Networks. (arXiv:2303.15919v1 [cs.CV])

    [http://arxiv.org/abs/2303.15919](http://arxiv.org/abs/2303.15919)

    本文提出了HCNN，是第一个专为计算机视觉任务设计的完全双曲卷积神经网络。在洛伦兹模型的基础上，我们推广了CNN的基本组件，并通过在完全双曲设置中的实验证明了HCNN框架和洛伦兹模型的有效性。

    

    真实世界的视觉数据呈现出固有的分层结构，这些结构可以在双曲空间中有效地表示。双曲神经网络是在这种空间中学习特征表示的一种有前途的方法。然而，计算机视觉中的当前方法依赖于欧几里得骨干，并且仅在任务头中将特征投影到双曲空间中，限制了它们充分利用双曲几何的好处的能力。为了解决这个问题，我们提出了HCNN，这是第一个专为计算机视觉任务设计的完全双曲卷积神经网络（CNN）。基于洛伦兹模型，我们推广了CNN的基本组件，并提出了卷积层、批量归一化和多项式逻辑回归（MLR）的新公式。在标准视觉任务的实验中，我们展示了我们的HCNN框架和洛伦兹模型在混合和完全双曲设置中的有效性。总的来说，我们旨在为将来在双曲几何中进行的研究铺平道路。

    Real-world visual data exhibit intrinsic hierarchical structures that can be represented effectively in hyperbolic spaces. Hyperbolic neural networks (HNNs) are a promising approach for learning feature representations in such spaces. However, current methods in computer vision rely on Euclidean backbones and only project features to the hyperbolic space in the task heads, limiting their ability to fully leverage the benefits of hyperbolic geometry. To address this, we present HCNN, the first fully hyperbolic convolutional neural network (CNN) designed for computer vision tasks. Based on the Lorentz model, we generalize fundamental components of CNNs and propose novel formulations of the convolutional layer, batch normalization, and multinomial logistic regression (MLR). Experimentation on standard vision tasks demonstrates the effectiveness of our HCNN framework and the Lorentz model in both hybrid and fully hyperbolic settings. Overall, we aim to pave the way for future research in h
    
[^111]: 可微分逻辑的逻辑：走向DL的统一语义

    Logic of Differentiable Logics: Towards a Uniform Semantics of DL. (arXiv:2303.10650v2 [cs.LO] UPDATED)

    [http://arxiv.org/abs/2303.10650](http://arxiv.org/abs/2303.10650)

    该论文介绍了一个元语言——LDL，用于定义DL，该元语言从语法和语义两方面上提高DL的形式化程度，使得对DL的性质和实现进行系统比较研究成为了可能。

    

    近期，可微分逻辑（DL）被提出作为一种训练神经网络满足逻辑规范的方法。DL包括语法和将语法中的表达式转化为损失函数的解释函数。这些损失函数可以在训练过程中与标准的梯度下降算法一起使用。 现有DL的多样性和对其形式化程度的不同处理使得对它们的性质和实现进行系统比较研究变得困难。该论文通过提出一个元语言——LDL作为DL定义的系统框架，从语法和语义两方面上提高DL的形式化程度，使得对DL的性质和实现进行系统比较研究成为了可能。

    Differentiable logics (DL) have recently been proposed as a method of training neural networks to satisfy logical specifications. A DL consists of a syntax in which specifications are stated and an interpretation function that translates expressions in the syntax into loss functions. These loss functions can then be used during training with standard gradient descent algorithms. The variety of existing DLs and the differing levels of formality with which they are treated makes a systematic comparative study of their properties and implementations difficult. This paper remedies this problem by suggesting a meta-language for defining DLs that we call the Logic of Differentiable Logics, or LDL. Syntactically, it generalises the syntax of existing DLs to FOL, and for the first time introduces the formalism for reasoning about vectors and learners. Semantically, it introduces a general interpretation function that can be instantiated to define loss functions arising from different existing 
    
[^112]: 使用扩散模型生成符号音乐

    Generating symbolic music using diffusion models. (arXiv:2303.08385v1 [cs.SD])

    [http://arxiv.org/abs/2303.08385](http://arxiv.org/abs/2303.08385)

    本文提出了一种使用扩散模型生成钢琴卷帘的方法，可以协调、生成、完善音乐；代码已公开共享。

    

    概率去噪扩散模型已成为简单而强大的生成模型。与其他生成模型不同，扩散模型不会出现模式崩溃，也不需要辨别器来生成高质量样本。本文提出了一种使用二项先验分布来生成钢琴卷帘的扩散模型。本文还提出了一种高效的方法来训练模型和生成样本。生成的音乐具有时间尺度上的一致性，可以达到训练钢琴卷帘段的长度。我们展示了这样一个模型是如何在输入的条件下进行工作，并可用于协调给定的旋律，完成不完整的钢琴卷帘或生成给定乐曲的变化。代码是公开共享的，以鼓励社区使用和开发该方法。

    Probabilistic Denoising Diffusion models have emerged as simple yet very powerful generative models. Diffusion models unlike other generative models do not suffer from mode collapse nor require a discriminator to generate high quality samples. In this paper, we propose a diffusion model that uses a binomial prior distribution to generate piano-rolls. The paper also proposes an efficient method to train the model and generate samples. The generated music has coherence at time scales up to the length of the training piano-roll segments. We show how such a model is conditioned on the input and can be used to harmonize a given melody, complete an incomplete piano-roll or generate a variation of a given piece. The code is shared publicly to encourage the use and development of the method by the community.
    
[^113]: 一种新的张量专家混合并行方法来扩展混合专家训练

    A Novel Tensor-Expert Hybrid Parallelism Approach to Scale Mixture-of-Experts Training. (arXiv:2303.06318v1 [cs.LG])

    [http://arxiv.org/abs/2303.06318](http://arxiv.org/abs/2303.06318)

    本文提出了一种新的混合并行算法，结合了张量、专家和数据并行，以实现MoE模型的训练，其基本模型比当前最先进的DeepSpeed-MoE大4-8倍。

    This paper proposes a novel hybrid parallel algorithm that combines tensor, expert, and data parallelism to enable the training of MoE models with 4-8x larger base models than the current state-of-the-art -- DeepSpeed-MoE.

    最近提出了一种名为Mixture-of-Experts（MoE）的新型神经网络架构，通过添加稀疏激活的专家块来增加神经网络（基本模型）的参数，而不改变训练或推理的总浮点操作数。理论上，这种架构允许我们训练任意大的模型，同时保持计算成本与基本模型相同。然而，在64到128个专家块之外，先前的工作观察到这些MoE模型的测试准确性递减。因此，训练高质量的MoE模型需要我们扩展基本模型的大小以及专家块的数量。在这项工作中，我们提出了一种新颖的三维混合并行算法，结合了张量、专家和数据并行，以实现MoE模型的训练，其基本模型比当前最先进的DeepSpeed-MoE大4-8倍。我们在优化器步骤中提出了内存优化。

    A new neural network architecture called Mixture-of-Experts (MoE) has been proposed recently that increases the parameters of a neural network (the base model) by adding sparsely activated expert blocks, without changing the total number of floating point operations for training or inference. In theory, this architecture allows us to train arbitrarily large models while keeping the computational costs same as that of the base model. However, beyond 64 to 128 experts blocks, prior work has observed diminishing returns in the test accuracies of these MoE models. Thus, training high quality MoE models requires us to scale the size of the base models, along with the number of expert blocks. In this work, we propose a novel, three-dimensional, hybrid parallel algorithm that combines tensor, expert, and data parallelism to enable the training of MoE models with 4-8x larger base models than the current state-of-the-art -- DeepSpeed-MoE. We propose memory optimizations in the optimizer step, a
    
[^114]: 基于图像修复的生成模型

    Restoration based Generative Models. (arXiv:2303.05456v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.05456](http://arxiv.org/abs/2303.05456)

    本文提出了一种基于图像修复的生成模型，即DDMs。通过整合IR文献，我们使用替代目标和多样的前向过程，提供了一个MAP先验损失函数的基础，消除了DDMs中昂贵的采样的需求。此外，我们提出了多尺度训练，提高了性能。我们展示了我们的模型在反问题上的适用性，并相信我们的框架可以铺平快速图像合成和修复的道路。

    

    最近由于其出色的合成质量，去噪扩散模型（DDMs）越来越受到关注。DDMs建立在将数据推向噪声分布的扩散过程上，模型学习去噪。在本文中，我们建立了DDMs在图像修复（IR）方面的解释。整合IR文献使我们可以使用替代目标和多样的前向过程，而不仅局限于扩散过程。通过在基于MAP估计的损失函数上施加先验知识，我们消除了DDMs中昂贵的采样的需求。此外，我们提出了多尺度训练，通过利用前向过程的灵活性，提高了性能。实验结果表明，我们的模型提高了训练和推断的质量和效率。此外，我们展示了我们的模型在反问题上的适用性。我们相信，我们的框架铺平了快速图像合成和修复的道路。

    Denoising diffusion models (DDMs) have recently attracted increasing attention by showing impressive synthesis quality. DDMs are built on a diffusion process that pushes data to the noise distribution and the models learn to denoise. In this paper, we establish the interpretation of DDMs in terms of image restoration (IR). Integrating IR literature allows us to use an alternative objective and diverse forward processes, not confining to the diffusion process. By imposing prior knowledge on the loss function grounded on MAP-based estimation, we eliminate the need for the expensive sampling of DDMs. Also, we propose a multi-scale training, which improves the performance compared to the diffusion process, by taking advantage of the flexibility of the forward process. Experimental results demonstrate that our model improves the quality and efficiency of both training and inference. Furthermore, we show the applicability of our model to inverse problems. We believe that our framework paves 
    
[^115]: CoolPINNs: 基于物理学的神经网络建模血管系统中的主动冷却

    CoolPINNs: A Physics-informed Neural Network Modeling of Active Cooling in Vascular Systems. (arXiv:2303.05300v2 [math.NA] UPDATED)

    [http://arxiv.org/abs/2303.05300](http://arxiv.org/abs/2303.05300)

    本文提出了一种基于物理学引导的神经网络的建模框架——CoolPINNs，用于旨在高效热调节的血管系统。它能够准确捕捉复杂血管布局中的热流急剧跳变，处理包括切向和法向分量的斜率导数，解决由于辐射热传递引起的非线性，并为实时监测提供高速预测，更便于稳健的反演建模。

    

    新兴技术如高超声速飞机、空间探索车和电池利用嵌入式微细循环进行高效热调节。然而，在这些工程系统的设计和操作阶段进行建模是至关重要的，但在开发建模框架方面存在许多挑战。本文通过物理学引导的神经网络（PINNs）的应用来解决这些挑战。我们开发了一个名为CoolPINNs的快速、可靠、准确的基于PINNs的科学机器学习（SciML）框架，用于血管系统的热调节建模。

    Emerging technologies like hypersonic aircraft, space exploration vehicles, and batteries avail fluid circulation in embedded microvasculatures for efficient thermal regulation. Modeling is vital during these engineered systems' design and operational phases. However, many challenges exist in developing a modeling framework. What is lacking is an accurate framework that (i) captures sharp jumps in the thermal flux across complex vasculature layouts, (ii) deals with oblique derivatives (involving tangential and normal components), (iii) handles nonlinearity because of radiative heat transfer, (iv) provides a high-speed forecast for real-time monitoring, and (v) facilitates robust inverse modeling. This paper addresses these challenges by availing the power of physics-informed neural networks (PINNs). We develop a fast, reliable, and accurate Scientific Machine Learning (SciML) framework for vascular-based thermal regulation -- called CoolPINNs: a PINNs-based modeling framework for activ
    
[^116]: QuickSRNet：适用于移动平台快速推理的简单单图像超分辨率架构

    QuickSRNet: Plain Single-Image Super-Resolution Architecture for Faster Inference on Mobile Platforms. (arXiv:2303.04336v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2303.04336](http://arxiv.org/abs/2303.04336)

    QuickSRNet 是一种适用于移动平台上的实时应用的高效超分辨率架构，它解决了移动设备上进行实时的深度学习超分辨率的挑战，并提供了比现有神经架构更好的准确性与延迟权衡。

    

    本文提出了一种高效的超分辨率架构QuickSRNet，适用于移动平台上的实时应用。超分辨率可以使图像更加清晰、锐利并提高分辨率。游戏和视频播放应用以及不断提升的电视、智能手机和VR头显的显示能力，推动了对高效提升分辨率的需求。尽管基于深度学习的超分辨率方法在视觉质量方面取得了令人印象深刻的成果，但在计算、热量和能源消耗方面，实现在移动设备上进行实时的深度学习超分辨率是具有挑战性的。为解决这些挑战，我们提出了QuickSRNet，这是一种简单而有效的架构，优于现有的神经架构，提供更好的准确性与延迟的权衡，用于单图像超分辨率。我们提出了训练技巧，可以加速现有的基于残差的超分辨率架构，同时保持对量化的稳健性。

    In this work, we present QuickSRNet, an efficient super-resolution architecture for real-time applications on mobile platforms. Super-resolution clarifies, sharpens, and upscales an image to higher resolution. Applications such as gaming and video playback along with the ever-improving display capabilities of TVs, smartphones, and VR headsets are driving the need for efficient upscaling solutions. While existing deep learning-based super-resolution approaches achieve impressive results in terms of visual quality, enabling real-time DL-based super-resolution on mobile devices with compute, thermal, and power constraints is challenging. To address these challenges, we propose QuickSRNet, a simple yet effective architecture that provides better accuracy-to-latency trade-offs than existing neural architectures for single-image super resolution. We present training tricks to speed up existing residual-based super-resolution architectures while maintaining robustness to quantization. Our pro
    
[^117]: 差分隐私训练的任意决策是一个隐藏成本

    Arbitrary Decisions are a Hidden Cost of Differentially Private Training. (arXiv:2302.14517v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14517](http://arxiv.org/abs/2302.14517)

    差分隐私训练会产生预测多样性，即使对于相同输入，使用不同的随机性也会得到不同的输出，这一成本不仅未被研究还未被审核或传达给模型设计者和利益相关者。

    

    隐私保护的机器学习中使用的机制通常旨在在模型训练期间保证差分隐私(DP)。在将模型参数拟合到隐私敏感数据时，实践中使用随机化方法(例如，在截断的梯度上添加高斯噪声)以确保DP训练。我们证明了这种随机化会产生预测多样性：对于给定的输入示例，由同样DP-保证的模型预测的输出取决于训练中使用的随机性。因此，对于给定的输入，即使使用相同的训练数据集重新训练模型，预测输出也可能发生巨大变化。尚未研究由DP训练引起的多样性成本，并且目前还未经过审核或向模型设计者和利益相关者传达。我们得出了一种在可靠地估计预测多样性所需的重新训练次数的上限，并通过广泛的实验分析了三种DP-确保训练方法的预测多样性成本，包括理论和实验两个方面。

    Mechanisms used in privacy-preserving machine learning often aim to guarantee differential privacy (DP) during model training. Practical DP-ensuring training methods use randomization when fitting model parameters to privacy-sensitive data (e.g., adding Gaussian noise to clipped gradients). We demonstrate that such randomization incurs predictive multiplicity: for a given input example, the output predicted by equally-private models depends on the randomness used in training. Thus, for a given input, the predicted output can vary drastically if a model is re-trained, even if the same training dataset is used. The predictive-multiplicity cost of DP training has not been studied, and is currently neither audited for nor communicated to model designers and stakeholders. We derive a bound on the number of re-trainings required to estimate predictive multiplicity reliably. We analyze--both theoretically and through extensive experiments--the predictive-multiplicity cost of three DP-ensuring
    
[^118]: 敏捷建模：从概念到分类器只需几分钟

    Agile Modeling: From Concept to Classifier in Minutes. (arXiv:2302.12948v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12948](http://arxiv.org/abs/2302.12948)

    本文介绍了敏捷建模的概念，即将任何主观视觉概念转化为计算机视觉模型的过程，并通过用户研究表明，用户可以在30分钟内轻松创建分类器。

    

    计算机视觉在主观细微应用方面的应用越来越多。虽然众包对于大多数客观任务（如标记“斑马”）已经为视觉社区服务得很好，但在概念具有实质性主观性的任务（例如识别“美食金枪鱼”）上，它现在面临失败。然而，让任何用户开发其概念的分类器在技术上是困难的：用户既不是机器学习专家，也没有耐心标记数千个示例。为此，我们提出了敏捷建模的问题：通过实时用户参与将任何主观视觉概念转化为计算机视觉模型的过程。我们为图像分类实例化了一个敏捷建模原型，并通过用户研究（N=14）表明，用户可以在30分钟内轻松创建分类器。我们将这个用户驱动的过程与传统的众包范式进行对比，并发现群体的观念常常与用户不同。

    The application of computer vision to nuanced subjective use cases is growing. While crowdsourcing has served the vision community well for most objective tasks (such as labeling a "zebra"), it now falters on tasks where there is substantial subjectivity in the concept (such as identifying "gourmet tuna"). However, empowering any user to develop a classifier for their concept is technically difficult: users are neither machine learning experts, nor have the patience to label thousands of examples. In reaction, we introduce the problem of Agile Modeling: the process of turning any subjective visual concept into a computer vision model through a real-time user-in-the-loop interactions. We instantiate an Agile Modeling prototype for image classification and show through a user study (N=14) that users can create classifiers with minimal effort under 30 minutes. We compare this user driven process with the traditional crowdsourcing paradigm and find that the crowd's notion often differs fro
    
[^119]: 用Metropolis-adjusted Langevin算法高效处理约束条件

    Efficiently handling constraints with Metropolis-adjusted Langevin algorithm. (arXiv:2302.11971v2 [stat.CO] UPDATED)

    [http://arxiv.org/abs/2302.11971](http://arxiv.org/abs/2302.11971)

    本研究提出了Metropolis-adjusted Langevin算法来处理带有约束条件的目标分布，理论和实验结果表明该算法在处理这种情况时效果优于竞争算法。

    

    本研究探讨了在目标分布支撑集上施加约束条件时，Metropolis-adjusted Langevin算法的性能。我们对其得出的马尔可夫链进行了严格分析，证明其收敛性并推导了其混合时间的上界。我们的结果表明，Metropolis-adjusted Langevin算法在处理这种具有挑战性的情况方面非常有效：我们得到的混合时间上界优于无接受-拒绝步骤的竞争算法的最佳已知上界。我们的数值实验支持这些理论发现，表明Metropolis-adjusted Langevin算法在处理目标分布支撑集约束时具有良好的性能。

    In this study, we investigate the performance of the Metropolis-adjusted Langevin algorithm in a setting with constraints on the support of the target distribution. We provide a rigorous analysis of the resulting Markov chain, establishing its convergence and deriving an upper bound for its mixing time. Our results demonstrate that the Metropolis-adjusted Langevin algorithm is highly effective in handling this challenging situation: the mixing time bound we obtain is superior to the best known bounds for competing algorithms without an accept-reject step. Our numerical experiments support these theoretical findings, indicating that the Metropolis-adjusted Langevin algorithm shows promising performance when dealing with constraints on the support of the target distribution.
    
[^120]: 引导深度核学习

    Guided Deep Kernel Learning. (arXiv:2302.09574v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09574](http://arxiv.org/abs/2302.09574)

    本文提出了一种引导深度核学习的方法，利用无限宽度神经网络学习深度核，通过神经网络高斯过程模型在优化中指导深度核学习模型，在遇到新数据点时能够适应目标置信度，既利用了贝叶斯行为，又保持了深度核的泛化能力、可扩展性和灵活性。

    

    现在通常通过深度核学习 (DKL) 将高斯过程与深度神经网络的表达能力结合起来。不幸的是，由于核优化过程，这种方法常常会失去它们的贝叶斯优势。在本研究中，我们提出了一种利用无限宽度神经网络学习深度核的新方法。我们提出使用神经网络高斯过程 (NNGP) 模型作为 DKl 模型在优化过程中的指导。我们的方法利用 NNGP 的可靠不确定性估计，以使 DKL 在遇到新数据点时能够适应目标置信度。因此，我们既利用了 NNGP 的贝叶斯行为 (即其抗过拟合性和准确的不确定性估计)，又保持了深度核的泛化能力，可扩展性和灵活性。在多个基准数据集上进行的实证分析表明，我们的方法在不同大小和维度的数据集上都取得了很好的效果。

    Combining Gaussian processes with the expressive power of deep neural networks is commonly done nowadays through deep kernel learning (DKL). Unfortunately, due to the kernel optimization process, this often results in losing their Bayesian benefits. In this study, we present a novel approach for learning deep kernels by utilizing infinite-width neural networks. We propose to use the Neural Network Gaussian Process (NNGP) model as a guide to the DKL model in the optimization process. Our approach harnesses the reliable uncertainty estimation of the NNGPs to adapt the DKL target confidence when it encounters novel data points. As a result, we get the best of both worlds, we leverage the Bayesian behavior of the NNGP, namely its robustness to overfitting, and accurate uncertainty estimation, while maintaining the generalization abilities, scalability, and flexibility of deep kernels. Empirically, we show on multiple benchmark datasets of varying sizes and dimensionality, that our method i
    
[^121]: 原型图像分类中补丁可视化的合理性检查和改进

    Sanity checks and improvements for patch visualisation in prototype-based image classification. (arXiv:2302.08508v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.08508](http://arxiv.org/abs/2302.08508)

    本文通过精细的数据集，发现了基于原型的视觉分类中可视化方法的局限性，并提出了使用更忠实方法的必要性。

    

    本文对基于原型的视觉分类中实施的可视化方法进行了深入分析，使用两个精细的数据集（CUB-200-2011 和 Stanford Cars）首先表明这种方法不能正确地识别图像中的感兴趣区域，因此不能反映出模型的行为。其次，使用删除度量，我们定量地证明了 Smoothgrads 或 PRP 等显著性方法提供了更忠实的图像补丁。我们还提出了一种基于某些数据集（例如 CUB-200-2011）中提供的对象分割的相关性度量，并展示了 ProtoPNet 和 ProtoTree 产生的不精确的补丁可视化可能会产生错误的偏见感，可以通过使用更忠实的方法来减轻。最后，我们讨论了我们的发现对其他使用相同可视化方法的基于原型的模型的影响。

    In this work, we perform an in-depth analysis of the visualisation methods implemented in two popular self-explaining models for visual classification based on prototypes - ProtoPNet and ProtoTree. Using two fine-grained datasets (CUB-200-2011 and Stanford Cars), we first show that such methods do not correctly identify the regions of interest inside of the images, and therefore do not reflect the model behaviour. Secondly, using a deletion metric, we demonstrate quantitatively that saliency methods such as Smoothgrads or PRP provide more faithful image patches. We also propose a new relevance metric based on the segmentation of the object provided in some datasets (e.g. CUB-200-2011) and show that the imprecise patch visualisations generated by ProtoPNet and ProtoTree can create a false sense of bias that can be mitigated by the use of more faithful methods. Finally, we discuss the implications of our findings for other prototype-based models sharing the same visualisation method.
    
[^122]: 智能材料中稀疏滞后模型的发现

    Discovering sparse hysteresis models for smart materials. (arXiv:2302.05313v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05313](http://arxiv.org/abs/2302.05313)

    本文提出了一种利用机器学习中的稀疏回归技术建模智能材料滞后的方法，并成功对压电材料的滞后现象进行建模和预测。同时在磁性材料方面提供了稀疏白盒建模滞后的见解。

    

    本文提出了一种利用机器学习中的稀疏回归技术建模智能材料（尤其是压电材料）滞后的方法。该研究采用了最小二乘算法和顺序阈值方法对负责滞后的动态系统进行建模，得到了简洁的模型，可以准确预测模拟和实验压电材料数据的滞后现象。文章还模拟了不同的数值实验，包括学习蝴蝶形滞后、对压电致动器的真实滞后数据进行建模。此外，还通过以非取向电工钢为例，提供了对磁性材料稀疏白盒建模滞后的见解。

    This article presents an approach for modelling hysteresis in smart materials, specifically piezoelectric materials, that leverages recent advancements in machine learning, particularly in sparse-regression techniques. While sparse regression has previously been used to model various scientific and engineering phenomena, its application to nonlinear hysteresis modelling in piezoelectric materials has yet to be explored. The study employs the least-squares algorithm with a sequential threshold to model the dynamic system responsible for hysteresis, resulting in a concise model that accurately predicts hysteresis for both simulated and experimental piezoelectric material data. Several numerical experiments are performed, including learning butterfly-shaped hysteresis and modelling real-world hysteresis data for a piezoelectric actuator. Additionally, insights are provided on sparse white-box modelling of hysteresis for magnetic materials taking non-oriented electrical steel as an example
    
[^123]: 利用剪枝神经网络中的稀疏性来优化大型模型训练

    Exploiting Sparsity in Pruned Neural Networks to Optimize Large Model Training. (arXiv:2302.05045v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05045](http://arxiv.org/abs/2302.05045)

    本文提出了一种新的方法，利用稀疏子网络来优化两种流行的深度学习并行算法 - 数据并行和层间并行的内存利用和通信。在512个NVIDIA V100 GPU上，我们的优化将27亿参数模型的内存消耗减少了74％，总通信时间减少了40％。

    This paper proposes a novel approach that exploits sparse subnetworks to optimize memory utilization and communication in two popular algorithms for parallel deep learning, and demonstrates significant reductions in memory consumption and communication time on a 2.7 billion parameter model using 512 NVIDIA V100 GPUs.

    由于通信开销的显著增加，规模化神经网络的并行训练具有挑战性。最近，深度学习研究人员开发了各种剪枝算法，能够剪枝（即将神经网络中的参数设置为零）80-90％的参数，以产生与未剪枝父网络相等的稀疏子网络。在本文中，我们提出了一种新的方法，利用这些稀疏子网络来优化两种流行的深度学习并行算法 - 数据并行和层间并行的内存利用和通信。我们将我们的方法集成到AxoNN中，这是一个高度可扩展的并行深度学习框架，依赖于数据和层间并行，并展示了通信时间和内存利用的减少。在512个NVIDIA V100 GPU上，我们的优化将27亿参数模型的内存消耗减少了74％，总通信时间减少了40％，从而提供了

    Parallel training of neural networks at scale is challenging due to significant overheads arising from communication. Recently, deep learning researchers have developed a variety of pruning algorithms that are capable of pruning (i.e. setting to zero) 80-90% of the parameters in a neural network to yield sparse subnetworks that equal the accuracy of the unpruned parent network. In this work, we propose a novel approach that exploits these sparse subnetworks to optimize the memory utilization and communication in two popular algorithms for parallel deep learning namely -- data and inter-layer parallelism. We integrate our approach into AxoNN, a highly scalable framework for parallel deep learning that relies on data and inter-layer parallelism, and demonstrate the reduction in communication time and memory utilization. On 512 NVIDIA V100 GPUs, our optimizations reduce the memory consumption of a 2.7 billion parameter model by 74%, and the total communication time by 40%, thus providing 
    
[^124]: 论校准的精细度

    On the Richness of Calibration. (arXiv:2302.04118v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04118](http://arxiv.org/abs/2302.04118)

    本文提出了一个新的校准评估框架，探索了校准分数设计中的不同选择，并研究了根据输入特征而不是预测结果对数据点进行分组的优势，从而帮助制定具有理想数学特性的新方法。

    

    通过与观测标签频率的比较，即通过校准的方法可以评估概率预测，最近，算法公平性方面的学者开始研究一个名为多校准的校准基准的不断增长的多样性，但还是比较局限的。在本文中，我们通过明确设计校准度量时涉及的选择，探索和分析了校准评估的各种形式。我们将这些选择分为三个分组选择和一个关于组错误合并的选择。这提供了一个框架，以比较以前提出的校准分数，并有助于制定具有理想数学特性的新方法。特别是，我们探讨了根据输入特征而不是预测对数据点进行分组的可能性，并正式展示这种方法的优点。我们还对适合的组错误合并函数的空间进行了表征。

    Probabilistic predictions can be evaluated through comparisons with observed label frequencies, that is, through the lens of calibration. Recent scholarship on algorithmic fairness has started to look at a growing variety of calibration-based objectives under the name of multi-calibration but has still remained fairly restricted. In this paper, we explore and analyse forms of evaluation through calibration by making explicit the choices involved in designing calibration scores. We organise these into three grouping choices and a choice concerning the agglomeration of group errors. This provides a framework for comparing previously proposed calibration scores and helps to formulate novel ones with desirable mathematical properties. In particular, we explore the possibility of grouping datapoints based on their input features rather than on predictions and formally demonstrate advantages of such approaches. We also characterise the space of suitable agglomeration functions for group erro
    
[^125]: 基于状态的安全强化学习：综述

    State-wise Safe Reinforcement Learning: A Survey. (arXiv:2302.03122v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03122](http://arxiv.org/abs/2302.03122)

    本文综合回顾了强化学习中解决基于状态约束的方法，讨论了它们在安全保障和可扩展性、安全性和奖励表现、收敛后和训练过程中的安全性等方面的联系、差异和权衡，并讨论了未来发展方向。

    

    尽管强化学习在仿真环境中取得了巨大的成功，但将其应用于实际场景仍然面临许多挑战。其中一个主要关注点是安全性，也就是约束满足。状态约束是实际应用中最常见且最具挑战性的约束之一，这对于许多挑战性任务，如自动驾驶、机器人操作等而言是必要和关键的。本文综述了现有的解决基于状态的约束的强化学习方法，并在状态约束马尔可夫决策过程的框架下，从安全保障和可扩展性、安全性和奖励表现、收敛后和训练过程中的安全性等方面，讨论了现有方法的联系、差异和权衡。我们还总结了当前方法的局限性并讨论了未来发展方向。

    Despite the tremendous success of Reinforcement Learning (RL) algorithms in simulation environments, applying RL to real-world applications still faces many challenges. A major concern is safety, in another word, constraint satisfaction. State-wise constraints are one of the most common constraints in real-world applications and one of the most challenging constraints in Safe RL. Enforcing state-wise constraints is necessary and essential to many challenging tasks such as autonomous driving, robot manipulation. This paper provides a comprehensive review of existing approaches that address state-wise constraints in RL. Under the framework of State-wise Constrained Markov Decision Process (SCMDP), we will discuss the connections, differences, and trade-offs of existing approaches in terms of (i) safety guarantee and scalability, (ii) safety and reward performance, and (iii) safety after convergence and during training. We also summarize limitations of current methods and discuss potentia
    
[^126]: 通过风险分解评估自监督学习

    Evaluating Self-Supervised Learning via Risk Decomposition. (arXiv:2302.03068v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03068](http://arxiv.org/abs/2302.03068)

    通过风险分解，提出四个误差部分评估自监督学习对169个视觉模型的影响，为SSL的设计和使用提供宝贵的见解。

    

    自监督学习（SSL）的流程设计涉及架构、增强和预训练数据等诸多选择。然而，SSL通常使用单一度量来评估，这并不能提供深入的洞察和改进方案。为解决这些问题，我们提出了一个SSL风险分解，从逼近、表示可用性、探针泛化和编码器泛化等角度对错误进行分解。我们分析了30个设计选择对169个在ImageNet上评估的SSL视觉模型的影响，并为每个组件提供了高效的估计器，为SSL模型的设计和使用提供宝贵的见解。

    Self-supervised learning (SSL) pipelines differ in many design choices such as the architecture, augmentations, or pretraining data. Yet SSL is typically evaluated using a single metric: linear probing on ImageNet. This does not provide much insight into why or when a model is better, now how to improve it. To address this, we propose an SSL risk decomposition, which generalizes the classical supervised approximation-estimation decomposition by considering errors arising from the representation learning step. Our decomposition consists of four error components: approximation, representation usability, probe generalization, and encoder generalization. We provide efficient estimators for each component and use them to analyze the effect of 30 design choices on 169 SSL vision models evaluated on ImageNet. Our analysis gives valuable insights for designing and using SSL models. For example, it highlights the main sources of error and shows how to improve SSL in specific settings (full- vs 
    
[^127]: 双排列等变性在知识图谱补全中的应用

    Double Permutation Equivariance for Knowledge Graph Completion. (arXiv:2302.01313v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01313](http://arxiv.org/abs/2302.01313)

    本研究提出了双排列等变性的KG表示方法，可以使神经网络在KG中执行复杂的逻辑推理任务，并在多个归纳KG完成任务中实现了最先进的Hits@10测试准确率。双排列等变性在KG中开辟了新的研究方向。

    

    本研究将知识图谱(KGs)形式化为一种新型的图，并称之为双交换属性图，其中节点和二元（两个节点之间的）表示必须对节点号和边（及节点）属性（关系和节点特征）的排列等变。双重排列等变的KG表示在KG中开辟了新的研究方向。我们展示了这种等变性对关系的结构表示产生的影响，从而使神经网络能够在KG中执行复杂的逻辑推理任务。最后，我们介绍了一种通用的等变表示蓝图，并测试了一种简单的基于GNN的双排列等变神经结构，在WN18RR、FB237和NELL995归纳KG完成任务中实现了最先进的Hits@10测试准确率，并能够准确执行现有方法无法执行的逻辑推理任务。

    This work provides a formalization of Knowledge Graphs (KGs) as a new class of graphs that we denote doubly exchangeable attributed graphs, where node and pairwise (joint 2-node) representations must be equivariant to permutations of both node ids and edge (& node) attributes (relations & node features). Double-permutation equivariant KG representations open a new research direction in KGs. We show that this equivariance imposes a structural representation of relations that allows neural networks to perform complex logical reasoning tasks in KGs. Finally, we introduce a general blueprint for such equivariant representations and test a simple GNN-based double-permutation equivariant neural architecture that achieve state-of-the-art Hits@10 test accuracy in the WN18RR, FB237 and NELL995 inductive KG completion tasks, and can accurately perform logical reasoning tasks that no existing methods can perform, to the best of our knowledge.
    
[^128]: 物理学约束下带不确定性量化的运动预测

    Physics Constrained Motion Prediction with Uncertainty Quantification. (arXiv:2302.01060v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2302.01060](http://arxiv.org/abs/2302.01060)

    该论文提出了一种物理学约束的运动预测方法，使用替代动力学模型来确保预测的轨迹在动力学上是可行的，同时通过构建适用于自动驾驶的预测区域，量化了不确定性，实验结果表明，在自主赛车数据集上实现了显著的预测精度提升。

    

    预测动态物体的运动对于保证自主系统的安全至关重要。其中一个挑战是运动预测算法应遵守动力学约束，并将预测不确定性量化为置信度的衡量标准。我们提出了一种物理学约束的运动预测方法，使用替代动力学模型来确保预测的轨迹在动力学上是可行的。我们提出了一个两步集成过程，包括意图预测和轨迹预测，同时满足了动力学约束。通过使用流行的统计工具，构建了适用于自动驾驶的预测区域，量化了不确定性。在自主赛车数据集的实验中，相比基线，物理学约束下带不确定性量化的运动预测实现了41%更好的ADE，56%更好的FDE和19%更好的IoU。

    Predicting the motion of dynamic agents is a critical task for guaranteeing the safety of autonomous systems. A particular challenge is that motion prediction algorithms should obey dynamics constraints and quantify prediction uncertainty as a measure of confidence. We present a physics-constrained approach for motion prediction which uses a surrogate dynamical model to ensure that predicted trajectories are dynamically feasible. We propose a two-step integration consisting of intent and trajectory prediction subject to dynamics constraints. We also construct prediction regions that quantify uncertainty and are tailored for autonomous driving by using conformal prediction, a popular statistical tool. Physics Constrained Motion Prediction achieves a 41% better ADE, 56% better FDE, and 19% better IoU over a baseline in experiments using an autonomous racing dataset.
    
[^129]: 通过偏向校准解决视觉语言模型中的偏差

    Debiasing Vision-Language Models via Biased Prompts. (arXiv:2302.00070v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00070](http://arxiv.org/abs/2302.00070)

    本研究提出了一种通用方法，通过在文本嵌入中投影出偏向方向来校准视觉语言基础模型的偏差，仅进行文本嵌入去偏执就足以生成严谨的分类器和公正的生成模型，有效减少了社交偏见和虚假相关，无需额外数据或培训。

    

    研究表明，机器学习模型会从它们的训练数据集中继承偏见。对于从互联网上爬取的未加筛选的数据集训练的视觉语言基础模型，这可能会特别具有问题。偏见可能会被放大并传播到其他应用程序，如零样本分类器和文本到图像生成模型中。在本研究中，我们提出了一种通用方法，通过在文本嵌入中投影出偏向方向来校准视觉语言基础模型的偏差。特别地，我们展示了仅使用经过校准的投影矩阵对文本嵌入进行去偏执就足以生成严谨的分类器和公正的生成模型。所提出的闭合形式解决方案使得易于在大规模管道中进行集成，实证结果表明，我们的方法有效地减少了区分性和生成性视觉语言模型的社交偏见和虚假相关，而不需要额外的数据或培训。

    Machine learning models have been shown to inherit biases from their training datasets. This can be particularly problematic for vision-language foundation models trained on uncurated datasets scraped from the internet. The biases can be amplified and propagated to downstream applications like zero-shot classifiers and text-to-image generative models. In this study, we propose a general approach for debiasing vision-language foundation models by projecting out biased directions in the text embedding. In particular, we show that debiasing only the text embedding with a calibrated projection matrix suffices to yield robust classifiers and fair generative models. The proposed closed-form solution enables easy integration into large-scale pipelines, and empirical results demonstrate that our approach effectively reduces social bias and spurious correlation in both discriminative and generative vision-language models without the need for additional data or training.
    
[^130]: LegendreTron：升级版多类别正确多项损失学习

    LegendreTron: Uprising Proper Multiclass Loss Learning. (arXiv:2301.11695v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.11695](http://arxiv.org/abs/2301.11695)

    本文提出了一种新颖和实用的方法{\sc LegendreTron}，用于联合学习多类别问题的正确标准损失和概率。这种方法在基准测试中经常优于其他方法。

    

    损失函数是监督学习的基础，通常在模型开发之前选择。为避免选择损失函数可能出现的特定选择，统计决策理论描述了损失的一种理想属性，称为“正确性”，它断言贝叶斯规则是最优的。最近的研究尝试联合学习损失和模型。现有方法通过拟合一个将$\mathbb{R}$单调映射到$[0,1]$的反解标准链接函数来估计二元问题的概率。本文通过使用凸函数梯度的单调性将单调性扩展到$\mathbb{R}^{C-1}$到概率的正投影$\tilde{\Delta}^{C-1}$的映射上。我们提出了一种新颖而实用的方法{\sc LegendreTron}，用于联合学习多类别问题的正确标准损失和概率。在最多1,000种类别的领域基准测试中，我们的实验结果表明，我们的方法始终优于其他基准方法。

    Loss functions serve as the foundation of supervised learning and are often chosen prior to model development. To avoid potentially ad hoc choices of losses, statistical decision theory describes a desirable property for losses known as \emph{properness}, which asserts that Bayes' rule is optimal. Recent works have sought to \emph{learn losses} and models jointly. Existing methods do this by fitting an inverse canonical link function which monotonically maps $\mathbb{R}$ to $[0,1]$ to estimate probabilities for binary problems. In this paper, we extend monotonicity to maps between $\mathbb{R}^{C-1}$ and the projected probability simplex $\tilde{\Delta}^{C-1}$ by using monotonicity of gradients of convex functions. We present {\sc LegendreTron} as a novel and practical method that jointly learns \emph{proper canonical losses} and probabilities for multiclass problems. Tested on a benchmark of domains with up to 1,000 classes, our experimental results show that our method consistently ou
    
[^131]: 上下文相关的基于核方法的隐马尔可夫模型用于时间序列分析

    Context-specific kernel-based hidden Markov model for time series analysis. (arXiv:2301.09870v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.09870](http://arxiv.org/abs/2301.09870)

    本文提出了一种可以捕获核依赖关系的基于核密度估计的隐马尔可夫模型。与传统模型和基于核密度估计的模型相比，该模型在具有依赖性的数据上具有更好的性能。

    

    传统的隐马尔可夫模型是理解和建模随机动态数据的有用工具；在非高斯数据的情况下，可以使用类似高斯混合隐马尔可夫模型的模型。但是，这些模型受到精度矩阵的计算以及具有很多不必要的参数的影响。因此，这样的模型在假定所有变量独立的情况下表现更好，这可能是不现实的假设。基于核密度估计的隐马尔可夫模型也能够建模非高斯数据，但它们假定变量之间是独立的。在本文中，我们引入了一种基于核密度估计的新型隐马尔可夫模型，能够利用上下文相关贝叶斯网络捕获核依赖关系。介绍了所提出模型及其基于期望最大化算法的学习算法。此外，还将该模型与相关的HMM在合成和实际数据上进行了比较。

    Traditional hidden Markov models have been a useful tool to understand and model stochastic dynamic data; in the case of non-Gaussian data, models such as mixture of Gaussian hidden Markov models can be used. However, these suffer from the computation of precision matrices and have a lot of unnecessary parameters. As a consequence, such models often perform better when it is assumed that all variables are independent, a hypothesis that may be unrealistic. Hidden Markov models based on kernel density estimation are also capable of modeling non-Gaussian data, but they assume independence between variables. In this article, we introduce a new hidden Markov model based on kernel density estimation, which is capable of capturing kernel dependencies using context-specific Bayesian networks. The proposed model is described, together with a learning algorithm based on the expectation-maximization algorithm. Additionally, the model is compared to related HMMs on synthetic and real data. From th
    
[^132]: 联邦推荐中的双重个性化

    Dual Personalization on Federated Recommendation. (arXiv:2301.08143v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2301.08143](http://arxiv.org/abs/2301.08143)

    本研究提出了一种新的个性化联邦推荐框架，可以学习轻量级模型并在智能设备上部署，同时实现对用户和物品的精细个性化。

    

    联邦推荐是一种旨在在联邦环境下提供隐私保护推荐服务的新型Internet服务架构。现有解决方案用于组合分布式推荐算法和隐私保护机制，因此从根本上采用服务器上的重量级模型，阻碍了在设备上部署智能模型。本文提出了一种新颖的个性化联邦推荐（PFedRec）框架，用于学习许多用户特定的轻量级模型，以便在智能设备上部署，而不是在服务器上使用重量级模型。此外，我们提出了一种新的双重个性化机制，以有效地学习用户和项目的细粒度个性化。整个学习过程被形式化为一个统一的联邦优化框架。

    Federated recommendation is a new Internet service architecture that aims to provide privacy-preserving recommendation services in federated settings. Existing solutions are used to combine distributed recommendation algorithms and privacy-preserving mechanisms. Thus it inherently takes the form of heavyweight models at the server and hinders the deployment of on-device intelligent models to end-users. This paper proposes a novel Personalized Federated Recommendation (PFedRec) framework to learn many user-specific lightweight models to be deployed on smart devices rather than a heavyweight model on a server. Moreover, we propose a new dual personalization mechanism to effectively learn fine-grained personalization on both users and items. The overall learning process is formulated into a unified federated optimization framework. Specifically, unlike previous methods that share exactly the same item embeddings across users in a federated system, dual personalization allows mild finetuni
    
[^133]: 用负通量聚合估计特征归因

    Negative Flux Aggregation to Estimate Feature Attributions. (arXiv:2301.06989v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.06989](http://arxiv.org/abs/2301.06989)

    该论文提出了一种新的算法，名为负通量聚合（NeFLAG），可用于估计深度神经网络中输入特征对预测的影响，该方法不需要拟合替代模型或路径积分梯度。

    

    由于增长中的安全和/或透明度问题，对于理解深度神经网络(DNN)行为的需求不断增加。 由于深度神经网络架构的多层非线性，解释DNN预测仍然是一个未解决的问题，这阻碍了我们深入了解机制。为了增强DNN的可解释性，我们使用离散和通量来估计预测任务的输入特征的归因。受矢量分析中的散度定理启发，我们开发了一种新的负通量聚合（NeFLAG）公式和有效的近似算法来估计归因图。与先前的技术不同，我们的方法不依赖于拟合替代模型，也不需要梯度的路径积分。定性和定量实验都证明了NeFLAG在生成更准确的归因图方面优于竞争方法。我们的代码可在 \url{https://git

    There are increasing demands for understanding deep neural networks' (DNNs) behavior spurred by growing security and/or transparency concerns. Due to multi-layer nonlinearity of the deep neural network architectures, explaining DNN predictions still remains as an open problem, preventing us from gaining a deeper understanding of the mechanisms. To enhance the explainability of DNNs, we estimate the input feature's attributions to the prediction task using divergence and flux. Inspired by the divergence theorem in vector analysis, we develop a novel Negative Flux Aggregation (NeFLAG) formulation and an efficient approximation algorithm to estimate attribution map. Unlike the previous techniques, ours doesn't rely on fitting a surrogate model nor need any path integration of gradients. Both qualitative and quantitative experiments demonstrate a superior performance of NeFLAG in generating more faithful attribution maps than the competing methods. Our code is available at \url{https://git
    
[^134]: MPAS-O与全球漂流器数据集的动态数据同化方法

    Dynamic Data Assimilation of MPAS-O and the Global Drifter Dataset. (arXiv:2301.05551v2 [physics.ao-ph] UPDATED)

    [http://arxiv.org/abs/2301.05551](http://arxiv.org/abs/2301.05551)

    本研究提出了一种新的动态数据同化方法，将全球漂流器数据集与海洋预测模型相结合，通过利用地球系统模型的动态和模式，提高了海洋温度预测的准确性。

    

    本研究提出了一种新的方法，将现场浮标测量结果与地球系统模型（ESMs）相结合，以提高海洋温度预测的准确性。该技术利用ESMs中确定的动态和模式来改善浮标测量的准确性，同时保留季节性等特征。使用这种技术，可以纠正MPAS-O模型产生的本地温度预测误差。我们证明了我们的方法相对于其他插值和数据同化方法能够提高准确性。我们将我们的方法应用于将跨尺度海洋预测模型（MPAS-O）与全球漂流器计划的现场海洋浮标数据集同化的情况。

    In this study, we propose a new method for combining in situ buoy measurements with Earth system models (ESMs) to improve the accuracy of temperature predictions in the ocean. The technique utilizes the dynamics and modes identified in ESMs to improve the accuracy of buoy measurements while still preserving features such as seasonality. Using this technique, errors in localized temperature predictions made by the MPAS-O model can be corrected. We demonstrate that our approach improves accuracy compared to other interpolation and data assimilation methods. We apply our method to assimilate the Model for Prediction Across Scales Ocean component (MPAS-O) with the Global Drifter Program's in-situ ocean buoy dataset.
    
[^135]: gRoMA: 一种衡量深度神经网络全局鲁棒性的工具

    gRoMA: a Tool for Measuring Deep Neural Networks Global Robustness. (arXiv:2301.02288v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.02288](http://arxiv.org/abs/2301.02288)

    gRoMA是一种衡量DNN全局鲁棒性的创新工具，采用概率验证方法评估特定输出类别遭受到对抗性输入的概率。该工具可运行于预训练的黑盒分类模型上，并对整个模型和每个输入样本产生鲁棒性测量结果。

    

    深度神经网络（DNN）是前沿技术的代表，在各种复杂任务中取得了显著的表现。然而，将它们应用于安全关键系统（如航空或汽车领域）时，由于对抗性输入（即可能导致DNN犯错的输入扰动）的威胁，存在重大挑战。多项研究表明即便是现代DNN也容易受到对抗性输入的影响，因此必须测量并降低这种风险才能在安全关键系统中部署DNN。在这里，我们提出了一种创新且可扩展的工具gRoMA（全局鲁棒性测量和评估），它实现了一种概率验证方法来测量DNN的全局分类鲁棒性。具体而言，gRoMA测量特定输出类别遇到对抗性输入的概率。我们的工具基于预训练的黑盒分类模型，产生整个模型和每个输入样本的鲁棒性测量结果。我们通过测量多个最先进的DNN在热门图像数据集上的鲁棒性并分析结果，证明了我们的工具的有效性。

    Deep neural networks (DNNs) are at the forefront of cutting-edge technology, and have been achieving remarkable performance in a variety of complex tasks. Nevertheless, their integration into safety-critical systems, such as in the aerospace or automotive domains, poses a significant challenge due to the threat of adversarial inputs: perturbations in inputs that might cause the DNN to make grievous mistakes. Multiple studies have demonstrated that even modern DNNs are susceptible to adversarial inputs; and this risk must thus be measured and mitigated to allow the deployment of DNNs in safety-critical systems.  Here, we present gRoMA (global Robustness Measurement and Assessment), an innovative and scalable tool that implements a probabilistic verification approach to measure the global categorial robustness of a DNN. Specifically, gRoMA measures the probability of encountering adversarial inputs for a specific output category. Our tool operates on pre-trained, black-box classification
    
[^136]: 关于注意力网络的可解释性研究

    On the Interpretability of Attention Networks. (arXiv:2212.14776v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.14776](http://arxiv.org/abs/2212.14776)

    本文研究注意力网络的可解释性，提出了选择依赖分类（SDC）变体分类问题，并演示了注意力模型可以准确无误但不具有可解释性的多种错误模式。该研究为评估SDC模型及其解释性提供了一种评估指标，并评估了不同架构的模型的解释性。

    

    注意力机制是多个成功深度学习架构的核心组件，基于一个关键想法：“输出仅取决于输入的一个小（但未知）部分”。在图像字幕和语言翻译等多个实际应用中，这通常是正确的。在具有注意力机制的训练模型中，编码与输出相关的输入段的中间模块的输出通常被用作窥视网络“推理”的一种方式。本文通过使用注意力模型体系结构解决一个变体分类问题，我们称之为选择依赖分类（SDC），从而更加清晰地阐述了这种概念。在这种情况下，我们演示了多种错误模式，其中注意力模型可以准确无误但不具有可解释性，并表明这种模型因训练而出现。我们还阐述了可以加强和减轻此行为的多种情况。最后，我们使用我们的目标定义了一种对于SDC模型及其解释性的评估指标，并评估了不同架构的模型的解释性。

    Attention mechanisms form a core component of several successful deep learning architectures, and are based on one key idea: ''The output depends only on a small (but unknown) segment of the input.'' In several practical applications like image captioning and language translation, this is mostly true. In trained models with an attention mechanism, the outputs of an intermediate module that encodes the segment of input responsible for the output is often used as a way to peek into the `reasoning` of the network. We make such a notion more precise for a variant of the classification problem that we term selective dependence classification (SDC) when used with attention model architectures. Under such a setting, we demonstrate various error modes where an attention model can be accurate but fail to be interpretable, and show that such models do occur as a result of training. We illustrate various situations that can accentuate and mitigate this behaviour. Finally, we use our objective def
    
[^137]: 深度线性网络的贝叶斯插值

    Bayesian Interpolation with Deep Linear Networks. (arXiv:2212.14457v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2212.14457](http://arxiv.org/abs/2212.14457)

    本文在线性网络的情况下，使用贝叶斯推理找到了预测后验和贝叶斯模型证据的非渐近表达，并通过这些表达式得到深度、宽度和数据集大小的联合作用的新图像，同时证明了线性网络在无限深度时提供了可证明的最优预测，并推导了有限网络的尖锐大偏差边界。

    

    在深度学习理论中，表征神经网络的深度、宽度和数据集大小如何共同影响模型质量是一个核心问题。我们在线性网络的特殊情况下，使用具有高斯权重先验和平均平方误差的贝叶斯推理对单输出维度进行了完整的解决方案。对于任何训练数据集、网络深度和隐藏层宽度，我们找到了预测后验和贝叶斯模型证据的非渐近表达，这些表达式是一类关于Meijer-G函数的亚纯特殊函数。通过这些Meijer-G函数的新型渐近展开，我们得到了深度、宽度和数据集大小的联合作用的丰富新图像。我们表明，线性网络在无限深度时可以提供可证明的最优预测：具有数据不可知先验的无限深度线性网络的后验概率与具有最大化数据依赖先验信息的浅网络的后验概率相同，且后验概率集中于线性函数。当网络是有限的时，我们还推导了后验距离线性函数的尖锐大偏差边界，并表明这些边界以高度错综复杂的方式取决于网络深度、宽度和数据集大小。最后，在大数据集极限下提供了完整的贝叶斯模型证据的渐近展开，并证明了对于固定宽度，证据是深度和数据集大小的多项式。

    Characterizing how neural network depth, width, and dataset size jointly impact model quality is a central problem in deep learning theory. We give here a complete solution in the special case of linear networks with output dimension one trained using zero noise Bayesian inference with Gaussian weight priors and mean squared error as a negative log-likelihood. For any training dataset, network depth, and hidden layer widths, we find non-asymptotic expressions for the predictive posterior and Bayesian model evidence in terms of Meijer-G functions, a class of meromorphic special functions of a single complex variable. Through novel asymptotic expansions of these Meijer-G functions, a rich new picture of the joint role of depth, width, and dataset size emerges. We show that linear networks make provably optimal predictions at infinite depth: the posterior of infinitely deep linear networks with data-agnostic priors is the same as that of shallow networks with evidence-maximizing data-depe
    
[^138]: 关于联邦超参数调整中的噪声评估

    On Noisy Evaluation in Federated Hyperparameter Tuning. (arXiv:2212.08930v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.08930](http://arxiv.org/abs/2212.08930)

    本论文为联邦超参数调整中的噪声评估问题提供了第一次系统研究，发现即使是小量的噪声也会显著影响现有的方法，提出了一种利用公共代理数据来提高评估信号的简单有效方法。

    

    超参数调整对于联邦学习应用的成功至关重要。然而，在联邦网络中适当地选择超参数是具有挑战性的。规模、隐私和异构性等问题会导致调整过程中产生噪声，并使评估各种超参数的性能变得困难。在这项工作中，我们进行了关于联邦超参数调整中噪声评估影响的第一次系统研究。我们首先确定并严格探索关键噪声源，包括客户端子采样、数据和系统的异构性以及数据隐私。令人惊讶的是，我们的结果表明，即使是小量的噪声也会显著影响调整方法，将尖端方法的性能降低到幼稚的基线水平。针对此类情况中的噪声评估问题，我们提出了一种简单有效的方法，利用公共代理数据来提高评估信号。我们的研究为解决联邦超参数调整中的噪声评估问题提供了一般性挑战、基线和解决方案。

    Hyperparameter tuning is critical to the success of federated learning applications. Unfortunately, appropriately selecting hyperparameters is challenging in federated networks. Issues of scale, privacy, and heterogeneity introduce noise in the tuning process and make it difficult to evaluate the performance of various hyperparameters. In this work, we perform the first systematic study on the effect of noisy evaluation in federated hyperparameter tuning. We first identify and rigorously explore key sources of noise, including client subsampling, data and systems heterogeneity, and data privacy. Surprisingly, our results indicate that even small amounts of noise can significantly impact tuning methods-reducing the performance of state-of-the-art approaches to that of naive baselines. To address noisy evaluation in such scenarios, we propose a simple and effective approach that leverages public proxy data to boost the evaluation signal. Our work establishes general challenges, baselines
    
[^139]: 多语言翻译中干扰的原因和解决方法探究

    Causes and Cures for Interference in Multilingual Translation. (arXiv:2212.07530v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.07530](http://arxiv.org/abs/2212.07530)

    研究探究了多语言机器翻译中干扰的主要因素，通过系统化试验发现使用不到10亿参数的标准Transformer配置可以在很大程度上缓解干扰并促进协同，同时发现调整采样温度以控制数据中每个语言对所占比例的方法是平衡语言对之间关系的关键。

    

    多语言机器翻译模型可以从不同语言对之间的协同中获益，但同时也会受到干扰的影响。虽然目前有越来越多的先进方法旨在消除干扰，但我们对干扰现象的理解仍然有限。本研究确定了导致多语言机器翻译中干扰的主要因素。通过系统化试验，我们发现干扰（或协同）主要由模型大小、数据大小和每个语言对在总数据集中所占比例来决定。我们观察到，当模型相对于可用的训练数据非常小的时候，会出现严重的干扰，而使用不到10亿参数的标准Transformer配置可以在很大程度上缓解干扰并促进协同。此外，我们还展示了通过调整采样温度以控制数据中每个语言对所占比例的方法是平衡语言对之间关系的关键。

    Multilingual machine translation models can benefit from synergy between different language pairs, but also suffer from interference. While there is a growing number of sophisticated methods that aim to eliminate interference, our understanding of interference as a phenomenon is still limited. This work identifies the main factors that contribute to interference in multilingual machine translation. Through systematic experimentation, we find that interference (or synergy) are primarily determined by model size, data size, and the proportion of each language pair within the total dataset. We observe that substantial interference occurs mainly when the model is very small with respect to the available training data, and that using standard transformer configurations with less than one billion parameters largely alleviates interference and promotes synergy. Moreover, we show that tuning the sampling temperature to control the proportion of each language pair in the data is key to balancin
    
[^140]: 区块链上的AI伦理: 基于Twitter数据的区块链安全主题分析

    AI Ethics on Blockchain: Topic Analysis on Twitter Data for Blockchain Security. (arXiv:2212.06951v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2212.06951](http://arxiv.org/abs/2212.06951)

    本研究分析了Twitter上与MEV相关的话题，结果表明推文讨论了深刻的伦理问题，包括安全、公平、情感情绪和对MEV解决方案的渴望。

    

    区块链使用分布式网络让计算机系统更安全，但当前的区块链设计在交易顺序方面存在公平性问题。矿工可以重新排序交易以生成利润，这被称为矿工可提取价值（MEV）问题。现有研究认为MEV是一个严重的安全问题，并提出了潜在的解决方案，包括知名的Flashbots。然而，以往的研究大多分析了区块链数据，这可能无法捕捉到MEV在更广泛的AI社会中的影响。因此，在这项研究中，我们应用自然语言处理（NLP）方法全面分析了MEV推文中的话题。我们收集了超过20,000个MEV和Flashbots标签的推文并分析了它们的话题。我们的结果显示，这些推文讨论了深刻的伦理问题，包括安全、公平、情感情绪和对MEV解决方案的渴望。我们还发现了区块链上MEV活动的共同运动。

    Blockchain has empowered computer systems to be more secure using a distributed network. However, the current blockchain design suffers from fairness issues in transaction ordering. Miners are able to reorder transactions to generate profits, the so-called miner extractable value (MEV). Existing research recognizes MEV as a severe security issue and proposes potential solutions, including prominent Flashbots. However, previous studies have mostly analyzed blockchain data, which might not capture the impacts of MEV in a much broader AI society. Thus, in this research, we applied natural language processing (NLP) methods to comprehensively analyze topics in tweets on MEV. We collected more than 20000 tweets with MEV and Flashbots hashtags and analyzed their topics. Our results show that the tweets discussed profound topics of ethical concern, including security, equity, emotional sentiments, and the desire for solutions to MEV. We also identify the co-movements of MEV activities on block
    
[^141]: 论解释与预测的关系：一种因果视角

    On the Relationship Between Explanation and Prediction: A Causal View. (arXiv:2212.06925v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.06925](http://arxiv.org/abs/2212.06925)

    本篇论文用因果推断的方法系统地评估了解释与预测的关系，结果表明这种关系远不如理想情况。

    

    提供模型决策解释的能力成为了机器学习模型开发、部署和应用的核心要求。然而，我们尚未理解解释方法的优缺点。数据、模型预测、超参数和随机初始化等上游因素如何影响下游的解释？虽然先前的研究提出了解释与预测之间关系较小的担忧，但缺乏确定性的研究来量化这种关系。我们的工作借鉴因果推断的方法系统地评估了这种关系。具体而言，我们通过干预解释和预测的因果祖先，在使用以显眼度为基础的解释或预测时对超参数和输入进行测量，来研究解释和预测之间的关系。我们的研究结果表明，解释和预测之间的关系远非理想。事实上，“理想”情况下的差距只会在更高的情况下增加。

    Being able to provide explanations for a model's decision has become a central requirement for the development, deployment, and adoption of machine learning models. However, we are yet to understand what explanation methods can and cannot do. How do upstream factors such as data, model prediction, hyperparameters, and random initialization influence downstream explanations? While previous work raised concerns that explanations (E) may have little relationship with the prediction (Y), there is a lack of conclusive study to quantify this relationship. Our work borrows tools from causal inference to systematically assay this relationship. More specifically, we study the relationship between E and Y by measuring the treatment effect when intervening on their causal ancestors, i.e., on hyperparameters and inputs used to generate saliency-based Es or Ys. Our results suggest that the relationships between E and Y is far from ideal. In fact, the gap between 'ideal' case only increase in higher
    
[^142]: 多智能体网络系统中的可扩展和高效分布式政策梯度算法

    Scalable and Sample Efficient Distributed Policy Gradient Algorithms in Multi-Agent Networked Systems. (arXiv:2212.06357v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2212.06357](http://arxiv.org/abs/2212.06357)

    本文介绍了一种分布式政策梯度算法，用于解决具有奖励耦合的多智能体强化学习问题，具有广泛的应用价值。

    

    本文研究了一类多智能体强化学习（MARL）问题，其中代理接收到的奖励取决于其他代理的状态，但下一个状态仅取决于代理自己的当前状态和动作。我们称其为REC-MARL，代表奖励耦合多智能体强化学习。REC-MARL具有一系列重要应用，如无线网络中的实时访问控制和分布式功率控制。本文提出了一种针对REC-MARL的分布式政策梯度算法。该算法在两个方面上是分布式的：（i）学习的策略是分布式策略，将代理的本地状态映射到其本地动作，（ii）训练是分布式的，每个代理基于自己和邻居的信息更新其策略。所学算法实现了一个稳定策略，其迭代复杂度边界取决于本地状态和动作的维数。实验结果表明了我们算法的有效性。

    This paper studies a class of multi-agent reinforcement learning (MARL) problems where the reward that an agent receives depends on the states of other agents, but the next state only depends on the agent's own current state and action. We name it REC-MARL standing for REward-Coupled Multi-Agent Reinforcement Learning. REC-MARL has a range of important applications such as real-time access control and distributed power control in wireless networks. This paper presents a distributed policy gradient algorithm for REC-MARL. The proposed algorithm is distributed in two aspects: (i) the learned policy is a distributed policy that maps a local state of an agent to its local action and (ii) the learning/training is distributed, during which each agent updates its policy based on its own and neighbors' information. The learned algorithm achieves a stationary policy and its iterative complexity bounds depend on the dimension of local states and actions. The experimental results of our algorithm
    
[^143]: 基于复高斯混合模型的深度语音增强中的不确定性估计

    Uncertainty Estimation in Deep Speech Enhancement Using Complex Gaussian Mixture Models. (arXiv:2212.04831v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2212.04831](http://arxiv.org/abs/2212.04831)

    本研究提出了一种基于复高斯混合模型的深度语音增强方法，可以估计干净语音的完整后验分布，而不是单一的乘性掩码。这种方法在准确性和预测不确定性的量化方面都表现优异，超越了现有的单通道语音增强方法。

    

    单通道深度语音增强方法通常估计单一的乘性掩码以提取干净的语音信号，但缺乏其准确性的度量。本研究提出在神经网络语音增强中量化与干净语音估计相关的不确定性。预测不确定性通常分为先验不确定性和模型不确定性。前者解释了数据固有的不确定性，后者则对应模型不确定性。为了实现鲁棒的干净语音估计和高效的预测不确定性量化，我们提出将统计复高斯混合模型（CGMM）集成到深度语音增强框架中。具体而言，我们通过条件概率密度函数随机建模输入和输出之间的依赖关系，并训练神经网络将噪声输入映射到干净语音的完整后验分布，该分布建模为多个复高斯分布的混合。在CHiME-5数据集上的实验结果表明，我们的方法优于现有的单通道语音增强方法，并提供了更可靠的预测干净语音相关的不确定性的度量。

    Single-channel deep speech enhancement approaches often estimate a single multiplicative mask to extract clean speech without a measure of its accuracy. Instead, in this work, we propose to quantify the uncertainty associated with clean speech estimates in neural network-based speech enhancement. Predictive uncertainty is typically categorized into aleatoric uncertainty and epistemic uncertainty. The former accounts for the inherent uncertainty in data and the latter corresponds to the model uncertainty. Aiming for robust clean speech estimation and efficient predictive uncertainty quantification, we propose to integrate statistical complex Gaussian mixture models (CGMMs) into a deep speech enhancement framework. More specifically, we model the dependency between input and output stochastically by means of a conditional probability density and train a neural network to map the noisy input to the full posterior distribution of clean speech, modeled as a mixture of multiple complex Gauss
    
[^144]: 连续学习的统计力学:变分原理和平均场势

    Statistical mechanics of continual learning: variational principle and mean-field potential. (arXiv:2212.02846v3 [cond-mat.stat-mech] UPDATED)

    [http://arxiv.org/abs/2212.02846](http://arxiv.org/abs/2212.02846)

    从物理学的角度将连续学习的问题转化为Franz-Parisi热力学势的框架，将之前学习到的任务作为先验和参考，提出了一个在场空间中训练神经网络的变分贝叶斯学习设置，用于调节任务间的突触资源。

    

    人工智能通用性的一个障碍是多种不同任务的连续学习。最近，涉及机器学习和神经科学的各种启发性技巧被提出，但它们缺乏一个统一的理论基础。本文关注二元权重的单层和多层神经网络的连续学习。提出了一个变分贝叶斯学习设置，其中神经网络在场空间而不是渐变未定义的离散权重空间中进行训练，并且自然地将权重不确定性合并，并调节任务间的突触资源。从物理学的角度，我们将变分的连续学习转化为Franz-Parisi热力学势的框架，其中以前的任务知识充当先验和参考。因此，我们将一个教师-学生设置中的二元感知器的连续学习解释为一个Franz-Parisi势。

    An obstacle to artificial general intelligence is set by the continual learning of multiple tasks of different nature. Recently, various heuristic tricks, both from machine learning and from neuroscience angles, were proposed, but they lack a unified theory ground. Here, we focus on the continual learning in single-layered and multi-layered neural networks of binary weights. A variational Bayesian learning setting is thus proposed, where the neural network is trained in a field-space, rather than the gradient-ill-defined discrete-weight space, and furthermore, the weight uncertainty is naturally incorporated, and modulates the synaptic resources among tasks. From a physics perspective, we translate the variational continual learning into the Franz-Parisi thermodynamic potential framework, where the previous task knowledge acts as a prior and a reference as well. We thus interprete the continual learning of the binary perceptron in a teacher-student setting as a Franz-Parisi potential c
    
[^145]: 相对过度泛化的课程学习

    Curriculum Learning for Relative Overgeneralization. (arXiv:2212.02733v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.02733](http://arxiv.org/abs/2212.02733)

    本论文提出了一种名为相对过度泛化的课程学习（CURO）的新算法来解决多智能体强化学习中存在的相对过度泛化 (RO) 问题，该方法在解决展示强RO的合作任务方面具有很好的表现。

    

    在多智能体强化学习 (MARL) 中，许多流行方法如 VDN 和 QMIX，都容易受到相对过度泛化 (RO) 这一关键性的多智能体病理的影响。当合作任务中最佳联合行动的效用低于次优联合行动时，就会出现RO。RO可能导致智能体陷入局部最优解或无法解决需要智能体之间在给定时间步长内进行大量协调的合作任务。最近的基于价值的MARL算法，如QPLEX和WQMIX可以在一定程度上克服RO。然而，我们的实验结果表明，它们仍然无法解决展示强RO的合作任务。在这项工作中，我们提出了一种称为相对过度泛化的课程学习（CURO）的新方法，以更好地克服RO。在CURO中，我们首先微调目标任务的奖励函数以生成适合当前能力的源任务来解决展示强RO的目标任务。

    In multi-agent reinforcement learning (MARL), many popular methods, such as VDN and QMIX, are susceptible to a critical multi-agent pathology known as relative overgeneralization (RO), which arises when the optimal joint action's utility falls below that of a sub-optimal joint action in cooperative tasks. RO can cause the agents to get stuck into local optima or fail to solve cooperative tasks that require significant coordination between agents within a given timestep. Recent value-based MARL algorithms such as QPLEX and WQMIX can overcome RO to some extent. However, our experimental results show that they can still fail to solve cooperative tasks that exhibit strong RO. In this work, we propose a novel approach called curriculum learning for relative overgeneralization (CURO) to better overcome RO. To solve a target task that exhibits strong RO, in CURO, we first fine-tune the reward function of the target task to generate source tasks that are tailored to the current ability of the 
    
[^146]: 物理学信解的基于模型的强化学习

    Physics-Informed Model-Based Reinforcement Learning. (arXiv:2212.02179v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.02179](http://arxiv.org/abs/2212.02179)

    本文介绍了一种应用于机器人任务的基于模型的强化学习算法，利用环境模型生成轨迹来更新策略，其中使用深度神经网络动态模型的性能比使用更准确的p型混合自动编码器动态模型的性能要差。

    

    本文将强化学习算法（RL）应用于机器人任务。传统RL算法的一个缺点是样本效率较低。提高样本效率的一种方法是基于模型的RL。在我们的基于模型的RL算法中，我们学习环境的模型，主要是其转换动态和奖励函数，利用其生成虚拟轨迹并通过其反向传播来更新策略，利用模型的可微性。直观地说，学习更准确的模型应该会导致更好的基于模型的RL性能。近年来，人们越来越关注利用底层物理结构开发更好的基于深度神经网络的物理系统动态模型。我们专注于没有联系的刚体运动的机器人系统。我们比较了我们基于模型的RL算法的两个版本，一个使用标准的基于深度神经网络的动态模型，另一个使用更准确的p型混合自动编码器动态模型。

    We apply reinforcement learning (RL) to robotics tasks. One of the drawbacks of traditional RL algorithms has been their poor sample efficiency. One approach to improve the sample efficiency is model-based RL. In our model-based RL algorithm, we learn a model of the environment, essentially its transition dynamics and reward function, use it to generate imaginary trajectories and backpropagate through them to update the policy, exploiting the differentiability of the model. Intuitively, learning more accurate models should lead to better model-based RL performance. Recently, there has been growing interest in developing better deep neural network based dynamics models for physical systems, by utilizing the structure of the underlying physics. We focus on robotic systems undergoing rigid body motion without contacts. We compare two versions of our model-based RL algorithm, one which uses a standard deep neural network based dynamics model and the other which uses a much more accurate, p
    
[^147]: 使用端口-哈密顿神经网络进行动态系统模型的组合学习

    Compositional Learning of Dynamical System Models Using Port-Hamiltonian Neural Networks. (arXiv:2212.00893v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.00893](http://arxiv.org/abs/2212.00893)

    本论文提出了一种组合学习动态系统模型的方法，使用端口-哈密顿神经网络来训练和组合子模型，从而实现模块化学习，避免需要更多复合系统数据的问题。

    

    许多动态系统都涉及许多相互作用的子系统，从与其周围环境交互的机器人到大规模的多物理系统。为了从数据中学习这种复合系统的模型，我们提出了一种组合神经网络框架，以及用于训练这些模型的算法，以及一种组合已学习模型的方法。最终结果是一种模块化的学习方法：神经网络子模型是在相对简单的子系统生成的轨迹数据上训练的，然后预测更复杂的复合系统的动力学，而不需要要求复合系统本身生成额外的数据。

    Many dynamical systems -- from robots interacting with their surroundings to large-scale multiphysics systems -- involve a number of interacting subsystems. Toward the objective of learning composite models of such systems from data, we present i) a framework for compositional neural networks, ii) algorithms to train these models, iii) a method to compose the learned models, iv) theoretical results that bound the error of the resulting composite models, and v) a method to learn the composition itself, when it is not known a priori. The end result is a modular approach to learning: neural network submodels are trained on trajectory data generated by relatively simple subsystems, and the dynamics of more complex composite systems are then predicted without requiring additional data generated by the composite systems themselves. We achieve this compositionality by representing the system of interest, as well as each of its subsystems, as a port-Hamiltonian neural network (PHNN) -- a class
    
[^148]: 基于李群力学变分积分器网络的机器人系统学习与控制

    Lie Group Forced Variational Integrator Networks for Learning and Control of Robot Systems. (arXiv:2211.16006v4 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2211.16006](http://arxiv.org/abs/2211.16006)

    本文提出了一种新的深度学习架构，LieFVIN，通过对动力学模型的结构特性进行建模，可以学习机器人系统的控制Lagrangian或Hamiltonian动力学，并在各种模拟机器人上得到了验证。

    

    把动力学系统的物理定律和结构特性应用于深度学习体系结构的设计已被证明是提高其计算效率和概括能力的强有力的技术。学习精确的机器人动力学模型对于安全和稳定控制非常关键。本文提出了一种新的结构保持的深度学习架构，即Lie Group Forced Variational Integrator Network (LieFVIN)，能够从位置-速度或仅位置数据中学习控制的Lagrangian或Hamiltonian动力学。通过设计，LieFVINs能够保持动力学演变的李群结构和底层的Hamiltonian或Lagrangian系统中的辛结构。所提出的架构可用于机器人系统的模拟与实际控制。我们在各种模拟机器人系统上展示了该方法的有效性，包括四轴飞行器、水下滑翔机和人形机器人。

    Incorporating prior knowledge of physics laws and structural properties of dynamical systems into the design of deep learning architectures has proven to be a powerful technique for improving their computational efficiency and generalization capacity. Learning accurate models of robot dynamics is critical for safe and stable control. Autonomous mobile robots, including wheeled, aerial, and underwater vehicles, can be modeled as controlled Lagrangian or Hamiltonian rigid-body systems evolving on matrix Lie groups. In this paper, we introduce a new structure-preserving deep learning architecture, the Lie group Forced Variational Integrator Network (LieFVIN), capable of learning controlled Lagrangian or Hamiltonian dynamics on Lie groups, either from position-velocity or position-only data. By design, LieFVINs preserve both the Lie group structure on which the dynamics evolve and the symplectic structure underlying the Hamiltonian or Lagrangian systems of interest. The proposed architectu
    
[^149]: PipeFisher: 利用管道并结合Fisher信息矩阵的高效大语言模型训练方法

    PipeFisher: Efficient Training of Large Language Models Using Pipelining and Fisher Information Matrices. (arXiv:2211.14133v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14133](http://arxiv.org/abs/2211.14133)

    PipeFisher提出了一种新的方法，将二阶优化方法K-FAC的工作分配给管道间隙，以加速收敛和提高大语言模型训练的有效性。

    

    管道并行技术可以在大规模分布式加速器集群上高效地训练大型语言模型。然而，在启动和关闭期间的管道间隙会降低加速器的利用率。虽然已经提出了一些高效的管道方案来最大化利用率，如微批处理和双向管道，但仍有大量间隙无法使用同步前向和后向传递填充。为了解决这个问题，我们建议将额外的工作分配给管道间隙，在大语言模型训练中获得辅助效益。作为这种方向的一个例子，我们提出了PipeFisher，它将基于Fisher信息矩阵的二阶优化方法K-FAC的工作分配给管道间隙，加速收敛。在BERT-Base和-Large模型的第一阶段预训练中，PipeFisher将（模拟的）训练时间缩短了50-75％，相比于使用一阶优化器，大大提高了加速器的利用率并获得更好的训练效果。

    Pipeline parallelism enables efficient training of Large Language Models (LLMs) on large-scale distributed accelerator clusters. Yet, pipeline bubbles during startup and tear-down reduce the utilization of accelerators. Although efficient pipeline schemes with micro-batching and bidirectional pipelines have been proposed to maximize utilization, a significant number of bubbles cannot be filled using synchronous forward and backward passes. To address this problem, we suggest that extra work be assigned to the bubbles to gain auxiliary benefits in LLM training. As an example in this direction, we propose PipeFisher, which assigns the work of K-FAC, a second-order optimization method based on the Fisher information matrix, to the bubbles to accelerate convergence. In Phase 1 pretraining of BERT-Base and -Large models, PipeFisher reduces the (simulated) training time to 50-75% compared to training with a first-order optimizer by greatly improving the accelerator utilization and benefiting
    
[^150]: 过去的事情很重要：高斯过程模型轨迹预测中后续状态的相关性

    The Past Does Matter: Correlation of Subsequent States in Trajectory Predictions of Gaussian Process Models. (arXiv:2211.11103v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.11103](http://arxiv.org/abs/2211.11103)

    高斯过程模型中，对预测轨迹的后续状态之间独立性的假设是错误的，本文提出了一种新的高斯过程分段线性近似方法来缓解这个问题。

    

    计算高斯过程模型中动态系统的轨迹分布是一个重要的挑战。在考虑到基于采样的方法的计算成本之后，我们考虑了模型输出和轨迹分布的近似方法。我们发现之前关于不确定性传播的工作，重点放在离散状态空间模型上，错误地包含了对预测轨迹的后续状态之间独立性的假设。将这些思想扩展到连续的常微分方程模型上，我们展示了这个假设的含义，并提出了一种新的高斯过程分段线性近似方法来缓解这个问题。

    Computing the distribution of trajectories from a Gaussian Process model of a dynamical system is an important challenge in utilizing such models. Motivated by the computational cost of sampling-based approaches, we consider approximations of the model's output and trajectory distribution. We show that previous work on uncertainty propagation, focussed on discrete state-space models, incorrectly included an independence assumption between subsequent states of the predicted trajectories. Expanding these ideas to continuous ordinary differential equation models, we illustrate the implications of this assumption and propose a novel piecewise linear approximation of Gaussian Processes to mitigate them.
    
[^151]: 带有卷积高斯神经过程的环境传感器放置

    Environmental Sensor Placement with Convolutional Gaussian Neural Processes. (arXiv:2211.10381v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.10381](http://arxiv.org/abs/2211.10381)

    本论文提出了一种新的方式——卷积高斯神经过程（ConvGNP），用于提高环境传感器的放置效率。ConvGNP使用神经网络来参数化联合高斯分布，通过学习空间和季节性非平稳性，优于传统的非平稳高斯过程模型。

    

    环境传感器对于监测天气和气候变化的影响至关重要。然而，在像南极这样的偏远地区，最大化测量信息和有效放置传感器是具有挑战性的。概率机器学习模型可以通过预测新传感器提供的不确定性减少来评估放置信息。高斯过程模型广泛用于此目的，但难以捕捉复杂的非平稳行为并缩放到大型数据集。本文提出使用卷积高斯神经过程（ConvGNP）来解决这些问题。ConvGNP使用神经网络来参数化任意目标位置的联合高斯分布，实现了灵活性和可扩展性。使用模拟的南极地区地面温度异常作为真实数据，ConvGNP学习了空间和季节性非平稳性，并优于非平稳GP基线。在模拟的s中，

    Environmental sensors are crucial for monitoring weather conditions and the impacts of climate change. However, it is challenging to maximise measurement informativeness and place sensors efficiently, particularly in remote regions like Antarctica. Probabilistic machine learning models can evaluate placement informativeness by predicting the uncertainty reduction provided by a new sensor. Gaussian process (GP) models are widely used for this purpose, but they struggle with capturing complex non-stationary behaviour and scaling to large datasets. This paper proposes using a convolutional Gaussian neural process (ConvGNP) to address these issues. A ConvGNP uses neural networks to parameterise a joint Gaussian distribution at arbitrary target locations, enabling flexibility and scalability. Using simulated surface air temperature anomaly over Antarctica as ground truth, the ConvGNP learns spatial and seasonal non-stationarities, outperforming a non-stationary GP baseline. In a simulated s
    
[^152]: 自主船舶的时空循环强化学习

    Spatial-temporal recurrent reinforcement learning for autonomous ships. (arXiv:2211.01004v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01004](http://arxiv.org/abs/2211.01004)

    本文提出了一个时空循环神经网络架构，通过考虑 COLREG 规则和引入最先进的碰撞风险度量方法，实现了针对自主船舶的深度 Q 网络。在真实环境和模拟环境中验证后，所提出的方法表现出在海上路径规划中的潜力并具有鲁棒性。

    

    本文提出了一个空间-时间循环神经网络架构，可用于驾驶自主船舶的深度 Q 网络。该网络设计使得可以处理任意数量的周围目标船只，并对局部观测具有鲁棒性。此外，提出了一种最先进的碰撞风险度量方法，以便智能体更容易评估不同情况。海上交通 COLREG 规则明确地考虑在奖励函数设计中。最终策略在自定义的 Around the Clock 单船遭遇问题集和包括 18 个多船情景的常用 Imazu (1987) 问题中进行了验证。通过与人工势场和速度障碍方法的性能比较表明了所提出方法在海上路径规划中的潜力。此外，新架构在多智能体场景中的部署也表现出鲁棒性。

    This paper proposes a spatial-temporal recurrent neural network architecture for deep $Q$-networks that can be used to steer an autonomous ship. The network design makes it possible to handle an arbitrary number of surrounding target ships while offering robustness to partial observability. Furthermore, a state-of-the-art collision risk metric is proposed to enable an easier assessment of different situations by the agent. The COLREG rules of maritime traffic are explicitly considered in the design of the reward function. The final policy is validated on a custom set of newly created single-ship encounters called `Around the Clock' problems and the commonly used Imazu (1987) problems, which include 18 multi-ship scenarios. Performance comparisons with artificial potential field and velocity obstacle methods demonstrate the potential of the proposed approach for maritime path planning. Furthermore, the new architecture exhibits robustness when it is deployed in multi-agent scenarios and
    
[^153]: 基于深度强化学习的自适应大邻域搜索算法在线控制

    Online Control of Adaptive Large Neighborhood Search using Deep Reinforcement Learning. (arXiv:2211.00759v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.00759](http://arxiv.org/abs/2211.00759)

    本研究提出一种基于深度强化学习的方法来在线控制自适应大邻域搜索算法，该方法能够自适应选择启发式策略、调整参数和控制接受标准，以获得优化问题的良好解，对应用组合优化问题中的实际问题具有重要意义。

    

    自适应大邻域搜索（ALNS）算法在解决复杂的组合优化问题（COPs）方面取得了相当的成功。ALNS在搜索过程中自适应地选择各种启发式策略，利用它们的优势来找到优化问题的良好解。然而，ALNS的有效性取决于其选择和接受参数的正确配置。为了解决这个限制，我们提出了一种基于深度强化学习（DRL）的方法，在搜索过程中选择启发式、调整参数和控制接受标准。所提出的方法旨在基于搜索的状态学习如何配置下一次ALNS迭代以获得好的优化问题解。我们在一个时间依赖的含有随机权重和时间窗口的导航问题上评估了所提出的方法，该问题用于IJCAI竞赛。结果表明，我们的方法优于普通的ALNS和具有默认参数设置的ALNS，展示了DRL方法在在线控制ALNS方面的有效性。

    The Adaptive Large Neighborhood Search (ALNS) algorithm has shown considerable success in solving complex combinatorial optimization problems (COPs). ALNS selects various heuristics adaptively during the search process, leveraging their strengths to find good solutions for optimization problems. However, the effectiveness of ALNS depends on the proper configuration of its selection and acceptance parameters. To address this limitation, we propose a Deep Reinforcement Learning (DRL) approach that selects heuristics, adjusts parameters, and controls the acceptance criteria during the search process. The proposed method aims to learn, based on the state of the search, how to configure the next iteration of the ALNS to obtain good solutions to the underlying optimization problem. We evaluate the proposed method on a time-dependent orienteering problem with stochastic weights and time windows, used in an IJCAI competition. The results show that our approach outperforms vanilla ALNS and ALNS
    
[^154]: 分析深度学习对车内实时LiDAR感知的点云表示

    Analyzing Deep Learning Representations of Point Clouds for Real-Time In-Vehicle LiDAR Perception. (arXiv:2210.14612v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.14612](http://arxiv.org/abs/2210.14612)

    本文提出了一种新的LiDAR点云表示的计算分类方法，通过迁移学习分析了不同网络架构在LiDAR点云分类和分割任务上的性能。实验结果表明，它可以帮助更好地理解和设计用于LiDAR点云的深度学习表示，提高自动驾驶的实时感知性能。

    

    LiDAR传感器作为自动驾驶汽车的重要组成部分，能够提供准确、高分辨率的车辆周围三维信息。然而，利用多个高分辨率LiDAR传感器的数据量逐渐增加，其计算难度也随之增加，这样的点云需要实时处理并从中提取语义信息。影响基于这些点云的深度神经网络运行性能和准确度的一个重要因素是其底层数据表示方式及计算方法。本文提出了一种新的LiDAR点云表示的计算分类方法，通过迁移学习分析了不同网络架构在LiDAR点云分类和分割任务上的性能。实验结果表明，我们提出的计算分类可以更好地理解和设计用于LiDAR点云的深度学习表示，提高自动驾驶的实时感知性能。

    LiDAR sensors are an integral part of modern autonomous vehicles as they provide an accurate, high-resolution 3D representation of the vehicle's surroundings. However, it is computationally difficult to make use of the ever-increasing amounts of data from multiple high-resolution LiDAR sensors. As frame-rates, point cloud sizes and sensor resolutions increase, real-time processing of these point clouds must still extract semantics from this increasingly precise picture of the vehicle's environment. One deciding factor of the run-time performance and accuracy of deep neural networks operating on these point clouds is the underlying data representation and the way it is computed. In this work, we examine the relationship between the computational representations used in neural networks and their performance characteristics. To this end, we propose a novel computational taxonomy of LiDAR point cloud representations used in modern deep neural networks for 3D point cloud processing. Using t
    
[^155]: 紧凑的神经网络验证中的抽象查询

    Tighter Abstract Queries in Neural Network Verification. (arXiv:2210.12871v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12871](http://arxiv.org/abs/2210.12871)

    CEGARETTE是一种新的验证机制，可以同时对系统和属性进行抽象和细化，从而产生大小适中且足够准确的抽象网络，使得神经网络验证更加快速高效。

    

    神经网络已成为计算机科学中各个领域反应性系统的关键组成部分。尽管其性能出色，但使用神经网络会带来许多风险，这些风险源于我们无法理解和推理其行为。针对这些风险，已提出了各种形式化方法来验证神经网络；但遗憾的是，这些方法通常面临可扩展性障碍。最近的尝试表明，抽象细化方法在缓解这些限制方面可能发挥重要作用；但这些方法通常会产生过于抽象的网络，使其不适合进行验证。为了解决这个问题，我们提出了CEGARETTE，一种新的验证机制，其中系统和属性同时进行抽象和细化。我们观察到，这种方法使我们能够产生既小又足够准确的抽象网络，从而实现快速验证。

    Neural networks have become critical components of reactive systems in various domains within computer science. Despite their excellent performance, using neural networks entails numerous risks that stem from our lack of ability to understand and reason about their behavior. Due to these risks, various formal methods have been proposed for verifying neural networks; but unfortunately, these typically struggle with scalability barriers. Recent attempts have demonstrated that abstraction-refinement approaches could play a significant role in mitigating these limitations; but these approaches can often produce networks that are so abstract, that they become unsuitable for verification. To deal with this issue, we present CEGARETTE, a novel verification mechanism where both the system and the property are abstracted and refined simultaneously. We observe that this approach allows us to produce abstract networks which are both small and sufficiently accurate, allowing for quick verification
    
[^156]: 利用深度学习方法消除角分辨光电子能谱中的网格结构

    Removing grid structure in angle-resolved photoemission spectra via deep learning method. (arXiv:2210.11200v2 [cond-mat.mtrl-sci] UPDATED)

    [http://arxiv.org/abs/2210.11200](http://arxiv.org/abs/2210.11200)

    本文提出了一种基于深度学习的方法，可以通过利用光谱本身的自相关信息来消除光谱中的网格结构和噪声，从而优化光谱质量，并且该方法有潜力扩展到其他光谱测量中以消除外部信号。

    

    光谱数据常常包含不必要的外部信号。例如，在ARPES实验中，通常会在CCD前放置一根金属网格以阻挡杂散的光电子，但在快速测量模式下会引起光谱中的网格结构。过去，这种结构通常是通过数学傅立叶滤波方法通过删除周期性结构来消除的。然而，该方法可能会导致光谱中信息的丢失和空缺，因为网格结构不是严格线性叠加的。在本文中，我们提出了一种深度学习方法来有效地解决这个问题。我们的方法利用光谱本身内部的自相关信息，可以同时消除网格结构和噪声，大大优化光谱质量。它有潜力扩展到所有光谱测量中，以消除其他外部信号并根据自相关性增强光谱质量。

    Spectroscopic data may often contain unwanted extrinsic signals. For example, in ARPES experiment, a wire mesh is typically placed in front of the CCD to block stray photo-electrons, but could cause a grid-like structure in the spectra during quick measurement mode. In the past, this structure was often removed using the mathematical Fourier filtering method by erasing the periodic structure. However, this method may lead to information loss and vacancies in the spectra because the grid structure is not strictly linearly superimposed. Here, we propose a deep learning method to effectively overcome this problem. Our method takes advantage of the self-correlation information within the spectra themselves and can greatly optimize the quality of the spectra while removing the grid structure and noise simultaneously. It has the potential to be extended to all spectroscopic measurements to eliminate other extrinsic signals and enhance the spectral quality based on the self-correlation of the
    
[^157]: 基于无需训练集的深度学习方法的光谱数据去噪

    Spectroscopic data de-noising via training-set-free deep learning method. (arXiv:2210.10494v2 [cond-mat.mtrl-sci] UPDATED)

    [http://arxiv.org/abs/2210.10494](http://arxiv.org/abs/2210.10494)

    本文提出了一种基于光谱自身信息的去噪方法，无需训练集，能够提取光谱的内在信息，保留了能带特征，可扩展性强。

    

    去噪在光谱后处理中起着关键作用。基于机器学习的方法在从嘈杂数据中提取内在信息方面表现出良好的性能，但通常需要高质量的训练集，而这在实际实验测量中通常是不可获得的。本文以角分辨光电子能谱（ARPES）中的光谱为例，开发了一种去噪方法，用于提取内在的光谱信息而无需训练集。这是可能的，因为我们的方法利用了光谱自身的自相关信息。它保留了内在的能带特征，从而有利于进一步的分析和处理。此外，由于我们的方法不受以前方法训练集特定属性的限制，因此它可能会扩展到在获取高质量多维训练数据具有挑战性的其他领域和应用场景。

    De-noising plays a crucial role in the post-processing of spectra. Machine learning-based methods show good performance in extracting intrinsic information from noisy data, but often require a high-quality training set that is typically inaccessible in real experimental measurements. Here, using spectra in angle-resolved photoemission spectroscopy (ARPES) as an example, we develop a de-noising method for extracting intrinsic spectral information without the need for a training set. This is possible as our method leverages the self-correlation information of the spectra themselves. It preserves the intrinsic energy band features and thus facilitates further analysis and processing. Moreover, since our method is not limited by specific properties of the training set compared to previous ones, it may well be extended to other fields and application scenarios where obtaining high-quality multidimensional training data is challenging.
    
[^158]: 加速受限制的最小-最大优化的单调用方法

    Accelerated Single-Call Methods for Constrained Min-Max Optimization. (arXiv:2210.03096v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2210.03096](http://arxiv.org/abs/2210.03096)

    本文提出了两种单调用单投影算法，分别是优化梯度方法和加速反射梯度方法，分别适用于满足弱Minty变分不等式和负共同单调性的包含问题，可以加速最小-最大优化，并分别取得了$O(\frac{1}{\sqrt{T}})$和$O(\frac{1}{T})$的收敛率。

    

    我们研究了约束最小-最大优化的一阶方法。现有方法每次迭代要求两个梯度调用或两个投影，这在某些应用中可能很昂贵。本文首先展示了一种乐观梯度方法的变种，即单次调用单次投影算法，对于满足弱Minty变分不等式（MVI）的包含问题，具有$O(\frac{1}{\sqrt{T}})$的最佳迭代收敛率。我们的第二个结果是第一个单调用单投影算法——加速反射梯度（ARG）方法，它对于满足负共同单调性的包含问题实现了最优的$O(\frac{1}{T})$后式收敛率。弱MVI和负共同单调性都是深入研究过的假设，涵盖了一组丰富的非凸非凹最小-最大优化问题。最后，我们展示另一种单调用单投影算法——反射梯度（RG）方法，具有$O(\frac{1}{\sqrt{T}})$的最佳迭代收敛率，但需要两个投影。

    We study first-order methods for constrained min-max optimization. Existing methods either require two gradient calls or two projections in each iteration, which may be costly in some applications. In this paper, we first show that a variant of the Optimistic Gradient (OG) method, a single-call single-projection algorithm, has $O(\frac{1}{\sqrt{T}})$ best-iterate convergence rate for inclusion problems with operators that satisfy the weak Minty variation inequality (MVI). Our second result is the first single-call single-projection algorithm -- the Accelerated Reflected Gradient (ARG) method that achieves the optimal $O(\frac{1}{T})$ last-iterate convergence rate for inclusion problems that satisfy negative comonotonicity. Both the weak MVI and negative comonotonicity are well-studied assumptions and capture a rich set of non-convex non-concave min-max optimization problems. Finally, we show that the Reflected Gradient (RG) method, another single-call single-projection algorithm, has $
    
[^159]: 通过缩放协方差矩阵自适应MAP模拟来训练多样化的高维度控制器

    Training Diverse High-Dimensional Controllers by Scaling Covariance Matrix Adaptation MAP-Annealing. (arXiv:2210.02622v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.02622](http://arxiv.org/abs/2210.02622)

    本论文提出了三种新的CMA-MAE变体，利用高效近似方法提高了其可扩展性，相较于ES基线在基准测试中表现更好，并且达到或超过最先进的深度强化学习算法的性能。

    

    在模拟中预先训练多样化的神经网络控制器使得机器人能够适应运动任务中的损伤。然而，寻找多样性高且性能优异的控制器需要昂贵的网络训练和大量的超参数调整。相比而言，基于进化策略（ES）的质量多样性算法“协方差矩阵自适应MAP模拟”没有这些限制，并在标准的QD基准测试中取得了最先进的性能。然而，由于其二次复杂度，CMA-MAE无法扩展到现代神经网络控制器。我们利用ES中的高效近似方法提出了三种新的CMA-MAE变体，这些变体能够扩展到高维度。我们的实验表明，这些变体在机器人运动任务基准测试中优于ES基线，同时可与最先进的基于深度强化学习的质量多样性算法媲美或超越其性能。

    Pre-training a diverse set of neural network controllers in simulation has enabled robots to adapt online to damage in robot locomotion tasks. However, finding diverse, high-performing controllers requires expensive network training and extensive tuning of a large number of hyperparameters. On the other hand, Covariance Matrix Adaptation MAP-Annealing (CMA-MAE), an evolution strategies (ES)-based quality diversity algorithm, does not have these limitations and has achieved state-of-the-art performance on standard QD benchmarks. However, CMA-MAE cannot scale to modern neural network controllers due to its quadratic complexity. We leverage efficient approximation methods in ES to propose three new CMA-MAE variants that scale to high dimensions. Our experiments show that the variants outperform ES-based baselines in benchmark robotic locomotion tasks, while being comparable with or exceeding state-of-the-art deep reinforcement learning-based quality diversity algorithms.
    
[^160]: 神经网络积分器精确保守律研究

    Exact conservation laws for neural network integrators of dynamical systems. (arXiv:2209.11661v2 [math.DS] UPDATED)

    [http://arxiv.org/abs/2209.11661](http://arxiv.org/abs/2209.11661)

    本文研究了使用神经网络积分器精确保守律的方法。相对于从数据中学习这些保守律，本文利用诺特定理将其固有地结合到了神经网络的结构中，并证明了其预测效果更好。

    

    最近，使用神经网络解决时变微分方程引起了广泛关注。其核心思想是从数据中学习控制解的演化的规律，其中数据可能会受到随机噪声的干扰。但是，与其他机器学习应用不同的是，通常已经了解了该系统的许多信息，例如，对于许多动力学系统，能量或者（角）动量等物理量将被完全保留。因此，神经网络必须从数据中学习这些守恒定律，并且由于有限的训练时间和随机噪声，这些守恒定律只能近似满足。在本文中，我们提出了一种另类方法，利用诺特定理将守恒定律固有地结合到神经网络的结构中。我们证明了这种方法对于三个模型系统的预测效果更好：非相对论情况下三维牛顿引力势中物质粒子的运动，受制动力学定律支配的杆从支撑点悬挂并施加张力，以及线性波动方程。

    The solution of time dependent differential equations with neural networks has attracted a lot of attention recently. The central idea is to learn the laws that govern the evolution of the solution from data, which might be polluted with random noise. However, in contrast to other machine learning applications, usually a lot is known about the system at hand. For example, for many dynamical systems physical quantities such as energy or (angular) momentum are exactly conserved. Hence, the neural network has to learn these conservation laws from data and they will only be satisfied approximately due to finite training time and random noise. In this paper we present an alternative approach which uses Noether's Theorem to inherently incorporate conservation laws into the architecture of the neural network. We demonstrate that this leads to better predictions for three model systems: the motion of a non-relativistic particle in a three-dimensional Newtonian gravitational potential, the moti
    
[^161]: 改进亚季节预测的自适应偏差校正方法

    Adaptive Bias Correction for Improved Subseasonal Forecasting. (arXiv:2209.10666v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.10666](http://arxiv.org/abs/2209.10666)

    本研究提出一种自适应偏差校正方法，应用于欧洲中期天气预报中心的亚季节模型，可以显著提高温度和降水预测精度。

    

    亚季节预测是预测未来2到6周温度和降水的重要手段，对于有效的水资源分配、野火管理以及干旱和洪涝灾害的缓解至关重要。本研究提出了一种自适应偏差校正（ABC）方法，该方法将最先进的数值模型与机器学习结合，旨在对气象动力学和物理学模型中的固有误差进行修正。在美国连续区域，当我们将ABC方法应用于欧洲中期天气预报中心（ECMWF）的领先亚季节模型时，发现温度预测技巧提高了60-90％（基线技巧在0.18-0.25之间），降水预测技巧提高了40-69％（基线技巧在0.11-0.15之间）。

    Subseasonal forecasting -- predicting temperature and precipitation 2 to 6 weeks ahead -- is critical for effective water allocation, wildfire management, and drought and flood mitigation. Recent international research efforts have advanced the subseasonal capabilities of operational dynamical models, yet temperature and precipitation prediction skills remain poor, partly due to stubborn errors in representing atmospheric dynamics and physics inside dynamical models. Here, to counter these errors, we introduce an adaptive bias correction (ABC) method that combines state-of-the-art dynamical forecasts with observations using machine learning. We show that, when applied to the leading subseasonal model from the European Centre for Medium-Range Weather Forecasts (ECMWF), ABC improves temperature forecasting skill by 60-90% (over baseline skills of 0.18-0.25) and precipitation forecasting skill by 40-69% (over baseline skills of 0.11-0.15) in the contiguous U.S. We couple these performance
    
[^162]: 关于离线策略强化学习中重复使用偏见的研究

    On the Reuse Bias in Off-Policy Reinforcement Learning. (arXiv:2209.07074v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.07074](http://arxiv.org/abs/2209.07074)

    本文揭示了离线强化学习中一个新的偏见问题：重复使用偏见，提出了一种简单有效的方法——重复使用感知重要性加权（RAW）来解决这个问题，并证明RAW显著提高了离线方法的样本效率和鲁棒性。

    

    重要性采样是离线评估中常用的技术，它通过重新加权回放缓冲区中的轨迹收益来提高样本效率。然而，使用重要性采样训练可能不稳定，并且以前解决这个问题的尝试主要集中在分析重要性采样的方差上。本文揭示了这种不稳定性也与一个新的重复使用偏见有关——由回放缓冲区的评估和优化重复使用造成的离线评估中的偏差。我们在理论上展示了当前策略的离线评估和优化与回放缓冲区的数据导致目标的过高估计，这可能导致错误的梯度更新并退化性能。我们进一步提供一个重复使用偏见的高概率上限，并展示通过引入离线算法的稳定性概念，可以通过控制上限的某一项来控制重复使用偏差。基于这些分析，我们提出了一个简单而有效的方法，称为重复使用感知重要性加权（RAW），来纠正重复使用偏见并提高离线策略强化学习的稳定性。我们还提供了实证证据来证明，RAW可以显着提高离线方法的样本效率和鲁棒性，包括DDPG、SAC和TD3。

    Importance sampling (IS) is a popular technique in off-policy evaluation, which re-weights the return of trajectories in the replay buffer to boost sample efficiency. However, training with IS can be unstable and previous attempts to address this issue mainly focus on analyzing the variance of IS. In this paper, we reveal that the instability is also related to a new notion of Reuse Bias of IS -- the bias in off-policy evaluation caused by the reuse of the replay buffer for evaluation and optimization. We theoretically show that the off-policy evaluation and optimization of the current policy with the data from the replay buffer result in an overestimation of the objective, which may cause an erroneous gradient update and degenerate the performance. We further provide a high-probability upper bound of the Reuse Bias, and show that controlling one term of the upper bound can control the Reuse Bias by introducing the concept of stability for off-policy algorithms. Based on these analyses
    
[^163]: 面向法律领域的预训练语言模型研究：以印度法律为例

    Pre-trained Language Models for the Legal Domain: A Case Study on Indian Law. (arXiv:2209.06049v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.06049](http://arxiv.org/abs/2209.06049)

    本研究针对印度法律文本，重新训练和从零开始训练了两个PLMs，即LegalBERT和CaseLawBERT，并采用基于印度法律文本的词汇表训练了一个模型。我们在几项基准法律NLP任务中，对印度和非印度的法律文本进行了应用。

    

    随着基于Transformer预训练语言模型（PLMs）在法律领域中应用的增多，特别是在欧美法律文本方面，PLMs获得了显著的成功。然而，印度等其他国家的法律文本具有很多特殊特征，因此也需要在这些方面进行预训练。本文尝试在印度法律领域进行预训练。我们在印度法律数据上重新训练（继续预训练）了两个流行的法律PLMs, LegalBERT和CaseLawBERT，以及使用基于印度法律文本的词汇表从零开始训练了一个模型。我们将这些PLMs应用于三个基准法律NLP任务——从事实中识别法律法规、对法院判决文件进行语义分割，以及预测法院上诉判决--在印度和非印度的文本上。

    NLP in the legal domain has seen increasing success with the emergence of Transformer-based Pre-trained Language Models (PLMs) pre-trained on legal text. PLMs trained over European and US legal text are available publicly; however, legal text from other domains (countries), such as India, have a lot of distinguishing characteristics. With the rapidly increasing volume of Legal NLP applications in various countries, it has become necessary to pre-train such LMs over legal text of other countries as well. In this work, we attempt to investigate pre-training in the Indian legal domain. We re-train (continue pre-training) two popular legal PLMs, LegalBERT and CaseLawBERT, on Indian legal data, as well as train a model from scratch with a vocabulary based on Indian legal text. We apply these PLMs over three benchmark legal NLP tasks -Legal Statute Identification from facts, Semantic Segmentation of Court Judgment Documents, and Court Appeal Judgment Prediction -- over both Indian and non-
    
[^164]: 张量积与近似正交码的图嵌入方法

    Graph Embeddings via Tensor Products and Approximately Orthonormal Codes. (arXiv:2208.10917v4 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2208.10917](http://arxiv.org/abs/2208.10917)

    本文介绍了一种嵌入图形到向量空间的方法，使用张量积以及球形码实现高效压缩和表征，在稀疏图表示和其他应用中具有潜在技术优势。

    

    我们分析了一种以保持结构方式来嵌入图形的方法，展示了其丰富的表征能力并建立了一些理论性质。我们的过程属于绑定和求和方法，并且我们显示了张量积是尊重叠加原理的最一般的绑定操作。我们还建立了一些精确的结果对我们方法的行为进行了表征，并且我们证明我们使用的球形码实现了一个装箱上限。我们建立了与邻接矩阵的联系，表明我们的方法在某种意义上是一种邻接矩阵的压缩，具有稀疏图表示的应用。

    We analyze a method for embedding graphs as vectors in a structure-preserving manner, showcasing its rich representational capacity and establishing some of its theoretical properties. Our procedure falls under the bind-and-sum approach, and we show that the tensor product is the most general binding operation that respects the superposition principle. We also establish some precise results characterizing the behavior of our method, and we show that our use of spherical codes achieves a packing upper bound. We establish a link to adjacency matrices, showing that our method is, in some sense, a compression of adjacency matrices with applications towards sparse graph representations.
    
[^165]: 分布式训练中拜占庭攻击的检测与缓解

    Detection and Mitigation of Byzantine Attacks in Distributed Training. (arXiv:2208.08085v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.08085](http://arxiv.org/abs/2208.08085)

    本文探讨了分布式训练中的拜占庭攻击及其检测和缓解方法。

    

    大量现代机器学习任务需要使用大规模分布式集群作为训练流程的关键组成部分。但是工作节点的异常拜占庭行为可能会破坏训练并危及推理的质量。此类行为可以归因于无意的系统故障或有组织的攻击，结果可能是一些节点向协调训练的参数服务器（PS）返回任意结果。最近的工作考虑了各种攻击模型并探索了强大的聚合和/或计算冗余来纠正扭曲的梯度。在这项工作中，我们考虑攻击模型从强到弱不等：$ q $具有完全了解防御协议并且每迭代都可以更改的全知对手到$ q $具有有限勾结能力并且仅每隔几次迭代更改的随机对手。我们的算法依赖于冗余任务分配、基于阈值的接收梯度过滤和基于时期的工作节点重启，以识别和缓解分布式训练中的拜占庭攻击。

    A plethora of modern machine learning tasks require the utilization of large-scale distributed clusters as a critical component of the training pipeline. However, abnormal Byzantine behavior of the worker nodes can derail the training and compromise the quality of the inference. Such behavior can be attributed to unintentional system malfunctions or orchestrated attacks; as a result, some nodes may return arbitrary results to the parameter server (PS) that coordinates the training. Recent work considers a wide range of attack models and has explored robust aggregation and/or computational redundancy to correct the distorted gradients.  In this work, we consider attack models ranging from strong ones: $q$ omniscient adversaries with full knowledge of the defense protocol that can change from iteration to iteration to weak ones: $q$ randomly chosen adversaries with limited collusion abilities which only change every few iterations at a time. Our algorithms rely on redundant task assignme
    
[^166]: 量子电路出生机器是否具有推广能力？

    Do Quantum Circuit Born Machines Generalize?. (arXiv:2207.13645v4 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2207.13645](http://arxiv.org/abs/2207.13645)

    本文研究了量子电路出生机器对基数受限分布的学习过程，提高了模型的泛化性能，并探究了此能力的资源需求，并提出了一种改进的训练框架，可以学习具有高准确性和泛化性能的复杂分布.

    

    在最近的量子电路模型生成任务的提议中，关于它们的表现的讨论仅限于它们重现已知目标分布的能力。例如，表达力强的模型系列，例如量子电路出生机器（QCBMs）几乎完全被评估其学习具有高准确性的给定目标分布的能力。虽然这方面对于某些任务可能是理想的，但它限制了生成模型评估其泛化能力的范围，而非其记忆数据的能力。因此，人们对模型的泛化性能以及此类能力与资源需求之间的关系了解甚少。在这项工作中，我们利用最近提出的泛化性评估框架来开始解决这一知识空白。我们首先研究了QCBM对基数受限分布的学习过程，并发现随着基数增加，泛化性能有所提高。然后我们展示了具有高泛化能力的学习模型表现出小样本学习能力，并探究了此能力的资源需求。最后，我们提出了一种改进的训练框架，具有课程学习策略，可以学习具有高准确性和泛化性能的复杂分布。

    In recent proposals of quantum circuit models for generative tasks, the discussion about their performance has been limited to their ability to reproduce a known target distribution. For example, expressive model families such as Quantum Circuit Born Machines (QCBMs) have been almost entirely evaluated on their capability to learn a given target distribution with high accuracy. While this aspect may be ideal for some tasks, it limits the scope of a generative model's assessment to its ability to memorize data rather than generalize. As a result, there has been little understanding of a model's generalization performance and the relation between such capability and the resource requirements, e.g., the circuit depth and the amount of training data. In this work, we leverage upon a recently proposed generalization evaluation framework to begin addressing this knowledge gap. We first investigate the QCBM's learning process of a cardinality-constrained distribution and see an increase in ge
    
[^167]: 揭示推进生成模型的潜在空间几何结构

    Unveiling the Latent Space Geometry of Push-Forward Generative Models. (arXiv:2207.10541v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.10541](http://arxiv.org/abs/2207.10541)

    本文研究了深度生成模型的潜在空间及其与模型性能之间的关系。借助几何测量理论，我们发现了优化的充分条件。我们提出了一种截断方法，可以在潜在空间中强制实行一个简单的簇结构，并提高了模型的性能。

    

    许多深度生成模型都是通过连续生成器推进高斯测量而定义的，例如生成对抗网络（GAN）或变分自动编码器（VAE）。本文探究了这些深度生成模型的潜在空间。这些模型的一个关键问题是，在学习不连通分布时，它们往往输出超出目标分布支持范围的样本。我们研究了这些模型的性能与它们的潜在空间几何之间的关系。借助几何测量理论的最新发展，我们在潜在空间的维度大于模的数量的情况下证明了优化的充分条件。通过对GAN进行实验，我们证明了我们理论结果的可靠性，并对这些模型的潜在空间几何结构获得了新的见解。此外，我们提出了一种截断方法，可以在潜在空间中强制实行一个简单的簇结构，并提高了模型的性能。

    Many deep generative models are defined as a push-forward of a Gaussian measure by a continuous generator, such as Generative Adversarial Networks (GANs) or Variational Auto-Encoders (VAEs). This work explores the latent space of such deep generative models. A key issue with these models is their tendency to output samples outside of the support of the target distribution when learning disconnected distributions. We investigate the relationship between the performance of these models and the geometry of their latent space. Building on recent developments in geometric measure theory, we prove a sufficient condition for optimality in the case where the dimension of the latent space is larger than the number of modes. Through experiments on GANs, we demonstrate the validity of our theoretical results and gain new insights into the latent space geometry of these models. Additionally, we propose a truncation method that enforces a simplicial cluster structure in the latent space and improve
    
[^168]: 日志数据异常检测的深度学习综述

    Deep Learning for Anomaly Detection in Log Data: A Survey. (arXiv:2207.03820v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.03820](http://arxiv.org/abs/2207.03820)

    该综述对深度学习在日志文件异常检测方面的应用进行了系统的文献综述，分析了现有模型、数据预处理机制、异常检测技术和评估方法，以及深度学习相对于传统机器学习技术的优点。

    

    自动日志文件分析可以提前检测重要事件，如系统故障。尤其是，自学习的异常检测技术捕捉日志数据中的模式，并随后向系统操作员报告意外日志事件发生，无需提前提供或手动建模异常场景。最近，出现了越来越多利用深度学习神经网络完成此任务的方法。这些方法在比较传统机器学习技术的检测性能方面表现出优越的性能，并同时解决了数据格式不稳定的问题。然而，有许多不同的深度学习架构，将未加处理的非结构化日志数据编码以供神经网络分析是非常复杂的。因此，我们进行了系统的文献综述，概述了部署的模型、数据预处理机制、异常检测技术和评估方法。

    Automatic log file analysis enables early detection of relevant incidents such as system failures. In particular, self-learning anomaly detection techniques capture patterns in log data and subsequently report unexpected log event occurrences to system operators without the need to provide or manually model anomalous scenarios in advance. Recently, an increasing number of approaches leveraging deep learning neural networks for this purpose have been presented. These approaches have demonstrated superior detection performance in comparison to conventional machine learning techniques and simultaneously resolve issues with unstable data formats. However, there exist many different architectures for deep learning and it is non-trivial to encode raw and unstructured log data to be analyzed by neural networks. We therefore carry out a systematic literature review that provides an overview of deployed models, data pre-processing mechanisms, anomaly detection techniques, and evaluations. The s
    
[^169]: 联邦多臂赌博机

    Federated X-Armed Bandit. (arXiv:2205.15268v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.15268](http://arxiv.org/abs/2205.15268)

    本文提出了第一个联邦多臂赌博机算法，通过利用全局目标的拓扑结构以及层次分割和弱平滑特性，实现了与客户端数量和评估预算相关的次线性累计遗憾度，对数通信只在中央服务器和客户端之间进行，保护了客户端的隐私。

    

    本文建立了第一个联邦 $\mathcal{X}$-armed bandit 框架，不同客户端面临在相同域上定义的异构局部目标函数，并需要协作地找出全局最优解。我们提出了针对此类问题的第一个联邦算法，称为 \texttt{Fed-PNE}。通过利用全局目标的拓扑结构以及层次分割和弱平滑特性，我们的算法实现了与客户端数量和评估预算相关的次线性累计遗憾度。同时，它只需要中央服务器和客户端之间的对数通信，保护了客户端的隐私。合成函数和真实数据集上的实验结果验证了 \texttt{Fed-PNE} 相对于各种集中式和联邦基线算法的优势。

    This work establishes the first framework of federated $\mathcal{X}$-armed bandit, where different clients face heterogeneous local objective functions defined on the same domain and are required to collaboratively figure out the global optimum. We propose the first federated algorithm for such problems, named \texttt{Fed-PNE}. By utilizing the topological structure of the global objective inside the hierarchical partitioning and the weak smoothness property, our algorithm achieves sublinear cumulative regret with respect to both the number of clients and the evaluation budget. Meanwhile, it only requires logarithmic communications between the central server and clients, protecting the client privacy. Experimental results on synthetic functions and real datasets validate the advantages of \texttt{Fed-PNE} over various centralized and federated baseline algorithms.
    
[^170]: 基于智能空间插值的冻霜预测方法学——有限局部数据上的人工神经网络应用

    Intelligent Spatial Interpolation-based Frost Prediction Methodology using Artificial Neural Networks with Limited Local Data. (arXiv:2204.08465v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.08465](http://arxiv.org/abs/2204.08465)

    本文提出了一种基于空间插值的冻霜预测方法，利用气象站的气候数据、数字高程模型和植被指数数据，预测目标场地的下一小时最低温度，达到了92.55%的探测率。

    

    冻霜天气现象对农业构成严重威胁。由于现有的预测方法都是基于现场历史数据和传感器数据，因此在新场地中需要耗费额外的时间进行数据收集和部署。本文旨在消除冻霜预测方法对现场历史数据和传感器数据的依赖性。本文提出了一种基于空间插值的冻霜预测方法。该模型利用现有气象站的气候数据、数字高程模型勘测和归一化差异植被指数数据来预测目标场地的下一小时最低温度。所提出的方法利用集成学习来提高模型的准确性。气候数据集来自澳大利亚新南威尔士和首都领地地区的75个气象站。结果表明，该方法的探测率最高可达92.55%。

    The weather phenomenon of frost poses great threats to agriculture. As recent frost prediction methods are based on on-site historical data and sensors, extra development and deployment time are required for data collection in any new site. The aim of this article is to eliminate the dependency on on-site historical data and sensors for frost prediction methods. In this article, a frost prediction method based on spatial interpolation is proposed. The models use climate data from existing weather stations, digital elevation models surveys, and normalized difference vegetation index data to estimate a target site's next hour minimum temperature. The proposed method utilizes ensemble learning to increase the model accuracy. Climate datasets are obtained from 75 weather stations across New South Wales and Australian Capital Territory areas of Australia. The results show that the proposed method reached a detection rate up to 92.55%.
    
[^171]: PADA: 基于剪枝的自监督语音表示学习领域自适应

    PADA: Pruning Assisted Domain Adaptation for Self-Supervised Speech Representations. (arXiv:2203.16965v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2203.16965](http://arxiv.org/abs/2203.16965)

    本文提出了PADA方法，在自监督语音表示学习领域自适应方面加入剪枝策略，使用CD-TAW方法从精细调整的OOT模型中获得初始剪枝掩码，并取得良好效果。

    

    自监督语音表示学习模型可用于多种下游任务，但这些模型常常会出现对未标注数据来源领域的过拟合现象。为缓解这个问题，本文提出了PADA (Pruning Assisted Domain Adaptation)，并从经过大量OOT(Out-of-domain)数据预训练的模型中减去多余的权重，为目标领域的ASR微调腾出空间。可以通过各种剪枝策略来识别冗余权重，本文详细讨论了最近发现的Task-Agnostic和Task-Aware剪枝对PADA的影响，并提出了一种新的基于后者的剪枝范式，称为Cross-Domain Task-Aware Pruning(CD-TAW)。CD-TAW从精细调整的OOT模型中获得初始剪枝掩码，这使其与本文中讨论的剪枝策略截然不同。我们提出的PADA方法可以通过微调模型和利用剪枝技术来消除不必要的权重，从而提高自监督语音表示学习模型的泛化能力。使用CD-TAW剪枝策略可以显著提高模型性能。

    While self-supervised speech representation learning (SSL) models serve a variety of downstream tasks, these models have been observed to overfit to the domain from which the unlabelled data originates. To alleviate this issue, we propose PADA (Pruning Assisted Domain Adaptation) and zero out redundant weights from models pre-trained on large amounts of out-of-domain (OOD) data. Intuitively, this helps to make space for the target-domain ASR finetuning. The redundant weights can be identified through various pruning strategies which have been discussed in detail as a part of this work. Specifically, we investigate the effect of the recently discovered Task-Agnostic and Task-Aware pruning on PADA and propose a new pruning paradigm based on the latter, which we call Cross-Domain Task-Aware Pruning (CD-TAW). CD-TAW obtains the initial pruning mask from a well fine-tuned OOD model, which makes it starkly different from the rest of the pruning strategies discussed in the paper. Our proposed
    
[^172]: 基于变分指令集的量子编译优化：实现高精度与快速量子计算

    Quantum compiling with variational instruction set for accurate and fast quantum computing. (arXiv:2203.15574v3 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2203.15574](http://arxiv.org/abs/2203.15574)

    本文提出了基于灵活设计的多量子比特门操作的量子变分指令集(QuVIS)，通过细粒度的时间优化算法，实现了快速和精确的量子计算，并且通过实验展示出了更低的误差积累和时间成本。

    

    量子指令集(QIS)定义为在控制量子比特状态下可以物理实现的一系列量子门操作，其在量子计算中起着基础性作用。本文提出了基于灵活设计的多量子比特门操作的量子变分指令集(QuVIS)，通过细粒度的时间优化算法来变分实现量子比特的控制，从而实现快速和精确的量子计算。与标准QIS 如量子微指令集(QuMIS)相比，QuVIS 用于多量子比特交换和量子傅里叶变换等门操作具有更低的误差积累和时间成本。在相同量子硬件要求下，本文的方法可大幅提升量子计算的效率。

    The quantum instruction set (QIS) is defined as the quantum gates that are physically realizable by controlling the qubits in a quantum hardware. Compiling quantum circuits into the product of the gates in a properly-defined QIS is a fundamental step in quantum computing. We here propose the \R{quantum variational instruction set (QuVIS)} formed by flexibly-designed multi-qubit gates for higher speed and accuracy of quantum computing. The controlling of qubits for realizing the gates in a QuVIS are variationally achieved using the fine-grained time optimization algorithm. Significant reductions on both the error accumulation and time cost are demonstrated in realizing the swaps of multiple qubits and quantum Fourier transformations, compared with the compiling by the standard QIS such as \RR{the quantum microinstruction set} (QuMIS, formed by several one- and two-qubit gates including the one-qubit rotations and controlled-NOT gate). With the same requirement on quantum hardware, the t
    
[^173]: CD-GAN:一种基于融合的强大生成对抗性网络，用于具有异构传感器的无监督遥感变化检测。

    CD-GAN: a robust fusion-based generative adversarial network for unsupervised remote sensing change detection with heterogeneous sensors. (arXiv:2203.00948v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2203.00948](http://arxiv.org/abs/2203.00948)

    CD-GAN是一种针对具有异构传感器的遥感图像的无监督变化检测方法，该方法利用了最近的融合技术进展和对抗性网络，能够有效解决传统方法中传感器空间和/或光谱分辨率不同造成的挑战。

    

    在地球观测的背景下，变化检测是通过多时相图像来实现的，这些图像由可能具有不同空间和/或光谱分辨率甚至不同模态（例如光学，雷达）的传感器获取。 即使限制为光学模态，只要传感器具有不同的空间和/或光谱分辨率，这项任务就已经被证明具有挑战性。本文提出了一种新的针对这种所谓的异构光学传感器获取的图像的无监督变化检测方法。该方法利用了最近的进展，将变化检测问题框架化为强大的融合框架。更具体地说，我们展示了一个深度对抗网络，旨在事先设计和训练以融合一对多波段光学图像，可以很容易地通过具有相同架构的网络来补充执行变化检测。生成的整体架构本身遵循对抗性策略，其中融合网络和其他网络。

    In the context of Earth observation, the detection of changes is performed from multitemporal images acquired by sensors with possibly different spatial and/or spectral resolutions or even different modalities (e.g. optical, radar). Even limiting to the optical modality, this task has proved to be challenging as soon as the sensors have different spatial and/or spectral resolutions. This paper proposes a novel unsupervised change detection method dedicated to images acquired with such so-called heterogeneous optical sensors. This method capitalizes on recent advances which frame the change detection problem into a robust fusion framework. More precisely, we show that a deep adversarial network designed and trained beforehand to fuse a pair of multiband optical images can be easily complemented by a network with the same architecture to perform change detection. The resulting overall architecture itself follows an adversarial strategy where the fusion network and the additional network 
    
[^174]: 关于带有线性函数逼近的SARSA算法收敛性的研究

    On the Convergence of SARSA with Linear Function Approximation. (arXiv:2202.06828v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.06828](http://arxiv.org/abs/2202.06828)

    本文通过对投影SARSA到有限区域的收敛速度的探究，取得了在带有线性函数逼近的SARSA算法收敛性方面的进展，发现收敛区域比想象的要小得多。

    

    SARSA是一种经典的增强学习算法，该算法在与线性函数逼近相结合时会出现震荡现象。本文通过展示投影SARSA到有限区域的收敛速度，取得了在此方面的一定进展。惊人的是，只要奖励的大小不是太大，收敛区域比想象中的要小得多。值得注意的是，现有的关于线性SARSA收敛性的研究都需要SARSA策略改进算子的Lipschitz常数足够小；与之不同的是，我们的分析适用于任意Lipschitz常数，从而揭示了线性SARSA的新行为范式。

    SARSA, a classical on-policy control algorithm for reinforcement learning, is known to chatter when combined with linear function approximation: SARSA does not diverge but oscillates in a bounded region. However, little is known about how fast SARSA converges to that region and how large the region is. In this paper, we make progress towards this open problem by showing the convergence rate of projected SARSA to a bounded region. Importantly, the region is much smaller than the region that we project into, provided that the magnitude of the reward is not too large. Existing works regarding the convergence of linear SARSA to a fixed point all require the Lipschitz constant of SARSA's policy improvement operator to be sufficiently small; our analysis instead applies to arbitrary Lipschitz constants and thus characterizes the behavior of linear SARSA for a new regime.
    
[^175]: 两时间尺度更新规则训练生成式对抗网络中的关键批次大小的存在和估计

    Existence and Estimation of Critical Batch Size for Training Generative Adversarial Networks with Two Time-Scale Update Rule. (arXiv:2201.11989v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.11989](http://arxiv.org/abs/2201.11989)

    本文研究了使用两时间尺度更新规则（TTUR）训练生成式对抗网络（GAN）时批次大小与训练所需步骤数量之间的关系，理论上证明了为了找到稳定点，随着批次大小的增加所需步骤数量会减少并且存在一个最小化随机一阶预言机（SFO）复杂度的关键批次大小。

    

    先前的研究表明，在理论和实践中，使用不同的学习率，如不同的恒定率或不同的衰减率等，使用两时间尺度更新规则（TTUR）有助于训练生成式对抗网络（GAN）。此外，批次大小对于使用TTUR训练GANs也很重要，两者都影响了训练所需的步骤数量。本文基于恒定学习率研究了批次大小与使用TTUR训练GANs所需步骤数量之间的关系。我们理论上表明，对于具有恒定学习率的TTUR，为了找到鉴别器和生成器损失函数的稳定点，所需步骤数随着批次大小的增加而减少，并且存在一个最小化随机一阶预言机（SFO）复杂度的关键批次大小。然后，我们使用Fr'echet Inception Distance（FID）作为训练的性能测量，并提供了...

    Previous results have shown that a two time-scale update rule (TTUR) using different learning rates, such as different constant rates or different decaying rates, is useful for training generative adversarial networks (GANs) in theory and in practice. Moreover, not only the learning rate but also the batch size is important for training GANs with TTURs and they both affect the number of steps needed for training. This paper studies the relationship between batch size and the number of steps needed for training GANs with TTURs based on constant learning rates. We theoretically show that, for a TTUR with constant learning rates, the number of steps needed to find stationary points of the loss functions of both the discriminator and generator decreases as the batch size increases and that there exists a critical batch size minimizing the stochastic first-order oracle (SFO) complexity. Then, we use the Fr'echet inception distance (FID) as the performance measure for training and provide nu
    
[^176]: 关于对抗性Bayes分类器存在性的研究（扩展版）

    On the Existence of the Adversarial Bayes Classifier (Extended Version). (arXiv:2112.01694v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.01694](http://arxiv.org/abs/2112.01694)

    本篇论文研究了对抗训练健壮性下Bayes最优分类器的存在性问题，提出了一般性的充分条件，并可以为研究对抗性代理损失和其一致性属性提供有用的工具。

    

    对抗训练健壮性在现代机器学习应用中至关重要。虽然最近已经有多项理论研究，但与对抗训练健壮性相关的许多重要问题仍然未被解决。本文研究了一个关于对抗训练健壮性下Bayes最优分类器存在性的基本问题。我们提出了一般的充分条件，以保证存在对抗训练健壮性下的Bayes最优分类器。我们的结果可以为对后续对抗训练健壮性下代理损失和它们的一致性属性的研究提供有用的工具。本文是“关于对抗性Bayes分类器存在性”的矫正和扩展版本，该稿件已发表在NeurIPS 2021上。原始论文中有两处定理错误，一处是对伪可证健壮性的定义，另一处是针对任意度量空间的$A^\e$可测性。

    Adversarial robustness is a critical property in a variety of modern machine learning applications. While it has been the subject of several recent theoretical studies, many important questions related to adversarial robustness are still open. In this work, we study a fundamental question regarding Bayes optimality for adversarial robustness. We provide general sufficient conditions under which the existence of a Bayes optimal classifier can be guaranteed for adversarial robustness. Our results can provide a useful tool for a subsequent study of surrogate losses in adversarial robustness and their consistency properties. This manuscript is the extended and corrected version of the paper \emph{On the Existence of the Adversarial Bayes Classifier} published in NeurIPS 2021. There were two errors in theorem statements in the original paper -- one in the definition of pseudo-certifiable robustness and the other in the measurability of $A^\e$ for arbitrary metric spaces. In this version we 
    
[^177]: PARIS：用于改善睡眠质量的个性化活动推荐系统

    PARIS: Personalized Activity Recommendation for Improving Sleep Quality. (arXiv:2110.13745v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2110.13745](http://arxiv.org/abs/2110.13745)

    该论文利用机器学习技术，结合可穿戴设备监测的数据，通过时间序列聚类找到与指定主题相关的行为模型并生成相应的睡眠质量活动建议，为提高睡眠质量提供了一种个性化解决方案。

    

    睡眠质量对人们的身体和心理健康有深远影响。睡眠不足的人更容易报告身体和心理困扰、活动受限、焦虑和疼痛。此外，过去几年中，活动监测和健康跟踪的应用和设备方兴未艾。从这些可穿戴设备收集到的信号可用于研究和改善睡眠质量。本文利用实体活动和睡眠质量之间的关系，利用机器学习技术找到协助人们改善睡眠的方法。对活动数据进行时间序列聚类，我们找到与特定主题最显着的行为模式相关的簇中心。然后为每个行为模式中的每个簇生成有助于良好睡眠质量的活动建议。这些活动建议供应给每位用户的个性化活动推荐系统。

    The quality of sleep has a deep impact on people's physical and mental health. People with insufficient sleep are more likely to report physical and mental distress, activity limitation, anxiety, and pain. Moreover, in the past few years, there has been an explosion of applications and devices for activity monitoring and health tracking. Signals collected from these wearable devices can be used to study and improve sleep quality. In this paper, we utilize the relationship between physical activity and sleep quality to find ways of assisting people improve their sleep using machine learning techniques. People usually have several behavior modes that their bio-functions can be divided into. Performing time series clustering on activity data, we find cluster centers that would correlate to the most evident behavior modes for a specific subject. Activity recipes are then generated for good sleep quality for each behavior mode within each cluster. These activity recipes are supplied to an a
    
[^178]: AxoNN: 一种异步、消息驱动的极规模深度学习并行框架

    AxoNN: An asynchronous, message-driven parallel framework for extreme-scale deep learning. (arXiv:2110.13005v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.13005](http://arxiv.org/abs/2110.13005)

    AxoNN是一种利用异步性和消息驱动执行调度每个GPU上神经网络操作的并行深度学习框架，将CPU内存作为冗余空间，降低GPU内存消耗，同时将每个GPU的参数数量增加4倍，从而减少了必需的GPU数量和训练时间。

    

    近年来，训练最先进的神经网络所需的存储器容量已远远超出现代硬件加速器的DRAM容量。这促使我们在大规模基于GPU的集群上开发高效算法并行训练这些神经网络。在现代GPU上，计算相对廉价，为了提取最大性能，设计和实现这些并行训练算法中极其高效的通信是至关重要的。本文介绍了AxoNN，一种利用异步性和消息驱动执行调度每个GPU上神经网络操作的并行深度学习框架，从而减少GPU空闲时间，最大化硬件效率。通过使用CPU内存作为冗余空间，在训练期间定期卸载数据，AxoNN能够将GPU内存消耗降低4倍。这使得我们能够将每个GPU的参数数量增加4倍，从而减少了必需的GPU数量和训练时间。

    In the last few years, the memory requirements to train state-of-the-art neural networks have far exceeded the DRAM capacities of modern hardware accelerators. This has necessitated the development of efficient algorithms to train these neural networks in parallel on large-scale GPU-based clusters. Since computation is relatively inexpensive on modern GPUs, designing and implementing extremely efficient communication in these parallel training algorithms is critical for extracting the maximum performance. This paper presents AxoNN, a parallel deep learning framework that exploits asynchrony and message-driven execution to schedule neural network operations on each GPU, thereby reducing GPU idle time and maximizing hardware efficiency. By using the CPU memory as a scratch space for offloading data periodically during training, AxoNN is able to reduce GPU memory consumption by four times. This allows us to increase the number of parameters per GPU by four times, thus reducing the amount 
    
[^179]: 当机会来临时进行交易：基于注意力机制和迭代细化标注的价格变动预测

    Trade When Opportunity Comes: Price Movement Forecasting via Locality-Aware Attention and Iterative Refinement Labeling. (arXiv:2107.11972v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.11972](http://arxiv.org/abs/2107.11972)

    本论文提出了一种名为LARA的新型价格变动预测框架，包括两个主要部分：局部感知注意力和迭代细化标注。在真实世界的金融数据集上的实验结果表明，LARA在准确性和盈利能力方面优于现有的最先进方法。

    

    价格变动预测旨在根据当前市场情况和其他相关信息预测金融资产的未来趋势。最近，机器学习（ML）方法在学术界和工业界中越来越受欢迎，并取得了令人满意的结果。然而，由于金融数据的低信噪比和随机性极强，好的交易机会极为稀少。因此，如果不仔细选择潜在的盈利样本，这些ML方法容易捕捉到噪声而不是真实信号的模式。为解决这个问题，本研究提出了一种名为LARA的新型价格变动预测框架，包括两个主要部分：局部感知注意力（LA-Attention）和迭代细化标注（IRL）。LA-Attention旨在有选择地关注金融数据中最具信息量的局部区域，而IRL则旨在迭代地细化标注过程，过滤掉噪声和无关样本。在真实世界的金融数据集上的实验结果表明，LARA在准确性和盈利能力方面优于现有的最先进方法。

    Price movement forecasting aims at predicting the future trends of financial assets based on the current market conditions and other relevant information. Recently, machine learning (ML) methods have become increasingly popular and achieved promising results for price movement forecasting in both academia and industry. Most existing ML solutions formulate the forecasting problem as a classification (to predict the direction) or a regression (to predict the return) problem over the entire set of training data. However, due to the extremely low signal-to-noise ratio and stochastic nature of financial data, good trading opportunities are extremely scarce. As a result, without careful selection of potentially profitable samples, such ML methods are prone to capture the patterns of noises instead of real signals. To address this issue, we propose a novel price movement forecasting framework named LARA consisting of two main components: Locality-Aware Attention (LA-Attention) and Iterative R
    
[^180]: 基于任务的显式超参数预测模型的学习

    Learning an Explicit Hyperparameter Prediction Function Conditioned on Tasks. (arXiv:2107.02378v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.02378](http://arxiv.org/abs/2107.02378)

    本研究探索了一种基于任务的学习方法，学习一个显式超参数预测函数以适应不同的查询任务。

    

    元学习近年来引起了机器学习界的广泛关注。与传统的机器学习旨在学习内在的预测规则以预测新的查询数据标签不同，元学习旨在从观察到的任务中学习机器学习的学习方法，以便通过利用元学习的学习方法对新的查询任务进行泛化。在本研究中，我们将这样的学习方法解释为学习一个显式超参数预测函数，该函数由所有训练任务共享。具体而言，这个函数被表示为一个参数化函数，称为元学习器，将训练/测试任务映射到其合适的超参数设置，提取自称为元学习机的预先指定的函数集。这种设置保证了元学习的学习方法能够灵活地适应不同的查询任务，而不是像许多现有的元学习方法一样，只获得固定的超参数，适应性较差。

    Meta learning has attracted much attention recently in machine learning community. Contrary to conventional machine learning aiming to learn inherent prediction rules to predict labels for new query data, meta learning aims to learn the learning methodology for machine learning from observed tasks, so as to generalize to new query tasks by leveraging the meta-learned learning methodology. In this study, we interpret such learning methodology as learning an explicit hyper-parameter prediction function shared by all training tasks. Specifically, this function is represented as a parameterized function called meta-learner, mapping from a training/test task to its suitable hyper-parameter setting, extracted from a pre-specified function set called meta learning machine. Such setting guarantees that the meta-learned learning methodology is able to flexibly fit diverse query tasks, instead of only obtaining fixed hyper-parameters by many current meta learning methods, with less adaptability 
    
[^181]: 通过自学习集合在无标签数据上检测错误并估计准确性

    Detecting Errors and Estimating Accuracy on Unlabeled Data with Self-training Ensembles. (arXiv:2106.15728v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.15728](http://arxiv.org/abs/2106.15728)

    本文提出了一种利用自学习集合框架，同时解决无标签数据上的误差检测和准确性估计的方法。

    

    当深度学习模型在未知分布的测试数据上运行时，它会遭遇到性能下降的情况。为了安全部署，需要对预训练模型在测试数据上的准确性进行估算。然而，在实践中，测试集的标注通常不会立即提供，并且获得它们可能是昂贵的。本文提出了一个实用、有原则的框架，同时解决了两个挑战性任务：（1）无监督准确性估计，目的是估计预先训练分类器在一组无标签测试样本上的准确性；（2）错误检测，目的是识别被错误分类的测试样本。所提出的框架迭代地学习了一个集合模型来识别被错误分类的数据点，并通过自学习来改进集合模型。理论分析演示了该框架的包容性和正确性。

    When a deep learning model is deployed in the wild, it can encounter test data drawn from distributions different from the training data distribution and suffer drop in performance. For safe deployment, it is essential to estimate the accuracy of the pre-trained model on the test data. However, the labels for the test inputs are usually not immediately available in practice, and obtaining them can be expensive. This observation leads to two challenging tasks: (1) unsupervised accuracy estimation, which aims to estimate the accuracy of a pre-trained classifier on a set of unlabeled test inputs; (2) error detection, which aims to identify mis-classified test inputs. In this paper, we propose a principled and practically effective framework that simultaneously addresses the two tasks. The proposed framework iteratively learns an ensemble of models to identify mis-classified data points and performs self-training to improve the ensemble with the identified points. Theoretical analysis demo
    
[^182]: 一种带有负相关学习的混合集成算法进行回归分析

    A hybrid ensemble method with negative correlation learning for regression. (arXiv:2104.02317v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2104.02317](http://arxiv.org/abs/2104.02317)

    该论文提出了一种带有负相关学习的混合集成算法，通过自动选择和加权子模型来解决在回归任务中模型不确定性问题，并在实验中表现出较好的性能。

    

    混合集成是集成学习中的一个重要分支，在回归领域得到了迅速发展，并证实了多样性的重要性。然而，以往的集成方法主要是在子模型训练阶段考虑多样性，改善效果有限。相比之下，这项研究从异构模型池中自动选择和加权子模型，并使用内点过滤线性搜索算法解决优化问题。目标函数创新地将负相关学习作为一个惩罚项，并选择多样化子模型子集。选择每个模型类的最佳子模型来构建NCL集成，其性能优于简单平均和其他最先进的加权方法。在目标函数中加入正则化项，还可以进一步改善NCL集成的效果。

    Hybrid ensemble, an essential branch of ensembles, has flourished in the regression field, with studies confirming diversity's importance. However, previous ensembles consider diversity in the sub-model training stage, with limited improvement compared to single models. In contrast, this study automatically selects and weights sub-models from a heterogeneous model pool. It solves an optimization problem using an interior-point filtering linear-search algorithm. The objective function innovatively incorporates negative correlation learning as a penalty term, with which a diverse model subset can be selected. The best sub-models from each model class are selected to build the NCL ensemble, which performance is better than the simple average and other state-of-the-art weighting methods. It is also possible to improve the NCL ensemble with a regularization term in the objective function. In practice, it is difficult to conclude the optimal sub-model for a dataset prior due to the model unc
    
[^183]: 应用于机器人布料操作的受控高斯过程动力学模型

    Controlled Gaussian Process Dynamical Models with Application to Robotic Cloth Manipulation. (arXiv:2103.06615v5 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2103.06615](http://arxiv.org/abs/2103.06615)

    本文提出了一种称为受控高斯过程动力学模型（CGPDM）的方法，用于学习高维非线性动态。该模型将高维状态空间投影到较小的维度潜在空间中。CGPDM由一个低维潜在空间组成，具有相关的动态，外部控制变量可以作用于该空间，并映射到观测空间。

    

    近年来，机器人操纵方面取得了显着进展，但处理非刚性物体（如布料）仍然是一个悬而未决的问题。与非刚性物体进行物理交互是不确定和复杂的建模。因此，从样本数据中提取有用信息可以极大地提高建模性能。然而，由于状态表示的高维性，这种模型的训练是一项具有挑战性的任务。在本文中，我们提出了受控高斯过程动力学模型（CGPDM），用于通过将其嵌入到低维流形中学习高维非线性动态。 CGPDM由一个低维潜在空间组成，具有相关的动态，外部控制变量可以作用于该空间，并映射到观测空间。考虑高斯过程（GP）先验分布来边缘化两个映射的参数。因此，CGPDM将高维状态空间投影到较小的维度潜在空间中。

    Over the last years, significant advances have been made in robotic manipulation, but still, the handling of non-rigid objects, such as cloth garments, is an open problem. Physical interaction with non-rigid objects is uncertain and complex to model. Thus, extracting useful information from sample data can considerably improve modeling performance. However, the training of such models is a challenging task due to the high-dimensionality of the state representation. In this paper, we propose Controlled Gaussian Process Dynamical Model (CGPDM) for learning high-dimensional, nonlinear dynamics by embedding it in a low-dimensional manifold. A CGPDM is constituted by a low-dimensional latent space, with an associated dynamics where external control variables can act and a mapping to the observation space. The parameters of both maps are marginalized out by considering Gaussian Process (GP) priors. Hence, a CGPDM projects a high-dimensional state space into a smaller dimension latent space, 
    
[^184]: 多任务注意力残差网络用于论述挖掘

    Multi-Task Attentive Residual Networks for Argument Mining. (arXiv:2102.12227v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2102.12227](http://arxiv.org/abs/2102.12227)

    本文提出了一种多任务注意力残差网络架构，通过利用集成方法、注意力机制和多任务学习，无需假设文档或论据结构，成功应用于多个论述挖掘任务中，成为了一种既通用又高性能的架构。

    

    本文探讨了多任务注意力残差网络在多个论述挖掘任务中的应用。我们提出了一种残差架构，利用了注意力、多任务学习，并使用集成方法，不对文档或论据结构做任何假设。我们在五个不同的用户生成评论、科学出版物和劝说性论文语料库上进行了广泛的实验评估。我们的结果表明，我们的方法是针对具有更高计算印记或特定于语料库设计的最先进架构的强有力的竞争对手，代表了通用性、性能精度和减少模型大小之间的有趣折衷。

    We explore the use of residual networks and neural attention for multiple argument mining tasks. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble, without any assumption on document or argument structure. We present an extensive experimental evaluation on five different corpora of user-generated comments, scientific publications, and persuasive essays. Our results show that our approach is a strong competitor against state-of-the-art architectures with a higher computational footprint or corpus-specific design, representing an interesting compromise between generality, performance accuracy and reduced model size.
    
[^185]: 低秩有向无环图与因果结构学习

    On Low Rank Directed Acyclic Graphs and Causal Structure Learning. (arXiv:2006.05691v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.05691](http://arxiv.org/abs/2006.05691)

    本文提出了利用DAG因果模型的低秩假设来解决高维情况下学习因果结构的难题，并成功地将现有的低秩技术应用到了因果结构学习中，实验表明这种方法对于稠密图的数据模型具有实用性。

    

    尽管近年来有了一些进展，但在高维情况下学习由有向无环图（DAG）表示的因果结构仍然是一项具有挑战性的任务，尤其是当要学习的图不是稀疏的情况下。在本文中，我们提出利用关于DAG因果模型的（加权）邻接矩阵的低秩假设来解决这个问题。我们利用现有的低秩技术来调整因果结构学习方法，以充分利用这个假设，并建立一些有用的结果，将可解释的图形条件与低秩假设相关联。具体而言，我们表明最大秩与中心节点高度相关，这表明在实践中经常遇到的无标度网络往往是低秩的。我们的实验证明了低秩适应对于各种数据模型的实用性，特别是对于相对大且密集的图。此外，通过验证过程，适应性方法始终保持卓越或比以前的方法更好的性能。

    Despite several advances in recent years, learning causal structures represented by directed acyclic graphs (DAGs) remains a challenging task in high dimensional settings when the graphs to be learned are not sparse. In this paper, we propose to exploit a low rank assumption regarding the (weighted) adjacency matrix of a DAG causal model to help address this problem. We utilize existing low rank techniques to adapt causal structure learning methods to take advantage of this assumption and establish several useful results relating interpretable graphical conditions to the low rank assumption. Specifically, we show that the maximum rank is highly related to hubs, suggesting that scale-free networks, which are frequently encountered in practice, tend to be low rank. Our experiments demonstrate the utility of the low rank adaptations for a variety of data models, especially with relatively large and dense graphs. Moreover, with a validation procedure, the adaptations maintain a superior or
    
[^186]: 一种带有权重聚集和动量加速的AdaGrad的统一分析

    A Unified Analysis of AdaGrad with Weighted Aggregation and Momentum Acceleration. (arXiv:1808.03408v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1808.03408](http://arxiv.org/abs/1808.03408)

    本论文提出了一种名为AdaUSM的AdaGrad变体，它采用了一种新的加权自适应学习率，可以统一AdaGrad、AccAdaGrad、Adam和RMSProp的学习率，同时通过使用统一动量方案，覆盖了重球动量和Nesterov加速梯度动量；在非凸随机设置中的收敛率为$\mathcal{O}(\log(T)/\sqrt{T})$。

    

    将自适应学习率和动量技术集成到SGD中会导致一类高效加速自适应随机算法，如AdaGrad，RMSProp，Adam，AccAdaGrad等。尽管它们在实践中有效，但它们的收敛理论仍存在很大差距，特别是在非凸随机设置中。为了填补这一差距，我们提出了“带有统一动量的加权AdaGrad”，称为AdaUSM，它具有以下主要特征：(1) 它融合了统一的动量方案，涵盖了重球动量和Nesterov加速梯度动量；(2) 它采用了一种新的加权自适应学习率，可以统一AdaGrad，AccAdaGrad，Adam和RMSProp的学习率。此外，当我们在AdaUSM中采用多项式增长的权重时，在非凸随机设置中可以得到其收敛率为$\mathcal{O}(\log(T)/\sqrt{T})$ 。我们还表明，Adam和RMSProp的自适应学习率在重加权的情况下是一致的。

    Integrating adaptive learning rate and momentum techniques into SGD leads to a large class of efficiently accelerated adaptive stochastic algorithms, such as AdaGrad, RMSProp, Adam, AccAdaGrad, \textit{etc}. In spite of their effectiveness in practice, there is still a large gap in their theories of convergences, especially in the difficult non-convex stochastic setting. To fill this gap, we propose \emph{weighted AdaGrad with unified momentum}, dubbed AdaUSM, which has the main characteristics that (1) it incorporates a unified momentum scheme which covers both the heavy ball momentum and the Nesterov accelerated gradient momentum; (2) it adopts a novel weighted adaptive learning rate that can unify the learning rates of AdaGrad, AccAdaGrad, Adam, and RMSProp. Moreover, when we take polynomially growing weights in AdaUSM, we obtain its $\mathcal{O}(\log(T)/\sqrt{T})$ convergence rate in the non-convex stochastic setting. We also show that the adaptive learning rates of Adam and RMSPro
    

