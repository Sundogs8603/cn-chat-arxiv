# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Tackling Unbounded State Spaces in Continuing Task Reinforcement Learning.](http://arxiv.org/abs/2306.01896) | 本文提出了一种基于李雅普诺夫思想的奖励塑形方法，用于在持续任务的强化学习中应对无界状态空间，旨在鼓励代理器学习稳定性和最优策略。 |
| [^2] | [Hierarchical Quadratic Random Forest Classifier.](http://arxiv.org/abs/2306.01893) | 本文提出了一种用于对多通道数据中的多分辨率样本进行分类的分层二次随机森林分类器。该分类器采用了惩罚的多元线性判别建立决策节点，并且使用组 Lasso 正则化器对特征进行处理。该分类器可以独立使用或者运用于基于图的分类器中。 |
| [^3] | [Kernel Metric Learning for Clustering Mixed-type Data.](http://arxiv.org/abs/2306.01890) | 提出了一种使用混合核测量不相似性的度量方法，并通过交叉验证找到最佳核带宽。该方法可为现有的基于距离的聚类算法提高聚类准确度，适用于包含混合类型数据的模拟和实际数据集。 |
| [^4] | [Multifunctionality in a Connectome-Based Reservoir Computer.](http://arxiv.org/abs/2306.01885) | 本文通过将果蝇侧角的连接组学移植到储层计算机中，探究其展示出的多功能性，并在“看见两倍”的问题上表现出更大的多功能性能力和解决能力。 |
| [^5] | [DiffECG: A Generalized Probabilistic Diffusion Model for ECG Signals Synthesis.](http://arxiv.org/abs/2306.01875) | 本文介绍了一种新颖的ECG信号合成方法——基于去噪扩散概率模型的DiffECG，能够涵盖三种情形，并且是ECG合成的第一个广义条件方法。实验证明该方法的有效性以及优于其他ECG生成模型并可提高分类器性能。 |
| [^6] | [SACSoN: Scalable Autonomous Data Collection for Social Navigation.](http://arxiv.org/abs/2306.01874) | 本文介绍了一个名为SACSoN的自主导航机器人系统，可以在人类占用的现实场景中，通过视觉理解和学习，自主收集数据，实现更好的数据集拓展。 |
| [^7] | [Layer-Wise Feedback Alignment is Conserved in Deep Neural Networks.](http://arxiv.org/abs/2306.01870) | 本论文揭示了层间反馈对齐在深度神经网络中的保守性，并发现FA与GD之间存在隐式偏差的相似之处，同时阐明了ReLU网络中与反馈矩阵对齐的充分条件。 |
| [^8] | [Fast $(1+\varepsilon)$-Approximation Algorithms for Binary Matrix Factorization.](http://arxiv.org/abs/2306.01869) | 本论文提出了一种高效的 $(1+\varepsilon)$ 近似算法，用于二进制矩阵分解问题。该算法在 running time 方面是单指数级别的，并且可用于多个变体。 |
| [^9] | [Discovering COVID-19 Coughing and Breathing Patterns from Unlabeled Data Using Contrastive Learning with Varying Pre-Training Domains.](http://arxiv.org/abs/2306.01864) | 本文提出了一种基于对比学习的建模方法，用于从非COVID咳嗽中发现COVID-19的咳嗽和呼吸模型。该模型的准确率高达0.81和0.86，有助于未来尽早发现新疾病的爆发。 |
| [^10] | [No Bidding, No Regret: Pairwise-Feedback Mechanisms for Digital Goods and Data Auctions.](http://arxiv.org/abs/2306.01860) | 本文提出了针对重复拍卖设置的对偶反馈机制，使用成对比较来从竞标者那里获取信息，避免了之前的学习出价问题，该机制被证明为渐近诚实、个体理性、福利和收益最大化的且适用于任何需要定制生产的商品拍卖场景。 |
| [^11] | [5IDER: Unified Query Rewriting for Steering, Intent Carryover, Disfluencies, Entity Carryover and Repair.](http://arxiv.org/abs/2306.01855) | 本文介绍了一种非自回归的查询重写体系结构，该体系不仅可以处理对话引导、意图携带、语法中断、实体携带和修复这五个任务，还可以处理它们的复杂组合。 |
| [^12] | [Reinforcement Learning with General Utilities: Simpler Variance Reduction and Large State-Action Space.](http://arxiv.org/abs/2306.01854) | 本文提出了一种适用于通用效用的强化学习算法，具有更简单、无参数的归一化策略梯度算法。算法包括递归动量方差缩减机制，针对大有限状态行动空间提出了线性函数逼近方法，具有较低的样本复杂度。 |
| [^13] | [Maximum Likelihood Training of Autoencoders.](http://arxiv.org/abs/2306.01843) | 本文介绍了一种成功的最大似然训练方法，用于非约束自编码器，将生成建模的优异性质与高效自编码器相结合。作者克服了两个挑战：设计了消除迭代的估计器并提出了稳定的最大似然训练目标。实验证明这种方法可以成功训练一系列非约束性自编码器，并取得了有竞争力的性能。 |
| [^14] | [Efficient Multi-Task and Transfer Reinforcement Learning with Parameter-Compositional Framework.](http://arxiv.org/abs/2306.01839) | 本文提出了一种基于参数组合的转移方法，可以在多任务训练阶段提高性能，并在各种操作任务上展示了有效的迁移能力。 |
| [^15] | [Enhancing the Protein Tertiary Structure Prediction by Multiple Sequence Alignment Generation.](http://arxiv.org/abs/2306.01824) | 本文介绍了一种利用生成语言模型和大规模多序列比对生成新蛋白质序列的方法，用于补充没有丰富同源家族数据库的蛋白质序列，增强蛋白质结构预测的准确性。 |
| [^16] | [ErfReLU: Adaptive Activation Function for Deep Neural Network.](http://arxiv.org/abs/2306.01822) | 本论文提出了一种基于误差函数和ReLU的新型激活函数'ErfReLU'，可以自适应地随着学习而训练，适用于各项任务。 |
| [^17] | [Concurrent Classifier Error Detection (CCED) in Large Scale Machine Learning Systems.](http://arxiv.org/abs/2306.01820) | 本文介绍了一种名为并发分类器错误检测 (CCED) 的方案，该方案使用并发机器学习分类器来检测大规模机器学习系统中的错误，而不需要额外硬件或软件开销。 |
| [^18] | [Beta Thalassemia Carriers detection empowered federated Learning.](http://arxiv.org/abs/2306.01818) | 本论文介绍了一种使用联邦学习技术的方法，快速、便宜、无需移动设备地检测Beta地中海贫血携带者。 |
| [^19] | [Heart Diseases Prediction Using Block-chain and Machine Learning.](http://arxiv.org/abs/2306.01817) | 本文研究了使用区块链和机器学习算法预测心脏疾病，并解决了现有架构下数据存储和传输的不安全性问题，有望优化心脏专业人士对于心脏病早期诊断的能力。 |
| [^20] | [Prediction of Citrus Diseases Using Machine Learning And Deep Learning: Classifier, Models SLR.](http://arxiv.org/abs/2306.01816) | 本文讨论了预测柑橘病害的方法，介绍了使用机器学习和深度学习的分类器和模型，并探讨了相关防治策略。 |
| [^21] | [Fast Interactive Search with a Scale-Free Comparison Oracle.](http://arxiv.org/abs/2306.01814) | 该论文提出了一种基于比较的快速交互式搜索算法，其中使用了称为 $\gamma$-CKL 的刻度自由概率预言模型，并开发了一种具有指数收敛速度的搜索算法。 |
| [^22] | [Learning the effective order of a hypergraph dynamical system.](http://arxiv.org/abs/2306.01813) | 本文提出了一种方法以确定超图动力系统的最小顺序，以精确地近似相应的动力学，并通过超图神经网络来直接学习动力学和超图的顺序。 |
| [^23] | [SAPI: Surroundings-Aware Vehicle Trajectory Prediction at Intersections.](http://arxiv.org/abs/2306.01812) | 本文提出了一种称为SAPI的深度学习模型，用于在路口预测车辆轨迹，通过实时地图、优先权和周围交通信息来表示和编码周围环境。SAPI能够在预测车辆轨迹时表现出有希望的性能，且优于基准方法。 |
| [^24] | [DVFO: Dynamic Voltage, Frequency Scaling and Workload Offloading for DNN Edge Inference.](http://arxiv.org/abs/2306.01811) | 提出了一种动态电压、频率缩放和工作负载卸载的DNN边缘推理框架DVFO，它通过深度强化学习来联合优化DVFS和卸载参数，实现了对边缘设备计算资源的优化，同时提高了DNN模型的能源效率。 |
| [^25] | [Adversarial Attack Based on Prediction-Correction.](http://arxiv.org/abs/2306.01809) | 本论文介绍了一种基于预测-校正的对抗攻击方法，该方法能够有效对抗深度神经网络中的梯度攻击，并具有较好的可扩展性。 |
| [^26] | [Extracting Reward Functions from Diffusion Models.](http://arxiv.org/abs/2306.01804) | 本论文提出了一种从两个Diffusion模型中提取奖励函数的实用学习算法，可以在导航环境中找到正确的奖励函数，并以此控制Diffusion模型学习足够复杂的任务。 |
| [^27] | [Linear Time GPs for Inferring Latent Trajectories from Neural Spike Trains.](http://arxiv.org/abs/2306.01802) | 本论文提出了cvHM，一种使用Hida-Mat'ern核和共轭计算变分推理（CVI）的潜在高斯过程模型的通用推理框架，能够以线性时间复杂度执行潜在神经轨迹的变分推断，以适应任意的似然函数。 |
| [^28] | [Pairwise Ranking Losses of Click-Through Rates Prediction for Welfare Maximization in Ad Auctions.](http://arxiv.org/abs/2306.01799) | 本文提出了一种新的加权排名损失函数来训练CTR模型，以实现广告拍卖中的福利最大化，并且没有假设eCPM的先验分布，同时避免了朴素地应用现有学习排名方法的问题。 |
| [^29] | [DiffPack: A Torsional Diffusion Model for Autoregressive Protein Side-Chain Packing.](http://arxiv.org/abs/2306.01794) | 本文提出了DiffPack，这是一个自回归的扭转扩散模型，通过在扭曲空间中进行扩散和去噪来学习侧链扭转角度的联合分布，从而准确地预测蛋白质侧链的构象，有效应用于蛋白质结构预测、设计和蛋白质相互作用等领域。 |
| [^30] | [Task Relation-aware Continual User Representation Learning.](http://arxiv.org/abs/2306.01792) | 本文提出了一种新的持续用户表示学习方法TERACON，它能够学习通用的用户表示，而不是为每个任务学习任务特定的用户表示，具有很强的实用性和学习能力。 |
| [^31] | [Responsible Design Patterns for Machine Learning Pipelines.](http://arxiv.org/abs/2306.01788) | 本文提出了一种综合框架，将负责任设计模式纳入机器学习流程中，以确保AI系统的伦理性和公正性。这个框架包括新的负责任AI设计模式，并指导AI开发人员、数据科学家和决策者在AI开发和部署中实施伦理实践。 |
| [^32] | [Beyond Rankings: Exploring the Impact of SERP Features on Organic Click-through Rates.](http://arxiv.org/abs/2306.01785) | 本研究探讨了SERP特征对点击率的影响，揭示SERP特征不仅是美学成分，而且强烈影响点击率和互联网用户的相关行为，能够显着调节网络流量。 |
| [^33] | [Generative Diffusion for 3D Turbulent Flows.](http://arxiv.org/abs/2306.01776) | 该论文提出了一种生成模型，可以在任意三维空间中模拟湍流现象，避免了湍流流动的不可预测性，能够快速生成高质量的流场。 |
| [^34] | [A Quantitative Review on Language Model Efficiency Research.](http://arxiv.org/abs/2306.01768) | 这篇论文从定量方面对基于Transformers和State Space Models的一系列研究进行了综述，并总结了如何提高语言模型效率。 |
| [^35] | [Pre-trained transformer for adversarial purification.](http://arxiv.org/abs/2306.01762) | 本文提出了一个快速防御对抗性攻击的方案RaPiD（Rapid Plug-in Defender），通过预训练的Transformer微调来提纯对抗样本，使其逼近清洁数据分布，实验结果表明，在有限数据情况下，该方法优于最先进的方法。 |
| [^36] | [Distinguishing Human Generated Text From ChatGPT Generated Text Using Machine Learning.](http://arxiv.org/abs/2306.01761) | 本文提出了一种机器学习方法，可以识别出由ChatGPT生成的文本，并以11种算法进行了分类对比分析，在测试中取得了77%的准确度。 |
| [^37] | [Transformer-based Vulnerability Detection in Code at EditTime: Zero-shot, Few-shot, or Fine-tuning?.](http://arxiv.org/abs/2306.01754) | 本研究使用深度学习在编辑代码的同时检测漏洞，可以高精度、低延迟地检测超过250种漏洞类型的复杂漏洞代码模式，使软件开发人员能够在引入潜在漏洞到代码库之前修复它们。 |
| [^38] | [Handling Label Uncertainty on the Example of Automatic Detection of Shepherd's Crook RCA in Coronary CT Angiography.](http://arxiv.org/abs/2306.01752) | 本论文提出了一种利用一维卷积神经网络在冠状动脉 CT 血管造影中自动检测牧羊人钩形右冠状动脉的方法，并探讨了如何处理标签不确定性的问题。 |
| [^39] | [Differential Privacy with Random Projections and Sign Random Projections.](http://arxiv.org/abs/2306.01751) | 本文提出了一系列差分隐私算法，其中iDP-SignRP算法在个体差分隐私设置下效果显著，DP-SignOPORP算法改进了现有算法，DP-OPORP算法表现最优，iDP提供了一种适用于特定数据集的隐私保护解决方案。 |
| [^40] | [Disproving XAI Myths with Formal Methods -- Initial Results.](http://arxiv.org/abs/2306.01744) | 论文介绍了XAI中最严重的一些误解，并展示了如何使用形式化方法来打破这些谬见和开发更实用的替代方案。 |
| [^41] | [Beyond Negativity: Re-Analysis and Follow-Up Experiments on Hope Speech Detection.](http://arxiv.org/abs/2306.01742) | 本研究旨在找到计算效率高、可比或更好的希望言论检测方法，并公开了代码库。 |
| [^42] | [Local Message Passing on Frustrated Systems.](http://arxiv.org/abs/2306.01494) | 本文提出了一种优化的基于困难系统的局部消息传递算法，能够在循环图上获得良好的表现。 |
| [^43] | [Generalist Equivariant Transformer Towards 3D Molecular Interaction Learning.](http://arxiv.org/abs/2306.01474) | 本文提出了一种通用等变Transformer用于学习3D分子相互作用，该模型具有双层注意力模块、前馈模块和层归一化模块，每个模块都是E（3）等变的，可以有效地捕捉块级和原子级的交互，实验结果表明其在预测蛋白质-蛋白质亲和力、配体结合亲和力和配体效力方面优于各种最先进的方法。 |
| [^44] | [Robust and Generalisable Segmentation of Subtle Epilepsy-causing Lesions: a Graph Convolutional Approach.](http://arxiv.org/abs/2306.01375) | 该论文提出了一种通过图卷积网络进行语义分割的方法，能够对细微的癫痫病灶进行识别，降低误报率。 |
| [^45] | [Differentially Private Episodic Reinforcement Learning with Heavy-tailed Rewards.](http://arxiv.org/abs/2306.01121) | 本研究针对重尾奖励的有限步骤表格马尔可夫决策过程问题探讨了差分隐私限制下的两种框架，即价值迭代和策略优化，同时考虑了联合差分隐私和本地差分隐私模型，并为两种情况提供了遗憾上限。 |
| [^46] | [SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds.](http://arxiv.org/abs/2306.00980) | 本文提出了一种通用方法，首次在移动设备上运行文本到图像扩散模型不到2秒，这是通过引入高效的网络架构和改进步骤蒸馏来实现的。 |
| [^47] | [STEVE-1: A Generative Model for Text-to-Behavior in Minecraft.](http://arxiv.org/abs/2306.00937) | STEVE-1 是一种新的生成模型，能够在Minecraft中跟随各种短期开放型文本和视觉指令。STEVE-1利用预先训练的模型和最佳实践，通过自监督的行为克隆和回顾重新标记来微调，避免了昂贵的人工注释。 |
| [^48] | [Class Anchor Margin Loss for Content-Based Image Retrieval.](http://arxiv.org/abs/2306.00630) | 本论文提出一种新颖的斥力-吸引力损失函数，该函数位于度量学习范式中，可以直接优化L2度量，无需生成成对，在多个数据集上的实验表明，在检索准确性和效率方面，该方法优于现有技术。 |
| [^49] | [Predicting Temporal Aspects of Movement for Predictive Replication in Fog Environments.](http://arxiv.org/abs/2306.00575) | 本文研究了时间预测在雾环境下预测复制中的应用，提出了一种利用Holder-Winter指数平滑法进行时间预测的新模型，可以在减少多余数据的同时，只有微小的数据可用性降低。 |
| [^50] | [Reconstructing Graph Diffusion History from a Single Snapshot.](http://arxiv.org/abs/2306.00488) | 本文研究了从单个快照中重建图扩散历史的问题，揭示了现有方法的局限性，并提出了一种新的方法。 |
| [^51] | [The Canadian Cropland Dataset: A New Land Cover Dataset for Multitemporal Deep Learning Classification in Agriculture.](http://arxiv.org/abs/2306.00114) | 该论文提出了一个时间补丁数据集，包含了加拿大农田的多时相遥感影像。该数据集是手动经过确认和筛选的高分辨率地理参考图像，覆盖四个农作物生产年度和五个月份。这个数据集可以用于提高土地覆盖分类的准确性。 |
| [^52] | [A Novel Black Box Process Quality Optimization Approach based on Hit Rate.](http://arxiv.org/abs/2305.20003) | 该论文提出了一种基于数据驱动的准凸方法，将原始的非凸问题转化为一组凸可行问题，以实现最佳命中率。 在钢铁生产的实际实验中，该方法优于传统模型，使命中率提高至少41.11％和31.01％。 |
| [^53] | [Vandermonde Neural Operators.](http://arxiv.org/abs/2305.19663) | 本文提出了一种新的神经网络结构，Vandermonde神经算子，能够有效地处理非均匀分布点上的输入数据，同时在速度和准确性上相较于以前的方法有所提升。 |
| [^54] | [Traffic Prediction using Artificial Intelligence: Review of Recent Advances and Emerging Opportunities.](http://arxiv.org/abs/2305.19591) | 该论文综述了交通预测方法的发展，重点介绍了基于人工智能的交通预测方法在多元交通时间序列模型研究方面的进展和机遇。 |
| [^55] | [Spotlight Attention: Robust Object-Centric Learning With a Spatial Locality Prior.](http://arxiv.org/abs/2305.19550) | 该论文提出了一个新的目标中心学习方法，通过加入空间局部性先验来提高模型的鲁棒性，使模型在合成和真实数据上实现了显著的物体分割改进，并且对模型超参数不太敏感。 |
| [^56] | [Criteria Tell You More than Ratings: Criteria Preference-Aware Light Graph Convolution for Effective Multi-Criteria Recommendation.](http://arxiv.org/abs/2305.18885) | 本文提出了一种面向多准则推荐的标准偏好感知轻量图卷积网络，该方法结合了MC扩展图，可以准确地捕捉用户的标准偏好，并进一步将用户对各个标准的偏好合并到最终的推荐列表中。 |
| [^57] | [Centralised rehearsal of decentralised cooperation: Multi-agent reinforcement learning for the scalable coordination of residential energy flexibility.](http://arxiv.org/abs/2305.18875) | 本文研究了基于多智能体强化学习的住宅能量灵活性的可扩展且保护隐私的协调方法，使用“集中化但分解的评论家”在执行前进行协调排练，实现了大规模协调。 |
| [^58] | [A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets.](http://arxiv.org/abs/2305.18486) | 本文对基准数据集上 ChatGPT 的性能进行了全面的评估，包括问答、文本摘要、代码生成、常识推理、数学问题求解、机器翻译、偏见检测和伦理考虑等任务。研究旨在验证 ChatGPT 的优势和弱点，并为使用语言模型的未来研究提供见解。 |
| [^59] | [Beyond the Meta: Leveraging Game Design Parameters for Patch-Agnostic Esport Analytics.](http://arxiv.org/abs/2305.18477) | 本论文提出了一种新的跨版本的电子竞技分析方法，通过利用游戏设计参数并利用聚类技术创建角色表征形式来解决传统方法短寿命的问题。以Dota 2为例验证了这种方法，取得了显著的性能提升。 |
| [^60] | [Continual Task Allocation in Meta-Policy Network via Sparse Prompting.](http://arxiv.org/abs/2305.18444) | 本文提出的CoTASP可以通过学习过完备字典来生成稀疏掩码作为提示，从而从元策略网络中提取与每个任务相关的子网络，实现了快速适应新任务，同时保留了之前任务的共同知识。 |
| [^61] | [Neural Sculpting: Uncovering hierarchically modular task structure through pruning and network analysis.](http://arxiv.org/abs/2305.18402) | 本文提出了一种名为“神经雕塑”的方法，该方法通过神经网络的修剪和分析生成的图形结构来揭示任务的子函数的层次结构。该方法在布尔任务上得到了有效验证。 |
| [^62] | [On Optimal Regularization Parameters via Bilevel Learning.](http://arxiv.org/abs/2305.18394) | 本文提出了一个利用双层学习确定正则化参数的方法，并提出了一个新的表征正确参数的条件。 |
| [^63] | [In-Context Analogical Reasoning with Pre-Trained Language Models.](http://arxiv.org/abs/2305.17626) | 本研究提出了一种基于语言模型的情境类比推理方法，通过将问题的感知特征编码成语言形式，能够实现高效的零-shot关系推理，超越传统方法和人类水平。 |
| [^64] | [Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark.](http://arxiv.org/abs/2305.17553) | 该论文探讨了大型语言模型编辑技术的现状，发现现有的特异性基准测试难以检测到不良副作用，并提出了一个改进的基准测试，该测试可以检测到动态组件，并通过基于KL散度的指标扩展了特异性的衡量方式。研究发现最近的模型编辑技术特异性较低，强调了改进特异性基准测试的重要性。 |
| [^65] | [Modeling Dynamic Environments with Scene Graph Memory.](http://arxiv.org/abs/2305.17537) | 本论文提出了一种新的场景图记忆状态表示，结合节点边缘预测器（NEP）的神经网络架构，能够帮助具有行动能力的AI代理在部分可观察动态场景中高效搜索。 |
| [^66] | [PFNs Are Flexible Models for Real-World Bayesian Optimization.](http://arxiv.org/abs/2305.17535) | 本文使用灵活的PFN作为BO代理建模，该模型能够允许进一步信息纳入以进行非远视BO。在三种不同的问题上得到了很好的结果。 |
| [^67] | [Distilling BlackBox to Interpretable models for Efficient Transfer Learning.](http://arxiv.org/abs/2305.17303) | 本论文提出了一种方法可以将黑盒模型转化成为可解释模型并在目标领域成功进行迁移学习，从而实现高效迁移学习。 |
| [^68] | [Selective Mixup Helps with Distribution Shifts, But Not (Only) because of Mixup.](http://arxiv.org/abs/2305.16817) | 选择性mixup通过非随机选择对提高训练分布，实现标签偏移的经典解决方案，从而提高了神经网络的泛化性能。 |
| [^69] | [Detecting Errors in Numerical Data via any Regression Model.](http://arxiv.org/abs/2305.16583) | 该论文提出了一种模型不可知的方法，通过考虑各种不确定性，可以利用任何回归器检测数值数据中的异常值与自然数据波动，能够有效区分真正的异常和自然数据波动。 |
| [^70] | [Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer.](http://arxiv.org/abs/2305.16380) | 本文分析了1层Transformer在下一个标记预测任务中的SGD训练动态，证明了自我关注层充当了“区分性扫描算法”，从而逐步关注到相关标记并排除不相关的标记，总结相关信息在编码表示中。同时研究了标记频率、上下文和初始化自我关注层等对Transformer性能的影响。 |
| [^71] | [Candidate Set Re-ranking for Composed Image Retrieval with Dual Multi-modal Encoder.](http://arxiv.org/abs/2305.16304) | 本论文提出了一种使用两阶段模式结合预先计算图像嵌入和参考文本-候选项三元组交互选择的方式进行组合图像检索候选集重排序的方法。 |
| [^72] | [Exploiting Noise as a Resource for Computation and Learning in Spiking Neural Networks.](http://arxiv.org/abs/2305.16044) | 本文提出了噪声脉冲神经元网络（NSNN）和噪声驱动学习规则（NDL），展示了噪声可以作为计算和学习的资源，并为一般脉冲神经元网络提供了一个框架。研究还展示了NSNNs在图像分类和语音识别等实际任务中的适用性，表明它们是未来神经形态计算系统的潜在有力工具。 |
| [^73] | [Exponential Smoothing for Off-Policy Learning.](http://arxiv.org/abs/2305.15877) | 本文研究了离线学习中最小化风险的倒数倾向评分(IPS)的平滑正则化，推导出了可处理、可扩展、可解释的学习证明，并确定了在何种情况下不需要正则化IPS。 |
| [^74] | [Transcending Grids: Point Clouds and Surface Representations Powering Neurological Processing.](http://arxiv.org/abs/2305.15426) | 本文提出了一种基于无结构点云的新方法，将基于格点的数据转换为其更高维度表达，以提高医疗图像分类的性能。 |
| [^75] | [Theoretically Principled Federated Learning for Balancing Privacy and Utility.](http://arxiv.org/abs/2305.15148) | 本文提出基于扭曲模型参数的保护机制的通用学习框架，用于实现联合学习中隐私保护和数据效用的平衡。算法可以在每个通信轮中实现个性化的效用-隐私折衷，我们在理论上证明了算法的次线性性质，该算法可以提高隐私保护的联合学习效力。 |
| [^76] | [Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding.](http://arxiv.org/abs/2305.14449) | 一种协同过滤新方法用于稳健对话理解，在历史用户-实体交互的基础上，利用多跳客户亲和力丰富每个用户的索引，并使用有限内存BFGS算法调整每个索引的权重，实验结果显示其明显优于最先进的个性化查询重写方法。 |
| [^77] | [Layer-wise Adaptive Step-Sizes for Stochastic First-Order Methods for Deep Learning.](http://arxiv.org/abs/2305.13664) | 本文提出了一种针对深度学习的随机一阶优化方法的分层自适应步长策略，通过利用深度神经网络中浅层的随机曲率信息为每一层计算自适应步长，消除了用户调整学习率的需求。实验结果显示，结合该策略的算法在DNN任务的训练中优于精细调整学习率版本以及流行的一阶和二阶算法。 |
| [^78] | [INVICTUS: Optimizing Boolean Logic Circuit Synthesis via Synergistic Learning and Search.](http://arxiv.org/abs/2305.13164) | INVICTUS是一个使用离线强化学习和搜索的模型，自动生成逻辑最小化启发式算法综合配方以优化电路面积和时延等指标。 |
| [^79] | [Many or Few Samples? Comparing Transfer, Contrastive and Meta-Learning in Encrypted Traffic Classification.](http://arxiv.org/abs/2305.12432) | 该论文比较了迁移学习、元学习和对比学习等方法在加密流量分类中的效果，发现对比学习是最优的方法，并证明通过重复利用学习到的表示，DL方法可以接近基于树的模式。 |
| [^80] | [Learning Joint 2D & 3D Diffusion Models for Complete Molecule Generation.](http://arxiv.org/abs/2305.12347) | 本文提出了一个新的联合2D和3D扩散模型，能够生成具有原子类型、化学键信息和3D坐标的完整分子，并能捕捉2D键合图和3D分子几何的联合分布。 |
| [^81] | [Learning Activation Functions for Sparse Neural Networks.](http://arxiv.org/abs/2305.10964) | 本论文针对稀疏神经网络的准确性下降问题，发现激活函数和超参数是导致问题的主要原因，提出学习为稀疏网络调整激活函数并分开超参数优化方案的解决方法。 |
| [^82] | [ProgSG: Cross-Modality Representation Learning for Programs in Electronic Design Automation.](http://arxiv.org/abs/2305.10838) | 该论文介绍了一种跨模态表征学习方法，用于电子自动化设计中高级综合工具的优化，并促进领域特定加速器（DSAs）的设计自动化化。 |
| [^83] | [Extracting Low-/High- Frequency Knowledge from Graph Neural Networks and Injecting it into MLPs: An Effective GNN-to-MLP Distillation Framework.](http://arxiv.org/abs/2305.10758) | 本文提出了一种有效的GNN-to-MLP蒸馏框架，将GNNs中的低/高频知识注入MLP。通过将GNNs学习到的知识分解为低/高频成分，在空间域中推导它们的对应关系。此外，还解决了现有GNN-to-MLP蒸馏中的信息淹没问题。 |
| [^84] | [Incremental Causal Graph Learning for Online Unsupervised Root Cause Analysis.](http://arxiv.org/abs/2305.10638) | 本文提出了CORAL，一种用于在线无监督根本原因分析的新框架，可以自动触发该过程并增量更新模型，包括三个主要部分：触发点检测，增量因果图学习和基于网络传播的根本原因定位。 |
| [^85] | [Lingo3DMol: Generation of a Pocket-based 3D Molecule using a Language Model.](http://arxiv.org/abs/2305.10133) | 本文提出了一种基于口袋的三维分子生成方法，利用扰动和恢复预训练任务和一种新的分子表示形式。 该方法结合了语言模型和几何深度学习的优点，使得语言模型可以生成准确的三维分子。 |
| [^86] | [Characterizing Long-Tail Categories on Graphs.](http://arxiv.org/abs/2305.09938) | 该研究提出了图上长尾分类的第一个泛化边界，并提出了一种可表征长尾类别的行为并提高机器学习模型在现实世界网络中的泛化性能的新图表示学习框架。 |
| [^87] | [Off-Policy Evaluation for Large Action Spaces via Conjunct Effect Modeling.](http://arxiv.org/abs/2305.08062) | 本研究提出了一个称为OffCEM的估计器，用于对大离散动作空间下上下文匹配策略进行离线策略评估。该估计器通过基于模型的奖励估计来处理残余因果效应，并在新的本地正确性条件下保持无偏性。结果表明，OffCEM在合成和实际大动作空间数据集上优于现有方法。 |
| [^88] | [Provably Convergent Schr\"odinger Bridge with Applications to Probabilistic Time Series Imputation.](http://arxiv.org/abs/2305.07247) | 本论文提出了一种基于近似投影的Schr\"odinger bridge算法，它能够应用于概率时间序列填充，并在医疗保健和环境数据方面实现最先进的结果。 |
| [^89] | [ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph Completion.](http://arxiv.org/abs/2305.06395) | 本文提出了一种名为ACTC的方法，在冷启动知识图谱补全时进行主动阈值校准，可以有效地利用有限的标记元组来找到每个关系的最佳阈值，同时也结合未标记元组进行了实验。 |
| [^90] | [K-SpecPart: A Supervised Spectral Framework for Multi-Way Hypergraph Partitioning Solution Improvement.](http://arxiv.org/abs/2305.06167) | 本论文提出了一个名为K-SpecPart的超图划分算法，通过解决广义特征值问题，捕捉平衡分区目标和全局超图结构，在多元划分中提高算法质量。 |
| [^91] | [Scalable Optimal Margin Distribution Machine.](http://arxiv.org/abs/2305.04837) | 本文提出了一种可扩展的最优边缘分布机（ODM）训练方法，与原始ODM训练方法相比，可实现近十倍的加速。对于非线性核，我们提出了一种新颖的分布感知分区方法，使得每个分区上训练的本地ODM接近全局的ODM，并快速收敛。在应用线性核时，我们扩展了一种通信有效的SVRG方法以进一步加速训练。 |
| [^92] | [Building Neural Networks on Matrix Manifolds: A Gyrovector Space Approach.](http://arxiv.org/abs/2305.04560) | 本文通过将陀螺矢量空间中的一些概念推广到SPD和Grassmann流形，提出了在这些流形上构建神经网络的新模型和新层，并在人类动作识别和知识图谱完成两个应用中展示了其有效性。 |
| [^93] | [Disentangled Multi-Fidelity Deep Bayesian Active Learning.](http://arxiv.org/abs/2305.04392) | 本论文提出了一种新的框架D-MFDAL，它可以学习具有多个保真度的函数分布，相比于现有基于高斯过程的方法，该方法具有更好的性能和高斯过程的灵活性和可解释性。 |
| [^94] | [Neuro-symbolic model for cantilever beams damage detection.](http://arxiv.org/abs/2305.03063) | 本文提出了一种神经符号模型用于悬臂梁损伤检测，该模型通过将卷积网络的处理能力与逻辑查询交互控制相结合，不仅能够准确检测损伤，而且还能够提供解释和定位，使其在操作条件下更可靠和可信。 |
| [^95] | [BitGNN: Unleashing the Performance Potential of Binary Graph Neural Networks on GPUs.](http://arxiv.org/abs/2305.02522) | 本文提出了一种从效率角度重新设计的二进制GNN推理后端算法，用于充分发挥GPU上位操作的特性，实验结果表明提出的算法比最先进的二进制GNN实现提高了8-22倍的性能，保持相同准确性。 |
| [^96] | [Clinical Note Generation from Doctor-Patient Conversations using Large Language Models: Insights from MEDIQA-Chat.](http://arxiv.org/abs/2305.02220) | 本文介绍了使用大型语言模型从医生-患者对话中自动生成临床笔记的研究，采用少样本上下文学习法所生成笔记表现优秀，且可与人工编写的笔记媲美。 |
| [^97] | [Finding Neurons in a Haystack: Case Studies with Sparse Probing.](http://arxiv.org/abs/2305.01610) | 本文通过训练$k$-稀疏线性分类器以预测输入特征是否存在，研究了大型语言模型（LLM）内部神经元激活的表示方式；对不同层次的神经元网络的研究表明，早期层利用神经元的稀疏组合来表示多种特征，中间层有特定的神经元表示高级上下文特征，增加规模使特征表示更加稀疏化。 |
| [^98] | [Model-agnostic Measure of Generalization Difficulty.](http://arxiv.org/abs/2305.01034) | 该论文提出了第一个无特定模型的、量化机器学习测试泛化难度的方法——归纳偏差复杂度度量。该方法量化了在任务上良好泛化所需的总信息量与数据提供的信息量之差，通常需要在许多维度上泛化的任务比涉及更少维度但要求更多细节的任务要困难得多。 |
| [^99] | [Dynamic Transfer Learning across Graphs.](http://arxiv.org/abs/2305.00664) | 该论文提出了一个新的问题：在动态图形环境下如何有效地进行跨图迁移学习，主要解决了领域演化对泛化性能的影响。 |
| [^100] | [Discover and Cure: Concept-aware Mitigation of Spurious Correlation.](http://arxiv.org/abs/2305.00650) | 本研究提出了一个可解释的方法框架(DISC)来抑制深度神经网络中的假相关，通过发现不稳定的概念并将其作为假属性干预训练数据来提高模型的泛化能力和可解释性。在目标识别任务中，DISC胜过了现有最先进的方法。 |
| [^101] | [NNSplitter: An Active Defense Solution to DNN Model via Automated Weight Obfuscation.](http://arxiv.org/abs/2305.00097) | NNSplitter是一种主动保护深度神经网络模型知识产权的方案，通过将模型分为混淆模型和模型秘密两部分，采用可信执行环境和基于强化学习的控制器来最大化精度下降和减少混淆权重的数量。 |
| [^102] | [Using Perturbation to Improve Goodness-of-Fit Tests based on Kernelized Stein Discrepancy.](http://arxiv.org/abs/2304.14762) | 本文提出了一种通过在样本中引入扰动，改进基于核化斯坦距的拟合优度检验方法的方法，以解决在同质但混合比例不同的情况下低功率的问题，并展示实验证据证明了该方法的功效。 |
| [^103] | [Convergence of Adam Under Relaxed Assumptions.](http://arxiv.org/abs/2304.13972) | 本文对Adam算法做了新的假设并进行了证明，证明了在更加现实的条件下，Adam能够以较小的梯度复杂度达到稳定点。 |
| [^104] | [Joint Age-based Client Selection and Resource Allocation for Communication-Efficient Federated Learning over NOMA Networks.](http://arxiv.org/abs/2304.08996) | 本文针对联邦学习在无线网络上通信受限、收敛速度慢和资源有限等问题，提出了一种基于年龄和资源分配的客户端选择方案，旨在最小化每轮联邦学习的总时间消耗，从而提高联邦学习的性能。 |
| [^105] | [Safe reinforcement learning with self-improving hard constraints for multi-energy management systems.](http://arxiv.org/abs/2304.08897) | 本论文提出了一种安全强化学习方法，能够实现多能源管理系统中的最优控制，在保证硬约束的前提下减少工程工作，降低建模偏差，并避免潜在的不安全行为。 |
| [^106] | [Label-Free Concept Bottleneck Models.](http://arxiv.org/abs/2304.06129) | 无标签CBM是一个可解释的框架，能够将任何神经网络转化为CBM，并且不需要标记数据，同时保持高准确性。 |
| [^107] | [Exploring the Connection between Robust and Generative Models.](http://arxiv.org/abs/2304.04033) | 本文探究鲁棒性判别分类器与生成模型之间的联系，并发现在输入空间中，非定向对抗点非常可能在鉴别性模型中隐含的生成模型中拥有低能量，提出了一种名为高能量PGD的新攻击。 |
| [^108] | [AutoRL Hyperparameter Landscapes.](http://arxiv.org/abs/2304.02396) | 本文提出了一种方法，在训练期间多次建立和分析AutoRL超参数的景观，证明代表算法（DQN和SAC）在不同环境下的超参数景观会随时间而变化。 |
| [^109] | [Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning.](http://arxiv.org/abs/2304.01203) | 本文介绍了一种新的强化学习方法——准度量强化学习（QRL），利用准度量模型来学习最优价值函数；在离线和在线的目标达成基准测试中，QRL展示了更好的采样效率和性能，包括基于状态和基于图像的观测。 |
| [^110] | [BOLT: An Automated Deep Learning Framework for Training and Deploying Large-Scale Neural Networks on Commodity CPU Hardware.](http://arxiv.org/abs/2303.17727) | BOLT是一种用于在标准CPU硬件上训练大型神经网络模型的稀疏深度学习库，它提供了一个灵活的高级API，使用户可以构建模型并抽象掉稀疏网络训练的算法细节。 |
| [^111] | [Neural signature kernels as infinite-width-depth-limits of controlled ResNets.](http://arxiv.org/abs/2303.17671) | 通过控制ResNets的欧拉离散化，提出了一种新的家族限制核，称为神经签名核。在无限深度情况下，有限宽度的受控ResNets按分布收敛至神经CDE。 |
| [^112] | [Transformer and Snowball Graph Convolution Learning for Biomedical Graph Classification.](http://arxiv.org/abs/2303.16132) | 本文介绍了一种新型Transformer和Snowball编码网络（TSEN），它将Transformer架构和图雪球连接引入GNNs。TSEN通过雪球编码层将图雪球连接与图Transformer结合起来，增强了捕捉多尺度信息和全局模式以学习整个图特征的能力。 |
| [^113] | [Scaling Multi-Objective Security Games Provably via Space Discretization Based Evolutionary Search.](http://arxiv.org/abs/2303.15821) | 本文提出了一个名为SDES的通用框架，其中包括离散化、优化、恢复和评估以及改进等四个组成部分，通过离散化连续解空间来实现多目标安全博弈的可扩展性。 |
| [^114] | [An Evaluation of Memory Optimization Methods for Training Neural Networks.](http://arxiv.org/abs/2303.14633) | 本文从系统角度对现有内存优化方法（MOMs）进行全面分析和评估，提出了在不同场景下选择不同的评估指标并对各种MOMs进行全面比较的方案，为研究人员和从业人员提供了选择和应用MOMs的指导。 |
| [^115] | [Linking generative semi-supervised learning and generative open-set recognition.](http://arxiv.org/abs/2303.11702) | 本研究旨在探究生成半监督学习和生成开放集识别之间的关系。SSL-GANs和OSR-GANs方法的相似性在于都要求生成器在互补空间中产生样本，并通过正则化来推广开放空间。研究结果表明SSL优化边缘-GAN在结合SSL-OSR任务方面树立新的标准，但在某些OSR任务中OSR优化的ARP-GAN仍然略优于SSL-GAN。 |
| [^116] | [Greedy Pruning with Group Lasso Provably Generalizes for Matrix Sensing and Neural Networks with Quadratic Activations.](http://arxiv.org/abs/2303.11453) | 本文在矩阵感知问题中研究了基于Group Lasso正则化器的贪婪剪枝方法，证明了修剪低$\ell_2$范数列的解可以泛化到新样本上。 |
| [^117] | [Practical and Matching Gradient Variance Bounds for Black-Box Variational Bayesian Inference.](http://arxiv.org/abs/2303.10472) | 本文表明黑盒变分推理（BBVI）满足SGD文献中的ABC条件，该结果适用于平滑和二次增长的对数似然函数，同时我们的结果推广到广泛应用于BBVI实践中的非线性协方差参数化。 |
| [^118] | [Automated patent extraction powers generative modeling in focused chemical spaces.](http://arxiv.org/abs/2303.08272) | 本研究通过开发自动化管道，使用专利数据源训练领域特定的生成模型，利用专利中的弱标记应用类别中尽可能多的信息实现化学空间内生成建模。 |
| [^119] | [Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament.](http://arxiv.org/abs/2303.07925) | 本文采用 Numerai 数据科学竞赛的数据，探究了多变量时间序列建模中不同特征工程和降维方法的应用；提出了一种新的集成方法，用于高维时间序列建模，该方法在通用性、鲁棒性和效率上优于一些深度学习模型。 |
| [^120] | [Tactile-Filter: Interactive Tactile Perception for Part Mating.](http://arxiv.org/abs/2303.06034) | 本文提出了一种基于视觉的触觉传感器进行零部件装配任务的交互式感知方法，并设计了一个二维粒子滤波器，用于自动搜索新的触觉观察，以最大化其精度。 |
| [^121] | [Lifelong Machine Learning Potentials.](http://arxiv.org/abs/2303.05911) | 该论文提出了一种使用元素包容的中心对称函数来开发终身机器学习潜力，这可以有效且准确地预测新系统，并为其他科学领域的终身机器学习潜力的发展打开了大门。 |
| [^122] | [Contribution of clinical course to outcome after traumatic brain injury: mining patient trajectories from European intensive care unit data.](http://arxiv.org/abs/2303.04630) | 本研究研发了一种整合患者ICU住院期间所有记录的数据来为每名TBI患者提供可解释的疾病进展的建模策略，并应用递归神经网络模型对患者状况进行预测，以帮助实现个体化治疗。 |
| [^123] | [Error convergence and engineering-guided hyperparameter search of PINNs: towards optimized I-FENN performance.](http://arxiv.org/abs/2303.03918) | 本文研究如何优化I-FENN性能，通过系统数值方法，探究PINN训练误差和全局误差的收敛行为和超参数-性能关系。 |
| [^124] | [Variance-reduced Clipping for Non-convex Optimization.](http://arxiv.org/abs/2303.00883) | 本文提出了一种非凸优化中的方差缩减裁剪方法SPIDER，可以实现在较少的随机梯度计算次数下找到一个较稳定的解决方案。 |
| [^125] | [Safe Peeling for L0-Regularized Least-Squares with supplementary material.](http://arxiv.org/abs/2302.14471) | 引入“安全剥离”方法加速解决L0正则化最小二乘问题，通过收缩松弛度允许更激进的剪枝，显著降低求解时间。 |
| [^126] | [A Closer Look at the Intervention Procedure of Concept Bottleneck Models.](http://arxiv.org/abs/2302.14260) | 本文研究了概念瓶颈模型介入程序的提高介入效果的方法，通过选择介入概念以及深入分析发现，在实际情况下，明智的介入策略可以将任务误差降低十倍以上，而且这个差异可相当明显。 |
| [^127] | [Evaluating Robustness and Uncertainty of Graph Models Under Structural Distributional Shifts.](http://arxiv.org/abs/2302.13875) | 该论文提出了一种基于图结构的多样化分布转换的方法，并且针对性地设计了数据集。实验结果表明这些分布转换对于现有的图模型具有挑战性。 |
| [^128] | [Diffusion Model-Augmented Behavioral Cloning.](http://arxiv.org/abs/2302.13335) | 本研究提出了一种模仿学习框架，扩散模型增强的行为克隆（DBC），该模型同时建模专家分布的条件和联合概率，有效避免了建模复杂度和推理时间的问题。 |
| [^129] | [Equivariant Polynomials for Graph Neural Networks.](http://arxiv.org/abs/2302.11556) | 本文提出了一种基于等变多项式的表达能力层次结构，可以更好的指导GNN的模型改进。通过定义一个具体的基底，全面刻画了所有等变图多项式。此外，我们设计和分析了新的GNN架构，在多个基准数据集上超越了现有的最先进的模型。 |
| [^130] | [Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC.](http://arxiv.org/abs/2302.11552) | 该论文提出了一种基于能量扩散模型和MCMC的组合生成方法，旨在解决现有技术在组合生成中的失败问题，并提出了新的成功的解决方案。 |
| [^131] | [In-context Example Selection with Influences.](http://arxiv.org/abs/2302.11042) | 本文提出使用带有影响力的示例选择方法来提高上下文学习(ICL)的性能，分析表明正面和负面示例对ICL取得的性能有高达16.3%的影响，案例研究中也发现了示例排序中的“最近性偏差”现象。 |
| [^132] | [On the Expressivity of Persistent Homology in Graph Learning.](http://arxiv.org/abs/2302.09826) | 本文通过在图学习任务中的理论讨论和实证分析，证明了持续同调技术在捕捉具有显著拓扑结构的数据集中的长程图性质方面表现出的高表达性。 |
| [^133] | [Efficient Exploration via Epistemic-Risk-Seeking Policy Optimization.](http://arxiv.org/abs/2302.09339) | 本文提出了一种基于认知风险的新型目标函数，将不确定性转化为价值，鼓励智能体探索未知领域。该方法可以在深度强化学中实现高效探索，即使在函数逼近下也具有保证。 |
| [^134] | [MiDi: Mixed Graph and 3D Denoising Diffusion for Molecule Generation.](http://arxiv.org/abs/2302.09048) | MiDi是一种新的扩散模型，可以端到端地生成分子图和原子三维排列，与其他方法相比，MiDi可以更有效地生成稳定的分子。 |
| [^135] | [GFlowNet-EM for learning compositional latent variable models.](http://arxiv.org/abs/2302.06576) | GFlowNet-EM采用GFlowNets算法作为建模潜变量后验概率的E步骤，从而实现了对具有离散组合潜变量的表现力强的LVM进行训练。 |
| [^136] | [Byzantine-Robust Learning on Heterogeneous Data via Gradient Splitting.](http://arxiv.org/abs/2302.06079) | 这篇论文提出了一种缓解目前健壮算法在非IID环境下表现下降的方法，名为GAS，该方法能够成功将现有的健壮算法用于非IID的数据，并且在真实数据集上有效。 |
| [^137] | [Controllability-Aware Unsupervised Skill Discovery.](http://arxiv.org/abs/2302.05103) | 本文提出了一种新的可控性感知的无监督技能发现方法，通过联合训练的距离函数降低简单易实现技能的奖励，逐步学习更具挑战性的技能。 |
| [^138] | [Cooperative Open-ended Learning Framework for Zero-shot Coordination.](http://arxiv.org/abs/2302.04831) | 该论文提出了一个COLE框架，通过构建合作游戏的开放式目标，从图论的角度评估和确定每个策略的协作能力，以有效地解决零样本协调中的合作不兼容性问题。 |
| [^139] | [Predictable MDP Abstraction for Unsupervised Model-Based RL.](http://arxiv.org/abs/2302.03921) | 该论文提出了可预测的MDP抽象方法，通过无监督学习将原始MDP转换为学习行动空间，使模型学习变得更加准确和稳定，在多项任务上得到了验证。 |
| [^140] | [Sampling-Based Accuracy Testing of Posterior Estimators for General Inference.](http://arxiv.org/abs/2302.03026) | 本文提出了一种基于“随机点精度测试”（TARP）覆盖测试的方法来估计生成后验估计器覆盖概率。该方法是一种估计生成模型中编码后验精度的必要和充分条件。该方法可用于测试高维空间中后验推断分析的结果。 |
| [^141] | [RLSbench: Domain Adaptation Under Relaxed Label Shift.](http://arxiv.org/abs/2302.03020) | 本文介绍了 RLSbench，它是一个大规模基准，用于宽松标签偏移。与现有基准不同，它旨在评估领域自适应方法在标签边际偏移下的表现。 |
| [^142] | [Rethinking Gauss-Newton for learning over-parameterized models.](http://arxiv.org/abs/2302.02904) | 本研究重新思考了在过参数模型中使用高斯-牛顿法的应用，通过实证研究发现，虽然GN在找到全局最优解方面比GD更快，但学习率和随机初始化网络权重方差对模型泛化性能影响很大，更小的方差初始化能够获得更好的泛化性能，而与GD不同的是，GN在实现更好的泛化方面使用更小的学习率能够取得成效。 |
| [^143] | [Revisiting Personalized Federated Learning: Robustness Against Backdoor Attacks.](http://arxiv.org/abs/2302.01677) | 研究展示了部分模型共享的个性化联邦学习方法可以显著提高对抗后门攻击的鲁棒性，提出了一个轻量级的防御方法Simple-Tuning，可用于提高对抗后门攻击的防御性能。 |
| [^144] | [ANTM: An Aligned Neural Topic Model for Exploring Evolving Topics.](http://arxiv.org/abs/2302.01501) | ANTM是一种对齐的神经主题模型，它利用重叠滑动窗口算法来维护演变主题的时间连续性，并通过对语义相似的文档进行对齐来捕捉出现和消退的趋势。实验证明ANTM在主题连贯性和多样性方面优于传统动态主题模型。 |
| [^145] | [Learning Prototype Classifiers for Long-Tailed Recognition.](http://arxiv.org/abs/2302.00491) | 本文介绍了针对长尾识别的原型分类器学习，通过联合学习原型以最小化基于概率分数与原型之间距离的平均交叉熵损失，并通过引入一种新的方法，自适应平衡来自不同类别的损失的重要性，进一步增强了原型分类器，从而实现在几个基准数据集上的竞争性性能。 |
| [^146] | [Generative Adversarial Symmetry Discovery.](http://arxiv.org/abs/2302.00236) | 使用LieGAN框架，可以自动发现数据集中的等变性，从而提高预测的准确性和泛化能力。 |
| [^147] | [Auxiliary Learning as an Asymmetric Bargaining Game.](http://arxiv.org/abs/2301.13501) | 本研究提出了一种名为AuxiNash的辅助学习方法，将问题形式化为广义议价博弈，并通过非对称任务谈判能力平衡辅助任务，有效地提高训练模型泛化能力，证明了其在多个多任务基准测试中具有更好的性能。 |
| [^148] | [Adaptive Computation with Elastic Input Sequence.](http://arxiv.org/abs/2301.13195) | 本文介绍了一种名为AdaTape的新方法，通过自适应磁带符号，允许神经网络进行动态计算，能够实现适应不同类型信息的自适应计算，在图像分类、语言建模和程序综合等多个任务中都表现出更好的性能。 |
| [^149] | [ClusterFuG: Clustering Fully connected Graphs by Multicut.](http://arxiv.org/abs/2301.12159) | 本文提出了一种基于多割的全连接图聚类方法，通过内积的形式表示聚类目标，使得算法更高效，并在ImageNet和CIFAR数据集上展现出了优异的性能。 |
| [^150] | [Alignment with human representations supports robust few-shot learning.](http://arxiv.org/abs/2301.11990) | 论文提出少样本学习的表现与人类表征的一致性存在U形关系，并通过计算机视觉模型的实验进行了验证。高度对齐的模型更加鲁棒，对数据的利用更加有效，但与人类对齐并非必要条件。 |
| [^151] | [Practical Differentially Private Hyperparameter Tuning with Subsampling.](http://arxiv.org/abs/2301.11989) | 本文提出了一种利用子采样实现实用的差分隐私超参数调整的方法，相比基准算法，可以在不损失最终评估结果的情况下提高隐私-实用性权衡。 |
| [^152] | [Adaptive whitening in neural populations with gain-modulating interneurons.](http://arxiv.org/abs/2301.11955) | 该论文提出了一种基于增益调节的自适应白化神经电路模型，通过调节个体神经元的增益来适应地白化其响应，从而增加了网络对病态输入的鲁棒性。 |
| [^153] | [Pre-training for Speech Translation: CTC Meets Optimal Transport.](http://arxiv.org/abs/2301.11716) | 本文提出了一种基于CTC和最优传输的语音翻译预训练方法，可以有效减小语音和文本模态之间的差距，提高最终的ST准确性。 |
| [^154] | [A Robust Optimisation Perspective on Counterexample-Guided Repair of Neural Networks.](http://arxiv.org/abs/2301.11342) | 本研究从鲁棒优化角度解决了神经网络反例引导修复是否保证终止的问题，并提出一种基于二次规划的线性回归模型修复新算法。 |
| [^155] | [Neural Inverse Operators for Solving PDE Inverse Problems.](http://arxiv.org/abs/2301.11167) | 提出一种名为神经反演算子(NIOs)的模型来解决PDE反问题，并在多种实验中表现优于基线模型，可以稳健、准确地解决PDE反问题，且速度更快。 |
| [^156] | [Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning.](http://arxiv.org/abs/2301.10886) | 本文提出了一种名为AIRS的自动内在奖励塑造探索方法，可以提供高质量的内在激励以增强强化学习中的探索性能；并开发了高效可靠的内在奖励工具包。实验表明，AIRS性能卓越，能够胜过基准方案。 |
| [^157] | [Pre-computed memory or on-the-fly encoding? A hybrid approach to retrieval augmentation makes the most of your compute.](http://arxiv.org/abs/2301.10448) | 本文提出了一种检索增强的混合方法，LUMEN，它预先计算大部分检索表示，并使用一个实时编码器进行完成编码，相较于纯内存和FiD，LUMEN在多个问答任务中具有更好的性能，且成本更低。 |
| [^158] | [On the Expressive Power of Geometric Graph Neural Networks.](http://arxiv.org/abs/2301.09308) | 本文提出了几何版本的Weisfeiler-Leman测试(GWL)，可以区分几何图形，揭示了关键设计选择如何影响几何GNN的表现力 |
| [^159] | [An SDE for Modeling SAM: Theory and Insights.](http://arxiv.org/abs/2301.08203) | 我们推导了适用于SAM及其变体的连续时间模型，并解释了为什么SAM更喜欢平坦的极小值而非尖峰值，同时证明了SAM在某些现实条件下会被吸引到鞍点。 |
| [^160] | [Learning the Relation between Similarity Loss and Clustering Loss in Self-Supervised Learning.](http://arxiv.org/abs/2301.03041) | 本文提出了一种新的自监督学习框架，该框架由相似性损失和聚类损失组成。该框架利用了两个不同图像之间的相似性，并利用聚类损失进一步增强表示。实验结果表明，该框架在ImageNet数据集和下游任务中优于最先进的自监督方法。 |
| [^161] | [DExT: Detector Explanation Toolkit.](http://arxiv.org/abs/2212.11409) | 本文提出了一个开源的Detector Explanation Toolkit (DExT)，使用某些基于梯度的解释方法实现了生成所有检测器决策的全面解释。它可以为边界框和分类决策产生解释，是一个帮助人们了解物体检测器系统决策原因的工具。 |
| [^162] | [Multi-hop Evidence Retrieval for Cross-document Relation Extraction.](http://arxiv.org/abs/2212.10786) | 本文介绍了一个名为MR.COD的多跳证据检索方法，用于支持跨文档关系抽取。实验表明该方法有效地获取了跨文档证据，并提升了封闭和开放设置中的性能。 |
| [^163] | [Beyond Contrastive Learning: A Variational Generative Model for Multilingual Retrieval.](http://arxiv.org/abs/2212.10726) | 本文提出了一种多语言检索的变分生成模型，可以有效地鼓励源分离，并展示其与对比学习方法的比较结果，该方法在实际检索任务中表现出效果。 |
| [^164] | [Dataless Knowledge Fusion by Merging Weights of Language Models.](http://arxiv.org/abs/2212.09849) | 本文提出了一种无数据知识融合方法，可以合并在不同训练数据集上建立的单个模型，以得到一个在所有数据集领域上表现良好且可以推广到域外数据的单一模型。 |
| [^165] | [APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning.](http://arxiv.org/abs/2212.09282) | 本文提出了APOLLO，一种适应性预训练语言模型，通过选择特定的Wikipedia子集进行预训练，并使用两个自监督损失函数，成功地提高了模型的逻辑推理能力。 |
| [^166] | [Graph Neural Networks are Inherently Good Generalizers: Insights by Bridging GNNs and MLPs.](http://arxiv.org/abs/2212.09034) | 本文通过将中间模型命名为 PMLP 并在测试时采用 GNNs 的架构，发现 GNNs 的表现出众不是其高级表现力的主要原因，而是其固有的泛化能力。 |
| [^167] | [FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference.](http://arxiv.org/abs/2212.08153) | 这项研究提出了一种名为FiDO的解码器融合模型，通过两个简单的更改，有效缓解了内存带宽约束，加快了模型的推理速度，大大提高了模型性能，达到了领先水平。 |
| [^168] | [Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models.](http://arxiv.org/abs/2211.17091) | 本文提出了“鉴别器引导”方法，通过在评分训练之后训练鉴别器，使模型评估更加准确，从而改善预训练扩散模型的样本生成。在 ImageNet 256x256 数据集上实现了 FID 1.83 和召回率 0.64 的最新结果，类似于验证数据的 FID 和召回率。 |
| [^169] | [Arbitrarily Large Labelled Random Satisfiability Formulas for Machine Learning Training.](http://arxiv.org/abs/2211.15368) | 本论文展示了一种基于概率方法的生成任意尺寸的正确标记的随机公式的方法，这一方法可以用于解决目前困难且常用于深度学习模型的组合问题。 |
| [^170] | [OpenFE: Automated Feature Generation with Expert-level Performance.](http://arxiv.org/abs/2211.12507) | 本文提出的自动特征生成工具OpenFE可以与机器学习专家提供的结果相媲美，具有高效和准确的特点，通过新颖的特征提升方法和两阶段修剪算法实现。 |
| [^171] | [Diffusion Denoising Process for Perceptron Bias in Out-of-distribution Detection.](http://arxiv.org/abs/2211.11255) | 本文针对深度学习中的异常检测问题，提出了一种新的方法——使用扩散模型作为非对称插值的方法来增强输入并减轻过度自信的问题，从而提高判别器模型在异常检测方面的性能。 |
| [^172] | [A Two-Stage Active Learning Algorithm for $k$-Nearest Neighbors.](http://arxiv.org/abs/2211.10773) | 本文提出了一种用于训练 $k$ 最近邻分类器的简单直观的主动学习算法，首次保持了 $k$ 最近邻投票概念的预测时间，并提供了一致性保证。 |
| [^173] | [Bayesian Learning of Coupled Biogeochemical-Physical Models.](http://arxiv.org/abs/2211.06714) | 本文提出了一种基于贝叶斯模型学习的方法，可以在候选模型空间内进行插值，同时估计状态场和参数值，解决了海洋生态系统预测动态模型因数据稀疏和模型多样性带来的不确定性问题。 |
| [^174] | [Causal Counterfactuals for Improving the Robustness of Reinforcement Learning.](http://arxiv.org/abs/2211.05551) | 本文提出了CausalCF，它是第一个完整的因果RL解决方案，能够通过因果反事实推断提高RL系统的鲁棒性，并已在机器人抓取和操纵任务中得到应用。 |
| [^175] | [A Decentralized Alternating Gradient Method for Communication-Efficient Bilevel Programming.](http://arxiv.org/abs/2211.04088) | 本文提出了一种通信高效的分散式交替梯度法解决双层规划问题，相较于其他方法，该算法具有更低的通信成本和更高的隐私性。 |
| [^176] | [Leveraging Statistical Shape Priors in GAN-based ECG Synthesis.](http://arxiv.org/abs/2211.02626) | 本文提出了一种基于统计形状先验知识和GAN的ECG信号生成方法，能够解决ECG信号的复杂动力学问题。使用来自MIT-BIH心律失常数据库的数据进行实验验证，生成的信号比现有最先进的基于GAN的生成基线更为逼真，对于提高ECG训练数据集质量具有重要意义，可提高ECG分类算法的性能。 |
| [^177] | [Construction of Hierarchical Neural Architecture Search Spaces based on Context-free Grammars.](http://arxiv.org/abs/2211.01842) | 本研究基于无上下文文法提出了一个统一的搜索空间设计框架，可以生成表达力强大的分层搜索空间，实现了对整个体系结构的搜索并促进结构的规律性。 |
| [^178] | [Fair and Optimal Classification via Post-Processing.](http://arxiv.org/abs/2211.01528) | 本文提出了一个后处理算法，通过评分函数推导公平分类器，达到公平对待不同群体的目的。 |
| [^179] | [Learning Failure-Inducing Models for Testing Software-Defined Networks.](http://arxiv.org/abs/2210.15469) | 本文介绍了一种名为FuzzSDN的机器学习引导模糊测试方法，可生成导致SDN系统失败的有效测试数据，并学习准确的故障诱导模型以表征此类系统失败的条件。 |
| [^180] | [Position tracking of a varying number of sound sources with sliding permutation invariant training.](http://arxiv.org/abs/2210.14536) | 本论文提出了一种新的基于深度学习模型的声源定位培训策略，使用滑动置换不变训练来跟踪多个变化的声源位置，最小化身份切换(IDS)，并在实验中证明该策略的有效性。 |
| [^181] | [Topology Optimization via Machine Learning and Deep Learning: A Review.](http://arxiv.org/abs/2210.10782) | 本文综述了机器学习和深度学习在拓扑优化中的应用，从TO和ML两个不同的角度进行了回顾和分析，并探讨了当前MLTO研究的限制和未来研究方向。 |
| [^182] | [Stochastic Differentially Private and Fair Learning.](http://arxiv.org/abs/2210.08781) | 本文提供了第一个能够保证收敛的随机差分隐私公平学习算法。该算法解决了高风险决策系统中面临的歧视和隐私泄漏问题。 |
| [^183] | [MonoNeRF: Learning Generalizable NeRFs from Monocular Videos without Camera Pose.](http://arxiv.org/abs/2210.07181) | MonoNeRF是一种无需深度和相机姿态标注的通用神经辐射场模型，可从单目视频中学得，可以用于多个应用，包括深度估计、相机姿态估计和单张图像的新视角合成。 |
| [^184] | [Centralized Training with Hybrid Execution in Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2210.06274) | 本文介绍了多智能体强化学习中的新范式，提供了MARO方法，通过集中式训练和自回归预测模型，在执行时间中预测缺失的智能体观测值，优于现有方法。 |
| [^185] | [Linkless Link Prediction via Relational Distillation.](http://arxiv.org/abs/2210.05801) | 本研究提出了一种基于关系蒸馏的框架(LLP)用于链接预测，通过匹配以每个（锚）节点为中心的关系知识来蒸馏链接预测的知识，该方法快捷而有效，达到或超过了现有技术的图神经网络的性能水平。 |
| [^186] | [Prediction intervals for neural network models using weighted asymmetric loss functions.](http://arxiv.org/abs/2210.04318) | 本论文提出了一种使用加权不对称损失函数的方法，生成可靠的预测区间，适用于复杂的机器学习情境，可扩展为参数化函数的PI预测。 |
| [^187] | [MAMO: Masked Multimodal Modeling for Fine-Grained Vision-Language Representation Learning.](http://arxiv.org/abs/2210.04183) | 本文提出了一种联合掩膜多模态建模方法，以学习细粒度的多模态表示，通过隐式和显式目标来恢复联合掩膜信号以提高细化的图像-文本交互。 |
| [^188] | [Test-time Recalibration of Conformal Predictors Under Distribution Shift Based on Unlabeled Examples.](http://arxiv.org/abs/2210.04166) | 本论文提出了一种基于未标记样本的分布漂移下测试时间校准置信度预测器。通过使用密度比估计技术来预测新分布的截止阈值，我们在几个标准图像数据集上展示了该方法优于最新的分布漂移下的测试时间校准方法。 |
| [^189] | [A Unified Hard-Constraint Framework for Solving Geometrically Complex PDEs.](http://arxiv.org/abs/2210.03526) | 我们提出了一个神经网络硬约束框架来解决几何复杂偏微分方程，将边界条件转化为线性方程，可以在不引入额外损失项的情况下训练网络并稳定训练过程。 |
| [^190] | [Teaching Yourself: Graph Self-Distillation on Neighborhood for Node Classification.](http://arxiv.org/abs/2210.02097) | 本文提出了一种GSDN框架，通过基于邻域的图形自我蒸馏，以减小GNN和MLP之间的差距。 |
| [^191] | [Sparsity by Redundancy: Solving $L_1$ with SGD.](http://arxiv.org/abs/2210.01212) | 该论文提出了一种通过冗余重参数化和简单的随机梯度下降来最小化带有$L_1$惩罚的通用可微损失函数的方法，称为\textit{spred}，是$L_1$的精确求解器，可用于训练稀疏神经网络以执行基因选择任务和神经网络压缩任务，弥合了深度学习中的稀疏性和传统统计学习之间的差距。 |
| [^192] | [Data Poisoning Attacks Against Multimodal Encoders.](http://arxiv.org/abs/2209.15266) | 本文研究了多模态编码器遭受数据污染攻击的情况，提出了三种攻击类型，并发现同时进行视觉和语言模态污染的攻击能够实现较好攻击性能。 |
| [^193] | [Learning GFlowNets from partial episodes for improved convergence and stability.](http://arxiv.org/abs/2209.12782) | 本文提出了一种 GFlowNets 训练目标——子轨迹平衡(SubTB($\lambda$))，从部分 episode 学习的方式可以加速采样器在环境中的收敛速度，并使得在之前难以训练的长动作序列和奖励稀疏的环境中也能够训练 GFlowNets。 |
| [^194] | [On Investigating the Conservative Property of Score-Based Generative Models.](http://arxiv.org/abs/2209.12753) | 本文研究基于分数的生成模型的保守性质，发现现有的模型不足以满足其要求，提出了一种准保守基于分数的模型，能够在保持原有模型优势的同时，有效地集成训练目标。 |
| [^195] | [Robust Collaborative Learning with Linear Gradient Overhead.](http://arxiv.org/abs/2209.10931) | MoNNA算法使用Polyak的局部梯度动量和最近邻平均方法，可在标准假设下证明鲁棒性，并且梯度计算开销与错误节点的比例成线性关系。 |
| [^196] | [Signs of Language: Embodied Sign Language Fingerspelling Acquisition from Demonstrations for Human-Robot Interaction.](http://arxiv.org/abs/2209.05135) | 本文提出了一种从视频示例学习手语拼写在机器人中的实现的方法。通过训练模仿运动的策略，利用预训练的深度视觉模型从RGB视频中提取手的三维姿态，并识别出最佳的模仿超参数集，本文成功展示了方法的普适性。 |
| [^197] | [Graph Embeddings via Tensor Products and Approximately Orthonormal Codes.](http://arxiv.org/abs/2208.10917) | 本文介绍了一种嵌入图形到向量空间的方法，使用张量积以及球形码实现高效压缩和表征，在稀疏图表示和其他应用中具有潜在技术优势。 |
| [^198] | [Adaptive Identification of Populations with Treatment Benefit in Clinical Trials: Machine Learning Challenges and Solutions.](http://arxiv.org/abs/2208.05844) | 本文提出了一种新的数据驱动的方法，同时使用了机器学习和自适应实验技术，用于自适应性临床试验中受益于给定治疗的患者亚群的识别。本文通过实验展示了该方法的优异表现，并解决了这一问题的独特挑战，产生了有效和有用的发现。 |
| [^199] | [D3Former: Debiased Dual Distilled Transformer for Incremental Learning.](http://arxiv.org/abs/2208.00777) | 本文提出了D3Former，一种用于类别增量学习的无偏差双重蒸馏Transformer。D3Former利用混合嵌套ViT设计，不会动态扩展其架构，并通过偏差校正模块和新的蒸馏目标来改进其CIL行为。 |
| [^200] | [Mitigating Off-Policy Bias in Actor-Critic Methods with One-Step Q-learning: A Novel Correction Approach.](http://arxiv.org/abs/2208.00755) | 本文提出一种新的策略相似度量来缓解离策略学习中的偏差问题，提供了一种自适应的、可扩展的解决方案。 |
| [^201] | [Adaptive Asynchronous Control Using Meta-learned Neural Ordinary Differential Equations.](http://arxiv.org/abs/2207.12062) | 通过元学习自适应动力学模型实现了一种通用框架，可以适应不规则/异步观察和行动，并在不同的episode之间产生巨大的环境动力学变化，这可以被应用于机器人控制。 |
| [^202] | [A Supervised Tensor Dimension Reduction-Based Prognostics Model for Applications with Incomplete Imaging Data.](http://arxiv.org/abs/2207.11353) | 本文提出了一种基于监督张量降维的预测模型，能处理不完整的成像数据，利用失效时间监督提取低维特征，提高了预测的准确性。 |
| [^203] | [Log Barriers for Safe Black-box Optimization with Application to Safe Reinforcement Learning.](http://arxiv.org/abs/2207.10415) | 提出了一种名为LB-SGD的方法，它基于对原始问题的对数障碍近似，并应用随机梯度下降。此方法可用于使学习过程中保持安全至关重要的高维非线性随机优化问题，包括一阶和零阶反馈的非凸、凸和强凸平滑约束问题的完全收敛分析。 |
| [^204] | [Deep Reinforcement Learning with Swin Transformers.](http://arxiv.org/abs/2206.15269) | 本文介绍了基于 Swin Transformer 的在线强化学习方案 Swin DQN，通过将组合的图像像素分成小的补丁并在局部应用自我注意力操作，实现了在 Atari 基准测试上超越现有基于 CNN 的强化学习方法的最先进性能。 |
| [^205] | [Evaluating Generative Patent Language Models.](http://arxiv.org/abs/2206.14578) | 本文构建了生成性专利语言模型，并通过基于人类节省按键数量的度量方法评估了其表现，发现在专利领域继续增加模型大小可能是不必要的。 |
| [^206] | [Communication-Efficient Federated Learning With Data and Client Heterogeneity.](http://arxiv.org/abs/2206.10032) | 本文提出了适用于具有数据和客户端异质性的通信高效的联邦学习算法，并通过实验结果验证了其在标准联合任务的快速收敛性，以及在有趣的参数范围内可以提供类似于经典联邦平均算法的收敛性。 |
| [^207] | [Orthonormal Expansions for Translation-Invariant Kernels.](http://arxiv.org/abs/2206.08648) | 该论文提出了一种傅里叶分析技术，用于从$\mathscr{L}_2(\mathbb{R})$的正交基中构建平移不变核函数的正交基展开，实现了马特尔核函数、柯西核函数和高斯核函数的明确展开表达式。 |
| [^208] | [Exploring Chemical Space with Score-based Out-of-distribution Generation.](http://arxiv.org/abs/2206.07632) | MOOD是一种基于分数的异分布扩散策略，它通过属性预测器的梯度进行条件生成，从而使得逆向时间扩散过程通过指导目标特性到高分数区域，从而允许我们搜索新颖且有意义的分子。 |
| [^209] | [Meta Optimal Transport.](http://arxiv.org/abs/2206.05262) | 本文提出了一种新的方法，利用过去问题的知识和信息来迅速预测和解决新问题，重复地解决不同度量之间的类似OT问题，从而改善最优输运问题的计算时间。 |
| [^210] | [XAudit : A Theoretical Look at Auditing with Explanations.](http://arxiv.org/abs/2206.04740) | 本文研究了如何通过解释来审计机器学习模型。作者提出了一种基于因果推断解释的算法来审计线性分类器和决策树的特征敏感性，结果表明该算法在审计中非常有帮助。 |
| [^211] | [Action Noise in Off-Policy Deep Reinforcement Learning: Impact on Exploration and Performance.](http://arxiv.org/abs/2206.03787) | 本文研究了动作噪声类型、噪声比例和减少规模因子对深度强化学习离线学习中策略性能和探索效果的影响，提出了一种更具鲁棒性的状态空间覆盖度量。 |
| [^212] | [Timing is Everything: Learning to Act Selectively with Costly Actions and Budgetary Constraints.](http://arxiv.org/abs/2205.15953) | LICRA是学习在代价高昂的行动和预算限制下进行选择性行动的强化学习框架。 |
| [^213] | [Hierarchies of Reward Machines.](http://arxiv.org/abs/2205.15752) | 本文提出了一种奖励状态机（RM）的层次化结构（HRM），利用它可以将任务进一步抽象为多个子任务，每个子任务都可以独立解决；使用 HRM 可以帮助加快收敛速度且在学习中是可行的。 |
| [^214] | [Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks.](http://arxiv.org/abs/2205.15171) | 提出一种新颖的模块化偏差缓解方法，在推理时间按需集成到核心模型中的独立去偏置子网络，在性别、种族和年龄等受保护属性的分类任务中，该方法在缓解偏差方面是有效的，并且在精度和灵活性方面优于现有技术方法。 |
| [^215] | [Precise Learning Curves and Higher-Order Scaling Limits for Dot Product Kernel Regression.](http://arxiv.org/abs/2205.14846) | 本文细致研究了点积核岭回归问题，针对 $m\propto d^r$ 高阶标度关系提出了精确的测试误差、偏差和方差公式。 |
| [^216] | [Comparison of meta-learners for estimating multi-valued treatment heterogeneous effects.](http://arxiv.org/abs/2205.14714) | 本文探讨了利用元学习器估计多值处理异质效应的问题，发现朴素扩展并不总是可行，提出并讨论了一些表现良好的元学习器。 |
| [^217] | [Fair Labeled Clustering.](http://arxiv.org/abs/2205.14358) | 本文提出了解决公平标记聚类问题的算法，考虑了下游应用和团体公平性的实现。 |
| [^218] | [Formalizing Preferences Over Runtime Distributions.](http://arxiv.org/abs/2205.13028) | 本文形式化了偏好运行时分布，提出了一种基于效用理论的替代方案来描述算法的评分函数，这些函数与随时间的推移和消费时间的分布有关。 |
| [^219] | [Heterformer: Transformer-based Deep Node Representation Learning on Heterogeneous Text-Rich Networks.](http://arxiv.org/abs/2205.10282) | 本论文提出了一种名为Heterformer的基于Transformer的深度节点表示学习方法，它可以同时处理异构结构和丰富的文本语义信息。 |
| [^220] | [Set-based Meta-Interpolation for Few-Task Meta-Learning.](http://arxiv.org/abs/2205.09990) | 本论文提出了一种基于集合的元互插方法，可以解决Few-Task Meta-Learning问题中任务数量少带来的瓶颈，同时该方法对领域不敏感。 |
| [^221] | [CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks.](http://arxiv.org/abs/2204.10965) | CLIP-Dissect是一种用于自动描述视觉网络中神经元功能的新技术，它可以无需任何标记数据或人类示例即将内部神经元标记为无需任何标记数据或人类示例的开放概念，并比现有方法提供了更准确的描述。此外，它具有灵活性、高效性和可扩展性。 |
| [^222] | [Deep surrogate accelerated delayed-acceptance HMC for Bayesian inference of spatio-temporal heat fluxes in rotating disc systems.](http://arxiv.org/abs/2204.02272) | 本文提出一种加速PDE求解的方法，用于解决贝叶斯反问题，具有保证的精度，并可用于其他设置，通过训练神经网络代理模拟参数向前模型，同时确定Biot数的近似后验分布，而无需外部求解。 |
| [^223] | [A Set Membership Approach to Discovering Feature Relevance and Explaining Neural Classifier Decisions.](http://arxiv.org/abs/2204.02241) | 该论文提出一种基于集合成员关系的方法来发现神经分类器所需的特征，并解释其决策。方法能够标识贡献每个特征对神经网络分类器所做出的预测，并解释分类器处理输入分布变化时的行为。 |
| [^224] | [Tensor Completion with Provable Consistency and Fairness Guarantees for Recommender Systems.](http://arxiv.org/abs/2204.01815) | 本文介绍了一种新的一致性方法来解决矩阵和张量补全问题，在推荐系统应用中，我们证明了通过保留单位比例和一致性两个约束条件可以实现解的存在性与唯一性。 |
| [^225] | [A Survey on Machine Learning Solutions for Graph Pattern Extraction.](http://arxiv.org/abs/2204.01057) | 本综述概括了机器学习在解决五个著名子图问题中的应用。这些问题包括子图同构（计数和匹配）、最大公共子图、社区检测和社区搜索问题。 |
| [^226] | [Disentangling speech from surroundings with neural embeddings.](http://arxiv.org/abs/2203.15578) | 本文提出了一种方法，利用神经音频编解码器中的嵌入空间将语音信号与嘈杂环境分离，并演示了该方法在从噪声或混响中分离语音中的应用。 |
| [^227] | [Machine Learning Testing in an ADAS Case Study Using Simulation-Integrated Bio-Inspired Search-Based Testing.](http://arxiv.org/abs/2203.12026) | 本文提出一种基于仿真集成的生物启发式搜索测试方法Deeper，用于生成用于测试基于深度神经网络的车道保持系统的故障发现测试场景，通过实证评估和与竞赛中的其他工具的比较展示了其性能的提高。 |
| [^228] | [Evaluation of Interpretability Methods and Perturbation Artifacts in Deep Neural Networks.](http://arxiv.org/abs/2203.02928) | 本文评估了几种常见的深度神经网络可解释性方法，展示了扰动伪影对可解释性方法评估的影响，强调在评估中需要考虑伪影的存在。 |
| [^229] | [Data-Efficient and Interpretable Tabular Anomaly Detection.](http://arxiv.org/abs/2203.02034) | 本文提出了DIAD框架，使用广义加性模型作为白盒模型，通过部分识别目标检测异常值，并能够在半监督设置中使用少量标记数据提高异常检测性能。 |
| [^230] | [Canonical foliations of neural networks: application to robustness.](http://arxiv.org/abs/2203.00922) | 本文探讨了利用黎曼几何和叶面理论创新应用于神经网络鲁棒性的新视角，提出了一种适用于数据空间的以曲率为考量因素的 two-step spectral 对抗攻击方法。 |
| [^231] | [Bayesian Active Learning for Discrete Latent Variable Models.](http://arxiv.org/abs/2202.13426) | 本文提出了一个新的框架，用于离散潜变量回归模型的最大相互信息输入选择。通过对线性回归混合物模型的Fisher信息分析，证明在这种情况下主动学习可以取得巨大的收益。同时，我们考虑了一类强大的时间结构潜变量模型，并展示了如何将我们的框架调整为在选择过程中融入时态依赖性。 |
| [^232] | [Does Label Differential Privacy Prevent Label Inference Attacks?.](http://arxiv.org/abs/2202.12968) | 标签差分隐私不能完全防止标签推断攻击（LIAs），但可以限制LIAs对手的优势和语义保护。 |
| [^233] | [MAML and ANIL Provably Learn Representations.](http://arxiv.org/abs/2202.03483) | 本文证明了MAML和ANIL能够在少样本学习中学习出共同的数据表示法，它们通过适应模型的最后一层来改善表示法，这也是导致共享表示法出现的原因。 |
| [^234] | [Evaluating natural language processing models with generalization metrics that do not need access to any training or testing data.](http://arxiv.org/abs/2202.02842) | 本文提出了一种无需访问任何数据即可评估自然语言处理模型的泛化度量标准，并通过对Huggingface预训练变压器的模型选择，得到一个简单高效且相关性强的有用度量标准。 |
| [^235] | [Discovering Personalized Semantics for Soft Attributes in Recommender Systems using Concept Activation Vectors.](http://arxiv.org/abs/2202.02830) | 我们使用概念激活向量来把用户描述商品的属性的语义表达出来，以改进推荐系统的效果。 |
| [^236] | [Low-Rank Updates of Matrix Square Roots.](http://arxiv.org/abs/2201.13156) | 本文研究了矩阵平方根和逆平方根运算。当给定一个矩阵的低秩扰动时，存在一种低秩近似校正方法。该方法可以通过代数Riccati方程的低秩解计算，并可以在两个数值例子中得到说明。 |
| [^237] | [Existence and Estimation of Critical Batch Size for Training Generative Adversarial Networks with Two Time-Scale Update Rule.](http://arxiv.org/abs/2201.11989) | 本文研究了使用两时间尺度更新规则（TTUR）训练生成式对抗网络（GAN）时批次大小与训练所需步骤数量之间的关系，理论上证明了为了找到稳定点，随着批次大小的增加所需步骤数量会减少并且存在一个最小化随机一阶预言机（SFO）复杂度的关键批次大小。 |
| [^238] | [The Eigenlearning Framework: A Conservation Law Perspective on Kernel Regression and Wide Neural Networks.](http://arxiv.org/abs/2110.03922) | 该论文提出了Eigenlearning框架，通过限制核回归在学习正交基函数方面的能力并利用守恒定律来解释模型的泛化能力，同时还为Nakkiran等人的“深度引导”现象，经典奇偶问题难度和对抗鲁棒性提供了理论支持，并与统计物理学中的一个系统进行了类比。 |
| [^239] | [Sinkhorn Distributionally Robust Optimization.](http://arxiv.org/abs/2109.11926) | 本文通过使用Sinkhorn距离进行分布鲁棒优化，推导出更容易处理且在实际中更合理的最坏情况分布，提出了解决方案，并展示了其优越性能。 |
| [^240] | [Learning Meta Representations for Agents in Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2108.12988) | 本文提出代理元表示（MRA），能够跨多种多智能体强化学习中变化的人口数量进行推广的代理，并证明了通过最大化互信息来实现代理性能的提升。 |
| [^241] | [A diffusion-map-based algorithm for gradient computation on manifolds and applications.](http://arxiv.org/abs/2108.06988) | 本文提出了一种基于扩散映射理论的流形梯度计算方法，可以避免微分项并提供了一种无需导数的优化方法，在影像重构和球堆积等应用中取得了良好的效果。 |
| [^242] | [Sequoia: A Software Framework to Unify Continual Learning Research.](http://arxiv.org/abs/2108.01005) | Sequoia是一个公开可用的软件框架，用于统一连续学习的研究。该框架提供了各种设置、基准数据集、评估指标和最先进的方法，旨在减少重复工作，鼓励连续学习的统一研究。 |
| [^243] | [Noise-robust Graph Learning by Estimating and Leveraging Pairwise Interactions.](http://arxiv.org/abs/2106.07451) | 本文提出了一种成对框架，可在噪声图上进行节点分类，并利用结构成对交互作为主要的学习代理。提出的PI-GNN框架包含两个新颖的组件，自适应估计PI标签的可信度感知PI估计模型和一种解耦训练方法。 |
| [^244] | [Deep Bayesian Active Learning for Accelerating Stochastic Simulation.](http://arxiv.org/abs/2106.02770) | 本文提出了一个名为交互式神经过程(INP)的深度贝叶斯主动学习框架，用于学习深度代理模型以加速随机模拟过程，其中通过使用空间时间神经过程(STNP)实现模拟器动态的模拟，以及利用潜在信息增益(LIG)的主动学习方式来减少样本的复杂度。 |
| [^245] | [Distributed Adaptive Nearest Neighbor Classifier: Algorithm and Theory.](http://arxiv.org/abs/2105.09788) | 提出一种新颖的分布式自适应NN分类器，通过随机选择数据驱动准则来调优最近邻数，提出了早期停止规则，实现了加速计算和改善有限样本性能。通过研究证明，当子样本大小足够大时，分类器实现了近乎最优的收敛速度。有效性已通过模拟和实证应用得到验证。 |
| [^246] | [Probabilistic Fair Clustering.](http://arxiv.org/abs/2006.10916) | 本文提出了一种通过概率分配获得组成员身份的不完美知识的公平聚类算法，并在这种更一般的设置中给出了逼近比保证。 |
| [^247] | [Deep Weakly-supervised Anomaly Detection.](http://arxiv.org/abs/1910.13601) | PReNet是一种深度弱监督方法，可以检测既有已知又有未知的异常情况，通过学习成对的关系特征和异常分数，实现了异常-异常、异常-正常和正常-正常的联合学习。 |
| [^248] | [Proposing a Model for Predicting Passenger Origin-Destination in Online Taxi-Hailing Systems.](http://arxiv.org/abs/1910.08145) | 本文提出了一种模型，采用K均值和非负矩阵分解等方法，预测网约车叫车系统中乘客行程的起点和终点，相较于现有模型，具有更高的预测准确度。 |
| [^249] | [Sparse tree search optimality guarantees in POMDPs with continuous observation spaces.](http://arxiv.org/abs/1910.04332) | 本研究证明了一种基于采样的算法，部分可观察加权稀疏采样（POWSS），可以在具有连续观测空间的POMDPs中准确估计Q值，并通过增加计算能力来实现接近最优解。 |
| [^250] | [Learning the Relation between Code Features and Code Transforms with Structured Prediction.](http://arxiv.org/abs/1907.09282) | 本文提出了一种用条件随机场在AST节点级别预测代码转换的方法，它在Java程序的修复转换预测的上下文中实例化，并成功解决了训练数据不平衡问题，发现了一组多样化的新的和有用的修复转换。 |

# 详细

[^1]: 应对持续任务强化学习中的无界状态空间

    Tackling Unbounded State Spaces in Continuing Task Reinforcement Learning. (arXiv:2306.01896v1 [cs.LG])

    [http://arxiv.org/abs/2306.01896](http://arxiv.org/abs/2306.01896)

    本文提出了一种基于李雅普诺夫思想的奖励塑形方法，用于在持续任务的强化学习中应对无界状态空间，旨在鼓励代理器学习稳定性和最优策略。

    

    尽管深度强化学习（RL）算法已成功应用于许多任务，但它们无法外推且强烈依赖周期性重置，这限制了它们在许多现实世界设置中的适用性。针对这个问题，本文提出了一种基于李雅普诺夫思想的奖励塑形方法，以鼓励代理首先学习稳定性（即实现有界成本），然后再学习最优策略。理论上证明了奖励塑形技术减少了代理器的发散率，并通过实验进一步证实了这一点。

    While deep reinforcement learning (RL) algorithms have been successfully applied to many tasks, their inability to extrapolate and strong reliance on episodic resets inhibits their applicability to many real-world settings. For instance, in stochastic queueing problems, the state space can be unbounded and the agent may have to learn online without the system ever being reset to states the agent has seen before. In such settings, we show that deep RL agents can diverge into unseen states from which they can never recover due to the lack of resets, especially in highly stochastic environments. Towards overcoming this divergence, we introduce a Lyapunov-inspired reward shaping approach that encourages the agent to first learn to be stable (i.e. to achieve bounded cost) and then to learn to be optimal. We theoretically show that our reward shaping technique reduces the rate of divergence of the agent and empirically find that it prevents it. We further combine our reward shaping approach 
    
[^2]: 分层二次随机森林分类器

    Hierarchical Quadratic Random Forest Classifier. (arXiv:2306.01893v1 [cs.LG])

    [http://arxiv.org/abs/2306.01893](http://arxiv.org/abs/2306.01893)

    本文提出了一种用于对多通道数据中的多分辨率样本进行分类的分层二次随机森林分类器。该分类器采用了惩罚的多元线性判别建立决策节点，并且使用组 Lasso 正则化器对特征进行处理。该分类器可以独立使用或者运用于基于图的分类器中。

    

    本文提出了一种用于对多分辨率样本进行分类的分层二次随机森林分类器，应用于多通道数据。该森林在每个决策节点中集成了一种惩罚的多元线性判别式，并使用平方特征处理实现原始特征空间中的二次决策边界。惩罚判别式基于多类别稀疏判别分析，并基于组 Lasso 正则化器进行惩罚，其位于 Lasso 和岭回归正则化器之间。该森林估计的分类概率和其决策节点学习的特征可以独立使用或促进基于图的分类器。

    In this paper, we proposed a hierarchical quadratic random forest classifier for classifying multiresolution samples extracted from multichannel data. This forest incorporated a penalized multivariate linear discriminant in each of its decision nodes and processed squared features to realize quadratic decision boundaries in the original feature space. The penalized discriminant was based on a multiclass sparse discriminant analysis and the penalization was based on a group Lasso regularizer which was an intermediate between the Lasso and the ridge regularizer. The classification probabilities estimated by this forest and the features learned by its decision nodes could be used standalone or foster graph-based classifiers.
    
[^3]: 混合类型数据的核度量学习

    Kernel Metric Learning for Clustering Mixed-type Data. (arXiv:2306.01890v1 [cs.LG])

    [http://arxiv.org/abs/2306.01890](http://arxiv.org/abs/2306.01890)

    提出了一种使用混合核测量不相似性的度量方法，并通过交叉验证找到最佳核带宽。该方法可为现有的基于距离的聚类算法提高聚类准确度，适用于包含混合类型数据的模拟和实际数据集。

    

    基于距离的聚类和分类广泛应用于各个领域，以将混合数值和分类数据分组。预定义的距离测量用于根据它们的不相似性来聚类数据点。虽然存在许多适用于具有纯数字属性和几个有序和无序分类指标的数据的基于距离的度量方法，但混合型数据的最佳距离是一个尚未解决的问题。许多度量将数字属性转换为分类属性或反之亦然。他们将数据点处理为单个属性类型，或者分别计算每个属性之间的距离并将它们相加。我们提出了一种度量方法，使用混合核测量不相似性，并进行交叉验证来寻找最佳核带宽。我们的方法对包含纯连续，分类和混合类型数据的模拟和实际数据集应用于现有的基于距离的聚类算法时，提高了聚类准确度。

    Distance-based clustering and classification are widely used in various fields to group mixed numeric and categorical data. A predefined distance measurement is used to cluster data points based on their dissimilarity. While there exist numerous distance-based measures for data with pure numerical attributes and several ordered and unordered categorical metrics, an optimal distance for mixed-type data is an open problem. Many metrics convert numerical attributes to categorical ones or vice versa. They handle the data points as a single attribute type or calculate a distance between each attribute separately and add them up. We propose a metric that uses mixed kernels to measure dissimilarity, with cross-validated optimal kernel bandwidths. Our approach improves clustering accuracy when utilized for existing distance-based clustering algorithms on simulated and real-world datasets containing pure continuous, categorical, and mixed-type data.
    
[^4]: 基于连接组学的储层计算机中的多功能性

    Multifunctionality in a Connectome-Based Reservoir Computer. (arXiv:2306.01885v1 [cs.LG])

    [http://arxiv.org/abs/2306.01885](http://arxiv.org/abs/2306.01885)

    本文通过将果蝇侧角的连接组学移植到储层计算机中，探究其展示出的多功能性，并在“看见两倍”的问题上表现出更大的多功能性能力和解决能力。

    

    多功能性是指神经网络能够在不改变其网络连接的情况下执行多个互相排斥的任务的能力；是围绕着储层计算机学习范式的一个新兴研究领域。多功能性已在人类和其他动物的大脑中观察到：特别是在果蝇的侧角。在这项工作中，我们将果蝇侧角的连接组学移植到储层计算机（RC）中，并使用“看见两倍”的问题作为基准测试来调查这个“果蝇RC”（FFRC）展示多功能性的程度。我们进一步探索了FFRC在变化网络的谱半径时如何实现多功能性的动态。与广泛使用的Erdos-Renyi储层计算机（ERRC）相比，我们报道了FFRC展示了更大的多功能性能力；在更广的超参数范围内实现了多功能性，并远远超越了解决“看见两倍”的ERRC。

    Multifunctionality describes the capacity for a neural network to perform multiple mutually exclusive tasks without altering its network connections; and is an emerging area of interest in the reservoir computing machine learning paradigm. Multifunctionality has been observed in the brains of humans and other animals: particularly, in the lateral horn of the fruit fly. In this work, we transplant the connectome of the fruit fly lateral horn to a reservoir computer (RC), and investigate the extent to which this 'fruit fly RC' (FFRC) exhibits multifunctionality using the 'seeing double' problem as a benchmark test. We furthermore explore the dynamics of how this FFRC achieves multifunctionality while varying the network's spectral radius. Compared to the widely-used Erd\"os-Renyi Reservoir Computer (ERRC), we report that the FFRC exhibits a greater capacity for multifunctionality; is multifunctional across a broader hyperparameter range; and solves the seeing double problem far beyond th
    
[^5]: DiffECG：ECG信号合成的一般化概率扩散模型

    DiffECG: A Generalized Probabilistic Diffusion Model for ECG Signals Synthesis. (arXiv:2306.01875v1 [cs.CV])

    [http://arxiv.org/abs/2306.01875](http://arxiv.org/abs/2306.01875)

    本文介绍了一种新颖的ECG信号合成方法——基于去噪扩散概率模型的DiffECG，能够涵盖三种情形，并且是ECG合成的第一个广义条件方法。实验证明该方法的有效性以及优于其他ECG生成模型并可提高分类器性能。

    

    近年来，深度生成模型在基于深度学习的ECG信号心脏疾病检测中作为一种有前途的数据增强解决方案备受关注。本文提出一种新颖的基于去噪扩散概率模型的ECG合成方法,覆盖了三种情形：心跳生成、部分信号完成和完整心跳预测。我们的方法是ECG合成的第一个广义条件方法，实验结果表明其对各种ECG相关任务的有效性。此外，我们还展示了我们的方法优于其他最先进的ECG生成模型并可以提高最先进的分类器的性能。

    In recent years, deep generative models have gained attention as a promising data augmentation solution for heart disease detection using deep learning approaches applied to ECG signals. In this paper, we introduce a novel approach based on denoising diffusion probabilistic models for ECG synthesis that covers three scenarios: heartbeat generation, partial signal completion, and full heartbeat forecasting. Our approach represents the first generalized conditional approach for ECG synthesis, and our experimental results demonstrate its effectiveness for various ECG-related tasks. Moreover, we show that our approach outperforms other state-of-the-art ECG generative models and can enhance the performance of state-of-the-art classifiers.
    
[^6]: SACSoN：面向社交导航的可扩展自主数据收集系统

    SACSoN: Scalable Autonomous Data Collection for Social Navigation. (arXiv:2306.01874v1 [cs.RO])

    [http://arxiv.org/abs/2306.01874](http://arxiv.org/abs/2306.01874)

    本文介绍了一个名为SACSoN的自主导航机器人系统，可以在人类占用的现实场景中，通过视觉理解和学习，自主收集数据，实现更好的数据集拓展。

    

    机器学习为构建符合社交规范的机器人系统提供了一个强大的工具，超越了对人类行为的简单预测模型。通过观察和理解过去经验中的人类交互，学习可以直接从数据中实现有效的社交导航行为。然而，在人类占用的环境中收集导航数据可能需要远程操作或持续监视，使得这个过程难以扩展。在本文中，我们提出了一个面向视觉导航的可扩展数据收集系统SACSoN，可以自主导航于挑战性的现实环境中的行人周围，并鼓励丰富的交互。SACSoN使用视觉观察来观察和回应其附近的人类。它将这种视觉理解与持续的学习和自主碰撞恢复系统相结合，从而限制了人操作员的参与，从而更好地扩展了数据集。我们使用这个系统来收集数据集

    Machine learning provides a powerful tool for building socially compliant robotic systems that go beyond simple predictive models of human behavior. By observing and understanding human interactions from past experiences, learning can enable effective social navigation behaviors directly from data. However, collecting navigation data in human-occupied environments may require teleoperation or continuous monitoring, making the process prohibitively expensive to scale. In this paper, we present a scalable data collection system for vision-based navigation, SACSoN, that can autonomously navigate around pedestrians in challenging real-world environments while encouraging rich interactions. SACSoN uses visual observations to observe and react to humans in its vicinity. It couples this visual understanding with continual learning and an autonomous collision recovery system that limits the involvement of a human operator, allowing for better dataset scaling. We use a this system to collect th
    
[^7]: 层间反馈对齐在深度神经网络中的保守性

    Layer-Wise Feedback Alignment is Conserved in Deep Neural Networks. (arXiv:2306.01870v1 [cs.LG])

    [http://arxiv.org/abs/2306.01870](http://arxiv.org/abs/2306.01870)

    本论文揭示了层间反馈对齐在深度神经网络中的保守性，并发现FA与GD之间存在隐式偏差的相似之处，同时阐明了ReLU网络中与反馈矩阵对齐的充分条件。

    

    为了提高深度神经网络的效率和生物可塑性，反馈对齐（FA）作为传统反向传播的替代方法应运而生，它将训练过程中的反向传输权重替换为随机矩阵。虽然FA的吸引力在于它能够绕过计算挑战和其可信的生物对齐性，但对于这种学习规则的理解还是有所欠缺的。本文揭示了支撑FA学习动态的一组守恒定律，揭示了FA和梯度下降（GD）之间的有趣相似之处。我们的分析表明，FA具有与GD表现出的隐式偏差相似的隐式偏差，挑战了现有的这些学习算法之间根本不同的流行说法。此外，我们证明，这些守恒定律阐明了ReLU网络中与反馈矩阵对齐的充分条件。我们进一步展示，这意味着过参数化的双线性网络中可以实现线性地代替后向权重。

    In the quest to enhance the efficiency and bio-plausibility of training deep neural networks, Feedback Alignment (FA), which replaces the backward pass weights with random matrices in the training process, has emerged as an alternative to traditional backpropagation. While the appeal of FA lies in its circumvention of computational challenges and its plausible biological alignment, the theoretical understanding of this learning rule remains partial. This paper uncovers a set of conservation laws underpinning the learning dynamics of FA, revealing intriguing parallels between FA and Gradient Descent (GD). Our analysis reveals that FA harbors implicit biases akin to those exhibited by GD, challenging the prevailing narrative that these learning algorithms are fundamentally different. Moreover, we demonstrate that these conservation laws elucidate sufficient conditions for layer-wise alignment with feedback matrices in ReLU networks. We further show that this implies over-parameterized tw
    
[^8]: 二进制矩阵分解的快速 $(1+\varepsilon)$ 近似算法

    Fast $(1+\varepsilon)$-Approximation Algorithms for Binary Matrix Factorization. (arXiv:2306.01869v1 [cs.DS])

    [http://arxiv.org/abs/2306.01869](http://arxiv.org/abs/2306.01869)

    本论文提出了一种高效的 $(1+\varepsilon)$ 近似算法，用于二进制矩阵分解问题。该算法在 running time 方面是单指数级别的，并且可用于多个变体。

    

    我们介绍了一种高效的 $(1+\varepsilon)$ 近似算法，用于二进制矩阵分解(BMF)问题，其中输入是矩阵 $\mathbf{A}\in\{0,1\}^{n\times d}$，排名参数 $k>0$，精度参数 $\varepsilon>0$，目的是将 $\mathbf{A}$ 近似为低秩因子 $\mathbf{U}\in\{0,1\}^{n\times k}$ 和 $\mathbf{V}\in\{0,1\}^{k\times d}$ 的乘积。等价地，我们要找到 $\mathbf{U}$ 和 $\mathbf{V}$ 以最小化 Frobenius 损失 $\|\mathbf{U}\mathbf{V}\mathbf{A}\|_F^2$。在这项工作之前，该问题的最先进近似算法是 Kumar 等人的算法 [ICML 2019]，该算法对于某个常数 $C \ge 576$ 可以实现 $C$ 近似。我们提供了第一个使用 $k$ 的指数时间的 $(1+\varepsilon)$ 近似算法，其中 $k$ 通常是小整数。我们的技术推广到 BMF 问题的其他常见变体，从而导致二标准 $(1+\varepsilon)$ 近似算法。

    We introduce efficient $(1+\varepsilon)$-approximation algorithms for the binary matrix factorization (BMF) problem, where the inputs are a matrix $\mathbf{A}\in\{0,1\}^{n\times d}$, a rank parameter $k>0$, as well as an accuracy parameter $\varepsilon>0$, and the goal is to approximate $\mathbf{A}$ as a product of low-rank factors $\mathbf{U}\in\{0,1\}^{n\times k}$ and $\mathbf{V}\in\{0,1\}^{k\times d}$. Equivalently, we want to find $\mathbf{U}$ and $\mathbf{V}$ that minimize the Frobenius loss $\|\mathbf{U}\mathbf{V} \mathbf{A}\|_F^2$. Before this work, the state-of-the-art for this problem was the approximation algorithm of Kumar et. al. [ICML 2019], which achieves a $C$-approximation for some constant $C\ge 576$. We give the first $(1+\varepsilon)$-approximation algorithm using running time singly exponential in $k$, where $k$ is typically a small integer. Our techniques generalize to other common variants of the BMF problem, admitting bicriteria $(1+\varepsilon)$-approximation 
    
[^9]: 利用变异预训练域中的对比学习从未标记的数据中发现COVID-19的咳嗽和呼吸模式

    Discovering COVID-19 Coughing and Breathing Patterns from Unlabeled Data Using Contrastive Learning with Varying Pre-Training Domains. (arXiv:2306.01864v1 [cs.LG])

    [http://arxiv.org/abs/2306.01864](http://arxiv.org/abs/2306.01864)

    本文提出了一种基于对比学习的建模方法，用于从非COVID咳嗽中发现COVID-19的咳嗽和呼吸模型。该模型的准确率高达0.81和0.86，有助于未来尽早发现新疾病的爆发。

    

    针对COVID-19这样的新疾病的快速发现可以促进及时的疫情应对，防止大规模传播并保护公共卫生。本文提出了一种基于对比学习的建模方法，用于从非COVID咳嗽中发现COVID-19的咳嗽和呼吸模式。为了验证我们的模型，我们使用了四个大型音频数据集和一个图像数据集进行了广泛的实验。我们进一步探讨了不同因素（如领域相关性和数据增强顺序）对预训练模型的影响。我们的结果表明，所提出的模型可以有效地从未标记的数据和标记的非COVID咳嗽中区分COVID-19的咳嗽和呼吸，准确度分别高达0.81和0.86。本研究的发现将有助于未来的研究尽早发现新疾病的爆发。

    Rapid discovery of new diseases, such as COVID-19 can enable a timely epidemic response, preventing the large-scale spread and protecting public health. However, limited research efforts have been taken on this problem. In this paper, we propose a contrastive learning-based modeling approach for COVID-19 coughing and breathing pattern discovery from non-COVID coughs. To validate our models, extensive experiments have been conducted using four large audio datasets and one image dataset. We further explore the effects of different factors, such as domain relevance and augmentation order on the pre-trained models. Our results show that the proposed model can effectively distinguish COVID-19 coughing and breathing from unlabeled data and labeled non-COVID coughs with an accuracy of up to 0.81 and 0.86, respectively. Findings from this work will guide future research to detect an outbreak of a new disease early.
    
[^10]: 无竞标，无遗憾：针对数字商品和数据拍卖的对偶反馈机制。

    No Bidding, No Regret: Pairwise-Feedback Mechanisms for Digital Goods and Data Auctions. (arXiv:2306.01860v1 [cs.GT])

    [http://arxiv.org/abs/2306.01860](http://arxiv.org/abs/2306.01860)

    本文提出了针对重复拍卖设置的对偶反馈机制，使用成对比较来从竞标者那里获取信息，避免了之前的学习出价问题，该机制被证明为渐近诚实、个体理性、福利和收益最大化的且适用于任何需要定制生产的商品拍卖场景。

    

    随着数据和 AI 生成数字商品（如个性化书面内容和艺术品）的需求增长，需要有效定价和反馈机制来考虑不确定的效用和昂贵的生产成本。为了解决这些问题，本研究提出了一种新颖的机制设计，适用于一个通用的重复拍卖设置，其中售出商品的效用在销售后揭示。该机制的新颖之处在于使用成对比较来从竞标者那里获取信息，相对于指定数量值来说对人类更容易理解。我们的机制使用 epsilon-greedy 策略选择分配，并且依赖于已分配商品的实现效用和任意值之间的成对比较，避免了以前的学习出价问题。我们证明了该机制是渐近诚实、个体理性、福利和收益最大化的。该机制适用范围广泛，适用于任何需要定制生产、没有固定价格的商品或服务的场景。

    The growing demand for data and AI-generated digital goods, such as personalized written content and artwork, necessitates effective pricing and feedback mechanisms that account for uncertain utility and costly production. Motivated by these developments, this study presents a novel mechanism design addressing a general repeated-auction setting where the utility derived from a sold good is revealed post-sale. The mechanism's novelty lies in using pairwise comparisons for eliciting information from the bidder, arguably easier for humans than assigning a numerical value. Our mechanism chooses allocations using an epsilon-greedy strategy and relies on pairwise comparisons between realized utility from allocated goods and an arbitrary value, avoiding the learning-to-bid problem explored in previous work. We prove this mechanism to be asymptotically truthful, individually rational, and welfare and revenue maximizing. The mechanism's relevance is broad, applying to any setting with made-to-o
    
[^11]: 5IDER: 统一查询重写技术用于对话引导、意图携带、语言中断、实体携带及修复

    5IDER: Unified Query Rewriting for Steering, Intent Carryover, Disfluencies, Entity Carryover and Repair. (arXiv:2306.01855v1 [cs.CL])

    [http://arxiv.org/abs/2306.01855](http://arxiv.org/abs/2306.01855)

    本文介绍了一种非自回归的查询重写体系结构，该体系不仅可以处理对话引导、意图携带、语法中断、实体携带和修复这五个任务，还可以处理它们的复杂组合。

    

    提供语音助手导航多轮对话的能力是一个具有挑战性的问题。 处理多轮互动需要系统理解各种会话用例，如对话引导、意图携带、语言中断、实体携带和修复。 这个问题的复杂性加剧了这些用例混合在一起的事实，通常同时在自然语言中出现。本文提出了一种非自回归的查询重写体系结构，可以处理五个任务以及这些用例的复杂组合。我们证明了我们提出的模型与基线方法具有相当的单任务性能，并且在用例组合方面甚至优于经过调优的T5模型，尽管在参数上小15倍，在延迟上快25倍。

    Providing voice assistants the ability to navigate multi-turn conversations is a challenging problem. Handling multi-turn interactions requires the system to understand various conversational use-cases, such as steering, intent carryover, disfluencies, entity carryover, and repair. The complexity of this problem is compounded by the fact that these use-cases mix with each other, often appearing simultaneously in natural language. This work proposes a non-autoregressive query rewriting architecture that can handle not only the five aforementioned tasks, but also complex compositions of these use-cases. We show that our proposed model has competitive single task performance compared to the baseline approach, and even outperforms a fine-tuned T5 model in use-case compositions, despite being 15 times smaller in parameters and 25 times faster in latency.
    
[^12]: 带有通用效用的强化学习：简单的方差缩减和大状态行动空间

    Reinforcement Learning with General Utilities: Simpler Variance Reduction and Large State-Action Space. (arXiv:2306.01854v1 [cs.LG])

    [http://arxiv.org/abs/2306.01854](http://arxiv.org/abs/2306.01854)

    本文提出了一种适用于通用效用的强化学习算法，具有更简单、无参数的归一化策略梯度算法。算法包括递归动量方差缩减机制，针对大有限状态行动空间提出了线性函数逼近方法，具有较低的样本复杂度。

    

    我们考虑具有通用效用的强化学习问题，其中包括最大化状态-动作占用度量函数。除了标准的累积奖励RL设置外，这个问题包括约束RL，纯探索和从演示中学习等特定情况。为这个问题，我们提出了一种更简单、单循环、无参数的归一化策略梯度算法。我们的算法实现了递归动量方差缩减机制，在适当的假设下，实现了$\tilde{\mathcal{O}}(\epsilon^{-3})$和$\tilde{\mathcal{O}}(\epsilon^{-2})$的样本复杂度，用于$\epsilon$-一阶稳定性和$\epsilon$-全局最优性。我们还通过占用度量的线性函数逼近解决了大有限状态行动空间的设置，并展示了一个使用线性回归子程序的简单策略梯度方法的$\tilde{\mathcal{O}}(\epsilon^{-4})$的样本复杂度。

    We consider the reinforcement learning (RL) problem with general utilities which consists in maximizing a function of the state-action occupancy measure. Beyond the standard cumulative reward RL setting, this problem includes as particular cases constrained RL, pure exploration and learning from demonstrations among others. For this problem, we propose a simpler single-loop parameter-free normalized policy gradient algorithm. Implementing a recursive momentum variance reduction mechanism, our algorithm achieves $\tilde{\mathcal{O}}(\epsilon^{-3})$ and $\tilde{\mathcal{O}}(\epsilon^{-2})$ sample complexities for $\epsilon$-first-order stationarity and $\epsilon$-global optimality respectively, under adequate assumptions. We further address the setting of large finite state action spaces via linear function approximation of the occupancy measure and show a $\tilde{\mathcal{O}}(\epsilon^{-4})$ sample complexity for a simple policy gradient method with a linear regression subroutine.
    
[^13]: 自编码器的最大似然训练

    Maximum Likelihood Training of Autoencoders. (arXiv:2306.01843v1 [cs.LG])

    [http://arxiv.org/abs/2306.01843](http://arxiv.org/abs/2306.01843)

    本文介绍了一种成功的最大似然训练方法，用于非约束自编码器，将生成建模的优异性质与高效自编码器相结合。作者克服了两个挑战：设计了消除迭代的估计器并提出了稳定的最大似然训练目标。实验证明这种方法可以成功训练一系列非约束性自编码器，并取得了有竞争力的性能。

    

    最大似然训练在生成建模中具有优异的统计性质，尤其是在归一化流模型中非常流行。另一方面，由于流形假设，生成自编码器有望比归一化流更高效。本文首次引入了非约束自编码器的成功最大似然训练，将这两种范式融合在一起。为此，我们识别并克服了两个挑战：首先，现有的自由格式网络的最大似然估计器过于缓慢，依赖于迭代方案，其成本随潜在维度呈线性增长。我们引入了一个改进的估计器，消除了迭代，从而使成本保持不变（每个批次的运行时间大约是普通自编码器的两倍）。其次，我们证明朴素地将最大似然应用于自编码器可能导致发散解决方案，并利用这个想法来推动稳定的最大似然训练目标。我们进行了实验，表明所提出的训练方法可以成功训练一系列非约束性自编码器，并在生成图像、插值和变换等任务中取得了有竞争力的性能。

    Maximum likelihood training has favorable statistical properties and is popular for generative modeling, especially with normalizing flows. On the other hand, generative autoencoders promise to be more efficient than normalizing flows due to the manifold hypothesis. In this work, we introduce successful maximum likelihood training of unconstrained autoencoders for the first time, bringing the two paradigms together. To do so, we identify and overcome two challenges: Firstly, existing maximum likelihood estimators for free-form networks are unacceptably slow, relying on iteration schemes whose cost scales linearly with latent dimension. We introduce an improved estimator which eliminates iteration, resulting in constant cost (roughly double the runtime per batch of a vanilla autoencoder). Secondly, we demonstrate that naively applying maximum likelihood to autoencoders can lead to divergent solutions and use this insight to motivate a stable maximum likelihood training objective. We per
    
[^14]: 参数组合框架下的高效多任务和迁移强化学习

    Efficient Multi-Task and Transfer Reinforcement Learning with Parameter-Compositional Framework. (arXiv:2306.01839v1 [cs.RO])

    [http://arxiv.org/abs/2306.01839](http://arxiv.org/abs/2306.01839)

    本文提出了一种基于参数组合的转移方法，可以在多任务训练阶段提高性能，并在各种操作任务上展示了有效的迁移能力。

    

    本文研究了在强化学习中改进多任务训练并利用其进行转移的潜力。我们识别了实现此目标的几个挑战，并提出了一种基于参数组合的转移方法。我们研究了改进多任务强化学习训练的方法，这是转移的基础。然后我们在各种操作任务上进行了多个转移实验。实验结果表明，所提出的方法可以在多任务训练阶段提高性能，并展示了在样本效率和性能方面的有效转移。

    In this work, we investigate the potential of improving multi-task training and also leveraging it for transferring in the reinforcement learning setting. We identify several challenges towards this goal and propose a transferring approach with a parameter-compositional formulation. We investigate ways to improve the training of multi-task reinforcement learning which serves as the foundation for transferring. Then we conduct a number of transferring experiments on various manipulation tasks. Experimental results demonstrate that the proposed approach can have improved performance in the multi-task training stage, and further show effective transferring in terms of both sample efficiency and performance.
    
[^15]: 利用多序列比对生成增强蛋白质三级结构预测的模型

    Enhancing the Protein Tertiary Structure Prediction by Multiple Sequence Alignment Generation. (arXiv:2306.01824v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.01824](http://arxiv.org/abs/2306.01824)

    本文介绍了一种利用生成语言模型和大规模多序列比对生成新蛋白质序列的方法，用于补充没有丰富同源家族数据库的蛋白质序列，增强蛋白质结构预测的准确性。

    

    深度学习方法大大推进了蛋白质折叠研究领域，AlphaFold2（AF2）表现出了卓越的性能和原子级精度。由于共同演化对蛋白质结构预测至关重要，因此多序列比对（MSA）的深度会对AF2的准确性产生显著影响，而MSA需要对大量蛋白质数据库进行广泛探索以查找相似序列。然而，并非所有蛋白质序列都具有丰富的同源家族，因此在这些查询上，AF2的性能可能会降低，并且有时无法产生有意义的结果。为了解决这个问题，我们引入了一种新颖的生成语言模型MSA-Augmenter，它利用蛋白质特定的注意力机制和大规模MSA生成目前尚未在数据库中发现的有用的新蛋白质序列。这些序列补充了浅层MSA，增强了结构属性预测的准确性。我们在CASP14上的实验表明，MSA-Augmenter可以增强蛋白质三级结构预测的准确性。

    The field of protein folding research has been greatly advanced by deep learning methods, with AlphaFold2 (AF2) demonstrating exceptional performance and atomic-level precision. As co-evolution is integral to protein structure prediction, AF2's accuracy is significantly influenced by the depth of multiple sequence alignment (MSA), which requires extensive exploration of a large protein database for similar sequences. However, not all protein sequences possess abundant homologous families, and consequently, AF2's performance can degrade on such queries, at times failing to produce meaningful results. To address this, we introduce a novel generative language model, MSA-Augmenter, which leverages protein-specific attention mechanisms and large-scale MSAs to generate useful, novel protein sequences not currently found in databases. These sequences supplement shallow MSAs, enhancing the accuracy of structural property predictions. Our experiments on CASP14 demonstrate that MSA-Augmenter can
    
[^16]: ErfReLU: 深度神经网络的自适应激活函数

    ErfReLU: Adaptive Activation Function for Deep Neural Network. (arXiv:2306.01822v1 [cs.NE])

    [http://arxiv.org/abs/2306.01822](http://arxiv.org/abs/2306.01822)

    本论文提出了一种基于误差函数和ReLU的新型激活函数'ErfReLU'，可以自适应地随着学习而训练，适用于各项任务。

    

    最近的研究发现，选择用于增加非线性的激活函数（AF）对于深度学习网络的有效性有很大影响。开发同时能够随着学习而自适应的激活函数是当前的需要。研究人员最近开始开发能够在学习过程中训练的激活函数，称为可训练或自适应激活函数（AAF）。增强结果的AAF研究仍处于早期阶段。本文提出了一种基于误差函数和ReLU的新型激活函数'ErfReLU'。简要介绍了Sigmoid、ReLU、Tanh等最新激活函数及其相关特性。还介绍了Tanhsoft1、Tanhsoft2、Tanhsoft3、TanhLU、SAAF、ErfAct、Pserf、Smish和Serf等自适应激活函数。最后，分析了9种可训练AAF在各项任务上的表现。

    Recent research has found that the activation function (AF) selected for adding non-linearity into the output can have a big impact on how effectively deep learning networks perform. Developing activation functions that can adapt simultaneously with learning is a need of time. Researchers recently started developing activation functions that can be trained throughout the learning process, known as trainable, or adaptive activation functions (AAF). Research on AAF that enhance the outcomes is still in its early stages. In this paper, a novel activation function 'ErfReLU' has been developed based on the erf function and ReLU. This function exploits the ReLU and the error function (erf) to its advantage. State of art activation functions like Sigmoid, ReLU, Tanh, and their properties have been briefly explained. Adaptive activation functions like Tanhsoft1, Tanhsoft2, Tanhsoft3, TanhLU, SAAF, ErfAct, Pserf, Smish, and Serf have also been described. Lastly, performance analysis of 9 traina
    
[^17]: 大规模机器学习系统中的并发分类器错误检测

    Concurrent Classifier Error Detection (CCED) in Large Scale Machine Learning Systems. (arXiv:2306.01820v1 [cs.LG])

    [http://arxiv.org/abs/2306.01820](http://arxiv.org/abs/2306.01820)

    本文介绍了一种名为并发分类器错误检测 (CCED) 的方案，该方案使用并发机器学习分类器来检测大规模机器学习系统中的错误，而不需要额外硬件或软件开销。

    

    机器学习系统的复杂性每年都在增加，当前实现的大型语言模型或文本到图像生成器具有数十亿的参数，并需要执行数十亿次算术运算。由于这些系统被广泛使用，确保它们的可靠运行正在成为设计要求。传统的错误检测机制引入了电路或时间冗余，对系统性能产生了显著影响。另一种选择是使用并发错误检测方案，这些方案与系统并行运行，并利用其属性来检测错误。并发错误检测对于大型机器学习系统非常有吸引力，因为它可以潜在地降低错误检测的成本。本文介绍了一种称为并发分类器错误检测 (CCED) 的方案，该方案使用并发机器学习分类器来检测错误并在主要机器学习系统中识别一组检查信号，并将其馈送到并发机器学习分类器中，该分类器经过训练可识别主系统的正确输出。CCED能够在不需要额外硬件或软件开销的情况下检测主系统的错误，使其成为大规模机器学习系统中错误检测的实用且高效的解决方案。

    The complexity of Machine Learning (ML) systems increases each year, with current implementations of large language models or text-to-image generators having billions of parameters and requiring billions of arithmetic operations. As these systems are widely utilized, ensuring their reliable operation is becoming a design requirement. Traditional error detection mechanisms introduce circuit or time redundancy that significantly impacts system performance. An alternative is the use of Concurrent Error Detection (CED) schemes that operate in parallel with the system and exploit their properties to detect errors. CED is attractive for large ML systems because it can potentially reduce the cost of error detection. In this paper, we introduce Concurrent Classifier Error Detection (CCED), a scheme to implement CED in ML systems using a concurrent ML classifier to detect errors. CCED identifies a set of check signals in the main ML system and feeds them to the concurrent ML classifier that is 
    
[^18]: Beta地中海贫血携带者检测的联邦学习方法

    Beta Thalassemia Carriers detection empowered federated Learning. (arXiv:2306.01818v1 [cs.LG])

    [http://arxiv.org/abs/2306.01818](http://arxiv.org/abs/2306.01818)

    本论文介绍了一种使用联邦学习技术的方法，快速、便宜、无需移动设备地检测Beta地中海贫血携带者。

    

    地中海贫血是一组遗传性血液疾病，当携带输氧至身体各处的红细胞中的蛋白质血红蛋白不足时会发生。如果父母双方都携带地中海贫血基因，孩子患病的几率会增加。确诊和治疗地中海贫血是防止其传递给下一代的关键。目前的血液检测方法对于检测Beta地中海贫血携带者过于昂贵、耗时，且需要大量的筛查设备。高效液相色谱是标准的检测方法，但也存在成本高、时间长、需要特殊设备等问题。因此，寻找一种快速、便宜的检测Beta地中海贫血携带者的方法是至关重要的。

    Thalassemia is a group of inherited blood disorders that happen when hemoglobin, the protein in red blood cells that carries oxygen, is not made enough. It is found all over the body and is needed for survival. If both parents have thalassemia, a child's chance of getting it increases. Genetic counselling and early diagnosis are essential for treating thalassemia and stopping it from being passed on to future generations. It may be hard for healthcare professionals to differentiate between people with thalassemia carriers and those without. The current blood tests for beta thalassemia carriers are too expensive, take too long, and require too much screening equipment. The World Health Organization says there is a high death rate for people with thalassemia. Therefore, it is essential to find thalassemia carriers to act quickly. High-performance liquid chromatography (HPLC), the standard test method, has problems such as cost, time, and equipment needs. So, there must be a quick and che
    
[^19]: 使用区块链和机器学习预测心脏疾病

    Heart Diseases Prediction Using Block-chain and Machine Learning. (arXiv:2306.01817v1 [cs.LG])

    [http://arxiv.org/abs/2306.01817](http://arxiv.org/abs/2306.01817)

    本文研究了使用区块链和机器学习算法预测心脏疾病，并解决了现有架构下数据存储和传输的不安全性问题，有望优化心脏专业人士对于心脏病早期诊断的能力。

    

    全球大部分人死于心脏疾病。其主要原因是医疗部门尚未建立安全的数据存储和传输基础设施。由于患者数据中的冗余性，心脏专业人士很难早期预测疾病。这种快速增加的死亡率可以通过监测和消除早期出现的一些关键因素（例如血压、胆固醇水平、体重和吸烟成瘾）来控制。在医疗部门中，心脏专业人士（Cp）可以使用先进的系统来监控患者数据，其中区块链是最可靠的提供者之一，提供了新的处理疾病的方法。本文使用了一种机器学习（ML）算法，称为正弦余弦支持向量机（Sine-Cosine SVM）。

    Most people around the globe are dying due to heart disease. The main reason behind the rapid increase in the death rate due to heart disease is that there is no infrastructure developed for the healthcare department that can provide a secure way of data storage and transmission. Due to redundancy in the patient data, it is difficult for cardiac Professionals to predict the disease early on. This rapid increase in the death rate due to heart disease can be controlled by monitoring and eliminating some of the key attributes in the early stages such as blood pressure, cholesterol level, body weight, and addiction to smoking. Patient data can be monitored by cardiac Professionals (Cp) by using the advanced framework in the healthcare departments. Blockchain is the world's most reliable provider. The use of advanced systems in the healthcare departments providing new ways of dealing with diseases has been developed as well. In this article Machine Learning (ML) algorithm known as a sine-co
    
[^20]: 使用机器学习和深度学习预测柑橘病害：分类器、模型和SLR

    Prediction of Citrus Diseases Using Machine Learning And Deep Learning: Classifier, Models SLR. (arXiv:2306.01816v1 [cs.LG])

    [http://arxiv.org/abs/2306.01816](http://arxiv.org/abs/2306.01816)

    本文讨论了预测柑橘病害的方法，介绍了使用机器学习和深度学习的分类器和模型，并探讨了相关防治策略。

    

    多年以来，柑橘病害一直是全球柑橘种植业的主要问题，它们会显著降低水果的质量。最危险的柑橘病害包括柑橘溃疡病、柑橘黄龙病、柑橘黑斑病、柑橘叶蛾，这些病害可导致全球柑橘产业出现显著经济损失，防治策略包括化学治疗等。柑橘病害分布在全球所有柑橘种植地区，影响柑橘树根、柑橘树叶、柑橘等水果。柑橘病害的存在对经济因素产生高度影响，同时也会产生低品质水果，增加病害管理费用。卫生和定期监测可以有效管理某些柑橘病害，但其他病害可能需要更加密集的治疗，例如化学或生物控制方法。

    Citrus diseases have been major issues for citrus growing worldwide for many years they can lead significantly reduce fruit quality. the most harmful citrus diseases are citrus canker, citrus greening, citrus black spot, citrus leaf miner which can have significant economic losses of citrus industry in worldwide prevention and management strategies like chemical treatments. Citrus diseases existing in all over the world where citrus is growing its effects the citrus tree root, citrus tree leaf, citrus tree orange etc. Existing of citrus diseases is highly impact on economic factor that can also produce low quality fruits and increased the rate for diseases management. Sanitation and routine monitoring can be effective in managing certain citrus diseases, but others may require more intensive treatments like chemical or biological control methods.
    
[^21]: 基于比较的快速交互式搜索算法及其应用

    Fast Interactive Search with a Scale-Free Comparison Oracle. (arXiv:2306.01814v1 [cs.IR])

    [http://arxiv.org/abs/2306.01814](http://arxiv.org/abs/2306.01814)

    该论文提出了一种基于比较的快速交互式搜索算法，其中使用了称为 $\gamma$-CKL 的刻度自由概率预言模型，并开发了一种具有指数收敛速度的搜索算法。

    

    基于比较的搜索算法可以让用户通过回答“项 $i$ 和 $j$ 哪一个更接近 $t$？”的查询来在数据库中找到目标项 $t$ 。我们提出了一种称为 $\gamma$-CKL 的刻度自由概率预言模型，用于表示这种相似性三元组 $(i,j;t)$。这种模型在控制预言的区分能力和包含项的特征空间维度方面具有独立的优势。我们开发了一种搜索算法，证明了在 $\gamma$-CKL 模型下具有指数收敛速度。我们还评估了算法在几个真实三元组数据库上的性能。

    A comparison-based search algorithm lets a user find a target item $t$ in a database by answering queries of the form, ``Which of items $i$ and $j$ is closer to $t$?'' Instead of formulating an explicit query (such as one or several keywords), the user navigates towards the target via a sequence of such (typically noisy) queries.  We propose a scale-free probabilistic oracle model called $\gamma$-CKL for such similarity triplets $(i,j;t)$, which generalizes the CKL triplet model proposed in the literature. The generalization affords independent control over the discriminating power of the oracle and the dimension of the feature space containing the items.  We develop a search algorithm with provably exponential rate of convergence under the $\gamma$-CKL oracle, thanks to a backtracking strategy that deals with the unavoidable errors in updating the belief region around the target.  We evaluate the performance of the algorithm both over the posited oracle and over several real-world tri
    
[^22]: 学习超图动力系统的有效顺序

    Learning the effective order of a hypergraph dynamical system. (arXiv:2306.01813v1 [cs.LG])

    [http://arxiv.org/abs/2306.01813](http://arxiv.org/abs/2306.01813)

    本文提出了一种方法以确定超图动力系统的最小顺序，以精确地近似相应的动力学，并通过超图神经网络来直接学习动力学和超图的顺序。

    

    超图动力系统可以展现出与成对交互系统不可观察到的丰富的行为。对于具有假设超图结构的分布式动力系统来说，有一个有趣的问题，即实际上有多少超图结构才是需要的，才能忠实地复制观察到的动态行为。为了回答这个问题，我们提出了一种方法来确定必要的最小超图顺序，以精确地近似相应的动力学。具体而言，当已知动力学类型时，我们开发了一个分析框架，以确定此顺序。我们将这些构想与超图神经网络结合使用，直接从包含观察到的系统轨迹的合成和真实数据集中学习动力学本身和超图的顺序。

    Dynamical systems on hypergraphs can display a rich set of behaviours not observable for systems with pairwise interactions. Given a distributed dynamical system with a putative hypergraph structure, an interesting question is thus how much of this hypergraph structure is actually necessary to faithfully replicate the observed dynamical behaviour. To answer this question, we propose a method to determine the minimum order of a hypergraph necessary to approximate the corresponding dynamics accurately. Specifically, we develop an analytical framework that allows us to determine this order when the type of dynamics is known. We utilize these ideas in conjunction with a hypergraph neural network to directly learn the dynamics itself and the resulting order of the hypergraph from both synthetic and real data sets consisting of observed system trajectories.
    
[^23]: SAPI:环境感知车辆在路口的轨迹预测

    SAPI: Surroundings-Aware Vehicle Trajectory Prediction at Intersections. (arXiv:2306.01812v1 [cs.LG])

    [http://arxiv.org/abs/2306.01812](http://arxiv.org/abs/2306.01812)

    本文提出了一种称为SAPI的深度学习模型，用于在路口预测车辆轨迹，通过实时地图、优先权和周围交通信息来表示和编码周围环境。SAPI能够在预测车辆轨迹时表现出有希望的性能，且优于基准方法。

    

    本文提出了一种深度学习模型，即SAPI，用于在路口预测车辆轨迹。SAPI使用抽象的方式来表示和编码周围环境，通过利用实时地图、优先权和周围交通信息。提出的模型包括两个卷积网络(CNN)和一个基于循环神经网络(RNN)的编码器以及一个解码器。为了充分利用原始历史轨迹信息，在模型中提出了一个细化器来进行回溯操作。我们使用自主车辆在实际路口采集的专有数据集评估了SAPI。结果表明，在预测路口车辆轨迹时，SAPI表现出有希望的性能，并且优于基准方法。6秒预测的平均位移误差(ADE)和最终位移误差(FDE)分别为1.84米和4.32米。我们还展示了该模型能够准确预测车辆轨迹。

    In this work we propose a deep learning model, i.e., SAPI, to predict vehicle trajectories at intersections. SAPI uses an abstract way to represent and encode surrounding environment by utilizing information from real-time map, right-of-way, and surrounding traffic. The proposed model consists of two convolutional network (CNN) and recurrent neural network (RNN)-based encoders and one decoder. A refiner is proposed to conduct a look-back operation inside the model, in order to make full use of raw history trajectory information. We evaluate SAPI on a proprietary dataset collected in real-world intersections through autonomous vehicles. It is demonstrated that SAPI shows promising performance when predicting vehicle trajectories at intersection, and outperforms benchmark methods. The average displacement error(ADE) and final displacement error(FDE) for 6-second prediction are 1.84m and 4.32m respectively. We also show that the proposed model can accurately predict vehicle trajectories i
    
[^24]: DVFO：DNN边缘推理的动态电压、频率缩放和工作负载卸载

    DVFO: Dynamic Voltage, Frequency Scaling and Workload Offloading for DNN Edge Inference. (arXiv:2306.01811v1 [cs.LG])

    [http://arxiv.org/abs/2306.01811](http://arxiv.org/abs/2306.01811)

    提出了一种动态电压、频率缩放和工作负载卸载的DNN边缘推理框架DVFO，它通过深度强化学习来联合优化DVFS和卸载参数，实现了对边缘设备计算资源的优化，同时提高了DNN模型的能源效率。

    

    由于边缘设备资源限制和深度神经网络（DNN）模型的不同特性，优化边缘设备上DNN推理性能（在能源消耗和推理延迟方面）是一个巨大的挑战。除了动态电压频率缩放（DVFS）技术，边缘云架构提供了一种协作方法，以实现高效的DNN推理。然而，当前的边缘云协作推理方法尚未对边缘设备上的各种计算资源进行优化。因此，我们提出了DVFO，这是一种新颖的基于DVFS的边缘云协作推理框架，它通过深度强化学习（DRL）联合优化DVFS和卸载参数。具体来说，DVFO自动共同优化了1）边缘设备的CPU、GPU和内存频率，以及2）要卸载到云服务器的特征映射。此外，它利用一种思考即行动的并发机制加速DRL学习过程，并利用空间通道关注机制进一步降低DNN模型的能源消耗。在真实数据集上的实验结果表明，DVFO在推理准确度和能源效率方面均优于现有的先进方法。

    Due to edge device resource constraints and different characteristics of deep neural network (DNN) models, it is a big challenge to optimize DNN inference performance in terms of energy consumption and inference latency on edge devices. In addition to the dynamic voltage frequency scaling (DVFS) technique, the edge-cloud architecture provides a collaborative approach to efficient DNN inference. However, current edge-cloud collaborative inference methods have not optimized various compute resources on edge devices. Thus, we propose DVFO, a novel DVFS-enabled edge-cloud collaborative inference framework, which jointly optimize DVFS and offloading parameters via deep reinforcement learning (DRL). Specifically, DVFO automatically co-optimizes 1) CPU, GPU and memory frequencies of edge devices, and 2) feature maps to be offloaded to cloud servers. In addition, it leverages a thinking-while-moving concurrent mechanism to accelerate the DRL learning process, and a spatialchannel attention mec
    
[^25]: 基于预测-校正的对抗攻击

    Adversarial Attack Based on Prediction-Correction. (arXiv:2306.01809v1 [cs.CR])

    [http://arxiv.org/abs/2306.01809](http://arxiv.org/abs/2306.01809)

    本论文介绍了一种基于预测-校正的对抗攻击方法，该方法能够有效对抗深度神经网络中的梯度攻击，并具有较好的可扩展性。

    

    深度神经网络(DNNs)容易受到对抗样本的攻击，攻击者将微小的扰动添加到原本的样本中。现有攻击方法中，添加的扰动主要由损失函数对输入的梯度决定。本文首次研究了梯度攻击与求解普通微分方程 (ODE) 数值方法之间的密切关系。受ODE数值解的启发，提出了基于预测-校正(PC)的新型对抗攻击。在我们提出的PC-based攻击中，可以先选择一些现有的攻击方法生成一个预测的样本，然后将预测样本和当前样本组合在一起，以确定所添加的扰动。所提出的方法具有良好的可扩展性，能够轻松应用于所有可用的梯度攻击。广泛的实验证明，与最先进的梯度对抗攻击相比，我们的方法可以更有效地比较进行压缩和加速计算。

    Deep neural networks (DNNs) are vulnerable to adversarial examples obtained by adding small perturbations to original examples. The added perturbations in existing attacks are mainly determined by the gradient of the loss function with respect to the inputs. In this paper, the close relationship between gradient-based attacks and the numerical methods for solving ordinary differential equation (ODE) is studied for the first time. Inspired by the numerical solution of ODE, a new prediction-correction (PC) based adversarial attack is proposed. In our proposed PC-based attack, some existing attack can be selected to produce a predicted example first, and then the predicted example and the current example are combined together to determine the added perturbations. The proposed method possesses good extensibility and can be applied to all available gradient-based attacks easily. Extensive experiments demonstrate that compared with the state-of-the-art gradient-based adversarial attacks, our
    
[^26]: 从Diffusion模型中提取奖励函数

    Extracting Reward Functions from Diffusion Models. (arXiv:2306.01804v1 [cs.LG])

    [http://arxiv.org/abs/2306.01804](http://arxiv.org/abs/2306.01804)

    本论文提出了一种从两个Diffusion模型中提取奖励函数的实用学习算法，可以在导航环境中找到正确的奖励函数，并以此控制Diffusion模型学习足够复杂的任务。

    

    Diffusion模型在图像生成方面取得了显著成果，也被用于学习序列决策任务中的高性能策略。我们考虑通过比较建模低奖励行为和建模高奖励行为的决策传播模型来提取奖励函数的问题；这与逆强化学习相关。我们设计了一种实用的学习算法，通过将神经网络参数化的奖励函数的梯度与两个Diffusion模型的输出差异对齐来提取奖励函数。我们的方法可以在导航环境中找到正确的奖励函数，并且表明可以通过控制Diffusion模型来学习足够复杂的任务。

    Diffusion models have achieved remarkable results in image generation, and have similarly been used to learn high-performing policies in sequential decision-making tasks. Decision-making diffusion models can be trained on lower-quality data, and then be steered with a reward function to generate near-optimal trajectories. We consider the problem of extracting a reward function by comparing a decision-making diffusion model that models low-reward behavior and one that models high-reward behavior; a setting related to inverse reinforcement learning. We first define the notion of a relative reward function of two diffusion models and show conditions under which it exists and is unique. We then devise a practical learning algorithm for extracting it by aligning the gradients of a reward function -- parametrized by a neural network -- to the difference in outputs of both diffusion models. Our method finds correct reward functions in navigation environments, and we demonstrate that steering 
    
[^27]: 线性时间高斯过程推断神经脉冲序列中的潜在轨迹

    Linear Time GPs for Inferring Latent Trajectories from Neural Spike Trains. (arXiv:2306.01802v1 [q-bio.NC])

    [http://arxiv.org/abs/2306.01802](http://arxiv.org/abs/2306.01802)

    本论文提出了cvHM，一种使用Hida-Mat'ern核和共轭计算变分推理（CVI）的潜在高斯过程模型的通用推理框架，能够以线性时间复杂度执行潜在神经轨迹的变分推断，以适应任意的似然函数。

    

    潜在高斯过程模型被广泛应用于神经科学中，以从顺序观测中揭示隐藏状态的演化，主要用于神经活动记录。虽然潜在GP模型在理论上提供了一个有原则和有力的解决方案，但在非共轭设置中的不可行后验需要近似推断方案，这些方案可能缺乏可扩展性。在这项工作中，我们提出了cvHM，一种使用Hida-Mat'ern核和共轭计算变分推理（CVI）的潜在GP模型的通用推理框架。使用cvHM，我们能够以线性时间复杂度执行潜在神经轨迹的变分推断，以适应任意的似然函数。使用Hida-Mat'ern GPs对平稳核进行重新参数化，帮助我们将编码先前假设的潜在变量模型与编码轨迹假设的GP连接起来，通过动力系统。与以前的工作不同，我们使用双向信息过滤，导致模型推断更清晰，估计更准确。

    Latent Gaussian process (GP) models are widely used in neuroscience to uncover hidden state evolutions from sequential observations, mainly in neural activity recordings. While latent GP models provide a principled and powerful solution in theory, the intractable posterior in non-conjugate settings necessitates approximate inference schemes, which may lack scalability. In this work, we propose cvHM, a general inference framework for latent GP models leveraging Hida-Mat\'ern kernels and conjugate computation variational inference (CVI). With cvHM, we are able to perform variational inference of latent neural trajectories with linear time complexity for arbitrary likelihoods. The reparameterization of stationary kernels using Hida-Mat\'ern GPs helps us connect the latent variable models that encode prior assumptions through dynamical systems to those that encode trajectory assumptions through GPs. In contrast to previous work, we use bidirectional information filtering, leading to a more
    
[^28]: 在广告拍卖中通过点击率预测来实现福利最大化的成对排名损失函数

    Pairwise Ranking Losses of Click-Through Rates Prediction for Welfare Maximization in Ad Auctions. (arXiv:2306.01799v1 [cs.GT])

    [http://arxiv.org/abs/2306.01799](http://arxiv.org/abs/2306.01799)

    本文提出了一种新的加权排名损失函数来训练CTR模型，以实现广告拍卖中的福利最大化，并且没有假设eCPM的先验分布，同时避免了朴素地应用现有学习排名方法的问题。

    

    本文研究了设计点击率（CTR）损失函数以在广告拍卖中优化（社会）福利。现有的研究要么只关注于CTR预测，而没有考虑广告拍卖中的业务目标（例如福利），要么假设参与者期望每次展示费用（eCPM）的分布是先验已知的，然后使用各种附加假设来推导用于预测CTR的损失函数。在这项工作中，我们将广告拍卖的福利目标带回CTR预测中，并提出了一种新型的加权排名损失函数来训练CTR模型。与现有文献相比，我们的方法在不假设eCPM分布的情况下提供了对福利的可证明保证，同时避免了朴素地应用现有学习排名方法时的棘手问题。此外，我们提出了一种在“老师网络”生成标签的情况下使用校准损失的理论可证明技术。

    We study the design of loss functions for click-through rates (CTR) to optimize (social) welfare in advertising auctions. Existing works either only focus on CTR predictions without consideration of business objectives (e.g., welfare) in auctions or assume that the distribution over the participants' expected cost-per-impression (eCPM) is known a priori, then use various additional assumptions on the parametric form of the distribution to derive loss functions for predicting CTRs. In this work, we bring back the welfare objectives of ad auctions into CTR predictions and propose a novel weighted rankloss to train the CTR model. Compared to existing literature, our approach provides a provable guarantee on welfare but without assumptions on the eCPMs' distribution while also avoiding the intractability of naively applying existing learning-to-rank methods. Further, we propose a theoretically justifiable technique for calibrating the losses using labels generated from a teacher network, o
    
[^29]: DiffPack：自回归蛋白质侧链包装的扭转扩散模型

    DiffPack: A Torsional Diffusion Model for Autoregressive Protein Side-Chain Packing. (arXiv:2306.01794v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.01794](http://arxiv.org/abs/2306.01794)

    本文提出了DiffPack，这是一个自回归的扭转扩散模型，通过在扭曲空间中进行扩散和去噪来学习侧链扭转角度的联合分布，从而准确地预测蛋白质侧链的构象，有效应用于蛋白质结构预测、设计和蛋白质相互作用等领域。

    

    蛋白质在生物功能中起着关键作用，它们的三维结构对于决定它们的功能至关重要。准确地预测给定骨架的蛋白质侧链的构象对于蛋白质结构预测、设计和蛋白质-蛋白质相互作用等应用非常重要。传统方法计算复杂度高，准确度有限，而现有的机器学习方法将问题视为回归任务，忽略了因常量共价键长度和角度所限制而带来的问题。在本研究中，我们提出了DiffPack，这是一个扭转扩散模型，通过在扭曲空间上进行扩散和去噪，学习侧链扭转角度的联合分布，这是侧链包装中唯一的自由度。为了避免同时扰动所有四个扭转角度带来的问题，我们建议从\c {hi}1到\c {hi}4自回归生成四个扭转角度，并训练扩散模型。

    Proteins play a critical role in carrying out biological functions, and their 3D structures are essential in determining their functions. Accurately predicting the conformation of protein side-chains given their backbones is important for applications in protein structure prediction, design and protein-protein interactions. Traditional methods are computationally intensive and have limited accuracy, while existing machine learning methods treat the problem as a regression task and overlook the restrictions imposed by the constant covalent bond lengths and angles. In this work, we present DiffPack, a torsional diffusion model that learns the joint distribution of side-chain torsional angles, the only degrees of freedom in side-chain packing, by diffusing and denoising on the torsional space. To avoid issues arising from simultaneous perturbation of all four torsional angles, we propose autoregressively generating the four torsional angles from \c{hi}1 to \c{hi}4 and training diffusion m
    
[^30]: 任务关系感知的持续用户表示学习

    Task Relation-aware Continual User Representation Learning. (arXiv:2306.01792v1 [cs.IR])

    [http://arxiv.org/abs/2306.01792](http://arxiv.org/abs/2306.01792)

    本文提出了一种新的持续用户表示学习方法TERACON，它能够学习通用的用户表示，而不是为每个任务学习任务特定的用户表示，具有很强的实用性和学习能力。

    

    用户建模是基于其过去行为学习将用户表示为低维表示空间的方法，它受到了工业界提供个性化服务的兴趣激增。以往的用户建模工作主要集中在学习为单一任务而设计的任务特定用户表示上。然而，由于为每个任务学习任务特定用户表示是不可行的，因此最近的研究引入了通用用户表示的概念，即与多种任务相关的更广义用户表示。尽管这些方法非常有效，但由于数据需求、灾难性遗忘以及为持续添加的任务提供有限的学习能力，现有的学习通用用户表示的方法在实际应用中是不切实际的。本文提出了一种新颖的持续用户表示学习方法TERACON，其学习能力不受任务数量限制。

    User modeling, which learns to represent users into a low-dimensional representation space based on their past behaviors, got a surge of interest from the industry for providing personalized services to users. Previous efforts in user modeling mainly focus on learning a task-specific user representation that is designed for a single task. However, since learning task-specific user representations for every task is infeasible, recent studies introduce the concept of universal user representation, which is a more generalized representation of a user that is relevant to a variety of tasks. Despite their effectiveness, existing approaches for learning universal user representations are impractical in real-world applications due to the data requirement, catastrophic forgetting and the limited learning capability for continually added tasks. In this paper, we propose a novel continual user representation learning method, called TERACON, whose learning capability is not limited as the number 
    
[^31]: 机器学习流程的负责任设计模式

    Responsible Design Patterns for Machine Learning Pipelines. (arXiv:2306.01788v1 [cs.SE])

    [http://arxiv.org/abs/2306.01788](http://arxiv.org/abs/2306.01788)

    本文提出了一种综合框架，将负责任设计模式纳入机器学习流程中，以确保AI系统的伦理性和公正性。这个框架包括新的负责任AI设计模式，并指导AI开发人员、数据科学家和决策者在AI开发和部署中实施伦理实践。

    

    将道德实践整合到人工智能(AI)开发过程中对于确保AI的安全、公平和负责任操作至关重要。AI伦理涉及将伦理原则应用于AI系统的整个生命周期。这对于减轻与AI相关的潜在风险和伤害（如算法偏见）至关重要。为实现这一目标，机器学习流程中的负责任设计模式（RDPs）对于确保伦理和公平结果至关重要。在本文中，我们提出了一个综合框架，将RDPs纳入ML流程中，以减轻风险并确保AI系统的伦理发展。我们的框架包括新的负责任AI设计模式，这些模式通过对AI伦理和数据管理专家的调查确定，并通过专家反馈的实际情况进行验证。该框架指导AI开发人员、数据科学家和决策者在AI开发和部署中实施伦理实践。

    Integrating ethical practices into the AI development process for artificial intelligence (AI) is essential to ensure safe, fair, and responsible operation. AI ethics involves applying ethical principles to the entire life cycle of AI systems. This is essential to mitigate potential risks and harms associated with AI, such as algorithm biases. To achieve this goal, responsible design patterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee ethical and fair outcomes. In this paper, we propose a comprehensive framework incorporating RDPs into ML pipelines to mitigate risks and ensure the ethical development of AI systems. Our framework comprises new responsible AI design patterns for ML pipelines identified through a survey of AI ethics and data management experts and validated through real-world scenarios with expert feedback. The framework guides AI developers, data scientists, and policy-makers to implement ethical practices in AI development and deploy responsibl
    
[^32]: 超越排名：探索SERP特征对有机点击率的影响。

    Beyond Rankings: Exploring the Impact of SERP Features on Organic Click-through Rates. (arXiv:2306.01785v1 [cs.IR])

    [http://arxiv.org/abs/2306.01785](http://arxiv.org/abs/2306.01785)

    本研究探讨了SERP特征对点击率的影响，揭示SERP特征不仅是美学成分，而且强烈影响点击率和互联网用户的相关行为，能够显着调节网络流量。

    

    搜索引擎结果页面（SERP）作为通往广阔互联网世界的数字门户。过去几十年以网站排名为中心的研究主要集中在这些页面上，以确定点击率（CTR）。然而，在这段时间内，SERP的景观发生了戏剧性的演变：SERP特征，包括知识面板、媒体画廊、常见问题解答等元素，已经成为这些结果页面中越来越突出的方面。我们的研究研究了这些特征的关键作用，揭示它们不仅是美学成分，而且强烈影响点击率和互联网用户的相关行为。我们展示了这些特征如何显着调节网络流量，无论是放大还是减弱它。我们使用涵盖40个不同美国电子商务领域的67,000个关键字及其相应的Google SERPs的独特数据集，分析了这些复杂的交互作用效应。

    Search Engine Result Pages (SERPs) serve as the digital gateways to the vast expanse of the internet. Past decades have witnessed a surge in research primarily centered on the influence of website ranking on these pages, to determine the click-through rate (CTR). However, during this period, the landscape of SERPs has undergone a dramatic evolution: SERP features, encompassing elements such as knowledge panels, media galleries, FAQs, and more, have emerged as an increasingly prominent facet of these result pages. Our study examines the crucial role of these features, revealing them to be not merely aesthetic components, but strongly influence CTR and the associated behavior of internet users. We demonstrate how these features can significantly modulate web traffic, either amplifying or attenuating it. We dissect these intricate interaction effects leveraging a unique dataset of 67,000 keywords and their respective Google SERPs, spanning over 40 distinct US-based e-commerce domains, gen
    
[^33]: 生成扩散在三维湍流流动中的应用

    Generative Diffusion for 3D Turbulent Flows. (arXiv:2306.01776v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2306.01776](http://arxiv.org/abs/2306.01776)

    该论文提出了一种生成模型，可以在任意三维空间中模拟湍流现象，避免了湍流流动的不可预测性，能够快速生成高质量的流场。

    

    湍流流动通常难以预测，但二维和三维的湍流流动性质不同。在二维情况下，湍流会形成大的、连续的结构，而在三维情况下，旋涡级联成越来越小的尺度，形成许多快速变化的小尺度结构，加剧了不可预测性，难以使用回归方法。本文提出了第一个生成模型，可以在任意三维空间中模拟湍流现象，并引入了一种基于Wasserstein距离的流场质量度量方法。在多个实验中，我们证明了我们的生成扩散模型可以避免湍流流动的不可预测性，并且可以只依靠几何信息生成高质量的样本。此外，我们还展示了我们的模型可以比工业级数值求解器更快地生成湍流流场。

    Turbulent flows are well known to be chaotic and hard to predict; however, their dynamics differ between two and three dimensions. While 2D turbulence tends to form large, coherent structures, in three dimensions vortices cascade to smaller and smaller scales. This cascade creates many fast-changing, small-scale structures and amplifies the unpredictability, making regression-based methods infeasible. We propose the first generative model for forced turbulence in arbitrary 3D geometries and introduce a sample quality metric for turbulent flows based on the Wasserstein distance of the generated velocity-vorticity distribution. In several experiments, we show that our generative diffusion model circumvents the unpredictability of turbulent flows and produces high-quality samples based solely on geometric information. Furthermore, we demonstrate that our model beats an industrial-grade numerical solver in the time to generate a turbulent flow field from scratch by an order of magnitude.
    
[^34]: 语言模型效率研究的定量评估

    A Quantitative Review on Language Model Efficiency Research. (arXiv:2306.01768v1 [cs.LG])

    [http://arxiv.org/abs/2306.01768](http://arxiv.org/abs/2306.01768)

    这篇论文从定量方面对基于Transformers和State Space Models的一系列研究进行了综述，并总结了如何提高语言模型效率。

    

    语言模型是被扩展用于处理一些复杂、自然语言处理任务的有效工具之一。本篇论文涵盖了对于提高语言模型效率这一核心问题的研究概述，并尝试对最近的基于Transformers和State Space Models的研究进行定量分析。

    Language models (LMs) are being scaled and becoming powerful. Improving their efficiency is one of the core research topics in neural information processing systems. Tay et al. (2022) provided a comprehensive overview of efficient Transformers that have become an indispensable staple in the field of NLP. However, in the section of "On Evaluation", they left an open question "which fundamental efficient Transformer one should consider," answered by "still a mystery" because "many research papers select their own benchmarks." Unfortunately, there was not quantitative analysis about the performances of Transformers on any benchmarks. Moreover, state space models (SSMs) have demonstrated their abilities of modeling long-range sequences with non-attention mechanisms, which were not discussed in the prior review. This article makes a meta analysis on the results from a set of papers on efficient Transformers as well as those on SSMs. It provides a quantitative review on LM efficiency researc
    
[^35]: 预训练Transformer用于对抗性样本提纯

    Pre-trained transformer for adversarial purification. (arXiv:2306.01762v1 [cs.CR])

    [http://arxiv.org/abs/2306.01762](http://arxiv.org/abs/2306.01762)

    本文提出了一个快速防御对抗性攻击的方案RaPiD（Rapid Plug-in Defender），通过预训练的Transformer微调来提纯对抗样本，使其逼近清洁数据分布，实验结果表明，在有限数据情况下，该方法优于最先进的方法。

    

    随着越来越多的深度神经网络被部署为各种日常服务，它们的可靠性至关重要。深度神经网络容易受到对抗性攻击的影响，其中逃避攻击是最普遍的一种。最近的研究通常通过对抗训练或利用大量清洁数据的知识来增强其健壮性。然而，在实际应用中，重新训练和部署模型需要大量的计算资源，对在线服务造成重大损失。此外，当检测到某种攻击的对抗性例子时，服务提供者只能获得有限的对抗性样本，而大量的清洁数据可能无法获取。针对这些问题，我们提出了一种新的方案，名为RaPiD（Rapid Plug-in Defender），旨在快速防御具有少量干净和对抗性示例限制的原始服务模型的某种攻击。受到预训练模型提供转移学习良好初始化的通用趋势的启发，我们建议通过微调预先训练的Transformer来提纯对抗性样本。预训练的Transformer作为正则化器，鼓励提纯后的对抗性样本接近清晰数据的分布。实验结果表明，RaPiD在防御各种具有限数据的攻击方面优于最先进的方法。

    With more and more deep neural networks being deployed as various daily services, their reliability is essential. It's frightening that deep neural networks are vulnerable and sensitive to adversarial attacks, the most common one of which for the services is evasion-based. Recent works usually strengthen the robustness by adversarial training or leveraging the knowledge of an amount of clean data. However, in practical terms, retraining and redeploying the model need a large computational budget, leading to heavy losses to the online service. In addition, when adversarial examples of a certain attack are detected, only limited adversarial examples are available for the service provider, while much clean data may not be accessible. Given the mentioned problems, we propose a new scenario, RaPiD (Rapid Plug-in Defender), which is to rapidly defend against a certain attack for the frozen original service model with limitations of few clean and adversarial examples. Motivated by the general
    
[^36]: 使用机器学习区分人类生成文本和ChatGPT生成文本

    Distinguishing Human Generated Text From ChatGPT Generated Text Using Machine Learning. (arXiv:2306.01761v1 [cs.CL])

    [http://arxiv.org/abs/2306.01761](http://arxiv.org/abs/2306.01761)

    本文提出了一种机器学习方法，可以识别出由ChatGPT生成的文本，并以11种算法进行了分类对比分析，在测试中取得了77%的准确度。

    

    ChatGPT是预训练大语言模型家族中的一员，是一种对话人工智能。这种文本生成模型通过监督学习和强化学习进行微调，可以生成看似由自然智能撰写的文本文件。虽然这种生成模型有很多优点，但也存在一些合理的担忧。本文提出了一种基于机器学习的解决方案，可以识别出ChatGPT生成的文本与人类编写的文本，以及在分类过程中共计11种机器学习和深度学习算法的对比分析。我们在一个Kaggle数据集上测试了所提出的模型，该数据集包含10,000个文本，其中5,204个文本是人类从新闻和社交媒体上收集的写作。在由GPT-3.5生成的语料库上，所提出的算法呈现出77%的准确度。

    ChatGPT is a conversational artificial intelligence that is a member of the generative pre-trained transformer of the large language model family. This text generative model was fine-tuned by both supervised learning and reinforcement learning so that it can produce text documents that seem to be written by natural intelligence. Although there are numerous advantages of this generative model, it comes with some reasonable concerns as well. This paper presents a machine learning-based solution that can identify the ChatGPT delivered text from the human written text along with the comparative analysis of a total of 11 machine learning and deep learning algorithms in the classification process. We have tested the proposed model on a Kaggle dataset consisting of 10,000 texts out of which 5,204 texts were written by humans and collected from news and social media. On the corpus generated by GPT-3.5, the proposed algorithm presents an accuracy of 77%.
    
[^37]: 基于Transformer的代码编辑时漏洞检测：零样本、小样本还是微调？

    Transformer-based Vulnerability Detection in Code at EditTime: Zero-shot, Few-shot, or Fine-tuning?. (arXiv:2306.01754v1 [cs.CR])

    [http://arxiv.org/abs/2306.01754](http://arxiv.org/abs/2306.01754)

    本研究使用深度学习在编辑代码的同时检测漏洞，可以高精度、低延迟地检测超过250种漏洞类型的复杂漏洞代码模式，使软件开发人员能够在引入潜在漏洞到代码库之前修复它们。

    

    软件漏洞会给企业带来重大损失。尽管针对软件漏洞检测方法的研究和开发已经进行了广泛的努力，但未发现的漏洞仍会对软件所有者和用户造成风险。许多当前的漏洞检测方法要求代码片段能够在尝试检测之前编译和构建。不幸的是，这会在注入漏洞到删除漏洞的时间之间引入很长的延迟，这可能会大大增加修复漏洞的成本。本文利用深度学习在包含250多种漏洞类型的大型数据集上学习复杂的漏洞代码模式，并在代码编辑时检测漏洞。我们讨论了训练Transformer的神经网络的方法，它在零样本、小样本和微调设置中实现了最先进的性能。我们的实验表明，我们的系统可以高精度、低延迟地检测漏洞，使软件开发人员能够在引入潜在漏洞到代码库之前修复它们。

    Software vulnerabilities bear enterprises significant costs. Despite extensive efforts in research and development of software vulnerability detection methods, uncaught vulnerabilities continue to put software owners and users at risk. Many current vulnerability detection methods require that code snippets can compile and build before attempting detection. This, unfortunately, introduces a long latency between the time a vulnerability is injected to the time it is removed, which can substantially increases the cost of fixing a vulnerability. We recognize that the current advances in machine learning can be used to detect vulnerable code patterns on syntactically incomplete code snippets as the developer is writing the code at EditTime. In this paper we present a practical system that leverages deep learning on a large-scale data set of vulnerable code patterns to learn complex manifestations of more than 250 vulnerability types and detect vulnerable code patterns at EditTime. We discus
    
[^38]: 在冠状动脉 CT 血管造影中处理标签不确定性的方法研究——以牧羊人钩形右冠状动脉为例

    Handling Label Uncertainty on the Example of Automatic Detection of Shepherd's Crook RCA in Coronary CT Angiography. (arXiv:2306.01752v1 [eess.IV])

    [http://arxiv.org/abs/2306.01752](http://arxiv.org/abs/2306.01752)

    本论文提出了一种利用一维卷积神经网络在冠状动脉 CT 血管造影中自动检测牧羊人钩形右冠状动脉的方法，并探讨了如何处理标签不确定性的问题。

    

    冠状动脉疾病（CAD）通常通过向患病冠状动脉插入导管进行微创治疗。如果患者出现牧羊人钩形（SC）右冠状动脉（RCA）——这是冠状血管的一种解剖正常变异——则会增加这种手术的复杂性。从冠状动脉 CT 血管造影筛查中自动报告此变异有助于风险评估。我们提出了一种一维卷积神经网络，利用一系列残差扩张卷积来自动确定这种正常变异，并从先前提取的血管中心线中提取信息。由于 SC RCA 相对于具体测量并不清晰，标签还包括定性方面。因此，我们的数据集中有 4.23% 的样本标记为不确定的 SC RCA，5.97% 的样本标记为确定的 SC RCA。我们探讨了处理这种标签不确定性的措施，包括全局/模型随机分配、排除和软标签分配。

    Coronary artery disease (CAD) is often treated minimally invasively with a catheter being inserted into the diseased coronary vessel. If a patient exhibits a Shepherd's Crook (SC) Right Coronary Artery (RCA) - an anatomical norm variant of the coronary vasculature - the complexity of this procedure is increased. Automated reporting of this variant from coronary CT angiography screening would ease prior risk assessment. We propose a 1D convolutional neural network which leverages a sequence of residual dilated convolutions to automatically determine this norm variant from a prior extracted vessel centerline. As the SC RCA is not clearly defined with respect to concrete measurements, labeling also includes qualitative aspects. Therefore, 4.23% samples in our dataset of 519 RCA centerlines were labeled as unsure SC RCAs, with 5.97% being labeled as sure SC RCAs. We explore measures to handle this label uncertainty, namely global/model-wise random assignment, exclusion, and soft label assi
    
[^39]: 随机投影和符号随机投影的差分隐私算法

    Differential Privacy with Random Projections and Sign Random Projections. (arXiv:2306.01751v1 [cs.CR])

    [http://arxiv.org/abs/2306.01751](http://arxiv.org/abs/2306.01751)

    本文提出了一系列差分隐私算法，其中iDP-SignRP算法在个体差分隐私设置下效果显著，DP-SignOPORP算法改进了现有算法，DP-OPORP算法表现最优，iDP提供了一种适用于特定数据集的隐私保护解决方案。

    

    本文提出了一系列基于随机投影（RP）的差分隐私（DP）算法，适用于机器学习、数据挖掘和信息检索等各种应用。其中，基于符号随机投影（SignRP）的iDP-SignRP算法在个体差分隐私（iDP）设置下非常有效，而DP-SignOPORP算法在标准DP设置下利用“一次排列+一次随机投影”（OPORP）极大地改进了文献中现有的算法。除不考虑符号之外，在DP-RP家族中，DP-OPORP算法表现最佳。iDP（个体差分隐私）的概念仅适用于特定的数据集。虽然iDP不是严格的DP，但在某些应用中（如向小组用户发布包括嵌入信息或个性化推荐等内容的数据集，而不泄露不属于该组的个人的任何私人信息）可能很有用。

    In this paper, we develop a series of differential privacy (DP) algorithms from a family of random projections (RP), for general applications in machine learning, data mining, and information retrieval. Among the presented algorithms, \textbf{iDP-SignRP} is remarkably effective under the setting of ``individual differential privacy'' (iDP), based on sign random projections (SignRP). Also, \textbf{DP-SignOPORP} considerably improves existing algorithms in the literature under the standard DP setting, using ``one permutation + one random projection'' (OPORP), where OPORP is a variant of the celebrated count-sketch method with fixed-length binning and normalization. Without taking signs, among the DP-RP family, \textbf{DP-OPORP} achieves the best performance.  The concept of iDP (individual differential privacy) is defined only on a particular dataset of interest. While iDP is not strictly DP, iDP might be useful in certain applications, such as releasing a dataset (including sharing embe
    
[^40]: 用形式化方法打破XAI神话-初步结果

    Disproving XAI Myths with Formal Methods -- Initial Results. (arXiv:2306.01744v1 [cs.AI])

    [http://arxiv.org/abs/2306.01744](http://arxiv.org/abs/2306.01744)

    论文介绍了XAI中最严重的一些误解，并展示了如何使用形式化方法来打破这些谬见和开发更实用的替代方案。

    

    近年来，机器学习（ML）的进展既令人印象深刻又深远。然而，ML模型的部署仍然受到人们对最佳表现的ML模型如何进行预测的信任缺乏的影响。在高风险或安全关键领域使用ML模型时，缺乏信任的问题更加严重。可解释的人工智能（XAI）是为提供可信赖AI而进行的不断努力的核心。不幸的是，XAI充斥着关键误解，这些误解助长了不信任而不是建立信任。本文详细介绍了XAI中最明显的一些误解，并展示了如何使用形式化方法来推翻这些误解，以及设计实际有效的替代方案。

    The advances in Machine Learning (ML) in recent years have been both impressive and far-reaching. However, the deployment of ML models is still impaired by a lack of trust in how the best-performing ML models make predictions. The issue of lack of trust is even more acute in the uses of ML models in high-risk or safety-critical domains. eXplainable artificial intelligence (XAI) is at the core of ongoing efforts for delivering trustworthy AI. Unfortunately, XAI is riddled with critical misconceptions, that foster distrust instead of building trust. This paper details some of the most visible misconceptions in XAI, and shows how formal methods have been used, both to disprove those misconceptions, but also to devise practically effective alternatives.
    
[^41]: 超越消极情绪：关于希望言论检测的重新分析和后续实验

    Beyond Negativity: Re-Analysis and Follow-Up Experiments on Hope Speech Detection. (arXiv:2306.01742v1 [cs.CL])

    [http://arxiv.org/abs/2306.01742](http://arxiv.org/abs/2306.01742)

    本研究旨在找到计算效率高、可比或更好的希望言论检测方法，并公开了代码库。

    

    健康专家们认为，希望在增强个人的身心健康、促进康复和恢复方面起着至关重要的作用。希望言论是指在评论、帖子和其他社交媒体消息中提供支持、安慰、建议、启示和见解的言论。希望言论的检测涉及这种文本内容的分析，旨在识别能够唤起人们积极情绪的信息。我们的研究旨在找到计算效率高、可比或更好的希望言论检测方法。我们还将我们的代码库公开在 https://github.com/aflah02/Hope_Speech_Detection 上。

    Health experts assert that hope plays a crucial role in enhancing individuals' physical and mental well-being, facilitating their recovery, and promoting restoration. Hope speech refers to comments, posts and other social media messages that offer support, reassurance, suggestions, inspiration, and insight. The detection of hope speech involves the analysis of such textual content, with the aim of identifying messages that invoke positive emotions in people. Our study aims to find computationally efficient yet comparable/superior methods for hope speech detection. We also make our codebase public at https://github.com/aflah02/Hope_Speech_Detection
    
[^42]: 基于困难系统的局部消息传递

    Local Message Passing on Frustrated Systems. (arXiv:2306.01494v1 [cs.LG])

    [http://arxiv.org/abs/2306.01494](http://arxiv.org/abs/2306.01494)

    本文提出了一种优化的基于困难系统的局部消息传递算法，能够在循环图上获得良好的表现。

    

    因子图上的消息传递是概率推理的强大框架，在各种科学领域中具有重要应用。最广泛使用的消息传递方案是求和-积算法（SPA），它在树上可得到精确结果，但在具有许多小循环的图上往往失败。我们寻求一种替代的消息传递算法，特别适用于这种循环图。为此，我们挑战了SPA的外部原则，这失去了在循环图上的客观性。我们进一步用数据驱动的方式将底层图的因子节点处的本地SPA消息更新规则替换为通用映射。这些修改提高了性能，同时保持了SPA的简单性。我们对两类循环图进行了评估：2x2完全连接的Ising网格和线性通信通道上的符号检测因子图。

    Message passing on factor graphs is a powerful framework for probabilistic inference, which finds important applications in various scientific domains. The most wide-spread message passing scheme is the sum-product algorithm (SPA) which gives exact results on trees but often fails on graphs with many small cycles. We search for an alternative message passing algorithm that works particularly well on such cyclic graphs. Therefore, we challenge the extrinsic principle of the SPA, which loses its objective on graphs with cycles. We further replace the local SPA message update rule at the factor nodes of the underlying graph with a generic mapping, which is optimized in a data-driven fashion. These modifications lead to a considerable improvement in performance while preserving the simplicity of the SPA. We evaluate our method for two classes of cyclic graphs: the 2x2 fully connected Ising grid and factor graphs for symbol detection on linear communication channels with inter-symbol interf
    
[^43]: 通用等变Transformer：用于3D分子相互作用学习

    Generalist Equivariant Transformer Towards 3D Molecular Interaction Learning. (arXiv:2306.01474v1 [cs.LG])

    [http://arxiv.org/abs/2306.01474](http://arxiv.org/abs/2306.01474)

    本文提出了一种通用等变Transformer用于学习3D分子相互作用，该模型具有双层注意力模块、前馈模块和层归一化模块，每个模块都是E（3）等变的，可以有效地捕捉块级和原子级的交互，实验结果表明其在预测蛋白质-蛋白质亲和力、配体结合亲和力和配体效力方面优于各种最先进的方法。

    

    生物学和药物开发中的许多过程涉及不同分子之间的各种3D相互作用，例如蛋白质与蛋白质，蛋白质与小分子等。设计一个通用模型来学习普适的分子相互作用具有重要价值，但也具有挑战性，因为不同的分子通常以不同粒度表示。本文首先提出了将3D分子通用表示为集合的几何图形图，与传统单层表示形式形成对比。在提出的统一表示下，我们提出了通用等变Transformer（GET），以有效地捕捉稀疏块级和密集原子级交互。具体而言，GET由双层注意力模块、前馈模块和层归一化模块组成，值得注意的是，每个模块都是E（3）等变的，以满足3D世界的对称性。在预测蛋白质-蛋白质亲和力、配体结合亲和力和配体效力方面进行了大量实验，表明GET优于各种最先进的方法。

    Many processes in biology and drug discovery involve various 3D interactions between different molecules, such as protein and protein, protein and small molecule, etc. Designing a generalist model to learn universal molecular interactions is valuable yet challenging, given that different molecules are usually represented in different granularity. In this paper, we first propose to universally represent a 3D molecule as a geometric graph of sets, in contrast to conventional single-level representations. Upon the proposed unified representation, we then propose a Generalist Equivariant Transformer (GET) to effectively capture both sparse block-level and dense atom-level interactions. To be specific, GET consists of a bilevel attention module, a feed-forward module and a layer normalization module, where, notably, each module is E(3) equivariant to meet the symmetry of 3D world. Extensive experiments on the prediction of protein-protein affinity, ligand binding affinity, and ligand effica
    
[^44]: 一种强鲁棒性和可推广性的图卷积方法用于细微癫痫病灶的分割

    Robust and Generalisable Segmentation of Subtle Epilepsy-causing Lesions: a Graph Convolutional Approach. (arXiv:2306.01375v1 [eess.IV])

    [http://arxiv.org/abs/2306.01375](http://arxiv.org/abs/2306.01375)

    该论文提出了一种通过图卷积网络进行语义分割的方法，能够对细微的癫痫病灶进行识别，降低误报率。

    

    该论文提出了使用图卷积网络（GCN）进行语义分割的方法，以解决细微的局限性皮质发育不良（FCD）病变的识别问题。研究人员通过添加辅助损失和弱监督分类损失的方式，使模型能够学习到更多的空间关系，从而降低误报率。该方法在多个中心的数据集上进行了测试。

    Focal cortical dysplasia (FCD) is a leading cause of drug-resistant focal epilepsy, which can be cured by surgery. These lesions are extremely subtle and often missed even by expert neuroradiologists. "Ground truth" manual lesion masks are therefore expensive, limited and have large inter-rater variability. Existing FCD detection methods are limited by high numbers of false positive predictions, primarily due to vertex- or patch-based approaches that lack whole-brain context. Here, we propose to approach the problem as semantic segmentation using graph convolutional networks (GCN), which allows our model to learn spatial relationships between brain regions. To address the specific challenges of FCD identification, our proposed model includes an auxiliary loss to predict distance from the lesion to reduce false positives and a weak supervision classification loss to facilitate learning from uncertain lesion masks. On a multi-centre dataset of 1015 participants with surface-based feature
    
[^45]: 带有重尾奖励的差分隐私式情节强化学习

    Differentially Private Episodic Reinforcement Learning with Heavy-tailed Rewards. (arXiv:2306.01121v1 [cs.LG])

    [http://arxiv.org/abs/2306.01121](http://arxiv.org/abs/2306.01121)

    本研究针对重尾奖励的有限步骤表格马尔可夫决策过程问题探讨了差分隐私限制下的两种框架，即价值迭代和策略优化，同时考虑了联合差分隐私和本地差分隐私模型，并为两种情况提供了遗憾上限。

    

    本文研究了差分隐私(DP)限制下的重尾奖励的（有限步骤表格）马尔可夫决策过程(MDP)问题。与先前的私有强化学习研究通常假设奖励来自一些有界或次高斯分布以确保DP相比，我们考虑奖励分布只有有限的$(1+v)$阶矩的情况，$v \in (0,1]$。通过使用奖励的健壮均值估计器，我们首先提出了两种针对重尾MDP的框架，即一个用于价值迭代，另一个用于策略优化。在每个框架下，我们考虑了联合差分隐私(JDP)和本地差分隐私(LDP)模型。基于我们的框架，我们为JDP和LDP情况提供了遗憾上限，并表明分布的矩和隐私预算都对遗憾有重要影响。最后，我们建立了遗憾最小化的下限。

    In this paper, we study the problem of (finite horizon tabular) Markov decision processes (MDPs) with heavy-tailed rewards under the constraint of differential privacy (DP). Compared with the previous studies for private reinforcement learning that typically assume rewards are sampled from some bounded or sub-Gaussian distributions to ensure DP, we consider the setting where reward distributions have only finite $(1+v)$-th moments with some $v \in (0,1]$. By resorting to robust mean estimators for rewards, we first propose two frameworks for heavy-tailed MDPs, i.e., one is for value iteration and another is for policy optimization. Under each framework, we consider both joint differential privacy (JDP) and local differential privacy (LDP) models. Based on our frameworks, we provide regret upper bounds for both JDP and LDP cases and show that the moment of distribution and privacy budget both have significant impacts on regrets. Finally, we establish a lower bound of regret minimization
    
[^46]: SnapFusion：移动设备上两秒内的文本到图像扩散模型

    SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds. (arXiv:2306.00980v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.00980](http://arxiv.org/abs/2306.00980)

    本文提出了一种通用方法，首次在移动设备上运行文本到图像扩散模型不到2秒，这是通过引入高效的网络架构和改进步骤蒸馏来实现的。

    

    文本到图像扩散模型可以从自然语言描述中创建出惊人的图像，不亚于专业艺术家和摄影师的作品。然而，这些模型较大，具有复杂的网络架构和数十个去噪迭代，使其计算昂贵且运行缓慢。因此，需要高端GPU和基于云的推理来按比例运行扩散模型。这是昂贵的，并且涉及隐私问题，尤其是当用户数据发送到第三方时。为了克服这些挑战，我们提出了一种通用方法，首次在不到2秒钟内解锁了在移动设备上运行文本到图像扩散模型，通过引入高效的网络架构和改进步骤蒸馏来实现此目标。

    Text-to-image diffusion models can create stunning images from natural language descriptions that rival the work of professional artists and photographers. However, these models are large, with complex network architectures and tens of denoising iterations, making them computationally expensive and slow to run. As a result, high-end GPUs and cloud-based inference are required to run diffusion models at scale. This is costly and has privacy implications, especially when user data is sent to a third party. To overcome these challenges, we present a generic approach that, for the first time, unlocks running text-to-image diffusion models on mobile devices in less than $2$ seconds. We achieve so by introducing efficient network architecture and improving step distillation. Specifically, we propose an efficient UNet by identifying the redundancy of the original model and reducing the computation of the image decoder via data distillation. Further, we enhance the step distillation by explori
    
[^47]: STEVE-1: 一个用于Minecraft中文本-行为生成的生成模型

    STEVE-1: A Generative Model for Text-to-Behavior in Minecraft. (arXiv:2306.00937v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2306.00937](http://arxiv.org/abs/2306.00937)

    STEVE-1 是一种新的生成模型，能够在Minecraft中跟随各种短期开放型文本和视觉指令。STEVE-1利用预先训练的模型和最佳实践，通过自监督的行为克隆和回顾重新标记来微调，避免了昂贵的人工注释。

    

    建立对文本指令做出响应的AI模型对于连续性决策任务来说是具有挑战性的。本文介绍了一种名为STEVE-1的Minecraft指令调整型视频预训练模型，展示了DALL-E 2中使用的unCLIP方法也对创建指令跟随连续决策代理非常有效。STEVE-1分为两个步骤进行训练：首先是将预先训练的VPT模型适应MineCLIP的潜在空间中的指令，然后训练一个先验模型以从文本预测潜在代码。这使我们能够通过自监督的行为克隆和回顾重新标记来微调VPT，避免需要昂贵的人工文本注释。通过利用VPT和MineCLIP等预先训练的模型，并采用文本条件的图像生成的最佳实践，STEVE-1的训练成本仅为60美元，并且可以在Minecraft中遵循各种短期开放型文本和视觉指令。STEVE-1为开放的指令跟随连续决策代理设定了一个新的标准。

    Constructing AI models that respond to text instructions is challenging, especially for sequential decision-making tasks. This work introduces an instruction-tuned Video Pretraining (VPT) model for Minecraft called STEVE-1, demonstrating that the unCLIP approach, utilized in DALL-E 2, is also effective for creating instruction-following sequential decision-making agents. STEVE-1 is trained in two steps: adapting the pretrained VPT model to follow commands in MineCLIP's latent space, then training a prior to predict latent codes from text. This allows us to finetune VPT through self-supervised behavioral cloning and hindsight relabeling, bypassing the need for costly human text annotations. By leveraging pretrained models like VPT and MineCLIP and employing best practices from text-conditioned image generation, STEVE-1 costs just $60 to train and can follow a wide range of short-horizon open-ended text and visual instructions in Minecraft. STEVE-1 sets a new bar for open-ended instructi
    
[^48]: 基于内容的图像检索的类锚点边距损失

    Class Anchor Margin Loss for Content-Based Image Retrieval. (arXiv:2306.00630v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.00630](http://arxiv.org/abs/2306.00630)

    本论文提出一种新颖的斥力-吸引力损失函数，该函数位于度量学习范式中，可以直接优化L2度量，无需生成成对，在多个数据集上的实验表明，在检索准确性和效率方面，该方法优于现有技术。

    

    神经网络在内容为基础的图像检索（CBIR）中的性能受所选的损失（目标）函数的影响很大。神经模型的大多数目标函数可以分为度量学习和统计学习两类。度量学习方法需要成对挖掘策略，这往往缺乏效率，而统计学习方法由于其间接特征优化而无法生成高度压缩的特征。为此，我们提出了一种新颖的斥力-吸引力损失函数，位于度量学习范式中，却可以直接优化L2度量，无需生成成对。我们的损失由三个组成部分组成。一个主要目标确保学习到的特征被吸引到各自指定的可学习类锚点。第二个损失组分对锚点进行调节，强制它们相互之间有一定间隔，而第三个目标确保锚点不会崩溃为零。此外，我们开发了一种更高效的变体，它不需要计算完整的成对距离矩阵。我们在多个数据集上的实验表明，我们提出的损失在检索准确性和效率方面优于现有技术。

    The performance of neural networks in content-based image retrieval (CBIR) is highly influenced by the chosen loss (objective) function. The majority of objective functions for neural models can be divided into metric learning and statistical learning. Metric learning approaches require a pair mining strategy that often lacks efficiency, while statistical learning approaches are not generating highly compact features due to their indirect feature optimization. To this end, we propose a novel repeller-attractor loss that falls in the metric learning paradigm, yet directly optimizes for the L2 metric without the need of generating pairs. Our loss is formed of three components. One leading objective ensures that the learned features are attracted to each designated learnable class anchor. The second loss component regulates the anchors and forces them to be separable by a margin, while the third objective ensures that the anchors do not collapse to zero. Furthermore, we develop a more eff
    
[^49]: 在雾环境中进行预测复制的时间运动预测

    Predicting Temporal Aspects of Movement for Predictive Replication in Fog Environments. (arXiv:2306.00575v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2306.00575](http://arxiv.org/abs/2306.00575)

    本文研究了时间预测在雾环境下预测复制中的应用，提出了一种利用Holder-Winter指数平滑法进行时间预测的新模型，可以在减少多余数据的同时，只有微小的数据可用性降低。

    

    充分利用雾环境的好处，有效地管理数据位置非常重要。盲目或反应式的数据复制无法充分利用雾计算的潜力，需要更先进的技术来预测客户端何时何地连接。虽然空间预测受到了相当多的关注，但时间预测仍未得到充分研究。本文通过研究将时间预测纳入现有空间预测模型的优势来填补这一空白。我们还在预测复制的背景下对时空预测模型（如深度神经网络和马尔可夫模型）进行全面分析。我们提出了一种利用Holder-Winter指数平滑法进行时间预测的新模型，利用顺序和周期性用户移动模式。在模拟真实用户轨迹的雾网络中，我们的模型在多余数据上实现了15％的降低，而数据可用性只有1％的微小降低。

    To fully exploit the benefits of the fog environment, efficient management of data locality is crucial. Blind or reactive data replication falls short in harnessing the potential of fog computing, necessitating more advanced techniques for predicting where and when clients will connect. While spatial prediction has received considerable attention, temporal prediction remains understudied.  Our paper addresses this gap by examining the advantages of incorporating temporal prediction into existing spatial prediction models. We also provide a comprehensive analysis of spatio-temporal prediction models, such as Deep Neural Networks and Markov models, in the context of predictive replication. We propose a novel model using Holt-Winter's Exponential Smoothing for temporal prediction, leveraging sequential and periodical user movement patterns. In a fog network simulation with real user trajectories our model achieves a 15% reduction in excess data with a marginal 1% decrease in data availabi
    
[^50]: 从单个快照中重建图扩散历史

    Reconstructing Graph Diffusion History from a Single Snapshot. (arXiv:2306.00488v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.00488](http://arxiv.org/abs/2306.00488)

    本文研究了从单个快照中重建图扩散历史的问题，揭示了现有方法的局限性，并提出了一种新的方法。

    

    图扩散在许多重要应用中普遍存在。在这些应用中，完整的扩散历史在确定动态模式、反思预防措施和预测干预效果方面起着至关重要的作用。尽管它们很重要，但完整的扩散历史很少可用，并且由于病态、爆炸性的搜索空间和训练数据的缺乏而具有极高的挑战性。迄今为止，很少有方法用于扩散历史重建。它们仅基于最大似然估计（MLE）公式，需要知道真实的扩散参数。在本文中，我们研究了一个更难的问题，即从单个快照（DASH）中重建扩散历史，我们试图仅从最终快照重建历史，而不知道真实的扩散参数。我们从理论分析开始，揭示了MLE公式的基本限制。

    Diffusion on graphs is ubiquitous with numerous high-impact applications. In these applications, complete diffusion histories play an essential role in terms of identifying dynamical patterns, reflecting on precaution actions, and forecasting intervention effects. Despite their importance, complete diffusion histories are rarely available and are highly challenging to reconstruct due to ill-posedness, explosive search space, and scarcity of training data. To date, few methods exist for diffusion history reconstruction. They are exclusively based on the maximum likelihood estimation (MLE) formulation and require to know true diffusion parameters. In this paper, we study an even harder problem, namely reconstructing Diffusion history from A single SnapsHot} (DASH), where we seek to reconstruct the history from only the final snapshot without knowing true diffusion parameters. We start with theoretical analyses that reveal a fundamental limitation of the MLE formulation. We prove: (a) est
    
[^51]: 加拿大农田数据集：用于农业多时相深度学习分类的新地表覆盖数据集

    The Canadian Cropland Dataset: A New Land Cover Dataset for Multitemporal Deep Learning Classification in Agriculture. (arXiv:2306.00114v1 [cs.CV])

    [http://arxiv.org/abs/2306.00114](http://arxiv.org/abs/2306.00114)

    该论文提出了一个时间补丁数据集，包含了加拿大农田的多时相遥感影像。该数据集是手动经过确认和筛选的高分辨率地理参考图像，覆盖四个农作物生产年度和五个月份。这个数据集可以用于提高土地覆盖分类的准确性。

    

    利用遥感监测土地覆盖是研究环境变化和通过粮食产量预测确保全球粮食安全的关键。尤其是，多时相遥感影像提供了关于场景动态的相关信息，已经被证明可以带来更好的土地覆盖分类结果。然而，由于难以获取可靠、细粒度和高质量的注释样本支持他们的假设，很少有研究受益于高空间和时间分辨率数据。因此，我们介绍了一个加拿大农田的时间补丁数据集，其中包含了来自10个农作物类别的78,536个手动经过确认和筛选的高分辨率(10米/像素，640 x 640米)地理参考图像，覆盖了四个农作物生产年度(2017-2020)和五个月份(六月-十月)。每个实例都包含12个光谱波段、一张RGB图像和额外的植被指数计算。

    Monitoring land cover using remote sensing is vital for studying environmental changes and ensuring global food security through crop yield forecasting. Specifically, multitemporal remote sensing imagery provides relevant information about the dynamics of a scene, which has proven to lead to better land cover classification results. Nevertheless, few studies have benefited from high spatial and temporal resolution data due to the difficulty of accessing reliable, fine-grained and high-quality annotated samples to support their hypotheses. Therefore, we introduce a temporal patch-based dataset of Canadian croplands, enriched with labels retrieved from the Canadian Annual Crop Inventory. The dataset contains 78,536 manually verified and curated high-resolution (10 m/pixel, 640 x 640 m) geo-referenced images from 10 crop classes collected over four crop production years (2017-2020) and five months (June-October). Each instance contains 12 spectral bands, an RGB image, and additional veget
    
[^52]: 基于命中率的黑盒过程质量优化新方法

    A Novel Black Box Process Quality Optimization Approach based on Hit Rate. (arXiv:2305.20003v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.20003](http://arxiv.org/abs/2305.20003)

    该论文提出了一种基于数据驱动的准凸方法，将原始的非凸问题转化为一组凸可行问题，以实现最佳命中率。 在钢铁生产的实际实验中，该方法优于传统模型，使命中率提高至少41.11％和31.01％。

    

    命中率是综合工业过程中预测产品质量的关键绩效指标，它表示在质量控制范围内被下游处理过程接受的产品的百分比。然而，优化命中率是一个非凸且具有挑战性的问题。为了解决这个问题，我们提出了一个数据驱动的准凸方法，将阶乘隐式马尔可夫模型、多任务弹性网络和准凸优化结合起来。我们的方法将原始的非凸问题转化为一组凸可行问题，实现了最佳命中率。我们通过Monte Carlo模拟和在钢铁生产中进行的实际实验验证了凸优化特性和准凸前沿。结果表明，我们的方法优于传统模型，在两个实际数据集上使命中率提高了至少41.11%和31.01%。此外，准凸前沿为解决方案的恶化提供了参考解释和可视化。

    Hit rate is a key performance metric in predicting process product quality in integrated industrial processes. It represents the percentage of products accepted by downstream processes within a controlled range of quality. However, optimizing hit rate is a non-convex and challenging problem. To address this issue, we propose a data-driven quasi-convex approach that combines factorial hidden Markov models, multitask elastic net, and quasi-convex optimization. Our approach converts the original non-convex problem into a set of convex feasible problems, achieving an optimal hit rate. We verify the convex optimization property and quasi-convex frontier through Monte Carlo simulations and real-world experiments in steel production. Results demonstrate that our approach outperforms classical models, improving hit rates by at least 41.11% and 31.01% on two real datasets. Furthermore, the quasi-convex frontier provides a reference explanation and visualization for the deterioration of solution
    
[^53]: Vandermonde神经算子

    Vandermonde Neural Operators. (arXiv:2305.19663v1 [cs.LG])

    [http://arxiv.org/abs/2305.19663](http://arxiv.org/abs/2305.19663)

    本文提出了一种新的神经网络结构，Vandermonde神经算子，能够有效地处理非均匀分布点上的输入数据，同时在速度和准确性上相较于以前的方法有所提升。

    

    Fourier神经算子（FNO）已成为非常受欢迎的机器学习体系结构，用于学习操作符，特别是那些在PDE中出现的操作符。然而，由于FNO依赖于快速傅里叶变换以实现计算效率，所以该体系结构可能仅限于笛卡尔网格上的输入数据。在这里，我们将FNO推广到处理分布在非均匀点分布上的输入数据。我们提出的模型称为Vandermonde神经运算符（VNO），利用Vandermonde结构矩阵来高效地计算正向和反向的傅里叶变换，即使在任意分布的点上也可以如此。我们进行了数值实验，证明VNO可以比FNO快得多，同时保持可比的准确性，并改进了可比的非均匀方法（如Geo-FNO）的准确性。

    Fourier Neural Operators (FNOs) have emerged as very popular machine learning architectures for learning operators, particularly those arising in PDEs. However, as FNOs rely on the fast Fourier transform for computational efficiency, the architecture can be limited to input data on equispaced Cartesian grids. Here, we generalize FNOs to handle input data on non-equispaced point distributions. Our proposed model, termed as Vandermonde Neural Operator (VNO), utilizes Vandermonde-structured matrices to efficiently compute forward and inverse Fourier transforms, even on arbitrarily distributed points. We present numerical experiments to demonstrate that VNOs can be significantly faster than FNOs, while retaining comparable accuracy, and improve upon accuracy of comparable non-equispaced methods such as the Geo-FNO.
    
[^54]: 基于人工智能的交通预测：近期进展与新机遇综述

    Traffic Prediction using Artificial Intelligence: Review of Recent Advances and Emerging Opportunities. (arXiv:2305.19591v1 [cs.LG])

    [http://arxiv.org/abs/2305.19591](http://arxiv.org/abs/2305.19591)

    该论文综述了交通预测方法的发展，重点介绍了基于人工智能的交通预测方法在多元交通时间序列模型研究方面的进展和机遇。

    

    交通预测在缓解全球性的交通拥堵问题中起着关键作用，其负面影响包括额外旅行时间的损失和燃料消耗的增加。将新兴技术融入交通系统可以显著改善交通预测，并带来新的研究问题。为了了解交通预测中的开放性研究挑战，本综述旨在提供交通预测方法的综合概述。具体而言，我们侧重于基于人工智能（AI）的交通预测方法在多变量交通时间序列建模方面的近期进展和新的研究机遇，这是由于近年来这类方法在交通预测中具有潜在的成功和潜力。

    Traffic prediction plays a crucial role in alleviating traffic congestion which represents a critical problem globally, resulting in negative consequences such as lost hours of additional travel time and increased fuel consumption. Integrating emerging technologies into transportation systems provides opportunities for improving traffic prediction significantly and brings about new research problems. In order to lay the foundation for understanding the open research challenges in traffic prediction, this survey aims to provide a comprehensive overview of traffic prediction methodologies. Specifically, we focus on the recent advances and emerging research opportunities in Artificial Intelligence (AI)-based traffic prediction methods, due to their recent success and potential in traffic prediction, with an emphasis on multivariate traffic time series modeling. We first provide a list and explanation of the various data types and resources used in the literature. Next, the essential data 
    
[^55]: Spotlight Attention: 具备空间局部性先验的鲁棒目标中心学习

    Spotlight Attention: Robust Object-Centric Learning With a Spatial Locality Prior. (arXiv:2305.19550v1 [cs.CV])

    [http://arxiv.org/abs/2305.19550](http://arxiv.org/abs/2305.19550)

    该论文提出了一个新的目标中心学习方法，通过加入空间局部性先验来提高模型的鲁棒性，使模型在合成和真实数据上实现了显著的物体分割改进，并且对模型超参数不太敏感。

    

    目标中心视觉的目的是构建场景中物体的显式表示。这种表示是通过一组可互换的模块(称为slot或对象文件)获得的，它们竞争图像的局部补丁。该竞争具有弱感性偏差，以保持空间连续性;因此，一个slot可能会宣称在整个图像中散布的补丁。与此相反，人类视觉的感性偏差很强，到了注意力经典用聚光灯比喻的程度。我们将空间局部性先验融入现代目标中心视觉模型，从而在合成和真实数据集中获得显着的物体分割改进。类似于人类视觉注意力，图像内容和空间约束的组合产生了具有鲁棒性的无监督目标中心学习，包括对模型超参数不太敏感。

    The aim of object-centric vision is to construct an explicit representation of the objects in a scene. This representation is obtained via a set of interchangeable modules called \emph{slots} or \emph{object files} that compete for local patches of an image. The competition has a weak inductive bias to preserve spatial continuity; consequently, one slot may claim patches scattered diffusely throughout the image. In contrast, the inductive bias of human vision is strong, to the degree that attention has classically been described with a spotlight metaphor. We incorporate a spatial-locality prior into state-of-the-art object-centric vision models and obtain significant improvements in segmenting objects in both synthetic and real-world datasets. Similar to human visual attention, the combination of image content and spatial constraints yield robust unsupervised object-centric learning, including less sensitivity to model hyperparameters.
    
[^56]: 标准比评分更重要：面向多准则推荐的标准偏好感知轻量图卷积网络

    Criteria Tell You More than Ratings: Criteria Preference-Aware Light Graph Convolution for Effective Multi-Criteria Recommendation. (arXiv:2305.18885v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2305.18885](http://arxiv.org/abs/2305.18885)

    本文提出了一种面向多准则推荐的标准偏好感知轻量图卷积网络，该方法结合了MC扩展图，可以准确地捕捉用户的标准偏好，并进一步将用户对各个标准的偏好合并到最终的推荐列表中。

    

    多准则推荐系统现在在广泛的电子商务领域中利用多准则 (MC) 评分信息，而深度学习中的图神经网络 (GNN) 已经被广泛应用于各种推荐系统的开发中。在这种情况下，本文首次尝试使用GNN辅助设计MC推荐系统。具体而言，我们提出了一种新颖的标准偏好感知轻量图卷积方法(CPA-LGC),可以准确捕捉用户的标准偏好以及复杂高阶连接中的协作信号。本文在MC扩展图上构建了一个能够将用户-物品MC评分转换为扩展二分图的MC扩展图，再进一步将标准重要性编码到图卷积过程中，并引入了一种新的标准偏好感知聚合方法来将用户对不同标准的偏好合并到最终的推荐列表中。

    The multi-criteria (MC) recommender system, which leverages MC rating information in a wide range of e-commerce areas, is ubiquitous nowadays. Surprisingly, although graph neural networks (GNNs) have been widely applied to develop various recommender systems due to GNN's high expressive capability in learning graph representations, it has been still unexplored how to design MC recommender systems with GNNs. In light of this, we make the first attempt towards designing a GNN-aided MC recommender system. Specifically, rather than straightforwardly adopting existing GNN-based recommendation methods, we devise a novel criteria preference-aware light graph convolution CPA-LGC method, which is capable of precisely capturing the criteria preference of users as well as the collaborative signal in complex high-order connectivities. To this end, we first construct an MC expansion graph that transforms user--item MC ratings into an expanded bipartite graph to potentially learn from the collaborat
    
[^57]: 集中化协作的分散式排练：基于多智能体强化学习的住宅能量灵活性可扩展协调研究

    Centralised rehearsal of decentralised cooperation: Multi-agent reinforcement learning for the scalable coordination of residential energy flexibility. (arXiv:2305.18875v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2305.18875](http://arxiv.org/abs/2305.18875)

    本文研究了基于多智能体强化学习的住宅能量灵活性的可扩展且保护隐私的协调方法，使用“集中化但分解的评论家”在执行前进行协调排练，实现了大规模协调。

    

    本文研究了深度多智能体强化学习如何实现住宅能量灵活性的可扩展且保护隐私的协调。分布式资源的协调，如电动汽车和供暖，将对成功整合大规模可再生能源到我们的电力网中至关重要，从而有助于减缓气候变化。预先学习每个强化学习策略可以实现分布式控制，在执行过程中不需要共享个人数据。然而，先前的多智能体强化学习方法不断增加训练计算负担，随着系统规模的增加。因此，我们采用了深度多智能体演员-评论家方法，使用“集中化但分解的评论家”在执行前进行协调排练。结果显示，协调可以在大规模情况下实现最少的信息和通讯。

    This paper investigates how deep multi-agent reinforcement learning can enable the scalable and privacy-preserving coordination of residential energy flexibility. The coordination of distributed resources such as electric vehicles and heating will be critical to the successful integration of large shares of renewable energy in our electricity grid and, thus, to help mitigate climate change. The pre-learning of individual reinforcement learning policies can enable distributed control with no sharing of personal data required during execution. However, previous approaches for multi-agent reinforcement learning-based distributed energy resources coordination impose an ever greater training computational burden as the size of the system increases. We therefore adopt a deep multi-agent actor-critic method which uses a \emph{centralised but factored critic} to rehearse coordination ahead of execution. Results show that coordination is achieved at scale, with minimal information and communica
    
[^58]: 基准数据集上 ChatGPT 的系统研究和全面评估

    A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. (arXiv:2305.18486v1 [cs.CL])

    [http://arxiv.org/abs/2305.18486](http://arxiv.org/abs/2305.18486)

    本文对基准数据集上 ChatGPT 的性能进行了全面的评估，包括问答、文本摘要、代码生成、常识推理、数学问题求解、机器翻译、偏见检测和伦理考虑等任务。研究旨在验证 ChatGPT 的优势和弱点，并为使用语言模型的未来研究提供见解。

    

    最近，如 ChatGPT 这样的大型语言模型（LLM）的开发引起了很多关注。然而，由于难以将该模型生成的产出与基本事实进行比较，因此其在基准学术数据集上的评估仍未充分探索。本文旨在对 ChatGPT 在包括问答、文本摘要、代码生成、常识推理、数学问题求解、机器翻译、偏见检测和伦理考虑等任务中的表现进行彻底评估。具体而言，我们在 140 个任务中评估了 ChatGPT，并分析了其在这些数据集中生成的 255K 次响应，这使我们的工作成为了在 NLP 基准测试中对 ChatGPT 进行的最大评估。简而言之，我们的研究旨在验证 ChatGPT 在各种任务中的优势和弱点，并为使用 LLM 的未来研究提供见解。我们还报告了一种新的迸发能力，即遵循多个查询指令。

    The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT's performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi-query instruct
    
[^59]: 超越元数据：利用游戏设计参数进行跨版本电子竞技分析

    Beyond the Meta: Leveraging Game Design Parameters for Patch-Agnostic Esport Analytics. (arXiv:2305.18477v1 [cs.LG])

    [http://arxiv.org/abs/2305.18477](http://arxiv.org/abs/2305.18477)

    本论文提出了一种新的跨版本的电子竞技分析方法，通过利用游戏设计参数并利用聚类技术创建角色表征形式来解决传统方法短寿命的问题。以Dota 2为例验证了这种方法，取得了显著的性能提升。

    

    电子竞技游戏是全球游戏市场的重要组成部分，并且是增长最快的游戏细分领域。这导致了电子竞技分析的领域产生，其使用游戏提取的遥测数据来为玩家、教练、播音员和其他利益相关者提供信息。与传统的体育比赛相比，电子竞技游戏的机制和规则经常发生快速变化。由于游戏参数的频繁更改，电子竞技分析模型的使用寿命可能很短，这在文献中很大程度上被忽略了。本文提取游戏设计信息（即补丁说明），利用聚类技术提出了一种新的角色表征形式。以Dota 2游戏中击杀次数的预测为案例，利用这种创新的角色表征技术训练了一个神经网络模型。然后将此模型的性能与包括常规技术在内的两个不同基线进行了评估。这个模型不仅达到了显著的表现水平，还克服了电子竞技游戏中版本更迭的困境。

    Esport games comprise a sizeable fraction of the global games market, and is the fastest growing segment in games. This has given rise to the domain of esports analytics, which uses telemetry data from games to inform players, coaches, broadcasters and other stakeholders. Compared to traditional sports, esport titles change rapidly, in terms of mechanics as well as rules. Due to these frequent changes to the parameters of the game, esport analytics models can have a short life-spam, a problem which is largely ignored within the literature. This paper extracts information from game design (i.e. patch notes) and utilises clustering techniques to propose a new form of character representation. As a case study, a neural network model is trained to predict the number of kills in a Dota 2 match utilising this novel character representation technique. The performance of this model is then evaluated against two distinct baselines, including conventional techniques. Not only did the model signi
    
[^60]: 基于稀疏提示的元策略网络中的持续任务分配

    Continual Task Allocation in Meta-Policy Network via Sparse Prompting. (arXiv:2305.18444v1 [cs.LG])

    [http://arxiv.org/abs/2305.18444](http://arxiv.org/abs/2305.18444)

    本文提出的CoTASP可以通过学习过完备字典来生成稀疏掩码作为提示，从而从元策略网络中提取与每个任务相关的子网络，实现了快速适应新任务，同时保留了之前任务的共同知识。

    

    如何通过不断学习一系列任务来训练一个具有一般化能力的元策略，是当前强化学习面临的挑战。本文提出了一种名为“连续任务分配的稀疏提示（CoTASP）”的解决方案，通过学习过完备字典来生成稀疏掩码作为提示，从元策略网络中提取与每个任务相关的子网络。通过交替优化子网络和提示，CoTASP更新了元策略，通过训练特定于任务的策略来实现。然后更新字典，以使优化后的提示与任务嵌入相匹配，从而捕捉其语义相关性。因此，相关任务通过相似的提示在元策略网络中共享更多的神经元，而跨任务干扰导致遗忘被有效地约束。给定经过训练的元策略和更新后的字典，我们可以通过推导相应的提示来迅速适应新任务，从而从元策略中提取相关的子网络。我们在一组导航任务上评估了CoTASP，并展示了它在任务完成度、样本效率和泛化能力方面优于现有的基线方法。

    How to train a generalizable meta-policy by continually learning a sequence of tasks? It is a natural human skill yet challenging to achieve by current reinforcement learning: the agent is expected to quickly adapt to new tasks (plasticity) meanwhile retaining the common knowledge from previous tasks (stability). We address it by "Continual Task Allocation via Sparse Prompting (CoTASP)", which learns over-complete dictionaries to produce sparse masks as prompts extracting a sub-network for each task from a meta-policy network. By optimizing the sub-network and prompts alternatively, CoTASP updates the meta-policy via training a task-specific policy. The dictionary is then updated to align the optimized prompts with tasks' embedding, thereby capturing their semantic correlations. Hence, relevant tasks share more neurons in the meta-policy network via similar prompts while cross-task interference causing forgetting is effectively restrained. Given a trained meta-policy with updated dicti
    
[^61]: 神经雕塑：通过修剪和网络分析揭示分层模块化任务结构

    Neural Sculpting: Uncovering hierarchically modular task structure through pruning and network analysis. (arXiv:2305.18402v1 [cs.LG])

    [http://arxiv.org/abs/2305.18402](http://arxiv.org/abs/2305.18402)

    本文提出了一种名为“神经雕塑”的方法，该方法通过神经网络的修剪和分析生成的图形结构来揭示任务的子函数的层次结构。该方法在布尔任务上得到了有效验证。

    

    自然目标函数和任务通常表现为分层模块化，可以将其分解为更简单的子函数以分层组织。这些子函数具有两个重要特征：它们有一组不同的输入（输入可分离性），并且在更高层次中作为输入被重用（可重复使用性）。以往的研究已经确立了分层模块化神经网络的优点，包括学习效率、泛化、多任务学习和可转移性。但是，对于给定的任务，如何识别潜在的子函数及其分层结构仍然具有挑战性。本文提出了一种名为“神经雕塑”的方法，该方法涉及神经网络的修剪和分析生成的图形结构，以揭示子函数的层次结构。我们在几个基准布尔任务上证明了神经雕塑的有效性，并表明它可以准确地识别任务的潜在模块化结构。此外，我们证明，修剪后的网络具有更好的泛化能力，更容易被人类解释。我们的方法也可以扩展到现实任务中，为复杂问题的潜在模块化结构提供洞察。

    Natural target functions and tasks typically exhibit hierarchical modularity - they can be broken down into simpler sub-functions that are organized in a hierarchy. Such sub-functions have two important features: they have a distinct set of inputs (input-separability) and they are reused as inputs higher in the hierarchy (reusability). Previous studies have established that hierarchically modular neural networks, which are inherently sparse, offer benefits such as learning efficiency, generalization, multi-task learning, and transferability. However, identifying the underlying sub-functions and their hierarchical structure for a given task can be challenging. The high-level question in this work is: if we learn a task using a sufficiently deep neural network, how can we uncover the underlying hierarchy of sub-functions in that task? As a starting point, we examine the domain of Boolean functions, where it is easier to determine whether a task is hierarchically modular. We propose an ap
    
[^62]: 基于双层学习的最优正则化参数研究

    On Optimal Regularization Parameters via Bilevel Learning. (arXiv:2305.18394v1 [math.OC])

    [http://arxiv.org/abs/2305.18394](http://arxiv.org/abs/2305.18394)

    本文提出了一个利用双层学习确定正则化参数的方法，并提出了一个新的表征正确参数的条件。

    

    变分正则化常用于解线性反问题，它通过添加正则化项来提高先验信息质量，并通过正则化参数加以权衡，而合适的正则化参数的选择至关重要。现有的策略例如差异原则和L-曲线可以用于确定合适的参数值，但是近年来，一种叫做双层学习的监督机器学习方法被用于确定最优参数。虽然以前的策略有各种理论结果，但在这种情况下，双层学习的良好性质仍然是一个发展中的领域。本文提出了一个更好的条件来表征确定正则化参数的正值性。

    Variational regularization is commonly used to solve linear inverse problems, and involves augmenting a data fidelity by a regularizer. The regularizer is used to promote a priori information, and is weighted by a regularization parameter. Selection of an appropriate regularization parameter is critical, with various choices leading to very different reconstructions. Existing strategies such as the discrepancy principle and L-curve can be used to determine a suitable parameter value, but in recent years a supervised machine learning approach called bilevel learning has been employed. Bilevel learning is a powerful framework to determine optimal parameters, and involves solving a nested optimisation problem. While previous strategies enjoy various theoretical results, the well-posedness of bilevel learning in this setting is still a developing field. One necessary property is positivity of the determined regularization parameter. In this work, we provide a new condition that better char
    
[^63]: 基于预先训练语言模型的情境类比推理研究

    In-Context Analogical Reasoning with Pre-Trained Language Models. (arXiv:2305.17626v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.17626](http://arxiv.org/abs/2305.17626)

    本研究提出了一种基于语言模型的情境类比推理方法，通过将问题的感知特征编码成语言形式，能够实现高效的零-shot关系推理，超越传统方法和人类水平。

    

    类比推理是人类认知的基本能力之一，可以通过将新的情况与过去的经验关联来进行抽象推理。虽然它被认为对于AI系统的强大推理至关重要，但传统方法需要进行大量的训练和/或固化特定的领域知识才能应用于基准任务中。受到认知科学研究发现人类语言与类比制作之间的联系的启发，我们探索使用直观的基于语言的抽象来支持人工智能系统中的类比。具体而言，我们使用大型预先训练的语言模型（PLMs）对视觉Raven的渐进矩阵（RPM）进行类比推理。通过将问题的感知特征简单地编码成语言形式，我们发现PLMs表现出了惊人的零-shot关系推理能力，超过了人类表现并接近于受监督的基于视觉的方法。我们探索了不同的编码方法，以变化抽象的水平。

    Analogical reasoning is a fundamental capacity of human cognition that allows us to reason abstractly about novel situations by relating them to past experiences. While it is thought to be essential for robust reasoning in AI systems, conventional approaches require significant training and/or hard-coding of domain knowledge to be applied to benchmark tasks. Inspired by cognitive science research that has found connections between human language and analogy-making, we explore the use of intuitive language-based abstractions to support analogy in AI systems. Specifically, we apply large pre-trained language models (PLMs) to visual Raven's Progressive Matrices (RPM), a common relational reasoning test. By simply encoding the perceptual features of the problem into language form, we find that PLMs exhibit a striking capacity for zero-shot relational reasoning, exceeding human performance and nearing supervised vision-based methods. We explore different encodings that vary the level of abs
    
[^64]: 检测大型语言模型编辑失败：一个改进的特异性基准测试

    Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark. (arXiv:2305.17553v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17553](http://arxiv.org/abs/2305.17553)

    该论文探讨了大型语言模型编辑技术的现状，发现现有的特异性基准测试难以检测到不良副作用，并提出了一个改进的基准测试，该测试可以检测到动态组件，并通过基于KL散度的指标扩展了特异性的衡量方式。研究发现最近的模型编辑技术特异性较低，强调了改进特异性基准测试的重要性。

    

    最近的模型编辑技术承诺在LLM训练过程中减轻记忆错误或过时关联的问题。然而，我们展示了这些技术可能会引入大量未被现有特异性基准测试检测到的不良副作用。我们扩展了现有的CounterFact基准测试以包括动态组件，并将我们的基准测试称为CounterFact+。此外，我们通过一个基于KL散度的本质指标扩展了用于衡量特异性的指标。我们使用这个改进的基准测试来评估最近的模型编辑技术，发现它们的特异性较低。我们的发现凸显了需要改进特异性基准测试以识别和预防不良副作用的重要性。

    Recent model editing techniques promise to mitigate the problem of memorizing false or outdated associations during LLM training. However, we show that these techniques can introduce large unwanted side effects which are not detected by existing specificity benchmarks. We extend the existing CounterFact benchmark to include a dynamic component and dub our benchmark CounterFact+. Additionally, we extend the metrics used for measuring specificity by a principled KL divergence-based metric. We use this improved benchmark to evaluate recent model editing techniques and find that they suffer from low specificity. Our findings highlight the need for improved specificity benchmarks that identify and prevent unwanted side effects.
    
[^65]: 使用场景图记忆建模动态环境

    Modeling Dynamic Environments with Scene Graph Memory. (arXiv:2305.17537v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.17537](http://arxiv.org/abs/2305.17537)

    本论文提出了一种新的场景图记忆状态表示，结合节点边缘预测器（NEP）的神经网络架构，能够帮助具有行动能力的AI代理在部分可观察动态场景中高效搜索。

    

    在大型环境中，如居室等，寻找物品的具有行动能力的AI代理需要基于部分信息预测物品位置来做出有效决策。我们将其形式化为一种新类型的链路预测问题：部分可观察动态图上的链路预测。我们的图表达了一个场景，其中房间和物品是节点，在边缘中编码它们之间的关系；在每个时间步骤上，代理人仅知道更改图的部分。这种部分可观测性对于现有的链路预测方法构成了挑战，我们进行了解决。我们提出了一种新颖的状态表示 - 场景图记忆（SGM） - 其中包括代理人的累积观察集合，以及一种名为节点边缘预测器（NEP）的神经网络架构，该架构从SGM中提取信息以进行高效搜索。我们在动态房屋模拟器中评估了我们的方法，这是一个新的基准，它按照语义模式创建不同的动态图形。

    Embodied AI agents that search for objects in large environments such as households often need to make efficient decisions by predicting object locations based on partial information. We pose this as a new type of link prediction problem: link prediction on partially observable dynamic graphs. Our graph is a representation of a scene in which rooms and objects are nodes, and their relationships are encoded in the edges; only parts of the changing graph are known to the agent at each timestep. This partial observability poses a challenge to existing link prediction approaches, which we address. We propose a novel state representation -- Scene Graph Memory (SGM) -- with captures the agent's accumulated set of observations, as well as a neural net architecture called a Node Edge Predictor (NEP) that extracts information from the SGM to search efficiently. We evaluate our method in the Dynamic House Simulator, a new benchmark that creates diverse dynamic graphs following the semantic patte
    
[^66]: PFN是适用于实际贝叶斯优化的灵活模型。

    PFNs Are Flexible Models for Real-World Bayesian Optimization. (arXiv:2305.17535v1 [cs.LG])

    [http://arxiv.org/abs/2305.17535](http://arxiv.org/abs/2305.17535)

    本文使用灵活的PFN作为BO代理建模，该模型能够允许进一步信息纳入以进行非远视BO。在三种不同的问题上得到了很好的结果。

    

    本文使用先验数据拟合网络(PFNs)作为贝叶斯优化(BO)的灵活代理。PFN是一种神经过程，被训练用于近似后验预测分布(PPD)，适用于任何可有效采样的先验分布。我们描述了如何利用这种灵活性来进行BO的代理建模。我们使用PFN来模拟一个朴素高斯过程(GP)，一个先进的GP和一个贝叶斯神经网络(BNN)。此外，我们展示了如何将进一步的信息纳入先验，例如允许有关最优位置的提示(用户先验)，忽略不相关的维度，并通过学习获取函数来执行非远视BO。这些扩展的灵活性为使用PFN进行BO开辟了广阔的可能性。我们在人工高斯过程样本和三个不同的超参数优化测试平台上展示了PFN对BO的有用性：HPO-B、Bayesmark和PD1。

    In this paper, we use Prior-data Fitted Networks (PFNs) as a flexible surrogate for Bayesian Optimization (BO). PFNs are neural processes that are trained to approximate the posterior predictive distribution (PPD) for any prior distribution that can be efficiently sampled from. We describe how this flexibility can be exploited for surrogate modeling in BO. We use PFNs to mimic a naive Gaussian process (GP), an advanced GP, and a Bayesian Neural Network (BNN). In addition, we show how to incorporate further information into the prior, such as allowing hints about the position of optima (user priors), ignoring irrelevant dimensions, and performing non-myopic BO by learning the acquisition function. The flexibility underlying these extensions opens up vast possibilities for using PFNs for BO. We demonstrate the usefulness of PFNs for BO in a large-scale evaluation on artificial GP samples and three different hyperparameter optimization testbeds: HPO-B, Bayesmark, and PD1. We publish code 
    
[^67]: 从黑盒模型到可解释模型的转化，用于高效的迁移学习

    Distilling BlackBox to Interpretable models for Efficient Transfer Learning. (arXiv:2305.17303v1 [cs.CV])

    [http://arxiv.org/abs/2305.17303](http://arxiv.org/abs/2305.17303)

    本论文提出了一种方法可以将黑盒模型转化成为可解释模型并在目标领域成功进行迁移学习，从而实现高效迁移学习。

    

    建立具有普适性的AI模型是医疗领域面临的主要挑战之一。神经网络（NN）模型即使输入分布轻微移位（例如扫描仪类型），也会受到影响，而放射科医生则依赖于异常性的通用描述性规则。微调模型以将知识从一个领域转移到另一个领域需要大量标记数据。在本文中，我们开发了一种可解释的模型，它可以在计算成本最小的情况下，高效地针对未知的目标域进行微调。我们认为NN的可解释组件大致是域不变的。然而，可解释模型通常表现不及它们的BB变体。在源域中我们先使用人类理解的概念从BB开始，将其提炼成一组浅显易懂的interpretable模型。由于每个interpretable模型都覆盖了数据的一个子集，具有一组interpretable模型的混合可以实现与BB相当的性能。

    Building generalizable AI models is one of the primary challenges in the healthcare domain. While radiologists rely on generalizable descriptive rules of abnormality, Neural Network (NN) models suffer even with a slight shift in input distribution (\eg scanner type). Fine-tuning a model to transfer knowledge from one domain to another requires a significant amount of labeled data in the target domain. In this paper, we develop an interpretable model that can be efficiently fine-tuned to an unseen target domain with minimal computational cost. We assume the interpretable component of NN to be approximately domain-invariant. However, interpretable models typically underperform compared to their Blackbox (BB) variants. We start with a BB in the source domain and distill it into a \emph{mixture} of shallow interpretable models using human-understandable concepts. As each interpretable model covers a subset of data, a mixture of interpretable models achieves comparable performance as BB. Fu
    
[^68]: 选择性混合有助于应对分布偏移，但不仅仅是因为混合技术

    Selective Mixup Helps with Distribution Shifts, But Not (Only) because of Mixup. (arXiv:2305.16817v1 [cs.LG])

    [http://arxiv.org/abs/2305.16817](http://arxiv.org/abs/2305.16817)

    选择性mixup通过非随机选择对提高训练分布，实现标签偏移的经典解决方案，从而提高了神经网络的泛化性能。

    

    Mixup是一种提高神经网络泛化性能的高度成功的技术，它通过随机配对的组合来增强训练数据。选择性mixup是一系列将mixup应用于特定对的方法，例如仅在类别或领域之间组合示例。这些方法声称在具有分布偏移的基准测试中有显着的提高，但它们的机制和限制尚不清楚。本文研究了选择性mixup的一个被忽视的方面，从一个全新的角度解释了它的成功。我们发现，非随机选择对会影响训练分布，并通过与混合技术完全无关的方式提高泛化性能。例如，在二元分类中，类别之间的mixup隐含地对数据进行重采样，以实现标签偏移的经典解决方案。我们经验证实，这种隐含重采样解释了先前工作中的大部分改进。在理论上，这些结果依赖于一个回归问题，其中我们需要区分真正的重采样和混合技术。

    Mixup is a highly successful technique to improve generalization of neural networks by augmenting the training data with combinations of random pairs. Selective mixup is a family of methods that apply mixup to specific pairs, e.g. only combining examples across classes or domains. These methods have claimed remarkable improvements on benchmarks with distribution shifts, but their mechanisms and limitations remain poorly understood.  We examine an overlooked aspect of selective mixup that explains its success in a completely new light. We find that the non-random selection of pairs affects the training distribution and improve generalization by means completely unrelated to the mixing. For example in binary classification, mixup across classes implicitly resamples the data for a uniform class distribution a classical solution to label shift. We show empirically that this implicit resampling explains much of the improvements in prior work. Theoretically, these results rely on a regress
    
[^69]: 通过任意回归模型检测数值数据中的错误。

    Detecting Errors in Numerical Data via any Regression Model. (arXiv:2305.16583v1 [stat.ML])

    [http://arxiv.org/abs/2305.16583](http://arxiv.org/abs/2305.16583)

    该论文提出了一种模型不可知的方法，通过考虑各种不确定性，可以利用任何回归器检测数值数据中的异常值与自然数据波动，能够有效区分真正的异常和自然数据波动。

    

    噪声困扰着许多数值数据集，其中数据记录的值可能由于错误的传感器、数据输入/处理错误或不完美的人类估计等原因而无法匹配真实的底层值。我们考虑估计沿数值列哪些数据值是不正确的。我们提出了一种模型不可知的方法，可以利用任何回归器（即基于数据集中的其他变量来预测该列值的统计学或机器学习模型）来解决问题。通过考虑各种不确定性，我们的方法区分了真正的异常和自然数据波动，条件是有可用的数据集信息。我们为我们的方法建立了理论保证，并表明其他方法（如符合性推断）难以检测错误。我们还提供了一个新的误差检测基准，涉及 5 个具有真实世界数字错误的回归数据集（对于其中的真实值）。

    Noise plagues many numerical datasets, where the recorded values in the data may fail to match the true underlying values due to reasons including: erroneous sensors, data entry/processing mistakes, or imperfect human estimates. Here we consider estimating \emph{which} data values are incorrect along a numerical column. We present a model-agnostic approach that can utilize \emph{any} regressor (i.e.\ statistical or machine learning model) which was fit to predict values in this column based on the other variables in the dataset. By accounting for various uncertainties, our approach distinguishes between genuine anomalies and natural data fluctuations, conditioned on the available information in the dataset. We establish theoretical guarantees for our method and show that other approaches like conformal inference struggle to detect errors. We also contribute a new error detection benchmark involving 5 regression datasets with real-world numerical errors (for which the true values are al
    
[^70]: 扫描与拍照：理解1层Transformer中的训练动态和标记组成

    Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer. (arXiv:2305.16380v1 [cs.CL])

    [http://arxiv.org/abs/2305.16380](http://arxiv.org/abs/2305.16380)

    本文分析了1层Transformer在下一个标记预测任务中的SGD训练动态，证明了自我关注层充当了“区分性扫描算法”，从而逐步关注到相关标记并排除不相关的标记，总结相关信息在编码表示中。同时研究了标记频率、上下文和初始化自我关注层等对Transformer性能的影响。

    

    Transformer架构在多个研究领域表现出了惊人的性能，并成为许多神经网络模型的基础。然而，我们对其如何工作的理解仍然有限。特别是，通过简单的预测性损失，表示如何从梯度训练动态中出现仍然是一个谜。在本文中，针对具有一个自我关注层和一个解码器层的1层Transformer，我们以数学严谨的方式分析其在下一个标记预测任务中的SGD训练动态。我们打开了自我关注层组合输入标记的动态过程的黑盒子，并揭示了底层归纳偏差的本质。具体而言，在没有位置编码、长输入序列和解码器层学习速度快于自我关注层的假设下，我们证明了自我关注层充当了“区分性扫描算法”：从均匀注意力开始，它逐渐关注到相关标记，排除不相关的标记，直到所有相关信息被扫描并总结在编码表示中。我们的分析还显示了标记频率和上下文如何影响注意权重，以及自我关注层初始化如何影响收敛速度。

    Transformer architecture has shown impressive performance in multiple research domains and has become the backbone of many neural network models. However, there is limited understanding on how it works. In particular, with a simple predictive loss, how the representation emerges from the gradient \emph{training dynamics} remains a mystery. In this paper, for 1-layer transformer with one self-attention layer plus one decoder layer, we analyze its SGD training dynamics for the task of next token prediction in a mathematically rigorous manner. We open the black box of the dynamic process of how the self-attention layer combines input tokens, and reveal the nature of underlying inductive bias. More specifically, with the assumption (a) no positional encoding, (b) long input sequence, and (c) the decoder layer learns faster than the self-attention layer, we prove that self-attention acts as a \emph{discriminative scanning algorithm}: starting from uniform attention, it gradually attends mor
    
[^71]: 具有双多模态编码器的组合图像检索候选集重排序

    Candidate Set Re-ranking for Composed Image Retrieval with Dual Multi-modal Encoder. (arXiv:2305.16304v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.16304](http://arxiv.org/abs/2305.16304)

    本论文提出了一种使用两阶段模式结合预先计算图像嵌入和参考文本-候选项三元组交互选择的方式进行组合图像检索候选集重排序的方法。

    

    组合图像检索旨在找到最匹配给定多模态用户查询(包括参考图像和文本对)的图像。现有方法通常预先计算整个语料库的图像嵌入，并在测试时将这些嵌入与经过查询文本修改的参考图像嵌入进行比较。然而，仅通过短文本描述引导修改参考图像嵌入可能很困难，特别是独立于潜在的候选项。一种替代方法是允许查询和每个可能的候选项之间的交互，即参考文本-候选项三元组，并从整个集合中选择最佳匹配。虽然这种方法更具有判别性，但对于大规模数据集，由于不能预先计算候选嵌入，因此计算成本是禁止性的。我们提出使用两阶段模式结合这两个方案的优点

    Composed image retrieval aims to find an image that best matches a given multi-modal user query consisting of a reference image and text pair. Existing methods commonly pre-compute image embeddings over the entire corpus and compare these to a reference image embedding modified by the query text at test time. Such a pipeline is very efficient at test time since fast vector distances can be used to evaluate candidates, but modifying the reference image embedding guided only by a short textual description can be difficult, especially independent of potential candidates. An alternative approach is to allow interactions between the query and every possible candidate, i.e., reference-text-candidate triplets, and pick the best from the entire set. Though this approach is more discriminative, for large-scale datasets the computational cost is prohibitive since pre-computation of candidate embeddings is no longer possible. We propose to combine the merits of both schemes using a two-stage mode
    
[^72]: 在脉冲神经网络中将噪声作为计算和学习资源

    Exploiting Noise as a Resource for Computation and Learning in Spiking Neural Networks. (arXiv:2305.16044v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2305.16044](http://arxiv.org/abs/2305.16044)

    本文提出了噪声脉冲神经元网络（NSNN）和噪声驱动学习规则（NDL），展示了噪声可以作为计算和学习的资源，并为一般脉冲神经元网络提供了一个框架。研究还展示了NSNNs在图像分类和语音识别等实际任务中的适用性，表明它们是未来神经形态计算系统的潜在有力工具。

    

    脉冲神经元网络是大脑非凡信息处理能力的基础，并已成为神经形态智能的支柱模型。本文介绍了噪声脉冲神经元网络（NSNN）和噪声驱动学习规则（NDL），采用带有噪声神经元动力学的脉冲神经元模型。该方法显示噪声可以作为计算和学习的资源，并理论上为一般脉冲神经元网络提供了一个框架。此外，NDL为代理梯度提供了深入的生物学合理性。通过将各种SNN架构和算法结合起来，我们展示了我们的方法表现出竞争性能，并且比确定性SNNs表现出更好的鲁棒性。此外，本文还展示了NSNNs在图像分类和语音识别等实际任务中的适用性，表明它们是未来神经形态计算系统的潜在有力工具。

    Networks of spiking neurons underpin the extraordinary information-processing capabilities of the brain and have emerged as pillar models in neuromorphic intelligence. Despite extensive research on spiking neural networks (SNNs), most are established on deterministic models. Integrating noise into SNNs leads to biophysically more realistic neural dynamics and may benefit model performance. This work presents the noisy spiking neural network (NSNN) and the noise-driven learning rule (NDL) by introducing a spiking neuron model incorporating noisy neuronal dynamics. Our approach shows how noise may act as a resource for computation and learning and theoretically provides a framework for general SNNs. Moreover, NDL provides an insightful biological rationale for surrogate gradients. By incorporating various SNN architectures and algorithms, we show that our approach exhibits competitive performance and improved robustness against challenging perturbations than deterministic SNNs. Additiona
    
[^73]: 指数平滑用于离线策略学习

    Exponential Smoothing for Off-Policy Learning. (arXiv:2305.15877v1 [cs.LG])

    [http://arxiv.org/abs/2305.15877](http://arxiv.org/abs/2305.15877)

    本文研究了离线学习中最小化风险的倒数倾向评分(IPS)的平滑正则化，推导出了可处理、可扩展、可解释的学习证明，并确定了在何种情况下不需要正则化IPS。

    

    离线策略学习旨在通过最小化风险的倒数倾向评分（IPS）来寻找改进的策略，通常使用记录的赌博数据。在本文中，我们研究了IPS的平滑正则化，推导出了一个双向PAC-Bayes泛化界限。该界限是可处理的、可扩展的、可解释的并提供了学习证明。我们通过一系列学习任务展示了我们方法的相关性和有利的性能。由于我们的界限适用于标准IPS，因此我们能够提供关于何时正则化IPS有用的见解。即，我们确定了不需要正则化的情况。这与在实践中，剪辑IPS常常比OPL中的标准IPS表现更好的信念相反。

    Off-policy learning (OPL) aims at finding improved policies from logged bandit data, often by minimizing the inverse propensity scoring (IPS) estimator of the risk. In this work, we investigate a smooth regularization for IPS, for which we derive a two-sided PAC-Bayes generalization bound. The bound is tractable, scalable, interpretable and provides learning certificates. In particular, it is also valid for standard IPS without making the assumption that the importance weights are bounded. We demonstrate the relevance of our approach and its favorable performance through a set of learning tasks. Since our bound holds for standard IPS, we are able to provide insight into when regularizing IPS is useful. Namely, we identify cases where regularization might not be needed. This goes against the belief that, in practice, clipped IPS often enjoys favorable performance than standard IPS in OPL.
    
[^74]: 超越格点：基于点云和表面表示的神经学处理方法

    Transcending Grids: Point Clouds and Surface Representations Powering Neurological Processing. (arXiv:2305.15426v1 [cs.CV])

    [http://arxiv.org/abs/2305.15426](http://arxiv.org/abs/2305.15426)

    本文提出了一种基于无结构点云的新方法，将基于格点的数据转换为其更高维度表达，以提高医疗图像分类的性能。

    

    在医疗保健中，准确分类医学图像至关重要，但传统的方法通常依赖于具有一致网格结构的医学数据，这可能限制了它们的整体性能。近期医学研究的重点是调整体系结构以获得更好的性能，而没有充分考虑数据的表述。在本文中，我们提出了一种将基于格点的数据转换为其更高维度表示的新方法，利用了无结构点云数据结构。我们首先通过将像素颜色信息作为空间坐标来从图像生成稀疏点云。接下来，我们构建了一个由点组成的超表面，基于图像尺寸，超表面内的每个平滑部分都表示特定的像素位置。多边形面构造是通过邻接张量完成的。最后，通过密集采样构造的超表面，生成了一个密集的点云，重点是重新采样以实现分类的最佳性能。

    In healthcare, accurately classifying medical images is vital, but conventional methods often hinge on medical data with a consistent grid structure, which may restrict their overall performance. Recent medical research has been focused on tweaking the architectures to attain better performance without giving due consideration to the representation of data. In this paper, we present a novel approach for transforming grid based data into its higher dimensional representations, leveraging unstructured point cloud data structures. We first generate a sparse point cloud from an image by integrating pixel color information as spatial coordinates. Next, we construct a hypersurface composed of points based on the image dimensions, with each smooth section within this hypersurface symbolizing a specific pixel location. Polygonal face construction is achieved using an adjacency tensor. Finally, a dense point cloud is generated by densely sampling the constructed hypersurface, with a focus on re
    
[^75]: 理论指导的联合学习实现了隐私保护和数据效用的平衡

    Theoretically Principled Federated Learning for Balancing Privacy and Utility. (arXiv:2305.15148v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15148](http://arxiv.org/abs/2305.15148)

    本文提出基于扭曲模型参数的保护机制的通用学习框架，用于实现联合学习中隐私保护和数据效用的平衡。算法可以在每个通信轮中实现个性化的效用-隐私折衷，我们在理论上证明了算法的次线性性质，该算法可以提高隐私保护的联合学习效力。

    

    我们提出了一种保护机制的通用学习框架，通过扭曲模型参数来保护隐私，实现隐私和效用之间的平衡。该算法适用于任意将扭曲映射到实值的隐私测量。在联合学习中，它可以为每个模型参数，每个客户端，在每个通信轮中实现个性化的效用-隐私折衷。这种自适应和细粒度的保护可以提高保护隐私的联合学习的效力。从理论上讲，我们证明了算法保护超参数的效用损失与最优保护超参数的效用损失之间的差距是总迭代次数的次线性。我们的算法次线性的特点表明，当迭代次数趋近于无穷大时，算法性能和最优性能之间的平均差距趋近于零。此外，我们提供了收敛性证明。

    We propose a general learning framework for the protection mechanisms that protects privacy via distorting model parameters, which facilitates the trade-off between privacy and utility. The algorithm is applicable to arbitrary privacy measurements that maps from the distortion to a real value. It can achieve personalized utility-privacy trade-off for each model parameter, on each client, at each communication round in federated learning. Such adaptive and fine-grained protection can improve the effectiveness of privacy-preserved federated learning.  Theoretically, we show that gap between the utility loss of the protection hyperparameter output by our algorithm and that of the optimal protection hyperparameter is sub-linear in the total number of iterations. The sublinearity of our algorithm indicates that the average gap between the performance of our algorithm and that of the optimal performance goes to zero when the number of iterations goes to infinity. Further, we provide the conv
    
[^76]: 图谱遇见LLM：一种用于稳健对话理解的协同过滤新方法

    Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding. (arXiv:2305.14449v1 [cs.AI])

    [http://arxiv.org/abs/2305.14449](http://arxiv.org/abs/2305.14449)

    一种协同过滤新方法用于稳健对话理解，在历史用户-实体交互的基础上，利用多跳客户亲和力丰富每个用户的索引，并使用有限内存BFGS算法调整每个索引的权重，实验结果显示其明显优于最先进的个性化查询重写方法。

    

    会话式人工智能系统（例如Alexa，Siri，Google Assistant等）需要理解存在缺陷的查询以确保稳健的会话理解并减少用户摩擦。这些有缺陷的查询通常是由用户的歧义和错误，自动语音识别（ASR）和自然语言理解（NLU）中的错误引起的。个性化查询重写（个性化QR）旨在减少身体和尾部用户查询流量中的缺陷，通常依赖于与对话式人工智能的过去成功的用户交互的索引。本文提出我们的“协同查询重写”方法，专注于重写用户历史中没有出现过的新型用户交互。该方法构建了一个“用户反馈交互图”（FIG），由历史用户-实体交互组成，并利用多跳客户亲和力来丰富每个用户的索引（即协同用户索引），从而帮助覆盖未来未曾见过的存在缺陷的查询。为了防止这些新的丰富索引被噪声反馈交互所支配，我们采用了有限内存BFGS（LLM）算法和回退方案来调整每个索引的权重。实验结果表明，我们的方法明显优于最先进的个性化QR方法，并在未看到的用户交互上取得了近乎完美的性能。

    Conversational AI systems (e.g. Alexa, Siri, Google Assistant, etc.) need to understand queries with defects to ensure robust conversational understanding and reduce user frictions. The defective queries are often induced by user ambiguities and mistakes, or errors in the automatic speech recognition (ASR) and natural language understanding (NLU).  Personalized query rewriting (personalized QR) targets reducing defects in the torso and tail user query traffic, and it typically relies on an index of past successful user interactions with the conversational AI. This paper presents our "Collaborative Query Rewriting" approach that focuses on rewriting novel user interactions unseen in the user history. This approach builds a "user Feedback Interaction Graph" (FIG) consisting of historical user-entity interactions, and leverages multi-hop customer affinity to enrich each user's index (i.e. the Collaborative User Index) that would help cover future unseen defective queries. To counteract th
    
[^77]: 针对深度学习的随机一阶优化方法的分层自适应步长策略

    Layer-wise Adaptive Step-Sizes for Stochastic First-Order Methods for Deep Learning. (arXiv:2305.13664v1 [cs.LG])

    [http://arxiv.org/abs/2305.13664](http://arxiv.org/abs/2305.13664)

    本文提出了一种针对深度学习的随机一阶优化方法的分层自适应步长策略，通过利用深度神经网络中浅层的随机曲率信息为每一层计算自适应步长，消除了用户调整学习率的需求。实验结果显示，结合该策略的算法在DNN任务的训练中优于精细调整学习率版本以及流行的一阶和二阶算法。

    

    我们提出了一种新的分层自适应步长策略，用于随机一阶优化方法来最小化深度学习中的经验损失函数，消除了用户调整学习率的需求。该方法利用深度神经网络（DNNs） 浅层中包含的对角线块的层随机曲率信息来计算每一层的自适应步长（即学习率）。该方法的内存需求与一阶方法相当，而其每次迭代的时间复杂度仅增加了约等于另一个梯度计算量的量。数值实验表明，结合所提出的分层步幅大小的SGD动量法和AdamW能够选择有效的学习率进度，并在Autoencoder、卷积神经网络（CNN）和循环神经网络（RNN）任务的DNN训练中优于这些方法的精细调整学习率版本以及流行的一阶和二阶算法。

    We propose a new per-layer adaptive step-size procedure for stochastic first-order optimization methods for minimizing empirical loss functions in deep learning, eliminating the need for the user to tune the learning rate (LR). The proposed approach exploits the layer-wise stochastic curvature information contained in the diagonal blocks of the Hessian in deep neural networks (DNNs) to compute adaptive step-sizes (i.e., LRs) for each layer. The method has memory requirements that are comparable to those of first-order methods, while its per-iteration time complexity is only increased by an amount that is roughly equivalent to an additional gradient computation. Numerical experiments show that SGD with momentum and AdamW combined with the proposed per-layer step-sizes are able to choose effective LR schedules and outperform fine-tuned LR versions of these methods as well as popular first-order and second-order algorithms for training DNNs on Autoencoder, Convolutional Neural Network (CN
    
[^78]: INVICTUS: 通过协同学习和搜索优化布尔逻辑电路综合

    INVICTUS: Optimizing Boolean Logic Circuit Synthesis via Synergistic Learning and Search. (arXiv:2305.13164v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.13164](http://arxiv.org/abs/2305.13164)

    INVICTUS是一个使用离线强化学习和搜索的模型，自动生成逻辑最小化启发式算法综合配方以优化电路面积和时延等指标。

    

    逻辑综合是芯片设计中的第一步也是最重要的一步。本文提出了一个模型INVICTUS，利用离线强化学习的方法，根据已有的设计数据集自动生成一系列逻辑最小化启发式算法（“综合配方”），优化电路面积和时延等指标。

    Logic synthesis is the first and most vital step in chip design. This steps converts a chip specification written in a hardware description language (such as Verilog) into an optimized implementation using Boolean logic gates. State-of-the-art logic synthesis algorithms have a large number of logic minimization heuristics, typically applied sequentially based on human experience and intuition. The choice of the order greatly impacts the quality (e.g., area and delay) of the synthesized circuit. In this paper, we propose INVICTUS, a model-based offline reinforcement learning (RL) solution that automatically generates a sequence of logic minimization heuristics ("synthesis recipe") based on a training dataset of previously seen designs. A key challenge is that new designs can range from being very similar to past designs (e.g., adders and multipliers) to being completely novel (e.g., new processor instructions). %Compared to prior work, INVICTUS is the first solution that uses a mix of R
    
[^79]: 多还是少样本？在加密流量分类中比较迁移学习、对比学习和元学习。

    Many or Few Samples? Comparing Transfer, Contrastive and Meta-Learning in Encrypted Traffic Classification. (arXiv:2305.12432v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.12432](http://arxiv.org/abs/2305.12432)

    该论文比较了迁移学习、元学习和对比学习等方法在加密流量分类中的效果，发现对比学习是最优的方法，并证明通过重复利用学习到的表示，DL方法可以接近基于树的模式。

    

    深度学习（DL）的流行，加上由于HTTPS、QUIC和DNS-SEC的广泛采用导致网络流量可见性降低，重新引起了对流量分类（TC）的关注。然而，为了控制依赖于特定任务的大型标记数据集的情况，我们需要寻找更好的方法来学习跨任务有效的表示。在这项工作中，我们将迁移学习、元学习和对比学习与参考机器学习（ML）基于树和单片式DL模型（总共16种方法）进行了比较。使用两个公开可用的数据集，即MIRAGE19（40类）和AppClassNet（500类），我们表明（i）使用大型数据集，我们可以得到更通用的表示，（ii）对比学习是最好的方法，（iii）元学习是最差的方法，（iv）虽然ML基于树的方法不能处理大型任务，但通过重复使用学习到的表示，DL方法正在接近基于树的模式。

    The popularity of Deep Learning (DL), coupled with network traffic visibility reduction due to the increased adoption of HTTPS, QUIC and DNS-SEC, re-ignited interest towards Traffic Classification (TC). However, to tame the dependency from task-specific large labeled datasets we need to find better ways to learn representations that are valid across tasks. In this work we investigate this problem comparing transfer learning, meta-learning and contrastive learning against reference Machine Learning (ML) tree-based and monolithic DL models (16 methods total). Using two publicly available datasets, namely MIRAGE19 (40 classes) and AppClassNet (500 classes), we show that (i) using large datasets we can obtain more general representations, (ii) contrastive learning is the best methodology and (iii) meta-learning the worst one, and (iv) while ML tree-based cannot handle large tasks but fits well small tasks, by means of reusing learned representations, DL methods are reaching tree-based mode
    
[^80]: 学习联合2D和3D扩散模型进行完整分子生成

    Learning Joint 2D & 3D Diffusion Models for Complete Molecule Generation. (arXiv:2305.12347v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2305.12347](http://arxiv.org/abs/2305.12347)

    本文提出了一个新的联合2D和3D扩散模型，能够生成具有原子类型、化学键信息和3D坐标的完整分子，并能捕捉2D键合图和3D分子几何的联合分布。

    

    设计新分子对于药物研发和材料科学至关重要。最近，旨在模拟分子分布的深度生成模型在缩小化学研究领域和生成高保真度分子方面取得了有希望的进展。然而，当前的生成模型只关注于模拟2D键合图或3D几何，这是分子的两个互补描述符。联合建模能力的不足限制了生成质量的提高和进一步的下游应用。在这篇论文中，我们提出了一种新的联合2D和3D扩散模型（JODO），它能够生成具有原子类型、化学键信息和3D坐标的完整分子。为了在扩散过程中捕获分子图和几何之间的相关性，我们开发了一个扩散图变换器来参数化数据预测模型，以从噪声数据中恢复原始数据。扩散图变换器在扩散过程中相互作用节点和边缘特征，使得JODO能够捕捉2D键合图和3D分子几何的联合分布。

    Designing new molecules is essential for drug discovery and material science. Recently, deep generative models that aim to model molecule distribution have made promising progress in narrowing down the chemical research space and generating high-fidelity molecules. However, current generative models only focus on modeling either 2D bonding graphs or 3D geometries, which are two complementary descriptors for molecules. The lack of ability to jointly model both limits the improvement of generation quality and further downstream applications. In this paper, we propose a new joint 2D and 3D diffusion model (JODO) that generates complete molecules with atom types, formal charges, bond information, and 3D coordinates. To capture the correlation between molecular graphs and geometries in the diffusion process, we develop a Diffusion Graph Transformer to parameterize the data prediction model that recovers the original data from noisy data. The Diffusion Graph Transformer interacts node and ed
    
[^81]: 学习为稀疏神经网络激活函数设置

    Learning Activation Functions for Sparse Neural Networks. (arXiv:2305.10964v1 [cs.LG])

    [http://arxiv.org/abs/2305.10964](http://arxiv.org/abs/2305.10964)

    本论文针对稀疏神经网络的准确性下降问题，发现激活函数和超参数是导致问题的主要原因，提出学习为稀疏网络调整激活函数并分开超参数优化方案的解决方法。

    

    稀疏神经网络（SNN）在推断时可以节省大量能量和内存，同时可以表现出类似于密集神经网络的性能。 然而，在高修剪比率下SNN的准确度降低可能在关键部署条件下成为问题。 在最近的研究中，通过复杂的修剪技术来缓解这个问题，但我们关注被忽略的因素：超参数和激活函数。 我们的分析表明，准确度下降可以额外归因于（i）普遍使用ReLU作为激活函数的默认选择，以及（ii）使用与密集网络相同的超参数来微调SNN。 因此，我们专注于学习为稀疏网络调整激活函数，并将其与稀疏网络的分开超参数优化方案相结合。 通过对在MNIST上训练的流行DNN模型（LeNet-5，VGG-16，ResNet-18和EfficientNet-B0）进行实验

    Sparse Neural Networks (SNNs) can potentially demonstrate similar performance to their dense counterparts while saving significant energy and memory at inference. However, the accuracy drop incurred by SNNs, especially at high pruning ratios, can be an issue in critical deployment conditions. While recent works mitigate this issue through sophisticated pruning techniques, we shift our focus to an overlooked factor: hyperparameters and activation functions. Our analyses have shown that the accuracy drop can additionally be attributed to (i) Using ReLU as the default choice for activation functions unanimously, and (ii) Fine-tuning SNNs with the same hyperparameters as dense counterparts. Thus, we focus on learning a novel way to tune activation functions for sparse networks and combining these with a separate hyperparameter optimization (HPO) regime for sparse networks. By conducting experiments on popular DNN models (LeNet-5, VGG-16, ResNet-18, and EfficientNet-B0) trained on MNIST, CI
    
[^82]: ProgSG：用于电子设计自动化程序的跨模态表征学习

    ProgSG: Cross-Modality Representation Learning for Programs in Electronic Design Automation. (arXiv:2305.10838v1 [cs.LG])

    [http://arxiv.org/abs/2305.10838](http://arxiv.org/abs/2305.10838)

    该论文介绍了一种跨模态表征学习方法，用于电子自动化设计中高级综合工具的优化，并促进领域特定加速器（DSAs）的设计自动化化。

    

    近年来，领域特定加速器（DSAs）（例如Google的TPUs）在加速各种应用程序（例如深度学习、搜索、自动驾驶等）方面越来越受欢迎。为了促进DSA设计，使用高级综合（HLS），它允许开发人员将C和C ++软件代码中的高级描述编译为低级硬件描述语言（例如VHDL或Verilog）上的设计，并最终合成为ASIC或FPGA上的DSA。然而，现有的HLS工具仍需要微架构决策，以pragma（例如并行化和流水线指令）的形式表示。为了使更多人设计DSA，希望能够通过深度学习自动化做出这些决策以预测HLS设计的质量。这需要我们更深入地了解程序，即原始代码的组合。

    Recent years have witnessed the growing popularity of domain-specific accelerators (DSAs), such as Google's TPUs, for accelerating various applications such as deep learning, search, autonomous driving, etc. To facilitate DSA designs, high-level synthesis (HLS) is used, which allows a developer to compile a high-level description in the form of software code in C and C++ into a design in low-level hardware description languages (such as VHDL or Verilog) and eventually synthesized into a DSA on an ASIC (application-specific integrated circuit) or FPGA (field-programmable gate arrays). However, existing HLS tools still require microarchitecture decisions, expressed in terms of pragmas (such as directives for parallelization and pipelining). To enable more people to design DSAs, it is desirable to automate such decisions with the help of deep learning for predicting the quality of HLS designs. This requires us a deeper understanding of the program, which is a combination of original code 
    
[^83]: 从图神经网络中提取低/高频知识注入MLP：一种有效的GNN-to-MLP蒸馏框架

    Extracting Low-/High- Frequency Knowledge from Graph Neural Networks and Injecting it into MLPs: An Effective GNN-to-MLP Distillation Framework. (arXiv:2305.10758v1 [cs.LG])

    [http://arxiv.org/abs/2305.10758](http://arxiv.org/abs/2305.10758)

    本文提出了一种有效的GNN-to-MLP蒸馏框架，将GNNs中的低/高频知识注入MLP。通过将GNNs学习到的知识分解为低/高频成分，在空间域中推导它们的对应关系。此外，还解决了现有GNN-to-MLP蒸馏中的信息淹没问题。

    

    最近几年，图神经网络（GNNs）在处理与图相关的任务方面取得了巨大成功。但是，由于可实现的推断效率和可扩展性，MLPs仍然是实际工业应用的主力军。为了缩小它们之间的差距，可以直接从精心设计的教师GNN中提取知识到学生MLP中，这被称为GNN-to-MLP蒸馏。但是，蒸馏的过程通常会导致信息损失，“哪些GNN的知识模式更可能会被保留并蒸馏到MLP中？”成为一个重要问题。在本文中，我们首先在频谱域中将GNNs学习到的知识分解为低/高频成分，然后推导它们在空间域中的对应关系。此外，我们还确定了现有GNN-to-MLP蒸馏存在潜在信息淹没问题，即预训练的GNNs的高频知识可能被低频知识所覆盖。

    Recent years have witnessed the great success of Graph Neural Networks (GNNs) in handling graph-related tasks. However, MLPs remain the primary workhorse for practical industrial applications due to their desirable inference efficiency and scalability. To reduce their gaps, one can directly distill knowledge from a well-designed teacher GNN to a student MLP, which is termed as GNN-to-MLP distillation. However, the process of distillation usually entails a loss of information, and ``which knowledge patterns of GNNs are more likely to be left and distilled into MLPs?" becomes an important question. In this paper, we first factorize the knowledge learned by GNNs into low- and high-frequency components in the spectral domain and then derive their correspondence in the spatial domain. Furthermore, we identified a potential information drowning problem for existing GNN-to-MLP distillation, i.e., the high-frequency knowledge of the pre-trained GNNs may be overwhelmed by the low-frequency know
    
[^84]: 增量因果图学习进行在线无监督根本原因分析

    Incremental Causal Graph Learning for Online Unsupervised Root Cause Analysis. (arXiv:2305.10638v1 [cs.LG])

    [http://arxiv.org/abs/2305.10638](http://arxiv.org/abs/2305.10638)

    本文提出了CORAL，一种用于在线无监督根本原因分析的新框架，可以自动触发该过程并增量更新模型，包括三个主要部分：触发点检测，增量因果图学习和基于网络传播的根本原因定位。

    

    根本原因分析（RCA）的任务是分析系统监控数据，以识别系统故障/失效的根本原因。有效的RCA可以大大加速系统故障恢复，并减轻系统损失或财务损失。然而，以前的研究大多集中在开发离线RCA算法上，这通常需要手动启动RCA过程，需要大量时间和数据来训练稳健的模型，然后需要从头开始重新训练新的系统故障。在本文中，我们提出了CORAL，一种新颖的在线RCA框架，可以自动触发RCA过程并增量更新RCA模型。CORAL包括触发点检测、增量解缠因果图学习和基于网络传播的根本原因定位。触发点检测组件旨在自动检测系统状态转换并进行准实时检测。为此，我们开发了一种基于m的在线触发点检测方法。

    The task of root cause analysis (RCA) is to identify the root causes of system faults/failures by analyzing system monitoring data. Efficient RCA can greatly accelerate system failure recovery and mitigate system damages or financial losses. However, previous research has mostly focused on developing offline RCA algorithms, which often require manually initiating the RCA process, a significant amount of time and data to train a robust model, and then being retrained from scratch for a new system fault.  In this paper, we propose CORAL, a novel online RCA framework that can automatically trigger the RCA process and incrementally update the RCA model. CORAL consists of Trigger Point Detection, Incremental Disentangled Causal Graph Learning, and Network Propagation-based Root Cause Localization. The Trigger Point Detection component aims to detect system state transitions automatically and in near-real-time. To achieve this, we develop an online trigger point detection approach based on m
    
[^85]: Lingo3DMol:利用语言模型生成基于口袋的三维分子

    Lingo3DMol: Generation of a Pocket-based 3D Molecule using a Language Model. (arXiv:2305.10133v1 [cs.LG])

    [http://arxiv.org/abs/2305.10133](http://arxiv.org/abs/2305.10133)

    本文提出了一种基于口袋的三维分子生成方法，利用扰动和恢复预训练任务和一种新的分子表示形式。 该方法结合了语言模型和几何深度学习的优点，使得语言模型可以生成准确的三维分子。

    

    近年来，深度生成模型推动的基于结构的药物设计备受瞩目。 语言模型已经展示了在二维结构中生成有效分子的强大能力，而基于几何深度学习的方法则可以直接产生具有准确三维坐标的分子。受这两种方法的启发，本文提出了一种基于口袋的三维分子生成方法，该方法利用语言模型具有生成三维坐标的能力。 由于高质量的蛋白质-配体复合物数据不足，因此设计了一种扰动和恢复预训练任务，可以利用大量的小分子数据。 还提出了一种新的分子表示形式，即带有局部和全局坐标的基于片段的SMILES，使语言模型能够有效地学习分子拓扑结构和空间位置信息。最终，CrossDocked和DUD-E数据集被用于评估和衡量。

    Structure-based drug design powered by deep generative models have attracted increasing research interest in recent years. Language models have demonstrated a robust capacity for generating valid molecules in 2D structures, while methods based on geometric deep learning can directly produce molecules with accurate 3D coordinates. Inspired by both methods, this article proposes a pocket-based 3D molecule generation method that leverages the language model with the ability to generate 3D coordinates. High quality protein-ligand complex data are insufficient; hence, a perturbation and restoration pre-training task is designed that can utilize vast amounts of small-molecule data. A new molecular representation, a fragment-based SMILES with local and global coordinates, is also presented, enabling the language model to learn molecular topological structures and spatial position information effectively. Ultimately, CrossDocked and DUD-E dataset is employed for evaluation and additional metri
    
[^86]: 图中长尾类别的特征化

    Characterizing Long-Tail Categories on Graphs. (arXiv:2305.09938v1 [cs.LG])

    [http://arxiv.org/abs/2305.09938](http://arxiv.org/abs/2305.09938)

    该研究提出了图上长尾分类的第一个泛化边界，并提出了一种可表征长尾类别的行为并提高机器学习模型在现实世界网络中的泛化性能的新图表示学习框架。

    

    长尾数据分布在许多现实世界的网络中普遍存在，包括金融交易网络、电子商务网络和合作网络。尽管最近取得了成功，但现有的作品主要集中于通过图增强或目标重新加权消除机器学习模型的偏见。然而，目前有限的文献提供理论工具来表征图上长尾类别的行为，并理解实际情况下的泛化性能。为填补这一空白，我们通过将问题形式化为多任务学习的方式，即每个任务对应于预测一个特定的类别，提出了图上长尾分类的第一个泛化边界。我们的理论结果表明，长尾分类的泛化性能受所有任务中的损失范围和任务总数的支配。在理论发现的基础上，我们提出了一种新的图表示学习框架，可表征长尾类别的行为，并提高机器学习模型在现实世界网络中的泛化性能。

    Long-tail data distributions are prevalent in many real-world networks, including financial transaction networks, e-commerce networks, and collaboration networks. Despite the success of recent developments, the existing works mainly focus on debiasing the machine learning models via graph augmentation or objective reweighting. However, there is limited literature that provides a theoretical tool to characterize the behaviors of long-tail categories on graphs and understand the generalization performance in real scenarios. To bridge this gap, we propose the first generalization bound for long-tail classification on graphs by formulating the problem in the fashion of multi-task learning, i.e., each task corresponds to the prediction of one particular category. Our theoretical results show that the generalization performance of long-tail classification is dominated by the range of losses across all tasks and the total number of tasks. Building upon the theoretical findings, we propose a n
    
[^87]: 基于连词效应建模的大动作空间离线策略评估

    Off-Policy Evaluation for Large Action Spaces via Conjunct Effect Modeling. (arXiv:2305.08062v1 [stat.ML])

    [http://arxiv.org/abs/2305.08062](http://arxiv.org/abs/2305.08062)

    本研究提出了一个称为OffCEM的估计器，用于对大离散动作空间下上下文匹配策略进行离线策略评估。该估计器通过基于模型的奖励估计来处理残余因果效应，并在新的本地正确性条件下保持无偏性。结果表明，OffCEM在合成和实际大动作空间数据集上优于现有方法。

    

    本文讨论了对于传统重要性加权方法方巨的大离散动作空间下的上下文匹配策略的离线策略评估（OPE）问题。为了解决方巨问题，我们提出了一个新的估计器OffCEM，该方法基于连词效应模型（CEM），这是一种新的因果效应分解方法，可以将效应分为群集效应和残差效应。OffCEM仅对行动群集应用重要性加权，通过基于模型的奖励估计来处理残余因果效应。我们表明，在新的本地正确性条件下，该估计器是无偏的，该条件仅要求残差效应模型保留每个群集中行动的相对期望奖励差异。为了充分利用CEM和本地正确性，我们还提出了一种新的两步过程，用于执行基于模型的估计，第一步最小化偏差，第二步最小化方差。我们发现，所得到的OPE估计器OffCEM在合成和实际大动作空间数据集上都明显优于现有的最先进方法。

    We study off-policy evaluation (OPE) of contextual bandit policies for large discrete action spaces where conventional importance-weighting approaches suffer from excessive variance. To circumvent this variance issue, we propose a new estimator, called OffCEM, that is based on the conjunct effect model (CEM), a novel decomposition of the causal effect into a cluster effect and a residual effect. OffCEM applies importance weighting only to action clusters and addresses the residual causal effect through model-based reward estimation. We show that the proposed estimator is unbiased under a new condition, called local correctness, which only requires that the residual-effect model preserves the relative expected reward differences of the actions within each cluster. To best leverage the CEM and local correctness, we also propose a new two-step procedure for performing model-based estimation that minimizes bias in the first step and variance in the second step. We find that the resulting O
    
[^88]: 应用于概率时间序列填充的Schr\"odinger bridge问题的收敛性分析和算法

    Provably Convergent Schr\"odinger Bridge with Applications to Probabilistic Time Series Imputation. (arXiv:2305.07247v1 [cs.LG])

    [http://arxiv.org/abs/2305.07247](http://arxiv.org/abs/2305.07247)

    本论文提出了一种基于近似投影的Schr\"odinger bridge算法，它能够应用于概率时间序列填充，并在医疗保健和环境数据方面实现最先进的结果。

    

    Schr\"odinger bridge问题（SBP）在生成建模中引起了越来越多的关注。然而，近似的投影是唯一可用的，其收敛性还不是十分清楚。我们提出了一种基于近似投影的Schr\"odinger bridge算法的第一个收敛分析。我们将SBP应用于概率时间序列填充，展示了优化传输成本可以提高性能，提出的算法在医疗保健和环境数据方面实现了最先进的结果。

    The Schr\"odinger bridge problem (SBP) is gaining increasing attention in generative modeling and showing promising potential even in comparison with the score-based generative models (SGMs). SBP can be interpreted as an entropy-regularized optimal transport problem, which conducts projections onto every other marginal alternatingly. However, in practice, only approximated projections are accessible and their convergence is not well understood. To fill this gap, we present a first convergence analysis of the Schr\"odinger bridge algorithm based on approximated projections. As for its practical applications, we apply SBP to probabilistic time series imputation by generating missing values conditioned on observed data. We show that optimizing the transport cost improves the performance and the proposed algorithm achieves the state-of-the-art result in healthcare and environmental data while exhibiting the advantage of exploring both temporal and feature patterns in probabilistic time ser
    
[^89]: ACTC: 冷启动知识图谱补全的主动阈值校准

    ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph Completion. (arXiv:2305.06395v1 [cs.LG])

    [http://arxiv.org/abs/2305.06395](http://arxiv.org/abs/2305.06395)

    本文提出了一种名为ACTC的方法，在冷启动知识图谱补全时进行主动阈值校准，可以有效地利用有限的标记元组来找到每个关系的最佳阈值，同时也结合未标记元组进行了实验。

    

    自监督的知识图谱补全(KGC)依赖于估计得分模型(实体，关系，实体)-元组，例如，通过嵌入初始知识图。通过调整预测阈值(使用手动注释的示例)，可以改善预测质量。本文尝试首次针对KGC进行冷启动校准，在此过程中初始没有注释的示例，并且只能选择有限数量的元组进行注释。我们的新方法ACTC基于有限的注释元组有效地找到好的每个关系的阈值。除了一些注释的元组外，ACTC还利用Logistic回归或高斯过程分类器估计的未标记元组的正确性。我们还通过密度和随机选择等不同方法选择候选元组进行注释。我们使用五个评分模型和一个oracle注释进行实验。

    Self-supervised knowledge-graph completion (KGC) relies on estimating a scoring model over (entity, relation, entity)-tuples, for example, by embedding an initial knowledge graph. Prediction quality can be improved by calibrating the scoring model, typically by adjusting the prediction thresholds using manually annotated examples. In this paper, we attempt for the first time cold-start calibration for KGC, where no annotated examples exist initially for calibration, and only a limited number of tuples can be selected for annotation. Our new method ACTC finds good per-relation thresholds efficiently based on a limited set of annotated tuples. Additionally to a few annotated tuples, ACTC also leverages unlabeled tuples by estimating their correctness with Logistic Regression or Gaussian Process classifiers. We also experiment with different methods for selecting candidate tuples for annotation: density-based and random selection. Experiments with five scoring models and an oracle annotat
    
[^90]: K-SpecPart: 一种用于多元超图划分解决方案改进的监督谱框架

    K-SpecPart: A Supervised Spectral Framework for Multi-Way Hypergraph Partitioning Solution Improvement. (arXiv:2305.06167v1 [cs.LG])

    [http://arxiv.org/abs/2305.06167](http://arxiv.org/abs/2305.06167)

    本论文提出了一个名为K-SpecPart的超图划分算法，通过解决广义特征值问题，捕捉平衡分区目标和全局超图结构，在多元划分中提高算法质量。

    

    现有的超图划分器采用多层次策略，构建多个更粗糙的超图来进行切割尺寸的优化，但存在一些局限性：（一）粗化过程依赖于局部邻域结构，忽略了全局超图结构；（二）优化启发式算法存在陷入局部最小值的风险。本文提出了一种新的监督谱框架——K-SpecPart，通过解决广义特征值问题解决了这些限制，捕捉了平衡分区目标和全局超图结构，并利用高质量的多层次划分方案作为提示。在多元划分的情况下，K-SpecPart从多元提示划分方案中获得多个双向划分方案。将这些方案整合到广义特征值问题中以计算特征向量，从而创建大维度嵌入。线性判别分析（LDA）用于将其转换为低维嵌入。

    State-of-the-art hypergraph partitioners follow the multilevel paradigm, constructing multiple levels of coarser hypergraphs to drive cutsize refinement. These partitioners face limitations: (i) coarsening processes depend on local neighborhood structure, ignoring global hypergraph structure; (ii) refinement heuristics risk entrapment in local minima. We introduce K-SpecPart, a supervised spectral framework addressing these limitations by solving a generalized eigenvalue problem, capturing balanced partitioning objectives and global hypergraph structure in a low-dimensional vertex embedding while leveraging high-quality multilevel partitioning solutions as hints. In multi-way partitioning, K-SpecPart derives multiple bipartitioning solutions from a multi-way hint partitioning solution. It integrates these solutions into the generalized eigenvalue problem to compute eigenvectors, creating a large-dimensional embedding. Linear Discriminant Analysis (LDA) is used to transform this into a 
    
[^91]: 可扩展的最优边缘分布机（Scalable Optimal Margin Distribution Machine）

    Scalable Optimal Margin Distribution Machine. (arXiv:2305.04837v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.04837](http://arxiv.org/abs/2305.04837)

    本文提出了一种可扩展的最优边缘分布机（ODM）训练方法，与原始ODM训练方法相比，可实现近十倍的加速。对于非线性核，我们提出了一种新颖的分布感知分区方法，使得每个分区上训练的本地ODM接近全局的ODM，并快速收敛。在应用线性核时，我们扩展了一种通信有效的SVRG方法以进一步加速训练。

    

    最优边缘分布机（ODM）是一种新型的统计学习框架，根据新的边缘理论建立，表现出比传统的基于大间隔的对应方法更好的泛化性能。然而，像其他核方法一样，它在计算时间和内存方面普遍存在可扩展性问题。本文提出了一种可扩展的ODM，与原始ODM训练方法相比，可实现近十倍的加速。对于非线性核，我们提出了一种新颖的分布感知分区方法，使得每个分区上训练的本地ODM接近全局的ODM，并快速收敛。当应用线性核时，我们扩展了一种通信有效的SVRG方法以进一步加速训练。大量经验证据表明，我们提出的方法在计算效率方面极高，并且几乎不会恶化泛化性能。

    Optimal margin Distribution Machine (ODM) is a newly proposed statistical learning framework rooting in the novel margin theory, which demonstrates better generalization performance than the traditional large margin based counterparts. Nonetheless, it suffers from the ubiquitous scalability problem regarding both computation time and memory as other kernel methods. This paper proposes a scalable ODM, which can achieve nearly ten times speedup compared to the original ODM training method. For nonlinear kernels, we propose a novel distribution-aware partition method to make the local ODM trained on each partition be close and converge fast to the global one. When linear kernel is applied, we extend a communication efficient SVRG method to accelerate the training further. Extensive empirical studies validate that our proposed method is highly computational efficient and almost never worsen the generalization.
    
[^92]: 基于矩阵流形的神经网络构建：陀螺矢量空间方法

    Building Neural Networks on Matrix Manifolds: A Gyrovector Space Approach. (arXiv:2305.04560v1 [stat.ML])

    [http://arxiv.org/abs/2305.04560](http://arxiv.org/abs/2305.04560)

    本文通过将陀螺矢量空间中的一些概念推广到SPD和Grassmann流形，提出了在这些流形上构建神经网络的新模型和新层，并在人类动作识别和知识图谱完成两个应用中展示了其有效性。

    

    矩阵流形，如对称正定（SPD）矩阵和Grassmann流形，出现在许多应用中。最近，通过应用陀螺群和陀螺矢量空间的理论——这是一个研究双曲几何的强大框架——一些工作尝试在矩阵流形上构建欧几里德神经网络的原则性推广。然而，由于缺乏考虑流形的内积和陀螺角等概念的陀螺矢量空间，相比于用于研究双曲几何的那些概念，这些工作提供的技术和数学工具仍然有限。在本文中，我们将陀螺矢量空间中的一些概念推广到SPD和Grassmann流形，并提出了在这些流形上构建神经网络的新模型和新层。我们展示了我们的方法在人类动作识别和知识图谱完成两个应用中的有效性。

    Matrix manifolds, such as manifolds of Symmetric Positive Definite (SPD) matrices and Grassmann manifolds, appear in many applications. Recently, by applying the theory of gyrogroups and gyrovector spaces that is a powerful framework for studying hyperbolic geometry, some works have attempted to build principled generalizations of Euclidean neural networks on matrix manifolds. However, due to the lack of many concepts in gyrovector spaces for the considered manifolds, e.g., the inner product and gyroangles, techniques and mathematical tools provided by these works are still limited compared to those developed for studying hyperbolic geometry. In this paper, we generalize some notions in gyrovector spaces for SPD and Grassmann manifolds, and propose new models and layers for building neural networks on these manifolds. We show the effectiveness of our approach in two applications, i.e., human action recognition and knowledge graph completion.
    
[^93]: 多保真度深度贝叶斯主动学习的解缠框架

    Disentangled Multi-Fidelity Deep Bayesian Active Learning. (arXiv:2305.04392v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.04392](http://arxiv.org/abs/2305.04392)

    本论文提出了一种新的框架D-MFDAL，它可以学习具有多个保真度的函数分布，相比于现有基于高斯过程的方法，该方法具有更好的性能和高斯过程的灵活性和可解释性。

    

    为了平衡质量和成本，科学和工程的多个领域会在多个复杂程度上运行模拟。多保真度主动学习旨在通过从多个保真度水平积极地获取数据来学习从输入参数到模拟输出的直接映射。然而，现有的基于高斯过程的方法很难扩展到高维数据。基于深度学习的方法通常在隐藏表示中强制实施分层结构，仅支持从低保真度到高保真度传递信息。这些方法可能导致从低保真度表示到高保真度表示中不良误差的传播。我们提出了一种称为解缠多保真度深度贝叶斯主动学习（D-MFDAL）的新框架，该框架学习了在多个保真度上函数分布的条件下的代理模型。在学习偏微分方程的深度代理的基准任务中，我们的方法展示了优越的性能，同时保留了高斯过程的灵活性和可解释性。

    To balance quality and cost, various domain areas of science and engineering run simulations at multiple levels of sophistication. Multi-fidelity active learning aims to learn a direct mapping from input parameters to simulation outputs at the highest fidelity by actively acquiring data from multiple fidelity levels. However, existing approaches based on Gaussian processes are hardly scalable to high-dimensional data. Deep learning-based methods often impose a hierarchical structure in hidden representations, which only supports passing information from low-fidelity to high-fidelity. These approaches can lead to the undesirable propagation of errors from low-fidelity representations to high-fidelity ones. We propose a novel framework called Disentangled Multi-fidelity Deep Bayesian Active Learning (D-MFDAL), that learns the surrogate models conditioned on the distribution of functions at multiple fidelities. On benchmark tasks of learning deep surrogates of partial differential equatio
    
[^94]: 悬臂梁损伤检测的神经符号模型

    Neuro-symbolic model for cantilever beams damage detection. (arXiv:2305.03063v1 [cs.LG])

    [http://arxiv.org/abs/2305.03063](http://arxiv.org/abs/2305.03063)

    本文提出了一种神经符号模型用于悬臂梁损伤检测，该模型通过将卷积网络的处理能力与逻辑查询交互控制相结合，不仅能够准确检测损伤，而且还能够提供解释和定位，使其在操作条件下更可靠和可信。

    

    在过去的十年中，损伤检测方法迅速从先进的信号处理方法转变为机器学习，尤其是深度学习模型，以准确地、非侵入性地估计梁结构状态。但随着深度学习模型达到巅峰表现，人们也观察到了它们适用性的限制和易受攻击的弱点。其中最重要的一个原因是深度学习系统内在可解释性的缺失，由于知识编码在张量值中而没有包含逻辑约束。本文提出了一种神经符号模型，基于新颖的认知架构，将卷积网络的处理能力与直接将实际逻辑包含到模型中的查询交互控制结合起来，用于悬臂梁损伤检测。该混合判别模型不仅能够精确地检测损伤，而且能够提供损伤的解释和定位，使其在操作条件下更可靠和可信。

    In the last decade, damage detection approaches swiftly changed from advanced signal processing methods to machine learning and especially deep learning models, to accurately and non-intrusively estimate the state of the beam structures. But as the deep learning models reached their peak performances, also their limitations in applicability and vulnerabilities were observed. One of the most important reason for the lack of trustworthiness in operational conditions is the absence of intrinsic explainability of the deep learning system, due to the encoding of the knowledge in tensor values and without the inclusion of logical constraints. In this paper, we propose a neuro-symbolic model for the detection of damages in cantilever beams based on a novel cognitive architecture in which we join the processing power of convolutional networks with the interactive control offered by queries realized through the inclusion of real logic directly into the model. The hybrid discriminative model is 
    
[^95]: BitGNN：释放二进制图神经网络在GPU上的性能潜力

    BitGNN: Unleashing the Performance Potential of Binary Graph Neural Networks on GPUs. (arXiv:2305.02522v1 [cs.DC])

    [http://arxiv.org/abs/2305.02522](http://arxiv.org/abs/2305.02522)

    本文提出了一种从效率角度重新设计的二进制GNN推理后端算法，用于充分发挥GPU上位操作的特性，实验结果表明提出的算法比最先进的二进制GNN实现提高了8-22倍的性能，保持相同准确性。

    

    最近的研究表明，通过对张量进行二值化，二进制图神经网络（GNN）可以节省GNN计算的计算量。然而，先前的工作主要集中在算法设计或训练技术上，没有完全实现将性能潜力显现到加速器硬件上的方法。本文从效率的角度重新设计了二进制GNN推理后端，并提出了一系列抽象和技术，以最佳地映射二进制GNN及其计算，以适应GPU上的位操作的特性。在使用GCNs、GraphSAGE和GraphSAINT的真实图上，实验结果表明，所提出的技术在保持相同准确性的同时，比最先进的二进制GNN实现提高了8-22倍。BitGNN代码已公开发布。

    Recent studies have shown that Binary Graph Neural Networks (GNNs) are promising for saving computations of GNNs through binarized tensors. Prior work, however, mainly focused on algorithm designs or training techniques, leaving it open to how to materialize the performance potential on accelerator hardware fully. This work redesigns the binary GNN inference backend from the efficiency perspective. It fills the gap by proposing a series of abstractions and techniques to map binary GNNs and their computations best to fit the nature of bit manipulations on GPUs. Results on real-world graphs with GCNs, GraphSAGE, and GraphSAINT show that the proposed techniques outperform state-of-the-art binary GNN implementations by 8-22X with the same accuracy maintained. BitGNN code is publicly available.
    
[^96]: 利用大型语言模型从医生-患者对话中生成临床笔记：来自MEDIQA-Chat的见解

    Clinical Note Generation from Doctor-Patient Conversations using Large Language Models: Insights from MEDIQA-Chat. (arXiv:2305.02220v1 [cs.CL])

    [http://arxiv.org/abs/2305.02220](http://arxiv.org/abs/2305.02220)

    本文介绍了使用大型语言模型从医生-患者对话中自动生成临床笔记的研究，采用少样本上下文学习法所生成笔记表现优秀，且可与人工编写的笔记媲美。

    

    本文描述了我们在MEDIQA-Chat 2023共享任务中提交的自动临床笔记生成方案。我们报告了两种方法的结果：第一种是在共享任务数据上微调预训练语言模型（PLM），第二种是使用大型语言模型（LLM）的少样本上下文学习（ICL）。两种方法都取得了高性能，如通过自动度量标准（例如ROUGE，BERTScore）测量，并分别在所有提交的方案中排名第二和第一。专家审核表明，通过基于ICL的方法使用GPT-4生成的笔记与人工编写的笔记一样受欢迎，这使得它成为从医生-患者对话中自动生成笔记的有前途的路径。

    This paper describes our submission to the MEDIQA-Chat 2023 shared task for automatic clinical note generation from doctor-patient conversations. We report results for two approaches: the first fine-tunes a pre-trained language model (PLM) on the shared task data, and the second uses few-shot in-context learning (ICL) with a large language model (LLM). Both achieve high performance as measured by automatic metrics (e.g. ROUGE, BERTScore) and ranked second and first, respectively, of all submissions to the shared task. Expert human scrutiny indicates that notes generated via the ICL-based approach with GPT-4 are preferred about as often as human-written notes, making it a promising path toward automated note generation from doctor-patient conversations.
    
[^97]: 在稀疏探测中寻找海量神经元: 实例研究

    Finding Neurons in a Haystack: Case Studies with Sparse Probing. (arXiv:2305.01610v1 [cs.LG])

    [http://arxiv.org/abs/2305.01610](http://arxiv.org/abs/2305.01610)

    本文通过训练$k$-稀疏线性分类器以预测输入特征是否存在，研究了大型语言模型（LLM）内部神经元激活的表示方式；对不同层次的神经元网络的研究表明，早期层利用神经元的稀疏组合来表示多种特征，中间层有特定的神经元表示高级上下文特征，增加规模使特征表示更加稀疏化。

    

    尽管大型语言模型(LLM)的应用和部署迅速增加，但这些模型的内部计算仍然不透明且难以理解。本文旨在了解高级可解释特征在LLM内部神经元激活中的表示方式。我们使用$k$-稀疏线性分类器(探针)来训练这些内部激活值，并预测输入的特征是否存在；通过改变$k$值，我们研究了学习表示的稀疏性以及随着模型规模的变化而变化的情况。当$k=1$时，我们定位某个特定特征非常相关的单个神经元，并进行了大量案例研究，以说明LLM的一般性质。特别是，我们展示了早期层利用神经元的稀疏组合来表示许多特征，中间层似乎具有专门的神经元来表示更高级的上下文特征，而增加的规模则导致表示稀疏性增加。

    Despite rapid adoption and deployment of large language models (LLMs), the internal computations of these models remain opaque and poorly understood. In this work, we seek to understand how high-level human-interpretable features are represented within the internal neuron activations of LLMs. We train $k$-sparse linear classifiers (probes) on these internal activations to predict the presence of features in the input; by varying the value of $k$ we study the sparsity of learned representations and how this varies with model scale. With $k=1$, we localize individual neurons which are highly relevant for a particular feature, and perform a number of case studies to illustrate general properties of LLMs. In particular, we show that early layers make use of sparse combinations of neurons to represent many features in superposition, that middle layers have seemingly dedicated neurons to represent higher-level contextual features, and that increasing scale causes representational sparsity to
    
[^98]: 无特定模型泛化难度度量

    Model-agnostic Measure of Generalization Difficulty. (arXiv:2305.01034v1 [cs.LG])

    [http://arxiv.org/abs/2305.01034](http://arxiv.org/abs/2305.01034)

    该论文提出了第一个无特定模型的、量化机器学习测试泛化难度的方法——归纳偏差复杂度度量。该方法量化了在任务上良好泛化所需的总信息量与数据提供的信息量之差，通常需要在许多维度上泛化的任务比涉及更少维度但要求更多细节的任务要困难得多。

    

    机器学习算法的度量是其可以执行的任务难度，足够困难的任务是强大机器学习模型的关键驱动因素。然而，量化机器学习测试的泛化难度一直是具有挑战性的。我们提出了据我们所知的第一个对任务固有泛化难度的无特定模型的度量。我们的归纳偏差复杂度度量量化了在任务上良好泛化所需的总信息量与数据提供的信息量之差。通过测量适合训练数据的假设在任务中泛化的分数占据的容积，来实现这一点。它与模型必须泛化的空间的内在维数成指数比例，但仅在每个维度的分辨率上呈多项式比例，表明需要在许多维度上泛化的任务比涉及更少维度的更多细节的任务要困难得多。

    The measure of a machine learning algorithm is the difficulty of the tasks it can perform, and sufficiently difficult tasks are critical drivers of strong machine learning models. However, quantifying the generalization difficulty of machine learning benchmarks has remained challenging. We propose what is to our knowledge the first model-agnostic measure of the inherent generalization difficulty of tasks. Our inductive bias complexity measure quantifies the total information required to generalize well on a task minus the information provided by the data. It does so by measuring the fractional volume occupied by hypotheses that generalize on a task given that they fit the training data. It scales exponentially with the intrinsic dimensionality of the space over which the model must generalize but only polynomially in resolution per dimension, showing that tasks which require generalizing over many dimensions are drastically more difficult than tasks involving more detail in fewer dimen
    
[^99]: 跨图动态迁移学习

    Dynamic Transfer Learning across Graphs. (arXiv:2305.00664v1 [cs.LG])

    [http://arxiv.org/abs/2305.00664](http://arxiv.org/abs/2305.00664)

    该论文提出了一个新的问题：在动态图形环境下如何有效地进行跨图迁移学习，主要解决了领域演化对泛化性能的影响。

    

    在许多高风险领域中，跨图传输知识起着关键作用，包括运输网络、电子商务网络、神经科学和金融领域。我们提出了一个新问题：在动态设置下，考虑已观察到的具有标签的源图和标签稀疏的目标图，如何有效地表征不断变化的领域偏差，并优化目标域在下一个时间戳的泛化性能？为了回答这个问题，我们首次提出了跨图动态迁移学习设置下的一般化界限，这意味着泛化性能由领域演化控制。

    Transferring knowledge across graphs plays a pivotal role in many high-stake domains, ranging from transportation networks to e-commerce networks, from neuroscience to finance. To date, the vast majority of existing works assume both source and target domains are sampled from a universal and stationary distribution. However, many real-world systems are intrinsically dynamic, where the underlying domains are evolving over time. To bridge the gap, we propose to shift the problem to the dynamic setting and ask: given the label-rich source graphs and the label-scarce target graphs observed in previous T timestamps, how can we effectively characterize the evolving domain discrepancy and optimize the generalization performance of the target domain at the incoming T+1 timestamp? To answer the question, for the first time, we propose a generalization bound under the setting of dynamic transfer learning across graphs, which implies the generalization performance is dominated by domain evolution
    
[^100]: 发现并校正：概念感知的假相关抑制方法。

    Discover and Cure: Concept-aware Mitigation of Spurious Correlation. (arXiv:2305.00650v1 [cs.LG])

    [http://arxiv.org/abs/2305.00650](http://arxiv.org/abs/2305.00650)

    本研究提出了一个可解释的方法框架(DISC)来抑制深度神经网络中的假相关，通过发现不稳定的概念并将其作为假属性干预训练数据来提高模型的泛化能力和可解释性。在目标识别任务中，DISC胜过了现有最先进的方法。

    

    深度神经网络经常依赖于假相关来进行预测，这会导致无法超越训练环境的一般化。例如，将猫与床作为背景联系的模型，在没有床的其他环境中可能无法预测到猫的存在。抑制假相关对于构建可信赖的模型至关重要。然而，现有的方法缺乏透明度提供有关抑制过程的洞察。本研究提出了一个可解释的框架Discover and Cure (DISC)来解决这个问题。使用人类可解释的概念，DISC迭代地 1)发现不稳定的概念，将其作为假属性在不同环境下，然后 2)使用发现的概念干预训练数据以减少假相关。在系统实验中，DISC提供了比现有方法更优秀的泛化能力和可解释性。具体地，在目标识别任务中，它胜过了现有的最先进方法。

    Deep neural networks often rely on spurious correlations to make predictions, which hinders generalization beyond training environments. For instance, models that associate cats with bed backgrounds can fail to predict the existence of cats in other environments without beds. Mitigating spurious correlations is crucial in building trustworthy models. However, the existing works lack transparency to offer insights into the mitigation process. In this work, we propose an interpretable framework, Discover and Cure (DISC), to tackle the issue. With human-interpretable concepts, DISC iteratively 1) discovers unstable concepts across different environments as spurious attributes, then 2) intervenes on the training data using the discovered concepts to reduce spurious correlation. Across systematic experiments, DISC provides superior generalization ability and interpretability than the existing approaches. Specifically, it outperforms the state-of-the-art methods on an object recognition task
    
[^101]: NNSplitter：基于自动权重混淆的DNN模型主动防御方案

    NNSplitter: An Active Defense Solution to DNN Model via Automated Weight Obfuscation. (arXiv:2305.00097v1 [cs.LG])

    [http://arxiv.org/abs/2305.00097](http://arxiv.org/abs/2305.00097)

    NNSplitter是一种主动保护深度神经网络模型知识产权的方案，通过将模型分为混淆模型和模型秘密两部分，采用可信执行环境和基于强化学习的控制器来最大化精度下降和减少混淆权重的数量。

    

    深度神经网络模型作为一种有价值的知识产权，已经通过数字水印等技术进行保护。然而，这种被动模型保护并不能完全防止模型滥用。本研究提出了一种主动模型知识产权保护方案，即NNSplitter，通过将模型分为两部分来主动保护模型：一个表现较差的混淆模型和由混淆权重的索引和原始值组成的模型秘密，只有授权用户才能访问。NNSplitter利用可信执行环境来保护秘密，并采用基于强化学习的控制器来减少混淆权重的数量，同时最大化精度下降。我们的实验表明，仅修改超过2800万个权重的313个（即0.001％），混淆VGG-11模型在Fashion-MNIST上的精度可以降低到10％。我们还证明NNSplitter具有隐蔽性和韧性。

    As a type of valuable intellectual property (IP), deep neural network (DNN) models have been protected by techniques like watermarking. However, such passive model protection cannot fully prevent model abuse. In this work, we propose an active model IP protection scheme, namely NNSplitter, which actively protects the model by splitting it into two parts: the obfuscated model that performs poorly due to weight obfuscation, and the model secrets consisting of the indexes and original values of the obfuscated weights, which can only be accessed by authorized users. NNSplitter uses the trusted execution environment to secure the secrets and a reinforcement learning-based controller to reduce the number of obfuscated weights while maximizing accuracy drop. Our experiments show that by only modifying 313 out of over 28 million (i.e., 0.001%) weights, the accuracy of the obfuscated VGG-11 model on Fashion-MNIST can drop to 10%. We also demonstrate that NNSplitter is stealthy and resilient aga
    
[^102]: 利用扰动来改善基于核化斯坦距的拟合优度检验

    Using Perturbation to Improve Goodness-of-Fit Tests based on Kernelized Stein Discrepancy. (arXiv:2304.14762v1 [stat.ML])

    [http://arxiv.org/abs/2304.14762](http://arxiv.org/abs/2304.14762)

    本文提出了一种通过在样本中引入扰动，改进基于核化斯坦距的拟合优度检验方法的方法，以解决在同质但混合比例不同的情况下低功率的问题，并展示实验证据证明了该方法的功效。

    

    核化斯坦距（KSD）是一种广泛用于拟合优度检验的基于得分的差异度量。即使目标分布具有未知的标准化因子，例如在贝叶斯分析中，也可以应用它。我们理论上和实验证明，当目标分布和替代分布具有相同且相距较远的模式但在混合比例上有所不同时，KSD检验可能会出现低功率问题。我们提出通过马尔科夫转移核对观测样本进行扰动，使其相对于目标分布不变。这使我们可以在扰动样本上使用KSD检验。我们提供的数值证据表明，使用适当选择的核时，所提出的方法可以比KSD检验具有更高的功率。

    Kernelized Stein discrepancy (KSD) is a score-based discrepancy widely used in goodness-of-fit tests. It can be applied even when the target distribution has an unknown normalising factor, such as in Bayesian analysis. We show theoretically and empirically that the KSD test can suffer from low power when the target and the alternative distribution have the same well-separated modes but differ in mixing proportions. We propose to perturb the observed sample via Markov transition kernels, with respect to which the target distribution is invariant. This allows us to then employ the KSD test on the perturbed sample. We provide numerical evidence that with suitably chosen kernels the proposed approach can lead to a substantially higher power than the KSD test.
    
[^103]: 松弛假设下Adam收敛性的证明

    Convergence of Adam Under Relaxed Assumptions. (arXiv:2304.13972v1 [math.OC])

    [http://arxiv.org/abs/2304.13972](http://arxiv.org/abs/2304.13972)

    本文对Adam算法做了新的假设并进行了证明，证明了在更加现实的条件下，Adam能够以较小的梯度复杂度达到稳定点。

    

    本文针对一类广泛的优化目标，对自适应矩估计（Adam）算法的收敛性进行了严格证明。虽然Adam算法在训练深度神经网络中的流行度和效率很高，但其理论性质尚未完全理解，现有的收敛性证明需要过于强的假设，如全局梯度有界，以证明收敛到稳定点。本文证明了在更为现实的条件下，Adam能以$\mathcal{O}(\epsilon^{-4})$梯度复杂度收敛到$\epsilon$-稳定点。我们分析的关键是根据一种广义光滑性假设给出的，沿着优化轨迹的梯度有界的新证明。根据该假设，局部光滑性(即存在时的Hessian norm)受梯度范数的次平方函数限制。此外，我们提出了一种方差约减版本的Adam与加速Gradient。

    In this paper, we provide a rigorous proof of convergence of the Adaptive Moment Estimate (Adam) algorithm for a wide class of optimization objectives. Despite the popularity and efficiency of the Adam algorithm in training deep neural networks, its theoretical properties are not yet fully understood, and existing convergence proofs require unrealistically strong assumptions, such as globally bounded gradients, to show the convergence to stationary points. In this paper, we show that Adam provably converges to $\epsilon$-stationary points with $\mathcal{O}(\epsilon^{-4})$ gradient complexity under far more realistic conditions. The key to our analysis is a new proof of boundedness of gradients along the optimization trajectory, under a generalized smoothness assumption according to which the local smoothness (i.e., Hessian norm when it exists) is bounded by a sub-quadratic function of the gradient norm. Moreover, we propose a variance-reduced version of Adam with an accelerated gradien
    
[^104]: 基于年龄和资源分配的NOMA网络下通信高效联邦学习的客户端选择方案

    Joint Age-based Client Selection and Resource Allocation for Communication-Efficient Federated Learning over NOMA Networks. (arXiv:2304.08996v1 [cs.LG])

    [http://arxiv.org/abs/2304.08996](http://arxiv.org/abs/2304.08996)

    本文针对联邦学习在无线网络上通信受限、收敛速度慢和资源有限等问题，提出了一种基于年龄和资源分配的客户端选择方案，旨在最小化每轮联邦学习的总时间消耗，从而提高联邦学习的性能。

    

    联邦学习（FL）是一种有效的分布式学习范式，它可以使得客户端使用本地数据协同训练全局模型。然而，当FL部署在无线网络上时，由于通信链路差和收敛速度慢，FL的性能常常受到限制。此外，由于无线资源受限，准确选取客户端和控制资源分配对于提高FL性能至关重要。鉴于这些挑战，在本文中提出了客户端选择和资源分配的联合优化问题，旨在最小化在非正交多址（NOMA）无线网络上每轮FL的总时间消耗。具体而言，我们首先提出了一种新的客户端选择方案，通过考虑收到的本地FL模型的新旧程度来设计，在此基础上，通过年龄更新（AoU）指标获得资源分配的闭合式解。

    Federated learning (FL) is a promising paradigm that enables distributed clients to collaboratively train a shared global model while keeping the training data locally. However, the performance of FL is often limited by poor communication links and slow convergence when FL is deployed over wireless networks. Besides, due to the limited radio resources, it is crucial to select clients and control resource allocation accurately for improved FL performance. Motivated by these challenges, a joint optimization problem of client selection and resource allocation is formulated in this paper, aiming to minimize the total time consumption of each round in FL over non-orthogonal multiple access (NOMA) enabled wireless network. Specifically, based on a metric termed the age of update (AoU), we first propose a novel client selection scheme by accounting for the staleness of the received local FL models. After that, the closed-form solutions of resource allocation are obtained by monotonicity analy
    
[^105]: 带有自我改进硬约束的多能源管理系统的安全强化学习

    Safe reinforcement learning with self-improving hard constraints for multi-energy management systems. (arXiv:2304.08897v1 [eess.SY])

    [http://arxiv.org/abs/2304.08897](http://arxiv.org/abs/2304.08897)

    本论文提出了一种安全强化学习方法，能够实现多能源管理系统中的最优控制，在保证硬约束的前提下减少工程工作，降低建模偏差，并避免潜在的不安全行为。

    

    带有硬约束保证的安全强化学习是多能源管理系统中最有前途的最优控制方向。它只需要在环境特定的约束函数本身上预先而不是完整的模型（即植物，干扰和噪声模型，以及未包括在植物模型中的状态的预测模型 - 例如需求，天气和价格预测）。因此，可减少项目特定的前期和持续的工程工作，仍可以学习更好地表示基础系统动态，并使建模偏差最小化（无基于模型的目标函数）。然而，即使仅约束函数本身有时也不总是容易提供准确的先验（例如能量平衡约束需要详细确定所有能量输入和输出），从而导致潜在的不安全行为。在本文中，我们提出了两个新的进展：（I）将Optlayer和SafeFallback方法结合起来，命名为O

    Safe reinforcement learning (RL) with hard constraint guarantees is a promising optimal control direction for multi-energy management systems. It only requires the environment-specific constraint functions itself a prior and not a complete model (i.e. plant, disturbance and noise models, and prediction models for states not included in the plant model - e.g. demand, weather, and price forecasts). The project-specific upfront and ongoing engineering efforts are therefore still reduced, better representations of the underlying system dynamics can still be learned and modeling bias is kept to a minimum (no model-based objective function). However, even the constraint functions alone are not always trivial to accurately provide in advance (e.g. an energy balance constraint requires the detailed determination of all energy inputs and outputs), leading to potentially unsafe behavior. In this paper, we present two novel advancements: (I) combining the Optlayer and SafeFallback method, named O
    
[^106]: 无标签概念瓶颈模型

    Label-Free Concept Bottleneck Models. (arXiv:2304.06129v1 [cs.LG])

    [http://arxiv.org/abs/2304.06129](http://arxiv.org/abs/2304.06129)

    无标签CBM是一个可解释的框架，能够将任何神经网络转化为CBM，并且不需要标记数据，同时保持高准确性。

    

    概念瓶颈模型(CBM)是一种创建更易解释的神经网络的流行方法，其采用隐藏层神经元对应于人类可理解的概念。然而，现有的CBM及其变体存在两个关键限制：首先，它们需要为每个预定义的概念收集标记数据，这是耗时且劳动密集的；其次，在更复杂的数据集上，CBM的准确性通常明显低于标准神经网络的准确性，这样的表现为其在实际世界应用中造成一定的障碍。鉴于这些挑战，我们提出了无标签CBM，它是一种将任何神经网络转换为可解释CBM的新框架，无需标记概念数据，同时保持高准确性。我们的无标签CBM有许多优点，它是可扩展的——我们提出了第一个可扩展到ImageNet的CBM，高效的——即使对于非常大的数据集，创建CBM仅需要几个小时，而且可以自动化进行，不需要人类干预。

    Concept bottleneck models (CBM) are a popular way of creating more interpretable neural networks by having hidden layer neurons correspond to human-understandable concepts. However, existing CBMs and their variants have two crucial limitations: first, they need to collect labeled data for each of the predefined concepts, which is time consuming and labor intensive; second, the accuracy of a CBM is often significantly lower than that of a standard neural network, especially on more complex datasets. This poor performance creates a barrier for adopting CBMs in practical real world applications. Motivated by these challenges, we propose Label-free CBM which is a novel framework to transform any neural network into an interpretable CBM without labeled concept data, while retaining a high accuracy. Our Label-free CBM has many advantages, it is: scalable - we present the first CBM scaled to ImageNet, efficient - creating a CBM takes only a few hours even for very large datasets, and automate
    
[^107]: 探究鲁棒性模型与生成模型之间的联系

    Exploring the Connection between Robust and Generative Models. (arXiv:2304.04033v1 [cs.LG])

    [http://arxiv.org/abs/2304.04033](http://arxiv.org/abs/2304.04033)

    本文探究鲁棒性判别分类器与生成模型之间的联系，并发现在输入空间中，非定向对抗点非常可能在鉴别性模型中隐含的生成模型中拥有低能量，提出了一种名为高能量PGD的新攻击。

    

    本研究将通过分解鲁棒性判别分类器的损失函数来探究鲁棒性判别分类器与能量基模型(EBM)形式的生成模型之间的联系。我们发现，尽管常见的假设是对抗点离开了输入数据的流形，但是在输入空间中，非定向对抗点非常可能在鉴别性模型中隐含的生成模型中拥有低能量。我们提出了两个证据:非定向攻击的概率甚至比自然数据还要高，并且随着攻击强度的增加，其概率也会增加。这使我们能够轻松地检测它们并设计一种名为高能量PGD的新攻击，能够欺骗分类器但具有与数据集相似的能量。

    We offer a study that connects robust discriminative classifiers trained with adversarial training (AT) with generative modeling in the form of Energy-based Models (EBM). We do so by decomposing the loss of a discriminative classifier and showing that the discriminative model is also aware of the input data density. Though a common assumption is that adversarial points leave the manifold of the input data, our study finds out that, surprisingly, untargeted adversarial points in the input space are very likely under the generative model hidden inside the discriminative classifier -- have low energy in the EBM. We present two evidence: untargeted attacks are even more likely than the natural data and their likelihood increases as the attack strength increases. This allows us to easily detect them and craft a novel attack called High-Energy PGD that fools the classifier yet has energy similar to the data set.
    
[^108]: AutoRL超参数景观

    AutoRL Hyperparameter Landscapes. (arXiv:2304.02396v1 [cs.LG])

    [http://arxiv.org/abs/2304.02396](http://arxiv.org/abs/2304.02396)

    本文提出了一种方法，在训练期间多次建立和分析AutoRL超参数的景观，证明代表算法（DQN和SAC）在不同环境下的超参数景观会随时间而变化。

    

    强化学习（RL）在取得令人瞩目成果的同时，其超参数对性能的影响限制了其应用范围。这经常使得在实践中难以获得良好的结果。自动化RL（AutoRL）解决了这个难题，但有关超参数优化（HPO）方法在搜索最佳配置时所遍历的超参数景观动态变化的信息很少。鉴于现有AutoRL方法动态调整超参数配置的情况，我们提出了一种方法，在训练期间不仅在一个时间点，而且在多个时间点上建立和分析这些超参数景观。针对关于这种动态AutoRL方法合法性的一个重要开放问题，我们提供了充分的证据，表明在不同种类的环境（Cartpole和Pendulum）中，来自RL文献的代表算法（DQN和SAC）的超参数景观会随时间而强烈变化。

    Although Reinforcement Learning (RL) has shown to be capable of producing impressive results, its use is limited by the impact of its hyperparameters on performance. This often makes it difficult to achieve good results in practice. Automated RL (AutoRL) addresses this difficulty, yet little is known about the dynamics of the hyperparameter landscapes that hyperparameter optimization (HPO) methods traverse in search of optimal configurations. In view of existing AutoRL approaches dynamically adjusting hyperparameter configurations, we propose an approach to build and analyze these hyperparameter landscapes not just for one point in time but at multiple points in time throughout training. Addressing an important open question on the legitimacy of such dynamic AutoRL approaches, we provide thorough empirical evidence that the hyperparameter landscapes strongly vary over time across representative algorithms from RL literature (DQN and SAC) in different kinds of environments (Cartpole and
    
[^109]: 基于准度量学习的最优目标达成强化学习方法

    Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning. (arXiv:2304.01203v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.01203](http://arxiv.org/abs/2304.01203)

    本文介绍了一种新的强化学习方法——准度量强化学习（QRL），利用准度量模型来学习最优价值函数；在离线和在线的目标达成基准测试中，QRL展示了更好的采样效率和性能，包括基于状态和基于图像的观测。

    

    在目标达成强化学习中，最优价值函数具有特定的几何结构，称为准度量结构。本文介绍了一种新的强化学习方法——准度量强化学习（QRL），利用准度量模型来学习最优价值函数。与以往的方法不同，QRL的目标是专门为准度量设计的，并提供了强大的理论恢复保证。在离散化的MountainCar环境上进行了全面分析，确定了QRL的性质以及其优于其他方法的优势。在离线和在线的目标达成基准测试中，QRL还展示了更好的采样效率和性能，包括基于状态和基于图像的观测。

    In goal-reaching reinforcement learning (RL), the optimal value function has a particular geometry, called quasimetric structure. This paper introduces Quasimetric Reinforcement Learning (QRL), a new RL method that utilizes quasimetric models to learn optimal value functions. Distinct from prior approaches, the QRL objective is specifically designed for quasimetrics, and provides strong theoretical recovery guarantees. Empirically, we conduct thorough analyses on a discretized MountainCar environment, identifying properties of QRL and its advantages over alternatives. On offline and online goal-reaching benchmarks, QRL also demonstrates improved sample efficiency and performance, across both state-based and image-based observations.
    
[^110]: BOLT：一种用于在普通CPU硬件上训练和部署大规模神经网络的自动化深度学习框架。

    BOLT: An Automated Deep Learning Framework for Training and Deploying Large-Scale Neural Networks on Commodity CPU Hardware. (arXiv:2303.17727v1 [cs.LG])

    [http://arxiv.org/abs/2303.17727](http://arxiv.org/abs/2303.17727)

    BOLT是一种用于在标准CPU硬件上训练大型神经网络模型的稀疏深度学习库，它提供了一个灵活的高级API，使用户可以构建模型并抽象掉稀疏网络训练的算法细节。

    

    有效地在普通CPU硬件上进行大规模神经网络的训练和推理对于民主化深度学习能力具有巨大的实际意义。目前，由数十亿个参数组成的大规模模型的训练过程需要广泛使用专用硬件加速器（例如GPU），这些加速器仅限于少数具有相当财务资源的机构。此外，训练和部署这些模型通常会带来惊人的碳足迹。在本文中，我们通过引入BOLT，一种用于在标准CPU硬件上训练大型神经网络模型的稀疏深度学习库来解决这些挑战。BOLT提供了一个灵活的高级API，用于构建模型，该API对于现有流行的深度学习框架的用户来说是熟悉的。通过自动调整专用超参数，BOLT也抽象掉了稀疏网络训练的算法细节。

    Efficient large-scale neural network training and inference on commodity CPU hardware is of immense practical significance in democratizing deep learning (DL) capabilities. Presently, the process of training massive models consisting of hundreds of millions to billions of parameters requires the extensive use of specialized hardware accelerators, such as GPUs, which are only accessible to a limited number of institutions with considerable financial resources. Moreover, there is often an alarming carbon footprint associated with training and deploying these models. In this paper, we address these challenges by introducing BOLT, a sparse deep learning library for training massive neural network models on standard CPU hardware. BOLT provides a flexible, high-level API for constructing models that will be familiar to users of existing popular DL frameworks. By automatically tuning specialized hyperparameters, BOLT also abstracts away the algorithmic details of sparse network training. We e
    
[^111]: 神经签名核作为受控ResNets的无限宽度-深度极限。(arXiv:2303.17671v1 [math.DS])

    Neural signature kernels as infinite-width-depth-limits of controlled ResNets. (arXiv:2303.17671v1 [math.DS])

    [http://arxiv.org/abs/2303.17671](http://arxiv.org/abs/2303.17671)

    通过控制ResNets的欧拉离散化，提出了一种新的家族限制核，称为神经签名核。在无限深度情况下，有限宽度的受控ResNets按分布收敛至神经CDE。

    

    受沉积计算范例的启发，我们考虑由神经受控微分方程（神经CDE）的欧拉离散化定义的随机初始化受控ResNets。我们表明，在无限宽度-深度限制和适当的缩放下，这些架构弱收敛到一些连续路径空间上索引的高斯过程，并且具有满足根据激活函数的选择变化的某些偏微分方程（PDE）的核。在激活为恒等式的特殊情况下，我们表明该方程式简化为线性PDE，极限核与Salvi等人的签名核一致。在这种情况下，我们还表明宽度-深度极限是可交换的。我们将这种新的限制核家族称为神经签名核。最后，我们表明，在无限深度的情况下，有限宽度的受控ResNets按分布收敛到具有随机向量场的神经CDE，具体取决于w。

    Motivated by the paradigm of reservoir computing, we consider randomly initialized controlled ResNets defined as Euler-discretizations of neural controlled differential equations (Neural CDEs). We show that in the infinite-width-then-depth limit and under proper scaling, these architectures converge weakly to Gaussian processes indexed on some spaces of continuous paths and with kernels satisfying certain partial differential equations (PDEs) varying according to the choice of activation function. In the special case where the activation is the identity, we show that the equation reduces to a linear PDE and the limiting kernel agrees with the signature kernel of Salvi et al. (2021). In this setting, we also show that the width-depth limits commute. We name this new family of limiting kernels neural signature kernels. Finally, we show that in the infinite-depth regime, finite-width controlled ResNets converge in distribution to Neural CDEs with random vector fields which, depending on w
    
[^112]: Transformer和Snowball图卷积学习用于生物医学图分类

    Transformer and Snowball Graph Convolution Learning for Biomedical Graph Classification. (arXiv:2303.16132v1 [cs.LG])

    [http://arxiv.org/abs/2303.16132](http://arxiv.org/abs/2303.16132)

    本文介绍了一种新型Transformer和Snowball编码网络（TSEN），它将Transformer架构和图雪球连接引入GNNs。TSEN通过雪球编码层将图雪球连接与图Transformer结合起来，增强了捕捉多尺度信息和全局模式以学习整个图特征的能力。

    

    图或网络已被广泛用于描述和建模生物医学中的复杂系统。深度学习方法，尤其是图神经网络（GNNs），已被开发用于学习和预测这种结构化数据。在本文中，我们提出了一种用于生物医学图分类的新型Transformer和Snowball编码网络（TSEN），它将Transformer架构和图雪球连接引入GNNs，以学习整个图的表示。

    Graph or network has been widely used for describing and modeling complex systems in biomedicine. Deep learning methods, especially graph neural networks (GNNs), have been developed to learn and predict with such structured data. In this paper, we proposed a novel transformer and snowball encoding networks (TSEN) for biomedical graph classification, which introduced transformer architecture with graph snowball connection into GNNs for learning whole-graph representation. TSEN combined graph snowball connection with graph transformer by snowball encoding layers, which enhanced the power to capture multi-scale information and global patterns to learn the whole-graph features. On the other hand, TSEN also used snowball graph convolution as position embedding in transformer structure, which was a simple yet effective method for capturing local patterns naturally. Results of experiments using four graph classification datasets demonstrated that TSEN outperformed the state-of-the-art typical
    
[^113]: 基于空间离散化演化搜索的多目标安全博弈可扩展性研究

    Scaling Multi-Objective Security Games Provably via Space Discretization Based Evolutionary Search. (arXiv:2303.15821v1 [cs.LG])

    [http://arxiv.org/abs/2303.15821](http://arxiv.org/abs/2303.15821)

    本文提出了一个名为SDES的通用框架，其中包括离散化、优化、恢复和评估以及改进等四个组成部分，通过离散化连续解空间来实现多目标安全博弈的可扩展性。

    

    在安全领域，多目标安全博弈(MOSGs)允许防御者同时保护多个异质性攻击者的目标。MOSGs旨在同时最大化所有异质性回报，例如生命、金钱和犯罪率，而不合并异质性攻击者。在现实世界的场景中，需要保护的异质性攻击者和目标数量可能超出大多数现有的最先进的方法的能力，MOSGs受到可扩展性问题的限制。因此，本文提出了一个通用框架称为SDES，该框架基于多目标进化搜索来扩展MOSGs的大规模目标和异质性攻击者。SDES由四个连续的关键组成部分组成，即离散化、优化、恢复和评估以及改进。具体来说，SDES使用博弈理论中的最大不平等性原理将原始的高维连续解空间离散化为低维离散解空间。

    In the field of security, multi-objective security games (MOSGs) allow defenders to simultaneously protect targets from multiple heterogeneous attackers. MOSGs aim to simultaneously maximize all the heterogeneous payoffs, e.g., life, money, and crime rate, without merging heterogeneous attackers. In real-world scenarios, the number of heterogeneous attackers and targets to be protected may exceed the capability of most existing state-of-the-art methods, i.e., MOSGs are limited by the issue of scalability. To this end, this paper proposes a general framework called SDES based on many-objective evolutionary search to scale up MOSGs to large-scale targets and heterogeneous attackers. SDES consists of four consecutive key components, i.e., discretization, optimization, restoration and evaluation, and refinement. Specifically, SDES first discretizes the originally high-dimensional continuous solution space to the low-dimensional discrete one by the maximal indifference property in game theo
    
[^114]: 训练神经网络的内存优化方法评估

    An Evaluation of Memory Optimization Methods for Training Neural Networks. (arXiv:2303.14633v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.14633](http://arxiv.org/abs/2303.14633)

    本文从系统角度对现有内存优化方法（MOMs）进行全面分析和评估，提出了在不同场景下选择不同的评估指标并对各种MOMs进行全面比较的方案，为研究人员和从业人员提供了选择和应用MOMs的指导。

    

    随着模型规模的不断增长，内存优化方法（MOMs）的开发已成为解决训练大型模型时遇到的内存瓶颈的解决方案。为全面检查各种MOM的实际价值，我们从系统角度对现有文献进行了彻底分析。我们的分析揭示了研究界一个显著的挑战:缺少标准度量来有效评估MOM的功效。信息有限的评估指标妨碍了研究人员和从业者可靠地比较和基准不同方法。因此，得出明确的结论并做出关于选择和应用MOMs的知情决策变得困难。为解决这个挑战，本文总结了MOM在模型训练中证明优势的场景。我们提议在不同场景下使用不同的评估指标，并对各种MOM进行全面比较，以提供有关其相对优势和局限性的见解。我们的评估结果证明了MOM在减少模型训练记忆需求的同时保持训练性能方面的实际价值。通过我们的分析，我们希望为研究人员和从业人员提供选择和应用MOMs的指导。

    As models continue to grow in size, the development of memory optimization methods (MOMs) has emerged as a solution to address the memory bottleneck encountered when training large models. To comprehensively examine the practical value of various MOMs, we have conducted a thorough analysis of existing literature from a systems perspective. Our analysis has revealed a notable challenge within the research community: the absence of standardized metrics for effectively evaluating the efficacy of MOMs. The scarcity of informative evaluation metrics hinders the ability of researchers and practitioners to compare and benchmark different approaches reliably. Consequently, drawing definitive conclusions and making informed decisions regarding the selection and application of MOMs becomes a challenging endeavor. To address the challenge, this paper summarizes the scenarios in which MOMs prove advantageous for model training. We propose the use of distinct evaluation metrics under different scen
    
[^115]: 连接生成半监督学习和生成开放集识别

    Linking generative semi-supervised learning and generative open-set recognition. (arXiv:2303.11702v1 [cs.CV])

    [http://arxiv.org/abs/2303.11702](http://arxiv.org/abs/2303.11702)

    本研究旨在探究生成半监督学习和生成开放集识别之间的关系。SSL-GANs和OSR-GANs方法的相似性在于都要求生成器在互补空间中产生样本，并通过正则化来推广开放空间。研究结果表明SSL优化边缘-GAN在结合SSL-OSR任务方面树立新的标准，但在某些OSR任务中OSR优化的ARP-GAN仍然略优于SSL-GAN。

    

    本研究在生成对抗网络（GANs）的背景下，探究了半监督学习（SSL）和开放集识别（OSR）之间的关系。尽管以前没有正式将SSL和OSR联系起来的研究，但它们各自的方法有惊人的相似之处。具体而言，SSL-GAN和OSR-GAN要求生成器在互补空间中产生样本。随后，通过对生成样本进行正则化，SSL和OSR分类器都可以完全识别开放空间。为了证明SSL和OSR之间的关联，我们在理论上和实验上比较了最先进的SSL-GAN方法和最先进的OSR-GAN方法。结果表明，文献基础更加牢固的SSL优化边缘-GAN在结合SSL-OSR任务方面树立新的标准，并在某些一般的OSR实验中取得了新的最先进的结果。然而，OSR优化的对抗性互惠点（ARP）-GAN在一些OSR任务中仍然略优于SSL-GAN。

    This study investigates the relationship between semi-supervised learning (SSL) and open-set recognition (OSR) in the context of generative adversarial networks (GANs). Although no previous study has formally linked SSL and OSR, their respective methods share striking similarities. Specifically, SSL-GANs and OSR-GANs require generator to produce samples in the complementary space. Subsequently, by regularising networks with generated samples, both SSL and OSR classifiers generalize the open space. To demonstrate the connection between SSL and OSR, we theoretically and experimentally compare state-of-the-art SSL-GAN methods with state-of-the-art OSR-GAN methods. Our results indicate that the SSL optimised margin-GANs, which have a stronger foundation in literature, set the new standard for the combined SSL-OSR task and achieves new state-of-other art results in certain general OSR experiments. However, the OSR optimised adversarial reciprocal point (ARP)-GANs still slightly out-performe
    
[^116]: 基于Group Lasso的贪婪剪枝在矩阵感知和二次激活神经网络上可证地泛化

    Greedy Pruning with Group Lasso Provably Generalizes for Matrix Sensing and Neural Networks with Quadratic Activations. (arXiv:2303.11453v1 [cs.LG])

    [http://arxiv.org/abs/2303.11453](http://arxiv.org/abs/2303.11453)

    本文在矩阵感知问题中研究了基于Group Lasso正则化器的贪婪剪枝方法，证明了修剪低$\ell_2$范数列的解可以泛化到新样本上。

    

    剪枝方案广泛用于降低具有大量参数的模型的复杂性。实践研究表明，修剪过度参数化模型并微调可很好地泛化到新样本上。虽然以上被称为剪枝+微调的流程在降低训练模型的复杂性方面非常成功，但其背后的理论仍然不甚了解。本文通过研究超参数化矩阵感知问题上的剪枝+微调框架来解决这个问题，其中真实结果表示为$U_\star \in \mathbb{R}^{d \times r}$，而超参数化模型表示为$U \in \mathbb{R}^{d \times k}$，其中$k \gg r$。我们研究加上Group Lasso正则化器的平滑版本$\sum_{i=1}^k \| U e_i \|_2$的平均误差的近似局部极小值，证明修剪低$\ell_2$范数列的解$U_{

    Pruning schemes have been widely used in practice to reduce the complexity of trained models with a massive number of parameters. Several practical studies have shown that pruning an overparameterized model and fine-tuning generalizes well to new samples. Although the above pipeline, which we refer to as pruning + fine-tuning, has been extremely successful in lowering the complexity of trained models, there is very little known about the theory behind this success. In this paper we address this issue by investigating the pruning + fine-tuning framework on the overparameterized matrix sensing problem, with the ground truth denoted $U_\star \in \mathbb{R}^{d \times r}$ and the overparameterized model $U \in \mathbb{R}^{d \times k}$ with $k \gg r$. We study the approximate local minima of the empirical mean square error, augmented with a smooth version of a group Lasso regularizer, $\sum_{i=1}^k \| U e_i \|_2$ and show that pruning the low $\ell_2$-norm columns results in a solution $U_{\
    
[^117]: 黑盒变分贝叶斯推理的实用匹配梯度方差界限

    Practical and Matching Gradient Variance Bounds for Black-Box Variational Bayesian Inference. (arXiv:2303.10472v1 [cs.LG])

    [http://arxiv.org/abs/2303.10472](http://arxiv.org/abs/2303.10472)

    本文表明黑盒变分推理（BBVI）满足SGD文献中的ABC条件，该结果适用于平滑和二次增长的对数似然函数，同时我们的结果推广到广泛应用于BBVI实践中的非线性协方差参数化。

    

    理解黑盒变分推理（BBVI）的梯度方差是建立其收敛性和算法改进的关键一步。然而，现有研究尚未表明BBVI的梯度方差满足用于研究随机梯度下降（SGD）收敛的条件。在本文中，我们展示了当应用于平滑和二次增长的对数似然函数时，BBVI满足与SGD文献中使用的ABC条件相匹配的界限。我们的结果推广到广泛应用于BBVI实践中的非线性协方差参数化。此外，我们表明，平均场参数化的方差具有经过验证的优越维度依赖性。

    Understanding the gradient variance of black-box variational inference (BBVI) is a crucial step for establishing its convergence and developing algorithmic improvements. However, existing studies have yet to show that the gradient variance of BBVI satisfies the conditions used to study the convergence of stochastic gradient descent (SGD), the workhorse of BBVI. In this work, we show that BBVI satisfies a matching bound corresponding to the $ABC$ condition used in the SGD literature when applied to smooth and quadratically-growing log-likelihoods. Our results generalize to nonlinear covariance parameterizations widely used in the practice of BBVI. Furthermore, we show that the variance of the mean-field parameterization has provably superior dimensional dependence.
    
[^118]: 自动化专利提取支持聚焦化学空间内的生成建模

    Automated patent extraction powers generative modeling in focused chemical spaces. (arXiv:2303.08272v1 [physics.chem-ph])

    [http://arxiv.org/abs/2303.08272](http://arxiv.org/abs/2303.08272)

    本研究通过开发自动化管道，使用专利数据源训练领域特定的生成模型，利用专利中的弱标记应用类别中尽可能多的信息实现化学空间内生成建模。

    

    深度生成模型已成为反向分子设计的一种令人兴奋的手段，其进展来自于训练算法和分子表示之间的相互作用。应用于材料科学和化学领域时，其中一个主要挑战是缺乏具有属性标签的大规模训练数据集。已发布的专利包含在其在期刊上发表之前披露新材料的信息，是一种相对未被充分利用的科学知识广泛来源。由于专利被提交是为了保护特定用途，因此专利中的分子可以被视为弱标记的应用类别。此外，由美国专利与商标局（USPTO）发布的专利具有可下载的机器可读文本和分子结构。在本研究中，我们通过开发自动化管道，使用专利数据源训练领域特定的生成模型。

    Deep generative models have emerged as an exciting avenue for inverse molecular design, with progress coming from the interplay between training algorithms and molecular representations. One of the key challenges in their applicability to materials science and chemistry has been the lack of access to sizeable training datasets with property labels. Published patents contain the first disclosure of new materials prior to their publication in journals, and are a vast source of scientific knowledge that has remained relatively untapped in the field of data-driven molecular design. Because patents are filed seeking to protect specific uses, molecules in patents can be considered to be weakly labeled into application classes. Furthermore, patents published by the US Patent and Trademark Office (USPTO) are downloadable and have machine-readable text and molecular structures. In this work, we train domain-specific generative models using patent data sources by developing an automated pipeline
    
[^119]: 通过 Numerai 数据科学竞赛案例，理解时间表格和多变量时间序列的模型复杂度

    Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament. (arXiv:2303.07925v1 [cs.LG])

    [http://arxiv.org/abs/2303.07925](http://arxiv.org/abs/2303.07925)

    本文采用 Numerai 数据科学竞赛的数据，探究了多变量时间序列建模中不同特征工程和降维方法的应用；提出了一种新的集成方法，用于高维时间序列建模，该方法在通用性、鲁棒性和效率上优于一些深度学习模型。

    

    本文探究了在多变量时间序列建模中使用不同特征工程和降维方法的应用。利用从 Numerai 数据竞赛创建的特征目标交叉相关时间序列数据集，我们证明在过度参数化的情况下，不同特征工程方法的性能与预测会收敛到可由再生核希尔伯特空间刻画的相同平衡态。我们提出了一种新的集成方法，该方法结合了不同的随机非线性变换，随后采用岭回归模型进行高维时间序列建模。与一些常用的用于序列建模的深度学习模型（如 LSTM 和 transformer）相比，我们的方法更加鲁棒（在不同的随机种子下具有较低的模型方差，且对架构的选择不太敏感），并且更有效率。我们方法的另一个优势在于模型的简单性，因为没有必要使用复杂的深度学习框架。

    In this paper, we explore the use of different feature engineering and dimensionality reduction methods in multi-variate time-series modelling. Using a feature-target cross correlation time series dataset created from Numerai tournament, we demonstrate under over-parameterised regime, both the performance and predictions from different feature engineering methods converge to the same equilibrium, which can be characterised by the reproducing kernel Hilbert space. We suggest a new Ensemble method, which combines different random non-linear transforms followed by ridge regression for modelling high dimensional time-series. Compared to some commonly used deep learning models for sequence modelling, such as LSTM and transformers, our method is more robust (lower model variance over different random seeds and less sensitive to the choice of architecture) and more efficient. An additional advantage of our method is model simplicity as there is no need to use sophisticated deep learning frame
    
[^120]: Tactile-Filter: 用于零部件装配的交互式触觉感知

    Tactile-Filter: Interactive Tactile Perception for Part Mating. (arXiv:2303.06034v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.06034](http://arxiv.org/abs/2303.06034)

    本文提出了一种基于视觉的触觉传感器进行零部件装配任务的交互式感知方法，并设计了一个二维粒子滤波器，用于自动搜索新的触觉观察，以最大化其精度。

    

    人类依赖触感和触觉传感器来完成很多巧妙的操作。我们的触觉传感器可以提供关于接触形式以及任何交互中物体的几何信息。出于这个动机，基于视觉的触觉传感器被广泛应用于各种机器人感知和控制任务。本论文提出了一种使用基于视觉的触觉传感器进行零部件装配任务的交互式感知方法，其中机器人可以使用触觉传感器和粒子滤波器的反馈机制，逐步改进其对零部件（销子和孔）的拟合估计，本论文通过训练一个深度神经网络，还设计了一个二维粒子滤波器，来实现机器人自动搜索新的触觉观察，以最大化其精度。

    Humans rely on touch and tactile sensing for a lot of dexterous manipulation tasks. Our tactile sensing provides us with a lot of information regarding contact formations as well as geometric information about objects during any interaction. With this motivation, vision-based tactile sensors are being widely used for various robotic perception and control tasks. In this paper, we present a method for interactive perception using vision-based tactile sensors for a part mating task, where a robot can use tactile sensors and a feedback mechanism using a particle filter to incrementally improve its estimate of objects (pegs and holes) that fit together. To do this, we first train a deep neural network that makes use of tactile images to predict the probabilistic correspondence between arbitrarily shaped objects that fit together. The trained model is used to design a particle filter which is used twofold. First, given one partial (or non-unique) observation of the hole, it incrementally im
    
[^121]: 终身机器学习潜力

    Lifelong Machine Learning Potentials. (arXiv:2303.05911v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.05911](http://arxiv.org/abs/2303.05911)

    该论文提出了一种使用元素包容的中心对称函数来开发终身机器学习潜力，这可以有效且准确地预测新系统，并为其他科学领域的终身机器学习潜力的发展打开了大门。

    

    在准确的量子化学数据上训练的机器学习势(Machine learning potentials, MLPs)可以保持高精度且计算需求小。但缺点是它们需要为每个系统训练。近年来，因为学习附加数据通常需要重新训练所有数据以不忘记先前获得的知识，所以已经从头开始训练了大量的MLP。此外，MLP的大多数常见结构描述符无法有效地表示许多不同的化学元素。在这项工作中，我们通过引入元素包容的中心对称函数 (eeACSFs)，结合来自周期表的结构属性和元素信息，来解决这些问题。这些eeACSFs是我们发展终身机器学习势(lMLP)的关键。不确定性量化可被利用来超越一个固定的、预先训练的MLP，以达到连续适应的lMLP，因为始终会实现预定义的精度水平和增强性能。我们的方法允许有效而准确地预测新系统，并为其他科学领域的终身机器学习潜力的发展打开了大门。

    Machine learning potentials (MLPs) trained on accurate quantum chemical data can retain the high accuracy, while inflicting little computational demands. On the downside, they need to be trained for each individual system. In recent years, a vast number of MLPs has been trained from scratch because learning additional data typically requires to train again on all data to not forget previously acquired knowledge. Additionally, most common structural descriptors of MLPs cannot represent efficiently a large number of different chemical elements. In this work, we tackle these problems by introducing element-embracing atom-centered symmetry functions (eeACSFs) which combine structural properties and element information from the periodic table. These eeACSFs are a key for our development of a lifelong machine learning potential (lMLP). Uncertainty quantification can be exploited to transgress a fixed, pre-trained MLP to arrive at a continuously adapting lMLP, because a predefined level of ac
    
[^122]: 创伤性脑损伤的临床进程对结果的贡献：从欧洲重症监护室数据中挖掘患者轨迹

    Contribution of clinical course to outcome after traumatic brain injury: mining patient trajectories from European intensive care unit data. (arXiv:2303.04630v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04630](http://arxiv.org/abs/2303.04630)

    本研究研发了一种整合患者ICU住院期间所有记录的数据来为每名TBI患者提供可解释的疾病进展的建模策略，并应用递归神经网络模型对患者状况进行预测，以帮助实现个体化治疗。

    

    现有的创伤性脑损伤（TBI）患者状况表述方法不能捕捉个体化治疗所需的上下文信息。本研究旨在开发一种建模策略，整合住院期间所有记录的数据，为每名TBI患者的重症监护室（ICU）住院提供可解释的疾病进展。研究随机选择了1550名来自欧洲65个中心、19个国家的TBI患者，提取了在ICU住院期间或之前收集的所有1166个变量，以及6个月的Glasgow Outcome Scale-Extended（GOSE）功能结果。研究使用递归神经网络模型，将所有变量的令牌嵌入时间序列表示（包括缺失数据）映射到每2个小时的GOSE预后。通过重复交叉验证，我们使用Somers' Dxy评估了GOSE的日常差异的校准和解释。此外，我们应用了TimeSHAP来计算变量及先前时间对每个病例的贡献。

    Existing methods to characterise the evolving condition of traumatic brain injury (TBI) patients in the intensive care unit (ICU) do not capture the context necessary for individualising treatment. We aimed to develop a modelling strategy which integrates all data stored in medical records to produce an interpretable disease course for each TBI patient's ICU stay. From a prospective, European cohort (n=1,550, 65 centres, 19 countries) of TBI patients, we extracted all 1,166 variables collected before or during ICU stay as well as 6-month functional outcome on the Glasgow Outcome Scale-Extended (GOSE). We trained recurrent neural network models to map a token-embedded time series representation of all variables (including missing data) to an ordinal GOSE prognosis every 2 hours. With repeated cross-validation, we evaluated calibration and the explanation of ordinal variance in GOSE with Somers' Dxy. Furthermore, we applied TimeSHAP to calculate the contribution of variables and prior ti
    
[^123]: PINNs的误差收敛性和工程引导超参数搜索：实现I-FENN性能最优化

    Error convergence and engineering-guided hyperparameter search of PINNs: towards optimized I-FENN performance. (arXiv:2303.03918v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03918](http://arxiv.org/abs/2303.03918)

    本文研究如何优化I-FENN性能，通过系统数值方法，探究PINN训练误差和全局误差的收敛行为和超参数-性能关系。

    

    在我们最近提出的I-FENN框架中，我们展示了如何基于有限元级别使用PINNs来快速逼近感兴趣的状态变量，并将其应用于非局部梯度增强损伤力学。本文通过关注其PINN组件的两个关键方面来增强I-FENN的严密性和性能：a）误差收敛性分析 和 b）超参数-性能关系。我们引入了一种基于全面性能指标的新型系统数值方法，以回答这两个目标。在第一个目标中，我们详细探讨了PINN训练误差和全局误差随网络大小和训练样本大小变化的收敛行为。我们证明了任何调查的网络大小和训练样本大小的组合都具有一致的收敛行为。

    In our recently proposed Integrated Finite Element Neural Network (I-FENN) framework (Pantidis and Mobasher, 2023) we showcased how PINNs can be deployed on a finite element-level basis to swiftly approximate a state variable of interest, and we applied it in the context of non-local gradient-enhanced damage mechanics. In this paper, we enhance the rigour and performance of I-FENN by focusing on two crucial aspects of its PINN component: a) the error convergence analysis and b) the hyperparameter-performance relationship. Guided by the available theoretical formulations in the field, we introduce a systematic numerical approach based on a novel set of holistic performance metrics to answer both objectives. In the first objective, we explore in detail the convergence of the PINN training error and the global error against the network size and the training sample size. We demonstrate a consistent converging behavior of the two error types for any investigated combination of network compl
    
[^124]: 非凸优化中的方差缩减裁剪

    Variance-reduced Clipping for Non-convex Optimization. (arXiv:2303.00883v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00883](http://arxiv.org/abs/2303.00883)

    本文提出了一种非凸优化中的方差缩减裁剪方法SPIDER，可以实现在较少的随机梯度计算次数下找到一个较稳定的解决方案。

    

    梯度裁剪是深度学习应用中的标准训练技术，用于减轻梯度爆炸等问题，最近的实验研究表明，当使用梯度裁剪进行训练时，训练目标沿着其轨迹的平滑性具有一种特殊的行为，即平滑性随着梯度范数增长而增长。这与民间非凸优化中广泛流传的$L$-平滑假设形成明显对比，即全局平滑性被假定为由常数$L$上界。最近引入的$(L_0,L_1)$-平滑度是一个更放松的概念，它捕捉到非凸优化中的这种特征。特别是，在这种放松的平滑性假设下，在SGD裁剪的情况下需要$O(\epsilon^{-4})$随机梯度计算才能找到一个$\epsilon$-稳定解。本文采用方差缩减技术SPIDER，并演示如何在理论上分析该方法的性质。

    Gradient clipping is a standard training technique used in deep learning applications such as large-scale language modeling to mitigate exploding gradients. Recent experimental studies have demonstrated a fairly special behavior in the smoothness of the training objective along its trajectory when trained with gradient clipping. That is, the smoothness grows with the gradient norm. This is in clear contrast to the well-established assumption in folklore non-convex optimization, a.k.a. $L$--smoothness, where the smoothness is assumed to be bounded by a constant $L$ globally. The recently introduced $(L_0,L_1)$--smoothness is a more relaxed notion that captures such behavior in non-convex optimization. In particular, it has been shown that under this relaxed smoothness assumption, SGD with clipping requires $O(\epsilon^{-4})$ stochastic gradient computations to find an $\epsilon$--stationary solution. In this paper, we employ a variance reduction technique, namely SPIDER, and demonstrate
    
[^125]: 安全剥离L0正则化最小二乘问题

    Safe Peeling for L0-Regularized Least-Squares with supplementary material. (arXiv:2302.14471v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14471](http://arxiv.org/abs/2302.14471)

    引入“安全剥离”方法加速解决L0正则化最小二乘问题，通过收缩松弛度允许更激进的剪枝，显著降低求解时间。

    

    我们引入了一种新的方法，称为“安全剥离”，通过分支定界算法加速解决L0正则化最小二乘问题。我们的程序使得在BnB决策树的每个节点处考虑到收缩松弛度，因此可能允许更加激进的剪枝。数值模拟表明，我们提出的方法在探索节点数量和整体求解时间方面具有显著的优势。

    We introduce a new methodology dubbed ``safe peeling'' to accelerate the resolution of L0-regularized least-squares problems via a Branch-and-Bound (BnB) algorithm. Our procedure enables to tighten the convex relaxation considered at each node of the BnB decision tree and therefore potentially allows for more aggressive pruning. Numerical simulations show that our proposed methodology leads to significant gains in terms of number of nodes explored and overall solving time.s show that our proposed methodology leads to significant gains in terms of number of nodes explored and overall solving time.
    
[^126]: 对概念瓶颈模型介入程序的更深入研究

    A Closer Look at the Intervention Procedure of Concept Bottleneck Models. (arXiv:2302.14260v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14260](http://arxiv.org/abs/2302.14260)

    本文研究了概念瓶颈模型介入程序的提高介入效果的方法，通过选择介入概念以及深入分析发现，在实际情况下，明智的介入策略可以将任务误差降低十倍以上，而且这个差异可相当明显。

    

    概念瓶颈模型(CBMs)是一类可解释的神经网络模型，基于其高级概念预测给定输入的目标响应。与标准的端到端模型不同，CBMs使领域专家能够在测试时对预测的概念进行干预并纠正任何错误，以便在最终进行更准确的任务预测。尽管这种可干预性提供了一种强大的控制途径，但介入程序的许多方面仍然相当未知。在本研究中，我们开发了各种方式来选择介入概念以提高介入效果，并进行了各种深入的分析，以了解在不同情况下它们的演变方式。具体而言，我们发现在实际情况下，经过明智介入的策略可以将任务误差降低十倍以上，而与当前基线相比，在相同数量的干预次数下，这个差异可以相当明显。

    Concept bottleneck models (CBMs) are a class of interpretable neural network models that predict the target response of a given input based on its high-level concepts. Unlike the standard end-to-end models, CBMs enable domain experts to intervene on the predicted concepts and rectify any mistakes at test time, so that more accurate task predictions can be made at the end. While such intervenability provides a powerful avenue of control, many aspects of the intervention procedure remain rather unexplored. In this work, we develop various ways of selecting intervening concepts to improve the intervention effectiveness and conduct an array of in-depth analyses as to how they evolve under different circumstances. Specifically, we find that an informed intervention strategy can reduce the task error more than ten times compared to the current baseline under the same amount of intervention counts in realistic settings, and yet, this can vary quite significantly when taking into account diffe
    
[^127]: 在结构分布偏移条件下评估图模型的鲁棒性和不确定性

    Evaluating Robustness and Uncertainty of Graph Models Under Structural Distributional Shifts. (arXiv:2302.13875v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13875](http://arxiv.org/abs/2302.13875)

    该论文提出了一种基于图结构的多样化分布转换的方法，并且针对性地设计了数据集。实验结果表明这些分布转换对于现有的图模型具有挑战性。

    

    在基于机器学习的可靠决策系统中，模型必须对分布偏移具有鲁棒性或提供其预测的不确定性。在图学习的节点级问题中，分布偏移可能尤为复杂，因为样本是相互依赖的。为了评估图模型的性能，重要的是在各种有意义的分布偏移下对它们进行测试。然而，大多数考虑节点级分布偏移的图基准主要关注节点特征，而结构属性对图问题也很重要。在这项工作中，我们提出了一种基于图结构引出多样化分布偏移的通用方法。我们使用这种方法根据几个节点的结构属性：流行度、局部性和密度来创建数据分割。在我们的实验中，我们全面评估了所提出的分布偏移，并表明它们对于现有的图模型可能非常具有挑战性。我们还修订了一些关于基准测试图模型的先前工作，并提出了一组新的基准测试，考虑了结构分布偏移条件。

    In reliable decision-making systems based on machine learning, models have to be robust to distributional shifts or provide the uncertainty of their predictions. In node-level problems of graph learning, distributional shifts can be especially complex since the samples are interdependent. To evaluate the performance of graph models, it is important to test them on diverse and meaningful distributional shifts. However, most graph benchmarks considering distributional shifts for node-level problems focus mainly on node features, while structural properties are also essential for graph problems. In this work, we propose a general approach for inducing diverse distributional shifts based on graph structure. We use this approach to create data splits according to several structural node properties: popularity, locality, and density. In our experiments, we thoroughly evaluate the proposed distributional shifts and show that they can be quite challenging for existing graph models. We also rev
    
[^128]: 扩散模型增强的行为克隆

    Diffusion Model-Augmented Behavioral Cloning. (arXiv:2302.13335v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13335](http://arxiv.org/abs/2302.13335)

    本研究提出了一种模仿学习框架，扩散模型增强的行为克隆（DBC），该模型同时建模专家分布的条件和联合概率，有效避免了建模复杂度和推理时间的问题。

    

    模仿学习解决了通过观察专家演示而没有访问环境奖励信号的学习挑战。大多数现有的不需要与环境交互的模仿学习方法，要么将专家分布建模为条件概率p(a|s)（例如，行为克隆，BC），要么将联合概率p(s,a)建模（例如，隐式行为克隆）。尽管行为克隆对于建模条件概率的简单性，但通常难以泛化。虽然对联合概率进行建模可以提高泛化性能，但推理过程可能耗时，并且往往会遭受流形过拟合的问题。本文提出了一个模仿学习框架，它从建模专家分布的条件和联合概率中受益。我们提出的扩散模型增强的行为克隆（DBC）采用训练有素的扩散模型来建模专家行为，并学习一种策略以最大化根据混合概率分布采样的回报。

    Imitation learning addresses the challenge of learning by observing an expert's demonstrations without access to reward signals from environments. Most existing imitation learning methods that do not require interacting with environments either model the expert distribution as the conditional probability p(a|s) (e.g., behavioral cloning, BC) or the joint probability p(s, a) (e.g., implicit behavioral cloning). Despite its simplicity, modeling the conditional probability with BC usually struggles with generalization. While modeling the joint probability can lead to improved generalization performance, the inference procedure can be time-consuming and it often suffers from manifold overfitting. This work proposes an imitation learning framework that benefits from modeling both the conditional and joint probability of the expert distribution. Our proposed diffusion model-augmented behavioral cloning (DBC) employs a diffusion model trained to model expert behaviors and learns a policy to o
    
[^129]: 图神经网络的等变多项式

    Equivariant Polynomials for Graph Neural Networks. (arXiv:2302.11556v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11556](http://arxiv.org/abs/2302.11556)

    本文提出了一种基于等变多项式的表达能力层次结构，可以更好的指导GNN的模型改进。通过定义一个具体的基底，全面刻画了所有等变图多项式。此外，我们设计和分析了新的GNN架构，在多个基准数据集上超越了现有的最先进的模型。

    

    图神经网络(GNN)在其表达能力上存在一定限制。最近的重要工作(Xu等，2019；Morris等，2019b)引入了Weisfeiler-Lehman(WL)层次结构作为表达能力的度量标准。虽然这个层次结构推动了GNN分析和架构发展上的显著进展，但它存在着一些显著的限制。其中包括一个复杂的定义，缺乏指导模型改进的直接指导以及一个过于粗糙无法研究当前GNN的WL层次结构。本文介绍了一种基于GNN能够计算特定次数的等变多项式的表达能力层次结构的替代方法。首先，我们通过引入一个具体的基底，显著推广了以前的结果，提供了所有等变图多项式的全面刻画。每个基底元素对应于一个特定的多图，其在某些图数据输入上的计算对应于一个张量收缩问题。其次，我们利用这些等变多项式来定义新的表达能力度量标准，扩展了WL层次结构。我们的度量标准更易于计算，并提供了更精细的信息，可以指导模型改进。最后，我们通过设计和分析新的GNN架构来证明我们方法的有用性，在多个基准数据集上超越了现有的最先进模型。

    Graph Neural Networks (GNN) are inherently limited in their expressive power. Recent seminal works (Xu et al., 2019; Morris et al., 2019b) introduced the Weisfeiler-Lehman (WL) hierarchy as a measure of expressive power. Although this hierarchy has propelled significant advances in GNN analysis and architecture developments, it suffers from several significant limitations. These include a complex definition that lacks direct guidance for model improvement and a WL hierarchy that is too coarse to study current GNNs. This paper introduces an alternative expressive power hierarchy based on the ability of GNNs to calculate equivariant polynomials of a certain degree. As a first step, we provide a full characterization of all equivariant graph polynomials by introducing a concrete basis, significantly generalizing previous results. Each basis element corresponds to a specific multi-graph, and its computation over some graph data input corresponds to a tensor contraction problem. Second, we 
    
[^130]: 减少、重复利用、回收：基于能量扩散模型和MCMC的组合生成

    Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC. (arXiv:2302.11552v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11552](http://arxiv.org/abs/2302.11552)

    该论文提出了一种基于能量扩散模型和MCMC的组合生成方法，旨在解决现有技术在组合生成中的失败问题，并提出了新的成功的解决方案。

    

    自从扩散模型问世以来，它在许多领域中已经迅速成为生成模型的主要方法。它们可以被解释为学习一系列时变的对数概率密度函数的梯度。这种解释已经激发了基于分类器和无分类器指导的思想成为后续控制扩散模型的方法。在这项工作中，我们建立在这些想法的基础上，利用扩散模型的分数-based解释，探索了用于涉及组合生成和指导的条件、修改和重复使用扩散模型的替代方法。特别是，我们调查了为什么某些类型的组合使用当前技术失败，并介绍了一些解决方案。我们得出结论，采样者(而不是模型)对此失败负有责任，并提出了新的采样器，受MCMC的启发，使组合生成成功。此外，我们提出了一种基于能量的扩散模型参数化方法，它使得逼近目标分布更加容易。

    Since their introduction, diffusion models have quickly become the prevailing approach to generative modeling in many domains. They can be interpreted as learning the gradients of a time-varying sequence of log-probability density functions. This interpretation has motivated classifier-based and classifier-free guidance as methods for post-hoc control of diffusion models. In this work, we build upon these ideas using the score-based interpretation of diffusion models, and explore alternative ways to condition, modify, and reuse diffusion models for tasks involving compositional generation and guidance. In particular, we investigate why certain types of composition fail using current techniques and present a number of solutions. We conclude that the sampler (not the model) is responsible for this failure and propose new samplers, inspired by MCMC, which enable successful compositional generation. Further, we propose an energy-based parameterization of diffusion models which enables the 
    
[^131]: 带有影响力的上下文示例选择

    In-context Example Selection with Influences. (arXiv:2302.11042v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.11042](http://arxiv.org/abs/2302.11042)

    本文提出使用带有影响力的示例选择方法来提高上下文学习(ICL)的性能，分析表明正面和负面示例对ICL取得的性能有高达16.3%的影响，案例研究中也发现了示例排序中的“最近性偏差”现象。

    

    上下文学习(ICL)是从大语言模型(LLM)中出现的一种强大的范例。尽管有着许多有利方面，ICL的性能仍然对输入示例非常敏感。本文提出使用$\textit{上下文影响}$来直接分析少样本ICL的性能。我们提出了基于影响力的示例选择方法，可以识别出正面和负面示例，在9个SuperGLUE任务的评估中优于几种基准线。我们的分析揭示，在使用最正面示例和最负面示例之间，性能差异可高达$16.3\%$。在一个案例研究中，我们将基于影响力的框架应用于量化少样本ICL示例排序中最近性偏差的现象。

    In-context learning (ICL) is a powerful paradigm emerged from large language models (LLMs). Despite its promises, ICL performance is known to be highly sensitive to input examples. In this work, we use $\textit{in-context influences}$ to analyze few-shot ICL performance directly from the in-context examples. Our proposed influence-based example selection method can identify both positive and negative examples, outperforming several baselines when evaluated on 9 SuperGLUE tasks. Our analysis uncovers up to a $16.3\%$ performance gap between using the most negative in-context examples compared to the most positive. In a case study, we apply our influence-based framework to quantify the phenomena of recency bias in example ordering for few-shot ICL.
    
[^132]: 持续同调在图学习中的表达性

    On the Expressivity of Persistent Homology in Graph Learning. (arXiv:2302.09826v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09826](http://arxiv.org/abs/2302.09826)

    本文通过在图学习任务中的理论讨论和实证分析，证明了持续同调技术在捕捉具有显著拓扑结构的数据集中的长程图性质方面表现出的高表达性。

    

    近来，计算拓扑学中的一项技术，持续同调展现出在图分类方面强大的实证性能。它能够通过高阶拓扑特征——如任意长度的环——以及多尺度拓扑描述符捕捉长程图性质，从而提高对具有显著拓扑结构的数据集——如分子——的预测性能。与此同时，持续同调的理论性质在这个背景下尚未得到正式评估。本文旨在通过提供持续同调在图中的简要介绍以及对其在图学习任务中的表达性进行理论讨论和实证分析，弥合计算拓扑学和图机器学习之间的差距。

    Persistent homology, a technique from computational topology, has recently shown strong empirical performance in the context of graph classification. Being able to capture long range graph properties via higher-order topological features, such as cycles of arbitrary length, in combination with multi-scale topological descriptors, has improved predictive performance for data sets with prominent topological structures, such as molecules. At the same time, the theoretical properties of persistent homology have not been formally assessed in this context. This paper intends to bridge the gap between computational topology and graph machine learning by providing a brief introduction to persistent homology in the context of graphs, as well as a theoretical discussion and empirical analysis of its expressivity for graph learning tasks.
    
[^133]: 通过认知风险导向策略优化的高效探索

    Efficient Exploration via Epistemic-Risk-Seeking Policy Optimization. (arXiv:2302.09339v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09339](http://arxiv.org/abs/2302.09339)

    本文提出了一种基于认知风险的新型目标函数，将不确定性转化为价值，鼓励智能体探索未知领域。该方法可以在深度强化学中实现高效探索，即使在函数逼近下也具有保证。

    

    在深度强化学习中，探索仍然是一个关键的挑战。在表格设置中，乐观主义是一种众所周知的启发式方法，具有理论保证，但如何将该原则最好地转化到涉及在线随机梯度和深度网络函数逼近器的深度强化学习中，尚未充分理解。本文提出了一种新的可微乐观目标，当优化时，产生一种可证明有效探索的策略，即使在函数逼近下也具有保证。我们的新目标是一种零和二人博弈，源于赋予代理一个认知风险导向效用函数，将不确定性转化为价值，并鼓励代理人探索不确定状态。我们证明了这个游戏的解决方案最小化了悔恨的一个上界，其中“玩家”各自尝试最小化特定悔恨分解的一个组成部分。我们推导了一种新的无模型算法

    Exploration remains a key challenge in deep reinforcement learning (RL). Optimism in the face of uncertainty is a well-known heuristic with theoretical guarantees in the tabular setting, but how best to translate the principle to deep reinforcement learning, which involves online stochastic gradients and deep network function approximators, is not fully understood. In this paper we propose a new, differentiable optimistic objective that when optimized yields a policy that provably explores efficiently, with guarantees even under function approximation. Our new objective is a zero-sum two-player game derived from endowing the agent with an epistemic-risk-seeking utility function, which converts uncertainty into value and encourages the agent to explore uncertain states. We show that the solution to this game minimizes an upper bound on the regret, with the 'players' each attempting to minimize one component of a particular regret decomposition. We derive a new model-free algorithm which
    
[^134]: MiDi：用于分子生成的混合图和三维去噪扩散

    MiDi: Mixed Graph and 3D Denoising Diffusion for Molecule Generation. (arXiv:2302.09048v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09048](http://arxiv.org/abs/2302.09048)

    MiDi是一种新的扩散模型，可以端到端地生成分子图和原子三维排列，与其他方法相比，MiDi可以更有效地生成稳定的分子。

    

    本文介绍了MiDi，一种新的扩散模型，用于共同生成分子图和相应的原子三维排列。与现有的方法依赖于预定义规则以基于三维构象确定分子键不同，MiDi提供了一个端到端可微分方法来简化分子生成过程。我们的实验结果证明了这种方法的有效性。在具有挑战性的GEOM-DRUGS数据集中，MiDi生成了92％的稳定分子，而先前使用原子间距进行键预测的EDM模型仅生成了6％，而使用EDM后跟直接优化键序以确保有效性的算法仅生成了40％。我们的代码可在github.com/cvignac/MiDi上获得。

    This work introduces MiDi, a novel diffusion model for jointly generating molecular graphs and their corresponding 3D arrangement of atoms. Unlike existing methods that rely on predefined rules to determine molecular bonds based on the 3D conformation, MiDi offers an end-to-end differentiable approach that streamlines the molecule generation process. Our experimental results demonstrate the effectiveness of this approach. On the challenging GEOM-DRUGS dataset, MiDi generates 92% of stable molecules, against 6% for the previous EDM model that uses interatomic distances for bond prediction, and 40% using EDM followed by an algorithm that directly optimize bond orders for validity. Our code is available at github.com/cvignac/MiDi.
    
[^135]: GFlowNet-EM用于学习组合隐变量模型

    GFlowNet-EM for learning compositional latent variable models. (arXiv:2302.06576v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06576](http://arxiv.org/abs/2302.06576)

    GFlowNet-EM采用GFlowNets算法作为建模潜变量后验概率的E步骤，从而实现了对具有离散组合潜变量的表现力强的LVM进行训练。

    

    具有离散组合潜变量的潜变量模型（LVM）是一个重要但具有挑战性的领域，由于潜变量的可能组合数量组合很大。在建模潜变量的后验概率时，表现和可跟踪的优化之间具有关键的权衡。对于基于期望最大化（EM）的算法，E-步骤往往在没有对后验进行限制的近似的情况下是不可跟踪的。我们提出使用GFlowNets，一种学习从未规范化的密度中采样的随机策略以进行顺序样本构建的算法，来处理这个不可跟踪的E-步骤。通过训练GFlowNets从潜变量后验中采样，我们利用了它们作为离散结构复杂分布的变分推理算法的优势。我们的方法，GFlowNet-EM，可以实现对具有离散组合潜变量的表现力强的LVM进行训练，如在非上下文无关文法归纳和图像翻译实验证明。

    Latent variable models (LVMs) with discrete compositional latents are an important but challenging setting due to a combinatorially large number of possible configurations of the latents. A key tradeoff in modeling the posteriors over latents is between expressivity and tractable optimization. For algorithms based on expectation-maximization (EM), the E-step is often intractable without restrictive approximations to the posterior. We propose the use of GFlowNets, algorithms for sampling from an unnormalized density by learning a stochastic policy for sequential construction of samples, for this intractable E-step. By training GFlowNets to sample from the posterior over latents, we take advantage of their strengths as amortized variational inference algorithms for complex distributions over discrete structures. Our approach, GFlowNet-EM, enables the training of expressive LVMs with discrete compositional latents, as shown by experiments on non-context-free grammar induction and on image
    
[^136]: 通过梯度分割实现对异构数据的拜占庭容错学习

    Byzantine-Robust Learning on Heterogeneous Data via Gradient Splitting. (arXiv:2302.06079v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06079](http://arxiv.org/abs/2302.06079)

    这篇论文提出了一种缓解目前健壮算法在非IID环境下表现下降的方法，名为GAS，该方法能够成功将现有的健壮算法用于非IID的数据，并且在真实数据集上有效。

    

    联邦学习对拜占庭攻击具有 vulnerabilities，即攻击者可以向中央服务器发送任意梯度以破坏全局模型的收敛和性能。一些健壮的聚合规则（AGRs）已被提出来以抵御对抗拜占庭攻击。但是，当数据不服从独立同分布（non-IID）时，拜占庭客户端仍然可以规避健壮的 AGRs。本文首先揭示了当前健壮 AGRs 在非IID环境下表现下降的根本原因：维度灾难和梯度异质性。为了解决这个问题，我们提出了 GAS，一种缩短方法，可以成功地将现有的健壮 AGRs 适应于非IID环境。当现有健壮 AGRs 与 GAS 组合时，我们还提供了详细的收敛分析。各种真实数据集上的实验证明了我们提出的 GAS 的有效性。实现代码可在 https://github.com/Y 中找到。

    Federated learning has exhibited vulnerabilities to Byzantine attacks, where the Byzantine attackers can send arbitrary gradients to a central server to destroy the convergence and performance of the global model. A wealth of robust AGgregation Rules (AGRs) have been proposed to defend against Byzantine attacks. However, Byzantine clients can still circumvent robust AGRs when data is non-Identically and Independently Distributed (non-IID). In this paper, we first reveal the root causes of performance degradation of current robust AGRs in non-IID settings: the curse of dimensionality and gradient heterogeneity. In order to address this issue, we propose GAS, a \shorten approach that can successfully adapt existing robust AGRs to non-IID settings. We also provide a detailed convergence analysis when the existing robust AGRs are combined with GAS. Experiments on various real-world datasets verify the efficacy of our proposed GAS. The implementation code is provided in https://github.com/Y
    
[^137]: 可控性感知的无监督技能发现

    Controllability-Aware Unsupervised Skill Discovery. (arXiv:2302.05103v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2302.05103](http://arxiv.org/abs/2302.05103)

    本文提出了一种新的可控性感知的无监督技能发现方法，通过联合训练的距离函数降低简单易实现技能的奖励，逐步学习更具挑战性的技能。

    

    智能代理的关键能力之一是在没有外部监督的情况下发现有用的技能。然而，目前的无监督技能发现方法往往只能获得简单、易学的技能，因为缺乏发现更复杂、有挑战性行为的动机。我们引入了一种新的无监督技能发现方法，可控性感知技能发现（CSD），它可以在没有监督的情况下主动寻找复杂、难以控制的技能。CSD的关键组成部分是可控性感知距离函数，它给当前技能实现更难的状态转换分配更大的值。与距离最大化的技能发现结合起来，CSD在训练过程中逐步学习更具挑战性的技能，因为我们联合训练的距离函数降低了简单易实现技能的奖励。我们在六个机器人操作和运动环境中的实验结果表明，CSD可以发现各种不同的复杂技能，胜过现有的无监督技能发现方法。

    One of the key capabilities of intelligent agents is the ability to discover useful skills without external supervision. However, the current unsupervised skill discovery methods are often limited to acquiring simple, easy-to-learn skills due to the lack of incentives to discover more complex, challenging behaviors. We introduce a novel unsupervised skill discovery method, Controllability-aware Skill Discovery (CSD), which actively seeks complex, hard-to-control skills without supervision. The key component of CSD is a controllability-aware distance function, which assigns larger values to state transitions that are harder to achieve with the current skills. Combined with distance-maximizing skill discovery, CSD progressively learns more challenging skills over the course of training as our jointly trained distance function reduces rewards for easy-to-achieve skills. Our experimental results in six robotic manipulation and locomotion environments demonstrate that CSD can discover diver
    
[^138]: 零样本协同合作学习框架的合作开放式学习

    Cooperative Open-ended Learning Framework for Zero-shot Coordination. (arXiv:2302.04831v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.04831](http://arxiv.org/abs/2302.04831)

    该论文提出了一个COLE框架，通过构建合作游戏的开放式目标，从图论的角度评估和确定每个策略的协作能力，以有效地解决零样本协调中的合作不兼容性问题。

    

    协作人工智能中的零样本协调仍然是一个重大挑战，有效地协调一系列看不见的合作伙伴。先前的算法试图通过优化种群中的固定目标来改善策略或行为的多样性来解决这一挑战。然而，这些方法可能导致学习损失和与种群中某些策略无法合作，即合作不兼容性。为了解决这个问题，我们提出了合作开放式学习（COLE）框架，该框架从图论的角度构建了协作游戏的开放式目标，以评估和确定每个策略的协作能力。我们进一步明确了框架并提出了一种实用的算法，该算法利用了博弈论和图论的知识。此外，对算法的学习过程进行的分析显示，它可以有效地克服学习困难。

    Zero-shot coordination in cooperative artificial intelligence (AI) remains a significant challenge, which means effectively coordinating with a wide range of unseen partners. Previous algorithms have attempted to address this challenge by optimizing fixed objectives within a population to improve strategy or behaviour diversity. However, these approaches can result in a loss of learning and an inability to cooperate with certain strategies within the population, known as cooperative incompatibility. To address this issue, we propose the Cooperative Open-ended LEarning (COLE) framework, which constructs open-ended objectives in cooperative games with two players from the perspective of graph theory to assess and identify the cooperative ability of each strategy. We further specify the framework and propose a practical algorithm that leverages knowledge from game theory and graph theory. Furthermore, an analysis of the learning process of the algorithm shows that it can efficiently overc
    
[^139]: 可预测的MDP抽象用于无监督的基于模型的强化学习

    Predictable MDP Abstraction for Unsupervised Model-Based RL. (arXiv:2302.03921v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03921](http://arxiv.org/abs/2302.03921)

    该论文提出了可预测的MDP抽象方法，通过无监督学习将原始MDP转换为学习行动空间，使模型学习变得更加准确和稳定，在多项任务上得到了验证。

    

    模型化强化学习（RL）的一个关键组件是一个能预测行动结果的动态模型。预测模型中的错误会降低模型化控制器的性能，复杂的马尔可夫决策过程（MDPs）可能会带来极其困难的预测问题。为了缓解这个问题，我们提出了可预测的MDP抽象（PMA）：不是在原始MDP上训练预测模型，而是在一个转换后的具有学习行动空间的MDP上训练模型，该行动空间只允许可预测、易建模的行动，同时尽可能地覆盖原始状态-行动空间。结果是，模型学习变得更加容易和准确，这允许鲁棒、稳定的基于模型的规划或基于模型的RL。这种转换是以无监督的方式学习的，在用户指定任何任务之前。随后，下游任务可以以零-shot方式通过模型化控制解决，而无需额外的环境交互。我们从理论上证明了方法的有效性，并在多个任务上进行了实验验证。

    A key component of model-based reinforcement learning (RL) is a dynamics model that predicts the outcomes of actions. Errors in this predictive model can degrade the performance of model-based controllers, and complex Markov decision processes (MDPs) can present exceptionally difficult prediction problems. To mitigate this issue, we propose predictable MDP abstraction (PMA): instead of training a predictive model on the original MDP, we train a model on a transformed MDP with a learned action space that only permits predictable, easy-to-model actions, while covering the original state-action space as much as possible. As a result, model learning becomes easier and more accurate, which allows robust, stable model-based planning or model-based RL. This transformation is learned in an unsupervised manner, before any task is specified by the user. Downstream tasks can then be solved with model-based control in a zero-shot fashion, without additional environment interactions. We theoretical
    
[^140]: 基于采样的通用推断后验估计精度测试

    Sampling-Based Accuracy Testing of Posterior Estimators for General Inference. (arXiv:2302.03026v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.03026](http://arxiv.org/abs/2302.03026)

    本文提出了一种基于“随机点精度测试”（TARP）覆盖测试的方法来估计生成后验估计器覆盖概率。该方法是一种估计生成模型中编码后验精度的必要和充分条件。该方法可用于测试高维空间中后验推断分析的结果。

    

    参数推断是许多科学学科中的一个核心问题，即在给定一些数据的情况下推断统计模型参数的后验分布。生成模型可用作马尔可夫链蒙特卡罗方法的替代方法，用于进行基于似然和基于模拟的后验推断问题。然而，评估生成模型中编码的后验精度并不简单。本文引入了“随机点精度测试”（TARP）覆盖测试作为一种估计生成后验估计器覆盖概率的方法。我们的方法不同于以前存在的需要后验评估的基于覆盖率的方法。我们证明了我们的方法是确定后验估计器准确性的必要和充分条件。我们在各种合成示例上演示了该方法，并表明TARP可用于测试高维空间中后验推断分析的结果。

    Parameter inference, i.e. inferring the posterior distribution of the parameters of a statistical model given some data, is a central problem to many scientific disciplines. Generative models can be used as an alternative to Markov Chain Monte Carlo methods for conducting posterior inference, both in likelihood-based and simulation-based problems. However, assessing the accuracy of posteriors encoded in generative models is not straightforward. In this paper, we introduce `Tests of Accuracy with Random Points' (TARP) coverage testing as a method to estimate coverage probabilities of generative posterior estimators. Our method differs from previously-existing coverage-based methods, which require posterior evaluations. We prove that our approach is necessary and sufficient to show that a posterior estimator is accurate. We demonstrate the method on a variety of synthetic examples, and show that TARP can be used to test the results of posterior inference analyses in high-dimensional spac
    
[^141]: RLSbench: 宽松标签偏移下的领域自适应

    RLSbench: Domain Adaptation Under Relaxed Label Shift. (arXiv:2302.03020v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03020](http://arxiv.org/abs/2302.03020)

    本文介绍了 RLSbench，它是一个大规模基准，用于宽松标签偏移。与现有基准不同，它旨在评估领域自适应方法在标签边际偏移下的表现。

    

    尽管出现了解决标签偏移下领域自适应的原则性方法，但对于类条件分布的偏移敏感性却未得到充分探索。同时，流行的深度领域自适应启发式方法在面对标签比例偏移时往往疲软。虽然有几篇论文改进了这些启发方法以尝试处理标签比例偏移，但评估标准、数据集和基线的不一致使得评估当前最佳实践变得困难。在这篇论文中，我们引入 RLSbench，一个大规模的宽松标签偏移基准，涵盖500多个分布偏移对，跨视觉、表格和语言模式，具有不同的标签比例。与现有基准主要关注类条件$p(x|y)$偏移不同，我们的基准还关注标签边际偏移。首先，我们评估了13种流行的领域自适应方法，证明在标签比例偏移下更普遍地失败。

    Despite the emergence of principled methods for domain adaptation under label shift, their sensitivity to shifts in class conditional distributions is precariously under explored. Meanwhile, popular deep domain adaptation heuristics tend to falter when faced with label proportions shifts. While several papers modify these heuristics in attempts to handle label proportions shifts, inconsistencies in evaluation standards, datasets, and baselines make it difficult to gauge the current best practices. In this paper, we introduce RLSbench, a large-scale benchmark for relaxed label shift, consisting of $>$500 distribution shift pairs spanning vision, tabular, and language modalities, with varying label proportions. Unlike existing benchmarks, which primarily focus on shifts in class-conditional $p(x|y)$, our benchmark also focuses on label marginal shifts. First, we assess 13 popular domain adaptation methods, demonstrating more widespread failures under label proportion shifts than were pre
    
[^142]: 重新思考高斯-牛顿方法在过参数模型学习中的应用

    Rethinking Gauss-Newton for learning over-parameterized models. (arXiv:2302.02904v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02904](http://arxiv.org/abs/2302.02904)

    本研究重新思考了在过参数模型中使用高斯-牛顿法的应用，通过实证研究发现，虽然GN在找到全局最优解方面比GD更快，但学习率和随机初始化网络权重方差对模型泛化性能影响很大，更小的方差初始化能够获得更好的泛化性能，而与GD不同的是，GN在实现更好的泛化方面使用更小的学习率能够取得成效。

    

    本研究探讨了在过参数化模型中，使用高斯-牛顿法（GN）对一层隐藏层网络进行优化时的全局收敛和泛化特性。我们首先在连续时间极限下确定了GN的全局收敛结果，由于改善了条件，其收敛速度比梯度下降（GD）更快。然后，我们在合成回归任务中进行了实证研究，以调查GN方法的隐式偏差。我们发现，虽然GN始终比GD更快地找到全局最优解，但学习模型在测试数据集上的表现受到学习率和随机初始化网络权重方差的影响。具体而言，我们发现使用更小的方差初始化结果会获得更好的泛化，这也是GD的一种行为。然而，与GD不同的是，我们发现使用更小的学习率可以使GN在实现更好的泛化方面取得成效。

    This work studies the global convergence and generalization properties of Gauss Newton's (GN) when optimizing one-hidden layer networks in the over-parameterized regime. We first establish a global convergence result for GN in the continuous-time limit exhibiting a faster convergence rate compared to GD due to improved conditioning. We then perform an empirical study on a synthetic regression task to investigate the implicit bias of GN's method. We find that, while GN is consistently faster than GD in finding a global optimum, the performance of the learned model on a test dataset is heavily influenced by both the learning rate and the variance of the randomly initialized network's weights. Specifically, we find that initializing with a smaller variance results in a better generalization, a behavior also observed for GD. However, in contrast to GD where larger learning rates lead to the best generalization, we find that GN achieves an improved generalization when using smaller learning
    
[^143]: 重新考虑个性化联邦学习：对抗后门攻击的鲁棒性

    Revisiting Personalized Federated Learning: Robustness Against Backdoor Attacks. (arXiv:2302.01677v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01677](http://arxiv.org/abs/2302.01677)

    研究展示了部分模型共享的个性化联邦学习方法可以显著提高对抗后门攻击的鲁棒性，提出了一个轻量级的防御方法Simple-Tuning，可用于提高对抗后门攻击的防御性能。

    

    本文研究个性化能否提高对抗后门攻击的鲁棒性，并在FEMNIST和CIFAR-10这两个基准数据集上对6种pFL方法进行了4种后门攻击的测试，进行了600次实验。结果表明，具有部分模型共享的pFL方法可以显著提高抵御后门攻击的鲁棒性。与此相反，具有完全模型共享的pFL方法并不表现出鲁棒性。我们还提供了不同pFL方法的全面剖析研究，以分析鲁棒性表现差异的原因。基于研究结果，我们进一步提出了一种轻量级的防御方法Simple-Tuning，这种方法可以在经验上提高对抗后门攻击的防御性能。我们认为，本研究提供了有关pFL在鲁棒性方面的应用指导，并为设计更可靠的FL方法提供了有价值的见解。

    In this work, besides improving prediction accuracy, we study whether personalization could bring robustness benefits to backdoor attacks. We conduct the first study of backdoor attacks in the pFL framework, testing 4 widely used backdoor attacks against 6 pFL methods on benchmark datasets FEMNIST and CIFAR-10, a total of 600 experiments. The study shows that pFL methods with partial model-sharing can significantly boost robustness against backdoor attacks. In contrast, pFL methods with full model-sharing do not show robustness. To analyze the reasons for varying robustness performances, we provide comprehensive ablation studies on different pFL methods. Based on our findings, we further propose a lightweight defense method, Simple-Tuning, which empirically improves defense performance against backdoor attacks. We believe that our work could provide both guidance for pFL application in terms of its robustness and offer valuable insights to design more robust FL methods in the future. W
    
[^144]: ANTM: 一种对齐的神经主题模型，用于探索演变的主题

    ANTM: An Aligned Neural Topic Model for Exploring Evolving Topics. (arXiv:2302.01501v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2302.01501](http://arxiv.org/abs/2302.01501)

    ANTM是一种对齐的神经主题模型，它利用重叠滑动窗口算法来维护演变主题的时间连续性，并通过对语义相似的文档进行对齐来捕捉出现和消退的趋势。实验证明ANTM在主题连贯性和多样性方面优于传统动态主题模型。

    

    本文提出了一种称为对齐神经主题模型（ANTM）的动态主题模型算法家族，它结合了新颖的数据挖掘算法，提供了一个模块化框架，用于发现演变的主题。ANTM利用先进的预训练大型语言模型从文档中提取时间感知特征，并使用重叠滑动窗口算法进行顺序文档聚类，从而维护了演变主题的时间连续性。这种重叠滑动窗口算法在每个时间框架内标识不同数量的主题，并在时间段内对语义相似的文档聚类进行对齐。这个过程捕捉了不同时期出现和消退的趋势，并允许更具可解释性的演变主题表示。针对四个不同数据集的实验表明，ANTM在主题连贯性和多样性指标方面优于概率动态主题模型。此外，它改善了动态主题建模的可扩展性和灵活性。

    This paper presents an algorithmic family of dynamic topic models called Aligned Neural Topic Models (ANTM), which combine novel data mining algorithms to provide a modular framework for discovering evolving topics. ANTM maintains the temporal continuity of evolving topics by extracting time-aware features from documents using advanced pre-trained Large Language Models (LLMs) and employing an overlapping sliding window algorithm for sequential document clustering. This overlapping sliding window algorithm identifies a different number of topics within each time frame and aligns semantically similar document clusters across time periods. This process captures emerging and fading trends across different periods and allows for a more interpretable representation of evolving topics. Experiments on four distinct datasets show that ANTM outperforms probabilistic dynamic topic models in terms of topic coherence and diversity metrics. Moreover, it improves the scalability and flexibility of dy
    
[^145]: 针对长尾识别的原型分类器学习

    Learning Prototype Classifiers for Long-Tailed Recognition. (arXiv:2302.00491v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.00491](http://arxiv.org/abs/2302.00491)

    本文介绍了针对长尾识别的原型分类器学习，通过联合学习原型以最小化基于概率分数与原型之间距离的平均交叉熵损失，并通过引入一种新的方法，自适应平衡来自不同类别的损失的重要性，进一步增强了原型分类器，从而实现在几个基准数据集上的竞争性性能。

    

    近年来，由于现实世界中物体的幂律分布，长尾识别(LTR)问题受到了关注。LTR中大多数最新的工作使用softmax分类器，其具有将分类器范数与给定类别的训练数据量相关联的倾向。另一方面，原型分类器不受这种缺点的困扰，并且只使用最近类平均值（NCM）即可交付有前途的结果，其中原型是经验质心。然而，在LTR中，原型分类器作为softmax的替代方法的潜力相对较少被探索。在这项工作中，我们提出了原型分类器，该分类器联合学习原型，以最小化基于概率分数与原型之间距离的平均交叉熵损失。我们从理论上分析了基于欧几里德距离的原型分类器的性质，这导致了稳定的基于梯度的优化，对异常值具有鲁棒性。我们通过引入一种新的方法，自适应平衡来自不同类别的损失的重要性，进一步增强了原型分类器。对几个基准数据集的实验证明，我们提出的原型分类器在LTR上实现了有竞争力的性能，在几种最新方法中表现出色。

    The problem of long-tailed recognition (LTR) has received attention in recent years due to the fundamental power-law distribution of objects in the real-world. Most recent works in LTR use softmax classifiers that have a tendency to correlate classifier norm with the amount of training data for a given class. On the other hand, Prototype classifiers do not suffer from this shortcoming and can deliver promising results simply using Nearest-Class-Mean (NCM), a special case where prototypes are empirical centroids. However, the potential of Prototype classifiers as an alternative to softmax in LTR is relatively underexplored. In this work, we propose Prototype classifiers, which jointly learn prototypes that minimize average cross-entropy loss based on probability scores from distances to prototypes. We theoretically analyze the properties of Euclidean distance based prototype classifiers that leads to stable gradient-based optimization which is robust to outliers. We further enhance Prot
    
[^146]: 对称生成对抗性发现

    Generative Adversarial Symmetry Discovery. (arXiv:2302.00236v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00236](http://arxiv.org/abs/2302.00236)

    使用LieGAN框架，可以自动发现数据集中的等变性，从而提高预测的准确性和泛化能力。

    

    尽管等变神经网络在科学应用中取得了成功，但它们需要事先知道对称群。然而在实践中，知道要使用哪个对称群作为归纳偏差可能很困难，错误地强制使用对称群甚至可能会损害性能。本文提出了一个LieGAN框架，通过类似生成对抗性训练的范式自动从数据集中发现等变性。生成器学习一组应用于数据的变换，这些变换保持原始分布并欺骗鉴别器。LieGAN使用可解释的李代数基表示对称性，并且可以在轨迹预测和顶夸克标记任务中发现各种对称性，例如旋转群$\mathrm{SO}(n)$，限制Lorentz群$\mathrm{SO}(1,3)^+$。所学习的对称性也可以轻松地应用于几个现有的等变神经网络，以提高预测的准确性和泛化能力。

    Despite the success of equivariant neural networks in scientific applications, they require knowing the symmetry group a priori. However, it may be difficult to know which symmetry to use as an inductive bias in practice. Enforcing the wrong symmetry could even hurt the performance. In this paper, we propose a framework, LieGAN, to automatically discover equivariances from a dataset using a paradigm akin to generative adversarial training. Specifically, a generator learns a group of transformations applied to the data, which preserve the original distribution and fool the discriminator. LieGAN represents symmetry as interpretable Lie algebra basis and can discover various symmetries such as the rotation group $\mathrm{SO}(n)$, restricted Lorentz group $\mathrm{SO}(1,3)^+$ in trajectory prediction and top-quark tagging tasks. The learned symmetry can also be readily used in several existing equivariant neural networks to improve accuracy and generalization in prediction.
    
[^147]: 辅助学习作为一种非对称博弈的方法

    Auxiliary Learning as an Asymmetric Bargaining Game. (arXiv:2301.13501v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13501](http://arxiv.org/abs/2301.13501)

    本研究提出了一种名为AuxiNash的辅助学习方法，将问题形式化为广义议价博弈，并通过非对称任务谈判能力平衡辅助任务，有效地提高训练模型泛化能力，证明了其在多个多任务基准测试中具有更好的性能。

    

    辅助学习是提高训练模型泛化能力的有效方法，特别是在处理小数据集时。然而，这种方法可能会遇到一些困难：（i）优化多个目标可能更具挑战性，（ii）如何平衡辅助任务以最好地帮助主要任务尚不清楚。在本研究中，我们提出了一种新的方法，名为AuxiNash，通过将问题形式化为具有非对称任务谈判能力的广义议价博弈来平衡辅助学习中的任务。此外，我们描述了一种基于任务对主要任务性能的贡献来学习任务谈判能力的有效程序，并为其收敛提供了理论保证。最后，我们在多个多任务基准测试上评估了AuxiNash，并发现它始终优于竞争方法。

    Auxiliary learning is an effective method for enhancing the generalization capabilities of trained models, particularly when dealing with small datasets. However, this approach may present several difficulties: (i) optimizing multiple objectives can be more challenging, and (ii) how to balance the auxiliary tasks to best assist the main task is unclear. In this work, we propose a novel approach, named AuxiNash, for balancing tasks in auxiliary learning by formalizing the problem as generalized bargaining game with asymmetric task bargaining power. Furthermore, we describe an efficient procedure for learning the bargaining power of tasks based on their contribution to the performance of the main task and derive theoretical guarantees for its convergence. Finally, we evaluate AuxiNash on multiple multi-task benchmarks and find that it consistently outperforms competing methods.
    
[^148]: 弹性输入序列的自适应计算

    Adaptive Computation with Elastic Input Sequence. (arXiv:2301.13195v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13195](http://arxiv.org/abs/2301.13195)

    本文介绍了一种名为AdaTape的新方法，通过自适应磁带符号，允许神经网络进行动态计算，能够实现适应不同类型信息的自适应计算，在图像分类、语言建模和程序综合等多个任务中都表现出更好的性能。

    

    人类有能力在解决问题时适应不同类型的信息、不同的处理方法和不同的时间花费。然而，大部分标准神经网络无论样本的性质或难度都有固定的函数类型和计算预算。自适应计算是一种强大的范式，因为它不仅赋予从业者灵活性，而且还可以作为解决某些具有挑战性的问题的强大归纳偏差。在本文中，我们介绍了一种名为AdaTape的新方法，通过自适应磁带符号，允许神经网络进行动态计算。AdaTape利用弹性输入序列，通过装备带有动态读写磁带的架构来实现。具体来说，我们使用来自磁带库的磁带符号来自适应生成输入序列，这些符号可训练或从输入数据中派生。我们研究了获得动态序列计算所需的挑战和要求，以及AdaTape的性质。我们的实验表明，AdaTape能够学习自适应计算策略，从而在几个基准任务（包括图像分类、语言建模和程序综合）上实现更好的性能。

    Humans have the ability to adapt the type of information they use, the procedure they employ, and the amount of time they spend when solving problems. However, most standard neural networks have a fixed function type and computation budget regardless of the sample's nature or difficulty. Adaptivity is a powerful paradigm as it not only imbues practitioners with flexibility pertaining to the downstream usage of these models but can also serve as a powerful inductive bias for solving certain challenging classes of problems. In this work, we introduce a new approach called AdaTape, which allows for dynamic computation in neural networks through adaptive tape tokens. AdaTape utilizes an elastic input sequence by equipping an architecture with a dynamic read-and-write tape. Specifically, we adaptively generate input sequences using tape tokens obtained from a tape bank which can be either trainable or derived from input data. We examine the challenges and requirements to obtain dynamic sequ
    
[^149]: 基于多割的全连接图聚类

    ClusterFuG: Clustering Fully connected Graphs by Multicut. (arXiv:2301.12159v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.12159](http://arxiv.org/abs/2301.12159)

    本文提出了一种基于多割的全连接图聚类方法，通过内积的形式表示聚类目标，使得算法更高效，并在ImageNet和CIFAR数据集上展现出了优异的性能。

    

    本文提出了一种基于多割（Multicut，又称加权相关聚类）的图聚类方法，该方法不需要像原始的稀疏多割方法一样指定图的拓扑结构，使得我们的方法更加简单，性能也有可能更好。与非加权相关聚类不同，我们允许更具表现力的加权成本结构。在密集多割中，聚类目标以节点特征向量的内积的形式表示，这使得任务的表示和推理相对于多割/加权相关聚类更加高效，后者在完整图上的表示和计算复杂度至少是二次的。我们展示了如何在我们的密集设置下重写经典贪心算法的多割方法，并如何修改它们以提高效率和解决方案质量。特别地，我们的算法可扩展到拥有数万个节点的图中。在ImageNet和CIFAR数据集上的实证证明了我们的方法优于或与现有方法不相上下，同时计算效率更高。

    We propose a graph clustering formulation based on multicut (a.k.a. weighted correlation clustering) on the complete graph. Our formulation does not need specification of the graph topology as in the original sparse formulation of multicut, making our approach simpler and potentially better performing. In contrast to unweighted correlation clustering we allow for a more expressive weighted cost structure. In dense multicut, the clustering objective is given in a factorized form as inner products of node feature vectors. This allows for an efficient formulation and inference in contrast to multicut/weighted correlation clustering, which has at least quadratic representation and computation complexity when working on the complete graph. We show how to rewrite classical greedy algorithms for multicut in our dense setting and how to modify them for greater efficiency and solution quality. In particular, our algorithms scale to graphs with tens of thousands of nodes. Empirical evidence on i
    
[^150]: 与人类表征的一致性支持鲁棒的少样本学习

    Alignment with human representations supports robust few-shot learning. (arXiv:2301.11990v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11990](http://arxiv.org/abs/2301.11990)

    论文提出少样本学习的表现与人类表征的一致性存在U形关系，并通过计算机视觉模型的实验进行了验证。高度对齐的模型更加鲁棒，对数据的利用更加有效，但与人类对齐并非必要条件。

    

    我们是否应该关心AI系统是否具有与人类相似的世界表征？我们提供了一个信息论分析，建议在少样本学习任务的表现度与人类表征的一致性之间应该存在一个U形关系。我们通过对491个计算机视觉模型的性能分析验证了这个预测的可行性，并且表明高度对齐的模型更加鲁棒于对抗攻击和域偏移。我们的结果表明，与人类对齐往往是模型有效利用有限数据、鲁棒性 以及泛化能力的充分但不必要条件。

    Should we care whether AI systems have representations of the world that are similar to those of humans? We provide an information-theoretic analysis that suggests that there should be a U-shaped relationship between the degree of representational alignment with humans and performance on few-shot learning tasks. We confirm this prediction empirically, finding such a relationship in an analysis of the performance of 491 computer vision models. We also show that highly-aligned models are more robust to both adversarial attacks and domain shifts. Our results suggest that human-alignment is often a sufficient, but not necessary, condition for models to make effective use of limited data, be robust, and generalize well.
    
[^151]: 利用子采样实现实用的差分隐私超参数调整

    Practical Differentially Private Hyperparameter Tuning with Subsampling. (arXiv:2301.11989v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11989](http://arxiv.org/abs/2301.11989)

    本文提出了一种利用子采样实现实用的差分隐私超参数调整的方法，相比基准算法，可以在不损失最终评估结果的情况下提高隐私-实用性权衡。

    

    调整差分隐私机器学习算法的超参数通常需要使用敏感数据，这可能通过超参数值泄漏私人信息。本文旨在通过仅使用敏感数据的随机子集并将最佳值外推到较大的数据集来降低这些方法的差分隐私界限和计算成本。我们提供了对该方法的 Renyi 差分隐私分析，并证明它在不损失所选超参数的最终评估准确性的情况下，可以始终实现更好的隐私-实用性权衡。

    Tuning the hyperparameters of differentially private (DP) machine learning (ML) algorithms often requires use of sensitive data and this may leak private information via hyperparameter values. Recently, Papernot and Steinke (2022) proposed a certain class of DP hyperparameter tuning algorithms, where the number of random search samples is randomized itself. Commonly, these algorithms still considerably increase the DP privacy parameter $\varepsilon$ over non-tuned DP ML model training and can be computationally heavy as evaluating each hyperparameter candidate requires a new training run. We focus on lowering both the DP bounds and the computational cost of these methods by using only a random subset of the sensitive data for the hyperparameter tuning and by extrapolating the optimal values to a larger dataset. We provide a R\'enyi differential privacy analysis for the proposed method and experimentally show that it consistently leads to better privacy-utility trade-off than the baseli
    
[^152]: 带增益调节的神经元种群自适应白化

    Adaptive whitening in neural populations with gain-modulating interneurons. (arXiv:2301.11955v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2301.11955](http://arxiv.org/abs/2301.11955)

    该论文提出了一种基于增益调节的自适应白化神经电路模型，通过调节个体神经元的增益来适应地白化其响应，从而增加了网络对病态输入的鲁棒性。

    

    在许多计算系统中，统计白化变换起着基础作用，也可能在生物感觉系统中发挥重要作用。现有的自适应白化神经电路模型通过修改突触相互作用来操作；然而，这种修改似乎既太慢又不够可逆。受到增益调节的广泛神经科学文献的启发，我们提出了一种替代模型，通过调节个体神经元的增益来适应地白化其响应。基于一种新的白化目标，我们推导了一种在线算法，通过调整过完备的投影的边际方差来白化其输出。我们将算法映射到一个具有固定突触权重和增益调节中间神经元的递归神经网络上。我们证明了增益的符号约束提高了网络对病态输入的鲁棒性，电路的一种泛化形式实现了一种本地的

    Statistical whitening transformations play a fundamental role in many computational systems, and may also play an important role in biological sensory systems. Existing neural circuit models of adaptive whitening operate by modifying synaptic interactions; however, such modifications would seem both too slow and insufficiently reversible. Motivated by the extensive neuroscience literature on gain modulation, we propose an alternative model that adaptively whitens its responses by modulating the gains of individual neurons. Starting from a novel whitening objective, we derive an online algorithm that whitens its outputs by adjusting the marginal variances of an overcomplete set of projections. We map the algorithm onto a recurrent neural network with fixed synaptic weights and gain-modulating interneurons. We demonstrate numerically that sign-constraining the gains improves robustness of the network to ill-conditioned inputs, and a generalization of the circuit achieves a form of local 
    
[^153]: 基于CTC和最优传输的语音翻译预训练方法

    Pre-training for Speech Translation: CTC Meets Optimal Transport. (arXiv:2301.11716v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.11716](http://arxiv.org/abs/2301.11716)

    本文提出了一种基于CTC和最优传输的语音翻译预训练方法，可以有效减小语音和文本模态之间的差距，提高最终的ST准确性。

    

    语音到文本翻译(ST)中的模态差距是一个重要挑战，该文提出了一种预训练方法来减轻这个问题，无需改变ST模型的架构。首先，本文表明连接时序分类(CTC)损失可以通过设计来减小模态差距。通过与更常见的交叉熵损失的定量比较，我们证明了使用CTC进行预训练可以始终实现更好的最终ST准确性。其次，我们提出了一种结合CTC和最优传输的新型预训练方法以进一步减小这种差距。我们的实验证明了使用CTC和最优传输进行预训练相对于仅使用CTC进行预训练和没有进行预训练的基线模型均能够提供持续改进。

    The gap between speech and text modalities is a major challenge in speech-to-text translation (ST). Different methods have been proposed to reduce this gap, but most of them require architectural changes in ST training. In this work, we propose to mitigate this issue at the pre-training stage, requiring no change in the ST model. First, we show that the connectionist temporal classification (CTC) loss can reduce the modality gap by design. We provide a quantitative comparison with the more common cross-entropy loss, showing that pre-training with CTC consistently achieves better final ST accuracy. Nevertheless, CTC is only a partial solution and thus, in our second contribution, we propose a novel pre-training method combining CTC and optimal transport to further reduce this gap. Our method pre-trains a Siamese-like model composed of two encoders, one for acoustic inputs and the other for textual inputs, such that they produce representations that are close to each other in the Wassers
    
[^154]: 对神经网络反例引导修复的鲁棒优化视角

    A Robust Optimisation Perspective on Counterexample-Guided Repair of Neural Networks. (arXiv:2301.11342v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11342](http://arxiv.org/abs/2301.11342)

    本研究从鲁棒优化角度解决了神经网络反例引导修复是否保证终止的问题，并提出一种基于二次规划的线性回归模型修复新算法。

    

    反例引导修复旨在创建具有数学安全性保证的神经网络，以便在安全关键领域应用神经网络。然而，反例引导修复是否保证终止仍然是一个未解之谜。我们通过表明反例引导修复可以被视为鲁棒优化算法来解决这个问题。虽然神经网络修复本身的终止保证仍然超出了我们的能力范围，但是我们证明了更受限制的机器学习模型的终止性，并证明了一般情况下的不终止性。我们通过实验证明了我们理论结果的实际影响，证明了常见验证器和违背者对修复的适用性，尽管理论结果具有不利因素。此外，我们使用我们的理论洞见设计了一种基于二次规划的线性回归模型修复新算法，超越了现有方法。

    Counterexample-guided repair aims at creating neural networks with mathematical safety guarantees, facilitating the application of neural networks in safety-critical domains. However, whether counterexample-guided repair is guaranteed to terminate remains an open question. We approach this question by showing that counterexample-guided repair can be viewed as a robust optimisation algorithm. While termination guarantees for neural network repair itself remain beyond our reach, we prove termination for more restrained machine learning models and disprove termination in a general setting. We empirically study the practical implications of our theoretical results, demonstrating the suitability of common verifiers and falsifiers for repair despite a disadvantageous theoretical result. Additionally, we use our theoretical insights to devise a novel algorithm for repairing linear regression models based on quadratic programming, surpassing existing approaches.
    
[^155]: 用于解决PDE反问题的神经反演算子

    Neural Inverse Operators for Solving PDE Inverse Problems. (arXiv:2301.11167v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11167](http://arxiv.org/abs/2301.11167)

    提出一种名为神经反演算子(NIOs)的模型来解决PDE反问题，并在多种实验中表现优于基线模型，可以稳健、准确地解决PDE反问题，且速度更快。

    

    PDE反问题的大部分反问题只有作为算子到函数映射才被很好地定义。现有的算子学习框架将函数映射到函数，需进行修改以从数据中学习反映射。我们提出了一种新颖的结构，称为神经反演算子(NIOs)，用于解决这些PDE反问题。受基础数学结构的启发，NIO基于DeepONets和FNOs的适当组合来逼近从算子到函数的映射。通过多种实验，表明NIO显著地优于基线模型，并能够稳健，准确地解决PDE反问题，且速度比现有的直接和PDE约束优化方法快几个数量级。

    A large class of inverse problems for PDEs are only well-defined as mappings from operators to functions. Existing operator learning frameworks map functions to functions and need to be modified to learn inverse maps from data. We propose a novel architecture termed Neural Inverse Operators (NIOs) to solve these PDE inverse problems. Motivated by the underlying mathematical structure, NIO is based on a suitable composition of DeepONets and FNOs to approximate mappings from operators to functions. A variety of experiments are presented to demonstrate that NIOs significantly outperform baselines and solve PDE inverse problems robustly, accurately and are several orders of magnitude faster than existing direct and PDE-constrained optimization methods.
    
[^156]: 深度强化学习中的自动内在奖励塑造探索方法研究

    Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning. (arXiv:2301.10886v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10886](http://arxiv.org/abs/2301.10886)

    本文提出了一种名为AIRS的自动内在奖励塑造探索方法，可以提供高质量的内在激励以增强强化学习中的探索性能；并开发了高效可靠的内在奖励工具包。实验表明，AIRS性能卓越，能够胜过基准方案。

    

    本文提出了一种名为AIRS的自动内在奖励塑造方法，通过智能和适应性的塑造函数，提供高质量的内在激励以增强强化学习中的探索性能。AIRS可以根据实时估计的任务回报从预定义的函数集中选择塑造函数，提供可靠的探索激励并解决偏置目标问题。此外，我们开发了一个内在奖励工具包，提供多种内在奖励方法的高效可靠实现方式。我们将AIRS应用在MiniGrid、Procgen和DeepMind控制套件的多项任务中进行测试。大量仿真结果表明，AIRS可以胜过基准方案，并具有简单的架构和卓越的性能。

    We present AIRS: Automatic Intrinsic Reward Shaping that intelligently and adaptively provides high-quality intrinsic rewards to enhance exploration in reinforcement learning (RL). More specifically, AIRS selects shaping function from a predefined set based on the estimated task return in real-time, providing reliable exploration incentives and alleviating the biased objective problem. Moreover, we develop an intrinsic reward toolkit to provide efficient and reliable implementations of diverse intrinsic reward approaches. We test AIRS on various tasks of MiniGrid, Procgen, and DeepMind Control Suite. Extensive simulation demonstrates that AIRS can outperform the benchmarking schemes and achieve superior performance with simple architecture.
    
[^157]: 预计算内存或实时编码？一种检索增强的混合方法使计算资源得到最大利用

    Pre-computed memory or on-the-fly encoding? A hybrid approach to retrieval augmentation makes the most of your compute. (arXiv:2301.10448v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.10448](http://arxiv.org/abs/2301.10448)

    本文提出了一种检索增强的混合方法，LUMEN，它预先计算大部分检索表示，并使用一个实时编码器进行完成编码，相较于纯内存和FiD，LUMEN在多个问答任务中具有更好的性能，且成本更低。

    

    检索增强语言模型，如解码器中的Fusion，具有强大的能力，在各种知识密集型任务中设置了最新的技术水平。然而，由于需要对大量检索到的段落进行编码，它们也非常昂贵。一些工作通过将文本语料库预编码为内存，并直接检索密集表示来避免这种成本。但是，预编码内存会导致严重的质量惩罚，因为内存表示未针对当前输入进行调整。我们提出了LUMEN，它是这两个极端之间的混合体，预先计算大部分检索表示，并使用实时编码器完成编码，该实时编码器是基于问题进行条件化的，并为任务进行了微调。我们表明，在多个问答任务中，LUMEN明显优于纯内存，同时比FiD便宜得多，并且在给定的计算资源预算下，LUMEN的效果优于两者。此外，当模型规模增大时，LUMEN相对于FiD的优势也增加。

    Retrieval-augmented language models such as Fusion-in-Decoder are powerful, setting the state of the art on a variety of knowledge-intensive tasks. However, they are also expensive, due to the need to encode a large number of retrieved passages. Some work avoids this cost by pre-encoding a text corpus into a memory and retrieving dense representations directly. However, pre-encoding memory incurs a severe quality penalty as the memory representations are not conditioned on the current input. We propose LUMEN, a hybrid between these two extremes, pre-computing the majority of the retrieval representation and completing the encoding on the fly using a live encoder that is conditioned on the question and fine-tuned for the task. We show that LUMEN significantly outperforms pure memory on multiple question-answering tasks while being much cheaper than FiD, and outperforms both for any given compute budget. Moreover, the advantage of LUMEN over FiD increases with model size.
    
[^158]: 论几何图神经网络表现力的研究

    On the Expressive Power of Geometric Graph Neural Networks. (arXiv:2301.09308v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.09308](http://arxiv.org/abs/2301.09308)

    本文提出了几何版本的Weisfeiler-Leman测试(GWL)，可以区分几何图形，揭示了关键设计选择如何影响几何GNN的表现力

    

    通过 Weisfeiler-Leman (WL) 图同构测试，已经广泛研究了图神经网络 (GNNs) 的表现力。然而，标准的 GNNs 和 WL 框架不适用于嵌入欧几里得空间的几何图形，例如生物分子、材料和其他物理系统。在本文中，我们提出了 WL 测试的几何版本 (GWL)，以区分几何图形，同时尊重底层物理对称性：排列、旋转、反射和平移。我们使用 GWL 来表征具有不变或等变于物理对称性的几何 GNN 的表现力，以区分几何图形。GWL 揭示了关键设计选择如何影响几何 GNN 的表现力：(1) 不变层表现力有限，因为它们无法区分一跳相同的几何图形；(2) 等变层通过传播局部邻域之外的几何信息，区分更大类别的图形；(3)

    The expressive power of Graph Neural Networks (GNNs) has been studied extensively through the Weisfeiler-Leman (WL) graph isomorphism test. However, standard GNNs and the WL framework are inapplicable for geometric graphs embedded in Euclidean space, such as biomolecules, materials, and other physical systems. In this work, we propose a geometric version of the WL test (GWL) for discriminating geometric graphs while respecting the underlying physical symmetries: permutations, rotation, reflection, and translation. We use GWL to characterise the expressive power of geometric GNNs that are invariant or equivariant to physical symmetries in terms of distinguishing geometric graphs. GWL unpacks how key design choices influence geometric GNN expressivity: (1) Invariant layers have limited expressivity as they cannot distinguish one-hop identical geometric graphs; (2) Equivariant layers distinguish a larger class of graphs by propagating geometric information beyond local neighbourhoods; (3)
    
[^159]: 用于建模SAM的随机微分方程：理论与洞见

    An SDE for Modeling SAM: Theory and Insights. (arXiv:2301.08203v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08203](http://arxiv.org/abs/2301.08203)

    我们推导了适用于SAM及其变体的连续时间模型，并解释了为什么SAM更喜欢平坦的极小值而非尖峰值，同时证明了SAM在某些现实条件下会被吸引到鞍点。

    

    我们研究了SAM（Sharpness-Aware Minimization）优化器，由于其在比更传统的随机梯度下降变体上表现出的更高性能，最近吸引了很多人的关注。我们的主要贡献是推导出连续时间模型（以SDEs的形式）来处理SAM及其两个变体，包括全批量和小批量设置。我们证明了这些SDE是离散时间算法的严格逼近（以弱意义，与学习速率线性缩放）。利用这些模型，我们解释了为什么SAM更喜欢平坦的极小值而非尖峰值-—通过展示SAM在最小化具有Hessian相关噪声结构的隐式正则化损失函数。最后，我们证明了SAM在某些现实条件下会被吸引到鞍点。我们的理论结果获得了详细实验证明。

    We study the SAM (Sharpness-Aware Minimization) optimizer which has recently attracted a lot of interest due to its increased performance over more classical variants of stochastic gradient descent. Our main contribution is the derivation of continuous-time models (in the form of SDEs) for SAM and two of its variants, both for the full-batch and mini-batch settings. We demonstrate that these SDEs are rigorous approximations of the real discrete-time algorithms (in a weak sense, scaling linearly with the learning rate). Using these models, we then offer an explanation of why SAM prefers flat minima over sharp ones~--~by showing that it minimizes an implicitly regularized loss with a Hessian-dependent noise structure. Finally, we prove that SAM is attracted to saddle points under some realistic conditions. Our theoretical results are supported by detailed experiments.
    
[^160]: 自监督学习中相似性损失和聚类损失之间关系的研究

    Learning the Relation between Similarity Loss and Clustering Loss in Self-Supervised Learning. (arXiv:2301.03041v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.03041](http://arxiv.org/abs/2301.03041)

    本文提出了一种新的自监督学习框架，该框架由相似性损失和聚类损失组成。该框架利用了两个不同图像之间的相似性，并利用聚类损失进一步增强表示。实验结果表明，该框架在ImageNet数据集和下游任务中优于最先进的自监督方法。

    

    自监督学习通过大量数据使网络学习具有辨别力的特征。大多数最先进的方法基于对比学习，通过最大化图像两个扩增版本之间的相似性来实现。通过利用两个扩增版本的一致性，可以解放手动注释的负担。然而，对比学习只是利用了实例级别的信息来学习鲁棒的特征，而学到的信息可能仅局限于同一实例的不同视图。本文试图利用两个不同图像之间的相似性来提高自监督学习中的表示。我们分析了相似性损失和特征级交叉熵损失之间的关系。这两个损失对大多数深度学习方法至关重要，但它们之间的关系并不清楚。相似性损失有助于在自监督学习中获得具有辨别力和鲁棒性的特征。聚类损失强调在聚类中分组相似数据并分离不同数据。我们提出了一个新的自监督学习框架，该框架由相似性损失和聚类损失组成。我们的框架利用了两个不同图像之间的相似性，并利用聚类损失进一步增强表示。实验结果表明，我们的框架在ImageNet数据集和下游任务中优于最先进的自监督方法。

    Self-supervised learning enables networks to learn discriminative features from massive data itself. Most state-of-the-art methods maximize the similarity between two augmentations of one image based on contrastive learning. By utilizing the consistency of two augmentations, the burden of manual annotations can be freed. Contrastive learning exploits instance-level information to learn robust features. However, the learned information is probably confined to different views of the same instance. In this paper, we attempt to leverage the similarity between two distinct images to boost representation in self-supervised learning. In contrast to instance-level information, the similarity between two distinct images may provide more useful information. Besides, we analyze the relation between similarity loss and feature-level cross-entropy loss. These two losses are essential for most deep learning methods. However, the relation between these two losses is not clear. Similarity loss helps o
    
[^161]: DExT：检测器说明工具包

    DExT: Detector Explanation Toolkit. (arXiv:2212.11409v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.11409](http://arxiv.org/abs/2212.11409)

    本文提出了一个开源的Detector Explanation Toolkit (DExT)，使用某些基于梯度的解释方法实现了生成所有检测器决策的全面解释。它可以为边界框和分类决策产生解释，是一个帮助人们了解物体检测器系统决策原因的工具。

    

    最先进的物体检测器由于其高度非线性的内部计算而被视为黑盒子。即使在检测器性能取得了前所未有的进展，由于无法解释其输出是如何生成的，限制了它们在安全关键的应用中的使用。先前的工作未能为边界框和分类决策产生解释，并且通常为各种检测器制作单独的解释。在本文中，我们提出了一个开源的Detector Explanation Toolkit (DExT)，它使用某些基于梯度的解释方法实现了生成所有检测器决策的全面解释。我们提出了各种多对象可视化方法，以合并图像中检测到的多个对象的解释以及单个图像中相应的检测结果。定量评估表明，与其他检测器相比，Single Shot MultiBox Detector (SSD) 更能忠实地解释。

    State-of-the-art object detectors are treated as black boxes due to their highly non-linear internal computations. Even with unprecedented advancements in detector performance, the inability to explain how their outputs are generated limits their use in safety-critical applications. Previous work fails to produce explanations for both bounding box and classification decisions, and generally make individual explanations for various detectors. In this paper, we propose an open-source Detector Explanation Toolkit (DExT) which implements the proposed approach to generate a holistic explanation for all detector decisions using certain gradient-based explanation methods. We suggests various multi-object visualization methods to merge the explanations of multiple objects detected in an image as well as the corresponding detections in a single image. The quantitative evaluation show that the Single Shot MultiBox Detector (SSD) is more faithfully explained compared to other detectors regardless
    
[^162]: 跨文档关系抽取的多跳证据检索

    Multi-hop Evidence Retrieval for Cross-document Relation Extraction. (arXiv:2212.10786v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10786](http://arxiv.org/abs/2212.10786)

    本文介绍了一个名为MR.COD的多跳证据检索方法，用于支持跨文档关系抽取。实验表明该方法有效地获取了跨文档证据，并提升了封闭和开放设置中的性能。

    

    关系抽取(RE)已经扩展到跨文档场景中，因为许多关系不仅仅在一个文档中描述。这不可避免地带来了有效的开放空间证据检索的挑战，以支持跨文档关系的推断，同时也带来了多跳推理的挑战，以处理散布在开放式文档集中的实体和证据。为了应对这些挑战，我们提出了MR.COD(跨文档关系抽取的多跳证据检索)，这是一种基于证据路径挖掘和排序的多跳证据检索方法。我们探索了多个检索器的变体，以显示证据检索在跨文档RE中的重要性。我们还为此设置提出了一种上下文密集的检索器。在CodRED上的实验表明，MR.COD的证据检索有效地获取了跨文档证据，并提升了封闭和开放设置中的端到端RE性能。

    Relation Extraction (RE) has been extended to cross-document scenarios because many relations are not simply described in a single document. This inevitably brings the challenge of efficient open-space evidence retrieval to support the inference of cross-document relations, along with the challenge of multi-hop reasoning on top of entities and evidence scattered in an open set of documents. To combat these challenges, we propose MR.COD (Multi-hop evidence retrieval for Cross-document relation extraction), which is a multi-hop evidence retrieval method based on evidence path mining and ranking. We explore multiple variants of retrievers to show evidence retrieval is essential in cross-document RE. We also propose a contextual dense retriever for this setting. Experiments on CodRED show that evidence retrieval with MR.COD effectively acquires crossdocument evidence and boosts end-to-end RE performance in both closed and open settings.
    
[^163]: 超越对比学习：一种多语言检索的变分生成模型

    Beyond Contrastive Learning: A Variational Generative Model for Multilingual Retrieval. (arXiv:2212.10726v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10726](http://arxiv.org/abs/2212.10726)

    本文提出了一种多语言检索的变分生成模型，可以有效地鼓励源分离，并展示其与对比学习方法的比较结果，该方法在实际检索任务中表现出效果。

    

    对比学习已被成功用于检索语义对齐的句子，但往往需要大批量处理或精心的工程才能奏效。本文提出了一种生成模型，用于学习多语言文本嵌入，可以用于检索或评分句子对。我们的模型在$N$种语言的并行数据上操作，并通过我们引入的一种近似方法，在这个多语言环境中有效地鼓励源分离，将翻译之间共享的语义信息与语体或语言特定变化分开。我们展示了对比和基于生成的方法在学习多语言文本嵌入方面的大规模仔细比较，这是我们所知道的尽管这些方法十分流行却从未比较过的。我们在一系列任务上评估了这种方法，包括语义相似性，双语挖掘和跨语言问题检索——最后一个任务将展示我们的方法在实际检索任务中的有效性。

    Contrastive learning has been successfully used for retrieval of semantically aligned sentences, but it often requires large batch sizes or careful engineering to work well. In this paper, we instead propose a generative model for learning multilingual text embeddings which can be used to retrieve or score sentence pairs. Our model operates on parallel data in $N$ languages and, through an approximation we introduce, efficiently encourages source separation in this multilingual setting, separating semantic information that is shared between translations from stylistic or language-specific variation. We show careful large-scale comparisons between contrastive and generation-based approaches for learning multilingual text embeddings, a comparison that has not been done to the best of our knowledge despite the popularity of these approaches. We evaluate this method on a suite of tasks including semantic similarity, bitext mining, and cross-lingual question retrieval -- the last of which w
    
[^164]: 通过合并语言模型的权重实现无数据知识融合

    Dataless Knowledge Fusion by Merging Weights of Language Models. (arXiv:2212.09849v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09849](http://arxiv.org/abs/2212.09849)

    本文提出了一种无数据知识融合方法，可以合并在不同训练数据集上建立的单个模型，以得到一个在所有数据集领域上表现良好且可以推广到域外数据的单一模型。

    

    微调预训练语言模型已成为构建下游NLP模型的流行范式。通常情况下，经过微调的模型已经可用，但其训练数据不可用，由于数据隐私或知识产权问题。这就造成了跨模型融合知识以产生更好的单一模型的障碍。在本文中，我们研究了建立在不同训练数据集上的单个模型之间合并的问题，以得到一个在所有数据集领域上表现良好且可以推广到域外数据的单一模型。我们提出了一种无数据知识融合方法，该方法在参数空间中合并模型，由权重引导，以最小化合并模型和单个模型之间的预测差异。在一系列评估设置中，我们展示了该方法显著优于如Fisher加权平均或模型集成等基线。此外，我们发现我们的方法是一个有前途的多语言微调替代方案，因为它可以在不需要任何额外注释数据的情况下实现可比的性能。

    Fine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Oftentimes fine-tuned models are readily available but their training data is not, due to data privacy or intellectual property concerns. This creates a barrier to fusing knowledge across individual models to yield a better single model. In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-of-domain data. We propose a dataless knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models. Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling. Further, we find that our method is a promising alternative to multi-
    
[^165]: APOLLO：面向逻辑推理的语言模型自适应预训练的简单方法

    APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning. (arXiv:2212.09282v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09282](http://arxiv.org/abs/2212.09282)

    本文提出了APOLLO，一种适应性预训练语言模型，通过选择特定的Wikipedia子集进行预训练，并使用两个自监督损失函数，成功地提高了模型的逻辑推理能力。

    

    文本的逻辑推理是一种重要的能力，需要理解文本中存在的信息、它们的相互联系，然后通过它们来推断新的结论。本文提出了APOLLO，一种适应性预训练语言模型，具有改进的逻辑推理能力。我们选择了Wikipedia的子集进行预训练，基于一组逻辑推理关键词。我们使用两个自监督损失函数：修改过的掩码语言建模损失只对可能需要更多推理而不仅仅是基本语言理解的特定词性的单词进行掩码，以及句子级分类损失，教导模型区分逻辑上连接和不连接的句子。我们的实验表明，APOLLO在多个逻辑推理数据集上优于最先进的语言模型，而不损失其在其他语言任务上的性能。

    Logical reasoning of text is an important ability that requires understanding the information present in the text, their interconnections, and then reasoning through them to infer new conclusions. Prior works on improving the logical reasoning ability of language models require complex processing of training data (e.g., aligning symbolic knowledge to text), yielding task-specific data augmentation solutions that restrict the learning of general logical reasoning skills. In this work, we propose APOLLO, an adaptively pretrained language model that has improved logical reasoning abilities. We select a subset of Wikipedia, based on a set of logical inference keywords, for continued pretraining of a language model. We use two self-supervised loss functions: a modified masked language modeling loss where only specific parts-of-speech words, that would likely require more reasoning than basic language understanding, are masked, and a sentence-level classification loss that teaches the model 
    
[^166]: GNN 和 MLP 相互联系揭示 GNN 在本质上是好的泛化器

    Graph Neural Networks are Inherently Good Generalizers: Insights by Bridging GNNs and MLPs. (arXiv:2212.09034v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.09034](http://arxiv.org/abs/2212.09034)

    本文通过将中间模型命名为 PMLP 并在测试时采用 GNNs 的架构，发现 GNNs 的表现出众不是其高级表现力的主要原因，而是其固有的泛化能力。

    

    图神经网络（GNNs）是图表示学习的事实上模型类别，它们建立在多层感知器（MLP）体系结构之上，并增加了额外的消息传递层以允许特征在节点之间流动。本文猜测 GNNs 的表现出众不是其高级表现力的主要原因，而是其固有的泛化能力。这项发现提供了一种新的方式来理解 GNNs 的学习行为，并可以用作更深层次的分析工具。

    Graph neural networks (GNNs), as the de-facto model class for representation learning on graphs, are built upon the multi-layer perceptrons (MLP) architecture with additional message passing layers to allow features to flow across nodes. While conventional wisdom commonly attributes the success of GNNs to their advanced expressivity, we conjecture that this is not the main cause of GNNs' superiority in node-level prediction tasks. This paper pinpoints the major source of GNNs' performance gain to their intrinsic generalization capability, by introducing an intermediate model class dubbed as P(ropagational)MLP, which is identical to standard MLP in training, but then adopts GNN's architecture in testing. Intriguingly, we observe that PMLPs consistently perform on par with (or even exceed) their GNN counterparts, while being much more efficient in training. This finding sheds new insights into understanding the learning behavior of GNNs, and can be used as an analytic tool for dissecting
    
[^167]: FiDO：针对更强的性能和更快的推理进行优化的解码器融合模型

    FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference. (arXiv:2212.08153v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.08153](http://arxiv.org/abs/2212.08153)

    这项研究提出了一种名为FiDO的解码器融合模型，通过两个简单的更改，有效缓解了内存带宽约束，加快了模型的推理速度，大大提高了模型性能，达到了领先水平。

    

    Fusion-in-Decoder (FiD)是一种强大的检索增强语言模型，在许多知识密集型NLP任务上树立了业界标杆。但是，FiD所使用的架构是通过对标准T5模型做最小修改而选择的，我们的分析表明这对于一个检索增强模型来说是高度不优化的。特别地，FiD将大部分FLOPs分配给了编码器，而大多数推理时间是由于解码器中的内存带宽限制。我们提出了两个简单的更改，以缓解内存带宽约束，并使推理速度提高7倍。这使我们能够以适度的成本使用更大的解码器。我们将经过上述修改的FiD称为FiDO，并展示它在广泛的推理预算范围内比现有的FiD模型显著地提高了性能。例如，FiDO-Large-XXL比FiD-Base进行更快的推理，并实现了比FiD-Large更好的性能。

    Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that sets the state-of-the-art on many knowledge-intensive NLP tasks. However, the architecture used for FiD was chosen by making minimal modifications to a standard T5 model, which our analysis shows to be highly suboptimal for a retrieval-augmented model. In particular, FiD allocates the bulk of FLOPs to the encoder, while the majority of inference time results from memory bandwidth constraints in the decoder. We propose two simple changes to the FiD architecture to alleviate memory bandwidth constraints, and speed up inference by 7x. This allows us to use a much larger decoder at modest cost. We denote FiD with the above modifications as FiDO, and show that it strongly improves performance over existing FiD models for a wide range of inference budgets. For example, FiDO-Large-XXL performs faster inference than FiD-Base and achieves better performance than FiD-Large.
    
[^168]: 利用鉴别器引导在基于评分的扩散模型中完善生成过程

    Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models. (arXiv:2211.17091v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.17091](http://arxiv.org/abs/2211.17091)

    本文提出了“鉴别器引导”方法，通过在评分训练之后训练鉴别器，使模型评估更加准确，从而改善预训练扩散模型的样本生成。在 ImageNet 256x256 数据集上实现了 FID 1.83 和召回率 0.64 的最新结果，类似于验证数据的 FID 和召回率。

    

    本文提出的“鉴别器引导”方法旨在改善预训练扩散模型的样本生成。该方法引入了一个鉴别器，明确地监督去噪样本路径是否真实。与 GAN 不同的是，我们的方法不需要联合训练评分和鉴别器网络。相反，在评分训练之后训练鉴别器，使鉴别器训练稳定且快速收敛。在样本生成中，我们向预训练的评分添加一个辅助项以欺骗鉴别器。该项将模型评分矫正为最优鉴别器处的数据评分，这意味着鉴别器以补充的方式帮助更好地评估分数。使用我们的算法，在 ImageNet 256x256 上实现了 FID 1.83 和召回率 0.64 的最新结果，类似于验证数据的 FID（1.68）和召回率（0.66）。我们在 https://github.com/alsdudrla10/DG 上公开了代码。

    The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66). We release the code at https://github.com/alsdudrla10/DG.
    
[^169]: 用于机器学习训练的任意大的标记随机可满足性公式

    Arbitrarily Large Labelled Random Satisfiability Formulas for Machine Learning Training. (arXiv:2211.15368v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.15368](http://arxiv.org/abs/2211.15368)

    本论文展示了一种基于概率方法的生成任意尺寸的正确标记的随机公式的方法，这一方法可以用于解决目前困难且常用于深度学习模型的组合问题。

    

    将深度学习应用于解决现实中困难的组合问题具有巨大的潜力。这方向的研究集中在布尔可满足性（SAT）问题上，这是由于它的理论核心性和实际重要性。但是，一个主要的障碍是，训练集仅限于比实际感兴趣的公式小数个数量级的随机公式，这引发了对泛化的严重担忧，因为标记越来越大的随机公式变得不可解。通过基本思想中的概率方法，我们完全消除了这个障碍：我们展示了如何生成任意所需尺寸的正确标记的随机公式，而无需解决底层决策问题。此外，通过改变简单标量参数的变化，我们生成的公式的分类任务的困难程度是可调的。这打开了一个全新的复杂程度。

    Applying deep learning to solve real-life instances of hard combinatorial problems has tremendous potential. Research in this direction has focused on the Boolean satisfiability (SAT) problem, both because of its theoretical centrality and practical importance. A major roadblock faced, though, is that training sets are restricted to random formulas of size several orders of magnitude smaller than formulas of practical interest, raising serious concerns about generalization. This is because labeling random formulas of increasing size rapidly becomes intractable. By exploiting the probabilistic method in a fundamental way, we remove this roadblock entirely: we show how to generate correctly labeled random formulas of any desired size, without having to solve the underlying decision problem. Moreover, the difficulty of the classification task for the formulas produced by our generator is tunable by varying a simple scalar parameter. This opens up an entirely new level of sophistication fo
    
[^170]: OpenFE: 具有专家级性能的自动特征生成工具

    OpenFE: Automated Feature Generation with Expert-level Performance. (arXiv:2211.12507v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.12507](http://arxiv.org/abs/2211.12507)

    本文提出的自动特征生成工具OpenFE可以与机器学习专家提供的结果相媲美，具有高效和准确的特点，通过新颖的特征提升方法和两阶段修剪算法实现。

    

    自动特征生成的目标是使机器学习专家摆脱手动特征生成的繁琐任务，这对于提高表格数据的学习性能至关重要。自动特征生成的主要挑战是从大量候选特征中高效准确地识别有效特征。本文提出了OpenFE，一种自动特征生成工具，可以与机器学习专家提供的结果相媲美。OpenFE通过两个组件实现高效和准确：1）一种新颖的特征提升方法，用于准确地评估候选特征的增量性能；2）一种两阶段修剪算法，以粗到细的方式进行特征修剪。在十个基准数据集上的广泛实验表明，OpenFE比现有基线方法表现更好。我们进一步在两个Kaggle比赛中对OpenFE进行了评估，这些比赛有数千个数据科学团队参与。

    The goal of automated feature generation is to liberate machine learning experts from the laborious task of manual feature generation, which is crucial for improving the learning performance of tabular data. The major challenge in automated feature generation is to efficiently and accurately identify effective features from a vast pool of candidate features. In this paper, we present OpenFE, an automated feature generation tool that provides competitive results against machine learning experts. OpenFE achieves high efficiency and accuracy with two components: 1) a novel feature boosting method for accurately evaluating the incremental performance of candidate features and 2) a two-stage pruning algorithm that performs feature pruning in a coarse-to-fine manner. Extensive experiments on ten benchmark datasets show that OpenFE outperforms existing baseline methods by a large margin. We further evaluate OpenFE in two Kaggle competitions with thousands of data science teams participating. 
    
[^171]: 基于扩散去噪过程的感知器偏置在异常检测中的应用

    Diffusion Denoising Process for Perceptron Bias in Out-of-distribution Detection. (arXiv:2211.11255v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.11255](http://arxiv.org/abs/2211.11255)

    本文针对深度学习中的异常检测问题，提出了一种新的方法——使用扩散模型作为非对称插值的方法来增强输入并减轻过度自信的问题，从而提高判别器模型在异常检测方面的性能。

    

    异常检测对于保证深度学习模型的可靠性和安全性至关重要。目前，判别器模型在这方面的表现超过其他方法。然而，判别器模型使用的特征提取过程容易丢失关键信息，留下不良情况和恶意攻击的空间。在本文中，我们引入了一个新的感知器偏置假设，它表明判别器模型对输入的某些特征更为敏感，导致过度自信的问题。为了解决这个问题，我们提出了一个新的框架，它结合了判别器和生成模型，并将扩散模型(DMs)集成到OOD检测中。我们证明了扩散去噪过程(DDP)作为一种新形式的非对称插值，很适合增强输入并减轻过度自信的问题。在DDP下，OOD数据的判别器模型特征表现为尖锐的变化，我们利用范数...

    Out-of-distribution (OOD) detection is a crucial task for ensuring the reliability and safety of deep learning. Currently, discriminator models outperform other methods in this regard. However, the feature extraction process used by discriminator models suffers from the loss of critical information, leaving room for bad cases and malicious attacks. In this paper, we introduce a new perceptron bias assumption that suggests discriminator models are more sensitive to certain features of the input, leading to the overconfidence problem. To address this issue, we propose a novel framework that combines discriminator and generation models and integrates diffusion models (DMs) into OOD detection. We demonstrate that the diffusion denoising process (DDP) of DMs serves as a novel form of asymmetric interpolation, which is well-suited to enhance the input and mitigate the overconfidence problem. The discriminator model features of OOD data exhibit sharp changes under DDP, and we utilize the norm
    
[^172]: 一种 $k$ 最近邻的两阶段主动学习算法

    A Two-Stage Active Learning Algorithm for $k$-Nearest Neighbors. (arXiv:2211.10773v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.10773](http://arxiv.org/abs/2211.10773)

    本文提出了一种用于训练 $k$ 最近邻分类器的简单直观的主动学习算法，首次保持了 $k$ 最近邻投票概念的预测时间，并提供了一致性保证。

    

    $k$ 最近邻分类是一种流行的非参数方法，因为它具有自动适应分布比例变化等优点。然而，迄今为止，针对自然保留这些优秀特性的本地投票分类器的训练，设计主动学习策略一直是困难的，因此 $k$ 最近邻分类的主动学习策略在文献中一直缺失。在本文中，我们介绍了一种简单直观的主动学习算法，用于训练 $k$ 最近邻分类器，这是文献中第一次保持了 $k$ 最近邻投票概念的预测时间。我们为通过我们方案获取的样本提供了一种修改的 $k$ 最近邻分类器的一致性保证，并且当条件概率函数 $\mathbb{P}(Y=y|X=x)$ 足够平滑并且 Tsybakov 噪声条件成立时，我们的主动训练的方法表现良好。

    $k$-nearest neighbor classification is a popular non-parametric method because of desirable properties like automatic adaption to distributional scale changes. Unfortunately, it has thus far proved difficult to design active learning strategies for the training of local voting-based classifiers that naturally retain these desirable properties, and hence active learning strategies for $k$-nearest neighbor classification have been conspicuously missing from the literature. In this work, we introduce a simple and intuitive active learning algorithm for the training of $k$-nearest neighbor classifiers, the first in the literature which retains the concept of the $k$-nearest neighbor vote at prediction time. We provide consistency guarantees for a modified $k$-nearest neighbors classifier trained on samples acquired via our scheme, and show that when the conditional probability function $\mathbb{P}(Y=y|X=x)$ is sufficiently smooth and the Tsybakov noise condition holds, our actively trained
    
[^173]: 联合生物地球化学-物理模型的贝叶斯学习

    Bayesian Learning of Coupled Biogeochemical-Physical Models. (arXiv:2211.06714v2 [cs.CE] UPDATED)

    [http://arxiv.org/abs/2211.06714](http://arxiv.org/abs/2211.06714)

    本文提出了一种基于贝叶斯模型学习的方法，可以在候选模型空间内进行插值，同时估计状态场和参数值，解决了海洋生态系统预测动态模型因数据稀疏和模型多样性带来的不确定性问题。

    

    海洋生态系统的预测动态模型可用于各种需求。由于测量稀疏和有限理解海洋过程的复杂性，存在很大的不确定性，主要表现在模型参数值、不同参数化的功能形式、所需的复杂程度以及状态场方面。我们提出了一种贝叶斯模型学习方法，可以在候选模型空间内进行插值，并从噪声、稀疏和间接观测中发现新模型，同时估计状态场和参数值，以及所有学习数量的联合概率密度函数。我们通过使用状态增强和计算高效的GMM-DO滤波器来解决由PDEs统治的高维度和多学科动力学的挑战。我们的创新包括随机公式和复杂度参数，将候选模型统一成单个通用模型，以及随机扩展参数。

    Predictive dynamical models for marine ecosystems are used for a variety of needs. Due to sparse measurements and limited understanding of the myriad of ocean processes, there is however significant uncertainty. There is model uncertainty in the parameter values, functional forms with diverse parameterizations, level of complexity needed, and thus in the state fields. We develop a Bayesian model learning methodology that allows interpolation in the space of candidate models and discovery of new models from noisy, sparse, and indirect observations, all while estimating state fields and parameter values, as well as the joint PDFs of all learned quantities. We address the challenges of high-dimensional and multidisciplinary dynamics governed by PDEs by using state augmentation and the computationally efficient GMM-DO filter. Our innovations include stochastic formulation and complexity parameters to unify candidate models into a single general model as well as stochastic expansion paramet
    
[^174]: 用因果反事实推断提高强化学习的鲁棒性

    Causal Counterfactuals for Improving the Robustness of Reinforcement Learning. (arXiv:2211.05551v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.05551](http://arxiv.org/abs/2211.05551)

    本文提出了CausalCF，它是第一个完整的因果RL解决方案，能够通过因果反事实推断提高RL系统的鲁棒性，并已在机器人抓取和操纵任务中得到应用。

    

    强化学习在各种机器人应用中被使用。强化学习使代理能够通过与环境交互自主地学习任务。任务越重要，对RL系统的鲁棒性的需求就越高。因果RL将RL和因果推断相结合，使RL更加鲁棒。因果RL代理使用因果表示来捕捉可以从一个任务转移到另一个任务的不变因果机制。目前，因果RL的研究有限，现有的解决方案通常不完整或不适用于实际应用。在这项工作中，我们提出了CausalCF，这是第一个完整的因果RL解决方案，结合了Causal Curiosity和CoPhy的思想。Causal Curiosity提供了一种使用干预的方法，并修改了CoPhy，使RL代理能够执行反事实推断。Causal Curiosity已应用于CausalWorld中的机器人抓取和操纵任务。CausalWorld提供了一个真实的仿真环境。

    Reinforcement learning (RL) is used in various robotic applications. RL enables agents to learn tasks autonomously by interacting with the environment. The more critical the tasks are, the higher the demand for the robustness of the RL systems. Causal RL combines RL and causal inference to make RL more robust. Causal RL agents use a causal representation to capture the invariant causal mechanisms that can be transferred from one task to another. Currently, there is limited research in Causal RL, and existing solutions are usually not complete or feasible for real-world applications. In this work, we propose CausalCF, the first complete Causal RL solution incorporating ideas from Causal Curiosity and CoPhy. Causal Curiosity provides an approach for using interventions, and CoPhy is modified to enable the RL agent to perform counterfactuals. Causal Curiosity has been applied to robotic grasping and manipulation tasks in CausalWorld. CausalWorld provides a realistic simulation environment
    
[^175]: 一种通信高效的分散式交替梯度法用于双层规划

    A Decentralized Alternating Gradient Method for Communication-Efficient Bilevel Programming. (arXiv:2211.04088v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.04088](http://arxiv.org/abs/2211.04088)

    本文提出了一种通信高效的分散式交替梯度法解决双层规划问题，相较于其他方法，该算法具有更低的通信成本和更高的隐私性。

    

    双层规划近期引起了学术界的广泛关注，因为它有许多应用，包括强化学习和超参数优化。然而，现有的解决方案通常采用单机或联邦学习的方式，并存在通信成本高和隐私泄露风险等问题。本文提出了一种基于惩罚函数的分散式算法来解决这类优化问题，改进了现有的方法并且在理论上得到了保证。

    Bilevel programming has recently received attention in the literature, due to a wide range of applications, including reinforcement learning and hyper-parameter optimization. However, it is widely assumed that the underlying bilevel optimization problem is solved either by a single machine or in the case of multiple machines connected in a star-shaped network, i.e., federated learning setting. The latter approach suffers from a high communication cost on the central node (e.g., parameter server) and exhibits privacy vulnerabilities. Hence, it is of interest to develop methods that solve bilevel optimization problems in a communication-efficient decentralized manner. To that end, this paper introduces a penalty function based decentralized algorithm with theoretical guarantees for this class of optimization problems. Specifically, a distributed alternating gradient-type algorithm for solving consensus bilevel programming over a decentralized network is developed. A key feature of the pr
    
[^176]: 基于GAN的心电图合成中利用统计形状先验知识的方法研究

    Leveraging Statistical Shape Priors in GAN-based ECG Synthesis. (arXiv:2211.02626v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2211.02626](http://arxiv.org/abs/2211.02626)

    本文提出了一种基于统计形状先验知识和GAN的ECG信号生成方法，能够解决ECG信号的复杂动力学问题。使用来自MIT-BIH心律失常数据库的数据进行实验验证，生成的信号比现有最先进的基于GAN的生成基线更为逼真，对于提高ECG训练数据集质量具有重要意义，可提高ECG分类算法的性能。

    

    在紧急情况下进行心电图(ECG)数据收集是具有挑战性的,因此心电图数据合成是应对高度不平衡的ECG训练数据集的有效解决方案。本文提出了一种新颖的方法，利用生成对抗网络(GAN)和统计ECG数据建模来生成ECG信号。我们的方法利用ECG动态的先验知识来合成逼真的信号，解决了ECG信号的复杂动力学问题。为了验证我们的方法，我们对来自MIT-BIH心律失常数据库的ECG信号进行了实验。结果表明，我们的方法将ECG信号的时间和幅度变化建模为2-D形状，相比现有最先进的基于GAN的生成基线，生成的信号更逼真。我们提出的方法对于提高ECG训练数据集的质量有重要意义，最终可以提高ECG分类算法的性能。

    Electrocardiogram (ECG) data collection during emergency situations is challenging, making ECG data generation an efficient solution for dealing with highly imbalanced ECG training datasets. In this paper, we propose a novel approach for ECG signal generation using Generative Adversarial Networks (GANs) and statistical ECG data modeling. Our approach leverages prior knowledge about ECG dynamics to synthesize realistic signals, addressing the complex dynamics of ECG signals. To validate our approach, we conducted experiments using ECG signals from the MIT-BIH arrhythmia database. Our results demonstrate that our approach, which models temporal and amplitude variations of ECG signals as 2-D shapes, generates more realistic signals compared to state-of-the-art GAN based generation baselines. Our proposed approach has significant implications for improving the quality of ECG training datasets, which can ultimately lead to better performance of ECG classification algorithms. This research c
    
[^177]: 基于无上下文文法的分层神经架构搜索空间构建

    Construction of Hierarchical Neural Architecture Search Spaces based on Context-free Grammars. (arXiv:2211.01842v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01842](http://arxiv.org/abs/2211.01842)

    本研究基于无上下文文法提出了一个统一的搜索空间设计框架，可以生成表达力强大的分层搜索空间，实现了对整个体系结构的搜索并促进结构的规律性。

    

    从简单的构建块中发现神经结构是神经架构搜索(NAS)的一个长期目标。分层搜索空间是实现这一目标的一个有前途的步骤，但缺乏统一的搜索空间设计框架，并且通常仅搜索一些限定方面的架构。在本研究中，我们介绍了一个基于无上下文文法的统一搜索空间设计框架，它可以自然而紧凑地生成表达力强大的分层搜索空间，比文献中常见的空间大几个数量级。通过增强和利用它们的属性，我们有效地实现了对整个体系结构的搜索，并促进了结构的规律性。此外，我们提出了一种高效的分层核设计用于贝叶斯优化搜索策略，以高效搜索如此庞大的空间。我们展示了我们搜索空间设计框架的多样性，并表明我们的搜索策略可以优于现有的NAS方法。

    The discovery of neural architectures from simple building blocks is a long-standing goal of Neural Architecture Search (NAS). Hierarchical search spaces are a promising step towards this goal but lack a unifying search space design framework and typically only search over some limited aspect of architectures. In this work, we introduce a unifying search space design framework based on context-free grammars that can naturally and compactly generate expressive hierarchical search spaces that are 100s of orders of magnitude larger than common spaces from the literature. By enhancing and using their properties, we effectively enable search over the complete architecture and can foster regularity. Further, we propose an efficient hierarchical kernel design for a Bayesian Optimization search strategy to efficiently search over such huge spaces. We demonstrate the versatility of our search space design framework and show that our search strategy can be superior to existing NAS approaches. Co
    
[^178]: 通过后处理实现公平和最优分类

    Fair and Optimal Classification via Post-Processing. (arXiv:2211.01528v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01528](http://arxiv.org/abs/2211.01528)

    本文提出了一个后处理算法，通过评分函数推导公平分类器，达到公平对待不同群体的目的。

    

    为了减轻机器学习模型所呈现的偏见，公平性标准可以整合到训练过程中，以确保在所有人口统计学中实现公平对待，然而这往往是以模型表现为代价的。因此，了解这种权衡是公平算法设计的基础。本文在最普遍的多组、多类别和嘈杂设置下，完整地表征了公平、达摩尔平等在分类问题中的内在权衡。具体而言，我们表明，通过随机化和属性感知公平分类器实现的最小错误率是由沃瑟斯坦重心问题的最优值给出的。在实践方面，我们的发现可以产生一个简单的后处理算法，从评分函数中推导出公平分类器，并在评分为贝叶斯最优时得到最优公平分类器。我们为我们的算法提供了次优性分析和样本复杂性，并展示了它的有效性。

    To mitigate the bias exhibited by machine learning models, fairness criteria can be integrated into the training process to ensure fair treatment across all demographics, but it often comes at the expense of model performance. Understanding such tradeoffs, therefore, underlies the design of fair algorithms. To this end, this paper provides a complete characterization of the inherent tradeoff of demographic parity on classification problems, under the most general multi-group, multi-class, and noisy setting. Specifically, we show that the minimum error rate achievable by randomized and attribute-aware fair classifiers is given by the optimal value of a Wasserstein-barycenter problem. On the practical side, our findings lead to a simple post-processing algorithm that derives fair classifiers from score functions, which yields the optimal fair classifier when the score is Bayes optimal. We provide suboptimality analysis and sample complexity for our algorithm, and demonstrate its effectiv
    
[^179]: 学习SDN软件定义网络测试失败模型

    Learning Failure-Inducing Models for Testing Software-Defined Networks. (arXiv:2210.15469v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2210.15469](http://arxiv.org/abs/2210.15469)

    本文介绍了一种名为FuzzSDN的机器学习引导模糊测试方法，可生成导致SDN系统失败的有效测试数据，并学习准确的故障诱导模型以表征此类系统失败的条件。

    

    软件定义网络（SDN）能够实现由中央化的软件控制器管理的灵活和高效的通信系统。然而，这样的控制器可能会破坏基于SDN的系统的底层通信网络，因此必须进行仔细测试。当基于SDN的系统失败时，工程师需要精确地了解其发生条件以应对此类故障。本文介绍了一种名为FuzzSDN的机器学习引导模糊测试方法，旨在（1）生成导致SDN系统失败的有效测试数据和（2）学习准确的故障诱导模型，以表征此类系统失败的条件。据我们所知，FuzzSDN是首次尝试同时解决这两个SDN目标的方法。我们通过将其应用于两个开源SDN控制器控制的系统来评估FuzzSDN。此外，我们还将FuzzSDN与两种最先进的SDN模糊测试方法和两个基准进行比较。

    Software-defined networks (SDN) enable flexible and effective communication systems that are managed by centralized software controllers. However, such a controller can undermine the underlying communication network of an SDN-based system and thus must be carefully tested. When an SDN-based system fails, in order to address such a failure, engineers need to precisely understand the conditions under which it occurs. In this article, we introduce a machine learning-guided fuzzing method, named FuzzSDN, aiming at both (1) generating effective test data leading to failures in SDN-based systems and (2) learning accurate failure-inducing models that characterize conditions under which such system fails. To our knowledge, FuzzSDN is the first attempt to simultaneously address these two objectives for SDNs. We evaluate FuzzSDN by applying it to systems controlled by two open-source SDN controllers. Further, we compare FuzzSDN with two state-of-the-art methods for fuzzing SDNs and two baselines
    
[^180]: 用滑动置换不变训练来跟踪多个变化的声源位置

    Position tracking of a varying number of sound sources with sliding permutation invariant training. (arXiv:2210.14536v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2210.14536](http://arxiv.org/abs/2210.14536)

    本论文提出了一种新的基于深度学习模型的声源定位培训策略，使用滑动置换不变训练来跟踪多个变化的声源位置，最小化身份切换(IDS)，并在实验中证明该策略的有效性。

    

    最近，基于数据和学习的声源定位（SSL）方法在具有挑战性的声学环境中展现出强大的性能。然而，很少有研究采用这些方法来持续跟踪出现和消失的多个声源，就像在现实中一样。在本文中，我们提出了一种新的培训策略，用于基于深度学习的SSL模型，其直接实现基于先前时间帧中估计和参考位置之间的最优关联的均方误差。它优化了跟踪系统的期望特性：处理时变的多个声源，并根据它们的轨迹排序定位估计，最小化身份切换(IDS)。在模拟多个混响移动来源的数据和两个模型架构的评估中，证明了它在减少身份切换的同时不损失逐帧定位精度的有效性。

    Recent data- and learning-based sound source localization (SSL) methods have shown strong performance in challenging acoustic scenarios. However, little work has been done on adapting such methods to track consistently multiple sources appearing and disappearing, as would occur in reality. In this paper, we present a new training strategy for deep learning SSL models with a straightforward implementation based on the mean squared error of the optimal association between estimated and reference positions in the preceding time frames. It optimizes the desired properties of a tracking system: handling a time-varying number of sources and ordering localization estimates according to their trajectories, minimizing identity switches (IDSs). Evaluation on simulated data of multiple reverberant moving sources and on two model architectures proves its effectiveness on reducing identity switches without compromising frame-wise localization accuracy.
    
[^181]: 基于机器学习和深度学习的拓扑优化研究综述

    Topology Optimization via Machine Learning and Deep Learning: A Review. (arXiv:2210.10782v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10782](http://arxiv.org/abs/2210.10782)

    本文综述了机器学习和深度学习在拓扑优化中的应用，从TO和ML两个不同的角度进行了回顾和分析，并探讨了当前MLTO研究的限制和未来研究方向。

    

    拓扑优化是一种通过满足一定的荷载和边界条件，推导出最佳设计的方法。这种方法可以在没有初始设计的情况下进行有效的设计，但由于计算成本高而受到限制。与此同时，21世纪的机器学习方法包括深度学习取得了巨大进展，因此许多研究已经进行，以通过将机器学习应用于拓扑优化来实现有效和快速的优化。因此，本研究回顾和分析了先前关于基于机器学习的拓扑优化（MLTO）的研究。使用两个不同的角度对 MLTO 研究进行了回顾，即(1) TO和(2) ML视角。TO的角度解答了为什么要将ML用于TO，而ML的角度则解答了如何将ML应用于TO。此外，还检查了目前MLTO研究的限制和未来研究方向。

    Topology optimization (TO) is a method of deriving an optimal design that satisfies a given load and boundary conditions within a design domain. This method enables effective design without initial design, but has been limited in use due to high computational costs. At the same time, machine learning (ML) methodology including deep learning has made great progress in the 21st century, and accordingly, many studies have been conducted to enable effective and rapid optimization by applying ML to TO. Therefore, this study reviews and analyzes previous research on ML-based TO (MLTO). Two different perspectives of MLTO are used to review studies: (1) TO and (2) ML perspectives. The TO perspective addresses "why" to use ML for TO, while the ML perspective addresses "how" to apply ML to TO. In addition, the limitations of current MLTO research and future research directions are examined.
    
[^182]: 随机差分隐私与公平学习

    Stochastic Differentially Private and Fair Learning. (arXiv:2210.08781v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.08781](http://arxiv.org/abs/2210.08781)

    本文提供了第一个能够保证收敛的随机差分隐私公平学习算法。该算法解决了高风险决策系统中面临的歧视和隐私泄漏问题。

    

    机器学习模型正在越来越多地应用于高风险决策系统中。在这些应用中，一个主要问题是这些模型有时会对某些人口统计学群体进行歧视，比如某些种族、性别或年龄的个体。这些应用中另一个主要问题是侵犯用户的隐私。虽然已经开发出公平学习算法来减轻歧视问题，但这些算法仍然可能泄漏敏感信息，如个人的健康或财务记录。利用差分隐私（DP）的概念，早期的研究旨在开发既能够保护隐私又能够进行公平学习的学习算法。然而，现有的随机 DP 公平学习算法要么不能保证收敛，要么需要每次算法迭代中使用完整的批量数据来进行收敛。在本文中，我们提供了第一个可以保证收敛的随机差分隐私公平学习算法。这里的“随机”一词指的是

    Machine learning models are increasingly used in high-stakes decision-making systems. In such applications, a major concern is that these models sometimes discriminate against certain demographic groups such as individuals with certain race, gender, or age. Another major concern in these applications is the violation of the privacy of users. While fair learning algorithms have been developed to mitigate discrimination issues, these algorithms can still leak sensitive information, such as individuals' health or financial records. Utilizing the notion of differential privacy (DP), prior works aimed at developing learning algorithms that are both private and fair. However, existing algorithms for DP fair learning are either not guaranteed to converge or require full batch of data in each iteration of the algorithm to converge. In this paper, we provide the first stochastic differentially private algorithm for fair learning that is guaranteed to converge. Here, the term "stochastic" refers
    
[^183]: MonoNeRF：在无相机姿态下，从单目视频中学习通用的NeRF模型

    MonoNeRF: Learning Generalizable NeRFs from Monocular Videos without Camera Pose. (arXiv:2210.07181v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.07181](http://arxiv.org/abs/2210.07181)

    MonoNeRF是一种无需深度和相机姿态标注的通用神经辐射场模型，可从单目视频中学得，可以用于多个应用，包括深度估计、相机姿态估计和单张图像的新视角合成。

    

    我们提出了一种名为MonoNeRF的通用神经辐射场模型，它可以在大规模的单目视频中进行训练，用来生成静态场景的移动图像，无需深度和相机姿态的标注。MonoNeRF的基础是自编码器，其中编码器估计单目深度和相机姿态，解码器根据深度编码器特征构建多平面NeRF表示，并使用估计的相机渲染输入帧。学习由重构误差监督。学得模型后，可用于多个应用包括深度估计、相机姿态估计和单张图像的新视角合成。更多定性结果可在以下网址查看：https://oasisyang.github.io/mononerf。

    We propose a generalizable neural radiance fields - MonoNeRF, that can be trained on large-scale monocular videos of moving in static scenes without any ground-truth annotations of depth and camera poses. MonoNeRF follows an Autoencoder-based architecture, where the encoder estimates the monocular depth and the camera pose, and the decoder constructs a Multiplane NeRF representation based on the depth encoder feature, and renders the input frames with the estimated camera. The learning is supervised by the reconstruction error. Once the model is learned, it can be applied to multiple applications including depth estimation, camera pose estimation, and single-image novel view synthesis. More qualitative results are available at: https://oasisyang.github.io/mononerf .
    
[^184]: 多智能体强化学习中的混合执行集中式训练

    Centralized Training with Hybrid Execution in Multi-Agent Reinforcement Learning. (arXiv:2210.06274v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.06274](http://arxiv.org/abs/2210.06274)

    本文介绍了多智能体强化学习中的新范式，提供了MARO方法，通过集中式训练和自回归预测模型，在执行时间中预测缺失的智能体观测值，优于现有方法。

    

    我们引入了混合执行集中式训练在多智能体强化学习中的新范式。在这种范式下，智能体们通过信息共享来完成合作任务，并在执行时间中实现通信级别的任意变化。为了形式化我们的设置，我们定义了一类名为“混合-POMDPs”的多智能体部分可观察马尔可夫决策过程，显式地建模了智能体之间的通信过程。我们提出了MARO方法，它利用自回归预测模型在集中式训练下预测执行时间中缺失的智能体观测值。我们评估了MARO在不同任务中的性能，并发现在各种协作任务中，MARO的表现都优于现有的MARL方法。

    We introduce hybrid execution in multi-agent reinforcement learning (MARL), a new paradigm in which agents aim to successfully complete cooperative tasks with arbitrary communication levels at execution time by taking advantage of information-sharing among the agents. Under hybrid execution, the communication level can range from a setting in which no communication is allowed between agents (fully decentralized), to a setting featuring full communication (fully centralized), but the agents do not know beforehand which communication level they will encounter at execution time. To formalize our setting, we define a new class of multi-agent partially observable Markov decision processes (POMDPs) that we name hybrid-POMDPs, which explicitly model a communication process between the agents. We contribute MARO, an approach that makes use of an auto-regressive predictive model, trained in a centralized manner, to estimate missing agents' observations at execution time. We evaluate MARO on sta
    
[^185]: 基于关系蒸馏的无链接预测

    Linkless Link Prediction via Relational Distillation. (arXiv:2210.05801v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.05801](http://arxiv.org/abs/2210.05801)

    本研究提出了一种基于关系蒸馏的框架(LLP)用于链接预测，通过匹配以每个（锚）节点为中心的关系知识来蒸馏链接预测的知识，该方法快捷而有效，达到或超过了现有技术的图神经网络的性能水平。

    

    图神经网络在预测链接的任务中表现出了出色的性能。尽管有效，由非平凡邻域数据依赖性带来的高延迟限制了图神经网络在实际部署中的应用。相比之下，已知的高效多层感知器由于缺乏联系知识而比图神经网络不够有效。为了结合图神经网络和多层感知器的优点，本文探索了直接知识蒸馏方法，即基于预测的逻辑匹配和节点表示匹配。通过观察到直接知识蒸馏方法在链接预测中表现不佳，我们提出了一种基于关系蒸馏的框架，Linkless Link Prediction（LLP），用于通过多层感知器蒸馏链接预测的知识。与简单的知识蒸馏方法匹配独立链接逻辑或节点表示不同，LLP蒸馏以每个（锚）节点为中心的关系知识到学生MLP。具体而言，我们提出了基于排名的匹配和基于分布的匹配来蒸馏关系知识。对基准数据集的实验结果表明，LLP在推理时速度快得多，而且达到或超过了现有技术的图神经网络的性能水平。

    Graph Neural Networks (GNNs) have shown exceptional performance in the task of link prediction. Despite their effectiveness, the high latency brought by non-trivial neighborhood data dependency limits GNNs in practical deployments. Conversely, the known efficient MLPs are much less effective than GNNs due to the lack of relational knowledge. In this work, to combine the advantages of GNNs and MLPs, we start with exploring direct knowledge distillation (KD) methods for link prediction, i.e., predicted logit-based matching and node representation-based matching. Upon observing direct KD analogs do not perform well for link prediction, we propose a relational KD framework, Linkless Link Prediction (LLP), to distill knowledge for link prediction with MLPs. Unlike simple KD methods that match independent link logits or node representations, LLP distills relational knowledge that is centered around each (anchor) node to the student MLP. Specifically, we propose rank-based matching and distri
    
[^186]: 使用加权不对称损失函数的神经网络模型预测区间

    Prediction intervals for neural network models using weighted asymmetric loss functions. (arXiv:2210.04318v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.04318](http://arxiv.org/abs/2210.04318)

    本论文提出了一种使用加权不对称损失函数的方法，生成可靠的预测区间，适用于复杂的机器学习情境，可扩展为参数化函数的PI预测。

    

    我们提出了一种简单而有效的方法来生成近似和预测趋势的预测区间（PIs）。我们利用加权不对称损失函数来估计PI的下限和上限，权重由区间宽度确定。我们提供了该方法的简洁数学证明，展示了如何将其扩展到为参数化函数推导PI，并论证了该方法为预测相关变量的PI而有效的原因。我们在基于神经网络的模型的真实世界预测任务上对该方法进行了测试，结果表明它在复杂的机器学习情境下可以产生可靠的PI。

    We propose a simple and efficient approach to generate prediction intervals (PIs) for approximated and forecasted trends. Our method leverages a weighted asymmetric loss function to estimate the lower and upper bounds of the PIs, with the weights determined by the interval width. We provide a concise mathematical proof of the method, show how it can be extended to derive PIs for parametrised functions and argue why the method works for predicting PIs of dependent variables. The presented tests of the method on a real-world forecasting task using a neural network-based model show that it can produce reliable PIs in complex machine learning scenarios.
    
[^187]: MAMO：面向细粒度视觉-语言表示学习的掩膜多模态建模方法

    MAMO: Masked Multimodal Modeling for Fine-Grained Vision-Language Representation Learning. (arXiv:2210.04183v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.04183](http://arxiv.org/abs/2210.04183)

    本文提出了一种联合掩膜多模态建模方法，以学习细粒度的多模态表示，通过隐式和显式目标来恢复联合掩膜信号以提高细化的图像-文本交互。

    

    多模态表示学习在各种视觉-语言任务中展现了很大的潜力。然而，当前大部分方法都主要致力于建立全局级别的图像与语言对齐，缺乏有效的细粒度图像-文本交互。为此，本文提出了一种联合掩膜多模态建模方法，以学习细粒度的多模态表示。我们的方法对图像-文本输入进行联合掩膜，并集成了显式和隐式目标来恢复掩膜信号。其中，隐式目标为视觉和语言提供统一且无偏差的目标，模型预测未掩膜输入的潜在多模态表示；显式目标则通过恢复图像块的动量视觉特征和单词标记的概念，进一步丰富多模态表示。通过这样的掩膜建模过程，我们的模型不仅可以学习到细粒度的多模态交互，还能学习到有意义的语义信息。

    Multimodal representation learning has shown promising improvements on various vision-language tasks. Most existing methods excel at building global-level alignment between vision and language while lacking effective fine-grained image-text interaction. In this paper, we propose a jointly masked multimodal modeling method to learn fine-grained multimodal representations. Our method performs joint masking on image-text input and integrates both implicit and explicit targets for the masked signals to recover. The implicit target provides a unified and debiased objective for vision and language, where the model predicts latent multimodal representations of the unmasked input. The explicit target further enriches the multimodal representations by recovering high-level and semantically meaningful information: momentum visual features of image patches and concepts of word tokens. Through such a masked modeling process, our model not only learns fine-grained multimodal interaction, but also a
    
[^188]: 基于未标记的样本的分布漂移下测试时间校准置信度预测器

    Test-time Recalibration of Conformal Predictors Under Distribution Shift Based on Unlabeled Examples. (arXiv:2210.04166v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.04166](http://arxiv.org/abs/2210.04166)

    本论文提出了一种基于未标记样本的分布漂移下测试时间校准置信度预测器。通过使用密度比估计技术来预测新分布的截止阈值，我们在几个标准图像数据集上展示了该方法优于最新的分布漂移下的测试时间校准方法。

    

    现代图像分类器非常准确，但预测没有不确定性估计。基于分类器的概率估计，基本置信度预测器通过计算一组包含具有用户指定概率的正确类的类来提供不确定性估计。为了提供这样的集合，置信度预测器常常基于校准集合估计概率估计的截断阈值。置信度预测器仅在校准集合与测试集相同时保证可靠性。因此，置信度预测器需要为新分布重新校准。但在实践中，很少有来自新分布的标记数据，使校准成为不可行的。在这项工作中，我们考虑了基于未标记样本预测新分布截止阈值的问题。虽然一般情况下不能保证基于未标记样本进行校准时的可靠性，但我们提出了一种基于密度比估计技术的方法来预测新分布的截止阈值。我们在几个标准图像数据集上评估了我们的方法，并显示出在分布漂移下进行测试时间校准的最新方法。

    Modern image classifiers are very accurate, but the predictions come without uncertainty estimates. Conformal predictors provide uncertainty estimates by computing a set of classes containing the correct class with a user-specified probability based on the classifier's probability estimates. To provide such sets, conformal predictors often estimate a cutoff threshold for the probability estimates based on a calibration set. Conformal predictors guarantee reliability only when the calibration set is from the same distribution as the test set. Therefore, conformal predictors need to be recalibrated for new distributions. However, in practice, labeled data from new distributions is rarely available, making calibration infeasible. In this work, we consider the problem of predicting the cutoff threshold for a new distribution based on unlabeled examples. While it is impossible in general to guarantee reliability when calibrating based on unlabeled examples, we propose a method that provides
    
[^189]: 解决几何复杂偏微分方程的统一硬约束框架

    A Unified Hard-Constraint Framework for Solving Geometrically Complex PDEs. (arXiv:2210.03526v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.03526](http://arxiv.org/abs/2210.03526)

    我们提出了一个神经网络硬约束框架来解决几何复杂偏微分方程，将边界条件转化为线性方程，可以在不引入额外损失项的情况下训练网络并稳定训练过程。

    

    我们提出了一个统一的硬约束框架，用神经网络解决几何复杂的偏微分方程，考虑了最常用的Dirichlet、Neumann和Robin边界条件。具体而言，我们首先从混合有限元方法引入“额外场”，重新构造偏微分方程，以等价的方式将三种类型的边界条件转化为线性方程。基于重新构造，我们对边界条件推导出通用解析解，用于构造一个符合边界条件的试验函数。有了这样一个框架，我们可以在不添加额外的损失项的情况下训练神经网络，从而有效地处理几何复杂的偏微分方程，缓解与边界条件和偏微分方程相对应的损失项之间失衡竞争的问题。我们在理论上证明了“额外场”可以稳定训练过程。在真实世界的几何复杂偏微分方程上的实验结果展示了我们方法的有效性。

    We present a unified hard-constraint framework for solving geometrically complex PDEs with neural networks, where the most commonly used Dirichlet, Neumann, and Robin boundary conditions (BCs) are considered. Specifically, we first introduce the "extra fields" from the mixed finite element method to reformulate the PDEs so as to equivalently transform the three types of BCs into linear equations. Based on the reformulation, we derive the general solutions of the BCs analytically, which are employed to construct an ansatz that automatically satisfies the BCs. With such a framework, we can train the neural networks without adding extra loss terms and thus efficiently handle geometrically complex PDEs, alleviating the unbalanced competition between the loss terms corresponding to the BCs and PDEs. We theoretically demonstrate that the "extra fields" can stabilize the training process. Experimental results on real-world geometrically complex PDEs showcase the effectiveness of our method co
    
[^190]: 自我教学：基于邻域的图形自我蒸馏用于节点分类

    Teaching Yourself: Graph Self-Distillation on Neighborhood for Node Classification. (arXiv:2210.02097v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.02097](http://arxiv.org/abs/2210.02097)

    本文提出了一种GSDN框架，通过基于邻域的图形自我蒸馏，以减小GNN和MLP之间的差距。

    

    近年来，图神经网络（GNN）在处理与图有关的任务方面取得了巨大的成功。尽管在学术界取得了巨大的成功，但在实际的工业应用中，多层感知器（MLP）仍然是主要的工作马力。造成学术界和工业界的差距之一是GNN中的数据依赖性所引起的邻域获取延迟，这使得很难在需要快速推理的需要低延迟的应用中使用。相反，没有涉及任何特征聚合，MLP没有数据依赖性，推理速度比GNN要快得多，但其性能不够竞争力。受这些互补的优势和劣势的启发，我们提出了一个基于邻域的图形自我蒸馏（GSDN）框架，以缩小GNN和MLP之间的差距。具体地，GSDN框架基于纯MLP，结构信息仅被隐式地用作先验，以指导邻域和目标之间的知识自我蒸馏，替换掉特征聚合。

    Recent years have witnessed great success in handling graph-related tasks with Graph Neural Networks (GNNs). Despite their great academic success, Multi-Layer Perceptrons (MLPs) remain the primary workhorse for practical industrial applications. One reason for this academic-industrial gap is the neighborhood-fetching latency incurred by data dependency in GNNs, which make it hard to deploy for latency-sensitive applications that require fast inference. Conversely, without involving any feature aggregation, MLPs have no data dependency and infer much faster than GNNs, but their performance is less competitive. Motivated by these complementary strengths and weaknesses, we propose a Graph Self-Distillation on Neighborhood (GSDN) framework to reduce the gap between GNNs and MLPs. Specifically, the GSDN framework is based purely on MLPs, where structural information is only implicitly used as prior to guide knowledge self-distillation between the neighborhood and the target, substituting th
    
[^191]: 通过冗余性实现稀疏性：用SGD求解$L_1$

    Sparsity by Redundancy: Solving $L_1$ with SGD. (arXiv:2210.01212v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01212](http://arxiv.org/abs/2210.01212)

    该论文提出了一种通过冗余重参数化和简单的随机梯度下降来最小化带有$L_1$惩罚的通用可微损失函数的方法，称为\textit{spred}，是$L_1$的精确求解器，可用于训练稀疏神经网络以执行基因选择任务和神经网络压缩任务，弥合了深度学习中的稀疏性和传统统计学习之间的差距。

    This paper proposes a method called "spred" to minimize a generic differentiable loss function with $L_1$ penalty using redundant reparametrization and straightforward stochastic gradient descent. It is an exact solver of $L_1$ and can be used to train sparse neural networks for gene selection tasks and neural network compression tasks, bridging the gap between sparsity in deep learning and conventional statistical learning.

    我们提出了一种通过冗余重参数化和简单的随机梯度下降来最小化带有$L_1$惩罚的通用可微损失函数的方法。我们的提议是$L_1$惩罚等价于带有权重衰减的可微重参数化的直接推广。我们证明了所提出的方法，即\textit{spred}，是$L_1$的精确求解器，并且对于通用的非凸函数，重参数化技巧是完全“良性”的。在实践中，我们展示了该方法的实用性，包括(1)训练稀疏神经网络以执行基因选择任务，其中涉及在非常高维空间中找到相关特征，以及(2)神经网络压缩任务，先前尝试应用$L_1$惩罚的方法均未成功。从概念上讲，我们的结果弥合了深度学习中的稀疏性和传统统计学习之间的差距。

    We propose to minimize a generic differentiable loss function with $L_1$ penalty with a redundant reparametrization and straightforward stochastic gradient descent. Our proposal is the direct generalization of a series of previous ideas that the $L_1$ penalty may be equivalent to a differentiable reparametrization with weight decay. We prove that the proposed method, \textit{spred}, is an exact solver of $L_1$ and that the reparametrization trick is completely ``benign" for a generic nonconvex function. Practically, we demonstrate the usefulness of the method in (1) training sparse neural networks to perform gene selection tasks, which involves finding relevant features in a very high dimensional space, and (2) neural network compression task, to which previous attempts at applying the $L_1$-penalty have been unsuccessful. Conceptually, our result bridges the gap between the sparsity in deep learning and conventional statistical learning.
    
[^192]: 多模态编码器遭受数据污染攻击

    Data Poisoning Attacks Against Multimodal Encoders. (arXiv:2209.15266v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2209.15266](http://arxiv.org/abs/2209.15266)

    本文研究了多模态编码器遭受数据污染攻击的情况，提出了三种攻击类型，并发现同时进行视觉和语言模态污染的攻击能够实现较好攻击性能。

    

    近年来，结合视觉和语言模态进行训练的多模态模型已经引起了越来越多的关注。然而，从大规模无标签数据集中学习也会使模型面临潜在的数据污染攻击风险，攻击者旨在扰动模型的训练数据以触发恶意行为。本文针对多模态模型的视觉和语言模态进行了数据污染攻击的探究，特别关注两个问题：（1）语言模态是否也容易受到数据污染攻击？（2）哪种模态最容易受到攻击？为了回答这两个问题，我们提出了三种多模态攻击类型。在不同的数据集和模型架构上进行了全面的评估，结果显示所有三种攻击都能够在维持模型准确性的同时，成功地实现了显著的攻击性能。

    Recently, the newly emerged multimodal models, which leverage both visual and linguistic modalities to train powerful encoders, have gained increasing attention. However, learning from a large-scale unlabeled dataset also exposes the model to the risk of potential poisoning attacks, whereby the adversary aims to perturb the model's training data to trigger malicious behaviors in it. In contrast to previous work, only poisoning visual modality, in this work, we take the first step to studying poisoning attacks against multimodal models in both visual and linguistic modalities. Specially, we focus on answering two questions: (1) Is the linguistic modality also vulnerable to poisoning attacks? and (2) Which modality is most vulnerable? To answer the two questions, we propose three types of poisoning attacks against multimodal models. Extensive evaluations on different datasets and model architectures show that all three attacks can achieve significant attack performance while maintaining 
    
[^193]: 从部分 episode 中学习 GFlowNets 以改善收敛性和稳定性

    Learning GFlowNets from partial episodes for improved convergence and stability. (arXiv:2209.12782v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.12782](http://arxiv.org/abs/2209.12782)

    本文提出了一种 GFlowNets 训练目标——子轨迹平衡(SubTB($\lambda$))，从部分 episode 学习的方式可以加速采样器在环境中的收敛速度，并使得在之前难以训练的长动作序列和奖励稀疏的环境中也能够训练 GFlowNets。

    

    生成流网络(GFlowNets)是一类算法，用于在未归一化的目标密度下对离散对象进行顺序采样训练，并已成功应用于各种概率建模任务中。现有的GFlowNets训练目标既可以局部关注状态或转换，也可以在整个采样轨迹上传播奖励信号。我们认为这些替代方案代表了梯度偏差—方差平衡的两端，并提出了一种利用这种平衡来减轻其有害影响的方法。受强化学习中的 TD($\lambda$)算法的启发，我们引入了子轨迹平衡(SubTB($\lambda$))，一种 GFlowNets 训练目标，可以从不同长度的部分动作子序列中学习。我们展示了 SubTB($\lambda$) 加速了在先前研究过的和新的环境中的采样器收敛，并使得在动作序列更长、奖励更稀疏的环境中训练 GFlowNets 成为了可能。

    Generative flow networks (GFlowNets) are a family of algorithms for training a sequential sampler of discrete objects under an unnormalized target density and have been successfully used for various probabilistic modeling tasks. Existing training objectives for GFlowNets are either local to states or transitions, or propagate a reward signal over an entire sampling trajectory. We argue that these alternatives represent opposite ends of a gradient bias-variance tradeoff and propose a way to exploit this tradeoff to mitigate its harmful effects. Inspired by the TD($\lambda$) algorithm in reinforcement learning, we introduce subtrajectory balance or SubTB($\lambda$), a GFlowNet training objective that can learn from partial action subsequences of varying lengths. We show that SubTB($\lambda$) accelerates sampler convergence in previously studied and new environments and enables training GFlowNets in environments with longer action sequences and sparser reward landscapes than what was poss
    
[^194]: 对基于分数的生成模型的保守性质的研究

    On Investigating the Conservative Property of Score-Based Generative Models. (arXiv:2209.12753v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.12753](http://arxiv.org/abs/2209.12753)

    本文研究基于分数的生成模型的保守性质，发现现有的模型不足以满足其要求，提出了一种准保守基于分数的模型，能够在保持原有模型优势的同时，有效地集成训练目标。

    

    现有的基于分数的模型可以根据其参数化方法分为约束型基于分数的模型（CSBM）或非约束型基于分数的模型（USBM）。CSBM将概率密度函数建模为玻尔兹曼分布，并将其预测作为一些标量值能量函数的负梯度。另一方面，USBM采用灵活的架构，能够直接估计分数而无需显式建模能量函数。本文证明了CSBM的架构限制可能会限制其建模能力。此外，我们还表明，USBM无法保持保守性质可能会在实践中导致性能下降。为解决上述问题，我们提出了准保守基于分数的模型（QCSBM）以保持CSBM和USBM的优势。我们的理论推导证明了QCSBM的训练目标可以通过利用两者的优势并高效地集成到训练过程中。

    Existing Score-Based Models (SBMs) can be categorized into constrained SBMs (CSBMs) or unconstrained SBMs (USBMs) according to their parameterization approaches. CSBMs model probability density functions as Boltzmann distributions, and assign their predictions as the negative gradients of some scalar-valued energy functions. On the other hand, USBMs employ flexible architectures capable of directly estimating scores without the need to explicitly model energy functions. In this paper, we demonstrate that the architectural constraints of CSBMs may limit their modeling ability. In addition, we show that USBMs' inability to preserve the property of conservativeness may lead to degraded performance in practice. To address the above issues, we propose Quasi-Conservative Score-Based Models (QCSBMs) for keeping the advantages of both CSBMs and USBMs. Our theoretical derivations demonstrate that the training objective of QCSBMs can be efficiently integrated into the training processes by lever
    
[^195]: 具有线性梯度开销的强健协同学习

    Robust Collaborative Learning with Linear Gradient Overhead. (arXiv:2209.10931v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.10931](http://arxiv.org/abs/2209.10931)

    MoNNA算法使用Polyak的局部梯度动量和最近邻平均方法，可在标准假设下证明鲁棒性，并且梯度计算开销与错误节点的比例成线性关系。

    

    协同学习算法，如分布式SGD（或D-SGD），很容易受到存在软件或硬件缺陷、有毒数据或恶意行为的故障节点的影响。针对这些问题，已经提出了许多解决方案，但以前的工作要么依赖于强假设（信任服务器，同质数据，特定噪声模型），要么施加的梯度计算成本比D-SGD高几个数量级。本文提出了MoNNA算法，具有以下特点：（a）在标准假设下可证明鲁棒性；（b）梯度计算开销与错误节点比例成线性关系，这被认为是最紧的。关键思想是使用Polyak的局部梯度动量进行本地更新，并使用最近邻平均（NNA）进行全局混合。虽然MoNNA算法相对简单易实现，但其分析更具挑战性，依赖于两个关键条件。

    Collaborative learning algorithms, such as distributed SGD (or D-SGD), are prone to faulty machines that may deviate from their prescribed algorithm because of software or hardware bugs, poisoned data or malicious behaviors. While many solutions have been proposed to enhance the robustness of D-SGD to such machines, previous works either resort to strong assumptions (trusted server, homogeneous data, specific noise model) or impose a gradient computational cost that is several orders of magnitude higher than that of D-SGD. We present MoNNA, a new algorithm that (a) is provably robust under standard assumptions and (b) has a gradient computation overhead that is linear in the fraction of faulty machines, which is conjectured to be tight. Essentially, MoNNA uses Polyak's momentum of local gradients for local updates and nearest-neighbor averaging (NNA) for global mixing, respectively. While MoNNA is rather simple to implement, its analysis has been more challenging and relies on two key 
    
[^196]: 语言符号的表现：基于示范的人机交互中，体感手语手指拼写的翻译机器人获取

    Signs of Language: Embodied Sign Language Fingerspelling Acquisition from Demonstrations for Human-Robot Interaction. (arXiv:2209.05135v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2209.05135](http://arxiv.org/abs/2209.05135)

    本文提出了一种从视频示例学习手语拼写在机器人中的实现的方法。通过训练模仿运动的策略，利用预训练的深度视觉模型从RGB视频中提取手的三维姿态，并识别出最佳的模仿超参数集，本文成功展示了方法的普适性。

    

    学习机器人中细致的动作是一个具挑战性的问题，特别是在机器人手的上下文中。本文提出一种方法，通过视频示例学习无额外信息下的熟练运动模仿，以获得手语拼写在机器人中的实现。我们首先建立了一个机器人手的URDF模型，并使每个关节只有一个致动器。然后我们利用预训练的深度视觉模型从RGB视频中提取手的三维姿态。接着，我们利用最先进的强化学习算法(即近端策略优化和软演员-评论家算法)来训练一种能够复制示范运动的策略。我们基于参考运动识别出最佳的模仿超参数集。最后，我们通过对六个对应于拼写字母的不同任务进行测试，证明了我们方法的普适性。

    Learning fine-grained movements is a challenging topic in robotics, particularly in the context of robotic hands. One specific instance of this challenge is the acquisition of fingerspelling sign language in robots. In this paper, we propose an approach for learning dexterous motor imitation from video examples without additional information. To achieve this, we first build a URDF model of a robotic hand with a single actuator for each joint. We then leverage pre-trained deep vision models to extract the 3D pose of the hand from RGB videos. Next, using state-of-the-art reinforcement learning algorithms for motion imitation (namely, proximal policy optimization and soft actor-critic), we train a policy to reproduce the movement extracted from the demonstrations. We identify the optimal set of hyperparameters for imitation based on a reference motion. Finally, we demonstrate the generalizability of our approach by testing it on six different tasks, corresponding to fingerspelled letters.
    
[^197]: 张量积与近似正交码的图嵌入方法

    Graph Embeddings via Tensor Products and Approximately Orthonormal Codes. (arXiv:2208.10917v4 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2208.10917](http://arxiv.org/abs/2208.10917)

    本文介绍了一种嵌入图形到向量空间的方法，使用张量积以及球形码实现高效压缩和表征，在稀疏图表示和其他应用中具有潜在技术优势。

    

    我们分析了一种以保持结构方式来嵌入图形的方法，展示了其丰富的表征能力并建立了一些理论性质。我们的过程属于绑定和求和方法，并且我们显示了张量积是尊重叠加原理的最一般的绑定操作。我们还建立了一些精确的结果对我们方法的行为进行了表征，并且我们证明我们使用的球形码实现了一个装箱上限。我们建立了与邻接矩阵的联系，表明我们的方法在某种意义上是一种邻接矩阵的压缩，具有稀疏图表示的应用。

    We analyze a method for embedding graphs as vectors in a structure-preserving manner, showcasing its rich representational capacity and establishing some of its theoretical properties. Our procedure falls under the bind-and-sum approach, and we show that the tensor product is the most general binding operation that respects the superposition principle. We also establish some precise results characterizing the behavior of our method, and we show that our use of spherical codes achieves a packing upper bound. We establish a link to adjacency matrices, showing that our method is, in some sense, a compression of adjacency matrices with applications towards sparse graph representations.
    
[^198]: 临床试验中受益人群的自适应识别：机器学习挑战与解决方案

    Adaptive Identification of Populations with Treatment Benefit in Clinical Trials: Machine Learning Challenges and Solutions. (arXiv:2208.05844v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2208.05844](http://arxiv.org/abs/2208.05844)

    本文提出了一种新的数据驱动的方法，同时使用了机器学习和自适应实验技术，用于自适应性临床试验中受益于给定治疗的患者亚群的识别。本文通过实验展示了该方法的优异表现，并解决了这一问题的独特挑战，产生了有效和有用的发现。

    

    本文研究了在确认性临床试验中自适应性地识别受益于给定治疗的患者亚群的问题。这种自适应性临床试验已经在生物统计学中得到了深入研究，但目前仅允许有限的自适应性。本文旨在放宽这样的设计的经典限制，并研究如何融合最近机器学习文献中的自适应和在线实验的思想，使试验更加灵活和高效。我们发现，亚群选择问题的独特特征——最重要的是，（i）通常感兴趣的是在有限的预算下找到任何受益于治疗的亚群（而不一定是单个效果最大的亚组），以及（ii）只需在平均水平上证明有效性——催生了有趣的挑战和设计算法解决方案的新要求。基于这些发现，我们提出了一种新的数据驱动的子群体识别方法，同时使用了机器学习和自适应实验技术。我们进行了大量的模拟和实际数据分析，展示了我们方法不仅表现优异，而且解决了这一问题的独特挑战，产生了有效和有用的发现。

    We study the problem of adaptively identifying patient subpopulations that benefit from a given treatment during a confirmatory clinical trial. This type of adaptive clinical trial has been thoroughly studied in biostatistics, but has been allowed only limited adaptivity so far. Here, we aim to relax classical restrictions on such designs and investigate how to incorporate ideas from the recent machine learning literature on adaptive and online experimentation to make trials more flexible and efficient. We find that the unique characteristics of the subpopulation selection problem -- most importantly that (i) one is usually interested in finding subpopulations with any treatment benefit (and not necessarily the single subgroup with largest effect) given a limited budget and that (ii) effectiveness only has to be demonstrated across the subpopulation on average -- give rise to interesting challenges and new desiderata when designing algorithmic solutions. Building on these findings, we 
    
[^199]: D3Former: 无偏差双重蒸馏变形器用于增量学习

    D3Former: Debiased Dual Distilled Transformer for Incremental Learning. (arXiv:2208.00777v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.00777](http://arxiv.org/abs/2208.00777)

    本文提出了D3Former，一种用于类别增量学习的无偏差双重蒸馏Transformer。D3Former利用混合嵌套ViT设计，不会动态扩展其架构，并通过偏差校正模块和新的蒸馏目标来改进其CIL行为。

    

    在类别增量学习（CIL）设置中，每次学习阶段向模型引入一组类别，目标是学习一个统一的模型，在所有观测到的类别上表现良好。考虑到Vision Transformers（ViTs）在传统分类设置中的普及，在本文中，我们开发了一种用于CIL的无偏差双重蒸馏Transformer，称为D3Former。所提出的模型利用混合嵌套ViT设计，以确保对小型和大型数据集的数据效率和可伸缩性。与最近的基于ViT的CIL方法相比，我们的D3Former在学习新的任务时不会动态扩展其架构，并且适用于大量增量任务。D3Former行为的改进归功于ViT设计的两个基本变化。首先，我们将增量学习视为长尾分类问题，引入了偏差校正模块，缓解了旧类别和新类别之间的不平衡。其次，我们提出了一种新的蒸馏目标，利用师生ViT之间的知识传输，确保更好地推广到新任务。

    In class incremental learning (CIL) setting, groups of classes are introduced to a model in each learning phase. The goal is to learn a unified model performant on all the classes observed so far. Given the recent popularity of Vision Transformers (ViTs) in conventional classification settings, an interesting question is to study their continual learning behaviour. In this work, we develop a Debiased Dual Distilled Transformer for CIL dubbed $\textrm{D}^3\textrm{Former}$. The proposed model leverages a hybrid nested ViT design to ensure data efficiency and scalability to small as well as large datasets. In contrast to a recent ViT based CIL approach, our $\textrm{D}^3\textrm{Former}$ does not dynamically expand its architecture when new tasks are learned and remains suitable for a large number of incremental tasks. The improved CIL behaviour of $\textrm{D}^3\textrm{Former}$ owes to two fundamental changes to the ViT design. First, we treat the incremental learning as a long-tail classi
    
[^200]: 使用单步 Q-learning 缓解 Actor-Critic 方法中的离策略偏差：一种新的纠正方法。

    Mitigating Off-Policy Bias in Actor-Critic Methods with One-Step Q-learning: A Novel Correction Approach. (arXiv:2208.00755v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.00755](http://arxiv.org/abs/2208.00755)

    本文提出一种新的策略相似度量来缓解离策略学习中的偏差问题，提供了一种自适应的、可扩展的解决方案。

    

    相较于基于策略的对比方法，离策略无模型深度强化学习可以通过重复使用以前收集的数据来提高数据使用效率。然而，当代理的策略和收集到的数据的基本分布之间的偏差增加时，离策略学习变得具有挑战性。尽管已经研究了重要性采样和离策略策略梯度技术来补偿这种偏差，但它们通常需要一系列长轨迹，并导致额外的问题，如消失/爆炸梯度或抛弃许多有用的经验，最终增加了计算复杂性。此外，它们对连续动作域或由确定性深度神经网络逼近的策略的泛化受到严格限制。为了克服这些限制，我们引入了一种新的策略相似度量来缓解连续控制中这种偏差的影响。我们的方法提供了一种自适应的、可扩展的解决方案，用于减轻 Actor-Critic 方法中离政策偏差的影响。

    Compared to on-policy counterparts, off-policy model-free deep reinforcement learning can improve data efficiency by repeatedly using the previously gathered data. However, off-policy learning becomes challenging when the discrepancy between the underlying distributions of the agent's policy and collected data increases. Although the well-studied importance sampling and off-policy policy gradient techniques were proposed to compensate for this discrepancy, they usually require a collection of long trajectories and induce additional problems such as vanishing/exploding gradients or discarding many useful experiences, which eventually increases the computational complexity. Moreover, their generalization to either continuous action domains or policies approximated by deterministic deep neural networks is strictly limited. To overcome these limitations, we introduce a novel policy similarity measure to mitigate the effects of such discrepancy in continuous control. Our method offers an ad
    
[^201]: 应用元学习的神经常微分方程实现自适应异步控制

    Adaptive Asynchronous Control Using Meta-learned Neural Ordinary Differential Equations. (arXiv:2207.12062v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.12062](http://arxiv.org/abs/2207.12062)

    通过元学习自适应动力学模型实现了一种通用框架，可以适应不规则/异步观察和行动，并在不同的episode之间产生巨大的环境动力学变化，这可以被应用于机器人控制。

    

    基于模型的强化学习和控制在包括机器人环境在内的各种顺序决策问题领域展示了巨大的潜力。然而，真实世界的机器人系统常常存在一些限制这些方法的应用。我们提出了一个可以适应不规则/异步观察和行动以及在不同episode之间产生巨大的环境动力学变化（例如，不同的载荷惯性特性）的元学习自适应动力学模型的通用框架，该方法是任务无关的，并且可以简单地适应新任务。我们在两个不同的机器人模拟和一个真实的工业机器人上进行了评估。

    Model-based Reinforcement Learning and Control have demonstrated great potential in various sequential decision making problem domains, including in robotics settings. However, real-world robotics systems often present challenges that limit the applicability of those methods. In particular, we note two problems that jointly happen in many industrial systems: 1) Irregular/asynchronous observations and actions and 2) Dramatic changes in environment dynamics from an episode to another (e.g. varying payload inertial properties). We propose a general framework that overcomes those difficulties by meta-learning adaptive dynamics models for continuous-time prediction and control. The proposed approach is task-agnostic and can be adapted to new tasks in a straight-forward manner. We present evaluations in two different robot simulations and on a real industrial robot.
    
[^202]: 一种基于监督张量降维的不完整成像数据的预测模型

    A Supervised Tensor Dimension Reduction-Based Prognostics Model for Applications with Incomplete Imaging Data. (arXiv:2207.11353v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.11353](http://arxiv.org/abs/2207.11353)

    本文提出了一种基于监督张量降维的预测模型，能处理不完整的成像数据，利用失效时间监督提取低维特征，提高了预测的准确性。

    

    本文提出了一种用于张量数据的监督降维方法，与大多数基于图像的预测模型相比具有两个优势。首先，该模型不要求张量数据完整，从而扩展了其应用范围；其次，利用失效时间（TTF）来监督提取低维特征，使提取的特征更有效地用于后续预测。此外，本文为参数估计提出了一种优化算法，并在特定分布下推导出了闭式解。

    This paper proposes a supervised dimension reduction methodology for tensor data which has two advantages over most image-based prognostic models. First, the model does not require tensor data to be complete which expands its application to incomplete data. Second, it utilizes time-to-failure (TTF) to supervise the extraction of low-dimensional features which makes the extracted features more effective for the subsequent prognostic. Besides, an optimization algorithm is proposed for parameter estimation and closed-form solutions are derived under certain distributions.
    
[^203]: 具有应用于安全强化学习的安全黑匣子优化的对数障碍

    Log Barriers for Safe Black-box Optimization with Application to Safe Reinforcement Learning. (arXiv:2207.10415v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2207.10415](http://arxiv.org/abs/2207.10415)

    提出了一种名为LB-SGD的方法，它基于对原始问题的对数障碍近似，并应用随机梯度下降。此方法可用于使学习过程中保持安全至关重要的高维非线性随机优化问题，包括一阶和零阶反馈的非凸、凸和强凸平滑约束问题的完全收敛分析。

    

    在制造业、机器人和许多其他领域中，优化嘈杂函数是一个关键的任务，因为在部署系统上评估目标函数需要进行实验。通常，我们事先不知道安全输入的限制条件，我们只能得到嘈杂的信息，指示我们距离违反约束条件的距离有多远。然而，安全在整个学习过程中必须保证，而不仅仅是在算法的最终输出上。

    Optimizing noisy functions online, when evaluating the objective requires experiments on a deployed system, is a crucial task arising in manufacturing, robotics and many others. Often, constraints on safe inputs are unknown ahead of time, and we only obtain noisy information, indicating how close we are to violating the constraints. Yet, safety must be guaranteed at all times, not only for the final output of the algorithm.  We introduce a general approach for seeking a stationary point in high dimensional non-linear stochastic optimization problems in which maintaining safety during learning is crucial. Our approach called LB-SGD is based on applying stochastic gradient descent (SGD) with a carefully chosen adaptive step size to a logarithmic barrier approximation of the original problem. We provide a complete convergence analysis of non-convex, convex, and strongly-convex smooth constrained problems, with first-order and zeroth-order feedback. Our approach yields efficient updates an
    
[^204]: Swin Transformer 深度强化学习

    Deep Reinforcement Learning with Swin Transformers. (arXiv:2206.15269v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.15269](http://arxiv.org/abs/2206.15269)

    本文介绍了基于 Swin Transformer 的在线强化学习方案 Swin DQN，通过将组合的图像像素分成小的补丁并在局部应用自我注意力操作，实现了在 Atari 基准测试上超越现有基于 CNN 的强化学习方法的最先进性能。

    

    Transformer 是神经网络模型，利用多层自我注意力头，并在自然语言处理任务中展示了巨大的潜力。同时，有人努力将 Transformer 适应于机器学习的视觉任务，包括 Vision Transformer 和 Swin Transformer。虽然一些研究人员将 Vision Transformer 用于强化学习任务，但由于高计算成本，实验仍停留在小规模。另一方面，大规模的实验必须依赖于技术来减少 Vision Transformer 的成本，这也会产生较差的结果。为了解决这一挑战，本文提出了基于 Swin Transformer 的第一个在线强化学习方案：Swin DQN。Swin Transformer 可作为神经网络中的主干骨干，将图像像素的组合分成小的补丁，并在固定大小的窗口内应用局部自我注意力操作。它们在 ImageNet 分类任务中表现卓越。Swin DQN 在 Atari 基准测试中实现了最先进的性能，优于现有的基于 CNN 的强化学习方法。

    Transformers are neural network models that utilize multiple layers of self-attention heads and have exhibited enormous potential in natural language processing tasks. Meanwhile, there have been efforts to adapt transformers to visual tasks of machine learning, including Vision Transformers and Swin Transformers. Although some researchers use Vision Transformers for reinforcement learning tasks, their experiments remain at a small scale due to the high computational cost. Experiments conducted at a large scale, on the other hand, have to rely on techniques to cut the costs of Vision Transformers, which also yield inferior results.  To address this challenge, this article presents the first online reinforcement learning scheme that is based on Swin Transformers: Swin DQN. Swin Transformers are promising as a backbone in neural networks by splitting groups of image pixels into small patches and applying local self-attention operations inside the (shifted) windows of fixed sizes. They hav
    
[^205]: 评估生成专利语言模型

    Evaluating Generative Patent Language Models. (arXiv:2206.14578v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.14578](http://arxiv.org/abs/2206.14578)

    本文构建了生成性专利语言模型，并通过基于人类节省按键数量的度量方法评估了其表现，发现在专利领域继续增加模型大小可能是不必要的。

    

    生成性语言模型在各个领域辅助人类写作方面具有很大的潜力。本文旨在构建专利领域的生成性语言模型，并从人类的角度评估模型的表现。我们通过度量基于生成性专利语言模型的自动补全所能节省的按键比率来衡量模型的表现，这种以按键为基础的度量方法不同于传统的基于标记的机器-centric的度量方法。本文中构建的最大模型大小为6B，在专利领域是最先进的。基于此度量方法，我们发现最大的模型并不一定是人类-centric度量方法的最佳选择，这意味着如果目的是通过自动补全来辅助人类写作，那么在专利领域继续增加模型大小可能是不必要的。

    Generative language models are promising for assisting human writing in various domains. This manuscript aims to build generative language models in the patent domain and evaluate model performance from a human-centric perspective. The perspective is to measure the ratio of keystrokes that can be saved by autocompletion based on generative patent language models. A higher ratio means a more effective model which can save more keystrokes. This metric can be used to benchmark model performance. The metric is different from conventional machine-centric metrics that are token-based instead of keystroke-based. In terms of model size, the largest model built in this manuscript is 6B, which is state-of-the-art in the patent domain. Based on the metric, it is found that the largest model is not necessarily the best for the human-centric metric. The finding means that keeping increasing model sizes in the patent domain might be unnecessary if the purpose is to assist human writing with autocomp
    
[^206]: 具有数据和客户端异质性的通信高效的联邦学习

    Communication-Efficient Federated Learning With Data and Client Heterogeneity. (arXiv:2206.10032v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.10032](http://arxiv.org/abs/2206.10032)

    本文提出了适用于具有数据和客户端异质性的通信高效的联邦学习算法，并通过实验结果验证了其在标准联合任务的快速收敛性，以及在有趣的参数范围内可以提供类似于经典联邦平均算法的收敛性。

    

    联邦学习（FL）允许机器学习模型进行大规模分布式训练，同时仍允许各个节点保持本地数据。然而，进行大规模FL时存在固有的实用挑战：1）局部节点数据分布的异质性，2）节点计算速度（异步性）的异质性，以及3）客户端和服务器之间通信的限制。在本文中，我们提出了第一种经典联邦平均算法（FedAvg）的变体，同时支持数据异质性、部分客户端异步性和通信压缩。我们的算法提供了严密的分析，表明尽管存在这些系统放宽，它仍然可以在有趣的参数范围内提供类似于FedAvg的收敛性。在多达300个节点的严格LEAF基准测试方案中的实验结果显示，我们的算法确保了标准联合任务的快速收敛，提高了

    Federated Learning (FL) enables large-scale distributed training of machine learning models, while still allowing individual nodes to maintain data locally.  However, executing FL at scale comes with inherent practical challenges:  1) heterogeneity of the local node data distributions,  2) heterogeneity of node computational speeds (asynchrony),  but also 3) constraints in the amount of communication between the clients and the server.  In this work, we present the first variant of the classic federated averaging (FedAvg) algorithm  which, at the same time, supports data heterogeneity, partial client asynchrony, and communication compression.  Our algorithm comes with a rigorous analysis showing that, in spite of these system relaxations,  it can provide similar convergence to FedAvg in interesting parameter regimes.  Experimental results in the rigorous LEAF benchmark on setups of up to $300$ nodes show that our algorithm ensures fast convergence for standard federated tasks, improvin
    
[^207]: 平移不变核函数的正交展开

    Orthonormal Expansions for Translation-Invariant Kernels. (arXiv:2206.08648v3 [math.CA] UPDATED)

    [http://arxiv.org/abs/2206.08648](http://arxiv.org/abs/2206.08648)

    该论文提出了一种傅里叶分析技术，用于从$\mathscr{L}_2(\mathbb{R})$的正交基中构建平移不变核函数的正交基展开，实现了马特尔核函数、柯西核函数和高斯核函数的明确展开表达式。

    

    我们提出了一种用于构建平移不变核函数的正交基展开的傅里叶分析技术，该技术利用$\mathscr{L}_2(\mathbb{R})$上的正交基，得到了实轴上所有半整数阶马特尔核函数、柯西核函数以及高斯核函数的明确展开表达式，分别由相关的拉盖尔函数、有理函数和厄米函数表示。

    We present a general Fourier analytic technique for constructing orthonormal basis expansions of translation-invariant kernels from orthonormal bases of $\mathscr{L}_2(\mathbb{R})$. This allows us to derive explicit expansions on the real line for (i) Mat\'ern kernels of all half-integer orders in terms of associated Laguerre functions, (ii) the Cauchy kernel in terms of rational functions, and (iii) the Gaussian kernel in terms of Hermite functions.
    
[^208]: 基于分数的异分布生成探索化学空间

    Exploring Chemical Space with Score-based Out-of-distribution Generation. (arXiv:2206.07632v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2206.07632](http://arxiv.org/abs/2206.07632)

    MOOD是一种基于分数的异分布扩散策略，它通过属性预测器的梯度进行条件生成，从而使得逆向时间扩散过程通过指导目标特性到高分数区域，从而允许我们搜索新颖且有意义的分子。

    

    现有的分子生成模型的一个众所周知的局限性是生成的分子与训练集中的分子高度相似。为了生成全新的分子以寻找更好的新颖药物，需要更强大的化学空间探索技术。为此，我们提出了分数-based异分布扩散策略(MOOD)，该策略在随机微分方程(SDE)的生成中结合了异分布(OOD)控制，同时通过简单的超参数控制而无需额外的成本。由于一些新颖分子可能无法满足现实药物的基本要求，MOOD利用属性预测器的梯度进行条件生成，从而使得逆向时间扩散过程通过指导目标特性（如蛋白质-配体相互作用、药物样性和可合成性）到高分数区域，从而允许MOOD搜索新颖且有意义的分子。

    A well-known limitation of existing molecular generative models is that the generated molecules highly resemble those in the training set. To generate truly novel molecules that may have even better properties for de novo drug discovery, more powerful exploration in the chemical space is necessary. To this end, we propose Molecular Out-Of-distribution Diffusion(MOOD), a score-based diffusion scheme that incorporates out-of-distribution (OOD) control in the generative stochastic differential equation (SDE) with simple control of a hyperparameter, thus requires no additional costs. Since some novel molecules may not meet the basic requirements of real-world drugs, MOOD performs conditional generation by utilizing the gradients from a property predictor that guides the reverse-time diffusion process to high-scoring regions according to target properties such as protein-ligand interactions, drug-likeness, and synthesizability. This allows MOOD to search for novel and meaningful molecules r
    
[^209]: 元最优输运

    Meta Optimal Transport. (arXiv:2206.05262v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.05262](http://arxiv.org/abs/2206.05262)

    本文提出了一种新的方法，利用过去问题的知识和信息来迅速预测和解决新问题，重复地解决不同度量之间的类似OT问题，从而改善最优输运问题的计算时间。

    

    我们研究了使用分摊优化来预测最优输运（OT）地图的方法，我们称之为元OT。这有助于通过利用过去问题的知识和信息来迅速预测和解决新问题，从而重复地解决不同度量之间的类似OT问题。否则，标准方法会忽略过去解决方案的知识，从头开始次优地重新解决每个问题。我们在灰度图像、球形数据、分类标签和颜色调色板之间实例化元OT模型，并使用它们来改善标准OT求解器的计算时间。我们的源代码可在此http URL找到。

    We study the use of amortized optimization to predict optimal transport (OT) maps from the input measures, which we call Meta OT. This helps repeatedly solve similar OT problems between different measures by leveraging the knowledge and information present from past problems to rapidly predict and solve new problems. Otherwise, standard methods ignore the knowledge of the past solutions and suboptimally re-solve each problem from scratch. We instantiate Meta OT models in discrete and continuous settings between grayscale images, spherical data, classification labels, and color palettes and use them to improve the computational time of standard OT solvers. Our source code is available at this http URL
    
[^210]: XAudit：审计与解释的理论研究

    XAudit : A Theoretical Look at Auditing with Explanations. (arXiv:2206.04740v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.04740](http://arxiv.org/abs/2206.04740)

    本文研究了如何通过解释来审计机器学习模型。作者提出了一种基于因果推断解释的算法来审计线性分类器和决策树的特征敏感性，结果表明该算法在审计中非常有帮助。

    

    为了负责地使用机器学习，需要对模型进行审计以避免不良后果。尽管已经有相关工作提出了使用解释进行审计，但如何进行以及为什么进行尚未得到很好的理解。本文 formalizes 解释在审计中的作用，并研究模型解释如何帮助审计。具体而言，我们提出了一种基于解释的算法，用于审计线性分类器和决策树的特征敏感性。我们的结果表明，因果推断解释在审计中非常有帮助。虽然 Anchors 和 decision paths 在最坏情况下可能不太有益，但在平均情况下它们确实很有帮助。

    Responsible use of machine learning requires models to be audited for undesirable properties. While a body of work has proposed using explanations for auditing, how to do so and why has remained relatively ill-understood. This work formalizes the role of explanations in auditing and investigates if and how model explanations can help audits. Specifically, we propose explanation-based algorithms for auditing linear classifiers and decision trees for feature sensitivity. Our results illustrate that Counterfactual explanations are extremely helpful for auditing. While Anchors and decision paths may not be as beneficial in the worst-case, in the average-case they do aid a lot.
    
[^211]: 强化学习中动作噪声对探索和性能的影响

    Action Noise in Off-Policy Deep Reinforcement Learning: Impact on Exploration and Performance. (arXiv:2206.03787v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.03787](http://arxiv.org/abs/2206.03787)

    本文研究了动作噪声类型、噪声比例和减少规模因子对深度强化学习离线学习中策略性能和探索效果的影响，提出了一种更具鲁棒性的状态空间覆盖度量。

    

    许多深度强化学习算法依赖于简单的探索形式，如连续控制领域常用的加性动作噪声。通常，在培训期间保持动作噪声的比例不变。本文侧重于连续控制的离线深度强化学习中的动作噪声。我们分析了噪声类型、噪声比例和减少规模因子的影响。我们考虑了两种最常见的动作噪声类型，高斯噪声和 Ornstein-Uhlenbeck 噪声，并通过系统地改变噪声类型和比例参数进行了大量实验研究，并通过测量策略的预期回报和探索期间的状态空间覆盖等有趣的变量来评估结果。对于后者，我们提出了一种新的状态空间覆盖度量X_𝒰rel，该方法对估计引起的估计误差具有更强的鲁棒性。

    Many Deep Reinforcement Learning (D-RL) algorithms rely on simple forms of exploration such as the additive action noise often used in continuous control domains. Typically, the scaling factor of this action noise is chosen as a hyper-parameter and is kept constant during training. In this paper, we focus on action noise in off-policy deep reinforcement learning for continuous control. We analyze how the learned policy is impacted by the noise type, noise scale, and impact scaling factor reduction schedule. We consider the two most prominent types of action noise, Gaussian and Ornstein-Uhlenbeck noise, and perform a vast experimental campaign by systematically varying the noise type and scale parameter, and by measuring variables of interest like the expected return of the policy and the state-space coverage during exploration. For the latter, we propose a novel state-space coverage measure $\operatorname{X}_{\mathcal{U}\text{rel}}$ that is more robust to estimation artifacts caused by
    
[^212]: 时机至关重要：学习在代价高昂的行动和预算限制下进行选择性行动

    Timing is Everything: Learning to Act Selectively with Costly Actions and Budgetary Constraints. (arXiv:2205.15953v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.15953](http://arxiv.org/abs/2205.15953)

    LICRA是学习在代价高昂的行动和预算限制下进行选择性行动的强化学习框架。

    

    许多实际应用场景中，执行行动都会产生成本；金融系统中的交易成本和燃油成本是常见的例子。在这些情况下，每个时间步骤执行行动迅速积累成本，导致极其不理想的结果。此外，反复行动会产生磨损和最终损坏。确定“何时行动”对于实现成功的结果至关重要，然而，在行动产生最小限制成本的情况下，高效地“学习”行为最优策略的挑战仍未得到解决。本文介绍了一种强化学习（RL）框架，名为Learnable Impulse Control Reinforcement Algorithm（LICRA），用于在行动产生成本的情况下学习选择何时行动和采取哪些行动以实现最优选择。

    Many real-world settings involve costs for performing actions; transaction costs in financial systems and fuel costs being common examples. In these settings, performing actions at each time step quickly accumulates costs leading to vastly suboptimal outcomes. Additionally, repeatedly acting produces wear and tear and ultimately, damage. Determining \textit{when to act} is crucial for achieving successful outcomes and yet, the challenge of efficiently \textit{learning} to behave optimally when actions incur minimally bounded costs remains unresolved. In this paper, we introduce a reinforcement learning (RL) framework named \textbf{L}earnable \textbf{I}mpulse \textbf{C}ontrol \textbf{R}einforcement \textbf{A}lgorithm (LICRA), for learning to optimally select both when to act and which actions to take when actions incur costs. At the core of LICRA is a nested structure that combines RL and a form of policy known as \textit{impulse control} which learns to maximise objectives when actions
    
[^213]: 奖励状态机的层次化结构

    Hierarchies of Reward Machines. (arXiv:2205.15752v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.15752](http://arxiv.org/abs/2205.15752)

    本文提出了一种奖励状态机（RM）的层次化结构（HRM），利用它可以将任务进一步抽象为多个子任务，每个子任务都可以独立解决；使用 HRM 可以帮助加快收敛速度且在学习中是可行的。

    

    奖励状态机（RM）是一种新的形式化工具，用于通过一个有限状态机来表示强化学习任务的奖励函数，其边缘使用高级事件编码任务的子目标。 RM的结构使得将一个任务分解成简单和独立可解的子任务成为可能，这有助于处理长期规划和/或奖励稀疏的任务。我们提出了一种形式化工具，通过赋予RM调用其他RM的能力，从而组合一个RM的层次结构（HRM）来进一步抽象子任务结构。我们利用HRM通过将对RM的每个调用视为单独可解的子任务来使用选项框架，并描述了一种基于课程的方法来从代理观察到的轨迹中学习HRM。我们的实验表明，利用手工制作的HRM比扁平的HRM收敛更快，并且在等价的扁平表示不可行的情况下，学习HRM是可行的。

    Reward machines (RMs) are a recent formalism for representing the reward function of a reinforcement learning task through a finite-state machine whose edges encode subgoals of the task using high-level events. The structure of RMs enables the decomposition of a task into simpler and independently solvable subtasks that help tackle long-horizon and/or sparse reward tasks. We propose a formalism for further abstracting the subtask structure by endowing an RM with the ability to call other RMs, thus composing a hierarchy of RMs (HRM). We exploit HRMs by treating each call to an RM as an independently solvable subtask using the options framework, and describe a curriculum-based method to learn HRMs from traces observed by the agent. Our experiments reveal that exploiting a handcrafted HRM leads to faster convergence than with a flat HRM, and that learning an HRM is feasible in cases where its equivalent flat representation is not.
    
[^214]: 带有属性删除子网络的模块化和按需偏差缓解方法

    Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks. (arXiv:2205.15171v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.15171](http://arxiv.org/abs/2205.15171)

    提出一种新颖的模块化偏差缓解方法，在推理时间按需集成到核心模型中的独立去偏置子网络，在性别、种族和年龄等受保护属性的分类任务中，该方法在缓解偏差方面是有效的，并且在精度和灵活性方面优于现有技术方法。

    

    社会偏见反映在大型预训练语言模型及其在下游任务中的微调版本中。常见的处理偏差的方法引入了额外的优化标准，并更新模型以达到新的去偏置状态。然而，在实践中，最终用户和从业人员可能更喜欢切换回原始模型，或仅对特定子集的保护属性应用去偏置。为了实现这一点，我们提出了一种新颖的模块化偏差缓解方法，包括独立高度稀疏的去偏置子网络，其中每个去偏置模块可以在推理时间按需集成到核心模型中。我们的方法借鉴了“diff”剪枝的概念，并提出了一种适合于各种表示分离优化的新型训练方式。我们在具有性别、种族和年龄等受保护属性的三个分类任务上进行了实验。结果表明，我们的模块化方法在缓解偏差方面是有效的，并且在精度和灵活性方面优于现有技术方法。

    Societal biases are reflected in large pre-trained language models and their fine-tuned versions on downstream tasks. Common in-processing bias mitigation approaches, such as adversarial training and mutual information removal, introduce additional optimization criteria, and update the model to reach a new debiased state. However, in practice, end-users and practitioners might prefer to switch back to the original model, or apply debiasing only on a specific subset of protected attributes. To enable this, we propose a novel modular bias mitigation approach, consisting of stand-alone highly sparse debiasing subnetworks, where each debiasing module can be integrated into the core model on-demand at inference time. Our approach draws from the concept of \emph{diff} pruning, and proposes a novel training regime adaptable to various representation disentanglement optimizations. We conduct experiments on three classification tasks with gender, race, and age as protected attributes. The resul
    
[^215]: 点积核回归的精确学习曲线和高阶标度极限

    Precise Learning Curves and Higher-Order Scaling Limits for Dot Product Kernel Regression. (arXiv:2205.14846v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14846](http://arxiv.org/abs/2205.14846)

    本文细致研究了点积核岭回归问题，针对 $m\propto d^r$ 高阶标度关系提出了精确的测试误差、偏差和方差公式。

    

    随着现代机器学习模型不断推进计算前沿，开发对不同模型和数据缩放方案下预期性能提高的精确估计变得越来越重要。目前，关于描述预测误差如何随着样本数量而变化的学习曲线的理论理解受限于大样本渐近性 ($m\to\infty$) 或对于某些简单数据分布的高维渐近性，其中样本数量与维数成线性比例 ($m\propto d$)。这两个范畴之间存在很大差距，包括所有高阶标度关系 $m\propto d^r$，这是本文的研究对象。我们专注于点积核岭回归的问题，并提供了在 $m/d\rightarrow2r$ 的 $r$ 阶渐近标度下（其中 $m\to\infty$），对于从球面上均匀抽取的数据，测试误差、偏差和方差的精确公式。

    As modern machine learning models continue to advance the computational frontier, it has become increasingly important to develop precise estimates for expected performance improvements under different model and data scaling regimes. Currently, theoretical understanding of the learning curves that characterize how the prediction error depends on the number of samples is restricted to either large-sample asymptotics ($m\to\infty$) or, for certain simple data distributions, to the high-dimensional asymptotics in which the number of samples scales linearly with the dimension ($m\propto d$). There is a wide gulf between these two regimes, including all higher-order scaling relations $m\propto d^r$, which are the subject of the present paper. We focus on the problem of kernel ridge regression for dot-product kernels and present precise formulas for the test error, bias, and variance, for data drawn uniformly from the sphere in the $r$th-order asymptotic scaling regime $m\to\infty$ with $m/d
    
[^216]: 元学习器用于多值处理异质作用估计的比较

    Comparison of meta-learners for estimating multi-valued treatment heterogeneous effects. (arXiv:2205.14714v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.14714](http://arxiv.org/abs/2205.14714)

    本文探讨了利用元学习器估计多值处理异质效应的问题，发现朴素扩展并不总是可行，提出并讨论了一些表现良好的元学习器。

    

    在利用观察数据进行因果推断时，条件平均处理效应（CATE）估计是主要挑战之一。除了基于机器学习的模型外，还开发出了称为元学习器的非参数估计器以估计CATE，其主要优点是不局限于特定的监督学习方法。然而，当处理不是二进制的时，一些朴素扩展的限制会出现，这样的任务就变得更加复杂。本文研究了元学习器用于估计多值处理异质效应。我们考虑了不同的元学习器，理论分析了它们的误差上界作为重要参数的函数，例如处理水平的数量，结果显示，朴素扩展并不总是提供满意的结果。我们引入和讨论了一些元学习器，它们在处理数量增多时表现良好。通过模拟研究和一项乙肝治疗研究的真实数据示例，我们证实了元学习器的优缺点。

    Conditional Average Treatment Effects (CATE) estimation is one of the main challenges in causal inference with observational data. In addition to Machine Learning based-models, nonparametric estimators called meta-learners have been developed to estimate the CATE with the main advantage of not restraining the estimation to a specific supervised learning method. This task becomes, however, more complicated when the treatment is not binary as some limitations of the naive extensions emerge. This paper looks into meta-learners for estimating the heterogeneous effects of multi-valued treatments. We consider different meta-learners, and we carry out a theoretical analysis of their error upper bounds as functions of important parameters such as the number of treatment levels, showing that the naive extensions do not always provide satisfactory results. We introduce and discuss meta-learners that perform well as the number of treatments increases. We empirically confirm the strengths and weak
    
[^217]: 公平标记聚类

    Fair Labeled Clustering. (arXiv:2205.14358v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14358](http://arxiv.org/abs/2205.14358)

    本文提出了解决公平标记聚类问题的算法，考虑了下游应用和团体公平性的实现。

    

    针对聚类中的公平性问题，已经出现了许多基于不同公平性概念的算法。目前最常见的公平概念是团体公平性，即每个聚类中都要保证团体比例的合理性。本文在这个方向上进行了扩展，考虑了聚类任务的下游应用以及在该应用中如何实现团体公平性。我们具体研究了一种常见情况，即决策者运行聚类算法，检查每个聚类的中心并为相应的聚类确定一个适当的结果（标签），例如招聘中的“录用”或“拒绝”。在这种情况下，为了确保团体公平性，我们希望每个标签都有符合比例的团体代表，但不一定要求每个聚类都具有两个团体的平衡性。我们提出了解决此类问题的算法。

    Numerous algorithms have been produced for the fundamental problem of clustering under many different notions of fairness. Perhaps the most common family of notions currently studied is group fairness, in which proportional group representation is ensured in every cluster. We extend this direction by considering the downstream application of clustering and how group fairness should be ensured for such a setting. Specifically, we consider a common setting in which a decision-maker runs a clustering algorithm, inspects the center of each cluster, and decides an appropriate outcome (label) for its corresponding cluster. In hiring for example, there could be two outcomes, positive (hire) or negative (reject), and each cluster would be assigned one of these two outcomes. To ensure group fairness in such a setting, we would desire proportional group representation in every label but not necessarily in every cluster as is done in group fair clustering. We provide algorithms for such problems 
    
[^218]: 形式化运行时分布上的偏好

    Formalizing Preferences Over Runtime Distributions. (arXiv:2205.13028v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2205.13028](http://arxiv.org/abs/2205.13028)

    本文形式化了偏好运行时分布，提出了一种基于效用理论的替代方案来描述算法的评分函数，这些函数与随时间的推移和消费时间的分布有关。

    

    在解决计算问题时，我们通常需要在能够返回正确结果的算法之间进行选择，但这些算法的运行时分布不同（例如SAT求解器，排序算法）。本文旨在通过形式化运行时分布上的偏好为这些选择奠定理论基础。我们往往希望选择预期运行时间最短的算法。然而，这样的偏好将完全受到算法在坏输入上表现如何而影响，而在实践中，我们通常愿意在长时间的运行没有结束之前将其切断。我们提出了一个基于效用理论的可替代方案，用于描述算法偏好的评分函数。这些函数取决于随着时间的推移，问题解决的价值如何下降以及消费时间的分布。我们提出了一些真实的效用函数示例，并展示了一些优势决策算法的选择过程，并对这些函数进行了实验评估。

    When trying to solve a computational problem, we are often faced with a choice between algorithms that are guaranteed to return the right answer but differ in their runtime distributions (e.g., SAT solvers, sorting algorithms). This paper aims to lay theoretical foundations for such choices by formalizing preferences over runtime distributions. It might seem that we should simply prefer the algorithm that minimizes expected runtime. However, such preferences would be driven by exactly how slow our algorithm is on bad inputs, whereas in practice we are typically willing to cut off occasional, sufficiently long runs before they finish. We propose a principled alternative, taking a utility-theoretic approach to characterize the scoring functions that describe preferences over algorithms. These functions depend on the way our value for solving our problem decreases with time and on the distribution from which captimes are drawn. We describe examples of realistic utility functions and show 
    
[^219]: Heterformer：基于Transformer的异构文本网络深度节点表示学习

    Heterformer: Transformer-based Deep Node Representation Learning on Heterogeneous Text-Rich Networks. (arXiv:2205.10282v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.10282](http://arxiv.org/abs/2205.10282)

    本论文提出了一种名为Heterformer的基于Transformer的深度节点表示学习方法，它可以同时处理异构结构和丰富的文本语义信息。

    

    网络表示学习旨在为每个节点推导出有意义的向量表示，从而促进诸如链接预测、节点分类和节点聚类等下游任务。在异构文本网络中，由于文本的存在与缺失以及多种类型的节点和边缘形成的异构网络结构，这项任务更加具有挑战性。由于预训练语言模型（PLMs）已经证明了在获得广泛通用性文本表示方面的有效性，在文本丰富的网络表示学习中，人们已经付出了大量的努力来将PLMs包含到其中。然而，很少有研究可以有效地共同考虑异构结构（网络）信息和每个节点的丰富文本语义信息。本文提出了Heterformer，一种异构网络强化的Transformer，可同时处理网络结构信息和每个节点的丰富文本语义信息。

    Representation learning on networks aims to derive a meaningful vector representation for each node, thereby facilitating downstream tasks such as link prediction, node classification, and node clustering. In heterogeneous text-rich networks, this task is more challenging due to (1) presence or absence of text: Some nodes are associated with rich textual information, while others are not; (2) diversity of types: Nodes and edges of multiple types form a heterogeneous network structure. As pretrained language models (PLMs) have demonstrated their effectiveness in obtaining widely generalizable text representations, a substantial amount of effort has been made to incorporate PLMs into representation learning on text-rich networks. However, few of them can jointly consider heterogeneous structure (network) information as well as rich textual semantic information of each node effectively. In this paper, we propose Heterformer, a Heterogeneous Network-Empowered Transformer that performs cont
    
[^220]: 针对Few-Task Meta-Learning的基于集合的元互插方法

    Set-based Meta-Interpolation for Few-Task Meta-Learning. (arXiv:2205.09990v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.09990](http://arxiv.org/abs/2205.09990)

    本论文提出了一种基于集合的元互插方法，可以解决Few-Task Meta-Learning问题中任务数量少带来的瓶颈，同时该方法对领域不敏感。

    

    元学习方法通过利用相关任务的知识，在给定少量示例的情况下使得机器学习系统能够适应新任务。然而，元训练任务的数量仍然需要很大，才能在元测试期间进行泛化，这对于只有少量任务的现实世界问题来说是一个关键的瓶颈，原因包括构建任务的困难和成本。我们提出了一种新颖的基于集合的元互插方法Meta-Interpolation，旨在解决这些限制。

    Meta-learning approaches enable machine learning systems to adapt to new tasks given few examples by leveraging knowledge from related tasks. However, a large number of meta-training tasks are still required for generalization to unseen tasks during meta-testing, which introduces a critical bottleneck for real-world problems that come with only few tasks, due to various reasons including the difficulty and cost of constructing tasks. Recently, several task augmentation methods have been proposed to tackle this issue using domain-specific knowledge to design augmentation techniques to densify the meta-training task distribution. However, such reliance on domain-specific knowledge renders these methods inapplicable to other domains. While Manifold Mixup based task augmentation methods are domain-agnostic, we empirically find them ineffective on non-image domains. To tackle these limitations, we propose a novel domain-agnostic task augmentation method, Meta-Interpolation, which utilizes e
    
[^221]: CLIP-Dissect：深度视觉网络中神经元表示的自动描述

    CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks. (arXiv:2204.10965v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2204.10965](http://arxiv.org/abs/2204.10965)

    CLIP-Dissect是一种用于自动描述视觉网络中神经元功能的新技术，它可以无需任何标记数据或人类示例即将内部神经元标记为无需任何标记数据或人类示例的开放概念，并比现有方法提供了更准确的描述。此外，它具有灵活性、高效性和可扩展性。

    

    本文提出了一种新技术CLIP-Dissect，可以自动描述视觉网络中单个隐藏神经元的功能。CLIP-Dissect利用了最近在多模态视觉/语言模型方面的进展，将内部神经元标记为无需任何标记数据或人类示例的开放概念。我们证明了CLIP-Dissect提供了比现有方法更准确的描述，其中包括具备“地面真相”（ground-truth）的最后一层神经元以及具备定性好的隐藏层神经元。此外，该方法非常灵活：它与模型无关，可以轻松处理新概念，可以扩展以利用未来更好的多模态模型。最后，CLIP-Dissect计算效率高，可以在短短4分钟内标记ResNet-50的五层所有神经元，比现有方法快10倍以上。我们的代码可在https://github.com/Trustworthy-ML-Lab/CLIP-dissect 上找到。

    In this paper, we propose CLIP-Dissect, a new technique to automatically describe the function of individual hidden neurons inside vision networks. CLIP-Dissect leverages recent advances in multimodal vision/language models to label internal neurons with open-ended concepts without the need for any labeled data or human examples. We show that CLIP-Dissect provides more accurate descriptions than existing methods for last layer neurons where the ground-truth is available as well as qualitatively good descriptions for hidden layer neurons. In addition, our method is very flexible: it is model agnostic, can easily handle new concepts and can be extended to take advantage of better multimodal models in the future. Finally CLIP-Dissect is computationally efficient and can label all neurons from five layers of ResNet-50 in just 4 minutes, which is more than 10 times faster than existing methods. Our code is available at https://github.com/Trustworthy-ML-Lab/CLIP-dissect.
    
[^222]: 旋转盘系统中时空热流的贝叶斯推断中，采用深度代理加速延迟接受 HMC 法

    Deep surrogate accelerated delayed-acceptance HMC for Bayesian inference of spatio-temporal heat fluxes in rotating disc systems. (arXiv:2204.02272v2 [math.NA] UPDATED)

    [http://arxiv.org/abs/2204.02272](http://arxiv.org/abs/2204.02272)

    本文提出一种加速PDE求解的方法，用于解决贝叶斯反问题，具有保证的精度，并可用于其他设置，通过训练神经网络代理模拟参数向前模型，同时确定Biot数的近似后验分布，而无需外部求解。

    

    我们介绍了一种基于深度学习的加速方法，用于解决 PDE 环境下的贝叶斯反问题，并具有保证的精度。这种方法是针对一个被称为 Biot 数的时空热流参数的不适定问题进行的，但是这种方法是通用的，可以用于其他设置。我们开发了一种新的训练方案，利用数据来自适应地训练神经网络代理，模拟参数向前模型。通过同时确定 Biot 数的近似后验分布，并根据它调整物理启发式训练损失的权重，我们的方法在没有外部求解的情况下逼近向前和反向解。使用随机切比雪夫级数，我们概述了如何逼近高斯过程先验，并使用代理应用哈密顿蒙特卡罗 (HMC) 从后验分布中采样。我们导出了代理后验的收敛性。

    We introduce a deep learning accelerated methodology to solve PDE-based Bayesian inverse problems with guaranteed accuracy. This is motivated by the ill-posed problem of inferring a spatio-temporal heat-flux parameter known as the Biot number given temperature data, however the methodology is generalisable to other settings. To accelerate Bayesian inference, we develop a novel training scheme that uses data to adaptively train a neural-network surrogate simulating the parametric forward model. By simultaneously identifying an approximate posterior distribution over the Biot number, and weighting a physics-informed training loss according to this, our approach approximates forward and inverse solution together without any need for external solves. Using a random Chebyshev series, we outline how to approximate a Gaussian process prior, and using the surrogate we apply Hamiltonian Monte Carlo (HMC) to sample from the posterior distribution. We derive convergence of the surrogate posterior
    
[^223]: 一种基于集合成员关系的方法来发现特征相关性并解释神经分类器的决策

    A Set Membership Approach to Discovering Feature Relevance and Explaining Neural Classifier Decisions. (arXiv:2204.02241v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.02241](http://arxiv.org/abs/2204.02241)

    该论文提出一种基于集合成员关系的方法来发现神经分类器所需的特征，并解释其决策。方法能够标识贡献每个特征对神经网络分类器所做出的预测，并解释分类器处理输入分布变化时的行为。

    

    神经分类器是提供模式类别决策的非线性系统。对于给定的问题，神经分类器的输出构成了某个未知函数的输出的近似，该函数将模式数据映射到其相应的类别。然而，由于缺乏该函数的知识以及神经分类器的复杂性，尤其是对于深度学习体系结构，往往无法获得有关如何进行具体预测的信息。因此，这些强大的学习系统被认为是黑匣子，在关键的应用中使用它们往往被认为是不合适的。本文提出了一种新的方案来解决这个问题，通过基于集合成员分析，我们把输入模式分成子集，以关联不同的输出基类。这种关联是通过计算将分类器决策引导到替代类别所需的最小输入摄动来推断的。我们的方法可以用于发现哪些特征被认为是相关的，并量化其对预测的贡献。此外，我们展示了如何使用我们的方法来解释分类器在处理输入分布变化时的行为。标准的图像分类数据集上的实验结果显示了我们的方法在发现相关输入特征和解释由深度神经网络体系结构做出的分类决策方面的有效性。

    Neural classifiers are non linear systems providing decisions on the classes of patterns, for a given problem they have learned. The output computed by a classifier for each pattern constitutes an approximation of the output of some unknown function, mapping pattern data to their respective classes. The lack of knowledge of such a function along with the complexity of neural classifiers, especially when these are deep learning architectures, do not permit to obtain information on how specific predictions have been made. Hence, these powerful learning systems are considered as black boxes and in critical applications their use tends to be considered inappropriate. Gaining insight on such a black box operation constitutes a one way approach in interpreting operation of neural classifiers and assessing the validity of their decisions. In this paper we tackle this problem introducing a novel methodology for discovering which features are considered relevant by a trained neural classifier a
    
[^224]: 具有可证明的一致性和公平保证的推荐系统张量补全

    Tensor Completion with Provable Consistency and Fairness Guarantees for Recommender Systems. (arXiv:2204.01815v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2204.01815](http://arxiv.org/abs/2204.01815)

    本文介绍了一种新的一致性方法来解决矩阵和张量补全问题，在推荐系统应用中，我们证明了通过保留单位比例和一致性两个约束条件可以实现解的存在性与唯一性。

    

    我们引入了一种新的基于一致性的方法来定义和解决非负/正矩阵和张量补全问题。该框架的新颖之处在于，我们不是人为地将问题形式化为任意优化问题，例如，最小化一个结构量，如秩或范数，而是展示了一个单一的属性/约束：保留单位比例一致性，保证了解的存在，并在相对较弱的支持假设下保证了解的唯一性。该框架和解算法也直接推广到任意维度的张量中，同时保持了固定维度 d 的问题规模的线性计算复杂性。在推荐系统应用中，我们证明了两个合理的性质，这些性质应该适用于任何 RS 问题的解，足以允许在我们的框架内建立唯一性保证。关键理论贡献是展示了这些约束下解的存在性与唯一性。

    We introduce a new consistency-based approach for defining and solving nonnegative/positive matrix and tensor completion problems. The novelty of the framework is that instead of artificially making the problem well-posed in the form of an application-arbitrary optimization problem, e.g., minimizing a bulk structural measure such as rank or norm, we show that a single property/constraint: preserving unit-scale consistency, guarantees the existence of both a solution and, under relatively weak support assumptions, uniqueness. The framework and solution algorithms also generalize directly to tensors of arbitrary dimensions while maintaining computational complexity that is linear in problem size for fixed dimension d. In the context of recommender system (RS) applications, we prove that two reasonable properties that should be expected to hold for any solution to the RS problem are sufficient to permit uniqueness guarantees to be established within our framework. Key theoretical contribu
    
[^225]: 机器学习解决方案综述：图模式提取问题

    A Survey on Machine Learning Solutions for Graph Pattern Extraction. (arXiv:2204.01057v3 [math.CO] UPDATED)

    [http://arxiv.org/abs/2204.01057](http://arxiv.org/abs/2204.01057)

    本综述概括了机器学习在解决五个著名子图问题中的应用。这些问题包括子图同构（计数和匹配）、最大公共子图、社区检测和社区搜索问题。

    

    通过使用给定图形的一部分顶点和边来构造子图。存在许多对于子图具有遗传性的图形属性。因此，来自不同社区的研究人员已经非常关注研究众多子图问题，而不仅是普通的图问题。许多算法已被提出用于研究子图问题，其中一种常见的方法是提取给定图形的模式和结构。由于某些类型的图形具有复杂结构，为了改善现有框架的整体性能，最近采用了机器学习技术来处理各种子图问题。在本文中，我们综述了使用机器学习方法解决的五个著名子图问题的综合研究。它们是子图同构（计数和匹配）、最大公共子图、社区检测和社区搜索问题。我们提供了每个方案的概述。

    A subgraph is constructed by using a subset of vertices and edges of a given graph. There exist many graph properties that are hereditary for subgraphs. Hence, researchers from different communities have paid a great deal of attention in studying numerous subgraph problems, on top of the ordinary graph problems. Many algorithms are proposed in studying subgraph problems, where one common approach is by extracting the patterns and structures of a given graph. Due to the complex structures of certain types of graphs and to improve overall performances of the existing frameworks, machine learning techniques have recently been employed in dealing with various subgraph problems. In this article, we present a comprehensive review on five well known subgraph problems that have been tackled by using machine learning methods. They are subgraph isomorphism (both counting and matching), maximum common subgraph, community detection and community search problems. We provide an outline of each propo
    
[^226]: 利用神经嵌入将语音与环境分离

    Disentangling speech from surroundings with neural embeddings. (arXiv:2203.15578v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2203.15578](http://arxiv.org/abs/2203.15578)

    本文提出了一种方法，利用神经音频编解码器中的嵌入空间将语音信号与嘈杂环境分离，并演示了该方法在从噪声或混响中分离语音中的应用。

    

    本文介绍了一种利用神经音频编解码器中的嵌入空间将语音信号与嘈杂环境分离的方法。我们引入了一个新的训练过程，使得我们的模型能够产生结构化的音频波形编码向量，其中一部分编码向量表示语音信号，其余部分表示环境。我们通过对不同输入波形的嵌入进行分区，并训练模型来从混合分区中忠实地重建音频，从而确保每个分区编码了一个单独的音频属性。作为使用案例，我们演示了从噪声或混响特性中分离语音的能力。我们的方法还允许有目的地调整音频输出特性。

    We present a method to separate speech signals from noisy environments in the embedding space of a neural audio codec. We introduce a new training procedure that allows our model to produce structured encodings of audio waveforms given by embedding vectors, where one part of the embedding vector represents the speech signal, and the rest represent the environment. We achieve this by partitioning the embeddings of different input waveforms and training the model to faithfully reconstruct audio from mixed partitions, thereby ensuring each partition encodes a separate audio attribute. As use cases, we demonstrate the separation of speech from background noise or from reverberation characteristics. Our method also allows for targeted adjustments of the audio output characteristics.
    
[^227]: 一种基于仿真集成的生物启发式搜索测试方法在ADAS案例研究中的应用

    Machine Learning Testing in an ADAS Case Study Using Simulation-Integrated Bio-Inspired Search-Based Testing. (arXiv:2203.12026v3 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2203.12026](http://arxiv.org/abs/2203.12026)

    本文提出一种基于仿真集成的生物启发式搜索测试方法Deeper，用于生成用于测试基于深度神经网络的车道保持系统的故障发现测试场景，通过实证评估和与竞赛中的其他工具的比较展示了其性能的提高。

    

    本文介绍Deeper的扩展版本，它是一种基于搜索实现的仿真集成测试解决方案，用于生成用于测试基于深度神经网络的车道保持系统的故障发现测试场景。在新版本中，我们利用了一组新的生物启发式搜索算法-遗传算法（GA）、（μ+λ）和（μ，λ）进化策略（ES）以及粒子群优化（PSO），这些算法利用质量种子种群以及为建模测试场景使用的特定领域交叉和突变操作。为了展示Deeper中新测试生成器的能力，我们进行了实证评估，并与SBST 2021的五个参赛工具的结果进行了比较。我们的评估结果表明，在新版本中，Deeper中的新测试生成器不仅在以前的版本上有了很大提升，而且...

    This paper presents an extended version of Deeper, a search-based simulation-integrated test solution that generates failure-revealing test scenarios for testing a deep neural network-based lane-keeping system. In the newly proposed version, we utilize a new set of bio-inspired search algorithms, genetic algorithm (GA), $({\mu}+{\lambda})$ and $({\mu},{\lambda})$ evolution strategies (ES), and particle swarm optimization (PSO), that leverage a quality population seed and domain-specific cross-over and mutation operations tailored for the presentation model used for modeling the test scenarios. In order to demonstrate the capabilities of the new test generators within Deeper, we carry out an empirical evaluation and comparison with regard to the results of five participating tools in the cyber-physical systems testing competition at SBST 2021. Our evaluation shows the newly proposed test generators in Deeper not only represent a considerable improvement on the previous version but also 
    
[^228]: 深度神经网络的可解释性方法和扰动伪影的评估

    Evaluation of Interpretability Methods and Perturbation Artifacts in Deep Neural Networks. (arXiv:2203.02928v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.02928](http://arxiv.org/abs/2203.02928)

    本文评估了几种常见的深度神经网络可解释性方法，展示了扰动伪影对可解释性方法评估的影响，强调在评估中需要考虑伪影的存在。

    

    深度神经网络在图像分类、检测和预测方面表现出色，但如何解释其决策仍然是一个未解决的问题，因此出现了许多可解释性方法。评估这些方法是一个重要的挑战，其中一种流行的方法是通过扰动输入特征来评估可解释性方法，但是扰动本身可能会引入伪影。本文提出了一种估计伪影影响的方法，使用此方法评估了几种流行的可解释性方法在不同数据集上的表现，并展示了扰动伪影对可解释性方法评估的影响。我们的结果突出了在评估可解释性方法时考虑伪影存在的重要性。

    Despite excellent performance of deep neural networks (DNNs) in image classification, detection, and prediction, characterizing how DNNs make a given decision remains an open problem, resulting in a number of interpretability methods. Post-hoc interpretability methods primarily aim to quantify the importance of input features with respect to the class probabilities. However, due to the lack of ground truth and the existence of interpretability methods with diverse operating characteristics, evaluating these methods is a crucial challenge. A popular approach to evaluate interpretability methods is to perturb input features deemed important for a given prediction and observe the decrease in accuracy. However, perturbation itself may introduce artifacts. We propose a method for estimating the impact of such artifacts on the fidelity estimation by utilizing model accuracy curves from perturbing input features according to the Most Import First (MIF) and Least Import First (LIF) orders. Usi
    
[^229]: 数据高效且可解释的表格异常检测

    Data-Efficient and Interpretable Tabular Anomaly Detection. (arXiv:2203.02034v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.02034](http://arxiv.org/abs/2203.02034)

    本文提出了DIAD框架，使用广义加性模型作为白盒模型，通过部分识别目标检测异常值，并能够在半监督设置中使用少量标记数据提高异常检测性能。

    

    异常检测在许多应用中发挥着重要作用。本文关注异常检测的两个重要方面。首先，大多数异常检测方法无法使用已标记数据，这些数据在实践中往往数量有限，但对于高精度的AD结果至关重要。其次，大多数异常检测方法不可解释，这会阻碍相关方了解异常产生的原因。为了解决这些问题，我们提出了DIAD框架，它采用广义加性模型作为白盒模型，通过部分识别目标检测异常值，自然地处理嘈杂或异质性特征，并能够在半监督设置中使用少量标记数据提高异常检测性能。我们证明了我们的框架相对于以前的工作在无监督和半监督设置下的优越性。

    Anomaly detection (AD) plays an important role in numerous applications. We focus on two understudied aspects of AD that are critical for integration into real-world applications. First, most AD methods cannot incorporate labeled data that are often available in practice in small quantities and can be crucial to achieve high AD accuracy. Second, most AD methods are not interpretable, a bottleneck that prevents stakeholders from understanding the reason behind the anomalies. In this paper, we propose a novel AD framework that adapts a white-box model class, Generalized Additive Models, to detect anomalies using a partial identification objective which naturally handles noisy or heterogeneous features. In addition, the proposed framework, DIAD, can incorporate a small amount of labeled data to further boost anomaly detection performances in semi-supervised settings. We demonstrate the superiority of our framework compared to previous work in both unsupervised and semi-supervised settings
    
[^230]: 神经网络的规范叶面：鲁棒性应用研究

    Canonical foliations of neural networks: application to robustness. (arXiv:2203.00922v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2203.00922](http://arxiv.org/abs/2203.00922)

    本文探讨了利用黎曼几何和叶面理论创新应用于神经网络鲁棒性的新视角，提出了一种适用于数据空间的以曲率为考量因素的 two-step spectral 对抗攻击方法。

    

    深度学习模型易受到对抗攻击。而对抗学习正在变得至关重要。本文提出了一种新的神经网络鲁棒性视角，采用黎曼几何和叶面理论。通过创建考虑数据空间曲率的新对抗攻击，即 two-step spectral attack，来说明这个想法。数据空间被视为一个配备了神经网络的 Fisher 信息度量（FIM）拉回的（退化的）黎曼流形。大多数情况下，该度量仅为半正定，其内核成为研究的核心对象。从该核中导出一个规范叶面。横向叶的曲率给出了适当的修正，从而得到了两步近似的测地线和一种新的高效对抗攻击。该方法首先在一个 2D 玩具示例中进行演示。

    Deep learning models are known to be vulnerable to adversarial attacks. Adversarial learning is therefore becoming a crucial task. We propose a new vision on neural network robustness using Riemannian geometry and foliation theory. The idea is illustrated by creating a new adversarial attack that takes into account the curvature of the data space. This new adversarial attack called the two-step spectral attack is a piece-wise linear approximation of a geodesic in the data space. The data space is treated as a (degenerate) Riemannian manifold equipped with the pullback of the Fisher Information Metric (FIM) of the neural network. In most cases, this metric is only semi-definite and its kernel becomes a central object to study. A canonical foliation is derived from this kernel. The curvature of transverse leaves gives the appropriate correction to get a two-step approximation of the geodesic and hence a new efficient adversarial attack. The method is first illustrated on a 2D toy example
    
[^231]: 离散潜变量模型的贝叶斯主动学习

    Bayesian Active Learning for Discrete Latent Variable Models. (arXiv:2202.13426v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.13426](http://arxiv.org/abs/2202.13426)

    本文提出了一个新的框架，用于离散潜变量回归模型的最大相互信息输入选择。通过对线性回归混合物模型的Fisher信息分析，证明在这种情况下主动学习可以取得巨大的收益。同时，我们考虑了一类强大的时间结构潜变量模型，并展示了如何将我们的框架调整为在选择过程中融入时态依赖性。

    

    主动学习旨在减少拟合模型参数所需的数据量，因此成为现代机器学习中的重要技术类别。然而，过去的主动学习研究往往忽视了在神经科学、心理学和各种工程和科学学科中发挥关键作用的潜变量模型。本文提出了一种新的框架，用于离散潜变量回归模型的最大相互信息输入选择。我们首先将我们的方法应用于一类称为“线性回归混合物”的模型。虽然已知对于线性高斯回归模型，主动学习并不具有优势，但我们使用Fisher信息进行分析，表明即使对于这种混合模型，主动学习仍然可以取得巨大的收益，并使用模拟和真实数据对此进行了验证。然后，我们考虑了一类强大的时间结构潜变量模型，并展示了如何将我们的框架调整为在选择过程中融入时态依赖性。总的来说，我们的工作展示了主动学习在一个新领域的有效性，并提供了一个通用的框架，可应用于广泛的离散潜变量模型，只需进行较少的修改。

    Active learning seeks to reduce the amount of data required to fit the parameters of a model, thus forming an important class of techniques in modern machine learning. However, past work on active learning has largely overlooked latent variable models, which play a vital role in neuroscience, psychology, and a variety of other engineering and scientific disciplines. Here we address this gap by proposing a novel framework for maximum-mutual-information input selection for discrete latent variable regression models. We first apply our method to a class of models known as "mixtures of linear regressions" (MLR). While it is well known that active learning confers no advantage for linear-Gaussian regression models, we use Fisher information to show analytically that active learning can nevertheless achieve large gains for mixtures of such models, and we validate this improvement using both simulations and real-world data. We then consider a powerful class of temporally structured latent var
    
[^232]: 标签差分隐私是否能够防止标签推断攻击？

    Does Label Differential Privacy Prevent Label Inference Attacks?. (arXiv:2202.12968v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.12968](http://arxiv.org/abs/2202.12968)

    标签差分隐私不能完全防止标签推断攻击（LIAs），但可以限制LIAs对手的优势和语义保护。

    

    标签差分隐私（label-DP）是一种流行的框架，用于在具有公共特征和敏感私有标签的数据集上训练私有机器学习模型。尽管它具有严格的隐私保证，但实践中观察到，label-DP并不能防止标签推断攻击（LIAs）：使用label-DP训练的模型可以在公共训练特征上进行评估，以高精度地恢复其旨在保护的非常私人标签。在本研究中，我们认为这种现象并不矛盾，并且label-DP旨在限制LIA对手的优势，以便与使用贝叶斯分类器预测训练标签进行比较。在label-DP $\epsilon=0$时，该优势为零，因此最佳攻击是根据贝叶斯分类器进行预测，并且与训练标签无关。我们的界限显示了label-DP提供的语义保护，并提供了如何选择 $\varepsilon$ 以将LIAs的威胁限制在某个水平以下的指南。最后，

    Label differential privacy (label-DP) is a popular framework for training private ML models on datasets with public features and sensitive private labels. Despite its rigorous privacy guarantee, it has been observed that in practice label-DP does not preclude label inference attacks (LIAs): Models trained with label-DP can be evaluated on the public training features to recover, with high accuracy, the very private labels that it was designed to protect. In this work, we argue that this phenomenon is not paradoxical and that label-DP is designed to limit the advantage of an LIA adversary compared to predicting training labels using the Bayes classifier. At label-DP $\epsilon=0$ this advantage is zero, hence the optimal attack is to predict according to the Bayes classifier and is independent of the training labels. Our bound shows the semantic protection conferred by label-DP and gives guidelines on how to choose $\varepsilon$ to limit the threat of LIAs below a certain level. Finally,
    
[^233]: MAML和ANIL被证明能够学习表示法

    MAML and ANIL Provably Learn Representations. (arXiv:2202.03483v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.03483](http://arxiv.org/abs/2202.03483)

    本文证明了MAML和ANIL能够在少样本学习中学习出共同的数据表示法，它们通过适应模型的最后一层来改善表示法，这也是导致共享表示法出现的原因。

    

    最近的经验证据让人们认为，基于梯度的元学习（GBML）方法在少样本学习上表现良好，因为它们学习了一种可共享的表达数据表示法。然而，从理论角度来看，GBML的机制仍然是个谜。在本文中，我们证明了两种著名的GBML方法，MAML和ANIL，以及它们的一阶近似都能够学习一组给定任务之间的共同表示法。具体而言，在著名的多任务线性表示学习环境中，它们能够以指数快的速度恢复地面实况表示法。此外，我们的分析阐明，驱动MAML和ANIL恢复潜在表示法的动力是它们调整模型的最后一层，利用潜在的任务多样性来改善所有感兴趣的方向的表示法。据我们所知，这是首个解释GBML方法导致共享表示法出现的理论工作。

    Recent empirical evidence has driven conventional wisdom to believe that gradient-based meta-learning (GBML) methods perform well at few-shot learning because they learn an expressive data representation that is shared across tasks. However, the mechanics of GBML have remained largely mysterious from a theoretical perspective. In this paper, we prove that two well-known GBML methods, MAML and ANIL, as well as their first-order approximations, are capable of learning common representation among a set of given tasks. Specifically, in the well-known multi-task linear representation learning setting, they are able to recover the ground-truth representation at an exponentially fast rate. Moreover, our analysis illuminates that the driving force causing MAML and ANIL to recover the underlying representation is that they adapt the final layer of their model, which harnesses the underlying task diversity to improve the representation in all directions of interest. To the best of our knowledge,
    
[^234]: 不需要访问任何训练或测试数据的泛化度量标准评估自然语言处理模型

    Evaluating natural language processing models with generalization metrics that do not need access to any training or testing data. (arXiv:2202.02842v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2202.02842](http://arxiv.org/abs/2202.02842)

    本文提出了一种无需访问任何数据即可评估自然语言处理模型的泛化度量标准，并通过对Huggingface预训练变压器的模型选择，得到一个简单高效且相关性强的有用度量标准。

    

    选择合适的结构参数和训练超参数对于提高机器学习（ML）模型的性能至关重要。最近的几项实证研究对神经网络（NNs）进行了大规模的相关分析，以寻找有效的泛化度量标准以指导模型选择。有效的度量标准通常预计与测试性能强相关。在本文中，我们通过以下目标扩展了先前的分析，进行了基于泛化度量标准的模型选择研究：（i）关注自然语言处理（NLP）任务，因为先前的工作主要集中于计算机视觉（CV）任务；（ii）考虑直接预测测试误差而非泛化差距的度量标准；（iii）探索不需要访问数据即可计算的度量标准。从这些目标出发，我们能够提供第一个使用泛化度量标准对来自Huggingface的大型预训练变压器进行模型选择的结果，并比较了许多不同的度量标准。结果表明，我们提出的有用度量标准不止与测试性能高度相关，而且更加简单高效。

    Selecting suitable architecture parameters and training hyperparameters is essential for enhancing machine learning (ML) model performance. Several recent empirical studies conduct large-scale correlational analysis on neural networks (NNs) to search for effective \emph{generalization metrics} that can guide this type of model selection. Effective metrics are typically expected to correlate strongly with test performance. In this paper, we expand on prior analyses by examining generalization-metric-based model selection with the following objectives: (i) focusing on natural language processing (NLP) tasks, as prior work primarily concentrates on computer vision (CV) tasks; (ii) considering metrics that directly predict \emph{test error} instead of the \emph{generalization gap}; (iii) exploring metrics that do not need access to data to compute. From these objectives, we are able to provide the first model selection results on large pretrained Transformers from Huggingface using general
    
[^235]: 使用概念激活向量在推荐系统中发现软属性的个性化语义

    Discovering Personalized Semantics for Soft Attributes in Recommender Systems using Concept Activation Vectors. (arXiv:2202.02830v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2202.02830](http://arxiv.org/abs/2202.02830)

    我们使用概念激活向量来把用户描述商品的属性的语义表达出来，以改进推荐系统的效果。

    

    交互式推荐系统已经成为一种有前途的范例，以克服传统推荐系统所使用的原始用户反馈的局限性（例如点击、项目消费、评分）。它们允许用户以更丰富的方式表达意图、偏好、约束和上下文，通常使用自然语言（包括分类搜索和对话）。然而，需要更多的研究来找到使用这些反馈的最有效方法。一个挑战是从经常用于描述所需项目的开放式术语或属性中推断用户的语义意图，并使用它来改进推荐结果。利用最近在机器学习中开发的模型可解释性方法——概念激活向量（CAVs），我们开发了一个框架，在推荐系统中学习一种表示，捕捉这些属性的语义，并将它们连接到用户的偏好和行为中。我们方法的一个新功能是它能够区分

    Interactive recommender systems have emerged as a promising paradigm to overcome the limitations of the primitive user feedback used by traditional recommender systems (e.g., clicks, item consumption, ratings). They allow users to express intent, preferences, constraints, and contexts in a richer fashion, often using natural language (including faceted search and dialogue). Yet more research is needed to find the most effective ways to use this feedback. One challenge is inferring a user's semantic intent from the open-ended terms or attributes often used to describe a desired item, and using it to refine recommendation results. Leveraging concept activation vectors (CAVs) [26], a recently developed approach for model interpretability in machine learning, we develop a framework to learn a representation that captures the semantics of such attributes and connects them to user preferences and behaviors in recommender systems. One novel feature of our approach is its ability to distinguis
    
[^236]: 矩阵平方根的低秩更新

    Low-Rank Updates of Matrix Square Roots. (arXiv:2201.13156v3 [math.NA] UPDATED)

    [http://arxiv.org/abs/2201.13156](http://arxiv.org/abs/2201.13156)

    本文研究了矩阵平方根和逆平方根运算。当给定一个矩阵的低秩扰动时，存在一种低秩近似校正方法。该方法可以通过代数Riccati方程的低秩解计算，并可以在两个数值例子中得到说明。

    

    在数据科学应用中，协方差矩阵具有稀疏矩阵加上低秩扰动的结构是常见的。为了避免耗时的矩阵计算，经常需要算法利用这种结构。其中一种方法是通过维持这种结构执行矩阵求逆的Sherman-Morrison-Woodbury公式。在本文中，我们考虑矩阵平方根和逆平方根运算。当给定一个矩阵的低秩扰动时，我们认为存在一个低秩近似校正(逆)平方根的方法。我们通过建立对真正校正的特征值的几何衰减界限来实现这一点。然后，我们将此校正方案作为代数Riccati方程的解，讨论了如何计算此方程的低秩解。我们分析了通过近似求解代数Riccati方程产生的逼近误差，并通过两个数值例子说明了我们的结果。

    Models in which the covariance matrix has the structure of a sparse matrix plus a low rank perturbation are ubiquitous in data science applications. It is often desirable for algorithms to take advantage of such structures, avoiding costly matrix computations that often require cubic time and quadratic storage. This is often accomplished by performing operations that maintain such structures, e.g. matrix inversion via the Sherman-Morrison-Woodbury formula. In this paper we consider the matrix square root and inverse square root operations. Given a low rank perturbation to a matrix, we argue that a low-rank approximate correction to the (inverse) square root exists. We do so by establishing a geometric decay bound on the true correction's eigenvalues. We then proceed to frame the correction as the solution of an algebraic Riccati equation, and discuss how a low-rank solution to that equation can be computed. We analyze the approximation error incurred when approximately solving the alge
    
[^237]: 两时间尺度更新规则训练生成式对抗网络中的关键批次大小的存在和估计

    Existence and Estimation of Critical Batch Size for Training Generative Adversarial Networks with Two Time-Scale Update Rule. (arXiv:2201.11989v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.11989](http://arxiv.org/abs/2201.11989)

    本文研究了使用两时间尺度更新规则（TTUR）训练生成式对抗网络（GAN）时批次大小与训练所需步骤数量之间的关系，理论上证明了为了找到稳定点，随着批次大小的增加所需步骤数量会减少并且存在一个最小化随机一阶预言机（SFO）复杂度的关键批次大小。

    

    先前的研究表明，在理论和实践中，使用不同的学习率，如不同的恒定率或不同的衰减率等，使用两时间尺度更新规则（TTUR）有助于训练生成式对抗网络（GAN）。此外，批次大小对于使用TTUR训练GANs也很重要，两者都影响了训练所需的步骤数量。本文基于恒定学习率研究了批次大小与使用TTUR训练GANs所需步骤数量之间的关系。我们理论上表明，对于具有恒定学习率的TTUR，为了找到鉴别器和生成器损失函数的稳定点，所需步骤数随着批次大小的增加而减少，并且存在一个最小化随机一阶预言机（SFO）复杂度的关键批次大小。然后，我们使用Fr'echet Inception Distance（FID）作为训练的性能测量，并提供了...

    Previous results have shown that a two time-scale update rule (TTUR) using different learning rates, such as different constant rates or different decaying rates, is useful for training generative adversarial networks (GANs) in theory and in practice. Moreover, not only the learning rate but also the batch size is important for training GANs with TTURs and they both affect the number of steps needed for training. This paper studies the relationship between batch size and the number of steps needed for training GANs with TTURs based on constant learning rates. We theoretically show that, for a TTUR with constant learning rates, the number of steps needed to find stationary points of the loss functions of both the discriminator and generator decreases as the batch size increases and that there exists a critical batch size minimizing the stochastic first-order oracle (SFO) complexity. Then, we use the Fr'echet inception distance (FID) as the performance measure for training and provide nu
    
[^238]: Eigenlearning框架：核回归和宽神经网络的守恒定律视角

    The Eigenlearning Framework: A Conservation Law Perspective on Kernel Regression and Wide Neural Networks. (arXiv:2110.03922v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.03922](http://arxiv.org/abs/2110.03922)

    该论文提出了Eigenlearning框架，通过限制核回归在学习正交基函数方面的能力并利用守恒定律来解释模型的泛化能力，同时还为Nakkiran等人的“深度引导”现象，经典奇偶问题难度和对抗鲁棒性提供了理论支持，并与统计物理学中的一个系统进行了类比。

    

    我们针对核岭回归（KRR）的测试风险和其他泛化指标导出了简单的闭式估计。相对于以前的工作，我们的推导大大简化，最终表达式更易于解释。这些改进得益于我们识别出的一个尖锐的守恒定律，它限制了KRR学习任何正交基函数的能力。测试风险和其他感兴趣的对象可以透明地用于我们在核特征基中评估的守恒量来表示。我们使用改进的框架来：i）为Nakkiran等人（2020）的“深度引导”提供理论解释，ii）推广先前关于经典奇偶问题难度的结果，iii）为对抗鲁棒性的研究提供理论工具，并iv）在统计物理学中研究核岭回归和熟知系统之间的严密类比。

    We derive simple closed-form estimates for the test risk and other generalization metrics of kernel ridge regression (KRR). Relative to prior work, our derivations are greatly simplified and our final expressions are more readily interpreted. These improvements are enabled by our identification of a sharp conservation law which limits the ability of KRR to learn any orthonormal basis of functions. Test risk and other objects of interest are expressed transparently in terms of our conserved quantity evaluated in the kernel eigenbasis. We use our improved framework to: i) provide a theoretical explanation for the "deep bootstrap" of Nakkiran et al (2020), ii) generalize a previous result regarding the hardness of the classic parity problem, iii) fashion a theoretical tool for the study of adversarial robustness, and iv) draw a tight analogy between KRR and a well-studied system in statistical physics.
    
[^239]: Sinkhorn分布鲁棒优化

    Sinkhorn Distributionally Robust Optimization. (arXiv:2109.11926v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2109.11926](http://arxiv.org/abs/2109.11926)

    本文通过使用Sinkhorn距离进行分布鲁棒优化，推导出更容易处理且在实际中更合理的最坏情况分布，提出了解决方案，并展示了其优越性能。

    

    我们研究了使用Sinkhorn距离 -一种基于熵正则化的Wasserstein距离变体- 的分布鲁棒优化（DRO）。我们为一般名义分布推导了凸规划对偶重构。相比于Wasserstein DRO，对于更大类的损失函数，它在计算上更容易处理，它的最坏情况分布对实际应用更合理。为了解决对偶重构，我们开发了一种使用有偏梯度神经元的随机镜像下降算法，并分析了其收敛速度。最后，我们提供了使用合成和真实数据的数值实例，以证明其优越性能。

    We study distributionally robust optimization (DRO) with Sinkhorn distance -a variant of Wasserstein distance based on entropic regularization. We derive convex programming dual reformulation for a general nominal distribution. Compared with Wasserstein DRO, it is computationally tractable for a larger class of loss functions, and its worst-case distribution is more reasonable for practical applications. To solve the dual reformulation, we develop a stochastic mirror descent algorithm using biased gradient oracles and analyze its convergence rate. Finally, we provide numerical examples using synthetic and real data to demonstrate its superior performance.
    
[^240]: 学习元表示来增强多智能体强化学习中的代理

    Learning Meta Representations for Agents in Multi-Agent Reinforcement Learning. (arXiv:2108.12988v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.12988](http://arxiv.org/abs/2108.12988)

    本文提出代理元表示（MRA），能够跨多种多智能体强化学习中变化的人口数量进行推广的代理，并证明了通过最大化互信息来实现代理性能的提升。

    

    在多智能体强化学习中，代理在单个马尔科夫博弈中学习的行为通常仅限于给定的代理数量。由人口数量变化引起的每个单个马尔科夫博弈可能具有不同的最佳联合策略和游戏特定知识，在现代多智能体强化学习算法中独立建模。本文针对创建可以跨人口数量变化的马尔科夫博弈进行推广的代理。每个代理不再学习单一策略，而是学习一个包含各种游戏有效策略的策略集。为了实现这一目标，我们提出了代理元表示（MRA），明确建模了游戏通用和特定的策略知识。通过用多模态潜在策略表示策略集，通过迭代优化过程发现游戏通用的战略知识和不同的战略模式。我们证明通过最大化相关限制的互信息可以获得代理的表现提升。

    In multi-agent reinforcement learning, the behaviors that agents learn in a single Markov Game (MG) are typically confined to the given agent number. Every single MG induced by varying the population may possess distinct optimal joint strategies and game-specific knowledge, which are modeled independently in modern multi-agent reinforcement learning algorithms. In this work, our focus is on creating agents that can generalize across population-varying MGs. Instead of learning a unimodal policy, each agent learns a policy set comprising effective strategies across a variety of games. To achieve this, we propose Meta Representations for Agents (MRA) that explicitly models the game-common and game-specific strategic knowledge. By representing the policy sets with multi-modal latent policies, the game-common strategic knowledge and diverse strategic modes are discovered through an iterative optimization procedure. We prove that by approximately maximizing the resulting constrained mutual i
    
[^241]: 基于扩散映射的流形梯度计算算法及应用

    A diffusion-map-based algorithm for gradient computation on manifolds and applications. (arXiv:2108.06988v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.06988](http://arxiv.org/abs/2108.06988)

    本文提出了一种基于扩散映射理论的流形梯度计算方法，可以避免微分项并提供了一种无需导数的优化方法，在影像重构和球堆积等应用中取得了良好的效果。

    

    本文提出一种利用流形内部点上的函数样本评估估计拉普拉斯-贝尔特拉米算子的方法，从而推导出定义在欧几里得空间 Riemannian 子流形上的函数的黎曼梯度。该方法基于扩散映射理论，避免了微分项，并证明了黎曼梯度拓展的分析收敛结果。我们将黎曼梯度估计应用于基于梯度的算法中，提供了一种无需导数的优化方法。我们在多个应用场景中进行了测试和验证，包括从未知随机角度分布中进行影像重构以及在二维和三维的球堆积问题中的应用。

    We recover the Riemannian gradient of a given function defined on interior points of a Riemannian submanifold in the Euclidean space based on a sample of function evaluations at points in the submanifold. This approach is based on the estimates of the Laplace-Beltrami operator proposed in the diffusion-maps theory. The Riemannian gradient estimates do not involve differential terms. Analytical convergence results of the Riemannian gradient expansion are proved. We apply the Riemannian gradient estimate in a gradient-based algorithm providing a derivative-free optimization method. We test and validate several applications, including tomographic reconstruction from an unknown random angle distribution, and the sphere packing problem in dimensions 2 and 3.
    
[^242]: Sequoia: 一个统一连续学习研究的软件框架。

    Sequoia: A Software Framework to Unify Continual Learning Research. (arXiv:2108.01005v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.01005](http://arxiv.org/abs/2108.01005)

    Sequoia是一个公开可用的软件框架，用于统一连续学习的研究。该框架提供了各种设置、基准数据集、评估指标和最先进的方法，旨在减少重复工作，鼓励连续学习的统一研究。

    

    连续学习的领域旨在通过与非稳态环境的交互来积累知识和技能，并开发算法。实际上，存在大量评估程序（设置）和算法解决方案（方法），每个解决方案都有其自己的假设集。这种多样性使连续学习的进展难以衡量。我们提出了一个设置的分类法，其中每个设置都被描述为一组假设。此视图的树形层次结构出现，其中更一般的设置成为具有更多限制性假设的设置的父级。这使得可以使用继承来共享和重用研究，因为为给定设置开发方法也使其直接适用于其任何子代。我们创建了一个名为Sequoia的公开可用软件框架，它具有来自连续监督学习（CSL）和连续强化学习（CRL）领域的各种设置，以及基准数据集、评估指标和最先进的方法。该框架旨在方便研究人员进行可扩展定制和扩展。通过Sequoia，我们旨在减少在孤立地开发方法和进行实验所产生的重复努力，并鼓励在连续学习上进行统一的研究。

    The field of Continual Learning (CL) seeks to develop algorithms that accumulate knowledge and skills over time through interaction with non-stationary environments. In practice, a plethora of evaluation procedures (settings) and algorithmic solutions (methods) exist, each with their own potentially disjoint set of assumptions. This variety makes measuring progress in CL difficult. We propose a taxonomy of settings, where each setting is described as a set of assumptions. A tree-shaped hierarchy emerges from this view, where more general settings become the parents of those with more restrictive assumptions. This makes it possible to use inheritance to share and reuse research, as developing a method for a given setting also makes it directly applicable onto any of its children. We instantiate this idea as a publicly available software framework called Sequoia, which features a wide variety of settings from both the Continual Supervised Learning (CSL) and Continual Reinforcement Learni
    
[^243]: 通过估计和利用成对交互来提高噪声鲁棒性的图学习

    Noise-robust Graph Learning by Estimating and Leveraging Pairwise Interactions. (arXiv:2106.07451v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.07451](http://arxiv.org/abs/2106.07451)

    本文提出了一种成对框架，可在噪声图上进行节点分类，并利用结构成对交互作为主要的学习代理。提出的PI-GNN框架包含两个新颖的组件，自适应估计PI标签的可信度感知PI估计模型和一种解耦训练方法。

    

    在真实世界的图学习应用中，教授图神经网络（GNN）准确分类严重噪声标签的节点是一个重要问题，但目前探索不足。本文提出了一种针对噪声图的成对框架，其依赖于节点之间的结构成对交互（PI）作为主要的学习代理，除了从带噪声节点类标签进行的逐点学习之外。我们提出的PI-GNN框架贡献了两个新颖的组件：（1）自适应估计PI标签的可信度感知PI估计模型，其被定义为两个节点是否共享相同的节点标签，以及（2）一种解耦训练方法。

    Teaching Graph Neural Networks (GNNs) to accurately classify nodes under severely noisy labels is an important problem in real-world graph learning applications, but is currently underexplored. Although pairwise training methods have demonstrated promise in supervised metric learning and unsupervised contrastive learning, they remain less studied on noisy graphs, where the structural pairwise interactions (PI) between nodes are abundant and thus might benefit label noise learning rather than the pointwise methods. This paper bridges the gap by proposing a pairwise framework for noisy node classification on graphs, which relies on the PI as a primary learning proxy in addition to the pointwise learning from the noisy node class labels. Our proposed framework PI-GNN contributes two novel components: (1) a confidence-aware PI estimation model that adaptively estimates the PI labels, which are defined as whether the two nodes share the same node labels, and (2) a decoupled training approac
    
[^244]: 深度贝叶斯主动学习用于加速随机模拟

    Deep Bayesian Active Learning for Accelerating Stochastic Simulation. (arXiv:2106.02770v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.02770](http://arxiv.org/abs/2106.02770)

    本文提出了一个名为交互式神经过程(INP)的深度贝叶斯主动学习框架，用于学习深度代理模型以加速随机模拟过程，其中通过使用空间时间神经过程(STNP)实现模拟器动态的模拟，以及利用潜在信息增益(LIG)的主动学习方式来减少样本的复杂度。

    

    针对实现精细粒度下的大规模空间时间年龄结构流行病模型等随机模拟方法的高计算代价，本文提出了一个名为交互式神经过程(INP)的深度贝叶斯主动学习框架，用于学习深度代理模型以加速随机模拟过程。该框架由两个部分组成，即建立在神经过程(NP)家族基础之上的空间时间代理模型和一个用于主动学习的收购函数。其中，我们提出了空间时间神经过程(STNP)来模拟模拟器动态，同时在NP模型的潜空间中提出了一种新颖的收获函数——潜在信息增益(LIG)。理论分析和实践证明，LIG相对于高维随机抽样可以降低模拟样本复杂度。

    Stochastic simulations such as large-scale, spatiotemporal, age-structured epidemic models are computationally expensive at fine-grained resolution. While deep surrogate models can speed up the simulations, doing so for stochastic simulations and with active learning approaches is an underexplored area. We propose Interactive Neural Process (INP), a deep Bayesian active learning framework for learning deep surrogate models to accelerate stochastic simulations. INP consists of two components, a spatiotemporal surrogate model built upon Neural Process (NP) family and an acquisition function for active learning. For surrogate modeling, we develop Spatiotemporal Neural Process (STNP) to mimic the simulator dynamics. For active learning, we propose a novel acquisition function, Latent Information Gain (LIG), calculated in the latent space of NP based models. We perform a theoretical analysis and demonstrate that LIG reduces sample complexity compared with random sampling in high dimensions.
    
[^245]: 分布式自适应最近邻分类器：算法和理论

    Distributed Adaptive Nearest Neighbor Classifier: Algorithm and Theory. (arXiv:2105.09788v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2105.09788](http://arxiv.org/abs/2105.09788)

    提出一种新颖的分布式自适应NN分类器，通过随机选择数据驱动准则来调优最近邻数，提出了早期停止规则，实现了加速计算和改善有限样本性能。通过研究证明，当子样本大小足够大时，分类器实现了近乎最优的收敛速度。有效性已通过模拟和实证应用得到验证。

    

    当数据规模异常庞大或分布在不同的位置上时，分布式最近邻（NN）分类器是一种吸引人的分类工具。我们提出了一种新颖的分布式自适应NN分类器，其中最近邻数是由数据驱动准则随机选择的调优参数。在寻找最优调优参数时提出了一种早期停止规则，这不仅加快了计算速度，还改善了所提算法的有限样本性能。在各种子样本大小组合下，研究了分布式自适应NN分类器的超额风险收敛速度。特别地，我们证明了当子样本大小足够大时，所提分类器实现了近乎最优的收敛速度。通过模拟研究和对真实世界数据集的实证应用，证明了所提方法的有效性。

    When data is of an extraordinarily large size or physically stored in different locations, the distributed nearest neighbor (NN) classifier is an attractive tool for classification. We propose a novel distributed adaptive NN classifier for which the number of nearest neighbors is a tuning parameter stochastically chosen by a data-driven criterion. An early stopping rule is proposed when searching for the optimal tuning parameter, which not only speeds up the computation but also improves the finite sample performance of the proposed Algorithm. Convergence rate of excess risk of the distributed adaptive NN classifier is investigated under various sub-sample size compositions. In particular, we show that when the sub-sample sizes are sufficiently large, the proposed classifier achieves the nearly optimal convergence rate. Effectiveness of the proposed approach is demonstrated through simulation studies as well as an empirical application to a real-world dataset.
    
[^246]: 概率公平聚类

    Probabilistic Fair Clustering. (arXiv:2006.10916v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.10916](http://arxiv.org/abs/2006.10916)

    本文提出了一种通过概率分配获得组成员身份的不完美知识的公平聚类算法，并在这种更一般的设置中给出了逼近比保证。

    

    在聚类问题中，一个中央决策者被赋予了一个顶点的完整度量图，并且必须提供顶点的聚类，以最小化某些客观函数。在公平聚类问题中，顶点被赋予了颜色（例如，属于一个组的成员资格），有效聚类的特征也可能包括颜色在该聚类中的表示。之前的公平聚类工作假设完全知道组成员身份。在本文中，我们通过假设通过概率分配来获得组成员身份的不完美知识，对以前的工作进行了推广。我们在这种更一般的设置中提出了聚类算法，并给出了逼近比担保。我们还解决了“度量成员身份”的问题，其中不同的组具有顺序和距离的概念。使用我们提出的算法以及基线进行实验，以验证我们的方法，并在不确定地知道组成员身份时揭示微妙的担忧。

    In clustering problems, a central decision-maker is given a complete metric graph over vertices and must provide a clustering of vertices that minimizes some objective function. In fair clustering problems, vertices are endowed with a color (e.g., membership in a group), and the features of a valid clustering might also include the representation of colors in that clustering. Prior work in fair clustering assumes complete knowledge of group membership. In this paper, we generalize prior work by assuming imperfect knowledge of group membership through probabilistic assignments. We present clustering algorithms in this more general setting with approximation ratio guarantees. We also address the problem of "metric membership", where different groups have a notion of order and distance. Experiments are conducted using our proposed algorithms as well as baselines to validate our approach and also surface nuanced concerns when group membership is not known deterministically.
    
[^247]: 深度弱监督异常检测

    Deep Weakly-supervised Anomaly Detection. (arXiv:1910.13601v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1910.13601](http://arxiv.org/abs/1910.13601)

    PReNet是一种深度弱监督方法，可以检测既有已知又有未知的异常情况，通过学习成对的关系特征和异常分数，实现了异常-异常、异常-正常和正常-正常的联合学习。

    

    最近的半监督异常检测方法使用较少的标记异常样本和大量的未标记数据（大多数为正常数据）进行训练，已经显示在与无监督方法相比可以大幅提高性能。然而，这些方法通常只会关注与给定异常样本对应的异常情况，因此无法泛化到不属于此类情况的新类型/类别的异常情况。为了检测既有已知又有未知的异常情况，我们提出了一种新的深度弱监督方法，称为Pairwise Relation Prediction Network (PReNet)，通过预测任意两个随机抽取的训练实例之间的关系来学习成对的关系特征和异常分数，其中成对的关系可以是异常-异常，异常-未标记或未标记-未标记。由于未标记的实例大多是正常的，这种关系预测强制进行异常-异常、异常-正常和正常-正常的联合学习。

    Recent semi-supervised anomaly detection methods that are trained using small labeled anomaly examples and large unlabeled data (mostly normal data) have shown largely improved performance over unsupervised methods. However, these methods often focus on fitting abnormalities illustrated by the given anomaly examples only (i.e.,, seen anomalies), and consequently they fail to generalize to those that are not, i.e., new types/classes of anomaly unseen during training. To detect both seen and unseen anomalies, we introduce a novel deep weakly-supervised approach, namely Pairwise Relation prediction Network (PReNet), that learns pairwise relation features and anomaly scores by predicting the relation of any two randomly sampled training instances, in which the pairwise relation can be anomaly-anomaly, anomaly-unlabeled, or unlabeled-unlabeled. Since unlabeled instances are mostly normal, the relation prediction enforces a joint learning of anomaly-anomaly, anomaly-normal, and normal-normal
    
[^248]: 在网约车叫车系统中预测乘客起终点的模型提出

    Proposing a Model for Predicting Passenger Origin-Destination in Online Taxi-Hailing Systems. (arXiv:1910.08145v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1910.08145](http://arxiv.org/abs/1910.08145)

    本文提出了一种模型，采用K均值和非负矩阵分解等方法，预测网约车叫车系统中乘客行程的起点和终点，相较于现有模型，具有更高的预测准确度。

    

    预测乘客的起终点对智能交通管理中的交通规划、交通管理和调度优化有着重要的意义。本文提出了一种模型，用于在给定时间窗口内预测行程的起点和终点。我们采用K均值聚类在四维空间中进行聚类，并设置了起点和终点区域的最大聚类大小约束来确定有效的出行流。由于集群数量庞大，我们采用非负矩阵分解降低出行集群的数量。此外，我们还实现了一种堆叠循环神经网络模型，用于预测每个集群中的出行次数。与现有模型的结果比较表明，我们的模型在1小时和30分钟的时间窗口内实现了5-7\%和14\%的较低均方绝对误差（MAPE）。

    Due to the significance of transportation planning, traffic management, and dispatch optimization, predicting passenger origin-destination has emerged as a crucial requirement for intelligent transportation systems management. In this study, we present a model designed to forecast the origin and destination of travels within a specified time window. To derive meaningful travel flows, we employ K-means clustering in a four-dimensional space with a maximum cluster size constraint for origin and destination zones. Given the large number of clusters, we utilize non-negative matrix factorization to reduce the number of travel clusters. Furthermore, we implement a stacked recurrent neural network model to predict the travel count in each cluster. A comparison of our results with existing models reveals that our proposed model achieves a 5-7\% lower mean absolute percentage error (MAPE) for 1-hour time windows and a 14\% lower MAPE for 30-minute time windows.
    
[^249]: 具有连续观测空间的POMDPs中的稀疏树搜索最优性保证

    Sparse tree search optimality guarantees in POMDPs with continuous observation spaces. (arXiv:1910.04332v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1910.04332](http://arxiv.org/abs/1910.04332)

    本研究证明了一种基于采样的算法，部分可观察加权稀疏采样（POWSS），可以在具有连续观测空间的POMDPs中准确估计Q值，并通过增加计算能力来实现接近最优解。

    

    具有连续状态和观测空间的部分可观察马尔可夫决策过程（POMDPs）具有表示实际决策和控制问题的强大灵活性，但解决起来非常困难。最近，使用观测权重的在线基于采样的算法在具有连续观测空间的领域中展现了前所未有的有效性。然而，这种技术尚未有正式的理论证明。本文提供了这样的证明，证明一种简化方法，部分可观察加权稀疏采样（POWSS），将正确地估计Q值，并且可以通过增加计算能力来使其接近最优解。

    Partially observable Markov decision processes (POMDPs) with continuous state and observation spaces have powerful flexibility for representing real-world decision and control problems but are notoriously difficult to solve. Recent online sampling-based algorithms that use observation likelihood weighting have shown unprecedented effectiveness in domains with continuous observation spaces. However there has been no formal theoretical justification for this technique. This work offers such a justification, proving that a simplified algorithm, partially observable weighted sparse sampling (POWSS), will estimate Q-values accurately with high probability and can be made to perform arbitrarily near the optimal solution by increasing computational power.
    
[^250]: 用结构化预测算法学习代码特征与代码转换之间的关系

    Learning the Relation between Code Features and Code Transforms with Structured Prediction. (arXiv:1907.09282v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/1907.09282](http://arxiv.org/abs/1907.09282)

    本文提出了一种用条件随机场在AST节点级别预测代码转换的方法，它在Java程序的修复转换预测的上下文中实例化，并成功解决了训练数据不平衡问题，发现了一组多样化的新的和有用的修复转换。

    

    本文提出了一种利用条件随机场（CRFs）在AST节点级别预测代码转换的方法，以有效地指导自动代码演变技术中代码转换空间的探索。我们的方法首先离线学习概率模型，捕捉特定代码转换应用于特定AST节点的方式，然后利用学习的模型为任意新的、看不见的代码片段预测转换。我们在Java程序的修复转换预测的上下文中实例化了方法，包括一组精心设计的代码特征，解决了训练数据不平衡问题，并涵盖不同语法表示之间的翻译，以便跨代码应用修复转换。实验结果表明，我们的方法优于基于启发式规则或机器学习的以往工作，并且有效地、高效地发现了一组多样化的新的和有用的修复转换。

    To effectively guide the exploration of the code transform space for automated code evolution techniques, we present in this paper the first approach for structurally predicting code transforms at the level of AST nodes using conditional random fields (CRFs). Our approach first learns offline a probabilistic model that captures how certain code transforms are applied to certain AST nodes, and then uses the learned model to predict transforms for arbitrary new, unseen code snippets. {Our approach involves a novel representation of both programs and code transforms. Specifically, we introduce the formal framework for defining the so-called AST-level code transforms and we demonstrate how the CRF model can be accordingly designed, learned, and used for prediction}. We instantiate our approach in the context of repair transform prediction for Java programs. Our instantiation contains a set of carefully designed code features, deals with the training data imbalance issue, and comprises tran
    

