# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Replicability and stability in learning.](http://arxiv.org/abs/2304.03757) | 该论文研究了机器学习中的可复制性和全局稳定性，并证明许多学习任务只能弱化地实现全局稳定性。 |
| [^2] | [Online Learning for Scheduling MIP Heuristics.](http://arxiv.org/abs/2304.03755) | 本文介绍了一种基于在线学习的启发式方法调度技术，针对单个问题实例采用适应性框架，超越现有文献中的工作，同时控制两个不同类型的启发式方法，经过基准测试表明该方法能有效提高性能。 |
| [^3] | [Perspectives on AI Architectures and Co-design for Earth System Predictability.](http://arxiv.org/abs/2304.03748) | 本研究探讨了人工智能架构和共设计对地球系统可预测性的影响，提供了发展新的架构和策略以促进应用人工智能在地球系统建模和预测领域的发展的观点。 |
| [^4] | [Assessing Perceived Fairness from Machine Learning Developer's Perspective.](http://arxiv.org/abs/2304.03745) | 机器学习开发者在公平性中优先考虑准确性、代表性和透明度，同时面临数据偏见和缺乏指导方针等挑战。 |
| [^5] | [Full Gradient Deep Reinforcement Learning for Average-Reward Criterion.](http://arxiv.org/abs/2304.03729) | 本论文将全梯度DQN算法从折扣奖励马尔可夫决策过程扩展至平均奖励问题，并在神经网络函数逼近环境中证明其在不同任务中具有更好的收敛速度。 |
| [^6] | [Predicting quantum chemical property with easy-to-obtain geometry via positional denoising.](http://arxiv.org/abs/2304.03724) | 该论文提出了一种方法，利用位置去噪预测易得几何结构的量子化学性质，可以用相对容易获得的几何结构，精确预测性质，在分子性质以及化学反应性质的预测任务中都表现优秀。 |
| [^7] | [Beyond Privacy: Navigating the Opportunities and Challenges of Synthetic Data.](http://arxiv.org/abs/2304.03722) | 合成数据的潜力远非私有化数据，而是包括创建更公平的数据，数据增强，模拟和生成更多样化的数据集，然而，合成数据的应用仍然需要克服基本挑战，最重要的是如何评估其可靠性。 |
| [^8] | [Representer Theorems for Metric and Preference Learning: A Geometric Perspective.](http://arxiv.org/abs/2304.03720) | 该论文提出了度量学习和偏好学习的新的表现定理，解决了度量学习任务以三元组比较为基础的表现定理问题。这种表现定理可以用内积诱导的范数来表示。 |
| [^9] | [Integrating Edge-AI in Structural Health Monitoring domain.](http://arxiv.org/abs/2304.03718) | 本文提出了在结构健康监测(SHM)领域中将边缘人工智能集成用于实时桥梁检测的框架，并使用商业边缘人工智能平台来开发和分析边缘人工智能设备的效果。 |
| [^10] | [On the Importance of Contrastive Loss in Multimodal Learning.](http://arxiv.org/abs/2304.03717) | 对比损失在多模态学习中是非常重要的，使模型能够有效地平衡所学表示，正对推动模型对齐表示，而负对则保持学习到的表示平衡。 |
| [^11] | [Deepfake Detection with Deep Learning: Convolutional Neural Networks versus Transformers.](http://arxiv.org/abs/2304.03698) | 本文研究了深度伪造检测中的卷积神经网络和变压器网络，开发出了八种有前途的深度学习架构用于检测深度伪造，并在多个数据集上进行了实验。在多个数据集上，我们的单模型检测器取得了较高的准确性和AUC性能。本研究还分析了模型的优劣和超参数对模型性能的影响。 |
| [^12] | [HumanLight: Incentivizing Ridesharing via Human-centric Deep Reinforcement Learning in Traffic Signal Control.](http://arxiv.org/abs/2304.03697) | 本文介绍了一种名为HumanLight的算法，采用人性化的强化学习方法，在交通信号控制中激励大家拼车，缓解交通拥堵和减少污染。算法通过奖励乘坐大容量载客工具的通勤者，实现对绿灯时间的公平分配。 |
| [^13] | [High Accuracy Uncertainty-Aware Interatomic Force Modeling with Equivariant Bayesian Neural Networks.](http://arxiv.org/abs/2304.03694) | 本文引入了一种新的Monte Carlo Markov Chain抽样算法以解决常用算法无法收敛的问题，同时结合了新型随机神经网络模型，可以实现高精度不确定性感知的分子间力建模。 |
| [^14] | [Feature Mining for Encrypted Malicious Traffic Detection with Deep Learning and Other Machine Learning Algorithms.](http://arxiv.org/abs/2304.03691) | 本文分析了恶意流量检测中的特征提取问题，并提出了一种专门针对加密恶意流量的特征和概念，同时提出了一个有深度学习和传统机器学习算法的检测框架，并在实验中取得了优异的检测表现。 |
| [^15] | [Machine Learning with Requirements: a Manifesto.](http://arxiv.org/abs/2304.03674) | 本文提出一个带需求的机器学习宣言，认为需求定义和满足可以在很大程度上使机器学习模型更适用于现实世界中的关键领域，作者提出了两个问题，其中（i）需求自然而然地出现，（ii）机器学习模型被或可以成功地部署，并且（iii）忽略需求可能会产生严重后果，提出了一种新型的金字塔式开发流程，在其中，需求定义可能会影响到流程中所有后续阶段，反之亦然。 |
| [^16] | [Don't Bet on Luck Alone: Enhancing Behavioral Reproducibility of Quality-Diversity Solutions in Uncertain Domains.](http://arxiv.org/abs/2304.03672) | 本文提出了一种称为ARIA的模块，可在任何QD算法中提高存档中存在的解决方案的可重现性。该方法通过优化解决方案的概率和适应度进行变异，从而应对不可预测的噪音环境。 |
| [^17] | [Contraction-Guided Adaptive Partitioning for Reachability Analysis of Neural Network Controlled Systems.](http://arxiv.org/abs/2304.03671) | 本文提出了一种基于收缩引导的自适应分区算法，用于改善带有神经网络控制器和干扰的非线性反馈回路中区间值鲁棒可达集估计，该算法通过将神经网络验证步骤和可达性分区层的解耦，可以在很小的计算成本下提供精度提升。 |
| [^18] | [Fairness through Aleatoric Uncertainty.](http://arxiv.org/abs/2304.03646) | 研究不确定性与公平性的关系，通过贝叶斯学习估算样本预测不确定性，发现低不确定性的数据更准确和公平，提出一种基于不确定性量化定义的新的公平性-效用目标。 |
| [^19] | [A Block Coordinate Descent Method for Nonsmooth Composite Optimization under Orthogonality Constraints.](http://arxiv.org/abs/2304.03641) | 本文提出了一种新的块坐标下降方法OBCD，用于解决具有正交约束的一般非光滑组合问题。 OBCD是一种可行的方法，具有低的计算复杂性，并且获得严格的收敛保证。 |
| [^20] | [FedDiSC: A Computation-efficient Federated Learning Framework for Power Systems Disturbance and Cyber Attack Discrimination.](http://arxiv.org/abs/2304.03640) | 本文提出了一种名为FedDiSC的联邦学习框架，能够在保护隐私和提高计算效率的条件下，同时检测电力系统干扰和网络攻击。 |
| [^21] | [Theoretical Conditions and Empirical Failure of Bracket Counting on Long Sequences with Linear Recurrent Networks.](http://arxiv.org/abs/2304.03639) | 本文研究了最简单的线性单元RNN在长序列计数问题上的极限，理论上使用的条件具有充分必要性；实验数据表明，通过标准的方法，该网络通常无法实现准确的计数行为。 |
| [^22] | [Compressed Regression over Adaptive Networks.](http://arxiv.org/abs/2304.03638) | 本文阐述了一个分布式智能体的网络如何合作解决回归问题，在通信约束、自适应和合作的情况下能够达到的性能，并探讨了分布式回归问题的基本属性与最优分配通信资源之间的定量关系。 |
| [^23] | [Asynchronous Federated Continual Learning.](http://arxiv.org/abs/2304.03626) | 该论文介绍了一种新的联邦学习环境——异步联邦连续学习(AFCL)，它使用基于原型的学习、表示损失、分形预训练以及修改的聚合策略，名为FedSpace。通过在CIFAR-100数据集上的实验，该方法在三种联邦划分下，分别使用50、100和500个客户端，得到了令人满意的结果。 |
| [^24] | [Revisiting Automated Prompting: Are We Actually Doing Better?.](http://arxiv.org/abs/2304.03609) | 本文重审自动提示技术在六个不同的任务和更广泛范围的K-shot学习设置上的表现，发现自动提示并不能始终优于手动提示，因此手动提示应该作为自动提示的一个基准线。 |
| [^25] | [Deep Reinforcement Learning-Based Mapless Crowd Navigation with Perceived Risk of the Moving Crowd for Mobile Robots.](http://arxiv.org/abs/2304.03593) | 本论文提出了一种基于碰撞概率的无图Crowd Navigation方法，使用深度强化学习(DRL)来感知人群的危险程度，确保机器人在通过拥挤环境时的安全，同时提高模型的可扩展性。 |
| [^26] | [Graphon Estimation in bipartite graphs with observable edge labels and unobservable node labels.](http://arxiv.org/abs/2304.03590) | 研究了标签可观测、节点不可观测下的二部图图估计问题，在分段常数和H\"older连续图谱的情况下找到了有限的样本风险界限。 |
| [^27] | [On Efficient Training of Large-Scale Deep Learning Models: A Literature Review.](http://arxiv.org/abs/2304.03589) | 研究深度学习模型高效训练方法已有不少成果，但尚缺乏全面总结。本文综述分为数据中心化、模型中心化、超参数优化、深度学习硬件和训练策略等五个方面，比较详尽地回顾了加速深度学习模型训练的基本组件。 |
| [^28] | [Anomalous Sound Detection using Audio Representation with Machine ID based Contrastive Learning Pretraining.](http://arxiv.org/abs/2304.03588) | 本文提出了一种使用基于机器ID的对比学习预训练的音频表示来进行异常声音检测的方法，在DCASE 2020 Challenge Task2 数据集上表现优于目前最先进的基于对比学习或自监督分类的方法。 |
| [^29] | [Weakly supervised segmentation with point annotations for histopathology images via contrast-based variational model.](http://arxiv.org/abs/2304.03572) | 本文提出了一种基于对比度变分模型的方法来实现组织病理学图像的弱监督分割，为训练深度学习分割模型提供可靠补充监督。 |
| [^30] | [$\beta$-Variational autoencoders and transformers for reduced-order modelling of fluid flows.](http://arxiv.org/abs/2304.03571) | 本文提出了一种使用$\beta$-VAE和Transformer相结合的方法来学习紧凑且近似正交的ROMs，该方法可用于混沌流体流动的降阶建模，并在准确性方面优于其他预测模型。 |
| [^31] | [A physics-informed neural network framework for modeling obstacle-related equations.](http://arxiv.org/abs/2304.03552) | 本文拓展了基于物理知识的神经网络(PINN) 来解决求解障碍物相关的偏微分方程问题，这种类型的问题需要解决数值方法的难度较大，但作者通过对多种情况的研究证明了PINN的有效性。 |
| [^32] | [AI Model Disgorgement: Methods and Choices.](http://arxiv.org/abs/2304.03545) | 随着机器学习模型的复杂性和数据量的增加，模型错误变得更难以修复，模型排除作为一种解决方案被提出。 |
| [^33] | [HyperTab: Hypernetwork Approach for Deep Learning on Small Tabular Datasets.](http://arxiv.org/abs/2304.03543) | HyperTab是一种基于超网络结合了随机森林和神经网络优点的小型表格数据深度学习方法，使用每个特定低维视图处理数据，虚拟增加训练样本数量，避免过度拟合。 |
| [^34] | [ChatPipe: Orchestrating Data Preparation Program by Optimizing Human-ChatGPT Interactions.](http://arxiv.org/abs/2304.03540) | ChatPipe 提出了一个新系统，旨在通过自然语言交互优化 ChatGPT 编排 ML 数据准备程序，有效推荐下一个数据准备操作，方便用户进行程序的修改和版本切换，能够显著减少 ML 数据准备所需的时间和精力。 |
| [^35] | [Adjustable Privacy using Autoencoder-based Learning Structure.](http://arxiv.org/abs/2304.03538) | 本论文介绍了一种基于自编码器的学习结构，可以管理数据提供商与推理中心之间的效用-隐私权衡，同时在保持机密特征的隐私的同时提供有关数据的非机密特征。 |
| [^36] | [Leveraging GANs for data scarcity of COVID-19: Beyond the hype.](http://arxiv.org/abs/2304.03536) | 本论文分析了43篇使用GAN产生合成数据的报告，发现这些研究存在数据偏见、缺乏可重复性以及缺乏临床相关性。GAN基于人工智能的方法有望为诊断COVID-19的AI模型产生合成数据，但需要解决这些问题。 |
| [^37] | [SSS at SemEval-2023 Task 10: Explainable Detection of Online Sexism using Majority Voted Fine-Tuned Transformers.](http://arxiv.org/abs/2304.03518) | 本文描述了使用细调BERT模型和多数投票集成模型来检测和解释在线性别歧视的方法。翻转显着降低了女性在社交媒体平台上经历不成比例的性别歧视的风险。 |
| [^38] | [Distributional Signals for Node Classification in Graph Neural Networks.](http://arxiv.org/abs/2304.03507) | 本文提出了一种通用的GNN正则化方法，通过使用节点标签的分布来进行编码，提升了节点分类任务中大多数基本GNN模型的性能。 |
| [^39] | [F-RDW: Redirected Walking with Forecasting Future Position.](http://arxiv.org/abs/2304.03497) | F-RDW是一种基于未来位置预测的重定向行走机制，可将预测信息与现有的重定向行走方法相结合，显著减少重置次数，同时能够适用于各种虚拟环境。 |
| [^40] | [Architecture-Preserving Provable Repair of Deep Neural Networks.](http://arxiv.org/abs/2304.03496) | 本文提出了一种保体系结构 V-多面体可证明修复深度神经网络的方法。修复只修改 DNN 的参数，具有灵活性，支持多种类型的层，并在多项式时间内运行。 |
| [^41] | [ParaGraph: Weighted Graph Representation for Performance Optimization of HPC Kernels.](http://arxiv.org/abs/2304.03487) | ParaGraph提出了一种基于加权图的程序表示法，用于优化HPC内核代码，帮助机器学习算法传递应用程序特定信息以帮助提高性能。 |
| [^42] | [RED-PSM: Regularization by Denoising of Partially Separable Models for Dynamic Imaging.](http://arxiv.org/abs/2304.03483) | 本文提出了一种称为RED-PSM的方法，将部分可分模型与去噪正则化相结合，用于解决动态成像问题，数值实验证明其优越性。 |
| [^43] | [Rethinking GNN-based Entity Alignment on Heterogeneous Knowledge Graphs: New Datasets and A New Method.](http://arxiv.org/abs/2304.03468) | 文章重新考虑了基于GNN的异构知识图谱实体对齐。为探究EA实际场景中的表现，提出了更接近现实的高度异构知识图谱数据集，并提出了新方法。 |
| [^44] | [A Policy for Early Sequence Classification.](http://arxiv.org/abs/2304.03463) | 本论文提出了一种用于早期序列分类的新策略——分类器诱导停止。与以往方法依赖探索学习停止和分类不同，本方法采用监督方法直接进行分类，AUC值可增加11.8%。 |
| [^45] | [Rethinking Evaluation Protocols of Visual Representations Learned via Self-supervised Learning.](http://arxiv.org/abs/2304.03456) | 本研究重新思考基于自监督学习的视觉表示的评价协议，发现对于线性探测来说，输入归一化是消除性能变化的关键因素。 |
| [^46] | [Graph Enabled Cross-Domain Knowledge Transfer.](http://arxiv.org/abs/2304.03452) | 稀缺知识对自动化决策造成了障碍，跨领域知识转移是通过融合来自不同领域的辅助信息，缓解不同领域知识差距的一种方法。 |
| [^47] | [Generative Agents: Interactive Simulacra of Human Behavior.](http://arxiv.org/abs/2304.03442) | 本文介绍了一种生成代理的架构，它能够仿真出具有可信度的人类行为，填充交互式沙盒环境，为创造更加真实的人机交互体验提供了一种新的思路。 |
| [^48] | [Supervised Contrastive Learning with Heterogeneous Similarity for Distribution Shifts.](http://arxiv.org/abs/2304.03440) | 本文提出了一种带有异构相似性的新的监督对比学习方法，用于解决分布偏移问题，防止过拟合影响模型性能。 |
| [^49] | [Domain Generalization In Robust Invariant Representation.](http://arxiv.org/abs/2304.03431) | 本文研究了不变表示的泛化性能，证明具有不变表示的模型可以学习到具有鲁棒性的非结构化潜在表示，因此使不变性成为域泛化的一个关键方面。 |
| [^50] | [Cleansing Jewel: A Neural Spelling Correction Model Built On Google OCR-ed Tibetan Manuscripts.](http://arxiv.org/abs/2304.03427) | 本文提出了一种基于谷歌OCR扫描的藏文手稿的神经拼写纠错模型，可以自动纠正OCR输出中的噪声。 |
| [^51] | [A modular framework for stabilizing deep reinforcement learning control.](http://arxiv.org/abs/2304.03422) | 该论文提出了一个结合了深度强化学习的优化驱动和无模型优势、使用Youla-Kucera参数化定义搜索域提供稳定性保证的框架，利用一个数据驱动的内部模型实现替代方法，采用神经网络无缝地与标准深度学习库集成，实现了在一个真实的两个水箱系统的模拟中，通过表达参数化的非线性稳定算子集的方法，设计反馈控制器。 |
| [^52] | [To Wake-up or Not to Wake-up: Reducing Keyword False Alarm by Successive Refinement.](http://arxiv.org/abs/2304.03416) | 本文提出了通过连续细化来减少关键词误报的方法，并展示其在多个模型上的有效性，可应用于任何深度关键词监测系统。 |
| [^53] | [Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks.](http://arxiv.org/abs/2304.03408) | 本研究分析了宽但有限的特征学习神经网络中有限宽度效应的动力学，提供了对网络权重随机初始化下DMFT序参数波动的表征以及特征学习如何动态地减少最终NTK和最终网络预测的方差。 |
| [^54] | [Localized Region Contrast for Enhancing Self-Supervised Learning in Medical Image Segmentation.](http://arxiv.org/abs/2304.03406) | 本文提出了一种基于局部区域对比的医学图像自监督学习增强框架，以提高多器官分割等密集预测任务的性能，实验结果表明该方法可以超越最先进的自监督学习方法。 |
| [^55] | [Quantum Conformal Prediction for Reliable Uncertainty Quantification in Quantum Machine Learning.](http://arxiv.org/abs/2304.03398) | 本文提出了一种通用方法，可以可靠地量化量子模型的不确定性，无论训练数据的数量、拍摄次数、ansatz、训练算法以及量子硬件噪声的存在如何。 |
| [^56] | [Deep Learning for Opinion Mining and Topic Classification of Course Reviews.](http://arxiv.org/abs/2304.03394) | 本文利用自然语言处理和深度学习技术，通过比较传统方法和现代机器学习方法，展示了如何处理大量课程评论，进行情感极性分析和主题分类。 |
| [^57] | [Personalizing Digital Health Behavior Change Interventions using Machine Learning and Domain Knowledge.](http://arxiv.org/abs/2304.03392) | 该论文提出了一种采用机器学习和领域知识进行个性化数字健康行为变革干预的系统，其利用反事实例子进行特征控制以预测干预效果并优化干预效果。 |
| [^58] | [EZClone: Improving DNN Model Extraction Attack via Shape Distillation from GPU Execution Profiles.](http://arxiv.org/abs/2304.03388) | 本论文介绍了两种不同威胁模型下的DNN结构提取技术，其中EZClone利用聚合GPU文件作为侧信道来预测DNN结构，并且通过实验验证了其有效性。 |
| [^59] | [Wide neural networks: From non-gaussian random fields at initialization to the NTK geometry of training.](http://arxiv.org/abs/2304.03385) | 本研究研究大规模但有限的神经网络行为。主要贡献为：（1）计算高斯性的修正，系数由参数初始化和激活函数的统计学确定。（2）通过计算网络与极限情况下的偏差来控制网络在训练时的输出，具有更好的效果。 |
| [^60] | [Scalable Causal Discovery with Score Matching.](http://arxiv.org/abs/2304.03382) | 该论文提出了一种利用分数匹配算法实现可扩展因果推断的方法，该算法可从非线性可加性高斯噪声模型的对数似然函数中发现整个因果图，并通过实现与当前最先进技术相当的准确性来降低了计算门槛。 |
| [^61] | [Self-Supervised Video Similarity Learning.](http://arxiv.org/abs/2304.03378) | 本文提出了自监督视频相似性学习的方法S$^2$VS，该方法通过学习实例区分解决多个检索和检测任务，无需用到标注数据，并在各个任务上都达到了最新的性能。 |
| [^62] | [Interpretable statistical representations of neural population dynamics and geometry.](http://arxiv.org/abs/2304.03376) | 该论文提出了一种基于统计分布的几何深度学习框架，用于表示非线性动态系统的几何感知或几何无感知表示，以对已测量轨迹进行无偏比较。利用该方法，能够解释神经动力学的嵌入，在灵长类似任务中取得了最先进的准确性。 |
| [^63] | [Optimizing Neural Networks through Activation Function Discovery and Automatic Weight Initialization.](http://arxiv.org/abs/2304.03374) | 本文介绍了发现更强大的激活函数和建立更稳健的神经网络权重初始化技术的方法，这些方法比传统方法更优秀，同时提供了神经网络优化的新视角。 |
| [^64] | [Reliable Learning for Test-time Attacks and Distribution Shift.](http://arxiv.org/abs/2304.03370) | 本文提出了可靠的学习方法以抵御测试时攻击和分布偏移，在测试时引入了新的可靠性保障方法，确保预测结果正确。同时，该学习方法能够适应任意测试点，具有非常好的可靠性。 |
| [^65] | [From Explanation to Action: An End-to-End Human-in-the-loop Framework for Anomaly Reasoning and Management.](http://arxiv.org/abs/2304.03368) | 本文提出了一种名为ALARM的端到端框架，支持从异常检测到人机交互式处理，从而最终实现新规则补充了基于规则的监督检测。 |
| [^66] | [Robust Decision-Focused Learning for Reward Transfer.](http://arxiv.org/abs/2304.03365) | 本文介绍了一种稳健决策重点（RDF）算法，利用非识别性的DF解，学习同时最大化期望回报和抵御奖励函数变化的模型，可以显著提高DF对奖励函数变化的稳健性，而不会降低智能体的总回报。 |
| [^67] | [NMR shift prediction from small data quantities.](http://arxiv.org/abs/2304.03361) | 提出了一种新的机器学习模型，能够以相对较少的数据量预测NMR化学位移，展示了在特定溶剂中预测小分子的19F和13C NMR化学位移方面的良好效果。 |
| [^68] | [Graph Collaborative Signals Denoising and Augmentation for Recommendation.](http://arxiv.org/abs/2304.03344) | 本文提出了一种新的图邻接矩阵，它包括了用户-用户和项目-项目的相关性，以及一个经过适当设计的用户-项目交互矩阵，并通过预训练和top-K采样增强了用户-项目交互矩阵，以更好地适应所有用户的需求。 |
| [^69] | [Spintronic Physical Reservoir for Autonomous Prediction and Long-Term Household Energy Load Forecasting.](http://arxiv.org/abs/2304.03343) | 本研究展示了利用自旋电子物理水库进行自治型长期预测任务的方法，可以用于建模混沌时间序列和动态时间序列数据，是适合在边缘设备上进行实时学习的。这里提出的基于微旋磁隧穿结的涡旋子可以作为实现此种RC的原型。 |
| [^70] | [Maximal Ordinal Two-Factorizations.](http://arxiv.org/abs/2304.03338) | 本文研究了最大序数二次因子分解问题，证明了其判定是否存在是一个NP完全问题，并提供了用于计算最大因子分解的算法Ord2Factor。 |
| [^71] | [On the Learnability of Multilabel Ranking.](http://arxiv.org/abs/2304.03337) | 研究了一系列排名损失函数下多标签排名问题在批处理和在线设置下的可学习性，并首次给出基于可学习性的排名损失函数的等价类。 |
| [^72] | [ChatGPT-Crawler: Find out if ChatGPT really knows what it's talking about.](http://arxiv.org/abs/2304.03325) | 本文分析了从不同对话QA语料库中生成的ChatGPT的响应，并比较了其与正确答案的相似度。研究发现ChatGPT在某些情况下提供了错误的答案，提供了潜在用户和开发者的宝贵见解。 |
| [^73] | [Towards Coherent Image Inpainting Using Denoising Diffusion Implicit Models.](http://arxiv.org/abs/2304.03322) | 本研究提出了一种基于Bayesian框架和去噪扩散隐式模型的方法CO-PAINT，用于解决图像修复中修复区域和未修复区域不协调问题，实现了最先进的性能表现。 |
| [^74] | [Adaptive Decision-Making with Constraints and Dependent Losses: Performance Guarantees and Applications to Online and Nonlinear Identification.](http://arxiv.org/abs/2304.03321) | 本文提出了一种带约束和依赖损失的自适应决策算法，并证明了其有效性。该算法可用于在线和非线性识别等任务。 |
| [^75] | [Neural Operator Learning for Ultrasound Tomography Inversion.](http://arxiv.org/abs/2304.03297) | 本文首次将神经操作符学习应用于超声断层成像反演，通过学习时间飞行数据和异质声速场之间的映射，实现了可避免计算密集型反演过程的预测异质声场模型。该模型有潜在的在乳腺成像中进行软组织分布预测和肿瘤识别的实时应用。 |
| [^76] | [SS-shapelets: Semi-supervised Clustering of Time Series Using Representative Shapelets.](http://arxiv.org/abs/2304.03292) | 本论文提出了一种名为SS-shapelets的半监督时间序列聚类方法，通过使用少量标记的和传播的伪标记时间序列来发现代表性形状子序列，从而提高聚类准确性。 |
| [^77] | [Comparing NARS and Reinforcement Learning: An Analysis of ONA and $Q$-Learning Algorithms.](http://arxiv.org/abs/2304.03291) | 本文比较了NARS和强化学习在解决序列任务方面的性能，发现NARS在各种环境中都有较好的表现，尤其是在非确定性环境中。 |
| [^78] | [VISHIEN-MAAT: Scrollytelling visualization design for explaining Siamese Neural Network concept to non-technical users.](http://arxiv.org/abs/2304.03288) | 本研究提出了一种基于滚动叙述(scrollytelling）的可视化设计，用于向非技术用户解释人工智能概念，其中以孪生神经网络为例，提供了具有直观解释的交互界面。 |
| [^79] | [Synthesis of Mathematical programs from Natural Language Specifications.](http://arxiv.org/abs/2304.03287) | 本论文关注于通过自然语言规范中的目标和约束合成数学程序，并通过评估CodeT5和使用GPT-3来生成需要的示例进行实验。 |
| [^80] | [Inductive Graph Unlearning.](http://arxiv.org/abs/2304.03093) | 本论文介绍了针对图形数据的反学习，旨在实现机器学习中的“被遗忘权”，与其他框架相比，提出了一个新颖的归纳式框架。这个框架可以让机器学习系统在处理动态改变的图形时更具有适应性。 |
| [^81] | [Heavy-Tailed Regularization of Weight Matrices in Deep Neural Networks.](http://arxiv.org/abs/2304.02911) | 本文介绍了一种名为重尾部正则化的技术，在深度神经网络中通过明确提倡更重的重尾谱来提高泛化性能。与标准正则化技术相比，该方法在基准数据集上实现了显着的改进。 |
| [^82] | [ViralVectors: Compact and Scalable Alignment-free Virome Feature Generation.](http://arxiv.org/abs/2304.02891) | ViralVectors是一种紧凑且可扩展的方法，从virome测序数据中生成Minimizers特征向量进行有效的下游分析。该方法优于现有非比对技术方法，可以区分不同的病毒家族，甚至属，并能够提供接近最优的SARS-CoV-2分类性能。 |
| [^83] | [ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast.](http://arxiv.org/abs/2304.02689) | 本文提出了一种改进的对比学习框架ACTION++，通过自适应的解剖对比来改善半监督医学图像分割。 |
| [^84] | [Local Intrinsic Dimensional Entropy.](http://arxiv.org/abs/2304.02223) | 本文提出了一种新的在连续空间中测量熵的方法，称为ID-Entropy，它可以用于多轮数据变换和扭曲，同时可以捕捉数据的维度。 |
| [^85] | [Initialization Approach for Nonlinear State-Space Identification via the Subspace Encoder Approach.](http://arxiv.org/abs/2304.02119) | 本论文介绍一个使用最佳线性逼近(BLA)初始化子空间编码器方法的初始方法，以提高非线性状态空间识别的收敛性。 |
| [^86] | [Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning.](http://arxiv.org/abs/2304.01203) | 本文介绍了一种新的强化学习方法——准度量强化学习（QRL），利用准度量模型来学习最优价值函数；在离线和在线的目标达成基准测试中，QRL展示了更好的采样效率和性能，包括基于状态和基于图像的观测。 |
| [^87] | [Ensemble weather forecast post-processing with a flexible probabilistic neural network approach.](http://arxiv.org/abs/2303.17610) | 本论文提出了一种神经网络和归一化流相结合的方法，可联合预测所有位置和提前期，从而放宽了许多传统后处理方法的分布假设，并通过EUPPBench基准测试证明了其超越性能。 |
| [^88] | [Efficient Parallel Split Learning over Resource-constrained Wireless Edge Networks.](http://arxiv.org/abs/2303.15991) | 本文提出了面向资源受限的无线边缘网络的高效并行分裂学习（EPSL）框架，旨在加速模型训练。EPSL并行化客户端模型训练，通过聚合梯度降低了反向传播的局部梯度维度，从而显著减少了服务器端的训练和通信延迟。同时，EPSL还设计了资源分配算法以优化计算和通信资源分配。 |
| [^89] | [GAS: A Gaussian Mixture Distribution-Based Adaptive Sampling Method for PINNs.](http://arxiv.org/abs/2303.15849) | GAS是一种基于高斯混合分布的自适应采样方法，用于加速PINNs的收敛过程并提高精度，已在2D到10D问题的数值模拟中表现出领先于深层求解器、与传统数值求解器相当的优异性能。 |
| [^90] | [Viewpoint Equivariance for Multi-View 3D Object Detection.](http://arxiv.org/abs/2303.14548) | 本文提出了一种利用多视角几何学习视点等变性以提高三维物体检测定位精度的框架VEDet，并通过基于查询的transformer架构和视角条件的查询来实现。 |
| [^91] | [Online Learning for the Random Feature Model in the Student-Teacher Framework.](http://arxiv.org/abs/2303.14083) | 本研究考虑了一种两层神经网络模型的在线学习，研究发现，当学生的隐藏层大小呈指数增长时，完美泛化是可行的，但对于任何有限的隐藏层大小和输入维度比，学生都无法完美泛化。 |
| [^92] | [Revisiting the Fragility of Influence Functions.](http://arxiv.org/abs/2303.12922) | 本文研究了影响函数的脆弱性，并提出在非凸条件下使用深层模型和更复杂数据集来解决这一问题。 |
| [^93] | [Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings.](http://arxiv.org/abs/2303.05737) | 本文提出了一种临床BERTScore（CBERTScore）度量，它比其他度量更严厉地惩罚临床相关的错误，更接近于临床医生对医学句子的偏好。作者还收集了13个临床医生对149个现实医学句子的偏好基准，称为临床转录偏好基准（CTP），证明CBERTScore更接近于临床医生的偏好，并将基准发布给社区以进一步开发具有临床意识的ASR度量。 |
| [^94] | [Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for Soft and Hard Label Prediction.](http://arxiv.org/abs/2303.02468) | 本研究研究了在软硬标签预测中，不同激活函数对于深度神经网络模型输出的影响，并引入了一种新的正弦激活函数。 |
| [^95] | [Safe-DS: A Domain Specific Language to Make Data Science Safe.](http://arxiv.org/abs/2302.14548) | 文章介绍了一种名为Safe-DS的领域特定语言，用于在数据科学中以静态安全的方式运行Python DS库，并可捕获出错的类型，并且拥有比Python Linters更强大的能力。 |
| [^96] | [Revisiting LQR Control from the Perspective of Receding-Horizon Policy Gradient.](http://arxiv.org/abs/2302.13144) | 本文从递推视角重新审视了LQR控制问题，并应用递推-视角策略梯度（RHPG）模型提供了一种采样复杂度分析，通过无需任何先验信息进行优化求解，并展示了RHPG在线性控制和估计中的普适性。 |
| [^97] | [Anomalous NO2 emitting ship detection with TROPOMI satellite data and machine learning.](http://arxiv.org/abs/2302.12744) | 本文提出了一种利用TROPOMI卫星数据和机器学习自动选择不符合船舶排放标准的方法。 |
| [^98] | [Complex QA and language models hybrid architectures, Survey.](http://arxiv.org/abs/2302.09051) | 本文综述了语言模型架构和策略的最新进展，并重点关注混合技术在复杂问题回答中的应用，讨论了该领域的挑战和未来研究方向。 |
| [^99] | [Modality-Agnostic Variational Compression of Implicit Neural Representations.](http://arxiv.org/abs/2301.09479) | 提出了一种无模态偏见的隐式神经表示变分压缩算法，能够在不同的数据模态上表现出卓越的压缩性能和效果。 |
| [^100] | [Compact Optimization Learning for AC Optimal Power Flow.](http://arxiv.org/abs/2301.08840) | 本文提出了一种压缩学习方法，利用主成分分析可以显著降低交流电最优潮流计算的维度，降低了可训练参数的数量，提高了可扩展性和有效性。同时，该方法的输出可以用于热启动精确的AC求解器以恢复可行性。 |
| [^101] | [Label Inference Attack against Split Learning under Regression Setting.](http://arxiv.org/abs/2301.07284) | 本文研究了在回归模型下分裂学习中标签推断攻击的问题。现有的攻击只适用于离散标签的分类模型，而本文提出了一种新的攻击策略以推断连续标签。 |
| [^102] | [Plug & Play Directed Evolution of Proteins with Gradient-based Discrete MCMC.](http://arxiv.org/abs/2212.09925) | 本文介绍了一种基于机器学习的蛋白质工程采样框架，能够在模拟中进化蛋白质，通过组合无监督模型和监督模型来提高评估未见过的突变的能力，并引入了一个使用梯度来提出有前途的突变的快速MCMC采样器。 |
| [^103] | [Multimodal and Explainable Internet Meme Classification.](http://arxiv.org/abs/2212.05612) | 本文提出了一种多模态和可解释的互联网迷因分类方法，旨在解决现有方法中忽略迷因语义和创建上下文导致公正内容管理困难的问题。作者采用示例和基于原型的推理并结合文本和视觉SOTA模型进行训练，成功在两个任务中检测了有害的迷因。 |
| [^104] | [Backdoor Cleansing with Unlabeled Data.](http://arxiv.org/abs/2211.12044) | 本文提出了一种无标签数据的后门清除方法，通过逐层权重重新初始化和知识蒸馏来有效清除可疑网络的后门行为，并在基准数据集上取得较好效果。 |
| [^105] | [LP-BFGS attack: An adversarial attack based on the Hessian with limited pixels.](http://arxiv.org/abs/2210.15446) | 本文针对深度神经网络的对抗攻击，基于有限像素的Hessian矩阵信息，提出了Limited Pixel BFGS（LP-BFGS）攻击方法，该方法在攻击成功率和计算成本方面具有竞争优势。 |
| [^106] | [Fast Lifelong Adaptive Inverse Reinforcement Learning from Demonstrations.](http://arxiv.org/abs/2209.11908) | 本文提出了一种快速生涯适应性逆强化学习框架，从学习的策略中构建多样策略的组合实现了对新的演示的快速适应，同时整合演示中的共性知识，实现准确的任务推断，还能够在大规模部署中通过维护一个精简的原型策略集合并通过策略组合来逼近所有行为。 |
| [^107] | [Clustering-based Imputation for Dropout Buyers in Large-scale Online Experimentation.](http://arxiv.org/abs/2209.06125) | 本文提出一种基于聚类方法的在线实验数据填补方法，将不完整指标值的用户分为访客和缺失购买者两组，使用$k$-最近邻填补方法，并考虑实验特定的特征和用户的购物路径活动，同时使用分层和聚类结合的方式提高填补效率。 |
| [^108] | [Consistency between ordering and clustering methods for graphs.](http://arxiv.org/abs/2208.12933) | 本文研究了基于谱技术的几种聚类和排序方法之间的关系，提出了一种新的度量方式——标签连续误差，并通过合成和真实世界数据集的评估，探究了排序方法和聚类方法分别识别模块结构和带状结构的效果。 |
| [^109] | [Retweet-BERT: Political Leaning Detection Using Language Features and Information Diffusion on Social Networks.](http://arxiv.org/abs/2207.08349) | Retweet-BERT是一个简单而可扩展的模型，用于估计Twitter用户的政治倾向。该模型利用转发网络结构和用户语言特征，并在COVID-19和2020年美国总统选举数据集上展现出有竞争力的性能。研究还表明，在Twitter上存在着右倾用户之间的政治回音室。 |
| [^110] | [Machine learning of percolation models using graph convolutional neural networks.](http://arxiv.org/abs/2207.03368) | 该论文利用图卷积神经网络以监督和非监督方式研究渗透问题，并成功地训练了不同晶格类型的数据，同时，结合混淆法，实现了对渗透阈值的预测。 |
| [^111] | [Denoised MDPs: Learning World Models Better Than the World Itself.](http://arxiv.org/abs/2206.15477) | 本文提出一种新的去噪MDP学习方法，该方法可以将现实数据中的噪声干扰因素去除，学习一个更好的世界模型，实验结果表明该方法在任务上表现更加优秀。 |
| [^112] | [Supervised segmentation of NO2 plumes from individual ships using TROPOMI satellite data.](http://arxiv.org/abs/2203.06993) | 本文提出了一种基于卷积神经网络从TROPOMI卫星数据中分割出单个船只NO2排放的监督方法，为基于遥感的全球排放监测系统提供了可能。 |
| [^113] | [Evaluating feasibility of batteries for second-life applications using machine learning.](http://arxiv.org/abs/2203.04249) | 本文使用机器学习技术对电动汽车电池进行快速评估，以判断其是否适合进行二次利用。所提出的算法使用电池电压和电流等特征，并采用高斯过程回归进行预测，验证结果显示其在不同条件下均有希望得到较好的性能表现。 |
| [^114] | [Predicting Influenza A Viral Host Using PSSM and Word Embeddings.](http://arxiv.org/abs/2201.01140) | 该研究利用机器学习模型和特征提取方法，成功预测了甲型流感病毒的原始宿主，为早期和快速控制病毒传播提供了帮助。 |
| [^115] | [Object-Aware Cropping for Self-Supervised Learning.](http://arxiv.org/abs/2112.00319) | 本文提出了一种针对真实世界数据集中多个小对象的自监督学习方法，该方法将随机裁剪替换为对象提议算法获得的裁剪。 |
| [^116] | [UBnormal: New Benchmark for Supervised Open-Set Video Anomaly Detection.](http://arxiv.org/abs/2111.08644) | UBnormal是一个新的监督式开放集Benchmark，它引入了用于视频异常检测的像素级异常事件注释。与现有数据集不同，它可以使用全监督学习方法进行异常事件检测。这使得它成为一个标准的开放集问题，与其他基准不同。 |
| [^117] | [DeLag: Using Multi-Objective Optimization to Enhance the Detection of Latency Degradation Patterns in Service-based Systems.](http://arxiv.org/abs/2110.11155) | DeLag是一种新颖的自动化搜索方法，使用多目标优化以诊断服务系统中的延迟降级模式，其效果优于现有方法和通用的机器学习聚类算法。 |
| [^118] | [A Dual Approach to Constrained Markov Decision Processes with Entropy Regularization.](http://arxiv.org/abs/2110.08923) | 本文研究了带熵正则化的约束马尔可夫决策过程，并提出了一种加速双下降方法，证明了其在最优间隙和约束违反方面的全局收敛速度为$\widetilde{\mathcal {O}}(1/T)$。 |
| [^119] | [Neural Operator: Learning Maps Between Function Spaces.](http://arxiv.org/abs/2108.08481) | 本文提出了一种神经算子，可以学习无限维函数空间之间的映射，可以逼近给定的非线性连续算子且离散不变，同时介绍了四类高效参数化的神经算子。这项技术的一个重要应用是学习代理映射f。 |
| [^120] | [Likelihood-Free Frequentist Inference: Confidence Sets with Correct Conditional Coverage.](http://arxiv.org/abs/2107.03920) | 本文提出了无似然假设下的频率学派推断（LF2I）框架，通过结合经典统计和现代机器学习，实现了构建具有正确条件覆盖的置信区间的实用程序和诊断方法，在包括宇宙学参数推断在内的多个例子中都实现了覆盖性质得到大幅改善。 |

# 详细

[^1]: 学习中的可复制性和稳定性

    Replicability and stability in learning. (arXiv:2304.03757v1 [cs.LG])

    [http://arxiv.org/abs/2304.03757](http://arxiv.org/abs/2304.03757)

    该论文研究了机器学习中的可复制性和全局稳定性，并证明许多学习任务只能弱化地实现全局稳定性。

    

    可复制性是科学中的关键，因为它使我们能够验证和验证研究结果。Impagliazzo、Lei、Pitassi和Sorrell（'22）最近开始研究机器学习中的可复制性。如果同一算法在两个独立同分布输入上使用相同的内部随机性时通常产生相同的输出，则学习算法是可复制的。我们研究了一种不涉及固定随机性的可复制性变体。如果一个算法在两个独立同分布的输入上（不固定内部随机性）应用时通常产生相同的输出，则算法满足这种形式的可复制性。这个变种被称为全局稳定性，并在差分隐私的上下文中由Bun、Livni和Moran（'20）介绍。 Impagliazzo等人展示了如何提高任何可复制算法的效果，以使其产生的输出概率无限接近于1。相反，我们证明了对于许多学习任务，只能弱化地实现全局稳定性，这里输出只有相同的部分。

    Replicability is essential in science as it allows us to validate and verify research findings. Impagliazzo, Lei, Pitassi and Sorrell (`22) recently initiated the study of replicability in machine learning. A learning algorithm is replicable if it typically produces the same output when applied on two i.i.d. inputs using the same internal randomness. We study a variant of replicability that does not involve fixing the randomness. An algorithm satisfies this form of replicability if it typically produces the same output when applied on two i.i.d. inputs (without fixing the internal randomness). This variant is called global stability and was introduced by Bun, Livni and Moran (`20) in the context of differential privacy.  Impagliazzo et al. showed how to boost any replicable algorithm so that it produces the same output with probability arbitrarily close to 1. In contrast, we demonstrate that for numerous learning tasks, global stability can only be accomplished weakly, where the same o
    
[^2]: 基于在线学习的混合整数规划启发式调度方法

    Online Learning for Scheduling MIP Heuristics. (arXiv:2304.03755v1 [math.OC])

    [http://arxiv.org/abs/2304.03755](http://arxiv.org/abs/2304.03755)

    本文介绍了一种基于在线学习的启发式方法调度技术，针对单个问题实例采用适应性框架，超越现有文献中的工作，同时控制两个不同类型的启发式方法，经过基准测试表明该方法能有效提高性能。

    

    混合整数规划(MIP)是NP难问题，但现代求解器通常可以在几分钟内解决大型实际问题。这种成功部分归功于启发式方法。由于它们的行为高度依赖于实例，因此依赖于从大型异构基准实例的经验测试推导出的硬编码规则可能会导致次优性能。在这项工作中，我们提出了一种基于在线学习的方法，使启发式方法能够适应当前问题实例。我们用一个适应性框架取代了通常使用的静态启发式处理方法，该框架利用关于启发式方法行为的过去观察结果来做出未来决策。特别地，我们将控制大邻域搜索和潜水 - 两个广泛且复杂的启发式方法的问题建模为多臂赌博机问题。超越了现有文献中的工作，我们通过单个学习代理同时控制了两个不同类别的启发式方法。我们在MIPLIB 2017基准集上验证了我们的方法，并表明我们的方法优于最先进的启发式处理技术。

    Mixed Integer Programming (MIP) is NP-hard, and yet modern solvers often solve large real-world problems within minutes. This success can partially be attributed to heuristics. Since their behavior is highly instance-dependent, relying on hard-coded rules derived from empirical testing on a large heterogeneous corpora of benchmark instances might lead to sub-optimal performance. In this work, we propose an online learning approach that adapts the application of heuristics towards the single instance at hand. We replace the commonly used static heuristic handling with an adaptive framework exploiting past observations about the heuristic's behavior to make future decisions. In particular, we model the problem of controlling Large Neighborhood Search and Diving - two broad and complex classes of heuristics as a multi-armed bandit problem. Going beyond existing work in the literature, we control two different classes of heuristics simultaneously by a single learning agent. We verify our
    
[^3]: 人工智能架构和共设计对地球系统可预测性的影响

    Perspectives on AI Architectures and Co-design for Earth System Predictability. (arXiv:2304.03748v1 [cs.LG])

    [http://arxiv.org/abs/2304.03748](http://arxiv.org/abs/2304.03748)

    本研究探讨了人工智能架构和共设计对地球系统可预测性的影响，提供了发展新的架构和策略以促进应用人工智能在地球系统建模和预测领域的发展的观点。

    

    美国能源部（DOE）科学办公室生物和环境研究（BER）和高级科学计算研究（ASCR）计划最近组织并举办了“面向地球系统可预测性的人工智能（AI4ESP）”研讨会系列。从这个研讨会中，DOE BER和ASCR社区得出的一个关键结论是需要发展一个新的地球系统可预测性范式，重点是在整个领域、实验室、建模和分析活动中实现人工智能（AI），称为ModEx。BER的“模型实验”，ModEx，是一种迭代方法，使过程模型能够生成假设。所开发的假设通知采集测量和观测数据的现场和实验室工作，随后用于参数化、驱动和测试模型（例如基于过程的）预测。在这个AI4ESP工作坊系列中共举行了17个技术会议。本文讨论了其中一次会议的主题“人工智能架构和共设计对地球系统可预测性的影响”。它提供了发展新的人工智能架构和共设计策略来解决地球系统建模和预测领域特定的科学和技术需求，以推进人工智能在地球系统可预测性中的作用的观点。

    Recently, the U.S. Department of Energy (DOE), Office of Science, Biological and Environmental Research (BER), and Advanced Scientific Computing Research (ASCR) programs organized and held the Artificial Intelligence for Earth System Predictability (AI4ESP) workshop series. From this workshop, a critical conclusion that the DOE BER and ASCR community came to is the requirement to develop a new paradigm for Earth system predictability focused on enabling artificial intelligence (AI) across the field, lab, modeling, and analysis activities, called ModEx. The BER's `Model-Experimentation', ModEx, is an iterative approach that enables process models to generate hypotheses. The developed hypotheses inform field and laboratory efforts to collect measurement and observation data, which are subsequently used to parameterize, drive, and test model (e.g., process-based) predictions. A total of 17 technical sessions were held in this AI4ESP workshop series. This paper discusses the topic of the `
    
[^4]: 从机器学习开发者的角度评估感知公平性

    Assessing Perceived Fairness from Machine Learning Developer's Perspective. (arXiv:2304.03745v1 [cs.LG])

    [http://arxiv.org/abs/2304.03745](http://arxiv.org/abs/2304.03745)

    机器学习开发者在公平性中优先考虑准确性、代表性和透明度，同时面临数据偏见和缺乏指导方针等挑战。

    

    机器学习应用中的公平性对于研究和工业界的开发人员来说是一项重要的实践。在机器学习应用中，不公平是由于数据中的偏见、策划过程中的歧视、错误的假设和算法开发过程中呈现的隐性偏见引起的。随着机器学习应用的广泛应用，开发公平的机器学习应用程序变得至关重要。文献指出了多种关于用户视角和作为未来开发者的学生视角下，机器学习公平性是如何描述的看法。特别地，机器学习开发者的感知公平性还没有得到研究的关注。本文报告了对机器学习开发者感知公平性的初步调查。在描述公平性感知方面，本文使用了一种系统的焦点小组方法来评估该概念的属性。在焦点小组中，我们要求参与者讨论三个问题：1）机器学习公平性的特征是什么？2）开发公平机器学习应用中的挑战是什么？3）机器学习开发者如何缓解机器学习应用中的不公平性？初步结果表明，开发者将准确性、代表性和透明度放在机器学习公平性的优先考虑位置，并面临数据偏见和缺乏明确指导方针等挑战。本研究突显了思考机器学习开发者视角在解决机器学习公平性问题上的重要性。

    Fairness in machine learning (ML) applications is an important practice for developers in research and industry. In ML applications, unfairness is triggered due to bias in the data, curation process, erroneous assumptions, and implicit bias rendered within the algorithmic development process. As ML applications come into broader use developing fair ML applications is critical. Literature suggests multiple views on how fairness in ML is described from the users perspective and students as future developers. In particular, ML developers have not been the focus of research relating to perceived fairness. This paper reports on a pilot investigation of ML developers perception of fairness. In describing the perception of fairness, the paper performs an exploratory pilot study to assess the attributes of this construct using a systematic focus group of developers. In the focus group, we asked participants to discuss three questions- 1) What are the characteristics of fairness in ML? 2) What 
    
[^5]: 全梯度深度强化学习在平均奖励标准下的应用

    Full Gradient Deep Reinforcement Learning for Average-Reward Criterion. (arXiv:2304.03729v1 [eess.SY])

    [http://arxiv.org/abs/2304.03729](http://arxiv.org/abs/2304.03729)

    本论文将全梯度DQN算法从折扣奖励马尔可夫决策过程扩展至平均奖励问题，并在神经网络函数逼近环境中证明其在不同任务中具有更好的收敛速度。

    

    本论文将已被证明可收敛的全梯度DQN算法从折扣奖励马尔可夫决策过程扩展至平均奖励问题。我们在神经网络函数逼近环境中，比较了广泛使用的RVI Q-learning与最近提出的Differential Q-learning以及全梯度DQN和DQN。我们还将其扩展到学习马尔可夫不静态多臂老虎机的Whittle指数。我们发现所提出的全梯度变体在不同任务中具有更好的收敛速度。

    We extend the provably convergent Full Gradient DQN algorithm for discounted reward Markov decision processes from Avrachenkov et al. (2021) to average reward problems. We experimentally compare widely used RVI Q-Learning with recently proposed Differential Q-Learning in the neural function approximation setting with Full Gradient DQN and DQN. We also extend this to learn Whittle indices for Markovian restless multi-armed bandits. We observe a better convergence rate of the proposed Full Gradient variant across different tasks.
    
[^6]: 利用位置去噪预测易得几何结构的量子化学性质

    Predicting quantum chemical property with easy-to-obtain geometry via positional denoising. (arXiv:2304.03724v1 [physics.chem-ph])

    [http://arxiv.org/abs/2304.03724](http://arxiv.org/abs/2304.03724)

    该论文提出了一种方法，利用位置去噪预测易得几何结构的量子化学性质，可以用相对容易获得的几何结构，精确预测性质，在分子性质以及化学反应性质的预测任务中都表现优秀。

    

    由于量子化学性质与其几何结构有重要关联，使用3D几何信息的图神经网络在许多任务中取得了较高的预测精度。然而，它们通常需要高级量子力学计算得出的3D几何结构，这在实际问题中是不可行的，限制了其在现实问题中的适用性。为了解决这个问题，我们提出了一种方法，利用相对容易获得的几何结构（例如来自分子力场的优化几何结构）精确预测性质。在这种方法中，输入几何结构逐渐接近正确几何结构，通过堆叠去噪层。我们使用3D消息传递体系结构研究了该方法在两个预测任务（分子性质和化学反应性质）中的性能。通过去噪过程减少位置误差有助于性能的提高。

    As quantum chemical properties have a significant dependence on their geometries, graph neural networks (GNNs) using 3D geometric information have achieved high prediction accuracy in many tasks. However, they often require 3D geometries obtained from high-level quantum mechanical calculations, which are practically infeasible, limiting their applicability in real-world problems. To tackle this, we propose a method to accurately predict the properties with relatively easy-to-obtain geometries (e.g., optimized geometries from the molecular force field). In this method, the input geometry, regarded as the corrupted geometry of the correct one, gradually approaches the correct one as it passes through the stacked denoising layers. We investigated the performance of the proposed method using 3D message-passing architectures for two prediction tasks: molecular properties and chemical reaction property. The reduction of positional errors through the denoising process contributed to performan
    
[^7]: 超越隐私：合成数据的机遇与挑战探究

    Beyond Privacy: Navigating the Opportunities and Challenges of Synthetic Data. (arXiv:2304.03722v1 [cs.LG])

    [http://arxiv.org/abs/2304.03722](http://arxiv.org/abs/2304.03722)

    合成数据的潜力远非私有化数据，而是包括创建更公平的数据，数据增强，模拟和生成更多样化的数据集，然而，合成数据的应用仍然需要克服基本挑战，最重要的是如何评估其可靠性。

    

    通过生成模型产生合成数据在机器学习社区和其他领域引发了广泛关注。过去，合成数据往往被视为公开私有数据的一种方式，但最近一系列的论文探讨了合成数据的潜力远非如此——从创建更公平的数据到数据增强，从模拟到ChatGPT生成的文本。在这篇文章中，我们探讨了合成数据是否及如何成为机器学习领域的主导力量，承诺未来数据集可以根据个人需求进行定制。同样重要的是，我们讨论了社区需要克服的基本挑战，以扩大合成数据的相关性和应用——其中最重要的是量化我们能够信任从合成数据中提取的任何发现或预测的程度。

    Generating synthetic data through generative models is gaining interest in the ML community and beyond. In the past, synthetic data was often regarded as a means to private data release, but a surge of recent papers explore how its potential reaches much further than this -- from creating more fair data to data augmentation, and from simulation to text generated by ChatGPT. In this perspective we explore whether, and how, synthetic data may become a dominant force in the machine learning world, promising a future where datasets can be tailored to individual needs. Just as importantly, we discuss which fundamental challenges the community needs to overcome for wider relevance and application of synthetic data -- the most important of which is quantifying how much we can trust any finding or prediction drawn from synthetic data.
    
[^8]: 度量学习与偏好学习的表现定理：基于几何的视角

    Representer Theorems for Metric and Preference Learning: A Geometric Perspective. (arXiv:2304.03720v1 [cs.LG])

    [http://arxiv.org/abs/2304.03720](http://arxiv.org/abs/2304.03720)

    该论文提出了度量学习和偏好学习的新的表现定理，解决了度量学习任务以三元组比较为基础的表现定理问题。这种表现定理可以用内积诱导的范数来表示。

    

    我们探讨了希尔伯特空间中的度量学习和偏好学习问题，并获得了一种新的度量学习和偏好学习的表现定理。我们的关键观察是，表现定理可以根据问题结构内在的内积所诱导的范数来表示。此外，我们展示了如何将我们的框架应用于三元组比较的度量学习任务，并展示它导致了一个简单且自包含的该任务的表现定理。在再生核希尔伯特空间(RKHS)的情况下，我们展示了学习问题的解可以使用类似于经典表现定理的核术语表示。

    We explore the metric and preference learning problem in Hilbert spaces. We obtain a novel representer theorem for the simultaneous task of metric and preference learning. Our key observation is that the representer theorem can be formulated with respect to the norm induced by the inner product inherent in the problem structure. Additionally, we demonstrate how our framework can be applied to the task of metric learning from triplet comparisons and show that it leads to a simple and self-contained representer theorem for this task. In the case of Reproducing Kernel Hilbert Spaces (RKHS), we demonstrate that the solution to the learning problem can be expressed using kernel terms, akin to classical representer theorems.
    
[^9]: 结构健康监测领域中的边缘人工智能集成

    Integrating Edge-AI in Structural Health Monitoring domain. (arXiv:2304.03718v1 [cs.LG])

    [http://arxiv.org/abs/2304.03718](http://arxiv.org/abs/2304.03718)

    本文提出了在结构健康监测(SHM)领域中将边缘人工智能集成用于实时桥梁检测的框架，并使用商业边缘人工智能平台来开发和分析边缘人工智能设备的效果。

    

    结构健康监测(SHM)任务比如损伤检测对于关于维护和劣化的决策至关重要。例如，对于桥梁维护来说，裂纹检测是至关重要的，因为裂纹的进展会导致结构不稳定。然而，现有文献中大多数AI/ML模型在实时环境下具有低延迟和延迟推理时间等问题。本研究旨在探索将边缘人工智能集成到SHM领域中，以进行实时桥梁检测。根据边缘人工智能文献，它的能力将是SHM任务实时决策支持系统中有价值的集成方式，以便可以在物理现场进行实时推理。本研究将利用商业边缘人工智能平台，例如Google Coral Dev Board 或 Kneron KL520来开发和分析边缘人工智能设备的效果。因此，本研究提出一个适用于结构健康监测领域的边缘人工智能框架。

    Structural health monitoring (SHM) tasks like damage detection are crucial for decision-making regarding maintenance and deterioration. For example, crack detection in SHM is crucial for bridge maintenance as crack progression can lead to structural instability. However, most AI/ML models in the literature have low latency and late inference time issues while performing in real-time environments. This study aims to explore the integration of edge-AI in the SHM domain for real-time bridge inspections. Based on edge-AI literature, its capabilities will be valuable integration for a real-time decision support system in SHM tasks such that real-time inferences can be performed on physical sites. This study will utilize commercial edge-AI platforms, such as Google Coral Dev Board or Kneron KL520, to develop and analyze the effectiveness of edge-AI devices. Thus, this study proposes an edge AI framework for the structural health monitoring domain. An edge-AI-compatible deep learning model is
    
[^10]: 关于对比损失在多模态学习中的重要性

    On the Importance of Contrastive Loss in Multimodal Learning. (arXiv:2304.03717v1 [cs.LG])

    [http://arxiv.org/abs/2304.03717](http://arxiv.org/abs/2304.03717)

    对比损失在多模态学习中是非常重要的，使模型能够有效地平衡所学表示，正对推动模型对齐表示，而负对则保持学习到的表示平衡。

    

    最近，对比学习方法（例如 CLIP（Radford 等人，2021））在多模态学习中取得了巨大的成功，其中模型尝试最小化同一数据点的不同视图（例如图像和其标题）的表示之间的距离，同时使不同数据点的表示彼此分离。然而，从理论的角度来看，当数据不是各向同性时，对比学习如何有效地学习来自不同视图的表示仍不清楚。在这项工作中，我们分析了一个简单的多模态对比学习模型的训练动态，并表明对比对是模型能够有效平衡所学表示的重要因素。尤其是，我们表明正对是能够推动模型在增加条件数的代价下对齐表示，而负对则降低条件数，保持学习到的表示平衡。

    Recently, contrastive learning approaches (e.g., CLIP (Radford et al., 2021)) have received huge success in multimodal learning, where the model tries to minimize the distance between the representations of different views (e.g., image and its caption) of the same data point while keeping the representations of different data points away from each other. However, from a theoretical perspective, it is unclear how contrastive learning can learn the representations from different views efficiently, especially when the data is not isotropic. In this work, we analyze the training dynamics of a simple multimodal contrastive learning model and show that contrastive pairs are important for the model to efficiently balance the learned representations. In particular, we show that the positive pairs will drive the model to align the representations at the cost of increasing the condition number, while the negative pairs will reduce the condition number, keeping the learned representations balance
    
[^11]: 基于深度学习的Deepfake检测：卷积神经网络与变压器网络的比较研究

    Deepfake Detection with Deep Learning: Convolutional Neural Networks versus Transformers. (arXiv:2304.03698v1 [cs.CR])

    [http://arxiv.org/abs/2304.03698](http://arxiv.org/abs/2304.03698)

    本文研究了深度伪造检测中的卷积神经网络和变压器网络，开发出了八种有前途的深度学习架构用于检测深度伪造，并在多个数据集上进行了实验。在多个数据集上，我们的单模型检测器取得了较高的准确性和AUC性能。本研究还分析了模型的优劣和超参数对模型性能的影响。

    

    深度伪造技术的迅速发展正在严重威胁着媒体信息的可信度。本研究探讨了深度学习架构的演进，特别是卷积神经网络和变压器网络。我们确定了八种有前途的深度学习架构，设计和开发了我们的深度伪造检测模型，并在知名的深度伪造数据集上进行了实验。我们评估了我们的单模型检测器在深度伪造检测和跨数据集评估方面的有效性。在FF++ 2020、Google DFD、Celeb-DF、Deeper Forensics和DFDC深度伪造的检测中，我们分别达到了88.74％、99.53％、97.68％、99.73％和92.02％的准确率和99.95％、100％、99.88％、99.99％和97.61％的AUC。我们还确定并展示了卷积神经网络和变压器模型的独特优势，并分析了模型的超参数对模型性能的影响。

    The rapid evolvement of deepfake creation technologies is seriously threating media information trustworthiness. The consequences impacting targeted individuals and institutions can be dire. In this work, we study the evolutions of deep learning architectures, particularly CNNs and Transformers. We identified eight promising deep learning architectures, designed and developed our deepfake detection models and conducted experiments over well-established deepfake datasets. These datasets included the latest second and third generation deepfake datasets. We evaluated the effectiveness of our developed single model detectors in deepfake detection and cross datasets evaluations. We achieved 88.74%, 99.53%, 97.68%, 99.73% and 92.02% accuracy and 99.95%, 100%, 99.88%, 99.99% and 97.61% AUC, in the detection of FF++ 2020, Google DFD, Celeb-DF, Deeper Forensics and DFDC deepfakes, respectively. We also identified and showed the unique strengths of CNNs and Transformers models and analysed the o
    
[^12]: 人性化深度强化学习在交通信号控制中的应用：通过激励拼车来缓解交通拥堵和减少污染

    HumanLight: Incentivizing Ridesharing via Human-centric Deep Reinforcement Learning in Traffic Signal Control. (arXiv:2304.03697v1 [cs.LG])

    [http://arxiv.org/abs/2304.03697](http://arxiv.org/abs/2304.03697)

    本文介绍了一种名为HumanLight的算法，采用人性化的强化学习方法，在交通信号控制中激励大家拼车，缓解交通拥堵和减少污染。算法通过奖励乘坐大容量载客工具的通勤者，实现对绿灯时间的公平分配。

    

    单人驾车已成为许多通勤者最受青睐的交通方式，导致交通拥堵和空气污染问题加剧。信息技术的进步为实现城市“轻车化”提供了机遇，可以通过智能解决方案激励拼车和切换到大容量载客工具。本研究提出HumanLight，一种新型的分散式自适应交通信号控制算法，旨在优化交叉口的人员通行量。所提出的控制器基于强化学习，奖励函数嵌入了以人为本的交通概念。通过激励大容量载客工具的通勤者合并乘车以节约时间，HumanLight实现了对绿灯时间的公平分配。除了采用FRAP作为先进的基础模型外，HumanLight还引入了“活跃车辆”这个概念，大致定义是临近交叉口且可能干扰决策过程的任何车辆。

    Single occupancy vehicles are the most attractive transportation alternative for many commuters, leading to increased traffic congestion and air pollution. Advancements in information technologies create opportunities for smart solutions that incentivize ridesharing and mode shift to higher occupancy vehicles (HOVs) to achieve the car lighter vision of cities. In this study, we present HumanLight, a novel decentralized adaptive traffic signal control algorithm designed to optimize people throughput at intersections. Our proposed controller is founded on reinforcement learning with the reward function embedding the transportation-inspired concept of pressure at the person-level. By rewarding HOV commuters with travel time savings for their efforts to merge into a single ride, HumanLight achieves equitable allocation of green times. Apart from adopting FRAP, a state-of-the-art (SOTA) base model, HumanLight introduces the concept of active vehicles, loosely defined as vehicles in proximit
    
[^13]: 基于等变贝叶斯神经网络的高精度不确定性感知分子间力建模

    High Accuracy Uncertainty-Aware Interatomic Force Modeling with Equivariant Bayesian Neural Networks. (arXiv:2304.03694v1 [physics.chem-ph])

    [http://arxiv.org/abs/2304.03694](http://arxiv.org/abs/2304.03694)

    本文引入了一种新的Monte Carlo Markov Chain抽样算法以解决常用算法无法收敛的问题，同时结合了新型随机神经网络模型，可以实现高精度不确定性感知的分子间力建模。

    

    尽管贝叶斯神经网络为建模不确定性、主动学习和结合先前的物理知识提供了一个有前途的框架，但很少有应用于分子间力建模的情况得到发现。其中主要的挑战之一是缺乏适合的MCMC抽样算法来取得后验密度，因为常用算法对许多最先进的架构在实际时间内无法收敛。作为应对这一挑战，我们在本文中引入了一种新的MCMC抽样算法，该算法可以绕过现有抽样方法的问题。此外，我们还介绍了一种基于NequIP体系结构的新型随机神经网络模型，并展示了当其与我们的新型抽样算法结合时，我们可以获得具有最先进的精度和好的不确定度度量的预测。

    Even though Bayesian neural networks offer a promising framework for modeling uncertainty, active learning and incorporating prior physical knowledge, few applications of them can be found in the context of interatomic force modeling. One of the main challenges in their application to learning interatomic forces is the lack of suitable Monte Carlo Markov chain sampling algorithms for the posterior density, as the commonly used algorithms do not converge in a practical amount of time for many of the state-of-the-art architectures. As a response to this challenge, we introduce a new Monte Carlo Markov chain sampling algorithm in this paper which can circumvent the problems of the existing sampling methods. In addition, we introduce a new stochastic neural network model based on the NequIP architecture and demonstrate that, when combined with our novel sampling algorithm, we obtain predictions with state-of-the-art accuracy as well as a good measure of uncertainty.
    
[^14]: 基于深度学习和其他机器学习算法的加密恶意流量检测中的特征挖掘

    Feature Mining for Encrypted Malicious Traffic Detection with Deep Learning and Other Machine Learning Algorithms. (arXiv:2304.03691v1 [cs.CR])

    [http://arxiv.org/abs/2304.03691](http://arxiv.org/abs/2304.03691)

    本文分析了恶意流量检测中的特征提取问题，并提出了一种专门针对加密恶意流量的特征和概念，同时提出了一个有深度学习和传统机器学习算法的检测框架，并在实验中取得了优异的检测表现。

    

    加密机制的普及对恶意流量检测提出了极大的挑战，传统检测技术在没有对加密流量解密的情况下无法工作。当前，对于不解密的加密恶意流量检测的研究集中在特征提取和机器学习或深度学习算法的选择上。本文首先提供流量特征的深入分析和比较不同的流量特征创建方法，同时提出了一种专门针对加密恶意流量分析设计的新概念和特征。此外，我们提出了一种用于加密恶意流量检测的框架，该框架是一个两层的检测框架，包括深度学习和传统机器学习算法。通过比较实验，该框架在检测准确度和假阳性率方面优于经典的深度学习和传统机器学习算法，如ResNet和随机森林。

    The popularity of encryption mechanisms poses a great challenge to malicious traffic detection. The reason is traditional detection techniques cannot work without the decryption of encrypted traffic. Currently, research on encrypted malicious traffic detection without decryption has focused on feature extraction and the choice of machine learning or deep learning algorithms. In this paper, we first provide an in-depth analysis of traffic features and compare different state-of-the-art traffic feature creation approaches, while proposing a novel concept for encrypted traffic feature which is specifically designed for encrypted malicious traffic analysis. In addition, we propose a framework for encrypted malicious traffic detection. The framework is a two-layer detection framework which consists of both deep learning and traditional machine learning algorithms. Through comparative experiments, it outperforms classical deep learning and traditional machine learning algorithms, such as Res
    
[^15]: 带需求的机器学习：一份宣言

    Machine Learning with Requirements: a Manifesto. (arXiv:2304.03674v1 [cs.LG])

    [http://arxiv.org/abs/2304.03674](http://arxiv.org/abs/2304.03674)

    本文提出一个带需求的机器学习宣言，认为需求定义和满足可以在很大程度上使机器学习模型更适用于现实世界中的关键领域，作者提出了两个问题，其中（i）需求自然而然地出现，（ii）机器学习模型被或可以成功地部署，并且（iii）忽略需求可能会产生严重后果，提出了一种新型的金字塔式开发流程，在其中，需求定义可能会影响到流程中所有后续阶段，反之亦然。

    

    在近年来，机器学习取得了长足的进步，成为许多不同应用领域突破的根源。然而，如何将它们应用到高风险或安全关键的应用领域仍然是一个未解决的问题，因为它们往往容易变得脆弱和不可靠。本文认为，需求定义和满足可以在很大程度上使机器学习模型更适用于现实世界中的关键领域。为此，我们提出了两个问题，其中（i）需求自然而然地出现，（ii）机器学习模型被或可以成功地部署，并且（iii）忽略需求可能会产生严重后果。我们展示了如何将需求规格说明有益地整合到标准的机器学习开发流程中，提出了一种新型的金字塔式开发流程，在其中，需求定义可能会影响到流程中所有后续阶段，反之亦然。

    In the recent years, machine learning has made great advancements that have been at the root of many breakthroughs in different application domains. However, it is still an open issue how make them applicable to high-stakes or safety-critical application domains, as they can often be brittle and unreliable. In this paper, we argue that requirements definition and satisfaction can go a long way to make machine learning models even more fitting to the real world, especially in critical domains. To this end, we present two problems in which (i) requirements arise naturally, (ii) machine learning models are or can be fruitfully deployed, and (iii) neglecting the requirements can have dramatic consequences. We show how the requirements specification can be fruitfully integrated into the standard machine learning development pipeline, proposing a novel pyramid development process in which requirements definition may impact all the subsequent phases in the pipeline, and viceversa.
    
[^16]: 不仅仅靠运气：提高质量多样性解决方案在不可预测领域中的行为重复性

    Don't Bet on Luck Alone: Enhancing Behavioral Reproducibility of Quality-Diversity Solutions in Uncertain Domains. (arXiv:2304.03672v1 [cs.NE])

    [http://arxiv.org/abs/2304.03672](http://arxiv.org/abs/2304.03672)

    本文提出了一种称为ARIA的模块，可在任何QD算法中提高存档中存在的解决方案的可重现性。该方法通过优化解决方案的概率和适应度进行变异，从而应对不可预测的噪音环境。

    

    质量多样性(QD)算法旨在在给定描述符空间中生成优秀解决方案的集合并最大化它们的多样性。然而，在存在不可预测的噪音的情况下，同一解决方案在不同评估中的适应度和描述符可能会有显著差异，导致估计这些值存在不确定性。鉴于QD算法的精英主义本质，在这些嘈杂的环境下，它们通常会得到许多退化的解决方案。在本文中，我们介绍了“档案可重现性改进算法”(ARIA)；一种即插即用的方法，它可以提高存档中存在的解决方案的可重现性。我们将其提议为一种单独的优化模块，依赖于自然进化策略，可以在任何QD算法的顶部执行。我们的模块对解决方案进行变异，以(1)优化其属于自己的领域的概率，和(2)最大化它们的适应度。我们的方法的性能在各种任务上进行了评估，包括...

    Quality-Diversity (QD) algorithms are designed to generate collections of high-performing solutions while maximizing their diversity in a given descriptor space. However, in the presence of unpredictable noise, the fitness and descriptor of the same solution can differ significantly from one evaluation to another, leading to uncertainty in the estimation of such values. Given the elitist nature of QD algorithms, they commonly end up with many degenerate solutions in such noisy settings. In this work, we introduce Archive Reproducibility Improvement Algorithm (ARIA); a plug-and-play approach that improves the reproducibility of the solutions present in an archive. We propose it as a separate optimization module, relying on natural evolution strategies, that can be executed on top of any QD algorithm. Our module mutates solutions to (1) optimize their probability of belonging to their niche, and (2) maximize their fitness. The performance of our method is evaluated on various tasks, incl
    
[^17]: 基于收缩引导的自适应分区法用于神经网络控制系统可达集分析

    Contraction-Guided Adaptive Partitioning for Reachability Analysis of Neural Network Controlled Systems. (arXiv:2304.03671v1 [eess.SY])

    [http://arxiv.org/abs/2304.03671](http://arxiv.org/abs/2304.03671)

    本文提出了一种基于收缩引导的自适应分区算法，用于改善带有神经网络控制器和干扰的非线性反馈回路中区间值鲁棒可达集估计，该算法通过将神经网络验证步骤和可达性分区层的解耦，可以在很小的计算成本下提供精度提升。

    

    本文提出了一种基于收缩引导的自适应分区算法，用于改进带神经网络控制器和扰动的非线性反馈回路中区间值鲁棒可达集估计。算法根据超逼近区间的收缩速率估计来选择何时何地进行分区。通过将神经网络验证步骤和可达性分区层的解耦，该算法可以在很小的计算成本下提供精度提升。该方法适用于任何具有足够精度的开环区间值可达性估计技术和任何用于界定神经网络输入输出行为的方法。使用基于收缩的鲁棒性分析，我们为混合单调可达性算法的性能提供了保证。最后，我们通过几个数值模拟来展示算法的性能，并将其与现有方法进行比较。

    In this paper, we present a contraction-guided adaptive partitioning algorithm for improving interval-valued robust reachable set estimates in a nonlinear feedback loop with a neural network controller and disturbances. Based on an estimate of the contraction rate of over-approximated intervals, the algorithm chooses when and where to partition. Then, by leveraging a decoupling of the neural network verification step and reachability partitioning layers, the algorithm can provide accuracy improvements for little computational cost. This approach is applicable with any sufficiently accurate open-loop interval-valued reachability estimation technique and any method for bounding the input-output behavior of a neural network. Using contraction-based robustness analysis, we provide guarantees of the algorithm's performance with mixed monotone reachability. Finally, we demonstrate the algorithm's performance through several numerical simulations and compare it with existing methods in the li
    
[^18]: 通过不确定性实现公平性

    Fairness through Aleatoric Uncertainty. (arXiv:2304.03646v1 [cs.LG])

    [http://arxiv.org/abs/2304.03646](http://arxiv.org/abs/2304.03646)

    研究不确定性与公平性的关系，通过贝叶斯学习估算样本预测不确定性，发现低不确定性的数据更准确和公平，提出一种基于不确定性量化定义的新的公平性-效用目标。

    

    我们提出了一种独特的解决方案，以解决机器学习分类任务中公平性和效用通常相互竞争的目标。 公平性确保模型的预测不带偏见地针对任何特定群体，而效用则专注于最大化模型预测的准确性。我们的目标是研究不确定性与公平性之间的关系。我们的方法利用贝叶斯学习来估算样本预测的不确定性，其中估算与受保护属性相关的混淆效应无关。通过实证证据，我们表明具有低分类不确定性的样本比具有高不确定性的样本更准确和公平地建模，可能具有偏差的表示和更高的预测误差。为了解决平衡公平性和效用的挑战，我们提出了一种基于不确定性量化定义的新的公平性-效用目标。

    We propose a unique solution to tackle the often-competing goals of fairness and utility in machine learning classification tasks. While fairness ensures that the model's predictions are unbiased and do not discriminate against any particular group, utility focuses on maximizing the accuracy of the model's predictions. Our aim is to investigate the relationship between uncertainty and fairness. Our approach leverages this concept by employing Bayesian learning to estimate the uncertainty in sample predictions where the estimation is independent of confounding effects related to the protected attribute. Through empirical evidence, we show that samples with low classification uncertainty are modeled more accurately and fairly than those with high uncertainty, which may have biased representations and higher prediction errors. To address the challenge of balancing fairness and utility, we propose a novel fairness-utility objective that is defined based on uncertainty quantification. The w
    
[^19]: 一种用于正交约束下的非光滑组合优化的块坐标下降方法

    A Block Coordinate Descent Method for Nonsmooth Composite Optimization under Orthogonality Constraints. (arXiv:2304.03641v1 [math.OC])

    [http://arxiv.org/abs/2304.03641](http://arxiv.org/abs/2304.03641)

    本文提出了一种新的块坐标下降方法OBCD，用于解决具有正交约束的一般非光滑组合问题。 OBCD是一种可行的方法，具有低的计算复杂性，并且获得严格的收敛保证。

    

    具有正交约束的非光滑组合优化在统计学习和数据科学中有广泛的应用。由于其非凸性和非光滑性质，该问题通常很难求解。现有的解决方案受到以下一个或多个限制的限制：（i）它们是需要每次迭代高计算成本的全梯度方法；（ii）它们无法解决一般的非光滑组合问题；（iii）它们是不可行方法，并且只能在极限点处实现解的可行性；（iv）它们缺乏严格的收敛保证；（v）它们只能获得关键点的弱最优性。在本文中，我们提出了一种新的块坐标下降方法OBCD，用于解决正交约束下的一般非光滑组合问题。OBCD是一种可行的方法，具有低的计算复杂性。在每次迭代中，我们的算法会更新...

    Nonsmooth composite optimization with orthogonality constraints has a broad spectrum of applications in statistical learning and data science. However, this problem is generally challenging to solve due to its non-convex and non-smooth nature. Existing solutions are limited by one or more of the following restrictions: (i) they are full gradient methods that require high computational costs in each iteration; (ii) they are not capable of solving general nonsmooth composite problems; (iii) they are infeasible methods and can only achieve the feasibility of the solution at the limit point; (iv) they lack rigorous convergence guarantees; (v) they only obtain weak optimality of critical points. In this paper, we propose \textit{\textbf{OBCD}}, a new Block Coordinate Descent method for solving general nonsmooth composite problems under Orthogonality constraints. \textit{\textbf{OBCD}} is a feasible method with low computation complexity footprints. In each iteration, our algorithm updates $
    
[^20]: FedDiSC: 一种面向电力系统干扰和网络攻击区分的计算高效的联邦学习框架

    FedDiSC: A Computation-efficient Federated Learning Framework for Power Systems Disturbance and Cyber Attack Discrimination. (arXiv:2304.03640v1 [cs.CR])

    [http://arxiv.org/abs/2304.03640](http://arxiv.org/abs/2304.03640)

    本文提出了一种名为FedDiSC的联邦学习框架，能够在保护隐私和提高计算效率的条件下，同时检测电力系统干扰和网络攻击。

    

    鉴于智能电网系统安全和隐私日益受到关注，针对关键电网组件（例如状态估计）的网络攻击已经被证明是网络安全问题的重点，近年来得到了重视。然而，智能电网中的网络攻击检测面临着新的挑战，包括隐私保护和具有战略数据所有者的分散电力区域。为了解决这些技术瓶颈，本文提出了一种新颖的基于联邦学习的隐私保护和通信高效的攻击检测框架，称为 FedDiSC，它能够区分电力系统干扰和网络攻击。具体而言，我们首先提出了联邦学习方法，使分散式电网区域的监视和数据采集子系统能够协作训练一种攻击检测模型，而无需共享敏感的电力相关数据。其次，我们提出了一种表示学习方法，从训练数据中提取判别特征，能够提高检测性能。第三，我们设计了一种基于卷积神经网络的轻量级攻击检测模型，以使每个本地系统能够进行高效的推论。最后，我们在实际智能电网数据集上评估了所提出的FedDiSC框架，并实验结果证明了其在隐私保护、计算效率和网络攻击检测准确性方面的优越性。

    With the growing concern about the security and privacy of smart grid systems, cyberattacks on critical power grid components, such as state estimation, have proven to be one of the top-priority cyber-related issues and have received significant attention in recent years. However, cyberattack detection in smart grids now faces new challenges, including privacy preservation and decentralized power zones with strategic data owners. To address these technical bottlenecks, this paper proposes a novel Federated Learning-based privacy-preserving and communication-efficient attack detection framework, known as FedDiSC, that enables Discrimination between power System disturbances and Cyberattacks. Specifically, we first propose a Federated Learning approach to enable Supervisory Control and Data Acquisition subsystems of decentralized power grid zones to collaboratively train an attack detection model without sharing sensitive power related data. Secondly, we put forward a representation lear
    
[^21]: 线性递归网络在长序列计数问题上的理论与实验困境

    Theoretical Conditions and Empirical Failure of Bracket Counting on Long Sequences with Linear Recurrent Networks. (arXiv:2304.03639v1 [cs.LG])

    [http://arxiv.org/abs/2304.03639](http://arxiv.org/abs/2304.03639)

    本文研究了最简单的线性单元RNN在长序列计数问题上的极限，理论上使用的条件具有充分必要性；实验数据表明，通过标准的方法，该网络通常无法实现准确的计数行为。

    

    先前的研究已经证明具有无限激活函数的RNN有计数的能力。然而，RNN的有效训练往往困难，通常无法学习准确的计数行为。本文通过研究最简单的线性单元RNN来解决这个问题。我们对线性RNN进行了理论分析，并确定了模型展现精确计数行为的条件。我们证明这些条件是必要且充分的。我们还使用涉及类似Dyck-1平衡符号的任务在两个不同的设置下进行了实证分析。我们发现线性RNN通常无法在标准方法训练下满足计数行为的必要且充分条件。我们研究了不同的训练序列长度和利用不同的目标类别对模型行为在训练期间和计数能力的影响。

    Previous work has established that RNNs with an unbounded activation function have the capacity to count exactly. However, it has also been shown that RNNs are challenging to train effectively and generally do not learn exact counting behaviour. In this paper, we focus on this problem by studying the simplest possible RNN, a linear single-cell network. We conduct a theoretical analysis of linear RNNs and identify conditions for the models to exhibit exact counting behaviour. We provide a formal proof that these conditions are necessary and sufficient. We also conduct an empirical analysis using tasks involving a Dyck-1-like Balanced Bracket language under two different settings. We observe that linear RNNs generally do not meet the necessary and sufficient conditions for counting behaviour when trained with the standard approach. We investigate how varying the length of training sequences and utilising different target classes impacts model behaviour during training and the ability of 
    
[^22]: 压缩回归与自适应网络

    Compressed Regression over Adaptive Networks. (arXiv:2304.03638v1 [cs.LG])

    [http://arxiv.org/abs/2304.03638](http://arxiv.org/abs/2304.03638)

    本文阐述了一个分布式智能体的网络如何合作解决回归问题，在通信约束、自适应和合作的情况下能够达到的性能，并探讨了分布式回归问题的基本属性与最优分配通信资源之间的定量关系。

    

    本文中，我们推导了一个分布式智能体网络在解决回归问题时，在通信约束、自适应和合作的情况下能够达到的性能。智能体使用了最近提出的 ACTC (adapt-compress-then-combine) 扩散策略，在这个策略中，邻近智能体交换的信号被随机不同压缩算子编码。我们详细阐述了均方估计误差的特征，其中包括了一项与没有通信约束的情况下智能体将要达到的误差有关的错误项，以及一项由于压缩而产生的误差项。分析揭示了分布式回归问题的基本属性，尤其是通过Perron特征向量引起的梯度噪声和网络拓扑结构（）。我们展示了知晓这些关系对于最优地分配智能体之间的通信资源是至关重要的。

    In this work we derive the performance achievable by a network of distributed agents that solve, adaptively and in the presence of communication constraints, a regression problem. Agents employ the recently proposed ACTC (adapt-compress-then-combine) diffusion strategy, where the signals exchanged locally by neighboring agents are encoded with randomized differential compression operators. We provide a detailed characterization of the mean-square estimation error, which is shown to comprise a term related to the error that agents would achieve without communication constraints, plus a term arising from compression. The analysis reveals quantitative relationships between the compression loss and fundamental attributes of the distributed regression problem, in particular, the stochastic approximation error caused by the gradient noise and the network topology (through the Perron eigenvector). We show that knowledge of such relationships is critical to allocate optimally the communication
    
[^23]: 异步联邦连续学习

    Asynchronous Federated Continual Learning. (arXiv:2304.03626v1 [cs.LG])

    [http://arxiv.org/abs/2304.03626](http://arxiv.org/abs/2304.03626)

    该论文介绍了一种新的联邦学习环境——异步联邦连续学习(AFCL)，它使用基于原型的学习、表示损失、分形预训练以及修改的聚合策略，名为FedSpace。通过在CIFAR-100数据集上的实验，该方法在三种联邦划分下，分别使用50、100和500个客户端，得到了令人满意的结果。

    

    在标准的类别增量连续学习设置中，假定看到一组任务，这些任务按照固定的、预定义的顺序一个接一个地出现。在联邦学习环境中，这不太现实，因为每个客户端都是独立地以异步方式工作，根据完全不相关的时间段和顺序获取不同任务的数据。我们引入了一种新的联邦学习设置（AFCL），其中多个任务的连续学习在每个客户端上，使用不同的顺序和异步时间段。我们使用基于原型的学习、表示损失、分形预训练和修改的聚合策略来解决这个新任务。我们的方法称为FedSpace，在CIFAR-100数据集上的结果以3种不同的联邦划分为50、100和500个客户端作为实验，表明其有效性。

    The standard class-incremental continual learning setting assumes a set of tasks seen one after the other in a fixed and predefined order. This is not very realistic in federated learning environments where each client works independently in an asynchronous manner getting data for the different tasks in time-frames and orders totally uncorrelated with the other ones. We introduce a novel federated learning setting (AFCL) where the continual learning of multiple tasks happens at each client with different orderings and in asynchronous time slots. We tackle this novel task using prototype-based learning, a representation loss, fractal pre-training, and a modified aggregation policy. Our approach, called FedSpace, effectively tackles this task as shown by the results on the CIFAR-100 dataset using 3 different federated splits with 50, 100, and 500 clients, respectively. The code and federated splits are available at https://github.com/LTTM/FedSpace.
    
[^24]: 重新审视自动提示：我们真的做得更好吗？

    Revisiting Automated Prompting: Are We Actually Doing Better?. (arXiv:2304.03609v1 [cs.CL])

    [http://arxiv.org/abs/2304.03609](http://arxiv.org/abs/2304.03609)

    本文重审自动提示技术在六个不同的任务和更广泛范围的K-shot学习设置上的表现，发现自动提示并不能始终优于手动提示，因此手动提示应该作为自动提示的一个基准线。

    

    当前的文献表明，大型语言模型(LLM)是出色的几乎不用学习的学习者，在几乎不用学习的情况下，提示显着提高了它们在多个下游任务中的表现。随后进行了试图自动化人类提示的尝试，并取得了一定进展。特别是，随后的工作表明，在某些K-shot学习场景中，自动化可以优于微调。在本文中，我们重新审视了自动提示在六个不同的下游任务和更大范围的K-shot学习设置上的技术。我们发现，自动提示不能始终优于简单的手动提示。我们的工作表明，在这一研究领域中，除了微调之外，手动提示应作为基线使用。

    Current literature demonstrates that Large Language Models (LLMs) are great few-shot learners, and prompting significantly increases their performance on a range of downstream tasks in a few-shot learning setting. An attempt to automate human-led prompting followed, with some progress achieved. In particular, subsequent work demonstrates automation can outperform fine-tuning in certain K-shot learning scenarios.  In this paper, we revisit techniques for automated prompting on six different downstream tasks and a larger range of K-shot learning settings. We find that automated prompting does not consistently outperform simple manual prompts. Our work suggests that, in addition to fine-tuning, manual prompts should be used as a baseline in this line of research.
    
[^25]: 基于深度强化学习的无图Crowd Navigation与感知风险控制的移动机器人

    Deep Reinforcement Learning-Based Mapless Crowd Navigation with Perceived Risk of the Moving Crowd for Mobile Robots. (arXiv:2304.03593v1 [cs.RO])

    [http://arxiv.org/abs/2304.03593](http://arxiv.org/abs/2304.03593)

    本论文提出了一种基于碰撞概率的无图Crowd Navigation方法，使用深度强化学习(DRL)来感知人群的危险程度，确保机器人在通过拥挤环境时的安全，同时提高模型的可扩展性。

    

    传统的基于地图的机器人导航方法在拥挤环境中往往会遇到“冻结机器人问题”。深度强化学习方法解决了此问题，但是存在泛化和可扩展性的问题。为了克服这些挑战，我们提出一种使用“碰撞概率”来帮助机器人安全通过人群的方法。将“碰撞概率”包括在观察空间中，给机器人提供了一个感知移动人群的危险程度的能力。机器人会在看似安全的情况下穿过人群，但在人群移动过于激烈时会绕路。通过关注最危险的障碍物，机器人不会在人群密度较高时混淆，确保模型的可扩展性。我们的方法使用深度强化学习(DRL)开发，并在Gazebo模拟器中进行了非合作人群环境中的训练，其中的障碍物以随机速度移动。

    Classical map-based navigation methods are commonly used for robot navigation, but they often struggle in crowded environments due to the Frozen Robot Problem (FRP). Deep reinforcement learning-based methods address the FRP problem, however, suffer from the issues of generalization and scalability. To overcome these challenges, we propose a method that uses Collision Probability (CP) to help the robot navigate safely through crowds. The inclusion of CP in the observation space gives the robot a sense of the level of danger of the moving crowd. The robot will navigate through the crowd when it appears safe but will take a detour when the crowd is moving aggressively. By focusing on the most dangerous obstacle, the robot will not be confused when the crowd density is high, ensuring scalability of the model. Our approach was developed using deep reinforcement learning (DRL) and trained using the Gazebo simulator in a non cooperative crowd environment with obstacles moving at randomized sp
    
[^26]: 标签可观测、节点不可观测下的二部图图估计问题研究

    Graphon Estimation in bipartite graphs with observable edge labels and unobservable node labels. (arXiv:2304.03590v1 [math.ST])

    [http://arxiv.org/abs/2304.03590](http://arxiv.org/abs/2304.03590)

    研究了标签可观测、节点不可观测下的二部图图估计问题，在分段常数和H\"older连续图谱的情况下找到了有限的样本风险界限。

    

    许多数据集都可以表示为一个矩阵，其条目对应于不同类型实体之间的交互（网页用户访问网页的次数、学生某科目的成绩、患者对医生的评价等）。本文假设上述交互是由描述每个实体的不可观测潜在变量确定的。我们的目标是估计给定不可观测变量的数据矩阵的条件期望。这被表示为估计称为图谱的双变量函数的问题。我们研究了分段常数和H\"older连续图谱的情况。我们为最小二乘估计和指数加权聚合建立了有限样本风险界限。这些界限强调了估计误差与数据集大小、交互强度最大值和噪声水平的依赖关系。由于分析的最小二乘估计量难以处理，

    Many real-world data sets can be presented in the form of a matrix whose entries correspond to the interaction between two entities of different natures (number of times a web user visits a web page, a student's grade in a subject, a patient's rating of a doctor, etc.). We assume in this paper that the mentioned interaction is determined by unobservable latent variables describing each entity. Our objective is to estimate the conditional expectation of the data matrix given the unobservable variables. This is presented as a problem of estimation of a bivariate function referred to as graphon. We study the cases of piecewise constant and H\"older-continuous graphons. We establish finite sample risk bounds for the least squares estimator and the exponentially weighted aggregate. These bounds highlight the dependence of the estimation error on the size of the data set, the maximum intensity of the interactions, and the level of noise. As the analyzed least-squares estimator is intractable
    
[^27]: 大规模深度学习模型高效训练方法：文献综述

    On Efficient Training of Large-Scale Deep Learning Models: A Literature Review. (arXiv:2304.03589v1 [cs.LG])

    [http://arxiv.org/abs/2304.03589](http://arxiv.org/abs/2304.03589)

    研究深度学习模型高效训练方法已有不少成果，但尚缺乏全面总结。本文综述分为数据中心化、模型中心化、超参数优化、深度学习硬件和训练策略等五个方面，比较详尽地回顾了加速深度学习模型训练的基本组件。

    

    深度学习领域在计算机视觉、自然语言处理和语音等方面取得了显著进展。采用大规模模型并在海量数据上进行训练，对于实际应用具有巨大的潜力，可以增强工业生产力，促进社会发展。然而，随着对计算能力要求的增加，尽管有许多研究探讨了高效训练方法，对于训练深度学习模型的加速技术的全面总结仍然备受期待。在本综述中，我们详细回顾了训练加速的研究进展，并将基本组件分为五个主要视角：（1）数据中心化：包括数据集正则化、数据采样和数据中心化课程学习技术，这些方法可以显著减少数据样本的计算复杂度；（2）模型中心化：包括基本模块的加速、模型压缩和模型蒸馏技术；(剩下同上原文)

    The field of deep learning has witnessed significant progress, particularly in computer vision (CV), natural language processing (NLP), and speech. The use of large-scale models trained on vast amounts of data holds immense promise for practical applications, enhancing industrial productivity and facilitating social development. With the increasing demands on computational capacity, though numerous studies have explored the efficient training, a comprehensive summarization on acceleration techniques of training deep learning models is still much anticipated. In this survey, we present a detailed review for training acceleration. We consider the fundamental update formulation and split its basic components into five main perspectives: (1) data-centric: including dataset regularization, data sampling, and data-centric curriculum learning techniques, which can significantly reduce the computational complexity of the data samples; (2) model-centric, including acceleration of basic modules,
    
[^28]: 使用基于机器ID的对比学习预训练的音频表示进行异常声音检测

    Anomalous Sound Detection using Audio Representation with Machine ID based Contrastive Learning Pretraining. (arXiv:2304.03588v1 [cs.SD])

    [http://arxiv.org/abs/2304.03588](http://arxiv.org/abs/2304.03588)

    本文提出了一种使用基于机器ID的对比学习预训练的音频表示来进行异常声音检测的方法，在DCASE 2020 Challenge Task2 数据集上表现优于目前最先进的基于对比学习或自监督分类的方法。

    

    现有的用于异常声音检测的对比学习方法通过利用样本增强的对比来精炼每个音频样本的音频表示。然而，由于缺乏机器声音的物理特性，它们可能会受到增强数据的偏差，从而限制检测性能。本文使用对比学习来精炼每个机器ID的音频表示，而不是每个音频样本。提出的两阶段方法使用对比学习来预训练音频表示模型，通过融入机器ID和自监督ID分类器来对学习的模型进行微调，同时增强来自相同ID的音频特征之间的关系。实验证明，我们的方法在DCASE 2020挑战任务2数据集上的整体异常检测性能和稳定性方面优于基于对比学习或自监督分类的最先进方法。

    Existing contrastive learning methods for anomalous sound detection refine the audio representation of each audio sample by using the contrast between the samples' augmentations (e.g., with time or frequency masking). However, they might be biased by the augmented data, due to the lack of physical properties of machine sound, thereby limiting the detection performance. This paper uses contrastive learning to refine audio representations for each machine ID, rather than for each audio sample. The proposed two-stage method uses contrastive learning to pretrain the audio representation model by incorporating machine ID and a self-supervised ID classifier to fine-tune the learnt model, while enhancing the relation between audio features from the same ID. Experiments show that our method outperforms the state-of-the-art methods using contrastive learning or self-supervised classification in overall anomaly detection performance and stability on DCASE 2020 Challenge Task2 dataset.
    
[^29]: 通过基于对比度变分模型的点注释实现组织病理学图像弱监督分割

    Weakly supervised segmentation with point annotations for histopathology images via contrast-based variational model. (arXiv:2304.03572v1 [eess.IV])

    [http://arxiv.org/abs/2304.03572](http://arxiv.org/abs/2304.03572)

    本文提出了一种基于对比度变分模型的方法来实现组织病理学图像的弱监督分割，为训练深度学习分割模型提供可靠补充监督。

    

    图像分割是成像和视觉领域的一项基本任务。监督深度学习分割在有足够带注释标签的训练数据时取得了无与伦比的成功。然而，注释众所周知难以获取，特别是对于组织病理学图像，其中目标区域通常具有高形态变异和不规则形状。因此，使用少量点的弱监督学习有望减轻注释工作量。在本文中，我们提出了一种基于对比度变分模型的方法来生成分割结果，这些结果可用作训练组织病理图像深度分割模型的可靠补充监督。所提出的方法考虑到了组织病理学图像中目标区域的共同特征，并可以进行端到端的训练。它可以生成更具区域一致性和更平滑的边界分割，对未标记的“新奇”区域更具鲁棒性。

    Image segmentation is a fundamental task in the field of imaging and vision. Supervised deep learning for segmentation has achieved unparalleled success when sufficient training data with annotated labels are available. However, annotation is known to be expensive to obtain, especially for histopathology images where the target regions are usually with high morphology variations and irregular shapes. Thus, weakly supervised learning with sparse annotations of points is promising to reduce the annotation workload. In this work, we propose a contrast-based variational model to generate segmentation results, which serve as reliable complementary supervision to train a deep segmentation model for histopathology images. The proposed method considers the common characteristics of target regions in histopathology images and can be trained in an end-to-end manner. It can generate more regionally consistent and smoother boundary segmentation, and is more robust to unlabeled `novel' regions. Exp
    
[^30]: $\beta$-变分自编码器和Transformer用于流体流动的降阶建模

    $\beta$-Variational autoencoders and transformers for reduced-order modelling of fluid flows. (arXiv:2304.03571v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2304.03571](http://arxiv.org/abs/2304.03571)

    本文提出了一种使用$\beta$-VAE和Transformer相结合的方法来学习紧凑且近似正交的ROMs，该方法可用于混沌流体流动的降阶建模，并在准确性方面优于其他预测模型。

    

    变分自编码器（VAE）架构有潜力开发混沌流体流动的降阶模型（ROMs）。我们提出一种使用$\beta$-VAE和Transformer相结合的方法来学习紧凑且近似正交的ROMs，同时在二维粘性流体流动的周期和混沌状态下进行数值测试。$\beta$-VAE被训练为学习流速的紧凑潜在表示形式，而Transformer则被训练为在潜在空间中预测时间动态。通过使用$\beta$-VAE来学习分离表示形式，我们获得了更可解释的流动模型，其特征类似于观察到的适当正交分解，但表示更高效。使用Poincaré图，结果表明我们的方法可以捕捉流体流动的基本动力学，优于其他预测模型。该方法在天气预报、地层流和气候模型等其他领域具有潜在应用。

    Variational autoencoder (VAE) architectures have the potential to develop reduced-order models (ROMs) for chaotic fluid flows. We propose a method for learning compact and near-orthogonal ROMs using a combination of a $\beta$-VAE and a transformer, tested on numerical data from a two-dimensional viscous flow in both periodic and chaotic regimes. The $\beta$-VAE is trained to learn a compact latent representation of the flow velocity, and the transformer is trained to predict the temporal dynamics in latent space. Using the $\beta$-VAE to learn disentangled representations in latent-space, we obtain a more interpretable flow model with features that resemble those observed in the proper orthogonal decomposition, but with a more efficient representation. Using Poincar\'e maps, the results show that our method can capture the underlying dynamics of the flow outperforming other prediction models. The proposed method has potential applications in other fields such as weather forecasting, st
    
[^31]: 基于物理知识的神经网络模型求解障碍物相关方程

    A physics-informed neural network framework for modeling obstacle-related equations. (arXiv:2304.03552v1 [cs.LG])

    [http://arxiv.org/abs/2304.03552](http://arxiv.org/abs/2304.03552)

    本文拓展了基于物理知识的神经网络(PINN) 来解决求解障碍物相关的偏微分方程问题，这种类型的问题需要解决数值方法的难度较大，但作者通过对多种情况的研究证明了PINN的有效性。

    

    深度学习在一些应用中取得了很大成功，但将其用于求解偏微分方程(PDE)　的研究则是近年来的热点，尤其在目前的机器学习库（如TensorFlow或PyTorch）的支持下取得了重大进展。基于物理知识的神经网络（PINN）可通过解析稀疏且噪声数据来求解偏微分方程，是一种有吸引力的工具。本文将拓展PINN来求解障碍物相关PDE，这类方程难度较大，需要可以得到准确解的数值方法。作者在正常和不规则的障碍情况下，对线性和非线性PDE的多个场景进行了演示，证明了所提出的PINNs性能的有效性。

    Deep learning has been highly successful in some applications. Nevertheless, its use for solving partial differential equations (PDEs) has only been of recent interest with current state-of-the-art machine learning libraries, e.g., TensorFlow or PyTorch. Physics-informed neural networks (PINNs) are an attractive tool for solving partial differential equations based on sparse and noisy data. Here extend PINNs to solve obstacle-related PDEs which present a great computational challenge because they necessitate numerical methods that can yield an accurate approximation of the solution that lies above a given obstacle. The performance of the proposed PINNs is demonstrated in multiple scenarios for linear and nonlinear PDEs subject to regular and irregular obstacles.
    
[^32]: AI模型排除：方法与选择。(arXiv：2304.03545v1 [cs.LG])

    AI Model Disgorgement: Methods and Choices. (arXiv:2304.03545v1 [cs.LG])

    [http://arxiv.org/abs/2304.03545](http://arxiv.org/abs/2304.03545)

    随着机器学习模型的复杂性和数据量的增加，模型错误变得更难以修复，模型排除作为一种解决方案被提出。

    

    数据的负责任使用是任何机器学习（ML）实现不可或缺的部分。ML开发人员必须仔细收集和策划他们的数据集，并记录它们的来源。他们还必须确保尊重知识产权，保护个人隐私，并以合法的方式使用数据。近年来，ML模型的大小和复杂性显着增加。这些模型需要大量的数据和计算能力进行训练，以至于训练语料库中的任何缺陷都不能通过从头开始重新训练模型轻松修复。尽管在训练数据上有复杂的控制，并且花费了大量的努力来确保训练语料库被正确组成，但是模型所需数据的大量使得手动检查每个数据都是具有挑战性的。解决训练语料库中数据缺陷的一个潜在方法是模型排除——不仅排除缺陷数据和样本，而将模型权重和训练代码都排除掉。

    Responsible use of data is an indispensable part of any machine learning (ML) implementation. ML developers must carefully collect and curate their datasets, and document their provenance. They must also make sure to respect intellectual property rights, preserve individual privacy, and use data in an ethical way. Over the past few years, ML models have significantly increased in size and complexity. These models require a very large amount of data and compute capacity to train, to the extent that any defects in the training corpus cannot be trivially remedied by retraining the model from scratch. Despite sophisticated controls on training data and a significant amount of effort dedicated to ensuring that training corpora are properly composed, the sheer volume of data required for the models makes it challenging to manually inspect each datum comprising a training corpus. One potential fix for training corpus data defects is model disgorgement -- the elimination of not just the improp
    
[^33]: HyperTab: 基于超网络的小型表格数据深度学习方法

    HyperTab: Hypernetwork Approach for Deep Learning on Small Tabular Datasets. (arXiv:2304.03543v1 [cs.LG])

    [http://arxiv.org/abs/2304.03543](http://arxiv.org/abs/2304.03543)

    HyperTab是一种基于超网络结合了随机森林和神经网络优点的小型表格数据深度学习方法，使用每个特定低维视图处理数据，虚拟增加训练样本数量，避免过度拟合。

    

    深度学习在许多领域取得了惊人的表现，例如计算机视觉和自然语言处理，但它在表格数据集上相对传统浅层方法的优势仍然值得商榷。在小型数据集（小于1k个样本）上超过树状集成（如XGBoost或随机森林）的表现尤其具有挑战性。为了解决这个问题，我们引入了HyperTab，这是一种基于超网络解决表格数据集小样本问题的方法。通过将随机森林和神经网络的优点结合起来，HyperTab生成了一个神经网络集合，其中每个目标模型专门处理数据的特定低维视图。由于每个视图扮演数据增强的角色，我们在保持可训练参数数量不变的情况下，虚拟增加了训练样本数量，从而避免了过度拟合。我们对40多个大小不同的表格数据集对HyperTab进行了评估。

    Deep learning has achieved impressive performance in many domains, such as computer vision and natural language processing, but its advantage over classical shallow methods on tabular datasets remains questionable. It is especially challenging to surpass the performance of tree-like ensembles, such as XGBoost or Random Forests, on small-sized datasets (less than 1k samples). To tackle this challenge, we introduce HyperTab, a hypernetwork-based approach to solving small sample problems on tabular datasets. By combining the advantages of Random Forests and neural networks, HyperTab generates an ensemble of neural networks, where each target model is specialized to process a specific lower-dimensional view of the data. Since each view plays the role of data augmentation, we virtually increase the number of training samples while keeping the number of trainable parameters unchanged, which prevents model overfitting. We evaluated HyperTab on more than 40 tabular datasets of a varying number
    
[^34]: ChatPipe：通过优化人-ChatGPT互动来编排数据准备程序

    ChatPipe: Orchestrating Data Preparation Program by Optimizing Human-ChatGPT Interactions. (arXiv:2304.03540v1 [cs.DB])

    [http://arxiv.org/abs/2304.03540](http://arxiv.org/abs/2304.03540)

    ChatPipe 提出了一个新系统，旨在通过自然语言交互优化 ChatGPT 编排 ML 数据准备程序，有效推荐下一个数据准备操作，方便用户进行程序的修改和版本切换，能够显著减少 ML 数据准备所需的时间和精力。

    

    编排高质量的数据准备程序对于成功的机器学习（ML）至关重要，但众所周知，这是耗时费力的。尽管像ChatGPT这样的大型语言模型在通过自然语言提示与用户交互生成程序方面具有令人印象深刻的能力，但仍存在限制。具体而言，用户必须提供特定的提示来引导ChatGPT迭代地改进数据准备程序，这需要对编程、使用的数据集和ML任务有一定的专业知识。此外，一旦生成了程序，在不重新开始整个过程的情况下回顾先前的版本或对程序进行更改是很困难的。在本文中，我们提出了ChatPipe，这是一个新颖的系统，旨在促进用户和ChatGPT之间的无缝交互。 ChatPipe为用户提供关于下一个数据准备操作的有效建议，并指导ChatGPT生成操作的程序。另外，Chatpipe使用户能够轻松修改生成的程序或切换到先前的版本。我们对三个真实的数据集进行了实验，结果表明，ChatPipe可以显著减少为ML任务准备数据所需的时间和精力。

    Orchestrating a high-quality data preparation program is essential for successful machine learning (ML), but it is known to be time and effort consuming. Despite the impressive capabilities of large language models like ChatGPT in generating programs by interacting with users through natural language prompts, there are still limitations. Specifically, a user must provide specific prompts to iteratively guide ChatGPT in improving data preparation programs, which requires a certain level of expertise in programming, the dataset used and the ML task. Moreover, once a program has been generated, it is non-trivial to revisit a previous version or make changes to the program without starting the process over again. In this paper, we present ChatPipe, a novel system designed to facilitate seamless interaction between users and ChatGPT. ChatPipe provides users with effective recommendation on next data preparation operations, and guides ChatGPT to generate program for the operations. Also, Cha
    
[^35]: 基于自编码器的学习结构的可调节隐私

    Adjustable Privacy using Autoencoder-based Learning Structure. (arXiv:2304.03538v1 [cs.LG])

    [http://arxiv.org/abs/2304.03538](http://arxiv.org/abs/2304.03538)

    本论文介绍了一种基于自编码器的学习结构，可以管理数据提供商与推理中心之间的效用-隐私权衡，同时在保持机密特征的隐私的同时提供有关数据的非机密特征。

    

    推理中心需要更多数据才能拥有更全面和有益的学习模型，为此，他们需要从数据提供者处收集数据。另一方面，数据提供者在隐私方面非常谨慎，不愿将其数据集提供给推理中心。本文通过修改自编码器的结构，提出了一种很好地管理效用-隐私权衡的方法。具体而言，数据首先通过编码器压缩，然后使用分类器分离与相关的机密和非机密特征。机密特征与噪声适当地结合，非机密特征则得到加强，最后通过解码器产生具有原始数据格式的数据。所提出的架构还允许数据提供者设置机密特征所需的隐私级别。所提出的方法已在图像和分类数据库上进行了测试，结果很好。

    Inference centers need more data to have a more comprehensive and beneficial learning model, and for this purpose, they need to collect data from data providers. On the other hand, data providers are cautious about delivering their datasets to inference centers in terms of privacy considerations. In this paper, by modifying the structure of the autoencoder, we present a method that manages the utility-privacy trade-off well. To be more precise, the data is first compressed using the encoder, then confidential and non-confidential features are separated and uncorrelated using the classifier. The confidential feature is appropriately combined with noise, and the non-confidential feature is enhanced, and at the end, data with the original data format is produced by the decoder. The proposed architecture also allows data providers to set the level of privacy required for confidential features. The proposed method has been examined for both image and categorical databases, and the results s
    
[^36]: 借助GAN应对COVID-19数据稀缺问题：别被炒作所蒙蔽

    Leveraging GANs for data scarcity of COVID-19: Beyond the hype. (arXiv:2304.03536v1 [eess.IV])

    [http://arxiv.org/abs/2304.03536](http://arxiv.org/abs/2304.03536)

    本论文分析了43篇使用GAN产生合成数据的报告，发现这些研究存在数据偏见、缺乏可重复性以及缺乏临床相关性。GAN基于人工智能的方法有望为诊断COVID-19的AI模型产生合成数据，但需要解决这些问题。

    

    基于人工智能（AI）的模型可以帮助从肺部CT扫描和X射线图像中诊断COVID-19，但是这些模型需要大量的数据来进行训练和验证。许多研究人员研究了生成对抗网络（GAN）来产生合成的肺部CT扫描和X射线图像，以提高AI模型的性能。目前尚未探索如何使用GAN生成可靠的合成数据。本项研究分析了43篇发表的报告，报告了GAN用于合成数据生成。这些研究中许多存在数据偏见、缺乏可重复性以及缺乏来自放射学家或其他领域专家的反馈。这些研究的一个普遍问题是源代码不可用，这妨碍了可重复性。这些研究中还报告了将输入图像重新缩放以训练现有的GAN架构的方法，但没有提供如何缩放的临床见解。最后，虽然基于GAN的方法有望为诊断COVID-19的AI模型产生合成数据，但本项研究分析的研究具有有限的可重复性和临床相关性。

    Artificial Intelligence (AI)-based models can help in diagnosing COVID-19 from lung CT scans and X-ray images; however, these models require large amounts of data for training and validation. Many researchers studied Generative Adversarial Networks (GANs) for producing synthetic lung CT scans and X-Ray images to improve the performance of AI-based models. It is not well explored how good GAN-based methods performed to generate reliable synthetic data. This work analyzes 43 published studies that reported GANs for synthetic data generation. Many of these studies suffered data bias, lack of reproducibility, and lack of feedback from the radiologists or other domain experts. A common issue in these studies is the unavailability of the source code, hindering reproducibility. The included studies reported rescaling of the input images to train the existing GANs architecture without providing clinical insights on how the rescaling was motivated. Finally, even though GAN-based methods have th
    
[^37]: SSS在SemEval-2023任务10中的论文：使用投票细调变压器可解释的检测在线性别歧视。 (arXiv：2304.03518v1 [cs.CL])

    SSS at SemEval-2023 Task 10: Explainable Detection of Online Sexism using Majority Voted Fine-Tuned Transformers. (arXiv:2304.03518v1 [cs.CL])

    [http://arxiv.org/abs/2304.03518](http://arxiv.org/abs/2304.03518)

    本文描述了使用细调BERT模型和多数投票集成模型来检测和解释在线性别歧视的方法。翻转显着降低了女性在社交媒体平台上经历不成比例的性别歧视的风险。

    

    本文描述了我们在SemEval 2023任务10中提交的作品-可解释的在线性别歧视检测（EDOS），分为三个子任务。社交媒体平台的不断增长导致女性在社交媒体平台上面临不成比例的性别歧视。这使得检测和解释在线性别歧视内容变得比以往更加重要，以使社交媒体对女性更加安全和可访问。我们的方法包括实验和微调基于BERT的模型，并使用多数投票集合模型，该模型优于单个基线模型得分。我们的系统在任务A中实现了宏F1分数0.8392，在任务B中为0.6092，在任务C中为0.4319。

    This paper describes our submission to Task 10 at SemEval 2023-Explainable Detection of Online Sexism (EDOS), divided into three subtasks. The recent rise in social media platforms has seen an increase in disproportionate levels of sexism experienced by women on social media platforms. This has made detecting and explaining online sexist content more important than ever to make social media safer and more accessible for women. Our approach consists of experimenting and finetuning BERT-based models and using a Majority Voting ensemble model that outperforms individual baseline model scores. Our system achieves a macro F1 score of 0.8392 for Task A, 0.6092 for Task B, and 0.4319 for Task C.
    
[^38]: 图神经网络中用于节点分类的分布式信号

    Distributional Signals for Node Classification in Graph Neural Networks. (arXiv:2304.03507v1 [eess.SP])

    [http://arxiv.org/abs/2304.03507](http://arxiv.org/abs/2304.03507)

    本文提出了一种通用的GNN正则化方法，通过使用节点标签的分布来进行编码，提升了节点分类任务中大多数基本GNN模型的性能。

    

    在图神经网络(GNN)中，节点特征和标签都是图信号处理(GSP)中的重要概念。虽然在学习和估计任务中，GSP中通常会施加信号平滑性约束，但如何针对离散的节点标签实现这一点尚不清楚。本文通过引入分布式图信号的概念解决了这一问题。在我们的框架中，我们使用节点标签的分布而不是值，并提出了这种分布式图信号的平滑性和非均匀性概念。然后，我们提出了一种通用的GNN正则化方法，允许我们在半监督节点分类任务中对模型输出的分布式平滑性和非均匀性进行编码。数值实验表明，我们的方法可以显著提高不同问题设置中的大多数基本GNN模型的性能。

    In graph neural networks (GNNs), both node features and labels are examples of graph signals, a key notion in graph signal processing (GSP). While it is common in GSP to impose signal smoothness constraints in learning and estimation tasks, it is unclear how this can be done for discrete node labels. We bridge this gap by introducing the concept of distributional graph signals. In our framework, we work with the distributions of node labels instead of their values and propose notions of smoothness and non-uniformity of such distributional graph signals. We then propose a general regularization method for GNNs that allows us to encode distributional smoothness and non-uniformity of the model output in semi-supervised node classification tasks. Numerical experiments demonstrate that our method can significantly improve the performance of most base GNN models in different problem settings.
    
[^39]: F-RDW: 基于未来位置预测的重定向行走

    F-RDW: Redirected Walking with Forecasting Future Position. (arXiv:2304.03497v1 [cs.HC])

    [http://arxiv.org/abs/2304.03497](http://arxiv.org/abs/2304.03497)

    F-RDW是一种基于未来位置预测的重定向行走机制，可将预测信息与现有的重定向行走方法相结合，显著减少重置次数，同时能够适用于各种虚拟环境。

    

    为了给用户提供更好的虚拟现实体验，现有的重定向行走预测方法利用未来信息来减少重置次数。然而，这些方法往往在部署时需要满足一个先决条件，可能限制其普适性。为了解决这个挑战，我们提出了一种新颖的机制 F-RDW，它具有两个方面：（1）预测虚拟空间中用户的未来位置，而不需要任何假设；（2）将这些信息与现有的重定向行走方法相结合。第一步的基础是一个基于 LSTM 的模型，该模型利用用户的空间和眼动跟踪数据来预测用户在虚拟空间中的未来位置，第二步将这些预测值以适用的方式输入到现有的重定向行走方法中（例如 MPCRed，S2C，TAPF 和 ARC）中。我们的仿真测试和用户研究的结果表明，F-RDW 可显著减少重置次数，同时使虚拟环境与现实世界保持一致。此外，F-RDW 可无缝应用于各种虚拟环境，而不需要满足任何先决条件。

    In order to serve better VR experiences to users, existing predictive methods of Redirected Walking (RDW) exploit future information to reduce the number of reset occurrences. However, such methods often impose a precondition during deployment, either in the virtual environment's layout or the user's walking direction, which constrains its universal applications. To tackle this challenge, we propose a novel mechanism F-RDW that is twofold: (1) forecasts the future information of a user in the virtual space without any assumptions, and (2) fuse this information while maneuvering existing RDW methods. The backbone of the first step is an LSTM-based model that ingests the user's spatial and eye-tracking data to predict the user's future position in the virtual space, and the following step feeds those predicted values into existing RDW methods (such as MPCRed, S2C, TAPF, and ARC) while respecting their internal mechanism in applicable ways.The results of our simulation test and user study
    
[^40]: 深度神经网络的保体系结构可证明修复方法

    Architecture-Preserving Provable Repair of Deep Neural Networks. (arXiv:2304.03496v1 [cs.LG])

    [http://arxiv.org/abs/2304.03496](http://arxiv.org/abs/2304.03496)

    本文提出了一种保体系结构 V-多面体可证明修复深度神经网络的方法。修复只修改 DNN 的参数，具有灵活性，支持多种类型的层，并在多项式时间内运行。

    

    深度神经网络（DNNs）成为了软件中越来越重要的组成部分，并被认为是解决许多问题（如图像识别）的最先进解决方案。然而，DNN 远非不可错误，DNN 的不正确行为可能会在现实世界中造成灾难性后果。本文解决了保体系结构 V-多面体可证明修复 DNNs 的问题。V-多面体使用其顶点表示法定义了一个凸约束多面体。V-多面体可证明修复保证修复后的 DNN 满足给定 V-多面体中无限点集上的规范。体系结构保持修复仅修改 DNN 的参数，而不修改其体系结构。修复有灵活性，可以修改 DNN 的多个层，并在多项式时间内运行。它支持具有一些线性部分的激活函数，以及完全连接的、卷积的、池化的和残余层的 DNNs。据我们所知，这是第一篇提供 DNN 体系结构保持可证明修复的正式框架的论文。

    Deep neural networks (DNNs) are becoming increasingly important components of software, and are considered the state-of-the-art solution for a number of problems, such as image recognition. However, DNNs are far from infallible, and incorrect behavior of DNNs can have disastrous real-world consequences. This paper addresses the problem of architecture-preserving V-polytope provable repair of DNNs. A V-polytope defines a convex bounded polytope using its vertex representation. V-polytope provable repair guarantees that the repaired DNN satisfies the given specification on the infinite set of points in the given V-polytope. An architecture-preserving repair only modifies the parameters of the DNN, without modifying its architecture. The repair has the flexibility to modify multiple layers of the DNN, and runs in polynomial time. It supports DNNs with activation functions that have some linear pieces, as well as fully-connected, convolutional, pooling and residual layers. To the best our 
    
[^41]: ParaGraph: 用于 HPC Kernel 性能优化的加权图表示法

    ParaGraph: Weighted Graph Representation for Performance Optimization of HPC Kernels. (arXiv:2304.03487v1 [cs.DC])

    [http://arxiv.org/abs/2304.03487](http://arxiv.org/abs/2304.03487)

    ParaGraph提出了一种基于加权图的程序表示法，用于优化HPC内核代码，帮助机器学习算法传递应用程序特定信息以帮助提高性能。

    

    基于 GPU 的 HPC 集群因其广泛的并行性和高能效性吸引了越来越多的科学应用程序开发人员。为了在多种多核架构中实现可移植性，一种应用程序开发人员的流行选择是利用基于指令的并行编程模型，例如 OpenMP。然而，即使使用 OpenMP，开发人员也必须从许多策略中选择用于利用 GPU 或 CPU。近年来，机器学习 (ML) 方法在 HPC 应用程序优化方面带来了显著进展。为此，已经提出了几种用于 ML 模型表示应用程序特征的方法。然而，现有技术未能捕获展示并行性至关重要的特征。因此，本文介绍了一种新的基于图形的程序表示法，用于表示控制和数据流信息，扩展了抽象语法树。本文的独创性在于对图中节点加权，传递应用程序特定信息，帮助机器学习算法优化 HPC 内核代码。

    GPU-based HPC clusters are attracting more scientific application developers due to their extensive parallelism and energy efficiency. In order to achieve portability among a variety of multi/many core architectures, a popular choice for an application developer is to utilize directive-based parallel programming models, such as OpenMP. However, even with OpenMP, the developer must choose from among many strategies for exploiting a GPU or a CPU. Recently, Machine Learning (ML) approaches have brought significant advances in the optimizations of HPC applications. To this end, several ways have been proposed to represent application characteristics for ML models. However, the available techniques fail to capture features that are crucial for exposing parallelism. In this paper, we introduce a new graph-based program representation for parallel applications that extends the Abstract Syntax Tree to represent control and data flow information. The originality of this work lies in the additio
    
[^42]: RED-PSM: 带去噪正则化的部分可分模型用于动态成像

    RED-PSM: Regularization by Denoising of Partially Separable Models for Dynamic Imaging. (arXiv:2304.03483v1 [eess.IV])

    [http://arxiv.org/abs/2304.03483](http://arxiv.org/abs/2304.03483)

    本文提出了一种称为RED-PSM的方法，将部分可分模型与去噪正则化相结合，用于解决动态成像问题，数值实验证明其优越性。

    

    动态成像是指利用被欠采样的测量数据恢复每个时间点上的时变二维或三维物体。尤其是在动态断层扫描中，每个时间点上只有一个视角下的单个投影可用，使得问题严重算不可逆。本文提出了一种称为RED-PSM的方法，首次将两种强大的技术结合起来解决这个具有挑战性的成像问题。第一种技术是部分可分模型，已经用于高效地为时空目标引入低秩先验。第二种是最近提出的去噪正则化(RED)，它提供了一种灵活的框架，利用最先进的图像去噪算法处理各种反问题。我们提出了一个带正则化的部分可分目标，通过变量分裂和ADMM优化方案，并证明了我们的目标收敛于一个满足优化问题的稳定点。我们通过在具有挑战性的动态断层扫描问题上进行了大量的数值实验，证明了所提出的RED-PSM方法的优越性，相较于现有的动态成像技术。

    Dynamic imaging addresses the recovery of a time-varying 2D or 3D object at each time instant using its undersampled measurements. In particular, in the case of dynamic tomography, only a single projection at a single view angle may be available at a time, making the problem severely ill-posed. In this work, we propose an approach, RED-PSM, which combines for the first time two powerful techniques to address this challenging imaging problem. The first, are partially separable models, which have been used to efficiently introduce a low-rank prior for the spatio-temporal object. The second is the recent Regularization by Denoising (RED), which provides a flexible framework to exploit the impressive performance of state-of-the-art image denoising algorithms, for various inverse problems. We propose a partially separable objective with RED and an optimization scheme with variable splitting and ADMM, and prove convergence of our objective to a value corresponding to a stationary point satis
    
[^43]: 重新考虑基于GNN的异构知识图谱实体对齐：新数据集和新方法

    Rethinking GNN-based Entity Alignment on Heterogeneous Knowledge Graphs: New Datasets and A New Method. (arXiv:2304.03468v1 [cs.LG])

    [http://arxiv.org/abs/2304.03468](http://arxiv.org/abs/2304.03468)

    文章重新考虑了基于GNN的异构知识图谱实体对齐。为探究EA实际场景中的表现，提出了更接近现实的高度异构知识图谱数据集，并提出了新方法。

    

    知识图谱（KG）应用的发展导致了需要从各种来源提取的异构KG之间的实体对齐（EA）的不断增长需求。近来，由于GNN的出色结构信息捕捉能力，在EA任务中广泛采用GNN。然而，我们观察到现有常见EA数据集的过于简单化的设置与现实场景相距甚远，这妨碍了对最近方法所取得进展的全面理解。这种现象使我们深思：现有基于GNN的EA方法是否真的取得了伟大进展？为了研究EA方法在现实情况下的性能，本文聚焦于高度异构的KG（HHKG）（例如，事件KG和通用KG）的对齐，这些KG在规模和结构上不同，并共享更少的重叠实体。首先，我们清理了不合理的设置，并提出了两个新的HHKG数据集，其密切地模拟了现实世界场景。

    The development of knowledge graph (KG) applications has led to a rising need for entity alignment (EA) between heterogeneous KGs that are extracted from various sources. Recently, graph neural networks (GNNs) have been widely adopted in EA tasks due to GNNs' impressive ability to capture structure information. However, we have observed that the oversimplified settings of the existing common EA datasets are distant from real-world scenarios, which obstructs a full understanding of the advancements achieved by recent methods. This phenomenon makes us ponder: Do existing GNN-based EA methods really make great progress?  In this paper, to study the performance of EA methods in realistic settings, we focus on the alignment of highly heterogeneous KGs (HHKGs) (e.g., event KGs and general KGs) which are different with regard to the scale and structure, and share fewer overlapping entities. First, we sweep the unreasonable settings, and propose two new HHKG datasets that closely mimic real-wo
    
[^44]: 一种用于序列早期分类的策略

    A Policy for Early Sequence Classification. (arXiv:2304.03463v1 [cs.LG])

    [http://arxiv.org/abs/2304.03463](http://arxiv.org/abs/2304.03463)

    本论文提出了一种用于早期序列分类的新策略——分类器诱导停止。与以往方法依赖探索学习停止和分类不同，本方法采用监督方法直接进行分类，AUC值可增加11.8%。

    

    序列往往不是一次性全部接收的，而是逐个元素地逐步接收。由于早期预测带来更大的收益，因此人们希望尽快、尽可能准确地对序列进行分类，而不必等到最后一个元素。为了实现这种早期序列分类，我们提出了一种新的分类器诱导停止方法。而以往的方法依赖于训练期间的探索来学习何时停止和分类，而我们的方法是一种更直接的监督方法。我们的分类器诱导停止在多个实验中平均 Pareto 前沿 AUC 增加了 11.8%。

    Sequences are often not received in their entirety at once, but instead, received incrementally over time, element by element. Early predictions yielding a higher benefit, one aims to classify a sequence as accurately as possible, as soon as possible, without having to wait for the last element. For this early sequence classification, we introduce our novel classifier-induced stopping. While previous methods depend on exploration during training to learn when to stop and classify, ours is a more direct, supervised approach. Our classifier-induced stopping achieves an average Pareto frontier AUC increase of 11.8% over multiple experiments.
    
[^45]: 重新思考基于自监督学习的视觉表示评价协议

    Rethinking Evaluation Protocols of Visual Representations Learned via Self-supervised Learning. (arXiv:2304.03456v1 [cs.CV])

    [http://arxiv.org/abs/2304.03456](http://arxiv.org/abs/2304.03456)

    本研究重新思考基于自监督学习的视觉表示的评价协议，发现对于线性探测来说，输入归一化是消除性能变化的关键因素。

    

    对学习自监督学习的视觉表示的质量进行评价常常采用在带标注的上游数据集上进行的线性探测和K-NN方法，以及将其迁移到各种下游数据集上的迁移学习。虽然现有的自监督学习方法已经在这些评价协议下表现出良好的性能，但我们观察到这些性能非常敏感于线性探测和迁移学习中涉及的超参数。本文试图通过进行大量实验来找出这种性能敏感性的原因。结果发现，对于线性探测来说，输入归一化是消除性能变化的关键因素。

    Linear probing (LP) (and $k$-NN) on the upstream dataset with labels (e.g., ImageNet) and transfer learning (TL) to various downstream datasets are commonly employed to evaluate the quality of visual representations learned via self-supervised learning (SSL). Although existing SSL methods have shown good performances under those evaluation protocols, we observe that the performances are very sensitive to the hyperparameters involved in LP and TL. We argue that this is an undesirable behavior since truly generic representations should be easily adapted to any other visual recognition task, i.e., the learned representations should be robust to the settings of LP and TL hyperparameters. In this work, we try to figure out the cause of performance sensitivity by conducting extensive experiments with state-of-the-art SSL methods. First, we find that input normalization for LP is crucial to eliminate performance variations according to the hyperparameters. Specifically, batch normalization be
    
[^46]: 图结构支持的跨领域知识转移

    Graph Enabled Cross-Domain Knowledge Transfer. (arXiv:2304.03452v1 [cs.LG])

    [http://arxiv.org/abs/2304.03452](http://arxiv.org/abs/2304.03452)

    稀缺知识对自动化决策造成了障碍，跨领域知识转移是通过融合来自不同领域的辅助信息，缓解不同领域知识差距的一种方法。

    

    为了将机器学习应用于任何决策过程中，必须将给定的知识（例如自然语言，非结构化文本）转化为可以被其兼容语言和数据格式的机器学习模型理解和处理的表示向量。然而，经常遇到的困难是，首先给定的知识并不充分或可靠。在这种情况下，人们会寻求融合来自单独领域的辅助信息来缓解好的表示学习和感兴趣领域的稀缺知识之间的差距。这种方法被称为跨领域知识转移。研究这个问题是至关重要的，因为在许多情况下，从在线医疗平台分析到金融市场风险量化，都存在稀缺知识的共性，这为我们从自动化决策中受益留下了障碍。从机器学习的角度来看，半监督学习范式利用了这种跨领域转移的思想。

    To leverage machine learning in any decision-making process, one must convert the given knowledge (for example, natural language, unstructured text) into representation vectors that can be understood and processed by machine learning model in their compatible language and data format. The frequently encountered difficulty is, however, the given knowledge is not rich or reliable enough in the first place. In such cases, one seeks to fuse side information from a separate domain to mitigate the gap between good representation learning and the scarce knowledge in the domain of interest. This approach is named Cross-Domain Knowledge Transfer. It is crucial to study the problem because of the commonality of scarce knowledge in many scenarios, from online healthcare platform analyses to financial market risk quantification, leaving an obstacle in front of us benefiting from automated decision making. From the machine learning perspective, the paradigm of semi-supervised learning takes advanta
    
[^47]: 生成代理: 人类行为的交互仿真器

    Generative Agents: Interactive Simulacra of Human Behavior. (arXiv:2304.03442v1 [cs.HC])

    [http://arxiv.org/abs/2304.03442](http://arxiv.org/abs/2304.03442)

    本文介绍了一种生成代理的架构，它能够仿真出具有可信度的人类行为，填充交互式沙盒环境，为创造更加真实的人机交互体验提供了一种新的思路。

    

    可信的人类行为仿真可赋能于从沉浸式环境到人际交流排练空间到原型工具的交互式应用程序。在本文中，我们介绍了生成代理——具有可信度的人类行为仿真的计算机软件代理。生成代理会起床，做早餐，去工作；艺术家画画，作家写作；他们形成观点，互相注意，并开始交谈；他们回忆过去的日子并计划未来。为了使生成代理能够实现，我们描述了一种架构，它将大型语言模型扩展到使用自然语言存储代理的经历的完整记录，随着时间的推移综合这些记忆到更高层次的反思，以及动态检索这些记忆以规划行为。我们实例化生成代理以填充受《模拟人生》启发的交互式沙盒环境，最终用户可以使用自然语言对话系统与25个代理交互。

    Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natur
    
[^48]: 带有异构相似性的监督对比学习用于分布偏移问题

    Supervised Contrastive Learning with Heterogeneous Similarity for Distribution Shifts. (arXiv:2304.03440v1 [cs.LG])

    [http://arxiv.org/abs/2304.03440](http://arxiv.org/abs/2304.03440)

    本文提出了一种带有异构相似性的新的监督对比学习方法，用于解决分布偏移问题，防止过拟合影响模型性能。

    

    数据的分布在训练和测试时发生变化会导致分布偏移问题，进而严重影响模型在实际应用中的性能表现。近期研究表明，过拟合是其原因之一，合适的正则化可以缓解这种影响，尤其适用于使用神经网络等高度具有代表性的模型。本文提出了一种新的监督对比学习方法，通过该方法可以防止过拟合，训练模型避免在分布偏移下性能退化。作者将对比损失中的余弦相似性扩展为更通用的相似性度量，并建议在比较样本与正样本或负样本时使用不同的参数，在理论上这一建议被证明可以作为对比损失中的一种边缘效应。实验在模拟分布偏移的基准数据集上进行，包括子种群偏移和...（原文未完成）

    Distribution shifts are problems where the distribution of data changes between training and testing, which can significantly degrade the performance of a model deployed in the real world. Recent studies suggest that one reason for the degradation is a type of overfitting, and that proper regularization can mitigate the degradation, especially when using highly representative models such as neural networks. In this paper, we propose a new regularization using the supervised contrastive learning to prevent such overfitting and to train models that do not degrade their performance under the distribution shifts. We extend the cosine similarity in contrastive loss to a more general similarity measure and propose to use different parameters in the measure when comparing a sample to a positive or negative example, which is analytically shown to act as a kind of margin in contrastive loss. Experiments on benchmark datasets that emulate distribution shifts, including subpopulation shift and do
    
[^49]: 鲁棒不变表示中的域泛化

    Domain Generalization In Robust Invariant Representation. (arXiv:2304.03431v1 [cs.LG])

    [http://arxiv.org/abs/2304.03431](http://arxiv.org/abs/2304.03431)

    本文研究了不变表示的泛化性能，证明具有不变表示的模型可以学习到具有鲁棒性的非结构化潜在表示，因此使不变性成为域泛化的一个关键方面。

    

    无监督学习常见变换的不变表示方法常用于目标识别。学习不变性使得模型更加鲁棒，并在实际场景中更容易应用。由于不改变对象固有属性的数据变换是识别任务中主要的复杂性来源，对这些变换具有不变性的模型有助于减少所需的训练数据。这进一步提高了模型的效率并简化了训练过程。本文研究了不变表示的泛化性能，并试图回答一个问题：具有某些变换不变性的模型在先前未见域中是否仍具有不变性？通过广泛的实验，我们证明了具有不变表示的模型可以学习到具有鲁棒性的非结构化潜在表示，因此使不变性成为域泛化的一个关键方面。

    Unsupervised approaches for learning representations invariant to common transformations are used quite often for object recognition. Learning invariances makes models more robust and practical to use in real-world scenarios. Since data transformations that do not change the intrinsic properties of the object cause the majority of the complexity in recognition tasks, models that are invariant to these transformations help reduce the amount of training data required. This further increases the model's efficiency and simplifies training. In this paper, we investigate the generalization of invariant representations on out-of-distribution data and try to answer the question: Do model representations invariant to some transformations in a particular seen domain also remain invariant in previously unseen domains? Through extensive experiments, we demonstrate that the invariant model learns unstructured latent representations that are robust to distribution shifts, thus making invariance a de
    
[^50]: 基于谷歌OCR扫描的藏文手稿的神经拼写纠错模型

    Cleansing Jewel: A Neural Spelling Correction Model Built On Google OCR-ed Tibetan Manuscripts. (arXiv:2304.03427v1 [cs.CL])

    [http://arxiv.org/abs/2304.03427](http://arxiv.org/abs/2304.03427)

    本文提出了一种基于谷歌OCR扫描的藏文手稿的神经拼写纠错模型，可以自动纠正OCR输出中的噪声。

    

    人文学者在研究历史、宗教和社会政治结构等方面经常依赖于古代手稿。虽然OCR技术可以将这些宝贵手稿数字化，但多数手稿因磨损而过时，OCR程序没办法识别翻页的虚淡或污渍。本文提出了一种基于谷歌OCR扫描的藏文手稿的神经拼写纠错模型，可以自动纠正OCR输出中的噪声。本文分为四个部分：数据集、模型架构、训练和分析。首先，我们对原始藏文电子文本语料库进行了特征工程，并将其转化为两组结构化数据框——一组匹配的玩具数据和一组匹配的真实数据。然后，我们在Transformer架构中实现了置信度得分机制来执行拼写校正任务。根据损失和字符错误率，我们的Transformer + 置信度得分机制比其他常用的拼写校正算法表现更好。

    Scholars in the humanities rely heavily on ancient manuscripts to study history, religion, and socio-political structures in the past. Many efforts have been devoted to digitizing these precious manuscripts using OCR technology, but most manuscripts were blemished over the centuries so that an Optical Character Recognition (OCR) program cannot be expected to capture faded graphs and stains on pages. This work presents a neural spelling correction model built on Google OCR-ed Tibetan Manuscripts to auto-correct OCR-ed noisy output. This paper is divided into four sections: dataset, model architecture, training and analysis. First, we feature-engineered our raw Tibetan etext corpus into two sets of structured data frames -- a set of paired toy data and a set of paired real data. Then, we implemented a Confidence Score mechanism into the Transformer architecture to perform spelling correction tasks. According to the Loss and Character Error Rate, our Transformer + Confidence score mechani
    
[^51]: 用于稳定深度强化学习控制的模块化框架

    A modular framework for stabilizing deep reinforcement learning control. (arXiv:2304.03422v1 [eess.SY])

    [http://arxiv.org/abs/2304.03422](http://arxiv.org/abs/2304.03422)

    该论文提出了一个结合了深度强化学习的优化驱动和无模型优势、使用Youla-Kucera参数化定义搜索域提供稳定性保证的框架，利用一个数据驱动的内部模型实现替代方法，采用神经网络无缝地与标准深度学习库集成，实现了在一个真实的两个水箱系统的模拟中，通过表达参数化的非线性稳定算子集的方法，设计反馈控制器。

    

    我们提出了一个框架来设计反馈控制器，该框架结合了深度强化学习的优化驱动和无模型优势，以及使用Youla-Kucera参数化定义搜索域提供的稳定性保证。最近行为系统的进展使我们能够构建数据驱动的内部模型；这使得可以基于输入输出探索数据完全实现使用Youla-Kucera参数化的替代方法。使用神经网络来表示参数化的非线性稳定算子集，可以无缝地与标准深度学习库集成。我们在一个真实的两个水箱系统的模拟中演示了这种方法。

    We propose a framework for the design of feedback controllers that combines the optimization-driven and model-free advantages of deep reinforcement learning with the stability guarantees provided by using the Youla-Kucera parameterization to define the search domain. Recent advances in behavioral systems allow us to construct a data-driven internal model; this enables an alternative realization of the Youla-Kucera parameterization based entirely on input-output exploration data. Using a neural network to express a parameterized set of nonlinear stable operators enables seamless integration with standard deep learning libraries. We demonstrate the approach on a realistic simulation of a two-tank system.
    
[^52]: 该文题目为："叫醒还是不叫醒：通过连续细化来减少关键词误报"

    To Wake-up or Not to Wake-up: Reducing Keyword False Alarm by Successive Refinement. (arXiv:2304.03416v1 [eess.SP])

    [http://arxiv.org/abs/2304.03416](http://arxiv.org/abs/2304.03416)

    本文提出了通过连续细化来减少关键词误报的方法，并展示其在多个模型上的有效性，可应用于任何深度关键词监测系统。

    

    关键词监测系统持续处理音频流以检测关键词。设计这种系统最具挑战性的任务之一是降低假警报（FA），即系统虽然未被唤醒但误注册关键词。在本文中，我们提出了一个简单而优雅的解决方案，该方案遵循全概率法则。我们展示现有的深度关键词监测机制可以通过连续细化得到改进，其中系统首先分类输入音频是否为语音，然后分类输入是否类似于关键词，最后分类口头发出的关键词是哪个。我们表明，跨多个模型，其参数范围从13K到2.41M不等，连续细化技术在域内留存FA数据上将FA减少了高达8倍，在域外（OOD）FA数据上则将其减少了高达7倍。此外，我们提出的方法是“即插即用”的，可应用于任何深度关键词监测系统。

    Keyword spotting systems continuously process audio streams to detect keywords. One of the most challenging tasks in designing such systems is to reduce False Alarm (FA) which happens when the system falsely registers a keyword despite the keyword not being uttered. In this paper, we propose a simple yet elegant solution to this problem that follows from the law of total probability. We show that existing deep keyword spotting mechanisms can be improved by Successive Refinement, where the system first classifies whether the input audio is speech or not, followed by whether the input is keyword-like or not, and finally classifies which keyword was uttered. We show across multiple models with size ranging from 13K parameters to 2.41M parameters, the successive refinement technique reduces FA by up to a factor of 8 on in-domain held-out FA data, and up to a factor of 7 on out-of-domain (OOD) FA data. Further, our proposed approach is "plug-and-play" and can be applied to any deep keyword 
    
[^53]: 有限宽度核和平均场神经网络中的预测波动动力学分析

    Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks. (arXiv:2304.03408v1 [stat.ML])

    [http://arxiv.org/abs/2304.03408](http://arxiv.org/abs/2304.03408)

    本研究分析了宽但有限的特征学习神经网络中有限宽度效应的动力学，提供了对网络权重随机初始化下DMFT序参数波动的表征以及特征学习如何动态地减少最终NTK和最终网络预测的方差。

    

    我们分析了宽但有限的特征学习神经网络中有限宽度效应的动力学。与许多先前的分析不同，我们的结果是针对特征学习强度的非微扰有限宽度的结果。从无限宽深度神经网络核和预测动力学的动力学平均场理论（DMFT）描述开始，我们提供了对网络权重的随机初始化下DMFT序参数$\mathcal{O}(1/\sqrt{\text{width}})$波动的表征。在网络训练的懒惰极限中，所有核都是随机的但在时间上静止的，预测方差具有通用形式。然而，在富有特征学习的区域，核和预测的波动是动态耦合且方差可以被自洽计算。在两层网络中，我们展示了特征学习如何动态地减少最终NTK和最终网络预测的方差。我们还展示了如何进行初始化。

    We analyze the dynamics of finite width effects in wide but finite feature learning neural networks. Unlike many prior analyses, our results, while perturbative in width, are non-perturbative in the strength of feature learning. Starting from a dynamical mean field theory (DMFT) description of infinite width deep neural network kernel and prediction dynamics, we provide a characterization of the $\mathcal{O}(1/\sqrt{\text{width}})$ fluctuations of the DMFT order parameters over random initialization of the network weights. In the lazy limit of network training, all kernels are random but static in time and the prediction variance has a universal form. However, in the rich, feature learning regime, the fluctuations of the kernels and predictions are dynamically coupled with variance that can be computed self-consistently. In two layer networks, we show how feature learning can dynamically reduce the variance of the final NTK and final network predictions. We also show how initialization
    
[^54]: 基于局部区域对比的医学图像自监督学习增强技术

    Localized Region Contrast for Enhancing Self-Supervised Learning in Medical Image Segmentation. (arXiv:2304.03406v1 [cs.CV])

    [http://arxiv.org/abs/2304.03406](http://arxiv.org/abs/2304.03406)

    本文提出了一种基于局部区域对比的医学图像自监督学习增强框架，以提高多器官分割等密集预测任务的性能，实验结果表明该方法可以超越最先进的自监督学习方法。

    

    最近自监督学习取得了很大的进展，表明可以从无标签图像中学习有效的视觉表示。这导致人们对将自监督学习应用于医学领域的兴趣增加了，因为无标签图像丰富而有标签图像很难获得。然而，大多数自监督学习方法被建模为图像级别的判别或生成代理任务，这可能无法捕捉多器官分割等密集预测任务所需的更细致级别的表示。本文提出了一种新颖的对比学习框架，它集成了局部区域对比（LRC）以增强现有的医学图像分割自监督预训练方法。我们的方法涉及使用Felzenszwalb算法识别超像素，并使用新的对比采样损失进行局部对比学习。通过在三个多器官分割数据集上进行广泛实验，我们证明了我们的方法可以提高分割性能并超越最先进的自监督学习方法。

    Recent advancements in self-supervised learning have demonstrated that effective visual representations can be learned from unlabeled images. This has led to increased interest in applying self-supervised learning to the medical domain, where unlabeled images are abundant and labeled images are difficult to obtain. However, most self-supervised learning approaches are modeled as image level discriminative or generative proxy tasks, which may not capture the finer level representations necessary for dense prediction tasks like multi-organ segmentation. In this paper, we propose a novel contrastive learning framework that integrates Localized Region Contrast (LRC) to enhance existing self-supervised pre-training methods for medical image segmentation. Our approach involves identifying Super-pixels by Felzenszwalb's algorithm and performing local contrastive learning using a novel contrastive sampling loss. Through extensive experiments on three multi-organ segmentation datasets, we demon
    
[^55]: 量子相容预测用于量子机器学习中的可靠不确定性量化

    Quantum Conformal Prediction for Reliable Uncertainty Quantification in Quantum Machine Learning. (arXiv:2304.03398v1 [quant-ph])

    [http://arxiv.org/abs/2304.03398](http://arxiv.org/abs/2304.03398)

    本文提出了一种通用方法，可以可靠地量化量子模型的不确定性，无论训练数据的数量、拍摄次数、ansatz、训练算法以及量子硬件噪声的存在如何。

    

    量子机器学习是在当前的噪声中间规模量子(NISQ)计算机时代中优化量子算法的有前途的编程范式。量子机器学习中的一个基本挑战是泛化性能，因为设计者的目标是在测试条件下获得良好的性能，但只能访问有限的训练数据。现有的泛化分析虽然能够识别重要的一般趋势和规模定律，但不能用于为量子模型所作出的决策分配可靠和有信息量的“误差条”。在本文中，我们提出了一种通用方法，可以可靠地量化量子模型的不确定性，无论训练数据的数量、拍摄次数、ansatz、训练算法以及量子硬件噪声的存在如何，在概率性相容预测的基础上构建方法，可以将预先训练的量子模型的任意可能小的拍摄次数转换为一组预测。

    Quantum machine learning is a promising programming paradigm for the optimization of quantum algorithms in the current era of noisy intermediate scale quantum (NISQ) computers. A fundamental challenge in quantum machine learning is generalization, as the designer targets performance under testing conditions, while having access only to limited training data. Existing generalization analyses, while identifying important general trends and scaling laws, cannot be used to assign reliable and informative "error bars" to the decisions made by quantum models. In this article, we propose a general methodology that can reliably quantify the uncertainty of quantum models, irrespective of the amount of training data, of the number of shots, of the ansatz, of the training algorithm, and of the presence of quantum hardware noise. The approach, which builds on probabilistic conformal prediction, turns an arbitrary, possibly small, number of shots from a pre-trained quantum model into a set predicti
    
[^56]: 深度学习应用于课程评论的观点挖掘和主题分类

    Deep Learning for Opinion Mining and Topic Classification of Course Reviews. (arXiv:2304.03394v1 [cs.CL])

    [http://arxiv.org/abs/2304.03394](http://arxiv.org/abs/2304.03394)

    本文利用自然语言处理和深度学习技术，通过比较传统方法和现代机器学习方法，展示了如何处理大量课程评论，进行情感极性分析和主题分类。

    

    对于教育工作者和管理者来说，学生对课程的反馈意见非常重要，无论课程的类型或机构如何。在机构级别或在线论坛上处理大量的开放反馈变得不可行。在本文中，我们收集和预处理了大量公开可用的课程评论。我们应用机器学习技术，目的是了解学生的情感和主题。具体而言，我们利用了当前的自然语言处理技术，如词嵌入和深度神经网络，并采用最先进的BERT（双向编码器表示来自变压器）、RoBERTa（经过优化的BERT方法）和XLNet（广义自回归预训练）技术。我们进行了广泛的实验，比较了这些技术与传统方法的差异。这项比较研究展示了如何应用现代机器学习方法进行情感极性分析和主题分类。

    Student opinions for a course are important to educators and administrators, regardless of the type of the course or the institution. Reading and manually analyzing open-ended feedback becomes infeasible for massive volumes of comments at institution level or online forums. In this paper, we collected and pre-processed a large number of course reviews publicly available online. We applied machine learning techniques with the goal to gain insight into student sentiments and topics. Specifically, we utilized current Natural Language Processing (NLP) techniques, such as word embeddings and deep neural networks, and state-of-the-art BERT (Bidirectional Encoder Representations from Transformers), RoBERTa (Robustly optimized BERT approach) and XLNet (Generalized Auto-regression Pre-training). We performed extensive experimentation to compare these techniques versus traditional approaches. This comparative study demonstrates how to apply modern machine learning approaches for sentiment polari
    
[^57]: 应用机器学习和领域知识个性化数字健康行为变革干预

    Personalizing Digital Health Behavior Change Interventions using Machine Learning and Domain Knowledge. (arXiv:2304.03392v1 [cs.LG])

    [http://arxiv.org/abs/2304.03392](http://arxiv.org/abs/2304.03392)

    该论文提出了一种采用机器学习和领域知识进行个性化数字健康行为变革干预的系统，其利用反事实例子进行特征控制以预测干预效果并优化干预效果。

    

    我们正在开发一种虚拟教练系统，帮助患者坚持行为变革干预（BCI）。我们的系统预测患者是否会执行目标行为，并使用反事实例子进行特征控制，以指导个性化BCI。我们使用具有不同响应水平的模拟患者数据评估了我们的预测模型。

    We are developing a virtual coaching system that helps patients adhere to behavior change interventions (BCI). Our proposed system predicts whether a patient will perform the targeted behavior and uses counterfactual examples with feature control to guide personalizsation of BCI. We evaluated our prediction model using simulated patient data with varying levels of receptivity to intervention.
    
[^58]: EZClone：通过GPU执行文件的形状精炼提高DNN模型提取攻击

    EZClone: Improving DNN Model Extraction Attack via Shape Distillation from GPU Execution Profiles. (arXiv:2304.03388v1 [cs.LG])

    [http://arxiv.org/abs/2304.03388](http://arxiv.org/abs/2304.03388)

    本论文介绍了两种不同威胁模型下的DNN结构提取技术，其中EZClone利用聚合GPU文件作为侧信道来预测DNN结构，并且通过实验验证了其有效性。

    

    由于在预测和分类问题上表现出色，深度神经网络（DNN）已经变得无处不在。然而，随着它们的使用扩展，它们面临各种威胁。模型提取攻击窃取DNN会危及知识产权、数据隐私和安全。先前的研究表明，系统级侧信道可用于通过暴露受害者DNN的体系结构来泄露模型的细节，从而加剧这些风险。我们提出了两种针对不同威胁模型的DNN结构提取技术。第一种技术使用恶意的、动态链接的PyTorch版本，在通过PyTorch分析器暴露受害者DNN结构。第二种技术称为EZClone，利用聚合（而不是时间序列）GPU文件作为侧信道来预测DNN结构，使用简单的方法，假设攻击者的能力比先前的研究低。我们在最小化攻击复杂性的情况下调查了EZClone的有效性，并在多种模型和数据集上进行了实验。

    Deep Neural Networks (DNNs) have become ubiquitous due to their performance on prediction and classification problems. However, they face a variety of threats as their usage spreads. Model extraction attacks, which steal DNNs, endanger intellectual property, data privacy, and security. Previous research has shown that system-level side-channels can be used to leak the architecture of a victim DNN, exacerbating these risks. We propose two DNN architecture extraction techniques catering to various threat models. The first technique uses a malicious, dynamically linked version of PyTorch to expose a victim DNN architecture through the PyTorch profiler. The second, called EZClone, exploits aggregate (rather than time-series) GPU profiles as a side-channel to predict DNN architecture, employing a simple approach and assuming little adversary capability as compared to previous work. We investigate the effectiveness of EZClone when minimizing the complexity of the attack, when applied to prun
    
[^59]: 宽神经网络：从初始化的非高斯随机场到训练中的NTK几何（arXiv:2304.03385v1 [cs.LG]）

    Wide neural networks: From non-gaussian random fields at initialization to the NTK geometry of training. (arXiv:2304.03385v1 [cs.LG])

    [http://arxiv.org/abs/2304.03385](http://arxiv.org/abs/2304.03385)

    本研究研究大规模但有限的神经网络行为。主要贡献为：（1）计算高斯性的修正，系数由参数初始化和激活函数的统计学确定。（2）通过计算网络与极限情况下的偏差来控制网络在训练时的输出，具有更好的效果。

    

    近期神经网络应用中参数达到$n=10^{14}$，因此研究此类网络的大规模行为变得极为重要。此前的大部分研究都聚焦于宽神经网络的宽度无限大，即$n \to +\infty$时的极限情况，表明它们在初始化时符合高斯过程。本研究将研究大但有限规模的神经网络的行为。我们的主要贡献为：（1）计算以$n^{-\frac{1}{2}}$为渐近级数的高斯性修正，该展开式的系数由参数初始化和激活函数的统计学确定。(2) 通过计算有限宽度$n$网络与极限情况下（在该情况下网络通过线性流演化）的偏差来控制网络在训练时的输出，具有更好的效果。这提高了以前的估计，得到了更好的衰减率。

    Recent developments in applications of artificial neural networks with over $n=10^{14}$ parameters make it extremely important to study the large $n$ behaviour of such networks. Most works studying wide neural networks have focused on the infinite width $n \to +\infty$ limit of such networks and have shown that, at initialization, they correspond to Gaussian processes. In this work we will study their behavior for large, but finite $n$. Our main contributions are the following:  (1) The computation of the corrections to Gaussianity in terms of an asymptotic series in $n^{-\frac{1}{2}}$. The coefficients in this expansion are determined by the statistics of parameter initialization and by the activation function.  (2) Controlling the evolution of the outputs of finite width $n$ networks, during training, by computing deviations from the limiting infinite width case (in which the network evolves through a linear flow). This improves previous estimates and yields sharper decay rates for t
    
[^60]: 基于分数匹配的可扩展因果推断

    Scalable Causal Discovery with Score Matching. (arXiv:2304.03382v1 [cs.LG])

    [http://arxiv.org/abs/2304.03382](http://arxiv.org/abs/2304.03382)

    该论文提出了一种利用分数匹配算法实现可扩展因果推断的方法，该算法可从非线性可加性高斯噪声模型的对数似然函数中发现整个因果图，并通过实现与当前最先进技术相当的准确性来降低了计算门槛。

    

    本文展示了如何在非线性可加性高斯噪声模型中利用对数似然函数的二阶导数来发现整个因果图。借助于可扩展的机器学习方法来逼近分数函数 $\nabla \log p(\mathbf{X})$，我们扩展了Rolland等人（2022）的工作，后者仅从分数中恢复拓扑顺序，并需要一个昂贵的修剪步骤来消除由此顺序允许的虚假边缘。我们的分析导致了DAS（即 Discovery At Scale，规模化发现）算法，它通过与图形大小成比例的因素减少修剪的复杂性。在实践中，DAS实现了与当前最先进技术相当的准确性，同时速度提升了一个数量级以上。总的来说，我们的方法实现了原则性和可扩展的因果推断，大大降低了计算门槛。

    This paper demonstrates how to discover the whole causal graph from the second derivative of the log-likelihood in observational non-linear additive Gaussian noise models. Leveraging scalable machine learning approaches to approximate the score function $\nabla \log p(\mathbf{X})$, we extend the work of Rolland et al. (2022) that only recovers the topological order from the score and requires an expensive pruning step removing spurious edges among those admitted by the ordering. Our analysis leads to DAS (acronym for Discovery At Scale), a practical algorithm that reduces the complexity of the pruning by a factor proportional to the graph size. In practice, DAS achieves competitive accuracy with current state-of-the-art while being over an order of magnitude faster. Overall, our approach enables principled and scalable causal discovery, significantly lowering the compute bar.
    
[^61]: 自监督视频相似性学习

    Self-Supervised Video Similarity Learning. (arXiv:2304.03378v1 [cs.CV])

    [http://arxiv.org/abs/2304.03378](http://arxiv.org/abs/2304.03378)

    本文提出了自监督视频相似性学习的方法S$^2$VS，该方法通过学习实例区分解决多个检索和检测任务，无需用到标注数据，并在各个任务上都达到了最新的性能。

    

    本文介绍了一种基于自监督学习的视频相似性学习方法S$^2$VS。与以往研究不同，这种方法使用自监督学习来实现视频相似性学习，并一次性解决多个检索和检测任务，而不需要使用带标签的数据。通过使用任务定制的增强和InfoNCE损失函数以及在自我相似性和硬负相似性上同时操作的附加损失函数，通过学习实例区分来实现。我们的方法在不同粒度下定义视频相关性的任务上进行了基准测试，涵盖了复制视频到描述相同事件的视频。我们学习了一个单一的通用模型，能够在所有任务上获得最新的表现，超越了以前使用标记数据的方法。代码和预训练模型是公开可用的。

    We introduce S$^2$VS, a video similarity learning approach with self-supervision. Self-Supervised Learning (SSL) is typically used to train deep models on a proxy task so as to have strong transferability on target tasks after fine-tuning. Here, in contrast to prior work, SSL is used to perform video similarity learning and address multiple retrieval and detection tasks at once with no use of labeled data. This is achieved by learning via instance-discrimination with task-tailored augmentations and the widely used InfoNCE loss together with an additional loss operating jointly on self-similarity and hard-negative similarity. We benchmark our method on tasks where video relevance is defined with varying granularity, ranging from video copies to videos depicting the same incident or event. We learn a single universal model that achieves state-of-the-art performance on all tasks, surpassing previously proposed methods that use labeled data. The code and pretrained models are publicly avai
    
[^62]: 神经群体动态和几何的可解释统计表示

    Interpretable statistical representations of neural population dynamics and geometry. (arXiv:2304.03376v1 [cs.LG])

    [http://arxiv.org/abs/2304.03376](http://arxiv.org/abs/2304.03376)

    该论文提出了一种基于统计分布的几何深度学习框架，用于表示非线性动态系统的几何感知或几何无感知表示，以对已测量轨迹进行无偏比较。利用该方法，能够解释神经动力学的嵌入，在灵长类似任务中取得了最先进的准确性。

    

    在各种任务中，神经元群体的动态通常在低维流形上演化。然而，区分几何和动态对编码相关行为变量的贡献仍然具有挑战性。在这里，我们引入了一种基于局部相轨特征的统计分布的非线性动态系统的几何深度学习框架，用于表示。我们的方法提供了对几何感知或几何无感知表示，以对已测量轨迹进行无偏比较。我们证明，我们的统计表示可以横跨神经网络实例进行推广，以区分计算机制，在具有几何对应的灵长类似任务中解释嵌入神经动力学，并开发具有最先进准确性的解码算法。我们的结果强调了使用内在流形结构优于时间信息的重要性。

    The dynamics of neuron populations during diverse tasks often evolve on low-dimensional manifolds. However, it remains challenging to discern the contributions of geometry and dynamics for encoding relevant behavioural variables. Here, we introduce an unsupervised geometric deep learning framework for representing non-linear dynamical systems based on statistical distributions of local phase portrait features. Our method provides robust geometry-aware or geometry-agnostic representations for the unbiased comparison of dynamics based on measured trajectories. We demonstrate that our statistical representation can generalise across neural network instances to discriminate computational mechanisms, obtain interpretable embeddings of neural dynamics in a primate reaching task with geometric correspondence to hand kinematics, and develop a decoding algorithm with state-of-the-art accuracy. Our results highlight the importance of using the intrinsic manifold structure over temporal informati
    
[^63]: 通过激活函数发现和自动权重初始化优化神经网络

    Optimizing Neural Networks through Activation Function Discovery and Automatic Weight Initialization. (arXiv:2304.03374v1 [cs.LG])

    [http://arxiv.org/abs/2304.03374](http://arxiv.org/abs/2304.03374)

    本文介绍了发现更强大的激活函数和建立更稳健的神经网络权重初始化技术的方法，这些方法比传统方法更优秀，同时提供了神经网络优化的新视角。

    

    自动机器学习（AutoML）方法通过优化已有模型的各个方面来改进。当前方法侧重于超参数和神经网络拓扑，但神经网络设计的其他方面也可以进行优化。为了进一步提高AutoML的现状，本篇论文介绍了发现更强大的激活函数和建立更稳健的神经网络权重初始化技术。这些贡献不仅提高了性能，而且提供了神经网络优化的新视角。首先，本文表明发现针对特定架构和任务的解决方案比重复使用通用方法性能更好。其次，它表明联合优化神经网络的不同组件是协同的，比单独优化组件的性能更好。第三，它证明了学习表示更容易优化。

    Automated machine learning (AutoML) methods improve upon existing models by optimizing various aspects of their design. While present methods focus on hyperparameters and neural network topologies, other aspects of neural network design can be optimized as well. To further the state of the art in AutoML, this dissertation introduces techniques for discovering more powerful activation functions and establishing more robust weight initialization for neural networks. These contributions improve performance, but also provide new perspectives on neural network optimization. First, the dissertation demonstrates that discovering solutions specialized to specific architectures and tasks gives better performance than reusing general approaches. Second, it shows that jointly optimizing different components of neural networks is synergistic, and results in better performance than optimizing individual components alone. Third, it demonstrates that learned representations are easier to optimize tha
    
[^64]: 可靠的学习方法应对测试时攻击与分布偏移

    Reliable Learning for Test-time Attacks and Distribution Shift. (arXiv:2304.03370v1 [cs.LG])

    [http://arxiv.org/abs/2304.03370](http://arxiv.org/abs/2304.03370)

    本文提出了可靠的学习方法以抵御测试时攻击和分布偏移，在测试时引入了新的可靠性保障方法，确保预测结果正确。同时，该学习方法能够适应任意测试点，具有非常好的可靠性。

    

    机器学习算法经常被用于即使经过精心获得的训练数据也无法准确捕捉的环境中，这既可能是由于测试时的“对抗性”攻击，也可能是因为“自然”的数据分布偏移。针对测试时攻击，我们提出并分析一种新颖的稳健性可靠性保证方法，要求学习器输出一个可靠半径 $\eta$ 的预测结果，意味着只要对手没有扰动测试点超过距离 $\eta$，它的预测结果就是正确的。我们提供了在任意测试点上都能输出最佳可靠性半径的最优学习器，并且特征化了可靠区域即可达到给定可靠性半径的点集。我们还分析了在分布偏移下的可靠学习方法，其中测试点可能来自于一个与训练分布不同的任意分布 $Q$。

    Machine learning algorithms are often used in environments which are not captured accurately even by the most carefully obtained training data, either due to the possibility of `adversarial' test-time attacks, or on account of `natural' distribution shift. For test-time attacks, we introduce and analyze a novel robust reliability guarantee, which requires a learner to output predictions along with a reliability radius $\eta$, with the meaning that its prediction is guaranteed to be correct as long as the adversary has not perturbed the test point farther than a distance $\eta$. We provide learners that are optimal in the sense that they always output the best possible reliability radius on any test point, and we characterize the reliable region, i.e. the set of points where a given reliability radius is attainable. We additionally analyze reliable learners under distribution shift, where the test points may come from an arbitrary distribution Q different from the training distribution 
    
[^65]: 从解释到行动：一种端到端的人机协作异常推理和管理框架

    From Explanation to Action: An End-to-End Human-in-the-loop Framework for Anomaly Reasoning and Management. (arXiv:2304.03368v1 [cs.LG])

    [http://arxiv.org/abs/2304.03368](http://arxiv.org/abs/2304.03368)

    本文提出了一种名为ALARM的端到端框架，支持从异常检测到人机交互式处理，从而最终实现新规则补充了基于规则的监督检测。

    

    异常通常是制造、医疗、金融、监控等各种系统中故障或低效的指标。尽管由于其实际意义而文献中存在着有效的检测算法，但自动异常检测在实际应用中很少使用。特别是在高风险应用中，通常需要人机协作处理，超出了检测等进程，例如验证和故障排除。本文介绍了ALARM（分析师协作异常推理和管理）：一种端到端的框架，全面支持异常挖掘周期，从检测到行动。除了针对新兴异常的无监督检测，它还提供异常解释和交互式GUI，以进行人机协作进程，包括可视化探索、感知和最终通过设计新的检测规则进行行动，从而帮助关闭“环路”，新规则补充了基于规则的监督检测。

    Anomalies are often indicators of malfunction or inefficiency in various systems such as manufacturing, healthcare, finance, surveillance, to name a few. While the literature is abundant in effective detection algorithms due to this practical relevance, autonomous anomaly detection is rarely used in real-world scenarios. Especially in high-stakes applications, a human-in-the-loop is often involved in processes beyond detection such as verification and troubleshooting. In this work, we introduce ALARM (for Analyst-in-the-Loop Anomaly Reasoning and Management); an end-to-end framework that supports the anomaly mining cycle comprehensively, from detection to action. Besides unsupervised detection of emerging anomalies, it offers anomaly explanations and an interactive GUI for human-in-the-loop processes -- visual exploration, sense-making, and ultimately action-taking via designing new detection rules -- that help close ``the loop'' as the new rules complement rule-based supervised detect
    
[^66]: 奖励转移的稳健决策重点学习

    Robust Decision-Focused Learning for Reward Transfer. (arXiv:2304.03365v1 [cs.LG])

    [http://arxiv.org/abs/2304.03365](http://arxiv.org/abs/2304.03365)

    本文介绍了一种稳健决策重点（RDF）算法，利用非识别性的DF解，学习同时最大化期望回报和抵御奖励函数变化的模型，可以显著提高DF对奖励函数变化的稳健性，而不会降低智能体的总回报。

    

    最近，决策重点（Decision-focused，DF）的基于模型的强化学习被介绍为一种强有力的算法，它可以专注于学习最有利于获得高报酬的MDP动态。虽然这种方法通过专注于直接优化报酬来提高智能体的性能，但从MLE的角度来看，它学习的动力学不够准确，因此可能对奖励函数的变化很脆弱。在这项工作中，我们开发了稳健决策重点（RDF）算法，它利用DF解的非识别性，学习同时最大化期望回报和抵御奖励函数变化的模型。我们在各种玩具示例和医疗模拟器上展示了RDF显着增加了DF对奖励函数变化的稳健性，而不会降低智能体的总回报。

    Decision-focused (DF) model-based reinforcement learning has recently been introduced as a powerful algorithm which can focus on learning the MDP dynamics which are most relevant for obtaining high rewards. While this approach increases the performance of agents by focusing the learning towards optimizing for the reward directly, it does so by learning less accurate dynamics (from a MLE standpoint), and may thus be brittle to changes in the reward function. In this work, we develop the robust decision-focused (RDF) algorithm which leverages the non-identifiability of DF solutions to learn models which maximize expected returns while simultaneously learning models which are robust to changes in the reward function. We demonstrate on a variety of toy example and healthcare simulators that RDF significantly increases the robustness of DF to changes in the reward function, without decreasing the overall return the agent obtains.
    
[^67]: 小数据量下的NMR位移预测

    NMR shift prediction from small data quantities. (arXiv:2304.03361v1 [physics.chem-ph])

    [http://arxiv.org/abs/2304.03361](http://arxiv.org/abs/2304.03361)

    提出了一种新的机器学习模型，能够以相对较少的数据量预测NMR化学位移，展示了在特定溶剂中预测小分子的19F和13C NMR化学位移方面的良好效果。

    

    使用机器学习方法进行NMR化学位移预测通常需要使用尽可能多的数据以获得最佳结果。但是在某些情况下，如异核核素，很难得到大量数据。我们展示了一种新颖的机器学习模型，该模型能够以相对较少的数据量取得良好的结果。我们通过预测特定溶剂中小分子的19F和13C NMR化学位移来展示这一点。

    Prediction of chemical shift in NMR using machine learning methods is typically done with the maximum amount of data available to achieve the best results. In some cases, such large amounts of data are not available, e.g. for heteronuclei. We demonstrate a novel machine learning model which is able to achieve good results with comparatively low amounts of data. We show this by predicting 19F and 13C NMR chemical shifts of small molecules in specific solvents.
    
[^68]: 推荐系统的图协作信号去噪与增强

    Graph Collaborative Signals Denoising and Augmentation for Recommendation. (arXiv:2304.03344v1 [cs.IR])

    [http://arxiv.org/abs/2304.03344](http://arxiv.org/abs/2304.03344)

    本文提出了一种新的图邻接矩阵，它包括了用户-用户和项目-项目的相关性，以及一个经过适当设计的用户-项目交互矩阵，并通过预训练和top-K采样增强了用户-项目交互矩阵，以更好地适应所有用户的需求。

    

    图协作过滤（GCF）是捕捉推荐系统中高阶协同信号的流行技术。然而，GCF的双向邻接矩阵，其定义了基于用户-项目交互进行聚合的邻居，对于有大量交互但不足的用户/项目来说可能是嘈杂的。此外，邻接矩阵忽略了用户-用户和项目-项目之间的相关性，这可能限制了聚合的有益邻居的范围。在这项工作中，我们提出了一种新的图邻接矩阵，它包括了用户-用户和项目-项目的相关性，以及一个经过适当设计的用户-项目交互矩阵，以平衡所有用户之间的交互数量。为了实现这一点，我们预先训练了一个基于图的推荐方法来获得用户/项目嵌入，然后通过top-K采样增强了用户-项目交互矩阵。我们还增强了对称的用户-用户和项目-项目相关组件，以更好地适应所有用户的需求。

    Graph collaborative filtering (GCF) is a popular technique for capturing high-order collaborative signals in recommendation systems. However, GCF's bipartite adjacency matrix, which defines the neighbors being aggregated based on user-item interactions, can be noisy for users/items with abundant interactions and insufficient for users/items with scarce interactions. Additionally, the adjacency matrix ignores user-user and item-item correlations, which can limit the scope of beneficial neighbors being aggregated.  In this work, we propose a new graph adjacency matrix that incorporates user-user and item-item correlations, as well as a properly designed user-item interaction matrix that balances the number of interactions across all users. To achieve this, we pre-train a graph-based recommendation method to obtain users/items embeddings, and then enhance the user-item interaction matrix via top-K sampling. We also augment the symmetric user-user and item-item correlation components to th
    
[^69]: 自主预测和长期家庭能源负荷预测的自旋电子物理水库

    Spintronic Physical Reservoir for Autonomous Prediction and Long-Term Household Energy Load Forecasting. (arXiv:2304.03343v1 [cs.LG])

    [http://arxiv.org/abs/2304.03343](http://arxiv.org/abs/2304.03343)

    本研究展示了利用自旋电子物理水库进行自治型长期预测任务的方法，可以用于建模混沌时间序列和动态时间序列数据，是适合在边缘设备上进行实时学习的。这里提出的基于微旋磁隧穿结的涡旋子可以作为实现此种RC的原型。

    

    本研究利用自旋电子物理水库进行了自治型长期预测。由于磁化动力学的短期记忆特性，水库状态中产生了非线性，可用于使用简单线性回归进行在线训练的长期预测任务。在预测阶段，输出直接馈入水库的输入中进行自治型预测。我们将所提出的水库用于建模诸如Mackey-Glass等混沌时间序列和动态时间序列数据，如家庭建筑能耗。由于只有RC的最后一层需要使用线性回归进行训练，因此它非常适合在边缘设备上进行实时学习。本文展示了基于微旋磁隧穿结的涡旋子可能用作原型RC，但任何具有非线性磁化行为的纳米磁隧道结都可以实现此种RC。通过比较我们的自旋电子物理RC方法

    In this study, we have shown autonomous long-term prediction with a spintronic physical reservoir. Due to the short-term memory property of the magnetization dynamics, non-linearity arises in the reservoir states which could be used for long-term prediction tasks using simple linear regression for online training. During the prediction stage, the output is directly fed to the input of the reservoir for autonomous prediction. We employ our proposed reservoir for the modeling of the chaotic time series such as Mackey-Glass and dynamic time-series data, such as household building energy loads. Since only the last layer of a RC needs to be trained with linear regression, it is well suited for learning in real time on edge devices. Here we show that a skyrmion based magnetic tunnel junction can potentially be used as a prototypical RC but any nanomagnetic magnetic tunnel junction with nonlinear magnetization behavior can implement such a RC. By comparing our spintronic physical RC approach 
    
[^70]: 最大序数二次因子分解

    Maximal Ordinal Two-Factorizations. (arXiv:2304.03338v1 [cs.AI])

    [http://arxiv.org/abs/2304.03338](http://arxiv.org/abs/2304.03338)

    本文研究了最大序数二次因子分解问题，证明了其判定是否存在是一个NP完全问题，并提供了用于计算最大因子分解的算法Ord2Factor。

    

    在一个形式背景中，序数因子是其关系的子集，形成概念格中的链，即对应于线性顺序的数据集的一部分。为了可视化形式上下文中的数据，Ganter和Glodeanu提出了基于两个序数因子的双图。为了使双图有用，重要的是这些因子尽可能包含更多数据点，即覆盖尽可能多的关系。本文研究这样的序数二次因子分解。首先，我们研究了省略序数二次因子分解的形式背景中两个因子的不相交性。然后，我们证明判定给定大小的二次因子分解是否存在是一个NP完全问题，这使得计算最大因子分解具有计算成本。最后，我们提供了算法Ord2Factor，它允许我们计算大的序数二次因子分解。

    Given a formal context, an ordinal factor is a subset of its incidence relation that forms a chain in the concept lattice, i.e., a part of the dataset that corresponds to a linear order. To visualize the data in a formal context, Ganter and Glodeanu proposed a biplot based on two ordinal factors. For the biplot to be useful, it is important that these factors comprise as much data points as possible, i.e., that they cover a large part of the incidence relation. In this work, we investigate such ordinal two-factorizations. First, we investigate for formal contexts that omit ordinal two-factorizations the disjointness of the two factors. Then, we show that deciding on the existence of two-factorizations of a given size is an NP-complete problem which makes computing maximal factorizations computationally expensive. Finally, we provide the algorithm Ord2Factor that allows us to compute large ordinal two-factorizations.
    
[^71]: 关于多标签排名的可学习性研究

    On the Learnability of Multilabel Ranking. (arXiv:2304.03337v1 [cs.LG])

    [http://arxiv.org/abs/2304.03337](http://arxiv.org/abs/2304.03337)

    研究了一系列排名损失函数下多标签排名问题在批处理和在线设置下的可学习性，并首次给出基于可学习性的排名损失函数的等价类。

    

    在机器学习中，多标签排名是一项重要任务，广泛应用于网络搜索、新闻报道、推荐系统等领域。但是，关于多标签排名设置中可学习性的最基本问题仍未解答。本文研究了一系列排名损失函数下多标签排名问题在批处理和在线设置下的可学习性，同时也首次给出了基于可学习性的排名损失函数的等价类。

    Multilabel ranking is a central task in machine learning with widespread applications to web search, news stories, recommender systems, etc. However, the most fundamental question of learnability in a multilabel ranking setting remains unanswered. In this paper, we characterize the learnability of multilabel ranking problems in both the batch and online settings for a large family of ranking losses. Along the way, we also give the first equivalence class of ranking losses based on learnability.
    
[^72]: ChatGPT-Crawler：发现ChatGPT是否真的知道自己在说什么。（arXiv:2304.03325v1 [cs.CL]）

    ChatGPT-Crawler: Find out if ChatGPT really knows what it's talking about. (arXiv:2304.03325v1 [cs.CL])

    [http://arxiv.org/abs/2304.03325](http://arxiv.org/abs/2304.03325)

    本文分析了从不同对话QA语料库中生成的ChatGPT的响应，并比较了其与正确答案的相似度。研究发现ChatGPT在某些情况下提供了错误的答案，提供了潜在用户和开发者的宝贵见解。

    

    大型语言模型因其在各种任务上的出色表现而引起了人们的极大兴趣。其中，OpenAI开发的ChatGPT已经成为早期采用者中非常流行的模型，他们甚至将其视为客户服务、教育、医疗和金融等许多领域的破坏性技术。理解这些初期用户的观点非常重要，因为它可以为不同领域技术的潜在优势、劣势、成功或失败提供有价值的洞见。本研究考察了ChatGPT从不同对话QA语料库中生成的响应。研究使用BERT相似度分数将这些响应与正确答案进行比较，并获得自然语言推理（NLI）标签。还计算并比较了评估分数，以确定GPT-3＆GPT-4的整体性能。此外，该研究还确定了ChatGPT提供错误答案的情况，为相关领域提供了洞见。

    Large language models have gained considerable interest for their impressive performance on various tasks. Among these models, ChatGPT developed by OpenAI has become extremely popular among early adopters who even regard it as a disruptive technology in many fields like customer service, education, healthcare, and finance. It is essential to comprehend the opinions of these initial users as it can provide valuable insights into the potential strengths, weaknesses, and success or failure of the technology in different areas. This research examines the responses generated by ChatGPT from different Conversational QA corpora. The study employed BERT similarity scores to compare these responses with correct answers and obtain Natural Language Inference(NLI) labels. Evaluation scores were also computed and compared to determine the overall performance of GPT-3 \& GPT-4. Additionally, the study identified instances where ChatGPT provided incorrect answers to questions, providing insights into
    
[^73]: 基于去噪扩散隐式模型的连续图像修复

    Towards Coherent Image Inpainting Using Denoising Diffusion Implicit Models. (arXiv:2304.03322v1 [cs.CV])

    [http://arxiv.org/abs/2304.03322](http://arxiv.org/abs/2304.03322)

    本研究提出了一种基于Bayesian框架和去噪扩散隐式模型的方法CO-PAINT，用于解决图像修复中修复区域和未修复区域不协调问题，实现了最先进的性能表现。

    

    图像修复是指在基于部分可见的参考图像的情况下生成完整、自然的图像任务。然而，直接使用固定的扩散模型进行修复往往会导致修复和未修复的区域不协调。为此，本研究提出了CO-PAINT算法，使用Bayesian框架来同时修改图像中的修复和未修复区域，并采用扩散隐式模型估计像素的后验分布。为了提高后验估计的效率，我们采取一种去噪方法来去除在不同扩散阶段产生的中间图像噪声。在各种基准数据集上进行的实验结果表明，我们的方法可以在视觉质量和定量结果方面达到最先进的性能。

    Image inpainting refers to the task of generating a complete, natural image based on a partially revealed reference image. Recently, many research interests have been focused on addressing this problem using fixed diffusion models. These approaches typically directly replace the revealed region of the intermediate or final generated images with that of the reference image or its variants. However, since the unrevealed regions are not directly modified to match the context, it results in incoherence between revealed and unrevealed regions. To address the incoherence problem, a small number of methods introduce a rigorous Bayesian framework, but they tend to introduce mismatches between the generated and the reference images due to the approximation errors in computing the posterior distributions. In this paper, we propose COPAINT, which can coherently inpaint the whole image without introducing mismatches. COPAINT also uses the Bayesian framework to jointly modify both revealed and unre
    
[^74]: 带约束和依赖损失的自适应决策：性能保证及其在在线和非线性识别中的应用。

    Adaptive Decision-Making with Constraints and Dependent Losses: Performance Guarantees and Applications to Online and Nonlinear Identification. (arXiv:2304.03321v1 [cs.LG])

    [http://arxiv.org/abs/2304.03321](http://arxiv.org/abs/2304.03321)

    本文提出了一种带约束和依赖损失的自适应决策算法，并证明了其有效性。该算法可用于在线和非线性识别等任务。

    

    我们考虑自适应决策问题，其中代理通过重复选择有限的选项来优化累积性能目标。相对于经典的预测与专家建议，我们考虑损失受约束的情况，并推导利用优化和计算上的有效方式的算法。我们的算法和分析是实例相关的，即利用环境的次优选择，并将其反映在我们的遗憾上限中。约束处理损失之间的一般依赖关系（甚至跨时间），并且足够灵活，可用于考虑损失预算，环境不允许超过该预算。在两个数值示例中突出了所得算法的性能，其中包括非线性和在线系统识别任务。

    We consider adaptive decision-making problems where an agent optimizes a cumulative performance objective by repeatedly choosing among a finite set of options. Compared to the classical prediction-with-expert-advice set-up, we consider situations where losses are constrained and derive algorithms that exploit the additional structure in optimal and computationally efficient ways. Our algorithm and our analysis is instance dependent, that is, suboptimal choices of the environment are exploited and reflected in our regret bounds. The constraints handle general dependencies between losses (even across time), and are flexible enough to also account for a loss budget, which the environment is not allowed to exceed. The performance of the resulting algorithms is highlighted in two numerical examples, which include a nonlinear and online system identification task.
    
[^75]: 超声断层成像反演的神经操作符学习

    Neural Operator Learning for Ultrasound Tomography Inversion. (arXiv:2304.03297v1 [eess.IV])

    [http://arxiv.org/abs/2304.03297](http://arxiv.org/abs/2304.03297)

    本文首次将神经操作符学习应用于超声断层成像反演，通过学习时间飞行数据和异质声速场之间的映射，实现了可避免计算密集型反演过程的预测异质声场模型。该模型有潜在的在乳腺成像中进行软组织分布预测和肿瘤识别的实时应用。

    

    神经操作符学习作为在计算科学和工程领域中进行复杂函数空间映射的一种手段已经引起了重视。在本文中，我们将神经操作符学习应用于飞行时间超声计算层析成像问题。我们使用全波求解器生成训练数据来学习时间飞行（TOF）数据和异质声速场之间的映射。该操作符学习的新颖应用规避了需要解决计算密集型迭代反问题的需求。该操作符学习在离线模式下学习非线性映射，并通过模型的单次前向通过来预测异质声场。这是第一次将操作符学习用于超声断层成像，也是潜在的实时预测乳腺成像中的软组织分布以进行肿瘤识别的第一步。

    Neural operator learning as a means of mapping between complex function spaces has garnered significant attention in the field of computational science and engineering (CS&E). In this paper, we apply Neural operator learning to the time-of-flight ultrasound computed tomography (USCT) problem. We learn the mapping between time-of-flight (TOF) data and the heterogeneous sound speed field using a full-wave solver to generate the training data. This novel application of operator learning circumnavigates the need to solve the computationally intensive iterative inverse problem. The operator learns the non-linear mapping offline and predicts the heterogeneous sound field with a single forward pass through the model. This is the first time operator learning has been used for ultrasound tomography and is the first step in potential real-time predictions of soft tissue distribution for tumor identification in beast imaging.
    
[^76]: SS-shapelets: 代表形状子序列的半监督时间序列聚类方法

    SS-shapelets: Semi-supervised Clustering of Time Series Using Representative Shapelets. (arXiv:2304.03292v1 [cs.LG])

    [http://arxiv.org/abs/2304.03292](http://arxiv.org/abs/2304.03292)

    本论文提出了一种名为SS-shapelets的半监督时间序列聚类方法，通过使用少量标记的和传播的伪标记时间序列来发现代表性形状子序列，从而提高聚类准确性。

    

    形状子序列是使用本地特征（子序列）鉴别时间序列的有前途的时间序列聚类方法。现有的时间序列聚类方法可能无法捕获代表性形状子序列，因为它们从大量无信息的子序列中发现形状子序列，因此聚类准确性较低。本文提出了一种使用少量标记的和传播的伪标记时间序列来帮助发现代表性形状子序列，从而提高聚类准确性的半监督时间序列聚类方法（SS-Shapelets）。在SS-Shapelets中，我们提出了两种技术来发现代表性形状子序列，以便有效地聚类时间序列。1）一个“显著子序列链”（$SSC$），可以从标记/伪标记的时间序列中提取显著子序列（作为候选形状子序列），从而有助于从池中删除大量无信息的子序列。2）一种“线性判别选择形状子序列”（$LDSS$），它选择在多个时间序列之间具有区别性的形状子序列。

    Shapelets that discriminate time series using local features (subsequences) are promising for time series clustering. Existing time series clustering methods may fail to capture representative shapelets because they discover shapelets from a large pool of uninformative subsequences, and thus result in low clustering accuracy. This paper proposes a Semi-supervised Clustering of Time Series Using Representative Shapelets (SS-Shapelets) method, which utilizes a small number of labeled and propagated pseudo-labeled time series to help discover representative shapelets, thereby improving the clustering accuracy. In SS-Shapelets, we propose two techniques to discover representative shapelets for the effective clustering of time series. 1) A \textit{salient subsequence chain} ($SSC$) that can extract salient subsequences (as candidate shapelets) of a labeled/pseudo-labeled time series, which helps remove massive uninformative subsequences from the pool. 2) A \textit{linear discriminant select
    
[^77]: 比较NARS和强化学习：对ONA和$Q$-Learning算法的分析

    Comparing NARS and Reinforcement Learning: An Analysis of ONA and $Q$-Learning Algorithms. (arXiv:2304.03291v1 [cs.LG])

    [http://arxiv.org/abs/2304.03291](http://arxiv.org/abs/2304.03291)

    本文比较了NARS和强化学习在解决序列任务方面的性能，发现NARS在各种环境中都有较好的表现，尤其是在非确定性环境中。

    

    近年来，强化学习（RL）已成为解决机器学习中基于序列任务的流行方法。然而，寻找RL的可行替代方案仍然是一个令人兴奋和创新的研究领域。其中一个备受关注的替代方案是非公理推理系统（NARS），它是一个通用的认知推理框架。本文研究了NARS作为RL替代方案在解决基于序列任务方面的潜力。为了研究这一点，我们在使用Open AI gym创建的各种环境中，对ONA作为NARS实现和$Q$-Learning的性能进行了比较分析。这些环境具有不同的难度级别，从简单到复杂不等。我们的研究结果表明，在各种环境中，尤其是在非确定性环境中，NARS是一个有竞争力的RL替代方案。

    In recent years, reinforcement learning (RL) has emerged as a popular approach for solving sequence-based tasks in machine learning. However, finding suitable alternatives to RL remains an exciting and innovative research area. One such alternative that has garnered attention is the Non-Axiomatic Reasoning System (NARS), which is a general-purpose cognitive reasoning framework. In this paper, we delve into the potential of NARS as a substitute for RL in solving sequence-based tasks. To investigate this, we conduct a comparative analysis of the performance of ONA as an implementation of NARS and $Q$-Learning in various environments that were created using the Open AI gym. The environments have different difficulty levels, ranging from simple to complex. Our results demonstrate that NARS is a promising alternative to RL, with competitive performance in diverse environments, particularly in non-deterministic ones.
    
[^78]: VISHIEN-MAAT：基于滚动叙事的可视化设计，为非技术用户解释孪生神经网络概念

    VISHIEN-MAAT: Scrollytelling visualization design for explaining Siamese Neural Network concept to non-technical users. (arXiv:2304.03288v1 [cs.HC])

    [http://arxiv.org/abs/2304.03288](http://arxiv.org/abs/2304.03288)

    本研究提出了一种基于滚动叙述(scrollytelling）的可视化设计，用于向非技术用户解释人工智能概念，其中以孪生神经网络为例，提供了具有直观解释的交互界面。

    

    过去的十年中，深度学习的突破以及人工智能研究的快速发展引领了人工智能技术在几乎所有领域的应用。因此，技术和非技术端用户必须了解这些技术以利用它们。然而，现有的资料都是为专家设计的，但非技术用户需要能够以易于理解的步骤呈现复杂思想的吸引人材料。一个适合这种情况的工具是滚动叙述(scrollytelling），这是一种讲故事的方法，为读者提供一种自然且丰富的阅读体验，以读者的节奏和深入的交互式解释复杂的概念。因此，本文提出了一种新的可视化设计，用于创建一个可滚动讲述，可以有效地向非技术用户解释人工智能概念。作为我们设计的一个示例，我们创建了一个可滚动叙述，以解释孪生神经网络用于实现视觉相似性匹配问题的工作原理。我们的方法通过提供具有直观解释的交互式界面，有助于创建一个有价值的可视化效果来展示孪生神经网络的工作流程。

    The past decade has witnessed rapid progress in AI research since the breakthrough in deep learning. AI technology has been applied in almost every field; therefore, technical and non-technical end-users must understand these technologies to exploit them. However existing materials are designed for experts, but non-technical users need appealing materials that deliver complex ideas in easy-to-follow steps. One notable tool that fits such a profile is scrollytelling, an approach to storytelling that provides readers with a natural and rich experience at the reader's pace, along with in-depth interactive explanations of complex concepts. Hence, this work proposes a novel visualization design for creating a scrollytelling that can effectively explain an AI concept to non-technical users. As a demonstration of our design, we created a scrollytelling to explain the Siamese Neural Network for the visual similarity matching problem. Our approach helps create a visualization valuable for a sho
    
[^79]: 自然语言规范中的数学程序合成

    Synthesis of Mathematical programs from Natural Language Specifications. (arXiv:2304.03287v1 [cs.AI])

    [http://arxiv.org/abs/2304.03287](http://arxiv.org/abs/2304.03287)

    本论文关注于通过自然语言规范中的目标和约束合成数学程序，并通过评估CodeT5和使用GPT-3来生成需要的示例进行实验。

    

    在各个商业领域中遇到的几个决策问题可以被建模为数学程序，即优化问题。进行这种建模的过程通常需要涉及到受过运筹学和高级算法培训的专家。令人惊讶的是，尽管程序和代码合成，AutoML，学习优化等方面的方法取得了重大进展，但几乎没有人关注自动化合成数学程序的任务。我们想象一种情景，在这种情况下，建模的规范，即目标和约束以自然语言的形式表达，并且必须从这样的自然语言规范中合成数学程序。在这项工作中，我们评估了使用带有数据增强和束后处理的CodeT5的功效。我们利用GPT-3进行背翻译以生成合成示例。此外，我们应用线性规划规则来评分。

    Several decision problems that are encountered in various business domains can be modeled as mathematical programs, i.e. optimization problems. The process of conducting such modeling often requires the involvement of experts trained in operations research and advanced algorithms. Surprisingly, despite the significant advances in the methods for program and code synthesis, AutoML, learning to optimize etc., there has been little or no attention paid to automating the task of synthesizing mathematical programs. We imagine a scenario where the specifications for modeling, i.e. the objective and constraints are expressed in an unstructured form in natural language (NL) and the mathematical program has to be synthesized from such an NL specification. In this work we evaluate the efficacy of employing CodeT5 with data augmentation and post-processing of beams. We utilize GPT-3 with back translation for generation of synthetic examples. Further we apply rules of linear programming to score b
    
[^80]: 归纳式图形反学习

    Inductive Graph Unlearning. (arXiv:2304.03093v1 [cs.LG])

    [http://arxiv.org/abs/2304.03093](http://arxiv.org/abs/2304.03093)

    本论文介绍了针对图形数据的反学习，旨在实现机器学习中的“被遗忘权”，与其他框架相比，提出了一个新颖的归纳式框架。这个框架可以让机器学习系统在处理动态改变的图形时更具有适应性。

    

    “机器反学习”是机器学习实现“被遗忘权”的方法，旨在完全删除要删除的样本对经过训练的模型的贡献和信息，同时不影响其他样本的贡献。近年来，许多反学习框架已被提出，其中大部分专注于图像和文本数据。为了将反学习扩展到图形数据，已经提出了GraphEraser。然而，一个关键的问题是GraphEraser专门针对转移图设定进行设计，在该设定下，图形是静态的，测试节点的属性和边缘在训练期间是可见的。对于归纳式的设置是不合适的，在此设置中，图形可以是动态的，测试图形信息事先是不可见的。这种归纳能力对于具有不断发展的图形（如社交媒体和交易网络）的生产机器学习系统至关重要。为了填补这一空白，我们提出了G...

    As a way to implement the "right to be forgotten" in machine learning, \textit{machine unlearning} aims to completely remove the contributions and information of the samples to be deleted from a trained model without affecting the contributions of other samples. Recently, many frameworks for machine unlearning have been proposed, and most of them focus on image and text data. To extend machine unlearning to graph data, \textit{GraphEraser} has been proposed. However, a critical issue is that \textit{GraphEraser} is specifically designed for the transductive graph setting, where the graph is static and attributes and edges of test nodes are visible during training. It is unsuitable for the inductive setting, where the graph could be dynamic and the test graph information is invisible in advance. Such inductive capability is essential for production machine learning systems with evolving graphs like social media and transaction networks. To fill this gap, we propose the \underline{{\bf G
    
[^81]: 深度神经网络的重尾部正则化

    Heavy-Tailed Regularization of Weight Matrices in Deep Neural Networks. (arXiv:2304.02911v1 [stat.ML])

    [http://arxiv.org/abs/2304.02911](http://arxiv.org/abs/2304.02911)

    本文介绍了一种名为重尾部正则化的技术，在深度神经网络中通过明确提倡更重的重尾谱来提高泛化性能。与标准正则化技术相比，该方法在基准数据集上实现了显着的改进。

    

    深度神经网络成功和显著的泛化能力背后的原因仍然是一个巨大的挑战。从随机矩阵理论得到的最新信息，特别是涉及深度神经网络中权重矩阵的谱分析的信息，为解决这个问题提供了有价值的线索。一个关键发现是，神经网络的泛化性能与其权重矩阵的谱的重尾程度相关。为了利用这一发现，我们介绍了一种新的正则化技术，称为重尾部正则化，通过正则化明确提倡权重矩阵中更重的重尾谱。首先，我们采用加权阿尔法和稳定秩作为惩罚项，两者都可微分，从而可以直接计算它们的梯度。为了避免过度正则化，我们介绍了两种惩罚函数的变体。然后，采用贝叶斯统计视角，我们提出了重尾部正则化的概率解释，使我们能够将其效果理解为权重矩阵的先验。在多个基准数据集上的实证评估表明，与标准正则化技术相比，我们的方法明显提高了泛化性能。

    Unraveling the reasons behind the remarkable success and exceptional generalization capabilities of deep neural networks presents a formidable challenge. Recent insights from random matrix theory, specifically those concerning the spectral analysis of weight matrices in deep neural networks, offer valuable clues to address this issue. A key finding indicates that the generalization performance of a neural network is associated with the degree of heavy tails in the spectrum of its weight matrices. To capitalize on this discovery, we introduce a novel regularization technique, termed Heavy-Tailed Regularization, which explicitly promotes a more heavy-tailed spectrum in the weight matrix through regularization. Firstly, we employ the Weighted Alpha and Stable Rank as penalty terms, both of which are differentiable, enabling the direct calculation of their gradients. To circumvent over-regularization, we introduce two variations of the penalty function. Then, adopting a Bayesian statistics
    
[^82]: ViralVectors：一种紧凑且可扩展的基于非比对技术生成virome特征的方法

    ViralVectors: Compact and Scalable Alignment-free Virome Feature Generation. (arXiv:2304.02891v1 [q-bio.GN])

    [http://arxiv.org/abs/2304.02891](http://arxiv.org/abs/2304.02891)

    ViralVectors是一种紧凑且可扩展的方法，从virome测序数据中生成Minimizers特征向量进行有效的下游分析。该方法优于现有非比对技术方法，可以区分不同的病毒家族，甚至属，并能够提供接近最优的SARS-CoV-2分类性能。

    

    对于SARS-CoV-2的测序数据量比其他大多数病毒都要大若干个数量级，而且SARS-CoV-2的数据量将继续呈几何级数增长，许多国家正在大力投资基因组监测工作。因此，我们需要处理大量的序列数据以实现有效而及时的决策。这些数据来自各种不同的来源：比对、未比对甚至未装配的原始核苷酸或氨基酸测序reads，涵盖整个基因组或某些区域（例如spike）。本研究提出了ViralVectors，一种从virome测序数据中生成紧凑特征向量的方法，以实现有效的下游分析。该生成方法基于minimizers，一种轻量级的序列“签名”，传统上用于组装和读取映射，据我们所知，首次使用minimizers进行这样的方法。我们验证了我们的方法在不同类型的测序数据上的表现：（a）2.5M SARS-CoV-2 nanopore reads，（b）1.5M 未比对的SARS-CoV-2 Illumina reads，以及（c）大量的virome reads。我们的方法优于最先进的基于非比对技术的方法，可以区分不同的病毒家族，甚至属。我们展示了我们的特征向量可以轻松地聚类和可视化，实现了直观的病毒发现和探索功能。我们进一步分析了公开的SARS-CoV-2数据集，并发现我们的方法可以在一个平衡的两类SARS-CoV-2数据集上提供接近最优的分类性能。

    The amount of sequencing data for SARS-CoV-2 is several orders of magnitude larger than any virus. This will continue to grow geometrically for SARS-CoV-2, and other viruses, as many countries heavily finance genomic surveillance efforts. Hence, we need methods for processing large amounts of sequence data to allow for effective yet timely decision-making. Such data will come from heterogeneous sources: aligned, unaligned, or even unassembled raw nucleotide or amino acid sequencing reads pertaining to the whole genome or regions (e.g., spike) of interest. In this work, we propose \emph{ViralVectors}, a compact feature vector generation from virome sequencing data that allows effective downstream analysis. Such generation is based on \emph{minimizers}, a type of lightweight "signature" of a sequence, used traditionally in assembly and read mapping -- to our knowledge, the first use minimizers in this way. We validate our approach on different types of sequencing data: (a) 2.5M SARS-CoV-
    
[^83]: ACTION++：使用自适应解剖对比度改善半监督医学图像分割

    ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast. (arXiv:2304.02689v1 [cs.CV])

    [http://arxiv.org/abs/2304.02689](http://arxiv.org/abs/2304.02689)

    本文提出了一种改进的对比学习框架ACTION++，通过自适应的解剖对比来改善半监督医学图像分割。

    

    医学数据通常表现为长尾分布，存在严重的类别不平衡，这自然导致少数类别（即边界区域或罕见物体）的分类困难。最近的工作通过配备无监督对比标准，在长尾场景中显着改进了半监督医学图像分割。然而，在类别分布也高度不平衡的标记数据部分中，它们的表现仍不清楚。在这项工作中，我们提出了ACTION++，一种改进的具有自适应解剖对比的对比学习框架，用于半监督医学分割。

    Medical data often exhibits long-tail distributions with heavy class imbalance, which naturally leads to difficulty in classifying the minority classes (i.e., boundary regions or rare objects). Recent work has significantly improved semi-supervised medical image segmentation in long-tailed scenarios by equipping them with unsupervised contrastive criteria. However, it remains unclear how well they will perform in the labeled portion of data where class distribution is also highly imbalanced. In this work, we present ACTION++, an improved contrastive learning framework with adaptive anatomical contrast for semi-supervised medical segmentation. Specifically, we propose an adaptive supervised contrastive loss, where we first compute the optimal locations of class centers uniformly distributed on the embedding space (i.e., off-line), and then perform online contrastive matching training by encouraging different class features to adaptively match these distinct and uniformly distributed cla
    
[^84]: 本地固有维度熵。

    Local Intrinsic Dimensional Entropy. (arXiv:2304.02223v1 [cs.LG])

    [http://arxiv.org/abs/2304.02223](http://arxiv.org/abs/2304.02223)

    本文提出了一种新的在连续空间中测量熵的方法，称为ID-Entropy，它可以用于多轮数据变换和扭曲，同时可以捕捉数据的维度。

    

    大多数熵测量依赖于概率分布在样本空间X上的展布情况，最大可实现熵与样本空间基数|X|成比例。对于有限|X|，这产生了满足许多重要属性（如对双射的不变性）的强大熵测量，而同样不能满足连续空间的要求（其中|X|=无穷大）。此外，由于R和R^d（d在Z+中）具有相同的基数（来自Cantor的对应论证），基数依赖性熵测量无法编码数据维度。在本文中，我们质疑了对连续空间定义熵测量中基数和分布展布的作用，这些连续空间可以进行多轮变换和扭曲，例如在神经网络中。我们发现如果用分布的局部固有维度的平均值来表示测量熵，被称为ID-Entropy，那么可以作为连续空间的强大熵测量，同时捕捉数据的维度。

    Most entropy measures depend on the spread of the probability distribution over the sample space X, and the maximum entropy achievable scales proportionately with the sample space cardinality |X|. For a finite |X|, this yields robust entropy measures which satisfy many important properties, such as invariance to bijections, while the same is not true for continuous spaces (where |X|=infinity). Furthermore, since R and R^d (d in Z+) have the same cardinality (from Cantor's correspondence argument), cardinality-dependent entropy measures cannot encode the data dimensionality. In this work, we question the role of cardinality and distribution spread in defining entropy measures for continuous spaces, which can undergo multiple rounds of transformations and distortions, e.g., in neural networks. We find that the average value of the local intrinsic dimension of a distribution, denoted as ID-Entropy, can serve as a robust entropy measure for continuous spaces, while capturing the data dimen
    
[^85]: 非线性状态空间识别的子空间编码器方法的初始化方法

    Initialization Approach for Nonlinear State-Space Identification via the Subspace Encoder Approach. (arXiv:2304.02119v1 [eess.SY])

    [http://arxiv.org/abs/2304.02119](http://arxiv.org/abs/2304.02119)

    本论文介绍一个使用最佳线性逼近(BLA)初始化子空间编码器方法的初始方法，以提高非线性状态空间识别的收敛性。

    

    SUBNET神经网络结构被开发用于从输入输出数据中识别非线性状态空间模型，它将展开的非线性状态空间方程和状态编码器函数组合起来作为神经网络参数。引入编码器函数来从过去的输入输出数据中重构当前状态，从而使展开的状态空间模型得以前向模拟。该方法已经证明提供了高精度和一致的模型估计，但是通过有效的训练过程初始化可以显著提高其收敛性。本文重点介绍使用最佳线性逼近(BLA)初始化子空间编码器方法的初始方法。使用BLA提供的状态空间矩阵及其相关的可重构映射来初始化网络的状态转移部分和编码器。改进的初始化方案的表现在Wiener-Hamme上进行了评估。

    The SUBNET neural network architecture has been developed to identify nonlinear state-space models from input-output data. To achieve this, it combines the rolled-out nonlinear state-space equations and a state encoder function, both parameterised as a neural network. The encoder function is introduced to reconstruct the current state from past input-output data. Hence it enables the forward simulation of the rolled-out state-space model. While this approach has shown to provide high-accuracy and consistent model estimation, its convergence can be significantly improved by efficient initialization of the training process. This paper focuses on such an initialisation of the subspace encoder approach using the Best Linear Approximation (BLA). Using the BLA provided state-space matrices and its associated reconstructability map both the state-transition part of the network and the encoder are initialized. The performance of the improved initialisation scheme is evaluated on a Wiener-Hamme
    
[^86]: 基于准度量学习的最优目标达成强化学习方法

    Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning. (arXiv:2304.01203v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.01203](http://arxiv.org/abs/2304.01203)

    本文介绍了一种新的强化学习方法——准度量强化学习（QRL），利用准度量模型来学习最优价值函数；在离线和在线的目标达成基准测试中，QRL展示了更好的采样效率和性能，包括基于状态和基于图像的观测。

    

    在目标达成强化学习中，最优价值函数具有特定的几何结构，称为准度量结构。本文介绍了一种新的强化学习方法——准度量强化学习（QRL），利用准度量模型来学习最优价值函数。与以往的方法不同，QRL的目标是专门为准度量设计的，并提供了强大的理论恢复保证。在离散化的MountainCar环境上进行了全面分析，确定了QRL的性质以及其优于其他方法的优势。在离线和在线的目标达成基准测试中，QRL还展示了更好的采样效率和性能，包括基于状态和基于图像的观测。

    In goal-reaching reinforcement learning (RL), the optimal value function has a particular geometry, called quasimetric structure. This paper introduces Quasimetric Reinforcement Learning (QRL), a new RL method that utilizes quasimetric models to learn optimal value functions. Distinct from prior approaches, the QRL objective is specifically designed for quasimetrics, and provides strong theoretical recovery guarantees. Empirically, we conduct thorough analyses on a discretized MountainCar environment, identifying properties of QRL and its advantages over alternatives. On offline and online goal-reaching benchmarks, QRL also demonstrates improved sample efficiency and performance, across both state-based and image-based observations.
    
[^87]: 采用灵活概率神经网络方法的集合天气预报后处理

    Ensemble weather forecast post-processing with a flexible probabilistic neural network approach. (arXiv:2303.17610v1 [cs.LG])

    [http://arxiv.org/abs/2303.17610](http://arxiv.org/abs/2303.17610)

    本论文提出了一种神经网络和归一化流相结合的方法，可联合预测所有位置和提前期，从而放宽了许多传统后处理方法的分布假设，并通过EUPPBench基准测试证明了其超越性能。

    

    集合预报后处理是生成准确概率预报的必要步骤。传统的后处理方法是根据每个位置或每个提前期估计参数统计分布。我们提出了一种基于神经网络的新方法，它可以联合预测所有位置和提前期。为了放宽许多后处理方法的分布假设，我们的方法采用归一化流作为灵活的参数分布估计器。这使我们能够以数学确切的方式模拟不同的预测分布。我们在EUPPBench基准测试中展示了我们方法的有效性，在该测试中，我们对西欧地区子区域的站点进行了温度预报后处理。我们展示了我们的新方法在基准测试中表现出卓越的性能，超越了我们之前的表现良好的成绩。此外，通过提供详细的比较，我们证明了我们方法的优越性。

    Ensemble forecast post-processing is a necessary step in producing accurate probabilistic forecasts. Conventional post-processing methods operate by estimating the parameters of a parametric distribution, frequently on a per-location or per-lead-time basis. We propose a novel, neural network-based method, which produces forecasts for all locations and lead times, jointly. To relax the distributional assumption of many post-processing methods, our approach incorporates normalizing flows as flexible parametric distribution estimators. This enables us to model varying forecast distributions in a mathematically exact way. We demonstrate the effectiveness of our method in the context of the EUPPBench benchmark, where we conduct temperature forecast post-processing for stations in a sub-region of western Europe. We show that our novel method exhibits state-of-the-art performance on the benchmark, outclassing our previous, well-performing entry. Additionally, by providing a detailed compariso
    
[^88]: 面向资源受限的无线边缘网络的高效并行分裂学习

    Efficient Parallel Split Learning over Resource-constrained Wireless Edge Networks. (arXiv:2303.15991v1 [cs.LG])

    [http://arxiv.org/abs/2303.15991](http://arxiv.org/abs/2303.15991)

    本文提出了面向资源受限的无线边缘网络的高效并行分裂学习（EPSL）框架，旨在加速模型训练。EPSL并行化客户端模型训练，通过聚合梯度降低了反向传播的局部梯度维度，从而显著减少了服务器端的训练和通信延迟。同时，EPSL还设计了资源分配算法以优化计算和通信资源分配。

    

    随着神经网络越来越深，这阻碍了联合学习等隐私增强分布式学习方式（如联邦学习）在资源受限的设备上的民主化。为了解决这个挑战，本文倡导将边缘计算范式和并行分裂学习（PSL）相结合，允许多个客户端设备通过逐层模型分裂将大量的训练工作负载卸载到边缘服务器上。通过观察到现有的PSL方案会产生过多的训练延迟和大量的数据传输，我们提出了一种创新的PSL框架——高效并行分裂学习（EPSL），以加速模型训练。具体而言，EPSL将客户端模型训练并行化，并通过最后一层梯度聚合降低了反向传播（BP）的局部梯度维度，从而显著减少了服务器端的训练和通信延迟。此外，通过考虑边缘设备的异构通道条件和计算能力，我们设计了资源分配算法以优化计算和通信资源分配。实验结果表明，EPSL通过将通信成本和训练时间分别降低76％和63％，优于最先进的PSL方法。

    The increasingly deeper neural networks hinder the democratization of privacy-enhancing distributed learning, such as federated learning (FL), to resource-constrained devices. To overcome this challenge, in this paper, we advocate the integration of edge computing paradigm and parallel split learning (PSL), allowing multiple client devices to offload substantial training workloads to an edge server via layer-wise model split. By observing that existing PSL schemes incur excessive training latency and large volume of data transmissions, we propose an innovative PSL framework, namely, efficient parallel split learning (EPSL), to accelerate model training. To be specific, EPSL parallelizes client-side model training and reduces the dimension of local gradients for back propagation (BP) via last-layer gradient aggregation, leading to a significant reduction in server-side training and communication latency. Moreover, by considering the heterogeneous channel conditions and computing capabil
    
[^89]: 一种基于高斯混合分布的自适应采样方法用于PINNs

    GAS: A Gaussian Mixture Distribution-Based Adaptive Sampling Method for PINNs. (arXiv:2303.15849v1 [cs.LG])

    [http://arxiv.org/abs/2303.15849](http://arxiv.org/abs/2303.15849)

    GAS是一种基于高斯混合分布的自适应采样方法，用于加速PINNs的收敛过程并提高精度，已在2D到10D问题的数值模拟中表现出领先于深层求解器、与传统数值求解器相当的优异性能。

    

    随着深度学习在科学计算中的应用研究，PINNs方法因其高维问题处理的高效性而引起了广泛的关注，但其准确性相对较低，特别是针对高度不规则的问题。受自适应有限元方法和增量学习思想启发，我们提出了GAS，一种基于高斯混合分布的自适应采样方法，用于PINNs。在训练过程中，GAS利用当前的残差信息生成高斯混合分布以采样其他点，这些数据将与历史数据一起训练，加快损失函数的收敛速度并实现更高的准确性。在2D到10D问题的数值模拟中，GAS是一种有前途的方法，它在深层求解器中达到了最先进的精度，同时与传统数值求解器相当。

    With recent study of the deep learning in scientific computation, the PINNs method has drawn widespread attention for solving PDEs. Compared with traditional methods, PINNs can efficiently handle high-dimensional problems, while the accuracy is relatively low, especially for highly irregular problems. Inspired by the idea of adaptive finite element methods and incremental learning, we propose GAS, a Gaussian mixture distribution-based adaptive sampling method for PINNs. During the training procedure, GAS uses the current residual information to generate a Gaussian mixture distribution for the sampling of additional points, which are then trained together with history data to speed up the convergence of loss and achieve a higher accuracy. Several numerical simulations on 2d to 10d problems show that GAS is a promising method which achieves the state-of-the-art accuracy among deep solvers, while being comparable with traditional numerical solvers.
    
[^90]: 多视角三维物体检测的观点等变性

    Viewpoint Equivariance for Multi-View 3D Object Detection. (arXiv:2303.14548v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.14548](http://arxiv.org/abs/2303.14548)

    本文提出了一种利用多视角几何学习视点等变性以提高三维物体检测定位精度的框架VEDet，并通过基于查询的transformer架构和视角条件的查询来实现。

    

    从视觉传感器进行三维物体检测是机器人系统的关键能力。现代方法侧重于从多视角相机输入推理和解码物体边界框。本文利用多视角一致性在三维场景理解和几何学习中的重要作用，介绍了VEDet，一种新颖的三维物体检测框架，通过视点感知和等变性利用三维多视角几何来提高定位精度。VEDet利用基于查询的transformer架构，并通过将图像特征和来自它们的三维透视几何的位置编码相结合来编码三维场景。我们在输出层设计了视角条件的查询，这使得在训练期间生成多个虚拟帧，通过强制多视角一致性来学习视点等变性。在输入层注入的多视角几何作为位置编码，并在损失层中进行正则化，提供了丰富的地理信息。

    3D object detection from visual sensors is a cornerstone capability of robotic systems. State-of-the-art methods focus on reasoning and decoding object bounding boxes from multi-view camera input. In this work we gain intuition from the integral role of multi-view consistency in 3D scene understanding and geometric learning. To this end, we introduce VEDet, a novel 3D object detection framework that exploits 3D multi-view geometry to improve localization through viewpoint awareness and equivariance. VEDet leverages a query-based transformer architecture and encodes the 3D scene by augmenting image features with positional encodings from their 3D perspective geometry. We design view-conditioned queries at the output level, which enables the generation of multiple virtual frames during training to learn viewpoint equivariance by enforcing multi-view consistency. The multi-view geometry injected at the input level as positional encodings and regularized at the loss level provides rich geo
    
[^91]: 学生-教师框架下随机特征模型的在线学习

    Online Learning for the Random Feature Model in the Student-Teacher Framework. (arXiv:2303.14083v1 [cs.LG])

    [http://arxiv.org/abs/2303.14083](http://arxiv.org/abs/2303.14083)

    本研究考虑了一种两层神经网络模型的在线学习，研究发现，当学生的隐藏层大小呈指数增长时，完美泛化是可行的，但对于任何有限的隐藏层大小和输入维度比，学生都无法完美泛化。

    

    深度神经网络是一种广泛应用的预测算法，随着权重数量的增加，其性能通常会提高，导致过度参数化。我们考虑一种两层神经网络，其第一层是冻结的，而最后一层是可训练的，称为随机特征模型。我们在学生-教师框架下研究了过度参数化，通过导出一组学习动态的微分方程。对于任何有限的隐藏层大小和输入维度比，学生都无法完美泛化，并计算非零渐近泛化误差。只有当学生的隐藏层大小呈指数增长时，才有可能实现完美泛化。

    Deep neural networks are widely used prediction algorithms whose performance often improves as the number of weights increases, leading to over-parametrization. We consider a two-layered neural network whose first layer is frozen while the last layer is trainable, known as the random feature model. We study over-parametrization in the context of a student-teacher framework by deriving a set of differential equations for the learning dynamics. For any finite ratio of hidden layer size and input dimension, the student cannot generalize perfectly, and we compute the non-zero asymptotic generalization error. Only when the student's hidden layer size is exponentially larger than the input dimension, an approach to perfect generalization is possible.
    
[^92]: 重新审视影响函数的脆弱性

    Revisiting the Fragility of Influence Functions. (arXiv:2303.12922v1 [cs.LG])

    [http://arxiv.org/abs/2303.12922](http://arxiv.org/abs/2303.12922)

    本文研究了影响函数的脆弱性，并提出在非凸条件下使用深层模型和更复杂数据集来解决这一问题。

    

    最近几年有很多论文致力于解释深度学习模型的预测。然而，很少有方法被提出来验证这些解释的准确性或可信度。最近，影响函数被证明是一种评估深度神经网络在单个样本上的灵敏度的方法。但是，先前的研究表明影响函数易受噪声和数据分布不对称性影响，缺乏鲁棒性。本文旨在研究影响函数的脆弱性，通过探究影响函数背后的机理，从而为增强影响函数的鲁棒性提供新思路。

    In the last few years, many works have tried to explain the predictions of deep learning models. Few methods, however, have been proposed to verify the accuracy or faithfulness of these explanations. Recently, influence functions, which is a method that approximates the effect that leave-one-out training has on the loss function, has been shown to be fragile. The proposed reason for their fragility remains unclear. Although previous work suggests the use of regularization to increase robustness, this does not hold in all cases. In this work, we seek to investigate the experiments performed in the prior work in an effort to understand the underlying mechanisms of influence function fragility. First, we verify influence functions using procedures from the literature under conditions where the convexity assumptions of influence functions are met. Then, we relax these assumptions and study the effects of non-convexity by using deeper models and more complex datasets. Here, we analyze the k
    
[^93]: 临床BERTScore：临床环境下自动语音识别性能的改进度量

    Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings. (arXiv:2303.05737v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2303.05737](http://arxiv.org/abs/2303.05737)

    本文提出了一种临床BERTScore（CBERTScore）度量，它比其他度量更严厉地惩罚临床相关的错误，更接近于临床医生对医学句子的偏好。作者还收集了13个临床医生对149个现实医学句子的偏好基准，称为临床转录偏好基准（CTP），证明CBERTScore更接近于临床医生的偏好，并将基准发布给社区以进一步开发具有临床意识的ASR度量。

    The paper proposes a Clinical BERTScore (CBERTScore) metric for ASR in medical contexts, which penalizes clinically-relevant mistakes more than other metrics and aligns more closely with clinician preferences. The authors also collect a benchmark of clinician preferences on medical sentences and release it for the community to further develop clinically-aware ASR metrics.

    医学环境中的自动语音识别（ASR）有潜力节省时间，降低成本，提高报告准确性并减少医生的疲劳。然而，由于避免医学相关的转录错误的重要性，医疗行业采用这种技术的速度较慢。在这项工作中，我们提出了临床BERTScore（CBERTScore），这是一种ASR度量，它比其他度量（WER、BLUE、METEOR等）更严厉地惩罚临床相关的错误。我们证明了这个度量更接近于临床医生对医学句子的偏好，有时差距很大。我们收集了13个临床医生对149个现实医学句子的偏好基准，称为临床转录偏好基准（CTP），证明CBERTScore更接近于临床医生的偏好，并将基准发布给社区以进一步开发具有临床意识的ASR度量。

    Automatic Speech Recognition (ASR) in medical contexts has the potential to save time, cut costs, increase report accuracy, and reduce physician burnout. However, the healthcare industry has been slower to adopt this technology, in part due to the importance of avoiding medically-relevant transcription mistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR metric that penalizes clinically-relevant mistakes more than others. We demonstrate that this metric more closely aligns with clinician preferences on medical sentences as compared to other metrics (WER, BLUE, METEOR, etc), sometimes by wide margins. We collect a benchmark of 13 clinician preferences on 149 realistic medical sentences called the Clinician Transcript Preference benchmark (CTP), demonstrate that CBERTScore more closely matches what clinicians prefer, and release the benchmark for the community to further develop clinically-aware ASR metrics.
    
[^94]: SemEval-2023任务11的Lon-ea：软硬标签预测中激活函数的比较。

    Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for Soft and Hard Label Prediction. (arXiv:2303.02468v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.02468](http://arxiv.org/abs/2303.02468)

    本研究研究了在软硬标签预测中，不同激活函数对于深度神经网络模型输出的影响，并引入了一种新的正弦激活函数。

    

    我们研究在学习不同意任务的软硬标签预测中，深度神经网络模型输出层中不同激活函数的影响。在该任务中，目标是通过预测软标签来量化不同意量。为了预测软标签，我们使用基于BERT的预处理器和编码器，并改变输出层中使用的激活函数，同时保持其他参数不变。然后将软标签用于硬标签预测。考虑的激活函数包括sigmoid函数以及添加到模型中的阶跃函数和本文中首次介绍的正弦激活函数。

    We study the influence of different activation functions in the output layer of deep neural network models for soft and hard label prediction in the learning with disagreement task. In this task, the goal is to quantify the amount of disagreement via predicting soft labels. To predict the soft labels, we use BERT-based preprocessors and encoders and vary the activation function used in the output layer, while keeping other parameters constant. The soft labels are then used for the hard label prediction. The activation functions considered are sigmoid as well as a step-function that is added to the model post-training and a sinusoidal activation function, which is introduced for the first time in this paper.
    
[^95]: Safe-DS: 一种使数据科学更加安全的领域特定语言

    Safe-DS: A Domain Specific Language to Make Data Science Safe. (arXiv:2302.14548v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2302.14548](http://arxiv.org/abs/2302.14548)

    文章介绍了一种名为Safe-DS的领域特定语言，用于在数据科学中以静态安全的方式运行Python DS库，并可捕获出错的类型，并且拥有比Python Linters更强大的能力。

    

    数据科学（DS）流水线运行时间较长，即使是小的编程错误，如果不能静态检测到，也可能非常昂贵。但是，由于大多数DS流水线都是用Python编写的，因此即使是基本的静态类型检查也很难。在本文中，我们展示了如何通过Safe-DS，一种面向DS的领域特定语言（DSL），以静态安全的方式使用丰富的Python DS库。Safe-DS可以捕获常规类型错误以及与范围限制、数据处理和函数调用顺序相关的错误，远远超出了当前Python Linters的能力。Python库通过一个存根语言来集成到Safe-DS中，以指定其声明的接口，以及一个能够从Python库的代码和文档中提取类型信息的API编辑器。

    Due to the long runtime of Data Science (DS) pipelines, even small programming mistakes can be very costly, if they are not detected statically. However, even basic static type checking of DS pipelines is difficult because most are written in Python. Static typing is available in Python only via external linters. These require static type annotations for parameters or results of functions, which many DS libraries do not provide. In this paper, we show how the wealth of Python DS libraries can be used in a statically safe way via Safe-DS, a domain specific language (DSL) for DS. Safe-DS catches conventional type errors plus errors related to range restrictions, data manipulation, and call order of functions, going well beyond the abilities of current Python linters. Python libraries are integrated into Safe-DS via a stub language for specifying the interface of its declarations, and an API-Editor that is able to extract type information from the code and documentation of Python librarie
    
[^96]: 从递推视角重新审视LQR控制

    Revisiting LQR Control from the Perspective of Receding-Horizon Policy Gradient. (arXiv:2302.13144v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2302.13144](http://arxiv.org/abs/2302.13144)

    本文从递推视角重新审视了LQR控制问题，并应用递推-视角策略梯度（RHPG）模型提供了一种采样复杂度分析，通过无需任何先验信息进行优化求解，并展示了RHPG在线性控制和估计中的普适性。

    

    本文从递推视角重新审视了离散时间线性二次调节器（LQR）问题。结合递推-视角策略梯度（RHPG）模型无需任何先验信息进行优化求解，提供了一种采样复杂度分析，能够学习到在ε-范数意义下接近LQR最优解的优化控制策略。在最近将RHPG应用于学习卡尔曼滤波中进行拓展分析之后，我们展示了RHPG在线性控制和估计中的普适性。

    We revisit in this paper the discrete-time linear quadratic regulator (LQR) problem from the perspective of receding-horizon policy gradient (RHPG), a newly developed model-free learning framework for control applications. We provide a fine-grained sample complexity analysis for RHPG to learn a control policy that is both stabilizing and $\epsilon$-close to the optimal LQR solution, and our algorithm does not require knowing a stabilizing control policy for initialization. Combined with the recent application of RHPG in learning the Kalman filter, we demonstrate the general applicability of RHPG in linear control and estimation with streamlined analyses.
    
[^97]: 利用TROPOMI卫星数据和机器学习进行异常NO2排放船舶检测

    Anomalous NO2 emitting ship detection with TROPOMI satellite data and machine learning. (arXiv:2302.12744v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12744](http://arxiv.org/abs/2302.12744)

    本文提出了一种利用TROPOMI卫星数据和机器学习自动选择不符合船舶排放标准的方法。

    

    从2021年开始，北海和波罗的海水域中的船舶排放$\text{NO}_\text{x}$的限制更加严格。由于目前用于船舶遵守监督的所有方法都需要耗费大量时间和资金，因此必须优先考虑检查有可能不符合标准的船舶。目前的先进方法是使用TROPOMI/S5P图像上的基于监督式机器学习的船排烟检测，然而，对于数据标注具有挑战性且用于验证的船舶排放代理不够复杂，限制了该模型用于船舶遵守监督的适用性。在本研究中，我们提出了一种基于TROPOMI卫星数据的机器学习模型组合自动选择可能不符合标准的船舶的方法。它基于一个提出的回归模型预测预计产生的$\text{NO}_\text{2}$的数量。

    Starting from 2021, more demanding $\text{NO}_\text{x}$ emission restrictions were introduced for ships operating in the North and Baltic Sea waters. Since all methods currently used for ship compliance monitoring are financially and time demanding, it is important to prioritize the inspection of ships that have high chances of being non-compliant. The current state-of-the-art approach for a large-scale ship $\text{NO}_\text{2}$ estimation is a supervised machine learning-based segmentation of ship plumes on TROPOMI/S5P images. However, challenging data annotation and insufficiently complex ship emission proxy used for the validation limit the applicability of the model for ship compliance monitoring. In this study, we present a method for the automated selection of potentially non-compliant ships using a combination of machine learning models on TROPOMI satellite data. It is based on a proposed regression model predicting the amount of $\text{NO}_\text{2}$ that is expected to be produ
    
[^98]: 复杂问答和语言模型混合架构综述

    Complex QA and language models hybrid architectures, Survey. (arXiv:2302.09051v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.09051](http://arxiv.org/abs/2302.09051)

    本文综述了语言模型架构和策略的最新进展，并重点关注混合技术在复杂问题回答中的应用，讨论了该领域的挑战和未来研究方向。

    

    本文回顾了语言模型架构和策略的最新进展，重点关注混合技术在复杂问题回答中的应用。大型语言模型能够在标准问题上利用公共数据，但在解决更具体的复杂问题时（如在不同文化中个人自由概念的变化如何？什么是为减少气候变化而实现的最佳发电方法组合？），需要特定的架构、知识、技能、方法、敏感数据保护、可解释性、人类审批和多功能反馈。最近的项目如ChatGPT和GALACTICA允许非专业人员了解LLM在复杂QA中的巨大潜力以及同等强大的局限性。在本文中，我们首先审查所需的技能和评估技术。然后，我们综述了现有的混合架构，将LLM与基于规则的方法、信息检索、知识图谱和其他AI/ML技术相结合。最后，我们指出这些CQA系统的挑战，并提出未来研究的可能方向。

    This paper reviews the state-of-the-art of language models architectures and strategies for "complex" question-answering (QA, CQA, CPS) with a focus on hybridization. Large Language Models (LLM) are good at leveraging public data on standard problems but once you want to tackle more specific complex questions or problems (e.g. How does the concept of personal freedom vary between different cultures ? What is the best mix of power generation methods to reduce climate change ?) you may need specific architecture, knowledge, skills, methods, sensitive data protection, explainability, human approval and versatile feedback... Recent projects like ChatGPT and GALACTICA have allowed non-specialists to grasp the great potential as well as the equally strong limitations of LLM in complex QA. In this paper, we start by reviewing required skills and evaluation techniques. We integrate findings from the robust community edited research papers BIG, BLOOM and HELM which open source, benchmark and an
    
[^99]: 无模态偏见的隐式神经表示变分压缩算法

    Modality-Agnostic Variational Compression of Implicit Neural Representations. (arXiv:2301.09479v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.09479](http://arxiv.org/abs/2301.09479)

    提出了一种无模态偏见的隐式神经表示变分压缩算法，能够在不同的数据模态上表现出卓越的压缩性能和效果。

    

    我们提出了一种基于数据的函数视图，并用隐式神经表示（INR）参数化的无模态神经压缩算法。我们通过软门控机制将非线性映射到紧凑的潜在表示中，从而弥合了潜在编码和稀疏性之间的差距。这允许每个数据项通过子网络选择来定制共享的INR网络的专业化。在获取这种潜在表示的数据集后，我们在无模态空间中直接优化速率/失真的折衷方案，使用神经压缩。隐式神经表示的变分压缩（VC-INR）在具有相同表示容量的量化之前显示出改进的性能，同时优于其他INR技术所使用的先前量化方案。我们的实验结果显示，使用相同的算法而不需要任何特定于模态的归纳偏差，可以在各种不同的模态上取得卓越的结果。我们展示了在图像、气候数据、文本和音频数据上的结果。

    We introduce a modality-agnostic neural compression algorithm based on a functional view of data and parameterised as an Implicit Neural Representation (INR). Bridging the gap between latent coding and sparsity, we obtain compact latent representations non-linearly mapped to a soft gating mechanism. This allows the specialisation of a shared INR network to each data item through subnetwork selection. After obtaining a dataset of such latent representations, we directly optimise the rate/distortion trade-off in a modality-agnostic space using neural compression. Variational Compression of Implicit Neural Representations (VC-INR) shows improved performance given the same representational capacity pre quantisation while also outperforming previous quantisation schemes used for other INR techniques. Our experiments demonstrate strong results over a large set of diverse modalities using the same algorithm without any modality-specific inductive biases. We show results on images, climate dat
    
[^100]: 压缩优化学习用于交流电最优潮流计算

    Compact Optimization Learning for AC Optimal Power Flow. (arXiv:2301.08840v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08840](http://arxiv.org/abs/2301.08840)

    本文提出了一种压缩学习方法，利用主成分分析可以显著降低交流电最优潮流计算的维度，降低了可训练参数的数量，提高了可扩展性和有效性。同时，该方法的输出可以用于热启动精确的AC求解器以恢复可行性。

    

    本文重新考虑了最优潮流计算（OPF）的端到端学习方法。现有的学习输入/输出映射的方法由于输出空间的高维度而存在可扩展性问题。本文首先展示了利用主成分分析（PCA）可以显著压缩最优解的空间。它提出了一种新方法——压缩学习，该方法在主要成分的子空间中进行学习，然后将向量转换为原始输出空间。这种压缩大大降低了可训练参数的数量，提高了可扩展性和有效性。压缩学习在PGLib的各种测试用例中进行了评估，最高可达30,000个总线。本文还表明，压缩学习的输出可以用于热启动精确的AC求解器以恢复可行性，并带来显着的加速。

    This paper reconsiders end-to-end learning approaches to the Optimal Power Flow (OPF). Existing methods, which learn the input/output mapping of the OPF, suffer from scalability issues due to the high dimensionality of the output space. This paper first shows that the space of optimal solutions can be significantly compressed using principal component analysis (PCA). It then proposes Compact Learning, a new method that learns in a subspace of the principal components before translating the vectors into the original output space. This compression reduces the number of trainable parameters substantially, improving scalability and effectiveness. Compact Learning is evaluated on a variety of test cases from the PGLib with up to 30,000 buses. The paper also shows that the output of Compact Learning can be used to warm-start an exact AC solver to restore feasibility, while bringing significant speed-ups.
    
[^101]: 基于回归设置下的分裂学习中标签推断攻击研究

    Label Inference Attack against Split Learning under Regression Setting. (arXiv:2301.07284v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2301.07284](http://arxiv.org/abs/2301.07284)

    本文研究了在回归模型下分裂学习中标签推断攻击的问题。现有的攻击只适用于离散标签的分类模型，而本文提出了一种新的攻击策略以推断连续标签。

    

    作为垂直联邦学习中的关键构建模块，分裂学习(SL)已经在两方模型训练协作中证明了其实践性，其中一方拥有数据样本的特征，另一方拥有相应的标签。这种方法被认为是私有的，因为共享信息仅是嵌入向量和梯度，而不是私有原始数据和标签。但是，一些最近的研究表明，私有标签可以通过梯度泄漏。这些现有的攻击仅适用于分类设置，其中私有标签是离散的。在这项工作中，我们进一步研究了回归模型的情况下泄漏的问题，其中私有标签是连续的数字（而不是分类中的离散标签）。由于输出范围无限，这使得先前的攻击更难推断连续标签。为了解决这个问题，我们提出了一种新颖的基于学习的攻击策略，它整合了梯度。

    As a crucial building block in vertical Federated Learning (vFL), Split Learning (SL) has demonstrated its practice in the two-party model training collaboration, where one party holds the features of data samples and another party holds the corresponding labels. Such method is claimed to be private considering the shared information is only the embedding vectors and gradients instead of private raw data and labels. However, some recent works have shown that the private labels could be leaked by the gradients. These existing attack only works under the classification setting where the private labels are discrete. In this work, we step further to study the leakage in the scenario of the regression model, where the private labels are continuous numbers (instead of discrete labels in classification). This makes previous attacks harder to infer the continuous labels due to the unbounded output range. To address the limitation, we propose a novel learning-based attack that integrates gradie
    
[^102]: 基于梯度的离散MCMC插入即用蛋白质定向进化。

    Plug & Play Directed Evolution of Proteins with Gradient-based Discrete MCMC. (arXiv:2212.09925v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.09925](http://arxiv.org/abs/2212.09925)

    本文介绍了一种基于机器学习的蛋白质工程采样框架，能够在模拟中进化蛋白质，通过组合无监督模型和监督模型来提高评估未见过的突变的能力，并引入了一个使用梯度来提出有前途的突变的快速MCMC采样器。

    

    基于机器学习的蛋白质工程的一个长期目标是加速发现改善已知蛋白质功能的新突变。我们引入了一种采样框架来在模拟中进化蛋白质，支持混合和匹配各种无监督模型(如蛋白质语言模型) 和从序列预测蛋白质功能的监督模型。通过组合这些模型，我们旨在提高评估未见过的突变的能力，并将搜索限制在可能包含功能蛋白的序列空间区域内。我们的框架通过在离散蛋白质空间中直接构建专家分布来实现这一点，而无需任何模型微调或重新训练。我们引入了一个快速的MCMC采样器，使用梯度来提出有前途的突变，而不是使用典型的定向进化方法中的暴力搜索或随机采样。我们在广泛的适应性空间上进行了模拟定向进化实验。

    A long-standing goal of machine-learning-based protein engineering is to accelerate the discovery of novel mutations that improve the function of a known protein. We introduce a sampling framework for evolving proteins in silico that supports mixing and matching a variety of unsupervised models, such as protein language models, and supervised models that predict protein function from sequence. By composing these models, we aim to improve our ability to evaluate unseen mutations and constrain search to regions of sequence space likely to contain functional proteins. Our framework achieves this without any model fine-tuning or re-training by constructing a product of experts distribution directly in discrete protein space. Instead of resorting to brute force search or random sampling, which is typical of classic directed evolution, we introduce a fast MCMC sampler that uses gradients to propose promising mutations. We conduct in silico directed evolution experiments on wide fitness lands
    
[^103]: 多模态和可解释的互联网迷因分类

    Multimodal and Explainable Internet Meme Classification. (arXiv:2212.05612v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.05612](http://arxiv.org/abs/2212.05612)

    本文提出了一种多模态和可解释的互联网迷因分类方法，旨在解决现有方法中忽略迷因语义和创建上下文导致公正内容管理困难的问题。作者采用示例和基于原型的推理并结合文本和视觉SOTA模型进行训练，成功在两个任务中检测了有害的迷因。

    

    在当前的环境中，网络平台已经被有效地武器化，被用于各种地缘政治事件和社会问题中，互联网迷因使得大规模的公正内容管理更加困难。现有的迷因分类和跟踪工作主要采用黑盒方法，没有明确考虑迷因的语义或其创建的上下文。在本文中，我们追求一种模块化和可解释的互联网迷因理解架构。我们设计并实现了多模态分类方法，对训练案例进行示例和基于原型的推理，并利用文本和视觉SOTA模型来表示各个案例。 我们研究了我们的模块化和可解释模型在检测两个现有任务中有害迷因的相关性：仇恨言论检测和厌女症分类。我们比较了基于示例和基于原型的方法以及文本，视觉和多模态模型之间的性能差异在不同的任务上。

    In the current context where online platforms have been effectively weaponized in a variety of geo-political events and social issues, Internet memes make fair content moderation at scale even more difficult. Existing work on meme classification and tracking has focused on black-box methods that do not explicitly consider the semantics of the memes or the context of their creation. In this paper, we pursue a modular and explainable architecture for Internet meme understanding. We design and implement multimodal classification methods that perform example- and prototype-based reasoning over training cases, while leveraging both textual and visual SOTA models to represent the individual cases. We study the relevance of our modular and explainable models in detecting harmful memes on two existing tasks: Hate Speech Detection and Misogyny Classification. We compare the performance between example- and prototype-based methods, and between text, vision, and multimodal models, across differen
    
[^104]: 无标签数据的后门清除

    Backdoor Cleansing with Unlabeled Data. (arXiv:2211.12044v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.12044](http://arxiv.org/abs/2211.12044)

    本文提出了一种无标签数据的后门清除方法，通过逐层权重重新初始化和知识蒸馏来有效清除可疑网络的后门行为，并在基准数据集上取得较好效果。

    

    随着深度神经网络的计算需求增加，公司和组织已经开始外部化训练过程。但是，外部训练的深度神经网络可能会面临后门攻击。因此，关键在于防御这种攻击，即后处理一个可疑模型，使其的后门行为得到缓解，同时其对于干净输入的正常预测能力仍然保持不受影响。为了消除异常的后门行为，现有方法主要依赖于额外的标记干净样本。然而，这样的要求可能是不切实际的，因为训练数据通常对最终用户不可用。本文研究了绕过这种障碍的可能性，并提出了一种新的防御方法，不需要训练标签。通过精心设计的逐层权重重新初始化和知识蒸馏，我们的方法可以有效地清除可疑网络的后门行为，同时对其正常行为的影响微乎其微。我们在基准数据集上评估了我们的方法，并显示它胜过现有的无标签防御方法。

    Due to the increasing computational demand of Deep Neural Networks (DNNs), companies and organizations have begun to outsource the training process. However, the externally trained DNNs can potentially be backdoor attacked. It is crucial to defend against such attacks, i.e., to postprocess a suspicious model so that its backdoor behavior is mitigated while its normal prediction power on clean inputs remain uncompromised. To remove the abnormal backdoor behavior, existing methods mostly rely on additional labeled clean samples. However, such requirement may be unrealistic as the training data are often unavailable to end users. In this paper, we investigate the possibility of circumventing such barrier. We propose a novel defense method that does not require training labels. Through a carefully designed layer-wise weight re-initialization and knowledge distillation, our method can effectively cleanse backdoor behaviors of a suspicious network with negligible compromise in its normal beh
    
[^105]: LP-BFGS攻击：基于Hessian矩阵的有限像素敌对样本攻击

    LP-BFGS attack: An adversarial attack based on the Hessian with limited pixels. (arXiv:2210.15446v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2210.15446](http://arxiv.org/abs/2210.15446)

    本文针对深度神经网络的对抗攻击，基于有限像素的Hessian矩阵信息，提出了Limited Pixel BFGS（LP-BFGS）攻击方法，该方法在攻击成功率和计算成本方面具有竞争优势。

    

    深度神经网络容易遭受对抗攻击。大多数基于$L_0$-norm的白盒攻击是通过计算模型对输入的梯度来构造扰动。由于计算Hessian矩阵的计算成本和内存限制，Hessian或近似Hessian在白盒攻击中的应用逐渐被搁置。本文注意到$L_0$-norm条件自然也要求扰动是稀疏的，因此可以利用Hessian矩阵的信息。我们研究了基于有限扰动像素的Hessian矩阵攻击方法的攻击性能和计算成本。具体而言，我们提出了Limited Pixel BFGS (LP-BFGS)攻击算法，该算法结合了扰动像素选择策略和BFGS算法，将由Integrated Gradient方法计算的前k个像素的表示分数视为优化变量。在不同网络和数据集上的实验结果表明，我们的方法在攻击成功率方面具有竞争优势。

    Deep neural networks are vulnerable to adversarial attacks. Most $L_{0}$-norm based white-box attacks craft perturbations by the gradient of models to the input. Since the computation cost and memory limitation of calculating the Hessian matrix, the application of Hessian or approximate Hessian in white-box attacks is gradually shelved. In this work, we note that the sparsity requirement on perturbations naturally lends itself to the usage of Hessian information. We study the attack performance and computation cost of the attack method based on the Hessian with a limited number of perturbation pixels. Specifically, we propose the Limited Pixel BFGS (LP-BFGS) attack method by incorporating the perturbation pixel selection strategy and the BFGS algorithm. Pixels with top-k attribution scores calculated by the Integrated Gradient method are regarded as optimization variables of the LP-BFGS attack. Experimental results across different networks and datasets demonstrate that our approach ha
    
[^106]: 来自演示的快速生涯适应性逆强化学习

    Fast Lifelong Adaptive Inverse Reinforcement Learning from Demonstrations. (arXiv:2209.11908v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.11908](http://arxiv.org/abs/2209.11908)

    本文提出了一种快速生涯适应性逆强化学习框架，从学习的策略中构建多样策略的组合实现了对新的演示的快速适应，同时整合演示中的共性知识，实现准确的任务推断，还能够在大规模部署中通过维护一个精简的原型策略集合并通过策略组合来逼近所有行为。

    

    演示学习（LfD）方法使终端用户通过所需行为的演示来教授机器人新任务，从而使机器人技术的使用面更广。然而，当前的LfD框架无法快速适应异构的人类演示，也不能在普适的机器人应用中进行大规模部署。在本文中，我们提出了一种新的LfD框架——快速生涯适应性逆强化学习（FLAIR）。我们的方法(1)利用学习策略构建多样策略的组合，从而快速适应新的演示，允许快速的终端用户个性化，(2)整合演示中的共性知识，实现准确的任务推断；(3)在终身部署中只在需要时扩展其模型，通过策略组合维护一个精简的原型策略集合，并能够逼近所有行为。我们在实验证明，FLAIR实现了适应性（即机器人适应了异构的、特定于用户的任务）并且在大规模部署中节省了模型大小。

    Learning from Demonstration (LfD) approaches empower end-users to teach robots novel tasks via demonstrations of the desired behaviors, democratizing access to robotics. However, current LfD frameworks are not capable of fast adaptation to heterogeneous human demonstrations nor the large-scale deployment in ubiquitous robotics applications. In this paper, we propose a novel LfD framework, Fast Lifelong Adaptive Inverse Reinforcement learning (FLAIR). Our approach (1) leverages learned strategies to construct policy mixtures for fast adaptation to new demonstrations, allowing for quick end-user personalization, (2) distills common knowledge across demonstrations, achieving accurate task inference; and (3) expands its model only when needed in lifelong deployments, maintaining a concise set of prototypical strategies that can approximate all behaviors via policy mixtures. We empirically validate that FLAIR achieves adaptability (i.e., the robot adapts to heterogeneous, user-specific task
    
[^107]: 基于聚类的缺失购买者在线实验数据填补方法

    Clustering-based Imputation for Dropout Buyers in Large-scale Online Experimentation. (arXiv:2209.06125v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.06125](http://arxiv.org/abs/2209.06125)

    本文提出一种基于聚类方法的在线实验数据填补方法，将不完整指标值的用户分为访客和缺失购买者两组，使用$k$-最近邻填补方法，并考虑实验特定的特征和用户的购物路径活动，同时使用分层和聚类结合的方式提高填补效率。

    

    在线实验中，合适的度量指标（比如购买）可以提供支持假设和增强决策过程的强有力证据。但是，在线实验中经常出现不完整的度量指标，使得可用数据比计划的在线实验（比如A/B测试）要少得多。在这项工作中，我们引入了缺失购买者的概念，并将指标值不完整的用户分为两组：访客和缺失购买者。为了分析不完整的指标，我们提出了一种基于聚类的$k$-最近邻填补方法。我们提出的填补方法考虑了实验特定的特征和用户沿购物路径的活动，允许不同的用户有不同的填补值。为了方便地填补在线实验中大规模数据集，所提出的方法使用分层和聚类结合的方式。所提出方法的性能与现有的比较方法相比较为优。

    In online experimentation, appropriate metrics (e.g., purchase) provide strong evidence to support hypotheses and enhance the decision-making process. However, incomplete metrics are frequently occurred in the online experimentation, making the available data to be much fewer than the planned online experiments (e.g., A/B testing). In this work, we introduce the concept of dropout buyers and categorize users with incomplete metric values into two groups: visitors and dropout buyers. For the analysis of incomplete metrics, we propose a clustering-based imputation method using $k$-nearest neighbors. Our proposed imputation method considers both the experiment-specific features and users' activities along their shopping paths, allowing different imputation values for different users. To facilitate efficient imputation of large-scale data sets in online experimentation, the proposed method uses a combination of stratification and clustering. The performance of the proposed method is compar
    
[^108]: 图形的排序和聚类方法之间的一致性

    Consistency between ordering and clustering methods for graphs. (arXiv:2208.12933v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.12933](http://arxiv.org/abs/2208.12933)

    本文研究了基于谱技术的几种聚类和排序方法之间的关系，提出了一种新的度量方式——标签连续误差，并通过合成和真实世界数据集的评估，探究了排序方法和聚类方法分别识别模块结构和带状结构的效果。

    

    通过聚类或排序的方式给每个元素分配标签来分析关系数据集。尽管聚类和排序方法都能实现相似的数据集描述，但前者比后者更活跃地研究了这一领域，特别是对于以图形表示的数据。本研究通过调查几种聚类和排序方法之间的方法关系，重点关注谱技术，填补了这一空白。此外，我们评估了聚类和排序方法的结果性能。为此，我们提出了一种称为标签连续误差的度量方式，它通常量化了一组元素的序列和分区之间的一致性程度。基于合成和真实世界的数据集，我们评估了排序方法识别模块结构和聚类方法识别带状结构的程度。

    A relational dataset is often analyzed by optimally assigning a label to each element through clustering or ordering. While similar characterizations of a dataset would be achieved by both clustering and ordering methods, the former has been studied much more actively than the latter, particularly for the data represented as graphs. This study fills this gap by investigating methodological relationships between several clustering and ordering methods, focusing on spectral techniques. Furthermore, we evaluate the resulting performance of the clustering and ordering methods. To this end, we propose a measure called the label continuity error, which generically quantifies the degree of consistency between a sequence and partition for a set of elements. Based on synthetic and real-world datasets, we evaluate the extents to which an ordering method identifies a module structure and a clustering method identifies a banded structure.
    
[^109]: Retweet-BERT：基于语言特征和社交网络信息扩散的政治倾向检测

    Retweet-BERT: Political Leaning Detection Using Language Features and Information Diffusion on Social Networks. (arXiv:2207.08349v3 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2207.08349](http://arxiv.org/abs/2207.08349)

    Retweet-BERT是一个简单而可扩展的模型，用于估计Twitter用户的政治倾向。该模型利用转发网络结构和用户语言特征，并在COVID-19和2020年美国总统选举数据集上展现出有竞争力的性能。研究还表明，在Twitter上存在着右倾用户之间的政治回音室。

    

    估计社交媒体用户的政治倾向是一个具有挑战性和越来越紧迫的问题，因为社交媒体消费量的增加。我们引入了Retweet-BERT，这是一个简单而可扩展的模型，用于估计Twitter用户的政治倾向。Retweet-BERT利用转发网络结构和用户个人资料描述中使用的语言。我们的假设来自于网络和语言同质性的模式，这些模式在那些分享相似意识形态的人们中非常普遍。Retweet-BERT在两个最近的Twitter数据集（一个COVID-19数据集和一个2020年美国总统选举数据集）上展现出与其他最新基线的有竞争力的性能，实现了96%-97%的宏F1。我们还进行了手动验证，以验证Retweet-BERT在训练数据之外的用户上的性能。最后，在COVID-19的案例研究中，我们展示了Twitter上政治回音室的存在，并表明这主要存在于右倾用户之间。我们的代码已开源。

    Estimating the political leanings of social media users is a challenging and ever more pressing problem given the increase in social media consumption. We introduce Retweet-BERT, a simple and scalable model to estimate the political leanings of Twitter users. Retweet-BERT leverages the retweet network structure and the language used in users' profile descriptions. Our assumptions stem from patterns of networks and linguistics homophily among people who share similar ideologies. Retweet-BERT demonstrates competitive performance against other state-of-the-art baselines, achieving 96%-97% macro-F1 on two recent Twitter datasets (a COVID-19 dataset and a 2020 United States presidential elections dataset). We also perform manual validation to validate the performance of Retweet-BERT on users not in the training data. Finally, in a case study of COVID-19, we illustrate the presence of political echo chambers on Twitter and show that it exists primarily among right-leaning users. Our code is 
    
[^110]: 利用图卷积神经网络进行渗透模型的机器学习

    Machine learning of percolation models using graph convolutional neural networks. (arXiv:2207.03368v2 [cond-mat.stat-mech] UPDATED)

    [http://arxiv.org/abs/2207.03368](http://arxiv.org/abs/2207.03368)

    该论文利用图卷积神经网络以监督和非监督方式研究渗透问题，并成功地训练了不同晶格类型的数据，同时，结合混淆法，实现了对渗透阈值的预测。

    

    渗透是气候、物理、材料科学、流行病学、金融等领域中的一个重要主题。使用机器学习方法预测渗透阈值仍然具有挑战性。本文使用图卷积神经网络以监督和非监督方式研究渗透。从监督学习的角度来看，图卷积神经网络同时正确地训练不同晶格类型（如正方形和三角形晶格）的数据。从非监督的角度来看，将图卷积神经网络与混淆法相结合，可以通过“W”形性能获得渗透阈值。本文的发现开辟了建立一个可以探测渗透现象的更广泛框架的可能性。

    Percolation is an important topic in climate, physics, materials science, epidemiology, finance, and so on. Prediction of percolation thresholds with machine learning methods remains challenging. In this paper, we build a powerful graph convolutional neural network to study the percolation in both supervised and unsupervised ways. From a supervised learning perspective, the graph convolutional neural network simultaneously and correctly trains data of different lattice types, such as the square and triangular lattices. For the unsupervised perspective, combining the graph convolutional neural network and the confusion method, the percolation threshold can be obtained by the "W" shaped performance. The finding of this work opens up the possibility of building a more general framework that can probe the percolation-related phenomenon.
    
[^111]: 消除噪声的MDPs：学习比现实世界本身更好的世界模型

    Denoised MDPs: Learning World Models Better Than the World Itself. (arXiv:2206.15477v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.15477](http://arxiv.org/abs/2206.15477)

    本文提出一种新的去噪MDP学习方法，该方法可以将现实数据中的噪声干扰因素去除，学习一个更好的世界模型，实验结果表明该方法在任务上表现更加优秀。

    

    分离信号与噪声，并能理性地把握有效信息对于智能至关重要。这使得人类可以高效地完成现实任务而不必考虑所有可能的烦琐因素。那么，人工智能代理如何才能做到这一点？代理可以放弃哪些信息以避免噪声的干扰？本文基于可控性和与奖励的关系将信息分为四类，并将有用信息定义为既可控制又与奖励相关的信息。该框架澄清了各种强化学习（RL）中的表示学习的先前工作删除的信息类型，并导致我们提出了学习消除某些噪声干扰的去噪MDP的方法。在DeepMind控制套件和RoboDesk的各种变体上进行的广泛实验表明，与仅使用原始观测数据以及先前的工作相比，我们的去噪世界模型表现出卓越的性能。

    The ability to separate signal from noise, and reason with clean abstractions, is critical to intelligence. With this ability, humans can efficiently perform real world tasks without considering all possible nuisance factors.How can artificial agents do the same? What kind of information can agents safely discard as noises?  In this work, we categorize information out in the wild into four types based on controllability and relation with reward, and formulate useful information as that which is both controllable and reward-relevant. This framework clarifies the kinds information removed by various prior work on representation learning in reinforcement learning (RL), and leads to our proposed approach of learning a Denoised MDP that explicitly factors out certain noise distractors. Extensive experiments on variants of DeepMind Control Suite and RoboDesk demonstrate superior performance of our denoised world model over using raw observations alone, and over prior works, across policy opt
    
[^112]: 利用TROPOMI卫星数据对单个船只的NO2排放进行监测

    Supervised segmentation of NO2 plumes from individual ships using TROPOMI satellite data. (arXiv:2203.06993v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.06993](http://arxiv.org/abs/2203.06993)

    本文提出了一种基于卷积神经网络从TROPOMI卫星数据中分割出单个船只NO2排放的监督方法，为基于遥感的全球排放监测系统提供了可能。

    

    航运业是最强的人类排放NOx物质的行业之一，而这种物质对人类健康和环境都有害。该行业的迅速增长导致社会对控制船只排放水平的压力增加。目前用于船舶排放监测的所有方法都很昂贵且需要接近船只，这使得全球和连续的排放监测变得不可能。一种有希望的方法是应用遥感技术。研究表明，利用Copernicus Sentinel 5光学载荷上的TROPOspheric Monitoring Instrument测量可以视觉上区别出单个船只的某些NO2排放。为了部署基于遥感的全球排放监测系统，需要自动化程序来估算单个船只的NO2排放。由于可用数据的信噪比极低且缺乏地面真实性，这是一个充满挑战的任务。本文提出了一种监督分割方法，该方法采用卷积神经网络，能够从TROPOMI卫星数据中准确地识别单个船只的NO2排放。

    The shipping industry is one of the strongest anthropogenic emitters of $\text{NO}_\text{x}$ -- substance harmful both to human health and the environment. The rapid growth of the industry causes societal pressure on controlling the emission levels produced by ships. All the methods currently used for ship emission monitoring are costly and require proximity to a ship, which makes global and continuous emission monitoring impossible. A promising approach is the application of remote sensing. Studies showed that some of the $\text{NO}_\text{2}$ plumes from individual ships can visually be distinguished using the TROPOspheric Monitoring Instrument on board the Copernicus Sentinel 5 Precursor (TROPOMI/S5P). To deploy a remote sensing-based global emission monitoring system, an automated procedure for the estimation of $\text{NO}_\text{2}$ emissions from individual ships is needed. The extremely low signal-to-noise ratio of the available data as well as the absence of ground truth makes th
    
[^113]: 机器学习评估电动汽车电池二次利用的可行性

    Evaluating feasibility of batteries for second-life applications using machine learning. (arXiv:2203.04249v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2203.04249](http://arxiv.org/abs/2203.04249)

    本文使用机器学习技术对电动汽车电池进行快速评估，以判断其是否适合进行二次利用。所提出的算法使用电池电压和电流等特征，并采用高斯过程回归进行预测，验证结果显示其在不同条件下均有希望得到较好的性能表现。

    

    本文提出了一种结合机器学习技术的方法，以便及时评估退役电动汽车电池是否适合进行二次利用，以扩展其原本的使用寿命，或者将其送到回收设施。所提出的算法利用简单的统计学应用从可用的电池电流和电压测量中生成特征，并使用相关性分析选择并排序特征，同时采用增强的高斯过程回归进行预测。该方法已在包括慢充和快充、不同阴极化学组成以及多样化操作条件下的超过200个电池的公开老化数据集上进行验证。在多个训练-测试划分的情况下，观察到有希望的结果，在最坏情况下，均方根百分比误差和平均百分比误差性能误差的均值分别小于1.48％和1.29％。

    This paper presents a combination of machine learning techniques to enable prompt evaluation of retired electric vehicle batteries as to either retain those batteries for a second-life application and extend their operation beyond the original and first intent or send them to recycle facilities. The proposed algorithm generates features from available battery current and voltage measurements with simple statistics, selects and ranks the features using correlation analysis, and employs Gaussian Process Regression enhanced with bagging. This approach is validated over publicly available aging datasets of more than 200 cells with slow and fast charging, with different cathode chemistries, and for diverse operating conditions. Promising results are observed based on multiple training-test partitions, wherein the mean of Root Mean Squared Percent Error and Mean Percent Error performance errors are found to be less than 1.48% and 1.29%, respectively, in the worst-case scenarios.
    
[^114]: 利用PSSM和词嵌入预测甲型流感病毒宿主

    Predicting Influenza A Viral Host Using PSSM and Word Embeddings. (arXiv:2201.01140v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2201.01140](http://arxiv.org/abs/2201.01140)

    该研究利用机器学习模型和特征提取方法，成功预测了甲型流感病毒的原始宿主，为早期和快速控制病毒传播提供了帮助。

    

    流感病毒的快速突变威胁公共健康，可能引发致命的大流行病。然而，检测病毒的原始宿主在爆发期间或爆发后是困难的，因为流感病毒可以在不同的物种之间循环传播。因此，早期和快速检测病毒宿主将有助于减少病毒的进一步传播。我们使用基于位置特异性得分矩阵（PSSM）和学习自词嵌入和词编码的特征的各种机器学习模型来推断病毒的原始宿主。结果表明，基于PSSM的模型的性能达到了95%左右的MCC和96%左右的F1。使用词嵌入的模型得到的MCC约为96％，F1约为97％。

    The rapid mutation of the influenza virus threatens public health. Reassortment among viruses with different hosts can lead to a fatal pandemic. However, it is difficult to detect the original host of the virus during or after an outbreak as influenza viruses can circulate between different species. Therefore, early and rapid detection of the viral host would help reduce the further spread of the virus. We use various machine learning models with features derived from the position-specific scoring matrix (PSSM) and features learned from word embedding and word encoding to infer the origin host of viruses. The results show that the performance of the PSSM-based model reaches the MCC around 95%, and the F1 around 96%. The MCC obtained using the model with word embedding is around 96%, and the F1 is around 97%.
    
[^115]: 面向自监督学习的对象感知裁剪

    Object-Aware Cropping for Self-Supervised Learning. (arXiv:2112.00319v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2112.00319](http://arxiv.org/abs/2112.00319)

    本文提出了一种针对真实世界数据集中多个小对象的自监督学习方法，该方法将随机裁剪替换为对象提议算法获得的裁剪。

    

    自监督学习的一个核心组成部分是数据增强的裁剪方法，它选择一个图像的子区域作为自监督损失中的正样本视角。其基本假设是在给定图像的随机裁剪和缩放的区域中，关于感兴趣的对象有共同信息，且所学的表示能够捕捉到这些信息。这种假设在ImageNet等有很大、位于中央的对象的数据集中得到了基本满足，但在OpenImages或COCO等更贴近真实世界非筛选数据的数据集中，通常有多个小对象。在这项工作中，我们表明基于随机裁剪的自监督学习对这样的数据集效果很差。因此，我们提出将随机裁剪中的一个或两个替换为通过对象提议算法获得的裁剪。这鼓励模型同时学习对象感知能力和特征表达。

    A core component of the recent success of self-supervised learning is cropping data augmentation, which selects sub-regions of an image to be used as positive views in the self-supervised loss. The underlying assumption is that randomly cropped and resized regions of a given image share information about the objects of interest, which the learned representation will capture. This assumption is mostly satisfied in datasets such as ImageNet where there is a large, centered object, which is highly likely to be present in random crops of the full image. However, in other datasets such as OpenImages or COCO, which are more representative of real world uncurated data, there are typically multiple small objects in an image. In this work, we show that self-supervised learning based on the usual random cropping performs poorly on such datasets. We propose replacing one or both of the random crops with crops obtained from an object proposal algorithm. This encourages the model to learn both obje
    
[^116]: UBnormal: 监督式开放集视频异常检测的新基准(arXiv:2111.08644v3 [cs.CV] UPDATED)

    UBnormal: New Benchmark for Supervised Open-Set Video Anomaly Detection. (arXiv:2111.08644v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2111.08644](http://arxiv.org/abs/2111.08644)

    UBnormal是一个新的监督式开放集Benchmark，它引入了用于视频异常检测的像素级异常事件注释。与现有数据集不同，它可以使用全监督学习方法进行异常事件检测。这使得它成为一个标准的开放集问题，与其他基准不同。

    

    在视频中检测异常事件通常被构建为一个单类分类任务，其中训练视频仅包含正常事件，而测试视频包含正常和异常事件。在这种情况下，异常检测是一个开放集问题。但是，一些研究将异常检测与动作识别相同。这是一个闭集场景，无法测试系统检测新异常类型的能力。为此，我们提出了UBnormal，一个由多个虚拟场景组成的新的监督式开放集Benchmark，用于视频异常检测。与现有数据集不同，我们在训练时引入了像素级异常事件注释，首次使异常事件检测可以使用全监督学习方法。为了保持典型的开放集公式，我们确保在视频的训练和测试集合中包含不相交的异常类型集合。据我们所知，UBnormal是第一个视频异常检测基准。

    Detecting abnormal events in video is commonly framed as a one-class classification task, where training videos contain only normal events, while test videos encompass both normal and abnormal events. In this scenario, anomaly detection is an open-set problem. However, some studies assimilate anomaly detection to action recognition. This is a closed-set scenario that fails to test the capability of systems at detecting new anomaly types. To this end, we propose UBnormal, a new supervised open-set benchmark composed of multiple virtual scenes for video anomaly detection. Unlike existing data sets, we introduce abnormal events annotated at the pixel level at training time, for the first time enabling the use of fully-supervised learning methods for abnormal event detection. To preserve the typical open-set formulation, we make sure to include disjoint sets of anomaly types in our training and test collections of videos. To our knowledge, UBnormal is the first video anomaly detection benc
    
[^117]: DeLag：使用多目标优化增强服务系统中延迟降级模式检测

    DeLag: Using Multi-Objective Optimization to Enhance the Detection of Latency Degradation Patterns in Service-based Systems. (arXiv:2110.11155v4 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2110.11155](http://arxiv.org/abs/2110.11155)

    DeLag是一种新颖的自动化搜索方法，使用多目标优化以诊断服务系统中的延迟降级模式，其效果优于现有方法和通用的机器学习聚类算法。

    

    在现代服务系统中，性能调试是一项基本活动。性能问题的诊断通常耗费大量时间，因为需要详细检查大量的跟踪和性能指标。本文提出了DeLag，一种新颖的自动化搜索方法，用于诊断服务系统中的性能问题。DeLag通过识别显示出潜在相关性能问题症状的请求子集来检测潜在的性能问题。我们称这些症状为延迟降级模式。DeLag在同时优化精度、召回率和延迟相似度的基础上搜索多个延迟降级模式。实验结果表明，DeLag比三种最先进的方法和通用的机器学习聚类算法提供更好和更稳定的效果。

    Performance debugging in production is a fundamental activity in modern service-based systems. The diagnosis of performance issues is often time-consuming, since it requires thorough inspection of large volumes of traces and performance indices. In this paper we present DeLag, a novel automated search-based approach for diagnosing performance issues in service-based systems. DeLag identifies subsets of requests that show, in the combination of their Remote Procedure Call execution times, symptoms of potentially relevant performance issues. We call such symptoms Latency Degradation Patterns. DeLag simultaneously searches for multiple latency degradation patterns while optimizing precision, recall and latency dissimilarity. Experimentation on 700 datasets of requests generated from two microservice-based systems shows that our approach provides better and more stable effectiveness than three state-of-the-art approaches and general purpose machine learning clustering algorithms. DeLag is 
    
[^118]: 带熵正则化的约束马尔可夫决策过程的双重方法研究

    A Dual Approach to Constrained Markov Decision Processes with Entropy Regularization. (arXiv:2110.08923v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.08923](http://arxiv.org/abs/2110.08923)

    本文研究了带熵正则化的约束马尔可夫决策过程，并提出了一种加速双下降方法，证明了其在最优间隙和约束违反方面的全局收敛速度为$\widetilde{\mathcal {O}}(1/T)$。

    

    本文研究了在soft-max参数化下的带熵正则化的约束马尔可夫决策过程(CMDPs)，其中智能体旨在最大化熵正则化价值函数的同时满足对期望总效用的约束。通过利用熵正则化，我们的理论分析表明它的Lagrangian对偶函数是平滑的，Lagrangian对偶间隙可以分解为原始最优间隙和约束违规。此外，我们提出了一种加速双下降方法用于带熵正则化的CMDPs。我们证明了我们的方法在熵正则化CMDPs的最优间隙和约束违反方面达到全局收敛速率$\widetilde{\mathcal {O}}(1/T)$。还提供了关于单约束CMDPs的线性收敛速率的讨论。

    We study entropy-regularized constrained Markov decision processes (CMDPs) under the soft-max parameterization, in which an agent aims to maximize the entropy-regularized value function while satisfying constraints on the expected total utility. By leveraging the entropy regularization, our theoretical analysis shows that its Lagrangian dual function is smooth and the Lagrangian duality gap can be decomposed into the primal optimality gap and the constraint violation. Furthermore, we propose an accelerated dual-descent method for entropy-regularized CMDPs. We prove that our method achieves the global convergence rate $\widetilde{\mathcal{O}}(1/T)$ for both the optimality gap and the constraint violation for entropy-regularized CMDPs. A discussion about a linear convergence rate for CMDPs with a single constraint is also provided.
    
[^119]: 神经算子：学习函数空间之间的映射

    Neural Operator: Learning Maps Between Function Spaces. (arXiv:2108.08481v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.08481](http://arxiv.org/abs/2108.08481)

    本文提出了一种神经算子，可以学习无限维函数空间之间的映射，可以逼近给定的非线性连续算子且离散不变，同时介绍了四类高效参数化的神经算子。这项技术的一个重要应用是学习代理映射f。

    

    传统神经网络的发展主要集中在学习有限维欧氏空间或有限集之间的映射上。我们提出了一种神经网络的泛化，称为神经算子，用于学习在无限维函数空间之间映射的算子。我们将神经算子构建为线性积分算子和非线性激活函数的组合。我们证明了我们提出的神经算子的通用逼近定理，表明它可以逼近任何给定的非线性连续算子。所提出的神经算子也是离散不变的，即它们在不同的基础函数空间离散化之间共享相同的模型参数。此外，我们介绍了四类高效参数化的神经算子，即图神经算子、多极图神经算子、低秩神经算子和傅里叶神经算子。神经算子的重要应用之一是学习代理映射f。

    The classical development of neural networks has primarily focused on learning mappings between finite dimensional Euclidean spaces or finite sets. We propose a generalization of neural networks to learn operators, termed neural operators, that map between infinite dimensional function spaces. We formulate the neural operator as a composition of linear integral operators and nonlinear activation functions. We prove a universal approximation theorem for our proposed neural operator, showing that it can approximate any given nonlinear continuous operator. The proposed neural operators are also discretization-invariant, i.e., they share the same model parameters among different discretization of the underlying function spaces. Furthermore, we introduce four classes of efficient parameterization, viz., graph neural operators, multi-pole graph neural operators, low-rank neural operators, and Fourier neural operators. An important application for neural operators is learning surrogate maps f
    
[^120]: 无似然假设下基于频率学派推断：具有正确条件覆盖的置信区间

    Likelihood-Free Frequentist Inference: Confidence Sets with Correct Conditional Coverage. (arXiv:2107.03920v6 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2107.03920](http://arxiv.org/abs/2107.03920)

    本文提出了无似然假设下的频率学派推断（LF2I）框架，通过结合经典统计和现代机器学习，实现了构建具有正确条件覆盖的置信区间的实用程序和诊断方法，在包括宇宙学参数推断在内的多个例子中都实现了覆盖性质得到大幅改善。

    

    许多科学领域都广泛使用计算机模拟器以隐含复杂系统的似然函数。传统的统计方法并不适用于这些称为无似然假设下推断（LFI）的情况，尤其是在渐近和低维的条件下。虽然新的机器学习方法，如归一化流，已经革新了LFI方法的样本效率和容量，但它们是否能为小样本大小产生具有正确条件覆盖的置信区间，仍然是一个开放问题。本文将经典统计和现代机器学习相结合，提出了（i）具有有限样本保证名义覆盖的内曼区间建设的实用程序，以及（ii）估计整个参数空间的条件覆盖的诊断。我们将我们的框架称为无似然假设下的频率学派推断（LF2I）。我们的框架可以使用定义测试统计量的任何方法，如似然比，因此具有广泛的适用性。我们将我们的方法应用于几个合成和实际的例子，包括宇宙学参数推断，并证明与现有的LFI方法相比，覆盖性质得到了大幅改善。

    Many areas of science make extensive use of computer simulators that implicitly encode likelihood functions of complex systems. Classical statistical methods are poorly suited for these so-called likelihood-free inference (LFI) settings, particularly outside asymptotic and low-dimensional regimes. Although new machine learning methods, such as normalizing flows, have revolutionized the sample efficiency and capacity of LFI methods, it remains an open question whether they produce confidence sets with correct conditional coverage for small sample sizes. This paper unifies classical statistics with modern machine learning to present (i) a practical procedure for the Neyman construction of confidence sets with finite-sample guarantees of nominal coverage, and (ii) diagnostics that estimate conditional coverage over the entire parameter space. We refer to our framework as likelihood-free frequentist inference (LF2I). Any method that defines a test statistic, like the likelihood ratio, can 
    

