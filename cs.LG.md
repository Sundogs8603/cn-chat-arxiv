# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Online Learning under Adversarial Nonlinear Constraints.](http://arxiv.org/abs/2306.03655) | 提出了一种在线学习算法CVV-Pro，可以处理对抗性的时变和非线性约束，只依赖于局部稀疏线性逼近，达到了$\sqrt{T}$遗憾率和$1/\sqrt{T}$的收敛速度。 |
| [^2] | [Supervised Knowledge May Hurt Novel Class Discovery Performance.](http://arxiv.org/abs/2306.03648) | 本文研究了监督知识在不同语义关联层面上的帮助作用，并提出了转移流度量标准。实验结果表明，使用此度量标准进行新类别发现，性能与语义相似性一致，监督知识不一定总是有帮助作用。 |
| [^3] | [Proximal Symmetric Non-negative Latent Factor Analysis: A Novel Approach to Highly-Accurate Representation of Undirected Weighted Networks.](http://arxiv.org/abs/2306.03647) | 本文提出了一种Proximal Symmetric Non-negative Latent Factor Analysis (PSNL)模型，用于处理无定向加权网络的信息表征问题，该模型在考虑到对称性和数据密度的同时，还具有高计算效率和表征精度。 |
| [^4] | [Dance Generation by Sound Symbolic Words.](http://arxiv.org/abs/2306.03646) | 本研究提出了一种使用拟声词生成舞蹈的新方法，能够打破语言和文化的限制，使更多人能够接触多样化的舞蹈创作。 |
| [^5] | [Provable convergence guarantees for black-box variational inference.](http://arxiv.org/abs/2306.03638) | 本文提出了一种基于密集高斯变分族的梯度估计器，在此基础上使用近端和投影随机梯度下降，提供了黑盒变分推断收敛于逼真推断问题的第一个严格保证。 |
| [^6] | [Understanding Progressive Training Through the Framework of Randomized Coordinate Descent.](http://arxiv.org/abs/2306.03626) | 该论文提出了一种名为RPT的算法，通过随机坐标下降框架理解了渐进式训练，并对其进行了收敛性分析，为一般光滑目标函数提供了可靠理论保证。 |
| [^7] | [Fair and Robust Estimation of Heterogeneous Treatment Effects for Policy Learning.](http://arxiv.org/abs/2306.03625) | 本文提出了一种公平且健壮的异质治疗效果的估计框架，可以在公平约束下非参数地估计，并可用于权衡公平和最大福利之间的关系。 |
| [^8] | [Spike-based computation using classical recurrent neural networks.](http://arxiv.org/abs/2306.03623) | 本文提出了一种新的脉冲神经网络方法，通过修改一种易于训练的循环神经网络的动态特性，使其产生基于脉冲的计算，并在进行了脉冲网络的训练后，在多个数据集上取得了最先进的性能。 |
| [^9] | [Zero-shot Preference Learning for Offline RL via Optimal Transport.](http://arxiv.org/abs/2306.03615) | 本文提出了一种利用源任务中的标签偏好数据来推断目标任务标签的零样本偏好学习算法，使用Gromov-Wasserstein距离对齐任务之间的轨迹分布。通过交替进行偏好推断和策略优化，同时改进推断出的标签并训练策略的方式，避免了需要大量人工标记的挑战。 |
| [^10] | [Buying Information for Stochastic Optimization.](http://arxiv.org/abs/2306.03607) | 本论文研究了如何购买信息用于随机优化，设计了一个2竞争性的确定性算法和一个e/(e-1)竞争性的随机算法来购买信息，证明了这个比率是最紧的，还考虑了一个自适应设置。 |
| [^11] | [Avoid Adversarial Adaption in Federated Learning by Multi-Metric Investigations.](http://arxiv.org/abs/2306.03600) | 本文提出了一个新概念——强适应对手，并通过实验表明，现有的防御方法不足以解决现实世界中的对手和数据分布问题。作者使用多度量调查来增强 FL 对这些对手的抵抗力。 |
| [^12] | [How does over-squashing affect the power of GNNs?.](http://arxiv.org/abs/2306.03589) | 本文通过测量节点之间成对交互的水平，提供了严格的分析，以确定具有一定容量的MPNN可以学习哪些节点特征的函数类别。结果表明，为了保证节点对之间的充分通信，MPNN的容量必须是... |
| [^13] | [L-C2ST: Local Diagnostics for Posterior Approximations in Simulation-Based Inference.](http://arxiv.org/abs/2306.03580) | 本文提出了一种名为 L-C2ST 的基于本地诊断实现模拟推断中后验近似的新方法，其可以在任何给定的观测下本地评估后验估计器，有效地解决了目前评估后验估计器限制解决方法的问题。 |
| [^14] | [Personalization Disentanglement for Federated Learning.](http://arxiv.org/abs/2306.03570) | 本文通过使用联邦双变分自编码器（FedDVA）明确分解潜在表示，捕捉共享知识和客户特定个性化，从而导致更可靠和有效的个性化联邦学习，并在广泛实验中验证了该方法的优越性。 |
| [^15] | [Memory-Based Dual Gaussian Processes for Sequential Learning.](http://arxiv.org/abs/2306.03566) | 本论文提出了一种基于记忆的双高斯过程用于序列学习的方法，能够控制误差并改善学习。 |
| [^16] | [CIN++: Enhancing Topological Message Passing.](http://arxiv.org/abs/2306.03561) | CIN++是一种拓扑信息传递方案，通过考虑底层复合体中环之间的相互作用来解决图神经网络在表达能力、处理长程交互和建模高阶结构等方面的局限性。 |
| [^17] | [Machine Unlearning: A Survey.](http://arxiv.org/abs/2306.03558) | 本文综述了机器取消学习技术的关键概念，分类和总结了现有的解决方案。 |
| [^18] | [State Regularized Policy Optimization on Data with Dynamics Shift.](http://arxiv.org/abs/2306.03552) | 本文提出了一种叫做 SRPO (状态规范化策略优化) 的算法，该算法利用训练数据中的稳态分布来规范新环境中的策略，在处理具有不同动态的多个环境时表现优异。 |
| [^19] | [Scalable Concept Extraction in Industry 4.0.](http://arxiv.org/abs/2306.03551) | 本文探讨了将概念提取方法应用于工业4.0场景，并改进了可扩展性，提出了具有可解释性的新概念重要性计算程序，将局部特征与整体图像联系起来。 |
| [^20] | [Learning Dynamical Systems from Noisy Data with Inverse-Explicit Integrators.](http://arxiv.org/abs/2306.03548) | 本文提出了一种新方法——均值反向积分器（MII），它可用于从嘈杂的数据中近似动态系统的向量场，通过与单隐式龙格-库塔方法（MIRK）的连接，显式地获得损失函数表达式，实现解锁被初值问题描述的对称和高阶积分器，与仅使用数值积分器的方式相比，通过将MIRK应用于MII的组合方法可以显着降低误差。 |
| [^21] | [How to Select Which Active Learning Strategy is Best Suited for Your Specific Problem and Budget.](http://arxiv.org/abs/2306.03543) | 针对主动学习中如何选择适合特定问题和预算的问题，我们提出了一种导数法实用方法，即动态地识别每个预算的最佳策略。 |
| [^22] | [On Pitfalls of Test-Time Adaptation.](http://arxiv.org/abs/2306.03536) | 这篇论文介绍了测试时间适应（TTA）最近被认为是解决在分布转移情况下鲁棒性挑战的一种很有前途的方法，提出了测试时间适应基准TTAB，并发现了之前方法中的三个常见缺陷。 |
| [^23] | [Continual Learning in Linear Classification on Separable Data.](http://arxiv.org/abs/2306.03534) | 本文从理论上探究了在可分数据上的线性分类持续学习，使得弱正则化学习退化为解决连续最大间隔问题；提出在循环和随机排序任务下的遗忘和其他量感兴趣的上限，并探讨了对训练方法的实际影响。 |
| [^24] | [BackpropTools: A Fast, Portable Deep Reinforcement Learning Library for Continuous Control.](http://arxiv.org/abs/2306.03530) | BackpropTools是一款快速、可移植的连续控制深度强化学习库，它通过模板元编程提供紧密集成的可组合组件，并在异构平台集合上无缝使用，同时在连续控制问题的深度RL代理高效可扩展训练方面具有优势。由于其可移植性和实时保证，它成为了在嵌入式设备上部署学来的策略的有价值的工具。 |
| [^25] | [Rec4Ad: A Free Lunch to Mitigate Sample Selection Bias for Ads CTR Prediction in Taobao.](http://arxiv.org/abs/2306.03527) | 提出了一种名为Rec4Ad的方法，用于消除淘宝广告CTR预测中的样本选择偏差。它利用推荐日志构建合成样本来模拟推断集的分布，然后将合成和原始样本组合以减少SSB。实验结果表明，Rec4Ad在模型性能和校准方面显著优于几种最先进的方法。 |
| [^26] | [A Functional Data Perspective and Baseline On Multi-Layer Out-of-Distribution Detection.](http://arxiv.org/abs/2306.03522) | 本文提出了一种基于网络的函数数据视角的原创方法，利用样本通过各层的轨迹及其统计上的依赖关系，优于现有最先进方法，实现了多层次带基准的ODD检测。 |
| [^27] | [Machine learning in and out of equilibrium.](http://arxiv.org/abs/2306.03521) | 该研究使用Fokker-Planck方法从统计物理学的角度探讨了训练神经网络算法与自然过程之间的相似之处，并验证了其在非平衡系统中的应用，对于研究神经网络的状态及其训练有着重要的意义。 |
| [^28] | [COPR: Consistency-Oriented Pre-Ranking for Online Advertising.](http://arxiv.org/abs/2306.03516) | 该论文提出了一种面向一致性的在线广告预排名框架，利用了一个基于块的采样模块和一个即插即用的排名对齐模块，来显式优化ECPM排名结果的一致性。他们采用了基于Delta NDCG的加权机制，以更好地区分重要性。 |
| [^29] | [Logic Diffusion for Knowledge Graph Reasoning.](http://arxiv.org/abs/2306.03515) | 该篇论文提出了一种名为逻辑扩散（LoD）的插件模块，解决了现有推理模型受训练样本限制、表现不够强的问题。LoD通过关系扩散、随机游走子逻辑采样和梯度自适应等方式实现了对未见查询的发现和不同模式之间的动态平衡，并配备了特殊的损失函数以实现稳健的逻辑扩散。 |
| [^30] | [Subgraph Networks Based Contrastive Learning.](http://arxiv.org/abs/2306.03506) | 本文提出了一种新的对比学习框架，名为基于子图网络的对比学习(SGNCL)，通过应用子图网络生成策略以产生增强视图，并探究了子结构相互作用对图形表示的影响。 |
| [^31] | [Russo-Ukrainian War: Prediction and explanation of Twitter suspension.](http://arxiv.org/abs/2306.03502) | 本研究分析了Twitter封禁机制，揭示了存在的政策违规、宣传、垃圾邮件等问题，并发现拥有更多粉丝的账户更可能被封禁。这些发现可以让Twitter和其他社交网络改进其内容过滤机制。 |
| [^32] | [Transition role of entangled data in quantum machine learning.](http://arxiv.org/abs/2306.03481) | 本研究证明了纠缠数据对量子机器学习的性能具有双重效应，有助于减少预测误差和减小训练数据大小，为量子机器学习模型设计提供了指南。 |
| [^33] | [GSHOT: Few-shot Generative Modeling of Labeled Graphs.](http://arxiv.org/abs/2306.03480) | GSHOT是一个用于少样本标记图生成建模的元学习框架，通过学习从类似的辅助图数据集中转移元知识，从而快速适应未见过的图数据集。 |
| [^34] | [Natural Language Commanding via Program Synthesis.](http://arxiv.org/abs/2306.03460) | 通过使用大型语言模型和办公室领域特定语言，语义解释器实现了自然语言命令并执行Office应用程序中的用户意图。 |
| [^35] | [Benchmarking Robustness of AI-enabled Multi-sensor Fusion Systems: Challenges and Opportunities.](http://arxiv.org/abs/2306.03454) | 本篇论文研究了AI多传感器融合系统的鲁棒性问题，特别关注在安全关键的环境中的表现和可靠性，并提出了基于AI的MSF感知系统基准测试套件。 |
| [^36] | [GRAFENNE: Learning on Graphs with Heterogeneous and Dynamic Feature Sets.](http://arxiv.org/abs/2306.03447) | GRAFENNE是一种新的图神经网络框架，通过在原图上进行异构转化，将节点和特征解耦，解决了现有方法普遍存在的特征静态、转移误差等问题，并且能适用于未知节点和特征。 |
| [^37] | [Quantifying the Variability Collapse of Neural Networks.](http://arxiv.org/abs/2306.03440) | 本文提出了一种叫做可变性崩溃指数（VCI）的新指标，用于量化神经网络的可变性崩溃现象，其优越性质包括在可逆线性变换下的不变性和数值稳定性，此指标可以指示预训练神经网络中的可变性崩溃和可转移性。 |
| [^38] | [Large Language Models of Code Fail at Completing Code with Potential Bugs.](http://arxiv.org/abs/2306.03438) | 本研究探讨了存在漏洞的代码补全问题，设计了两个数据集并发现这些漏洞显著降低了Code-LLMs的生成性能。 |
| [^39] | [Protecting the Intellectual Property of Diffusion Models by the Watermark Diffusion Process.](http://arxiv.org/abs/2306.03436) | 本文提出了一种新的水印扩散过程来保护扩散模型的知识产权，该过程通过训练或微调扩散模型来学习，不同于任务数据的标准扩散过程，能够在不降低性能的情况下提取出嵌入的水印。 |
| [^40] | [On the Role of Attention in Prompt-tuning.](http://arxiv.org/abs/2306.03435) | 本论文研究了Prompt-tuning在注意力架构中的应用，通过探索上下文混合模型，表明softmax-prompt-attention在表达上优于其他模型，同时也证明了该方法可以高效的使用数据学习提示。 |
| [^41] | [Learning-Based Heuristic for Combinatorial Optimization of the Minimum Dominating Set Problem using Graph Convolutional Networks.](http://arxiv.org/abs/2306.03434) | 本论文提出了一种采用图卷积网络计算最小支配集问题的学习启发式方法，取得了胜过传统算法的成果，并且在不同数据集上表现出很好的泛化能力。 |
| [^42] | [Learning to Simulate Tree-Branch Dynamics for Manipulation.](http://arxiv.org/abs/2306.03410) | 本论文提出了一种仿真驱动的逆推理方法来学习树枝动态，并可以操纵可变形的植被，以解决密集植被中容易遮挡的任务，算法结合了生物学上的假设和传统参数推理方法的有限差分方案。 |
| [^43] | [Agents Explore the Environment Beyond Good Actions to Improve Their Model for Better Decisions.](http://arxiv.org/abs/2306.03408) | 通过探索未被访问的决策树状态和引入随机性，MuZero智能体改进了树搜索规划和模型预测之间的不一致性，提高了决策能力。 |
| [^44] | [Deep neural networks architectures from the perspective of manifold learning.](http://arxiv.org/abs/2306.03406) | 本文从几何学和拓扑学的角度，使用拓扑数据分析和持久同调分形维度对神经网络体系结构进行全面比较和描述，旨在为可解释和可解释的人工智能的发展做出贡献。 |
| [^45] | [Vehicle Dynamics Modeling for Autonomous Racing Using Gaussian Processes.](http://arxiv.org/abs/2306.03405) | 本文介绍了一种使用高斯过程回归逼近车辆动力学模型的方法，在高速自主赛车中，平衡计算需求和模型精度后，该方法可行。 |
| [^46] | [SGAT4PASS: Spherical Geometry-Aware Transformer for PAnoramic Semantic Segmentation.](http://arxiv.org/abs/2306.03403) | 本论文提出了SGAT4PASS，一种面向球面几何意识的全景语义分割Transformer，通过加入球面几何感知的约束，能更好地捕捉全景图像的3D属性，从而提高分割性能。 |
| [^47] | [Binary Classification with Instance and Label Dependent Label Noise.](http://arxiv.org/abs/2306.03402) | 本文研究解决带有实例和标签相关的标签噪声对于二分类问题的困难，通过理论分析得到经验风险最小化可以实现最优的超额风险界限。 |
| [^48] | [A Lightweight Method for Tackling Unknown Participation Probabilities in Federated Averaging.](http://arxiv.org/abs/2306.03401) | 本文提出了一种轻量级方法来调整联邦平均中的聚合权重，通过根据每个客户的参与历史来处理具有不同参与率的客户，解决了在联邦学习中未知参与概率的问题。 |
| [^49] | [G-CAME: Gaussian-Class Activation Mapping Explainer for Object Detectors.](http://arxiv.org/abs/2306.03400) | G-CAME 提出了一种面向目标检测的高斯类激活映射解释器，通过使用激活映射与高斯核生成显著性图来突出显示图像中与预测框相关的重要区域，具有很短时间解释对象等优点。 |
| [^50] | [Minimum intrinsic dimension scaling for entropic optimal transport.](http://arxiv.org/abs/2306.03398) | 该研究提出了最小内在维度缩放现象，在不做出数据分布假设的情况下，可以应用于各种熵优化输运问题中，以达到更优的结果。 |
| [^51] | [Origin-Destination Network Generation via Gravity-Guided GAN.](http://arxiv.org/abs/2306.03390) | 本文结合物理科学知识和数据驱动的ML方法，提出了一种名为ODGN的模型来生成起始-目的地网络，以更好地进行人口流动建模。 |
| [^52] | [Online Tensor Learning: Computational and Statistical Trade-offs, Adaptivity and Optimal Regret.](http://arxiv.org/abs/2306.03372) | 本文提出了在线黎曼梯度下降算法，用于在在线情况下估计潜在的低秩张量。其中，我们在处理连续或分类变量时提供了灵活的方法，并在在线情况下尝试了两个具体的应用，即在线张量补全和在线二元张量学习。我们还建立了逐个条目的精确错误界限，这是在在线张量补全中首次纳入噪声。我们观察到，在存在噪声的情况下，计算和统计方面存在着令人惊讶的权衡。 |
| [^53] | [Learning Representations on the Unit Sphere: Application to Online Continual Learning.](http://arxiv.org/abs/2306.03364) | 该论文提出了一种基于单位球的表示学习方法，通过将表示推向固定方向，使得学习策略对数据漂移具有弹性，从而能够应对在线连续学习的挑战性问题。 |
| [^54] | [Boosting Offline Reinforcement Learning with Action Preference Query.](http://arxiv.org/abs/2306.03362) | 本文提出了一种名为Offline-with-Action-Preferences（OAP）的无交互训练方案，通过查询先前收集的和学习到的行动之间的偏好，来帮助解决错误估计问题，从而获得对未见数据更精确的评估。 |
| [^55] | [Vid2Act: Activate Offline Videos for Visual RL.](http://arxiv.org/abs/2306.03360) | Vid2Act是一种基于模型的强化学习方法，它通过使用世界模型来传输领域相关的动态和策略，从而显著提高了样本效率。 |
| [^56] | [Query Complexity of Active Learning for Function Family With Nearly Orthogonal Basis.](http://arxiv.org/abs/2306.03356) | 该论文分析了近正交基函数族的主动学习查询复杂度，提出了相应的算法优化方案。 |
| [^57] | [BatchSampler: Sampling Mini-Batches for Contrastive Learning in Vision, Language, and Graphs.](http://arxiv.org/abs/2306.03355) | 本文提出了一个新的对比学习方法BatchSampler，通过从输入数据中采样难以区分的实例的小批量，并利用重启随机游走来形成小批量，以提高性能。 |
| [^58] | [Stabilizing Contrastive RL: Techniques for Offline Goal Reaching.](http://arxiv.org/abs/2306.03346) | 本文提出了一种稳定的对比强化学习方法，通过浅而宽的结构，结合谨慎的权重初始化和数据增强等实验方法，在具有挑战性的仿真基准测试中显著提高了性能，并演示了对比方法可以解决现实世界的机器人任务。 |
| [^59] | [Inference-Time Intervention: Eliciting Truthful Answers from a Language Model.](http://arxiv.org/abs/2306.03341) | 本研究提出推理时间干预（ITI）技术，通过在推理过程中跨越有限数量的注意力头，显着提高大型语言模型的真实性。在TruthfulQA基准上，ITI使LLaMA模型的真实性从32.5%提高到65.1%。ITI是一种最小程度的干扰，计算廉价，且数据效率高。 |
| [^60] | [Unraveling Projection Heads in Contrastive Learning: Insights from Expansion and Shrinkage.](http://arxiv.org/abs/2306.03335) | 本文研究了对比学习中的投影头，在理论和实践中找到了两个关键效应：信号方向的扩展和收缩，提出了一系列线性变换来改善下游分类准确性。 |
| [^61] | [A Robust Likelihood Model for Novelty Detection.](http://arxiv.org/abs/2306.03331) | 提出了一种鲁棒性高的新颖性检测似然模型，以防御攻击，并取得了优秀的实验结果。 |
| [^62] | [AVIDa-hIL6: A Large-Scale VHH Dataset Produced from an Immunized Alpaca for Predicting Antigen-Antibody Interactions.](http://arxiv.org/abs/2306.03329) | 这是一个大规模的VHH数据集，用于预测抗原-抗体相互作用。通过利用VHHs的结构，该数据集包括573,891个抗原-VHH对，并且是当前公开数据集中最大、最全面的之一。 |
| [^63] | [Stochastic Multi-Level Compositional Optimization Algorithms over Networks with Level-Independent Convergence Rate.](http://arxiv.org/abs/2306.03322) | 本文提出了面向网络的随机多层组合优化算法，其中包括两种新的分散式优化算法，可实现独立于层数的收敛速度，理论结果和实验证明它们更加有效。 |
| [^64] | [Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents.](http://arxiv.org/abs/2306.03314) | 本文提出了一种新的框架，利用多智能体系统发挥大型语言模型的能力，解决循环问题、安全风险、可扩展性、系统评估以及道德考虑等挑战，提供一个结合大型语言模型和多智能体系统优势的方式，更高效和有效地处理复杂任务。 |
| [^65] | [Learning Embeddings for Sequential Tasks Using Population of Agents.](http://arxiv.org/abs/2306.03311) | 该研究基于代理人群体提出了一个信息理论框架，用于在强化学习任务中学习固定维度的嵌入，可以通过观察代理在一小组任务上的表现，来预测其在测试任务上的表现，并且可以从给定的任务选项中选择具有所需特征的任务。 |
| [^66] | [Global universal approximation of functional input maps on weighted spaces.](http://arxiv.org/abs/2306.03303) | 本文提出了功能性输入神经网络，可以在带权重空间上完成全局函数逼近。这一方法适用于连续函数的推广，还可用于路径空间函数的逼近，同时也可以逼近线性函数签名。 |
| [^67] | [Estimating Conditional Mutual Information for Dynamic Feature Selection.](http://arxiv.org/abs/2306.03301) | 本文提出了一种动态特征选择方法，该方法基于特征与响应变量的互信息进行优先级排序，并设计了估计互信息的判别式方法。同时，本文还引入了多项改进措施以应对更多场景。 |
| [^68] | [Switching Autoregressive Low-rank Tensor Models.](http://arxiv.org/abs/2306.03291) | 该文提出了一种切换自回归低秩张量（SALT）模型，它将自回归隐Markov模型（ARHMM）和切换线性动态系统（SLDS）的优点结合起来，通过低秩参数化提高了模型性能。 |
| [^69] | [Deep Learning From Crowdsourced Labels: Coupled Cross-entropy Minimization, Identifiability, and Regularization.](http://arxiv.org/abs/2306.03288) | 本文提出了使用众包标签进行深度学习的方法，通过耦合交叉熵最小化和正则化使学习过程更加鲁棒，同时提出了性能保证。 |
| [^70] | [Survival Instinct in Offline Reinforcement Learning.](http://arxiv.org/abs/2306.03286) | 离线强化学习算法即使使用错误的奖励标签，也能产生良好的表现和安全的策略，这种鲁棒性属性是由离线强化学习算法的悲观主义和常见数据收集实践中的偏见之间相互作用的结果，赋予了代理生存本能。 |
| [^71] | [Optimizing Sampling Patterns for Compressed Sensing MRI with Diffusion Generative Models.](http://arxiv.org/abs/2306.03284) | 本论文提出了一种基于扩散生成模型的MRI压缩感知采样模式优化方法，可以只用五张训练图像来学习有效的采样模式，并在不同的解剖结构、加速因子和模式类型下获得具有竞争性的重建效果。 |
| [^72] | [Under-Counted Tensor Completion with Neural Incorporation of Attributes.](http://arxiv.org/abs/2306.03273) | 本文提出了一种联合了低秩张量补全和神经网络学习算法的未计数张量模型，可以从部分观察到的条目中恢复完全计数条目和每个条目的未计数概率。 |
| [^73] | [Brain Tumor Recurrence vs. Radiation Necrosis Classification and Patient Survivability Prediction.](http://arxiv.org/abs/2306.03270) | 该研究提出了具有统计学严谨性的计算模型，旨在区分磁共振成像（MRI）中rBT和RN，并以此来预测GBM患者的生存率。这对促进患者早期接受治疗和获得更好的治疗结果至关重要。 |
| [^74] | [Towards Arbitrarily Expressive GNNs in $O(n^2)$ Space by Rethinking Folklore Weisfeiler-Lehman.](http://arxiv.org/abs/2306.03266) | 本文提出了$(k, t)$-FWL和$k$-FWL+两种方法，理论上证明了它们可以在$O(n^2)$的空间复杂度下，解决图同构问题。 |
| [^75] | [Has the Machine Learning Review Process Become More Arbitrary as the Field Has Grown? The NeurIPS 2021 Consistency Experiment.](http://arxiv.org/abs/2306.03262) | 本研究展示了 NeurIPS 2021 一致性实验结果，该实验发现机器学习审稿过程中的随意性较高，两个委员会对于 23% 的论文存在不同的接受/拒绝推荐，如果重新随机运行审稿过程，接受论文清单中约有一半的论文将会改变。因此，本文强调客观衡量研究质量的困难，建议作者不应因拒绝的作品而过分沮丧。 |
| [^76] | [Generating Private Synthetic Data with Genetic Algorithms.](http://arxiv.org/abs/2306.03257) | 本文提出了一种使用遗传算法生成差分隐私合成数据的方法，避免了一阶优化方法的限制，并展示了生成数据的高准确度和隐私性。 |
| [^77] | [Explaining and Adapting Graph Conditional Shift.](http://arxiv.org/abs/2306.03256) | 本研究通过量化输入特征和输出标签之间的条件偏移量，对图神经网络易受分布偏移影响的问题进行理论分析。研究发现，图形异质性和模型架构都会导致条件偏移，影响性能。作者提出了一种方法，通过条件偏移的估计和最小化来应对这一问题，该方法在节点分类和图分类任务上表现更优。 |
| [^78] | [Probabilistic Unrolling: Scalable, Inverse-Free Maximum Likelihood Estimation for Latent Gaussian Models.](http://arxiv.org/abs/2306.03249) | 概率展开是用于可扩展、无反演最大似然估计的方法，可以在学习潜在高斯模型时比传统的期望最大化算法快一个数量级，而模型性能损失很小。 |
| [^79] | [Understanding the Effectiveness of Early Weight Averaging for Training Large Language Models.](http://arxiv.org/abs/2306.03241) | 本文研究了使用早期权重平均化方法来提高大型语言模型质量的有效性，证明该方法可以加速收敛且测试和零样本泛化效果显著，同时有效缓解了训练中的损失波动问题。 |
| [^80] | [Improving Accelerated Federated Learning with Compression and Importance Sampling.](http://arxiv.org/abs/2306.03240) | 本文提出了利用压缩和重要性抽样提高加速的联邦学习方法，将本地训练、压缩和部分参与结合应用，以获得最先进的收敛结果。 |
| [^81] | [Information Flow Control in Machine Learning through Modular Model Architecture.](http://arxiv.org/abs/2306.03235) | 本文提出了机器学习信息流控制的概念，并通过MoE架构实现了训练数据对模型输出的控制，从而提高了模型准确性。通过在推理时仅基于访问策略启用子集的专家，实现了对安全访问控制的支持。 |
| [^82] | [Discovering Novel Biological Traits From Images Using Phylogeny-Guided Neural Networks.](http://arxiv.org/abs/2306.03228) | 本研究发现一种基于生物进化树的神经网络方法，可以从没有特征标签的生物图像中发现进化特征，能在多个下游任务中产生生物学上有意义的结果。 |
| [^83] | [Structural Re-weighting Improves Graph Domain Adaptation.](http://arxiv.org/abs/2306.03221) | 本文提出了一种名为结构重加权（StruRW）的新方法，用于解决当前图领域自适应（GDA）方法无法处理的条件结构偏移（CSS）问题，并在多个领域得到验证。 |
| [^84] | [Optimal transport for automatic alignment of untargeted metabolomic data.](http://arxiv.org/abs/2306.03218) | 本文提出了一种名为GromovMatcher的算法，通过使用最优输运自动合并LC-MS数据集，可提高数据对齐的准确性和鲁棒性，有效解决代谢组学数据合并的挑战。 |
| [^85] | [End-to-end Differentiable Clustering with Associative Memories.](http://arxiv.org/abs/2306.03209) | 本文提出了基于关联记忆的端到端可微聚类算法ClAM，结合模式完成技术开发自监督聚类损失函数，相对传统算法和最新的连续聚类松弛方案在轮廓系数方面提高了60%。 |
| [^86] | [Nonlinear Distributionally Robust Optimization.](http://arxiv.org/abs/2306.03202) | 本文提出一种新的非线性分布鲁棒优化算法，用于处理一类分布鲁棒优化问题，通过 Gateaux Derivative 处理一般风险度量。经过实验验证，该方法成功处理分布的非线性目标函数。 |
| [^87] | [Lumos in the Night Sky: AI-enabled Visual Tool for Exploring Night-Time Light Patterns.](http://arxiv.org/abs/2306.03195) | 夜Pulse是一个交互式工具，可用于夜间光（NTL）数据的可视化和分析。它可以通过图像分割、聚类和变化模式检测来识别城市发展和扩展模式，并回答有关人口因素、城市边界和异常差异的问题。 |
| [^88] | [Personalized Federated Domain Adaptation for Item-to-Item Recommendation.](http://arxiv.org/abs/2306.03191) | 本论文提出了一种个性化联邦域适应方法，可在缺乏大量关系数据的情况下为项目推荐建立有效的热启动模型。 |
| [^89] | [Flipping Coins to Estimate Pseudocounts for Exploration in Reinforcement Learning.](http://arxiv.org/abs/2306.03186) | 研究提出了利用硬币翻转来推导状态的访问计数，并将其作为强化学习探索奖励，相比以往的方法在多个具有挑战性的任务上表现更好。 |
| [^90] | [Infusing Lattice Symmetry Priors in Attention Mechanisms for Sample-Efficient Abstract Geometric Reasoning.](http://arxiv.org/abs/2306.03175) | LatFormer是一种将格点对称先验融入到注意力掩码中的模型，能够用卷积网络生成软掩码来调整注意力权重。该模型在合成几何推理中取得了较好效果。 |
| [^91] | [Linear Distance Metric Learning.](http://arxiv.org/abs/2306.03173) | 本文介绍了一种线性距离度量学习方法，可以有效地从一个欧几里得度量空间中的数据学习出另一个欧几里得度量空间中的线性映射，即使数据中存在噪声，也可以以任意精度学习真实的线性距离度量，并提供相应的样本复杂度上界。此外，提供了一种有效的低秩模型截断方法，可以保证模型的准确性和精度。 |
| [^92] | [How Can We Train Deep Learning Models Across Clouds and Continents? An Experimental Study.](http://arxiv.org/abs/2306.03163) | 本文通过实验研究，探究了在不同大陆、云供应商和数据中心范围内，使用分布式数据并行点深度学习训练是否是更具成本效益的选择，并比较了其与集中式训练的可扩展性潜力。 |
| [^93] | [On the Role of Entanglement and Statistics in Learning.](http://arxiv.org/abs/2306.03161) | 本论文探讨了量子学习中量子纠缠和统计学的作用，研究了纠缠测量与可分离测量以及纠缠测量与统计测量在学习模型中的区别，证明了QSQ学习与利用纠缠测量的量子学习之间的指数差异。 |
| [^94] | [DISCount: Counting in Large Image Collections with Detector-Based Importance Sampling.](http://arxiv.org/abs/2306.03151) | 这篇论文提出了一种基于检测器重要性采样的大规模图像计数方法DISCount，通过将不完美的检测器与人工筛选相结合，产生无偏估计的计数。该方法可以解决多个空间或时间区域的计数问题，并可根据估计的准确性对筛选样本数和置信区间进行估计和调整。 |
| [^95] | [Transferring Annotator- and Instance-dependent Transition Matrix for Learning from Crowds.](http://arxiv.org/abs/2306.03116) | 本文提出了一个高效的方法来估算特定注释者和特定实例的转移矩阵以及真实标签比例，解决了从众包中学习的标签噪声问题，并在实验中证明了方法的优越性。 |
| [^96] | [AutoExp: A multidisciplinary, multi-sensor framework to evaluate human activities in self-driving cars.](http://arxiv.org/abs/2306.03115) | 本文提出一个多学科、多传感器框架，用于评估自动驾驶汽车中乘客的活动，并在最近实际条件下捕获真实数据来创建数据集。 |
| [^97] | [Synthesizing Affective Neurophysiological Signals Using Generative Models: A Review Paper.](http://arxiv.org/abs/2306.03112) | 本文综述了使用生成模型合成神经生理信号来解决公共情感数据集稀缺性的问题。通过对领域中使用的不同生成模型进行全面分析，我们提供了有关生成模型在情感识别系统应用的优势、挑战和有前途的未来方向的深入见解。 |
| [^98] | [Bootstrapped Training of Score-Conditioned Generator for Offline Design of Biological Sequences.](http://arxiv.org/abs/2306.03111) | 本文提出了一种BootGen算法，使用代理得分函数增强生物序列生成器的训练数据集，并产生多样化的设计，将其应用于优化生物序列，取得了比竞争对手更好的结果。 |
| [^99] | [Machine Learning Force Fields with Data Cost Aware Training.](http://arxiv.org/abs/2306.03109) | 本论文提出了ASTEROID框架，利用便宜而不准确的数据和昂贵而准确的数据来降低MLFF的数据成本。该框架采用了偏差感知损失函数，让MLFF模型在防止过拟合的同时，也能捕捉基础力场的复杂结构。 |
| [^100] | [Data driven localized wave solution of the Fokas-Lenells equation using modified PINN.](http://arxiv.org/abs/2306.03105) | 本研究利用改进的PINN方法基于数据，成功获得了Fokas-Lenells方程中的明暗孤子解，并提出使用守恒量信息的损失函数可以提高预测和精确解的准确性。 |
| [^101] | [CrystalGPT: Enhancing system-to-system transferability in crystallization prediction and control using time-series-transformers.](http://arxiv.org/abs/2306.03099) | 该论文介绍了一种基于时间序列转换的新型框架CrystalGPT，利用变压器算法中的强大转移学习，提高了晶体预测与控制中的系统对系统转移性。与现有技术相比，CrystalGPT的累积误差降低了8倍。 |
| [^102] | [Hiding in Plain Sight: Disguising Data Stealing Attacks in Federated Learning.](http://arxiv.org/abs/2306.03013) | 该研究提出了一个新的恶意服务器攻击框架SEER，可以在联邦学习中窃取用户数据，而且不易被客户端侦测出来。 |
| [^103] | [Nonparametric Iterative Machine Teaching.](http://arxiv.org/abs/2306.03007) | 本文提出了解决非参数目标模型的迭代机器教学问题的方法，包括随机和贪心泛函教学算法。 |
| [^104] | [Time Interpret: a Unified Model Interpretability Library for Time Series.](http://arxiv.org/abs/2306.02968) | Time Interpret是一个基于Captum的模型解释库，专门用于解释时间序列数据，并实现了多种特征归因方法。此外，它还提供了各种PyTorch模型和数据集，以及一组用于评估特征归因的方法。 |
| [^105] | [Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic.](http://arxiv.org/abs/2306.02865) | 该论文提出了 BEE 操作符，通过充分利用过去的成功经验，并保持探索乐观性，解决了离线策略演员-评论家中 Q 值高估与低估问题，提高了策略学习和样本效率。 |
| [^106] | [Towards Better Explanations for Object Detection.](http://arxiv.org/abs/2306.02744) | 该论文提出了一种名为D-CLOSE的方法，用于解释任何目标检测模型的决策，通过使用多个分割级别和一种组合它们的过程，可以提供更好的质量和更少的噪音解释。 |
| [^107] | [Calibrated Stackelberg Games: Learning Optimal Commitments Against Calibrated Agents.](http://arxiv.org/abs/2306.02704) | 本文提出了一种新的校准史塔克伯格博弈（CSG）框架，其中智能体根据校准预测进行最佳响应。同时引入了自适应校准概念，提供精细的任何时候校准保证。在有限CSG中，主体可以获得最优解。 |
| [^108] | [Predicting malaria dynamics in Burundi using deep Learning Models.](http://arxiv.org/abs/2306.02685) | 该论文利用机器学习方法建立疟疾预测模型，成功预测了布隆迪疟疾时空动态，为疟疾防治和干预设计提供了重要依据。 |
| [^109] | [A Generalized Alternating Method for Bilevel Learning under the Polyak-{\L}ojasiewicz Condition.](http://arxiv.org/abs/2306.02422) | 本研究提出了一种基于Polyak-{\L}ojasiewicz条件的双层学习的广义交替方法，即GALET，可以用于解决非凸下层目标的双层问题。该方法可以在$\tilde{\cal O}(\epsilon^{-1})$迭代次数内实现问题的$\epsilon$-静态度量。 |
| [^110] | [Cycle Consistency Driven Object Discovery.](http://arxiv.org/abs/2306.02204) | 该方法通过循环一致性目标的引入，明确优化场景中每个物体应映射到不同槽位的约束，从而实现了在完全无监督的情况下有效地学习发现物体。在实验中表现出了优于现有方法的性能。 |
| [^111] | [Transforming to Yoked Neural Networks to Improve ANN Structure.](http://arxiv.org/abs/2306.02157) | 本文提出了一种叫做YNN的方法，将同一级别的ANN节点连接在一起形成神经模块，解决了普通ANN无法共享信息的缺陷，显著提高了信息传输和性能。 |
| [^112] | [MultiLegalPile: A 689GB Multilingual Legal Corpus.](http://arxiv.org/abs/2306.02069) | MultiLegalPile是一个689GB的多语言法律语料库，包含来自17个司法管辖区的24种语言的不同法律数据源，允许在公平使用下针对预训练NLP模型。该语料库为多语言模型的预训练提供了新的最佳表现，并在LexGLUE上表现最佳。 |
| [^113] | [Provable Dynamic Fusion for Low-Quality Multimodal Data.](http://arxiv.org/abs/2306.02050) | 本文从泛化的角度出发，提供了理论上对可证明动态多模态融合方法的解释。提出了一种质量感知多模态融合（QMF）框架，可以提高分类准确性和模型的鲁棒性。 |
| [^114] | [GAD-NR: Graph Anomaly Detection via Neighborhood Reconstruction.](http://arxiv.org/abs/2306.01951) | 本文提出了一种新的图形异常检测方法GAD-NR，与现有的GAE模型不同的是，GAD-NR采用邻域重构的方式来检测更复杂非聚类的结构异常。 |
| [^115] | [Responsible Design Patterns for Machine Learning Pipelines.](http://arxiv.org/abs/2306.01788) | 本文提出了一种综合框架，将负责任设计模式纳入机器学习流程中，以确保AI系统的伦理性和公正性。这个框架包括新的负责任AI设计模式，并指导AI开发人员、数据科学家和决策者在AI开发和部署中实施伦理实践。 |
| [^116] | [Prediction of Post-Operative Renal and Pulmonary Complications Using Transformers.](http://arxiv.org/abs/2306.00698) | 本文研究使用基于Transformer的模型来预测术后并发症，包括肾衰、肺部问题和住院死亡，并且表现很好。 |
| [^117] | [Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning.](http://arxiv.org/abs/2306.00477) | 本研究尝试实现在预训练语言模型中运用可逆模型实现高效的微调，并发现在初始化微调时保留PLM的起点非常重要。 |
| [^118] | [Learning Gaussian Mixture Representations for Tensor Time Series Forecasting.](http://arxiv.org/abs/2306.00390) | 本文提出了一种新的张量时间序列预测框架GMRL，它可以单独模拟时间、位置和源变量中所暗示的每个异构性组件，相比于最先进的基准方法，本文的方法显示出了优越性。 |
| [^119] | [CapText: Large Language Model-based Caption Generation From Image Context and Description.](http://arxiv.org/abs/2306.00301) | 研究提出了一种基于大型语言模型的图像字幕生成方法，从文本描述和上下文中生成字幕，而不直接处理图像。在CIDEr指标上，优于当前最先进的图像文本对齐模型。 |
| [^120] | [Provable Benefit of Mixup for Finding Optimal Decision Boundaries.](http://arxiv.org/abs/2306.00267) | 本研究证明了使用Mixup训练具有可证实的益处，可以显著降低在更可分离数据分布中寻找最佳决策边界的样本复杂度。 |
| [^121] | [MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training.](http://arxiv.org/abs/2306.00107) | 提出了一个带有大规模自监督训练的音乐理解模型MERT，利用了教师模型并采用了一种优于传统的语音和音频方法的组合方式。 |
| [^122] | [Discovering New Interpretable Conservation Laws as Sparse Invariants.](http://arxiv.org/abs/2305.19525) | 这篇论文介绍了一种名为Sparse Invariant Detector（SID）的算法，它能够自动发现微分方程中的保守律。该算法可以重新发现已知的保守律，甚至发现新的保守律，并且已发现的保守律具有稳健性和可解释性。 |
| [^123] | [Criteria Tell You More than Ratings: Criteria Preference-Aware Light Graph Convolution for Effective Multi-Criteria Recommendation.](http://arxiv.org/abs/2305.18885) | 本文提出了一种面向多准则推荐的标准偏好感知轻量图卷积网络，该方法结合了MC扩展图，可以准确地捕捉用户的标准偏好，并进一步将用户对各个标准的偏好合并到最终的推荐列表中。 |
| [^124] | [PlaNeRF: SVD Unsupervised 3D Plane Regularization for NeRF Large-Scale Scene Reconstruction.](http://arxiv.org/abs/2305.16914) | 本文提出了一种利用SVD无监督三维平面正则化的方法，仅使用RGB图像和语义地图即可改善NeRF的三维结构，有效解决了训练视图的过拟合导致低纹理区域的几何重建不佳的问题。 |
| [^125] | [Linear Neural Network Layers Promote Learning Single- and Multiple-Index Models.](http://arxiv.org/abs/2305.15598) | 本研究探究了过度参数化的深度神经网络的偏见，发现在ReLU网络中添加线性层有助于逼近具有低秩线性算子和低表示成本函数组成的函数，从而得到一个与低维子空间垂直方向近乎恒定的插值函数。 |
| [^126] | [Distribution-Free Matrix Prediction Under Arbitrary Missing Pattern.](http://arxiv.org/abs/2305.11640) | 本文提出了两种实用算法，能够在任意丢失模式下有效地保证覆盖率的有效性，并量化了缺失对预测精度的影响。 |
| [^127] | [From Random Search to Bandit Learning in Metric Measure Spaces.](http://arxiv.org/abs/2305.11509) | 本文介绍了随机搜索及其性能，引入了“散射维度”的概念，描述了底层函数的状态，量化了随机搜索的性能，并证明了在无噪声和有界噪声情况下的输出分别以一定概率收敛到最优值。 |
| [^128] | [Physics Inspired Approaches Towards Understanding Gaussian Processes.](http://arxiv.org/abs/2305.10748) | 本文利用物理学方法分析了高斯过程模型的损失景观，提出了考虑更广泛的ν使得性能更佳的优化方法，同时提供了一种用于评估GP集成效果的方法和基于损失领域的物理属性的投票方法。 |
| [^129] | [Bridging the Gap: Enhancing the Utility of Synthetic Data via Post-Processing Techniques.](http://arxiv.org/abs/2305.10118) | 本文介绍了一种利用生成对抗网络生成合成数据集，并通过三种新颖的后处理技术改进合成数据集质量和多样性的方法。作者称其为Gap Filler (GaFi)流程并在真实图像上进行评估。 |
| [^130] | [Cold PAWS: Unsupervised class discovery and the cold-start problem.](http://arxiv.org/abs/2305.10071) | 本文提出了一种新方法，通过结合自我监督、聚类和流形学习技术，解决冷启动或无监督选择标记问题，并在多个公共数据集上进行了测试，获得了更好的性能。 |
| [^131] | [Seeing is Believing: Brain-Inspired Modular Training for Mechanistic Interpretability.](http://arxiv.org/abs/2305.08746) | BIMT方法使得神经网络更加模块化和可诠释，并且能够直接展示模块化结构，为许多简单任务提供了有用的信息，并可以补充当前的机理解释策略。 |
| [^132] | [Provable Multi-instance Deep AUC Maximization with Stochastic Pooling.](http://arxiv.org/abs/2305.08040) | 本文提出了在多实例学习中使用深度AUC最大化（DAM）的方法，并根据包含大量实例的情况下训练的计算挑战，提出了一种基于方差减少的随机池化方法，使得只需对每个包进行少量采样即可计算MIDAM模型，提高了效率和准确性。 |
| [^133] | [RHINO: Rotated DETR with Dynamic Denoising via Hungarian Matching for Oriented Object Detection.](http://arxiv.org/abs/2305.07598) | 本文提出了一种面向定向目标检测的DINO基线模型RHINO。并通过匈牙利匹配和查询对齐的方式实现动态降噪，解决了重复预测的问题，从而在公共基准测试中达到最先进的性能水平。 |
| [^134] | [Fisher Information Embedding for Node and Graph Learning.](http://arxiv.org/abs/2305.07580) | 本文提出了一种新的基于注意力机制的图节点嵌入框架，可以更好地理解基于注意力机制的GNN。 |
| [^135] | [The Role of Relevance in Fair Ranking.](http://arxiv.org/abs/2305.05608) | 本文结合社会学、信息检索和机器学习公平性的角度和工具，着眼于相关性在公平排序中的应用和作用，并推导出相关性评分应满足的一组期望标准以实现有意义地指导公平干预措施。 |
| [^136] | [Explanation-based Finetuning Makes Models More Robust to Spurious Cues.](http://arxiv.org/abs/2305.04990) | 本文提出一种新型方法——解释性微调，通过让模型在给出答案的同时生成支持该答案的自由文本解释，来减轻LLMs依赖虚假关联，使得模型对虚假提示更加强韧，并具有广泛适用性。 |
| [^137] | [An Evidential Real-Time Multi-Mode Fault Diagnosis Approach Based on Broad Learning System.](http://arxiv.org/abs/2305.00169) | 本文提出了一种基于证据推理算法和广义学习系统的实时多模态故障诊断方法，该方法在更新模型参数和计算效率方面具有优势，并且在基准数据集上取得了比现有方法更好的故障诊断性能。 |
| [^138] | [A transparent approach to data representation.](http://arxiv.org/abs/2304.14209) | 使用二元属性表示模型对Netflix观众对电影的评分数据集进行数据表示，属性易于解释，且需要较少属性即可达到相同水平的误差。 |
| [^139] | [Communication-Constrained Bandits under Additive Gaussian Noise.](http://arxiv.org/abs/2304.12680) | 本文研究了在受限通信和加性高斯噪声下的多臂赌博机问题，提出了一个多阶段赌博算法，并给出了信息理论下限。 |
| [^140] | [Multi-aspect Repetition Suppression and Content Moderation of Large Language Models.](http://arxiv.org/abs/2304.10611) | 本文介绍了一种使用标记和序列级别的不可能性损失，以及在培训期间的重复惩罚、推理和后处理等多层面方法来抑制大型语言模型中的重复，并避免生成攻击性内容的能力。 |
| [^141] | [Domain Generalization for Mammographic Image Analysis via Contrastive Learning.](http://arxiv.org/abs/2304.10226) | 研究人员开发出一种基于对比学习的域泛化方法，通过自监督学习生成多种风格和视角的特征嵌入，进一步微调骨干网络以提高分类任务性能。 |
| [^142] | [Regions of Reliability in the Evaluation of Multivariate Probabilistic Forecasts.](http://arxiv.org/abs/2304.09836) | 本研究通过有限样本和功率分析确定了多元概率时间序列预测评分规则的可靠性区域，并在电力生产问题上评估了结果对真实世界任务的普适性。 |
| [^143] | [Hyperbolic Image-Text Representations.](http://arxiv.org/abs/2304.09172) | 本文提出了一个使用双曲表示捕捉图像和文本层次结构的对比模型MERU，并证明其在多模态任务上与CLIP相当。 |
| [^144] | [Comments on 'Fast and scalable search of whole-slide images via self-supervised deep learning'.](http://arxiv.org/abs/2304.08297) | 对陈等人发表在《自然—生物医学工程》杂志上的“通过自监督深度学习进行快速和可扩展的全幻灯片图像搜索”一文的评论和关切。 |
| [^145] | [G2T: A simple but versatile framework for topic modeling based on pretrained language model and community detection.](http://arxiv.org/abs/2304.06653) | G2T是一种基于预训练语言模型和社区检测的主题建模框架，自动评估表明，G2T在多个数据集上均与当前最先进的方法相比表现更好。 |
| [^146] | [Model sparsification can simplify machine unlearning.](http://arxiv.org/abs/2304.04934) | 本文提出了一种基于模型稀疏化的机器反学习方案，称为prune first, then unlearn和sparsity-aware unlearning。此方案可以提高近似反学习器的多标准反学习性能，并在不同的场景中表现出一致的效果。 |
| [^147] | [ImageCaptioner$^2$: Image Captioner for Image Captioning Bias Amplification Assessment.](http://arxiv.org/abs/2304.04874) | 本文提出了一种新的图像字幕生成器 ImageCaptioner$^2$ ，用于针对图像字幕偏差放大进行评估。 |
| [^148] | [Deep learning for diffusion in porous media.](http://arxiv.org/abs/2304.02104) | 本文研究使用深度学习来预测多孔介质的基本特性，包括孔隙率和有效扩散系数，并通过构建U-Net架构成功重构了多孔介质的几何结构和浓度分布图。 |
| [^149] | [oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes.](http://arxiv.org/abs/2303.17612) | oBERTa是一组易于使用的语言模型，通过改进初始化、蒸馏、剪枝等技术，可以在不需要模型压缩方面的专业知识的情况下提高稀疏迁移学习的效率和准确性。 |
| [^150] | [Prototype-Sample Relation Distillation: Towards Replay-Free Continual Learning.](http://arxiv.org/abs/2303.14771) | 本文提出了一种无需回放以往数据的持续学习方法，通过共同学习特征表示和类原型来避免灾难性遗忘。 |
| [^151] | [Inverse Reinforcement Learning without Reinforcement Learning.](http://arxiv.org/abs/2303.14623) | 该论文提出了一种新的逆强化学习简化方法，通过利用专家的状态分布来减少强化学习子例程的全局探索部分，实现了指数级的加速，大大提高了样本复杂度和时间复杂度的效果。 |
| [^152] | [What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement.](http://arxiv.org/abs/2303.11249) | 本文通过采用量子物理学的理论工具，提出了一种判定数据适合于局部连接神经网络的必要且充分条件，并导出了一种相应的预处理方法。 |
| [^153] | [Graph Neural Rough Differential Equations for Traffic Forecasting.](http://arxiv.org/abs/2303.10909) | 本文提出一种基于图神经粗糙微分方程的交通预测方法(STG-NRDE)，通过两个NRDE进行时空处理并组合起来构成一个框架。实验结果表明，在6个基准数据集和27个基线模型上表现最佳。 |
| [^154] | [Fast Rates for Maximum Entropy Exploration.](http://arxiv.org/abs/2303.08059) | 本文解决了强化学习中稀疏奖励下的最大熵探索问题，提出了两种类型的最大熵探索方法，并提高了其样本复杂度。 |
| [^155] | [Variational formulations of ODE-Net as a mean-field optimal control problem and existence results.](http://arxiv.org/abs/2303.05924) | 本文探讨了ODE-Net在最小化损失函数的同时约束参数ODE的数学问题，并提出了一种测度论均场最优控制问题的形式化表述，并针对线性神经网络证明了最小化器的存在性结果。 |
| [^156] | [Scaling Up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation.](http://arxiv.org/abs/2303.05785) | 本文提出了一种名为RepUX-Net的纯卷积神经网络体系结构，具有简单的大内核块设计，在医学图像分割中具有十分优越的性能。 |
| [^157] | [Ewald-based Long-Range Message Passing for Molecular Graphs.](http://arxiv.org/abs/2303.04791) | 提出了一种Ewald消息传递的非局部傅立叶空间方案来解决消息传递神经网络范式中长程相互作用难以学习的问题，在多个数据集中表现出更强的预测精度和泛化性能。 |
| [^158] | [Exploring the Limits of Model-Targeted Indiscriminate Data Poisoning Attacks.](http://arxiv.org/abs/2303.03592) | 该研究探究了模型定向无差别数据污染攻击的极限。作者引入了模型污染可达性的概念，推导出了易于计算的阈值，并发现了一种惊人的相变现象：只有当污染比超过阈值时，攻击才能实现某些目标参数。 |
| [^159] | [Federated Virtual Learning on Heterogeneous Data with Local-global Distillation.](http://arxiv.org/abs/2303.02278) | 该论文提出了一种名为FedLGD的新方法，通过本地和全局数据集的蒸馏组合来创建一个更小的合成数据集，以解决联邦学习中处理异构数据时的性能问题，同时使用迭代分布匹配来处理同步和类别不平衡问题。 |
| [^160] | [Learning machines for health and beyond.](http://arxiv.org/abs/2303.01513) | 适用于建立预测模型的机器学习技术在医疗领域和其他领域具有广泛应用。模型的维护和监控很关键，因为模型的性能与数据的变化和传输有关。 |
| [^161] | [Safe Peeling for L0-Regularized Least-Squares with supplementary material.](http://arxiv.org/abs/2302.14471) | 引入“安全剥离”方法加速解决L0正则化最小二乘问题，通过收缩松弛度允许更激进的剪枝，显著降低求解时间。 |
| [^162] | [Causal isotonic calibration for heterogeneous treatment effects.](http://arxiv.org/abs/2302.14011) | 提出了因果等保校准方法及其数据有效的变体交叉校准，这两种方法都能快速稳健地校准异质性处理效应的预测器，而且可以应用于任何黑盒学习算法。 |
| [^163] | [PITS: Variational Pitch Inference without Fundamental Frequency for End-to-End Pitch-controllable TTS.](http://arxiv.org/abs/2302.12391) | PITS是一种基于变分推断的端到端音高可控TTS模型，相较于以往基频建模的方法，具有更高的合成语音方差和音高可控性。 |
| [^164] | [Learning Physical Models that Can Respect Conservation Laws.](http://arxiv.org/abs/2302.11002) | 这项工作提出了ProbConserv框架，通过将守恒约束与贝叶斯更新相结合，将守恒约束纳入通用科学机器学习体系结构中，以便在学习高难度的PDE运算中应用。 |
| [^165] | [Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels.](http://arxiv.org/abs/2302.10586) | 本文介绍了一种名为双伪训练（DPT）的训练策略，该策略结合了强大的半监督学习器和扩散模型来进一步推进半监督生成和分类任务。实验结果表明，DPT在各种情况下都能实现半监督生成和分类任务的SOTA性能，特别是在每个类别只有一个或两个标签的情况下，超过了其他一些模型。 |
| [^166] | [Aligning Language Models with Preferences through f-divergence Minimization.](http://arxiv.org/abs/2302.08215) | 本文提出一种新的方法f-DPG，用于对齐语言模型和偏好，该方法适用于评估任何目标分布，统一了现有的各种框架和逼近方法。 |
| [^167] | [Understanding Oversquashing in GNNs through the Lens of Effective Resistance.](http://arxiv.org/abs/2302.06835) | 本文提出了一种通过分析图中节点之间的有效电阻来理解和减轻GNN中过度压缩的方法，并提出了使用总有效电阻作为压缩总量限制的方法，进而开发了一种算法以减轻过度压缩。 |
| [^168] | [ConCerNet: A Contrastive Learning Based Framework for Automated Conservation Law Discovery and Trustworthy Dynamical System Prediction.](http://arxiv.org/abs/2302.05783) | 本文提出了一种基于对比学习的框架ConCerNet，用于提高DNN动力学建模的可靠性，实现对系统不变量的自动捕捉和保留，经实验证明其性能优于传统神经网络方法。 |
| [^169] | [Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples.](http://arxiv.org/abs/2302.04578) | 该论文提出了一种利用对抗样本来保护人类创造的艺术品，对抗侵权者利用未经授权的绘画训练DMs生成类似风格的新颖绘画的方法。 |
| [^170] | [In Search of Insights, Not Magic Bullets: Towards Demystification of the Model Selection Dilemma in Heterogeneous Treatment Effect Estimation.](http://arxiv.org/abs/2302.02923) | 本文研究在具有高风险应用的个性化处理效应估计中，不同模型选择标准的优点和缺点，并提出未来研究方向。 |
| [^171] | [Stochastic Gradient Descent-Induced Drift of Representation in a Two-Layer Neural Network.](http://arxiv.org/abs/2302.02563) | 本文研究了一个两层神经网络中因随机梯度下降而引起的表示漂移问题，发现其前者对应于有限方差波动，而后者可以被视为流形上的有效扩散过程。 |
| [^172] | [Enhancing Exploration in Latent Space Bayesian Optimization.](http://arxiv.org/abs/2302.02399) | 本文提出了一种新的方法来提高潜空间贝叶斯优化（LSBO）的探索能力。方法包括潜在一致性感知获取函数（LCA-AF）和增加一致性点的潜空间生成方法（LCA-VAE），将它们结合起来形成了LCA-LSBO。实验证明LCA-LSBO在图像生成和全新的化学设计任务中表现出改进的性能。 |
| [^173] | [A Theory of Link Prediction via Relational Weisfeiler-Leman.](http://arxiv.org/abs/2302.02209) | 本文提出了一种基于关系Weisfeiler-Leman算法的理论，为知识图谱中的图神经网络提供了理论解释，并对各种模型的表达能力进行了表征，并解释了一些广泛采用的实际设计选择的优点。 |
| [^174] | [Deep Reinforcement Learning for Online Error Detection in Cyber-Physical Systems.](http://arxiv.org/abs/2302.01567) | 本文提出了一种基于深度强化学习（DRL）的新型在线错误检测方法。 |
| [^175] | [The Power of Preconditioning in Overparameterized Low-Rank Matrix Sensing.](http://arxiv.org/abs/2302.01186) | 该研究提出了ScaledGD(𝜆)方法，相较于传统梯度下降法更加鲁棒，并且在处理低秩矩阵感知问题时具有很好的表现。 |
| [^176] | [Revisiting Bellman Errors for Offline Model Selection.](http://arxiv.org/abs/2302.00141) | 本文重新审视 Bellman Errors，发现之前的Bellman Errors 方法需要在特定条件下才能表现良好，同时提出了更准确的 MSBE 估计器，在离散控制任务方面表现出色。 |
| [^177] | [Toward Efficient Gradient-Based Value Estimation.](http://arxiv.org/abs/2301.13757) | 本研究研究了梯度为基础的值估计方法慢的根本原因，并提出了一种低复杂度的方法以解决损失函数带来的不良影响，该方法在效率上比剩余梯度方法更快，几乎具有相同的计算复杂度，并且在经典问题上与TD具有竞争力。 |
| [^178] | [On the Correctness of Automatic Differentiation for Neural Networks with Machine-Representable Parameters.](http://arxiv.org/abs/2301.13370) | 本论文研究了神经网络参数为机器可表示数字时自动微分的正确性问题，证明了神经网络带偏置参数时自动微分始终正确，给出了限制不可微性在激活函数中数目的界，并提供了判断参数是否在不可微参数组中的条件。 |
| [^179] | [Overcoming Simplicity Bias in Deep Networks using a Feature Sieve.](http://arxiv.org/abs/2301.13293) | 提出了一种特征筛选方法，通过抑制网络较低层易计算虚假特征，使得更高层的网络提取和利用更丰富、更有意义的特征表示，从而克服了深度神经网络中的简单偏差。 |
| [^180] | [AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning.](http://arxiv.org/abs/2301.12132) | AutoPEFT是一个自动化的PEFT（参数高效微调）配置搜索方法，它能够自动地找到最佳的PEFT模块和体系结构，以优化任务的性能和参数效率。在典型的NLP任务中，AutoPEFT表现出比手动设计更好的性能。 |
| [^181] | [Direct Parameterization of Lipschitz-Bounded Deep Networks.](http://arxiv.org/abs/2301.11526) | 本文提出了一种直接参数化的深度神经网络，其具有拉普拉斯界限，通过标准梯度方法进行训练，避免了计算密集型的投影或障碍项。 |
| [^182] | [Rigid body flows for sampling molecular crystal structures.](http://arxiv.org/abs/2301.11355) | 本文介绍了一种新型的正则化流，专为三维空间中多个物体的位置和方向建模而设计。通过在单位四元数群上定义平滑和表现力强的流以及定义适当的密度，在旋转群上进行训练，我们可以成功地采样分子晶体结构。 |
| [^183] | [A Watermark for Large Language Models.](http://arxiv.org/abs/2301.10226) | 本文提出了一种在大型语言模型中实现水印技术的方法，该技术可以在不降低文本质量的前提下嵌入信号，且可以使用高效的开源算法进行检测，并且该技术十分鲁棒和安全。 |
| [^184] | [Abstracting Imperfect Information Away from Two-Player Zero-Sum Games.](http://arxiv.org/abs/2301.09159) | 通过正则化均衡，可以将两人零和博弈中的不完美信息抽象出来并作为完全信息问题处理。 |
| [^185] | [Tier Balancing: Towards Dynamic Fairness over Underlying Causal Factors.](http://arxiv.org/abs/2301.08987) | 本研究提出了层次平衡的概念，该概念捕捉了未观察到的潜在因果因素的情况变化，并探讨了动态公平性的实现。在指定的动态下，通常不能通过一步干预来实现长期公平目标。 |
| [^186] | [A Communication-Efficient Adaptive Algorithm for Federated Learning under Cumulative Regret.](http://arxiv.org/abs/2301.08869) | 本文提出了一种通信高效自适应算法于联邦学习中，通过低比特数的传输实现了最优累积损失。 |
| [^187] | [Trustworthiness Score to Evaluate CNNs Predictions.](http://arxiv.org/abs/2301.08839) | 本文提出了一种用于评估CNN预测可信度的可信度分数（TS）度量标准，通过检查CNN所做预测中的某些特征的存在来量化预测的可信度。 |
| [^188] | [Faster Gradient-Free Algorithms for Nonsmooth Nonconvex Stochastic Optimization.](http://arxiv.org/abs/2301.06428) | 本文提出了一种使用随机递归梯度估计器的更高效算法，来解决无平滑非凸随机优化问题，其复杂度为 $\mathcal{O}(L^3 d^{3/2} \epsilon^{-3}+ \Delta L^2 d^{3/2} \delta^{-1} \epsilon^{-3})$。 |
| [^189] | [NarrowBERT: Accelerating Masked Language Model Pretraining and Inference.](http://arxiv.org/abs/2301.04761) | NarrowBERT是一种改进的Transformer编码器，通过稀疏化模型并仅对掩码令牌进行操作，在掩码语言模型预训练中提高了$2\times$以上的吞吐量，并在推理时将吞吐量提高了多达$3.5\times$。NarrowBERT在几种自然语言处理任务中的性能与标准BERT相当。 |
| [^190] | [Navigating Alignment for Non-identical Client Class Sets: A Label Name-Anchored Federated Learning Framework.](http://arxiv.org/abs/2301.00489) | 本文提出了一种名为FedAlign的联邦学习框架，用于在标签和数据的层面上对于非相同客户类别集合进行潜在空间的对齐，从而解决了传统方法中不同客户端本地编码器的异质性问题。 |
| [^191] | [Human-in-the-loop Embodied Intelligence with Interactive Simulation Environment for Surgical Robot Learning.](http://arxiv.org/abs/2301.00452) | 本文介绍了一种基于交互式模拟环境的人机协同体现智能机器人学习的新平台，以及人类专家指导下机器人学习表现的提高和人体示范对手术机器人控制策略的影响。 |
| [^192] | [QuantArt: Quantizing Image Style Transfer Towards High Visual Fidelity.](http://arxiv.org/abs/2212.10431) | QuantArt是一个新的风格转移框架，通过向量量化将生成的艺术品的潜在表示推向真实艺术品分布的中心，从而实现高可视化保真的样式化。该框架实现了显著更高的视觉保真度。 |
| [^193] | [Can In-context Learners Learn a Reasoning Concept from Demonstrations?.](http://arxiv.org/abs/2212.01692) | 本文介绍了一种概念性少样本学习方法，以帮助在场学习者学习新技能。通过选择与预测示例共享可能信息的演示，这个方法可以在模型记忆独立的情况下区分模型的在场学习能力。 |
| [^194] | [Topological Data Analysis for Speech Processing.](http://arxiv.org/abs/2211.17223) | 本论文将拓扑数据分析应用于语音分类问题和预训练语音模型的内省，并介绍了一系列基于Transformer注意力图和嵌入的拓扑和代数特征。在这些特征基础上构建的简单线性分类器胜过精调分类器头部，并实现了在许多数据集上的最新最优性能。拓扑特征能够揭示语音Transformer头的功能角色，这表明TDA是一种有前途的语音分析方法。 |
| [^195] | [ArrayFlex: A Systolic Array Architecture with Configurable Transparent Pipelining.](http://arxiv.org/abs/2211.12600) | 设计了一种可配置透明流水线的并行阵列，ArrayFlex，可以选择每个CNN层的最佳流水线配置，从而缩短推理延迟11％，使用比传统固定流水线阵列少13％的能量。 |
| [^196] | [Retrieval-Augmented Multimodal Language Modeling.](http://arxiv.org/abs/2211.12561) | 本文提出了一种新的基于检索的多模态模型，使其可以从外部记忆检索相关图片和文本信息，以更可扩展和模块化的方式集成知识，提供更高质量的生成结果。 |
| [^197] | [SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models.](http://arxiv.org/abs/2211.10438) | SmoothQuant是一种训练无需的通用后训练量化（PTQ）解决方案，可以在保持精度的情况下实现大型语言模型的8位权重、8位激活（W8A8）量化。SmoothQuant通过数学等效转换将量化难度从激活移到权重，使得所有矩阵乘法的权重和激活的INT8量化成为可能，具有最高1.56倍加速和2倍内存减少的效果。 |
| [^198] | [Machine Learned Calabi-Yau Metrics and Curvature.](http://arxiv.org/abs/2211.09801) | 这篇论文使用机器学习方法构建了逼近Calabi-Yau度量的神经网络模型，研究了光滑和奇异K3曲面和Calabi-Yau三体上的数值Ricci-flat度量，并观察到神经网络模型对于计算出的拓扑特征的稳定性有较大影响。 |
| [^199] | [Empirical Study on Optimizer Selection for Out-of-Distribution Generalization.](http://arxiv.org/abs/2211.08583) | 本实证研究通过测试不同分布偏移下超过20,000个模型的表现，发现自适应优化器（如Adam和Adagrad）及可以减少时间相关特征的优化器具有更好的分布外泛化性能。 |
| [^200] | [When is Realizability Sufficient for Off-Policy Reinforcement Learning?.](http://arxiv.org/abs/2211.05311) | 本文研究了基于模型的算法对于满足贝尔曼完整性条件的要求，提出了新的基于策略外强化学习的有限样本保证。 |
| [^201] | [Can Querying for Bias Leak Protected Attributes? Achieving Privacy With Smooth Sensitivity.](http://arxiv.org/abs/2211.02139) | 本文论证了查询公平指标可能会泄露受保护属性，提出了解决方案以保护隐私。 |
| [^202] | [I Prefer not to Say: Protecting User Consent in Models with Optional Personal Data.](http://arxiv.org/abs/2210.13954) | 该论文研究了个人可以选择与决策系统共享可选个人信息的机器学习模型，并提出了保护用户同意的PUC概念，为用户隐私保护提供了有力的解决方案。 |
| [^203] | [Improving Medical Predictions by Irregular Multimodal Electronic Health Records Modeling.](http://arxiv.org/abs/2210.12156) | 本文利用门控机制和时间注意力机制分别建模不规则时间序列和临床笔记表征，进而通过交替注意力机制整合多模态信息，从而改进医疗预测。 |
| [^204] | [Surgical Fine-Tuning Improves Adaptation to Distribution Shifts.](http://arxiv.org/abs/2210.11466) | 本研究表明，选择性地微调预训练模型的子集层（手术微调）在适应分布偏移的任务中效果更好，在真实数据中得到了验证，还在理论上证明在理想环境下，手术微调可以优于全层微调。 |
| [^205] | ["Why did the Model Fail?": Attributing Model Performance Changes to Distribution Shifts.](http://arxiv.org/abs/2210.10769) | 本文介绍了一种将模型性能变化归因于底层数据生成机制的分布偏移的方法，并通过推导一种重要性权重方法来计算任意一组分布的价值。 |
| [^206] | [A Kernel-Based View of Language Model Fine-Tuning.](http://arxiv.org/abs/2210.05643) | 本文研究神经切线核 (NTK) 在描述预训练语言模型微调过程中的适用性。实验证明在14个NLP任务中使用掩码词预测问题作为下游任务，可以取得好的效果。 |
| [^207] | [Less is More: Task-aware Layer-wise Distillation for Language Model Compression.](http://arxiv.org/abs/2210.01351) | 本文提出了一种新的面向任务的分层蒸馏方法（TED），通过设计任务感知的滤波器来对齐学生和教师的隐藏表示，选择有用的知识，减少知识差距，使学生模型更好地适应目标任务，实现了比最先进方法更少的参数下可比或更好的性能。 |
| [^208] | [Sparsity by Redundancy: Solving $L_1$ with SGD.](http://arxiv.org/abs/2210.01212) | 该论文提出了一种通过冗余重参数化和简单的随机梯度下降来最小化带有$L_1$惩罚的通用可微损失函数的方法，称为\textit{spred}，是$L_1$的精确求解器，可用于训练稀疏神经网络以执行基因选择任务和神经网络压缩任务，弥合了深度学习中的稀疏性和传统统计学习之间的差距。 |
| [^209] | [Block-wise Training of Residual Networks via the Minimizing Movement Scheme.](http://arxiv.org/abs/2210.00949) | 本文提出了一种适用于ResNets的块状训练方法，能够解决端到端反向传播存在的问题，包括受限制的环境下无法加载整个模型以及禁止并行训练各层等，并且能够缓解层次训练中出现的停滞问题，其测试精度与完整模型的端到端训练相竞争。 |
| [^210] | [Transfer Learning for Individual Treatment Effect Estimation.](http://arxiv.org/abs/2210.00380) | 本论文探讨了个体治疗效果估计中迁移因果知识的问题，并提出了一个使用CITA度量进行ITE知识转移的框架，实验证明该方法的有效性。 |
| [^211] | [Entropy-driven Unsupervised Keypoint Representation Learning in Videos.](http://arxiv.org/abs/2209.15404) | 本文提出了一种基于图像空间熵的无监督视频关键点表示学习方法，将视觉特征抽象为关键点的十分简明的表示，并通过局部熵计算的信息熵损失指导模型发现一致的关键点表示。该方法在多种基准数据集上展示了优异的性能，超过了现有最先进的方法。 |
| [^212] | [Fed-CBS: A Heterogeneity-Aware Client Sampling Mechanism for Federated Learning via Class-Imbalance Reduction.](http://arxiv.org/abs/2209.15245) | 本文提出了Fed-CBS机制，通过类别不平衡性降低实现去中心化的学习方法的性能劣化，从而提高了federated learning（FL）在非独立同分布（non-IID）数据上的性能。 |
| [^213] | [Learning to predict 3D rotational dynamics from images of a rigid body with unknown mass distribution.](http://arxiv.org/abs/2209.11355) | 该研究提出了一种基于物理知识的神经网络模型，通过多级预测流程，从刚体图像序列中预测3D旋转动力学，解决了标准深度学习方法无法揭示体内质量分布影响的问题。 |
| [^214] | [SELTO: Sample-Efficient Learned Topology Optimization.](http://arxiv.org/abs/2209.05098) | 本文旨在提出一个高效学习拓扑优化方法，采用物理学预处理和等变网络构建样本高效的深度学习管道，并发布了两个TO数据集，旨在提高比较性和未来的进展。 |
| [^215] | [DL-DRL: A double-level deep reinforcement learning approach for large-scale task scheduling of multi-UAV.](http://arxiv.org/abs/2208.02447) | 提出了一个双层深度强化学习方法(DL-DRL)来解决多UAV大规模任务调度问题，采用分治框架进行任务分配和路径规划，能够显着提高计算效率，优化任务执行效果。 |
| [^216] | [A Survey of Learning on Small Data: Generalization, Optimization, and Challenge.](http://arxiv.org/abs/2207.14443) | 小数据学习综述，包括主动学习、少样本学习、元学习和无监督学习等方面，但它们在泛化性能上缺乏理论保证；考虑到不同目的的多个学习社区都可以产生小数据，我们将小数据分为不同大小和复杂度的光谱。 |
| [^217] | [Homomorphism Autoencoder -- Learning Group Structured Representations from Observed Transitions.](http://arxiv.org/abs/2207.12067) | 本文提出了一种同态自编码器方法，使机器能够在行动中学习到与其行为相一致的感知信息的内部表示，并捕获环境中的转换结构。 |
| [^218] | [When are Post-hoc Conceptual Explanations Identifiable?.](http://arxiv.org/abs/2206.13872) | 本论文提出了可识别的概念发现方法，可以恢复出多个已知的概念，以确保解释的可靠性。对于具有依赖关系的概念，提出了两种新的方法，利用图像生成过程的功能组合性质。该方法明显优于现有方法。 |
| [^219] | [Robust Universal Adversarial Perturbations.](http://arxiv.org/abs/2206.10858) | 本文提出了一种对现实世界变换具有鲁棒性的通用对抗扰动生成算法，并在多个数据集上进行了评估。 |
| [^220] | [Beyond Uniform Lipschitz Condition in Differentially Private Optimization.](http://arxiv.org/abs/2206.10713) | 本文提出了一种新的差分隐私优化算法来处理其它算法无法处理的非均匀李普希茨情形，并且在具体应用中提供了相应的参数调整方案。 |
| [^221] | [Global Context Vision Transformers.](http://arxiv.org/abs/2206.09959) | 提出了全局上下文视觉Transformer (GC ViT) 架构，利用全局上下文自注意力模块和标准的局部自注意力对长距离和短距离空间相互作用进行有效而高效的建模，同时解决了ViTs中缺乏归纳偏差的问题，在图像分类、目标检测和语义分割任务中表现出最先进的结果。 |
| [^222] | [I Know What You Trained Last Summer: A Survey on Stealing Machine Learning Models and Defences.](http://arxiv.org/abs/2206.08451) | 随着机器学习即服务（MLaaS）的普及，用户可以使用最复杂的机器学习模型的预测，但也危及了MLaaS提供商的知识产权，攻击者可以创建一个具备相同行为的模型副本，本文针对模型窃取的攻击和相应的对策进行了综合调查。 |
| [^223] | [A Unification Framework for Euclidean and Hyperbolic Graph Neural Networks.](http://arxiv.org/abs/2206.04285) | 本文提出了一种双曲正规化技术，将双曲线图神经网络和欧几里得图神经网络有机结合起来，以摆脱多个不一致向量空间之间的限制，同时保留双曲线网络模拟分层图结构的表现能力。 |
| [^224] | [A Communication-efficient Algorithm with Linear Convergence for Federated Minimax Learning.](http://arxiv.org/abs/2206.01132) | 本文提出了一种高效的通信算法，用于联邦极小值学习，具有线性收敛特性。研究了重要的特殊情况，并提供了学习该目标的泛化界限。针对联邦设置，提出了Local Stochastic Gradient Descent Ascent (SGDA)算法，可保证收敛性。 |
| [^225] | [Covariance Matrix Adaptation MAP-Annealing.](http://arxiv.org/abs/2205.10752) | 本文提出了一种新的质量多样性算法——协方差矩阵自适应MAP退火（CMA-MAE），用于解决过早地放弃目标以进行探索、难以探索平坦目标以及低分辨率档案性能差等问题，实现了最先进的性能和鲁棒性。 |
| [^226] | [blob loss: instance imbalance aware loss functions for semantic segmentation.](http://arxiv.org/abs/2205.08209) | 该论文提出了一种针对实例不平衡问题的语义分割损失函数——Blob Loss，用于多实例检测，可通过提高F1分数和灵敏度等指标来优化性能。 |
| [^227] | [Optimally tackling covariate shift in RKHS-based nonparametric regression.](http://arxiv.org/abs/2205.02986) | 本文研究了在RKHS的非参数回归中的协变量转移问题，针对两个不同的似然比族，证明了使用KRR估计量具有极小化率最优的特点，尤其是在似然比被均匀有界时。与此同时，本文也证明了，在协变量转移下一个naive的估计器相比于KRR是严格次优的。 |
| [^228] | [Clifford Circuits can be Properly PAC Learned if and only if $\textsf{RP}=\textsf{NP}$.](http://arxiv.org/abs/2204.06638) | 本论文证明了Clifford电路只有在$\textsf{RP}=\textsf{NP}$的情况下才能被适当地PAC学习；如果此条件成立，则可以使用随机多项式时间算法解决半正定规划问题。 |
| [^229] | [Concept-based Explanations for Out-Of-Distribution Detectors.](http://arxiv.org/abs/2203.02586) | 本论文提出了一种基于学习的高级概念提供OOD检测器解释的方法，通过提出新的度量标准，学习一组具有高检测完整性和概念可分离性特征的概念，有效提高了OOD检测器的解释性能。 |
| [^230] | [A Lightweight, Efficient and Explainable-by-Design Convolutional Neural Network for Internet Traffic Classification.](http://arxiv.org/abs/2202.05535) | 本文提出了一种名为LEXNet的轻量级、高效率和可解释的卷积神经网络，通过使用新的残差块和原型层解决了现有流量分类器所面临的问题，并在商业级数据集的测试中表现出更好的准确率和速度。 |
| [^231] | [L-SVRG and L-Katyusha with Adaptive Sampling.](http://arxiv.org/abs/2201.13387) | 本文提出了一种自适应采样策略来提高L-SVRG和L-Katyusha优化方法在训练机器学习模型中的性能表现，可以在少量计算开销内实现采样分布的学习，同时不需要先验知识，并证明了其收敛性保证。 |
| [^232] | [Learning Intuitive Policies Using Action Features.](http://arxiv.org/abs/2201.12658) | 本论文研究了网络体系结构对多智能体协作中学习算法利用动作特征和观测特征之间语义关系的影响，发现联合处理观察特征和动作特征的特征表示的注意力机制架构可以学习直觉策略，并且这样的代理与人类协作而无需接受人类数据训练。 |
| [^233] | [No-Regret Caching via Online Mirror Descent.](http://arxiv.org/abs/2101.12588) | 本文提出了一种基于在线镜像下降算法的缓存算法，具有无遗憾的特性，关键依赖于请求过程的多样性比率。同时，作者证明了在需要整个文件缓存时，也能保证无遗憾性。 |
| [^234] | [A Symmetric Loss Perspective of Reliable Machine Learning.](http://arxiv.org/abs/2101.01366) | 对称损失是一种新型的代理损失，能够使得学习过程对于受损标签更加鲁棒，从而提高分类器的性能。 |
| [^235] | [Growing Efficient Deep Networks by Structured Continuous Sparsification.](http://arxiv.org/abs/2007.15353) | 本文提出一种结构化连续稀疏化的深度网络结构生长方法，通过连续松弛和采样稀疏子网络，可以在训练过程中达到紧凑的修剪网络结构，同时大幅降低计算复杂度并保持较高的准确率。 |
| [^236] | [UnRectDepthNet: Self-Supervised Monocular Depth Estimation using a Generic Framework for Handling Common Camera Distortion Models.](http://arxiv.org/abs/2007.06676) | 本文提出了一个通用的自监督管道用于从未矫正的单目视频中估计深度、欧几里得距离和视觉里程计，其可隐式地组织计算机视觉中的矫正过程，避免了其对模型复杂度或推理时间的影响。 |
| [^237] | [Conditional Sampling with Monotone GANs: from Generative Models to Likelihood-Free Inference.](http://arxiv.org/abs/2006.06755) | 本文提出了一种新的概率测度条件采样框架，使用单调GAN学习块状三角形映射，仅使用来自底层联合概率测度的样本实现无似然推断。 |
| [^238] | [Denise: Deep Robust Principal Component Analysis for Positive Semidefinite Matrices.](http://arxiv.org/abs/2004.13612) | Denise是一种基于深度学习的算法，用于对协方差矩阵进行低秩加稀疏分解，达到了与最先进技术相当的性能而且近乎接近20倍的加速。 |
| [^239] | [Certified Reinforcement Learning with Logic Guidance.](http://arxiv.org/abs/1902.00778) | 本文提出了一种模型无关的强化学习算法，能够使用线性时态逻辑来制定马尔科夫决策过程的目标，将LTL属性转化为LDGBA自动机，通过调整同步奖励函数最大概率获得满足LTL规定要求的控制策略。 |
| [^240] | [Orthogonal Statistical Learning.](http://arxiv.org/abs/1901.09036) | 本文提出了一个两阶段样本拆分的元算法，该算法能够在评估总体风险时考虑干扰参数，并且实现的超额风险界的影响为二次。 |
| [^241] | [Gaussian Error Linear Units (GELUs).](http://arxiv.org/abs/1606.08415) | 本文提出了一种高效的神经网络激活函数——GELU。通过对输入值进行加权而非符号门限控制，GELU在多项任务中均取得了性能提升。 |

# 详细

[^1]: 对抗性非线性约束下的在线学习

    Online Learning under Adversarial Nonlinear Constraints. (arXiv:2306.03655v1 [cs.LG])

    [http://arxiv.org/abs/2306.03655](http://arxiv.org/abs/2306.03655)

    提出了一种在线学习算法CVV-Pro，可以处理对抗性的时变和非线性约束，只依赖于局部稀疏线性逼近，达到了$\sqrt{T}$遗憾率和$1/\sqrt{T}$的收敛速度。

    

    在许多应用程序中，学习系统需要处理连续的非稳态数据流。我们在在线学习框架中研究了这个问题，并提出了一种算法，可以处理对抗性的时变和非线性约束。正如我们在这项工作中所展示的那样，这个名为Constraint Violation Velocity Projection (CVV-Pro)的算法达到了$\sqrt{T}$的遗憾率，并以$1/\sqrt{T}$的速度收敛于可行集，尽管可行集缓慢地随时间变化且不为学习者所知。CVV-Pro仅依赖于可行集的局部稀疏线性逼近，因此避免了在每次迭代中优化整个集合，这与投影梯度或Frank-Wolfe方法形成鲜明对比。我们还在两个玩家游戏中对算法进行了实证评估，其中玩家受到共享约束的限制。

    In many applications, learning systems are required to process continuous non-stationary data streams. We study this problem in an online learning framework and propose an algorithm that can deal with adversarial time-varying and nonlinear constraints. As we show in our work, the algorithm called Constraint Violation Velocity Projection (CVV-Pro) achieves $\sqrt{T}$ regret and converges to the feasible set at a rate of $1/\sqrt{T}$, despite the fact that the feasible set is slowly time-varying and a priori unknown to the learner. CVV-Pro only relies on local sparse linear approximations of the feasible set and therefore avoids optimizing over the entire set at each iteration, which is in sharp contrast to projected gradients or Frank-Wolfe methods. We also empirically evaluate our algorithm on two-player games, where the players are subjected to a shared constraint.
    
[^2]: 监督知识可能会损害新类别发现性能

    Supervised Knowledge May Hurt Novel Class Discovery Performance. (arXiv:2306.03648v1 [cs.LG])

    [http://arxiv.org/abs/2306.03648](http://arxiv.org/abs/2306.03648)

    本文研究了监督知识在不同语义关联层面上的帮助作用，并提出了转移流度量标准。实验结果表明，使用此度量标准进行新类别发现，性能与语义相似性一致，监督知识不一定总是有帮助作用。

    

    新类别发现(NCD)通过利用标记集合的先验知识来推断未标记数据集中的新类别。鉴于大多数现有文献主要关注在方法论层面上利用标记集合的监督知识，本文考虑一个问题：监督知识在不同语义关联层面上总是有帮助的吗？首先，我们提出了一个新的测量标准，所谓的转移流，用于衡量标记/未标记数据集之间的语义相似性。为了展示所提出的度量标准的有效性，我们利用ImageNet的层次分类结构建立了一个带有各种语义相似度程度的大规模基准测试。基于所提出的基准测试的结果表明，所提出的转移流与层次分类结构一致，并且NCD的性能与语义相似性一致(由所提出的转移流度量)。

    Novel class discovery (NCD) aims to infer novel categories in an unlabeled dataset by leveraging prior knowledge of a labeled set comprising disjoint but related classes. Given that most existing literature focuses primarily on utilizing supervised knowledge from a labeled set at the methodology level, this paper considers the question: Is supervised knowledge always helpful at different levels of semantic relevance? To proceed, we first establish a novel metric, so-called transfer flow, to measure the semantic similarity between labeled/unlabeled datasets. To show the validity of the proposed metric, we build up a large-scale benchmark with various degrees of semantic similarities between labeled/unlabeled datasets on ImageNet by leveraging its hierarchical class structure. The results based on the proposed benchmark show that the proposed transfer flow is in line with the hierarchical class structure; and that NCD performance is consistent with the semantic similarities (measured by 
    
[^3]: 近端对称非负潜在因子分析:一种高精度表征无定向加权网络的新方法

    Proximal Symmetric Non-negative Latent Factor Analysis: A Novel Approach to Highly-Accurate Representation of Undirected Weighted Networks. (arXiv:2306.03647v1 [cs.LG])

    [http://arxiv.org/abs/2306.03647](http://arxiv.org/abs/2306.03647)

    本文提出了一种Proximal Symmetric Non-negative Latent Factor Analysis (PSNL)模型，用于处理无定向加权网络的信息表征问题，该模型在考虑到对称性和数据密度的同时，还具有高计算效率和表征精度。

    

    无定向加权网络 (UWN) 在大数据相关应用中很常见。注意到这样一个网络与其节点和边连接的信息可以表示为一个对称的、高维的且不完整的( SHDI)矩阵。然而，现有模型在建模其固有对称性或低数据密度方面均存在问题，导致低模型可扩展性或表征学习能力。为了解决这个问题，提出了一种近端对称的非负潜在因子分析 (PSNL)模型。它将一个接近项纳入考虑对称性和数据密度的目标函数中，以提高表征精度。然后，通过树状的 Parzen 估计器 (TPE)方法实现了自适应交替方向乘子算法 (ADMM)的学习方案，以获得高计算效率。对四个 UWNs 进行的实证研究表明，PSNL 模型实现了比现有最先进模型更高的精度收益，以及高度竞争力的计算复杂性。

    An Undirected Weighted Network (UWN) is commonly found in big data-related applications. Note that such a network's information connected with its nodes, and edges can be expressed as a Symmetric, High-Dimensional and Incomplete (SHDI) matrix. However, existing models fail in either modeling its intrinsic symmetry or low-data density, resulting in low model scalability or representation learning ability. For addressing this issue, a Proximal Symmetric Nonnegative Latent-factor-analysis (PSNL) model is proposed. It incorporates a proximal term into symmetry-aware and data density-oriented objective function for high representation accuracy. Then an adaptive Alternating Direction Method of Multipliers (ADMM)-based learning scheme is implemented through a Tree-structured of Parzen Estimators (TPE) method for high computational efficiency. Empirical studies on four UWNs demonstrate that PSNL achieves higher accuracy gain than state-of-the-art models, as well as highly competitive computati
    
[^4]: 通过声音象征性词语生成舞蹈

    Dance Generation by Sound Symbolic Words. (arXiv:2306.03646v1 [cs.LG])

    [http://arxiv.org/abs/2306.03646](http://arxiv.org/abs/2306.03646)

    本研究提出了一种使用拟声词生成舞蹈的新方法，能够打破语言和文化的限制，使更多人能够接触多样化的舞蹈创作。

    

    本研究引入了一种新颖的方法，使用拟声词作为输入来生成舞蹈动作，旨在增强舞蹈生成的创造力和多样性。与文本和音乐不同，拟声词通过抽象的词语表达传达节奏和含义，没有表达的限制，也不需要专业知识。我们改进了AI编舞师框架并使用Sakamoto系统，一种针对音素和音节的拟声词特征提取方法。此外，我们还通过用户调查收集了一个由40个拟声词-舞蹈动作对组成的新数据集。我们的结果表明，所提出的方法能够实现更直观的舞蹈生成，并能够使用来自多种语言的声音象征性词语创建舞蹈动作，包括那些没有拟声词的语言。这突显了跨语言和文化的多样化舞蹈创作的潜力，让更广泛的受众能够接触到。我们模型的定性样本可在以下网址找到：ht

    This study introduces a novel approach to generate dance motions using onomatopoeia as input, with the aim of enhancing creativity and diversity in dance generation. Unlike text and music, onomatopoeia conveys rhythm and meaning through abstract word expressions without constraints on expression and without need for specialized knowledge. We adapt the AI Choreographer framework and employ the Sakamoto system, a feature extraction method for onomatopoeia focusing on phonemes and syllables. Additionally, we present a new dataset of 40 onomatopoeia-dance motion pairs collected through a user survey. Our results demonstrate that the proposed method enables more intuitive dance generation and can create dance motions using sound-symbolic words from a variety of languages, including those without onomatopoeia. This highlights the potential for diverse dance creation across different languages and cultures, accessible to a wider audience. Qualitative samples from our model can be found at: ht
    
[^5]: 黑盒变分推断的收敛性保证

    Provable convergence guarantees for black-box variational inference. (arXiv:2306.03638v1 [cs.LG])

    [http://arxiv.org/abs/2306.03638](http://arxiv.org/abs/2306.03638)

    本文提出了一种基于密集高斯变分族的梯度估计器，在此基础上使用近端和投影随机梯度下降，提供了黑盒变分推断收敛于逼真推断问题的第一个严格保证。

    

    尽管黑盒变分推断被广泛应用，但没有证明其随机优化成功的证明。我们提出这是现有随机优化证明中的理论差距，即具有异常噪声边界和复合非平滑目标的梯度估计器的挑战。对于密集的高斯变分族，我们观察到现有的基于再参数化的梯度估计器满足二次噪声界，并为使用该界限的近端和投影随机梯度下降提供新的收敛保证。这提供了第一个黑盒变分推断收敛于逼真推断问题的严格保证。

    While black-box variational inference is widely used, there is no proof that its stochastic optimization succeeds. We suggest this is due to a theoretical gap in existing stochastic optimization proofs-namely the challenge of gradient estimators with unusual noise bounds, and a composite non-smooth objective. For dense Gaussian variational families, we observe that existing gradient estimators based on reparameterization satisfy a quadratic noise bound and give novel convergence guarantees for proximal and projected stochastic gradient descent using this bound. This provides the first rigorous guarantee that black-box variational inference converges for realistic inference problems.
    
[^6]: 通过随机坐标下降框架理解渐进式训练

    Understanding Progressive Training Through the Framework of Randomized Coordinate Descent. (arXiv:2306.03626v1 [cs.LG])

    [http://arxiv.org/abs/2306.03626](http://arxiv.org/abs/2306.03626)

    该论文提出了一种名为RPT的算法，通过随机坐标下降框架理解了渐进式训练，并对其进行了收敛性分析，为一般光滑目标函数提供了可靠理论保证。

    

    我们提出了随机渐进式训练算法（RPT）——渐进式训练方法（PT）（Karras等，2017）的随机代理。最初设计用于训练GAN（Goodfellow等，2014），PT被提出作为一种启发式方法，即使对于最简单的目标函数也没有收敛分析。相反，据我们所知，RPT是第一个对于一般光滑目标函数具有严格和可靠理论保证的PT类型算法。我们将我们的方法转化为随机坐标下降（RCD）（Nesterov，2012；Richtárk＆Takáč，2014）的成熟框架，对此提出了一种新颖、简单和通用的收敛性分析，包括强凸、凸和非凸目标。然后我们使用这个框架来建立RPT的收敛理论。最后，我们通过广泛的计算实验证明了我们方法的有效性。

    We propose a Randomized Progressive Training algorithm (RPT) -- a stochastic proxy for the well-known Progressive Training method (PT) (Karras et al., 2017). Originally designed to train GANs (Goodfellow et al., 2014), PT was proposed as a heuristic, with no convergence analysis even for the simplest objective functions. On the contrary, to the best of our knowledge, RPT is the first PT-type algorithm with rigorous and sound theoretical guarantees for general smooth objective functions. We cast our method into the established framework of Randomized Coordinate Descent (RCD) (Nesterov, 2012; Richt\'arik & Tak\'a\v{c}, 2014), for which (as a by-product of our investigations) we also propose a novel, simple and general convergence analysis encapsulating strongly-convex, convex and nonconvex objectives. We then use this framework to establish a convergence theory for RPT. Finally, we validate the effectiveness of our method through extensive computational experiments.
    
[^7]: 公平且健壮的异质治疗效果政策学习估计

    Fair and Robust Estimation of Heterogeneous Treatment Effects for Policy Learning. (arXiv:2306.03625v1 [stat.ME])

    [http://arxiv.org/abs/2306.03625](http://arxiv.org/abs/2306.03625)

    本文提出了一种公平且健壮的异质治疗效果的估计框架，可以在公平约束下非参数地估计，并可用于权衡公平和最大福利之间的关系。

    

    我们提出了一种简单且通用的框架，用于在公平约束条件下非参数估计异质治疗效果。在标准正则条件下，我们证明了所得到的估计器具有双重健壮性。我们利用此框架来表征公平和最佳政策可实现的最大福利之间的权衡。我们在模拟研究中评估了该方法，并在一个真实世界的案例研究中进行了说明。

    We propose a simple and general framework for nonparametric estimation of heterogeneous treatment effects under fairness constraints. Under standard regularity conditions, we show that the resulting estimators possess the double robustness property. We use this framework to characterize the trade-off between fairness and the maximum welfare achievable by the optimal policy. We evaluate the methods in a simulation study and illustrate them in a real-world case study.
    
[^8]: 经典循环神经网络的脉冲计算

    Spike-based computation using classical recurrent neural networks. (arXiv:2306.03623v1 [cs.NE])

    [http://arxiv.org/abs/2306.03623](http://arxiv.org/abs/2306.03623)

    本文提出了一种新的脉冲神经网络方法，通过修改一种易于训练的循环神经网络的动态特性，使其产生基于脉冲的计算，并在进行了脉冲网络的训练后，在多个数据集上取得了最先进的性能。

    

    脉冲神经网络是一种人工神经网络，其中神经元之间的通信仅由事件或所谓的脉冲组成。这种特性使得神经网络能够进行异步和稀疏计算，并因此在专用硬件上运行时大幅减少能源消耗。本文中，我们尝试采用一种对称的方法：修改一种已知的、易于训练的循环神经网络的动态特性，使其产生基于脉冲的计算。通过明确引入脉冲阈值和重置机制，我们使网络能够仅使用脉冲来执行前向和循环计算。然后，我们展示了这种修改后的构架既可以实现，同时在两个基准数据集上实现了最先进的性能，包括具有挑战性的ImageNet数据集。

    Spiking neural networks are a type of artificial neural networks in which communication between neurons is only made of events, also called spikes. This property allows neural networks to make asynchronous and sparse computations and therefore to drastically decrease energy consumption when run on specialized hardware. However, training such networks is known to be difficult, mainly due to the non-differentiability of the spike activation, which prevents the use of classical backpropagation. This is because state-of-the-art spiking neural networks are usually derived from biologically-inspired neuron models, to which are applied machine learning methods for training. Nowadays, research about spiking neural networks focuses on the design of training algorithms whose goal is to obtain networks that compete with their non-spiking version on specific tasks. In this paper, we attempt the symmetrical approach: we modify the dynamics of a well-known, easily trainable type of recurrent neural 
    
[^9]: 通过最优传输实现离线RL的零样本偏好学习

    Zero-shot Preference Learning for Offline RL via Optimal Transport. (arXiv:2306.03615v1 [cs.LG])

    [http://arxiv.org/abs/2306.03615](http://arxiv.org/abs/2306.03615)

    本文提出了一种利用源任务中的标签偏好数据来推断目标任务标签的零样本偏好学习算法，使用Gromov-Wasserstein距离对齐任务之间的轨迹分布。通过交替进行偏好推断和策略优化，同时改进推断出的标签并训练策略的方式，避免了需要大量人工标记的挑战。

    

    基于偏好的强化学习(PbRL)已经在将奖励与人类意图对齐方面表现出了显著的效果。然而，需要大量的人工标记仍然是一个重要的挑战，而且从先前任务中获取的昂贵的偏好数据通常不能重复使用于后续任务学习中，导致需要对每个新任务进行大量的标注。本文提出了一种新颖的零样本基于偏好的RL算法，以利用源任务中的标记偏好数据来推断目标任务的标签，从而消除了对人类查询的要求。我们的方法利用了Gromov-Wasserstein距离来对齐源任务和目标任务之间的轨迹分布。求解的最优传输矩阵作为两个任务之间轨迹对应关系的一个对应关系，使得识别任务之间对应的轨迹对和传递偏好标签成为可能。然而，直接从推断出的标签中学习可能会导致错误的累积。为了缓解这个问题，我们提出了一个交替式的学习框架，交替进行偏好推断和策略优化，同时改进推断出的标签并训练策略。我们的实验结果表明，我们提出的算法在零样本基于偏好的RL方面优于其他几种最先进的算法。

    Preference-based Reinforcement Learning (PbRL) has demonstrated remarkable efficacy in aligning rewards with human intentions. However, a significant challenge lies in the need of substantial human labels, which is costly and time-consuming. Additionally, the expensive preference data obtained from prior tasks is not typically reusable for subsequent task learning, leading to extensive labeling for each new task. In this paper, we propose a novel zero-shot preference-based RL algorithm that leverages labeled preference data from source tasks to infer labels for target tasks, eliminating the requirement for human queries. Our approach utilizes Gromov-Wasserstein distance to align trajectory distributions between source and target tasks. The solved optimal transport matrix serves as a correspondence between trajectories of two tasks, making it possible to identify corresponding trajectory pairs between tasks and transfer the preference labels. However, learning directly from inferred lab
    
[^10]: 购买信息用于随机优化

    Buying Information for Stochastic Optimization. (arXiv:2306.03607v1 [cs.DS])

    [http://arxiv.org/abs/2306.03607](http://arxiv.org/abs/2306.03607)

    本论文研究了如何购买信息用于随机优化，设计了一个2竞争性的确定性算法和一个e/(e-1)竞争性的随机算法来购买信息，证明了这个比率是最紧的，还考虑了一个自适应设置。

    

    随机优化是机器学习和理论计算机科学中的核心问题之一。在标准模型中，算法会提前给出一个固定的分布。但在实践中，可能需要花费额外的代价来获取更好的决策所需的信息。本文研究了如何购买信息以进行随机优化，并将这个问题形式化为一个在线学习问题。假设学习者有原始优化问题的预言机，我们设计了一个 $2$- 竞争性确定性算法和一个 $e/(e-1)$- 竞争性随机算法来购买信息。我们证明了这个比率是最紧的，因为这个问题等价于一个称为超级鞅停止的滑雪租用问题的鲁棒泛化。我们还考虑了一个自适应设置，在这种设置下，学习者可以在对底层优化问题采取一些行动后选择购买信息。我们专注于经典的优化问题，最小化总集覆盖，其中th

    Stochastic optimization is one of the central problems in Machine Learning and Theoretical Computer Science. In the standard model, the algorithm is given a fixed distribution known in advance. In practice though, one may acquire at a cost extra information to make better decisions. In this paper, we study how to buy information for stochastic optimization and formulate this question as an online learning problem. Assuming the learner has an oracle for the original optimization problem, we design a $2$-competitive deterministic algorithm and a $e/(e-1)$-competitive randomized algorithm for buying information. We show that this ratio is tight as the problem is equivalent to a robust generalization of the ski-rental problem, which we call super-martingale stopping.  We also consider an adaptive setting where the learner can choose to buy information after taking some actions for the underlying optimization problem. We focus on the classic optimization problem, Min-Sum Set Cover, where th
    
[^11]: 通过多度量调查避免联邦学习中的对抗适应

    Avoid Adversarial Adaption in Federated Learning by Multi-Metric Investigations. (arXiv:2306.03600v1 [cs.LG])

    [http://arxiv.org/abs/2306.03600](http://arxiv.org/abs/2306.03600)

    本文提出了一个新概念——强适应对手，并通过实验表明，现有的防御方法不足以解决现实世界中的对手和数据分布问题。作者使用多度量调查来增强 FL 对这些对手的抵抗力。

    

    联邦学习 (FL) 可以在分布在多个设备上的数据上训练机器学习模型，避免将数据传输到中央位置。这提高了隐私保护，减少了通信成本，并增强了模型性能。然而，FL 容易受到污染攻击，可以是非定向的，旨在降低模型性能，也可以是有目的的，即所谓的后门攻击，这种攻击会添加对抗性行为，可以通过适当制作的输入触发。为了追求隐蔽性，后门攻击更难应对。对抗性攻击缓解技术依赖于监视某些度量标准和过滤恶意模型更新。然而，以前的工作没有考虑到现实世界的对手和数据分布。为了支持我们的论点，我们定义了一个新概念——强适应对手，它可以同时适应多个目标，并通过广泛的测试证明，现有的防御方法在这种对手模型中可以被绕过。我们还证明可以使用多度量标准调查来显著提高 FL 对强适应对手的韧性，而无需额外的数据或模型假设。

    Federated Learning (FL) trains machine learning models on data distributed across multiple devices, avoiding data transfer to a central location. This improves privacy, reduces communication costs, and enhances model performance. However, FL is prone to poisoning attacks, which can be untargeted aiming to reduce the model performance, or targeted, so-called backdoors, which add adversarial behavior that can be triggered with appropriately crafted inputs. Striving for stealthiness, backdoor attacks are harder to deal with.  Mitigation techniques against poisoning attacks rely on monitoring certain metrics and filtering malicious model updates. However, previous works didn't consider real-world adversaries and data distributions. To support our statement, we define a new notion of strong adaptive adversaries that can simultaneously adapt to multiple objectives and demonstrate through extensive tests, that existing defense methods can be circumvented in this adversary model. We also demon
    
[^12]: 过度压缩如何影响GNN的能力？

    How does over-squashing affect the power of GNNs?. (arXiv:2306.03589v1 [cs.LG])

    [http://arxiv.org/abs/2306.03589](http://arxiv.org/abs/2306.03589)

    本文通过测量节点之间成对交互的水平，提供了严格的分析，以确定具有一定容量的MPNN可以学习哪些节点特征的函数类别。结果表明，为了保证节点对之间的充分通信，MPNN的容量必须是...

    

    图神经网络（GNN）是处理图结构数据的机器学习的最先进模型。最流行的GNN类别是通过相邻节点间的信息交换来操作的，称为消息传递神经网络（MPNN）。鉴于它们的广泛应用，了解MPNN的表达能力是一个关键问题。然而，现有结果通常考虑具有无信息节点特征的环境。在本文中，我们提供了一种严格的分析方法，以确定具有一定容量的MPNN可以学习哪些节点特征的函数类别。我们通过测量MPNN允许的节点之间的成对交互水平来实现此目的。该测量提供了一种新的量化特性，即所谓的过度压缩效应，该效应被观察到是当大量的信息聚合成固定大小的向量时发生的。使用我们的测量，我们证明，为了保证节点对之间的充分通信，MPNN的容量必须是...

    Graph Neural Networks (GNNs) are the state-of-the-art model for machine learning on graph-structured data. The most popular class of GNNs operate by exchanging information between adjacent nodes, and are known as Message Passing Neural Networks (MPNNs). Given their widespread use, understanding the expressive power of MPNNs is a key question. However, existing results typically consider settings with uninformative node features. In this paper, we provide a rigorous analysis to determine which function classes of node features can be learned by an MPNN of a given capacity. We do so by measuring the level of pairwise interactions between nodes that MPNNs allow for. This measure provides a novel quantitative characterization of the so-called over-squashing effect, which is observed to occur when a large volume of messages is aggregated into fixed-size vectors. Using our measure, we prove that, to guarantee sufficient communication between pairs of nodes, the capacity of the MPNN must be l
    
[^13]: L-C2ST: 基于本地诊断实现模拟推断中后验近似

    L-C2ST: Local Diagnostics for Posterior Approximations in Simulation-Based Inference. (arXiv:2306.03580v1 [stat.ML])

    [http://arxiv.org/abs/2306.03580](http://arxiv.org/abs/2306.03580)

    本文提出了一种名为 L-C2ST 的基于本地诊断实现模拟推断中后验近似的新方法，其可以在任何给定的观测下本地评估后验估计器，有效地解决了目前评估后验估计器限制解决方法的问题。

    

    最近许多模拟推断（SBI）的工作都依赖于深度生成模型来近似复杂、高维度的后验分布。然而，评估这些近似是否可信仍是一个挑战。大多数方法仅在观测空间期望下评估后验估计器。这限制了它们的可解释性，并不能足够地确定哪些观测结果可以信任这些近似或应该改进。我们基于著名的分类器两样本检验 (C2ST)，引入 L-C2ST，一个新方法，允许在任何给定的观测下本地评估后验估计器。它提供有理论基础和易于解释的，如图示诊断。与 C2ST 不同的是，L-C2ST 不需要访问真实后验的样本。对于基于归一化流的后验估计器，L-C2ST 可以专门提供更好的统计功率，同时计算效率更高。

    Many recent works in simulation-based inference (SBI) rely on deep generative models to approximate complex, high-dimensional posterior distributions. However, evaluating whether or not these approximations can be trusted remains a challenge. Most approaches evaluate the posterior estimator only in expectation over the observation space. This limits their interpretability and is not sufficient to identify for which observations the approximation can be trusted or should be improved. Building upon the well-known classifier two-sample test (C2ST), we introduce L-C2ST, a new method that allows for a local evaluation of the posterior estimator at any given observation. It offers theoretically grounded and easy to interpret - e.g. graphical - diagnostics, and unlike C2ST, does not require access to samples from the true posterior. In the case of normalizing flow-based posterior estimators, L-C2ST can be specialized to offer better statistical power, while being computationally more efficien
    
[^14]: 联邦学习的个性化分离

    Personalization Disentanglement for Federated Learning. (arXiv:2306.03570v1 [cs.LG])

    [http://arxiv.org/abs/2306.03570](http://arxiv.org/abs/2306.03570)

    本文通过使用联邦双变分自编码器（FedDVA）明确分解潜在表示，捕捉共享知识和客户特定个性化，从而导致更可靠和有效的个性化联邦学习，并在广泛实验中验证了该方法的优越性。

    

    个性化联邦学习（PFL）通过在客户端之间平衡的知识共享和模型个性化之间进行训练，从而联合训练各种局部模型。本文通过将潜在表示明确分解为两个部分来解决PFL问题，以捕捉共享知识和客户特定个性化，从而导致更可靠和有效的PFL。该分离是通过一种新颖的联邦双变分自编码器（FedDVA）实现的，它使用两个编码器来推断两种类型的表示。FedDVA可以更好地理解PFL中全局知识共享和本地个性化之间的权衡。此外，它可以与现有的FL方法集成，并将它们转变为用于异构下游任务的个性化模型。广泛的实验证实了分离所带来的优势，并表明经过分离训练的模型明显优于那些普通方法。

    Personalized federated learning (PFL) jointly trains a variety of local models through balancing between knowledge sharing across clients and model personalization per client. This paper addresses PFL via explicit disentangling latent representations into two parts to capture the shared knowledge and client-specific personalization, which leads to more reliable and effective PFL. The disentanglement is achieved by a novel Federated Dual Variational Autoencoder (FedDVA), which employs two encoders to infer the two types of representations. FedDVA can produce a better understanding of the trade-off between global knowledge sharing and local personalization in PFL. Moreover, it can be integrated with existing FL methods and turn them into personalized models for heterogeneous downstream tasks. Extensive experiments validate the advantages caused by disentanglement and show that models trained with disentangled representations substantially outperform those vanilla methods.
    
[^15]: 基于记忆的双高斯过程用于序列学习

    Memory-Based Dual Gaussian Processes for Sequential Learning. (arXiv:2306.03566v1 [cs.LG])

    [http://arxiv.org/abs/2306.03566](http://arxiv.org/abs/2306.03566)

    本论文提出了一种基于记忆的双高斯过程用于序列学习的方法，能够控制误差并改善学习。

    

    在连续和主动学习中，访问过去数据的能力有限，因此使用高斯过程（GPs）进行序列学习是具有挑战性的。在后验、超参数和诱导点的不准确性导致错误随时间累积的情况下，准确学习变得困难。在这里，我们提出了一种使用最近提出的双稀疏变分高斯过程来控制所有这些误差的方法。我们的方法能够进行通用似然的准确推断，并通过主动建立和更新过去数据的记忆来改善学习。我们在涉及贝叶斯优化、主动学习和连续学习的几个应用中展示了其有效性。

    Sequential learning with Gaussian processes (GPs) is challenging when access to past data is limited, for example, in continual and active learning. In such cases, errors can accumulate over time due to inaccuracies in the posterior, hyperparameters, and inducing points, making accurate learning challenging. Here, we present a method to keep all such errors in check using the recently proposed dual sparse variational GP. Our method enables accurate inference for generic likelihoods and improves learning by actively building and updating a memory of past data. We demonstrate its effectiveness in several applications involving Bayesian optimization, active learning, and continual learning.
    
[^16]: CIN++：增强拓扑信息传递

    CIN++: Enhancing Topological Message Passing. (arXiv:2306.03561v1 [cs.LG])

    [http://arxiv.org/abs/2306.03561](http://arxiv.org/abs/2306.03561)

    CIN++是一种拓扑信息传递方案，通过考虑底层复合体中环之间的相互作用来解决图神经网络在表达能力、处理长程交互和建模高阶结构等方面的局限性。

    

    图神经网络已经在学习图结构化数据方面获得了显着的成功。然而，它们在表达能力上存在显著的局限性，难以处理长程交互，并缺乏对建模高阶结构和群体相互作用的基本方法。细胞同构网络最近通过基于细胞复合体的信息传递方案解决了大部分这些挑战。尽管具有优势，但CIN仅使用边界和上部信息，而没有考虑底层复合体中存在的环之间的直接相互作用。考虑到这些相互作用可能对学习许多真实复杂现象的表示非常重要，如超分子组装的动力学、脑内神经活动和基因调控过程。因此本文提出了CIN++，这是CIN引入的拓扑信息传递方案的一种增强版。我们的信息传递方案考虑了环之间的相互作用。

    Graph Neural Networks (GNNs) have demonstrated remarkable success in learning from graph-structured data. However, they face significant limitations in expressive power, struggling with long-range interactions and lacking a principled approach to modeling higher-order structures and group interactions. Cellular Isomorphism Networks (CINs) recently addressed most of these challenges with a message passing scheme based on cell complexes. Despite their advantages, CINs make use only of boundary and upper messages which do not consider a direct interaction between the rings present in the underlying complex. Accounting for these interactions might be crucial for learning representations of many real-world complex phenomena such as the dynamics of supramolecular assemblies, neural activity within the brain, and gene regulation processes. In this work, we propose CIN++, an enhancement of the topological message passing scheme introduced in CINs. Our message passing scheme accounts for the af
    
[^17]: 机器取消学习：一项综述（arXiv:2306.03558v1 [cs.CR]）

    Machine Unlearning: A Survey. (arXiv:2306.03558v1 [cs.CR])

    [http://arxiv.org/abs/2306.03558](http://arxiv.org/abs/2306.03558)

    本文综述了机器取消学习技术的关键概念，分类和总结了现有的解决方案。

    

    机器学习已经吸引了广泛的关注，并发展成为广泛成功应用的促成技术，例如智能计算机视觉、语音识别、医学诊断等等。然而，由于隐私、可用性和/或遗忘权，有一个特殊的需求需要从模型中删除有关某些特定样本的信息，称为机器取消学习。这种新兴技术因其创新和实用性而引起了学术界和工业界的广泛兴趣。同时，这个雄心勃勃的问题已经导致了众多的研究努力，旨在应对它的挑战。据我们所知，没有一项研究分析这个复杂的主题或在不同种类的情景中比较现有的取消学习解决方案的可行性。因此，通过这个综述，我们旨在把握取消学习技术的关键概念。现有的解决方案根据它们的特征进行分类和总结。

    Machine learning has attracted widespread attention and evolved into an enabling technology for a wide range of highly successful applications, such as intelligent computer vision, speech recognition, medical diagnosis, and more. Yet a special need has arisen where, due to privacy, usability, and/or the right to be forgotten, information about some specific samples needs to be removed from a model, called machine unlearning. This emerging technology has drawn significant interest from both academics and industry due to its innovation and practicality. At the same time, this ambitious problem has led to numerous research efforts aimed at confronting its challenges. To the best of our knowledge, no study has analyzed this complex topic or compared the feasibility of existing unlearning solutions in different kinds of scenarios. Accordingly, with this survey, we aim to capture the key concepts of unlearning techniques. The existing solutions are classified and summarized based on their ch
    
[^18]: 数据中动态偏移的状态规范化策略优化

    State Regularized Policy Optimization on Data with Dynamics Shift. (arXiv:2306.03552v1 [cs.LG])

    [http://arxiv.org/abs/2306.03552](http://arxiv.org/abs/2306.03552)

    本文提出了一种叫做 SRPO (状态规范化策略优化) 的算法，该算法利用训练数据中的稳态分布来规范新环境中的策略，在处理具有不同动态的多个环境时表现优异。

    

    在许多实际场景中，强化学习算法使用的数据受到动态偏移的影响，即具有不同的环境动态。目前的大多数方法通过训练上下文编码器来识别环境参数来解决这个问题。根据其环境参数将带有动态漂移的数据分开以训练相应的策略。然而，这些方法可能会出现样本效率低下的问题，因为数据是“特定场景”使用的，针对某个环境训练的策略不能从收集在其他具有不同动态的所有其他环境中的数据中受益。本文发现，在许多具有相似结构和不同动态的环境中，最优策略具有类似的稳态分布。我们利用这种特性，并从具有动态漂移的数据中学习稳态分布，以实现高效的数据重用。这种分布用于规范新环境中训练的策略，导致了 SRPO（状态规范化策略优化）算法的出现。实验结果表明，SRPO 在具有动态偏移的任务上显著优于现有的方法。

    In many real-world scenarios, Reinforcement Learning (RL) algorithms are trained on data with dynamics shift, i.e., with different underlying environment dynamics. A majority of current methods address such issue by training context encoders to identify environment parameters. Data with dynamics shift are separated according to their environment parameters to train the corresponding policy. However, these methods can be sample inefficient as data are used \textit{ad hoc}, and policies trained for one dynamics cannot benefit from data collected in all other environments with different dynamics. In this paper, we find that in many environments with similar structures and different dynamics, optimal policies have similar stationary state distributions. We exploit such property and learn the stationary state distribution from data with dynamics shift for efficient data reuse. Such distribution is used to regularize the policy trained in a new environment, leading to the SRPO (\textbf{S}tat
    
[^19]: 工业4.0中可扩展的概念抽取

    Scalable Concept Extraction in Industry 4.0. (arXiv:2306.03551v1 [cs.AI])

    [http://arxiv.org/abs/2306.03551](http://arxiv.org/abs/2306.03551)

    本文探讨了将概念提取方法应用于工业4.0场景，并改进了可扩展性，提出了具有可解释性的新概念重要性计算程序，将局部特征与整体图像联系起来。

    

    工业4.0正在利用数字技术和机器学习技术来连接和优化制造过程。这一概念的核心在于将原始数据转化为可靠的数据驱动决策的人类可理解的知识。卷积神经网络（CNN）在处理图像数据方面发挥了重要作用，但是它们的“黑盒子”本质使得它们的预测过程难以理解。在这个背景下，解释性人工智能（XAI）领域的最新进展提出了概念的提取和定位，即视觉线索如何介入CNN的预测过程。本文探讨了将概念提取（CE）方法应用于工业4.0场景的问题。为此，我们修改了最近开发的技术“使用本地聚合描述符提取概念”（ECLAD），并改进了其可扩展性。具体而言，我们提出了一种新的概念重要性计算程序，利用加权平均来将局部特征与整体图像联系起来，改善ECLAD的性能。

    The industry 4.0 is leveraging digital technologies and machine learning techniques to connect and optimize manufacturing processes. Central to this idea is the ability to transform raw data into human understandable knowledge for reliable data-driven decision-making. Convolutional Neural Networks (CNNs) have been instrumental in processing image data, yet, their ``black box'' nature complicates the understanding of their prediction process. In this context, recent advances in the field of eXplainable Artificial Intelligence (XAI) have proposed the extraction and localization of concepts, or which visual cues intervene on the prediction process of CNNs. This paper tackles the application of concept extraction (CE) methods to industry 4.0 scenarios. To this end, we modify a recently developed technique, ``Extracting Concepts with Local Aggregated Descriptors'' (ECLAD), improving its scalability. Specifically, we propose a novel procedure for calculating concept importance, utilizing a w
    
[^20]: 用反显式积分器从嘈杂数据中学习动态系统

    Learning Dynamical Systems from Noisy Data with Inverse-Explicit Integrators. (arXiv:2306.03548v1 [cs.LG])

    [http://arxiv.org/abs/2306.03548](http://arxiv.org/abs/2306.03548)

    本文提出了一种新方法——均值反向积分器（MII），它可用于从嘈杂的数据中近似动态系统的向量场，通过与单隐式龙格-库塔方法（MIRK）的连接，显式地获得损失函数表达式，实现解锁被初值问题描述的对称和高阶积分器，与仅使用数值积分器的方式相比，通过将MIRK应用于MII的组合方法可以显着降低误差。

    

    我们引入了均值反向积分器（MII），这是一种新方法，用于在训练神经网络从嘈杂的数据中近似动态系统的向量场时增加准确性。该方法可用于平均通过数值积分器（如龙格-库塔方法）获得的多条轨迹。我们表明，当与MII连接时，类别为单隐式龙格-库塔方法（MIRK）具有特殊优势。当将训练数据插入MIRK公式中时，可获得用于向量场近似的显式损失函数表达式，解锁在初值问题发生时，否则为隐式的对称和高阶积分器。将MIRK应用于MII中的组合方法与仅使用数值积分器而不平均轨迹相比，误差显着降低 。这是通过使用来自几个（混沌）Hamiltonian系统的数据进行实验来证明的。此外，我们进行了敏感性分析，以确定MII和MIRK的参数值对于近似结果的影响。

    We introduce the mean inverse integrator (MII), a novel approach to increase the accuracy when training neural networks to approximate vector fields of dynamical systems from noisy data. This method can be used to average multiple trajectories obtained by numerical integrators such as Runge-Kutta methods. We show that the class of mono-implicit Runge-Kutta methods (MIRK) has particular advantages when used in connection with MII. When training vector field approximations, explicit expressions for the loss functions are obtained when inserting the training data in the MIRK formulae, unlocking symmetric and high-order integrators that would otherwise be implicit for initial value problems. The combined approach of applying MIRK within MII yields a significantly lower error compared to the plain use of the numerical integrator without averaging the trajectories. This is demonstrated with experiments using data from several (chaotic) Hamiltonian systems. Additionally, we perform a sensitiv
    
[^21]: 如何选择适合特定问题和预算的主动学习策略

    How to Select Which Active Learning Strategy is Best Suited for Your Specific Problem and Budget. (arXiv:2306.03543v1 [cs.LG])

    [http://arxiv.org/abs/2306.03543](http://arxiv.org/abs/2306.03543)

    针对主动学习中如何选择适合特定问题和预算的问题，我们提出了一种导数法实用方法，即动态地识别每个预算的最佳策略。

    

    在主动学习中，学习者在一定的预算约束下主动选择未标记示例以向神谕请求其标记。不同的主动学习查询策略更适合不同的问题和预算。因此，在实践中，事先知道哪个主动学习策略最适合手头的问题仍然是一个未解决的问题。为了解决这个挑战，我们提出了一种基于导数的实用方法，动态地识别每个预算的最佳策略。我们提供了一个简化情况的理论分析来激发我们的方法并建立直觉。然后介绍了一种基于具体问题和预算来动态选择主动学习策略的方法。实证结果展示了我们的方法在不同预算和计算机视觉任务中的有效性。

    In Active Learning (AL), a learner actively chooses which unlabeled examples to query for labels from an oracle, under some budget constraints. Different AL query strategies are more suited to different problems and budgets. Therefore, in practice, knowing in advance which AL strategy is most suited for the problem at hand remains an open problem. To tackle this challenge, we propose a practical derivative-based method that dynamically identifies the best strategy for each budget. We provide theoretical analysis of a simplified case to motivate our approach and build intuition. We then introduce a method to dynamically select an AL strategy based on the specific problem and budget. Empirical results showcase the effectiveness of our approach across diverse budgets and computer vision tasks.
    
[^22]: 浅谈测试时间适应的陷阱

    On Pitfalls of Test-Time Adaptation. (arXiv:2306.03536v1 [cs.LG])

    [http://arxiv.org/abs/2306.03536](http://arxiv.org/abs/2306.03536)

    这篇论文介绍了测试时间适应（TTA）最近被认为是解决在分布转移情况下鲁棒性挑战的一种很有前途的方法，提出了测试时间适应基准TTAB，并发现了之前方法中的三个常见缺陷。

    

    测试时间适应（TTA）最近被认为是解决在分布转移情况下鲁棒性挑战的一种很有前途的方法。然而，之前文献中缺乏一致的设置和系统性研究，这妨碍了现有方法的彻底评估。为了解决这个问题，我们提出了TTAB，一个测试时间适应基准，包括十种最先进的算法，多种不同的分布转移情况和两种评估协议。通过大量实验，我们的基准揭示了之前工作中的三个常见缺陷。首先，由于在线批次依赖性，选择适当的超参数，特别是模型选择，非常困难。其次，TTA的有效性因被适应的模型的质量和属性而有很大差异。第三，即使在最佳算法条件下，现有方法也不能解决所有常见类型的分布转移情况。我们的发现强调了未来研究测试时间适应技术及其一致和全面的评估协议的重要性。

    Test-Time Adaptation (TTA) has recently emerged as a promising approach for tackling the robustness challenge under distribution shifts. However, the lack of consistent settings and systematic studies in prior literature hinders thorough assessments of existing methods. To address this issue, we present TTAB, a test-time adaptation benchmark that encompasses ten state-of-the-art algorithms, a diverse array of distribution shifts, and two evaluation protocols. Through extensive experiments, our benchmark reveals three common pitfalls in prior efforts. First, selecting appropriate hyper-parameters, especially for model selection, is exceedingly difficult due to online batch dependency. Second, the effectiveness of TTA varies greatly depending on the quality and properties of the model being adapted. Third, even under optimal algorithmic conditions, none of the existing methods are capable of addressing all common types of distribution shifts. Our findings underscore the need for future r
    
[^23]: 分类模型在可分数据上的持续学习

    Continual Learning in Linear Classification on Separable Data. (arXiv:2306.03534v1 [cs.LG])

    [http://arxiv.org/abs/2306.03534](http://arxiv.org/abs/2306.03534)

    本文从理论上探究了在可分数据上的线性分类持续学习，使得弱正则化学习退化为解决连续最大间隔问题；提出在循环和随机排序任务下的遗忘和其他量感兴趣的上限，并探讨了对训练方法的实际影响。

    

    本文分析了在具有二元标签的可分线性分类任务序列上的持续学习。我们从理论上证明，弱正则化学习退化为解决连续最大间隔问题，对应于凸集投影框架的特殊情况。然后我们在不同设置下开发了遗忘和其他感兴趣量的上限，包括循环和随机排序的任务。我们讨论了对流行的训练方法（如正则化调度和加权）的几个实际影响。我们指出持续分类设置和最近研究的持续回归设置之间的几个理论差异。

    We analyze continual learning on a sequence of separable linear classification tasks with binary labels. We show theoretically that learning with weak regularization reduces to solving a sequential max-margin problem, corresponding to a special case of the Projection Onto Convex Sets (POCS) framework. We then develop upper bounds on the forgetting and other quantities of interest under various settings with recurring tasks, including cyclic and random orderings of tasks. We discuss several practical implications to popular training practices like regularization scheduling and weighting. We point out several theoretical differences between our continual classification setting and a recently studied continual regression setting.
    
[^24]: BackpropTools: 一款快速、可移植的连续控制深度强化学习库

    BackpropTools: A Fast, Portable Deep Reinforcement Learning Library for Continuous Control. (arXiv:2306.03530v1 [cs.LG])

    [http://arxiv.org/abs/2306.03530](http://arxiv.org/abs/2306.03530)

    BackpropTools是一款快速、可移植的连续控制深度强化学习库，它通过模板元编程提供紧密集成的可组合组件，并在异构平台集合上无缝使用，同时在连续控制问题的深度RL代理高效可扩展训练方面具有优势。由于其可移植性和实时保证，它成为了在嵌入式设备上部署学来的策略的有价值的工具。

    

    深度强化学习在许多领域中已被证明可以产生出具有能力的代理和控制策略，但常常受到训练时间过长的困扰。此外，在连续控制问题的情况下，现有深度学习库的实时性和可移植性的缺乏限制了学习策略在实际嵌入式设备上的应用。为了解决这些问题，我们提出了BackpropTools，一种依赖性-free、header-only、pure C++的深度监督和强化学习库。利用最近C++标准的模板元编程能力，我们提供了可以由编译器紧密集成的可组合组件。其新颖的架构允许BackpropTools在异构平台集合上无缝使用，从HPC集群、工作站和笔记本电脑到智能手机、智能手表和微控制器。具体来说，由于RL算法与模拟环境的紧密集成，BackpropTools在连续控制问题的深度RL代理的高效可扩展训练方面具有优势。此外，它的可移植性和实时保证使其成为在嵌入式设备上部署学来的策略的有价值的工具。

    Deep Reinforcement Learning (RL) has been demonstrated to yield capable agents and control policies in several domains but is commonly plagued by prohibitively long training times. Additionally, in the case of continuous control problems, the applicability of learned policies on real-world embedded devices is limited due to the lack of real-time guarantees and portability of existing deep learning libraries. To address these challenges, we present BackpropTools, a dependency-free, header-only, pure C++ library for deep supervised and reinforcement learning. Leveraging the template meta-programming capabilities of recent C++ standards, we provide composable components that can be tightly integrated by the compiler. Its novel architecture allows BackpropTools to be used seamlessly on a heterogeneous set of platforms, from HPC clusters over workstations and laptops to smartphones, smartwatches, and microcontrollers. Specifically, due to the tight integration of the RL algorithms with simu
    
[^25]: Rec4Ad: 用于淘宝广告点击率预测的消除样本选择偏差的方法

    Rec4Ad: A Free Lunch to Mitigate Sample Selection Bias for Ads CTR Prediction in Taobao. (arXiv:2306.03527v1 [cs.IR])

    [http://arxiv.org/abs/2306.03527](http://arxiv.org/abs/2306.03527)

    提出了一种名为Rec4Ad的方法，用于消除淘宝广告CTR预测中的样本选择偏差。它利用推荐日志构建合成样本来模拟推断集的分布，然后将合成和原始样本组合以减少SSB。实验结果表明，Rec4Ad在模型性能和校准方面显著优于几种最先进的方法。

    

    点击率（CTR）预测作为在线广告的一个基本组成部分，其训练常常基于广告展示和用户反馈。然而，这样的广告展示往往是有选择性的，由于其与推断分布不同，因此存在着样本选择偏差（SSB），影响模型的性能。现有研究主要采用样本再平衡技术来消除SSB，但其存在高方差且模型校准差的问题。另一些研究依赖于不足以训练工业模型的昂贵均匀数据。因此，探索一种无需均匀数据框架消除工业模型中的SSB是值得探讨的。根据淘宝等平台展示有机物品和赞助物品的混合结果，提出了一个名叫Rec4Ad的简单有效框架来消除广告的CTR预测中的SSB。具体来说，Rec4Ad利用推荐日志构建合成样本，使其模拟推断集的分布。然后，它将合成和原始样本组合以减少SSB。实验结果表明，Rec4Ad在模型性能和校准方面显著优于几种最先进的方法。

    Click-Through Rate (CTR) prediction serves as a fundamental component in online advertising. A common practice is to train a CTR model on advertisement (ad) impressions with user feedback. Since ad impressions are purposely selected by the model itself, their distribution differs from the inference distribution and thus exhibits sample selection bias (SSB) that affects model performance. Existing studies on SSB mainly employ sample re-weighting techniques which suffer from high variance and poor model calibration. Another line of work relies on costly uniform data that is inadequate to train industrial models. Thus mitigating SSB in industrial models with a uniform-data-free framework is worth exploring. Fortunately, many platforms display mixed results of organic items (i.e., recommendations) and sponsored items (i.e., ads) to users, where impressions of ads and recommendations are selected by different systems but share the same user decision rationales. Based on the above characteri
    
[^26]: 多层次带基准的异常检测的函数数据视角

    A Functional Data Perspective and Baseline On Multi-Layer Out-of-Distribution Detection. (arXiv:2306.03522v1 [cs.LG])

    [http://arxiv.org/abs/2306.03522](http://arxiv.org/abs/2306.03522)

    本文提出了一种基于网络的函数数据视角的原创方法，利用样本通过各层的轨迹及其统计上的依赖关系，优于现有最先进方法，实现了多层次带基准的ODD检测。

    

    实现外样本检测的关键特征是通过多层分类器提取统计模式和数据间的关系，以检测预期输入数据分布的变化。但是，现有的一些最先进的方法仅使用倒数第二层或最后一层的输出，留下了用于ODD检测的有价值的信息。本文提出了一种基于网络的函数视角的原创方法，利用样本通过各层的轨迹及其统计上的依赖关系。它超越了多元特征聚合，并引入了基于函数异常检测的基准。在这个新的框架中，ODD检测转化为检测样本的轨迹与训练集所表现的典型行为不同的情况。我们在各种基准测试中验证了我们的方法，并展示了通过利用网络的所有层的信息，其在性能上优于现有的最先进方法。

    A key feature of out-of-distribution (OOD) detection is to exploit a trained neural network by extracting statistical patterns and relationships through the multi-layer classifier to detect shifts in the expected input data distribution. Despite achieving solid results, several state-of-the-art methods rely on the penultimate or last layer outputs only, leaving behind valuable information for OOD detection. Methods that explore the multiple layers either require a special architecture or a supervised objective to do so. This work adopts an original approach based on a functional view of the network that exploits the sample's trajectories through the various layers and their statistical dependencies. It goes beyond multivariate features aggregation and introduces a baseline rooted in functional anomaly detection. In this new framework, OOD detection translates into detecting samples whose trajectories differ from the typical behavior characterized by the training set. We validate our me
    
[^27]: 非平衡条件下的机器学习

    Machine learning in and out of equilibrium. (arXiv:2306.03521v1 [cs.LG])

    [http://arxiv.org/abs/2306.03521](http://arxiv.org/abs/2306.03521)

    该研究使用Fokker-Planck方法从统计物理学的角度探讨了训练神经网络算法与自然过程之间的相似之处，并验证了其在非平衡系统中的应用，对于研究神经网络的状态及其训练有着重要的意义。

    

    用于训练神经网络的算法，如随机梯度下降（SGD），与自然过程存在密切的相似之处，例如蛋白质折叠或进化。我们采用从统计物理学中改编的Fokker-Planck方法，在一个统一的框架中探讨这些相似之处。我们特别关注系统在长时间极限下的平衡状态，这在传统的SGD中是不平衡的，在网络参数空间中表现出持续的电流。与其物理类比一样，该电流与任何给定训练轨迹的熵产生率相关。这些率的平衡分布遵循积分和详细波动定理，即热力学第二定律的非平衡推广。我们在两个数值例子中验证了这些关系，一个是非线性回归网络，另一个是MNIST数字分类。虽然波动定理是

    The algorithms used to train neural networks, like stochastic gradient descent (SGD), have close parallels to natural processes that navigate a high-dimensional parameter space -- for example protein folding or evolution. Our study uses a Fokker-Planck approach, adapted from statistical physics, to explore these parallels in a single, unified framework. We focus in particular on the stationary state of the system in the long-time limit, which in conventional SGD is out of equilibrium, exhibiting persistent currents in the space of network parameters. As in its physical analogues, the current is associated with an entropy production rate for any given training trajectory. The stationary distribution of these rates obeys the integral and detailed fluctuation theorems -- nonequilibrium generalizations of the second law of thermodynamics. We validate these relations in two numerical examples, a nonlinear regression network and MNIST digit classification. While the fluctuation theorems are 
    
[^28]: COPR：面向一致性的在线广告预排名

    COPR: Consistency-Oriented Pre-Ranking for Online Advertising. (arXiv:2306.03516v1 [cs.IR])

    [http://arxiv.org/abs/2306.03516](http://arxiv.org/abs/2306.03516)

    该论文提出了一种面向一致性的在线广告预排名框架，利用了一个基于块的采样模块和一个即插即用的排名对齐模块，来显式优化ECPM排名结果的一致性。他们采用了基于Delta NDCG的加权机制，以更好地区分重要性。

    

    级联架构被广泛应用于大规模广告系统中以平衡效率和效果。在这种架构中，预排名模型被期望成为一个轻量级的排名模型近似，以处理更多具有严格延迟要求的候选者。由于模型容量的差距，预排名和排名模型通常会生成不一致的排名结果，从而损害整个系统的效果。提出了得分对齐的范式以规范它们的原始分数，使它们保持一致。然而，在在线广告中应用时，由于必然的对齐误差和竞标的误差放大，它会遭受困扰。为此，我们引入了一个面向一致性的在线广告预排名框架，该框架采用了一个基于块的采样模块和一个即插即用的排名对齐模块，来显式优化ECPM排名结果的一致性。采用了基于$\Delta NDCG$的加权机制，以更好地区分重要性。

    Cascading architecture has been widely adopted in large-scale advertising systems to balance efficiency and effectiveness. In this architecture, the pre-ranking model is expected to be a lightweight approximation of the ranking model, which handles more candidates with strict latency requirements. Due to the gap in model capacity, the pre-ranking and ranking models usually generate inconsistent ranked results, thus hurting the overall system effectiveness. The paradigm of score alignment is proposed to regularize their raw scores to be consistent. However, it suffers from inevitable alignment errors and error amplification by bids when applied in online advertising. To this end, we introduce a consistency-oriented pre-ranking framework for online advertising, which employs a chunk-based sampling module and a plug-and-play rank alignment module to explicitly optimize consistency of ECPM-ranked results. A $\Delta NDCG$-based weighting mechanism is adopted to better distinguish the import
    
[^29]: 知识图谱推理的逻辑扩散

    Logic Diffusion for Knowledge Graph Reasoning. (arXiv:2306.03515v1 [cs.LG])

    [http://arxiv.org/abs/2306.03515](http://arxiv.org/abs/2306.03515)

    该篇论文提出了一种名为逻辑扩散（LoD）的插件模块，解决了现有推理模型受训练样本限制、表现不够强的问题。LoD通过关系扩散、随机游走子逻辑采样和梯度自适应等方式实现了对未见查询的发现和不同模式之间的动态平衡，并配备了特殊的损失函数以实现稳健的逻辑扩散。

    

    最近的研究集中于回答一阶逻辑查询，通过多跳逻辑预测来探索知识图谱推理。然而，现有的推理模型受到训练样本所围绕的逻辑范式的限制，导致在未见逻辑推理上表现还不够强。为了解决这些问题，我们提出了一个名为逻辑扩散（LoD）的插件模块，能够从周围环境中发现未见查询，并实现不同模式之间的动态平衡。LoD的基本思想是关系扩散和随机游走子逻辑采样以及一种特殊的训练机制——梯度自适应。此外，LoD还配备了一种新颖的损失函数，以进一步在训练或测试集中应对嘈杂数据时实现稳健的逻辑扩散。在四个公共数据集上的大量实验证明，带有LoD的主流知识图谱推理模型优于最先进的模型。此外，我们的消融研究证明了逻辑扩散在克服现有推理模型的局限性和实现更好的未见逻辑推理方面的潜力。

    Most recent works focus on answering first order logical queries to explore the knowledge graph reasoning via multi-hop logic predictions. However, existing reasoning models are limited by the circumscribed logical paradigms of training samples, which leads to a weak generalization of unseen logic. To address these issues, we propose a plug-in module called Logic Diffusion (LoD) to discover unseen queries from surroundings and achieves dynamical equilibrium between different kinds of patterns. The basic idea of LoD is relation diffusion and sampling sub-logic by random walking as well as a special training mechanism called gradient adaption. Besides, LoD is accompanied by a novel loss function to further achieve the robust logical diffusion when facing noisy data in training or testing sets. Extensive experiments on four public datasets demonstrate the superiority of mainstream knowledge graph reasoning models with LoD over state-of-the-art. Moreover, our ablation study proves the gene
    
[^30]: 基于子图网络的对比学习

    Subgraph Networks Based Contrastive Learning. (arXiv:2306.03506v1 [cs.LG])

    [http://arxiv.org/abs/2306.03506](http://arxiv.org/abs/2306.03506)

    本文提出了一种新的对比学习框架，名为基于子图网络的对比学习(SGNCL)，通过应用子图网络生成策略以产生增强视图，并探究了子结构相互作用对图形表示的影响。

    

    图对比学习(GCL)是一种自监督学习方法，可解决注释数据稀缺的问题。 它在未注释的图形中挖掘显式特征以生成下游任务的有利图形表示。大多数现有的GCL方法侧重于图形增强策略和相互信息估计操作的设计。 然而，这些方法没有考虑子图中存在的相互作用。为了探索子结构相互作用对图形表示的影响，我们提出了一种名为subgraph network-based contrastive learning (SGNCL)的新框架。SGNCL应用子图网络生成策略以产生增强视图。该策略将原始图转换为具有拓扑和属性特征的边到节点映射网络。单次增强视图是

    Graph contrastive learning (GCL), as a self-supervised learning method, can solve the problem of annotated data scarcity. It mines explicit features in unannotated graphs to generate favorable graph representations for downstream tasks. Most existing GCL methods focus on the design of graph augmentation strategies and mutual information estimation operations. Graph augmentation produces augmented views by graph perturbations. These views preserve a locally similar structure and exploit explicit features. However, these methods have not considered the interaction existing in subgraphs. To explore the impact of substructure interactions on graph representations, we propose a novel framework called subgraph network-based contrastive learning (SGNCL). SGNCL applies a subgraph network generation strategy to produce augmented views. This strategy converts the original graph into an Edge-to-Node mapping network with both topological and attribute features. The single-shot augmented view is a 
    
[^31]: 俄乌战争：预测和解释Twitter的封禁

    Russo-Ukrainian War: Prediction and explanation of Twitter suspension. (arXiv:2306.03502v1 [cs.SI])

    [http://arxiv.org/abs/2306.03502](http://arxiv.org/abs/2306.03502)

    本研究分析了Twitter封禁机制，揭示了存在的政策违规、宣传、垃圾邮件等问题，并发现拥有更多粉丝的账户更可能被封禁。这些发现可以让Twitter和其他社交网络改进其内容过滤机制。

    

    2022年2月24日，俄罗斯入侵乌克兰，开始了现在已知的俄乌战争，并在社交媒体上引发了在线话语。Twitter作为最受欢迎的社交网络之一，以其开放和民主的特点，在其庞大的用户群中实现了透明的讨论。不幸的是，这往往会导致Twitter的政策违规、宣传、滥用行为、侵犯公民权利，因此导致用户账户被封禁和删除。本研究着重探讨了Twitter的封禁机制，并分析了可能导致账户被封禁的共享内容和用户账户的特征。为此，我们利用Twitter API获得了包含107.7M条推文的数据集，来自980万用户。我们提取了被封禁账户的共享内容类别，并通过提取文本嵌入和余弦相似性聚类来解释其特征。我们的研究结果揭示了一些滥用Twitter政策标准的骗子活动、垃圾邮件和宣传活动。此外，我们发现相对于粉丝数较少的账户，拥有更多粉丝的账户更有可能被封禁。这些发现可以为Twitter和其他社交网络改进其内容过滤机制，最小化有害内容的传播提供有用的参考。

    On 24 February 2022, Russia invaded Ukraine, starting what is now known as the Russo-Ukrainian War, initiating an online discourse on social media. Twitter as one of the most popular SNs, with an open and democratic character, enables a transparent discussion among its large user base. Unfortunately, this often leads to Twitter's policy violations, propaganda, abusive actions, civil integrity violation, and consequently to user accounts' suspension and deletion. This study focuses on the Twitter suspension mechanism and the analysis of shared content and features of the user accounts that may lead to this. Toward this goal, we have obtained a dataset containing 107.7M tweets, originating from 9.8 million users, using Twitter API. We extract the categories of shared content of the suspended accounts and explain their characteristics, through the extraction of text embeddings in junction with cosine similarity clustering. Our results reveal scam campaigns taking advantage of trending top
    
[^32]: 量子机器学习中纠缠数据的转换作用

    Transition role of entangled data in quantum machine learning. (arXiv:2306.03481v1 [quant-ph])

    [http://arxiv.org/abs/2306.03481](http://arxiv.org/abs/2306.03481)

    本研究证明了纠缠数据对量子机器学习的性能具有双重效应，有助于减少预测误差和减小训练数据大小，为量子机器学习模型设计提供了指南。

    

    纠缠作为增强量子计算的资源，已经在学习量子动力学中得到了应用。将纠缠融入到量子机器学习模型的操作或测量中，可以显著降低训练数据大小，同时在达到指定预测误差阈值时取得了更优的结果。然而，关于纠缠程度对模型性能的影响，目前仍缺乏分析性理解。本研究通过在学习量子动力学中使用纠缠数据，建立了量子不免费午餐定理。与以往发现的结果相反，我们证明了纠缠数据对预测误差的影响呈现出双重效应，取决于允许的测量次数。在有充分的测量次数的情况下，增加训练数据的纠缠度可以持续降低预测误差，或减少达到给定误差阈值所需的训练数据大小。本研究阐明了纠缠数据在量子机器学习中的关键转换作用，并提供了改进性能的量子机器学习模型设计指南。

    Entanglement serves as the resource to empower quantum computing. Recent progress has highlighted its positive impact on learning quantum dynamics, wherein the integration of entanglement into quantum operations or measurements of quantum machine learning (QML) models leads to substantial reductions in training data size, surpassing a specified prediction error threshold. However, an analytical understanding of how the entanglement degree in data affects model performance remains elusive. In this study, we address this knowledge gap by establishing a quantum no-free-lunch (NFL) theorem for learning quantum dynamics using entangled data. Contrary to previous findings, we prove that the impact of entangled data on prediction error exhibits a dual effect, depending on the number of permitted measurements. With a sufficient number of measurements, increasing the entanglement of training data consistently reduces the prediction error or decreases the required size of the training data to ac
    
[^33]: GSHOT: 少样本标记图生成建模

    GSHOT: Few-shot Generative Modeling of Labeled Graphs. (arXiv:2306.03480v1 [cs.LG])

    [http://arxiv.org/abs/2306.03480](http://arxiv.org/abs/2306.03480)

    GSHOT是一个用于少样本标记图生成建模的元学习框架，通过学习从类似的辅助图数据集中转移元知识，从而快速适应未见过的图数据集。

    

    近年来，深度图生成建模因其直接学习潜在隐藏图分布的惊人能力而受到极大关注。尽管这些技术最初取得了成功，但像许多现有的深度生成方法一样，需要大量的训练样本才能学习一个好的模型。不幸的是，在罕见疾病的药物发现等场景中，可能不总是有足够的训练样本可用。同时，最近少样本学习的进展为训练数据有限的应用打开了大门。本文介绍了少样本图生成建模这一迄今未曾探索的范式。为此，我们开发了GSHOT，一个基于元学习的框架，用于少样本标记图生成建模。GSHOT学习从类似的辅助图数据集中转移元知识。利用这些先前的经验，GSHOT通过自适应的自我调整快速适应未见过的图数据集。

    Deep graph generative modeling has gained enormous attraction in recent years due to its impressive ability to directly learn the underlying hidden graph distribution. Despite their initial success, these techniques, like much of the existing deep generative methods, require a large number of training samples to learn a good model. Unfortunately, large number of training samples may not always be available in scenarios such as drug discovery for rare diseases. At the same time, recent advances in few-shot learning have opened door to applications where available training data is limited. In this work, we introduce the hitherto unexplored paradigm of few-shot graph generative modeling. Towards this, we develop GSHOT, a meta-learning based framework for few-shot labeled graph generative modeling. GSHOT learns to transfer meta-knowledge from similar auxiliary graph datasets. Utilizing these prior experiences, GSHOT quickly adapts to an unseen graph dataset through self-paced fine-tuning. 
    
[^34]: 通过程序综合实现自然语言命令

    Natural Language Commanding via Program Synthesis. (arXiv:2306.03460v1 [cs.LG])

    [http://arxiv.org/abs/2306.03460](http://arxiv.org/abs/2306.03460)

    通过使用大型语言模型和办公室领域特定语言，语义解释器实现了自然语言命令并执行Office应用程序中的用户意图。

    

    我们提出了语义解释器，这是一种自然语言友好型的人工智能系统，用于生产力软件，如微软Office，利用大型语言模型（LLM）跨应用程序功能执行用户意图。虽然LLM在理解以自然语言表达的用户意图方面表现出色，但对于需要超过文本到文本转换的应用程序特定用户意图的实现不足。因此，我们引入了办公域特定语言（ODSL），这是一种简洁、高级别的语言，专门用于在Office应用程序中执行操作并与实体交互。语义解释器利用分析检索提示构造方法与LLM进行程序综合，将自然语言用户话语转换为可以被转换为应用程序API并执行的ODSL程序。我们主要关注Microsoft PowerPoint的研究探索。

    We present Semantic Interpreter, a natural language-friendly AI system for productivity software such as Microsoft Office that leverages large language models (LLMs) to execute user intent across application features. While LLMs are excellent at understanding user intent expressed as natural language, they are not sufficient for fulfilling application-specific user intent that requires more than text-to-text transformations. We therefore introduce the Office Domain Specific Language (ODSL), a concise, high-level language specialized for performing actions in and interacting with entities in Office applications. Semantic Interpreter leverages an Analysis-Retrieval prompt construction method with LLMs for program synthesis, translating natural language user utterances to ODSL programs that can be transpiled to application APIs and then executed. We focus our discussion primarily on a research exploration for Microsoft PowerPoint.
    
[^35]: AI多传感器融合系统鲁棒性基准测试：挑战和机遇

    Benchmarking Robustness of AI-enabled Multi-sensor Fusion Systems: Challenges and Opportunities. (arXiv:2306.03454v1 [cs.SE])

    [http://arxiv.org/abs/2306.03454](http://arxiv.org/abs/2306.03454)

    本篇论文研究了AI多传感器融合系统的鲁棒性问题，特别关注在安全关键的环境中的表现和可靠性，并提出了基于AI的MSF感知系统基准测试套件。

    

    基于多传感器融合（MSF）的感知系统是支撑许多工业应用和领域的基础，例如自动驾驶汽车、机器人臂和无人机。近年来，数据驱动的人工智能（AI）在深度学习技术方面的快速进步为MSF系统的性能提高提供了快速增长的趋势，特别是在智能系统及其感知系统方面。尽管提出了许多基于AI的MSF感知系统和技术，但到目前为止，公开的专注于MSF感知的基准测试有限。鉴于许多智能系统（如自动驾驶汽车）在感知系统在安全关键的环境中运行，这就急需更深入地了解这些MSF系统的性能和可靠性。为了弥补这一差距，我们朝着这个方向迈出了早期的一步，并构建了一个基于AI的MSF感知系统基准测试套件，专注于对抗性攻击的鲁棒性和不同环境下的泛化性能。

    Multi-Sensor Fusion (MSF) based perception systems have been the foundation in supporting many industrial applications and domains, such as self-driving cars, robotic arms, and unmanned aerial vehicles. Over the past few years, the fast progress in data-driven artificial intelligence (AI) has brought a fast-increasing trend to empower MSF systems by deep learning techniques to further improve performance, especially on intelligent systems and their perception systems. Although quite a few AI-enabled MSF perception systems and techniques have been proposed, up to the present, limited benchmarks that focus on MSF perception are publicly available. Given that many intelligent systems such as self-driving cars are operated in safety-critical contexts where perception systems play an important role, there comes an urgent need for a more in-depth understanding of the performance and reliability of these MSF systems. To bridge this gap, we initiate an early step in this direction and construc
    
[^36]: GRAFENNE：在具有异质和动态特征集的图上进行学习

    GRAFENNE: Learning on Graphs with Heterogeneous and Dynamic Feature Sets. (arXiv:2306.03447v1 [cs.LG])

    [http://arxiv.org/abs/2306.03447](http://arxiv.org/abs/2306.03447)

    GRAFENNE是一种新的图神经网络框架，通过在原图上进行异构转化，将节点和特征解耦，解决了现有方法普遍存在的特征静态、转移误差等问题，并且能适用于未知节点和特征。

    

    图神经网络（GNN）通常基于对图中每个节点的静态特征集的假设来构建。然而在实践中这一假设经常被违反，现有方法通过特征插补部分解决了这个问题，但是这些方法存在特征集均一、转移误差、无法适应动态特征等局限。本文提出了一种新的GNN框架GRAFENNE来应对这些限制，通过在原图上进行异构转化，将节点和特征解耦，通过精心设计的信息传递方法使得模型参数大小与特征数量无关，能够适用于未知节点和特征。我们证明了GRAFENNE在Weisfeil方程性能上至少与现有的信息传递GNN一样具有表现力。

    Graph neural networks (GNNs), in general, are built on the assumption of a static set of features characterizing each node in a graph. This assumption is often violated in practice. Existing methods partly address this issue through feature imputation. However, these techniques (i) assume uniformity of feature set across nodes, (ii) are transductive by nature, and (iii) fail to work when features are added or removed over time. In this work, we address these limitations through a novel GNN framework called GRAFENNE. GRAFENNE performs a novel allotropic transformation on the original graph, wherein the nodes and features are decoupled through a bipartite encoding. Through a carefully chosen message passing framework on the allotropic transformation, we make the model parameter size independent of the number of features and thereby inductive to both unseen nodes and features. We prove that GRAFENNE is at least as expressive as any of the existing message-passing GNNs in terms of Weisfeil
    
[^37]: 神经网络的可变性崩溃的量化

    Quantifying the Variability Collapse of Neural Networks. (arXiv:2306.03440v1 [cs.LG])

    [http://arxiv.org/abs/2306.03440](http://arxiv.org/abs/2306.03440)

    本文提出了一种叫做可变性崩溃指数（VCI）的新指标，用于量化神经网络的可变性崩溃现象，其优越性质包括在可逆线性变换下的不变性和数值稳定性，此指标可以指示预训练神经网络中的可变性崩溃和可转移性。

    

    最近的研究从实证上展示了神经网络的可转移性和最后一层特征的内类变化之间的正相关关系。最近发现的神经崩溃（NC）现象提供了一种新的视角，以理解神经网络的最后一层几何特征。在本文中，我们提出了一种新的指标，称为可变性崩溃指数（VCI），用于量化NC范式中的可变性崩溃现象。VCI指标具有很强的动机性，并且本质上与最后一层特征的线性探测损失有关。此外，它具有理论上和实证上的优越性质，包括在可逆线性变换下的不变性和数值稳定性，这与之前的指标有所区别。我们的实验验证了VCI在预训练神经网络中指示可变性崩溃和可转移性的能力。

    Recent studies empirically demonstrate the positive relationship between the transferability of neural networks and the within-class variation of the last layer features. The recently discovered Neural Collapse (NC) phenomenon provides a new perspective of understanding such last layer geometry of neural networks. In this paper, we propose a novel metric, named Variability Collapse Index (VCI), to quantify the variability collapse phenomenon in the NC paradigm. The VCI metric is well-motivated and intrinsically related to the linear probing loss on the last layer features. Moreover, it enjoys desired theoretical and empirical properties, including invariance under invertible linear transformations and numerical stability, that distinguishes it from previous metrics. Our experiments verify that VCI is indicative of the variability collapse and the transferability of pretrained neural networks.
    
[^38]: 代码大语言模型在填写可能存在漏洞的代码时存在失败问题

    Large Language Models of Code Fail at Completing Code with Potential Bugs. (arXiv:2306.03438v1 [cs.LG])

    [http://arxiv.org/abs/2306.03438](http://arxiv.org/abs/2306.03438)

    本研究探讨了存在漏洞的代码补全问题，设计了两个数据集并发现这些漏洞显著降低了Code-LLMs的生成性能。

    

    最近，代码大语言模型（Code-LLMs）在代码补全方面取得了巨大进展，这是编程辅助和代码智能的基本功能。然而，大多数现有的研究忽略了在生成过程中代码上下文中可能存在的漏洞问题，在软件开发中这是不可避免的。因此，我们引入并研究了存在漏洞的代码补全问题，受实时代码建议的现实场景启发，代码上下文中包含可能的漏洞-反模式，这些反模式可以成为完成程序中的漏洞。为了系统地研究任务，我们引入了两个数据集：一个是从语义改变操作中派生的合成漏洞数据集（buggy-HumanEval），另一个是从用户提交的编程问题中派生的现实漏洞数据集（buggy-FixEval）。我们发现，可能存在漏洞的情况显著降低了高性能Code-LLMs的生成性能。例如，CodeGen-2B-mono在测试数据集上的通过率

    Large language models of code (Code-LLMs) have recently brought tremendous advances to code completion, a fundamental feature of programming assistance and code intelligence. However, most existing works ignore the possible presence of bugs in the code context for generation, which are inevitable in software development. Therefore, we introduce and study the buggy-code completion problem, inspired by the realistic scenario of real-time code suggestion where the code context contains potential bugs -- anti-patterns that can become bugs in the completed program. To systematically study the task, we introduce two datasets: one with synthetic bugs derived from semantics-altering operator changes (buggy-HumanEval) and one with realistic bugs derived from user submissions to coding problems (buggy-FixEval). We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs. For instance, the passing rates of CodeGen-2B-mono on test 
    
[^39]: 通过水印扩散过程保护扩散模型的知识产权

    Protecting the Intellectual Property of Diffusion Models by the Watermark Diffusion Process. (arXiv:2306.03436v1 [cs.CR])

    [http://arxiv.org/abs/2306.03436](http://arxiv.org/abs/2306.03436)

    本文提出了一种新的水印扩散过程来保护扩散模型的知识产权，该过程通过训练或微调扩散模型来学习，不同于任务数据的标准扩散过程，能够在不降低性能的情况下提取出嵌入的水印。

    

    随着生成任务的不断增加，扩散模型已成为最先进的深度生成架构。训练大型扩散模型以获得良好性能需要高资源成本，因此需要保护它们的知识产权。本文提出了一种新的扩散模型水印方法WDM，包括水印嵌入、提取和验证。WDM通过训练或微调扩散模型来学习一个水印扩散过程（WDP）来嵌入水印数据，该过程与任务数据的标准扩散过程不同。嵌入的水印可以通过使用学习WDP的共享反向噪声进行采样而不会降低原始任务的性能。我们还通过将WDP与修改扩散过程连接来提供所提出方法的理论基础和分析。

    Diffusion models have emerged as state-of-the-art deep generative architectures with the increasing demands for generation tasks. Training large diffusion models for good performance requires high resource costs, making them valuable intellectual properties to protect. While most of the existing ownership solutions, including watermarking, mainly focus on discriminative models. This paper proposes WDM, a novel watermarking method for diffusion models, including watermark embedding, extraction, and verification. WDM embeds the watermark data through training or fine-tuning the diffusion model to learn a Watermark Diffusion Process (WDP), different from the standard diffusion process for the task data. The embedded watermark can be extracted by sampling using the shared reverse noise from the learned WDP without degrading performance on the original task. We also provide theoretical foundations and analysis of the proposed method by connecting the WDP to the diffusion process with a modi
    
[^40]: 关注点对Prompt-tuning的作用

    On the Role of Attention in Prompt-tuning. (arXiv:2306.03435v1 [cs.LG])

    [http://arxiv.org/abs/2306.03435](http://arxiv.org/abs/2306.03435)

    本论文研究了Prompt-tuning在注意力架构中的应用，通过探索上下文混合模型，表明softmax-prompt-attention在表达上优于其他模型，同时也证明了该方法可以高效的使用数据学习提示。

    

    Prompt-tuning 是一种新兴的策略，通过从数据中学习 (软) 提示参数，使大型语言模型 (LLM) 适应下游任务。尽管其在 LLM 中取得了成功，但对于 Prompt-tuning 的能力及关注机制在提示中的作用，理论理解尚有限。在这项工作中，我们探索一个注意力架构的 Prompt-tuning，并研究上下文混合模型，其中每个输入表示属于上下文相关或无关集合。我们通过一个自包含的提示-注意力模型来隔离 Prompt-tuning 的作用。我们的贡献如下：(1) 我们表明在我们的上下文数据模型下，softmax-prompt-attention 在可证明地比softmax-self-attention 和线性提示注意力更具表达力。(2) 我们分析了渐变下降的初始轨迹，并展示可以通过近乎最优的样本复杂度学习提示和预测头，从而证明了提示可以证明地注意到稀疏的上下文相关信息。(3)

    Prompt-tuning is an emerging strategy to adapt large language models (LLM) to downstream tasks by learning a (soft-)prompt parameter from data. Despite its success in LLMs, there is limited theoretical understanding of the power of prompt-tuning and the role of the attention mechanism in prompting. In this work, we explore prompt-tuning for one-layer attention architectures and study contextual mixture-models where each input token belongs to a context-relevant or -irrelevant set. We isolate the role of prompt-tuning through a self-contained prompt-attention model. Our contributions are as follows: (1) We show that softmax-prompt-attention is provably more expressive than softmax-self-attention and linear-prompt-attention under our contextual data model. (2) We analyze the initial trajectory of gradient descent and show that it learns the prompt and prediction head with near-optimal sample complexity and demonstrate how prompt can provably attend to sparse context-relevant tokens. (3) 
    
[^41]: 采用图卷积网络的组合优化最小支配集问题的学习启发式方法

    Learning-Based Heuristic for Combinatorial Optimization of the Minimum Dominating Set Problem using Graph Convolutional Networks. (arXiv:2306.03434v1 [cs.LG])

    [http://arxiv.org/abs/2306.03434](http://arxiv.org/abs/2306.03434)

    本论文提出了一种采用图卷积网络计算最小支配集问题的学习启发式方法，取得了胜过传统算法的成果，并且在不同数据集上表现出很好的泛化能力。

    

    图的支配集是指一个顶点子集 $S\subseteq\mathcal{V}$，使得集合外的每个顶点 $v\in \mathcal{V} \setminus S$ 都与集合内的某个顶点 $u\in S$ 相邻。最小支配集问题旨在寻找具有最小基数的支配集，并且是一个已知的 NP-难问题。我们提出了一种新颖的基于学习的启发式方法，利用图卷积网络计算最小支配集问题的解。我们对所提出的方法进行了大量的实验评估，包括随机生成的图和真实世界的图数据集。我们的结果表明，所提出的基于学习的方法可以胜过传统的贪婪逼近算法。此外，我们演示了图卷积网络在数据集上的泛化能力以及其能够扩展到比其训练数据所包含的更高级别的图上的能力。

    A dominating set of a graph $\mathcal{G=(V, E)}$ is a subset of vertices $S\subseteq\mathcal{V}$ such that every vertex $v\in \mathcal{V} \setminus S$ outside the dominating set is adjacent to a vertex $u\in S$ within the set. The minimum dominating set problem seeks to find a dominating set of minimum cardinality and is a well-established NP-hard combinatorial optimization problem. We propose a novel learning-based heuristic approach to compute solutions for the minimum dominating set problem using graph convolutional networks. We conduct an extensive experimental evaluation of the proposed method on a combination of randomly generated graphs and real-world graph datasets. Our results indicate that the proposed learning-based approach can outperform a classical greedy approximation algorithm. Furthermore, we demonstrate the generalization capability of the graph convolutional network across datasets and its ability to scale to graphs of higher order than those on which it was trained.
    
[^42]: 学习模拟树枝动力学以进行操纵

    Learning to Simulate Tree-Branch Dynamics for Manipulation. (arXiv:2306.03410v1 [cs.RO])

    [http://arxiv.org/abs/2306.03410](http://arxiv.org/abs/2306.03410)

    本论文提出了一种仿真驱动的逆推理方法来学习树枝动态，并可以操纵可变形的植被，以解决密集植被中容易遮挡的任务，算法结合了生物学上的假设和传统参数推理方法的有限差分方案。

    

    我们提出使用仿真驱动的逆推理方法来模拟操纵下的树枝关节动力学。学习枝干动态并获得操纵可变形植被的能力可帮助处理容易遮挡的任务，例如在密集树叶中采摘水果、移动悬垂的藤蔓和树枝，以便在密集植被中导航。植物的可变形几何形状通过在并行、不可微模拟器上执行的粗略弹簧抽象来实现。由模拟器定义的隐式统计模型、通过主动探测的地面真实情况获得的参考轨迹和贝叶斯形式主义一起指导弹簧参数后验密度估计。我们的无参数推理算法基于斯坦变分梯度下降，并将生物学上的假设作为神经网络驱动的学习联合先验合并到推理过程中。此外，它利用了有限差分方案来对函数梯度进行估计，从而克服了传统参数推理方法中的梯度计算困难和维度灾难问题。

    We propose to use a simulation driven inverse inference approach to model the joint dynamics of tree branches under manipulation. Learning branch dynamics and gaining the ability to manipulate deformable vegetation can help with occlusion-prone tasks, such as fruit picking in dense foliage, as well as moving overhanging vines and branches for navigation in dense vegetation. The underlying deformable tree geometry is encapsulated as coarse spring abstractions executed on parallel, non-differentiable simulators. The implicit statistical model defined by the simulator, reference trajectories obtained by actively probing the ground truth, and the Bayesian formalism, together guide the spring parameter posterior density estimation. Our non-parametric inference algorithm, based on Stein Variational Gradient Descent, incorporates biologically motivated assumptions into the inference process as neural network driven learnt joint priors; moreover, it leverages the finite difference scheme for g
    
[^43]: 智能体通过探索决策树中的状态来提高其决策模型的性能

    Agents Explore the Environment Beyond Good Actions to Improve Their Model for Better Decisions. (arXiv:2306.03408v1 [cs.AI])

    [http://arxiv.org/abs/2306.03408](http://arxiv.org/abs/2306.03408)

    通过探索未被访问的决策树状态和引入随机性，MuZero智能体改进了树搜索规划和模型预测之间的不一致性，提高了决策能力。

    

    提高智能体决策能力是人工智能发展道路上的一个关键挑战。MuZero智能体通过网络模型的预测和基于预测结果的树搜索规划相结合来提高规划技能，但当模型预测结果不准确时，学习进程可能会遇到瓶颈。我们通过让智能体探索环境中决策树中一些不会被访问到的状态来改进模型的性能。具体而言，智能体首先通过规划得到改进策略，然后在每个训练阶段的开始时随机偏离这个策略。在一个随机的时间阶段，智能体将又恢复到改进策略以得到环境奖励并对期望价值进行学习。我们在井字棋游戏中展示了该方法对智能体性能的提升。

    Improving the decision-making capabilities of agents is a key challenge on the road to artificial intelligence. To improve the planning skills needed to make good decisions, MuZero's agent combines prediction by a network model and planning by a tree search using the predictions. MuZero's learning process can fail when predictions are poor but planning requires them. We use this as an impetus to get the agent to explore parts of the decision tree in the environment that it otherwise would not explore. The agent achieves this, first by normal planning to come up with an improved policy. Second, it randomly deviates from this policy at the beginning of each training episode. And third, it switches back to the improved policy at a random time step to experience the rewards from the environment associated with the improved policy, which is the basis for learning the correct value expectation. The simple board game Tic-Tac-Toe is used to illustrate how this approach can improve the agent's 
    
[^44]: 从流形学习的角度分析深度神经网络结构

    Deep neural networks architectures from the perspective of manifold learning. (arXiv:2306.03406v1 [cs.LG])

    [http://arxiv.org/abs/2306.03406](http://arxiv.org/abs/2306.03406)

    本文从几何学和拓扑学的角度，使用拓扑数据分析和持久同调分形维度对神经网络体系结构进行全面比较和描述，旨在为可解释和可解释的人工智能的发展做出贡献。

    

    虽然深度学习在各个领域得到了显著进展，但神经网络模型的学习过程仍是一个重要的开放问题。本文旨在从几何学和拓扑学的角度全面比较和描述神经网络体系结构。我们关注神经网络的内部表示以及在不同层上数据流形的拓扑和几何结构的动态变化。在本文中，我们使用了拓扑数据分析（TDA）和持久同调分形维度的概念。我们使用各种数据集和卷积神经网络（CNN）结构以及在计算机视觉和自然语言处理任务中使用的变压器进行了广泛的实验。我们的工作是在几何深度学习的框架内为可解释和可解释的人工智能的发展做出贡献。

    Despite significant advances in the field of deep learning in ap-plications to various areas, an explanation of the learning pro-cess of neural network models remains an important open ques-tion. The purpose of this paper is a comprehensive comparison and description of neural network architectures in terms of ge-ometry and topology. We focus on the internal representation of neural networks and on the dynamics of changes in the topology and geometry of a data manifold on different layers. In this paper, we use the concepts of topological data analysis (TDA) and persistent homological fractal dimension. We present a wide range of experiments with various datasets and configurations of convolutional neural network (CNNs) architectures and Transformers in CV and NLP tasks. Our work is a contribution to the development of the important field of explainable and interpretable AI within the framework of geometrical deep learning.
    
[^45]: 使用高斯过程的自主赛车车辆动力学建模

    Vehicle Dynamics Modeling for Autonomous Racing Using Gaussian Processes. (arXiv:2306.03405v1 [cs.RO])

    [http://arxiv.org/abs/2306.03405](http://arxiv.org/abs/2306.03405)

    本文介绍了一种使用高斯过程回归逼近车辆动力学模型的方法，在高速自主赛车中，平衡计算需求和模型精度后，该方法可行。

    

    自主赛车越来越成为当前自主驾驶技术在极限情况下的一个试验场。本文提出了一种使用学习方法，如高斯过程回归，来逼近车辆动力学模型的方法。通过平衡计算需求和模型精度，本文提供了一种可行的车辆动力学模型选择方法。

    Autonomous racing is increasingly becoming a proving ground for autonomous vehicle technology at the limits of its current capabilities. The most prominent examples include the F1Tenth racing series, Formula Student Driverless (FSD), Roborace, and the Indy Autonomous Challenge (IAC). Especially necessary, in high speed autonomous racing, is the knowledge of accurate racecar vehicle dynamics. The choice of the vehicle dynamics model has to be made by balancing the increasing computational demands in contrast to improved accuracy of more complex models. Recent studies have explored learning-based methods, such as Gaussian Process (GP) regression for approximating the vehicle dynamics model. However, these efforts focus on higher level constructs such as motion planning, or predictive control and lack both in realism and rigor of the GP modeling process, which is often over-simplified. This paper presents the most detailed analysis of the applicability of GP models for approximating vehic
    
[^46]: SGAT4PASS：面向球面几何意识的全景语义分割Transformer

    SGAT4PASS: Spherical Geometry-Aware Transformer for PAnoramic Semantic Segmentation. (arXiv:2306.03403v1 [cs.CV])

    [http://arxiv.org/abs/2306.03403](http://arxiv.org/abs/2306.03403)

    本论文提出了SGAT4PASS，一种面向球面几何意识的全景语义分割Transformer，通过加入球面几何感知的约束，能更好地捕捉全景图像的3D属性，从而提高分割性能。

    

    作为计算机视觉中一个重要且具有挑战性的问题，全景语义分割可以根据超广角观察到的完整场景来进行感知。传统的针对2D全景图像的PASS方法侧重于解决图像畸变问题，但缺乏对原始360°数据的3D属性的考虑。因此，当输入具有3D扰动的全景图像时，它们的性能会大幅下降。为了更好地应对3D扰动，我们提出了一种面向球面几何意识的全景语义分割Transformer，即SGAT4PASS。具体来说，我们提出了一个球面几何意识的分割框架，它包括三个模块，即球面几何感知图像投影，球面可形变补丁嵌入和全景感知损失，它对具有3D扰动的输入图像进行处理，并对已有的可形变补丁嵌入加入了球面几何感知的约束。

    As an important and challenging problem in computer vision, PAnoramic Semantic Segmentation (PASS) gives complete scene perception based on an ultra-wide angle of view. Usually, prevalent PASS methods with 2D panoramic image input focus on solving image distortions but lack consideration of the 3D properties of original $360^{\circ}$ data. Therefore, their performance will drop a lot when inputting panoramic images with the 3D disturbance. To be more robust to 3D disturbance, we propose our Spherical Geometry-Aware Transformer for PAnoramic Semantic Segmentation (SGAT4PASS), considering 3D spherical geometry knowledge. Specifically, a spherical geometry-aware framework is proposed for PASS. It includes three modules, i.e., spherical geometry-aware image projection, spherical deformable patch embedding, and a panorama-aware loss, which takes input images with 3D disturbance into account, adds a spherical geometry-aware constraint on the existing deformable patch embedding, and indicates
    
[^47]: 带有实例和标签相关的标签噪声的二分类问题

    Binary Classification with Instance and Label Dependent Label Noise. (arXiv:2306.03402v1 [stat.ML])

    [http://arxiv.org/abs/2306.03402](http://arxiv.org/abs/2306.03402)

    本文研究解决带有实例和标签相关的标签噪声对于二分类问题的困难，通过理论分析得到经验风险最小化可以实现最优的超额风险界限。

    

    学习带有标签相关的标签噪声在理论和实践中得到了广泛探讨，然而处理带有实例和标签相关的标签噪声仍然是一项具有挑战性的任务。这种困难在于噪声率因每个实例而异，使得准确估计噪声率成为一项具有挑战性的任务。目前还没有解决能否仅使用含有噪声样本来学习可靠模型的问题。我们通过理论分析回答了这个问题，提供了匹配的上界和下界。令人惊讶的是，我们的结果表明，不需要任何额外的假设，经验风险最小化可以实现最优的超额风险界限。具体而言，我们通过比较从干净样本和噪声样本中得到的经验风险最小化器来导出一种与噪声水平成比例的新的超额风险界限，在非常一般的情况下都成立。其次，我们表明了0-1损失的极小极大下界是一个与标签数成比例的常数。

    Learning with label dependent label noise has been extensively explored in both theory and practice; however, dealing with instance (i.e., feature) and label dependent label noise continues to be a challenging task. The difficulty arises from the fact that the noise rate varies for each instance, making it challenging to estimate accurately. The question of whether it is possible to learn a reliable model using only noisy samples remains unresolved. We answer this question with a theoretical analysis that provides matching upper and lower bounds. Surprisingly, our results show that, without any additional assumptions, empirical risk minimization achieves the optimal excess risk bound. Specifically, we derive a novel excess risk bound proportional to the noise level, which holds in very general settings, by comparing the empirical risk minimizers obtained from clean samples and noisy samples. Second, we show that the minimax lower bound for the 0-1 loss is a constant proportional to the
    
[^48]: 处理联邦平均中未知参与概率的轻量级方法

    A Lightweight Method for Tackling Unknown Participation Probabilities in Federated Averaging. (arXiv:2306.03401v1 [cs.LG])

    [http://arxiv.org/abs/2306.03401](http://arxiv.org/abs/2306.03401)

    本文提出了一种轻量级方法来调整联邦平均中的聚合权重，通过根据每个客户的参与历史来处理具有不同参与率的客户，解决了在联邦学习中未知参与概率的问题。

    

    在联邦学习中，客户端通常具有先验未知的不同参与率，如果不适当处理，则可能会对联邦学习的性能造成重大影响。现有的解决方法通常基于全局方差缩减，这需要大量额外的内存，其乘法因子等于客户总数。一个重要的未解决问题是找到一种轻量级方法来处理具备不同参与率客户的联邦学习。在这篇论文中，我们通过根据每个客户的参与历史来调整联邦平均（FedAvg）中的聚合权重来解决此问题。我们首先展示了在具有异构参与概率的情况下，非最优聚合权重的FedAvg可能会从原始FL目标的最优解偏离，这表明需要找到最优聚合权重。然而，当参与概率不可知时计算最优权重非常困难。

    In federated learning (FL), clients usually have diverse participation probabilities that are unknown a priori, which can significantly harm the performance of FL if not handled properly. Existing works aiming at addressing this problem are usually based on global variance reduction, which requires a substantial amount of additional memory in a multiplicative factor equal to the total number of clients. An important open problem is to find a lightweight method for FL in the presence of clients with unknown participation rates. In this paper, we address this problem by adapting the aggregation weights in federated averaging (FedAvg) based on the participation history of each client. We first show that, with heterogeneous participation probabilities, FedAvg with non-optimal aggregation weights can diverge from the optimal solution of the original FL objective, indicating the need of finding optimal aggregation weights. However, it is difficult to compute the optimal weights when the part
    
[^49]: G-CAME: 面向目标检测的高斯类激活映射解释器

    G-CAME: Gaussian-Class Activation Mapping Explainer for Object Detectors. (arXiv:2306.03400v1 [cs.CV])

    [http://arxiv.org/abs/2306.03400](http://arxiv.org/abs/2306.03400)

    G-CAME 提出了一种面向目标检测的高斯类激活映射解释器，通过使用激活映射与高斯核生成显著性图来突出显示图像中与预测框相关的重要区域，具有很短时间解释对象等优点。

    

    当今，图像目标检测的深度神经网络非常普及。然而，由于这些网络的复杂性，用户很难理解模型为什么会检测出这些对象。我们提出了高斯类激活映射解释器（G-CAME），它生成显著性图作为目标检测模型的说明。 G-CAME 可以被认为是一种基于 CAM 的方法，它使用选择层的激活映射与高斯核来突出显示图像中与预测框相关的重要区域。与其他基于区域的方法相比，G-CAME 可以超越时间限制，因为它只需要很短时间就能解释一个对象。我们还在 MS-COCO 2017 数据集上使用 YOLOX 定量和定性地评估了我们的方法，并指导将 G-CAME 应用于两阶段 Faster-RCNN 模型。

    Nowadays, deep neural networks for object detection in images are very prevalent. However, due to the complexity of these networks, users find it hard to understand why these objects are detected by models. We proposed Gaussian Class Activation Mapping Explainer (G-CAME), which generates a saliency map as the explanation for object detection models. G-CAME can be considered a CAM-based method that uses the activation maps of selected layers combined with the Gaussian kernel to highlight the important regions in the image for the predicted box. Compared with other Region-based methods, G-CAME can transcend time constraints as it takes a very short time to explain an object. We also evaluated our method qualitatively and quantitatively with YOLOX on the MS-COCO 2017 dataset and guided to apply G-CAME into the two-stage Faster-RCNN model.
    
[^50]: 熵优化输运的最小内在维度缩放

    Minimum intrinsic dimension scaling for entropic optimal transport. (arXiv:2306.03398v1 [math.ST])

    [http://arxiv.org/abs/2306.03398](http://arxiv.org/abs/2306.03398)

    该研究提出了最小内在维度缩放现象，在不做出数据分布假设的情况下，可以应用于各种熵优化输运问题中，以达到更优的结果。

    

    受流形假说的启发，我们对熵优化输运进行精细的统计界限设计，该界限对数据的内在维度敏感，并仅在单一距离尺度下衡量盲区内在维度。我们称之为最小内在维度缩放（MID scaling）现象，它表明仅有这些单尺度内在维度的最小值才控制收敛速度。我们的结果显示，MID scaling是一个通用现象，可以应用于不同的熵优化输运问题，不需要对数据分布进行假设。当一个分布集中在流形上时，我们的结果有更强的对应物。

    Motivated by the manifold hypothesis, which states that data with a high extrinsic dimension may yet have a low intrinsic dimension, we develop refined statistical bounds for entropic optimal transport that are sensitive to the intrinsic dimension of the data. Our bounds involve a robust notion of intrinsic dimension, measured at only a single distance scale depending on the regularization parameter, and show that it is only the minimum of these single-scale intrinsic dimensions which governs the rate of convergence. We call this the Minimum Intrinsic Dimension scaling (MID scaling) phenomenon, and establish MID scaling with no assumptions on the data distributions so long as the cost is bounded and Lipschitz, and for various entropic optimal transport quantities beyond just values, with stronger analogs when one distribution is supported on a manifold. Our results significantly advance the theoretical state of the art by showing that MID scaling is a generic phenomenon, and provide th
    
[^51]: 通过引力导向的GAN生成起始-目的地网络

    Origin-Destination Network Generation via Gravity-Guided GAN. (arXiv:2306.03390v1 [cs.LG])

    [http://arxiv.org/abs/2306.03390](http://arxiv.org/abs/2306.03390)

    本文结合物理科学知识和数据驱动的ML方法，提出了一种名为ODGN的模型来生成起始-目的地网络，以更好地进行人口流动建模。

    

    起始-目的地（OD）流包含有价值的人口迁移信息，包括方向和体积，在许多城市应用中至关重要，例如城市规划，交通管理等。然而，由于高成本或隐私问题，OD数据并不总是易于获得。因此，我们必须考虑通过数学模型生成OD。现有的方法利用物理定律或机器学习（ML）模型来构建城市结构和OD流之间的关联性，这两种方法分别受到过度简化和较差的泛化能力的限制。本文提出采用物理知识和数据驱动的ML方法相结合的物理知识驱动ML范式，构建了一种名为Origin-Destination Generation Networks（ODGN）的模型，通过利用物理和ML方法的互补优势来实现更好地人口流动建模。具体而言，我们首先建立一个多视图模型

    Origin-destination (OD) flow, which contains valuable population mobility information including direction and volume, is critical in many urban applications, such as urban planning, transportation management, etc. However, OD data is not always easy to access due to high costs or privacy concerns. Therefore, we must consider generating OD through mathematical models. Existing works utilize physics laws or machine learning (ML) models to build the association between urban structures and OD flows while these two kinds of methods suffer from the limitation of over-simplicity and poor generalization ability, respectively. In this paper, we propose to adopt physics-informed ML paradigm, which couple the physics scientific knowledge and data-driven ML methods, to construct a model named Origin-Destination Generation Networks (ODGN) for better population mobility modeling by leveraging the complementary strengths of combining physics and ML methods. Specifically, we first build a Multi-view 
    
[^52]: 在线张量学习：计算和统计权衡，适应性和最优遗憾

    Online Tensor Learning: Computational and Statistical Trade-offs, Adaptivity and Optimal Regret. (arXiv:2306.03372v1 [stat.ML])

    [http://arxiv.org/abs/2306.03372](http://arxiv.org/abs/2306.03372)

    本文提出了在线黎曼梯度下降算法，用于在在线情况下估计潜在的低秩张量。其中，我们在处理连续或分类变量时提供了灵活的方法，并在在线情况下尝试了两个具体的应用，即在线张量补全和在线二元张量学习。我们还建立了逐个条目的精确错误界限，这是在在线张量补全中首次纳入噪声。我们观察到，在存在噪声的情况下，计算和统计方面存在着令人惊讶的权衡。

    

    我们研究了一个广义框架，用于在线情况下估计潜在的低秩张量，包括线性和广义线性模型。该框架提供了一种处理连续或分类变量的灵活方法。此外，我们研究了两个具体的应用：在线张量补全和在线二元张量学习。为了应对这些挑战，我们提出了在线黎曼梯度下降算法，在所有应用程序中都可以根据适当的条件线性收敛并恢复低秩组件。此外，我们为在线张量补全建立了精确的逐个条目错误界限。值得注意的是，我们的工作代表了首次尝试在在线低秩张量恢复任务中纳入噪声的努力。有趣的是，我们观察到在存在噪声的情况下，在计算和统计方面存在着令人惊讶的权衡。增加步长可以加快收敛，但会导致更高的统计误差。

    We investigate a generalized framework for estimating latent low-rank tensors in an online setting, encompassing both linear and generalized linear models. This framework offers a flexible approach for handling continuous or categorical variables. Additionally, we investigate two specific applications: online tensor completion and online binary tensor learning. To address these challenges, we propose the online Riemannian gradient descent algorithm, which demonstrates linear convergence and the ability to recover the low-rank component under appropriate conditions in all applications. Furthermore, we establish a precise entry-wise error bound for online tensor completion. Notably, our work represents the first attempt to incorporate noise in the online low-rank tensor recovery task. Intriguingly, we observe a surprising trade-off between computational and statistical aspects in the presence of noise. Increasing the step size accelerates convergence but leads to higher statistical error
    
[^53]: 在单位球上学习表示：应用于在线连续学习

    Learning Representations on the Unit Sphere: Application to Online Continual Learning. (arXiv:2306.03364v1 [cs.LG])

    [http://arxiv.org/abs/2306.03364](http://arxiv.org/abs/2306.03364)

    该论文提出了一种基于单位球的表示学习方法，通过将表示推向固定方向，使得学习策略对数据漂移具有弹性，从而能够应对在线连续学习的挑战性问题。

    

    我们使用最大后验估计原理来学习分布在单位球上的表示。我们针对对称方向数据建立了 von Mises-Fisher 分布和角高斯分布的损失函数。我们方法的一个显著特点是，学习到的表示被推向固定的方向，使得学习策略对数据漂移具有弹性。这使得它适合于在线连续学习，即在连续的数据流上训练神经网络的问题，其中多个分类任务按顺序呈现，因此过去任务的数据不再可用，当前任务的数据只能看一次。为了应对这种具有挑战性的情况，我们提出了一种基于记忆的表示学习技术，配备了我们的新损失函数。我们的方法不需要负数据或任务边界的知识，并且在较小的批处理下表现良好。

    We use the maximum a posteriori estimation principle for learning representations distributed on the unit sphere. We derive loss functions for the von Mises-Fisher distribution and the angular Gaussian distribution, both designed for modeling symmetric directional data. A noteworthy feature of our approach is that the learned representations are pushed toward fixed directions, allowing for a learning strategy that is resilient to data drift. This makes it suitable for online continual learning, which is the problem of training neural networks on a continuous data stream, where multiple classification tasks are presented sequentially so that data from past tasks are no longer accessible, and data from the current task can be seen only once. To address this challenging scenario, we propose a memory-based representation learning technique equipped with our new loss functions. Our approach does not require negative data or knowledge of task boundaries and performs well with smaller batch s
    
[^54]: 使用行为偏好查询提升离线强化学习

    Boosting Offline Reinforcement Learning with Action Preference Query. (arXiv:2306.03362v1 [cs.LG])

    [http://arxiv.org/abs/2306.03362](http://arxiv.org/abs/2306.03362)

    本文提出了一种名为Offline-with-Action-Preferences（OAP）的无交互训练方案，通过查询先前收集的和学习到的行动之间的偏好，来帮助解决错误估计问题，从而获得对未见数据更精确的评估。

    

    训练实用代理通常涉及离线和在线强化学习以平衡政策的性能和交互成本。本文介绍了一种无需交互的训练方案 Offline-with-Action-Preferences（OAP）。 OAP的主要见解是，与在线微调相比，查询事先收集的和学习到的行为之间的偏好可以同样或甚至更有助于解决错误估计问题。通过根据行为偏好自适应地鼓励或抑制策略约束，OAP可以区分过度估计和有益的策略改进，从而获得对未见数据更精确的评估。

    Training practical agents usually involve offline and online reinforcement learning (RL) to balance the policy's performance and interaction costs. In particular, online fine-tuning has become a commonly used method to correct the erroneous estimates of out-of-distribution data learned in the offline training phase. However, even limited online interactions can be inaccessible or catastrophic for high-stake scenarios like healthcare and autonomous driving. In this work, we introduce an interaction-free training scheme dubbed Offline-with-Action-Preferences (OAP). The main insight is that, compared to online fine-tuning, querying the preferences between pre-collected and learned actions can be equally or even more helpful to the erroneous estimate problem. By adaptively encouraging or suppressing policy constraint according to action preferences, OAP could distinguish overestimation from beneficial policy improvement and thus attains a more accurate evaluation of unseen data. Theoretica
    
[^55]: Vid2Act：为视觉强化学习激活离线视频

    Vid2Act: Activate Offline Videos for Visual RL. (arXiv:2306.03360v1 [cs.LG])

    [http://arxiv.org/abs/2306.03360](http://arxiv.org/abs/2306.03360)

    Vid2Act是一种基于模型的强化学习方法，它通过使用世界模型来传输领域相关的动态和策略，从而显著提高了样本效率。

    

    在离线视频数据集上预训练强化学习模型是提高其在线任务效率的有前途的方法，但由于跨域中任务、动态和行为的固有不匹配性而具有挑战性。最近，一种名为APV的模型避免了离线数据集中的伴随动作记录，而是专注于在源域内预训练与任务无关的、不涉及操作的世界模型。我们提出了Vid2Act，一种基于模型的强化学习方法，它学习从离线到在线环境中传输有价值的动作条件动态和潜在有用的动作演示。其主要思想是不仅将世界模型用作行为学习的模拟器，还将其用作测量领域相关性的工具，以便进行动态表示传输和策略传输。具体地，我们通过域选择知识蒸馏损失训练世界模型生成一组时间变化的任务相似度。这些相似度有两个目的：（i）自适应地将最相关的领域的动态传输到在线环境，和（ii）在在线环境中指导代理集中执行任务相关的动作。在Atari和DMControl连续控制任务上的实验结果表明了我们方法的有效性，其在样本效率方面大大优于之前的最先进的离线强化学习方法。

    Pretraining RL models on offline video datasets is a promising way to improve their training efficiency in online tasks, but challenging due to the inherent mismatch in tasks, dynamics, and behaviors across domains. A recent model, APV, sidesteps the accompanied action records in offline datasets and instead focuses on pretraining a task-irrelevant, action-free world model within the source domains. We present Vid2Act, a model-based RL method that learns to transfer valuable action-conditioned dynamics and potentially useful action demonstrations from offline to online settings. The main idea is to use the world models not only as simulators for behavior learning but also as tools to measure the domain relevance for both dynamics representation transfer and policy transfer. Specifically, we train the world models to generate a set of time-varying task similarities using a domain-selective knowledge distillation loss. These similarities serve two purposes: (i) adaptively transferring th
    
[^56]: 近正交基函数族的主动学习查询复杂度分析

    Query Complexity of Active Learning for Function Family With Nearly Orthogonal Basis. (arXiv:2306.03356v1 [cs.LG])

    [http://arxiv.org/abs/2306.03356](http://arxiv.org/abs/2306.03356)

    该论文分析了近正交基函数族的主动学习查询复杂度，提出了相应的算法优化方案。

    

    许多机器学习算法需要大量标记数据才能提供最先进的结果。在医学诊断和欺诈检测等领域，虽然存在大量未标记数据，但将这些数据标记成本高昂。主动学习算法旨在减少所需的标记数据点数目，同时保持性能。对于许多凸优化问题，例如线性回归和$p$- 范数回归，都已经存在了所需的标记数目实现某种准确性的理论界限。我们称之为主动学习的查询复杂度。然而，今天的主动学习算法需要所学习函数的正交基与目标函数相匹配。例如，当将主动学习应用于线性回归时，要求目标函数是一组正交线性函数的线性组合，并且可以使用主动学习找到这些线性函数的系数。

    Many machine learning algorithms require large numbers of labeled data to deliver state-of-the-art results. In applications such as medical diagnosis and fraud detection, though there is an abundance of unlabeled data, it is costly to label the data by experts, experiments, or simulations. Active learning algorithms aim to reduce the number of required labeled data points while preserving performance. For many convex optimization problems such as linear regression and $p$-norm regression, there are theoretical bounds on the number of required labels to achieve a certain accuracy. We call this the query complexity of active learning. However, today's active learning algorithms require the underlying learned function to have an orthogonal basis. For example, when applying active learning to linear regression, the requirement is the target function is a linear composition of a set of orthogonal linear functions, and active learning can find the coefficients of these linear functions. We p
    
[^57]: BatchSampler：用于视觉、语言和图形的对比学习的小批量采样器

    BatchSampler: Sampling Mini-Batches for Contrastive Learning in Vision, Language, and Graphs. (arXiv:2306.03355v1 [cs.LG])

    [http://arxiv.org/abs/2306.03355](http://arxiv.org/abs/2306.03355)

    本文提出了一个新的对比学习方法BatchSampler，通过从输入数据中采样难以区分的实例的小批量，并利用重启随机游走来形成小批量，以提高性能。

    

    In-Batch对比学习是一种最先进的自我监督方法，它将语义相似的实例聚集在一起，同时将不相似的实例推到远离mini-batch之外。其成功的关键在于负样本共享策略，其中每个实例都作为mini-batch中其他实例的负样本。最近的研究旨在通过在当前mini-batch范围内采样难负样本来提高性能，但其质量仅受限于mini-batch本身。在这项工作中，我们提出通过从输入数据中采样mini-batch来改进对比学习。我们提出了BatchSampler来采样难以区分的（即彼此难以区分的困难和真实的负样本）实例的小批量。为了使每个小批量具有更少的假负样本，我们设计了随机选择实例的接近度图。为了形成小批量，我们利用接近度图上的重启随机游走来辅助采样。

    In-Batch contrastive learning is a state-of-the-art self-supervised method that brings semantically-similar instances close while pushing dissimilar instances apart within a mini-batch. Its key to success is the negative sharing strategy, in which every instance serves as a negative for the others within the mini-batch. Recent studies aim to improve performance by sampling hard negatives \textit{within the current mini-batch}, whose quality is bounded by the mini-batch itself. In this work, we propose to improve contrastive learning by sampling mini-batches from the input data. We present BatchSampler\footnote{The code is available at \url{https://github.com/THUDM/BatchSampler}} to sample mini-batches of hard-to-distinguish (i.e., hard and true negatives to each other) instances. To make each mini-batch have fewer false negatives, we design the proximity graph of randomly-selected instances. To form the mini-batch, we leverage random walk with restart on the proximity graph to help sam
    
[^58]: 稳定对比强化学习: 离线目标达成的技术

    Stabilizing Contrastive RL: Techniques for Offline Goal Reaching. (arXiv:2306.03346v1 [cs.LG])

    [http://arxiv.org/abs/2306.03346](http://arxiv.org/abs/2306.03346)

    本文提出了一种稳定的对比强化学习方法，通过浅而宽的结构，结合谨慎的权重初始化和数据增强等实验方法，在具有挑战性的仿真基准测试中显著提高了性能，并演示了对比方法可以解决现实世界的机器人任务。

    

    计算机视觉和自然语言处理领域已经开发了自监督方法，强化学习也可以被视为自监督问题：学习达到任何目标，而不需要人类指定的奖励或标签。然而，为强化学习建立自监督基础实际上面临着一些重要的挑战。基于此前对比学习方法，我们进行了细致的剖析实验，并发现一个浅而宽的结构，结合谨慎的权重初始化和数据增强，可以显着提高与对比强化学习方法的性能，特别是在具有挑战性的仿真基准测试中。此外，我们还演示了通过这些设计决策，对比方法可以解决现实世界的机器人操作任务，其中任务由训练后提供的单个目标图像指定。

    In the same way that the computer vision (CV) and natural language processing (NLP) communities have developed self-supervised methods, reinforcement learning (RL) can be cast as a self-supervised problem: learning to reach any goal, without requiring human-specified rewards or labels. However, actually building a self-supervised foundation for RL faces some important challenges. Building on prior contrastive approaches to this RL problem, we conduct careful ablation experiments and discover that a shallow and wide architecture, combined with careful weight initialization and data augmentation, can significantly boost the performance of these contrastive RL approaches on challenging simulated benchmarks. Additionally, we demonstrate that, with these design decisions, contrastive approaches can solve real-world robotic manipulation tasks, with tasks being specified by a single goal image provided after training.
    
[^59]: 推理时间干预：从语言模型中引导出真实的答案

    Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. (arXiv:2306.03341v1 [cs.LG])

    [http://arxiv.org/abs/2306.03341](http://arxiv.org/abs/2306.03341)

    本研究提出推理时间干预（ITI）技术，通过在推理过程中跨越有限数量的注意力头，显着提高大型语言模型的真实性。在TruthfulQA基准上，ITI使LLaMA模型的真实性从32.5%提高到65.1%。ITI是一种最小程度的干扰，计算廉价，且数据效率高。

    

    我们介绍了推理时间干预（ITI）技术，旨在增强大型语言模型（LLMs）的真实性。ITI通过在推理过程中沿着一组方向移动模型激活，跨越有限数量的注意力头。这种干预显着提高了LLaMA模型在TruthfulQA基准上的表现。在指令微调的LLaMA Alpaca上，ITI将其真实性从32.5％提高到65.1％。我们确定了真实性和可用性之间的权衡，并演示了如何通过调整干预强度来平衡它。ITI 取得了最低程度的干扰且计算廉价。此外，该技术在数据效率上表现优异：虽然像RLHF这样的方法需要广泛注释，但是ITI仅使用了几百个例子就能定位真实的方向。我们的研究结果表明，LLMs可能具有某种内部表示方法来表示某事是真实的可能性，即使它们在表面上产生了虚假的结果。

    We introduce Inference-Time Intervention (ITI), a technique designed to enhance the truthfulness of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.
    
[^60]: 对比学习中的投影头：扩展和收缩的启示

    Unraveling Projection Heads in Contrastive Learning: Insights from Expansion and Shrinkage. (arXiv:2306.03335v1 [stat.ML])

    [http://arxiv.org/abs/2306.03335](http://arxiv.org/abs/2306.03335)

    本文研究了对比学习中的投影头，在理论和实践中找到了两个关键效应：信号方向的扩展和收缩，提出了一系列线性变换来改善下游分类准确性。

    

    本文研究了对比学习中编码器-投影器框架（例如SimCLR）中的投影头，也称为投影仪，的作用。我们旨在揭示一个观察现象的真相：通过下游线性分类准确度的衡量，即使在投影头本身是线性的情况下，也可以学习出在投影器之前的表示优于之后。通过实证和理论分析，我们首先确定了两个由对比损失引起的关键效应。本质上，对比损失会扩展或收缩编码器学习的表示中的信号方向，具体取决于如增强强度，对比损失中使用的温度等因素。其次，受到扩展和收缩现象的启示，我们提出了一系列线性变换来准确建模投影头。我们提出的变换可以提高下游的分类准确度，而且计算成本低，易于实现。

    We investigate the role of projection heads, also known as projectors, within the encoder-projector framework (e.g., SimCLR) used in contrastive learning. We aim to demystify the observed phenomenon where representations learned before projectors outperform those learned after -- measured using the downstream linear classification accuracy, even when the projectors themselves are linear.  In this paper, we make two significant contributions towards this aim. Firstly, through empirical and theoretical analysis, we identify two crucial effects -- expansion and shrinkage -- induced by the contrastive loss on the projectors. In essence, contrastive loss either expands or shrinks the signal direction in the representations learned by an encoder, depending on factors such as the augmentation strength, the temperature used in contrastive loss, etc. Secondly, drawing inspiration from the expansion and shrinkage phenomenon, we propose a family of linear transformations to accurately model the p
    
[^61]: 一种鲁棒性高的新颖性检测似然模型

    A Robust Likelihood Model for Novelty Detection. (arXiv:2306.03331v1 [cs.CV])

    [http://arxiv.org/abs/2306.03331](http://arxiv.org/abs/2306.03331)

    提出了一种鲁棒性高的新颖性检测似然模型，以防御攻击，并取得了优秀的实验结果。

    

    当前的新颖性或异常检测方法基于深度神经网络。然而，尽管它们很有效，神经网络也容易受到输入数据微小变形的影响。这在关键应用或数据被对抗性攻击改变的情况下是一个严重问题。目前有一些针对监督学习的解决方案，但是在新颖性检测中接受的关注很有限。我们提出了一种新的先验，旨在学习一个鲁棒的似然度，以防御攻击。我们还将相同的先验与一种先进的新颖性检测方法结合使用。由于该方法的几何特性，所得到的鲁棒性训练非常有效。初步的理论分析突显了我们方法的潜在优势，随后得到了实验结果的确认。

    Current approaches to novelty or anomaly detection are based on deep neural networks. Despite their effectiveness, neural networks are also vulnerable to imperceptible deformations of the input data. This is a serious issue in critical applications, or when data alterations are generated by an adversarial attack. While this is a known problem that has been studied in recent years for the case of supervised learning, the case of novelty detection has received very limited attention. Indeed, in this latter setting the learning is typically unsupervised because outlier data is not available during training, and new approaches for this case need to be investigated. We propose a new prior that aims at learning a robust likelihood for the novelty test, as a defense against attacks. We also integrate the same prior with a state-of-the-art novelty detection approach. Because of the geometric properties of that approach, the resulting robust training is computationally very efficient. An initia
    
[^62]: AVIDa-hIL6：基于一只被免疫羊驼的大规模VHH数据集用于预测抗原 - 抗体相互作用

    AVIDa-hIL6: A Large-Scale VHH Dataset Produced from an Immunized Alpaca for Predicting Antigen-Antibody Interactions. (arXiv:2306.03329v1 [cs.LG])

    [http://arxiv.org/abs/2306.03329](http://arxiv.org/abs/2306.03329)

    这是一个大规模的VHH数据集，用于预测抗原-抗体相互作用。通过利用VHHs的结构，该数据集包括573,891个抗原-VHH对，并且是当前公开数据集中最大、最全面的之一。

    

    抗体已经成为治疗人类疾病的重要药物。为了加速治疗抗体的发现，计算方法，特别是机器学习，已经引起了相当大的关注，用于预测在抗体候选和目标抗原（如病毒和细菌）之间的特定相互作用。然而，现有研究中公开可用的数据集具有明显的限制，如规模小，缺乏非结合样本和准确的氨基酸序列。为了克服这些限制，我们开发了AVIDa-hIL6, 一个大规模的VHH数据集，用于预测具有人类白细胞介素-6（IL-6）蛋白，作为抗原的VHHs的抗原 - 抗体相互作用。通过利用VHHs的简单结构，有利于通过DNA测序技术识别全长氨基酸序列，AVIDa-hIL6包含573,891个抗原 - VHH对，其中62,067对是结合样本。

    Antibodies have become an important class of therapeutic agents to treat human diseases. To accelerate therapeutic antibody discovery, computational methods, especially machine learning, have attracted considerable interest for predicting specific interactions between antibody candidates and target antigens such as viruses and bacteria. However, the publicly available datasets in existing works have notable limitations, such as small sizes and the lack of non-binding samples and exact amino acid sequences. To overcome these limitations, we have developed AVIDa-hIL6, a large-scale dataset for predicting antigen-antibody interactions in the variable domain of heavy chain of heavy chain antibodies (VHHs), produced from an alpaca immunized with the human interleukin-6 (IL-6) protein, as antigens. By leveraging the simple structure of VHHs, which facilitates identification of full-length amino acid sequences by DNA sequencing technology, AVIDa-hIL6 contains 573,891 antigen-VHH pairs with am
    
[^63]: 面向网络的随机多层组合优化算法，具有独立于层数的收敛速度

    Stochastic Multi-Level Compositional Optimization Algorithms over Networks with Level-Independent Convergence Rate. (arXiv:2306.03322v1 [cs.LG])

    [http://arxiv.org/abs/2306.03322](http://arxiv.org/abs/2306.03322)

    本文提出了面向网络的随机多层组合优化算法，其中包括两种新的分散式优化算法，可实现独立于层数的收敛速度，理论结果和实验证明它们更加有效。

    

    随机多层组合优化问题涵盖了很多新的机器学习范例，如多步骤模型无关元学习，需要大规模应用的高效优化算法。本文研究了分散随机多层优化算法，这是一项具有挑战性的任务，因为多层结构和分散式通讯方案可能增加层数对收敛速度的影响。为此，我们开发了两种新的分散式优化算法，来处理多层函数和其梯度。我们的理论结果表明，与现有的单机算法相比，这两种算法均能在非凸问题中实现独立于层数的收敛速度，条件更加宽松。据我们所知，这是首次在分散式设置下实现独立于层数的收敛速度的工作。此外，广泛的实验证实了其有效性。

    Stochastic multi-level compositional optimization problems cover many new machine learning paradigms, e.g., multi-step model-agnostic meta-learning, which require efficient optimization algorithms for large-scale applications. This paper studies the decentralized stochastic multi-level optimization algorithm, which is challenging because the multi-level structure and decentralized communication scheme may make the number of levels affect the order of the convergence rate. To this end, we develop two novel decentralized optimization algorithms to deal with the multi-level function and its gradient. Our theoretical results show that both algorithms can achieve the level-independent convergence rate for nonconvex problems under much milder conditions compared with existing single-machine algorithms. To the best of our knowledge, this is the first work that achieves the level-independent convergence rate under the decentralized setting. Moreover, extensive experiments confirm the efficacy 
    
[^64]: 多智能体协作：发挥智能 LLM 智能体的力量

    Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents. (arXiv:2306.03314v1 [cs.AI])

    [http://arxiv.org/abs/2306.03314](http://arxiv.org/abs/2306.03314)

    本文提出了一种新的框架，利用多智能体系统发挥大型语言模型的能力，解决循环问题、安全风险、可扩展性、系统评估以及道德考虑等挑战，提供一个结合大型语言模型和多智能体系统优势的方式，更高效和有效地处理复杂任务。

    

    本文提出了一种利用多智能体系统发挥大型语言模型（LLMs）能力的新框架。我们的框架引入了一个协作环境，多个智能体组件，每个智能体都具有独特的属性和角色，共同处理复杂任务，更加高效有效。我们通过人工智能（AGI）中的案例研究，特别关注Auto-GPT 和BabyAGI 模型，展示了我们框架的实用性和多样性。我们还研究了“Gorilla”模型，该模型将外部 API 集成到 LLM中。我们的框架解决了循环问题、安全风险、可扩展性、系统评估以及道德考虑等限制和挑战。通过对法庭模拟和软件开发场景等不同领域的建模，我们展示了提议的多智能体系统的潜在应用和益处。我们的框架提供了一个结合大型语言模型和多智能体系统优势的途径，实现对复杂任务的更高效和有效处理。

    In this paper, we present a novel framework for enhancing the capabilities of large language models (LLMs) by leveraging the power of multi-agent systems. Our framework introduces a collaborative environment where multiple intelligent agent components, each with distinctive attributes and roles, work together to handle complex tasks more efficiently and effectively. We demonstrate the practicality and versatility of our framework through case studies in artificial general intelligence (AGI), specifically focusing on the Auto-GPT and BabyAGI models. We also examine the "Gorilla" model, which integrates external APIs into the LLM. Our framework addresses limitations and challenges such as looping issues, security risks, scalability, system evaluation, and ethical considerations. By modeling various domains such as courtroom simulations and software development scenarios, we showcase the potential applications and benefits of our proposed multi-agent system. Our framework provides an aven
    
[^65]: 使用代理人群体学习序列任务的嵌入

    Learning Embeddings for Sequential Tasks Using Population of Agents. (arXiv:2306.03311v1 [cs.LG])

    [http://arxiv.org/abs/2306.03311](http://arxiv.org/abs/2306.03311)

    该研究基于代理人群体提出了一个信息理论框架，用于在强化学习任务中学习固定维度的嵌入，可以通过观察代理在一小组任务上的表现，来预测其在测试任务上的表现，并且可以从给定的任务选项中选择具有所需特征的任务。

    

    我们提出了一个信息理论框架，用于在强化学习任务中学习固定维度的嵌入。我们利用这样的想法：如果观察一个代理在一个任务上的表现减少了我们关于他在另一个任务上表现的不确定性，那么两个任务就相似。我们的信息理论准则捕捉了这种直觉，使用多样化的代理人群体来测量序列决策环境中任务之间的相似性。除了定性评估，我们还通过对两个应用场景进行量化比较，基于任务嵌入展示了我们技术的有效性：通过观察代理在一小组任务上的表现，来预测其在测试任务上的表现；从给定的任务选项中选择具有所需特征的任务。

    We present an information-theoretic framework to learn fixed-dimensional embeddings for tasks in reinforcement learning. We leverage the idea that two tasks are similar to each other if observing an agent's performance on one task reduces our uncertainty about its performance on the other. This intuition is captured by our information-theoretic criterion which uses a diverse population of agents to measure similarity between tasks in sequential decision-making settings. In addition to qualitative assessment, we empirically demonstrate the effectiveness of our techniques based on task embeddings by quantitative comparisons against strong baselines on two application scenarios: predicting an agent's performance on a test task by observing its performance on a small quiz of tasks, and selecting tasks with desired characteristics from a given set of options.
    
[^66]: 带权重空间上功能性输入映射的全局普适逼近

    Global universal approximation of functional input maps on weighted spaces. (arXiv:2306.03303v1 [stat.ML])

    [http://arxiv.org/abs/2306.03303](http://arxiv.org/abs/2306.03303)

    本文提出了功能性输入神经网络，可以在带权重空间上完成全局函数逼近。这一方法适用于连续函数的推广，还可用于路径空间函数的逼近，同时也可以逼近线性函数签名。

    

    我们引入了所谓的功能性输入神经网络，定义在可能是无限维带权重空间上，其值也在可能是无限维的输出空间中。为此，我们使用一个加性族作为隐藏层映射，以及一个非线性激活函数应用于每个隐藏层。依靠带权重空间上的Stone-Weierstrass定理，我们可以证明连续函数的推广的全局普适逼近结果，超越了常规紧集逼近。这特别适用于通过功能性输入神经网络逼近（非先见之明的）路径空间函数。作为带权Stone-Weierstrass定理的进一步应用，我们证明了线性函数签名的全局普适逼近结果。我们还在这个设置中引入了高斯过程回归的观点，并展示了签名内核的再生核希尔伯特空间是某些高斯过程的Cameron-Martin空间。

    We introduce so-called functional input neural networks defined on a possibly infinite dimensional weighted space with values also in a possibly infinite dimensional output space. To this end, we use an additive family as hidden layer maps and a non-linear activation function applied to each hidden layer. Relying on Stone-Weierstrass theorems on weighted spaces, we can prove a global universal approximation result for generalizations of continuous functions going beyond the usual approximation on compact sets. This then applies in particular to approximation of (non-anticipative) path space functionals via functional input neural networks. As a further application of the weighted Stone-Weierstrass theorem we prove a global universal approximation result for linear functions of the signature. We also introduce the viewpoint of Gaussian process regression in this setting and show that the reproducing kernel Hilbert space of the signature kernels are Cameron-Martin spaces of certain Gauss
    
[^67]: 动态特征选择中条件互信息的估计

    Estimating Conditional Mutual Information for Dynamic Feature Selection. (arXiv:2306.03301v1 [cs.LG])

    [http://arxiv.org/abs/2306.03301](http://arxiv.org/abs/2306.03301)

    本文提出了一种动态特征选择方法，该方法基于特征与响应变量的互信息进行优先级排序，并设计了估计互信息的判别式方法。同时，本文还引入了多项改进措施以应对更多场景。

    

    动态特征选择是一种有前途的范例，它通过顺序查询特征以在最小的预算内进行准确预测，以减少特征获取成本，并为预测过程提供透明度。尽管如此，这个问题很具有挑战性，因为它要求使用任意特征集进行预测，并学习策略以确定最有价值的选择。本文从信息理论的角度出发，根据特征与响应变量的互信息对特征进行优先级排序。其中的主要挑战是学习此选择策略，我们设计了一个直接新的建模方法，以判别而非生成模式估计互信息。建立在我们的学习方法之上，我们引入了几个进一步的改进：允许在样本之间进行可变的特征预算、支持不同特征之间的非均匀成本、结合先前的信息和探究现代架构以处理部分输入。

    Dynamic feature selection, where we sequentially query features to make accurate predictions with a minimal budget, is a promising paradigm to reduce feature acquisition costs and provide transparency into the prediction process. The problem is challenging, however, as it requires both making predictions with arbitrary feature sets and learning a policy to identify the most valuable selections. Here, we take an information-theoretic perspective and prioritize features based on their mutual information with the response variable. The main challenge is learning this selection policy, and we design a straightforward new modeling approach that estimates the mutual information in a discriminative rather than generative fashion. Building on our learning approach, we introduce several further improvements: allowing variable feature budgets across samples, enabling non-uniform costs between features, incorporating prior information, and exploring modern architectures to handle partial input in
    
[^68]: 切换自回归低秩张量模型

    Switching Autoregressive Low-rank Tensor Models. (arXiv:2306.03291v1 [cs.LG])

    [http://arxiv.org/abs/2306.03291](http://arxiv.org/abs/2306.03291)

    该文提出了一种切换自回归低秩张量（SALT）模型，它将自回归隐Markov模型（ARHMM）和切换线性动态系统（SLDS）的优点结合起来，通过低秩参数化提高了模型性能。

    

    时序分析中一个重要的问题是对具有时变动力学的系统进行建模。共同连续和离散潜态的概率模型为这样的数据提供了可解释、高效和实验性有用的描述。常用的模型包括自回归隐Markov模型（ARHMM）和切换线性动态系统（SLDS），它们各有优缺点。ARHMM允许精确推理和简单的参数估计，但在对长依赖关系建模时具有参数密集性，因此容易出现过拟合。相比之下，通过马尔可夫潜态动力学，SLDS可以以参数高效的方式捕捉长距离依赖性，但困难的参数估计任务和一个难以处理的似然函数却是其具有挑战性的地方。在本文中，我们提出了切换自回归低秩张量（SALT）模型，该模型保留了两种方法的优点，同时改善了其局限性。SALT将ARHMM的张量参数化为低秩形式。

    An important problem in time-series analysis is modeling systems with time-varying dynamics. Probabilistic models with joint continuous and discrete latent states offer interpretable, efficient, and experimentally useful descriptions of such data. Commonly used models include autoregressive hidden Markov models (ARHMMs) and switching linear dynamical systems (SLDSs), each with its own advantages and disadvantages. ARHMMs permit exact inference and easy parameter estimation, but are parameter intensive when modeling long dependencies, and hence are prone to overfitting. In contrast, SLDSs can capture long-range dependencies in a parameter efficient way through Markovian latent dynamics, but present an intractable likelihood and a challenging parameter estimation task. In this paper, we propose switching autoregressive low-rank tensor (SALT) models, which retain the advantages of both approaches while ameliorating the weaknesses. SALT parameterizes the tensor of an ARHMM with a low-rank 
    
[^69]: 从众包标签进行深度学习：耦合交叉熵最小化、可识别性和正则化

    Deep Learning From Crowdsourced Labels: Coupled Cross-entropy Minimization, Identifiability, and Regularization. (arXiv:2306.03288v1 [cs.LG])

    [http://arxiv.org/abs/2306.03288](http://arxiv.org/abs/2306.03288)

    本文提出了使用众包标签进行深度学习的方法，通过耦合交叉熵最小化和正则化使学习过程更加鲁棒，同时提出了性能保证。

    

    使用多个注释者提供的有噪声的众包标签，一个基于深度学习的端到端 (E2E) 系统旨在同时学习标签校正机制和神经分类器。耦合交叉熵最小化 (CCEM) 类型准则直观且在实践中表现良好。本文提出了对 CCEM 准则的性能保证，并提出了一种正则化方法，从而使学习过程更加鲁棒。

    Using noisy crowdsourced labels from multiple annotators, a deep learning-based end-to-end (E2E) system aims to learn the label correction mechanism and the neural classifier simultaneously. To this end, many E2E systems concatenate the neural classifier with multiple annotator-specific ``label confusion'' layers and co-train the two parts in a parameter-coupled manner. The formulated coupled cross-entropy minimization (CCEM)-type criteria are intuitive and work well in practice. Nonetheless, theoretical understanding of the CCEM criterion has been limited. The contribution of this work is twofold: First, performance guarantees of the CCEM criterion are presented. Our analysis reveals for the first time that the CCEM can indeed correctly identify the annotators' confusion characteristics and the desired ``ground-truth'' neural classifier under realistic conditions, e.g., when only incomplete annotator labeling and finite samples are available. Second, based on the insights learned from
    
[^70]: 离线强化学习中的生存本能

    Survival Instinct in Offline Reinforcement Learning. (arXiv:2306.03286v1 [cs.LG])

    [http://arxiv.org/abs/2306.03286](http://arxiv.org/abs/2306.03286)

    离线强化学习算法即使使用错误的奖励标签，也能产生良好的表现和安全的策略，这种鲁棒性属性是由离线强化学习算法的悲观主义和常见数据收集实践中的偏见之间相互作用的结果，赋予了代理生存本能。

    

    我们提出了一个关于离线强化学习算法行为的新观察：在许多基准数据集上，离线强化学习即使使用“错误”的奖励标签（例如在所有地方都为零或是真实奖励的负数），也能产生良好的表现和安全的策略。这种现象不能仅通过离线强化学习的回报最大化目标来解释。此外，它赋予了离线强化学习一定的鲁棒性，这在其在线强化学习对应物中是不典型的，因为后者对奖励设计敏感。我们证明了此惊人的鲁棒性属性是离线强化学习算法中悲观主义概念和常见数据收集实践中某种偏见之间相互作用的结果。悲观主义赋予了代理生存本能，即长期内留在数据支持中的激励，而有限且有偏见的数据覆盖进一步限制了生存行为集合。

    We present a novel observation about the behavior of offline reinforcement learning (RL) algorithms: on many benchmark datasets, offline RL can produce well-performing and safe policies even when trained with "wrong" reward labels, such as those that are zero everywhere or are negatives of the true rewards. This phenomenon cannot be easily explained by offline RL's return maximization objective. Moreover, it gives offline RL a degree of robustness that is uncharacteristic of its online RL counterparts, which are known to be sensitive to reward design. We demonstrate that this surprising robustness property is attributable to an interplay between the notion of pessimism in offline RL algorithms and a certain bias implicit in common data collection practices. As we prove in this work, pessimism endows the agent with a "survival instinct", i.e., an incentive to stay within the data support in the long term, while the limited and biased data coverage further constrains the set of survival 
    
[^71]: 基于扩散生成模型的MRI压缩感知采样模式优化

    Optimizing Sampling Patterns for Compressed Sensing MRI with Diffusion Generative Models. (arXiv:2306.03284v1 [cs.LG])

    [http://arxiv.org/abs/2306.03284](http://arxiv.org/abs/2306.03284)

    本论文提出了一种基于扩散生成模型的MRI压缩感知采样模式优化方法，可以只用五张训练图像来学习有效的采样模式，并在不同的解剖结构、加速因子和模式类型下获得具有竞争性的重建效果。

    

    基于扩散生成模型已被用作磁共振成像(MRI)重建的强大先验。我们提出了一种学习方法，利用预先训练的扩散生成模型来优化压缩感知多线圈MRI的子采样模式。在训练过程中，我们使用基于扩散模型和MRI测量过程的后验平均估计的单步重建。在不同解剖结构、加速因子和模式类型的实验中，我们学习到的采样运算符比基线模式具有竞争性，而在2D模式的情况下，重建效果得到了改善。我们的方法只需要五个训练图像就可以学习到有效的采样模式。

    Diffusion-based generative models have been used as powerful priors for magnetic resonance imaging (MRI) reconstruction. We present a learning method to optimize sub-sampling patterns for compressed sensing multi-coil MRI that leverages pre-trained diffusion generative models. Crucially, during training we use a single-step reconstruction based on the posterior mean estimate given by the diffusion model and the MRI measurement process. Experiments across varying anatomies, acceleration factors, and pattern types show that sampling operators learned with our method lead to competitive, and in the case of 2D patterns, improved reconstructions compared to baseline patterns. Our method requires as few as five training images to learn effective sampling patterns.
    
[^72]: 具有属性神经融合的未计入张量补全

    Under-Counted Tensor Completion with Neural Incorporation of Attributes. (arXiv:2306.03273v1 [cs.LG])

    [http://arxiv.org/abs/2306.03273](http://arxiv.org/abs/2306.03273)

    本文提出了一种联合了低秩张量补全和神经网络学习算法的未计数张量模型，可以从部分观察到的条目中恢复完全计数条目和每个条目的未计数概率。

    

    在许多领域收集的数据都存在系统性的未计数效应，例如流行病学和生态学。未计数张量补全（UC-TC）对于许多数据分析任务具有很好的动机，例如从相邻区域的未计数病例数字推断未观察到的地点的传染病病例数字。本文提出了一种低秩泊松张量模型，其中包含表达丰富的未知非线性侧面信息提取器，用于处理未计数的多方面数据。设计了一种联合低秩张量补全和神经网络学习算法来回复模型。此外，UC-TC公式得到了理论分析的支持，可以证明张量的完全计数条目和每个条目的未计数概率可以从部分观察到的条目中被恢复。

    Systematic under-counting effects are observed in data collected across many disciplines, e.g., epidemiology and ecology. Under-counted tensor completion (UC-TC) is well-motivated for many data analytics tasks, e.g., inferring the case numbers of infectious diseases at unobserved locations from under-counted case numbers in neighboring regions. However, existing methods for similar problems often lack supports in theory, making it hard to understand the underlying principles and conditions beyond empirical successes. In this work, a low-rank Poisson tensor model with an expressive unknown nonlinear side information extractor is proposed for under-counted multi-aspect data. A joint low-rank tensor completion and neural network learning algorithm is designed to recover the model. Moreover, the UC-TC formulation is supported by theoretical analysis showing that the fully counted entries of the tensor and each entry's under-counting probability can be provably recovered from partial observ
    
[^73]: 大脑肿瘤复发与放射性坏死鉴别以及患者生存率预测

    Brain Tumor Recurrence vs. Radiation Necrosis Classification and Patient Survivability Prediction. (arXiv:2306.03270v1 [eess.IV])

    [http://arxiv.org/abs/2306.03270](http://arxiv.org/abs/2306.03270)

    该研究提出了具有统计学严谨性的计算模型，旨在区分磁共振成像（MRI）中rBT和RN，并以此来预测GBM患者的生存率。这对促进患者早期接受治疗和获得更好的治疗结果至关重要。

    

    成人最凶险的脑肿瘤是胶质母细胞瘤（GBM），即使经过手术和放疗治疗后且积极治疗，其短期生存率也很低。GBM患者接受放疗后，磁共振成像（MRI）的变化表明出现放射性坏死（RN）或脑肿瘤复发（rBT）。在早期筛选rBT和RN对于促进患者更快地接受治疗和获得更好的治疗结果至关重要。区分rBT和RN是有挑战性的，因为两者在MRI上可能呈现出类似的放射性和临床特征。此外，使用MRI进行机器学习的rBT与RN分类可能会因缺乏患者数据而导致类别不平衡。虚拟模型产生的合成数据可以通过生成模型来解决类别不平衡，但会导致合成或增广数据中的基础数据表示不同。本研究提出了具有统计学严谨性的计算模型。

    GBM (Glioblastoma multiforme) is the most aggressive type of brain tumor in adults that has a short survival rate even after aggressive treatment with surgery and radiation therapy. The changes on magnetic resonance imaging (MRI) for patients with GBM after radiotherapy are indicative of either radiation-induced necrosis (RN) or recurrent brain tumor (rBT). Screening for rBT and RN at an early stage is crucial for facilitating faster treatment and better outcomes for the patients. Differentiating rBT from RN is challenging as both may present with similar radiological and clinical characteristics on MRI. Moreover, learning-based rBT versus RN classification using MRI may suffer from class imbalance due to lack of patient data. While synthetic data generation using generative models has shown promise to address class imbalance, the underlying data representation may be different in synthetic or augmented data. This study proposes computational modeling with statistically rigorous repeat
    
[^74]: 通过重新思考民间威斯费勒-莱曼算法，实现$O(n^2)$空间内任意表达能力的GNNs

    Towards Arbitrarily Expressive GNNs in $O(n^2)$ Space by Rethinking Folklore Weisfeiler-Lehman. (arXiv:2306.03266v1 [cs.LG])

    [http://arxiv.org/abs/2306.03266](http://arxiv.org/abs/2306.03266)

    本文提出了$(k, t)$-FWL和$k$-FWL+两种方法，理论上证明了它们可以在$O(n^2)$的空间复杂度下，解决图同构问题。

    

    近年来，消息传递神经网络（MPNNs）已成为图神经网络（GNNs）中最受欢迎的框架。然而，其表达能力受到一维威斯费勒-莱曼（1-WL）测试的限制。一些研究受到$k$-WL/FWL（民间WL）的启发并设计其相应的神经版本。尽管具有很高的表达能力，但这一研究方向存在严重局限性。为解决这些问题，作者提出了$(k, t)$-FWL和$k$-FWL+，并在理论上证明了它们的有效性。

    Message passing neural networks (MPNNs) have emerged as the most popular framework of graph neural networks (GNNs) in recent years. However, their expressive power is limited by the 1-dimensional Weisfeiler-Lehman (1-WL) test. Some works are inspired by $k$-WL/FWL (Folklore WL) and design the corresponding neural versions. Despite the high expressive power, there are serious limitations in this line of research. In particular, (1) $k$-WL/FWL requires at least $O(n^k)$ space complexity, which is impractical for large graphs even when $k=3$; (2) The design space of $k$-WL/FWL is rigid, with the only adjustable hyper-parameter being $k$. To tackle the first limitation, we propose an extension, $(k, t)$-FWL. We theoretically prove that even if we fix the space complexity to $O(n^2)$ in $(k, t)$-FWL, we can construct an expressiveness hierarchy up to solving the graph isomorphism problem. To tackle the second problem, we propose $k$-FWL+, which considers any equivariant set as neighbors ins
    
[^75]: 机器学习审稿过程是否随着该领域的发展变得更加随意性？NeurIPS 2021一致性实验。

    Has the Machine Learning Review Process Become More Arbitrary as the Field Has Grown? The NeurIPS 2021 Consistency Experiment. (arXiv:2306.03262v1 [cs.LG])

    [http://arxiv.org/abs/2306.03262](http://arxiv.org/abs/2306.03262)

    本研究展示了 NeurIPS 2021 一致性实验结果，该实验发现机器学习审稿过程中的随意性较高，两个委员会对于 23% 的论文存在不同的接受/拒绝推荐，如果重新随机运行审稿过程，接受论文清单中约有一半的论文将会改变。因此，本文强调客观衡量研究质量的困难，建议作者不应因拒绝的作品而过分沮丧。

    

    我们提出了NeurIPS 2021一致性实验，这是2014年NeurIPS实验的更大规模变体，在该实验中，10%的会议投稿由两个独立委员会评审，以量化审稿过程中的随机性。我们发现，两个委员会在23%的论文的接受/拒绝推荐上存在分歧，并且与2014年的结果一致，如果重新随机运行审稿过程，接受论文清单中约有一半的论文将会改变。我们的分析表明，使会议更具选择性会增加这一过程的随意性。结合先前的研究，我们的结果凸显了客观衡量研究质量的困难，并表明作者不应因拒绝的作品而过分沮丧。

    We present the NeurIPS 2021 consistency experiment, a larger-scale variant of the 2014 NeurIPS experiment in which 10% of conference submissions were reviewed by two independent committees to quantify the randomness in the review process. We observe that the two committees disagree on their accept/reject recommendations for 23% of the papers and that, consistent with the results from 2014, approximately half of the list of accepted papers would change if the review process were randomly rerun. Our analysis suggests that making the conference more selective would increase the arbitrariness of the process. Taken together with previous research, our results highlight the inherent difficulty of objectively measuring the quality of research, and suggest that authors should not be excessively discouraged by rejected work.
    
[^76]: 使用遗传算法生成私有合成数据

    Generating Private Synthetic Data with Genetic Algorithms. (arXiv:2306.03257v1 [cs.OH])

    [http://arxiv.org/abs/2306.03257](http://arxiv.org/abs/2306.03257)

    本文提出了一种使用遗传算法生成差分隐私合成数据的方法，避免了一阶优化方法的限制，并展示了生成数据的高准确度和隐私性。

    

    我们研究了高效生成近似于基础敏感数据集统计属性的差分私有合成数据的问题。近年来，已有一系列工作利用一阶优化技术来解决这一问题。然而，这样的技术仅限于仅可微目标的优化，严重限制了可以进行的分析类型。因此，我们提出了基于零阶优化启发式的私有遗传算法 Private-GSD。我们的方法使用基于族群的搜索，通过遗传算子启发式搜索空间，从而学习合成数据生成器。我们证明 Private-GSD 生成的合成数据密切逼近基础敏感数据集的统计属性，同时具有强差分隐私保证。

    We study the problem of efficiently generating differentially private synthetic data that approximate the statistical properties of an underlying sensitive dataset. In recent years, there has been a growing line of work that approaches this problem using first-order optimization techniques. However, such techniques are restricted to optimizing differentiable objectives only, severely limiting the types of analyses that can be conducted. For example, first-order mechanisms have been primarily successful in approximating statistical queries only in the form of marginals for discrete data domains. In some cases, one can circumvent such issues by relaxing the task's objective to maintain differentiability. However, even when possible, these approaches impose a fundamental limitation in which modifications to the minimization problem become additional sources of error. Therefore, we propose Private-GSD, a private genetic algorithm based on zeroth-order optimization heuristics that do not re
    
[^77]: 解释与调整图形条件转移

    Explaining and Adapting Graph Conditional Shift. (arXiv:2306.03256v1 [cs.LG])

    [http://arxiv.org/abs/2306.03256](http://arxiv.org/abs/2306.03256)

    本研究通过量化输入特征和输出标签之间的条件偏移量，对图神经网络易受分布偏移影响的问题进行理论分析。研究发现，图形异质性和模型架构都会导致条件偏移，影响性能。作者提出了一种方法，通过条件偏移的估计和最小化来应对这一问题，该方法在节点分类和图分类任务上表现更优。

    

    图神经网络在图结构数据上表现出卓越的性能。然而，最近的实证研究表明，GNN非常容易受到分布偏移的影响。目前关于为什么基于图形的模型似乎更容易受到这些偏移影响的问题还存在显著的歧义。在这项工作中，我们通过量化输入特征和输出标签之间的条件偏移量，对它进行了彻底的理论分析。我们的研究结果表明，图形异质性和模型架构都加剧了条件偏移，导致性能下降。为了应对这一问题，我们提出了一种方法，涉及对图形上的无监督域适应性进行条件偏移的估计和最小化。在我们的控制性综合实验中，我们的算法表现出对分布偏移的鲁棒性，相对第二优算法实现了高达10%的ROC AUC绝对改善。此外，对节点分类和图分类任务的全面实验表明，我们的方法始终优于最先进的域适应方法。

    Graph Neural Networks (GNNs) have shown remarkable performance on graph-structured data. However, recent empirical studies suggest that GNNs are very susceptible to distribution shift. There is still significant ambiguity about why graph-based models seem more vulnerable to these shifts. In this work we provide a thorough theoretical analysis on it by quantifying the magnitude of conditional shift between the input features and the output label. Our findings show that both graph heterophily and model architecture exacerbate conditional shifts, leading to performance degradation. To address this, we propose an approach that involves estimating and minimizing the conditional shift for unsupervised domain adaptation on graphs. In our controlled synthetic experiments, our algorithm demonstrates robustness towards distribution shift, resulting in up to 10% absolute ROC AUC improvement versus the second-best algorithm. Furthermore, comprehensive experiments on both node classification and gr
    
[^78]: 概率展开：用于潜在高斯模型的可扩展、无反演最大似然估计

    Probabilistic Unrolling: Scalable, Inverse-Free Maximum Likelihood Estimation for Latent Gaussian Models. (arXiv:2306.03249v1 [cs.LG])

    [http://arxiv.org/abs/2306.03249](http://arxiv.org/abs/2306.03249)

    概率展开是用于可扩展、无反演最大似然估计的方法，可以在学习潜在高斯模型时比传统的期望最大化算法快一个数量级，而模型性能损失很小。

    

    潜在高斯模型在统计学和机器学习中有着悠久的历史，应用范围从因子分析到压缩感知再到时间序列分析。最大化这些模型的似然函数的传统方法是期望最大化(EM)算法。对于具有高维潜在变量和大型数据集的问题，由于需要求解与数据点数量一样多的大型协方差矩阵的逆矩阵，因此EM的可扩展性很差。我们引入了概率展开，这是一种将蒙特卡罗抽样与迭代线性求解器相结合的方法，可以绕过矩阵求逆。我们的理论分析揭示了展开和反向传播通过求解器的迭代可以加速最大似然估计的梯度估计。在模拟和真实数据的实验中，我们展示了概率展开学习潜在高斯模型的速度比梯度EM快一个数量级，而模型性能的损失很小。

    Latent Gaussian models have a rich history in statistics and machine learning, with applications ranging from factor analysis to compressed sensing to time series analysis. The classical method for maximizing the likelihood of these models is the expectation-maximization (EM) algorithm. For problems with high-dimensional latent variables and large datasets, EM scales poorly because it needs to invert as many large covariance matrices as the number of data points. We introduce probabilistic unrolling, a method that combines Monte Carlo sampling with iterative linear solvers to circumvent matrix inversion. Our theoretical analyses reveal that unrolling and backpropagation through the iterations of the solver can accelerate gradient estimation for maximum likelihood estimation. In experiments on simulated and real data, we demonstrate that probabilistic unrolling learns latent Gaussian models up to an order of magnitude faster than gradient EM, with minimal losses in model performance.
    
[^79]: 理解早期权重平均对训练大语言模型的有效性

    Understanding the Effectiveness of Early Weight Averaging for Training Large Language Models. (arXiv:2306.03241v1 [cs.LG])

    [http://arxiv.org/abs/2306.03241](http://arxiv.org/abs/2306.03241)

    本文研究了使用早期权重平均化方法来提高大型语言模型质量的有效性，证明该方法可以加速收敛且测试和零样本泛化效果显著，同时有效缓解了训练中的损失波动问题。

    

    训练大型语言模型代价高昂，最近的研究表明训练至收敛并不高效。在本文中，我们研究了一种简单的想法，即在训练过程中沿着轨迹进行检查点平均化，以在模型收敛之前提高其质量。这种方法在训练或推理期间不会产生额外的成本。具体而言，我们分析了具有10亿到120亿参数的Pythia LLM的训练轨迹，并证明特别是在训练的早期和中期阶段，这种想法可以加速收敛并提高测试和零样本泛化效果。损失波动是LLM训练中众所周知的问题；在我们的分析中，我们遇到了两种基础轨迹的这种情况，并且我们的平均化可以缓解这两种情况。例如，对于一个拥有69亿参数的LLM，我们的早期权重平均化配方可以节省高达4200小时的GPU时间，这对云计算成本来说是显著的节约。

    Training LLMs is expensive, and recent evidence indicates training all the way to convergence is inefficient. In this paper, we investigate the ability of a simple idea, checkpoint averaging along the trajectory of a training run to improve the quality of models before they have converged. This approach incurs no extra cost during training or inference. Specifically, we analyze the training trajectories of Pythia LLMs with 1 to 12 billion parameters and demonstrate that, particularly during the early to mid stages of training, this idea accelerates convergence and improves both test and zero-shot generalization. Loss spikes are a well recognized problem in LLM training; in our analysis we encountered two instances of this in the underlying trajectories, and both instances were mitigated by our averaging.  For a 6.9B parameter LLM, for example, our early weight averaging recipe can save upto 4200 hours of GPU time, which corresponds to significant savings in cloud compute costs.
    
[^80]: 利用压缩和重要性抽样提高加速的联邦学习

    Improving Accelerated Federated Learning with Compression and Importance Sampling. (arXiv:2306.03240v1 [cs.LG])

    [http://arxiv.org/abs/2306.03240](http://arxiv.org/abs/2306.03240)

    本文提出了利用压缩和重要性抽样提高加速的联邦学习方法，将本地训练、压缩和部分参与结合应用，以获得最先进的收敛结果。

    

    联邦学习是一种利用分布在大量客户端上的异构数据的协作训练框架。在聚合步骤中，要求并处理所有客户端是不切实际的，必须支持部分参与。在这种情况下，服务器与客户端之间的通信是一个主要的瓶颈。为了减少通信负担，有两种主要方法：压缩和本地步骤。最近的研究通过使用本地步骤技术引入了新的ProxSkip方法，实现了加速率。后续的研究成功地将本地步骤加速与部分参与和梯度压缩相结合。本文最终提出了一个完整的联邦学习方法，包括所有必要的成分：本地训练、压缩和部分参与。我们获得了最先进的收敛结果。

    Federated Learning is a collaborative training framework that leverages heterogeneous data distributed across a vast number of clients. Since it is practically infeasible to request and process all clients during the aggregation step, partial participation must be supported. In this setting, the communication between the server and clients poses a major bottleneck. To reduce communication loads, there are two main approaches: compression and local steps. Recent work by Mishchenko et al. [2022] introduced the new ProxSkip method, which achieves an accelerated rate using the local steps technique. Follow-up works successfully combined local steps acceleration with partial participation [Grudzie\'n et al., 2023, Condat et al. 2023] and gradient compression [Condat et al. [2022]. In this paper, we finally present a complete method for Federated Learning that incorporates all necessary ingredients: Local Training, Compression, and Partial Participation. We obtain state-of-the-art convergenc
    
[^81]: 模块化模型架构中的机器学习信息流控制

    Information Flow Control in Machine Learning through Modular Model Architecture. (arXiv:2306.03235v1 [cs.LG])

    [http://arxiv.org/abs/2306.03235](http://arxiv.org/abs/2306.03235)

    本文提出了机器学习信息流控制的概念，并通过MoE架构实现了训练数据对模型输出的控制，从而提高了模型准确性。通过在推理时仅基于访问策略启用子集的专家，实现了对安全访问控制的支持。

    

    在当今的机器学习模型中，训练数据的任何部分都可以影响其输出。当访问控制只允许个人用户访问数据子集时，从训练数据到模型输出的信息流控制不足成为训练敏感数据模型的主要障碍。为了实现访问控制数据的安全机器学习，我们提出了机器学习信息流控制的概念，并基于混合专家（MoE）架构开发了一个安全Transformer型语言模型。通过限制来自每个安全领域的训练数据对单个专家模块的影响，并仅基于访问控制策略在推理时启用专家的子集，安全MoE架构控制了信息流。使用大型文本数据语料库进行的评估表明，所提出的MoE架构具有最小的性能开销（1.9%），并且可以显著提高模型准确性（最高可达37%），从而实现训练准确和安全分类器。

    In today's machine learning (ML) models, any part of the training data can affect its output. This lack of control for information flow from training data to model output is a major obstacle in training models on sensitive data when access control only allows individual users to access a subset of data. To enable secure machine learning for access controlled data, we propose the notion of information flow control for machine learning, and develop a secure Transformer-based language model based on the Mixture-of-Experts (MoE) architecture. The secure MoE architecture controls information flow by limiting the influence of training data from each security domain to a single expert module, and only enabling a subset of experts at inference time based on an access control policy. The evaluation using a large corpus of text data shows that the proposed MoE architecture has minimal (1.9%) performance overhead and can significantly improve model accuracy (up to 37%) by enabling training on acc
    
[^82]: 使用基于生物进化树的神经网络从图像中发现新的生物特征

    Discovering Novel Biological Traits From Images Using Phylogeny-Guided Neural Networks. (arXiv:2306.03228v1 [cs.LG])

    [http://arxiv.org/abs/2306.03228](http://arxiv.org/abs/2306.03228)

    本研究发现一种基于生物进化树的神经网络方法，可以从没有特征标签的生物图像中发现进化特征，能在多个下游任务中产生生物学上有意义的结果。

    

    发现沿生命之树（也称为进化树）跨物种遗传的进化特征对生物学家理解生物多样性和进化模式具有极大的兴趣。然而，特征的测量通常是一个主观和耗时的过程，使得特征发现成为一个高度稀缺标签的问题。本文提出了一种新的方法，可以直接从没有特征标签的图像中发现进化特征。我们提出的方法 Phylo-NN，将一个生物体的图像编码成一系列量化的特征向量序列，其中序列的不同段在进化树的不同祖先层次捕获进化信号。我们使用鱼类物种作为目标示例，在许多下游任务中展示了我们的方法在产生生物学上有意义的结果中的有效性，包括物种图像生成和物种到物种的图像翻译。

    Discovering evolutionary traits that are heritable across species on the tree of life (also referred to as a phylogenetic tree) is of great interest to biologists to understand how organisms diversify and evolve. However, the measurement of traits is often a subjective and labor-intensive process, making trait discovery a highly label-scarce problem. We present a novel approach for discovering evolutionary traits directly from images without relying on trait labels. Our proposed approach, Phylo-NN, encodes the image of an organism into a sequence of quantized feature vectors -- or codes -- where different segments of the sequence capture evolutionary signals at varying ancestry levels in the phylogeny. We demonstrate the effectiveness of our approach in producing biologically meaningful results in a number of downstream tasks including species image generation and species-to-species image translation, using fish species as a target example.
    
[^83]: 结构重加权改善图领域自适应

    Structural Re-weighting Improves Graph Domain Adaptation. (arXiv:2306.03221v1 [cs.LG])

    [http://arxiv.org/abs/2306.03221](http://arxiv.org/abs/2306.03221)

    本文提出了一种名为结构重加权（StruRW）的新方法，用于解决当前图领域自适应（GDA）方法无法处理的条件结构偏移（CSS）问题，并在多个领域得到验证。

    

    在许多实际应用中，用于训练和测试的图结构化数据具有不同的分布，例如在高能物理学中，用于训练的模拟数据可能与实验不匹配。图领域自适应（GDA）是一种用于解决这些差异的方法。然而，当前的GDA主要通过对单个图神经网络编码器输出的节点表示的分布进行对齐来工作，这可能会产生次优解。这项工作研究了由图结构或节点属性引起的不同分布偏移的不同影响，并确定了一种名为条件结构偏移（CSS）的新类型偏移，证明了当前的GDA方法无法处理。提出了一种称为结构重加权（StruRW）的新方法，并在合成图、四个基准数据集和新的高能物理学应用中进行了测试。StruRW已经显示出显著的性能。

    In many real-world applications, graph-structured data used for training and testing have differences in distribution, such as in high energy physics (HEP) where simulation data used for training may not match real experiments. Graph domain adaptation (GDA) is a method used to address these differences. However, current GDA primarily works by aligning the distributions of node representations output by a single graph neural network encoder shared across the training and testing domains, which may often yield sub-optimal solutions. This work examines different impacts of distribution shifts caused by either graph structure or node attributes and identifies a new type of shift, named conditional structure shift (CSS), which current GDA approaches are provably sub-optimal to deal with. A novel approach, called structural reweighting (StruRW), is proposed to address this issue and is tested on synthetic graphs, four benchmark datasets, and a new application in HEP. StruRW has shown signifi
    
[^84]: 用于无目标代谢组学数据自动对齐的最优输运

    Optimal transport for automatic alignment of untargeted metabolomic data. (arXiv:2306.03218v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.03218](http://arxiv.org/abs/2306.03218)

    本文提出了一种名为GromovMatcher的算法，通过使用最优输运自动合并LC-MS数据集，可提高数据对齐的准确性和鲁棒性，有效解决代谢组学数据合并的挑战。

    

    液相色谱-质谱（LC-MS）通过测量生物标本中的大量代谢物推动药物研发，疾病诊断和风险预测的进展。然而，LC-MS的低通量对于生物标记物发现，注释和实验比较构成了主要挑战，需要合并多个数据集。当前的数据池化方法由于对数据变化和超参数依赖性的脆弱性而遇到实际限制。本文介绍了GromovMatcher，一种灵活且用户友好的算法，使用最优输运自动结合LC-MS数据集。通过利用特征强度相关结构，GromovMatcher提供了比现有方法更高的对齐准确性和鲁棒性。该算法可扩展到需要最小超参数调整的数千个特征。将我们的方法应用于肝癌和胰腺癌的实验患者研究

    Untargeted metabolomic profiling through liquid chromatography-mass spectrometry (LC-MS) measures a vast array of metabolites within biospecimens, advancing drug development, disease diagnosis, and risk prediction. However, the low throughput of LC-MS poses a major challenge for biomarker discovery, annotation, and experimental comparison, necessitating the merging of multiple datasets. Current data pooling methods encounter practical limitations due to their vulnerability to data variations and hyperparameter dependence. Here we introduce GromovMatcher, a flexible and user-friendly algorithm that automatically combines LC-MS datasets using optimal transport. By capitalizing on feature intensity correlation structures, GromovMatcher delivers superior alignment accuracy and robustness compared to existing approaches. This algorithm scales to thousands of features requiring minimal hyperparameter tuning. Applying our method to experimental patient studies of liver and pancreatic cancer, 
    
[^85]: 带有关联记忆的端到端可微聚类

    End-to-end Differentiable Clustering with Associative Memories. (arXiv:2306.03209v1 [cs.LG])

    [http://arxiv.org/abs/2306.03209](http://arxiv.org/abs/2306.03209)

    本文提出了基于关联记忆的端到端可微聚类算法ClAM，结合模式完成技术开发自监督聚类损失函数，相对传统算法和最新的连续聚类松弛方案在轮廓系数方面提高了60%。

    

    聚类是一种广泛使用的无监督学习技术，涉及强烈的离散优化问题。关联记忆模型或AM是可微的神经网络，定义了递归动态系统，并已与各种深度学习体系结构集成。我们揭示了AM动态和聚类中固有的离散分配之间的新联系，以提出一种新的无约束连续传播离散聚类问题，使得AM可以进行端到端可微的聚类，称为ClAM。利用AM的模式完成能力，我们进一步开发了一种新的自监督聚类损失。我们对各种数据集的评估表明，ClAM从自监督中受益，并且在传统的Lloyd's k-means算法和最近的连续聚类松弛方案上均显着改善了（在轮廓系数方面提高了60％）。

    Clustering is a widely used unsupervised learning technique involving an intensive discrete optimization problem. Associative Memory models or AMs are differentiable neural networks defining a recursive dynamical system, which have been integrated with various deep learning architectures. We uncover a novel connection between the AM dynamics and the inherent discrete assignment necessary in clustering to propose a novel unconstrained continuous relaxation of the discrete clustering problem, enabling end-to-end differentiable clustering with AM, dubbed ClAM. Leveraging the pattern completion ability of AMs, we further develop a novel self-supervised clustering loss. Our evaluations on varied datasets demonstrate that ClAM benefits from the self-supervision, and significantly improves upon both the traditional Lloyd's k-means algorithm, and more recent continuous clustering relaxations (by upto 60% in terms of the Silhouette Coefficient).
    
[^86]: 非线性分布鲁棒优化

    Nonlinear Distributionally Robust Optimization. (arXiv:2306.03202v1 [stat.ML])

    [http://arxiv.org/abs/2306.03202](http://arxiv.org/abs/2306.03202)

    本文提出一种新的非线性分布鲁棒优化算法，用于处理一类分布鲁棒优化问题，通过 Gateaux Derivative 处理一般风险度量。经过实验验证，该方法成功处理分布的非线性目标函数。

    

    本文关注一类分布鲁棒优化（DRO）问题，其中目标函数在分布上可能是非线性的，这与现有的文献有所不同。为解决在概率空间中优化非线性函数面临的理论和计算挑战，我们提出了一种Derivative和相应的平滑度概念，基于Gateaux Derivative来处理一般风险度量。我们通过Var、entropic risk和有限支持集上的三个运行风险度量示例来解释这些概念。然后，我们为概率空间中一般非线性优化问题提出了一种基于G-derivative的Frank-Wolfe（FW）算法，并以完全独立于范数的方式推导出其收敛性在提出的平滑度概念下。我们利用FW算法的设置来设计一种计算非线性DRO问题鞍点的方法。我们通过数值实验展示了我们方法处理分布的非线性目标函数的成功。

    This article focuses on a class of distributionally robust optimization (DRO) problems where, unlike the growing body of the literature, the objective function is potentially non-linear in the distribution. Existing methods to optimize nonlinear functions in probability space use the Frechet derivatives, which present both theoretical and computational challenges. Motivated by this, we propose an alternative notion for the derivative and corresponding smoothness based on Gateaux (G)-derivative for generic risk measures. These concepts are explained via three running risk measure examples of variance, entropic risk, and risk on finite support sets. We then propose a G-derivative based Frank-Wolfe~(FW) algorithm for generic non-linear optimization problems in probability spaces and establish its convergence under the proposed notion of smoothness in a completely norm-independent manner. We use the set-up of the FW algorithm to devise a methodology to compute a saddle point of the non-lin
    
[^87]: 夜空中的Lumos：用于探索夜间光模式的AI可视化工具

    Lumos in the Night Sky: AI-enabled Visual Tool for Exploring Night-Time Light Patterns. (arXiv:2306.03195v1 [cs.HC])

    [http://arxiv.org/abs/2306.03195](http://arxiv.org/abs/2306.03195)

    夜Pulse是一个交互式工具，可用于夜间光（NTL）数据的可视化和分析。它可以通过图像分割、聚类和变化模式检测来识别城市发展和扩展模式，并回答有关人口因素、城市边界和异常差异的问题。

    

    我们介绍了NightPulse，这是一种交互式工具，用于夜间光（NTL）数据可视化和分析，使研究人员和利益相关者能够使用用户友好的平台探索和分析NTL数据。由高效的系统架构驱动，NightPulse支持图像分割、聚类和变化模式检测，以识别城市发展和扩展模式。它捕捉NTL的时间趋势和城市的语义，回答关于人口因素、城市边界和异常差异的问题。

    We introduce NightPulse, an interactive tool for Night-time light (NTL) data visualization and analytics, which enables researchers and stakeholders to explore and analyze NTL data with a user-friendly platform. Powered by efficient system architecture, NightPulse supports image segmentation, clustering, and change pattern detection to identify urban development and sprawl patterns. It captures temporal trends of NTL and semantics of cities, answering questions about demographic factors, city boundaries, and unusual differences.
    
[^88]: 面向项目推荐的个性化联邦域自适应

    Personalized Federated Domain Adaptation for Item-to-Item Recommendation. (arXiv:2306.03191v1 [cs.IR])

    [http://arxiv.org/abs/2306.03191](http://arxiv.org/abs/2306.03191)

    本论文提出了一种个性化联邦域适应方法，可在缺乏大量关系数据的情况下为项目推荐建立有效的热启动模型。

    

    基于项目语义相似性，项目对项目（I2I）推荐在大部分推荐系统中都是一个重要的功能，可以为特定的项目生成替换或补充建议。基于推荐系统中的某些项目子集可能由同一组客户共同交互，图形模型（如图形神经网络）提供了一种自然的框架来结合、摄取和提取来自这些目录化项目的高阶关系交互以及它们的元数据特性所提供的有价值的见解，许多最近的研究已经证明了这一点。然而，有效地为I2I学习GNN需要摄取大量的关系数据，这可能并不总是可用的，尤其是在新兴市场领域。为了缓解这种数据瓶颈，我们假设可以从现有成熟市场领域（具有私有数据）中学习到的推荐模式可以被适应，从而建立起有效的热启动模型。

    Item-to-Item (I2I) recommendation is an important function in most recommendation systems, which generates replacement or complement suggestions for a particular item based on its semantic similarities to other cataloged items. Given that subsets of items in a recommendation system might be co-interacted with by the same set of customers, graph-based models, such as graph neural networks (GNNs), provide a natural framework to combine, ingest and extract valuable insights from such high-order relational interactions between cataloged items, as well as their metadata features, as has been shown in many recent studies. However, learning GNNs effectively for I2I requires ingesting a large amount of relational data, which might not always be available, especially in new, emerging market segments. To mitigate this data bottleneck, we postulate that recommendation patterns learned from existing mature market segments (with private data) could be adapted to build effective warm-start models fo
    
[^89]: 翻转硬币来估计强化学习中探索的伪计数

    Flipping Coins to Estimate Pseudocounts for Exploration in Reinforcement Learning. (arXiv:2306.03186v1 [cs.LG])

    [http://arxiv.org/abs/2306.03186](http://arxiv.org/abs/2306.03186)

    研究提出了利用硬币翻转来推导状态的访问计数，并将其作为强化学习探索奖励，相比以往的方法在多个具有挑战性的任务上表现更好。

    

    我们提出了一种在高维状态空间中基于计数的探索新方法。与依赖密度模型的以往研究不同，我们展示了计数可以通过从Rademacher分布（或硬币翻转）中平均样本得到。利用这一见解，我们设置了一个简单的监督学习目标，当优化时，会产生一个状态的访问计数。我们展示了我们的方法比以前的工作更能有效地推导出真正的访问计数。当作为模型无关强化学习算法的探索奖励时，我们的方法在包括Atari游戏Montezuma's Revenge在内的9个具有挑战性的探索任务中优于现有方法。

    We propose a new method for count-based exploration in high-dimensional state spaces. Unlike previous work which relies on density models, we show that counts can be derived by averaging samples from the Rademacher distribution (or coin flips). This insight is used to set up a simple supervised learning objective which, when optimized, yields a state's visitation count. We show that our method is significantly more effective at deducing ground-truth visitation counts than previous work; when used as an exploration bonus for a model-free reinforcement learning algorithm, it outperforms existing approaches on most of 9 challenging exploration tasks, including the Atari game Montezuma's Revenge.
    
[^90]: 基于格点对注意机制进行先验加入，以提高抽象几何推理的样本效率

    Infusing Lattice Symmetry Priors in Attention Mechanisms for Sample-Efficient Abstract Geometric Reasoning. (arXiv:2306.03175v1 [cs.AI])

    [http://arxiv.org/abs/2306.03175](http://arxiv.org/abs/2306.03175)

    LatFormer是一种将格点对称先验融入到注意力掩码中的模型，能够用卷积网络生成软掩码来调整注意力权重。该模型在合成几何推理中取得了较好效果。

    

    抽象和推理语料库（ARC）及其最近的语言完整实例（LARC）被认为是通往通用人工智能的重要一步。然而，即使是最先进的机器学习模型在这些问题上也难以实现有意义的性能，落后于非学习方法。我们认为解决这些任务需要极端的泛化能力，只有通过适当考虑核心知识先验才能实现。为了达到这个目标，我们聚焦于几何先验，并引入LatFormer模型，将格点对称先验融入到注意力掩码中。我们证明了对于超立方格的任何变换，都存在一个二值注意力掩码来实现该群作用。因此，我们的研究激发了对标准注意力机制的修改，其中使用卷积网络生成的软掩码来调整关注权重。在合成几何推理方面的实验表明，LatFormer

    The Abstraction and Reasoning Corpus (ARC) (Chollet, 2019) and its most recent language-complete instantiation (LARC) has been postulated as an important step towards general AI. Yet, even state-of-the-art machine learning models struggle to achieve meaningful performance on these problems, falling behind non-learning based approaches. We argue that solving these tasks requires extreme generalization that can only be achieved by proper accounting for core knowledge priors. As a step towards this goal, we focus on geometry priors and introduce LatFormer, a model that incorporates lattice symmetry priors in attention masks. We show that, for any transformation of the hypercubic lattice, there exists a binary attention mask that implements that group action. Hence, our study motivates a modification to the standard attention mechanism, where attention weights are scaled using soft masks generated by a convolutional network. Experiments on synthetic geometric reasoning show that LatFormer 
    
[^91]: 线性距离度量学习

    Linear Distance Metric Learning. (arXiv:2306.03173v1 [cs.LG])

    [http://arxiv.org/abs/2306.03173](http://arxiv.org/abs/2306.03173)

    本文介绍了一种线性距离度量学习方法，可以有效地从一个欧几里得度量空间中的数据学习出另一个欧几里得度量空间中的线性映射，即使数据中存在噪声，也可以以任意精度学习真实的线性距离度量，并提供相应的样本复杂度上界。此外，提供了一种有效的低秩模型截断方法，可以保证模型的准确性和精度。

    

    在线性距离度量学习中，给定欧几里得度量空间中的数据，目标是寻找一个适当的线性映射到另一个欧几里得度量空间，尽可能地满足一定的距离条件。本文规范了一种简单优美的方法，它简化为一个连续的凸损失函数优化问题，对于不同的噪声模型，我们推导出了相应的损失函数。我们展示了即使数据有噪声，只要有足够的样本，就可以以任意精度学习真实的线性距离度量，并提供相应的样本复杂度上界。此外，我们提供了一种有效的方法将学习到的模型截断为低秩模型，可以证明在损失函数和参数的准确性方面保持精度，这是这种类型的首个结果。对合成和真实数据集的几个实验观察支持和证明了我们的理论结果。

    In linear distance metric learning, we are given data in one Euclidean metric space and the goal is to find an appropriate linear map to another Euclidean metric space which respects certain distance conditions as much as possible. In this paper, we formalize a simple and elegant method which reduces to a general continuous convex loss optimization problem, and for different noise models we derive the corresponding loss functions. We show that even if the data is noisy, the ground truth linear metric can be learned with any precision provided access to enough samples, and we provide a corresponding sample complexity bound. Moreover, we present an effective way to truncate the learned model to a low-rank model that can provably maintain the accuracy in loss function and in parameters -- the first such results of this type. Several experimental observations on synthetic and real data sets support and inform our theoretical results.
    
[^92]: 如何跨越云和大陆培训深度学习模型？一项实验研究。

    How Can We Train Deep Learning Models Across Clouds and Continents? An Experimental Study. (arXiv:2306.03163v1 [cs.LG])

    [http://arxiv.org/abs/2306.03163](http://arxiv.org/abs/2306.03163)

    本文通过实验研究，探究了在不同大陆、云供应商和数据中心范围内，使用分布式数据并行点深度学习训练是否是更具成本效益的选择，并比较了其与集中式训练的可扩展性潜力。

    

    在云端或专用硬件上训练深度学习模型是昂贵的。一种更具成本效益的选择是提供点实例的高超规模云，这是一个便宜但短暂的选择，用于替代按需资源。由于点实例的可用性可能会因日期、大陆和云供应商不同而发生变化，因此在全球范围内分配资源可能更具成本效益。但是，尚未调查地理分布式数据并行点深度学习训练是否是集中式训练的更具成本效益的替代方案。本文旨在回答一个问题：深度学习模型能否在覆盖不同数据中心和云提供商的点 VM 全球市场上以更具成本效益的方式进行训练？为了提供指导，我们广泛评估了不同区域、大陆和云对代表性 CV 和 NLP 模型的成本和吞吐量影响。为了进一步扩展当前的培训选择，我们比较了可扩展性潜力。

    Training deep learning models in the cloud or on dedicated hardware is expensive. A more cost-efficient option are hyperscale clouds offering spot instances, a cheap but ephemeral alternative to on-demand resources. As spot instance availability can change depending on the time of day, continent, and cloud provider, it could be more cost-efficient to distribute resources over the world. Still, it has not been investigated whether geo-distributed, data-parallel spot deep learning training could be a more cost-efficient alternative to centralized training.  This paper aims to answer the question: Can deep learning models be cost-efficiently trained on a global market of spot VMs spanning different data centers and cloud providers? To provide guidance, we extensively evaluate the cost and throughput implications of training in different zones, continents, and clouds for representative CV and NLP models. To expand the current training options further, we compare the scalability potential f
    
[^93]: 论量子学习中量子纠缠和统计学的作用

    On the Role of Entanglement and Statistics in Learning. (arXiv:2306.03161v1 [quant-ph])

    [http://arxiv.org/abs/2306.03161](http://arxiv.org/abs/2306.03161)

    本论文探讨了量子学习中量子纠缠和统计学的作用，研究了纠缠测量与可分离测量以及纠缠测量与统计测量在学习模型中的区别，证明了QSQ学习与利用纠缠测量的量子学习之间的指数差异。

    

    本论文探究了量子统计查询(QSQ)模型中利用量子纠缠、可分离以及统计测量方法进行学习模型之间的关系。我们得出下列结论。$\textbf{纠缠测量与可分离测量}$：在一个给定的概念类$C\subseteq \{f:\{0,1\}^n\rightarrow [k]\}$中，利用$\frac{1}{\sqrt{2^n}}\sum_x\vert x,f(x)\rangle$的副本来学习一个未知函数$f$，如果利用纠缠测量，则需要$T$个副本即可完成学习，则只需利用可分离测量，就需要$O(nT^2)$个副本。$\textbf{纠缠测量与统计测量}$：在可分离测量和统计测量的基础上学习一个函数$f\in C$。我们构建了一个类$C$，证明了QSQ学习与利用纠缠测量的量子学习之间的指数差异（即使在存在噪声的情况下也是如此），这证明了量子学习的量子版本。

    In this work we make progress in understanding the relationship between learning models with access to entangled, separable and statistical measurements in the quantum statistical query (QSQ) model. To this end, we show the following results.  $\textbf{Entangled versus separable measurements.}$ The goal here is to learn an unknown $f$ from the concept class $C\subseteq \{f:\{0,1\}^n\rightarrow [k]\}$ given copies of $\frac{1}{\sqrt{2^n}}\sum_x \vert x,f(x)\rangle$. We show that, if $T$ copies suffice to learn $f$ using entangled measurements, then $O(nT^2)$ copies suffice to learn $f$ using just separable measurements.  $\textbf{Entangled versus statistical measurements}$ The goal here is to learn a function $f \in C$ given access to separable measurements and statistical measurements. We exhibit a class $C$ that gives an exponential separation between QSQ learning and quantum learning with entangled measurements (even in the presence of noise). This proves the "quantum analogue" of th
    
[^94]: DISCount: 基于检测器重要性采样的大规模图像计数方法

    DISCount: Counting in Large Image Collections with Detector-Based Importance Sampling. (arXiv:2306.03151v1 [cs.CV])

    [http://arxiv.org/abs/2306.03151](http://arxiv.org/abs/2306.03151)

    这篇论文提出了一种基于检测器重要性采样的大规模图像计数方法DISCount，通过将不完美的检测器与人工筛选相结合，产生无偏估计的计数。该方法可以解决多个空间或时间区域的计数问题，并可根据估计的准确性对筛选样本数和置信区间进行估计和调整。

    

    许多现代应用程序使用计算机视觉在海量图像集合中检测和计数对象。然而，当检测任务非常困难或存在领域转移时，即使投入了大量的培训数据和模型开发，计数结果也可能不准确。我们提出了DISCount——一种基于检测器重要性采样框架的大规模图像计数方法，将一个不完美的检测器与人工筛选相结合，产生无偏估计的计数。我们提出了解决多个空间或时间区域计数问题的技术，只需使用少量的筛选样本并估计置信区间。这使得最终用户在估计足够准确时可以停止筛选，这通常是科学研究的目标。在技术方面，我们开发了基于控制变量的方差缩减技术，并证明了估计器的（条件）无偏性。DISCount导致了9-12倍的降低计数误差。

    Many modern applications use computer vision to detect and count objects in massive image collections. However, when the detection task is very difficult or in the presence of domain shifts, the counts may be inaccurate even with significant investments in training data and model development. We propose DISCount -- a detector-based importance sampling framework for counting in large image collections that integrates an imperfect detector with human-in-the-loop screening to produce unbiased estimates of counts. We propose techniques for solving counting problems over multiple spatial or temporal regions using a small number of screened samples and estimate confidence intervals. This enables end-users to stop screening when estimates are sufficiently accurate, which is often the goal in a scientific study. On the technical side we develop variance reduction techniques based on control variates and prove the (conditional) unbiasedness of the estimators. DISCount leads to a 9-12x reduction
    
[^95]: 通过转化特定注释者和特定实例的转移矩阵从众包中学习

    Transferring Annotator- and Instance-dependent Transition Matrix for Learning from Crowds. (arXiv:2306.03116v1 [cs.HC])

    [http://arxiv.org/abs/2306.03116](http://arxiv.org/abs/2306.03116)

    本文提出了一个高效的方法来估算特定注释者和特定实例的转移矩阵以及真实标签比例，解决了从众包中学习的标签噪声问题，并在实验中证明了方法的优越性。

    

    本文描述了从众包服务中获取训练数据的注释方法。每个注释者都完成自己的小部分注释，不同注释者的标注错误往往不同。通过标签噪声的转移矩阵来建模噪声产生过程是解决标签噪声的一种有效工具。在实际众包模型中，转移矩阵既由注释者依赖，也由实例依赖。然而，由于注释者和实例依赖的转移矩阵(AIDTM)具有高复杂度，而实际注释往往涉及注释稀疏性，这使得建立AIDTM非常具有挑战性。既要保持建模的广泛性，又能更真实地解决问题，本文提出了一种高效的算法，可以同时估算AIDTM和真实标签比例。我们还提供了理论分析，证明了我们的算法的收敛性。在合成数据集和真实数据集上的实验结果表明，我们的算法优于基准方法。

    Learning from crowds describes that the annotations of training data are obtained with crowd-sourcing services. Multiple annotators each complete their own small part of the annotations, where labeling mistakes that depend on annotators occur frequently. Modeling the label-noise generation process by the noise transition matrix is a power tool to tackle the label noise. In real-world crowd-sourcing scenarios, noise transition matrices are both annotator- and instance-dependent. However, due to the high complexity of annotator- and instance-dependent transition matrices (AIDTM), \textit{annotation sparsity}, which means each annotator only labels a little part of instances, makes modeling AIDTM very challenging. Prior works simplify the problem by assuming the transition matrix is instance-independent or using simple parametric way, while lose modeling generality. Motivated by this, we target a more realistic problem, estimating general AIDTM in practice. Without losing modeling general
    
[^96]: AutoExp: 一种多学科、多传感器框架，用于评估自动驾驶汽车中的人类活动

    AutoExp: A multidisciplinary, multi-sensor framework to evaluate human activities in self-driving cars. (arXiv:2306.03115v1 [cs.HC])

    [http://arxiv.org/abs/2306.03115](http://arxiv.org/abs/2306.03115)

    本文提出一个多学科、多传感器框架，用于评估自动驾驶汽车中乘客的活动，并在最近实际条件下捕获真实数据来创建数据集。

    

    自动驾驶汽车的普及将彻底改变我们的生活，即使它们的全自主化可能比最初预计的需要更长时间。目前，该技术的第一批车辆已经出现在世界某些城市中，作为实验性机器人出租车服务的一部分。本文提出了一个实验性框架，利用多学科方法（计算机视觉与人文社会科学相结合），特别是非驾驶相关活动，来研究自动驾驶汽车乘客的活动。该框架由实验场景和数据采集模块组成，旨在首先在最近可能的实际条件下捕获有关车辆使用的真实数据，并创建一个包含大量传感器数据和日志记录的数据集。

    The adoption of self-driving cars will certainly revolutionize our lives, even though they may take more time to become fully autonomous than initially predicted. The first vehicles are already present in certain cities of the world, as part of experimental robot-taxi services. However, most existing studies focus on the navigation part of such vehicles. We currently miss methods, datasets, and studies to assess the in-cabin human component of the adoption of such technology in real-world conditions. This paper proposes an experimental framework to study the activities of occupants of self-driving cars using a multidisciplinary approach (computer vision associated with human and social sciences), particularly non-driving related activities. The framework is composed of an experimentation scenario, and a data acquisition module. We seek firstly to capture real-world data about the usage of the vehicle in the nearest possible, real-world conditions, and secondly to create a dataset conta
    
[^97]: 使用生成模型合成情感神经生理信号的综述论文

    Synthesizing Affective Neurophysiological Signals Using Generative Models: A Review Paper. (arXiv:2306.03112v1 [cs.HC])

    [http://arxiv.org/abs/2306.03112](http://arxiv.org/abs/2306.03112)

    本文综述了使用生成模型合成神经生理信号来解决公共情感数据集稀缺性的问题。通过对领域中使用的不同生成模型进行全面分析，我们提供了有关生成模型在情感识别系统应用的优势、挑战和有前途的未来方向的深入见解。

    

    在提高人机交互中，将情感智能引入机器是重要的一步。这需要开发可靠的端到端情感识别系统。然而，公共情感数据集的稀缺性提出了一个挑战。在本文献评文章中，我们强调了使用生成模型来解决神经生理学信号中，尤其是脑电图（EEG）和功能性近红外光谱（fNIRS）中这个问题。我们对领域中使用的不同生成模型进行了全面的分析，检验了它们的输入格式、部署策略以及用于评估合成数据质量的方法。本文的综述是一篇全面的概述，提供了有关生成模型在情感识别系统应用中的优势、挑战和有前途的未来方向的深入见解。通过这篇综述，我们旨在促进神经生理数据增强的进展。

    The integration of emotional intelligence in machines is an important step in advancing human-computer interaction. This demands the development of reliable end-to-end emotion recognition systems. However, the scarcity of public affective datasets presents a challenge. In this literature review, we emphasize the use of generative models to address this issue in neurophysiological signals, particularly Electroencephalogram (EEG) and Functional Near-Infrared Spectroscopy (fNIRS). We provide a comprehensive analysis of different generative models used in the field, examining their input formulation, deployment strategies, and methodologies for evaluating the quality of synthesized data. This review serves as a comprehensive overview, offering insights into the advantages, challenges, and promising future directions in the application of generative models in emotion recognition systems. Through this review, we aim to facilitate the progression of neurophysiological data augmentation, there
    
[^98]: 针对离线设计生物序列的得分条件生成器的自助增强训练

    Bootstrapped Training of Score-Conditioned Generator for Offline Design of Biological Sequences. (arXiv:2306.03111v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.03111](http://arxiv.org/abs/2306.03111)

    本文提出了一种BootGen算法，使用代理得分函数增强生物序列生成器的训练数据集，并产生多样化的设计，将其应用于优化生物序列，取得了比竞争对手更好的结果。

    

    本文研究了优化生物序列（如蛋白质、DNA和RNA）以最大化仅在离线数据集中评估的黑匣子得分函数的问题。我们提出了一种新颖的解决方案——得分条件生成器的自助增强训练（BootGen）算法。我们的算法重复了一个两阶段过程。在第一阶段，我们的算法使用排名加权法训练生物序列生成器，以提高基于高分数的序列生成的准确性。接下来的阶段涉及到自助增强，通过自动生成的数据并标记代理得分函数，来增强训练数据集。我们的关键思想是将基于得分的生成与代理得分函数对齐，将代理得分函数的知识传递给生成器。训练后，我们聚合来自多个自助增强生成器和代理的样本，产生多样化的设计。大量实验表明，我们的方法在生物序列优化方面胜过竞争对手。

    We study the problem of optimizing biological sequences, e.g., proteins, DNA, and RNA, to maximize a black-box score function that is only evaluated in an offline dataset. We propose a novel solution, bootstrapped training of score-conditioned generator (BootGen) algorithm. Our algorithm repeats a two-stage process. In the first stage, our algorithm trains the biological sequence generator with rank-based weights to enhance the accuracy of sequence generation based on high scores. The subsequent stage involves bootstrapping, which augments the training dataset with self-generated data labeled by a proxy score function. Our key idea is to align the score-based generation with a proxy score function, which distills the knowledge of the proxy score function to the generator. After training, we aggregate samples from multiple bootstrapped generators and proxies to produce a diverse design. Extensive experiments show that our method outperforms competitive baselines on biological sequential
    
[^99]: 数据费用感知训练的机器学习力场

    Machine Learning Force Fields with Data Cost Aware Training. (arXiv:2306.03109v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.03109](http://arxiv.org/abs/2306.03109)

    本论文提出了ASTEROID框架，利用便宜而不准确的数据和昂贵而准确的数据来降低MLFF的数据成本。该框架采用了偏差感知损失函数，让MLFF模型在防止过拟合的同时，也能捕捉基础力场的复杂结构。

    

    机器学习力场(MLFF)被提出以加速分子动力学(MD)模拟，该模拟在化学和生物医学研究中应用广泛。即使对于最节约数据的MLFF，要达到化学精度可能需要数百帧由昂贵的量子力学算法生成的力和能量标签，这些算法的规模可能为$O(n^3)$至$O(n^7)$，其中$n$与基函数的数量成比例。为了解决这个问题，我们提出了一个多阶段计算框架ASTEROID，通过利用便宜的不准确数据和昂贵的准确数据降低了MLFF的数据成本。ASTEROID的动机是，尽管不准确的数据会产生很大的偏差，但它可以帮助捕捉基础力场的复杂结构。因此，我们首先在大量不准确的训练数据上使用偏差感知损失函数训练MLFF模型，以防止模型过拟合可能导致的偏差。

    Machine learning force fields (MLFF) have been proposed to accelerate molecular dynamics (MD) simulation, which finds widespread applications in chemistry and biomedical research. Even for the most data-efficient MLFFs, reaching chemical accuracy can require hundreds of frames of force and energy labels generated by expensive quantum mechanical algorithms, which may scale as $O(n^3)$ to $O(n^7)$, with $n$ proportional to the number of basis functions. To address this issue, we propose a multi-stage computational framework -ASTEROID, which lowers the data cost of MLFFs by leveraging a combination of cheap inaccurate data and expensive accurate data. The motivation behind ASTEROID is that inaccurate data, though incurring large bias, can help capture the sophisticated structures of the underlying force field. Therefore, we first train a MLFF model on a large amount of inaccurate training data, employing a bias-aware loss function to prevent the model from overfitting tahe potential bia
    
[^100]: 使用改进的PINN方法基于数据驱动的局域波解决Fokas-Lenells方程

    Data driven localized wave solution of the Fokas-Lenells equation using modified PINN. (arXiv:2306.03105v1 [nlin.PS])

    [http://arxiv.org/abs/2306.03105](http://arxiv.org/abs/2306.03105)

    本研究利用改进的PINN方法基于数据，成功获得了Fokas-Lenells方程中的明暗孤子解，并提出使用守恒量信息的损失函数可以提高预测和精确解的准确性。

    

    本文利用物理信息神经网络(PINN)研究了Fokas-Lenells方程的数据驱动局域波解。我们通过在残差损失函数中引入控制参数来改进基本的PINN，并添加守恒量作为另一个损失项以修改PINN。使用改进的PINN，我们获得了Fokas-Lenells方程的数据驱动的明暗孤子解。守恒量信息的损失函数在预测和精确孤子解之间的相对L2误差方面实现更高的准确性。我们希望本研究对于研究在非线性光学和其他非线性物理学领域中深度学习的应用有用。源代码可在https://github.com/gautamksaharia/Fokas-Lenells获得。

    We investigate data driven localized wave solutions of the Fokas-Lenells equation by using physics informed neural network(PINN). We improve basic PINN by incorporating control parameters into the residual loss function. We also add conserve quantity as another loss term to modify the PINN. Using modified PINN we obtain the data driven bright soliton and dark soliton solutions of Fokas-Lenells equation. Conserved quantities informed loss function achieve more accuracy in terms of relative L2 error between predicted and exact soliton solutions. We hope that the present investigation would be useful to study the applications of deep learning in nonlinear optics and other branches of nonlinear physics. Source codes are available at https://github.com/gautamksaharia/Fokas-Lenells
    
[^101]: CrystalGPT：基于时间序列转换的晶体预测与控制中系统对系统转移性的增强

    CrystalGPT: Enhancing system-to-system transferability in crystallization prediction and control using time-series-transformers. (arXiv:2306.03099v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2306.03099](http://arxiv.org/abs/2306.03099)

    该论文介绍了一种基于时间序列转换的新型框架CrystalGPT，利用变压器算法中的强大转移学习，提高了晶体预测与控制中的系统对系统转移性。与现有技术相比，CrystalGPT的累积误差降低了8倍。

    

    为了预测和实时控制任务，经常使用基于机器学习（ML）的数字孪生。 然而，尽管这些模型通常准确，但它们是为单个系统定制的，使得系统对系统（S2S）转移困难。 即使在不同化学系统的过程动力学存在重要相似性时也会出现这种情况。 为了应对这一挑战，我们开发了一种新颖的时间序列转换器（TST）框架，利用变压器算法中内在的强大转移学习能力。 这是使用从在各种操作场景下工作的不同晶体器中获得的可用过程数据展示的。 使用这个广泛的数据集，我们训练了一个TST模型（CrystalGPT），它在所有预先建立的系统之间展现了显着的S2S可转移性，甚至对于未遇到的系统也是如此。 CrystalGPT在所有系统中共同实现的累积误差是现有最先进技术的8倍。

    For prediction and real-time control tasks, machine-learning (ML)-based digital twins are frequently employed. However, while these models are typically accurate, they are custom-designed for individual systems, making system-to-system (S2S) transferability difficult. This occurs even when substantial similarities exist in the process dynamics across different chemical systems. To address this challenge, we developed a novel time-series-transformer (TST) framework that exploits the powerful transfer learning capabilities inherent in transformer algorithms. This was demonstrated using readily available process data obtained from different crystallizers operating under various operational scenarios. Using this extensive dataset, we trained a TST model (CrystalGPT) to exhibit remarkable S2S transferability not only across all pre-established systems, but also to an unencountered system. CrystalGPT achieved a cumulative error across all systems, which is eight times superior to that of exi
    
[^102]: 明藏暗窃：伪装数据窃取攻击在联邦学习中的应用

    Hiding in Plain Sight: Disguising Data Stealing Attacks in Federated Learning. (arXiv:2306.03013v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2306.03013](http://arxiv.org/abs/2306.03013)

    该研究提出了一个新的恶意服务器攻击框架SEER，可以在联邦学习中窃取用户数据，而且不易被客户端侦测出来。

    

    恶意服务器攻击已经使得联邦学习中的数据窃取在大批量和安全聚合等之前被视为私密的设置中变得可行。然而，许多关于恶意服务器攻击客户端侦测性的疑虑被提出，这使得它们在被公开知晓后的实用性受到质疑。在本研究中，我们首次全面研究了客户端侦测的问题。我们证明了许多基于两个关键原则的恶意服务器攻击都可以通过合理的客户端检查来检测出来。此外，我们制定了实用恶意服务器攻击的理想要求，并提出了SEER攻击框架，它满足所有理想要求，可以从现实网络的梯度中窃取用户数据，即使是在大批量(我们的实验中最大可达512)和安全聚合的情况下。SEER攻击的关键是使用秘密解码器，在共享模型上进行联合训练。我们的工作是迈向实用恶意服务器攻击的有前途的第一步。

    Malicious server (MS) attacks have enabled the scaling of data stealing in federated learning to large batch sizes and secure aggregation, settings previously considered private. However, many concerns regarding client-side detectability of MS attacks were raised, questioning their practicality once they are publicly known. In this work, for the first time, we thoroughly study the problem of client-side detectability.We demonstrate that most prior MS attacks, which fundamentally rely on one of two key principles, are detectable by principled client-side checks. Further, we formulate desiderata for practical MS attacks and propose SEER, a novel attack framework that satisfies all desiderata, while stealing user data from gradients of realistic networks, even for large batch sizes (up to 512 in our experiments) and under secure aggregation. The key insight of SEER is the use of a secret decoder, which is jointly trained with the shared model. Our work represents a promising first step to
    
[^103]: 非参数迭代机器教学

    Nonparametric Iterative Machine Teaching. (arXiv:2306.03007v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.03007](http://arxiv.org/abs/2306.03007)

    本文提出了解决非参数目标模型的迭代机器教学问题的方法，包括随机和贪心泛函教学算法。

    

    本文研究了迭代机器教学(IMT)问题。现有IMT算法基于参数化的目标模型。本文研究了更普遍的任务——非参数迭代机器教学(NIMT)，旨在以迭代方式向学习者教授非参数目标模型。为了解决这一问题，我们将NIMT视为一个在函数空间中的泛函优化问题，并提出了随机和贪心泛函教学算法。

    In this paper, we consider the problem of Iterative Machine Teaching (IMT), where the teacher provides examples to the learner iteratively such that the learner can achieve fast convergence to a target model. However, existing IMT algorithms are solely based on parameterized families of target models. They mainly focus on convergence in the parameter space, resulting in difficulty when the target models are defined to be functions without dependency on parameters. To address such a limitation, we study a more general task -Nonparametric Iterative Machine Teaching (NIMT), which aims to teach nonparametric target models to learners in an iterative fashion. Unlike parametric IMT that merely operates in the parameter space, we cast NIMT as a functional optimization problem in the function space. To solve it, we propose both random and greedy functional teaching algorithms. We obtain the iterative teaching dimension (ITD) of the random teaching algorithm under proper assumptions, which se
    
[^104]: Time Interpret: 一种序列数据可解释性统一模型库

    Time Interpret: a Unified Model Interpretability Library for Time Series. (arXiv:2306.02968v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02968](http://arxiv.org/abs/2306.02968)

    Time Interpret是一个基于Captum的模型解释库，专门用于解释时间序列数据，并实现了多种特征归因方法。此外，它还提供了各种PyTorch模型和数据集，以及一组用于评估特征归因的方法。

    

    我们介绍了一款名为$\texttt{time_interpret}$的库，它是以Captum为基础设计的，专门用于解释时间序列数据。该库实现了多种特征归因方法，可用于解释任何Pytorch模型的预测。此外，$\texttt{time_interpret}$还提供了多种合成和真实的时间序列数据集、各种PyTorch模型以及一组用于评估特征归因的方法。虽然该库主要用于解释基于时间数据的预测，但它的某些组件也有不同的应用，例如解释语言模型的预测。本文概述了该库的基本内容，并提供了几种以前未公开的特征归因方法，这些方法是与$\texttt{time_interpret}$同时开发的。

    We introduce $\texttt{time_interpret}$, a library designed as an extension of Captum, with a specific focus on temporal data. As such, this library implements several feature attribution methods that can be used to explain predictions made by any Pytorch model. $\texttt{time_interpret}$ also provides several synthetic and real world time series datasets, various PyTorch models, as well as a set of methods to evaluate feature attributions. Moreover, while being primarily developed to explain predictions based on temporal data, some of its components have a different application, including for instance methods explaining predictions made by language models. In this paper, we give a general introduction of this library. We also present several previously unpublished feature attribution methods, which have been developed along with $\texttt{time_interpret}$.
    
[^105]: 抓住意外收获：在离线策略演员-评论家中利用过去成功的价值(arXiv:2306.02865v2 [cs.LG]已更新)

    Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic. (arXiv:2306.02865v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02865](http://arxiv.org/abs/2306.02865)

    该论文提出了 BEE 操作符，通过充分利用过去的成功经验，并保持探索乐观性，解决了离线策略演员-评论家中 Q 值高估与低估问题，提高了策略学习和样本效率。

    

    学习高质量的 Q 值函数在许多现代离线深度强化学习 (RL) 算法的成功中起着关键作用。之前的研究集中解决采用函数逼近器和离线学习所导致的值过高的问题。与这种普遍观点不同，我们观察到 Q 值在 RL 训练过程的后期实际上被低估了，主要是由于贝尔曼更新中，当前策略使用比回放缓冲区中更优的动作样本差。我们假设这个长期被忽视的现象可能阻碍了策略学习，降低了样本效率。我们的想法是在保持探索乐观性的同时，结合充分利用过去成功的经验。我们提出了混合利用和探索 (BEE) 操作符，这是一种简单而有效的方法，使用历史上表现最佳的动作和当前策略生成的动作来更新 Q 值。

    Learning high-quality Q-value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. Previous works focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. Deviating from the common viewpoint, we observe that Q-values are indeed underestimated in the latter stage of the RL training process, primarily related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer. We hypothesize that this long-neglected phenomenon potentially hinders policy learning and reduces sample efficiency. Our insight to address this issue is to incorporate sufficient exploitation of past successes while maintaining exploration optimism. We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates Q-value using both historical best-performing actions and
    
[^106]: 迈向更好的目标检测解释

    Towards Better Explanations for Object Detection. (arXiv:2306.02744v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.02744](http://arxiv.org/abs/2306.02744)

    该论文提出了一种名为D-CLOSE的方法，用于解释任何目标检测模型的决策，通过使用多个分割级别和一种组合它们的过程，可以提供更好的质量和更少的噪音解释。

    

    人工智能技术的最新进展促进了它们在几乎所有领域的使用。深度神经网络的增加的复杂性使解释网络内部工作和决策变得越来越困难和重要。但是，目前大多数解释深度神经网络的技术主要集中在解释分类任务上。本文提出了一种名为D-CLOSE的方法，用于解释任何目标检测模型的决策。为了密切跟踪模型的行为，我们在图像上使用了多个分割级别和一种组合它们的过程。我们使用YOLOX模型在MS-COCO数据集上进行了测试，结果表明我们的方法优于D-RISE，可以提供更好的质量和更少的噪音解释。

    Recent advances in Artificial Intelligence (AI) technology have promoted their use in almost every field. The growing complexity of deep neural networks (DNNs) makes it increasingly difficult and important to explain the inner workings and decisions of the network. However, most current techniques for explaining DNNs focus mainly on interpreting classification tasks. This paper proposes a method to explain the decision for any object detection model called D-CLOSE. To closely track the model's behavior, we used multiple levels of segmentation on the image and a process to combine them. We performed tests on the MS-COCO dataset with the YOLOX model, which shows that our method outperforms D-RISE and can give a better quality and less noise explanation.
    
[^107]: 校准史塔克伯格博弈：学习对抗校准智能体的最优承诺

    Calibrated Stackelberg Games: Learning Optimal Commitments Against Calibrated Agents. (arXiv:2306.02704v1 [cs.GT] CROSS LISTED)

    [http://arxiv.org/abs/2306.02704](http://arxiv.org/abs/2306.02704)

    本文提出了一种新的校准史塔克伯格博弈（CSG）框架，其中智能体根据校准预测进行最佳响应。同时引入了自适应校准概念，提供精细的任何时候校准保证。在有限CSG中，主体可以获得最优解。

    

    本文提出了标准史塔克伯格博弈（SG）框架的一种推广：校准史塔克伯格博弈（CSG）。在CSG中，一个主体与一个智能体反复交互，后者不像标准SG一样直接访问主体的动作，而是对其进行校准预测，以达到最佳响应。CSG是一个强大的建模工具，超越了假定代理使用特定算法进行战略交互的做法，因此更加鲁棒地应对了SG最初旨在捕捉的现实应用。除了CSG外，本文还介绍了更强的校准概念，称为自适应校准，可针对敌对序列提供精细的任何时候校准保证。本文给出了获得自适应校准算法的一般方法，并将其专门用于有限CSG。在我们的主要技术结果中，我们证明在CSG中，主体可以获得收敛于最优解的效用。

    In this paper, we introduce a generalization of the standard Stackelberg Games (SGs) framework: Calibrated Stackelberg Games (CSGs). In CSGs, a principal repeatedly interacts with an agent who (contrary to standard SGs) does not have direct access to the principal's action but instead best-responds to calibrated forecasts about it. CSG is a powerful modeling tool that goes beyond assuming that agents use ad hoc and highly specified algorithms for interacting in strategic settings and thus more robustly addresses real-life applications that SGs were originally intended to capture. Along with CSGs, we also introduce a stronger notion of calibration, termed adaptive calibration, that provides fine-grained any-time calibration guarantees against adversarial sequences. We give a general approach for obtaining adaptive calibration algorithms and specialize them for finite CSGs. In our main technical result, we show that in CSGs, the principal can achieve utility that converges to the optimum
    
[^108]: 利用深度学习模型预测布隆迪疟疾动态

    Predicting malaria dynamics in Burundi using deep Learning Models. (arXiv:2306.02685v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02685](http://arxiv.org/abs/2306.02685)

    该论文利用机器学习方法建立疟疾预测模型，成功预测了布隆迪疟疾时空动态，为疟疾防治和干预设计提供了重要依据。

    

    疟疾继续在非洲大陆特别是撒哈拉以南非洲地区成为主要的公共卫生问题。然而，我们仍在努力并取得了显著进展。在布隆迪，疟疾是主要的公共卫生问题之一。然而，目前关于布隆迪疟疾预测模型的研究还很有限。我们建立基于机器学习的模型来估计布隆迪的疟疾病例。利用气候变化相关因素如温度、降雨和相对湿度以及疟疾历史数据和人口数据，采用一种深度学习模型——LSTM模型，对省级和国家范围内的疟疾情况进行了预测。根据模型结果，可以确定在不同参数调整下，国家级别的最低和最高预期疟疾病例数。

    Malaria continues to be a major public health problem on the African continent, particularly in Sub-Saharan Africa. Nonetheless, efforts are ongoing, and significant progress has been made. In Burundi, malaria is among the main public health concerns. In the literature, there are limited prediction models for Burundi. We know that such tools are much needed for interventions design. In our study, we built machine-learning based models to estimates malaria cases in Burundi. The forecast of malaria cases was carried out at province level and national scale as well. Long short term memory (LSTM) model, a type of deep learning model has been used to achieve best results using climate-change related factors such as temperature, rainfal, and relative humidity, together with malaria historical data and human population. With this model, the results showed that at country level different tuning of parameters can be used in order to determine the minimum and maximum expected malaria cases. The 
    
[^109]: 一种基于Polyak-{\L}ojasiewicz条件的双层学习的广义交替方法

    A Generalized Alternating Method for Bilevel Learning under the Polyak-{\L}ojasiewicz Condition. (arXiv:2306.02422v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2306.02422](http://arxiv.org/abs/2306.02422)

    本研究提出了一种基于Polyak-{\L}ojasiewicz条件的双层学习的广义交替方法，即GALET，可以用于解决非凸下层目标的双层问题。该方法可以在$\tilde{\cal O}(\epsilon^{-1})$迭代次数内实现问题的$\epsilon$-静态度量。

    

    双层优化近年来因其在超参数优化、元学习和强化学习等新兴机器学习领域中的应用而重新引起人们的关注。最近的结果显示，对于具有强凸下层目标的双层问题，简单的交替（隐式）基于梯度的算法可以实现与单层梯度下降（GD）相同的收敛速率。然而，对于超出此基本设置的双层问题，尚不清楚是否可以推广该结果。在本文中，我们提出了一种基于满足Polyak-{\L}ojasiewicz (PL)条件的非凸下层目标的双层优化的广义交替方法（GALET）。我们首先介绍了所考虑的双层问题的一个静态度量，它推广了现有的度量。然后我们证明GALET在$\tilde{\cal O}(\epsilon^{-1})$迭代次数内实现了所考虑问题的$\epsilon$-静态度量。

    Bilevel optimization has recently regained interest owing to its applications in emerging machine learning fields such as hyperparameter optimization, meta-learning, and reinforcement learning. Recent results have shown that simple alternating (implicit) gradient-based algorithms can achieve the same convergence rate of single-level gradient descent (GD) for bilevel problems with a strongly convex lower-level objective. However, it remains unclear whether this result can be generalized to bilevel problems beyond this basic setting. In this paper, we propose a Generalized ALternating mEthod for bilevel opTimization (GALET) with a nonconvex lower-level objective that satisfies the Polyak-{\L}ojasiewicz (PL) condition. We first introduce a stationary metric for the considered bilevel problems, which generalizes the existing metric. We then establish that GALET achieves an $\epsilon$-stationary metric for the considered problem within $\tilde{\cal O}(\epsilon^{-1})$ iterations, which match
    
[^110]: 循环一致性驱动的物体发现方法

    Cycle Consistency Driven Object Discovery. (arXiv:2306.02204v1 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2306.02204](http://arxiv.org/abs/2306.02204)

    该方法通过循环一致性目标的引入，明确优化场景中每个物体应映射到不同槽位的约束，从而实现了在完全无监督的情况下有效地学习发现物体。在实验中表现出了优于现有方法的性能。

    

    开发能够有效学习类似于人类认知的以物体为中心的表示的深度学习模型仍然是一项具有挑战性的任务。现有的方法利用架构先验或辅助信息（例如深度图或流场图）来探索基于槽位的方法，以表示对象为称为“槽位”或“对象文件”的固定大小的向量，从而促进物体发现。 然而，依赖于架构先验会引入不可靠性，并需要精心设计才能识别正确的对象。 同样，依赖辅助信息的方法也不够优越，因为这种信息通常在大多数自然情况下不可用。为了解决这些限制，我们提出了一种明确优化场景中每个对象应映射到一个不同槽位的方法。我们通过引入循环一致性目标来形式化这个约束，称之为循环一致性目标。通过应用这些限制，我们的方法可以在完全无监督的情况下有效地学习发现物体。 在实验中，我们展示了我们的方法在无监督物体发现和少样本物体分类基准测试中均优于现有的最先进方法。

    Developing deep learning models that effectively learn object-centric representations, akin to human cognition, remains a challenging task. Existing approaches have explored slot-based methods utilizing architectural priors or auxiliary information such as depth maps or flow maps to facilitate object discovery by representing objects as fixed-size vectors, called ``slots'' or ``object files''. However, reliance on architectural priors introduces unreliability and requires meticulous engineering to identify the correct objects. Likewise, methods relying on auxiliary information are suboptimal as such information is often unavailable for most natural scenes. To address these limitations, we propose a method that explicitly optimizes the constraint that each object in a scene should be mapped to a distinct slot. We formalize this constraint by introducing consistency objectives which are cyclic in nature. We refer to them as the \textit{cycle-consistency} objectives. By applying these con
    
[^111]: 将神经网络转化为Yoked神经网络以改进ANN结构

    Transforming to Yoked Neural Networks to Improve ANN Structure. (arXiv:2306.02157v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02157](http://arxiv.org/abs/2306.02157)

    本文提出了一种叫做YNN的方法，将同一级别的ANN节点连接在一起形成神经模块，解决了普通ANN无法共享信息的缺陷，显著提高了信息传输和性能。

    

    大部分已经存在的经典人工神经网络（ANN）都被设计成树形结构以模拟神经网络。本文认为，树形结构的连接不足以描述神经网络。同一级别的节点不能连接在一起，即这些神经元不能相互共享信息，这是ANN的一个重大缺陷。我们提出了一种方法，即为同一级别的节点建立双向完全图，将同一级别的节点连接到一起形成神经模块。我们把我们的模型称为YNN。YNN显著促进了信息传输，明显有助于提高该方法的性能。我们的YNN可以更好地模拟神经网络，相比其他ANN方法有着明显的优势。

    Most existing classical artificial neural networks (ANN) are designed as a tree structure to imitate neural networks. In this paper, we argue that the connectivity of a tree is not sufficient to characterize a neural network. The nodes of the same level of a tree cannot be connected with each other, i.e., these neural unit cannot share information with each other, which is a major drawback of ANN. Although ANN has been significantly improved in recent years to more complex structures, such as the directed acyclic graph (DAG), these methods also have unidirectional and acyclic bias for ANN. In this paper, we propose a method to build a bidirectional complete graph for the nodes in the same level of an ANN, which yokes the nodes of the same level to formulate a neural module. We call our model as YNN in short. YNN promotes the information transfer significantly which obviously helps in improving the performance of the method. Our YNN can imitate neural networks much better compared with 
    
[^112]: MultiLegalPile：689GB的多语言法律语料库

    MultiLegalPile: A 689GB Multilingual Legal Corpus. (arXiv:2306.02069v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.02069](http://arxiv.org/abs/2306.02069)

    MultiLegalPile是一个689GB的多语言法律语料库，包含来自17个司法管辖区的24种语言的不同法律数据源，允许在公平使用下针对预训练NLP模型。该语料库为多语言模型的预训练提供了新的最佳表现，并在LexGLUE上表现最佳。

    

    大型高质量的数据集对于训练大型语言模型(LLMs)至关重要。然而，目前为止，专业领域（如法律）可用的数据集很少，而且经常仅限于英语。我们整理并发布了MultiLegalPile，这是一个包含来自17个司法管辖区的24种语言的689GB语料库。MultiLegalPile语料库包括各种许可证的不同法律数据源，允许在公平使用下针对预训练自然语言处理(NLP)模型，对于Eurlex Resources和Legal mC4子集拥有更宽松的许可证。我们进行了两个RoBERTa模型和一个多语言Longformer的预训练，并分别在每种特定语言子集上进行了24个单语模型的预训练，并在LEXTREME上对它们进行了评估。此外，我们在LexGLUE上对英语和多语言模型进行了评估。我们的多语言模型在LEXTREME上创造了新的最佳表现(SotA)，英语模型则在LexGLUE上表现最佳。我们将数据集、训练模型和代码全部释放在最开放的许可证下。

    Large, high-quality datasets are crucial for training Large Language Models (LLMs). However, so far, there are few datasets available for specialized critical domains such as law and the available ones are often only for the English language. We curate and release MultiLegalPile, a 689GB corpus in 24 languages from 17 jurisdictions. The MultiLegalPile corpus, which includes diverse legal data sources with varying licenses, allows for pretraining NLP models under fair use, with more permissive licenses for the Eurlex Resources and Legal mC4 subsets. We pretrain two RoBERTa models and one Longformer multilingually, and 24 monolingual models on each of the language-specific subsets and evaluate them on LEXTREME. Additionally, we evaluate the English and multilingual models on LexGLUE. Our multilingual models set a new SotA on LEXTREME and our English models on LexGLUE. We release the dataset, the trained models, and all of the code under the most open possible licenses.
    
[^113]: 对低质量多模态数据的可证明动态融合

    Provable Dynamic Fusion for Low-Quality Multimodal Data. (arXiv:2306.02050v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02050](http://arxiv.org/abs/2306.02050)

    本文从泛化的角度出发，提供了理论上对可证明动态多模态融合方法的解释。提出了一种质量感知多模态融合（QMF）框架，可以提高分类准确性和模型的鲁棒性。

    

    多模态融合的困难在于精确捕捉跨模态相关性和灵活进行跨模态交互。为了充分释放每个模态的价值并减轻低质量多模态数据的影响，动态多模态融合成为有前途的学习范式。尽管被广泛使用，但该领域仍然缺乏理论证明。本文从泛化角度的最流行的多模态融合框架出发，提供了理论上的解释。我们接着揭示，多种不确定性估计解决方法可以自然地实现健壮的多模态融合。然后，提出一种名为质量感知多模态融合（QMF）的新型多模态融合框架，可以提高分类准确性和模型的鲁棒性。在多个基准测试上进行广泛的实验结果。

    The inherent challenge of multimodal fusion is to precisely capture the cross-modal correlation and flexibly conduct cross-modal interaction. To fully release the value of each modality and mitigate the influence of low-quality multimodal data, dynamic multimodal fusion emerges as a promising learning paradigm. Despite its widespread use, theoretical justifications in this field are still notably lacking. Can we design a provably robust multimodal fusion method? This paper provides theoretical understandings to answer this question under a most popular multimodal fusion framework from the generalization perspective. We proceed to reveal that several uncertainty estimation solutions are naturally available to achieve robust multimodal fusion. Then a novel multimodal fusion framework termed Quality-aware Multimodal Fusion (QMF) is proposed, which can improve the performance in terms of classification accuracy and model robustness. Extensive experimental results on multiple benchmarks can
    
[^114]: GAD-NR: 通过邻域重构实现图形异常检测

    GAD-NR: Graph Anomaly Detection via Neighborhood Reconstruction. (arXiv:2306.01951v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.01951](http://arxiv.org/abs/2306.01951)

    本文提出了一种新的图形异常检测方法GAD-NR，与现有的GAE模型不同的是，GAD-NR采用邻域重构的方式来检测更复杂非聚类的结构异常。

    

    图形异常检测（GAD）是一种技术，用于识别图中的异常节点，在网络安全、欺诈检测、社交媒体垃圾检测和其他各种领域中有应用。GAD的常见方法是图自编码器（GAEs），它将图形数据编码成节点表示，并根据这些表示来评估图形的重构质量，以识别异常。然而，现有的GAE模型主要针对直接链接重构进行优化，导致在潜在空间中连接图中的节点被聚类。因此，它们擅长检测聚类型结构异常，但对不符合聚类的更复杂的结构异常存在困难。为了解决这个限制，我们提出了一种新颖的解决方案，称为GAD-NR，它是GAE的一个新变体，融合邻域重构进行图形异常检测。GAD-NR的目标是重构节点的整个邻域，涵盖本地结构

    Graph Anomaly Detection (GAD) is a technique used to identify abnormal nodes within graphs, finding applications in network security, fraud detection, social media spam detection, and various other domains. A common method for GAD is Graph Auto-Encoders (GAEs), which encode graph data into node representations and identify anomalies by assessing the reconstruction quality of the graphs based on these representations. However, existing GAE models are primarily optimized for direct link reconstruction, resulting in nodes connected in the graph being clustered in the latent space. As a result, they excel at detecting cluster-type structural anomalies but struggle with more complex structural anomalies that do not conform to clusters. To address this limitation, we propose a novel solution called GAD-NR, a new variant of GAE that incorporates neighborhood reconstruction for graph anomaly detection. GAD-NR aims to reconstruct the entire neighborhood of a node, encompassing the local structu
    
[^115]: 机器学习流程的负责任设计模式

    Responsible Design Patterns for Machine Learning Pipelines. (arXiv:2306.01788v1 [cs.SE])

    [http://arxiv.org/abs/2306.01788](http://arxiv.org/abs/2306.01788)

    本文提出了一种综合框架，将负责任设计模式纳入机器学习流程中，以确保AI系统的伦理性和公正性。这个框架包括新的负责任AI设计模式，并指导AI开发人员、数据科学家和决策者在AI开发和部署中实施伦理实践。

    

    将道德实践整合到人工智能(AI)开发过程中对于确保AI的安全、公平和负责任操作至关重要。AI伦理涉及将伦理原则应用于AI系统的整个生命周期。这对于减轻与AI相关的潜在风险和伤害（如算法偏见）至关重要。为实现这一目标，机器学习流程中的负责任设计模式（RDPs）对于确保伦理和公平结果至关重要。在本文中，我们提出了一个综合框架，将RDPs纳入ML流程中，以减轻风险并确保AI系统的伦理发展。我们的框架包括新的负责任AI设计模式，这些模式通过对AI伦理和数据管理专家的调查确定，并通过专家反馈的实际情况进行验证。该框架指导AI开发人员、数据科学家和决策者在AI开发和部署中实施伦理实践。

    Integrating ethical practices into the AI development process for artificial intelligence (AI) is essential to ensure safe, fair, and responsible operation. AI ethics involves applying ethical principles to the entire life cycle of AI systems. This is essential to mitigate potential risks and harms associated with AI, such as algorithm biases. To achieve this goal, responsible design patterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee ethical and fair outcomes. In this paper, we propose a comprehensive framework incorporating RDPs into ML pipelines to mitigate risks and ensure the ethical development of AI systems. Our framework comprises new responsible AI design patterns for ML pipelines identified through a survey of AI ethics and data management experts and validated through real-world scenarios with expert feedback. The framework guides AI developers, data scientists, and policy-makers to implement ethical practices in AI development and deploy responsibl
    
[^116]: 使用Transformer预测术后肾功能和肺功能并发症

    Prediction of Post-Operative Renal and Pulmonary Complications Using Transformers. (arXiv:2306.00698v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.00698](http://arxiv.org/abs/2306.00698)

    本文研究使用基于Transformer的模型来预测术后并发症，包括肾衰、肺部问题和住院死亡，并且表现很好。

    

    术后并发症在医疗行业中是一个重大挑战，导致医疗费用上升和住院时间延长，少数情况下甚至会导致患者死亡。为了改善患者的预后和降低医疗费用，医疗服务提供商依赖于各种围手术期风险评分来指导临床决策和优先处理。近年来，机器学习技术在预测术后并发症和死亡方面显示出了希望，深度学习模型在医疗应用中取得了显着的成功。然而，对于术中麻醉管理数据应用深度学习模型的研究仍然很有限。在本文中，我们评估了基于Transformer的模型在预测术后急性肾衰竭、术后肺部并发症和术后住院死亡方面的性能。我们将我们的方法与最先进的表格数据预测模型进行比较，包括Gradient Boosting和LightGBM等。

    Postoperative complications pose a significant challenge in the healthcare industry, resulting in elevated healthcare expenses and prolonged hospital stays, and in rare instances, patient mortality. To improve patient outcomes and reduce healthcare costs, healthcare providers rely on various perioperative risk scores to guide clinical decisions and prioritize care. In recent years, machine learning techniques have shown promise in predicting postoperative complications and fatality, with deep learning models achieving remarkable success in healthcare applications. However, research on the application of deep learning models to intra-operative anesthesia management data is limited. In this paper, we evaluate the performance of transformer-based models in predicting postoperative acute renal failure, postoperative pulmonary complications, and postoperative in-hospital mortality. We compare our method's performance with state-of-the-art tabular data prediction models, including gradient b
    
[^117]: 使预训练模型具有可逆性：从参数到内存高效的微调

    Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning. (arXiv:2306.00477v1 [cs.CL])

    [http://arxiv.org/abs/2306.00477](http://arxiv.org/abs/2306.00477)

    本研究尝试实现在预训练语言模型中运用可逆模型实现高效的微调，并发现在初始化微调时保留PLM的起点非常重要。

    

    预训练语言模型（PLM）的参数高效微调已经成为一种非常成功的方法，只需训练少量参数而不会降低性能，并随着PLM越来越大而成为事实上的学习范式。然而，现有的PEFT方法不具备内存效率，因为它们仍需要存储大部分中间激活值以便计算梯度，类似于微调。一个减少激活内存的有效方法是应用可逆模型，这样中间激活值就无需缓存，可以重新计算。然而，将PLM修改为它的可逆变体并进行PEFT并不是一件容易的事，因为可逆模型具有与当前发布的PLM不同的体系结构。本文首先调查现有PEFT方法成功的关键因素，认识到在初始化PEFT时保留PLM的起点是至关重要的。

    Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) has emerged as a highly successful approach, with training only a small number of parameters without sacrificing performance and becoming the de-facto learning paradigm with the increasing size of PLMs. However, existing PEFT methods are not memory-efficient, because they still require caching most of the intermediate activations for the gradient calculation, akin to fine-tuning. One effective way to reduce the activation memory is to apply a reversible model, so the intermediate activations are not necessary to be cached and can be recomputed. Nevertheless, modifying a PLM to its reversible variant with PEFT is not straightforward, since the reversible model has a distinct architecture from the currently released PLMs. In this paper, we first investigate what is a key factor for the success of existing PEFT methods, and realize that it's essential to preserve the PLM's starting point when initializing a PEFT 
    
[^118]: 学习高斯混合表示用于张量时间序列预测

    Learning Gaussian Mixture Representations for Tensor Time Series Forecasting. (arXiv:2306.00390v1 [cs.LG])

    [http://arxiv.org/abs/2306.00390](http://arxiv.org/abs/2306.00390)

    本文提出了一种新的张量时间序列预测框架GMRL，它可以单独模拟时间、位置和源变量中所暗示的每个异构性组件，相比于最先进的基准方法，本文的方法显示出了优越性。

    

    张量时间序列（TTS）数据是高维空间中一维时间序列的一般化，是现实场景中万能的存在，特别是在涉及多源时空数据的监测系统中（例如交通需求和空气污染物）。与建模时间序列或多元时间序列相比，在最近几年已经受到广泛关注并取得了巨大进展的情况下，张量时间序列付出的努力较少。由于其高维和复杂的内部结构，正确处理张量时间序列是一个更具挑战性的任务。在本文中，我们开发了一种新的TTS预测框架，该框架旨在单独模拟时间、位置和源变量中所暗示的每个异构性组件。我们将此框架命名为GMRL，即高斯混合表示学习。在两个实际TTS数据集上的实验结果验证了我们的方法相对于最先进的基准方法的优越性。

    Tensor time series (TTS) data, a generalization of one-dimensional time series on a high-dimensional space, is ubiquitous in real-world scenarios, especially in monitoring systems involving multi-source spatio-temporal data (e.g., transportation demands and air pollutants). Compared to modeling time series or multivariate time series, which has received much attention and achieved tremendous progress in recent years, tensor time series has been paid less effort. Properly coping with the tensor time series is a much more challenging task, due to its high-dimensional and complex inner structure. In this paper, we develop a novel TTS forecasting framework, which seeks to individually model each heterogeneity component implied in the time, the location, and the source variables. We name this framework as GMRL, short for Gaussian Mixture Representation Learning. Experiment results on two real-world TTS datasets verify the superiority of our approach compared with the state-of-the-art baseli
    
[^119]: CapText: 基于大型语言模型的图像内容和描述生成字幕

    CapText: Large Language Model-based Caption Generation From Image Context and Description. (arXiv:2306.00301v1 [cs.LG])

    [http://arxiv.org/abs/2306.00301](http://arxiv.org/abs/2306.00301)

    研究提出了一种基于大型语言模型的图像字幕生成方法，从文本描述和上下文中生成字幕，而不直接处理图像。在CIDEr指标上，优于当前最先进的图像文本对齐模型。

    

    尽管深度学习模型在图像到文本数据集上表现良好，但在实践中难以用于图像字幕生成，因为传统的图片字幕往往是与图像相关的，并且提供有关图像的补充信息，而模型往往生成描述图像视觉特征的“描述”。在字幕生成方面的研究已探索了使用模型在提供对应的描述或上下文信息的情况下生成字幕的方法。我们提出并评估了一种新的方法，该方法利用现有的大型语言模型从文本描述和上下文中生成字幕，而不直接处理图像。我们证明，在微调后，我们的方法在 CIDEr 指标上胜过了当前最先进的图像文本对齐模型，如 OSCAR-VinVL。

    While deep-learning models have been shown to perform well on image-to-text datasets, it is difficult to use them in practice for captioning images. This is because \textit{captions} traditionally tend to be context-dependent and offer complementary information about an image, while models tend to produce \textit{descriptions} that describe the visual features of the image. Prior research in caption generation has explored the use of models that generate captions when provided with the images alongside their respective descriptions or contexts. We propose and evaluate a new approach, which leverages existing large language models to generate captions from textual descriptions and context alone, without ever processing the image directly. We demonstrate that after fine-tuning, our approach outperforms current state-of-the-art image-text alignment models like OSCAR-VinVL on this task on the CIDEr metric.
    
[^120]: Mixup在寻找最佳决策边界中的可证实益处

    Provable Benefit of Mixup for Finding Optimal Decision Boundaries. (arXiv:2306.00267v1 [cs.LG])

    [http://arxiv.org/abs/2306.00267](http://arxiv.org/abs/2306.00267)

    本研究证明了使用Mixup训练具有可证实的益处，可以显著降低在更可分离数据分布中寻找最佳决策边界的样本复杂度。

    

    本文研究了像Mixup这样的成对数据增强技术如何影响在二元线性分类问题中寻找最佳决策边界的样本复杂度。针对一类具有可分离常数$\kappa$的数据分布，我们分析了训练损失最优分类器与测试准确率最优分类器（即贝叶斯最优分类器）之间的对齐程度。对于没有增强的普通训练，我们发现了一种有趣的现象，称为可分离性的诅咒。随着我们增加$\kappa$使数据分布更加可分离，普通训练的样本复杂度会在$\kappa$中呈指数增长。也许更令人惊讶的是，对于更可分离的数据分布而言，寻找最佳决策边界的任务变得更加困难。针对Mixup训练，我们展示了Mixup减轻了这个问题，通过显著降低样本复杂度。为此，我们开发了适用于Mixup考虑的$n^2$成对增强数据点的新的集中结果。我们的结果提供了关于Mixup的泛化益处的可证保证，并提供了理解Mixup为什么在实践中表现良好的见解。

    We investigate how pair-wise data augmentation techniques like Mixup affect the sample complexity of finding optimal decision boundaries in a binary linear classification problem. For a family of data distributions with a separability constant $\kappa$, we analyze how well the optimal classifier in terms of training loss aligns with the optimal one in test accuracy (i.e., Bayes optimal classifier). For vanilla training without augmentation, we uncover an interesting phenomenon named the curse of separability. As we increase $\kappa$ to make the data distribution more separable, the sample complexity of vanilla training increases exponentially in $\kappa$; perhaps surprisingly, the task of finding optimal decision boundaries becomes harder for more separable distributions. For Mixup training, we show that Mixup mitigates this problem by significantly reducing the sample complexity. To this end, we develop new concentration results applicable to $n^2$ pair-wise augmented data points cons
    
[^121]: MERT:带有大规模自监督训练的声学音乐理解模型

    MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training. (arXiv:2306.00107v1 [cs.SD])

    [http://arxiv.org/abs/2306.00107](http://arxiv.org/abs/2306.00107)

    提出了一个带有大规模自监督训练的音乐理解模型MERT，利用了教师模型并采用了一种优于传统的语音和音频方法的组合方式。

    

    自监督学习（SSL）最近在视觉、文本和语音领域中已被证明是训练通用模型的一种很有前景的范例，对于跨越音乐领域的应用，尤其是对于调性和音高这样的特殊音乐知识的建模颇具挑战性。为了解决这一问题，我们提出了一个基于大规模自监督训练的声学音乐理解模型，即MERT。在我们的探索中，我们确定了更优秀的教师模型组合，这种组合方法在性能方面优于传统的语音和音频方法。

    Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is primarily due to the distinctive challenges associated with modelling musical knowledge, particularly its tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified a superior combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a m
    
[^122]: 发现新的可解释保守律作为稀疏不变量

    Discovering New Interpretable Conservation Laws as Sparse Invariants. (arXiv:2305.19525v1 [math.DS])

    [http://arxiv.org/abs/2305.19525](http://arxiv.org/abs/2305.19525)

    这篇论文介绍了一种名为Sparse Invariant Detector（SID）的算法，它能够自动发现微分方程中的保守律。该算法可以重新发现已知的保守律，甚至发现新的保守律，并且已发现的保守律具有稳健性和可解释性。

    

    发现给定动力系统的保守律是重要但具有挑战性的任务。在理论设置（已知微分方程和基函数）中，我们提出了Sparse Invariant Detector（SID），这是一种从微分方程中自动发现保守律的算法。其算法简单性确保了已发现保守数量的稳健性和可解释性。我们展示了SID能够在各种系统中重新发现已知保守律，甚至发现新的保守律。在流体力学和大气化学的两个例子中，SID分别发现了14个和3个守恒量，而这些领域专家先前只知道12个和2个。

    Discovering conservation laws for a given dynamical system is important but challenging. In a theorist setup (differential equations and basis functions are both known), we propose the Sparse Invariant Detector (SID), an algorithm that auto-discovers conservation laws from differential equations. Its algorithmic simplicity allows robustness and interpretability of the discovered conserved quantities. We show that SID is able to rediscover known and even discover new conservation laws in a variety of systems. For two examples in fluid mechanics and atmospheric chemistry, SID discovers 14 and 3 conserved quantities, respectively, where only 12 and 2 were previously known to domain experts.
    
[^123]: 标准比评分更重要：面向多准则推荐的标准偏好感知轻量图卷积网络

    Criteria Tell You More than Ratings: Criteria Preference-Aware Light Graph Convolution for Effective Multi-Criteria Recommendation. (arXiv:2305.18885v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2305.18885](http://arxiv.org/abs/2305.18885)

    本文提出了一种面向多准则推荐的标准偏好感知轻量图卷积网络，该方法结合了MC扩展图，可以准确地捕捉用户的标准偏好，并进一步将用户对各个标准的偏好合并到最终的推荐列表中。

    

    多准则推荐系统现在在广泛的电子商务领域中利用多准则 (MC) 评分信息，而深度学习中的图神经网络 (GNN) 已经被广泛应用于各种推荐系统的开发中。在这种情况下，本文首次尝试使用GNN辅助设计MC推荐系统。具体而言，我们提出了一种新颖的标准偏好感知轻量图卷积方法(CPA-LGC),可以准确捕捉用户的标准偏好以及复杂高阶连接中的协作信号。本文在MC扩展图上构建了一个能够将用户-物品MC评分转换为扩展二分图的MC扩展图，再进一步将标准重要性编码到图卷积过程中，并引入了一种新的标准偏好感知聚合方法来将用户对不同标准的偏好合并到最终的推荐列表中。

    The multi-criteria (MC) recommender system, which leverages MC rating information in a wide range of e-commerce areas, is ubiquitous nowadays. Surprisingly, although graph neural networks (GNNs) have been widely applied to develop various recommender systems due to GNN's high expressive capability in learning graph representations, it has been still unexplored how to design MC recommender systems with GNNs. In light of this, we make the first attempt towards designing a GNN-aided MC recommender system. Specifically, rather than straightforwardly adopting existing GNN-based recommendation methods, we devise a novel criteria preference-aware light graph convolution CPA-LGC method, which is capable of precisely capturing the criteria preference of users as well as the collaborative signal in complex high-order connectivities. To this end, we first construct an MC expansion graph that transforms user--item MC ratings into an expanded bipartite graph to potentially learn from the collaborat
    
[^124]: PlaNeRF：SVD无监督三维平面正则化用于NeRF大规模场景重建。

    PlaNeRF: SVD Unsupervised 3D Plane Regularization for NeRF Large-Scale Scene Reconstruction. (arXiv:2305.16914v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.16914](http://arxiv.org/abs/2305.16914)

    本文提出了一种利用SVD无监督三维平面正则化的方法，仅使用RGB图像和语义地图即可改善NeRF的三维结构，有效解决了训练视图的过拟合导致低纹理区域的几何重建不佳的问题。

    

    神经辐射场（NeRF）利用2D图像和相机姿态进行3D场景重建以进行新颖视图合成。尽管NeRF能产生逼真的结果，但它经常遭受过拟合于训练视图的困扰，导致几何重建不佳，尤其是在低纹理区域。这种限制限制了许多需要准确几何形态的重要应用，例如外推NVS，高清映射和场景编辑。为了解决这个问题，我们提出了一种新的方法，仅使用RGB图像和语义地图即可改善NeRF的三维结构。我们的方法引入了基于奇异值分解（SVD）的新颖平面正则化，不依赖于任何几何先验知识。此外，我们在我们的损失设计中利用结构相似性指数测量（SSIM）来正确初始化NeRF的体积表示。定量和定性的结果表明，我们的方法优于流行的正则化方法以实现准确的几何重建。

    Neural Radiance Fields (NeRF) enable 3D scene reconstruction from 2D images and camera poses for Novel View Synthesis (NVS). Although NeRF can produce photorealistic results, it often suffers from overfitting to training views, leading to poor geometry reconstruction, especially in low-texture areas. This limitation restricts many important applications which require accurate geometry, such as extrapolated NVS, HD mapping and scene editing. To address this limitation, we propose a new method to improve NeRF's 3D structure using only RGB images and semantic maps. Our approach introduces a novel plane regularization based on Singular Value Decomposition (SVD), that does not rely on any geometric prior. In addition, we leverage the Structural Similarity Index Measure (SSIM) in our loss design to properly initialize the volumetric representation of NeRF. Quantitative and qualitative results show that our method outperforms popular regularization approaches in accurate geometry reconstructi
    
[^125]: 线性神经网络层促进学习单指数和多指数模型

    Linear Neural Network Layers Promote Learning Single- and Multiple-Index Models. (arXiv:2305.15598v1 [cs.LG])

    [http://arxiv.org/abs/2305.15598](http://arxiv.org/abs/2305.15598)

    本研究探究了过度参数化的深度神经网络的偏见，发现在ReLU网络中添加线性层有助于逼近具有低秩线性算子和低表示成本函数组成的函数，从而得到一个与低维子空间垂直方向近乎恒定的插值函数。

    

    本文探究了深度大于两层的过度参数化神经网络的隐含偏见。我们的框架考虑了一类深度不同但容量相同的网络，它们具有不同的显式定义的表示成本。神经网络架构诱导的函数的表示成本是网络表示该函数所需的平方权重之和的最小值；它反映了与该架构相关的函数空间偏差。结果表明，将线性层添加到ReLU网络会产生一个表示成本，这有利于使用两层网络来逼近由低秩线性算子和具有低表示成本的函数组成的函数。具体来说，使用神经网络以最小的表示成本拟合训练数据会得到一个与低维子空间垂直方向近乎恒定的插值函数。

    This paper explores the implicit bias of overparameterized neural networks of depth greater than two layers. Our framework considers a family of networks of varying depths that all have the same capacity but different implicitly defined representation costs. The representation cost of a function induced by a neural network architecture is the minimum sum of squared weights needed for the network to represent the function; it reflects the function space bias associated with the architecture. Our results show that adding linear layers to a ReLU network yields a representation cost that favors functions that can be approximated by a low-rank linear operator composed with a function with low representation cost using a two-layer network. Specifically, using a neural network to fit training data with minimum representation cost yields an interpolating function that is nearly constant in directions orthogonal to a low-dimensional subspace. This means that the learned network will approximate
    
[^126]: 任意缺失模式下的无分布矩阵预测

    Distribution-Free Matrix Prediction Under Arbitrary Missing Pattern. (arXiv:2305.11640v1 [cs.LG])

    [http://arxiv.org/abs/2305.11640](http://arxiv.org/abs/2305.11640)

    本文提出了两种实用算法，能够在任意丢失模式下有效地保证覆盖率的有效性，并量化了缺失对预测精度的影响。

    

    本文研究了在行/列可交换矩阵中预测缺失条目的问题。虽然矩阵设置提出了新颖和独特的挑战，但是在这个有趣的主题上存在很少的工作。我们精细地定义了问题，将其与密切相关的问题区分开来，并严格划分了可达成和不可能的目标的边界。然后我们提出了两种实用算法。第一种方法提供了全面的预测的快速仿真，而第二种方法利用算法稳定性技术加速计算。这两种方法计算效率高，能够在任意丢失模式下有效地保证覆盖率的有效性。此外，我们量化了缺失对预测精度的影响，并建立了基本的极限结果。来自合成和真实数据集的经验证据证实了我们提出的方法的卓越性能。

    This paper studies the open problem of conformalized entry prediction in a row/column-exchangeable matrix. The matrix setting presents novel and unique challenges, but there exists little work on this interesting topic. We meticulously define the problem, differentiate it from closely related problems, and rigorously delineate the boundary between achievable and impossible goals. We then propose two practical algorithms. The first method provides a fast emulation of the full conformal prediction, while the second method leverages the technique of algorithmic stability for acceleration. Both methods are computationally efficient and can effectively safeguard coverage validity in presence of arbitrary missing pattern. Further, we quantify the impact of missingness on prediction accuracy and establish fundamental limit results. Empirical evidence from synthetic and real-world data sets corroborates the superior performance of our proposed methods.
    
[^127]: 从随机搜索到度量测度空间中的赌博学习

    From Random Search to Bandit Learning in Metric Measure Spaces. (arXiv:2305.11509v1 [cs.LG])

    [http://arxiv.org/abs/2305.11509](http://arxiv.org/abs/2305.11509)

    本文介绍了随机搜索及其性能，引入了“散射维度”的概念，描述了底层函数的状态，量化了随机搜索的性能，并证明了在无噪声和有界噪声情况下的输出分别以一定概率收敛到最优值。

    

    随机搜索是超参数优化中最常用的方法之一，对于深度学习模型的成功至关重要。尽管其性能令人惊叹，但很少有非启发式的理论用于描述其工作机制。本文给出了关于随机搜索的理论解释。我们引入了“散射维度”的概念，描述了底层函数的状态，并量化了随机搜索的性能。我们表明，当环境没有噪声时，随机搜索的输出以概率收敛到最优值，其速率为$ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \right)^{ \frac{1}{d_s} } \right) $，其中$ d_s \ge 0 $是底层函数的散射维度。当观察到的函数值受到有界的独立同分布噪声影响时，随机搜索的输出以概率收敛到最优值，速率为$ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \right)^{ \frac{2}{2+d_s} } \right) $。

    Random Search is one of the most widely-used method for Hyperparameter Optimization, and is critical to the success of deep learning models. Despite its astonishing performance, little non-heuristic theory has been developed to describe the underlying working mechanism. This paper gives a theoretical accounting of Random Search. We introduce the concept of \emph{scattering dimension} that describes the landscape of the underlying function, and quantifies the performance of random search. We show that, when the environment is noise-free, the output of random search converges to the optimal value in probability at rate $ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \right)^{ \frac{1}{d_s} } \right) $, where $ d_s \ge 0 $ is the scattering dimension of the underlying function. When the observed function values are corrupted by bounded $iid$ noise, the output of random search converges to the optimal value in probability at rate $ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \rig
    
[^128]: 受物理启发的方法理解高斯过程

    Physics Inspired Approaches Towards Understanding Gaussian Processes. (arXiv:2305.10748v1 [cs.LG])

    [http://arxiv.org/abs/2305.10748](http://arxiv.org/abs/2305.10748)

    本文利用物理学方法分析了高斯过程模型的损失景观，提出了考虑更广泛的ν使得性能更佳的优化方法，同时提供了一种用于评估GP集成效果的方法和基于损失领域的物理属性的投票方法。

    

    通过内核可以将先验有关潜在函数的信念纳入高斯过程(GP)中以形成归纳偏置，但除了内核选择外，GP模型的决策过程仍然很难理解。本文利用物理学方法对GP模型的损失景观进行了分析，演示了Matern内核的ν连续性，并概述了梯度场关键点的灾变理论方面。通过将ν直接包含在Matern内核的超参数优化中，我们发现，尽管在文献中ν的典型值增加了计算速度，但其在性能方面远非最佳。我们还提供了一种事先评估GP集合效果的方法，并讨论了基于损失景观物理属性的各种投票方法。这些方法的实用性在多种合成和真实数据集上得到了证明。我们的发现提供了对GP模型决策过程的深入理解，并为超参数优化和模型选择提供了新的洞察。

    Prior beliefs about the latent function to shape inductive biases can be incorporated into a Gaussian Process (GP) via the kernel. However, beyond kernel choices, the decision-making process of GP models remains poorly understood. In this work, we contribute an analysis of the loss landscape for GP models using methods from physics. We demonstrate $\nu$-continuity for Matern kernels and outline aspects of catastrophe theory at critical points in the loss landscape. By directly including $\nu$ in the hyperparameter optimisation for Matern kernels, we find that typical values of $\nu$ are far from optimal in terms of performance, yet prevail in the literature due to the increased computational speed. We also provide an a priori method for evaluating the effect of GP ensembles and discuss various voting approaches based on physical properties of the loss landscape. The utility of these approaches is demonstrated for various synthetic and real datasets. Our findings provide an enhanced und
    
[^129]: 架起桥梁：通过后处理技术增强合成数据的实用性

    Bridging the Gap: Enhancing the Utility of Synthetic Data via Post-Processing Techniques. (arXiv:2305.10118v1 [cs.CV])

    [http://arxiv.org/abs/2305.10118](http://arxiv.org/abs/2305.10118)

    本文介绍了一种利用生成对抗网络生成合成数据集，并通过三种新颖的后处理技术改进合成数据集质量和多样性的方法。作者称其为Gap Filler (GaFi)流程并在真实图像上进行评估。

    

    获取和注释用于训练深度学习模型的合适数据集是具有挑战性的。生成模型已经成为一种有前途的解决方案，可生成替代或增强现实世界数据的合成数据集。尽管如此，合成数据的有效性受到其不能完全捕捉现实世界数据的复杂性和多样性的限制。为了解决这个问题，我们探索使用生成对抗网络生成用于训练分类器的合成数据集，随后在真实图像上进行评估。为了改进合成数据集的质量和多样性，我们提出了三种新颖的后处理技术：动态样本过滤，动态数据集回收和扩展技巧。此外，我们引入了一种名为“ Gap Filler (GaFi)”的流程，在最佳和协调的方式下应用这些技术，以最大程度地提高分类的准确性。

    Acquiring and annotating suitable datasets for training deep learning models is challenging. This often results in tedious and time-consuming efforts that can hinder research progress. However, generative models have emerged as a promising solution for generating synthetic datasets that can replace or augment real-world data. Despite this, the effectiveness of synthetic data is limited by their inability to fully capture the complexity and diversity of real-world data. To address this issue, we explore the use of Generative Adversarial Networks to generate synthetic datasets for training classifiers that are subsequently evaluated on real-world images. To improve the quality and diversity of the synthetic dataset, we propose three novel post-processing techniques: Dynamic Sample Filtering, Dynamic Dataset Recycle, and Expansion Trick. In addition, we introduce a pipeline called Gap Filler (GaFi), which applies these techniques in an optimal and coordinated manner to maximise classifica
    
[^130]: 冷启动问题：无监督的类别发现方法。

    Cold PAWS: Unsupervised class discovery and the cold-start problem. (arXiv:2305.10071v1 [cs.CV])

    [http://arxiv.org/abs/2305.10071](http://arxiv.org/abs/2305.10071)

    本文提出了一种新方法，通过结合自我监督、聚类和流形学习技术，解决冷启动或无监督选择标记问题，并在多个公共数据集上进行了测试，获得了更好的性能。

    

    在许多机器学习应用中，标记数据集常常是一项艰苦且耗时的任务。虽然研究表明半监督学习技术可以在计算机视觉领域中使用非常少的标签实现高准确性，但很少有人关注如何选择数据集中的图像进行标记。本文提出了一种基于自监督学习、聚类和流形学习技术的新方法，以解决首次选择信息图像子集进行标记的挑战，即冷启动或无监督选择标记问题。我们使用几个公共数据集（包括CIFAR10、Imagenette、DeepWeeds和EuroSAT）测试我们的方法，并观察到当使用我们的标签选择策略时，与随机抽样相比，在监督和半监督学习策略均表现出更好的性能。我们还在d方面获得了更优秀的性能

    In many machine learning applications, labeling datasets can be an arduous and time-consuming task. Although research has shown that semi-supervised learning techniques can achieve high accuracy with very few labels within the field of computer vision, little attention has been given to how images within a dataset should be selected for labeling. In this paper, we propose a novel approach based on well-established self-supervised learning, clustering, and manifold learning techniques that address this challenge of selecting an informative image subset to label in the first instance, which is known as the cold-start or unsupervised selective labelling problem. We test our approach using several publicly available datasets, namely CIFAR10, Imagenette, DeepWeeds, and EuroSAT, and observe improved performance with both supervised and semi-supervised learning strategies when our label selection strategy is used, in comparison to random sampling. We also obtain superior performance for the d
    
[^131]: 见证就是信仰：脑启发模块化训练促进机理诠释

    Seeing is Believing: Brain-Inspired Modular Training for Mechanistic Interpretability. (arXiv:2305.08746v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2305.08746](http://arxiv.org/abs/2305.08746)

    BIMT方法使得神经网络更加模块化和可诠释，并且能够直接展示模块化结构，为许多简单任务提供了有用的信息，并可以补充当前的机理解释策略。

    

    我们提出了一种名为脑启发模块化训练（Brain-Inspired Modular Training, BIMT）的方法，旨在使神经网络更加模块化和可诠释。BIMT从大脑受启发，将神经元嵌入到几何空间中，并通过成本与神经元连接长度成正比的方式增强损失函数。我们证明了BIMT可以为许多简单任务发现有用的模块化神经网络，揭示了符号公式中的组合结构、可解释的决策边界和分类特征，以及算法数据集中的数学结构。直接眼睛看到模块的能力可以补充当前的机理解释策略，例如探针，干预或凝视所有权重。

    We introduce Brain-Inspired Modular Training (BIMT), a method for making neural networks more modular and interpretable. Inspired by brains, BIMT embeds neurons in a geometric space and augments the loss function with a cost proportional to the length of each neuron connection. We demonstrate that BIMT discovers useful modular neural networks for many simple tasks, revealing compositional structures in symbolic formulas, interpretable decision boundaries and features for classification, and mathematical structure in algorithmic datasets. The ability to directly see modules with the naked eye can complement current mechanistic interpretability strategies such as probes, interventions or staring at all weights.
    
[^132]: 基于随机池化的可证明多实例深度AUC最大化方法

    Provable Multi-instance Deep AUC Maximization with Stochastic Pooling. (arXiv:2305.08040v1 [cs.LG])

    [http://arxiv.org/abs/2305.08040](http://arxiv.org/abs/2305.08040)

    本文提出了在多实例学习中使用深度AUC最大化（DAM）的方法，并根据包含大量实例的情况下训练的计算挑战，提出了一种基于方差减少的随机池化方法，使得只需对每个包进行少量采样即可计算MIDAM模型，提高了效率和准确性。

    

    本文提出了一种深度AUC最大化（DAM）的新型应用，用于多实例学习（MIL），其中将单个类标签分配给一组实例（例如，患者的多个CT扫描的多个2D切片）。我们在DAM的背景下解决了MIL中被忽略但非常重要的计算挑战，即包大小过大，无法在反向传播时加载到GPU内存中，这是MIL标准池化方法所必需的。为了解决这个问题，我们提出了一种基于方差减少的随机池化方法，这种方法可以将关于汇聚预测的损失函数构造为多级组合函数。通过综合随机组合优化和非凸极小最大优化技术，我们提出了一种统一且可证明的多实例DAM（MIDAM）算法，其使用随机平滑最大池化或随机注意力池化，仅对每个包对应的实例进行少量采样来计算 sto。

    This paper considers a novel application of deep AUC maximization (DAM) for multi-instance learning (MIL), in which a single class label is assigned to a bag of instances (e.g., multiple 2D slices of a CT scan for a patient). We address a neglected yet non-negligible computational challenge of MIL in the context of DAM, i.e., bag size is too large to be loaded into {GPU} memory for backpropagation, which is required by the standard pooling methods of MIL. To tackle this challenge, we propose variance-reduced stochastic pooling methods in the spirit of stochastic optimization by formulating the loss function over the pooled prediction as a multi-level compositional function. By synthesizing techniques from stochastic compositional optimization and non-convex min-max optimization, we propose a unified and provable muli-instance DAM (MIDAM) algorithm with stochastic smoothed-max pooling or stochastic attention-based pooling, which only samples a few instances for each bag to compute a sto
    
[^133]: RHINO：通过匈牙利匹配实现动态降噪的旋转目标检测的旋转DETR

    RHINO: Rotated DETR with Dynamic Denoising via Hungarian Matching for Oriented Object Detection. (arXiv:2305.07598v1 [cs.CV])

    [http://arxiv.org/abs/2305.07598](http://arxiv.org/abs/2305.07598)

    本文提出了一种面向定向目标检测的DINO基线模型RHINO。并通过匈牙利匹配和查询对齐的方式实现动态降噪，解决了重复预测的问题，从而在公共基准测试中达到最先进的性能水平。

    

    随着DINO的发布，一种DETR的变体，检测变压器正在通过其端到端设计和可扩展性在目标检测基准中刷新记录。然而，虽然预计从其端到端架构中获得更多的好处，如消除NMS和与锚相关的成本，但尚未彻底研究DETR在定向目标检测方面的扩展。本文提出了首个面向定向目标检测的DINO基线。我们发现，直接使用DETR进行定向目标检测并不能保证不重复预测，并提出了一种简单的成本来减轻这种情况。此外，我们介绍了一种新的去噪策略，该策略使用匈牙利匹配来过滤冗余的带噪声的查询，并使用查询对齐来保持Transformer解码器层之间的匹配一致性。我们提出的模型在公共基准测试中优于以前的旋转DETR和其他对手，实现了最先进的性能。

    With the publication of DINO, a variant of the Detection Transformer (DETR), Detection Transformers are breaking the record in the object detection benchmark with the merits of their end-to-end design and scalability. However, the extension of DETR to oriented object detection has not been thoroughly studied although more benefits from its end-to-end architecture are expected such as removing NMS and anchor-related costs. In this paper, we propose a first strong DINO-based baseline for oriented object detection. We found that straightforward employment of DETRs for oriented object detection does not guarantee non-duplicate prediction, and propose a simple cost to mitigate this. Furthermore, we introduce a novel denoising strategy that uses Hungarian matching to filter redundant noised queries and query alignment to preserve matching consistency between Transformer decoder layers. Our proposed model outperforms previous rotated DETRs and other counterparts, achieving state-of-the-art pe
    
[^134]: 基于Fisher信息嵌入的节点和图学习

    Fisher Information Embedding for Node and Graph Learning. (arXiv:2305.07580v1 [stat.ML])

    [http://arxiv.org/abs/2305.07580](http://arxiv.org/abs/2305.07580)

    本文提出了一种新的基于注意力机制的图节点嵌入框架，可以更好地理解基于注意力机制的GNN。

    

    基于注意力机制的图神经网络（GNN），例如图注意力网络（GAT），已成为处理图结构数据和学习节点嵌入的流行神经网络结构。尽管这些模型在经验上取得了成功，但它们依赖于标注数据，且这些模型的理论属性尚未完全理解。本文提出了一种新颖的基于注意力机制的图节点嵌入框架。我们的框架建立在一种多重集合内节点周围子图的分层核之上（例如，邻域），并且每个核利用平滑统计流形的几何来比较多重集合的成对差异，通过将多重集合“映射”到流形上。通过显式计算高斯混合物流形中的节点嵌入，我们的方法引导出了一种新的关注机制进行邻域聚合。我们提供了有关嵌入的泛化和表达能力的理论见解，为更深入理解基于注意力机制的GNN做出了贡献。

    Attention-based graph neural networks (GNNs), such as graph attention networks (GATs), have become popular neural architectures for processing graph-structured data and learning node embeddings. Despite their empirical success, these models rely on labeled data and the theoretical properties of these models have yet to be fully understood. In this work, we propose a novel attention-based node embedding framework for graphs. Our framework builds upon a hierarchical kernel for multisets of subgraphs around nodes (e.g. neighborhoods) and each kernel leverages the geometry of a smooth statistical manifold to compare pairs of multisets, by "projecting" the multisets onto the manifold. By explicitly computing node embeddings with a manifold of Gaussian mixtures, our method leads to a new attention mechanism for neighborhood aggregation. We provide theoretical insights into genralizability and expressivity of our embeddings, contributing to a deeper understanding of attention-based GNNs. We p
    
[^135]: 相关性在公平排序中的作用

    The Role of Relevance in Fair Ranking. (arXiv:2305.05608v1 [cs.IR])

    [http://arxiv.org/abs/2305.05608](http://arxiv.org/abs/2305.05608)

    本文结合社会学、信息检索和机器学习公平性的角度和工具，着眼于相关性在公平排序中的应用和作用，并推导出相关性评分应满足的一组期望标准以实现有意义地指导公平干预措施。

    

    在线平台在机会获取中起着重要作用：基于相关性的排名通过在招聘平台的工作职位、求职者或在线市场的卖家中分配曝光机会来创建和限制选项。为了负责任地这样做，这些社会相关系统采用各种公平措施和干预措施，其中许多措施试图根据价值分配曝光机会。但是，因为这些构造通常不是直接可观察的，所以平台必须使用代理评分，如相关性，并从搜索者的行为信号中推断出它们。然而，关键问题仍然存在，即相关性在高风险的公平排序中是否履行其作为价值评分这样的作用。本文结合社会学、信息检索和机器学习公平性的角度和工具，推导出相关性评分应满足的一组期望标准，以便有意义地指导公平干预措施。

    Online platforms mediate access to opportunity: relevance-based rankings create and constrain options by allocating exposure to job openings and job candidates in hiring platforms, or sellers in a marketplace. In order to do so responsibly, these socially consequential systems employ various fairness measures and interventions, many of which seek to allocate exposure based on worthiness. Because these constructs are typically not directly observable, platforms must instead resort to using proxy scores such as relevance and infer them from behavioral signals such as searcher clicks. Yet, it remains an open question whether relevance fulfills its role as such a worthiness score in high-stakes fair rankings.  In this paper, we combine perspectives and tools from the social sciences, information retrieval, and fairness in machine learning to derive a set of desired criteria that relevance scores should satisfy in order to meaningfully guide fairness interventions. We then empirically show 
    
[^136]: 解释性微调使模型对虚假提示更强韧

    Explanation-based Finetuning Makes Models More Robust to Spurious Cues. (arXiv:2305.04990v1 [cs.CL])

    [http://arxiv.org/abs/2305.04990](http://arxiv.org/abs/2305.04990)

    本文提出一种新型方法——解释性微调，通过让模型在给出答案的同时生成支持该答案的自由文本解释，来减轻LLMs依赖虚假关联，使得模型对虚假提示更加强韧，并具有广泛适用性。

    

    大型语言模型（LLMs）非常强大，有时会学习到标签和与任务无关的特征之间的相关性，导致在分布外数据上泛化能力差。我们提出解释性微调作为减轻LLMs依赖虚假关联的一种新的通用方法。与标准微调只在给定输入的情况下预测答案不同，我们微调模型以生成支持其答案的自由文本解释。为了评估我们的方法，我们在人工构建的训练集上微调模型，该训练集包含不同类型的虚假提示，并在没有这些提示的测试集上进行测试。与标准微调相比，我们的方法在四个分类任务的准确性下降方面使模型极其强韧：ComVE（+1.2），CREAK（+9.1），e-SNLI（+15.4）和SBIC（+6.5）。此外，我们的方法与模型生成的解释同样有效，这意味着我们的方法具有广泛的适用性。

    Large Language Models (LLMs) are so powerful that they sometimes learn correlations between labels and features that are irrelevant to the task, leading to poor generalization on out-of-distribution data. We propose explanation-based finetuning as a novel and general approach to mitigate LLMs' reliance on spurious correlations. Unlike standard finetuning where the model only predicts the answer given the input, we finetune the model to additionally generate a free-text explanation supporting its answer. To evaluate our method, we finetune the model on artificially constructed training sets containing different types of spurious cues, and test it on a test set without these cues. Compared to standard finetuning, our method makes models remarkably more robust against spurious cues in terms of accuracy drop across four classification tasks: ComVE (+1.2), CREAK (+9.1), e-SNLI (+15.4), and SBIC (+6.5). Moreover, our method works equally well with explanations generated by the model, implyin
    
[^137]: 一种基于广义学习系统的证据实时多模态故障诊断方法

    An Evidential Real-Time Multi-Mode Fault Diagnosis Approach Based on Broad Learning System. (arXiv:2305.00169v1 [cs.LG])

    [http://arxiv.org/abs/2305.00169](http://arxiv.org/abs/2305.00169)

    本文提出了一种基于证据推理算法和广义学习系统的实时多模态故障诊断方法，该方法在更新模型参数和计算效率方面具有优势，并且在基准数据集上取得了比现有方法更好的故障诊断性能。

    

    由于多种工况表现出的非高斯、多模态和中心漂移特征，故障诊断是工业界研究的重要领域。目前，数据驱动方法是该领域的主要研究方向，但它们在连续故障分类和故障分类器参数更新方面提出了挑战，尤其在多种操作模式和实时环境中。因此，实现工业系统的实时多模态故障诊断是一个迫切的问题。为了解决这个问题，本文提出了一种新的方法，利用证据推理（ER）算法来融合信息并合并来自不同基分类器的输出。这些基分类器使用广义学习系统（BLS）开发，以提高良好的故障诊断性能。此外，在这种方法中，采用伪标签学习方法来实时更新模型参数。为了证明所提出方法的有效性，我们在基准数据集上进行实验并与现有方法进行比较。结果表明，我们提出的方法在准确性和计算效率方面优于现有方法。

    Fault diagnosis is a crucial area of research in the industry due to diverse operating conditions that exhibit non-Gaussian, multi-mode, and center-drift characteristics. Currently, data-driven approaches are the main focus in the field, but they pose challenges for continuous fault classification and parameter updates of fault classifiers, particularly in multiple operating modes and real-time settings. Therefore, a pressing issue is to achieve real-time multi-mode fault diagnosis for industrial systems. To address this problem, this paper proposes a novel approach that utilizes an evidence reasoning (ER) algorithm to fuse information and merge outputs from different base classifiers. These base classifiers are developed using a broad learning system (BLS) to improve good fault diagnosis performance. Moreover, in this approach, the pseudo-label learning method is employed to update model parameters in real-time. To demonstrate the effectiveness of the proposed approach, we perform exp
    
[^138]: 一种透明化数据表示的方法

    A transparent approach to data representation. (arXiv:2304.14209v1 [cs.LG])

    [http://arxiv.org/abs/2304.14209](http://arxiv.org/abs/2304.14209)

    使用二元属性表示模型对Netflix观众对电影的评分数据集进行数据表示，属性易于解释，且需要较少属性即可达到相同水平的误差。

    

    我们使用二元属性表示（BAR）模型来描述Netflix观众对电影的评分数据集。我们使用离散的二进制位而不是连续的参数对观众进行分类，这使得表示紧凑而透明。这些属性易于解释，我们需要比类似方法少得多的属性才能达到相同水平的误差。我们还利用数据集中电影评分的非均匀分布，在不影响其余电影性能的情况下，选择少量电影进行训练。

    We use a binary attribute representation (BAR) model to describe a data set of Netflix viewers' ratings of movies. We classify the viewers with discrete bits rather than continuous parameters, which makes the representation compact and transparent. The attributes are easy to interpret, and we need far fewer attributes than similar methods do to achieve the same level of error. We also take advantage of the nonuniform distribution of ratings among the movies in the data set to train on a small selection of movies without compromising performance on the rest of the movies.
    
[^139]: 受限通信加性高斯噪声下的多臂赌博机问题研究

    Communication-Constrained Bandits under Additive Gaussian Noise. (arXiv:2304.12680v1 [cs.LG])

    [http://arxiv.org/abs/2304.12680](http://arxiv.org/abs/2304.12680)

    本文研究了在受限通信和加性高斯噪声下的多臂赌博机问题，提出了一个多阶段赌博算法，并给出了信息理论下限。

    

    本文研究了一个分布式随机多臂赌博机,其中客户端根据相应的拉臂奖励提供受限通信反馈给学习者。在我们的设定下,客户端必须编码奖励，使得编码奖励的二阶矩不超过P，并且这个编码奖励会被方差为$\sigma^2$的加性高斯噪声所污染；学习者只能访问这个被污染的奖励。我们在这个设置中导出了任何方案的最小化后悔的信息论下限$\Omega\left(\sqrt{\frac{KT}{\mathtt{SNR} \wedge1}} \right)$，其中 $ \mathtt{SNR} := \frac{P}{\sigma^2}$，$K$和$T$分别是臂数和时间长度。此外，我们提出了一个多阶段赌博算法$\mathtt{UE\text{-}UCB++}$，它可以将这个下限的值加上一个微小的可加性因子。$\mathtt{UE\text{-}UCB++}$在其初始阶段执行均匀探索，然后在后续阶段使用“上置信界”(UCB)算法。我们还展示了数值结果，表明在实际情况下需要这样的通信有效算法。

    We study a distributed stochastic multi-armed bandit where a client supplies the learner with communication-constrained feedback based on the rewards for the corresponding arm pulls. In our setup, the client must encode the rewards such that the second moment of the encoded rewards is no more than $P$, and this encoded reward is further corrupted by additive Gaussian noise of variance $\sigma^2$; the learner only has access to this corrupted reward. For this setting, we derive an information-theoretic lower bound of $\Omega\left(\sqrt{\frac{KT}{\mathtt{SNR} \wedge1}} \right)$ on the minimax regret of any scheme, where $ \mathtt{SNR} := \frac{P}{\sigma^2}$, and $K$ and $T$ are the number of arms and time horizon, respectively. Furthermore, we propose a multi-phase bandit algorithm, $\mathtt{UE\text{-}UCB++}$, which matches this lower bound to a minor additive factor. $\mathtt{UE\text{-}UCB++}$ performs uniform exploration in its initial phases and then utilizes the {\em upper confidence
    
[^140]: 大型语言模型的多方面重复抑制和内容调控

    Multi-aspect Repetition Suppression and Content Moderation of Large Language Models. (arXiv:2304.10611v1 [cs.CL])

    [http://arxiv.org/abs/2304.10611](http://arxiv.org/abs/2304.10611)

    本文介绍了一种使用标记和序列级别的不可能性损失，以及在培训期间的重复惩罚、推理和后处理等多层面方法来抑制大型语言模型中的重复，并避免生成攻击性内容的能力。

    

    自然语言生成在NLP领域是最具影响力的领域之一，近年来由大型语言模型(LLMs)带来的进步得到了人们的关注。作为编写助手应用程序的关键工具，它们通常容易复制或扩展输入中提供的具有攻击性的内容。在低资源数据环境中，它们也可能导致输出重复的问题。本文介绍了一种精确和非精确重复抑制的结合方法，使用标记和序列级别的不可能性损失，培训期间的重复惩罚、推理和后处理。我们进一步探讨了多级不可能性损失的范围，以赋予模型避免从一开始产生攻击性词汇和短语的能力。最后，通过全面的实验，在多个度量标准上证明了我们提出的方法的有效性。

    Natural language generation is one of the most impactful fields in NLP, and recent years have witnessed its evolution brought about by large language models (LLMs). As the key instrument for writing assistance applications, they are generally prone to replicating or extending offensive content provided in the input. In low-resource data regime, they can also lead to repetitive outputs (Holtzman et al., 2019) [1]. Usually, offensive content and repetitions are mitigated with post-hoc methods, including n-gram level blocklists, top-k and nucleus sampling. In this paper, we introduce a combination of exact and non-exact repetition suppression using token and sequence level unlikelihood loss, repetition penalty during training, inference, and post-processing respectively. We further explore multi-level unlikelihood loss to the extent that it endows the model with abilities to avoid generating offensive words and phrases from the beginning. Finally, with comprehensive experiments, we demons
    
[^141]: 通过对比学习实现乳腺X线摄影图像分析的域泛化

    Domain Generalization for Mammographic Image Analysis via Contrastive Learning. (arXiv:2304.10226v1 [cs.CV])

    [http://arxiv.org/abs/2304.10226](http://arxiv.org/abs/2304.10226)

    研究人员开发出一种基于对比学习的域泛化方法，通过自监督学习生成多种风格和视角的特征嵌入，进一步微调骨干网络以提高分类任务性能。

    

    乳腺X线摄影图像分析是医学影像学领域的一个基本问题，近年来，随着深度学习的不断发展，该领域取得了显著的进展。然而，构建深度学习模型需要大量的具有多样性的图像数据，尤其是对于不同厂商的图像风格，这往往需要非常庞大的样本集。因此，为了提高深度学习模型泛化到不同厂商图像的能力，研究者提出了一种基于对比学习的策略。

    Mammographic image analysis is a fundamental problem in the computer-aided diagnosis scheme, which has recently made remarkable progress with the advance of deep learning. However, the construction of a deep learning model requires training data that are large and sufficiently diverse in terms of image style and quality. In particular, the diversity of image style may be majorly attributed to the vendor factor. However, mammogram collection from vendors as many as possible is very expensive and sometimes impractical for laboratory-scale studies. Accordingly, to further augment the generalization capability of deep learning models to various vendors with limited resources, a new contrastive learning scheme is developed. Specifically, the backbone network is firstly trained with a multi-style and multi-view unsupervised self-learning scheme for the embedding of invariant features to various vendor styles. Afterward, the backbone network is then recalibrated to the downstream tasks of mas
    
[^142]: 多元概率预测评估中的可靠性区域研究

    Regions of Reliability in the Evaluation of Multivariate Probabilistic Forecasts. (arXiv:2304.09836v1 [cs.LG])

    [http://arxiv.org/abs/2304.09836](http://arxiv.org/abs/2304.09836)

    本研究通过有限样本和功率分析确定了多元概率时间序列预测评分规则的可靠性区域，并在电力生产问题上评估了结果对真实世界任务的普适性。

    

    在多元概率时间序列预测的评估中，通常使用适当的评分规则进行评估，即对于基准分布期望最小的函数。然而，在非渐进情况下，这一属性不能保证具有良好的区分度。在本文中，我们提供了第一篇系统的有限样本适当评分规则研究，通过功率分析，我们确定了一个分数规则的“可靠性区域”，即它可以可靠地识别预测误差的一组实际条件。我们在一个全面的人造基准测试上进行了分析，该测试专门设计以测试基准分布与预测分布之间的几个关键差异，并通过在电力生产问题上应用来评估我们的结果对真实世界任务的普适性。我们的结果揭示了在多元概率预测的评估中的重大缺陷。

    Multivariate probabilistic time series forecasts are commonly evaluated via proper scoring rules, i.e., functions that are minimal in expectation for the ground-truth distribution. However, this property is not sufficient to guarantee good discrimination in the non-asymptotic regime. In this paper, we provide the first systematic finite-sample study of proper scoring rules for time-series forecasting evaluation. Through a power analysis, we identify the "region of reliability" of a scoring rule, i.e., the set of practical conditions where it can be relied on to identify forecasting errors. We carry out our analysis on a comprehensive synthetic benchmark, specifically designed to test several key discrepancies between ground-truth and forecast distributions, and we gauge the generalizability of our findings to real-world tasks with an application to an electricity production problem. Our results reveal critical shortcomings in the evaluation of multivariate probabilistic forecasts as co
    
[^143]: 双曲线图像文本表示方法

    Hyperbolic Image-Text Representations. (arXiv:2304.09172v1 [cs.CV])

    [http://arxiv.org/abs/2304.09172](http://arxiv.org/abs/2304.09172)

    本文提出了一个使用双曲表示捕捉图像和文本层次结构的对比模型MERU，并证明其在多模态任务上与CLIP相当。

    

    视觉和语言概念自然而然地组织成一个层次结构，其中一个文本概念“狗”包含所有包含狗的图像。尽管直觉上这是正确的，但目前的大规模视觉和语言模型（如CLIP）并没有明确地捕捉到这种层次结构。我们提出了MERU，一个对图像和文本进行双曲表示的对比模型。双曲空间具有嵌入树状数据的合适几何属性，因此MERU可以更好地捕捉图像文本数据的底层层次结构。我们的结果表明，MERU学习到了一个高度可解释的表示空间，同时在图像分类和图像文本检索等多模态任务上与CLIP的性能相当。

    Visual and linguistic concepts naturally organize themselves in a hierarchy, where a textual concept ``dog'' entails all images that contain dogs. Despite being intuitive, current large-scale vision and language models such as CLIP do not explicitly capture such hierarchy. We propose MERU, a contrastive model that yields hyperbolic representations of images and text. Hyperbolic spaces have suitable geometric properties to embed tree-like data, so MERU can better capture the underlying hierarchy in image-text data. Our results show that MERU learns a highly interpretable representation space while being competitive with CLIP's performance on multi-modal tasks like image classification and image-text retrieval.
    
[^144]: 对“通过自监督深度学习进行快速和可扩展的全幻灯片图像搜索”的评论（arXiv: 2304.08297v2 [eess.IV] UPDATED）

    Comments on 'Fast and scalable search of whole-slide images via self-supervised deep learning'. (arXiv:2304.08297v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2304.08297](http://arxiv.org/abs/2304.08297)

    对陈等人发表在《自然—生物医学工程》杂志上的“通过自监督深度学习进行快速和可扩展的全幻灯片图像搜索”一文的评论和关切。

    

    最近，陈等人（Chen2022）在《自然—生物医学工程》杂志上发表了题为“通过自监督深度学习进行快速和可扩展的全幻灯片图像搜索”的文章。该文章作者称其方法为“组织学自监督图像搜索，简称SISH。”我们对SISH表示了关切，因为它是Yottixel的增量修改，使用了MinMax二值化但未引用原始作品，并基于一个误称“自监督图像搜索”的概念。此外，我们还指出了陈等人进行实验和比较时存在的几个问题。

    Chen et al. [Chen2022] recently published the article 'Fast and scalable search of whole-slide images via self-supervised deep learning' in Nature Biomedical Engineering. The authors call their method 'self-supervised image search for histology', short SISH. We express our concerns that SISH is an incremental modification of Yottixel, has used MinMax binarization but does not cite the original works, and is based on a misnomer 'self-supervised image search'. As well, we point to several other concerns regarding experiments and comparisons performed by Chen et al.
    
[^145]: G2T: 基于预训练语言模型和社区检测的主题建模框架

    G2T: A simple but versatile framework for topic modeling based on pretrained language model and community detection. (arXiv:2304.06653v1 [cs.CL])

    [http://arxiv.org/abs/2304.06653](http://arxiv.org/abs/2304.06653)

    G2T是一种基于预训练语言模型和社区检测的主题建模框架，自动评估表明，G2T在多个数据集上均与当前最先进的方法相比表现更好。

    

    先前的研究表明，基于聚类的主题模型能够通过适当的词语筛选方法聚类高质量的句子嵌入，生成比生成式概率主题模型更好的主题。然而，这些方法存在选择合适参数的困难以及不完整的模型忽略单词与主题及主题与文本之间的定量关系的问题。为了解决这些问题，我们提出了一种简洁但有效的主题建模框架，即图主题（G2T）。

    It has been reported that clustering-based topic models, which cluster high-quality sentence embeddings with an appropriate word selection method, can generate better topics than generative probabilistic topic models. However, these approaches suffer from the inability to select appropriate parameters and incomplete models that overlook the quantitative relation between words with topics and topics with text. To solve these issues, we propose graph to topic (G2T), a simple but effective framework for topic modelling. The framework is composed of four modules. First, document representation is acquired using pretrained language models. Second, a semantic graph is constructed according to the similarity between document representations. Third, communities in document semantic graphs are identified, and the relationship between topics and documents is quantified accordingly. Fourth, the word--topic distribution is computed based on a variant of TFIDF. Automatic evaluation suggests that G2
    
[^146]: 模型稀疏化可以简化机器反学习

    Model sparsification can simplify machine unlearning. (arXiv:2304.04934v1 [cs.LG])

    [http://arxiv.org/abs/2304.04934](http://arxiv.org/abs/2304.04934)

    本文提出了一种基于模型稀疏化的机器反学习方案，称为prune first, then unlearn和sparsity-aware unlearning。此方案可以提高近似反学习器的多标准反学习性能，并在不同的场景中表现出一致的效果。

    

    最近的数据管制要求机器反学习（MU）：从模型中移除指定样例的影响。虽然可以通过使用剩余数据从头开始进行模型重新训练来进行精确反学习，但是其计算成本导致了近似但高效的反学习方案的开发。除了数据中心的MU解决方案，我们通过一种新颖的基于模型的视角推进MU：通过权值修剪进行稀疏化。我们的理论和实践结果表明，模型稀疏性可以提高近似反学习器的多标准反学习性能，缩小近似间隙，同时保持高效。有了这个认识，我们制定了两个新的稀疏感知反学习元方案，称为“先修剪，然后反学习”和“稀疏感知反学习”。广泛的实验表明，我们的发现和提议在各种场景下始终有益于MU，包括按类数据擦除、随机数据擦除和后门数据伪造等。

    Recent data regulations necessitate machine unlearning (MU): The removal of the effect of specific examples from the model. While exact unlearning is possible by conducting a model retraining with the remaining data from scratch, its computational cost has led to the development of approximate but efficient unlearning schemes. Beyond data-centric MU solutions, we advance MU through a novel model-based viewpoint: sparsification via weight pruning. Our results in both theory and practice indicate that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. With this insight, we develop two new sparsity-aware unlearning meta-schemes, termed `prune first, then unlearn' and `sparsity-aware unlearning'. Extensive experiments show that our findings and proposals consistently benefit MU in various scenarios, including class-wise data scrubbing, random data scrubbing, and backdoor data forge
    
[^147]: ImageCaptioner$^2$: 针对图像字幕偏差放大评估的图像字幕生成器

    ImageCaptioner$^2$: Image Captioner for Image Captioning Bias Amplification Assessment. (arXiv:2304.04874v1 [cs.CV])

    [http://arxiv.org/abs/2304.04874](http://arxiv.org/abs/2304.04874)

    本文提出了一种新的图像字幕生成器 ImageCaptioner$^2$ ，用于针对图像字幕偏差放大进行评估。

    

    大多数预训练学习系统都会受到偏差的影响，这通常来自数据、模型或两者。衡量和量化偏差及其来源是一项具有挑战性的任务，并在图像字幕生成方面得到了广泛的研究。然而，我们观察到现有评估指标在包括视觉信号方面存在一定不一致性。本文提出了一种新的针对图像字幕生成的偏差评估指标，称为 ImageCaptioner$^2$。与现有方法仅基于生成的字幕评估图像字幕算法不同，ImageCaptioner$^2$在测量偏差时考虑图像。我们还设计了一种公式来作为基于提示的图像字幕生成来测量生成字幕的偏差，而不是使用传统方法。

    Most pre-trained learning systems are known to suffer from bias, which typically emerges from the data, the model, or both. Measuring and quantifying bias and its sources is a challenging task and has been extensively studied in image captioning. Despite the significant effort in this direction, we observed that existing metrics lack consistency in the inclusion of the visual signal. In this paper, we introduce a new bias assessment metric, dubbed $ImageCaptioner^2$, for image captioning. Instead of measuring the absolute bias in the model or the data, $ImageCaptioner^2$ pay more attention to the bias introduced by the model w.r.t the data bias, termed bias amplification. Unlike the existing methods, which only evaluate the image captioning algorithms based on the generated captions only, $ImageCaptioner^2$ incorporates the image while measuring the bias. In addition, we design a formulation for measuring the bias of generated captions as prompt-based image captioning instead of using 
    
[^148]: 深度学习用于多孔介质中的扩散

    Deep learning for diffusion in porous media. (arXiv:2304.02104v1 [physics.comp-ph])

    [http://arxiv.org/abs/2304.02104](http://arxiv.org/abs/2304.02104)

    本文研究使用深度学习来预测多孔介质的基本特性，包括孔隙率和有效扩散系数，并通过构建U-Net架构成功重构了多孔介质的几何结构和浓度分布图。

    

    本文利用卷积神经网络（CNN）预测多孔介质的基本特性，考虑了两种不同的介质类型：一种模拟砂岩，另一种模拟生物组织的细胞外空间衍生系统。采用Lattice Boltzmann方法获得必要的标记数据，并进行有监督的学习。我们区分了两个任务，在第一个任务中，基于系统几何分析的网络预测孔隙率和有效扩散系数，第二个任务中，网络重构了系统的几何结构和浓度分布。在第一个任务中，我们提出了两种CNN模型：C-Net和U-Net的编码器部分。两个网络均添加了自归一化模块。模型的预测结果仅在其训练的数据类型内有合理准确性。在第二个任务中，我们提出了使用ResNet编码器的U-Net架构重构多孔介质的几何结构和浓度分布图。结果表明该架构可以成功重构两种介质类型。

    We adopt convolutional neural networks (CNN) to predict the basic properties of the porous media. Two different media types are considered: one mimics the sandstone, and the other mimics the systems derived from the extracellular space of biological tissues. The Lattice Boltzmann Method is used to obtain the labeled data necessary for performing supervised learning. We distinguish two tasks. In the first, networks based on the analysis of the system's geometry predict porosity and effective diffusion coefficient. In the second, networks reconstruct the system's geometry and concentration map. In the first task, we propose two types of CNN models: the C-Net and the encoder part of the U-Net. Both networks are modified by adding a self-normalization module. The models predict with reasonable accuracy but only within the data type, they are trained on. For instance, the model trained on sandstone-like samples overshoots or undershoots for biological-like samples. In the second task, we pr
    
[^149]: oBERTa: 通过改进初始化、蒸馏和剪枝来提高稀疏迁移学习

    oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes. (arXiv:2303.17612v1 [cs.CL])

    [http://arxiv.org/abs/2303.17612](http://arxiv.org/abs/2303.17612)

    oBERTa是一组易于使用的语言模型，通过改进初始化、蒸馏、剪枝等技术，可以在不需要模型压缩方面的专业知识的情况下提高稀疏迁移学习的效率和准确性。

    

    本文介绍了oBERTa语言模型的范围，它是一组易于使用的语言模型，允许自然语言处理（NLP）从业者在不需要模型压缩方面的专业知识的情况下获得3.8到24.3倍的更快速的模型。oBERTa扩展了现有的剪枝、知识蒸馏和量化工作，并利用冻结的嵌入来改进知识蒸馏，并改进模型初始化，以在广泛的传递任务上提供更高的准确性。在生成oBERTa时，我们探索了高度优化的RoBERTa与BERT在预训练和微调期间剪枝方面的不同之处，并发现它在微调期间不太适合压缩。我们探索了oBERTa在七个具有代表性的NLP任务上的使用，并发现改进的压缩技术使得经过剪枝的oBERTa模型能够匹配BERTBASE的性能，并超过SQUAD V1.1问答数据的Prune OFA Large的性能。

    In this paper, we introduce the range of oBERTa language models, an easy-to-use set of language models, which allows Natural Language Processing (NLP) practitioners to obtain between 3.8 and 24.3 times faster models without expertise in model compression. Specifically, oBERTa extends existing work on pruning, knowledge distillation, and quantization and leverages frozen embeddings to improve knowledge distillation, and improved model initialization to deliver higher accuracy on a a broad range of transfer tasks. In generating oBERTa, we explore how the highly optimized RoBERTa differs from the BERT with respect to pruning during pre-training and fine-tuning and find it less amenable to compression during fine-tuning. We explore the use of oBERTa on a broad seven representative NLP tasks and find that the improved compression techniques allow a pruned oBERTa model to match the performance of BERTBASE and exceed the performance of Prune OFA Large on the SQUAD V1.1 Question Answering data
    
[^150]: 原型样本关系蒸馏：实现无需回放的持续学习

    Prototype-Sample Relation Distillation: Towards Replay-Free Continual Learning. (arXiv:2303.14771v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.14771](http://arxiv.org/abs/2303.14771)

    本文提出了一种无需回放以往数据的持续学习方法，通过共同学习特征表示和类原型来避免灾难性遗忘。

    

    在持续学习中，平衡有效适应和抵御灾难性遗忘是一个核心难题。许多最近表现最佳的方法利用各种形式的先前任务数据来解决灾难性遗忘问题。然而，在许多实际场景中，访问以前的任务数据可能具有限制性，例如当任务数据是敏感的或专有的时候。本文提出了一种综合方法来共同学习特征表示和类原型，同时保持旧类原型及其内在相似性的相关性，从而克服了使用先前任务数据的必要性。

    In Continual learning (CL) balancing effective adaptation while combating catastrophic forgetting is a central challenge. Many of the recent best-performing methods utilize various forms of prior task data, e.g. a replay buffer, to tackle the catastrophic forgetting problem. Having access to previous task data can be restrictive in many real-world scenarios, for example when task data is sensitive or proprietary. To overcome the necessity of using previous tasks' data, in this work, we start with strong representation learning methods that have been shown to be less prone to forgetting. We propose a holistic approach to jointly learn the representation and class prototypes while maintaining the relevance of old class prototypes and their embedded similarities. Specifically, samples are mapped to an embedding space where the representations are learned using a supervised contrastive loss. Class prototypes are evolved continually in the same latent space, enabling learning and prediction
    
[^151]: 无需强化学习的逆强化学习

    Inverse Reinforcement Learning without Reinforcement Learning. (arXiv:2303.14623v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.14623](http://arxiv.org/abs/2303.14623)

    该论文提出了一种新的逆强化学习简化方法，通过利用专家的状态分布来减少强化学习子例程的全局探索部分，实现了指数级的加速，大大提高了样本复杂度和时间复杂度的效果。

    

    逆强化学习 (IRL) 是一种强大的模仿学习技术，旨在学习合乎逻辑的专家演示的奖励函数。然而，传统的IRL方法存在计算上的弱点：它们需要将解决难度高的强化学习（RL）问题作为子例程进行反复求解。这与归约的观点相矛盾：我们已将模仿学习的较易问题归约为反复解决强化学习的更难问题。另一方面的工作证明，访问强策略花费时间的状态分布的侧面信息可以大大降低解决RL问题的样本和计算复杂度。在本研究中，我们首次展示了一种更加明智的模仿学习简化方法，利用专家的状态分布来缓解RL子例程的全局探索部分，理论上提供了指数级的加速。实际上，我们的算法在多个基准任务中在样本复杂度和时间复杂度方面都显著优于现有的IRL方法。

    Inverse Reinforcement Learning (IRL) is a powerful set of techniques for imitation learning that aims to learn a reward function that rationalizes expert demonstrations. Unfortunately, traditional IRL methods suffer from a computational weakness: they require repeatedly solving a hard reinforcement learning (RL) problem as a subroutine. This is counter-intuitive from the viewpoint of reductions: we have reduced the easier problem of imitation learning to repeatedly solving the harder problem of RL. Another thread of work has proved that access to the side-information of the distribution of states where a strong policy spends time can dramatically reduce the sample and computational complexities of solving an RL problem. In this work, we demonstrate for the first time a more informed imitation learning reduction where we utilize the state distribution of the expert to alleviate the global exploration component of the RL subroutine, providing an exponential speedup in theory. In practice
    
[^152]: 什么让数据适合于局部连接神经网络？一种基于量子纠缠的必要且充分条件

    What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement. (arXiv:2303.11249v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.11249](http://arxiv.org/abs/2303.11249)

    本文通过采用量子物理学的理论工具，提出了一种判定数据适合于局部连接神经网络的必要且充分条件，并导出了一种相应的预处理方法。

    

    关于数据分布适用于深度学习的问题是一个基本的开放性问题。本文采用来自量子物理学的理论工具，针对包括卷积神经网络、循环神经网络和局部自注意力模型在内的广泛的局部连接神经网络，解决了这个问题。我们的主要理论结果是，在某些特征的规范划分下，当数据分布接受低量子纠缠时，特定的局部连接神经网络才能够准确地预测该数据分布。作为本结果的实际应用，我们导出了一种预处理方法，以增强数据分布适合局部连接神经网络的性能。在各种数据集上对广泛的模型进行实验，证明了我们的发现。我们希望我们使用量子纠缠将鼓励形式推理的物理工具来进一步采用。

    The question of what makes a data distribution suitable for deep learning is a fundamental open problem. Focusing on locally connected neural networks (a prevalent family of architectures that includes convolutional and recurrent neural networks as well as local self-attention models), we address this problem by adopting theoretical tools from quantum physics. Our main theoretical result states that a certain locally connected neural network is capable of accurate prediction over a data distribution if and only if the data distribution admits low quantum entanglement under certain canonical partitions of features. As a practical application of this result, we derive a preprocessing method for enhancing the suitability of a data distribution to locally connected neural networks. Experiments with widespread models over various datasets demonstrate our findings. We hope that our use of quantum entanglement will encourage further adoption of tools from physics for formally reasoning about 
    
[^153]: 基于图神经粗糙微分方程的交通预测方法

    Graph Neural Rough Differential Equations for Traffic Forecasting. (arXiv:2303.10909v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.10909](http://arxiv.org/abs/2303.10909)

    本文提出一种基于图神经粗糙微分方程的交通预测方法(STG-NRDE)，通过两个NRDE进行时空处理并组合起来构成一个框架。实验结果表明，在6个基准数据集和27个基线模型上表现最佳。

    

    交通预测是机器学习中最受欢迎的时空任务之一。目前广泛使用的方法是将图卷积网络和循环神经网络组合起来进行时空处理。本文提出了一种新的时空图神经粗糙微分方程（STG-NRDE）方法。我们利用神经粗糙微分方程（NRDE）的对数签名变换将时间序列数据转换为较短的特征向量序列，并将其扩展应用于时空处理。我们将两种NRDE设计成一个框架用于交通预测。实验数据集包括6个基准数据集和27个基线模型。实验结果表明，STG-NRDE在所有情况下都表现出最佳准确性，胜过这27个基线模型。

    Traffic forecasting is one of the most popular spatio-temporal tasks in the field of machine learning. A prevalent approach in the field is to combine graph convolutional networks and recurrent neural networks for the spatio-temporal processing. There has been fierce competition and many novel methods have been proposed. In this paper, we present the method of spatio-temporal graph neural rough differential equation (STG-NRDE). Neural rough differential equations (NRDEs) are a breakthrough concept for processing time-series data. Their main concept is to use the log-signature transform to convert a time-series sample into a relatively shorter series of feature vectors. We extend the concept and design two NRDEs: one for the temporal processing and the other for the spatial processing. After that, we combine them into a single framework. We conduct experiments with 6 benchmark datasets and 27 baselines. STG-NRDE shows the best accuracy in all cases, outperforming all those 27 baselines 
    
[^154]: 快速率的最大熵探索方法

    Fast Rates for Maximum Entropy Exploration. (arXiv:2303.08059v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.08059](http://arxiv.org/abs/2303.08059)

    本文解决了强化学习中稀疏奖励下的最大熵探索问题，提出了两种类型的最大熵探索方法，并提高了其样本复杂度。

    

    当智能体在一个未知的、稀疏或没有奖励的环境中操作时，我们解决了强化学习（RL）中探索的挑战。在本文中，我们研究了两种不同类型的最大熵探索问题。第一种类型是回访熵最大化，这在折扣设置中已经由Hazan et al.（2019）考虑过。对于这种类型的探索，我们提出了一种博弈论算法，其样本复杂性为$\widetilde{\mathcal{O}}(H^3S^2A/\varepsilon^2)$，从而改进了现有结果的$\varepsilon$依赖关系，其中$S$是状态数，$A$是行动数，$H$是每个回合的长度，$\varepsilon$是期望的精度。我们研究的第二种熵是轨迹熵。这个目标函数与熵正则化MDPs密切相关，我们提出了一个简单的算法，其样本复杂度为$\widetilde{\mathcal{O}}(\mathrm{poly}(S,A,H)/\varepsilon)$。有趣的是，这是第一个在具有$\mathrm{poly}(S,A,H)$样本复杂度的情况下解决轨迹熵最大化问题的算法。

    We address the challenge of exploration in reinforcement learning (RL) when the agent operates in an unknown environment with sparse or no rewards. In this work, we study the maximum entropy exploration problem of two different types. The first type is visitation entropy maximization previously considered by Hazan et al.(2019) in the discounted setting. For this type of exploration, we propose a game-theoretic algorithm that has $\widetilde{\mathcal{O}}(H^3S^2A/\varepsilon^2)$ sample complexity thus improving the $\varepsilon$-dependence upon existing results, where $S$ is a number of states, $A$ is a number of actions, $H$ is an episode length, and $\varepsilon$ is a desired accuracy. The second type of entropy we study is the trajectory entropy. This objective function is closely related to the entropy-regularized MDPs, and we propose a simple algorithm that has a sample complexity of order $\widetilde{\mathcal{O}}(\mathrm{poly}(S,A,H)/\varepsilon)$. Interestingly, it is the first th
    
[^155]: ODE-Net的变分形式：均场最优控制问题及存在性结果

    Variational formulations of ODE-Net as a mean-field optimal control problem and existence results. (arXiv:2303.05924v2 [math.AP] UPDATED)

    [http://arxiv.org/abs/2303.05924](http://arxiv.org/abs/2303.05924)

    本文探讨了ODE-Net在最小化损失函数的同时约束参数ODE的数学问题，并提出了一种测度论均场最优控制问题的形式化表述，并针对线性神经网络证明了最小化器的存在性结果。

    

    本文对ODE-Net进行了数学分析，它是深度神经网络（DNN）的连续模型。近年来，机器学习研究人员提出了用ODE代替DNN深度结构作为连续极限的想法。这些研究将ODE-Net的"学习"视为最小化由参数ODE约束的"损失"。虽然需要假定该最小化问题的存在，但只有少数研究详细地分析了其存在性。本文将ODE-Net的形式化表述为一种测度论均场最优控制问题，并基于此讨论了最小化器的存在性结果。当描述ODE-Net向量场的神经网络针对可学习参数是线性的时，证明了最小化器的存在性。证明使用测度论形式化和变分法的直接方法相结合。其次，本文提出了一个理想化的最小化问题，以消除针对基于边界条件的"恒等"ODE的最优控制问题的一些技术困难。

    This paper presents a mathematical analysis of ODE-Net, a continuum model of deep neural networks (DNNs). In recent years, Machine Learning researchers have introduced ideas of replacing the deep structure of DNNs with ODEs as a continuum limit. These studies regard the "learning" of ODE-Net as the minimization of a "loss" constrained by a parametric ODE. Although the existence of a minimizer for this minimization problem needs to be assumed, only a few studies have investigated its existence analytically in detail. In the present paper, the existence of a minimizer is discussed based on a formulation of ODE-Net as a measure-theoretic mean-field optimal control problem. The existence result is proved when a neural network, which describes a vector field of ODE-Net, is linear with respect to learnable parameters. The proof employs the measure-theoretic formulation combined with the direct method of Calculus of Variations. Secondly, an idealized minimization problem is proposed to remove
    
[^156]: 基于贝叶斯频率重参数化的医学图像分割三维卷积核扩展

    Scaling Up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation. (arXiv:2303.05785v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2303.05785](http://arxiv.org/abs/2303.05785)

    本文提出了一种名为RepUX-Net的纯卷积神经网络体系结构，具有简单的大内核块设计，在医学图像分割中具有十分优越的性能。

    

    本文灵感源自视觉变换器，重新回归深度卷积概念，提出使用大内核尺寸来提供大有效感受野（ERF）来进行医学图像分割。然而，在卷积神经网络中使用$21\times21\times21$等大内核尺寸时，分割性能可能会饱和甚至下降。我们假设，使用大内核尺寸的卷积受限于维护局部学习的最优收敛性。而结构重参数化（SR）通过使用小内核并行增强了局部收敛性，但是优化的小内核分支可能会阻碍训练的计算效率。在本文中，我们提出了RepUX-Net，这是一个纯卷积神经网络体系结构，具有简单的大内核块设计，使用6个具有挑战性的公共数据集比拼了当前网络的最新技术（例如3D UX-Net，SwinUNETR）。我们推导出内核重参数化和支路长度等效的关系。

    With the inspiration of vision transformers, the concept of depth-wise convolution revisits to provide a large Effective Receptive Field (ERF) using Large Kernel (LK) sizes for medical image segmentation. However, the segmentation performance might be saturated and even degraded as the kernel sizes scaled up (e.g., $21\times 21\times 21$) in a Convolutional Neural Network (CNN). We hypothesize that convolution with LK sizes is limited to maintain an optimal convergence for locality learning. While Structural Re-parameterization (SR) enhances the local convergence with small kernels in parallel, optimal small kernel branches may hinder the computational efficiency for training. In this work, we propose RepUX-Net, a pure CNN architecture with a simple large kernel block design, which competes favorably with current network state-of-the-art (SOTA) (e.g., 3D UX-Net, SwinUNETR) using 6 challenging public datasets. We derive an equivalency between kernel re-parameterization and the branch-wi
    
[^157]: 分子图的Ewald长程消息传递

    Ewald-based Long-Range Message Passing for Molecular Graphs. (arXiv:2303.04791v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04791](http://arxiv.org/abs/2303.04791)

    提出了一种Ewald消息传递的非局部傅立叶空间方案来解决消息传递神经网络范式中长程相互作用难以学习的问题，在多个数据集中表现出更强的预测精度和泛化性能。

    

    近年来，从分子数据中学习势能表面的神经架构有了快速的改进，这个成功的关键在于传递神经网络范式。其中，对消息的空间距离限制是部分优化网站大小的关键驱动因素。虽然这种局部关注是一种有用的归纳偏见，但它也阻碍了对长程相互作用（如静电相互作用和范德华力）的学习。为解决这个缺点，我们提出了Ewald消息传递：一种非局部傅立叶空间方案，它通过频率截断而不是距离限制交互，并在Ewald求和方法中理论上确定。它可以作为现有MPNN架构的增强，因为它计算廉价且不关注架构细节。我们用四个基线模型和两个包含不同周期（OC20）和非周期结构（OE62）的数据集测试了这种方法。我们观察到Ewald消息传递在两个数据集上都比基线模型有更强的预测精度和泛化性能。

    Neural architectures that learn potential energy surfaces from molecular data have undergone fast improvement in recent years. A key driver of this success is the Message Passing Neural Network (MPNN) paradigm. Its favorable scaling with system size partly relies upon a spatial distance limit on messages. While this focus on locality is a useful inductive bias, it also impedes the learning of long-range interactions such as electrostatics and van der Waals forces. To address this drawback, we propose Ewald message passing: a nonlocal Fourier space scheme which limits interactions via a cutoff on frequency instead of distance, and is theoretically well-founded in the Ewald summation method. It can serve as an augmentation on top of existing MPNN architectures as it is computationally inexpensive and agnostic to architectural details. We test the approach with four baseline models and two datasets containing diverse periodic (OC20) and aperiodic structures (OE62). We observe robust impro
    
[^158]: 探究模型定向无差别数据污染攻击的极限

    Exploring the Limits of Model-Targeted Indiscriminate Data Poisoning Attacks. (arXiv:2303.03592v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03592](http://arxiv.org/abs/2303.03592)

    该研究探究了模型定向无差别数据污染攻击的极限。作者引入了模型污染可达性的概念，推导出了易于计算的阈值，并发现了一种惊人的相变现象：只有当污染比超过阈值时，攻击才能实现某些目标参数。

    

    无差别数据污染攻击旨在通过注入少量损坏的训练数据来降低模型的测试准确性。尽管受到了相当大的关注，但现有攻击对现代机器学习（ML）架构仍然相对无效。在这项工作中，我们引入了模型污染可达性的概念作为探索数据污染攻击向目标参数（即模型定向攻击）的固有限制的技术工具。我们推导出一个易于计算的阈值，以确定和量化受欢迎的ML模型之间的惊人相变现象：只有当污染比超过我们的阈值时，数据污染攻击才能实现某些目标参数。在现有参数损坏攻击的基础上，并改进了梯度取消攻击，我们进行了广泛的实验，以验证我们的理论发现，测试我们的转换阈值的可预测性，并显着改善现有的无差别数据污染攻击。

    Indiscriminate data poisoning attacks aim to decrease a model's test accuracy by injecting a small amount of corrupted training data. Despite significant interest, existing attacks remain relatively ineffective against modern machine learning (ML) architectures. In this work, we introduce the notion of model poisoning reachability as a technical tool to explore the intrinsic limits of data poisoning attacks towards target parameters (i.e., model-targeted attacks). We derive an easily computable threshold to establish and quantify a surprising phase transition phenomenon among popular ML models: data poisoning attacks can achieve certain target parameters only when the poisoning ratio exceeds our threshold. Building on existing parameter corruption attacks and refining the Gradient Canceling attack, we perform extensive experiments to confirm our theoretical findings, test the predictability of our transition threshold, and significantly improve existing indiscriminate data poisoning ba
    
[^159]: 基于本地全局蒸馏的异构数据联邦虚拟学习

    Federated Virtual Learning on Heterogeneous Data with Local-global Distillation. (arXiv:2303.02278v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02278](http://arxiv.org/abs/2303.02278)

    该论文提出了一种名为FedLGD的新方法，通过本地和全局数据集的蒸馏组合来创建一个更小的合成数据集，以解决联邦学习中处理异构数据时的性能问题，同时使用迭代分布匹配来处理同步和类别不平衡问题。

    

    虽然联邦学习已成为分布式学习机器学习模型的趋势，但在处理异构数据时，其性能容易出现下降。此外，联邦学习不可避免地面临同步、效率和隐私等挑战。近来，数据集蒸馏已被研究，以通过创建一个保留本地私有数据集训练模型性能的较小的合成数据集来提高FL的效率和可扩展性。同时，我们也发现使用蒸馏的本地数据集会放大联邦学习中的异构性问题。为了解决这个问题，我们提出了一种新的方法，称为基于本地全局蒸馏的异构数据联邦虚拟学习（FedLGD），该方法使用一个较小的合成数据集（称为虚拟数据），该数据集是通过本地和全局数据集蒸馏的组合创建的。具体来说，为了处理同步和类别不平衡问题，我们提出了迭代分布匹配，允许客户端从全局模型中获取知识并通过模型反馈来共同学习。

    Despite Federated Learning (FL)'s trend for learning machine learning models in a distributed manner, it is susceptible to performance drops when training on heterogeneous data. In addition, FL inevitability faces the challenges of synchronization, efficiency, and privacy. Recently, dataset distillation has been explored in order to improve the efficiency and scalability of FL by creating a smaller, synthetic dataset that retains the performance of a model trained on the local private datasets. We discover that using distilled local datasets can amplify the heterogeneity issue in FL. To address this, we propose a new method, called Federated Virtual Learning on Heterogeneous Data with Local-Global Distillation (FedLGD), which trains FL using a smaller synthetic dataset (referred as virtual data) created through a combination of local and global dataset distillation. Specifically, to handle synchronization and class imbalance, we propose iterative distribution matching to allow clients 
    
[^160]: 学习机器在医疗及其他方面的应用

    Learning machines for health and beyond. (arXiv:2303.01513v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01513](http://arxiv.org/abs/2303.01513)

    适用于建立预测模型的机器学习技术在医疗领域和其他领域具有广泛应用。模型的维护和监控很关键，因为模型的性能与数据的变化和传输有关。

    

    机器学习技术在构建预测模型方面具有良好效果，因为它们擅长识别大型数据集中的模式。然而，对于复杂的现实问题，模型的开发往往停留在发表论文、概念验证或通过某种部署模式的可访问性。然而，在医疗领域里，模型的患者人口会发生变化，因此模型的维护和监控是确保其长期安全有效使用的关键。由于机器学习技术是有效地训练以在可用数据集中寻找模式的，因此，对于复杂的现实问题，模型的性能不会在发表或部署时达到峰值后固定不变。相反，数据会随着时间的变化而产生变化，而当模型被运往新的地方供新的人群使用时，它们也会发生变化。

    Machine learning techniques are effective for building predictive models because they are good at identifying patterns in large datasets. Development of a model for complex real life problems often stops at the point of publication, proof of concept or when made accessible through some mode of deployment. However, a model in the medical domain risks becoming obsolete as soon as patient demographic changes. The maintenance and monitoring of predictive models post-publication is crucial to guarantee their safe and effective long term use. As machine learning techniques are effectively trained to look for patterns in available datasets, the performance of a model for complex real life problems will not peak and remain fixed at the point of publication or even point of deployment. Rather, data changes over time, and they also changed when models are transported to new places to be used by new demography.
    
[^161]: 安全剥离L0正则化最小二乘问题

    Safe Peeling for L0-Regularized Least-Squares with supplementary material. (arXiv:2302.14471v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14471](http://arxiv.org/abs/2302.14471)

    引入“安全剥离”方法加速解决L0正则化最小二乘问题，通过收缩松弛度允许更激进的剪枝，显著降低求解时间。

    

    我们引入了一种新的方法，称为“安全剥离”，通过分支定界算法加速解决L0正则化最小二乘问题。我们的程序使得在BnB决策树的每个节点处考虑到收缩松弛度，因此可能允许更加激进的剪枝。数值模拟表明，我们提出的方法在探索节点数量和整体求解时间方面具有显著的优势。

    We introduce a new methodology dubbed ``safe peeling'' to accelerate the resolution of L0-regularized least-squares problems via a Branch-and-Bound (BnB) algorithm. Our procedure enables to tighten the convex relaxation considered at each node of the BnB decision tree and therefore potentially allows for more aggressive pruning. Numerical simulations show that our proposed methodology leads to significant gains in terms of number of nodes explored and overall solving time.s show that our proposed methodology leads to significant gains in terms of number of nodes explored and overall solving time.
    
[^162]: 异质性处理效应的因果等保校准方法

    Causal isotonic calibration for heterogeneous treatment effects. (arXiv:2302.14011v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.14011](http://arxiv.org/abs/2302.14011)

    提出了因果等保校准方法及其数据有效的变体交叉校准，这两种方法都能快速稳健地校准异质性处理效应的预测器，而且可以应用于任何黑盒学习算法。

    

    本文提出一种新的非参数方法——因果等保校准方法，用于校准异质性处理效应的预测器。此外，我们还介绍了交叉校准，这是一种数据有效的校准变体，消除了保留校准集的需要。交叉校准利用交叉拟合的预测器，并使用所有可用数据生成一个单一的校准预测器。在不要求单调性的弱条件下，我们证明了因果等保校准和交叉校准都能实现快速双重稳健校准速率，只要利用类似意义下精确估计了倾向得分或后果回归。这种因果等保校准器可以包装在任何黑盒学习算法周围，提供强健和分布自由的校准保证，同时保持预测性能。

    We propose causal isotonic calibration, a novel nonparametric method for calibrating predictors of heterogeneous treatment effects. Furthermore, we introduce cross-calibration, a data-efficient variant of calibration that eliminates the need for hold-out calibration sets. Cross-calibration leverages cross-fitted predictors and generates a single calibrated predictor using all available data. Under weak conditions that do not assume monotonicity, we establish that both causal isotonic calibration and cross-calibration achieve fast doubly-robust calibration rates, as long as either the propensity score or outcome regression is estimated accurately in a suitable sense. The proposed causal isotonic calibrator can be wrapped around any black-box learning algorithm, providing robust and distribution-free calibration guarantees while preserving predictive performance.
    
[^163]: PITS：基于变分推断的无基频端到端音高可控TTS

    PITS: Variational Pitch Inference without Fundamental Frequency for End-to-End Pitch-controllable TTS. (arXiv:2302.12391v3 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2302.12391](http://arxiv.org/abs/2302.12391)

    PITS是一种基于变分推断的端到端音高可控TTS模型，相较于以往基频建模的方法，具有更高的合成语音方差和音高可控性。

    

    先前音高可控TTS模型依赖于直接建模基频，导致合成语音的方差很低。为解决这个问题，我们提出了PITS，一种利用变分推断对音高进行建模的端到端音高可控TTS模型。基于VITS，PITS结合了Yingram编码器，Yingram解码器以及音高移位合成的对抗性训练，实现了音高可控性。实验证明，PITS生成的高质量语音与原始语音不可区分，并具有高品质的音高可控性。代码、音频示例和演示可以在https://github.com/anonymous-pits/pits 上获得。

    Previous pitch-controllable text-to-speech (TTS) models rely on directly modeling fundamental frequency, leading to low variance in synthesized speech. To address this issue, we propose PITS, an end-to-end pitch-controllable TTS model that utilizes variational inference to model pitch. Based on VITS, PITS incorporates the Yingram encoder, the Yingram decoder, and adversarial training of pitch-shifted synthesis to achieve pitch-controllability. Experiments demonstrate that PITS generates high-quality speech that is indistinguishable from ground truth speech and has high pitch-controllability without quality degradation. Code, audio samples, and demo are available at https://github.com/anonymous-pits/pits.
    
[^164]: 学习可以遵守守恒定律的物理模型

    Learning Physical Models that Can Respect Conservation Laws. (arXiv:2302.11002v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11002](http://arxiv.org/abs/2302.11002)

    这项工作提出了ProbConserv框架，通过将守恒约束与贝叶斯更新相结合，将守恒约束纳入通用科学机器学习体系结构中，以便在学习高难度的PDE运算中应用。

    

    科学机器学习的最近一些工作集中在将偏微分方程（PDE）信息融入学习过程中。其中许多工作集中在相对“容易”的PDE算子（例如椭圆和抛物线）上，而相对“困难”的PDE算子（例如双曲线）则较少。在数值PDE方面，后一种问题类需要控制一种体积元素或守恒约束类型，这被视为具有挑战性的问题。为了实现科学机器学习的承诺，需要无缝地将这两种类型的问题融入学习过程中。

    Recent work in scientific machine learning (SciML) has focused on incorporating partial differential equation (PDE) information into the learning process. Much of this work has focused on relatively ``easy'' PDE operators (e.g., elliptic and parabolic), with less emphasis on relatively ``hard'' PDE operators (e.g., hyperbolic). Within numerical PDEs, the latter problem class requires control of a type of volume element or conservation constraint, which is known to be challenging. Delivering on the promise of SciML requires seamlessly incorporating both types of problems into the learning process. To address this issue, we propose ProbConserv, a framework for incorporating conservation constraints into a generic SciML architecture. To do so, ProbConserv combines the integral form of a conservation law with a Bayesian update. We provide a detailed analysis of ProbConserv on learning with the Generalized Porous Medium Equation (GPME), a widely-applicable parameterized family of PDEs that 
    
[^165]: 分布模型和半监督学习器在少量标签上互相受益

    Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels. (arXiv:2302.10586v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.10586](http://arxiv.org/abs/2302.10586)

    本文介绍了一种名为双伪训练（DPT）的训练策略，该策略结合了强大的半监督学习器和扩散模型来进一步推进半监督生成和分类任务。实验结果表明，DPT在各种情况下都能实现半监督生成和分类任务的SOTA性能，特别是在每个类别只有一个或两个标签的情况下，超过了其他一些模型。

    

    为了进一步推进半监督生成和分类任务，本文提出了一种简单而有效的训练策略——双伪训练（DPT），该策略建立在强大的半监督学习器和扩散模型之上。DPT分为三个阶段：使用部分标记数据训练分类器以预测伪标签；使用这些伪标签训练条件生成模型以生成伪图像；并使用真实和伪造的图像混合重新训练分类器。实验结果表明，在各种情况下，DPT始终实现了半监督生成和分类的SOTA性能。特别是，在每个类别只有一个或两个标签的情况下，在ImageNet 256x256上，DPT的Fr\'echet Inception Distance（FID）得分分别为3.08或2.52，超过了具有完整标签的强扩散模型（如IDDPM，CDM，ADM和LDM）。此外，DPT在ImageNet分类任务上显著优于竞争性的半监督基线，实现了顶级1的准确性。

    In an effort to further advance semi-supervised generative and classification tasks, we propose a simple yet effective training strategy called dual pseudo training (DPT), built upon strong semi-supervised learners and diffusion models. DPT operates in three stages: training a classifier on partially labeled data to predict pseudo-labels; training a conditional generative model using these pseudo-labels to generate pseudo images; and retraining the classifier with a mix of real and pseudo images. Empirically, DPT consistently achieves SOTA performance of semi-supervised generation and classification across various settings. In particular, with one or two labels per class, DPT achieves a Fr\'echet Inception Distance (FID) score of 3.08 or 2.52 on ImageNet 256x256, surpassing strong diffusion models with full labels, such as IDDPM, CDM, ADM, and LDM. Besides, DPT outperforms competitive semi-supervised baselines substantially on ImageNet classification tasks, achieving top-1 accuracies o
    
[^166]: 通过f-散度最小化对齐语言模型与偏好

    Aligning Language Models with Preferences through f-divergence Minimization. (arXiv:2302.08215v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.08215](http://arxiv.org/abs/2302.08215)

    本文提出一种新的方法f-DPG，用于对齐语言模型和偏好，该方法适用于评估任何目标分布，统一了现有的各种框架和逼近方法。

    

    对齐语言模型和偏好可以被看作是对目标分布进行逼近，以期达到某种所需行为。现有的方法在目标分布的函数形式和用于逼近目标分布的算法上存在差异。本文提出了一种新方法f-DPG，该方法允许使用任何可评估的f-散度逼近任何目标分布，从而统一了现有的各种框架和逼近方法。我们展示了各种散度目标的实际好处，并证明了没有普适的最佳选择。

    Aligning language models with preferences can be posed as approximating a target distribution representing some desired behavior. Existing approaches differ both in the functional form of the target distribution and the algorithm used to approximate it. For instance, Reinforcement Learning from Human Feedback (RLHF) corresponds to minimizing a reverse KL from an implicit target distribution arising from a KL penalty in the objective. On the other hand, Generative Distributional Control (GDC) has an explicit target distribution and minimizes a forward KL from it using the Distributional Policy Gradient (DPG) algorithm. In this paper, we propose a new approach, f-DPG, which allows the use of any f-divergence to approximate any target distribution that can be evaluated. f-DPG unifies both frameworks (RLHF, GDC) and the approximation methods (DPG, RL with KL penalties). We show the practical benefits of various choices of divergence objectives and demonstrate that there is no universally o
    
[^167]: 从有效电阻的角度理解GNN中的过度压缩

    Understanding Oversquashing in GNNs through the Lens of Effective Resistance. (arXiv:2302.06835v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06835](http://arxiv.org/abs/2302.06835)

    本文提出了一种通过分析图中节点之间的有效电阻来理解和减轻GNN中过度压缩的方法，并提出了使用总有效电阻作为压缩总量限制的方法，进而开发了一种算法以减轻过度压缩。

    

    消息传递图神经网络（GNN）是用于图结构数据的流行学习架构。然而，GNN所遇到的一个问题是过度压缩，即GNN在远距离节点之间传递信息时存在困难。最近，理解和缓解过度压缩引起了研究界的重视。本文通过分析输入图中节点之间的有效电阻来理解和减轻过度压缩。有效电阻直观地捕捉图中两个节点之间路径的“强度”，并在图理论的许多领域具有丰富的文献。我们建议使用总有效电阻作为图中过度压缩总量的限制，并为其使用提供理论验证。我们进一步开发了一种算法，以识别要添加到输入图中的边以最小化总有效电阻，从而减轻过度压缩。我们在合成和现实世界数据集上提供了实验，证明了我们方法的有效性。

    Message passing graph neural networks (GNNs) are a popular learning architectures for graph-structured data. However, one problem GNNs experience is oversquashing, where a GNN has difficulty sending information between distant nodes. Understanding and mitigating oversquashing has recently received significant attention from the research community. In this paper, we continue this line of work by analyzing oversquashing through the lens of the effective resistance between nodes in the input graph. Effective resistance intuitively captures the ``strength'' of connection between two nodes by paths in the graph, and has a rich literature spanning many areas of graph theory. We propose to use total effective resistance as a bound of the total amount of oversquashing in a graph and provide theoretical justification for its use. We further develop an algorithm to identify edges to be added to an input graph to minimize the total effective resistance, thereby alleviating oversquashing. We provi
    
[^168]: ConCerNet：一种基于对比学习的自动发现守恒律和可靠动力学系统预测框架

    ConCerNet: A Contrastive Learning Based Framework for Automated Conservation Law Discovery and Trustworthy Dynamical System Prediction. (arXiv:2302.05783v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05783](http://arxiv.org/abs/2302.05783)

    本文提出了一种基于对比学习的框架ConCerNet，用于提高DNN动力学建模的可靠性，实现对系统不变量的自动捕捉和保留，经实验证明其性能优于传统神经网络方法。

    

    深度神经网络(DNN)在动力学系统建模方面表现出极大的能力；然而，它们通常不遵守物理约束，如守恒定律。本文提出了一种名为ConCerNet的新的学习框架，以提高基于DNN的动力学建模的可靠性，赋予不变的属性。ConCerNet由两个步骤组成:(i)对比学习方法，自动捕捉轨迹观测中的系统不变量(即守恒性质)；(ii)神经投影层，保证学习到的动力学模型保留学习到的不变量。我们从理论上证明了学习到的潜在表示和未知系统不变量函数之间的功能关系。实验表明，我们的方法在坐标误差和守恒指标方面始终比基线神经网络表现出更好的效果。使用基于神经网络的参数化且不依赖于先前知识，我们的方法在动力学方面具有广阔的应用前景。

    Deep neural networks (DNN) have shown great capacity of modeling a dynamical system; nevertheless, they usually do not obey physics constraints such as conservation laws. This paper proposes a new learning framework named ConCerNet to improve the trustworthiness of the DNN based dynamics modeling to endow the invariant properties. ConCerNet consists of two steps: (i) a contrastive learning method to automatically capture the system invariants (i.e. conservation properties) along the trajectory observations; (ii) a neural projection layer to guarantee that the learned dynamics models preserve the learned invariants. We theoretically prove the functional relationship between the learned latent representation and the unknown system invariant function. Experiments show that our method consistently outperforms the baseline neural networks in both coordinate error and conservation metrics by a large margin. With neural network based parameterization and no dependence on prior knowledge, our 
    
[^169]: 对抗样本起到了积极作用：通过对抗样本防止扩散模型模仿绘画

    Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples. (arXiv:2302.04578v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.04578](http://arxiv.org/abs/2302.04578)

    该论文提出了一种利用对抗样本来保护人类创造的艺术品，对抗侵权者利用未经授权的绘画训练DMs生成类似风格的新颖绘画的方法。

    

    最近，扩散模型（DMs）在人工智能艺术领域掀起了一股热潮，但同时也引发了新的版权问题，即侵权者利用未经授权的绘画训练DMs生成类似风格的新颖绘画。为了解决这些新兴的版权问题，我们首次探索并提出利用对抗样本保护人类创造的艺术品的方法。具体而言，我们首先建立了一个理论框架来定义和评估DMs的对抗样本。然后，基于这个框架，我们设计了一种新算法，命名为AdvDM，它通过对从DMs的反向过程中抽样的不同潜变量进行蒙特卡罗估计的对抗样本进行优化。广泛的实验证明，生成的对抗样本可以有效地阻碍DMs提取它们的特征。因此，我们的方法可以成为保护人类艺术家版权的强有力工具，以对抗装备有DMs的侵权者。

    Recently, Diffusion Models (DMs) boost a wave in AI for Art yet raise new copyright concerns, where infringers benefit from using unauthorized paintings to train DMs to generate novel paintings in a similar style. To address these emerging copyright violations, in this paper, we are the first to explore and propose to utilize adversarial examples for DMs to protect human-created artworks. Specifically, we first build a theoretical framework to define and evaluate the adversarial examples for DMs. Then, based on this framework, we design a novel algorithm, named AdvDM, which exploits a Monte-Carlo estimation of adversarial examples for DMs by optimizing upon different latent variables sampled from the reverse process of DMs. Extensive experiments show that the generated adversarial examples can effectively hinder DMs from extracting their features. Therefore, our method can be a powerful tool for human artists to protect their copyright against infringers equipped with DM-based AI-for-A
    
[^170]: 不是神奇药丸，而是洞察力之搜寻：消除异质性处理效应估计中的模型选择困境

    In Search of Insights, Not Magic Bullets: Towards Demystification of the Model Selection Dilemma in Heterogeneous Treatment Effect Estimation. (arXiv:2302.02923v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.02923](http://arxiv.org/abs/2302.02923)

    本文研究在具有高风险应用的个性化处理效应估计中，不同模型选择标准的优点和缺点，并提出未来研究方向。

    

    个性化处理效应估计在高风险应用中经常备受关注，因此，在实践中部署估计这种效应的模型之前，需要确信已经选择了最好的机器学习工具箱中的候选模型。不幸的是，由于实践中缺乏反事实信息，通常无法依靠标准验证指标完成此任务，导致了处理效应估计文献中已知的模型选择困境。虽然最近已经研究了一些解决方案，但对不同模型选择标准的优缺点的系统理解仍然缺乏。因此，在本文中，我们并没有试图宣布全局“胜者”，而是对不同选择标准的成功和失败模式进行了实证研究。我们强调选择策略，候选估计量和用于比较它们的数据之间存在复杂的相互作用，并提出了未来研究的方向。

    Personalized treatment effect estimates are often of interest in high-stakes applications -- thus, before deploying a model estimating such effects in practice, one needs to be sure that the best candidate from the ever-growing machine learning toolbox for this task was chosen. Unfortunately, due to the absence of counterfactual information in practice, it is usually not possible to rely on standard validation metrics for doing so, leading to a well-known model selection dilemma in the treatment effect estimation literature. While some solutions have recently been investigated, systematic understanding of the strengths and weaknesses of different model selection criteria is still lacking. In this paper, instead of attempting to declare a global `winner', we therefore empirically investigate success- and failure modes of different selection criteria. We highlight that there is a complex interplay between selection strategies, candidate estimators and the data used for comparing them, an
    
[^171]: 一种两层神经网络中随机梯度下降引起的表示漂移问题

    Stochastic Gradient Descent-Induced Drift of Representation in a Two-Layer Neural Network. (arXiv:2302.02563v2 [cond-mat.dis-nn] UPDATED)

    [http://arxiv.org/abs/2302.02563](http://arxiv.org/abs/2302.02563)

    本文研究了一个两层神经网络中因随机梯度下降而引起的表示漂移问题，发现其前者对应于有限方差波动，而后者可以被视为流形上的有效扩散过程。

    

    表示漂移是指神经元激活随时间变化而任务表现保持稳定。尽管在大脑和人工网络中观察到，但漂移的机制及其影响仍不完全了解。受最近在海马皮质中发现的刺激依赖性漂移的实验结果的启发，我们使用理论和模拟研究了一个两层线性前馈网络中的这种现象。具体而言，在连续在线学习场景中，我们研究了随机梯度下降（SGD）固有噪声引起的漂移。通过将学习动态分解为最小损失流形的正切空间和切空间，我们发现前者对应于有限方差波动，而后者可以被视为流形上的有效扩散过程。我们分析地计算了隐藏层刺激表示的波动和扩散系数作为网络参数的函数。

    Representational drift refers to over-time changes in neural activation accompanied by a stable task performance. Despite being observed in the brain and in artificial networks, the mechanisms of drift and its implications are not fully understood. Motivated by recent experimental findings of stimulus-dependent drift in the piriform cortex, we use theory and simulations to study this phenomenon in a two-layer linear feedforward network. Specifically, in a continual online learning scenario, we study the drift induced by the noise inherent in the Stochastic Gradient Descent (SGD). By decomposing the learning dynamics into the normal and tangent spaces of the minimum-loss manifold, we show the former corresponds to a finite variance fluctuation, while the latter could be considered as an effective diffusion process on the manifold. We analytically compute the fluctuation and the diffusion coefficients for the stimuli representations in the hidden layer as functions of network parameters 
    
[^172]: 增强潜空间贝叶斯优化中的探索能力

    Enhancing Exploration in Latent Space Bayesian Optimization. (arXiv:2302.02399v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02399](http://arxiv.org/abs/2302.02399)

    本文提出了一种新的方法来提高潜空间贝叶斯优化（LSBO）的探索能力。方法包括潜在一致性感知获取函数（LCA-AF）和增加一致性点的潜空间生成方法（LCA-VAE），将它们结合起来形成了LCA-LSBO。实验证明LCA-LSBO在图像生成和全新的化学设计任务中表现出改进的性能。

    

    潜空间贝叶斯优化（LSBO）将生成模型（通常是变分自编码器）与贝叶斯优化相结合，以生成感兴趣的全新对象。然而，由于贝叶斯优化和变分自编码器之间的目标不匹配，导致了LSBO面临挑战和推广能力的减弱。本文提出了增强LSBO效率并克服这一挑战的新思路。首先，我们引入了潜在的一致性和不一致性的概念，这是LSBO中一个关键的问题，起源于BO-VAE之间的不匹配。为了解决这个问题，我们提出了潜在的一致意识获取函数（LCA-AF），利用LSBO中的一致性区域。此外，我们提出了LCA-VAE，一种新的VAE方法，它生成具有增加的一致性点的潜空间，提高了BO的推广能力。结合LCA-VAE和LCA-AF，我们发展了LCA-LSBO。实验评估证实了LCA-LSBO在图像生成和全新的化学设计任务中的改进性能。

    Latent Space Bayesian Optimization (LSBO) combines generative models, typically Variational Autoencoders (VAE), with Bayesian Optimization (BO) to generate de novo objects of interest. However, LSBO faces challenges due to the mismatch between the objectives of BO and VAE, resulting in poor extrapolation capabilities. In this paper, we propose novel contributions to enhance LSBO efficiency and overcome this challenge. We first introduce the concept of latent consistency/inconsistency as a crucial problem in LSBO, arising from the BO-VAE mismatch. To address this, we propose the Latent Consistent Aware-Acquisition Function (LCA-AF) that leverages consistent regions in LSBO. Additionally, we present LCA-VAE, a novel VAE method that generates a latent space with increased consistent points, improving BO's extrapolation capabilities. Combining LCA-VAE and LCA-AF, we develop LCA-LSBO. Experimental evaluations validate the improved performance of LCA-LSBO in image generation and de-novo chem
    
[^173]: 基于关系Weisfeiler-Leman的链路预测理论

    A Theory of Link Prediction via Relational Weisfeiler-Leman. (arXiv:2302.02209v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02209](http://arxiv.org/abs/2302.02209)

    本文提出了一种基于关系Weisfeiler-Leman算法的理论，为知识图谱中的图神经网络提供了理论解释，并对各种模型的表达能力进行了表征，并解释了一些广泛采用的实际设计选择的优点。

    

    图神经网络是用于图结构数据表示学习的重要模型。尽管我们已经很好地理解了这些模型在简单图上的能力和局限性，但对于知识图谱，我们的理解仍然不完整。本文的目标是为知识图谱中的图神经网络提供系统性的理解，以解决链路预测等重要任务。我们的分析涉及一种统一的视角、看似不相关的模型，并解锁了一系列其他模型。通过相应的关系Weisfeiler-Leman算法，表征了各种模型的表达能力。此分析被扩展以对图神经网络类别捕捉的函数类进行精确逻辑描述。提出的理论发现解释了一些广泛采用的实际设计选择的优点，并得到了经验验证。

    Graph neural networks are prominent models for representation learning over graph-structured data. While the capabilities and limitations of these models are well-understood for simple graphs, our understanding remains incomplete in the context of knowledge graphs. Our goal is to provide a systematic understanding of the landscape of graph neural networks for knowledge graphs pertaining to the prominent task of link prediction. Our analysis entails a unifying perspective on seemingly unrelated models and unlocks a series of other models. The expressive power of various models is characterized via a corresponding relational Weisfeiler-Leman algorithm. This analysis is extended to provide a precise logical characterization of the class of functions captured by a class of graph neural networks. The theoretical findings presented in this paper explain the benefits of some widely employed practical design choices, which are validated empirically.
    
[^174]: 基于深度强化学习的网络物理系统在线错误检测

    Deep Reinforcement Learning for Online Error Detection in Cyber-Physical Systems. (arXiv:2302.01567v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01567](http://arxiv.org/abs/2302.01567)

    本文提出了一种基于深度强化学习（DRL）的新型在线错误检测方法。

    

    可靠性是网络物理系统中主要的设计标准之一。这是由于CPS中存在一些关键应用程序，它们的失效是灾难性的。因此，在CPS中使用强大的错误检测和纠正机制是不可避免的。传统的容错方法包括冗余时间、硬件、信息和/或软件。然而，这些方法除了低错误覆盖率外，还会带来极大的开销，限制了它们的适用性。本文提出了一种基于深度强化学习（DRL）的新型错误检测方法。

    Reliability is one of the major design criteria in Cyber-Physical Systems (CPSs). This is because of the existence of some critical applications in CPSs and their failure is catastrophic. Therefore, employing strong error detection and correction mechanisms in CPSs is inevitable. CPSs are composed of a variety of units, including sensors, networks, and microcontrollers. Each of these units is probable to be in a faulty state at any time and the occurred fault can result in erroneous output. The fault may cause the units of CPS to malfunction and eventually crash. Traditional fault-tolerant approaches include redundancy time, hardware, information, and/or software. However, these approaches impose significant overheads besides their low error coverage, which limits their applicability. In addition, the interval between error occurrence and detection is too long in these approaches. In this paper, based on Deep Reinforcement Learning (DRL), a new error detection approach is proposed that
    
[^175]: 预条件对超参化低秩矩阵感知的影响

    The Power of Preconditioning in Overparameterized Low-Rank Matrix Sensing. (arXiv:2302.01186v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01186](http://arxiv.org/abs/2302.01186)

    该研究提出了ScaledGD(𝜆)方法，相较于传统梯度下降法更加鲁棒，并且在处理低秩矩阵感知问题时具有很好的表现。

    

    本文提出了ScaledGD(𝜆)方法来解决低秩矩阵感知中矩阵可能病态以及真实秩未知的问题。该方法使用超参式表示，从一个小的随机初始化开始，通过使用特定形式的阻尼预条件梯度下降来对抗超参化和病态曲率的影响。与基准梯度下降（GD）相比，尽管预处理需要轻微的计算开销，但ScaledGD（𝜆）在面对病态问题时表现出了出色的鲁棒性。在高斯设计下，ScaledGD($\lambda$) 会在仅迭代数对数级别的情况下，以线性速率收敛到真实的低秩矩阵。

    We propose $\textsf{ScaledGD($\lambda$)}$, a preconditioned gradient descent method to tackle the low-rank matrix sensing problem when the true rank is unknown, and when the matrix is possibly ill-conditioned. Using overparametrized factor representations, $\textsf{ScaledGD($\lambda$)}$ starts from a small random initialization, and proceeds by gradient descent with a specific form of damped preconditioning to combat bad curvatures induced by overparameterization and ill-conditioning. At the expense of light computational overhead incurred by preconditioners, $\textsf{ScaledGD($\lambda$)}$ is remarkably robust to ill-conditioning compared to vanilla gradient descent ($\textsf{GD}$) even with overprameterization. Specifically, we show that, under the Gaussian design, $\textsf{ScaledGD($\lambda$)}$ converges to the true low-rank matrix at a constant linear rate after a small number of iterations that scales only logarithmically with respect to the condition number and the problem dimensi
    
[^176]: 重新审视 Bellman Errors 用于离线模型选择

    Revisiting Bellman Errors for Offline Model Selection. (arXiv:2302.00141v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00141](http://arxiv.org/abs/2302.00141)

    本文重新审视 Bellman Errors，发现之前的Bellman Errors 方法需要在特定条件下才能表现良好，同时提出了更准确的 MSBE 估计器，在离散控制任务方面表现出色。

    

    离线模型选择（OMS）即在只有已记录数据的情况下从众多策略中选择最佳策略，对于在实际环境下应用离线RL至关重要。一个经过广泛探讨的想法是根据相关Q函数的均方Bellman误差（MSBE）选择策略。然而，之前的研究一直在使用Bellman误差时无法获得足够的OMS性能，导致许多研究人员放弃此想法。为此，本文阐述了为什么之前的结果使用Bellman误差时会看到悲观的结果，并确定了基于Bellman误差的OMS算法将表现良好的条件。此外，我们开发了一个比之前方法更准确的MSBE的新的估计器。我们的估计器在不同的离散控制任务（包括 Atari 游戏）上获得了出色的OMS性能。

    Offline model selection (OMS), that is, choosing the best policy from a set of many policies given only logged data, is crucial for applying offline RL in real-world settings. One idea that has been extensively explored is to select policies based on the mean squared Bellman error (MSBE) of the associated Q-functions. However, previous work has struggled to obtain adequate OMS performance with Bellman errors, leading many researchers to abandon the idea. To this end, we elucidate why previous work has seen pessimistic results with Bellman errors and identify conditions under which OMS algorithms based on Bellman errors will perform well. Moreover, we develop a new estimator of the MSBE that is more accurate than prior methods. Our estimator obtains impressive OMS performance on diverse discrete control tasks, including Atari games.
    
[^177]: 面向高效梯度为基础的值估计

    Toward Efficient Gradient-Based Value Estimation. (arXiv:2301.13757v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13757](http://arxiv.org/abs/2301.13757)

    本研究研究了梯度为基础的值估计方法慢的根本原因，并提出了一种低复杂度的方法以解决损失函数带来的不良影响，该方法在效率上比剩余梯度方法更快，几乎具有相同的计算复杂度，并且在经典问题上与TD具有竞争力。

    

    强化学习中基于梯度的值估计方法具有良好的稳定性，但通常比时间差异（TD）学习方法慢得多。我们研究了这种缓慢的根本原因，并表明均方贝尔曼误差（MSBE）是一种病态的损失函数，其黑塞矩阵具有较大的条件数。为了解决MSBE的不良条件对基于梯度的方法的负面影响，我们提出了一种低复杂度的无批处理近端方法，它近似遵循高斯牛顿方向，并在参数化方面渐近鲁棒。我们的主要算法称为RANS，它在效率上比剩余梯度方法更快，几乎具有相同的计算复杂度，并且在我们测试的经典问题上与TD具有竞争力。

    Gradient-based methods for value estimation in reinforcement learning have favorable stability properties, but they are typically much slower than Temporal Difference (TD) learning methods. We study the root causes of this slowness and show that Mean Square Bellman Error (MSBE) is an ill-conditioned loss function in the sense that its Hessian has large condition-number. To resolve the adverse effect of poor conditioning of MSBE on gradient based methods, we propose a low complexity batch-free proximal method that approximately follows the Gauss-Newton direction and is asymptotically robust to parameterization. Our main algorithm, called RANS, is efficient in the sense that it is significantly faster than the residual gradient methods while having almost the same computational complexity, and is competitive with TD on the classic problems that we tested.
    
[^178]: 关于具有机器可表示参数的神经网络自动微分正确性的研究

    On the Correctness of Automatic Differentiation for Neural Networks with Machine-Representable Parameters. (arXiv:2301.13370v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13370](http://arxiv.org/abs/2301.13370)

    本论文研究了神经网络参数为机器可表示数字时自动微分的正确性问题，证明了神经网络带偏置参数时自动微分始终正确，给出了限制不可微性在激活函数中数目的界，并提供了判断参数是否在不可微参数组中的条件。

    

    最近的研究表明，实数域上的前向和反向模式自动微分几乎始终在数学上是准确的。然而，实际编程使用的是机器可表示的数字（例如浮点数），而不是实数。本文研究了当神经网络的参数空间仅由机器可表示的数字组成时，自动微分的正确性。我们分析了两组可能导致自动微分不正确的参数：一组是网络可微但自动微分无法计算其导数的参数组，另一组是网络不可微的参数组。对于带有偏置参数的神经网络，我们首先证明了第一组参数组始终为空。然后我们给出了一个线性上限来限制第二组参数组中不可微性在激活函数中的数目，并给出了一个简单的必要和充分条件来判断一个参数是否在这个参数组中。

    Recent work has shown that forward- and reverse- mode automatic differentiation (AD) over the reals is almost always correct in a mathematically precise sense. However, actual programs work with machine-representable numbers (e.g., floating-point numbers), not reals. In this paper, we study the correctness of AD when the parameter space of a neural network consists solely of machine-representable numbers. In particular, we analyze two sets of parameters on which AD can be incorrect: the incorrect set on which the network is differentiable but AD does not compute its derivative, and the non-differentiable set on which the network is non-differentiable. For a neural network with bias parameters, we first prove that the incorrect set is always empty. We then prove a tight bound on the size of the non-differentiable set, which is linear in the number of non-differentiabilities in activation functions, and give a simple necessary and sufficient condition for a parameter to be in this set. W
    
[^179]: 使用特征筛选克服深度神经网络中的简单偏差

    Overcoming Simplicity Bias in Deep Networks using a Feature Sieve. (arXiv:2301.13293v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13293](http://arxiv.org/abs/2301.13293)

    提出了一种特征筛选方法，通过抑制网络较低层易计算虚假特征，使得更高层的网络提取和利用更丰富、更有意义的特征表示，从而克服了深度神经网络中的简单偏差。

    

    简单偏差是深度神经网络倾向于依赖简单且预测性较弱特征的令人担忧的趋势，从而排除更强、更复杂的特征。在真实世界应用中，由于训练数据有限和虚假特征标签相关性，导致了偏向性、不正确的预测。我们提出了一种直接、干预深度神经网络中简单偏差的方法，称为特征筛选。我们的目标是自动识别和抑制网络较低层的易计算虚假特征，从而让更高层的网络提取和利用更丰富、更有意义的表示。我们提供了控制数据集和真实世界图像上有关有效特征不同压制和增强的具体证据，并在许多真实世界去偏差基准测试中报告了显著性提高（Imagenet-A相对增益11.4％；BAR 3.2％等）。关键是，我们不依赖虚假属性的先验知识。

    Simplicity bias is the concerning tendency of deep networks to over-depend on simple, weakly predictive features, to the exclusion of stronger, more complex features. This is exacerbated in real-world applications by limited training data and spurious feature-label correlations, leading to biased, incorrect predictions. We propose a direct, interventional method for addressing simplicity bias in DNNs, which we call the feature sieve. We aim to automatically identify and suppress easily-computable spurious features in lower layers of the network, thereby allowing the higher network levels to extract and utilize richer, more meaningful representations. We provide concrete evidence of this differential suppression & enhancement of relevant features on both controlled datasets and real-world images, and report substantial gains on many real-world debiasing benchmarks (11.4% relative gain on Imagenet-A; 3.2% on BAR, etc). Crucially, we do not depend on prior knowledge of spurious attributes
    
[^180]: AutoPEFT：用于参数高效微调的自动配置搜索

    AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning. (arXiv:2301.12132v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.12132](http://arxiv.org/abs/2301.12132)

    AutoPEFT是一个自动化的PEFT（参数高效微调）配置搜索方法，它能够自动地找到最佳的PEFT模块和体系结构，以优化任务的性能和参数效率。在典型的NLP任务中，AutoPEFT表现出比手动设计更好的性能。

    

    大型预训练语言模型通过专门的微调用于下游NLP任务，但这样的过程可能很昂贵。最近，参数高效微调（PEFT）方法通过更新比完整模型微调（FFT）少得多的参数，实现了强大的任务性能。然而，在PEFT配置方面做出明智的设计选择是不容易的，例如它们的体系结构、可调参数的数量，甚至是PEFT模块插入的图层。因此，目前的手动设计配置很可能在性能效率权衡方面是次优的。受神经架构搜索的进展启发，我们提出了AutoPEFT来自动选择PEFT配置：首先设计具有多个代表性PEFT模块的表达配置搜索空间。然后使用多目标贝叶斯优化进行低成本的设置，从而发现优化任务性能和参数效率的Pareto优化配置。我们在几个典型的NLP任务，包括文本分类、问答和命名实体识别上评估了AutoPEFT，并展示了其优于手动设计基线的性能。

    Large pretrained language models are widely used in downstream NLP tasks via task-specific fine-tuning, but such procedures can be costly. Recently, Parameter-Efficient Fine-Tuning (PEFT) methods have achieved strong task performance while updating a much smaller number of parameters compared to full model fine-tuning (FFT). However, it is non-trivial to make informed design choices on the PEFT configurations, such as their architecture, the number of tunable parameters, and even the layers in which the PEFT modules are inserted. Consequently, it is highly likely that the current, manually designed configurations are suboptimal in terms of their performance-efficiency trade-off. Inspired by advances in neural architecture search, we propose AutoPEFT for automatic PEFT configuration selection: we first design an expressive configuration search space with multiple representative PEFT modules as building blocks. Using multi-objective Bayesian optimisation in a low-cost setup, we then disc
    
[^181]: 拉普拉斯有界深度神经网络的直接参数化

    Direct Parameterization of Lipschitz-Bounded Deep Networks. (arXiv:2301.11526v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11526](http://arxiv.org/abs/2301.11526)

    本文提出了一种直接参数化的深度神经网络，其具有拉普拉斯界限，通过标准梯度方法进行训练，避免了计算密集型的投影或障碍项。

    

    本文引入了一种新的深度神经网络参数化方式（全连接和卷积网络），具有有限灵敏度的拉普拉斯界限。与SDP方法不同的是，我们提供了一个"直接"参数化方式，并通过标准的梯度方法进行训练，而不需要任何计算密集型的投影或障碍项。

    This paper introduces a new parameterization of deep neural networks (both fully-connected and convolutional) with guaranteed Lipschitz bounds, i.e. limited sensitivity to perturbations. The Lipschitz guarantees are equivalent to the tightest-known bounds based on certification via a semidefinite program (SDP), which does not scale to large models. In contrast to the SDP approach, we provide a ``direct'' parameterization, i.e. a smooth mapping from $\mathbb R^N$ onto the set of weights of Lipschitz-bounded networks. This enables training via standard gradient methods, without any computationally intensive projections or barrier terms. The new parameterization can equivalently be thought of as either a new layer type (the \textit{sandwich layer}), or a novel parameterization of standard feedforward networks with parameter sharing between neighbouring layers. Finally, the comprehensive set of experiments on image classification shows that sandwich layers outperform previous approaches on
    
[^182]: 用于采样分子晶体结构的刚体流

    Rigid body flows for sampling molecular crystal structures. (arXiv:2301.11355v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11355](http://arxiv.org/abs/2301.11355)

    本文介绍了一种新型的正则化流，专为三维空间中多个物体的位置和方向建模而设计。通过在单位四元数群上定义平滑和表现力强的流以及定义适当的密度，在旋转群上进行训练，我们可以成功地采样分子晶体结构。

    

    正则化流(NF)是一类强大的生成模型，由于其高度灵活和表现力，近年来广受欢迎。在本文中，我们介绍了一种新型的正则化流，专为三维空间中多个物体的位置和方向建模而设计，例如晶体中的分子。我们的方法基于两个关键思想:首先，我们在单位四元数群上定义平滑和表现力强的流，从而可以捕捉刚体的连续旋转运动;其次，我们利用单位四元数的双覆盖特性，在旋转群上定义一个适当的密度。这确保我们的模型可以使用标准的基于似然方法或基于热力学目标密度的变分推断进行训练。我们通过训练两个分子示例的Boltzmann生成器来评估该方法，即四面体系统的多模态密度。

    Normalizing flows (NF) are a class of powerful generative models that have gained popularity in recent years due to their ability to model complex distributions with high flexibility and expressiveness. In this work, we introduce a new type of normalizing flow that is tailored for modeling positions and orientations of multiple objects in three-dimensional space, such as molecules in a crystal. Our approach is based on two key ideas: first, we define smooth and expressive flows on the group of unit quaternions, which allows us to capture the continuous rotational motion of rigid bodies; second, we use the double cover property of unit quaternions to define a proper density on the rotation group. This ensures that our model can be trained using standard likelihood-based methods or variational inference with respect to a thermodynamic target density. We evaluate the method by training Boltzmann generators for two molecular examples, namely the multi-modal density of a tetrahedral system 
    
[^183]: 大型语言模型的水印技术

    A Watermark for Large Language Models. (arXiv:2301.10226v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10226](http://arxiv.org/abs/2301.10226)

    本文提出了一种在大型语言模型中实现水印技术的方法，该技术可以在不降低文本质量的前提下嵌入信号，且可以使用高效的开源算法进行检测，并且该技术十分鲁棒和安全。

    

    通过在生成的文本中嵌入信号，即将水印技术应用于模型输出，可以减轻大型语言模型潜在的危害。我们提出了一种专有语言模型的水印技术框架。水印可以嵌入到文本中，对文本质量的影响可以忽略不计，并且可以使用高效的开源算法在不访问语言模型API或参数的情况下进行检测。水印技术通过在生成单词之前选择一组随机的“绿色”标记，然后在抽样过程中软性地推广使用这些标记。我们提出了一个可解释的P值统计检验方法，用于检测水印技术， 并推导了一个信息论框架来分析水印技术的敏感性。我们使用Open Pretrained Transformer（OPT）家族的一个数十亿参数模型来测试水印技术，并讨论了其鲁棒性和安全性。

    Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of "green" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.
    
[^184]: 从两人零和博弈中抽象出不完美信息

    Abstracting Imperfect Information Away from Two-Player Zero-Sum Games. (arXiv:2301.09159v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2301.09159](http://arxiv.org/abs/2301.09159)

    通过正则化均衡，可以将两人零和博弈中的不完美信息抽象出来并作为完全信息问题处理。

    

    Nayyar等人在其开创性的工作中表明，通过在游戏过程中让玩家公开宣布其策略，不完美信息可以被从共同效益游戏中抽象出来。这个见解是支撑共同效益游戏合理的求解器和决策时间规划算法的基础。不幸的是，将同样的见解简单应用于两人零和博弈会失败，因为具有公开策略宣布的游戏的纳什均衡可能与原始游戏的纳什均衡不相对应。因此，现有的合理的决策时间规划算法需要复杂的额外机制，其具有不吸引人的特性。本文的主要贡献是展示某些正则化均衡不具有上述的不对应问题，因此，计算它们可以被视为完全信息问题。因为这些正则化均衡可以被无限接近纳什均衡，我们的结果为一种新的视角打开了大门。

    In their seminal work, Nayyar et al. (2013) showed that imperfect information can be abstracted away from common-payoff games by having players publicly announce their policies as they play. This insight underpins sound solvers and decision-time planning algorithms for common-payoff games. Unfortunately, a naive application of the same insight to two-player zero-sum games fails because Nash equilibria of the game with public policy announcements may not correspond to Nash equilibria of the original game. As a consequence, existing sound decision-time planning algorithms require complicated additional mechanisms that have unappealing properties. The main contribution of this work is showing that certain regularized equilibria do not possess the aforementioned non-correspondence problem -- thus, computing them can be treated as perfect-information problems. Because these regularized equilibria can be made arbitrarily close to Nash equilibria, our result opens the door to a new perspectiv
    
[^185]: 层次平衡：实现基于潜在因果因素的动态公平性

    Tier Balancing: Towards Dynamic Fairness over Underlying Causal Factors. (arXiv:2301.08987v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08987](http://arxiv.org/abs/2301.08987)

    本研究提出了层次平衡的概念，该概念捕捉了未观察到的潜在因果因素的情况变化，并探讨了动态公平性的实现。在指定的动态下，通常不能通过一步干预来实现长期公平目标。

    

    实现长期公平性需要决策和底层数据生成过程之间的相互作用。本文通过在决策和分布交互作用上进行因果建模，并提出层次平衡的概念，从动态的角度探讨实现长期公平性的可能性。与之前仅在观察变量上定义公平概念的方法不同，我们的方法在进一步捕捉未观察到的潜在因果因素方面更为精确。在指定的动态下，我们证明通常不能通过一步干预来实现长期公平目标。此外，在努力实现长期公平性的过程中，我们考虑调节公平度和性能度之间的权衡。

    The pursuit of long-term fairness involves the interplay between decision-making and the underlying data generating process. In this paper, through causal modeling with a directed acyclic graph (DAG) on the decision-distribution interplay, we investigate the possibility of achieving long-term fairness from a dynamic perspective. We propose Tier Balancing, a technically more challenging but more natural notion to achieve in the context of long-term, dynamic fairness analysis. Different from previous fairness notions that are defined purely on observed variables, our notion goes one step further, capturing behind-the-scenes situation changes on the unobserved latent causal factors that directly carry out the influence from the current decision to the future data distribution. Under the specified dynamics, we prove that in general one cannot achieve the long-term fairness goal only through one-step interventions. Furthermore, in the effort of approaching long-term fairness, we consider th
    
[^186]: 一种累积损失下联邦学习的通信高效自适应算法

    A Communication-Efficient Adaptive Algorithm for Federated Learning under Cumulative Regret. (arXiv:2301.08869v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08869](http://arxiv.org/abs/2301.08869)

    本文提出了一种通信高效自适应算法于联邦学习中，通过低比特数的传输实现了最优累积损失。

    

    本文考虑了分布式设置下$M$个客户端通过中央服务器进行在线随机优化的问题。我们开发了一种分布式在线学习算法，通过在整个学习过程中传输的比特数低而实现了一阶最优累积损失。这与现有的研究侧重于用简单失效率来衡量离线度量的做法形成了对比。综合衡量通信成本的方法也与现有做法不同，现有做法通常将通信频率和每次通信传输的比特数分开考虑。

    We consider the problem of online stochastic optimization in a distributed setting with $M$ clients connected through a central server. We develop a distributed online learning algorithm that achieves order-optimal cumulative regret with low communication cost measured in the total number of bits transmitted over the entire learning horizon. This is in contrast to existing studies which focus on the offline measure of simple regret for learning efficiency. The holistic measure for communication cost also departs from the prevailing approach that \emph{separately} tackles the communication frequency and the number of bits in each communication round.
    
[^187]: 用于评估CNN预测的可信度分数

    Trustworthiness Score to Evaluate CNNs Predictions. (arXiv:2301.08839v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08839](http://arxiv.org/abs/2301.08839)

    本文提出了一种用于评估CNN预测可信度的可信度分数（TS）度量标准，通过检查CNN所做预测中的某些特征的存在来量化预测的可信度。

    This paper proposes a trustworthiness score (TS) metric to evaluate the confidence of CNN predictions, which quantifies the trustworthiness by checking for the existence of certain features in the predictions made by the CNN.

    由于卷积神经网络（CNN）的黑盒特性，无法在操作期间持续验证CNN，这使得开发人员和监管机构难以对使用CNN的自主系统的部署获得信心。在操作期间，了解CNN的预测何时可信或可疑对安全至关重要。基本方法是使用模型的输出置信度分数来评估预测是否可信或可疑。然而，模型的置信度分数是来自黑盒计算的结果，因此缺乏透明度，使得很难将可信度归因于预测。我们引入了可信度分数（TS），这是一种简单的度量标准，提供了一种更透明和有效的方式来提供CNN预测的信心。该度量标准通过检查CNN所做预测中的某些特征的存在来量化预测的可信度。

    Due to the black box nature of Convolutional neural networks (CNNs), the continuous validation of CNNs during operation is infeasible. As a result this makes it difficult for developers and regulators to gain confidence in the deployment of autonomous systems employing CNNs. It is critical for safety during operation to know when a CNN's predictions are trustworthy or suspicious. The basic approach is to use the model's output confidence score to assess if predictions are trustworthy or suspicious. However, the model's confidence score is a result of computations coming from a black box, therefore lacks transparency and makes it challenging to credit trustworthiness to predictions. We introduce the trustworthiness score (TS), a simple metric that provides a more transparent and effective way of providing confidence in CNNs predictions. The metric quantifies the trustworthiness in a prediction by checking for the existence of certain features in the predictions made by the CNN. The TS m
    
[^188]: 无平滑非凸随机优化问题的更快无梯度算法

    Faster Gradient-Free Algorithms for Nonsmooth Nonconvex Stochastic Optimization. (arXiv:2301.06428v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2301.06428](http://arxiv.org/abs/2301.06428)

    本文提出了一种使用随机递归梯度估计器的更高效算法，来解决无平滑非凸随机优化问题，其复杂度为 $\mathcal{O}(L^3 d^{3/2} \epsilon^{-3}+ \Delta L^2 d^{3/2} \delta^{-1} \epsilon^{-3})$。

    

    本文考虑形如 $\min_{x \in \mathbb{R}^d} f(x) \triangleq \mathbb{E}_{\xi} [F(x; \xi)]$ 的优化问题，其中分量 $F(x;\xi)$ 是 $L$ 平均均方偏差的 Lipschitz 但可能是非凸非光滑函数。最近提出的无梯度方法最多需要 $\mathcal{O}( L^4 d^{3/2} \epsilon^{-4} + \Delta L^3 d^{3/2} \delta^{-1} \epsilon^{-4})$ 的随机零阶预处理器复杂度来找到目标函数的 $(\delta,\epsilon)$-Goldstein 静止点，其中 $\Delta = f(x_0) - \inf_{x \in \mathbb{R}^d} f(x)$，$x_0$ 是算法的初始点。本文提出了一种更高效的算法，使用随机递归梯度估计器，将复杂度改进为 $\mathcal{O}(L^3 d^{3/2} \epsilon^{-3}+ \Delta L^2 d^{3/2} \delta^{-1} \epsilon^{-3})$。

    We consider the optimization problem of the form $\min_{x \in \mathbb{R}^d} f(x) \triangleq \mathbb{E}_{\xi} [F(x; \xi)]$, where the component $F(x;\xi)$ is $L$-mean-squared Lipschitz but possibly nonconvex and nonsmooth. The recently proposed gradient-free method requires at most $\mathcal{O}( L^4 d^{3/2} \epsilon^{-4} + \Delta L^3 d^{3/2} \delta^{-1} \epsilon^{-4})$ stochastic zeroth-order oracle complexity to find a $(\delta,\epsilon)$-Goldstein stationary point of objective function, where $\Delta = f(x_0) - \inf_{x \in \mathbb{R}^d} f(x)$ and $x_0$ is the initial point of the algorithm. This paper proposes a more efficient algorithm using stochastic recursive gradient estimators, which improves the complexity to $\mathcal{O}(L^3 d^{3/2} \epsilon^{-3}+ \Delta L^2 d^{3/2} \delta^{-1} \epsilon^{-3})$.
    
[^189]: NarrowBERT: 加速掩码语言模型的预训练和推理

    NarrowBERT: Accelerating Masked Language Model Pretraining and Inference. (arXiv:2301.04761v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.04761](http://arxiv.org/abs/2301.04761)

    NarrowBERT是一种改进的Transformer编码器，通过稀疏化模型并仅对掩码令牌进行操作，在掩码语言模型预训练中提高了$2\times$以上的吞吐量，并在推理时将吞吐量提高了多达$3.5\times$。NarrowBERT在几种自然语言处理任务中的性能与标准BERT相当。

    

    大规模语言模型预训练是自监督学习在自然语言处理中非常成功的形式，但随着模型和预训练语料库的不断增大，执行预训练的成本变得越来越高。我们提出了NarrowBERT，一种改进的Transformer编码器，通过$2\times$以上的速度提高了掩码语言模型预训练的吞吐量。NarrowBERT稀疏化了Transformer模型，在预训练期间，自注意力查询和前馈层仅对每个句子的掩码令牌进行操作，而不是像通常的Transformer编码器那样对所有令牌进行操作。我们还展示了NarrowBERT在推理时将吞吐量提高了多达$3.5\times$，在像MNLI这样的句子编码任务上，性能几乎没有或没有明显降低。最后，我们考察了NarrowBERT在IMDB和Amazon评论分类以及CoNLL NER任务上的性能，并展示其与标准BERT的性能相当。

    Large-scale language model pretraining is a very successful form of self-supervised learning in natural language processing, but it is increasingly expensive to perform as the models and pretraining corpora have become larger over time. We propose NarrowBERT, a modified transformer encoder that increases the throughput for masked language model pretraining by more than $2\times$. NarrowBERT sparsifies the transformer model such that the self-attention queries and feedforward layers only operate on the masked tokens of each sentence during pretraining, rather than all of the tokens as with the usual transformer encoder. We also show that NarrowBERT increases the throughput at inference time by as much as $3.5\times$ with minimal (or no) performance degradation on sentence encoding tasks like MNLI. Finally, we examine the performance of NarrowBERT on the IMDB and Amazon reviews classification and CoNLL NER tasks and show that it is also comparable to standard BERT performance.
    
[^190]: 针对非相同客户类别集合的对齐导航：一种基于标签名称锚定的联邦学习框架

    Navigating Alignment for Non-identical Client Class Sets: A Label Name-Anchored Federated Learning Framework. (arXiv:2301.00489v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.00489](http://arxiv.org/abs/2301.00489)

    本文提出了一种名为FedAlign的联邦学习框架，用于在标签和数据的层面上对于非相同客户类别集合进行潜在空间的对齐，从而解决了传统方法中不同客户端本地编码器的异质性问题。

    

    传统的联邦分类方法即使是为非IID客户设计的，也假设每个客户端都根据相同的通用类集对其本地数据进行注释。本文关注于更一般但实用的场景——非相同客户类集，其中客户端关注于自己（不同甚至不重叠）的类集，并寻求适用于这些类的联合的全局模型。如果将分类视为在数据/标签编码器产生的表示之间找到最佳匹配，则客户类别集合的异质性会带来新的重大挑战-不同客户端的本地编码器可能在不同甚至独立的潜在空间中运行，使得在服务器上进行聚合变得困难。我们提出了一种新颖的框架，FedAlign，从标签和数据的角度来对齐客户端的潜在空间。从标签的角度，我们利用表达自然语言的类名作为标签编码器的共同基础来实现对齐。

    Traditional federated classification methods, even those designed for non-IID clients, assume that each client annotates its local data with respect to the same universal class set. In this paper, we focus on a more general yet practical setting, non-identical client class sets, where clients focus on their own (different or even non-overlapping) class sets and seek a global model that works for the union of these classes. If one views classification as finding the best match between representations produced by data/label encoder, such heterogeneity in client class sets poses a new significant challenge -local encoders at different clients may operate in different and even independent latent spaces, making it hard to aggregate at the server. We propose a novel framework, FedAlign, to align the latent spaces across clients from both label and data perspectives. From a label perspective, we leverage the expressive natural language class names as a common ground for label encoders to an
    
[^191]: 基于交互式模拟环境的人机协同体现智能在外科手术机器人学习中的应用

    Human-in-the-loop Embodied Intelligence with Interactive Simulation Environment for Surgical Robot Learning. (arXiv:2301.00452v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2301.00452](http://arxiv.org/abs/2301.00452)

    本文介绍了一种基于交互式模拟环境的人机协同体现智能机器人学习的新平台，以及人类专家指导下机器人学习表现的提高和人体示范对手术机器人控制策略的影响。

    

    在过去的十年里，外科手术机器人自动化已经吸引了越来越多的研究兴趣，预期其潜力能够惠及外科医生、护士和患者。最近，具有体现智能的学习范式展示了学习各种复杂任务的良好控制策略的能力，其中体现智能模拟器在促进相关研究方面发挥了核心作用。然而，现有的外科手术机器人开源模拟器仍未足够支持通过物理输入设备进行人机交互，这进一步限制了对人体示范如何影响策略学习的有效调查。在这项工作中，我们研究了一种基于交互式模拟平台的人机协同体现智能，用于外科手术机器人的学习。具体而言，我们建立了基于先前发布的SurRoL模拟器的平台，并与其他工程师一起开发了几个新功能，以允许通过输入设备进行高质量的人机交互。我们展示了在人类专家的指导下，机器人学习表现的提高，并确定了不同类型人体示范对手术机器人学习的控制策略所产生的影响。

    Surgical robot automation has attracted increasing research interest over the past decade, expecting its potential to benefit surgeons, nurses and patients. Recently, the learning paradigm of embodied intelligence has demonstrated promising ability to learn good control policies for various complex tasks, where embodied AI simulators play an essential role to facilitate relevant research. However, existing open-sourced simulators for surgical robot are still not sufficiently supporting human interactions through physical input devices, which further limits effective investigations on how the human demonstrations would affect policy learning. In this work, we study human-in-the-loop embodied intelligence with a new interactive simulation platform for surgical robot learning. Specifically, we establish our platform based on our previously released SurRoL simulator with several new features co-developed to allow high-quality human interaction via an input device. We showcase the improveme
    
[^192]: QuantArt：量化图像风格转移以实现视觉高保真度

    QuantArt: Quantizing Image Style Transfer Towards High Visual Fidelity. (arXiv:2212.10431v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.10431](http://arxiv.org/abs/2212.10431)

    QuantArt是一个新的风格转移框架，通过向量量化将生成的艺术品的潜在表示推向真实艺术品分布的中心，从而实现高可视化保真的样式化。该框架实现了显著更高的视觉保真度。

    

    现有的风格转移算法的机制是通过最小化混合损失函数来推动生成的图像朝向内容和风格的高度相似性。然而，这种方法不能保证视觉保真度，即生成的艺术作品应该与真实的艺术品难以区分。在本文中，我们设计了一个称为QuantArt的新风格转移框架，以实现高可视化保真的样式化。QuantArt通过向量量化将生成的艺术品的潜在表示推向真实艺术品分布的中心。通过融合量化和连续的潜在表示，QuantArt允许在内容保留、风格相似性和视觉保真度方面对生成的艺术品进行灵活控制。在各种风格转移设置上的实验证明，我们的QuantArt框架与现有的风格转移方法相比，可以实现显著更高的视觉保真度。

    The mechanism of existing style transfer algorithms is by minimizing a hybrid loss function to push the generated image toward high similarities in both content and style. However, this type of approach cannot guarantee visual fidelity, i.e., the generated artworks should be indistinguishable from real ones. In this paper, we devise a new style transfer framework called QuantArt for high visual-fidelity stylization. QuantArt pushes the latent representation of the generated artwork toward the centroids of the real artwork distribution with vector quantization. By fusing the quantized and continuous latent representations, QuantArt allows flexible control over the generated artworks in terms of content preservation, style similarity, and visual fidelity. Experiments on various style transfer settings show that our QuantArt framework achieves significantly higher visual fidelity compared with the existing style transfer methods.
    
[^193]: 在场学习者能否从演示中学习推理概念？

    Can In-context Learners Learn a Reasoning Concept from Demonstrations?. (arXiv:2212.01692v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.01692](http://arxiv.org/abs/2212.01692)

    本文介绍了一种概念性少样本学习方法，以帮助在场学习者学习新技能。通过选择与预测示例共享可能信息的演示，这个方法可以在模型记忆独立的情况下区分模型的在场学习能力。

    

    大型语言模型展示了从少量输入-输出演示中学习新任务的新能力。然而，最近的研究表明，在场学习者大部分依赖于他们的预训练知识，如标签的情感，而不是在输入中找到新的关联性。然而，常用的少样本评估设置使用随机选择的在场演示无法区分模型从演示中学习新技能的能力，因为大部分随机选择的演示并不呈现超越暴露于新任务分布的预测的关系。为了在模型记忆独立的情况下区分模型的在场学习能力，我们引入了一个概念性少样本学习方法，选择与预测示例共享可能信息的演示。我们从注释解释中提取了一组这样的概念，并测量了模型展示这些概念可以获得多少好处。

    Large language models show an emergent ability to learn a new task from a small number of input-output demonstrations. However, recent work shows that in-context learners largely rely on their pre-trained knowledge, such as the sentiment of the labels, instead of finding new associations in the input. However, the commonly-used few-shot evaluation settings using a random selection of in-context demonstrations can not disentangle models' ability to learn a new skill from demonstrations, as most of the randomly-selected demonstrations do not present relations informative for prediction beyond exposing the new task distribution.  To disentangle models' in-context learning ability independent of models' memory, we introduce a Conceptual few-shot learning method selecting the demonstrations sharing a possibly-informative concept with the predicted sample. We extract a set of such concepts from annotated explanations and measure how much can models benefit from presenting these concepts in f
    
[^194]: 基于拓扑数据分析的语音处理

    Topological Data Analysis for Speech Processing. (arXiv:2211.17223v3 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2211.17223](http://arxiv.org/abs/2211.17223)

    本论文将拓扑数据分析应用于语音分类问题和预训练语音模型的内省，并介绍了一系列基于Transformer注意力图和嵌入的拓扑和代数特征。在这些特征基础上构建的简单线性分类器胜过精调分类器头部，并实现了在许多数据集上的最新最优性能。拓扑特征能够揭示语音Transformer头的功能角色，这表明TDA是一种有前途的语音分析方法。

    

    我们将拓扑数据分析（TDA）应用于语音分类问题及预训练语音模型HuBERT的内省。为此，我们介绍了一些基于Transformer注意力图和嵌入的拓扑和代数特征。我们证明，在这些特征基础上构建的简单线性分类器胜过精调分类器头部。特别地，在四个常见数据集上，我们实现了约9%的准确率提高和5%的ERR提高。在CREMA-D数据集上，提出的特征集达到了准确率80.155的新的最优性能。我们还展示了拓扑特征能够揭示语音Transformer头的功能角色。例如，我们发现在没有任何下游精调的情况下，这些头可区分样本来源（自然/合成）或声音对。我们的结果表明，TDA是一种有前途的语音分析方法，尤其是对于需要结构预测的任务。此外，我们还提供了附录、对TDA和HuBERT模型的介绍以及使用其他数据集的实验。

    We apply topological data analysis (TDA) to speech classification problems and to the introspection of a pretrained speech model, HuBERT. To this end, we introduce a number of topological and algebraic features derived from Transformer attention maps and embeddings. We show that a simple linear classifier built on top of such features outperforms a fine-tuned classification head. In particular, we achieve an improvement of about $9\%$ accuracy and $5\%$ ERR on four common datasets; on CREMA-D, the proposed feature set reaches a new state of the art performance with accuracy $80.155$. We also show that topological features are able to reveal functional roles of speech Transformer heads; e.g., we find the heads capable to distinguish between pairs of sample sources (natural/synthetic) or voices without any downstream fine-tuning. Our results demonstrate that TDA is a promising new approach for speech analysis, especially for tasks that require structural prediction. Appendices, an introd
    
[^195]: ArrayFlex:一种可配置透明流水线的并行阵列结构

    ArrayFlex: A Systolic Array Architecture with Configurable Transparent Pipelining. (arXiv:2211.12600v2 [cs.AR] UPDATED)

    [http://arxiv.org/abs/2211.12600](http://arxiv.org/abs/2211.12600)

    设计了一种可配置透明流水线的并行阵列，ArrayFlex，可以选择每个CNN层的最佳流水线配置，从而缩短推理延迟11％，使用比传统固定流水线阵列少13％的能量。

    

    卷积神经网络是许多深度学习应用的最先进解决方案。为了最大程度的可伸缩性，它们的计算应该结合高性能和能源效率。在实践中，每个CNN层的卷积都被映射到一个矩阵乘法中，并包括每层的所有输入特征和卷积核，这个过程是使用并行阵列进行计算的。在这项工作中，我们关注的是带有可配置流水线的并行阵列的设计，目标是选择每个CNN层的最佳流水线配置。所提出的并行阵列称为ArrayFlex，可以在正常或浅流水线模式下运行，从而平衡执行周期时间和操作时钟频率。通过选择每个CNN层的适当流水线配置，ArrayFlex将最先进的CNN的推理延迟平均缩短了11％，与传统的固定流水线并行阵列相比。最重要的是，在使用比固定流水线阵列少13％的能量的同时实现了这个结果。

    Convolutional Neural Networks (CNNs) are the state-of-the-art solution for many deep learning applications. For maximum scalability, their computation should combine high performance and energy efficiency. In practice, the convolutions of each CNN layer are mapped to a matrix multiplication that includes all input features and kernels of each layer and is computed using a systolic array. In this work, we focus on the design of a systolic array with configurable pipeline with the goal to select an optimal pipeline configuration for each CNN layer. The proposed systolic array, called ArrayFlex, can operate in normal, or in shallow pipeline mode, thus balancing the execution time in cycles and the operating clock frequency. By selecting the appropriate pipeline configuration per CNN layer, ArrayFlex reduces the inference latency of state-of-the-art CNNs by 11%, on average, as compared to a traditional fixed-pipeline systolic array. Most importantly, this result is achieved while using 13%
    
[^196]: 基于检索的多模态语言建模

    Retrieval-Augmented Multimodal Language Modeling. (arXiv:2211.12561v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.12561](http://arxiv.org/abs/2211.12561)

    本文提出了一种新的基于检索的多模态模型，使其可以从外部记忆检索相关图片和文本信息，以更可扩展和模块化的方式集成知识，提供更高质量的生成结果。

    

    最近出现的多模态模型如DALL-E和CM3在文本与图像生成方面取得了显著进展。但是，这些模型将所有学习到的知识（例如埃菲尔铁塔的外观）存储在模型参数中，需要越来越大的模型和训练数据来捕捉更多的知识。为了以更可扩展和模块化的方式集成知识，我们提出了一种检索增强的多模态模型，使基本的多模态模型（生成器）能够参考由检索器从外部记忆（例如网上的文本和图像）中提取的相关信息。具体来说，我们使用预训练的CLIP作为检索器，使用在LAION数据集上训练的CM3 Transformer作为生成器。我们得到的模型称为检索增强CM3（RA-CM3），是第一个可以检索和生成文本和图像的多模态模型。我们证明RA-CM3在图像和图片生成方面均显著优于基线多模态模型如DALL-E和CM3，并且其检索模块可以有效地整合外部知识，以提高所生成结果的质量。

    Recent multimodal models such as DALL-E and CM3 have achieved remarkable progress in text-to-image and image-to-text generation. However, these models store all learned knowledge (e.g., the appearance of the Eiffel Tower) in the model parameters, requiring increasingly larger models and training data to capture more knowledge. To integrate knowledge in a more scalable and modular way, we propose a retrieval-augmented multimodal model, which enables a base multimodal model (generator) to refer to relevant text and images fetched by a retriever from external memory (e.g., documents on the web). Specifically, for the retriever, we use a pretrained CLIP, and for the generator, we train a CM3 Transformer on the LAION dataset. Our resulting model, named Retrieval-Augmented CM3 (RA-CM3), is the first multimodal model that can retrieve and generate both text and images. We show that RA-CM3 significantly outperforms baseline multimodal models such as DALL-E and CM3 on both image and caption gen
    
[^197]: SmoothQuant：用于大型语言模型的精确高效的后训练量化方法

    SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models. (arXiv:2211.10438v5 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.10438](http://arxiv.org/abs/2211.10438)

    SmoothQuant是一种训练无需的通用后训练量化（PTQ）解决方案，可以在保持精度的情况下实现大型语言模型的8位权重、8位激活（W8A8）量化。SmoothQuant通过数学等效转换将量化难度从激活移到权重，使得所有矩阵乘法的权重和激活的INT8量化成为可能，具有最高1.56倍加速和2倍内存减少的效果。

    

    大型语言模型（LLMs）表现出优异的性能，但需要大量计算和内存。量化可以减少内存并加速推理。然而，现有方法无法在保持精度和硬件效率的同时维持。我们提出了SmoothQuant，一种无需训练、保持精度和通用的后训练量化（PTQ）解决方案，以实现LLMs的8位权重、8位激活（W8A8）量化。基于权重易于量化而激活不易量化的事实，SmoothQuant通过数学等效转换将量化难度从激活移至权重，通过离线平滑激活的异常值来实现此目标。SmoothQuant使所有矩阵乘法的权重和激活的INT8量化成为可能，包括OPT、BLOOM、GLM、MT-NLG和LLaMA系列。我们演示了LLMs的最高1.56倍加速和2倍内存减少，并且几乎不会有精度损失。SmoothQuant可以为530B LLM提供服务。

    Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, and LLaMA family. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM wi
    
[^198]: 机器学习的Calabi-Yau度量和曲率

    Machine Learned Calabi-Yau Metrics and Curvature. (arXiv:2211.09801v3 [hep-th] UPDATED)

    [http://arxiv.org/abs/2211.09801](http://arxiv.org/abs/2211.09801)

    这篇论文使用机器学习方法构建了逼近Calabi-Yau度量的神经网络模型，研究了光滑和奇异K3曲面和Calabi-Yau三体上的数值Ricci-flat度量，并观察到神经网络模型对于计算出的拓扑特征的稳定性有较大影响。

    

    寻找Ricci-flat（Calabi-Yau）度量是几何学中一个长期存在的问题，对于弦论和现象学有着深远的影响。解决这个问题的新方法是使用神经网络构建在给定Kähler类中逼近Calabi-Yau度量。在本文中，我们研究了光滑和奇异K3曲面和Calabi-Yau三体上的数值Ricci-flat度量。使用这些度量逼近Cefalù家族的四次型双曲面和Dwork家族的五次型三体，我们研究这些几何体上的特征形式。我们观察到，数值计算出的拓扑特征的稳定性受到神经网络模型选择的影响较大，我们简要讨论了一种不同的神经网络模型，即谱网络，它可以正确逼近Calabi-Yau的拓扑特性。使用持久同调，我们展示了流形的高曲率区域形成一个聚类。

    Finding Ricci-flat (Calabi-Yau) metrics is a long standing problem in geometry with deep implications for string theory and phenomenology. A new attack on this problem uses neural networks to engineer approximations to the Calabi-Yau metric within a given K\"ahler class. In this paper we investigate numerical Ricci-flat metrics over smooth and singular K3 surfaces and Calabi-Yau threefolds. Using these Ricci-flat metric approximations for the Cefal\'u family of quartic twofolds and the Dwork family of quintic threefolds, we study characteristic forms on these geometries. We observe that the numerical stability of the numerically computed topological characteristic is heavily influenced by the choice of the neural network model, in particular, we briefly discuss a different neural network model, namely Spectral networks, which correctly approximate the topological characteristic of a Calabi-Yau. Using persistent homology, we show that high curvature regions of the manifolds form cluster
    
[^199]: 关于优化器选择提高模型对分布似然小变化的泛化性能的实证研究

    Empirical Study on Optimizer Selection for Out-of-Distribution Generalization. (arXiv:2211.08583v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08583](http://arxiv.org/abs/2211.08583)

    本实证研究通过测试不同分布偏移下超过20,000个模型的表现，发现自适应优化器（如Adam和Adagrad）及可以减少时间相关特征的优化器具有更好的分布外泛化性能。

    

    当测试数据分布与训练数据分布稍有不同时，现代深度学习系统的泛化能力不佳。虽然已经有了很多有前途的工作来解决这个问题，但对优化器及其在分布外泛化性能中的作用进行系统研究尚未进行。本研究通过在经验风险最小化和不变风险最小化下，使用DomainBed，WILDS和Backgrounds Challenge分别作为测试平台研究不同类型偏移（即相关性和多样性变化）对图像和文本分类的影响，并使用广泛的超参数范围搜索并测试超过20,000个模型的分类准确性（分布内和分布外）。我们的研究得出以下结论，我们相信对实践者是有帮助的，i) 自适应优化器（例如 Adam 和 Adagrad）具有泛化性能更好。ii) 当偏移有很强的时间局部性时，能够减少时间相关特征的优化器具有泛化性能更好，而当偏移有强的时间整体性时则没有性能优势。

    Modern deep learning systems do not generalize well when the test data distribution is slightly different to the training data distribution. While much promising work has been accomplished to address this fragility, a systematic study of the role of optimizers and their out-of-distribution generalization performance has not been undertaken. In this study, we examine the performance of popular first-order optimizers for different classes of distributional shift under empirical risk minimization and invariant risk minimization. We address this question for image and text classification using DomainBed, WILDS, and Backgrounds Challenge as testbeds for studying different types of shifts -- namely correlation and diversity shift. We search over a wide range of hyperparameters and examine classification accuracy (in-distribution and out-of-distribution) for over 20,000 models. We arrive at the following findings, which we expect to be helpful for practitioners: i) adaptive optimizers (e.g., 
    
[^200]: 基于策略外学习的可实现性何时足够？

    When is Realizability Sufficient for Off-Policy Reinforcement Learning?. (arXiv:2211.05311v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.05311](http://arxiv.org/abs/2211.05311)

    本文研究了基于模型的算法对于满足贝尔曼完整性条件的要求，提出了新的基于策略外强化学习的有限样本保证。

    

    对于强化学习而言，基于模型的算法通常需要满足贝尔曼完整性条件，以便在函数逼近的情况下成功地进行基于策略外的操作。但贝尔曼完整性是一种比实际要求更强的条件，难以在实践中得到满足。本文放松了这一条件，研究了仅在所规定的函数类具有可实现性时，基于策略外强化学习的统计复杂性，提出了基于策略外强化学习的有限样本保证，不受逼近误差影响，取决于三个因素的相互作用。

    Model-free algorithms for reinforcement learning typically require a condition called Bellman completeness in order to successfully operate off-policy with function approximation, unless additional conditions are met. However, Bellman completeness is a requirement that is much stronger than realizability and that is deemed to be too strong to hold in practice. In this work, we relax this structural assumption and analyze the statistical complexity of off-policy reinforcement learning when only realizability holds for the prescribed function class.  We establish finite-sample guarantees for off-policy reinforcement learning that are free of the approximation error term known as inherent Bellman error, and that depend on the interplay of three factors. The first two are well known: they are the metric entropy of the function class and the concentrability coefficient that represents the cost of learning off-policy. The third factor is new, and it measures the violation of Bellman complete
    
[^201]: 通过平滑敏感度实现隐私保护的偏差查询

    Can Querying for Bias Leak Protected Attributes? Achieving Privacy With Smooth Sensitivity. (arXiv:2211.02139v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.02139](http://arxiv.org/abs/2211.02139)

    本文论证了查询公平指标可能会泄露受保护属性，提出了解决方案以保护隐私。

    

    现有法规禁止模型开发人员访问受保护属性（性别，种族等），这经常导致在不知道他们的受保护组的情况下对人群进行公平性评估。在这种情况下，机构通常采用模型开发人员（不可访问受保护属性训练模型）和合规团队（可能全面访问数据集以用于审计目的）之间的分离。但是，模型开发人员可能被允许通过查询合规团队获取组公平性指标来测试其模型的偏差。本文首先证明了仅仅查询公平指标（例如统计平等和平等赔率）可能会泄露个人的受保护属性给模型开发人员。我们证明了模型开发人员总是可以通过单个查询从测试数据集中识别目标个体的受保护属性。我们特别展示了一种方法，通过它可以平滑地减小敏感度来解决这个问题并保护隐私。

    Existing regulations prohibit model developers from accessing protected attributes (gender, race, etc.), often resulting in fairness assessments on populations without knowing their protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset for auditing purposes). However, the model developers might be allowed to test their models for bias by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. In particular, we show that one can rec
    
[^202]: 我不想说：在可选个人数据模型中保护用户同意

    I Prefer not to Say: Protecting User Consent in Models with Optional Personal Data. (arXiv:2210.13954v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13954](http://arxiv.org/abs/2210.13954)

    该论文研究了个人可以选择与决策系统共享可选个人信息的机器学习模型，并提出了保护用户同意的PUC概念，为用户隐私保护提供了有力的解决方案。

    

    我们研究了一种机器学习模型，其中个人可以选择与决策系统共享可选个人信息，这在现代保险定价模型中很常见。一些用户同意使用他们的数据，而其他人则反对并保持其数据未公开。本文表明，不共享数据的决定本身可以被视为信息，应该受到保护，以尊重用户的隐私。这一观察结果引发了一个被忽视的问题，即如何确保保护其个人数据的用户不会因此受到任何不利影响。为了解决这个问题，我们对仅使用获得积极用户同意的信息的模型进行了保护要求的正式化。这排除了作出共享数据与否决定所包含的隐含信息。我们提出了Protected User Consent (PUC)概念，这是我们证明在保护要求下损失最小的解决方案。

    We examine machine learning models in a setup where individuals have the choice to share optional personal information with a decision-making system, as seen in modern insurance pricing models. Some users consent to their data being used whereas others object and keep their data undisclosed. In this work, we show that the decision not to share data can be considered as information in itself that should be protected to respect users' privacy. This observation raises the overlooked problem of how to ensure that users who protect their personal data do not suffer any disadvantages as a result. To address this problem, we formalize protection requirements for models which only use the information for which active user consent was obtained. This excludes implicit information contained in the decision to share data or not. We offer the first solution to this problem by proposing the notion of Protected User Consent (PUC), which we prove to be loss-optimal under our protection requirement. To
    
[^203]: 通过建模不规则多模态电子健康记录，改进医疗预测

    Improving Medical Predictions by Irregular Multimodal Electronic Health Records Modeling. (arXiv:2210.12156v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12156](http://arxiv.org/abs/2210.12156)

    本文利用门控机制和时间注意力机制分别建模不规则时间序列和临床笔记表征，进而通过交替注意力机制整合多模态信息，从而改进医疗预测。

    

    电子健康记录 (EHRs) 通常包含数字时间序列和长的临床笔记序列，这些数据都是在不规则时间间隔内采集的。如何处理每个模态中的不规则性，并将其整合到多模态表示中以改进医疗预测，是一个具有挑战性的问题。本文提出的方法首先通过以下两种方式处理单个模态中的不规则性：(1)通过门控机制动态地将手工制作的填充嵌入式表征与学习到的插值嵌入式表征相结合来建模不规则时间序列，(2)通过时间注意力机制将一系列临床笔记表征转换成多变量不规则时间序列，并解决不规则性。对于多模态融合中的不规则性，本文提出了一种交替注意力机制，该机制跨越时间步长进行。据我们所知，这是第一篇彻底对多模态不规则性进行建模的工作。

    Health conditions among patients in intensive care units (ICUs) are monitored via electronic health records (EHRs), composed of numerical time series and lengthy clinical note sequences, both taken at irregular time intervals. Dealing with such irregularity in every modality, and integrating irregularity into multimodal representations to improve medical predictions, is a challenging problem. Our method first addresses irregularity in each single modality by (1) modeling irregular time series by dynamically incorporating hand-crafted imputation embeddings into learned interpolation embeddings via a gating mechanism, and (2) casting a series of clinical note representations as multivariate irregular time series and tackling irregularity via a time attention mechanism. We further integrate irregularity in multimodal fusion with an interleaved attention mechanism across temporal steps. To the best of our knowledge, this is the first work to thoroughly model irregularity in multimodalities
    
[^204]: 手术微调提高了适应分布偏移的效果

    Surgical Fine-Tuning Improves Adaptation to Distribution Shifts. (arXiv:2210.11466v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.11466](http://arxiv.org/abs/2210.11466)

    本研究表明，选择性地微调预训练模型的子集层（手术微调）在适应分布偏移的任务中效果更好，在真实数据中得到了验证，还在理论上证明在理想环境下，手术微调可以优于全层微调。

    

    在分布偏移下的迁移学习中，常见的方法是微调预训练模型的最后几层，保留已学特征同时适应新任务。本文表明，在这种情况下，有选择性地微调预训练模型的子集层（我们称之为手术微调）可以达到与或优于常用的微调方法，且不同类型的分布偏移影响着能够微调的层数。我们在七个真实数据任务中系统验证了这一结论。此外，理论上证明了在理想环境下，手术微调可以胜过全层微调。

    A common approach to transfer learning under distribution shift is to fine-tune the last few layers of a pre-trained model, preserving learned features while also adapting to the new task. This paper shows that in such settings, selectively fine-tuning a subset of layers (which we term surgical fine-tuning) matches or outperforms commonly used fine-tuning approaches. Moreover, the type of distribution shift influences which subset is more effective to tune: for example, for image corruptions, fine-tuning only the first few layers works best. We validate our findings systematically across seven real-world data tasks spanning three types of distribution shifts. Theoretically, we prove that for two-layer neural networks in an idealized setting, first-layer tuning can outperform fine-tuning all layers. Intuitively, fine-tuning more parameters on a small target dataset can cause information learned during pre-training to be forgotten, and the relevant information depends on the type of shif
    
[^205]: 为什么模型会失败？将模型性能变化归因于分布偏移

    "Why did the Model Fail?": Attributing Model Performance Changes to Distribution Shifts. (arXiv:2210.10769v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10769](http://arxiv.org/abs/2210.10769)

    本文介绍了一种将模型性能变化归因于底层数据生成机制的分布偏移的方法，并通过推导一种重要性权重方法来计算任意一组分布的价值。

    

    机器学习模型在分布偏移下经常会出现性能下降的情况。这种偏移的根本原因可能是多重的因素，比如数据质量的变化、特定协变量分布的差异或者标签与特征之间的关系变化等。当模型在部署时失败时，将性能变化归因于这些因素对于模型开发人员来说至关重要，以识别根本原因并采取缓解措施。在本文中，我们介绍了将环境之间的性能差异归因于底层数据生成机制的分布偏移问题。我们将该问题构造为一种合作博弈的形式，其中玩家是分布。我们定义一组分布的价值为当只有这组分布在环境之间发生变化时模型性能的变化，并推导出一种重要性权重方法以计算任意一组分布的价值。

    Machine learning models frequently experience performance drops under distribution shifts. The underlying cause of such shifts may be multiple simultaneous factors such as changes in data quality, differences in specific covariate distributions, or changes in the relationship between label and features. When a model does fail during deployment, attributing performance change to these factors is critical for the model developer to identify the root cause and take mitigating actions. In this work, we introduce the problem of attributing performance differences between environments to distribution shifts in the underlying data generating mechanisms. We formulate the problem as a cooperative game where the players are distributions. We define the value of a set of distributions to be the change in model performance when only this set of distributions has changed between environments, and derive an importance weighting method for computing the value of an arbitrary set of distributions. The
    
[^206]: 基于核函数的语言模型微调视角

    A Kernel-Based View of Language Model Fine-Tuning. (arXiv:2210.05643v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.05643](http://arxiv.org/abs/2210.05643)

    本文研究神经切线核 (NTK) 在描述预训练语言模型微调过程中的适用性。实验证明在14个NLP任务中使用掩码词预测问题作为下游任务，可以取得好的效果。

    

    在自然语言处理中，通过对预训练语言模型 (LMs) 进行微调，在低数据情况下解决 NLP 任务已经成为标准做法。但是，目前对于经验成功背后的理论机制了解很少，例如为什么在几十个训练点上微调一个有 $10^8$ 个或更多参数的模型不会导致过拟合。本文研究了神经切线核 (NTK) 在描述预训练语言模型的微调过程中的适用性。我们扩展了 NTK 形式化方法以应用于 Adam，并使用 Tensor Programs 描述了 NTK 适用于描述预训练语言模型微调更新的条件。我们在 14 个 NLP 任务上进行了广泛的实验验证了我们的理论，并表明通过提示将下游任务表述为掩码词预测问题可以取得良好的效果。

    It has become standard to solve NLP tasks by fine-tuning pre-trained language models (LMs), especially in low-data settings. There is minimal theoretical understanding of empirical success, e.g., why fine-tuning a model with $10^8$ or more parameters on a couple dozen training points does not result in overfitting. We investigate whether the Neural Tangent Kernel (NTK) - which originated as a model to study the gradient descent dynamics of infinitely wide networks with suitable random initialization - describes fine-tuning of pre-trained LMs. This study was inspired by the decent performance of NTK for computer vision tasks (Wei et al., 2022). We extend the NTK formalism to Adam and use Tensor Programs (Yang, 2020) to characterize conditions under which the NTK lens may describe fine-tuning updates to pre-trained language models. Extensive experiments on 14 NLP tasks validate our theory and show that formulating the downstream task as a masked word prediction problem through prompting 
    
[^207]: 少即是多：面向任务的分层蒸馏用于语言模型压缩

    Less is More: Task-aware Layer-wise Distillation for Language Model Compression. (arXiv:2210.01351v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.01351](http://arxiv.org/abs/2210.01351)

    本文提出了一种新的面向任务的分层蒸馏方法（TED），通过设计任务感知的滤波器来对齐学生和教师的隐藏表示，选择有用的知识，减少知识差距，使学生模型更好地适应目标任务，实现了比最先进方法更少的参数下可比或更好的性能。

    

    分层蒸馏是将大模型（即教师模型）压缩为小模型（即学生模型）的强大工具。学生通过在每个中间层模仿教师的隐藏表示来从教师中蒸馏知识。然而，分层蒸馏也存在一些挑战。由于学生的模型容量比教师小，它通常会出现欠拟合;此外，教师的隐藏表示包含了学生未必需要的冗余信息。为了解决这些问题，我们提出了一种新颖的面向任务的分层蒸馏（TED）方法。TED设计任务感知滤波器来对齐每一层的学生和教师的隐藏表示。这些滤波器从隐藏表示中选择对目标任务有用的知识。因此，TED减少了两个模型之间的知识差距，并帮助学生更好地适应目标任务。我们在多种语言模型任务中评估了TED，并表明它可以在比最先进的方法少得多的参数情况下实现可比或甚至更好的性能。

    Layer-wise distillation is a powerful tool to compress large models (i.e. teacher models) into small ones (i.e., student models). The student distills knowledge from the teacher by mimicking the hidden representations of the teacher at every intermediate layer. However, layer-wise distillation is difficult. Since the student has a smaller model capacity than the teacher, it is often under-fitted. Furthermore, the hidden representations of the teacher contain redundant information that the student does not necessarily need for the target task's learning. To address these challenges, we propose a novel Task-aware layEr-wise Distillation (TED). TED designs task-aware filters to align the hidden representations of the student and the teacher at each layer. The filters select the knowledge that is useful for the target task from the hidden representations. As such, TED reduces the knowledge gap between the two models and helps the student to fit better on the target task. We evaluate TED in
    
[^208]: 通过冗余性实现稀疏性：用SGD求解$L_1$

    Sparsity by Redundancy: Solving $L_1$ with SGD. (arXiv:2210.01212v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01212](http://arxiv.org/abs/2210.01212)

    该论文提出了一种通过冗余重参数化和简单的随机梯度下降来最小化带有$L_1$惩罚的通用可微损失函数的方法，称为\textit{spred}，是$L_1$的精确求解器，可用于训练稀疏神经网络以执行基因选择任务和神经网络压缩任务，弥合了深度学习中的稀疏性和传统统计学习之间的差距。

    This paper proposes a method called "spred" to minimize a generic differentiable loss function with $L_1$ penalty using redundant reparametrization and straightforward stochastic gradient descent. It is an exact solver of $L_1$ and can be used to train sparse neural networks for gene selection tasks and neural network compression tasks, bridging the gap between sparsity in deep learning and conventional statistical learning.

    我们提出了一种通过冗余重参数化和简单的随机梯度下降来最小化带有$L_1$惩罚的通用可微损失函数的方法。我们的提议是$L_1$惩罚等价于带有权重衰减的可微重参数化的直接推广。我们证明了所提出的方法，即\textit{spred}，是$L_1$的精确求解器，并且对于通用的非凸函数，重参数化技巧是完全“良性”的。在实践中，我们展示了该方法的实用性，包括(1)训练稀疏神经网络以执行基因选择任务，其中涉及在非常高维空间中找到相关特征，以及(2)神经网络压缩任务，先前尝试应用$L_1$惩罚的方法均未成功。从概念上讲，我们的结果弥合了深度学习中的稀疏性和传统统计学习之间的差距。

    We propose to minimize a generic differentiable loss function with $L_1$ penalty with a redundant reparametrization and straightforward stochastic gradient descent. Our proposal is the direct generalization of a series of previous ideas that the $L_1$ penalty may be equivalent to a differentiable reparametrization with weight decay. We prove that the proposed method, \textit{spred}, is an exact solver of $L_1$ and that the reparametrization trick is completely ``benign" for a generic nonconvex function. Practically, we demonstrate the usefulness of the method in (1) training sparse neural networks to perform gene selection tasks, which involves finding relevant features in a very high dimensional space, and (2) neural network compression task, to which previous attempts at applying the $L_1$-penalty have been unsuccessful. Conceptually, our result bridges the gap between the sparsity in deep learning and conventional statistical learning.
    
[^209]: 通过最小化动态调整方案的块状残差网络训练

    Block-wise Training of Residual Networks via the Minimizing Movement Scheme. (arXiv:2210.00949v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00949](http://arxiv.org/abs/2210.00949)

    本文提出了一种适用于ResNets的块状训练方法，能够解决端到端反向传播存在的问题，包括受限制的环境下无法加载整个模型以及禁止并行训练各层等，并且能够缓解层次训练中出现的停滞问题，其测试精度与完整模型的端到端训练相竞争。

    

    端到端的反向传播存在一些不足之处：在训练过程中需要加载整个模型，这在受限制的环境下可能是不可能的，并且受到三个锁定问题的困扰（前向锁定、更新锁定和后向锁定），这些问题禁止并行训练各层。通过逐层优化问题可以解决这些问题，并已在神经网络的设备端上使用。我们开发了一种适用于ResNets的逐层训练方法，受启发于分布空间梯度流的最小化运动方案。该方法相当于每个块的动能正则化，使块成为最优输运映射，并赋予其正则化性质。它通过缓解层次训练中观察到的停滞问题工作，即贪婪训练早期层会过拟合，更深的层在一定深度后停止提高测试精度。我们展示了在分类任务上，块状训练ResNets的测试精度与完整模型的端到端训练相竞争，并且可扩展到大规模数据集和模型，即使在有限的计算资源上也能实现。

    End-to-end backpropagation has a few shortcomings: it requires loading the entire model during training, which can be impossible in constrained settings, and suffers from three locking problems (forward locking, update locking and backward locking), which prohibit training the layers in parallel. Solving layer-wise optimization problems can address these problems and has been used in on-device training of neural networks. We develop a layer-wise training method, particularly welladapted to ResNets, inspired by the minimizing movement scheme for gradient flows in distribution space. The method amounts to a kinetic energy regularization of each block that makes the blocks optimal transport maps and endows them with regularity. It works by alleviating the stagnation problem observed in layer-wise training, whereby greedily-trained early layers overfit and deeper layers stop increasing test accuracy after a certain depth. We show on classification tasks that the test accuracy of block-wise
    
[^210]: 个体治疗效果估计的迁移学习

    Transfer Learning for Individual Treatment Effect Estimation. (arXiv:2210.00380v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00380](http://arxiv.org/abs/2210.00380)

    本论文探讨了个体治疗效果估计中迁移因果知识的问题，并提出了一个使用CITA度量进行ITE知识转移的框架，实验证明该方法的有效性。

    

    本研究探讨了在个体治疗效果估计中通过任务之间转移因果知识的问题。我们从理论上评估了转移个体治疗效果知识的可行性，并提出了一个实用的框架来实现有效的转移。我们引入了一个下界来说明目标任务的个体治疗效果误差存在挑战，因为缺乏反事实信息。尽管如此，我们建立了目标任务反事实损失和个体治疗效果误差的泛化上界，证明了个体治疗效果知识转移的可行性。随后，我们引入了一个框架，其中使用新的因果推断任务亲和度（CITA）度量进行ITE知识转移。具体而言，我们使用CITA来找到最接近目标任务的源任务，并利用它来进行ITE知识转移。我们提供了实证研究，证明了所提出方法的有效性。我们观察到，ITE知识转移可以显著（

    This work considers the problem of transferring causal knowledge between tasks for Individual Treatment Effect (ITE) estimation. To this end, we theoretically assess the feasibility of transferring ITE knowledge and present a practical framework for efficient transfer. A lower bound is introduced on the ITE error of the target task to demonstrate that ITE knowledge transfer is challenging due to the absence of counterfactual information. Nevertheless, we establish generalization upper bounds on the counterfactual loss and ITE error of the target task, demonstrating the feasibility of ITE knowledge transfer. Subsequently, we introduce a framework with a new Causal Inference Task Affinity (CITA) measure for ITE knowledge transfer. Specifically, we use CITA to find the closest source task to the target task and utilize it for ITE knowledge transfer. Empirical studies are provided, demonstrating the efficacy of the proposed method. We observe that ITE knowledge transfer can significantly (
    
[^211]: 视频中基于信息熵的无监督关键点表示学习

    Entropy-driven Unsupervised Keypoint Representation Learning in Videos. (arXiv:2209.15404v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.15404](http://arxiv.org/abs/2209.15404)

    本文提出了一种基于图像空间熵的无监督视频关键点表示学习方法，将视觉特征抽象为关键点的十分简明的表示，并通过局部熵计算的信息熵损失指导模型发现一致的关键点表示。该方法在多种基准数据集上展示了优异的性能，超过了现有最先进的方法。

    

    从视频中提取有意义的表示是有效地学习各种下游任务的基础。本文提出了一种新的方法，利用图像空间熵（ISE）的概念从视频中无监督地学习有意义的视觉表示。我们认为像素邻域的局部熵和它们的时间变化可以为学习突出的特征提供有价值的内在监督信号。基于这个思想，我们将视觉特征抽象为关键点的简明表示，作为动态信息传递者，并设计了一个深度学习模型，从视频帧中无监督地直接学习空间和时间上一致的表示。利用局部熵计算的两个原始的信息熵损失指导我们的模型发现一致的关键点表示；一个最大化关键点覆盖的空间信息的损失和一个最大化关键点在连续帧之间时间一致性的损失。我们在几个基准数据集上评估了我们的方法，并展示了它在关键点检测、动作识别和无监督聚类方面的有效性，优于现有的最先进方法。

    Extracting informative representations from videos is fundamental for effectively learning various downstream tasks. We present a novel approach for unsupervised learning of meaningful representations from videos, leveraging the concept of image spatial entropy (ISE) that quantifies the per-pixel information in an image. We argue that \textit{local entropy} of pixel neighborhoods and their temporal evolution create valuable intrinsic supervisory signals for learning prominent features. Building on this idea, we abstract visual features into a concise representation of keypoints that act as dynamic information transmitters, and design a deep learning model that learns, purely unsupervised, spatially and temporally consistent representations \textit{directly} from video frames. Two original information-theoretic losses, computed from local entropy, guide our model to discover consistent keypoint representations; a loss that maximizes the spatial information covered by the keypoints and a
    
[^212]: Fed-CBS：一种考虑异构性的客户端抽样机制，通过类别不平衡性降低实现去中心化的学习方法的性能劣化

    Fed-CBS: A Heterogeneity-Aware Client Sampling Mechanism for Federated Learning via Class-Imbalance Reduction. (arXiv:2209.15245v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.15245](http://arxiv.org/abs/2209.15245)

    本文提出了Fed-CBS机制，通过类别不平衡性降低实现去中心化的学习方法的性能劣化，从而提高了federated learning（FL）在非独立同分布（non-IID）数据上的性能。

    

    由于边缘设备通信能力有限，大多数现有的联邦学习（FL）方法仅随机选择部分设备参与每一轮通信的训练。与同时涉及所有可用客户端相比，随机选择机制可能导致非独立同分布（non-IID）数据的显着性能下降。本文展示了我们的关键观察结果：随机选择客户端所导致的性能下降的根本原因是所选客户端中的数据类别不平衡问题。基于这一观察结果，我们设计了一种高效的异构感知客户端抽样机制——联邦类平衡抽样（Fed-CBS），其可有效地降低从有意选择的客户端中获取的组数据的类别不平衡程度。特别是，我们提出了一种类别不平衡度量，并采用同态加密方式以隐私保护的方式推导出该度量。基于该度量，我们设计了一种具有异构感知的客户端抽样机制，使客户端实现了类平衡状态，从而提高了FL在非IID数据上的性能。我们介绍了Fed-CBS机制的设计和实现，并在真实世界的数据集上评估了其性能。

    Due to limited communication capacities of edge devices, most existing federated learning (FL) methods randomly select only a subset of devices to participate in training for each communication round. Compared with engaging all the available clients, the random-selection mechanism can lead to significant performance degradation on non-IID (independent and identically distributed) data. In this paper, we show our key observation that the essential reason resulting in such performance degradation is the class-imbalance of the grouped data from randomly selected clients. Based on our key observation, we design an efficient heterogeneity-aware client sampling mechanism, i.e., Federated Class-balanced Sampling (Fed-CBS), which can effectively reduce class-imbalance of the group dataset from the intentionally selected clients. In particular, we propose a measure of class-imbalance and then employ homomorphic encryption to derive this measure in a privacy-preserving way. Based on this measure
    
[^213]: 从刚体图像中预测3D旋转动力学（未知质量分布）

    Learning to predict 3D rotational dynamics from images of a rigid body with unknown mass distribution. (arXiv:2209.11355v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.11355](http://arxiv.org/abs/2209.11355)

    该研究提出了一种基于物理知识的神经网络模型，通过多级预测流程，从刚体图像序列中预测3D旋转动力学，解决了标准深度学习方法无法揭示体内质量分布影响的问题。

    

    在许多实际情况下，当低维度测量不可用时，会有自由旋转的3D刚体的图像观察。然而，图像数据的高维数阻止了使用经典估计技术来学习动态。标准深度学习方法的有用性也受限于一个刚体图像无法揭示体内质量分布，而质量分布与初始角速度一起决定刚体旋转方式。我们提出了一种基于物理知识的神经网络模型，从图像序列中估计和预测3D旋转动力学。我们使用多级预测流程实现了这一目标，该流程将单个图像映射到与 $\mathbf{SO}(3)$ 同胚的潜在表示中，从潜在对中计算角速度，并使用Hamilton运动方程预测未来的潜在状态。我们在新的旋转刚体数据集上展示了我们方法的有效性。

    In many real-world settings, image observations of freely rotating 3D rigid bodies, may be available when low-dimensional measurements are not. However, the high-dimensionality of image data precludes the use of classical estimation techniques to learn the dynamics. The usefulness of standard deep learning methods is also limited because an image of a rigid body reveals nothing about the distribution of mass inside the body, which, together with initial angular velocity, is what determines how the body will rotate. We present a physics-informed neural network model to estimate and predict 3D rotational dynamics from image sequences. We achieve this using a multi-stage prediction pipeline that maps individual images to a latent representation homeomorphic to $\mathbf{SO}(3)$, computes angular velocities from latent pairs, and predicts future latent states using the Hamiltonian equations of motion. We demonstrate the efficacy of our approach on new rotating rigid-body datasets of sequenc
    
[^214]: SELTO：高效学习拓扑优化方法

    SELTO: Sample-Efficient Learned Topology Optimization. (arXiv:2209.05098v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.05098](http://arxiv.org/abs/2209.05098)

    本文旨在提出一个高效学习拓扑优化方法，采用物理学预处理和等变网络构建样本高效的深度学习管道，并发布了两个TO数据集，旨在提高比较性和未来的进展。

    

    深度学习技术的发展为拓扑优化提供了广泛的潜力，然而目前仍缺乏关于基本方法和数据集的系统研究。本文旨在解决这两个问题。首先，我们利用基于物理学的预处理和等变网络构建样本高效的深度学习管道。在大规模的消融研究中进行端到端监督学习评估。结果表明，与先前方法相比，我们的方法在样本效率和预测物理正确性方面都有了显著的提高。其次，为了提高比较性和未来的进展，我们发布了前两个拓扑优化问题的数据集和相应的答案。

    Recent developments in Deep Learning (DL) suggest a vast potential for Topology Optimization (TO). However, while there are some promising attempts, the subfield still lacks a firm footing regarding basic methods and datasets. We aim to address both points. First, we explore physics-based preprocessing and equivariant networks to create sample-efficient components for TO DL pipelines. We evaluate them in a large-scale ablation study using end-to-end supervised training. The results demonstrate a drastic improvement in sample efficiency and the predictions' physical correctness. Second, to improve comparability and future progress, we publish the two first TO datasets containing problems and corresponding ground truth solutions.
    
[^215]: DL-DRL：面向多无人机大规模任务调度的双层深度强化学习方法

    DL-DRL: A double-level deep reinforcement learning approach for large-scale task scheduling of multi-UAV. (arXiv:2208.02447v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.02447](http://arxiv.org/abs/2208.02447)

    提出了一个双层深度强化学习方法(DL-DRL)来解决多UAV大规模任务调度问题，采用分治框架进行任务分配和路径规划，能够显着提高计算效率，优化任务执行效果。

    

    近年来，利用无人机（UAV）执行任务越来越受欢迎。为解决潜在的任务调度问题，基于深度强化学习（DRL）的方法显示出与传统启发式方法相比的显着优势，因为它们更少依赖手工制定的规则。然而，随着问题规模的不断扩大，它们的决策空间也会变得非常庞大，这会破坏计算效率。为了解决这个问题，我们提出了一个双层深度强化学习（DL-DRL）方法，基于一个分治框架（DCF），将多UAV的任务调度分解为任务分配和路径规划两个层次。特别地，在我们的上层DRL模型中设计了一个编码器-解码器结构化的策略网络，将任务分配给不同的UAV，并在我们的下层DRL模型中利用另一个基于注意力的策略网络为每个UAV构造路径，以最大化完成的任务数。

    Exploiting unmanned aerial vehicles (UAVs) to execute tasks is gaining growing popularity recently. To solve the underlying task scheduling problem, the deep reinforcement learning (DRL) based methods demonstrate notable advantage over the conventional heuristics as they rely less on hand-engineered rules. However, their decision space will become prohibitively huge as the problem scales up, thus deteriorating the computation efficiency. To alleviate this issue, we propose a double-level deep reinforcement learning (DL-DRL) approach based on a divide and conquer framework (DCF), where we decompose the task scheduling of multi-UAV into task allocation and route planning. Particularly, we design an encoder-decoder structured policy network in our upper-level DRL model to allocate the tasks to different UAVs, and we exploit another attention based policy network in our lower-level DRL model to construct the route for each UAV, with the objective to maximize the number of executed tasks gi
    
[^216]: 小数据学习综述：泛化、优化和挑战

    A Survey of Learning on Small Data: Generalization, Optimization, and Challenge. (arXiv:2207.14443v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.14443](http://arxiv.org/abs/2207.14443)

    小数据学习综述，包括主动学习、少样本学习、元学习和无监督学习等方面，但它们在泛化性能上缺乏理论保证；考虑到不同目的的多个学习社区都可以产生小数据，我们将小数据分为不同大小和复杂度的光谱。

    

    在人工智能取得了成功的今天，大规模数据学习带来的标注和训练成本是非常昂贵的。在未来，利用小数据进行学习以逼近大数据的泛化能力是人工智能的终极目标之一，这需要机器能够像人类一样在少量数据中识别目标和场景。目前的研究方向主要包括主动学习和少样本学习，但是它们的泛化性能缺乏理论保证，大多数设置为被动学习，即已知分布下有限的训练资源中控制标签分布。本综述采用以 PAC (Probably Approximately Correct) 框架下的不可知主动采样理论来分析模型无关监督和非监督小数据学习中的泛化误差和标签复杂度。考虑到不同目的的多个学习社区都可以产生小数据，我们将小数据分为不同大小和复杂度的光谱。然后，我们回顾了一系列针对小数据的学习方法，包括主动学习、少样本学习、元学习和无监督学习，以及它们相关的挑战，如领域自适应、类别不平衡和标签噪音。最后，我们总结了小数据学习的潜在研究方向。

    Learning on big data brings success for artificial intelligence (AI), but the annotation and training costs are expensive. In future, learning on small data that approximates the generalization ability of big data is one of the ultimate purposes of AI, which requires machines to recognize objectives and scenarios relying on small data as humans. A series of learning topics is going on this way such as active learning and few-shot learning. However, there are few theoretical guarantees for their generalization performance. Moreover, most of their settings are passive, that is, the label distribution is explicitly controlled by finite training resources from known distributions. This survey follows the agnostic active sampling theory under a PAC (Probably Approximately Correct) framework to analyze the generalization error and label complexity of learning on small data in model-agnostic supervised and unsupervised fashion. Considering multiple learning communities could produce small dat
    
[^217]: 同态自编码器 - 从观察到转化学习群组结构表现

    Homomorphism Autoencoder -- Learning Group Structured Representations from Observed Transitions. (arXiv:2207.12067v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.12067](http://arxiv.org/abs/2207.12067)

    本文提出了一种同态自编码器方法，使机器能够在行动中学习到与其行为相一致的感知信息的内部表示，并捕获环境中的转换结构。

    

    如何让机器学习系统学习到准确表示其与真实世界交互的内在模型是一个尚待解决的问题。为了构建不仅包含观察性知识，也包含干预性知识的表现学习框架，我们使用了表示学习和群论的方法来研究该问题。我们提出一种方法，使机器能够在行动过程中学习到与之相一致的感知信息的内部表示，而这些行动实际上是变换这些信息的。我们使用一个自编码器并在其潜在空间上应用群组表示，通过利用等变损失强制实施适当的同态性质以完成训练。与现有方法不同的是，我们的方法不需要先验群组知识，并且不限制代理可执行的行动集合。理论上，我们证明了该方法的有效性，并通过实验证明了其能够学习到行动的群组表示，从而捕获了环境中的转换结构。

    How can agents learn internal models that veridically represent interactions with the real world is a largely open question. As machine learning is moving towards representations containing not just observational but also interventional knowledge, we study this problem using tools from representation learning and group theory. We propose methods enabling an agent acting upon the world to learn internal representations of sensory information that are consistent with actions that modify it. We use an autoencoder equipped with a group representation acting on its latent space, trained using an equivariance-derived loss in order to enforce a suitable homomorphism property on the group representation. In contrast to existing work, our approach does not require prior knowledge of the group and does not restrict the set of actions the agent can perform. We motivate our method theoretically, and show empirically that it can learn a group representation of the actions, thereby capturing the str
    
[^218]: 后验概念解释何时可识别？

    When are Post-hoc Conceptual Explanations Identifiable?. (arXiv:2206.13872v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.13872](http://arxiv.org/abs/2206.13872)

    本论文提出了可识别的概念发现方法，可以恢复出多个已知的概念，以确保解释的可靠性。对于具有依赖关系的概念，提出了两种新的方法，利用图像生成过程的功能组合性质。该方法明显优于现有方法。

    

    学习嵌入通常需要通过概念解释来理解和分解，这种需求在解释中不包含有效概念标签的情况下尤为显著。为了提供后验解释，概念发现方法会在已训练的嵌入空间中搜索解释性强的概念，例如物体形状或颜色。与之前的工作不同，我们认为概念发现应该是可识别的，这意味着可以被证明地恢复出多个已知的概念，以确保解释的可靠性。为了作为一个起点，我们明确地将概念发现与传统方法（例如主成分分析和独立成分分析）联系起来，并通过表明它们可以恢复具有非高斯分布的独立概念来阐明这一点。对于具有依赖关系的概念，我们提出了两种新的方法，利用图像生成过程的功能组合性质。我们的可证明可识别的概念发现方法明显优于现有方法。

    Interest in understanding and factorizing learned embedding spaces through conceptual explanations is steadily growing. When no human concept labels are available, concept discovery methods search trained embedding spaces for interpretable concepts like object shape or color that can be used to provide post-hoc explanations for decisions. Unlike previous work, we argue that concept discovery should be identifiable, meaning that a number of known concepts can be provably recovered to guarantee reliability of the explanations. As a starting point, we explicitly make the connection between concept discovery and classical methods like Principal Component Analysis and Independent Component Analysis by showing that they can recover independent concepts with non-Gaussian distributions. For dependent concepts, we propose two novel approaches that exploit functional compositionality properties of image-generating processes. Our provably identifiable concept discovery methods substantially outpe
    
[^219]: 鲁棒性通用对抗扰动

    Robust Universal Adversarial Perturbations. (arXiv:2206.10858v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.10858](http://arxiv.org/abs/2206.10858)

    本文提出了一种对现实世界变换具有鲁棒性的通用对抗扰动生成算法，并在多个数据集上进行了评估。

    

    通用对抗扰动（UAP）是一种不可察觉的图像无关向量，可以导致深度神经网络（DNN）高概率地将输入错误分类。在实际攻击场景中，对抗性扰动可能会在添加到DNN输入之前经历像素强度的变化、缩放等变换。现有方法不能创建对这些现实世界变换具有鲁棒性的UAP，因此在实际攻击场景中应用受到限制。在本文中，我们介绍并制定了对现实世界变换具有鲁棒性的UAP。我们使用概率鲁棒性界限构建了一个迭代算法，并构造了这样的UAP，它们对由任意子可微变换函数组成的变换具有鲁棒性。我们对流行的CIFAR-10和ILSVRC 2012数据集进行了广泛评估，衡量了我们的UAP在一系列常见的现实世界变换下的鲁棒性，例如旋转、对比度变化等。我们进一步展示。

    Universal Adversarial Perturbations (UAPs) are imperceptible, image-agnostic vectors that cause deep neural networks (DNNs) to misclassify inputs with high probability. In practical attack scenarios, adversarial perturbations may undergo transformations such as changes in pixel intensity, scaling, etc. before being added to DNN inputs. Existing methods do not create UAPs robust to these real-world transformations, thereby limiting their applicability in practical attack scenarios. In this work, we introduce and formulate UAPs robust against real-world transformations. We build an iterative algorithm using probabilistic robustness bounds and construct such UAPs robust to transformations generated by composing arbitrary sub-differentiable transformation functions. We perform an extensive evaluation on the popular CIFAR-10 and ILSVRC 2012 datasets measuring our UAPs' robustness under a wide range common, real-world transformations such as rotation, contrast changes, etc. We further show t
    
[^220]: 差分隐私优化中超越统一李普希茨条件

    Beyond Uniform Lipschitz Condition in Differentially Private Optimization. (arXiv:2206.10713v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.10713](http://arxiv.org/abs/2206.10713)

    本文提出了一种新的差分隐私优化算法来处理其它算法无法处理的非均匀李普希茨情形，并且在具体应用中提供了相应的参数调整方案。

    

    大多数关于差分隐私随机梯度下降（DP-SGD）的先前结果都是在统一李普希茨性的简化假设下导出的，即每个样本的梯度都是均匀有界的。我们通过假定每个样本的梯度具有样本相关的上界，即每个样本的李普希茨常数，从而推广了统一李普希茨性。这些本身可能是无界的。当每个样本的李普希茨常数是有界的时，我们为DP-SGD在凸超参数化设置中选择剪辑范数提供了原则性指导；具体而言，我们建议仅调整剪辑范数，直到最小每个样本李普希茨常数的值。这在深度网络上预先训练公共数据的 softmax 层的私人训练中有应用。我们通过对8个数据集的实验验证了我们的建议的功效。此外，我们还为DP-SGD在凸和非凸函数上提供了新的收敛结果。

    Most prior results on differentially private stochastic gradient descent (DP-SGD) are derived under the simplistic assumption of uniform Lipschitzness, i.e., the per-sample gradients are uniformly bounded. We generalize uniform Lipschitzness by assuming that the per-sample gradients have sample-dependent upper bounds, i.e., per-sample Lipschitz constants, which themselves may be unbounded. We provide principled guidance on choosing the clip norm in DP-SGD for convex over-parameterized settings satisfying our general version of Lipschitzness when the per-sample Lipschitz constants are bounded; specifically, we recommend tuning the clip norm only till values up to the minimum per-sample Lipschitz constant. This finds application in the private training of a softmax layer on top of a deep network pre-trained on public data. We verify the efficacy of our recommendation via experiments on 8 datasets. Furthermore, we provide new convergence results for DP-SGD on convex and nonconvex function
    
[^221]: 全局上下文视觉Transformer

    Global Context Vision Transformers. (arXiv:2206.09959v5 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.09959](http://arxiv.org/abs/2206.09959)

    提出了全局上下文视觉Transformer (GC ViT) 架构，利用全局上下文自注意力模块和标准的局部自注意力对长距离和短距离空间相互作用进行有效而高效的建模，同时解决了ViTs中缺乏归纳偏差的问题，在图像分类、目标检测和语义分割任务中表现出最先进的结果。

    

    我们提出了一种新颖的架构——全局上下文视觉Transformer (GC ViT), 可以增强计算机视觉中的参数和计算利用。我们的方法利用全局上下文自注意力模块和标准的局部自注意力，对长距离和短距离空间相互作用进行有效而高效的建模，无需进行像计算注意力掩码或移动本地窗口这样的昂贵操作。并且，我们解决了ViTs中缺乏归纳偏差的问题，并在我们的架构中使用一种修改后的融合反向残差块。我们提出的GC ViT在图像分类、目标检测和语义分割任务中均取得了最先进的成果。在ImageNet-1K数据集上进行分类，GC ViT的51M、90M和201M参数变体在224像素分辨率下都能够达到84.3%、85.0%和85.7%的Top-1精度，而且无需任何预训练，因此超越了CNN-based Conv等先前的艺术品。

    We propose global context vision transformer (GC ViT), a novel architecture that enhances parameter and compute utilization for computer vision. Our method leverages global context self-attention modules, joint with standard local self-attention, to effectively and efficiently model both long and short-range spatial interactions, without the need for expensive operations such as computing attention masks or shifting local windows. In addition, we address the lack of the inductive bias in ViTs, and propose to leverage a modified fused inverted residual blocks in our architecture. Our proposed GC ViT achieves state-of-the-art results across image classification, object detection and semantic segmentation tasks. On ImageNet-1K dataset for classification, the variants of GC ViT with 51M, 90M and 201M parameters achieve 84.3%, 85.0% and 85.7% Top-1 accuracy, respectively, at 224 image resolution and without any pre-training, hence surpassing comparably-sized prior art such as CNN-based Conv
    
[^222]: 我知道你去年训练了什么：关于窃取机器学习模型和防御的调查

    I Know What You Trained Last Summer: A Survey on Stealing Machine Learning Models and Defences. (arXiv:2206.08451v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.08451](http://arxiv.org/abs/2206.08451)

    随着机器学习即服务（MLaaS）的普及，用户可以使用最复杂的机器学习模型的预测，但也危及了MLaaS提供商的知识产权，攻击者可以创建一个具备相同行为的模型副本，本文针对模型窃取的攻击和相应的对策进行了综合调查。

    

    机器学习即服务（MLaaS）已经成为一种广泛的范 paradigm，通过按需付费的原则，甚至可以为客户提供最复杂的机器学习模型的预测。这使用户可以避免耗时的数据收集、超参数调整和模型训练过程。然而，通过让客户访问（其预测的）模型，MLaaS 提供商危及其知识产权，如敏感的训练数据、优化的超参数或学习的模型参数。攻击者可以使用仅预测标签创建具有（几乎）相同行为的模型副本。虽然描述了许多这种攻击的变体，但只提出了分散的防御策略，涉及孤立的威胁。这提出了对模型窃取领域进行彻底系统化的必要性，以全面了解为什么这些攻击成功以及如何全面地进行防御。我们通过提供一项综合性调查来解决这一问题，涵盖模型窃取攻击和相应的对策。

    Machine Learning-as-a-Service (MLaaS) has become a widespread paradigm, making even the most complex machine learning models available for clients via e.g. a pay-per-query principle. This allows users to avoid time-consuming processes of data collection, hyperparameter tuning, and model training. However, by giving their customers access to the (predictions of their) models, MLaaS providers endanger their intellectual property, such as sensitive training data, optimised hyperparameters, or learned model parameters. Adversaries can create a copy of the model with (almost) identical behavior using the the prediction labels only. While many variants of this attack have been described, only scattered defence strategies have been proposed, addressing isolated threats. This raises the necessity for a thorough systematisation of the field of model stealing, to arrive at a comprehensive understanding why these attacks are successful, and how they could be holistically defended against. We addr
    
[^223]: 欧几里得图神经网络和双曲线图神经网络结合的统一框架

    A Unification Framework for Euclidean and Hyperbolic Graph Neural Networks. (arXiv:2206.04285v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.04285](http://arxiv.org/abs/2206.04285)

    本文提出了一种双曲正规化技术，将双曲线图神经网络和欧几里得图神经网络有机结合起来，以摆脱多个不一致向量空间之间的限制，同时保留双曲线网络模拟分层图结构的表现能力。

    

    双曲线图神经网络能够有效地捕捉图数据集的内在层次结构，成为强大的图神经网络选择。然而，它们在图层内纠缠着多个不一致的（陀螺）向量空间，这使它们在泛化和可扩展性方面受限。在本文中，我们将Poincare disk模型作为我们的搜索空间，将所有近似都应用于该磁盘上（就好像该磁盘是从原点派生出的切空间），从而摆脱了所有空间转换。这样的方法使我们能够提出一个双曲正规化层，并进一步将整个双曲线模型简化为欧几里得模型，后者是我们提出的双曲正规化层的级联结果。我们将我们提出的非线性双曲正规化应用于当前最先进的同质和多关系图网络。我们证明，我们的模型不仅利用了欧几里得网络的解释性和各种模型组件的高效执行的优势，而且还保留了双曲线网络模拟分层图结构的表现能力。

    Hyperbolic neural networks can effectively capture the inherent hierarchy of graph datasets, and consequently a powerful choice of GNNs. However, they entangle multiple incongruent (gyro-)vector spaces within a layer, which makes them limited in terms of generalization and scalability. In this work, we propose the Poincare disk model as our search space, and apply all approximations on the disk (as if the disk is a tangent space derived from the origin), thus getting rid of all inter-space transformations. Such an approach enables us to propose a hyperbolic normalization layer and to further simplify the entire hyperbolic model to a Euclidean model cascaded with our hyperbolic normalization layer. We applied our proposed nonlinear hyperbolic normalization to the current state-of-the-art homogeneous and multi-relational graph networks. We demonstrate that our model not only leverages the power of Euclidean networks such as interpretability and efficient execution of various model compon
    
[^224]: 一种具有线性收敛的高效通信算法，用于联邦极小值学习

    A Communication-efficient Algorithm with Linear Convergence for Federated Minimax Learning. (arXiv:2206.01132v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.01132](http://arxiv.org/abs/2206.01132)

    本文提出了一种高效的通信算法，用于联邦极小值学习，具有线性收敛特性。研究了重要的特殊情况，并提供了学习该目标的泛化界限。针对联邦设置，提出了Local Stochastic Gradient Descent Ascent (SGDA)算法，可保证收敛性。

    

    本文研究了大规模多智能体极小化最优化问题，该问题模拟了统计学习和博弈论中的许多有趣应用，包括生成式对抗网络(GAN)。总体目标是代理的私有本地目标函数之和。我们首先分析了重要的特殊情况，即经验最小极小值问题，在其中，整体目标通过统计样本近似一个真实的极小值风险。我们通过Rademacher复杂度分析提供了学习此目标的泛化界限。然后，我们关注联邦设置，其中代理可以执行本地计算并与中央服务器通信。大多数现有的联邦极小值算法要么需要每次迭代进行通信，要么缺乏性能保证，除了本地随机梯度下降上升(SGDA)，这是一种多个本地更新下降上升算法，在缩小步长的情况下保证收敛。

    In this paper, we study a large-scale multi-agent minimax optimization problem, which models many interesting applications in statistical learning and game theory, including Generative Adversarial Networks (GANs). The overall objective is a sum of agents' private local objective functions. We first analyze an important special case, empirical minimax problem, where the overall objective approximates a true population minimax risk by statistical samples. We provide generalization bounds for learning with this objective through Rademacher complexity analysis. Then, we focus on the federated setting, where agents can perform local computation and communicate with a central server. Most existing federated minimax algorithms either require communication per iteration or lack performance guarantees with the exception of Local Stochastic Gradient Descent Ascent (SGDA), a multiple-local-update descent ascent algorithm which guarantees convergence under a diminishing stepsize. By analyzing Loca
    
[^225]: 协方差矩阵自适应MAP退火算法

    Covariance Matrix Adaptation MAP-Annealing. (arXiv:2205.10752v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.10752](http://arxiv.org/abs/2205.10752)

    本文提出了一种新的质量多样性算法——协方差矩阵自适应MAP退火（CMA-MAE），用于解决过早地放弃目标以进行探索、难以探索平坦目标以及低分辨率档案性能差等问题，实现了最先进的性能和鲁棒性。

    

    单目标优化算法通过目标函数寻找最高质量的单一解决方案。质量多样性（QD）优化算法，例如协方差矩阵自适应MAP-精英（CMA-ME），寻找一组既在目标函数方面高质量、又在特定度量函数方面多样性的解决方案集。但是CMA-ME存在三个主要的限制：过早地放弃目标以进行探索、难以探索平坦目标以及低分辨率档案性能差。我们提出了一种新的质量多样性算法，协方差矩阵自适应MAP退火（CMA-MAE），以解决所有三个限制。我们提供了每个限制的新算法的理论证明。我们的理论支撑了我们的实验，结果表明CMA-MAE达到了最先进的性能和鲁棒性。

    Single-objective optimization algorithms search for the single highest-quality solution with respect to an objective. Quality diversity (QD) optimization algorithms, such as Covariance Matrix Adaptation MAP-Elites (CMA-ME), search for a collection of solutions that are both high-quality with respect to an objective and diverse with respect to specified measure functions. However, CMA-ME suffers from three major limitations highlighted by the QD community: prematurely abandoning the objective in favor of exploration, struggling to explore flat objectives, and having poor performance for low-resolution archives. We propose a new quality diversity algorithm, Covariance Matrix Adaptation MAP-Annealing (CMA-MAE), that addresses all three limitations. We provide theoretical justifications for the new algorithm with respect to each limitation. Our theory informs our experiments, which support the theory and show that CMA-MAE achieves state-of-the-art performance and robustness.
    
[^226]: 基于实例不平衡感知的语义分割损失函数：Blob Loss

    blob loss: instance imbalance aware loss functions for semantic segmentation. (arXiv:2205.08209v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2205.08209](http://arxiv.org/abs/2205.08209)

    该论文提出了一种针对实例不平衡问题的语义分割损失函数——Blob Loss，用于多实例检测，可通过提高F1分数和灵敏度等指标来优化性能。

    

    深度卷积神经网络在语义分割任务中表现出良好的效果。然而，已有的损失函数虽然针对改进体量得分进行了设计，如Dice系数（DSC），但其无法识别类别内的实例不平衡问题。这导致大的前景实例可以支配小的实例而仍然产生令人满意的DSC。我们提出了一种新的损失函数——Blob Loss，旨在最大化感知性能指标, 如F1分数和灵敏度，针对多实例检测的语义分割问题。

    Deep convolutional neural networks (CNN) have proven to be remarkably effective in semantic segmentation tasks. Most popular loss functions were introduced targeting improved volumetric scores, such as the Dice coefficient (DSC). By design, DSC can tackle class imbalance, however, it does not recognize instance imbalance within a class. As a result, a large foreground instance can dominate minor instances and still produce a satisfactory DSC. Nevertheless, detecting tiny instances is crucial for many applications, such as disease monitoring. For example, it is imperative to locate and surveil small-scale lesions in the follow-up of multiple sclerosis patients. We propose a novel family of loss functions, \emph{blob loss}, primarily aimed at maximizing instance-level detection metrics, such as F1 score and sensitivity. \emph{Blob loss} is designed for semantic segmentation problems where detecting multiple instances matters. We extensively evaluate a DSC-based \emph{blob loss} in five c
    
[^227]: 在RKHS的非参数回归中最优解决协变量转移问题

    Optimally tackling covariate shift in RKHS-based nonparametric regression. (arXiv:2205.02986v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2205.02986](http://arxiv.org/abs/2205.02986)

    本文研究了在RKHS的非参数回归中的协变量转移问题，针对两个不同的似然比族，证明了使用KRR估计量具有极小化率最优的特点，尤其是在似然比被均匀有界时。与此同时，本文也证明了，在协变量转移下一个naive的估计器相比于KRR是严格次优的。

    

    在再生核希尔伯特空间（RKHS）上研究了非参数回归中的协变量转移问题。我们关注两个使用源和目标分布之间的似然比定义的自然协变量转移问题族。当似然比被均匀有界时，我们证明带有精心选择的正则化参数的核岭回归(KRR)估计量是极小化率最优的（最多差一个对数因子），对于一大类具有正则核特征值的RKHS而言。有趣的是，除了似然比上界之外，KRR不需要对似然比有完全的知识。与没有协变量转移的标准统计设置形成鲜明对比的是，我们还证明了一个简单估计器，即在函数类中最小化经验风险，与KRR相比，在协变量转移下是严格次优的。然后，我们解决了更大的协变量转移问题类，其中似然比可能是无界的。

    We study the covariate shift problem in the context of nonparametric regression over a reproducing kernel Hilbert space (RKHS). We focus on two natural families of covariate shift problems defined using the likelihood ratios between the source and target distributions. When the likelihood ratios are uniformly bounded, we prove that the kernel ridge regression (KRR) estimator with a carefully chosen regularization parameter is minimax rate-optimal (up to a log factor) for a large family of RKHSs with regular kernel eigenvalues. Interestingly, KRR does not require full knowledge of likelihood ratios apart from an upper bound on them. In striking contrast to the standard statistical setting without covariate shift, we also demonstrate that a naive estimator, which minimizes the empirical risk over the function class, is strictly sub-optimal under covariate shift as compared to KRR. We then address the larger class of covariate shift problems where the likelihood ratio is possibly unbounde
    
[^228]: Clifford电路可以被适当地PAC学习当且仅当$\textsf{RP}=\textsf{NP}$

    Clifford Circuits can be Properly PAC Learned if and only if $\textsf{RP}=\textsf{NP}$. (arXiv:2204.06638v4 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2204.06638](http://arxiv.org/abs/2204.06638)

    本论文证明了Clifford电路只有在$\textsf{RP}=\textsf{NP}$的情况下才能被适当地PAC学习；如果此条件成立，则可以使用随机多项式时间算法解决半正定规划问题。

    

    鉴于一个输入状态、测量和概率的数据集，是否有可能有效地预测与量子电路相关的测量概率？ Caro和Datta（2020）的最新工作在信息理论意义上研究了PAC学习量子电路的问题，但存在计算效率问题。特别地，可能存在一种候选电路类可以进行有效学习，即Clifford电路，因为这种电路所生成的相应状态集合，称为稳定器状态，已知可以被有效地PAC学习（Rocchetto 2018）。在这里，我们提供了一个负的结果，展示CNOT电路的适当学习对于经典学习者来说是困难的，除非$\textsf{RP} = \textsf{NP}$。由于作为Clifford电路的经典模拟和子集，这自然地导致了Clifford电路的困难结果。此外，我们还展示了如果$\textsf{RP} = \textsf{NP}$，则存在一种能够高效求解半正定规划问题的随机多项式时间算法。

    Given a dataset of input states, measurements, and probabilities, is it possible to efficiently predict the measurement probabilities associated with a quantum circuit? Recent work of Caro and Datta (2020) studied the problem of PAC learning quantum circuits in an information theoretic sense, leaving open questions of computational efficiency. In particular, one candidate class of circuits for which an efficient learner might have been possible was that of Clifford circuits, since the corresponding set of states generated by such circuits, called stabilizer states, are known to be efficiently PAC learnable (Rocchetto 2018). Here we provide a negative result, showing that proper learning of CNOT circuits is hard for classical learners unless $\textsf{RP} = \textsf{NP}$. As the classical analogue and subset of Clifford circuits, this naturally leads to a hardness result for Clifford circuits as well. Additionally, we show that if $\textsf{RP} = \textsf{NP}$ then there would exist efficie
    
[^229]: 基于概念的解释方法用于外分布检测器

    Concept-based Explanations for Out-Of-Distribution Detectors. (arXiv:2203.02586v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.02586](http://arxiv.org/abs/2203.02586)

    本论文提出了一种基于学习的高级概念提供OOD检测器解释的方法，通过提出新的度量标准，学习一组具有高检测完整性和概念可分离性特征的概念，有效提高了OOD检测器的解释性能。

    

    外分布（OOD）检测在确保深度神经网络（DNN）分类器安全部署方面发挥着关键作用。虽然有大量方法致力于提高OOD检测器的性能，但其决策的解释仍存在重要缺陷。为解决该问题，我们提出了一种基于学习的高级概念提供OOD检测器解释的方法。首先我们提出了两个新的度量标准，用于评估一组特定概念解释OOD检测器的效果：1）检测完整性，用于量化概念对于解释OOD检测器决策的充分程度；2）概念可分离度，用于捕捉概念空间中正常分布和OOD数据之间的分布差异。基于这些指标，我们提出了一种无监督的框架，用于学习一组具有高检测完整性和概念可分离性特征的概念，并展示了其在提供基于概念解释方面的有效性。

    Out-of-distribution (OOD) detection plays a crucial role in ensuring the safe deployment of deep neural network (DNN) classifiers. While a myriad of methods have focused on improving the performance of OOD detectors, a critical gap remains in interpreting their decisions. We help bridge this gap by providing explanations for OOD detectors based on learned high-level concepts. We first propose two new metrics for assessing the effectiveness of a particular set of concepts for explaining OOD detectors: 1) detection completeness, which quantifies the sufficiency of concepts for explaining an OOD-detector's decisions, and 2) concept separability, which captures the distributional separation between in-distribution and OOD data in the concept space. Based on these metrics, we propose an unsupervised framework for learning a set of concepts that satisfy the desired properties of high detection completeness and concept separability, and demonstrate its effectiveness in providing concept-based
    
[^230]: 一种轻量级、高效率和可解释的设计卷积神经网络，用于网络流量分类

    A Lightweight, Efficient and Explainable-by-Design Convolutional Neural Network for Internet Traffic Classification. (arXiv:2202.05535v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.05535](http://arxiv.org/abs/2202.05535)

    本文提出了一种名为LEXNet的轻量级、高效率和可解释的卷积神经网络，通过使用新的残差块和原型层解决了现有流量分类器所面临的问题，并在商业级数据集的测试中表现出更好的准确率和速度。

    

    流量分类，即识别在网络中流动的应用类型，对于众多活动（例如入侵检测、路由）都是一个战略性任务。然而当前的深度学习方法并没有解决这个任务所面临的一些关键挑战。现有方法的设计没有考虑到网络硬件（例如路由器）通常是使用有限的计算资源来运行。此外，它们没有满足监管机构所强调的精确解释的需求。最后，这些流量分类器是基于小型数据集进行评估的，这无法反映实际应用程序的多样性。因此，本文提出了一种新型的轻量级、高效率和可解释的卷积神经网络（LEXNet），用于互联网流量分类，该方法依赖于一种新的残差块（用于轻量级和高效率目的）和原型层（用于解释性）。通过商业级数据集的测试，LEXNet 在准确率和速度等方面都显著优于先前的流量分类器。

    Traffic classification, i.e. the identification of the type of applications flowing in a network, is a strategic task for numerous activities (e.g., intrusion detection, routing). This task faces some critical challenges that current deep learning approaches do not address. The design of current approaches do not take into consideration the fact that networking hardware (e.g., routers) often runs with limited computational resources. Further, they do not meet the need for faithful explainability highlighted by regulatory bodies. Finally, these traffic classifiers are evaluated on small datasets which fail to reflect the diversity of applications in real-world settings.  Therefore, this paper introduces a new Lightweight, Efficient and eXplainable-by-design convolutional neural network (LEXNet) for Internet traffic classification, which relies on a new residual block (for lightweight and efficiency purposes) and prototype layer (for explainability). Based on a commercial-grade dataset, 
    
[^231]: 自适应采样的L-SVRG和L-Katyusha优化方法

    L-SVRG and L-Katyusha with Adaptive Sampling. (arXiv:2201.13387v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.13387](http://arxiv.org/abs/2201.13387)

    本文提出了一种自适应采样策略来提高L-SVRG和L-Katyusha优化方法在训练机器学习模型中的性能表现，可以在少量计算开销内实现采样分布的学习，同时不需要先验知识，并证明了其收敛性保证。

    

    基于随机梯度的优化方法，如L-SVRG及其加速变种L-Katyusha在训练机器学习模型时得到广泛应用。本文提出了一种自适应采样策略，可以实现在少量计算开销内学习采样分布，同时可以随着迭代而改变，而且不需要先验知识。对于凸目标，我们证明了L-SVRG和L-Katyusha的收敛性保证。

    Stochastic gradient-based optimization methods, such as L-SVRG and its accelerated variant L-Katyusha (Kovalev et al., 2020), are widely used to train machine learning models.The theoretical and empirical performance of L-SVRG and L-Katyusha can be improved by sampling observations from a non-uniform distribution (Qian et al., 2021). However,designing a desired sampling distribution requires prior knowledge of smoothness constants, which can be computationally intractable to obtain in practice when the dimension of the model parameter is high. To address this issue, we propose an adaptive sampling strategy for L-SVRG and L-Katyusha that can learn the sampling distribution with little computational overhead, while allowing it to change with iterates, and at the same time does not require any prior knowledge of the problem parameters. We prove convergence guarantees for L-SVRG and L-Katyusha for convex objectives when the sampling distribution changes with iterates. Our results show that
    
[^232]: 使用动作特征学习直觉策略

    Learning Intuitive Policies Using Action Features. (arXiv:2201.12658v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.12658](http://arxiv.org/abs/2201.12658)

    本论文研究了网络体系结构对多智能体协作中学习算法利用动作特征和观测特征之间语义关系的影响，发现联合处理观察特征和动作特征的特征表示的注意力机制架构可以学习直觉策略，并且这样的代理与人类协作而无需接受人类数据训练。

    

    多智能体协作中未解决的挑战是使AI代理能够利用动作特征和观测特征之间的语义关系。人类以高度直觉的方式利用这些关系。为了解决这个挑战，我们研究了网络体系结构对学习算法利用这些语义关系的倾向的影响。在一个程序生成的协作任务中，我们发现联合处理观察特征和动作特征的特征表示的注意力机制架构具有更好的归纳偏差，可以学习直觉策略。通过细粒度的评估和场景分析，我们展示了得到的策略是可解释的。此外，这样的代理与人类协作而无需接受任何人类数据训练。

    An unaddressed challenge in multi-agent coordination is to enable AI agents to exploit the semantic relationships between the features of actions and the features of observations. Humans take advantage of these relationships in highly intuitive ways. For instance, in the absence of a shared language, we might point to the object we desire or hold up our fingers to indicate how many objects we want. To address this challenge, we investigate the effect of network architecture on the propensity of learning algorithms to exploit these semantic relationships. Across a procedurally generated coordination task, we find that attention-based architectures that jointly process a featurized representation of observations and actions have a better inductive bias for learning intuitive policies. Through fine-grained evaluation and scenario analysis, we show that the resulting policies are human-interpretable. Moreover, such agents coordinate with people without training on any human data.
    
[^233]: 通过在线镜像下降实现无遗憾缓存

    No-Regret Caching via Online Mirror Descent. (arXiv:2101.12588v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2101.12588](http://arxiv.org/abs/2101.12588)

    本文提出了一种基于在线镜像下降算法的缓存算法，具有无遗憾的特性，关键依赖于请求过程的多样性比率。同时，作者证明了在需要整个文件缓存时，也能保证无遗憾性。

    

    我们研究了一种在线缓存问题，其中请求可以通过本地缓存来服务，以避免从远程服务器检索成本。缓存可以在请求的一批之后更新其状态，并存储每个文件的任意小部分。我们研究了基于在线镜像下降（OMD）策略的无遗憾算法。我们证明了遗憾的界限关键取决于请求过程的多样性，由请求批处理大小R和给定批处理中请求的最大重复度h所提供的多样性比率R/h。我们表征了不同多样性方案下OMD缓存策略关于遗憾的最优性。我们还证明了当缓存必须存储整个文件而不是一部分时，OMD策略可以与随机舍入方案相耦合，即使不能忽略更新成本，也能保持遗憾保证。我们通过最优输运理论为舍入问题提供了正式的表征，并提出了一种新的基于在线镜像下降策略的缓存算法。

    We study an online caching problem in which requests can be served by a local cache to avoid retrieval costs from a remote server. The cache can update its state after a batch of requests and store an arbitrarily small fraction of each file. We study no-regret algorithms based on Online Mirror Descent (OMD) strategies. We show that bounds for the regret crucially depend on the diversity of the request process, provided by the diversity ratio R/h, where R is the size of the batch, and h is the maximum multiplicity of a request in a given batch. We characterize the optimality of OMD caching policies w.r.t. regret under different diversity regimes. We also prove that, when the cache must store the entire file, rather than a fraction, OMD strategies can be coupled with a randomized rounding scheme that preserves regret guarantees, even when update costs cannot be neglected. We provide a formal characterization of the rounding problem through optimal transport theory, and moreover we propos
    
[^234]: 可靠机器学习的对称损失视角

    A Symmetric Loss Perspective of Reliable Machine Learning. (arXiv:2101.01366v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2101.01366](http://arxiv.org/abs/2101.01366)

    对称损失是一种新型的代理损失，能够使得学习过程对于受损标签更加鲁棒，从而提高分类器的性能。

    

    当在二元分类中最小化经验风险时，常常将零一损失替换为代理损失，以使学习目标易于优化。二元分类的代理损失例如逻辑损失，hinge损失和sigmoid损失广为人知。已知代理损失的选择会极大地影响训练分类器的性能，因此应该仔细选择。最近，满足某些对称条件（称为对称损失）的代理损失已经证明了它们在学习来自损坏标签的数据方面的实用性。在本文中，我们提供对称损失及其应用的概述。首先，我们回顾了对称损失如何在平衡误差率（BER）最小化和操作特征曲线下面积（AUC）最大化中产生鲁棒分类的方法。然后，我们演示了鲁棒AUC最大化方法可以受益于受损标签，尤其是与其他代理损失相比。

    When minimizing the empirical risk in binary classification, it is a common practice to replace the zero-one loss with a surrogate loss to make the learning objective feasible to optimize. Examples of well-known surrogate losses for binary classification include the logistic loss, hinge loss, and sigmoid loss. It is known that the choice of a surrogate loss can highly influence the performance of the trained classifier and therefore it should be carefully chosen. Recently, surrogate losses that satisfy a certain symmetric condition (aka., symmetric losses) have demonstrated their usefulness in learning from corrupted labels. In this article, we provide an overview of symmetric losses and their applications. First, we review how a symmetric loss can yield robust classification from corrupted labels in balanced error rate (BER) minimization and area under the receiver operating characteristic curve (AUC) maximization. Then, we demonstrate how the robust AUC maximization method can benefi
    
[^235]: 结构化连续稀疏化增强深度网络的训练效率

    Growing Efficient Deep Networks by Structured Continuous Sparsification. (arXiv:2007.15353v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2007.15353](http://arxiv.org/abs/2007.15353)

    本文提出一种结构化连续稀疏化的深度网络结构生长方法，通过连续松弛和采样稀疏子网络，可以在训练过程中达到紧凑的修剪网络结构，同时大幅降低计算复杂度并保持较高的准确率。

    

    我们开发了一种在训练过程中以精度和稀疏性为驱动的深度网络结构生长方法。与现有的基于完整模型或超网格架构的剪枝或架构搜索技术不同，我们的方法可以从一个小而简单的种子架构开始，动态地增长和修剪层和过滤器。通过将离散网络结构优化的连续松弛与采样稀疏子网络方案相结合，我们可以产生紧凑的修剪网络，同时显著降低训练的计算复杂度。例如，在ImageNet上，与基线ResNet-50相比，我们实现了49.7％的推理FLOPs和47.4％的训练FLOPs节省，同时保持75.2％的top-1精度--所有这些都没有任何专门的微调阶段。在CIFAR，ImageNet，PASCAL VOC和Penn Treebank上进行实验，使用卷积网络进行图像分类和语义分割。

    We develop an approach to growing deep network architectures over the course of training, driven by a principled combination of accuracy and sparsity objectives. Unlike existing pruning or architecture search techniques that operate on full-sized models or supernet architectures, our method can start from a small, simple seed architecture and dynamically grow and prune both layers and filters. By combining a continuous relaxation of discrete network structure optimization with a scheme for sampling sparse subnetworks, we produce compact, pruned networks, while also drastically reducing the computational expense of training. For example, we achieve $49.7\%$ inference FLOPs and $47.4\%$ training FLOPs savings compared to a baseline ResNet-50 on ImageNet, while maintaining $75.2\%$ top-1 accuracy -- all without any dedicated fine-tuning stage. Experiments across CIFAR, ImageNet, PASCAL VOC, and Penn Treebank, with convolutional networks for image classification and semantic segmentation, 
    
[^236]: UnRectDepthNet：使用通用处理常见相机失真模型的框架进行自监督单目深度估计

    UnRectDepthNet: Self-Supervised Monocular Depth Estimation using a Generic Framework for Handling Common Camera Distortion Models. (arXiv:2007.06676v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2007.06676](http://arxiv.org/abs/2007.06676)

    本文提出了一个通用的自监督管道用于从未矫正的单目视频中估计深度、欧几里得距离和视觉里程计，其可隐式地组织计算机视觉中的矫正过程，避免了其对模型复杂度或推理时间的影响。

    

    在传统的计算机视觉中，矫正是多视角深度估计的重要部分，通常包括极线校正和镜头畸变校正。这个过程显着简化了深度估计，因此被CNN方法采用。然而，矫正有一些副作用，包括视野缩小、重采样畸变和对校准误差的敏感性。这些效应在存在明显畸变（例如广角鱼眼摄像头）的情况下尤为明显。在本文中，我们提出了一个通用的自监督管道用于从未矫正的单目视频中估计深度、欧几里得距离和视觉里程计。我们在具有桶形畸变的未矫正KITTI数据集上展示了与矫正后KITTI数据集相当的精度水平。其调整的过程被CNN模型隐式地组织，学习失真模型而不增加模型复杂度或推理时间。

    In classical computer vision, rectification is an integral part of multi-view depth estimation. It typically includes epipolar rectification and lens distortion correction. This process simplifies the depth estimation significantly, and thus it has been adopted in CNN approaches. However, rectification has several side effects, including a reduced field of view (FOV), resampling distortion, and sensitivity to calibration errors. The effects are particularly pronounced in case of significant distortion (e.g., wide-angle fisheye cameras). In this paper, we propose a generic scale-aware self-supervised pipeline for estimating depth, euclidean distance, and visual odometry from unrectified monocular videos. We demonstrate a similar level of precision on the unrectified KITTI dataset with barrel distortion comparable to the rectified KITTI dataset. The intuition being that the rectification step can be implicitly absorbed within the CNN model, which learns the distortion model without incre
    
[^237]: 基于单调GAN的条件采样：从生成模型到无似然推断

    Conditional Sampling with Monotone GANs: from Generative Models to Likelihood-Free Inference. (arXiv:2006.06755v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2006.06755](http://arxiv.org/abs/2006.06755)

    本文提出了一种新的概率测度条件采样框架，使用单调GAN学习块状三角形映射，仅使用来自底层联合概率测度的样本实现无似然推断。

    

    我们提出了一种新的概率测度条件采样框架，使用了块状三角形传输映射。我们在Banach空间设置下开发了块状三角形传输的理论基础，建立了可以实现条件采样的一般条件，并在单调块状三角形映射与最优传输之间建立联系。基于该理论，我们提出了一种计算方法，称为单调生成对抗网络（M-GAN），用于学习合适的块状三角形映射。我们的算法仅使用来自底层联合概率测度的样本，因此无需似然。M-GAN的数值实验证明了在合成示例、涉及常微分方程和偏微分方程的贝叶斯反问题，以及概率图像修复中准确采样条件测度的能力。

    We present a novel framework for conditional sampling of probability measures, using block triangular transport maps. We develop the theoretical foundations of block triangular transport in a Banach space setting, establishing general conditions under which conditional sampling can be achieved and drawing connections between monotone block triangular maps and optimal transport. Based on this theory, we then introduce a computational approach, called monotone generative adversarial networks (M-GANs), to learn suitable block triangular maps. Our algorithm uses only samples from the underlying joint probability measure and is hence likelihood-free. Numerical experiments with M-GAN demonstrate accurate sampling of conditional measures in synthetic examples, Bayesian inverse problems involving ordinary and partial differential equations, and probabilistic image in-painting.
    
[^238]: Denise: 面向半正定矩阵的深度健壮主成分分析

    Denise: Deep Robust Principal Component Analysis for Positive Semidefinite Matrices. (arXiv:2004.13612v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2004.13612](http://arxiv.org/abs/2004.13612)

    Denise是一种基于深度学习的算法，用于对协方差矩阵进行低秩加稀疏分解，达到了与最先进技术相当的性能而且近乎接近20倍的加速。

    

    协方差矩阵的健壮主成分分析在隔离关键解释特征方面发挥着至关重要的作用。目前可用的执行低秩加稀疏分解的方法是针对特定矩阵的，也就是说，这些算法必须针对每个新的矩阵重新运行。由于这些算法的计算成本很高，因此最好学习和存储一个函数，在评估时几乎立即执行此分解。因此，我们引入了 Denise，一种基于深度学习的协方差矩阵的健壮主成分分析算法，或更一般地说，对称半正定矩阵，它学习到了这样一个函数。我们提供了 Denise 的理论保证。这些包括一个新的通用逼近定理，适用于我们的几何深度学习问题，并趋于学习问题的最优解。我们的实验表明，Denise 在分解质量方面与最先进的性能相匹配，同时近乎接近20倍的加速。

    The robust PCA of covariance matrices plays an essential role when isolating key explanatory features. The currently available methods for performing such a low-rank plus sparse decomposition are matrix specific, meaning, those algorithms must re-run for every new matrix. Since these algorithms are computationally expensive, it is preferable to learn and store a function that nearly instantaneously performs this decomposition when evaluated. Therefore, we introduce Denise, a deep learning-based algorithm for robust PCA of covariance matrices, or more generally, of symmetric positive semidefinite matrices, which learns precisely such a function. Theoretical guarantees for Denise are provided. These include a novel universal approximation theorem adapted to our geometric deep learning problem and convergence to an optimal solution to the learning problem. Our experiments show that Denise matches state-of-the-art performance in terms of decomposition quality, while being approximately $20
    
[^239]: 通过逻辑指导的认证强化学习

    Certified Reinforcement Learning with Logic Guidance. (arXiv:1902.00778v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1902.00778](http://arxiv.org/abs/1902.00778)

    本文提出了一种模型无关的强化学习算法，能够使用线性时态逻辑来制定马尔科夫决策过程的目标，将LTL属性转化为LDGBA自动机，通过调整同步奖励函数最大概率获得满足LTL规定要求的控制策略。

    

    强化学习在各种控制问题上得到了广泛应用。然而，在安全关键领域的应用需要一个系统和正式的方法来指定任务或目标。我们提出了一种无模型强化学习算法，它能够使用线性时态逻辑（LTL）来制定未知连续状态/动作马尔科夫决策过程（MDPs）的目标。给定的LTL属性被转化为极限确定化广义布氏自动机（LDGBA），通过LDGBA在行进过程中不断调整同步奖励函数。在某些假设下，该算法将保证合成出一个控制策略，其轨迹最大概率满足LTL规定的要求。

    Reinforcement Learning (RL) is a widely employed machine learning architecture that has been applied to a variety of control problems. However, applications in safety-critical domains require a systematic and formal approach to specifying requirements as tasks or goals. We propose a model-free RL algorithm that enables the use of Linear Temporal Logic (LTL) to formulate a goal for unknown continuous-state/action Markov Decision Processes (MDPs). The given LTL property is translated into a Limit-Deterministic Generalised Buchi Automaton (LDGBA), which is then used to shape a synchronous reward function on-the-fly. Under certain assumptions, the algorithm is guaranteed to synthesise a control policy whose traces satisfy the LTL specification with maximal probability.
    
[^240]: 正交统计学习

    Orthogonal Statistical Learning. (arXiv:1901.09036v4 [math.ST] UPDATED)

    [http://arxiv.org/abs/1901.09036](http://arxiv.org/abs/1901.09036)

    本文提出了一个两阶段样本拆分的元算法，该算法能够在评估总体风险时考虑干扰参数，并且实现的超额风险界的影响为二次。

    

    我们在一个统计学习的设置下提供了关于非渐近超额风险保证，其中目标参数所评估的总体风险取决于必须从数据中估计的未知干扰参数。我们分析了一个两阶段样本拆分的元算法，该算法将任意估计目标参数和干扰参数的算法作为输入。我们表明，如果总体风险满足一个称为Neyman正交性的条件，则干扰估计误差对元算法实现的超额风险界的影响为二次。我们的定理不关心用于目标和干扰的特定算法，只做出了有关它们各自性能的假设。这样，就可以利用现有机器学习的大量结果，为带有干扰组成的学习提供新的保证。此外，通过关注超额风险而不是参数估计，我们可以提供一个弱化的速率。

    We provide non-asymptotic excess risk guarantees for statistical learning in a setting where the population risk with respect to which we evaluate the target parameter depends on an unknown nuisance parameter that must be estimated from data. We analyze a two-stage sample splitting meta-algorithm that takes as input arbitrary estimation algorithms for the target parameter and nuisance parameter. We show that if the population risk satisfies a condition called Neyman orthogonality, the impact of the nuisance estimation error on the excess risk bound achieved by the meta-algorithm is of second order. Our theorem is agnostic to the particular algorithms used for the target and nuisance and only makes an assumption on their individual performance. This enables the use of a plethora of existing results from machine learning to give new guarantees for learning with a nuisance component. Moreover, by focusing on excess risk rather than parameter estimation, we can provide rates under weaker a
    
[^241]: 高斯误差线性单元（Gaussian Error Linear Units）

    Gaussian Error Linear Units (GELUs). (arXiv:1606.08415v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1606.08415](http://arxiv.org/abs/1606.08415)

    本文提出了一种高效的神经网络激活函数——GELU。通过对输入值进行加权而非符号门限控制，GELU在多项任务中均取得了性能提升。

    

    我们提出了高效的神经网络激活函数——高斯误差线性单元（GELU）。GELU激活函数为$x\Phi(x)$，其中$\Phi(x)$代表标准高斯累积分布函数。与ReLU（$x\mathbf{1}_{x>0}$）激活函数通过输入的符号进行门限控制不同，GELU非线性按输入值加权。我们对GELU非线性、ReLU以及ELU激活函数进行了实证评估，并发现在所有计算机视觉、自然语言处理和语音任务中，GELU均取得了性能提升。

    We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is $x\Phi(x)$, where $\Phi(x)$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs ($x\mathbf{1}_{x>0}$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.
    

