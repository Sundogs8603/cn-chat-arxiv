# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining.](http://arxiv.org/abs/2305.10429) | DoReMi方法使用分组分布式鲁棒优化训练小型代理模型以产生域权重，再使用这些权重重新采样数据集训练大型模型，相比使用默认权重的基线模型，在The Pile和GLaM数据集上平均提高了6.5%和4.7%的few-shot下游准确度，分别使用2.6倍和相同的训练步骤达到基线准确度。 |
| [^2] | [Accelerating Transformer Inference for Translation via Parallel Decoding.](http://arxiv.org/abs/2305.10427) | 通过并行解码，本文提出了一种快速推断Transformer在翻译中的应用的方法，不需要修改现有模型并在保持翻译质量的同时加速了现有模型。 |
| [^3] | [ZeroFlow: Fast Zero Label Scene Flow via Distillation.](http://arxiv.org/abs/2305.10424) | ZeroFlow是一种简单的蒸馏算法，使用无标签方法生成伪标签以监督前向传递模型，实现了在使用零人工标签情况下对大规模点云进行实时场景流估计。 |
| [^4] | [Evolving Tsukamoto Neuro Fuzzy Model for Multiclass Covid 19 Classification with Chest X Ray Images.](http://arxiv.org/abs/2305.10421) | 本研究提出了一种基于演化的Tsukamoto神经模糊模型，在六个从胸透图像中提取的纹理特征的帮助下，将Covid-19疾病从正常和肺炎病例中区分出来，并实验表明其检测精度高于现有模型。 |
| [^5] | [On Consistency of Signatures Using Lasso.](http://arxiv.org/abs/2305.10413) | 本文重新审视了Lasso回归对于签名变换的一致性问题，并发现对于不同的过程和时间序列，选择适当的签名定义和随机模型可以提高Lasso回归的一致性。 |
| [^6] | [Wasserstein Gradient Flows for Optimizing Gaussian Mixture Policies.](http://arxiv.org/abs/2305.10411) | 本文提出了使用Wasserstein梯度流来优化基于高斯混合模型的机器人运动策略的方法，通过$L^2$-Wasserstein距离来约束策略更新，提高了策略优化的稳定性和样本效率。 |
| [^7] | [Variational Classification.](http://arxiv.org/abs/2305.10406) | 提出一种新的变分分类方法，通过引入潜变量建模来优化训练，允许灵活的设计选择以改善校准和对抗鲁棒性，实验结果表明其对于域外数据的分类准确性得到了保持。 |
| [^8] | [End-To-End Latent Variational Diffusion Models for Inverse Problems in High Energy Physics.](http://arxiv.org/abs/2305.10399) | 本研究提出了一种新的生成式深度学习方法，潜变分扩散模型，用于解决大型强子对撞机中的反问题。实现了重建理论动力学量的全局分布的有效性，同时确保所学后验分布遵守已知物理。 |
| [^9] | [RelationMatch: Matching In-batch Relationships for Semi-supervised Learning.](http://arxiv.org/abs/2305.10397) | RelationMatch是一种利用矩阵交叉熵（MCE）损失函数的方法，可以匹配批内关系，有效提高半监督学习和监督学习的性能。 |
| [^10] | [Optimality of Message-Passing Architectures for Sparse Graphs.](http://arxiv.org/abs/2305.10391) | 本研究证明了将消息传递神经网络应用于稀疏图的节点分类任务是渐近本地贝叶斯最优的，提出了一种实现最优分类器的算法，并将最优分类器的性能理论上与现有学习方法进行了比较。 |
| [^11] | [Raising the Bar for Certified Adversarial Robustness with Diffusion Models.](http://arxiv.org/abs/2305.10388) | 本研究提出使用扩散模型生成训练数据来提高认证防御模型的鲁棒性，并给出了扩展这一方法的建议，证明了广义间隙是一个良好的预测指标。 |
| [^12] | [Logit-Based Ensemble Distribution Distillation for Robust Autoregressive Sequence Uncertainties.](http://arxiv.org/abs/2305.10384) | 本论文介绍了一种基于Logit的集成模型蒸馏方法，能够有效地将知识（epistemic）和数据（aleatoric）不确定性分开，对于大规模自然语言序列到序列的任务能够提高student模型的表现。 |
| [^13] | [Active Learning in Symbolic Regression Performance with Physical Constraints.](http://arxiv.org/abs/2305.10379) | 本文探讨了利用进化符号回归作为主动学习中的方法来提出哪些数据应该被采集，通过“委员会查询”来减少所需数据，并在重新发现已知方程所需的数据方面实现最新的结果。 |
| [^14] | [Human Choice Prediction in Non-Cooperative Games: Simulation-based Off-Policy Evaluation.](http://arxiv.org/abs/2305.10361) | 本文研究了语言游戏中的离线策略评估，并提出了一种结合真实和模拟数据的新方法。 |
| [^15] | [NUANCE: Near Ultrasound Attack On Networked Communication Environments.](http://arxiv.org/abs/2305.10358) | 本研究探究了利用近超声波特洛伊木马对亚马逊Alexa语音服务的主要不可听攻击向量，并提出了针对企业、移动和工控系统的攻击防御策略。 |
| [^16] | [Spectral Clustering via Orthogonalization-Free Methods.](http://arxiv.org/abs/2305.10356) | 本文提出了四种无正交化方法作为谱聚类降维，不需要昂贵的特征值估计，在聚类质量和计算成本方面均优于已有方法，适合于并行计算。 |
| [^17] | [An Ensemble Learning Approach for Exercise Detection in Type 1 Diabetes Patients.](http://arxiv.org/abs/2305.10353) | 为解决人工胰腺系统缺乏对运动引起的血糖摄取的检测能力，本文提出了一个集成学习框架，结合了数据驱动的生理模型和Siamese网络，能够高精度地利用多个生理信号流进行运动检测。 |
| [^18] | [Appliance Detection Using Very Low-Frequency Smart Meter Time Series.](http://arxiv.org/abs/2305.10352) | 本文对时间序列分类器在极低频智能电表数据中检测不同家电的存在/缺失进行了深入评估和比较，结果表明...... |
| [^19] | [BIOT: Cross-data Biosignal Learning in the Wild.](http://arxiv.org/abs/2305.10351) | 该论文提出了一种可以在多个生物信号数据源上进行训练，并可在所有生物信号任务上进行微调的基础模型BIOT。通过将不同的生物信号令牌化为统一的“生物信号句子”，该模型可以实现跨数据源的学习，处理各种格式的生物信号，具有出色的性能。 |
| [^20] | [Multiverse at the Edge: Interacting Real World and Digital Twins for Wireless Beamforming.](http://arxiv.org/abs/2305.10350) | 本文提出了一个数字孪生体的“多元宇宙”范式，其中有几个数字孪生体试图在不同级别的保真度下捕捉真实世界，通过进行自学习以增强基于DL的实时决策。这项工作将有助于加速移动环境中的方向波束选择。 |
| [^21] | [Subject-based Non-contrastive Self-Supervised Learning for ECG Signal Processing.](http://arxiv.org/abs/2305.10347) | 本文提出了一种基于受试者的无对照自监督学习方法，用于心电信号处理，不需要数据增强或负对，并在ECG处理中实现了优于最先进SSL方法的表现。 |
| [^22] | [G-Adapter: Towards Structure-Aware Parameter-Efficient Transfer Learning for Graph Transformer Networks.](http://arxiv.org/abs/2305.10329) | 本文提出一种名为G-Adapter的面向图形Transformer网络的结构感知参数高效迁移学习算法，通过对一组下游图形任务进行广泛测试，证明了将PEFT技术应用于GTNs并非最佳解决方案。 |
| [^23] | [Automatic Photo Orientation Detection with Convolutional Neural Networks.](http://arxiv.org/abs/2305.10319) | 本论文使用CNN解决了照片方向检测的问题，并在数据集上显著提高性能。使用Guided Backpropagation获得了CNN检测方向的见解。 |
| [^24] | [MetaModulation: Learning Variational Feature Hierarchies for Few-Shot Learning with Fewer Tasks.](http://arxiv.org/abs/2305.10309) | 提出了一种名为MetaModulation的少任务Few-Shot Learning方法，使用神经网络在元训练期间调制批量归一化参数以增加元训练任务的密度。本方法通过变分MetaModulation介绍了学习变分特征层次，可以考虑任务不确定性并生成更多样的任务。 |
| [^25] | [Rethinking Data Augmentation for Tabular Data in Deep Learning.](http://arxiv.org/abs/2305.10308) | 本研究提出了一种新的表格数据增强方法“随机连续嵌入”（Random Continuous Embedding，RCE），能够提高 Transformer-based 预训练模型的自监督学习性能，大幅优于现有方法，并使得自监督学习模型能够在监督表格学习中优于树形方法。 |
| [^26] | [Estimation of Remaining Useful Life and SOH of Lithium Ion Batteries (For EV Vehicles).](http://arxiv.org/abs/2305.10298) | 本文介绍不同方法估计锂电池剩余寿命，提出了一种利用机器学习技术的新方法，可以利用电池性能参数准确预测电池的寿命。 |
| [^27] | [DualFL: A Duality-based Federated Learning Algorithm with Communication Acceleration in the General Convex Regime.](http://arxiv.org/abs/2305.10294) | DualFL是一种基于对偶的联邦学习算法，通过具体对偶形式解决分布式优化问题，并保证了即使使用不精确的本地解决方案也可以实现最佳通信复杂度。 |
| [^28] | [Infinite Class Mixup.](http://arxiv.org/abs/2305.10293) | 本文提出了一种直接通过混合分类器而不是标签来增强样本的策略。新的分类器是输入对分类器向量的线性插值，使分类器之间的关系更加准确，从而提高深度网络的分类性能。 |
| [^29] | [Reward-agnostic Fine-tuning: Provable Statistical Benefits of Hybrid Reinforcement Learning.](http://arxiv.org/abs/2305.10282) | 本文提出一种新的三阶段混合RL算法，不需要奖励信息，有效地利用在线和离线数据，从而实现细调以获得更好的结果。 |
| [^30] | [Large-Scale Package Manipulation via Learned Metrics of Pick Success.](http://arxiv.org/abs/2305.10272) | 本文讨论了基于学习度量的大规模包裹操作，通过训练拾取成功预测器和学习拾取质量度量，实现了能够大规模部署的强力抓握策略。 |
| [^31] | [State Representation Learning Using an Unbalanced Atlas.](http://arxiv.org/abs/2305.10267) | 本文介绍了一种使用不平衡图册（UA）方法的状态表示学习，该方法可以超越最先进的自监督学习方法。 |
| [^32] | [Sharpness & Shift-Aware Self-Supervised Learning.](http://arxiv.org/abs/2305.10252) | 本文针对自监督学习中的分类下游任务，开发了锐度与数据位移感知理论并提出了基于对比学习的方法(SSA-CLR)，通过特征提取器锐度最小化与傅里叶变换数据增强技术缓解理想分布与实际分布间的数据位移，实现了对分类任务的改进。 |
| [^33] | [Energy Loss Prediction in IoT Energy Services.](http://arxiv.org/abs/2305.10238) | 本文提出了一种新颖的IoT能量服务的能量损失预测框架ELP，基于Easeformer算法用于预测共享能量环境中物联网设备的电池电量，进而估算能量损失，实验结果表明该框架明显优于现有方法。 |
| [^34] | [Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility.](http://arxiv.org/abs/2305.10235) | 本研究是一项关于大型语言模型方面的实证研究，对主流语言模型进行了大量查询和分析，结果发现这些模型存在着鲁棒性、一致性和可信性方面的潜在风险。 |
| [^35] | [Exploring Inductive Biases in Contrastive Learning: A Clustering Perspective.](http://arxiv.org/abs/2305.10229) | 本文比较了对比学习和监督学习方法形成的簇，揭示了对比学习可以生成具有局部密度但无全局密度的簇，而监督学习创建具有局部和全局密度的簇。同时，作者提出了使用图卷积网络分类器作为处理局部密集簇的线性分类器的替代方法，并利用t-SNE可视化证明了对比和监督学习方法产生的特征之间的差异。 |
| [^36] | [Reaching Kesten-Stigum Threshold in the Stochastic Block Model under Node Corruptions.](http://arxiv.org/abs/2305.10227) | 我们提出了第一个多项式时间算法，在小的破坏节点比例下即可达到Kesten-Stigum临界点的弱恢复。 |
| [^37] | [rWISDM: Repaired WISDM, a Public Dataset for Human Activity Recognition.](http://arxiv.org/abs/2305.10222) | 本文研究了行为识别公共数据集WISDM，发现其中存在一些问题，降低了分类器的性能和信任度。通过修复数据集，提高了分类器的性能。 |
| [^38] | [Separability and Scatteredness (S&S) Ratio-Based Efficient SVM Regularization Parameter, Kernel, and Kernel Parameter Selection.](http://arxiv.org/abs/2305.10219) | 该文通过分析数据的可分性和离散度，提出了一种基于S&S比的有效SVM正则化参数、核函数和核参数选择方法，表现较传统方法更优。 |
| [^39] | [A Novel Stochastic LSTM Model Inspired by Quantum Machine Learning.](http://arxiv.org/abs/2305.10212) | 本文提出了一种新型随机LSTM模型，受到变分量子算法的启发，探讨在非量子框架下接近QML的报告成功和潜在好处。 |
| [^40] | [Towards Object Re-Identification from Point Clouds for 3D MOT.](http://arxiv.org/abs/2305.10210) | 该论文研究面向三维MOT中的点云再识别问题，提出了一种轻量级匹配头用于点云ReID的网络，通过实验结果表明，随着传感器分辨率的提高和观测点密度的增加，点云ReID的表现逐渐接近于图像ReID。 |
| [^41] | [Exploring the Space of Key-Value-Query Models with Intention.](http://arxiv.org/abs/2305.10203) | 该论文探索了共享 Attention 输入结构但不限于 Attention 计算的模型空间，发现可以使用神经网络计算最小二乘问题的解，这是 Attention 无法高效逼近的，对于神经网络而言也是一种严格的扩展。 |
| [^42] | [Sparsifying Spiking Networks through Local Rhythms.](http://arxiv.org/abs/2305.10191) | 本文展示了如何通过使用局部节律来稀疏脉冲网络，从而减少通信和计算能量的要求。 |
| [^43] | [Evaluating Dynamic Conditional Quantile Treatment Effects with Applications in Ridesharing.](http://arxiv.org/abs/2305.10187) | 该论文提出了一种评估代步共享平台中动态条件分位治疗效果的框架，通过个体CQTE之和来简化动态CQTE的评估。 |
| [^44] | [Algorithms for Boolean Matrix Factorization using Integer Programming.](http://arxiv.org/abs/2305.10185) | 本文提出了一种基于整数规划的交替优化策略，解决了布尔矩阵分解的NP难题，并且使用另一个整数规划将多个解组合成最优解，实验表明算法在中等规模问题上优于现有技术。 |
| [^45] | [Exploring the cloud of feature interaction scores in a Rashomon set.](http://arxiv.org/abs/2305.10181) | 本文通过探索拉绍蒙集合中准确性类似的模型，引入了特征交互分数（FIS）来检测特征的相互作用。相较于从单个预先指定的模型中提取特征交互，本文提供了更为可靠的方式。 |
| [^46] | [Goal-Conditioned Supervised Learning with Sub-Goal Prediction.](http://arxiv.org/abs/2305.10171) | 本文提出了一种扩展GCSL算法的方法， TraIL利用轨迹中的信息预测子目标，以显著提高性能。 |
| [^47] | [A Global-Local Approximation Framework for Large-Scale Gaussian Process Modeling.](http://arxiv.org/abs/2305.10158) | 本文提出了一种名为TwinGP的新的全局-局部近似框架，使用子集数据方法，并将相关函数建模为全局和局部核的组合。TwinGP在计算成本的一小部分下表现与最先进的GP建模方法相当或更好。 |
| [^48] | [Provably Correct Physics-Informed Neural Networks.](http://arxiv.org/abs/2305.10157) | 该论文提出了一种名为$\partial$-CROWN的框架，以保证物理知识神经网络（PINN）具有全局正确性的最坏剩余误差，并证明了该框架在获得有效证书方面的有效性。 |
| [^49] | [Lingo3DMol: Generation of a Pocket-based 3D Molecule using a Language Model.](http://arxiv.org/abs/2305.10133) | 本文提出了一种基于口袋的三维分子生成方法，利用扰动和恢复预训练任务和一种新的分子表示形式。 该方法结合了语言模型和几何深度学习的优点，使得语言模型可以生成准确的三维分子。 |
| [^50] | [Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models.](http://arxiv.org/abs/2305.10120) | 针对大规模文本到图像模型可能被误用生成有害内容的问题，该论文提出了一种选择性遗忘方法，即持续学习方法，可在深度生成模型中实现可控的遗忘，用户可指定消除哪些概念。 |
| [^51] | [Bridging the Gap: Enhancing the Utility of Synthetic Data via Post-Processing Techniques.](http://arxiv.org/abs/2305.10118) | 本文介绍了一种利用生成对抗网络生成合成数据集，并通过三种新颖的后处理技术改进合成数据集质量和多样性的方法。作者称其为Gap Filler (GaFi)流程并在真实图像上进行评估。 |
| [^52] | [Can Deep Learning Reliably Recognize Abnormality Patterns on Chest X-rays? A Multi-Reader Study Examining One Month of AI Implementation in Everyday Radiology Clinical Practice.](http://arxiv.org/abs/2305.10116) | 本研究开发了一种基于深度学习的自动检测算法，可以在胸部X光片上检测出七种特定放射学发现，并且该算法的性能优于评估图像的六名放射科医师。 |
| [^53] | [An Ensemble Deep Learning Approach for COVID-19 Severity Prediction Using Chest CT Scans.](http://arxiv.org/abs/2305.10115) | 该论文基于深度学习集成的方法，使用STOIC数据集对COVID-19胸部CT扫描进行严重程度预测，并采用了数据增强技术和测试时间增强以提高性能。 |
| [^54] | [Automatic Hyperparameter Tuning in Sparse Matrix Factorization.](http://arxiv.org/abs/2305.10114) | 稀疏矩阵分解中使用贝叶斯框架，提出了一种通过评估稀疏矩阵先验中归一化因子的零点来进行超参数调整的新型数值方法，并在地面真实稀疏矩阵重建中表现出优异性能。 |
| [^55] | [Neuro-Symbolic AI for Compliance Checking of Electrical Control Panels.](http://arxiv.org/abs/2305.10113) | 本文提出了一种基于神经符号的AI方法，结合深度学习技术和答案集编程来自动化电气控制面板的合规性验证。该方法可以在只有非常有限的训练数据下识别可能存在的异常和错误，实验结果表明该方法具有很好的效果。 |
| [^56] | [Predicting Tweet Engagement with Graph Neural Networks.](http://arxiv.org/abs/2305.10103) | 本研究提出TweetGage，一个基于图神经网络的解决方案，通过表示发布帖子间的语义关联来预测用户在社交媒体上的互动，相对其他研究只考虑帖子文本和发布用户等因素，有效提高了预测准确性。 |
| [^57] | [A proof of imitation of Wasserstein inverse reinforcement learning for multi-objective optimization.](http://arxiv.org/abs/2305.10089) | 本文证明了Wasserstein反向强化学习模型适用于多目标优化问题，可让学习者的奖励值和最优解模仿专家，具有一定的实用价值。 |
| [^58] | [Cold PAWS: Unsupervised class discovery and the cold-start problem.](http://arxiv.org/abs/2305.10071) | 本文提出了一种新方法，通过结合自我监督、聚类和流形学习技术，解决冷启动或无监督选择标记问题，并在多个公共数据集上进行了测试，获得了更好的性能。 |
| [^59] | [XAI for Self-supervised Clustering of Wireless Spectrum Activity.](http://arxiv.org/abs/2305.10060) | 本文提出了一种用于解释深度聚类的自监督学习架构的方法，使用基于卷积神经网络的表示学习部分和聚类部分，并利用引导反向传播技术解释相交区域的含义。 |
| [^60] | [A hybrid feature learning approach based on convolutional kernels for ATM fault prediction using event-log data.](http://arxiv.org/abs/2305.10059) | 该论文提出了一种基于卷积核的混合特征学习方法，通过结合深度学习、核方法和基于规则的特征选择从ATM事件日志中提取相关特征，用于早期ATM故障预测，在真实的ATM事件日志数据上进行的广泛实验表明其优于现有技术。 |
| [^61] | [Physics-driven machine learning for the prediction of coronal mass ejections' travel times.](http://arxiv.org/abs/2305.10057) | 该论文介绍了物理驱动的人工智能方法用于日冕物质抛射旅行时间预测，使用了确定性拖拽模型和两个神经网络，利用远程监测和现场数据，显着提高了预测的准确性和稳健性。 |
| [^62] | [SHoP: A Deep Learning Framework for Solving High-order Partial Differential Equations.](http://arxiv.org/abs/2305.10033) | 该论文提出了一种名为SHoP的深度学习框架，以解决高阶偏微分方程问题。该框架通过神经网络推导出高阶导数规则，并将网络展开成泰勒级数，提供一个明确的PDE解决方案。 |
| [^63] | [Transfer Learning for Fine-grained Classification Using Semi-supervised Learning and Visual Transformers.](http://arxiv.org/abs/2305.10018) | 使用半监督学习技术对ViT模型进行微调，可用于缺乏注释数据的细粒度分类任务，比传统CNN和ViT都表现更好，有助于解决电子商务中图像标注问题。 |
| [^64] | [Utility Theory of Synthetic Data Generation.](http://arxiv.org/abs/2305.10015) | 本文从统计学角度建立效用理论，旨在基于一般性指标定量评估合成算法的效用，效用指标的分析界限揭示了指标收敛的关键条件，令人惊讶的是，只要下游学习任务中的模型规范是正确的，合成特征分布不一定与原始特征分布相同，效用指标会收敛。 |
| [^65] | [A Survey on Multi-Objective based Parameter Optimization for Deep Learning.](http://arxiv.org/abs/2305.10014) | 本文综述了单一目标的深度学习优化技术，重点介绍了多目标优化方法在深度学习中的应用，并探讨了多目标优化在深度学习中的挑战和未来方向。 |
| [^66] | [Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling.](http://arxiv.org/abs/2305.09993) | Reprompting是一种无需人类干预的算法，通过迭代采样新配方解决多步推理任务，比人类编写的思维链提示表现更好，还可以提高较弱模型的性能。 |
| [^67] | [A robust multi-domain network for short-scanning amyloid PET reconstruction.](http://arxiv.org/abs/2305.09986) | 本论文介绍了一种鲁棒的多域网络，旨在恢复短时间内获取的低质量淀粉样PET图像。网络的核心贡献在于引入了映射标签进行有效学习，使得该网络可在多个训练域和未知域中高效校正淀粉样PET数据集。 |
| [^68] | [Stochastic Ratios Tracking Algorithm for Large Scale Machine Learning Problems.](http://arxiv.org/abs/2305.09978) | 本文提出了一种新的算法，在经典的SGD框架下实现自适应步长选择，在逻辑回归和深度神经网络上测试了所提出的算法，并证明了该算法可以生成与手动调整得到的最佳步长相当的步长。 |
| [^69] | [Real-Time Flying Object Detection with YOLOv8.](http://arxiv.org/abs/2305.09972) | 本文提出了一个基于YOLOv8的通用模型，可实现实时检测的飞行物体；通过进一步训练，生成精细模型，克服了在真实环境数据中存在的问题。 |
| [^70] | [Variable Length Embeddings.](http://arxiv.org/abs/2305.09967) | 本文介绍了可变长度嵌入（VLEs）的深度学习架构，采用自回归模型来生成由任意数量标记组成的潜在表示。VLE在涉及重构和图像分解的任务中表现出色，且比最先进的VAE使用更少的参数。 |
| [^71] | [Deep quantum neural networks form Gaussian processes.](http://arxiv.org/abs/2305.09957) | 本文证明了基于Haar随机酉或正交深量子神经网络的某些模型的输出会收敛于高斯过程。然而，这种高斯过程不能用于通过贝叶斯统计学来有效预测QNN的输出。 |
| [^72] | [Understanding the Initial Condensation of Convolutional Neural Networks.](http://arxiv.org/abs/2305.09947) | 本研究揭示了小初始化和基于梯度的训练方法下卷积神经网络的凝聚现象，并在理论上证明了在有限训练期间，CNN的卷积核将会收敛到一个或几个方向。 |
| [^73] | [DeepMSS: Deep Multi-Modality Segmentation-to-Survival Learning for Survival Outcome Prediction from PET/CT Images.](http://arxiv.org/abs/2305.09946) | 提出了一种DeepMSS模型，采用新颖的Segmentated-to-Survival（STS）框架，使用多模态渐进聚合网络（MMPAN）来探索肿瘤内外的预后信息，并通过自我注意力机制增强的深度生存模型进行生存预测，取得了在两个公共PET/CT图像数据集上优于几种最先进的方法的结果。 |
| [^74] | [Pittsburgh Learning Classifier Systems for Explainable Reinforcement Learning: Comparing with XCS.](http://arxiv.org/abs/2305.09945) | 本文介绍了两个新的匹兹堡学习分类器系统，与经典的密歇根系统 XCS 进行了比较，并在确定性和随机 FrozenLake 环境中获得了较好的表现。 |
| [^75] | [Demonstration-free Autonomous Reinforcement Learning via Implicit and Bidirectional Curriculum.](http://arxiv.org/abs/2305.09943) | 本文提出了一种无需演示的自主强化学习算法（IBC），通过辅助代理和基于最优输运的双向目标课程，能够在无需先前数据依赖的情况下，实现从非周期性交互中学习，并在稀疏任务相关交互的环境中取得更好的表现。 |
| [^76] | ["I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation.](http://arxiv.org/abs/2305.09941) | 本论文研究了如何以TGNB人群的声音为中心，评估开放式语言生成中的偏见。通过理解TGNB个体的经历，提出了以TGNB人群为中心的OLG系统评估框架，并且包括一个为TGNB人群设计的调查工具和分析方法。 |
| [^77] | [Characterizing Long-Tail Categories on Graphs.](http://arxiv.org/abs/2305.09938) | 该研究提出了图上长尾分类的第一个泛化边界，并提出了一种可表征长尾类别的行为并提高机器学习模型在现实世界网络中的泛化性能的新图表示学习框架。 |
| [^78] | [Mitigating Group Bias in Federated Learning: Beyond Local Fairness.](http://arxiv.org/abs/2305.09931) | 本文研究了联邦学习中群体偏见问题，并提出了一种全局公平、利用拉格朗日乘数考虑群体公平的训练算法，实验证明该方法在公平性和准确性方面优于现有方法。 |
| [^79] | [Model-based Validation as Probabilistic Inference.](http://arxiv.org/abs/2305.09930) | 本文提出了模型验证作为概率推断的方法，在自主系统中估计故障分布。该方法使用基于模型的方法表示故障轨迹分布，并利用自动微分计算轨迹梯度。在多个场景下进行了演示，结果表明在样本效率和参数空间覆盖范围方面取得了改善。 |
| [^80] | [Tinto: Multisensor Benchmark for 3D Hyperspectral Point Cloud Segmentation in the Geosciences.](http://arxiv.org/abs/2305.09928) | Tinto是一个多传感器数字露头基准测试数据集，旨在促进开发和验证地质制图的深度学习方法，特别是针对非结构化的3D数据如点云。 |
| [^81] | [A Genetic Fuzzy System for Interpretable and Parsimonious Reinforcement Learning Policies.](http://arxiv.org/abs/2305.09922) | 提出了一种基于基因模糊系统的强化学习策略，可演化出可解释的简约策略，并能有效平衡策略性能与复杂性。 |
| [^82] | [Assessing the Impact of Context Inference Error and Partial Observability on RL Methods for Just-In-Time Adaptive Interventions.](http://arxiv.org/abs/2305.09913) | 本文探讨了在及时自适应干预中，强化学习方法如何学习干预选项选择策略，结果表明上下文推断误差和部分可观察性对学习有效策略的能力产生影响，通过在上下文不确定性增加时从上下文推断中传播的不确定性可以提高干预效果，而策略梯度算法可以提供对部分观察到的行为状态信息的非凡鲁棒性。 |
| [^83] | [Incremental Outlier Detection Modelling Using Streaming Analytics in Finance & Health Care.](http://arxiv.org/abs/2305.09907) | 本文利用在线异常检测算法建立了流式环境下的增量学习模型，在金融和医疗保健领域取得实际应用。 |
| [^84] | [On the ISS Property of the Gradient Flow for Single Hidden-Layer Neural Networks with Linear Activations.](http://arxiv.org/abs/2305.09904) | 过拟合如何影响梯度下降训练在梯度估计不确定时的稳健性，该研究研究了具有单个任意宽度的隐藏层和任意输入输出数量的线性神经网络。 |
| [^85] | [Privacy Loss of Noisy Stochastic Gradient Descent Might Converge Even for Non-Convex Losses.](http://arxiv.org/abs/2305.09903) | 本文研究了DP-SGD，一种常用的噪声性SGD变体，发现其隐私损失呈指数收敛，不需要损失函数强凸或平滑假设。 |
| [^86] | [Equivariant Few-Shot Learning from Pretrained Models.](http://arxiv.org/abs/2305.09900) | 本文提出了一种基于预训练模型的$\lambda$-\textit{equitune}方法，它使用\textit{重要性权重}$\lambda$对特征进行平均，可以显著提高等变小样本学习的表现。 |
| [^87] | [Complementary Classifier Induced Partial Label Learning.](http://arxiv.org/abs/2305.09897) | 本文提出了一种利用补充标签诱导补充分类器的方法，该分类器与传统PLL分类器形成对抗关系，以消除候选标签集中的误报标签，并使用动态图协助消除歧义。实验结果表明该方法优于现有PLL方法。 |
| [^88] | [SS-BSN: Attentive Blind-Spot Network for Self-Supervised Denoising with Nonlocal Self-Similarity.](http://arxiv.org/abs/2305.09890) | 本文提出了一种自监督训练去噪的方法，其使用了一种新颖的自注意力模块——自相似性注意力（SS-Attention），可以捕捉非局部自相似性并解决性能上的限制。 |
| [^89] | [Simplifying Distributed Neural Network Training on Massive Graphs: Randomized Partitions Improve Model Aggregation.](http://arxiv.org/abs/2305.09887) | 本论文提出了一种简化的分布式 GNN 训练框架，只进行定期的模型聚合，并使用随机分割来隐式交换信息，实现了比现有方法更好的可扩展性、收敛速度和性能。 |
| [^90] | [A Signed Subgraph Encoding Approach via Linear Optimization for Link Sign Prediction.](http://arxiv.org/abs/2305.09869) | 本文提出了一种名为SELO的链路符号预测模型，使用子图编码方法学习有向网络中的边嵌入。通过引入有符号子图编码方法，并使用线性优化方法将每个子图嵌入到似然矩阵中而非邻接矩阵中，该模型优于其他最先进的方法。 |
| [^91] | [The Principle of Uncertain Maximum Entropy.](http://arxiv.org/abs/2305.09868) | 介绍了不确定最大熵原理，该原理可以处理模型元素不可观测的情况，并优于特定条件下的最大熵方法。同时将黑匣子机器学习模型的输出用作不确定机器熵框架的输入，性能得到了提高。 |
| [^92] | [Explaining black box text modules in natural language with language models.](http://arxiv.org/abs/2305.09863) | 本文介绍了一种名为Summarize and Score（SASC）的方法，该方法可以自动获取黑盒文本模块的自然语言解释以及解释可靠程度的分数。研究者们已经在合成模块和BERT模型中使用SASC，让我们可以解释模块的选择性，这对于增强大型语言模型的可解释性非常重要。 |
| [^93] | [Epsilon Sampling Rocks: Investigating Sampling Strategies for \\Minimum Bayes Risk Decoding for Machine Translation.](http://arxiv.org/abs/2305.09860) | 本文研究了用于机器翻译最小贝叶斯风险解码的不同采样策略，并发现了epsilon采样方式能够使得解码结果显著地优于其他所有已测试的采样方式和束搜索解码。 |
| [^94] | [Smaller Language Models are Better Black-box Machine-Generated Text Detectors.](http://arxiv.org/abs/2305.09859) | 本文研究发现，小型语言模型更适用于作为通用文本检测器，可以更加精确地检测出机器生成的文本，而检测器和生成模型是否具有相同的架构或语料库并不会对检测性能产生显著影响。 |
| [^95] | [Knowledge Graph Completion Models are Few-shot Learners: An Empirical Study of Relation Labeling in E-commerce with LLMs.](http://arxiv.org/abs/2305.09858) | 本文通过对知识图谱中关系标注的实证研究，发现大型语言模型具有强大的学习能力以及在少量标记数据下预测产品类型之间关系的有效性。 |
| [^96] | [Keep It Simple: Fault Tolerance Evaluation of Federated Learning with Unreliable Clients.](http://arxiv.org/abs/2305.09856) | 本文评估了具有不可靠客户端的联邦学习的容错性，研究表明相对较简单的FL算法在此情境下也能表现良好。 |
| [^97] | [Selective Guidance: Are All the Denoising Steps of Guided Diffusion Important?.](http://arxiv.org/abs/2305.09847) | 该研究提出通过优化去噪步骤来减少稳定扩散（SD）引导式推理流程的复杂性，并证明后续迭代对优化的敏感度较低。优化去噪处理循环的最后20％或最后50％可以分别将推理时间减少8.2％或20.3％，同时对人眼几乎没有可察觉的变化。 |
| [^98] | [A Note on Dimensionality Reduction in Deep Neural Networks using Empirical Interpolation Method.](http://arxiv.org/abs/2305.09842) | 本文提出了一种名为DNN-EIM的算法来在监督机器学习中使用EIM算法有效地减少训练数据的维数。同时考虑了在分类和PDEs方面的应用，该算法可以为每个类别或EIM点设计并行的DNN，所需权重比传统方法少得多。 |
| [^99] | [Coagent Networks: Generalized and Scaled.](http://arxiv.org/abs/2305.09838) | 论文提出了一种强大而灵活的合作智能网络框架，可以异步计算网络不同部分、吸收反向传播不能使用的不可微组件、探索和/或时态抽象的分层网络，并使用高效算法进行分布式和并行学习。在基准问题上的模拟表明，该算法在性能上有显著提高。 |
| [^100] | [Revisiting the Minimalist Approach to Offline Reinforcement Learning.](http://arxiv.org/abs/2305.09836) | 这篇论文提出了一种名为ReBRAC的极简算法，它在TD3+BC方法的基础上整合了设计元素，通过对近期离线强化学习研究的回顾性分析，证明其在离线强化学习上的领先地位。 |
| [^101] | [Mimetic Initialization of Self-Attention Layers.](http://arxiv.org/abs/2305.09828) | 本文介绍一种名为拟态初始化的方法，通过仅仅调整自注意力层的权重初始化，即可在视觉任务中大大提高Transformer的精度。 |
| [^102] | [Machine-Made Media: Monitoring the Mobilization of Machine-Generated Articles on Misinformation and Mainstream News Websites.](http://arxiv.org/abs/2305.09820) | 这篇论文研究了机器生成文章在虚假新闻和主流新闻网站的普及程度，发现虚假新闻网站上合成文章的使用速度比主流网站上更快。 |
| [^103] | [A Method for Training-free Person Image Picture Generation.](http://arxiv.org/abs/2305.09817) | 本文提出一种无需训练的角色图像特征编码器模型，使得用户可以通过简单提供角色的图片，生成匹配期望的图像并调整各种细节，无需为每个个体/动画角色图像单独训练模型。 |
| [^104] | [On Dataset Transferability in Active Learning for Transformers.](http://arxiv.org/abs/2305.09807) | 本文研究了基于transformer的预训练语言模型的主动学习中数据集的可迁移性问题，发现具有相似获取序列的主动学习方法产生的数据集在不同模型之间具有高度的可迁移性。 |
| [^105] | [Reinforcement Learning for Safe Robot Control using Control Lyapunov Barrier Functions.](http://arxiv.org/abs/2305.09793) | 本研究利用控制李亚普诺夫屏障函数及LBAC算法，提出了一种模型无关的强化学习方法，实现了基于数据的安全性和可达性条件下机器人控制。在实际2D四旋翼导航任务中验证该方法的有效性，优于其他模型无关强化学习方法。 |
| [^106] | [A score-based operator Newton method for measure transport.](http://arxiv.org/abs/2305.09792) | 本文提出一种新的基于分数的算子Newton方法，可以迭代构造一个易处理的原概率测度，该方法可以在满足目标分数光滑性假设下，实现快速收敛性。 |
| [^107] | [Molecule-Morphology Contrastive Pretraining for Transferable Molecular Representation.](http://arxiv.org/abs/2305.09790) | 本文提出了Molecule-Morphology Contrastive Pretraining (MoCoP)框架，用于学习分子图形和细胞形态的多模态表示。实验结果表明，MoCoP可以提高图神经网络在分子属性预测任务上的表现，具有良好的实用性。 |
| [^108] | [Codesign of Edge Intelligence and Automated Guided Vehicle Control.](http://arxiv.org/abs/2305.09788) | 本文介绍了一种自主引导车辆（AGV）控制、边缘智能和人类输入的和谐设计，以实现工业环境中的自主运输，其核心技术是通过无线网络连接人工智能（AI）和AGV实现人机协同。 |
| [^109] | [Deep Learning for Solving and Estimating Dynamic Macro-Finance Models.](http://arxiv.org/abs/2305.09783) | 该论文介绍了一种利用深度学习同时求解和估计金融经济中经典的连续时间一般均衡模型的方法，并在产业动态和带有金融摩擦的宏观经济模型两个样例中展示了其优点，具有多种用途。 |
| [^110] | [SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification.](http://arxiv.org/abs/2305.09781) | SpecInfer是一种LLM服务系统，通过利用推测推断和令牌树验证来加速生成式大语言模型的推断过程，显著减少了为它们提供服务所需的端到端延迟和计算要求，同时确保模型质量。 |
| [^111] | [A Scalable Walsh-Hadamard Regularizer to Overcome the Low-degree Spectral Bias of Neural Networks.](http://arxiv.org/abs/2305.09779) | 本文提出了一种新型的可扩展Walsh-Hadamard正则化器，可避免神经网络学习低阶频率以及误识别低阶频率，从而提高了神经网络的泛化性能。 |
| [^112] | [BSGAN: A Novel Oversampling Technique for Imbalanced Pattern Recognitions.](http://arxiv.org/abs/2305.09777) | 本论文提出了一种名为BSGAN的混合过采样技术，旨在缓解现有边界SMOTE过采样技术中存在的过采样后发生的边缘化问题，并生成更多多样性数据以提高模型的预测准确性。 |
| [^113] | [OpenVR: Teleoperation for Manipulation.](http://arxiv.org/abs/2305.09765) | 该论文提出了一种基于虚拟现实技术的远程控制方法，旨在解决演示文件的质量不足与收集困难的问题，并实现了代码开源、硬件易得、易于修改和使用。 |
| [^114] | [Assessment of few-hits machine learning classification algorithms for low energy physics in liquid argon detectors.](http://arxiv.org/abs/2305.09744) | 本文评估了在液体氩探测器低能物理中使用Few-Hits机器学习分类算法的效果，证明在单比特与双比特事件的分类问题上，卷积神经网络和Transformer-Encoder方法优于传统算法，并针对DUNE Phase II探测器优化了探测器参数。 |
| [^115] | [CQural: A Novel CNN based Hybrid Architecture for Quantum Continual Machine Learning.](http://arxiv.org/abs/2305.09738) | 本文展示了一种基于混合CNN的量子持续机器学习架构，可以通过解释哪些特征对于分类最重要来避免遗忘，并声称如果使用这些解释来训练模型，则会获得更好的性能。 |
| [^116] | [ADDSL: Hand Gesture Detection and Sign Language Recognition on Annotated Danish Sign Language.](http://arxiv.org/abs/2305.09736) | 本研究引入了一个新的丹麦手语注释数据集（ADDSL）并使用该数据集训练了一个基于YOLOv5的手势检测和字母数字识别模型，准确率最高可达92%。与同领域现有工作相比，该模型更高效和准确。 |
| [^117] | [What In-Context Learning "Learns" In-Context: Disentangling Task Recognition and Task Learning.](http://arxiv.org/abs/2305.09731) | 本研究通过任务识别和任务学习两种方式表征了ICL利用演示的方式，发现LLMs利用不同机制进行任务的解决，TR主要利用先验知识，而TL则具备学习新的输入-标签映射的能力。 |
| [^118] | [FedHGN: A Federated Framework for Heterogeneous Graph Neural Networks.](http://arxiv.org/abs/2305.09729) | FedHGN是一种用于异构图神经网络的联邦学习框架，它采用模式权重解耦和系数对齐技术，使得不同客户端可以共享知识而不泄露隐私，相比于现有方法表现更加优秀。 |
| [^119] | [Random Edge Coding: One-Shot Bits-Back Coding of Large Labeled Graphs.](http://arxiv.org/abs/2305.09705) | 随机边编码是一种压缩大型标记图的一次性方法，它使用bits-back编码从边缘列表中无替换地对边缘和顶点进行采样，并在随机图模型下实现了最优性。 |
| [^120] | [Dynamic Causal Explanation Based Diffusion-Variational Graph Neural Network for Spatio-temporal Forecasting.](http://arxiv.org/abs/2305.09703) | 本文提出了一种新颖的基于因果关系解释的扩散变分图神经网络，用于时空预测，在动态图构建上考虑了邻居节点之间的因果关系和不确定性，解决了动态图算法的可解释性和稳定性问题。 |
| [^121] | [Generative Table Pre-training Empowers Models for Tabular Prediction.](http://arxiv.org/abs/2305.09696) | 本文提出了TapTap，一种通过表格预训练生成高质量合成表格来提高表格预测性能的方法。在12个数据集实验中，TapTap在不同场景下优于16个基线，并可以与多个骨干模型结合使用。 |
| [^122] | [Applying Machine Learning Analysis for Software Quality Test.](http://arxiv.org/abs/2305.09695) | 研究通过应用机器学习，利用可用数据计算累计软件故障水平，并提出预测残留缺陷的方法。 |
| [^123] | [Evaluation Strategy of Time-series Anomaly Detection with Decay Function.](http://arxiv.org/abs/2305.09691) | 本文提出了带衰减函数的点调整协议（PAdf）以解决现有时间序列异常检测算法评估方式高估或低估性能的问题。通过在基准数据集上的重新评估，我们发现PAdf协议不仅考虑要查找尽可能多的段数，还考虑快速准确地检测异常。 |
| [^124] | [A Whisper transformer for audio captioning trained with synthetic captions and transfer learning.](http://arxiv.org/abs/2305.09690) | 这篇论文介绍了一种音频描述方法，采用预训练的Whisper模型和合成字幕的预训练。实验结果表明，不同的训练策略会影响音频描述模型的性能。 |
| [^125] | [OOD-Speech: A Large Bengali Speech Recognition Dataset for Out-of-Distribution Benchmarking.](http://arxiv.org/abs/2305.09688) | OOD-Speech 是用于 Bengali 语音识别的越域基准数据集，由众包收集了母语为 Bengali 的 22,645 名说话者录制的 1177.94 小时语音数据，并经过手动注释。数据集包含 17 种不同的资源，如 Bengali 电视剧、有声读物、脱口秀、在线教学以及伊斯兰讲道等，可作为 Bengali 语音识别的分布变化方面的基准测试数据集。 |
| [^126] | [Data Bias Management.](http://arxiv.org/abs/2305.09686) | 本文讲述了数据偏差在机器学习中的应用、影响及可能的解决方案 |
| [^127] | [Anomaly Detection Dataset for Industrial Control Systems.](http://arxiv.org/abs/2305.09678) | 本文介绍了'ICS-Flow'数据集，其中包括了监督和无监督机器学习算法评估所需的网络数据和过程状态变量日志，并提供了几种流行的算法在该数据集上的基础性能评估，旨在促进工控系统的入侵检测系统发展和加强关键基础设施的网络安全性。 |
| [^128] | [Vulnerability Detection Using Two-Stage Deep Learning Models.](http://arxiv.org/abs/2305.09673) | 本文提出了一种双阶段解决方案，采用了两个深度学习模型用于漏洞检测，其可在识别和分类各种类型的漏洞方面达到高准确率，优于传统SAST和DAST方法。 |
| [^129] | [Pick your Poison: Undetectability versus Robustness in Data Poisoning Attacks against Deep Image Classification.](http://arxiv.org/abs/2305.09671) | 深度图像分类数据污染攻击存在检测性与鲁棒性之争：污染太少导致攻击失效，污染太多易被检测到。该论文提出两种防御措施，对有限的信任图像标签对进行后处理来检测和修复被污染的模型，并证明其有效性。 |
| [^130] | [Mean Estimation Under Heterogeneous Privacy: Some Privacy Can Be Free.](http://arxiv.org/abs/2305.09668) | 本文研究了在不同隐私要求下的均值估计问题，提出的算法在两组具有不同隐私级别的用户时是极小化的最优的，并揭示了一个有趣的饱和现象。 |
| [^131] | [Protein Complex Invariant Embedding with Cross-Gate MLP is A One-Shot Antibody Designer.](http://arxiv.org/abs/2305.09480) | 本文提出了一种深度生成模型，可以一次性地共同设计抗体CDR的1D序列和3D结构，解决几何建模和低效推断的问题。 |
| [^132] | [Measuring Implicit Bias Using SHAP Feature Importance and Fuzzy Cognitive Maps.](http://arxiv.org/abs/2305.09399) | 本文使用SHAP特征重要性和模糊认知地图模型，对隐性偏见进行测量，结果表明特征重要性作为绝对工具不适应于测量隐性偏见，受保护特征的偏见数量可能因特征是数值编码还是分类编码而有所不同。 |
| [^133] | [Touch Sensing on Semi-Elastic Textiles with Border-Based Sensors.](http://arxiv.org/abs/2305.09222) | 本研究提出一种基于半弹性纺织品表面上边缘的传感器进行接触感测的方法，无需在感测区域放置额外传感器。该方法可在可穿戴技术和智能纺织品等领域中应用，能够以82.85%的准确度分类识别三个压力水平，具有潜在的应用价值。 |
| [^134] | [Motion Question Answering via Modular Motion Programs.](http://arxiv.org/abs/2305.08953) | 提出了一种新的HumanMotionQA任务，用于评估模型在复杂的人体运动序列上进行复杂、多步推理的能力。同时，提出了一种特殊的NSPose方法，利用符号化推理和模块化设计，成功地应用于该任务中，并超过所有基线方法。 |
| [^135] | [Neurosymbolic AI and its Taxonomy: a survey.](http://arxiv.org/abs/2305.08876) | 本文调查研究了神经符号人工智能的研究，探索了学习数据分布和推理先前和学习知识相结合的方法，以探索实现人工智能通用性的替代方案。 |
| [^136] | [Ship-D: Ship Hull Dataset for Design Optimization using Machine Learning.](http://arxiv.org/abs/2305.08279) | 本文介绍了一个适用于船舶设计优化的机器学习数据集Ship-D，该数据集包含三万艘船舶外形信息、设计参数、网格表示、点云数据、图像表示以及三十二种水动力阻力数据，并可支持人类和计算方法进行设计。 |
| [^137] | [Leveraging Large Language Models in Conversational Recommender Systems.](http://arxiv.org/abs/2305.07961) | 本文提出了一种使用大型语言模型构建端到端大规模对话推荐系统的路线图，解决在该系统中有效利用大型语言模型所面临的技术挑战。 |
| [^138] | [Monitoring and Adapting ML Models on Mobile Devices.](http://arxiv.org/abs/2305.07772) | 这篇论文介绍了Nazr，这是一个能够在移动设备上连续监测和调整机器学习模型，以提高模型准确性的端到端系统。 |
| [^139] | [Continual Vision-Language Representaion Learning with Off-Diagonal Information.](http://arxiv.org/abs/2305.07437) | 本文探讨了通过流数据持续训练CLIP模型的可行性，提出了一种有效的连续学习框架Mod-X，并证明内部旋转和跨模态偏差导致了CLIP在跨模态检索任务中性能下降。 |
| [^140] | [A Generic Approach to Integrating Time into Spatial-Temporal Forecasting via Conditional Neural Fields.](http://arxiv.org/abs/2305.06827) | 本文提出了一种将时间组件融入预测模型的通用方法，通过使用条件神经场来表示辅助特征，解决了在利用时间序列进行预测时存在的一个未解决问题。 |
| [^141] | [Exploring the Landscape of Machine Unlearning: A Survey and Taxonomy.](http://arxiv.org/abs/2305.06360) | 本文综述了机器遗忘的现状和技术应用，包括数据删除、扰动和模型更新，讨论了MU在隐私、安全和公正性等领域的潜在益处，以及它在自然语言处理、计算机视觉和推荐系统中的未来发展方向。 |
| [^142] | [A proof of convergence of inverse reinforcement learning for multi-objective optimization.](http://arxiv.org/abs/2305.06137) | 本论文证明了多目标优化的逆强化学习方法在理论层面上的收敛性，包括Wasserstein逆强化学习和常规逆强化学习方法。 |
| [^143] | [The emergence of clusters in self-attention dynamics.](http://arxiv.org/abs/2305.05465) | 本文证实了当Transformer处理一系列token时，出现“领导者”的经验观察，即随着时间趋于无穷大，代表token的粒子会聚集在特定的极限对象附近，这取决于价值矩阵的谱。 |
| [^144] | [UQ for Credit Risk Management: A deep evidence regression approach.](http://arxiv.org/abs/2305.04967) | 本文扩展了Deep Evidence Regression方法，将其应用于预测信用风险中的违约损失；我们提供了相关的学习框架，并在模拟和实际数据上进行了验证。 |
| [^145] | [A Comprehensive Study on Dataset Distillation: Performance, Privacy, Robustness and Fairness.](http://arxiv.org/abs/2305.03355) | 本研究对当前最先进的数据集压缩方法进行了全面评估，发现其存在隐私风险并可能放大模型的不公平性，提供了大规模的基准测试框架。 |
| [^146] | [Distributing Synergy Functions: Unifying Game-Theoretic Interaction Methods for Machine-Learning Explainability.](http://arxiv.org/abs/2305.03100) | 本文提出了一种统一的框架，用于游戏理论驱动的归因和k阶交互方法，通过假设，可以在连续输入设置中得到唯一全面的特征交互解释，即协同作用。 |
| [^147] | [Segment Anything Model for Medical Image Analysis: an Experimental Study.](http://arxiv.org/abs/2304.10517) | 本研究对医学图像分割模型SAM在各种不同情况下的表现进行了广泛评估，结果表明在单点提示下其表现高度变化，是一项具有挑战性的工作。 |
| [^148] | [PED-ANOVA: Efficiently Quantifying Hyperparameter Importance in Arbitrary Subspaces.](http://arxiv.org/abs/2304.10255) | PED-ANOVA 提出了一个新的 f-ANOVA 公式，能够在任意子空间中高效地计算超参数的重要性，有助于深度学习中好的超参数空间设计。 |
| [^149] | [NetGPT: Generative Pretrained Transformer for Network Traffic.](http://arxiv.org/abs/2304.09513) | 本文提出了首个网络流量生成预训练变压器模型NetGPT，该模型可以优化网络任务的训练效率和有效性。 |
| [^150] | [Classification of US Supreme Court Cases using BERT-Based Techniques.](http://arxiv.org/abs/2304.08649) | 本文基于BERT技术探究了对美国最高法院案例进行分类的方法，比较了使用BERT模型与其他先进模型的准确性，最终在15个广泛类别上取得了80%的准确度，在279个细粒度类别上取得了60%的准确度。 |
| [^151] | [Multimodal Short Video Rumor Detection System Based on Contrastive Learning.](http://arxiv.org/abs/2304.08401) | 本研究基于对比学习设计出一个多模态短视频谣言检测系统，通过构建具有多种特征的短视频数据集和使用多模态特征融合与外部知识，能有效地区分短视频谣言。 |
| [^152] | [Attentive Q-Matrix Learning for Knowledge Tracing.](http://arxiv.org/abs/2304.08168) | 本文提出了一种基于Q-矩阵的注意力知识追踪模型，能够在不存在事先确定的技能标签的情况下应用于大规模的在线教育平台。 |
| [^153] | [Sample Average Approximation for Black-Box VI.](http://arxiv.org/abs/2304.06803) | 该论文提出了一种用于黑盒变分推断的样本平均估计方法，有效地解决了随机梯度上升等问题，实验结果表明其比现有方法更快且性能更佳。 |
| [^154] | [AutoRL Hyperparameter Landscapes.](http://arxiv.org/abs/2304.02396) | 本文提出了一种方法，在训练期间多次建立和分析AutoRL超参数的景观，证明代表算法（DQN和SAC）在不同环境下的超参数景观会随时间而变化。 |
| [^155] | [Temporal Dynamic Synchronous Functional Brain Network for Schizophrenia Diagnosis and Lateralization Analysis.](http://arxiv.org/abs/2304.01347) | 本文提出了一种基于动态功能连接的脑网络分析模型，通过构建动态同步特征和革命性的图卷积方法实现精神分裂症诊断和侧化分析，并在实验证明其表现优于其他最先进模型。 |
| [^156] | [Topological Reconstruction of Particle Physics Processes using Graph Neural Networks.](http://arxiv.org/abs/2303.13937) | Topograph是一种利用图神经网络和粒子衰变自然规律的拓扑结构重建方法，不仅解决了观测到的末态对象组合指派问题，还预测了中间粒子的性质及其后续衰变，比标准方法效果更好，与现代机器学习技术表现相当。 |
| [^157] | [Particle Mean Field Variational Bayes.](http://arxiv.org/abs/2303.13930) | 本论文提出了一种新的基于粒子的MFVB方法，有效扩展了其适用范围，可应用于贝叶斯逻辑回归、随机波动和深度神经网络。 |
| [^158] | [Phase Aberration Correction without Reference Data: An Adaptive Mixed Loss Deep Learning Approach.](http://arxiv.org/abs/2303.05747) | 本文提出了一种深度学习的自适应混合损失方法，用于无需参考数据的超声相位像差修复。 |
| [^159] | [DeepSaDe: Learning Neural Networks that Guarantee Domain Constraint Satisfaction.](http://arxiv.org/abs/2303.01141) | 本文提出了一种学习神经网络的方法，该神经网络可以强制执行多样化的约束并且保证所有可能的预测都满足约束限制。 |
| [^160] | [Learning curves for deep structured Gaussian feature models.](http://arxiv.org/abs/2303.00564) | 该论文针对深度高斯模型的特征各向异性展开研究。研究结果表明，在第一层特征行之间允许存在相关性可以促进泛化，而后续层的结构通常是不利的。 |
| [^161] | [MCoCo: Multi-level Consistency Collaborative Multi-view Clustering.](http://arxiv.org/abs/2302.13339) | MCoCo提出了一种多级一致性协作学习框架，利用对齐语义空间和多级一致性协作策略，实现了多视角聚类。 |
| [^162] | [One Fits All:Power General Time Series Analysis by Pretrained LM.](http://arxiv.org/abs/2302.11939) | 本论文提出了一种称为 Frozen Pretrained Transformer (FPT) 的预训练模型，利用从数十亿标记训练出来的语言或 CV 模型进行时间序列分析的所有主要类型任务的微调，进而使其在所有任务中都具备着最先进的性能和泛化能力。 |
| [^163] | [Gaussian processes at the Helm(holtz): A more fluid model for ocean currents.](http://arxiv.org/abs/2302.10364) | 该论文提出了一种更符合已知电流物理特性的模型，在通过Helmholtz分解获得的向量场的发散和无旋分量上使用高斯过程来预测海流。证明了这种方法在模拟数据和真实浮标数据方面都比之前的方法更有效。 |
| [^164] | [Growing Steerable Neural Cellular Automata.](http://arxiv.org/abs/2302.10197) | 本文讨论了"Growing Steerable Neural Cellular Automata"，通过使每个细胞负责其自己的方向，产生了具有变化方向的细胞的模型。 |
| [^165] | [Firmware implementation of a recurrent neural network for the computation of the energy deposited in the liquid argon calorimeter of the ATLAS experiment.](http://arxiv.org/abs/2302.07555) | 本文介绍了基于最先进的FPGA技术实现的ATLAS实验用于计算液态氩闪烁计数器中沉积能量的循环神经网络(RNN)，优于目前基于滤波算法的计算方案。 |
| [^166] | [Domain Generalization by Functional Regression.](http://arxiv.org/abs/2302.04724) | 通过函数回归实现领域泛化，构建了线性算子将输入边缘分布与输出条件分布联系起来，并提出了一种基于源分布依赖性的再生核希尔伯特空间预测算法。 |
| [^167] | [Leveraging Demonstrations to Improve Online Learning: Quality Matters.](http://arxiv.org/abs/2302.03319) | 本篇论文探讨了离线演示数据如何改进在线学习的问题，提出了一种利用演示数据的TS算法，并给出了依赖于先验知识的贝叶斯遗憾界；研究发现，预训练可以大幅提高在线性能，改进程度随专家能力水平的提高而增加。 |
| [^168] | [Probabilistic Contrastive Learning Recovers the Correct Aleatoric Uncertainty of Ambiguous Inputs.](http://arxiv.org/abs/2302.02865) | 本文提出利用概率对比学习方法可以恢复具有不确定性输入的正确估计，通过扩展InfoNCE目标和编码器以预测潜变量分布来实现，在计算已知查询图像的可信区间方面具有应用价值。 |
| [^169] | [Direct Uncertainty Quantification.](http://arxiv.org/abs/2302.02420) | 本文提出一种新的直接不确定量化（DirectUQ）方法，它能在神经网络中直接输出均值和方差，同时结合了传统神经网络和贝叶斯神经网络的优点，有助于改进模型的正则化器和风险边界等方面。 |
| [^170] | [GAN-based Vertical Federated Learning for Label Protection in Binary Classification.](http://arxiv.org/abs/2302.02245) | GAFM是一种用于竖直联邦学习中标签保护的新方法，它利用生成对抗网络间接利用标签信息来减轻梯度标签泄漏问题，并采用交叉熵损失来提高预测准确性。 |
| [^171] | [Dual Self-Awareness Value Decomposition Framework without Individual Global Max for Cooperative Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2302.02180) | 提出了一种基于双重自我意识价值分解框架的协作多智体强化学习方法，完全摒弃了个体全局最大值的前提，具有良好的性能表现。 |
| [^172] | [Average-Constrained Policy Optimization.](http://arxiv.org/abs/2302.00808) | 本研究提出了一种新的基于函数逼近算法的带平均标准约束 MDP 的策略优化算法，具有较好的性能表现。 |
| [^173] | [Distillation Policy Optimization.](http://arxiv.org/abs/2302.00533) | 本文展示了一种演员-评论家的学习框架，该框架通过蒸馏优势在利用过去经验的同时遵循稳定的在线策略，实现了快速学习并可以适用于广泛的算法类别。 |
| [^174] | [Anti-Exploration by Random Network Distillation.](http://arxiv.org/abs/2301.13616) | 本文介绍了基于特征线性调制的随机网络碾压算法，可以有效防止探索，避免了区分度的问题，在 D4RL 基准测试中取得了可与集成方法相媲美的性能。 |
| [^175] | [FedRC: Tackling Diverse Distribution Shifts Challenge in Federated Learning by Robust Clustering.](http://arxiv.org/abs/2301.12379) | 本文提出了一种名为FedRC的新型聚类算法框架，用于解决联邦学习中多样分布偏移的挑战，并通过鲁棒性损失函数来改进现有聚类方法。 |
| [^176] | [Certified Invertibility in Neural Networks via Mixed-Integer Programming.](http://arxiv.org/abs/2301.11783) | 本研究利用混合整数规划来认证神经网络的可逆性，并探究了过于不变性的现象在两个情境下的表现，同时讨论了如何用这些发现进行神经网络之间的可逆性认证。 |
| [^177] | [Incorporating Knowledge into Document Summarisation: an Application of Prefix-Tuning on GPT-2.](http://arxiv.org/abs/2301.11719) | 本论文研究了将事实知识纳入生成的摘要的可能性，具体采用前缀调整的方法，实验结果表明，此方法可以生成保留知识的摘要，而且可以提升整体性能。 |
| [^178] | [Neural networks learn to magnify areas near decision boundaries.](http://arxiv.org/abs/2301.11375) | 神经网络训练能够放大决策边界附近的局部区域，改善整个系统的泛化能力。 |
| [^179] | [Hierarchical Data-efficient Representation Learning for Tertiary Structure-based RNA Design.](http://arxiv.org/abs/2301.10774) | 本研究提出了一个基于层次数据有效表示学习的RNA设计流程，通过构建大型数据集并设计全面的结构建模方法，实现了更高效的RNA序列设计。 |
| [^180] | [Compact Optimization Learning for AC Optimal Power Flow.](http://arxiv.org/abs/2301.08840) | 本文提出了一种压缩学习方法，利用主成分分析可以显著降低交流电最优潮流计算的维度，降低了可训练参数的数量，提高了可扩展性和有效性。同时，该方法的输出可以用于热启动精确的AC求解器以恢复可行性。 |
| [^181] | [A-NeSI: A Scalable Approximate Method for Probabilistic Neurosymbolic Inference.](http://arxiv.org/abs/2212.12393) | 本文介绍了一种名为A-NeSI的新颖PNL框架，它使用神经网络实现了近似推理，能够保证概率逻辑语义的同时解决了PNL的可扩展性问题，能够在安全关键应用中保证逻辑约束的满足。 |
| [^182] | [Towards Causal Credit Assignment.](http://arxiv.org/abs/2212.11636) | 本论文研究了后见信用分配这种分配信用的方法，并提出了一种结合因果结构的状态表示来提高效率的方法。 |
| [^183] | [Policy Learning for Active Target Tracking over Continuous SE(3) Trajectories.](http://arxiv.org/abs/2212.01498) | 本论文提出了一种基于模型的策略梯度算法，用于利用移动机器人跟踪动态目标，通过设计神经网络控制策略和注意力层，从而实现目标熵对网络参数的梯度运算，以实现高效的基于模型的策略梯度优化。 |
| [^184] | [Learning Robust State Observers using Neural ODEs (longer version).](http://arxiv.org/abs/2212.00866) | 本研究使用神经常微分学习非线性系统的状态观测，设计了适用于部分已知和完全未知非线性动力学系统的鲁棒观测器，提高观测器的鲁棒性使其更适合实际应用。 |
| [^185] | [NEVIS'22: A Stream of 100 Tasks Sampled from 30 Years of Computer Vision Research.](http://arxiv.org/abs/2211.11747) | NEVIS'22基准测试包含了来自计算机视觉会议三十年来均匀取样的100个视觉分类任务，可以反映研究社区的进展和变化。 |
| [^186] | [Simple and Effective Augmentation Methods for CSI Based Indoor Localization.](http://arxiv.org/abs/2211.10790) | 本论文提出了两种简单但惊人有效的数据增广算法，基于通道状态信息（CSI）的室内定位可以减少测量次数一个数量级。 |
| [^187] | [A Riemannian ADMM.](http://arxiv.org/abs/2211.02163) | 该论文提出了一种适用于机器学习和统计学中重要应用的黎曼交替方向乘子法（ADMM），能够同时处理非凸约束和非光滑的目标函数，并分析了算法得到$\epsilon$-稳定点的迭代复杂度。 |
| [^188] | [Balancing Utility and Fairness in Submodular Maximization (Technical Report).](http://arxiv.org/abs/2211.00980) | 本文提出了一个新的问题，称为“二标准子模最大化”，以平衡效用和公平性。该问题要求找到一个固定大小的解，以最大化效用函数为目标。 |
| [^189] | [Pruning Pre-trained Language Models Without Fine-Tuning.](http://arxiv.org/abs/2210.06210) | 本文提出了静态模型剪枝（SMP），它只使用一阶剪枝来适应下游任务，同时实现目标稀疏度水平，在大量实验证明SMP具有显著的改进。 |
| [^190] | [Variance Tolerance Factors For Interpreting ALL Neural Networks.](http://arxiv.org/abs/2209.13858) | 本文提出一种用于解释黑盒神经网络的方差容忍因子（VTF）理论，通过排名特征的方式探索特征的重要性，同时构建一个基本模型和特征模型的新型架构，来探索所有表现良好的神经网络中特征的重要性，并且经过基准测试和应用于实际环境中的实验验证了方法的可靠性。 |
| [^191] | [Deep Unfolding of the DBFB Algorithm with Application to ROI CT Imaging with Limited Angular Density.](http://arxiv.org/abs/2209.13264) | 本文提出了一种用于计算机断层扫描（CT）重建感兴趣区（ROI）的深度展开网络U-RDBFB。它具有快速的重建速度和高重建质量，并且能够有效地处理视角较少的截断数据。 |
| [^192] | [Learning Continuous Control Policies for Information-Theoretic Active Perception.](http://arxiv.org/abs/2209.12427) | 本文提出了一种使用信息理论代价来学习主动地标定位和探索的连续控制策略方法，该方法使用卡尔曼滤波器转换部分可观察问题为MDP，使用视野来塑造奖励，使用基于注意力机制的神经网络来表示控制策略，并与主动体积建图结合以促进地标定位和探索。 |
| [^193] | [Generalizing Goal-Conditioned Reinforcement Learning with Variational Causal Reasoning.](http://arxiv.org/abs/2207.09081) | 本文提出了一种增强的目标条件强化学习框架，它使用因果图来发现和表示因果关系来实现模型的泛化性。 |
| [^194] | [The Consistency of Adversarial Training for Binary Classification.](http://arxiv.org/abs/2206.09099) | 研究了对于绝对连续于勒贝格度量的分布，哪些基于上确界的代理风险是一致的；定量计算了对抗性代理风险与对抗性分类风险的关系；探讨了对抗性训练的 $\cH$- 一致性的影响。 |
| [^195] | [A Language Model With Million Sample Context For Raw Audio Using Transformer Architectures.](http://arxiv.org/abs/2206.08297) | 本文提出了一种采用Transformer结构和百万级样本上下文进行原始音频语言模型的自回归生成架构，能够高效地建模音频信号的长期依赖性，并取得了最先进的性能表现。 |
| [^196] | [Timing is Everything: Learning to Act Selectively with Costly Actions and Budgetary Constraints.](http://arxiv.org/abs/2205.15953) | LICRA是学习在代价高昂的行动和预算限制下进行选择性行动的强化学习框架。 |
| [^197] | [Bandwidth Selection for Gaussian Kernel Ridge Regression via Jacobian Control.](http://arxiv.org/abs/2205.11956) | 本文提出了一种基于雅可比控制的带宽选择启发式方法，该方法具有闭式、计算非常轻的特点，并且在关注带宽的同时可以获得更好的模型泛化性能。 |
| [^198] | [Latent-space disentanglement with untrained generator networks for the isolation of different motion types in video data.](http://arxiv.org/abs/2205.10367) | 本文通过未经训练的生成器网络与特定的潜空间解缠技术方法，仅利用最小的底层动态信息，有效隔离视频数据中不同非线性运动类型，而不需要预训练模型。 |
| [^199] | [Neighborhood Attention Transformer.](http://arxiv.org/abs/2204.07143) | 提出了针对视觉任务的高效和可扩展的滑动窗口注意力机制——邻域关注（NA）。基于NA，开发了NAT，NAT-Tiny在ImageNet上达到了83.2％的top-1准确率，能够提高图像分类和下游视觉性能。 |
| [^200] | [FedComm: Federated Learning as a Medium for Covert Communication.](http://arxiv.org/abs/2201.08786) | 本文探究了联邦学习作为隐蔽通信媒介的可能性，提出了一种名为FedComm的隐蔽通信技术，其有效地实现了在FL框架内共享和传输有针对性的负载，使得通信更为隐蔽，且技术检测困难。 |
| [^201] | [Ensuring DNN Solution Feasibility for Optimization Problems with Convex Constraints and Its Application to DC Optimal Power Flow Problems.](http://arxiv.org/abs/2112.08091) | 本文提出了一个“预防性学习”框架，以在满足对约束标定的条件下，保证具有凸约束和一般目标函数的问题的DNN解的可行性，而无需后处理。通过系统标定不等式约束，我们预示预测误差并确保所得到的解仍然是可行的。同时提出了一种新的对抗样本感知的训练算法以提高DNN的最优性能而不牺牲可行性保证。 |
| [^202] | [Theoretical Foundations for Pseudo-Inversion of Nonlinear Operators.](http://arxiv.org/abs/2111.10755) | 本文研究了非线性算子的伪逆，包括其存在性和唯一性条件以及性质分析，给出了一些众所周知的不可逆非线性算子PI的解析表达式，并讨论了其与小波阈值的关系。 |
| [^203] | [Asymptotics of Network Embeddings Learned via Subsampling.](http://arxiv.org/abs/2107.02363) | 本研究将网络嵌入方法封装为一个统一框架，并从理论上证明了使用子采样学习的网络嵌入的渐近分布，同时提供了潜在参数的收敛速率和算法选择与统计效率之间的权衡。 |
| [^204] | [Stratified Learning: A General-Purpose Statistical Method for Improved Learning under Covariate Shift.](http://arxiv.org/abs/2106.11211) | 该论文提出了一种用于处理训练集不具代表性的协变量漂移情况下改进监督式学习的分层学习方法，并在宇宙学领域的两个问题中证明了其有效性，大幅提升了目标预测结果。 |
| [^205] | [DiGS : Divergence guided shape implicit neural representation for unoriented point clouds.](http://arxiv.org/abs/2106.10811) | 本文提出了一种新的散度导向形状表示学习方法，无需输入法向量，以软约束倾向于平滑解决方案，可靠地定位每个点的未知法向量的梯度，甚至比使用真实法向量更好。此外，还引入了一种新的正弦INR几何初始化方法，以进一步提高收敛性能。 |
| [^206] | [Selecting the Number of Clusters $K$ with a Stability Trade-off: an Internal Validation Criterion.](http://arxiv.org/abs/2006.08530) | 提出了一种新的聚类验证标准，基于聚类稳定性的内部验证原则，在聚类稳定性和聚类质量方面胜过现有的方法。 |

# 详细

[^1]: DoReMi: 优化数据混合加速语言模型预训练

    DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining. (arXiv:2305.10429v1 [cs.CL])

    [http://arxiv.org/abs/2305.10429](http://arxiv.org/abs/2305.10429)

    DoReMi方法使用分组分布式鲁棒优化训练小型代理模型以产生域权重，再使用这些权重重新采样数据集训练大型模型，相比使用默认权重的基线模型，在The Pile和GLaM数据集上平均提高了6.5%和4.7%的few-shot下游准确度，分别使用2.6倍和相同的训练步骤达到基线准确度。

    

    预训练数据域的混合比例（例如，维基百科、图书、网页文本）极大地影响语言模型（LM）性能。在本文中，我们提出了一种称为DoReMi的Domain Reweighting with Minimax Optimization方法，它首先使用分组分布式鲁棒优化（Group DRO）训练一个小代理模型，以产生域权重（混合比例），而不需要知道下游任务的知识。然后我们使用这些域权重重新采样一个数据集，并训练一个更大的，全尺寸的模型。在我们的实验中，我们使用DoReMi在一个280M参数的代理模型上，更有效地找到训练一个8B参数模型（30倍大）的域权重。在The Pile上，即使在减小一些域的比重时，DoReMi也能提高所有域的perplexity。相比使用The Pile的默认域权重训练的基线模型，DoReMi将平均few-shot下游准确度提高了6.5%，并使用2.6倍的训练步骤达到基线准确度。在GLaM数据集上，DoReMi没有任何关于下游任务的知识，提高了4.7%（次于现有最先进的模型）的few-shot准确度，在相同的训练步骤下提高了9.0%的准确度。

    The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full-sized model. In our experiments, we use DoReMi on a 280M-parameter proxy model to find domain weights for training an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves perplexity across all domains, even when it downweights a domain. DoReMi improves average few-shot downstream accuracy by 6.5% over a baseline model trained using The Pile's default domain weights and reaches the baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has no know
    
[^2]: 并行解码加速Transformer在翻译中的应用

    Accelerating Transformer Inference for Translation via Parallel Decoding. (arXiv:2305.10427v1 [cs.CL])

    [http://arxiv.org/abs/2305.10427](http://arxiv.org/abs/2305.10427)

    通过并行解码，本文提出了一种快速推断Transformer在翻译中的应用的方法，不需要修改现有模型并在保持翻译质量的同时加速了现有模型。

    

    自回归解码限制了Transformer在机器翻译中的效率。社区提出了特定的网络架构和基于学习的方法来解决这个问题，但它们都很昂贵并且需要改变机器翻译模型，以推导出解码速度和翻译质量之间的平衡。本文从解码算法的角度提出了一个解决方案，将标准的贪心自回归解码转化为并行解码，并利用雅克比和高斯-塞德尔迭代方法实现快速推断。该算法不需要修改现有模型，并在保持翻译质量的同时加速了现有模型。我们提出了三种并行解码算法，并在不同语言和模型上进行了测试，证明并行化解码相对于标准自回归解码可提高达38％的速度，当扩展模型时，速度几乎提高了2倍。

    Autoregressive decoding limits the efficiency of transformers for Machine Translation (MT). The community proposed specific network architectures and learning-based methods to solve this issue, which are expensive and require changes to the MT model, trading inference speed at the cost of the translation quality. In this paper, we propose to address the problem from the point of view of decoding algorithms, as a less explored but rather compelling direction. We propose to reframe the standard greedy autoregressive decoding of MT with a parallel formulation leveraging Jacobi and Gauss-Seidel fixed-point iteration methods for fast inference. This formulation allows to speed up existing models without training or modifications while retaining translation quality. We present three parallel decoding algorithms and test them on different languages and models showing how the parallelization introduces a speedup up to 38% w.r.t. the standard autoregressive decoding and nearly 2x when scaling t
    
[^3]: ZeroFlow: 通过蒸馏实现快速零标签场景流

    ZeroFlow: Fast Zero Label Scene Flow via Distillation. (arXiv:2305.10424v1 [cs.CV])

    [http://arxiv.org/abs/2305.10424](http://arxiv.org/abs/2305.10424)

    ZeroFlow是一种简单的蒸馏算法，使用无标签方法生成伪标签以监督前向传递模型，实现了在使用零人工标签情况下对大规模点云进行实时场景流估计。

    

    场景流估计是描述连续点云之间的三维运动场的任务。最先进的方法使用强大的先验知识和测试时优化技术，但对于大规模点云需要数十秒的时间，使其无法作为实时应用程序（如开放世界目标检测）的计算机视觉基元使用。前向传递方法相对快速，对于大规模点云的运行时间在数十至数百毫秒之间，但需要昂贵的人力监督。为了解决这两个限制，我们提出了一种简单的蒸馏框架 Scene Flow via Distillation，使用无标签优化方法来生成伪标签以监督前向传递模型。我们实现了这个框架中的 ZeroFlow，使用零人工标签，在大规模点云上实时生成场景流估计结果，同时质量竞争状态下的最先进方法。值得注意的是，在测试时 ZeroFlow

    Scene flow estimation is the task of describing the 3D motion field between temporally successive point clouds. State-of-the-art methods use strong priors and test-time optimization techniques, but require on the order of tens of seconds for large-scale point clouds, making them unusable as computer vision primitives for real-time applications such as open world object detection. Feed forward methods are considerably faster, running on the order of tens to hundreds of milliseconds for large-scale point clouds, but require expensive human supervision. To address both limitations, we propose Scene Flow via Distillation, a simple distillation framework that uses a label-free optimization method to produce pseudo-labels to supervise a feed forward model. Our instantiation of this framework, ZeroFlow, produces scene flow estimates in real-time on large-scale point clouds at quality competitive with state-of-the-art methods while using zero human labels. Notably, at test-time ZeroFlow is ove
    
[^4]: 基于演化的Tsukamoto神经模糊模型在多类别Covid 19胸透图像分类中的应用

    Evolving Tsukamoto Neuro Fuzzy Model for Multiclass Covid 19 Classification with Chest X Ray Images. (arXiv:2305.10421v1 [eess.IV])

    [http://arxiv.org/abs/2305.10421](http://arxiv.org/abs/2305.10421)

    本研究提出了一种基于演化的Tsukamoto神经模糊模型，在六个从胸透图像中提取的纹理特征的帮助下，将Covid-19疾病从正常和肺炎病例中区分出来，并实验表明其检测精度高于现有模型。

    

    随着人口的快速增长和运用人工智能作出快速决策的需求，开发基于机器学习的病症检测模型和异常识别系统已经极大地提高了医疗诊断水平。由于Covid-19已成为世界上最严重的疾病之一，开发自动的Covid-19检测框架有助于医生诊断疾病并快速提供正确的结果。在本文中，我们提出了一种基于机器学习的Covid 19检测框架。该模型使用Tsukamoto神经模糊推理网络来识别和区分Covid 19疾病和正常及肺炎病例。与传统的训练方法通过梯度下降算法和最小二乘法调整神经模糊模型的参数不同，我们使用基于演化的优化算法，即猫群算法来更新参数。此外，本研究结合了六个从胸透图像中提取的纹理特征以更好地检测疾病。我们的实验表明，相较于现有的模型，我们提出的模型在检测精度上有更高的准确性。

    Du e to rapid population growth and the need to use artificial intelligence to make quick decisions, developing a machine learning-based disease detection model and abnormality identification system has greatly improved the level of medical diagnosis Since COVID-19 has become one of the most severe diseases in the world, developing an automatic COVID-19 detection framework helps medical doctors in the diagnostic process of disease and provides correct and fast results. In this paper, we propose a machine lear ning based framework for the detection of Covid 19. The proposed model employs a Tsukamoto Neuro Fuzzy Inference network to identify and distinguish Covid 19 disease from normal and pneumonia cases. While the traditional training methods tune the parameters of the neuro-fuzzy model by gradient-based algorithms and recursive least square method, we use an evolutionary-based optimization, the Cat swarm algorithm to update the parameters. In addition, six texture features extracted f
    
[^5]: 使用Lasso的签名一致性研究

    On Consistency of Signatures Using Lasso. (arXiv:2305.10413v1 [stat.ML])

    [http://arxiv.org/abs/2305.10413](http://arxiv.org/abs/2305.10413)

    本文重新审视了Lasso回归对于签名变换的一致性问题，并发现对于不同的过程和时间序列，选择适当的签名定义和随机模型可以提高Lasso回归的一致性。

    

    签名变换是连续和离散时间序列数据的迭代路径积分，它们的普遍非线性通过线性化特征选择问题。本文在理论和数值上重新审视了Lasso回归对于签名变换的一致性问题。我们的研究表明，对于更接近布朗运动或具有较弱跨维度相关性的过程和时间序列，签名定义为It\^o积分的Lasso回归更具一致性；对于均值回归过程和时间序列，其签名定义为Stratonovich积分在Lasso回归中具有更高的一致性。我们的发现强调了在统计推断和机器学习中选择适当的签名和随机模型的重要性。

    Signature transforms are iterated path integrals of continuous and discrete-time time series data, and their universal nonlinearity linearizes the problem of feature selection. This paper revisits the consistency issue of Lasso regression for the signature transform, both theoretically and numerically. Our study shows that, for processes and time series that are closer to Brownian motion or random walk with weaker inter-dimensional correlations, the Lasso regression is more consistent for their signatures defined by It\^o integrals; for mean reverting processes and time series, their signatures defined by Stratonovich integrals have more consistency in the Lasso regression. Our findings highlight the importance of choosing appropriate definitions of signatures and stochastic models in statistical inference and machine learning.
    
[^6]: Wasserstein梯度流用于优化高斯混合策略

    Wasserstein Gradient Flows for Optimizing Gaussian Mixture Policies. (arXiv:2305.10411v1 [cs.LG])

    [http://arxiv.org/abs/2305.10411](http://arxiv.org/abs/2305.10411)

    本文提出了使用Wasserstein梯度流来优化基于高斯混合模型的机器人运动策略的方法，通过$L^2$-Wasserstein距离来约束策略更新，提高了策略优化的稳定性和样本效率。

    

    机器人在执行各种复杂任务时通常依赖于以前学习到的运动策略库。当面临未知任务条件或出现新任务要求时，机器人必须相应地调整它们的运动策略。在这种情况下，策略优化是将机器人策略作为任务特定目标的函数适应的“事实上”的范例。大多数常用的运动策略具有特定的结构，这些结构经常被忽略在策略优化算法中。我们提出利用概率策略的结构，将策略优化作为最优输运问题进行投影。具体而言，我们专注于机器人运动策略，该策略基于高斯混合模型(GMMs)，并将策略优化构成GMMs空间上的Wassertein梯度流。这自然地允许我们通过GMMs之间的$L^2$-Wasserstein距离约束策略更新，以增强策略优化过程的稳定性。此外，我们展示了如何推导梯度更新的闭式表达式，并推导出一种近端点算法，允许我们将我们的方法扩展到大量的混合成分。我们在模拟和真实机器人任务上的实验表明，与现有的策略优化算法相比，我们的方法导致了改进的样本效率和减少了可变性。

    Robots often rely on a repertoire of previously-learned motion policies for performing tasks of diverse complexities. When facing unseen task conditions or when new task requirements arise, robots must adapt their motion policies accordingly. In this context, policy optimization is the \emph{de facto} paradigm to adapt robot policies as a function of task-specific objectives. Most commonly-used motion policies carry particular structures that are often overlooked in policy optimization algorithms. We instead propose to leverage the structure of probabilistic policies by casting the policy optimization as an optimal transport problem. Specifically, we focus on robot motion policies that build on Gaussian mixture models (GMMs) and formulate the policy optimization as a Wassertein gradient flow over the GMMs space. This naturally allows us to constrain the policy updates via the $L^2$-Wasserstein distance between GMMs to enhance the stability of the policy optimization process. Furthermor
    
[^7]: 变分分类

    Variational Classification. (arXiv:2305.10406v1 [cs.LG])

    [http://arxiv.org/abs/2305.10406](http://arxiv.org/abs/2305.10406)

    提出一种新的变分分类方法，通过引入潜变量建模来优化训练，允许灵活的设计选择以改善校准和对抗鲁棒性，实验结果表明其对于域外数据的分类准确性得到了保持。

    

    我们提出了一种传统神经网络方法的新型扩展，称为变分分类 (VC)。通过引入潜变量建模，类似于变分自编码器和传统自编码器之间的关系，我们得到了一个基于证据下界 (ELBO) 的训练目标，采用对抗性方法优化。我们的VC模型允许在设计选择方面更加灵活，特别是类条件潜先验，而不是在现成的softmax分类器中做出的隐式假设。在图像和文本分类数据集上的实证评估表明，我们的方法在保持预测准确性的同时，改善了其他良好特性，如校准和对抗鲁棒性，即使应用于域外数据。

    We present a novel extension of the traditional neural network approach to classification tasks, referred to as variational classification (VC). By incorporating latent variable modeling, akin to the relationship between variational autoencoders and traditional autoencoders, we derive a training objective based on the evidence lower bound (ELBO), optimized using an adversarial approach. Our VC model allows for more flexibility in design choices, in particular class-conditional latent priors, in place of the implicit assumptions made in off-the-shelf softmax classifiers. Empirical evaluation on image and text classification datasets demonstrates the effectiveness of our approach in terms of maintaining prediction accuracy while improving other desirable properties such as calibration and adversarial robustness, even when applied to out-of-domain data.
    
[^8]: 高能物理反问题的端到端潜变分扩散模型

    End-To-End Latent Variational Diffusion Models for Inverse Problems in High Energy Physics. (arXiv:2305.10399v1 [hep-ex])

    [http://arxiv.org/abs/2305.10399](http://arxiv.org/abs/2305.10399)

    本研究提出了一种新的生成式深度学习方法，潜变分扩散模型，用于解决大型强子对撞机中的反问题。实现了重建理论动力学量的全局分布的有效性，同时确保所学后验分布遵守已知物理。

    

    大型强子对撞机（LHC）上的高能碰撞提供了粒子物理学中待解决问题的有价值见解。然而，在测量结果能够与特定理论预测或来自其他探测器的测量结果进行比较之前，必须纠正探测器的影响。解决将探测器观测映射到基础碰撞的理论量的反问题的方法是LHC许多物理分析中必不可少的部分。我们研究并比较了各种生成式深度学习方法来近似反映映射。我们引入了一种新的统一架构，称为潜变分扩散模型，将最新的生成艺术方法的潜在学习与端到端变分框架相结合。我们展示了该方法用于重建理论动力学量的全局分布的有效性，以及确保所学后验分布遵守已知物理。

    High-energy collisions at the Large Hadron Collider (LHC) provide valuable insights into open questions in particle physics. However, detector effects must be corrected before measurements can be compared to certain theoretical predictions or measurements from other detectors. Methods to solve this \textit{inverse problem} of mapping detector observations to theoretical quantities of the underlying collision are essential parts of many physics analyses at the LHC. We investigate and compare various generative deep learning methods to approximate this inverse mapping. We introduce a novel unified architecture, termed latent variation diffusion models, which combines the latent learning of cutting-edge generative art approaches with an end-to-end variational framework. We demonstrate the effectiveness of this approach for reconstructing global distributions of theoretical kinematic quantities, as well as for ensuring the adherence of the learned posterior distributions to known physics c
    
[^9]: RelationMatch：用于半监督学习的批内关系匹配技术

    RelationMatch: Matching In-batch Relationships for Semi-supervised Learning. (arXiv:2305.10397v1 [cs.LG])

    [http://arxiv.org/abs/2305.10397](http://arxiv.org/abs/2305.10397)

    RelationMatch是一种利用矩阵交叉熵（MCE）损失函数的方法，可以匹配批内关系，有效提高半监督学习和监督学习的性能。

    

    半监督学习通过利用少量标记数据和未标记数据中的信息，已经在许多领域取得了显着的成功。然而，现有算法通常集中在来自相同来源的成对数据点的预测对准上，并忽略了每个批次内的点间关系。本文介绍了一种新方法RelationMatch，它利用一种矩阵交叉熵（MCE）损失函数来发掘批内关系。通过应用MCE，我们的方法在各种视觉数据集中始终优于现有最先进的方法，如FixMatch和FlexMatch。值得注意的是，在仅使用40个标签的STL-10数据集上，我们观察到相对于FlexMatch有15.21％的显著提高。此外，我们将MCE应用于监督学习场景，并观察到了一致的改进。

    Semi-supervised learning has achieved notable success by leveraging very few labeled data and exploiting the wealth of information derived from unlabeled data. However, existing algorithms usually focus on aligning predictions on paired data points augmented from an identical source, and overlook the inter-point relationships within each batch. This paper introduces a novel method, RelationMatch, which exploits in-batch relationships with a matrix cross-entropy (MCE) loss function. Through the application of MCE, our proposed method consistently surpasses the performance of established state-of-the-art methods, such as FixMatch and FlexMatch, across a variety of vision datasets. Notably, we observed a substantial enhancement of 15.21% in accuracy over FlexMatch on the STL-10 dataset using only 40 labels. Moreover, we apply MCE to supervised learning scenarios, and observe consistent improvements as well.
    
[^10]: 稀疏图的消息传递架构的最优性

    Optimality of Message-Passing Architectures for Sparse Graphs. (arXiv:2305.10391v1 [cs.LG])

    [http://arxiv.org/abs/2305.10391](http://arxiv.org/abs/2305.10391)

    本研究证明了将消息传递神经网络应用于稀疏图的节点分类任务是渐近本地贝叶斯最优的，提出了一种实现最优分类器的算法，并将最优分类器的性能理论上与现有学习方法进行了比较。

    

    我们研究了特征装饰图上的节点分类问题，在稀疏设置下，即节点的预期度数为节点数的O(1)时。这样的图通常被称为本地树状图。我们引入了一种叫做渐近本地贝叶斯最优性的节点分类任务的贝叶斯最优性概念，并根据这个标准计算了具有任意节点特征和边连接分布的相当一般的统计数据模型的最优分类器。该最优分类器可以使用消息传递图神经网络架构实现。然后我们计算了该分类器的泛化误差，并在一个已经研究充分的统计模型上从理论上与现有的学习方法进行比较。我们发现，在低图信号的情况下，最佳消息传递架构插值于标准MLP和一种典型的c架构之间。

    We study the node classification problem on feature-decorated graphs in the sparse setting, i.e., when the expected degree of a node is $O(1)$ in the number of nodes. Such graphs are typically known to be locally tree-like. We introduce a notion of Bayes optimality for node classification tasks, called asymptotic local Bayes optimality, and compute the optimal classifier according to this criterion for a fairly general statistical data model with arbitrary distributions of the node features and edge connectivity. The optimal classifier is implementable using a message-passing graph neural network architecture. We then compute the generalization error of this classifier and compare its performance against existing learning methods theoretically on a well-studied statistical model with naturally identifiable signal-to-noise ratios (SNRs) in the data. We find that the optimal message-passing architecture interpolates between a standard MLP in the regime of low graph signal and a typical c
    
[^11]: 基于扩散模型的认证对抗性鲁棒性打破现有记录

    Raising the Bar for Certified Adversarial Robustness with Diffusion Models. (arXiv:2305.10388v1 [cs.LG])

    [http://arxiv.org/abs/2305.10388](http://arxiv.org/abs/2305.10388)

    本研究提出使用扩散模型生成训练数据来提高认证防御模型的鲁棒性，并给出了扩展这一方法的建议，证明了广义间隙是一个良好的预测指标。

    

    证明对抗攻击防御提供了模型鲁棒性的形式保证，使其比经验方法如对抗性训练更可靠，后者的有效性通常会被未知攻击削弱。然而，目前可实现的认证鲁棒性受限，已成为实际采用的瓶颈。Gowal等人和Wang等人已经表明，使用最先进的扩散模型生成额外的训练数据可以显著提高对抗训练的鲁棒性，本文证明类似的方法也可以显著提高确定性认证防御的鲁棒性。此外，我们提供了一系列建议，以扩展认证训练方法的鲁棒性。我们的主要见解之一是，即，广义间隙，即原始模型的训练和测试准确性之间的差异，是使用额外数据进行训练时鲁棒性改善幅度的良好预测指标。

    Certified defenses against adversarial attacks offer formal guarantees on the robustness of a model, making them more reliable than empirical methods such as adversarial training, whose effectiveness is often later reduced by unseen attacks. Still, the limited certified robustness that is currently achievable has been a bottleneck for their practical adoption. Gowal et al. and Wang et al. have shown that generating additional training data using state-of-the-art diffusion models can considerably improve the robustness of adversarial training. In this work, we demonstrate that a similar approach can substantially improve deterministic certified defenses. In addition, we provide a list of recommendations to scale the robustness of certified training approaches. One of our main insights is that the generalization gap, i.e., the difference between the training and test accuracy of the original model, is a good predictor of the magnitude of the robustness improvement when using additional g
    
[^12]: 基于Logit的集成分布蒸馏在自回归序列的不确定性中的应用

    Logit-Based Ensemble Distribution Distillation for Robust Autoregressive Sequence Uncertainties. (arXiv:2305.10384v1 [cs.LG])

    [http://arxiv.org/abs/2305.10384](http://arxiv.org/abs/2305.10384)

    本论文介绍了一种基于Logit的集成模型蒸馏方法，能够有效地将知识（epistemic）和数据（aleatoric）不确定性分开，对于大规模自然语言序列到序列的任务能够提高student模型的表现。

    

    高效可靠地估计不确定性是深度学习的一个重要目标，特别是在训练和推理成本通常非常高的自回归序列任务中。本文研究了应用于大规模自然语言序列到序列数据的集成分布蒸馏（Ensemble Distribution Distillation，EDD）方法。EDD旨在将昂贵的（teacher）集成模型的优越不确定性性能压缩到更便宜的（student）单一模型中。重要的是，它保留了将知识（认知）和数据（随机）不确定性分开的能力。现有的概率空间方法对于大词汇量的任务来说不易扩展。我们表明，在大规模翻译任务的现代Transformers模型中，对集成模型的logits进行建模比对softmax概率进行建模，能够显著提高student模型的表现。

    Efficiently and reliably estimating uncertainty is an important objective in deep learning. It is especially pertinent to autoregressive sequence tasks, where training and inference costs are typically very high. However, existing research has predominantly focused on tasks with static data such as image classification. In this work, we investigate Ensemble Distribution Distillation (EDD) applied to large-scale natural language sequence-to-sequence data. EDD aims to compress the superior uncertainty performance of an expensive (teacher) ensemble into a cheaper (student) single model. Importantly, the ability to separate knowledge (epistemic) and data (aleatoric) uncertainty is retained. Existing probability-space approaches to EDD, however, are difficult to scale to large vocabularies. We show, for modern transformer architectures on large-scale translation tasks, that modelling the ensemble logits, instead of softmax probabilities, leads to significantly better students. Moreover, the
    
[^13]: 基于物理约束的符号回归中主动学习的表现

    Active Learning in Symbolic Regression Performance with Physical Constraints. (arXiv:2305.10379v1 [cs.LG])

    [http://arxiv.org/abs/2305.10379](http://arxiv.org/abs/2305.10379)

    本文探讨了利用进化符号回归作为主动学习中的方法来提出哪些数据应该被采集，通过“委员会查询”来减少所需数据，并在重新发现已知方程所需的数据方面实现最新的结果。

    

    进化符号回归（SR）是一种将符号方程拟合到数据中的方法，可以得到简洁易懂的模型。本文探讨使用SR作为主动学习中的方法来提出哪些数据应该被采集，在此过程中考虑物理约束。基于主动学习的SR通过“委员会查询”来提出下一步实验。物理约束可以在非常低的数据情况下改善所建议的方程。这些方法可以减少SR所需的数据，并在重新发现已知方程所需的数据方面实现最新的结果。

    Evolutionary symbolic regression (SR) fits a symbolic equation to data, which gives a concise interpretable model. We explore using SR as a method to propose which data to gather in an active learning setting with physical constraints. SR with active learning proposes which experiments to do next. Active learning is done with query by committee, where the Pareto frontier of equations is the committee. The physical constraints improve proposed equations in very low data settings. These approaches reduce the data required for SR and achieves state of the art results in data required to rediscover known equations.
    
[^14]: 非合作博弈中的人类选择预测：基于模拟的离线策略评估

    Human Choice Prediction in Non-Cooperative Games: Simulation-based Off-Policy Evaluation. (arXiv:2305.10361v1 [cs.LG])

    [http://arxiv.org/abs/2305.10361](http://arxiv.org/abs/2305.10361)

    本文研究了语言游戏中的离线策略评估，并提出了一种结合真实和模拟数据的新方法。

    

    说服游戏在经济和人工智能研究中具有重要意义并具有重要的实际应用。本文探讨了在基于语言的说服游戏中离线策略评估（OPE）的挑战性问题，提出了一种结合真实和模拟人类 - 机器人交互数据的新方法，并给出了一种深度学习训练算法，该算法有效地整合了真实交互和模拟数据。

    Persuasion games have been fundamental in economics and AI research, and have significant practical applications. Recent works in this area have started to incorporate natural language, moving beyond the traditional stylized message setting. However, previous research has focused on on-policy prediction, where the train and test data have the same distribution, which is not representative of real-life scenarios. In this paper, we tackle the challenging problem of off-policy evaluation (OPE) in language-based persuasion games. To address the inherent difficulty of human data collection in this setup, we propose a novel approach which combines real and simulated human-bot interaction data. Our simulated data is created by an exogenous model assuming decision makers (DMs) start with a mixture of random and decision-theoretic based behaviors and improve over time. We present a deep learning training algorithm that effectively integrates real interaction and simulated data, substantially im
    
[^15]: NUANCE: 网络通信环境下利用近超声波进行攻击研究

    NUANCE: Near Ultrasound Attack On Networked Communication Environments. (arXiv:2305.10358v1 [cs.CR])

    [http://arxiv.org/abs/2305.10358](http://arxiv.org/abs/2305.10358)

    本研究探究了利用近超声波特洛伊木马对亚马逊Alexa语音服务的主要不可听攻击向量，并提出了针对企业、移动和工控系统的攻击防御策略。

    

    本研究探究了一种利用近超声波特洛伊木马对亚马逊Alexa语音服务的主要不可听攻击向量，并着重表征了攻击面并考察了发出不可听语音指令的实际影响。该研究将每个攻击向量映射到MITRE ATT＆CK矩阵中的一种策略或技术，涵盖企业、移动和工控系统（ICS）框架。实验涉及生成和调查50个近超声波音频以评估攻击的有效性，未经处理的指令具有100％的成功率，处理后的指令实现了58％的总体成功率。该系统性方法刺激了以前未得到解决的攻击面，确保了全面的检测和攻击设计，并将每个ATT＆CK标识符与测试过的防御方法搭配，为快速响应提供攻击和防御策略选项。主要发现揭示了该攻击方法采用单边带幅度调制。

    This study investigates a primary inaudible attack vector on Amazon Alexa voice services using near ultrasound trojans and focuses on characterizing the attack surface and examining the practical implications of issuing inaudible voice commands. The research maps each attack vector to a tactic or technique from the MITRE ATT&CK matrix, covering enterprise, mobile, and Industrial Control System (ICS) frameworks. The experiment involved generating and surveying fifty near-ultrasonic audios to assess the attacks' effectiveness, with unprocessed commands having a 100% success rate and processed ones achieving a 58% overall success rate. This systematic approach stimulates previously unaddressed attack surfaces, ensuring comprehensive detection and attack design while pairing each ATT&CK Identifier with a tested defensive method, providing attack and defense tactics for prompt-response options. The main findings reveal that the attack method employs Single Upper Sideband Amplitude Modulatio
    
[^16]: 通过无正交化方法的谱聚类

    Spectral Clustering via Orthogonalization-Free Methods. (arXiv:2305.10356v1 [eess.SP])

    [http://arxiv.org/abs/2305.10356](http://arxiv.org/abs/2305.10356)

    本文提出了四种无正交化方法作为谱聚类降维，不需要昂贵的特征值估计，在聚类质量和计算成本方面均优于已有方法，适合于并行计算。

    

    在谱聚类的降维中，通常使用图信号滤波器需要昂贵的特征值估计。我们在最优化设置中分析了滤波器并提出使用四种无正交化方法作为谱聚类中的降维。所提出的方法不利用任何正交化方法，在并行计算环境中不可伸缩。我们的方法在理论上构造了足够的特征空间，最多是规范化拉普拉斯矩阵特征空间的加权改变。我们在数值上假设所提出的方法与利用精确特征值但需要昂贵特征值估计的理想图信号滤波器在聚类质量上等效。数值结果表明，所提出的方法在聚类质量和计算成本方面优于基于幂迭代的方法和图信号滤波器。与基于幂迭代的方法不同，我们的方法可以轻松并行化。

    Graph Signal Filter used as dimensionality reduction in spectral clustering usually requires expensive eigenvalue estimation. We analyze the filter in an optimization setting and propose to use four orthogonalization-free methods by optimizing objective functions as dimensionality reduction in spectral clustering. The proposed methods do not utilize any orthogonalization, which is known as not well scalable in a parallel computing environment. Our methods theoretically construct adequate feature space, which is, at most, a weighted alteration to the eigenspace of a normalized Laplacian matrix. We numerically hypothesize that the proposed methods are equivalent in clustering quality to the ideal Graph Signal Filter, which exploits the exact eigenvalue needed without expensive eigenvalue estimation. Numerical results show that the proposed methods outperform Power Iteration-based methods and Graph Signal Filter in clustering quality and computation cost. Unlike Power Iteration-based meth
    
[^17]: 一种用于1型糖尿病患者运动检测的集成学习方法

    An Ensemble Learning Approach for Exercise Detection in Type 1 Diabetes Patients. (arXiv:2305.10353v1 [eess.SP])

    [http://arxiv.org/abs/2305.10353](http://arxiv.org/abs/2305.10353)

    为解决人工胰腺系统缺乏对运动引起的血糖摄取的检测能力，本文提出了一个集成学习框架，结合了数据驱动的生理模型和Siamese网络，能够高精度地利用多个生理信号流进行运动检测。

    

    1型糖尿病是一种严重的疾病，使患者无法调节血糖水平，导致各种医疗并发症。人工胰腺系统已经开发出来作为1型糖尿病患者的解决方案，以模拟胰腺的行为和调节血糖水平。然而，目前的人工胰腺系统缺乏对运动引起的血糖摄取的检测能力，这可能会持续4到8个小时。这种无能会导致低血糖，如果不加治疗，可能会造成严重后果，包括死亡。现有的运动检测方法要么局限于单一传感器数据，要么使用不准确的模型进行运动检测，使它们在实践中不太有效。在这项工作中，我们提出了一个集成学习框架，结合了数据驱动的生理模型和Siamese网络，利用多个生理信号流进行高精度的运动检测。

    Type 1 diabetes is a serious disease in which individuals are unable to regulate their blood glucose levels, leading to various medical complications. Artificial pancreas (AP) systems have been developed as a solution for type 1 diabetic patients to mimic the behavior of the pancreas and regulate blood glucose levels. However, current AP systems lack detection capabilities for exercise-induced glucose intake, which can last up to 4 to 8 hours. This incapability can lead to hypoglycemia, which if left untreated, could have serious consequences, including death. Existing exercise detection methods are either limited to single sensor data or use inaccurate models for exercise detection, making them less effective in practice. In this work, we propose an ensemble learning framework that combines a data-driven physiological model and a Siamese network to leverage multiple physiological signal streams for exercise detection with high accuracy. To evaluate the effectiveness of our proposed ap
    
[^18]: 采用极低频智能电表时间序列进行家电检测

    Appliance Detection Using Very Low-Frequency Smart Meter Time Series. (arXiv:2305.10352v1 [eess.SP])

    [http://arxiv.org/abs/2305.10352](http://arxiv.org/abs/2305.10352)

    本文对时间序列分类器在极低频智能电表数据中检测不同家电的存在/缺失进行了深入评估和比较，结果表明......

    

    近年来，智能电表被广泛采用，以改善智能电网系统的管理，这些电表通常以极低的频率（每30分钟）收集能源消耗数据，以更准确地向客户计费。为了提供更个性化的建议，下一步是检测客户拥有的家电，由于极低的计量读数频率，这是一个具有挑战性的问题。尽管家电检测问题可以被视为时间序列分类问题，并且已经在文献中提出了许多这样的分类器，但没有研究将它们应用于这个具体的问题并进行比较。本文提出了对最新时间序列分类器在极低频智能电表数据中检测不同家电的存在/缺失进行深入评估和比较的研究。我们报告了5个真实数据集的结果。我们首先研究了13个时序分类器检测质量的影响。

    In recent years, smart meters have been widely adopted by electricity suppliers to improve the management of the smart grid system. These meters usually collect energy consumption data at a very low frequency (every 30min), enabling utilities to bill customers more accurately. To provide more personalized recommendations, the next step is to detect the appliances owned by customers, which is a challenging problem, due to the very-low meter reading frequency. Even though the appliance detection problem can be cast as a time series classification problem, with many such classifiers having been proposed in the literature, no study has applied and compared them on this specific problem. This paper presents an in-depth evaluation and comparison of state-of-the-art time series classifiers applied to detecting the presence/absence of diverse appliances in very low-frequency smart meter data. We report results with five real datasets. We first study the impact of the detection quality of 13 di
    
[^19]: BIOT：在野外跨数据源生物信号学习

    BIOT: Cross-data Biosignal Learning in the Wild. (arXiv:2305.10351v1 [eess.SP])

    [http://arxiv.org/abs/2305.10351](http://arxiv.org/abs/2305.10351)

    该论文提出了一种可以在多个生物信号数据源上进行训练，并可在所有生物信号任务上进行微调的基础模型BIOT。通过将不同的生物信号令牌化为统一的“生物信号句子”，该模型可以实现跨数据源的学习，处理各种格式的生物信号，具有出色的性能。

    

    生物信号，如脑电图（EEG），在许多临床应用中发挥着至关重要的作用，显示出多种数据格式和质量特征。当前的生物信号深度学习模型通常专门针对特定数据集和临床设置进行了优化，从而限制了它们的应用范围。受到大型语言模型在文本处理方面的成功启发，我们探索开发基于多个数据源进行训练并可在不同基础生物信号任务上进行微调的基础模型。为了克服与各种格式的生物信号相关的独特挑战，例如不匹配的通道，可变的采样长度和普遍存在的缺失值，我们提出了一种Biosignal Transformer模型（简称\method）。所提出的\method模型可以通过将不同的生物信号令牌化为统一的“生物信号句子”来实现通道不匹配、长度可变和缺失值普遍存在的跨数据学习。具体来说，我们将每个通道令牌化为固定长度的特征，并在通道之间进行聚合，以形成完整的生物信号句子。在EEG分类和EEG转语音任务上的实验表明，与当前最先进的基线相比，我们的模型具有卓越的性能。

    Biological signals, such as electroencephalograms (EEG), play a crucial role in numerous clinical applications, exhibiting diverse data formats and quality profiles. Current deep learning models for biosignals are typically specialized for specific datasets and clinical settings, limiting their broader applicability. Motivated by the success of large language models in text processing, we explore the development of foundational models that are trained from multiple data sources and can be fine-tuned on different downstream biosignal tasks.  To overcome the unique challenges associated with biosignals of various formats, such as mismatched channels, variable sample lengths, and prevalent missing values, we propose a Biosignal Transformer (\method). The proposed \method model can enable cross-data learning with mismatched channels, variable lengths, and missing values by tokenizing diverse biosignals into unified "biosignal sentences". Specifically, we tokenize each channel into fixed-le
    
[^20]: 边缘多元宇宙：交互真实世界和数字孪生体的无线波束成形

    Multiverse at the Edge: Interacting Real World and Digital Twins for Wireless Beamforming. (arXiv:2305.10350v1 [eess.SP])

    [http://arxiv.org/abs/2305.10350](http://arxiv.org/abs/2305.10350)

    本文提出了一个数字孪生体的“多元宇宙”范式，其中有几个数字孪生体试图在不同级别的保真度下捕捉真实世界，通过进行自学习以增强基于DL的实时决策。这项工作将有助于加速移动环境中的方向波束选择。

    

    通过先进的仿真软件和普遍的计算能力，创建一个与真实世界的许多复杂交互和结果密切相似的数字世界是可能的。这种基于软件的与真实世界中存在的实体密切模拟的实体被称为“数字孪生体”。在本文中，我们考虑了一个车载毫米波段无线电的孪生体，并展示了如何加速移动环境中的方向波束选择。为了实现这一目标，我们超越了实例化一个单一的孪生体，并提出了“多元宇宙”范式，有几个可能的数字孪生体试图在不同级别的保真度下捕捉真实世界。为此，本文描述（i）车辆上的决策策略，确定由于计算和延迟限制必须使用哪个孪生体，以及（ii）一种利用多元宇宙引导的光束结果来增强基于DL的实时决策的自学习方案。

    Creating a digital world that closely mimics the real world with its many complex interactions and outcomes is possible today through advanced emulation software and ubiquitous computing power. Such a software-based emulation of an entity that exists in the real world is called a 'digital twin'. In this paper, we consider a twin of a wireless millimeter-wave band radio that is mounted on a vehicle and show how it speeds up directional beam selection in mobile environments. To achieve this, we go beyond instantiating a single twin and propose the 'Multiverse' paradigm, with several possible digital twins attempting to capture the real world at different levels of fidelity. Towards this goal, this paper describes (i) a decision strategy at the vehicle that determines which twin must be used given the computational and latency limitations, and (ii) a self-learning scheme that uses the Multiverse-guided beam outcomes to enhance DL-based decision-making in the real world over time. Our work
    
[^21]: 基于受试者的无对照自监督学习用于心电信号处理

    Subject-based Non-contrastive Self-Supervised Learning for ECG Signal Processing. (arXiv:2305.10347v1 [eess.SP])

    [http://arxiv.org/abs/2305.10347](http://arxiv.org/abs/2305.10347)

    本文提出了一种基于受试者的无对照自监督学习方法，用于心电信号处理，不需要数据增强或负对，并在ECG处理中实现了优于最先进SSL方法的表现。

    

    从心电图（ECG）信号中提取信息是心脏病数字健康技术设计的关键步骤。近年来，已经提出了几种用于自动提取ECG信息的机器学习（ML）算法。监督学习方法已成功用于识别信号中的特定方面，如节律障碍（心律失常）的检测。另一方面，自监督学习（SSL）方法可用于提取数据中包含的所有特征。模型在没有特定目标的情况下进行优化，并从数据本身进行学习。通过将最先进的计算机视觉方法适应信号处理领域，最近报道了几种ECG处理的SSL方法。但是，这些SSL方法需要数据增强或负对，这限制了该方法仅查找两个ECG输入之间的相似之处，即同一信号的两个版本或来自不同患者的两个信号。在本文中，我们提出了一种基于受试者的非对照自监督学习方法，用于ECG信号处理，它不需要任何数据增强或负对。我们的方法旨在通过将ECG记录视为来自特定受试者的测量序列，而不是比较两个不同的信号来学习ECG信号的潜在结构。我们的实验结果表明，我们提出的方法在ECG处理中优于最先进的SSL方法。

    Extracting information from the electrocardiography (ECG) signal is an essential step in the design of digital health technologies in cardiology. In recent years, several machine learning (ML) algorithms for automatic extraction of information in ECG have been proposed. Supervised learning methods have successfully been used to identify specific aspects in the signal, like detection of rhythm disorders (arrhythmias). Self-supervised learning (SSL) methods, on the other hand, can be used to extract all the features contained in the data. The model is optimized without any specific goal and learns from the data itself. By adapting state-of-the-art computer vision methodologies to the signal processing domain, a few SSL approaches have been reported recently for ECG processing. However, such SSL methods require either data augmentation or negative pairs, which limits the method to only look for similarities between two ECG inputs, either two versions of the same signal or two signals from
    
[^22]: G-Adapter: 面向图形Transformer网络的结构感知参数高效迁移学习

    G-Adapter: Towards Structure-Aware Parameter-Efficient Transfer Learning for Graph Transformer Networks. (arXiv:2305.10329v1 [cs.LG])

    [http://arxiv.org/abs/2305.10329](http://arxiv.org/abs/2305.10329)

    本文提出一种名为G-Adapter的面向图形Transformer网络的结构感知参数高效迁移学习算法，通过对一组下游图形任务进行广泛测试，证明了将PEFT技术应用于GTNs并非最佳解决方案。

    

    将大规模预训练模型的知识通过微调整个模型参数传递到各个下游任务已成为一种流行的范例。然而，随着模型规模的增长和下游任务数量的增加，这种范例不可避免地面临着计算消耗和内存占用问题。最近，参数高效微调（如Adapter、LoRA、BitFit）展示了一种有望通过仅更新一部分参数来缓解这些问题的范例。尽管这些技术已经在自然语言处理方面展示出了令人满意的性能，但它在图形Transformer网络（GTNs）下的适用性仍然不够广泛。因此，在本文中，我们通过在一系列基于图形的下游任务上进行广泛的基准测试来填补这一空白。我们的实证研究表明，直接将PEFT技术应用于GTNs并不是最优的解决方案。

    It has become a popular paradigm to transfer the knowledge of large-scale pre-trained models to various downstream tasks via fine-tuning the entire model parameters. However, with the growth of model scale and the rising number of downstream tasks, this paradigm inevitably meets the challenges in terms of computation consumption and memory footprint issues. Recently, Parameter-Efficient Fine-Tuning (PEFT) (e.g., Adapter, LoRA, BitFit) shows a promising paradigm to alleviate these concerns by updating only a portion of parameters. Despite these PEFTs having demonstrated satisfactory performance in natural language processing, it remains under-explored for the question of whether these techniques could be transferred to graph-based tasks with Graph Transformer Networks (GTNs). Therefore, in this paper, we fill this gap by providing extensive benchmarks with traditional PEFTs on a range of graph-based downstream tasks. Our empirical study shows that it is sub-optimal to directly transfer 
    
[^23]: 基于卷积神经网络的自动照片方向检测

    Automatic Photo Orientation Detection with Convolutional Neural Networks. (arXiv:2305.10319v1 [cs.CV])

    [http://arxiv.org/abs/2305.10319](http://arxiv.org/abs/2305.10319)

    本论文使用CNN解决了照片方向检测的问题，并在数据集上显著提高性能。使用Guided Backpropagation获得了CNN检测方向的见解。

    

    本文探讨了利用卷积神经网络(CNN)来解决确定消费者照片正确方向(0°, 90°, 180°和270°)的问题，特别对于模拟照片的数字化非常重要。我们在标准数据集上显著提高了性能，并在更困难的消费者照片大型数据集上进行了测试。我们使用引导反向传播(Guided Backpropagation)来获得关于CNN如何检测照片方向的见解，并解释其错误。

    We apply convolutional neural networks (CNN) to the problem of image orientation detection in the context of determining the correct orientation (from 0, 90, 180, and 270 degrees) of a consumer photo. The problem is especially important for digitazing analog photographs. We substantially improve on the published state of the art in terms of the performance on one of the standard datasets, and test our system on a more difficult large dataset of consumer photos. We use Guided Backpropagation to obtain insights into how our CNN detects photo orientation, and to explain its mistakes.
    
[^24]: MetaModulation：在少任务情况下学习变分特征层次的Few-Shot Learning方法

    MetaModulation: Learning Variational Feature Hierarchies for Few-Shot Learning with Fewer Tasks. (arXiv:2305.10309v1 [cs.LG])

    [http://arxiv.org/abs/2305.10309](http://arxiv.org/abs/2305.10309)

    提出了一种名为MetaModulation的少任务Few-Shot Learning方法，使用神经网络在元训练期间调制批量归一化参数以增加元训练任务的密度。本方法通过变分MetaModulation介绍了学习变分特征层次，可以考虑任务不确定性并生成更多样的任务。

    

    元学习算法能够利用先前学习的知识来学习新任务，但通常需要大量元训练任务，这些任务可能不容易得到。为解决这个问题，我们提出了一种名为MetaModulation的少任务Few-Shot Learning方法。关键思想是使用神经网络在元训练期间调制批量归一化参数以增加元训练任务的密度。此外，我们在各个网络层次修改参数，而不仅仅是单个层次，以增加任务多样性。为了考虑有限的训练任务所引起的不确定性，我们提出了一种变分MetaModulation，其中调制参数被视为潜在变量。我们还通过变分MetaModulation介绍了学习变分特征层次，该方法调制所有层次的特征，可以考虑任务不确定性并生成更多样的任务。消融研究证明了本方法的优越性。

    Meta-learning algorithms are able to learn a new task using previously learned knowledge, but they often require a large number of meta-training tasks which may not be readily available. To address this issue, we propose a method for few-shot learning with fewer tasks, which we call MetaModulation. The key idea is to use a neural network to increase the density of the meta-training tasks by modulating batch normalization parameters during meta-training. Additionally, we modify parameters at various network levels, rather than just a single layer, to increase task diversity. To account for the uncertainty caused by the limited training tasks, we propose a variational MetaModulation where the modulation parameters are treated as latent variables. We also introduce learning variational feature hierarchies by the variational MetaModulation, which modulates features at all layers and can consider task uncertainty and generate more diverse tasks. The ablation studies illustrate the advantage
    
[^25]: 深度学习中考虑表格数据数据增强的新思路

    Rethinking Data Augmentation for Tabular Data in Deep Learning. (arXiv:2305.10308v1 [cs.LG])

    [http://arxiv.org/abs/2305.10308](http://arxiv.org/abs/2305.10308)

    本研究提出了一种新的表格数据增强方法“随机连续嵌入”（Random Continuous Embedding，RCE），能够提高 Transformer-based 预训练模型的自监督学习性能，大幅优于现有方法，并使得自监督学习模型能够在监督表格学习中优于树形方法。

    

    表格数据是机器学习中最广泛使用的数据格式。虽然在有监督学习中，树形方法优于深度学习方法；但最近的文献报告称，Transformer-based 预训练模型的自监督学习优于树形方法。在关于表格数据的自监督学习的现有文献中，对比学习是主导方法。然而，由于表格数据的独特结构和高复杂性，表格数据的数据增强一直是困难的。此外，现有方法将模型结构、自监督学习方法和数据增强三个主要组成部分一起提出。因此，以往的研究在综合考虑这些组成部分的情况下进行对比，每个组成部分对实际性能的影响还不清楚。本研究关注数据增强，以解决这些限制。具体地，我们提出了一种新的数据增强方法“随机连续嵌入”（RCE），通过向连续变量注入噪声来生成增强的表格数据。我们在几个基准数据集上评估了我们的方法，并表明 RCE 在使用 Transformer-based 模型进行自监督学习时一致优于现有的数据增强方法。我们还进行筛选研究以显示 RCE 的有效性，并证明 RCE 使 Transformer-based 模型的自监督学习可在监督表格学习中优于树形方法。

    Tabular data is the most widely used data format in machine learning (ML). While tree-based methods outperform DL-based methods in supervised learning, recent literature reports that self-supervised learning with Transformer-based models outperforms tree-based methods. In the existing literature on self-supervised learning for tabular data, contrastive learning is the predominant method. In contrastive learning, data augmentation is important to generate different views. However, data augmentation for tabular data has been difficult due to the unique structure and high complexity of tabular data. In addition, three main components are proposed together in existing methods: model structure, self-supervised learning methods, and data augmentation. Therefore, previous works have compared the performance without comprehensively considering these components, and it is not clear how each component affects the actual performance.  In this study, we focus on data augmentation to address these 
    
[^26]: 针对锂离子电池（用于电动汽车）的剩余寿命和SOH估计

    Estimation of Remaining Useful Life and SOH of Lithium Ion Batteries (For EV Vehicles). (arXiv:2305.10298v1 [cs.LG])

    [http://arxiv.org/abs/2305.10298](http://arxiv.org/abs/2305.10298)

    本文介绍不同方法估计锂电池剩余寿命，提出了一种利用机器学习技术的新方法，可以利用电池性能参数准确预测电池的寿命。

    

    锂离子电池被广泛应用于各种领域，包括便携式电子设备、电动汽车和可再生能源存储系统。准确估计这些电池的剩余寿命对于确保其最佳性能，预防意外故障以及降低维护成本至关重要。本文对现有的估计锂离子电池剩余寿命的方法进行了综合评估，包括基于数据驱动方法、基于物理模型以及混合方法。我们还提出了一种基于机器学习技术的新方法，可以准确预测锂离子电池的剩余寿命。我们的方法利用电池性能参数，包括电压、电流和温度，训练了一个预测模型，可以准确地估计电池的剩余寿命。我们在一个锂离子电池周期的数据集上评估了我们的方法的性能。

    Lithium-ion batteries are widely used in various applications, including portable electronic devices, electric vehicles, and renewable energy storage systems. Accurately estimating the remaining useful life of these batteries is crucial for ensuring their optimal performance, preventing unexpected failures, and reducing maintenance costs. In this paper, we present a comprehensive review of the existing approaches for estimating the remaining useful life of lithium-ion batteries, including data-driven methods, physics-based models, and hybrid approaches. We also propose a novel approach based on machine learning techniques for accurately predicting the remaining useful life of lithium-ion batteries. Our approach utilizes various battery performance parameters, including voltage, current, and temperature, to train a predictive model that can accurately estimate the remaining useful life of the battery. We evaluate the performance of our approach on a dataset of lithium-ion battery cycles
    
[^27]: DualFL：一种基于对偶的Federated Learning算法及在一般凸情形下加速通讯

    DualFL: A Duality-based Federated Learning Algorithm with Communication Acceleration in the General Convex Regime. (arXiv:2305.10294v1 [cs.LG])

    [http://arxiv.org/abs/2305.10294](http://arxiv.org/abs/2305.10294)

    DualFL是一种基于对偶的联邦学习算法，通过具体对偶形式解决分布式优化问题，并保证了即使使用不精确的本地解决方案也可以实现最佳通信复杂度。

    

    我们提出了一种名为DualFL（Dualized Federated Learning）的新型训练算法，用于解决联邦学习的分布式优化问题。我们的方法基于联邦学习问题的特定对偶形式。DualFL在不同的光滑性和强凸性设置下实现通讯加速。此外，它在理论上保证使用不精确的本地求解器，即使是使用不精确的本地解决方案，也可以保持其最佳通信复杂度。DualFL是第一个实现通讯加速的联邦学习算法，即使成本函数既非光滑也非强凸，也可以使用。数值结果表明，DualFL的实际性能与最先进的联邦学习算法相当，并且对超参数调整是稳健的。

    We propose a novel training algorithm called DualFL (Dualized Federated Learning), for solving a distributed optimization problem in federated learning. Our approach is based on a specific dual formulation of the federated learning problem. DualFL achieves communication acceleration under various settings on smoothness and strong convexity of the problem. Moreover, it theoretically guarantees the use of inexact local solvers, preserving its optimal communication complexity even with inexact local solutions. DualFL is the first federated learning algorithm that achieves communication acceleration, even when the cost function is either nonsmooth or non-strongly convex. Numerical results demonstrate that the practical performance of DualFL is comparable to those of state-of-the-art federated learning algorithms, and it is robust with respect to hyperparameter tuning.
    
[^28]: 无限类别混合策略

    Infinite Class Mixup. (arXiv:2305.10293v1 [cs.CV])

    [http://arxiv.org/abs/2305.10293](http://arxiv.org/abs/2305.10293)

    本文提出了一种直接通过混合分类器而不是标签来增强样本的策略。新的分类器是输入对分类器向量的线性插值，使分类器之间的关系更加准确，从而提高深度网络的分类性能。

    

    Mixup 是一种广泛采用的深度网络训练策略，通过插值输入和标签的训练对来增加额外的样本。 Mixup 已经证明可以提高分类性能、网络校准和超出分布概括。虽然有效，但Mixup的一个基石是网络在类别之间学习线性行为模式，但它只是间接地通过概率级别进行输出插值而强制执行。这篇论文旨在通过直接混合分类器而不是混合每个混合对的标签来解决这个限制。我们建议将每个增强的样本的目标定义为一个唯一的新分类器，其参数是输入对的分类器向量的线性插值。所有可能分类器的空间是连续的，涵盖了分类器对之间的所有插值。为了使优化可行，我们提出了一个双对比无限类混合损失，其中我们对每个样本对向所有其他样本对进行对比。

    Mixup is a widely adopted strategy for training deep networks, where additional samples are augmented by interpolating inputs and labels of training pairs. Mixup has shown to improve classification performance, network calibration, and out-of-distribution generalisation. While effective, a cornerstone of Mixup, namely that networks learn linear behaviour patterns between classes, is only indirectly enforced since the output interpolation is performed at the probability level. This paper seeks to address this limitation by mixing the classifiers directly instead of mixing the labels for each mixed pair. We propose to define the target of each augmented sample as a uniquely new classifier, whose parameters are a linear interpolation of the classifier vectors of the input pair. The space of all possible classifiers is continuous and spans all interpolations between classifier pairs. To make optimisation tractable, we propose a dual-contrastive Infinite Class Mixup loss, where we contrast 
    
[^29]: “无任何奖励信息的细调 Fine-tuning:基于混合增强学习的可证明统计优势”

    Reward-agnostic Fine-tuning: Provable Statistical Benefits of Hybrid Reinforcement Learning. (arXiv:2305.10282v1 [cs.LG])

    [http://arxiv.org/abs/2305.10282](http://arxiv.org/abs/2305.10282)

    本文提出一种新的三阶段混合RL算法，不需要奖励信息，有效地利用在线和离线数据，从而实现细调以获得更好的结果。

    

    本论文研究了在混合环境中进行表格强化学习(RL)，该环境假设可以访问离线数据集并在未知环境中进行在线交互。其中一个核心问题在于如何利用在线数据收集来加强和补充离线数据集，从而实现有效的策略细调。本文借鉴了最近的无奖励探索和基于模型的离线RL 的进展，设计了一个三阶段的混合RL算法，其在样本复杂度方面优于仅使用离线RL 和仅使用在线RL 的最佳结果。所提出的算法在数据收集过程中不需要任何奖励信息。我们的理论是基于一个新概念——单策略局部集中性的，该概念捕捉了分布不匹配和覆盖错误之间的权衡，并指导离线和在线数据之间的相互作用。

    This paper studies tabular reinforcement learning (RL) in the hybrid setting, which assumes access to both an offline dataset and online interactions with the unknown environment. A central question boils down to how to efficiently utilize online data collection to strengthen and complement the offline dataset and enable effective policy fine-tuning. Leveraging recent advances in reward-agnostic exploration and model-based offline RL, we design a three-stage hybrid RL algorithm that beats the best of both worlds -- pure offline RL and pure online RL -- in terms of sample complexities. The proposed algorithm does not require any reward information during data collection. Our theory is developed based on a new notion called single-policy partial concentrability, which captures the trade-off between distribution mismatch and miscoverage and guides the interplay between offline and online data.
    
[^30]: 基于学习度量的大规模包裹操作

    Large-Scale Package Manipulation via Learned Metrics of Pick Success. (arXiv:2305.10272v1 [cs.RO])

    [http://arxiv.org/abs/2305.10272](http://arxiv.org/abs/2305.10272)

    本文讨论了基于学习度量的大规模包裹操作，通过训练拾取成功预测器和学习拾取质量度量，实现了能够大规模部署的强力抓握策略。

    

    自动化仓储操作可以降低物流成本，最终降低消费品价格，提高交货速度，并增强对劳动力波动的抵抗能力。近年来，自动化重复任务的兴趣增加，但大多数是在受控环境中进行的。从杂乱的堆堆中挑选物品等任务直到最近才变得足够强大，可以在最小人工干预下进行大规模部署。本文展示了亚马逊机器人的Robot Induction（Robin）群的大规模包裹操作，该群利用在实际生产数据上训练的拾取成功预测器。具体而言，该系统在超过394K个拾取上进行了训练。它用于把每天高达5百万个包裹进行了分离，本文的评估期间操作了超过2亿个包裹。开发的学习拾取质量度量实时排名各种拾取替代方案，并采用高成功率的强力抓握策略。

    Automating warehouse operations can reduce logistics overhead costs, ultimately driving down the final price for consumers, increasing the speed of delivery, and enhancing the resiliency to workforce fluctuations. The past few years have seen increased interest in automating such repeated tasks but mostly in controlled settings. Tasks such as picking objects from unstructured, cluttered piles have only recently become robust enough for large-scale deployment with minimal human intervention.  This paper demonstrates a large-scale package manipulation from unstructured piles in Amazon Robotics' Robot Induction (Robin) fleet, which utilizes a pick success predictor trained on real production data. Specifically, the system was trained on over 394K picks. It is used for singulating up to 5~million packages per day and has manipulated over 200~million packages during this paper's evaluation period.  The developed learned pick quality measure ranks various pick alternatives in real-time and p
    
[^31]: 使用不平衡图册的状态表示学习

    State Representation Learning Using an Unbalanced Atlas. (arXiv:2305.10267v1 [cs.LG])

    [http://arxiv.org/abs/2305.10267](http://arxiv.org/abs/2305.10267)

    本文介绍了一种使用不平衡图册（UA）方法的状态表示学习，该方法可以超越最先进的自监督学习方法。

    

    流形假说认为，高维数据通常位于较低维的流形上，并且利用该流形作为目标空间可以产生更有效的表示。虽然存在许多传统的基于流形的技术用于降维，但它们在自监督学习中的应用进展缓慢。最近的MSIMCLR方法将流形编码与SimCLR相结合，但需要极低的目标编码维度才能胜过SimCLR，从而限制了其适用性。本文介绍了一种使用不平衡图册（UA）的新型学习方法，能够超越最先进的自监督学习方法。我们通过系统地调整时空DeepInfomax（ST-DIM）框架以与我们提议的UA模式保持一致，并在整个过程中采用严谨的科学方法来精心研究和设计了使用UA的DeepInfomax（DIM-UA）方法。

    The manifold hypothesis posits that high-dimensional data often lies on a lower-dimensional manifold and that utilizing this manifold as the target space yields more efficient representations. While numerous traditional manifold-based techniques exist for dimensionality reduction, their application in self-supervised learning has witnessed slow progress. The recent MSIMCLR method combines manifold encoding with SimCLR but requires extremely low target encoding dimensions to outperform SimCLR, limiting its applicability. This paper introduces a novel learning paradigm using an unbalanced atlas (UA), capable of surpassing state-of-the-art self-supervised learning approaches. We meticulously investigated and engineered the DeepInfomax with an unbalanced atlas (DIM-UA) method by systematically adapting the Spatiotemporal DeepInfomax (ST-DIM) framework to align with our proposed UA paradigm, employing rigorous scientific methodologies throughout the process. The efficacy of DIM-UA is demons
    
[^32]: 锐度与位移感知的自监督学习

    Sharpness & Shift-Aware Self-Supervised Learning. (arXiv:2305.10252v1 [cs.LG])

    [http://arxiv.org/abs/2305.10252](http://arxiv.org/abs/2305.10252)

    本文针对自监督学习中的分类下游任务，开发了锐度与数据位移感知理论并提出了基于对比学习的方法(SSA-CLR)，通过特征提取器锐度最小化与傅里叶变换数据增强技术缓解理想分布与实际分布间的数据位移，实现了对分类任务的改进。

    

    自监督学习旨在从无标签数据中提取有意义的特征以进行下游任务。本文考虑分类作为第二阶段下游任务，并开发了严密的理论来实现影响该分类任务的隐含因素。我们的理论表明，锐度感知的特征提取器有利于第二阶段的分类任务，并且理论开发中的理想分布与实际实现中的实际分布之间的数据位移也极大地影响了该分类任务。进一步利用这些理论发现，我们提出了最小化特征提取器的锐度和一种新的基于傅里叶变换的数据增强技术，以缓解实现中分布位移的影响，从而实现了锐度和位移感知的对比学习(SSA-CLR)。我们进行了大量实验以验证我们方法的有效性。

    Self-supervised learning aims to extract meaningful features from unlabeled data for further downstream tasks. In this paper, we consider classification as a downstream task in phase 2 and develop rigorous theories to realize the factors that implicitly influence the general loss of this classification task. Our theories signify that sharpness-aware feature extractors benefit the classification task in phase 2 and the existing data shift between the ideal (i.e., the ideal one used in theory development) and practical (i.e., the practical one used in implementation) distributions to generate positive pairs also remarkably affects this classification task. Further harvesting these theoretical findings, we propose to minimize the sharpness of the feature extractor and a new Fourier-based data augmentation technique to relieve the data shift in the distributions generating positive pairs, reaching Sharpness & Shift-Aware Contrastive Learning (SSA-CLR). We conduct extensive experiments to v
    
[^33]: IoT能量服务中能量损失预测

    Energy Loss Prediction in IoT Energy Services. (arXiv:2305.10238v1 [cs.DC])

    [http://arxiv.org/abs/2305.10238](http://arxiv.org/abs/2305.10238)

    本文提出了一种新颖的IoT能量服务的能量损失预测框架ELP，基于Easeformer算法用于预测共享能量环境中物联网设备的电池电量，进而估算能量损失，实验结果表明该框架明显优于现有方法。

    

    我们提出了一种新颖的能量损失预测（ELP）框架，用于估计共享众包能源服务中的能量损失。众包无线能源服务是一种新颖和便利的解决方案，可实现附近物联网设备的普遍充电。因此，捕捉无线能源分享损失对于成功部署有效的能源服务组合技术至关重要。我们提出了Easeformer，一种基于注意力机制的算法，用于预测共享能量环境中物联网设备的电池电量。预测的电池电量用于估算能量损失。一系列实验被进行来证明所提出的框架的可行性和有效性。我们在真实无线能源数据集上进行了广泛的实验，证明了我们的框架明显优于现有的方法。

    We propose a novel Energy Loss Prediction(ELP) framework that estimates the energy loss in sharing crowdsourced energy services. Crowdsourcing wireless energy services is a novel and convenient solution to enable the ubiquitous charging of nearby IoT devices. Therefore, capturing the wireless energy sharing loss is essential for the successful deployment of efficient energy service composition techniques. We propose Easeformer, a novel attention-based algorithm to predict the battery levels of IoT devices in a crowdsourced energy sharing environment. The predicted battery levels are used to estimate the energy loss. A set of experiments were conducted to demonstrate the feasibility and effectiveness of the proposed framework. We conducted extensive experiments on real wireless energy datasets to demonstrate that our framework significantly outperforms existing methods.
    
[^34]: 评估LLM的隐藏风险：关于鲁棒性、一致性和可信性的实证研究

    Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility. (arXiv:2305.10235v1 [cs.LG])

    [http://arxiv.org/abs/2305.10235](http://arxiv.org/abs/2305.10235)

    本研究是一项关于大型语言模型方面的实证研究，对主流语言模型进行了大量查询和分析，结果发现这些模型存在着鲁棒性、一致性和可信性方面的潜在风险。

    

    大型语言模型（LLMs）的普及对于许多领域产生了重大影响，特别是在其开放式环境（如API、开源模型和插件）中。然而，随着LLMs的广泛部署，缺乏全面讨论和分析潜在风险的研究。因此，我们进行了一项初步但开创性的研究，涵盖了LLMs系统的鲁棒性、一致性和可信性。我们提出了一个自动化工作流程来处理大量查询/响应。总体而言，我们对包括ChatGPT、LLaMA和OPT在内的主流LLMs进行了100多万个查询。我们的工作流核心包括数据原语，随后是自动解释器，评估这些LLMs在不同的对抗性度量系统下的表现。结果，我们得出了几个、也许是不幸的结论，这些结论相当不同

    The recent popularity of large language models (LLMs) has brought a significant impact to boundless fields, particularly through their open-ended ecosystem such as the APIs, open-sourced models, and plugins. However, with their widespread deployment, there is a general lack of research that thoroughly discusses and analyzes the potential risks concealed. In that case, we intend to conduct a preliminary but pioneering study covering the robustness, consistency, and credibility of LLMs systems. With most of the related literature in the era of LLM uncharted, we propose an automated workflow that copes with an upscaled number of queries/responses. Overall, we conduct over a million queries to the mainstream LLMs including ChatGPT, LLaMA, and OPT. Core to our workflow consists of a data primitive, followed by an automated interpreter that evaluates these LLMs under different adversarial metrical systems. As a result, we draw several, and perhaps unfortunate, conclusions that are quite unco
    
[^35]: 探索对比学习中的归纳偏差：从聚类角度出发

    Exploring Inductive Biases in Contrastive Learning: A Clustering Perspective. (arXiv:2305.10229v1 [cs.LG])

    [http://arxiv.org/abs/2305.10229](http://arxiv.org/abs/2305.10229)

    本文比较了对比学习和监督学习方法形成的簇，揭示了对比学习可以生成具有局部密度但无全局密度的簇，而监督学习创建具有局部和全局密度的簇。同时，作者提出了使用图卷积网络分类器作为处理局部密集簇的线性分类器的替代方法，并利用t-SNE可视化证明了对比和监督学习方法产生的特征之间的差异。

    

    本文研究对比学习方法和监督学习方法之间数据组织的差异，重点关注局部密集簇的概念。我们引入一个新的度量指标，相对局部密度（RLD），用于定量测量簇内的局部密度。我们提供了视觉示例，以突出局部密集簇和全局密集簇之间的区别。通过对比对比学习和监督学习形成的簇，我们发现对比学习生成具有局部密度而无全局密度的簇，而监督学习创建具有局部和全局密度的簇。我们进一步探讨了使用图卷积网络（GCN）分类器作为处理局部密集簇的线性分类器的替代方法。最后，我们利用t-SNE可视化来证明对比和监督学习方法产生的特征之间的差异。我们提出了未来的研究方向来结束本文。

    This paper investigates the differences in data organization between contrastive and supervised learning methods, focusing on the concept of locally dense clusters. We introduce a novel metric, Relative Local Density (RLD), to quantitatively measure local density within clusters. Visual examples are provided to highlight the distinctions between locally dense clusters and globally dense ones. By comparing the clusters formed by contrastive and supervised learning, we reveal that contrastive learning generates locally dense clusters without global density, while supervised learning creates clusters with both local and global density. We further explore the use of a Graph Convolutional Network (GCN) classifier as an alternative to linear classifiers for handling locally dense clusters. Finally, we utilize t-SNE visualizations to substantiate the differences between the features generated by contrastive and supervised learning methods. We conclude by proposing future research directions, 
    
[^36]: 在节点破坏下的随机块模型达到Kesten-Stigum临界点

    Reaching Kesten-Stigum Threshold in the Stochastic Block Model under Node Corruptions. (arXiv:2305.10227v1 [cs.SI])

    [http://arxiv.org/abs/2305.10227](http://arxiv.org/abs/2305.10227)

    我们提出了第一个多项式时间算法，在小的破坏节点比例下即可达到Kesten-Stigum临界点的弱恢复。

    

    我们研究了在节点破坏随机块模型下的强健社区检测，其中对于$n$个顶点中的一个部分具有任意修改边缘权重的敌手。我们提出了第一个多项式时间算法，在小的破坏节点比例下即可达到Kesten-Stigum临界点的弱恢复。在这项工作之前，即使是最先进的强健性算法，在接近Kesten-Stigum临界点时也会被节点破坏敌手攻破。我们进一步将我们的技术扩展到$Z_2$同步问题，其中我们的算法在存在类似的强破坏敌手波动时也能达到最优恢复阈值。我们算法的关键因素是一种新型的可识别性证明，利用主子矩阵的Grothendieck范数推出的推出　作用。

    We study robust community detection in the context of node-corrupted stochastic block model, where an adversary can arbitrarily modify all the edges incident to a fraction of the $n$ vertices. We present the first polynomial-time algorithm that achieves weak recovery at the Kesten-Stigum threshold even in the presence of a small constant fraction of corrupted nodes. Prior to this work, even state-of-the-art robust algorithms were known to break under such node corruption adversaries, when close to the Kesten-Stigum threshold.  We further extend our techniques to the $Z_2$ synchronization problem, where our algorithm reaches the optimal recovery threshold in the presence of similar strong adversarial perturbations.  The key ingredient of our algorithm is a novel identifiability proof that leverages the push-out effect of the Grothendieck norm of principal submatrices.
    
[^37]: rWISDM：修复后的行为识别公共数据集

    rWISDM: Repaired WISDM, a Public Dataset for Human Activity Recognition. (arXiv:2305.10222v1 [eess.SP])

    [http://arxiv.org/abs/2305.10222](http://arxiv.org/abs/2305.10222)

    本文研究了行为识别公共数据集WISDM，发现其中存在一些问题，降低了分类器的性能和信任度。通过修复数据集，提高了分类器的性能。

    

    人类活动识别已成为近年来科学研究的亮点，因为它在医疗保健、竞技比赛、智慧城市和智能家居等各个领域都有应用。虽然研究人员专注于处理数据的方法，但用户仍在疑惑用于活动识别的人工智能方法是否可信。信任主要取决于系统的可靠性或鲁棒性。为了调查HAR系统的鲁棒性，我们分析了几个合适的当前公共数据集，并选择WISDM进行我们对深度学习方法的研究。虽然WISDM发布的规格符合我们的基本要求（如大型、平衡、可多硬件操作），但在分析过程中发现了一些隐藏问题。这些问题降低了分类器的性能和整体信任度。通过识别问题并修复数据集，分类器的性能得到了提高。本文介绍了我们用于修复数据集的方法。

    Human Activity Recognition (HAR) has become a spotlight in recent scientific research because of its applications in various domains such as healthcare, athletic competitions, smart cities, and smart home. While researchers focus on the methodology of processing data, users wonder if the Artificial Intelligence (AI) methods used for HAR can be trusted. Trust depends mainly on the reliability or robustness of the system. To investigate the robustness of HAR systems, we analyzed several suitable current public datasets and selected WISDM for our investigation of Deep Learning approaches. While the published specification of WISDM matched our fundamental requirements (e.g., large, balanced, multi-hardware), several hidden issues were found in the course of our analysis. These issues reduce the performance and the overall trust of the classifier. By identifying the problems and repairing the dataset, the performance of the classifier was increased. This paper presents the methods by which 
    
[^38]: 基于可分性和离散度比的SVM正则化参数、核函数和核参数选择方法

    Separability and Scatteredness (S&S) Ratio-Based Efficient SVM Regularization Parameter, Kernel, and Kernel Parameter Selection. (arXiv:2305.10219v1 [stat.ML])

    [http://arxiv.org/abs/2305.10219](http://arxiv.org/abs/2305.10219)

    该文通过分析数据的可分性和离散度，提出了一种基于S&S比的有效SVM正则化参数、核函数和核参数选择方法，表现较传统方法更优。

    

    支持向量机（SVM）是一种具有广泛应用的鲁棒机器学习算法，可用于分类、回归和异常值检测。SVM需要调整正则化参数（RP）来控制模型容量和泛化性能。传统上，通过交叉验证（CV）过程对一系列备选RP进行比较以找到最佳RP。此外，对于非线性可分数据，SVM使用核函数，在核函数的网格中选择一组具有一组参数的核函数。RP和核网格的最佳选择是通过CV的网格搜索获得的。通过随机分析正则化参数的行为，本文展示了SVM性能可以建模为数据的可分性和离散度（S&S）的函数。可分性是类别之间距离的度量，离散度是数据点的传播比率。特别地，对于铰链损失成本函数，S&S比可以有效地估计最优RP。此外，本文提出了一种基于S&S比的高效选择核函数及其参数方法。在各种基准数据集上比较了所提出方法与传统方法的性能，结果表明，所提出方法具有更少的需要调整的超参数且性能优异或可比。

    Support Vector Machine (SVM) is a robust machine learning algorithm with broad applications in classification, regression, and outlier detection. SVM requires tuning the regularization parameter (RP) which controls the model capacity and the generalization performance. Conventionally, the optimum RP is found by comparison of a range of values through the Cross-Validation (CV) procedure. In addition, for non-linearly separable data, the SVM uses kernels where a set of kernels, each with a set of parameters, denoted as a grid of kernels, are considered. The optimal choice of RP and the grid of kernels is through the grid-search of CV. By stochastically analyzing the behavior of the regularization parameter, this work shows that the SVM performance can be modeled as a function of separability and scatteredness (S&S) of the data. Separability is a measure of the distance between classes, and scatteredness is the ratio of the spread of data points. In particular, for the hinge loss cost fun
    
[^39]: 一种受量子机器学习启发的新型随机LSTM模型

    A Novel Stochastic LSTM Model Inspired by Quantum Machine Learning. (arXiv:2305.10212v1 [cs.LG])

    [http://arxiv.org/abs/2305.10212](http://arxiv.org/abs/2305.10212)

    本文提出了一种新型随机LSTM模型，受到变分量子算法的启发，探讨在非量子框架下接近QML的报告成功和潜在好处。

    

    近年来，量子机器学习(QML)的研究表明，QML算法不仅可以像传统算法一样发挥其功能，而且在某些情况下甚至可以超越其表现。在众多研究中，许多QML模型利用变分量子算法(VQA)电路，因为其规模通常足够小，可以与NISQ设备兼容，并且用于优化电路参数的自动微分方法熟悉于机器学习(ML)。尽管这些结果在量子机器更易于使用的时代，带来了有趣的前景，但如果通过非量子方法可以达到类似的结果，那么从业者可能会获得更近期的优势。因此，本文探讨了利用受变分量子LSTM模型启发的随机方法的使用，以试图在非量子框架中接近QML的报告成功和潜在好处。

    Works in quantum machine learning (QML) over the past few years indicate that QML algorithms can function just as well as their classical counterparts, and even outperform them in some cases. Among the corpus of recent work, many current QML models take advantage of variational quantum algorithm (VQA) circuits, given that their scale is typically small enough to be compatible with NISQ devices and the method of automatic differentiation for optimizing circuit parameters is familiar to machine learning (ML). While the results bear interesting promise for an era when quantum machines are more readily accessible, if one can achieve similar results through non-quantum methods then there may be a more near-term advantage available to practitioners. To this end, the nature of this work is to investigate the utilization of stochastic methods inspired by a variational quantum version of the long short-term memory (LSTM) model in an attempt to approach the reported successes in performance and 
    
[^40]: 面向三维MOT中的点云目标再识别

    Towards Object Re-Identification from Point Clouds for 3D MOT. (arXiv:2305.10210v1 [cs.CV])

    [http://arxiv.org/abs/2305.10210](http://arxiv.org/abs/2305.10210)

    该论文研究面向三维MOT中的点云再识别问题，提出了一种轻量级匹配头用于点云ReID的网络，通过实验结果表明，随着传感器分辨率的提高和观测点密度的增加，点云ReID的表现逐渐接近于图像ReID。

    

    本研究旨在通过学习从剪裁的点云观测中匹配对象对（例如使用其预测的三维边界框）来解决三维多目标跟踪（MOT）上的对象再识别（ReID）问题。 我们不关心三维MOT的SOTA性能，而是追求回答以下问题：在实际的跟踪检测环境中，与图片中的ReID相比，来自点云的对象ReID的表现如何？ 为了实现这样的研究，我们提出了一个可以连接到任何集合或序列处理骨干（例如PointNet或ViT）的轻量级匹配头，为两种模态创造可比较的对象ReID网络家族。在孪生样式下运行，我们提出的点云ReID网络可以在实时（10 hz）中进行数千个成对比较。我们的研究结果表明，其表现随着更高的传感器分辨率而提高，并在观测足够密集时接近图像ReID的表现。

    In this work, we study the problem of object re-identification (ReID) in a 3D multi-object tracking (MOT) context, by learning to match pairs of objects from cropped (e.g., using their predicted 3D bounding boxes) point cloud observations. We are not concerned with SOTA performance for 3D MOT, however. Instead, we seek to answer the following question: In a realistic tracking by-detection context, how does object ReID from point clouds perform relative to ReID from images? To enable such a study, we propose a lightweight matching head that can be concatenated to any set or sequence processing backbone (e.g., PointNet or ViT), creating a family of comparable object ReID networks for both modalities. Run in siamese style, our proposed point-cloud ReID networks can make thousands of pairwise comparisons in real-time (10 hz). Our findings demonstrate that their performance increases with higher sensor resolution and approaches that of image ReID when observations are sufficiently dense. Ad
    
[^41]: 带有意图的键值查询模型的空间探索

    Exploring the Space of Key-Value-Query Models with Intention. (arXiv:2305.10203v1 [cs.LG])

    [http://arxiv.org/abs/2305.10203](http://arxiv.org/abs/2305.10203)

    该论文探索了共享 Attention 输入结构但不限于 Attention 计算的模型空间，发现可以使用神经网络计算最小二乘问题的解，这是 Attention 无法高效逼近的，对于神经网络而言也是一种严格的扩展。

    

    基于注意力机制的模型已成为深度学习最新突破的关键要素。注意力的两个关键组成部分是其输入的结构（包含键，值和查询）以及这三个部分如何进行组合的计算。在本文中，我们探索了共享上述输入结构但不仅限于注意力计算的模型空间。我们称这个空间为键-值-查询（KVQ）空间。我们的目标是确定是否存在Attention无法高效逼近、我们可以使用当前的深度学习工具箱实现，并解决社区感兴趣的问题的其他可堆叠模型。也许令人惊讶的是，标准的最小二乘问题的解决方案满足这些属性。能够计算这个解的神经网络模块不仅丰富了神经网络表示的计算集合，而且还被证明是线性回归的一个严格的推广。

    Attention-based models have been a key element of many recent breakthroughs in deep learning. Two key components of Attention are the structure of its input (which consists of keys, values and queries) and the computations by which these three are combined. In this paper we explore the space of models that share said input structure but are not restricted to the computations of Attention. We refer to this space as Keys-Values-Queries (KVQ) Space. Our goal is to determine whether there are any other stackable models in KVQ Space that Attention cannot efficiently approximate, which we can implement with our current deep learning toolbox and that solve problems that are interesting to the community. Maybe surprisingly, the solution to the standard least squares problem satisfies these properties. A neural network module that is able to compute this solution not only enriches the set of computations that a neural network can represent but is also provably a strict generalisation of Linear 
    
[^42]: 通过局部节律稀疏脉冲网络

    Sparsifying Spiking Networks through Local Rhythms. (arXiv:2305.10191v1 [cs.NE])

    [http://arxiv.org/abs/2305.10191](http://arxiv.org/abs/2305.10191)

    本文展示了如何通过使用局部节律来稀疏脉冲网络，从而减少通信和计算能量的要求。

    

    已经确认，在传统神经网络中，每层产生的许多值都是零。在这项工作中，我展示了脉冲型神经网络可以通过使用局部信息防止传输代表接近零的脉冲，从而减少这些网络所需的通信和计算能量，同时保持准确性。此外，这还展示了生物观察到的脉冲节律的新颖应用。

    It has been well-established that within conventional neural networks, many of the values produced at each layer are zero. In this work, I demonstrate that spiking neural networks can prevent the transmission of spikes representing values close to zero using local information. This can reduce the amount of energy required for communication and computation in these networks while preserving accuracy. Additionally, this demonstrates a novel application of biologically observed spiking rhythms.
    
[^43]: 评估具有应用于代步共享的动态条件分位治疗效果

    Evaluating Dynamic Conditional Quantile Treatment Effects with Applications in Ridesharing. (arXiv:2305.10187v1 [stat.ME])

    [http://arxiv.org/abs/2305.10187](http://arxiv.org/abs/2305.10187)

    该论文提出了一种评估代步共享平台中动态条件分位治疗效果的框架，通过个体CQTE之和来简化动态CQTE的评估。

    

    许多现代科技公司，如Google、Uber和Didi，利用在线实验（也称为A / B测试）评估新政策与现有政策之间的差异。虽然大多数研究集中在平均治疗效应上，但偏斜和重尾的结果分布情况可能会受益于其他标准，例如分位数。然而，在处理来自代步共享平台的数据时，评估动态分位治疗效果（QTE）仍然是一个挑战，因为涉及跨时间和空间的顺序决策。在本文中，我们建立了一个正式框架来计算在治疗独立于特征的条件下的QTE。在特定的模型假设下，我们证明了动态条件QTE（CQTE）等于时间上的个体CQTE之和，尽管累积奖励的条件分位数不一定等于个体奖励的条件分位数之和。这一关键洞察力显著简化了动态CQTE的评估。

    Many modern tech companies, such as Google, Uber, and Didi, utilize online experiments (also known as A/B testing) to evaluate new policies against existing ones. While most studies concentrate on average treatment effects, situations with skewed and heavy-tailed outcome distributions may benefit from alternative criteria, such as quantiles. However, assessing dynamic quantile treatment effects (QTE) remains a challenge, particularly when dealing with data from ride-sourcing platforms that involve sequential decision-making across time and space. In this paper, we establish a formal framework to calculate QTE conditional on characteristics independent of the treatment. Under specific model assumptions, we demonstrate that the dynamic conditional QTE (CQTE) equals the sum of individual CQTEs across time, even though the conditional quantile of cumulative rewards may not necessarily equate to the sum of conditional quantiles of individual rewards. This crucial insight significantly strea
    
[^44]: 布尔矩阵分解的整数规划算法。

    Algorithms for Boolean Matrix Factorization using Integer Programming. (arXiv:2305.10185v1 [math.OC])

    [http://arxiv.org/abs/2305.10185](http://arxiv.org/abs/2305.10185)

    本文提出了一种基于整数规划的交替优化策略，解决了布尔矩阵分解的NP难题，并且使用另一个整数规划将多个解组合成最优解，实验表明算法在中等规模问题上优于现有技术。

    

    布尔矩阵分解（BMF）将一个给定的二进制输入矩阵近似表示为两个更小的二进制因子的乘积。相对于使用标准算术的二进制矩阵分解，BMF使用布尔OR和布尔AND操作进行矩阵乘积运算，从而导致更低的重构误差。BMF是一个NP难题。在本文中，我们首先提出了一种交替优化（AO）策略，使用整数规划（IP）解决BMF中一个因子矩阵的子问题。我们还提供了两种初始化因子的方法。然后，我们展示了如何使用另一个IP将BMF的多个解组合到最优解。这使我们能够提出一种新算法：使用AO生成多个解，然后将它们以最优的方式组合起来。实验表明，我们的算法（可在GitLab上获得）在中等规模问题上优于现有技术的状态。

    Boolean matrix factorization (BMF) approximates a given binary input matrix as the product of two smaller binary factors. As opposed to binary matrix factorization which uses standard arithmetic, BMF uses the Boolean OR and Boolean AND operations to perform matrix products, which leads to lower reconstruction errors. BMF is an NP-hard problem. In this paper, we first propose an alternating optimization (AO) strategy that solves the subproblem in one factor matrix in BMF using an integer program (IP). We also provide two ways to initialize the factors within AO. Then, we show how several solutions of BMF can be combined optimally using another IP. This allows us to come up with a new algorithm: it generates several solutions using AO and then combines them in an optimal way. Experiments show that our algorithms (available on gitlab) outperform the state of the art on medium-scale problems.
    
[^45]: 探索拉绍蒙集合中特征交互得分的云

    Exploring the cloud of feature interaction scores in a Rashomon set. (arXiv:2305.10181v1 [cs.LG])

    [http://arxiv.org/abs/2305.10181](http://arxiv.org/abs/2305.10181)

    本文通过探索拉绍蒙集合中准确性类似的模型，引入了特征交互分数（FIS）来检测特征的相互作用。相较于从单个预先指定的模型中提取特征交互，本文提供了更为可靠的方式。

    

    特征间的相互作用是理解机器学习模型行为的核心。最近的研究在单个预测模型中检测和量化特征交互方面取得了重大进展。然而，我们认为从单个预先指定的模型中提取的特征交互可能不可信，因为：一个训练良好的预测模型可能不会保留真实的特征交互，存在多个在特征交互强度方面存在差异但准确性相近的预测模型。因此，我们建议在准确性近似相等的预测模型类中探索特征交互强度。在这项工作中，我们在拉绍蒙集合的上下文中引入了特征交互分数（FIS），表示在给定任务上实现类似准确性的模型集合。我们提出了一种通用且实用的算法来计算模型类中的FIS。我们通过合成数据展示了FIS的属性，并建立了与单个模型中提取的交互特征的联系。

    Interactions among features are central to understanding the behavior of machine learning models. Recent research has made significant strides in detecting and quantifying feature interactions in single predictive models. However, we argue that the feature interactions extracted from a single pre-specified model may not be trustworthy since: a well-trained predictive model may not preserve the true feature interactions and there exist multiple well-performing predictive models that differ in feature interaction strengths. Thus, we recommend exploring feature interaction strengths in a model class of approximately equally accurate predictive models. In this work, we introduce the feature interaction score (FIS) in the context of a Rashomon set, representing a collection of models that achieve similar accuracy on a given task. We propose a general and practical algorithm to calculate the FIS in the model class. We demonstrate the properties of the FIS via synthetic data and draw connecti
    
[^46]: 带子目标预测的目标条件监督学习

    Goal-Conditioned Supervised Learning with Sub-Goal Prediction. (arXiv:2305.10171v1 [cs.LG])

    [http://arxiv.org/abs/2305.10171](http://arxiv.org/abs/2305.10171)

    本文提出了一种扩展GCSL算法的方法， TraIL利用轨迹中的信息预测子目标，以显著提高性能。

    

    最近，提出了一种简单而有效的算法——基于目标条件的监督学习(GCSL)，以处理目标条件的强化学习。GCSL基于事后学习原则：通过观察先前执行的轨迹中访问的状态并将其视为达到的目标，GCSL通过监督学习学习相应的操作。然而，GCSL仅学习目标条件的策略，在此过程中丢弃其他信息。我们的洞察力是，同样的事后学习原则可用于从同一轨迹学习预测目标条件的子目标。基于这个想法，我们提出了Trajectory Iterative Learner(TraIL)，这是GCSL的扩展，进一步利用轨迹中的信息，并用于学习预测动作和子目标。我们研究了TraIL可以更好地利用数据的设置，并发现对于几个流行的问题设置，将GCSL中的真实目标替换为TraIL子目标可以显著提高性能。

    Recently, a simple yet effective algorithm -- goal-conditioned supervised-learning (GCSL) -- was proposed to tackle goal-conditioned reinforcement-learning. GCSL is based on the principle of hindsight learning: by observing states visited in previously executed trajectories and treating them as attained goals, GCSL learns the corresponding actions via supervised learning. However, GCSL only learns a goal-conditioned policy, discarding other information in the process. Our insight is that the same hindsight principle can be used to learn to predict goal-conditioned sub-goals from the same trajectory. Based on this idea, we propose Trajectory Iterative Learner (TraIL), an extension of GCSL that further exploits the information in a trajectory, and uses it for learning to predict both actions and sub-goals. We investigate the settings in which TraIL can make better use of the data, and discover that for several popular problem settings, replacing real goals in GCSL with predicted TraIL su
    
[^47]: 一种用于大规模高斯过程建模的全局-局部近似框架

    A Global-Local Approximation Framework for Large-Scale Gaussian Process Modeling. (arXiv:2305.10158v1 [stat.ML])

    [http://arxiv.org/abs/2305.10158](http://arxiv.org/abs/2305.10158)

    本文提出了一种名为TwinGP的新的全局-局部近似框架，使用子集数据方法，并将相关函数建模为全局和局部核的组合。TwinGP在计算成本的一小部分下表现与最先进的GP建模方法相当或更好。

    

    本文提出了一种新的大规模高斯过程（GP）建模框架。与文献中提出的解决精确GP建模的全局和局部近似相反，我们在构建近似时采用了全局和局部相结合的方法。我们的框架使用子集数据方法，其中子集是一个旨在捕捉数据全局趋势的全局点集的并集，以及一个旨在捕捉给定测试位置周围局部趋势的局部点集。相关函数也被建模为全局和局部核的组合。我们的框架性能，即TwinGP，在计算成本的一小部分下与最先进的GP建模方法相当或更好。

    In this work, we propose a novel framework for large-scale Gaussian process (GP) modeling. Contrary to the global, and local approximations proposed in the literature to address the computational bottleneck with exact GP modeling, we employ a combined global-local approach in building the approximation. Our framework uses a subset-of-data approach where the subset is a union of a set of global points designed to capture the global trend in the data, and a set of local points specific to a given testing location to capture the local trend around the testing location. The correlation function is also modeled as a combination of a global, and a local kernel. The performance of our framework, which we refer to as TwinGP, is on par or better than the state-of-the-art GP modeling methods at a fraction of their computational cost.
    
[^48]: 证明正确性的物理知识神经网络

    Provably Correct Physics-Informed Neural Networks. (arXiv:2305.10157v1 [cs.LG])

    [http://arxiv.org/abs/2305.10157](http://arxiv.org/abs/2305.10157)

    该论文提出了一种名为$\partial$-CROWN的框架，以保证物理知识神经网络（PINN）具有全局正确性的最坏剩余误差，并证明了该框架在获得有效证书方面的有效性。

    

    最近的研究提供了有希望的证据表明，物理知识神经网络（PINN）可以高效地解决偏微分方程（PDE）。然而，以往的研究未能保证PINN在整个时空域内的最坏剩余误差，这是类似于数字求解器的公差的一种度量，而是集中于在一组输入上通过点对点比较来得到解决方案和求解器得到解决方案的结果。在实际应用中，不能认为在一组有限点上的测试就足以使得部署成立，因为在另一组点上性能可能大不相同。为了解决这个问题，我们建立了对整个输入域的PINN基于公差的正确性条件。为了验证它们的有效程度，我们介绍了$\partial$-CROWN：一个通用的、高效的、可扩展的、后训练框架，用于限制PINN的剩余误差。我们演示了它在获得紧密证书方面的有效性。

    Recent work provides promising evidence that Physics-informed neural networks (PINN) can efficiently solve partial differential equations (PDE). However, previous works have failed to provide guarantees on the worst-case residual error of a PINN across the spatio-temporal domain - a measure akin to the tolerance of numerical solvers - focusing instead on point-wise comparisons between their solution and the ones obtained by a solver on a set of inputs. In real-world applications, one cannot consider tests on a finite set of points to be sufficient grounds for deployment, as the performance could be substantially worse on a different set. To alleviate this issue, we establish tolerance-based correctness conditions for PINNs over the entire input domain. To verify the extent to which they hold, we introduce $\partial$-CROWN: a general, efficient and scalable post-training framework to bound PINN residual errors. We demonstrate its effectiveness in obtaining tight certificates by applying
    
[^49]: Lingo3DMol:利用语言模型生成基于口袋的三维分子

    Lingo3DMol: Generation of a Pocket-based 3D Molecule using a Language Model. (arXiv:2305.10133v1 [cs.LG])

    [http://arxiv.org/abs/2305.10133](http://arxiv.org/abs/2305.10133)

    本文提出了一种基于口袋的三维分子生成方法，利用扰动和恢复预训练任务和一种新的分子表示形式。 该方法结合了语言模型和几何深度学习的优点，使得语言模型可以生成准确的三维分子。

    

    近年来，深度生成模型推动的基于结构的药物设计备受瞩目。 语言模型已经展示了在二维结构中生成有效分子的强大能力，而基于几何深度学习的方法则可以直接产生具有准确三维坐标的分子。受这两种方法的启发，本文提出了一种基于口袋的三维分子生成方法，该方法利用语言模型具有生成三维坐标的能力。 由于高质量的蛋白质-配体复合物数据不足，因此设计了一种扰动和恢复预训练任务，可以利用大量的小分子数据。 还提出了一种新的分子表示形式，即带有局部和全局坐标的基于片段的SMILES，使语言模型能够有效地学习分子拓扑结构和空间位置信息。最终，CrossDocked和DUD-E数据集被用于评估和衡量。

    Structure-based drug design powered by deep generative models have attracted increasing research interest in recent years. Language models have demonstrated a robust capacity for generating valid molecules in 2D structures, while methods based on geometric deep learning can directly produce molecules with accurate 3D coordinates. Inspired by both methods, this article proposes a pocket-based 3D molecule generation method that leverages the language model with the ability to generate 3D coordinates. High quality protein-ligand complex data are insufficient; hence, a perturbation and restoration pre-training task is designed that can utilize vast amounts of small-molecule data. A new molecular representation, a fragment-based SMILES with local and global coordinates, is also presented, enabling the language model to learn molecular topological structures and spatial position information effectively. Ultimately, CrossDocked and DUD-E dataset is employed for evaluation and additional metri
    
[^50]: 选择性遗忘：深度生成模型中的持续学习方法

    Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models. (arXiv:2305.10120v1 [cs.LG])

    [http://arxiv.org/abs/2305.10120](http://arxiv.org/abs/2305.10120)

    针对大规模文本到图像模型可能被误用生成有害内容的问题，该论文提出了一种选择性遗忘方法，即持续学习方法，可在深度生成模型中实现可控的遗忘，用户可指定消除哪些概念。

    

    近年来，大规模文本到图像模型的广泛使用引发了人们对这些模型可能被误用生成有害、误导或不当内容的担忧。受此问题的启发，我们提出了一种受持续学习启发的技术，用于有选择性地遗忘预训练的深度生成模型中的概念。我们的方法称为选择性遗忘，可以实现可控的遗忘，用户可以指定该如何遗忘一个概念。选择性遗忘可应用于变分似然模型，涵盖了各种流行的深度生成框架，包括变分自编码器和大规模文本到图像扩散模型。不同模型上的实验证明，我们的方法可以诱导遗忘各种概念，从标准数据集中的整个类别到文本到图像模型中的名人和裸体提示。我们的代码可公开访问，网址为https://github.com/clear-nus/selective-amnesia。

    The recent proliferation of large-scale text-to-image models has led to growing concerns that such models may be misused to generate harmful, misleading, and inappropriate content. Motivated by this issue, we derive a technique inspired by continual learning to selectively forget concepts in pretrained deep generative models. Our method, dubbed Selective Amnesia, enables controllable forgetting where a user can specify how a concept should be forgotten. Selective Amnesia can be applied to conditional variational likelihood models, which encompass a variety of popular deep generative frameworks, including variational autoencoders and large-scale text-to-image diffusion models. Experiments across different models demonstrate that our approach induces forgetting on a variety of concepts, from entire classes in standard datasets to celebrity and nudity prompts in text-to-image models. Our code is publicly available at https://github.com/clear-nus/selective-amnesia.
    
[^51]: 架起桥梁：通过后处理技术增强合成数据的实用性

    Bridging the Gap: Enhancing the Utility of Synthetic Data via Post-Processing Techniques. (arXiv:2305.10118v1 [cs.CV])

    [http://arxiv.org/abs/2305.10118](http://arxiv.org/abs/2305.10118)

    本文介绍了一种利用生成对抗网络生成合成数据集，并通过三种新颖的后处理技术改进合成数据集质量和多样性的方法。作者称其为Gap Filler (GaFi)流程并在真实图像上进行评估。

    

    获取和注释用于训练深度学习模型的合适数据集是具有挑战性的。生成模型已经成为一种有前途的解决方案，可生成替代或增强现实世界数据的合成数据集。尽管如此，合成数据的有效性受到其不能完全捕捉现实世界数据的复杂性和多样性的限制。为了解决这个问题，我们探索使用生成对抗网络生成用于训练分类器的合成数据集，随后在真实图像上进行评估。为了改进合成数据集的质量和多样性，我们提出了三种新颖的后处理技术：动态样本过滤，动态数据集回收和扩展技巧。此外，我们引入了一种名为“ Gap Filler (GaFi)”的流程，在最佳和协调的方式下应用这些技术，以最大程度地提高分类的准确性。

    Acquiring and annotating suitable datasets for training deep learning models is challenging. This often results in tedious and time-consuming efforts that can hinder research progress. However, generative models have emerged as a promising solution for generating synthetic datasets that can replace or augment real-world data. Despite this, the effectiveness of synthetic data is limited by their inability to fully capture the complexity and diversity of real-world data. To address this issue, we explore the use of Generative Adversarial Networks to generate synthetic datasets for training classifiers that are subsequently evaluated on real-world images. To improve the quality and diversity of the synthetic dataset, we propose three novel post-processing techniques: Dynamic Sample Filtering, Dynamic Dataset Recycle, and Expansion Trick. In addition, we introduce a pipeline called Gap Filler (GaFi), which applies these techniques in an optimal and coordinated manner to maximise classifica
    
[^52]: 深度学习可靠地识别胸部X光异常模式吗？一项多读者研究，检查AI在临床实践中一个月的实施情况。

    Can Deep Learning Reliably Recognize Abnormality Patterns on Chest X-rays? A Multi-Reader Study Examining One Month of AI Implementation in Everyday Radiology Clinical Practice. (arXiv:2305.10116v1 [eess.IV])

    [http://arxiv.org/abs/2305.10116](http://arxiv.org/abs/2305.10116)

    本研究开发了一种基于深度学习的自动检测算法，可以在胸部X光片上检测出七种特定放射学发现，并且该算法的性能优于评估图像的六名放射科医师。

    

    本研究开发了一种基于深度学习的自动检测算法（DLAD，Carebot AI CXR），用于检测和定位胸部X线片上的七种特定放射学发现（肺不张（ATE），实变（CON），胸腔积液（EFF），肺部病变（LES），皮下气肿（SCE），心脏扩大（CMG），气胸（PNO））。我们收集了956张胸部X线片，并将DLAD的性能与在医院环境下评估图像的六名单个放射科医师的表现进行了比较。即使与放射科医师相比，所提出的DLAD也取得了高灵敏度（ATE 1.000（0.624-1.000），CON 0.864（0.671-0.956），EFF 0.953（0.887-0.983），LES 0.905（0.715-0.978），SCE 1.000（0.366-1.000），CMG 0.837（0.711-0.917），PNO 0.875（0.538-0.986））（最低：ATE 0.000（0.000-0.376），CON 0.182（0.070-0.382），EFF 0.400（0.302-0.506），LES 0.238（0.103-0.448），SCE 0.000（0.000-0.634），CMG 0.347（0.228-0.486），PNO 0.375（0.134-0.691），最高：ATE 1.000（0.624-1.000），CON 0.864（0.671-0.956），EFF 0.953（0.887-0.983），LES 0.905（0.715-0.978），SCE 1.000（0.366-1.000），CMG 0.837（0.711-0.917），PNO 0.875（0.538-0.986））。

    In this study, we developed a deep-learning-based automatic detection algorithm (DLAD, Carebot AI CXR) to detect and localize seven specific radiological findings (atelectasis (ATE), consolidation (CON), pleural effusion (EFF), pulmonary lesion (LES), subcutaneous emphysema (SCE), cardiomegaly (CMG), pneumothorax (PNO)) on chest X-rays (CXR). We collected 956 CXRs and compared the performance of the DLAD with that of six individual radiologists who assessed the images in a hospital setting. The proposed DLAD achieved high sensitivity (ATE 1.000 (0.624-1.000), CON 0.864 (0.671-0.956), EFF 0.953 (0.887-0.983), LES 0.905 (0.715-0.978), SCE 1.000 (0.366-1.000), CMG 0.837 (0.711-0.917), PNO 0.875 (0.538-0.986)), even when compared to the radiologists (LOWEST: ATE 0.000 (0.000-0.376), CON 0.182 (0.070-0.382), EFF 0.400 (0.302-0.506), LES 0.238 (0.103-0.448), SCE 0.000 (0.000-0.634), CMG 0.347 (0.228-0.486), PNO 0.375 (0.134-0.691), HIGHEST: ATE 1.000 (0.624-1.000), CON 0.864 (0.671-0.956), E
    
[^53]: 一种基于深度学习集成的方法进行COVID-19胸部CT扫描的严重程度预测

    An Ensemble Deep Learning Approach for COVID-19 Severity Prediction Using Chest CT Scans. (arXiv:2305.10115v1 [eess.IV])

    [http://arxiv.org/abs/2305.10115](http://arxiv.org/abs/2305.10115)

    该论文基于深度学习集成的方法，使用STOIC数据集对COVID-19胸部CT扫描进行严重程度预测，并采用了数据增强技术和测试时间增强以提高性能。

    

    胸部X射线广泛用于COVID-19筛查，但是3D计算机断层扫描（CT）是一种更有效的成像方式。我们使用STOIC数据集对COVID-19胸部CT扫描进行严重程度预测。我们开发了一种基于深度学习集成模型的方法，该模型结合了多个神经网络以提高预测准确性。为了解决数据不平衡问题，我们使用了切片函数和数据增强技术。我们通过测试时间数据增强进一步提高了性能。我们的方法采用简单而有效的深度学习模型集成，结合强大的测试时间增强，与更复杂的方法相比实现了可比拟的结果，并在STOIC2021 COVID-19 AI挑战赛中获得了第四名。我们的代码可在以下链接找到：https://github.com/aleemsidra/stoic2021-baseline-finalphase-main

    Chest X-rays have been widely used for COVID-19 screening; however, 3D computed tomography (CT) is a more effective modality. We present our findings on COVID-19 severity prediction from chest CT scans using the STOIC dataset. We developed an ensemble deep learning based model that incorporates multiple neural networks to improve predictions. To address data imbalance, we used slicing functions and data augmentation. We further improved performance using test time data augmentation. Our approach which employs a simple yet effective ensemble of deep learning-based models with strong test time augmentations, achieved results comparable to more complex methods and secured the fourth position in the STOIC2021 COVID-19 AI Challenge. Our code is available on online: at: https://github.com/aleemsidra/stoic2021- baseline-finalphase-main.
    
[^54]: 稀疏矩阵分解中的自动超参数调整

    Automatic Hyperparameter Tuning in Sparse Matrix Factorization. (arXiv:2305.10114v1 [stat.ML])

    [http://arxiv.org/abs/2305.10114](http://arxiv.org/abs/2305.10114)

    稀疏矩阵分解中使用贝叶斯框架，提出了一种通过评估稀疏矩阵先验中归一化因子的零点来进行超参数调整的新型数值方法，并在地面真实稀疏矩阵重建中表现出优异性能。

    

    我们研究了贝叶斯框架下稀疏矩阵分解中的超参数调整问题。在先前的工作中，基于几个近似，通过变分贝叶斯方法得到了具有拉普拉斯先验的稀疏矩阵分解的分析解。基于此解，我们提出了一种通过评估稀疏矩阵先验中归一化因子的零点来进行超参数调整的新型数值方法。我们还通过与广泛使用的稀疏主成分分析算法进行比较，验证了我们的方法在地面真实稀疏矩阵重建中表现出的优异性能。

    We study the problem of hyperparameter tuning in sparse matrix factorization under Bayesian framework. In the prior work, an analytical solution of sparse matrix factorization with Laplace prior was obtained by variational Bayes method under several approximations. Based on this solution, we propose a novel numerical method of hyperparameter tuning by evaluating the zero point of normalization factor in sparse matrix prior. We also verify that our method shows excellent performance for ground-truth sparse matrix reconstruction by comparing it with the widely-used algorithm of sparse principal component analysis.
    
[^55]: 基于神经符号的AI用于电气控制面板的合规检查

    Neuro-Symbolic AI for Compliance Checking of Electrical Control Panels. (arXiv:2305.10113v1 [cs.AI])

    [http://arxiv.org/abs/2305.10113](http://arxiv.org/abs/2305.10113)

    本文提出了一种基于神经符号的AI方法，结合深度学习技术和答案集编程来自动化电气控制面板的合规性验证。该方法可以在只有非常有限的训练数据下识别可能存在的异常和错误，实验结果表明该方法具有很好的效果。

    

    人工智能在支持和改善智能制造和工业4.0方面发挥着重要作用，通过使领域专家手动执行的不同类型的任务自动化。特别是，评估产品与相对原理图的符合性是一项耗时且容易出错的过程。在本文中，我们针对特定的工业场景解决此问题。具体而言，我们定义了一种神经符号方法来自动化电气控制面板的合规性验证。我们的方法基于深度学习技术和答案集编程（ASP）的组合，即使只有非常有限的训练数据，也可以识别出最终产品中可能存在的异常和错误。通过意大利一家从事电气控制面板生产的公司提供的实际测试案例进行的实验表明了所提出方法的有效性。

    Artificial Intelligence plays a main role in supporting and improving smart manufacturing and Industry 4.0, by enabling the automation of different types of tasks manually performed by domain experts. In particular, assessing the compliance of a product with the relative schematic is a time-consuming and prone-to-error process. In this paper, we address this problem in a specific industrial scenario. In particular, we define a Neuro-Symbolic approach for automating the compliance verification of the electrical control panels. Our approach is based on the combination of Deep Learning techniques with Answer Set Programming (ASP), and allows for identifying possible anomalies and errors in the final product even when a very limited amount of training data is available. The experiments conducted on a real test case provided by an Italian Company operating in electrical control panel production demonstrate the effectiveness of the proposed approach.
    
[^56]: 基于图神经网络的推特互动预测

    Predicting Tweet Engagement with Graph Neural Networks. (arXiv:2305.10103v1 [cs.SI])

    [http://arxiv.org/abs/2305.10103](http://arxiv.org/abs/2305.10103)

    本研究提出TweetGage，一个基于图神经网络的解决方案，通过表示发布帖子间的语义关联来预测用户在社交媒体上的互动，相对其他研究只考虑帖子文本和发布用户等因素，有效提高了预测准确性。

    

    社交网络是最重要的在线内容分享平台之一，预测发布内容的互动情况是利用社交媒体实现盈利的关键。本文认为发布的内容之间的语义关联也是互动数增长的关键因素。因此，我们提出了TweetGage，一个基于图的神经网络解决方案，通过新颖的基于图的模型来表示帖子间的关系，从而预测用户的互动。为了验证我们的提议，我们针对Twitter平台进行了全面的实验研究，证明了该方法的优良性能。

    Social Networks represent one of the most important online sources to share content across a world-scale audience. In this context, predicting whether a post will have any impact in terms of engagement is of crucial importance to drive the profitable exploitation of these media. In the literature, several studies address this issue by leveraging direct features of the posts, typically related to the textual content and the user publishing it. In this paper, we argue that the rise of engagement is also related to another key component, which is the semantic connection among posts published by users in social media. Hence, we propose TweetGage, a Graph Neural Network solution to predict the user engagement based on a novel graph-based model that represents the relationships among posts. To validate our proposal, we focus on the Twitter platform and perform a thorough experimental campaign providing evidence of its quality.
    
[^57]: 一种多目标优化的Wasserstein反向强化学习模型的证明

    A proof of imitation of Wasserstein inverse reinforcement learning for multi-objective optimization. (arXiv:2305.10089v1 [cs.LG])

    [http://arxiv.org/abs/2305.10089](http://arxiv.org/abs/2305.10089)

    本文证明了Wasserstein反向强化学习模型适用于多目标优化问题，可让学习者的奖励值和最优解模仿专家，具有一定的实用价值。

    

    本文证明了Wasserstein反向强化学习模型可以在有限次迭代中让学习者的奖励值模仿专家的奖励值，并证明了在词典序的多目标优化中，Wasserstein反向强化学习模型可以让学习者的最优解模仿专家的最优解。

    We prove Wasserstein inverse reinforcement learning enables the learner's reward values to imitate the expert's reward values in a finite iteration for multi-objective optimizations. Moreover, we prove Wasserstein inverse reinforcement learning enables the learner's optimal solutions to imitate the expert's optimal solutions for multi-objective optimizations with lexicographic order.
    
[^58]: 冷启动问题：无监督的类别发现方法。

    Cold PAWS: Unsupervised class discovery and the cold-start problem. (arXiv:2305.10071v1 [cs.CV])

    [http://arxiv.org/abs/2305.10071](http://arxiv.org/abs/2305.10071)

    本文提出了一种新方法，通过结合自我监督、聚类和流形学习技术，解决冷启动或无监督选择标记问题，并在多个公共数据集上进行了测试，获得了更好的性能。

    

    在许多机器学习应用中，标记数据集常常是一项艰苦且耗时的任务。虽然研究表明半监督学习技术可以在计算机视觉领域中使用非常少的标签实现高准确性，但很少有人关注如何选择数据集中的图像进行标记。本文提出了一种基于自监督学习、聚类和流形学习技术的新方法，以解决首次选择信息图像子集进行标记的挑战，即冷启动或无监督选择标记问题。我们使用几个公共数据集（包括CIFAR10、Imagenette、DeepWeeds和EuroSAT）测试我们的方法，并观察到当使用我们的标签选择策略时，与随机抽样相比，在监督和半监督学习策略均表现出更好的性能。我们还在d方面获得了更优秀的性能

    In many machine learning applications, labeling datasets can be an arduous and time-consuming task. Although research has shown that semi-supervised learning techniques can achieve high accuracy with very few labels within the field of computer vision, little attention has been given to how images within a dataset should be selected for labeling. In this paper, we propose a novel approach based on well-established self-supervised learning, clustering, and manifold learning techniques that address this challenge of selecting an informative image subset to label in the first instance, which is known as the cold-start or unsupervised selective labelling problem. We test our approach using several publicly available datasets, namely CIFAR10, Imagenette, DeepWeeds, and EuroSAT, and observe improved performance with both supervised and semi-supervised learning strategies when our label selection strategy is used, in comparison to random sampling. We also obtain superior performance for the d
    
[^59]: 用于自监督聚类无线电频谱活动的XAI

    XAI for Self-supervised Clustering of Wireless Spectrum Activity. (arXiv:2305.10060v1 [cs.LG])

    [http://arxiv.org/abs/2305.10060](http://arxiv.org/abs/2305.10060)

    本文提出了一种用于解释深度聚类的自监督学习架构的方法，使用基于卷积神经网络的表示学习部分和聚类部分，并利用引导反向传播技术解释相交区域的含义。

    

    所谓的黑盒深度学习模型在许多科学领域中，包括无线通信领域，被越来越多地用于分类任务。在这一趋势中，监督式深度学习模型似乎是领域相关分类问题最常见的解决方案。虽然它们被证明具有无与伦比的性能，但需要大量标记的训练数据和不可解释的推理作为两个主要缺点，限制了它们的使用。自监督架构出现作为一个减少所需标记数据规模的有前途的解决方案，但解释性问题仍然存在。在本文中，我们提出了一种解释深度聚类的方法，自监督学习架构由基于卷积神经网络（CNN）的表示学习部分和聚类部分组成。对于最先进的表示学习部分，我们的方法采用了引导反向传播来解释相交区域的含义。

    The so-called black-box deep learning (DL) models are increasingly used in classification tasks across many scientific disciplines, including wireless communications domain. In this trend, supervised DL models appear as most commonly proposed solutions to domain-related classification problems. Although they are proven to have unmatched performance, the necessity for large labeled training data and their intractable reasoning, as two major drawbacks, are constraining their usage. The self-supervised architectures emerged as a promising solution that reduces the size of the needed labeled data, but the explainability problem remains. In this paper, we propose a methodology for explaining deep clustering, self-supervised learning architectures comprised of a representation learning part based on a Convolutional Neural Network (CNN) and a clustering part. For the state of the art representation learning part, our methodology employs Guided Backpropagation to interpret the regions of inter
    
[^60]: 基于卷积核的混合特征学习方法用于ATM故障预测的研究

    A hybrid feature learning approach based on convolutional kernels for ATM fault prediction using event-log data. (arXiv:2305.10059v1 [cs.LG])

    [http://arxiv.org/abs/2305.10059](http://arxiv.org/abs/2305.10059)

    该论文提出了一种基于卷积核的混合特征学习方法，通过结合深度学习、核方法和基于规则的特征选择从ATM事件日志中提取相关特征，用于早期ATM故障预测，在真实的ATM事件日志数据上进行的广泛实验表明其优于现有技术。

    

    预测性维护（PdM）方法旨在在设备故障之前安排维护工作。在这种情况下，检测自动取款机（ATM）的早期故障变得越来越重要，因为这些机器易受各种不可预测的故障影响。ATM通过生成大量的事件日志数据来跟踪执行状态，这些数据收集与故障事件无关的系统消息。 基于事件日志预测故障会导致额外的挑战，主要在于提取可能表示即将发生故障的事件序列的特征。因此，特征学习方法目前正在PdM中使用，其中从最小处理的传感器数据中自动学习有信息量的特征。但是，仍然存在如何利用这些方法来从基于事件日志的数据中导出相关特征的空白。为了填补这个空白，我们提出了基于卷积核的预测模型（MiniROCKET）用于早期ATM故障预测。我们提出的方法包括一种混合特征学习技术，通过结合深度学习、核方法和基于规则的特征选择从ATM事件日志中提取相关特征。我们对真实的ATM事件日志数据进行了广泛的实验，结果表明我们提出的方法在分类性能、稳定性、可解释性和可扩展性方面优于现有技术。

    Predictive Maintenance (PdM) methods aim to facilitate the scheduling of maintenance work before equipment failure. In this context, detecting early faults in automated teller machines (ATMs) has become increasingly important since these machines are susceptible to various types of unpredictable failures. ATMs track execution status by generating massive event-log data that collect system messages unrelated to the failure event. Predicting machine failure based on event logs poses additional challenges, mainly in extracting features that might represent sequences of events indicating impending failures. Accordingly, feature learning approaches are currently being used in PdM, where informative features are learned automatically from minimally processed sensor data. However, a gap remains to be seen on how these approaches can be exploited for deriving relevant features from event-log-based data. To fill this gap, we present a predictive model based on a convolutional kernel (MiniROCKET
    
[^61]: 物理驱动的机器学习在日冕物质抛射旅行时间预测中的应用

    Physics-driven machine learning for the prediction of coronal mass ejections' travel times. (arXiv:2305.10057v1 [astro-ph.SR])

    [http://arxiv.org/abs/2305.10057](http://arxiv.org/abs/2305.10057)

    该论文介绍了物理驱动的人工智能方法用于日冕物质抛射旅行时间预测，使用了确定性拖拽模型和两个神经网络，利用远程监测和现场数据，显着提高了预测的准确性和稳健性。

    

    日冕物质抛射（CMEs）是指从太阳冕层中将等离子体和磁场剧烈地抛射到太阳系中的过程。CMEs在科学上具有重要性，因为它们参与了表征活跃太阳的物理机制。然而，近年来，CMEs因其对空间天气的影响而受到关注，因为它们与地磁风暴相关并且可能引发太阳高能粒子流的产生。在这个空间天气框架下，本文介绍了一种基于物理驱动的人工智能（AI）方法来预测CMEs旅行时间，其中利用确定性拖拽模型来改善级联两个神经网络的训练阶段，以供远程监测和现场数据。这项研究表明，在AI架构中利用物理信息显着提高了旅行时间预测的准确性和稳健性。

    Coronal Mass Ejections (CMEs) correspond to dramatic expulsions of plasma and magnetic field from the solar corona into the heliosphere. CMEs are scientifically relevant because they are involved in the physical mechanisms characterizing the active Sun. However, more recently CMEs have attracted attention for their impact on space weather, as they are correlated to geomagnetic storms and may induce the generation of Solar Energetic Particles streams. In this space weather framework, the present paper introduces a physics-driven artificial intelligence (AI) approach to the prediction of CMEs travel time, in which the deterministic drag-based model is exploited to improve the training phase of a cascade of two neural networks fed with both remote sensing and in-situ data. This study shows that the use of physical information in the AI architecture significantly improves both the accuracy and the robustness of the travel time prediction.
    
[^62]: SHoP：一种深度学习框架用于求解高阶偏微分方程

    SHoP: A Deep Learning Framework for Solving High-order Partial Differential Equations. (arXiv:2305.10033v1 [cs.LG])

    [http://arxiv.org/abs/2305.10033](http://arxiv.org/abs/2305.10033)

    该论文提出了一种名为SHoP的深度学习框架，以解决高阶偏微分方程问题。该框架通过神经网络推导出高阶导数规则，并将网络展开成泰勒级数，提供一个明确的PDE解决方案。

    

    求解偏微分方程（PDE）一直是计算科学的基本问题，也是科学和工程研究的广泛应用领域。由于神经网络具有通用逼近性，因此广泛用于逼近PDE的解。然而，由于高阶导数的计算精度不足，现有方法无法解决高阶PDE，并且所得到的网络是一个没有明确解释的黑盒子。为了解决这些问题，我们提出了一种名为SHoP的深度学习框架，用于解决高阶PDE。具体而言，我们推导了神经网络的高阶导数规则，以便快速准确地获得导数；此外，我们把网络展开成泰勒级数，为PDE提供了一个明确的解决方案。我们对不同维度的四个高阶PDE进行了实验验证，表明我们可以高效而准确地解决高阶PDE。

    Solving partial differential equations (PDEs) has been a fundamental problem in computational science and of wide applications for both scientific and engineering research. Due to its universal approximation property, neural network is widely used to approximate the solutions of PDEs. However, existing works are incapable of solving high-order PDEs due to insufficient calculation accuracy of higher-order derivatives, and the final network is a black box without explicit explanation. To address these issues, we propose a deep learning framework to solve high-order PDEs, named SHoP. Specifically, we derive the high-order derivative rule for neural network, to get the derivatives quickly and accurately; moreover, we expand the network into a Taylor series, providing an explicit solution for the PDEs. We conduct experimental validations four high-order PDEs with different dimensions, showing that we can solve high-order PDEs efficiently and accurately.
    
[^63]: 利用半监督学习和视觉Transformer进行细粒度分类的迁移学习

    Transfer Learning for Fine-grained Classification Using Semi-supervised Learning and Visual Transformers. (arXiv:2305.10018v1 [cs.CV])

    [http://arxiv.org/abs/2305.10018](http://arxiv.org/abs/2305.10018)

    使用半监督学习技术对ViT模型进行微调，可用于缺乏注释数据的细粒度分类任务，比传统CNN和ViT都表现更好，有助于解决电子商务中图像标注问题。

    

    细粒度分类是一项具有挑战性的任务，涉及识别同一类别内物体之间微小差异。在数据稀缺的情况下，这项任务尤其具有挑战性。由于其使用自注意力机制学习视觉数据高度表现力的能力，视觉Transformer（ViT）最近已成为图像分类的强大工具。在本研究中，我们探讨了半监督ViT，一种应用于缺乏注释数据情况下的ViT模型微调的半监督学习技术，这在电子商务中特别常见，其中图像易于获取但标签有噪音、不存在或获取昂贵。我们的结果表明，即使使用有限的注释数据进行微调，半监督ViT仍然优于传统的卷积神经网络（CNN）和ViT。这些发现表明，半监督ViT在需要精确和细粒度分类的应用中具有重要的前景。

    Fine-grained classification is a challenging task that involves identifying subtle differences between objects within the same category. This task is particularly challenging in scenarios where data is scarce. Visual transformers (ViT) have recently emerged as a powerful tool for image classification, due to their ability to learn highly expressive representations of visual data using self-attention mechanisms. In this work, we explore Semi-ViT, a ViT model fine tuned using semi-supervised learning techniques, suitable for situations where we have lack of annotated data. This is particularly common in e-commerce, where images are readily available but labels are noisy, nonexistent, or expensive to obtain. Our results demonstrate that Semi-ViT outperforms traditional convolutional neural networks (CNN) and ViTs, even when fine-tuned with limited annotated data. These findings indicate that Semi-ViTs hold significant promise for applications that require precise and fine-grained classifi
    
[^64]: 合成数据生成的效用理论

    Utility Theory of Synthetic Data Generation. (arXiv:2305.10015v1 [stat.ML])

    [http://arxiv.org/abs/2305.10015](http://arxiv.org/abs/2305.10015)

    本文从统计学角度建立效用理论，旨在基于一般性指标定量评估合成算法的效用，效用指标的分析界限揭示了指标收敛的关键条件，令人惊讶的是，只要下游学习任务中的模型规范是正确的，合成特征分布不一定与原始特征分布相同，效用指标会收敛。

    

    评估合成数据的效用对于衡量合成算法的有效性和效率至关重要。现有的结果侧重于对合成数据效用的经验评估，而针对合成数据算法如何影响效用的理论理解仍然未被充分探索。本文从统计学角度建立效用理论，旨在基于一般性指标定量评估合成算法的效用。该指标定义为在合成和原始数据集上训练的模型之间泛化的绝对差异。我们建立了该效用指标的分析界限来研究指标收敛的关键条件。一个有趣的结果是，只要下游学习任务中的模型规范是正确的，合成特征分布不一定与原始特征分布相同，则该效用指标会收敛。另一个重要的效用指标基于合成和原始数据之间潜在的因果机制一致性。该理论使用几种合成算法进行说明，并分析了它们的效用属性。

    Evaluating the utility of synthetic data is critical for measuring the effectiveness and efficiency of synthetic algorithms. Existing results focus on empirical evaluations of the utility of synthetic data, whereas the theoretical understanding of how utility is affected by synthetic data algorithms remains largely unexplored. This paper establishes utility theory from a statistical perspective, aiming to quantitatively assess the utility of synthetic algorithms based on a general metric. The metric is defined as the absolute difference in generalization between models trained on synthetic and original datasets. We establish analytical bounds for this utility metric to investigate critical conditions for the metric to converge. An intriguing result is that the synthetic feature distribution is not necessarily identical to the original one for the convergence of the utility metric as long as the model specification in downstream learning tasks is correct. Another important utility metri
    
[^65]: 深度学习的多目标参数优化方法综述

    A Survey on Multi-Objective based Parameter Optimization for Deep Learning. (arXiv:2305.10014v1 [cs.LG])

    [http://arxiv.org/abs/2305.10014](http://arxiv.org/abs/2305.10014)

    本文综述了单一目标的深度学习优化技术，重点介绍了多目标优化方法在深度学习中的应用，并探讨了多目标优化在深度学习中的挑战和未来方向。

    

    深度学习模型是目前最为强大的机器学习模型之一，尤其是在提取重要特征方面表现出色。目前，大部分深度神经网络的设计参数仍需要手动调整，因此得到高性能的模型非常费时且时常不可能。优化深度网络参数需要更高收敛率的改进优化算法。而基于单一目标的优化方法通常耗时且不能在所有情况下保证最佳性能。包含多个需要同时优化的目标函数的数学优化问题属于多目标优化范畴，有时也被称为帕累托优化。多目标优化问题是参数优化的一种备选且实用的方案，但这个领域的研究还不够深入。本文将首先综述单一目标的深度学习优化技术，然后重点关注面向深度学习模型的多目标优化方法，同时提供多目标优化在深度学习中的挑战和未来方向的见解。

    Deep learning models form one of the most powerful machine learning models for the extraction of important features. Most of the designs of deep neural models, i.e., the initialization of parameters, are still manually tuned. Hence, obtaining a model with high performance is exceedingly time-consuming and occasionally impossible. Optimizing the parameters of the deep networks, therefore, requires improved optimization algorithms with high convergence rates. The single objective-based optimization methods generally used are mostly time-consuming and do not guarantee optimum performance in all cases. Mathematical optimization problems containing multiple objective functions that must be optimized simultaneously fall under the category of multi-objective optimization sometimes referred to as Pareto optimization. Multi-objective optimization problems form one of the alternatives yet useful options for parameter optimization. However, this domain is a bit less explored. In this survey, we f
    
[^66]: Reprompting: 通过吉布斯采样自动推断思维链的提示

    Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling. (arXiv:2305.09993v1 [cs.LG])

    [http://arxiv.org/abs/2305.09993](http://arxiv.org/abs/2305.09993)

    Reprompting是一种无需人类干预的算法，通过迭代采样新配方解决多步推理任务，比人类编写的思维链提示表现更好，还可以提高较弱模型的性能。

    

    我们引入了Reprompting，这是一种迭代采样算法，可以在没有人类干预的情况下搜索给定任务的思维链配方。通过吉布斯采样，我们推断适用于一组训练样例的思维链配方。我们的方法使用先前采样的解作为父提示，迭代地采样新的配方来解决其他训练问题。在需要多步推理的五个Big-Bench Hard任务中，Reprompting的表现始终优于零样本、少样本和人类编写的思维链基线。Reprompting还可以促进知识从一个更强的模型到一个较弱的模型的转移，从而大大提高了较弱模型的性能。总体而言，Reprompting相对于使用人类编写的思维链提示的先前最先进方法，带来了高达+17个性能改进。

    We introduce Reprompting, an iterative sampling algorithm that searches for the Chain-of-Thought (CoT) recipes for a given task without human intervention. Through Gibbs sampling, we infer CoT recipes that work consistently well for a set of training samples. Our method iteratively samples new recipes using previously sampled solutions as parent prompts to solve other training problems. On five Big-Bench Hard tasks that require multi-step reasoning, Reprompting achieves consistently better performance than the zero-shot, few-shot, and human-written CoT baselines. Reprompting can also facilitate transfer of knowledge from a stronger model to a weaker model leading to substantially improved performance of the weaker model. Overall, Reprompting brings up to +17 point improvements over the previous state-of-the-art method that uses human-written CoT prompts.
    
[^67]: 鲁棒的多域网络用于短时间扫描淀粉样PET重建

    A robust multi-domain network for short-scanning amyloid PET reconstruction. (arXiv:2305.09986v1 [eess.IV])

    [http://arxiv.org/abs/2305.09986](http://arxiv.org/abs/2305.09986)

    本论文介绍了一种鲁棒的多域网络，旨在恢复短时间内获取的低质量淀粉样PET图像。网络的核心贡献在于引入了映射标签进行有效学习，使得该网络可在多个训练域和未知域中高效校正淀粉样PET数据集。

    

    本论文介绍了一种鲁棒的多域网络，旨在恢复短时间内获取的低质量淀粉样PET图像。使用多个域中来自短（2分钟）和标准（20分钟）扫描时间的PET图像对来进行训练，并引入了映射标签进行有效学习。该网络可在多个训练域和未知域中高效校正淀粉样PET数据集。

    This paper presents a robust multi-domain network designed to restore low-quality amyloid PET images acquired in a short period of time. The proposed method is trained on pairs of PET images from short (2 minutes) and standard (20 minutes) scanning times, sourced from multiple domains. Learning relevant image features between these domains with a single network is challenging. Our key contribution is the introduction of a mapping label, which enables effective learning of specific representations between different domains. The network, trained with various mapping labels, can efficiently correct amyloid PET datasets in multiple training domains and unseen domains, such as those obtained with new radiotracers, acquisition protocols, or PET scanners. Internal, temporal, and external validations demonstrate the effectiveness of the proposed method. Notably, for external validation datasets from unseen domains, the proposed method achieved comparable or superior results relative to methods
    
[^68]: 大规模机器学习问题的随机比率跟踪算法

    Stochastic Ratios Tracking Algorithm for Large Scale Machine Learning Problems. (arXiv:2305.09978v1 [cs.LG])

    [http://arxiv.org/abs/2305.09978](http://arxiv.org/abs/2305.09978)

    本文提出了一种新的算法，在经典的SGD框架下实现自适应步长选择，在逻辑回归和深度神经网络上测试了所提出的算法，并证明了该算法可以生成与手动调整得到的最佳步长相当的步长。

    

    许多机器学习应用和任务都依赖于随机梯度下降（SGD）算法及其变体。有效的步长选择对算法的成功至关重要，这促进了诸如ADAM或AdaGrad之类的算法的发展。在本文中，我们提出了一种新颖的算法，在经典的SGD框架下实现自适应步长选择，它可以轻松适应其他随机算法。我们的算法灵感来自传统的非线性优化技术，并受到分析发现的支持。我们展示了在合理条件下，该算法产生符合良好理论要求的步长，并在期望下生成收敛于解的静止邻域的迭代。我们在逻辑回归和深度神经网络上测试了所提出的算法，并证明了该算法可以生成与手动调整得到的最佳步长相当的步长。

    Many machine learning applications and tasks rely on the stochastic gradient descent (SGD) algorithm and its variants. Effective step length selection is crucial for the success of these algorithms, which has motivated the development of algorithms such as ADAM or AdaGrad. In this paper, we propose a novel algorithm for adaptive step length selection in the classical SGD framework, which can be readily adapted to other stochastic algorithms. Our proposed algorithm is inspired by traditional nonlinear optimization techniques and is supported by analytical findings. We show that under reasonable conditions, the algorithm produces step lengths in line with well-established theoretical requirements, and generates iterates that converge to a stationary neighborhood of a solution in expectation. We test the proposed algorithm on logistic regressions and deep neural networks and demonstrate that the algorithm can generate step lengths comparable to the best step length obtained from manual tu
    
[^69]: 基于YOLOv8的实时飞行物体检测

    Real-Time Flying Object Detection with YOLOv8. (arXiv:2305.09972v1 [cs.CV])

    [http://arxiv.org/abs/2305.09972](http://arxiv.org/abs/2305.09972)

    本文提出了一个基于YOLOv8的通用模型，可实现实时检测的飞行物体；通过进一步训练，生成精细模型，克服了在真实环境数据中存在的问题。

    

    本文提出了一个通用模型，用于实时检测飞行物体，可用于迁移学习和进一步研究，以及一个可供实施的精细模型。我们先使用包含40个不同类别的数据集对通用模型进行训练，强制模型提取抽象的特征表示。然后，我们使用这些学习到的参数进行迁移学习，以在更具代表性的真实环境数据集上生成我们的精细模型。由于飞行物体的物体空间大小/纵横比、速度、遮挡和背景的差异很大，因此飞行物体的检测仍然具有挑战性。为了应对这些挑战，同时最大限度地提高性能，我们利用了最新的单次检测器YOLOv8，以找到最佳的性能平衡点。

    This paper presents a generalized model for real-time detection of flying objects that can be used for transfer learning and further research, as well as a refined model that is ready for implementation. We achieve this by training our first generalized model on a data set containing 40 different classes of flying objects, forcing the model to extract abstract feature representations. We then perform transfer learning with these learned parameters on a data set more representative of real world environments (i.e., higher frequency of occlusion, small spatial sizes, rotations, etc.) to generate our refined model. Object detection of flying objects remains challenging due to large variance object spatial sizes/aspect ratios, rate of speed, occlusion, and clustered backgrounds. To address some of the presented challenges while simultaneously maximizing performance, we utilize the current state of the art single-shot detector, YOLOv8, in an attempt to find the best tradeoff between inferen
    
[^70]: 可变长度嵌入

    Variable Length Embeddings. (arXiv:2305.09967v1 [cs.CV])

    [http://arxiv.org/abs/2305.09967](http://arxiv.org/abs/2305.09967)

    本文介绍了可变长度嵌入（VLEs）的深度学习架构，采用自回归模型来生成由任意数量标记组成的潜在表示。VLE在涉及重构和图像分解的任务中表现出色，且比最先进的VAE使用更少的参数。

    

    本文介绍了一种新颖的深度学习架构，即可变长度嵌入（VLEs），它是一种自回归模型，可以生成由任意数量标记组成的潜在表示。作为概念验证，我们展示了VLEs在涉及重构和图像分解的任务中的能力。我们在iNaturalist和ImageNet数据集混合使用的实验中进行了评估，并发现VLE与最先进的VAE相比，达到了可比较的重构结果，仅使用了不到十分之一的参数。

    In this work, we introduce a novel deep learning architecture, Variable Length Embeddings (VLEs), an autoregressive model that can produce a latent representation composed of an arbitrary number of tokens. As a proof of concept, we demonstrate the capabilities of VLEs on tasks that involve reconstruction and image decomposition. We evaluate our experiments on a mix of the iNaturalist and ImageNet datasets and find that VLEs achieve comparable reconstruction results to a state of the art VAE, using less than a tenth of the parameters.
    
[^71]: 深度量子神经网络对应高斯过程

    Deep quantum neural networks form Gaussian processes. (arXiv:2305.09957v1 [quant-ph])

    [http://arxiv.org/abs/2305.09957](http://arxiv.org/abs/2305.09957)

    本文证明了基于Haar随机酉或正交深量子神经网络的某些模型的输出会收敛于高斯过程。然而，这种高斯过程不能用于通过贝叶斯统计学来有效预测QNN的输出。

    

    众所周知，从独立同分布的先验条件开始初始化的人工神经网络在隐藏层神经元数目足够大的极限下收敛到高斯过程。本文证明了量子神经网络（QNNs）也存在类似的结果。特别地，我们证明了基于Haar随机酉或正交深QNNs的某些模型的输出在希尔伯特空间维度$d$足够大时会收敛于高斯过程。由于输入状态、测量的可观测量以及酉矩阵的元素不独立等因素的作用，本文对这一结果的推导比经典情形更加微妙。我们分析的一个重要后果是，这个结果得到的高斯过程不能通过贝叶斯统计学来有效地预测QNN的输出。此外，我们的定理表明，Haar随机QNNs中的测量现象比以前认为的要更严重，我们证明了演员的集中现象。

    It is well known that artificial neural networks initialized from independent and identically distributed priors converge to Gaussian processes in the limit of large number of neurons per hidden layer. In this work we prove an analogous result for Quantum Neural Networks (QNNs). Namely, we show that the outputs of certain models based on Haar random unitary or orthogonal deep QNNs converge to Gaussian processes in the limit of large Hilbert space dimension $d$. The derivation of this result is more nuanced than in the classical case due the role played by the input states, the measurement observable, and the fact that the entries of unitary matrices are not independent. An important consequence of our analysis is that the ensuing Gaussian processes cannot be used to efficiently predict the outputs of the QNN via Bayesian statistics. Furthermore, our theorems imply that the concentration of measure phenomenon in Haar random QNNs is much worse than previously thought, as we prove that ex
    
[^72]: 理解卷积神经网络的初始凝结

    Understanding the Initial Condensation of Convolutional Neural Networks. (arXiv:2305.09947v1 [cs.LG])

    [http://arxiv.org/abs/2305.09947](http://arxiv.org/abs/2305.09947)

    本研究揭示了小初始化和基于梯度的训练方法下卷积神经网络的凝聚现象，并在理论上证明了在有限训练期间，CNN的卷积核将会收敛到一个或几个方向。

    

    先前的研究表明，具有小初始化和基于梯度的训练方法的全连接网络在训练期间表现出一种称为凝结的现象。这种现象指的是隐层神经元的输入权重在训练期间凝聚成孤立的方向，揭示了参数空间中朝向简单解决方案的隐含偏差。然而，神经网络结构对凝聚的影响尚未得到研究。在本研究中，我们专注于卷积神经网络（CNN）的研究。我们的实验表明，当受到小初始化和基于梯度的训练方法的影响时，CNN层内的卷积核权重在训练期间也会聚集在一起，显示出显着的凝聚度。在理论上，我们证明在有限的训练时间内，小初始化的两层CNN的卷积核将会收敛到一个或几个方向。这项工作代表了更好地理解卷积神经网络在训练期间的行为的一步。

    Previous research has shown that fully-connected networks with small initialization and gradient-based training methods exhibit a phenomenon known as condensation during training. This phenomenon refers to the input weights of hidden neurons condensing into isolated orientations during training, revealing an implicit bias towards simple solutions in the parameter space. However, the impact of neural network structure on condensation has not been investigated yet. In this study, we focus on the investigation of convolutional neural networks (CNNs). Our experiments suggest that when subjected to small initialization and gradient-based training methods, kernel weights within the same CNN layer also cluster together during training, demonstrating a significant degree of condensation. Theoretically, we demonstrate that in a finite training period, kernels of a two-layer CNN with small initialization will converge to one or a few directions. This work represents a step towards a better under
    
[^73]: DeepMSS：基于PET/CT图像的深度多模态切片到生存预测学习

    DeepMSS: Deep Multi-Modality Segmentation-to-Survival Learning for Survival Outcome Prediction from PET/CT Images. (arXiv:2305.09946v1 [eess.IV])

    [http://arxiv.org/abs/2305.09946](http://arxiv.org/abs/2305.09946)

    提出了一种DeepMSS模型，采用新颖的Segmentated-to-Survival（STS）框架，使用多模态渐进聚合网络（MMPAN）来探索肿瘤内外的预后信息，并通过自我注意力机制增强的深度生存模型进行生存预测，取得了在两个公共PET/CT图像数据集上优于几种最先进的方法的结果。

    

    生存预测是癌症管理的主要关注点。基于深度学习的深度生存模型已被广泛采用，用于在医学图像上执行端到端的生存预测。最近的深度生存模型通过联合执行肿瘤分割和生存预测，采用多任务学习指导模型提取与肿瘤相关的信息，取得了有希望的性能。然而，现有的深度生存模型在探索肿瘤外预后信息（例如，局部淋巴结转移和邻近组织侵袭）方面存在困难。此外，现有的深度生存模型在利用多模态图像方面欠发展。为了解决这些问题，我们提出了一种名为DeepMSS的深度多模态切片到生存模型。该模型采用一种新颖的Segmentated-to-Survival（STS）框架，通过分离分割和生存预测任务来进行。对于分割，我们使用一种新颖的多模态渐进聚合网络（MMPAN）来探索肿瘤内外的预后信息。对于生存预测，我们提出了一种自我注意力机制增强的深度生存模型，该模型学习MMPAN的特征表示并执行生存预测。在两个公共PET/CT图像数据集上的实验结果表明，我们提出的DeepMSS模型在生存预测方面优于几种最先进的方法。

    Survival prediction is a major concern for cancer management. Deep survival models based on deep learning have been widely adopted to perform end-to-end survival prediction from medical images. Recent deep survival models achieved promising performance by jointly performing tumor segmentation with survival prediction, where the models were guided to extract tumor-related information through Multi-Task Learning (MTL). However, existing deep survival models have difficulties in exploring out-of-tumor prognostic information (e.g., local lymph node metastasis and adjacent tissue invasions). In addition, existing deep survival models are underdeveloped in utilizing multi-modality images. Empirically-designed strategies were commonly adopted to fuse multi-modality information via fixed pre-designed networks. In this study, we propose a Deep Multi-modality Segmentation-to-Survival model (DeepMSS) for survival prediction from PET/CT images. Instead of adopting MTL, we propose a novel Segmentat
    
[^74]: 可解释的强化学习中的匹兹堡学习分类器系统：与 XCS 的比较

    Pittsburgh Learning Classifier Systems for Explainable Reinforcement Learning: Comparing with XCS. (arXiv:2305.09945v1 [cs.LG])

    [http://arxiv.org/abs/2305.09945](http://arxiv.org/abs/2305.09945)

    本文介绍了两个新的匹兹堡学习分类器系统，与经典的密歇根系统 XCS 进行了比较，并在确定性和随机 FrozenLake 环境中获得了较好的表现。

    

    最近，由于深度学习技术的应用，对强化学习（RL）的兴趣有所增加，但是与象征性系统相比，这些连接主义方法不透明。学习分类器系统 (LCS) 是进化机器学习系统，可被归类为可解释的人工智能 (XAI)，因为它们基于规则的本质。密歇根 LCS 通常在 RL 领域中使用，而匹兹堡系统（例如 SAMUEL）由于复杂的算法设计和高计算要求而很少使用；然而，它们可以产生比密歇根系统更简洁/可解释的解决方案。我们旨在开发两个新的匹兹堡 LCS，以解决 RL 领域的问题：PPL-DL 和 PPL-ST。前者充当“零级”系统，后者重访了 SAMUEL 的核心蒙特卡罗学习机制，用于估计规则强度。我们将我们的两个匹兹堡系统与密歇根系统 XCS 在确定性和随机 FrozenLake 环境中进行比较。结果表明，PPL-ST在两种情况下表现良好。

    Interest in reinforcement learning (RL) has recently surged due to the application of deep learning techniques, but these connectionist approaches are opaque compared with symbolic systems. Learning Classifier Systems (LCSs) are evolutionary machine learning systems that can be categorised as eXplainable AI (XAI) due to their rule-based nature. Michigan LCSs are commonly used in RL domains as the alternative Pittsburgh systems (e.g. SAMUEL) suffer from complex algorithmic design and high computational requirements; however they can produce more compact/interpretable solutions than Michigan systems. We aim to develop two novel Pittsburgh LCSs to address RL domains: PPL-DL and PPL-ST. The former acts as a "zeroth-level" system, and the latter revisits SAMUEL's core Monte Carlo learning mechanism for estimating rule strength. We compare our two Pittsburgh systems to the Michigan system XCS across deterministic and stochastic FrozenLake environments. Results show that PPL-ST performs on-pa
    
[^75]: 无需演示的自主增强学习：隐式双向课程法

    Demonstration-free Autonomous Reinforcement Learning via Implicit and Bidirectional Curriculum. (arXiv:2305.09943v1 [cs.LG])

    [http://arxiv.org/abs/2305.09943](http://arxiv.org/abs/2305.09943)

    本文提出了一种无需演示的自主强化学习算法（IBC），通过辅助代理和基于最优输运的双向目标课程，能够在无需先前数据依赖的情况下，实现从非周期性交互中学习，并在稀疏任务相关交互的环境中取得更好的表现。

    

    虽然强化学习在仅通过与环境交互来获得复杂技能方面取得了巨大成功，但它假设在每个周期结束时都可以轻易地回到初始状态。这种假设妨碍了具身代理的自主学习，因为在物理世界中进行重置需要耗费时间和繁琐的解决方案。因此，对于能够从非周期性交互中学习的自主强化学习（ARL）方法越来越受到关注。然而，现有的ARL方法受到其对先前数据的依赖的限制，无法在任务相关交互稀疏的环境中学习。相反，我们提出了一种通过隐式和双向课程的无演示ARL算法（IBC）。通过辅助代理以及基于最优输运的双向目标课程，我们的方法表现优于以前的方法，甚至比利用演示的方法还要好。

    While reinforcement learning (RL) has achieved great success in acquiring complex skills solely from environmental interactions, it assumes that resets to the initial state are readily available at the end of each episode. Such an assumption hinders the autonomous learning of embodied agents due to the time-consuming and cumbersome workarounds for resetting in the physical world. Hence, there has been a growing interest in autonomous RL (ARL) methods that are capable of learning from non-episodic interactions. However, existing works on ARL are limited by their reliance on prior data and are unable to learn in environments where task-relevant interactions are sparse. In contrast, we propose a demonstration-free ARL algorithm via Implicit and Bi-directional Curriculum (IBC). With an auxiliary agent that is conditionally activated upon learning progress and a bidirectional goal curriculum based on optimal transport, our method outperforms previous methods, even the ones that leverage dem
    
[^76]: “我全然成为我自己”：以TGNB人群为中心，评估开放式语言生成中的偏见

    "I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation. (arXiv:2305.09941v1 [cs.CL])

    [http://arxiv.org/abs/2305.09941](http://arxiv.org/abs/2305.09941)

    本论文研究了如何以TGNB人群的声音为中心，评估开放式语言生成中的偏见。通过理解TGNB个体的经历，提出了以TGNB人群为中心的OLG系统评估框架，并且包括一个为TGNB人群设计的调查工具和分析方法。

    

    跨性别和非二元（TGNB）人群在日常生活中经历了不成比例的歧视和排斥。随着语言生成技术的日益普及和应用，进一步边缘化这一人群的可能性也在增加。虽然大量的NLP公平文献着重于阐明和解决性别偏见，但评估TGNB身份所带来的性别伤害需要理解这些身份如何独特地与社会性别规范互动以及与性别二元中心的视角相区分。这样的测量框架本质上需要以TGNB声音为中心，帮助指导包容性别的自然语言处理应该为谁服务。为实现这一目标，我们以TGNB社区和现有的跨学科文献为基础，评估了TGNB个体经历边缘化所形成的社会现实是如何影响和存在于开放式语言生成（OLG）中。首先理解TGNB个体的经历，我们提出了一个评估OLG系统的框架，旨在以TGNB人群为中心，度量与该人群相关的偏见。我们的框架包括特别为TGNB人群设计的调查工具，以及交叉分析结果的交叉方法。我们相信，这项工作将有助于实现更公平、更包容的自然语言处理社区，并潜在地解决NLP研究中广泛的交叉身份问题。

    Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization by TGNB persons contributes to and persists within Open Language Generation (OLG). By first understandi
    
[^77]: 图中长尾类别的特征化

    Characterizing Long-Tail Categories on Graphs. (arXiv:2305.09938v1 [cs.LG])

    [http://arxiv.org/abs/2305.09938](http://arxiv.org/abs/2305.09938)

    该研究提出了图上长尾分类的第一个泛化边界，并提出了一种可表征长尾类别的行为并提高机器学习模型在现实世界网络中的泛化性能的新图表示学习框架。

    

    长尾数据分布在许多现实世界的网络中普遍存在，包括金融交易网络、电子商务网络和合作网络。尽管最近取得了成功，但现有的作品主要集中于通过图增强或目标重新加权消除机器学习模型的偏见。然而，目前有限的文献提供理论工具来表征图上长尾类别的行为，并理解实际情况下的泛化性能。为填补这一空白，我们通过将问题形式化为多任务学习的方式，即每个任务对应于预测一个特定的类别，提出了图上长尾分类的第一个泛化边界。我们的理论结果表明，长尾分类的泛化性能受所有任务中的损失范围和任务总数的支配。在理论发现的基础上，我们提出了一种新的图表示学习框架，可表征长尾类别的行为，并提高机器学习模型在现实世界网络中的泛化性能。

    Long-tail data distributions are prevalent in many real-world networks, including financial transaction networks, e-commerce networks, and collaboration networks. Despite the success of recent developments, the existing works mainly focus on debiasing the machine learning models via graph augmentation or objective reweighting. However, there is limited literature that provides a theoretical tool to characterize the behaviors of long-tail categories on graphs and understand the generalization performance in real scenarios. To bridge this gap, we propose the first generalization bound for long-tail classification on graphs by formulating the problem in the fashion of multi-task learning, i.e., each task corresponds to the prediction of one particular category. Our theoretical results show that the generalization performance of long-tail classification is dominated by the range of losses across all tasks and the total number of tasks. Building upon the theoretical findings, we propose a n
    
[^78]: 缓解联邦学习中的群体偏见:超越本地公平

    Mitigating Group Bias in Federated Learning: Beyond Local Fairness. (arXiv:2305.09931v1 [cs.LG])

    [http://arxiv.org/abs/2305.09931](http://arxiv.org/abs/2305.09931)

    本文研究了联邦学习中群体偏见问题，并提出了一种全局公平、利用拉格朗日乘数考虑群体公平的训练算法，实验证明该方法在公平性和准确性方面优于现有方法。

    

    机器学习模型中群体公平的问题已经被认识到一段时间了，其中某些子人群或群体被优先考虑。虽然在集中式学习中提出了许多缓解策略，但这些方法在联邦学习中不直接适用，因为数据存在于多个客户端上并被私下存储。为了解决这个问题，许多提议试图在聚合之前在客户端水平上缓解偏差，我们称之为本地公平训练。然而，这些方法的有效性尚未得到很好的理解。在这项工作中，我们通过研究全局模型公平性和本地模型公平性之间的关系，研究了本地公平训练的理论基础。此外，我们证明了对于广泛的公平指标类别，可以仅使用本地客户端的汇总统计信息来获得全局模型的公平性。基于此，我们提出了一种具有全局公平性的训练算法，该算法直接最小化全局模型和本地模型之间的经验惩罚L2距离，同时使用拉格朗日乘数考虑群体公平。在合成数据集和真实数据集上的实验结果显示，我们提出的方法在公平性和准确性方面优于现有方法。

    The issue of group fairness in machine learning models, where certain sub-populations or groups are favored over others, has been recognized for some time. While many mitigation strategies have been proposed in centralized learning, many of these methods are not directly applicable in federated learning, where data is privately stored on multiple clients. To address this, many proposals try to mitigate bias at the level of clients before aggregation, which we call locally fair training. However, the effectiveness of these approaches is not well understood. In this work, we investigate the theoretical foundation of locally fair training by studying the relationship between global model fairness and local model fairness. Additionally, we prove that for a broad class of fairness metrics, the global model's fairness can be obtained using only summary statistics from local clients. Based on that, we propose a globally fair training algorithm that directly minimizes the penalized empirical l
    
[^79]: 模型验证作为概率推断的方法

    Model-based Validation as Probabilistic Inference. (arXiv:2305.09930v1 [cs.RO])

    [http://arxiv.org/abs/2305.09930](http://arxiv.org/abs/2305.09930)

    本文提出了模型验证作为概率推断的方法，在自主系统中估计故障分布。该方法使用基于模型的方法表示故障轨迹分布，并利用自动微分计算轨迹梯度。在多个场景下进行了演示，结果表明在样本效率和参数空间覆盖范围方面取得了改善。

    

    评估失败分布是验证自主系统的关键步骤。现有的方法侧重于寻找一小范围初始条件下的故障或对测试系统的属性做出限制性假设。我们将顺序系统的故障轨迹分布估计视为贝叶斯推断问题，并采用基于模型的方法利用系统动态的模拟结果表示故障轨迹的分布。在计算轨迹梯度时采用自动微分。我们在倒立摆控制系统、自主驾驶汽车场景和部分可观测月球着陆器中演示了我们的方法。使用开箱即用的Hamiltonian Monte Carlo进行采样，同时使用多链以捕获多模态，并采用梯度平滑以实现安全轨迹。在所有实验中，与黑盒基线相比，我们观察到了样本效率和参数空间覆盖范围的改善。

    Estimating the distribution over failures is a key step in validating autonomous systems. Existing approaches focus on finding failures for a small range of initial conditions or make restrictive assumptions about the properties of the system under test. We frame estimating the distribution over failure trajectories for sequential systems as Bayesian inference. Our model-based approach represents the distribution over failure trajectories using rollouts of system dynamics and computes trajectory gradients using automatic differentiation. Our approach is demonstrated in an inverted pendulum control system, an autonomous vehicle driving scenario, and a partially observable lunar lander. Sampling is performed using an off-the-shelf implementation of Hamiltonian Monte Carlo with multiple chains to capture multimodality and gradient smoothing for safe trajectories. In all experiments, we observed improvements in sample efficiency and parameter space coverage compared to black-box baseline a
    
[^80]: Tinto：地球科学中3D高光谱点云分割的多传感器基准测试

    Tinto: Multisensor Benchmark for 3D Hyperspectral Point Cloud Segmentation in the Geosciences. (arXiv:2305.09928v1 [cs.CV])

    [http://arxiv.org/abs/2305.09928](http://arxiv.org/abs/2305.09928)

    Tinto是一个多传感器数字露头基准测试数据集，旨在促进开发和验证地质制图的深度学习方法，特别是针对非结构化的3D数据如点云。

    

    深度学习技术的使用减少了地质图的解释时间，并通过从数字露头模型自动派生地质图来理想地减少了解释者偏见。然而，由于地质图解释的主观性以及收集定量验证数据的困难，对这些自动化制图方法的准确验证是一项重大挑战。此外，许多最先进的深度学习方法仅限于2D图像数据，这对于诸如超级云这样的3D数字露头是不足够的。为了解决这些挑战，我们提出了Tinto，这是一个多传感器数字露头基准测试数据集，旨在促进开发和验证地质制图的深度学习方法，特别是针对非结构化的3D数据如点云。Tinto包括两个互补的数据集：1）来自Corta Atalaya（西班牙）的真实数字露头模型，具有光谱属性和地面真实数据，以及2）使用潜在变量的合成数据集，以模拟真实数据的各种变化和不确定性。

    The increasing use of deep learning techniques has reduced interpretation time and, ideally, reduced interpreter bias by automatically deriving geological maps from digital outcrop models. However, accurate validation of these automated mapping approaches is a significant challenge due to the subjective nature of geological mapping and the difficulty in collecting quantitative validation data. Additionally, many state-of-the-art deep learning methods are limited to 2D image data, which is insufficient for 3D digital outcrops, such as hyperclouds. To address these challenges, we present Tinto, a multi-sensor benchmark digital outcrop dataset designed to facilitate the development and validation of deep learning approaches for geological mapping, especially for non-structured 3D data like point clouds. Tinto comprises two complementary sets: 1) a real digital outcrop model from Corta Atalaya (Spain), with spectral attributes and ground-truth data, and 2) a synthetic twin that uses latent
    
[^81]: 一种适用于可解释和简约强化学习策略的基因模糊系统

    A Genetic Fuzzy System for Interpretable and Parsimonious Reinforcement Learning Policies. (arXiv:2305.09922v1 [cs.LG])

    [http://arxiv.org/abs/2305.09922](http://arxiv.org/abs/2305.09922)

    提出了一种基于基因模糊系统的强化学习策略，可演化出可解释的简约策略，并能有效平衡策略性能与复杂性。

    

    强化学习（RL）正经历着研究兴趣的复苏，学习分类器系统（LCS）已经被应用多年。然而，传统的密歇根方法往往会演化成大量的规则库，这些规则库难以解释或扩展到超出标准迷宫之外的领域。提出了一种匹兹堡基因模糊系统，名为Fuzzy MoCoCo，其利用多目标和合作协同进化机制，为RL环境演化模糊规则策略。系统中的多目标与策略性能与复杂性有关。连续状态的RL环境Mountain Car被用作测试基础。结果表明，该系统能够有效探索策略性能与复杂性之间的权衡，并学习出可解释的，高性能的策略，并尽可能少地使用规则。

    Reinforcement learning (RL) is experiencing a resurgence in research interest, where Learning Classifier Systems (LCSs) have been applied for many years. However, traditional Michigan approaches tend to evolve large rule bases that are difficult to interpret or scale to domains beyond standard mazes. A Pittsburgh Genetic Fuzzy System (dubbed Fuzzy MoCoCo) is proposed that utilises both multiobjective and cooperative coevolutionary mechanisms to evolve fuzzy rule-based policies for RL environments. Multiobjectivity in the system is concerned with policy performance vs. complexity. The continuous state RL environment Mountain Car is used as a testing bed for the proposed system. Results show the system is able to effectively explore the trade-off between policy performance and complexity, and learn interpretable, high-performing policies that use as few rules as possible.
    
[^82]: 评估上下文推断误差和部分可观察性对及时自适应干预RL方法的影响

    Assessing the Impact of Context Inference Error and Partial Observability on RL Methods for Just-In-Time Adaptive Interventions. (arXiv:2305.09913v1 [cs.LG])

    [http://arxiv.org/abs/2305.09913](http://arxiv.org/abs/2305.09913)

    本文探讨了在及时自适应干预中，强化学习方法如何学习干预选项选择策略，结果表明上下文推断误差和部分可观察性对学习有效策略的能力产生影响，通过在上下文不确定性增加时从上下文推断中传播的不确定性可以提高干预效果，而策略梯度算法可以提供对部分观察到的行为状态信息的非凡鲁棒性。

    

    及时自适应干预(JITAIs)是行为科学界开发的一类个性化健康干预。 JITAIs旨在通过从预定义的组件集中迭代选择干预选项序列来响应每个个体的时间变化状态，以提供正确类型和数量的支持。在这项工作中，我们探讨了强化学习方法应用于学习干预选项选择策略的问题。我们研究了上下文推断误差和部分可观察性对学习有效策略的能力的影响。我们的结果表明，当上下文不确定性增加时，从上下文推断中传播的不确定性对提高干预效果至关重要，而策略梯度算法可以提供对部分观察到的行为状态信息的非凡鲁棒性。

    Just-in-Time Adaptive Interventions (JITAIs) are a class of personalized health interventions developed within the behavioral science community. JITAIs aim to provide the right type and amount of support by iteratively selecting a sequence of intervention options from a pre-defined set of components in response to each individual's time varying state. In this work, we explore the application of reinforcement learning methods to the problem of learning intervention option selection policies. We study the effect of context inference error and partial observability on the ability to learn effective policies. Our results show that the propagation of uncertainty from context inferences is critical to improving intervention efficacy as context uncertainty increases, while policy gradient algorithms can provide remarkable robustness to partially observed behavioral state information.
    
[^83]: 利用流式分析在金融和医疗保健中进行增量异常检测建模

    Incremental Outlier Detection Modelling Using Streaming Analytics in Finance & Health Care. (arXiv:2305.09907v1 [cs.LG])

    [http://arxiv.org/abs/2305.09907](http://arxiv.org/abs/2305.09907)

    本文利用在线异常检测算法建立了流式环境下的增量学习模型，在金融和医疗保健领域取得实际应用。

    

    本文构建了在线模型，该模型通过使用流环境下的在线异常检测算法进行增量构建。我们认识到应当使用流式模型来处理流式数据的高度必要性。本项目的目标是研究和分析适用于现实环境的流式模型的重要性。本文实现了各种异常检测算法，如One class支持向量机（OC-SVM）、孤立森林自适应滑动窗口方法（IForest ASD）、Exact Storm、基于角度的异常检测（ABOD）、局部异常因子（LOF）、KitNet、KNN ASD方法。并验证了上述构建模型在各种金融问题上的有效性和正确性，例如信用卡欺诈检测、流失预测、以太坊欺诈预测。此外，我们还分析了模型在健康预测问题上的表现，如心脏中风预测、糖尿病预测等。

    In this paper, we had built the online model which are built incrementally by using online outlier detection algorithms under the streaming environment. We identified that there is highly necessity to have the streaming models to tackle the streaming data. The objective of this project is to study and analyze the importance of streaming models which is applicable in the real-world environment. In this work, we built various Outlier Detection (OD) algorithms viz., One class Support Vector Machine (OC-SVM), Isolation Forest Adaptive Sliding window approach (IForest ASD), Exact Storm, Angle based outlier detection (ABOD), Local outlier factor (LOF), KitNet, KNN ASD methods. The effectiveness and validity of the above-built models on various finance problems such as credit card fraud detection, churn prediction, ethereum fraud prediction. Further, we also analyzed the performance of the models on the health care prediction problems such as heart stroke prediction, diabetes prediction and h
    
[^84]: 单隐层神经网络梯度流的ISS属性研究

    On the ISS Property of the Gradient Flow for Single Hidden-Layer Neural Networks with Linear Activations. (arXiv:2305.09904v1 [cs.LG])

    [http://arxiv.org/abs/2305.09904](http://arxiv.org/abs/2305.09904)

    过拟合如何影响梯度下降训练在梯度估计不确定时的稳健性，该研究研究了具有单个任意宽度的隐藏层和任意输入输出数量的线性神经网络。

    

    近期神经网络和机器学习的研究表明，使用比初始回归问题所需参数更多的参数可以导致更准确或更快收敛的模型-与经典统计学的信念相反。这种现象有时被称为“良性过拟合”，它引发了对过度参数化可能如何影响学习问题属性的问题。本文研究了过拟合对梯度下降训练稳健性的影响，当梯度估计不确定时，会自然产生这种不确定性，这种不确定性会由于从噪声数据或直接测量的梯度估计而产生。我们研究了一个具有单个任意宽度的隐藏层和任意数量的输入和输出的线性神经网络。本文解决了输入和输出为一维的情况，导出了足够的条件。

    Recent research in neural networks and machine learning suggests that using many more parameters than strictly required by the initial complexity of a regression problem can result in more accurate or faster-converging models -contrary to classical statistical belief. This phenomenon, sometimes known as ``benign overfitting'', raises questions regarding in what other ways might overparameterization affect the properties of a learning problem. In this work, we investigate the effects of overfitting on the robustness of gradient-descent training when subject to uncertainty on the gradient estimation. This uncertainty arises naturally if the gradient is estimated from noisy data or directly measured. Our object of study is a linear neural network with a single, arbitrarily wide, hidden layer and an arbitrary number of inputs and outputs. In this paper we solve the problem for the case where the input and output of our neural-network are one-dimensional, deriving sufficient conditions fo
    
[^85]: 噪声随机梯度下降的隐私损失可能会收敛，即使是对于非凸损失

    Privacy Loss of Noisy Stochastic Gradient Descent Might Converge Even for Non-Convex Losses. (arXiv:2305.09903v1 [cs.LG])

    [http://arxiv.org/abs/2305.09903](http://arxiv.org/abs/2305.09903)

    本文研究了DP-SGD，一种常用的噪声性SGD变体，发现其隐私损失呈指数收敛，不需要损失函数强凸或平滑假设。

    

    噪声随机梯度下降算法被广泛用于私密训练机器学习模型。但传统的隐私分析假定内部状态公开，导致随着迭代次数的增加隐私损失边界会无限增加。然而，最近的研究表明，如果内部状态保持隐藏，则隐私损失可能保持有限。本文解决了DP-SGD的问题，这是一种常用的包括梯度剪裁的噪声性SGD变体，以限制个体样本对训练过程的影响，在不需要损失函数强凸或平滑假设的情况下，我们发现经过投影的DP-SGD的隐私损失呈指数收敛。

    The Noisy-SGD algorithm is widely used for privately training machine learning models. Traditional privacy analyses of this algorithm assume that the internal state is publicly revealed, resulting in privacy loss bounds that increase indefinitely with the number of iterations. However, recent findings have shown that if the internal state remains hidden, then the privacy loss might remain bounded. Nevertheless, this remarkable result heavily relies on the assumption of (strong) convexity of the loss function. It remains an important open problem to further relax this condition while proving similar convergent upper bounds on the privacy loss. In this work, we address this problem for DP-SGD, a popular variant of Noisy-SGD that incorporates gradient clipping to limit the impact of individual samples on the training process. Our findings demonstrate that the privacy loss of projected DP-SGD converges exponentially fast, without requiring convexity or smoothness assumptions on the loss fu
    
[^86]: 基于预训练模型的等变小样本学习

    Equivariant Few-Shot Learning from Pretrained Models. (arXiv:2305.09900v1 [cs.LG])

    [http://arxiv.org/abs/2305.09900](http://arxiv.org/abs/2305.09900)

    本文提出了一种基于预训练模型的$\lambda$-\textit{equitune}方法，它使用\textit{重要性权重}$\lambda$对特征进行平均，可以显著提高等变小样本学习的表现。

    

    高效的迁移学习算法是基础模型在有限数据情况下在各种下游任务上取得成功的关键。最近的作品 \cite{basu2022equi} 和 \cite{kaba2022equivariance} 分别提出了使用从群变换输入得到的特征的群平均值（\textit{equitune}）和基于优化的方法来从不等变的神经网络获取等变输出。虽然 \cite{kaba2022equivariance} 只关注从头开始训练，但我们发现即使在良好的微调结果下，\textit{equitune} 在等变零样本任务上表现不佳。我们认为这是因为预训练模型为某些转换提供了更高质量的特征，而对其进行简单平均会产生不良影响。因此，我们提出了一种使用\textit{重要性权重}$\lambda$对特征进行平均的$\lambda$-\textit{equitune} 方法。这些权重是使用一个小型神经网络直接从数据中学习的，从而导致出色的零样本和微调结果。

    Efficient transfer learning algorithms are key to the success of foundation models on diverse downstream tasks even with limited data. Recent works of \cite{basu2022equi} and \cite{kaba2022equivariance} propose group averaging (\textit{equitune}) and optimization-based methods, respectively, over features from group-transformed inputs to obtain equivariant outputs from non-equivariant neural networks. While \cite{kaba2022equivariance} are only concerned with training from scratch, we find that equitune performs poorly on equivariant zero-shot tasks despite good finetuning results. We hypothesize that this is because pretrained models provide better quality features for certain transformations than others and simply averaging them is deleterious. Hence, we propose $\lambda$-\textit{equitune} that averages the features using \textit{importance weights}, $\lambda$s. These weights are learned directly from the data using a small neural network, leading to excellent zero-shot and finetuned 
    
[^87]: 补充分类器诱导的部分标签学习

    Complementary Classifier Induced Partial Label Learning. (arXiv:2305.09897v1 [cs.LG])

    [http://arxiv.org/abs/2305.09897](http://arxiv.org/abs/2305.09897)

    本文提出了一种利用补充标签诱导补充分类器的方法，该分类器与传统PLL分类器形成对抗关系，以消除候选标签集中的误报标签，并使用动态图协助消除歧义。实验结果表明该方法优于现有PLL方法。

    

    在部分标签学习（PLL）中，每个训练样本都与一组候选标签相关联，其中仅有一个是有效的。PLL的核心是消除候选标签中的歧义，以获得真实的标签。在消除歧义方面，现有的工作通常没有充分研究非候选标签集（也称为补充标签）的有效性，它准确地指示了一个不属于样本的标签集。在本文中，我们使用非候选标签来诱导一个补充分类器，它自然地形成对传统PLL分类器的对抗关系，以消除候选标签集中的误报标签。此外，我们假设特征空间和标签空间共享由动态图捕获的相同局部拓扑结构，并将其用于协助消除歧义。广泛的实验结果验证了该方法在4个控制UCI数据集和1个真实世界数据集上优于现有PLL方法。

    In partial label learning (PLL), each training sample is associated with a set of candidate labels, among which only one is valid. The core of PLL is to disambiguate the candidate labels to get the ground-truth one. In disambiguation, the existing works usually do not fully investigate the effectiveness of the non-candidate label set (a.k.a. complementary labels), which accurately indicates a set of labels that do not belong to a sample. In this paper, we use the non-candidate labels to induce a complementary classifier, which naturally forms an adversarial relationship against the traditional PLL classifier, to eliminate the false-positive labels in the candidate label set. Besides, we assume the feature space and the label space share the same local topological structure captured by a dynamic graph, and use it to assist disambiguation. Extensive experimental results validate the superiority of the proposed approach against state-of-the-art PLL methods on 4 controlled UCI data sets an
    
[^88]: SS-BSN:注意力盲点网络，用于自监督去噪与非局部自相似性

    SS-BSN: Attentive Blind-Spot Network for Self-Supervised Denoising with Nonlocal Self-Similarity. (arXiv:2305.09890v1 [cs.CV])

    [http://arxiv.org/abs/2305.09890](http://arxiv.org/abs/2305.09890)

    本文提出了一种自监督训练去噪的方法，其使用了一种新颖的自注意力模块——自相似性注意力（SS-Attention），可以捕捉非局部自相似性并解决性能上的限制。

    

    最近，有许多关于基于监督学习的图像去噪方法的研究，但这些方法依赖于大规模的噪声-干净图像对，这在实践中很难获得。为了解决这个限制问题，提出了一种可以只使用有噪声的图像进行训练的自监督训练的去噪方法。这些方法基于卷积神经网络（CNN），并展现了很好的性能。然而，基于CNN的方法没有考虑到传统方法中使用的非局部自相似性，这可能会导致性能上的限制。本文提出了自相似性注意力（SS-Attention），一种能够捕捉非局部自相似性并解决该问题的新型自注意力模块。我们的重点是以像素为基础设计一个轻量级的自注意力模块，而经典的自注意力模块则无法实现，因为其随着空间分辨率而呈二次递增的复杂度会导致实现困难。

    Recently, numerous studies have been conducted on supervised learning-based image denoising methods. However, these methods rely on large-scale noisy-clean image pairs, which are difficult to obtain in practice. Denoising methods with self-supervised training that can be trained with only noisy images have been proposed to address the limitation. These methods are based on the convolutional neural network (CNN) and have shown promising performance. However, CNN-based methods do not consider using nonlocal self-similarities essential in the traditional method, which can cause performance limitations. This paper presents self-similarity attention (SS-Attention), a novel self-attention module that can capture nonlocal self-similarities to solve the problem. We focus on designing a lightweight self-attention module in a pixel-wise manner, which is nearly impossible to implement using the classic self-attention module due to the quadratically increasing complexity with spatial resolution. F
    
[^89]: 简化大规模图上分布式神经网络训练：随机分割改善模型聚合

    Simplifying Distributed Neural Network Training on Massive Graphs: Randomized Partitions Improve Model Aggregation. (arXiv:2305.09887v1 [cs.LG])

    [http://arxiv.org/abs/2305.09887](http://arxiv.org/abs/2305.09887)

    本论文提出了一种简化的分布式 GNN 训练框架，只进行定期的模型聚合，并使用随机分割来隐式交换信息，实现了比现有方法更好的可扩展性、收敛速度和性能。

    

    分布式 GNN 的训练可以使我们学习超出单个机器存储和计算能力的大规模图（如社交和电子商务网络）。为了达到与集中式训练相当的性能，分布式框架专注于通过实例间通信或定期回退到集中式训练来最大限度地恢复跨实例节点的依赖关系，这需要额外的开销并限制了框架的可扩展性。本文提出了一个简化的分布式 GNN 训练框架，不需要以上昂贵的操作，具有比现有方法更好的可扩展性、收敛速度和性能。具体而言，我们的框架（1）组装独立的训练器，每个训练器异步地在训练图的本地部分上学习本地模型，（2）只进行定期的（基于时间的）模型聚合，以同步各个本地模型。经过理论分析支持，我们表明，随机图分割使得独立的训练器能够隐式地交换信息，而无需显式通信，因此定期聚合就足以收敛到与集中式训练相同的性能。我们的方法简单、高效，并且适用于各种 GNN 模型（例如 GCN、GAT 等），使其非常适用于实际的分布式 GNN 应用。

    Distributed training of GNNs enables learning on massive graphs (e.g., social and e-commerce networks) that exceed the storage and computational capacity of a single machine. To reach performance comparable to centralized training, distributed frameworks focus on maximally recovering cross-instance node dependencies with either communication across instances or periodic fallback to centralized training, which create overhead and limit the framework scalability. In this work, we present a simplified framework for distributed GNN training that does not rely on the aforementioned costly operations, and has improved scalability, convergence speed and performance over the state-of-the-art approaches. Specifically, our framework (1) assembles independent trainers, each of which asynchronously learns a local model on locally-available parts of the training graph, and (2) only conducts periodic (time-based) model aggregation to synchronize the local models. Backed by our theoretical analysis, 
    
[^90]: 基于线性优化的有符号子图编码方法用于链路符号预测

    A Signed Subgraph Encoding Approach via Linear Optimization for Link Sign Prediction. (arXiv:2305.09869v1 [cs.LG])

    [http://arxiv.org/abs/2305.09869](http://arxiv.org/abs/2305.09869)

    本文提出了一种名为SELO的链路符号预测模型，使用子图编码方法学习有向网络中的边嵌入。通过引入有符号子图编码方法，并使用线性优化方法将每个子图嵌入到似然矩阵中而非邻接矩阵中，该模型优于其他最先进的方法。

    

    本文研究了如何在有限的有符号数据中有效地推断链路的符号问题。我们提出了一个名为SELO的链路符号预测模型，该模型使用子图编码方法来学习有向网络中的边嵌入。特别地，我们通过线性优化方法引入了有符号子图编码方法，将每个子图嵌入到似然矩阵中而不是邻接矩阵中。在六个真实的有符号网络上进行了广泛的实验，并使用AUC、F1、micro-F1和Macro-F1作为评估指标。实验结果表明，所提出的SELO模型在所有四个评估指标上均优于其他最先进的方法。

    In this paper, we consider the problem of inferring the sign of a link based on limited sign data in signed networks. Regarding this link sign prediction problem, SDGNN (Signed Directed Graph Neural Networks) provides the best prediction performance currently to the best of our knowledge. In this paper, we propose a different link sign prediction architecture call SELO (Subgraph Encoding via Linear Optimization), which obtains overall leading prediction performances compared the state-of-the-art algorithm SDGNN. The proposed model utilizes a subgraph encoding approach to learn edge embeddings for signed directed networks. In particular, a signed subgraph encoding approach is introduced to embed each subgraph into a likelihood matrix instead of the adjacency matrix through a linear optimization method. Comprehensive experiments are conducted on six real-world signed networks with AUC, F1, micro-F1, and Macro-F1 as the evaluation metrics. The experiment results show that the proposed SEL
    
[^91]: 不确定最大熵原理

    The Principle of Uncertain Maximum Entropy. (arXiv:2305.09868v1 [cs.IT])

    [http://arxiv.org/abs/2305.09868](http://arxiv.org/abs/2305.09868)

    介绍了不确定最大熵原理，该原理可以处理模型元素不可观测的情况，并优于特定条件下的最大熵方法。同时将黑匣子机器学习模型的输出用作不确定机器熵框架的输入，性能得到了提高。

    

    最大熵原理在信息理论中的引入，为统计力学，机器学习和生态学等各个领域的发展做出了贡献。其得到的解决方案作为催化剂，促进研究人员将他们的经验观察映射到获取无偏模型，同时加深了对复杂系统和现象的理解。然而，在模型元素不直接可观测的情况下，例如存在噪声或眼部遮挡的情况下，标准最大熵方法可能会失败，因为它们无法匹配特征约束。在这里，我们展示了不确定最大熵原理作为一种方法，尽管存在任意噪声观察，它同时将所有可用信息编码，而且优于一些特定条件下的最大熵方法的准确度。此外，我们将黑匣子机器学习模型的输出用作不确定机器熵框架的输入，从而在与最大似然算法相比时建立了改进的性能。

    The principle of maximum entropy, as introduced by Jaynes in information theory, has contributed to advancements in various domains such as Statistical Mechanics, Machine Learning, and Ecology. Its resultant solutions have served as a catalyst, facilitating researchers in mapping their empirical observations to the acquisition of unbiased models, whilst deepening the understanding of complex systems and phenomena. However, when we consider situations in which the model elements are not directly observable, such as when noise or ocular occlusion is present, possibilities arise for which standard maximum entropy approaches may fail, as they are unable to match feature constraints. Here we show the Principle of Uncertain Maximum Entropy as a method that both encodes all available information in spite of arbitrarily noisy observations while surpassing the accuracy of some ad-hoc methods. Additionally, we utilize the output of a black-box machine learning model as input into an uncertain ma
    
[^92]: 利用语言模型用自然语言解释黑盒文本模块

    Explaining black box text modules in natural language with language models. (arXiv:2305.09863v1 [cs.AI])

    [http://arxiv.org/abs/2305.09863](http://arxiv.org/abs/2305.09863)

    本文介绍了一种名为Summarize and Score（SASC）的方法，该方法可以自动获取黑盒文本模块的自然语言解释以及解释可靠程度的分数。研究者们已经在合成模块和BERT模型中使用SASC，让我们可以解释模块的选择性，这对于增强大型语言模型的可解释性非常重要。

    

    大型语言模型已经证明在各种任务中具有出色的预测性能。然而，它们的快速增长和不透明性已经引起了对可解释性的需求。本文询问是否可以自动获取黑盒文本模块的自然语言解释。一个“文本模块”是将文本映射到标量连续值的任何函数，例如LLM内的子模块或大脑区域的拟合模型。“黑盒”表示我们只能访问模块的输入/输出。我们引入了Summarize and Score（SASC）方法，它接受文本模块并返回模块选择性的自然语言解释以及解释可靠程度的分数。我们在三个上下文中研究SASC。首先，我们在合成模块上评估SASC，并发现它经常恢复基本真相说明。其次，我们使用SASC来解释预训练BERT模型中的模块，使得检查BERT的模块成为可能。

    Large language models (LLMs) have demonstrated remarkable prediction performance for a growing array of tasks. However, their rapid proliferation and increasing opaqueness have created a growing need for interpretability. Here, we ask whether we can automatically obtain natural language explanations for black box text modules. A "text module" is any function that maps text to a scalar continuous value, such as a submodule within an LLM or a fitted model of a brain region. "Black box" indicates that we only have access to the module's inputs/outputs.  We introduce Summarize and Score (SASC), a method that takes in a text module and returns a natural language explanation of the module's selectivity along with a score for how reliable the explanation is. We study SASC in 3 contexts. First, we evaluate SASC on synthetic modules and find that it often recovers ground truth explanations. Second, we use SASC to explain modules found within a pre-trained BERT model, enabling inspection of the 
    
[^93]: Epsilon Sampling Rocks: 研究用于机器翻译最小贝叶斯风险解码的采样策略

    Epsilon Sampling Rocks: Investigating Sampling Strategies for \\Minimum Bayes Risk Decoding for Machine Translation. (arXiv:2305.09860v1 [cs.CL])

    [http://arxiv.org/abs/2305.09860](http://arxiv.org/abs/2305.09860)

    本文研究了用于机器翻译最小贝叶斯风险解码的不同采样策略，并发现了epsilon采样方式能够使得解码结果显著地优于其他所有已测试的采样方式和束搜索解码。

    

    机器翻译中的最小贝叶斯风险（MBR）解码已经显示出是一种强大的替代束搜索解码的方法，尤其是与基于神经网络的效用函数相结合时。然而，MBR解码的性能严重依赖于从模型中采样的方法和数量。本文探讨了用于MBR解码的不同采样方法对性能的影响。我们评估了一些流行的采样方法，例如祖先采样，核采样和top-k采样。基于我们对它们局限性的认识，我们尝试了最近提出的epsilon采样方法，该方法通过修剪所有小于epsilon的标记，以确保样本中的每个标记获得公平的概率质量。通过广泛的人类评估，我们证明了基于epsilon采样的MBR解码显著优于不仅是束搜索解码，而且还优于所有其他已测试的采样方法的MBR解码。

    Recent advances in machine translation (MT) have shown that Minimum Bayes Risk (MBR) decoding can be a powerful alternative to beam search decoding, especially when combined with neural-based utility functions. However, the performance of MBR decoding depends heavily on how and how many candidates are sampled from the model. In this paper, we explore how different sampling approaches for generating candidate lists for MBR decoding affect performance. We evaluate popular sampling approaches, such as ancestral, nucleus, and top-k sampling. Based on our insights into their limitations, we experiment with the recently proposed epsilon-sampling approach, which prunes away all tokens with a probability smaller than epsilon, ensuring that each token in a sample receives a fair probability mass. Through extensive human evaluations, we demonstrate that MBR decoding based on epsilon-sampling significantly outperforms not only beam search decoding, but also MBR decoding with all other tested samp
    
[^94]: 小型语言模型更适合作为黑匣子机器生成文本检测器

    Smaller Language Models are Better Black-box Machine-Generated Text Detectors. (arXiv:2305.09859v1 [cs.CL])

    [http://arxiv.org/abs/2305.09859](http://arxiv.org/abs/2305.09859)

    本文研究发现，小型语言模型更适用于作为通用文本检测器，可以更加精确地检测出机器生成的文本，而检测器和生成模型是否具有相同的架构或语料库并不会对检测性能产生显著影响。

    

    随着流畅的生成语言模型的出现，它们可以生成与人类写作的非常相似的令人信服的话语，因此区分一段文本是由机器生成的还是人类写作的变得更加具有挑战性和重要性，因为这样的模型可以用于传播错误信息、虚假新闻、虚假评论并模仿某些作者和人物。为此，已经提出了许多检测机器生成文本的方法。其中大部分方法需要访问目标模型的 logits，或需要可以从目标模型中进行采样的能力。其中一种黑匣子检测方法依赖于观察到生成文本在生成器的似然函数下是局部最优的，而人类写作的文本则不是。我们发现，总体而言，较小且部分训练的模型更适合作为通用文本检测器：它们可以更精确地检测来自小型和大型模型的生成文本。有趣的是，我们发现检测器和生成模型是否具有相同的架构或相同的语料库对检测性能没有显著影响。

    With the advent of fluent generative language models that can produce convincing utterances very similar to those written by humans, distinguishing whether a piece of text is machine-generated or human-written becomes more challenging and more important, as such models could be used to spread misinformation, fake news, fake reviews and to mimic certain authors and figures. To this end, there have been a slew of methods proposed to detect machine-generated text. Most of these methods need access to the logits of the target model or need the ability to sample from the target. One such black-box detection method relies on the observation that generated text is locally optimal under the likelihood function of the generator, while human-written text is not. We find that overall, smaller and partially-trained models are better universal text detectors: they can more precisely detect text generated from both small and larger models. Interestingly, we find that whether the detector and generat
    
[^95]: 知识图谱补全模型是少样本学习者：以 LLMS 在电商中的关系标注为例的经验研究

    Knowledge Graph Completion Models are Few-shot Learners: An Empirical Study of Relation Labeling in E-commerce with LLMs. (arXiv:2305.09858v1 [cs.IR])

    [http://arxiv.org/abs/2305.09858](http://arxiv.org/abs/2305.09858)

    本文通过对知识图谱中关系标注的实证研究，发现大型语言模型具有强大的学习能力以及在少量标记数据下预测产品类型之间关系的有效性。

    

    知识图谱在增强电子商务系统性能方面发挥着至关重要的作用，提供了关于实体及其关系的结构化信息，例如产品或产品类型之间的互补或替代关系，这些信息可以在推荐系统中利用。然而，由于电子商务领域的动态性和人力成本相关的原因，知识图谱中的关系标注仍然是一个具有挑战性的任务。最近，大型语言模型（LLM）的突破在许多自然语言处理任务中展示了出乎意料的结果。在本文中，我们进行了一个关于 LLM 在电子商务知识图谱中进行关系标注的实证研究，研究它们在自然语言方面强大的学习能力以及在有限标记数据下预测产品类型之间关系的有效性。我们评估了各种 LLM，包括 PaLM 和 GPT-3.5，在基准数据集上，证明它们能够达到与人类相当的关系性能水平。

    Knowledge Graphs (KGs) play a crucial role in enhancing e-commerce system performance by providing structured information about entities and their relationships, such as complementary or substitutable relations between products or product types, which can be utilized in recommender systems. However, relation labeling in KGs remains a challenging task due to the dynamic nature of e-commerce domains and the associated cost of human labor. Recently, breakthroughs in Large Language Models (LLMs) have shown surprising results in numerous natural language processing tasks. In this paper, we conduct an empirical study of LLMs for relation labeling in e-commerce KGs, investigating their powerful learning capabilities in natural language and effectiveness in predicting relations between product types with limited labeled data. We evaluate various LLMs, including PaLM and GPT-3.5, on benchmark datasets, demonstrating their ability to achieve competitive performance compared to humans on relation
    
[^96]: 简单易用：具有不可靠客户端的联邦学习容错性评估

    Keep It Simple: Fault Tolerance Evaluation of Federated Learning with Unreliable Clients. (arXiv:2305.09856v1 [cs.LG])

    [http://arxiv.org/abs/2305.09856](http://arxiv.org/abs/2305.09856)

    本文评估了具有不可靠客户端的联邦学习的容错性，研究表明相对较简单的FL算法在此情境下也能表现良好。

    

    作为一种新兴的人工智能方法，联邦学习（FL）可以在多个设备上进行分散模型训练，而不泄露本地训练数据。虽然已经有研究提出了提高FL容错性的方法，但现实应用中不可靠设备（例如掉线、错误配置、差数据质量）的真实影响尚未得到充分调查。我们精心选择了两个具有有限客户端的代表性实际分类问题，以更好地分析FL容错性。与直觉相反，简单的FL算法在存在不可靠客户端的情况下可以出奇地表现良好。

    Federated learning (FL), as an emerging artificial intelligence (AI) approach, enables decentralized model training across multiple devices without exposing their local training data. FL has been increasingly gaining popularity in both academia and industry. While research works have been proposed to improve the fault tolerance of FL, the real impact of unreliable devices (e.g., dropping out, misconfiguration, poor data quality) in real-world applications is not fully investigated. We carefully chose two representative, real-world classification problems with a limited numbers of clients to better analyze FL fault tolerance. Contrary to the intuition, simple FL algorithms can perform surprisingly well in the presence of unreliable clients.
    
[^97]: 选择性指导：引导扩散的所有去噪步骤都重要吗？

    Selective Guidance: Are All the Denoising Steps of Guided Diffusion Important?. (arXiv:2305.09847v1 [cs.LG])

    [http://arxiv.org/abs/2305.09847](http://arxiv.org/abs/2305.09847)

    该研究提出通过优化去噪步骤来减少稳定扩散（SD）引导式推理流程的复杂性，并证明后续迭代对优化的敏感度较低。优化去噪处理循环的最后20％或最后50％可以分别将推理时间减少8.2％或20.3％，同时对人眼几乎没有可察觉的变化。

    

    本研究考察了优化稳定扩散（SD）引导式推理流程的影响。我们建议通过将噪声计算限制为条件噪声并消除无条件噪声计算来优化某些去噪步骤，从而将目标迭代的复杂性降低50%。此外，我们证明SD的后续迭代对优化的敏感度较低，因此它们是应用建议优化的理想候选。我们的实验表明，优化去噪处理循环的最后20％会导致推理时间减少8.2％，同时对人眼几乎没有可察觉的变化。此外，我们发现将优化扩展到最后50％的迭代可以将推理时间减少约20.3％，同时生成视觉上令人满意的图像。

    This study examines the impact of optimizing the Stable Diffusion (SD) guided inference pipeline. We propose optimizing certain denoising steps by limiting the noise computation to conditional noise and eliminating unconditional noise computation, thereby reducing the complexity of the target iterations by 50%. Additionally, we demonstrate that later iterations of the SD are less sensitive to optimization, making them ideal candidates for applying the suggested optimization. Our experiments show that optimizing the last 20% of the denoising loop iterations results in an 8.2% reduction in inference time with almost no perceivable changes to the human eye. Furthermore, we found that by extending the optimization to 50% of the last iterations, we can reduce inference time by approximately 20.3%, while still generating visually pleasing images.
    
[^98]: 使用经验插值方法在深度神经网络中进行降维的注记

    A Note on Dimensionality Reduction in Deep Neural Networks using Empirical Interpolation Method. (arXiv:2305.09842v1 [cs.LG])

    [http://arxiv.org/abs/2305.09842](http://arxiv.org/abs/2305.09842)

    本文提出了一种名为DNN-EIM的算法来在监督机器学习中使用EIM算法有效地减少训练数据的维数。同时考虑了在分类和PDEs方面的应用，该算法可以为每个类别或EIM点设计并行的DNN，所需权重比传统方法少得多。

    

    经验插值方法（EIM）是一种有效估计参数化函数的技术。本文提出了一种名为DNN-EIM的算法，使用EIM算法在监督机器学习中有效地减少训练数据的维数。考虑了在数据科学（例如MNIST）和参数（以及时变）偏微分方程（PDE）方面的应用。对于分类，所提出的DNN是为每个类别并行训练。这种方法是顺序的，即可以添加新的类别，而不必重新训练网络。在PDE的情况下，为每个EIM点设计了一个DNN。同样，可以为每个EIM点并行训练这些网络。在所有情况下，与训练权重相比，并行网络所需的权重少于10倍。通过本文所提出的方法，在不牺牲准确性的情况下，显著缩短了训练时间。

    Empirical interpolation method (EIM) is a well-known technique to efficiently approximate parameterized functions. This paper proposes to use EIM algorithm to efficiently reduce the dimension of the training data within supervised machine learning. This is termed as DNN-EIM. Applications in data science (e.g., MNIST) and parameterized (and time-dependent) partial differential equations (PDEs) are considered. The proposed DNNs in case of classification are trained in parallel for each class. This approach is sequential, i.e., new classes can be added without having to retrain the network. In case of PDEs, a DNN is designed corresponding to each EIM point. Again, these networks can be trained in parallel, for each EIM point. In all cases, the parallel networks require fewer than ten times the number of training weights. Significant gains are observed in terms of training times, without sacrificing accuracy.
    
[^99]: 合作智能网络：泛化和扩展

    Coagent Networks: Generalized and Scaled. (arXiv:2305.09838v1 [cs.LG])

    [http://arxiv.org/abs/2305.09838](http://arxiv.org/abs/2305.09838)

    论文提出了一种强大而灵活的合作智能网络框架，可以异步计算网络不同部分、吸收反向传播不能使用的不可微组件、探索和/或时态抽象的分层网络，并使用高效算法进行分布式和并行学习。在基准问题上的模拟表明，该算法在性能上有显著提高。

    

    强化学习中的合作智能网络为任意随机神经网络的原则性学习提供了一种强大而灵活的框架。它不仅能够异步计算网络的不同部分，还能够吸收一些反向传播不能使用的不可微组件。此外，它还可以在动作空间级别以上进行探索，即可以设计为探索和/或时态抽象的分层网络。本文将协作理论和学习规则推广到任意网络拓扑，并通过使用分布式和并行学习的高效算法来扩展它。我们在基准问题上进行了模拟，证明了该算法在性能上的显著提高。

    Coagent networks for reinforcement learning (RL) [Thomas and Barto, 2011] provide a powerful and flexible framework for deriving principled learning rules for arbitrary stochastic neural networks. The coagent framework offers an alternative to backpropagation-based deep learning (BDL) that overcomes some of backpropagation's main limitations. For example, coagent networks can compute different parts of the network \emph{asynchronously} (at different rates or at different times), can incorporate non-differentiable components that cannot be used with backpropagation, and can explore at levels higher than their action spaces (that is, they can be designed as hierarchical networks for exploration and/or temporal abstraction). However, the coagent framework is not just an alternative to BDL; the two approaches can be blended: BDL can be combined with coagent learning rules to create architectures with the advantages of both approaches. This work generalizes the coagent theory and learning r
    
[^100]: 重新审视离线强化学习的极简方法

    Revisiting the Minimalist Approach to Offline Reinforcement Learning. (arXiv:2305.09836v1 [cs.LG])

    [http://arxiv.org/abs/2305.09836](http://arxiv.org/abs/2305.09836)

    这篇论文提出了一种名为ReBRAC的极简算法，它在TD3+BC方法的基础上整合了设计元素，通过对近期离线强化学习研究的回顾性分析，证明其在离线强化学习上的领先地位。

    

    近年来，离线强化学习取得了显着的进展，出现了许多具有不同复杂度的算法。虽然这些算法带来了显著的改进，但很多算法包含了看似微不足道的设计选择，这些选择对算法的有效性产生了影响，超出了核心算法的进步。然而，这些设计选择对于已有基线算法的影响尚未得到充分研究。在这项工作中，我们旨在通过对近期离线强化学习研究的回顾性分析，提出一种名为ReBRAC的极简算法，该算法在TD3+BC方法的基础上整合了这些设计元素。我们使用D4RL和V-D4RL基准测试评估了ReBRAC在51个具有自我感知和视觉状态空间的数据集上的性能，证明了其在不需要集成的方法中处于领先地位。为了进一步说明这些设计选择的有效性，我们进行了大规模消融研究和超参数敏感性分析，揭示了ReBRAC的成功源于其基于策略改进和评论家正则化的原则性设计选择。

    Recent years have witnessed significant advancements in offline reinforcement learning (RL), resulting in the development of numerous algorithms with varying degrees of complexity. While these algorithms have led to noteworthy improvements, many incorporate seemingly minor design choices that impact their effectiveness beyond core algorithmic advances. However, the effect of these design choices on established baselines remains understudied. In this work, we aim to bridge this gap by conducting a retrospective analysis of recent works in offline RL and propose ReBRAC, a minimalistic algorithm that integrates such design elements built on top of the TD3+BC method. We evaluate ReBRAC on 51 datasets with both proprioceptive and visual state spaces using D4RL and V-D4RL benchmarks, demonstrating its state-of-the-art performance among ensemble-free methods. To further illustrate the efficacy of these design choices, we perform a large-scale ablation study and hyperparameter sensitivity anal
    
[^101]: 自注意力层的拟态初始化

    Mimetic Initialization of Self-Attention Layers. (arXiv:2305.09828v1 [cs.CV])

    [http://arxiv.org/abs/2305.09828](http://arxiv.org/abs/2305.09828)

    本文介绍一种名为拟态初始化的方法，通过仅仅调整自注意力层的权重初始化，即可在视觉任务中大大提高Transformer的精度。

    

    在小规模数据集上训练Transformer十分困难。通常需要以大规模预训练模型作为起点。我们探索了这些预训练Transformer的权重（尤其是用于视觉任务），试图找到造成这种差异的原因。惊讶的是，我们发现仅仅通过初始化自注意力层的权重，使其“看起来”更像预训练模型，就能够更快且更高精度地训练普通Transformer，尤其是在像CIFAR-10和ImageNet分类这样的视觉任务上，我们的精度提高超过5％和4％。我们的初始化方案是闭式的、无需学习的、非常简单：我们将查询和键权重的乘积设置为近似于标识，将值和投影权重的乘积近似于负标识。由于这类似于我们在预训练Transformer中看到的模式，所以我们称为“拟态初始化”技术。

    It is notoriously difficult to train Transformers on small datasets; typically, large pre-trained models are instead used as the starting point. We explore the weights of such pre-trained Transformers (particularly for vision) to attempt to find reasons for this discrepancy. Surprisingly, we find that simply initializing the weights of self-attention layers so that they "look" more like their pre-trained counterparts allows us to train vanilla Transformers faster and to higher final accuracies, particularly on vision tasks such as CIFAR-10 and ImageNet classification, where we see gains in accuracy of over 5% and 4%, respectively. Our initialization scheme is closed form, learning-free, and very simple: we set the product of the query and key weights to be approximately the identity, and the product of the value and projection weights to approximately the negative identity. As this mimics the patterns we saw in pre-trained Transformers, we call the technique "mimetic initialization".
    
[^102]: 机器制造的媒体：监测虚假新闻和主流新闻网站上机器生成文章的动向。

    Machine-Made Media: Monitoring the Mobilization of Machine-Generated Articles on Misinformation and Mainstream News Websites. (arXiv:2305.09820v1 [cs.CY])

    [http://arxiv.org/abs/2305.09820](http://arxiv.org/abs/2305.09820)

    这篇论文研究了机器生成文章在虚假新闻和主流新闻网站的普及程度，发现虚假新闻网站上合成文章的使用速度比主流网站上更快。

    

    随着像ChatGPT这样的生成式大型语言模型（LLM）日益流行，越来越多的新闻网站开始利用它们生成文章。然而，这些语言模型不仅可能在声誉良好的网站上产生事实不准确的文章，而且不良新闻网站也可以利用这些LLM批量生产虚假信息。为了开始理解这一现象，我们提出了首个大规模研究合成文章在线新闻媒体中普及率的研究。为此，我们训练了一个基于DeBERTa的合成新闻检测器，并对3074个虚假新闻和主流新闻网站的超过1291万篇文章进行分类。我们发现，在2022年1月1日至2023年4月1日期间，合成新闻文章的相对数量在主流网站上增加了79.4％，而在虚假信息网站上增加了342％。分析ChatGPT发布的影响，使用中断时间序列，我们发现，虽然它的发布导致合成文章的使用显著增加，但虚假信息网站上的合成文章使用速度比主流网站上的快。

    With the increasing popularity of generative large language models (LLMs) like ChatGPT, an increasing number of news websites have begun utilizing them to generate articles. However, not only can these language models produce factually inaccurate articles on reputable websites but disreputable news sites can utilize these LLMs to mass produce misinformation. To begin to understand this phenomenon, we present one of the first large-scale studies of the prevalence of synthetic articles within online news media. To do this, we train a DeBERTa-based synthetic news detector and classify over 12.91 million articles from 3,074 misinformation and mainstream news websites. We find that between January 1, 2022 and April 1, 2023, the relative number of synthetic news articles increased by 79.4% on mainstream websites while increasing by 342% on misinformation sites. Analyzing the impact of the release of ChatGPT using an interrupted-time-series, we show that while its release resulted in a marked
    
[^103]: 一种无需训练的人像图像生成方法

    A Method for Training-free Person Image Picture Generation. (arXiv:2305.09817v1 [cs.CV])

    [http://arxiv.org/abs/2305.09817](http://arxiv.org/abs/2305.09817)

    本文提出一种无需训练的角色图像特征编码器模型，使得用户可以通过简单提供角色的图片，生成匹配期望的图像并调整各种细节，无需为每个个体/动画角色图像单独训练模型。

    

    当前最先进的扩散模型已经在生成图像方面表现出色。然而，这些图片都是单调的，大多数都是训练集中人物图片的分布结果，难以为固定数量的个体生成多张图片。如果要解决这个问题，通常必须通过微调模型。这意味着，如果要绘制每个个体/动画角色图像，必须对其进行训练，而这种训练的硬件和成本常常超出了普通用户的能力，而普通用户实际上占了人数最多的一部分。为了解决这个问题，本文提出的角色图像特征编码器模型使得用户可以通过简单提供角色的图片，使生成的图像中的角色与期望匹配。此外，在过程中可以使用提示调整各种细节。与传统的图像到图像模型不同，本文提出的角色图像特征编码器模型不依赖于特定数据集的训练，因此是一种无需训练的人像图像生成方法。

    The current state-of-the-art Diffusion model has demonstrated excellent results in generating images. However, the images are monotonous and are mostly the result of the distribution of images of people in the training set, making it challenging to generate multiple images for a fixed number of individuals. This problem can often only be solved by fine-tuning the training of the model. This means that each individual/animated character image must be trained if it is to be drawn, and the hardware and cost of this training is often beyond the reach of the average user, who accounts for the largest number of people. To solve this problem, the Character Image Feature Encoder model proposed in this paper enables the user to use the process by simply providing a picture of the character to make the image of the character in the generated image match the expectation. In addition, various details can be adjusted during the process using prompts. Unlike traditional Image-to-Image models, the Ch
    
[^104]: 关于transformer主动学习中数据集可迁移性的研究

    On Dataset Transferability in Active Learning for Transformers. (arXiv:2305.09807v1 [cs.LG])

    [http://arxiv.org/abs/2305.09807](http://arxiv.org/abs/2305.09807)

    本文研究了基于transformer的预训练语言模型的主动学习中数据集的可迁移性问题，发现具有相似获取序列的主动学习方法产生的数据集在不同模型之间具有高度的可迁移性。

    

    主动学习旨在通过查询对模型学习最有益的示例来减少标注成本。尽管已经证明了对于微调基于transformer的预训练语言模型（PLMs），主动学习的有效性，但不清楚一个模型中获得的主动学习收益在多大程度上适用于其他模型。我们考虑在文本分类中积极获取的数据集的可迁移性问题，并调查了使用主动学习构建的数据集在使用不同PLM训练时能否保持AL收益。我们将AL数据集的可迁移性与不同PLMs查询到的实例的相似性联系起来，并表明具有类似获取序列的AL方法生成的数据集非常具有可迁移性，无论使用哪种模型。此外，我们表明，获取序列的相似性更受到AL方法的选择而非模型的影响。

    Active learning (AL) aims to reduce labeling costs by querying the examples most beneficial for model learning. While the effectiveness of AL for fine-tuning transformer-based pre-trained language models (PLMs) has been demonstrated, it is less clear to what extent the AL gains obtained with one model transfer to others. We consider the problem of transferability of actively acquired datasets in text classification and investigate whether AL gains persist when a dataset built using AL coupled with a specific PLM is used to train a different PLM. We link the AL dataset transferability to the similarity of instances queried by the different PLMs and show that AL methods with similar acquisition sequences produce highly transferable datasets regardless of the models used. Additionally, we show that the similarity of acquisition sequences is influenced more by the choice of the AL method than the choice of the model.
    
[^105]: 应用控制李亚普诺夫屏障函数的强化学习安全机器人控制

    Reinforcement Learning for Safe Robot Control using Control Lyapunov Barrier Functions. (arXiv:2305.09793v1 [cs.RO])

    [http://arxiv.org/abs/2305.09793](http://arxiv.org/abs/2305.09793)

    本研究利用控制李亚普诺夫屏障函数及LBAC算法，提出了一种模型无关的强化学习方法，实现了基于数据的安全性和可达性条件下机器人控制。在实际2D四旋翼导航任务中验证该方法的有效性，优于其他模型无关强化学习方法。

    

    当面对复杂的机器人控制任务时，强化学习（RL）展现了优异的性能。然而，由于缺乏强大的安全保障，其在物理机器人上的广泛应用受到了限制。为了克服这一挑战，本文探讨了控制李亚普诺夫屏障函数（CLBF），仅基于数据分析安全性和可达性，而无需明确使用动态模型。我们还提出了LyapunovBarrierActor-Critic（LBAC）算法，这是一种基于模型的强化学习算法，用于寻找满足基于数据的安全和可达性条件的控制器。我们通过仿真和真实机器人控制实验，即2D四旋翼导航任务，展示了所提出方法的有效性。实验结果表明，此方法在可达性和安全性方面的效果优于其他基于模型的强化学习方法。

    Reinforcement learning (RL) exhibits impressive performance when managing complicated control tasks for robots. However, its wide application to physical robots is limited by the absence of strong safety guarantees. To overcome this challenge, this paper explores the control Lyapunov barrier function (CLBF) to analyze the safety and reachability solely based on data without explicitly employing a dynamic model. We also proposed the Lyapunov barrier actor-critic (LBAC), a model-free RL algorithm, to search for a controller that satisfies the data-based approximation of the safety and reachability conditions. The proposed approach is demonstrated through simulation and real-world robot control experiments, i.e., a 2D quadrotor navigation task. The experimental findings reveal this approach's effectiveness in reachability and safety, surpassing other model-free RL methods.
    
[^106]: 基于分数的算子 Newton 方法用于测量运输

    A score-based operator Newton method for measure transport. (arXiv:2305.09792v1 [math.ST])

    [http://arxiv.org/abs/2305.09792](http://arxiv.org/abs/2305.09792)

    本文提出一种新的基于分数的算子Newton方法，可以迭代构造一个易处理的原概率测度，该方法可以在满足目标分数光滑性假设下，实现快速收敛性。

    

    概率测度的运输是统计学和机器学习中许多核心任务的基础，从变分推理到生成建模。一个典型的目标是将一个感兴趣的目标概率测度表示为通过学习的映射将一个易处理的原概率测度推向前面。我们提出了一种新的构建这样一个运输映射的方法，给出了评估目标分布分数的能力。具体而言，我们将该映射特征化为一个无穷维的分数残差算子的零，并推导出一种迭代构造这样一个零的牛顿类型方法。通过调用偏微分方程的经典椭圆正则性理论，我们证明了这些迭代的收敛性，并表明在目标分数光滑性假设下，这种构造具有快速收敛性。我们方法的一个关键元素是将基本的牛顿方法推广到无穷维算子，其他形式的无穷维算子已经出现在非线性 PDE 中。

    Transportation of probability measures underlies many core tasks in statistics and machine learning, from variational inference to generative modeling. A typical goal is to represent a target probability measure of interest as the push-forward of a tractable source measure through a learned map. We present a new construction of such a transport map, given the ability to evaluate the score of the target distribution. Specifically, we characterize the map as a zero of an infinite-dimensional score-residual operator and derive a Newton-type method for iteratively constructing such a zero. We prove convergence of these iterations by invoking classical elliptic regularity theory for partial differential equations (PDE) and show that this construction enjoys rapid convergence, under smoothness assumptions on the target score. A key element of our approach is a generalization of the elementary Newton method to infinite-dimensional operators, other forms of which have appeared in nonlinear PDE
    
[^107]: 分子形态对比预训练提高分子表示迁移能力

    Molecule-Morphology Contrastive Pretraining for Transferable Molecular Representation. (arXiv:2305.09790v1 [q-bio.BM])

    [http://arxiv.org/abs/2305.09790](http://arxiv.org/abs/2305.09790)

    本文提出了Molecule-Morphology Contrastive Pretraining (MoCoP)框架，用于学习分子图形和细胞形态的多模态表示。实验结果表明，MoCoP可以提高图神经网络在分子属性预测任务上的表现，具有良好的实用性。

    

    过去十年中，基于图像的分析技术因其在目标鉴定、作用机制推断和测定发展中的应用而越来越受欢迎。这些技术产生了大量细胞形态的数据集，通常用于研究小分子干扰物的效果。本文通过引入Molecule-Morphology Contrastive Pretraining (MoCoP)，提出了一种学习分子图形和细胞形态的多模态表示的框架，将这样的数据集的影响扩展到了改进量化结构-活性关系 (QSAR) 模型。使用来自JUMP-CP联盟的数据，将MoCoP扩展到了约100K的分子和约600K的形态文件，结果表明，MoCoP在ChEMBL20上的分子属性预测任务中，始终提高了图神经网络 (GNNs) 的表现，且在内部 GSK药代动力学数据和其他数据集上具有良好的实用性。

    Image-based profiling techniques have become increasingly popular over the past decade for their applications in target identification, mechanism-of-action inference, and assay development. These techniques have generated large datasets of cellular morphologies, which are typically used to investigate the effects of small molecule perturbagens. In this work, we extend the impact of such dataset to improving quantitative structure-activity relationship (QSAR) models by introducing Molecule-Morphology Contrastive Pretraining (MoCoP), a framework for learning multi-modal representation of molecular graphs and cellular morphologies. We scale MoCoP to approximately 100K molecules and 600K morphological profiles using data from the JUMP-CP Consortium and show that MoCoP consistently improves performances of graph neural networks (GNNs) on molecular property prediction tasks in ChEMBL20 across all dataset sizes. The pretrained GNNs are also evaluated on internal GSK pharmacokinetic data and s
    
[^108]: 边缘智能与自动引导车辆控制的协同设计

    Codesign of Edge Intelligence and Automated Guided Vehicle Control. (arXiv:2305.09788v1 [cs.CV])

    [http://arxiv.org/abs/2305.09788](http://arxiv.org/abs/2305.09788)

    本文介绍了一种自主引导车辆（AGV）控制、边缘智能和人类输入的和谐设计，以实现工业环境中的自主运输，其核心技术是通过无线网络连接人工智能（AI）和AGV实现人机协同。

    

    本文介绍了一种自主引导车辆（AGV）控制、边缘智能和人类输入的和谐设计，以实现工业环境中的自主运输。该AGV具有在源和目标之间导航并拾取/放置物品的能力。人类输入隐含地提供了目的地和准确的卸货点的偏好，这些偏好来自于网络边缘的人工智能（AI）模块，并通过无线网络与AGV共享。演示表明，所提出的硬件、软件和AI设计的综合设计达到了技术成熟度水平（TRL）4-5的范围。

    This work presents a harmonic design of autonomous guided vehicle (AGV) control, edge intelligence, and human input to enable autonomous transportation in industrial environments. The AGV has the capability to navigate between a source and destinations and pick/place objects. The human input implicitly provides preferences of the destination and exact drop point, which are derived from an artificial intelligence (AI) module at the network edge and shared with the AGV over a wireless network. The demonstration indicates that the proposed integrated design of hardware, software, and AI design achieve a technology readiness level (TRL) of range 4-5
    
[^109]: 应用深度学习求解和估计宏观金融动态模型

    Deep Learning for Solving and Estimating Dynamic Macro-Finance Models. (arXiv:2305.09783v1 [q-fin.CP])

    [http://arxiv.org/abs/2305.09783](http://arxiv.org/abs/2305.09783)

    该论文介绍了一种利用深度学习同时求解和估计金融经济中经典的连续时间一般均衡模型的方法，并在产业动态和带有金融摩擦的宏观经济模型两个样例中展示了其优点，具有多种用途。

    

    我们开发了一种利用深度学习同时解决和估计金融经济中经典的连续时间一般均衡模型的方法。我们在两个样例中展示了我们的方法：（1）企业的产业动态和（2）带有金融摩擦的宏观经济模型。通过这些应用，我们展示了我们的方法的优点：通用性、同时求解和估计、运用最先进的机器学习技术和处理大规模状态空间的能力。该方法具有多种用途。

    We develop a methodology that utilizes deep learning to simultaneously solve and estimate canonical continuous-time general equilibrium models in financial economics. We illustrate our method in two examples: (1) industrial dynamics of firms and (2) macroeconomic models with financial frictions. Through these applications, we illustrate the advantages of our method: generality, simultaneous solution and estimation, leveraging the state-of-art machine-learning techniques, and handling large state space. The method is versatile and can be applied to a vast variety of problems.
    
[^110]: SpecInfer：利用推测推断和令牌树验证加速生成式大语言模型的服务

    SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification. (arXiv:2305.09781v1 [cs.CL])

    [http://arxiv.org/abs/2305.09781](http://arxiv.org/abs/2305.09781)

    SpecInfer是一种LLM服务系统，通过利用推测推断和令牌树验证来加速生成式大语言模型的推断过程，显著减少了为它们提供服务所需的端到端延迟和计算要求，同时确保模型质量。

    

    由于生成式大语言模型（LLMs）需要高计算和内存需求，因此快速和廉价地为它们提供服务是具有挑战性的。本文介绍SpecInfer，一个LLM服务系统，它利用推测推断和令牌树验证加速生成式LLM推断。SpecInfer背后的关键是将各种小型语言模型进行集体提升调整，共同预测LLM的输出； 预测结果组织成一个令牌树，其中每个节点都表示候选令牌序列。通过一种新颖的基于树的并行解码机制，以LMM作为令牌树验证器来验证令牌树所代表的所有候选令牌序列的正确性。SpecInfer使用LLM作为令牌树验证器，而不是增量解码器，从而显著减少了为生成式LLM提供服务所需的端到端延迟和计算要求，同时可确保模型质量。

    The high computational and memory requirements of generative large language models (LLMs) make it challenging to serve them quickly and cheaply. This paper introduces SpecInfer, an LLM serving system that accelerates generative LLM inference with speculative inference and token tree verification. A key insight behind SpecInfer is to combine various collectively boost-tuned small language models to jointly predict the LLM's outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is verified by the LLM in parallel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree verifier instead of an incremental decoder, which significantly reduces the end-to-end latency and computational requirement for serving generative LLMs while provably preserving model quality.
    
[^111]: 一种可扩展的Walsh-Hadamard正则化器，以克服神经网络的低阶谱偏差

    A Scalable Walsh-Hadamard Regularizer to Overcome the Low-degree Spectral Bias of Neural Networks. (arXiv:2305.09779v1 [cs.LG])

    [http://arxiv.org/abs/2305.09779](http://arxiv.org/abs/2305.09779)

    本文提出了一种新型的可扩展Walsh-Hadamard正则化器，可避免神经网络学习低阶频率以及误识别低阶频率，从而提高了神经网络的泛化性能。

    

    尽管神经网络具有学习任意函数的能力，但通过梯度下降训练的模型常常表现出对“更简单”函数的偏好。本文通过傅里叶（Walsh-Hadamard）变换，从离散（零一）输入的神经网络的角度探讨了简单性的概念，其中可以通过傅里叶系数的“阶”来捕捉简单性概念。我们实证表明神经网络有学习较低阶频率的趋势。我们展示了这种谱偏差向较简单特征的趋势实际上会损害神经网络在真实世界数据集上的泛化能力。为了解决这个问题，我们提出了一种新的可扩展的功能正则化方案，以帮助神经网络学习更高的阶频率。我们的正则化器还有助于避免对低阶频率的错误识别，从而进一步提高了泛化能力。我们在计算机视觉、自然语言处理和语音识别中应用各种神经网络架构进行分类任务的广泛评估。我们的实验结果表明，我们的正则化器在低数据量环境下显著提高了泛化性能。

    Despite the capacity of neural nets to learn arbitrary functions, models trained through gradient descent often exhibit a bias towards ``simpler'' functions. Various notions of simplicity have been introduced to characterize this behavior. Here, we focus on the case of neural networks with discrete (zero-one) inputs through the lens of their Fourier (Walsh-Hadamard) transforms, where the notion of simplicity can be captured through the \emph{degree} of the Fourier coefficients. We empirically show that neural networks have a tendency to learn lower-degree frequencies. We show how this spectral bias towards simpler features can in fact \emph{hurt} the neural network's generalization on real-world datasets. To remedy this we propose a new scalable functional regularization scheme that aids the neural network to learn higher degree frequencies. Our regularizer also helps avoid erroneous identification of low-degree frequencies, which further improves generalization. We extensively evaluat
    
[^112]: BSGAN：一种新的不平衡模式识别过采样技术

    BSGAN: A Novel Oversampling Technique for Imbalanced Pattern Recognitions. (arXiv:2305.09777v1 [cs.LG])

    [http://arxiv.org/abs/2305.09777](http://arxiv.org/abs/2305.09777)

    本论文提出了一种名为BSGAN的混合过采样技术，旨在缓解现有边界SMOTE过采样技术中存在的过采样后发生的边缘化问题，并生成更多多样性数据以提高模型的预测准确性。

    

    类不平衡问题（CIP）是开发无偏机器学习（ML）模型进行预测的潜在挑战之一。CIP发生在两个或多个类之间的数据样本未等分布时。边界合成少数样本过采样技术（SMOTE）是一种用于通过过采样少数（有限）样本来平衡不平衡数据的方法。现有的边界SMOTE的潜在缺点是它侧重于位于边缘点的数据样本并更加关注极端观测值，最终限制了过采样后更多不同样本的创造，这几乎是大多数边界SMOTE的基于过采样策略的情况。因此，过采样后会发生边缘化。为了解决这些问题，在本文中，我们提出了一种混合过采样技术，通过结合边界SMOTE和生成对抗网络的力量来生成更多多样性数据。

    Class imbalanced problems (CIP) are one of the potential challenges in developing unbiased Machine Learning (ML) models for predictions. CIP occurs when data samples are not equally distributed between the two or multiple classes. Borderline-Synthetic Minority Oversampling Techniques (SMOTE) is one of the approaches that has been used to balance the imbalance data by oversampling the minor (limited) samples. One of the potential drawbacks of existing Borderline-SMOTE is that it focuses on the data samples that lay at the border point and gives more attention to the extreme observations, ultimately limiting the creation of more diverse data after oversampling, and that is the almost scenario for the most of the borderline-SMOTE based oversampling strategies. As an effect, marginalization occurs after oversampling. To address these issues, in this work, we propose a hybrid oversampling technique by combining the power of borderline SMOTE and Generative Adversarial Network to generate mor
    
[^113]: OpenVR：操作的远程控制

    OpenVR: Teleoperation for Manipulation. (arXiv:2305.09765v1 [cs.RO])

    [http://arxiv.org/abs/2305.09765](http://arxiv.org/abs/2305.09765)

    该论文提出了一种基于虚拟现实技术的远程控制方法，旨在解决演示文件的质量不足与收集困难的问题，并实现了代码开源、硬件易得、易于修改和使用。

    

    在机器人领域，高质量的演示文件是许多控制流程不可或缺的一部分。然而，收集高质量的演示轨迹仍然费时费力，往往导致演示的数量成为性能瓶颈。为了解决这个问题，我们提出了一种虚拟现实（VR）远程控制方法，使用Oculus VR头戴式显示设备对Franka Emika Panda机器人进行远程操控。虽然还存在其他VR远程控制方法，但我们的代码是开源的，适用于易得的消费级硬件，易于修改，对实验设备设置不具有特定的要求，简单易用。

    Across the robotics field, quality demonstrations are an integral part of many control pipelines. However, collecting high-quality demonstration trajectories remains time-consuming and difficult, often resulting in the number of demonstrations being the performance bottleneck. To address this issue, we present a method of Virtual Reality (VR) Teleoperation that uses an Oculus VR headset to teleoperate a Franka Emika Panda robot. Although other VR teleoperation methods exist, our code is open source, designed for readily available consumer hardware, easy to modify, agnostic to experimental setup, and simple to use.
    
[^114]: 评估用于液体氩探测器低能物理的Few-Hits机器学习分类算法

    Assessment of few-hits machine learning classification algorithms for low energy physics in liquid argon detectors. (arXiv:2305.09744v1 [physics.ins-det])

    [http://arxiv.org/abs/2305.09744](http://arxiv.org/abs/2305.09744)

    本文评估了在液体氩探测器低能物理中使用Few-Hits机器学习分类算法的效果，证明在单比特与双比特事件的分类问题上，卷积神经网络和Transformer-Encoder方法优于传统算法，并针对DUNE Phase II探测器优化了探测器参数。

    

    在低能区域，大型液体氩TPCs的物理潜力仍未充分利用，因为Few-Hits事件所编码的信息很难被传统分类算法利用。机器学习技术在这些类型的分类问题中表现最佳。本文评估了它们在传统（确定性）算法中的表现。我们证明，卷积神经网络（CNN）和Transformer-Encoder方法在低能物理中最具挑战性的分类问题（单比特与双比特事件）中优于确定性算法。我们讨论了Transformer-Encoder方法相对于CNN的优缺点，并利用这些方法优化了探测器参数，重点关注DUNE Phase II探测器（"机会模块"）。

    The physics potential of massive liquid argon TPCs in the low-energy regime is still to be fully reaped because few-hits events encode information that can hardly be exploited by conventional classification algorithms. Machine learning (ML) techniques give their best in these types of classification problems. In this paper, we evaluate their performance against conventional (deterministic) algorithms. We demonstrate that both Convolutional Neural Networks (CNN) and Transformer-Encoder methods outperform deterministic algorithms in one of the most challenging classification problems of low-energy physics (single- versus double-beta events). We discuss the advantages and pitfalls of Transformer-Encoder methods versus CNN and employ these methods to optimize the detector parameters, with an emphasis on the DUNE Phase II detectors ("Module of Opportunity").
    
[^115]: CQural：一种基于混合CNN的量子持续机器学习架构

    CQural: A Novel CNN based Hybrid Architecture for Quantum Continual Machine Learning. (arXiv:2305.09738v1 [cs.LG])

    [http://arxiv.org/abs/2305.09738](http://arxiv.org/abs/2305.09738)

    本文展示了一种基于混合CNN的量子持续机器学习架构，可以通过解释哪些特征对于分类最重要来避免遗忘，并声称如果使用这些解释来训练模型，则会获得更好的性能。

    

    在增量式学习中训练机器学习模型不仅很重要，而且是实现人工通用智能的有效方法。然而，当前的神经网络模型在持续学习方面很容易出现灾难性遗忘的问题。许多研究人员提出了许多技术来减少神经网络的遗忘影响，但是所有的技术都是在经典学习上研究的，很少有人关注机器学习模型结构的改变。在本研究中，我们展示了使用新的混合经典 - 量子神经网络可以避免持续学习中的灾难性遗忘，并解释了哪些功能对于分类最重要。此外，我们还声称如果使用这些解释来训练模型，则会获得更好的性能。

    Training machine learning models in an incremental fashion is not only important but also an efficient way to achieve artificial general intelligence. The ability that humans possess of continuous or lifelong learning helps them to not forget previously learned tasks. However, current neural network models are prone to catastrophic forgetting when it comes to continual learning. Many researchers have come up with several techniques in order to reduce the effect of forgetting from neural networks, however, all techniques are studied classically with a very less focus on changing the machine learning model architecture. In this research paper, we show that it is not only possible to circumvent catastrophic forgetting in continual learning with novel hybrid classical-quantum neural networks, but also explains what features are most important to learn for classification. In addition, we also claim that if the model is trained with these explanations, it tends to give better performance and
    
[^116]: ADDSL: 基于标注的丹麦手语的手势检测与识别

    ADDSL: Hand Gesture Detection and Sign Language Recognition on Annotated Danish Sign Language. (arXiv:2305.09736v1 [cs.CV])

    [http://arxiv.org/abs/2305.09736](http://arxiv.org/abs/2305.09736)

    本研究引入了一个新的丹麦手语注释数据集（ADDSL）并使用该数据集训练了一个基于YOLOv5的手势检测和字母数字识别模型，准确率最高可达92%。与同领域现有工作相比，该模型更高效和准确。

    

    长期以来，将手势检测并将其识别为字母或数字一直是一项具有挑战性的任务。这给残障人士带来了沟通障碍。本文介绍了一个新的数据集，即丹麦手语注释数据集（ADDSL）。使用开源工具LabelImg在YOLO格式中制作了数据集的注释。利用此数据集，使用CSP-DarkNet53骨干和YOLOv3头的单阶段目标检测器模型（YOLOv5）通过每类仅使用七个独特的图像（不进行数据增强）来训练，以识别字母（A-Z）和数字（0-9）。训练五个模型，共350个周期，得到每张图像的平均推断时间为9.02ms，与之前的研究相比，最佳准确率为92%。我们的结果表明，修改后的模型比同领域的现有工作更高效和准确。我们模型的代码库可在GitHub存储库https://github.com/s4nyam/pvt-addsl 上获得。

    For a long time, detecting hand gestures and recognizing them as letters or numbers has been a challenging task. This creates communication barriers for individuals with disabilities. This paper introduces a new dataset, the Annotated Dataset for Danish Sign Language (ADDSL). Annota-tions for the dataset were made using the open-source tool LabelImg in the YOLO format. Using this dataset, a one-stage ob-ject detector model (YOLOv5) was trained with the CSP-DarkNet53 backbone and YOLOv3 head to recognize letters (A-Z) and numbers (0-9) using only seven unique images per class (without augmen-tation). Five models were trained with 350 epochs, resulting in an average inference time of 9.02ms per image and a best accu-racy of 92% when compared to previous research. Our results show that modified model is efficient and more accurate than existing work in the same field. The code repository for our model is available at the GitHub repository https://github.com/s4nyam/pvt-addsl.
    
[^117]: 在语境中学习：“学习”语境中的任务识别和任务学习的区分。

    What In-Context Learning "Learns" In-Context: Disentangling Task Recognition and Task Learning. (arXiv:2305.09731v1 [cs.CL])

    [http://arxiv.org/abs/2305.09731](http://arxiv.org/abs/2305.09731)

    本研究通过任务识别和任务学习两种方式表征了ICL利用演示的方式，发现LLMs利用不同机制进行任务的解决，TR主要利用先验知识，而TL则具备学习新的输入-标签映射的能力。

    

    大型语言模型通过利用语境中的学习来解决只有少数演示的任务，但其机制尚未得到很好的理解。一些研究表明LLMs仅回忆来自预训练的已学概念，而其他研究则暗示ICL执行演示的隐含学习。本文通过任务识别(TR)和任务学习(TL)两种方式表征了ICL利用演示的方式。我们使用各种分类数据集和三个LLM系列（GPT-3、LLaMA和OPT）进行控制实验，在ICL中区分TR和TL的角色。我们发现：（1）模型只使用TR就能取得非平凡的性能，TR不会随着更大的模型或更多的演示而进一步改善；（2）LLMs能够通过TL学习新的输入-标签映射，而TR则主要利用预先训练的先验知识。

    Large language models (LLMs) exploit in-context learning (ICL) to solve tasks with only a few demonstrations, but its mechanisms are not yet well-understood. Some works suggest that LLMs only recall already learned concepts from pre-training, while others hint that ICL performs implicit learning over demonstrations. We characterize two ways through which ICL leverages demonstrations. Task recognition (TR) captures the extent to which LLMs can recognize a task through demonstrations -- even without ground-truth labels -and apply their pre-trained priors, whereas task learning (TL) is the ability to capture new input-label mappings unseen in pre-training. Using a wide range of classification datasets and three LLM families (GPT-3, LLaMA and OPT), we design controlled experiments to disentangle the roles of TR and TL in ICL. We show that (1) models can achieve non-trivial performance with only TR, and TR does not further improve with larger models or more demonstrations; (2) LLMs acquir
    
[^118]: FedHGN：异构图神经网络的联邦学习框架

    FedHGN: A Federated Framework for Heterogeneous Graph Neural Networks. (arXiv:2305.09729v1 [cs.LG])

    [http://arxiv.org/abs/2305.09729](http://arxiv.org/abs/2305.09729)

    FedHGN是一种用于异构图神经网络的联邦学习框架，它采用模式权重解耦和系数对齐技术，使得不同客户端可以共享知识而不泄露隐私，相比于现有方法表现更加优秀。

    

    与传统GNN相比，异构图神经网络（HGNN）可以更有效地从类型化和关系化图数据中学习。由于隐私法规（例如GDPR），实际应用中的训练数据往往很少，而使用更大的参数空间可能需要更多的训练数据。联邦图学习（FGL）使多个客户端共同训练GNN而不共享本地数据。然而，现有的FGL方法主要集中在同构GNN或知识图嵌入上；很少考虑异构图和HGNN。在联邦异构图学习中，客户端可能拥有私有图模式，尝试定义全局HGNN模型的传统FL/FGL方法会侵犯模式隐私。为了解决这些挑战，我们提出了FedHGN，一种新颖的HGNN FGL框架。FedHGN采用模式权重解耦来实现独立于模式的知识共享，并采用系数对齐来稳定训练过程和提高HGNN泛化能力。我们在合成和现实数据集上进行了广泛的实验，证明了FedHGN相对于现有的最先进方法的有效性。

    Heterogeneous graph neural networks (HGNNs) can learn from typed and relational graph data more effectively than conventional GNNs. With larger parameter spaces, HGNNs may require more training data, which is often scarce in real-world applications due to privacy regulations (e.g., GDPR). Federated graph learning (FGL) enables multiple clients to train a GNN collaboratively without sharing their local data. However, existing FGL methods mainly focus on homogeneous GNNs or knowledge graph embeddings; few have considered heterogeneous graphs and HGNNs. In federated heterogeneous graph learning, clients may have private graph schemas. Conventional FL/FGL methods attempting to define a global HGNN model would violate schema privacy. To address these challenges, we propose FedHGN, a novel and general FGL framework for HGNNs. FedHGN adopts schema-weight decoupling to enable schema-agnostic knowledge sharing and employs coefficients alignment to stabilize the training process and improve HGNN
    
[^119]: 随机边编码：大型标记图的一次性Bits-Back编码

    Random Edge Coding: One-Shot Bits-Back Coding of Large Labeled Graphs. (arXiv:2305.09705v1 [cs.LG])

    [http://arxiv.org/abs/2305.09705](http://arxiv.org/abs/2305.09705)

    随机边编码是一种压缩大型标记图的一次性方法，它使用bits-back编码从边缘列表中无替换地对边缘和顶点进行采样，并在随机图模型下实现了最优性。

    

    我们提出了一种压缩大型标记图的一次性方法，称为随机边编码。当与基于Pólya's Urn的无参数模型配对使用时，最坏情况的计算和内存复杂度随观察到的边数几乎线性和线性地缩放，使其在稀疏图上高效，并且仅需要整数算术。我们方法的关键在于bits-back编码，它被用于从边缘列表中无替换地对边缘和顶点进行采样，以保留图的结构。在一类随机图模型下证明了最优性，该模型对排列的边和边内顶点的排列具有不变性。实验表明，随机边编码可以在现实世界的网络数据集上实现具有竞争力的压缩性能，并且可以扩展到具有数百万个节点和边的图形。

    We present a one-shot method for compressing large labeled graphs called Random Edge Coding. When paired with a parameter-free model based on P\'olya's Urn, the worst-case computational and memory complexities scale quasi-linearly and linearly with the number of observed edges, making it efficient on sparse graphs, and requires only integer arithmetic. Key to our method is bits-back coding, which is used to sample edges and vertices without replacement from the edge-list in a way that preserves the structure of the graph. Optimality is proven under a class of random graph models that are invariant to permutations of the edges and of vertices within an edge. Experiments indicate Random Edge Coding can achieve competitive compression performance on real-world network datasets and scales to graphs with millions of nodes and edges.
    
[^120]: 基于因果关系解释的扩散变分图神经网络用于时空预测

    Dynamic Causal Explanation Based Diffusion-Variational Graph Neural Network for Spatio-temporal Forecasting. (arXiv:2305.09703v1 [cs.LG])

    [http://arxiv.org/abs/2305.09703](http://arxiv.org/abs/2305.09703)

    本文提出了一种新颖的基于因果关系解释的扩散变分图神经网络，用于时空预测，在动态图构建上考虑了邻居节点之间的因果关系和不确定性，解决了动态图算法的可解释性和稳定性问题。

    

    图神经网络，特别是动态图神经网络，已经成为时空预测问题中的研究热点。虽然许多动态图构建方法已经被开发，但其中相对较少的探索了邻居节点之间的因果关系。因此，由此产生的模型缺乏对动态生成图的邻居节点之间因果关系的强大可解释性，这很容易导致后续决策的风险。此外，很少有人考虑基于时间序列数据集的动态图的不确定性和噪声，而这在真实世界的图结构网络中是普遍存在的。在本文中，我们提出了一种新颖的基于扩散变分图神经网络（DVGNN）的时空预测方法。对于动态图构建，设计了一种无监督生成模型。在编码器阶段，应用两层图卷积网络（GCN）来计算潜在节点嵌入的后验分布。

    Graph neural networks (GNNs), especially dynamic GNNs, have become a research hotspot in spatio-temporal forecasting problems. While many dynamic graph construction methods have been developed, relatively few of them explore the causal relationship between neighbour nodes. Thus, the resulting models lack strong explainability for the causal relationship between the neighbour nodes of the dynamically generated graphs, which can easily lead to a risk in subsequent decisions. Moreover, few of them consider the uncertainty and noise of dynamic graphs based on the time series datasets, which are ubiquitous in real-world graph structure networks. In this paper, we propose a novel Dynamic Diffusion-Variational Graph Neural Network (DVGNN) for spatio-temporal forecasting. For dynamic graph construction, an unsupervised generative model is devised. Two layers of graph convolutional network (GCN) are applied to calculate the posterior distribution of the latent node embeddings in the encoder sta
    
[^121]: 生成式表格预训练增强了表格预测模型

    Generative Table Pre-training Empowers Models for Tabular Prediction. (arXiv:2305.09696v1 [cs.LG])

    [http://arxiv.org/abs/2305.09696](http://arxiv.org/abs/2305.09696)

    本文提出了TapTap，一种通过表格预训练生成高质量合成表格来提高表格预测性能的方法。在12个数据集实验中，TapTap在不同场景下优于16个基线，并可以与多个骨干模型结合使用。

    

    近年来，表格预训练已经成为研究的热点，但如何利用表格预训练来提高表格预测的性能仍然是一个开放性挑战。本文提出了TapTap，这是第一个利用表格预训练来增强表格预测模型的尝试。在对大量实际世界的表格数据进行预训练后，TapTap能够生成高质量的合成表格，以支持各种表格数据应用，包括隐私保护、低资源环境、缺失值插补和失衡分类。在12个数据集上的广泛实验表明，TapTap在不同场景下优于16个基线。同时，它可以轻松地与各种骨干模型结合使用，包括LightGBM、多层感知机（MLP）和Transformer。此外，在表格预训练的帮助下，使用TapTap生成的合成数据进行训练的模型甚至可以与使用原始真实数据的模型竞争，实现了相当甚至更好的性能。

    Recently, the topic of table pre-training has attracted considerable research interest. However, how to employ table pre-training to boost the performance of tabular prediction remains an open challenge. In this paper, we propose TapTap, the first attempt that leverages table pre-training to empower models for tabular prediction. After pre-training on a large corpus of real-world tabular data, TapTap can generate high-quality synthetic tables to support various applications on tabular data, including privacy protection, low resource regime, missing value imputation, and imbalanced classification. Extensive experiments on 12 datasets demonstrate that TapTap outperforms a total of 16 baselines in different scenarios. Meanwhile, it can be easily combined with various backbone models, including LightGBM, Multilayer Perceptron (MLP) and Transformer. Moreover, with the aid of table pre-training, models trained using synthetic data generated by TapTap can even compete with models using the or
    
[^122]: 应用机器学习分析软件质量测试

    Applying Machine Learning Analysis for Software Quality Test. (arXiv:2305.09695v1 [cs.SE])

    [http://arxiv.org/abs/2305.09695](http://arxiv.org/abs/2305.09695)

    研究通过应用机器学习，利用可用数据计算累计软件故障水平，并提出预测残留缺陷的方法。

    

    软件维护是软件开发中最大的开销之一。因此，了解维护的触发因素以及是否可以预测它是至关重要的。研究表明，评估程序复杂度的特定方法可以产生有用的预测模型，以确定由于软件故障导致维护的可能性。本文将机器学习应用于可用数据，以计算累计软件故障水平，并提出使用机器学习预测软件残留缺陷的技术，作为预测残留缺陷挑战的解决方案。

    One of the biggest expense in software development is the maintenance. Therefore, it is critical to comprehend what triggers maintenance and if it may be predicted. Numerous research have demonstrated that specific methods of assessing the complexity of created programs may produce useful prediction models to ascertain the possibility of maintenance due to software failures. As a routine it is performed prior to the release, and setting up the models frequently calls for certain, object-oriented software measurements. It is not always the case that software developers have access to these measurements. In this paper, the machine learning is applied on the available data to calculate the cumulative software failure levels. A technique to forecast a software`s residual defectiveness using machine learning can be looked into as a solution to the challenge of predicting residual flaws. Software metrics and defect data were separated out of the static source code repository. Static code is 
    
[^123]: 带衰减函数的时间序列异常检测评估策略

    Evaluation Strategy of Time-series Anomaly Detection with Decay Function. (arXiv:2305.09691v1 [cs.LG])

    [http://arxiv.org/abs/2305.09691](http://arxiv.org/abs/2305.09691)

    本文提出了带衰减函数的点调整协议（PAdf）以解决现有时间序列异常检测算法评估方式高估或低估性能的问题。通过在基准数据集上的重新评估，我们发现PAdf协议不仅考虑要查找尽可能多的段数，还考虑快速准确地检测异常。

    

    近期，时间序列异常检测算法一般采用点调整协议来评估其性能。然而，这种协议容易高估检测算法的性能，因为它只考虑检测到的异常段数和大小。本文提出了一种新的评估协议——带衰减函数的点调整协议（PAdf），以评估时间序列异常检测算法的性能。该协议考虑了快速准确地检测异常和避免误报的理想要求。本文从理论和实验两个方面证明了PAdf协议解决了现有协议如PA和PA\%K等的高估和低估问题。通过在基准数据集上重新评估SOTA模型，我们发现PA协议只考虑查找尽可能多的异常段，而PAdf协议则不仅考虑查找尽可能多的段数，同时还考虑快速检测异常。

    Recent algorithms of time-series anomaly detection have been evaluated by applying a Point Adjustment (PA) protocol. However, the PA protocol has a problem of overestimating the performance of the detection algorithms because it only depends on the number of detected abnormal segments and their size. We propose a novel evaluation protocol called the Point-Adjusted protocol with decay function (PAdf) to evaluate the time-series anomaly detection algorithm by reflecting the following ideal requirements: detect anomalies quickly and accurately without false alarms. This paper theoretically and experimentally shows that the PAdf protocol solves the over- and under-estimation problems of existing protocols such as PA and PA\%K. By conducting re-evaluations of SOTA models in benchmark datasets, we show that the PA protocol only focuses on finding many anomalous segments, whereas the score of the PAdf protocol considers not only finding many segments but also detecting anomalies quickly witho
    
[^124]: 用合成字幕和迁移学习训练的音频描述模型Whisper Transformer的研究(arXiv:2305.09690v1 [cs.SD])

    A Whisper transformer for audio captioning trained with synthetic captions and transfer learning. (arXiv:2305.09690v1 [cs.SD])

    [http://arxiv.org/abs/2305.09690](http://arxiv.org/abs/2305.09690)

    这篇论文介绍了一种音频描述方法，采用预训练的Whisper模型和合成字幕的预训练。实验结果表明，不同的训练策略会影响音频描述模型的性能。

    

    随着大规模音频数据集的出现和深度学习技术的进步，音频描述领域在近年来取得了显著的进展。在这篇技术报告中，我们介绍了一种音频描述的方法，重点关注预训练的语音转文本Whisper模型和用于合成字幕的预训练。我们讨论了我们的训练过程，并呈现了我们的实验结果，包括模型大小变化、数据集混合和其他超参数。我们的发现表明了不同训练策略对音频描述模型性能的影响。我们的代码和训练模型公开在GitHub和Hugging Face Hub上。

    The field of audio captioning has seen significant advancements in recent years, driven by the availability of large-scale audio datasets and advancements in deep learning techniques. In this technical report, we present our approach to audio captioning, focusing on the use of a pretrained speech-to-text Whisper model and pretraining on synthetic captions. We discuss our training procedures and present our experiments' results, which include model size variations, dataset mixtures, and other hyperparameters. Our findings demonstrate the impact of different training strategies on the performance of the audio captioning model. Our code and trained models are publicly available on GitHub and Hugging Face Hub.
    
[^125]: OOD-Speech: 用于 Bengali 语音识别的大规模越域基准数据集

    OOD-Speech: A Large Bengali Speech Recognition Dataset for Out-of-Distribution Benchmarking. (arXiv:2305.09688v1 [eess.AS])

    [http://arxiv.org/abs/2305.09688](http://arxiv.org/abs/2305.09688)

    OOD-Speech 是用于 Bengali 语音识别的越域基准数据集，由众包收集了母语为 Bengali 的 22,645 名说话者录制的 1177.94 小时语音数据，并经过手动注释。数据集包含 17 种不同的资源，如 Bengali 电视剧、有声读物、脱口秀、在线教学以及伊斯兰讲道等，可作为 Bengali 语音识别的分布变化方面的基准测试数据集。

    

    我们提出了 OOD-Speech，这是 Bengali 的第一个用于自动语音识别的越域基准数据集。作为全球使用最广泛的语言之一，Bengali 展示了大量的方言和韵律特征，这要求 ASR 框架对分布变化具有鲁棒性。例如，Bengali 中的伊斯兰宗教讲道是用明显不同的语调进行的，这也成为了分布变化的例子。我们的训练数据集是通过在线众包活动收集并筛选而来，共收集了来自南亚的 22,645 名母语为 Bengali 的说话者所录制的 1177.94 小时。我们的测试数据集则包括来自 17 个不同资源（如 Bengali 电视剧、有声读物、脱口秀、在线教学以及伊斯兰讲道等）的 23.03 小时语音数据，这些数据也都经过了手动注释。OOD-Speech 既是当前公开的最大的语音数据集，也是 Bengali 语音识别的第一个越域基准数据集。

    We present OOD-Speech, the first out-of-distribution (OOD) benchmarking dataset for Bengali automatic speech recognition (ASR). Being one of the most spoken languages globally, Bengali portrays large diversity in dialects and prosodic features, which demands ASR frameworks to be robust towards distribution shifts. For example, islamic religious sermons in Bengali are delivered with a tonality that is significantly different from regular speech. Our training dataset is collected via massively online crowdsourcing campaigns which resulted in 1177.94 hours collected and curated from $22,645$ native Bengali speakers from South Asia. Our test dataset comprises 23.03 hours of speech collected and manually annotated from 17 different sources, e.g., Bengali TV drama, Audiobook, Talk show, Online class, and Islamic sermons to name a few. OOD-Speech is jointly the largest publicly available speech dataset, as well as the first out-of-distribution ASR benchmarking dataset for Bengali.
    
[^126]: 数据偏差管理

    Data Bias Management. (arXiv:2305.09686v1 [cs.LG])

    [http://arxiv.org/abs/2305.09686](http://arxiv.org/abs/2305.09686)

    本文讲述了数据偏差在机器学习中的应用、影响及可能的解决方案

    

    鉴于数据驱动系统在我们日常生活中的广泛应用，偏差和公平等概念在科研人员和从业人员，无论是在产业界还是学术界中，都受到了重视。这些问题通常源于用于训练机器学习系统的数据质量不同。随着这些系统被商业化和部署，有时被委托做出改变生活的决策，人们正在做出重大努力来确定和消除可能导致数据偏差的来源。本文提供了研究结果，展示数据偏见如何影响最终用户，偏差的起源以及我们应该如何解决该问题。我们认为，不必在所有情况下消除数据偏差，而是应将研究重点转向偏见的识别。

    Due to the widespread use of data-powered systems in our everyday lives, concepts like bias and fairness gained significant attention among researchers and practitioners, in both industry and academia. Such issues typically emerge from the data, which comes with varying levels of quality, used to train supervised machine learning systems. With the commercialization and deployment of such systems that are sometimes delegated to make life-changing decisions, significant efforts are being made towards the identification and removal of possible sources of data bias that may resurface to the final end user or in the decisions being made. In this paper, we present research results that show how bias in data affects end users, where bias is originated, and provide a viewpoint about what we should do about it. We argue that data bias is not something that should necessarily be removed in all cases, and that research attention should instead shift from bias removal towards the identification, m
    
[^127]: 工控系统异常检测数据集

    Anomaly Detection Dataset for Industrial Control Systems. (arXiv:2305.09678v1 [cs.CR])

    [http://arxiv.org/abs/2305.09678](http://arxiv.org/abs/2305.09678)

    本文介绍了'ICS-Flow'数据集，其中包括了监督和无监督机器学习算法评估所需的网络数据和过程状态变量日志，并提供了几种流行的算法在该数据集上的基础性能评估，旨在促进工控系统的入侵检测系统发展和加强关键基础设施的网络安全性。

    

    过去几十年中，工控系统(ICSS)已经成为网络攻击的目标，随着越来越多的ICS与互联网相连，安全性变得越来越脆弱。使用机器学习(ML)进行入侵检测系统(IDS)是保护ICS网络的有前途的方法，但缺乏适合评估ML算法的数据集是一个挑战。本文介绍了'ICS-Flow'数据集，为监督和无监督的基于ML的IDS评估提供了网络数据和过程状态变量日志。网络数据包括从模拟的ICS组件和仿真网络中捕获的正常和异常网络数据包和流。异常是通过黑客常用的各种攻击技术注入到系统中的，用于修改网络流量和攻击ICS。我们还提供了几种流行的ML算法在ICS-Flow数据集上的基础性能评估，并与传统基于规则的IDS进行比较。数据集和评估结果将有助于开发先进的ICS入侵检测系统和提高关键基础设施的网络安全性。

    Over the past few decades, Industrial Control Systems (ICSs) have been targeted by cyberattacks and are becoming increasingly vulnerable as more ICSs are connected to the internet. Using Machine Learning (ML) for Intrusion Detection Systems (IDS) is a promising approach for ICS cyber protection, but the lack of suitable datasets for evaluating ML algorithms is a challenge. Although there are a few commonly used datasets, they may not reflect realistic ICS network data, lack necessary features for effective anomaly detection, or be outdated. This paper presents the 'ICS-Flow' dataset, which offers network data and process state variables logs for supervised and unsupervised ML-based IDS assessment. The network data includes normal and anomalous network packets and flows captured from simulated ICS components and emulated networks. The anomalies were injected into the system through various attack techniques commonly used by hackers to modify network traffic and compromise ICSs. We also 
    
[^128]: 使用双阶段深度学习模型进行漏洞检测

    Vulnerability Detection Using Two-Stage Deep Learning Models. (arXiv:2305.09673v1 [cs.CR])

    [http://arxiv.org/abs/2305.09673](http://arxiv.org/abs/2305.09673)

    本文提出了一种双阶段解决方案，采用了两个深度学习模型用于漏洞检测，其可在识别和分类各种类型的漏洞方面达到高准确率，优于传统SAST和DAST方法。

    

    应用程序安全是现代软件开发的重要组成部分，许多攻击取决于软件中的漏洞。由于技术进步，攻击数量正在全球范围内增加。公司必须在开发、测试和部署软件的每个阶段中都包含安全功能，以防止数据泄露。检测软件漏洞的方法有许多种，如非AI方法(如SAST和DAST)。然而，这些方法存在大量的误报和漏报。与此相对，研究人员一直致力于开发基于AI的漏洞检测系统，采用了BERT、BLSTM等深度学习模型。在本文中，我们提出了一种双阶段解决方案，提出了两个深度学习模型，用于C/C++源代码的漏洞检测。第一阶段是CNN，用于检测源代码是否包含任何漏洞(二元分类)，第二阶段是基于LSTM的模型，用于识别特定类型的漏洞(多类分类)。所提出的解决方案在检测和分类各种类型的漏洞方面实现了高精度率，优于传统的SAST和DAST方法。

    Application security is an essential part of developing modern software, as lots of attacks depend on vulnerabilities in software. The number of attacks is increasing globally due to technological advancements. Companies must include security in every stage of developing, testing, and deploying their software in order to prevent data breaches. There are several methods to detect software vulnerability Non-AI-based such as Static Application Security Testing (SAST) and Dynamic Application Security Testing (DAST). However, these approaches have substantial false-positive and false-negative rates. On the other side, researchers have been interested in developing an AI-based vulnerability detection system employing deep learning models like BERT, BLSTM, etc. In this paper, we proposed a two-stage solution, two deep learning models were proposed for vulnerability detection in C/C++ source codes, the first stage is CNN which detects if the source code contains any vulnerability (binary class
    
[^129]: 选择你的毒药：深度图像分类数据污染攻击中的检测性与鲁棒性之争

    Pick your Poison: Undetectability versus Robustness in Data Poisoning Attacks against Deep Image Classification. (arXiv:2305.09671v1 [cs.CR])

    [http://arxiv.org/abs/2305.09671](http://arxiv.org/abs/2305.09671)

    深度图像分类数据污染攻击存在检测性与鲁棒性之争：污染太少导致攻击失效，污染太多易被检测到。该论文提出两种防御措施，对有限的信任图像标签对进行后处理来检测和修复被污染的模型，并证明其有效性。

    

    在大量网络爬取数据上训练的深度图像分类模型容易受到数据污染攻击，这是一种暗藏后门的机制。即使培训过程中只有少量污染样本，也足以在推理过程中破坏模型的完整性。虽然已知污染更多的样本可以增强攻击的效果和鲁棒性，但尚不清楚污染太多样本是否会使攻击变得更易被检测到从而削弱攻击效果。我们观察到数据污染攻击中存在一个基本的检测性/鲁棒性权衡：污染太少的样本会导致攻击失效和不鲁棒，但污染太多的样本则会使攻击易被检测到。这提高了数据污染攻击者的门槛，他们必须权衡这种权衡以保持鲁棒和不易被检测。我们的工作提出了两种防御方法，旨在使用有限的信任图像标签对作为培训后的后处理步骤来检测和修复被污染的模型。我们展示了我们的防御措施可以减轻大量污染攻击，同时对逃避尝试保持抵抗力。

    Deep image classification models trained on large amounts of web-scraped data are vulnerable to data poisoning, a mechanism for backdooring models. Even a few poisoned samples seen during training can entirely undermine the model's integrity during inference. While it is known that poisoning more samples enhances an attack's effectiveness and robustness, it is unknown whether poisoning too many samples weakens an attack by making it more detectable. We observe a fundamental detectability/robustness trade-off in data poisoning attacks: Poisoning too few samples renders an attack ineffective and not robust, but poisoning too many samples makes it detectable. This raises the bar for data poisoning attackers who have to balance this trade-off to remain robust and undetectable. Our work proposes two defenses designed to (i) detect and (ii) repair poisoned models as a post-processing step after training using a limited amount of trusted image-label pairs. We show that our defenses mitigate a
    
[^130]: 异构隐私下的均值估计: 部分隐私是可以免费的。

    Mean Estimation Under Heterogeneous Privacy: Some Privacy Can Be Free. (arXiv:2305.09668v1 [cs.CR])

    [http://arxiv.org/abs/2305.09668](http://arxiv.org/abs/2305.09668)

    本文研究了在不同隐私要求下的均值估计问题，提出的算法在两组具有不同隐私级别的用户时是极小化的最优的，并揭示了一个有趣的饱和现象。

    

    差分隐私 (DP) 是一种被广泛运用用于衡量算法隐私损失的框架。传统的DP形式对所有用户强制施加一致的隐私要求，这与现实场景通常不一致，因为用户个体决定他们的隐私偏好。本文探讨了在异构DP约束下的平均数估计问题，其中每个用户可以施加自己独特的隐私水平。我们提出的算法在两组具有不同隐私级别的用户时被证明是极小化的最优的。我们的结果揭示了一个有趣的饱和现象，即在一组用户的隐私水平被放宽而另一组用户的隐私水平保持不变时发生。也就是说，在某个特定情形下，进一步放宽前一组的隐私要求并不会改善最小二乘平均数估计器的性能。因此，中央服务器可以提供一定程度的隐私而不会牺牲性能。

    Differential Privacy (DP) is a well-established framework to quantify privacy loss incurred by any algorithm. Traditional DP formulations impose a uniform privacy requirement for all users, which is often inconsistent with real-world scenarios in which users dictate their privacy preferences individually. This work considers the problem of mean estimation under heterogeneous DP constraints, where each user can impose their own distinct privacy level. The algorithm we propose is shown to be minimax optimal when there are two groups of users with distinct privacy levels. Our results elicit an interesting saturation phenomenon that occurs as one group's privacy level is relaxed, while the other group's privacy level remains constant. Namely, after a certain point, further relaxing the privacy requirement of the former group does not improve the performance of the minimax optimal mean estimator. Thus, the central server can offer a certain degree of privacy without any sacrifice in perform
    
[^131]: 交叉门控多层感知机下的蛋白质复合物不变嵌入是一种一次性抗体设计器

    Protein Complex Invariant Embedding with Cross-Gate MLP is A One-Shot Antibody Designer. (arXiv:2305.09480v1 [q-bio.BM])

    [http://arxiv.org/abs/2305.09480](http://arxiv.org/abs/2305.09480)

    本文提出了一种深度生成模型，可以一次性地共同设计抗体CDR的1D序列和3D结构，解决几何建模和低效推断的问题。

    

    抗体是由免疫系统产生的针对外来物质或抗原的重要蛋白质。抗体的特异性由其互补决定区（CDR）决定，CDR位于抗体链的可变区域中，形成与抗原结合的位点。以往的研究利用复杂的技术生成CDR，但它们遭受了几何建模不足的问题。此外，常见的迭代精化策略导致了低效的推断。本文提出了一种深度生成模型，可以一次性地共同设计CDR的1D序列和3D结构。为了实现这一目标，我们将抗体CDR设计分为两个阶段：（i）蛋白质结构的几何建模和（ii）序列结构共学习。我们开发了一种蛋白质复合物不变嵌入，可捕捉蛋白质骨架原子（包括Cα、N、C和O原子）之间的内部和外部组分相互作用，以实现全面的几何建模。

    Antibodies are crucial proteins produced by the immune system in response to foreign substances or antigens. The specificity of an antibody is determined by its complementarity-determining regions (CDRs), which are located in the variable domains of the antibody chains and form the antigen-binding site. Previous studies have utilized complex techniques to generate CDRs, but they suffer from inadequate geometric modeling. Moreover, the common iterative refinement strategies lead to an inefficient inference. In this paper, we propose a deep generative model that can co-design 1D sequences and 3D structures of CDRs in a one-shot manner. To achieve this, we decouple the antibody CDR design into two stages: (i) geometric modeling of protein structures and (ii) sequence-structure co-learning. We develop a protein complex invariant embedding that captures both intra- and inter-component interactions among the backbone atoms including C$\alpha$, N, C, and O atoms to achieve comprehensive geome
    
[^132]: 使用SHAP特征重要性和模糊认知地图测量隐性偏见

    Measuring Implicit Bias Using SHAP Feature Importance and Fuzzy Cognitive Maps. (arXiv:2305.09399v1 [cs.LG])

    [http://arxiv.org/abs/2305.09399](http://arxiv.org/abs/2305.09399)

    本文使用SHAP特征重要性和模糊认知地图模型，对隐性偏见进行测量，结果表明特征重要性作为绝对工具不适应于测量隐性偏见，受保护特征的偏见数量可能因特征是数值编码还是分类编码而有所不同。

    

    本文将特征重要性概念与模式分类中的隐性偏见相结合，通过三步方法实现：（i）构建一个分类器和调整其超参数，（ii）构建一个能量化隐性偏见的模糊认知地图模型，（iii）使用SHAP特征重要性在模拟中激活神经元概念。以关于公平研究的实际案例研究结果支持我们的双重假设。一方面，阐明了使用特征重要性作为绝对工具来衡量隐性偏见的风险。另一方面，得出结论：对受保护特征的偏见数量可能因特征是数值编码还是分类编码而有所不同。

    In this paper, we integrate the concepts of feature importance with implicit bias in the context of pattern classification. This is done by means of a three-step methodology that involves (i) building a classifier and tuning its hyperparameters, (ii) building a Fuzzy Cognitive Map model able to quantify implicit bias, and (iii) using the SHAP feature importance to active the neural concepts when performing simulations. The results using a real case study concerning fairness research support our two-fold hypothesis. On the one hand, it is illustrated the risks of using a feature importance method as an absolute tool to measure implicit bias. On the other hand, it is concluded that the amount of bias towards protected features might differ depending on whether the features are numerically or categorically encoded.
    
[^133]: 基于边缘传感器的半弹性纺织品触摸感测技术研究

    Touch Sensing on Semi-Elastic Textiles with Border-Based Sensors. (arXiv:2305.09222v1 [cs.LG])

    [http://arxiv.org/abs/2305.09222](http://arxiv.org/abs/2305.09222)

    本研究提出一种基于半弹性纺织品表面上边缘的传感器进行接触感测的方法，无需在感测区域放置额外传感器。该方法可在可穿戴技术和智能纺织品等领域中应用，能够以82.85%的准确度分类识别三个压力水平，具有潜在的应用价值。

    

    本研究提出了一种新的接触感测方法，使用半弹性纺织品表面上边缘的传感器，而不需要在接触区域放置额外的传感器。通过在弹性运动织物上进行多种机器学习模型的实验，验证了所提出的方法。其中一种基于边缘的传感器设计的性能进行了深入评估。通过使用视觉标记，最佳表现的视觉传感器预测125mm×125mm区域上一个点的平均均方误差为1.36mm。我们制作了一种仅用纺织品实现的原型，能够以82.85%的准确度分类识别三个压力水平（0、15和20mm）。我们的结果表明，这种方法在可穿戴技术和智能纺织品中具有潜在的应用，以进一步探索这些领域，这将是一个充满前途的方向。

    This study presents a novel approach for touch sensing using semi-elastic textile surfaces that does not require the placement of additional sensors in the sensing area, instead relying on sensors located on the border of the textile. The proposed approach is demonstrated through experiments involving an elastic Jersey fabric and a variety of machine-learning models. The performance of one particular border-based sensor design is evaluated in depth. By using visual markers, the best-performing visual sensor arrangement predicts a single touch point with a mean squared error of 1.36 mm on an area of 125mm by 125mm. We built a textile only prototype that is able to classify touch at three indent levels (0, 15, and 20 mm) with an accuracy of 82.85%. Our results suggest that this approach has potential applications in wearable technology and smart textiles, making it a promising avenue for further exploration in these fields.
    
[^134]: 基于模块化动作程序的动作问答

    Motion Question Answering via Modular Motion Programs. (arXiv:2305.08953v1 [cs.CV])

    [http://arxiv.org/abs/2305.08953](http://arxiv.org/abs/2305.08953)

    提出了一种新的HumanMotionQA任务，用于评估模型在复杂的人体运动序列上进行复杂、多步推理的能力。同时，提出了一种特殊的NSPose方法，利用符号化推理和模块化设计，成功地应用于该任务中，并超过所有基线方法。

    

    为了构建能够感知和理解真实世界中的人类行为的人工智能系统，我们必须首先设计模型，对动作序列进行复杂的时空推理。为了实现这个目标，我们提出了HumanMotionQA任务，评估模型在长时间人类运动序列上进行复杂、多步推理的能力。我们生成了一个问答对数据集，需要在动作序列的小部分中检测运动线索，对事件发生的时间进行推理，并查询特定的运动属性。此外，我们还提出了NSPose，一种神经符号方法，用于处理该任务，它利用符号化推理和模块化设计，通过学习运动概念、属性神经操作符和时间关系，来处理动作。我们证明了NSPose在HumanMotionQA任务中的适用性，胜过了所有基线方法。

    In order to build artificial intelligence systems that can perceive and reason with human behavior in the real world, we must first design models that conduct complex spatio-temporal reasoning over motion sequences. Moving towards this goal, we propose the HumanMotionQA task to evaluate complex, multi-step reasoning abilities of models on long-form human motion sequences. We generate a dataset of question-answer pairs that require detecting motor cues in small portions of motion sequences, reasoning temporally about when events occur, and querying specific motion attributes. In addition, we propose NSPose, a neuro-symbolic method for this task that uses symbolic reasoning and a modular design to ground motion through learning motion concepts, attribute neural operators, and temporal relations. We demonstrate the suitability of NSPose for the HumanMotionQA task, outperforming all baseline methods.
    
[^135]: 神经符号人工智能及其分类法：一项调查研究

    Neurosymbolic AI and its Taxonomy: a survey. (arXiv:2305.08876v1 [cs.NE])

    [http://arxiv.org/abs/2305.08876](http://arxiv.org/abs/2305.08876)

    本文调查研究了神经符号人工智能的研究，探索了学习数据分布和推理先前和学习知识相结合的方法，以探索实现人工智能通用性的替代方案。

    

    神经符号人工智能涉及组合符号处理（如经典人工智能）和神经网络的模型，是一个非常成熟的领域。这些模型作为实现人工智能通用性的一种尝试，在探索除了增加数据集和模型尺寸以外的替代方案以及将学习数据分布和推理先前和学习知识相结合方面具有独特作用。本次调查研究了这一领域近年来的研究论文，并提供了这些模型的分类和比较，同时介绍了应用案例。

    Neurosymbolic AI deals with models that combine symbolic processing, like classic AI, and neural networks, as it's a very established area. These models are emerging as an effort toward Artificial General Intelligence (AGI) by both exploring an alternative to just increasing datasets' and models' sizes and combining Learning over the data distribution, Reasoning on prior and learned knowledge, and by symbiotically using them. This survey investigates research papers in this area during recent years and brings classification and comparison between the presented models as well as applications.
    
[^136]: 用于船舶设计优化的机器学习数据集：Ship-D

    Ship-D: Ship Hull Dataset for Design Optimization using Machine Learning. (arXiv:2305.08279v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.08279](http://arxiv.org/abs/2305.08279)

    本文介绍了一个适用于船舶设计优化的机器学习数据集Ship-D，该数据集包含三万艘船舶外形信息、设计参数、网格表示、点云数据、图像表示以及三十二种水动力阻力数据，并可支持人类和计算方法进行设计。

    

    近来，机器学习在减少复杂产品设计周期方面已取得重大进展。目前船舶设计需要数年时间，且生产规模很小，因此可以从这些进展中受益。本文通过开发一种基于机器学习的船舶设计工具，学习不同类型的船舶设计，从而识别和优化船舶设计中的协同效应。然而，目前缺乏公开的船舶设计数据集限制了机器学习在通用船舶设计中的潜力。为了解决这个问题，本文介绍了一个包含三万艘船舶外形信息、设计参数、网格表示、点云数据、图像表示以及在不同运行条件下的三十二种水动力阻力的大型数据集--Ship-D，该数据集旨在为人类和计算方法提供支持。

    Machine learning has recently made significant strides in reducing design cycle time for complex products. Ship design, which currently involves years long cycles and small batch production, could greatly benefit from these advancements. By developing a machine learning tool for ship design that learns from the design of many different types of ships, tradeoffs in ship design could be identified and optimized. However, the lack of publicly available ship design datasets currently limits the potential for leveraging machine learning in generalized ship design. To address this gap, this paper presents a large dataset of thirty thousand ship hulls, each with design and functional performance information, including parameterization, mesh, point cloud, and image representations, as well as thirty two hydrodynamic drag measures under different operating conditions. The dataset is structured to allow human input and is also designed for computational methods. Additionally, the paper introduce
    
[^137]: 在对话推荐系统中利用大型语言模型

    Leveraging Large Language Models in Conversational Recommender Systems. (arXiv:2305.07961v1 [cs.IR])

    [http://arxiv.org/abs/2305.07961](http://arxiv.org/abs/2305.07961)

    本文提出了一种使用大型语言模型构建端到端大规模对话推荐系统的路线图，解决在该系统中有效利用大型语言模型所面临的技术挑战。

    

    对话推荐系统通过启用实时的多轮对话使用户更加透明和掌控。最近，大型语言模型展现了与人类对话自然的能力，并将世界知识和常识推理融入到语言理解中，进一步释放了这一范式的潜力。然而，在对话推荐系统中有效利用大型语言模型引入了新的技术挑战，包括适当地理解和控制复杂的对话和从外部信息源检索。由于大而不断增长的项目语料库和缺乏对话数据进行训练，这些问题加剧了。在本文中，我们提供了使用大型语言模型构建端到端大规模对话推荐系统的路线图。特别地，我们提出了用户偏好理解、灵活的对话管理和可解释的推荐作为整个系统的一部分的新实现方式。

    A Conversational Recommender System (CRS) offers increased transparency and control to users by enabling them to engage with the system through a real-time multi-turn dialogue. Recently, Large Language Models (LLMs) have exhibited an unprecedented ability to converse naturally and incorporate world knowledge and common-sense reasoning into language understanding, unlocking the potential of this paradigm. However, effectively leveraging LLMs within a CRS introduces new technical challenges, including properly understanding and controlling a complex conversation and retrieving from external sources of information. These issues are exacerbated by a large, evolving item corpus and a lack of conversational data for training. In this paper, we provide a roadmap for building an end-to-end large-scale CRS using LLMs. In particular, we propose new implementations for user preference understanding, flexible dialogue management and explainable recommendations as part of an integrated architecture
    
[^138]: 在移动设备上监测和调整机器学习模型

    Monitoring and Adapting ML Models on Mobile Devices. (arXiv:2305.07772v1 [cs.LG])

    [http://arxiv.org/abs/2305.07772](http://arxiv.org/abs/2305.07772)

    这篇论文介绍了Nazr，这是一个能够在移动设备上连续监测和调整机器学习模型，以提高模型准确性的端到端系统。

    

    为了实现低延迟推理和离线操作，机器学习模型越来越多地被部署到移动设备上。然而，一旦部署了模型，运营者难以追踪其精确度，可能会因为数据漂移等问题而不可预测地降低。我们设计了Nazr，这是第一个端到端的系统，可以在移动设备上连续监测和调整模型，无需用户反馈。我们的关键观察是，模型退化通常是由特定的根本原因造成的，这可能会影响大量设备。因此，一旦Nazr检测到大量设备上的一致性退化，它就会采用根本原因分析来确定问题的起源，并应用特定于原因的适应。我们在两个计算机视觉数据集上评估了Nazr，并展示了与现有方法相比，它在提高准确性方面始终表现出色。在一个包含从驾驶汽车中收集的照片的数据集上，Nazr的平均准确性提高了15％。

    ML models are increasingly being pushed to mobile devices, for low-latency inference and offline operation. However, once the models are deployed, it is hard for ML operators to track their accuracy, which can degrade unpredictably (e.g., due to data drift). We design Nazar, the first end-to-end system for continuously monitoring and adapting models on mobile devices without requiring feedback from users. Our key observation is that often model degradation is due to a specific root cause, which may affect a large group of devices. Therefore, once Nazar detects a consistent degradation across a large number of devices, it employs a root cause analysis to determine the origin of the problem and applies a cause-specific adaptation. We evaluate Nazar on two computer vision datasets, and show it consistently boosts accuracy compared to existing approaches. On a dataset containing photos collected from driving cars, Nazar improves the accuracy on average by 15%.
    
[^139]: 带有非对角信息的视觉-语言连续表示学习

    Continual Vision-Language Representaion Learning with Off-Diagonal Information. (arXiv:2305.07437v1 [cs.LG])

    [http://arxiv.org/abs/2305.07437](http://arxiv.org/abs/2305.07437)

    本文探讨了通过流数据持续训练CLIP模型的可行性，提出了一种有效的连续学习框架Mod-X，并证明内部旋转和跨模态偏差导致了CLIP在跨模态检索任务中性能下降。

    

    本文讨论了通过流数据持续训练CLIP模型的可行性。通过追踪连续更新的CLIP模型中表示向量的方向变化，我们探索和总结了这些空间变化，称为空间混乱（SD），可以分为内部旋转和跨模态偏差。此外，我们从经验和理论上证明了内部旋转和跨模态偏差如何导致CLIP在跨模态检索任务中性能下降。为了缓解空间混乱，我们提出了一种简单而有效的连续学习框架Mod-X: 维护非对角信息矩阵。在各种不同规模和范围的常用数据集上的实验表明了我们方法的有效性。

    This paper discusses the feasibility of continuously training the CLIP model through streaming data. Then, by tracking the directional changes of the representation vectors in the continuously updated CLIP model, we explore and summarize these spatial variations as Spatial Disorder (SD), which can be divided into Intra-modal Rotation and Inter-modal Deviation. Moreover, we demonstrate how intra-modal rotation and inter-modal deviation lead to a performance decline for CLIP on cross-modal retrieval tasks in both empirically and theoretically. To alleviate the spatial disorder, we propose a simple yet effective continual learning framework Mod-X: Maintain off-diagonal information-matriX. The experiments (in Section \ref{method}, \ref{experiments} and Appendix \ref{Appendix_to_experiments}) on commonly used datasets with different scales and scopes have illustrated the effectiveness of our method.
    
[^140]: 通过条件神经场在时空预测中将时间融入到通用方法中

    A Generic Approach to Integrating Time into Spatial-Temporal Forecasting via Conditional Neural Fields. (arXiv:2305.06827v1 [cs.LG])

    [http://arxiv.org/abs/2305.06827](http://arxiv.org/abs/2305.06827)

    本文提出了一种将时间组件融入预测模型的通用方法，通过使用条件神经场来表示辅助特征，解决了在利用时间序列进行预测时存在的一个未解决问题。

    

    自我意识是自主系统的关键能力，例如自主驾驶网络，需要高效的时间序列预测算法，使系统能够推断环境的未来状态以及其随时间推移对系统行为的影响。最近，大量利用卷积神经网络和图神经网络的预测算法已被开发出来，以利用时间序列中存在的复杂时空依赖关系。虽然这些解决方案在统计方法上显示出了显著的优势，但一个未解决的问题是如何有效地将表示季节性模式的全局信息通过时间序列的时间组件整合到预测模型中以提高其精度。本文提出了一种将时间组件融入预测模型的通用方法。其主要思想是使用条件神经场来表示辅助特征。

    Self-awareness is the key capability of autonomous systems, e.g., autonomous driving network, which relies on highly efficient time series forecasting algorithm to enable the system to reason about the future state of the environment, as well as its effect on the system behavior as time progresses. Recently, a large number of forecasting algorithms using either convolutional neural networks or graph neural networks have been developed to exploit the complex temporal and spatial dependencies present in the time series. While these solutions have shown significant advantages over statistical approaches, one open question is to effectively incorporate the global information which represents the seasonality patterns via the time component of time series into the forecasting models to improve their accuracy. This paper presents a general approach to integrating the time component into forecasting models. The main idea is to employ conditional neural fields to represent the auxiliary feature
    
[^141]: 探索机器遗忘的领域：一篇综述与分类

    Exploring the Landscape of Machine Unlearning: A Survey and Taxonomy. (arXiv:2305.06360v1 [cs.LG])

    [http://arxiv.org/abs/2305.06360](http://arxiv.org/abs/2305.06360)

    本文综述了机器遗忘的现状和技术应用，包括数据删除、扰动和模型更新，讨论了MU在隐私、安全和公正性等领域的潜在益处，以及它在自然语言处理、计算机视觉和推荐系统中的未来发展方向。

    

    机器遗忘是一个越来越受关注的领域，因为需要删除或修改机器学习模型所做出的预测。虽然训练模型变得更加有效和准确，但在某些领域（如隐私、安全和公正性），遗忘先前学到的信息的重要性变得越来越显著。本文介绍了机器遗忘的综述，涵盖了当前最先进的技术和方法，包括数据删除、扰动和模型更新。此外，文中还介绍了常用的度量标准和数据集。文章还强调了需要解决的挑战，包括攻击复杂性、标准化、可转移性、可解释性、训练数据和资源限制。本文的贡献包括讨论MU的潜在益处以及它在自然语言处理、计算机视觉和推荐系统中的未来方向。

    Machine unlearning (MU) is a field that is gaining increasing attention due to the need to remove or modify predictions made by machine learning (ML) models. While training models have become more efficient and accurate, the importance of unlearning previously learned information has become increasingly significant in fields such as privacy, security, and fairness. This paper presents a comprehensive survey of MU, covering current state-of-the-art techniques and approaches, including data deletion, perturbation, and model updates. In addition, commonly used metrics and datasets are also presented. The paper also highlights the challenges that need to be addressed, including attack sophistication, standardization, transferability, interpretability, training data, and resource constraints. The contributions of this paper include discussions about the potential benefits of MU and its future directions in Natural Language Processing, Computer vision, and Recommender Systems. Additionally, 
    
[^142]: 多目标优化的逆强化学习的收敛性证明研究

    A proof of convergence of inverse reinforcement learning for multi-objective optimization. (arXiv:2305.06137v1 [cs.LG])

    [http://arxiv.org/abs/2305.06137](http://arxiv.org/abs/2305.06137)

    本论文证明了多目标优化的逆强化学习方法在理论层面上的收敛性，包括Wasserstein逆强化学习和常规逆强化学习方法。

    

    本文通过将等效于多目标优化的WIRL问题的逆问题与投影次梯度法相结合，证明了Wasserstein逆强化学习（WIRL）在多目标优化中的收敛性。此外，我们还证明了逆强化学习（最大熵逆强化学习，导引成本学习）在多目标优化中的收敛性。

    We show the convergence of Wasserstein inverse reinforcement learning (WIRL) for multi-objective optimizations with the projective subgradient method by formulating an inverse problem of the optimization problem that is equivalent to WIRL for multi-objective optimizations.  In addition, we prove convergence of inverse reinforcement learning (maximum entropy inverse reinforcement learning, guid cost learning) for multi-objective optimization with the projective subgradient method.
    
[^143]: 自注意力动态中的聚类现象

    The emergence of clusters in self-attention dynamics. (arXiv:2305.05465v1 [cs.LG])

    [http://arxiv.org/abs/2305.05465](http://arxiv.org/abs/2305.05465)

    本文证实了当Transformer处理一系列token时，出现“领导者”的经验观察，即随着时间趋于无穷大，代表token的粒子会聚集在特定的极限对象附近，这取决于价值矩阵的谱。

    

    将Transformer视为相互作用的粒子系统，当权重不随时间变化时，本文描述了学习表示的几何形状。我们展示了代表token的粒子随着时间趋于无穷大而趋向于特定的极限对象。出现的极限对象类型取决于价值矩阵的谱。此外，在一维情况下，我们证明了自我注意力矩阵收敛于低秩布尔矩阵。这些结果的组合在数学上证实了Vaswani等人的经验观察，即Transformer处理一系列token时会出现“领导者”。

    Viewing Transformers as interacting particle systems, we describe the geometry of learned representations when the weights are not time dependent. We show that particles, representing tokens, tend to cluster toward particular limiting objects as time tends to infinity. The type of limiting object that emerges depends on the spectrum of the value matrix. Additionally, in the one-dimensional case we prove that the self-attention matrix converges to a low-rank Boolean matrix. The combination of these results mathematically confirms the empirical observation made by Vaswani et al. \cite{vaswani2017attention} that \emph{leaders} appear in a sequence of tokens when processed by Transformers.
    
[^144]: 信用风险管理中的量化不确定性：一种深度证据回归方法

    UQ for Credit Risk Management: A deep evidence regression approach. (arXiv:2305.04967v1 [q-fin.RM])

    [http://arxiv.org/abs/2305.04967](http://arxiv.org/abs/2305.04967)

    本文扩展了Deep Evidence Regression方法，将其应用于预测信用风险中的违约损失；我们提供了相关的学习框架，并在模拟和实际数据上进行了验证。

    

    机器学习已经广泛应用于各种信用风险应用程序中。由于信用风险的固有性质，量化预测风险指标的不确定性是必要的，将考虑不确定性的深度学习模型应用于信用风险设置中非常有帮助。在本项工作中，我们探索了一种可扩展的UQ感知深度学习技术，Deep Evidence Regression，并将其应用于预测违约损失。我们通过将Deep Evidence Regression方法扩展到通过Weibull过程生成的目标变量的学习来为文献做出了贡献，并提供了相关的学习框架。我们展示了我们的方法在模拟和实际数据上的应用。

    Machine Learning has invariantly found its way into various Credit Risk applications. Due to the intrinsic nature of Credit Risk, quantifying the uncertainty of the predicted risk metrics is essential, and applying uncertainty-aware deep learning models to credit risk settings can be very helpful. In this work, we have explored the application of a scalable UQ-aware deep learning technique, Deep Evidence Regression and applied it to predicting Loss Given Default. We contribute to the literature by extending the Deep Evidence Regression methodology to learning target variables generated by a Weibull process and provide the relevant learning framework. We demonstrate the application of our approach to both simulated and real-world data.
    
[^145]: 数据集压缩综合研究：性能、隐私、鲁棒性以及公平性

    A Comprehensive Study on Dataset Distillation: Performance, Privacy, Robustness and Fairness. (arXiv:2305.03355v1 [cs.LG])

    [http://arxiv.org/abs/2305.03355](http://arxiv.org/abs/2305.03355)

    本研究对当前最先进的数据集压缩方法进行了全面评估，发现其存在隐私风险并可能放大模型的不公平性，提供了大规模的基准测试框架。

    

    数据集压缩旨在将原始数据集的丰富特征编码成小型数据集，是一种加速神经网络训练和相关研究的有前途的方法。已经提出了不同的方法来改善压缩图像的信息性和泛化性能。然而，目前还没有从安全性角度全面分析这一技术的工作，并且对潜在风险缺乏系统理解。在本文中，我们进行了大量实验，评估了当前最先进的数据集压缩方法。我们成功使用成员推理攻击来显示仍然存在隐私风险。本文还表明，数据集压缩在模型鲁棒性方面可能会产生不同程度的影响，并在进行预测时放大类别间的模型不公平性。本研究为数据集压缩评估提供了大规模的基准测试框架。

    The aim of dataset distillation is to encode the rich features of an original dataset into a tiny dataset. It is a promising approach to accelerate neural network training and related studies. Different approaches have been proposed to improve the informativeness and generalization performance of distilled images. However, no work has comprehensively analyzed this technique from a security perspective and there is a lack of systematic understanding of potential risks. In this work, we conduct extensive experiments to evaluate current state-of-the-art dataset distillation methods. We successfully use membership inference attacks to show that privacy risks still remain. Our work also demonstrates that dataset distillation can cause varying degrees of impact on model robustness and amplify model unfairness across classes when making predictions. This work offers a large-scale benchmarking framework for dataset distillation evaluation.
    
[^146]: 分布式协同功能：统一博弈论交互方法来解释机器学习

    Distributing Synergy Functions: Unifying Game-Theoretic Interaction Methods for Machine-Learning Explainability. (arXiv:2305.03100v1 [cs.LG])

    [http://arxiv.org/abs/2305.03100](http://arxiv.org/abs/2305.03100)

    本文提出了一种统一的框架，用于游戏理论驱动的归因和k阶交互方法，通过假设，可以在连续输入设置中得到唯一全面的特征交互解释，即协同作用。

    

    深度学习已经彻底改变了机器学习的许多领域，从计算机视觉到自然语言处理，但这些高性能模型通常是“黑盒子”。解释此类模型将提高AI决策透明度和信任，并且对于理解其他实际需求（如鲁棒性和公平性）是必要的。增强模型透明度的一种流行方法是量化单个输入对模型输出的贡献（称为归因）以及群组输入之间的相互作用的强度。越来越多的这些方法导入博弈论的概念和结果来产生归因和交互作用。本文提出了一个统一的框架，用于博弈论驱动的归因和k阶交互方法。我们展示了在连续输入设置中，假设适度，可以得到特征之间交互的唯一全面说明，即协同作用。我们确定了各种方法的特征。

    Deep learning has revolutionized many areas of machine learning, from computer vision to natural language processing, but these high-performance models are generally "black box." Explaining such models would improve transparency and trust in AI-powered decision making and is necessary for understanding other practical needs such as robustness and fairness. A popular means of enhancing model transparency is to quantify how individual inputs contribute to model outputs (called attributions) and the magnitude of interactions between groups of inputs. A growing number of these methods import concepts and results from game theory to produce attributions and interactions. This work presents a unifying framework for game-theory-inspired attribution and $k^\text{th}$-order interaction methods. We show that, given modest assumptions, a unique full account of interactions between features, called synergies, is possible in the continuous input setting. We identify how various methods are characte
    
[^147]: 医学图像分析中的任意分割模型：一项实验研究

    Segment Anything Model for Medical Image Analysis: an Experimental Study. (arXiv:2304.10517v1 [cs.CV])

    [http://arxiv.org/abs/2304.10517](http://arxiv.org/abs/2304.10517)

    本研究对医学图像分割模型SAM在各种不同情况下的表现进行了广泛评估，结果表明在单点提示下其表现高度变化，是一项具有挑战性的工作。

    

    由于数据注释的有限可用性和获取成本，训练医学图像分割模型仍然具有挑战性。Segment Anything Model（SAM）是一种基础模型，经过超过10亿个注释的训练，主要用于自然图像，旨在能够以交互方式分割用户定义的感兴趣的对象。尽管SAM在自然图像上表现出色，但不清楚该模型在转换到医学图像领域时会受到多大影响。在这里，我们对SAM在各种模态和解剖学的11个医学图像数据集上进行了广泛的评估。在我们的实验中，我们使用标准方法生成点提示来模拟交互分割。实验结果表明，SAM基于单点提示的表现在任务和数据集方面高度变化，即从脊柱MRI数据集的0.1135到髋关节X射线数据集的0.8650。

    Training segmentation models for medical images continues to be challenging due to the limited availability and acquisition expense of data annotations. Segment Anything Model (SAM) is a foundation model trained on over 1 billion annotations, predominantly for natural images, that is intended to be able to segment the user-defined object of interest in an interactive manner. Despite its impressive performance on natural images, it is unclear how the model is affected when shifting to medical image domains. Here, we perform an extensive evaluation of SAM's ability to segment medical images on a collection of 11 medical imaging datasets from various modalities and anatomies. In our experiments, we generated point prompts using a standard method that simulates interactive segmentation. Experimental results show that SAM's performance based on single prompts highly varies depending on the task and the dataset, i.e., from 0.1135 for a spine MRI dataset to 0.8650 for a hip x-ray dataset, eva
    
[^148]: PED-ANOVA: 在任意子空间中高效量化超参数重要性

    PED-ANOVA: Efficiently Quantifying Hyperparameter Importance in Arbitrary Subspaces. (arXiv:2304.10255v1 [cs.LG])

    [http://arxiv.org/abs/2304.10255](http://arxiv.org/abs/2304.10255)

    PED-ANOVA 提出了一个新的 f-ANOVA 公式，能够在任意子空间中高效地计算超参数的重要性，有助于深度学习中好的超参数空间设计。

    

    深度学习中超参数优化的流行使得好的超参数空间设计对于训练强模型至关重要，而好的超参数空间设计又严重依赖于了解不同超参数的作用。这激发了关于超参数重要性的研究，例如使用功能方差分析 (f-ANOVA) 的流行方法。然而，原始的 f-ANOVA 公式不适用于算法设计师最相关的子空间，例如由最佳性能定义的子空间。为了解决这个问题，我们推导了一个新的针对任意子空间的 f-ANOVA 公式，并提出了一个算法，使用 Pearson 散度 (PED) 实现超参数重要性的闭式计算。我们证明，这个新算法，称为 PED-ANOVA，能够成功地识别不同子空间中重要的超参数，同时计算效率极高。

    The recent rise in popularity of Hyperparameter Optimization (HPO) for deep learning has highlighted the role that good hyperparameter (HP) space design can play in training strong models. In turn, designing a good HP space is critically dependent on understanding the role of different HPs. This motivates research on HP Importance (HPI), e.g., with the popular method of functional ANOVA (f-ANOVA). However, the original f-ANOVA formulation is inapplicable to the subspaces most relevant to algorithm designers, such as those defined by top performance. To overcome this problem, we derive a novel formulation of f-ANOVA for arbitrary subspaces and propose an algorithm that uses Pearson divergence (PED) to enable a closed-form computation of HPI. We demonstrate that this new algorithm, dubbed PED-ANOVA, is able to successfully identify important HPs in different subspaces while also being extremely computationally efficient.
    
[^149]: NetGPT：网络流量生成预训练变压器模型

    NetGPT: Generative Pretrained Transformer for Network Traffic. (arXiv:2304.09513v1 [cs.NI])

    [http://arxiv.org/abs/2304.09513](http://arxiv.org/abs/2304.09513)

    本文提出了首个网络流量生成预训练变压器模型NetGPT，该模型可以优化网络任务的训练效率和有效性。

    

    预训练模型可以利用大规模的原始数据学习网络流量的基本特征，并为输入流量生成可区分的结果，而不考虑特定的下游任务。有效的预训练模型可以显著优化下游任务的训练效率和有效性，例如流量分类、攻击检测、资源调度、协议分析和流量生成。本文提出了NetGPT，旨在为网络流量构建预训练模型并解决多样的挑战。

    Pretrained models for network traffic can utilize large-scale raw data to learn the essential characteristics of network traffic, and generate distinguishable results for input traffic without considering specific downstream tasks. Effective pretrained models can significantly optimize the training efficiency and effectiveness of downstream tasks, such as traffic classification, attack detection, resource scheduling, protocol analysis, and traffic generation. Despite the great success of pretraining in natural language processing, there is no work in the network field. Considering the diverse demands and characteristics of network traffic and network tasks, it is non-trivial to build a pretrained model for network traffic and we face various challenges, especially the heterogeneous headers and payloads in the multi-pattern network traffic and the different dependencies for contexts of diverse downstream network tasks.  To tackle these challenges, in this paper, we make the first attemp
    
[^150]: 基于BERT的技术对美国最高法院案例进行分类

    Classification of US Supreme Court Cases using BERT-Based Techniques. (arXiv:2304.08649v1 [cs.CL])

    [http://arxiv.org/abs/2304.08649](http://arxiv.org/abs/2304.08649)

    本文基于BERT技术探究了对美国最高法院案例进行分类的方法，比较了使用BERT模型与其他先进模型的准确性，最终在15个广泛类别上取得了80%的准确度，在279个细粒度类别上取得了60%的准确度。

    

    基于双向编码器表示来自变压器的模型（BERT）在许多自然语言处理（NLP）任务（如命名实体识别（NER），词性（POS）标记等）上产生了最新技术（SOTA）结果。当分类长文档（例如来自美国最高法院的文档）时，使用BERT模型可能比较困难。本文中，我们尝试了几种基于BERT的分类技术，用于对美国最高法院决定或最高法院数据库（SCDB）进行分类，并将其与先前的SOTA结果进行了比较。我们还将我们的结果与针对长文档的SOTA模型进行了比较。我们对两个分类任务进行了比较：（1）广泛的分类任务，具有15个类别；（2）细粒度的分类任务，具有279个类别。我们的最佳结果在15个广泛类别上产生80％的准确度，在279个细粒度类别上产生60％的准确度。

    Models based on bidirectional encoder representations from transformers (BERT) produce state of the art (SOTA) results on many natural language processing (NLP) tasks such as named entity recognition (NER), part-of-speech (POS) tagging etc. An interesting phenomenon occurs when classifying long documents such as those from the US supreme court where BERT-based models can be considered difficult to use on a first-pass or out-of-the-box basis. In this paper, we experiment with several BERT-based classification techniques for US supreme court decisions or supreme court database (SCDB) and compare them with the previous SOTA results. We then compare our results specifically with SOTA models for long documents. We compare our results for two classification tasks: (1) a broad classification task with 15 categories and (2) a fine-grained classification task with 279 categories. Our best result produces an accuracy of 80\% on the 15 broad categories and 60\% on the fine-grained 279 categories 
    
[^151]: 基于对比学习的多模态短视频谣言检测系统

    Multimodal Short Video Rumor Detection System Based on Contrastive Learning. (arXiv:2304.08401v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2304.08401](http://arxiv.org/abs/2304.08401)

    本研究基于对比学习设计出一个多模态短视频谣言检测系统，通过构建具有多种特征的短视频数据集和使用多模态特征融合与外部知识，能有效地区分短视频谣言。

    

    随着短视频平台成为新闻分享的重要渠道之一，中国主要短视频平台逐渐成为虚假新闻的新滋生地。由于短视频包含了大量信息和特征，以及视频之间的严重同质化和相似性，因此很难区分短视频谣言。为了减轻短视频谣言的传播，我们的团队考虑到每种算法的优缺点，构建了多模态特征融合并引入外部知识进行短视频谣言检测。检测的主要思路是：（1）创建数据集：构建具有多种特征的短视频数据集；（2）多模态谣言检测模型：首先使用TSN视频编码模型提取视频特征；然后使用OCR和ASR提取视频的文本特征；最后，将这些特征进行融合来进行短视频谣言检测。

    With short video platforms becoming one of the important channels for news sharing, major short video platforms in China have gradually become new breeding grounds for fake news. However, it is not easy to distinguish short video rumors due to the great amount of information and features contained in short videos, as well as the serious homogenization and similarity of features among videos. In order to mitigate the spread of short video rumors, our group decides to detect short video rumors by constructing multimodal feature fusion and introducing external knowledge after considering the advantages and disadvantages of each algorithm. The ideas of detection are as follows: (1) dataset creation: to build a short video dataset with multiple features; (2) multimodal rumor detection model: firstly, we use TSN (Temporal Segment Networks) video coding model to extract video features; then, we use OCR (Optical Character Recognition) and ASR (Automatic Character Recognition) to extract video 
    
[^152]: 知识追踪的注意力Q-矩阵学习

    Attentive Q-Matrix Learning for Knowledge Tracing. (arXiv:2304.08168v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2304.08168](http://arxiv.org/abs/2304.08168)

    本文提出了一种基于Q-矩阵的注意力知识追踪模型，能够在不存在事先确定的技能标签的情况下应用于大规模的在线教育平台。

    

    在智能教学系统的迅猛发展中，追踪学生的知识状态越来越重要，以便提供个性化的学习指导。这是知识追踪 (KT) 的主要思想，它根据学生在平台上的过去交互来建立模型，以模拟学生掌握知识概念 (KCs，解决问题所需的技能)。虽然已经提出了许多KT模型，并表现出了卓越的性能，但其中大部分模型使用概念来索引问题，这意味着需要预先确定每个问题所需的技能标签，以指示正确回答该问题所需的KC。这使得这些模型很难应用于大规模的在线教育平台，因为问题通常没有按照技能标签进行很好的组织。本文提出了基于Q-矩阵的注意力知识追踪 (QAKT)，这是一种端到端的模型，能够将注意力方法应用于场景中，其中不存在事先确定的技能标签。

    As the rapid development of Intelligent Tutoring Systems (ITS) in the past decade, tracing the students' knowledge state has become more and more important in order to provide individualized learning guidance. This is the main idea of Knowledge Tracing (KT), which models students' mastery of knowledge concepts (KCs, skills needed to solve a question) based on their past interactions on platforms. Plenty of KT models have been proposed and have shown remarkable performance recently. However, the majority of these models use concepts to index questions, which means the predefined skill tags for each question are required in advance to indicate the KCs needed to answer that question correctly. This makes it pretty hard to apply on large-scale online education platforms where questions are often not well-organized by skill tags. In this paper, we propose Q-matrix-based Attentive Knowledge Tracing (QAKT), an end-to-end style model that is able to apply the attentive method to scenes where n
    
[^153]: 用于黑盒变分推断的样本平均估计方法

    Sample Average Approximation for Black-Box VI. (arXiv:2304.06803v1 [cs.LG])

    [http://arxiv.org/abs/2304.06803](http://arxiv.org/abs/2304.06803)

    该论文提出了一种用于黑盒变分推断的样本平均估计方法，有效地解决了随机梯度上升等问题，实验结果表明其比现有方法更快且性能更佳。

    

    我们提出了一种新的方法，用于解决随机梯度上升的困难，包括选择步长的任务。我们的方法涉及使用一系列样本平均估计问题（SAA）。通过将随机优化问题转化为确定性问题，SAA逼近了随机优化问题的解。我们使用拟牛顿方法和线性搜索来解决每个确定性优化问题，并提出了一种启发式策略来自动选择超参数。我们的实验表明，我们的方法简化了变分推断问题，并实现了比现有方法更快的性能。

    We present a novel approach for black-box VI that bypasses the difficulties of stochastic gradient ascent, including the task of selecting step-sizes. Our approach involves using a sequence of sample average approximation (SAA) problems. SAA approximates the solution of stochastic optimization problems by transforming them into deterministic ones. We use quasi-Newton methods and line search to solve each deterministic optimization problem and present a heuristic policy to automate hyperparameter selection. Our experiments show that our method simplifies the VI problem and achieves faster performance than existing methods.
    
[^154]: AutoRL超参数景观

    AutoRL Hyperparameter Landscapes. (arXiv:2304.02396v1 [cs.LG])

    [http://arxiv.org/abs/2304.02396](http://arxiv.org/abs/2304.02396)

    本文提出了一种方法，在训练期间多次建立和分析AutoRL超参数的景观，证明代表算法（DQN和SAC）在不同环境下的超参数景观会随时间而变化。

    

    强化学习（RL）在取得令人瞩目成果的同时，其超参数对性能的影响限制了其应用范围。这经常使得在实践中难以获得良好的结果。自动化RL（AutoRL）解决了这个难题，但有关超参数优化（HPO）方法在搜索最佳配置时所遍历的超参数景观动态变化的信息很少。鉴于现有AutoRL方法动态调整超参数配置的情况，我们提出了一种方法，在训练期间不仅在一个时间点，而且在多个时间点上建立和分析这些超参数景观。针对关于这种动态AutoRL方法合法性的一个重要开放问题，我们提供了充分的证据，表明在不同种类的环境（Cartpole和Pendulum）中，来自RL文献的代表算法（DQN和SAC）的超参数景观会随时间而强烈变化。

    Although Reinforcement Learning (RL) has shown to be capable of producing impressive results, its use is limited by the impact of its hyperparameters on performance. This often makes it difficult to achieve good results in practice. Automated RL (AutoRL) addresses this difficulty, yet little is known about the dynamics of the hyperparameter landscapes that hyperparameter optimization (HPO) methods traverse in search of optimal configurations. In view of existing AutoRL approaches dynamically adjusting hyperparameter configurations, we propose an approach to build and analyze these hyperparameter landscapes not just for one point in time but at multiple points in time throughout training. Addressing an important open question on the legitimacy of such dynamic AutoRL approaches, we provide thorough empirical evidence that the hyperparameter landscapes strongly vary over time across representative algorithms from RL literature (DQN and SAC) in different kinds of environments (Cartpole and
    
[^155]: 时间动态同步功能脑网络在精神分裂症诊断和侧化分析中的应用

    Temporal Dynamic Synchronous Functional Brain Network for Schizophrenia Diagnosis and Lateralization Analysis. (arXiv:2304.01347v1 [q-bio.NC])

    [http://arxiv.org/abs/2304.01347](http://arxiv.org/abs/2304.01347)

    本文提出了一种基于动态功能连接的脑网络分析模型，通过构建动态同步特征和革命性的图卷积方法实现精神分裂症诊断和侧化分析，并在实验证明其表现优于其他最先进模型。

    

    有证据表明，动态功能连接可以捕捉静息态功能磁共振成像数据中的脑活动时变异常，并在揭示精神分裂症（SZ）患者异常脑活动机制方面具有天然优势。因此，本文采用了一种先进的动态脑网络分析模型——时态脑类别图卷积网络（temporal-BCGCN）。首先设计了独特的动态脑网络分析模块DSF-BrainNet，用于构建动态同步特征。随后，提出了一种革命性的图卷积方法TemporalConv，基于特征的同步时间属性。最后，提出了一种基于静息态功能磁共振成像数据的深度学习模块化异常半球侧化检测工具，称为CategoryPool。该研究在COBRE和UCLA数据集上进行验证，分别达到83.62％和89.71％的平均准确率，优于基线模型和其他最先进模型，在精神分裂症诊断和侧化分析方面表现出色。

    Available evidence suggests that dynamic functional connectivity (dFC) can capture time-varying abnormalities in brain activity in rs-fMRI data and has a natural advantage in uncovering mechanisms of abnormal brain activity in schizophrenia(SZ) patients. Hence, an advanced dynamic brain network analysis model called the temporal brain category graph convolutional network (temporal-BCGCN) was employed. Firstly, a unique dynamic brain network analysis module, DSF-BrainNet, was designed to construct dynamic synchronization features. Subsequently, a revolutionary graph convolution method, TemporalConv, was proposed, based on the synchronous temporal properties of feature. Finally, the first modular abnormal hemispherical lateralization test tool in deep learning based on rs-fMRI data, named CategoryPool, was proposed. This study was validated on COBRE and UCLA datasets and achieved 83.62% and 89.71% average accuracy, respectively, outperforming the baseline model and other State-of-the-Art
    
[^156]: 使用图神经网络重建粒子物理过程的拓扑结构

    Topological Reconstruction of Particle Physics Processes using Graph Neural Networks. (arXiv:2303.13937v1 [hep-ph])

    [http://arxiv.org/abs/2303.13937](http://arxiv.org/abs/2303.13937)

    Topograph是一种利用图神经网络和粒子衰变自然规律的拓扑结构重建方法，不仅解决了观测到的末态对象组合指派问题，还预测了中间粒子的性质及其后续衰变，比标准方法效果更好，与现代机器学习技术表现相当。

    

    我们提出了一种新的方法，称为Topograph，它利用粒子物理衰变的本质和信息传递图神经网络的灵活性，重建了包括中介粒子在内的底层物理过程。Topograph不仅解决了观测到的末态对象的组合指派问题，将它们与它们原来的母粒子关联起来，而且直接预测了硬散射过程中中间粒子的性质及其后续衰变。与标准的组合方法或现代图神经网络方法相比，它的复杂度与重构对象的数量成线性关系。我们应用Topograph于全强子衰变模式下的顶夸克对产生问题，相对标准方法，我们的方法表现更优，与最先进的机器学习技术相当。

    We present a new approach, the Topograph, which reconstructs underlying physics processes, including the intermediary particles, by leveraging underlying priors from the nature of particle physics decays and the flexibility of message passing graph neural networks. The Topograph not only solves the combinatoric assignment of observed final state objects, associating them to their original mother particles, but directly predicts the properties of intermediate particles in hard scatter processes and their subsequent decays. In comparison to standard combinatoric approaches or modern approaches using graph neural networks, which scale exponentially or quadratically, the complexity of Topographs scales linearly with the number of reconstructed objects.  We apply Topographs to top quark pair production in the all hadronic decay channel, where we outperform the standard approach and match the performance of the state-of-the-art machine learning technique.
    
[^157]: 粒子平均场变分贝叶斯方法

    Particle Mean Field Variational Bayes. (arXiv:2303.13930v1 [stat.CO])

    [http://arxiv.org/abs/2303.13930](http://arxiv.org/abs/2303.13930)

    本论文提出了一种新的基于粒子的MFVB方法，有效扩展了其适用范围，可应用于贝叶斯逻辑回归、随机波动和深度神经网络。

    

    平均场变分贝叶斯（MFVB）方法是一种计算效率最高的贝叶斯推断技术之一，然而其使用仅限于具有共轭先验或需要解析计算的模型。本文提出了一种新的基于粒子的MFVB方法，大大扩展了MFVB方法的适用范围。我们通过利用Wasserstein梯度流与Langevin扩散动力学之间的联系构建了新方法的理论基础，并使用贝叶斯逻辑回归、随机波动和深度神经网络证明了这种方法的有效性。

    The Mean Field Variational Bayes (MFVB) method is one of the most computationally efficient techniques for Bayesian inference. However, its use has been restricted to models with conjugate priors or those that require analytical calculations. This paper proposes a novel particle-based MFVB approach that greatly expands the applicability of the MFVB method. We establish the theoretical basis of the new method by leveraging the connection between Wasserstein gradient flows and Langevin diffusion dynamics, and demonstrate the effectiveness of this approach using Bayesian logistic regression, stochastic volatility, and deep neural networks.
    
[^158]: 无需参考数据的相位像差修复：一种自适应混合损失深度学习方法

    Phase Aberration Correction without Reference Data: An Adaptive Mixed Loss Deep Learning Approach. (arXiv:2303.05747v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2303.05747](http://arxiv.org/abs/2303.05747)

    本文提出了一种深度学习的自适应混合损失方法，用于无需参考数据的超声相位像差修复。

    

    相位像差是超声成像质量下降的主要原因之一，由介质中声速的空间变化引起。这种效应破坏了传输波并阻止回波信号的相干叠加，导致图像质量低下。在实际实验中，获得非像差的基础真值可能会极具挑战性，如果不是不可能的话。这种效应阻碍了基于深度学习的相位像差修复技术的表现，因为仅依赖于模拟数据以及模拟和实验数据之间存在领域转移。在这里，我们首次提出了一种基于深度学习的方法，它不需要参考数据来补偿相位误差效应。我们训练了一个网络，其中输入和目标输出都是随机畸变的射频（RF）数据。此外，我们证明传统损失函数，如均方误差，对于训练网络以获取可实现的相位修复是不充分的。

    Phase aberration is one of the primary sources of image quality degradation in ultrasound, which is induced by spatial variations in sound speed across the heterogeneous medium. This effect disrupts transmitted waves and prevents coherent summation of echo signals, resulting in suboptimal image quality. In real experiments, obtaining non-aberrated ground truths can be extremely challenging, if not infeasible. It hinders the performance of deep learning-based phase aberration correction techniques due to sole reliance on simulated data and the presence of domain shift between simulated and experimental data. Here, for the first time, we propose a deep learning-based method that does not require reference data to compensate for the phase aberration effect. We train a network wherein both input and target output are randomly aberrated radio frequency (RF) data. Moreover, we demonstrate that a conventional loss function such as mean square error is inadequate for training the network to ac
    
[^159]: DeepSaDe: 学习确保满足领域约束的神经网络

    DeepSaDe: Learning Neural Networks that Guarantee Domain Constraint Satisfaction. (arXiv:2303.01141v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01141](http://arxiv.org/abs/2303.01141)

    本文提出了一种学习神经网络的方法，该神经网络可以强制执行多样化的约束并且保证所有可能的预测都满足约束限制。

    

    随着机器学习模型的普及，尤其是神经网络，人们越来越关注它们的可信度，特别是在安全关键应用中，如自动驾驶汽车的行为必须是安全的。当前一些方法可以对神经网络进行约束，但它们不能保证所有可能的预测都满足约束限制（即使在未看过的数据上），或者它们对可强制执行的约束类型有限制。为了解决这些问题，本文提出了一种方法，用于训练可以强制执行广泛约束并保证所有可能预测都满足约束的神经网络。该方法基于以往将学习线性模型视为约束满足问题（CSP）的工作。为了将这个想法应用于神经网络，本文增加了两个关键的新元素：网络层上的约束传播和权重更新。

    As machine learning models, specifically neural networks, are becoming increasingly popular, there are concerns regarding their trustworthiness, specially in safety-critical applications, e.g. actions of an autonomous vehicle must be safe. There are approaches that can train neural networks where such domain requirements are enforced as constraints, but they either cannot guarantee that the constraint will be satisfied by all possible predictions (even on unseen data) or they are limited in the type of constraints that can be enforced. In this paper, we present an approach to train neural networks which can enforce a wide variety of constraints and guarantee that the constraint is satisfied by all possible predictions. The approach builds on earlier work where learning linear models is formulated as a constraint satisfaction problem (CSP). To make this idea applicable to neural networks, two crucial new elements are added: constraint propagation over the network layers, and weight upda
    
[^160]: 深度结构高斯特征模型的学习曲线

    Learning curves for deep structured Gaussian feature models. (arXiv:2303.00564v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.00564](http://arxiv.org/abs/2303.00564)

    该论文针对深度高斯模型的特征各向异性展开研究。研究结果表明，在第一层特征行之间允许存在相关性可以促进泛化，而后续层的结构通常是不利的。

    

    近年来，深度学习理论中对于多层高斯随机特征模型的泛化性能分析引起了广泛关注。然而，很少有研究考虑特征各向异性的影响；大多数模型都假设特征是使用独立同分布的高斯权重生成的。在这篇论文中，我们为具有许多层结构高斯特征的模型导出了学习曲线。我们表明，允许第一层特征的行之间存在相关性可促进泛化，而后续层的结构通常是不利的。我们的结果揭示了权重结构如何影响可解模型的泛化性能。

    In recent years, significant attention in deep learning theory has been devoted to analyzing the generalization performance of models with multiple layers of Gaussian random features. However, few works have considered the effect of feature anisotropy; most assume that features are generated using independent and identically distributed Gaussian weights. Here, we derive learning curves for models with many layers of structured Gaussian features. We show that allowing correlations between the rows of the first layer of features can aid generalization, while structure in later layers is generally detrimental. Our results shed light on how weight structure affects generalization in a simple class of solvable models.
    
[^161]: MCoCo：多级一致性协作多视角聚类

    MCoCo: Multi-level Consistency Collaborative Multi-view Clustering. (arXiv:2302.13339v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13339](http://arxiv.org/abs/2302.13339)

    MCoCo提出了一种多级一致性协作学习框架，利用对齐语义空间和多级一致性协作策略，实现了多视角聚类。

    

    多视角聚类可以利用不同视角的一致信息指导聚类。大多数现有工作都专注于在特征空间中追求浅层次的一致性，并将多个视角的信息集成到一个统一的表示中进行聚类。这些方法没有充分考虑和探索语义空间中的一致性。为了解决这个问题，我们提出了一种新的多级一致性协作学习框架(MCoCo)来进行多视角聚类。具体来说，MCoCo通过对比学习，在特征空间中联合学习多个视角的集群分配，并在语义空间中对不同视角的语义标签进行对齐。此外，我们设计了一种多级一致性协作策略，利用语义空间的一致信息作为自监督信号与特征空间中的集群分配进行协作。因此，不同级别的空间相互协作，同时实现自己的一致性目标。

    Multi-view clustering can explore consistent information from different views to guide clustering. Most existing works focus on pursuing shallow consistency in the feature space and integrating the information of multiple views into a unified representation for clustering. These methods did not fully consider and explore the consistency in the semantic space. To address this issue, we proposed a novel Multi-level Consistency Collaborative learning framework (MCoCo) for multi-view clustering. Specifically, MCoCo jointly learns cluster assignments of multiple views in feature space and aligns semantic labels of different views in semantic space by contrastive learning. Further, we designed a multi-level consistency collaboration strategy, which utilizes the consistent information of semantic space as a self-supervised signal to collaborate with the cluster assignments in feature space. Thus, different levels of spaces collaborate with each other while achieving their own consistency goal
    
[^162]: 一站式解决方案：利用预训练 LM 进行强大的时间序列分析

    One Fits All:Power General Time Series Analysis by Pretrained LM. (arXiv:2302.11939v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11939](http://arxiv.org/abs/2302.11939)

    本论文提出了一种称为 Frozen Pretrained Transformer (FPT) 的预训练模型，利用从数十亿标记训练出来的语言或 CV 模型进行时间序列分析的所有主要类型任务的微调，进而使其在所有任务中都具备着最先进的性能和泛化能力。

    

    尽管预训练模型在自然语言处理 (NLP) 和计算机视觉 (CV) 领域取得了巨大成功，但在通用时间序列分析领域取得的进展有限。与 NLP 和 CV 不同的是，这些领域采用统一模型即可执行不同的任务，而在每个时间序列分析任务中，专门设计的方法仍然占据主导地位，如分类、异常检测、预测和少样本学习。阻碍预训练模型发展的主要挑战是缺乏大量用于训练的数据。在本文中，我们通过利用从数十亿标记训练出来的语言或 CV 模型，来解决这一挑战，用于时间序列分析。具体而言，我们避免改变预训练语言或图像模型中残差块中的自注意力和前向传递层。这种模型被称为冻结的预训练变压器 (FPT)，通过对涉及时间序列分析的所有主要类型的任务进行微调进行评估，包括分类、异常检测、预测和少样本学习等。实验结果证明，FPT 在所有任务中都具有最先进的性能和泛化能力。

    Although we have witnessed great success of pre-trained models in natural language processing (NLP) and computer vision (CV), limited progress has been made for general time series analysis. Unlike NLP and CV where a unified model can be used to perform different tasks, specially designed approach still dominates in each time series analysis task such as classification, anomaly detection, forecasting, and few-shot learning. The main challenge that blocks the development of pre-trained model for time series analysis is the lack of a large amount of data for training. In this work, we address this challenge by leveraging language or CV models, pre-trained from billions of tokens, for time series analysis. Specifically, we refrain from altering the self-attention and feedforward layers of the residual blocks in the pre-trained language or image model. This model, known as the Frozen Pretrained Transformer (FPT), is evaluated through fine-tuning on all major types of tasks involving time s
    
[^163]: 高斯过程在赫赫尔姆霍兹分解中的应用：一种更流体的海洋气流模型

    Gaussian processes at the Helm(holtz): A more fluid model for ocean currents. (arXiv:2302.10364v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2302.10364](http://arxiv.org/abs/2302.10364)

    该论文提出了一种更符合已知电流物理特性的模型，在通过Helmholtz分解获得的向量场的发散和无旋分量上使用高斯过程来预测海流。证明了这种方法在模拟数据和真实浮标数据方面都比之前的方法更有效。

    

    海洋学家有兴趣预测海流和基于浮标速度的稀疏观测数据来识别当前矢量场中的发散性。高斯过程(GPs)在空间位置上充当连续但高度非线性功能的速度提供了一种吸引人的模型。但我们表明，将具有标准平稳核的GP直接应用于浮标数据可能在当前预测和发散性识别方面遇到困难—由于一些物理上不切实际的先验假设。为了更好地反映已知的电流物理特性，我们建议将标准平稳核放在通过Helmholtz分解获得的向量场的发散和无旋分量上。我们表明，由于该分解仅通过混合偏导数与原始向量场相关，因此我们仍然可以在原始数据给定的情况下进行推理，并且只需要额外进行少数计算。我们通过模拟数据和真实浮标数据证明了这种螺旋GP的有效性，从而在均方预测误差方面优于先前的方法。

    Oceanographers are interested in predicting ocean currents and identifying divergences in a current vector field based on sparse observations of buoy velocities. Since we expect current velocity to be a continuous but highly non-linear function of spatial location, Gaussian processes (GPs) offer an attractive model. But we show that applying a GP with a standard stationary kernel directly to buoy data can struggle at both current prediction and divergence identification -- due to some physically unrealistic prior assumptions. To better reflect known physical properties of currents, we propose to instead put a standard stationary kernel on the divergence and curl-free components of a vector field obtained through a Helmholtz decomposition. We show that, because this decomposition relates to the original vector field just via mixed partial derivatives, we can still perform inference given the original data with only a small constant multiple of additional computational expense. We illust
    
[^164]: 生长可操纵神经元细胞自动机

    Growing Steerable Neural Cellular Automata. (arXiv:2302.10197v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2302.10197](http://arxiv.org/abs/2302.10197)

    本文讨论了"Growing Steerable Neural Cellular Automata"，通过使每个细胞负责其自己的方向，产生了具有变化方向的细胞的模型。

    

    神经元细胞自动机（NCA）模型显示出了惊人的模式形成能力和源于局部协调的复杂全局行为。然而，在原始的NCA实现中，细胞无法调整自己的方向，定向是由模型设计师在外部定向。最近的各向同性NCA变体（Growing Isotropic Neural Cellular Automata）通过消除对其邻域中空间状态梯度感知的依赖性，使模型与定向无关 - 细胞无法从上到下或从左到右，。在这项工作中，我们采用不同的方法重新审视NCA：我们使每个细胞负责其自己的定向，允许其根据可调整的内部状态“转向”。由此产生的可操纵NCA包含嵌入同一模式中具有不同方向的细胞。 我们观察到，虽然各向同性NCA没有定向，可操纵的NCA具有手性：它们有预定

    Neural Cellular Automata (NCA) models have shown remarkable capacity for pattern formation and complex global behaviors stemming from local coordination. However, in the original implementation of NCA, cells are incapable of adjusting their own orientation, and it is the responsibility of the model designer to orient them externally. A recent isotropic variant of NCA (Growing Isotropic Neural Cellular Automata) makes the model orientation-independent - cells can no longer tell up from down, nor left from right - by removing its dependency on perceiving the gradient of spatial states in its neighborhood. In this work, we revisit NCA with a different approach: we make each cell responsible for its own orientation by allowing it to "turn" as determined by an adjustable internal state. The resulting Steerable NCA contains cells of varying orientation embedded in the same pattern. We observe how, while Isotropic NCA are orientation-agnostic, Steerable NCA have chirality: they have a predete
    
[^165]: ATLAS实验中液态氩闪烁计数器中沉积能量计算的循环神经网络固件实现

    Firmware implementation of a recurrent neural network for the computation of the energy deposited in the liquid argon calorimeter of the ATLAS experiment. (arXiv:2302.07555v2 [physics.ins-det] UPDATED)

    [http://arxiv.org/abs/2302.07555](http://arxiv.org/abs/2302.07555)

    本文介绍了基于最先进的FPGA技术实现的ATLAS实验用于计算液态氩闪烁计数器中沉积能量的循环神经网络(RNN)，优于目前基于滤波算法的计算方案。

    

    ATLAS实验测量LHC质子-质子碰撞产物的属性。在LHC高亮度阶段之前，ATLAS探测器将进行重大升级。ATLAS液态氩闪烁计数器测量探测器中电磁相互作用产生的粒子的能量。该闪烁计数器的读出电子将在ATLAS升级期间更换。新的电子板将基于来自英特尔的最先进的可编程门阵列(FPGA)，允许实现嵌入式神经网络固件。神经网络已经显示出优于当前用于计算计数器中沉积能量的最佳滤波算法。本文介绍了在Stratix 10 FPGA上实现循环神经网络(RNN)以重建沉积在计数器中能量的实现。高级综合语言(HLS)实现使快速原型设计成为可能。

    The ATLAS experiment measures the properties of particles that are products of proton-proton collisions at the LHC. The ATLAS detector will undergo a major upgrade before the high luminosity phase of the LHC. The ATLAS liquid argon calorimeter measures the energy of particles interacting electromagnetically in the detector. The readout electronics of this calorimeter will be replaced during the aforementioned ATLAS upgrade. The new electronic boards will be based on state-of-the-art field-programmable gate arrays (FPGA) from Intel allowing the implementation of neural networks embedded in firmware. Neural networks have been shown to outperform the current optimal filtering algorithms used to compute the energy deposited in the calorimeter. This article presents the implementation of a recurrent neural network (RNN) allowing the reconstruction of the energy deposited in the calorimeter on Stratix 10 FPGAs. The implementation in high level synthesis (HLS) language allowed fast prototypin
    
[^166]: 函数回归的领域泛化

    Domain Generalization by Functional Regression. (arXiv:2302.04724v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04724](http://arxiv.org/abs/2302.04724)

    通过函数回归实现领域泛化，构建了线性算子将输入边缘分布与输出条件分布联系起来，并提出了一种基于源分布依赖性的再生核希尔伯特空间预测算法。

    

    领域泛化问题是学习如何在给定多种不同源分布的数据时获得良好的泛化能力，以适用于只通过未标记样本观察到的新目标分布的模型。本文以函数回归的形式研究领域泛化问题。我们提出了一种新算法来学习从输入的边缘分布到给定输入的条件下输出的条件分布的线性算子。该算法允许基于源分布依赖性构建预测的再生核希尔伯特空间，并满足理想风险的有限样本误差界。数值实现和源代码已经提供。

    The problem of domain generalization is to learn, given data from different source distributions, a model that can be expected to generalize well on new target distributions which are only seen through unlabeled samples. In this paper, we study domain generalization as a problem of functional regression. Our concept leads to a new algorithm for learning a linear operator from marginal distributions of inputs to the corresponding conditional distributions of outputs given inputs. Our algorithm allows a source distribution-dependent construction of reproducing kernel Hilbert spaces for prediction, and, satisfies finite sample error bounds for the idealized risk. Numerical implementations and source code are available.
    
[^167]: 利用演示数据改进在线学习:质量至关重要

    Leveraging Demonstrations to Improve Online Learning: Quality Matters. (arXiv:2302.03319v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03319](http://arxiv.org/abs/2302.03319)

    本篇论文探讨了离线演示数据如何改进在线学习的问题，提出了一种利用演示数据的TS算法，并给出了依赖于先验知识的贝叶斯遗憾界；研究发现，预训练可以大幅提高在线性能，改进程度随专家能力水平的提高而增加。

    

    我们研究了离线演示数据可以如何改进在线学习，自然而然地期望会有一定的改进，但问题在于如何改进以及可以改进多少？我们表明，改进的程度必须取决于演示数据的质量。为了生成可移植的见解，我们将重点放在了作为典型在线学习算法和模型的多臂赌博机上应用汤普森抽样（TS）。演示数据是由具有给定能力水平的专家生成的，这是我们引入的一个概念。我们提出了一种知情TS算法，通过贝叶斯定理以一致的方式利用演示数据并导出依赖于先验的贝叶斯遗憾界。这提供了洞见，即预训练如何极大地提高在线性能，以及改进程度随专家能力水平的提高而增加。我们还通过贝叶斯引导实现了实用的、近似的知情TS算法，并通过实验证明了实现了实质性的遗憾减少。

    We investigate the extent to which offline demonstration data can improve online learning. It is natural to expect some improvement, but the question is how, and by how much? We show that the degree of improvement must depend on the quality of the demonstration data. To generate portable insights, we focus on Thompson sampling (TS) applied to a multi-armed bandit as a prototypical online learning algorithm and model. The demonstration data is generated by an expert with a given competence level, a notion we introduce. We propose an informed TS algorithm that utilizes the demonstration data in a coherent way through Bayes' rule and derive a prior-dependent Bayesian regret bound. This offers insight into how pretraining can greatly improve online performance and how the degree of improvement increases with the expert's competence level. We also develop a practical, approximate informed TS algorithm through Bayesian bootstrapping and show substantial empirical regret reduction through exp
    
[^168]: 概率对比学习恢复了不确定性输入的正确估计

    Probabilistic Contrastive Learning Recovers the Correct Aleatoric Uncertainty of Ambiguous Inputs. (arXiv:2302.02865v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02865](http://arxiv.org/abs/2302.02865)

    本文提出利用概率对比学习方法可以恢复具有不确定性输入的正确估计，通过扩展InfoNCE目标和编码器以预测潜变量分布来实现，在计算已知查询图像的可信区间方面具有应用价值。

    

    最近，对比学习编码器被证明可以翻转数据生成过程：它们可以将每个输入（如图像）编码成生成该图像的真实潜变量（Zimmermann等人，2021）。然而，现实世界的观察结果通常存在内在的模糊性。例如，图像可能模糊或只显示3D物体的2D视图，因此可能有多个潜变量生成它们。这使得潜变量的真实后验概率具有异方差不确定性。在这种设置下，我们扩展了常见的InfoNCE目标和编码器，以预测潜变量分布而不是点。我们证明这些分布恢复了数据生成过程的正确后验分布，包括其不确定性水平的估计，该估计存在潜变量空间的旋转。除了提供校准的不确定性估计之外，这些后验分布还允许在图像检索中计算可信区间。它们包括具有与给定查询相同的潜变量的图像。

    Contrastively trained encoders have recently been proven to invert the data-generating process: they encode each input, e.g., an image, into the true latent vector that generated the image (Zimmermann et al., 2021). However, real-world observations often have inherent ambiguities. For instance, images may be blurred or only show a 2D view of a 3D object, so multiple latents could have generated them. This makes the true posterior for the latent vector probabilistic with heteroscedastic uncertainty. In this setup, we extend the common InfoNCE objective and encoders to predict latent distributions instead of points. We prove that these distributions recover the correct posteriors of the data-generating process, including its level of aleatoric uncertainty, up to a rotation of the latent space. In addition to providing calibrated uncertainty estimates, these posteriors allow the computation of credible intervals in image retrieval. They comprise images with the same latent as a given quer
    
[^169]: 直接不确定量化

    Direct Uncertainty Quantification. (arXiv:2302.02420v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02420](http://arxiv.org/abs/2302.02420)

    本文提出一种新的直接不确定量化（DirectUQ）方法，它能在神经网络中直接输出均值和方差，同时结合了传统神经网络和贝叶斯神经网络的优点，有助于改进模型的正则化器和风险边界等方面。

    

    传统神经网络易于训练，但会产生过于自信的预测；而贝叶斯神经网络提供了良好的不确定量化，但优化它们需要耗费时间。本文介绍了一种新的方法——“直接不确定量化”（DirectUQ），它结合了它们的优点，其中神经网络直接输出最后一层的均值和方差。DirectUQ可以导出为一个替代的变分下界，因此从落单变分推理中获益，提供了改进的正则化器。另一方面，像非概率模型一样，DirectUQ具有简单的训练方法，可以使用Rademacher复杂性为模型提供风险边界。实验表明，DirectUQ和DirectUQ集成提供了时间和不确定性量化方面的良好平衡，特别是对于分布之外的数据。

    Traditional neural networks are simple to train but they produce overconfident predictions, while Bayesian neural networks provide good uncertainty quantification but optimizing them is time consuming. This paper introduces a new approach, direct uncertainty quantification (DirectUQ), that combines their advantages where the neural network directly outputs the mean and variance of the last layer. DirectUQ can be derived as an alternative variational lower bound, and hence benefits from collapsed variational inference that provides improved regularizers. On the other hand, like non-probabilistic models, DirectUQ enjoys simple training and one can use Rademacher complexity to provide risk bounds for the model. Experiments show that DirectUQ and ensembles of DirectUQ provide a good tradeoff in terms of run time and uncertainty quantification, especially for out of distribution data.
    
[^170]: 面向二元分类中标签保护的基于GAN的竖直联邦学习方法

    GAN-based Vertical Federated Learning for Label Protection in Binary Classification. (arXiv:2302.02245v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02245](http://arxiv.org/abs/2302.02245)

    GAFM是一种用于竖直联邦学习中标签保护的新方法，它利用生成对抗网络间接利用标签信息来减轻梯度标签泄漏问题，并采用交叉熵损失来提高预测准确性。

    

    分裂学习（SplitNN）已成为解决竖直联邦学习（VFL）中高计算成本和低建模效率问题的常用策略。然而，尽管SplitNN很受欢迎，但其缺乏加密保护，因此容易出现隐私泄露问题，特别是梯度标签泄漏（LLG）。出于对使用标签训练引起的LLG问题的关注，我们提出了生成对抗竖直联邦模型（GAFM），这是一种新方法，专门设计用于通过将SplitNN与生成对抗网络（GAN）集成来增强标签隐私保护。GAFM利用GAN间接利用标签信息，学习标签分布而不是依赖于显式标签，从而减轻LLG问题。 GAFM还采用基于噪声标签的交叉熵损失，进一步提高预测准确性。我们的消融实验表明，GAN和交叉熵损失的组合可以显著提高模型的性能。

    Split learning (splitNN) has emerged as a popular strategy for addressing the high computational costs and low modeling efficiency in Vertical Federated Learning (VFL). However, despite its popularity, vanilla splitNN lacks encryption protection, leaving it vulnerable to privacy leakage issues, especially Label Leakage from Gradients (LLG). Motivated by the LLG issue resulting from the use of labels during training, we propose the Generative Adversarial Federated Model (GAFM), a novel method designed specifically to enhance label privacy protection by integrating splitNN with Generative Adversarial Networks (GANs). GAFM leverages GANs to indirectly utilize label information by learning the label distribution rather than relying on explicit labels, thereby mitigating LLG. GAFM also employs an additional cross-entropy loss based on the noisy labels to further improve the prediction accuracy. Our ablation experiment demonstrates that the combination of GAN and the cross-entropy loss compo
    
[^171]: 基于双重自我意识价值分解框架的协作多智体强化学习

    Dual Self-Awareness Value Decomposition Framework without Individual Global Max for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2302.02180v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2302.02180](http://arxiv.org/abs/2302.02180)

    提出了一种基于双重自我意识价值分解框架的协作多智体强化学习方法，完全摒弃了个体全局最大值的前提，具有良好的性能表现。

    

    在协作多智体强化学习领域，价值分解方法已经变得越来越流行。然而，几乎所有现有方法都遵循个体全局最大值（IGM）或其变体的原则，这限制了它们的问题解决能力。为了解决这个问题，我们提出了一个基于心理学中双重自我意识概念的双重自我意识价值分解框架，完全摒弃了IGM前提。每个智能体包括自我策略以进行动作选择和替身价值函数以解决信用分配问题。价值函数分解可以利用显式搜索过程忽略IGM假设。在此基础上，我们还提出了一种新颖的反自我探索机制，以避免算法陷入局部最优。作为第一个完全不用IGM的价值分解方法，我们提出的框架在各种协作任务中实现了期望的性能。

    Value decomposition methods have gained popularity in the field of cooperative multi-agent reinforcement learning. However, almost all existing methods follow the principle of Individual Global Max (IGM) or its variants, which limits their problem-solving capabilities. To address this, we propose a dual self-awareness value decomposition framework, inspired by the notion of dual self-awareness in psychology, that entirely rejects the IGM premise. Each agent consists of an ego policy for action selection and an alter ego value function to solve the credit assignment problem. The value function factorization can ignore the IGM assumption by utilizing an explicit search procedure. On the basis of the above, we also suggest a novel anti-ego exploration mechanism to avoid the algorithm becoming stuck in a local optimum. As the first fully IGM-free value decomposition method, our proposed framework achieves desirable performance in various cooperative tasks.
    
[^172]: 平均限制策略优化

    Average-Constrained Policy Optimization. (arXiv:2302.00808v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00808](http://arxiv.org/abs/2302.00808)

    本研究提出了一种新的基于函数逼近算法的带平均标准约束 MDP 的策略优化算法，具有较好的性能表现。

    

    有限制条件的强化学习对于各种应用变得越来越重要。通常，平均标准比折扣标准更合适。然而，针对平均限制 CMDP 的强化学习仍然是一个具有挑战性的问题。针对折扣限制 RL 问题设计的算法通常在平均 CMDP 环境下表现不佳。在本文中，我们引入了一种新的基于函数逼近算法的带平均标准约束 MDP 的策略优化算法。平均限制策略优化（ACPO）算法的灵感来自基于信任区域方法的著名 PPO 类算法。我们发展了基本的平均 MDP 敏感性理论，然后在算法设计中使用相应的界限。我们提供了其性能的理论保证，并通过在各种具有挑战性的 MuJoCo 环境中进行大量实验工作，展示了该算法与其他常规算法相比的卓越表现。

    Reinforcement Learning (RL) with constraints is becoming an increasingly important problem for various applications. Often, the average criterion is more suitable than the discounted criterion. Yet, RL for average criterion-constrained MDPs remains a challenging problem. Algorithms designed for discounted constrained RL problems often do not perform well for the average CMDP setting. In this paper, we introduce a new policy optimization with function approximation algorithm for constrained MDPs with the average criterion. The Average-Constrained Policy Optimization (ACPO) algorithm is inspired by the famed PPO-type algorithms based on trust region methods. We develop basic sensitivity theory for average MDPs, and then use the corresponding bounds in the design of the algorithm. We provide theoretical guarantees on its performance, and through extensive experimental work in various challenging MuJoCo environments, show the superior performance of the algorithm when compared to other sta
    
[^173]: 蒸馏策略优化

    Distillation Policy Optimization. (arXiv:2302.00533v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00533](http://arxiv.org/abs/2302.00533)

    本文展示了一种演员-评论家的学习框架，该框架通过蒸馏优势在利用过去经验的同时遵循稳定的在线策略，实现了快速学习并可以适用于广泛的算法类别。

    

    本文提出了一个演员-评论家学习框架，它借鉴了分布式学习的视角和两种策略改进数据的交叉融合，实现了快速学习并可应用于广泛的算法类别。在该框架中，首先提出了方差减少机制，例如统一优势估计器 (UAE) 和一个学习的基线，不仅是连接到动作值函数的桥梁，还能提炼优势。

    On-policy algorithms are supposed to be stable, however, sample-intensive yet. Off-policy algorithms utilizing past experiences are deemed to be sample-efficient, nevertheless, unstable in general. Can we design an algorithm that can employ the off-policy data, while exploit the stable learning by sailing along the course of the on-policy walkway? In this paper, we present an actor-critic learning framework that borrows the distributional perspective of interest to evaluate, and cross-breeds two sources of the data for policy improvement, which enables fast learning and can be applied to a wide class of algorithms. In its backbone, the variance reduction mechanisms, such as unified advantage estimator (UAE), that extends generalized advantage estimator (GAE) to be applicable on any state-dependent baseline, and a learned baseline, that is competent to stabilize the policy gradient, are firstly put forward to not merely be a bridge to the action-value function but also distill the advan
    
[^174]: 随机网络碾压对防止探索的影响

    Anti-Exploration by Random Network Distillation. (arXiv:2301.13616v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13616](http://arxiv.org/abs/2301.13616)

    本文介绍了基于特征线性调制的随机网络碾压算法，可以有效防止探索，避免了区分度的问题，在 D4RL 基准测试中取得了可与集成方法相媲美的性能。

    

    尽管随机网络碾压 (RND) 在各种领域都取得了成功，但在用作离线强化学习中惩罚越界操作的不确定性估计器时，它被证明不具有足够的区分度。在本文中，我们重新审视了这些结果，并表明，通过对 RND 先验进行朴素的调节选择，演员有效地最小化反探索奖励变得不可行，并且区分度不再是问题。我们展示了可以通过基于特征线性调制 (FiLM) 的调节来避免这种局限性，从而得到一个简单而高效的基于软行为者-评论家算法的无集成算法。我们在 D4RL 基准测试上进行了评估，结果表明，它能够实现与基于集成的方法相当的性能，并显著优于无集成方法。

    Despite the success of Random Network Distillation (RND) in various domains, it was shown as not discriminative enough to be used as an uncertainty estimator for penalizing out-of-distribution actions in offline reinforcement learning. In this paper, we revisit these results and show that, with a naive choice of conditioning for the RND prior, it becomes infeasible for the actor to effectively minimize the anti-exploration bonus and discriminativity is not an issue. We show that this limitation can be avoided with conditioning based on Feature-wise Linear Modulation (FiLM), resulting in a simple and efficient ensemble-free algorithm based on Soft Actor-Critic. We evaluate it on the D4RL benchmark, showing that it is capable of achieving performance comparable to ensemble-based methods and outperforming ensemble-free approaches by a wide margin.
    
[^175]: FedRC：通过鲁棒聚类解决联邦学习中多样分布偏移的挑战

    FedRC: Tackling Diverse Distribution Shifts Challenge in Federated Learning by Robust Clustering. (arXiv:2301.12379v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12379](http://arxiv.org/abs/2301.12379)

    本文提出了一种名为FedRC的新型聚类算法框架，用于解决联邦学习中多样分布偏移的挑战，并通过鲁棒性损失函数来改进现有聚类方法。

    

    联邦学习是一种机器学习范式，通过在边缘设备上保留客户端数据来保护隐私。然而，由于学习系统的多样性和异质性，优化联邦学习在实践中可能会面临挑战。尽管最近的研究集中于当客户端之间出现分布转移时改善联邦学习的优化，但在客户端之间同时发生多种类型的分布转移，例如特征分布转移、标签分布转移和概念转移时，如何确保全局性能仍然是未充分探索的问题。在本文中，我们确定了多样分布转移同时发生时所带来的学习挑战，并提出了一种聚类原则来克服这些挑战。通过我们的研究，我们发现现有方法未能解决聚类原则。因此，我们提出了一种新的聚类算法框架——FedRC，它遵循我们提出的聚类原则，通过包含鲁棒性损失函数改进现有聚类方法。

    Federated Learning (FL) is a machine learning paradigm that safeguards privacy by retaining client data on edge devices. However, optimizing FL in practice can be challenging due to the diverse and heterogeneous nature of the learning system. Though recent research has focused on improving the optimization of FL when distribution shifts occur among clients, ensuring global performance when multiple types of distribution shifts occur simultaneously among clients -- such as feature distribution shift, label distribution shift, and concept shift -- remain under-explored.  In this paper, we identify the learning challenges posed by the simultaneous occurrence of diverse distribution shifts and propose a clustering principle to overcome these challenges. Through our research, we find that existing methods failed to address the clustering principle. Therefore, we propose a novel clustering algorithm framework, dubbed as FedRC, which adheres to our proposed clustering principle by incorporati
    
[^176]: 混合整数规划在神经网络中的可逆性认证

    Certified Invertibility in Neural Networks via Mixed-Integer Programming. (arXiv:2301.11783v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11783](http://arxiv.org/abs/2301.11783)

    本研究利用混合整数规划来认证神经网络的可逆性，并探究了过于不变性的现象在两个情境下的表现，同时讨论了如何用这些发现进行神经网络之间的可逆性认证。

    

    众所周知，神经网络容易遭受对抗攻击，即微小但影响显著的扰动可以改变网络的输出。相反，可能存在大的、有意义的扰动，但不影响网络的判断（过于不变性）。本研究探究这后一种现象在两个情境下的表现：（a）离散时间动态系统识别，以及（b）将神经网络的输出校准到另一个网络的输出。我们通过数学优化的方式来研究非可逆性，其中全局解通过与非可逆性边界的距离来度量网络预测的“安全性”。我们针对ReLU网络和$L_p$范数（$p = 1,2,\infty$）构建了混合整数规划（MIP），这些规划适用于神经网络近似动态系统的情况。我们还讨论了如何将我们的发现用于神经网络之间的可逆性认证，例如不同网络之间的转换。

    Neural networks are known to be vulnerable to adversarial attacks, which are small, imperceptible perturbations that can significantly alter the network's output. Conversely, there may exist large, meaningful perturbations that do not affect the network's decision (excessive invariance). In our research, we investigate this latter phenomenon in two contexts: (a) discrete-time dynamical system identification, and (b) the calibration of a neural network's output to that of another network. We examine noninvertibility through the lens of mathematical optimization, where the global solution measures the ``safety" of the network predictions by their distance from the non-invertibility boundary. We formulate mixed-integer programs (MIPs) for ReLU networks and $L_p$ norms ($p=1,2,\infty$) that apply to neural network approximators of dynamical systems. We also discuss how our findings can be useful for invertibility certification in transformations between neural networks, e.g. between differ
    
[^177]: 将知识纳入文档摘要生成中：基于GPT-2的前缀调整应用

    Incorporating Knowledge into Document Summarisation: an Application of Prefix-Tuning on GPT-2. (arXiv:2301.11719v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.11719](http://arxiv.org/abs/2301.11719)

    本论文研究了将事实知识纳入生成的摘要的可能性，具体采用前缀调整的方法，实验结果表明，此方法可以生成保留知识的摘要，而且可以提升整体性能。

    

    尽管现在文档摘要技术得到了很大的发展，但是生成的摘要和原始文本之间的事实不一致仍然时有发生。本研究探索了采用提示来将事实知识纳入生成的摘要的可能性。我们具体研究了前缀调整，它使用一组可训练的连续前缀提示和离散自然语言提示来帮助摘要生成。实验结果表明，可训练的前缀可以帮助摘要模型准确地从离散提示中提取信息，从而生成保留知识的摘要，这些摘要在事实上与离散提示一致。生成的摘要的ROUGE改进表明，将事实知识明确地添加到摘要生成过程中可以提升整体性能，显示出在其他自然语言处理任务中应用的巨大潜力。

    Despite the great development of document summarisation techniques nowadays, factual inconsistencies between the generated summaries and the original texts still occur from time to time. This study explores the possibility of adopting prompts to incorporate factual knowledge into generated summaries. We specifically study prefix-tuning that uses a set of trainable continuous prefix prompts together with discrete natural language prompts to aid summary generation. Experimental results demonstrate that the trainable prefixes can help the summarisation model extract information from discrete prompts precisely, thus generating knowledge-preserving summaries that are factually consistent with the discrete prompts. The ROUGE improvements of the generated summaries indicate that explicitly adding factual knowledge into the summarisation process could boost the overall performance, showing great potential for applying it to other natural language processing tasks.
    
[^178]: 神经网络学习放大决策边界附近的区域

    Neural networks learn to magnify areas near decision boundaries. (arXiv:2301.11375v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11375](http://arxiv.org/abs/2301.11375)

    神经网络训练能够放大决策边界附近的局部区域，改善整个系统的泛化能力。

    

    我们研究了训练如何塑造神经网络特征图诱导的黎曼几何。在宽度为无限的情况下，具有随机参数的神经网络在输入空间上引导高度对称的度量。训练分类任务的网络中的特征学习放大了沿决策边界的局部区域。这些变化与先前提出的用于手动调整核方法以改善泛化的几何方法一致。

    We study how training molds the Riemannian geometry induced by neural network feature maps. At infinite width, neural networks with random parameters induce highly symmetric metrics on input space. Feature learning in networks trained to perform classification tasks magnifies local areas along decision boundaries. These changes are consistent with previously proposed geometric approaches for hand-tuning of kernel methods to improve generalization.
    
[^179]: 基于层次数据有效表示学习的RNA三级结构设计

    Hierarchical Data-efficient Representation Learning for Tertiary Structure-based RNA Design. (arXiv:2301.10774v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2301.10774](http://arxiv.org/abs/2301.10774)

    本研究提出了一个基于层次数据有效表示学习的RNA设计流程，通过构建大型数据集并设计全面的结构建模方法，实现了更高效的RNA序列设计。

    

    尽管人工智能已在揭示生物大分子的一级序列与三级结构之间的关系方面取得了显着进展，但基于特定三级结构设计RNA序列仍然具有挑战性。虽然蛋白质设计中的现有方法已经彻底探索了蛋白质中结构到序列的依赖性，但RNA设计仍面临结构复杂性和数据稀缺性的困难。与此同时，虽然RNA与蛋白质共享类似的结构组分，但直接将蛋白质设计方法移植到RNA设计中却无法取得令人满意的结果。本研究旨在系统构建数据驱动的RNA设计流程。我们构建了一个大型、精心策划的基准数据集，并设计了一个全面的结构建模方法来表示复杂的RNA三级结构。更重要的是，我们提出了一个层次数据有效表示学习框架，学习结构表示的多个层次特征，以实现更高效的RNA序列设计。

    While artificial intelligence has made remarkable strides in revealing the relationship between biological macromolecules' primary sequence and tertiary structure, designing RNA sequences based on specified tertiary structures remains challenging. Though existing approaches in protein design have thoroughly explored structure-to-sequence dependencies in proteins, RNA design still confronts difficulties due to structural complexity and data scarcity. Adding to the problem, direct transplantation of protein design methodologies into RNA design fails to achieve satisfactory outcomes although sharing similar structural components. In this study, we aim to systematically construct a data-driven RNA design pipeline. We crafted a large, well-curated benchmark dataset and designed a comprehensive structural modeling approach to represent the complex RNA tertiary structure. More importantly, we proposed a hierarchical data-efficient representation learning framework that learns structural repre
    
[^180]: 压缩优化学习用于交流电最优潮流计算

    Compact Optimization Learning for AC Optimal Power Flow. (arXiv:2301.08840v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08840](http://arxiv.org/abs/2301.08840)

    本文提出了一种压缩学习方法，利用主成分分析可以显著降低交流电最优潮流计算的维度，降低了可训练参数的数量，提高了可扩展性和有效性。同时，该方法的输出可以用于热启动精确的AC求解器以恢复可行性。

    

    本文重新考虑了最优潮流计算（OPF）的端到端学习方法。现有的学习输入/输出映射的方法由于输出空间的高维度而存在可扩展性问题。本文首先展示了利用主成分分析（PCA）可以显著压缩最优解的空间。它提出了一种新方法——压缩学习，该方法在主要成分的子空间中进行学习，然后将向量转换为原始输出空间。这种压缩大大降低了可训练参数的数量，提高了可扩展性和有效性。压缩学习在PGLib的各种测试用例中进行了评估，最高可达30,000个总线。本文还表明，压缩学习的输出可以用于热启动精确的AC求解器以恢复可行性，并带来显着的加速。

    This paper reconsiders end-to-end learning approaches to the Optimal Power Flow (OPF). Existing methods, which learn the input/output mapping of the OPF, suffer from scalability issues due to the high dimensionality of the output space. This paper first shows that the space of optimal solutions can be significantly compressed using principal component analysis (PCA). It then proposes Compact Learning, a new method that learns in a subspace of the principal components before translating the vectors into the original output space. This compression reduces the number of trainable parameters substantially, improving scalability and effectiveness. Compact Learning is evaluated on a variety of test cases from the PGLib with up to 30,000 buses. The paper also shows that the output of Compact Learning can be used to warm-start an exact AC solver to restore feasibility, while bringing significant speed-ups.
    
[^181]: A-NeSI: 一种可扩展的近似方法用于概率神经符号推理。

    A-NeSI: A Scalable Approximate Method for Probabilistic Neurosymbolic Inference. (arXiv:2212.12393v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.12393](http://arxiv.org/abs/2212.12393)

    本文介绍了一种名为A-NeSI的新颖PNL框架，它使用神经网络实现了近似推理，能够保证概率逻辑语义的同时解决了PNL的可扩展性问题，能够在安全关键应用中保证逻辑约束的满足。

    

    本文研究了将神经网络与符号推理相结合的问题。最近引入的概率神经符号学习（PNL）框架，如DeepProbLog，执行指数时间的精确推理，限制了PNL解决方案的可扩展性。我们介绍了近似神经符号推理（A-NeSI）：一种新的PNL框架，它使用神经网络进行可扩展的近似推理。A-NeSI 1) 在不改变概率逻辑语义的情况下，以多项式时间执行近似推理；2) 使用由背景知识生成的数据进行训练；3) 可以生成有关预测的符号解释；4) 可以在测试时间保证逻辑约束的满足，这在安全关键应用中非常重要。我们的实验表明，A-NeSI是第一个能够解决具有指数组合扩展的三种神经符号任务的端到端方法。最后，我们的实验表明，A-NeSI实现了可解释性和安全性，而没有惩罚。

    We study the problem of combining neural networks with symbolic reasoning. Recently introduced frameworks for Probabilistic Neurosymbolic Learning (PNL), such as DeepProbLog, perform exponential-time exact inference, limiting the scalability of PNL solutions. We introduce Approximate Neurosymbolic Inference (A-NeSI): a new framework for PNL that uses neural networks for scalable approximate inference. A-NeSI 1) performs approximate inference in polynomial time without changing the semantics of probabilistic logics; 2) is trained using data generated by the background knowledge; 3) can generate symbolic explanations of predictions; and 4) can guarantee the satisfaction of logical constraints at test time, which is vital in safety-critical applications. Our experiments show that A-NeSI is the first end-to-end method to solve three neurosymbolic tasks with exponential combinatorial scaling. Finally, our experiments show that A-NeSI achieves explainability and safety without a penalty in p
    
[^182]: 朝向因果责任划分的探索

    Towards Causal Credit Assignment. (arXiv:2212.11636v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.11636](http://arxiv.org/abs/2212.11636)

    本论文研究了后见信用分配这种分配信用的方法，并提出了一种结合因果结构的状态表示来提高效率的方法。

    

    在强化学习中，如何根据行为的贡献适当地将信用分配给未来的结果是一个长期存在的挑战。目前广泛使用的信用分配方法的假设在决策效果不立即显现的任务中具有劣势。此外，该方法只能评估代理已选择的动作，因此效率极低。本研究探讨后见信用分配的潜力和局限，并提出一种利用环境因果结构的分解状态表示来提高后见信用分配效率的方法。

    Adequately assigning credit to actions for future outcomes based on their contributions is a long-standing open challenge in Reinforcement Learning. The assumptions of the most commonly used credit assignment method are disadvantageous in tasks where the effects of decisions are not immediately evident. Furthermore, this method can only evaluate actions that have been selected by the agent, making it highly inefficient. Still, no alternative methods have been widely adopted in the field. Hindsight Credit Assignment is a promising, but still unexplored candidate, which aims to solve the problems of both long-term and counterfactual credit assignment. In this thesis, we empirically investigate Hindsight Credit Assignment to identify its main benefits, and key points to improve. Then, we apply it to factored state representations, and in particular to state representations based on the causal structure of the environment. In this setting, we propose a variant of Hindsight Credit Assignmen
    
[^183]: 在连续的 SE(3) 轨迹上学习用于动态目标跟踪的策略学习算法

    Policy Learning for Active Target Tracking over Continuous SE(3) Trajectories. (arXiv:2212.01498v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2212.01498](http://arxiv.org/abs/2212.01498)

    本论文提出了一种基于模型的策略梯度算法，用于利用移动机器人跟踪动态目标，通过设计神经网络控制策略和注意力层，从而实现目标熵对网络参数的梯度运算，以实现高效的基于模型的策略梯度优化。

    

    本文提出了一种基于模型的策略梯度算法，用于利用搭载有限视野传感器的移动机器人跟踪动态目标。任务是获取连续的控制策略，以收集传感器测量结果，从而减少目标状态的不确定性，其由目标分布熵度量。我们设计了一个神经网络控制策略，以机器人的 SE(3) 姿态、联合目标分布的均值向量和信息矩阵作为输入，并使用注意力层来处理可变数量的目标。我们还明确导出了目标熵对网络参数的梯度，从而允许高效的基于模型的策略梯度优化。

    This paper proposes a novel model-based policy gradient algorithm for tracking dynamic targets using a mobile robot, equipped with an onboard sensor with limited field of view. The task is to obtain a continuous control policy for the mobile robot to collect sensor measurements that reduce uncertainty in the target states, measured by the target distribution entropy. We design a neural network control policy with the robot $SE(3)$ pose and the mean vector and information matrix of the joint target distribution as inputs and attention layers to handle variable numbers of targets. We also derive the gradient of the target entropy with respect to the network parameters explicitly, allowing efficient model-based policy gradient optimization.
    
[^184]: 使用神经常微分学习鲁棒状态估计器

    Learning Robust State Observers using Neural ODEs (longer version). (arXiv:2212.00866v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2212.00866](http://arxiv.org/abs/2212.00866)

    本研究使用神经常微分学习非线性系统的状态观测，设计了适用于部分已知和完全未知非线性动力学系统的鲁棒观测器，提高观测器的鲁棒性使其更适合实际应用。

    

    本研究利用神经常微分的最新研究成果，提出了一种基于神经常微分的非线性系统状态观测器设计方法，学习部分已知非线性动力学系统的Luenberger观测器及其非线性扩展，同时学习完全未知非线性动力学系统的Kazantzis-Kravaris-Luenberger (KKL)观测器。特别地，对可调节的KKL观测器的设计与其在收敛速度和鲁棒性之间的权衡关系进行了分析，将其用作改善训练中基于学习的观测器的鲁棒性的基础。我们在数值模拟中证明了该方法的优越性。

    Relying on recent research results on Neural ODEs, this paper presents a methodology for the design of state observers for nonlinear systems based on Neural ODEs, learning Luenberger-like observers and their nonlinear extension (Kazantzis-Kravaris-Luenberger (KKL) observers) for systems with partially-known nonlinear dynamics and fully unknown nonlinear dynamics, respectively. In particular, for tuneable KKL observers, the relationship between the design of the observer and its trade-off between convergence speed and robustness is analysed and used as a basis for improving the robustness of the learning-based observer in training. We illustrate the advantages of this approach in numerical simulations.
    
[^185]: NEVIS'22: 从30年计算机视觉研究中取样的100个任务流

    NEVIS'22: A Stream of 100 Tasks Sampled from 30 Years of Computer Vision Research. (arXiv:2211.11747v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11747](http://arxiv.org/abs/2211.11747)

    NEVIS'22基准测试包含了来自计算机视觉会议三十年来均匀取样的100个视觉分类任务，可以反映研究社区的进展和变化。

    

    持续学习、元学习和迁移学习等多个机器学习社区的共同目标是设计出能够高效且鲁棒地适应未知任务的算法和模型。更进一步的目标是构建那些不断适应并通过恰当地传递已获得知识逐渐变得更加高效的模型。除了研究学习算法和模型架构外，我们建立这类模型的还面临许多困难，例如选择学习协议、成功度量和验证研究假设所需的数据等。在本文中，我们引入了NEVIS'22（Never-Ending VIsual-classification Stream），这是一个基准测试，由100个视觉分类任务组成，按时间顺序排序，从计算机视觉会议中均匀取样的论文中提取而来，跨越了过去三十年。得到的数据流反映了研究社区认为有意义的问题和解决方案的变化。

    A shared goal of several machine learning communities like continual learning, meta-learning and transfer learning, is to design algorithms and models that efficiently and robustly adapt to unseen tasks. An even more ambitious goal is to build models that never stop adapting, and that become increasingly more efficient through time by suitably transferring the accrued knowledge. Beyond the study of the actual learning algorithm and model architecture, there are several hurdles towards our quest to build such models, such as the choice of learning protocol, metric of success and data needed to validate research hypotheses. In this work, we introduce the Never-Ending VIsual-classification Stream (NEVIS'22), a benchmark consisting of a stream of over 100 visual classification tasks, sorted chronologically and extracted from papers sampled uniformly from computer vision proceedings spanning the last three decades. The resulting stream reflects what the research community thought was meanin
    
[^186]: 基于CSI的室内定位的简单有效增广方法

    Simple and Effective Augmentation Methods for CSI Based Indoor Localization. (arXiv:2211.10790v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2211.10790](http://arxiv.org/abs/2211.10790)

    本论文提出了两种简单但惊人有效的数据增广算法，基于通道状态信息（CSI）的室内定位可以减少测量次数一个数量级。

    

    室内定位是一项具有挑战性的任务。与GPS在室外环境中占主导地位不同，目前还没有稳健且几乎普遍适用的方法。最近，机器学习（ML）已成为实现准确室内定位的最有前途的方法。然而，其主要挑战是需要大量的数据集来训练神经网络。数据收集程序成本高昂，费时费力，需要针对不同的室内环境进行广泛的测量和标记过程。数据增广（DA）可改善这种情况，它是扩大ML数据集的一般框架，使ML系统更加稳健，并增加其泛化能力。本文提出了两种简单但惊人有效的DA算法，用于基于通道状态信息（CSI）的室内定位，并得到了物理考虑的激励。我们发现，在给定精度要求的情况下，测量次数可能减少一个数量级。具体而言，我们提出了一种基于相似变换的算法和一种基于模拟的算法。

    Indoor localization is a challenging task. Compared to outdoor environments where GPS is dominant, there is no robust and almost-universal approach. Recently, machine learning (ML) has emerged as the most promising approach for achieving accurate indoor localization. Nevertheless, its main challenge is requiring large datasets to train the neural networks. The data collection procedure is costly and laborious, requiring extensive measurements and labeling processes for different indoor environments. The situation can be improved by Data Augmentation (DA), a general framework to enlarge the datasets for ML, making ML systems more robust and increasing their generalization capabilities. This paper proposes two simple yet surprisingly effective DA algorithms for channel state information (CSI) based indoor localization motivated by physical considerations. We show that the number of measurements for a given accuracy requirement may be decreased by an order of magnitude. Specifically, we d
    
[^187]: 一种黎曼ADMM方法

    A Riemannian ADMM. (arXiv:2211.02163v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2211.02163](http://arxiv.org/abs/2211.02163)

    该论文提出了一种适用于机器学习和统计学中重要应用的黎曼交替方向乘子法（ADMM），能够同时处理非凸约束和非光滑的目标函数，并分析了算法得到$\epsilon$-稳定点的迭代复杂度。

    

    我们考虑了一类黎曼优化问题，其中目标函数是在环空间中的光滑函数和非光滑函数之和。这类问题在机器学习和统计学中有重要的应用，例如稀疏主成分分析、稀疏谱聚类和正交字典学习。我们提出了一种黎曼交替方向乘子法（ADMM）来解决这一类问题。每次迭代我们采用易于计算的步骤。在温和的假设下，我们分析了算法得到$\epsilon$-稳定点的迭代复杂度。现有的用于解决非凸问题的ADMM方法要么不允许非凸约束集合，要么不允许非光滑的目标函数。反之，我们的复杂度结果适用于同时具有非光滑目标和流形约束的问题。通过数值实验，我们展示了该算法的优点。

    We consider a class of Riemannian optimization problems where the objective is the sum of a smooth function and a nonsmooth function, considered in the ambient space. This class of problems finds important applications in machine learning and statistics such as the sparse principal component analysis, sparse spectral clustering, and orthogonal dictionary learning. We propose a Riemannian alternating direction method of multipliers (ADMM) to solve this class of problems. Our algorithm adopts easily computable steps in each iteration. The iteration complexity of the proposed algorithm for obtaining an $\epsilon$-stationary point is analyzed under mild assumptions. Existing ADMM for solving nonconvex problems either does not allow nonconvex constraint set, or does not allow nonsmooth objective function. In contrast, our complexity result is established for problems with simultaneous nonsmooth objective and manifold constraint. Numerical experiments are conducted to demonstrate the advanta
    
[^188]: 在子模最大化中平衡效用和公平性（技术报告）

    Balancing Utility and Fairness in Submodular Maximization (Technical Report). (arXiv:2211.00980v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2211.00980](http://arxiv.org/abs/2211.00980)

    本文提出了一个新的问题，称为“二标准子模最大化”，以平衡效用和公平性。该问题要求找到一个固定大小的解，以最大化效用函数为目标。

    

    子模函数最大化是一个基本的组合优化问题，具有许多应用，包括数据汇总、影响力最大化和推荐等。在许多问题中，目标是找到一解，使得对于每个用户，效用函数是单调子模的情况下，平均效用最大化。然而，当用户群体由几个人口统计学分组组成时，另一个关键问题是效用是否公平地分配在不同的群体中。虽然效用和公平目标都是可取的，但它们可能互相矛盾，并且据我们所知，很少有人关注如何一起优化它们。在本文中，我们提出了一个新问题，称为“二标准子模最大化”（BSM），以在效用和公平性之间取得平衡，具体而言，它要求找到一个固定大小的解，以最大化效用函数为目标。

    Submodular function maximization is a fundamental combinatorial optimization problem with plenty of applications -- including data summarization, influence maximization, and recommendation. In many of these problems, the goal is to find a solution that maximizes the average utility over all users, for each of whom the utility is defined by a monotone submodular function. However, when the population of users is composed of several demographic groups, another critical problem is whether the utility is fairly distributed across different groups. Although the \emph{utility} and \emph{fairness} objectives are both desirable, they might contradict each other, and, to the best of our knowledge, little attention has been paid to optimizing them jointly.  In this paper, we propose a new problem called \emph{Bicriteria Submodular Maximization} (BSM) to strike a balance between utility and fairness. Specifically, it requires finding a fixed-size solution to maximize the utility function, subject
    
[^189]: 不需要微调的预训练语言模型剪枝

    Pruning Pre-trained Language Models Without Fine-Tuning. (arXiv:2210.06210v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.06210](http://arxiv.org/abs/2210.06210)

    本文提出了静态模型剪枝（SMP），它只使用一阶剪枝来适应下游任务，同时实现目标稀疏度水平，在大量实验证明SMP具有显著的改进。

    

    为了克服预训练语言模型中过于参数化的问题，我们广泛地使用剪枝作为一种简单和直接的压缩方法，直接去除不重要的权重。先前的一阶方法成功地将PLMs压缩到极高的稀疏性，同时表现几乎不下降，如运动剪枝等。这些方法使用一阶信息来剪枝PLMs，同时微调其余的权重。在这项工作中，我们认为对于一阶剪枝，微调是多余的，因为一阶剪枝足以将PLMs收敛到下游任务，而无需微调。在这个初衷下，我们提出了静态模型剪枝（SMP），它只使用一阶剪枝来使PLMs适应下游任务，同时实现目标稀疏度水平。此外，我们还设计了一个新的蒙版函数和训练目标，以进一步改进SMP。大量各种稀疏度水平下的实验证明了SMP比一阶和零阶方法具有显著的改进。

    To overcome the overparameterized problem in Pre-trained Language Models (PLMs), pruning is widely used as a simple and straightforward compression method by directly removing unimportant weights. Previous first-order methods successfully compress PLMs to extremely high sparsity with little performance drop. These methods, such as movement pruning, use first-order information to prune PLMs while fine-tuning the remaining weights. In this work, we argue fine-tuning is redundant for first-order pruning, since first-order pruning is sufficient to converge PLMs to downstream tasks without fine-tuning. Under this motivation, we propose Static Model Pruning (SMP), which only uses first-order pruning to adapt PLMs to downstream tasks while achieving the target sparsity level. In addition, we also design a new masking function and training objective to further improve SMP. Extensive experiments at various sparsity levels show SMP has significant improvements over first-order and zero-order met
    
[^190]: 解释所有神经网络的方差容忍因子

    Variance Tolerance Factors For Interpreting ALL Neural Networks. (arXiv:2209.13858v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.13858](http://arxiv.org/abs/2209.13858)

    本文提出一种用于解释黑盒神经网络的方差容忍因子（VTF）理论，通过排名特征的方式探索特征的重要性，同时构建一个基本模型和特征模型的新型架构，来探索所有表现良好的神经网络中特征的重要性，并且经过基准测试和应用于实际环境中的实验验证了方法的可靠性。

    

    黑匣子模型只提供深度学习任务的结果，缺乏有关如何获得这些结果的详细信息。知道输入变量与输出的关系，以及为什么它们相关，可以在将预测转化为实验或在受到审查时维护模型预测的关键时刻起到重要作用。在本文中，我们提出了一个一般性理论，通过定义一个受影响函数启发的方差容忍因子（VTF），从排名特征的角度解释黑匣子神经网络中的特征，并构建一个包含基本模型和特征模型的新型架构，以探索包含所有表现良好的神经网络的瑞士军刀集中的特征重要性。创建并探索了两种Rashomon集中的特征重要性排名方法和基于VTF的特征选择方法。我们提供了对合成数据集和基准数据集的彻底评估，并将该方法应用于基因组学和材料科学中的两个真实世界实验。

    Black box models only provide results for deep learning tasks, and lack informative details about how these results were obtained. Knowing how input variables are related to outputs, in addition to why they are related, can be critical to translating predictions into laboratory experiments, or defending a model prediction under scrutiny. In this paper, we propose a general theory that defines a variance tolerance factor (VTF) inspired by influence function, to interpret features in the context of black box neural networks by ranking the importance of features, and construct a novel architecture consisting of a base model and feature model to explore the feature importance in a Rashomon set that contains all well-performing neural networks. Two feature importance ranking methods in the Rashomon set and a feature selection method based on the VTF are created and explored. A thorough evaluation on synthetic and benchmark datasets is provided, and the method is applied to two real world ex
    
[^191]: 应用于角度密度有限的兴趣区计算机断层扫描成像的深度展开DBFB算法

    Deep Unfolding of the DBFB Algorithm with Application to ROI CT Imaging with Limited Angular Density. (arXiv:2209.13264v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2209.13264](http://arxiv.org/abs/2209.13264)

    本文提出了一种用于计算机断层扫描（CT）重建感兴趣区（ROI）的深度展开网络U-RDBFB。它具有快速的重建速度和高重建质量，并且能够有效地处理视角较少的截断数据。

    

    本文提出了一种新的方法，用于从有限数量的计算机断层扫描（CT）测量中重建感兴趣区（ROI）。传统的基于模型的迭代重建方法导致具有可预测特征的图像。但是它们经常受到繁琐的参数化和缓慢的收敛的困扰。相反，深度学习方法快速，并且可以通过利用来自大型数据集的信息来达到高重建质量，但它们缺乏可解释性。在这两种方法的交叉点上，最近提出了深度展开网络。它们的设计包括成像系统的物理学和迭代优化算法的步骤。受这些网络在各种应用中的成功启发，我们介绍了一种名为U-RDBFB的展开神经网络，专门用于从有限数据中重建ROI CT。由于强大的非凸数据保真度项与稀疏诱导正则化项相结合，可以有效地处理少视角截断数据。

    This paper presents a new method for reconstructing regions of interest (ROI) from a limited number of computed tomography (CT) measurements. Classical model-based iterative reconstruction methods lead to images with predictable features. Still, they often suffer from tedious parameterization and slow convergence. On the contrary, deep learning methods are fast, and they can reach high reconstruction quality by leveraging information from large datasets, but they lack interpretability. At the crossroads of both methods, deep unfolding networks have been recently proposed. Their design includes the physics of the imaging system and the steps of an iterative optimization algorithm. Motivated by the success of these networks for various applications, we introduce an unfolding neural network called U-RDBFB designed for ROI CT reconstruction from limited data. Few-view truncated data are effectively handled thanks to a robust non-convex data fidelity term combined with a sparsity-inducing r
    
[^192]: 学习信息理论主动感知的连续控制策略

    Learning Continuous Control Policies for Information-Theoretic Active Perception. (arXiv:2209.12427v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2209.12427](http://arxiv.org/abs/2209.12427)

    本文提出了一种使用信息理论代价来学习主动地标定位和探索的连续控制策略方法，该方法使用卡尔曼滤波器转换部分可观察问题为MDP，使用视野来塑造奖励，使用基于注意力机制的神经网络来表示控制策略，并与主动体积建图结合以促进地标定位和探索。

    

    本文提出了一种使用信息理论代价来学习主动地标定位和探索的连续控制策略方法。我们考虑一个移动机器人在有限感知范围内检测地标，并解决学习控制策略的问题，该策略最大化地标状态与传感器观测之间的互信息。我们采用卡尔曼滤波器把地标状态的部分可观察问题转换为马尔科夫决策过程（MDP），使用可微分的视野来塑造奖励，并使用基于注意力机制的神经网络来表示控制策略。该方法进一步与主动体积建图结合起来，以促进地标定位和探索。在与基准方法的比较中，本文展示了在几个模拟的地标定位任务中的性能。

    This paper proposes a method for learning continuous control policies for active landmark localization and exploration using an information-theoretic cost. We consider a mobile robot detecting landmarks within a limited sensing range, and tackle the problem of learning a control policy that maximizes the mutual information between the landmark states and the sensor observations. We employ a Kalman filter to convert the partially observable problem in the landmark state to Markov decision process (MDP), a differentiable field of view to shape the reward, and an attention-based neural network to represent the control policy. The approach is further unified with active volumetric mapping to promote exploration in addition to landmark localization. The performance is demonstrated in several simulated landmark localization tasks in comparison with benchmark methods.
    
[^193]: 引入变分因果推理的目标条件强化学习的泛化

    Generalizing Goal-Conditioned Reinforcement Learning with Variational Causal Reasoning. (arXiv:2207.09081v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.09081](http://arxiv.org/abs/2207.09081)

    本文提出了一种增强的目标条件强化学习框架，它使用因果图来发现和表示因果关系来实现模型的泛化性。

    

    推理作为人类智能中达成通用解决方案的关键部分，通过总结部分到整体的参数和发现因果关系，为强化学习（RL）代理人实现向各种目标的泛化提供了巨大的潜力。然而，如何发现和表示因果关系仍然是阻碍因果RL发展的重大障碍。在本文中，我们通过使用因果图结构来增强目标条件RL（GCRL），该结构建立在对象和事件之间的关系上。我们将GCRL问题新颖地制定为具有CG作为潜在变量的变分似然最大化。为了优化派生目标，我们提出了一个具有理论性能保证的框架，交替使用干预数据来估计CG的后验概率，使用CG来学习通用模型和可解释的策略。由于缺乏验证推理下泛化能力的公共基准，我们设计了...

    As a pivotal component to attaining generalizable solutions in human intelligence, reasoning provides great potential for reinforcement learning (RL) agents' generalization towards varied goals by summarizing part-to-whole arguments and discovering cause-and-effect relations. However, how to discover and represent causalities remains a huge gap that hinders the development of causal RL. In this paper, we augment Goal-Conditioned RL (GCRL) with Causal Graph (CG), a structure built upon the relation between objects and events. We novelly formulate the GCRL problem into variational likelihood maximization with CG as latent variables. To optimize the derived objective, we propose a framework with theoretical performance guarantees that alternates between two steps: using interventional data to estimate the posterior of CG; using CG to learn generalizable models and interpretable policies. Due to the lack of public benchmarks that verify generalization capability under reasoning, we design 
    
[^194]: 对于二元分类问题，对抗性训练的一致性研究

    The Consistency of Adversarial Training for Binary Classification. (arXiv:2206.09099v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.09099](http://arxiv.org/abs/2206.09099)

    研究了对于绝对连续于勒贝格度量的分布，哪些基于上确界的代理风险是一致的；定量计算了对抗性代理风险与对抗性分类风险的关系；探讨了对抗性训练的 $\cH$- 一致性的影响。

    

    在现代机器学习中，对抗扰动的鲁棒性是至关重要的。训练强健分类器的最先进方法之一是对抗性训练，其中涉及最小化基于上确界的代理风险。代理风险的统计一致性在标准机器学习的背景下已经被很好地理解，但在对抗性背景下仍有待进一步研究。在本文中，我们刻画了绝对连续于勒贝格度量的分布中哪些基于上确界的代理风险是一致的，同时我们获得了有关对抗性代理风险与对抗性分类风险的定量界限。最后，我们讨论了对抗性训练的 $\cH$- 一致性的影响。

    Robustness to adversarial perturbations is of paramount concern in modern machine learning. One of the state-of-the-art methods for training robust classifiers is adversarial training, which involves minimizing a supremum-based surrogate risk. The statistical consistency of surrogate risks is well understood in the context of standard machine learning, but not in the adversarial setting. In this paper, we characterize which supremum-based surrogates are consistent for distributions absolutely continuous with respect to Lebesgue measure in binary classification. Furthermore, we obtain quantitative bounds relating adversarial surrogate risks to the adversarial classification risk. Lastly, we discuss implications for the $\cH$-consistency of adversarial training.
    
[^195]: 一种使用Transformer结构并利用百万级样本上下文进行原始音频的语言模型

    A Language Model With Million Sample Context For Raw Audio Using Transformer Architectures. (arXiv:2206.08297v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2206.08297](http://arxiv.org/abs/2206.08297)

    本文提出了一种采用Transformer结构和百万级样本上下文进行原始音频语言模型的自回归生成架构，能够高效地建模音频信号的长期依赖性，并取得了最先进的性能表现。

    

    对于音频信号进行长期依赖性建模是一个特别具有挑战性的问题，因为即使在小的时间尺度上，也会产生数十万个样本。最近，随着Transformer的出现，神经结构变得擅长于对长期依赖性建模，但它们受到二次约束的影响。我们提出了一种生成自回归架构，可以模拟相当大的上下文超过500,000个样本的音频波形。我们的工作通过使用CNN前端来学习潜在表示，然后使用Transformer编码器在这些表示之上学习依赖项，完全端对端地进行了训练：从而允许它根据下一个样本自行学习表示。与以前用不同的时间尺度进行比较以展示改进的作品不同，我们使用标准数据集，并使用相同数目的参数/上下文显示了改进。我们实现了最先进的性能。

    Modeling long-term dependencies for audio signals is a particularly challenging problem, as even small-time scales yield on the order of a hundred thousand samples. With the recent advent of Transformers, neural architectures became good at modeling dependencies over longer time scales, but they suffered from quadratic constraints to scale them. We propose a generative auto-regressive architecture that can model audio waveforms over quite a large context, greater than 500,000 samples. Our work is adapted to learn time dependencies by learning a latent representation by a CNN front-end, and then learning dependencies over these representations using Transformer encoders, fully trained end-to-end: thereby allowing to learn representations as it deems fit for the next sample. Unlike previous works that compared different time scales to show improvement, we use a standard dataset, with the same number of parameters/context to show improvements. We achieve a state-of-the-art performance as 
    
[^196]: 时机至关重要：学习在代价高昂的行动和预算限制下进行选择性行动

    Timing is Everything: Learning to Act Selectively with Costly Actions and Budgetary Constraints. (arXiv:2205.15953v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.15953](http://arxiv.org/abs/2205.15953)

    LICRA是学习在代价高昂的行动和预算限制下进行选择性行动的强化学习框架。

    

    许多实际应用场景中，执行行动都会产生成本；金融系统中的交易成本和燃油成本是常见的例子。在这些情况下，每个时间步骤执行行动迅速积累成本，导致极其不理想的结果。此外，反复行动会产生磨损和最终损坏。确定“何时行动”对于实现成功的结果至关重要，然而，在行动产生最小限制成本的情况下，高效地“学习”行为最优策略的挑战仍未得到解决。本文介绍了一种强化学习（RL）框架，名为Learnable Impulse Control Reinforcement Algorithm（LICRA），用于在行动产生成本的情况下学习选择何时行动和采取哪些行动以实现最优选择。

    Many real-world settings involve costs for performing actions; transaction costs in financial systems and fuel costs being common examples. In these settings, performing actions at each time step quickly accumulates costs leading to vastly suboptimal outcomes. Additionally, repeatedly acting produces wear and tear and ultimately, damage. Determining \textit{when to act} is crucial for achieving successful outcomes and yet, the challenge of efficiently \textit{learning} to behave optimally when actions incur minimally bounded costs remains unresolved. In this paper, we introduce a reinforcement learning (RL) framework named \textbf{L}earnable \textbf{I}mpulse \textbf{C}ontrol \textbf{R}einforcement \textbf{A}lgorithm (LICRA), for learning to optimally select both when to act and which actions to take when actions incur costs. At the core of LICRA is a nested structure that combines RL and a form of policy known as \textit{impulse control} which learns to maximise objectives when actions
    
[^197]: 通过雅可比控制选择高斯核岭回归带宽

    Bandwidth Selection for Gaussian Kernel Ridge Regression via Jacobian Control. (arXiv:2205.11956v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.11956](http://arxiv.org/abs/2205.11956)

    本文提出了一种基于雅可比控制的带宽选择启发式方法，该方法具有闭式、计算非常轻的特点，并且在关注带宽的同时可以获得更好的模型泛化性能。

    

    大多数机器学习方法需要调整超参数。对于高斯核岭回归，超参数是带宽。带宽指定核函数的长度尺度，必须小心选择才能获得具有良好泛化性能模型。带宽选择的默认方法是交叉验证和边缘似然最大化，这通常会产生良好的结果，尽管计算成本高。此外，这些方法提供的估计往往具有非常高的方差，特别是在训练数据不足时。受雅可比正则化的启发，我们制定了一个近似表达式，用于描述高斯核岭回归推断函数的导数如何取决于核带宽。然后，我们使用这个表达式来提出一种基于雅可比控制的闭式、计算非常轻的带宽选择启发式方法。此外，这个雅可比表达式表明了在检查带宽选择的质量时应关注什么。

    Most machine learning methods require tuning of hyper-parameters. For kernel ridge regression with the Gaussian kernel, the hyper-parameter is the bandwidth. The bandwidth specifies the length-scale of the kernel and has to be carefully selected in order to obtain a model with good generalization. The default methods for bandwidth selection is cross-validation and marginal likelihood maximization, which often yields good results, albeit at high computational costs. Furthermore, the estimates provided by these methods tend to have very high variance, especially when training data are scarce. Inspired by Jacobian regularization, we formulate an approximate expression for how the derivatives of the functions inferred by kernel ridge regression with the Gaussian kernel depend on the kernel bandwidth. We then use this expression to propose a closed-form, computationally feather-light, bandwidth selection heuristic based on controlling the Jacobian. In addition, the Jacobian expression illum
    
[^198]: 未训练生成器网络的潜空间解缠方法用于隔离视频数据中的不同运动类型

    Latent-space disentanglement with untrained generator networks for the isolation of different motion types in video data. (arXiv:2205.10367v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2205.10367](http://arxiv.org/abs/2205.10367)

    本文通过未经训练的生成器网络与特定的潜空间解缠技术方法，仅利用最小的底层动态信息，有效隔离视频数据中不同非线性运动类型，而不需要预训练模型。

    

    隔离视频数据中不同类型的动作是视频分析中一个非常相关的问题。例如在动态医学或生物成像中，感兴趣动态的分析和进一步处理通常会因为附加的非关键动态（例如被测主体的运动）而变得复杂。本文通过实证分析表明，利用未经训练的生成器网络对视频数据进行表示，再结合一种特殊的潜空间解缠技术，可仅利用对一些底层动态的最小一维信息，有效隔离不同的高度非线性的运动类型。特别地，这种表示允许冻结任何动作类型的选择，并获得所需的其他关键动态的准确独立表示。获得这种表示不需要对训练数据集进行任何预训练，即生成器网络的所有参数都是直接学习得出的。

    Isolating different types of motion in video data is a highly relevant problem in video analysis. Applications can be found, for example, in dynamic medical or biological imaging, where the analysis and further processing of the dynamics of interest is often complicated by additional, unwanted dynamics, such as motion of the measurement subject. In this work, it is empirically shown that a representation of video data via untrained generator networks, together with a specific technique for latent space disentanglement that uses minimal, one-dimensional information on some of the underlying dynamics, allows to efficiently isolate different, highly non-linear motion types. In particular, such a representation allows to freeze any selection of motion types, and to obtain accurate independent representations of other dynamics of interest. Obtaining such a representation does not require any pre-training on a training data set, i.e., all parameters of the generator network are learned direc
    
[^199]: 《邻域注意力变换器》

    Neighborhood Attention Transformer. (arXiv:2204.07143v5 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2204.07143](http://arxiv.org/abs/2204.07143)

    提出了针对视觉任务的高效和可扩展的滑动窗口注意力机制——邻域关注（NA）。基于NA，开发了NAT，NAT-Tiny在ImageNet上达到了83.2％的top-1准确率，能够提高图像分类和下游视觉性能。

    

    我们提出了“邻域关注”（NA），这是第一种针对视觉任务的高效和可扩展的滑动窗口注意力机制。NA是一种像素级运算，将自注意力（SA）局限于最近的相邻像素，因此与SA的二次复杂度相比，具有线性的时间和空间复杂度。滑动窗口模式使NA的感受野能够增长而不需要额外的像素移位，并且保留了平移等变性，这与Swin Transformer的窗口自注意力（WSA）不同。我们开发了NATTEN（邻域关注扩展），这是一个具有高效的C++和CUDA内核的Python包，使NA的运行速度比Swin的WSA快高达40％，同时使用的内存少了25％。我们进一步提出了基于NA的新层次结构变换器设计——邻域关注变换器（NAT），以提高图像分类和下游视觉性能。在NAT上的实验结果表明具有竞争力，NAT-Tiny在ImageNet上达到了83.2％的top-1准确率和51.4％的mAP。

    We present Neighborhood Attention (NA), the first efficient and scalable sliding-window attention mechanism for vision. NA is a pixel-wise operation, localizing self attention (SA) to the nearest neighboring pixels, and therefore enjoys a linear time and space complexity compared to the quadratic complexity of SA. The sliding-window pattern allows NA's receptive field to grow without needing extra pixel shifts, and preserves translational equivariance, unlike Swin Transformer's Window Self Attention (WSA). We develop NATTEN (Neighborhood Attention Extension), a Python package with efficient C++ and CUDA kernels, which allows NA to run up to 40% faster than Swin's WSA while using up to 25% less memory. We further present Neighborhood Attention Transformer (NAT), a new hierarchical transformer design based on NA that boosts image classification and downstream vision performance. Experimental results on NAT are competitive; NAT-Tiny reaches 83.2% top-1 accuracy on ImageNet, 51.4% mAP on M
    
[^200]: FedComm: 基于联邦学习的隐蔽通信方法研究

    FedComm: Federated Learning as a Medium for Covert Communication. (arXiv:2201.08786v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2201.08786](http://arxiv.org/abs/2201.08786)

    本文探究了联邦学习作为隐蔽通信媒介的可能性，提出了一种名为FedComm的隐蔽通信技术，其有效地实现了在FL框架内共享和传输有针对性的负载，使得通信更为隐蔽，且技术检测困难。

    

    为了减轻深度学习带来的隐私问题，联邦学习（FL）被提出作为一种解决方案，大量参与者能够成功训练深度神经网络，无需向外界透露私密训练数据。本文深入探究了FL方案的通信能力，并提出了FedComm，一种多系统隐蔽通信技术，能够在FL框架内稳健地共享和传输有针对性的负载。我们的理论和实证评估表明，FedComm提供了一个隐蔽的通信通道，对训练过程的干扰最小。实验结果表明，FedComm实现了高传输速率和低检测率，是实际场景中隐蔽通信的有前景的候选方法。

    Proposed as a solution to mitigate the privacy implications related to the adoption of deep learning, Federated Learning (FL) enables large numbers of participants to successfully train deep neural networks without having to reveal the actual private training data. To date, a substantial amount of research has investigated the security and privacy properties of FL, resulting in a plethora of innovative attack and defense strategies. This paper thoroughly investigates the communication capabilities of an FL scheme. In particular, we show that a party involved in the FL learning process can use FL as a covert communication medium to send an arbitrary message. We introduce FedComm, a novel multi-system covert-communication technique that enables robust sharing and transfer of targeted payloads within the FL framework. Our extensive theoretical and empirical evaluations show that FedComm provides a stealthy communication channel, with minimal disruptions to the training process. Our experi
    
[^201]: 基于凸约束的优化问题中DNN方案的可行性保证及其在直流最优潮流问题中的应用

    Ensuring DNN Solution Feasibility for Optimization Problems with Convex Constraints and Its Application to DC Optimal Power Flow Problems. (arXiv:2112.08091v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.08091](http://arxiv.org/abs/2112.08091)

    本文提出了一个“预防性学习”框架，以在满足对约束标定的条件下，保证具有凸约束和一般目标函数的问题的DNN解的可行性，而无需后处理。通过系统标定不等式约束，我们预示预测误差并确保所得到的解仍然是可行的。同时提出了一种新的对抗样本感知的训练算法以提高DNN的最优性能而不牺牲可行性保证。

    

    在开发用于解决受限制优化问题的深度神经网络（DNN）方案时，确保解的可行性是一个关键挑战，由于DNN固有的预测误差。本文提出了一个“预防性学习”框架，以在满足对约束标定的温和条件下，保证具有凸约束和一般目标函数的问题的DNN解的可行性，而无需后处理。我们无失一般性地关注只有不等式约束的问题。我们系统地标定DNN训练中使用的不等式约束，从而预示预测误差并确保所得到的解仍然是可行的。我们表征了标定量和DNN大小足以确保通用可行性。我们提出了一种新的对抗样本感知的训练算法，以提高DNN的最优性能，而不会牺牲可行性保证。总的来说，该框架提供了两个DNN解。

    Ensuring solution feasibility is a key challenge in developing Deep Neural Network (DNN) schemes for solving constrained optimization problems, due to inherent DNN prediction errors. In this paper, we propose a ``preventive learning'' framework to guarantee DNN solution feasibility for problems with convex constraints and general objective functions without post-processing, upon satisfying a mild condition on constraint calibration. Without loss of generality, we focus on problems with only inequality constraints. We systematically calibrate inequality constraints used in DNN training, thereby anticipating prediction errors and ensuring the resulting solutions remain feasible. We characterize the calibration magnitudes and the DNN size sufficient for ensuring universal feasibility. We propose a new Adversarial-Sample Aware training algorithm to improve DNN's optimality performance without sacrificing feasibility guarantee. Overall, the framework provides two DNNs. The first one from ch
    
[^202]: 非线性算子的伪逆的理论基础

    Theoretical Foundations for Pseudo-Inversion of Nonlinear Operators. (arXiv:2111.10755v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2111.10755](http://arxiv.org/abs/2111.10755)

    本文研究了非线性算子的伪逆，包括其存在性和唯一性条件以及性质分析，给出了一些众所周知的不可逆非线性算子PI的解析表达式，并讨论了其与小波阈值的关系。

    

    Moore-Penrose伪逆在物理学、统计学和各个工程领域被广泛使用。在数据科学中，非线性算子被广泛使用。本文研究了非线性算子的伪逆，广义地定义了这个概念，首先对于一般集合，然后对于赋范空间进行了细化。当算子是矩阵时，赋范空间的PI产生了Moore-Penrose伪逆。我们给出了PI存在和唯一性的条件，并建立了关于其性质的理论结果，如连续性、算子组合和投影算子的价值等。我们对一些众所周知的不可逆非线性算子的PI给出了解析表达式，例如硬/软阈值和ReLU。最后，我们分析了一个神经层，并讨论了与小波阈值有关的关系。

    The Moore-Penrose inverse is widely used in physics, statistics, and various fields of engineering. It captures well the notion of inversion of linear operators in the case of overcomplete data. In data science, nonlinear operators are extensively used. In this paper we characterize the fundamental properties of a pseudo-inverse (PI) for nonlinear operators.  The concept is defined broadly. First for general sets, and then a refinement for normed spaces. The PI for normed spaces yields the Moore-Penrose inverse when the operator is a matrix. We present conditions for existence and uniqueness of a PI and establish theoretical results investigating its properties, such as continuity, its value for operator compositions and projection operators, and others. Analytic expressions are given for the PI of some well-known, non-invertible, nonlinear operators, such as hard- or soft-thresholding and ReLU. Finally, we analyze a neural layer and discuss relations to wavelet thresholding.
    
[^203]: 子采样学习的网络嵌入的渐近分析

    Asymptotics of Network Embeddings Learned via Subsampling. (arXiv:2107.02363v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2107.02363](http://arxiv.org/abs/2107.02363)

    本研究将网络嵌入方法封装为一个统一框架，并从理论上证明了使用子采样学习的网络嵌入的渐近分布，同时提供了潜在参数的收敛速率和算法选择与统计效率之间的权衡。

    

    网络数据在现代机器学习中无处不在，相关任务包括节点分类、节点聚类和链接预测。一种常用的方法是首先学习网络的欧几里得嵌入，然后应用于向量值数据开发的算法。对于大型网络，可以使用随机梯度方法学习嵌入，其中子采样方案可以自由选择。尽管这种方法具有强大的实证性能，但它们的理论理解还不够充分。我们的工作将诸如node2vec之类的表示方法封装到一个统一的框架中。在假设图是可交换的情况下，我们证明了学习到的嵌入向量的分布在渐近意义下分解。此外，我们根据潜在参数，包括损失函数和嵌入维数的选择，表征了渐近分布并提供了收敛速率。这为使用子采样学习的网络嵌入提供了基本见解，并阐明了算法选择和统计效率之间的权衡。

    Network data are ubiquitous in modern machine learning, with tasks of interest including node classification, node clustering and link prediction. A frequent approach begins by learning an Euclidean embedding of the network, to which algorithms developed for vector-valued data are applied. For large networks, embeddings are learned using stochastic gradient methods where the sub-sampling scheme can be freely chosen. Despite the strong empirical performance of such methods, they are not well understood theoretically. Our work encapsulates representation methods using a subsampling approach, such as node2vec, into a single unifying framework. We prove, under the assumption that the graph is exchangeable, that the distribution of the learned embedding vectors asymptotically decouples. Moreover, we characterize the asymptotic distribution and provided rates of convergence, in terms of the latent parameters, which includes the choice of loss function and the embedding dimension. This provid
    
[^204]: 分层学习：一种用于改善协变量漂移下学习的一般性统计方法

    Stratified Learning: A General-Purpose Statistical Method for Improved Learning under Covariate Shift. (arXiv:2106.11211v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2106.11211](http://arxiv.org/abs/2106.11211)

    该论文提出了一种用于处理训练集不具代表性的协变量漂移情况下改进监督式学习的分层学习方法，并在宇宙学领域的两个问题中证明了其有效性，大幅提升了目标预测结果。

    

    我们提出了一种简单、具有统计学原理和理论基础的方法，用于在训练集不具代表性的情况下改进监督学习，这种情况被称为协变量漂移。我们建立在因果推断中一种成熟的方法基础之上，表明协变量漂移的影响可以通过在因次分数上进行条件约束来减少或消除。在实践中，通过在估计的因次分数的基础上对数据进行分层构造，从而实现平衡协变量，显著提高目标预测结果。我们在两个现代宇宙学研究问题上证明了我们这种一般性方法的有效性，超越了最先进的重要性加权方法。我们在更新的“超新星光度分类挑战”中获得了最好的AUC值（0.958），并改进了现有的SDSS数据中的星系红移条件密度估计。

    We propose a simple, statistically principled, and theoretically justified method to improve supervised learning when the training set is not representative, a situation known as covariate shift. We build upon a well-established methodology in causal inference, and show that the effects of covariate shift can be reduced or eliminated by conditioning on propensity scores. In practice, this is achieved by fitting learners within strata constructed by partitioning the data based on the estimated propensity scores, leading to approximately balanced covariates and much-improved target prediction. We demonstrate the effectiveness of our general-purpose method on two contemporary research questions in cosmology, outperforming state-of-the-art importance weighting methods. We obtain the best reported AUC (0.958) on the updated "Supernovae photometric classification challenge", and we improve upon existing conditional density estimation of galaxy redshift from Sloan Data Sky Survey (SDSS) data.
    
[^205]: DiGS：基于散度导向的未定向点云形状隐式神经表示

    DiGS : Divergence guided shape implicit neural representation for unoriented point clouds. (arXiv:2106.10811v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2106.10811](http://arxiv.org/abs/2106.10811)

    本文提出了一种新的散度导向形状表示学习方法，无需输入法向量，以软约束倾向于平滑解决方案，可靠地定位每个点的未知法向量的梯度，甚至比使用真实法向量更好。此外，还引入了一种新的正弦INR几何初始化方法，以进一步提高收敛性能。

    

    形状隐式神经表示（INR）最近已经在形状分析和重建任务中表现出有效性。现有的INR需要几何坐标来学习形状的隐式水平集。当每个点都有法向量时，可以学习到更高保真度的表示，然而通常原始数据中很少提供法向量。此外，已经表明方法的初始化对于表面重构起着至关重要的作用。在本文中，我们提出了一种不需要法向量作为输入的散度导向形状表示学习方法。我们展示了在距离函数的散度上添加软约束会倾向于平滑的解决方案，从而可靠地定位每个点的未知法向量的梯度，某些情况下甚至比直接使用地面真实法向量的方法表现得更好。此外，我们还引入了一种新的正弦INR几何初始化方法，进一步改善了收敛性能。

    Shape implicit neural representations (INRs) have recently shown to be effective in shape analysis and reconstruction tasks. Existing INRs require point coordinates to learn the implicit level sets of the shape. When a normal vector is available for each point, a higher fidelity representation can be learned, however normal vectors are often not provided as raw data. Furthermore, the method's initialization has been shown to play a crucial role for surface reconstruction. In this paper, we propose a divergence guided shape representation learning approach that does not require normal vectors as input. We show that incorporating a soft constraint on the divergence of the distance function favours smooth solutions that reliably orients gradients to match the unknown normal at each point, in some cases even better than approaches that use ground truth normal vectors directly. Additionally, we introduce a novel geometric initialization method for sinusoidal INRs that further improves conve
    
[^206]: 带有稳定性折衷的选择聚类数目 $K$：一种内部验证标准

    Selecting the Number of Clusters $K$ with a Stability Trade-off: an Internal Validation Criterion. (arXiv:2006.08530v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.08530](http://arxiv.org/abs/2006.08530)

    提出了一种新的聚类验证标准，基于聚类稳定性的内部验证原则，在聚类稳定性和聚类质量方面胜过现有的方法。

    

    模型选择是非参数聚类中的主要挑战之一。毫无疑问，没有可以作为标准答案的真实数据存在，因此评价聚类结果的通用方法尚未出现。聚类目标的不确定性导致了普遍接受的评价标准难以确定。在这方面，聚类稳定性作为一种自然且无需模型的原则而出现：聚类算法应发现数据中稳定的结构。如果数据集从相同的基础分布中重复采样，则算法应找到相似的分区。然而，单纯的稳定性并不适合确定聚类数目。例如，它无法检测聚类数目是否太小。我们提出了一个新的原则：一种好的聚类应该是稳定的，且在每个聚类内部，不存在稳定的子分区。这个原则带来了一种基于聚类稳定性的新型聚类验证标准，克服了传统基于稳定性标准的局限性。我们的框架计算效率高且易于实现。我们在合成和真实世界数据集上展示了我们的标准能够以高精度恢复真实的聚类数目，并且在聚类稳定性和聚类质量方面胜过现有的方法。

    Model selection is a major challenge in non-parametric clustering. There is no universally admitted way to evaluate clustering results for the obvious reason that no ground truth is available. The difficulty to find a universal evaluation criterion is a consequence of the ill-defined objective of clustering. In this perspective, clustering stability has emerged as a natural and model-agnostic principle: an algorithm should find stable structures in the data. If data sets are repeatedly sampled from the same underlying distribution, an algorithm should find similar partitions. However, stability alone is not well-suited to determine the number of clusters. For instance, it is unable to detect if the number of clusters is too small. We propose a new principle: a good clustering should be stable, and within each cluster, there should exist no stable partition. This principle leads to a novel clustering validation criterion based on between-cluster and within-cluster stability, overcoming 
    

