# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Towards Optimal Feature-Shaping Methods for Out-of-Distribution Detection](https://arxiv.org/abs/2402.00865) | 本研究针对越界检测，通过优化特征塑造方法，提出了一种抽象优化框架和具体简化方法，以增强特征塑造方法的泛化能力，并提供了一个无法访问越界数据的闭式解决方案。 |
| [^2] | [Early Time Classification with Accumulated Accuracy Gap Control](https://arxiv.org/abs/2402.00857) | 本文介绍了一种早期时间分类算法，该算法通过统计框架和校准停止规则实现了对完全分类与早期时间分类之间的准确度间隔的有限样本、分布无关的控制。其主要贡献是提出了一种框架，该框架在累计停止时间的条件下控制了一种更强的错误概念的准确度间隔。 |
| [^3] | [SymbolicAI: A framework for logic-based approaches combining generative models and solvers](https://arxiv.org/abs/2402.00854) | SymbolicAI是一个基于逻辑的框架，将生成模型与多种求解器无缝集成，通过将大型语言模型作为语义解析器，实现了符号推理与生成式人工智能的融合。 |
| [^4] | [LTAU-FF: Loss Trajectory Analysis for Uncertainty in Atomistic Force Fields](https://arxiv.org/abs/2402.00853) | LTAU-FF是一种利用损失轨迹分析来估计原子力场中不确定性的方法，通过累积分布函数和模型潜空间的相似性搜索，实现了高效的模型集合表示和不确定度量，无需评估多个模型，能准确预测测试误差。 |
| [^5] | [Data Augmentation Scheme for Raman Spectra with Highly Correlated Annotations](https://arxiv.org/abs/2402.00851) | 该论文提出了一种利用光谱可加性的方法，从给定数据集中生成具有统计独立标签的额外数据点，用于训练能够处理非高斯噪声和非线性依赖关系的卷积神经网络。 |
| [^6] | [Score-based Causal Representation Learning: Linear and General Transformations](https://arxiv.org/abs/2402.00849) | 这篇论文提出了一种基于得分的算法类，用于干预范围内的因果表示学习，涵盖了线性和一般转化。算法保证了可识别性和实现性，并且通过创造性地将得分函数与因果表示学习相结合。 |
| [^7] | [X-CBA: Explainability Aided CatBoosted Anomal-E for Intrusion Detection System](https://arxiv.org/abs/2402.00839) | 本论文介绍了一种名为X-CBA的新颖可解释型IDS方法，该方法利用图神经网络处理网络流量数据，并采用了新的可解释人工智能方法。它通过利用更广泛的流量数据，包括边属性，以处理网络威胁的挑战。 |
| [^8] | [ALISON: Fast and Effective Stylometric Authorship Obfuscation](https://arxiv.org/abs/2402.00835) | ALISON是一种快速有效的风格学作者身份混淆方法，通过攻击AA模型来保护隐私，相比竞争方法，具有更快的混淆速度和更好的混淆成功率。 |
| [^9] | [A YANG-aided Unified Strategy for Black Hole Detection for Backbone Networks](https://arxiv.org/abs/2402.00831) | 该论文介绍了一种使用YANG数据模型与黑洞敏感度量矩阵分析的方法，用于在骨干网络中进行黑洞检测。这种方法填补了骨干网络黑洞检测方法的空白，提供了有效的检测策略。 |
| [^10] | [Resolution invariant deep operator network for PDEs with complex geometries](https://arxiv.org/abs/2402.00825) | 本文提出了一种名为分辨率不变的深度算子网络（RDO）的新框架，通过解耦输入和输出的空间域来解决神经算子（NO）的应用限制问题，可以解决具有复杂几何体的PDE。 (RDO is a novel framework that decouples the spatial domain of input and output, addressing the limitation of neural operators (NO) and enabling the resolution of PDEs with complex geometries.) |
| [^11] | [SLIM: Skill Learning with Multiple Critics](https://arxiv.org/abs/2402.00823) | SLIM是一种多判别器学习方法，通过在机器人操作中组合多个判别器的奖励函数，显著改善了潜变量技能发现，克服了奖励之间的干扰。 |
| [^12] | [Leveraging Approximate Model-based Shielding for Probabilistic Safety Guarantees in Continuous Environments](https://arxiv.org/abs/2402.00816) | 本文提出了在连续环境中利用近似基于模型的屏蔽技术实现概率安全保证的方法，并通过与约束强化学习算法的对比实验证明了其通用性和稳定性。 |
| [^13] | [An Analysis of the Variance of Diffusion-based Speech Enhancement](https://arxiv.org/abs/2402.00811) | 本研究发现，方差的规模是影响语音增强性能的主要参数，较大的方差可增加噪声抑制并减少计算量。 |
| [^14] | [Position Paper: Bayesian Deep Learning in the Age of Large-Scale AI](https://arxiv.org/abs/2402.00809) | 《在大规模人工智能时代的贝叶斯深度学习》这篇立场论文探讨了贝叶斯深度学习在各种不同设置下的优势，并指出了与之相关的挑战和有趣的研究方向。未来的研究重点将放在如何将大规模基础模型与贝叶斯深度学习相结合，以发挥它们的全部潜力。 |
| [^15] | [Distilling Conditional Diffusion Models for Offline Reinforcement Learning through Trajectory Stitching](https://arxiv.org/abs/2402.00807) | 通过新的轨迹拼接算法和奖励生成器，使用条件性扩散模型生成的高回报轨迹与原始轨迹混合，应用于行为克隆的结果表明，学得的规模较小的浅层策略在多个D4RL基准测试中超过或接近深度生成规划器。 |
| [^16] | [Signal Quality Auditing for Time-series Data](https://arxiv.org/abs/2402.00803) | 这个论文介绍了一个开源软件工具包，用于评估时间序列数据的信号质量，提供了一系列信号质量指标和去噪方法的研究，并提供了基准数据的验证。 |
| [^17] | [Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents](https://arxiv.org/abs/2402.00798) | 本文提出了一种将自然语言和形式语言整合的“正式-LLM”框架，用于解决现有LLM智能体无法控制的计划生成问题。实验证明，该框架在提高生成计划性能和确保可控性方面取得了显著改进。 |
| [^18] | [LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law](https://arxiv.org/abs/2402.00795) | 本文研究了预训练语言模型LLMs对动力系统行为的外推能力，发现LLaMA 2能够准确预测动力系统的时间序列。此外，输入上下文窗口的长度越长，学习到的物理规则的准确性越高，揭示了一种上下文中的神经比例定律。 |
| [^19] | [ReAGent: Towards A Model-agnostic Feature Attribution Method for Generative Language Models](https://arxiv.org/abs/2402.00794) | 本论文介绍了一种面向生成语言模型的模型无关特征归因方法，称为ReAGent。该方法解决了现有方法在文本生成中的适用性问题，并提供了计算上效率更高的选择。 |
| [^20] | [Distinguishing the Indistinguishable: Human Expertise in Algorithmic Prediction](https://arxiv.org/abs/2402.00793) | 本论文介绍了一种新的框架，将人类专业知识纳入算法预测中，重点在于利用人的判断力区分对于任何可行的预测算法来说“看起来相同”的输入。 |
| [^21] | [Graph-Mamba: Towards Long-Range Graph Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2402.00789) | Graph-Mamba是第一个尝试通过将Mamba模块与输入相关的节点选择机制集成来增强图网络中长程上下文建模的方法。 |
| [^22] | [Learning and Calibrating Heterogeneous Bounded Rational Market Behaviour with Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2402.00787) | 本论文提出了一种方法，利用多智能体强化学习学习和校准异质有界理性市场行为。这种方法可以消除在基于智能体的模型中手动定义行为规则的需要，并将智能体表示与经济和金融模型相一致。 |
| [^23] | [CroissantLLM: A Truly Bilingual French-English Language Model](https://arxiv.org/abs/2402.00786) | CroissantLLM是一个1.3B的双语语言模型，通过使用1:1的英语-法语预训练数据比例、自定义的分词器和双语调优数据集进行训练，实现了高性能和开源。模型还发布了训练数据集和多个检查点，以及一个法语基准测试 FrenchBench。 |
| [^24] | [Dense Reward for Free in Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2402.00782) | 在强化学习中，我们通过利用奖励模型中的注意权重，将奖励重新分配到完成的所有标记上，从而稠密化信号并突出显示最重要的标记。这项工作使得在传统的强化学习中很难优化的问题得到了解决。 |
| [^25] | [Hybrid Quantum Vision Transformers for Event Classification in High Energy Physics](https://arxiv.org/abs/2402.00776) | 该论文提出了一种基于量子的混合视觉转换器模型，用于高能物理中的事件分类任务。通过减少训练和操作时间，该模型可以达到与经典模型相当的性能。 |
| [^26] | [Mesh motion in fluid-structure interaction with deep operator networks](https://arxiv.org/abs/2402.00774) | 提出了一种基于深度运算网络的网格运动模型，在流体-结构相互作用问题中表现出与传统的双调和网格运动相当的性能。 |
| [^27] | [AnimateLCM: Accelerating the Animation of Personalized Diffusion Models and Adapters with Decoupled Consistency Learning](https://arxiv.org/abs/2402.00769) | AnimateLCM提出了一种分离的一致性学习策略，通过将图像生成优先级和动作生成优先级的蒸馏分离开来，提高了训练效率并增强了生成的视觉质量。 |
| [^28] | [Control-Theoretic Techniques for Online Adaptation of Deep Neural Networks in Dynamical Systems](https://arxiv.org/abs/2402.00761) | 本论文提出在动态系统中使用控制理论技术在线调整深度神经网络参数，以解决稳定性和转移学习问题。 |
| [^29] | [EuroPED-NN: Uncertainty aware surrogate model](https://arxiv.org/abs/2402.00760) | 本研究成功生成了不确定性感知的EuroPED代理模型，并通过物理验证证实了模型的稳健性和可靠性。 |
| [^30] | [Building Expressive and Tractable Probabilistic Generative Models: A Review](https://arxiv.org/abs/2402.00759) | 本文综述了富有表现力和可处理的概率生成建模领域的进展和技术，并重点关注了概率电路。文章提供了关于表达能力和可处理性之间权衡的统一视角，并说明了设计原则和算法扩展，成功地构建了富有表现力和高效的概率电路。此外，文章还讨论了最新的深度和混合概率电路研究，并概述了未来研究的挑战和开放性问题。 |
| [^31] | [Unlearnable Algorithms for In-context Learning](https://arxiv.org/abs/2402.00751) | 本文提出了一种针对预先训练的大型语言模型的高效去学习方法，通过选择少量训练示例来实现任务适应训练数据的精确去学习，并与微调方法进行了比较和讨论。 |
| [^32] | [Benefits of Transformer: In-Context Learning in Linear Regression Tasks with Unstructured Data](https://arxiv.org/abs/2402.00743) | 本研究通过线性回归任务的实验研究了Transformer在非结构化数据中的上下文学习能力，并解释了其中的关键组件。 |
| [^33] | [MobilityDL: A Review of Deep Learning From Trajectory Data](https://arxiv.org/abs/2402.00732) | 本综述论文全面概述了深度学习从轨迹数据中的应用，并对最近的工作进行了数据中心的分析。从详细的个体轨迹到稀疏轨迹和聚合轨迹，我们对八个移动性用例进行了研究，并总结了相关深度学习模型和训练数据的应用情况。 |
| [^34] | [Dropout-Based Rashomon Set Exploration for Efficient Predictive Multiplicity Estimation](https://arxiv.org/abs/2402.00728) | 本文提出了一种利用Dropout技术探索拉肖蒙集中模型的新框架，以度量和减轻预测多重性。通过严格的理论推导和广泛的实验评价，结果表明我们的技术始终优于基线模型。 |
| [^35] | [Automatic Segmentation of the Spinal Cord Nerve Rootlets](https://arxiv.org/abs/2402.00724) | 本研究开发了一种自动化的方法，可以从MRI扫描中准确地分割脊髓神经根分支，并且具有良好的性能和稳定性。 |
| [^36] | [Neural Style Transfer with Twin-Delayed DDPG for Shared Control of Robotic Manipulators](https://arxiv.org/abs/2402.00722) | 本论文提出了一种基于神经风格转移和TD3网络的共享控制方法，可以将多种风格应用于机器人操纵器的运动中。该方法通过使用自动编码器和双延迟深度确定性策略梯度网络来生成机器人控制策略，实现了机器人运动的风格转移。 |
| [^37] | [Explaining Text Classifiers with Counterfactual Representations](https://arxiv.org/abs/2402.00711) | 本论文提出了一种使用反事实表示解释文本分类器的方法，通过干预文本表示来生成反事实，并通过实验证实了方法的有效性。 |
| [^38] | [Non-Exchangeable Conformal Language Generation with Nearest Neighbors](https://arxiv.org/abs/2402.00707) | 本文介绍了一种利用最近邻方法扩展的非交换的共形语言生成框架，用于量化自动生成文本的不确定性，并提供带有统计保证的预测集。 |
| [^39] | [Combining the Strengths of Dutch Survey and Register Data in a Data Challenge to Predict Fertility (PreFer)](https://arxiv.org/abs/2402.00705) | 该论文介绍了两个数据集，分别基于荷兰的调查数据和登记数据，用于研究荷兰生育结果的预测能力。研究者提供了数据集的信息和样本，并描述了生育结果的具体内容。他们还介绍了生育率预测的方法。 |
| [^40] | [PeaTMOSS: A Dataset and Initial Analysis of Pre-Trained Models in Open-Source Software](https://arxiv.org/abs/2402.00699) | 本研究介绍了PeaTMOSS数据集，用于记录和分析开源软件中预训练模型的元数据和应用情况。这对于了解预训练模型的采用和重复使用的影响具有重要意义。 |
| [^41] | [Approximating Optimal Morphing Attacks using Template Inversion](https://arxiv.org/abs/2402.00695) | 本文提出了一种基于模板反演的形态攻击方法，可以通过反演优化形态嵌入恢复逼真的人脸图像。该方法在白盒和黑盒攻击场景中表现出与以往技术相竞争并超越其的有效性。 |
| [^42] | [Real Evaluations Tractability using Continuous Goal-Directed Actions in Smart City Applications](https://arxiv.org/abs/2402.00678) | 以往的机器人编程方法对非专家用户不友好，这篇论文提出了使用连续目标导向动作来实现真实评估的方法，通过编码动作的特征变化来适应智能城市应用中的各种特征，并使用进化算法来计算机器人的关节轨迹。 |
| [^43] | [Neural Policy Style Transfer](https://arxiv.org/abs/2402.00677) | 本研究提出了神经策略风格转换算法，通过深度强化学习来实现控制策略的风格转换。通过训练不同的网络来最大化预期奖励，同时编码了行为的目标和风格，从而将一个策略的风格转移到另一个策略而保持其内容不变。通过逆强化学习和用户演示实现模型的训练和风格的编码。 |
| [^44] | [Deep Robot Sketching: An application of Deep Q-Learning Networks for human-like sketching](https://arxiv.org/abs/2402.00676) | 本研究提出在艺术机器人应用中引入深度Q学习网络，旨在改进艺术机器人应用的控制策略。 |
| [^45] | [Modeling Freight Mode Choice Using Machine Learning Classifiers: A Comparative Study Using the Commodity Flow Survey (CFS) Data](https://arxiv.org/abs/2402.00659) | 本研究使用机器学习分类器对货运方式选择进行建模，并比较了八种常用的分类器。其中，基于树的集成分类器表现最佳，尤其是随机森林模型的预测准确性最高。 |
| [^46] | [Improving the accuracy of freight mode choice models: A case study using the 2017 CFS PUF data set and ensemble learning techniques](https://arxiv.org/abs/2402.00654) | 该论文通过使用2017年的商品流动调查公共使用文件数据集和集成学习技术，改进了货运模式选择模型的准确性，包括构建本地模型、提取地理特征和应用集成学习方法。实验结果表明，该方法在没有内存限制的情况下实现了超过92%的准确性。 |
| [^47] | [Coherent Feed Forward Quantum Neural Network](https://arxiv.org/abs/2402.00653) | 这项研究提出了一种相干前馈量子神经网络模型，该模型在电路深度和量子比特需求方面更为高效，能够适应真实世界的机器学习任务。 |
| [^48] | [Spectrally Transformed Kernel Regression](https://arxiv.org/abs/2402.00645) | 光谱变换核回归是一种能够利用无标签数据的通用和可扩展的方法，具有学习充分平滑函数的能力，并且在感知范式中提供了可扩展的实现。 |
| [^49] | [Random Forest-Based Prediction of Stroke Outcome](https://arxiv.org/abs/2402.00638) | 该论文研究了与中风患者结果相关的因素，利用机器学习技术生成了一个预测模型，能够有效预测入院后3个月的死亡率和发病率。 |
| [^50] | [Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks](https://arxiv.org/abs/2402.00626) | 这项研究深入研究了大规模视觉语言模型（LVLM）对于自动生成的排版攻击的易受攻击性，并引入了一种新的、更有效的自动生成的排版攻击方法，为此设计了一个独特的测试基准。通过使用该基准，研究发现排版攻击对LVLM构成了重大威胁。 |
| [^51] | [Bayesian Causal Inference with Gaussian Process Networks](https://arxiv.org/abs/2402.00623) | 以高斯过程网络为基础，通过模拟干预效果和传播干预效果，进行灵活的贝叶斯因果推断，同时以局部变量为函数估计干预分布并使用加性高斯过程对条件分布进行建模。 |
| [^52] | [Deep Clustering Using the Soft Silhouette Score: Towards Compact and Well-Separated Clusters](https://arxiv.org/abs/2402.00608) | 本研究提出了软轮廓系数，通过在深度聚类中优化该系数，可以实现形成紧凑且互相分离的簇。同时引入了适用于该方法的自编码器深度学习架构。 |
| [^53] | [Are Synthetic Time-series Data Really not as Good as Real Data?](https://arxiv.org/abs/2402.00607) | 本研究引入了InfoBoost，一种高度通用的跨领域数据合成框架，具备时间序列表示学习能力，并开发了一种基于合成数据的方法，可以实现超越真实数据训练的模型性能。此外，我们还训练了一个通用特征提取器，适用于所有时间序列数据。实验证明，我们的方法能够克服多个干扰源的影响，提高泛化能力。 |
| [^54] | [Uncertainty-Aware Partial-Label Learning](https://arxiv.org/abs/2402.00592) | 本文提出了一种基于最近邻的部分标签学习算法，利用Dempster-Shafer理论实现对模糊标记的数据的训练。实验结果表明，该算法能够提供良好的不确定性估计，并具有竞争力的预测性能。 |
| [^55] | [Continuous Unsupervised Domain Adaptation Using Stabilized Representations and Experience Replay](https://arxiv.org/abs/2402.00580) | 本论文提出了一种解决连续学习中无监督领域自适应问题的算法，通过稳定表示和经验回放来增强模型在新领域上的泛化能力。 |
| [^56] | [Tropical Decision Boundaries for Neural Networks Are Robust Against Adversarial Attacks](https://arxiv.org/abs/2402.00576) | 这项研究引入了一种新的神经网络架构，该架构利用热带性质能够抵抗对抗攻击，并通过实验证明了其在图像数据集上的稳健性。 |
| [^57] | [Secure Supervised Learning-Based Smart Home Authentication Framework](https://arxiv.org/abs/2402.00568) | 本论文提出了一个基于监督学习的安全的智能家居认证框架，解决了现有认证协议无法实现安全相互认证的问题，提高了智能家居环境中设备认证的可行性，并减少了会话密钥泄露、冒充和盗窃智能设备攻击的可能性。 |
| [^58] | [A Single Graph Convolution Is All You Need: Efficient Grayscale Image Classification](https://arxiv.org/abs/2402.00564) | 本论文提出了一种高效的灰度图像分类方法，通过将图像视为矢量，并使用单个图卷积层进行处理，提高了分类模型的准确性和稳定性。 |
| [^59] | [Quantum-Assisted Hilbert-Space Gaussian Process Regression](https://arxiv.org/abs/2402.00544) | 该论文提出了一种基于希尔伯特空间近似的量子算法，用于解决高斯过程回归的计算复杂度问题。这个方法结合了经典基函数展开和量子计算技术，通过量子主成分分析、条件旋转和哈达玛和Swap测试来估计特征值和评估高斯过程的后验。 |
| [^60] | [A Manifold Representation of the Key in Vision Transformers](https://arxiv.org/abs/2402.00534) | 该论文提出了一种在视觉转换器中将关键信息与查询和数值分离并采用流形表示的方法，实验证明这种方法能够提升模型性能，并在ImageNet-1K和COCO数据集上取得了积极的结果。 |
| [^61] | [Preconditioning for Physics-Informed Neural Networks](https://arxiv.org/abs/2402.00531) | 使用条件数作为度量标准来诊断和缓解物理信息神经网络中的训练病态。使用预处理来改善条件数。在18个PDE问题的评估中，我们的方法展示出了优越的性能，特别是在7个问题中将误差减少了一个数量级。 |
| [^62] | [Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling](https://arxiv.org/abs/2402.00522) | 本研究系统地探讨了Transformer在长序列建模中的近似性质，并研究了其关键组件对表达能力的影响机制。这些发现揭示了关键参数对Transformer的作用，并为替代架构提供了自然建议。 |
| [^63] | [EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit Large Language Models](https://arxiv.org/abs/2402.00518) | 该论文介绍了一种经济且可扩展的解决方案EE-Tuning，可以使用较少的计算资源和训练数据针对早期终止大型语言模型进行调整，通过性能优化和3D并行性实现卓越的训练效率。实验证实，即使在有限的训练预算下，也可以实现有效的早期终止LLM推理。 |
| [^64] | [Developing A Multi-Agent and Self-Adaptive Framework with Deep Reinforcement Learning for Dynamic Portfolio Risk Management](https://arxiv.org/abs/2402.00515) | 本研究开发了一种基于深度强化学习的多智能体自适应框架，用于动态投资组合风险管理。通过两个协同反应的智能体，平衡整体投资组合回报和潜在风险，解决了在复杂金融市场环境下的投资策略问题。 |
| [^65] | [Equivalence of the Empirical Risk Minimization to Regularization on the Family of f-Divergences](https://arxiv.org/abs/2402.00501) | 经验风险最小化与f-分布族的正则化的解决方案在特定条件下是唯一的，并且可以通过使用不同的f-分布正则化等效地表示。 |
| [^66] | [CPT: Competence-progressive Training Strategy for Few-shot Node Classification](https://arxiv.org/abs/2402.00450) | CPT是一种新颖的两阶段课程学习方法，弥补了传统元学习方法在少样本节点分类上的困难。它使用能力递进的训练策略来提高元学习器的效果和稳定性。 |
| [^67] | [A Survey of Data-Efficient Graph Learning](https://arxiv.org/abs/2402.00447) | 这项研究提出了数据高效图学习（DEGL）的概念，并总结了近期在这一领域的进展。DEGL的目标是在资源有限的场景下提高图机器学习的性能，通过探索各种最小监督方法来解决大规模标记数据的挑战。 |
| [^68] | [A practical existence theorem for reduced order models based on convolutional autoencoders](https://arxiv.org/abs/2402.00435) | 本论文提出了基于卷积自编码器的降阶模型的实用存在定理，解决了在处理复杂非线性问题方面传统方法的不足，并讨论了如何学习潜在特征的挑战。 |
| [^69] | [Merging Multi-Task Models via Weight-Ensembling Mixture of Experts](https://arxiv.org/abs/2402.00433) | 通过权重集成专家的方法可以将训练在不同任务上的Transformer模型合并为一个统一的模型，通过动态整合共享和特定任务的知识来提供更灵活的解决方案。 |
| [^70] | [From PARIS to LE-PARIS: Toward Patent Response Automation with Recommender Systems and Collaborative Large Language Models](https://arxiv.org/abs/2402.00421) | 本研究介绍了专利响应智能系统PARIS和LE-PARIS，通过构建OA主题数据库、开发响应模板以及实施推荐系统和基于LLM的响应生成，旨在加快专利律师处理审查意见回应的效率。 通过多范式分析和长期数据验证，证明了OA主题的建设性和LLM对于回应自动生成的可行性。 |
| [^71] | [Short: Benchmarking transferable adversarial attacks](https://arxiv.org/abs/2402.00418) | 本研究首次全面评估了可转移性对抗攻击的方面，引入了一个基准框架并系统分类和评估了各种增强对抗攻击可转移性的方法，为不同的模型架构提供了一个标准化的平台。 |
| [^72] | [Multi-scale Traffic Pattern Bank for Cross-city Few-shot Traffic Forecasting](https://arxiv.org/abs/2402.00397) | 我们提出了一种跨城市少样本交通预测的解决方案，利用多尺度交通模式库从数据丰富的源城市学习并预测其他城市的交通情况。 |
| [^73] | [Efficient Exploration for LLMs](https://arxiv.org/abs/2402.00396) | 高效探索在改善大型语言模型方面具有显著优势，可以以较少的查询实现较高的性能水平。不确定性估计和探索方案的选择是关键因素。 |
| [^74] | [Loss Function Considering Dead Zone for Neural Networks](https://arxiv.org/abs/2402.00393) | 本文提出了一种新的损失函数，考虑机械手臂死区内的逆动力学计算。该方法能够提高训练可用的运动数据量，并提高逆动力学计算的准确性。 |
| [^75] | [Cumulative Distribution Function based General Temporal Point Processes](https://arxiv.org/abs/2402.00388) | 本研究引入了CuFun模型，基于累积分布函数的通用时间点过程，解决了深度时间点过程模型中的强度函数建模、积分计算复杂性和长期时序依赖性捕捉的问题。 |
| [^76] | [Image2Points:A 3D Point-based Context Clusters GAN for High-Quality PET Image Reconstruction](https://arxiv.org/abs/2402.00376) | 这项研究提出了一种基于3D点的上下文聚类GAN方法，用于通过低剂量PET图像重建高质量的正电子发射断层扫描（PET）图像。这种方法通过使用点的表示和上下文聚类策略，增强了图像结构的明确表达，并减轻了重建图像中小结构的模糊性。 |
| [^77] | [Adaptive Primal-Dual Method for Safe Reinforcement Learning](https://arxiv.org/abs/2402.00355) | 本论文提出了一种自适应原始-对偶方法用于安全强化学习，通过调整自适应学习速率以优化策略，实现了算法的收敛性、最优性和可行性。实验结果表明，该方法在安全强化学习中具有更好的性能和稳定性。 |
| [^78] | [Machine Unlearning for Image-to-Image Generative Models](https://arxiv.org/abs/2402.00351) | 本文提出了一种适用于图像到图像生成模型的机器遗忘方法，该方法通过提供一个统一的框架和一个高效的算法，实现在遗忘样本中删除信息且在保留样本上性能几乎没有下降。实证研究证明该方法不依赖于保留样本的可用性，符合数据保留政策。 |
| [^79] | [ODICE: Revealing the Mystery of Distribution Correction Estimation via Orthogonal-gradient Update](https://arxiv.org/abs/2402.00348) | ODICE研究了离线强化学习和模仿学习中重要的分布校正估计（DICE）方法，并发现在使用真梯度更新学习值函数时存在前向梯度和后向梯度两个梯度项。为了解决这个问题，他们提出了一种简单但有效的修正方法。 |
| [^80] | [Diverse Explanations from Data-driven and Domain-driven Perspectives for Machine Learning Models](https://arxiv.org/abs/2402.00347) | 本文关注机器学习模型解释的不一致性，提出了从一组同样好的模型中选择具备预期解释的准确模型的方法，以加强物理定律并满足利益相关者的要求，并为将可解释人工智能整合到科学领域做出贡献。 |
| [^81] | [Survey of Privacy Threats and Countermeasures in Federated Learning](https://arxiv.org/abs/2402.00342) | 本文调查了联邦学习中的隐私威胁和对策，并对水平联邦学习、垂直联邦学习和迁移联邦学习的典型类型进行了分类和描述。 |
| [^82] | [PirateNets: Physics-informed Deep Learning with Residual Adaptive Networks](https://arxiv.org/abs/2402.00326) | PirateNets是一种物理知识驱动的深度学习框架，解决了多层感知机网络在较大深度时性能下降的问题，通过引入自适应残差连接实现了稳定和高效的训练，并提升了模型的性能。 |
| [^83] | [A Consistent Lebesgue Measure for Multi-label Learning](https://arxiv.org/abs/2402.00324) | 提出了一种基于一致勒贝格测度的多标签学习器(CLML)，通过理论证明该方法可以实现一致性。实验证据表明，CLML可以一致地取得最先进的结果，其主要性能因素是勒贝格测度设计。 |
| [^84] | [Analog-digital Scheduling for Federated Learning: A Communication-Efficient Approach](https://arxiv.org/abs/2402.00318) | 本文提出一种模拟数字FL方案，通过在每一轮中，通过模拟OTA方案上传梯度或者通过正交RB传输量化梯度的方式调度设备，解决了联邦学习中性能受限于信噪比最差设备问题的差异，以实现通信高效和降低噪声。 |
| [^85] | [Online Distribution Learning with Local Private Constraints](https://arxiv.org/abs/2402.00315) | 在这项研究中，我们讨论了在局部差分隐私下具有无界标签集的在线条件分布估计问题。我们证明了在$(\epsilon,0)$-局部差分隐私的情况下，KL风险随着时间的增长速度为$\tilde{\Theta}(\frac{1}{\epsilon}\sqrt{KT})$，其中$K=|\mathcal{F}|$，这与有界标签集的情况形成明显的对比。 |
| [^86] | [Control in Stochastic Environment with Delays: A Model-based Reinforcement Learning Approach](https://arxiv.org/abs/2402.00313) | 这篇论文介绍了一种新的基于模型的强化学习方法，用于控制带有延迟反馈的随机环境中的问题，该方法采用随机规划，能够嵌入风险偏好，并在确定性转换问题中恢复最优策略。 |
| [^87] | [Seismic Traveltime Tomography with Label-free Learning](https://arxiv.org/abs/2402.00310) | 这项研究提出了一种使用无标签学习的地震走时层析成像方法，该方法通过将深度学习和字典学习与传统的层析-最小二乘法相结合，来提高低分辨率的速度模型。 |
| [^88] | [An Accurate and Low-Parameter Machine Learning Architecture for Next Location Prediction](https://arxiv.org/abs/2402.00306) | 本文提出了一个节能、小型且低参数的机器学习架构，用于准确预测用户的下一个位置。通过大量实验，成功将模型参数数量从2.02亿减少到200万，模型大小从791 MB减少到8 MB，训练时间减少了四倍。 |
| [^89] | [Self-supervised learning of video representations from a child's perspective](https://arxiv.org/abs/2402.00300) | 本研究从儿童的视角进行自监督学习，通过长时间的头戴式摄像记录训练视频模型，结果表明这些模型在促进从少量样本中学习行动概念方面非常有效。 |
| [^90] | [Attention-based Dynamic Multilayer Graph Neural Networks for Loan Default Prediction](https://arxiv.org/abs/2402.00299) | 该研究提出了一种基于注意力机制的动态多层图神经网络模型，用于信用风险评估和贷款违约预测，并考虑了借款人之间的关联和连接随时间的演变。 |
| [^91] | [PAP-REC: Personalized Automatic Prompt for Recommendation Language Model](https://arxiv.org/abs/2402.00284) | 本研究提出了PAP-REC框架，用于生成个性化自动提示的推荐语言模型。该框架通过自动生成个性化提示标记来减轻手动设计提示所带来的效率和效果问题。 |
| [^92] | [Understanding Neural Network Systems for Image Analysis using Vector Spaces and Inverse Maps](https://arxiv.org/abs/2402.00261) | 本文使用线性代数技术将神经网络层视为信号空间之间的映射，并引入了可逆网络的概念和计算产生特定输出的输入图像的算法。 |
| [^93] | [Multi-group Learning for Hierarchical Groups](https://arxiv.org/abs/2402.00258) | 本研究将多组学习扩展到具有层次结构的情况，设计了一个近乎最优的样本复杂度的算法，输出可解释且确定性的决策树预测器，并在真实数据集上取得了有吸引力的广义化特性。 |
| [^94] | [Vertical Symbolic Regression via Deep Policy Gradient](https://arxiv.org/abs/2402.00254) | 通过使用深度策略梯度的垂直符号回归（VSR-DPG），我们能够从实验数据中发现涉及多个独立变量的符号方程，超过了以前的方法和变体。 |
| [^95] | [A Survey on Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2402.00253) | 这份综合调查研究了大规模视觉-语言模型中的幻觉问题，澄清了幻觉的概念，提出了评估方法，并对幻觉的根本原因进行了调查。 |
| [^96] | [Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning](https://arxiv.org/abs/2402.00251) | 本文提出了一种高效的非参数化不确定性量化方法，用于黑盒大型语言模型和决策规划。该方法可以有效地估计输入-决策之间的逐点依赖关系，并提供统计上对决策可信度的解释。另外，本文还提出了一个系统化的决策代理设计，根据用户提示生成动作，并在存在多个高估计逐点依赖性的动作时要求用户提供偏好。 |
| [^97] | [Spectral Norm of Convolutional Layers with Circular and Zero Paddings](https://arxiv.org/abs/2402.00240) | 本文介绍了一种使用Gram迭代计算谱范数的方法，其中包括循环卷积层和零填充卷积层的应用。通过引入谱重缩放方法，我们提供了一种增强神经网络鲁棒性的解决方案。实验证明，该方法在精度、计算成本和可扩展性方面优于现有技术。 |
| [^98] | [CNN-FL for Biotechnology Industry Empowered by Internet-of-BioNano Things and Digital Twins](https://arxiv.org/abs/2402.00238) | 本论文提出了一种集成互联网生物纳米物联网和卷积神经网络(CNN)与联邦学习(FL)的框架，可以有效应对微观和纳米尺度上数字孪生的挑战，并对生物技术行业进行赋能。 |
| [^99] | [Positional Encoding Helps Recurrent Neural Networks Handle a Large Vocabulary](https://arxiv.org/abs/2402.00236) | 本研究探讨了位置编码在循环神经网络中的作用，发现即使与RNN结合使用，位置编码仍然有效，尤其是在处理大词汇量和多样观察结果时。这为使用输入驱动和自主时间表示的组合研究提供了新的方向，同时研究结果也对神经元振荡的生物学意义提供了讨论。 |
| [^100] | [Are Generative AI systems Capable of Supporting Information Needs of Patients?](https://arxiv.org/abs/2402.00234) | 生成式AI系统被应用于支持患者信息需求的研究中，以提高患者对放射学数据的理解和管理能力。通过与患者和医疗专家的对话分析，我们确定了常见的医学信息需求和问题。 |
| [^101] | [Learning Label Hierarchy with Supervised Contrastive Learning](https://arxiv.org/abs/2402.00232) | 本文介绍了一种将标签层次结构融入有监督对比学习的方法，通过调整实例之间的距离和引入额外的对比，创建一个更为良好结构化和有区分性的特征空间。 |
| [^102] | [Uncover the nature of overlapping community in cities](https://arxiv.org/abs/2402.00222) | 本研究通过分析明尼苏达州双城地区的移动电话定位数据，揭示了城市社区中重叠的复杂本质，并发现其与收入和种族指标之间的显著相关性，解开了美国城市中复杂的隔离模式。 |
| [^103] | [FedCore: Straggler-Free Federated Learning with Distributed Coresets](https://arxiv.org/abs/2402.00219) | FedCore是一种通过分布式选择核心集解决联邦学习中慢速客户端问题的算法，可以显著减少训练时间，并保护隐私。 |
| [^104] | [MP-SL: Multihop Parallel Split Learning](https://arxiv.org/abs/2402.00208) | 引入了多跳并行分割学习（MP-SL）来缓解联邦学习（FL）中处理异构设备和大量参数的挑战。该方法允许多个资源受限设备积极参与协作训练过程，并减少计算节点的内存需求。 |
| [^105] | [Decentralised, Collaborative, and Privacy-preserving Machine Learning for Multi-Hospital Data](https://arxiv.org/abs/2402.00205) | 本文提出了一种用于多医院数据的分散、协作和保护隐私的机器学习方法（DeCaPH），它可以允许不同方在不共享私有数据集的情况下协作训练机器学习模型，并通过限制数据泄露和隐私侵犯来保护患者隐私。 |
| [^106] | [An Experiment on Feature Selection using Logistic Regression](https://arxiv.org/abs/2402.00201) | 这篇论文探讨了一种基于逻辑回归的特征选择方法，通过综合L1和L2正则化的结果，对CIC-IDS2018数据集进行了实验。没有发现显著结果。 |
| [^107] | [Determination of Trace Organic Contaminant Concentration via Machine Classification of Surface-Enhanced Raman Spectra](https://arxiv.org/abs/2402.00197) | 本研究通过机器学习的方法，利用表面增强拉曼光谱（SERS）从未经处理的数据中预测微量有机污染物的浓度。使用频域变换方法对三种模拟微污染物的拉曼光谱进行处理，并训练机器分类器，以解决SERS分析中的困难。 |
| [^108] | [Dataset Condensation Driven Machine Unlearning](https://arxiv.org/abs/2402.00195) | 这篇论文通过引入数据集精简作为机器遗忘的重要组成部分，在图像分类中解决了持久的计算挑战，并改进了近似遗忘的计算复杂性。 |
| [^109] | [Adversarial Quantum Machine Learning: An Information-Theoretic Generalization Analysis](https://arxiv.org/abs/2402.00176) | 本文研究了对抗性训练的量子分类器的泛化特性，并提出了新颖的信息论上界。 |
| [^110] | [Continuous Treatment Effects with Surrogate Outcomes](https://arxiv.org/abs/2402.00168) | 本文研究了在部分缺失主要结果的情况下，使用替代结果来估计连续治疗效果，并提出了一种双重稳健方法，通过使用标记和未标记数据，可以有效地纳入替代结果并避免选择偏误问题。该方法的估计值渐近正态性，并在方差方面可能比仅使用标记数据的方法有所改进。模拟实验证明了该方法的良好实证性能。 |
| [^111] | [Behind the Myth of Exploration in Policy Gradients](https://arxiv.org/abs/2402.00162) | 本论文提出了对政策梯度算法中探索项的新分析方法，区分了其平滑学习目标和增加梯度估计的两种不同作用。同时，详细讨论和实证了基于熵奖励的探索策略的局限性，并开辟了未来对这些策略设计和分析的研究方向。 |
| [^112] | [Fully Data-Driven Model for Increasing Sampling Rate Frequency of Seismic Data using Super-Resolution Generative Adversarial Networks](https://arxiv.org/abs/2402.00153) | 本研究首次应用了超分辨率生成对抗网络（SRGANs）来提高地震工程中传感器网络获取的时间历史数据的分辨率，从而增加了数据采样频率，提高了地震工程应用的准确性和可靠性。 |
| [^113] | [Deeper or Wider: A Perspective from Optimal Generalization Error with Sobolev Loss](https://arxiv.org/abs/2402.00152) | 本文比较了更深的神经网络和更宽的神经网络在Sobolev损失的最优泛化误差方面的表现，研究发现神经网络的架构受多种因素影响，参数数量更多倾向于选择更宽的网络，而样本点数量和损失函数规则性更高倾向于选择更深的网络。 |
| [^114] | [Decomposable Submodular Maximization in Federated Setting](https://arxiv.org/abs/2402.00138) | 该论文提出了一种联邦优化设置用于在计算上不可行的可分解子模函数优化问题中。在这个设置中，客户端拥有私有的组件函数，通过并行计算和集中聚合的方式来求解最大化问题。 |
| [^115] | [Multimodal Neurodegenerative Disease Subtyping Explained by ChatGPT](https://arxiv.org/abs/2402.00137) | 本论文提出了一个多模态框架，使用成像、遗传学和临床评估等早期指标将阿尔茨海默病患者在早期阶段划分为不同的亚型。同时，利用ChatGPT等大型语言模型解释模型的发现。 |
| [^116] | [Comparing Template-based and Template-free Language Model Probing](https://arxiv.org/abs/2402.00123) | 本文比较了基于模板和非模板语言模型的探测方法，发现它们在模型排名、绝对得分和与困惑度的关系等方面存在差异。 |
| [^117] | [Code-Aware Prompting: A study of Coverage Guided Test Generation in Regression Setting using LLM](https://arxiv.org/abs/2402.00097) | 本文提出了一种代码感知提示策略（SymPrompt），用于基于LLM的测试生成，通过将测试生成过程分解为多阶段序列，并以驱动策略推动每个阶段，改善了测试生成的覆盖率。 |
| [^118] | [Deep Neural Networks: A Formulation Via Non-Archimedean Analysis](https://arxiv.org/abs/2402.00094) | 该论文引入了一种新的深度神经网络（DNNs）类别，其采用多层树状结构的架构并使用非阿基米德局部域的整数环进行编码。这些DNNs是稳健的对实值函数和实值平方可积函数的普遍逼近器。 |
| [^119] | [ChIRAAG: ChatGPT Informed Rapid and Automated Assertion Generation](https://arxiv.org/abs/2402.00093) | 本研究设计了一个基于大语言模型的流水线，通过自然语言规格生成英语、线性时态逻辑和SVA断言，并成功减少了断言错误率。 |
| [^120] | [Episodic-free Task Selection for Few-shot Learning](https://arxiv.org/abs/2402.00092) | 本文提出了一种超越分集训练的元训练框架，通过选择一些无分集任务对元学习器进行训练，并使用亲和性标准来评估其有效性。实验结果显示，这种方法在少样本学习中取得了更好的效果。 |
| [^121] | [Retrosynthesis prediction enhanced by in-silico reaction data augmentation](https://arxiv.org/abs/2402.00086) | RetroWISE 是一个利用计算模拟的反应数据增强的 retrosynthesis 预测的框架，通过使用易于获取的非配对数据生成配对数据，提高了模型的训练性能。 |
| [^122] | [Scheduled Curiosity-Deep Dyna-Q: Efficient Exploration for Dialog Policy Learning](https://arxiv.org/abs/2402.00085) | 本论文提出了Scheduled Curiosity-Deep Dyna-Q (SC-DDQ)框架，通过引入预定学习和好奇心，显著提高了基于强化学习的任务导向对话代理的效果。 |
| [^123] | [EPSD: Early Pruning with Self-Distillation for Efficient Model Compression](https://arxiv.org/abs/2402.00084) | 提出了一种早期修剪与自蒸馏相结合的框架，实现了高效的模型压缩。 |
| [^124] | [Unlocking the Power of Multi-institutional Data: Integrating and Harmonizing Genomic Data Across Institutions](https://arxiv.org/abs/2402.00077) | 该研究介绍了一种名为Bridge的模型，致力于解决利用多机构测序数据时面临的挑战，包括基因组板块的变化、测序技术的差异以及数据的高维度和稀疏性等。 |
| [^125] | [Explainable AI for survival analysis: a median-SHAP approach](https://arxiv.org/abs/2402.00072) | 这项研究介绍了一种中位数-SHAP方法，用于解释黑盒子模型在预测个体生存时间方面产生的解释偏差问题。 |
| [^126] | [Unraveling the Impact of Initial Choices and In-Loop Interventions on Learning Dynamics in Autonomous Scanning Probe Microscopy](https://arxiv.org/abs/2402.00071) | 本文研究了自主扫描探针显微术中初始选择和循环干预对学习动力学的影响，并探讨了“种子效应”和种子点干预的概念，对深度内核学习的有效性进行了实证分析。 |
| [^127] | [EvoMerge: Neuroevolution for Large Language Models](https://arxiv.org/abs/2402.00070) | EvoMerge是一种针对大型语言模型训练和合并的系统性方法，通过权重交叉和微调进行权重变异，旨在将模型推向超越传统微调限制的进化过程。 |
| [^128] | [GPT4Battery: An LLM-driven Framework for Adaptive State of Health Estimation of Raw Li-ion Batteries](https://arxiv.org/abs/2402.00068) | 本论文提出了一种基于LLM的框架，可以适应不同类型的锂离子电池，实现准确的健康状态估计。这项工作解决了生成训练数据的时间和资源成本高的挑战，并在实际应用中具有良好的泛化能力。 |
| [^129] | [Online speaker diarization of meetings guided by speech separation](https://arxiv.org/abs/2402.00067) | 本研究提出了一种适用于在线对长时间会议录音进行说话人辨识的新颖方法，通过语音分离来引导辨识过程。该方法能有效处理具有可变说话人数量的真实数据，并通过微调模型实现端到端的优化。 |
| [^130] | [TrackGPT -- A generative pre-trained transformer for cross-domain entity trajectory forecasting](https://arxiv.org/abs/2402.00066) | TrackGPT是一种基于GPT的实体轨迹预测模型，能够生成准确的预测结果，包括长期预测和短期预测，并在多个领域展现出良好的性能。 |
| [^131] | [FengWu-GHR: Learning the Kilometer-scale Medium-range Global Weather Forecasting](https://arxiv.org/abs/2402.00059) | FengWu-GHR是全球首个以数据驱动方式运行的公里级全球天气预报模型，具有更高的分辨率和可比甚至更高的预报技巧。 |
| [^132] | [Predicting loss-of-function impact of genetic mutations: a machine learning approach](https://arxiv.org/abs/2402.00054) | 本研究使用机器学习的方法，根据基因突变的属性预测了功能丧失影响，为识别有害突变提供了新的手段。 |
| [^133] | [Are We Wasting Time? A Fast, Accurate Performance Evaluation Framework for Knowledge Graph Link Predictors](https://arxiv.org/abs/2402.00053) | 这篇论文提出了一种快速、准确的知识图谱链接预测评估框架，解决了现有方法中随机抽样带来的排名指标不准确的问题。 |
| [^134] | [Introducing PetriRL: An Innovative Framework for JSSP Resolution Integrating Petri nets and Event-based Reinforcement Learning](https://arxiv.org/abs/2402.00046) | PetriRL是一个创新框架，将Petri网和基于事件的强化学习集成，用于解决工业作业车间调度问题（JSSP）。该框架利用Petri网进行建模，提高了可解释性，并通过事件驱动控制和动作屏蔽的集成取得了竞争性的性能。 |
| [^135] | [Detecting Multimedia Generated by Large AI Models: A Survey](https://arxiv.org/abs/2402.00045) | 本文综述了大型人工智能模型（LAIMs）生成的多媒体内容的检测方法和研究进展，旨在填补现有研究中的空白。我们提供了一种新颖的分类法，并介绍了纯检测和应用场景两个角度来增强检测性能。 |
| [^136] | [Training microrobots to swim by a large language model](https://arxiv.org/abs/2402.00044) | 本研究利用大型语言模型(GPT-4)训练两个微型机器人游泳，采用少样本学习方法，开发了一个简洁统一提示，成功引导这两种微型机器人掌握其特色划水动作，从而超越了传统方法。 |
| [^137] | [Interactive and Intelligent Root Cause Analysis in Manufacturing with Causal Bayesian Networks and Knowledge Graphs](https://arxiv.org/abs/2402.00043) | 本文提出了一个互动智能的制造业根本原因分析工具，通过结合电动汽车制造过程的专家知识和数据驱动的机器学习方法，利用大规模知识图谱进行推理并学习一个因果贝叶斯网络。 |
| [^138] | [Spatial-temporal-demand clustering for solving large-scale vehicle routing problems with time windows](https://arxiv.org/abs/2402.00041) | 该论文提出了一种基于聚类的解决大规模车辆路径问题的方法，通过结合空间、时间、需求数据，将问题分解为子路径问题，然后分别解决，并通过局部搜索来改进整体解决方案。 |
| [^139] | [Detecting Brain Tumors through Multimodal Neural Networks](https://arxiv.org/abs/2402.00038) | 通过多模态神经网络检测脑肿瘤，在图像处理和分类中取得了令人满意的结果，并具有98%的准确率。 |
| [^140] | [Kronecker Product Feature Fusion for Convolutional Neural Network in Remote Sensing Scene Classification](https://arxiv.org/abs/2402.00036) | 本文提出了一种新颖的特征融合算法，使用Kronecker积(KPFF)将加法和连接方法统一起来，并通过实验证明其能够提高卷积神经网络在遥感场景分类中的准确性。 |
| [^141] | [Robustness Assessment of a Runway Object Classifier for Safe Aircraft Taxiing](https://arxiv.org/abs/2402.00035) | 本文介绍了对航班滑行安全的跑道物体分类器的鲁棒性评估，使用形式方法评估了该分类器对三种常见图像扰动类型的鲁棒性，并提出了一种利用单调性的方法。 |
| [^142] | [An Integrated Framework for Team Formation and Winner Prediction in the FIRST Robotics Competition: Model, Algorithm, and Analysis](https://arxiv.org/abs/2402.00031) | 本研究提出了一个集成框架，通过在团队形成之前的队员技能数据上进行优化，预测团队表现，填补了FIRST Robotics Competition领域的研究空白。 |
| [^143] | [Comparative Analysis of LLaMA and ChatGPT Embeddings for Molecule Embedding](https://arxiv.org/abs/2402.00024) | LLaMA和ChatGPT比较分析了它们在SMILES字符串嵌入中的性能，在分子性质预测和药物-药物相互作用预测中，LLaMA相对于ChatGPT表现更好并且与现有方法相当。 |
| [^144] | [Using Multi-Temporal Sentinel-1 and Sentinel-2 data for water bodies mapping](https://arxiv.org/abs/2402.00023) | 本论文通过整合Sentinel-1和Sentinel-2数据，提出了一种新的多源和多时相的数据集，用于全面水资源监测。应用土壤含水指数和归一化差异水体指数等指标以及无监督机器学习分类器，取得了有希望的结果。 |
| [^145] | [Diffusion MRI with Machine Learning](https://arxiv.org/abs/2402.00019) | 本文评估了机器学习在弥散磁共振成像中的应用，重点关注了微结构映射、纤维束描记、白质纤维束分析以及数据预处理和协调的方法。通过对现有方法的总结，提出了未来研究的主题。 |
| [^146] | [Hybrid quantum cycle generative adversarial network for small molecule generation](https://arxiv.org/abs/2402.00014) | 本文介绍了一种基于参数化量子电路的新型生成对抗网络模型，通过引入强化学习原理的多参数奖励函数，成功优化了小分子生成的药物样性估计，最高提升了30%。 |
| [^147] | [Choosing the Right Path for AI Integration in Engineering Companies: A Strategic Guide](https://arxiv.org/abs/2402.00011) | 本文为EPC公司提供了一个转向AI的框架，旨在帮助他们根据自身业务战略和资源状况，以最有效的方式获取AI技术。这是基于全球最大的EPC承包商之一在基于AI的产品开发项目执行中的经验和已将AI整合到工程解决方案中的EPC供应商公司的见解。 |
| [^148] | [GD doesn't make the cut: Three ways that non-differentiability affects neural network training](https://arxiv.org/abs/2401.08426) | 本文研究了非可微性对神经网络训练的影响，包括收敛性差异、$L_1$正则化问题的矛盾性质以及稳定边界现象的不适用性。 |
| [^149] | [Reliability and Interpretability in Science and Deep Learning](https://arxiv.org/abs/2401.07359) | 这篇论文强调了科学与深度学习中模型假设的重要性，并提供了对模型假设认识论复杂性的分析，同时结合标准错误分析与深度神经网络模型的特点，来评估模型可靠性。 |
| [^150] | [Commonsense for Zero-Shot Natural Language Video Localization](https://arxiv.org/abs/2312.17429) | 本文研究了零样本自然语言-视频定位中常识推理的有效性，并提出了CORONET框架，该框架利用常识进行视频和生成的伪查询之间的桥接。实验证明CORONET在零样本和传统NLVL方法上都取得了优越的结果。 |
| [^151] | [Comparing Machine Learning Algorithms by Union-Free Generic Depth](https://arxiv.org/abs/2312.12839) | 本研究提出了一种描述性分析偏序集合的框架，通过改进的无交并泛深度 (ufg) 比较机器学习算法，并在标准基准数据集上提供了示例。研究结果展示了基于ufg方法的多样性分析方法，并与现有的基准测试方法有很大区别。 |
| [^152] | [EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism](https://arxiv.org/abs/2312.04916) | EE-LLM是一个用于大规模训练和推理早退出大型语言模型的框架，具有三维并行性和多项算法创新。研究发现EE-LLM在训练效率上表现出色，计算开销极小。 |
| [^153] | [RLHF and IIA: Perverse Incentives](https://arxiv.org/abs/2312.01057) | RLHF算法中的IIA假设导致了倒置激励，限制了查询格式和学习算法的创新。 |
| [^154] | [Geometry-Aware Normalizing Wasserstein Flows for Optimal Causal Inference](https://arxiv.org/abs/2311.18826) | 本文提出了一种几何感知的归一化Wasserstein流的方法，通过整合连续归一化流（CNFs）和参数子模型，优化了因果推断的表现，并在最优传输理论中提高了鲁棒性。 |
| [^155] | [On the Second-Order Convergence of Biased Policy Gradient Algorithms](https://arxiv.org/abs/2311.02546) | 该论文研究了偏置策略梯度算法的二阶收敛性，包括基于蒙特卡洛轨迹采样的普通梯度估计器和基于双循环评论家-演员算法的演员-评论家方法。实现在实际应用中的偏置主要来自于有限时间采样和对价值函数的逼近。 |
| [^156] | [AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents](https://arxiv.org/abs/2310.09971) | AMAGO是一个可扩展的上下文强化学习智能体，使用序列模型解决了泛化、长期存储和元学习等挑战，并通过离线学习成功训练了长序列Transformer，具有强大的性能和适用性。 |
| [^157] | [InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining](https://arxiv.org/abs/2310.07713) | InstructRetro是目前规模最大的使用检索预训练的LLM，扩展了基础模型Retro 48B，通过指令调优在各种零样例任务上取得显著改进。 |
| [^158] | [Generative quantum machine learning via denoising diffusion probabilistic models](https://arxiv.org/abs/2310.05866) | 通过引入量子去噪扩散概率模型（QuDDPM），我们实现了对量子数据的高效可训练的生成学习，该模型采用足够层数的电路以保证表达能力，并引入多个中间训练任务以避免贫瘠平原并保证高效的训练。 |
| [^159] | [On Accelerating Diffusion-based Molecular Conformation Generation in SE(3)-invariant Space](https://arxiv.org/abs/2310.04915) | 本文研究在SE(3)不变空间中加速扩散机制，提出新的加速方案，可以以50倍到100倍的速度生成高质量的分子构象。 |
| [^160] | [Towards Cross-Table Masked Pretraining for Web Data Mining](https://arxiv.org/abs/2307.04308) | 本论文旨在研究面向网络数据挖掘的跨表掩码预训练方法，解决了表格数据预训练中的关键挑战，显示出对挖掘网络表格数据的潜力。 |
| [^161] | [Leveraging Open Information Extraction for More Robust Domain Transfer of Event Trigger Detection](https://arxiv.org/abs/2305.14163) | 本研究利用开放信息抽取系统中的主谓关系将事件触发器在不同领域间进行耦合，以提升事件触发识别的领域转移性能。在从高资源到低资源领域的转移中，该方法可减轻性能下降，特别是在从维基百科到新闻领域的转移中效果显著。同时结合遮蔽语言建模能进一步增强转移效果。 |
| [^162] | [Generative machine learning methods for multivariate ensemble post-processing](https://arxiv.org/abs/2211.01345) | 本研究提出了一种基于生成式机器学习的多变量集合后处理方法，通过生成神经网络直接输出多变量预测分布的样本，解决了两步方法难以包含附加预测变量的问题。 |
| [^163] | [FORESEE: Prediction with Expansion-Compression Unscented Transform for Online Policy Optimization](https://arxiv.org/abs/2209.12644) | 本研究引入了扩展压缩无香克特变换的状态预测方法，在解决在线策略优化问题时能够以较低的计算成本与蒙特卡洛方法相媲美。 |
| [^164] | [Feed-Forward Latent Domain Adaptation](https://arxiv.org/abs/2207.07624) | 本文研究了前馈潜在领域自适应的问题，提出了一种通过元学习和交叉注意力实现动态适应的方法，该方法在资源受限的边缘设备上取得了一致的改进。 |
| [^165] | [Fair Machine Learning in Healthcare: A Review](https://arxiv.org/abs/2206.14397) | 这篇综述论文研究了机器学习在医疗保健领域的公平问题。通过采用基于分配公正原则的框架，将公平问题分为资源平均分配和性能平等两个类别，并从机器学习的角度对相关的公正度量进行了批判性的回顾。论文还讨论了机器学习生命周期各个阶段的偏见和缓解策略，探讨了偏见与其对策之间的关系。 |
| [^166] | [Collaborative likelihood-ratio estimation over graphs](https://arxiv.org/abs/2205.14461) | 该论文提出了基于图的协同似然比估计方法，通过考虑图结构信息，估计每个节点间的似然比，节点可以协作来更高效地解决问题。 |
| [^167] | [The curse of overparametrization in adversarial training: Precise analysis of robust generalization for random features regression](https://arxiv.org/abs/2201.05149) | 本文精确分析了对对抗训练中过度参数化的影响，发现过度参数化模型对微小对抗扰动非常脆弱，显示了鲁棒泛化的性能明显差于标准泛化的性能。 |
| [^168] | [Probability-Generating Function Kernels for Spherical Data](https://arxiv.org/abs/2112.00365) | 该论文介绍了一种在球面数据分析中应用的概率生成函数核，扩展了RBF核并引入了半参数学习算法。 |
| [^169] | [A theoretical and empirical study of new adaptive algorithms with additional momentum steps and shifted updates for stochastic non-convex optimization](https://arxiv.org/abs/2110.08531) | 本文介绍了一种新的自适应算法，用于解决随机非凸优化问题，并展示了加速方法与AMSGrad类型动量方法之间的深层关联。 |
| [^170] | [Online Graph Topology Learning from Matrix-valued Time Series](https://arxiv.org/abs/2107.08020) | 本文通过研究矩阵值时间序列的统计分析，提出了在线图拓扑学习的方法。首先，将VAR模型扩展为矩阵变量模型以适用于图形学习。其次，提出了两种在线过程，针对低维和高维情况快速更新系数的估计。这些方法在高维情况下引入了一种新的Lasso-type进行拓扑处理。 |
| [^171] | [Engineering A Large Language Model From Scratch.](http://arxiv.org/abs/2401.16736) | Atinuke是一种基于Transformer的神经网络，通过在处理时序数据的层与注意机制交织在一起，模拟人类语言，从而优化各种语言任务的性能。 |
| [^172] | [OptiState: State Estimation of Legged Robots using Gated Networks with Transformer-based Vision and Kalman Filtering.](http://arxiv.org/abs/2401.16719) | 本研究提出了一种名为OptiState的腿式机器人状态估计方法，该方法通过整合Kalman滤波、优化和学习模式的混合解决方案，结合本体感和外感信息，以精确估计机器人主体的状态。该方法利用关节编码器、IMU测量和基于凸规划的模型预测控制优化，通过Gate循环单元和Vision Transformer自编码器改进了估计结果。研究结果表明，该方法能够提供准确的机器人状态估计，并减小传感器测量和模型简化引起的非线性误差。 |
| [^173] | [Generalization of LiNGAM that allows confounding.](http://arxiv.org/abs/2401.16661) | 本文提出了一种名为LiNGAM-MMI的方法，可以增强LiNGAM模型以处理混淆问题。该方法使用KL散度量化混淆程度，并通过最短路径问题解决方案高效地确定变量顺序，不论是否存在混淆情况。实验证明，LiNGAM-MMI可以更准确地识别正确的变量顺序。 |
| [^174] | [A Multi-Grained Symmetric Differential Equation Model for Learning Protein-Ligand Binding Dynamics.](http://arxiv.org/abs/2401.15122) | 提出了一种能够促进数值MD模拟并有效模拟蛋白质-配体结合动力学的NeuralMD方法，采用物理信息多级对称框架，实现了准确建模多级蛋白质-配体相互作用。 |
| [^175] | [Hi-Core: Hierarchical Knowledge Transfer for Continual Reinforcement Learning.](http://arxiv.org/abs/2401.15098) | Hi-Core提出了一种新的框架，通过层次化的知识迁移来增强连续强化学习。该框架包括利用大型语言模型的推理能力设定目标的高层策略制定和通过强化学习按照高层目标导向的低层策略学习。在实验中，Hi-Core展现了较强的知识迁移能力。 |
| [^176] | [Conformal Prediction Sets Improve Human Decision Making.](http://arxiv.org/abs/2401.13744) | 该研究表明，通过规范预测量化模型的不确定性，可以提高人类决策的准确性和效果，对人机协同决策具有实用价值。 |
| [^177] | [Mitigating System Bias in Resource Constrained Asynchronous Federated Learning Systems.](http://arxiv.org/abs/2401.13366) | 该论文提出了一种在资源受限的异步联邦学习系统中减轻系统偏差的动态全局模型聚合方法，通过根据客户端的上传频率评分和调整模型更新的权重，以适应异构设备和非同分布数据的挑战。实验结果表明，在仿真环境中，与最先进的方法相比，该方法在全局模型准确性上有显著的改善。 |
| [^178] | [Emergent Dominance Hierarchies in Reinforcement Learning Agents.](http://arxiv.org/abs/2401.12258) | 本研究在强化学习中探讨了一种新的支配等级现象，并证明了在没有明确编程和内在奖励的情况下，强化学习代理能够自主发明、学习、实施和传递支配等级给新的群体。 |
| [^179] | [Diffusion Model Conditioning on Gaussian Mixture Model and Negative Gaussian Mixture Gradient.](http://arxiv.org/abs/2401.11261) | 本研究提出了一种利用高斯混合模型作为特征条件来引导扩散模型的去噪过程的条件机制，并通过实验证实了基于特征的条件潜在分布能够产生更少的缺陷生成。 |
| [^180] | [Langevin Unlearning: A New Perspective of Noisy Gradient Descent for Machine Unlearning.](http://arxiv.org/abs/2401.10371) | Langevin遗忘是一种基于噪声梯度下降的遗忘框架，能够在近似遗忘问题中提供隐私保证，并且具有算法上的优势。 |
| [^181] | [Physics-constrained convolutional neural networks for inverse problems in spatiotemporal partial differential equations.](http://arxiv.org/abs/2401.10306) | 本研究提出了一种物理约束卷积神经网络（PC-CNN），用于解决非线性且时空变化的偏微分方程中的两种反问题。该网络可以揭示受偏差影响的真实状态，并在给定稀疏信息的情况下以高分辨率重建解。 |
| [^182] | [Towards Learning from Graphs with Heterophily: Progress and Future.](http://arxiv.org/abs/2401.09769) | 本调查综合概述了关于从带有异质性的图中学习的现有研究，并根据学习策略、模型架构和实际应用等方面对方法进行了分类。同时讨论了现有研究的主要挑战，并提出了未来研究的潜在方向。 |
| [^183] | [Differentially Private Estimation of CATE in Adaptive Experiment.](http://arxiv.org/abs/2401.08224) | 本研究提出在自适应实验中使用差分隐私估计条件平均处理效果(CATE)，平衡社会福利损失和统计功率，并通过匹配上下界以及帕累托最优性概念来获得最优解。 |
| [^184] | [Remixing Music for Hearing Aids Using Ensemble of Fine-Tuned Source Separators.](http://arxiv.org/abs/2401.06203) | 本文介绍了一个使用精调的源分离器合奏混音音乐以改善助听器音质的系统，该系统在Cadenza ICASSP 2024大挑战中名列第一，并在评估数据集上获得了最佳的助听器音质指数（HAAQI）平均分数。 |
| [^185] | [Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents.](http://arxiv.org/abs/2401.05821) | SCoBots是一种可解释的概念瓶颈代理，能够透明化整个决策流程，帮助领域专家理解和规范强化学习代理的行为，从而可能实现更好的人类对齐强化学习。 |
| [^186] | [Enhancing Blood Flow Assessment in Diffuse Correlation Spectroscopy: A Transfer Learning Approach with Noise Robustness Analysis.](http://arxiv.org/abs/2401.05580) | 本研究提出了一种迁移学习方法，用于增强扩散相关光谱学中的血流评估，并通过噪声鲁棒性分析展示了其鲁棒性。 |
| [^187] | [Fast Cerebral Blood Flow Analysis via Extreme Learning Machine.](http://arxiv.org/abs/2401.05578) | 本论文提出了一种通过极限学习机快速精确分析脑血流的方法，并通过与现有算法的综合比较验证了其优越性。它展示了强大的泛化能力，在各种噪声和光学参数下都具有更高的准确性。同时，与计算效率高的神经网络相比，该方法具有较短的训练和推理时间。这种策略适用于在线训练的边缘计算应用。 |
| [^188] | [Fair Sampling in Diffusion Models through Switching Mechanism.](http://arxiv.org/abs/2401.03140) | 本论文提出了一种称为“属性切换”的公平抽样机制，用于解决扩散模型中公平性的问题。通过在生成的数据中混淆敏感属性，该方法能够实现生成公平数据和保持数据效用的目标。 |
| [^189] | [A First Look at Information Highlighting in Stack Overflow Answers.](http://arxiv.org/abs/2401.01472) | 本论文进行了首次大规模的探索性研究，研究了Stack Overflow回答中的信息高亮。通过使用神经网络架构，开发了自动推荐突出内容的方法。 |
| [^190] | [Deep Learning Architecture for Network-Efficiency at the Edge.](http://arxiv.org/abs/2311.05739) | 本文提出了一种自适应压缩感知分离学习方法，将深度学习模型与边缘云资源集成，从而实现网络高效性和更快的计算速度，适用于部署在较弱设备上。 |
| [^191] | [Implicit Manifold Gaussian Process Regression.](http://arxiv.org/abs/2310.19390) | 本文提出了一种能够从数据中直接推断隐式结构的高斯过程回归技术，能够处理高维数据，并可能改善预测性能和校准。 |
| [^192] | [DP-SGD with weight clipping.](http://arxiv.org/abs/2310.18001) | 本研究提出了一种带权重剪裁的差分隐私梯度下降方法，通过利用公共信息对全局模型进行改进，获得更精确的灵敏度界限和噪声水平调整，提供了更好的差分隐私保证。 |
| [^193] | [Tackling Interference Induced by Data Training Loops in A/B Tests: A Weighted Training Approach.](http://arxiv.org/abs/2310.17496) | 该论文提出了一种名为加权训练的方法，用于解决A/B测试中由数据训练循环引起的干扰。该方法通过训练模型预测每个数据点在实验组或控制组中出现的概率，并在模型训练过程中应用加权损失，从而实现了最小方差的估计结果，并且不会引起训练分布的变化。 |
| [^194] | [Instilling Inductive Biases with Subnetworks.](http://arxiv.org/abs/2310.10899) | 通过子网络注入归纳偏置，这项研究探索了理解和控制神经网络行为的方法。通过发现功能子网络并利用它们，可以显著减少训练模型所需的数据量。 |
| [^195] | [A decoder-only foundation model for time-series forecasting.](http://arxiv.org/abs/2310.10688) | 本论文介绍了一种基于补丁解码器式注意力模型的时间序列预测基础模型，该模型在零样本情况下在各种公共数据集上的性能接近最先进的监督预测模型。 |
| [^196] | [The Noise Geometry of Stochastic Gradient Descent: A Quantitative and Analytical Characterization.](http://arxiv.org/abs/2310.00692) | 本文对随机梯度下降（SGD）中的噪声几何进行了全面的理论研究，发现噪声与损失函数的局部几何特征有利的一致性。通过实验证明，SGD在逃脱尖锐极小值时与GD形成鲜明对比，逃脱方向在平坦方向上有显著分量。 |
| [^197] | [SELF: Language-Driven Self-Evolution for Large Language Model.](http://arxiv.org/abs/2310.00533) | SELF提出了一种基于语言驱动的创新方法，允许大型语言模型（LLM）不断自我进化，并通过语言反馈作为评估工具来改进模型的响应能力和训练稳定性。 |
| [^198] | [HyperMask: Adaptive Hypernetwork-based Masks for Continual Learning.](http://arxiv.org/abs/2310.00113) | HyperMask是一种用于持续学习的方法，它使用基于超网络的掩码来训练一个单一网络，以克服人工神经网络在多任务上的灾难性遗忘问题。 |
| [^199] | [Causal Reasoning: Charting a Revolutionary Course for Next-Generation AI-Native Wireless Networks.](http://arxiv.org/abs/2309.13223) | 本文提出了一种基于因果推理的框架，用于构建AI本地化的无线网络，以应对现有“AI for wireless”范式的短板。该框架通过解决AI模型的黑匣子特性、曲线拟合特性、对大量训练数据的依赖以及大型神经网络的能量效率低下等问题，克服了数据驱动型、训练密集型AI的局限性。 |
| [^200] | [ACT: Empowering Decision Transformer with Dynamic Programming via Advantage Conditioning.](http://arxiv.org/abs/2309.05915) | 这篇论文提出了一种通过将动态规划应用于决策Transformer来增强其能力的方法。作者提出了三个步骤来实现这一目标：使用样本内值迭代获得近似值函数，结合估计的优势评估动作质量，并训练ACT生成基于估计优势的动作。该方法在测试中表现出良好的性能。 |
| [^201] | [FECoM: A Step towards Fine-Grained Energy Measurement for Deep Learning.](http://arxiv.org/abs/2308.12264) | FECoM是一个用于细粒度深度学习能耗测量的框架，通过静态仪器分析和考虑计算负载和温度稳定性等因素，为研究人员和开发人员提供了对深度学习API进行概要分析的机制。 |
| [^202] | [Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language.](http://arxiv.org/abs/2308.05061) | 本文提出了一种使用传感器数据、方程和自然语言提示上下文中运算符学习的方法。通过整合人类知识和语言描述，该方法不仅扩展了物理信息学习的灵活性和普适性，而且显著提高了学习性能和减少了数据需求。 |
| [^203] | [SMARLA: A Safety Monitoring Approach for Deep Reinforcement Learning Agents.](http://arxiv.org/abs/2308.02594) | 本文提出了一种基于机器学习的安全监测方法SMARLA，用于深度强化学习智能体。该方法设计为黑盒子，利用状态抽象减少状态空间，实现对智能体状态的安全违规预测。经验证，SMARLA具有准确的违规预测能力，并可在智能体执行的早期阶段进行预测。 |
| [^204] | [SkullGAN: Synthetic Skull CT Generation with Generative Adversarial Networks.](http://arxiv.org/abs/2308.00206) | 使用SkullGAN，一种生成对抗网络（GAN），生成合成的颅骨CT图像，可以减少对真实图像的依赖，加速将机器学习应用于医疗保健领域的整合。 |
| [^205] | [Corruption-Robust Lipschitz Contextual Search.](http://arxiv.org/abs/2307.13903) | 该论文研究了学习具有被篡改的二进制信号的Lipschitz函数的问题，提出了一种腐败鲁棒算法。该算法在不同损失函数下实现了不同程度的后悔。 |
| [^206] | [Relationship between Batch Size and Number of Steps Needed for Nonconvex Optimization of Stochastic Gradient Descent using Armijo Line Search.](http://arxiv.org/abs/2307.13831) | 这项研究分析了使用Armijo线搜索的随机梯度下降非凸优化中批大小和步数的关系，并发现随着批大小的增加，所需的步数减少。 |
| [^207] | [Discovering interpretable elastoplasticity models via the neural polynomial method enabled symbolic regressions.](http://arxiv.org/abs/2307.13149) | 本文介绍了一种通过神经多项式方法实现可解释的弹性塑性模型的机器学习方法，该方法通过分为两个步骤，先通过监督学习得到一组特征映射，再通过符号回归将其转化为数学公式，从而克服了传统神经网络模型的缺乏可解释性的问题。 |
| [^208] | [The Power of Populations in Decentralized Learning Dynamics.](http://arxiv.org/abs/2306.08670) | 本文研究了分散式学习动力学中个体群体的力量。我们介绍了一种分散式的多臂赌博机设置，并分析了几个针对此任务的分散式动力学家族。我们展示了这些动力学与一类“零和”乘法权重更新算法的联系，并开发了一个通用框架来分析这些协议的群体级遗憾。在广泛的参数范围下，我们得到了次线性的遗憾界限。 |
| [^209] | [MC-NN: An End-to-End Multi-Channel Neural Network Approach for Predicting Influenza A Virus Hosts and Antigenic Types.](http://arxiv.org/abs/2306.05587) | 提出了一种利用多通道神经网络模型预测流感A病毒宿主和抗原亚型的方法，数据显示多通道神经网络模型具有较高的准确性和预测能力。 |
| [^210] | [SiBBlInGS: Similarity-driven Building-Block Inference using Graphs across States.](http://arxiv.org/abs/2306.04817) | 本文提出了一种跨状态的图形相似性驱动的模块推理框架，可以同时考虑数据中的状态间和状态内关系，并允许状态之间的会话计数和持续时间的差异。它可以提取非正交组件，并且能够识别特定状态与状态非特定模块。 |
| [^211] | [Simple High Quality OoD Detection with L2 Normalization.](http://arxiv.org/abs/2306.04072) | 本篇论文介绍了在ResNet模型训练中应用L2归一化技术，可以以几乎没有成本的方式实现与最先进的OoD检测性能相媲美的结果，测试时仅需移除L2归一化即可。 |
| [^212] | [MutateNN: Mutation Testing of Image Recognition Models Deployed on Hardware Accelerators.](http://arxiv.org/abs/2306.01697) | MutateNN是一种用于探索硬件加速器上深度学习图像识别模型鲁棒性的工具，提供突变测试和分析能力，且有效性已在多种预训练深度神经网络模型中得到验证。 |
| [^213] | [Minimum Width of Leaky-ReLU Neural Networks for Uniform Universal Approximation.](http://arxiv.org/abs/2305.18460) | 研究表明具有临界宽度的Leaky-ReLU神经网络可以在紧致域K上实现$L^p(K,\mathbb{R}^{d_y})$的UAP，而本文给出的最小宽度$w_{\min}=\max(d_x+1,d_y)+1_{d_y=d_x+1}$则适用于函数类$C(K,\mathbb{R}^{d_y})$，考虑到输出维度的影响。 |
| [^214] | [Acceleration of stochastic gradient descent with momentum by averaging: finite-sample rates and asymptotic normality.](http://arxiv.org/abs/2305.17665) | 研究了动量随机梯度下降（SGDM）和其Polyak-averaging版本的特性，表明在较大的批量大小下，小批量SGDM比小批量SGD更快地收敛到最优值的邻域。 |
| [^215] | [Small Language Models Improve Giants by Rewriting Their Outputs.](http://arxiv.org/abs/2305.13514) | 本论文提出了一种方法，通过使用小语言模型重写大语言模型的输出，从而提高其性能。实验证明，该方法可以显着改善大语言模型的少样本学习能力和泛化性能。 |
| [^216] | [Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions.](http://arxiv.org/abs/2305.07303) | 本论文提出了一种从自然语言定义中学习多关系双曲词向量的框架，以捕捉由定义所引起的分层和多分辨率结构。 |
| [^217] | [Piecewise Normalizing Flows.](http://arxiv.org/abs/2305.02930) | 介绍了一种分段归一化流方法，将目标分布分成集群，并通过训练模拟复杂的多模态目标。这种方法可以更好地匹配标准正态基础分布的拓扑结构。 |
| [^218] | [FedIN: Federated Intermediate Layers Learning for Model Heterogeneity.](http://arxiv.org/abs/2304.00759) | FedIN是一种新型的联邦学习方法，支持异构模型，无需公共数据集。在FedIN中，提取器和分类器的模型结构在所有设备中都相同，而中间层的架构可以根据异构设备的资源容量而变化。IN训练可用于利用特征知识，实验结果表明了该方法在图像分类任务上的有效性。 |
| [^219] | [Machine learning for sports betting: should forecasting models be optimised for accuracy or calibration?.](http://arxiv.org/abs/2303.06021) | 该论文研究了机器学习在体育博彩中的应用，提出了优化预测模型校准性比准确度更重要的假设，并通过实验证明了此假设的正确性。 |
| [^220] | [Revisiting LQR Control from the Perspective of Receding-Horizon Policy Gradient.](http://arxiv.org/abs/2302.13144) | 本文从递推视角重新审视了LQR控制问题，并应用递推-视角策略梯度（RHPG）模型提供了一种采样复杂度分析，通过无需任何先验信息进行优化求解，并展示了RHPG在线性控制和估计中的普适性。 |
| [^221] | [Breaking the Communication-Privacy-Accuracy Tradeoff with $f$-Differential Privacy.](http://arxiv.org/abs/2302.09624) | 本文研究了具有隐私问题和有限通信能力的多个用户的协作数据分析问题，在局部差分隐私保证的视角下，我们通过推导离散值机制的紧密$f$-差分隐私保证，进一步研究了隐私放大的稀疏化方法。 |
| [^222] | [Efficacy of MRI data harmonization in the age of machine learning. A multicenter study across 36 datasets.](http://arxiv.org/abs/2211.04125) | 本文描述了一项多中心研究，旨在评估MRI数据协调在机器学习中的效用;作者提出了一种“调和器变压器”方法，在不泄露信息的前提下，在机器学习的预处理步骤中实现了数据协调。 |

# 详细

[^1]: 为解决越界检测而优化特征塑造方法的研究

    Towards Optimal Feature-Shaping Methods for Out-of-Distribution Detection

    [https://arxiv.org/abs/2402.00865](https://arxiv.org/abs/2402.00865)

    本研究针对越界检测，通过优化特征塑造方法，提出了一种抽象优化框架和具体简化方法，以增强特征塑造方法的泛化能力，并提供了一个无法访问越界数据的闭式解决方案。

    

    特征塑造是一类方法，其在越界（OOD）检测中展示出最先进的性能。这些方法通过操作特征表示来区分正常（ID）样本和越界（OOD）样本。然而，现有的特征塑造方法通常使用手动设计的规则，针对具体的模型架构和越界数据集，从而限制了它们的泛化能力。为了解决这个问题，我们首先制定了一个抽象的优化框架来研究特征塑造方法。然后，我们提出了一个具体的简化问题，并证明现有的特征塑造方法近似于该问题的最优解。此外，我们假设越界数据是无法访问的，我们提出了一个公式，可以得到一个分段常数塑造的闭式解决方案。

    Feature shaping refers to a family of methods that exhibit state-of-the-art performance for out-of-distribution (OOD) detection. These approaches manipulate the feature representation, typically from the penultimate layer of a pre-trained deep learning model, so as to better differentiate between in-distribution (ID) and OOD samples. However, existing feature-shaping methods usually employ rules manually designed for specific model architectures and OOD datasets, which consequently limit their generalization ability. To address this gap, we first formulate an abstract optimization framework for studying feature-shaping methods. We then propose a concrete reduction of the framework with a simple piecewise constant shaping function and show that existing feature-shaping methods approximate the optimal solution to the concrete optimization problem. Further, assuming that OOD data is inaccessible, we propose a formulation that yields a closed-form solution for the piecewise constant shapin
    
[^2]: 早期时间分类中的累积准确度间隔控制

    Early Time Classification with Accumulated Accuracy Gap Control

    [https://arxiv.org/abs/2402.00857](https://arxiv.org/abs/2402.00857)

    本文介绍了一种早期时间分类算法，该算法通过统计框架和校准停止规则实现了对完全分类与早期时间分类之间的准确度间隔的有限样本、分布无关的控制。其主要贡献是提出了一种框架，该框架在累计停止时间的条件下控制了一种更强的错误概念的准确度间隔。

    

    早期时间分类算法旨在在不处理完整输入流的情况下对特征流进行标记，同时保持与应用分类器到整个输入时相当的准确性。在本文中，我们引入了一个可以应用于任何顺序分类器的统计框架，制定了一个校准停止规则。这个数据驱动的规则在完全分类和早期时间分类之间的准确度间隔上获得了有限样本的、分布无关的控制。我们首先提出了一种新颖的方法，借鉴了学习-测试校准框架，在独立同分布实例上对这个间隔进行了平均控制。由于这种算法往往会产生过高的早停时间准确度间隔，我们的主要贡献是提出了一个框架，该框架在累计停止时间的条件下控制了一种更强的错误概念，其中准确度间隔受到控制。数值实验证明了该方法的有效性、适用性和实用性。

    Early time classification algorithms aim to label a stream of features without processing the full input stream, while maintaining accuracy comparable to that achieved by applying the classifier to the entire input. In this paper, we introduce a statistical framework that can be applied to any sequential classifier, formulating a calibrated stopping rule. This data-driven rule attains finite-sample, distribution-free control of the accuracy gap between full and early-time classification. We start by presenting a novel method that builds on the Learn-then-Test calibration framework to control this gap marginally, on average over i.i.d. instances. As this algorithm tends to yield an excessively high accuracy gap for early halt times, our main contribution is the proposal of a framework that controls a stronger notion of error, where the accuracy gap is controlled conditionally on the accumulated halt times. Numerical experiments demonstrate the effectiveness, applicability, and usefulnes
    
[^3]: SymbolicAI: 一个结合生成模型和求解器的基于逻辑的方法的框架

    SymbolicAI: A framework for logic-based approaches combining generative models and solvers

    [https://arxiv.org/abs/2402.00854](https://arxiv.org/abs/2402.00854)

    SymbolicAI是一个基于逻辑的框架，将生成模型与多种求解器无缝集成，通过将大型语言模型作为语义解析器，实现了符号推理与生成式人工智能的融合。

    

    我们介绍了SymbolicAI，这是一个多功能且模块化的框架，采用基于逻辑的方法来处理生成过程中的概念学习和流程管理。SymbolicAI通过将大型语言模型（LLM）作为语义解析器来执行基于自然语言和形式语言指令的任务，从而弥合了符号推理和生成式人工智能之间的差距，使生成模型与各种求解器无缝集成。我们利用概率编程原理来处理复杂任务，并利用可微分和经典编程范 paradigms 的各自优势。该框架引入了一系列多态的、组合的和自指的数据流操作，将LLM的输出与用户的目标对齐。因此，我们可以在具有零次和少次学习能力的各种基础模型之间进行过渡，并与擅长解决特定问题的专业化调优模型或求解器配合使用。

    We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes. SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI. We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths. The framework introduces a set of polymorphic, compositional, and self-referential operations for data stream manipulation, aligning LLM outputs with user objectives. As a result, we can transition between the capabilities of various foundation models endowed with zero- and few-shot learning capabilities and specialized, fine-tuned models or solvers proficient in addres
    
[^4]: LTAU-FF: 原子力场中不确定性的损失轨迹分析

    LTAU-FF: Loss Trajectory Analysis for Uncertainty in Atomistic Force Fields

    [https://arxiv.org/abs/2402.00853](https://arxiv.org/abs/2402.00853)

    LTAU-FF是一种利用损失轨迹分析来估计原子力场中不确定性的方法，通过累积分布函数和模型潜空间的相似性搜索，实现了高效的模型集合表示和不确定度量，无需评估多个模型，能准确预测测试误差。

    

    模型集合是估计深度学习原子力场预测不确定性的简单而有效的工具。尽管如此，使用基于集合的不确定度量技术的广泛应用受到训练和推理过程中集合产生的高计算成本的限制。在这项工作中，我们利用在训练过程中获得的逐样本误差的累积分布函数（CDF）来高效表示模型集合，并将它们与基于距离的模型潜空间中的相似性搜索相结合。利用这些工具，我们开发了一个简单的不确定度量指标（称为LTAU），它在训练或推理过程中无需评估多个模型，同时发挥了集合技术的优势。作为初始测试，我们将我们的方法应用于估计原子力场中的认知不确定性（LTAU-FF），并证明它可以被轻松地校准以准确预测测试误差。

    Model ensembles are simple and effective tools for estimating the prediction uncertainty of deep learning atomistic force fields. Despite this, widespread adoption of ensemble-based uncertainty quantification (UQ) techniques is limited by the high computational costs incurred by ensembles during both training and inference. In this work we leverage the cumulative distribution functions (CDFs) of per-sample errors obtained over the course of training to efficiently represent the model ensemble, and couple them with a distance-based similarity search in the model latent space. Using these tools, we develop a simple UQ metric (which we call LTAU) that leverages the strengths of ensemble-based techniques without requiring the evaluation of multiple models during either training or inference. As an initial test, we apply our method towards estimating the epistemic uncertainty in atomistic force fields (LTAU-FF) and demonstrate that it can be easily calibrated to accurately predict test erro
    
[^5]: 具有高度相关注释的拉曼光谱的数据增强方案

    Data Augmentation Scheme for Raman Spectra with Highly Correlated Annotations

    [https://arxiv.org/abs/2402.00851](https://arxiv.org/abs/2402.00851)

    该论文提出了一种利用光谱可加性的方法，从给定数据集中生成具有统计独立标签的额外数据点，用于训练能够处理非高斯噪声和非线性依赖关系的卷积神经网络。

    

    在生物技术中，拉曼光谱法作为一种过程分析技术（PAT）快速得到了广泛应用，它可以测量细胞密度、底物和产物浓度。由于拉曼光谱记录了分子的振动模式，因此可以非侵入性地在一个光谱中提供相关信息。通常，偏最小二乘（PLS）是从光谱中推断感兴趣变量信息的模型选择。然而，生物过程以其复杂性而闻名，其中卷积神经网络（CNN）是一个强大的替代方法。它们可以处理非高斯噪声，并考虑光束错位、像素故障或其他物质的存在。然而，它们在模型训练过程中需要大量数据，并且能够捕捉到过程变量之间的非线性依赖关系。在这项工作中，我们利用光谱的可加性来生成从给定数据集中得到的具有统计独立标签的额外数据点，以便训练网络。

    In biotechnology Raman Spectroscopy is rapidly gaining popularity as a process analytical technology (PAT) that measures cell densities, substrate- and product concentrations. As it records vibrational modes of molecules it provides that information non-invasively in a single spectrum. Typically, partial least squares (PLS) is the model of choice to infer information about variables of interest from the spectra. However, biological processes are known for their complexity where convolutional neural networks (CNN) present a powerful alternative. They can handle non-Gaussian noise and account for beam misalignment, pixel malfunctions or the presence of additional substances. However, they require a lot of data during model training, and they pick up non-linear dependencies in the process variables. In this work, we exploit the additive nature of spectra in order to generate additional data points from a given dataset that have statistically independent labels so that a network trained on
    
[^6]: 基于得分的因果表示学习：线性和一般的转化

    Score-based Causal Representation Learning: Linear and General Transformations

    [https://arxiv.org/abs/2402.00849](https://arxiv.org/abs/2402.00849)

    这篇论文提出了一种基于得分的算法类，用于干预范围内的因果表示学习，涵盖了线性和一般转化。算法保证了可识别性和实现性，并且通过创造性地将得分函数与因果表示学习相结合。

    

    本篇论文针对一般非参数潜在因果模型和将潜在变量映射到观测变量的未知转化，研究了基于干预的因果表示学习（CRL）。研究了线性和一般的转化。这篇论文同时讨论了可识别性和实现性两个方面。可识别性是指确定算法不相关的条件，以确保恢复真实的潜在因果变量和潜在因果图。实现性是指算法方面，解决设计算法来实现可识别保证的问题。通过将得分函数（即密度函数对数的梯度）与CRL之间建立新联系，本文设计了一种得分为基础的算法类，确保了可识别性和实现性。首先，本文专注于线性转化，并展示了每个n个随机硬干预下该转化的因果表示可识别。

    This paper addresses intervention-based causal representation learning (CRL) under a general nonparametric latent causal model and an unknown transformation that maps the latent variables to the observed variables. Linear and general transformations are investigated. The paper addresses both the \emph{identifiability} and \emph{achievability} aspects. Identifiability refers to determining algorithm-agnostic conditions that ensure recovering the true latent causal variables and the latent causal graph underlying them. Achievability refers to the algorithmic aspects and addresses designing algorithms that achieve identifiability guarantees. By drawing novel connections between \emph{score functions} (i.e., the gradients of the logarithm of density functions) and CRL, this paper designs a \emph{score-based class of algorithms} that ensures both identifiability and achievability. First, the paper focuses on \emph{linear} transformations and shows that one stochastic hard intervention per n
    
[^7]: X-CBA: 基于可解释性的CatBoosted Anomal-E用于入侵检测系统

    X-CBA: Explainability Aided CatBoosted Anomal-E for Intrusion Detection System

    [https://arxiv.org/abs/2402.00839](https://arxiv.org/abs/2402.00839)

    本论文介绍了一种名为X-CBA的新颖可解释型IDS方法，该方法利用图神经网络处理网络流量数据，并采用了新的可解释人工智能方法。它通过利用更广泛的流量数据，包括边属性，以处理网络威胁的挑战。

    

    在网络威胁日益复杂的时代，入侵检测系统（IDS）的效果至关重要。机器学习（ML）和深度学习（DL）模型为识别计算机网络中的攻击和异常提供了高效准确的解决方案。然而，在IDS中使用ML和DL模型导致了信任赤字，因为它们的决策过程不透明。这种IDS研究中的透明度差距显著，影响了信心和问责制。为了解决这个问题，本文引入了一种新颖的可解释型IDS方法，称为X-CBA，它利用图神经网络（GNN）的结构优势来有效处理网络流量数据，并采用新的可解释人工智能（XAI）方法。与大多数以GNN为基础的IDS不同，我们的方法不仅依赖于标记的网络流量和节点特征，还通过网络流量，包括边属性，来利用更广泛的流量数据。

    The effectiveness of Intrusion Detection Systems (IDS) is critical in an era where cyber threats are becoming increasingly complex. Machine learning (ML) and deep learning (DL) models provide an efficient and accurate solution for identifying attacks and anomalies in computer networks. However, using ML and DL models in IDS has led to a trust deficit due to their non-transparent decision-making. This transparency gap in IDS research is significant, affecting confidence and accountability. To address, this paper introduces a novel Explainable IDS approach, called X-CBA, that leverages the structural advantages of Graph Neural Networks (GNNs) to effectively process network traffic data, while also adapting a new Explainable AI (XAI) methodology. Unlike most GNN-based IDS that depend on labeled network traffic and node features, thereby overlooking critical packet-level information, our approach leverages a broader range of traffic data through network flows, including edge attributes, to
    
[^8]: ALISON: 快速有效的风格学作者身份混淆

    ALISON: Fast and Effective Stylometric Authorship Obfuscation

    [https://arxiv.org/abs/2402.00835](https://arxiv.org/abs/2402.00835)

    ALISON是一种快速有效的风格学作者身份混淆方法，通过攻击AA模型来保护隐私，相比竞争方法，具有更快的混淆速度和更好的混淆成功率。

    

    作者归属度（AA）和作者身份混淆（AO）是隐私研究中日益重要的两项竞争任务。现代AA利用作者的一贯写作风格，使用AA分类器将文本与其作者匹配。AO是相应的对抗性任务，旨在以一种方式修改文本，使其语义得到保留，但AA模型无法正确推断其作者。为了解决最先进的AA方法引发的隐私问题，提出了新的AO方法，但由于其训练和混淆速度过慢（通常需要数小时），使用起来仍然不太实际。面对这一挑战，我们提出了一种实用的AO方法ALISON，它（1）大大减少了训练/混淆时间，演示了比最先进的AO方法快10倍以上的混淆速度，（2）通过攻击两个基准数据集上的三种基于Transformer的AA方法，实现了更好的混淆成功率，通常比竞争方法表现好15%。

    Authorship Attribution (AA) and Authorship Obfuscation (AO) are two competing tasks of increasing importance in privacy research. Modern AA leverages an author's consistent writing style to match a text to its author using an AA classifier. AO is the corresponding adversarial task, aiming to modify a text in such a way that its semantics are preserved, yet an AA model cannot correctly infer its authorship. To address privacy concerns raised by state-of-the-art (SOTA) AA methods, new AO methods have been proposed but remain largely impractical to use due to their prohibitively slow training and obfuscation speed, often taking hours. To this challenge, we propose a practical AO method, ALISON, that (1) dramatically reduces training/obfuscation time, demonstrating more than 10x faster obfuscation than SOTA AO methods, (2) achieves better obfuscation success through attacking three transformer-based AA methods on two benchmark datasets, typically performing 15% better than competing method
    
[^9]: YANG辅助下的骨干网络黑洞检测统一策略

    A YANG-aided Unified Strategy for Black Hole Detection for Backbone Networks

    [https://arxiv.org/abs/2402.00831](https://arxiv.org/abs/2402.00831)

    该论文介绍了一种使用YANG数据模型与黑洞敏感度量矩阵分析的方法，用于在骨干网络中进行黑洞检测。这种方法填补了骨干网络黑洞检测方法的空白，提供了有效的检测策略。

    

    尽管在互联网骨干网络中解决黑洞故障的重要性不可忽视，但骨干网络中的有效检测策略仍然缺乏。这主要是因为先前的研究主要集中在移动自组网(MANETs)上，而MANETs在动态、协议和拓扑上有着完全不同的操作，因此其研究结果无法直接应用于骨干网络。此外，骨干网络中的黑洞故障检测是一项特别具有挑战性的任务。由于需要考虑多样的条件，这需要收集大量的网络数据，使得数据收集和分析变得并不直观。为填补这一空白，我们的研究引入了一种新颖的方法，使用专门的Yet Another Next Generation (YANG)数据模型与黑洞敏感度量矩阵(BHMM)分析来进行骨干网络中的黑洞检测。本文详细介绍了我们选择和分析了与黑洞检测相关的四个YANG模型的方法。

    Despite the crucial importance of addressing Black Hole failures in Internet backbone networks, effective detection strategies in backbone networks are lacking. This is largely because previous research has been centered on Mobile Ad-hoc Networks (MANETs), which operate under entirely different dynamics, protocols, and topologies, making their findings not directly transferable to backbone networks. Furthermore, detecting Black Hole failures in backbone networks is particularly challenging. It requires a comprehensive range of network data due to the wide variety of conditions that need to be considered, making data collection and analysis far from straightforward. Addressing this gap, our study introduces a novel approach for Black Hole detection in backbone networks using specialized Yet Another Next Generation (YANG) data models with Black Hole-sensitive Metric Matrix (BHMM) analysis. This paper details our method of selecting and analyzing four YANG models relevant to Black Hole de
    
[^10]: 具有复杂几何体的PDE的分辨率不变的深度算子网络

    Resolution invariant deep operator network for PDEs with complex geometries

    [https://arxiv.org/abs/2402.00825](https://arxiv.org/abs/2402.00825)

    本文提出了一种名为分辨率不变的深度算子网络（RDO）的新框架，通过解耦输入和输出的空间域来解决神经算子（NO）的应用限制问题，可以解决具有复杂几何体的PDE。 (RDO is a novel framework that decouples the spatial domain of input and output, addressing the limitation of neural operators (NO) and enabling the resolution of PDEs with complex geometries.)

    

    神经算子（NO）是具有功能性输出的离散不变深度学习方法，可以逼近任何连续性算子。NO在解决偏微分方程（PDE）方面表现出优势，超过其他深度学习方法。然而，其输入函数的空间域需要与输出函数相同，这限制了其适用性。例如，广泛使用的傅立叶神经算子（FNO）无法逼近将边界条件映射到PDE解的算子。为解决这个问题，我们提出了一种称为分辨率不变深度算子（RDO）的新框架，它将输入和输出的空间域解耦。RDO受到深度算子网络（DeepONet）的启发，与DeepONet相比，它不需要对网络进行重新训练来适应输入/输出的变化。RDO接受功能性输入，其输出也是功能性的，因此保持NO的分辨率不变性质。它还可以解决具有复杂几何体的PDE。

    Neural operators (NO) are discretization invariant deep learning methods with functional output and can approximate any continuous operator. NO have demonstrated the superiority of solving partial differential equations (PDEs) over other deep learning methods. However, the spatial domain of its input function needs to be identical to its output, which limits its applicability. For instance, the widely used Fourier neural operator (FNO) fails to approximate the operator that maps the boundary condition to the PDE solution. To address this issue, we propose a novel framework called resolution-invariant deep operator (RDO) that decouples the spatial domain of the input and output. RDO is motivated by the Deep operator network (DeepONet) and it does not require retraining the network when the input/output is changed compared with DeepONet. RDO takes functional input and its output is also functional so that it keeps the resolution invariant property of NO. It can also resolve PDEs with com
    
[^11]: SLIM: 多判别器在技能学习中的应用

    SLIM: Skill Learning with Multiple Critics

    [https://arxiv.org/abs/2402.00823](https://arxiv.org/abs/2402.00823)

    SLIM是一种多判别器学习方法，通过在机器人操作中组合多个判别器的奖励函数，显著改善了潜变量技能发现，克服了奖励之间的干扰。

    

    自我监督的技能学习旨在获取利用环境的底层动态的有用行为。基于互信息最大化的潜变量模型在此任务中取得了显著的成功，但在机器人操作领域仍存在困难。由于机器人操作可能涉及到环境中很多自由度，单纯的互信息最大化无法产生有用的操作行为。为了解决这个问题，我们引入了SLIM，一种针对机器人操作的多判别器学习方法。我们的主要观点是，在演员-评论者框架中利用多个判别器来优雅地组合多个奖励函数，能够显著改善机器人操作的潜变量技能发现，同时克服奖励之间可能发生的干扰，阻碍对有用技能的收敛。

    Self-supervised skill learning aims to acquire useful behaviors that leverage the underlying dynamics of the environment. Latent variable models, based on mutual information maximization, have been particularly successful in this task but still struggle in the context of robotic manipulation. As it requires impacting a possibly large set of degrees of freedom composing the environment, mutual information maximization fails alone in producing useful manipulation behaviors. To address this limitation, we introduce SLIM, a multi-critic learning approach for skill discovery with a particular focus on robotic manipulation. Our main insight is that utilizing multiple critics in an actor-critic framework to gracefully combine multiple reward functions leads to a significant improvement in latent-variable skill discovery for robotic manipulation while overcoming possible interference occurring among rewards which hinders convergence to useful skills. Furthermore, in the context of tabletop man
    
[^12]: 在连续环境中利用近似基于模型的屏蔽技术实现概率安全保证

    Leveraging Approximate Model-based Shielding for Probabilistic Safety Guarantees in Continuous Environments

    [https://arxiv.org/abs/2402.00816](https://arxiv.org/abs/2402.00816)

    本文提出了在连续环境中利用近似基于模型的屏蔽技术实现概率安全保证的方法，并通过与约束强化学习算法的对比实验证明了其通用性和稳定性。

    

    屏蔽技术是实现安全增强学习的一种流行技术。然而，传统的屏蔽方法存在相当严格的假设，使其难以在复杂环境中部署，特别是在具有连续状态或行动空间的环境中。本文将更通用的近似基于模型的屏蔽（AMBS）框架扩展到连续环境中。我们使用Safety Gym作为我们的测试平台，可以更直接地将AMBS与流行的约束强化学习算法进行比较。我们还为连续环境提供了强大的概率安全保证。此外，我们提出了两种新颖的惩罚技术，直接修改策略梯度，在我们的实验中实现了更稳定的收敛。

    Shielding is a popular technique for achieving safe reinforcement learning (RL). However, classical shielding approaches come with quite restrictive assumptions making them difficult to deploy in complex environments, particularly those with continuous state or action spaces. In this paper we extend the more versatile approximate model-based shielding (AMBS) framework to the continuous setting. In particular we use Safety Gym as our test-bed, allowing for a more direct comparison of AMBS with popular constrained RL algorithms. We also provide strong probabilistic safety guarantees for the continuous setting. In addition, we propose two novel penalty techniques that directly modify the policy gradient, which empirically provide more stable convergence in our experiments.
    
[^13]: 扩散基于语音增强的方差分析

    An Analysis of the Variance of Diffusion-based Speech Enhancement

    [https://arxiv.org/abs/2402.00811](https://arxiv.org/abs/2402.00811)

    本研究发现，方差的规模是影响语音增强性能的主要参数，较大的方差可增加噪声抑制并减少计算量。

    

    扩散模型被证明是用于生成语音增强的强大模型。在最近的SGMSE+方法中，训练涉及到控制演化过程中均值和方差的随机微分方程，在逐渐添加高斯噪声和环境噪声到干净语音信号中。语音增强性能取决于选择用来控制添加环境和高斯噪声下演化过程的随机微分方程。在这项工作中，我们强调方差的规模是影响语音增强性能的主要参数，并且展示了它控制着噪声抑制和语音失真之间的权衡。更具体地说，我们展示了较大的方差增加了噪声抑制并且减少了计算量，因为产生估计的函数评估次数减少了。

    Diffusion models proved to be powerful models for generative speech enhancement. In recent SGMSE+ approaches, training involves a stochastic differential equation for the diffusion process, adding both Gaussian and environmental noise to the clean speech signal gradually. The speech enhancement performance varies depending on the choice of the stochastic differential equation that controls the evolution of the mean and the variance along the diffusion processes when adding environmental and Gaussian noise. In this work, we highlight that the scale of the variance is a dominant parameter for speech enhancement performance and show that it controls the tradeoff between noise attenuation and speech distortions. More concretely, we show that a larger variance increases the noise attenuation and allows for reducing the computational footprint, as fewer function evaluations for generating the estimate are required.
    
[^14]: 《在大规模人工智能时代的贝叶斯深度学习》的立场论文

    Position Paper: Bayesian Deep Learning in the Age of Large-Scale AI

    [https://arxiv.org/abs/2402.00809](https://arxiv.org/abs/2402.00809)

    《在大规模人工智能时代的贝叶斯深度学习》这篇立场论文探讨了贝叶斯深度学习在各种不同设置下的优势，并指出了与之相关的挑战和有趣的研究方向。未来的研究重点将放在如何将大规模基础模型与贝叶斯深度学习相结合，以发挥它们的全部潜力。

    

    在当前的深度学习研究领域中，人们主要关注在涉及大规模图像和语言数据集的监督任务中实现高预测准确性。然而，更广泛的视角揭示了许多被忽视的度量标准、任务和数据类型，如不确定性、主动和持续学习以及科学数据，这些方面需要关注。贝叶斯深度学习（BDL）是一条有前景的道路，可以在这些不同的设置中提供优势。本文认为BDL可以提升深度学习的能力。它重新审视了BDL的优势、承认了现有的挑战，并重点介绍了一些旨在解决这些障碍的有趣的研究方向。展望未来，讨论集中在可能的方式上，将大规模基础模型与BDL相结合，以充分发挥它们的潜力。

    In the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets. However, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention. Bayesian deep learning (BDL) constitutes a promising avenue, offering advantages across these diverse settings. This paper posits that BDL can elevate the capabilities of deep learning. It revisits the strengths of BDL, acknowledges existing challenges, and highlights some exciting research avenues aimed at addressing these obstacles. Looking ahead, the discussion focuses on possible ways to combine large-scale foundation models with BDL to unlock their full potential.
    
[^15]: 通过轨迹拼接将条件性扩散模型提炼为离线强化学习的方法

    Distilling Conditional Diffusion Models for Offline Reinforcement Learning through Trajectory Stitching

    [https://arxiv.org/abs/2402.00807](https://arxiv.org/abs/2402.00807)

    通过新的轨迹拼接算法和奖励生成器，使用条件性扩散模型生成的高回报轨迹与原始轨迹混合，应用于行为克隆的结果表明，学得的规模较小的浅层策略在多个D4RL基准测试中超过或接近深度生成规划器。

    

    深度生成模型最近已经成为离线强化学习的一种有效方法。然而，它们的大模型规模在计算上提出了挑战。为了解决这个问题，我们提出了一种基于数据增强的知识蒸馏方法。具体而言，我们从条件性扩散模型中生成高回报轨迹，并通过一种新的拼接算法将它们与原始轨迹混合，该算法利用了一种新的奖励生成器。将所得到的数据集应用于行为克隆，学得的规模较小的浅层策略在几个D4RL基准测试中胜过或接近深度生成规划器。

    Deep generative models have recently emerged as an effective approach to offline reinforcement learning. However, their large model size poses challenges in computation. We address this issue by proposing a knowledge distillation method based on data augmentation. In particular, high-return trajectories are generated from a conditional diffusion model, and they are blended with the original trajectories through a novel stitching algorithm that leverages a new reward generator. Applying the resulting dataset to behavioral cloning, the learned shallow policy whose size is much smaller outperforms or nearly matches deep generative planners on several D4RL benchmarks.
    
[^16]: 时间序列数据信号质量审计

    Signal Quality Auditing for Time-series Data

    [https://arxiv.org/abs/2402.00803](https://arxiv.org/abs/2402.00803)

    这个论文介绍了一个开源软件工具包，用于评估时间序列数据的信号质量，提供了一系列信号质量指标和去噪方法的研究，并提供了基准数据的验证。

    

    信号质量评估（SQA）对于监测数据采集系统的可靠性至关重要，特别是在人工智能驱动的预测性维护（PMx）应用环境中。SQA对于解决数据采集硬件和软件的“静默故障”至关重要，这些故障如果不注意，会误导数据用户，增加不正确决策的风险，可能导致意外甚至灾难性后果。我们开发了一个开源软件实现信号质量指标（SQIs）用于分析时间序列数据。我们编写了一系列SQIs，使用已建立的基准数据进行演示，并且证明它们可以有效地评估信号质量。我们还研究了用于去噪时间序列数据的替代方法，试图改善已经受损的信号质量，并在相关的真实世界数据上进行了实证评估。据我们所知，我们的软件工具包是第一个提供广泛范围信号质量评估开源实现的。

    Signal quality assessment (SQA) is required for monitoring the reliability of data acquisition systems, especially in AI-driven Predictive Maintenance (PMx) application contexts. SQA is vital for addressing "silent failures" of data acquisition hardware and software, which when unnoticed, misinform the users of data, creating the risk for incorrect decisions with unintended or even catastrophic consequences. We have developed an open-source software implementation of signal quality indices (SQIs) for the analysis of time-series data. We codify a range of SQIs, demonstrate them using established benchmark data, and show that they can be effective for signal quality assessment. We also study alternative approaches to denoising time-series data in an attempt to improve the quality of the already degraded signal, and evaluate them empirically on relevant real-world data. To our knowledge, our software toolkit is the first to provide an open source implementation of a broad range of signal 
    
[^17]: 正式-LLM：将形式语言和自然语言集成于可控的LLM智能体中

    Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents

    [https://arxiv.org/abs/2402.00798](https://arxiv.org/abs/2402.00798)

    本文提出了一种将自然语言和形式语言整合的“正式-LLM”框架，用于解决现有LLM智能体无法控制的计划生成问题。实验证明，该框架在提高生成计划性能和确保可控性方面取得了显著改进。

    

    最近，对于大型语言模型（LLMs）的进展使得人工智能智能体能够自动生成和执行解决复杂任务的多步计划。然而，由于LLM的内容生成过程几乎无法控制，当前的LLM智能体经常生成无效或不可执行的计划，这损害了生成计划的性能并破坏了用户对LLM智能体的信任。为应对这个问题，本文提出了一种新颖的“正式-LLM”框架，用于LLM智能体，通过将自然语言的表达力和形式语言的精确性进行整合。具体而言，该框架允许人类用户将他们对计划过程的要求或约束表达为自动机。然后，在自动机的监督下，使用基于堆栈的LLM计划生成过程来确保生成的计划满足约束条件，从而使计划过程可控。我们在基准任务和实际的真实任务上进行了实验，并且obtained significant improvements over existing LLM-based agents, demonstrating the effectiveness and controllability of the proposed Formal-LLM framework.

    Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and o
    
[^18]: LLMs学习动力系统的控制原理，揭示了上下文中的神经比例定律

    LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law

    [https://arxiv.org/abs/2402.00795](https://arxiv.org/abs/2402.00795)

    本文研究了预训练语言模型LLMs对动力系统行为的外推能力，发现LLaMA 2能够准确预测动力系统的时间序列。此外，输入上下文窗口的长度越长，学习到的物理规则的准确性越高，揭示了一种上下文中的神经比例定律。

    

    预训练的大型语言模型（LLMs）在零-shot任务，包括时间序列预测方面表现出惊人的有效性。然而，由于模型的复杂性，理解其背后的机制仍然极具挑战性。本文研究了LLMs对受物理原理控制的动力系统行为的外推能力。我们的结果表明，主要在文本上进行训练的语言模型LLaMA 2在没有微调或提示工程的情况下，能够准确预测动力系统的时间序列。此外，学习到的物理规则的准确性随着输入上下文窗口的长度增加而增加，揭示了一种上下文中的神经比例定律。同时，我们还提出了一种灵活高效的算法，用于直接从LLMs中提取多位数的概率密度函数。

    Pretrained large language models (LLMs) are surprisingly effective at performing zero-shot tasks, including time-series forecasting. However, understanding the mechanisms behind such capabilities remains highly challenging due to the complexity of the models. In this paper, we study LLMs' ability to extrapolate the behavior of dynamical systems whose evolution is governed by principles of physical interest. Our results show that LLaMA 2, a language model trained primarily on texts, achieves accurate predictions of dynamical system time series without fine-tuning or prompt engineering. Moreover, the accuracy of the learned physical rules increases with the length of the input context window, revealing an in-context version of neural scaling law. Along the way, we present a flexible and efficient algorithm for extracting probability density functions of multi-digit numbers directly from LLMs.
    
[^19]: ReAGent: 一个面向生成语言模型的模型无关特征归因方法

    ReAGent: Towards A Model-agnostic Feature Attribution Method for Generative Language Models

    [https://arxiv.org/abs/2402.00794](https://arxiv.org/abs/2402.00794)

    本论文介绍了一种面向生成语言模型的模型无关特征归因方法，称为ReAGent。该方法解决了现有方法在文本生成中的适用性问题，并提供了计算上效率更高的选择。

    

    特征归因方法（FAs），如梯度和注意力机制，被广泛应用于确定所有输入特征对模型预测的重要性。现有自然语言处理领域的研究主要集中在为仅有编码器的语言模型（LMs）开发和测试FAs，用于分类任务。然而，尚不清楚在文本生成上是否可以使用这些FAs来处理仅有解码器的模型，因为模型架构和任务设置之间存在固有差异。此外，先前的研究已经证明，没有一个通用的FA适用于所有模型和任务。这使得针对大型LMs选择FA计算上非常昂贵，因为输入重要性的推导通常需要多个前向和反向传递，包括可能是限制性的梯度计算。为了解决这些问题，我们提出了一种面向生成LMs的模型无关FA，称为递归归因生成器（ReAGent）。

    Feature attribution methods (FAs), such as gradients and attention, are widely employed approaches to derive the importance of all input features to the model predictions. Existing work in natural language processing has mostly focused on developing and testing FAs for encoder-only language models (LMs) in classification tasks. However, it is unknown if it is faithful to use these FAs for decoder-only models on text generation, due to the inherent differences between model architectures and task settings respectively. Moreover, previous work has demonstrated that there is no `one-wins-all' FA across models and tasks. This makes the selection of a FA computationally expensive for large LMs since input importance derivation often requires multiple forward and backward passes including gradient computations that might be prohibitive even with access to large compute. To address these issues, we present a model-agnostic FA for generative LMs called Recursive Attribution Generator (ReAGent)
    
[^20]: 无法区分的区分：算法预测中的人类专业知识

    Distinguishing the Indistinguishable: Human Expertise in Algorithmic Prediction

    [https://arxiv.org/abs/2402.00793](https://arxiv.org/abs/2402.00793)

    本论文介绍了一种新的框架，将人类专业知识纳入算法预测中，重点在于利用人的判断力区分对于任何可行的预测算法来说“看起来相同”的输入。

    

    我们引入了一种将人类专业知识纳入算法预测的新框架。我们的方法主要关注利用人的判断力来区分那些对于任何可行的预测算法来说“看起来相同”的输入。我们认为，这种框架能够澄清人工智能与人类协作预测任务中的问题，因为专家通常具有信息的访问权限——特别是主观信息——而这些信息是算法训练数据中没有编码的。基于这一认识，我们开发了一组有原则的算法，仅在任何可行的预测器的性能有所改善时才选择性地纳入人类反馈。经验结果表明，尽管算法在平均水平上往往优于人类对应任务的能力，但人类判断在特定情况下（可以预先确定）能够显著提高算法预测的性能。在一个X射线分类任务中，我们发现这个子集在患者群体中占据了近30%。我们的方法提供了一种自然的方式，

    We introduce a novel framework for incorporating human expertise into algorithmic predictions. Our approach focuses on the use of human judgment to distinguish inputs which `look the same' to any feasible predictive algorithm. We argue that this framing clarifies the problem of human/AI collaboration in prediction tasks, as experts often have access to information -- particularly subjective information -- which is not encoded in the algorithm's training data. We use this insight to develop a set of principled algorithms for selectively incorporating human feedback only when it improves the performance of any feasible predictor. We find empirically that although algorithms often outperform their human counterparts on average, human judgment can significantly improve algorithmic predictions on specific instances (which can be identified ex-ante). In an X-ray classification task, we find that this subset constitutes nearly 30% of the patient population. Our approach provides a natural way
    
[^21]: Graph-Mamba: 通过选择性状态空间进行长程图序列建模

    Graph-Mamba: Towards Long-Range Graph Sequence Modeling with Selective State Spaces

    [https://arxiv.org/abs/2402.00789](https://arxiv.org/abs/2402.00789)

    Graph-Mamba是第一个尝试通过将Mamba模块与输入相关的节点选择机制集成来增强图网络中长程上下文建模的方法。

    

    注意力机制在图变换器中广泛用于捕捉节点之间的长程依赖关系。由于二次计算成本的限制，注意力机制在大型图中无法扩展。最近的计算效率改进主要通过使用随机或基于启发式的图子采样进行注意力稀疏化实现，但在数据相关的上下文推理方面效果不佳。状态空间模型（SSM）（如Mamba）因其在序列数据中建模长程依赖关系的效果和效率而受到关注。然而，将SSM适应非序列图数据是一个显著的挑战。在这项工作中，我们介绍了图-Mamba，这是第一个通过将Mamba模块与基于输入的节点选择机制集成，以增强图网络中的长程上下文建模的尝试。具体而言，我们制定了以图为中心的节点优先级和排列策略来增强上下文感知推理，从而实现了实质性的效果改进。

    Attention mechanisms have been widely used to capture long-range dependencies among nodes in Graph Transformers. Bottlenecked by the quadratic computational cost, attention mechanisms fail to scale in large graphs. Recent improvements in computational efficiency are mainly achieved by attention sparsification with random or heuristic-based graph subsampling, which falls short in data-dependent context reasoning. State space models (SSMs), such as Mamba, have gained prominence for their effectiveness and efficiency in modeling long-range dependencies in sequential data. However, adapting SSMs to non-sequential graph data presents a notable challenge. In this work, we introduce Graph-Mamba, the first attempt to enhance long-range context modeling in graph networks by integrating a Mamba block with the input-dependent node selection mechanism. Specifically, we formulate graph-centric node prioritization and permutation strategies to enhance context-aware reasoning, leading to a substantia
    
[^22]: 用多智能体强化学习学习和校准异质有界理性市场行为

    Learning and Calibrating Heterogeneous Bounded Rational Market Behaviour with Multi-Agent Reinforcement Learning

    [https://arxiv.org/abs/2402.00787](https://arxiv.org/abs/2402.00787)

    本论文提出了一种方法，利用多智能体强化学习学习和校准异质有界理性市场行为。这种方法可以消除在基于智能体的模型中手动定义行为规则的需要，并将智能体表示与经济和金融模型相一致。

    

    基于智能体的模型（ABM）已经显示出在无法与传统均衡分析兼容的各种现实世界现象建模方面的潜力。然而，一个关键问题是在ABM中手动定义行为规则。最近在多智能体强化学习（MARL）方面的发展提供了一种从优化角度解决这个问题的方法，智能体们努力最大化自己的效用，消除了手动规则规定的需要。这种以学习为重点的方法通过使用理性的效用最大化智能体与已有的经济和金融模型相一致。然而，这种表达方式违背了ABM的根本动机：可以建模从有界理性和智能体异质性中产生的真实动态。为了解决这两种方法之间的明显不一致，我们提出了一种在MARL框架中表示异质处理受限代理的新技术。

    Agent-based models (ABMs) have shown promise for modelling various real world phenomena incompatible with traditional equilibrium analysis. However, a critical concern is the manual definition of behavioural rules in ABMs. Recent developments in multi-agent reinforcement learning (MARL) offer a way to address this issue from an optimisation perspective, where agents strive to maximise their utility, eliminating the need for manual rule specification. This learning-focused approach aligns with established economic and financial models through the use of rational utility-maximising agents. However, this representation departs from the fundamental motivation for ABMs: that realistic dynamics emerging from bounded rationality and agent heterogeneity can be modelled. To resolve this apparent disparity between the two approaches, we propose a novel technique for representing heterogeneous processing-constrained agents within a MARL framework. The proposed approach treats agents as constraine
    
[^23]: CroissantLLM: 一个真正的双语法语-英语语言模型

    CroissantLLM: A Truly Bilingual French-English Language Model

    [https://arxiv.org/abs/2402.00786](https://arxiv.org/abs/2402.00786)

    CroissantLLM是一个1.3B的双语语言模型，通过使用1:1的英语-法语预训练数据比例、自定义的分词器和双语调优数据集进行训练，实现了高性能和开源。模型还发布了训练数据集和多个检查点，以及一个法语基准测试 FrenchBench。

    

    我们介绍了CroissantLLM，这是一个在3T个英语和法语标记上预训练的13亿语言模型，为研究和工业社区带来了一种高性能的、完全开源的双语模型，可以在消费级本地硬件上快速运行。为此，我们首次尝试使用1:1的英语-法语预训练数据比例、自定义的分词器和双语调优数据集来训练一种内在双语的模型。我们发布了训练数据集，其中包含了一个法语分割，其中包含了手工策划、高质量和多样化的数据源。为了评估在英语以外的性能，我们创建了一个新的基准测试 FrenchBench，包括一系列分类和生成任务，涵盖了模型在法语语言中性能的各个方面。此外，为了保持透明度并促进进一步的大规模语言模型研究，我们发布了代码库和各种模型规模、训练数据分布上的几十个检查点。

    We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T English and French tokens, to bring to the research and industrial community a high-performance, fully open-sourced bilingual model that runs swiftly on consumer-grade local hardware. To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets. We release the training dataset, notably containing a French split with manually curated, high-quality, and varied data sources. To assess performance outside of English, we craft a novel benchmark, FrenchBench, consisting of an array of classification and generation tasks, covering various orthogonal aspects of model performance in the French Language. Additionally, rooted in transparency and to foster further Large Language Model research, we release codebases, and dozens of checkpoints across various model sizes, training data distributions, 
    
[^24]: 强化学习从人类反馈中获得的稠密奖励自由

    Dense Reward for Free in Reinforcement Learning from Human Feedback

    [https://arxiv.org/abs/2402.00782](https://arxiv.org/abs/2402.00782)

    在强化学习中，我们通过利用奖励模型中的注意权重，将奖励重新分配到完成的所有标记上，从而稠密化信号并突出显示最重要的标记。这项工作使得在传统的强化学习中很难优化的问题得到了解决。

    

    强化学习从人类反馈中获得的稠密奖励自由（RLHF）被认为是使大型语言模型（LLMs）能够有效地遵循指示并产生有用协助的关键进展。传统上，这涉及到在回答查询之前从LLM中生成完成，并使用单独的奖励模型为完整的完成指定一个分数。作为一个自回归过程，LLM必须经历许多“动作”（选择单个标记），并在一个episode结束时只收到一个单独的稀疏奖励，这一设置被认为在传统的强化学习中很难优化。在这项工作中，我们利用奖励模型包含的不仅仅是标量输出的更多信息，尤其是作为transformer架构的一部分，它计算了一个对标记进行注意力映射。我们利用这些注意力权重来重新分配奖励，使信号变得密集并突出显示最重要的标记，而无需

    Reinforcement Learning from Human Feedback (RLHF) has been credited as the key advance that has allowed Large Language Models (LLMs) to effectively follow instructions and produce useful assistance. Classically, this involves generating completions from the LLM in response to a query before using a separate reward model to assign a score to the full completion. As an auto-regressive process, the LLM has to take many "actions" (selecting individual tokens) and only receives a single, sparse reward at the end of an episode, a setup that is known to be difficult to optimise in traditional reinforcement learning. In this work we leverage the fact that the reward model contains more information than just its scalar output, in particular, it calculates an attention map over tokens as part of the transformer architecture. We use these attention weights to redistribute the reward along the whole completion, effectively densifying the signal and highlighting the most important tokens, all witho
    
[^25]: 混合量子视觉转换器用于高能物理事件分类

    Hybrid Quantum Vision Transformers for Event Classification in High Energy Physics

    [https://arxiv.org/abs/2402.00776](https://arxiv.org/abs/2402.00776)

    该论文提出了一种基于量子的混合视觉转换器模型，用于高能物理中的事件分类任务。通过减少训练和操作时间，该模型可以达到与经典模型相当的性能。

    

    基于视觉转换器架构的模型被认为是图像分类任务中的最先进技术。然而，它们在训练和部署中都需要大量的计算资源。随着数据的数量和复杂性增加，这个问题变得更加严重。基于量子的视觉转换器模型可能通过减少训练和操作时间来缓解这个问题，同时保持相同的预测能力。尽管当前的量子计算机尚不能执行高维任务，但它们提供了未来最高效的解决方案之一。在这项工作中，我们构建了几种不同的量子混合视觉转换器，用于高能物理中的分类问题（区分电子和光子在电磁量能器中）。我们将它们与经典的视觉转换器架构进行了测试。我们的研究结果表明，混合模型可以达到与经典模型相当的性能。

    Models based on vision transformer architectures are considered state-of-the-art when it comes to image classification tasks. However, they require extensive computational resources both for training and deployment. The problem is exacerbated as the amount and complexity of the data increases. Quantum-based vision transformer models could potentially alleviate this issue by reducing the training and operating time while maintaining the same predictive power. Although current quantum computers are not yet able to perform high-dimensional tasks yet, they do offer one of the most efficient solutions for the future. In this work, we construct several variations of a quantum hybrid vision transformer for a classification problem in high energy physics (distinguishing photons and electrons in the electromagnetic calorimeter). We test them against classical vision transformer architectures. Our findings indicate that the hybrid models can achieve comparable performance to their classical anal
    
[^26]: 流体-结构相互作用中的深度运算网络中的网格运动

    Mesh motion in fluid-structure interaction with deep operator networks

    [https://arxiv.org/abs/2402.00774](https://arxiv.org/abs/2402.00774)

    提出了一种基于深度运算网络的网格运动模型，在流体-结构相互作用问题中表现出与传统的双调和网格运动相当的性能。

    

    提出了一种基于深度运算网络的网格运动模型。该模型在流体-结构相互作用基准问题上进行训练和评估，并在双调和网格运动模型失败的情况下进行进一步评估。所提出的网格运动模型在测试问题上的性能与双调和网格运动相当。

    A mesh motion model based on deep operator networks is presented. The model is trained on and evaluated against a biharmonic mesh motion model on a fluid-structure interaction benchmark problem and further evaluated in a setting where biharmonic mesh motion fails. The performance of the proposed mesh motion model is comparable to the biharmonic mesh motion on the test problems.
    
[^27]: AnimateLCM: 使用分离的一致性学习加速个性化的扩散模型和适配器的动画生成

    AnimateLCM: Accelerating the Animation of Personalized Diffusion Models and Adapters with Decoupled Consistency Learning

    [https://arxiv.org/abs/2402.00769](https://arxiv.org/abs/2402.00769)

    AnimateLCM提出了一种分离的一致性学习策略，通过将图像生成优先级和动作生成优先级的蒸馏分离开来，提高了训练效率并增强了生成的视觉质量。

    

    视频扩散模型因其能够产生连贯且高保真度的视频而受到越来越多的关注。然而，迭代的去噪过程使其计算密集且耗时，从而限制了其应用。受一致性模型（CM）的启发，该模型通过最小的步骤蒸馏预训练的图像扩散模型以加速采样，以及其在条件图像生成上的成功扩展——潜在一致性模型（LCM），我们提出了AnimateLCM，允许在最小的步骤内生成高保真度的视频。我们提出了一种分离的一致性学习策略，将图像生成优先级和动作生成优先级的蒸馏分离开来，这提高了训练效率并增强了生成的视觉质量。此外，为了实现稳定的扩散社区中的即插即用适配器的组合以实现各种修改，我们还引入了适配器的概念。

    Video diffusion models has been gaining increasing attention for its ability to produce videos that are both coherent and of high fidelity. However, the iterative denoising process makes it computationally intensive and time-consuming, thus limiting its applications. Inspired by the Consistency Model (CM) that distills pretrained image diffusion models to accelerate the sampling with minimal steps and its successful extension Latent Consistency Model (LCM) on conditional image generation, we propose AnimateLCM, allowing for high-fidelity video generation within minimal steps. Instead of directly conducting consistency learning on the raw video dataset, we propose a decoupled consistency learning strategy that decouples the distillation of image generation priors and motion generation priors, which improves the training efficiency and enhance the generation visual quality. Additionally, to enable the combination of plug-and-play adapters in stable diffusion community to achieve various 
    
[^28]: 动态系统中在线调整深度神经网络的控制理论技术

    Control-Theoretic Techniques for Online Adaptation of Deep Neural Networks in Dynamical Systems

    [https://arxiv.org/abs/2402.00761](https://arxiv.org/abs/2402.00761)

    本论文提出在动态系统中使用控制理论技术在线调整深度神经网络参数，以解决稳定性和转移学习问题。

    

    深度神经网络（DNNs）是现代人工智能、机器学习和数据科学中主要的工具，通过梯度优化和反向传播进行训练。在许多应用中，DNNs通过监督学习或强化学习进行离线训练，并在线部署进行推断。然而，使用标准的反向传播和梯度优化训练DNNs无法提供DNN的固有性能保证或界限，这对于包括控制在内的应用非常重要。此外，许多离线训练和在线推断问题，如强化学习策略的模拟到实际转移，经历了从训练分布到现实世界分布的域偏移。为了解决这些稳定性和转移学习问题，我们提出使用控制理论的技术在线更新DNN参数。我们将全连接前向DNNs形式化为连续时间的动态系统，并提出了新的方法.

    Deep neural networks (DNNs), trained with gradient-based optimization and backpropagation, are currently the primary tool in modern artificial intelligence, machine learning, and data science. In many applications, DNNs are trained offline, through supervised learning or reinforcement learning, and deployed online for inference. However, training DNNs with standard backpropagation and gradient-based optimization gives no intrinsic performance guarantees or bounds on the DNN, which is essential for applications such as controls. Additionally, many offline-training and online-inference problems, such as sim2real transfer of reinforcement learning policies, experience domain shift from the training distribution to the real-world distribution. To address these stability and transfer learning issues, we propose using techniques from control theory to update DNN parameters online. We formulate the fully-connected feedforward DNN as a continuous-time dynamical system, and we propose novel las
    
[^29]: EuroPED-NN: 不确定性感知的代理模型

    EuroPED-NN: Uncertainty aware surrogate model

    [https://arxiv.org/abs/2402.00760](https://arxiv.org/abs/2402.00760)

    本研究成功生成了不确定性感知的EuroPED代理模型，并通过物理验证证实了模型的稳健性和可靠性。

    

    本研究通过使用基于噪声对比先验（BNN-NCP）技术的贝叶斯神经网络，成功生成了对EuroPED等离子体底座模型的不确定性感知的代理模型，并使用来自JET-ILW底座数据库和后续模型评估的数据进行验证。这些代理模型称为EuroPED-NN。BNN-NCP技术被证明是适用于不确定性感知的代理模型的好选择，与普通神经网络相同的输出结果，提供预测的置信度作为不确定性，并利用代理模型的不确定性突出显示出分布范围外（OOD）区域。这为模型的稳健性和可靠性提供了关键见解。EuroPED-NN已经得到了物理验证，首先通过分析电子密度$n_e\!\left(\psi_{\text{pol}}=0.94\right)$随等离子体电流$I_p$的增加而变化，并验证了与EuroPED模型相关的$\Delta-\beta_{p,ped}$关系。这证实了代理模型所学到的底层物理学的稳健性。

    This work successfully generates uncertainty aware surrogate models, via the Bayesian neural network with noise contrastive prior (BNN-NCP) technique, of the EuroPED plasma pedestal model using data from the JET-ILW pedestal database and subsequent model evaluations. All this conform EuroPED-NN. The BNN-NCP technique is proven to be a good fit for uncertainty aware surrogate models, matching the output results as a regular neural network, providing prediction's confidence as uncertainties, and highlighting the out of distribution (OOD) regions using surrogate model uncertainties. This provides critical insights into model robustness and reliability. EuroPED-NN has been physically validated, first, analyzing electron density $n_e\!\left(\psi_{\text{pol}}=0.94\right)$ with respect to increasing plasma current, $I_p$, and second, validating the $\Delta-\beta_{p,ped}$ relation associated with the EuroPED model. Affirming the robustness of the underlying physics learned by the surrogate mod
    
[^30]: 构建富有表现力和可处理的概率生成模型：一项综述

    Building Expressive and Tractable Probabilistic Generative Models: A Review

    [https://arxiv.org/abs/2402.00759](https://arxiv.org/abs/2402.00759)

    本文综述了富有表现力和可处理的概率生成建模领域的进展和技术，并重点关注了概率电路。文章提供了关于表达能力和可处理性之间权衡的统一视角，并说明了设计原则和算法扩展，成功地构建了富有表现力和高效的概率电路。此外，文章还讨论了最新的深度和混合概率电路研究，并概述了未来研究的挑战和开放性问题。

    

    我们对可处理的概率生成建模领域中的进展和技术进行了全面的调查，重点关注概率电路（PCs）。我们提供了关于表达能力和可处理性之间固有权衡的统一视角，突出了使PCs富有表现力和高效的设计原则和算法扩展，并提供了该领域的分类法。我们还讨论了最近通过融合深度神经模型概念来构建深度和混合PCs的努力，并概述了指导未来研究的挑战和开放性问题。

    We present a comprehensive survey of the advancements and techniques in the field of tractable probabilistic generative modeling, primarily focusing on Probabilistic Circuits (PCs). We provide a unified perspective on the inherent trade-offs between expressivity and the tractability, highlighting the design principles and algorithmic extensions that have enabled building expressive and efficient PCs, and provide a taxonomy of the field. We also discuss recent efforts to build deep and hybrid PCs by fusing notions from deep neural models, and outline the challenges and open questions that can guide future research in this evolving field.
    
[^31]: 无法学习的算法用于上下文学习

    Unlearnable Algorithms for In-context Learning

    [https://arxiv.org/abs/2402.00751](https://arxiv.org/abs/2402.00751)

    本文提出了一种针对预先训练的大型语言模型的高效去学习方法，通过选择少量训练示例来实现任务适应训练数据的精确去学习，并与微调方法进行了比较和讨论。

    

    随着模型被越来越多地部署在未知来源的数据上，机器去学习变得越来越受欢迎。然而，要实现精确的去学习——在没有使用要遗忘的数据的情况下获得与模型分布匹配的模型——是具有挑战性或低效的，通常需要大量的重新训练。在本文中，我们专注于预先训练的大型语言模型（LLM）的任务适应阶段的高效去学习方法。我们观察到LLM进行任务适应的上下文学习能力可以实现任务适应训练数据的高效精确去学习。我们提供了一种算法，用于选择少量训练示例加到LLM的提示前面（用于任务适应），名为ERASE，它的去学习操作成本与模型和数据集的大小无关，意味着它适用于大型模型和数据集。我们还将我们的方法与微调方法进行了比较，并讨论了两种方法之间的权衡。这使我们得到了以下结论：

    Machine unlearning is a desirable operation as models get increasingly deployed on data with unknown provenance. However, achieving exact unlearning -- obtaining a model that matches the model distribution when the data to be forgotten was never used -- is challenging or inefficient, often requiring significant retraining. In this paper, we focus on efficient unlearning methods for the task adaptation phase of a pretrained large language model (LLM). We observe that an LLM's ability to do in-context learning for task adaptation allows for efficient exact unlearning of task adaptation training data. We provide an algorithm for selecting few-shot training examples to prepend to the prompt given to an LLM (for task adaptation), ERASE, whose unlearning operation cost is independent of model and dataset size, meaning it scales to large models and datasets. We additionally compare our approach to fine-tuning approaches and discuss the trade-offs between the two approaches. This leads us to p
    
[^32]: Transformer的好处：在非结构化数据的线性回归任务中的上下文学习

    Benefits of Transformer: In-Context Learning in Linear Regression Tasks with Unstructured Data

    [https://arxiv.org/abs/2402.00743](https://arxiv.org/abs/2402.00743)

    本研究通过线性回归任务的实验研究了Transformer在非结构化数据中的上下文学习能力，并解释了其中的关键组件。

    

    实践中观察到，基于Transformer的模型在推理阶段能够学习上下文中的概念。现有的文献，例如\citet{zhang2023trained,huang2023context}对这种上下文学习能力提供了理论解释，但是他们假设每个样本的输入$x_i$和输出$y_i$都被嵌入到相同的令牌中（即结构化数据）。然而，在现实中，它们呈现为两个令牌（即非结构化数据\cite{wibisono2023role}）。在这种情况下，本文进行了线性回归任务的实验，研究了Transformer架构的好处，并提供了一些相应的理论直觉，解释了为什么Transformer可以从非结构化数据中学习。我们研究了在Transformer中起到上下文学习作用的确切组件。特别地，我们观察到（1）带有两层softmax（自我）注意力和前瞻性注意力掩码的Transformer可以从提示中学习，如果$y_i$在令牌中。

    In practice, it is observed that transformer-based models can learn concepts in context in the inference stage. While existing literature, e.g., \citet{zhang2023trained,huang2023context}, provide theoretical explanations on this in-context learning ability, they assume the input $x_i$ and the output $y_i$ for each sample are embedded in the same token (i.e., structured data). However, in reality, they are presented in two tokens (i.e., unstructured data \cite{wibisono2023role}). In this case, this paper conducts experiments in linear regression tasks to study the benefits of the architecture of transformers and provides some corresponding theoretical intuitions to explain why the transformer can learn from unstructured data. We study the exact components in a transformer that facilitate the in-context learning. In particular, we observe that (1) a transformer with two layers of softmax (self-)attentions with look-ahead attention mask can learn from the prompt if $y_i$ is in the token n
    
[^33]: MobilityDL:从轨迹数据中深度学习的综述

    MobilityDL: A Review of Deep Learning From Trajectory Data

    [https://arxiv.org/abs/2402.00732](https://arxiv.org/abs/2402.00732)

    本综述论文全面概述了深度学习从轨迹数据中的应用，并对最近的工作进行了数据中心的分析。从详细的个体轨迹到稀疏轨迹和聚合轨迹，我们对八个移动性用例进行了研究，并总结了相关深度学习模型和训练数据的应用情况。

    

    轨迹数据结合了时间序列、空间数据和（有时是非理性的）运动行为的复杂性。随着数据可用性和计算能力的增加，深度学习从轨迹数据中变得越来越流行。本综述论文为轨迹数据的深度学习方法提供了首个全面的概述。我们识别了八个具体的移动性用例，根据使用的深度学习模型和训练数据进行了分析。除了对2018年以来文献的全面定量审查外，我们工作的主要贡献是对该领域最新工作的以数据为中心的分析，将其置于移动性数据连续性的范畴中，从详细的个体移动者密集轨迹（准连续追踪数据）到稀疏轨迹（如签到数据）和聚合轨迹（人群信息）。

    Trajectory data combines the complexities of time series, spatial data, and (sometimes irrational) movement behavior. As data availability and computing power have increased, so has the popularity of deep learning from trajectory data. This review paper provides the first comprehensive overview of deep learning approaches for trajectory data. We have identified eight specific mobility use cases which we analyze with regards to the deep learning models and the training data used. Besides a comprehensive quantitative review of the literature since 2018, the main contribution of our work is the data-centric analysis of recent work in this field, placing it along the mobility data continuum which ranges from detailed dense trajectories of individual movers (quasi-continuous tracking data), to sparse trajectories (such as check-in data), and aggregated trajectories (crowd information).
    
[^34]: 用于高效预测多重性评估的基于Dropout的拉肖蒙集探索

    Dropout-Based Rashomon Set Exploration for Efficient Predictive Multiplicity Estimation

    [https://arxiv.org/abs/2402.00728](https://arxiv.org/abs/2402.00728)

    本文提出了一种利用Dropout技术探索拉肖蒙集中模型的新框架，以度量和减轻预测多重性。通过严格的理论推导和广泛的实验评价，结果表明我们的技术始终优于基线模型。

    

    预测多重性是指分类任务可能存在多个竞争模型，它们实现了几乎最优性能，但为单个样本生成了相互冲突的输出。这带来了显著的担忧，因为它可能导致系统性排除、难以解释的歧视和不公平的实际应用。然而，由于需要在可能庞大的假设空间中探索所有这些几乎最优的模型，即拉肖蒙集，度量和减轻预测多重性在计算上具有挑战性。为了应对这一挑战，我们提出了一个利用Dropout技术探索拉肖蒙集中模型的新框架。我们提供了严格的理论推导，将Dropout参数与拉肖蒙集的属性相连接，并通过广泛的实验对我们的框架进行了实证评价。数值结果显示，我们的技术在性能上始终优于基线模型。

    Predictive multiplicity refers to the phenomenon in which classification tasks may admit multiple competing models that achieve almost-equally-optimal performance, yet generate conflicting outputs for individual samples. This presents significant concerns, as it can potentially result in systemic exclusion, inexplicable discrimination, and unfairness in practical applications. Measuring and mitigating predictive multiplicity, however, is computationally challenging due to the need to explore all such almost-equally-optimal models, known as the Rashomon set, in potentially huge hypothesis spaces. To address this challenge, we propose a novel framework that utilizes dropout techniques for exploring models in the Rashomon set. We provide rigorous theoretical derivations to connect the dropout parameters to properties of the Rashomon set, and empirically evaluate our framework through extensive experimentation. Numerical results show that our technique consistently outperforms baselines in
    
[^35]: 脊髓神经根分支的自动分割

    Automatic Segmentation of the Spinal Cord Nerve Rootlets

    [https://arxiv.org/abs/2402.00724](https://arxiv.org/abs/2402.00724)

    本研究开发了一种自动化的方法，可以从MRI扫描中准确地分割脊髓神经根分支，并且具有良好的性能和稳定性。

    

    准确识别脊髓神经根分支对于描绘脊髓的功能活动具有重要意义。本研究旨在开发一种自动方法，用于从T2加权磁共振成像（MRI）扫描中语义分割脊髓神经根分支。通过使用两个开放存取的MRI数据集中的图像，采用主动学习方法训练了一个三维多类卷积神经网络，以分割C2-C8背面神经根分支。每个输出类别对应一个脊髓水平。该方法在训练阶段未见的数据集中进行了测试，以评估不同场地、不同会话和不同分辨率之间的变异性。测试结果的Dice分数为0.67+-0.16（均值+-标准差，针对不同神经根分支水平），表明方法具有良好的性能。该方法还展示了低厂商间和场地间的变异性（变异系数<=1.41%），以及低会话间变异性（变异系数

    Precise identification of spinal nerve rootlets is relevant to delineate spinal levels for the study of functional activity in the spinal cord. The goal of this study was to develop an automatic method for the semantic segmentation of spinal nerve rootlets from T2-weighted magnetic resonance imaging (MRI) scans. Images from two open-access MRI datasets were used to train a 3D multi-class convolutional neural network using an active learning approach to segment C2-C8 dorsal nerve rootlets. Each output class corresponds to a spinal level. The method was tested on 3T T2-weighted images from datasets unseen during training to assess inter-site, inter-session, and inter-resolution variability. The test Dice score was 0.67 +- 0.16 (mean +- standard deviation across rootlets levels), suggesting a good performance. The method also demonstrated low inter-vendor and inter-site variability (coefficient of variation <= 1.41 %), as well as low inter-session variability (coefficient of variation <= 
    
[^36]: 基于双延迟DDPG的神经风格转移用于机器人操纵器的共享控制

    Neural Style Transfer with Twin-Delayed DDPG for Shared Control of Robotic Manipulators

    [https://arxiv.org/abs/2402.00722](https://arxiv.org/abs/2402.00722)

    本论文提出了一种基于神经风格转移和TD3网络的共享控制方法，可以将多种风格应用于机器人操纵器的运动中。该方法通过使用自动编码器和双延迟深度确定性策略梯度网络来生成机器人控制策略，实现了机器人运动的风格转移。

    

    神经风格转移（NST）是一类能够使元素（通常为图像）采用另一个元素的外观或风格的算法。每个元素由内容和风格组成：内容可以概念上定义为元素的“是什么”，而风格则是元素的“如何”。在这个背景下，我们提出了一个定制的NST框架，用于将一组风格转移到机器人操纵器的运动中，例如，相同的机器人任务可以采用愤怒、快乐、平静或悲伤的方式执行。一个自动编码器架构提取和定义目标机器人运动的内容和风格。双延迟深度确定性策略梯度（TD3）网络使用自动编码器定义的损失生成机器人控制策略。所提出的神经策略风格转移TD3（NPST3）通过引入训练风格来改变机器人运动。这种方法可以在动态环境中以在线或离线的方式实现自主机器人运动。

    Neural Style Transfer (NST) refers to a class of algorithms able to manipulate an element, most often images, to adopt the appearance or style of another one. Each element is defined as a combination of Content and Style: the Content can be conceptually defined as the what and the Style as the how of said element. In this context, we propose a custom NST framework for transferring a set of styles to the motion of a robotic manipulator, e.g., the same robotic task can be carried out in an angry, happy, calm, or sad way. An autoencoder architecture extracts and defines the Content and the Style of the target robot motions. A Twin Delayed Deep Deterministic Policy Gradient (TD3) network generates the robot control policy using the loss defined by the autoencoder. The proposed Neural Policy Style Transfer TD3 (NPST3) alters the robot motion by introducing the trained style. Such an approach can be implemented either offline, for carrying out autonomous robot motions in dynamic environments
    
[^37]: 使用反事实表示解释文本分类器

    Explaining Text Classifiers with Counterfactual Representations

    [https://arxiv.org/abs/2402.00711](https://arxiv.org/abs/2402.00711)

    本论文提出了一种使用反事实表示解释文本分类器的方法，通过干预文本表示来生成反事实，并通过实验证实了方法的有效性。

    

    一种基于反事实的解释方法可以为分类器提供合理的解释，其中反事实是指除了一个分类特征之外，与真实观察完全相同的假设事件。然而，在文本领域构建这种反事实存在特定挑战，因为某些属性值可能与现实世界的事件不一致。在这篇论文中，我们提出了一种简单的方法，通过对文本表示进行干预来生成反事实，从而克服了这个限制。我们认为我们的干预方法是最小程度的干扰，并且在理论上是可靠的，因为它们与Pearl的因果推断框架中定义的反事实是一致的。为了验证我们的方法，我们首先在合成数据集上进行实验，比较了基于真实反事实（通过明确的文本干预获得）和我们的反事实（通过对文本表示的干预得到）的分类器预测。

    One well motivated explanation method for classifiers leverages counterfactuals which are hypothetical events identical to real observations in all aspects except for one categorical feature. Constructing such counterfactual poses specific challenges for texts, however, as some attribute values may not necessarily align with plausible real-world events. In this paper we propose a simple method for generating counterfactuals by intervening in the space of text representations which bypasses this limitation. We argue that our interventions are minimally disruptive and that they are theoretically sound as they align with counterfactuals as defined in Pearl's causal inference framework. To validate our method, we first conduct experiments on a synthetic dataset of counterfactuals, allowing for a direct comparison between classifier predictions based on ground truth counterfactuals (obtained through explicit text interventions) and our counterfactuals, derived through interventions in the r
    
[^38]: 非交换的共形语言生成与最近邻

    Non-Exchangeable Conformal Language Generation with Nearest Neighbors

    [https://arxiv.org/abs/2402.00707](https://arxiv.org/abs/2402.00707)

    本文介绍了一种利用最近邻方法扩展的非交换的共形语言生成框架，用于量化自动生成文本的不确定性，并提供带有统计保证的预测集。

    

    在自动生成的文本中量化不确定性对于让人们检查潜在的错觉和使系统更可靠是很重要的。共形预测是一个有吸引力的框架，能够提供带有统计保证的预测，然而，将其应用于文本生成是具有挑战性的，因为任何独立同分布的假设都是不现实的。在本文中，我们通过利用最近关于非交换的共形预测的结果来填补这一差距，该方法仍然确保覆盖范围。结果--非交换的共形核采样，是对基于最近邻的生成的共形预测框架的一种新颖扩展。我们的方法可以用于任意模型的事后处理，无需额外训练，并提供带有统计保证的标记级别、校准的预测集。在机器翻译和语言建模的实验中，我们展示了令人鼓舞的生成质量结果。通过同时产生具有良好覆盖度的更紧密的预测集，

    Quantifying uncertainty in automatically generated text is important for letting humans check potential hallucinations and making systems more reliable. Conformal prediction is an attractive framework to provide predictions imbued with statistical guarantees, however, its application to text generation is challenging since any i.i.d. assumptions are not realistic. In this paper, we bridge this gap by leveraging recent results on non-exchangeable conformal prediction, which still ensures bounds on coverage. The result, non-exchangeable conformal nucleus sampling, is a novel extension of the conformal prediction framework to generation based on nearest neighbors. Our method can be used post-hoc for an arbitrary model without extra training and supplies token-level, calibrated prediction sets equipped with statistical guarantees. Experiments in machine translation and language modeling show encouraging results in generation quality. By also producing tighter prediction sets with good cove
    
[^39]: 结合荷兰调查和登记数据的数据挑战，预测生育率（PreFer）

    Combining the Strengths of Dutch Survey and Register Data in a Data Challenge to Predict Fertility (PreFer)

    [https://arxiv.org/abs/2402.00705](https://arxiv.org/abs/2402.00705)

    该论文介绍了两个数据集，分别基于荷兰的调查数据和登记数据，用于研究荷兰生育结果的预测能力。研究者提供了数据集的信息和样本，并描述了生育结果的具体内容。他们还介绍了生育率预测的方法。

    

    社会科学领域已经积累了大量有关生育结果的研究，即人们是否以及何时生育子女的决定因素。然而，这些决定因素和基本理论的预测能力很少在新数据上进行评估。这使得我们无法系统地比较研究，阻碍了知识的评估和积累。在本文中，我们介绍了两个数据集，用于研究荷兰生育结果的可预测性。一个数据集基于LISS面板，这是一个纵向调查，包括了数千个关于各种主题的变量，包括个体偏好和价值观。另一个数据集基于荷兰登记数据，缺乏态度数据，但包括了数百万荷兰居民生活轨迹的详细信息。我们提供关于数据集和样本的信息，并描述感兴趣的生育结果。我们还介绍了生育率预测的方法。

    The social sciences have produced an impressive body of research on determinants of fertility outcomes, or whether and when people have children. However, the strength of these determinants and underlying theories are rarely evaluated on their predictive ability on new data. This prevents us from systematically comparing studies, hindering the evaluation and accumulation of knowledge. In this paper, we present two datasets which can be used to study the predictability of fertility outcomes in the Netherlands. One dataset is based on the LISS panel, a longitudinal survey which includes thousands of variables on a wide range of topics, including individual preferences and values. The other is based on the Dutch register data which lacks attitudinal data but includes detailed information about the life courses of millions of Dutch residents. We provide information about the datasets and the samples, and describe the fertility outcome of interest. We also introduce the fertility prediction
    
[^40]: PeaTMOSS: 一个开源软件中预训练模型的数据集和初步分析

    PeaTMOSS: A Dataset and Initial Analysis of Pre-Trained Models in Open-Source Software

    [https://arxiv.org/abs/2402.00699](https://arxiv.org/abs/2402.00699)

    本研究介绍了PeaTMOSS数据集，用于记录和分析开源软件中预训练模型的元数据和应用情况。这对于了解预训练模型的采用和重复使用的影响具有重要意义。

    

    深度学习模型的开发和训练变得越来越昂贵和复杂。因此，软件工程师正在采用预训练模型(PTMs)来进行后续应用。PTM供应链的动态仍然很少被探索，这表明需要结构化的数据集，不仅记录元数据，还记录这些模型的后续应用。没有这样的数据，MSR社区无法全面理解PTM的采用和重复使用的影响。本文提出了PeaTMOSS数据集，其中包括281,638个PTM的元数据和超过50个月下载量的所有PTM的详细快照(14,296个PTMs)，以及利用这些模型的28,575个来自GitHub的开源软件代码库。此外，数据集还包括15,129个GitHub代码库到它们使用的2,530个PTMs的44,337个映射。为了提高数据集的全面性，我们为一个大型语言模型开发了提示，以自动地进行摘要生成。

    The development and training of deep learning models have become increasingly costly and complex. Consequently, software engineers are adopting pre-trained models (PTMs) for their downstream applications. The dynamics of the PTM supply chain remain largely unexplored, signaling a clear need for structured datasets that document not only the metadata but also the subsequent applications of these models. Without such data, the MSR community cannot comprehensively understand the impact of PTM adoption and reuse. This paper presents the PeaTMOSS dataset, which comprises metadata for 281,638 PTMs and detailed snapshots for all PTMs with over 50 monthly downloads (14,296 PTMs), along with 28,575 open-source software repositories from GitHub that utilize these models. Additionally, the dataset includes 44,337 mappings from 15,129 downstream GitHub repositories to the 2,530 PTMs they use. To enhance the dataset's comprehensiveness, we developed prompts for a large language model to automatical
    
[^41]: 近似优化模板反演的形态攻击

    Approximating Optimal Morphing Attacks using Template Inversion

    [https://arxiv.org/abs/2402.00695](https://arxiv.org/abs/2402.00695)

    本文提出了一种基于模板反演的形态攻击方法，可以通过反演优化形态嵌入恢复逼真的人脸图像。该方法在白盒和黑盒攻击场景中表现出与以往技术相竞争并超越其的有效性。

    

    最近的研究论文表明，可以通过反演人脸识别系统来恢复逼真的人脸图像。我们利用这样的模板反演模型，开发了一种基于反演理论上的优化形态嵌入的新型深度形态攻击方法，该方法通过对源图像的人脸嵌入求平均得到。我们尝试了两种变种：第一种利用完全自适应的嵌入转图像反演模型，而第二种利用预训练StyleGAN网络的合成网络增加形态的逼真度。我们从多个源数据集生成形态攻击，并研究这些攻击对多个人脸识别系统的有效性。我们展示了我们的方法在白盒和黑盒攻击场景中在有效性方面与以往基于深度学习的形态生成的最新技术相竞争并经常超越它们。

    Recent works have demonstrated the feasibility of inverting face recognition systems, enabling to recover convincing face images using only their embeddings. We leverage such template inversion models to develop a novel type ofdeep morphing attack based on inverting a theoretical optimal morph embedding, which is obtained as an average of the face embeddings of source images. We experiment with two variants of this approach: the first one exploits a fully self-contained embedding-to-image inversion model, while the second leverages the synthesis network of a pretrained StyleGAN network for increased morph realism. We generate morphing attacks from several source datasets and study the effectiveness of those attacks against several face recognition networks. We showcase that our method can compete with and regularly beat the previous state of the art for deep-learning based morph generation in terms of effectiveness, both in white-box and black-box attack scenarios, and is additionally 
    
[^42]: 使用智能城市应用中的连续目标导向动作来实现真实评估的可追踪性

    Real Evaluations Tractability using Continuous Goal-Directed Actions in Smart City Applications

    [https://arxiv.org/abs/2402.00678](https://arxiv.org/abs/2402.00678)

    以往的机器人编程方法对非专家用户不友好，这篇论文提出了使用连续目标导向动作来实现真实评估的方法，通过编码动作的特征变化来适应智能城市应用中的各种特征，并使用进化算法来计算机器人的关节轨迹。

    

    智能城市应用的一个重要挑战是使系统适应与非专家用户的交互。机器人模仿框架旨在通过允许用户通过示范直接编程来简化和减少机器人编程的时间。在传统的框架中，动作被建模为关节或笛卡尔空间轨迹。然而，使用这些纯几何方法并不能很好地表示其他特征，比如视觉特征。连续目标导向动作(CGDA)是这些方法的一种替代方案，它将动作编码为可以从环境中提取的任何特征的变化。为了满足这种无关特征的编码，机器人执行的关节轨迹必须完全计算。为了实现这一点，通常需要使用进化算法(EA)进行计算，但这通常需要过多的评估才能在实际机器人中进行这个进化步骤。目前的策略是在模拟中进行评估，然后将评估结果转移到实际机器人上。

    One of the most important challenges of Smart City Applications is to adapt the system to interact with non-expert users. Robot imitation frameworks aim to simplify and reduce times of robot programming by allowing users to program directly through demonstrations. In classical frameworks, actions are modeled using joint or Cartesian space trajectories. Other features, such as visual ones, are not always well represented with these pure geometrical approaches. Continuous Goal-Directed Actions (CGDA) is an alternative to these methods, as it encodes actions as changes of any feature that can be extracted from the environment. As a consequence of this, the robot joint trajectories for execution must be fully computed to comply with this feature-agnostic encoding. This is achieved using Evolutionary Algorithms (EA), which usually requires too many evaluations to perform this evolution step in the actual robot. Current strategies involve performing evaluations in a simulation, transferring 
    
[^43]: 神经策略风格转换

    Neural Policy Style Transfer

    [https://arxiv.org/abs/2402.00677](https://arxiv.org/abs/2402.00677)

    本研究提出了神经策略风格转换算法，通过深度强化学习来实现控制策略的风格转换。通过训练不同的网络来最大化预期奖励，同时编码了行为的目标和风格，从而将一个策略的风格转移到另一个策略而保持其内容不变。通过逆强化学习和用户演示实现模型的训练和风格的编码。

    

    风格转换已经在许多领域中被提出：美术、自然语言处理和固定轨迹。本文将这个概念扩展到了深度强化学习架构中的控制策略。每个网络都被训练成最大化预期的奖励，通常编码了一种行为的目标，可以描述为内容。深度神经网络的表达能力使得可以编码第二个任务，可以描述为风格。提出了神经策略风格转换（NPST）算法，用于将一个策略的风格转移到另一个策略，同时保持后者的内容。通过深度 Q-Network 架构定义了不同的策略。使用逆强化学习通过演示进行模型训练。进行了两组不同的用户演示，一组为内容，另一组为风格。不同的风格通过用户演示进行编码。生成的策略是通过将内容策略输送到一个生成器中来得到的。

    Style Transfer has been proposed in a number of fields: fine arts, natural language processing, and fixed trajectories. We scale this concept up to control policies within a Deep Reinforcement Learning infrastructure. Each network is trained to maximize the expected reward, which typically encodes the goal of an action, and can be described as the content. The expressive power of deep neural networks enables encoding a secondary task, which can be described as the style. The Neural Policy Style Transfer (NPST) algorithm is proposed to transfer the style of one policy to another, while maintaining the content of the latter. Different policies are defined via Deep Q-Network architectures. These models are trained using demonstrations through Inverse Reinforcement Learning. Two different sets of user demonstrations are performed, one for content and other for style. Different styles are encoded as defined by user demonstrations. The generated policy is the result of feeding a content poli
    
[^44]: 深度机器人素描：深度Q学习网络在人类素描中的应用

    Deep Robot Sketching: An application of Deep Q-Learning Networks for human-like sketching

    [https://arxiv.org/abs/2402.00676](https://arxiv.org/abs/2402.00676)

    本研究提出在艺术机器人应用中引入深度Q学习网络，旨在改进艺术机器人应用的控制策略。

    

    当前强化学习算法在复杂环境中的性能取得了巨大成功，这激发了许多最新的认知科学理论方法。艺术环境被认知科学界视为丰富、自然、多感官、多文化的环境。在这项工作中，我们提出使用强化学习改进艺术机器人应用的控制。深度Q学习神经网络（DQN）是在机器人中实现强化学习最成功的算法之一。DQN方法在各种环境中生成复杂的控制策略来执行复杂的机器人应用。当前的艺术绘画机器人应用使用简单的控制法则，限制了框架的适应性。本研究提出在艺术绘画机器人应用中引入DQN。目标是研究如何引入复杂的控制策略来改进艺术机器人应用的控制。

    The current success of Reinforcement Learning algorithms for its performance in complex environments has inspired many recent theoretical approaches to cognitive science. Artistic environments are studied within the cognitive science community as rich, natural, multi-sensory, multi-cultural environments. In this work, we propose the introduction of Reinforcement Learning for improving the control of artistic robot applications. Deep Q-learning Neural Networks (DQN) is one of the most successful algorithms for the implementation of Reinforcement Learning in robotics. DQN methods generate complex control policies for the execution of complex robot applications in a wide set of environments. Current art painting robot applications use simple control laws that limits the adaptability of the frameworks to a set of simple environments. In this work, the introduction of DQN within an art painting robot application is proposed. The goal is to study how the introduction of a complex control pol
    
[^45]: 使用机器学习分类器建模货运方式选择：基于商品流调查数据的比较研究

    Modeling Freight Mode Choice Using Machine Learning Classifiers: A Comparative Study Using the Commodity Flow Survey (CFS) Data

    [https://arxiv.org/abs/2402.00659](https://arxiv.org/abs/2402.00659)

    本研究使用机器学习分类器对货运方式选择进行建模，并比较了八种常用的分类器。其中，基于树的集成分类器表现最佳，尤其是随机森林模型的预测准确性最高。

    

    本研究探讨了使用机器学习分类器建模货运方式选择的实用性。我们调查了八种常用的机器学习分类器，包括朴素贝叶斯、支持向量机、人工神经网络、K近邻、分类回归树、随机森林、Boosting和Bagging，以及经典的多项式逻辑模型。我们使用美国2012年商品流调查数据作为主要数据来源，并使用二级数据来源的空间属性进行补充。根据预测准确性结果比较了分类器的性能。当前研究还研究了样本大小和训练测试数据分离比率对各种方法的预测能力的影响。此外，估计了变量的重要性，以确定这些变量如何影响货运方式选择。结果显示，基于树的集成分类器表现最好。具体而言，随机森林产生了最准确的预测结果。

    This study explores the usefulness of machine learning classifiers for modeling freight mode choice. We investigate eight commonly used machine learning classifiers, namely Naive Bayes, Support Vector Machine, Artificial Neural Network, K-Nearest Neighbors, Classification and Regression Tree, Random Forest, Boosting and Bagging, along with the classical Multinomial Logit model. US 2012 Commodity Flow Survey data are used as the primary data source; we augment it with spatial attributes from secondary data sources. The performance of the classifiers is compared based on prediction accuracy results. The current research also examines the role of sample size and training-testing data split ratios on the predictive ability of the various approaches. In addition, the importance of variables is estimated to determine how the variables influence freight mode choice. The results show that the tree-based ensemble classifiers perform the best. Specifically, Random Forest produces the most accura
    
[^46]: 提高货运模式选择模型的准确性：基于2017年CFS PUF数据集和集成学习技术的案例研究

    Improving the accuracy of freight mode choice models: A case study using the 2017 CFS PUF data set and ensemble learning techniques

    [https://arxiv.org/abs/2402.00654](https://arxiv.org/abs/2402.00654)

    该论文通过使用2017年的商品流动调查公共使用文件数据集和集成学习技术，改进了货运模式选择模型的准确性，包括构建本地模型、提取地理特征和应用集成学习方法。实验结果表明，该方法在没有内存限制的情况下实现了超过92%的准确性。

    

    美国人口普查局收集了两轮实验数据，提供了全国商品流动的运输特征，分别在2012年（即公共使用微数据）和2017年（即公共使用文件）发布。基于这些信息，数据驱动方法在理解货运物流的详细模式方面变得越来越有价值。在本研究中，我们使用2017年的商品流动调查公共使用文件数据集，探索构建一个高性能的货运模式选择模型，考虑到三个主要改进：（1）为每个独立的商品/行业类别构建本地模型；（2）提取有用的地理特征，尤其是每种货运模式在起点/终点区域之间的衍生距离；（3）利用集成学习方法，如堆叠或投票，将本地和统一模型的结果相结合，以提高性能。所提出的方法在没有内存限制的情况下实现了超过92%的准确性。

    The US Census Bureau has collected two rounds of experimental data from the Commodity Flow Survey, providing shipment-level characteristics of nationwide commodity movements, published in 2012 (i.e., Public Use Microdata) and in 2017 (i.e., Public Use File). With this information, data-driven methods have become increasingly valuable for understanding detailed patterns in freight logistics. In this study, we used the 2017 Commodity Flow Survey Public Use File data set to explore building a high-performance freight mode choice model, considering three main improvements: (1) constructing local models for each separate commodity/industry category; (2) extracting useful geographical features, particularly the derived distance of each freight mode between origin/destination zones; and (3) applying additional ensemble learning methods such as stacking or voting to combine results from local and unified models for improved performance. The proposed method achieved over 92% accuracy without in
    
[^47]: 相干前馈量子神经网络

    Coherent Feed Forward Quantum Neural Network

    [https://arxiv.org/abs/2402.00653](https://arxiv.org/abs/2402.00653)

    这项研究提出了一种相干前馈量子神经网络模型，该模型在电路深度和量子比特需求方面更为高效，能够适应真实世界的机器学习任务。

    

    量子机器学习，特别是量子神经网络(QNNs)，仍然是一个广阔的未知领域。目前的QNN模型主要采用变分电路或量子特征映射，在ansatz或者量子特征图上进行，通常需要多个纠缠层。这种方法不仅增加了电路的计算成本，超出了在近期量子设备上的实际可行性，而且由于与典型的前馈神经网络(FFNN)结构的偏离，误导性地将这些模型标记为神经网络。此外，这些模型的电路深度和量子比特需求在数据特征数量增加时扩展性较差，从而对真实世界的机器学习任务造成了效率挑战。我们引入了一个地道的QNN模型，它在可调整的中间层和节点上与传统的FFNN相协调，在没有中间测量的情况下，整个模型是相干的。该模型以其减少的电路深度和量子比特需求脱颖而出，使其在真实世界的机器学习任务中效率更高。

    Quantum machine learning, focusing on quantum neural networks (QNNs), remains a vastly uncharted field of study. Current QNN models primarily employ variational circuits on an ansatz or a quantum feature map, often requiring multiple entanglement layers. This methodology not only increases the computational cost of the circuit beyond what is practical on near-term quantum devices but also misleadingly labels these models as neural networks, given their divergence from the structure of a typical feed-forward neural network (FFNN). Moreover, the circuit depth and qubit needs of these models scale poorly with the number of data features, resulting in an efficiency challenge for real-world machine-learning tasks. We introduce a bona fide QNN model, which seamlessly aligns with the versatility of a traditional FFNN in terms of its adaptable intermediate layers and nodes, absent from intermediate measurements such that our entire model is coherent. This model stands out with its reduced circ
    
[^48]: 光谱变换核回归

    Spectrally Transformed Kernel Regression

    [https://arxiv.org/abs/2402.00645](https://arxiv.org/abs/2402.00645)

    光谱变换核回归是一种能够利用无标签数据的通用和可扩展的方法，具有学习充分平滑函数的能力，并且在感知范式中提供了可扩展的实现。

    

    无标签数据是现代机器学习的关键组成部分。一般来说，无标签数据的作用是通过基础核（如ε-邻居核或图的邻接矩阵）中编码的相似性信息来实现一种平滑性形式。本研究重新审视了光谱变换核回归（STKR）的经典思想，并提供了一类新的通用和可扩展的STKR估计器，能够利用无标签数据。通过光谱变换，STKR利用了无标签数据提供的数据分布的信息。首先，我们证明了STKR是一种原则性和通用性的方法，通过表征一种"目标平滑性"的通用类型，并证明任何充分平滑的函数都可以通过STKR学习。其次，我们提供了可扩展的STKR实现，适用于感知范式，并提供了一般的变换函数，而先前的工作大部分限于推导范式。第三，我们推导出统计属性...

    Unlabeled data is a key component of modern machine learning. In general, the role of unlabeled data is to impose a form of smoothness, usually from the similarity information encoded in a base kernel, such as the $\epsilon$-neighbor kernel or the adjacency matrix of a graph. This work revisits the classical idea of spectrally transformed kernel regression (STKR), and provides a new class of general and scalable STKR estimators able to leverage unlabeled data. Intuitively, via spectral transformation, STKR exploits the data distribution for which unlabeled data can provide additional information. First, we show that STKR is a principled and general approach, by characterizing a universal type of "target smoothness", and proving that any sufficiently smooth function can be learned by STKR. Second, we provide scalable STKR implementations for the inductive setting and a general transformation function, while prior work is mostly limited to the transductive setting. Third, we derive stati
    
[^49]: 基于随机森林的中风结果预测

    Random Forest-Based Prediction of Stroke Outcome

    [https://arxiv.org/abs/2402.00638](https://arxiv.org/abs/2402.00638)

    该论文研究了与中风患者结果相关的因素，利用机器学习技术生成了一个预测模型，能够有效预测入院后3个月的死亡率和发病率。

    

    我们研究了与中风患者结果相关的临床、生化和神经成像因素，利用机器学习技术生成了一个预测模型，用于预测入院后3个月的死亡率和发病率。数据集包括欧洲三级医院中风单元入院的缺血性中风（IS）和非创伤性颅内出血（ICH）患者。我们识别了机器学习随机森林（RF）的主要变量，生成了一个可以估计患者死亡率/发病率的预测模型。总之，机器学习算法随机森林可以有效地用于中风患者的长期结果预测死亡率和发病率。

    We research into the clinical, biochemical and neuroimaging factors associated with the outcome of stroke patients to generate a predictive model using machine learning techniques for prediction of mortality and morbidity 3 months after admission. The dataset consisted of patients with ischemic stroke (IS) and non-traumatic intracerebral hemorrhage (ICH) admitted to Stroke Unit of a European Tertiary Hospital prospectively registered. We identified the main variables for machine learning Random Forest (RF), generating a predictive model that can estimate patient mortality/morbidity. In conclusion, machine learning algorithms RF can be effectively used in stroke patients for long-term outcome prediction of mortality and morbidity.
    
[^50]: Vision-LLMs通过自动生成的排版攻击可以自欺欺人

    Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks

    [https://arxiv.org/abs/2402.00626](https://arxiv.org/abs/2402.00626)

    这项研究深入研究了大规模视觉语言模型（LVLM）对于自动生成的排版攻击的易受攻击性，并引入了一种新的、更有效的自动生成的排版攻击方法，为此设计了一个独特的测试基准。通过使用该基准，研究发现排版攻击对LVLM构成了重大威胁。

    

    最近，在大规模视觉语言模型（LVLM）方面取得了重大进展；这是一种利用大型预训练语言模型的全新类别的视觉语言模型。然而，LVLM对于涉及将误导性文本叠加到图像上的从排版攻击的容易受攻击性却没有研究。此外，先前的排版攻击依赖于从预定义类别集合中随机选择一个误导性类别。然而，随机选择的类别可能不是最有效的攻击类别。为了解决这些问题，我们首先引入了一种独特设计的新颖基准来测试LVLM对排版攻击的容易受攻击性。此外，我们介绍了一种新而更有效的排版攻击：自动生成的排版攻击。实际上，我们的方法通过简单地提示GPT-4V等模型利用其强大的语言能力推荐一种排版攻击来为给定的图像生成攻击。使用我们的新颖基准，我们发现排版攻击对LVLM构成了重大威胁。

    Recently, significant progress has been made on Large Vision-Language Models (LVLMs); a new class of VL models that make use of large pre-trained language models. Yet, their vulnerability to Typographic attacks, which involve superimposing misleading text onto an image remain unstudied. Furthermore, prior work typographic attacks rely on sampling a random misleading class from a predefined set of classes. However, the random chosen class might not be the most effective attack. To address these issues, we first introduce a novel benchmark uniquely designed to test LVLMs vulnerability to typographic attacks. Furthermore, we introduce a new and more effective typographic attack: Self-Generated typographic attacks. Indeed, our method, given an image, make use of the strong language capabilities of models like GPT-4V by simply prompting them to recommend a typographic attack. Using our novel benchmark, we uncover that typographic attacks represent a significant threat against LVLM(s). Furth
    
[^51]: 基于高斯过程网络的贝叶斯因果推断

    Bayesian Causal Inference with Gaussian Process Networks

    [https://arxiv.org/abs/2402.00623](https://arxiv.org/abs/2402.00623)

    以高斯过程网络为基础，通过模拟干预效果和传播干预效果，进行灵活的贝叶斯因果推断，同时以局部变量为函数估计干预分布并使用加性高斯过程对条件分布进行建模。

    

    从观测数据中进行因果推断和推断是统计学中的一个重要问题，它既涉及建模问题，也涉及计算问题。通常情况下，人们会对联合分布做出严格的假设，如线性性。我们考虑在高斯过程网络（GPN）模型中，对假设干预效果的贝叶斯估计问题，这是一个灵活的因果框架，可以非参数地描述因果关系。我们详细介绍了如何通过在整个网络上模拟干预效果，并在下游变量上传播干预效果来进行GPN的因果推断。我们进一步推导了一个简化的计算近似方法，通过仅将干预分布估计为局部变量的函数，并通过加性高斯过程对条件分布进行建模。我们将这两个框架扩展到了不仅仅是已知因果图的情况下，并引入了对因果结构的不确定性。

    Causal discovery and inference from observational data is an essential problem in statistics posing both modeling and computational challenges. These are typically addressed by imposing strict assumptions on the joint distribution such as linearity. We consider the problem of the Bayesian estimation of the effects of hypothetical interventions in the Gaussian Process Network (GPN) model, a flexible causal framework which allows describing the causal relationships nonparametrically. We detail how to perform causal inference on GPNs by simulating the effect of an intervention across the whole network and propagating the effect of the intervention on downstream variables. We further derive a simpler computational approximation by estimating the intervention distribution as a function of local variables only, modeling the conditional distributions via additive Gaussian processes. We extend both frameworks beyond the case of a known causal graph, incorporating uncertainty about the causal s
    
[^52]: 使用软轮廓分数的深度聚类：朝向紧凑且互相分离的簇

    Deep Clustering Using the Soft Silhouette Score: Towards Compact and Well-Separated Clusters

    [https://arxiv.org/abs/2402.00608](https://arxiv.org/abs/2402.00608)

    本研究提出了软轮廓系数，通过在深度聚类中优化该系数，可以实现形成紧凑且互相分离的簇。同时引入了适用于该方法的自编码器深度学习架构。

    

    在大数据时代，无监督学习因其能从未标记的数据集中提取有价值的见解而日益重要。深度聚类已成为重要的无监督学习方法之一，旨在利用神经网络的非线性映射能力来提升聚类性能。大部分深度聚类的文献都致力于在某个嵌入空间中最小化内部聚类变异性，同时保持学习到的表示与原始高维数据一致。本文提出了软轮廓，即轮廓系数的概率形式。软轮廓与传统的轮廓系数一样，奖励紧凑且明显分离的聚类解决方案。在深度聚类框架中优化软轮廓可以引导学习到的表示形成紧凑且互相分离的簇。此外，我们还介绍了一个基于自编码器的深度学习架构，适用于这种深度聚类方法。

    Unsupervised learning has gained prominence in the big data era, offering a means to extract valuable insights from unlabeled datasets. Deep clustering has emerged as an important unsupervised category, aiming to exploit the non-linear mapping capabilities of neural networks in order to enhance clustering performance. The majority of deep clustering literature focuses on minimizing the inner-cluster variability in some embedded space while keeping the learned representation consistent with the original high-dimensional dataset. In this work, we propose soft silhoutte, a probabilistic formulation of the silhouette coefficient. Soft silhouette rewards compact and distinctly separated clustering solutions like the conventional silhouette coefficient. When optimized within a deep clustering framework, soft silhouette guides the learned representations towards forming compact and well-separated clusters. In addition, we introduce an autoencoder-based deep learning architecture that is suita
    
[^53]: 人工合成的时间序列数据真的不如真实数据吗？

    Are Synthetic Time-series Data Really not as Good as Real Data?

    [https://arxiv.org/abs/2402.00607](https://arxiv.org/abs/2402.00607)

    本研究引入了InfoBoost，一种高度通用的跨领域数据合成框架，具备时间序列表示学习能力，并开发了一种基于合成数据的方法，可以实现超越真实数据训练的模型性能。此外，我们还训练了一个通用特征提取器，适用于所有时间序列数据。实验证明，我们的方法能够克服多个干扰源的影响，提高泛化能力。

    

    时间序列数据存在数据质量问题、偏见和脆弱性以及泛化问题。整合通用的数据合成方法有望提高泛化能力。然而，当前方法无法保证生成器的输出包含所有未见过的真实数据。在本文中，我们引入了InfoBoost -- 一种高度通用的跨领域数据合成框架，具备时间序列表示学习能力。我们开发了一种基于合成数据的方法，可以实现模型训练而无需真实数据，超越了使用真实数据训练的模型的性能。此外，我们基于合成数据训练了一个适用于所有时间序列数据的通用特征提取器。我们的方法克服了多个源的节奏信号、噪声干扰和超过采样窗口能力的长周期特征的干扰。通过实验证明，我们的非深度学习合成数据实现了

    Time-series data presents limitations stemming from data quality issues, bias and vulnerabilities, and generalization problem. Integrating universal data synthesis methods holds promise in improving generalization. However, current methods cannot guarantee that the generator's output covers all unseen real data. In this paper, we introduce InfoBoost -- a highly versatile cross-domain data synthesizing framework with time series representation learning capability. We have developed a method based on synthetic data that enables model training without the need for real data, surpassing the performance of models trained with real data. Additionally, we have trained a universal feature extractor based on our synthetic data that is applicable to all time-series data. Our approach overcomes interference from multiple sources rhythmic signal, noise interference, and long-period features that exceed sampling window capabilities. Through experiments, our non-deep-learning synthetic data enables 
    
[^54]: 不确定性感知的部分标签学习

    Uncertainty-Aware Partial-Label Learning

    [https://arxiv.org/abs/2402.00592](https://arxiv.org/abs/2402.00592)

    本文提出了一种基于最近邻的部分标签学习算法，利用Dempster-Shafer理论实现对模糊标记的数据的训练。实验结果表明，该算法能够提供良好的不确定性估计，并具有竞争力的预测性能。

    

    在现实世界的应用中，人们经常遇到标记模糊的数据，即不同的标注者为相同样本分配了冲突的类别标签。部分标签学习允许在这种弱监督的情况下训练分类器。虽然最先进的方法已经具有良好的预测性能，但它们往往受到错误的不确定性估计的影响。然而，在医学和自动驾驶等安全关键领域，具有良好校准的不确定性估计尤为重要。在本文中，我们提出了一种基于最近邻的部分标签学习算法，该算法利用了Dempster-Shafer理论。对人工数据集和实际数据集进行的广泛实验表明，所提出的方法能够提供良好的不确定性估计，并具有竞争力的预测性能。此外，我们还证明了我们的算法具有风险一致性。

    In real-world applications, one often encounters ambiguously labeled data, where different annotators assign conflicting class labels. Partial-label learning allows training classifiers in this weakly supervised setting. While state-of-the-art methods already feature good predictive performance, they often suffer from miscalibrated uncertainty estimates. However, having well-calibrated uncertainty estimates is important, especially in safety-critical domains like medicine and autonomous driving. In this article, we propose a novel nearest-neighbor-based partial-label-learning algorithm that leverages Dempster-Shafer theory. Extensive experiments on artificial and real-world datasets show that the proposed method provides a well-calibrated uncertainty estimate and achieves competitive prediction performance. Additionally, we prove that our algorithm is risk-consistent.
    
[^55]: 使用稳定表示和经验回放的连续无监督领域自适应算法

    Continuous Unsupervised Domain Adaptation Using Stabilized Representations and Experience Replay

    [https://arxiv.org/abs/2402.00580](https://arxiv.org/abs/2402.00580)

    本论文提出了一种解决连续学习中无监督领域自适应问题的算法，通过稳定表示和经验回放来增强模型在新领域上的泛化能力。

    

    我们提出了一种算法来解决在连续学习场景下无监督领域自适应（UDA）问题。主要目标是在只能访问无标签数据的情况下，通过更新基础模型来保持模型在域转移下的泛化能力。虽然已经存在许多UDA算法，但它们通常需要同时访问源域和目标域数据集。相反，现有的连续学习方法可以处理所有具有标签数据的任务。我们的解决方案是通过稳定学习的内部分布来增强模型在新领域上的泛化能力。内部分布是通过隐藏层的网络响应来建模的。我们使用高斯混合模型（GMM）来建模这个内部分布，并通过将新领域的内部学习分布与估计的GMM进行匹配来更新模型。此外，我们利用经验回放来克服用户体验中的问题。

    We introduce an algorithm for tackling the problem of unsupervised domain adaptation (UDA) in continual learning (CL) scenarios. The primary objective is to maintain model generalization under domain shift when new domains arrive continually through updating a base model when only unlabeled data is accessible in subsequent tasks. While there are many existing UDA algorithms, they typically require access to both the source and target domain datasets simultaneously. Conversely, existing CL approaches can handle tasks that all have labeled data. Our solution is based on stabilizing the learned internal distribution to enhances the model generalization on new domains. The internal distribution is modeled by network responses in hidden layer. We model this internal distribution using a Gaussian mixture model (GMM ) and update the model by matching the internally learned distribution of new domains to the estimated GMM. Additionally, we leverage experience replay to overcome the problem of 
    
[^56]: 热带决策边界对神经网络的抗对抗攻击是稳健的

    Tropical Decision Boundaries for Neural Networks Are Robust Against Adversarial Attacks

    [https://arxiv.org/abs/2402.00576](https://arxiv.org/abs/2402.00576)

    这项研究引入了一种新的神经网络架构，该架构利用热带性质能够抵抗对抗攻击，并通过实验证明了其在图像数据集上的稳健性。

    

    我们引入了一种简单、易于实现且计算效率高的热带卷积神经网络架构，该架构对抗对抗攻击具有稳健性。通过将数据嵌入到热带投影托洛斯中的一个隐藏层中，我们利用分段线性神经网络的热带性质，该层可以添加到任何模型中。我们在理论上研究了其决策边界的几何性质，并通过计算实验证明了其在图像数据集上对抗对抗攻击的稳健性。

    We introduce a simple, easy to implement, and computationally efficient tropical convolutional neural network architecture that is robust against adversarial attacks. We exploit the tropical nature of piece-wise linear neural networks by embedding the data in the tropical projective torus in a single hidden layer which can be added to any model. We study the geometry of its decision boundary theoretically and show its robustness against adversarial attacks on image datasets using computational experiments.
    
[^57]: 安全的基于监督学习的智能家居认证框架

    Secure Supervised Learning-Based Smart Home Authentication Framework

    [https://arxiv.org/abs/2402.00568](https://arxiv.org/abs/2402.00568)

    本论文提出了一个基于监督学习的安全的智能家居认证框架，解决了现有认证协议无法实现安全相互认证的问题，提高了智能家居环境中设备认证的可行性，并减少了会话密钥泄露、冒充和盗窃智能设备攻击的可能性。

    

    随着近几十年来物联网和信息通信技术的系统进步，智能家居具备为用户提供家庭服务的能力。智能设备提供的家庭服务帮助用户在提高生活质量的目标上达到最大化的舒适水平。由于用户和智能设备之间通过不安全的通道进行通信，智能家居环境容易出现安全和隐私问题。需要在智能家居环境中建立一个安全的认证协议，使得智能设备和用户之间的设备认证成为可能。大多数现有的智能家居认证协议被发现无法实现安全的相互认证，增加了会话密钥泄露、冒充和盗窃智能设备攻击的可能性。本文提出了基于监督学习的安全的智能家居认证框架。

    The Smart home possesses the capability of facilitating home services to their users with the systematic advance in The Internet of Things (IoT) and information and communication technologies (ICT) in recent decades. The home service offered by the smart devices helps the users in utilize maximized level of comfort for the objective of improving life quality. As the user and smart devices communicate through an insecure channel, the smart home environment is prone to security and privacy problems. A secure authentication protocol needs to be established between the smart devices and the user, such that a situation for device authentication can be made feasible in smart home environments. Most of the existing smart home authentication protocols were identified to fail in facilitating a secure mutual authentication and increases the possibility of lunching the attacks of session key disclosure, impersonation and stolen smart device. In this paper, Secure Supervised Learning-based Smart H
    
[^58]: 一次图卷积就够了：高效灰度图像分类

    A Single Graph Convolution Is All You Need: Efficient Grayscale Image Classification

    [https://arxiv.org/abs/2402.00564](https://arxiv.org/abs/2402.00564)

    本论文提出了一种高效的灰度图像分类方法，通过将图像视为矢量，并使用单个图卷积层进行处理，提高了分类模型的准确性和稳定性。

    

    图像分类器通常依赖于卷积神经网络(CNN)来完成任务，而CNN相比于多层感知机(MLP)更加庞大，这在实时应用中可能会带来问题。此外，许多图像分类模型适用于RGB和灰度数据集，但仅仅使用灰度图像的分类器相对较少见。灰度图像分类具有广泛的应用，包括但不限于医学图像分类和合成孔径雷达(SAR)自动目标识别(ATR)。因此，我们提出了一种使用图像的矢量化视图的新型灰度(单通道)图像分类方法。我们通过将图像视为矢量，并将问题设置为灰度图像分类问题，充分利用了MLP的轻量级特性。我们发现，批次级别使用单个图卷积层可以提高模型的准确性并减小性能的差异。此外，我们开发了定制的准确率估计方法。

    Image classifiers often rely on convolutional neural networks (CNN) for their tasks, which are inherently more heavyweight than multilayer perceptrons (MLPs), which can be problematic in real-time applications. Additionally, many image classification models work on both RGB and grayscale datasets. Classifiers that operate solely on grayscale images are much less common. Grayscale image classification has diverse applications, including but not limited to medical image classification and synthetic aperture radar (SAR) automatic target recognition (ATR). Thus, we present a novel grayscale (single channel) image classification approach using a vectorized view of images. We exploit the lightweightness of MLPs by viewing images as a vector and reducing our problem setting to the grayscale image classification setting. We find that using a single graph convolutional layer batch-wise increases accuracy and reduces variance in the performance of our model. Moreover, we develop a customized acc
    
[^59]: 量子辅助的希尔伯特空间高斯过程回归

    Quantum-Assisted Hilbert-Space Gaussian Process Regression

    [https://arxiv.org/abs/2402.00544](https://arxiv.org/abs/2402.00544)

    该论文提出了一种基于希尔伯特空间近似的量子算法，用于解决高斯过程回归的计算复杂度问题。这个方法结合了经典基函数展开和量子计算技术，通过量子主成分分析、条件旋转和哈达玛和Swap测试来估计特征值和评估高斯过程的后验。

    

    高斯过程是机器学习中常用的概率模型，用作函数先验。由于它们的概率性质，可以捕捉噪声统计、函数平滑度和训练数据不确定性的先验信息。然而，随着数据集规模的增长，它们的计算复杂度迅速变得难以处理。我们提出了一种基于希尔伯特空间近似的量子算法来解决高斯过程回归的限制。我们的方法结合了经典基函数展开和量子计算技术，包括量子主成分分析、条件旋转和哈达玛和Swap测试。量子主成分分析用于估计特征值，条件旋转和哈达玛和Swap测试用于评估高斯过程的后验均值和方差。我们的方法实现了多项式计算复杂度的降低。

    Gaussian processes are probabilistic models that are commonly used as functional priors in machine learning. Due to their probabilistic nature, they can be used to capture the prior information on the statistics of noise, smoothness of the functions, and training data uncertainty. However, their computational complexity quickly becomes intractable as the size of the data set grows. We propose a Hilbert space approximation-based quantum algorithm for Gaussian process regression to overcome this limitation. Our method consists of a combination of classical basis function expansion with quantum computing techniques of quantum principal component analysis, conditional rotations, and Hadamard and Swap tests. The quantum principal component analysis is used to estimate the eigenvalues while the conditional rotations and the Hadamard and Swap tests are employed to evaluate the posterior mean and variance of the Gaussian process. Our method provides polynomial computational complexity reductio
    
[^60]: 视觉转换器中关键信息的流形表示

    A Manifold Representation of the Key in Vision Transformers

    [https://arxiv.org/abs/2402.00534](https://arxiv.org/abs/2402.00534)

    该论文提出了一种在视觉转换器中将关键信息与查询和数值分离并采用流形表示的方法，实验证明这种方法能够提升模型性能，并在ImageNet-1K和COCO数据集上取得了积极的结果。

    

    视觉转换器通过堆叠多个注意力块实现了多头自注意力（MSA）。在这些块中，查询、关键信息和数值通常纠缠在一起，并通过单个共享线性变换生成。本文探索了将关键信息从查询和数值中解耦，并为关键信息采用流形表示的概念。我们的实验表明，解耦并赋予关键信息流形结构可以提升模型性能。具体而言，ViT-B在ImageNet-1K数据集上的top-1准确率提高了0.87％，而Swin-T则在top-1准确率上提高了0.52％，使用了八个流形关键图。我们的方法在COCO数据集上的目标检测和实例分割任务中也取得了积极的结果。通过详细的消融研究，我们证明了这些性能提升不仅仅是由于增加了更多参数和计算的简单性。未来的研究可以探索减少计算复杂度的策略。

    Vision Transformers implement multi-head self-attention (MSA) via stacking multiple attention blocks. The query, key, and value are often intertwined and generated within those blocks via a single, shared linear transformation. This paper explores the concept of disentangling the key from the query and value, and adopting a manifold representation for the key. Our experiments reveal that decoupling and endowing the key with a manifold structure can enhance the model performance. Specifically, ViT-B exhibits a 0.87% increase in top-1 accuracy, while Swin-T sees a boost of 0.52% in top-1 accuracy on the ImageNet-1K dataset, with eight charts in the manifold key. Our approach also yields positive results in object detection and instance segmentation tasks on the COCO dataset. Through detailed ablation studies, we establish that these performance gains are not merely due to the simplicity of adding more parameters and computations. Future research may investigate strategies for cutting the
    
[^61]: 预处理对物理信息神经网络的作用

    Preconditioning for Physics-Informed Neural Networks

    [https://arxiv.org/abs/2402.00531](https://arxiv.org/abs/2402.00531)

    使用条件数作为度量标准来诊断和缓解物理信息神经网络中的训练病态。使用预处理来改善条件数。在18个PDE问题的评估中，我们的方法展示出了优越的性能，特别是在7个问题中将误差减少了一个数量级。

    

    物理信息神经网络（PINNs）已经展示出在解决各种偏微分方程（PDEs）方面的潜力。然而，训练病态影响了PINNs的收敛性和预测精度，进一步限制了它们的实际应用。本文中，我们提出使用条件数作为一种度量标准来诊断和缓解PINNs中的训练病态。受经典数值分析的启发，其中条件数测量敏感性和稳定性，我们强调其在PINNs的训练动态中的关键作用。我们证明了定理，揭示了条件数与PINNs的误差控制和收敛性的关系。随后，我们提出了一种利用预处理来改善条件数的算法。对18个PDE问题的评估展示了我们方法的优越性能。值得注意的是，在其中的7个问题中，我们的方法将错误减少了一个数量级。这些实证发现验证了条件数的关键作用。

    Physics-informed neural networks (PINNs) have shown promise in solving various partial differential equations (PDEs). However, training pathologies have negatively affected the convergence and prediction accuracy of PINNs, which further limits their practical applications. In this paper, we propose to use condition number as a metric to diagnose and mitigate the pathologies in PINNs. Inspired by classical numerical analysis, where the condition number measures sensitivity and stability, we highlight its pivotal role in the training dynamics of PINNs. We prove theorems to reveal how condition number is related to both the error control and convergence of PINNs. Subsequently, we present an algorithm that leverages preconditioning to improve the condition number. Evaluations of 18 PDE problems showcase the superior performance of our method. Significantly, in 7 of these problems, our method reduces errors by an order of magnitude. These empirical findings verify the critical role of the c
    
[^62]: 理解Transformer在序列建模中的表达能力和机制

    Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling

    [https://arxiv.org/abs/2402.00522](https://arxiv.org/abs/2402.00522)

    本研究系统地探讨了Transformer在长序列建模中的近似性质，并研究了其关键组件对表达能力的影响机制。这些发现揭示了关键参数对Transformer的作用，并为替代架构提供了自然建议。

    

    我们对Transformer在长、稀疏和复杂记忆的序列建模中的近似性质进行了系统研究。我们调查了Transformer的不同组件（如点积自注意力、位置编码和前馈层）是如何影响其表达能力的机制，并通过建立明确的近似率来研究它们的综合影响。我们的研究揭示了Transformer中关键参数（如层数和注意力头数）的作用，并且这些洞察还为替代架构提供了自然建议。

    We conduct a systematic study of the approximation properties of Transformer for sequence modeling with long, sparse and complicated memory. We investigate the mechanisms through which different components of Transformer, such as the dot-product self-attention, positional encoding and feed-forward layer, affect its expressive power, and we study their combined effects through establishing explicit approximation rates. Our study reveals the roles of critical parameters in the Transformer, such as the number of layers and the number of attention heads, and these insights also provide natural suggestions for alternative architectures.
    
[^63]: EE-Tuning:一种经济且可扩展的调整早期终止大型语言模型的解决方案

    EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit Large Language Models

    [https://arxiv.org/abs/2402.00518](https://arxiv.org/abs/2402.00518)

    该论文介绍了一种经济且可扩展的解决方案EE-Tuning，可以使用较少的计算资源和训练数据针对早期终止大型语言模型进行调整，通过性能优化和3D并行性实现卓越的训练效率。实验证实，即使在有限的训练预算下，也可以实现有效的早期终止LLM推理。

    

    本文介绍了EE-Tuning，一种轻量且经济实用的解决方案，可以训练/调整早期终止的大型语言模型（LLMs）。与完整参数的预训练常见方法不同，EE-Tuning通过在参数高效方式下增加额外的早期终止层，与任何预训练（可能是微调）的标准LLM相结合，从而大大降低了计算资源和训练数据的需求。我们通过广泛的性能优化和完全兼容3D并行性的可扩展性，实现了EE-Tuning的卓越训练效率。系统实验证实了EE-Tuning的有效性，证明了在有限的训练预算下可以实现有效的早期终止LLM推理。为了将早期终止LLMs推广到社区，我们在https://github.com/pan-x-c/EE-LLM上发布了EE-Tuning的源代码。

    This work introduces EE-Tuning, a lightweight and economical solution to training/tuning early-exit large language models (LLMs). In contrast to the common approach of full-parameter pre-training, EE-Tuning augments any pre-trained (and possibly fine-tuned) standard LLM with additional early-exit layers that are tuned in a parameter-efficient manner, which requires significantly less computational resources and training data. Our implementation of EE-Tuning achieves outstanding training efficiency via extensive performance optimizations, as well as scalability due to its full compatibility with 3D parallelism. Results of systematic experiments validate the efficacy of EE-Tuning, confirming that effective early-exit LLM inference can be achieved with a limited training budget. In hope of making early-exit LLMs accessible to the community, we release the source code of our implementation of EE-Tuning at https://github.com/pan-x-c/EE-LLM.
    
[^64]: 开发基于深度强化学习的多智能体自适应框架用于动态投资组合风险管理

    Developing A Multi-Agent and Self-Adaptive Framework with Deep Reinforcement Learning for Dynamic Portfolio Risk Management

    [https://arxiv.org/abs/2402.00515](https://arxiv.org/abs/2402.00515)

    本研究开发了一种基于深度强化学习的多智能体自适应框架，用于动态投资组合风险管理。通过两个协同反应的智能体，平衡整体投资组合回报和潜在风险，解决了在复杂金融市场环境下的投资策略问题。

    

    近年来，深度学习和强化学习方法已被用作反应性智能体以在高度动荡的金融市场环境下快速学习并响应新的投资策略，用于投资组合管理。然而，在复杂的金融行业之间存在非常复杂的关联性和不断变化的趋势的情况下，深度学习或强化学习基于的智能体可能会偏向于最大化新制定的投资组合的总回报，而忽视其在全球或区域部门的各种市场条件动荡下的潜在风险。因此，提出了一种名为MASA的多智能体自适应框架，通过两个协同和反应的智能体采用复杂的多智能体强化学习方法来仔细动态平衡整体投资组合回报和潜在风险之间的权衡。

    Deep or reinforcement learning (RL) approaches have been adapted as reactive agents to quickly learn and respond with new investment strategies for portfolio management under the highly turbulent financial market environments in recent years. In many cases, due to the very complex correlations among various financial sectors, and the fluctuating trends in different financial markets, a deep or reinforcement learning based agent can be biased in maximising the total returns of the newly formulated investment portfolio while neglecting its potential risks under the turmoil of various market conditions in the global or regional sectors. Accordingly, a multi-agent and self-adaptive framework namely the MASA is proposed in which a sophisticated multi-agent reinforcement learning (RL) approach is adopted through two cooperating and reactive agents to carefully and dynamically balance the trade-off between the overall portfolio returns and their potential risks. Besides, a very flexible and p
    
[^65]: 经验风险最小化与f-分布族正则化的等价性

    Equivalence of the Empirical Risk Minimization to Regularization on the Family of f-Divergences

    [https://arxiv.org/abs/2402.00501](https://arxiv.org/abs/2402.00501)

    经验风险最小化与f-分布族的正则化的解决方案在特定条件下是唯一的，并且可以通过使用不同的f-分布正则化等效地表示。

    

    在对f中的温和条件下，给出了经验风险最小化与f-分布的正则化（ERM-$f$DR）的解法。在这些条件下，最优测度被证明是唯一的。并给出了特定选择函数f的解决方案的示例。通过利用f-分布族的灵活性，获得了先前对常见正则化选择的已知解决方案，包括相对熵正则化的唯一解（Type-I和Type-II）。对解的分析揭示了在ERM-$f$DR问题中使用f-分布时的以下属性：$i)$ f-分布正则化强制将解的支持与参考测度的支持重合，引入了在训练数据提供的证据中占主导地位的强归纳偏差；$ii)$ 任何f-分布的正则化都等价于另一种f-分布的正则化。

    The solution to empirical risk minimization with $f$-divergence regularization (ERM-$f$DR) is presented under mild conditions on $f$. Under such conditions, the optimal measure is shown to be unique. Examples of the solution for particular choices of the function $f$ are presented. Previously known solutions to common regularization choices are obtained by leveraging the flexibility of the family of $f$-divergences. These include the unique solutions to empirical risk minimization with relative entropy regularization (Type-I and Type-II). The analysis of the solution unveils the following properties of $f$-divergences when used in the ERM-$f$DR problem: $i\bigl)$ $f$-divergence regularization forces the support of the solution to coincide with the support of the reference measure, which introduces a strong inductive bias that dominates the evidence provided by the training data; and $ii\bigl)$ any $f$-divergence regularization is equivalent to a different $f$-divergence regularization 
    
[^66]: CPT: 应用于少样本节点分类的能 力递进式训练策略

    CPT: Competence-progressive Training Strategy for Few-shot Node Classification

    [https://arxiv.org/abs/2402.00450](https://arxiv.org/abs/2402.00450)

    CPT是一种新颖的两阶段课程学习方法，弥补了传统元学习方法在少样本节点分类上的困难。它使用能力递进的训练策略来提高元学习器的效果和稳定性。

    

    图神经网络（GNNs）在节点分类方面取得了显著的进展，但其成功仍然依赖于训练数据中每个类别有足够的标记节点。现实世界中的图数据通常呈现出长尾分布，标签稀疏，强调了GNN在少样本节点分类中的重要性，即使用有限的数据对节点进行分类。传统的情节元学习方法在这个领域显示出了潜力，但它们面临着固有的限制：随机和均匀任务分配可能导致模型收敛到次优解，忽视了任务的难度水平。这可能导致元学习器过早地面临复杂任务，阻碍了正常的学习。理想情况下，元学习器应该从简单概念开始，逐渐进入更复杂的概念，就像人类学习一样。因此，我们引入了CPT，一种新颖的两阶段课程学习方法，将任务难度与元学习器的递进能力相匹配，增强了元学习的效果和稳定性。

    Graph Neural Networks (GNNs) have made significant advancements in node classification, but their success relies on sufficient labeled nodes per class in the training data. Real-world graph data often exhibits a long-tail distribution with sparse labels, emphasizing the importance of GNNs' ability in few-shot node classification, which entails categorizing nodes with limited data. Traditional episodic meta-learning approaches have shown promise in this domain, but they face an inherent limitation: it might lead the model to converge to suboptimal solutions because of random and uniform task assignment, ignoring task difficulty levels. This could lead the meta-learner to face complex tasks too soon, hindering proper learning. Ideally, the meta-learner should start with simple concepts and advance to more complex ones, like human learning. So, we introduce CPT, a novel two-stage curriculum learning method that aligns task difficulty with the meta-learner's progressive competence, enhanci
    
[^67]: 数据高效图学习的综述

    A Survey of Data-Efficient Graph Learning

    [https://arxiv.org/abs/2402.00447](https://arxiv.org/abs/2402.00447)

    这项研究提出了数据高效图学习（DEGL）的概念，并总结了近期在这一领域的进展。DEGL的目标是在资源有限的场景下提高图机器学习的性能，通过探索各种最小监督方法来解决大规模标记数据的挑战。

    

    图结构化数据在社交网络到生物化学分析等领域中广泛存在，是各种现实世界系统的基础。虽然图神经网络在建模这种数据方面表现出色，但它们的成功往往依赖于大量标记数据，这在标注资源有限的实际场景中构成了挑战。为了解决这个问题，我们致力于通过探索各种最小监督方法来提高低资源设置下的图机器学习性能。本文介绍了一种新颖的数据高效图学习(DEGL)的研究前沿，并提供了对DEGL当前进展的首次综述。我们首先强调了使用大规模标记数据训练模型所固有的挑战，为我们对DEGL的探索铺平了道路。接下来，我们从几个关键方面系统地回顾了这一主题的最新进展，其中包括...

    Graph-structured data, prevalent in domains ranging from social networks to biochemical analysis, serve as the foundation for diverse real-world systems. While graph neural networks demonstrate proficiency in modeling this type of data, their success is often reliant on significant amounts of labeled data, posing a challenge in practical scenarios with limited annotation resources. To tackle this problem, tremendous efforts have been devoted to enhancing graph machine learning performance under low-resource settings by exploring various approaches to minimal supervision. In this paper, we introduce a novel concept of Data-Efficient Graph Learning (DEGL) as a research frontier, and present the first survey that summarizes the current progress of DEGL. We initiate by highlighting the challenges inherent in training models with large labeled data, paving the way for our exploration into DEGL. Next, we systematically review recent advances on this topic from several key aspects, including 
    
[^68]: 基于卷积自编码器的降阶模型的实用存在定理

    A practical existence theorem for reduced order models based on convolutional autoencoders

    [https://arxiv.org/abs/2402.00435](https://arxiv.org/abs/2402.00435)

    本论文提出了基于卷积自编码器的降阶模型的实用存在定理，解决了在处理复杂非线性问题方面传统方法的不足，并讨论了如何学习潜在特征的挑战。

    

    近年来，深度学习在偏微分方程和降阶建模领域越发受欢迎，提供了基于物理知识的神经网络、神经算子、深度算子网络和深度学习降阶模型等强大的数据驱动技术。在这种情况下，基于卷积神经网络的深度自编码器表现出极高的效果，在处理复杂的非线性问题时，优于传统的降阶方法。然而，尽管基于CNN的自编码器在实践中取得了成功，但目前只有少数理论结果支持这些架构，通常以万能逼近定理的形式陈述。尤其是，尽管现有文献为设计卷积自编码器提供了指导方针，但学习潜在特征的后续挑战几乎没有被探究。

    In recent years, deep learning has gained increasing popularity in the fields of Partial Differential Equations (PDEs) and Reduced Order Modeling (ROM), providing domain practitioners with new powerful data-driven techniques such as Physics-Informed Neural Networks (PINNs), Neural Operators, Deep Operator Networks (DeepONets) and Deep-Learning based ROMs (DL-ROMs). In this context, deep autoencoders based on Convolutional Neural Networks (CNNs) have proven extremely effective, outperforming established techniques, such as the reduced basis method, when dealing with complex nonlinear problems. However, despite the empirical success of CNN-based autoencoders, there are only a few theoretical results supporting these architectures, usually stated in the form of universal approximation theorems. In particular, although the existing literature provides users with guidelines for designing convolutional autoencoders, the subsequent challenge of learning the latent features has been barely inv
    
[^69]: 通过权重集成专家的多任务模型合并

    Merging Multi-Task Models via Weight-Ensembling Mixture of Experts

    [https://arxiv.org/abs/2402.00433](https://arxiv.org/abs/2402.00433)

    通过权重集成专家的方法可以将训练在不同任务上的Transformer模型合并为一个统一的模型，通过动态整合共享和特定任务的知识来提供更灵活的解决方案。

    

    将训练在不同任务上的各种特定任务的Transformer模型合并为一个统一的模型，可以同时执行所有任务。以任务算术为例的先前方法已被证明既有效又可扩展。现有方法主要集中在寻找原始模型参数空间内的静态最优解。一个显著的挑战是减轻不同模型参数之间的干扰，这可能会严重削弱性能。在本文中，我们提出了一种方法，将大多数参数合并在一起，同时将Transformer层的MLP扩展为权重集成专家（MoE）模块，该模块可以根据输入动态地整合共享和特定任务的知识，从而提供一个更灵活的解决方案，可以适应每个实例的特定需求。我们的关键见解是通过识别和分离共享知识和特定任务知识，然后动态地整合它们，

    Merging various task-specific Transformer-based models trained on different tasks into a single unified model can execute all the tasks concurrently. Previous methods, exemplified by task arithmetic, have been proven to be both effective and scalable. Existing methods have primarily focused on seeking a static optimal solution within the original model parameter space. A notable challenge is mitigating the interference between parameters of different models, which can substantially deteriorate performance. In this paper, we propose to merge most of the parameters while upscaling the MLP of the Transformer layers to a weight-ensembling mixture of experts (MoE) module, which can dynamically integrate shared and task-specific knowledge based on the input, thereby providing a more flexible solution that can adapt to the specific needs of each instance. Our key insight is that by identifying and separating shared knowledge and task-specific knowledge, and then dynamically integrating them, 
    
[^70]: 从PARIS到LE-PARIS：通过推荐系统和协作大型语言模型实现专利响应自动化

    From PARIS to LE-PARIS: Toward Patent Response Automation with Recommender Systems and Collaborative Large Language Models

    [https://arxiv.org/abs/2402.00421](https://arxiv.org/abs/2402.00421)

    本研究介绍了专利响应智能系统PARIS和LE-PARIS，通过构建OA主题数据库、开发响应模板以及实施推荐系统和基于LLM的响应生成，旨在加快专利律师处理审查意见回应的效率。 通过多范式分析和长期数据验证，证明了OA主题的建设性和LLM对于回应自动生成的可行性。

    

    在专利审查中，对于及时和有效地回应审查意见（OAs）对于获得专利至关重要，然而过去的自动化和人工智能研究很少涉及到这一方面。为了弥补这一空白，我们的研究介绍了专利审查意见响应智能系统（PARIS）及其先进版本LE-PARIS。这些系统旨在加快专利律师在协作处理OA回应方面的效率。系统的关键特征包括构建OA主题数据库，开发响应模板，以及实施推荐系统和基于LLM的响应生成。我们的验证涉及使用USPTO Office Action数据库和律师与我们系统的长期交互数据进行的多范式分析，为期六年。通过五个研究，我们利用主题建模和提出的Delphi过程来检验OA主题的建设性（研究1和2），还有使用推荐系统和基于LLM的响应生成来提高回应质量（研究3和4），以及经过训练的LLM对于回应自动生成的可行性（研究5）。

    In patent prosecution, timely and effective responses to Office Actions (OAs) are crucial for acquiring patents, yet past automation and AI research have scarcely addressed this aspect. To address this gap, our study introduces the Patent Office Action Response Intelligence System (PARIS) and its advanced version, the Large Language Model Enhanced PARIS (LE-PARIS). These systems are designed to expedite the efficiency of patent attorneys in collaboratively handling OA responses. The systems' key features include the construction of an OA Topics Database, development of Response Templates, and implementation of Recommender Systems and LLM-based Response Generation. Our validation involves a multi-paradigmatic analysis using the USPTO Office Action database and longitudinal data of attorney interactions with our systems over six years. Through five studies, we examine the constructiveness of OA topics (studies 1 and 2) using topic modeling and the proposed Delphi process, the efficacy of
    
[^71]: 短文: 基准测试可转移性对抗攻击

    Short: Benchmarking transferable adversarial attacks

    [https://arxiv.org/abs/2402.00418](https://arxiv.org/abs/2402.00418)

    本研究首次全面评估了可转移性对抗攻击的方面，引入了一个基准框架并系统分类和评估了各种增强对抗攻击可转移性的方法，为不同的模型架构提供了一个标准化的平台。

    

    深度学习模型对抗攻击的稳健性仍然是一个关键关注点。本研究首次全面评估了可转移性对抗攻击的方面。它系统地分类和批判性评估了各种增强对抗攻击可转移性的方法。该研究涵盖了一系列技术，包括生成结构、语义相似性、梯度编辑、目标修改和集成方法。与此同时，本文介绍了一个基准框架"TAA-Bench"，集成了十种主要的对抗攻击可转移性方法，从而为不同的模型架构提供了一个标准化和系统化的比较分析平台。通过全面的审查，我们揭示了每种方法的效力和限制，并阐明了它们的操作原理和实际效用。本综述试图成为一种基于多模型架构进行对比分析的标准化平台。

    The robustness of deep learning models against adversarial attacks remains a pivotal concern. This study presents, for the first time, an exhaustive review of the transferability aspect of adversarial attacks. It systematically categorizes and critically evaluates various methodologies developed to augment the transferability of adversarial attacks. This study encompasses a spectrum of techniques, including Generative Structure, Semantic Similarity, Gradient Editing, Target Modification, and Ensemble Approach. Concurrently, this paper introduces a benchmark framework \textit{TAA-Bench}, integrating ten leading methodologies for adversarial attack transferability, thereby providing a standardized and systematic platform for comparative analysis across diverse model architectures. Through comprehensive scrutiny, we delineate the efficacy and constraints of each method, shedding light on their underlying operational principles and practical utility. This review endeavors to be a quintesse
    
[^72]: 跨城市少样本交通预测的多尺度交通模式库

    Multi-scale Traffic Pattern Bank for Cross-city Few-shot Traffic Forecasting

    [https://arxiv.org/abs/2402.00397](https://arxiv.org/abs/2402.00397)

    我们提出了一种跨城市少样本交通预测的解决方案，利用多尺度交通模式库从数据丰富的源城市学习并预测其他城市的交通情况。

    

    交通预测对智能交通系统具有重要意义，可以帮助高效分配资源和有效控制交通。然而，其有效性往往严重依赖于丰富的交通数据，而许多城市由于设备支持有限而缺乏足够的数据，这对交通预测构成了重大挑战。鉴于这一挑战，我们做出了一个显著的观察：交通模式在不同城市之间存在相似性。基于这一关键洞察，我们提出了一种解决跨城市少样本交通预测问题的方法，称为多尺度交通模式库（MTPB）。主要上，MTPB通过利用数据丰富的源城市启动其学习过程，通过空间-时间感知的预训练过程有效获取全面的交通知识。随后，该框架采用先进的聚类技术从学习到的知识中系统生成一个多尺度交通模式库。接下来，该框架使用准确的交通模式检索机制进行跨城市的少样本交通预测。

    Traffic forecasting is crucial for intelligent transportation systems (ITS), aiding in efficient resource allocation and effective traffic control. However, its effectiveness often relies heavily on abundant traffic data, while many cities lack sufficient data due to limited device support, posing a significant challenge for traffic forecasting. Recognizing this challenge, we have made a noteworthy observation: traffic patterns exhibit similarities across diverse cities. Building on this key insight, we propose a solution for the cross-city few-shot traffic forecasting problem called Multi-scale Traffic Pattern Bank (MTPB). Primarily, MTPB initiates its learning process by leveraging data-rich source cities, effectively acquiring comprehensive traffic knowledge through a spatial-temporal-aware pre-training process. Subsequently, the framework employs advanced clustering techniques to systematically generate a multi-scale traffic pattern bank derived from the learned knowledge. Next, th
    
[^73]: LLMs的高效探索

    Efficient Exploration for LLMs

    [https://arxiv.org/abs/2402.00396](https://arxiv.org/abs/2402.00396)

    高效探索在改善大型语言模型方面具有显著优势，可以以较少的查询实现较高的性能水平。不确定性估计和探索方案的选择是关键因素。

    

    我们提供了证据，表明高效探索在获取人类反馈以改善大型语言模型方面具有显著优势。在我们的实验中，一个代理程序在收到反馈时将奖励模型拟合到查询上。我们表现最佳的代理程序使用双Thompson采样生成查询，不确定性由认知神经网络表示。我们的结果表明，高效探索使得性能水平可以在较少的查询下达到较高水平。此外，不确定性估计和探索方案的选择起着关键作用。

    We present evidence of substantial benefit from efficient exploration in gathering human feedback to improve large language models. In our experiments, an agent sequentially generates queries while fitting a reward model to the feedback received. Our best-performing agent generates queries using double Thompson sampling, with uncertainty represented by an epistemic neural network. Our results demonstrate that efficient exploration enables high levels of performance with far fewer queries. Further, both uncertainty estimation and the choice of exploration scheme play critical roles.
    
[^74]: 考虑死区的神经网络损失函数

    Loss Function Considering Dead Zone for Neural Networks

    [https://arxiv.org/abs/2402.00393](https://arxiv.org/abs/2402.00393)

    本文提出了一种新的损失函数，考虑机械手臂死区内的逆动力学计算。该方法能够提高训练可用的运动数据量，并提高逆动力学计算的准确性。

    

    揭示机械手臂的逆动力学是提高基于模型控制的控制性能的重要任务。神经网络是表示复杂逆动力学的有望技术，但需要大量的运动数据。然而，在执行机构的死区中的运动数据不适合用于模型训练，这降低了可用于训练的有用数据量。本研究基于机械手臂关节在死区内不工作的事实，提出了一种新的损失函数，只考虑不在死区的关节的误差。该方法使得可用于训练的运动数据量增加，并提高了逆动力学计算的准确性。通过使用三自由度机械手臂的实际设备进行的实验表明，该方法的准确性优于传统方法。我们还验证并讨论了在死区中使用所提出方法的模型的行为。

    It is important to reveal the inverse dynamics of manipulators to improve control performance of model-based control. Neural networks (NNs) are promising techniques to represent complicated inverse dynamics while they require a large amount of motion data. However, motion data in dead zones of actuators is not suitable for training models decreasing the number of useful training data. In this study, based on the fact that the manipulator joint does not work irrespective of input torque in dead zones, we propose a new loss function that considers only errors of joints not in dead zones. The proposed method enables to increase in the amount of motion data available for training and the accuracy of the inverse dynamics computation. Experiments on actual equipment using a three-degree-of-freedom (DOF) manipulator showed higher accuracy than conventional methods. We also confirmed and discussed the behavior of the model of the proposed method in dead zones.
    
[^75]: 基于累积分布函数的通用时间点过程

    Cumulative Distribution Function based General Temporal Point Processes

    [https://arxiv.org/abs/2402.00388](https://arxiv.org/abs/2402.00388)

    本研究引入了CuFun模型，基于累积分布函数的通用时间点过程，解决了深度时间点过程模型中的强度函数建模、积分计算复杂性和长期时序依赖性捕捉的问题。

    

    时间点过程在建模各个领域中的事件序列（包括社交网络和电子商务）中发挥着关键作用，并对推荐系统和信息检索策略的进展做出了重大贡献。通过分析用户交互和交易等事件，时间点过程提供了有价值的行为模式洞察，有助于预测未来的趋势。然而，由于这些模式的复杂性，准确预测未来事件仍然是一个巨大挑战。将神经网络与时间点过程相结合，开发了先进的深度时间点过程模型。虽然这些模型在处理复杂和非线性时间数据方面表现出色，但在建模强度函数、复杂积分计算和有效捕捉长期时序依赖方面存在局限性。在本研究中，我们介绍了CuFun模型，该模型代表了一种新的方法来解决这些问题。

    Temporal Point Processes (TPPs) hold a pivotal role in modeling event sequences across diverse domains, including social networking and e-commerce, and have significantly contributed to the advancement of recommendation systems and information retrieval strategies. Through the analysis of events such as user interactions and transactions, TPPs offer valuable insights into behavioral patterns, facilitating the prediction of future trends. However, accurately forecasting future events remains a formidable challenge due to the intricate nature of these patterns. The integration of Neural Networks with TPPs has ushered in the development of advanced deep TPP models. While these models excel at processing complex and nonlinear temporal data, they encounter limitations in modeling intensity functions, grapple with computational complexities in integral computations, and struggle to capture long-range temporal dependencies effectively. In this study, we introduce the CuFun model, representing
    
[^76]: Image2Points：一种基于3D点的上下文聚类GAN用于高质量PET图像重建

    Image2Points:A 3D Point-based Context Clusters GAN for High-Quality PET Image Reconstruction

    [https://arxiv.org/abs/2402.00376](https://arxiv.org/abs/2402.00376)

    这项研究提出了一种基于3D点的上下文聚类GAN方法，用于通过低剂量PET图像重建高质量的正电子发射断层扫描（PET）图像。这种方法通过使用点的表示和上下文聚类策略，增强了图像结构的明确表达，并减轻了重建图像中小结构的模糊性。

    

    为了获取高质量的正电子发射断层扫描（PET）图像同时最小化辐射暴露，已经提出了许多方法来从相应的低剂量PET（LPET）图像重建标准剂量PET（SPET）图像。然而，这些方法严重依赖于基于体素的表示，无法充分考虑精确结构和细粒度上下文，导致重建结果受损。在本文中，我们提出了一种基于3D点的上下文聚类GAN，即PCC-GAN，用于从LPET重建高质量的SPET图像。具体而言，受到点的几何表示能力的启发，我们使用点的表示方法来增强图像结构的明确表达，从而方便带有更细节的重建。此外，我们采用上下文聚类策略来探索点之间的上下文关系，从而减轻重建图像中小结构的模糊性。实验结果表明，

    To obtain high-quality Positron emission tomography (PET) images while minimizing radiation exposure, numerous methods have been proposed to reconstruct standard-dose PET (SPET) images from the corresponding low-dose PET (LPET) images. However, these methods heavily rely on voxel-based representations, which fall short of adequately accounting for the precise structure and fine-grained context, leading to compromised reconstruction. In this paper, we propose a 3D point-based context clusters GAN, namely PCC-GAN, to reconstruct high-quality SPET images from LPET. Specifically, inspired by the geometric representation power of points, we resort to a point-based representation to enhance the explicit expression of the image structure, thus facilitating the reconstruction with finer details. Moreover, a context clustering strategy is applied to explore the contextual relationships among points, which mitigates the ambiguities of small structures in the reconstructed images. Experiments on 
    
[^77]: 自适应原始-对偶方法用于安全强化学习

    Adaptive Primal-Dual Method for Safe Reinforcement Learning

    [https://arxiv.org/abs/2402.00355](https://arxiv.org/abs/2402.00355)

    本论文提出了一种自适应原始-对偶方法用于安全强化学习，通过调整自适应学习速率以优化策略，实现了算法的收敛性、最优性和可行性。实验结果表明，该方法在安全强化学习中具有更好的性能和稳定性。

    

    原始-对偶方法在安全强化学习中有自然应用，被提出作为一个约束策略优化问题。然而，在实践中，将原始-对偶方法应用于安全强化学习是具有挑战性的，因为每次解决嵌入的无约束强化学习问题时，学习速率（LR）和拉格朗日乘子（对偶变量）之间存在相互依赖。在本文中，我们提出、分析和评估了适应性原始-对偶（APD）方法用于安全强化学习，在每次迭代中，调整两个自适应LR以使之优化策略。我们从理论上建立了APD算法的收敛性、最优性和可行性。最后，我们使用Bullet-Safey-Gym中的四个知名环境，利用两个先进的安全强化学习算法（PPO-Lagrangian和DDPG-Lagrangian）对实际APD算法进行了数值评估。所有实验表明，实际APD算法的性能优于（或达到可比较的性能），并且具有更稳定的效果。

    Primal-dual methods have a natural application in Safe Reinforcement Learning (SRL), posed as a constrained policy optimization problem. In practice however, applying primal-dual methods to SRL is challenging, due to the inter-dependency of the learning rate (LR) and Lagrangian multipliers (dual variables) each time an embedded unconstrained RL problem is solved. In this paper, we propose, analyze and evaluate adaptive primal-dual (APD) methods for SRL, where two adaptive LRs are adjusted to the Lagrangian multipliers so as to optimize the policy in each iteration. We theoretically establish the convergence, optimality and feasibility of the APD algorithm. Finally, we conduct numerical evaluation of the practical APD algorithm with four well-known environments in Bullet-Safey-Gym employing two state-of-the-art SRL algorithms: PPO-Lagrangian and DDPG-Lagrangian. All experiments show that the practical APD algorithm outperforms (or achieves comparable performance) and attains more stable
    
[^78]: 图像到图像生成模型的机器遗忘

    Machine Unlearning for Image-to-Image Generative Models

    [https://arxiv.org/abs/2402.00351](https://arxiv.org/abs/2402.00351)

    本文提出了一种适用于图像到图像生成模型的机器遗忘方法，该方法通过提供一个统一的框架和一个高效的算法，实现在遗忘样本中删除信息且在保留样本上性能几乎没有下降。实证研究证明该方法不依赖于保留样本的可用性，符合数据保留政策。

    

    机器遗忘已经成为一种新的范式，用于从给定模型中有意地遗忘数据样本以符合严格的规定。然而，现有的机器遗忘方法主要集中在分类模型上，对于生成模型的遗忘领域相对未被探索。本文作为一座桥梁，通过提供一个统一的框架来探讨图像到图像生成模型的机器遗忘问题。在该框架下，我们提出了一种计算效率高的算法，通过严格的理论分析证明在保留样本上性能下降可忽略，同时有效地从遗忘样本中删除信息。在两个大规模数据集ImageNet-1K和Places-365上的实证研究进一步表明，我们的算法不依赖于保留样本的可用性，进一步符合数据保留政策。据我们所知，这项工作是第一个进行图像到图像生成模型遗忘研究的工作。

    Machine unlearning has emerged as a new paradigm to deliberately forget data samples from a given model in order to adhere to stringent regulations. However, existing machine unlearning methods have been primarily focused on classification models, leaving the landscape of unlearning for generative models relatively unexplored. This paper serves as a bridge, addressing the gap by providing a unifying framework of machine unlearning for image-to-image generative models. Within this framework, we propose a computationally-efficient algorithm, underpinned by rigorous theoretical analysis, that demonstrates negligible performance degradation on the retain samples, while effectively removing the information from the forget samples. Empirical studies on two large-scale datasets, ImageNet-1K and Places-365, further show that our algorithm does not rely on the availability of the retain samples, which further complies with data retention policy. To our best knowledge, this work is the first tha
    
[^79]: ODICE: 通过正交梯度更新揭示分布校正估计的奥秘

    ODICE: Revealing the Mystery of Distribution Correction Estimation via Orthogonal-gradient Update

    [https://arxiv.org/abs/2402.00348](https://arxiv.org/abs/2402.00348)

    ODICE研究了离线强化学习和模仿学习中重要的分布校正估计（DICE）方法，并发现在使用真梯度更新学习值函数时存在前向梯度和后向梯度两个梯度项。为了解决这个问题，他们提出了一种简单但有效的修正方法。

    

    在这项研究中，我们调查了分布校正估计（DICE）方法，这是离线强化学习（RL）和模仿学习（IL）中重要的研究方向。基于DICE的方法对状态行为级别的行为约束施加了，这对于离线学习是一个理想的选择。然而，它们通常比仅使用动作级别行为约束的当前最先进方法表现得更差。在重新审视了基于DICE的方法后，我们发现在使用真梯度更新学习值函数时存在两个梯度项：前向梯度（在当前状态上）和后向梯度（在下一个状态上）。使用前向梯度与许多离线RL方法有很大的相似之处，因此可以被视为应用动作级别约束。然而，如果这两个梯度有相互冲突的方向，直接加上后向梯度可能会退化或抵消其效果。为了解决这个问题，我们提出了一个简单但有效的修正方法。

    In this study, we investigate the DIstribution Correction Estimation (DICE) methods, an important line of work in offline reinforcement learning (RL) and imitation learning (IL). DICE-based methods impose state-action-level behavior constraint, which is an ideal choice for offline learning. However, they typically perform much worse than current state-of-the-art (SOTA) methods that solely use action-level behavior constraint. After revisiting DICE-based methods, we find there exist two gradient terms when learning the value function using true-gradient update: forward gradient (taken on the current state) and backward gradient (taken on the next state). Using forward gradient bears a large similarity to many offline RL methods, and thus can be regarded as applying action-level constraint. However, directly adding the backward gradient may degenerate or cancel out its effect if these two gradients have conflicting directions. To resolve this issue, we propose a simple yet effective modi
    
[^80]: 来自数据驱动和领域驱动视角的机器学习模型多样解释

    Diverse Explanations from Data-driven and Domain-driven Perspectives for Machine Learning Models

    [https://arxiv.org/abs/2402.00347](https://arxiv.org/abs/2402.00347)

    本文关注机器学习模型解释的不一致性，提出了从一组同样好的模型中选择具备预期解释的准确模型的方法，以加强物理定律并满足利益相关者的要求，并为将可解释人工智能整合到科学领域做出贡献。

    

    机器学习模型的解释是重要的，特别是在化学、生物和物理等科学领域中，它们指导未来的实验室实验和资源需求。这些解释可以从训练良好的机器学习模型（数据驱动视角）或特定领域知识（领域驱动视角）中获得。然而，由于准确但具有误导性的机器学习模型和具有特定需求、愿望或目标的各方存在不一致性。本文提出了对这些不一致性的关注，并提出了一种方法，从一组同样好的模型中找到一个具有预期解释的准确模型，以加强物理定律并满足利益相关者的要求，这些模型也被称为拉诗孟（Rashomon）模型集。我们的目标是促进对这些不一致性的全面理解，并最终为将可解释人工智能（XAI）整合到科学领域做出贡献。

    Explanations of machine learning models are important, especially in scientific areas such as chemistry, biology, and physics, where they guide future laboratory experiments and resource requirements. These explanations can be derived from well-trained machine learning models (data-driven perspective) or specific domain knowledge (domain-driven perspective). However, there exist inconsistencies between these perspectives due to accurate yet misleading machine learning models and various stakeholders with specific needs, wants, or aims. This paper calls attention to these inconsistencies and suggests a way to find an accurate model with expected explanations that reinforce physical laws and meet stakeholders' requirements from a set of equally-good models, also known as Rashomon sets. Our goal is to foster a comprehensive understanding of these inconsistencies and ultimately contribute to the integration of eXplainable Artificial Intelligence (XAI) into scientific domains.
    
[^81]: 联邦学习中隐私威胁和对策的调查

    Survey of Privacy Threats and Countermeasures in Federated Learning

    [https://arxiv.org/abs/2402.00342](https://arxiv.org/abs/2402.00342)

    本文调查了联邦学习中的隐私威胁和对策，并对水平联邦学习、垂直联邦学习和迁移联邦学习的典型类型进行了分类和描述。

    

    联邦学习被广泛认为是一种注重隐私的学习方法，因为客户端之间没有直接交换训练数据。然而，联邦学习中存在隐私威胁，并且已经对隐私对策进行了研究。然而，我们注意到常见和独特的隐私威胁在典型类型的联邦学习中尚未以全面和具体的方式进行分类和描述。在本文中，我们描述了水平联邦学习、垂直联邦学习和迁移联邦学习的隐私威胁和对策。

    Federated learning is widely considered to be as a privacy-aware learning method because no training data is exchanged directly between clients. Nevertheless, there are threats to privacy in federated learning, and privacy countermeasures have been studied. However, we note that common and unique privacy threats among typical types of federated learning have not been categorized and described in a comprehensive and specific way. In this paper, we describe privacy threats and countermeasures for the typical types of federated learning; horizontal federated learning, vertical federated learning, and transfer federated learning.
    
[^82]: PirateNets：采用残差自适应网络的物理知识驱动深度学习

    PirateNets: Physics-informed Deep Learning with Residual Adaptive Networks

    [https://arxiv.org/abs/2402.00326](https://arxiv.org/abs/2402.00326)

    PirateNets是一种物理知识驱动的深度学习框架，解决了多层感知机网络在较大深度时性能下降的问题，通过引入自适应残差连接实现了稳定和高效的训练，并提升了模型的性能。

    

    虽然物理知识驱动神经网络(PINNs)已成为解决由偏微分方程(PDEs)控制的正向和反向问题的流行深度学习框架，但在采用更大和更深的神经网络架构时，它们的性能会下降。我们的研究发现，这种反直觉行为的根源在于使用不适合的初始化方案的多层感知机(MLP)网络结构，导致网络导数的可训练性较差，并最终导致PDE残差损失的不稳定最小化。为了解决这个问题，我们提出了物理知识驱动残差自适应网络(PirateNets)，这是一种新型架构，旨在促进深度PINN模型的稳定和高效训练。PirateNets利用一种新颖的自适应残差连接，允许网络作为浅层网络进行初始化，并在训练过程中逐渐加深。我们还展示了所提出的初始化方案可以提高PINN模型的训练效果并改善性能。

    While physics-informed neural networks (PINNs) have become a popular deep learning framework for tackling forward and inverse problems governed by partial differential equations (PDEs), their performance is known to degrade when larger and deeper neural network architectures are employed. Our study identifies that the root of this counter-intuitive behavior lies in the use of multi-layer perceptron (MLP) architectures with non-suitable initialization schemes, which result in poor trainablity for the network derivatives, and ultimately lead to an unstable minimization of the PDE residual loss. To address this, we introduce Physics-informed Residual Adaptive Networks (PirateNets), a novel architecture that is designed to facilitate stable and efficient training of deep PINN models. PirateNets leverage a novel adaptive residual connection, which allows the networks to be initialized as shallow networks that progressively deepen during training. We also show that the proposed initializatio
    
[^83]: 多标签学习的一致勒贝格测度

    A Consistent Lebesgue Measure for Multi-label Learning

    [https://arxiv.org/abs/2402.00324](https://arxiv.org/abs/2402.00324)

    提出了一种基于一致勒贝格测度的多标签学习器(CLML)，通过理论证明该方法可以实现一致性。实验证据表明，CLML可以一致地取得最先进的结果，其主要性能因素是勒贝格测度设计。

    

    多标签损失函数通常是非可微的，需要替代损失函数进行梯度优化。 替代损失函数的一致性尚未证明，并且由于多标签损失函数的冲突性质而变得更加严重。 为了直接从多个相关但潜在冲突的多标签损失函数中学习，我们提出了一种基于一致勒贝格测度的多标签学习器（CLML），并证明了在贝叶斯风险框架下CLML可以实现理论上的一致性。 经验证据支持我们的理论，通过展示：（1）CLML可以一贯地取得最先进的结果；（2）主要的性能因素是勒贝格测度设计，因为CLML优化了一个更简单的前馈模型，没有额外的标签图、基于扰动的条件或语义嵌入；以及（3）结果的分析不仅可以区分CLML的有效性，还凸显了替代损失函数与期望损失函数之间的不一致性。

    Multi-label loss functions are usually non-differentiable, requiring surrogate loss functions for gradient-based optimisation. The consistency of surrogate loss functions is not proven and is exacerbated by the conflicting nature of multi-label loss functions. To directly learn from multiple related, yet potentially conflicting multi-label loss functions, we propose a Consistent Lebesgue Measure-based Multi-label Learner (CLML) and prove that CLML can achieve theoretical consistency under a Bayes risk framework. Empirical evidence supports our theory by demonstrating that: (1) CLML can consistently achieve state-of-the-art results; (2) the primary performance factor is the Lebesgue measure design, as CLML optimises a simpler feedforward model without additional label graph, perturbation-based conditioning, or semantic embeddings; and (3) an analysis of the results not only distinguishes CLML's effectiveness but also highlights inconsistencies between the surrogate and the desired loss 
    
[^84]: 模拟数字调度对于联邦学习的通信高效方法

    Analog-digital Scheduling for Federated Learning: A Communication-Efficient Approach

    [https://arxiv.org/abs/2402.00318](https://arxiv.org/abs/2402.00318)

    本文提出一种模拟数字FL方案，通过在每一轮中，通过模拟OTA方案上传梯度或者通过正交RB传输量化梯度的方式调度设备，解决了联邦学习中性能受限于信噪比最差设备问题的差异，以实现通信高效和降低噪声。

    

    最近，通过无线网络进行机器学习模型训练的通信高效的联邦学习（FL）范式中出现了一种称为OTA计算的方法。然而，其性能受到信噪比最差的设备的限制，导致更新速度快但噪声较多。另一方面，通过数字通道将正交资源块（RB）分配给每个设备可以减轻噪声问题，但会增加通信延迟。在本文中，我们解决了这个差异，并提出了ADFL，一种新颖的模拟数字FL方案：在每一轮中，参数服务器（PS）将每个设备调度为通过模拟OTA方案上传其梯度，或者使用“数字”方案通过正交RB传输其量化梯度。针对单个FL轮次，我们将最优调度问题转化为最小化PS估计的全局梯度的均方误差（MSE）的问题，并在延迟约束下得到最优设备调度配置。

    Over-the-air (OTA) computation has recently emerged as a communication-efficient Federated Learning (FL) paradigm to train machine learning models over wireless networks. However, its performance is limited by the device with the worst SNR, resulting in fast yet noisy updates. On the other hand, allocating orthogonal resource blocks (RB) to individual devices via digital channels mitigates the noise problem, at the cost of increased communication latency. In this paper, we address this discrepancy and present ADFL, a novel Analog-Digital FL scheme: in each round, the parameter server (PS) schedules each device to either upload its gradient via the analog OTA scheme or transmit its quantized gradient over an orthogonal RB using the ``digital" scheme. Focusing on a single FL round, we cast the optimal scheduling problem as the minimization of the mean squared error (MSE) on the estimated global gradient at the PS, subject to a delay constraint, yielding the optimal device scheduling conf
    
[^85]: 在局部私有约束下的在线条件分布学习

    Online Distribution Learning with Local Private Constraints

    [https://arxiv.org/abs/2402.00315](https://arxiv.org/abs/2402.00315)

    在这项研究中，我们讨论了在局部差分隐私下具有无界标签集的在线条件分布估计问题。我们证明了在$(\epsilon,0)$-局部差分隐私的情况下，KL风险随着时间的增长速度为$\tilde{\Theta}(\frac{1}{\epsilon}\sqrt{KT})$，其中$K=|\mathcal{F}|$，这与有界标签集的情况形成明显的对比。

    

    我们研究了在局部差分隐私下具有无界标签集的在线条件分布估计问题。我们目标是以在线方式估计一个未知的函数$f\in \mathcal{F}$，在时间$t$提供了上下文$\boldsymbol{x}_t$时，我们可以只知道从$f(\boldsymbol{x}_t)$中取样的真实标签的私有化版本，生成一个$f(\boldsymbol{x}_t)$的KL散度估计。最终的目标是在有限时间$T$内最小化累积的KL风险。我们证明，在私有化标签的$(\epsilon,0)$-局部差分隐私下，KL风险增长的速度为$\tilde{\Theta}(\frac{1}{\epsilon}\sqrt{KT})$，其中$K=|\mathcal{F}|$，与Wu等人(2023a)对于有界标签集的$\tilde{\Theta}(\sqrt{T\log K})$界限形成鲜明对比。作为副产品，我们的结果还恢复出一个...

    We study the problem of online conditional distribution estimation with \emph{unbounded} label sets under local differential privacy. Let $\mathcal{F}$ be a distribution-valued function class with unbounded label set. We aim at estimating an \emph{unknown} function $f\in \mathcal{F}$ in an online fashion so that at time $t$ when the context $\boldsymbol{x}_t$ is provided we can generate an estimate of $f(\boldsymbol{x}_t)$ under KL-divergence knowing only a privatized version of the true labels sampling from $f(\boldsymbol{x}_t)$. The ultimate objective is to minimize the cumulative KL-risk of a finite horizon $T$. We show that under $(\epsilon,0)$-local differential privacy of the privatized labels, the KL-risk grows as $\tilde{\Theta}(\frac{1}{\epsilon}\sqrt{KT})$ upto poly-logarithmic factors where $K=|\mathcal{F}|$. This is in stark contrast to the $\tilde{\Theta}(\sqrt{T\log K})$ bound demonstrated by Wu et al. (2023a) for bounded label sets. As a byproduct, our results recover a 
    
[^86]: 控制随机环境中带延迟的问题：一种基于模型的强化学习方法

    Control in Stochastic Environment with Delays: A Model-based Reinforcement Learning Approach

    [https://arxiv.org/abs/2402.00313](https://arxiv.org/abs/2402.00313)

    这篇论文介绍了一种新的基于模型的强化学习方法，用于控制带有延迟反馈的随机环境中的问题，该方法采用随机规划，能够嵌入风险偏好，并在确定性转换问题中恢复最优策略。

    

    本文介绍了一种新的强化学习方法，用于带有延迟反馈的控制问题。具体而言，我们的方法采用了随机规划，而不是之前使用确定性规划的方法。这使得我们能够在策略优化问题中嵌入风险偏好。我们证明了该公式可以恢复具有确定性转换问题的最优策略。我们将我们的策略与文献中的两种先前方法进行对比。我们将该方法应用于简单任务，以了解其特点。然后，我们比较了在控制多个Atari游戏中的方法性能。

    In this paper we are introducing a new reinforcement learning method for control problems in environments with delayed feedback. Specifically, our method employs stochastic planning, versus previous methods that used deterministic planning. This allows us to embed risk preference in the policy optimization problem. We show that this formulation can recover the optimal policy for problems with deterministic transitions. We contrast our policy with two prior methods from literature. We apply the methodology to simple tasks to understand its features. Then, we compare the performance of the methods in controlling multiple Atari games.
    
[^87]: 使用无标签学习的地震走时层析成像

    Seismic Traveltime Tomography with Label-free Learning

    [https://arxiv.org/abs/2402.00310](https://arxiv.org/abs/2402.00310)

    这项研究提出了一种使用无标签学习的地震走时层析成像方法，该方法通过将深度学习和字典学习与传统的层析-最小二乘法相结合，来提高低分辨率的速度模型。

    

    近年来，深度学习技术在地震走时层析成像中被应用于构建速度模型（VMs），并显示出令人鼓舞的表现。然而，它们需要生成带标签的样本（即输入和标签的对应），以进行端到端学习的深度神经网络（NN）训练，而现实数据反演的真实标签通常缺失或非常昂贵。一些传统层析方法可以快速实施，但其效果通常受到先验假设的限制。为了避免生成带标签的样本，我们提出了一种新方法，通过将深度学习和字典学习与传统的层析-最小二乘法（LSQR）相结合，以提高低分辨率的VMs。我们首先设计了一种浅层简单的NN来降低计算成本，然后提出了一种两步策略来提高低分辨率的VMs：（1）预热阶段。通过字典学习，从LSQR估计中训练出初始字典。

    Deep learning techniques have been used to build velocity models (VMs) for seismic traveltime tomography and have shown encouraging performance in recent years. However, they need to generate labeled samples (i.e., pairs of input and label) to train the deep neural network (NN) with end-to-end learning, and the real labels for field data inversion are usually missing or very expensive. Some traditional tomographic methods can be implemented quickly, but their effectiveness is often limited by prior assumptions. To avoid generating labeled samples, we propose a novel method by integrating deep learning and dictionary learning to enhance the VMs with low resolution by using the traditional tomography-least square method (LSQR). We first design a type of shallow and simple NN to reduce computational cost followed by proposing a two-step strategy to enhance the VMs with low resolution: (1) Warming up. An initial dictionary is trained from the estimation by LSQR through dictionary learning 
    
[^88]: 一个准确且低参数的机器学习架构用于下一个位置的预测

    An Accurate and Low-Parameter Machine Learning Architecture for Next Location Prediction

    [https://arxiv.org/abs/2402.00306](https://arxiv.org/abs/2402.00306)

    本文提出了一个节能、小型且低参数的机器学习架构，用于准确预测用户的下一个位置。通过大量实验，成功将模型参数数量从2.02亿减少到200万，模型大小从791 MB减少到8 MB，训练时间减少了四倍。

    

    下一个位置的预测是一门涉及预测用户下一个位置的学科。其应用包括资源分配、服务质量、能源效率和交通管理。本文提出了一种节能、小型和低参数的机器学习（ML）架构，用于准确的下一个位置预测，可部署在普通基站和边缘设备上。为了实现这一目标，我们对一个整个城市的完整人员流动模式进行了一百个超参数实验，以确定一个精确的ML架构，其准确度达到了最少数量的模型参数的平台。我们成功地将已发表的ML架构的模型参数数量从2.02亿减少到200万。这将模型参数的总大小从791 MB减少到8 MB。此外，训练时间减少了四倍，训练所需的图形处理单元（GPU）内存量也减少了一个因素。

    Next location prediction is a discipline that involves predicting a users next location. Its applications include resource allocation, quality of service, energy efficiency, and traffic management. This paper proposes an energy-efficient, small, and low parameter machine learning (ML) architecture for accurate next location prediction, deployable on modest base stations and edge devices. To accomplish this we ran a hundred hyperparameter experiments on the full human mobility patterns of an entire city, to determine an exact ML architecture that reached a plateau of accuracy with the least amount of model parameters. We successfully achieved a reduction in the number of model parameters within published ML architectures from 202 million down to 2 million. This reduced the total size of the model parameters from 791 MB down to 8 MB. Additionally, this decreased the training time by a factor of four, the amount of graphics processing unit (GPU) memory needed for training by a factor of t
    
[^89]: 从儿童视角进行自监督学习的视频表示

    Self-supervised learning of video representations from a child's perspective

    [https://arxiv.org/abs/2402.00300](https://arxiv.org/abs/2402.00300)

    本研究从儿童的视角进行自监督学习，通过长时间的头戴式摄像记录训练视频模型，结果表明这些模型在促进从少量样本中学习行动概念方面非常有效。

    

    儿童通过几年的自我视觉经验学习到了强大的世界内部模型。这些内部模型能否通过儿童的视觉体验和通用的自监督学习算法来学习，还是需要强大的归纳偏差？最近，在收集大规模、纵向的发展现实视频数据集以及通用的自监督学习算法的进展使我们能够开始探讨这个本质与养育之间的问题。然而，现有的工作通常关注基于图像的自监督学习算法和可以从静态图像中学习的视觉能力（例如目标识别），从而忽略了世界的时间性质。为了弥合这一差距，我们在一个儿童早期发展阶段（6-31个月）从儿童的头戴式摄像记录中训练自监督视频模型。所得到的模型在促进从少量样本中学习行动概念方面非常有效。

    Children learn powerful internal models of the world around them from a few years of egocentric visual experience. Can such internal models be learned from a child's visual experience with highly generic learning algorithms or do they require strong inductive biases? Recent advances in collecting large-scale, longitudinal, developmentally realistic video datasets and generic self-supervised learning (SSL) algorithms are allowing us to begin to tackle this nature vs. nurture question. However, existing work typically focuses on image-based SSL algorithms and visual capabilities that can be learned from static images (e.g. object recognition), thus ignoring temporal aspects of the world. To close this gap, here we train self-supervised video models on longitudinal, egocentric headcam recordings collected from a child over a two year period in their early development (6-31 months). The resulting models are highly effective at facilitating the learning of action concepts from a small numbe
    
[^90]: 基于注意力机制的动态多层图神经网络用于贷款违约预测

    Attention-based Dynamic Multilayer Graph Neural Networks for Loan Default Prediction

    [https://arxiv.org/abs/2402.00299](https://arxiv.org/abs/2402.00299)

    该研究提出了一种基于注意力机制的动态多层图神经网络模型，用于信用风险评估和贷款违约预测，并考虑了借款人之间的关联和连接随时间的演变。

    

    传统的信用评分倾向于仅使用个体借款人或贷款级别的预测因素，然而长期以来，人们认识到借款人之间的关联可能会导致风险在网络上的传播。本文提出了一种利用由图神经网络和循环神经网络构建的动态多层网络的模型，用于信用风险评估，其中每一层反映了不同来源的网络连接。我们使用美国抵押贷款公司Freddie Mac提供的数据集，在行为信用评分的背景下测试了我们的方法，其中不同类型的连接源自借款人的地理位置和他们选择的抵押贷款提供商。所提出的模型考虑了这两种连接以及这些连接随时间的演变。我们通过使用自定义的注意力机制来增强模型，根据其重要性对不同的时间快照进行加权。经过多次配置测试后，

    Whereas traditional credit scoring tends to employ only individual borrower- or loan-level predictors, it has been acknowledged for some time that connections between borrowers may result in default risk propagating over a network. In this paper, we present a model for credit risk assessment leveraging a dynamic multilayer network built from a Graph Neural Network and a Recurrent Neural Network, each layer reflecting a different source of network connection. We test our methodology in a behavioural credit scoring context using a dataset provided by U.S. mortgage financier Freddie Mac, in which different types of connections arise from the geographical location of the borrower and their choice of mortgage provider. The proposed model considers both types of connections and the evolution of these connections over time. We enhance the model by using a custom attention mechanism that weights the different time snapshots according to their importance. After testing multiple configurations, 
    
[^91]: PAP-REC: 个性化自动提示的推荐语言模型

    PAP-REC: Personalized Automatic Prompt for Recommendation Language Model

    [https://arxiv.org/abs/2402.00284](https://arxiv.org/abs/2402.00284)

    本研究提出了PAP-REC框架，用于生成个性化自动提示的推荐语言模型。该框架通过自动生成个性化提示标记来减轻手动设计提示所带来的效率和效果问题。

    

    最近出现的基于提示的推荐语言模型（RLM）可以统一解决多个推荐任务。这些RLM充分利用了从丰富的预训练数据中学到的遗传知识，通过提示来解决下游推荐任务，而不需要引入额外的参数或网络训练。然而，手工设计的提示需要显著的专业知识和人力投入，稍微改写提示就可能导致性能的巨大变化。在本文中，我们提出了PAP-REC，一个用于生成个性化自动提示的推荐语言模型的框架，以缓解手动设计提示导致的低效率和低效果问题。具体而言，个性化自动提示允许不同的用户在相同任务中具有不同的提示标记，这些标记是使用梯度下降法自动生成的。个性化自动提示生成推荐语言模型的一个挑战是庞大的搜索空间。

    Recently emerged prompt-based Recommendation Language Models (RLM) can solve multiple recommendation tasks uniformly. The RLMs make full use of the inherited knowledge learned from the abundant pre-training data to solve the downstream recommendation tasks by prompts, without introducing additional parameters or network training. However, handcrafted prompts require significant expertise and human effort since slightly rewriting prompts may cause massive performance changes. In this paper, we propose PAP-REC, a framework to generate the Personalized Automatic Prompt for RECommendation language models to mitigate the inefficiency and ineffectiveness problems derived from manually designed prompts. Specifically, personalized automatic prompts allow different users to have different prompt tokens for the same task, automatically generated using a gradient-based method. One challenge for personalized automatic prompt generation for recommendation language models is the extremely large sear
    
[^92]: 使用向量空间和逆映射了解图像分析中神经网络系统的研究

    Understanding Neural Network Systems for Image Analysis using Vector Spaces and Inverse Maps

    [https://arxiv.org/abs/2402.00261](https://arxiv.org/abs/2402.00261)

    本文使用线性代数技术将神经网络层视为信号空间之间的映射，并引入了可逆网络的概念和计算产生特定输出的输入图像的算法。

    

    开发数学方法来理解图像分析中复杂的神经网络系统具有极大的兴趣。本文介绍了利用线性代数技术将神经网络层视为信号空间之间的映射的方法。首先，我们演示了如何使用信号空间来可视化权重空间和卷积层卷积核。其次，我们引入了可逆网络的概念和计算产生特定输出的输入图像的算法。我们在两个可逆网络和ResNet18上演示了我们的方法。

    There is strong interest in developing mathematical methods that can be used to understand complex neural networks used in image analysis. In this paper, we introduce techniques from Linear Algebra to model neural network layers as maps between signal spaces. First, we demonstrate how signal spaces can be used to visualize weight spaces and convolutional layer kernels. We also demonstrate how residual vector spaces can be used to further visualize information lost at each layer. Second, we introduce the concept of invertible networks and an algorithm for computing input images that yield specific outputs. We demonstrate our approach on two invertible networks and ResNet18.
    
[^93]: 多组学习的层次组模型

    Multi-group Learning for Hierarchical Groups

    [https://arxiv.org/abs/2402.00258](https://arxiv.org/abs/2402.00258)

    本研究将多组学习扩展到具有层次结构的情况，设计了一个近乎最优的样本复杂度的算法，输出可解释且确定性的决策树预测器，并在真实数据集上取得了有吸引力的广义化特性。

    

    多组学习模型将学习场景规范化为单一预测器在多个可能重叠的兴趣子组上必须广义化。我们将多组学习的研究扩展到了具有层次结构的自然情况。我们设计了一个算法，用于输出可解释且确定性的决策树预测器，具有近乎最优的样本复杂度。然后，我们对该算法进行经验评估，并发现它在具有层次组结构的真实数据集上具有有吸引力的广义化特性。

    The multi-group learning model formalizes the learning scenario in which a single predictor must generalize well on multiple, possibly overlapping subgroups of interest. We extend the study of multi-group learning to the natural case where the groups are hierarchically structured. We design an algorithm for this setting that outputs an interpretable and deterministic decision tree predictor with near-optimal sample complexity. We then conduct an empirical evaluation of our algorithm and find that it achieves attractive generalization properties on real datasets with hierarchical group structure.
    
[^94]: 通过深度策略梯度进行垂直符号回归

    Vertical Symbolic Regression via Deep Policy Gradient

    [https://arxiv.org/abs/2402.00254](https://arxiv.org/abs/2402.00254)

    通过使用深度策略梯度的垂直符号回归（VSR-DPG），我们能够从实验数据中发现涉及多个独立变量的符号方程，超过了以前的方法和变体。

    

    最近提出了垂直符号回归（VSR），用于从实验数据中快速发现具有多个独立变量的符号方程。VSR通过构建由涉及一部分独立变量的简化形式方程到完整方程的垂直发现路径来减少搜索空间。深度神经网络已经被许多符号回归器证明是成功的，预计能进一步扩大VSR的规模。然而，直接将VSR与深度神经网络结合将导致梯度传递困难和其他工程问题。我们提出了使用深度策略梯度的垂直符号回归（VSR-DPG），并证明了VSR-DPG可以恢复涉及多个输入变量的真实方程，显著超过基于深度强化学习的方法和先前的VSR变体。我们的VSR-DPG将符号回归建模为一个顺序决策过程，其中方程是通过多次应用来构建的。

    Vertical Symbolic Regression (VSR) recently has been proposed to expedite the discovery of symbolic equations with many independent variables from experimental data. VSR reduces the search spaces following the vertical discovery path by building from reduced-form equations involving a subset of independent variables to full-fledged ones. Proved successful by many symbolic regressors, deep neural networks are expected to further scale up VSR. Nevertheless, directly combining VSR with deep neural networks will result in difficulty in passing gradients and other engineering issues. We propose Vertical Symbolic Regression using Deep Policy Gradient (VSR-DPG) and demonstrate that VSR-DPG can recover ground-truth equations involving multiple input variables, significantly beyond both deep reinforcement learning-based approaches and previous VSR variants. Our VSR-DPG models symbolic regression as a sequential decision-making process, in which equations are built from repeated applications of 
    
[^95]: 大规模视觉-语言模型中的幻觉调查

    A Survey on Hallucination in Large Vision-Language Models

    [https://arxiv.org/abs/2402.00253](https://arxiv.org/abs/2402.00253)

    这份综合调查研究了大规模视觉-语言模型中的幻觉问题，澄清了幻觉的概念，提出了评估方法，并对幻觉的根本原因进行了调查。

    

    大规模视觉-语言模型（LVLMs）的发展引起了人工智能领域越来越多的关注，因为它具有实际的实施潜力。然而，“幻觉”，或者更具体地说，即视觉内容与相应文本生成之间的不一致，在利用LVLMs方面提出了重大挑战。在这份综合调查中，我们对LVLM相关的幻觉进行了深入剖析，旨在建立一个概览并促进未来的缓解。我们首先澄清了LVLMs中幻觉概念，呈现了各种幻觉症状，并强调了LVLM幻觉固有的独特挑战。随后，我们概述了专门用于评估LVLM独特幻觉的基准和方法论。此外，我们深入调查了这些幻觉的根本原因，包括来自训练数据和模型组件的见解。我们还对现有的幻觉缓解方法进行了批判性的回顾。

    Recent development of Large Vision-Language Models (LVLMs) has attracted growing attention within the AI landscape for its practical implementation potential. However, ``hallucination'', or more specifically, the misalignment between factual visual content and corresponding textual generation, poses a significant challenge of utilizing LVLMs. In this comprehensive survey, we dissect LVLM-related hallucinations in an attempt to establish an overview and facilitate future mitigation. Our scrutiny starts with a clarification of the concept of hallucinations in LVLMs, presenting a variety of hallucination symptoms and highlighting the unique challenges inherent in LVLM hallucinations. Subsequently, we outline the benchmarks and methodologies tailored specifically for evaluating hallucinations unique to LVLMs. Additionally, we delve into an investigation of the root causes of these hallucinations, encompassing insights from the training data and model components. We also critically review e
    
[^96]: 高效的非参数化不确定性量化方法用于黑盒大型语言模型和决策规划

    Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning

    [https://arxiv.org/abs/2402.00251](https://arxiv.org/abs/2402.00251)

    本文提出了一种高效的非参数化不确定性量化方法，用于黑盒大型语言模型和决策规划。该方法可以有效地估计输入-决策之间的逐点依赖关系，并提供统计上对决策可信度的解释。另外，本文还提出了一个系统化的决策代理设计，根据用户提示生成动作，并在存在多个高估计逐点依赖性的动作时要求用户提供偏好。

    

    大型语言模型(LLMs)的逐步决策规划在人工智能代理的发展中受到关注。本文主要研究带有不确定性估计的决策规划，以解决语言模型中的幻觉问题。现有方法要么是白盒方法，要么是计算复杂，限制了黑盒专有LLMs的使用。本文的第一个贡献是一种非参数化的LLMs不确定性量化方法，通过单一推理有效地估计输入-决策之间的逐点依赖关系，而不需要访问令牌logits。该估计器提供了对决策可信度的统计解释。第二个贡献是一个系统化的决策代理设计，根据用户提示如“打开浴室灯”，生成动作。当有多个动作的估计逐点依赖性都很高时，用户将被要求提供偏好。总结地说，我们的未参数化不确定性量化方法提供了一种高效的决策规划方法，可以在黑盒大型语言模型中应用。

    Step-by-step decision planning with large language models (LLMs) is gaining attention in AI agent development. This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models. Existing approaches are either white-box or computationally demanding, limiting use of black-box proprietary LLMs within budgets. The paper's first contribution is a non-parametric uncertainty quantification method for LLMs, efficiently estimating point-wise dependencies between input-decision on the fly with a single inference, without access to token logits. This estimator informs the statistical interpretation of decision trustworthiness. The second contribution outlines a systematic design for a decision-making agent, generating actions like ``turn on the bathroom light'' based on user prompts such as ``take a bath''. Users will be asked to provide preferences when more than one action has high estimated point-wise dependencies. In conclusion, our un
    
[^97]: 循环卷积层和零填充卷积层的谱范数

    Spectral Norm of Convolutional Layers with Circular and Zero Paddings

    [https://arxiv.org/abs/2402.00240](https://arxiv.org/abs/2402.00240)

    本文介绍了一种使用Gram迭代计算谱范数的方法，其中包括循环卷积层和零填充卷积层的应用。通过引入谱重缩放方法，我们提供了一种增强神经网络鲁棒性的解决方案。实验证明，该方法在精度、计算成本和可扩展性方面优于现有技术。

    

    本文利用了一种高效、确定性且可微的方法——Gram迭代来计算谱范数，并保证了上界。针对循环卷积层，我们将Gram迭代的使用推广到零填充卷积层，并证明了其二次收敛性。我们还提供了一些定理来弥合循环卷积和零填充卷积的谱范数之间的差距。我们设计了一种谱重缩放方法，可以作为一种竞争性的1-Lipschitz层，增强神经网络的鲁棒性。通过实验证明，我们的方法在精度、计算成本和可扩展性方面优于现有技术。实验代码可在https://github.com/blaisedelattre/lip4conv获取。

    This paper leverages the use of \emph{Gram iteration} an efficient, deterministic, and differentiable method for computing spectral norm with an upper bound guarantee. Designed for circular convolutional layers, we generalize the use of the Gram iteration to zero padding convolutional layers and prove its quadratic convergence. We also provide theorems for bridging the gap between circular and zero padding convolution's spectral norm. We design a \emph{spectral rescaling} that can be used as a competitive $1$-Lipschitz layer that enhances network robustness. Demonstrated through experiments, our method outperforms state-of-the-art techniques in precision, computational cost, and scalability. The code of experiments is available at https://github.com/blaisedelattre/lip4conv.
    
[^98]: 互联网生物纳米物联网和数字孪生技术赋能生物技术行业的CNN-FL

    CNN-FL for Biotechnology Industry Empowered by Internet-of-BioNano Things and Digital Twins

    [https://arxiv.org/abs/2402.00238](https://arxiv.org/abs/2402.00238)

    本论文提出了一种集成互联网生物纳米物联网和卷积神经网络(CNN)与联邦学习(FL)的框架，可以有效应对微观和纳米尺度上数字孪生的挑战，并对生物技术行业进行赋能。

    

    数字孪生技术(DTs)通过实现生物资产、微生物、药物开发过程和数字健康应用的复杂数字化表示，正在革命性地改变生物技术行业。然而，微观和纳米尺度上的数字孪生，特别是对细菌等复杂实体进行建模，面临着在需要先进的物联网基础设施和计算方法来实现更高的准确性和可扩展性等挑战。在这项工作中，我们提出了一个新的框架，将互联网生物纳米物联网(IoBNT)与先进的机器学习技术，特别是卷积神经网络(CNN)和联邦学习(FL)相结合，以有效应对所识别的挑战。在我们的框架中，IoBNT设备被部署在各种物理环境中，收集基于图像的生物数据，充分利用CNN在机器视觉和模式识别方面的强大能力。

    Digital twins (DTs) are revolutionizing the biotechnology industry by enabling sophisticated digital representations of biological assets, microorganisms, drug development processes, and digital health applications. However, digital twinning at micro and nano scales, particularly in modeling complex entities like bacteria, presents significant challenges in terms of requiring advanced Internet of Things (IoT) infrastructure and computing approaches to achieve enhanced accuracy and scalability. In this work, we propose a novel framework that integrates the Internet of Bio-Nano Things (IoBNT) with advanced machine learning techniques, specifically convolutional neural networks (CNN) and federated learning (FL), to effectively tackle the identified challenges. Within our framework, IoBNT devices are deployed to gather image-based biological data across various physical environments, leveraging the strong capabilities of CNNs for robust machine vision and pattern recognition. Subsequently,
    
[^99]: 位置编码有助于循环神经网络处理大词汇量

    Positional Encoding Helps Recurrent Neural Networks Handle a Large Vocabulary

    [https://arxiv.org/abs/2402.00236](https://arxiv.org/abs/2402.00236)

    本研究探讨了位置编码在循环神经网络中的作用，发现即使与RNN结合使用，位置编码仍然有效，尤其是在处理大词汇量和多样观察结果时。这为使用输入驱动和自主时间表示的组合研究提供了新的方向，同时研究结果也对神经元振荡的生物学意义提供了讨论。

    

    本研究讨论了位置编码在利用合成基准测试的循环神经网络（RNN）中的影响。位置编码将时间序列中的数据点“时间戳化”，并补充了Transformer神经网络的能力，后者缺乏表示数据顺序的内在机制。相反，RNN可以自己对数据点进行时间编码，使得它们对位置编码的使用似乎是“冗余”的。然而，经验研究表明，即使与RNN结合使用，位置编码的有效性仍然很高，特别是用于处理产生多样观察结果的大词汇量。这些发现为循环神经网络上的新的研究方向铺平了道路，涉及输入驱动和自主时间表示的组合。此外，本研究还讨论了计算/模拟结果的生物学意义，考虑到位置编码的正弦实现与神经元振荡之间的关联。

    This study discusses the effects of positional encoding on recurrent neural networks (RNNs) utilizing synthetic benchmarks. Positional encoding "time-stamps" data points in time series and complements the capabilities of Transformer neural networks, which lack an inherent mechanism for representing the data order. By contrast, RNNs can encode the temporal information of data points on their own, rendering their use of positional encoding seemingly "redundant". Nonetheless, empirical investigations reveal the effectiveness of positional encoding even when coupled with RNNs, specifically for handling a large vocabulary that yields diverse observations. These findings pave the way for a new line of research on RNNs, concerning the combination of input-driven and autonomous time representation. Additionally, biological implications of the computational/simulational results are discussed, in the light of the affinity between the sinusoidal implementation of positional encoding and neural os
    
[^100]: 生成式AI系统能否支持患者的信息需求？

    Are Generative AI systems Capable of Supporting Information Needs of Patients?

    [https://arxiv.org/abs/2402.00234](https://arxiv.org/abs/2402.00234)

    生成式AI系统被应用于支持患者信息需求的研究中，以提高患者对放射学数据的理解和管理能力。通过与患者和医疗专家的对话分析，我们确定了常见的医学信息需求和问题。

    

    患有复杂疾病如癌症的患者面临复杂的信息挑战，他们不仅需要了解他们的疾病，还需要学会如何管理它。与医疗专家（放射科医师、肿瘤科医师）密切互动可以提高患者的学习能力，从而改善疾病预后。然而，这种方法资源密集且占用了专家的时间，使他们无法完成其他关键任务。鉴于生成式AI模型在改进医疗系统方面的最新进展，我们的工作研究了生成式视觉问答系统在放射学成像数据背景下如何负责任地支持患者的信息需求。我们进行了一项形成性需求发现研究，参与者讨论了一个虚构近亲的胸部计算机断层扫描（CT）图像和相关的放射学报告。通过对参与者和医疗专家之间的对话的主题分析，我们确定常见的医学信息需求和问题。

    Patients managing a complex illness such as cancer face a complex information challenge where they not only must learn about their illness but also how to manage it. Close interaction with healthcare experts (radiologists, oncologists) can improve patient learning and thereby, their disease outcome. However, this approach is resource intensive and takes expert time away from other critical tasks. Given the recent advancements in Generative AI models aimed at improving the healthcare system, our work investigates whether and how generative visual question answering systems can responsibly support patient information needs in the context of radiology imaging data. We conducted a formative need-finding study in which participants discussed chest computed tomography (CT) scans and associated radiology reports of a fictitious close relative with a cardiothoracic radiologist. Using thematic analysis of the conversation between participants and medical experts, we identified commonly occurrin
    
[^101]: 使用有监督对比学习学习标签层次结构

    Learning Label Hierarchy with Supervised Contrastive Learning

    [https://arxiv.org/abs/2402.00232](https://arxiv.org/abs/2402.00232)

    本文介绍了一种将标签层次结构融入有监督对比学习的方法，通过调整实例之间的距离和引入额外的对比，创建一个更为良好结构化和有区分性的特征空间。

    

    有监督对比学习（SCL）框架将每个类别视为独立的，因此认为所有类别同等重要。这忽略了标签层次存在的一般情况，即同一类别下的细粒度类别之间比非常不同的类别更相似。本文引入了一系列标签感知的SCL方法（LASCL），通过利用类别之间的相似性将层次信息融入SCL中，从而创建一个更为良好结构化和有区分性的特征空间。这是通过先根据类别之间的接近程度调整实例之间的距离来实现的，通过缩放的实例-实例对比。引入了一个额外的实例-中心对比，将同类别的示例移动到它们的中心附近，中心由一组可学习的标签参数表示。学习到的标签参数可以直接用作最近邻分类器，无需进一步微调。

    Supervised contrastive learning (SCL) frameworks treat each class as independent and thus consider all classes to be equally important. This neglects the common scenario in which label hierarchy exists, where fine-grained classes under the same category show more similarity than very different ones. This paper introduces a family of Label-Aware SCL methods (LASCL) that incorporates hierarchical information to SCL by leveraging similarities between classes, resulting in creating a more well-structured and discriminative feature space. This is achieved by first adjusting the distance between instances based on measures of the proximity of their classes with the scaled instance-instance-wise contrastive. An additional instance-center-wise contrastive is introduced to move within-class examples closer to their centers, which are represented by a set of learnable label parameters. The learned label parameters can be directly used as a nearest neighbor classifier without further finetuning. 
    
[^102]: 揭示城市中重叠社区的本质

    Uncover the nature of overlapping community in cities

    [https://arxiv.org/abs/2402.00222](https://arxiv.org/abs/2402.00222)

    本研究通过分析明尼苏达州双城地区的移动电话定位数据，揭示了城市社区中重叠的复杂本质，并发现其与收入和种族指标之间的显著相关性，解开了美国城市中复杂的隔离模式。

    

    城市空间虽然常被视为离散社区，但实际上被不同功能和社会群体共享。本研究引入了一种基于图形的物理感知深度学习框架，揭示了城市社区中错综复杂的重叠本质。通过对美国明尼苏达州双城大都市区（TCMA）的个人移动电话定位数据进行分析，我们的研究发现，95.7％的城市功能复杂性源于工作日社区的重叠结构。更重要的是，我们的研究不仅量化了这些重叠，还揭示了它们与收入和种族指标之间的显著相关性，解开了美国城市中复杂的隔离模式。作为第一个阐明城市社区重叠本质的研究，本工作提供了一种独特的地理空间视角观察城市结构，突显了城市内社会经济动力的微妙相互作用。

    Urban spaces, though often perceived as discrete communities, are shared by various functional and social groups. Our study introduces a graph-based physics-aware deep learning framework, illuminating the intricate overlapping nature inherent in urban communities. Through analysis of individual mobile phone positioning data at Twin Cities metro area (TCMA) in Minnesota, USA, our findings reveal that 95.7 % of urban functional complexity stems from the overlapping structure of communities during weekdays. Significantly, our research not only quantifies these overlaps but also reveals their compelling correlations with income and racial indicators, unraveling the complex segregation patterns in U.S. cities. As the first to elucidate the overlapping nature of urban communities, this work offers a unique geospatial perspective on looking at urban structures, highlighting the nuanced interplay of socioeconomic dynamics within cities.
    
[^103]: FedCore: 使用分布式核心集解决无拖车现象的联邦学习

    FedCore: Straggler-Free Federated Learning with Distributed Coresets

    [https://arxiv.org/abs/2402.00219](https://arxiv.org/abs/2402.00219)

    FedCore是一种通过分布式选择核心集解决联邦学习中慢速客户端问题的算法，可以显著减少训练时间，并保护隐私。

    

    联邦学习（FL）是一种机器学习范例，允许多个客户端在保留自己的数据的前提下，共同训练一个共享模型。然而，由于慢速客户端，拖车现象经常影响FL的效率和可扩展性。本文提出了FedCore，一种通过分布式选择核心集（数据集的代表子集）创新地解决拖车问题的算法。与现有的集中式核心集方法不同，FedCore以分布式方式直接在每个客户端上创建核心集，确保在FL中保护隐私。FedCore将核心集优化问题转化为更易处理的k-medoids聚类问题，并在每个客户端上进行分布式操作。理论分析证实了FedCore的收敛性，实际评估显示FL训练时间减少了8倍，而模型准确性没有降低。我们广泛的评估还表明，FedCore对现有的FL框架具有很好的泛化性。

    Federated learning (FL) is a machine learning paradigm that allows multiple clients to collaboratively train a shared model while keeping their data on-premise. However, the straggler issue, due to slow clients, often hinders the efficiency and scalability of FL. This paper presents FedCore, an algorithm that innovatively tackles the straggler problem via the decentralized selection of coresets, representative subsets of a dataset. Contrary to existing centralized coreset methods, FedCore creates coresets directly on each client in a distributed manner, ensuring privacy preservation in FL. FedCore translates the coreset optimization problem into a more tractable k-medoids clustering problem and operates distributedly on each client. Theoretical analysis confirms FedCore's convergence, and practical evaluations demonstrate an 8x reduction in FL training time, without compromising model accuracy. Our extensive evaluations also show that FedCore generalizes well to existing FL frameworks.
    
[^104]: MP-SL：多跳并行分割学习

    MP-SL: Multihop Parallel Split Learning

    [https://arxiv.org/abs/2402.00208](https://arxiv.org/abs/2402.00208)

    引入了多跳并行分割学习（MP-SL）来缓解联邦学习（FL）中处理异构设备和大量参数的挑战。该方法允许多个资源受限设备积极参与协作训练过程，并减少计算节点的内存需求。

    

    联邦学习（FL）作为一种广泛采用的协议，可以在保持分散化数据的同时促进机器学习（ML）模型的训练。然而，当处理不同参与设备的异构集时，会遇到挑战，导致训练过程中出现延迟，特别是在资源有限的设备中。此外，使用大量参数进行ML模型训练需要计算和内存资源超出小设备（例如移动设备和物联网设备）的能力。为了解决这些问题，引入了一些技术，如并行分割学习（SL），允许多个资源受限设备在有资源的计算节点的帮助下积极参与协作训练过程。然而，并行SL的缺点是需要在计算节点上分配大量内存，例如使用100个参与者训练VGG-19需要80 GB的内存。在本文中，我们介绍了多跳并行分割学习（MP-SL），该方法可以减少计算节点的内存需求。

    Federated Learning (FL) stands out as a widely adopted protocol facilitating the training of Machine Learning (ML) models while maintaining decentralized data. However, challenges arise when dealing with a heterogeneous set of participating devices, causing delays in the training process, particularly among devices with limited resources. Moreover, the task of training ML models with a vast number of parameters demands computing and memory resources beyond the capabilities of small devices, such as mobile and Internet of Things (IoT) devices. To address these issues, techniques like Parallel Split Learning (SL) have been introduced, allowing multiple resource-constrained devices to actively participate in collaborative training processes with assistance from resourceful compute nodes. Nonetheless, a drawback of Parallel SL is the substantial memory allocation required at the compute nodes, for instance training VGG-19 with 100 participants needs 80 GB. In this paper, we introduce Multi
    
[^105]: 分散、协作和保护隐私的多医院数据机器学习

    Decentralised, Collaborative, and Privacy-preserving Machine Learning for Multi-Hospital Data

    [https://arxiv.org/abs/2402.00205](https://arxiv.org/abs/2402.00205)

    本文提出了一种用于多医院数据的分散、协作和保护隐私的机器学习方法（DeCaPH），它可以允许不同方在不共享私有数据集的情况下协作训练机器学习模型，并通过限制数据泄露和隐私侵犯来保护患者隐私。

    

    机器学习在医疗数据分析方面展现出巨大潜力。来自不同源头和环境的大型数据集对于医疗保健领域的机器学习模型实现更高的准确性和泛化能力至关重要。由于复杂且多变的隐私和监管要求，跨不同医疗机构共享数据是具有挑战性的。因此，允许多个方参与合作训练机器学习模型，在不直接共享数据集或通过合作损害数据集隐私的情况下利用各方现有的私有数据集具有困难但至关重要。在本文中，我们通过提出用于多医院数据的分散、协作和保护隐私的机器学习方法（DeCaPH）来解决这个问题。它具有以下关键优点：（1）允许不同方在不传输私有数据集的情况下协作训练机器学习模型；（2）通过限制潜在的数据泄露和隐私侵犯来保护患者隐私。

    Machine Learning (ML) has demonstrated its great potential on medical data analysis. Large datasets collected from diverse sources and settings are essential for ML models in healthcare to achieve better accuracy and generalizability. Sharing data across different healthcare institutions is challenging because of complex and varying privacy and regulatory requirements. Hence, it is hard but crucial to allow multiple parties to collaboratively train an ML model leveraging the private datasets available at each party without the need for direct sharing of those datasets or compromising the privacy of the datasets through collaboration. In this paper, we address this challenge by proposing Decentralized, Collaborative, and Privacy-preserving ML for Multi-Hospital Data (DeCaPH). It offers the following key benefits: (1) it allows different parties to collaboratively train an ML model without transferring their private datasets; (2) it safeguards patient privacy by limiting the potential pr
    
[^106]: 用逻辑回归进行特征选择的实验

    An Experiment on Feature Selection using Logistic Regression

    [https://arxiv.org/abs/2402.00201](https://arxiv.org/abs/2402.00201)

    这篇论文探讨了一种基于逻辑回归的特征选择方法，通过综合L1和L2正则化的结果，对CIC-IDS2018数据集进行了实验。没有发现显著结果。

    

    在监督机器学习中，特征选择在提高可解释性和以计算时间和准确性相关度量为衡量标准方面起着非常重要的作用。本文基于逻辑回归（LR）中广为人知的L1和L2正则化策略，研究了一种特征选择方法。众所周知，学到的系数可以用作权重来对特征进行排序。我们的方法是综合L1和L2正则化的结果。在我们的实验中，我们选择了CIC-IDS2018数据集，部分原因是其规模，同时因为存在两个难以区分的问题类别。我们首先排除其中一个问题类别，然后再包含它。我们先通过L1进行特征排序，然后再通过L2进行特征排序，并通过改变两个排序中特征集的大小来比较带有L1的逻辑回归（LR+L1）和带有L2的逻辑回归（LR+L2）。我们没有发现显著的结果。

    In supervised machine learning, feature selection plays a very important role by potentially enhancing explainability and performance as measured by computing time and accuracy-related metrics. In this paper, we investigate a method for feature selection based on the well-known L1 and L2 regularization strategies associated with logistic regression (LR). It is well known that the learned coefficients, which serve as weights, can be used to rank the features. Our approach is to synthesize the findings of L1 and L2 regularization. For our experiment, we chose the CIC-IDS2018 dataset owing partly to its size and also to the existence of two problematic classes that are hard to separate. We report first with the exclusion of one of them and then with its inclusion. We ranked features first with L1 and then with L2, and then compared logistic regression with L1 (LR+L1) against that with L2 (LR+L2) by varying the sizes of the feature sets for each of the two rankings. We found no significant
    
[^107]: 通过机器分类表面增强拉曼光谱确定微量有机污染物浓度

    Determination of Trace Organic Contaminant Concentration via Machine Classification of Surface-Enhanced Raman Spectra

    [https://arxiv.org/abs/2402.00197](https://arxiv.org/abs/2402.00197)

    本研究通过机器学习的方法，利用表面增强拉曼光谱（SERS）从未经处理的数据中预测微量有机污染物的浓度。使用频域变换方法对三种模拟微污染物的拉曼光谱进行处理，并训练机器分类器，以解决SERS分析中的困难。

    

    准确检测和分析水中微量持久性有机污染物对于环境监测和食品质量控制等许多领域都是重要的，因为它们具有较长的环境稳定性和潜在的生物积累性。传统的有机污染物分析需要昂贵的设备，而表面增强拉曼光谱（SERS）已经展示了准确检测这些污染物的巨大潜力。然而，SERS分析中的困难，如光谱预处理、去噪和基质引起的光谱变化，阻碍了该技术的广泛应用。在这里，我们通过机器学习的方法，展示了一种从凌乱的、未经处理的拉曼数据中预测样品污染物浓度的方法。频域变换方法，包括傅立叶变换和Walsh-Hadamard变换，被应用于三个模拟微污染物（罗丹明6G、氯氰菊酯和三氯生）在水中的拉曼光谱集合，然后用于训练机器分类器来预测污染物的浓度。

    Accurate detection and analysis of traces of persistent organic pollutants in water is important in many areas, including environmental monitoring and food quality control, due to their long environmental stability and potential bioaccumulation. While conventional analysis of organic pollutants requires expensive equipment, surface enhanced Raman spectroscopy (SERS) has demonstrated great potential for accurate detection of these contaminants. However, SERS analytical difficulties, such as spectral preprocessing, denoising, and substrate-based spectral variation, have hindered widespread use of the technique. Here, we demonstrate an approach for predicting the concentration of sample pollutants from messy, unprocessed Raman data using machine learning. Frequency domain transform methods, including the Fourier and Walsh Hadamard transforms, are applied to sets of Raman spectra of three model micropollutants in water (rhodamine 6G, chlorpyrifos, and triclosan), which are then used to tra
    
[^108]: 数据集精简驱动的机器遗忘

    Dataset Condensation Driven Machine Unlearning

    [https://arxiv.org/abs/2402.00195](https://arxiv.org/abs/2402.00195)

    这篇论文通过引入数据集精简作为机器遗忘的重要组成部分，在图像分类中解决了持久的计算挑战，并改进了近似遗忘的计算复杂性。

    

    数据监管要求和注重隐私保护的机器学习当前的趋势强调了机器遗忘的重要性。通过重新训练忘记样本的补集来遗忘训练数据的朴素方法容易受到计算挑战的影响。这些挑战已经通过一系列属于机器遗忘范畴的技术得到了有效解决。然而，在处理持久性计算挑战与未遗忘模型的实用性和隐私性之间仍然存在不足。我们将其归因于从训练数据集的角度改进近似遗忘的计算复杂性方面的工作不足。在本文中，我们旨在填补这一空白，引入数据集精简作为机器遗忘在图像分类上的重要组成部分。为了实现这个目标，我们提出了新的数据集精简技术和创新的遗忘方案。

    The current trend in data regulation requirements and privacy-preserving machine learning has emphasized the importance of machine unlearning. The naive approach to unlearning training data by retraining over the complement of the forget samples is susceptible to computational challenges. These challenges have been effectively addressed through a collection of techniques falling under the umbrella of machine unlearning. However, there still exists a lack of sufficiency in handling persistent computational challenges in harmony with the utility and privacy of unlearned model. We attribute this to the lack of work on improving the computational complexity of approximate unlearning from the perspective of the training dataset. In this paper, we aim to fill this gap by introducing dataset condensation as an essential component of machine unlearning in the context of image classification. To achieve this goal, we propose new dataset condensation techniques and an innovative unlearning schem
    
[^109]: 对抗性量子机器学习：信息论的泛化分析

    Adversarial Quantum Machine Learning: An Information-Theoretic Generalization Analysis

    [https://arxiv.org/abs/2402.00176](https://arxiv.org/abs/2402.00176)

    本文研究了对抗性训练的量子分类器的泛化特性，并提出了新颖的信息论上界。

    

    类似于经典分类器，量子分类器也容易受到扰动其输入的对抗性攻击。一种有希望的对策是采用一个攻击感知或对抗性的损失函数来训练量子分类器。本文研究了针对有界范数白盒攻击进行对抗性训练的量子分类器的泛化特性。具体来说，量子对手通过将输入状态ρ(x)转化为与原始状态ρ(x)在p-Schatten距离上ε接近的状态λ来最大化分类器的损失。在量子嵌入ρ(x)的适当假设下，我们对对抗性训练的量子分类器在p = 1和p = ∞时的泛化误差导出了新颖的信息论上界。导出的上界包含两个项：第一个是经典数据和量子嵌入之间的2-Rényi相互信息的指数函数，

    In a manner analogous to their classical counterparts, quantum classifiers are vulnerable to adversarial attacks that perturb their inputs. A promising countermeasure is to train the quantum classifier by adopting an attack-aware, or adversarial, loss function. This paper studies the generalization properties of quantum classifiers that are adversarially trained against bounded-norm white-box attacks. Specifically, a quantum adversary maximizes the classifier's loss by transforming an input state $\rho(x)$ into a state $\lambda$ that is $\epsilon$-close to the original state $\rho(x)$ in $p$-Schatten distance. Under suitable assumptions on the quantum embedding $\rho(x)$, we derive novel information-theoretic upper bounds on the generalization error of adversarially trained quantum classifiers for $p = 1$ and $p = \infty$. The derived upper bounds consist of two terms: the first is an exponential function of the 2-R\'enyi mutual information between classical data and quantum embedding,
    
[^110]: 使用替代结果进行连续治疗效果研究

    Continuous Treatment Effects with Surrogate Outcomes

    [https://arxiv.org/abs/2402.00168](https://arxiv.org/abs/2402.00168)

    本文研究了在部分缺失主要结果的情况下，使用替代结果来估计连续治疗效果，并提出了一种双重稳健方法，通过使用标记和未标记数据，可以有效地纳入替代结果并避免选择偏误问题。该方法的估计值渐近正态性，并在方差方面可能比仅使用标记数据的方法有所改进。模拟实验证明了该方法的良好实证性能。

    

    在许多实际因果推断应用中，主要结果（标签）常常是部分缺失的，特别是如果它们很昂贵或很难收集。如果缺失依赖于协变量（即缺失不完全随机），仅基于完全观测样本的分析可能存在偏误。在这种情况下，结合与主要结果相关的完全观测的治疗后变量（替代结果）可以改进估计。在本文中，我们研究了替代结果在估计连续治疗效果中的作用，并提出了一种双重稳健方法，以高效地将替代结果纳入分析中，该方法使用了标记和未标记数据，并且不会受到上述选择偏误问题的影响。重要的是，我们建立了所提估计器的渐近正态性，并展示了与仅使用标记数据的方法相比，方差的可能改进。广泛的模拟显示我们的方法具有吸引人的经验性能。

    In many real-world causal inference applications, the primary outcomes (labels) are often partially missing, especially if they are expensive or difficult to collect. If the missingness depends on covariates (i.e., missingness is not completely at random), analyses based on fully-observed samples alone may be biased. Incorporating surrogates, which are fully observed post-treatment variables related to the primary outcome, can improve estimation in this case. In this paper, we study the role of surrogates in estimating continuous treatment effects and propose a doubly robust method to efficiently incorporate surrogates in the analysis, which uses both labeled and unlabeled data and does not suffer from the above selection bias problem. Importantly, we establish asymptotic normality of the proposed estimator and show possible improvements on the variance compared with methods that solely use labeled data. Extensive simulations show our methods enjoy appealing empirical performance.
    
[^111]: 政策梯度探索背后的神话

    Behind the Myth of Exploration in Policy Gradients

    [https://arxiv.org/abs/2402.00162](https://arxiv.org/abs/2402.00162)

    本论文提出了对政策梯度算法中探索项的新分析方法，区分了其平滑学习目标和增加梯度估计的两种不同作用。同时，详细讨论和实证了基于熵奖励的探索策略的局限性，并开辟了未来对这些策略设计和分析的研究方向。

    

    政策梯度算法是解决具有连续状态和动作空间的控制问题的有效强化学习方法。为了计算接近最优的策略，在实践中必须在学习目标中包含探索项。尽管这些项的有效性通常通过对探索环境的内在需求进行证明，但我们提出了一种新的分析方法，区分了这些技术的两种不同含义。首先，它们使得平滑学习目标成为可能，并在保持全局最大值的同时消除了局部最优解。其次，它们修改了梯度估计，增加了随机参数更新最终提供最优策略的概率。基于这些效应，我们讨论并实证了基于熵奖励的探索策略，突出了其局限性，并为设计和分析这些策略的未来研究开辟了新方向。

    Policy-gradient algorithms are effective reinforcement learning methods for solving control problems with continuous state and action spaces. To compute near-optimal policies, it is essential in practice to include exploration terms in the learning objective. Although the effectiveness of these terms is usually justified by an intrinsic need to explore environments, we propose a novel analysis and distinguish two different implications of these techniques. First, they make it possible to smooth the learning objective and to eliminate local optima while preserving the global maximum. Second, they modify the gradient estimates, increasing the probability that the stochastic parameter update eventually provides an optimal policy. In light of these effects, we discuss and illustrate empirically exploration strategies based on entropy bonuses, highlighting their limitations and opening avenues for future works in the design and analysis of such strategies.
    
[^112]: 使用超分辨率生成对抗网络增加地震数据采样频率的全数据驱动模型

    Fully Data-Driven Model for Increasing Sampling Rate Frequency of Seismic Data using Super-Resolution Generative Adversarial Networks

    [https://arxiv.org/abs/2402.00153](https://arxiv.org/abs/2402.00153)

    本研究首次应用了超分辨率生成对抗网络（SRGANs）来提高地震工程中传感器网络获取的时间历史数据的分辨率，从而增加了数据采样频率，提高了地震工程应用的准确性和可靠性。

    

    高质量的数据是任何工程应用的关键要求之一。在地震工程实践中，准确的数据对于预测结构响应或在结构健康监测（SHM）应用中进行损伤检测过程至关重要且不确定性较小。然而，获取高分辨率数据面临诸多挑战，如昂贵的成本、庞大的数据通道和巨大的存储需求。为了应对这些挑战，本研究采用超分辨率生成对抗网络（SRGANs）改善时间历史数据的分辨率，例如SHM应用中传感器网络获取的数据，这是首次在地震工程领域应用SRGANs。时间序列数据被转换为RGB值，将原始数据转换为图像。然后使用SRGANs来提升这些低分辨率图像，从而增强整体传感器分辨率。这种方法不仅有潜力减少数据通道数量，还可以提高地震数据采样频率，从而增加地震工程应用的准确性和可靠性。

    High-quality data is one of the key requirements for any engineering application. In earthquake engineering practice, accurate data is pivotal in predicting the response of structure or damage detection process in an Structural Health Monitoring (SHM) application with less uncertainty. However, obtaining high-resolution data is fraught with challenges, such as significant costs, extensive data channels, and substantial storage requirements. To address these challenges, this study employs super-resolution generative adversarial networks (SRGANs) to improve the resolution of time-history data such as the data obtained by a sensor network in an SHM application, marking the first application of SRGANs in earthquake engineering domain. The time-series data are transformed into RGB values, converting raw data into images. SRGANs are then utilized to upscale these low-resolution images, thereby enhancing the overall sensor resolution. This methodology not only offers potential reductions in d
    
[^113]: 更深还是更宽: 从Sobolev损失的最优泛化误差角度看

    Deeper or Wider: A Perspective from Optimal Generalization Error with Sobolev Loss

    [https://arxiv.org/abs/2402.00152](https://arxiv.org/abs/2402.00152)

    本文比较了更深的神经网络和更宽的神经网络在Sobolev损失的最优泛化误差方面的表现，研究发现神经网络的架构受多种因素影响，参数数量更多倾向于选择更宽的网络，而样本点数量和损失函数规则性更高倾向于选择更深的网络。

    

    构建神经网络的架构是机器学习界一个具有挑战性的追求，到底是更深还是更宽一直是一个持续的问题。本文探索了更深的神经网络（DeNNs）和具有有限隐藏层的更宽的神经网络（WeNNs）在Sobolev损失的最优泛化误差方面的比较。通过分析研究发现，神经网络的架构可以受到多种因素的显著影响，包括样本点的数量，神经网络内的参数以及损失函数的规则性。具体而言，更多的参数倾向于选择WeNNs，而更多的样本点和更高的损失函数规则性倾向于选择DeNNs。最后，我们将这个理论应用于使用深度Ritz和物理感知神经网络（PINN）方法解决偏微分方程的问题。

    Constructing the architecture of a neural network is a challenging pursuit for the machine learning community, and the dilemma of whether to go deeper or wider remains a persistent question. This paper explores a comparison between deeper neural networks (DeNNs) with a flexible number of layers and wider neural networks (WeNNs) with limited hidden layers, focusing on their optimal generalization error in Sobolev losses. Analytical investigations reveal that the architecture of a neural network can be significantly influenced by various factors, including the number of sample points, parameters within the neural networks, and the regularity of the loss function. Specifically, a higher number of parameters tends to favor WeNNs, while an increased number of sample points and greater regularity in the loss function lean towards the adoption of DeNNs. We ultimately apply this theory to address partial differential equations using deep Ritz and physics-informed neural network (PINN) methods,
    
[^114]: 在联邦设置中可分解子模函数的最大化

    Decomposable Submodular Maximization in Federated Setting

    [https://arxiv.org/abs/2402.00138](https://arxiv.org/abs/2402.00138)

    该论文提出了一种联邦优化设置用于在计算上不可行的可分解子模函数优化问题中。在这个设置中，客户端拥有私有的组件函数，通过并行计算和集中聚合的方式来求解最大化问题。

    

    在机器学习、推荐系统和福利最大化等众多应用中，子模函数以及可分解子模函数及其优化问题都得到了广泛应用。然而，对于具有数百万个组分函数的可分解子模函数的优化问题，在计算上是不可行的。此外，组分函数可能是私有的（例如可能表示用户偏好函数），不能广泛共享。为了解决这些问题，我们提出了一种适用于可分解子模函数优化的“联邦优化”设置。在这种设置下，客户端拥有自己的偏好函数，需要最大化这些偏好的加权和。我们在该设置中实现了流行的“连续贪婪”算法，其中客户端以并行的方式朝着局部解向前迈出小的局部步骤，然后将局部变化聚合到一个中央服务器上。

    Submodular functions, as well as the sub-class of decomposable submodular functions, and their optimization appear in a wide range of applications in machine learning, recommendation systems, and welfare maximization. However, optimization of decomposable submodular functions with millions of component functions is computationally prohibitive. Furthermore, the component functions may be private (they might represent user preference function, for example) and cannot be widely shared. To address these issues, we propose a {\em federated optimization} setting for decomposable submodular optimization. In this setting, clients have their own preference functions, and a weighted sum of these preferences needs to be maximized. We implement the popular {\em continuous greedy} algorithm in this setting where clients take parallel small local steps towards the local solution and then the local changes are aggregated at a central server. To address the large number of clients, the aggregation is 
    
[^115]: 通过ChatGPT解释的多模态神经退行性疾病亚型划分

    Multimodal Neurodegenerative Disease Subtyping Explained by ChatGPT

    [https://arxiv.org/abs/2402.00137](https://arxiv.org/abs/2402.00137)

    本论文提出了一个多模态框架，使用成像、遗传学和临床评估等早期指标将阿尔茨海默病患者在早期阶段划分为不同的亚型。同时，利用ChatGPT等大型语言模型解释模型的发现。

    

    阿尔茨海默病（AD）是最常见的神经退行性疾病，但目前可用的治疗方法仅限于停止疾病进展。此外，由于疾病的异质性，这些治疗方法的有效性并不保证。因此，能够在早期阶段确定疾病亚型非常重要。当前基于数据驱动的方法可以在AD或相关疾病后期对亚型进行分类，但在预测无症状或前驱期时存在困难。此外，大多数现有模型要么缺乏分类背后的可解释性，要么只使用单一模态进行评估，限制了其分析范围。因此，我们提出了一个多模态框架，利用成像、遗传学和临床评估等早期指标将AD患者分类为早期亚型。类似地，我们建立提示信息，并使用ChatGPT等大型语言模型解释我们模型的发现。

    Alzheimer's disease (AD) is the most prevalent neurodegenerative disease; yet its currently available treatments are limited to stopping disease progression. Moreover, effectiveness of these treatments is not guaranteed due to the heterogenetiy of the disease. Therefore, it is essential to be able to identify the disease subtypes at a very early stage. Current data driven approaches are able to classify the subtypes at later stages of AD or related disorders, but struggle when predicting at the asymptomatic or prodromal stage. Moreover, most existing models either lack explainability behind the classification or only use a single modality for the assessment, limiting scope of its analysis. Thus, we propose a multimodal framework that uses early-stage indicators such as imaging, genetics and clinical assessments to classify AD patients into subtypes at early stages. Similarly, we build prompts and use large language models, such as ChatGPT, to interpret the findings of our model. In our
    
[^116]: 比较基于模板和非模板语言模型的探测方法

    Comparing Template-based and Template-free Language Model Probing

    [https://arxiv.org/abs/2402.00123](https://arxiv.org/abs/2402.00123)

    本文比较了基于模板和非模板语言模型的探测方法，发现它们在模型排名、绝对得分和与困惑度的关系等方面存在差异。

    

    以专家制作的模板和自然发生的文本为基础的语言模型探测方法的差异经常被忽视。在这里，我们评估了16种不同的语言模型在10个英文探测数据集上的性能，其中包括4个基于模板的和6个非模板的数据集，并针对以下研究问题进行了回答：（RQ1）模型排名在两种方法中是否不同？（RQ2）模型的绝对得分在两种方法中是否不同？（RQ3）RQ1和RQ2的答案在一般和领域特定模型之间是否不同？我们的发现是：1）除了顶级的领域特定模型外，基于模板和非模板方法通常排名不同。2）与平行的非模板和模板提示相比，准确度下降了最多42%。3）在非模板方法中，困惑度与准确度呈负相关，但是在基于模板的探测中，它们呈正相关，这与直觉相反。4）模型倾向于预测相同的内容。

    The differences between cloze-task language model (LM) probing with 1) expert-made templates and 2) naturally-occurring text have often been overlooked. Here, we evaluate 16 different LMs on 10 probing English datasets -- 4 template-based and 6 template-free -- in general and biomedical domains to answer the following research questions: (RQ1) Do model rankings differ between the two approaches? (RQ2) Do models' absolute scores differ between the two approaches? (RQ3) Do the answers to RQ1 and RQ2 differ between general and domain-specific models? Our findings are: 1) Template-free and template-based approaches often rank models differently, except for the top domain-specific models. 2) Scores decrease by up to 42% Acc@1 when comparing parallel template-free and template-based prompts. 3) Perplexity is negatively correlated with accuracy in the template-free approach, but, counter-intuitively, they are positively correlated for template-based probing. 4) Models tend to predict the same
    
[^117]: 代码感知提示：基于LLM的回归设置下覆盖率导向的测试生成研究

    Code-Aware Prompting: A study of Coverage Guided Test Generation in Regression Setting using LLM

    [https://arxiv.org/abs/2402.00097](https://arxiv.org/abs/2402.00097)

    本文提出了一种代码感知提示策略（SymPrompt），用于基于LLM的测试生成，通过将测试生成过程分解为多阶段序列，并以驱动策略推动每个阶段，改善了测试生成的覆盖率。

    

    测试在确保软件质量方面起着至关重要的作用，然而传统的基于搜索的软件测试方法经常在复杂的软件单元上遇到困难，达不到最佳的测试覆盖率。最近使用大型语言模型（LLMs）进行测试生成的研究一直致力于通过优化测试生成上下文和纠正模型输出中的错误来改进生成质量，但使用了固定的提示策略，即提示模型在没有额外指导的情况下生成测试。因此，LLM生成的测试套件仍然存在低覆盖率的问题。在本文中，我们提出了SymPrompt，一种用于LLMs的代码感知提示策略来进行测试生成。SymPrompt的方法是基于最近的研究，该研究证明了LLMs在以多步方式思考问题时可以解决更复杂的逻辑问题。我们将这种方法应用于测试生成，将测试套件生成过程分解为多阶段序列，每个阶段都由一种驱动策略来推动。

    Testing plays a pivotal role in ensuring software quality, yet conventional Search Based Software Testing (SBST) methods often struggle with complex software units, achieving suboptimal test coverage. Recent work using large language models (LLMs) for test generation have focused on improving generation quality through optimizing the test generation context and correcting errors in model outputs, but use fixed prompting strategies that prompt the model to generate tests without additional guidance. As a result LLM-generated test suites still suffer from low coverage. In this paper, we present SymPrompt, a code-aware prompting strategy for LLMs in test generation. SymPrompt's approach is based on recent work that demonstrates LLMs can solve more complex logical problems when prompted to reason about the problem in a multi-step fashion. We apply this methodology to test generation by deconstructing the testsuite generation process into a multi-stage sequence, each of which is driven by a
    
[^118]: 深度神经网络: 非阿基米德分析的表述方式

    Deep Neural Networks: A Formulation Via Non-Archimedean Analysis

    [https://arxiv.org/abs/2402.00094](https://arxiv.org/abs/2402.00094)

    该论文引入了一种新的深度神经网络（DNNs）类别，其采用多层树状结构的架构并使用非阿基米德局部域的整数环进行编码。这些DNNs是稳健的对实值函数和实值平方可积函数的普遍逼近器。

    

    我们引入了一种新的深度神经网络（DNNs），采用多层树状结构的架构。这些架构使用非阿基米德局部域的整数环中的数字进行编码。这些环具有自然的层次结构，类似无限根树。这些环上的自然态射使我们能够构建有限的多层架构。新的DNNs是对在所提到的环上定义的实值函数的稳健的普遍逼近器。我们还证明了DNNs也是对在单位区间上定义的实值平方可积函数的稳健的普遍逼近器。

    We introduce a new class of deep neural networks (DNNs) with multilayered tree-like architectures. The architectures are codified using numbers from the ring of integers of non-Archimdean local fields. These rings have a natural hierarchical organization as infinite rooted trees. Natural morphisms on these rings allow us to construct finite multilayered architectures. The new DNNs are robust universal approximators of real-valued functions defined on the mentioned rings. We also show that the DNNs are robust universal approximators of real-valued square-integrable functions defined in the unit interval.
    
[^119]: ChIRAAG: 通过ChatGPT生成快速和自动断言的方法

    ChIRAAG: ChatGPT Informed Rapid and Automated Assertion Generation

    [https://arxiv.org/abs/2402.00093](https://arxiv.org/abs/2402.00093)

    本研究设计了一个基于大语言模型的流水线，通过自然语言规格生成英语、线性时态逻辑和SVA断言，并成功减少了断言错误率。

    

    System Verilog Assertion (SVA)的形式化是Formal Property Verification (FPV)过程中的一个关键但复杂的任务。传统上，SVA的形式化需要经验丰富的专家解释规格。这是耗时且容易出错的。然而，最近大语言模型（LLM）的进展使得基于LLM的自动断言生成引起了人们的兴趣。我们设计了一种新颖的基于LLM的流水线，用于从自然语言规格中生成英语、线性时态逻辑和SVA的断言。我们开发了一个基于OpenAI GPT4的自定义LLM用于实验。此外，我们还开发了测试平台来验证LLM生成的断言。只有43%的LLM生成的原始断言存在错误，包括语法和逻辑错误。通过使用从测试案例失败中得出的精心设计的提示，迭代地促使LLM，该流水线在最多九次提示迭代后可以生成正确的SVA。

    System Verilog Assertion (SVA) formulation, a critical yet complex task, is a pre-requisite in the Formal Property Verification (FPV) process. Traditionally, SVA formulation involves expert-driven interpretation of specifications. This is time consuming and prone to human error. However, recent advances in Large Language Models (LLM), LLM-informed automatic assertion generation is gaining interest. We designed a novel LLM-based pipeline to generate assertions in English Language, Linear Temporal Logic, and SVA from natural language specifications. We developed a custom LLM-based on OpenAI GPT4 for our experiments. Furthermore, we developed testbenches to verify/validate the LLM-generated assertions. Only 43% of LLM-generated raw assertions had errors, including syntax and logical errors. By iteratively prompting the LLMs using carefully crafted prompts derived from test case failures, the pipeline could generate correct SVAs after a maximum of nine iterations of prompting. Our results 
    
[^120]: 无分集任务选择对于少样本学习的影响

    Episodic-free Task Selection for Few-shot Learning

    [https://arxiv.org/abs/2402.00092](https://arxiv.org/abs/2402.00092)

    本文提出了一种超越分集训练的元训练框架，通过选择一些无分集任务对元学习器进行训练，并使用亲和性标准来评估其有效性。实验结果显示，这种方法在少样本学习中取得了更好的效果。

    

    分集训练是少样本学习中主流的训练策略。然而，在少样本的场景下，这种策略往往劣于一些非分集训练策略，如邻域分量分析（NCA），这挑战了训练条件必须与测试条件匹配的原则。因此，自然而然地提出了一个问题：如何搜索无分集任务从而获得更好的少样本学习效果？在这项工作中，我们提出了一种超越分集训练的新型元训练框架。在这个框架中，分集任务不直接用于训练，而是用于评估从任务集中选择的一些无分集任务对元学习器的训练效果。选择标准是通过亲和性设计的，亲和性衡量了在使用选定任务训练后，执行目标任务时损失降低的程度。在实验中，训练任务集包含了一些有前景的类型，如对比学习和分类。

    Episodic training is a mainstream training strategy for few-shot learning. In few-shot scenarios, however, this strategy is often inferior to some non-episodic training strategy, e. g., Neighbourhood Component Analysis (NCA), which challenges the principle that training conditions must match testing conditions. Thus, a question is naturally asked: How to search for episodic-free tasks for better few-shot learning? In this work, we propose a novel meta-training framework beyond episodic training. In this framework, episodic tasks are not used directly for training, but for evaluating the effectiveness of some selected episodic-free tasks from a task set that are performed for training the meta-learners. The selection criterion is designed with the affinity, which measures the degree to which loss decreases when executing the target tasks after training with the selected tasks. In experiments, the training task set contains some promising types, e. g., contrastive learning and classifica
    
[^121]: 利用计算模拟反应数据增强的 retrosynthesis 预测

    Retrosynthesis prediction enhanced by in-silico reaction data augmentation

    [https://arxiv.org/abs/2402.00086](https://arxiv.org/abs/2402.00086)

    RetroWISE 是一个利用计算模拟的反应数据增强的 retrosynthesis 预测的框架，通过使用易于获取的非配对数据生成配对数据，提高了模型的训练性能。

    

    机器学习在 retrosynthesis 研究中取得了重要进展，帮助化学家更高效地设计实验。然而，所有基于机器学习的方法都需要大量成对的训练数据（即化学反应：产物-反应物对），而这些数据获取成本高昂。此外，公司将反应数据视为有价值的资产，并限制对研究人员的可访问性。这些问题阻碍了更强大的 retrosynthesis 模型的创建，因为这些模型依赖于数据。为了解决这个问题，我们利用易于获取的非配对数据（即产物-反应物对中的一个组分）生成计算模拟的配对数据，为模型训练提供帮助。具体地，我们提出了 RetroWISE，一个自我增强的框架，它使用从真实配对数据推断出的基础模型，利用非配对数据执行计算模拟的反应生成和增强，最终实现了一个卓越的模型。在三个基准数据集上，RetroWISE 实现最佳的整体性能。

    Recent advances in machine learning (ML) have expedited retrosynthesis research by assisting chemists to design experiments more efficiently. However, all ML-based methods consume substantial amounts of paired training data (i.e., chemical reaction: product-reactant(s) pair), which is costly to obtain. Moreover, companies view reaction data as a valuable asset and restrict the accessibility to researchers. These issues prevent the creation of more powerful retrosynthesis models due to their data-driven nature. As a response, we exploit easy-to-access unpaired data (i.e., one component of product-reactant(s) pair) for generating in-silico paired data to facilitate model training. Specifically, we present RetroWISE, a self-boosting framework that employs a base model inferred from real paired data to perform in-silico reaction generation and augmentation using unpaired data, ultimately leading to a superior model. On three benchmark datasets, RetroWISE achieves the best overall performan
    
[^122]: 预定好奇心-深度动态-Q：对话策略学习的高效探索

    Scheduled Curiosity-Deep Dyna-Q: Efficient Exploration for Dialog Policy Learning

    [https://arxiv.org/abs/2402.00085](https://arxiv.org/abs/2402.00085)

    本论文提出了Scheduled Curiosity-Deep Dyna-Q (SC-DDQ)框架，通过引入预定学习和好奇心，显著提高了基于强化学习的任务导向对话代理的效果。

    

    基于强化学习的任务导向对话代理的培训是耗时的，并需要与真实用户进行大量的交互。如何在有限的对话经验中掌握对话策略仍然是使代理培训过程更加高效的障碍。此外，大多数先前的框架通过随机选择培训样本开始培训，这与人类学习方法不同，损害了培训的效率和稳定性。因此，我们提出了一种基于最先进的基于模型的强化学习对话模型Deep Dyna-Q(DDQ)的预定好奇心-深度动态-Q (SC-DDQ)的好奇心驱动课程学习框架。此外，我们分别为SC-DDQ和DDQ设计了学习计划，遵循两种相反的培训策略：经典课程学习及其逆向版本。我们的结果表明，通过引入预定学习和好奇心，新框架在DDQ和Dee的基础上取得了显著的改进。

    Training task-oriented dialog agents based on reinforcement learning is time-consuming and requires a large number of interactions with real users. How to grasp dialog policy within limited dialog experiences remains an obstacle that makes the agent training process less efficient. In addition, most previous frameworks start training by randomly choosing training samples, which differs from the human learning method and hurts the efficiency and stability of training. Therefore, we propose Scheduled Curiosity-Deep Dyna-Q (SC-DDQ), a curiosity-driven curriculum learning framework based on a state-of-the-art model-based reinforcement learning dialog model, Deep Dyna-Q (DDQ). Furthermore, we designed learning schedules for SC-DDQ and DDQ, respectively, following two opposite training strategies: classic curriculum learning and its reverse version. Our results show that by introducing scheduled learning and curiosity, the new framework leads to a significant improvement over the DDQ and Dee
    
[^123]: EPSD: 早期修剪与自蒸馏相结合的高效模型压缩

    EPSD: Early Pruning with Self-Distillation for Efficient Model Compression

    [https://arxiv.org/abs/2402.00084](https://arxiv.org/abs/2402.00084)

    提出了一种早期修剪与自蒸馏相结合的框架，实现了高效的模型压缩。

    

    神经网络压缩技术，如知识蒸馏和网络修剪，受到越来越多的关注。最近的研究“先修剪，然后蒸馏”表明，修剪后的适合学生的教师网络可以提高知识蒸馏的性能。然而，传统的教师-学生流程，涉及繁琐的教师预训练和复杂的压缩步骤，使得使用知识蒸馏的修剪变得不够高效。除了压缩模型，最近的压缩技术也强调效率方面的考量。早期修剪与传统的修剪方法相比，要求的计算成本要小得多，因为它不需要一个大型预训练模型。同样，自蒸馏作为知识蒸馏的一种特殊情况，更加高效，因为它不需要预训练或学生-教师对的选择。这激发了我们将早期修剪与自蒸馏相结合，实现高效的模型压缩。在这项工作中，我们提出了名为“早期修剪”的框架。

    Neural network compression techniques, such as knowledge distillation (KD) and network pruning, have received increasing attention. Recent work `Prune, then Distill' reveals that a pruned student-friendly teacher network can benefit the performance of KD. However, the conventional teacher-student pipeline, which entails cumbersome pre-training of the teacher and complicated compression steps, makes pruning with KD less efficient. In addition to compressing models, recent compression techniques also emphasize the aspect of efficiency. Early pruning demands significantly less computational cost in comparison to the conventional pruning methods as it does not require a large pre-trained model. Likewise, a special case of KD, known as self-distillation (SD), is more efficient since it requires no pre-training or student-teacher pair selection. This inspires us to collaborate early pruning with SD for efficient model compression. In this work, we propose the framework named Early Pruning wi
    
[^124]: 多机构数据的释放力量：整合和协调跨机构的基因组数据

    Unlocking the Power of Multi-institutional Data: Integrating and Harmonizing Genomic Data Across Institutions

    [https://arxiv.org/abs/2402.00077](https://arxiv.org/abs/2402.00077)

    该研究介绍了一种名为Bridge的模型，致力于解决利用多机构测序数据时面临的挑战，包括基因组板块的变化、测序技术的差异以及数据的高维度和稀疏性等。

    

    癌症是由基因突变驱动的复杂疾病，肿瘤测序已成为癌症患者临床护理的重要手段。出现的多机构测序数据为学习真实世界的证据以增强精准肿瘤医学提供了强大的资源。由美国癌症研究协会领导的GENIE BPC建立了一个独特的数据库，将基因组数据与多个癌症中心的临床信息联系起来。然而，利用这种多机构测序数据面临着重大挑战。基因组板块的变化导致在使用常见基因集进行分析时信息丢失。此外，不同的测序技术和机构之间的患者异质性增加了复杂性。高维数据、稀疏基因突变模式以及个体基因水平上的弱信号进一步增加了问题的复杂性。在这些现实世界的挑战的推动下，我们引入了Bridge模型。

    Cancer is a complex disease driven by genomic alterations, and tumor sequencing is becoming a mainstay of clinical care for cancer patients. The emergence of multi-institution sequencing data presents a powerful resource for learning real-world evidence to enhance precision oncology. GENIE BPC, led by the American Association for Cancer Research, establishes a unique database linking genomic data with clinical information for patients treated at multiple cancer centers. However, leveraging such multi-institutional sequencing data presents significant challenges. Variations in gene panels result in loss of information when the analysis is conducted on common gene sets. Additionally, differences in sequencing techniques and patient heterogeneity across institutions add complexity. High data dimensionality, sparse gene mutation patterns, and weak signals at the individual gene level further complicate matters. Motivated by these real-world challenges, we introduce the Bridge model. It use
    
[^125]: 可解释的人工智能用于生存分析：一种中位数-SHAP方法

    Explainable AI for survival analysis: a median-SHAP approach

    [https://arxiv.org/abs/2402.00072](https://arxiv.org/abs/2402.00072)

    这项研究介绍了一种中位数-SHAP方法，用于解释黑盒子模型在预测个体生存时间方面产生的解释偏差问题。

    

    随着机器学习在临床实践中的应用，对于医疗应用来说，需要针对性的可解释的人工智能方法。Shapley值在局部解释模型方面引起了广泛关注。在这里，我们展示了Shapley值的解释性强烈依赖于摘要统计量和估计量，这些统计量和估计量定义了我们所认为的“锚点”。我们表明，使用均值锚点的惯例可能在生存分析中产生误导性的解释，并引入了一种称为中位数-SHAP的方法，用于解释预测个体生存时间的黑盒子模型。

    With the adoption of machine learning into routine clinical practice comes the need for Explainable AI methods tailored to medical applications. Shapley values have sparked wide interest for locally explaining models. Here, we demonstrate their interpretation strongly depends on both the summary statistic and the estimator for it, which in turn define what we identify as an 'anchor point'. We show that the convention of using a mean anchor point may generate misleading interpretations for survival analysis and introduce median-SHAP, a method for explaining black-box models predicting individual survival times.
    
[^126]: 揭示自主扫描探针显微术中初始选择和循环干预对学习动力学的影响

    Unraveling the Impact of Initial Choices and In-Loop Interventions on Learning Dynamics in Autonomous Scanning Probe Microscopy

    [https://arxiv.org/abs/2402.00071](https://arxiv.org/abs/2402.00071)

    本文研究了自主扫描探针显微术中初始选择和循环干预对学习动力学的影响，并探讨了“种子效应”和种子点干预的概念，对深度内核学习的有效性进行了实证分析。

    

    自主实验（AE）目前的重点是开发有效进行AE的鲁棒工作流。这需要明确定义的方法来指导AE过程，包括超参数调整策略和工作流循环中的高级人员干预。本文通过分析自主扫描探针显微术中深度内核学习（DKL）的学习动力学，全面阐述了初始实验条件和循环干预对学习过程的影响。我们探讨了“种子效应”的概念，即初始实验设置对后续学习轨迹具有重要影响。此外，我们引入了AE中种子点干预的方法，使操作者能够影响探索过程。通过使用PbTiO3薄膜上的Piezoresponse力显微镜（PFM）数据集，我们展示了“种子效应”和循环种子干预对DKL的有效性的影响。

    The current focus in Autonomous Experimentation (AE) is on developing robust workflows to conduct the AE effectively. This entails the need for well-defined approaches to guide the AE process, including strategies for hyperparameter tuning and high-level human interventions within the workflow loop. This paper presents a comprehensive analysis of the influence of initial experimental conditions and in-loop interventions on the learning dynamics of Deep Kernel Learning (DKL) within the realm of AE in Scanning Probe Microscopy. We explore the concept of 'seed effect', where the initial experiment setup has a substantial impact on the subsequent learning trajectory. Additionally, we introduce an approach of the seed point interventions in AE allowing the operator to influence the exploration process. Using a dataset from Piezoresponse Force Microscopy (PFM) on PbTiO3 thin films, we illustrate the impact of the 'seed effect' and in-loop seed interventions on the effectiveness of DKL in pre
    
[^127]: EvoMerge:针对大型语言模型的神经进化

    EvoMerge: Neuroevolution for Large Language Models

    [https://arxiv.org/abs/2402.00070](https://arxiv.org/abs/2402.00070)

    EvoMerge是一种针对大型语言模型训练和合并的系统性方法，通过权重交叉和微调进行权重变异，旨在将模型推向超越传统微调限制的进化过程。

    

    在大型语言模型的广泛微调中，并不总能取得更好的结果。往往模型更擅长模仿一种数据形式而不具备更强的推理能力，甚至可能失去一些智能。这里我介绍了EvoMerge，一种用于大型语言模型训练和合并的系统性方法。通过利用权重交叉和微调进行权重变异，EvoMerge建立了一个旨在将模型推向超越传统微调限制的进化过程。

    Extensive fine-tuning on Large Language Models does not always yield better results. Oftentimes, models tend to get better at imitating one form of data without gaining greater reasoning ability and may even end up losing some intelligence. Here I introduce EvoMerge, a systematic approach to large language model training and merging. Leveraging model merging for weight crossover and fine-tuning for weight mutation, EvoMerge establishes an evolutionary process aimed at pushing models beyond the limits of conventional fine-tuning.
    
[^128]: GPT4Battery: 一种基于LLM驱动的自适应锂离子电池健康状态估计框架

    GPT4Battery: An LLM-driven Framework for Adaptive State of Health Estimation of Raw Li-ion Batteries

    [https://arxiv.org/abs/2402.00068](https://arxiv.org/abs/2402.00068)

    本论文提出了一种基于LLM的框架，可以适应不同类型的锂离子电池，实现准确的健康状态估计。这项工作解决了生成训练数据的时间和资源成本高的挑战，并在实际应用中具有良好的泛化能力。

    

    健康状态（SOH）是评估电池退化水平的关键指标，无法直接测量但需要估计。准确的SOH估计提升了锂离子电池的检测、控制和反馈能力，实现安全高效的能源管理，并指导新一代电池的发展。尽管在数据驱动的SOH估计方面取得了显著进展，但为生成寿命长期训练数据而进行的耗时且资源密集的退化实验在建立一个能处理多样化锂离子电池（例如，跨化学、跨制造商和跨容量）的大型模型方面存在挑战。因此，本文利用大型语言模型（LLM）的强大泛化能力，提出了一种适用于不同电池的可调整SOH估计的新型框架。为了适应实际情景，其中未标记的数据按顺序以及分布变化的方式到达，所提出的模型在测试时进行了修改。

    State of health (SOH) is a crucial indicator for assessing the degradation level of batteries that cannot be measured directly but requires estimation. Accurate SOH estimation enhances detection, control, and feedback for Li-ion batteries, allowing for safe and efficient energy management and guiding the development of new-generation batteries. Despite the significant progress in data-driven SOH estimation, the time and resource-consuming degradation experiments for generating lifelong training data pose a challenge in establishing one large model capable of handling diverse types of Li-ion batteries, e.g., cross-chemistry, cross-manufacturer, and cross-capacity. Hence, this paper utilizes the strong generalization capability of large language model (LLM) to proposes a novel framework for adaptable SOH estimation across diverse batteries. To match the real scenario where unlabeled data sequentially arrives in use with distribution shifts, the proposed model is modified by a test-time t
    
[^129]: 会议时长在线说话人辨识引导的语音分离

    Online speaker diarization of meetings guided by speech separation

    [https://arxiv.org/abs/2402.00067](https://arxiv.org/abs/2402.00067)

    本研究提出了一种适用于在线对长时间会议录音进行说话人辨识的新颖方法，通过语音分离来引导辨识过程。该方法能有效处理具有可变说话人数量的真实数据，并通过微调模型实现端到端的优化。

    

    重叠语音对说话人辨识系统一直是问题。因此，最近提出了使用语音分离来改善其性能。尽管有前景，但是由于它们在具有固定人数的模拟混合物上进行训练，语音分离模型在现实数据上很难应对。在这项工作中，我们引入了一种适用于在线对长时间会议录音进行说话人辨识的语音分离引导方案，这些录音具有可变的说话人数量，就像在AMI语料库中一样。我们可以将ConvTasNet和DPRNN视为分离网络的替代方案，输出两个或三个源。为了得到说话人辨识结果，对每个估计的源应用语音活动检测。在首先使用AMI将分离模型适应于真实数据后，最终模型进行了端到端的微调。系统在短段上操作，使用说话人嵌入和增量式 stitching 以进行推断。

    Overlapped speech is notoriously problematic for speaker diarization systems. Consequently, the use of speech separation has recently been proposed to improve their performance. Although promising, speech separation models struggle with realistic data because they are trained on simulated mixtures with a fixed number of speakers. In this work, we introduce a new speech separation-guided diarization scheme suitable for the online speaker diarization of long meeting recordings with a variable number of speakers, as present in the AMI corpus. We envisage ConvTasNet and DPRNN as alternatives for the separation networks, with two or three output sources. To obtain the speaker diarization result, voice activity detection is applied on each estimated source. The final model is fine-tuned end-to-end, after first adapting the separation to real data using AMI. The system operates on short segments, and inference is performed by stitching the local predictions using speaker embeddings and increm
    
[^130]: TrackGPT -- 用于跨领域实体轨迹预测的生成式预训练变换器

    TrackGPT -- A generative pre-trained transformer for cross-domain entity trajectory forecasting

    [https://arxiv.org/abs/2402.00066](https://arxiv.org/abs/2402.00066)

    TrackGPT是一种基于GPT的实体轨迹预测模型，能够生成准确的预测结果，包括长期预测和短期预测，并在多个领域展现出良好的性能。

    

    在商业和国防领域的应用中，对未来时间点的实体轨迹进行预测是一个关键的能力缺口。近年来，变换器和特别是生成式预训练变换器（GPT）网络已经在人工智能的几个领域革命化，其中最为著名的是使用开放AI的ChatGPT等大型语言模型（LLM）的自然语言处理（NLP）。在本研究论文中，我们介绍了TrackGPT，一种基于GPT的实体轨迹预测模型，该模型在海上和航空领域都显示出了实用性，并且我们预计在其他领域也能表现出色。TrackGPT是一种开创性的GPT模型，能够在不同的实体时间序列数据集上产生准确的预测，展示了长期预测具有持续准确性和短期预测具有高精度的能力。我们与最先进的深度学习技术进行的基准测试显示，TrackGPT的预测能力很高。

    The forecasting of entity trajectories at future points in time is a critical capability gap in applications across both Commercial and Defense sectors. Transformers, and specifically Generative Pre-trained Transformer (GPT) networks have recently revolutionized several fields of Artificial Intelligence, most notably Natural Language Processing (NLP) with the advent of Large Language Models (LLM) like OpenAI's ChatGPT. In this research paper, we introduce TrackGPT, a GPT-based model for entity trajectory forecasting that has shown utility across both maritime and air domains, and we expect to perform well in others. TrackGPT stands as a pioneering GPT model capable of producing accurate predictions across diverse entity time series datasets, demonstrating proficiency in generating both long-term forecasts with sustained accuracy and short-term forecasts with high precision. We present benchmarks against state-of-the-art deep learning techniques, showing that TrackGPT's forecasting capa
    
[^131]: FengWu-GHR: 学习公里级中程全球天气预报

    FengWu-GHR: Learning the Kilometer-scale Medium-range Global Weather Forecasting

    [https://arxiv.org/abs/2402.00059](https://arxiv.org/abs/2402.00059)

    FengWu-GHR是全球首个以数据驱动方式运行的公里级全球天气预报模型，具有更高的分辨率和可比甚至更高的预报技巧。

    

    公里级别的全球大气动力学建模可以实现精细化的天气预报，降低灾害性天气和气候活动的风险。因此，建立公里级全球预报模型是气象领域一直以来的追求。过去几十年，国际社会积极努力改善数值天气模型的空间分辨率。然而，由于计算资源的消耗巨大，发展更高分辨率的数值模型仍然是一个长期存在的挑战。最近，数据驱动的全球天气预报模型利用再分析数据进行模型训练，并展示出与数值模型相当甚至更高的预报技巧。然而，它们都受限于再分析数据的分辨率，无法生成更高分辨率的预报。本文介绍了FengWu-GHR，这是首个以数据驱动方式运行、0.09$^{\circ}$水平分辨率的全球天气预报模型。

    Kilometer-scale modeling of global atmosphere dynamics enables fine-grained weather forecasting and decreases the risk of disastrous weather and climate activity. Therefore, building a kilometer-scale global forecast model is a persistent pursuit in the meteorology domain. Active international efforts have been made in past decades to improve the spatial resolution of numerical weather models. Nonetheless, developing the higher resolution numerical model remains a long-standing challenge due to the substantial consumption of computational resources. Recent advances in data-driven global weather forecasting models utilize reanalysis data for model training and have demonstrated comparable or even higher forecasting skills than numerical models. However, they are all limited by the resolution of reanalysis data and incapable of generating higher-resolution forecasts. This work presents FengWu-GHR, the first data-driven global weather forecasting model running at the 0.09$^{\circ}$ horizo
    
[^132]: 预测基因突变的功能丧失影响：一种机器学习方法

    Predicting loss-of-function impact of genetic mutations: a machine learning approach

    [https://arxiv.org/abs/2402.00054](https://arxiv.org/abs/2402.00054)

    本研究使用机器学习的方法，根据基因突变的属性预测了功能丧失影响，为识别有害突变提供了新的手段。

    

    下一代测序(NGS)技术的创新大大降低了基因组测序的价格，降低了未来医学研究的门槛；现在可以将基因组测序应用于以前成本效益低的研究中。在大量复杂的高维基因组测序数据中识别有害或致病突变对研究人员可能特别感兴趣。因此，本文旨在训练机器学习模型，使用基因突变的属性来预测LoFtool评分（衡量基因对功能丧失突变的不耐受性）。这些属性包括但不限于突变在染色体上的位置、氨基酸的变化以及突变引起的密码子的变化。使用单变量特征选择技术f-regression结合K最近邻（KNN）、支持向量机（SVM）、随机抽样共识（RANSAC）、决策树、逻辑回归等构建模型。

    The innovation of next-generation sequencing (NGS) techniques has significantly reduced the price of genome sequencing, lowering barriers to future medical research; it is now feasible to apply genome sequencing to studies where it would have previously been cost-inefficient. Identifying damaging or pathogenic mutations in vast amounts of complex, high-dimensional genome sequencing data may be of particular interest to researchers. Thus, this paper's aims were to train machine learning models on the attributes of a genetic mutation to predict LoFtool scores (which measure a gene's intolerance to loss-of-function mutations). These attributes included, but were not limited to, the position of a mutation on a chromosome, changes in amino acids, and changes in codons caused by the mutation. Models were built using the univariate feature selection technique f-regression combined with K-nearest neighbors (KNN), Support Vector Machine (SVM), Random Sample Consensus (RANSAC), Decision Trees, R
    
[^133]: 我们在浪费时间吗？一种快速、准确的知识图谱链接预测评估框架

    Are We Wasting Time? A Fast, Accurate Performance Evaluation Framework for Knowledge Graph Link Predictors

    [https://arxiv.org/abs/2402.00053](https://arxiv.org/abs/2402.00053)

    这篇论文提出了一种快速、准确的知识图谱链接预测评估框架，解决了现有方法中随机抽样带来的排名指标不准确的问题。

    

    用于衡量知识图谱完善方法质量的标准评估协议通常包括对知识图谱的每个实体进行排名，以评估它们作为候选链接头部或尾部的适合程度。然而，在规模较大的知识图谱中，这个任务很快变得难以承受。先前的方法通过对实体进行随机抽样来评估预测或建议方法的链接质量，但我们展示了这种方法存在严重局限性，因为产生的排名指标不正确地反映了真实结果。本文对这些效应进行了彻底分析，并得出以下发现。首先，我们通过实证研究和理论论证找出了为什么随机均匀抽样极大地高估了方法的排名表现。我们展示了这可以归因于易/难度负候选者的影响。

    The standard evaluation protocol for measuring the quality of Knowledge Graph Completion methods - the task of inferring new links to be added to a graph - typically involves a step which ranks every entity of a Knowledge Graph to assess their fit as a head or tail of a candidate link to be added. In Knowledge Graphs on a larger scale, this task rapidly becomes prohibitively heavy. Previous approaches mitigate this problem by using random sampling of entities to assess the quality of links predicted or suggested by a method. However, we show that this approach has serious limitations since the ranking metrics produced do not properly reflect true outcomes. In this paper, we present a thorough analysis of these effects along with the following findings. First, we empirically find and theoretically motivate why sampling uniformly at random vastly overestimates the ranking performance of a method. We show that this can be attributed to the effect of easy versus hard negative candidates. S
    
[^134]: 引入PetriRL：将Petri网和基于事件的强化学习集成的JSSP解决方案的创新框架

    Introducing PetriRL: An Innovative Framework for JSSP Resolution Integrating Petri nets and Event-based Reinforcement Learning

    [https://arxiv.org/abs/2402.00046](https://arxiv.org/abs/2402.00046)

    PetriRL是一个创新框架，将Petri网和基于事件的强化学习集成，用于解决工业作业车间调度问题（JSSP）。该框架利用Petri网进行建模，提高了可解释性，并通过事件驱动控制和动作屏蔽的集成取得了竞争性的性能。

    

    在工业车间中，优质调度至关重要。尽管神经网络在解决这些问题方面表现出色，但其有限的可解释性阻碍了其在工业中的广泛应用。在本研究中，我们介绍了一个创新的框架来解决作业车间调度问题（JSSP）。我们的方法利用Petri网对作业车间进行建模，不仅提高了可解释性，还能直接将原始数据纳入其中，无需对JSSP实例进行预处理成非交替图。Petri网的控制能力还可以管理过程中的自动化组件，使代理人能够专注于关键决策，特别是资源分配。我们的方法中事件驱动控制和动作屏蔽的集成在公共测试基准上取得了竞争性的性能。跨一系列优化解决方案（包括启发式算法、元启发式算法和学习算法）进行的比较分析突出了其竞争性。

    Quality scheduling in industrial job shops is crucial. Although neural networks excel in solving these problems, their limited explainability hinders their widespread industrial adoption. In this research, we introduce an innovative framework for solving job shop scheduling problems (JSSP). Our methodology leverages Petri nets to model the job shop, not only improving explainability but also enabling direct incorporation of raw data without the need to preprocess JSSP instances into disjunctive graphs. The Petri net, with its controlling capacities, also governs the automated components of the process, allowing the agent to focus on critical decision-making, particularly resource allocation. The integration of event-based control and action masking in our approach yields competitive performance on public test benchmarks. Comparative analyses across a wide spectrum of optimization solutions, including heuristics, metaheuristics, and learning-based algorithms, highlight the competitivene
    
[^135]: 检测大型人工智能模型生成的多媒体内容：一项调查研究

    Detecting Multimedia Generated by Large AI Models: A Survey

    [https://arxiv.org/abs/2402.00045](https://arxiv.org/abs/2402.00045)

    本文综述了大型人工智能模型（LAIMs）生成的多媒体内容的检测方法和研究进展，旨在填补现有研究中的空白。我们提供了一种新颖的分类法，并介绍了纯检测和应用场景两个角度来增强检测性能。

    

    大型人工智能模型（LAIMs）的快速发展，尤其是扩散模型和大型语言模型，标志着一种新的时代，人工智能生成的多媒体内容被越来越多地整合到日常生活的各个方面。尽管在许多领域有益，但这些内容也带来了重大风险，包括潜在的滥用、社会破坏和伦理问题。因此，检测由LAIMs生成的多媒体内容变得至关重要，相关研究也大幅增加。尽管如此，目前仍然存在一个明显的问题，即缺乏系统性的调查研究，专门关注检测LAIMs生成的多媒体内容。为了解决这个问题，我们提供了第一份全面涵盖现有研究的调查报告，重点关注检测LAIMs生成的多媒体内容（如文本、图像、视频、音频和多模态内容）。具体而言，我们引入了一种新颖的检测方法分类法，按媒体形式分类，并与纯检测（旨在提高检测性能）和应用场景对齐。

    The rapid advancement of Large AI Models (LAIMs), particularly diffusion models and large language models, has marked a new era where AI-generated multimedia is increasingly integrated into various aspects of daily life. Although beneficial in numerous fields, this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns. Consequently, detecting multimedia generated by LAIMs has become crucial, with a marked rise in related research. Despite this, there remains a notable gap in systematic surveys that focus specifically on detecting LAIM-generated multimedia. Addressing this, we provide the first survey to comprehensively cover existing research on detecting multimedia (such as text, images, videos, audio, and multimodal content) created by LAIMs. Specifically, we introduce a novel taxonomy for detection methods, categorized by media modality, and aligned with two perspectives: pure detection (aiming to enhance detection performance) an
    
[^136]: 使用大型语言模型训练微型机器人游泳

    Training microrobots to swim by a large language model

    [https://arxiv.org/abs/2402.00044](https://arxiv.org/abs/2402.00044)

    本研究利用大型语言模型(GPT-4)训练两个微型机器人游泳，采用少样本学习方法，开发了一个简洁统一提示，成功引导这两种微型机器人掌握其特色划水动作，从而超越了传统方法。

    

    机器学习和人工智能近年来成为设计和优化各种尺度机器人系统的流行范式。最近的研究展示了大型语言模型（LLMs）在工业控制[1]和指导腿式行走机器人[2]方面的创新应用。在本研究中，我们利用一个LLM，GPT-4，来训练两个原型微型机器人在粘性流体中游泳。采用少样本学习方法，我们开发了一个仅由五个句子组成的简洁统一提示。相同的简洁提示成功地引导了两种不同的关节式微型机器人——三连杆游泳机器人和三球游泳机器人——掌握它们的特色划水动作。这些动作最初由物理学家构思，现在通过LLM得到有效的解释和应用，使得微型机器人能够克服微小运动中固有的物理约束。值得注意的是，我们基于LLM的决策策略明显超越了传统方法。

    Machine learning and artificial intelligence have recently represented a popular paradigm for designing and optimizing robotic systems across various scales. Recent studies have showcased the innovative application of large language models (LLMs) in industrial control [1] and in directing legged walking robots [2]. In this study, we utilize an LLM, GPT-4, to train two prototypical microrobots for swimming in viscous fluids. Adopting a few-shot learning approach, we develop a minimal, unified prompt composed of only five sentences. The same concise prompt successfully guides two distinct articulated microrobots -- the three-link swimmer and the three-sphere swimmer -- in mastering their signature strokes. These strokes, initially conceptualized by physicists, are now effectively interpreted and applied by the LLM, enabling the microrobots to circumvent the physical constraints inherent to micro-locomotion. Remarkably, our LLM-based decision-making strategy substantially surpasses a trad
    
[^137]: 利用因果贝叶斯网络和知识图谱进行制造业根本原因分析的互动智能方法

    Interactive and Intelligent Root Cause Analysis in Manufacturing with Causal Bayesian Networks and Knowledge Graphs

    [https://arxiv.org/abs/2402.00043](https://arxiv.org/abs/2402.00043)

    本文提出了一个互动智能的制造业根本原因分析工具，通过结合电动汽车制造过程的专家知识和数据驱动的机器学习方法，利用大规模知识图谱进行推理并学习一个因果贝叶斯网络。

    

    电动汽车制造之中，根本原因分析(RCA)是识别故障原因的过程。传统上，RCA是依靠过程专家知识手工进行的。同时，传感器网络在制造过程中收集了大量的数据，利用这些数据进行RCA能够提高效率。然而，纯数据驱动的方法(如因果贝叶斯网络)在大规模的实际制造过程中具有扩展性问题，因为存在大量的潜在因果关系(CERs)。此外，纯数据驱动的方法有可能忽略已知的CERs或学习到错误的CERs。本文提出了一个互动智能的RCA工具，将电动汽车制造过程的专家知识和数据驱动的机器学习方法相结合。该工具利用制造过程的大规模知识图谱进行推理，同时学习一个因果贝叶斯网络。

    Root Cause Analysis (RCA) in the manufacturing of electric vehicles is the process of identifying fault causes. Traditionally, the RCA is conducted manually, relying on process expert knowledge. Meanwhile, sensor networks collect significant amounts of data in the manufacturing process. Using this data for RCA makes it more efficient. However, purely data-driven methods like Causal Bayesian Networks have problems scaling to large-scale, real-world manufacturing processes due to the vast amount of potential cause-effect relationships (CERs). Furthermore, purely data-driven methods have the potential to leave out already known CERs or to learn spurious CERs. The paper contributes by proposing an interactive and intelligent RCA tool that combines expert knowledge of an electric vehicle manufacturing process and a data-driven machine learning method. It uses reasoning over a large-scale Knowledge Graph of the manufacturing process while learning a Causal Bayesian Network. In addition, an I
    
[^138]: 空间-时间-需求聚类用于解决具有时间窗口的大规模车辆路径问题

    Spatial-temporal-demand clustering for solving large-scale vehicle routing problems with time windows

    [https://arxiv.org/abs/2402.00041](https://arxiv.org/abs/2402.00041)

    该论文提出了一种基于聚类的解决大规模车辆路径问题的方法，通过结合空间、时间、需求数据，将问题分解为子路径问题，然后分别解决，并通过局部搜索来改进整体解决方案。

    

    几种元启发式方法使用分解和修剪策略来解决车辆路径问题（VRP）的大规模实例。这些复杂性减少技术通常依赖于简单的、问题特定的规则。然而，可用数据的增长和计算机硬件的进步使得可以使用机器学习（ML）的基于数据的方法来改善解决方案算法的可扩展性。我们提出了一个分解-路径改进（DRI）框架，使用聚类将客户分组。其相似度度量指标包括客户的空间、时间和需求数据，并且被制定成反映问题目标函数和约束的形式。导致的子路径问题可以使用任何合适的算法独立地解决。我们在解决的子问题之间应用修剪的局部搜索（LS）来改进整体解。修剪基于在分解阶段获得的客户相似度信息。在计算研究中，我们参数化并比较现有的聚类方法。

    Several metaheuristics use decomposition and pruning strategies to solve large-scale instances of the vehicle routing problem (VRP). Those complexity reduction techniques often rely on simple, problem-specific rules. However, the growth in available data and advances in computer hardware enable data-based approaches that use machine learning (ML) to improve scalability of solution algorithms. We propose a decompose-route-improve (DRI) framework that groups customers using clustering. Its similarity metric incorporates customers' spatial, temporal, and demand data and is formulated to reflect the problem's objective function and constraints. The resulting sub-routing problems can independently be solved using any suitable algorithm. We apply pruned local search (LS) between solved subproblems to improve the overall solution. Pruning is based on customers' similarity information obtained in the decomposition phase. In a computational study, we parameterize and compare existing clustering
    
[^139]: 通过多模态神经网络检测脑肿瘤

    Detecting Brain Tumors through Multimodal Neural Networks

    [https://arxiv.org/abs/2402.00038](https://arxiv.org/abs/2402.00038)

    通过多模态神经网络检测脑肿瘤，在图像处理和分类中取得了令人满意的结果，并具有98%的准确率。

    

    肿瘤可以以各种形式出现在人体的不同部位。由于脑组织的复杂性，脑肿瘤的诊断和治疗特别困难。及时检测肿瘤可以降低死亡风险，并为患者的治疗过程提供便利。使用人工智能（AI）和深度学习等技术，可以显著减少通过成像技术获取图像来发现和识别肿瘤的时间和资源成本。本研究旨在评估一种多模态模型在将处理成灰度图像的磁共振成像（MRI）扫描用于分类时的性能。结果令人鼓舞，并与类似研究一致，模型准确率约为98％。我们还强调了解释性和透明性的必要性，以确保人类控制和安全性。

    Tumors can manifest in various forms and in different areas of the human body. Brain tumors are specifically hard to diagnose and treat because of the complexity of the organ in which they develop. Detecting them in time can lower the chances of death and facilitate the therapy process for patients. The use of Artificial Intelligence (AI) and, more specifically, deep learning, has the potential to significantly reduce costs in terms of time and resources for the discovery and identification of tumors from images obtained through imaging techniques. This research work aims to assess the performance of a multimodal model for the classification of Magnetic Resonance Imaging (MRI) scans processed as grayscale images. The results are promising, and in line with similar works, as the model reaches an accuracy of around 98\%. We also highlight the need for explainability and transparency to ensure human control and safety.
    
[^140]: Kronecker积特征融合用于遥感场景分类的卷积神经网络

    Kronecker Product Feature Fusion for Convolutional Neural Network in Remote Sensing Scene Classification

    [https://arxiv.org/abs/2402.00036](https://arxiv.org/abs/2402.00036)

    本文提出了一种新颖的特征融合算法，使用Kronecker积(KPFF)将加法和连接方法统一起来，并通过实验证明其能够提高卷积神经网络在遥感场景分类中的准确性。

    

    遥感场景分类是一个具有挑战性和价值的研究课题，在其中卷积神经网络(CNN)发挥了关键作用。CNN可以从遥感图像中提取层次的卷积特征，并通过不同层的特征融合来增强CNN的性能。一些最先进的CNN算法中采用了两种成功的特征融合方法，即加法和连接方法。本文提出了一种新颖的特征融合算法，使用Kronecker积(KPFF)将上述方法统一起来，并讨论了与该算法相关的反向传播过程。为了验证所提方法的有效性，设计并进行了一系列实验。结果表明，在遥感场景分类中，该方法能够提高CNN的准确性。

    Remote Sensing Scene Classification is a challenging and valuable research topic, in which Convolutional Neural Network (CNN) has played a crucial role. CNN can extract hierarchical convolutional features from remote sensing imagery, and Feature Fusion of different layers can enhance CNN's performance. Two successful Feature Fusion methods, Add and Concat, are employed in certain state-of-the-art CNN algorithms. In this paper, we propose a novel Feature Fusion algorithm, which unifies the aforementioned methods using the Kronecker Product (KPFF), and we discuss the Backpropagation procedure associated with this algorithm. To validate the efficacy of the proposed method, a series of experiments are designed and conducted. The results demonstrate its effectiveness of enhancing CNN's accuracy in Remote sensing scene classification.
    
[^141]: 航班滑行安全的跑道物体分类器的鲁棒性评估

    Robustness Assessment of a Runway Object Classifier for Safe Aircraft Taxiing

    [https://arxiv.org/abs/2402.00035](https://arxiv.org/abs/2402.00035)

    本文介绍了对航班滑行安全的跑道物体分类器的鲁棒性评估，使用形式方法评估了该分类器对三种常见图像扰动类型的鲁棒性，并提出了一种利用单调性的方法。

    

    随着深度神经网络(DNNs)在许多计算问题上成为主要解决方案，航空业希望探索它们在减轻飞行员负担和改善运营安全方面的潜力。然而，在这类安全关键应用中使用DNNs需要进行彻底的认证过程。这一需求可以通过形式验证来解决，形式验证提供了严格的保证，例如证明某些误判的不存在。在本文中，我们使用Airbus当前正在开发的图像分类器DNN作为案例研究，旨在在飞机滑行阶段使用。我们使用形式方法来评估这个DNN对三种常见图像扰动类型的鲁棒性：噪声、亮度和对比度，以及它们的部分组合。这个过程涉及多次调用底层验证器，这可能在计算上是昂贵的；因此，我们提出了一种利用单调性的方法。

    As deep neural networks (DNNs) are becoming the prominent solution for many computational problems, the aviation industry seeks to explore their potential in alleviating pilot workload and in improving operational safety. However, the use of DNNs in this type of safety-critical applications requires a thorough certification process. This need can be addressed through formal verification, which provides rigorous assurances -- e.g.,~by proving the absence of certain mispredictions. In this case-study paper, we demonstrate this process using an image-classifier DNN currently under development at Airbus and intended for use during the aircraft taxiing phase. We use formal methods to assess this DNN's robustness to three common image perturbation types: noise, brightness and contrast, and some of their combinations. This process entails multiple invocations of the underlying verifier, which might be computationally expensive; and we therefore propose a method that leverages the monotonicity
    
[^142]: FIRST Robotics Competition中团队组建和冠军预测的综合框架：模型，算法和分析

    An Integrated Framework for Team Formation and Winner Prediction in the FIRST Robotics Competition: Model, Algorithm, and Analysis

    [https://arxiv.org/abs/2402.00031](https://arxiv.org/abs/2402.00031)

    本研究提出了一个集成框架，通过在团队形成之前的队员技能数据上进行优化，预测团队表现，填补了FIRST Robotics Competition领域的研究空白。

    

    本研究旨在开发一种分析方法，基于团队成员技能的数据，优化团队组建和预测团队表现，以在竞争环境中最大化团队效能。目前，科学文献中有几种优化团队表现和预测的方法。然而，大多数研究都使用了个体成员的细粒度技能统计或团队成员组合的约束条件。目前还没有研究涉及到高度约束的FIRST Robotics Competition领域。本研究旨在填补这一空白，通过提供一种分析方法，在允许这些约束条件的情况下，仅利用先前团队表现的指标来优化和预测团队表现。我们将这种方法应用于FIRST Robotics竞赛的选秀过程，这个领域的技能每年改变，团队成员也会发生变化。

    This research work aims to develop an analytical approach for optimizing team formation and predicting team performance in a competitive environment based on data on the competitors' skills prior to the team formation. There are several approaches in scientific literature to optimize and predict a team's performance. However, most studies employ fine-grained skill statistics of the individual members or constraints such as teams with a set group of members. Currently, no research tackles the highly constrained domain of the FIRST Robotics Competition. This research effort aims to fill this gap by providing an analytical method for optimizing and predicting team performance in a competitive environment while allowing these constraints and only using metrics on previous team performance, not on each individual member's performance. We apply our method to the drafting process of the FIRST Robotics competition, a domain in which the skills change year-over-year, team members change through
    
[^143]: LLaMA和ChatGPT嵌入在分子嵌入中的比较分析

    Comparative Analysis of LLaMA and ChatGPT Embeddings for Molecule Embedding

    [https://arxiv.org/abs/2402.00024](https://arxiv.org/abs/2402.00024)

    LLaMA和ChatGPT比较分析了它们在SMILES字符串嵌入中的性能，在分子性质预测和药物-药物相互作用预测中，LLaMA相对于ChatGPT表现更好并且与现有方法相当。

    

    目的：ChatGPT和LLaMA等大型语言模型在化学信息学领域越来越受到重视，特别是在解释Simplified Molecular Input Line Entry System (SMILES)方面。这些语言模型可以将SMILES字符串解码为向量表示，为理解化学图提供了一种新的方法。方法：我们研究了ChatGPT和LLaMA在嵌入SMILES字符串方面的性能。我们的评估集中在两个关键应用领域：分子性质（MP）预测和药物-药物相互作用（DDI）预测，这在药物开发和医疗保健中至关重要。结果：我们发现，使用LLaMA生成的SMILES嵌入在MP和DDI预测任务中表现优于ChatGPT。值得注意的是，基于LLaMA的SMILES嵌入在这两个预测任务中显示了与现有方法相当的结果。结论：在化学信息学中应用LLMs，特别是在利用SMILES进行嵌入方面，是可行的。

    Purpose: Large Language Models (LLMs) like ChatGPT and LLaMA are increasingly recognized for their potential in the field of cheminformatics, particularly in interpreting Simplified Molecular Input Line Entry System (SMILES), a standard method for representing chemical structures. These LLMs can decode SMILES strings into vector representations, providing a novel approach to understanding chemical graphs.   Methods: We investigate the performance of ChatGPT and LLaMA in embedding SMILES strings. Our evaluation focuses on two key applications: molecular property (MP) prediction and drug-drug interaction (DDI) prediction, both essential in drug development and healthcare.   Results: We find that SMILES embeddings generated using LLaMA outperform those from ChatGPT in both MP and DDI prediction tasks. Notably, LLaMA-based SMILES embeddings show results comparable to existing methods in both prediction tasks.   Conclusion: The application of LLMs in cheminformatics, particularly in utilizi
    
[^144]: 使用多时相Sentinel-1和Sentinel-2数据进行水体测绘

    Using Multi-Temporal Sentinel-1 and Sentinel-2 data for water bodies mapping

    [https://arxiv.org/abs/2402.00023](https://arxiv.org/abs/2402.00023)

    本论文通过整合Sentinel-1和Sentinel-2数据，提出了一种新的多源和多时相的数据集，用于全面水资源监测。应用土壤含水指数和归一化差异水体指数等指标以及无监督机器学习分类器，取得了有希望的结果。

    

    气候变化加剧了极端天气事件，导致水资源匮乏和降雨的不可预测性，对可持续发展、生物多样性以及水和卫生设施的可及性构成威胁。本文旨在为在各种气象条件下进行全面水资源监测提供有价值的见解。提出了扩展SEN2DWATER数据集以增强其对水盆地分割的能力。通过将Sentinel-1数据中的时间和空间对齐的雷达信息与现有的多光谱Sentinel-2数据集集成，生成了一种新的多源和多时相的数据集。对增强的数据集进行基准测试，包括应用土壤含水指数（SWI）和归一化差异水体指数（NDWI）以及无监督机器学习分类器（K均值聚类）。取得了有希望的结果，并可能产生未来的发展和应用。

    Climate change is intensifying extreme weather events, causing both water scarcity and severe rainfall unpredictability, and posing threats to sustainable development, biodiversity, and access to water and sanitation. This paper aims to provide valuable insights for comprehensive water resource monitoring under diverse meteorological conditions. An extension of the SEN2DWATER dataset is proposed to enhance its capabilities for water basin segmentation. Through the integration of temporally and spatially aligned radar information from Sentinel-1 data with the existing multispectral Sentinel-2 data, a novel multisource and multitemporal dataset is generated. Benchmarking the enhanced dataset involves the application of indices such as the Soil Water Index (SWI) and Normalized Difference Water Index (NDWI), along with an unsupervised Machine Learning (ML) classifier (k-means clustering). Promising results are obtained and potential future developments and applications arising from this re
    
[^145]: 机器学习在弥散磁共振成像中的应用

    Diffusion MRI with Machine Learning

    [https://arxiv.org/abs/2402.00019](https://arxiv.org/abs/2402.00019)

    本文评估了机器学习在弥散磁共振成像中的应用，重点关注了微结构映射、纤维束描记、白质纤维束分析以及数据预处理和协调的方法。通过对现有方法的总结，提出了未来研究的主题。

    

    弥散加权磁共振成像（dMRI）具有非侵入性评估大脑微结构和结构连接的独特能力。然而，分析dMRI数据以提取临床和科学目的的有用信息具有挑战性。 dMRI测量通常受到强噪声和伪影的干扰，数据中通常存在高的会话间和扫描者间异质性，以及大脑结构的相当大的个体间变异，并且测量和感兴趣现象之间的关系可能非常复杂。近年来，机器学习方法在dMRI分析中的应用越来越多。本文旨在评估这些尝试，重点关注已经解决了微结构映射、纤维束描记、白质纤维束分析以及数据预处理和协调的方法。我们总结了现有方法的主要发现、优点和缺点，并提出了未来研究的主题。

    Diffusion-weighted magnetic resonance imaging (dMRI) offers unique capabilities such as noninvasive assessment of brain's micro-structure and structural connectivity. However, analyzing the dMRI data to extract useful information for clinical and scientific purposes is challenging. The dMRI measurements often suffer from strong noise and artifacts, there is usually high inter-session and inter-scanner heterogeneity in the data and considerable inter-subject variability in brain structure, and the relationship between measurements and the phenomena of interest can be highly complex. Recent years have witnessed increasing use of machine learning methods for dMRI analysis. This manuscript aims to assess these efforts, with a focus on methods that have addressed micro-structure mapping, tractography, white matter tract analysis, as well as data preprocessing and harmonization. We summarize the main findings, strengths, and weaknesses of the existing methods and suggest topics for future re
    
[^146]: 混合量子循环生成对抗网络用于小分子生成

    Hybrid quantum cycle generative adversarial network for small molecule generation

    [https://arxiv.org/abs/2402.00014](https://arxiv.org/abs/2402.00014)

    本文介绍了一种基于参数化量子电路的新型生成对抗网络模型，通过引入强化学习原理的多参数奖励函数，成功优化了小分子生成的药物样性估计，最高提升了30%。

    

    当代药物设计过程需要大量的时间和资源来开发首次进入市场的每个新化合物。生成小分子是药物发现的一个关键方面，对于开发创新的制药产品至关重要。然而，尚未充分发挥独特性、有效性、多样性、药物样性、可合成性和溶解度分子药代动力学性质。本文介绍了几种基于参数化量子电路工程整合到已知分子生成对抗网络中的新型生成对抗网络模型。引入的机器学习模型结合了以强化学习原理为基础的新型多参数奖励函数。通过在基准药物设计数据集QM9和PC9上进行广泛的实验，显示出引入的模型优于以前的评分。最重要的是，新的评分表明药物样性定量估计增加了最多30%。

    The contemporary drug design process demands considerable time and resources to develop each new compound entering the market. Generating small molecules is a pivotal aspect of drug discovery, essential for developing innovative pharmaceuticals. Uniqueness, validity, diversity, druglikeliness, synthesizability, and solubility molecular pharmacokinetic properties, however, are yet to be maximized. This work introduces several new generative adversarial network models based on engineering integration of parametrized quantum circuits into known molecular generative adversarial networks. The introduced machine learning models incorporate a new multi-parameter reward function grounded in reinforcement learning principles. Through extensive experimentation on benchmark drug design datasets, QM9 and PC9, the introduced models are shown to outperform scores achieved previously. Most prominently, the new scores indicate an increase of up to 30% in the druglikeness quantitative estimation. The n
    
[^147]: 在工程公司中选择正确的AI整合路径：战略指南

    Choosing the Right Path for AI Integration in Engineering Companies: A Strategic Guide

    [https://arxiv.org/abs/2402.00011](https://arxiv.org/abs/2402.00011)

    本文为EPC公司提供了一个转向AI的框架，旨在帮助他们根据自身业务战略和资源状况，以最有效的方式获取AI技术。这是基于全球最大的EPC承包商之一在基于AI的产品开发项目执行中的经验和已将AI整合到工程解决方案中的EPC供应商公司的见解。

    

    在能源领域的工程采购建设（EPC）业务中，人工智能（AI）的重要性日益被认同。许多EPC公司及其客户已意识到应用AI对业务进行自动化、提高生产力并简化未来操作的好处，以在竞争激烈的行业中保持竞争力。当前的AI市场提供了各种解决方案和服务来支持这个行业，但组织必须根据其业务战略和可用资源的情况，了解如何以最有益的方式获取AI技术。本文提出了一个面向EPC公司的转型AI框架。我们的工作基于全球最大的EPC承包商之一的基于AI的产品开发项目执行的案例，并结合已经将AI整合到其工程解决方案中的EPC供应商公司的见解。本文涵盖了整个li阶段。

    The Engineering, Procurement and Construction (EPC) businesses operating within the energy sector are recognizing the increasing importance of Artificial Intelligence (AI). Many EPC companies and their clients have realized the benefits of applying AI to their businesses in order to reduce manual work, drive productivity, and streamline future operations of engineered installations in a highly competitive industry. The current AI market offers various solutions and services to support this industry, but organizations must understand how to acquire AI technology in the most beneficial way based on their business strategy and available resources. This paper presents a framework for EPC companies in their transformation towards AI. Our work is based on examples of project execution of AI-based products development at one of the biggest EPC contractors worldwide and on insights from EPC vendor companies already integrating AI into their engineering solutions. The paper covers the entire li
    
[^148]: GD无法胜任：非可微性对神经网络训练的三种影响方式

    GD doesn't make the cut: Three ways that non-differentiability affects neural network training

    [https://arxiv.org/abs/2401.08426](https://arxiv.org/abs/2401.08426)

    本文研究了非可微性对神经网络训练的影响，包括收敛性差异、$L_1$正则化问题的矛盾性质以及稳定边界现象的不适用性。

    

    本文研究了应用于非可微函数（NGDMs）和应用于可微函数的传统梯度下降（GDs）之间的区别。首先，我们证明了NGDMs的收敛性质与GDs存在显著差异，挑战了基于$L$-光滑性的广泛神经网络收敛文献对非光滑神经网络的适用性。接下来，我们展示了NGDM解决$L_1$正则化问题的矛盾性质，表明增加正则化惩罚会导致NGDMs中最优解的$L_1$范数增加。因此，我们证明了广泛采用的基于$L_1$惩罚的网络修剪技术并未产生预期结果。最后，我们探索了稳定边界现象（Edge of Stability），指出即使对于Lipschitz连续凸可微函数，它也不适用于非凸非可微的神经网络。

    This paper investigates the distinctions between gradient methods applied to non-differentiable functions (NGDMs) and classical gradient descents (GDs) designed for differentiable functions. First, we demonstrate significant differences in the convergence properties of NGDMs compared to GDs, challenging the applicability of the extensive neural network convergence literature based on $L-smoothness$ to non-smooth neural networks. Next, we demonstrate the paradoxical nature of NGDM solutions for $L_{1}$-regularized problems, showing that increasing the regularization penalty leads to an increase in the $L_{1}$ norm of optimal solutions in NGDMs. Consequently, we show that widely adopted $L_{1}$ penalization-based techniques for network pruning do not yield expected results. Finally, we explore the Edge of Stability phenomenon, indicating its inapplicability even to Lipschitz continuous convex differentiable functions, leaving its relevance to non-convex non-differentiable neural networks
    
[^149]: 科学与深度学习中的可靠性和解释性

    Reliability and Interpretability in Science and Deep Learning

    [https://arxiv.org/abs/2401.07359](https://arxiv.org/abs/2401.07359)

    这篇论文强调了科学与深度学习中模型假设的重要性，并提供了对模型假设认识论复杂性的分析，同时结合标准错误分析与深度神经网络模型的特点，来评估模型可靠性。

    

    近年来，机器学习（ML）方法的可靠性问题日益重要，并且与此相关的不确定性分析已经激发了大量的研究。然而，大部分研究都仅将标准错误分析应用于深度神经网络（DNN）模型，这在很大程度上与标准科学建模有所不同。因此，有必要将标准错误分析与对DNN模型与标准科学建模的可能差异以及这些差异在可靠性评估中可能产生的影响的更深层次的认识论分析相结合。本文提供了几个贡献。首先，强调了模型假设（在ML和传统科学中均存在）在无理论科学的错觉下的普遍作用。其次，从（认识论的）复杂性角度分析了模型假设，同时还展示了模型假设在可靠性评估中的作用。

    In recent years, the question of the reliability of Machine Learning (ML) methods has acquired significant importance, and the analysis of the associated uncertainties has motivated a growing amount of research. However, most of these studies have applied standard error analysis to ML models, and in particular Deep Neural Network (DNN) models, which represent a rather significant departure from standard scientific modelling. It is therefore necessary to integrate the standard error analysis with a deeper epistemological analysis of the possible differences between DNN models and standard scientific modelling and the possible implications of these differences in the assessment of reliability. This article offers several contributions. First, it emphasises the ubiquitous role of model assumptions (both in ML and traditional Science) against the illusion of theory-free science. Secondly, model assumptions are analysed from the point of view of their (epistemic) complexity, which is shown 
    
[^150]: 零样本自然语言视频定位中的常识推理

    Commonsense for Zero-Shot Natural Language Video Localization

    [https://arxiv.org/abs/2312.17429](https://arxiv.org/abs/2312.17429)

    本文研究了零样本自然语言-视频定位中常识推理的有效性，并提出了CORONET框架，该框架利用常识进行视频和生成的伪查询之间的桥接。实验证明CORONET在零样本和传统NLVL方法上都取得了优越的结果。

    

    零样本自然语言-视频定位（NLVL）方法通过动态生成视频片段和伪查询注释，在仅用原始视频数据训练NLVL模型方面取得了令人期待的结果。然而，现有的伪查询经常缺乏对源视频的扎实基础，导致内容不结构化和不连贯。本文研究了零样本NLVL中常识推理的有效性。具体而言，我们提出了CORONET，一个零样本NLVL框架，通过常识增强模块桥接视频和生成的伪查询之间的差距。CORONET使用图卷积网络（GCN）来编码从知识图中提取的常识信息，条件是视频，以及交叉注意机制来增强编码视频和伪查询表示以进行定位。通过对两个基准数据集进行实证评估，我们证明CORONET在零样本和传统NLVL方法上都取得了优越的结果。

    Zero-shot Natural Language-Video Localization (NLVL) methods have exhibited promising results in training NLVL models exclusively with raw video data by dynamically generating video segments and pseudo-query annotations. However, existing pseudo-queries often lack grounding in the source video, resulting in unstructured and disjointed content. In this paper, we investigate the effectiveness of commonsense reasoning in zero-shot NLVL. Specifically, we present CORONET, a zero-shot NLVL framework that leverages commonsense to bridge the gap between videos and generated pseudo-queries via a commonsense enhancement module. CORONET employs Graph Convolution Networks (GCN) to encode commonsense information extracted from a knowledge graph, conditioned on the video, and cross-attention mechanisms to enhance the encoded video and pseudo-query representations prior to localization. Through empirical evaluations on two benchmark datasets, we demonstrate that CORONET surpasses both zero-shot and w
    
[^151]: 通过无交并的泛深度比较机器学习算法

    Comparing Machine Learning Algorithms by Union-Free Generic Depth

    [https://arxiv.org/abs/2312.12839](https://arxiv.org/abs/2312.12839)

    本研究提出了一种描述性分析偏序集合的框架，通过改进的无交并泛深度 (ufg) 比较机器学习算法，并在标准基准数据集上提供了示例。研究结果展示了基于ufg方法的多样性分析方法，并与现有的基准测试方法有很大区别。

    

    我们提出了一个基于深度函数概念的描述性分析偏序集合的框架。尽管线性空间和度量空间的研究非常深入，但关于偏序集合等非标准数据类型的深度函数的讨论几乎没有。我们介绍了一种适用于所有偏序集合的著名简单深度的改进版本，无交并泛深度 (ufg)。此外，我们利用我们的ufg深度来比较基于多维性能指标的机器学习算法。具体而言，我们提供了两个示例，对标准基准数据集的分类器比较。我们的结果有希望地展示了基于ufg方法的不同分析方法的广泛多样性。此外，这些示例说明了我们的方法与现有的基准测试方法有很大区别，因此为分类器比较的热烈讨论增添了新的视角。

    We propose a framework for descriptively analyzing sets of partial orders based on the concept of depth functions. Despite intensive studies in linear and metric spaces, there is very little discussion on depth functions for non-standard data types such as partial orders. We introduce an adaptation of the well-known simplicial depth to the set of all partial orders, the union-free generic (ufg) depth. Moreover, we utilize our ufg depth for a comparison of machine learning algorithms based on multidimensional performance measures. Concretely, we provide two examples of classifier comparisons on samples of standard benchmark data sets. Our results demonstrate promisingly the wide variety of different analysis approaches based on ufg methods. Furthermore, the examples outline that our approach differs substantially from existing benchmarking approaches, and thus adds a new perspective to the vivid debate on classifier comparison.
    
[^152]: EE-LLM: 大规模训练和推理具有三维并行性的早退出大型语言模型

    EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism

    [https://arxiv.org/abs/2312.04916](https://arxiv.org/abs/2312.04916)

    EE-LLM是一个用于大规模训练和推理早退出大型语言模型的框架，具有三维并行性和多项算法创新。研究发现EE-LLM在训练效率上表现出色，计算开销极小。

    

    我们提出了EE-LLM，这是一个用于大规模训练和推理早退出大型语言模型（LLM）的框架。虽然最近的研究已经初步证明了早退出在加速LLM推理方面的有效性，但EE-LLM通过支持大规模的三维并行性来推动早退出LLM的规模化。基于Megatron-LM构建的EE-LLM实现了各种算法创新和性能优化，以适应早退出，包括一种轻量级的方法，利用流水线并行性促进早退出训练目标的反向传播，利用原始流水线调度中的空闲资源进行与早退出层相关的计算的技术，以及两种与KV缓存兼容的早退出推理方法，用于自回归生成。我们的分析和实证研究表明，与忽略的计算开销相比，EE-LLM在训练效率上取得了很好的效果。

    We present EE-LLM, a framework for large-scale training and inference of early-exit large language models (LLMs). While recent works have shown preliminary evidence for the efficacy of early exiting in accelerating LLM inference, EE-LLM makes a foundational step towards scaling up early-exit LLMs by supporting their training and inference with massive 3D parallelism. Built upon Megatron-LM, EE-LLM implements a variety of algorithmic innovations and performance optimizations tailored to early exiting, including a lightweight method that facilitates backpropagation for the early-exit training objective with pipeline parallelism, techniques of leveraging idle resources in the original pipeline schedule for computation related to early-exit layers, and two approaches of early-exit inference that are compatible with KV caching for autoregressive generation. Our analytical and empirical study shows that EE-LLM achieves great training efficiency with negligible computational overhead compared
    
[^153]: RLHF和IIA：倒置激励

    RLHF and IIA: Perverse Incentives

    [https://arxiv.org/abs/2312.01057](https://arxiv.org/abs/2312.01057)

    RLHF算法中的IIA假设导致了倒置激励，限制了查询格式和学习算法的创新。

    

    现有的基于人类反馈的强化学习算法（RLHF）可以激励与偏好不符的回应，因为它们基于假设无关概括的模型（IIA）。IIA引发的倒置激励阻碍了查询格式和学习算法的创新。

    Existing algorithms for reinforcement learning from human feedback (RLHF) can incentivize responses at odds with preferences because they are based on models that assume independence of irrelevant alternatives (IIA). The perverse incentives induced by IIA hinder innovations on query formats and learning algorithms.
    
[^154]: 几何感知的归一化Wasserstein流在最优因果推断中的应用

    Geometry-Aware Normalizing Wasserstein Flows for Optimal Causal Inference

    [https://arxiv.org/abs/2311.18826](https://arxiv.org/abs/2311.18826)

    本文提出了一种几何感知的归一化Wasserstein流的方法，通过整合连续归一化流（CNFs）和参数子模型，优化了因果推断的表现，并在最优传输理论中提高了鲁棒性。

    

    本文通过将连续归一化流（CNFs）与参数子模型相结合，提出了一种突破性的因果推断方法，增强了它们对几何敏感性，并改进了传统的目标最大似然估计（TMLE）。我们的方法利用CNFs改进TMLE，优化Cram\'er-Rao界限，并从预定义分布$p_0$过渡到数据驱动的分布$p_1$。我们进一步创新地将Wasserstein梯度流嵌入到Fokker-Planck方程中，从而在最优传输理论中添加了几何结构，提高了CNFs的鲁棒性。

    This paper presents a groundbreaking approach to causal inference by integrating continuous normalizing flows (CNFs) with parametric submodels, enhancing their geometric sensitivity and improving upon traditional Targeted Maximum Likelihood Estimation (TMLE). Our method employs CNFs to refine TMLE, optimizing the Cram\'er-Rao bound and transitioning from a predefined distribution $p_0$ to a data-driven distribution $p_1$. We innovate further by embedding Wasserstein gradient flows within Fokker-Planck equations, thus imposing geometric structures that boost the robustness of CNFs, particularly in optimal transport theory.   Our approach addresses the disparity between sample and population distributions, a critical factor in parameter estimation bias. We leverage optimal transport and Wasserstein gradient flows to develop causal inference methodologies with minimal variance in finite-sample settings, outperforming traditional methods like TMLE and AIPW. This novel framework, centered o
    
[^155]: 关于偏置策略梯度算法的二阶收敛性研究

    On the Second-Order Convergence of Biased Policy Gradient Algorithms

    [https://arxiv.org/abs/2311.02546](https://arxiv.org/abs/2311.02546)

    该论文研究了偏置策略梯度算法的二阶收敛性，包括基于蒙特卡洛轨迹采样的普通梯度估计器和基于双循环评论家-演员算法的演员-评论家方法。实现在实际应用中的偏置主要来自于有限时间采样和对价值函数的逼近。

    

    由于强化学习问题的目标函数通常是高度非凸的，因此希望策略梯度算法能够脱离鞍点并达到二阶稳定点。现有的结果只考虑了带有无偏梯度估计器的普通策略梯度算法，但在无限时间折扣回报设置下，实际实现是有偏的，因为有限时间采样。此外，由于评论家对价值函数的逼近，评论家-演员方法的二阶收敛性也未被证实。我们提供了对有偏策略梯度方法的新颖的二阶分析，包括通过蒙特卡洛轨迹采样计算得到的普通梯度估计器，以及双循环评论家-演员算法，在内循环中，评论家通过TD(0)学习改进了对价值函数的逼近。另外，我们还证明了TD(0)的收敛性。

    Since the objective functions of reinforcement learning problems are typically highly nonconvex, it is desirable that policy gradient, the most popular algorithm, escapes saddle points and arrives at second-order stationary points. Existing results only consider vanilla policy gradient algorithms with unbiased gradient estimators, but practical implementations under the infinite-horizon discounted reward setting are biased due to finite-horizon sampling. Moreover, actor-critic methods, whose second-order convergence has not yet been established, are also biased due to the critic approximation of the value function. We provide a novel second-order analysis of biased policy gradient methods, including the vanilla gradient estimator computed from Monte-Carlo sampling of trajectories as well as the double-loop actor-critic algorithm, where in the inner loop the critic improves the approximation of the value function via TD(0) learning. Separately, we also establish the convergence of TD(0)
    
[^156]: AMAGO：可扩展的上下文强化学习适应性智能体

    AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents

    [https://arxiv.org/abs/2310.09971](https://arxiv.org/abs/2310.09971)

    AMAGO是一个可扩展的上下文强化学习智能体，使用序列模型解决了泛化、长期存储和元学习等挑战，并通过离线学习成功训练了长序列Transformer，具有强大的性能和适用性。

    

    我们介绍了AMAGO，一个上下文强化学习（RL）智能体，它使用序列模型来解决泛化、长期存储和元学习的挑战。最近的研究表明，离线学习可以使具有循环策略的上下文RL成为可能。然而，这些方法需要大量调整，并通过在智能体的内存容量、规划范围和模型大小上创建关键瓶颈而限制了可扩展性。AMAGO重新审视和重新设计了离线上下文方法，成功地训练了在RL的整个展开过程中并行地使用长序列Transformer。我们的智能体具有可扩展性，适用于各种问题，并通过元-RL和长期存储领域的实验证明了其出色的性能。AMAGO对稀疏奖励和离线数据的关注还使得上下文学习能够扩展到具有具有挑战性的探索目标问题。当与多目标延迟重新标记方案相结合时，AMAGO可以解决

    We introduce AMAGO, an in-context Reinforcement Learning (RL) agent that uses sequence models to tackle the challenges of generalization, long-term memory, and meta-learning. Recent works have shown that off-policy learning can make in-context RL with recurrent policies viable. Nonetheless, these approaches require extensive tuning and limit scalability by creating key bottlenecks in agents' memory capacity, planning horizon, and model size. AMAGO revisits and redesigns the off-policy in-context approach to successfully train long-sequence Transformers over entire rollouts in parallel with end-to-end RL. Our agent is scalable and applicable to a wide range of problems, and we demonstrate its strong performance empirically in meta-RL and long-term memory domains. AMAGO's focus on sparse rewards and off-policy data also allows in-context learning to extend to goal-conditioned problems with challenging exploration. When combined with a multi-goal hindsight relabeling scheme, AMAGO can sol
    
[^157]: InstructRetro: 检索增强的预训练中指令调优

    InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining

    [https://arxiv.org/abs/2310.07713](https://arxiv.org/abs/2310.07713)

    InstructRetro是目前规模最大的使用检索预训练的LLM，扩展了基础模型Retro 48B，通过指令调优在各种零样例任务上取得显著改进。

    

    使用检索增强技术对自回归大型语言模型（LLM）进行预训练可以提高困惑度和事实准确性。然而，现有的预训练检索增强LLM的规模仍然有限（如Retro具有75亿个参数），这限制了指令调优和零样例泛化的效果。本文介绍了Retro 48B，这是目前规模最大的使用检索预训练的LLM。具体来说，我们使用检索技术从1.2万亿个标记中继续预训练一个43B的GPT模型，并借助Retro方法将其扩展到4800亿个参数。值得注意的是，所得到的基础模型Retro 48B在困惑度方面显著优于仅使用1.2万亿个标记进行训练的43B GPT模型，且只增加了2.58%的GPU使用时间，展示了该方法的显著扩展潜力。在对Retro进行指令调优后，InstructRetro在各种零样例任务上表现出显著的改进。

    Pretraining auto-regressive large language models (LLMs) with retrieval demonstrates better perplexity and factual accuracy by leveraging external databases. However, the size of existing pretrained retrieval-augmented LLM is still limited (e.g., Retro has 7.5B parameters), which limits the effectiveness of instruction tuning and zero-shot generalization. In this work, we introduce Retro 48B, the largest LLM pretrained with retrieval. Specifically, we continue to pretrain a 43B GPT model on additional 100 billion tokens using the Retro augmentation method by retrieving from 1.2 trillion tokens. Notably, the obtained foundation model, Retro 48B, largely outperforms the counterpart GPT 43B trained on 1.2T tokens in terms of perplexity with only 2.58% additional GPU hours, demonstrating the significant scaling potential of the method. After instruction tuning on Retro, InstructRetro demonstrates significant improvement over the instruction tuned GPT on a wide range of zero-shot tasks. Spe
    
[^158]: 通过去噪扩散概率模型进行生成性量子机器学习

    Generative quantum machine learning via denoising diffusion probabilistic models

    [https://arxiv.org/abs/2310.05866](https://arxiv.org/abs/2310.05866)

    通过引入量子去噪扩散概率模型（QuDDPM），我们实现了对量子数据的高效可训练的生成学习，该模型采用足够层数的电路以保证表达能力，并引入多个中间训练任务以避免贫瘠平原并保证高效的训练。

    

    深度生成模型是计算机视觉、文本生成和大型语言模型的关键技术。最近，由于其能够生成多样化和高质量的样本，以及结构灵活、训练简单的特点，去噪扩散概率模型（DDPMs）在许多计算机视觉任务中受到了广泛关注。量子生成模型利用纠缠和叠加的能力为学习经典和量子数据带来了新的见解。受经典模型的启发，我们提出了“量子去噪扩散概率模型”（QuDDPM），以实现对量子数据的高效可训练的生成学习。QuDDPM采用足够层数的电路来保证表达能力，同时引入多个中间训练任务，将目标分布与噪声之间的插值作为训练过程，以避免贫瘠平原并保证高效的训练。我们给出了学习误差的上界和...（未完待续）

    Deep generative models are key-enabling technology to computer vision, text generation and large language models. Denoising diffusion probabilistic models (DDPMs) have recently gained much attention due to their ability to generate diverse and high-quality samples in many computer vision tasks, as well as to incorporate flexible model architectures and relatively simple training scheme. Quantum generative models, empowered by entanglement and superposition, have brought new insight to learning classical and quantum data. Inspired by the classical counterpart, we propose the \emph{quantum denoising diffusion probabilistic model} (QuDDPM) to enable efficiently trainable generative learning of quantum data. QuDDPM adopts sufficient layers of circuits to guarantee expressivity, while introduces multiple intermediate training tasks as interpolation between the target distribution and noise to avoid barren plateau and guarantee efficient training. We provide bounds on the learning error and 
    
[^159]: 关于加速SE(3)不变空间中基于扩散的分子构象生成的研究

    On Accelerating Diffusion-based Molecular Conformation Generation in SE(3)-invariant Space

    [https://arxiv.org/abs/2310.04915](https://arxiv.org/abs/2310.04915)

    本文研究在SE(3)不变空间中加速扩散机制，提出新的加速方案，可以以50倍到100倍的速度生成高质量的分子构象。

    

    在SE(3)不变空间中，基于扩散的生成模型在分子构象生成方面表现出有希望的性能，但通常需要解决具有数千个更新步骤的随机微分方程(SDEs)。迄今为止，如何在SE(3)不变空间中明确有效地加速这个过程仍然不清楚，这极大地阻碍了其在实际环境中的广泛应用。在本文中，我们通过现有方法引起的近似误差来系统研究SE(3)不变空间中的扩散机制。因此，我们在投影微分方程的上下文中发展了更精确的SE(3)内的近似方法。我们还提供了理论分析，并通过实证验证将超参数与这些误差联系起来。总体而言，我们提出了一种新的加速方案，用于在SE(3)不变空间中生成分子构象。实验结果表明，我们的方案可以以50倍到100倍的速度生成高质量的构象。

    Diffusion-based generative models in SE(3)-invariant space have demonstrated promising performance in molecular conformation generation, but typically require solving stochastic differential equations (SDEs) with thousands of update steps. Till now, it remains unclear how to effectively accelerate this procedure explicitly in SE(3)-invariant space, which greatly hinders its wide application in the real world. In this paper, we systematically study the diffusion mechanism in SE(3)-invariant space via the lens of approximate errors induced by existing methods. Thereby, we develop more precise approximate in SE(3) in the context of projected differential equations. Theoretical analysis is further provided as well as empirical proof relating hyper-parameters with such errors. Altogether, we propose a novel acceleration scheme for generating molecular conformations in SE(3)-invariant space. Experimentally, our scheme can generate high-quality conformations with 50x--100x speedup compared to
    
[^160]: 面向网络数据挖掘的跨表掩码预训练研究

    Towards Cross-Table Masked Pretraining for Web Data Mining

    [https://arxiv.org/abs/2307.04308](https://arxiv.org/abs/2307.04308)

    本论文旨在研究面向网络数据挖掘的跨表掩码预训练方法，解决了表格数据预训练中的关键挑战，显示出对挖掘网络表格数据的潜力。

    

    表格数据在全球网络中广泛存在，在支撑在线信息的数字架构中起着基础性作用。鉴于ChatGPT和SAM等大规模预训练模型在各个领域的影响力，探索将预训练技术应用于网络表格数据挖掘已成为一个极具潜力的研究方向。尽管最近有一些关于这个主题的研究，但大多数（如果不是全部）都局限于固定模式/单表的范围。由于数据集的规模和先前模型的参数大小，我们认为在普遍存在的表格数据中还没有达到“BERT时刻”。这一领域的发展明显滞后于自然语言处理等相应研究领域。在这项工作中，我们首先确定了表格数据预训练背后的关键挑战，特别是克服跨表障碍。作为一项开创性的努力，本研究展示了通过跨表预训练来挖掘网络表格数据的潜力。

    Tabular data pervades the landscape of the World Wide Web, playing a foundational role in the digital architecture that underpins online information. Given the recent influence of large-scale pretrained models like ChatGPT and SAM across various domains, exploring the application of pretraining techniques for mining tabular data on the web has emerged as a highly promising research direction. Indeed, there have been some recent works around this topic where most (if not all) of them are limited in the scope of a fixed-schema/single table. Due to the scale of the dataset and the parameter size of the prior models, we believe that we have not reached the ''BERT moment'' for the ubiquitous tabular data. The development on this line significantly lags behind the counterpart research domains such as natural language processing. In this work, we first identify the crucial challenges behind tabular data pretraining, particularly overcoming the cross-table hurdle. As a pioneering endeavor, thi
    
[^161]: 利用开放信息抽取来增强事件触发识别的领域转移鲁棒性

    Leveraging Open Information Extraction for More Robust Domain Transfer of Event Trigger Detection

    [https://arxiv.org/abs/2305.14163](https://arxiv.org/abs/2305.14163)

    本研究利用开放信息抽取系统中的主谓关系将事件触发器在不同领域间进行耦合，以提升事件触发识别的领域转移性能。在从高资源到低资源领域的转移中，该方法可减轻性能下降，特别是在从维基百科到新闻领域的转移中效果显著。同时结合遮蔽语言建模能进一步增强转移效果。

    

    事件触发识别是许多领域中关键的信息抽取任务，如维基百科或新闻。该任务通常依赖于触发识别（TD）—识别文本中引起特定事件的标记范围。尽管触发器的概念应理想地适用于所有领域，但从高资源领域到低资源领域的TD领域转移会导致性能显著下降。我们通过使用基于规则的开放信息抽取（OIE）系统获取的主谓关系将触发器在领域之间进行耦合来解决TD中的负转移问题。我们证明通过多任务训练注入的OIE关系可以作为不同领域触发器之间的中介，增强零样本和少样本的TD领域转移，并减少性能下降，特别是从高资源源领域（维基百科）转移到低资源目标领域（新闻）。此外，我们将改进的转移与遮蔽语言建模结合起来。

    Event detection is a crucial information extraction task in many domains, such as Wikipedia or news. The task typically relies on trigger detection (TD) -- identifying token spans in the text that evoke specific events. While the notion of triggers should ideally be universal across domains, domain transfer for TD from high- to low-resource domains results in significant performance drops. We address the problem of negative transfer in TD by coupling triggers between domains using subject-object relations obtained from a rule-based open information extraction (OIE) system. We demonstrate that OIE relations injected through multi-task training can act as mediators between triggers in different domains, enhancing zero- and few-shot TD domain transfer and reducing performance drops, in particular when transferring from a high-resource source domain (Wikipedia) to a low(er)-resource target domain (news). Additionally, we combine this improved transfer with masked language modeling on the t
    
[^162]: 生成式机器学习方法用于多变量集合后处理

    Generative machine learning methods for multivariate ensemble post-processing

    [https://arxiv.org/abs/2211.01345](https://arxiv.org/abs/2211.01345)

    本研究提出了一种基于生成式机器学习的多变量集合后处理方法，通过生成神经网络直接输出多变量预测分布的样本，解决了两步方法难以包含附加预测变量的问题。

    

    基于多次运行数值天气预报模型的集合天气预报通常会显示系统误差，并需要进行后处理才能获得可靠的预报。在许多实际应用中准确建模多变量依赖关系至关重要，因此提出了各种多变量后处理方法，其中集合预测首先在每个边际上分别进行后处理，然后通过copulas恢复多变量依赖关系。这些两步方法存在共同的主要局限性，特别是在建模依赖关系时难以包含附加的预测变量。我们提出了一种基于生成式机器学习的创新多变量后处理方法来解决这些挑战。在这种新的非参数化数据驱动分布回归模型中，多变量预测分布的样本直接作为生成神经网络的输出得到。通过优化适当的目标函数训练生成模型。

    Ensemble weather forecasts based on multiple runs of numerical weather prediction models typically show systematic errors and require post-processing to obtain reliable forecasts. Accurately modeling multivariate dependencies is crucial in many practical applications, and various approaches to multivariate post-processing have been proposed where ensemble predictions are first post-processed separately in each margin and multivariate dependencies are then restored via copulas. These two-step methods share common key limitations, in particular the difficulty to include additional predictors in modeling the dependencies. We propose a novel multivariate post-processing method based on generative machine learning to address these challenges. In this new class of nonparametric data-driven distributional regression models, samples from the multivariate forecast distribution are directly obtained as output of a generative neural network. The generative model is trained by optimizing a proper 
    
[^163]: FORESEE:使用扩展压缩无香克特变换进行在线策略优化的预测

    FORESEE: Prediction with Expansion-Compression Unscented Transform for Online Policy Optimization

    [https://arxiv.org/abs/2209.12644](https://arxiv.org/abs/2209.12644)

    本研究引入了扩展压缩无香克特变换的状态预测方法，在解决在线策略优化问题时能够以较低的计算成本与蒙特卡洛方法相媲美。

    

    通过一个通用的、不确定的非线性动态模型传播状态分布被认为是难以处理的，通常会引发数值或分析近似。我们介绍了一种称为扩展压缩无香克特变换的状态预测方法，并将其用于解决一类在线策略优化问题。我们提出的算法通过一个状态相关的分布传播有限数量的西格玛点，这在每个时间步骤都需要增加西格玛点的数量来表示生成的分布；这就是我们所谓的扩展操作。为了保持算法的可扩展性，我们将扩展操作与基于矩匹配的压缩操作相结合，从而在多个时间步骤上保持西格玛点的数量恒定。通过实验证明，该算法的性能与蒙特卡洛方法相当，但计算成本更低。在状态和控制输入约束下，状态预测表现出良好的效果。

    Propagating state distributions through a generic, uncertain nonlinear dynamical model is known to be intractable and usually begets numerical or analytical approximations. We introduce a method for state prediction, called the Expansion-Compression Unscented Transform, and use it to solve a class of online policy optimization problems. Our proposed algorithm propagates a finite number of sigma points through a state-dependent distribution, which dictates an increase in the number of sigma points at each time step to represent the resulting distribution; this is what we call the expansion operation. To keep the algorithm scalable, we augment the expansion operation with a compression operation based on moment matching, thereby keeping the number of sigma points constant across predictions over multiple time steps. Its performance is empirically shown to be comparable to Monte Carlo but at a much lower computational cost. Under state and control input constraints, the state prediction i
    
[^164]: 前馈潜在领域自适应

    Feed-Forward Latent Domain Adaptation

    [https://arxiv.org/abs/2207.07624](https://arxiv.org/abs/2207.07624)

    本文研究了前馈潜在领域自适应的问题，提出了一种通过元学习和交叉注意力实现动态适应的方法，该方法在资源受限的边缘设备上取得了一致的改进。

    

    我们研究了一种新的高度实用的问题设置，使资源受限的边缘设备能够适应其本地数据分布的预训练模型。认识到设备的数据很可能来自包含混合未标记的领域相关和领域不相关示例的多个潜在领域，我们专注于相对较少研究的潜在领域自适应问题。考虑到边缘设备的局限性，我们目标是仅使用预训练模型，并以前馈方式进行适应，而无需使用反向传播和无需访问源数据。建模这些现实约束将我们带到前馈潜在领域自适应的新颖和实用重要的问题设置。我们的解决方案是元学习一个网络，能够将混合相关目标数据集嵌入，并使用交叉注意力动态地适应目标示例的推理。所得到的框架相对于强ERM基线方法能够持续改进。

    We study a new highly-practical problem setting that enables resource-constrained edge devices to adapt a pre-trained model to their local data distributions. Recognizing that device's data are likely to come from multiple latent domains that include a mixture of unlabelled domain-relevant and domain-irrelevant examples, we focus on the comparatively under-studied problem of latent domain adaptation. Considering limitations of edge devices, we aim to only use a pre-trained model and adapt it in a feed-forward way, without using back-propagation and without access to the source data. Modelling these realistic constraints bring us to the novel and practically important problem setting of feed-forward latent domain adaptation. Our solution is to meta-learn a network capable of embedding the mixed-relevance target dataset and dynamically adapting inference for target examples using cross-attention. The resulting framework leads to consistent improvements over strong ERM baselines. We also 
    
[^165]: 在医疗保健中实现公平的机器学习：一项综述

    Fair Machine Learning in Healthcare: A Review

    [https://arxiv.org/abs/2206.14397](https://arxiv.org/abs/2206.14397)

    这篇综述论文研究了机器学习在医疗保健领域的公平问题。通过采用基于分配公正原则的框架，将公平问题分为资源平均分配和性能平等两个类别，并从机器学习的角度对相关的公正度量进行了批判性的回顾。论文还讨论了机器学习生命周期各个阶段的偏见和缓解策略，探讨了偏见与其对策之间的关系。

    

    医疗数据的数字化与计算能力的进步推动了机器学习在医疗保健领域的应用。然而，这些方法可能会加剧或甚至加重现有的差异，导致资源不均和不同人群之间的诊断准确性不一致等公平问题。解决这些公平问题对于防止社会不公正的进一步巩固至关重要。在本综述中，我们分析了机器学习与医疗保健不公平的交叉点。我们采用了一个基于分配公正原则的框架，将公平问题分为两个不同的类别：资源平均分配和性能平等。我们从机器学习的角度对相关的公正度量进行了批判性的回顾，并讨论了机器学习生命周期各个阶段的偏见和缓解策略，探讨了偏见与其对策之间的关系。

    The digitization of healthcare data coupled with advances in computational capabilities has propelled the adoption of machine learning (ML) in healthcare. However, these methods can perpetuate or even exacerbate existing disparities, leading to fairness concerns such as the unequal distribution of resources and diagnostic inaccuracies among different demographic groups. Addressing these fairness problem is paramount to prevent further entrenchment of social injustices. In this survey, we analyze the intersection of fairness in machine learning and healthcare disparities. We adopt a framework based on the principles of distributive justice to categorize fairness concerns into two distinct classes: equal allocation and equal performance. We provide a critical review of the associated fairness metrics from a machine learning standpoint and examine biases and mitigation strategies across the stages of the ML lifecycle, discussing the relationship between biases and their countermeasures. T
    
[^166]: 图上的协同似然比估计

    Collaborative likelihood-ratio estimation over graphs

    [https://arxiv.org/abs/2205.14461](https://arxiv.org/abs/2205.14461)

    该论文提出了基于图的协同似然比估计方法，通过考虑图结构信息，估计每个节点间的似然比，节点可以协作来更高效地解决问题。

    

    假设我们有来自两个未知概率密度函数 (pdfs) p 和 q 的独立同分布的观测值，似然比估计（LRE）是一种优雅的方法，只依靠现有数据来比较这两个概率密度函数。在本文中，我们介绍了目前为止首个基于图的扩展问题，其假设固定图的每个节点 v 都可以访问来自两个未知节点特定概率密度函数 p_v 和 q_v 的观测值，并且目标是通过同时考虑图结构提供的信息来估计每个节点之间的似然比。节点级别的估计任务应该展现出图传递的相似性，这暗示着节点可以协作来更有效地解决它们。我们以一个具体的非参数方法 GRULSIF 来开发这个想法。我们推导了我们方法的收敛速度。

    Assuming we have iid observations from two unknown probability density functions (pdfs), $p$ and $q$, the likelihood-ratio estimation (LRE) is an elegant approach to compare the two pdfs only by relying on the available data. In this paper, we introduce the first -to the best of our knowledge-graph-based extension of this problem, which reads as follows: Suppose each node $v$ of a fixed graph has access to observations coming from two unknown node-specific pdfs, $p_v$ and $q_v$, and the goal is to estimate for each node the likelihood-ratio between both pdfs by also taking into account the information provided by the graph structure. The node-level estimation tasks are supposed to exhibit similarities conveyed by the graph, which suggests that the nodes could collaborate to solve them more efficiently. We develop this idea in a concrete non-parametric method that we call Graph-based Relative Unconstrained Least-squares Importance Fitting (GRULSIF). We derive convergence rates for our c
    
[^167]: 对对抗训练中过度参数化的诅咒：随机特征回归的鲁棒泛化的精确分析

    The curse of overparametrization in adversarial training: Precise analysis of robust generalization for random features regression

    [https://arxiv.org/abs/2201.05149](https://arxiv.org/abs/2201.05149)

    本文精确分析了对对抗训练中过度参数化的影响，发现过度参数化模型对微小对抗扰动非常脆弱，显示了鲁棒泛化的性能明显差于标准泛化的性能。

    

    成功的深度学习模型通常涉及训练神经网络架构，其参数数量超过训练样本的数量。过度参数化模型在最近几年中得到了广泛研究，过度参数化的优点从统计学角度（通过双下降现象）和计算角度（通过优化景观的结构特性）已经得到建立。尽管过度参数化的深度学习架构取得了显著的成功，但众所周知，这些模型对其输入中的微小对抗扰动非常脆弱。即使在经过对抗训练的情况下，它们在被扰动的输入上的性能（鲁棒泛化）也明显比在良性输入上的最佳性能（标准泛化）要差。因此，了解过度参数化如何从根本上影响鲁棒性至关重要。

    Successful deep learning models often involve training neural network architectures that contain more parameters than the number of training samples. Such overparametrized models have been extensively studied in recent years, and the virtues of overparametrization have been established from both the statistical perspective, via the double-descent phenomenon, and the computational perspective via the structural properties of the optimization landscape.   Despite the remarkable success of deep learning architectures in the overparametrized regime, it is also well known that these models are highly vulnerable to small adversarial perturbations in their inputs. Even when adversarially trained, their performance on perturbed inputs (robust generalization) is considerably worse than their best attainable performance on benign inputs (standard generalization). It is thus imperative to understand how overparametrization fundamentally affects robustness.   In this paper, we will provide a preci
    
[^168]: 概率生成函数核在球面数据中的应用

    Probability-Generating Function Kernels for Spherical Data

    [https://arxiv.org/abs/2112.00365](https://arxiv.org/abs/2112.00365)

    该论文介绍了一种在球面数据分析中应用的概率生成函数核，扩展了RBF核并引入了半参数学习算法。

    

    引入概率生成函数（PGF）核，构成了一类在单位超球上支持的核，用于球面数据分析。PGF核在球面数据的背景下推广了RBF核。研究了PGF核的特性。引入了半参数学习算法，使得可以在球面数据中使用PGF核。

    Probability-generating function (PGF) kernels are introduced, which constitute a class of kernels supported on the unit hypersphere, for the purposes of spherical data analysis. PGF kernels generalize RBF kernels in the context of spherical data. The properties of PGF kernels are studied. A semi-parametric learning algorithm is introduced to enable the use of PGF kernels with spherical data.
    
[^169]: 对于随机非凸优化问题，一种新的自适应算法的理论和实证研究，带有附加动量步骤和平移更新

    A theoretical and empirical study of new adaptive algorithms with additional momentum steps and shifted updates for stochastic non-convex optimization

    [https://arxiv.org/abs/2110.08531](https://arxiv.org/abs/2110.08531)

    本文介绍了一种新的自适应算法，用于解决随机非凸优化问题，并展示了加速方法与AMSGrad类型动量方法之间的深层关联。

    

    已知自适应优化算法是机器学习领域蓬勃发展的关键。在优化文献中，许多研究都致力于加速梯度法，但只有最近才从理论角度对自适应迭代技术进行了分析。本文介绍了一种新的带有动量项的自适应算法，用于解决随机非凸优化问题。我们的目的是展示加速方法与AMSGrad类型动量方法之间的深层关联。我们的方法基于随机和可能非凸目标映射的框架，以及在自适应算法研究中经常使用的一些假设。除了讨论有限时间内与特定最终迭代相关的分析以及到达稳定点的几乎确定收敛性外，我们还将关注最坏情况迭代复杂度。

    It is known that adaptive optimization algorithms represent the key pillar behind the rise of the Machine Learning field. In the Optimization literature numerous studies have been devoted to accelerated gradient methods but only recently adaptive iterative techniques were analyzed from a theoretical point of view. In the present paper we introduce new adaptive algorithms endowed with momentum terms for stochastic non-convex optimization problems. Our purpose is to show a deep connection between accelerated methods endowed with different inertial steps and AMSGrad-type momentum methods. Our methodology is based on the framework of stochastic and possibly non-convex objective mappings, along with some assumptions that are often used in the investigation of adaptive algorithms. In addition to discussing the finite-time horizon analysis in relation to a certain final iteration and the almost sure convergence to stationary points, we shall also look at the worst-case iteration complexity. T
    
[^170]: 基于矩阵值时间序列的在线图拓扑学习

    Online Graph Topology Learning from Matrix-valued Time Series

    [https://arxiv.org/abs/2107.08020](https://arxiv.org/abs/2107.08020)

    本文通过研究矩阵值时间序列的统计分析，提出了在线图拓扑学习的方法。首先，将VAR模型扩展为矩阵变量模型以适用于图形学习。其次，提出了两种在线过程，针对低维和高维情况快速更新系数的估计。这些方法在高维情况下引入了一种新的Lasso-type进行拓扑处理。

    

    本文研究了矩阵值时间序列的统计分析。这些数据是在一个传感器网络上收集的（通常是一组空间位置），观测到每个传感器的每个时间点的特征向量。因此，每个传感器由一个向量时序列来描述。我们希望识别这些传感器之间的依赖结构，并用图形来表示它。当每个传感器只有一个特征时，矢量自回归模型已被广泛应用于推断格兰杰因果关系的结构。所得到的图被称为因果图。我们的第一个贡献是将VAR模型扩展为矩阵变量模型，以用于图形学习的目的。其次，我们提出了两种在线过程，分别适用于低维和高维情况，在新样本到达时可以快速更新系数的估计。特别是在高维情况下，引入了一种新的Lasso-type，并对其进行了拓扑处理。

    This paper is concerned with the statistical analysis of matrix-valued time series. These are data collected over a network of sensors (typically a set of spatial locations) along time, where a vector of features is observed per time instant per sensor. Thus each sensor is characterized by a vectorial time series. We would like to identify the dependency structure among these sensors and represent it by a graph. When there is only one feature per sensor, the vector auto-regressive models have been widely adapted to infer the structure of Granger causality. The resulting graph is referred to as causal graph. Our first contribution is then extending VAR models to matrix-variate models to serve the purpose of graph learning. Secondly, we propose two online procedures respectively in low and high dimensions, which can update quickly the estimates of coefficients when new samples arrive. In particular in high dimensional regime, a novel Lasso-type is introduced and we develop its homotopy a
    
[^171]: 从零开始构建一个大型语言模型

    Engineering A Large Language Model From Scratch. (arXiv:2401.16736v1 [cs.CL])

    [http://arxiv.org/abs/2401.16736](http://arxiv.org/abs/2401.16736)

    Atinuke是一种基于Transformer的神经网络，通过在处理时序数据的层与注意机制交织在一起，模拟人类语言，从而优化各种语言任务的性能。

    

    深度学习在自然语言处理（NLP）领域的普及导致了能够理解和生成人类语言的创新技术的开发和发布。Atinuke是一种基于Transformer的神经网络，通过利用独特的配置，在各种语言任务上优化性能。该架构通过将处理时序数据的层与注意机制交织在一起，从而在输入和输出之间建立有意义的关联。由于其拓扑结构和超参数调整的配置，它可以提取特征并学习复杂的映射，从而模仿人类语言。Atinuke是模块化、可扩展的，并可以与现有的机器学习流程无缝集成。softmax、嵌入和多头注意力等高级矩阵操作使得对文本、声音和视觉信号的细致处理成为可能。通过将现代深度学习技术与软件设计原则和数学方法相结合

    The proliferation of deep learning in natural language processing (NLP) has led to the development and release of innovative technologies capable of understanding and generating human language with remarkable proficiency. Atinuke, a Transformer-based neural network, optimises performance across various language tasks by utilising a unique configuration. The architecture interweaves layers for processing sequential data with attention mechanisms to draw meaningful affinities between inputs and outputs. Due to the configuration of its topology and hyperparameter tuning, it can emulate human-like language by extracting features and learning complex mappings. Atinuke is modular, extensible, and integrates seamlessly with existing machine learning pipelines. Advanced matrix operations like softmax, embeddings, and multi-head attention enable nuanced handling of textual, acoustic, and visual signals. By unifying modern deep learning techniques with software design principles and mathematical
    
[^172]: OptiState：基于门控网络、Transformer视觉和卡尔曼滤波的腿式机器人状态估计

    OptiState: State Estimation of Legged Robots using Gated Networks with Transformer-based Vision and Kalman Filtering. (arXiv:2401.16719v1 [cs.RO])

    [http://arxiv.org/abs/2401.16719](http://arxiv.org/abs/2401.16719)

    本研究提出了一种名为OptiState的腿式机器人状态估计方法，该方法通过整合Kalman滤波、优化和学习模式的混合解决方案，结合本体感和外感信息，以精确估计机器人主体的状态。该方法利用关节编码器、IMU测量和基于凸规划的模型预测控制优化，通过Gate循环单元和Vision Transformer自编码器改进了估计结果。研究结果表明，该方法能够提供准确的机器人状态估计，并减小传感器测量和模型简化引起的非线性误差。

    

    由于腿式机器人的高动态运动和传感器精度的局限性，腿式机器人的状态估计具有挑战性。通过整合卡尔曼滤波、优化和基于学习的模态，我们提出了一种混合解决方案，结合了本体感和外感信息，用于估计机器人主体的状态。借助关节编码器和IMU测量，我们的卡尔曼滤波器通过单刚体模型进行增强，该模型还结合了基于凸规划的模型预测控制优化的接地反力控制输出。通过门控循环单元进一步改进估计结果，该方法还考虑了从深度图像上应用视觉Transformer自编码器获得的语义洞察和机器人高度。该框架不仅提供准确的机器人状态估计，包括不确定性评估，还可以通过学习来减小传感器测量和模型简化引起的非线性误差。所提出的方法经过评估。

    State estimation for legged robots is challenging due to their highly dynamic motion and limitations imposed by sensor accuracy. By integrating Kalman filtering, optimization, and learning-based modalities, we propose a hybrid solution that combines proprioception and exteroceptive information for estimating the state of the robot's trunk. Leveraging joint encoder and IMU measurements, our Kalman filter is enhanced through a single-rigid body model that incorporates ground reaction force control outputs from convex Model Predictive Control optimization. The estimation is further refined through Gated Recurrent Units, which also considers semantic insights and robot height from a Vision Transformer autoencoder applied on depth images. This framework not only furnishes accurate robot state estimates, including uncertainty evaluations, but can minimize the nonlinear errors that arise from sensor measurements and model simplifications through learning. The proposed methodology is evaluated
    
[^173]: 允许混淆的LiNGAM的泛化

    Generalization of LiNGAM that allows confounding. (arXiv:2401.16661v1 [cs.LG])

    [http://arxiv.org/abs/2401.16661](http://arxiv.org/abs/2401.16661)

    本文提出了一种名为LiNGAM-MMI的方法，可以增强LiNGAM模型以处理混淆问题。该方法使用KL散度量化混淆程度，并通过最短路径问题解决方案高效地确定变量顺序，不论是否存在混淆情况。实验证明，LiNGAM-MMI可以更准确地识别正确的变量顺序。

    

    LiNGAM使用加性噪声模型来确定因果关系的变量顺序，但在混淆方面面临挑战。先前的方法在保持LiNGAM的基本结构的同时，试图识别和处理受混淆影响的变量。结果是，不论是否存在混淆，这些方法都需要大量的计算资源，并且不能确保检测到所有的混淆类型。相比之下，本文通过引入LiNGAM-MMI对LiNGAM进行了增强，该方法使用KL散度量化混淆程度，并安排变量以最小化其影响。该方法通过最短路径问题的形式高效地实现全局最优的变量顺序。在无混淆的情况下，LiNGAM-MMI的处理数据效率与传统LiNGAM相当，同时有效处理混淆情况。我们的实验结果表明，LiNGAM-MMI更准确地确定了正确的变量顺序...

    LiNGAM determines the variable order from cause to effect using additive noise models, but it faces challenges with confounding. Previous methods maintained LiNGAM's fundamental structure while trying to identify and address variables affected by confounding. As a result, these methods required significant computational resources regardless of the presence of confounding, and they did not ensure the detection of all confounding types. In contrast, this paper enhances LiNGAM by introducing LiNGAM-MMI, a method that quantifies the magnitude of confounding using KL divergence and arranges the variables to minimize its impact. This method efficiently achieves a globally optimal variable order through the shortest path problem formulation. LiNGAM-MMI processes data as efficiently as traditional LiNGAM in scenarios without confounding while effectively addressing confounding situations. Our experimental results suggest that LiNGAM-MMI more accurately determines the correct variable order, bo
    
[^174]: 一种多级对称微分方程模型用于学习蛋白质-配体结合动力学

    A Multi-Grained Symmetric Differential Equation Model for Learning Protein-Ligand Binding Dynamics. (arXiv:2401.15122v1 [cs.LG])

    [http://arxiv.org/abs/2401.15122](http://arxiv.org/abs/2401.15122)

    提出了一种能够促进数值MD模拟并有效模拟蛋白质-配体结合动力学的NeuralMD方法，采用物理信息多级对称框架，实现了准确建模多级蛋白质-配体相互作用。

    

    在药物发现中，蛋白质-配体结合的分子动力学（MD）模拟提供了一种强大的工具，用于预测结合亲和力，估计运输性能和探索口袋位点。通过改进数值方法以及最近通过机器学习（ML）方法增强MD模拟的效率已经有了很长的历史。然而，仍然存在一些挑战，例如准确建模扩展时间尺度的模拟。为了解决这个问题，我们提出了NeuralMD，这是第一个能够促进数值MD并提供准确的蛋白质-配体结合动力学模拟的ML辅助工具。我们提出了一个合理的方法，将一种新的物理信息多级对称框架纳入模型中。具体而言，我们提出了（1）一个使用向量框架满足群对称性并捕获多级蛋白质-配体相互作用的BindingNet模型，以及（2）一个增强的神经微分方程求解器，学习轨迹的演化。

    In drug discovery, molecular dynamics (MD) simulation for protein-ligand binding provides a powerful tool for predicting binding affinities, estimating transport properties, and exploring pocket sites. There has been a long history of improving the efficiency of MD simulations through better numerical methods and, more recently, by augmenting them with machine learning (ML) methods. Yet, challenges remain, such as accurate modeling of extended-timescale simulations. To address this issue, we propose NeuralMD, the first ML surrogate that can facilitate numerical MD and provide accurate simulations of protein-ligand binding dynamics. We propose a principled approach that incorporates a novel physics-informed multi-grained group symmetric framework. Specifically, we propose (1) a BindingNet model that satisfies group symmetry using vector frames and captures the multi-level protein-ligand interactions, and (2) an augmented neural differential equation solver that learns the trajectory und
    
[^175]: Hi-Core: 面向连续强化学习的层次化知识迁移

    Hi-Core: Hierarchical Knowledge Transfer for Continual Reinforcement Learning. (arXiv:2401.15098v1 [cs.LG])

    [http://arxiv.org/abs/2401.15098](http://arxiv.org/abs/2401.15098)

    Hi-Core提出了一种新的框架，通过层次化的知识迁移来增强连续强化学习。该框架包括利用大型语言模型的推理能力设定目标的高层策略制定和通过强化学习按照高层目标导向的低层策略学习。在实验中，Hi-Core展现了较强的知识迁移能力。

    

    连续强化学习（Continual Reinforcement Learning, CRL）赋予强化学习智能体从一系列任务中学习的能力，保留先前的知识并利用它来促进未来的学习。然而，现有的方法往往专注于在类似任务之间传输低层次的知识，忽视了人类认知控制的层次结构，导致在各种任务之间的知识迁移不足。为了增强高层次的知识迁移，我们提出了一种名为Hi-Core (Hierarchical knowledge transfer for Continual reinforcement learning)的新框架，它由两层结构组成：1) 利用大型语言模型（Large Language Model, LLM）的强大推理能力设定目标的高层策略制定和2) 通过强化学习按照高层目标导向的低层策略学习。此外，构建了一个知识库（策略库）来存储可以用于层次化知识迁移的策略。在MiniGr实验中进行了实验。

    Continual reinforcement learning (CRL) empowers RL agents with the ability to learn from a sequence of tasks, preserving previous knowledge and leveraging it to facilitate future learning. However, existing methods often focus on transferring low-level knowledge across similar tasks, which neglects the hierarchical structure of human cognitive control, resulting in insufficient knowledge transfer across diverse tasks. To enhance high-level knowledge transfer, we propose a novel framework named Hi-Core (Hierarchical knowledge transfer for Continual reinforcement learning), which is structured in two layers: 1) the high-level policy formulation which utilizes the powerful reasoning ability of the Large Language Model (LLM) to set goals and 2) the low-level policy learning through RL which is oriented by high-level goals. Moreover, the knowledge base (policy library) is constructed to store policies that can be retrieved for hierarchical knowledge transfer. Experiments conducted in MiniGr
    
[^176]: 《规范预测集提升人类决策能力》

    Conformal Prediction Sets Improve Human Decision Making. (arXiv:2401.13744v1 [cs.LG])

    [http://arxiv.org/abs/2401.13744](http://arxiv.org/abs/2401.13744)

    该研究表明，通过规范预测量化模型的不确定性，可以提高人类决策的准确性和效果，对人机协同决策具有实用价值。

    

    作为对日常查询的回应，人类明确地表达不确定性，并在不确定的情况下提供替代答案。通过规范预测输出校准的预测集，模仿了人类的这种行为；更大的预测集表示更大的不确定性，同时提供了替代方案。在这项工作中，我们通过实施预注册的随机对照试验，并给人类受试者提供规范预测集，研究了规范预测集对人类决策的实用性。通过统计学显著性，我们发现当人类获得规范预测集时，他们在任务上的准确性比使用相同覆盖保证的固定尺寸预测集时有所提高。结果表明，用规范预测量化模型的不确定性有助于人机协同决策和人工智能团队的决策。

    In response to everyday queries, humans explicitly signal uncertainty and offer alternative answers when they are unsure. Machine learning models that output calibrated prediction sets through conformal prediction mimic this human behaviour; larger sets signal greater uncertainty while providing alternatives. In this work, we study the usefulness of conformal prediction sets as an aid for human decision making by conducting a pre-registered randomized controlled trial with conformal prediction sets provided to human subjects. With statistical significance, we find that when humans are given conformal prediction sets their accuracy on tasks improves compared to fixed-size prediction sets with the same coverage guarantee. The results show that quantifying model uncertainty with conformal prediction is helpful for human-in-the-loop decision making and human-AI teams.
    
[^177]: 在资源受限的异步联邦学习系统中减轻系统偏差

    Mitigating System Bias in Resource Constrained Asynchronous Federated Learning Systems. (arXiv:2401.13366v1 [cs.LG])

    [http://arxiv.org/abs/2401.13366](http://arxiv.org/abs/2401.13366)

    该论文提出了一种在资源受限的异步联邦学习系统中减轻系统偏差的动态全局模型聚合方法，通过根据客户端的上传频率评分和调整模型更新的权重，以适应异构设备和非同分布数据的挑战。实验结果表明，在仿真环境中，与最先进的方法相比，该方法在全局模型准确性上有显著的改善。

    

    联邦学习系统在处理异构设备和客户端非同分布数据时面临性能挑战。我们提出一种动态全局模型聚合方法来解决这些问题。我们的聚合方法根据客户端的上传频率对其模型更新进行评分和调整权重，以适应设备能力的差异。此外，我们还在客户端上传本地模型后立即向其提供更新的全局模型，以减少空闲时间并提高训练效率。我们在一个包含10个模拟客户端的异步联邦学习部署中评估了我们的方法，这些客户端具有异构计算约束和非IID数据。使用FashionMNIST数据集的仿真结果表明，与最先进的方法PAPAYA和FedAsync相比，全局模型的准确性改善了10%和19%。我们的动态聚合方法可以实现可靠的全局模型更新。

    Federated learning (FL) systems face performance challenges in dealing with heterogeneous devices and non-identically distributed data across clients. We propose a dynamic global model aggregation method within Asynchronous Federated Learning (AFL) deployments to address these issues. Our aggregation method scores and adjusts the weighting of client model updates based on their upload frequency to accommodate differences in device capabilities. Additionally, we also immediately provide an updated global model to clients after they upload their local models to reduce idle time and improve training efficiency. We evaluate our approach within an AFL deployment consisting of 10 simulated clients with heterogeneous compute constraints and non-IID data. The simulation results, using the FashionMNIST dataset, demonstrate over 10% and 19% improvement in global model accuracy compared to state-of-the-art methods PAPAYA and FedAsync, respectively. Our dynamic aggregation method allows reliable g
    
[^178]: 强化学习代理中的新兴支配等级

    Emergent Dominance Hierarchies in Reinforcement Learning Agents. (arXiv:2401.12258v1 [cs.MA])

    [http://arxiv.org/abs/2401.12258](http://arxiv.org/abs/2401.12258)

    本研究在强化学习中探讨了一种新的支配等级现象，并证明了在没有明确编程和内在奖励的情况下，强化学习代理能够自主发明、学习、实施和传递支配等级给新的群体。

    

    现代强化学习算法在各种任务中能够胜过人类。多智能体强化学习(MARL)设置提出了额外的挑战，成功的混合动机代理协作取决于个体和群体目标之间的微妙平衡。社会习惯和规范，往往受到人类机构的启发，被用作实现这种平衡的工具。在本文中，我们研究了一种基本且经过深入研究的社会习惯，即支配等级，它在动物和人类社会中都存在。我们将支配等级的行为理论应用于人工智能代理，并尽可能少地修改现有的术语和定义。我们证明，在没有明确编程或内在奖励的情况下，强化学习代理的群体能够发明、学习、实施和传递支配等级给新的群体。所产生的支配等级有一个

    Modern Reinforcement Learning (RL) algorithms are able to outperform humans in a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings present additional challenges, and successful cooperation in mixed-motive groups of agents depends on a delicate balancing act between individual and group objectives. Social conventions and norms, often inspired by human institutions, are used as tools for striking this balance.  In this paper, we examine a fundamental, well-studied social convention that underlies cooperation in both animal and human societies: Dominance hierarchies.  We adapt the ethological theory of dominance hierarchies to artificial agents, borrowing the established terminology and definitions with as few amendments as possible. We demonstrate that populations of RL agents, operating without explicit programming or intrinsic rewards, can invent, learn, enforce, and transmit a dominance hierarchy to new populations. The dominance hierarchies that emerge have a 
    
[^179]: 基于高斯混合模型和负高斯混合梯度的扩散模型条件设计

    Diffusion Model Conditioning on Gaussian Mixture Model and Negative Gaussian Mixture Gradient. (arXiv:2401.11261v1 [cs.LG])

    [http://arxiv.org/abs/2401.11261](http://arxiv.org/abs/2401.11261)

    本研究提出了一种利用高斯混合模型作为特征条件来引导扩散模型的去噪过程的条件机制，并通过实验证实了基于特征的条件潜在分布能够产生更少的缺陷生成。

    

    扩散模型（DMs）是一种对图像合成和其他领域有巨大影响的生成模型。它们在各种生成任务中取得了最先进的生成结果。可以使用各种多样的条件输入，如文本或边界框，来控制生成过程。本研究中，我们提出了一种使用高斯混合模型（GMM）作为特征条件来引导去噪过程的条件机制。基于集合论，我们提供了一种全面的理论分析，表明基于特征和类别的条件潜在分布显著不同，因此基于特征的条件潜在分布比基于类别的条件潜在分布产生更少的缺陷生成。分别训练了两个基于高斯混合模型的扩散模型进行比较。实验证实了我们的发现。我们提出了一种新的梯度函数，称为负高斯混合梯度（NGMG），并应用于扩散模型训练中。

    Diffusion models (DMs) are a type of generative model that has a huge impact on image synthesis and beyond. They achieve state-of-the-art generation results in various generative tasks. A great diversity of conditioning inputs, such as text or bounding boxes, are accessible to control the generation. In this work, we propose a conditioning mechanism utilizing Gaussian mixture models (GMMs) as feature conditioning to guide the denoising process. Based on set theory, we provide a comprehensive theoretical analysis that shows that conditional latent distribution based on features and classes is significantly different, so that conditional latent distribution on features produces fewer defect generations than conditioning on classes. Two diffusion models conditioned on the Gaussian mixture model are trained separately for comparison. Experiments support our findings. A novel gradient function called the negative Gaussian mixture gradient (NGMG) is proposed and applied in diffusion model tr
    
[^180]: Langevin遗忘：噪声梯度下降的机器遗忘新视角

    Langevin Unlearning: A New Perspective of Noisy Gradient Descent for Machine Unlearning. (arXiv:2401.10371v1 [cs.LG])

    [http://arxiv.org/abs/2401.10371](http://arxiv.org/abs/2401.10371)

    Langevin遗忘是一种基于噪声梯度下降的遗忘框架，能够在近似遗忘问题中提供隐私保证，并且具有算法上的优势。

    

    随着采用确保“被遗忘权”的法律，机器遗忘引起了极大的兴趣。研究人员提供了一个概率性的近似遗忘定义，类似于差分隐私（DP）的定义，其中隐私被定义为对重新训练的统计不可区分性。我们提出了Langevin遗忘，这是一个基于噪声梯度下降的近似遗忘问题的隐私保证的遗忘框架。Langevin遗忘在算法上统一了DP学习过程和隐私认证的遗忘过程。其中包括非凸问题的近似认证遗忘，相对于重新训练的复杂度节省，以及用于多个遗忘请求的顺序和批量遗忘。我们通过在基准数据集上的实验验证了Langevin遗忘的实用性，并展示了它对梯度下降的优势。

    Machine unlearning has raised significant interest with the adoption of laws ensuring the ``right to be forgotten''. Researchers have provided a probabilistic notion of approximate unlearning under a similar definition of Differential Privacy (DP), where privacy is defined as statistical indistinguishability to retraining from scratch. We propose Langevin unlearning, an unlearning framework based on noisy gradient descent with privacy guarantees for approximate unlearning problems. Langevin unlearning unifies the DP learning process and the privacy-certified unlearning process with many algorithmic benefits. These include approximate certified unlearning for non-convex problems, complexity saving compared to retraining, sequential and batch unlearning for multiple unlearning requests. We verify the practicality of Langevin unlearning by studying its privacy-utility-complexity trade-off via experiments on benchmark datasets, and also demonstrate its superiority against gradient-decent-p
    
[^181]: 物理约束卷积神经网络用于时空偏微分方程中的反问题

    Physics-constrained convolutional neural networks for inverse problems in spatiotemporal partial differential equations. (arXiv:2401.10306v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2401.10306](http://arxiv.org/abs/2401.10306)

    本研究提出了一种物理约束卷积神经网络（PC-CNN），用于解决非线性且时空变化的偏微分方程中的两种反问题。该网络可以揭示受偏差影响的真实状态，并在给定稀疏信息的情况下以高分辨率重建解。

    

    我们提出了一种物理约束卷积神经网络（PC-CNN）来解决偏微分方程中两种类型的反问题，这些方程在空间和时间上都是非线性且变化的。在第一个反问题中，我们给出了受空间变化的系统误差（即偏差，也称为认识不确定性）偏移的数据。任务是从偏差数据中揭示真实状态，即PDE的解。在第二个反问题中，我们给出了PDE解的稀疏信息。任务是以高分辨率重建空间中的解。首先，我们介绍了PC-CNN，它通过简单的时间窗口方案约束PDE来处理时序数据。其次，我们分析了PC-CNN在从偏差数据中揭示解的性能。我们分析了线性和非线性对流扩散方程以及纳维-斯托克斯方程，后者描述了湍流流动的时空混沌动力学。

    We propose a physics-constrained convolutional neural network (PC-CNN) to solve two types of inverse problems in partial differential equations (PDEs), which are nonlinear and vary both in space and time. In the first inverse problem, we are given data that is offset by spatially varying systematic error (i.e., the bias, also known the epistemic uncertainty). The task is to uncover from the biased data the true state, which is the solution of the PDE. In the second inverse problem, we are given sparse information on the solution of a PDE. The task is to reconstruct the solution in space with high-resolution. First, we present the PC-CNN, which constrains the PDE with a simple time-windowing scheme to handle sequential data. Second, we analyse the performance of the PC-CNN for uncovering solutions from biased data. We analyse both linear and nonlinear convection-diffusion equations, and the Navier-Stokes equations, which govern the spatiotemporally chaotic dynamics of turbulent flows. W
    
[^182]: 走向异质图学习：进展与未来

    Towards Learning from Graphs with Heterophily: Progress and Future. (arXiv:2401.09769v1 [cs.SI])

    [http://arxiv.org/abs/2401.09769](http://arxiv.org/abs/2401.09769)

    本调查综合概述了关于从带有异质性的图中学习的现有研究，并根据学习策略、模型架构和实际应用等方面对方法进行了分类。同时讨论了现有研究的主要挑战，并提出了未来研究的潜在方向。

    

    图是用来模拟现实世界实体之间复杂关系的结构化数据。最近，异质图，其中连接的节点往往具有不同的标签或不同的特征，引起了广泛关注并找到了许多应用。与此同时，人们也在不断努力推进从异质图中学习。虽然有关该主题的调查存在，但它们只关注于异质图神经网络（GNNs），而忽略了异质图学习的其他子主题。在本调查中，我们全面回顾了关于从带有异质性的图中学习的现有研究。首先，我们收集了180多篇论文，介绍了该领域的发展。然后，我们根据层次分类法对现有方法进行了系统分类，包括学习策略、模型架构和实际应用。最后，我们讨论了现有研究的主要挑战，并突出了未来研究的潜在方向。

    Graphs are structured data that models complex relations between real-world entities. Heterophilous graphs, where linked nodes are prone to be with different labels or dissimilar features, have recently attracted significant attention and found many applications. Meanwhile, increasing efforts have been made to advance learning from heterophilous graphs. Although there exist surveys on the relevant topic, they focus on heterophilous GNNs, which are only sub-topics of heterophilous graph learning. In this survey, we comprehensively overview existing works on learning from graphs with heterophily.First, we collect over 180 publications and introduce the development of this field. Then, we systematically categorize existing methods based on a hierarchical taxonomy including learning strategies, model architectures and practical applications. Finally, we discuss the primary challenges of existing studies and highlight promising avenues for future research.More publication details and corres
    
[^183]: 隐私保护下自适应实验中CATE的差分隐私估计

    Differentially Private Estimation of CATE in Adaptive Experiment. (arXiv:2401.08224v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2401.08224](http://arxiv.org/abs/2401.08224)

    本研究提出在自适应实验中使用差分隐私估计条件平均处理效果(CATE)，平衡社会福利损失和统计功率，并通过匹配上下界以及帕累托最优性概念来获得最优解。

    

    自适应实验广泛应用于临床试验和其他场景中估计条件平均处理效果(CATE)。虽然实验的主要目标是最大化估计精度，但由于社会福利的要求，为患者提供具有优越结果的治疗也是至关重要的，这可以通过环境批次框架中的遗憾来衡量。这两个目标经常导致对比优化分配机制。此外，在包含敏感数据（如患者健康记录）的临床场景中出现隐私问题。因此，治疗分配机制必须纳入强大的隐私保护措施。在本文中，我们研究了环境批次实验中社会福利损失和统计功率之间的权衡。我们为多目标优化问题提出了匹配的上下界，并采用帕累托最优性的概念来数学地刻画最优解。

    Adaptive experiment is widely adopted to estimate conditional average treatment effect (CATE) in clinical trials and many other scenarios. While the primary goal in experiment is to maximize estimation accuracy, due to the imperative of social welfare, it's also crucial to provide treatment with superior outcomes to patients, which is measured by regret in contextual bandit framework. These two objectives often lead to contrast optimal allocation mechanism. Furthermore, privacy concerns arise in clinical scenarios containing sensitive data like patients health records. Therefore, it's essential for the treatment allocation mechanism to incorporate robust privacy protection measures. In this paper, we investigate the tradeoff between loss of social welfare and statistical power in contextual bandit experiment. We propose a matched upper and lower bound for the multi-objective optimization problem, and then adopt the concept of Pareto optimality to mathematically characterize the optimal
    
[^184]: 使用精调的源分离器合奏混音音乐以改善助听器音质

    Remixing Music for Hearing Aids Using Ensemble of Fine-Tuned Source Separators. (arXiv:2401.06203v1 [eess.AS])

    [http://arxiv.org/abs/2401.06203](http://arxiv.org/abs/2401.06203)

    本文介绍了一个使用精调的源分离器合奏混音音乐以改善助听器音质的系统，该系统在Cadenza ICASSP 2024大挑战中名列第一，并在评估数据集上获得了最佳的助听器音质指数（HAAQI）平均分数。

    

    本文介绍了我们在Cadenza ICASSP 2024大挑战中的系统提交，该大挑战提出了为助听器用户混音和增强音乐的问题。我们的系统在挑战中名列第一，在评估数据集上获得了最佳的助听器音质指数（HAAQI）平均分数。我们描述了该系统，它使用了一组经过细调的深度学习音乐源分离器，这些分离器在挑战数据上进行了细调。我们通过挑战结果证明了我们系统的有效性，并通过消融研究分析了不同系统方面的重要性。

    This paper introduces our system submission for the Cadenza ICASSP 2024 Grand Challenge, which presents the problem of remixing and enhancing music for hearing aid users. Our system placed first in the challenge, achieving the best average Hearing-Aid Audio Quality Index (HAAQI) score on the evaluation data set. We describe the system, which uses an ensemble of deep learning music source separators that are fine tuned on the challenge data. We demonstrate the effectiveness of our system through the challenge results and analyze the importance of different system aspects through ablation studies.
    
[^185]: 可解释概念瓶颈用于对齐强化学习代理

    Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents. (arXiv:2401.05821v1 [cs.LG])

    [http://arxiv.org/abs/2401.05821](http://arxiv.org/abs/2401.05821)

    SCoBots是一种可解释的概念瓶颈代理，能够透明化整个决策流程，帮助领域专家理解和规范强化学习代理的行为，从而可能实现更好的人类对齐强化学习。

    

    奖励稀疏性、难以归因的问题以及不对齐等等都是深度强化学习代理学习最优策略困难甚至不可能的原因之一。不幸的是，深度网络的黑盒特性阻碍了领域专家的参与，这些专家可以解释模型并纠正错误行为。为此，我们引入了连续概念瓶颈代理（SCoBots），通过整合连续的概念瓶颈层，使整个决策流程透明化。SCoBots不仅利用相关的对象属性，还利用关系概念。实验结果强有力地证明，SCoBots使领域专家能够有效理解和规范他们的行为，从而可能实现更好的人类对齐强化学习。通过这种方式，SCoBots使我们能够识别最简单且具有代表性的视频游戏Pong中的不对齐问题，并加以解决。

    Reward sparsity, difficult credit assignment, and misalignment are only a few of the many issues that make it difficult, if not impossible, for deep reinforcement learning (RL) agents to learn optimal policies. Unfortunately, the black-box nature of deep networks impedes the inclusion of domain experts who could interpret the model and correct wrong behavior. To this end, we introduce Successive Concept Bottlenecks Agents (SCoBots), which make the whole decision pipeline transparent via the integration of consecutive concept bottleneck layers. SCoBots make use of not only relevant object properties but also of relational concepts. Our experimental results provide strong evidence that SCoBots allow domain experts to efficiently understand and regularize their behavior, resulting in potentially better human-aligned RL. In this way, SCoBots enabled us to identify a misalignment problem in the most simple and iconic video game, Pong, and resolve it.
    
[^186]: 加强扩散相关光谱学中的血流评估：一种带有噪声鲁棒性分析的迁移学习方法

    Enhancing Blood Flow Assessment in Diffuse Correlation Spectroscopy: A Transfer Learning Approach with Noise Robustness Analysis. (arXiv:2401.05580v1 [cs.LG])

    [http://arxiv.org/abs/2401.05580](http://arxiv.org/abs/2401.05580)

    本研究提出了一种迁移学习方法，用于增强扩散相关光谱学中的血流评估，并通过噪声鲁棒性分析展示了其鲁棒性。

    

    扩散相关光谱学（DCS）是一种新兴的非侵入性技术，通过使用近红外相干点光源照射来检测光谱变化来测量组织血流。尽管机器学习已经显示出测量血流指数（BFi）的巨大潜力，但一个有关该方法成功与否的问题是其在涉及不同信噪比（SNR）和各种不同临床应用和设置的数据集之间的偏差方面的鲁棒性。本研究提出了一种迁移学习方法，旨在评估SNR对学习特征的泛化能力的影响，并展示迁移学习的鲁棒性。使用不同添加噪声水平的合成数据集来模拟不同的SNR。所提出的网络以1x64的自相关曲线为输入，并生成BFi和相关参数beta。所提出的模型表现出极好的性能。

    Diffuse correlation spectroscopy (DCS) is an emerging noninvasive technique that measures the tissue blood flow, by using near-infrared coherent point-source illumination to detect spectral changes. While machine learning has demonstrated significant potential for measuring blood flow index (BFi), an open question concerning the success of this approach pertains to its robustness in scenarios involving deviations between datasets with varying Signal-to-Noise Ratios (SNRs) originating from diverse clinical applications and various setups. This study proposes a transfer learning approach, aims to assess the influence of SNRs on the generalization ability of learned features, and demonstrate the robustness for transfer learning. A synthetic dataset with varying levels of added noise is utilized to simulate different SNRs. The proposed network takes a 1x64 autocorrelation curve as input and generates BFi and the correlation parameter beta. The proposed model demonstrates excellent performa
    
[^187]: 通过极限学习机快速分析脑血流

    Fast Cerebral Blood Flow Analysis via Extreme Learning Machine. (arXiv:2401.05578v1 [cs.LG])

    [http://arxiv.org/abs/2401.05578](http://arxiv.org/abs/2401.05578)

    本论文提出了一种通过极限学习机快速精确分析脑血流的方法，并通过与现有算法的综合比较验证了其优越性。它展示了强大的泛化能力，在各种噪声和光学参数下都具有更高的准确性。同时，与计算效率高的神经网络相比，该方法具有较短的训练和推理时间。这种策略适用于在线训练的边缘计算应用。

    

    我们引入一种快速精确的分析方法，利用扩散相关光谱学（DCS）和极限学习机（ELM）来分析脑血流（CBF）。我们评估了ELM和现有算法，并使用综合指标对这些算法进行了比较。我们使用合成数据集对半无穷和多层模型进行了评估。结果表明，在各种噪声水平和光学参数下，ELM始终具有更高的准确性，展示了强大的泛化能力，并优于迭代拟合算法。通过与计算效率高的神经网络进行比较，ELM获得了可比较的准确性，同时减少了训练和推理时间。值得注意的是，在ELM的训练过程中，没有反向传播过程，导致训练速度比现有的神经网络方法更快。这种提出的策略在在线训练的边缘计算应用中具有潜力。

    We introduce a rapid and precise analytical approach for analyzing cerebral blood flow (CBF) using Diffuse Correlation Spectroscopy (DCS) with the application of the Extreme Learning Machine (ELM). Our evaluation of ELM and existing algorithms involves a comprehensive set of metrics. We assess these algorithms using synthetic datasets for both semi-infinite and multi-layer models. The results demonstrate that ELM consistently achieves higher fidelity across various noise levels and optical parameters, showcasing robust generalization ability and outperforming iterative fitting algorithms. Through a comparison with a computationally efficient neural network, ELM attains comparable accuracy with reduced training and inference times. Notably, the absence of a back-propagation process in ELM during training results in significantly faster training speeds compared to existing neural network approaches. This proposed strategy holds promise for edge computing applications with online training
    
[^188]: 通过切换机制，在扩散模型中实现公平抽样

    Fair Sampling in Diffusion Models through Switching Mechanism. (arXiv:2401.03140v1 [cs.LG])

    [http://arxiv.org/abs/2401.03140](http://arxiv.org/abs/2401.03140)

    本论文提出了一种称为“属性切换”的公平抽样机制，用于解决扩散模型中公平性的问题。通过在生成的数据中混淆敏感属性，该方法能够实现生成公平数据和保持数据效用的目标。

    

    扩散模型通过良好逼近潜在概率分布，在生成任务中展现了高效性。然而，扩散模型在公平性方面受到训练数据的内在偏差的放大。尽管扩散模型的抽样过程可以通过条件引导来控制，但之前的研究试图找到实证引导来实现定量公平性。为了解决这个限制，我们提出了一种称为“属性切换”机制的具有公平意识的抽样方法，用于扩散模型。在不需要额外训练的情况下，所提出的抽样方法可以在生成的数据中混淆敏感属性，而不依赖分类器。我们在两个关键方面从数学上证明了和实验证明了所提方法的有效性：(i)生成公平数据和(ii)保持生成数据的效用。

    Diffusion models have shown their effectiveness in generation tasks by well-approximating the underlying probability distribution. However, diffusion models are known to suffer from an amplified inherent bias from the training data in terms of fairness. While the sampling process of diffusion models can be controlled by conditional guidance, previous works have attempted to find empirical guidance to achieve quantitative fairness. To address this limitation, we propose a fairness-aware sampling method called \textit{attribute switching} mechanism for diffusion models. Without additional training, the proposed sampling can obfuscate sensitive attributes in generated data without relying on classifiers. We mathematically prove and experimentally demonstrate the effectiveness of the proposed method on two key aspects: (i) the generation of fair data and (ii) the preservation of the utility of the generated data.
    
[^189]: Stack Overflow回答中信息高亮的初探

    A First Look at Information Highlighting in Stack Overflow Answers. (arXiv:2401.01472v1 [cs.CL])

    [http://arxiv.org/abs/2401.01472](http://arxiv.org/abs/2401.01472)

    本论文进行了首次大规模的探索性研究，研究了Stack Overflow回答中的信息高亮。通过使用神经网络架构，开发了自动推荐突出内容的方法。

    

    背景：浏览Stack Overflow（SO）的知识仍然具有挑战性。为了使帖子对用户更生动，SO允许用户使用Markdown或HTML编写和编辑帖子，以便用户可以利用各种格式化样式（例如粗体、斜体和代码）来突出重要信息。然而，关于突出信息的研究仍然有限。目标：我们在最近的研究中进行了首次大规模的探索性研究，研究了SO回答中的信息高亮。为了扩展我们之前的研究，我们利用最初设计用于命名实体识别任务的神经网络架构，开发了自动推荐带有格式化样式的突出内容的方法。方法：本文研究了Stack Overflow的31,169,429个回答。为了训练推荐模型，我们选择了CNN和BERT模型，针对每种格式化类型（即粗体、斜体、代码和标题）使用我们从SO回答收集的突出信息数据集。

    Context: Navigating the knowledge of Stack Overflow (SO) remains challenging. To make the posts vivid to users, SO allows users to write and edit posts with Markdown or HTML so that users can leverage various formatting styles (e.g., bold, italic, and code) to highlight the important information. Nonetheless, there have been limited studies on the highlighted information. Objective: We carried out the first large-scale exploratory study on the information highlighted in SO answers in our recent study. To extend our previous study, we develop approaches to automatically recommend highlighted content with formatting styles using neural network architectures initially designed for the Named Entity Recognition task. Method: In this paper, we studied 31,169,429 answers of Stack Overflow. For training recommendation models, we choose CNN and BERT models for each type of formatting (i.e., Bold, Italic, Code, and Heading) using the information highlighting dataset we collected from SO answers.
    
[^190]: 边缘网络效率的深度学习架构

    Deep Learning Architecture for Network-Efficiency at the Edge. (arXiv:2311.05739v3 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2311.05739](http://arxiv.org/abs/2311.05739)

    本文提出了一种自适应压缩感知分离学习方法，将深度学习模型与边缘云资源集成，从而实现网络高效性和更快的计算速度，适用于部署在较弱设备上。

    

    移动设备上越来越多的人工智能应用导致了将深度学习模型与现有边缘云资源集成的解决方案；由于其在设备能耗、延迟、网络利用和隐私改进等方面的多重好处，将深度学习模型在分布式环境中分离并计算的分离学习已成为一个广泛研究的领域。结合对通信数据进行压缩的压缩感知方法，该方法的好处进一步提高，可以作为传统方法（如联邦学习方法）的替代方案。在本文中，我们开发了一种自适应压缩感知分离学习方法（'deprune'），以改善和训练深度学习模型，使其更加网络高效（使用更少的网络资源和更快），这将使它们成为在较弱设备上部署的理想选择。

    The growing number of AI-driven applications in the mobile devices has led to solutions that integrate deep learning models with the available edge-cloud resources; due to multiple benefits such as reduction in on-device energy consumption, improved latency, improved network usage, and certain privacy improvements, split learning, where deep learning models are split away from the mobile device and computed in a distributed manner, has become an extensively explored topic. Combined with compression-aware methods where learning adapts to compression of communicated data, the benefits of this approach have further improved and could serve as an alternative to established approaches like federated learning methods. In this work, we develop an adaptive compression-aware split learning method ('deprune') to improve and train deep learning models so that they are much more network-efficient (use less network resources and are faster), which would make them ideal to deploy in weaker devices w
    
[^191]: 隐式流形高斯过程回归

    Implicit Manifold Gaussian Process Regression. (arXiv:2310.19390v1 [stat.ML])

    [http://arxiv.org/abs/2310.19390](http://arxiv.org/abs/2310.19390)

    本文提出了一种能够从数据中直接推断隐式结构的高斯过程回归技术，能够处理高维数据，并可能改善预测性能和校准。

    

    高斯过程回归因其能够提供良好校准的不确定性估计和处理小型或稀疏数据集的能力而被广泛应用。然而，对于高维数据，它存在一定困难。一种将这种技术扩展到更高维度的可能途径是利用数据实际所处的隐式低维流形，这是流形假设所假定的。先前的工作通常要求显式提供流形结构，即由网格或已知为众所周知的流形之一（如球体）给出。相比之下，在本文中，我们提出了一种能够以完全可微的方式从数据（标记和未标记的）中推断出隐式结构的高斯过程回归技术。对于得到的模型，我们讨论了其在假设流形上收敛于Matérn高斯过程。我们的技术可扩展到数十万个数据点，并且可能改善预测性能和校准。

    Gaussian process regression is widely used because of its ability to provide well-calibrated uncertainty estimates and handle small or sparse datasets. However, it struggles with high-dimensional data. One possible way to scale this technique to higher dimensions is to leverage the implicit low-dimensional manifold upon which the data actually lies, as postulated by the manifold hypothesis. Prior work ordinarily requires the manifold structure to be explicitly provided though, i.e. given by a mesh or be known to be one of the well-known manifolds like the sphere. In contrast, in this paper we propose a Gaussian process regression technique capable of inferring implicit structure directly from data (labeled and unlabeled) in a fully differentiable way. For the resulting model, we discuss its convergence to the Mat\'ern Gaussian process on the assumed manifold. Our technique scales up to hundreds of thousands of data points, and may improve the predictive performance and calibration of t
    
[^192]: 带权重剪裁的差分隐私梯度下降方法

    DP-SGD with weight clipping. (arXiv:2310.18001v1 [cs.LG])

    [http://arxiv.org/abs/2310.18001](http://arxiv.org/abs/2310.18001)

    本研究提出了一种带权重剪裁的差分隐私梯度下降方法，通过利用公共信息对全局模型进行改进，获得更精确的灵敏度界限和噪声水平调整，提供了更好的差分隐私保证。

    

    最近，由于深度神经网络和其他依赖于目标函数优化的方法的高度流行，以及对数据隐私的关注，差分隐私梯度下降方法引起了极大的兴趣。为了在提供最小噪声的情况下实现差分隐私保证，能够准确地限制参与者将观察到的信息的灵敏度非常重要。在本研究中，我们提出了一种新颖的方法，弥补了传统梯度剪裁产生的偏差。通过利用关于当前全局模型及其在搜索领域中位置的公共信息，我们可以获得改进的梯度界限，从而实现更精确的灵敏度确定和噪声水平调整。我们扩展了最先进的算法，提供了更好的差分隐私保证，需要更少的噪声，并进行了实证评估。

    Recently, due to the popularity of deep neural networks and other methods whose training typically relies on the optimization of an objective function, and due to concerns for data privacy, there is a lot of interest in differentially private gradient descent methods. To achieve differential privacy guarantees with a minimum amount of noise, it is important to be able to bound precisely the sensitivity of the information which the participants will observe. In this study, we present a novel approach that mitigates the bias arising from traditional gradient clipping. By leveraging public information concerning the current global model and its location within the search domain, we can achieve improved gradient bounds, leading to enhanced sensitivity determinations and refined noise level adjustments. We extend the state of the art algorithms, present improved differential privacy guarantees requiring less noise and present an empirical evaluation.
    
[^193]: 解决A/B测试中数据训练循环引起的干扰：一种加权训练方法

    Tackling Interference Induced by Data Training Loops in A/B Tests: A Weighted Training Approach. (arXiv:2310.17496v1 [stat.ME])

    [http://arxiv.org/abs/2310.17496](http://arxiv.org/abs/2310.17496)

    该论文提出了一种名为加权训练的方法，用于解决A/B测试中由数据训练循环引起的干扰。该方法通过训练模型预测每个数据点在实验组或控制组中出现的概率，并在模型训练过程中应用加权损失，从而实现了最小方差的估计结果，并且不会引起训练分布的变化。

    

    在现代推荐系统中，标准流程涉及使用历史数据训练机器学习模型来预测用户行为并持续改进推荐。然而，这些数据训练循环可能在A/B测试中引入干扰，其中控制组和实验组算法生成的数据，可能具有不同的分布，被合并在一起。为了解决这些挑战，我们提出了一种新颖的方法，称为加权训练。该方法包括训练一个模型来预测每个数据点出现在实验组或控制组数据中的概率，并在模型训练过程中应用加权损失。我们通过模拟研究证明了这种方法在所有估计量中具有最小的方差，且不会导致训练分布发生变化。我们通过模拟研究证明了与其他方法相比，我们的方法具有较低的偏差和方差。

    In modern recommendation systems, the standard pipeline involves training machine learning models on historical data to predict user behaviors and improve recommendations continuously. However, these data training loops can introduce interference in A/B tests, where data generated by control and treatment algorithms, potentially with different distributions, are combined. To address these challenges, we introduce a novel approach called weighted training. This approach entails training a model to predict the probability of each data point appearing in either the treatment or control data and subsequently applying weighted losses during model training. We demonstrate that this approach achieves the least variance among all estimators without causing shifts in the training distributions. Through simulation studies, we demonstrate the lower bias and variance of our approach compared to other methods.
    
[^194]: 通过子网络注入归纳偏置

    Instilling Inductive Biases with Subnetworks. (arXiv:2310.10899v1 [cs.LG])

    [http://arxiv.org/abs/2310.10899](http://arxiv.org/abs/2310.10899)

    通过子网络注入归纳偏置，这项研究探索了理解和控制神经网络行为的方法。通过发现功能子网络并利用它们，可以显著减少训练模型所需的数据量。

    

    尽管人工神经网络在各种任务上取得了最近的成功，但对于这些模型的精确解决方案，我们几乎没有知识或控制能力。注入归纳偏置--对一些解决方案偏好--是理解和控制这些模型行为的一个有前景的途径。已经进行了大量工作来研究模型固有的归纳偏置，并通过手动设计的结构或精心策划的训练方式注入不同的归纳偏置。在这项工作中，我们探索了一种更机械的方法：子任务归纳。我们的方法发现了一个在训练模型中实现特定子任务的功能子网络，并使用它来注入对利用该子任务的解决方案的归纳偏置。子任务归纳灵活高效，在两个实验中我们证明了它的有效性。

    Despite the recent success of artificial neural networks on a variety of tasks, we have little knowledge or control over the exact solutions these models implement. Instilling inductive biases -- preferences for some solutions over others -- into these models is one promising path toward understanding and controlling their behavior. Much work has been done to study the inherent inductive biases of models and instill different inductive biases through hand-designed architectures or carefully curated training regimens. In this work, we explore a more mechanistic approach: Subtask Induction. Our method discovers a functional subnetwork that implements a particular subtask within a trained model and uses it to instill inductive biases towards solutions utilizing that subtask. Subtask Induction is flexible and efficient, and we demonstrate its effectiveness with two experiments. First, we show that Subtask Induction significantly reduces the amount of training data required for a model to a
    
[^195]: 一种仅解码器的时间序列预测基础模型

    A decoder-only foundation model for time-series forecasting. (arXiv:2310.10688v1 [cs.CL])

    [http://arxiv.org/abs/2310.10688](http://arxiv.org/abs/2310.10688)

    本论文介绍了一种基于补丁解码器式注意力模型的时间序列预测基础模型，该模型在零样本情况下在各种公共数据集上的性能接近最先进的监督预测模型。

    

    受到自然语言处理中大型语言模型的最新进展的启发，我们设计了一种用于预测的时间序列基础模型，其在各种公共数据集上的开箱即用的零样本性能接近每个个别数据集上最先进的监督预测模型的准确性。我们的模型基于在大型时间序列语料库上预训练的补丁解码器式注意力模型，并可以适用于不同的预测历史长度、预测长度和时间粒度。

    Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.
    
[^196]: 随机梯度下降的噪声几何：定量和分析特征的研究

    The Noise Geometry of Stochastic Gradient Descent: A Quantitative and Analytical Characterization. (arXiv:2310.00692v1 [cs.LG])

    [http://arxiv.org/abs/2310.00692](http://arxiv.org/abs/2310.00692)

    本文对随机梯度下降（SGD）中的噪声几何进行了全面的理论研究，发现噪声与损失函数的局部几何特征有利的一致性。通过实验证明，SGD在逃脱尖锐极小值时与GD形成鲜明对比，逃脱方向在平坦方向上有显著分量。

    

    实证研究表明，随机梯度下降（SGD）中的噪声与损失函数的局部几何特征有利的一致性。然而，对于这种现象的理论和定量解释仍然不足。本文对过参数化线性模型和两层神经网络的上述“噪声几何”进行了全面的理论研究。我们细致地研究了平均和方向的一致性，特别关注样本大小和输入数据退化对一致性强度的影响。作为特定应用，我们利用噪声几何特征研究了SGD如何从尖锐极小值中逃脱，发现逃脱方向在平坦方向上有显著分量，这与只在最尖锐方向逃脱的梯度下降方法GD形成鲜明对比。为了验证我们的理论发现，我们进行了合成和真实世界的实验。

    Empirical studies have demonstrated that the noise in stochastic gradient descent (SGD) aligns favorably with the local geometry of loss landscape. However, theoretical and quantitative explanations for this phenomenon remain sparse. In this paper, we offer a comprehensive theoretical investigation into the aforementioned {\em noise geometry} for over-parameterized linear (OLMs) models and two-layer neural networks. We scrutinize both average and directional alignments, paying special attention to how factors like sample size and input data degeneracy affect the alignment strength. As a specific application, we leverage our noise geometry characterizations to study how SGD escapes from sharp minima, revealing that the escape direction has significant components along flat directions. This is in stark contrast to GD, which escapes only along the sharpest directions. To substantiate our theoretical findings, both synthetic and real-world experiments are provided.
    
[^197]: SELF：基于语言驱动的大型语言模型自主进化

    SELF: Language-Driven Self-Evolution for Large Language Model. (arXiv:2310.00533v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.00533](http://arxiv.org/abs/2310.00533)

    SELF提出了一种基于语言驱动的创新方法，允许大型语言模型（LLM）不断自我进化，并通过语言反馈作为评估工具来改进模型的响应能力和训练稳定性。

    

    大型语言模型（LLM）展示了在多个领域中的卓越适应能力。然而，实现人类水平的学习和推动自主人工智能的关键——模型自主进化的路径仍然未知。我们引入了一种创新的方法，名为"SELF"（带有语言反馈的自主进化）。这种方法使LLM能够不断地自我进化。此外，SELF利用语言反馈作为一种多功能、全面的评估工具，精确定位响应改进的领域，并提高自主进化训练的稳定性。SELF首先进行元技能学习，专注于自我反馈和自我精炼。这些元技能是关键，引导模型在自制数据的持续训练周期中进行后续的自我进化，从而增强其内在能力。在给定无标签指令的情况下，SELF使模型具备了能够...

    Large Language Models (LLMs) have showcased remarkable versatility across diverse domains. However, the pathway toward autonomous model development, a cornerstone for achieving human-level learning and advancing autonomous AI, remains largely uncharted. We introduce an innovative approach, termed "SELF" (Self-Evolution with Language Feedback). This methodology empowers LLMs to undergo continual self-evolution. Furthermore, SELF employs language-based feedback as a versatile and comprehensive evaluative tool, pinpointing areas for response refinement and bolstering the stability of self-evolutionary training. Initiating with meta-skill learning, SELF acquires foundational meta-skills with a focus on self-feedback and self-refinement. These meta-skills are critical, guiding the model's subsequent self-evolution through a cycle of perpetual training with self-curated data, thereby enhancing its intrinsic abilities. Given unlabeled instructions, SELF equips the model with the capability to
    
[^198]: HyperMask: 自适应的基于超网络的掩码用于持续学习

    HyperMask: Adaptive Hypernetwork-based Masks for Continual Learning. (arXiv:2310.00113v1 [cs.LG])

    [http://arxiv.org/abs/2310.00113](http://arxiv.org/abs/2310.00113)

    HyperMask是一种用于持续学习的方法，它使用基于超网络的掩码来训练一个单一网络，以克服人工神经网络在多任务上的灾难性遗忘问题。

    

    当人工神经网络在多个任务上顺序训练时，往往会出现灾难性遗忘的问题。为了克服这个问题，已经存在许多持续学习策略，其中最有效的之一是基于超网络的方法。超网络根据任务的特征生成目标模型的权重。然而，该模型的主要限制是超网络对于每个任务可以产生完全不同的网络结构，因此每个任务都是单独解决的。模型在学习后续任务时不使用之前任务所关联的网络信息，并实际上产生了新的网络架构。为了解决这个问题，我们使用了彩票票证假设，该假设认为存在稀疏的子网络（即中奖票），可以保持完整网络的性能。在本文中，我们提出了一种名为HyperMask的方法，该方法为所有任务训练一个单一网络。超网络产生半二进制掩码，以获取目标子网络。

    Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, there exist many continual learning strategies. One of the most effective is the hypernetwork-based approach. The hypernetwork generates the weights of a target model based on the task's identity. The model's main limitation is that hypernetwork can produce completely different nests for each task. Consequently, each task is solved separately. The model does not use information from the network dedicated to previous tasks and practically produces new architectures when it learns the subsequent tasks. To solve such a problem, we use the lottery ticket hypothesis, which postulates the existence of sparse subnetworks, named winning tickets, that preserve the performance of a full network.  In the paper, we propose a method called HyperMask, which trains a single network for all tasks. Hypernetwork produces semi-binary masks to obtain target subnetw
    
[^199]: 因果推理：为下一代AI本地化无线网络开辟革命性道路

    Causal Reasoning: Charting a Revolutionary Course for Next-Generation AI-Native Wireless Networks. (arXiv:2309.13223v1 [cs.IT])

    [http://arxiv.org/abs/2309.13223](http://arxiv.org/abs/2309.13223)

    本文提出了一种基于因果推理的框架，用于构建AI本地化的无线网络，以应对现有“AI for wireless”范式的短板。该框架通过解决AI模型的黑匣子特性、曲线拟合特性、对大量训练数据的依赖以及大型神经网络的能量效率低下等问题，克服了数据驱动型、训练密集型AI的局限性。

    

    尽管基本前提是下一代无线网络（例如6G）将是人工智能（AI）本地化的，但到目前为止，大多数现有的工作仍然要么是定性的，要么是对现有“AI用于无线”范式的增量扩展。实际上，创建AI本地化的无线网络面临着重要的技术挑战，因为数据驱动型、训练密集型的AI的局限性。这些限制包括AI模型的黑匣子特性、它们的曲线拟合特性（这可能限制它们的推理和适应能力）、它们对大量训练数据的依赖以及大型神经网络的能量效率低下等。作为对这些限制的回应，本文提出了一个全面的、具有前瞻性的愿景，通过引入一个基于因果推理的新框架来解决这些缺点。该框架基于因果发现、因果表示学习和因果推断。

    Despite the basic premise that next-generation wireless networks (e.g., 6G) will be artificial intelligence (AI)-native, to date, most existing efforts remain either qualitative or incremental extensions to existing ``AI for wireless'' paradigms. Indeed, creating AI-native wireless networks faces significant technical challenges due to the limitations of data-driven, training-intensive AI. These limitations include the black-box nature of the AI models, their curve-fitting nature, which can limit their ability to reason and adapt, their reliance on large amounts of training data, and the energy inefficiency of large neural networks. In response to these limitations, this article presents a comprehensive, forward-looking vision that addresses these shortcomings by introducing a novel framework for building AI-native wireless networks; grounded in the emerging field of causal reasoning. Causal reasoning, founded on causal discovery, causal representation learning, and causal inference, c
    
[^200]: 通过优势调节使用动态规划增强决策Transformer

    ACT: Empowering Decision Transformer with Dynamic Programming via Advantage Conditioning. (arXiv:2309.05915v1 [cs.LG])

    [http://arxiv.org/abs/2309.05915](http://arxiv.org/abs/2309.05915)

    这篇论文提出了一种通过将动态规划应用于决策Transformer来增强其能力的方法。作者提出了三个步骤来实现这一目标：使用样本内值迭代获得近似值函数，结合估计的优势评估动作质量，并训练ACT生成基于估计优势的动作。该方法在测试中表现出良好的性能。

    

    决策Transformer (DT) 利用表达丰富的序列建模技术来执行动作生成，已成为离线策略优化的一种有前景的方法。然而，DT 生成的动作是基于期望未来回报的条件，已知具有某些弱点，比如易受环境随机性影响。为了克服DT的弱点，我们提出了在DT中增加动态规划能力的方法。我们的方法包括三个步骤。首先，我们使用样本内值迭代来获得近似值函数，这涉及到MDP结构上的动态规划。第二，我们结合估计的优势来评估动作的质量。我们引入了两种优势估计器，分别适用于不同的任务。第三，我们训练了一个以估计的优势为条件生成动作的优势条件Transformer (ACT)。最后，在测试阶段，ACT根据所需的优势生成动作。我们的评估结果表明...

    Decision Transformer (DT), which employs expressive sequence modeling techniques to perform action generation, has emerged as a promising approach to offline policy optimization. However, DT generates actions conditioned on a desired future return, which is known to bear some weaknesses such as the susceptibility to environmental stochasticity. To overcome DT's weaknesses, we propose to empower DT with dynamic programming. Our method comprises three steps. First, we employ in-sample value iteration to obtain approximated value functions, which involves dynamic programming over the MDP structure. Second, we evaluate action quality in context with estimated advantages. We introduce two types of advantage estimators, IAE and GAE, which are suitable for different tasks. Third, we train an Advantage-Conditioned Transformer (ACT) to generate actions conditioned on the estimated advantages. Finally, during testing, ACT generates actions conditioned on a desired advantage. Our evaluation resul
    
[^201]: FECoM: 朝着深度学习的细粒度能耗测量迈进

    FECoM: A Step towards Fine-Grained Energy Measurement for Deep Learning. (arXiv:2308.12264v1 [cs.LG])

    [http://arxiv.org/abs/2308.12264](http://arxiv.org/abs/2308.12264)

    FECoM是一个用于细粒度深度学习能耗测量的框架，通过静态仪器分析和考虑计算负载和温度稳定性等因素，为研究人员和开发人员提供了对深度学习API进行概要分析的机制。

    

    随着深度学习模型的使用、规模和复杂性增加，其能源消耗迅速增长已成为一个关键问题。促进绿色发展和不同粒度的能源意识，以限制深度学习系统的碳排放是当务之急。然而，缺乏准确测量和优化细粒度（例如方法级别）能耗的标准和可重复工具阻碍了该领域的进展。在本文中，我们介绍了FECoM（细粒度能耗测量仪），这是一个用于细粒度深度学习能耗测量的框架。具体而言，FECoM为研究人员和开发人员提供了一种对深度学习API进行概要分析的机制。FECoM通过使用静态仪器分析和考虑计算负载和温度稳定性等各种因素来解决细粒度能耗测量的挑战。我们评估了FECoM在最常用的深度学习模型之一上测量细粒度能耗的能力。

    With the increasing usage, scale, and complexity of Deep Learning (DL) models, their rapidly growing energy consumption has become a critical concern. Promoting green development and energy awareness at different granularities is the need of the hour to limit carbon emissions of DL systems. However, the lack of standard and repeatable tools to accurately measure and optimize energy consumption at a fine granularity (e.g., at method level) hinders progress in this area. In this paper, we introduce FECoM (Fine-grained Energy Consumption Meter), a framework for fine-grained DL energy consumption measurement. Specifically, FECoM provides researchers and developers a mechanism to profile DL APIs. FECoM addresses the challenges of measuring energy consumption at fine-grained level by using static instrumentation and considering various factors, including computational load and temperature stability. We assess FECoM's capability to measure fine-grained energy consumption for one of the most p
    
[^202]: 使用传感器数据、方程和自然语言提示上下文中的运算符学习

    Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language. (arXiv:2308.05061v1 [cs.LG])

    [http://arxiv.org/abs/2308.05061](http://arxiv.org/abs/2308.05061)

    本文提出了一种使用传感器数据、方程和自然语言提示上下文中运算符学习的方法。通过整合人类知识和语言描述，该方法不仅扩展了物理信息学习的灵活性和普适性，而且显著提高了学习性能和减少了数据需求。

    

    在科学机器学习领域中，上下文中的运算符学习已经展示出了在推理阶段从提示数据中学习运算符的显著潜力，而无需进行权重更新。然而，当前模型对传感器数据的过度依赖可能会无意中忽视运算符的宝贵的人类洞察力。为了解决这个问题，我们将上下文中的运算符学习转化为一种多模式范式。我们提出使用“标题”来整合通过自然语言描述和方程式表达的运算符的人类知识。我们演示了这种方法不仅扩展了物理信息学习的灵活性和普遍性，而且还显著提高了学习性能并减少了数据需求。此外，我们引入了一种更高效的多模式上下文运算符学习的神经网络架构，称为“ICON-LM”，基于类似于语言模型的架构。

    In the growing domain of scientific machine learning, in-context operator learning has demonstrated notable potential in learning operators from prompted data during inference stage without weight updates. However, the current model's overdependence on sensor data, may inadvertently overlook the invaluable human insight into the operator. To address this, we present a transformation of in-context operator learning into a multi-modal paradigm. We propose the use of "captions" to integrate human knowledge about the operator, expressed through natural language descriptions and equations. We illustrate how this method not only broadens the flexibility and generality of physics-informed learning, but also significantly boosts learning performance and reduces data needs. Furthermore, we introduce a more efficient neural network architecture for multi-modal in-context operator learning, referred to as "ICON-LM", based on a language-model-like architecture. We demonstrate the viability of "ICO
    
[^203]: SMARLA：一种用于深度强化学习智能体的安全监测方法

    SMARLA: A Safety Monitoring Approach for Deep Reinforcement Learning Agents. (arXiv:2308.02594v1 [cs.LG])

    [http://arxiv.org/abs/2308.02594](http://arxiv.org/abs/2308.02594)

    本文提出了一种基于机器学习的安全监测方法SMARLA，用于深度强化学习智能体。该方法设计为黑盒子，利用状态抽象减少状态空间，实现对智能体状态的安全违规预测。经验证，SMARLA具有准确的违规预测能力，并可在智能体执行的早期阶段进行预测。

    

    深度强化学习算法(DRL)越来越多地应用于安全关键系统。确保DRL智能体的安全性在这种情况下是一个关键问题。然而，仅依靠测试是不足以确保安全性的，因为它不能提供保证。构建安全监测器是缓解这一挑战的一种解决方案。本文提出了SMARLA，一种基于机器学习的安全监测方法，专为DRL智能体设计。出于实际原因，SMARLA被设计为黑盒子(因为它不需要访问智能体的内部)，并利用状态抽象来减少状态空间，从而促进从智能体的状态学习安全违规预测模型。我们在两个知名的RL案例研究中验证了SMARLA。经验分析表明，SMARLA具有准确的违规预测能力，误报率低，并且可以在智能体执行的一半左右的早期阶段预测安全违规。

    Deep reinforcement learning algorithms (DRL) are increasingly being used in safety-critical systems. Ensuring the safety of DRL agents is a critical concern in such contexts. However, relying solely on testing is not sufficient to ensure safety as it does not offer guarantees. Building safety monitors is one solution to alleviate this challenge. This paper proposes SMARLA, a machine learning-based safety monitoring approach designed for DRL agents. For practical reasons, SMARLA is designed to be black-box (as it does not require access to the internals of the agent) and leverages state abstraction to reduce the state space and thus facilitate the learning of safety violation prediction models from agent's states. We validated SMARLA on two well-known RL case studies. Empirical analysis reveals that SMARLA achieves accurate violation prediction with a low false positive rate, and can predict safety violations at an early stage, approximately halfway through the agent's execution before 
    
[^204]: SkullGAN: 使用生成对抗网络生成合成的颅骨CT图像

    SkullGAN: Synthetic Skull CT Generation with Generative Adversarial Networks. (arXiv:2308.00206v1 [eess.IV])

    [http://arxiv.org/abs/2308.00206](http://arxiv.org/abs/2308.00206)

    使用SkullGAN，一种生成对抗网络（GAN），生成合成的颅骨CT图像，可以减少对真实图像的依赖，加速将机器学习应用于医疗保健领域的整合。

    

    深度学习在涉及人类颅骨的各种医疗应用中具有潜力，但需要大量经过策划的医学图像数据集。为了解决这个挑战，我们提出了SkullGAN，一种生成对抗网络（GAN），用于创建大规模的合成颅骨CT切片数据集，减少对真实图像的依赖并加速将机器学习应用于医疗保健领域的整合。在我们的方法中，对38个受试者进行了颅骨CT切片输入SkullGAN，这是一个包含超过2亿个参数的神经网络。生成的合成颅骨图像根据三个定量放射学特征进行评估：颅骨密度比（SDR）、平均厚度和平均强度。同时，使用t-分布随机邻域嵌入（t-SNE）进行进一步的分析，并将SkullGAN判别器作为分类器进行应用。结果表明，SkullGAN生成的图像与真实颅骨具有类似的关键定量放射学特征。进一步的确定性分析是通过进行了解决这个问题的办法。

    Deep learning offers potential for various healthcare applications involving the human skull but requires extensive datasets of curated medical images. To overcome this challenge, we propose SkullGAN, a generative adversarial network (GAN), to create large datasets of synthetic skull CT slices, reducing reliance on real images and accelerating the integration of machine learning into healthcare. In our method, CT slices of 38 subjects were fed to SkullGAN, a neural network comprising over 200 million parameters. The synthetic skull images generated were evaluated based on three quantitative radiological features: skull density ratio (SDR), mean thickness, and mean intensity. They were further analyzed using t-distributed stochastic neighbor embedding (t-SNE) and by applying the SkullGAN discriminator as a classifier. The results showed that SkullGAN-generated images demonstrated similar key quantitative radiological features to real skulls. Further definitive analysis was undertaken by
    
[^205]: 腐败鲁棒的Lipschitz上下文搜索

    Corruption-Robust Lipschitz Contextual Search. (arXiv:2307.13903v1 [cs.LG])

    [http://arxiv.org/abs/2307.13903](http://arxiv.org/abs/2307.13903)

    该论文研究了学习具有被篡改的二进制信号的Lipschitz函数的问题，提出了一种腐败鲁棒算法。该算法在不同损失函数下实现了不同程度的后悔。

    

    我研究了学习具有被篡改的二进制信号的Lipschitz函数的问题。学习者试图学习一个由对手选择的Lipschitz函数$f$。在每一轮中，对手在输入空间中选择一个上下文向量$x_t$，学习者对真实函数值$f(x_t)$进行猜测，并接收一个指示猜测是高还是低的二进制信号。在总共$C$轮中，信号可能被篡改，但学习者不知道$C$的值。学习者的目标是造成小的累积损失。我提出了一个自然而强大的技术验证，对设计腐败鲁棒算法非常有用。我设计了一些算法（将Lipschitz参数$L$视为常数）：对于对称损失，学习者在$d=1$时达到后悔$O(C\log T)$，在$d>1$时达到后悔$O_d(C\log T + T^{(d-1)/d})$；对于计价损失，学习者在$d/(d+1)$时达到后悔$\widetilde{O}(T^{d/(d+1)} + C\cdot T^{1/(d+1)})$。

    I study the problem of learning a Lipschitz function with corrupted binary signals. The learner tries to learn a Lipschitz function $f$ that the adversary chooses. In each round, the adversary selects a context vector $x_t$ in the input space, and the learner makes a guess to the true function value $f(x_t)$ and receives a binary signal indicating whether the guess was high or low. In a total of $C$ rounds, the signal may be corrupted, though the value of $C$ is unknown to the learner. The learner's goal is to incur a small cumulative loss. I present a natural yet powerful technique sanity check, which proves useful in designing corruption-robust algorithms. I design algorithms which (treating the Lipschitz parameter $L$ as constant): for the symmetric loss, the learner achieves regret $O(C\log T)$ with $d = 1$ and $O_d(C\log T + T^{(d-1)/d})$ with $d > 1$; for the pricing loss the learner achieves regret $\widetilde{O} (T^{d/(d+1)} + C\cdot T^{1/(d+1)})$.
    
[^206]: 批大小和步数与使用Armijo线搜索的随机梯度下降非凸优化之间的关系

    Relationship between Batch Size and Number of Steps Needed for Nonconvex Optimization of Stochastic Gradient Descent using Armijo Line Search. (arXiv:2307.13831v1 [cs.LG])

    [http://arxiv.org/abs/2307.13831](http://arxiv.org/abs/2307.13831)

    这项研究分析了使用Armijo线搜索的随机梯度下降非凸优化中批大小和步数的关系，并发现随着批大小的增加，所需的步数减少。

    

    随机梯度下降（SGD）是训练深度神经网络最简单的深度学习优化器。虽然SGD可以使用各种学习率，如常数或递减的学习率，但之前的数值结果表明，当SGD使用线搜索方法给出的学习率时，它的表现优于其他深度学习优化器。本文对使用Armijo线搜索给出学习率的非凸优化中的SGD进行了收敛性分析。分析表明，当步数和批大小都很大时，全梯度的平方范数的期望上界变小。接下来，我们展示了对于使用Armijo线搜索学习率的SGD来说，非凸优化所需的步数是批大小的单调递减凸函数；也就是说，随着批大小的增加，非凸优化所需的步数减少。此外，我们还展示了随机火灾的贡献。

    Stochastic gradient descent (SGD) is the simplest deep learning optimizer with which to train deep neural networks. While SGD can use various learning rates, such as constant or diminishing rates, the previous numerical results showed that SGD performs better than other deep learning optimizers using when it uses learning rates given by line search methods. In this paper, we perform a convergence analysis on SGD with a learning rate given by an Armijo line search for nonconvex optimization. The analysis indicates that the upper bound of the expectation of the squared norm of the full gradient becomes small when the number of steps and the batch size are large. Next, we show that, for SGD with the Armijo-line-search learning rate, the number of steps needed for nonconvex optimization is a monotone decreasing convex function of the batch size; that is, the number of steps needed for nonconvex optimization decreases as the batch size increases. Furthermore, we show that the stochastic fir
    
[^207]: 通过神经多项式方法实现可解释的弹性塑性模型的发现

    Discovering interpretable elastoplasticity models via the neural polynomial method enabled symbolic regressions. (arXiv:2307.13149v1 [cs.CE])

    [http://arxiv.org/abs/2307.13149](http://arxiv.org/abs/2307.13149)

    本文介绍了一种通过神经多项式方法实现可解释的弹性塑性模型的机器学习方法，该方法通过分为两个步骤，先通过监督学习得到一组特征映射，再通过符号回归将其转化为数学公式，从而克服了传统神经网络模型的缺乏可解释性的问题。

    

    传统神经网络弹性塑性模型通常被认为缺乏可解释性。本文介绍了一种两步机器学习方法，可以返回专家可解释的数学模型。具体而言，我们引入了一个替代模型，其中屈服曲面是通过监督学习得到的一组单变量特征映射来表示的。然后，通过符号回归将这组单变量神经网络映射函数重新解释为数学形式。这种分而治之的方法具有几个重要优势。首先，它使我们能够克服符号回归算法的扩展问题。从实际角度来看，它提高了用不同编程语言编写的偏微分方程求解器的学习模型的可移植性。最后，它使我们能够对材料的属性（如凸性和对称性）有一个具体的理解。

    Conventional neural network elastoplasticity models are often perceived as lacking interpretability. This paper introduces a two-step machine-learning approach that returns mathematical models interpretable by human experts. In particular, we introduce a surrogate model where yield surfaces are expressed in terms of a set of single-variable feature mappings obtained from supervised learning. A postprocessing step is then used to re-interpret the set of single-variable neural network mapping functions into mathematical form through symbolic regression. This divide-and-conquer approach provides several important advantages. First, it enables us to overcome the scaling issue of symbolic regression algorithms. From a practical perspective, it enhances the portability of learned models for partial differential equation solvers written in different programming languages. Finally, it enables us to have a concrete understanding of the attributes of the materials, such as convexity and symmetri
    
[^208]: 分散式学习动力学中个体群体的力量

    The Power of Populations in Decentralized Learning Dynamics. (arXiv:2306.08670v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.08670](http://arxiv.org/abs/2306.08670)

    本文研究了分散式学习动力学中个体群体的力量。我们介绍了一种分散式的多臂赌博机设置，并分析了几个针对此任务的分散式动力学家族。我们展示了这些动力学与一类“零和”乘法权重更新算法的联系，并开发了一个通用框架来分析这些协议的群体级遗憾。在广泛的参数范围下，我们得到了次线性的遗憾界限。

    

    我们研究了一种分散式多臂赌博机设置，在一个由$n$个受内存限制的节点组成的种群中，采用了谣言模型：每轮，每个节点本地采用$m$个臂之一，观察从臂的（对抗选择的）分布中抽取的奖励，然后与随机抽取的邻居进行通信，交换信息，以确定下一轮的策略。我们介绍并分析了几个针对此任务的分散式动力学家族：每个节点的决策完全是局部的，只依赖于其最新获得的奖励以及它抽样的邻居的奖励。我们展示了这些分散式动力学的全局演化与特定类型的“零和”乘法权重更新算法之间的联系，并且开发了一个分析这些自然协议的群体级遗憾的通用框架。利用这个框架，我们在广泛的参数范围（即，种群的大小和nu的大小）下推导了次线性遗憾界限。

    We study a distributed multi-armed bandit setting among a population of $n$ memory-constrained nodes in the gossip model: at each round, every node locally adopts one of $m$ arms, observes a reward drawn from the arm's (adversarially chosen) distribution, and then communicates with a randomly sampled neighbor, exchanging information to determine its policy in the next round. We introduce and analyze several families of dynamics for this task that are decentralized: each node's decision is entirely local and depends only on its most recently obtained reward and that of the neighbor it sampled. We show a connection between the global evolution of these decentralized dynamics with a certain class of "zero-sum" multiplicative weights update algorithms, and we develop a general framework for analyzing the population-level regret of these natural protocols. Using this framework, we derive sublinear regret bounds under a wide range of parameter regimes (i.e., the size of the population and nu
    
[^209]: MC-NN：一种端到端的多通道神经网络方法，用于预测流感病毒宿主和抗原类型。

    MC-NN: An End-to-End Multi-Channel Neural Network Approach for Predicting Influenza A Virus Hosts and Antigenic Types. (arXiv:2306.05587v1 [cs.LG])

    [http://arxiv.org/abs/2306.05587](http://arxiv.org/abs/2306.05587)

    提出了一种利用多通道神经网络模型预测流感A病毒宿主和抗原亚型的方法，数据显示多通道神经网络模型具有较高的准确性和预测能力。

    

    流感对公共卫生构成重大威胁，特别是对老年人、儿童和患有潜在疾病的人来说更为严重。严重病况的发生，如肺炎，凸显了预防流感传播的重要性。准确而具有成本效益的预测流感A病毒的宿主和抗原亚型对于应对这一问题至关重要，特别是在资源有限的地区。在本研究中，我们提出了一种多通道神经网络模型，用于从血凝素和神经氨酸酶蛋白序列预测流感A病毒的宿主和抗原亚型。我们的模型是在一个完整蛋白质序列的全面数据集上进行训练的，并在各种完整和不完整序列的测试数据集上进行评估。结果表明，使用多通道神经网络来预测来自完整和部分蛋白质序列的流感A病毒的宿主和抗原亚型具有潜力和实用性。

    Influenza poses a significant threat to public health, particularly among the elderly, young children, and people with underlying dis-eases. The manifestation of severe conditions, such as pneumonia, highlights the importance of preventing the spread of influenza. An accurate and cost-effective prediction of the host and antigenic sub-types of influenza A viruses is essential to addressing this issue, particularly in resource-constrained regions. In this study, we propose a multi-channel neural network model to predict the host and antigenic subtypes of influenza A viruses from hemagglutinin and neuraminidase protein sequences. Our model was trained on a comprehensive data set of complete protein sequences and evaluated on various test data sets of complete and incomplete sequences. The results demonstrate the potential and practicality of using multi-channel neural networks in predicting the host and antigenic subtypes of influenza A viruses from both full and partial protein sequence
    
[^210]: SiBBlInGS: 使用跨状态的图形相似性驱动模块推理的建模块方法

    SiBBlInGS: Similarity-driven Building-Block Inference using Graphs across States. (arXiv:2306.04817v1 [stat.ML])

    [http://arxiv.org/abs/2306.04817](http://arxiv.org/abs/2306.04817)

    本文提出了一种跨状态的图形相似性驱动的模块推理框架，可以同时考虑数据中的状态间和状态内关系，并允许状态之间的会话计数和持续时间的差异。它可以提取非正交组件，并且能够识别特定状态与状态非特定模块。

    

    对于多维时间序列来说，提取有意义的模块是发现复杂系统中有价值见解的关键。本文提出了一种基于图形相似性驱动的模块推理框架(SiBBlInGS)，用于发现模块，同时考虑到数据中的状态间和状态内关系，能够提取非正交组件，并允许状态之间的会话计数和持续时间差异。此外，SiBBlInGS还允许跨状态变化模块结构和每次试验的时间变异，并可识别特定状态与状态非特定模块。

    Interpretable methods for extracting meaningful building blocks (BBs) underlying multi-dimensional time series are vital for discovering valuable insights in complex systems. Existing techniques, however, encounter limitations that restrict their applicability to real-world systems, like reliance on orthogonality assumptions, inadequate incorporation of inter- and intra-state variability, and incapability to handle sessions of varying duration. Here, we present a framework for Similarity-driven Building Block Inference using Graphs across States (SiBBlInGS). SiBBlInGS employs a graph-based dictionary learning approach for BB discovery, simultaneously considers both inter- and intra-state relationships in the data, can extract non-orthogonal components, and allows for variations in session counts and duration across states. Additionally, SiBBlInGS allows for cross-state variations in BB structure and per-trial temporal variability, can identify state-specific vs state-invariant BBs, and
    
[^211]: L2归一化技术在简单高质量OoD检测中的应用

    Simple High Quality OoD Detection with L2 Normalization. (arXiv:2306.04072v1 [cs.LG])

    [http://arxiv.org/abs/2306.04072](http://arxiv.org/abs/2306.04072)

    本篇论文介绍了在ResNet模型训练中应用L2归一化技术，可以以几乎没有成本的方式实现与最先进的OoD检测性能相媲美的结果，测试时仅需移除L2归一化即可。

    

    我们在标准的ResNet模型训练中提出了一种简单的修改方法--在特征空间中进行L2归一化--能够产生与最先进的OoD检测性能相媲美的结果。当在测试时移除L2归一化时，特征向量的L2范数成为网络不确定性的一个惊人的替代者，而当没有L2归一化训练时，这种行为却没有那么有效。直观上，熟悉的图像会产生大的向量，而陌生的图像则会产生小的向量。值得注意的是，在训练时几乎没有额外的成本，在测试时也没有成本。

    We propose a simple modification to standard ResNet architectures during training--L2 normalization over feature space--that produces results competitive with state-of-the-art Out-of-Distribution (OoD) detection performance. When L2 normalization is removed at test time, the L2 norm of feature vectors becomes a surprisingly good proxy for network uncertainty, whereas this behaviour is not nearly as effective when training without L2 normalization. Intuitively, familiar images result in large magnitude vectors, while unfamiliar images result in small magnitudes. Notably, this is achievable with almost no additional cost during training, and no cost at test time.
    
[^212]: MutateNN：用于硬件加速器上图像识别模型的突变测试

    MutateNN: Mutation Testing of Image Recognition Models Deployed on Hardware Accelerators. (arXiv:2306.01697v1 [cs.LG])

    [http://arxiv.org/abs/2306.01697](http://arxiv.org/abs/2306.01697)

    MutateNN是一种用于探索硬件加速器上深度学习图像识别模型鲁棒性的工具，提供突变测试和分析能力，且有效性已在多种预训练深度神经网络模型中得到验证。

    

    随着人工智能的研究进展，解决现实世界问题并推动技术发展的新机遇应运而生。图像识别模型特别是被分配了感知任务，以解决复杂的现实世界挑战并导致新的解决方案。此外，这类模型的计算复杂度和资源需求也有所增加。为了解决这个问题，模型优化和硬件加速已成为关键技术，但有效整合这些概念是一个具有挑战性和容易出错的过程。为了让开发人员和研究人员能够探索在不同硬件加速设备上部署的深度学习图像识别模型的鲁棒性，我们提出了MutateNN，这是一个为此目的提供突变测试和分析能力的工具。为了展示其功能，我们对7个广为人知的预训练深度神经网络模型进行了21种变异。我们在4种不同类型的硬件加速器上部署了我们的变异体，分析了它们的行为，并评估了MutateNN在检测出不正确或不精确的模型行为方面的有效性。

    With the research advancement of Artificial Intelligence in the last years, there are new opportunities to mitigate real-world problems and advance technologically. Image recognition models in particular, are assigned with perception tasks to mitigate complex real-world challenges and lead to new solutions. Furthermore, the computational complexity and demand for resources of such models has also increased. To mitigate this, model optimization and hardware acceleration has come into play, but effectively integrating such concepts is a challenging and error-prone process.  In order to allow developers and researchers to explore the robustness of deep learning image recognition models deployed on different hardware acceleration devices, we propose MutateNN, a tool that provides mutation testing and analysis capabilities for that purpose. To showcase its capabilities, we utilized 21 mutations for 7 widely-known pre-trained deep neural network models. We deployed our mutants on 4 different
    
[^213]: Leaky-ReLU神经网络在均匀通用逼近中的最小宽度研究

    Minimum Width of Leaky-ReLU Neural Networks for Uniform Universal Approximation. (arXiv:2305.18460v1 [cs.LG])

    [http://arxiv.org/abs/2305.18460](http://arxiv.org/abs/2305.18460)

    研究表明具有临界宽度的Leaky-ReLU神经网络可以在紧致域K上实现$L^p(K,\mathbb{R}^{d_y})$的UAP，而本文给出的最小宽度$w_{\min}=\max(d_x+1,d_y)+1_{d_y=d_x+1}$则适用于函数类$C(K,\mathbb{R}^{d_y})$，考虑到输出维度的影响。

    

    对神经网络的通用逼近性质（UAP）的研究历史悠久。当网络宽度不受限制时，只需要一个隐藏层即可进行UAP。相反，当深度不受限制时，UAP的宽度需要不小于临界宽度$w^*_{\min}=\max(d_x,d_y)$, 其中$d_x$和$d_y$分别是输入和输出的维度。最近，\cite{cai2022achieve}表明，具有这种临界宽度的Leaky-ReLU神经网络可以在紧致域$K$上实现$L^p$函数的UAP，即$L^p(K,\mathbb{R}^{d_y})$的UAP。本文研究了函数类$C(K,\mathbb{R}^{d_y})$的均匀UAP，并给出了Leaky-ReLU NN的确切最小宽度，为$w_{\min}=\max(d_x+1,d_y)+1_{d_y=d_x+1}$，其中涉及输出维度的影响。为了得到这个结果，我们提出了一种新的lift-flow-discretization方法，证明了均匀UAP与拓扑理论有着深刻的联系。

    The study of universal approximation properties (UAP) for neural networks (NN) has a long history. When the network width is unlimited, only a single hidden layer is sufficient for UAP. In contrast, when the depth is unlimited, the width for UAP needs to be not less than the critical width $w^*_{\min}=\max(d_x,d_y)$, where $d_x$ and $d_y$ are the dimensions of the input and output, respectively. Recently, \cite{cai2022achieve} shows that a leaky-ReLU NN with this critical width can achieve UAP for $L^p$ functions on a compact domain $K$, \emph{i.e.,} the UAP for $L^p(K,\mathbb{R}^{d_y})$. This paper examines a uniform UAP for the function class $C(K,\mathbb{R}^{d_y})$ and gives the exact minimum width of the leaky-ReLU NN as $w_{\min}=\max(d_x+1,d_y)+1_{d_y=d_x+1}$, which involves the effects of the output dimensions. To obtain this result, we propose a novel lift-flow-discretization approach that shows that the uniform UAP has a deep connection with topological theory.
    
[^214]: 通过平均加速动量随机梯度下降：有限样本速率和渐近正态性

    Acceleration of stochastic gradient descent with momentum by averaging: finite-sample rates and asymptotic normality. (arXiv:2305.17665v1 [cs.LG])

    [http://arxiv.org/abs/2305.17665](http://arxiv.org/abs/2305.17665)

    研究了动量随机梯度下降（SGDM）和其Polyak-averaging版本的特性，表明在较大的批量大小下，小批量SGDM比小批量SGD更快地收敛到最优值的邻域。

    

    动量随机梯度下降（SGDM）被广泛应用于许多机器学习和统计应用中。尽管SGDM相对于传统的随机梯度下降具有观察到的经验优势，但在优化过程中动量对不同学习率的作用的理论理解仍然是开放的。我们在强凸设置下分析了SGDM的有限样本收敛速率，并表明在较大的批量大小下，小批量SGDM比小批量SGD更快地收敛到最优值的邻域。此外，我们分析了SGDM估计量的Polyak平均版本，建立了它的渐近正态性，并证明了它与平均SGD的渐近等价性。

    Stochastic gradient descent with momentum (SGDM) has been widely used in many machine learning and statistical applications. Despite the observed empirical benefits of SGDM over traditional SGD, the theoretical understanding of the role of momentum for different learning rates in the optimization process remains widely open. We analyze the finite-sample convergence rate of SGDM under the strongly convex settings and show that, with a large batch size, the mini-batch SGDM converges faster than mini-batch SGD to a neighborhood of the optimal value. Furthermore, we analyze the Polyak-averaging version of the SGDM estimator, establish its asymptotic normality, and justify its asymptotic equivalence to the averaged SGD.
    
[^215]: 小语言模型通过重写其输出来提高巨型模型的性能

    Small Language Models Improve Giants by Rewriting Their Outputs. (arXiv:2305.13514v1 [cs.CL])

    [http://arxiv.org/abs/2305.13514](http://arxiv.org/abs/2305.13514)

    本论文提出了一种方法，通过使用小语言模型重写大语言模型的输出，从而提高其性能。实验证明，该方法可以显着改善大语言模型的少样本学习能力和泛化性能。

    

    大型语言模型(LLMs)展示了令人印象深刻的少样本学习能力，但它们在挑战性任务上的表现通常不如微调模型。此外，它们的巨大体积和通过API的受限访问使得针对任务的微调不切实际。而且，LLMs对提示的不同方面（例如，演示的选择和顺序）很敏感，因此可能需要耗费时间进行提示工程。因此，我们提出了一种方法，可以在不依赖其权重的情况下纠正LLM的输出。首先，我们通过少样本提示LLM生成一个候选池。其次，我们使用一个更小的模型，LM-corrector（LMCor）来改进LLM生成的输出。LMCor被训练用于对候选者进行排名、组合和重写，以产生最终的目标输出。我们的实验表明，即使是一个小的LMCor模型（250M），也可以显着改善LLMs（62B）的少样本性能，适用于各种任务。此外，我们还证明LMCor表现出对提示变化的改进鲁棒性和更好的泛化性。总体而言，我们的方法展示了改善LLMs实际可用性的有希望的结果。

    Large language models (LLMs) have demonstrated impressive few-shot learning capabilities, but they often underperform compared to fine-tuned models on challenging tasks. Furthermore, their large size and restricted access only through APIs make task-specific fine-tuning impractical. Moreover, LLMs are sensitive to different aspects of prompts (e.g., the selection and order of demonstrations) and can thus require time-consuming prompt engineering. In this light, we propose a method to correct LLM outputs without relying on their weights. First, we generate a pool of candidates by few-shot prompting an LLM. Second, we refine the LLM-generated outputs using a smaller model, the LM-corrector (LMCor), which is trained to rank, combine and rewrite the candidates to produce the final target output. Our experiments demonstrate that even a small LMCor model (250M) substantially improves the few-shot performance of LLMs (62B) across diverse tasks. Moreover, we illustrate that the LMCor exhibits 
    
[^216]: 从自然语言定义中学习多关系双曲词向量

    Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions. (arXiv:2305.07303v1 [cs.CL])

    [http://arxiv.org/abs/2305.07303](http://arxiv.org/abs/2305.07303)

    本论文提出了一种从自然语言定义中学习多关系双曲词向量的框架，以捕捉由定义所引起的分层和多分辨率结构。

    

    仅使用分布信息的神经词向量一直以来都能为下游任务提供有用的含义表示。然而，现有的方法通常会导致难以解释和控制的表示。相反，自然语言定义具有递归的，自说明的语义结构，可以支持能够保留向量空间中显式概念关系和约束的新型表示学习范 paradigm。本文提出了一个神经符号、多关系框架，通过联合映射定义和定义术语及其相应的语义关系，仅从自然语言定义中学习词向量。通过自动从定义语料库中提取关系，并通过一个翻译目标规范化学习问题，我们将框架专门设定为在双曲空间中捕获由定义引起的分层和多分辨率结构。

    Neural-based word embeddings using solely distributional information have consistently produced useful meaning representations for downstream tasks. However, existing approaches often result in representations that are hard to interpret and control. Natural language definitions, on the other side, possess a recursive, self-explanatory semantic structure that can support novel representation learning paradigms able to preserve explicit conceptual relations and constraints in the vector space.  This paper proposes a neuro-symbolic, multi-relational framework to learn word embeddings exclusively from natural language definitions by jointly mapping defined and defining terms along with their corresponding semantic relations. By automatically extracting the relations from definitions corpora and formalising the learning problem via a translational objective, we specialise the framework in hyperbolic space to capture the hierarchical and multi-resolution structure induced by the definitions.
    
[^217]: 分段归一化流

    Piecewise Normalizing Flows. (arXiv:2305.02930v1 [stat.ML])

    [http://arxiv.org/abs/2305.02930](http://arxiv.org/abs/2305.02930)

    介绍了一种分段归一化流方法，将目标分布分成集群，并通过训练模拟复杂的多模态目标。这种方法可以更好地匹配标准正态基础分布的拓扑结构。

    

    归一化流是一种通过从基础分布进行可逆转换来对复杂概率密度进行建模的成熟方法。然而，目标分布能否精确地被归一化流所捕捉，强烈受到基础分布的拓扑结构的影响。目标和基础分布之间的拓扑不匹配可能导致性能差，如对于多模态问题。一些不同的工作试图通过使用高斯混合模型 [Izmailov et al., 2020、Ardizzone et al., 2020、Hagemann and Neumayer, 2021] 或学习接受/拒绝采样 [Stimper et al., 2022] 来修改基础分布的拓扑结构以更好地匹配目标分布。我们引入了分段归一化流，将目标分布分成集群，并训练一系列流来模拟复杂的多模态目标。

    Normalizing flows are an established approach for modelling complex probability densities through invertible transformations from a base distribution. However, the accuracy with which the target distribution can be captured by the normalizing flow is strongly influenced by the topology of the base distribution. A mismatch between the topology of the target and the base can result in a poor performance, as is the case for multi-modal problems. A number of different works have attempted to modify the topology of the base distribution to better match the target, either through the use of Gaussian Mixture Models [Izmailov et al., 2020, Ardizzone et al., 2020, Hagemann and Neumayer, 2021] or learned accept/reject sampling [Stimper et al., 2022]. We introduce piecewise normalizing flows which divide the target distribution into clusters, with topologies that better match the standard normal base distribution, and train a series of flows to model complex multi-modal targets. The piecewise nat
    
[^218]: FedIN：用于模型异构的联邦中间层学习

    FedIN: Federated Intermediate Layers Learning for Model Heterogeneity. (arXiv:2304.00759v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.00759](http://arxiv.org/abs/2304.00759)

    FedIN是一种新型的联邦学习方法，支持异构模型，无需公共数据集。在FedIN中，提取器和分类器的模型结构在所有设备中都相同，而中间层的架构可以根据异构设备的资源容量而变化。IN训练可用于利用特征知识，实验结果表明了该方法在图像分类任务上的有效性。

    

    联邦学习（FL）使得边缘设备能够合作训练全局共享模型，同时在本地和私密地保留训练数据。然而，在FL中一个普遍但不切实际的假设是参与边缘设备拥有相同的必需资源并共享相同的全局模型架构。本研究提出了一种名为Federated Intermediate Layers Learning（FedIN）的新型FL方法，支持异构模型而不使用任何公共数据集。FedIN中的训练模型分为三部分，包括提取器、中间层和分类器。提取器和分类器的模型结构在所有设备中都相同，以保持中间层特征的一致性，而中间层的架构可以根据异构设备的资源容量而变化。为了利用特征知识，我们提出了IN训练，以IN标准化为基础训练中间层。在图像分类任务上的实验结果表明了我们方法在精度和效率方面的有效性。

    Federated learning (FL) facilitates edge devices to cooperatively train a global shared model while maintaining the training data locally and privately. However, a common but impractical assumption in FL is that the participating edge devices possess the same required resources and share identical global model architecture. In this study, we propose a novel FL method called Federated Intermediate Layers Learning (FedIN), supporting heterogeneous models without utilizing any public dataset. The training models in FedIN are divided into three parts, including an extractor, the intermediate layers, and a classifier. The model architectures of the extractor and classifier are the same in all devices to maintain the consistency of the intermediate layer features, while the architectures of the intermediate layers can vary for heterogeneous devices according to their resource capacities. To exploit the knowledge from features, we propose IN training, training the intermediate layers in line 
    
[^219]: 体育博彩的机器学习：预测模型应优化准确性还是校准性？

    Machine learning for sports betting: should forecasting models be optimised for accuracy or calibration?. (arXiv:2303.06021v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06021](http://arxiv.org/abs/2303.06021)

    该论文研究了机器学习在体育博彩中的应用，提出了优化预测模型校准性比准确度更重要的假设，并通过实验证明了此假设的正确性。

    

    美国最近对体育博彩进行了联邦合法化，这与机器学习的黄金时代相遇。如果博彩者能够利用数据准确地预测结果的概率，他们可以认识到何时书maker的赔率对他们有利。由于体育博彩仅在美国的市场上就是一个数十亿美元的行业，因此找到这样的机会可能会非常有利可图。许多研究人员已将机器学习应用于体育赛果预测问题，通常使用准确度来评估预测模型的性能。我们提出假设，对于体育博彩问题，模型校准比准确度更重要。为了测试这一假设，我们在多个赛季的NBA数据上对模型进行训练，并在单个赛季上使用已发布的赔率进行博彩实验。通过评估各种博彩系统，我们表明优化校准的预测模型比优化准确度平均带来更高的回报率（投资回报率为$110.42％$）。

    Sports betting's recent federal legalisation in the USA coincides with the golden age of machine learning. If bettors can leverage data to accurately predict the probability of an outcome, they can recognise when the bookmaker's odds are in their favour. As sports betting is a multi-billion dollar industry in the USA alone, identifying such opportunities could be extremely lucrative. Many researchers have applied machine learning to the sports outcome prediction problem, generally using accuracy to evaluate the performance of forecasting models. We hypothesise that for the sports betting problem, model calibration is more important than accuracy. To test this hypothesis, we train models on NBA data over several seasons and run betting experiments on a single season, using published odds. Evaluating various betting systems, we show that optimising the forecasting model for calibration leads to greater returns than optimising for accuracy, on average (return on investment of $110.42\%$ v
    
[^220]: 从递推视角重新审视LQR控制

    Revisiting LQR Control from the Perspective of Receding-Horizon Policy Gradient. (arXiv:2302.13144v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2302.13144](http://arxiv.org/abs/2302.13144)

    本文从递推视角重新审视了LQR控制问题，并应用递推-视角策略梯度（RHPG）模型提供了一种采样复杂度分析，通过无需任何先验信息进行优化求解，并展示了RHPG在线性控制和估计中的普适性。

    

    本文从递推视角重新审视了离散时间线性二次调节器（LQR）问题。结合递推-视角策略梯度（RHPG）模型无需任何先验信息进行优化求解，提供了一种采样复杂度分析，能够学习到在ε-范数意义下接近LQR最优解的优化控制策略。在最近将RHPG应用于学习卡尔曼滤波中进行拓展分析之后，我们展示了RHPG在线性控制和估计中的普适性。

    We revisit in this paper the discrete-time linear quadratic regulator (LQR) problem from the perspective of receding-horizon policy gradient (RHPG), a newly developed model-free learning framework for control applications. We provide a fine-grained sample complexity analysis for RHPG to learn a control policy that is both stabilizing and $\epsilon$-close to the optimal LQR solution, and our algorithm does not require knowing a stabilizing control policy for initialization. Combined with the recent application of RHPG in learning the Kalman filter, we demonstrate the general applicability of RHPG in linear control and estimation with streamlined analyses.
    
[^221]: 通过$f$-差分隐私打破通信-隐私-准确性的权衡

    Breaking the Communication-Privacy-Accuracy Tradeoff with $f$-Differential Privacy. (arXiv:2302.09624v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2302.09624](http://arxiv.org/abs/2302.09624)

    本文研究了具有隐私问题和有限通信能力的多个用户的协作数据分析问题，在局部差分隐私保证的视角下，我们通过推导离散值机制的紧密$f$-差分隐私保证，进一步研究了隐私放大的稀疏化方法。

    

    本文考虑了一个联邦数据分析问题，其中服务器协调具有隐私问题和有限通信能力的多个用户的协作数据分析。通常采用的压缩方案在改善通信效率的同时引入了局部数据的信息损失，但这些离散值机制是否提供了任何隐私保护仍然是一个开放问题。在本文中，我们通过$f$-差分隐私（DP）的视角研究了具有有限输出空间的离散值机制的局部差分隐私保证。具体而言，我们通过推导出各种离散值机制的紧密$f$-差分隐私保证，包括用于隐私保护的二项噪声和二项机制以及用于数据压缩的基于符号的方法，进一步研究了稀疏化对隐私的放大，并提出了解决的方法。

    We consider a federated data analytics problem in which a server coordinates the collaborative data analysis of multiple users with privacy concerns and limited communication capability. The commonly adopted compression schemes introduce information loss into local data while improving communication efficiency, and it remains an open problem whether such discrete-valued mechanisms provide any privacy protection. In this paper, we study the local differential privacy guarantees of discrete-valued mechanisms with finite output space through the lens of $f$-differential privacy (DP). More specifically, we advance the existing literature by deriving tight $f$-DP guarantees for a variety of discrete-valued mechanisms, including the binomial noise and the binomial mechanisms that are proposed for privacy preservation, and the sign-based methods that are proposed for data compression, in closed-form expressions. We further investigate the amplification in privacy by sparsification and propose
    
[^222]: 机器学习时代MRI数据协调的有效性：一项跨36个数据集的多中心研究

    Efficacy of MRI data harmonization in the age of machine learning. A multicenter study across 36 datasets. (arXiv:2211.04125v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.04125](http://arxiv.org/abs/2211.04125)

    本文描述了一项多中心研究，旨在评估MRI数据协调在机器学习中的效用;作者提出了一种“调和器变压器”方法，在不泄露信息的前提下，在机器学习的预处理步骤中实现了数据协调。

    

    从多个网站汇集公开可用的MRI数据可以组装大量受试对象，增加统计功率，并通过机器学习技术促进数据重用。多中心数据的协调是减少数据中与非生物来源的变异度量的混杂效应的必要条件。然而，将协调应用于机器学习之前的整个数据集会导致数据泄漏，因为训练集之外的信息可能会影响模型构建并可能导致性能过高。我们提出了1）数据协调的有效性测量方法；2）“调和器变压器”，即ComBat协调方法的一个实现，它允许将其封装在机器学习的预处理步骤中，避免数据泄漏。我们使用了来自36个网站的1740名健康受试者的大脑T1加权MRI数据来测试这些工具。经过协调后，网站效应被删除或减少了，

    Pooling publicly-available MRI data from multiple sites allows to assemble extensive groups of subjects, increase statistical power, and promote data reuse with machine learning techniques. The harmonization of multicenter data is necessary to reduce the confounding effect associated with non-biological sources of variability in the data. However, when applied to the entire dataset before machine learning, the harmonization leads to data leakage, because information outside the training set may affect model building, and potentially falsely overestimate performance. We propose a 1) measurement of the efficacy of data harmonization; 2) harmonizer transformer, i.e., an implementation of the ComBat harmonization allowing its encapsulation among the preprocessing steps of a machine learning pipeline, avoiding data leakage. We tested these tools using brain T1-weighted MRI data from 1740 healthy subjects acquired at 36 sites. After harmonization, the site effect was removed or reduced, and 
    

