# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [SA-Solver: Stochastic Adams Solver for Fast Sampling of Diffusion Models.](http://arxiv.org/abs/2309.05019) | 本文提出了一种改进的高效随机亚当方法SA-Solver，用于解扩散随机微分方程以生成高质量的数据，实验结果显示它在少步采样中相较于现有最先进的方法有改进或可比的性能，并达到了SOTA FID分数。 |
| [^2] | [Linear Speedup of Incremental Aggregated Gradient Methods on Streaming Data.](http://arxiv.org/abs/2309.04980) | 本文研究了增量汇聚梯度方法在流数据上的线性加速。研究表明，在确定性梯度情况下，流IAG方法可以实现线性加速，并且即使数据样本分布不均匀，只要工人频繁更新，期望的最优解平方距离可以以O((1+T)/(nt))的速度衰减。 |
| [^3] | [AVARS -- Alleviating Unexpected Urban Road Traffic Congestion using UAVs.](http://arxiv.org/abs/2309.04976) | 这篇论文提出使用无人机来缓解意外的城市道路交通拥堵。研究指出，传统的交通信号控制系统效率低下，而基于摄像机和深度强化学习算法的系统更加有效。然而，由于成本问题，无人机可能是一个更好的选择。 |
| [^4] | [Continual Robot Learning using Self-Supervised Task Inference.](http://arxiv.org/abs/2309.04974) | 本文提出了一种使用自我监督的任务推断方法，以实现机器人的连续学习。通过学习动作和意图嵌入以及行为嵌入，机器人可以推断出当前正在进行的任务，从而实现不断学习新任务的能力。 |
| [^5] | [LMBiS-Net: A Lightweight Multipath Bidirectional Skip Connection based CNN for Retinal Blood Vessel Segmentation.](http://arxiv.org/abs/2309.04968) | 本文提出了一种名为LMBiS-Net的轻量级卷积神经网络用于视网膜血管分割，具有极低的可学习参数数量和优化的模型效率，通过多路径特征提取和双向跳跃连接实现了较好的分割性能。 |
| [^6] | [A multiple k-means cluster ensemble framework for clustering citation trajectories.](http://arxiv.org/abs/2309.04949) | 本文提出了一种基于特征的多个k-means聚类集成框架，用于聚类引用轨迹。这有助于理解知识传播过程，并解决了现有方法依赖参数、定义模糊和只捕捉极端轨迹的问题。 |
| [^7] | [Distance-Restricted Folklore Weisfeiler-Leman GNNs with Provable Cycle Counting Power.](http://arxiv.org/abs/2309.04941) | 本文提出了一种称为$d$-DRFWL(2) GNNs的新型图神经网络，它通过限制节点之间的距离来实现循环计数能力，克服了子图GNNs的预处理和计算成本高的限制。 |
| [^8] | [A Review of Machine Learning-based Security in Cloud Computing.](http://arxiv.org/abs/2309.04911) | 云计算的快速发展带来了安全风险，机器学习的应用可以在识别和解决安全问题方面减少人工干预的需求，并通过分析大量数据进行准确预测，从而改变了云服务提供商在安全方面的方法。 |
| [^9] | [Symplectic Structure-Aware Hamiltonian (Graph) Embeddings.](http://arxiv.org/abs/2309.04885) | 本文提出了SAH-GNN，一种在图神经网络中应用结构感知的辛系统哈密顿嵌入方法。与传统方法不同，SAH-GNN通过在训练过程中自适应学习辛结构，避免了依赖预定义标准辛结构形式的限制，并能够适应不同的图数据集，同时保持物理意义上的能量守恒。 |
| [^10] | [A Gentle Introduction to Gradient-Based Optimization and Variational Inequalities for Machine Learning.](http://arxiv.org/abs/2309.04877) | 这篇论文介绍了渐变优化和变分不等式在机器学习中的应用，强调了从模式识别到决策和多智能体问题的转变，以及涉及均衡和博弈论的数学挑战，提供了一些算法的收敛性证明，但主要关注于提供动机和直观理解。 |
| [^11] | [Approximating ReLU on a Reduced Ring for Efficient MPC-based Private Inference.](http://arxiv.org/abs/2309.04875) | 本文提出了一种名为HummingBird的MPC框架，通过在较小的环上使用一部分比特来显著减小ReLU的通信开销，以提高MPC基于私有推理的效率。 |
| [^12] | [Approximation Results for Gradient Descent trained Neural Networks.](http://arxiv.org/abs/2309.04860) | 该论文研究了采用梯度下降训练的神经网络的近似保证，利用连续的误差范数对网络进行分析，并发现在欠参数化的情况下相对于已有的逼近方法存在逼近率下降的问题。 |
| [^13] | [Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System.](http://arxiv.org/abs/2309.04858) | 本文介绍了一种方法，可以逆向工程用于生成文本的解码方法，并发现了这些方法对于检测生成文本以及揭示由于解码设置导致的偏见的重要意义。 |
| [^14] | [AmbientFlow: Invertible generative models from incomplete, noisy measurements.](http://arxiv.org/abs/2309.04856) | AmbientFlow是一个从噪声和不完整数据中直接学习基于流的生成模型的新框架，通过使用变分贝叶斯方法来建立这种模型，展示了其在图像科学中的应用潜力。 |
| [^15] | [Speech Emotion Recognition with Distilled Prosodic and Linguistic Affect Representations.](http://arxiv.org/abs/2309.04849) | 该论文提出了EmoDistill，这是一个利用知识蒸馏来学习从语音中获取情感的强大的语言和语音表示的语音情感识别框架。通过在训练过程中利用经过SER微调的预训练语音和语言教师进行信息蒸馏，该方法在IEMOCAP基准测试中实现了最新的最高准确率，表明其在单模态和多模态技术中的优越性能。 |
| [^16] | [HAct: Out-of-Distribution Detection with Neural Net Activation Histograms.](http://arxiv.org/abs/2309.04837) | 本论文提出了一种用于神经网络的数据集外检测方法，使用HAct激活直方图描述符对OOD进行检测，具有简单高效和准确性，并且在多个OOD图像分类基准测试中表现优于先前最先进的方法。 |
| [^17] | [Global Convergence of Receding-Horizon Policy Search in Learning Estimator Designs.](http://arxiv.org/abs/2309.04831) | RHPG算法是第一个在学习最优线性估计器设计方面具有可证明全局收敛性的PG算法，通过将普通的PG集成到动态规划的外循环中，将无约束且强凸的静态估计问题分解成受限且非凸的无穷时域KF问题，从而实现了全局收敛。 |
| [^18] | [Correcting sampling biases via importancereweighting for spatial modeling.](http://arxiv.org/abs/2309.04824) | 通过重要性重新加权修正采样偏差的空间建模方法，可以有效解决分布偏移问题，实现目标错误的无偏估计。该方法在人工数据实验中表现出优势，使预测误差从7%降至2%，且样本量越大效果越好。 |
| [^19] | [ABC Easy as 123: A Blind Counter for Exemplar-Free Multi-Class Class-agnostic Counting.](http://arxiv.org/abs/2309.04820) | ABC123是一种无需使用示例进行训练或推断的多类别类别无关计数方法，通过引入一个新的范式，它在多种对象同时存在的情况下优于现有方法。 |
| [^20] | [Detecting Violations of Differential Privacy for Quantum Algorithms.](http://arxiv.org/abs/2309.04819) | 本文定义了一个形式化框架，用于检测量子算法的差分隐私违规行为，并开发了相应的检测算法。算法通过使用张量网络作为数据结构，并在TensorFlow Quantum和TorchQuantum上执行，来验证量子算法是否具有差分隐私。当差分隐私违规报告时，算法会自动生成错误信息，包括违反隐私的量子状态，以说明违规的原因。 |
| [^21] | [Neural Latent Geometry Search: Product Manifold Inference via Gromov-Hausdorff-Informed Bayesian Optimization.](http://arxiv.org/abs/2309.04810) | 本论文提出了神经潜在几何搜索(NLGS)的概念，旨在通过格罗莫夫-豪斯多夫距离来自动识别下游任务的最佳潜在几何结构，以提高机器学习模型的性能。 |
| [^22] | [A Full-fledged Commit Message Quality Checker Based on Machine Learning.](http://arxiv.org/abs/2309.04797) | 基于机器学习的全面提交消息质量检查器，能够自动评估提交消息的质量，并提供上下文和语义的检查。 |
| [^23] | [Stochastic Gradient Descent outperforms Gradient Descent in recovering a high-dimensional signal in a glassy energy landscape.](http://arxiv.org/abs/2309.04788) | 本研究利用动力学均场理论研究了随机梯度下降（SGD）在高维非凸成本函数优化中的表现。实验结果表明，SGD在恢复高维非线性加密信号问题上明显优于梯度下降（GD）。 |
| [^24] | [RRCNN$^{+}$: An Enhanced Residual Recursive Convolutional Neural Network for Non-stationary Signal Decomposition.](http://arxiv.org/abs/2309.04782) | RRCNN$^{+}$是一种增强的残差递归卷积神经网络，用于非平稳信号分解。它能更稳定地分解信号并具有较低的计算成本，深度学习为非平稳信号分解提供了新的视角。 |
| [^25] | [Towards Robust Model Watermark via Reducing Parametric Vulnerability.](http://arxiv.org/abs/2309.04777) | 这篇论文提出了一种通过减少参数弱点来改进模型水印的方法，实验结果表明这种方法可以在近邻中找到并恢复去水印模型的水印行为。 |
| [^26] | [AudRandAug: Random Image Augmentations for Audio Classification.](http://arxiv.org/abs/2309.04762) | AudRandAug是一种应用于音频数据的随机图像增强方法，它从专门的音频搜索空间中选择数据增强策略，并在准确率表现方面优于其他方法。 |
| [^27] | [A Comprehensive Survey on Deep Learning Techniques in Educational Data Mining.](http://arxiv.org/abs/2309.04761) | 本调研综合审查了在教育数据挖掘中深度学习技术的最新研究进展，包括对知识跟踪、学生不良行为检测、性能预测和个性化推荐等典型教育场景的应用。同时提供了公共数据集和处理工具的综合概述，并指出了未来的研究方向。 |
| [^28] | [RR-CP: Reliable-Region-Based Conformal Prediction for Trustworthy Medical Image Classification.](http://arxiv.org/abs/2309.04760) | RR-CP 提出了一种可靠区域基础的确认预测方法，为医学图像分类提供高效干预和质量检查。它通过优化预测集的大小，在用户指定的错误率下提供更强的统计保证。 |
| [^29] | [Affine Invariant Ensemble Transform Methods to Improve Predictive Uncertainty in ReLU Networks.](http://arxiv.org/abs/2309.04742) | 本文提出了一种仿射不变集成变换方法，可以改善在ReLU网络中的预测不确定性。通过基于集合卡尔曼滤波的贝叶斯推断，我们提出了两个相互作用的粒子系统，并证明了它们的收敛性。同时，我们还探讨了这些方法用于量化预测不确定性的有效性。 |
| [^30] | [Training of Spiking Neural Network joint Curriculum Learning Strategy.](http://arxiv.org/abs/2309.04737) | 该论文提出了一种将课程学习引入脉冲神经网络的训练模型，使其更类似于人类学习过程，并提高了其生物解释性。 |
| [^31] | [A Spatiotemporal Deep Neural Network for Fine-Grained Multi-Horizon Wind Prediction.](http://arxiv.org/abs/2309.04733) | 提出了一种用于细粒度多时空风力预测的多时空网络模型(MHSTN)，该模型能够从多个数据源中提取特征，并产生精确和高效的预测结果。 |
| [^32] | [TCGAN: Convolutional Generative Adversarial Network for Time Series Classification and Clustering.](http://arxiv.org/abs/2309.04732) | TCGAN是一个用于时间序列识别的卷积生成对抗网络，通过对抗博弈学习时间序列的分层表示，无需标记信息。 |
| [^33] | [Transitions in echo index and dependence on input repetitions.](http://arxiv.org/abs/2309.04728) | 回声指数是一个非自治动力系统中同时稳定渐近响应的数量，我们研究了回声指数对依赖于参数以及输入重复的关系。 |
| [^34] | [Toward Reproducing Network Research Results Using Large Language Models.](http://arxiv.org/abs/2309.04716) | 本文提出使用大型语言模型（LLMs）来重现网络研究结果，通过一个小规模实验证明了其可行性，并以ChatGPT为工具重现了不同发表于著名会议和期刊的网络系统。 |
| [^35] | [Advantage Actor-Critic with Reasoner: Explaining the Agent's Behavior from an Exploratory Perspective.](http://arxiv.org/abs/2309.04707) | 本文提出了一种新颖的基于Reasoner的优势演员-评论员（A2CR）方法，通过预定义和分类演员行为的潜在目的，自动生成一个更全面和可解释的理解代理决策过程的范例。 |
| [^36] | [Analysis of Disinformation and Fake News Detection Using Fine-Tuned Large Language Model.](http://arxiv.org/abs/2309.04704) | 本研究考虑使用LLM模型通过细调实现虚假信息和假新闻的深入分析，揭示复杂的风格和叙事，并提取命名实体的情感，以此作为监督机器学习模型中的预测性特征。 |
| [^37] | [Weak-PDE-LEARN: A Weak Form Based Approach to Discovering PDEs From Noisy, Limited Data.](http://arxiv.org/abs/2309.04699) | 弱-PDE-LEARN是一种基于弱形式的算法，可以从有噪声、有限数据中发现非线性PDEs。 |
| [^38] | [Redundancy-Free Self-Supervised Relational Learning for Graph Clustering.](http://arxiv.org/abs/2309.04694) | 本文提出了一种名为无冗余关系图聚类（R^2FGC）的自监督深度图聚类方法，该方法从全局和局部视角提取属性和结构级别的关系信息，从而有效地利用了图结构化数据的语义信息。 |
| [^39] | [Flexible and Robust Counterfactual Explanations with Minimal Satisfiable Perturbations.](http://arxiv.org/abs/2309.04676) | 本论文提出了一种名为CEMSP的解决方案，通过限制异常特征的变化值来提供灵活且健壮的对事实解释。 |
| [^40] | [Compact: Approximating Complex Activation Functions for Secure Computation.](http://arxiv.org/abs/2309.04664) | Compact提出了一种逼近复杂激活函数的方法，可以与MPC技术高效配合使用，并且几乎不会损失模型准确性。 |
| [^41] | [MADLAD-400: A Multilingual And Document-Level Large Audited Dataset.](http://arxiv.org/abs/2309.04662) | MADLAD-400是一种覆盖419种语言的多语言文档级数据集，通过自我审核揭示了局限性，通过训练众多参数的机器翻译模型取得了竞争力，并提供了基准模型给研究界使用。 |
| [^42] | [Intelligent upper-limb exoskeleton using deep learning to predict human intention for sensory-feedback augmentation.](http://arxiv.org/abs/2309.04655) | 创新点在于引入了云端深度学习和嵌入式柔性传感器，实现了智能上肢外骨骼系统来预测人类上肢运动的意图并提供感觉反馈，准确率达到96.2%，能以人类意图为基础进行操作。 |
| [^43] | [Towards Understanding Neural Collapse: The Effects of Batch Normalization and Weight Decay.](http://arxiv.org/abs/2309.04644) | 本文研究了批归一化和权重衰减对神经网络坍塌现象的影响，并提出了对坍塌现象进行度量的方法。通过理论和实验，证明了在接近最优时，批归一化和权重衰减能够促使神经网络坍塌的出现。 |
| [^44] | [Few-Shot Learning of Force-Based Motions From Demonstration Through Pre-training of Haptic Representation.](http://arxiv.org/abs/2309.04640) | 该论文提出了一种能够从少量示范中学习力基动作的方法，通过对触觉表示进行预训练，并利用示范学习来训练动作生成器。实验证明，预训练显著提高了模型识别物理特性和生成所需动作的能力。 |
| [^45] | [Probabilistic Safety Regions Via Finite Families of Scalable Classifiers.](http://arxiv.org/abs/2309.04627) | 本论文提出了概率安全区域的概念，用于描述一个输入空间子集，在这个子集中，误分类实例的数量可以被概率上得到控制。同时，利用可伸缩分类器来将机器学习的调参与误差控制相结合。 |
| [^46] | [Perceptual adjustment queries and an inverted measurement paradigm for low-rank metric learning.](http://arxiv.org/abs/2309.04626) | 感知调整查询（PAQ）是一种新的用于收集人类反馈的查询机制，采用反向测量方案，结合了基数查询和序数查询的优点。我们将PAQ应用于度量学习问题中，通过PAQ测量来学习未知的马氏距离，并开发了一个两阶段估计器，提供了样本复杂性保证。 |
| [^47] | [Knowledge Distillation-Empowered Digital Twin for Anomaly Detection.](http://arxiv.org/abs/2309.04616) | 本文提出了基于知识蒸馏的数字孪生模型KDDT，用于列车控制和管理系统（TCMS）的异常检测。KDDT利用语言模型和长短期记忆网络分别提取上下文和时间顺序特征，通过知识蒸馏丰富数据量。实验结果表明KDDT在两个数据集上取得了较高的F1分数0.931和0.91。 |
| [^48] | [Leveraging World Model Disentanglement in Value-Based Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2309.04615) | 本文提出了一种新的基于模型的多智能体强化学习方法，通过使用模块化的世界模型，减少了多智能体系统中训练的样本复杂性，并成功预测了联合动作价值函数。 |
| [^49] | [Self-optimizing Feature Generation via Categorical Hashing Representation and Hierarchical Reinforcement Crossing.](http://arxiv.org/abs/2309.04612) | 通过分类哈希表示和分层强化交叉，提出了一种自优化特征生成的通用框架，解决了现有系统中的一些挑战，包括有意义、稳健和高效的生成。 |
| [^50] | [Motif-aware Attribute Masking for Molecular Graph Pre-training.](http://arxiv.org/abs/2309.04589) | 本研究提出并研究了一种模式感知的属性屏蔽策略，通过利用相邻模式中的原子信息来捕捉模式间的结构，从而提高分子图预训练的效果。 |
| [^51] | [Dynamic Mesh-Aware Radiance Fields.](http://arxiv.org/abs/2309.04581) | 本文介绍了一种将多边形网格与神经辐射场 (NeRF) 相互耦合的方法，以实现在渲染和模拟过程中对网格进行物理一致性处理。训练了使用高动态范围图像的 NeRF，并提出了估计光源和投射阴影的策略。同时考虑了如何高效地集成混合表面-体积表达式与图形渲染框架。 |
| [^52] | [Unleashing the Power of Graph Learning through LLM-based Autonomous Agents.](http://arxiv.org/abs/2309.04565) | 本文提出了一种使用大型语言模型（LLMs）作为自主代理的方法，以简化多样化的现实世界图中的学习过程，并克服了现有方法中的限制。 |
| [^53] | [When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale.](http://arxiv.org/abs/2309.04564) | 在这项工作中，研究人员探究了数据修剪对大规模预训练语言模型(LLMs)的影响。通过比较数据质量评估器和修剪预训练语料库后训练的LLMs，他们发现困惑度作为一种简单的技术优于更加计算密集的评分方法。 |
| [^54] | [Towards Interpretable Solar Flare Prediction with Attention-based Deep Neural Networks.](http://arxiv.org/abs/2309.04558) | 用基于注意力的深度学习模型改进了太阳耀斑预测，成功预测了≥M1.0级太阳耀斑的发生，并通过注意力图可解释模型的决策。 |
| [^55] | [Regret-Optimal Federated Transfer Learning for Kernel Regression with Applications in American Option Pricing.](http://arxiv.org/abs/2309.04557) | 本论文提出了一种遗憾最优算法的迭代方案，用于联邦迁移学习，在核回归模型中具体化，并提出了一个几乎遗憾最优的启发式算法，可以减小生成参数与专门参数之间的累积偏差。 |
| [^56] | [Connecting NTK and NNGP: A Unified Theoretical Framework for Neural Network Learning Dynamics in the Kernel Regime.](http://arxiv.org/abs/2309.04522) | 本文提出了一个马尔可夫近似学习模型，统一了神经切向核（NTK）和神经网络高斯过程（NNGP）核，用于描述无限宽度深层网络的学习动力学。 |
| [^57] | [End-to-End Speech Recognition and Disfluency Removal with Acoustic Language Model Pretraining.](http://arxiv.org/abs/2309.04516) | 该论文研究了利用声学语言模型预训练的端到端语音识别和消除语言迟滞的方法，发现基于音频的语言模型使用自监督目标进行预训练可以与两阶段模型的性能相匹配甚至超越。 |
| [^58] | [Privacy Preserving Federated Learning with Convolutional Variational Bottlenecks.](http://arxiv.org/abs/2309.04515) | 本文研究了使用卷积变分瓶颈的隐私保护联合学习，并发现了PRECODE的工作原理和影响。PRECODE通过引入随机性梯度防止梯度反转攻击的收敛，但也提出了一种攻击方法来禁用其隐私保护效果。 |
| [^59] | [Decreasing the Computing Time of Bayesian Optimization using Generalizable Memory Pruning.](http://arxiv.org/abs/2309.04510) | 该论文提出了一个通用的贝叶斯优化方法，通过记忆修剪和有界优化，可以降低计算时间，并适用于任何代理模型和获取函数。 |
| [^60] | [Spatial-Temporal Graph Attention Fuser for Calibration in IoT Air Pollution Monitoring Systems.](http://arxiv.org/abs/2309.04508) | 这项研究提出了一种新颖的方法，利用图神经网络中的图注意力网络模块，通过融合传感器阵列的数据来增强IoT空气污染监测平台中传感器的校准精度。 |
| [^61] | [Generating drawdown-realistic financial price paths using path signatures.](http://arxiv.org/abs/2309.04507) | 本文提出了一种新颖的机器学习方法，使用路径签名来生成逼近实际数据的金融价格路径，并应用于定价回撤保险期权和投资组合回撤控制策略。 |
| [^62] | [COVID-19 Detection System: A Comparative Analysis of System Performance Based on Acoustic Features of Cough Audio Signals.](http://arxiv.org/abs/2309.04505) | 本研究通过比较分析声学特征对咳嗽音频信号的机器学习模型性能的影响，提出了一种高效的COVID-19检测系统。 |
| [^63] | [Compositional Learning of Visually-Grounded Concepts Using Reinforcement.](http://arxiv.org/abs/2309.04504) | 本研究探讨了深度强化学习代理如何学习和组合基于颜色和形状的组合指令，以解决空间导航任务中的新颖组合。通过利用冻结的文本编码器，代理所需的训练回合数减少了20倍。 |
| [^64] | [Weighted Unsupervised Domain Adaptation Considering Geometry Features and Engineering Performance of 3D Design Data.](http://arxiv.org/abs/2309.04499) | 本论文提出了一种考虑几何特征和工程性能的加权无监督领域适应方法，专门用于基于深度学习的工程性能预测。通过对抗训练策略，可以提取领域不变特征。 |
| [^65] | [Safe Neural Control for Non-Affine Control Systems with Differentiable Control Barrier Functions.](http://arxiv.org/abs/2309.04492) | 本文提出了一种安全神经控制器方法，通过将高阶控制屏障函数整合到神经网络中，解决了非仿射控制系统的安全关键控制问题，并改进了现有的解决方案的次优性能。 |
| [^66] | [Addressing the Accuracy-Cost Tradeoff in Material Property Prediction: A Teacher-Student Strategy.](http://arxiv.org/abs/2309.04482) | 本论文提出了一种师生策略，利用预训练的结构模型作为“老师”，以提高基于化学组分的材料性能预测模型的准确性。通过验证，该策略在不同网络结构上都能有效提高模型的准确性。 |
| [^67] | [Multimodal machine learning for materials science: composition-structure bimodal learning for experimentally measured properties.](http://arxiv.org/abs/2309.04478) | 本论文介绍了一种基于成分-结构双模态学习的多模态机器学习方法，用于增强对实验测量材料性质的学习和预测，从而降低预测误差。 |
| [^68] | [Crystal Structure Prediction by Joint Equivariant Diffusion.](http://arxiv.org/abs/2309.04475) | 本文提出了DiffCSP，一种利用周期-E(3)-等变去噪模型的扩散模型，用于通过联合生成晶体的晶格和原子坐标，以解决晶体结构预测中的对称性挑战。 |
| [^69] | [Weakly supervised learning for pattern classification in serial femtosecond crystallography.](http://arxiv.org/abs/2309.04474) | 本文介绍了在串行飞秒晶体学中通过弱监督算法对衍射图进行分类的工作，旨在尽可能减少训练所需的标签数据集的规模。 |
| [^70] | [Improved theoretical guarantee for rank aggregation via spectral method.](http://arxiv.org/abs/2309.03808) | 本论文通过谱方法改进了排名聚合问题的理论保证，通过研究基于未归一化和归一化数据矩阵的谱排名算法，提供了更准确的扰动误差界限。 |
| [^71] | [Generating quantum feature maps using multi-objective genetic algorithm.](http://arxiv.org/abs/2309.03307) | 本文介绍了一种使用多目标遗传算法生成量子特征映射的方法，以实现对高维希尔伯特空间的访问，并在优化电路配置时同时考虑分类准确性和门成本。实验结果显示最佳电路配置中纠缠门需要相应的数量，与之前研究相反。同时，我们还提出了使用数据的可分离性指数确定最佳配置的方法。 |
| [^72] | [No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function.](http://arxiv.org/abs/2309.03224) | 该论文提出了一种通过蒙特卡洛树搜索和能量函数引导来释放大型语言模型的数学推理能力的方法，以解决当前在数学推理任务中的不足和错误。该方法不需要进一步的微调步骤，通过重新定义模型和引入路径验证器的方式，实现了对输出空间的搜索和推理路径的评估。 |
| [^73] | [CR-VAE: Contrastive Regularization on Variational Autoencoders for Preventing Posterior Collapse.](http://arxiv.org/abs/2309.02968) | CR-VAE是一种对比正则化变分自动编码器的方法，通过增加对比目标来最大化类似视觉输入之间的互信息，从而避免后验坍塌现象，并在多个视觉数据集上表现出比最先进方法更好的性能。 |
| [^74] | [On Reducing Undesirable Behavior in Deep Reinforcement Learning Models.](http://arxiv.org/abs/2309.02869) | 本论文提出了一个旨在降低深度强化学习模型不良行为的框架，通过从错误的状态-动作对中提取决策树分类器，并将其整合到训练循环中，来惩罚系统错误行为。这一框架在保持卓越性能的同时，为工程师提供了针对不良行为的可理解表征。 |
| [^75] | [Efficient Defense Against Model Stealing Attacks on Convolutional Neural Networks.](http://arxiv.org/abs/2309.01838) | 本文提出了一种简单但有效且高效的防御替代方案，引入一种启发式方法来扰动输出概率，可以轻松集成到模型中而无需额外训练，并且在抵御最先进的窃取攻击中表现出有效性。 |
| [^76] | [Les Houches Lectures on Deep Learning at Large & Infinite Width.](http://arxiv.org/abs/2309.01592) | 本论文主要以无穷宽度和大宽度范围内的深度神经网络为研究对象，讨论了这些网络的各种统计和动力学特性，包括随机网络的性质、训练后的网络与线性模型、核函数和高斯过程之间的关系，以及对大但有限宽度网络在初始化和训练后的摄动和非摄动处理。 |
| [^77] | [Comparative Analysis of Deep Learning Architectures for Breast Cancer Diagnosis Using the BreaKHis Dataset.](http://arxiv.org/abs/2309.01007) | 本研究评估了五个深度学习模型在使用BreaKHis数据集进行乳腺癌诊断时的性能，并发现Xception模型在准确性和F1得分方面表现最佳，这表明了深度学习方法在乳腺癌诊断中的重要性。 |
| [^78] | [Area-norm COBRA on Conditional Survival Prediction.](http://arxiv.org/abs/2309.00417) | 本文提出了一种基于组合回归的条件生存预测方法，其中使用面积作为相似度度量，通过选择最重要的变量来提高模型性能。 |
| [^79] | [Efficacy of Neural Prediction-Based NAS for Zero-Shot NAS Paradigm.](http://arxiv.org/abs/2308.16775) | 这项研究提出了一种新的方法，通过深度学习进行零样本架构搜索，通过使用可学习的傅里叶正弦和求和编码来构建计算的前馈图，从而解决了基于预测的神经架构搜索中性能指标泛化的限制。 |
| [^80] | [Online GentleAdaBoost -- Technical Report.](http://arxiv.org/abs/2308.14004) | 该论文研究了在线版的GentleAdaboost算法，通过在线方式将弱分类器与强分类器结合，提出了一种通过线搜索将批处理方法扩展为在线方法的方法，并与其他在线方法在各种基准数据集上进行了对比。 |
| [^81] | [Stochastic Configuration Machines for Industrial Artificial Intelligence.](http://arxiv.org/abs/2308.13570) | 本文提出了一种新颖的随机学习器模型，称为随机配置机（SCMs），其基于随机配置网络（SCNs），旨在强调工业人工智能中的有效建模和节约数据大小。SCMs通过压缩模型存储，并保持有利的预测性能，具有在工业应用中很大的潜力。 |
| [^82] | [MLLM-DataEngine: An Iterative Refinement Approach for MLLM.](http://arxiv.org/abs/2308.13566) | 本文提出了一种名为MLLM-DataEngine的迭代改进方法，它通过分析模型弱点，生成适当的增量数据集并迭代地增强模型能力。与以往方法相比，MLLM-DataEngine生成的数据在定位、质量和正确性方面表现更好。 |
| [^83] | [Gotta match 'em all: Solution diversification in graph matching matched filters.](http://arxiv.org/abs/2308.13451) | 本文提出了一种在大规模背景图中查找多个嵌入的模板图的新方法，通过迭代惩罚相似度矩阵来实现多样化匹配的发现，并提出了算法加速措施。在理论验证和实验证明中，证明了该方法的可行性和实用性。 |
| [^84] | [Objective-Agnostic Enhancement of Molecule Properties via Multi-Stage VAE.](http://arxiv.org/abs/2308.13066) | 本文提出了一种多阶段VAE方法，可以改善在药物发现领域中恢复低维流形的问题。实验结果表明，该方法显著改善了生成分子的属性统计，而无需集成属性预测器。 |
| [^85] | [Financial News Analytics Using Fine-Tuned Llama 2 GPT Model.](http://arxiv.org/abs/2308.13032) | 本研究通过精细调整的Llama 2模型实现了金融新闻的多任务分析，包括文本分析、摘要和情感提取等。实验结果显示，提取的命名实体情感可以作为有监督机器学习模型的预测特征。 |
| [^86] | [An Effective Transformer-based Contextual Model and Temporal Gate Pooling for Speaker Identification.](http://arxiv.org/abs/2308.11241) | 本文介绍了一种基于Transformer的上下文模型和时间门池化的有效方法，应用于说话人识别，并在准确率85.9%的情况下比较了其性能与wav2vec2方法。 |
| [^87] | [Beyond expectations: Residual Dynamic Mode Decomposition and Variance for Stochastic Dynamical Systems.](http://arxiv.org/abs/2308.10697) | 本文研究了随机动力系统中的动态模式分解(DMD)方法。通过在Koopman框架中引入方差来解决挑战，包括虚假模式和本征谱。结果表明，需要超越预期来有效处理随机系统。 |
| [^88] | [Multi-Task Pseudo-Label Learning for Non-Intrusive Speech Quality Assessment Model.](http://arxiv.org/abs/2308.09262) | 这篇论文介绍了一种基于多任务伪标签学习的非侵入式语音质量评估模型（MTQ-Net），通过与预训练模型结合使用伪标签来进行训练，实验结果证明了其相对于从头训练模型的优势。 |
| [^89] | [SciRE-Solver: Efficient Sampling of Diffusion Probabilistic Models by Score-integrand Solver with Recursive Derivative Estimation.](http://arxiv.org/abs/2308.07896) | SciRE-Solver是一种高效的采样器，通过引入得分积分求解器和递归导数估计方法，它解决了扩散概率模型采样过程缓慢的挑战，并实现了最先进的采样性能。 |
| [^90] | [Adaptive Tracking of a Single-Rigid-Body Character in Various Environments.](http://arxiv.org/abs/2308.07491) | 本研究提出了一种基于单刚体角色仿真的深度强化学习方法，通过训练一个能够自适应各种环境变化的策略，实现在不需要额外学习的情况下完成各种任务。 |
| [^91] | [Deep Metric Learning for the Hemodynamics Inference with Electrocardiogram Signals.](http://arxiv.org/abs/2308.04650) | 本研究提出了一种利用心电图信号进行血流动力学推理的深度度量学习方法。通过使用非侵入性信号进行心脏压力的评估，既可以在住院环境中用于诊断和治疗预后，也可以应用于门诊设置中。 |
| [^92] | [Exploiting Synthetic Data for Data Imbalance Problems: Baselines from a Data Perspective.](http://arxiv.org/abs/2308.00994) | 本文提出了一个简单而有效的基准线SYNAuG，利用合成数据来解决数据不平衡问题，并在多个数据集上取得了令人印象深刻的性能，超过了现有方法。 |
| [^93] | [Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback.](http://arxiv.org/abs/2307.15217) | 本文调查了从人类反馈中进行强化学习的开放问题和基本限制，并提出了加强社会监督的审计和披露标准。 |
| [^94] | [InVAErt networks: a data-driven framework for emulation, inference and identifiability analysis.](http://arxiv.org/abs/2307.12586) | InVAErt网络是一个数据驱动的框架，用于分析和合成物理系统，具有模型反演和可识别性分析的能力。 |
| [^95] | [Interpreting and Correcting Medical Image Classification with PIP-Net.](http://arxiv.org/abs/2307.10404) | 本研究利用PIP-Net开展了可解释的机器学习技术在医学图像分类中的应用，并展示了其在骨折检测和皮肤癌诊断方面的准确性和可解释性。通过无监督的预训练，PIP-Net能够轻松识别数据质量问题，并且我们还发现人们可以通过手动禁用不良原型来纠正PIP-Net的推理过程。 |
| [^96] | [Creating a Dataset Supporting Translation Between OpenMP Fortran and C++ Code.](http://arxiv.org/abs/2307.07686) | 本研究创建了一个数据集，用于训练机器学习模型在OpenMP Fortran和C++代码之间进行翻译。这个数据集通过精细的代码相似性测试确保了可靠性和适用性，并且能够显著提升大规模语言模型的翻译能力。 |
| [^97] | [Weisfeiler and Lehman Go Measurement Modeling: Probing the Validity of the WL Test.](http://arxiv.org/abs/2307.05775) | 本文通过系统分析，揭示了$k$-WL的可靠性和有效性问题，并提出了基于基准的表达能力的外延定义和测量。 |
| [^98] | [A Survey From Distributed Machine Learning to Distributed Deep Learning.](http://arxiv.org/abs/2307.05232) | 这篇综述论文介绍了分布式机器学习和分布式深度学习的研究现状和方法，强调了分布式深度学习在解决复杂问题方面取得的重要进展。 |
| [^99] | [An Algorithm with Optimal Dimension-Dependence for Zero-Order Nonsmooth Nonconvex Stochastic Optimization.](http://arxiv.org/abs/2307.04504) | 提出了一个维度依赖优化度为$O(d\delta^{-1}\epsilon^{-3})$的最优算法，并证明了非凸随机零阶设置中非光滑优化与光滑优化的一样容易。 |
| [^100] | [Towards Trustworthy Explanation: On Causal Rationalization.](http://arxiv.org/abs/2306.14115) | 该论文介绍了一种新的因果关系解释方法，通过在解释中引入非虚假性和效率，从因果推断的角度定义了因果概率，从而建立了必要和充分解释的主要组成部分，相比现有的基于关联的解释方法，这种方法有更加优越的性能表现。 |
| [^101] | [Aircraft Environmental Impact Segmentation via Metric Learning.](http://arxiv.org/abs/2306.13830) | 本文介绍了机载环境影响建模中的度量学习方法，在弱监督度量学习任务下取得了显著性能提升，有望实现对机载环境影响更高效、更精准的建模。 |
| [^102] | [QNNRepair: Quantized Neural Network Repair.](http://arxiv.org/abs/2306.13793) | QNNRepair 是一种用于修复量化神经网络的方法，通过解决神经元权重参数以修复在神经网络量化过程中导致性能下降的神经元，从而在不影响通过测试的性能的同时，提高在失败测试上的性能。 |
| [^103] | [Estimating the Value of Evidence-Based Decision Making.](http://arxiv.org/abs/2306.13681) | 本文提出了一个实证框架，用于估算证据决策的价值和统计精度投资回报。 |
| [^104] | [An Overview of Catastrophic AI Risks.](http://arxiv.org/abs/2306.12001) | 本文综述了人工智能灾难性风险的四个主要来源，包括恶意使用、人工智能竞赛、组织风险和流氓人工智能。 |
| [^105] | [In Search of netUnicorn: A Data-Collection Platform to Develop Generalizable ML Models for Network Security Problems.](http://arxiv.org/abs/2306.08853) | 该论文提出了一个用于网络安全问题的通用机器学习模型开发的数据收集平台，解决了ML模型在不同网络环境中的一般化问题，并提出了一个利用可解释的机器学习工具来指导网络数据收集的增强ML流程。 |
| [^106] | [Improving Offline-to-Online Reinforcement Learning with Q-Ensembles.](http://arxiv.org/abs/2306.06871) | 我们提出了一种名为Q-Ensembles的新框架，通过增加Q网络的数量，无缝地连接离线预训练和在线微调，同时不降低性能。此外，我们适当放宽Q值估计的悲观性，并将基于集合的探索机制融入我们的框架中，从而提升了离线到在线强化学习的性能。 |
| [^107] | [Continually learning out-of-distribution spatiotemporal data for robust energy forecasting.](http://arxiv.org/abs/2306.06385) | 该论文提出了一种持续学习方法来对非分布式的时空数据进行能源预测，以适应入住模式的变化，并通过在线学习调整能源使用。 |
| [^108] | [Analysing high resolution digital Mars images using machine learning.](http://arxiv.org/abs/2305.19958) | 本研究利用卷积神经网络应用于高分辨率数码火星图像，以分析寻找火星上的短暂液态水斑块。先前的手动图像分析已经发现37张图像含有较小冰斑，并通过亮度、颜色和与地形遮蔽的强关联性进行了区分。 |
| [^109] | [Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning.](http://arxiv.org/abs/2305.17256) | 本文探讨了大型语言模型在上下文学习中利用提示中的捷径的依赖性，发现大型模型更有可能在推理过程中利用提示中的捷径，这为评估上下文学习的稳健性和检测和缓解提示中捷径的使用提供了新的视角和挑战。 |
| [^110] | [Graph Neural Network Interatomic Potential Ensembles with Calibrated Aleatoric and Epistemic Uncertainty on Energy and Forces.](http://arxiv.org/abs/2305.16325) | 本文提出了一个完整的框架来培训和重新校准图神经网络合奏模型，以产生带有校准不确定性估计的能量和力的准确预测，可以应用于材料的结构优化和分子动力学模拟。 |
| [^111] | [Concept-Centric Transformers: Concept Transformers with Object-Centric Concept Learning for Interpretability.](http://arxiv.org/abs/2305.15775) | 本文研究了以物体为中心的概念学习，它可以提高基于概念的Transformer模型的分类性能和可解释性。 |
| [^112] | [Successor-Predecessor Intrinsic Exploration.](http://arxiv.org/abs/2305.15277) | 后续前导内在探索是一种基于新颖内在奖励的探索算法，它利用回顾信息并结合前瞻信息，以在强化学习中实现高效且生态学合理的探索行为。 |
| [^113] | [Bayesian Numerical Integration with Neural Networks.](http://arxiv.org/abs/2305.13248) | 本文提出了一种基于神经网络架构的贝叶斯数值积分方法，称为贝叶斯 Stein 神经网络。该方法可高效地编码积分先验信息并计算积分估计的不确定性。在实际问题中，该方法展现出数量级的加速的优势。 |
| [^114] | [Massively Scalable Inverse Reinforcement Learning in Google Maps.](http://arxiv.org/abs/2305.11290) | 本文提出了一种新的逆强化学习算法（RHIP），通过图压缩、并行化和基于主特征向量的问题初始化解决了全球规模的MDPs、大型数据集和高度参数化的模型的问题，在谷歌地图中实现了16-24%的全球路线质量改进。 |
| [^115] | [Document Understanding Dataset and Evaluation (DUDE).](http://arxiv.org/abs/2305.08455) | DUDE推出了一个新的数据集和评估方法，旨在创造一个更实际的基准测试并推动当前方法的边界，以更准确地模拟真实世界的情况 |
| [^116] | [Provably Convergent Schr\"odinger Bridge with Applications to Probabilistic Time Series Imputation.](http://arxiv.org/abs/2305.07247) | 本论文提出了一种基于近似投影的Schr\"odinger bridge算法，它能够应用于概率时间序列填充，并在医疗保健和环境数据方面实现最先进的结果。 |
| [^117] | [ST-GIN: An Uncertainty Quantification Approach in Traffic Data Imputation with Spatio-temporal Graph Attention and Bidirectional Recurrent United Neural Networks.](http://arxiv.org/abs/2305.06480) | 本研究提出了一种创新的交通数据插值方法，利用图注意力和双向神经网络捕捉时空相关性，实验结果表明在处理缺失值方面优于其他基准技术。 |
| [^118] | [The EarlyBIRD Catches the Bug: On Exploiting Early Layers of Encoder Models for More Efficient Code Classification.](http://arxiv.org/abs/2305.04940) | 本文介绍了一种早期层组合的方法EarlyBIRD，该方法可以有效利用深度自然语言处理模型的资源和可用信息，从而提高代码分类的性能，在缺陷检测方面平均可提高2个点。 |
| [^119] | [Matching-based Data Valuation for Generative Model.](http://arxiv.org/abs/2304.10701) | 本论文提出了基于匹配的生成模型数据估值方法，这是一个针对任何生成模型的模型无关方法，可以对数据实例进行估值，而无需重新训练模型，并在估值效果上表现出色。 |
| [^120] | [SURFSUP: Learning Fluid Simulation for Novel Surfaces.](http://arxiv.org/abs/2304.06197) | SURFSUP采用有符号距离函数连续表示对象，提供更准确和高效的流体对象相互作用的学习方法；且能够适用于分布之外的复杂真实场景和对象，可以反演适用于物体操纵流体流动。 |
| [^121] | [Learning Personalized Models with Clustered System Identification.](http://arxiv.org/abs/2304.01395) | 该文提出了一种基于聚类的系统识别学习个性化模型的算法，将多个系统划分为群集，同一簇中的系统可以从其他系统的观察中获益。该算法实现了正确估计群集标识并具有高效和个性化的系统识别过程。 |
| [^122] | [Inverse Reinforcement Learning without Reinforcement Learning.](http://arxiv.org/abs/2303.14623) | 该论文提出了一种新的逆强化学习简化方法，通过利用专家的状态分布来减少强化学习子例程的全局探索部分，实现了指数级的加速，大大提高了样本复杂度和时间复杂度的效果。 |
| [^123] | [Deepfake in the Metaverse: Security Implications for Virtual Gaming, Meetings, and Offices.](http://arxiv.org/abs/2303.14612) | 元宇宙中的深度伪造带来了严重的安全影响，特别是在虚拟游戏、会议和办公场所等场景中，攻击者可以通过深度伪造冒充他人进行攻击。 |
| [^124] | [Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament.](http://arxiv.org/abs/2303.07925) | 本文采用 Numerai 数据科学竞赛的数据，探究了多变量时间序列建模中不同特征工程和降维方法的应用；提出了一种新的集成方法，用于高维时间序列建模，该方法在通用性、鲁棒性和效率上优于一些深度学习模型。 |
| [^125] | [TSMixer: An all-MLP Architecture for Time Series Forecasting.](http://arxiv.org/abs/2303.06053) | TSMixer是一种通过堆叠多层感知器（MLP）设计的新型结构，基于沿时间和特征维度的混合操作，能够在时间序列预测中表现出极好的性能。 |
| [^126] | [Online Control Barrier Functions for Decentralized Multi-Agent Navigation.](http://arxiv.org/abs/2303.04313) | 本文提出了一种在连续域中实现可保证安全的多智能体导航的方法，通过在线调整CBF的超参数来适应不同的环境条件，使用强化学习和图神经网络实现无模型的CBF调整策略。 |
| [^127] | [Sample-efficient Real-time Planning with Curiosity Cross-Entropy Method and Contrastive Learning.](http://arxiv.org/abs/2303.03787) | 本文提出了一种用好奇心触发探索的改进版本的交叉熵方法（CCEM），以解决现有规划方法在复杂高维环境下无法扩展的问题。通过最大化规划视角内状态-动作Q值的和，我们的方法鼓励达到新颖的观察，并利用对比表示学习提高性能。 |
| [^128] | [Why Do Facial Deepfake Detectors Fail?.](http://arxiv.org/abs/2302.13156) | 近年来快速发展的深度伪造技术对人类身份验证提出了重大挑战，已提出多种深度伪造检测算法，但检测器经常不可靠且无法检测到深度伪造内容。这项研究强调了检测器面临的挑战，包括预处理中的伪造痕迹和未考虑到新样本生成器。需要进一步研究和开发以创建更稳健可靠的深度伪造检测器。 |
| [^129] | [Mitigating Adversarial Attacks in Deepfake Detection: An Exploration of Perturbation and AI Techniques.](http://arxiv.org/abs/2302.11704) | 本论文探索了扰动和人工智能技术在深假检测中缓解对抗攻击的方法，这是一项迫切需要解决的安全和道德问题。 |
| [^130] | [Efficient Generator of Mathematical Expressions for Symbolic Regression.](http://arxiv.org/abs/2302.09893) | 我们提出了一种基于变分自动编码器的方法来生成数学表达式，并用于符号回归。实验结果表明，我们的方法可以高效地训练，并且能够准确地编码表达式。通过优化方法探索生成的潜在空间能够解决符号回归问题，且效果优于传统的方法。 |
| [^131] | [Leveraging Reviews: Learning to Price with Buyer and Seller Uncertainty.](http://arxiv.org/abs/2302.09700) | 这项研究研究了在线市场中买家和卖家不确定性下的定价问题，买家利用其他具有相同属性的买家的评论来估算产品价值，卖家使用评论来衡量商品的需求。 |
| [^132] | [Benchmarking Continuous Time Models for Predicting Multiple Sclerosis Progression.](http://arxiv.org/abs/2302.07854) | 本研究评估了连续时间模型在预测多发性硬化进展方面的性能，发现最佳的连续模型通常能够胜过最佳的离散时间模型，并且标准化现有特征可以带来更大的性能提升。 |
| [^133] | [Prior Density Learning in Variational Bayesian Phylogenetic Parameters Inference.](http://arxiv.org/abs/2302.02522) | 本文提出了一种使用学习参数的灵活先验方法，通过多个马尔可夫链替代模型的模拟得出，该方法在估计系统发育参数方面非常有效。 |
| [^134] | [Gradient Estimation for Unseen Domain Risk Minimization with Pre-Trained Models.](http://arxiv.org/abs/2302.01497) | 该论文提出了一种新的领域广义化方法，使用大规模预训练模型估计不可观测梯度来减少未知领域中的潜在风险，从而增强模型的广义能力。 |
| [^135] | [Quantum Ridgelet Transform: Winning Lottery Ticket of Neural Networks with Quantum Computation.](http://arxiv.org/abs/2301.11936) | 这篇论文提出了量子小波变换(QRT)作为神经网络中的重要应用，通过量子计算实现了对量子态的小波变换，并且可以高效地找到大型神经网络的稀疏可训练子网络。 |
| [^136] | [PECAN: A Deterministic Certified Defense Against Backdoor Attacks.](http://arxiv.org/abs/2301.11824) | PECAN是一种有效且经过认证的后门攻击防御方法，通过在不相交分区上训练一组神经网络并应用测试时间逃避认证技术，可以显著提高防御强度和效率，降低攻击成功率。 |
| [^137] | [Unifying Synergies between Self-supervised Learning and Dynamic Computation.](http://arxiv.org/abs/2301.09164) | 本文提出了自监督学习和动态计算之间相互作用的新视角。通过在自监督学习的设置中同时学习密集和门控子网络，无需额外的微调或剪枝步骤，可以获得一个通用且高效的架构，适用于资源受限的工业环境。 |
| [^138] | [Projective Integral Updates for High-Dimensional Variational Inference.](http://arxiv.org/abs/2301.08374) | 该论文介绍了一种适用于高维变分推理的投影积分更新方法，并通过降低参数敏感性来实现更强健的预测。 |
| [^139] | [Domain-adapted Learning and Imitation: DRL for Power Arbitrage.](http://arxiv.org/abs/2301.08360) | 本文提出了一种领域自适应学习和模仿的深度强化学习方法，用于电力套利交易。通过利用奖励工程和订单分段的方式，该方法能够提高训练的收敛性，增加竞标成功率，并显著提高利润和损失。 |
| [^140] | [Relativistic Digital Twin: Bringing the IoT to the Future.](http://arxiv.org/abs/2301.07390) | 本文提出了相对论数字孪生（RDT）框架，通过不断观察物联网实体的真实对应物来自动生成通用型数字孪生，并调整其行为模型。框架利用物联网之物（WoT）提供标准化接口。 |
| [^141] | [Language as a Latent Sequence: deep latent variable models for semi-supervised paraphrase generation.](http://arxiv.org/abs/2301.02275) | 本文提出了用于半监督释义生成的深度潜变量模型，通过将未标记数据的缺失目标对建模为潜在释义序列，并结合双向学习和改进的权重初始化方案进行训练，实验结果表明这个模型在性能上与最先进的有监督基线模型有竞争力。 |
| [^142] | [Physics-Informed Neural Networks for Prognostics and Health Management of Lithium-Ion Batteries.](http://arxiv.org/abs/2301.00776) | 本研究提出了一种基于物理感知神经网络（PINN）的模型融合方案，用于预测和管理锂离子电池的健康状态。该方法能够将经验或物理动态模型与数据驱动模型相结合，以充分利用各种信息来源，提高预测和管理的准确性。 |
| [^143] | [A soft nearest-neighbor framework for continual semi-supervised learning.](http://arxiv.org/abs/2212.05102) | 该论文介绍了一种用于持续半监督学习的软最近邻框架，该框架利用最近邻分类器来非线性地划分特征空间并非参数地建模潜在的数据分布，以避免模型忘记对未标记数据表示并过度拟合标记的样本。 |
| [^144] | [Understanding Sinusoidal Neural Networks.](http://arxiv.org/abs/2212.01833) | 本研究探究了正弦神经网络的结构和表达能力，这些网络使用正弦函数作为激活函数。正弦MLP具有平滑性和紧凑性两个关键性质，并提供了控制机制来定义和训练这些网络。 |
| [^145] | [A Survey of Deep Graph Clustering: Taxonomy, Challenge, Application, and Open Resource.](http://arxiv.org/abs/2211.12875) | 在这篇论文中，作者对深度图聚类进行了综述研究。首先介绍了该领域的定义、评估和发展，然后提出了深度图聚类方法的分类学，并对现有方法进行了分析，总结出了挑战和机会。 |
| [^146] | [Model Based Residual Policy Learning with Applications to Antenna Control.](http://arxiv.org/abs/2211.08796) | 本文介绍了一种基于模型的残差策略学习（MBRPL）方法，用于解决天线控制问题。该方法通过增强现有策略，提高了样本效率，并减少了与实际环境的交互次数。实验结果表明，该方法在初始性能方面表现出色，并为将这些算法应用于实际网络迈出了一步。 |
| [^147] | [Discover, Explanation, Improvement: An Automatic Slice Detection Framework for Natural Language Processing.](http://arxiv.org/abs/2211.04476) | 本研究提出了一个自动片段检测框架用于自然语言处理任务，通过发现、解释和改进模型的错误，提供了对模型行为的理解和未来模型设计的见解。 |
| [^148] | [Efficient ECG-based Atrial Fibrillation Detection via Parameterised Hypercomplex Neural Networks.](http://arxiv.org/abs/2211.02678) | 本文提出了一种基于参数化超复数神经网络的轻量级卷积神经网络方法，用于心房颤动检测。该方法在可穿戴设备上训练小规模CNN，克服了有限的计算资源。在两个公开可用的ECG数据集上，该方法表现出与实值CNN相当的性能，但使用了显着较少的模型参数。 |
| [^149] | [Variational Hierarchical Mixtures for Probabilistic Learning of Inverse Dynamics.](http://arxiv.org/abs/2211.01120) | 本文提出了一种可变层次混合模型方法，通过概率层次化建模来学习机器人应用中的逆动力学。该方法结合了传统回归模型的优势，实现了计算高效的表示和自适应数据复杂性的灵活性。 |
| [^150] | [Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy.](http://arxiv.org/abs/2210.17546) | 防止神经语言模型逐字记忆无法真正保护隐私，本文设计的布隆过滤器虽然防止了所有逐字记忆，但仍然无法防止训练数据泄露，容易被合理修改的“样式转换”提示绕过。 |
| [^151] | [Multi-view Multi-label Anomaly Network Traffic Classification based on MLP-Mixer Neural Network.](http://arxiv.org/abs/2210.16719) | 本文提出了一种基于MLP-Mixer的多视图多标签神经网络用于网络流量分类，该方法能够更好地捕捉流量数据的全局信息关联，通过利用不同场景之间的相关性来提高分类性能。 |
| [^152] | [PopArt: Efficient Sparse Regression and Experimental Design for Optimal Sparse Linear Bandits.](http://arxiv.org/abs/2210.15345) | 本文提出了一种称为PopArt的高效稀疏线性估计方法，相比于Lasso，在许多问题中具有更紧的$\ell_1$恢复保证，并基于此推导出稀疏线性摇臂算法，具有改进的遗憾上界。同时，我们证明了在数据稀缺情况下稀疏线性摇臂的匹配下界。 |
| [^153] | [Adaptive Top-K in SGD for Communication-Efficient Distributed Learning.](http://arxiv.org/abs/2210.13532) | 这篇论文提出了一种自适应的Top-K方法用于降低通信成本的分布式学习，通过自适应调整稀疏化程度来优化收敛性能，在数值实验中取得了良好的结果。 |
| [^154] | [TiDAL: Learning Training Dynamics for Active Learning.](http://arxiv.org/abs/2210.06788) | 提出了一种新的主动学习方法TiDAL，利用训练动力学来量化未标记数据的不确定性，为了解决大规模数据的跟踪问题，利用预测模块学习标记数据的训练动力学。 |
| [^155] | [Learning to Optimize Quasi-Newton Methods.](http://arxiv.org/abs/2210.06171) | 本文介绍了一种名为LODO的机器学习优化器，它通过将学习优化技术与拟牛顿方法相结合，实时学习并适应损失景观的局部特征，从而在线元学习最佳的预条件矩阵。 |
| [^156] | [Finite time analysis of temporal difference learning with linear function approximation: Tail averaging and regularisation.](http://arxiv.org/abs/2210.05918) | 本研究通过引入尾平均和正则化技术，对时序差异(TD)学习算法进行了有限时间行为的研究。我们得出结论，尾平均TD能以最优速率 $O(1/t)$ 收敛，并且初始误差衰减速率更快。此外，正则化的TD版本在具有病态特征的问题上很有用。 |
| [^157] | [Generalized Kernel Regularized Least Squares.](http://arxiv.org/abs/2209.14355) | 本论文提出了广义核正则化最小二乘法 (gKRLS)，解决了核正则化最小二乘法 (KRLS) 在当前使用中的两个限制：它的扩展能力不足，且即使在小规模数据集上，其计算代价也非常高昂。 |
| [^158] | [Neural Moving Horizon Estimation for Robust Flight Control.](http://arxiv.org/abs/2206.10397) | 本文提出了一种神经移动视界估计器（NeuroMHE），它可以自动调整神经网络建模的关键参数，并适应不同的飞行场景。通过推导出与加权矩阵相关的解析梯度，我们实现了将该估计器作为可学习层嵌入神经网络进行高效学习。此外，我们还开发了一种基于模型的策略梯度算法，以从四旋翼轨迹跟踪误差中直接训练NeuroMHE，而不需要地面真实干扰数据。 |
| [^159] | [Multi-scale Wasserstein Shortest-path Filtration Kernels on Graphs.](http://arxiv.org/abs/2206.00979) | 这篇论文提出了一种名为多尺度Wasserstein最短路径过滤图核心（MWSPF）的新型最短路径图核心，解决了传统核心的信息丢失和缺乏多个尺度考虑的问题。 |
| [^160] | [Deep neural networks with dependent weights: Gaussian Process mixture limit, heavy tails, sparsity and compressibility.](http://arxiv.org/abs/2205.08187) | 本文研究了具有相关权重的深度神经网络的极限行为，发现无限宽度神经网络的每一层可以通过两个简单的量来刻画，当其中至少一层的量是非平凡的时候，得到了高斯过程的混合模型。 |
| [^161] | [A deep branching solver for fully nonlinear partial differential equations.](http://arxiv.org/abs/2203.03234) | 本文提出了一个多维深度学习实现的随机分支算法，用于解决全非线性偏微分方程。与其他方法相比，该算法能够在功能非线性和梯度项方面取得更好的解决效果，并提供了这些方法无法获得的解估计值。 |
| [^162] | [Attacking c-MARL More Effectively: A Data Driven Approach.](http://arxiv.org/abs/2202.03558) | 本研究提出了一种数据驱动的方法来评估合作多智能体强化学习 (c-MARL) 代理的稳健性，并通过模型构建更强大的对抗状态扰动以降低团队总奖励。同时，还提出了首个受害代理选择策略和数据驱动方法，以定义目标失败状态，从而实现更强大的对抗性攻击。实验证明，该方法在被测试的所有环境中一致优于其他基线。 |
| [^163] | [Control Theoretic Analysis of Temporal Difference Learning.](http://arxiv.org/abs/2112.14417) | 本研究对Temporal Difference学习算法进行了控制论分析，并引入了一个有限时间的框架，从控制论角度提供了对TD学习机制和强化学习领域的更深入洞察。 |
| [^164] | [On Expressivity and Trainability of Quadratic Networks.](http://arxiv.org/abs/2110.06081) | 该论文基于二次人工神经元在深度学习中的重要作用，研究了二次网络的表达能力和训练性。通过应用样条理论和代数几何中的度量，证明了二次网络相比传统网络具有更好的模型表达能力。 |
| [^165] | [Fat-Shattering Dimension of $k$-fold Aggregations.](http://arxiv.org/abs/2110.04763) | 该论文估计了实值函数类聚合规则的破裂维数，并给出了关于线性和仿射函数类的更尖锐上界和匹配的下界。同时改进了已知结果，并纠正了文献中的一些错误论断。 |
| [^166] | [Robust Feature-Level Adversaries are Interpretability Tools.](http://arxiv.org/abs/2110.03605) | 鲁棒的特征级对抗攻击不仅提供了对模型表示的研究，还具有独特的多功能性和高度鲁棒性，可以用于各种规模的图像攻击，并且可以作为可解释性工具帮助识别网络中的错误。 |
| [^167] | [Complexity-Optimized Sparse Bayesian Learning for Scalable Classification Tasks.](http://arxiv.org/abs/2107.08195) | 本文提出了一种复杂度优化稀疏贝叶斯学习方法DQN-SBL来解决高维特征空间或大数据规模问题中的内存溢出和计算复杂度高的问题，并在大规模问题上展现了竞争力的泛化能力。 |
| [^168] | [AngularGrad: A New Optimization Technique for Angular Convergence of Convolutional Neural Networks.](http://arxiv.org/abs/2105.10190) | AngularGrad是一种新的优化器，通过考虑连续梯度的方向/角度行为来优化卷积神经网络的角度收敛。通过捕捉角度信息以获得更准确的步长，优化步骤变得更加平滑。 |
| [^169] | [Explainable AI by BAPC -- Before and After correction Parameter Comparison.](http://arxiv.org/abs/2103.07155) | 该论文介绍了一种通过纠正简单的基模型来解释AI预测的局部替代方法。研究结果表明，通过确定准确性损失、准确性和替代品忠实度之间的准确关系，可以得到理想大小的解释实例邻域，以实现最大的准确性和忠实度。 |
| [^170] | [MMD-regularized Unbalanced Optimal Transport.](http://arxiv.org/abs/2011.05001) | 本文研究了使用MMD正则化的非平衡最优输运问题，提出了基于Fenchel对偶性的新度量方法，还提出了基于有限样本的凸规划用于估算问题，证明了估计量的一致性和误差速率。 |
| [^171] | [ResNet After All? Neural ODEs and Their Numerical Solution.](http://arxiv.org/abs/2007.15386) | 神经ODE模型的性能取决于训练过程中使用的数值方法，如果使用过于粗糙的解算器进行训练，则使用另一个数值误差相等或更小的解算器进行测试会导致准确性下降。 |
| [^172] | [Dance Revolution: Long-Term Dance Generation with Music via Curriculum Learning.](http://arxiv.org/abs/2006.06119) | 本文提出了一种基于课程学习和音乐的长期舞蹈生成方法，通过设计新的seq2seq架构和处理长序列的策略，成功捕捉了音乐与舞蹈之间的细粒度对应关系，并减轻了错误。 |
| [^173] | [Intelligent GPS Spoofing Attack Detection in Power Grids.](http://arxiv.org/abs/2005.04513) | 本文提出了一种使用动态电力系统中PMU数据的神经网络GPS欺骗检测（NNGSD）方法，用于检测电网中的GPS欺骗攻击，并展示了该检测方法的实时性能。 |
| [^174] | [A Unifying Framework of Bilinear LSTMs.](http://arxiv.org/abs/1910.10294) | 本文提出了一个统一的双线性LSTM框架，通过平衡线性和双线性项的表达能力，实现了对序列数据集中输入特征的非线性交互的利用，以实现更好的性能，同时不增加更多的学习参数。 |
| [^175] | [Improving the Resolution of CNN Feature Maps Efficiently with Multisampling.](http://arxiv.org/abs/1805.10766) | 本论文提出了一种称为多样抽样的CNN子采样技术，通过子采样层显著增加特征图保留的信息量，实验证明粗糙的特征图是影响神经网络在图像分类中性能的瓶颈。 |

# 详细

[^1]: SA-Solver：用于快速采样扩散模型的随机亚当求解器

    SA-Solver: Stochastic Adams Solver for Fast Sampling of Diffusion Models. (arXiv:2309.05019v1 [cs.LG])

    [http://arxiv.org/abs/2309.05019](http://arxiv.org/abs/2309.05019)

    本文提出了一种改进的高效随机亚当方法SA-Solver，用于解扩散随机微分方程以生成高质量的数据，实验结果显示它在少步采样中相较于现有最先进的方法有改进或可比的性能，并达到了SOTA FID分数。

    

    扩散概率模型在生成任务中取得了相当大的成功。由于从扩散概率模型中进行采样相当于解扩散随机微分方程或常微分方程，这是一项耗时的工作，因此提出了许多基于改进的微分方程求解器的快速采样方法。这些技术中的大部分方法都考虑解扩散常微分方程，因为它具有更好的效率。然而，随机采样可以在生成多样化和高质量数据方面提供额外的优势。在这项工作中，我们从两个方面进行了对随机采样的综合分析：方差控制的扩散随机微分方程和线性多步扩散随机微分方程求解器。基于我们的分析，我们提出了SA-Solver，它是一种改进的高效随机亚当方法，用于解扩散随机微分方程以生成高质量的数据。我们的实验结果显示，SA-Solver实现了：1）在少步采样中与现有最先进的采样方法相比，有改进或可比性能；2）SOTA FID分数。

    Diffusion Probabilistic Models (DPMs) have achieved considerable success in generation tasks. As sampling from DPMs is equivalent to solving diffusion SDE or ODE which is time-consuming, numerous fast sampling methods built upon improved differential equation solvers are proposed. The majority of such techniques consider solving the diffusion ODE due to its superior efficiency. However, stochastic sampling could offer additional advantages in generating diverse and high-quality data. In this work, we engage in a comprehensive analysis of stochastic sampling from two aspects: variance-controlled diffusion SDE and linear multi-step SDE solver. Based on our analysis, we propose SA-Solver, which is an improved efficient stochastic Adams method for solving diffusion SDE to generate data with high quality. Our experiments show that SA-Solver achieves: 1) improved or comparable performance compared with the existing state-of-the-art sampling methods for few-step sampling; 2) SOTA FID scores o
    
[^2]: 增量汇聚梯度方法在流数据上的线性加速

    Linear Speedup of Incremental Aggregated Gradient Methods on Streaming Data. (arXiv:2309.04980v1 [math.OC])

    [http://arxiv.org/abs/2309.04980](http://arxiv.org/abs/2309.04980)

    本文研究了增量汇聚梯度方法在流数据上的线性加速。研究表明，在确定性梯度情况下，流IAG方法可以实现线性加速，并且即使数据样本分布不均匀，只要工人频繁更新，期望的最优解平方距离可以以O((1+T)/(nt))的速度衰减。

    

    本文考虑了一种用于大规模分布式优化的增量汇聚梯度(IAG)方法。IAG方法非常适合参数服务器架构，因为参数服务器可以轻松地汇聚工人贡献的可能过期的梯度。尽管IAG在确定性梯度情况下的收敛性已经有了很好的研究，但对于基于流数据的随机变体的研究结果还很有限。在考虑强凸优化的情况下，本文展示了当工人频繁更新时，流IAG方法可以实现线性加速，即使工人之间的数据样本分布不均匀。我们的分析涉及对带有过期梯度的条件期望的仔细处理和递归系统的处理。

    This paper considers a type of incremental aggregated gradient (IAG) method for large-scale distributed optimization. The IAG method is well suited for the parameter server architecture as the latter can easily aggregate potentially staled gradients contributed by workers. Although the convergence of IAG in the case of deterministic gradient is well known, there are only a few results for the case of its stochastic variant based on streaming data. Considering strongly convex optimization, this paper shows that the streaming IAG method achieves linear speedup when the workers are updating frequently enough, even if the data sample distribution across workers are heterogeneous. We show that the expected squared distance to optimal solution decays at O((1+T)/(nt)), where $n$ is the number of workers, t is the iteration number, and T/n is the update frequency of workers. Our analysis involves careful treatments of the conditional expectations with staled gradients and a recursive system wi
    
[^3]: 使用无人机缓解意外的城市道路交通拥堵

    AVARS -- Alleviating Unexpected Urban Road Traffic Congestion using UAVs. (arXiv:2309.04976v1 [cs.LG])

    [http://arxiv.org/abs/2309.04976](http://arxiv.org/abs/2309.04976)

    这篇论文提出使用无人机来缓解意外的城市道路交通拥堵。研究指出，传统的交通信号控制系统效率低下，而基于摄像机和深度强化学习算法的系统更加有效。然而，由于成本问题，无人机可能是一个更好的选择。

    

    减少由途中事件（例如路段关闭，车祸等）引起的意外城市交通拥堵通常需要快速准确地选择最合适的交通信号。传统的交通信号控制系统，如SCATS和SCOOT，由感应线圈提供的交通数据更新频率较低（即超过1分钟）。此外，这些系统使用的交通信号灯计划是事发前预先编程的候选计划的有限集合中选择的。最近的研究表明，由深度强化学习（DRL）算法控制的基于摄像机的交通信号系统在减少交通拥堵方面更加有效，摄像机可以提供高频高分辨率的交通数据。然而，由于道路基础设施需要过多的潜在升级，这些系统的部署成本在大城市中较高。在本文中，我们认为无人机（UAVs）可以在缓解意外的城市道路交通拥堵方面提供一种有效的解决方案。

    Reducing unexpected urban traffic congestion caused by en-route events (e.g., road closures, car crashes, etc.) often requires fast and accurate reactions to choose the best-fit traffic signals. Traditional traffic light control systems, such as SCATS and SCOOT, are not efficient as their traffic data provided by induction loops has a low update frequency (i.e., longer than 1 minute). Moreover, the traffic light signal plans used by these systems are selected from a limited set of candidate plans pre-programmed prior to unexpected events' occurrence. Recent research demonstrates that camera-based traffic light systems controlled by deep reinforcement learning (DRL) algorithms are more effective in reducing traffic congestion, in which the cameras can provide high-frequency high-resolution traffic data. However, these systems are costly to deploy in big cities due to the excessive potential upgrades required to road infrastructure. In this paper, we argue that Unmanned Aerial Vehicles (
    
[^4]: 使用自我监督的任务推断进行连续机器人学习

    Continual Robot Learning using Self-Supervised Task Inference. (arXiv:2309.04974v1 [cs.RO])

    [http://arxiv.org/abs/2309.04974](http://arxiv.org/abs/2309.04974)

    本文提出了一种使用自我监督的任务推断方法，以实现机器人的连续学习。通过学习动作和意图嵌入以及行为嵌入，机器人可以推断出当前正在进行的任务，从而实现不断学习新任务的能力。

    

    赋予机器人像人类一样学习不断发展的技能而不是掌握单一任务的能力是机器人学习中的一个开放问题。尽管已经提出了多任务学习方法来解决这个问题，但他们很少关注任务推断。为了不断学习新任务，机器人首先需要推断出当前正在进行的任务，而不需要预定义的任务表示。在本文中，我们提出了一种自我监督的任务推断方法。我们的方法通过观察到的未标记示范的运动和效果部分的自组织来学习动作和意图嵌入，并通过联合动作-意图嵌入的自组织来学习高层行为嵌入。我们构建了一个行为匹配的自我监督学习目标，来训练一个新的任务推断网络（TINet）将未标记示范映射到其最近的行为嵌入，然后将其用作任务表示。

    Endowing robots with the human ability to learn a growing set of skills over the course of a lifetime as opposed to mastering single tasks is an open problem in robot learning. While multi-task learning approaches have been proposed to address this problem, they pay little attention to task inference. In order to continually learn new tasks, the robot first needs to infer the task at hand without requiring predefined task representations. In this paper, we propose a self-supervised task inference approach. Our approach learns action and intention embeddings from self-organization of the observed movement and effect parts of unlabeled demonstrations and a higher-level behavior embedding from self-organization of the joint action-intention embeddings. We construct a behavior-matching self-supervised learning objective to train a novel Task Inference Network (TINet) to map an unlabeled demonstration to its nearest behavior embedding, which we use as the task representation. A multi-task p
    
[^5]: LMBiS-Net：基于轻量级多路径双向跳跃连接的视网膜血管分割的卷积神经网络

    LMBiS-Net: A Lightweight Multipath Bidirectional Skip Connection based CNN for Retinal Blood Vessel Segmentation. (arXiv:2309.04968v1 [eess.IV])

    [http://arxiv.org/abs/2309.04968](http://arxiv.org/abs/2309.04968)

    本文提出了一种名为LMBiS-Net的轻量级卷积神经网络用于视网膜血管分割，具有极低的可学习参数数量和优化的模型效率，通过多路径特征提取和双向跳跃连接实现了较好的分割性能。

    

    失明眼疾通常与改变的视网膜形态相关，可以通过分割眼底图像中的视网膜结构在临床上进行识别。然而，当前的方法在准确分割精细血管方面常常存在不足。虽然深度学习已经在医学图像分割中显示出了潜力，但是它对重复的卷积和池化操作的依赖可能会阻碍边缘信息的表示，从而限制了整体分割的准确性。本文提出了一种名为LMBiS-Net的轻量级像素级卷积神经网络，用于以极低数量的可学习参数（仅0.172M）进行视网膜血管的分割。该网络采用了多路径特征提取块，并将双向跳跃连接用于编码器和解码器之间的信息流。此外，我们通过精心选择滤波器的数量来优化模型的效率，避免了滤波器重叠。这种优化显著改善了模型的性能。

    Blinding eye diseases are often correlated with altered retinal morphology, which can be clinically identified by segmenting retinal structures in fundus images. However, current methodologies often fall short in accurately segmenting delicate vessels. Although deep learning has shown promise in medical image segmentation, its reliance on repeated convolution and pooling operations can hinder the representation of edge information, ultimately limiting overall segmentation accuracy. In this paper, we propose a lightweight pixel-level CNN named LMBiS-Net for the segmentation of retinal vessels with an exceptionally low number of learnable parameters \textbf{(only 0.172 M)}. The network used multipath feature extraction blocks and incorporates bidirectional skip connections for the information flow between the encoder and decoder. Additionally, we have optimized the efficiency of the model by carefully selecting the number of filters to avoid filter overlap. This optimization significantl
    
[^6]: 用于聚类引用轨迹的多个K-means聚类集成框架

    A multiple k-means cluster ensemble framework for clustering citation trajectories. (arXiv:2309.04949v1 [cs.SI])

    [http://arxiv.org/abs/2309.04949](http://arxiv.org/abs/2309.04949)

    本文提出了一种基于特征的多个k-means聚类集成框架，用于聚类引用轨迹。这有助于理解知识传播过程，并解决了现有方法依赖参数、定义模糊和只捕捉极端轨迹的问题。

    

    引用成熟时间因文章而异，然而所有文章的影响力都是在一个固定窗口内衡量的。对它们的引用轨迹进行聚类有助于理解知识扩散过程，并揭示并非所有文章在发表后都立即获得成功。此外，对轨迹进行聚类也对论文影响力推荐算法至关重要。由于引用时间序列具有非线性和非平稳特性，这是一个具有挑战性的问题。先前的工作提出了一组任意的阈值和基于规则的固定方法。所有的方法主要都依赖于参数，因此在定义相似的轨迹和关于特定数目的模糊性方面导致了不一致性。大多数研究只捕捉了极端的轨迹。因此，需要一个通用的聚类框架。本文提出了一个基于特征的多个k-means聚类集成框架。

    Citation maturity time varies for different articles. However, the impact of all articles is measured in a fixed window. Clustering their citation trajectories helps understand the knowledge diffusion process and reveals that not all articles gain immediate success after publication. Moreover, clustering trajectories is necessary for paper impact recommendation algorithms. It is a challenging problem because citation time series exhibit significant variability due to non linear and non stationary characteristics. Prior works propose a set of arbitrary thresholds and a fixed rule based approach. All methods are primarily parameter dependent. Consequently, it leads to inconsistencies while defining similar trajectories and ambiguities regarding their specific number. Most studies only capture extreme trajectories. Thus, a generalised clustering framework is required. This paper proposes a feature based multiple k means cluster ensemble framework. 1,95,783 and 41,732 well cited articles f
    
[^7]: 限制距离的民间传说Weisfeiler-Leman图神经网络及可证明的循环计数能力

    Distance-Restricted Folklore Weisfeiler-Leman GNNs with Provable Cycle Counting Power. (arXiv:2309.04941v1 [cs.LG])

    [http://arxiv.org/abs/2309.04941](http://arxiv.org/abs/2309.04941)

    本文提出了一种称为$d$-DRFWL(2) GNNs的新型图神经网络，它通过限制节点之间的距离来实现循环计数能力，克服了子图GNNs的预处理和计算成本高的限制。

    

    图神经网络（GNNs）的能力在广泛的任务中成功计数特定的图子结构，尤其是循环，对于GNNs的成功非常关键。最近，它已被用作评估GNNs表达能力的一种常用指标。许多具有可证明的循环计数能力的GNN模型都基于子图GNNs，即从输入图中提取一组子图，为每个子图生成表示，并使用它们来增强输入图的表示。然而，这些方法需要进行繁重的预处理，并且时间和内存成本较高。在本文中，我们通过提出一种新的GNN类别-- $d$-Distance-Restricted FWL(2) GNNs，或者 $d$-DRFWL(2) GNNs，克服了子图GNNs的上述限制。$d$-DRFWL(2) GNNs将互相之间距离不超过$d$的节点对作为信息传递的单位，以平衡表达能力和复杂性。

    The ability of graph neural networks (GNNs) to count certain graph substructures, especially cycles, is important for the success of GNNs on a wide range of tasks. It has been recently used as a popular metric for evaluating the expressive power of GNNs. Many of the proposed GNN models with provable cycle counting power are based on subgraph GNNs, i.e., extracting a bag of subgraphs from the input graph, generating representations for each subgraph, and using them to augment the representation of the input graph. However, those methods require heavy preprocessing, and suffer from high time and memory costs. In this paper, we overcome the aforementioned limitations of subgraph GNNs by proposing a novel class of GNNs -- $d$-Distance-Restricted FWL(2) GNNs, or $d$-DRFWL(2) GNNs. $d$-DRFWL(2) GNNs use node pairs whose mutual distances are at most $d$ as the units for message passing to balance the expressive power and complexity. By performing message passing among distance-restricted node
    
[^8]: 机器学习在云计算安全中的应用综述

    A Review of Machine Learning-based Security in Cloud Computing. (arXiv:2309.04911v1 [cs.CR])

    [http://arxiv.org/abs/2309.04911](http://arxiv.org/abs/2309.04911)

    云计算的快速发展带来了安全风险，机器学习的应用可以在识别和解决安全问题方面减少人工干预的需求，并通过分析大量数据进行准确预测，从而改变了云服务提供商在安全方面的方法。

    

    云计算（CC）正在改变向用户提供IT资源的方式，使他们能够以更高的成本效益和简化的基础设施来访问和管理系统。然而，随着CC的增长，出现了一系列安全风险，包括对可用性、完整性和机密性的威胁。为了解决这些挑战，云服务提供商（CSPs）越来越多地使用机器学习（ML）来减少在识别和解决安全问题方面的人工干预。ML能够分析大量数据并进行高准确性预测，可以改变CSPs在安全方面的方法。在本文中，我们将探讨机器学习在云计算安全领域的一些最新研究。我们将研究一系列ML算法的特性和有效性，突出它们独特的优势和潜在限制。我们的目标是提供关于ML在云计算中的当前状况的综合概述。

    Cloud Computing (CC) is revolutionizing the way IT resources are delivered to users, allowing them to access and manage their systems with increased cost-effectiveness and simplified infrastructure. However, with the growth of CC comes a host of security risks, including threats to availability, integrity, and confidentiality. To address these challenges, Machine Learning (ML) is increasingly being used by Cloud Service Providers (CSPs) to reduce the need for human intervention in identifying and resolving security issues. With the ability to analyze vast amounts of data, and make high-accuracy predictions, ML can transform the way CSPs approach security. In this paper, we will explore some of the most recent research in the field of ML-based security in Cloud Computing. We will examine the features and effectiveness of a range of ML algorithms, highlighting their unique strengths and potential limitations. Our goal is to provide a comprehensive overview of the current state of ML in c
    
[^9]: 结构感知的辛系统哈密顿（图）嵌入

    Symplectic Structure-Aware Hamiltonian (Graph) Embeddings. (arXiv:2309.04885v1 [cs.LG])

    [http://arxiv.org/abs/2309.04885](http://arxiv.org/abs/2309.04885)

    本文提出了SAH-GNN，一种在图神经网络中应用结构感知的辛系统哈密顿嵌入方法。与传统方法不同，SAH-GNN通过在训练过程中自适应学习辛结构，避免了依赖预定义标准辛结构形式的限制，并能够适应不同的图数据集，同时保持物理意义上的能量守恒。

    

    在传统的图神经网络（GNNs）中，固定嵌入流形的假设常常限制了其对不同图几何结构的适应性。最近，提出了基于哈密顿系统的GNNs，通过将物理定律纳入节点特征更新中，来解决这类嵌入的动态特性。在这项工作中，我们提出了SAH-GNN，一种新颖的方法，将哈密顿动力学推广到更灵活的节点特征更新中。与现有的受哈密顿启发的GNNs不同，SAH-GNN在训练过程中采用辛斯蒂费尔流形上的黎曼优化，自适应地学习潜在的辛结构，从而规避了现有依赖预定义标准辛结构形式的哈密顿GNNs的局限性。这一创新使得SAH-GNN能够在没有大量超参数调整的情况下自动适应各种图数据集。此外，它在训练过程中保持能量守恒，使得隐式哈密顿系统具有物理意义。

    In traditional Graph Neural Networks (GNNs), the assumption of a fixed embedding manifold often limits their adaptability to diverse graph geometries. Recently, Hamiltonian system-inspired GNNs are proposed to address the dynamic nature of such embeddings by incorporating physical laws into node feature updates. In this work, we present SAH-GNN, a novel approach that generalizes Hamiltonian dynamics for more flexible node feature updates. Unlike existing Hamiltonian-inspired GNNs, SAH-GNN employs Riemannian optimization on the symplectic Stiefel manifold to adaptively learn the underlying symplectic structure during training, circumventing the limitations of existing Hamiltonian GNNs that rely on a pre-defined form of standard symplectic structure. This innovation allows SAH-GNN to automatically adapt to various graph datasets without extensive hyperparameter tuning. Moreover, it conserves energy during training such that the implicit Hamiltonian system is physically meaningful. To thi
    
[^10]: 渐变优化和变分不等式在机器学习中的温和介绍

    A Gentle Introduction to Gradient-Based Optimization and Variational Inequalities for Machine Learning. (arXiv:2309.04877v1 [cs.LG])

    [http://arxiv.org/abs/2309.04877](http://arxiv.org/abs/2309.04877)

    这篇论文介绍了渐变优化和变分不等式在机器学习中的应用，强调了从模式识别到决策和多智能体问题的转变，以及涉及均衡和博弈论的数学挑战，提供了一些算法的收敛性证明，但主要关注于提供动机和直观理解。

    

    近年来机器学习的快速发展基于与渐变优化的紧密联系。进一步的进展部分取决于从模式识别到决策和多智能体问题的转变。在这些更广泛的背景下，涉及均衡和博弈论而不是极值的新的数学挑战出现了。基于梯度的方法仍然至关重要--考虑到机器学习问题的高维度和大规模--但简单的梯度下降不再是算法设计的出发点。我们提供了一个对机器学习中基于梯度的算法的更广泛框架的温和介绍，从鞍点和单调博弈开始，然后到一般的变分不等式。虽然我们对所提出的几个算法进行了收敛性证明，但我们的主要关注点是提供动机和直观理解。

    The rapid progress in machine learning in recent years has been based on a highly productive connection to gradient-based optimization. Further progress hinges in part on a shift in focus from pattern recognition to decision-making and multi-agent problems. In these broader settings, new mathematical challenges emerge that involve equilibria and game theory instead of optima. Gradient-based methods remain essential -- given the high dimensionality and large scale of machine-learning problems -- but simple gradient descent is no longer the point of departure for algorithm design. We provide a gentle introduction to a broader framework for gradient-based algorithms in machine learning, beginning with saddle points and monotone games, and proceeding to general variational inequalities. While we provide convergence proofs for several of the algorithms that we present, our main focus is that of providing motivation and intuition.
    
[^11]: 在减小的环上近似ReLU以实现高效的基于MPC的私有推理

    Approximating ReLU on a Reduced Ring for Efficient MPC-based Private Inference. (arXiv:2309.04875v1 [cs.LG])

    [http://arxiv.org/abs/2309.04875](http://arxiv.org/abs/2309.04875)

    本文提出了一种名为HummingBird的MPC框架，通过在较小的环上使用一部分比特来显著减小ReLU的通信开销，以提高MPC基于私有推理的效率。

    

    安全的多方计算(MPC)允许用户在不必共享其敏感数据的情况下将机器学习推理外包给不可信的服务器。尽管MPC基于私有推理具有很强的安全性，但由于高通信开销，它在现实世界中并未被广泛采用。当评估ReLU层时，MPC协议需要大量的通信，导致其整体执行时间比非私有推理慢很多。本文提出了一种名为HummingBird的MPC框架，通过在较小的环上使用一部分比特来显著减小ReLU的通信开销。基于理论分析，HummingBird在ReLU评估中识别了不影响准确性的秘密分享的比特，并在评估过程中排除它们以减少通信。通过其高效的搜索引擎，HummingBird在ReLU期间舍弃了87-91%的比特，并仍然保持较高的准确性。

    Secure multi-party computation (MPC) allows users to offload machine learning inference on untrusted servers without having to share their privacy-sensitive data. Despite their strong security properties, MPC-based private inference has not been widely adopted in the real world due to their high communication overhead. When evaluating ReLU layers, MPC protocols incur a significant amount of communication between the parties, making the end-to-end execution time multiple orders slower than its non-private counterpart.  This paper presents HummingBird, an MPC framework that reduces the ReLU communication overhead significantly by using only a subset of the bits to evaluate ReLU on a smaller ring. Based on theoretical analyses, HummingBird identifies bits in the secret share that are not crucial for accuracy and excludes them during ReLU evaluation to reduce communication. With its efficient search engine, HummingBird discards 87--91% of the bits during ReLU and still maintains high accur
    
[^12]: 梯度下降训练的神经网络的近似结果

    Approximation Results for Gradient Descent trained Neural Networks. (arXiv:2309.04860v1 [cs.LG])

    [http://arxiv.org/abs/2309.04860](http://arxiv.org/abs/2309.04860)

    该论文研究了采用梯度下降训练的神经网络的近似保证，利用连续的误差范数对网络进行分析，并发现在欠参数化的情况下相对于已有的逼近方法存在逼近率下降的问题。

    

    该论文对采用梯度流训练的神经网络进行了近似保证，其中误差以连续的$L_2(\mathbb{S}^{d-1})$范数在$d$维单位球面上测量，目标为Sobolev平滑。网络是完全连接的，深度恒定，宽度递增。虽然所有层都进行了训练，但梯度流的收敛性是基于对于非凸的倒数第二层的神经切向核(NTK)的论证。与标准的NTK分析不同，连续误差范数暗示了一个欠参数化的区域，在逼近时需要自然的光滑性假设。典型的过参数化通过逼近率的损失以及相对于Sobolev平滑函数的已建立的逼近方法而重新进入结果中。

    The paper contains approximation guarantees for neural networks that are trained with gradient flow, with error measured in the continuous $L_2(\mathbb{S}^{d-1})$-norm on the $d$-dimensional unit sphere and targets that are Sobolev smooth. The networks are fully connected of constant depth and increasing width. Although all layers are trained, the gradient flow convergence is based on a neural tangent kernel (NTK) argument for the non-convex second but last layer. Unlike standard NTK analysis, the continuous error norm implies an under-parametrized regime, possible by the natural smoothness assumption required for approximation. The typical over-parametrization re-enters the results in form of a loss in approximation rate relative to established approximation methods for Sobolev smooth functions.
    
[^13]: 逆向工程解码策略：在对语言生成系统进行黑盒访问的情况下

    Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System. (arXiv:2309.04858v1 [cs.LG])

    [http://arxiv.org/abs/2309.04858](http://arxiv.org/abs/2309.04858)

    本文介绍了一种方法，可以逆向工程用于生成文本的解码方法，并发现了这些方法对于检测生成文本以及揭示由于解码设置导致的偏见的重要意义。

    

    神经语言模型越来越多地被部署在允许用户输入提示并接收生成文本的API和网站上。许多系统不会透露生成参数。在本文中，我们介绍了一种方法，可以逆向工程用于生成文本的解码方法（即，top-k或nucleus采样）。我们发现所使用的解码策略对于检测生成文本具有重要意义。此外，发现解码策略的过程可以揭示由于选择解码设置而导致的偏见，这严重截断了模型的预测分布。我们在几个开源语言模型家族以及生产系统上（例如，ChatGPT）上执行攻击。

    Neural language models are increasingly deployed into APIs and websites that allow a user to pass in a prompt and receive generated text. Many of these systems do not reveal generation parameters. In this paper, we present methods to reverse-engineer the decoding method used to generate text (i.e., top-$k$ or nucleus sampling). Our ability to discover which decoding strategy was used has implications for detecting generated text. Additionally, the process of discovering the decoding strategy can reveal biases caused by selecting decoding settings which severely truncate a model's predicted distributions. We perform our attack on several families of open-source language models, as well as on production systems (e.g., ChatGPT).
    
[^14]: AmbientFlow: 来自不完整、噪声测量的可逆生成模型

    AmbientFlow: Invertible generative models from incomplete, noisy measurements. (arXiv:2309.04856v1 [cs.LG])

    [http://arxiv.org/abs/2309.04856](http://arxiv.org/abs/2309.04856)

    AmbientFlow是一个从噪声和不完整数据中直接学习基于流的生成模型的新框架，通过使用变分贝叶斯方法来建立这种模型，展示了其在图像科学中的应用潜力。

    

    生成模型在图像科学中具有潜在的应用，如图像重建、后验采样和数据共享。基于流的生成模型特别有吸引力，因为它们能以可行的方式提供精确的密度估计以及快速、廉价和多样的样本。然而，训练这样的模型需要一个大型、高质量的对象数据集。在计算成像等应用中，由于需要长时间获取或高辐射剂量，往往难以获取这样的数据，而获取这些对象的噪声或部分观测测量更可行。在这项工作中，我们提出了AmbientFlow，一个从噪声和不完整数据直接学习基于流的生成模型的框架。使用变分贝叶斯方法，提出了一个从噪声、不完整数据建立基于流的生成模型的新框架。广泛的数值研究证明了该方法的有效性。

    Generative models have gained popularity for their potential applications in imaging science, such as image reconstruction, posterior sampling and data sharing. Flow-based generative models are particularly attractive due to their ability to tractably provide exact density estimates along with fast, inexpensive and diverse samples. Training such models, however, requires a large, high quality dataset of objects. In applications such as computed imaging, it is often difficult to acquire such data due to requirements such as long acquisition time or high radiation dose, while acquiring noisy or partially observed measurements of these objects is more feasible. In this work, we propose AmbientFlow, a framework for learning flow-based generative models directly from noisy and incomplete data. Using variational Bayesian methods, a novel framework for establishing flow-based generative models from noisy, incomplete data is proposed. Extensive numerical studies demonstrate the effectiveness o
    
[^15]: 通过提取精炼的语音和语言情感表示进行语音情感识别

    Speech Emotion Recognition with Distilled Prosodic and Linguistic Affect Representations. (arXiv:2309.04849v1 [cs.CL])

    [http://arxiv.org/abs/2309.04849](http://arxiv.org/abs/2309.04849)

    该论文提出了EmoDistill，这是一个利用知识蒸馏来学习从语音中获取情感的强大的语言和语音表示的语音情感识别框架。通过在训练过程中利用经过SER微调的预训练语音和语言教师进行信息蒸馏，该方法在IEMOCAP基准测试中实现了最新的最高准确率，表明其在单模态和多模态技术中的优越性能。

    

    我们提出了EmoDistill，这是一个新颖的语音情感识别（SER）框架，利用跨模态知识蒸馏来学习从语音中获取情感的强大的语言和语音表示。在推理过程中，我们的方法仅使用一串语音信号来进行单模态SER，从而减少计算开销并避免运行时的转录和语音特征提取错误。在训练过程中，我们的方法从一对经过SER微调的预训练的语音和语言教师中的嵌入和逻辑层面蒸馏信息。在IEMOCAP基准测试中的实验表明，我们的方法在准确率上优于其他单模态和多模态技术，并达到了77.49％的无权重准确率和78.91％的加权准确率的最新成绩。详细的消融研究还展示了我们方法的每个组件的影响。

    We propose EmoDistill, a novel speech emotion recognition (SER) framework that leverages cross-modal knowledge distillation during training to learn strong linguistic and prosodic representations of emotion from speech. During inference, our method only uses a stream of speech signals to perform unimodal SER thus reducing computation overhead and avoiding run-time transcription and prosodic feature extraction errors. During training, our method distills information at both embedding and logit levels from a pair of pre-trained Prosodic and Linguistic teachers that are fine-tuned for SER. Experiments on the IEMOCAP benchmark demonstrate that our method outperforms other unimodal and multimodal techniques by a considerable margin, and achieves state-of-the-art performance of 77.49% unweighted accuracy and 78.91% weighted accuracy. Detailed ablation studies demonstrate the impact of each component of our method.
    
[^16]: HAct：带有神经网络激活直方图的数据集外检测

    HAct: Out-of-Distribution Detection with Neural Net Activation Histograms. (arXiv:2309.04837v1 [cs.LG])

    [http://arxiv.org/abs/2309.04837](http://arxiv.org/abs/2309.04837)

    本论文提出了一种用于神经网络的数据集外检测方法，使用HAct激活直方图描述符对OOD进行检测，具有简单高效和准确性，并且在多个OOD图像分类基准测试中表现优于先前最先进的方法。

    

    我们提出了一种简单、高效、准确的方法，用于检测经过训练的神经网络对于数据集外（OOD）数据的检测，这是用于OOD泛化方法的潜在第一步。我们提出了一种新颖的描述符，即HAct激活直方图，用于OOD检测，即通过直方图来近似表示神经网络层输出值的概率分布。我们证明HAct在多种OOD图像分类基准测试上比最先进的方法更准确。例如，在标准的OOD基准测试上，我们的方法使用Resnet-50实现了95%的真正例率（TPR），而只有0.05%的误报率，使得其在误报率上相较于之前最先进的方法提高了20.66%（在相同的95%TPR下）。低计算复杂度和易于实现使得HAct适合大规模实践中在线监测部署的神经网络。

    We propose a simple, efficient, and accurate method for detecting out-of-distribution (OOD) data for trained neural networks, a potential first step in methods for OOD generalization. We propose a novel descriptor, HAct activation histograms, for OOD detection, that is, probability distributions (approximated by histograms) of output values of neural network layers under the influence of incoming data. We demonstrate that HAct is significantly more accurate than state-of-the-art on multiple OOD image classification benchmarks. For instance, our approach achieves a true positive rate (TPR) of 95% with only 0.05% false-positives using Resnet-50 on standard OOD benchmarks, outperforming previous state-of-the-art by 20.66% in the false positive rate (at the same TPR of 95%). The low computational complexity and the ease of implementation make HAct suitable for online implementation in monitoring deployed neural networks in practice at scale.
    
[^17]: 在学习估计器设计中，递减时域策略搜索的全局收敛性研究

    Global Convergence of Receding-Horizon Policy Search in Learning Estimator Designs. (arXiv:2309.04831v1 [math.OC])

    [http://arxiv.org/abs/2309.04831](http://arxiv.org/abs/2309.04831)

    RHPG算法是第一个在学习最优线性估计器设计方面具有可证明全局收敛性的PG算法，通过将普通的PG集成到动态规划的外循环中，将无约束且强凸的静态估计问题分解成受限且非凸的无穷时域KF问题，从而实现了全局收敛。

    

    我们引入了递减时域策略梯度（RHPG）算法，这是第一个在学习最优线性估计器设计（即卡尔曼滤波器）方面具有可证明全局收敛性的PG算法。值得注意的是，RHPG算法不需要任何关于系统的先验知识作为初始化，也不需要目标系统是开环稳定的。RHPG的关键是将普通的PG（或其他策略搜索方向）集成到动态规划的外循环中，将在策略参数中受限且非凸的无穷时域KF问题分解为一系列无约束且强凸的静态估计问题，从而实现全局收敛。我们进一步对RHPG的优化路线进行了详细分析，并详细说明了算法的收敛性和样本复杂度保证。这项工作是针对控制器设计开展强化学习算法的初步尝试。

    We introduce the receding-horizon policy gradient (RHPG) algorithm, the first PG algorithm with provable global convergence in learning the optimal linear estimator designs, i.e., the Kalman filter (KF). Notably, the RHPG algorithm does not require any prior knowledge of the system for initialization and does not require the target system to be open-loop stable. The key of RHPG is that we integrate vanilla PG (or any other policy search directions) into a dynamic programming outer loop, which iteratively decomposes the infinite-horizon KF problem that is constrained and non-convex in the policy parameter into a sequence of static estimation problems that are unconstrained and strongly-convex, thus enabling global convergence. We further provide fine-grained analyses of the optimization landscape under RHPG and detail the convergence and sample complexity guarantees of the algorithm. This work serves as an initial attempt to develop reinforcement learning algorithms specifically for con
    
[^18]: 通过重要性重新加权纠正采样偏差的空间建模方法

    Correcting sampling biases via importancereweighting for spatial modeling. (arXiv:2309.04824v1 [cs.LG])

    [http://arxiv.org/abs/2309.04824](http://arxiv.org/abs/2309.04824)

    通过重要性重新加权修正采样偏差的空间建模方法，可以有效解决分布偏移问题，实现目标错误的无偏估计。该方法在人工数据实验中表现出优势，使预测误差从7%降至2%，且样本量越大效果越好。

    

    在机器学习模型中，由于分布偏差，特别是在环境研究中的空间数据，对错误的估计通常是复杂的。我们引入了一种基于重要性采样思想的方法，以获得目标错误的无偏估计。通过考虑期望错误和可用数据之间的差异，我们的方法在每个样本点上重新加权错误并抵消偏移。我们使用重要性采样技术和核密度估计来进行重加权。我们使用类似真实世界空间数据集的人工数据验证了我们方法的有效性。我们的研究结果证明了我们提出的方法在目标错误估计方面的优势，并为分布偏移问题提供了解决方案。总体预测误差从7%降低到仅为2%，且随着样本量的增加而进一步降低。

    In machine learning models, the estimation of errors is often complex due to distribution bias, particularly in spatial data such as those found in environmental studies. We introduce an approach based on the ideas of importance sampling to obtain an unbiased estimate of the target error. By taking into account difference between desirable error and available data, our method reweights errors at each sample point and neutralizes the shift. Importance sampling technique and kernel density estimation were used for reweighteing. We validate the effectiveness of our approach using artificial data that resemble real-world spatial datasets. Our findings demonstrate advantages of the proposed approach for the estimation of the target error, offering a solution to a distribution shift problem. Overall error of predictions dropped from 7% to just 2% and it gets smaller for larger samples.
    
[^19]: ABC简单如123：一种用于无先例多类别类别无关计数的盲目计数方法

    ABC Easy as 123: A Blind Counter for Exemplar-Free Multi-Class Class-agnostic Counting. (arXiv:2309.04820v1 [cs.CV])

    [http://arxiv.org/abs/2309.04820](http://arxiv.org/abs/2309.04820)

    ABC123是一种无需使用示例进行训练或推断的多类别类别无关计数方法，通过引入一个新的范式，它在多种对象同时存在的情况下优于现有方法。

    

    类别无关计数方法可以对任意类别的对象进行计数，在许多领域中提供了巨大的实用性。然而，现有的方法只能适用于需要一组特定类型的示例或图像中仅包含一种类型对象的情况。这些方法的局限之一是缺乏适用于多种对象同时存在的计数设置的数据集。为了解决这些问题，我们提出了第一个多类别、类别无关计数数据集（MCAC）以及一种名为ABC123的盲目计数方法，该方法可以在训练或推断过程中不使用特定类型示例来同时计数多种对象。ABC123引入了一种新的范式，在计数阶段后找到示例来帮助用户理解生成的输出，而不需要先导样本来引导计数。我们展示了ABC123在MCAC上优于现有方法，而无需人工干预进行标注。

    Class-agnostic counting methods enumerate objects of an arbitrary class, providing tremendous utility in many fields. Prior works have limited usefulness as they require either a set of examples of the type to be counted or that the image contains only a single type of object. A significant factor in these shortcomings is the lack of a dataset to properly address counting in settings with more than one kind of object present. To address these issues, we propose the first Multi-class, Class-Agnostic Counting dataset (MCAC) and A Blind Counter (ABC123), a method that can count multiple types of objects simultaneously without using examples of type during training or inference. ABC123 introduces a new paradigm where instead of requiring exemplars to guide the enumeration, examples are found after the counting stage to help a user understand the generated outputs. We show that ABC123 outperforms contemporary methods on MCAC without the requirement of human in-the-loop annotations. We also 
    
[^20]: 检测量子算法差分隐私的违规行为

    Detecting Violations of Differential Privacy for Quantum Algorithms. (arXiv:2309.04819v1 [quant-ph])

    [http://arxiv.org/abs/2309.04819](http://arxiv.org/abs/2309.04819)

    本文定义了一个形式化框架，用于检测量子算法的差分隐私违规行为，并开发了相应的检测算法。算法通过使用张量网络作为数据结构，并在TensorFlow Quantum和TorchQuantum上执行，来验证量子算法是否具有差分隐私。当差分隐私违规报告时，算法会自动生成错误信息，包括违反隐私的量子状态，以说明违规的原因。

    

    在过去的十年中，提出了解决各种实际问题的量子算法，例如数据搜索和分析、产品推荐和信用评分。对量子计算中的隐私和其他伦理问题的关注自然而然地增加。本文定义了一个形式化框架，用于检测量子算法的差分隐私违规行为。我们开发了一个检测算法，用于验证一个（带有噪声的）量子算法是否具有差分隐私，并在报告差分隐私违规时自动生成错误信息。该信息包括一对违反隐私的量子状态，以说明违规的原因。我们的算法采用了张量网络作为高效的数据结构，并在TensorFlow Quantum和TorchQuantum上执行，它们分别是著名机器学习平台TensorFlow和PyTorch的量子扩展。

    Quantum algorithms for solving a wide range of practical problems have been proposed in the last ten years, such as data search and analysis, product recommendation, and credit scoring. The concern about privacy and other ethical issues in quantum computing naturally rises up. In this paper, we define a formal framework for detecting violations of differential privacy for quantum algorithms. A detection algorithm is developed to verify whether a (noisy) quantum algorithm is differentially private and automatically generate bugging information when the violation of differential privacy is reported. The information consists of a pair of quantum states that violate the privacy, to illustrate the cause of the violation. Our algorithm is equipped with Tensor Networks, a highly efficient data structure, and executed both on TensorFlow Quantum and TorchQuantum which are the quantum extensions of famous machine learning platforms -- TensorFlow and PyTorch, respectively. The effectiveness and e
    
[^21]: 神经潜在几何搜索：通过格罗莫夫-豪斯多夫信息驱动的贝叶斯优化来进行乘积流形推断

    Neural Latent Geometry Search: Product Manifold Inference via Gromov-Hausdorff-Informed Bayesian Optimization. (arXiv:2309.04810v1 [cs.LG])

    [http://arxiv.org/abs/2309.04810](http://arxiv.org/abs/2309.04810)

    本论文提出了神经潜在几何搜索(NLGS)的概念，旨在通过格罗莫夫-豪斯多夫距离来自动识别下游任务的最佳潜在几何结构，以提高机器学习模型的性能。

    

    最近的研究表明，通过将潜在空间的几何结构与底层数据结构对齐，可以提高机器学习模型的性能。研究人员提出使用具有恒定曲率的双曲和球形空间，或者它们的组合，来更好地建模潜在空间并增强模型性能，而不仅仅依赖于欧几里得空间。然而，目前对自动识别下游任务的最佳潜在几何结构问题还没有给予足够关注。我们在数学上定义了这个新颖的问题，并将其称为神经潜在几何搜索(NLGS)。具体而言，我们引入了一种基于格罗莫夫-豪斯多夫距离的候选潜在几何结构之间的新概念距离，以实现这一目标。为了计算格罗莫夫-豪斯多夫距离，我们提出了一种通过最小查询评估搜索由恒定曲率模型空间乘积组成的潜在几何结构的原则方法。

    Recent research indicates that the performance of machine learning models can be improved by aligning the geometry of the latent space with the underlying data structure. Rather than relying solely on Euclidean space, researchers have proposed using hyperbolic and spherical spaces with constant curvature, or combinations thereof, to better model the latent space and enhance model performance. However, little attention has been given to the problem of automatically identifying the optimal latent geometry for the downstream task. We mathematically define this novel formulation and coin it as neural latent geometry search (NLGS). More specifically, we introduce a principled method that searches for a latent geometry composed of a product of constant curvature model spaces with minimal query evaluations. To accomplish this, we propose a novel notion of distance between candidate latent geometries based on the Gromov-Hausdorff distance from metric geometry. In order to compute the Gromov-Ha
    
[^22]: 基于机器学习的全面的提交消息质量检查器

    A Full-fledged Commit Message Quality Checker Based on Machine Learning. (arXiv:2309.04797v1 [cs.SE])

    [http://arxiv.org/abs/2309.04797](http://arxiv.org/abs/2309.04797)

    基于机器学习的全面提交消息质量检查器，能够自动评估提交消息的质量，并提供上下文和语义的检查。

    

    提交消息（CMs）是版本控制的重要组成部分。通过提供有关更改内容和原因的重要上下文，它们极大地支持软件维护和演进。但是撰写良好的CMs是困难的，开发人员经常忽视。到目前为止，还没有适合实践的工具可以自动评估CM的编写质量，包括其含义和上下文。鉴于此任务的挑战性，我们提出研究问题：机器学习方法能够如何衡量CM质量，包括语义和上下文？通过考虑最流行的CM质量指南的所有规则，创建这些规则的数据集，并训练和评估最先进的机器学习模型来检查这些规则，我们可以回答这个研究问题：对于最具挑战性的任务，实践中具有82.9％的最低F1分数足够好。我们开发了一个全面的开源框架，检查所有这些CM质量规则。它可以用来辅助开发人员编写更好的CMs。

    Commit messages (CMs) are an essential part of version control. By providing important context in regard to what has changed and why, they strongly support software maintenance and evolution. But writing good CMs is difficult and often neglected by developers. So far, there is no tool suitable for practice that automatically assesses how well a CM is written, including its meaning and context. Since this task is challenging, we ask the research question: how well can the CM quality, including semantics and context, be measured with machine learning methods? By considering all rules from the most popular CM quality guideline, creating datasets for those rules, and training and evaluating state-of-the-art machine learning models to check those rules, we can answer the research question with: sufficiently well for practice, with the lowest F$_1$ score of 82.9\%, for the most challenging task. We develop a full-fledged open-source framework that checks all these CM quality rules. It is use
    
[^23]: 随机梯度下降在高维信号恢复的玻璃能量景观中表现优于梯度下降

    Stochastic Gradient Descent outperforms Gradient Descent in recovering a high-dimensional signal in a glassy energy landscape. (arXiv:2309.04788v1 [cs.LG])

    [http://arxiv.org/abs/2309.04788](http://arxiv.org/abs/2309.04788)

    本研究利用动力学均场理论研究了随机梯度下降（SGD）在高维非凸成本函数优化中的表现。实验结果表明，SGD在恢复高维非线性加密信号问题上明显优于梯度下降（GD）。

    

    随机梯度下降（SGD）是一种非平衡算法，广泛用于训练人工神经网络。然而，我们对SGD在这项技术的成功中至关重要的程度以及相对于其他优化算法（如梯度下降）在优化高维非凸成本函数方面的效果知之甚少。在这项工作中，我们利用动力学均场理论在高维极限中准确分析了其性能。我们考虑恢复隐藏的高维非线性加密信号问题，即一个典型的高维非凸的难优化问题。我们比较了SGD和GD的性能，并表明SGD大大优于GD。特别地，对这些算法的弛豫时间进行幂律拟合表明，SGD在小批量大小的情况下的恢复阈值小于GD的对应阈值。

    Stochastic Gradient Descent (SGD) is an out-of-equilibrium algorithm used extensively to train artificial neural networks. However very little is known on to what extent SGD is crucial for to the success of this technology and, in particular, how much it is effective in optimizing high-dimensional non-convex cost functions as compared to other optimization algorithms such as Gradient Descent (GD). In this work we leverage dynamical mean field theory to analyze exactly its performances in the high-dimensional limit. We consider the problem of recovering a hidden high-dimensional non-linearly encrypted signal, a prototype high-dimensional non-convex hard optimization problem. We compare the performances of SGD to GD and we show that SGD largely outperforms GD. In particular, a power law fit of the relaxation time of these algorithms shows that the recovery threshold for SGD with small batch size is smaller than the corresponding one of GD.
    
[^24]: RRCNN$^{+}$：一种增强的用于非平稳信号分解的残差递归卷积神经网络

    RRCNN$^{+}$: An Enhanced Residual Recursive Convolutional Neural Network for Non-stationary Signal Decomposition. (arXiv:2309.04782v1 [cs.LG])

    [http://arxiv.org/abs/2309.04782](http://arxiv.org/abs/2309.04782)

    RRCNN$^{+}$是一种增强的残差递归卷积神经网络，用于非平稳信号分解。它能更稳定地分解信号并具有较低的计算成本，深度学习为非平稳信号分解提供了新的视角。

    

    时间频率分析是许多应用中的一个重要且具有挑战性的任务。傅里叶变换和小波分析是两种经典方法，在许多领域取得了显著的成功。然而，这些方法在处理非线性和非平稳信号时存在一定的局限性。为了应对这一挑战，研究人员提出了一系列非线性和自适应的方法，其中以经验模态分解方法为先驱。它们的目标是将非平稳信号分解为准平稳分量，以在时间频率分析中揭示更好的特征。最近，受深度学习的启发，我们提出了一种新颖的方法，称为残差递归卷积神经网络（RRCNN）。RRCNN不仅可以在对大规模信号进行批处理时实现更稳定的分解，而且还为非平稳信号分解提供了独特的视角。在本研究中，我们旨在借助服务器的帮助进一步改进RRCNN。

    Time-frequency analysis is an important and challenging task in many applications. Fourier and wavelet analysis are two classic methods that have achieved remarkable success in many fields. They also exhibit limitations when applied to nonlinear and non-stationary signals. To address this challenge, a series of nonlinear and adaptive methods, pioneered by the empirical mode decomposition method have been proposed. Their aim is to decompose a non-stationary signal into quasi-stationary components which reveal better features in the time-frequency analysis. Recently, inspired by deep learning, we proposed a novel method called residual recursive convolutional neural network (RRCNN). Not only RRCNN can achieve more stable decomposition than existing methods while batch processing large-scale signals with low computational cost, but also deep learning provides a unique perspective for non-stationary signal decomposition. In this study, we aim to further improve RRCNN with the help of sever
    
[^25]: 通过减少参数的弱点 改进稳健模型数

    Towards Robust Model Watermark via Reducing Parametric Vulnerability. (arXiv:2309.04777v1 [cs.CR])

    [http://arxiv.org/abs/2309.04777](http://arxiv.org/abs/2309.04777)

    这篇论文提出了一种通过减少参数弱点来改进模型水印的方法，实验结果表明这种方法可以在近邻中找到并恢复去水印模型的水印行为。

    

    深度神经网络由于其商业价值和对资源的巨大需求而成为宝贵的资产，然而为了保护深度神经网络的版权，最近基于后门的拥有权验证变得流行起来。在这种验证方式中，模型所有者可以在发布之前通过嵌入特定的后门行为对模型进行水印标记。防御方（通常是模型所有者）可以根据行为的存在来判断可疑的第三方模型是否是从他们那里“偷”来的。不幸的是，这些水印已经被证明对移除攻击（甚至如微调）非常脆弱。为了进一步探索这种脆弱性，我们对参数空间进行了研究，并发现存在许多在水印模型附近的去水印模型，这些模型可能很容易被用于移除攻击。受到这一发现的启发，我们提出了一个迷你最大化最小化问题来找到这些去水印模型并恢复它们的水印行为。大量实验表明，我们的方法可以找到并恢复去水印模型的水印行为。

    Deep neural networks are valuable assets considering their commercial benefits and huge demands for costly annotation and computation resources. To protect the copyright of DNNs, backdoor-based ownership verification becomes popular recently, in which the model owner can watermark the model by embedding a specific backdoor behavior before releasing it. The defenders (usually the model owners) can identify whether a suspicious third-party model is ``stolen'' from them based on the presence of the behavior. Unfortunately, these watermarks are proven to be vulnerable to removal attacks even like fine-tuning. To further explore this vulnerability, we investigate the parameter space and find there exist many watermark-removed models in the vicinity of the watermarked one, which may be easily used by removal attacks. Inspired by this finding, we propose a mini-max formulation to find these watermark-removed models and recover their watermark behavior. Extensive experiments demonstrate that o
    
[^26]: AudRandAug：用于音频分类的随机图像增强方法

    AudRandAug: Random Image Augmentations for Audio Classification. (arXiv:2309.04762v1 [cs.SD])

    [http://arxiv.org/abs/2309.04762](http://arxiv.org/abs/2309.04762)

    AudRandAug是一种应用于音频数据的随机图像增强方法，它从专门的音频搜索空间中选择数据增强策略，并在准确率表现方面优于其他方法。

    

    数据增强已被证明在训练神经网络中非常有效。最近，提出了一种称为RandAug的方法，该方法从预定义的搜索空间中随机选择数据增强技术。RandAug在图像相关任务中展示了显著的性能提升，同时附加的计算开销很小。然而，以往的研究尚未探索将RandAug应用于将音频转化为图像模式的音频数据增强。为了填补这一空白，我们引入了AudRandAug，这是一种针对音频数据的RandAug改进方法。AudRandAug从专门的音频搜索空间中选择数据增强策略。为了评估AudRandAug的有效性，我们使用了各种模型和数据集进行了实验。我们的研究结果表明，AudRandAug在准确率表现方面优于其他现有的数据增强方法。

    Data augmentation has proven to be effective in training neural networks. Recently, a method called RandAug was proposed, randomly selecting data augmentation techniques from a predefined search space. RandAug has demonstrated significant performance improvements for image-related tasks while imposing minimal computational overhead. However, no prior research has explored the application of RandAug specifically for audio data augmentation, which converts audio into an image-like pattern. To address this gap, we introduce AudRandAug, an adaptation of RandAug for audio data. AudRandAug selects data augmentation policies from a dedicated audio search space. To evaluate the effectiveness of AudRandAug, we conducted experiments using various models and datasets. Our findings indicate that AudRandAug outperforms other existing data augmentation methods regarding accuracy performance.
    
[^27]: 在教育数据挖掘中深度学习技术的综合调研

    A Comprehensive Survey on Deep Learning Techniques in Educational Data Mining. (arXiv:2309.04761v1 [cs.LG])

    [http://arxiv.org/abs/2309.04761](http://arxiv.org/abs/2309.04761)

    本调研综合审查了在教育数据挖掘中深度学习技术的最新研究进展，包括对知识跟踪、学生不良行为检测、性能预测和个性化推荐等典型教育场景的应用。同时提供了公共数据集和处理工具的综合概述，并指出了未来的研究方向。

    

    教育数据挖掘(EDM)作为研究的重要领域，利用计算技术来分析教育数据。随着教育数据的复杂性和多样性增加，深度学习技术在解决分析和建模这些数据所面临的挑战方面表现出了显著的优势。本调研旨在系统地审查深度学习在EDM领域的最新研究进展。我们首先提供了关于EDM和深度学习的简要介绍，强调了它们在现代教育环境中的重要性。接下来，我们详细回顾了在四个典型教育场景中应用的深度学习技术，包括知识跟踪、学生不良行为检测、性能预测和个性化推荐。此外，我们还提供了EDM的公共数据集和处理工具的综合概述。最后，我们指出了该研究领域的新兴趋势和未来方向。

    Educational Data Mining (EDM) has emerged as a vital field of research, which harnesses the power of computational techniques to analyze educational data. With the increasing complexity and diversity of educational data, Deep Learning techniques have shown significant advantages in addressing the challenges associated with analyzing and modeling this data. This survey aims to systematically review the state-of-the-art in EDM with Deep Learning. We begin by providing a brief introduction to EDM and Deep Learning, highlighting their relevance in the context of modern education. Next, we present a detailed review of Deep Learning techniques applied in four typical educational scenarios, including knowledge tracing, undesirable student detecting, performance prediction, and personalized recommendation. Furthermore, a comprehensive overview of public datasets and processing tools for EDM is provided. Finally, we point out emerging trends and future directions in this research area.
    
[^28]: RR-CP: 可靠区域基础的可信医学图像分类的一种确认预测方法

    RR-CP: Reliable-Region-Based Conformal Prediction for Trustworthy Medical Image Classification. (arXiv:2309.04760v1 [cs.LG])

    [http://arxiv.org/abs/2309.04760](http://arxiv.org/abs/2309.04760)

    RR-CP 提出了一种可靠区域基础的确认预测方法，为医学图像分类提供高效干预和质量检查。它通过优化预测集的大小，在用户指定的错误率下提供更强的统计保证。

    

    确认预测（CP）为给定的测试样本生成一组预测，使得预测集几乎总是包含真实标签（例如，99.5％的时间）。 CP对给定测试样本的可能标签提供全面的预测，而集合的大小表示预测的确定程度（例如，大于一的集合是“不确定的”）。 CP的这些独特属性使人类专家和医学AI模型之间能够有效合作，在临床决策过程中进行高效干预和质量检查。 在本文中，我们提出了一种新的方法，称为可靠区域基础的确认预测（RR-CP），旨在提供更强的统计保证，以便在测试时间内达到用户指定的错误率（例如，0.5％），并在此约束条件下，优化预测集的大小（尽量小）。 当用户指定的错误率达到时，我们认为预测集大小是一个重要的衡量标准。

    Conformal prediction (CP) generates a set of predictions for a given test sample such that the prediction set almost always contains the true label (e.g., 99.5\% of the time). CP provides comprehensive predictions on possible labels of a given test sample, and the size of the set indicates how certain the predictions are (e.g., a set larger than one is `uncertain'). Such distinct properties of CP enable effective collaborations between human experts and medical AI models, allowing efficient intervention and quality check in clinical decision-making. In this paper, we propose a new method called Reliable-Region-Based Conformal Prediction (RR-CP), which aims to impose a stronger statistical guarantee so that the user-specified error rate (e.g., 0.5\%) can be achieved in the test time, and under this constraint, the size of the prediction set is optimized (to be small). We consider a small prediction set size an important measure only when the user-specified error rate is achieved. Experi
    
[^29]: 改进ReLU网络的预测不确定性的仿射不变集成变换方法

    Affine Invariant Ensemble Transform Methods to Improve Predictive Uncertainty in ReLU Networks. (arXiv:2309.04742v1 [stat.ML])

    [http://arxiv.org/abs/2309.04742](http://arxiv.org/abs/2309.04742)

    本文提出了一种仿射不变集成变换方法，可以改善在ReLU网络中的预测不确定性。通过基于集合卡尔曼滤波的贝叶斯推断，我们提出了两个相互作用的粒子系统，并证明了它们的收敛性。同时，我们还探讨了这些方法用于量化预测不确定性的有效性。

    

    我们考虑使用合适的集合卡尔曼滤波的扩展进行逻辑回归的贝叶斯推断问题。我们提出了两个相互作用的粒子系统，从近似后验分布中采样，并证明当粒子数量趋于无穷时，这些相互作用粒子系统收敛到均场极限的量化收敛速率。此外，我们应用这些技术并考察它们作为贝叶斯近似方法在ReLU网络中量化预测不确定性的有效性。

    We consider the problem of performing Bayesian inference for logistic regression using appropriate extensions of the ensemble Kalman filter. Two interacting particle systems are proposed that sample from an approximate posterior and prove quantitative convergence rates of these interacting particle systems to their mean-field limit as the number of particles tends to infinity. Furthermore, we apply these techniques and examine their effectiveness as methods of Bayesian approximation for quantifying predictive uncertainty in ReLU networks.
    
[^30]: Spiking Neural Network联合课程学习策略的训练

    Training of Spiking Neural Network joint Curriculum Learning Strategy. (arXiv:2309.04737v1 [cs.LG])

    [http://arxiv.org/abs/2309.04737](http://arxiv.org/abs/2309.04737)

    该论文提出了一种将课程学习引入脉冲神经网络的训练模型，使其更类似于人类学习过程，并提高了其生物解释性。

    

    从简单到复杂，逐渐引入难度的概念是人类学习的自然过程。脉冲神经网络（SNNs）旨在模拟人类信息处理的方式，但目前的SNNs模型将所有样本视为平等，这与人类学习的原则不符，并忽视了SNNs的生物合理性。为了解决这个问题，我们提出了一个将课程学习（CL）引入SNNs的CL-SNN模型，使SNNs更像人类学习，并提供更高的生物解释性。CL是一种训练策略，提倡在逐渐引入更具挑战性的数据之前向模型展示更容易的数据，模拟了人类学习的过程。我们使用具有信心感知的损失来衡量和处理不同难度水平的样本。通过学习不同样本的置信度，模型自动减少了难样本对参数优化的贡献。我们在实验中进行了研究

    Starting with small and simple concepts, and gradually introducing complex and difficult concepts is the natural process of human learning. Spiking Neural Networks (SNNs) aim to mimic the way humans process information, but current SNNs models treat all samples equally, which does not align with the principles of human learning and overlooks the biological plausibility of SNNs. To address this, we propose a CL-SNN model that introduces Curriculum Learning(CL) into SNNs, making SNNs learn more like humans and providing higher biological interpretability. CL is a training strategy that advocates presenting easier data to models before gradually introducing more challenging data, mimicking the human learning process. We use a confidence-aware loss to measure and process the samples with different difficulty levels. By learning the confidence of different samples, the model reduces the contribution of difficult samples to parameter optimization automatically. We conducted experiments on st
    
[^31]: 用于细粒度多时空风力预测的深度神经网络

    A Spatiotemporal Deep Neural Network for Fine-Grained Multi-Horizon Wind Prediction. (arXiv:2309.04733v1 [cs.LG])

    [http://arxiv.org/abs/2309.04733](http://arxiv.org/abs/2309.04733)

    提出了一种用于细粒度多时空风力预测的多时空网络模型(MHSTN)，该模型能够从多个数据源中提取特征，并产生精确和高效的预测结果。

    

    风速和风向的预测对于航空和风能发电等许多实际应用具有重要影响，由于天气数据中的高随机性和复杂相关性，这一预测非常具有挑战性。现有方法通常只关注一部分影响因素，因此缺乏对问题的系统处理。此外，在文献中对于细粒度预测的关注较少，而细粒度预测对于高效的行业运营至关重要。在本研究中，我们提出了一种新颖的数据驱动模型，即多时空风力网络(MHSTN)，以实现精确和高效的细粒度风力预测。MHSTN将针对不同因素的多个深度神经网络集成到一个序列到序列(Seq2Seq)骨架中，以有效地从各种数据源提取特征，并为给定区域内的所有站点产生多时空的预测。MHSTN由四个主要模块组成。首先，一个时间模块

    The prediction of wind in terms of both wind speed and direction, which has a crucial impact on many real-world applications like aviation and wind power generation, is extremely challenging due to the high stochasticity and complicated correlation in the weather data. Existing methods typically focus on a sub-set of influential factors and thus lack a systematic treatment of the problem. In addition, fine-grained forecasting is essential for efficient industry operations, but has been less attended in the literature. In this work, we propose a novel data-driven model, Multi-Horizon SpatioTemporal Network (MHSTN), generally for accurate and efficient fine-grained wind prediction. MHSTN integrates multiple deep neural networks targeting different factors in a sequence-to-sequence (Seq2Seq) backbone to effectively extract features from various data sources and produce multi-horizon predictions for all sites within a given region. MHSTN is composed of four major modules. First, a temporal
    
[^32]: TCGAN: 用于时间序列分类和聚类的卷积生成对抗网络

    TCGAN: Convolutional Generative Adversarial Network for Time Series Classification and Clustering. (arXiv:2309.04732v1 [cs.LG])

    [http://arxiv.org/abs/2309.04732](http://arxiv.org/abs/2309.04732)

    TCGAN是一个用于时间序列识别的卷积生成对抗网络，通过对抗博弈学习时间序列的分层表示，无需标记信息。

    

    最近的研究表明，监督式卷积神经网络（CNN）在学习时间序列数据的分层表示方面具有优越性，可用于成功的分类。然而，这些方法需要足够大的标记数据进行稳定学习，但是获取高质量的标记时间序列数据可能代价高昂，也可能不可行。生成对抗网络（GANs）在增强无监督和半监督学习方面取得了巨大成功。然而，据我们所知，GANs如何有效地作为学习时间序列识别（即分类和聚类）的通用解决方案仍然不清楚。上述考虑激发了我们引入了一个时间序列卷积GAN（TCGAN）。TCGAN通过在没有标签信息的情况下，两个一维CNN（即生成器和判别器）之间进行对抗博弈来学习。然后，训练的TCGAN的一部分被重复利用来构建一个表示学习器。

    Recent works have demonstrated the superiority of supervised Convolutional Neural Networks (CNNs) in learning hierarchical representations from time series data for successful classification. These methods require sufficiently large labeled data for stable learning, however acquiring high-quality labeled time series data can be costly and potentially infeasible. Generative Adversarial Networks (GANs) have achieved great success in enhancing unsupervised and semi-supervised learning. Nonetheless, to our best knowledge, it remains unclear how effectively GANs can serve as a general-purpose solution to learn representations for time series recognition, i.e., classification and clustering. The above considerations inspire us to introduce a Time-series Convolutional GAN (TCGAN). TCGAN learns by playing an adversarial game between two one-dimensional CNNs (i.e., a generator and a discriminator) in the absence of label information. Parts of the trained TCGAN are then reused to construct a rep
    
[^33]: 回声指数的转变及对输入重复的依赖

    Transitions in echo index and dependence on input repetitions. (arXiv:2309.04728v1 [math.DS])

    [http://arxiv.org/abs/2309.04728](http://arxiv.org/abs/2309.04728)

    回声指数是一个非自治动力系统中同时稳定渐近响应的数量，我们研究了回声指数对依赖于参数以及输入重复的关系。

    

    回声指数是一个非自治（即受输入驱动）动力系统中同时稳定渐近响应的数量。它推广了递归神经网络的回声状态性质，这对应于回声指数等于一。在本文中，我们研究了回声指数如何依赖于控制系统对强迫动力学的有限状态随机外部输入的参数。我们考虑了一个在有限一组映射之间切换的非自治系统的回声指数，其中我们假设每个映射具有有限一组双曲型平衡吸引子。我们发现每个映射的最小和最大重复对于得到的回声指数是至关重要的。将我们的理论发现用RNN计算框架表示，我们得到对于小幅强迫，回声指数对应于无输入系统的吸引子数量，而对于大幅强迫，回声指数减少到一。中间的情况

    The echo index counts the number of simultaneously stable asymptotic responses of a nonautonomous (i.e. input-driven) dynamical system. It generalizes the well-known echo state property for recurrent neural networks this corresponds to the echo index being equal to one. In this paper, we investigate how the echo index depends on parameters that govern typical responses to a finite-state ergodic external input that forces the dynamics. We consider the echo index for a nonautonomous system that switches between a finite set of maps, where we assume that each map possesses a finite set of hyperbolic equilibrium attractors. We find the minimum and maximum repetitions of each map are crucial for the resulting echo index. Casting our theoretical findings in the RNN computing framework, we obtain that for small amplitude forcing the echo index corresponds to the number of attractors for the input-free system, while for large amplitude forcing, the echo index reduces to one. The intermediate
    
[^34]: 利用大型语言模型重现网络研究结果

    Toward Reproducing Network Research Results Using Large Language Models. (arXiv:2309.04716v1 [cs.LG])

    [http://arxiv.org/abs/2309.04716](http://arxiv.org/abs/2309.04716)

    本文提出使用大型语言模型（LLMs）来重现网络研究结果，通过一个小规模实验证明了其可行性，并以ChatGPT为工具重现了不同发表于著名会议和期刊的网络系统。

    

    在网络学术界和工业界中，重现研究结果非常重要。当前的最佳实践通常有三种方法：（1）寻找公开可用的原型；（2）联系作者获取私有原型；以及（3）根据论文描述手动实现原型。然而，大多数已发表的网络研究没有公开原型，而获取私有原型也很困难。因此，大部分重现工作都花费在根据论文描述进行手动实现上，这既耗时又费力，容易出错。本文大胆地提出使用新兴的大型语言模型（LLMs）来重现网络研究结果。特别地，我们首先通过小规模实验证明了其可行性，其中四名具备必要网络知识的学生使用ChatGPT进行了不同发表于著名会议和期刊的网络系统的重现工作。

    Reproducing research results in the networking community is important for both academia and industry. The current best practice typically resorts to three approaches: (1) looking for publicly available prototypes; (2) contacting the authors to get a private prototype; and (3) manually implementing a prototype following the description of the publication. However, most published network research does not have public prototypes and private prototypes are hard to get. As such, most reproducing efforts are spent on manual implementation based on the publications, which is both time and labor consuming and error-prone. In this paper, we boldly propose reproducing network research results using the emerging large language models (LLMs). In particular, we first prove its feasibility with a small-scale experiment, in which four students with essential networking knowledge each reproduces a different networking system published in prominent conferences and journals by prompt engineering ChatGPT
    
[^35]: 从探索性视角解释代理行为的优势演员-评论员

    Advantage Actor-Critic with Reasoner: Explaining the Agent's Behavior from an Exploratory Perspective. (arXiv:2309.04707v1 [cs.AI])

    [http://arxiv.org/abs/2309.04707](http://arxiv.org/abs/2309.04707)

    本文提出了一种新颖的基于Reasoner的优势演员-评论员（A2CR）方法，通过预定义和分类演员行为的潜在目的，自动生成一个更全面和可解释的理解代理决策过程的范例。

    

    强化学习（RL）是解决复杂决策问题的强大工具，但它的缺乏透明度和解释性在决策具有实际后果的领域中一直是一个重要挑战。本文提出了一种新颖的基于Reasoner的优势演员-评论员（A2CR）方法，可以轻松应用于基于演员-评论员的RL模型，并使其具有可解释性。A2CR由三个相互连接的网络组成：策略网络，价值网络和Reasoner网络。通过预定义和分类演员行为的潜在目的，A2CR自动生成了一个更全面和可解释的理解代理决策过程的范例。它提供了诸如基于目的的显著性、早期失败检测和模型监管等一系列功能，从而促进负责任和可信赖的RL。在动作丰富的超级马里奥兄弟环境中进行的评估产生了有趣的发现：Reasoner-。

    Reinforcement learning (RL) is a powerful tool for solving complex decision-making problems, but its lack of transparency and interpretability has been a major challenge in domains where decisions have significant real-world consequences. In this paper, we propose a novel Advantage Actor-Critic with Reasoner (A2CR), which can be easily applied to Actor-Critic-based RL models and make them interpretable. A2CR consists of three interconnected networks: the Policy Network, the Value Network, and the Reasoner Network. By predefining and classifying the underlying purpose of the actor's actions, A2CR automatically generates a more comprehensive and interpretable paradigm for understanding the agent's decision-making process. It offers a range of functionalities such as purpose-based saliency, early failure detection, and model supervision, thereby promoting responsible and trustworthy RL. Evaluations conducted in action-rich Super Mario Bros environments yield intriguing findings: Reasoner-
    
[^36]: 通过优化大型语言模型进行虚假信息和假新闻的检测分析

    Analysis of Disinformation and Fake News Detection Using Fine-Tuned Large Language Model. (arXiv:2309.04704v1 [cs.CL])

    [http://arxiv.org/abs/2309.04704](http://arxiv.org/abs/2309.04704)

    本研究考虑使用LLM模型通过细调实现虚假信息和假新闻的深入分析，揭示复杂的风格和叙事，并提取命名实体的情感，以此作为监督机器学习模型中的预测性特征。

    

    本文考虑使用LLM（Llama 2大型语言模型）通过细调进行虚假信息分析和假新闻的检测。采用了基于PEFT/LoRA的细调方法。研究中，该模型对以下任务进行了细调：揭示虚假信息和宣传叙事的文本分析，事实核查，假新闻检测，操纵分析以及提取带有情感的命名实体。所得结果表明，经过细调的Llama 2模型能够对文本进行深入分析，并揭示复杂的风格和叙事。带有情感的命名实体可以作为监督机器学习模型中的预测性特征。

    The paper considers the possibility of fine-tuning Llama 2 large language model (LLM) for the disinformation analysis and fake news detection. For fine-tuning, the PEFT/LoRA based approach was used. In the study, the model was fine-tuned for the following tasks: analysing a text on revealing disinformation and propaganda narratives, fact checking, fake news detection, manipulation analytics, extracting named entities with their sentiments. The obtained results show that the fine-tuned Llama 2 model can perform a deep analysis of texts and reveal complex styles and narratives. Extracted sentiments for named entities can be considered as predictive features in supervised machine learning models.
    
[^37]: 弱-PDE-LEARN：一种基于弱形式的方法，从有噪声、有限数据中发现PDEs

    Weak-PDE-LEARN: A Weak Form Based Approach to Discovering PDEs From Noisy, Limited Data. (arXiv:2309.04699v1 [cs.LG])

    [http://arxiv.org/abs/2309.04699](http://arxiv.org/abs/2309.04699)

    弱-PDE-LEARN是一种基于弱形式的算法，可以从有噪声、有限数据中发现非线性PDEs。

    

    我们介绍了一种弱-PDE-LEARN算法，它可以从具有噪声、有限测量解的情况下识别非线性PDEs。弱-PDE-LEARN使用基于弱形式的自适应损失函数来训练神经网络U，以近似PDE解，并同时识别控制PDE。这种方法能够从具有噪声、有限测量解的情况下发现多种PDEs。我们通过学习几个基准PDEs来证明弱-PDE-LEARN的有效性。

    We introduce Weak-PDE-LEARN, a Partial Differential Equation (PDE) discovery algorithm that can identify non-linear PDEs from noisy, limited measurements of their solutions. Weak-PDE-LEARN uses an adaptive loss function based on weak forms to train a neural network, $U$, to approximate the PDE solution while simultaneously identifying the governing PDE. This approach yields an algorithm that is robust to noise and can discover a range of PDEs directly from noisy, limited measurements of their solutions. We demonstrate the efficacy of Weak-PDE-LEARN by learning several benchmark PDEs.
    
[^38]: 无冗余的自监督关系学习方法用于图聚类

    Redundancy-Free Self-Supervised Relational Learning for Graph Clustering. (arXiv:2309.04694v1 [cs.LG])

    [http://arxiv.org/abs/2309.04694](http://arxiv.org/abs/2309.04694)

    本文提出了一种名为无冗余关系图聚类（R^2FGC）的自监督深度图聚类方法，该方法从全局和局部视角提取属性和结构级别的关系信息，从而有效地利用了图结构化数据的语义信息。

    

    图聚类是数据分析中一项基础且具有挑战性的任务，其通过学习节点表示来进行有效的聚类分配，并在近年来伴随图神经网络的关注而受到广泛关注。然而，大多数现有方法都忽视了图中非独立和非同分布节点之间的内在关系信息。由于对关系属性的探索不足，图结构化数据的语义信息无法被充分利用，导致聚类性能较差。本文提出了一种名为无冗余关系图聚类（R^2FGC）的自监督深度图聚类方法，以解决这个问题。该方法基于自编码器和图自编码器从全局和局部视角提取属性和结构级别的关系信息。为了获得语义信息的有效表示，我们保留增强数据中的一致关系。

    Graph clustering, which learns the node representations for effective cluster assignments, is a fundamental yet challenging task in data analysis and has received considerable attention accompanied by graph neural networks in recent years. However, most existing methods overlook the inherent relational information among the non-independent and non-identically distributed nodes in a graph. Due to the lack of exploration of relational attributes, the semantic information of the graph-structured data fails to be fully exploited which leads to poor clustering performance. In this paper, we propose a novel self-supervised deep graph clustering method named Relational Redundancy-Free Graph Clustering (R$^2$FGC) to tackle the problem. It extracts the attributeand structure-level relational information from both global and local views based on an autoencoder and a graph autoencoder. To obtain effective representations of the semantic information, we preserve the consistent relation among aug
    
[^39]: 灵活而健壮的具有最小满足扰动的对事实解释

    Flexible and Robust Counterfactual Explanations with Minimal Satisfiable Perturbations. (arXiv:2309.04676v1 [cs.LG])

    [http://arxiv.org/abs/2309.04676](http://arxiv.org/abs/2309.04676)

    本论文提出了一种名为CEMSP的解决方案，通过限制异常特征的变化值来提供灵活且健壮的对事实解释。

    

    对事实解释（CFEs）展示了如何通过最小修改特征向量来实现不同实例的预测。CFEs可以增强信息公平性和可信度，并为收到不利预测的用户提供建议。然而，最近的研究表明，对于相同实例或稍有差异的实例，可以提供多个CFEs。多个CFEs为用户选择提供了灵活性和多样性的需求。然而，如果返回不稳定的CFEs且成本不同，将会损害个体公平性和模型可靠性。现有方法无法同时利用灵活性并解决非健壮性的问题。为了解决这些问题，我们提出了一个概念上简单而有效的解决方案，命名为具有最小满足扰动的对事实解释（CEMSP）。

    Counterfactual explanations (CFEs) exemplify how to minimally modify a feature vector to achieve a different prediction for an instance. CFEs can enhance informational fairness and trustworthiness, and provide suggestions for users who receive adverse predictions. However, recent research has shown that multiple CFEs can be offered for the same instance or instances with slight differences. Multiple CFEs provide flexible choices and cover diverse desiderata for user selection. However, individual fairness and model reliability will be damaged if unstable CFEs with different costs are returned. Existing methods fail to exploit flexibility and address the concerns of non-robustness simultaneously. To address these issues, we propose a conceptually simple yet effective solution named Counterfactual Explanations with Minimal Satisfiable Perturbations (CEMSP). Specifically, CEMSP constrains changing values of abnormal features with the help of their semantically meaningful normal ranges. Fo
    
[^40]: Compact：逼近复杂激活函数用于安全计算

    Compact: Approximating Complex Activation Functions for Secure Computation. (arXiv:2309.04664v1 [cs.CR])

    [http://arxiv.org/abs/2309.04664](http://arxiv.org/abs/2309.04664)

    Compact提出了一种逼近复杂激活函数的方法，可以与MPC技术高效配合使用，并且几乎不会损失模型准确性。

    

    安全多方计算（Secure multi-party computation，MPC）技术可以用于在用户查询部署在公共云上的深度神经网络（DNN）模型时提供数据隐私。最先进的MPC技术可以直接用于使用简单激活函数（AFs）的DNN模型，如ReLU。然而，为了最新的应用设计的DNN模型架构通常使用复杂且高度非线性的AFs。为这些复杂AFs设计高效的MPC技术是一个尚未解决的问题。为此，我们提出了Compact，它产生复杂AFs的分段多项式逼近，以便与最先进的MPC技术高效配合使用。Compact既不需要也不强加任何模型训练的限制，并且导致几乎相同的模型准确性。我们在使用流行的复杂AFs SiLU、GeLU和Mish的DNN架构的四个不同机器学习任务上对Compact进行了广泛评估。我们的实验结果表明，与传统方法相比，Compact几乎不会损失准确性。

    Secure multi-party computation (MPC) techniques can be used to provide data privacy when users query deep neural network (DNN) models hosted on a public cloud. State-of-the-art MPC techniques can be directly leveraged for DNN models that use simple activation functions (AFs) such as ReLU. However, DNN model architectures designed for cutting-edge applications often use complex and highly non-linear AFs. Designing efficient MPC techniques for such complex AFs is an open problem.  Towards this, we propose Compact, which produces piece-wise polynomial approximations of complex AFs to enable their efficient use with state-of-the-art MPC techniques. Compact neither requires nor imposes any restriction on model training and results in near-identical model accuracy. We extensively evaluate Compact on four different machine-learning tasks with DNN architectures that use popular complex AFs SiLU, GeLU, and Mish. Our experimental results show that Compact incurs negligible accuracy loss compared
    
[^41]: MADLAD-400: 一种多语言和文档级的大规模审核数据集

    MADLAD-400: A Multilingual And Document-Level Large Audited Dataset. (arXiv:2309.04662v1 [cs.CL])

    [http://arxiv.org/abs/2309.04662](http://arxiv.org/abs/2309.04662)

    MADLAD-400是一种覆盖419种语言的多语言文档级数据集，通过自我审核揭示了局限性，通过训练众多参数的机器翻译模型取得了竞争力，并提供了基准模型给研究界使用。

    

    我们介绍了MADLAD-400，这是一个基于CommonCrawl的手动审核的通用领域3T token单语数据集，涵盖了419种语言。我们讨论了通过自我审核MADLAD-400揭示出的局限性，以及数据审核在数据集创建过程中的作用。然后，我们使用公开可用的数据训练并发布了一个涵盖450多种语言、2500亿个标记的10.7B参数的多语言机器翻译模型，并发现它在不同领域上与规模更大的模型相比具有竞争力，并报告了结果。此外，我们还训练了一个8B参数的语言模型，并评估了少样本翻译的结果。我们将基准模型提供给研究界使用。

    We introduce MADLAD-400, a manually audited, general domain 3T token monolingual dataset based on CommonCrawl, spanning 419 languages. We discuss the limitations revealed by self-auditing MADLAD-400, and the role data auditing had in the dataset creation process. We then train and release a 10.7B-parameter multilingual machine translation model on 250 billion tokens covering over 450 languages using publicly available data, and find that it is competitive with models that are significantly larger, and report the results on different domains. In addition, we train a 8B-parameter language model, and assess the results on few-shot translation. We make the baseline models available to the research community.
    
[^42]: 使用深度学习预测人类意图的智能上肢外骨骼系统以增强感觉反馈

    Intelligent upper-limb exoskeleton using deep learning to predict human intention for sensory-feedback augmentation. (arXiv:2309.04655v1 [cs.RO])

    [http://arxiv.org/abs/2309.04655](http://arxiv.org/abs/2309.04655)

    创新点在于引入了云端深度学习和嵌入式柔性传感器，实现了智能上肢外骨骼系统来预测人类上肢运动的意图并提供感觉反馈，准确率达到96.2%，能以人类意图为基础进行操作。

    

    随着年龄增长和中风等因素，人体肌肉骨骼力量下降，影响了使用上肢进行日常任务的能力。虽然现有一些外骨骼装置，但由于缺乏传感器反馈和对运动意图的预测，需要手动操作。本研究引入了一种智能上肢外骨骼系统，利用云端深度学习来预测人类的意图以增强力量。嵌入式柔性传感器通过收集实时肌肉信号提供感觉反馈，并同时计算以确定用户的意图运动。云端深度学习预测四种上肢关节运动，平均准确率达到96.2%，响应速度为200-250毫秒，表明外骨骼系统完全依靠人类意图进行操作。此外，一组柔性气动装置通过提供897牛顿力和78.7毫米的位移来辅助意图运动。

    The age and stroke-associated decline in musculoskeletal strength degrades the ability to perform daily human tasks using the upper extremities. Although there are a few examples of exoskeletons, they need manual operations due to the absence of sensor feedback and no intention prediction of movements. Here, we introduce an intelligent upper-limb exoskeleton system that uses cloud-based deep learning to predict human intention for strength augmentation. The embedded soft wearable sensors provide sensory feedback by collecting real-time muscle signals, which are simultaneously computed to determine the user's intended movement. The cloud-based deep-learning predicts four upper-limb joint motions with an average accuracy of 96.2% at a 200-250 millisecond response rate, suggesting that the exoskeleton operates just by human intention. In addition, an array of soft pneumatics assists the intended movements by providing 897 newton of force and 78.7 millimeter of displacement at maximum. Col
    
[^43]: 探索神经网络坍塌现象：批归一化和权重衰减的影响

    Towards Understanding Neural Collapse: The Effects of Batch Normalization and Weight Decay. (arXiv:2309.04644v1 [cs.LG])

    [http://arxiv.org/abs/2309.04644](http://arxiv.org/abs/2309.04644)

    本文研究了批归一化和权重衰减对神经网络坍塌现象的影响，并提出了对坍塌现象进行度量的方法。通过理论和实验，证明了在接近最优时，批归一化和权重衰减能够促使神经网络坍塌的出现。

    

    神经网络坍塌是最近观察到的一种在神经网络分类器最后一层中出现的几何结构。具体来说，神经网络坍塌指的是在神经网络训练的终端阶段，1）最后一层特征的类内变异趋向于零，2）类特征均值构成等角紧框架（ETF），3）最后一层类特征和权重在缩放上相等，4）分类行为崩溃到最近的类中心（NCC）决策规则。本文研究了批归一化和权重衰减对神经网络坍塌现象的影响。我们提出了几何直观的类内和类间余弦相似度度量，捕捉了神经网络坍塌的多个核心方面。利用这个度量，我们在正则化交叉熵损失接近最优时，提供了关于批归一化和权重衰减下神经网络坍塌的理论保证。我们还进行了进一步的实验。

    Neural Collapse is a recently observed geometric structure that emerges in the final layer of neural network classifiers. Specifically, Neural Collapse states that at the terminal phase of neural networks training, 1) the intra-class variability of last-layer features tends to zero, 2) the class feature means form an Equiangular Tight Frame (ETF), 3) last-layer class features and weights becomes equal up the scaling, and 4) classification behavior collapses to the nearest class center (NCC) decision rule. This paper investigates the effect of batch normalization and weight decay on the emergence of Neural Collapse. We propose the geometrically intuitive intra-class and inter-class cosine similarity measure which captures multiple core aspects of Neural Collapse. With this measure, we provide theoretical guarantees of Neural Collapse emergence with last-layer batch normalization and weight decay when the regularized cross-entropy loss is near optimal. We also perform further experiments
    
[^44]: 从示范中预训练触觉表示，实现力基动作的少样本学习

    Few-Shot Learning of Force-Based Motions From Demonstration Through Pre-training of Haptic Representation. (arXiv:2309.04640v1 [cs.RO])

    [http://arxiv.org/abs/2309.04640](http://arxiv.org/abs/2309.04640)

    该论文提出了一种能够从少量示范中学习力基动作的方法，通过对触觉表示进行预训练，并利用示范学习来训练动作生成器。实验证明，预训练显著提高了模型识别物理特性和生成所需动作的能力。

    

    在许多接触丰富的任务中，力传感在适应操纵对象的物理特性方面起着至关重要的作用。为了使机器人能够捕捉到学习的操纵任务泛化到未知对象所需的对象属性的基本分布，现有的示范学习方法需要大量昂贵的人类示范。我们提出的半监督示范学习方法将学习模型分解为触觉表示编码器和动作生成解码器。这使我们能够使用大量无监督数据预先训练触觉表示编码器，同时使用少样本示范学习训练动作生成解码器，从人类学习技能的好处。我们在使用不同刚度和表面摩擦的海绵上验证了该方法在擦拭任务中的效果。我们的结果表明，预训练显著提高了示范学习模型识别物理特性和生成所需擦拭动作的能力。

    In many contact-rich tasks, force sensing plays an essential role in adapting the motion to the physical properties of the manipulated object. To enable robots to capture the underlying distribution of object properties necessary for generalising learnt manipulation tasks to unseen objects, existing Learning from Demonstration (LfD) approaches require a large number of costly human demonstrations. Our proposed semi-supervised LfD approach decouples the learnt model into an haptic representation encoder and a motion generation decoder. This enables us to pre-train the first using large amount of unsupervised data, easily accessible, while using few-shot LfD to train the second, leveraging the benefits of learning skills from humans. We validate the approach on the wiping task using sponges with different stiffness and surface friction. Our results demonstrate that pre-training significantly improves the ability of the LfD model to recognise physical properties and generate desired wipin
    
[^45]: 使用可伸缩分类器的概率安全区域

    Probabilistic Safety Regions Via Finite Families of Scalable Classifiers. (arXiv:2309.04627v1 [stat.ML])

    [http://arxiv.org/abs/2309.04627](http://arxiv.org/abs/2309.04627)

    本论文提出了概率安全区域的概念，用于描述一个输入空间子集，在这个子集中，误分类实例的数量可以被概率上得到控制。同时，利用可伸缩分类器来将机器学习的调参与误差控制相结合。

    

    监督分类可以识别数据中的模式以分离不同的行为类别。然而，机器学习的数值逼近性质决定了分类算法上的误差问题。数据分析师可能会通过减小某个类别的错误来增加其他类别的错误。然而，这种设计阶段的误差控制通常以启发式的方式进行。因此，有必要发展一种理论基础，能够对获得的分类器进行概率证明。在这个视角下，我们引入了概率安全区域的概念，用来描述一个输入空间子集，其中误分类实例的数量可以概率上得到控制。然后，我们利用可伸缩分类器来将机器学习的调参与误差控制相结合。通过合成数据提供了多种测试来验证该方法，以突出所有步骤。

    Supervised classification recognizes patterns in the data to separate classes of behaviours. Canonical solutions contain misclassification errors that are intrinsic to the numerical approximating nature of machine learning. The data analyst may minimize the classification error on a class at the expense of increasing the error of the other classes. The error control of such a design phase is often done in a heuristic manner. In this context, it is key to develop theoretical foundations capable of providing probabilistic certifications to the obtained classifiers. In this perspective, we introduce the concept of probabilistic safety region to describe a subset of the input space in which the number of misclassified instances is probabilistically controlled. The notion of scalable classifiers is then exploited to link the tuning of machine learning with error control. Several tests corroborate the approach. They are provided through synthetic data in order to highlight all the steps invo
    
[^46]: 低秩度度量学习中的感知调整查询和反向测量范式

    Perceptual adjustment queries and an inverted measurement paradigm for low-rank metric learning. (arXiv:2309.04626v1 [stat.ML])

    [http://arxiv.org/abs/2309.04626](http://arxiv.org/abs/2309.04626)

    感知调整查询（PAQ）是一种新的用于收集人类反馈的查询机制，采用反向测量方案，结合了基数查询和序数查询的优点。我们将PAQ应用于度量学习问题中，通过PAQ测量来学习未知的马氏距离，并开发了一个两阶段估计器，提供了样本复杂性保证。

    

    我们引入了一种新的用于收集人类反馈的查询机制，称为感知调整查询（PAQ）。PAQ采用了反向测量方案，既具有信息量又轻量级，结合了基数查询和序数查询的优点。我们将PAQ展示在度量学习问题中，利用PAQ测量来学习未知的马氏距离。这导致了一个高维低秩矩阵估计问题，无法应用标准矩阵估计器。因此，我们开发了一个从PAQ中学习度量的两阶段估计器，并提供了该估计器的样本复杂性保证。我们通过数值模拟展示了该估计器的性能和显著特性。

    We introduce a new type of query mechanism for collecting human feedback, called the perceptual adjustment query ( PAQ). Being both informative and cognitively lightweight, the PAQ adopts an inverted measurement scheme, and combines advantages from both cardinal and ordinal queries. We showcase the PAQ in the metric learning problem, where we collect PAQ measurements to learn an unknown Mahalanobis distance. This gives rise to a high-dimensional, low-rank matrix estimation problem to which standard matrix estimators cannot be applied. Consequently, we develop a two-stage estimator for metric learning from PAQs, and provide sample complexity guarantees for this estimator. We present numerical simulations demonstrating the performance of the estimator and its notable properties.
    
[^47]: 基于知识蒸馏的数字孪生模型在异常检测中的应用

    Knowledge Distillation-Empowered Digital Twin for Anomaly Detection. (arXiv:2309.04616v1 [cs.LG])

    [http://arxiv.org/abs/2309.04616](http://arxiv.org/abs/2309.04616)

    本文提出了基于知识蒸馏的数字孪生模型KDDT，用于列车控制和管理系统（TCMS）的异常检测。KDDT利用语言模型和长短期记忆网络分别提取上下文和时间顺序特征，通过知识蒸馏丰富数据量。实验结果表明KDDT在两个数据集上取得了较高的F1分数0.931和0.91。

    

    物联网系统，在关键基础设施中越来越普及，如列车控制和管理系统（TCMS）。作为安全关键系统，确保其在操作过程中的可靠性至关重要。数字孪生（DTs）由于具有运行时监控和警告、异常预测和检测等能力而越来越受到研究关注。然而，在TCMS中构建用于异常检测的数字孪生模型需要充足的训练数据，并提取具有高质量的时间顺序和上下文特征。因此，在本文中，我们提出了一种名为KDDT的新方法，用于TCMS的异常检测。KDDT利用语言模型（LM）和长短期记忆（LSTM）网络分别提取上下文和时间顺序特征。为了增加数据量，KDDT利用知识蒸馏（KD）的方法来利用领域外数据。我们使用我们的工业合作伙伴阿尔斯通的两个数据集对KDDT进行了评估，并获得了0.931和0.91的F1分数。

    Cyber-physical systems (CPSs), like train control and management systems (TCMS), are becoming ubiquitous in critical infrastructures. As safety-critical systems, ensuring their dependability during operation is crucial. Digital twins (DTs) have been increasingly studied for this purpose owing to their capability of runtime monitoring and warning, prediction and detection of anomalies, etc. However, constructing a DT for anomaly detection in TCMS necessitates sufficient training data and extracting both chronological and context features with high quality. Hence, in this paper, we propose a novel method named KDDT for TCMS anomaly detection. KDDT harnesses a language model (LM) and a long short-term memory (LSTM) network to extract contexts and chronological features, respectively. To enrich data volume, KDDT benefits from out-of-domain data with knowledge distillation (KD). We evaluated KDDT with two datasets from our industry partner Alstom and obtained the F1 scores of 0.931 and 0.91
    
[^48]: 利用世界模型分解在基于值的多智能体强化学习中的应用

    Leveraging World Model Disentanglement in Value-Based Multi-Agent Reinforcement Learning. (arXiv:2309.04615v1 [cs.LG])

    [http://arxiv.org/abs/2309.04615](http://arxiv.org/abs/2309.04615)

    本文提出了一种新的基于模型的多智能体强化学习方法，通过使用模块化的世界模型，减少了多智能体系统中训练的样本复杂性，并成功预测了联合动作价值函数。

    

    本文提出了一种新颖的基于模型的多智能体强化学习方法，名为Value Decomposition Framework with Disentangled World Model，旨在解决在相同环境中多个智能体达成共同目标时的样本复杂性问题。由于多智能体系统的可扩展性和非平稳性问题，无模型方法依赖于大量样本进行训练。相反地，我们使用模块化的世界模型，包括动作条件、无动作和静态分支，来解开环境动态并根据过去的经验产生想象中的结果，而不是直接从真实环境中采样。我们使用变分自动编码器和变分图自动编码器来学习世界模型的潜在表示，将其与基于值的框架合并，以预测联合动作价值函数并优化整体训练目标。我们提供实验结果。

    In this paper, we propose a novel model-based multi-agent reinforcement learning approach named Value Decomposition Framework with Disentangled World Model to address the challenge of achieving a common goal of multiple agents interacting in the same environment with reduced sample complexity. Due to scalability and non-stationarity problems posed by multi-agent systems, model-free methods rely on a considerable number of samples for training. In contrast, we use a modularized world model, composed of action-conditioned, action-free, and static branches, to unravel the environment dynamics and produce imagined outcomes based on past experience, without sampling directly from the real environment. We employ variational auto-encoders and variational graph auto-encoders to learn the latent representations for the world model, which is merged with a value-based framework to predict the joint action-value function and optimize the overall training objective. We present experimental results 
    
[^49]: 通过分类哈希表示和分层强化交叉进行自优化特征生成

    Self-optimizing Feature Generation via Categorical Hashing Representation and Hierarchical Reinforcement Crossing. (arXiv:2309.04612v1 [cs.LG])

    [http://arxiv.org/abs/2309.04612](http://arxiv.org/abs/2309.04612)

    通过分类哈希表示和分层强化交叉，提出了一种自优化特征生成的通用框架，解决了现有系统中的一些挑战，包括有意义、稳健和高效的生成。

    

    特征生成旨在生成新的有意义的特征，以创建一个具有区分度的表示空间。当生成的特征来自具有固有特征交互的特征对时，生成的特征是有意义的。在现实世界中，经验丰富的数据科学家可以识别出潜在有用的特征-特征交互，并从指数级的搜索空间中生成有意义的维度，在最优的生成路径上以最优的交叉形式。但是，机器的人类化能力有限。我们将这类学习任务概括为自优化特征生成。自优化特征生成对现有系统提出了几个未解决的挑战：有意义、稳健和高效的生成。为了解决这些挑战，我们提出了一种基于原则和通用的表示交叉框架来解决自优化特征生成。为了实现哈希表示，我们提出了一个三步法：特征离散化、特征哈希和...

    Feature generation aims to generate new and meaningful features to create a discriminative representation space.A generated feature is meaningful when the generated feature is from a feature pair with inherent feature interaction. In the real world, experienced data scientists can identify potentially useful feature-feature interactions, and generate meaningful dimensions from an exponentially large search space, in an optimal crossing form over an optimal generation path. But, machines have limited human-like abilities.We generalize such learning tasks as self-optimizing feature generation. Self-optimizing feature generation imposes several under-addressed challenges on existing systems: meaningful, robust, and efficient generation. To tackle these challenges, we propose a principled and generic representation-crossing framework to solve self-optimizing feature generation.To achieve hashing representation, we propose a three-step approach: feature discretization, feature hashing, and 
    
[^50]: 面向分子图预训练的模式感知属性屏蔽

    Motif-aware Attribute Masking for Molecular Graph Pre-training. (arXiv:2309.04589v1 [cs.LG])

    [http://arxiv.org/abs/2309.04589](http://arxiv.org/abs/2309.04589)

    本研究提出并研究了一种模式感知的属性屏蔽策略，通过利用相邻模式中的原子信息来捕捉模式间的结构，从而提高分子图预训练的效果。

    

    在图神经网络的预训练中，属性重构用于预测节点或边的特征。通过给定大量的分子，它们学习捕捉结构知识，这对于各种下游属性预测任务在化学、生物医学和材料科学中至关重要。先前的策略是随机选择节点进行属性屏蔽，利用局部邻居的信息。然而，对这些邻居的过度依赖抑制了模型从更高级的亚结构中学习。例如，模型从预测苯环中的三个碳原子中学到的信息很少，但是可以从功能基团之间的相互连接中学到更多信息，也可以称为化学模式。在这项工作中，我们提出并研究了模式感知的属性屏蔽策略，通过利用相邻模式中的原子信息来捕捉模式间的结构。一旦每个图被分解为不相交的

    Attribute reconstruction is used to predict node or edge features in the pre-training of graph neural networks. Given a large number of molecules, they learn to capture structural knowledge, which is transferable for various downstream property prediction tasks and vital in chemistry, biomedicine, and material science. Previous strategies that randomly select nodes to do attribute masking leverage the information of local neighbors However, the over-reliance of these neighbors inhibits the model's ability to learn from higher-level substructures. For example, the model would learn little from predicting three carbon atoms in a benzene ring based on the other three but could learn more from the inter-connections between the functional groups, or called chemical motifs. In this work, we propose and investigate motif-aware attribute masking strategies to capture inter-motif structures by leveraging the information of atoms in neighboring motifs. Once each graph is decomposed into disjoint
    
[^51]: 动态网格感知辐射场

    Dynamic Mesh-Aware Radiance Fields. (arXiv:2309.04581v1 [cs.GR])

    [http://arxiv.org/abs/2309.04581](http://arxiv.org/abs/2309.04581)

    本文介绍了一种将多边形网格与神经辐射场 (NeRF) 相互耦合的方法，以实现在渲染和模拟过程中对网格进行物理一致性处理。训练了使用高动态范围图像的 NeRF，并提出了估计光源和投射阴影的策略。同时考虑了如何高效地集成混合表面-体积表达式与图形渲染框架。

    

    本文探讨了在将多边形网格资产嵌入到逼真的神经辐射场 (NeRF) 体积中，以便以与 NeRF 物理一致的方式进行渲染和动力学模拟的系统视角。设计了在渲染和模拟过程中网格与 NeRF 之间的双向耦合。首先回顾了网格和 NeRF 的光传输方程，然后将它们总结为一种沿着任意反射次数的光线更新辐射和透过率的高效算法。为了解决路径追踪器所假设的线性色彩空间和标准 NeRF 使用的 sRGB 色彩空间之间的差异，我们使用高动态范围 (HDR) 图像训练了 NeRF。还提出了一种估计 NeRF 上的光源并投射阴影的策略。最后，考虑了如何将混合表面-体积表达式与高性能的图形渲染框架高效集成。

    Embedding polygonal mesh assets within photorealistic Neural Radience Fields (NeRF) volumes, such that they can be rendered and their dynamics simulated in a physically consistent manner with the NeRF, is under-explored from the system perspective of integrating NeRF into the traditional graphics pipeline. This paper designs a two-way coupling between mesh and NeRF during rendering and simulation. We first review the light transport equations for both mesh and NeRF, then distill them into an efficient algorithm for updating radiance and throughput along a cast ray with an arbitrary number of bounces. To resolve the discrepancy between the linear color space that the path tracer assumes and the sRGB color space that standard NeRF uses, we train NeRF with High Dynamic Range (HDR) images. We also present a strategy to estimate light sources and cast shadows on the NeRF. Finally, we consider how the hybrid surface-volumetric formulation can be efficiently integrated with a high-performance
    
[^52]: 解放图学习的力量：基于LLM的自主代理机制

    Unleashing the Power of Graph Learning through LLM-based Autonomous Agents. (arXiv:2309.04565v1 [cs.LG])

    [http://arxiv.org/abs/2309.04565](http://arxiv.org/abs/2309.04565)

    本文提出了一种使用大型语言模型（LLMs）作为自主代理的方法，以简化多样化的现实世界图中的学习过程，并克服了现有方法中的限制。

    

    图结构化数据在现实世界中广泛存在和应用，但有效地处理这些多样化的数据和在图上进行学习任务是一项挑战。面对复杂的图学习任务，专家们在近年来设计了各种图神经网络（GNN）。他们还实施了图中的自动机器学习，也称为AutoGraph，以自动生成数据特定的解决方案。尽管取得了成功，但他们在以下方面存在限制：（1）在不同层级上管理各种学习任务，（2）处理图学习中不同的流程（超过架构设计），以及（3）使用AutoGraph时对先验知识的巨大需求。本文中，我们提出使用大型语言模型（LLMs）作为自主代理来简化多样化的现实世界图中的学习过程。具体来说，针对用户请求（该请求可能包含节点、边缘或图级别的不同数据和学习目标），复杂图中的学习过程将由LLM自主代理机制来处理。

    Graph structured data are widely existed and applied in the real-world applications, while it is a challenge to handling these diverse data and learning tasks on graph in an efficient manner. When facing the complicated graph learning tasks, experts have designed diverse Graph Neural Networks (GNNs) in recent years. They have also implemented AutoML in Graph, also known as AutoGraph, to automatically generate data-specific solutions. Despite their success, they encounter limitations in (1) managing diverse learning tasks at various levels, (2) dealing with different procedures in graph learning beyond architecture design, and (3) the huge requirements on the prior knowledge when using AutoGraph. In this paper, we propose to use Large Language Models (LLMs) as autonomous agents to simplify the learning process on diverse real-world graphs. Specifically, in response to a user request which may contain varying data and learning targets at the node, edge, or graph levels, the complex graph
    
[^53]: 当少就意味着更多：探究数据修剪对大规模预训练语言模型( LLMS )的影响

    When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale. (arXiv:2309.04564v1 [cs.CL])

    [http://arxiv.org/abs/2309.04564](http://arxiv.org/abs/2309.04564)

    在这项工作中，研究人员探究了数据修剪对大规模预训练语言模型(LLMs)的影响。通过比较数据质量评估器和修剪预训练语料库后训练的LLMs，他们发现困惑度作为一种简单的技术优于更加计算密集的评分方法。

    

    最近几年，大量的文本数据对于大型语言模型( LLMS )的发展做出了显著贡献。这些数据通常通过从互联网上抓取获取，导致预训练数据集由嘈杂的网络文本构成。过去，为了减小数据集并使其更高质量，采用了以规则为基础的手工启发式过滤器。在这项工作中，我们采取更广泛的视角，探讨了可估算的数据质量，以系统性地衡量预训练数据的质量。我们在大规模上进行了严格比较，包括使用困惑度的简单数据质量评估器，以及更复杂和计算密集的错误L2-范数和记忆化评估。这些指标用于对预训练语料库进行排序和修剪，并随后比较在这些修剪后数据集上训练的LLMs。令人惊讶的是，我们发现困惑度作为一种简单的技术优于更加计算密集的评分方法。

    Large volumes of text data have contributed significantly to the development of large language models (LLMs) in recent years. This data is typically acquired by scraping the internet, leading to pretraining datasets comprised of noisy web text. To date, efforts to prune these datasets down to a higher quality subset have relied on hand-crafted heuristics encoded as rule-based filters. In this work, we take a wider view and explore scalable estimates of data quality that can be used to systematically measure the quality of pretraining data. We perform a rigorous comparison at scale of the simple data quality estimator of perplexity, as well as more sophisticated and computationally intensive estimates of the Error L2-Norm and memorization. These metrics are used to rank and prune pretraining corpora, and we subsequently compare LLMs trained on these pruned datasets. Surprisingly, we find that the simple technique of perplexity outperforms our more computationally expensive scoring metho
    
[^54]: 用基于注意力的深度神经网络实现可解释的太阳耀斑预测

    Towards Interpretable Solar Flare Prediction with Attention-based Deep Neural Networks. (arXiv:2309.04558v1 [cs.LG])

    [http://arxiv.org/abs/2309.04558](http://arxiv.org/abs/2309.04558)

    用基于注意力的深度学习模型改进了太阳耀斑预测，成功预测了≥M1.0级太阳耀斑的发生，并通过注意力图可解释模型的决策。

    

    太阳耀斑的预测是空间天气预报中的一个核心问题，最近机器学习和深度学习的发展加速了基于数据驱动的复杂模型在太阳耀斑预测中的应用。本文中，我们开发了一种基于注意力的深度学习模型，作为传统卷积神经网络（CNN）流水线的改进，用于预测在接下来的24小时内是否会出现≥M1.0级太阳耀斑。对于这个任务，我们收集了由全盘瞄线（LoS）磁图创建的压缩图像。我们使用数据增强的过采样来解决类别不平衡问题，并使用真实技能统计（TSS）和Heidke技能评分（HSS）作为评估指标。此外，我们通过在输入磁图上叠加注意力图来解释我们的模型，并可视化模型关注的重要区域，从而导致最终的决策。本研究的重要发现有：（i）我们成功地使用注意力机制的深度学习模型来预测太阳耀斑的发生。

    Solar flare prediction is a central problem in space weather forecasting and recent developments in machine learning and deep learning accelerated the adoption of complex models for data-driven solar flare forecasting. In this work, we developed an attention-based deep learning model as an improvement over the standard convolutional neural network (CNN) pipeline to perform full-disk binary flare predictions for the occurrence of $\geq$M1.0-class flares within the next 24 hours. For this task, we collected compressed images created from full-disk line-of-sight (LoS) magnetograms. We used data-augmented oversampling to address the class imbalance issue and used true skill statistic (TSS) and Heidke skill score (HSS) as the evaluation metrics. Furthermore, we interpreted our model by overlaying attention maps on input magnetograms and visualized the important regions focused on by the model that led to the eventual decision. The significant findings of this study are: (i) We successfully 
    
[^55]: 用于核回归的遗憾最优联邦迁移学习及其在美式期权定价中的应用

    Regret-Optimal Federated Transfer Learning for Kernel Regression with Applications in American Option Pricing. (arXiv:2309.04557v1 [cs.LG])

    [http://arxiv.org/abs/2309.04557](http://arxiv.org/abs/2309.04557)

    本论文提出了一种遗憾最优算法的迭代方案，用于联邦迁移学习，在核回归模型中具体化，并提出了一个几乎遗憾最优的启发式算法，可以减小生成参数与专门参数之间的累积偏差。

    

    我们提出了一种最优的迭代方案，用于联邦迁移学习，其中中心计划者可以访问同一学习模型 $f_{\theta}$ 的数据集 ${\cal D}_1,\dots,{\cal D}_N$。我们的目标是在尊重模型 $f_{\theta(T)}$ 的损失函数的情况下，尽量减小生成参数 $\{\theta_i(t)\}_{t=0}^T$ 在所有 $T$ 次迭代中与每个数据集得到的专门参数$\theta^\star_{1},\ldots,\theta^\star_N$ 的累积偏差。我们仅允许每个专门模型（节点/代理）和中心计划者（服务器）在每次迭代（轮）之间进行持续通信。对于模型 $f_{\theta}$ 是有限秩核回归的情况，我们得出了遗憾最优算法的显式更新公式。通过利用遗憾最优算法中的对称性，我们进一步开发了一种几乎遗憾最优的启发式算法，其运行需要较少的 $\mathcal{O}(Np^2)$ 个基本运算。

    We propose an optimal iterative scheme for federated transfer learning, where a central planner has access to datasets ${\cal D}_1,\dots,{\cal D}_N$ for the same learning model $f_{\theta}$. Our objective is to minimize the cumulative deviation of the generated parameters $\{\theta_i(t)\}_{t=0}^T$ across all $T$ iterations from the specialized parameters $\theta^\star_{1},\ldots,\theta^\star_N$ obtained for each dataset, while respecting the loss function for the model $f_{\theta(T)}$ produced by the algorithm upon halting. We only allow for continual communication between each of the specialized models (nodes/agents) and the central planner (server), at each iteration (round). For the case where the model $f_{\theta}$ is a finite-rank kernel regression, we derive explicit updates for the regret-optimal algorithm. By leveraging symmetries within the regret-optimal algorithm, we further develop a nearly regret-optimal heuristic that runs with $\mathcal{O}(Np^2)$ fewer elementary operati
    
[^56]: 连接NTK和NNGP：神经网络学习动力学在核区域的统一理论框架

    Connecting NTK and NNGP: A Unified Theoretical Framework for Neural Network Learning Dynamics in the Kernel Regime. (arXiv:2309.04522v1 [cs.LG])

    [http://arxiv.org/abs/2309.04522](http://arxiv.org/abs/2309.04522)

    本文提出了一个马尔可夫近似学习模型，统一了神经切向核（NTK）和神经网络高斯过程（NNGP）核，用于描述无限宽度深层网络的学习动力学。

    

    人工神经网络近年来在机器学习领域取得了革命性的进展，但其学习过程缺乏一个完整的理论框架。对于无限宽度网络，已经取得了重大进展。在这个范式中，使用了两种不同的理论框架来描述网络的输出：一种基于神经切向核（NTK）的框架，假设了线性化的梯度下降动力学；另一种是基于神经网络高斯过程（NNGP）核的贝叶斯框架。然而，这两种框架之间的关系一直不明确。本文通过一个马尔可夫近似学习模型，统一了这两种不同的理论，用于描述随机初始化的无限宽度深层网络的学习动力学。我们推导出了在学习过程中和学习后的网络输入-输出函数的精确分析表达式，并引入了一个新的时间相关的神经动态核（NDK），这个核可以同时产生NTK和NNGP。

    Artificial neural networks have revolutionized machine learning in recent years, but a complete theoretical framework for their learning process is still lacking. Substantial progress has been made for infinitely wide networks. In this regime, two disparate theoretical frameworks have been used, in which the network's output is described using kernels: one framework is based on the Neural Tangent Kernel (NTK) which assumes linearized gradient descent dynamics, while the Neural Network Gaussian Process (NNGP) kernel assumes a Bayesian framework. However, the relation between these two frameworks has remained elusive. This work unifies these two distinct theories using a Markov proximal learning model for learning dynamics in an ensemble of randomly initialized infinitely wide deep networks. We derive an exact analytical expression for the network input-output function during and after learning, and introduce a new time-dependent Neural Dynamical Kernel (NDK) from which both NTK and NNGP
    
[^57]: 利用声学语言模型预训练的端到端语音识别和消除语言迟滞

    End-to-End Speech Recognition and Disfluency Removal with Acoustic Language Model Pretraining. (arXiv:2309.04516v1 [eess.AS])

    [http://arxiv.org/abs/2309.04516](http://arxiv.org/abs/2309.04516)

    该论文研究了利用声学语言模型预训练的端到端语音识别和消除语言迟滞的方法，发现基于音频的语言模型使用自监督目标进行预训练可以与两阶段模型的性能相匹配甚至超越。

    

    在处理不连贯和对话语音的转录中，近年来，分为两个阶段的模型，在转录和清理阶段之间进行分离。我们认为之前的端到端消除语言迟滞的尝试不够成功，是因为大规模语言模型预训练给予了词汇模型表示优势。直到最近，大规模自监督预训练的目标对于学习有效音频表示的高维度和有限可用的大型音频数据集的发展受到限制，从而给两阶段方法带来了相对优势，该方法利用预训练表示进行词汇标记。基于最近大规模音频预训练的成功，我们重新考虑了两阶段和端到端模型之间的性能比较，并发现基于音频的语言模型使用弱自监督目标进行预训练的性能与相似训练过的两阶段模型的性能相匹配甚至超越。

    The SOTA in transcription of disfluent and conversational speech has in recent years favored two-stage models, with separate transcription and cleaning stages. We believe that previous attempts at end-to-end disfluency removal have fallen short because of the representational advantage that large-scale language model pretraining has given to lexical models. Until recently, the high dimensionality and limited availability of large audio datasets inhibited the development of large-scale self-supervised pretraining objectives for learning effective audio representations, giving a relative advantage to the two-stage approach, which utilises pretrained representations for lexical tokens. In light of recent successes in large scale audio pretraining, we revisit the performance comparison between two-stage and end-to-end model and find that audio based language models pretrained using weak self-supervised objectives match or exceed the performance of similarly trained two-stage models, and fu
    
[^58]: 使用卷积变分瓶颈的隐私保护联合学习

    Privacy Preserving Federated Learning with Convolutional Variational Bottlenecks. (arXiv:2309.04515v1 [cs.LG])

    [http://arxiv.org/abs/2309.04515](http://arxiv.org/abs/2309.04515)

    本文研究了使用卷积变分瓶颈的隐私保护联合学习，并发现了PRECODE的工作原理和影响。PRECODE通过引入随机性梯度防止梯度反转攻击的收敛，但也提出了一种攻击方法来禁用其隐私保护效果。

    

    梯度反转攻击是联合学习中普遍存在的威胁，它通过利用梯度泄漏来重构本应是私密的训练数据。最近的工作提出了一种基于变分建模的隐私增强模块（PRECODE），以防止梯度泄漏而不损失模型的效用。但在没有进一步的分析之前，已经证明PRECODE成功防止了梯度反转攻击。在本文中，我们做出了多个贡献。首先，我们研究了PRECODE对梯度反转攻击的影响，以揭示其基本的工作原理。我们证明了变分建模将随机性引入了PRECODE和神经网络中后续层的梯度中。这些层的随机梯度阻止了迭代梯度反转攻击的收敛。其次，我们提出了一种攻击方法，通过有意地在攻击优化过程中省略随机梯度，来禁用PRECODE的隐私保护效果。

    Gradient inversion attacks are an ubiquitous threat in federated learning as they exploit gradient leakage to reconstruct supposedly private training data. Recent work has proposed to prevent gradient leakage without loss of model utility by incorporating a PRivacy EnhanCing mODulE (PRECODE) based on variational modeling. Without further analysis, it was shown that PRECODE successfully protects against gradient inversion attacks. In this paper, we make multiple contributions. First, we investigate the effect of PRECODE on gradient inversion attacks to reveal its underlying working principle. We show that variational modeling introduces stochasticity into the gradients of PRECODE and the subsequent layers in a neural network. The stochastic gradients of these layers prevent iterative gradient inversion attacks from converging. Second, we formulate an attack that disables the privacy preserving effect of PRECODE by purposefully omitting stochastic gradients during attack optimization. To
    
[^59]: 减少贝叶斯优化的计算时间：通用的记忆修剪方法

    Decreasing the Computing Time of Bayesian Optimization using Generalizable Memory Pruning. (arXiv:2309.04510v1 [cs.LG])

    [http://arxiv.org/abs/2309.04510](http://arxiv.org/abs/2309.04510)

    该论文提出了一个通用的贝叶斯优化方法，通过记忆修剪和有界优化，可以降低计算时间，并适用于任何代理模型和获取函数。

    

    在处理高维度或大规模数据集时，贝叶斯优化（BO）所需的计算时间较长。这是由于高斯过程代理模型与实验数量具有多项式时间复杂度导致的。由于计算时间的这种复杂度扩展，运行BO在高维度或大规模数据集上变得困难，从而阻碍了实验。已经开发了替代的代理模型来降低BO过程的计算利用率，然而，这些方法需要对继承的代理函数进行数学修改，限制了只能使用该函数。在本文中，我们展示了一个通用的BO包装器——记忆修剪和有界优化，能够与任何代理模型和获取函数一起使用。通过使用这种记忆修剪方法，我们展示了BO每个实验的墙钟计算时间从多项式增加模式下降到锯齿模式。

    Bayesian optimization (BO) suffers from long computing times when processing highly-dimensional or large data sets. These long computing times are a result of the Gaussian process surrogate model having a polynomial time complexity with the number of experiments. Running BO on high-dimensional or massive data sets becomes intractable due to this time complexity scaling, in turn, hindering experimentation. Alternative surrogate models have been developed to reduce the computing utilization of the BO procedure, however, these methods require mathematical alteration of the inherit surrogate function, pigeonholing use into only that function. In this paper, we demonstrate a generalizable BO wrapper of memory pruning and bounded optimization, capable of being used with any surrogate model and acquisition function. Using this memory pruning approach, we show a decrease in wall-clock computing times per experiment of BO from a polynomially increasing pattern to a sawtooth pattern that has a n
    
[^60]: IoT空气污染监测系统中的时空图注意力融合器用于校准

    Spatial-Temporal Graph Attention Fuser for Calibration in IoT Air Pollution Monitoring Systems. (arXiv:2309.04508v1 [cs.LG])

    [http://arxiv.org/abs/2309.04508](http://arxiv.org/abs/2309.04508)

    这项研究提出了一种新颖的方法，利用图神经网络中的图注意力网络模块，通过融合传感器阵列的数据来增强IoT空气污染监测平台中传感器的校准精度。

    

    随着物联网(IoT)传感器在空气污染监测中的广泛应用，低成本传感器的部署大幅增加。尽管这一进展，准确校准这些传感器在不受控制的环境条件下仍然是一个挑战。为了解决这个问题，我们提出了一种新颖的方法，利用图神经网络，特别是图注意力网络模块，通过融合传感器阵列的数据来增强校准过程。通过我们的实验，我们展示了我们的方法在显著提高IoT空气污染监测平台中传感器的校准精度方面的有效性。

    The use of Internet of Things (IoT) sensors for air pollution monitoring has significantly increased, resulting in the deployment of low-cost sensors. Despite this advancement, accurately calibrating these sensors in uncontrolled environmental conditions remains a challenge. To address this, we propose a novel approach that leverages graph neural networks, specifically the graph attention network module, to enhance the calibration process by fusing data from sensor arrays. Through our experiments, we demonstrate the effectiveness of our approach in significantly improving the calibration accuracy of sensors in IoT air pollution monitoring platforms.
    
[^61]: 使用路径签名生成逼近实际数据的金融价格路径

    Generating drawdown-realistic financial price paths using path signatures. (arXiv:2309.04507v1 [q-fin.CP])

    [http://arxiv.org/abs/2309.04507](http://arxiv.org/abs/2309.04507)

    本文提出了一种新颖的机器学习方法，使用路径签名来生成逼近实际数据的金融价格路径，并应用于定价回撤保险期权和投资组合回撤控制策略。

    

    本文提出了一种新颖的生成式机器学习方法，用于模拟具有接近实际数据的回撤的金融价格序列。应用场景如定价回撤保险期权或开发投资组合回撤控制策略需要使用接近真实的回撤路径。历史情景可能不足以有效训练和回测策略，而标准的参数化蒙特卡罗方法无法充分保留回撤。我们提倡一种非参数化蒙特卡罗方法，将变分自编码生成模型与回撤重建损失函数相结合。为了克服数值复杂性和非可微性问题，我们将回撤近似为路径的矩函数，即路径签名。我们证明了回撤函数的所需正则性和近似的一致性。此外，我们使用线性回归获得了接近的数值近似解。

    A novel generative machine learning approach for the simulation of sequences of financial price data with drawdowns quantifiably close to empirical data is introduced. Applications such as pricing drawdown insurance options or developing portfolio drawdown control strategies call for a host of drawdown-realistic paths. Historical scenarios may be insufficient to effectively train and backtest the strategy, while standard parametric Monte Carlo does not adequately preserve drawdowns. We advocate a non-parametric Monte Carlo approach combining a variational autoencoder generative model with a drawdown reconstruction loss function. To overcome issues of numerical complexity and non-differentiability, we approximate drawdown as a linear function of the moments of the path, known in the literature as path signatures. We prove the required regularity of drawdown function and consistency of the approximation. Furthermore, we obtain close numerical approximations using linear regression for fr
    
[^62]: COVID-19检测系统：基于咳嗽音频信号的声学特征系统性能比较分析

    COVID-19 Detection System: A Comparative Analysis of System Performance Based on Acoustic Features of Cough Audio Signals. (arXiv:2309.04505v1 [cs.SD])

    [http://arxiv.org/abs/2309.04505](http://arxiv.org/abs/2309.04505)

    本研究通过比较分析声学特征对咳嗽音频信号的机器学习模型性能的影响，提出了一种高效的COVID-19检测系统。

    

    各种呼吸道疾病如感冒和流感、哮喘以及COVID-19等在全球范围内影响着人们的日常生活。在医学实践中，呼吸声音被广泛用于医疗服务中，用于诊断各种呼吸系统疾病和肺部疾病。传统的诊断方法需要专业知识，成本高且依赖于人类专业知识。最近，咳嗽音频记录被用来自动化检测呼吸系统疾病的过程。本研究旨在检查各种声学特征，以提高机器学习模型在咳嗽信号中检测COVID-19的性能。本研究调查了三种特征提取技术（MFCC，Chroma和Spectral Contrast特征）在两种机器学习算法（SVM和MLP）上的功效，并提出了一种高效的COVID-19检测系统。

    A wide range of respiratory diseases, such as cold and flu, asthma, and COVID-19, affect people's daily lives worldwide. In medical practice, respiratory sounds are widely used in medical services to diagnose various respiratory illnesses and lung disorders. The traditional diagnosis of such sounds requires specialized knowledge, which can be costly and reliant on human expertise. Recently, cough audio recordings have been used to automate the process of detecting respiratory conditions. This research aims to examine various acoustic features that enhance the performance of machine learning (ML) models in detecting COVID-19 from cough signals. This study investigates the efficacy of three feature extraction techniques, including Mel Frequency Cepstral Coefficients (MFCC), Chroma, and Spectral Contrast features, on two ML algorithms, Support Vector Machine (SVM) and Multilayer Perceptron (MLP), and thus proposes an efficient COVID-19 detection system. The proposed system produces a prac
    
[^63]: 使用强化学习进行基于视觉的概念组合学习

    Compositional Learning of Visually-Grounded Concepts Using Reinforcement. (arXiv:2309.04504v1 [cs.LG])

    [http://arxiv.org/abs/2309.04504](http://arxiv.org/abs/2309.04504)

    本研究探讨了深度强化学习代理如何学习和组合基于颜色和形状的组合指令，以解决空间导航任务中的新颖组合。通过利用冻结的文本编码器，代理所需的训练回合数减少了20倍。

    

    深度强化学习代理需要通过数百万个回合的训练才能较好地解决与指令相关的导航任务，并且它们是否能够推广到新颖的指令组合的能力尚不清楚。有趣的是，儿童可以分解基于语言的指令并导航到指定的物体，即使他们之前没有见过这些查询的组合。因此，我们创建了三个3D环境，研究深度强化学习代理如何学习和组合基于颜色和形状的组合指令，以解决空间导航任务中的新颖组合。首先，我们探讨代理是否能够进行组合学习，并且它们是否可以利用冻结的文本编码器（例如CLIP、BERT）在更少的回合中学习单词组合。接下来，我们证明当代理在形状或颜色概念上进行预训练时，它们所需的训练回合数减少了20倍，可以解决未见过的指令组合。最后，我们展示了...

    Deep reinforcement learning agents need to be trained over millions of episodes to decently solve navigation tasks grounded to instructions. Furthermore, their ability to generalize to novel combinations of instructions is unclear. Interestingly however, children can decompose language-based instructions and navigate to the referred object, even if they have not seen the combination of queries prior. Hence, we created three 3D environments to investigate how deep RL agents learn and compose color-shape based combinatorial instructions to solve novel combinations in a spatial navigation task. First, we explore if agents can perform compositional learning, and whether they can leverage on frozen text encoders (e.g. CLIP, BERT) to learn word combinations in fewer episodes. Next, we demonstrate that when agents are pretrained on the shape or color concepts separately, they show a 20 times decrease in training episodes needed to solve unseen combinations of instructions. Lastly, we show tha
    
[^64]: 考虑几何特征和工程性能的加权无监督领域适应方法

    Weighted Unsupervised Domain Adaptation Considering Geometry Features and Engineering Performance of 3D Design Data. (arXiv:2309.04499v1 [cs.LG])

    [http://arxiv.org/abs/2309.04499](http://arxiv.org/abs/2309.04499)

    本论文提出了一种考虑几何特征和工程性能的加权无监督领域适应方法，专门用于基于深度学习的工程性能预测。通过对抗训练策略，可以提取领域不变特征。

    

    制造业中的产品设计过程涉及迭代的设计建模和分析，以实现目标工程性能，但这样的迭代过程耗时且计算成本高。最近，基于深度学习的工程性能预测模型被提出来加速设计优化。然而，它们只能保证对训练数据的预测，并且在应用于新的领域数据时可能不准确。特别是，3D设计数据具有复杂的特征，意味着存在具有不同分布的领域。因此，利用深度学习受限于大量的数据收集和训练负担。我们提出了一种考虑几何特征和工程性能的双加权无监督领域适应方法，专用于基于深度学习的工程性能预测。通过使用对抗训练策略，可以提取领域不变特征。

    The product design process in manufacturing involves iterative design modeling and analysis to achieve the target engineering performance, but such an iterative process is time consuming and computationally expensive. Recently, deep learning-based engineering performance prediction models have been proposed to accelerate design optimization. However, they only guarantee predictions on training data and may be inaccurate when applied to new domain data. In particular, 3D design data have complex features, which means domains with various distributions exist. Thus, the utilization of deep learning has limitations due to the heavy data collection and training burdens. We propose a bi-weighted unsupervised domain adaptation approach that considers the geometry features and engineering performance of 3D design data. It is specialized for deep learning-based engineering performance predictions. Domain-invariant features can be extracted through an adversarial training strategy by using hypot
    
[^65]: 针对非仿射控制系统的安全神经控制器与可微的控制屏障函数

    Safe Neural Control for Non-Affine Control Systems with Differentiable Control Barrier Functions. (arXiv:2309.04492v1 [cs.SY])

    [http://arxiv.org/abs/2309.04492](http://arxiv.org/abs/2309.04492)

    本文提出了一种安全神经控制器方法，通过将高阶控制屏障函数整合到神经网络中，解决了非仿射控制系统的安全关键控制问题，并改进了现有的解决方案的次优性能。

    

    本文解决了非仿射控制系统的安全关键控制问题。通过使用控制屏障函数（CBFs），已经证明优化二次成本在状态和控制约束下可以通过一系列二次规划（QPs）来进行次优化。我们最近提出的高阶CBFs（HOCBFs）可以容纳任意相对阶数的约束。该方法的主要挑战是需要仿射控制动态以及CBF-based QP的解决方案是点解法，因此是次优解。为了解决这些挑战，我们将高阶CBFs整合到基于神经常微分方程的学习模型中，作为可微的CBFs，以确保非仿射控制系统的安全性。可微的CBFs可以通过它们的参数进行训练，因此可以解决CBFs过度保守的问题，使系统状态不必要地远离安全边界。

    This paper addresses the problem of safety-critical control for non-affine control systems. It has been shown that optimizing quadratic costs subject to state and control constraints can be sub-optimally reduced to a sequence of quadratic programs (QPs) by using Control Barrier Functions (CBFs). Our recently proposed High Order CBFs (HOCBFs) can accommodate constraints of arbitrary relative degree. The main challenges in this approach are that it requires affine control dynamics and the solution of the CBF-based QP is sub-optimal since it is solved point-wise. To address these challenges, we incorporate higher-order CBFs into neural ordinary differential equation-based learning models as differentiable CBFs to guarantee safety for non-affine control systems. The differentiable CBFs are trainable in terms of their parameters, and thus, they can address the conservativeness of CBFs such that the system state will not stay unnecessarily far away from safe set boundaries. Moreover, the imi
    
[^66]: 解决材料性能预测中的精确度和成本权衡问题：一种师生策略

    Addressing the Accuracy-Cost Tradeoff in Material Property Prediction: A Teacher-Student Strategy. (arXiv:2309.04482v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2309.04482](http://arxiv.org/abs/2309.04482)

    本论文提出了一种师生策略，利用预训练的结构模型作为“老师”，以提高基于化学组分的材料性能预测模型的准确性。通过验证，该策略在不同网络结构上都能有效提高模型的准确性。

    

    深度学习已经彻底改变了新材料发现的过程，最先进的模型现在能够仅基于化学组分预测材料性能，从而消除了对材料结构的需求。然而，这种成本效益高的方法导致了模型精确度的权衡。具体而言，基于化学组分的材料性能预测模型（CPMs）的准确度显著落后于基于结构的材料性能预测模型（SPMs）。为了解决这个挑战，我们提出了一种创新的师生（T-S）策略，其中预训练的 SPM 充当“老师”，以提高 CPM 的准确性。利用师生策略，T-S CrabNet 已经成为目前 CPM 中最准确的模型。首先，我们证明了这种策略的普适性。在 Materials Project（MP）和 Jarvis 数据集上，我们验证了 T-S 策略在提高 CPM 准确性方面的有效性。

    Deep learning has revolutionized the process of new material discovery, with state-of-the-art models now able to predict material properties based solely on chemical compositions, thus eliminating the necessity for material structures. However, this cost-effective method has led to a trade-off in model accuracy. Specifically, the accuracy of Chemical Composition-based Property Prediction Models (CPMs) significantly lags behind that of Structure-based Property Prediction Models (SPMs). To tackle this challenge, we propose an innovative Teacher-Student (T-S) strategy, where a pre-trained SPM serves as the 'teacher' to enhance the accuracy of the CPM. Leveraging the T-S strategy, T-S CrabNet has risen to become the most accurate model among current CPMs. Initially, we demonstrated the universality of this strategy. On the Materials Project (MP) and Jarvis datasets, we validated the effectiveness of the T-S strategy in boosting the accuracy of CPMs with two distinct network structures, nam
    
[^67]: 多模态机器学习在材料科学中的应用：基于成分-结构双模态学习的实验测量性质

    Multimodal machine learning for materials science: composition-structure bimodal learning for experimentally measured properties. (arXiv:2309.04478v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2309.04478](http://arxiv.org/abs/2309.04478)

    本论文介绍了一种基于成分-结构双模态学习的多模态机器学习方法，用于增强对实验测量材料性质的学习和预测，从而降低预测误差。

    

    广泛应用的GPT-4等多模态机器学习模型已经在计算机视觉和自然语言处理等多个研究领域引起了革命。然而，在材料信息学领域，尽管存在各种模态的材料数据，比如成分和结构，但其实际应用还未充分探索。大规模计算数据集训练的机器学习模型的有效性依赖于计算的准确性，而实验数据集往往数据有限且信息不完整。本文提出了一种新颖的材料科学中的多模态机器学习方法，通过成分-结构双模态学习来增强对实验测量材料性质的学习和预测。所提出的COmposition-Structure双模态网络（COSNet）旨在减少对具有不完整结构信息的材料性质的预测误差。

    The widespread application of multimodal machine learning models like GPT-4 has revolutionized various research fields including computer vision and natural language processing. However, its implementation in materials informatics remains underexplored, despite the presence of materials data across diverse modalities, such as composition and structure. The effectiveness of machine learning models trained on large calculated datasets depends on the accuracy of calculations, while experimental datasets often have limited data availability and incomplete information. This paper introduces a novel approach to multimodal machine learning in materials science via composition-structure bimodal learning. The proposed COmposition-Structure Bimodal Network (COSNet) is designed to enhance learning and predictions of experimentally measured materials properties that have incomplete structure information. Bimodal learning significantly reduces prediction errors across distinct materials properties 
    
[^68]: 通过联合等变扩散进行晶体结构预测

    Crystal Structure Prediction by Joint Equivariant Diffusion. (arXiv:2309.04475v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2309.04475](http://arxiv.org/abs/2309.04475)

    本文提出了DiffCSP，一种利用周期-E(3)-等变去噪模型的扩散模型，用于通过联合生成晶体的晶格和原子坐标，以解决晶体结构预测中的对称性挑战。

    

    晶体结构预测在各个科学学科中至关重要。尽管目前流行的生成模型（如扩散模型）可以用于解决晶体结构预测的任务，但由于晶体结构的对称几何特性（平移、旋转和周期性的不变性），该任务面临着独特的挑战。为了融入以上对称性，本文提出了DiffCSP，一种新颖的扩散模型，通过采用周期-E(3)-等变去噪模型，联合生成每个晶体的晶格和原子坐标，以更好地建模晶体几何。值得注意的是，与相关的等变生成方法不同，DiffCSP利用分数坐标而不是笛卡尔坐标来表示晶体，显著提升了原子位置的扩散和生成过程。大量实验证明，我们的DiffCSP明显优于现有方法。

    Crystal Structure Prediction (CSP) is crucial in various scientific disciplines. While CSP can be addressed by employing currently-prevailing generative models (e.g. diffusion models), this task encounters unique challenges owing to the symmetric geometry of crystal structures -- the invariance of translation, rotation, and periodicity. To incorporate the above symmetries, this paper proposes DiffCSP, a novel diffusion model to learn the structure distribution from stable crystals. To be specific, DiffCSP jointly generates the lattice and atom coordinates for each crystal by employing a periodic-E(3)-equivariant denoising model, to better model the crystal geometry. Notably, different from related equivariant generative approaches, DiffCSP leverages fractional coordinates other than Cartesian coordinates to represent crystals, remarkably promoting the diffusion and the generation process of atom positions. Extensive experiments verify that our DiffCSP significantly outperforms existing
    
[^69]: 弱监督学习在串行飞秒晶体学中的模式分类中的应用

    Weakly supervised learning for pattern classification in serial femtosecond crystallography. (arXiv:2309.04474v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2309.04474](http://arxiv.org/abs/2309.04474)

    本文介绍了在串行飞秒晶体学中通过弱监督算法对衍射图进行分类的工作，旨在尽可能减少训练所需的标签数据集的规模。

    

    X射线自由电子激光设施上的串行飞秒晶体学为晶体结构的确定开辟了新的时代。然而，这些实验的数据处理面临前所未有的挑战，因为确定高分辨率结构所需的衍射图的总数巨大。机器学习方法在处理如此大量的数据方面可能起到重要作用。卷积神经网络在模式分类领域取得了巨大成功，然而，训练这些网络需要带有标签的大规模数据集。对标签数据集的强依赖严重限制了网络的应用，因为注释大量衍射图非常昂贵。本文介绍了我们在弱监督算法下对衍射图进行分类的工作，旨在尽可能减少训练所需的标签数据集的规模。

    Serial femtosecond crystallography at X-ray free electron laser facilities opens a new era for the determination of crystal structure. However, the data processing of those experiments is facing unprecedented challenge, because the total number of diffraction patterns needed to determinate a high-resolution structure is huge. Machine learning methods are very likely to play important roles in dealing with such a large volume of data. Convolutional neural networks have made a great success in the field of pattern classification, however, training of the networks need very large datasets with labels. Th is heavy dependence on labeled datasets will seriously restrict the application of networks, because it is very costly to annotate a large number of diffraction patterns. In this article we present our job on the classification of diffraction pattern by weakly supervised algorithms, with the aim of reducing as much as possible the size of the labeled dataset required for training. Our res
    
[^70]: 通过谱方法改进了排名聚合的理论保证

    Improved theoretical guarantee for rank aggregation via spectral method. (arXiv:2309.03808v1 [stat.ML])

    [http://arxiv.org/abs/2309.03808](http://arxiv.org/abs/2309.03808)

    本论文通过谱方法改进了排名聚合问题的理论保证，通过研究基于未归一化和归一化数据矩阵的谱排名算法，提供了更准确的扰动误差界限。

    

    给定多个项目之间的成对比较，如何对它们进行排名以使得排名与观察相匹配？这个问题被称为排名聚合，在体育、推荐系统和其他网络应用中已经找到了许多应用。由于找到最小化不匹配的全局排名通常是NP困难的（称为Kemeny优化），我们将重点放在Erd\"os-R\'enyi离群点（ERO）模型上。在这个排名问题中，每个成对比较是真实分数差异的被损坏副本。我们研究了基于未归一化和归一化数据矩阵的谱排名算法。关键是理解它们在从观察数据中恢复每个项目的潜在分数方面的性能。这归结为推导未归一化/归一化数据矩阵的前几个特征向量和其总体对应物之间的逐个项扰动误差界限。通过使用留出技术，我们提供了一个更准确的$\ell_{\infty}$-norm扰动误差界限。

    Given pairwise comparisons between multiple items, how to rank them so that the ranking matches the observations? This problem, known as rank aggregation, has found many applications in sports, recommendation systems, and other web applications. As it is generally NP-hard to find a global ranking that minimizes the mismatch (known as the Kemeny optimization), we focus on the Erd\"os-R\'enyi outliers (ERO) model for this ranking problem. Here, each pairwise comparison is a corrupted copy of the true score difference. We investigate spectral ranking algorithms that are based on unnormalized and normalized data matrices. The key is to understand their performance in recovering the underlying scores of each item from the observed data. This reduces to deriving an entry-wise perturbation error bound between the top eigenvectors of the unnormalized/normalized data matrix and its population counterpart. By using the leave-one-out technique, we provide a sharper $\ell_{\infty}$-norm perturbati
    
[^71]: 使用多目标遗传算法生成量子特征映射的方法

    Generating quantum feature maps using multi-objective genetic algorithm. (arXiv:2309.03307v1 [quant-ph])

    [http://arxiv.org/abs/2309.03307](http://arxiv.org/abs/2309.03307)

    本文介绍了一种使用多目标遗传算法生成量子特征映射的方法，以实现对高维希尔伯特空间的访问，并在优化电路配置时同时考虑分类准确性和门成本。实验结果显示最佳电路配置中纠缠门需要相应的数量，与之前研究相反。同时，我们还提出了使用数据的可分离性指数确定最佳配置的方法。

    

    我们提出了一种新的方法，通过使用多目标遗传算法，能够高效地生成用于量子增强支持向量机的量子特征映射，从而实现对高维希尔伯特空间的访问。我们的方法同时最大化分类准确性，同时最小化量子特征映射电路的本地门成本和非本地门成本。为了实现这一目标，我们为本地门和纠缠门定义了不同的适应度函数。与经典分类器的比较有助于理解使用量子机器学习的优势。令人惊讶的是，我们的实验揭示了量子核方法的最佳电路配置中包含了相应数量的非本地门用于纠缠，与之前的文献相反，之前的文献中非本地门被大部分抑制。此外，我们还证明数据的可分离性指数可以有效地用于确定最佳的配置。

    We present a novel approach for efficiently generating quantum feature maps for quantum-enhanced support vector machines, a kernel-based classifier, enabling access to high-dimensional Hilbert space. Our method employs a multi-objective genetic algorithm that simultaneously maximizes classification accuracy while minimizing both the local and non-local gate costs of the quantum feature map's circuit. To achieve this, we define distinct fitness functions for local gates and entanglement gates. Comparisons with classical classifiers are given in order to understand the advantages of using quantum machine learning. Surprisingly, our experiments reveal that the optimal configuration of quantum circuits for the quantum kernel method incorporates a proportional number of non-local gates for entanglement, contrary to previous literature where non-local gates were largely suppressed.  Furthermore, we demonstrate that the separability indexes of data can be effectively leveraged to determine th
    
[^72]: 无需训练依然能获益：通过蒙特卡洛树搜索和能量函数引导实现大型语言模型的数学推理

    No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function. (arXiv:2309.03224v1 [cs.AI])

    [http://arxiv.org/abs/2309.03224](http://arxiv.org/abs/2309.03224)

    该论文提出了一种通过蒙特卡洛树搜索和能量函数引导来释放大型语言模型的数学推理能力的方法，以解决当前在数学推理任务中的不足和错误。该方法不需要进一步的微调步骤，通过重新定义模型和引入路径验证器的方式，实现了对输出空间的搜索和推理路径的评估。

    

    大型语言模型（LLMs）展现出令人印象深刻的语言理解和背景学习能力，包括自然语言处理（NLP）任务和具有挑战性的数学推理。然而，由于缺乏过程监督，将PLMs应用于数学推理任务通常无法生成正确的推理步骤和最终答案，即使解决方案概率很高。为了在没有进一步的微调步骤的情况下发挥微调的LLMs的数学推理能力，我们提出了一种方法，通过蒙特卡洛树搜索（MCTS）和轻量级能量函数为LLMs赋予即时反应和精细推理系统。具体而言，我们首先将微调的LLMs重新定义为基于残差的能量模型（Residual-EBM），并应用噪声对比估计来估计能量函数的参数。然后，我们使用带有能量函数的MCTS作为路径验证器来搜索输出空间并评估推理路径。通过广泛的实验证明了我们方法的有效性。

    Large language models (LLMs) exhibit impressive language understanding and in-context learning abilities including natural language processing (NLP) tasks and challenging mathematical reasoning. However, due to the lack of process-supervision, applying PLMs to mathematical reasoning tasks often fail to generate correct reasoning steps and final answer even though solutions have high probabilities. To unleash the mathematical reasoning of finetuned-LLMs without any further fineutuning steps, we propose a method to endow LLMs with immediate reaction and delicate reasoning system via Monte Carlo Tree Search(MCTS) and a light energy function to rank the decision steps. In particular, We first re-formalize the finetuned-LLMs to a Residual-based Energy Model~(Residual-EBM) and apply noise contrastive estimation to estimate the parameters of energy function . Then we use MCTS with energy function as path verifier to search the output space and evaluating the reasoning path. Through extensive 
    
[^73]: CR-VAE:对变分自动编码器进行对比正则化以防止后验坍塌

    CR-VAE: Contrastive Regularization on Variational Autoencoders for Preventing Posterior Collapse. (arXiv:2309.02968v1 [cs.LG])

    [http://arxiv.org/abs/2309.02968](http://arxiv.org/abs/2309.02968)

    CR-VAE是一种对比正则化变分自动编码器的方法，通过增加对比目标来最大化类似视觉输入之间的互信息，从而避免后验坍塌现象，并在多个视觉数据集上表现出比最先进方法更好的性能。

    

    变分自动编码器（VAE）已知存在后验坍塌现象，即模型生成的潜在表示与输入之间变得独立。这导致输入的表示退化，这归因于VAE目标函数的限制。在这项工作中，我们提出了一种新的解决方案，即对比正则化变分自动编码器（CR-VAE）。我们的方法的核心是在原始VAE中增加对比目标，最大化类似视觉输入的表示之间的互信息。这种策略确保了输入与其潜在表示之间的信息流最大化，有效地避免了后验坍塌。我们在一系列视觉数据集上评估了我们的方法，并证明CR-VAE在防止后验坍塌方面优于最先进的方法。

    The Variational Autoencoder (VAE) is known to suffer from the phenomenon of \textit{posterior collapse}, where the latent representations generated by the model become independent of the inputs. This leads to degenerated representations of the input, which is attributed to the limitations of the VAE's objective function. In this work, we propose a novel solution to this issue, the Contrastive Regularization for Variational Autoencoders (CR-VAE). The core of our approach is to augment the original VAE with a contrastive objective that maximizes the mutual information between the representations of similar visual inputs. This strategy ensures that the information flow between the input and its latent representation is maximized, effectively avoiding posterior collapse. We evaluate our method on a series of visual datasets and demonstrate, that CR-VAE outperforms state-of-the-art approaches in preventing posterior collapse.
    
[^74]: 降低深度强化学习模型中的不良行为

    On Reducing Undesirable Behavior in Deep Reinforcement Learning Models. (arXiv:2309.02869v1 [cs.LG])

    [http://arxiv.org/abs/2309.02869](http://arxiv.org/abs/2309.02869)

    本论文提出了一个旨在降低深度强化学习模型不良行为的框架，通过从错误的状态-动作对中提取决策树分类器，并将其整合到训练循环中，来惩罚系统错误行为。这一框架在保持卓越性能的同时，为工程师提供了针对不良行为的可理解表征。

    

    深度强化学习（DRL）在大量应用领域证明了其极大的效用。然而，即使是成功的基于DRL的软件也可能表现出极其不良的行为。这是因为DRL训练是基于最大化一个奖励函数，该函数通常能捕捉到一般趋势，但不能准确捕捉或排除系统的某些行为。在本文中，我们提出了一个新的框架，旨在大幅降低基于DRL的软件的不良行为，同时保持其出色的性能。此外，我们的框架可以帮助工程师对这种不良行为进行可理解的表征。在底层，我们的方法基于从错误的状态-动作对中提取决策树分类器，然后将这些决策树整合到DRL训练循环中，当系统发生错误时对其进行惩罚。我们提供了一个我们方法的概念验证实现，并用它来评估该技术。

    Deep reinforcement learning (DRL) has proven extremely useful in a large variety of application domains. However, even successful DRL-based software can exhibit highly undesirable behavior. This is due to DRL training being based on maximizing a reward function, which typically captures general trends but cannot precisely capture, or rule out, certain behaviors of the system. In this paper, we propose a novel framework aimed at drastically reducing the undesirable behavior of DRL-based software, while maintaining its excellent performance. In addition, our framework can assist in providing engineers with a comprehensible characterization of such undesirable behavior. Under the hood, our approach is based on extracting decision tree classifiers from erroneous state-action pairs, and then integrating these trees into the DRL training loop, penalizing the system whenever it performs an error. We provide a proof-of-concept implementation of our approach, and use it to evaluate the techniqu
    
[^75]: 针对卷积神经网络的模型窃取攻击的高效防御方法

    Efficient Defense Against Model Stealing Attacks on Convolutional Neural Networks. (arXiv:2309.01838v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.01838](http://arxiv.org/abs/2309.01838)

    本文提出了一种简单但有效且高效的防御替代方案，引入一种启发式方法来扰动输出概率，可以轻松集成到模型中而无需额外训练，并且在抵御最先进的窃取攻击中表现出有效性。

    

    模型窃取攻击对深度学习模型构成了严重威胁，攻击者可以通过查询其黑盒API来窃取已训练的模型。这可能导致知识产权盗窃和其他安全与隐私风险。目前针对模型窃取攻击的最先进防御方法建议向预测概率添加扰动，但其计算较重且对攻击者提出了不切实际的假设。通常需要训练辅助模型，这可能耗时且资源密集，妨碍了该防御方法在实际应用中的部署。本文提出了一种简单但有效且高效的防御替代方案。我们引入一种启发式方法来扰动输出概率。该防御方法可以轻松集成到模型中而无需额外训练。我们展示了我们的防御方法在抵御三种最先进的窃取攻击中的有效性。我们进行了评估。

    Model stealing attacks have become a serious concern for deep learning models, where an attacker can steal a trained model by querying its black-box API. This can lead to intellectual property theft and other security and privacy risks. The current state-of-the-art defenses against model stealing attacks suggest adding perturbations to the prediction probabilities. However, they suffer from heavy computations and make impracticable assumptions about the adversary. They often require the training of auxiliary models. This can be time-consuming and resource-intensive which hinders the deployment of these defenses in real-world applications. In this paper, we propose a simple yet effective and efficient defense alternative. We introduce a heuristic approach to perturb the output probabilities. The proposed defense can be easily integrated into models without additional training. We show that our defense is effective in defending against three state-of-the-art stealing attacks. We evaluate
    
[^76]: 大尺度和无穷宽度下的深度学习勒让演讲

    Les Houches Lectures on Deep Learning at Large & Infinite Width. (arXiv:2309.01592v1 [stat.ML])

    [http://arxiv.org/abs/2309.01592](http://arxiv.org/abs/2309.01592)

    本论文主要以无穷宽度和大宽度范围内的深度神经网络为研究对象，讨论了这些网络的各种统计和动力学特性，包括随机网络的性质、训练后的网络与线性模型、核函数和高斯过程之间的关系，以及对大但有限宽度网络在初始化和训练后的摄动和非摄动处理。

    

    这些演讲是在2022年勒让夏季学校统计物理和机器学习课程上展示的，着重探讨了深度神经网络在无限宽度和大宽度范围内的情况。涵盖的主题包括这些网络的各种统计和动力学特性。特别是，讲师们讨论了随机深度神经网络的特性；训练过的深度神经网络，线性模型，核函数和高斯过程之间的联系，这些联系在无穷宽度的极限下出现；以及在初始化和训练后对大但有限宽度网络的摄动和非摄动处理。

    These lectures, presented at the 2022 Les Houches Summer School on Statistical Physics and Machine Learning, focus on the infinite-width limit and large-width regime of deep neural networks. Topics covered include various statistical and dynamical properties of these networks. In particular, the lecturers discuss properties of random deep neural networks; connections between trained deep neural networks, linear models, kernels, and Gaussian processes that arise in the infinite-width limit; and perturbative and non-perturbative treatments of large but finite-width networks, at initialization and after training.
    
[^77]: 使用BreaKHis数据集进行乳腺癌诊断的深度学习架构比较分析

    Comparative Analysis of Deep Learning Architectures for Breast Cancer Diagnosis Using the BreaKHis Dataset. (arXiv:2309.01007v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2309.01007](http://arxiv.org/abs/2309.01007)

    本研究评估了五个深度学习模型在使用BreaKHis数据集进行乳腺癌诊断时的性能，并发现Xception模型在准确性和F1得分方面表现最佳，这表明了深度学习方法在乳腺癌诊断中的重要性。

    

    癌症是一个非常困难且危险的健康问题，因为它以很多不同的方式表现出来，影响着许多不同的器官和组织。本研究的主要目标是评估深度学习模型在使用BreakHis数据集来正确识别乳腺癌病例方面的能力。BreakHis数据集通过其大量的组织病理学图片涵盖了广泛的乳腺癌亚型。在本研究中，我们使用并比较了五个知名的深度学习模型（VGG，ResNet，Xception，Inception和InceptionResNet）在癌症分类方面的性能。结果显示，Xception模型表现最佳，F1得分为0.9，准确率为89%。同时，Inception和InceptionResNet模型的准确率都达到了87%。然而，Inception模型的F1得分为87，而InceptionResNet模型的F1得分为86。这些结果展示了深度学习方法在正确诊断乳腺癌中的重要性。

    Cancer is an extremely difficult and dangerous health problem because it manifests in so many different ways and affects so many different organs and tissues. The primary goal of this research was to evaluate deep learning models' ability to correctly identify breast cancer cases using the BreakHis dataset. The BreakHis dataset covers a wide range of breast cancer subtypes through its huge collection of histopathological pictures. In this study, we use and compare the performance of five well-known deep learning models for cancer classification: VGG, ResNet, Xception, Inception, and InceptionResNet. The results placed the Xception model at the top, with an F1 score of 0.9 and an accuracy of 89%. At the same time, the Inception and InceptionResNet models both hit accuracy of 87% . However, the F1 score for the Inception model was 87, while that for the InceptionResNet model was 86. These results demonstrate the importance of deep learning methods in making correct breast cancer diagnose
    
[^78]: 条件生存预测中的面积规范COBRA

    Area-norm COBRA on Conditional Survival Prediction. (arXiv:2309.00417v1 [cs.LG])

    [http://arxiv.org/abs/2309.00417](http://arxiv.org/abs/2309.00417)

    本文提出了一种基于组合回归的条件生存预测方法，其中使用面积作为相似度度量，通过选择最重要的变量来提高模型性能。

    

    本文探讨了一种不同的组合回归策略来计算条件生存函数。我们使用基于回归的弱学习器来创建所提出的集成技术。所提出的组合回归策略使用相似度度量作为两个生存曲线之间的面积。所提出的模型表明其表现优于随机生存森林。本文讨论了一种在组合回归设置中选择最重要变量的新技术。我们进行了一项模拟研究，表明我们对变量相关性的提议效果很好。我们还使用了三个真实数据集来说明该模型。

    The paper explores a different variation of combined regression strategy to calculate the conditional survival function. We use regression based weak learners to create the proposed ensemble technique. The proposed combined regression strategy uses proximity measure as area between two survival curves. The proposed model shows a construction which ensures that it performs better than the Random Survival Forest. The paper discusses a novel technique to select the most important variable in the combined regression setup. We perform a simulation study to show that our proposition for finding relevance of the variables works quite well. We also use three real-life datasets to illustrate the model.
    
[^79]: 基于神经预测的零样本NAS范式的有效性

    Efficacy of Neural Prediction-Based NAS for Zero-Shot NAS Paradigm. (arXiv:2308.16775v1 [cs.LG])

    [http://arxiv.org/abs/2308.16775](http://arxiv.org/abs/2308.16775)

    这项研究提出了一种新的方法，通过深度学习进行零样本架构搜索，通过使用可学习的傅里叶正弦和求和编码来构建计算的前馈图，从而解决了基于预测的神经架构搜索中性能指标泛化的限制。

    

    在基于预测的神经架构搜索（NAS）中，通过图卷积网络得到的性能指标取得了显著的成功。然而，通过one-hot编码将前馈结构表示为组件图的这些指标面临一个限制：无法在不同的搜索空间中评估架构的性能。相反，手工性能指标（零样本NAS）可以在多个搜索空间中泛化，因为它们使用相同的架构和随机初始化。为了解决这个限制，我们提出了一种新的深度学习方法，用于零样本NAS。我们的方法采用傅里叶正弦和求和编码来进行卷积核的编码，从而构建了一个计算的前馈图，其结构类似于正在评估的架构。这些编码是可学习的，并提供了架构拓扑信息的全面视图。然后，伴随的多层感知器（MLP）对架构进行排序。

    In prediction-based Neural Architecture Search (NAS), performance indicators derived from graph convolutional networks have shown significant success. These indicators, achieved by representing feed-forward structures as component graphs through one-hot encoding, face a limitation: their inability to evaluate architecture performance across varying search spaces. In contrast, handcrafted performance indicators (zero-shot NAS), which use the same architecture with random initialization, can generalize across multiple search spaces. Addressing this limitation, we propose a novel approach for zero-shot NAS using deep learning. Our method employs Fourier sum of sines encoding for convolutional kernels, enabling the construction of a computational feed-forward graph with a structure similar to the architecture under evaluation. These encodings are learnable and offer a comprehensive view of the architecture's topological information. An accompanying multi-layer perceptron (MLP) then ranks t
    
[^80]: 在线GentleAdaBoost -- 技术报告

    Online GentleAdaBoost -- Technical Report. (arXiv:2308.14004v1 [stat.ML])

    [http://arxiv.org/abs/2308.14004](http://arxiv.org/abs/2308.14004)

    该论文研究了在线版的GentleAdaboost算法，通过在线方式将弱分类器与强分类器结合，提出了一种通过线搜索将批处理方法扩展为在线方法的方法，并与其他在线方法在各种基准数据集上进行了对比。

    

    我们研究了在线版的GentleAdaboost，其中我们以在线方式将一个弱分类器与一个强分类器结合起来。我们提供了一种通过线搜索应用的方法，将批处理方法扩展为在线方法，并通过理论证明了其正确性。最后，我们在各种基准数据集上比较了我们的在线boosting方法与其他在线方法。

    We study the online variant of GentleAdaboost, where we combine a weak learner to a strong learner in an online fashion. We provide an approach to extend the batch approach to an online approach with theoretical justifications through application of line search. Finally we compare our online boosting approach with other online approaches across a variety of benchmark datasets.
    
[^81]: 工业人工智能中的随机配置机

    Stochastic Configuration Machines for Industrial Artificial Intelligence. (arXiv:2308.13570v1 [cs.LG])

    [http://arxiv.org/abs/2308.13570](http://arxiv.org/abs/2308.13570)

    本文提出了一种新颖的随机学习器模型，称为随机配置机（SCMs），其基于随机配置网络（SCNs），旨在强调工业人工智能中的有效建模和节约数据大小。SCMs通过压缩模型存储，并保持有利的预测性能，具有在工业应用中很大的潜力。

    

    在工业人工智能（IAI）中，需要实时、准确的预测建模，神经网络在其中起到关键作用。工业人工智能中的神经网络需要强大的高性能计算设备来处理大量的浮点数据。本文基于随机配置网络（SCNs），提出了一种新的随机学习器模型，称为随机配置机（SCMs），以强调对于工业应用非常有用和有价值的有效建模和节约数据大小。与具有二值化实现的随机向量功能链接（RVFL）网络相比，SCMs的模型存储可以显著压缩，同时保持有利的预测性能。除了SCM学习器模型的架构和学习算法，作为本文的重要部分，我们还通过分析模型的复杂性提供了SCMs的学习能力的理论基础。实验研究也进行了。

    Real-time predictive modelling with desired accuracy is highly expected in industrial artificial intelligence (IAI), where neural networks play a key role. Neural networks in IAI require powerful, high-performance computing devices to operate a large number of floating point data. Based on stochastic configuration networks (SCNs), this paper proposes a new randomized learner model, termed stochastic configuration machines (SCMs), to stress effective modelling and data size saving that are useful and valuable for industrial applications. Compared to SCNs and random vector functional-link (RVFL) nets with binarized implementation, the model storage of SCMs can be significantly compressed while retaining favourable prediction performance. Besides the architecture of the SCM learner model and its learning algorithm, as an important part of this contribution, we also provide a theoretical basis on the learning capacity of SCMs by analysing the model's complexity. Experimental studies are ca
    
[^82]: MLLM-DataEngine：一种MLLM的迭代改进方法

    MLLM-DataEngine: An Iterative Refinement Approach for MLLM. (arXiv:2308.13566v1 [cs.LG])

    [http://arxiv.org/abs/2308.13566](http://arxiv.org/abs/2308.13566)

    本文提出了一种名为MLLM-DataEngine的迭代改进方法，它通过分析模型弱点，生成适当的增量数据集并迭代地增强模型能力。与以往方法相比，MLLM-DataEngine生成的数据在定位、质量和正确性方面表现更好。

    

    尽管在指导数据集构建和基准测试方面，多模态大型语言模型（MLLM）取得了很大的进展，但训练和评估的独立性使得当前的MLLM很难在相对较低的人力成本下进一步提高其能力。本文提出了一种新颖的封闭循环系统MLLM-DataEngine，它连接了数据生成、模型训练和评估。在每个循环迭代中，MLLM-DataEngine首先根据评估结果分析模型的弱点，然后生成合适的增量数据集用于下一次训练迭代，并迭代地增强模型的能力。与先前与基准测试分离的数据收集方法相比，MLLM-DataEngine生成的数据在定位、质量和正确性方面都表现得更好。

    Despite the great advance of Multimodal Large Language Models (MLLMs) in both instruction dataset building and benchmarking, the independence of training and evaluation makes current MLLMs hard to further improve their capability under the guidance of evaluation results with a relatively low human cost. In this paper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data generation, model training, and evaluation. Within each loop iteration, the MLLM-DataEngine first analyze the weakness of the model based on the evaluation results, then generate a proper incremental dataset for the next training iteration and enhance the model capability iteratively. Compared with previous data collection methods which are separate from the benchmarking, the data generated by MLLM-DataEngine shows better targeting, quality, and correctness. For targeting, we propose an Adaptive Bad-case Sampling module, which adjusts the ratio of different types of data within each incremental datas
    
[^83]: 抓住它们：图匹配匹配滤波中的解决方案多样化

    Gotta match 'em all: Solution diversification in graph matching matched filters. (arXiv:2308.13451v1 [stat.ML])

    [http://arxiv.org/abs/2308.13451](http://arxiv.org/abs/2308.13451)

    本文提出了一种在大规模背景图中查找多个嵌入的模板图的新方法，通过迭代惩罚相似度矩阵来实现多样化匹配的发现，并提出了算法加速措施。在理论验证和实验证明中，证明了该方法的可行性和实用性。

    

    我们提出了一种在非常大的背景图中查找多个嵌入在其中的模板图的新方法。我们的方法基于Sussman等人提出的图匹配匹配滤波技术，通过在匹配滤波算法中迭代地惩罚合适的节点对相似度矩阵来实现多样化匹配的发现。此外，我们提出了算法加速，极大地提高了我们的匹配滤波方法的可扩展性。我们在相关的Erdos-Renyi图设置中对我们的方法进行了理论上的验证，显示其在温和的模型条件下能够顺序地发现多个模板。我们还通过使用模拟模型和真实世界数据集（包括人脑连接组和大型交易知识库）进行了大量实验证明了我们方法的实用性。

    We present a novel approach for finding multiple noisily embedded template graphs in a very large background graph. Our method builds upon the graph-matching-matched-filter technique proposed in Sussman et al., with the discovery of multiple diverse matchings being achieved by iteratively penalizing a suitable node-pair similarity matrix in the matched filter algorithm. In addition, we propose algorithmic speed-ups that greatly enhance the scalability of our matched-filter approach. We present theoretical justification of our methodology in the setting of correlated Erdos-Renyi graphs, showing its ability to sequentially discover multiple templates under mild model conditions. We additionally demonstrate our method's utility via extensive experiments both using simulated models and real-world dataset, include human brain connectomes and a large transactional knowledge base.
    
[^84]: 通过多阶段VAE对分子属性进行客观无关的改进

    Objective-Agnostic Enhancement of Molecule Properties via Multi-Stage VAE. (arXiv:2308.13066v1 [cs.LG])

    [http://arxiv.org/abs/2308.13066](http://arxiv.org/abs/2308.13066)

    本文提出了一种多阶段VAE方法，可以改善在药物发现领域中恢复低维流形的问题。实验结果表明，该方法显著改善了生成分子的属性统计，而无需集成属性预测器。

    

    变分自编码器（VAE）是药物发现的一种流行方法，已经提出了各种架构和流程来改善其性能。然而，已知VAE方法在数据位于高维环境空间中嵌入的低维流形上时，很难恢复流形。在药物发现领域，这一问题尚未得到充分探索。本文中，我们探索了一种可以改善合成数据集上的流形恢复的多阶段VAE方法，并将其应用到药物发现领域。我们使用ChEMBL数据集对我们的多阶段VAE方法进行了实验评估，并展示了该方法在不将属性预测器纳入训练流程的前提下，显著改善了所生成分子的属性统计。我们进一步在两个精心策划且较小的分子数据集上对模型进行了微调，这两个数据集针对不同的蛋白质。实验结果显示了属性的增加。

    Variational autoencoder (VAE) is a popular method for drug discovery and various architectures and pipelines have been proposed to improve its performance. However, VAE approaches are known to suffer from poor manifold recovery when the data lie on a low-dimensional manifold embedded in a higher dimensional ambient space [Dai and Wipf, 2019]. The consequences of it in drug discovery are somewhat under-explored. In this paper, we explore applying a multi-stage VAE approach, that can improve manifold recovery on a synthetic dataset, to the field of drug discovery. We experimentally evaluate our multi-stage VAE approach using the ChEMBL dataset and demonstrate its ability to improve the property statistics of generated molecules substantially from pre-existing methods without incorporating property predictors into the training pipeline. We further fine-tune our models on two curated and much smaller molecule datasets that target different proteins. Our experiments show an increase in the 
    
[^85]: 使用精细调整的Llama 2 GPT模型进行金融新闻分析

    Financial News Analytics Using Fine-Tuned Llama 2 GPT Model. (arXiv:2308.13032v1 [cs.CL])

    [http://arxiv.org/abs/2308.13032](http://arxiv.org/abs/2308.13032)

    本研究通过精细调整的Llama 2模型实现了金融新闻的多任务分析，包括文本分析、摘要和情感提取等。实验结果显示，提取的命名实体情感可以作为有监督机器学习模型的预测特征。

    

    本文考虑了使用精细调整的Llama 2 Large Language Model (LLM) 对金融新闻进行多任务分析的可能性。通过PEFT/LoRA方法对模型进行精细调整，主要包括从金融市场角度分析文本、突出文本的主要观点、对文本进行摘要和提取具有适当情感的命名实体等任务。实验结果表明，经过精细调整的Llama 2模型能够进行多任务的金融新闻分析，其响应的结构可以部分为结构化文本，另一部分数据可以采用JSON格式进一步处理。提取的命名实体情感可以被视为具有定量目标变量的监督机器学习模型的预测特征。

    The paper considers the possibility to fine-tune Llama 2 Large Language Model (LLM) for the multitask analysis of financial news. For fine-tuning, the PEFT/LoRA based approach was used. In the study, the model was fine-tuned for the following tasks: analysing a text from financial market perspectives, highlighting main points of a text, summarizing a text and extracting named entities with appropriate sentiments. The obtained results show that the fine-tuned Llama 2 model can perform a multitask financial news analysis with a specified structure of response, part of response can be a structured text and another part of data can have JSON format for further processing. Extracted sentiments for named entities can be considered as predictive features in supervised machine learning models with quantitative target variables.
    
[^86]: 一个有效的基于Transformer的上下文模型和时间门池化用于说话人识别

    An Effective Transformer-based Contextual Model and Temporal Gate Pooling for Speaker Identification. (arXiv:2308.11241v1 [cs.SD])

    [http://arxiv.org/abs/2308.11241](http://arxiv.org/abs/2308.11241)

    本文介绍了一种基于Transformer的上下文模型和时间门池化的有效方法，应用于说话人识别，并在准确率85.9%的情况下比较了其性能与wav2vec2方法。

    

    Wav2vec2在语音识别中应用Transformer架构和自监督学习取得了成功。最近，这些方法不仅用于语音识别，还用于整个语音处理。本文介绍了一种应用了基于Transformer的上下文模型的有效端到端说话人识别模型。我们探索了参数与性能之间的关系，以确定一个有效模型的结构。此外，我们提出了一种具有强大学习能力的池化方法，称为时间门池化(Temporal Gate Pooling)，用于说话人识别。我们将Conformer作为编码器，并利用BEST-RQ进行预训练，并使用VoxCeleb1的说话人识别进行了评估。该方法在仅有28.5M个参数的情况下，实现了85.9%的准确率，与具有317.7M个参数的wav2vec2相当。代码可在https://github.com/HarunoriKawano/speaker-identification-with-tgp获得。

    Wav2vec2 has achieved success in applying Transformer architecture and self-supervised learning to speech recognition. Recently, these have come to be used not only for speech recognition but also for the entire speech processing. This paper introduces an effective end-to-end speaker identification model applied Transformer-based contextual model. We explored the relationship between the parameters and the performance in order to discern the structure of an effective model. Furthermore, we propose a pooling method, Temporal Gate Pooling, with powerful learning ability for speaker identification. We applied Conformer as encoder and BEST-RQ for pre-training and conducted an evaluation utilizing the speaker identification of VoxCeleb1. The proposed method has achieved an accuracy of 85.9% with 28.5M parameters, demonstrating comparable precision to wav2vec2 with 317.7M parameters. Code is available at https://github.com/HarunoriKawano/speaker-identification-with-tgp.
    
[^87]: 超越预期：随机动力系统的剩余动态模式分解和方差

    Beyond expectations: Residual Dynamic Mode Decomposition and Variance for Stochastic Dynamical Systems. (arXiv:2308.10697v2 [math.DS] UPDATED)

    [http://arxiv.org/abs/2308.10697](http://arxiv.org/abs/2308.10697)

    本文研究了随机动力系统中的动态模式分解(DMD)方法。通过在Koopman框架中引入方差来解决挑战，包括虚假模式和本征谱。结果表明，需要超越预期来有效处理随机系统。

    

    Koopman算符将非线性动力系统线性化，使得其谱信息至关重要。已经开发了许多算法来近似这些谱特性，其中动态模式分解(DMD)是基于投影方法的典型代表。尽管Koopman算符本身是线性的，但它在无穷维的可观测空间中起作用，这带来了挑战，包括虚假模式，本征谱和Koopman模式分解的验证。虽然最近的研究已经解决了确定性系统的这些挑战，但对于随机系统的已验证的DMD方法仍存在明显的差距，其中Koopman算符测量可观测量的期望值。我们表明，需要超越预期来解决这些问题。通过将方差纳入Koopman框架，我们解决了这些挑战。通过额外的类似DMD的矩阵，我们近似一个残差平方和的和以及方差。

    Koopman operators linearize nonlinear dynamical systems, making their spectral information of crucial interest. Numerous algorithms have been developed to approximate these spectral properties, and Dynamic Mode Decomposition (DMD) stands out as the poster child of projection-based methods. Although the Koopman operator itself is linear, the fact that it acts in an infinite-dimensional space of observables poses challenges. These include spurious modes, essential spectra, and the verification of Koopman mode decompositions. While recent work has addressed these challenges for deterministic systems, there remains a notable gap in verified DMD methods for stochastic systems, where the Koopman operator measures the expectation of observables. We show that it is necessary to go beyond expectations to address these issues. By incorporating variance into the Koopman framework, we address these challenges. Through an additional DMD-type matrix, we approximate the sum of a squared residual and 
    
[^88]: 多任务伪标签学习在非侵入式语音质量评估模型中的应用

    Multi-Task Pseudo-Label Learning for Non-Intrusive Speech Quality Assessment Model. (arXiv:2308.09262v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2308.09262](http://arxiv.org/abs/2308.09262)

    这篇论文介绍了一种基于多任务伪标签学习的非侵入式语音质量评估模型（MTQ-Net），通过与预训练模型结合使用伪标签来进行训练，实验结果证明了其相对于从头训练模型的优势。

    

    本研究提出了一种基于多任务伪标签学习（MPL）的非侵入式语音质量评估模型MTQ-Net。MPL由两个阶段组成：从预训练模型中获取伪标签分数和执行多任务学习。评估目标是三个3QUEST指标，即语音MOS（S-MOS），噪声MOS（N-MOS）和通用MOS（G-MOS）。预训练的MOSA-Net模型被用来估计三个伪标签：主观评估语音质量（PESQ），短时客观可懂性（STOI）和语音失真指数（SDI）。然后利用多任务学习通过结合有监督损失（通过估计分数与真实标签之间的差异导出）和半监督损失（通过估计分数与伪标签之间的差异导出）来训练MTQ-Net，其中使用Huber损失函数作为损失函数。实验结果首先证明了MPL相较于从头训练模型的优势。

    This study proposes a multi-task pseudo-label learning (MPL)-based non-intrusive speech quality assessment model called MTQ-Net. MPL consists of two stages: obtaining pseudo-label scores from a pretrained model and performing multi-task learning. The 3QUEST metrics, namely Speech-MOS (S-MOS), Noise-MOS (N-MOS), and General-MOS (G-MOS), are the assessment targets. The pretrained MOSA-Net model is utilized to estimate three pseudo labels: perceptual evaluation of speech quality (PESQ), short-time objective intelligibility (STOI), and speech distortion index (SDI). Multi-task learning is then employed to train MTQ-Net by combining a supervised loss (derived from the difference between the estimated score and the ground-truth label) and a semi-supervised loss (derived from the difference between the estimated score and the pseudo label), where the Huber loss is employed as the loss function. Experimental results first demonstrate the advantages of MPL compared to training a model from scra
    
[^89]: SciRE-Solver: 用得分积分求解器和递归导数估计快速采样扩散概率模型

    SciRE-Solver: Efficient Sampling of Diffusion Probabilistic Models by Score-integrand Solver with Recursive Derivative Estimation. (arXiv:2308.07896v1 [stat.ML])

    [http://arxiv.org/abs/2308.07896](http://arxiv.org/abs/2308.07896)

    SciRE-Solver是一种高效的采样器，通过引入得分积分求解器和递归导数估计方法，它解决了扩散概率模型采样过程缓慢的挑战，并实现了最先进的采样性能。

    

    扩散概率模型(DPMs)是一类强大的生成模型，以其生成高保真图像样本的能力而闻名。DPMs的实现面临的主要挑战是采样过程缓慢。在这项工作中，我们提出了一种高效的DPMs采样器。具体而言，我们针对与DPMs采样过程对应的扩散ODE提出了一个基于得分的精确解决方案范式，该范式为求解扩散ODE的数值算法开发提供了新的视角。为了实现高效的采样器，我们提出了一种递归导数估计(RDE)方法来减小估计误差。通过我们提出的解决方案范式和RDE方法，我们提出了具有收敛顺序保证的得分积分求解器(SciRE-Solver)来解决扩散ODEs。SciRE-Solver在离散时间和连续时间DPMs上获得了最先进的采样性能，并且仅需有限数量的得分函数评估(NFE)。

    Diffusion probabilistic models (DPMs) are a powerful class of generative models known for their ability to generate high-fidelity image samples. A major challenge in the implementation of DPMs is the slow sampling process. In this work, we bring a high-efficiency sampler for DPMs. Specifically, we propose a score-based exact solution paradigm for the diffusion ODEs corresponding to the sampling process of DPMs, which introduces a new perspective on developing numerical algorithms for solving diffusion ODEs. To achieve an efficient sampler, we propose a recursive derivative estimation (RDE) method to reduce the estimation error. With our proposed solution paradigm and RDE method, we propose the score-integrand solver with the convergence order guarantee as efficient solver (SciRE-Solver) for solving diffusion ODEs. The SciRE-Solver attains state-of-the-art (SOTA) sampling performance with a limited number of score function evaluations (NFE) on both discrete-time and continuous-time DPMs
    
[^90]: 在不同环境中对单刚体角色的自适应跟踪

    Adaptive Tracking of a Single-Rigid-Body Character in Various Environments. (arXiv:2308.07491v1 [cs.RO])

    [http://arxiv.org/abs/2308.07491](http://arxiv.org/abs/2308.07491)

    本研究提出了一种基于单刚体角色仿真的深度强化学习方法，通过训练一个能够自适应各种环境变化的策略，实现在不需要额外学习的情况下完成各种任务。

    

    自从DeepMimic的引入以来，后续研究一直致力于在不同情景下扩展模拟动作的范畴。在本研究中，我们提出了一个替代方法，一种基于单刚体角色仿真的深度强化学习方法。利用质心动力学模型（CDM）将全身角色表示为单刚体（SRB），并训练一个跟踪参考动作的策略，我们可以得到一个能够适应各种未观测环境变化和控制器转换的策略，而不需要额外的学习。由于状态和动作空间的降维，学习过程具有高样本效率。最终的全身动作以物理合理的方式基于模拟SRB角色的状态进行运动生成。SRB仿真被制定为一个二次规划问题，策略输出一个动作，允许角色在不同环境中完成任务。

    Since the introduction of DeepMimic [Peng et al. 2018], subsequent research has focused on expanding the repertoire of simulated motions across various scenarios. In this study, we propose an alternative approach for this goal, a deep reinforcement learning method based on the simulation of a single-rigid-body character. Using the centroidal dynamics model (CDM) to express the full-body character as a single rigid body (SRB) and training a policy to track a reference motion, we can obtain a policy that is capable of adapting to various unobserved environmental changes and controller transitions without requiring any additional learning. Due to the reduced dimension of state and action space, the learning process is sample-efficient. The final full-body motion is kinematically generated in a physically plausible way, based on the state of the simulated SRB character. The SRB simulation is formulated as a quadratic programming (QP) problem, and the policy outputs an action that allows th
    
[^91]: 用心电图信号进行血流动力学推理的深度度量学习

    Deep Metric Learning for the Hemodynamics Inference with Electrocardiogram Signals. (arXiv:2308.04650v1 [cs.LG])

    [http://arxiv.org/abs/2308.04650](http://arxiv.org/abs/2308.04650)

    本研究提出了一种利用心电图信号进行血流动力学推理的深度度量学习方法。通过使用非侵入性信号进行心脏压力的评估，既可以在住院环境中用于诊断和治疗预后，也可以应用于门诊设置中。

    

    心力衰竭是一种影响全球数百万人的致残疾患，对他们的生活质量和死亡率具有重大影响。心脏压力的客观评估仍然是诊断和治疗预后的重要方法。尽管心脏导管化验是估计中心血液动力学压力的黄金标准，但它是一种有潜在风险的侵入性操作，对某些患者来说可能是危险的。利用非侵入性信号（如心电图（ECG））的方法可以使常规估计心脏压力在住院和门诊环境中成为可能。先前训练用于以监督的方式估计心脏内压力（例如平均肺毛细血管楔压（mPCWP））的模型显示了良好的鉴别能力，但仅限于心力衰竭队列的标记数据集。

    Heart failure is a debilitating condition that affects millions of people worldwide and has a significant impact on their quality of life and mortality rates. An objective assessment of cardiac pressures remains an important method for the diagnosis and treatment prognostication for patients with heart failure. Although cardiac catheterization is the gold standard for estimating central hemodynamic pressures, it is an invasive procedure that carries inherent risks, making it a potentially dangerous procedure for some patients. Approaches that leverage non-invasive signals - such as electrocardiogram (ECG) - have the promise to make the routine estimation of cardiac pressures feasible in both inpatient and outpatient settings. Prior models trained to estimate intracardiac pressures (e.g., mean pulmonary capillary wedge pressure (mPCWP)) in a supervised fashion have shown good discriminatory ability but have been limited to the labeled dataset from the heart failure cohort. To address th
    
[^92]: 利用合成数据解决数据不平衡问题：基于数据角度的基准线

    Exploiting Synthetic Data for Data Imbalance Problems: Baselines from a Data Perspective. (arXiv:2308.00994v1 [cs.CV])

    [http://arxiv.org/abs/2308.00994](http://arxiv.org/abs/2308.00994)

    本文提出了一个简单而有效的基准线SYNAuG，利用合成数据来解决数据不平衡问题，并在多个数据集上取得了令人印象深刻的性能，超过了现有方法。

    

    我们生活在一个庞大的数据海洋中，深度神经网络也不例外。然而，这些数据表现出一种固有的不平衡现象。这种不平衡给深度神经网络产生偏见的预测带来了风险，可能导致严重的道德和社会后果。为了应对这些挑战，我们认为利用生成模型是一种有前景的方法，鉴于最近扩散模型在生成高质量图像方面的显著进展。在这项工作中，我们提出了一个简单而有效的基准线SYNAuG，利用合成数据作为解决数据不平衡问题之前使用的任务特定算法的初步步骤。这种直接的方法在CIFAR100-LT、ImageNet100-LT、UTKFace和Waterbird等数据集上取得了令人印象深刻的性能，超过了现有的任务特定方法的性能。虽然我们并不声称我们的方法作为问题的完整解决方案

    We live in a vast ocean of data, and deep neural networks are no exception to this. However, this data exhibits an inherent phenomenon of imbalance. This imbalance poses a risk of deep neural networks producing biased predictions, leading to potentially severe ethical and social consequences. To address these challenges, we believe that the use of generative models is a promising approach for comprehending tasks, given the remarkable advancements demonstrated by recent diffusion models in generating high-quality images. In this work, we propose a simple yet effective baseline, SYNAuG, that utilizes synthetic data as a preliminary step before employing task-specific algorithms to address data imbalance problems. This straightforward approach yields impressive performance on datasets such as CIFAR100-LT, ImageNet100-LT, UTKFace, and Waterbird, surpassing the performance of existing task-specific methods. While we do not claim that our approach serves as a complete solution to the problem
    
[^93]: 从人类反馈中进行强化学习的开放问题和基本限制

    Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback. (arXiv:2307.15217v1 [cs.AI])

    [http://arxiv.org/abs/2307.15217](http://arxiv.org/abs/2307.15217)

    本文调查了从人类反馈中进行强化学习的开放问题和基本限制，并提出了加强社会监督的审计和披露标准。

    

    从人类反馈中进行强化学习（RLHF）是一种训练人工智能系统与人类目标保持一致的技术。RLHF已成为微调最新的大型语言模型（LLM）的核心方法。尽管如此受欢迎，但系统性地系统化其缺陷的公开工作相对较少。在本文中，我们（1）调查了RLHF及相关方法的开放问题和基本限制；（2）概述了了解、改进和补充RLHF的实践技术；以及（3）提出了审计和披露标准以改进RLHF系统的社会监督。我们的工作强调了RLHF的局限性，并强调了以多方面方法开发更安全的人工智能系统的重要性。

    Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.
    
[^94]: InVAErt网络：一个数据驱动的框架用于仿真、推理和可识别性分析。

    InVAErt networks: a data-driven framework for emulation, inference and identifiability analysis. (arXiv:2307.12586v1 [cs.LG])

    [http://arxiv.org/abs/2307.12586](http://arxiv.org/abs/2307.12586)

    InVAErt网络是一个数据驱动的框架，用于分析和合成物理系统，具有模型反演和可识别性分析的能力。

    

    目前，基于物理的系统使用生成模型和深度学习主要用于仿真任务。然而，数据驱动结构提供的出色灵活性表明应将该表示扩展到系统综合的其他方面，包括模型反演和可识别性。我们引入了InVAErt网络，这是一个综合的数据驱动分析和合成参数化物理系统的框架，它使用确定性编码器和解码器表示前向和逆向解映射，用归一化流来捕捉系统输出的概率分布，并设计了一种变分编码器来学习输入和输出之间缺乏双射性的紧凑潜在表示。我们正式研究了损失函数中惩罚系数的选择和潜在空间采样策略，因为我们发现这些因素会显著影响训练和测试性能。我们有效地验证了我们的模型。

    Use of generative models and deep learning for physics-based systems is currently dominated by the task of emulation. However, the remarkable flexibility offered by data-driven architectures would suggest to extend this representation to other aspects of system synthesis including model inversion and identifiability. We introduce inVAErt (pronounced \emph{invert}) networks, a comprehensive framework for data-driven analysis and synthesis of parametric physical systems which uses a deterministic encoder and decoder to represent the forward and inverse solution maps, normalizing flow to capture the probabilistic distribution of system outputs, and a variational encoder designed to learn a compact latent representation for the lack of bijectivity between inputs and outputs. We formally investigate the selection of penalty coefficients in the loss function and strategies for latent space sampling, since we find that these significantly affect both training and testing performance. We valid
    
[^95]: 使用PIP-Net解释和纠正医学图像分类

    Interpreting and Correcting Medical Image Classification with PIP-Net. (arXiv:2307.10404v1 [cs.CV])

    [http://arxiv.org/abs/2307.10404](http://arxiv.org/abs/2307.10404)

    本研究利用PIP-Net开展了可解释的机器学习技术在医学图像分类中的应用，并展示了其在骨折检测和皮肤癌诊断方面的准确性和可解释性。通过无监督的预训练，PIP-Net能够轻松识别数据质量问题，并且我们还发现人们可以通过手动禁用不良原型来纠正PIP-Net的推理过程。

    

    部分原型模型是可解释性的图像分类器，是黑盒人工智能的有希望的替代方案。本文探讨了可解释性机器学习的适用性和潜力，特别是对于真实世界的医学成像数据的自动诊断支持。PIP-Net学习人类可理解的典型图像部分，并评估其在骨折检测和皮肤癌诊断方面的准确性和可解释性。我们发现PIP-Net的决策过程符合医学分类标准，仅提供图像级别的类标签。由于PIP-Net对原型进行了无监督的预训练，因此可以轻松识别X光中的不良文本或标签错误等数据质量问题。此外，我们是第一个显示人们可以通过直接禁用不良原型来手动纠正PIP-Net的推理过程。我们得出结论，部分原型模型对医学应用具有潜力，因为它们具有相互参考性。

    Part-prototype models are explainable-by-design image classifiers, and a promising alternative to black box AI. This paper explores the applicability and potential of interpretable machine learning, in particular PIP-Net, for automated diagnosis support on real-world medical imaging data. PIP-Net learns human-understandable prototypical image parts and we evaluate its accuracy and interpretability for fracture detection and skin cancer diagnosis. We find that PIP-Net's decision making process is in line with medical classification standards, while only provided with image-level class labels. Because of PIP-Net's unsupervised pretraining of prototypes, data quality problems such as undesired text in an X-ray or labelling errors can be easily identified. Additionally, we are the first to show that humans can manually correct the reasoning of PIP-Net by directly disabling undesired prototypes. We conclude that part-prototype models are promising for medical applications due to their inter
    
[^96]: 创建一个支持OpenMP Fortran和C++代码相互翻译的数据集

    Creating a Dataset Supporting Translation Between OpenMP Fortran and C++ Code. (arXiv:2307.07686v1 [cs.SE])

    [http://arxiv.org/abs/2307.07686](http://arxiv.org/abs/2307.07686)

    本研究创建了一个数据集，用于训练机器学习模型在OpenMP Fortran和C++代码之间进行翻译。这个数据集通过精细的代码相似性测试确保了可靠性和适用性，并且能够显著提升大规模语言模型的翻译能力。

    

    在本研究中，我们提出了一个新颖的数据集，用于训练在OpenMP Fortran和C++代码之间进行翻译的机器学习模型。通过精细的代码相似性测试，我们确保了数据集的可靠性和适用性。我们使用定量（CodeBLEU）和定性（人工评估）方法评估了我们数据集的有效性。我们展示了这个数据集如何显著提高大规模语言模型的翻译能力，对于没有先前编码知识的模型，提高了5.1倍，对于具有一定编码熟悉度的模型，提高了9.9倍。我们的工作突显了这个数据集在高性能计算的代码翻译领域的潜力。

    In this study, we present a novel dataset for training machine learning models translating between OpenMP Fortran and C++ code. To ensure reliability and applicability, the dataset is initially refined using a meticulous code similarity test. The effectiveness of our dataset is assessed using both quantitative (CodeBLEU) and qualitative (human evaluation) methods. We demonstrate how this dataset can significantly improve the translation capabilities of large-scale language models, with improvements of \times 5.1 for models with no prior coding knowledge and \times 9.9 for models with some coding familiarity. Our work highlights the potential of this dataset to advance the field of code translation for high-performance computing.
    
[^97]: Weisfeiler和Lehman进行度量建模：探索WL测试的有效性

    Weisfeiler and Lehman Go Measurement Modeling: Probing the Validity of the WL Test. (arXiv:2307.05775v1 [cs.LG])

    [http://arxiv.org/abs/2307.05775](http://arxiv.org/abs/2307.05775)

    本文通过系统分析，揭示了$k$-WL的可靠性和有效性问题，并提出了基于基准的表达能力的外延定义和测量。

    

    图神经网络的表达能力通常通过比较一个架构能够区分的非同构图或节点对的数量与$k$-维Weisfeiler-Lehman ($k$-WL)测试能够区分的数量来衡量。本文通过对$k$-WL的可靠性和有效性进行系统分析，揭示了从业者对表达能力和$k$-WL的概念化之间的不一致性。我们进一步进行了对从业者的调查（n=18），以了解他们对表达能力的概念以及对$k$-WL的假设。与从业者的观点相反，我们的分析（借鉴了图论和基准审核）揭示了$k$-WL不能保证等距、可能与现实世界的图任务无关，并且可能不促进泛化或可信度。我们主张基于基准的表达能力的外延定义和测量，进一步贡献了构建此类基准的指导问题。

    The expressive power of graph neural networks is usually measured by comparing how many pairs of graphs or nodes an architecture can possibly distinguish as non-isomorphic to those distinguishable by the $k$-dimensional Weisfeiler-Lehman ($k$-WL) test. In this paper, we uncover misalignments between practitioners' conceptualizations of expressive power and $k$-WL through a systematic analysis of the reliability and validity of $k$-WL. We further conduct a survey ($n = 18$) of practitioners to surface their conceptualizations of expressive power and their assumptions about $k$-WL. In contrast to practitioners' opinions, our analysis (which draws from graph theory and benchmark auditing) reveals that $k$-WL does not guarantee isometry, can be irrelevant to real-world graph tasks, and may not promote generalization or trustworthiness. We argue for extensional definitions and measurement of expressive power based on benchmarks; we further contribute guiding questions for constructing such 
    
[^98]: 从分布式机器学习到分布式深度学习的调查

    A Survey From Distributed Machine Learning to Distributed Deep Learning. (arXiv:2307.05232v1 [cs.LG])

    [http://arxiv.org/abs/2307.05232](http://arxiv.org/abs/2307.05232)

    这篇综述论文介绍了分布式机器学习和分布式深度学习的研究现状和方法，强调了分布式深度学习在解决复杂问题方面取得的重要进展。

    

    近年来，人工智能在处理复杂任务方面取得了重大成功。这一成功归功于机器学习算法和硬件加速的进步。为了获得更准确的结果和解决更复杂的问题，算法必须用更多的数据进行训练。这么大量的数据可能需要消耗大量的时间和计算资源来处理。通过将数据和算法分布在多台机器上，可以实现这个解决方案，这被称为分布式机器学习。在分布式机器学习算法方面，已经投入了相当多的努力，目前已经提出了不同的方法。本文通过对这些算法的综述，对该领域的最新状态进行全面总结。我们将这些算法分成分类和聚类（传统机器学习）、深度学习和深度强化学习几组。分布式深度学习的研究也取得了显著的进展。

    Artificial intelligence has achieved significant success in handling complex tasks in recent years. This success is due to advances in machine learning algorithms and hardware acceleration. In order to obtain more accurate results and solve more complex problems, algorithms must be trained with more data. This huge amount of data could be time-consuming to process and require a great deal of computation. This solution could be achieved by distributing the data and algorithm across several machines, which is known as distributed machine learning. There has been considerable effort put into distributed machine learning algorithms, and different methods have been proposed so far. In this article, we present a comprehensive summary of the current state-of-the-art in the field through the review of these algorithms. We divide this algorithms in classification and clustering (traditional machine learning), deep learning and deep reinforcement learning groups. Distributed deep learning has ga
    
[^99]: 零阶非光滑非凸随机优化算法的最优维度依赖性

    An Algorithm with Optimal Dimension-Dependence for Zero-Order Nonsmooth Nonconvex Stochastic Optimization. (arXiv:2307.04504v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2307.04504](http://arxiv.org/abs/2307.04504)

    提出了一个维度依赖优化度为$O(d\delta^{-1}\epsilon^{-3})$的最优算法，并证明了非凸随机零阶设置中非光滑优化与光滑优化的一样容易。

    

    我们研究了使用仅有嘈杂函数评估来产生Lipschitz目标的$(\delta, \epsilon)$-稳定点的复杂度，其中目标可能既不光滑也不凸。最近的研究提出了几种解决这个任务的随机零阶算法，所有这些算法都受到了$\Omega(d^{3/2})$维度依赖性的困扰，其中$d$是问题的维度，这被推测为最优。我们通过提供一个更快的算法来驳斥这个猜想，该算法的复杂度为$O(d\delta^{-1}\epsilon^{-3})$，这是关于$d$的最优（在数值常数上），对于精度参数$\delta, \epsilon$也是最优的，从而解决了Lin等人留下的一个开放问题（NeurIPS'22）。此外，我们算法实现的收敛速度对于光滑目标也是最优的，证明在非凸随机零阶设置中，非光滑优化与光滑优化一样容易。我们提供了实现上述优化的算法。

    We study the complexity of producing $(\delta,\epsilon)$-stationary points of Lipschitz objectives which are possibly neither smooth nor convex, using only noisy function evaluations. Recent works proposed several stochastic zero-order algorithms that solve this task, all of which suffer from a dimension-dependence of $\Omega(d^{3/2})$ where $d$ is the dimension of the problem, which was conjectured to be optimal. We refute this conjecture by providing a faster algorithm that has complexity $O(d\delta^{-1}\epsilon^{-3})$, which is optimal (up to numerical constants) with respect to $d$ and also optimal with respect to the accuracy parameters $\delta,\epsilon$, thus solving an open question due to Lin et al. (NeurIPS'22). Moreover, the convergence rate achieved by our algorithm is also optimal for smooth objectives, proving that in the nonconvex stochastic zero-order setting, nonsmooth optimization is as easy as smooth optimization. We provide algorithms that achieve the aforementioned 
    
[^100]: 朝着可信的解释：因果关系解释论文研究

    Towards Trustworthy Explanation: On Causal Rationalization. (arXiv:2306.14115v1 [cs.LG])

    [http://arxiv.org/abs/2306.14115](http://arxiv.org/abs/2306.14115)

    该论文介绍了一种新的因果关系解释方法，通过在解释中引入非虚假性和效率，从因果推断的角度定义了因果概率，从而建立了必要和充分解释的主要组成部分，相比现有的基于关联的解释方法，这种方法有更加优越的性能表现。

    

    随着自然语言处理的最新进展，解释成为了通过选择输入文本的子集来解释黑盒模型中主要变化的一个基本的自我解释图。然而，现有的基于关联的解释方法在两个或多个片段高度互相关联时无法识别真正的解释，因此对预测准确性提供类似的贡献，所谓的虚假性。为了解决这一限制，我们从因果推断的角度新颖地将两个因果期望值（非虚假性和效率）引入了解释中。我们根据一种新提出的解释结构因果模型定义了一系列的因果概率，通过其理论鉴定，建立了必要和充分解释的主要组成部分。我们在真实世界的评论和医疗数据集上证明了所提出的因果关系解释的优越性能。

    With recent advances in natural language processing, rationalization becomes an essential self-explaining diagram to disentangle the black box by selecting a subset of input texts to account for the major variation in prediction. Yet, existing association-based approaches on rationalization cannot identify true rationales when two or more snippets are highly inter-correlated and thus provide a similar contribution to prediction accuracy, so-called spuriousness. To address this limitation, we novelly leverage two causal desiderata, non-spuriousness and efficiency, into rationalization from the causal inference perspective. We formally define a series of probabilities of causation based on a newly proposed structural causal model of rationalization, with its theoretical identification established as the main component of learning necessary and sufficient rationales. The superior performance of the proposed causal rationalization is demonstrated on real-world review and medical datasets w
    
[^101]: 机载环境影响的度量学习分析

    Aircraft Environmental Impact Segmentation via Metric Learning. (arXiv:2306.13830v1 [cs.LG])

    [http://arxiv.org/abs/2306.13830](http://arxiv.org/abs/2306.13830)

    本文介绍了机载环境影响建模中的度量学习方法，在弱监督度量学习任务下取得了显著性能提升，有望实现对机载环境影响更高效、更精准的建模。

    

    度量学习是指为特定任务学习定制距离度量的过程。这一高级机器学习子领域对于依靠计算对象之间距离或相似度进行机器学习或数据挖掘任务的任何应用都是有用的。近年来，机器学习技术已被广泛应用于航空航天工程中，用于预测、提取模式、发现知识等。然而，度量学习作为一个可以提高复杂机器学习任务性能的元素，迄今在相关文献中很少被使用。本研究将经典的度量学习公式与新颖的元素应用于机载环境影响建模中，并通过弱监督度量学习任务，在新出现的机载环境影响描述和划分问题上实现了显著提高。这一结果将实现对机载环境影响更高效、更精准的建模，对于航空产业的可持续发展具有重要意义。

    Metric learning is the process of learning a tailored distance metric for a particular task. This advanced subfield of machine learning is useful to any machine learning or data mining task that relies on the computation of distances or similarities over objects. In recently years, machine learning techniques have been extensively used in aviation and aerospace engineering to make predictions, extract patterns, discover knowledge, etc. Nevertheless, metric learning, an element that can advance the performance of complex machine learning tasks, has so far been hardly utilized in relevant literature. In this study, we apply classic metric learning formulations with novel components on aviation environmental impact modeling. Through a weakly-supervised metric learning task, we achieve significant improvement in the newly emerged problem of aircraft characterization and segmentation for environmental impacts. The result will enable the more efficient and accurate modeling of aircraft envir
    
[^102]: QNNRepair：量化神经网络修复的方法

    QNNRepair: Quantized Neural Network Repair. (arXiv:2306.13793v1 [cs.LG])

    [http://arxiv.org/abs/2306.13793](http://arxiv.org/abs/2306.13793)

    QNNRepair 是一种用于修复量化神经网络的方法，通过解决神经元权重参数以修复在神经网络量化过程中导致性能下降的神经元，从而在不影响通过测试的性能的同时，提高在失败测试上的性能。

    

    本文提出了QNNRepair，这是文献中第一个修正量化神经网络（QNNs）的方法。其旨在提高在量化之后的神经网络模型的准确性，其接受全精度和权重量化的神经网络以及一个修复数据集。QNNRepair将修复问题转化为线性规划，通过解决神经元权重参数以修复在神经网络量化过程中导致性能下降的神经元，从而在不影响通过测试的性能的同时，提高在失败测试上的性能。

    We present QNNRepair, the first method in the literature for repairing quantized neural networks (QNNs). QNNRepair aims to improve the accuracy of a neural network model after quantization. It accepts the full-precision and weight-quantized neural networks and a repair dataset of passing and failing tests. At first, QNNRepair applies a software fault localization method to identify the neurons that cause performance degradation during neural network quantization. Then, it formulates the repair problem into a linear programming problem of solving neuron weights parameters, which corrects the QNN's performance on failing tests while not compromising its performance on passing tests. We evaluate QNNRepair with widely used neural network architectures such as MobileNetV2, ResNet, and VGGNet on popular datasets, including high-resolution images. We also compare QNNRepair with the state-of-the-art data-free quantization method SQuant. According to the experiment results, we conclude that QNN
    
[^103]: 估算基于证据决策的价值

    Estimating the Value of Evidence-Based Decision Making. (arXiv:2306.13681v1 [stat.ME])

    [http://arxiv.org/abs/2306.13681](http://arxiv.org/abs/2306.13681)

    本文提出了一个实证框架，用于估算证据决策的价值和统计精度投资回报。

    

    商业/政策决策通常基于随机实验和观察性研究的证据。本文提出了一个实证框架来估算基于证据的决策（EBDM）的价值和统计精度投资回报。

    Business/policy decisions are often based on evidence from randomized experiments and observational studies. In this article we propose an empirical framework to estimate the value of evidence-based decision making (EBDM) and the return on the investment in statistical precision.
    
[^104]: 人工智能灾难性风险综述

    An Overview of Catastrophic AI Risks. (arXiv:2306.12001v1 [cs.CY])

    [http://arxiv.org/abs/2306.12001](http://arxiv.org/abs/2306.12001)

    本文综述了人工智能灾难性风险的四个主要来源，包括恶意使用、人工智能竞赛、组织风险和流氓人工智能。

    

    人工智能的快速发展引起了专家、政策制定者和世界各国领导人对越来越先进的人工智能系统可能带来灾难性风险的担忧。虽然已经有很多风险被单独详细介绍过，但迫切需要系统地讨论和说明潜在危险，以更好地支持减轻这些风险的努力。本文概述了人工智能灾难性风险的主要来源，我们将其分为四个类别：恶意使用，即个人或团体有意使用人工智能造成伤害；人工智能竞赛，即竞争环境促使行动者部署不安全的人工智能或放弃控制权交给人工智能；组织风险，突出人为和复杂系统如何增加灾难性事故发生的可能性；以及流氓人工智能，描述了控制比人类智能更高的代理程序困难的固有难题。对于每个风险类别，我们描述了具体的危害。

    Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards,
    
[^105]: 追寻网络独角兽：一个用于开发网络安全问题通用机器学习模型的数据收集平台

    In Search of netUnicorn: A Data-Collection Platform to Develop Generalizable ML Models for Network Security Problems. (arXiv:2306.08853v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2306.08853](http://arxiv.org/abs/2306.08853)

    该论文提出了一个用于网络安全问题的通用机器学习模型开发的数据收集平台，解决了ML模型在不同网络环境中的一般化问题，并提出了一个利用可解释的机器学习工具来指导网络数据收集的增强ML流程。

    

    使用基于机器学习的解决方案解决网络安全问题的显著成功受到了ML模型在不同网络环境中使用时无法保持有效性的影响，因为不同网络环境展现出不同的网络行为。这个问题通常被称为ML模型的一般化问题。社区认识到训练数据集在这个上下文中扮演的关键角色，并开发了各种技术来改善数据集的策划，以克服这个问题。不幸的是，在网络安全领域，这些方法通常不适用，甚至可能产生不现实或质量低下的数据集。为了解决这个问题，我们提出了一个增强的机器学习流程，利用可解释的机器学习工具以迭代的方式指导网络数据的收集。为了确保数据的真实性和质量，我们要求新的数据集在这个迭代过程中应该被内生地收集，从而调整了现有的数据集不足之处。

    The remarkable success of the use of machine learning-based solutions for network security problems has been impeded by the developed ML models' inability to maintain efficacy when used in different network environments exhibiting different network behaviors. This issue is commonly referred to as the generalizability problem of ML models. The community has recognized the critical role that training datasets play in this context and has developed various techniques to improve dataset curation to overcome this problem. Unfortunately, these methods are generally ill-suited or even counterproductive in the network security domain, where they often result in unrealistic or poor-quality datasets.  To address this issue, we propose an augmented ML pipeline that leverages explainable ML tools to guide the network data collection in an iterative fashion. To ensure the data's realism and quality, we require that the new datasets should be endogenously collected in this iterative process, thus ad
    
[^106]: 提升离线到在线强化学习的Q-Ensembles方法

    Improving Offline-to-Online Reinforcement Learning with Q-Ensembles. (arXiv:2306.06871v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06871](http://arxiv.org/abs/2306.06871)

    我们提出了一种名为Q-Ensembles的新框架，通过增加Q网络的数量，无缝地连接离线预训练和在线微调，同时不降低性能。此外，我们适当放宽Q值估计的悲观性，并将基于集合的探索机制融入我们的框架中，从而提升了离线到在线强化学习的性能。

    

    离线强化学习是一种学习范式，代理根据固定的经验数据集进行学习。然而，仅从静态数据集中学习可能限制了性能，因为缺乏探索能力。为了克服这个问题，将离线预训练与在线微调结合起来的离线到在线强化学习方法能够让代理与环境实时交互，进一步完善其策略。然而，现有的离线到在线强化学习方法存在性能下降和在线阶段改进缓慢的问题。为了解决这些挑战，我们提出了一种名为Q-Ensembles的新框架，它通过增加Q网络的数量，无缝地连接离线预训练和在线微调，同时不降低性能。此外，为了加快在线性能提升，我们适当放宽Q值估计的悲观性，并将基于集合的探索机制融入我们的框架中。

    Offline reinforcement learning (RL) is a learning paradigm where an agent learns from a fixed dataset of experience. However, learning solely from a static dataset can limit the performance due to the lack of exploration. To overcome it, offline-to-online RL combines offline pre-training with online fine-tuning, which enables the agent to further refine its policy by interacting with the environment in real-time. Despite its benefits, existing offline-to-online RL methods suffer from performance degradation and slow improvement during the online phase. To tackle these challenges, we propose a novel framework called Ensemble-based Offline-to-Online (E2O) RL. By increasing the number of Q-networks, we seamlessly bridge offline pre-training and online fine-tuning without degrading performance. Moreover, to expedite online performance enhancement, we appropriately loosen the pessimism of Q-value estimation and incorporate ensemble-based exploration mechanisms into our framework. Experiment
    
[^107]: 对于鲁棒的能源预测，持续学习带有时空特征的非分布数据

    Continually learning out-of-distribution spatiotemporal data for robust energy forecasting. (arXiv:2306.06385v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06385](http://arxiv.org/abs/2306.06385)

    该论文提出了一种持续学习方法来对非分布式的时空数据进行能源预测，以适应入住模式的变化，并通过在线学习调整能源使用。

    

    对建筑能源使用进行预测对于促进可持续发展和减少浪费至关重要，因为它使建筑管理者能够优化能源消耗并降低成本。这一重要性在异常期间被放大，例如COVID-19大流行期间，它扰乱了入住模式，使准确的预测更具挑战性。在异常期间预测能源使用是困难的，因为入住模式和能源使用行为发生变化。其中一个主要原因是入住模式分布的转变，许多人在家工作或学习。这就产生了对能够适应变化的入住模式的新预测方法的需求。在线学习已经成为解决这个挑战的有希望的解决方案，因为它使建筑管理者能够适应入住模式的变化，并相应地调整能源使用。通过在线学习，模型可以以每个新数据点为基础进行增量更新，使其能够不断学习。

    Forecasting building energy usage is essential for promoting sustainability and reducing waste, as it enables building managers to optimize energy consumption and reduce costs. This importance is magnified during anomalous periods, such as the COVID-19 pandemic, which have disrupted occupancy patterns and made accurate forecasting more challenging. Forecasting energy usage during anomalous periods is difficult due to changes in occupancy patterns and energy usage behavior. One of the primary reasons for this is the shift in distribution of occupancy patterns, with many people working or learning from home. This has created a need for new forecasting methods that can adapt to changing occupancy patterns. Online learning has emerged as a promising solution to this challenge, as it enables building managers to adapt to changes in occupancy patterns and adjust energy usage accordingly. With online learning, models can be updated incrementally with each new data point, allowing them to lear
    
[^108]: 使用机器学习分析高分辨率数码火星图像

    Analysing high resolution digital Mars images using machine learning. (arXiv:2305.19958v2 [astro-ph.EP] UPDATED)

    [http://arxiv.org/abs/2305.19958](http://arxiv.org/abs/2305.19958)

    本研究利用卷积神经网络应用于高分辨率数码火星图像，以分析寻找火星上的短暂液态水斑块。先前的手动图像分析已经发现37张图像含有较小冰斑，并通过亮度、颜色和与地形遮蔽的强关联性进行了区分。

    

    在火星上寻找短暂液态水的活动是一项持续进行的任务。在火星季节性极地冰帽消退后，由于火星表面和大气层的低热导率，小水冰斑块可能会残留在阴暗的地方。在春末夏初，这些斑块可能会暴露在阳光下，并迅速升温，使液态相出现。为了了解此类冰斑的空间和时间分布，应该对光学图像进行搜索和检查。先前对由火星勘测轨道飞行器上的高分辨率成像科学实验（HiRISE）相机捕获的南半球110张图像进行了手动图像分析。其中，37张图像被鉴定为含有较小冰斑，这些冰斑可通过它们的亮度、颜色和与局部地形遮蔽的强关联性来区分。本研究应用了卷积神经网络（CNN）来寻找更多图像。

    The search for ephemeral liquid water on Mars is an ongoing activity. After the recession of the seasonal polar ice cap on Mars, small water ice patches may be left behind in shady places due to the low thermal conductivity of the Martian surface and atmosphere. During late spring and early summer, these patches may be exposed to direct sunlight and warm up rapidly enough for the liquid phase to emerge. To see the spatial and temporal occurrence of such ice patches, optical images should be searched for and checked. Previously a manual image analysis was conducted on 110 images from the southern hemisphere, captured by the High Resolution Imaging Science Experiment (HiRISE) camera onboard the Mars Reconnaissance Orbiter space mission. Out of these, 37 images were identified with smaller ice patches, which were distinguishable by their brightness, colour and strong connection to local topographic shading. In this study, a convolutional neural network (CNN) is applied to find further ima
    
[^109]: 大型语言模型可能是懒惰的学习者：分析上下文学习中的捷径

    Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning. (arXiv:2305.17256v1 [cs.CL])

    [http://arxiv.org/abs/2305.17256](http://arxiv.org/abs/2305.17256)

    本文探讨了大型语言模型在上下文学习中利用提示中的捷径的依赖性，发现大型模型更有可能在推理过程中利用提示中的捷径，这为评估上下文学习的稳健性和检测和缓解提示中捷径的使用提供了新的视角和挑战。

    

    最近，大型语言模型（LLM）在上下文学习中展现出巨大潜力，其中LLM通过几个输入-标签对（提示）的条件来学习新任务。尽管其潜力巨大，但我们对影响最终任务性能和上下文学习稳健性的因素的理解仍然有限。本文旨在通过研究LLM对提示内捷径或假相关的依赖关系来弥补这一知识差距。通过分类和抽取任务的全面实验，我们揭示了LLM是“懒惰学习者”的事实，它往往利用提示中的捷径来获取下游任务的性能提升。此外，我们还发现一个令人惊讶的发现，即较大的模型更有可能在推理过程中利用提示中的捷径。我们的发现为评估上下文学习的稳健性和检测和缓解提示中捷径的使用提供了新的视角和挑战。

    Large language models (LLMs) have recently shown great potential for in-context learning, where LLMs learn a new task simply by conditioning on a few input-label pairs (prompts). Despite their potential, our understanding of the factors influencing end-task performance and the robustness of in-context learning remains limited. This paper aims to bridge this knowledge gap by investigating the reliance of LLMs on shortcuts or spurious correlations within prompts. Through comprehensive experiments on classification and extraction tasks, we reveal that LLMs are "lazy learners" that tend to exploit shortcuts in prompts for downstream tasks. Additionally, we uncover a surprising finding that larger models are more likely to utilize shortcuts in prompts during inference. Our findings provide a new perspective on evaluating robustness in in-context learning and pose new challenges for detecting and mitigating the use of shortcuts in prompts.
    
[^110]: 具有校准的Aleatoric和Epistemic不确定性的图神经网络相互作用势合奏运算法

    Graph Neural Network Interatomic Potential Ensembles with Calibrated Aleatoric and Epistemic Uncertainty on Energy and Forces. (arXiv:2305.16325v1 [physics.chem-ph])

    [http://arxiv.org/abs/2305.16325](http://arxiv.org/abs/2305.16325)

    本文提出了一个完整的框架来培训和重新校准图神经网络合奏模型，以产生带有校准不确定性估计的能量和力的准确预测，可以应用于材料的结构优化和分子动力学模拟。

    

    廉价的机器学习势场越来越被用于加速材料的结构优化和分子动力学模拟，通过迭代性地预测和应用原子间力。在这些情况下，检测到预测的不可靠性以避免错误或误导性结果至关重要。本文提出了一个完整的框架来培训和重新校准图神经网络合奏模型，以产生带有校准不确定性估计的能量和力的准确预测。所提出的方法考虑到了Epistemic和Aleatoric不确定性，通过非线性缩放函数在后期重新调整了总不确定性，从而在以前未见过的数据上实现良好的校准性能，而不会损失预测准确性。该方法在两个具有挑战性且公开可用的数据集上进行了演示和评估，ANI-1x（Smith等人）和Transition1x（Schreiner等人），这两个数据集都包含远离平衡状态的多样化构象。

    Inexpensive machine learning potentials are increasingly being used to speed up structural optimization and molecular dynamics simulations of materials by iteratively predicting and applying interatomic forces. In these settings, it is crucial to detect when predictions are unreliable to avoid wrong or misleading results. Here, we present a complete framework for training and recalibrating graph neural network ensemble models to produce accurate predictions of energy and forces with calibrated uncertainty estimates. The proposed method considers both epistemic and aleatoric uncertainty and the total uncertainties are recalibrated post hoc using a nonlinear scaling function to achieve good calibration on previously unseen data, without loss of predictive accuracy. The method is demonstrated and evaluated on two challenging, publicly available datasets, ANI-1x (Smith et al.) and Transition1x (Schreiner et al.), both containing diverse conformations far from equilibrium. A detailed analys
    
[^111]: 以概念为中心的Transformer：具有面向物体的概念学习，以实现可解释性。

    Concept-Centric Transformers: Concept Transformers with Object-Centric Concept Learning for Interpretability. (arXiv:2305.15775v1 [cs.LG])

    [http://arxiv.org/abs/2305.15775](http://arxiv.org/abs/2305.15775)

    本文研究了以物体为中心的概念学习，它可以提高基于概念的Transformer模型的分类性能和可解释性。

    

    注意机制大大提高了深度学习模型在视觉、NLP和多模态任务上的性能，同时也提供了工具来帮助模型的可解释性。最近提出的概念Transformer（CT）将Transformer的注意力机制从低级输入特征泛化到更抽象的中间层潜在概念，更好地允许人类分析员直接评估解释关于任何特定输出分类的推理。然而，CT采用的概念学习默认假设类别中的每个图像都对表征该类别的概念作出了相同的贡献，而使用以物体为中心的概念可能会导致更好的分类结果。

    Attention mechanisms have greatly improved the performance of deep-learning models on visual, NLP, and multimodal tasks while also providing tools to aid in the model's interpretability. In particular, attention scores over input regions or concrete image features can be used to measure how much the attended elements contribute to the model inference. The recently proposed Concept Transformer (CT) generalizes the Transformer attention mechanism from such low-level input features to more abstract, intermediate-level latent concepts that better allow human analysts to more directly assess an explanation for the reasoning of the model about any particular output classification. However, the concept learning employed by CT implicitly assumes that across every image in a class, each image patch makes the same contribution to concepts that characterize membership in that class. Instead of using the CT's image-patch-centric concepts, object-centric concepts could lead to better classification
    
[^112]: 后续前导内在探索

    Successor-Predecessor Intrinsic Exploration. (arXiv:2305.15277v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15277](http://arxiv.org/abs/2305.15277)

    后续前导内在探索是一种基于新颖内在奖励的探索算法，它利用回顾信息并结合前瞻信息，以在强化学习中实现高效且生态学合理的探索行为。

    

    在强化学习中，探索对于那些外部奖励稀缺的环境非常重要。本文侧重于利用内在奖励进行探索，即代理器使用自我生成的内在奖励临时增加外部奖励。尽管内在奖励的研究有着悠久的历史，但现有的方法都集中于根据状态的未来前景度量来构成内在奖励，忽视了转移序列的回顾结构中所蕴含的信息。在本文中，我们认为代理器可以利用回顾信息来生成具有结构感知能力的探索行为，以基于整体而非局部信息的方式实现高效的探索。我们提出了一种基于新颖内在奖励的模型——后续前导内在探索 (SPIE) 算法。实验结果表明，SPIE 能够产生更加高效和生态学合理的探索行为。

    Exploration is essential in reinforcement learning, particularly in environments where external rewards are sparse. Here we focus on exploration with intrinsic rewards, where the agent transiently augments the external rewards with self-generated intrinsic rewards. Although the study of intrinsic rewards has a long history, existing methods focus on composing the intrinsic reward based on measures of future prospects of states, ignoring the information contained in the retrospective structure of transition sequences. Here we argue that the agent can utilise retrospective information to generate explorative behaviour with structure-awareness, facilitating efficient exploration based on global instead of local information. We propose Successor-Predecessor Intrinsic Exploration (SPIE), an exploration algorithm based on a novel intrinsic reward combining prospective and retrospective information. We show that SPIE yields more efficient and ethologically plausible exploratory behaviour in e
    
[^113]: 基于神经网络的贝叶斯数值积分方法

    Bayesian Numerical Integration with Neural Networks. (arXiv:2305.13248v1 [stat.ML])

    [http://arxiv.org/abs/2305.13248](http://arxiv.org/abs/2305.13248)

    本文提出了一种基于神经网络架构的贝叶斯数值积分方法，称为贝叶斯 Stein 神经网络。该方法可高效地编码积分先验信息并计算积分估计的不确定性。在实际问题中，该方法展现出数量级的加速的优势。

    

    贝叶斯概率数值方法对于数值积分具有显著优势：可以编码积分的先验信息，可以量化积分估计的不确定性。但是，这类方法中最流行的贝叶斯积分算法（Bayesian Quadrature）基于高斯过程模型，因此计算成本很高。为了提高可扩展性，我们提出了一种基于贝叶斯神经网络的替代方法，称为贝叶斯 Stein 神经网络。关键成分是基于 Stein 算子的神经网络结构，以及基于 Laplace 近似的贝叶斯后验的近似。我们展示了这导致了在流行的 Genz 函数基准测试和在贝叶斯动力系统分析以及大规模风力发电预测中规模的数量级加速。

    Bayesian probabilistic numerical methods for numerical integration offer significant advantages over their non-Bayesian counterparts: they can encode prior information about the integrand, and can quantify uncertainty over estimates of an integral. However, the most popular algorithm in this class, Bayesian quadrature, is based on Gaussian process models and is therefore associated with a high computational cost. To improve scalability, we propose an alternative approach based on Bayesian neural networks which we call Bayesian Stein networks. The key ingredients are a neural network architecture based on Stein operators, and an approximation of the Bayesian posterior based on the Laplace approximation. We show that this leads to orders of magnitude speed-ups on the popular Genz functions benchmark, and on challenging problems arising in the Bayesian analysis of dynamical systems, and the prediction of energy production for a large-scale wind farm.
    
[^114]: 谷歌地图中的大规模可扩展逆强化学习

    Massively Scalable Inverse Reinforcement Learning in Google Maps. (arXiv:2305.11290v1 [cs.LG])

    [http://arxiv.org/abs/2305.11290](http://arxiv.org/abs/2305.11290)

    本文提出了一种新的逆强化学习算法（RHIP），通过图压缩、并行化和基于主特征向量的问题初始化解决了全球规模的MDPs、大型数据集和高度参数化的模型的问题，在谷歌地图中实现了16-24%的全球路线质量改进。

    

    优化人类潜在偏好是路线推荐中的巨大挑战，全球可扩展解决方案仍是一个未解决的问题。尽管过去的研究为逆强化学习的应用创建了越来越通用的解决方案，但这些解决方案尚未成功扩展到世界规模的MDP、大型数据集和高度参数化的模型，分别涉及数亿个状态、轨迹和参数。本文通过一系列的改进，聚焦于图压缩、并行化和基于主特征向量的问题初始化，突破以往的限制。我们引入了逆向规划递进地平面(RHIP)，它可以概括现有的工作，并通过其规划水平控制关键性能平衡。我们的策略在全球路线质量方面实现了16-24%的改进，就我们所知，它是迄今为止实现在真实世界环境下的最大的逆强化学习示例。我们的结果显示了更好的导航行为和路径规划。

    Optimizing for humans' latent preferences is a grand challenge in route recommendation, where globally-scalable solutions remain an open problem. Although past work created increasingly general solutions for the application of inverse reinforcement learning (IRL), these have not been successfully scaled to world-sized MDPs, large datasets, and highly parameterized models; respectively hundreds of millions of states, trajectories, and parameters. In this work, we surpass previous limitations through a series of advancements focused on graph compression, parallelization, and problem initialization based on dominant eigenvectors. We introduce Receding Horizon Inverse Planning (RHIP), which generalizes existing work and enables control of key performance trade-offs via its planning horizon. Our policy achieves a 16-24% improvement in global route quality, and, to our knowledge, represents the largest instance of IRL in a real-world setting to date. Our results show critical benefits to mor
    
[^115]: 文档理解数据集和评估（DUDE）

    Document Understanding Dataset and Evaluation (DUDE). (arXiv:2305.08455v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.08455](http://arxiv.org/abs/2305.08455)

    DUDE推出了一个新的数据集和评估方法，旨在创造一个更实际的基准测试并推动当前方法的边界，以更准确地模拟真实世界的情况

    

    我们呼吁文档AI社区重新评估当前的方法论，拥抱创建更实际取向的基准测试的挑战。文档理解数据集和评估（DUDE）旨在纠正在理解视觉丰富文档（VRD）方面的研究进展停滞不前的情况。我们提供了一个新的数据集，其中包括与多行业、多领域和多页VRD相关的问题类型、答案和文档布局的创新，具有各种来源和日期。此外，我们通过创建多任务和多领域的评估设置来推动当前方法的边界，这些设置更准确地模拟了真实世界的情况，在这些情况下需要在低资源环境下进行强大的泛化和适应。DUDE旨在成为一个更实际、更长期的基准测试标准，并希望它会引领未来的扩展和贡献，以应对真实世界的挑战。最后，我们的工作说明了以下重要性。

    We call on the Document AI (DocAI) community to reevaluate current methodologies and embrace the challenge of creating more practically-oriented benchmarks. Document Understanding Dataset and Evaluation (DUDE) seeks to remediate the halted research progress in understanding visually-rich documents (VRDs). We present a new dataset with novelties related to types of questions, answers, and document layouts based on multi-industry, multi-domain, and multi-page VRDs of various origins, and dates. Moreover, we are pushing the boundaries of current methods by creating multi-task and multi-domain evaluation setups that more accurately simulate real-world situations where powerful generalization and adaptation under low-resource settings are desired. DUDE aims to set a new standard as a more practical, long-standing benchmark for the community, and we hope that it will lead to future extensions and contributions that address real-world challenges. Finally, our work illustrates the importance o
    
[^116]: 应用于概率时间序列填充的Schr\"odinger bridge问题的收敛性分析和算法

    Provably Convergent Schr\"odinger Bridge with Applications to Probabilistic Time Series Imputation. (arXiv:2305.07247v1 [cs.LG])

    [http://arxiv.org/abs/2305.07247](http://arxiv.org/abs/2305.07247)

    本论文提出了一种基于近似投影的Schr\"odinger bridge算法，它能够应用于概率时间序列填充，并在医疗保健和环境数据方面实现最先进的结果。

    

    Schr\"odinger bridge问题（SBP）在生成建模中引起了越来越多的关注。然而，近似的投影是唯一可用的，其收敛性还不是十分清楚。我们提出了一种基于近似投影的Schr\"odinger bridge算法的第一个收敛分析。我们将SBP应用于概率时间序列填充，展示了优化传输成本可以提高性能，提出的算法在医疗保健和环境数据方面实现了最先进的结果。

    The Schr\"odinger bridge problem (SBP) is gaining increasing attention in generative modeling and showing promising potential even in comparison with the score-based generative models (SGMs). SBP can be interpreted as an entropy-regularized optimal transport problem, which conducts projections onto every other marginal alternatingly. However, in practice, only approximated projections are accessible and their convergence is not well understood. To fill this gap, we present a first convergence analysis of the Schr\"odinger bridge algorithm based on approximated projections. As for its practical applications, we apply SBP to probabilistic time series imputation by generating missing values conditioned on observed data. We show that optimizing the transport cost improves the performance and the proposed algorithm achieves the state-of-the-art result in healthcare and environmental data while exhibiting the advantage of exploring both temporal and feature patterns in probabilistic time ser
    
[^117]: ST-GIN:一种具有时空图注意力和双向循环联合神经网络的交通数据插值不确定性量化方法

    ST-GIN: An Uncertainty Quantification Approach in Traffic Data Imputation with Spatio-temporal Graph Attention and Bidirectional Recurrent United Neural Networks. (arXiv:2305.06480v1 [cs.LG])

    [http://arxiv.org/abs/2305.06480](http://arxiv.org/abs/2305.06480)

    本研究提出了一种创新的交通数据插值方法，利用图注意力和双向神经网络捕捉时空相关性，实验结果表明在处理缺失值方面优于其他基准技术。

    

    交通数据是智能交通系统中的基本组成部分。然而，从环路检测器或类似来源收集的现实世界交通数据通常包含缺失值(MVs)，这可能会对相关应用和研究产生不利影响。为了恢复这些缺失值，研究人员利用数字统计、张量分解和深度学习技术等方法实现了数据插值。本文提出了一种创新的深度学习方法用于插值缺失数据。该方法采用图注意架构来捕捉交通数据中存在的空间相关性，同时利用双向神经网络学习时间信息。实验结果表明，我们提出的方法优于所有其他基准技术，证明其有效性。

    Traffic data serves as a fundamental component in both research and applications within intelligent transportation systems. However, real-world transportation data, collected from loop detectors or similar sources, often contain missing values (MVs), which can adversely impact associated applications and research. Instead of discarding this incomplete data, researchers have sought to recover these missing values through numerical statistics, tensor decomposition, and deep learning techniques. In this paper, we propose an innovative deep-learning approach for imputing missing data. A graph attention architecture is employed to capture the spatial correlations present in traffic data, while a bidirectional neural network is utilized to learn temporal information. Experimental results indicate that our proposed method outperforms all other benchmark techniques, thus demonstrating its effectiveness.
    
[^118]: 早起的鸟儿捉到虫：利用编码器模型的早期层进行更有效的代码分类

    The EarlyBIRD Catches the Bug: On Exploiting Early Layers of Encoder Models for More Efficient Code Classification. (arXiv:2305.04940v1 [cs.SE])

    [http://arxiv.org/abs/2305.04940](http://arxiv.org/abs/2305.04940)

    本文介绍了一种早期层组合的方法EarlyBIRD，该方法可以有效利用深度自然语言处理模型的资源和可用信息，从而提高代码分类的性能，在缺陷检测方面平均可提高2个点。

    

    现代自然语言处理技术在软件工程任务如漏洞检测和类型推理方面表现出了卓越的优势。然而，训练深度自然语言处理模型需要大量计算资源。本文探讨了一些技术，旨在实现这些模型中资源和可用信息的最佳利用。我们提出了一种通用的方法EarlyBIRD，从预训练的transformer模型的早期层构建代码的复合表示。我们通过比较12种创建复合表示的策略与仅使用最后一个编码器层的标准实践，在CodeBERT模型上实证研究了这种方法的可行性。我们在4个数据集上的评估表明，几个早期层的组合在缺陷检测方面产生更好的性能，而一些组合则改进了多类分类。具体而言，我们获得了平均检测增强2。

    The use of modern Natural Language Processing (NLP) techniques has shown to be beneficial for software engineering tasks, such as vulnerability detection and type inference. However, training deep NLP models requires significant computational resources. This paper explores techniques that aim at achieving the best usage of resources and available information in these models.  We propose a generic approach, EarlyBIRD, to build composite representations of code from the early layers of a pre-trained transformer model. We empirically investigate the viability of this approach on the CodeBERT model by comparing the performance of 12 strategies for creating composite representations with the standard practice of only using the last encoder layer.  Our evaluation on four datasets shows that several early layer combinations yield better performance on defect detection, and some combinations improve multi-class classification. More specifically, we obtain a +2 average improvement of detection 
    
[^119]: 基于匹配的生成模型数据估值方法

    Matching-based Data Valuation for Generative Model. (arXiv:2304.10701v1 [cs.CV])

    [http://arxiv.org/abs/2304.10701](http://arxiv.org/abs/2304.10701)

    本论文提出了基于匹配的生成模型数据估值方法，这是一个针对任何生成模型的模型无关方法，可以对数据实例进行估值，而无需重新训练模型，并在估值效果上表现出色。

    

    数据估值对于机器学习非常重要，因为它有助于增强模型的透明度并保护数据特性。现有的数据估值方法主要集中在判别模型上，忽略了最近吸引了大量关注的深度生成模型。与判别模型类似，需要评估深度生成模型中数据贡献的紧迫需求也存在。然而，以往的数据估值方法主要依赖于判别模型性能指标，并需要对模型进行重新训练。因此，它们不能在实际中直接高效地应用于近期的深度生成模型，例如生成对抗网络和扩散模型。为了弥补这一差距，我们从相似性匹配的角度对生成模型中的数据估值问题进行了构建。具体地，我们引入了“Generative Model Valuator”（GMValuator）——第一个针对任何生成模型的模型无关方法，旨在为生成模型提供数据估值而无需重新训练模型。我们的方法利用数据实例及由生成模型生成的相应合成实例之间的相似度来估计原始数据的价值。大量实验证明了我们的方法在为不同的生成模型（包括GAN和扩散模型）评估数据实例方面的优越性。

    Data valuation is critical in machine learning, as it helps enhance model transparency and protect data properties. Existing data valuation methods have primarily focused on discriminative models, neglecting deep generative models that have recently gained considerable attention. Similar to discriminative models, there is an urgent need to assess data contributions in deep generative models as well. However, previous data valuation approaches mainly relied on discriminative model performance metrics and required model retraining. Consequently, they cannot be applied directly and efficiently to recent deep generative models, such as generative adversarial networks and diffusion models, in practice. To bridge this gap, we formulate the data valuation problem in generative models from a similarity-matching perspective. Specifically, we introduce Generative Model Valuator (GMValuator), the first model-agnostic approach for any generative models, designed to provide data valuation for gener
    
[^120]: SURFSUP：学习新颖表面流体模拟

    SURFSUP: Learning Fluid Simulation for Novel Surfaces. (arXiv:2304.06197v1 [cs.LG])

    [http://arxiv.org/abs/2304.06197](http://arxiv.org/abs/2304.06197)

    SURFSUP采用有符号距离函数连续表示对象，提供更准确和高效的流体对象相互作用的学习方法；且能够适用于分布之外的复杂真实场景和对象，可以反演适用于物体操纵流体流动。

    

    在设计、图形和机器人领域，模拟复杂场景中流体的力学是至关重要的。基于学习的方法提供了快速和可微的流体模拟器，但是先前的大部分工作不能精确地模拟流体如何与在训练中未见过的新颖表面相互作用。我们引入了SURFSUP，这是一个使用有符号距离函数（SDF）隐式表示对象的框架，而不是显式表示网格或粒子。几何体的这种连续表示使得在长时间段内更准确地模拟流体对象相互作用成为可能，同时使计算更加高效。此外，SURFSUP在简单的形状基元上训练，能够广泛地适用于分布之外，甚至适用于复杂的真实场景和对象。最后，我们展示了我们可以反演我们的模型来设计简单的物体来操纵流体流动。

    Modeling the mechanics of fluid in complex scenes is vital to applications in design, graphics, and robotics. Learning-based methods provide fast and differentiable fluid simulators, however most prior work is unable to accurately model how fluids interact with genuinely novel surfaces not seen during training. We introduce SURFSUP, a framework that represents objects implicitly using signed distance functions (SDFs), rather than an explicit representation of meshes or particles. This continuous representation of geometry enables more accurate simulation of fluid-object interactions over long time periods while simultaneously making computation more efficient. Moreover, SURFSUP trained on simple shape primitives generalizes considerably out-of-distribution, even to complex real-world scenes and objects. Finally, we show we can invert our model to design simple objects to manipulate fluid flow.
    
[^121]: 基于聚类的系统识别学习个性化模型

    Learning Personalized Models with Clustered System Identification. (arXiv:2304.01395v1 [math.OC])

    [http://arxiv.org/abs/2304.01395](http://arxiv.org/abs/2304.01395)

    该文提出了一种基于聚类的系统识别学习个性化模型的算法，将多个系统划分为群集，同一簇中的系统可以从其他系统的观察中获益。该算法实现了正确估计群集标识并具有高效和个性化的系统识别过程。

    

    我们解决从不同系统动力学观察多个轨迹中学习线性系统模型的问题。这个框架包括协作场景，其中寻求估计其动力学的多个系统被划分为根据其系统相似性的群集。因此，同一簇中的系统可以从其他系统的观察中获益。考虑到这个框架，我们提出了一个算法，每个系统交替估计其群集标识并执行动态估计。然后聚合以更新每个群集的模型。我们证明，在温和的假设下，我们的算法正确地估计了群集标识，并实现了近似样本复杂度，其与群集中系统数成反比，从而促进了更高效和个性化的系统识别过程。

    We address the problem of learning linear system models from observing multiple trajectories from different system dynamics. This framework encompasses a collaborative scenario where several systems seeking to estimate their dynamics are partitioned into clusters according to their system similarity. Thus, the systems within the same cluster can benefit from the observations made by the others. Considering this framework, we present an algorithm where each system alternately estimates its cluster identity and performs an estimation of its dynamics. This is then aggregated to update the model of each cluster. We show that under mild assumptions, our algorithm correctly estimates the cluster identities and achieves an approximate sample complexity that scales inversely with the number of systems in the cluster, thus facilitating a more efficient and personalized system identification process.
    
[^122]: 无需强化学习的逆强化学习

    Inverse Reinforcement Learning without Reinforcement Learning. (arXiv:2303.14623v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.14623](http://arxiv.org/abs/2303.14623)

    该论文提出了一种新的逆强化学习简化方法，通过利用专家的状态分布来减少强化学习子例程的全局探索部分，实现了指数级的加速，大大提高了样本复杂度和时间复杂度的效果。

    

    逆强化学习 (IRL) 是一种强大的模仿学习技术，旨在学习合乎逻辑的专家演示的奖励函数。然而，传统的IRL方法存在计算上的弱点：它们需要将解决难度高的强化学习（RL）问题作为子例程进行反复求解。这与归约的观点相矛盾：我们已将模仿学习的较易问题归约为反复解决强化学习的更难问题。另一方面的工作证明，访问强策略花费时间的状态分布的侧面信息可以大大降低解决RL问题的样本和计算复杂度。在本研究中，我们首次展示了一种更加明智的模仿学习简化方法，利用专家的状态分布来缓解RL子例程的全局探索部分，理论上提供了指数级的加速。实际上，我们的算法在多个基准任务中在样本复杂度和时间复杂度方面都显著优于现有的IRL方法。

    Inverse Reinforcement Learning (IRL) is a powerful set of techniques for imitation learning that aims to learn a reward function that rationalizes expert demonstrations. Unfortunately, traditional IRL methods suffer from a computational weakness: they require repeatedly solving a hard reinforcement learning (RL) problem as a subroutine. This is counter-intuitive from the viewpoint of reductions: we have reduced the easier problem of imitation learning to repeatedly solving the harder problem of RL. Another thread of work has proved that access to the side-information of the distribution of states where a strong policy spends time can dramatically reduce the sample and computational complexities of solving an RL problem. In this work, we demonstrate for the first time a more informed imitation learning reduction where we utilize the state distribution of the expert to alleviate the global exploration component of the RL subroutine, providing an exponential speedup in theory. In practice
    
[^123]: 元宇宙中的深度伪造: 对虚拟游戏、会议和办公场所的安全影响

    Deepfake in the Metaverse: Security Implications for Virtual Gaming, Meetings, and Offices. (arXiv:2303.14612v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2303.14612](http://arxiv.org/abs/2303.14612)

    元宇宙中的深度伪造带来了严重的安全影响，特别是在虚拟游戏、会议和办公场所等场景中，攻击者可以通过深度伪造冒充他人进行攻击。

    

    元宇宙由于其创造完全沉浸式和交互式虚拟世界的潜力，引起了各行各业的广泛关注。然而，深度伪造在元宇宙中的整合带来了严重的安全影响，特别是冒充。本文研究了深度伪造在元宇宙中的安全影响，特别是在游戏、在线会议和虚拟办公场所的背景下。本文讨论了深度伪造如何用于冒充游戏场景中的角色，如何在元宇宙的在线会议中打开冒充的大门，以及在元宇宙的虚拟办公场所缺乏身份认证，使得攻击者更容易冒充他人。本文讨论了这些安全问题对机密性、完整性和可用性（CIA三元性）的影响。本文进一步探讨了与「暗黑宇宙」和数字复制等相关问题，以及与此相关的监管和隐私问题。

    The metaverse has gained significant attention from various industries due to its potential to create a fully immersive and interactive virtual world. However, the integration of deepfakes in the metaverse brings serious security implications, particularly with regard to impersonation. This paper examines the security implications of deepfakes in the metaverse, specifically in the context of gaming, online meetings, and virtual offices. The paper discusses how deepfakes can be used to impersonate in gaming scenarios, how online meetings in the metaverse open the door for impersonation, and how virtual offices in the metaverse lack physical authentication, making it easier for attackers to impersonate someone. The implications of these security concerns are discussed in relation to the confidentiality, integrity, and availability (CIA) triad. The paper further explores related issues such as the darkverse, and digital cloning, as well as regulatory and privacy concerns associated with a
    
[^124]: 通过 Numerai 数据科学竞赛案例，理解时间表格和多变量时间序列的模型复杂度

    Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament. (arXiv:2303.07925v1 [cs.LG])

    [http://arxiv.org/abs/2303.07925](http://arxiv.org/abs/2303.07925)

    本文采用 Numerai 数据科学竞赛的数据，探究了多变量时间序列建模中不同特征工程和降维方法的应用；提出了一种新的集成方法，用于高维时间序列建模，该方法在通用性、鲁棒性和效率上优于一些深度学习模型。

    

    本文探究了在多变量时间序列建模中使用不同特征工程和降维方法的应用。利用从 Numerai 数据竞赛创建的特征目标交叉相关时间序列数据集，我们证明在过度参数化的情况下，不同特征工程方法的性能与预测会收敛到可由再生核希尔伯特空间刻画的相同平衡态。我们提出了一种新的集成方法，该方法结合了不同的随机非线性变换，随后采用岭回归模型进行高维时间序列建模。与一些常用的用于序列建模的深度学习模型（如 LSTM 和 transformer）相比，我们的方法更加鲁棒（在不同的随机种子下具有较低的模型方差，且对架构的选择不太敏感），并且更有效率。我们方法的另一个优势在于模型的简单性，因为没有必要使用复杂的深度学习框架。

    In this paper, we explore the use of different feature engineering and dimensionality reduction methods in multi-variate time-series modelling. Using a feature-target cross correlation time series dataset created from Numerai tournament, we demonstrate under over-parameterised regime, both the performance and predictions from different feature engineering methods converge to the same equilibrium, which can be characterised by the reproducing kernel Hilbert space. We suggest a new Ensemble method, which combines different random non-linear transforms followed by ridge regression for modelling high dimensional time-series. Compared to some commonly used deep learning models for sequence modelling, such as LSTM and transformers, our method is more robust (lower model variance over different random seeds and less sensitive to the choice of architecture) and more efficient. An additional advantage of our method is model simplicity as there is no need to use sophisticated deep learning frame
    
[^125]: TSMixer：一种全MLP架构用于时间序列预测

    TSMixer: An all-MLP Architecture for Time Series Forecasting. (arXiv:2303.06053v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06053](http://arxiv.org/abs/2303.06053)

    TSMixer是一种通过堆叠多层感知器（MLP）设计的新型结构，基于沿时间和特征维度的混合操作，能够在时间序列预测中表现出极好的性能。

    

    实际时间序列数据集通常是多变量且具有复杂的动态。为了捕获这种复杂性，像循环或基于注意力的顺序深度学习模型这样的高容量结构变得受欢迎。然而，最近的研究表明，简单的单变量线性模型可以在几个常用的学术基准测试中胜过这样的深度学习模型。扩展它们，本文研究线性模型在时间序列预测中的能力，并提出了时序混合器（TSMixer），这是一种通过堆叠多层感知器（MLP）设计的新型结构。 TSMixer基于沿时间和特征维度的混合操作，以有效地提取信息。在流行的学术基准测试上，简单易行的TSMixer与利用特定基准的归纳偏差的专业先进模型相媲美。在具有挑战性和大规模的M5基准测试中，即一个实际的零售数据集上，TSMixer表现出非常出色的性能。

    Real-world time-series datasets are often multivariate with complex dynamics. To capture this complexity, high capacity architectures like recurrent- or attention-based sequential deep learning models have become popular. However, recent work demonstrates that simple univariate linear models can outperform such deep learning models on several commonly used academic benchmarks. Extending them, in this paper, we investigate the capabilities of linear models for time-series forecasting and present Time-Series Mixer (TSMixer), a novel architecture designed by stacking multi-layer perceptrons (MLPs). TSMixer is based on mixing operations along both the time and feature dimensions to extract information efficiently. On popular academic benchmarks, the simple-to-implement TSMixer is comparable to specialized state-of-the-art models that leverage the inductive biases of specific benchmarks. On the challenging and large scale M5 benchmark, a real-world retail dataset, TSMixer demonstrates super
    
[^126]: 在线控制屏障函数用于分散式多智能体导航

    Online Control Barrier Functions for Decentralized Multi-Agent Navigation. (arXiv:2303.04313v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.04313](http://arxiv.org/abs/2303.04313)

    本文提出了一种在连续域中实现可保证安全的多智能体导航的方法，通过在线调整CBF的超参数来适应不同的环境条件，使用强化学习和图神经网络实现无模型的CBF调整策略。

    

    控制屏障函数(CBF)在连续域中实现了可保证安全的多智能体导航。然而，得到的导航性能高度敏感于底层超参数。传统方法考虑了固定的CBF（参数预先调整），因此通常在杂乱和高动态环境中表现不佳：保守的参数值可能导致代理轨迹低效，甚至无法达到目标位置，而激进的参数值可能导致反应不可行。为了解决这些问题，本文提出了在线CBFs，其中超参数随着代理在其邻近环境中的感知实时调整。由于CBFs和导航性能之间的显式关系很难建模，我们利用强化学习以无模型的方式学习CBF调整策略。由于我们使用图神经网络(GNN)对策略进行参数化，我们可以实时适应不同的环境条件。

    Control barrier functions (CBFs) enable guaranteed safe multi-agent navigation in the continuous domain. The resulting navigation performance, however, is highly sensitive to the underlying hyperparameters. Traditional approaches consider fixed CBFs (where parameters are tuned apriori), and hence, typically do not perform well in cluttered and highly dynamic environments: conservative parameter values can lead to inefficient agent trajectories, or even failure to reach goal positions, whereas aggressive parameter values can lead to infeasible controls. To overcome these issues, in this paper, we propose online CBFs, whereby hyperparameters are tuned in real-time, as a function of what agents perceive in their immediate neighborhood. Since the explicit relationship between CBFs and navigation performance is hard to model, we leverage reinforcement learning to learn CBF-tuning policies in a model-free manner. Because we parameterize the policies with graph neural networks (GNNs), we are 
    
[^127]: 用好奇心交叉熵方法和对比学习进行的样本高效实时规划

    Sample-efficient Real-time Planning with Curiosity Cross-Entropy Method and Contrastive Learning. (arXiv:2303.03787v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03787](http://arxiv.org/abs/2303.03787)

    本文提出了一种用好奇心触发探索的改进版本的交叉熵方法（CCEM），以解决现有规划方法在复杂高维环境下无法扩展的问题。通过最大化规划视角内状态-动作Q值的和，我们的方法鼓励达到新颖的观察，并利用对比表示学习提高性能。

    

    基于模型的强化学习（MBRL）与实时规划在运动和操作控制任务中显示出巨大潜力。然而，现有的规划方法，如交叉熵方法（CEM），在复杂的高维环境中无法很好地扩展。导致性能不佳的一个关键原因是缺乏探索，因为这些规划方法只旨在最大化规划视角内的累积外在奖励。此外，在没有观察到的紧凑潜在空间内进行规划使得使用基于好奇心的内在动机变得具有挑战性。我们提出了好奇心交叉熵方法（CCEM），这是对CEM算法的改进版本，用于通过好奇心鼓励探索。我们的方法在规划视角内最大化状态-动作Q值的和，其中这些Q值估计未来的外在和内在奖励，从而鼓励达到新颖的观察。此外，我们的模型使用对比表示学习。

    Model-based reinforcement learning (MBRL) with real-time planning has shown great potential in locomotion and manipulation control tasks. However, the existing planning methods, such as the Cross-Entropy Method (CEM), do not scale well to complex high-dimensional environments. One of the key reasons for underperformance is the lack of exploration, as these planning methods only aim to maximize the cumulative extrinsic reward over the planning horizon. Furthermore, planning inside the compact latent space in the absence of observations makes it challenging to use curiosity-based intrinsic motivation. We propose Curiosity CEM (CCEM), an improved version of the CEM algorithm for encouraging exploration via curiosity. Our proposed method maximizes the sum of state-action Q values over the planning horizon, in which these Q values estimate the future extrinsic and intrinsic reward, hence encouraging reaching novel observations. In addition, our model uses contrastive representation learning
    
[^128]: 为什么面部深度伪造检测器会失败？

    Why Do Facial Deepfake Detectors Fail?. (arXiv:2302.13156v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2302.13156](http://arxiv.org/abs/2302.13156)

    近年来快速发展的深度伪造技术对人类身份验证提出了重大挑战，已提出多种深度伪造检测算法，但检测器经常不可靠且无法检测到深度伪造内容。这项研究强调了检测器面临的挑战，包括预处理中的伪造痕迹和未考虑到新样本生成器。需要进一步研究和开发以创建更稳健可靠的深度伪造检测器。

    

    近年来，深度伪造技术快速发展，使得创建高度逼真的伪造媒体成为可能，包括视频、图像和音频。这些材料对人类身份验证提出了重大挑战，例如冒充、误导甚至对国家安全构成威胁。为了跟上这些快速发展，提出了多种深度伪造检测算法，从而引发了伪造者和检测器之间的持续博弈。然而，这些检测器经常存在不可靠性，并且经常无法检测到深度伪造的内容。本研究重点介绍了深度伪造检测器面临的挑战，包括（1）预处理流程中的伪造图像痕迹（artifact）和（2）在构建防御模型时未考虑到生成新的、未见过的深度伪造样本。我们的工作揭示了在这个领域需要进一步的研究和开发，以创建更为稳健可靠的检测器。

    Recent rapid advancements in deepfake technology have allowed the creation of highly realistic fake media, such as video, image, and audio. These materials pose significant challenges to human authentication, such as impersonation, misinformation, or even a threat to national security. To keep pace with these rapid advancements, several deepfake detection algorithms have been proposed, leading to an ongoing arms race between deepfake creators and deepfake detectors. Nevertheless, these detectors are often unreliable and frequently fail to detect deepfakes. This study highlights the challenges they face in detecting deepfakes, including (1) the pre-processing pipeline of artifacts and (2) the fact that generators of new, unseen deepfake samples have not been considered when building the defense models. Our work sheds light on the need for further research and development in this field to create more robust and reliable detectors.
    
[^129]: 缓解深假检测中的对抗攻击：对扰动和人工智能技术的探索

    Mitigating Adversarial Attacks in Deepfake Detection: An Exploration of Perturbation and AI Techniques. (arXiv:2302.11704v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11704](http://arxiv.org/abs/2302.11704)

    本论文探索了扰动和人工智能技术在深假检测中缓解对抗攻击的方法，这是一项迫切需要解决的安全和道德问题。

    

    深度学习在机器学习领域起着重要作用，具有识别图像、自然语言处理等任务的显著能力。然而，这种强大的特性也使得深度学习模型容易受到对抗性示例的攻击，这种现象在各种应用中普遍存在。这些对抗性示例通过巧妙地注入微小的扰动到清晰的图像或视频中，从而导致深度学习算法误分类或产生错误输出。这种易受攻击的情况不仅局限于数字领域，因为对抗性示例也可以被精心设计来针对人类认知，从而产生欺骗性媒体，如深度伪造。特别是，深度伪造已经成为一种操控舆论和破坏公众人物声誉的有力工具，这凸显了需要解决与对抗攻击相关的安全和道德问题的紧迫性。

    Deep learning constitutes a pivotal component within the realm of machine learning, offering remarkable capabilities in tasks ranging from image recognition to natural language processing. However, this very strength also renders deep learning models susceptible to adversarial examples, a phenomenon pervasive across a diverse array of applications. These adversarial examples are characterized by subtle perturbations artfully injected into clean images or videos, thereby causing deep learning algorithms to misclassify or produce erroneous outputs. This susceptibility extends beyond the confines of digital domains, as adversarial examples can also be strategically designed to target human cognition, leading to the creation of deceptive media, such as deepfakes. Deepfakes, in particular, have emerged as a potent tool to manipulate public opinion and tarnish the reputations of public figures, underscoring the urgent need to address the security and ethical implications associated with adve
    
[^130]: 高效的数学表达式生成器用于符号回归

    Efficient Generator of Mathematical Expressions for Symbolic Regression. (arXiv:2302.09893v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09893](http://arxiv.org/abs/2302.09893)

    我们提出了一种基于变分自动编码器的方法来生成数学表达式，并用于符号回归。实验结果表明，我们的方法可以高效地训练，并且能够准确地编码表达式。通过优化方法探索生成的潜在空间能够解决符号回归问题，且效果优于传统的方法。

    

    我们提出了一种基于新颖的变分自动编码器（HVAE）的符号回归方法，用于生成层级结构。它将简单的原子单元与共享权重相结合，以递归地编码和解码层级中的各个节点。编码是自下而上进行的，解码是自上而下进行的。我们实验上证明了HVAE可以在数学表达式的小语料库上高效地进行训练，并可以将表达式准确地编码成平滑的低维潜在空间。后者可以通过各种优化方法高效地进行探索，以解决符号回归的任务。事实上，通过HVAE的潜在空间进行随机搜索比通过手工设计的数学表达式的随机搜索效果更好。最后，应用进化算法到HVAE的潜在空间中的EDHiE系统可以更好地从标准符号回归基准中重建方程。

    We propose an approach to symbolic regression based on a novel variational autoencoder for generating hierarchical structures, HVAE. It combines simple atomic units with shared weights to recursively encode and decode the individual nodes in the hierarchy. Encoding is performed bottom-up and decoding top-down. We empirically show that HVAE can be trained efficiently with small corpora of mathematical expressions and can accurately encode expressions into a smooth low-dimensional latent space. The latter can be efficiently explored with various optimization methods to address the task of symbolic regression. Indeed, random search through the latent space of HVAE performs better than random search through expressions generated by manually crafted probabilistic grammars for mathematical expressions. Finally, EDHiE system for symbolic regression, which applies an evolutionary algorithm to the latent space of HVAE, reconstructs equations from a standard symbolic regression benchmark better 
    
[^131]: 利用评论：学习在买家和卖家不确定性中定价

    Leveraging Reviews: Learning to Price with Buyer and Seller Uncertainty. (arXiv:2302.09700v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2302.09700](http://arxiv.org/abs/2302.09700)

    这项研究研究了在线市场中买家和卖家不确定性下的定价问题，买家利用其他具有相同属性的买家的评论来估算产品价值，卖家使用评论来衡量商品的需求。

    

    在在线市场中，顾客可以获得某个产品的数百条评论。买家经常使用来自与他们具有相同属性的其他顾客的评论来估算他们可能之前并不了解的产品价值，比如衣物的身高、护肤品的肤质和户外家具的地理位置。由于缺乏相关评论，有些顾客可能会犹豫是否购买，除非价格低，因此对于卖家来说，设定高价格和确保有足够的评论让买家能够自信地估算产品价值之间存在着一种紧张关系。同时，卖家也可以利用评论来衡量他们希望销售的商品的需求。在这项工作中，我们研究了在线环境中的这个定价问题，其中卖家与一系列不同类型的买家逐一交互，在一系列的T轮中进行。在每一轮中，卖家首先设定一个价格，然后一个买家到达并查看具有相同类型的先前买家的评论，这些评论透露了那些买家对产品的评价。

    In online marketplaces, customers have access to hundreds of reviews for a single product. Buyers often use reviews from other customers that share their type -- such as height for clothing, skin type for skincare products, and location for outdoor furniture -- to estimate their values, which they may not know a priori. Customers with few relevant reviews may hesitate to make a purchase except at a low price, so for the seller, there is a tension between setting high prices and ensuring that there are enough reviews so that buyers can confidently estimate their values. Simultaneously, sellers may use reviews to gauge the demand for items they wish to sell.  In this work, we study this pricing problem in an online setting where the seller interacts with a set of buyers of finitely many types, one by one, over a series of $T$ rounds. At each round, the seller first sets a price. Then a buyer arrives and examines the reviews of the previous buyers with the same type, which reveal those bu
    
[^132]: 评估连续时间模型预测多发性硬化进展

    Benchmarking Continuous Time Models for Predicting Multiple Sclerosis Progression. (arXiv:2302.07854v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07854](http://arxiv.org/abs/2302.07854)

    本研究评估了连续时间模型在预测多发性硬化进展方面的性能，发现最佳的连续模型通常能够胜过最佳的离散时间模型，并且标准化现有特征可以带来更大的性能提升。

    

    多发性硬化是一种影响大脑和脊髓的疾病，会导致严重残疾且没有已知的治愈方法。大多数关于多发性硬化的机器学习研究都集中在使用磁共振成像扫描或实验室测试，这些模态都昂贵且不可靠。最近的一项研究表明，使用性能结果和人口数据可以有效预测疾病进展。我们的工作在此基础上研究了建模方面，使用连续时间模型来预测进展。我们使用公开可用的多发性硬化数据集对四种连续时间模型进行了基准测试。我们发现最佳的连续模型通常能够优于最佳的离散时间模型。我们还进行了广泛的消融实验以发现性能提升的来源，结果显示，对现有特征进行标准化比插值处理能够带来更大的性能提升。

    Multiple sclerosis is a disease that affects the brain and spinal cord, it can lead to severe disability and has no known cure. The majority of prior work in machine learning for multiple sclerosis has been centered around using Magnetic Resonance Imaging scans or laboratory tests; these modalities are both expensive to acquire and can be unreliable. In a recent paper it was shown that disease progression can be predicted effectively using performance outcome measures and demographic data. In our work we build on this to investigate the modeling side, using continuous time models to predict progression. We benchmark four continuous time models using a publicly available multiple sclerosis dataset. We find that the best continuous model is often able to outperform the best benchmarked discrete time model. We also carry out an extensive ablation to discover the sources of performance gains, we find that standardizing existing features leads to a larger performance increase than interpola
    
[^133]: 变分贝叶斯系统发育参数推断中的先验密度学习

    Prior Density Learning in Variational Bayesian Phylogenetic Parameters Inference. (arXiv:2302.02522v2 [q-bio.PE] UPDATED)

    [http://arxiv.org/abs/2302.02522](http://arxiv.org/abs/2302.02522)

    本文提出了一种使用学习参数的灵活先验方法，通过多个马尔可夫链替代模型的模拟得出，该方法在估计系统发育参数方面非常有效。

    

    变分推断在贝叶斯估计问题中提供了有希望的路径。这些进步使得变分系统发育推断成为逼近系统发育后验的蒙特卡罗马尔科夫链方法的替代方法。然而，这种方法的主要缺点之一是通过固定分布来建模先验，如果它们远离当前数据分布，可能会偏倚后验逼近。在本文中，我们提出了一种方法和实施框架，通过学习它们的参数，使用基于梯度的方法和基于神经网络的参数化来放松先验密度的严格性。我们将这种方法应用于多个马尔可夫链替代模型下的支长度和进化参数估计。所进行的模拟结果显示出这种方法在估计支长度和进化模型参数方面非常有效。它们还表明灵活的先验可以提高推断的准确性。

    The advances in variational inference are providing promising paths in Bayesian estimation problems. These advances make variational phylogenetic inference an alternative approach to Markov Chain Monte Carlo methods for approximating the phylogenetic posterior. However, one of the main drawbacks of such approaches is the modelling of the prior through fixed distributions, which could bias the posterior approximation if they are distant from the current data distribution. In this paper, we propose an approach and an implementation framework to relax the rigidity of the prior densities by learning their parameters using a gradient-based method and a neural network-based parameterization. We applied this approach for branch lengths and evolutionary parameters estimation under several Markov chain substitution models. The results of performed simulations show that the approach is powerful in estimating branch lengths and evolutionary model parameters. They also show that a flexible prior m
    
[^134]: 使用预训练模型进行未知领域风险最小化的梯度估计

    Gradient Estimation for Unseen Domain Risk Minimization with Pre-Trained Models. (arXiv:2302.01497v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01497](http://arxiv.org/abs/2302.01497)

    该论文提出了一种新的领域广义化方法，使用大规模预训练模型估计不可观测梯度来减少未知领域中的潜在风险，从而增强模型的广义能力。

    

    领域广义化旨在构建在只有源领域用于模型优化时在未知领域上表现良好的广义模型。最近的研究表明，大规模的预训练模型可以通过利用它们的广义能力来增强领域广义化。然而，由于预训练目标与目标任务之间存在差异，这些预训练模型缺乏目标任务特定的知识。虽然可以通过微调从源领域中学习任务特定的知识，但由于梯度偏差导致对源领域的广义能力受损。为了缓解这个问题，我们提出了一种新的领域广义化方法，它通过使用大规模的预训练模型估计不可观测的梯度，在未知领域中减少潜在风险。这些估计的不可观测梯度使预训练模型能够进一步学习任务特定的知识，同时保持其广义能力，从而减轻了潜在的风险。

    Domain generalization aims to build generalized models that perform well on unseen domains when only source domains are available for model optimization. Recent studies have shown that large-scale pre-trained models can enhance domain generalization by leveraging their generalization power. However, these pre-trained models lack target task-specific knowledge yet due to discrepancies between the pre-training objectives and the target task. Although the task-specific knowledge could be learned from source domains by fine-tuning, this hurts the generalization power of pre-trained models due to gradient bias toward the source domains. To alleviate this problem, we propose a new domain generalization method that estimates unobservable gradients that reduce potential risks in unseen domains using a large-scale pre-trained model. These estimated unobservable gradients allow the pre-trained model to learn task-specific knowledge further while preserving its generalization ability by relieving
    
[^135]: 量子小波变换：具有量子计算优势的神经网络中的重要应用

    Quantum Ridgelet Transform: Winning Lottery Ticket of Neural Networks with Quantum Computation. (arXiv:2301.11936v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2301.11936](http://arxiv.org/abs/2301.11936)

    这篇论文提出了量子小波变换(QRT)作为神经网络中的重要应用，通过量子计算实现了对量子态的小波变换，并且可以高效地找到大型神经网络的稀疏可训练子网络。

    

    量子机器学习(QML)领域中的一个重要挑战是建立量子计算在加速神经网络等常见机器学习任务中的应用。小波变换一直是神经网络理论研究中的基本数学工具，但由于传统经典计算的数值实现需要指数级运行时间$\exp(O(D))$，因此小波变换的实际应用性受到限制，尤其在数据维度$D$增加时。为了解决这个问题，我们开发了量子小波变换(QRT)，它在量子计算的线性运行时间$O(D)$内实现了对量子态的小波变换。作为一个应用，我们还展示了利用QRT作为QML的基本子程序，可以高效地找到大型浅宽神经网络的稀疏可训练子网络，而无需对原始网络进行大规模优化。

    A significant challenge in the field of quantum machine learning (QML) is to establish applications of quantum computation to accelerate common tasks in machine learning such as those for neural networks. Ridgelet transform has been a fundamental mathematical tool in the theoretical studies of neural networks, but the practical applicability of ridgelet transform to conducting learning tasks was limited since its numerical implementation by conventional classical computation requires an exponential runtime $\exp(O(D))$ as data dimension $D$ increases. To address this problem, we develop a quantum ridgelet transform (QRT), which implements the ridgelet transform of a quantum state within a linear runtime $O(D)$ of quantum computation. As an application, we also show that one can use QRT as a fundamental subroutine for QML to efficiently find a sparse trainable subnetwork of large shallow wide neural networks without conducting large-scale optimization of the original network. This appli
    
[^136]: PECAN: 一种针对后门攻击的确定性认证防御方法

    PECAN: A Deterministic Certified Defense Against Backdoor Attacks. (arXiv:2301.11824v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2301.11824](http://arxiv.org/abs/2301.11824)

    PECAN是一种有效且经过认证的后门攻击防御方法，通过在不相交分区上训练一组神经网络并应用测试时间逃避认证技术，可以显著提高防御强度和效率，降低攻击成功率。

    

    神经网络容易受到后门攻击，攻击者恶意污染训练集并在测试输入中插入触发器以改变受害模型的预测结果。现有的后门攻击防御方法要么没有提供正式的保证，要么具有计算昂贵且无效的概率保证。我们提出了一种有效且经过认证的后门攻击防御方法PECAN。PECAN的核心洞见是在数据的不相交分区上训练一组神经网络，并应用现成的测试时间逃避认证技术。我们在图像分类和恶意软件检测数据集上评估了PECAN。结果表明PECAN在防御强度和效率方面显著优于现有的经过认证的后门防御方法，并且在真实后门攻击中，与文献中的一系列基线相比，PECAN可以将攻击成功率降低一个数量级。

    Neural networks are vulnerable to backdoor poisoning attacks, where the attackers maliciously poison the training set and insert triggers into the test input to change the prediction of the victim model. Existing defenses for backdoor attacks either provide no formal guarantees or come with expensive-to-compute and ineffective probabilistic guarantees. We present PECAN, an efficient and certified approach for defending against backdoor attacks. The key insight powering PECAN is to apply off-the-shelf test-time evasion certification techniques on a set of neural networks trained on disjoint partitions of the data. We evaluate PECAN on image classification and malware detection datasets. Our results demonstrate that PECAN can (1) significantly outperform the state-of-the-art certified backdoor defense, both in defense strength and efficiency, and (2) on real back-door attacks, PECAN can reduce attack success rate by order of magnitude when compared to a range of baselines from the litera
    
[^137]: 自监督学习和动态计算之间的统一协同作用

    Unifying Synergies between Self-supervised Learning and Dynamic Computation. (arXiv:2301.09164v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.09164](http://arxiv.org/abs/2301.09164)

    本文提出了自监督学习和动态计算之间相互作用的新视角。通过在自监督学习的设置中同时学习密集和门控子网络，无需额外的微调或剪枝步骤，可以获得一个通用且高效的架构，适用于资源受限的工业环境。

    

    在资源受限的工业环境中，计算昂贵的训练策略使得自监督学习（SSL）变得不切实际。为了获取轻量级模型，通常会使用知识蒸馏（KD）、动态计算（DC）和剪枝等技术，这通常涉及到对大型预训练模型进行多次微调（或蒸馏步骤），使得计算更具挑战性。在这项工作中，我们提出了自监督学习和动态计算范式之间相互作用的新视角。特别地，我们展示了在自监督学习的设置中，同时从零开始学习密集和门控子网络是可行的，而不需要额外的微调或剪枝步骤。预训练期间密集和门控编码器的共同演化提供了良好的准确性与效率之间的平衡，从而为应用特定的工业环境提供了通用且多功能的架构。我们在包括CIFAR-10/100等多个图像分类基准上进行了大量实验。

    Computationally expensive training strategies make self-supervised learning (SSL) impractical for resource constrained industrial settings. Techniques like knowledge distillation (KD), dynamic computation (DC), and pruning are often used to obtain a lightweightmodel, which usually involves multiple epochs of fine-tuning (or distilling steps) of a large pre-trained model, making it more computationally challenging. In this work we present a novel perspective on the interplay between SSL and DC paradigms. In particular, we show that it is feasible to simultaneously learn a dense and gated sub-network from scratch in a SSL setting without any additional fine-tuning or pruning steps. The co-evolution during pre-training of both dense and gated encoder offers a good accuracy-efficiency trade-off and therefore yields a generic and multi-purpose architecture for application specific industrial settings. Extensive experiments on several image classification benchmarks including CIFAR-10/100, S
    
[^138]: 高维变分推理的投影积分更新

    Projective Integral Updates for High-Dimensional Variational Inference. (arXiv:2301.08374v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08374](http://arxiv.org/abs/2301.08374)

    该论文介绍了一种适用于高维变分推理的投影积分更新方法，并通过降低参数敏感性来实现更强健的预测。

    

    变分推理是一种贝叶斯推理的近似框架，通过优化简化的参数分布来代替完整的后验分布，从而改善预测中的量化不确定性。捕捉与训练数据一致的模型变化可以通过降低参数敏感性来实现更强健的预测。本研究引入了一种适用于变分推理的固定点最优化方法，当每个可行的对数密度可以表示为给定基函数的线性组合时，该方法生效。在这种情况下，优化器成为投影积分更新的一个不动点。当基函数跨越每个参数的二次函数时，可行密度为高斯分布，投影积分更新产生了准牛顿变分贝叶斯 (QNVB)。其他基函数和更新方法也是可能的。由于这些更新需要高维积分，本研究首先提出了一种高效的准随机积分序列用于均匀均匀均匀均匀均匀均匀积分。

    Variational inference is an approximation framework for Bayesian inference that seeks to improve quantified uncertainty in predictions by optimizing a simplified distribution over parameters to stand in for the full posterior. Capturing model variations that remain consistent with training data enables more robust predictions by reducing parameter sensitivity. This work introduces a fixed-point optimization for variational inference that is applicable when every feasible log density can be expressed as a linear combination of functions from a given basis. In such cases, the optimizer becomes a fixed-point of projective integral updates. When the basis spans univariate quadratics in each parameter, feasible densities are Gaussian and the projective integral updates yield quasi-Newton variational Bayes (QNVB). Other bases and updates are also possible. As these updates require high-dimensional integration, this work first proposes an efficient quasirandom quadrature sequence for mean-fie
    
[^139]: 领域自适应学习和模仿：用于电力套利的深度强化学习

    Domain-adapted Learning and Imitation: DRL for Power Arbitrage. (arXiv:2301.08360v2 [q-fin.TR] UPDATED)

    [http://arxiv.org/abs/2301.08360](http://arxiv.org/abs/2301.08360)

    本文提出了一种领域自适应学习和模仿的深度强化学习方法，用于电力套利交易。通过利用奖励工程和订单分段的方式，该方法能够提高训练的收敛性，增加竞标成功率，并显著提高利润和损失。

    

    本文讨论荷兰电力市场，由日前市场和即时平衡市场组成，并且运作方式类似拍卖。由于供需波动，通常存在不平衡导致两个市场价格不同，从而提供套利机会。为了解决这个问题，我们对问题进行了重构，并提出了一种协作式双代理深度强化学习方法，用于欧洲电力套利交易的双层仿真与优化。我们还引入了两种新的实现方式，通过模仿电力交易员的交易行为来融入领域特定知识。通过利用奖励工程来模仿领域专业知识，我们能够重新设计强化学习代理的奖励系统，改善训练中的收敛性并提高整体性能。此外，订单分段增加了竞标成功率，显著提高了利润和损失。我们的研究演示了通过领域自适应学习和模仿的深度强化学习方法在电力套利中的应用。

    In this paper, we discuss the Dutch power market, which is comprised of a day-ahead market and an intraday balancing market that operates like an auction. Due to fluctuations in power supply and demand, there is often an imbalance that leads to different prices in the two markets, providing an opportunity for arbitrage. To address this issue, we restructure the problem and propose a collaborative dual-agent reinforcement learning approach for this bi-level simulation and optimization of European power arbitrage trading. We also introduce two new implementations designed to incorporate domain-specific knowledge by imitating the trading behaviours of power traders. By utilizing reward engineering to imitate domain expertise, we are able to reform the reward system for the RL agent, which improves convergence during training and enhances overall performance. Additionally, the tranching of orders increases bidding success rates and significantly boosts profit and loss (P&L). Our study demo
    
[^140]: 相对论数字孪生：将物联网引入未来

    Relativistic Digital Twin: Bringing the IoT to the Future. (arXiv:2301.07390v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2301.07390](http://arxiv.org/abs/2301.07390)

    本文提出了相对论数字孪生（RDT）框架，通过不断观察物联网实体的真实对应物来自动生成通用型数字孪生，并调整其行为模型。框架利用物联网之物（WoT）提供标准化接口。

    

    复杂的物联网生态系统通常需要使用其物理资产的数字孪生（DT）来进行预测分析和模拟“假设”情景。数字孪生能够复制物联网设备，并随着时间的推移适应其行为变化。然而，物联网中的数字孪生通常针对特定的用例，无法无缝适应不同的情境。此外，物联网的碎片化使得在具有多种数据格式和物联网网络协议的异构情景中部署数字孪生更具挑战性。在本文中，我们提出了相对论数字孪生（RDT）框架，通过该框架我们可以自动生成物联网实体的通用型数字孪生，并通过不断观察其真实对应物来调整其行为模型。该框架依赖于通过物联网之物（WoT）来进行对象表示，为每个物联网设备及其数字孪生提供标准化接口。

    Complex IoT ecosystems often require the usage of Digital Twins (DTs) of their physical assets in order to perform predictive analytics and simulate what-if scenarios. DTs are able to replicate IoT devices and adapt over time to their behavioral changes. However, DTs in IoT are typically tailored to a specific use case, without the possibility to seamlessly adapt to different scenarios. Further, the fragmentation of IoT poses additional challenges on how to deploy DTs in heterogeneous scenarios characterized by the usage of multiple data formats and IoT network protocols. In this paper, we propose the Relativistic Digital Twin (RDT) framework, through which we automatically generate general-purpose DTs of IoT entities and tune their behavioral models over time by constantly observing their real counterparts. The framework relies on the object representation via the Web of Things (WoT), to offer a standardized interface to each of the IoT devices as well as to their DTs. To this purpose
    
[^141]: 语言作为潜在序列：用于半监督释义生成的深度潜变量模型

    Language as a Latent Sequence: deep latent variable models for semi-supervised paraphrase generation. (arXiv:2301.02275v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.02275](http://arxiv.org/abs/2301.02275)

    本文提出了用于半监督释义生成的深度潜变量模型，通过将未标记数据的缺失目标对建模为潜在释义序列，并结合双向学习和改进的权重初始化方案进行训练，实验结果表明这个模型在性能上与最先进的有监督基线模型有竞争力。

    

    本文探讨了用于半监督释义生成的深度潜变量模型，其中未标记数据的缺失目标对被建模为潜在释义序列。我们提出了一种名为变分序列自编码重构（VSAR）的新型无监督模型，该模型可在给定观察文本的情况下进行潜在序列推断。为了利用文本对的信息，我们还引入了一种名为双向学习（DDL）的新型监督模型，该模型旨在与我们提出的VSAR模型结合使用。将VSAR与DDL（DDL+VSAR）结合起来使我们能够进行半监督学习。然而，组合模型存在冷启动问题。为了进一步解决这个问题，我们提出了一种改进的权重初始化解决方案，从而导致一个名为知识增强学习（KRL）的新型两阶段训练方案。我们的实证评估表明，组合模型在性能上与最先进的有监督基线模型竞争力持平。

    This paper explores deep latent variable models for semi-supervised paraphrase generation, where the missing target pair for unlabelled data is modelled as a latent paraphrase sequence. We present a novel unsupervised model named variational sequence auto-encoding reconstruction (VSAR), which performs latent sequence inference given an observed text. To leverage information from text pairs, we additionally introduce a novel supervised model we call dual directional learning (DDL), which is designed to integrate with our proposed VSAR model. Combining VSAR with DDL (DDL+VSAR) enables us to conduct semi-supervised learning. Still, the combined model suffers from a cold-start problem. To further combat this issue, we propose an improved weight initialisation solution, leading to a novel two-stage training scheme we call knowledge-reinforced-learning (KRL). Our empirical evaluations suggest that the combined model yields competitive performance against the state-of-the-art supervised basel
    
[^142]: 利用物理感知神经网络对锂离子电池的预测和健康管理进行建模

    Physics-Informed Neural Networks for Prognostics and Health Management of Lithium-Ion Batteries. (arXiv:2301.00776v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2301.00776](http://arxiv.org/abs/2301.00776)

    本研究提出了一种基于物理感知神经网络（PINN）的模型融合方案，用于预测和管理锂离子电池的健康状态。该方法能够将经验或物理动态模型与数据驱动模型相结合，以充分利用各种信息来源，提高预测和管理的准确性。

    

    对于锂离子电池的预测和健康管理（PHM），已建立了许多模型来描述其衰减过程。现有的经验或物理模型可以揭示有关衰减动态的重要信息。然而，目前还没有一种通用且灵活的方法来融合这些模型所代表的信息。物理感知神经网络（PINN）是一种将经验或物理动态模型与数据驱动模型相结合的高效工具。为了充分利用各种信息来源，我们提出了一种基于PINN的模型融合方案。通过开发半经验半物理的偏微分方程（PDE）来建模锂离子电池的衰减动态。当对动态了解较少时，我们利用数据驱动的深度隐藏物理模型（DeepHPM）来发现潜在的主导动态模型。然后将发现的动态信息与挖掘的信息进行融合。

    For Prognostics and Health Management (PHM) of Lithium-ion (Li-ion) batteries, many models have been established to characterize their degradation process. The existing empirical or physical models can reveal important information regarding the degradation dynamics. However, there are no general and flexible methods to fuse the information represented by those models. Physics-Informed Neural Network (PINN) is an efficient tool to fuse empirical or physical dynamic models with data-driven models. To take full advantage of various information sources, we propose a model fusion scheme based on PINN. It is implemented by developing a semi-empirical semi-physical Partial Differential Equation (PDE) to model the degradation dynamics of Li-ion batteries. When there is little prior knowledge about the dynamics, we leverage the data-driven Deep Hidden Physics Model (DeepHPM) to discover the underlying governing dynamic models. The uncovered dynamics information is then fused with that mined by 
    
[^143]: 一种用于持续半监督学习的软最近邻框架

    A soft nearest-neighbor framework for continual semi-supervised learning. (arXiv:2212.05102v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.05102](http://arxiv.org/abs/2212.05102)

    该论文介绍了一种用于持续半监督学习的软最近邻框架，该框架利用最近邻分类器来非线性地划分特征空间并非参数地建模潜在的数据分布，以避免模型忘记对未标记数据表示并过度拟合标记的样本。

    

    尽管取得了显着进展，最先进的持续学习方法的表现仍然依赖于完全标记的数据的不现实情况。在本文中，我们解决了这一挑战，提出了一种适用于持续半监督学习的方法——一种未标记的数据样本不全部标记的情况。在这种情况下的一个主要问题是模型会忘记未标记数据的表示，并过度拟合标记的样本。我们利用最近邻分类器的威力来非线性地划分特征空间，并由于其非参数性质而灵活地对潜在的数据分布进行建模。这使得模型能够为当前任务学习强大的表示，并从以前的任务中提炼相关的信息。我们进行了彻底的实验评估，并展示了我们的方法以较大的优势优于所有现有的方法，在持续半监督学习范式上树立了坚实的现有技术。

    Despite significant advances, the performance of state-of-the-art continual learning approaches hinges on the unrealistic scenario of fully labeled data. In this paper, we tackle this challenge and propose an approach for continual semi-supervised learning--a setting where not all the data samples are labeled. A primary issue in this scenario is the model forgetting representations of unlabeled data and overfitting the labeled samples. We leverage the power of nearest-neighbor classifiers to nonlinearly partition the feature space and flexibly model the underlying data distribution thanks to its non-parametric nature. This enables the model to learn a strong representation for the current task, and distill relevant information from previous tasks. We perform a thorough experimental evaluation and show that our method outperforms all the existing approaches by large margins, setting a solid state of the art on the continual semi-supervised learning paradigm. For example, on CIFAR-100 we
    
[^144]: 理解正弦神经网络

    Understanding Sinusoidal Neural Networks. (arXiv:2212.01833v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01833](http://arxiv.org/abs/2212.01833)

    本研究探究了正弦神经网络的结构和表达能力，这些网络使用正弦函数作为激活函数。正弦MLP具有平滑性和紧凑性两个关键性质，并提供了控制机制来定义和训练这些网络。

    

    在这项工作中，我们研究了正弦多层感知机网络（MLP）的结构和表示能力，该网络使用正弦函数作为激活函数。这些神经网络（也被称为神经场）在计算机图形中表示常见信号（如图像、有符号距离函数和辐射场）方面具有基础性作用。正弦MLP的成功主要归因于其两个关键性质：平滑性和紧凑性。这些函数是平滑的，因为它们由仿射映射与正弦函数的组合得到。本工作提供了理论结果来证明正弦MLP的紧凑性，并在这些网络的定义和训练中提供了控制机制。我们提出通过将其展开为谐波和来研究正弦MLP。首先，我们观察到其第一层可以被看作是一个谐波字典，我们称之为输入正弦神经元。然后，隐藏层使用仿射变换来结合这个字典。

    In this work, we investigate the structure and representation capacity of sinusoidal MLPs - multilayer perceptron networks that use sine as the activation function. These neural networks (known as neural fields) have become fundamental in representing common signals in computer graphics, such as images, signed distance functions, and radiance fields. This success can be primarily attributed to two key properties of sinusoidal MLPs: smoothness and compactness. These functions are smooth because they arise from the composition of affine maps with the sine function. This work provides theoretical results to justify the compactness property of sinusoidal MLPs and provides control mechanisms in the definition and training of these networks.  We propose to study a sinusoidal MLP by expanding it as a harmonic sum. First, we observe that its first layer can be seen as a harmonic dictionary, which we call the input sinusoidal neurons. Then, a hidden layer combines this dictionary using an affin
    
[^145]: 深度图聚类综述：分类、挑战、应用和开放资源

    A Survey of Deep Graph Clustering: Taxonomy, Challenge, Application, and Open Resource. (arXiv:2211.12875v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.12875](http://arxiv.org/abs/2211.12875)

    在这篇论文中，作者对深度图聚类进行了综述研究。首先介绍了该领域的定义、评估和发展，然后提出了深度图聚类方法的分类学，并对现有方法进行了分析，总结出了挑战和机会。

    

    图聚类旨在将图中的节点划分为若干不同的簇，是一项基础且具有挑战性的任务。近年来，借助深度学习强大的表示能力，深度图聚类方法取得了巨大成功。然而，对于这个领域的综述论文相对较少，综述该领域的工作势在必行。基于此动机，我们进行了深度图聚类的综述研究。首先，我们介绍了该领域的公式化定义、评估和发展。其次，基于图类型、网络架构、学习范式和聚类方法等四个不同的标准，提出了深度图聚类方法的分类学。第三，我们通过广泛的实验对现有方法进行了详细分析，并从图数据质量、稳定性、可扩展性、辨别能力和未知簇数等五个角度总结了挑战和机会。

    Graph clustering, which aims to divide nodes in the graph into several distinct clusters, is a fundamental yet challenging task. Benefiting from the powerful representation capability of deep learning, deep graph clustering methods have achieved great success in recent years. However, the corresponding survey paper is relatively scarce, and it is imminent to make a summary of this field. From this motivation, we conduct a comprehensive survey of deep graph clustering. Firstly, we introduce formulaic definition, evaluation, and development in this field. Secondly, the taxonomy of deep graph clustering methods is presented based on four different criteria, including graph type, network architecture, learning paradigm, and clustering method. Thirdly, we carefully analyze the existing methods via extensive experiments and summarize the challenges and opportunities from five perspectives, including graph data quality, stability, scalability, discriminative capability, and unknown cluster nu
    
[^146]: 基于模型的残差策略学习及其在天线控制中的应用

    Model Based Residual Policy Learning with Applications to Antenna Control. (arXiv:2211.08796v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08796](http://arxiv.org/abs/2211.08796)

    本文介绍了一种基于模型的残差策略学习（MBRPL）方法，用于解决天线控制问题。该方法通过增强现有策略，提高了样本效率，并减少了与实际环境的交互次数。实验结果表明，该方法在初始性能方面表现出色，并为将这些算法应用于实际网络迈出了一步。

    

    非可微控制器和基于规则的策略广泛应用于控制诸如电信网络和机器人等实际系统。特别是，移动网络基站天线的参数可以通过这些策略进行动态配置，以改善用户覆盖率和服务质量。受天线倾斜控制问题的启发，我们引入了一种实用的强化学习方法——基于模型的残差策略学习（MBRPL）。MBRPL通过基于模型的方法增强现有策略，提高样本效率，并与现成的强化学习方法相比减少与实际环境的交互次数。据我们所知，这是第一篇研究基于模型的天线控制方法的论文。实验结果表明，我们的方法在初始性能方面表现出色，并提高了样本效率，这是将这些算法应用于实际网络的一步。

    Non-differentiable controllers and rule-based policies are widely used for controlling real systems such as telecommunication networks and robots. Specifically, parameters of mobile network base station antennas can be dynamically configured by these policies to improve users coverage and quality of service. Motivated by the antenna tilt control problem, we introduce Model-Based Residual Policy Learning (MBRPL), a practical reinforcement learning (RL) method. MBRPL enhances existing policies through a model-based approach, leading to improved sample efficiency and a decreased number of interactions with the actual environment when compared to off-the-shelf RL methods.To the best of our knowledge, this is the first paper that examines a model-based approach for antenna control. Experimental results reveal that our method delivers strong initial performance while improving sample efficiency over previous RL methods, which is one step towards deploying these algorithms in real networks.
    
[^147]: 发现、解释、改进：一种用于自然语言处理的自动片段检测框架

    Discover, Explanation, Improvement: An Automatic Slice Detection Framework for Natural Language Processing. (arXiv:2211.04476v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.04476](http://arxiv.org/abs/2211.04476)

    本研究提出了一个自动片段检测框架用于自然语言处理任务，通过发现、解释和改进模型的错误，提供了对模型行为的理解和未来模型设计的见解。

    

    预训练的自然语言处理（NLP）模型取得了高整体性能，但仍然存在系统性的错误。与手动错误分析不同，对于自动识别表现不佳的数据组的片段检测模型（SDM）的研究，在计算机视觉领域引起了广泛关注，可以理解模型行为并为未来模型训练和设计提供见解。然而，在NLP任务上，对SDM的研究和其有效性的定量评估还很少。本文通过提出一个名为“Discover, Explain, Improve(DEIM)”的NLP分类任务基准和一个新的SDM模型Edisa来填补这一空白。Edisa发现了一致且表现不佳的数据组；DEIM将它们统一为人类可理解的概念，并提供了全面的评估任务和相应的定量指标。在DEIM的评估中，结果显示Edisa能够准确选择易出错的数据点，并提供了有用的信息。

    Pretrained natural language processing (NLP) models have achieved high overall performance, but they still make systematic errors. Instead of manual error analysis, research on slice detection models (SDM), which automatically identify underperforming groups of datapoints, has caught escalated attention in Computer Vision for both understanding model behaviors and providing insights for future model training and designing. However, little research on SDM and quantitative evaluation of their effectiveness have been conducted on NLP tasks. Our paper fills the gap by proposing a benchmark named "Discover, Explain, Improve (DEIM)" for classification NLP tasks along with a new SDM Edisa. Edisa discovers coherent and underperforming groups of datapoints; DEIM then unites them under human-understandable concepts and provides comprehensive evaluation tasks and corresponding quantitative metrics. The evaluation in DEIM shows that Edisa can accurately select error-prone datapoints with informati
    
[^148]: 基于参数化超复数神经网络的心房颤动检测的高效ECG方法

    Efficient ECG-based Atrial Fibrillation Detection via Parameterised Hypercomplex Neural Networks. (arXiv:2211.02678v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2211.02678](http://arxiv.org/abs/2211.02678)

    本文提出了一种基于参数化超复数神经网络的轻量级卷积神经网络方法，用于心房颤动检测。该方法在可穿戴设备上训练小规模CNN，克服了有限的计算资源。在两个公开可用的ECG数据集上，该方法表现出与实值CNN相当的性能，但使用了显着较少的模型参数。

    This paper proposes a lightweight convolutional neural network method based on parameterized hypercomplex neural networks for atrial fibrillation detection. The method trains small-scale CNNs on wearable devices, overcoming limited computing resources. The approach shows comparable performance to real-valued CNNs on two publicly available ECG datasets using significantly fewer model parameters.

    心房颤动（AF）是最常见的心律失常，与中风等严重疾病的高风险相关。嵌入自动和及时的AF评估的可穿戴设备使用心电图（ECG）已被证明在预防危及生命的情况方面具有前景。虽然深度神经网络在模型性能方面表现出优越性，但它们在可穿戴设备上的使用受到模型性能和复杂性之间的权衡的限制。在这项工作中，我们提出使用带有参数化超复数（PH）层的轻量级卷积神经网络（CNN）来基于ECG进行AF检测。所提出的方法训练小规模CNN，从而克服了可穿戴设备上的有限计算资源。我们使用显着较少的模型参数在两个公开可用的ECG数据集上展示了与相应的实值CNN相当的性能。PH模型比其他超复数神经网络更灵活，可以在...

    Atrial fibrillation (AF) is the most common cardiac arrhythmia and associated with a high risk for serious conditions like stroke. The use of wearable devices embedded with automatic and timely AF assessment from electrocardiograms (ECGs) has shown to be promising in preventing life-threatening situations. Although deep neural networks have demonstrated superiority in model performance, their use on wearable devices is limited by the trade-off between model performance and complexity. In this work, we propose to use lightweight convolutional neural networks (CNNs) with parameterised hypercomplex (PH) layers for AF detection based on ECGs. The proposed approach trains small-scale CNNs, thus overcoming the limited computing resources on wearable devices. We show comparable performance to corresponding real-valued CNNs on two publicly available ECG datasets using significantly fewer model parameters. PH models are more flexible than other hypercomplex neural networks and can operate on an
    
[^149]: 可变层次混合模型用于概率逆动力学学习

    Variational Hierarchical Mixtures for Probabilistic Learning of Inverse Dynamics. (arXiv:2211.01120v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01120](http://arxiv.org/abs/2211.01120)

    本文提出了一种可变层次混合模型方法，通过概率层次化建模来学习机器人应用中的逆动力学。该方法结合了传统回归模型的优势，实现了计算高效的表示和自适应数据复杂性的灵活性。

    

    随着数据集的快速增长和任务的复杂化，良好校准的概率回归模型在机器人应用中是至关重要的学习组成部分。不幸的是，经典的回归模型通常要么是具有灵活结构但不随数据优雅扩展的概率核机器，要么是具有限制参数形式和较差正则化的确定性且可扩展的自动机。在本文中，我们考虑了一种概率层次建模范式，将两者的优势结合起来，以提供具有内在复杂性正则化的计算高效的表示。所提出的方法是对局部回归技术的概率解释，通过一组局部线性或多项式单元来逼近非线性函数。重要的是，我们依赖于贝叶斯非参数的原则，构建了适应数据复杂性并且潜在地涵盖无限个模型的灵活模型。

    Well-calibrated probabilistic regression models are a crucial learning component in robotics applications as datasets grow rapidly and tasks become more complex. Unfortunately, classical regression models are usually either probabilistic kernel machines with a flexible structure that does not scale gracefully with data or deterministic and vastly scalable automata, albeit with a restrictive parametric form and poor regularization. In this paper, we consider a probabilistic hierarchical modeling paradigm that combines the benefits of both worlds to deliver computationally efficient representations with inherent complexity regularization. The presented approaches are probabilistic interpretations of local regression techniques that approximate nonlinear functions through a set of local linear or polynomial units. Importantly, we rely on principles from Bayesian nonparametrics to formulate flexible models that adapt their complexity to the data and can potentially encompass an infinite nu
    
[^150]: 防止神经语言模型的逐字记忆会产生虚假隐私保护感

    Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy. (arXiv:2210.17546v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17546](http://arxiv.org/abs/2210.17546)

    防止神经语言模型逐字记忆无法真正保护隐私，本文设计的布隆过滤器虽然防止了所有逐字记忆，但仍然无法防止训练数据泄露，容易被合理修改的“样式转换”提示绕过。

    

    通过研究神经语言模型中数据记忆的现象，本研究帮助我们理解与隐私或版权相关的风险，并有助于评估对策。然而逐字记忆定义过于严格，未能捕捉更为微妙的记忆形式。本文基于布隆过滤器设计并实现了一种有效的防御方法，但该“完美”过滤器并不能防止训练数据泄露。

    Studying data memorization in neural language models helps us understand the risks (e.g., to privacy or copyright) associated with models regurgitating training data, and aids in the evaluation of potential countermeasures. Many prior works -- and some recently deployed defenses -- focus on "verbatim memorization", defined as a model generation that exactly matches a substring from the training set. We argue that verbatim memorization definitions are too restrictive and fail to capture more subtle forms of memorization. Specifically, we design and implement an efficient defense based on Bloom filters that perfectly prevents all verbatim memorization. And yet, we demonstrate that this "perfect" filter does not prevent the leakage of training data. Indeed, it is easily circumvented by plausible and minimally modified "style-transfer" prompts -- and in some cases even the non-modified original prompts -- to extract memorized information. For example, instructing the model to output ALL-CA
    
[^151]: 基于MLP-Mixer神经网络的多视图多标签异常网络流量分类

    Multi-view Multi-label Anomaly Network Traffic Classification based on MLP-Mixer Neural Network. (arXiv:2210.16719v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.16719](http://arxiv.org/abs/2210.16719)

    本文提出了一种基于MLP-Mixer的多视图多标签神经网络用于网络流量分类，该方法能够更好地捕捉流量数据的全局信息关联，通过利用不同场景之间的相关性来提高分类性能。

    

    网络流量分类是许多网络安全应用的基础，已经在网络安全领域引起了足够的关注。现有基于卷积神经网络（CNN）的网络流量分类通常强调流量数据的局部模式，而忽视了全局信息的关联。本文提出了一种基于MLP-Mixer的多视图多标签神经网络用于网络流量分类。与现有的基于CNN的方法相比，我们的方法采用了MLP-Mixer结构，这与数据包的结构更加符合。在我们的方法中，一个数据包被分为数据包头和数据包主体，加上不同视图下的流特征作为输入。我们利用多标签设置同时学习不同场景，通过利用不同场景之间的相关性来提高分类性能。借助于

    Network traffic classification is the basis of many network security applications and has attracted enough attention in the field of cyberspace security. Existing network traffic classification based on convolutional neural networks (CNNs) often emphasizes local patterns of traffic data while ignoring global information associations. In this paper, we propose an MLP-Mixer based multi-view multi-label neural network for network traffic classification. Compared with the existing CNN-based methods, our method adopts the MLP-Mixer structure, which is more in line with the structure of the packet than the conventional convolution operation. In our method, one packet is divided into the packet header and the packet body, together with the flow features of the packet as input from different views. We utilize a multi-label setting to learn different scenarios simultaneously to improve the classification performance by exploiting the correlations between different scenarios. Taking advantage of
    
[^152]: PopArt: 高效稀疏回归和优化稀疏线性摇臂的实验设计

    PopArt: Efficient Sparse Regression and Experimental Design for Optimal Sparse Linear Bandits. (arXiv:2210.15345v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.15345](http://arxiv.org/abs/2210.15345)

    本文提出了一种称为PopArt的高效稀疏线性估计方法，相比于Lasso，在许多问题中具有更紧的$\ell_1$恢复保证，并基于此推导出稀疏线性摇臂算法，具有改进的遗憾上界。同时，我们证明了在数据稀缺情况下稀疏线性摇臂的匹配下界。

    

    在稀疏线性摇臂中，学习代理按顺序选择一个动作并接收奖励反馈，而奖励函数线性依赖于动作的一些坐标的协变量。这在许多实际的顺序决策问题中都有应用。在本文中，我们提出了一种简单而计算高效的稀疏线性估计方法，称为PopArt，与Lasso（Tibshirani, 1996）相比，它在许多问题中具有更紧的$\ell_1$恢复保证。我们的界限自然地激发了一种凸实验设计准则，因此在计算上是高效的解决方法。基于我们的新估计器和设计准则，我们推导出稀疏线性摇臂算法，其在给定动作集的几何性方面相对于现有技术（Hao et al., 2020）具有改进的遗憾上界。最后，我们证明了在数据稀缺情况下稀疏线性摇臂的匹配下界，这填补了上界和下界之间的差距。

    In sparse linear bandits, a learning agent sequentially selects an action and receive reward feedback, and the reward function depends linearly on a few coordinates of the covariates of the actions. This has applications in many real-world sequential decision making problems. In this paper, we propose a simple and computationally efficient sparse linear estimation method called PopArt that enjoys a tighter $\ell_1$ recovery guarantee compared to Lasso (Tibshirani, 1996) in many problems. Our bound naturally motivates an experimental design criterion that is convex and thus computationally efficient to solve. Based on our novel estimator and design criterion, we derive sparse linear bandit algorithms that enjoy improved regret upper bounds upon the state of the art (Hao et al., 2020), especially w.r.t. the geometry of the given action set. Finally, we prove a matching lower bound for sparse linear bandits in the data-poor regime, which closes the gap between upper and lower bounds in pr
    
[^153]: 自适应的SGD中的Top-K方法用于思降低通信成本的分布式学习

    Adaptive Top-K in SGD for Communication-Efficient Distributed Learning. (arXiv:2210.13532v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13532](http://arxiv.org/abs/2210.13532)

    这篇论文提出了一种自适应的Top-K方法用于降低通信成本的分布式学习，通过自适应调整稀疏化程度来优化收敛性能，在数值实验中取得了良好的结果。

    

    分布式随机梯度下降(SGD)与梯度压缩已成为一种流行的降低通信成本的分布式学习加速解决方案。梯度压缩的常用方法是Top-K稀疏化，它在模型训练过程中通过固定的程度稀疏化梯度。然而，缺乏一种自适应的方法来调整稀疏化程度以最大化模型性能或训练速度的潜力。本文提出了一种新颖的自适应SGD中的Top-K框架，它可以为每个梯度下降步骤提供自适应稀疏化程度，通过平衡通信成本和收敛误差之间的权衡来优化收敛性能。首先，推导了自适应稀疏化方案和损失函数的收敛误差上界。其次，设计了一种算法来在通信成本约束下最小化收敛误差。最后，给出了在MNIS数据集上的数值实验结果。

    Distributed stochastic gradient descent (SGD) with gradient compression has become a popular communication-efficient solution for accelerating distributed learning. One commonly used method for gradient compression is Top-K sparsification, which sparsifies the gradients by a fixed degree during model training. However, there has been a lack of an adaptive approach to adjust the sparsification degree to maximize the potential of the model's performance or training speed. This paper proposes a novel adaptive Top-K in SGD framework that enables an adaptive degree of sparsification for each gradient descent step to optimize the convergence performance by balancing the trade-off between communication cost and convergence error. Firstly, an upper bound of convergence error is derived for the adaptive sparsification scheme and the loss function. Secondly, an algorithm is designed to minimize the convergence error under the communication cost constraints. Finally, numerical results on the MNIS
    
[^154]: TiDAL: 学习主动学习的训练动力学

    TiDAL: Learning Training Dynamics for Active Learning. (arXiv:2210.06788v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.06788](http://arxiv.org/abs/2210.06788)

    提出了一种新的主动学习方法TiDAL，利用训练动力学来量化未标记数据的不确定性，为了解决大规模数据的跟踪问题，利用预测模块学习标记数据的训练动力学。

    

    主动学习旨在在有限预算下从未标记的数据池中选择最有用的数据样本并对其进行注释，以扩展标记数据集。尤其是基于不确定性的方法选择最不确定的样本，在改善模型性能方面已被证明是有效的。然而，主动学习文献经常忽视训练动力学（TD），即通过随机梯度下降进行优化时模型行为的不断变化，尽管文献的其他领域经验证明TD提供了衡量样本不确定性的重要线索。在本文中，我们提出了一种新颖的主动学习方法，即训练动力学主动学习（TiDAL），它利用TD来量化未标记数据的不确定性。由于跟踪大规模未标记数据的TD是不切实际的，所以TiDAL利用一个额外的预测模块来学习标记数据的TD。为了进一步证明TiDAL的设计，我们提供了理论和实证证据。

    Active learning (AL) aims to select the most useful data samples from an unlabeled data pool and annotate them to expand the labeled dataset under a limited budget. Especially, uncertainty-based methods choose the most uncertain samples, which are known to be effective in improving model performance. However, AL literature often overlooks training dynamics (TD), defined as the ever-changing model behavior during optimization via stochastic gradient descent, even though other areas of literature have empirically shown that TD provides important clues for measuring the sample uncertainty. In this paper, we propose a novel AL method, Training Dynamics for Active Learning (TiDAL), which leverages the TD to quantify uncertainties of unlabeled data. Since tracking the TD of all the large-scale unlabeled data is impractical, TiDAL utilizes an additional prediction module that learns the TD of labeled data. To further justify the design of TiDAL, we provide theoretical and empirical evidence t
    
[^155]: 学习优化拟牛顿方法

    Learning to Optimize Quasi-Newton Methods. (arXiv:2210.06171v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.06171](http://arxiv.org/abs/2210.06171)

    本文介绍了一种名为LODO的机器学习优化器，它通过将学习优化技术与拟牛顿方法相结合，实时学习并适应损失景观的局部特征，从而在线元学习最佳的预条件矩阵。

    

    快速基于梯度的优化算法对于计算高效地训练机器学习模型变得越来越重要。一种技术是将梯度乘以一个预条件矩阵来产生一步，但最好的预条件矩阵是不清楚的。本文引入了一种新颖的机器学习优化器LODO，它尝试在优化过程中在线元学习最佳的预条件矩阵。具体而言，我们的优化器将学习优化（L2O）技术与拟牛顿方法相结合，以神经网络为参数化的预条件矩阵进行学习；它们比其他拟牛顿方法中的预条件矩阵更灵活。与其他L2O方法不同，LODO不需要在训练任务分布上进行元训练，而是在测试任务上进行优化时实时学习优化，适应遍历中的损失景观的局部特征。从理论上讲，我们证明了我们的优化器近似地拟合了负Hessian矩阵。

    Fast gradient-based optimization algorithms have become increasingly essential for the computationally efficient training of machine learning models. One technique is to multiply the gradient by a preconditioner matrix to produce a step, but it is unclear what the best preconditioner matrix is. This paper introduces a novel machine learning optimizer called LODO, which tries to online meta-learn the best preconditioner during optimization. Specifically, our optimizer merges Learning to Optimize (L2O) techniques with quasi-Newton methods to learn preconditioners parameterized as neural networks; they are more flexible than preconditioners in other quasi-Newton methods. Unlike other L2O methods, LODO does not require any meta-training on a training task distribution, and instead learns to optimize on the fly while optimizing on the test task, adapting to the local characteristics of the loss landscape while traversing it. Theoretically, we show that our optimizer approximates the inverse
    
[^156]: 有限时间内使用线性函数逼近进行时序差异学习的分析：尾平均和正则化

    Finite time analysis of temporal difference learning with linear function approximation: Tail averaging and regularisation. (arXiv:2210.05918v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.05918](http://arxiv.org/abs/2210.05918)

    本研究通过引入尾平均和正则化技术，对时序差异(TD)学习算法进行了有限时间行为的研究。我们得出结论，尾平均TD能以最优速率 $O(1/t)$ 收敛，并且初始误差衰减速率更快。此外，正则化的TD版本在具有病态特征的问题上很有用。

    

    本文研究了将流行的时序差异(TD)学习算法与尾平均相结合时的有限时间行为。我们在不需要关于底层投影TD不动点矩阵的特征值信息的步长选择下，推导了尾平均TD迭代的参数误差的有限时间界。我们的分析表明，尾平均TD以期望速率和高概率收敛于最优的 $O(1/t)$ 速率。此外，我们的界限展示了初始误差(偏差)的更快衰减速率，这是对所有迭代的平均值的改进。我们还提出并分析了一种结合正则化的TD变体。通过分析，我们得出结论认为正则化的TD版本在具有病态特征的问题上是有用的。

    We study the finite-time behaviour of the popular temporal difference (TD) learning algorithm when combined with tail-averaging. We derive finite time bounds on the parameter error of the tail-averaged TD iterate under a step-size choice that does not require information about the eigenvalues of the matrix underlying the projected TD fixed point. Our analysis shows that tail-averaged TD converges at the optimal $O\left(1/t\right)$ rate, both in expectation and with high probability. In addition, our bounds exhibit a sharper rate of decay for the initial error (bias), which is an improvement over averaging all iterates. We also propose and analyse a variant of TD that incorporates regularisation. From analysis, we conclude that the regularised version of TD is useful for problems with ill-conditioned features.
    
[^157]: 广义核正则化最小二乘法

    Generalized Kernel Regularized Least Squares. (arXiv:2209.14355v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.14355](http://arxiv.org/abs/2209.14355)

    本论文提出了广义核正则化最小二乘法 (gKRLS)，解决了核正则化最小二乘法 (KRLS) 在当前使用中的两个限制：它的扩展能力不足，且即使在小规模数据集上，其计算代价也非常高昂。

    

    核正则化最小二乘法 (KRLS) 是一种流行的方法，用于灵活地估计具有复杂变量关系的模型。然而，其可用性因两个原因而受到许多研究人员的限制。首先，现有方法缺乏灵活性，不允许将KRLS与理论动机下的扩展如随机效应、未经正则化的固定效应或非高斯结果组合使用。其次，即使是规模较小的数据集，估计也非常计算密集。本文通过引入广义KRLS (gKRLS) 来解决这两个问题。我们指出，KRLS可以重新设定为分层模型，从而允许轻松推理和模块化模型构建，在其中KRLS可以与随机效应、样条和未经正则化的固定效应并用。在计算方面，我们还实现了随机草图方法，以极大地加速估计，并在估计质量上承担有限的惩罚。我们证明gKRLS可适用于具有大量样本的数据集的拟合。

    Kernel Regularized Least Squares (KRLS) is a popular method for flexibly estimating models that may have complex relationships between variables. However, its usefulness to many researchers is limited for two reasons. First, existing approaches are inflexible and do not allow KRLS to be combined with theoretically-motivated extensions such as random effects, unregularized fixed effects, or non-Gaussian outcomes. Second, estimation is extremely computationally intensive for even modestly sized datasets. Our paper addresses both concerns by introducing generalized KRLS (gKRLS). We note that KRLS can be re-formulated as a hierarchical model thereby allowing easy inference and modular model construction where KRLS can be used alongside random effects, splines, and unregularized fixed effects. Computationally, we also implement random sketching to dramatically accelerate estimation while incurring a limited penalty in estimation quality. We demonstrate that gKRLS can be fit on datasets with
    
[^158]: 鲁棒飞行控制的神经移动视界估计

    Neural Moving Horizon Estimation for Robust Flight Control. (arXiv:2206.10397v10 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2206.10397](http://arxiv.org/abs/2206.10397)

    本文提出了一种神经移动视界估计器（NeuroMHE），它可以自动调整神经网络建模的关键参数，并适应不同的飞行场景。通过推导出与加权矩阵相关的解析梯度，我们实现了将该估计器作为可学习层嵌入神经网络进行高效学习。此外，我们还开发了一种基于模型的策略梯度算法，以从四旋翼轨迹跟踪误差中直接训练NeuroMHE，而不需要地面真实干扰数据。

    

    估计和应对干扰对于四旋翼飞行控制至关重要。现有的估计器通常需要对特定飞行场景进行大量调整，或者经过广泛的地面真实干扰数据训练，才能实现令人满意的性能。在本文中，我们提出了一种神经移动视界估计器（NeuroMHE），它可以自动调整由神经网络建模的关键参数，并适应不同的飞行场景。我们通过推导与加权矩阵相关的MHE估计的解析梯度，实现了将MHE作为可学习层嵌入神经网络以实现高效学习的无缝融合。有趣的是，我们证明可以使用递归形式的卡尔曼滤波器高效地计算出梯度。此外，我们还开发了一种基于模型的策略梯度算法，以从四旋翼轨迹跟踪误差中直接训练NeuroMHE，而不需要地面真实干扰数据。

    Estimating and reacting to disturbances is crucial for robust flight control of quadrotors. Existing estimators typically require significant tuning for a specific flight scenario or training with extensive ground-truth disturbance data to achieve satisfactory performance. In this paper, we propose a neural moving horizon estimator (NeuroMHE) that can automatically tune the key parameters modeled by a neural network and adapt to different flight scenarios. We achieve this by deriving the analytical gradients of the MHE estimates with respect to the weighting matrices, which enables a seamless embedding of the MHE as a learnable layer into neural networks for highly effective learning. Interestingly, we show that the gradients can be computed efficiently using a Kalman filter in a recursive form. Moreover, we develop a model-based policy gradient algorithm to train NeuroMHE directly from the quadrotor trajectory tracking error without needing the ground-truth disturbance data. The effec
    
[^159]: 图上的多尺度Wasserstein最短路径过滤核心

    Multi-scale Wasserstein Shortest-path Filtration Kernels on Graphs. (arXiv:2206.00979v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.00979](http://arxiv.org/abs/2206.00979)

    这篇论文提出了一种名为多尺度Wasserstein最短路径过滤图核心（MWSPF）的新型最短路径图核心，解决了传统核心的信息丢失和缺乏多个尺度考虑的问题。

    

    传统的最短路径图核心（SP）是最受欢迎的图核心之一。它将图分解为最短路径，并计算每个图中最短路径的频率。然而，SP面临两个主要挑战：首先，最短路径的三元表示失去了信息。其次，SP比较图时没有考虑到图结构的多个不同尺度，而这在现实世界的图中很常见，例如社交网络中的链状结构、环状结构和星状结构。为了克服这两个挑战，我们开发了一种新颖的最短路径图核心，称为多尺度Wasserstein最短路径过滤图核心（MWSPF）。它使用以每个顶点为根的某个深度的BFS树来限制考虑最短路径的最大长度，考虑到小世界特性。它考虑了最短路径中所有顶点的标签。为了方便在多个不同尺度上比较图，它从顶点和

    The traditional shortest-path graph kernel (SP) is one of the most popular graph kernels. It decomposes graphs into shortest paths and computes their frequencies in each graph. However, SP has two main challenges: Firstly, the triplet representation of the shortest path loses information. Secondly, SP compares graphs without considering the multiple different scales of the graph structure which is common in real-world graphs, e.g., the chain-, ring-, and star-structures in social networks. To overcome these two challenges, we develop a novel shortest-path graph kernel called the Multi-scale Wasserstein Shortest-Path Filtration graph kernel (MWSPF). It uses a BFS tree of a certain depth rooted at each vertex to restrict the maximum length of the shortest path considering the small world property. It considers the labels of all the vertices in the shortest path. To facilitate the comparison of graphs at multiple different scales, it augments graphs from both the aspects of the vertex and
    
[^160]: 具有相关权重的深度神经网络：高斯过程混合极限、重尾、稀疏性和可压缩性

    Deep neural networks with dependent weights: Gaussian Process mixture limit, heavy tails, sparsity and compressibility. (arXiv:2205.08187v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.08187](http://arxiv.org/abs/2205.08187)

    本文研究了具有相关权重的深度神经网络的极限行为，发现无限宽度神经网络的每一层可以通过两个简单的量来刻画，当其中至少一层的量是非平凡的时候，得到了高斯过程的混合模型。

    

    本文研究了具有相关权重并通过高斯分布混合建模的无限宽度前馈深度神经网络的极限。网络的每个隐藏节点被分配一个非负随机变量，该随机变量控制该节点的输出权重的方差。我们对这些节点随机变量做了最小的假设：它们是独立同分布的，并且在无限宽度极限下，每一层的随机变量和收敛到一些有限的随机变量。在这个模型下，我们证明了无限宽度神经网络的每一层可以通过两个简单的量来刻画：一个非负标量参数和一个正实数上的Lévy测度。如果标量参数严格为正且所有隐藏层的Lévy测度都是平凡的，那么就得到了经典的高斯过程(GP)极限，即通过独立同分布的高斯权重获得。更有趣的是，如果至少一层的Lévy测度是非平凡的，我们得到了高斯过程的混合模型。

    This article studies the infinite-width limit of deep feedforward neural networks whose weights are dependent, and modelled via a mixture of Gaussian distributions. Each hidden node of the network is assigned a nonnegative random variable that controls the variance of the outgoing weights of that node. We make minimal assumptions on these per-node random variables: they are iid and their sum, in each layer, converges to some finite random variable in the infinite-width limit. Under this model, we show that each layer of the infinite-width neural network can be characterised by two simple quantities: a non-negative scalar parameter and a L\'evy measure on the positive reals. If the scalar parameters are strictly positive and the L\'evy measures are trivial at all hidden layers, then one recovers the classical Gaussian process (GP) limit, obtained with iid Gaussian weights. More interestingly, if the L\'evy measure of at least one layer is non-trivial, we obtain a mixture of Gaussian pro
    
[^161]: 用于全非线性偏微分方程的深度分支求解器

    A deep branching solver for fully nonlinear partial differential equations. (arXiv:2203.03234v2 [math.NA] UPDATED)

    [http://arxiv.org/abs/2203.03234](http://arxiv.org/abs/2203.03234)

    本文提出了一个多维深度学习实现的随机分支算法，用于解决全非线性偏微分方程。与其他方法相比，该算法能够在功能非线性和梯度项方面取得更好的解决效果，并提供了这些方法无法获得的解估计值。

    

    我们提出了一个多维深度学习实现的随机分支算法，用于数值解全非线性偏微分方程。这种方法旨在通过将神经网络与蒙特卡罗分支算法相结合，解决包含任意阶梯度项的功能非线性问题。与其他基于深度学习的偏微分方程求解器相比，它还可以检查学习到的神经网络函数的一致性。所展示的数值实验表明，该算法在全非线性示例中可以优于基于反向随机微分方程或Galerkin方法的深度学习方法，并提供这些方法无法获得的解估计值。

    We present a multidimensional deep learning implementation of a stochastic branching algorithm for the numerical solution of fully nonlinear PDEs. This approach is designed to tackle functional nonlinearities involving gradient terms of any orders, by combining the use of neural networks with a Monte Carlo branching algorithm. In comparison with other deep learning PDE solvers, it also allows us to check the consistency of the learned neural network function. Numerical experiments presented show that this algorithm can outperform deep learning approaches based on backward stochastic differential equations or the Galerkin method, and provide solution estimates that are not obtained by those methods in fully nonlinear examples.
    
[^162]: 更有效地攻击c-MARL：一种数据驱动的方法

    Attacking c-MARL More Effectively: A Data Driven Approach. (arXiv:2202.03558v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.03558](http://arxiv.org/abs/2202.03558)

    本研究提出了一种数据驱动的方法来评估合作多智能体强化学习 (c-MARL) 代理的稳健性，并通过模型构建更强大的对抗状态扰动以降低团队总奖励。同时，还提出了首个受害代理选择策略和数据驱动方法，以定义目标失败状态，从而实现更强大的对抗性攻击。实验证明，该方法在被测试的所有环境中一致优于其他基线。

    

    最近几年，针对合作多智能体强化学习（c-MARL）已经涌现了很多方法。然而，c-MARL代理在面对对抗攻击时的稳健性很少被研究。本文提出了一种名为 c-MBA 的基于模型的方法，用于评估 c-MARL 代理的稳健性。我们的方法可以产生比现有的基于模型的方法更强大的对抗性状态扰动，从而降低团队总奖励。另外，我们提出了第一个受害代理的选择策略和第一个基于数据驱动的方法来定义目标失败状态，每个失败状态都能帮助我们开发更强大的对抗性攻击，而无需对底层环境拥有专业知识。我们在两个代表性的 MARL 测试基准上进行了数值实验，结果表明我们的方法在所有测试环境中都优于其他基线：我们的基于模型的攻击在所有测试环境中都持续优于其他基线。

    In recent years, a proliferation of methods were developed for cooperative multi-agent reinforcement learning (c-MARL). However, the robustness of c-MARL agents against adversarial attacks has been rarely explored. In this paper, we propose to evaluate the robustness of c-MARL agents via a model-based approach, named c-MBA. Our proposed formulation can craft much stronger adversarial state perturbations of c-MARL agents to lower total team rewards than existing model-free approaches. In addition, we propose the first victim-agent selection strategy and the first data-driven approach to define targeted failure states where each of them allows us to develop even stronger adversarial attack without the expert knowledge to the underlying environment. Our numerical experiments on two representative MARL benchmarks illustrate the advantage of our approach over other baselines: our model-based attack consistently outperforms other baselines in all tested environments.
    
[^163]: Temporal Difference学习算法的控制论分析

    Control Theoretic Analysis of Temporal Difference Learning. (arXiv:2112.14417v5 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2112.14417](http://arxiv.org/abs/2112.14417)

    本研究对Temporal Difference学习算法进行了控制论分析，并引入了一个有限时间的框架，从控制论角度提供了对TD学习机制和强化学习领域的更深入洞察。

    

    本文旨在对Temporal Difference (TD)学习算法进行控制论分析。TD学习作为强化学习领域的基石，提供了一种近似计算与马尔科夫决策过程中给定策略相关的值函数的方法。尽管已存在多篇关于TD学习理论理解的研究成果，但直到最近几年，研究人员才能对其统计效率提供具体保证。本文引入了一个有限时间的控制论框架，用于分析TD学习，借鉴了线性系统控制领域的已有概念。因此，本文通过使用控制论导出的简单分析工具，为TD学习的机制和强化学习的更广阔领域提供了额外的洞察。

    The goal of this manuscript is to conduct a controltheoretic analysis of Temporal Difference (TD) learning algorithms. TD-learning serves as a cornerstone in the realm of reinforcement learning, offering a methodology for approximating the value function associated with a given policy in a Markov Decision Process. Despite several existing works that have contributed to the theoretical understanding of TD-learning, it is only in recent years that researchers have been able to establish concrete guarantees on its statistical efficiency. In this paper, we introduce a finite-time, control-theoretic framework for analyzing TD-learning, leveraging established concepts from the field of linear systems control. Consequently, this paper provides additional insights into the mechanics of TD learning and the broader landscape of reinforcement learning, all while employing straightforward analytical tools derived from control theory.
    
[^164]: 关于二次网络的表达能力和训练性的研究

    On Expressivity and Trainability of Quadratic Networks. (arXiv:2110.06081v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.06081](http://arxiv.org/abs/2110.06081)

    该论文基于二次人工神经元在深度学习中的重要作用，研究了二次网络的表达能力和训练性。通过应用样条理论和代数几何中的度量，证明了二次网络相比传统网络具有更好的模型表达能力。

    

    受生物神经元的多样性启发，二次人工神经元在深度学习模型中起着重要的作用。我们感兴趣的二次神经元类型将传统神经元中的内积运算替换为二次函数。尽管二次神经元网络取得了有希望的结果，但仍存在一些重要问题尚未得到很好的解决。从理论上讲，二次网络相比传统网络或传统网络通过二次激活的表达能力优越性尚未完全阐明，这使得二次网络的使用缺乏坚实的理论基础。从实践上讲，虽然可以通过通用反向传播方法训练二次网络，但它可能面临比传统对应网络更高的崩溃风险。为了解决这些问题，我们首先应用样条理论和代数几何中的一个度量来给出两个定理，证明了二次网络相比传统网络具有更好的模型表达能力。

    Inspired by the diversity of biological neurons, quadratic artificial neurons can play an important role in deep learning models. The type of quadratic neurons of our interest replaces the inner-product operation in the conventional neuron with a quadratic function. Despite promising results so far achieved by networks of quadratic neurons, there are important issues not well addressed. Theoretically, the superior expressivity of a quadratic network over either a conventional network or a conventional network via quadratic activation is not fully elucidated, which makes the use of quadratic networks not well grounded. Practically, although a quadratic network can be trained via generic backpropagation, it can be subject to a higher risk of collapse than the conventional counterpart. To address these issues, we first apply the spline theory and a measure from algebraic geometry to give two theorems that demonstrate better model expressivity of a quadratic network than the conventional c
    
[^165]: $k$次聚合的破裂维数（arXiv:2110.04763v2 [math.FA]已更新）

    Fat-Shattering Dimension of $k$-fold Aggregations. (arXiv:2110.04763v2 [math.FA] UPDATED)

    [http://arxiv.org/abs/2110.04763](http://arxiv.org/abs/2110.04763)

    该论文估计了实值函数类聚合规则的破裂维数，并给出了关于线性和仿射函数类的更尖锐上界和匹配的下界。同时改进了已知结果，并纠正了文献中的一些错误论断。

    

    我们对实值函数类聚合规则的破裂维数进行了估计。后者包括选择$k$个函数（每个类选择一个）并计算它们的点函数，如中值、平均值和最大值的所有方式。该界限是基于组成类的破裂维数表述的。对于线性和仿射函数类，我们提供了一个明显更尖锐的上界和一个相匹配的下界，实现了对$k$的最优依赖。在此过程中，我们改进了几个已知结果，同时指出和纠正了文献中的一些错误论断。

    We provide estimates on the fat-shattering dimension of aggregation rules of real-valued function classes. The latter consists of all ways of choosing $k$ functions, one from each of the $k$ classes, and computing a pointwise function of them, such as the median, mean, and maximum. The bound is stated in terms of the fat-shattering dimensions of the component classes. For linear and affine function classes, we provide a considerably sharper upper bound and a matching lower bound, achieving, in particular, an optimal dependence on $k$. Along the way, we improve several known results in addition to pointing out and correcting a number of erroneous claims in the literature.
    
[^166]: 鲁棒的特征级对抗是可解释性工具

    Robust Feature-Level Adversaries are Interpretability Tools. (arXiv:2110.03605v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.03605](http://arxiv.org/abs/2110.03605)

    鲁棒的特征级对抗攻击不仅提供了对模型表示的研究，还具有独特的多功能性和高度鲁棒性，可以用于各种规模的图像攻击，并且可以作为可解释性工具帮助识别网络中的错误。

    

    计算机视觉中对抗攻击的文献通常关注像素级扰动，这些扰动往往很难解释。最近的研究通过操纵图像生成器的潜在表示来创建“特征级”对抗扰动，为我们提供了探索可感知、可解释的对抗攻击的机会。我们做出了三个贡献。首先，我们观察到特征级对抗攻击提供了用于研究模型表示的有用输入类别。第二，我们展示了这些对抗攻击的独特多功能性和高度鲁棒性。我们证明它们可以用于在ImageNet规模上产生有针对性、通用性、伪装性、物理可实现性和黑盒攻击。第三，我们展示了如何将这些对抗图像用作实际的可解释性工具，用于识别网络中的错误。我们利用这些对抗攻击对特征和类别之间的虚假关联进行预测，然后通过设计“...

    The literature on adversarial attacks in computer vision typically focuses on pixel-level perturbations. These tend to be very difficult to interpret. Recent work that manipulates the latent representations of image generators to create "feature-level" adversarial perturbations gives us an opportunity to explore perceptible, interpretable adversarial attacks. We make three contributions. First, we observe that feature-level attacks provide useful classes of inputs for studying representations in models. Second, we show that these adversaries are uniquely versatile and highly robust. We demonstrate that they can be used to produce targeted, universal, disguised, physically-realizable, and black-box attacks at the ImageNet scale. Third, we show how these adversarial images can be used as a practical interpretability tool for identifying bugs in networks. We use these adversaries to make predictions about spurious associations between features and classes which we then test by designing "
    
[^167]: 用于可扩展分类任务的复杂度优化稀疏贝叶斯学习

    Complexity-Optimized Sparse Bayesian Learning for Scalable Classification Tasks. (arXiv:2107.08195v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.08195](http://arxiv.org/abs/2107.08195)

    本文提出了一种复杂度优化稀疏贝叶斯学习方法DQN-SBL来解决高维特征空间或大数据规模问题中的内存溢出和计算复杂度高的问题，并在大规模问题上展现了竞争力的泛化能力。

    

    稀疏贝叶斯学习（Sparse Bayesian Learning，SBL）构建了一个极其稀疏的概率模型，具有竞争力的泛化能力。然而，SBL需要求解一个复杂度为$O(M^3)$（M：特征维度）的大型协方差矩阵以更新正则化先验，这使得在特征空间维度高或数据规模大的问题中变得困难，容易遭遇内存溢出问题。本文提出了一种称为DQN-SBL的用于SBL的新型对角拟牛顿（Diagonal Quasi-Newton，DQN）方法来解决这个问题，它忽略了大型协方差矩阵的求逆，从而将复杂度降低到$O(M)$。利用各种不同规模的基准进行了对非线性和线性分类问题的全面评估。实验证明，DQN-SBL在具有非常稀疏模型的情况下具有竞争力的泛化能力，并且能够很好地扩展到大规模问题。

    Sparse Bayesian Learning (SBL) constructs an extremely sparse probabilistic model with very competitive generalization. However, SBL needs to invert a big covariance matrix with complexity $O(M^3)$ (M: feature size) for updating the regularization priors, making it difficult for problems with high dimensional feature space or large data size. As it may easily suffer from the memory overflow issue in such problems. This paper addresses this issue with a newly proposed diagonal Quasi-Newton (DQN) method for SBL called DQN-SBL where the inversion of big covariance matrix is ignored so that the complexity is reduced to $O(M)$. The DQN-SBL is thoroughly evaluated for non linear and linear classifications with various benchmarks of different sizes. Experimental results verify that DQN-SBL receives competitive generalization with a very sparse model and scales well to large-scale problems.
    
[^168]: AngularGrad：一种用于卷积神经网络角度收敛的新优化技术

    AngularGrad: A New Optimization Technique for Angular Convergence of Convolutional Neural Networks. (arXiv:2105.10190v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.10190](http://arxiv.org/abs/2105.10190)

    AngularGrad是一种新的优化器，通过考虑连续梯度的方向/角度行为来优化卷积神经网络的角度收敛。通过捕捉角度信息以获得更准确的步长，优化步骤变得更加平滑。

    

    卷积神经网络(CNNs)使用基于随机梯度下降(SGD)的优化器进行训练。最近，自适应时刻估计(Adam)优化器因其自适应动量而变得非常流行，从而解决了SGD的梯度消失问题。然而，现有的优化器仍然无法有效利用优化曲率信息。本文提出了一种新的AngularGrad优化器，它考虑了连续梯度的方向/角度的行为。这是文献中第一次尝试利用梯度角度信息而不仅仅是梯度大小。所提出的AngularGrad根据先前迭代的梯度角度信息生成一个得分来控制步长。因此，通过角度信息捕获到更准确的近期梯度步长，优化步骤变得更加平滑。基于使用正切或余弦函数的两个AngularGrad变体得到了开发。

    Convolutional neural networks (CNNs) are trained using stochastic gradient descent (SGD)-based optimizers. Recently, the adaptive moment estimation (Adam) optimizer has become very popular due to its adaptive momentum, which tackles the dying gradient problem of SGD. Nevertheless, existing optimizers are still unable to exploit the optimization curvature information efficiently. This paper proposes a new AngularGrad optimizer that considers the behavior of the direction/angle of consecutive gradients. This is the first attempt in the literature to exploit the gradient angular information apart from its magnitude. The proposed AngularGrad generates a score to control the step size based on the gradient angular information of previous iterations. Thus, the optimization steps become smoother as a more accurate step size of immediate past gradients is captured through the angular information. Two variants of AngularGrad are developed based on the use of Tangent or Cosine functions for comp
    
[^169]: 通过BAPC——先后参数比较解释AI

    Explainable AI by BAPC -- Before and After correction Parameter Comparison. (arXiv:2103.07155v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2103.07155](http://arxiv.org/abs/2103.07155)

    该论文介绍了一种通过纠正简单的基模型来解释AI预测的局部替代方法。研究结果表明，通过确定准确性损失、准确性和替代品忠实度之间的准确关系，可以得到理想大小的解释实例邻域，以实现最大的准确性和忠实度。

    

    介绍了一种用于纠正简单“基”模型的AI模型的局部替代品，表示解释AI预测的分析方法。在这里，将该方法应用于基模型为线性回归的情况下进行研究。AI模型逼近了线性模型的残差误差，并以可解释的基模型参数的变化形式提出了解释。为AI模型的准确性损失、AI模型的准确性和替代品忠实度之间的准确关系制定了准则。研究结果表明，在假设观测数据存在一定噪声的情况下，这些准则导致了一个理想大小的需要解释的实例邻域，以实现最大的准确性和忠实度。

    A local surrogate for an AI-model correcting a simpler 'base' model is introduced representing an analytical method to yield explanations of AI-predictions. The approach is studied here in the context of the base model being linear regression. The AI-model approximates the residual error of the linear model and the explanations are formulated in terms of the change of the interpretable base model's parameters. Criteria are formulated for the precise relation between lost accuracy of the surrogate, the accuracy of the AI-model, and the surrogate fidelity. It is shown that, assuming a certain maximal amount of noise in the observed data, these criteria induce neighborhoods of the instances to be explained which have an ideal size in terms of maximal accuracy and fidelity.
    
[^170]: MMD正则化的非平衡最优输运问题

    MMD-regularized Unbalanced Optimal Transport. (arXiv:2011.05001v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2011.05001](http://arxiv.org/abs/2011.05001)

    本文研究了使用MMD正则化的非平衡最优输运问题，提出了基于Fenchel对偶性的新度量方法，还提出了基于有限样本的凸规划用于估算问题，证明了估计量的一致性和误差速率。

    

    本文研究了非平衡最优输运（UOT）问题，其中使用最大平均偏差（MMD）正则化来实现边际约束。我们的研究动机在于观察到，现有的UOT研究主要关注基于$\phi$-散度（例如KL）的正则化。MMD作为互补的积分概率度量（IPM）家族之一，在UOT上的作用似乎不太被理解。我们的主要结果基于Fenchel对偶性，利用它我们能够研究MMD正则化的UOT（MMD-UOT）的特性。这种对偶结果的一个有趣结果是MMD-UOT诱导了一种新的度量方法，也属于IPM家族。此外，我们提出了基于有限样本的凸规划，用于估算MMD-UOT和相应的重心。在温和的条件下，我们证明了我们基于凸规划的估计量是一致的，而且估计误差以$\mathcal{O}(m^{-1/2})$的速率衰减。

    We study the unbalanced optimal transport (UOT) problem, where the marginal constraints are enforced using Maximum Mean Discrepancy (MMD) regularization. Our study is motivated by the observation that existing works on UOT have mainly focused on regularization based on $\phi$-divergence (e.g., KL). The role of MMD, which belongs to the complementary family of integral probability metrics (IPMs), as a regularizer in the context of UOT seems to be less understood. Our main result is based on Fenchel duality, using which we are able to study the properties of MMD-regularized UOT (MMD-UOT). One interesting outcome of this duality result is that MMD-UOT induces a novel metric over measures, which again belongs to the IPM family. Further, we present finite-sample-based convex programs for estimating MMD-UOT and the corresponding barycenter. Under mild conditions, we prove that our convex-program-based estimators are consistent, and the estimation error decays at a rate $\mathcal{O}\left(m^{-
    
[^171]: 究竟是ResNet？神经ODE及其数值解

    ResNet After All? Neural ODEs and Their Numerical Solution. (arXiv:2007.15386v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2007.15386](http://arxiv.org/abs/2007.15386)

    神经ODE模型的性能取决于训练过程中使用的数值方法，如果使用过于粗糙的解算器进行训练，则使用另一个数值误差相等或更小的解算器进行测试会导致准确性下降。

    

    最近提出的神经常微分方程(ODE)框架具有连续时间扩展离散残差神经网络的特点。但是，我们在这里展示，训练的神经ODE模型实际上取决于训练过程中使用的特定数值方法。如果训练出的模型被认为是从ODE生成的流动，那么可以选择另一个数值解算器，其数值误差大小相同或更小，而不会损失性能。我们观察到，如果训练依赖于过于粗糙的离散化解算器，则使用另一个数值误差相等或更小的解算器进行测试会导致准确性急剧下降。在这种情况下，向量场和数值方法的组合不能被解释为从ODE生成的流动，这可以被认为是神经ODE概念的致命断裂。然而，我们观察到存在一个临界步长，超过该步长，训练会产生一个有效的ODE向量场。

    A key appeal of the recently proposed Neural Ordinary Differential Equation (ODE) framework is that it seems to provide a continuous-time extension of discrete residual neural networks. As we show herein, though, trained Neural ODE models actually depend on the specific numerical method used during training. If the trained model is supposed to be a flow generated from an ODE, it should be possible to choose another numerical solver with equal or smaller numerical error without loss of performance. We observe that if training relies on a solver with overly coarse discretization, then testing with another solver of equal or smaller numerical error results in a sharp drop in accuracy. In such cases, the combination of vector field and numerical method cannot be interpreted as a flow generated from an ODE, which arguably poses a fatal breakdown of the Neural ODE concept. We observe, however, that there exists a critical step size beyond which the training yields a valid ODE vector field. W
    
[^172]: 舞蹈革命：通过课程学习和音乐生成长期舞蹈

    Dance Revolution: Long-Term Dance Generation with Music via Curriculum Learning. (arXiv:2006.06119v8 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2006.06119](http://arxiv.org/abs/2006.06119)

    本文提出了一种基于课程学习和音乐的长期舞蹈生成方法，通过设计新的seq2seq架构和处理长序列的策略，成功捕捉了音乐与舞蹈之间的细粒度对应关系，并减轻了错误。

    

    

    Dancing to music is one of human's innate abilities since ancient times. In machine learning research, however, synthesizing dance movements from music is a challenging problem. Recently, researchers synthesize human motion sequences through autoregressive models like recurrent neural network (RNN). Such an approach often generates short sequences due to an accumulation of prediction errors that are fed back into the neural network. This problem becomes even more severe in the long motion sequence generation. Besides, the consistency between dance and music in terms of style, rhythm and beat is yet to be taken into account during modeling. In this paper, we formalize the music-conditioned dance generation as a sequence-to-sequence learning problem and devise a novel seq2seq architecture to efficiently process long sequences of music features and capture the fine-grained correspondence between music and dance. Furthermore, we propose a novel curriculum learning strategy to alleviate err
    
[^173]: 电网中智能GPS欺骗攻击检测

    Intelligent GPS Spoofing Attack Detection in Power Grids. (arXiv:2005.04513v1 [eess.SY] CROSS LISTED)

    [http://arxiv.org/abs/2005.04513](http://arxiv.org/abs/2005.04513)

    本文提出了一种使用动态电力系统中PMU数据的神经网络GPS欺骗检测（NNGSD）方法，用于检测电网中的GPS欺骗攻击，并展示了该检测方法的实时性能。

    

    GPS容易受到GPS欺骗攻击（GSA），从而导致GPS接收器的时间和位置结果出现混乱。在电网中，相量测量装置（PMUs）使用GPS构建时间标记的测量结果，因此它们容易受到这种攻击的影响。本文提出了一种利用动态电力系统中PMU数据的神经网络GPS欺骗检测（NNGSD）方法来检测GSA。不同条件下的数值结果显示了所提出的检测方法的实时性能。

    The GPS is vulnerable to GPS spoofing attack (GSA), which leads to disorder in time and position results of the GPS receiver. In power grids, phasor measurement units (PMUs) use GPS to build time-tagged measurements, so they are susceptible to this attack. As a result of this attack, sampling time and phase angle of the PMU measurements change. In this paper, a neural network GPS spoofing detection (NNGSD) with employing PMU data from the dynamic power system is presented to detect GSAs. Numerical results in different conditions show the real-time performance of the proposed detection method.
    
[^174]: 一个统一的双线性LSTM框架

    A Unifying Framework of Bilinear LSTMs. (arXiv:1910.10294v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1910.10294](http://arxiv.org/abs/1910.10294)

    本文提出了一个统一的双线性LSTM框架，通过平衡线性和双线性项的表达能力，实现了对序列数据集中输入特征的非线性交互的利用，以实现更好的性能，同时不增加更多的学习参数。

    

    本文提出了一个新颖的统一双线性LSTM框架，可以表示和利用序列数据集中输入特征的非线性交互，以实现比线性LSTM更好的性能，同时不会增加更多需要学习的参数。为了实现这一点，我们的统一框架允许通过调整隐藏状态向量的大小与双线性项中权重矩阵的逼近质量之间的权衡来平衡线性和双线性项的表达能力，从而优化我们的双线性LSTM的性能，同时不会增加更多需要学习的参数。我们在几个基于语言的序列学习任务中对我们的双线性LSTM的性能进行了实证评估，以展示其普适性。

    This paper presents a novel unifying framework of bilinear LSTMs that can represent and utilize the nonlinear interaction of the input features present in sequence datasets for achieving superior performance over a linear LSTM and yet not incur more parameters to be learned. To realize this, our unifying framework allows the expressivity of the linear vs. bilinear terms to be balanced by correspondingly trading off between the hidden state vector size vs. approximation quality of the weight matrix in the bilinear term so as to optimize the performance of our bilinear LSTM, while not incurring more parameters to be learned. We empirically evaluate the performance of our bilinear LSTM in several language-based sequence learning tasks to demonstrate its general applicability.
    
[^175]: 用多样抽样有效地提高CNN特征图的分辨率

    Improving the Resolution of CNN Feature Maps Efficiently with Multisampling. (arXiv:1805.10766v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/1805.10766](http://arxiv.org/abs/1805.10766)

    本论文提出了一种称为多样抽样的CNN子采样技术，通过子采样层显著增加特征图保留的信息量，实验证明粗糙的特征图是影响神经网络在图像分类中性能的瓶颈。

    

    我们描述了一种新的CNN子采样技术类别，称为多样抽样，通过子采样层大大增加了特征图保留的信息量。我们的其中一个方法称为方格子采样，能够显著提高现有架构（如DenseNet和ResNet）的准确性，而无需任何额外参数，并且非常显著地提高了某些预先训练好的ImageNet模型的准确性，且无需训练或微调。我们对数据增强的性质有了一些可能的观察，并通过实验证明，粗糙的特征图是影响神经网络在图像分类中性能的瓶颈。

    We describe a new class of subsampling techniques for CNNs, termed multisampling, that significantly increases the amount of information kept by feature maps through subsampling layers. One version of our method, which we call checkered subsampling, significantly improves the accuracy of state-of-the-art architectures such as DenseNet and ResNet without any additional parameters and, remarkably, improves the accuracy of certain pretrained ImageNet models without any training or fine-tuning. We glean possible insight into the nature of data augmentations and demonstrate experimentally that coarse feature maps are bottlenecking the performance of neural networks in image classification.
    

