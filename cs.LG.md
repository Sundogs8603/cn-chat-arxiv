# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem.](http://arxiv.org/abs/2309.15111) | 本研究通过在两层神经网络上使用小批量SGD算法，在具有二次真实函数分隔数据的情况下，通过训练数量级为$d \:\text{polylog}(d)$的样本，将网络训练到了人口误差为$o(1)$的程度。这是首次在标准神经网络上以及标准训练下，展示了在各向同性数据上高效学习XOR函数的样本复杂度为$\tilde{O}(d)$。 |
| [^2] | [Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models.](http://arxiv.org/abs/2309.15098) | 本研究使用约束满足问题框架研究了语言模型的内部行为，发现模型对约束标记的关注程度与事实准确性强正相关。提出了一种方法可以预测约束满足和事实错误，并允许早期错误识别，进一步提高了大型语言模型的可靠性。 |
| [^3] | [Fixing the NTK: From Neural Network Linearizations to Exact Convex Programs.](http://arxiv.org/abs/2309.15096) | 本文从两个方向对深度神经网络进行理论分析，提供了通过神经切线核（NTK）和通过凸重塑ReLU网络的全局优化训练目标的方法。此外，我们还提出了一种与NTK相连的多核学习模型，称为门控ReLU网络，通过加权数据屏蔽特征映射来实现全局优化。 |
| [^4] | [Identifying Simulation Model Through Alternative Techniques for a Medical Device Assembly Process.](http://arxiv.org/abs/2309.15094) | 本论文通过使用Spline函数和机器学习模型来识别仿真模型，以提高对医疗设备组装过程中的快拍流程的理解和决策支持能力。 |
| [^5] | [VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning.](http://arxiv.org/abs/2309.15091) | 本文提出了VideoDirectorGPT，一种利用LLMs的知识实现一致多场景视频生成的框架，通过视频内容规划和基于内容的视频生成来生成时间上一致的长视频。 |
| [^6] | [Single Biological Neurons as Temporally Precise Spatio-Temporal Pattern Recognizers.](http://arxiv.org/abs/2309.15090) | 这篇博士论文的核心观点是，单一生物神经元具有时空模式识别的精确性和复杂性，与目前大多数神经科学家的观点不同。这一点对神经元组成的脑回路和神经活动的信息编码具有广泛的影响。 |
| [^7] | [On Excess Risk Convergence Rates of Neural Network Classifiers.](http://arxiv.org/abs/2309.15075) | 本文研究了基于神经网络的插值分类器在二分类问题中的性能，通过超额风险来衡量。研究考虑了更一般的场景，使得神经网络可以轻松应用数值优化方法。虽然函数类很大，但无维度速率是可能的。 |
| [^8] | [QUILT: Effective Multi-Class Classification on Quantum Computers Using an Ensemble of Diverse Quantum Classifiers.](http://arxiv.org/abs/2309.15056) | QUILT是一个针对当前容易出错的量子计算机设计的框架，其通过使用多样化的量子分类器集合，在MNIST数据集中表现出高达85%的多类分类准确率。 |
| [^9] | [Class Incremental Learning via Likelihood Ratio Based Task Prediction.](http://arxiv.org/abs/2309.15048) | 该论文提出了一种基于似然比的任务预测的类增量学习方法，利用离群检测器进行任务标识预测，解决了无任务标识符的测试样本的任务预测问题。 |
| [^10] | [Combining Survival Analysis and Machine Learning for Mass Cancer Risk Prediction using EHR data.](http://arxiv.org/abs/2309.15039) | 该论文介绍了一种利用 EHR 数据进行大规模肿瘤风险预测的新方法，其创新之处在于只需利用历史的医疗服务代码和诊断信息来实现最小化的数据需求，通过将存活分析和机器学习相结合，可以在大规模应用中实现对患者癌症风险的个性化评估。 |
| [^11] | [HPCR: Holistic Proxy-based Contrastive Replay for Online Continual Learning.](http://arxiv.org/abs/2309.15038) | HPCR是一种用于在线连续学习的新方法，该方法综合了基于代理和对比损失的重放方式。通过在对比损失中使用锚点-代理对替换锚点-样本对，HPCR能够减轻遗忘现象，并有效学习更细粒度的语义信息。实验证明，HPCR在多个任务上实现了最先进的性能。 |
| [^12] | [Making PPO even better: Value-Guided Monte-Carlo Tree Search decoding.](http://arxiv.org/abs/2309.15028) | 本文提出了一种基于值导向的Monte-Carlo Tree Search解码算法PPO-MCTS，通过在PPO之上集成MCTS，解决了训练和测试之间部分输出评分机制的不匹配问题，实验证明该算法可以显著提升性能。 |
| [^13] | [Synthia's Melody: A Benchmark Framework for Unsupervised Domain Adaptation in Audio.](http://arxiv.org/abs/2309.15024) | Synthia's melody提出了一个新颖的音频数据生成框架，为无监督领域自适应提供了一个适当的基准数据集，这个框架能够模拟各种旋律，并评估声学深度学习模型对分布转移的敏感性。 |
| [^14] | [Automating question generation from educational text.](http://arxiv.org/abs/2309.15004) | 本文设计并评估了一个用于学校形成性和总结性评估的自动化问题生成工具，通过对教师的调查，证明了自动化生成问题的需求，并提出了一个基于Transformer的语言模型的模块化框架，用于从文本内容中自动生成多项选择题。 |
| [^15] | [Measurement Models For Sailboats Price vs. Features And Regional Areas.](http://arxiv.org/abs/2309.14994) | 这项研究调查了帆船技术规格和价格之间的关系以及区域定价的影响。通过应用多个机器学习模型，我们发现单体船通常比双体船更实惠，并且长度、宽度、排水量和帆面积等特定规格与较高的价格直接相关。此外，我们还发现美国是平均帆船价格最高的国家，而国内生产总值与帆船价格没有直接相关关系。 |
| [^16] | [Tempo Adaption in Non-stationary Reinforcement Learning.](http://arxiv.org/abs/2309.14989) | 该论文解决了非平稳强化学习中"时间同步"问题，通过考虑墙钟时间而不是情节进展来实现对环境变化的适应。 |
| [^17] | [Statistical Analysis of Quantum State Learning Process in Quantum Neural Networks.](http://arxiv.org/abs/2309.14980) | 本文研究了在量子神经网络中学习未知量子态的问题，并提出了一个不可行定理。当损失值低于临界阈值时，避免局部极小值的概率随着量子比特数指数级减少，而随着电路深度多项式增长。局部极小值的曲率集中在量子Fisher信息乘以损失相关常数上，该常数表征输出态相对于QNN参数的敏感性。 |
| [^18] | [Recurrent Hypernetworks are Surprisingly Strong in Meta-RL.](http://arxiv.org/abs/2309.14970) | 递归超网络和循环神经网络在元强化学习中的端到端学习表现出惊人的强大性能，相比于现有专门方法更为简单但效果更好。 |
| [^19] | [Context-Aware Generative Models for Prediction of Aircraft Ground Tracks.](http://arxiv.org/abs/2309.14957) | 本研究提出了一种上下文感知的生成模型用于航空器地面航迹预测，通过利用概率机器学习建模飞行员行为和空中交通管制员意图的不确定性，从而改善了传统预测方法的局限性。训练的模型可以针对特定的飞行区域进行个性化预测，并且在繁忙的空域实验中取得了良好的性能。 |
| [^20] | [Towards Real-World Test-Time Adaptation: Tri-Net Self-Training with Balanced Normalization.](http://arxiv.org/abs/2309.14949) | 本文研究了真实世界测试时间自适应问题，在全局类别不平衡的测试集上补充了现有的协议，并提出了一种平衡归一化层来适应不平衡的测试数据，以解决现有方法的失败。 |
| [^21] | [Learning Generative Models for Climbing Aircraft from Radar Data.](http://arxiv.org/abs/2309.14941) | 本文提出了一种利用雷达数据学习的生成模型，能够准确预测攀升飞机的轨迹，并通过学习修正推力的函数来提高预测准确性。该方法的优势包括：与标准模型相比，到达时间的预测误差减少了66.3%；生成的轨迹与测试数据相比更加真实；并且能够以最小的计算成本计算置信区间。 |
| [^22] | [Parallel Multi-Objective Hyperparameter Optimization with Uniform Normalization and Bounded Objectives.](http://arxiv.org/abs/2309.14936) | 这项研究提出了一种多目标贝叶斯优化算法，通过统一目标归一化和随机化权重进行标量化，解决了多目标超参数优化中的挑战问题。 |
| [^23] | [Noise-Tolerant Unsupervised Adapter for Vision-Language Models.](http://arxiv.org/abs/2309.14928) | 这篇论文介绍了一种噪声容忍的无监督适配器(NtUA)，它可以使用少样本无标签目标样本来学习优秀的视觉语言模型。NtUA通过自适应缓存形成和伪标签修正来对抗伪标签噪声。 |
| [^24] | [Label Deconvolution for Node Representation Learning on Large-scale Attributed Graphs against Learning Bias.](http://arxiv.org/abs/2309.14907) | 本文提出了一种标签解卷积技术(LD)，通过对图神经网络(GNNs)的逆映射进行高效的近似，来解决在大规模属性图上进行节点表示学习时的学习偏差挑战。 |
| [^25] | [Verifiable Learned Behaviors via Motion Primitive Composition: Applications to Scooping of Granular Media.](http://arxiv.org/abs/2309.14894) | 通过运动原语构成的机器人行为模型能够从自然语言输入中实时生成可验证的行为，为工业机器人的灵活性提供了增强。实验表明这种模型在探索任务和处理颗粒状介质的实际情境下具有可验证性。 |
| [^26] | [Locality-preserving Directions for Interpreting the Latent Space of Satellite Image GANs.](http://arxiv.org/abs/2309.14883) | 我们提出了一种用于解释卫星图像GAN潜在空间的局部保持方法，能够捕捉卫星图像特有的大尺度和谱变异性，并恢复解释性方向，从而用于有导向合成。通过保持局部性，得到的向量更鲁棒，能更好地保持类别信息。 |
| [^27] | [Credit Card Fraud Detection with Subspace Learning-based One-Class Classification.](http://arxiv.org/abs/2309.14880) | 本文研究了基于子空间学习的单类分类方法在信用卡欺诈检测中的应用，该方法能够处理不平衡数据分布并具备预测和对抗未知欺诈技术的能力。 |
| [^28] | [Navigating Text-To-Image Customization:From LyCORIS Fine-Tuning to Model Evaluation.](http://arxiv.org/abs/2309.14859) | 本文介绍了LyCORIS，一个开源库，提供了多种稳定扩散模型的微调方法，并提出了一个系统评估的全面框架。 |
| [^29] | [Cluster Exploration using Informative Manifold Projections.](http://arxiv.org/abs/2309.14857) | 该论文提出了一种新颖的方法来生成信息丰富的嵌入，以揭示高维数据中的聚类结构。通过线性组合对比PCA和峰度投影追踪两个目标，该方法能够排除先验信息相关的结构并实现有意义的数据分离。 |
| [^30] | [Realtime Motion Generation with Active Perception Using Attention Mechanism for Cooking Robot.](http://arxiv.org/abs/2309.14837) | 该论文介绍了一种使用注意机制的预测性递归神经网络，能够实现实时感知和动作生成，以支持烹饪机器人在煮鸡蛋过程中对鸡蛋状态的感知和搅拌动作的调整。 |
| [^31] | [OS-net: Orbitally Stable Neural Networks.](http://arxiv.org/abs/2309.14822) | OS-net是一种专门用于周期动力数据的神经网络架构，通过利用常微分方程理论和伴随方法的反向传播方法，确保网络权重的稳定性，成功应用于发现R\"{o}ssler和Sprott系统的动力学。 |
| [^32] | [A Comparative Study of Population-Graph Construction Methods and Graph Neural Networks for Brain Age Regression.](http://arxiv.org/abs/2309.14816) | 本研究比较了不同的人口图构建方法和图神经网络对于大脑年龄回归的性能影响。 |
| [^33] | [Revisiting Softmax Masking for Stability in Continual Learning.](http://arxiv.org/abs/2309.14808) | 本文重新审视了用于连续学习中的Softmax掩码的影响，并提出了一种利用其置信度保持效果的方法，通过增加稳定性同时保持准确性。 |
| [^34] | [Evaluating Soccer Match Prediction Models: A Deep Learning Approach and Feature Optimization for Gradient-Boosted Trees.](http://arxiv.org/abs/2309.14807) | 本研究评估了足球比赛预测模型，并采用深度学习方法和梯度增强树特征优化。研究发现，在这个特定的任务中，深度学习模型经常被忽视。 |
| [^35] | [Transferring climate change knowledge.](http://arxiv.org/abs/2309.14780) | 通过转移学习方法，研究表明机器学习，尤其是深度神经网络，可以通过充分利用地球系统模型模拟和历史观测所获得的知识，更准确地预测21世纪的全球表面温度场。 |
| [^36] | [Exploring Small Language Models with Prompt-Learning Paradigm for Efficient Domain-Specific Text Classification.](http://arxiv.org/abs/2309.14779) | 本研究探索了将小型语言模型（SLMs）与提示学习范式结合应用于领域特定文本分类的潜力，并在零售业的客户和代理人交互中进行了评估。结果显示，在有限的标记数据下，SLM T5-base能够实现约75%的准确率，展现了SLMs与提示学习的潜力。 |
| [^37] | [Markov Chain Mirror Descent On Data Federation.](http://arxiv.org/abs/2309.14775) | 本文在联邦学习场景中提出了一种新的随机镜像下降方法MarchOn，在这种方法中，模型通过马尔科夫链进行迭代，具有最佳的收敛性能。 |
| [^38] | [BLIP-Adapter: Parameter-Efficient Transfer Learning for Mobile Screenshot Captioning.](http://arxiv.org/abs/2309.14774) | 本研究提出了一种参数高效的迁移学习方法，通过冻结图像标题模型的参数并只调整附加模块，解决了移动设备屏幕截图标题生成任务中大量参数的开销问题。 |
| [^39] | [Age Minimization in Massive IoT via UAV Swarm: A Multi-agent Reinforcement Learning Approach.](http://arxiv.org/abs/2309.14757) | 本论文通过应用多智能体深度强化学习方法，利用无人机群从物联网设备收集实时信息，以实现大规模物联网中信息的年龄最小化。研究结果表明，合作和部分合作的多智能体深度强化学习方法能够优于传统的集中式深度强化学习方法，在大规模网络中具有更好的性能表现。 |
| [^40] | [Effective Multi-Agent Deep Reinforcement Learning Control with Relative Entropy Regularization.](http://arxiv.org/abs/2309.14727) | 本文提出了一种新的多智能体强化学习方法，通过相对熵正则化解决了多个智能体策略更新的不一致性问题，并证明在多智能体合作和竞争任务以及传统控制任务中表现出显著的学习能力和样本效率。 |
| [^41] | [PLMM: Personal Large Models on Mobile Devices.](http://arxiv.org/abs/2309.14726) | 本文提出了一种从传统大型语言模型中提取的个人大型模型，该模型更适应于本地用户的个人信息，并且能够保护用户的隐私。该模型分为个人级别、专家级别和传统级别，同时还需要小型化以适应个人计算机或移动设备，并实现实时响应以提供更好的用户体验。 |
| [^42] | [QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models.](http://arxiv.org/abs/2309.14717) | 本文提出了QA-LoRA算法，它通过使用量化意识以及组内运算符来实现大语言模型的低秩适应。QA-LoRA能够将模型权重量化以减少时间和内存的使用，同时在不损失准确性的情况下将模型集成为一个量化模型。 |
| [^43] | [Explaining Deep Face Algorithms through Visualization: A Survey.](http://arxiv.org/abs/2309.14715) | 本文调查了解释深度人脸算法的可视化方法，并发现了人脸网络的结构和层次结构的有价值见解，为AI从业者提供了实用人脸可视化的设计考虑因素。 |
| [^44] | [On the Computational Complexity and Formal Hierarchy of Second Order Recurrent Neural Networks.](http://arxiv.org/abs/2309.14691) | 本研究扩展了二阶循环神经网络的理论基础，并证明了存在一类有界时间的二阶RNN是图灵完备的。该模型通过将转移表编码到其循环权重中实现有界时间计算，并在识别任务上优于现代模型。 |
| [^45] | [Are Human-generated Demonstrations Necessary for In-context Learning?.](http://arxiv.org/abs/2309.14681) | 本文研究了上下文学习中人工生成的演示是否有必要，并提出了一种新的自反思提示策略（SEC），通过这种策略，大型语言模型（LLMs）可以自行生成演示和最终输出，避免了手动生成过程的复杂性。 |
| [^46] | [FedCompass: Efficient Cross-Silo Federated Learning on Heterogeneous Client Devices using a Computing Power Aware Scheduler.](http://arxiv.org/abs/2309.14675) | FedCompass是一种创新的半异步联邦学习算法，通过在服务器端使用计算能力感知调度器，解决了异构客户端和数据中跨边界联邦学习的效率和收敛准确性问题。 |
| [^47] | [Leveraging Herpangina Data to Enhance Hospital-level Prediction of Hand-Foot-and-Mouth Disease Admissions Using UPTST.](http://arxiv.org/abs/2309.14674) | 提出了一种新颖的基于Transformer的UPTST模型，利用腭咽口炎数据提升手足口病住院预测的准确性，且在医院级别的预测准确性上优于现有方法。 |
| [^48] | [ALEX: Towards Effective Graph Transfer Learning with Noisy Labels.](http://arxiv.org/abs/2309.14673) | ALEX是一种用于解决存在标签噪声的图传输学习问题的新技术，通过使用图对比学习和平衡标签分布的子图构建方法来提供稳健的节点表示。 |
| [^49] | [DONNAv2 -- Lightweight Neural Architecture Search for Vision tasks.](http://arxiv.org/abs/2309.14670) | DONNAv2是一种轻量级神经体系结构搜索方法，用于视觉任务。它采用计算效率的方式探索不同学习任务的高效体系结构，并通过消除精度预测器来实现计算效率的设置。 |
| [^50] | [ZiCo-BC: A Bias Corrected Zero-Shot NAS for Vision Tasks.](http://arxiv.org/abs/2309.14666) | 本文针对零样本神经架构搜索（NAS）方法的主要问题进行了研究，发现现有的零样本代理存在偏差，并提出了一种名为ZiCo-BC的修正偏差的方法。实验证明ZiCo-BC在多个视觉任务上都能成功搜索出优化的架构。 |
| [^51] | [Tranformer-based classification of user queries for medical consultancy with respect to expert specialisation.](http://arxiv.org/abs/2309.14662) | 本研究利用RuBERT模型和Transformer技术，提出了一种用于医学咨询的用户查询分类方法，重点关注专家特长，表现出超过92%的性能，具有良好的泛化性能和实际应用价值。 |
| [^52] | [Learning the Uncertainty Sets for Control Dynamics via Set Membership: A Non-Asymptotic Analysis.](http://arxiv.org/abs/2309.14648) | 本文提供了对于线性动力系统中通过集合成员身份估计产生的不确定性集合直径的非渐近界限，并将结果应用于鲁棒自适应模型预测控制。通过数值实验证明了鲁棒自适应控制器的快速接近离线最优模型预测控制器的性能。 |
| [^53] | [Gray-box Adversarial Attack of Deep Reinforcement Learning-based Trading Agents.](http://arxiv.org/abs/2309.14615) | 本研究展示了一种通过在同一股票市场进行交易的方式，利用灰盒方法对基于深度强化学习的交易代理进行攻击的可能性。这种方法可以应对交易代理受到对手操纵的问题。 |
| [^54] | [Reparameterized Variational Rejection Sampling.](http://arxiv.org/abs/2309.14612) | 本文提出了一种重参数化变分拒绝采样（VRS）方法，通过将参数化的提议分布与拒绝采样结合，定义了一个丰富的非参数分布族，明确利用已知的目标分布，为具有连续潜变量的模型提供了一种吸引人的推断策略。 |
| [^55] | [Unsupervised Graph Deep Learning Reveals Emergent Flood Risk Profile of Urban Areas.](http://arxiv.org/abs/2309.14610) | 本研究基于无监督图深度学习模型提出了集成城市洪水风险评级模型，能够捕捉区域之间的空间依赖关系和洪水危险与城市要素之间的复杂相互作用，揭示了城市地区的突发洪水风险概况 |
| [^56] | [Neuro-Visualizer: An Auto-encoder-based Loss Landscape Visualization Method.](http://arxiv.org/abs/2309.14601) | Neuro-Visualizer是一种基于自动编码器的非线性地形可视化方法，可以更好地表示高维损失地形，通过对机器学习中的问题进行实验，证实其优于其他方法，并提供有关神经网络损失地形的有用见解。 |
| [^57] | [Policy Optimization in a Noisy Neighborhood: On Return Landscapes in Continuous Control.](http://arxiv.org/abs/2309.14597) | 本论文提供了新的视角，研究了连续控制中深度强化学习智能体性能不稳定的原因。通过对回报景观进行分析，发现了策略空间中的失败区域和策略品质的隐藏维度。此外，提出了一种分布感知的方法，改善了策略的鲁棒性。 |
| [^58] | [Efficient Post-training Quantization with FP8 Formats.](http://arxiv.org/abs/2309.14592) | 本研究研究了FP8格式在后训练量化中的优势，并开发了一个通用的量化工作流程。实验结果表明，FP8相对于INT8具有更好的工作负载覆盖率和模型准确性。 |
| [^59] | [Applications of Sequential Learning for Medical Image Classification.](http://arxiv.org/abs/2309.14591) | 该论文介绍了一种应用顺序学习的医学图像分类方法，通过开发神经网络训练框架对少量的医学图像数据进行持续训练，并提供了评估训练的启发式方法。通过解决过拟合、灾难性遗忘和概念漂移等问题，该方法能够有效地进行医学图像分类。 |
| [^60] | [Joint Communication and Computation Framework for Goal-Oriented Semantic Communication with Distortion Rate Resilience.](http://arxiv.org/abs/2309.14587) | 本论文提出了一个创新的联合通信和计算框架，利用率畸变理论来分析通信和语义压缩引起的畸变，从而评估其对目标导向语义通信中人工智能模型性能的影响，使目标导向语义通信问题成为可能。 |
| [^61] | [DifAttack: Query-Efficient Black-Box Attack via Disentangled Feature Space.](http://arxiv.org/abs/2309.14585) | DifAttack是一种基于解耦特征空间的高效黑盒攻击方法，通过迭代优化对抗特征并利用受害者模型的查询反馈生成成功的对抗样本。 |
| [^62] | [CWCL: Cross-Modal Transfer with Continuously Weighted Contrastive Loss.](http://arxiv.org/abs/2309.14580) | 本文提出了一种称为CWCL的损失函数，用于跨模态迁移中的对比训练。相比于传统的二进制对比训练，CWCL使用连续的相似性度量，可以更好地对齐实例的表示并提高跨模态的迁移性能。 |
| [^63] | [Integrating Higher-Order Dynamics and Roadway-Compliance into Constrained ILQR-based Trajectory Planning for Autonomous Vehicles.](http://arxiv.org/abs/2309.14566) | 本文将高阶动态和道路合规性整合到基于约束的ILQR轨迹规划中的自动驾驶车辆中，提供了更安全和舒适的路径规划。 |
| [^64] | [Disruption Detection for a Cognitive Digital Supply Chain Twin Using Hybrid Deep Learning.](http://arxiv.org/abs/2309.14557) | 本文介绍了一种使用混合深度学习的方法，用于在认知数字供应链双胞胎框架中进行打乱检测，以增强供应链的韧性。所提出的方法在实时的打乱情况下，能够帮助决策者和供应链实践者做出适当的决策，以最小化打乱事件的负面影响。 |
| [^65] | [Tactile Estimation of Extrinsic Contact Patch for Stable Placement.](http://arxiv.org/abs/2309.14552) | 本文介绍了一种利用触觉读数推测物体放置稳定性的方法，通过对接触区域的估计可以有效设计机器人的反馈技能，提高机器人的精细操控能力。 |
| [^66] | [Cluster-based Method for Eavesdropping Identification and Localization in Optical Links.](http://arxiv.org/abs/2309.14541) | 本文提出了一种基于聚类的方法，用于在光线系统中检测和定位小功率损失的窃听事件。研究结果表明，通过光性能监测数据可以检测这种微小的窃听损失，同时通过在线数据可以有效地定位这类事件。 |
| [^67] | [Effect of roundabout design on the behavior of road users: A case study of roundabouts with application of Unsupervised Machine Learning.](http://arxiv.org/abs/2309.14540) | 本研究通过无监督机器学习应用的圆环案例研究，评估了圆环的性能，并研究了人类驾驶员与圆环的互动行为。研究发现，圆环可以显著降低转弯路口的速度，而其对速度的影响取决于道路使用者的行为评级。对于巴士、汽车和卡车驾驶员的行为进行了分类，并开发出一种预测圆环交叉口道路使用者行为的方法。安全主要归功于圆环的两个固有特征。 |
| [^68] | [Detach-ROCKET: Sequential feature selection for time series classification with random convolutional kernels.](http://arxiv.org/abs/2309.14518) | 本文提出了一种名为Detach-ROCKET的方法，用于时间序列分类中的顺序特征选择。通过利用随机卷积核模型中的大量特征，并使用顺序特征分离方法剪枝非主要特征，提高了模型的可扩展性和泛化能力。 |
| [^69] | [DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models.](http://arxiv.org/abs/2309.14509) | 本论文介绍了DeepSpeed-Ulysses，一种用于实现具备极长序列长度的高效可扩展LLM训练的新颖方法。 |
| [^70] | [Uncertainty Aware Deep Learning for Particle Accelerators.](http://arxiv.org/abs/2309.14502) | 本文针对粒子加速器的问题，提出了一种不确定性感知的深度学习方法。通过实施距离感知不确定性估计，可以在输入样本与训练数据不相似时检测并提供预测的可信度。该方法在Spallation Neutron Source (SNS)加速器的错误束流预测（分类）和Fermi National Accelerator Lab (FNAL) Booster Accelerator Complex（回归）的代理模型中取得了良好的效果。 |
| [^71] | [Era Splitting.](http://arxiv.org/abs/2309.14496) | 本研究提出了两种新的分裂准则，使得决策树模型能够利用时代信息进行优化，从而将超分布泛化研究中的思想应用于决策树模型。 |
| [^72] | [Classifying token frequencies using angular Minkowski $p$-distance.](http://arxiv.org/abs/2309.14495) | 使用角度明可夫斯基$p$-距离可以获得比传统余弦相似度更高的分类性能 |
| [^73] | [Explainable and Accurate Natural Language Understanding for Voice Assistants and Beyond.](http://arxiv.org/abs/2309.14485) | 这项研究改进了联合NLU模型的准确性，并使其可解释，同时扩展适用于其他分类任务。 |
| [^74] | [Unveiling the Potential of Deep Learning Models for Solar Flare Prediction in Near-Limb Regions.](http://arxiv.org/abs/2309.14483) | 本研究评估了使用深度学习模型预测近边区域太阳耀斑的性能，发现基于AlexNet的模型表现出最佳预测敏感性，可为太阳耀斑预测提供潜在的解决方案。 |
| [^75] | [LogGPT: Log Anomaly Detection via GPT.](http://arxiv.org/abs/2309.14482) | 本论文提出了LogGPT，它是一个使用GPT进行日志异常检测的框架。通过语言建模和强化学习策略，LogGPT能够有效地检测系统日志中的异常情况。 |
| [^76] | [Adapting Double Q-Learning for Continuous Reinforcement Learning.](http://arxiv.org/abs/2309.14471) | 本文介绍了一种新颖的校正偏差方法，通过使用两个组件的混合策略并由分开的网络进行评估，消除了离线策略强化学习算法中的过高估计偏差。在一小组MuJoCo环境中，该方法显示出了有希望接近SOTA的结果。 (校正偏差方法，混合策略，分开的网络评估) |
| [^77] | [FARSEC: A Reproducible Framework for Automatic Real-Time Vehicle Speed Estimation Using Traffic Cameras.](http://arxiv.org/abs/2309.14468) | FARSEC是一种可重现的基于交通摄像头的自动实时车辆速度估计框架，可以应对不同数据集的鲁棒性问题，并通过估计道路段长度的新颖技术提高了准确性。 |
| [^78] | [DefGoalNet: Contextual Goal Learning from Demonstrations For Deformable Object Manipulation.](http://arxiv.org/abs/2309.14463) | 本文提出了一种名为DefGoalNet的神经网络，通过少量人类示范直接学习可变形物体的目标形状。实验证明，在各种机器人任务中取得了显著的进展。 |
| [^79] | [Skilog: A Smart Sensor System for Performance Analysis and Biofeedback in Ski Jumping.](http://arxiv.org/abs/2309.14455) | 该论文介绍了一种用于滑雪跳台运动中的智能传感器系统，可以在实时中进行性能分析和生物反馈。通过测量滑雪靴鞋垫上的脚压力，系统可以提供给教练改善反馈和运动员即时的行动反馈。 |
| [^80] | [Learning dislocation dynamics mobility laws from large-scale MD simulations.](http://arxiv.org/abs/2309.14450) | 本文介绍了一种机器学习框架，用于从大规模分子动力学模拟中学习位错动力学的迁移规律。通过将迁移规律建模为图神经网络，并在BCC钨上进行大规模离散位错动力学模拟，我们证明了我们的方法能够准确地重现具有挑战性拉伸/压缩行为。 |
| [^81] | [Self-Recovery Prompting: Promptable General Purpose Service Robot System with Foundation Models and Self-Recovery.](http://arxiv.org/abs/2309.14425) | 本文研究开发了一个通用服务机器人系统，该系统可以根据不同任务和环境的变化进行自适应，并通过自恢复机制解决信息不足、计划生成错误和执行失败等问题，实现了任务的成功完成。 |
| [^82] | [On the expressivity of embedding quantum kernels.](http://arxiv.org/abs/2309.14419) | 量子核方法是量子和经典机器学习之间最自然的联系之一。本文探讨了嵌入式量子核的表达能力，并得出结论：通过引入计算普适性，任何核函数都可以表示为量子特征映射和嵌入式量子核。 |
| [^83] | [Provable advantages of kernel-based quantum learners and quantum preprocessing based on Grover's algorithm.](http://arxiv.org/abs/2309.14406) | 本研究展示了基于核的量子学习器和基于Grover算法的量子预处理的可证明优势，通过应用于模式匹配问题，提供了实际可行的优势，并结合经典分类方法进一步提高分类器性能。 |
| [^84] | [pLMFPPred: a novel approach for accurate prediction of functional peptides integrating embedding from pre-trained protein language model and imbalanced learning.](http://arxiv.org/abs/2309.14404) | 该论文提出了一种名为pLMFPPred的新方法，通过结合预训练蛋白质语言模型的嵌入和不平衡学习技术，准确预测功能肽。实验结果表明，pLMFPPred在预测功能肽方面优于现有的方法。 |
| [^85] | [DECORAIT -- DECentralized Opt-in/out Registry for AI Training.](http://arxiv.org/abs/2309.14400) | DECORAIT是一个去中心化的注册表，为内容创作者提供选择加入或退出AI训练的权利，并通过组合分层聚类和on/off-chain存储的方式来追踪GenAI训练数据的来源，以确定训练同意并奖励贡献该数据的创意人。 |
| [^86] | [Seeing and hearing what has not been said; A multimodal client behavior classifier in Motivational Interviewing with interpretable fusion.](http://arxiv.org/abs/2309.14398) | 本文提出了一个多模态分类器，在动机性访谈中准确区分了变化话语、持续话语和跟随/中立话语三种类别。该分类器利用文本、声调、面部表情和身体表现等多模态特征，并对AnnoMI数据集进行了注释和训练。研究还找到了决策过程中最重要的模态，提供了宝贵的洞察。 |
| [^87] | [Predicting environment effects on breast cancer by implementing machine learning.](http://arxiv.org/abs/2309.14397) | 乳腺癌的发展不仅与遗传因素有关，环境因素也起着重要作用。本研究综述了影响乳腺癌风险、发病率和结局的各种环境因素，包括生活方式决策和环境污染物。 |
| [^88] | [Guess & Sketch: Language Model Guided Transpilation.](http://arxiv.org/abs/2309.14396) | 本论文通过结合概率性神经语言模型和符号化方法，提出了一种语言模型引导的转译方法，用于自动翻译汇编代码程序，以缩短维护遗留软件的时间和工程成本。 |
| [^89] | [Implicit Sensing in Traffic Optimization: Advanced Deep Reinforcement Learning Techniques.](http://arxiv.org/abs/2309.14395) | 本论文提出了一个基于深度强化学习的集成车辆跟随和变道决策控制系统，旨在解决高速公路上突发路障情况下智能车辆的行车问题。 |
| [^90] | [Multiple Noises in Diffusion Model for Semi-Supervised Multi-Domain Translation.](http://arxiv.org/abs/2309.14394) | 本文提出了一种多噪声扩散模型（MDD）用于半监督多域翻译，通过引入噪声级别来对缺失的域进行建模，实现了任意域之间的翻译而不需要训练单独的模型。 |
| [^91] | [LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models.](http://arxiv.org/abs/2309.14393) | 本研究提出了LLMCarbon，一个针对密集型和MoE LLMs设计的端到端碳足迹预测模型，解决了现有工具的限制，并显著提升了估计的准确性。 |
| [^92] | [Unveiling Fairness Biases in Deep Learning-Based Brain MRI Reconstruction.](http://arxiv.org/abs/2309.14392) | 本研究发现基于深度学习的脑MRI重建模型存在性别和年龄子组之间的显著性能偏差，并揭示了数据不平衡和训练歧视不是偏见的主要来源。 |
| [^93] | [An AI Chatbot for Explaining Deep Reinforcement Learning Decisions of Service-oriented Systems.](http://arxiv.org/abs/2309.14391) | 一个AI聊天机器人被介绍来解释深度强化学习在面向服务系统中的决策过程，通过提供自然语言解释来帮助用户理解和建立信任。 |
| [^94] | [Early Churn Prediction from Large Scale User-Product Interaction Time Series.](http://arxiv.org/abs/2309.14390) | 本文通过对历史数据进行全面研究，提出了一种预测用户早期流失的模型，以促进业务决策和行动。 |
| [^95] | [Exploring Robot Morphology Spaces through Breadth-First Search and Random Query.](http://arxiv.org/abs/2309.14387) | 通过比较分析广度优先搜索（BFS）和随机查询在模块化机器人脑体共同进化中的影响，本研究发现这两种查询机制对机器人形态的进化和性能具有重要影响。 |
| [^96] | [Sampling - Variational Auto Encoder - Ensemble: In the Quest of Explainable Artificial Intelligence.](http://arxiv.org/abs/2309.14385) | 本研究通过采样-变分自编码器（VAE）-集成异常检测（SVEAD）框架的实证评估，在可解释人工智能（XAI）领域做出了贡献。研究发现，集成堆叠、VAE和SHAP的结合不仅可以提高模型性能，还提供了一个易于解释的框架。 |
| [^97] | [Towards using Cough for Respiratory Disease Diagnosis by leveraging Artificial Intelligence: A Survey.](http://arxiv.org/abs/2309.14383) | 使用人工智能利用咳嗽诊断呼吸道疾病已经成为一个有希望的趋势，通过研究咳嗽特征进行深度学习算法的发展可以可靠准确地检测特定呼吸系统疾病的发作。 |
| [^98] | [A Text Classification-Based Approach for Evaluating and Enhancing the Machine Interpretability of Building Codes.](http://arxiv.org/abs/2309.14374) | 该研究提出了一种基于文本分类的方法，自动评估和增强建筑法规的机器可解释性。研究考虑了条款和文档层面的机器可解释性，通过引入几个类别进行分类并开发了一个数据集进行模型训练，并开发了一个高效的文本分类模型。 |
| [^99] | [Human Transcription Quality Improvement.](http://arxiv.org/abs/2309.14372) | 本文提出了一种可靠的方法来收集高质量的语音转录数据，通过在标注阶段进行置信度估计的重新处理和在标注后阶段进行自动词错误修正，成功降低了转录词误率（WER），并发现了转录错误对ASR模型性能的强相关性。 |
| [^100] | [A Unitary Weights Based One-Iteration Quantum Perceptron Algorithm for Non-Ideal Training Sets.](http://arxiv.org/abs/2309.14366) | 提出了一种基于单位权重的高效量子感知器算法，能够解决非理想训练集问题并实现一次迭代学习。算法通过计算训练集中总权重矩阵的奇异值分解来使权重矩阵成为单位阵，并能够精确实现任意量子门。与其他量子感知器算法相比，该算法具有更好的适用性、准确性和可用性。同时，该算法的适用性得到了进一步验证，演示了由多个基本量子门构成的量子复合门。 |
| [^101] | [Domain-Guided Conditional Diffusion Model for Unsupervised Domain Adaptation.](http://arxiv.org/abs/2309.14360) | 该论文提出了一种领域引导条件扩散模型（DACDM），通过生成高质量和多样性的目标域样本，帮助现有的无监督域适应方法进行更轻松的迁移，从而提高了性能。 |
| [^102] | [COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs.](http://arxiv.org/abs/2309.14356) | COCO-Counterfactuals是一个自动构建图像-文本对的反事实例的框架，通过使用文本到图像扩散模型来自动生成多模态反事实例。通过人工评估，我们验证了COCO-Counterfactuals的质量，并展示了其对于改善域外泛化能力的实用性。 |
| [^103] | [PopBERT. Detecting populism and its host ideologies in the German Bundestag.](http://arxiv.org/abs/2309.14355) | 本文提出了一种可靠、有效且可扩展的方法来衡量民粹主义立场，通过在德国联邦议院的演讲中进行语言标记，训练了一个基于Transformer的模型（PopBERT）来检测和量化民粹主义的核心维度，并确定民粹主义陈述与左翼或右翼主导意识形态的关联。 |
| [^104] | [Limited Communications Distributed Optimization via Deep Unfolded Distributed ADMM.](http://arxiv.org/abs/2309.14353) | 该论文提出了展开的D-ADMM算法来解决分布式优化中的通信限制问题，通过深度展开的方法，减少了消息交换的数量并保持了D-ADMM的操作。 |
| [^105] | [Training neural mapping schemes for satellite altimetry with simulation data.](http://arxiv.org/abs/2309.14350) | 本研究利用仿真数据和卫星测高仪培训了用于海洋表面高度的神经映射方案，并评估了不同海洋模拟数据对性能的影响。 |
| [^106] | [Corporate Credit Rating: A Survey.](http://arxiv.org/abs/2309.14349) | 本论文对企业信用评级进行了系统的调查，总结了CCR的发展背景，比较了不同模型的优缺点，并展望了CCR的未来。 |
| [^107] | [Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM.](http://arxiv.org/abs/2309.14348) | 本文提出了一种稳健对齐的LLM（RA-LLM），用于防御可能发生的对齐破坏攻击。RA-LLM可以直接在现有的对齐LLM上构建，并通过稳健的对齐检查函数来确保其有效性。 |
| [^108] | [A post-selection algorithm for improving dynamic ensemble selection methods.](http://arxiv.org/abs/2309.14307) | 提出了一种后选择动态集成选择（PS-DES）方法，通过使用不同的度量指标评估多种DES技术选择的集成，来提高准确性。 |
| [^109] | [MoDem-V2: Visuo-Motor World Models for Real-World Robot Manipulation.](http://arxiv.org/abs/2309.14236) | MoDem-V2是一个能够在非仪器化的真实世界中直接学习接触丰富操作的系统。 |
| [^110] | [Pseudo Label Selection is a Decision Problem.](http://arxiv.org/abs/2309.13926) | 伪标签选择是半监督学习中的一种方法，通过嵌入决策理论，提出了BPLS框架来解决伪标签选择中的确认偏差问题。 |
| [^111] | [On Calibration of Modern Quantized Efficient Neural Networks.](http://arxiv.org/abs/2309.13866) | 这项研究探索了在不同精度下三种架构和两个数据集的神经网络校准性能，发现校准质量与量化质量相关，并观察到在低精度下性能和校准质量均变差。GhostNet-VGG表现出最高稳健性，温度缩放可以改善量化网络的校准误差。 |
| [^112] | [Explainable Machine Learning for ICU Readmission Prediction.](http://arxiv.org/abs/2309.13781) | 本研究提出了一个标准化且可解释的机器学习流程，用于在多中心数据库中预测加护病房患者的再入院情况。 |
| [^113] | [Benchmarking Encoder-Decoder Architectures for Biplanar X-ray to 3D Shape Reconstruction.](http://arxiv.org/abs/2309.13587) | 对于双平面X射线到3D形状重建，我们提出了一个基准测试框架，该框架包括评估与真实临床情况相关的任务，并提供了8个模型的参考实现和6个公共数据集的收集和预处理。通过这个开源平台，我们可以比较不同模型的性能，评估其对真实世界临床情景的适用性。 |
| [^114] | [Probabilistic Weight Fixing: Large-scale training of neural network weight uncertainties for quantization.](http://arxiv.org/abs/2309.13575) | 本文提出了一种基于贝叶斯神经网络和变分松弛的概率框架，用于通过将权重值限制在一组有限值上来减少推理过程中的能量消耗。通过利用权重值的概率分布，提高了噪声鲁棒性和可压缩性。迭代聚类过程展示了超越现有方法的优势。 |
| [^115] | [Turbulence in Focus: Benchmarking Scaling Behavior of 3D Volumetric Super-Resolution with BLASTNet 2.0 Data.](http://arxiv.org/abs/2309.13457) | 本研究提出了BLASTNet 2.0，包含三维高保真压缩湍流流动模拟数据，通过对五种深度学习方法的基准测试和神经缩放分析，揭示了模型规模、成本和架构对预测性能的影响。 |
| [^116] | [InvestLM: A Large Language Model for Investment using Financial Domain Instruction Tuning.](http://arxiv.org/abs/2309.13064) | InvestLM是一个通过对金融领域指导数据集进行调优的大型语言模型，具有强大的理解金融文本的能力，并在投资相关问题上提供有帮助的回答。金融专家评价其与最先进的商业模型可媲美，并在金融NLP基准问题上展现了强大的泛化能力。 |
| [^117] | [Recurrent Temporal Revision Graph Networks.](http://arxiv.org/abs/2309.12694) | 该论文提出了一种循环时间修订图网络的新框架，通过使用循环神经网络和逐节点的隐藏状态，将所有历史邻居的信息整合起来，以获得每个节点的完整邻居信息。该框架在理论表现能力和实际应用中都取得了优越的性能。 |
| [^118] | [Multimodal Deep Learning for Scientific Imaging Interpretation.](http://arxiv.org/abs/2309.12460) | 本研究提出了一种多模态深度学习框架，通过模拟人类与扫描电子显微镜图像的交互，利用文本和视觉数据进行精细数据合成和评估。该模型（GlassLLaVA）能够准确解释、识别关键特征和检测以前未见的SEM图像中的缺陷，同时引入了适用于多种科学成像应用的灵活评估指标。 |
| [^119] | [Fairness Hub Technical Briefs: AUC Gap.](http://arxiv.org/abs/2309.12371) | 本论文介绍了一种称为AUC Gap的指标，它可以测量AI/ML模型在不同子群体中的性能差异，从而实现非二元的公平评估，为实现共同目标提供了基准和策略分享的基础。 |
| [^120] | [A Machine Learning-oriented Survey on Tiny Machine Learning.](http://arxiv.org/abs/2309.11932) | 本综述旨在提供关于微型机器学习（TinyML）的学习算法的最新综述，重点关注其在人工智能领域的应用与潜在贡献。 |
| [^121] | [TMac: Temporal Multi-Modal Graph Learning for Acoustic Event Classification.](http://arxiv.org/abs/2309.11845) | TMac是一个时态多模态图学习方法，用于声音事件分类。它通过图学习的方式对多模态数据中的时态信息进行建模，提高了声音事件分类的性能。 |
| [^122] | [A Comprehensive Review of Community Detection in Graphs.](http://arxiv.org/abs/2309.11798) | 本综述对图中的社区检测进行了全面回顾。社区结构是真实世界图的重要特征，社区检测方法的研究具有社会学、生物学和计算机科学方面的应用。尽管科学家们做出了努力，但尚未找到一个令人满意的解决方案。本综述介绍了社区结构的概念，各种社区检测方法，以及在各种网络中的实际应用。 |
| [^123] | [Efficient Core-selecting Incentive Mechanism for Data Sharing in Federated Learning.](http://arxiv.org/abs/2309.11722) | 本文介绍了一个联邦学习的数据共享博弈模型，并利用博弈论的方法设计了一个选核激励机制，以激励真实输入数据并促进稳定合作。 |
| [^124] | [Investigating the Catastrophic Forgetting in Multimodal Large Language Models.](http://arxiv.org/abs/2309.10313) | 本论文针对多模态大规模语言模型中的灾难性遗忘问题进行研究，引入了EMT方法来评估灾难性遗忘，并发现在标准图像分类任务上，几乎所有评估的模型都无法保持与视觉编码器相同的性能水平。研究结果表明，早期微调阶段对性能至关重要。 |
| [^125] | [Domain Generalization with Fourier Transform and Soft Thresholding.](http://arxiv.org/abs/2309.09866) | 本研究提出了一种基于傅里叶变换和软阈值的领域泛化方法，用于提高神经网络在不同源上的视网膜底图图像分割任务中的性能。 |
| [^126] | [Elucidating the solution space of extended reverse-time SDE for diffusion models.](http://arxiv.org/abs/2309.06169) | 这项工作介绍了扩展反向时间随机微分方程（ER SDE）用于解决扩散模型中的采样问题，并提供了精确解和高阶近似解，并解释了在快速采样方面ODE求解器优于SDE求解器的数学洞察力。 |
| [^127] | [Stream-based Active Learning by Exploiting Temporal Properties in Perception with Temporal Predicted Loss.](http://arxiv.org/abs/2309.05517) | 该论文提出了一种利用时序特性的感知流中的基于流的主动学习方法，并引入了一种新的时序预测损失（TPL）方法。通过实验证明，该方法显著改善了选择的多样性，并且与基于不确定性的方法相比，是一种更常见的感知应用中的AL方法。 |
| [^128] | [Training of Spiking Neural Network joint Curriculum Learning Strategy.](http://arxiv.org/abs/2309.04737) | 该论文提出了一种将课程学习引入脉冲神经网络的训练模型，使其更类似于人类学习过程，并提高了其生物解释性。 |
| [^129] | [RTLLM: An Open-Source Benchmark for Design RTL Generation with Large Language Model.](http://arxiv.org/abs/2308.05345) | RTLLM是一个用自然语言指令生成设计RTL的开源基准测试，旨在解决现有工作中目标设计简单且规模小的问题，并提供对基于LLM的解决方案进行设计质量的定量评估。 |
| [^130] | [Is One Epoch All You Need For Multi-Fidelity Hyperparameter Optimization?.](http://arxiv.org/abs/2307.15422) | 传统的基准线在多层次超参数优化中取得了与其他方法类似的结果，并大幅减少计算成本。研究人员应该使用该基准线并扩大MF-HPO基准测试的多样性。 |
| [^131] | [Learning Nonlinear Projections for Reduced-Order Modeling of Dynamical Systems using Constrained Autoencoders.](http://arxiv.org/abs/2307.15288) | 该论文提出了一种使用受约束的自编码器学习非线性投影的方法，用于动态系统的降阶建模。这种方法可以有效地对非线性动力学系统进行逼近，并解决了在流形附近对瞬态动力学建模时所面临的问题。 |
| [^132] | [TinyMetaFed: Efficient Federated Meta-Learning for TinyML.](http://arxiv.org/abs/2307.06822) | TinyMetaFed是一个适用于TinyML的高效联邦元学习框架，通过协同训练神经网络初始化，在小型设备上能够快速微调，同时实现通信节省和隐私保护。 |
| [^133] | [Multi-Task Learning to Enhance Generazability of Neural Network Equalizers in Coherent Optical Systems.](http://arxiv.org/abs/2307.05374) | 首次提出了多任务学习用于提高相干系统中基于神经网络的均衡器的灵活性，并且通过一个"单一"的基于神经网络的均衡器，在不重新训练的情况下，即使在发射功率、符号速率或传输距离变化的情况下，也能将Q因子提高最高达4 dB。 |
| [^134] | [ACDNet: Attention-guided Collaborative Decision Network for Effective Medication Recommendation.](http://arxiv.org/abs/2307.03332) | 本文提出了一种基于注意力引导的协同决策网络（ACDNet）用于药物推荐，通过利用注意力机制和Transformer对患者的健康状况和药物记录进行建模，同时采用协同决策框架，从患者药物记录和具体药物之间的相似性出发，有效地个性化推荐药物。 |
| [^135] | [Synthesizing Artistic Cinemagraphs from Text.](http://arxiv.org/abs/2307.03190) | 本论文介绍了一种通过文本描述来创建艺术性影动图的自动化方法。通过合成图像双胞胎，即一对艺术图像和与之对齐的真实图像，可以同时满足艺术风格和外观的要求并简化动作分析。同时，利用现有数据集可以准确地分割真实图像并预测合理的运动。 |
| [^136] | [LLQL: Logistic Likelihood Q-Learning for Reinforcement Learning.](http://arxiv.org/abs/2307.02345) | 本研究通过研究在线和离线增强学习中 Bellman 近似误差的分布发现，Bellman 误差符合逻辑分布。基于这一发现，本研究提出了一种使用 Logistic 最大似然函数作为替代方法的方案，并通过实验证明了其有效性。 |
| [^137] | [Safety-Critical Scenario Generation Via Reinforcement Learning Based Editing.](http://arxiv.org/abs/2306.14131) | 提出了一种基于强化学习的场景生成方法，通过顺序编辑生成安全关键场景，克服了维度挑战，并生成了质量更高的场景。 |
| [^138] | [CompanyKG: A Large-Scale Heterogeneous Graph for Company Similarity Quantification.](http://arxiv.org/abs/2306.10649) | 本研究提出了CompanyKG，一种用于公司相似性量化的大规模异构图数据集。通过丰富的公司特征和关系表示，以及多个评估任务的基准测试，为公司相似性量化方法的综合评估提供了支持。 |
| [^139] | [Can ChatGPT Enable ITS? The Case of Mixed Traffic Control via Reinforcement Learning.](http://arxiv.org/abs/2306.08094) | 本文研究探讨使用 ChatGPT 解决混合交通流控制问题，通过大规模用户研究发现 ChatGPT 在某些环境下能够提高成功策略数量 |
| [^140] | [Resilient Constrained Learning.](http://arxiv.org/abs/2306.02426) | 本论文提出了一个名为“抗干扰约束学习”的方法来解决在部署机器学习解决方案时需要满足除了准确性以外的多个要求，并以平衡从放宽中获得的性能增益与用户定义的放宽成本之间的关系的方式放松学习约束。 |
| [^141] | [Pre-trained transformer for adversarial purification.](http://arxiv.org/abs/2306.01762) | 本文提出了一个快速防御对抗性攻击的方案RaPiD（Rapid Plug-in Defender），通过预训练的Transformer微调来提纯对抗样本，使其逼近清洁数据分布，实验结果表明，在有限数据情况下，该方法优于最先进的方法。 |
| [^142] | [GRD: A Generative Approach for Interpretable Reward Redistribution in Reinforcement Learning.](http://arxiv.org/abs/2305.18427) | 本文提出了一种可解释奖励再分配的方法，通过因果透视建模状态和行动贡献，产生可解释的返回分解。生成返回分解（GRD）框架用于延迟奖励场景中的策略优化。 |
| [^143] | [Weakly-Supervised Visual-Textual Grounding with Semantic Prior Refinement.](http://arxiv.org/abs/2305.10913) | 本文提出了一种利用语义先验细化的弱监督视觉-文本对齐方法，仅使用图像-句子对进行学习，其目标是实现实体表示中的区域-短语对应关系，通过联合两个主要模块的输出进行预测。 |
| [^144] | [Distilling Knowledge for Short-to-Long Term Trajectory Prediction.](http://arxiv.org/abs/2305.08553) | 本文提出了一种新的方法Di-Long，用于解决长期轨迹预测中越来越不确定和不可预测的问题。该方法利用蒸馏短期轨迹模型预测器来指导训练过程中的长期轨迹预测学生网络。学生网络观察短序列并预测长轨迹，教师网络观察更长序列并预测剩余短目标轨迹。 |
| [^145] | [How to Index Item IDs for Recommendation Foundation Models.](http://arxiv.org/abs/2305.06569) | 本研究对推荐基础模型的项目索引问题进行了系统检查，提出了一种新的上下文感知索引方法，该方法在项目推荐准确性和文本生成质量方面具有优势。 |
| [^146] | [FMG-Net and W-Net: Multigrid Inspired Deep Learning Architectures For Medical Imaging Segmentation.](http://arxiv.org/abs/2304.02725) | FMG-Net和W-Net是两种受多重网格启发的深度学习架构，能够解决医学图像分割中面临的细节特征和尺度变化的挑战，能够提高肿瘤分割的精度。 |
| [^147] | [Learning Stable and Robust Linear Parameter-Varying State-Space Models.](http://arxiv.org/abs/2304.01828) | 该论文提出了稳定和鲁棒的线性参数可变状态空间模型的两种直接参数化方法，训练出的模型具有收缩意义或通过用户定义的值被限制在 Lipschitz 常数内，对进一步的凸分析或控制器设计非常有用。 |
| [^148] | [Online Learning with Adversaries: A Differential Inclusion Analysis.](http://arxiv.org/abs/2304.01525) | 本文提出了第一个能够以几乎确定的方式收敛到 $\mu$ 的异步在线算法，应用了微分包容分析，并提供了两个关键亮点。 |
| [^149] | [Borda Regret Minimization for Generalized Linear Dueling Bandits.](http://arxiv.org/abs/2303.08816) | 本文解决了通用广义线性对抗性排名问题中的博尔达后悔最小化问题，提出了高度表达力的模型，并使用一种新的“先探索再执行”算法避免了困难的后悔下限。 |
| [^150] | [Fast exploration and learning of latent graphs with aliased observations.](http://arxiv.org/abs/2303.07397) | 本文介绍了一种在具有别名观测的潜在图上，能够显著提高最大化探索效率的政策算法 eFeX，相比于随机策略，该算法能够更快地恢复各种拓扑结构下的图表。 |
| [^151] | [Quantifying Causes of Arctic Amplification via Deep Learning based Time-series Causal Inference.](http://arxiv.org/abs/2303.07122) | 该研究提出了一种基于循环神经网络的时间序列因果推断模型TCINet，用于推断大气过程对海冰融化的因果效应。通过实验证明，该模型能够显著提高量化北极海冰融化的主要原因的能力。 |
| [^152] | [Investigating Stateful Defenses Against Black-Box Adversarial Examples.](http://arxiv.org/abs/2303.06280) | 本文探究了有状态防御黑盒对抗样本的方法，提出了一种新的有状态防御模型，可以在CIFAR10数据集上达到82.2％的准确性，在ImageNet数据集上达到76.5％的准确性。 |
| [^153] | [Understanding the Diffusion Objective as a Weighted Integral of ELBOs.](http://arxiv.org/abs/2303.00848) | 本文深入理解了扩散目标，并揭示了加权损失和ELBO目标之间的直接关系。 |
| [^154] | [Transformed Low-Rank Parameterization Can Help Robust Generalization for Tensor Neural Networks.](http://arxiv.org/abs/2303.00196) | 这项研究首次通过推导泛化误差上界回答了转换的低秩参数化如何影响张量神经网络的学习行为，结果显示通过精确的转换低秩参数化压缩的t-NNs可以实现更尖锐的对抗泛化上界。 |
| [^155] | [Permutation Equivariant Neural Functionals.](http://arxiv.org/abs/2302.14040) | 本文介绍了置换等变神经功能网络的设计，通过对权重进行置换对称性编码，实现对其他网络权重或梯度进行处理，为学习优化、处理隐式神经表示等应用提供了架构原则。 |
| [^156] | [Directed Diffusion: Direct Control of Object Placement through Attention Guidance.](http://arxiv.org/abs/2302.13153) | 本论文介绍了一种有导向性的扩散方法，通过关注引导在图像中直接控制对象的放置。通过观察提示词的交叉注意力映射，引入优化目标，在特定位置产生“激活”，从而改进了场景组合能力。 |
| [^157] | [Mixed Traffic Control and Coordination from Pixels.](http://arxiv.org/abs/2302.09167) | 本研究考虑利用图像观察作为替代方法来进行混合交通控制。 |
| [^158] | [UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models.](http://arxiv.org/abs/2302.04867) | 本文提出了一种称为UniPC的统一预测-修正框架，用于快速采样扩散模型(DPMs)，该框架通过引入统一修正器(UniC)和统一预测器(UniP)，可以显著提高采样质量，尤其是在较少步骤的情况下。 |
| [^159] | [GPU-based Private Information Retrieval for On-Device Machine Learning Inference.](http://arxiv.org/abs/2301.10904) | 本论文提出了一种基于GPU的私密信息检索的方法，可以在设备上进行机器学习推断，而无需将私密用户数据透露给远程服务器。通过加速私密信息检索并与下游机器学习应用程序共同设计，我们的方法极大地提高了系统吞吐量。 |
| [^160] | [Data Distillation: A Survey.](http://arxiv.org/abs/2301.04272) | 这篇综述介绍了数据精炼的概念和方法，以及针对不同数据类型的应用。数据精炼方法可以用于模型训练、推理和架构搜索等场景，以解决使用大型数据集训练模型所带来的问题。 |
| [^161] | [VeriX: Towards Verified Explainability of Deep Neural Networks.](http://arxiv.org/abs/2212.01051) | VeriX是一个可以产生最优健壮解释和生成反事实的系统，通过使用约束求解技术和基于特征级敏感性排名的启发式方法，我们在图像识别和自主飞机滑行场景中验证了该方法的有效性。 |
| [^162] | [A Unified Algorithm Framework for Unsupervised Discovery of Skills based on Determinantal Point Process.](http://arxiv.org/abs/2212.00211) | 本文提出了一个使用确定性点过程（DPP）的统一算法框架，实现了对无监督技能发现的多样性和覆盖性的显式优化。 |
| [^163] | [Geodesic Sinkhorn for Fast and Accurate Optimal Transport on Manifolds.](http://arxiv.org/abs/2211.00805) | Geodesic Sinkhorn是一种基于浸润曲面上热核扩散的方法，用于在流形上快速准确地计算分布之间的最优输运距离。该方法通过使用基于稀疏图拉普拉斯矩阵的切比雪夫多项式来逼近热核，从而实现了$O(n\log n)$的计算复杂度。在实验中，我们将该方法应用于高维单细胞数据的多个分布的几何中心计算。 |
| [^164] | [Solving Continuous Control via Q-learning.](http://arxiv.org/abs/2210.12566) | 本研究通过对Q-learning进行简单修改，通过将bang-bang动作离散化与值分解相结合，将单智能体控制视为合作多智能体强化学习来解决连续控制问题，并取得了与最先进的连续actor-critic方法相匹配的性能。 |
| [^165] | [Finite-time analysis of single-timescale actor-critic.](http://arxiv.org/abs/2210.09921) | 这项研究提出了一种在线单时间尺度演员-评论家方法，通过线性函数逼近和马尔可夫样本更新，在连续状态空间中找到了一个$\epsilon$-近似的稳定点，并且在样本复杂度为$\widetilde{\mathcal{O}}(\epsilon^{-2})$的情况下证明了其收敛性。 |
| [^166] | [Env-Aware Anomaly Detection: Ignore Style Changes, Stay True to Content!.](http://arxiv.org/abs/2210.03103) | 本研究提出了一种环境感知的异常检测方法，通过忽略风格变化，专注于内容，在无监督场景下达到了较好的性能，并通过考虑环境标签提高了对比方法的准确性。 |
| [^167] | [Polar Encoding: A Simple Baseline Approach for Classification with Missing Values.](http://arxiv.org/abs/2210.01905) | 极化编码是一种用于处理具有缺失值的分类问题的简单基线方法，它能保留缺失信息、无需插补，让决策树自由选择如何处理缺失值。 |
| [^168] | [Testing predictions of representation cost theory with CNNs.](http://arxiv.org/abs/2210.01257) | 通过理论和实验证明，训练的卷积神经网络（CNNs）对低频信号具有敏感性，这是因为自然图像的频率分布使大部分能量集中在低到中频。 |
| [^169] | [Delving into the Devils of Bird's-eye-view Perception: A Review, Evaluation and Recipe.](http://arxiv.org/abs/2209.05324) | 本综述文章探讨了鸟瞰视角感知领域的挑战和方法，主要关注了从透视视图到鸟瞰视图的信息转换、地面真值注释获取、特征融合以及整体流程的构建。 |
| [^170] | [E2EG: End-to-End Node Classification Using Graph Topology and Text-based Node Attributes.](http://arxiv.org/abs/2208.04609) | E2EG是一个端到端节点分类模型，通过利用图拓扑和基于文本的节点属性，消除了嵌入和分类两个阶段，引入了主要和辅助分类目标的串行利用，减少了参数数量并提高了使用的便捷性。 |
| [^171] | [Mitigating Off-Policy Bias in Actor-Critic Methods with One-Step Q-learning: A Novel Correction Approach.](http://arxiv.org/abs/2208.00755) | 本文提出一种新的策略相似度量来缓解离策略学习中的偏差问题，提供了一种自适应的、可扩展的解决方案。 |
| [^172] | [Asynchronous Decentralized Bayesian Optimization for Large Scale Hyperparameter Optimization.](http://arxiv.org/abs/2207.00479) | 这项研究提出了一种异步分散的贝叶斯优化方法，可以实现大规模超参数优化，并在Polairs超级计算机上展示了模型准确性的改进。 |
| [^173] | [Exploiting Semantic Role Contextualized Video Features for Multi-Instance Text-Video Retrieval EPIC-KITCHENS-100 Multi-Instance Retrieval Challenge 2022.](http://arxiv.org/abs/2206.14381) | 本论文提出了一种在EPIC-KITCHENS-100多实例检索挑战2022中利用语义角色上下文化的视频特征进行文本-视频检索的方法，通过三元损失函数在多个嵌入空间中融合视频和文本特征，超过了强基线，在nDCG和mAP方面获得了较好的排名。 |
| [^174] | [Optimization with access to auxiliary information.](http://arxiv.org/abs/2206.00395) | 本文研究了在计算目标函数梯度很昂贵或有限的情况下，给定一些辅助函数的情况下，如何最小化目标函数。作者提出了两种通用的新算法，并证明了这个框架可以受益于目标和辅助信息之间的Hessian相似性假设。 |
| [^175] | [NN2Poly: A polynomial representation for deep feed-forward artificial neural networks.](http://arxiv.org/abs/2112.11397) | 本文提出了一种名为NN2Poly的方法，用于生成已经训练好的全连接前馈人工神经网络的精确多项式表示。该方法适用于任意深度的多层感知器（MLP）且计算成本相对较低，能够在回归和分类任务中提供非常准确的逼近结果。 |
| [^176] | [Interpretable and Interactive Deep Multiple Instance Learning for Dental Caries Classification in Bitewing X-rays.](http://arxiv.org/abs/2112.09694) | 本研究提出了一种可解释且交互式的深度多实例学习方法，用于在牙齿放射图中的龋齿分类。该方法首先输出局部补丁分类概率的热图，并可根据分割标签进行训练，与现有方法相比表现出了竞争性的性能，并且用户可以解释预测并与模型交互。 |
| [^177] | [Path Regularization: A Convexity and Sparsity Inducing Regularization for Parallel ReLU Networks.](http://arxiv.org/abs/2110.09548) | 路径正则化为并行ReLU网络提供了一种简化的凸优化问题，通过群稀疏性引导实现了凸模型，并提出了一个近似算法，在所有数据维度上具备完全多项式时间复杂度。 |
| [^178] | [Continuous Treatment Recommendation with Deep Survival Dose Response Function.](http://arxiv.org/abs/2108.10453) | 本论文提出了一个通用公式，称为深度生存剂量反应函数（DeepSDRF），用于解决临床生存数据中的连续治疗推荐问题。通过校正选择偏差，DeepSDRF估计的治疗效果可以用于开发推荐算法。在模拟研究和实际医学数据库上的测试中，DeepSDRF表现出良好的性能。 |
| [^179] | [Causal Graph Discovery from Self and Mutually Exciting Time Series.](http://arxiv.org/abs/2106.02600) | 该论文提出了一个新的因果图发现方法，结合了广义线性结构因果模型和自适应正则化方法，通过解决凸优化问题来恢复因果DAGs，并建立了置信区间以量化不确定性，经过实验证明其在恢复高度可解释的因果DAGs上表现出竞争性能。 |
| [^180] | [Human-like Energy Management Based on Deep Reinforcement Learning and Historical Driving Experiences.](http://arxiv.org/abs/2007.10126) | 本文提出了一种基于深度强化学习和历史驾驶经验的人类化能量管理框架，通过采用深度确定性策略梯度算法和驾驶数据训练模型，提高了混合动力电动车能量管理的性能。 |

# 详细

[^1]: SGD在具有接近最优样本复杂度的双层神经网络中寻找并调整特征：以XOR问题为案例研究

    SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem. (arXiv:2309.15111v1 [cs.LG])

    [http://arxiv.org/abs/2309.15111](http://arxiv.org/abs/2309.15111)

    本研究通过在两层神经网络上使用小批量SGD算法，在具有二次真实函数分隔数据的情况下，通过训练数量级为$d \:\text{polylog}(d)$的样本，将网络训练到了人口误差为$o(1)$的程度。这是首次在标准神经网络上以及标准训练下，展示了在各向同性数据上高效学习XOR函数的样本复杂度为$\tilde{O}(d)$。

    

    本文研究了小批量随机梯度下降（SGD）在具有二次真实函数分隔数据的双层神经网络上的优化过程。我们证明，对于从$d$维布尔超立方体中由二次“XOR”函数$y = -x_ix_j$标记的数据，可以通过标准小批量SGD在逻辑损失上同时训练两层ReLU激活的双层神经网络，用$d \:\text{polylog}(d)$个样本将其训练到人口误差为$o(1)$的程度。据我们所知，这是首次给出了在标准神经网络上以及标准训练下，对于在各向同性数据上高效学习XOR函数的样本复杂度为$\tilde{O}(d)$。我们的主要技术是展示网络演化有两个阶段：一个”信号发现“阶段，在此网络规模较小且许多神经元独立演化以寻找特征，以及一个”信号密集“阶段，其中许多神经元相互作用以优化预测。

    In this work, we consider the optimization process of minibatch stochastic gradient descent (SGD) on a 2-layer neural network with data separated by a quadratic ground truth function. We prove that with data drawn from the $d$-dimensional Boolean hypercube labeled by the quadratic ``XOR'' function $y = -x_ix_j$, it is possible to train to a population error $o(1)$ with $d \:\text{polylog}(d)$ samples. Our result considers simultaneously training both layers of the two-layer-neural network with ReLU activations via standard minibatch SGD on the logistic loss. To our knowledge, this work is the first to give a sample complexity of $\tilde{O}(d)$ for efficiently learning the XOR function on isotropic data on a standard neural network with standard training. Our main technique is showing that the network evolves in two phases: a $\textit{signal-finding}$ phase where the network is small and many of the neurons evolve independently to find features, and a $\textit{signal-heavy}$ phase, wher
    
[^2]: 满足关注：对语言模型事实错误的约束满足视角的研究

    Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models. (arXiv:2309.15098v1 [cs.CL])

    [http://arxiv.org/abs/2309.15098](http://arxiv.org/abs/2309.15098)

    本研究使用约束满足问题框架研究了语言模型的内部行为，发现模型对约束标记的关注程度与事实准确性强正相关。提出了一种方法可以预测约束满足和事实错误，并允许早期错误识别，进一步提高了大型语言模型的可靠性。

    

    本研究调查了基于Transformer的大型语言模型（LLM）在生成事实上错误的文本时的内部行为。我们将事实查询建模为约束满足问题，并利用这一框架研究模型如何与事实约束进行内部交互。具体而言，我们发现模型对约束标记的关注程度与其响应的事实准确性存在强正相关关系。在我们的11个数据集中，总计超过40,000个提示的精心策划套装中，我们研究了使用Llama-2系列在所有规模（7B，13B，70B）上预测事实错误的任务。我们提出了SAT Probe，一种探查自注意模式的方法，可以预测约束满足和事实错误，并允许早期错误识别。这一方法和发现表明，利用对LLM中事实性的机械理解可以增强可靠性。

    We investigate the internal behavior of Transformer-based Large Language Models (LLMs) when they generate factually incorrect text. We propose modeling factual queries as Constraint Satisfaction Problems and use this framework to investigate how the model interacts internally with factual constraints. Specifically, we discover a strong positive relation between the model's attention to constraint tokens and the factual accuracy of its responses. In our curated suite of 11 datasets with over 40,000 prompts, we study the task of predicting factual errors with the Llama-2 family across all scales (7B, 13B, 70B). We propose SAT Probe, a method probing self-attention patterns, that can predict constraint satisfaction and factual errors, and allows early error identification. The approach and findings demonstrate how using the mechanistic understanding of factuality in LLMs can enhance reliability.
    
[^3]: 修复NTK：从神经网络线性化到精确的凸程序

    Fixing the NTK: From Neural Network Linearizations to Exact Convex Programs. (arXiv:2309.15096v1 [cs.LG])

    [http://arxiv.org/abs/2309.15096](http://arxiv.org/abs/2309.15096)

    本文从两个方向对深度神经网络进行理论分析，提供了通过神经切线核（NTK）和通过凸重塑ReLU网络的全局优化训练目标的方法。此外，我们还提出了一种与NTK相连的多核学习模型，称为门控ReLU网络，通过加权数据屏蔽特征映射来实现全局优化。

    

    最近，深度神经网络的理论分析主要集中在两个方向上：1）通过在隐藏层宽度无限大和学习率无穷小的情况下进行的SGD训练的理论洞察力（也称为梯度流）通过神经切线核（NTK）；2）通过锥约束凸重塑ReLU网络的全局优化训练目标。后一种研究方向还提供了ReLU网络的另一种公式，称为门控ReLU网络，可通过高效的无约束凸程序进行全局优化。在这项工作中，我们将门控ReLU网络的凸问题解释为具有加权数据屏蔽特征映射的多核学习（MKL）模型，并与NTK建立了连接。具体而言，我们证明了对于那些与学习目标无关的特定选择的掩码权重，该核等效于门控ReLU网络在训练样本上的NTK。

    Recently, theoretical analyses of deep neural networks have broadly focused on two directions: 1) Providing insight into neural network training by SGD in the limit of infinite hidden-layer width and infinitesimally small learning rate (also known as gradient flow) via the Neural Tangent Kernel (NTK), and 2) Globally optimizing the regularized training objective via cone-constrained convex reformulations of ReLU networks. The latter research direction also yielded an alternative formulation of the ReLU network, called a gated ReLU network, that is globally optimizable via efficient unconstrained convex programs. In this work, we interpret the convex program for this gated ReLU network as a Multiple Kernel Learning (MKL) model with a weighted data masking feature map and establish a connection to the NTK. Specifically, we show that for a particular choice of mask weights that do not depend on the learning targets, this kernel is equivalent to the NTK of the gated ReLU network on the tra
    
[^4]: 通过替代技术识别医疗设备组装过程的仿真模型

    Identifying Simulation Model Through Alternative Techniques for a Medical Device Assembly Process. (arXiv:2309.15094v1 [cs.LG])

    [http://arxiv.org/abs/2309.15094](http://arxiv.org/abs/2309.15094)

    本论文通过使用Spline函数和机器学习模型来识别仿真模型，以提高对医疗设备组装过程中的快拍流程的理解和决策支持能力。

    

    本科学论文探讨了两种不同的方法来识别和逼近仿真模型，特别是在医疗设备组装过程中关键的快拍流程中的应用。仿真模型在提供工程师对工业过程的洞察力方面起着关键作用，使其在实际组装之前进行实验和故障排除成为可能。然而，它们的复杂性常常导致耗时的计算。为了减轻这种复杂性，我们提出了两种不同的识别仿真模型的方法：一种利用样条函数，另一种利用机器学习模型。我们的目标是创建具有适应性的模型，可以准确地代表快拍过程并适应多种情景。这样的模型有望提高流程理解和决策支持能力，特别是在数据可用性有限的情况下。

    This scientific paper explores two distinct approaches for identifying and approximating the simulation model, particularly in the context of the snap process crucial to medical device assembly. Simulation models play a pivotal role in providing engineers with insights into industrial processes, enabling experimentation and troubleshooting before physical assembly. However, their complexity often results in time-consuming computations.  To mitigate this complexity, we present two distinct methods for identifying simulation models: one utilizing Spline functions and the other harnessing Machine Learning (ML) models. Our goal is to create adaptable models that accurately represent the snap process and can accommodate diverse scenarios. Such models hold promise for enhancing process understanding and aiding in decision-making, especially when data availability is limited.
    
[^5]: VideoDirectorGPT: 通过LLM引导的规划实现一致的多场景视频生成

    VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning. (arXiv:2309.15091v1 [cs.CV])

    [http://arxiv.org/abs/2309.15091](http://arxiv.org/abs/2309.15091)

    本文提出了VideoDirectorGPT，一种利用LLMs的知识实现一致多场景视频生成的框架，通过视频内容规划和基于内容的视频生成来生成时间上一致的长视频。

    

    尽管最近的文本到视频生成方法取得了显著的进展，但大多数工作集中在生成单个事件和单一背景的短视频片段（即单场景视频）。与此同时，最近的大型语言模型（LLMs）已经证明了它们在生成布局和控制下游视觉模块（如图像生成模型）的程序方面的能力。这引发了一个重要问题：我们能否利用这些LLMs中嵌入的知识用于生成时间上一致的长视频？在本文中，我们提出了VideoDirectorGPT，这是一个用于一致的多场景视频生成的新型框架，它利用LLMs的知识进行视频内容规划和基于内容的视频生成。具体而言，我们首先将单个文本提示输入我们的视频规划器LLM（GPT-4）中，将其扩展为“视频计划”，其中包括生成场景描述、实体及其布局、每个场景的背景以及保持一致性等内容。

    Although recent text-to-video (T2V) generation methods have seen significant advancements, most of these works focus on producing short video clips of a single event with a single background (i.e., single-scene videos). Meanwhile, recent large language models (LLMs) have demonstrated their capability in generating layouts and programs to control downstream visual modules such as image generation models. This raises an important question: can we leverage the knowledge embedded in these LLMs for temporally consistent long video generation? In this paper, we propose VideoDirectorGPT, a novel framework for consistent multi-scene video generation that uses the knowledge of LLMs for video content planning and grounded video generation. Specifically, given a single text prompt, we first ask our video planner LLM (GPT-4) to expand it into a 'video plan', which involves generating the scene descriptions, the entities with their respective layouts, the background for each scene, and consistency 
    
[^6]: 单一生物神经元作为精确的时空模式识别器

    Single Biological Neurons as Temporally Precise Spatio-Temporal Pattern Recognizers. (arXiv:2309.15090v1 [q-bio.NC])

    [http://arxiv.org/abs/2309.15090](http://arxiv.org/abs/2309.15090)

    这篇博士论文的核心观点是，单一生物神经元具有时空模式识别的精确性和复杂性，与目前大多数神经科学家的观点不同。这一点对神经元组成的脑回路和神经活动的信息编码具有广泛的影响。

    

    这篇博士论文的重点是认为大脑中的单个神经元应被视为临时精确且高度复杂的时空模式识别器。与当今大多数神经科学家认为的生物神经元是简单且主要是空间模式识别器的观点相反。在这篇论文中，我将尝试证明这是一个重要的区别，主要是因为单个神经元的上述计算特性对神经元组成的各种脑回路以及神经活动如何编码信息都具有广泛的影响。即这些单个神经元层面的“低级”细节对整个系统有着重大的影响。在引言中，我们将强调组成神经微电路的主要组成部分，并从系统的角度阐明这些组成部分的相互依赖关系。

    This PhD thesis is focused on the central idea that single neurons in the brain should be regarded as temporally precise and highly complex spatio-temporal pattern recognizers. This is opposed to the prevalent view of biological neurons as simple and mainly spatial pattern recognizers by most neuroscientists today. In this thesis, I will attempt to demonstrate that this is an important distinction, predominantly because the above-mentioned computational properties of single neurons have far-reaching implications with respect to the various brain circuits that neurons compose, and on how information is encoded by neuronal activity in the brain. Namely, that these particular "low-level" details at the single neuron level have substantial system-wide ramifications. In the introduction we will highlight the main components that comprise a neural microcircuit that can perform useful computations and illustrate the inter-dependence of these components from a system perspective. In chapter 1 
    
[^7]: 关于神经网络分类器超额风险收敛速率的研究

    On Excess Risk Convergence Rates of Neural Network Classifiers. (arXiv:2309.15075v1 [stat.ML])

    [http://arxiv.org/abs/2309.15075](http://arxiv.org/abs/2309.15075)

    本文研究了基于神经网络的插值分类器在二分类问题中的性能，通过超额风险来衡量。研究考虑了更一般的场景，使得神经网络可以轻松应用数值优化方法。虽然函数类很大，但无维度速率是可能的。

    

    最近神经网络在模式识别和分类问题上的成功表明，与其他更经典的分类器（如SVM或boosting分类器）相比，神经网络具有独特的特点。本文研究了基于神经网络的插值分类器在二分类问题中的性能，通过其超额风险来衡量。与文献中所规定的典型条件相比，我们考虑了一个更一般的场景，它在两个方面与实际应用类似：首先，要近似的函数类包括了Barron函数作为正子集；其次，构建的神经网络分类器是通过最小化一个替代损失函数而不是0-1损失函数来实现的，从而可以轻松应用基于梯度下降的数值优化方法。虽然我们考虑的函数类非常大，最优速率不能超过$n^{-\frac{1}{3}}$，但在这种情况下，无维度速率是可能的。

    The recent success of neural networks in pattern recognition and classification problems suggests that neural networks possess qualities distinct from other more classical classifiers such as SVMs or boosting classifiers. This paper studies the performance of plug-in classifiers based on neural networks in a binary classification setting as measured by their excess risks. Compared to the typical settings imposed in the literature, we consider a more general scenario that resembles actual practice in two respects: first, the function class to be approximated includes the Barron functions as a proper subset, and second, the neural network classifier constructed is the minimizer of a surrogate loss instead of the $0$-$1$ loss so that gradient descent-based numerical optimizations can be easily applied. While the class of functions we consider is quite large that optimal rates cannot be faster than $n^{-\frac{1}{3}}$, it is a regime in which dimension-free rates are possible and approximat
    
[^8]: QUILT：使用多样化的量子分类器集合在量子计算机上进行有效的多类分类

    QUILT: Effective Multi-Class Classification on Quantum Computers Using an Ensemble of Diverse Quantum Classifiers. (arXiv:2309.15056v1 [quant-ph])

    [http://arxiv.org/abs/2309.15056](http://arxiv.org/abs/2309.15056)

    QUILT是一个针对当前容易出错的量子计算机设计的框架，其通过使用多样化的量子分类器集合，在MNIST数据集中表现出高达85%的多类分类准确率。

    

    量子计算机在理论上可以比经典计算机有显著的加速，但由于量子比特数量有限且容易出错，量子计算机的近期发展受到限制。Quilt是一个框架，用于在当前容易出错的量子计算机上有效执行多类分类任务。Quilt使用真实的量子机器以及未来噪音水平的评估，随着量子机器变得更加噪音少。在五比特系统上，Quilt在MNIST数据集上展示了高达85%的多类分类准确率。

    Quantum computers can theoretically have significant acceleration over classical computers; but, the near-future era of quantum computing is limited due to small number of qubits that are also error prone. Quilt is a framework for performing multi-class classification task designed to work effectively on current error-prone quantum computers. Quilt is evaluated with real quantum machines as well as with projected noise levels as quantum machines become more noise-free. Quilt demonstrates up to 85% multi-class classification accuracy with the MNIST dataset on a five-qubit system.
    
[^9]: 基于似然比的任务预测的类增量学习

    Class Incremental Learning via Likelihood Ratio Based Task Prediction. (arXiv:2309.15048v1 [cs.LG])

    [http://arxiv.org/abs/2309.15048](http://arxiv.org/abs/2309.15048)

    该论文提出了一种基于似然比的任务预测的类增量学习方法，利用离群检测器进行任务标识预测，解决了无任务标识符的测试样本的任务预测问题。

    

    类增量学习是一种具有挑战性的不断学习的设置，通过顺序学习一系列任务。每个任务由一组唯一的类组成。类增量学习的关键特点是，在测试时不提供每个测试样本的任务标识符（或任务ID）。为每个测试样本预测任务ID是一个具有挑战性的问题。一种新兴的理论上合理且有效的方法是根据任务增量学习的方法，在共享网络中为所有任务训练每个任务的任务特定模型，以处理遗忘。该方法中每个任务的模型是一个非常规分类器而不是传统分类器的离群检测器。离群检测器可以对任务内（分布内（IND））的类进行预测和识别离群数据。在推断期间，离群检测能力是每个测试样本的任务ID预测的关键。然而，本文认为使用传统的离群检测器进行任务ID预测是次优的。

    Class incremental learning (CIL) is a challenging setting of continual learning, which learns a series of tasks sequentially. Each task consists of a set of unique classes. The key feature of CIL is that no task identifier (or task-id) is provided at test time for each test sample. Predicting the task-id for each test sample is a challenging problem. An emerging theoretically justified and effective approach is to train a task-specific model for each task in a shared network for all tasks based on a task-incremental learning (TIL) method to deal with forgetting. The model for each task in this approach is an out-of-distribution (OOD) detector rather than a conventional classifier. The OOD detector can perform both within-task (in-distribution (IND)) class prediction and OOD detection. The OOD detection capability is the key for task-id prediction during inference for each test sample. However, this paper argues that using a traditional OOD detector for task-id prediction is sub-optimal
    
[^10]: 结合存活分析和机器学习利用电子健康记录数据进行肿瘤风险预测

    Combining Survival Analysis and Machine Learning for Mass Cancer Risk Prediction using EHR data. (arXiv:2309.15039v1 [cs.LG])

    [http://arxiv.org/abs/2309.15039](http://arxiv.org/abs/2309.15039)

    该论文介绍了一种利用 EHR 数据进行大规模肿瘤风险预测的新方法，其创新之处在于只需利用历史的医疗服务代码和诊断信息来实现最小化的数据需求，通过将存活分析和机器学习相结合，可以在大规模应用中实现对患者癌症风险的个性化评估。

    

    纯粹的医学肿瘤筛查方法通常费用高昂、耗时长，并且仅适用于大规模应用。先进的人工智能（AI）方法在癌症检测方面发挥了巨大作用，但需要特定或深入的医学数据。这些方面影响了癌症筛查方法的大规模实施。因此，基于已有的电子健康记录（EHR）数据对患者进行大规模个性化癌症风险评估应用AI方法是一种颠覆性的改变。本文提出了一种利用EHR数据进行大规模肿瘤风险预测的新方法。与其他方法相比，我们的方法通过最小的数据贪婪策略脱颖而出，仅需要来自EHR的医疗服务代码和诊断历史。我们将问题形式化为二分类问题。该数据集包含了175441名不记名的患者（其中2861名被诊断为癌症）。作为基准，我们实现了一个基于循环神经网络（RNN）的解决方案。我们提出了一种方法，将存活分析和机器学习相结合，

    Purely medical cancer screening methods are often costly, time-consuming, and weakly applicable on a large scale. Advanced Artificial Intelligence (AI) methods greatly help cancer detection but require specific or deep medical data. These aspects affect the mass implementation of cancer screening methods. For these reasons, it is a disruptive change for healthcare to apply AI methods for mass personalized assessment of the cancer risk among patients based on the existing Electronic Health Records (EHR) volume.  This paper presents a novel method for mass cancer risk prediction using EHR data. Among other methods, our one stands out by the minimum data greedy policy, requiring only a history of medical service codes and diagnoses from EHR. We formulate the problem as a binary classification. This dataset contains 175 441 de-identified patients (2 861 diagnosed with cancer). As a baseline, we implement a solution based on a recurrent neural network (RNN). We propose a method that combine
    
[^11]: HPCR: 基于代理的综合对比重放用于在线连续学习

    HPCR: Holistic Proxy-based Contrastive Replay for Online Continual Learning. (arXiv:2309.15038v1 [cs.LG])

    [http://arxiv.org/abs/2309.15038](http://arxiv.org/abs/2309.15038)

    HPCR是一种用于在线连续学习的新方法，该方法综合了基于代理和对比损失的重放方式。通过在对比损失中使用锚点-代理对替换锚点-样本对，HPCR能够减轻遗忘现象，并有效学习更细粒度的语义信息。实验证明，HPCR在多个任务上实现了最先进的性能。

    

    在线连续学习（OCL）旨在通过一次在线数据流传递持续学习新数据。然而，它通常会面临灾难性遗忘问题。现有的基于重放的方法通过以代理为基础或对比为基础的重放方式有效地缓解了这个问题。在本文中，我们对这两种重放方式进行了全面分析，并发现它们可以相互补充。受到这一发现的启发，我们提出了一种新颖的基于重放的方法称为代理对比重放（PCR），它将对比损失中的锚点-样本对替换为锚点-代理对，以减轻遗忘现象。基于PCR，我们进一步开发了一种更高级的方法，称为综合代理对比重放（HPCR），它由三个组件组成。对比组件在PCR的基础上条件性地将锚点-样本对纳入其中，通过大型训练批次学习更细粒度的语义信息。第二个组件是重放组件，它在样本选择上采用了多样性策略，以确保代理数据与当前任务具有更高的关联性。第三个组件是正则化组件，通过缩小样本空间，促进学习模型对任务特定特征的更好表示。实验证明，HPCR方法在多个在线连续学习任务上实现了最先进的性能。

    Online continual learning (OCL) aims to continuously learn new data from a single pass over the online data stream. It generally suffers from the catastrophic forgetting issue. Existing replay-based methods effectively alleviate this issue by replaying part of old data in a proxy-based or contrastive-based replay manner. In this paper, we conduct a comprehensive analysis of these two replay manners and find they can be complementary. Inspired by this finding, we propose a novel replay-based method called proxy-based contrastive replay (PCR), which replaces anchor-to-sample pairs with anchor-to-proxy pairs in the contrastive-based loss to alleviate the phenomenon of forgetting. Based on PCR, we further develop a more advanced method named holistic proxy-based contrastive replay (HPCR), which consists of three components. The contrastive component conditionally incorporates anchor-to-sample pairs to PCR, learning more fine-grained semantic information with a large training batch. The sec
    
[^12]: 让PPO变得更好：基于值导向的Monte-Carlo Tree Search解码

    Making PPO even better: Value-Guided Monte-Carlo Tree Search decoding. (arXiv:2309.15028v1 [cs.CL])

    [http://arxiv.org/abs/2309.15028](http://arxiv.org/abs/2309.15028)

    本文提出了一种基于值导向的Monte-Carlo Tree Search解码算法PPO-MCTS，通过在PPO之上集成MCTS，解决了训练和测试之间部分输出评分机制的不匹配问题，实验证明该算法可以显著提升性能。

    

    在生成自然语言文本时，使用最新的强化学习算法，如Proximal Policy Optimization (PPO)，因此可以认为推理时间的搜索算法，如Monte-Carlo Tree Search (MCTS) 是不必要的。本文证明了通过在PPO之上集成MCTS，可以进一步提升PPO的性能。关键思想是在解码文本时，不要丢弃值网络，即PPO训练时用于评估部分输出序列的副产品，而是将其与策略网络紧密结合。具体而言，本文提出了一种称为PPO-MCTS的新颖的值导向解码算法，可以将来自PPO的值网络与推理时间产生的策略网络紧密结合。与基于MCTS的控制文本生成的先前方法相比，我们的方法的关键优势在于减少了训练和测试之间部分输出的评分机制的基本不匹配。在四个文本生成任务上的评估结果表明，PPO-MCTS可以显著提升性能。

    Inference-time search algorithms such as Monte-Carlo Tree Search (MCTS) may seem unnecessary when generating natural language text based on state-of-the-art reinforcement learning such as Proximal Policy Optimization (PPO). In this paper, we demonstrate that it is possible to get extra mileage out of PPO by integrating MCTS on top. The key idea is not to throw out the value network, a byproduct of PPO training for evaluating partial output sequences, when decoding text out of the policy network. More concretely, we present a novel value-guided decoding algorithm called PPO-MCTS, which can integrate the value network from PPO to work closely with the policy network during inference-time generation. Compared to prior approaches based on MCTS for controlled text generation, the key strength of our approach is to reduce the fundamental mismatch of the scoring mechanisms of the partial outputs between training and test. Evaluation on four text generation tasks demonstrate that PPO-MCTS grea
    
[^13]: Synthia的旋律：无监督领域自适应音频基准框架

    Synthia's Melody: A Benchmark Framework for Unsupervised Domain Adaptation in Audio. (arXiv:2309.15024v1 [cs.SD])

    [http://arxiv.org/abs/2309.15024](http://arxiv.org/abs/2309.15024)

    Synthia's melody提出了一个新颖的音频数据生成框架，为无监督领域自适应提供了一个适当的基准数据集，这个框架能够模拟各种旋律，并评估声学深度学习模型对分布转移的敏感性。

    

    尽管在视觉和自然语言的深度学习方面取得了重大进展，但音频领域的无监督领域自适应仍然相对未开发。其中一个原因是缺乏适当的基准数据集。为了填补这一空白，我们提出了Synthia的旋律，一种新颖的音频数据生成框架，能够模拟具有用户指定混淆结构的无限多种4秒旋律，这些结构由音乐键、音色和音量特征化。与观测设置下收集的现有数据集不同，Synthia的旋律没有未观察到的偏差，确保了实验的可重复性和可比性。为了展示其实用性，我们生成了两种类型的分布转移-领域转移和样本选择偏差，并评估了这些转移下声学深度学习模型的性能。我们的评估结果表明，Synthia的旋律为检验这些模型对不同分布级别的敏感性提供了一个稳健的实验平台。

    Despite significant advancements in deep learning for vision and natural language, unsupervised domain adaptation in audio remains relatively unexplored. We, in part, attribute this to the lack of an appropriate benchmark dataset. To address this gap, we present Synthia's melody, a novel audio data generation framework capable of simulating an infinite variety of 4-second melodies with user-specified confounding structures characterised by musical keys, timbre, and loudness. Unlike existing datasets collected under observational settings, Synthia's melody is free of unobserved biases, ensuring the reproducibility and comparability of experiments. To showcase its utility, we generate two types of distribution shifts-domain shift and sample selection bias-and evaluate the performance of acoustic deep learning models under these shifts. Our evaluations reveal that Synthia's melody provides a robust testbed for examining the susceptibility of these models to varying levels of distribution 
    
[^14]: 从教育文本中自动生成问题

    Automating question generation from educational text. (arXiv:2309.15004v1 [cs.CL])

    [http://arxiv.org/abs/2309.15004](http://arxiv.org/abs/2309.15004)

    本文设计并评估了一个用于学校形成性和总结性评估的自动化问题生成工具，通过对教师的调查，证明了自动化生成问题的需求，并提出了一个基于Transformer的语言模型的模块化框架，用于从文本内容中自动生成多项选择题。

    

    问题式活动（QBA）在教育中得到广泛应用，传统上是学习和评估过程的一个重要组成部分。本文设计并评估了一个用于学校形成性和总结性评估的自动化问题生成工具。通过对104名教师的专家调查，我们展示了自动化生成QBA的需求，作为一个能够显著减轻教师工作量并促进个性化学习体验的工具。利用生成型AI的最新进展，我们提出了一个基于Transformer的语言模型的模块化框架，用于从文本内容中自动生成多项选择题（MCQ）。所提出的解决方案具有问题生成、正确答案预测和干扰项制定的不同模块，使我们能够评估不同的语言模型和生成技术。最后，我们进行了广泛的定量和定性评估。

    The use of question-based activities (QBAs) is wide-spread in education, traditionally forming an integral part of the learning and assessment process. In this paper, we design and evaluate an automated question generation tool for formative and summative assessment in schools. We present an expert survey of one hundred and four teachers, demonstrating the need for automated generation of QBAs, as a tool that can significantly reduce the workload of teachers and facilitate personalized learning experiences. Leveraging the recent advancements in generative AI, we then present a modular framework employing transformer based language models for automatic generation of multiple-choice questions (MCQs) from textual content. The presented solution, with distinct modules for question generation, correct answer prediction, and distractor formulation, enables us to evaluate different language models and generation techniques. Finally, we perform an extensive quantitative and qualitative evaluat
    
[^15]: 针对帆船价格和特征以及区域地区的测量模型

    Measurement Models For Sailboats Price vs. Features And Regional Areas. (arXiv:2309.14994v1 [cs.LG])

    [http://arxiv.org/abs/2309.14994](http://arxiv.org/abs/2309.14994)

    这项研究调查了帆船技术规格和价格之间的关系以及区域定价的影响。通过应用多个机器学习模型，我们发现单体船通常比双体船更实惠，并且长度、宽度、排水量和帆面积等特定规格与较高的价格直接相关。此外，我们还发现美国是平均帆船价格最高的国家，而国内生产总值与帆船价格没有直接相关关系。

    

    在这项研究中，我们调查了帆船技术规格与其价格之间的关系，以及区域定价的影响。利用包括长度、宽度、吃水、排水量、帆面积和水线等特征的数据集，我们应用多个机器学习模型来预测帆船价格。梯度下降模型表现出优秀的性能，产生了最低的MSE和MAE。我们的分析发现，单体船通常比双体船更实惠，而长度、宽度、排水量和帆面积等特定规格与较高的价格直接相关。有趣的是，较低的吃水与较高的挂牌价格有关联。我们还探讨了区域定价因素，并发现美国在平均帆船价格上居首，其次是欧洲、香港和加勒比地区。与我们最初的假设相反，一个国家的GDP与帆船价格没有直接相关关系。

    In this study, we investigated the relationship between sailboat technical specifications and their prices, as well as regional pricing influences. Utilizing a dataset encompassing characteristics like length, beam, draft, displacement, sail area, and waterline, we applied multiple machine learning models to predict sailboat prices. The gradient descent model demonstrated superior performance, producing the lowest MSE and MAE. Our analysis revealed that monohulled boats are generally more affordable than catamarans, and that certain specifications such as length, beam, displacement, and sail area directly correlate with higher prices. Interestingly, lower draft was associated with higher listing prices. We also explored regional price determinants and found that the United States tops the list in average sailboat prices, followed by Europe, Hong Kong, and the Caribbean. Contrary to our initial hypothesis, a country's GDP showed no direct correlation with sailboat prices. Utilizing a 50
    
[^16]: 非平稳强化学习中的节奏适应

    Tempo Adaption in Non-stationary Reinforcement Learning. (arXiv:2309.14989v1 [cs.LG])

    [http://arxiv.org/abs/2309.14989](http://arxiv.org/abs/2309.14989)

    该论文解决了非平稳强化学习中"时间同步"问题，通过考虑墙钟时间而不是情节进展来实现对环境变化的适应。

    

    首先我们提出并解决了非平稳强化学习中的“时间同步”问题，这是阻碍其在真实世界应用中的一个关键因素。现实中，环境的变化是按照墙钟时间（$\mathfrak{t}$）而不是按照情节进展（$k$）发生的，其中墙钟时间表示固定持续时间$\mathfrak{t} \in [0, T]$内实际流逝的时间。在现有的工作中，在情节$k$时，智能体生成一个轨迹并训练一个策略，然后转入情节$k+1$。然而，在时间不同步的环境下，智能体在时间$\mathfrak{t}_k$分配$\Delta \mathfrak{t}$用于轨迹生成和训练，然后在$\mathfrak{t}_{k+1}=\mathfrak{t}_{k}+\Delta \mathfrak{t}$时刻转入下一个情节。尽管情节总数固定（$K$），智能体根据相互作用时间的选择（$\mathfrak{t}_1,\mathfrak{t}_2,...,\mathfrak{t}_K$）积累不同的轨迹。

    We first raise and tackle ``time synchronization'' issue between the agent and the environment in non-stationary reinforcement learning (RL), a crucial factor hindering its real-world applications. In reality, environmental changes occur over wall-clock time ($\mathfrak{t}$) rather than episode progress ($k$), where wall-clock time signifies the actual elapsed time within the fixed duration $\mathfrak{t} \in [0, T]$. In existing works, at episode $k$, the agent rollouts a trajectory and trains a policy before transitioning to episode $k+1$. In the context of the time-desynchronized environment, however, the agent at time $\mathfrak{t}_k$ allocates $\Delta \mathfrak{t}$ for trajectory generation and training, subsequently moves to the next episode at $\mathfrak{t}_{k+1}=\mathfrak{t}_{k}+\Delta \mathfrak{t}$. Despite a fixed total episode ($K$), the agent accumulates different trajectories influenced by the choice of \textit{interaction times} ($\mathfrak{t}_1,\mathfrak{t}_2,...,\mathfra
    
[^17]: 量子神经网络中的量子态学习过程的统计分析

    Statistical Analysis of Quantum State Learning Process in Quantum Neural Networks. (arXiv:2309.14980v1 [quant-ph])

    [http://arxiv.org/abs/2309.14980](http://arxiv.org/abs/2309.14980)

    本文研究了在量子神经网络中学习未知量子态的问题，并提出了一个不可行定理。当损失值低于临界阈值时，避免局部极小值的概率随着量子比特数指数级减少，而随着电路深度多项式增长。局部极小值的曲率集中在量子Fisher信息乘以损失相关常数上，该常数表征输出态相对于QNN参数的敏感性。

    

    量子神经网络(QNN)是追求近期量子优势的各个领域中一个有前途的框架，其中许多应用可以看作学习编码有用数据的量子态。作为概率分布学习的量子模拟，量子态学习在量子机器学习中理论上和实践上都是重要的。本文针对使用QNN学习未知量子态的问题提出一个不可行定理，即使从高保真度的初始态开始。我们证明，当损失值低于临界阈值时，避免局部极小值的概率随着量子比特数指数级减少，而随着电路深度多项式增长。局部极小值的曲率集中在量子Fisher信息乘以损失相关常数上，该常数表征输出态相对于QNN参数的敏感性。这些结果适用于任何电路结构、初始化策略以及……

    Quantum neural networks (QNNs) have been a promising framework in pursuing near-term quantum advantage in various fields, where many applications can be viewed as learning a quantum state that encodes useful data. As a quantum analog of probability distribution learning, quantum state learning is theoretically and practically essential in quantum machine learning. In this paper, we develop a no-go theorem for learning an unknown quantum state with QNNs even starting from a high-fidelity initial state. We prove that when the loss value is lower than a critical threshold, the probability of avoiding local minima vanishes exponentially with the qubit count, while only grows polynomially with the circuit depth. The curvature of local minima is concentrated to the quantum Fisher information times a loss-dependent constant, which characterizes the sensibility of the output state with respect to parameters in QNNs. These results hold for any circuit structures, initialization strategies, and 
    
[^18]: 递归超网络在元强化学习中表现出惊人的强大性能

    Recurrent Hypernetworks are Surprisingly Strong in Meta-RL. (arXiv:2309.14970v1 [cs.LG])

    [http://arxiv.org/abs/2309.14970](http://arxiv.org/abs/2309.14970)

    递归超网络和循环神经网络在元强化学习中的端到端学习表现出惊人的强大性能，相比于现有专门方法更为简单但效果更好。

    

    深度强化学习在实际应用时因样本效率低而不易部署。元强化学习通过学习在元训练时利用相关任务的分布来实现少样本学习，直接解决了这个样本效率问题。最近的研究表明，与专门的元强化学习方法相比，与一个通用的序列模型（如循环神经网络）结合的端到端学习是一个令人惊讶的强基准。然而，这样的观点由于有限的支持证据而引起了争议，特别是在之前的研究中确立了完全相反的观点。在本文中，我们进行了实证研究。虽然我们同样发现循环网络可以达到强大的性能，但我们证明了超网络的使用对于发挥循环基线的潜力至关重要。令人惊讶的是，与超网络相结合时，这种远比现有专门方法简单的循环基准实际上能取得更好的表现。

    Deep reinforcement learning (RL) is notoriously impractical to deploy due to sample inefficiency. Meta-RL directly addresses this sample inefficiency by learning to perform few-shot learning when a distribution of related tasks is available for meta-training. While many specialized meta-RL methods have been proposed, recent work suggests that end-to-end learning in conjunction with an off-the-shelf sequential model, such as a recurrent network, is a surprisingly strong baseline. However, such claims have been controversial due to limited supporting evidence, particularly in the face of prior work establishing precisely the opposite. In this paper, we conduct an empirical investigation. While we likewise find that a recurrent network can achieve strong performance, we demonstrate that the use of hypernetworks is crucial to maximizing their potential. Surprisingly, when combined with hypernetworks, the recurrent baselines that are far simpler than existing specialized methods actually ac
    
[^19]: 用于航空器地面航迹预测的上下文感知生成模型

    Context-Aware Generative Models for Prediction of Aircraft Ground Tracks. (arXiv:2309.14957v1 [eess.SY])

    [http://arxiv.org/abs/2309.14957](http://arxiv.org/abs/2309.14957)

    本研究提出了一种上下文感知的生成模型用于航空器地面航迹预测，通过利用概率机器学习建模飞行员行为和空中交通管制员意图的不确定性，从而改善了传统预测方法的局限性。训练的模型可以针对特定的飞行区域进行个性化预测，并且在繁忙的空域实验中取得了良好的性能。

    

    航迹预测在支持空中交通管制员的决策中起着重要作用。传统的预测方法是确定性的和基于物理的，使用全球收集的航空监测数据进行校准。然而，这些模型忽视了飞行员和空中交通管制员的意图，这可能对观测到的航迹产生显著影响，特别是在横向平面上。本研究提出了一种用于横向航迹预测的生成方法，利用概率机器学习来建模来自飞行员行为和空中交通管制员意图未知影响的认知不确定性。模型被训练成特定于一个特定的飞行区域，可以建模本地的协调进出点等程序。使用了一周时间的通过英国上层空域一个繁忙区域的航空监测数据集来训练和测试这些模型。

    Trajectory prediction (TP) plays an important role in supporting the decision-making of Air Traffic Controllers (ATCOs). Traditional TP methods are deterministic and physics-based, with parameters that are calibrated using aircraft surveillance data harvested across the world. These models are, therefore, agnostic to the intentions of the pilots and ATCOs, which can have a significant effect on the observed trajectory, particularly in the lateral plane. This work proposes a generative method for lateral TP, using probabilistic machine learning to model the effect of the epistemic uncertainty arising from the unknown effect of pilot behaviour and ATCO intentions. The models are trained to be specific to a particular sector, allowing local procedures such as coordinated entry and exit points to be modelled. A dataset comprising a week's worth of aircraft surveillance data, passing through a busy sector of the United Kingdom's upper airspace, was used to train and test the models. Specifi
    
[^20]: 面向真实世界测试时间自适应：具有平衡归一化的三网络自训练

    Towards Real-World Test-Time Adaptation: Tri-Net Self-Training with Balanced Normalization. (arXiv:2309.14949v1 [cs.LG])

    [http://arxiv.org/abs/2309.14949](http://arxiv.org/abs/2309.14949)

    本文研究了真实世界测试时间自适应问题，在全局类别不平衡的测试集上补充了现有的协议，并提出了一种平衡归一化层来适应不平衡的测试数据，以解决现有方法的失败。

    

    测试时间自适应旨在将源域模型适应到推断阶段的测试数据中，在适应到未见过的破损情况下取得了成功。然而，在更具挑战性的真实世界情境下，这些尝试可能会失败。现有的研究主要考虑非独立同分布的数据流和持续的领域转移下的真实世界测试时间自适应。在这项工作中，我们首先用全局类别不平衡的测试集来补充现有的真实世界TTA协议。我们证明把所有设置结合起来对现有方法提出了新的挑战。我们认为最先失败的现有方法是因为不加选择地将归一化层适应到不平衡的测试数据上所导致的。为了解决这个缺点，我们提出了一个平衡批量归一化层，在推断阶段替换原来的批量归一化。新的批量归一化层能够适应而不偏向多数类别。我们受到自学习（ST）在无标签学习中的成功启发。

    Test-Time Adaptation aims to adapt source domain model to testing data at inference stage with success demonstrated in adapting to unseen corruptions. However, these attempts may fail under more challenging real-world scenarios. Existing works mainly consider real-world test-time adaptation under non-i.i.d. data stream and continual domain shift. In this work, we first complement the existing real-world TTA protocol with a globally class imbalanced testing set. We demonstrate that combining all settings together poses new challenges to existing methods. We argue the failure of state-of-the-art methods is first caused by indiscriminately adapting normalization layers to imbalanced testing data. To remedy this shortcoming, we propose a balanced batchnorm layer to swap out the regular batchnorm at inference stage. The new batchnorm layer is capable of adapting without biasing towards majority classes. We are further inspired by the success of self-training~(ST) in learning from unlabeled 
    
[^21]: 从雷达数据学习攀升飞机的生成模型

    Learning Generative Models for Climbing Aircraft from Radar Data. (arXiv:2309.14941v1 [eess.SY])

    [http://arxiv.org/abs/2309.14941](http://arxiv.org/abs/2309.14941)

    本文提出了一种利用雷达数据学习的生成模型，能够准确预测攀升飞机的轨迹，并通过学习修正推力的函数来提高预测准确性。该方法的优势包括：与标准模型相比，到达时间的预测误差减少了66.3%；生成的轨迹与测试数据相比更加真实；并且能够以最小的计算成本计算置信区间。

    

    攀升飞机的准确轨迹预测受到机载设备操作的不确定性的影响，可能导致预测轨迹与观测轨迹之间存在显著的差异。本文提出了一种生成模型，通过从数据中学习修正推力的函数来丰富标准的飞机基础数据（BADA）模型。该方法具有三个特点：与BADA相比，到达时间的预测误差减少了66.3%；生成的轨迹与测试数据相比更加真实；并且能够以最小的计算成本计算置信区间。

    Accurate trajectory prediction (TP) for climbing aircraft is hampered by the presence of epistemic uncertainties concerning aircraft operation, which can lead to significant misspecification between predicted and observed trajectories. This paper proposes a generative model for climbing aircraft in which the standard Base of Aircraft Data (BADA) model is enriched by a functional correction to the thrust that is learned from data. The method offers three features: predictions of the arrival time with 66.3% less error when compared to BADA; generated trajectories that are realistic when compared to test data; and a means of computing confidence bounds for minimal computational cost.
    
[^22]: 平行多目标超参数优化与统一归一化与有界目标

    Parallel Multi-Objective Hyperparameter Optimization with Uniform Normalization and Bounded Objectives. (arXiv:2309.14936v1 [cs.LG])

    [http://arxiv.org/abs/2309.14936](http://arxiv.org/abs/2309.14936)

    这项研究提出了一种多目标贝叶斯优化算法，通过统一目标归一化和随机化权重进行标量化，解决了多目标超参数优化中的挑战问题。

    

    机器学习方法提供了一系列可配置的超参数，这些超参数对于性能有着重要影响。尽管准确度是一个常用的性能目标，但在许多情况下，仅准确度是不足够的。在许多设置下，优化机器学习模型与准确度、置信度、公平性、校准、隐私、延迟和内存消耗等多个目标对其至关重要。为了解决这个问题，超参数优化，即系统地优化超参数的方法，对于单一目标已经很具挑战性了，对于多个目标则更加困难。此外，目标尺度的差异、失败和异常值的存在使问题更加困难。我们提出了一种多目标贝叶斯优化(MoBO)算法，通过统一目标归一化和随机化权重进行标量化，来解决这些问题。我们通过施加约束来提高我们方法的效率。

    Machine learning (ML) methods offer a wide range of configurable hyperparameters that have a significant influence on their performance. While accuracy is a commonly used performance objective, in many settings, it is not sufficient. Optimizing the ML models with respect to multiple objectives such as accuracy, confidence, fairness, calibration, privacy, latency, and memory consumption is becoming crucial. To that end, hyperparameter optimization, the approach to systematically optimize the hyperparameters, which is already challenging for a single objective, is even more challenging for multiple objectives. In addition, the differences in objective scales, the failures, and the presence of outlier values in objectives make the problem even harder. We propose a multi-objective Bayesian optimization (MoBO) algorithm that addresses these problems through uniform objective normalization and randomized weights in scalarization. We increase the efficiency of our approach by imposing constra
    
[^23]: 噪声容忍的无监督视觉语言模型适配器

    Noise-Tolerant Unsupervised Adapter for Vision-Language Models. (arXiv:2309.14928v1 [cs.CV])

    [http://arxiv.org/abs/2309.14928](http://arxiv.org/abs/2309.14928)

    这篇论文介绍了一种噪声容忍的无监督适配器(NtUA)，它可以使用少样本无标签目标样本来学习优秀的视觉语言模型。NtUA通过自适应缓存形成和伪标签修正来对抗伪标签噪声。

    

    最近在大规模的视觉语言模型中取得了非常显著的表现，在各种零样本图像分类任务中获得了良好的性能。然而，先前的研究通过引入少样本有标签目标样本已经取得了显著的改进，但仍需要目标样本的标注，这在处理各种视觉识别任务时大大降低了可扩展性。我们设计了一种噪声容忍的无监督适配器(NtUA)，它允许使用少样本无标签目标样本来学习优秀的目标模型。NtUA作为一个键值缓存，将少样本无标签目标样本的视觉特征和预测的伪标签作为键值对进行建模。它由两个互补的设计组成。第一个是自适应缓存形成，通过根据其预测置信度对键值对进行加权，以对抗伪标签的噪声。第二个是伪标签修正，它通过利用键值对的权重来修正伪标签以及缓存权重。

    Recent advances in large-scale vision-language models have achieved very impressive performance in various zero-shot image classification tasks. While prior studies have demonstrated significant improvements by introducing few-shot labelled target samples, they still require labelling of target samples, which greatly degrades their scalability while handling various visual recognition tasks. We design NtUA, a Noise-tolerant Unsupervised Adapter that allows learning superior target models with few-shot unlabelled target samples. NtUA works as a key-value cache that formulates visual features and predicted pseudo-labels of the few-shot unlabelled target samples as key-value pairs. It consists of two complementary designs. The first is adaptive cache formation that combats pseudo-label noises by weighting the key-value pairs according to their prediction confidence. The second is pseudo-label rectification, which corrects both pair values (i.e., pseudo-labels) and cache weights by leverag
    
[^24]: 对大规模属性图上的节点表示学习进行标签解卷积以抵抗学习偏差的研究

    Label Deconvolution for Node Representation Learning on Large-scale Attributed Graphs against Learning Bias. (arXiv:2309.14907v1 [cs.LG])

    [http://arxiv.org/abs/2309.14907](http://arxiv.org/abs/2309.14907)

    本文提出了一种标签解卷积技术(LD)，通过对图神经网络(GNNs)的逆映射进行高效的近似，来解决在大规模属性图上进行节点表示学习时的学习偏差挑战。

    

    在带属性的图中，节点表示学习对许多重要的下游任务起着关键作用。为了同时编码属性和图结构，最近的研究将预训练模型与图神经网络(GNNs)进行整合，其中预训练模型作为节点编码器(NEs)来编码属性。由于在大规模图上同时训练大型NEs和GNNs存在严重的可伸缩性问题，许多方法提出了分别训练NEs和GNNs的方法。因此，在NEs的训练阶段中，他们没有考虑到GNNs中的特征卷积，导致了与联合训练相比的显著学习偏差。为了解决这个挑战，我们提出了一种高效的标签正则化技术，即标签解卷积(LD)，通过对GNNs的逆映射进行新颖且高度可伸缩的近似，以减轻学习偏差。

    Node representation learning on attributed graphs -- whose nodes are associated with rich attributes (e.g., texts and protein sequences) -- plays a crucial role in many important downstream tasks. To encode the attributes and graph structures simultaneously, recent studies integrate pre-trained models with graph neural networks (GNNs), where pre-trained models serve as node encoders (NEs) to encode the attributes. As jointly training large NEs and GNNs on large-scale graphs suffers from severe scalability issues, many methods propose to train NEs and GNNs separately. Consequently, they do not take feature convolutions in GNNs into consideration in the training phase of NEs, leading to a significant learning bias from that by the joint training. To address this challenge, we propose an efficient label regularization technique, namely Label Deconvolution (LD), to alleviate the learning bias by a novel and highly scalable approximation to the inverse mapping of GNNs. The inverse mapping l
    
[^25]: 可验证的学习行为通过运动原语构成：应用于颗粒状介质的铲取

    Verifiable Learned Behaviors via Motion Primitive Composition: Applications to Scooping of Granular Media. (arXiv:2309.14894v1 [cs.RO])

    [http://arxiv.org/abs/2309.14894](http://arxiv.org/abs/2309.14894)

    通过运动原语构成的机器人行为模型能够从自然语言输入中实时生成可验证的行为，为工业机器人的灵活性提供了增强。实验表明这种模型在探索任务和处理颗粒状介质的实际情境下具有可验证性。

    

    一种能够实时可靠地从自然语言输入中生成行为的机器人行为模型，将在很大程度上加快工业机器人的采用，因为它增强了系统的灵活性。为了促进这些努力，我们构建了一个框架，其中通过自然语言摘要器创建的学习行为可以通过构造进行验证。利用最近在运动原语和概率验证方面的进展，我们构建了一个自然语言行为摘要器，通过合成所提供的运动原语的有向图生成行为。如果这些组成的运动原语符合我们指定的标准，所得到的行为具有概率上的可验证性。我们在探索任务的模拟和使用机器人在硬件上铲取颗粒状介质的实验中展示了这种可验证的行为生成能力。

    A robotic behavior model that can reliably generate behaviors from natural language inputs in real time would substantially expedite the adoption of industrial robots due to enhanced system flexibility. To facilitate these efforts, we construct a framework in which learned behaviors, created by a natural language abstractor, are verifiable by construction. Leveraging recent advancements in motion primitives and probabilistic verification, we construct a natural-language behavior abstractor that generates behaviors by synthesizing a directed graph over the provided motion primitives. If these component motion primitives are constructed according to the criteria we specify, the resulting behaviors are probabilistically verifiable. We demonstrate this verifiable behavior generation capacity in both simulation on an exploration task and on hardware with a robot scooping granular media.
    
[^26]: 卫星图像GAN潜在空间的局部保持方向解释方法

    Locality-preserving Directions for Interpreting the Latent Space of Satellite Image GANs. (arXiv:2309.14883v1 [cs.CV])

    [http://arxiv.org/abs/2309.14883](http://arxiv.org/abs/2309.14883)

    我们提出了一种用于解释卫星图像GAN潜在空间的局部保持方法，能够捕捉卫星图像特有的大尺度和谱变异性，并恢复解释性方向，从而用于有导向合成。通过保持局部性，得到的向量更鲁棒，能更好地保持类别信息。

    

    我们提出了一种适用于基于小波的生成对抗网络（GAN）的解释潜在空间的局部感知方法，能够很好地捕捉到卫星图像特有的大尺度和谱变异性。通过专注于保持局部性，所提出的方法能够分解预训练GAN的权重空间，并恢复与高级语义概念（如城市化、结构密度、植被存在）相对应的可解释方向，从而用于卫星图像的有导向合成。与通常采用的在降维空间中捕捉权重空间变异性的方法（如基于主成分分析PCA），我们显示保持局部性可以得到不同角度的向量，这些向量对伪影更加鲁棒，能更好地保持类别信息。通过一组定量和定性实例，我们进一步展示了所提出方法的优势。

    We present a locality-aware method for interpreting the latent space of wavelet-based Generative Adversarial Networks (GANs), that can well capture the large spatial and spectral variability that is characteristic to satellite imagery. By focusing on preserving locality, the proposed method is able to decompose the weight-space of pre-trained GANs and recover interpretable directions that correspond to high-level semantic concepts (such as urbanization, structure density, flora presence) - that can subsequently be used for guided synthesis of satellite imagery. In contrast to typically used approaches that focus on capturing the variability of the weight-space in a reduced dimensionality space (i.e., based on Principal Component Analysis, PCA), we show that preserving locality leads to vectors with different angles, that are more robust to artifacts and can better preserve class information. Via a set of quantitative and qualitative examples, we further show that the proposed approach 
    
[^27]: 基于子空间学习的单类分类方法在信用卡欺诈检测中的应用

    Credit Card Fraud Detection with Subspace Learning-based One-Class Classification. (arXiv:2309.14880v1 [cs.LG])

    [http://arxiv.org/abs/2309.14880](http://arxiv.org/abs/2309.14880)

    本文研究了基于子空间学习的单类分类方法在信用卡欺诈检测中的应用，该方法能够处理不平衡数据分布并具备预测和对抗未知欺诈技术的能力。

    

    在日益数字化的商业环境中，信用卡欺诈的激增和欺诈技术的不断演变导致了巨大的财务损失。自动化信用卡欺诈检测是加速检测、减少响应时间和最小化潜在财务损失的可行方法。然而，由于数据集的高度不平衡性，即真实交易远远多于欺诈交易，解决这一挑战变得复杂。此外，特征集合中的高维度数量引发了“维度灾难”。在本文中，我们研究以单类分类（OCC）算法为核心的基于子空间学习的方法，该方法在处理不平衡数据分布方面表现优异，并具有预见和对抗尚未发明的欺诈技术的能力。研究通过调查子空间学习的OCC算法的潜力来突出该方法的优势。

    In an increasingly digitalized commerce landscape, the proliferation of credit card fraud and the evolution of sophisticated fraudulent techniques have led to substantial financial losses. Automating credit card fraud detection is a viable way to accelerate detection, reducing response times and minimizing potential financial losses. However, addressing this challenge is complicated by the highly imbalanced nature of the datasets, where genuine transactions vastly outnumber fraudulent ones. Furthermore, the high number of dimensions within the feature set gives rise to the ``curse of dimensionality". In this paper, we investigate subspace learning-based approaches centered on One-Class Classification (OCC) algorithms, which excel in handling imbalanced data distributions and possess the capability to anticipate and counter the transactions carried out by yet-to-be-invented fraud techniques. The study highlights the potential of subspace learning-based OCC algorithms by investigating th
    
[^28]: 导航文本到图像定制：从LyCORIS微调到模型评估

    Navigating Text-To-Image Customization:From LyCORIS Fine-Tuning to Model Evaluation. (arXiv:2309.14859v1 [cs.CV])

    [http://arxiv.org/abs/2309.14859](http://arxiv.org/abs/2309.14859)

    本文介绍了LyCORIS，一个开源库，提供了多种稳定扩散模型的微调方法，并提出了一个系统评估的全面框架。

    

    文本到图像生成模型因其能够从文本提示生成高保真度图像而受到广泛关注。其中，稳定扩散模型作为领先的开源模型在这个快速发展的领域中表现出色。然而，微调这些模型的复杂性给新方法的整合和系统评估带来了多重挑战。本文介绍了LyCORIS（Lora beYond Conventional methods，Other Rank adaptation Implementations for Stable diffusion）[https://github.com/KohakuBlueleaf/LyCORIS]，这是一个开源库，提供了多种稳定扩散模型的微调方法。此外，我们还提出了一个系统评估的全面框架，该框架采用了多样化的指标，并深入研究了微调的多个方面，包括超参数调整和在不同概念类别下使用不同提示类型的评估。

    Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts. Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field. However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation. Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion) [https://github.com/KohakuBlueleaf/LyCORIS], an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion. Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categori
    
[^29]: 使用信息流形投影进行聚类探索

    Cluster Exploration using Informative Manifold Projections. (arXiv:2309.14857v1 [cs.LG])

    [http://arxiv.org/abs/2309.14857](http://arxiv.org/abs/2309.14857)

    该论文提出了一种新颖的方法来生成信息丰富的嵌入，以揭示高维数据中的聚类结构。通过线性组合对比PCA和峰度投影追踪两个目标，该方法能够排除先验信息相关的结构并实现有意义的数据分离。

    

    降维是可视化探索高维数据和发现其在二维或三维空间中的聚类结构的关键工具之一。现有文献中的大部分降维方法并未考虑实践者可能对所考虑数据集的任何先验知识。我们提出了一种新颖的方法来生成信息丰富的嵌入，不仅排除与先验知识相关的结构，而且旨在揭示任何剩余的潜在结构。为了实现这一目标，我们采用了两个目标的线性组合：首先是对比PCA，能够消除与先验信息相关的结构，其次是峰度投影追踪，可以确保在得到的嵌入中有意义的数据分离。我们将这个任务定义为流形优化问题，并在考虑三种不同类型的先验知识的各种数据集上进行了实证验证。

    Dimensionality reduction (DR) is one of the key tools for the visual exploration of high-dimensional data and uncovering its cluster structure in two- or three-dimensional spaces. The vast majority of DR methods in the literature do not take into account any prior knowledge a practitioner may have regarding the dataset under consideration. We propose a novel method to generate informative embeddings which not only factor out the structure associated with different kinds of prior knowledge but also aim to reveal any remaining underlying structure. To achieve this, we employ a linear combination of two objectives: firstly, contrastive PCA that discounts the structure associated with the prior information, and secondly, kurtosis projection pursuit which ensures meaningful data separation in the obtained embeddings. We formulate this task as a manifold optimization problem and validate it empirically across a variety of datasets considering three distinct types of prior knowledge. Lastly, 
    
[^30]: 使用注意机制进行实时动作生成和主动感知的烹饪机器人

    Realtime Motion Generation with Active Perception Using Attention Mechanism for Cooking Robot. (arXiv:2309.14837v1 [cs.RO])

    [http://arxiv.org/abs/2309.14837](http://arxiv.org/abs/2309.14837)

    该论文介绍了一种使用注意机制的预测性递归神经网络，能够实现实时感知和动作生成，以支持烹饪机器人在煮鸡蛋过程中对鸡蛋状态的感知和搅拌动作的调整。

    

    为了支持人类的日常生活，机器人需要自主学习，适应物体和环境，并执行适当的动作。我们尝试使用真实的食材煮炒鸡蛋的任务，其中机器人需要实时感知鸡蛋的状态并调整搅拌动作，同时鸡蛋被加热且状态不断变化。在以前的研究中，处理变化的物体被发现是具有挑战性的，因为感知信息包括动态的、重要或嘈杂的信息，而且每次应该关注的模态不断变化，这使得实现实时感知和动作生成变得困难。我们提出了一个带有注意机制的预测性递归神经网络，可以权衡传感器输入，区分每种模态的重要性和可靠性，实现快速和高效的感知和动作生成。模型通过示范学习进行训练，并允许不断更新。

    To support humans in their daily lives, robots are required to autonomously learn, adapt to objects and environments, and perform the appropriate actions. We tackled on the task of cooking scrambled eggs using real ingredients, in which the robot needs to perceive the states of the egg and adjust stirring movement in real time, while the egg is heated and the state changes continuously. In previous works, handling changing objects was found to be challenging because sensory information includes dynamical, both important or noisy information, and the modality which should be focused on changes every time, making it difficult to realize both perception and motion generation in real time. We propose a predictive recurrent neural network with an attention mechanism that can weigh the sensor input, distinguishing how important and reliable each modality is, that realize quick and efficient perception and motion generation. The model is trained with learning from the demonstration, and allow
    
[^31]: OS-net: 轨道稳定神经网络

    OS-net: Orbitally Stable Neural Networks. (arXiv:2309.14822v1 [math.DS])

    [http://arxiv.org/abs/2309.14822](http://arxiv.org/abs/2309.14822)

    OS-net是一种专门用于周期动力数据的神经网络架构，通过利用常微分方程理论和伴随方法的反向传播方法，确保网络权重的稳定性，成功应用于发现R\"{o}ssler和Sprott系统的动力学。

    

    我们引入了OS-net（轨道稳定神经网络），一种专门设计用于周期动力数据的神经网络架构。OS-net是神经常微分方程（NODEs）的特例，并充分利用基于伴随方法的反向传播方法。利用常微分方程理论，我们推导了网络权重的条件，以确保所得动力学的稳定性。通过将OS-net应用于发现R\"{o}ssler和Sprott系统的动力学，我们证明了我们方法的有效性，这两个动力系统因其倍频吸引子和混沌行为而闻名。

    We introduce OS-net (Orbitally Stable neural NETworks), a new family of neural network architectures specifically designed for periodic dynamical data. OS-net is a special case of Neural Ordinary Differential Equations (NODEs) and takes full advantage of the adjoint method based backpropagation method. Utilizing ODE theory, we derive conditions on the network weights to ensure stability of the resulting dynamics. We demonstrate the efficacy of our approach by applying OS-net to discover the dynamics underlying the R\"{o}ssler and Sprott's systems, two dynamical systems known for their period doubling attractors and chaotic behavior.
    
[^32]: 人口图构建方法和图神经网络在大脑年龄回归中的比较研究

    A Comparative Study of Population-Graph Construction Methods and Graph Neural Networks for Brain Age Regression. (arXiv:2309.14816v1 [cs.LG])

    [http://arxiv.org/abs/2309.14816](http://arxiv.org/abs/2309.14816)

    本研究比较了不同的人口图构建方法和图神经网络对于大脑年龄回归的性能影响。

    

    一个人的实际年龄和生物年龄之间的差异可以成为神经退行性疾病的重要生物标志物，因此在临床环境中，大脑年龄估计至关重要。将多模态信息纳入该估计的一种方法是通过人口图，它结合了各种类型的成像数据，并捕捉了人群内个体之间的关联。在医学成像中，人口图已经显示出有望的结果，主要用于分类任务。在大多数情况下，图结构是预先定义好的，并且在训练过程中保持静态。然而，提取人口图是一项非常复杂的任务，它可以显著影响对于图神经网络（GNNs）的性能，因为GNNs对于图结构非常敏感。在这项工作中，我们强调了有意义的图构建的重要性，并尝试了不同的人口图构建方法，并研究了它们对于大脑年龄估计中GNN性能的影响。

    The difference between the chronological and biological brain age of a subject can be an important biomarker for neurodegenerative diseases, thus brain age estimation can be crucial in clinical settings. One way to incorporate multimodal information into this estimation is through population graphs, which combine various types of imaging data and capture the associations among individuals within a population. In medical imaging, population graphs have demonstrated promising results, mostly for classification tasks. In most cases, the graph structure is pre-defined and remains static during training. However, extracting population graphs is a non-trivial task and can significantly impact the performance of Graph Neural Networks (GNNs), which are sensitive to the graph structure. In this work, we highlight the importance of a meaningful graph construction and experiment with different population-graph construction methods and their effect on GNN performance on brain age estimation. We us
    
[^33]: 重新审视用于连续学习中的Softmax掩码以提高稳定性

    Revisiting Softmax Masking for Stability in Continual Learning. (arXiv:2309.14808v1 [cs.LG])

    [http://arxiv.org/abs/2309.14808](http://arxiv.org/abs/2309.14808)

    本文重新审视了用于连续学习中的Softmax掩码的影响，并提出了一种利用其置信度保持效果的方法，通过增加稳定性同时保持准确性。

    

    在连续学习中，许多分类器使用Softmax函数来学习置信度。然而，许多研究指出其无法准确确定离群值的置信度分布，通常称为认识不确定性。这种固有限制还限制了在连续学习过程中选择何时忘记和保留先前训练的置信度分布的准确决策。为了解决这个问题，我们重新审视了掩码Softmax函数的影响。尽管这种方法在文献中既简单又普遍，但对于在连续学习过程中保持置信度分布（也称为稳定性）的影响尚未得到充分调查。在本文中，我们重新审视了Softmax掩码的影响，并引入了一种利用其置信度保持效果的方法。在具有和不具有记忆重放的类-和任务增量学习基准测试中，我们的方法显著增加了稳定性同时保持了足够大的准确性。

    In continual learning, many classifiers use softmax function to learn confidence. However, numerous studies have pointed out its inability to accurately determine confidence distributions for outliers, often referred to as epistemic uncertainty. This inherent limitation also curtails the accurate decisions for selecting what to forget and keep in previously trained confidence distributions over continual learning process. To address the issue, we revisit the effects of masking softmax function. While this method is both simple and prevalent in literature, its implication for retaining confidence distribution during continual learning, also known as stability, has been under-investigated. In this paper, we revisit the impact of softmax masking, and introduce a methodology to utilize its confidence preservation effects. In class- and task-incremental learning benchmarks with and without memory replay, our approach significantly increases stability while maintaining sufficiently large pla
    
[^34]: 评估足球比赛预测模型：深度学习方法和梯度增强树特征优化的研究

    Evaluating Soccer Match Prediction Models: A Deep Learning Approach and Feature Optimization for Gradient-Boosted Trees. (arXiv:2309.14807v1 [cs.LG])

    [http://arxiv.org/abs/2309.14807](http://arxiv.org/abs/2309.14807)

    本研究评估了足球比赛预测模型，并采用深度学习方法和梯度增强树特征优化。研究发现，在这个特定的任务中，深度学习模型经常被忽视。

    

    机器学习模型越来越受欢迎地用于预测足球比赛结果，然而，缺乏公开的基准数据集使得模型评估变得具有挑战性。2023年足球预测挑战要求首先预测每支球队的准确进球数，其次预测胜负平的概率。竞赛提供了原始的训练集和特征，但还增加了在2023年4月4日至4月13日期间进行的额外比赛，这代表了训练集截止日期到首次预测比赛之间的时期（用于评估性能）。使用pi-ratings作为特征的CatBoost模型被应用，最初被确定为计算胜负平概率的最佳选择。值得注意的是，深度学习模型在这个特定任务中经常被忽视。

    Machine learning models have become increasingly popular for predicting the results of soccer matches, however, the lack of publicly-available benchmark datasets has made model evaluation challenging. The 2023 Soccer Prediction Challenge required the prediction of match results first in terms of the exact goals scored by each team, and second, in terms of the probabilities for a win, draw, and loss. The original training set of matches and features, which was provided for the competition, was augmented with additional matches that were played between 4 April and 13 April 2023, representing the period after which the training set ended, but prior to the first matches that were to be predicted (upon which the performance was evaluated). A CatBoost model was employed using pi-ratings as the features, which were initially identified as the optimal choice for calculating the win/draw/loss probabilities. Notably, deep learning models have frequently been disregarded in this particular task. 
    
[^35]: 转移气候变化知识

    Transferring climate change knowledge. (arXiv:2309.14780v1 [physics.ao-ph])

    [http://arxiv.org/abs/2309.14780](http://arxiv.org/abs/2309.14780)

    通过转移学习方法，研究表明机器学习，尤其是深度神经网络，可以通过充分利用地球系统模型模拟和历史观测所获得的知识，更准确地预测21世纪的全球表面温度场。

    

    准确的气候预测对于气候适应和减缓至关重要。用于预测气候变化的地球系统模型模拟在对小尺度物理过程（例如云）的表示中本质上进行了近似，这是全球平均温度对增加的温室气体浓度的响应中不确定性的根源。已经开发了多种方法，用于使用历史观测约束未来预测，并减少气候预测和气候反馈的不确定性。然而，这些方法无法捕捉气候系统固有的非线性复杂性。通过使用转移学习方法，我们展示了机器学习，特别是深度神经网络，可以用于最大程度地利用和整合从地球系统模型模拟和历史观测中获得的知识，以更准确地预测21世纪全球表面温度场。

    Accurate climate projections are required for climate adaptation and mitigation. Earth system model simulations, used to project climate change, inherently make approximations in their representation of small-scale physical processes, such as clouds, that are at the root of the uncertainties in global mean temperature's response to increased greenhouse gas concentrations. Several approaches have been developed to use historical observations to constrain future projections and reduce uncertainties in climate projections and climate feedbacks. Yet those methods cannot capture the non-linear complexity inherent in the climate system. Using a Transfer Learning approach, we show that Machine Learning, in particular Deep Neural Networks, can be used to optimally leverage and merge the knowledge gained from Earth system model simulations and historical observations to more accurately project global surface temperature fields in the 21st century. For the Shared Socioeconomic Pathways (SSPs) 2-
    
[^36]: 使用提示学习范式探索小型语言模型在高效领域特定文本分类中的应用

    Exploring Small Language Models with Prompt-Learning Paradigm for Efficient Domain-Specific Text Classification. (arXiv:2309.14779v1 [cs.CL])

    [http://arxiv.org/abs/2309.14779](http://arxiv.org/abs/2309.14779)

    本研究探索了将小型语言模型（SLMs）与提示学习范式结合应用于领域特定文本分类的潜力，并在零售业的客户和代理人交互中进行了评估。结果显示，在有限的标记数据下，SLM T5-base能够实现约75%的准确率，展现了SLMs与提示学习的潜力。

    

    面对手动标记的高成本，领域特定文本分类面临稀缺的标记数据的挑战。提示学习作为传统微调方法的替代方案，在少样本场景中表现出高效性。此外，虽然大型语言模型（LLMs）已经引起了关注，但小型语言模型（SLMs，小于10亿个参数）在领域特定任务中具有显著的定制性、适应性和成本效益，符合工业约束。本研究探讨了将SLMs与提示学习范式结合应用于领域特定文本分类的潜力，尤其是在零售业的客户和代理人交互中。我们的评估结果显示，在少样本的情况下，当可以进行基于提示的模型微调时，具有220M参数的典型SLM T5-base能够在有限的标记数据上实现约75%的准确率（达到完整数据的15%），显示出SLMs与提示学习的巨大潜力。

    Domain-specific text classification faces the challenge of scarce labeled data due to the high cost of manual labeling. Prompt-learning, known for its efficiency in few-shot scenarios, is proposed as an alternative to traditional fine-tuning methods. And besides, although large language models (LLMs) have gained prominence, small language models (SLMs, with under 1B parameters) offer significant customizability, adaptability, and cost-effectiveness for domain-specific tasks, given industry constraints. In this study, we investigate the potential of SLMs combined with prompt-learning paradigm for domain-specific text classification, specifically within customer-agent interactions in retail. Our evaluations show that, in few-shot settings when prompt-based model fine-tuning is possible, T5-base, a typical SLM with 220M parameters, achieve approximately 75% accuracy with limited labeled data (up to 15% of full data), which shows great potentials of SLMs with prompt-learning. Based on this
    
[^37]: 数据联合上的马尔科夫链镜像下降

    Markov Chain Mirror Descent On Data Federation. (arXiv:2309.14775v1 [cs.LG])

    [http://arxiv.org/abs/2309.14775](http://arxiv.org/abs/2309.14775)

    本文在联邦学习场景中提出了一种新的随机镜像下降方法MarchOn，在这种方法中，模型通过马尔科夫链进行迭代，具有最佳的收敛性能。

    

    由于计算成本低，随机优化方法如镜像下降具有广泛应用。这些方法在独立同分布的假设下得到了深入研究，并通常达到了亚线性收敛率。然而，这个假设在实际应用场景中可能过于强大和不切实际。最近的研究探究了从马尔科夫链中抽取实例时的随机梯度下降。不幸的是，对于随机镜像下降来说，知之甚少。本文中，我们在联邦学习场景中提出了一种新版本的随机镜像下降，称之为MarchOn。在一个分布式网络中，模型会从一个节点迭代地随机移动到其邻居节点之一。此外，我们还提出了一个新的框架来分析MarchOn，为凸、强凸和非凸损失提供了最佳收敛率。最后，我们进行了实证研究，评估了MarchOn的收敛性，并验证了其性能。

    Stochastic optimization methods such as mirror descent have wide applications due to low computational cost. Those methods have been well studied under assumption of the independent and identical distribution, and usually achieve sublinear rate of convergence. However, this assumption may be too strong and unpractical in real application scenarios. Recent researches investigate stochastic gradient descent when instances are sampled from a Markov chain. Unfortunately, few results are known for stochastic mirror descent. In the paper, we propose a new version of stochastic mirror descent termed by MarchOn in the scenario of the federated learning. Given a distributed network, the model iteratively travels from a node to one of its neighbours randomly. Furthermore, we propose a new framework to analyze MarchOn, which yields best rates of convergence for convex, strongly convex, and non-convex loss. Finally, we conduct empirical studies to evaluate the convergence of MarchOn, and validate 
    
[^38]: BLIP-Adapter：移动设备屏幕截图标题生成的参数高效迁移学习

    BLIP-Adapter: Parameter-Efficient Transfer Learning for Mobile Screenshot Captioning. (arXiv:2309.14774v1 [cs.LG])

    [http://arxiv.org/abs/2309.14774](http://arxiv.org/abs/2309.14774)

    本研究提出了一种参数高效的迁移学习方法，通过冻结图像标题模型的参数并只调整附加模块，解决了移动设备屏幕截图标题生成任务中大量参数的开销问题。

    

    本研究旨在探索有效的调整方法来处理屏幕截图标题生成任务。近年来，图像标题生成取得了显著进展，但是对于移动设备屏幕截图的标题生成任务的研究相对较少。目前的数据集和使用案例中对产品截屏中的用户行为的描述相对有限。因此，我们尝试对现有模型进行微调，以解决屏幕截图标题生成任务。然而，微调大型预训练模型可能消耗大量资源，需要考虑到图像标题生成模型中大量参数的时间、计算力和存储开销。为了解决这一挑战，本研究提出了一种适配器方法的组合，只需要调整模型中的附加模块。这些方法最初是为视觉或语言任务设计的，我们的意图是将它们应用于解决屏幕截图标题生成中的类似挑战。通过冻结图像标题模型的参数并训练其他模块，可以实现参数高效的迁移学习。

    This study aims to explore efficient tuning methods for the screenshot captioning task. Recently, image captioning has seen significant advancements, but research in captioning tasks for mobile screens remains relatively scarce. Current datasets and use cases describing user behaviors within product screenshots are notably limited. Consequently, we sought to fine-tune pre-existing models for the screenshot captioning task. However, fine-tuning large pre-trained models can be resource-intensive, requiring considerable time, computational power, and storage due to the vast number of parameters in image captioning models. To tackle this challenge, this study proposes a combination of adapter methods, which necessitates tuning only the additional modules on the model. These methods are originally designed for vision or language tasks, and our intention is to apply them to address similar challenges in screenshot captioning. By freezing the parameters of the image caption models and trainin
    
[^39]: 通过无人机群实现大规模物联网的年龄最小化：一种多智能体强化学习方法

    Age Minimization in Massive IoT via UAV Swarm: A Multi-agent Reinforcement Learning Approach. (arXiv:2309.14757v1 [cs.LG])

    [http://arxiv.org/abs/2309.14757](http://arxiv.org/abs/2309.14757)

    本论文通过应用多智能体深度强化学习方法，利用无人机群从物联网设备收集实时信息，以实现大规模物联网中信息的年龄最小化。研究结果表明，合作和部分合作的多智能体深度强化学习方法能够优于传统的集中式深度强化学习方法，在大规模网络中具有更好的性能表现。

    

    在许多大规模物联网通信场景中，物联网设备需要由能够靠近物联网设备并减少上行能量消耗的动态单元进行覆盖。一种强大的解决方案是部署大量无人机（无人机群）提供覆盖并为物联网网络提供更好的视线连通性。然而，研究这些具有大量服务单元的大规模物联网场景会引导出具有高复杂性的高维问题。在本文中，我们应用多智能体深度强化学习来解决由部署无人机群从物联网设备收集实时信息引起的高维问题。目标是将物联网网络中的信息年龄最小化。结果表明，合作和部分合作的多智能体深度强化学习方法能够胜过高复杂性的集中式深度强化学习方法，在大规模网络中表现出无能为力。

    In many massive IoT communication scenarios, the IoT devices require coverage from dynamic units that can move close to the IoT devices and reduce the uplink energy consumption. A robust solution is to deploy a large number of UAVs (UAV swarm) to provide coverage and a better line of sight (LoS) for the IoT network. However, the study of these massive IoT scenarios with a massive number of serving units leads to high dimensional problems with high complexity. In this paper, we apply multi-agent deep reinforcement learning to address the high-dimensional problem that results from deploying a swarm of UAVs to collect fresh information from IoT devices. The target is to minimize the overall age of information in the IoT network. The results reveal that both cooperative and partially cooperative multi-agent deep reinforcement learning approaches are able to outperform the high-complexity centralized deep reinforcement learning approach, which stands helpless in large-scale networks.
    
[^40]: 高效的多智能体深度强化学习控制与相对熵正则化

    Effective Multi-Agent Deep Reinforcement Learning Control with Relative Entropy Regularization. (arXiv:2309.14727v1 [eess.SY])

    [http://arxiv.org/abs/2309.14727](http://arxiv.org/abs/2309.14727)

    本文提出了一种新的多智能体强化学习方法，通过相对熵正则化解决了多个智能体策略更新的不一致性问题，并证明在多智能体合作和竞争任务以及传统控制任务中表现出显著的学习能力和样本效率。

    

    本文提出了一种新颖的多智能体强化学习（MARL）方法，即多智能体连续动态策略梯度（MACDPP），用于解决多个智能体控制的各种场景中存在的能力有限和样本效率问题。通过引入相对熵正则化到中心化训练与分散执行（CTDE）框架中的Actor-Critic（AC）结构，它缓解了多个智能体策略更新的不一致性。通过对多智能体合作和竞争任务以及包括OpenAI基准和机械臂操作在内的传统控制任务的评估，MACDPP在学习能力和样本效率方面相较于相关的多智能体和广泛使用的单智能体基线表现出显著优势，因此扩展了MARL在有效学习具有挑战性的控制场景中的潜力。

    In this paper, a novel Multi-agent Reinforcement Learning (MARL) approach, Multi-Agent Continuous Dynamic Policy Gradient (MACDPP) was proposed to tackle the issues of limited capability and sample efficiency in various scenarios controlled by multiple agents. It alleviates the inconsistency of multiple agents' policy updates by introducing the relative entropy regularization to the Centralized Training with Decentralized Execution (CTDE) framework with the Actor-Critic (AC) structure. Evaluated by multi-agent cooperation and competition tasks and traditional control tasks including OpenAI benchmarks and robot arm manipulation, MACDPP demonstrates significant superiority in learning capability and sample efficiency compared with both related multi-agent and widely implemented signal-agent baselines and therefore expands the potential of MARL in effectively learning challenging control scenarios.
    
[^41]: PLMM：移动设备上的个人大型模型

    PLMM: Personal Large Models on Mobile Devices. (arXiv:2309.14726v1 [cs.CV])

    [http://arxiv.org/abs/2309.14726](http://arxiv.org/abs/2309.14726)

    本文提出了一种从传统大型语言模型中提取的个人大型模型，该模型更适应于本地用户的个人信息，并且能够保护用户的隐私。该模型分为个人级别、专家级别和传统级别，同时还需要小型化以适应个人计算机或移动设备，并实现实时响应以提供更好的用户体验。

    

    在本文中，受到联邦学习的启发，我们提出了从传统大型语言模型中提取的个人大型模型，这些模型更适应本地用户的个人信息，如教育背景和爱好。我们将大型语言模型分为三个级别：个人级别，专家级别和传统级别。个人级别模型适应用户的个人信息，对用户的输入进行加密并保护其隐私。专家级别模型专注于合并特定领域的知识，如金融、IT和艺术。传统模型专注于普遍知识的发现和提升专家模型。在这样的分类中，个人模型直接与用户交互。对于整个系统来说，个人模型具有用户的（加密的）个人信息。此外，这些模型必须足够小以在个人计算机或移动设备上运行。最后，它们还必须实时响应，以提供更好的用户体验。

    Inspired by Federated Learning, in this paper, we propose personal large models that are distilled from traditional large language models but more adaptive to local users' personal information such as education background and hobbies. We classify the large language models into three levels: the personal level, expert level and traditional level. The personal level models are adaptive to users' personal information. They encrypt the users' input and protect their privacy. The expert level models focus on merging specific knowledge such as finance, IT and art. The traditional models focus on the universal knowledge discovery and upgrading the expert models. In such classifications, the personal models directly interact with the user. For the whole system, the personal models have users' (encrypted) personal information. Moreover, such models must be small enough to be performed on personal computers or mobile devices. Finally, they also have to response in real-time for better user exper
    
[^42]: QA-LoRA: 基于量化意识的大语言模型低秩适应

    QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models. (arXiv:2309.14717v1 [cs.LG])

    [http://arxiv.org/abs/2309.14717](http://arxiv.org/abs/2309.14717)

    本文提出了QA-LoRA算法，它通过使用量化意识以及组内运算符来实现大语言模型的低秩适应。QA-LoRA能够将模型权重量化以减少时间和内存的使用，同时在不损失准确性的情况下将模型集成为一个量化模型。

    

    近年来，大型语言模型（LLMs）得到了快速发展。尽管在许多语言理解任务中具有强大的能力，但沉重的计算负担在很大程度上限制了LLMs的应用，特别是当需要将它们部署到边缘设备时。本文提出了一种基于量化意识的低秩适应（QA-LoRA）算法。动机在于量化和适应的自由度不平衡，解决方案是使用组内运算符，增加量化的自由度，同时减少适应的自由度。QA-LoRA可以用几行代码轻松实现，并使原始的LoRA具备了两个能力：（i）在微调过程中，LLM的权重被量化（例如转换为INT4），以减少时间和内存的使用；（ii）经过微调后，LLM和辅助权重自然地集成到一个量化模型中，而不会损失准确性。我们将QA-LoRA应用到LLaMA和LLaMA2模型家族中。

    Recently years have witnessed a rapid development of large language models (LLMs). Despite the strong ability in many language-understanding tasks, the heavy computational burden largely restricts the application of LLMs especially when one needs to deploy them onto edge devices. In this paper, we propose a quantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies in the imbalanced degrees of freedom of quantization and adaptation, and the solution is to use group-wise operators which increase the degree of freedom of quantization meanwhile decreasing that of adaptation. QA-LoRA is easily implemented with a few lines of code, and it equips the original LoRA with two-fold abilities: (i) during fine-tuning, the LLM's weights are quantized (e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the LLM and auxiliary weights are naturally integrated into a quantized model without loss of accuracy. We apply QA-LoRA to the LLaMA and LLaMA2 model famil
    
[^43]: 通过可视化解释深度人脸算法：一项调查

    Explaining Deep Face Algorithms through Visualization: A Survey. (arXiv:2309.14715v1 [cs.CV])

    [http://arxiv.org/abs/2309.14715](http://arxiv.org/abs/2309.14715)

    本文调查了解释深度人脸算法的可视化方法，并发现了人脸网络的结构和层次结构的有价值见解，为AI从业者提供了实用人脸可视化的设计考虑因素。

    

    尽管当前的人脸任务深度模型在某些基准测试中超过了人类的表现，但我们并不了解它们是如何工作的。因此，我们无法预测它对新输入的反应，导致算法出现灾难性的失败和不希望的偏见。可解释的人工智能有助于填补这一差距，但目前针对人脸的可视化算法非常少。本研究首次进行了人脸领域可解释性算法的综述。我们探索了将通用可视化算法调整为适用于人脸领域的细微差别和注意事项，并通过对流行人脸模型进行计算可视化进行了说明。我们回顾了现有的人脸可解释性研究，并揭示了人脸网络结构和层次结构的有价值的见解。我们还通过对各种可解释性算法的实用性进行用户研究，确定了可为AI从业者提供的实用人脸可视化的设计考虑因素。

    Although current deep models for face tasks surpass human performance on some benchmarks, we do not understand how they work. Thus, we cannot predict how it will react to novel inputs, resulting in catastrophic failures and unwanted biases in the algorithms. Explainable AI helps bridge the gap, but currently, there are very few visualization algorithms designed for faces. This work undertakes a first-of-its-kind meta-analysis of explainability algorithms in the face domain. We explore the nuances and caveats of adapting general-purpose visualization algorithms to the face domain, illustrated by computing visualizations on popular face models. We review existing face explainability works and reveal valuable insights into the structure and hierarchy of face networks. We also determine the design considerations for practical face visualizations accessible to AI practitioners by conducting a user study on the utility of various explainability algorithms.
    
[^44]: 计算复杂度和形式层次的二阶循环神经网络

    On the Computational Complexity and Formal Hierarchy of Second Order Recurrent Neural Networks. (arXiv:2309.14691v1 [cs.LG])

    [http://arxiv.org/abs/2309.14691](http://arxiv.org/abs/2309.14691)

    本研究扩展了二阶循环神经网络的理论基础，并证明了存在一类有界时间的二阶RNN是图灵完备的。该模型通过将转移表编码到其循环权重中实现有界时间计算，并在识别任务上优于现代模型。

    

    已经证明具有循环和自注意力的人工神经网络(ANNs)是图灵完备的(TC)。然而，现有的工作表明，即使在权重无限制的情况下，这些ANNs也需要多次迭代或无限计算时间才能识别TC语法。然而，在固定或有界精度神经元和时间的约束下，无记忆的ANNs被证明很难识别甚至是上下文自由语言。在这项工作中，我们扩展了二阶循环网络($2^{nd}$ RNN)的理论基础，并证明存在一类有界时间的$2^{nd}$ RNN是图灵完备的。该模型能够直接将转移表编码到其循环权重中，实现有界时间计算，并且具有可解释性。我们还证明，在有界权重和时间约束下，无记忆的二阶RNNs在识别上优于现代模型，如基本RNNs和门控循环单元。

    Artificial neural networks (ANNs) with recurrence and self-attention have been shown to be Turing-complete (TC). However, existing work has shown that these ANNs require multiple turns or unbounded computation time, even with unbounded precision in weights, in order to recognize TC grammars. However, under constraints such as fixed or bounded precision neurons and time, ANNs without memory are shown to struggle to recognize even context-free languages. In this work, we extend the theoretical foundation for the $2^{nd}$-order recurrent network ($2^{nd}$ RNN) and prove there exists a class of a $2^{nd}$ RNN that is Turing-complete with bounded time. This model is capable of directly encoding a transition table into its recurrent weights, enabling bounded time computation and is interpretable by design. We also demonstrate that $2$nd order RNNs, without memory, under bounded weights and time constraints, outperform modern-day models such as vanilla RNNs and gated recurrent units in recogn
    
[^45]: 人工生成的演示是否对于上下文学习有必要？

    Are Human-generated Demonstrations Necessary for In-context Learning?. (arXiv:2309.14681v1 [cs.LG])

    [http://arxiv.org/abs/2309.14681](http://arxiv.org/abs/2309.14681)

    本文研究了上下文学习中人工生成的演示是否有必要，并提出了一种新的自反思提示策略（SEC），通过这种策略，大型语言模型（LLMs）可以自行生成演示和最终输出，避免了手动生成过程的复杂性。

    

    尽管大型语言模型（LLMs）具备良好的少样本能力，但在上下文学习（ICL）的标准范式中存在以下弊端：易受选定演示的影响，生成这些演示的复杂性。本文提出了对于ICL，人工生成的演示是否有必要的基本问题，并提出了自反思提示策略（SEC），这是一种不依赖人工演示的范例。SEC的关键点在于，不使用手工制作的示例作为ICL中的演示，而是要求LLMs首先自行创建演示，然后生成最终输出。SEC是一种灵活的框架，可适应原始ICL和“思维链”（CoT），并且更加便捷：因为可以节省示例和理由的手动生成过程。在算术推理、常识推理和多任务语言理解方面进行了大量实验。

    Despite the promising few-shot ability of large language models (LLMs), the standard paradigm of In-context Learning (ICL) suffers the disadvantages of susceptibility to selected demonstrations and the intricacy to generate these demonstrations. In this paper, we raise the fundamental question that whether human-generated demonstrations are necessary for ICL. To answer this question, we propose self-contemplation prompting strategy (SEC), a paradigm free from human-crafted demonstrations. The key point of SEC is that, instead of using hand-crafted examples as demonstrations in ICL, SEC asks LLMs to first create demonstrations on their own, based on which the final output is generated. SEC is a flexible framework and can be adapted to both the vanilla ICL and the chain-of-thought (CoT), but with greater ease: as the manual-generation process of both examples and rationale can be saved. Extensive experiments in arithmetic reasoning, commonsense reasoning, multi-task language understandin
    
[^46]: FedCompass：基于计算能力感知调度器的异构客户端设备的高效跨边界联邦学习

    FedCompass: Efficient Cross-Silo Federated Learning on Heterogeneous Client Devices using a Computing Power Aware Scheduler. (arXiv:2309.14675v1 [cs.LG])

    [http://arxiv.org/abs/2309.14675](http://arxiv.org/abs/2309.14675)

    FedCompass是一种创新的半异步联邦学习算法，通过在服务器端使用计算能力感知调度器，解决了异构客户端和数据中跨边界联邦学习的效率和收敛准确性问题。

    

    跨边界联邦学习为协同训练鲁棒且泛化的AI模型提供了有希望的解决方案，而不会损害本地数据集的隐私，如医疗、金融以及缺乏集中式数据设施的科学项目。然而，由于不同客户端（即设备异构性）之间的计算资源差异，同步联邦学习算法在等待阻塞客户端时效率下降。同样，非同分布（non-IID）异构数据集上的异步联邦学习算法由于过时的本地模型和客户端偏移，导致收敛速度和最终模型准确性降低。为了解决这些异构客户端和数据中的跨边界联邦学习的局限性，我们提出了FedCompass，这是一种具有计算能力感知调度器的创新半异步联邦学习算法。

    Cross-silo federated learning offers a promising solution to collaboratively train robust and generalized AI models without compromising the privacy of local datasets, e.g., healthcare, financial, as well as scientific projects that lack a centralized data facility. Nonetheless, because of the disparity of computing resources among different clients (i.e., device heterogeneity), synchronous federated learning algorithms suffer from degraded efficiency when waiting for straggler clients. Similarly, asynchronous federated learning algorithms experience degradation in the convergence rate and final model accuracy on non-identically and independently distributed (non-IID) heterogeneous datasets due to stale local models and client drift. To address these limitations in cross-silo federated learning with heterogeneous clients and data, we propose FedCompass, an innovative semi-asynchronous federated learning algorithm with a computing power aware scheduler on the server side, which adaptive
    
[^47]: 利用腭咽口炎数据提升基于UPTST的手足口病住院预测的准确性

    Leveraging Herpangina Data to Enhance Hospital-level Prediction of Hand-Foot-and-Mouth Disease Admissions Using UPTST. (arXiv:2309.14674v1 [cs.LG])

    [http://arxiv.org/abs/2309.14674](http://arxiv.org/abs/2309.14674)

    提出了一种新颖的基于Transformer的UPTST模型，利用腭咽口炎数据提升手足口病住院预测的准确性，且在医院级别的预测准确性上优于现有方法。

    

    手足口病（HFMD）爆发与严重的发病率和死亡率相关。因此，准确预测儿科HFMD患者的每日住院人数对于协助医院应对潜在的爆发和减少医院内传播至关重要。为了解决这一迫切需求，我们提出了一种新颖的基于Transformer的模型，它具有U-net形状，并利用了与HFMD密切相关的腭咽口炎的见解。该模型还通过引入重构损失作为辅助损失来整合表示学习。结果显示，我们的UPTST模型在医院级别的HFMD长短臂预测准确性方面优于现有方法。此外，探索性的扩展实验表明该模型的能力超出了传染病的预测，提示...

    Outbreaks of hand-foot-and-mouth disease(HFMD) have been associated with significant morbidity and, in severe cases, mortality. Accurate forecasting of daily admissions of pediatric HFMD patients is therefore crucial for aiding the hospital in preparing for potential outbreaks and mitigating nosocomial transmissions. To address this pressing need, we propose a novel transformer-based model with a U-net shape, utilizing the patching strategy and the joint prediction strategy that capitalizes on insights from herpangina, a disease closely correlated with HFMD. This model also integrates representation learning by introducing reconstruction loss as an auxiliary loss. The results show that our U-net Patching Time Series Transformer (UPTST) model outperforms existing approaches in both long- and short-arm prediction accuracy of HFMD at hospital-level. Furthermore, the exploratory extension experiments show that the model's capabilities extend beyond prediction of infectious disease, suggest
    
[^48]: ALEX: 朝向带有噪声标签的有效图传输学习

    ALEX: Towards Effective Graph Transfer Learning with Noisy Labels. (arXiv:2309.14673v1 [cs.LG])

    [http://arxiv.org/abs/2309.14673](http://arxiv.org/abs/2309.14673)

    ALEX是一种用于解决存在标签噪声的图传输学习问题的新技术，通过使用图对比学习和平衡标签分布的子图构建方法来提供稳健的节点表示。

    

    图神经网络(GNNs)因在各种图机器学习任务中的出色表现而引起了人们的广泛关注。然而，大部分基于GNN的方法都是使用完全注释的基准数据集进行研究，导致在真实世界的图学习场景中表现不佳。为了弥补这一差距，本论文研究了在存在标签噪声的情况下的图传输学习问题，该问题将知识从带有噪声的源图传输到未标记的目标图。我们引入了一种名为Balance Alignment and Information-aware Examination (ALEX)的新技术来解决这个挑战。ALEX首先使用奇异值分解生成具有关键结构语义的不同视图，利用图对比学习来提供稳健的节点表示。为了减轻标签偏移和领域偏移，我们估计一个先验分布来构建具有平衡标签分布的子图。

    Graph Neural Networks (GNNs) have garnered considerable interest due to their exceptional performance in a wide range of graph machine learning tasks. Nevertheless, the majority of GNN-based approaches have been examined using well-annotated benchmark datasets, leading to suboptimal performance in real-world graph learning scenarios. To bridge this gap, the present paper investigates the problem of graph transfer learning in the presence of label noise, which transfers knowledge from a noisy source graph to an unlabeled target graph. We introduce a novel technique termed Balance Alignment and Information-aware Examination (ALEX) to address this challenge. ALEX first employs singular value decomposition to generate different views with crucial structural semantics, which help provide robust node representations using graph contrastive learning. To mitigate both label shift and domain shift, we estimate a prior distribution to build subgraphs with balanced label distributions. Building o
    
[^49]: DONNAv2-轻量级神经体系结构搜索用于视觉任务

    DONNAv2 -- Lightweight Neural Architecture Search for Vision tasks. (arXiv:2309.14670v1 [cs.CV])

    [http://arxiv.org/abs/2309.14670](http://arxiv.org/abs/2309.14670)

    DONNAv2是一种轻量级神经体系结构搜索方法，用于视觉任务。它采用计算效率的方式探索不同学习任务的高效体系结构，并通过消除精度预测器来实现计算效率的设置。

    

    随着对视觉应用和在边缘设备上部署的需求不断增长，开发对硬件友好的体系结构并在设备部署期间保持性能变得至关重要。神经体系结构搜索（NAS）技术以计算效率的方式探索各种方法，以发现用于不同学习任务的高效体系结构。在本文中，我们提出了用于计算效率的神经体系结构蒸馏的下一代神经体系结构设计-DONNAv2。传统的NAS算法依赖于一个计算密集的阶段，在该阶段中，学习精度预测器以预估搜索空间内模型的性能。建立精度预测器有助于预测不进行微调的模型的性能。在这里，我们提出了一种优雅的方法，消除了建立精度预测器并将DONNA扩展到计算效率的设置中。形成网络的个体块的损失指标

    With the growing demand for vision applications and deployment across edge devices, the development of hardware-friendly architectures that maintain performance during device deployment becomes crucial. Neural architecture search (NAS) techniques explore various approaches to discover efficient architectures for diverse learning tasks in a computationally efficient manner. In this paper, we present the next-generation neural architecture design for computationally efficient neural architecture distillation - DONNAv2 . Conventional NAS algorithms rely on a computationally extensive stage where an accuracy predictor is learned to estimate model performance within search space. This building of accuracy predictors helps them predict the performance of models that are not being finetuned. Here, we have developed an elegant approach to eliminate building the accuracy predictor and extend DONNA to a computationally efficient setting. The loss metric of individual blocks forming the network s
    
[^50]: ZiCo-BC：一种用于视觉任务的修正偏差的零样本NAS

    ZiCo-BC: A Bias Corrected Zero-Shot NAS for Vision Tasks. (arXiv:2309.14666v1 [cs.CV])

    [http://arxiv.org/abs/2309.14666](http://arxiv.org/abs/2309.14666)

    本文针对零样本神经架构搜索（NAS）方法的主要问题进行了研究，发现现有的零样本代理存在偏差，并提出了一种名为ZiCo-BC的修正偏差的方法。实验证明ZiCo-BC在多个视觉任务上都能成功搜索出优化的架构。

    

    零样本神经架构搜索（NAS）方法提出了一种新颖的无需训练的度量标准，称为零样本代理，以大幅减少与传统基于训练的NAS相比的搜索时间。尽管在图像分类上取得了成功，但对于语义分割和目标检测等复杂视觉任务，零样本代理的有效性很少得到评估。此外，现有的零样本代理被证明对特定模型特征具有偏向性，限制了它们的广泛应用。在本文中，我们通过实证研究零样本代理ZiCo在多个视觉任务上的偏差，发现ZiCo对于更瘦更深的网络有偏差，导致了次优的架构。为了解决这个问题，我们提出了一种名为ZiCo-BC的对ZiCo进行偏差修正的新方法。我们在各种视觉任务（图像分类、目标检测和语义分割）上进行了大量实验，结果表明我们的方法可以成功搜索出优化的架构。

    Zero-Shot Neural Architecture Search (NAS) approaches propose novel training-free metrics called zero-shot proxies to substantially reduce the search time compared to the traditional training-based NAS. Despite the success on image classification, the effectiveness of zero-shot proxies is rarely evaluated on complex vision tasks such as semantic segmentation and object detection. Moreover, existing zero-shot proxies are shown to be biased towards certain model characteristics which restricts their broad applicability. In this paper, we empirically study the bias of state-of-the-art (SOTA) zero-shot proxy ZiCo across multiple vision tasks and observe that ZiCo is biased towards thinner and deeper networks, leading to sub-optimal architectures. To solve the problem, we propose a novel bias correction on ZiCo, called ZiCo-BC. Our extensive experiments across various vision tasks (image classification, object detection and semantic segmentation) show that our approach can successfully sear
    
[^51]: 基于Transformer的医学咨询用户查询分类与专家特长相关的研究

    Tranformer-based classification of user queries for medical consultancy with respect to expert specialisation. (arXiv:2309.14662v1 [cs.LG])

    [http://arxiv.org/abs/2309.14662](http://arxiv.org/abs/2309.14662)

    本研究利用RuBERT模型和Transformer技术，提出了一种用于医学咨询的用户查询分类方法，重点关注专家特长，表现出超过92%的性能，具有良好的泛化性能和实际应用价值。

    

    在数字医疗时代，对于熟练的医疗支持的需求正在增长。本研究提出了一种创新策略，利用RuBERT模型，将医学咨询领域的用户查询进行分类，并着重关注专家的特长。通过利用Transformer模型的能力，我们在多样化的数据集上对预训练的RuBERT模型进行微调，实现了查询与特定医学专长之间的精确对应。通过使用全面的数据集，我们证明了我们的方法在交叉验证和传统的测试和训练集划分下均具有优秀的性能，F1得分超过92%。我们的方法在心脏病学、神经病学和皮肤科等医学领域的泛化性能也非常出色。这种方法提供了实际益处，可以将用户引导至适当的专家以获得及时而有针对性的医疗建议。它还提高了医疗系统的效率，减少了从业者的负担。

    The need for skilled medical support is growing in the era of digital healthcare. This research presents an innovative strategy, utilising the RuBERT model, for categorising user inquiries in the field of medical consultation with a focus on expert specialisation. By harnessing the capabilities of transformers, we fine-tuned the pre-trained RuBERT model on a varied dataset, which facilitates precise correspondence between queries and particular medical specialisms. Using a comprehensive dataset, we have demonstrated our approach's superior performance with an F1-score of over 92%, calculated through both cross-validation and the traditional split of test and train datasets. Our approach has shown excellent generalisation across medical domains such as cardiology, neurology and dermatology. This methodology provides practical benefits by directing users to appropriate specialists for prompt and targeted medical advice. It also enhances healthcare system efficiency, reduces practitioner 
    
[^52]: 学习通过集合成员身份确定控制动态的不确定性集合: 非渐近分析

    Learning the Uncertainty Sets for Control Dynamics via Set Membership: A Non-Asymptotic Analysis. (arXiv:2309.14648v1 [math.OC])

    [http://arxiv.org/abs/2309.14648](http://arxiv.org/abs/2309.14648)

    本文提供了对于线性动力系统中通过集合成员身份估计产生的不确定性集合直径的非渐近界限，并将结果应用于鲁棒自适应模型预测控制。通过数值实验证明了鲁棒自适应控制器的快速接近离线最优模型预测控制器的性能。

    

    集合成员身份估计广泛应用于需要对模型不确定性集合具有鲁棒性的自适应/基于学习的控制算法，例如在线鲁棒稳定控制和鲁棒自适应模型预测控制。尽管具有广泛的应用，但随机环境下的非渐近估计误差界限有限。本文在有界、独立同分布干扰下提供了线性动态系统中由集合成员身份估计产生的不确定性集合直径的非渐近界限。此外，将该结果应用于由集合成员身份更新的鲁棒自适应模型预测控制。通过数值实验证明了鲁棒自适应控制器的性能，该控制器与离线最优模型预测控制器的性能快速接近，相对于基于最小二乘估计置信区域的控制设计。

    Set-membership estimation is commonly used in adaptive/learning-based control algorithms that require robustness over the model uncertainty sets, e.g., online robustly stabilizing control and robust adaptive model predictive control. Despite having broad applications, non-asymptotic estimation error bounds in the stochastic setting are limited. This paper provides such a non-asymptotic bound on the diameter of the uncertainty sets generated by set membership estimation on linear dynamical systems under bounded, i.i.d. disturbances. Further, this result is applied to robust adaptive model predictive control with uncertainty sets updated by set membership. We numerically demonstrate the performance of the robust adaptive controller, which rapidly approaches the performance of the offline optimal model predictive controller, in comparison with the control design based on least square estimation's confidence regions.
    
[^53]: 深度强化学习交易代理的灰盒对抗攻击

    Gray-box Adversarial Attack of Deep Reinforcement Learning-based Trading Agents. (arXiv:2309.14615v1 [cs.LG])

    [http://arxiv.org/abs/2309.14615](http://arxiv.org/abs/2309.14615)

    本研究展示了一种通过在同一股票市场进行交易的方式，利用灰盒方法对基于深度强化学习的交易代理进行攻击的可能性。这种方法可以应对交易代理受到对手操纵的问题。

    

    近年来，深度强化学习（Deep RL）已成功应用于诸如复杂游戏、自动驾驶汽车和聊天机器人等许多系统中，其中一个有趣的应用案例是将其作为自动化股票交易代理。一般来说，任何自动化交易代理都容易受到交易环境中的对手的操纵，因此研究其鲁棒性对于其实践成功至关重要。然而，用于研究RL鲁棒性的典型机制，即基于白盒梯度基础的对抗样本生成技术（如FGSM），对于这种用例来说已经过时，因为模型受到安全的国际交易所API的保护，如纳斯达克。在这项研究中，我们证明了一种“灰盒”方法可以攻击基于Deep RL的交易代理，仅通过在同一股票市场进行交易，而无需额外接触交易代理。在我们提出的方法中，对手代理使用了一个混合的深度神经网络

    In recent years, deep reinforcement learning (Deep RL) has been successfully implemented as a smart agent in many systems such as complex games, self-driving cars, and chat-bots. One of the interesting use cases of Deep RL is its application as an automated stock trading agent. In general, any automated trading agent is prone to manipulations by adversaries in the trading environment. Thus studying their robustness is vital for their success in practice. However, typical mechanism to study RL robustness, which is based on white-box gradient-based adversarial sample generation techniques (like FGSM), is obsolete for this use case, since the models are protected behind secure international exchange APIs, such as NASDAQ. In this research, we demonstrate that a "gray-box" approach for attacking a Deep RL-based trading agent is possible by trading in the same stock market, with no extra access to the trading agent. In our proposed approach, an adversary agent uses a hybrid Deep Neural Netwo
    
[^54]: 重参数化变分拒绝采样

    Reparameterized Variational Rejection Sampling. (arXiv:2309.14612v1 [stat.ML])

    [http://arxiv.org/abs/2309.14612](http://arxiv.org/abs/2309.14612)

    本文提出了一种重参数化变分拒绝采样（VRS）方法，通过将参数化的提议分布与拒绝采样结合，定义了一个丰富的非参数分布族，明确利用已知的目标分布，为具有连续潜变量的模型提供了一种吸引人的推断策略。

    

    传统的变分推断方法依赖于参数化的变分分布族，选择的分布族在确定后验近似的准确性方面起着关键作用。简单的mean-field分布族通常导致较差的近似，而像归一化流这样的丰富分布族往往难以优化，并且通常不包含已知目标分布的结构，因为其是黑箱的。为了扩展灵活的变分分布族空间，我们重新考虑变分拒绝采样（VRS）[Grover et al., 2018]，它将参数化提议分布与拒绝采样结合起来，定义了一个丰富的非参数分布族，明确利用已知的目标分布。通过引入对提议分布参数的低方差重参数化梯度估计器，我们使VRS成为具有连续潜变量的吸引人的推断策略。

    Traditional approaches to variational inference rely on parametric families of variational distributions, with the choice of family playing a critical role in determining the accuracy of the resulting posterior approximation. Simple mean-field families often lead to poor approximations, while rich families of distributions like normalizing flows can be difficult to optimize and usually do not incorporate the known structure of the target distribution due to their black-box nature. To expand the space of flexible variational families, we revisit Variational Rejection Sampling (VRS) [Grover et al., 2018], which combines a parametric proposal distribution with rejection sampling to define a rich non-parametric family of distributions that explicitly utilizes the known target distribution. By introducing a low-variance reparameterized gradient estimator for the parameters of the proposal distribution, we make VRS an attractive inference strategy for models with continuous latent variables.
    
[^55]: 无监督的图深度学习揭示了城市地区突发洪水风险概况

    Unsupervised Graph Deep Learning Reveals Emergent Flood Risk Profile of Urban Areas. (arXiv:2309.14610v1 [cs.LG])

    [http://arxiv.org/abs/2309.14610](http://arxiv.org/abs/2309.14610)

    本研究基于无监督图深度学习模型提出了集成城市洪水风险评级模型，能够捕捉区域之间的空间依赖关系和洪水危险与城市要素之间的复杂相互作用，揭示了城市地区的突发洪水风险概况

    

    城市洪水风险源于与洪水危险、洪水暴露以及社会和物理脆弱性相关的多个要素之间的复杂和非线性相互作用，以及复杂的空间洪水依赖关系。然而，现有的用于表征城市洪水风险的方法主要是基于洪水平原地图，侧重于有限数量的要素，主要是危险和暴露要素，没有考虑要素之间的相互作用或空间区域之间的依赖关系。为了填补这一空白，本研究提出了一种基于新颖的无监督图深度学习模型（称为FloodRisk-Net）的集成城市洪水风险评级模型。FloodRisk-Net能够捕捉区域之间的空间依赖关系以及洪水危险和城市要素之间的复杂和非线性相互作用，从而确定突发洪水风险。利用美国多个都市统计区（MSAs）的数据，该模型将它们的洪水风险特征化为

    Urban flood risk emerges from complex and nonlinear interactions among multiple features related to flood hazard, flood exposure, and social and physical vulnerabilities, along with the complex spatial flood dependence relationships. Existing approaches for characterizing urban flood risk, however, are primarily based on flood plain maps, focusing on a limited number of features, primarily hazard and exposure features, without consideration of feature interactions or the dependence relationships among spatial areas. To address this gap, this study presents an integrated urban flood-risk rating model based on a novel unsupervised graph deep learning model (called FloodRisk-Net). FloodRisk-Net is capable of capturing spatial dependence among areas and complex and nonlinear interactions among flood hazards and urban features for specifying emergent flood risk. Using data from multiple metropolitan statistical areas (MSAs) in the United States, the model characterizes their flood risk into
    
[^56]: 神经视觉化: 基于自动编码器的损失地形可视化方法

    Neuro-Visualizer: An Auto-encoder-based Loss Landscape Visualization Method. (arXiv:2309.14601v1 [cs.LG])

    [http://arxiv.org/abs/2309.14601](http://arxiv.org/abs/2309.14601)

    Neuro-Visualizer是一种基于自动编码器的非线性地形可视化方法，可以更好地表示高维损失地形，通过对机器学习中的问题进行实验，证实其优于其他方法，并提供有关神经网络损失地形的有用见解。

    

    在近年来，越来越多的研究者对神经网络的损失地形进行可视化感兴趣。线性的地形可视化方法，例如主成分分析，已经被广泛使用，因为它们直观地帮助研究者研究神经网络及其训练过程。然而，这些线性方法由于缺乏灵活性和低保真度以表示高维地形而受到限制和缺陷。在本文中，我们提出了一种新颖的基于自动编码器的非线性地形可视化方法Neuro-Visualizer，它解决了这些缺点，并提供了关于神经网络损失地形的有用见解。为了证明其潜力，我们在两个知识引导机器学习（KGML）应用的各种问题上进行了实验。我们的发现表明，Neuro-Visualizer优于其他线性和非线性基准，并有助于证实，并有时对机器学习提出的主张提出质疑。

    In recent years, there has been a growing interest in visualizing the loss landscape of neural networks. Linear landscape visualization methods, such as principal component analysis, have become widely used as they intuitively help researchers study neural networks and their training process. However, these linear methods suffer from limitations and drawbacks due to their lack of flexibility and low fidelity at representing the high dimensional landscape. In this paper, we present a novel auto-encoder-based non-linear landscape visualization method called Neuro-Visualizer that addresses these shortcoming and provides useful insights about neural network loss landscapes. To demonstrate its potential, we run experiments on a variety of problems in two separate applications of knowledge-guided machine learning (KGML). Our findings show that Neuro-Visualizer outperforms other linear and non-linear baselines and helps corroborate, and sometime challenge, claims proposed by machine learning 
    
[^57]: 在连续控制中的噪声邻域中的策略优化

    Policy Optimization in a Noisy Neighborhood: On Return Landscapes in Continuous Control. (arXiv:2309.14597v1 [cs.LG])

    [http://arxiv.org/abs/2309.14597](http://arxiv.org/abs/2309.14597)

    本论文提供了新的视角，研究了连续控制中深度强化学习智能体性能不稳定的原因。通过对回报景观进行分析，发现了策略空间中的失败区域和策略品质的隐藏维度。此外，提出了一种分布感知的方法，改善了策略的鲁棒性。

    

    已知连续控制问题中的深度强化学习智能体在性能上会出现显著的不稳定性。本文从研究策略和回报之间的映射即回报景观的角度为这些行为提供了新的视角。我们发现，流行的算法在这个景观的噪声邻域中穿行，一个策略参数的单次更新会导致回报在很大范围内变化。通过对这些回报进行分布处理，我们对景观进行了映射，描述了策略空间中容易产生失败的区域，并揭示了策略品质的隐藏维度。我们还展示了景观的惊人结构，通过在参数空间中找到简单的路径来提高策略的稳定性。最后，我们开发了一个分布感知的方法，通过避开噪声邻域来提高策略的鲁棒性。综上所述，我们的结果为优化和评估提供了新的洞察。

    Deep reinforcement learning agents for continuous control are known to exhibit significant instability in their performance over time. In this work, we provide a fresh perspective on these behaviors by studying the return landscape: the mapping between a policy and a return. We find that popular algorithms traverse noisy neighborhoods of this landscape, in which a single update to the policy parameters leads to a wide range of returns. By taking a distributional view of these returns, we map the landscape, characterizing failure-prone regions of policy space and revealing a hidden dimension of policy quality. We show that the landscape exhibits surprising structure by finding simple paths in parameter space which improve the stability of a policy. To conclude, we develop a distribution-aware procedure which finds such paths, navigating away from noisy neighborhoods in order to improve the robustness of a policy. Taken together, our results provide new insight into the optimization, eva
    
[^58]: 使用FP8格式的高效后训练量化方法

    Efficient Post-training Quantization with FP8 Formats. (arXiv:2309.14592v1 [cs.LG])

    [http://arxiv.org/abs/2309.14592](http://arxiv.org/abs/2309.14592)

    本研究研究了FP8格式在后训练量化中的优势，并开发了一个通用的量化工作流程。实验结果表明，FP8相对于INT8具有更好的工作负载覆盖率和模型准确性。

    

    最近深度学习方法的进展，如LLMs和扩散模型，提出了对改进的量化方法的需求，以满足这些现代架构的计算需求，同时保持准确性。为了实现这一目标，我们研究了FP8数据格式在75个不同网络架构上进行后训练量化的优势，这些网络架构涵盖了多种任务，包括机器翻译、语言建模、文本生成、图像分类、生成和分割。我们研究了三种不同的FP8表示（E5M2、E4M3和E3M4），以研究在动态范围和精度之间不同权衡程度对模型准确性的影响。基于我们的广泛研究，我们开发了一个量化工作流程，可以概括适用于不同的网络架构。我们的实证结果表明，FP8格式在多个方面优于INT8，包括工作负载覆盖率（92.64％对65.87％）、模型准确性和适用于更广泛的操作范围。

    Recent advances in deep learning methods such as LLMs and Diffusion models have created a need for improved quantization methods that can meet the computational demands of these modern architectures while maintaining accuracy. Towards this goal, we study the advantages of FP8 data formats for post-training quantization across 75 unique network architectures covering a wide range of tasks, including machine translation, language modeling, text generation, image classification, generation, and segmentation. We examine three different FP8 representations (E5M2, E4M3, and E3M4) to study the effects of varying degrees of trade-off between dynamic range and precision on model accuracy. Based on our extensive study, we developed a quantization workflow that generalizes across different network architectures. Our empirical results show that FP8 formats outperform INT8 in multiple aspects, including workload coverage (92.64% vs. 65.87%), model accuracy and suitability for a broader range of ope
    
[^59]: 应用顺序学习的医学图像分类

    Applications of Sequential Learning for Medical Image Classification. (arXiv:2309.14591v1 [eess.IV])

    [http://arxiv.org/abs/2309.14591](http://arxiv.org/abs/2309.14591)

    该论文介绍了一种应用顺序学习的医学图像分类方法，通过开发神经网络训练框架对少量的医学图像数据进行持续训练，并提供了评估训练的启发式方法。通过解决过拟合、灾难性遗忘和概念漂移等问题，该方法能够有效地进行医学图像分类。

    

    目的：本研究的目标是开发一个神经网络训练框架，用于对少量医学图像数据进行持续训练，并创建在缺乏验证集或测试集的情况下评估训练的启发式方法。材料和方法：我们制定了一个回顾性的顺序学习方法，该方法可以随着时间的推移对医学图像的小批量进行训练和持续更新模型。我们通过PyTorch卷积神经网络（CNN）和公开可用的医学MNIST和NIH胸部X射线成像数据集来解决顺序学习中出现的过拟合、灾难性遗忘和概念漂移等问题。我们首先比较了两种顺序训练的CNN方法：有基础预训练和无基础预训练。然后，我们转向两种独特的训练和验证数据招募方法，以估计完整信息的提取而不会过拟合。最后，我们考虑了一个显示我们的方法如何看待现实生活数据的示例。

    Purpose: The aim of this work is to develop a neural network training framework for continual training of small amounts of medical imaging data and create heuristics to assess training in the absence of a hold-out validation or test set.  Materials and Methods: We formulated a retrospective sequential learning approach that would train and consistently update a model on mini-batches of medical images over time. We address problems that impede sequential learning such as overfitting, catastrophic forgetting, and concept drift through PyTorch convolutional neural networks (CNN) and publicly available Medical MNIST and NIH Chest X-Ray imaging datasets. We begin by comparing two methods for a sequentially trained CNN with and without base pre-training. We then transition to two methods of unique training and validation data recruitment to estimate full information extraction without overfitting. Lastly, we consider an example of real-life data that shows how our approach would see mainstre
    
[^60]: 目标导向语义通信的联合通信和计算框架，具有畸变率鲁棒性

    Joint Communication and Computation Framework for Goal-Oriented Semantic Communication with Distortion Rate Resilience. (arXiv:2309.14587v1 [cs.LG])

    [http://arxiv.org/abs/2309.14587](http://arxiv.org/abs/2309.14587)

    本论文提出了一个创新的联合通信和计算框架，利用率畸变理论来分析通信和语义压缩引起的畸变，从而评估其对目标导向语义通信中人工智能模型性能的影响，使目标导向语义通信问题成为可能。

    

    最近关于语义通信的研究主要考虑准确性作为优化目标导向通信系统的主要问题。然而，这些方法引入了一个悖论：人工智能任务的准确性应该通过训练自然地出现，而不是由网络约束所决定。鉴于这个困境，本文引入了一种创新的方法，利用率畸变理论来分析由通信和语义压缩引起的畸变，并分析学习过程。具体来说，我们研究了原始数据和畸变数据之间的分布偏移，从而评估其对人工智能模型性能的影响。基于这个分析，我们可以预先估计人工智能任务的实际准确性，使目标导向语义通信问题变得可行。为了实现这个目标，我们提出了我们方法的理论基础，并进行了模拟和实验。

    Recent research efforts on semantic communication have mostly considered accuracy as a main problem for optimizing goal-oriented communication systems. However, these approaches introduce a paradox: the accuracy of artificial intelligence (AI) tasks should naturally emerge through training rather than being dictated by network constraints. Acknowledging this dilemma, this work introduces an innovative approach that leverages the rate-distortion theory to analyze distortions induced by communication and semantic compression, thereby analyzing the learning process. Specifically, we examine the distribution shift between the original data and the distorted data, thus assessing its impact on the AI model's performance. Founding upon this analysis, we can preemptively estimate the empirical accuracy of AI tasks, making the goal-oriented semantic communication problem feasible. To achieve this objective, we present the theoretical foundation of our approach, accompanied by simulations and ex
    
[^61]: DifAttack: 基于解耦特征空间的高效黑盒攻击方法

    DifAttack: Query-Efficient Black-Box Attack via Disentangled Feature Space. (arXiv:2309.14585v1 [cs.CV])

    [http://arxiv.org/abs/2309.14585](http://arxiv.org/abs/2309.14585)

    DifAttack是一种基于解耦特征空间的高效黑盒攻击方法，通过迭代优化对抗特征并利用受害者模型的查询反馈生成成功的对抗样本。

    

    本研究研究了具有高攻击成功率和良好泛化能力的高效基于分数的黑盒对抗攻击。我们设计了一种基于解耦特征空间的新型攻击方法，称为DifAttack，与现有方法在整个特征空间上操作有显著差异。具体而言，DifAttack首先将图像的潜在特征解耦为对抗特征和视觉特征，其中前者主导图像的对抗能力，而后者主要决定其视觉外观。我们使用由可用替代模型通过白盒攻击方法生成的干净图像和其对抗性样本来训练自动编码器进行解耦。最终，DifAttack根据受害者模型的查询反馈，迭代优化对抗特征，直到成功生成对抗样本，同时保持视觉特征不变。此外，由于避免使用...

    This work investigates efficient score-based black-box adversarial attacks with a high Attack Success Rate (ASR) and good generalizability. We design a novel attack method based on a Disentangled Feature space, called DifAttack, which differs significantly from the existing ones operating over the entire feature space. Specifically, DifAttack firstly disentangles an image's latent feature into an adversarial feature and a visual feature, where the former dominates the adversarial capability of an image, while the latter largely determines its visual appearance. We train an autoencoder for the disentanglement by using pairs of clean images and their Adversarial Examples (AEs) generated from available surrogate models via white-box attack methods. Eventually, DifAttack iteratively optimizes the adversarial feature according to the query feedback from the victim model until a successful AE is generated, while keeping the visual feature unaltered. In addition, due to the avoidance of using
    
[^62]: CWCL：连续加权对比损失下的跨模态迁移

    CWCL: Cross-Modal Transfer with Continuously Weighted Contrastive Loss. (arXiv:2309.14580v1 [cs.LG])

    [http://arxiv.org/abs/2309.14580](http://arxiv.org/abs/2309.14580)

    本文提出了一种称为CWCL的损失函数，用于跨模态迁移中的对比训练。相比于传统的二进制对比训练，CWCL使用连续的相似性度量，可以更好地对齐实例的表示并提高跨模态的迁移性能。

    

    本文考虑使用对比训练进行跨模态零样本迁移，其中一个模态中的预训练模型用于在另一个领域中进行表示学习，使用成对数据。然后，后一个领域中学到的模型可以以一种零样本的方式用于各种任务，类似于最近引起相当关注的“对比语言-图像预训练（CLIP）”和“锁定图像调整（LiT）”。大多数现有的跨模态表示对齐方法（包括CLIP和LiT）使用标准的对比训练目标，它使用一组正样本和负样本来对齐相似和驱散不相似的训练数据样本。然而，训练样本之间的相似性具有更连续的性质，因此需要更“非二进制”的处理。为了解决这个问题，我们提出了一种称为连续加权对比损失（CWCL）的新型损失函数，它使用连续的相似度测量。使用CWCL，我们旨在对齐实例的表示并提高跨模态的迁移性能。

    This paper considers contrastive training for cross-modal 0-shot transfer wherein a pre-trained model in one modality is used for representation learning in another domain using pairwise data. The learnt models in the latter domain can then be used for a diverse set of tasks in a zero-shot way, similar to ``Contrastive Language-Image Pre-training (CLIP)'' and ``Locked-image Tuning (LiT)'' that have recently gained considerable attention. Most existing works for cross-modal representation alignment (including CLIP and LiT) use the standard contrastive training objective, which employs sets of positive and negative examples to align similar and repel dissimilar training data samples. However, similarity amongst training examples has a more continuous nature, thus calling for a more `non-binary' treatment. To address this, we propose a novel loss function called Continuously Weighted Contrastive Loss (CWCL) that employs a continuous measure of similarity. With CWCL, we seek to align the e
    
[^63]: 将高阶动态和道路合规性整合到基于约束的ILQR轨迹规划中的自动驾驶车辆中

    Integrating Higher-Order Dynamics and Roadway-Compliance into Constrained ILQR-based Trajectory Planning for Autonomous Vehicles. (arXiv:2309.14566v1 [cs.RO])

    [http://arxiv.org/abs/2309.14566](http://arxiv.org/abs/2309.14566)

    本文将高阶动态和道路合规性整合到基于约束的ILQR轨迹规划中的自动驾驶车辆中，提供了更安全和舒适的路径规划。

    

    本文解决了自动乘客车辆（APV）的道路轨迹规划领域的进展。轨迹规划旨在为APV生成全局最优的路径，考虑到诸如车辆动力学、约束和检测到的障碍物等各种因素。传统技术涉及采样方法与优化算法的组合，前者确保全局感知，后者优化局部最优解。值得注意的是，约束迭代线性二次调节器（CILQR）优化算法最近出现，针对APV系统进行了改进，强调提高安全性和舒适度。然而，使用车辆自行车运动模型的现有实现可能无法保证可控的轨迹。我们通过加入高阶项，包括曲率和纵向加速度的一阶和二阶导数，来增强这个模型。这种包含有助于在成本和约束设计中获得更丰富的表达。

    This paper addresses the advancements in on-road trajectory planning for Autonomous Passenger Vehicles (APV). Trajectory planning aims to produce a globally optimal route for APVs, considering various factors such as vehicle dynamics, constraints, and detected obstacles. Traditional techniques involve a combination of sampling methods followed by optimization algorithms, where the former ensures global awareness and the latter refines for local optima. Notably, the Constrained Iterative Linear Quadratic Regulator (CILQR) optimization algorithm has recently emerged, adapted for APV systems, emphasizing improved safety and comfort. However, existing implementations utilizing the vehicle bicycle kinematic model may not guarantee controllable trajectories. We augment this model by incorporating higher-order terms, including the first and second-order derivatives of curvature and longitudinal jerk. This inclusion facilitates a richer representation in our cost and constraint design. We also
    
[^64]: 使用混合深度学习进行认知数字供应链双胞胎的打乱检测

    Disruption Detection for a Cognitive Digital Supply Chain Twin Using Hybrid Deep Learning. (arXiv:2309.14557v1 [cs.LG])

    [http://arxiv.org/abs/2309.14557](http://arxiv.org/abs/2309.14557)

    本文介绍了一种使用混合深度学习的方法，用于在认知数字供应链双胞胎框架中进行打乱检测，以增强供应链的韧性。所提出的方法在实时的打乱情况下，能够帮助决策者和供应链实践者做出适当的决策，以最小化打乱事件的负面影响。

    

    目的：近期的打乱事件，如COVID-19和俄乌冲突，对全球供应链产生了重大影响。数字供应链双胞胎被提出，以为决策者提供有效和高效的工具来减轻打乱影响。方法：本文介绍了一种混合深度学习方法，用于在认知数字供应链双胞胎框架中进行打乱检测，以增强供应链的韧性。所提出的打乱检测模块利用深度自编码器神经网络结合一类支持向量机算法。此外，开发了长短时记忆神经网络模型，用于识别受到打乱影响的阶层并预测恢复时间。结果：所提出方法获取的信息将帮助决策者和供应链实践者根据实时的打乱情况做出适当的决策，以最小化打乱事件的负面影响。

    Purpose: Recent disruptive events, such as COVID-19 and Russia-Ukraine conflict, had a significant impact of global supply chains. Digital supply chain twins have been proposed in order to provide decision makers with an effective and efficient tool to mitigate disruption impact. Methods: This paper introduces a hybrid deep learning approach for disruption detection within a cognitive digital supply chain twin framework to enhance supply chain resilience. The proposed disruption detection module utilises a deep autoencoder neural network combined with a one-class support vector machine algorithm. In addition, long-short term memory neural network models are developed to identify the disrupted echelon and predict time-to-recovery from the disruption effect. Results: The obtained information from the proposed approach will help decision-makers and supply chain practitioners make appropriate decisions aiming at minimizing negative impact of disruptive events based on real-time disruption 
    
[^65]: 稳定放置的外部接触块的触觉估计

    Tactile Estimation of Extrinsic Contact Patch for Stable Placement. (arXiv:2309.14552v1 [cs.RO])

    [http://arxiv.org/abs/2309.14552](http://arxiv.org/abs/2309.14552)

    本文介绍了一种利用触觉读数推测物体放置稳定性的方法，通过对接触区域的估计可以有效设计机器人的反馈技能，提高机器人的精细操控能力。

    

    对于机器人的精细操作技能来说，准确感知接触交互至关重要。本文提出了一种为机器人设计反馈技能的方法，该机器人必须学习将复杂形状的物体堆叠在一起。为了设计这样一个系统，机器人应该能够根据非常轻微的接触交互来推理放置的稳定性。我们的实验结果表明，可以根据接触形成过程中的触觉读数来推测物体放置的稳定性。具体而言，我们使用力和触觉观测来估计抓取物体和其环境之间的接触区域，从而估计接触形成过程中物体的稳定性。这种接触区域可以用来估计释放抓取后物体的稳定性。所提出的方法在一款非常流行的棋盘游戏中使用了多种物体对进行了验证。

    Precise perception of contact interactions is essential for the fine-grained manipulation skills for robots. In this paper, we present the design of feedback skills for robots that must learn to stack complex-shaped objects on top of each other. To design such a system, a robot should be able to reason about the stability of placement from very gentle contact interactions. Our results demonstrate that it is possible to infer the stability of object placement based on tactile readings during contact formation between the object and its environment. In particular, we estimate the contact patch between a grasped object and its environment using force and tactile observations to estimate the stability of the object during a contact formation. The contact patch could be used to estimate the stability of the object upon the release of the grasp. The proposed method is demonstrated on various pairs of objects that are used in a very popular board game.
    
[^66]: 光链路中的窃听识别和定位的基于聚类的方法

    Cluster-based Method for Eavesdropping Identification and Localization in Optical Links. (arXiv:2309.14541v1 [stat.ML])

    [http://arxiv.org/abs/2309.14541](http://arxiv.org/abs/2309.14541)

    本文提出了一种基于聚类的方法，用于在光线系统中检测和定位小功率损失的窃听事件。研究结果表明，通过光性能监测数据可以检测这种微小的窃听损失，同时通过在线数据可以有效地定位这类事件。

    

    我们提出了一种基于聚类的方法，用于检测和定位光线系统中小功率损失的窃听事件。我们的研究结果表明，光性能监测（OPM）数据仅通过接收器收集就可以检测到这种微小的窃听损失。另一方面，通过利用在线OPM数据可以有效地实现对这类事件的定位。

    We propose a cluster-based method to detect and locate eavesdropping events in optical line systems characterized by small power losses. Our findings indicate that detecting such subtle losses from eavesdropping can be accomplished solely through optical performance monitoring (OPM) data collected at the receiver. On the other hand, the localization of such events can be effectively achieved by leveraging in-line OPM data.
    
[^67]: 圆环设计对道路使用者行为的影响：基于无监督机器学习应用的圆环案例研究

    Effect of roundabout design on the behavior of road users: A case study of roundabouts with application of Unsupervised Machine Learning. (arXiv:2309.14540v1 [cs.LG])

    [http://arxiv.org/abs/2309.14540](http://arxiv.org/abs/2309.14540)

    本研究通过无监督机器学习应用的圆环案例研究，评估了圆环的性能，并研究了人类驾驶员与圆环的互动行为。研究发现，圆环可以显著降低转弯路口的速度，而其对速度的影响取决于道路使用者的行为评级。对于巴士、汽车和卡车驾驶员的行为进行了分类，并开发出一种预测圆环交叉口道路使用者行为的方法。安全主要归功于圆环的两个固有特征。

    

    本研究旨在评估圆环的性能并研究人类驾驶员与圆环的互动行为。近年来，由于其安全性、容量和环境优势以及为过境和整合提供安全和流畅车辆流动，圆环在国家间的应用越来越多。结果表明，圆环可以显著降低转弯路口的速度、入口速度以及其对速度的影响取决于道路使用者的行为评级。在我们的研究中，（巴士、汽车、卡车）驾驶员受到特别关注，并将其行为分为保守型、正常型和侵略型。预测和识别驾驶员行为是一个重要的挑战。因此，本研究旨在研究圆环对这些分类器的影响，并开发一种预测圆环交叉口道路使用者行为的方法。安全主要归功于圆环的两个固有特征。

    This research aims to evaluate the performance of the rotors and study the behavior of the human driver in interacting with the rotors. In recent years, rotors have been increasingly used between countries due to their safety, capacity, and environmental advantages, and because they provide safe and fluid flows of vehicles for transit and integration. It turns out that roundabouts can significantly reduce speed at twisting intersections, entry speed and the resulting effect on speed depends on the rating of road users. In our research, (bus, car, truck) drivers were given special attention and their behavior was categorized into (conservative, normal, aggressive). Anticipating and recognizing driver behavior is an important challenge. Therefore, the aim of this research is to study the effect of roundabouts on these classifiers and to develop a method for predicting the behavior of road users at roundabout intersections. Safety is primarily due to two inherent features of the rotor. Fi
    
[^68]: Detach-ROCKET: 基于随机卷积核的时间序列分类中的顺序特征选择

    Detach-ROCKET: Sequential feature selection for time series classification with random convolutional kernels. (arXiv:2309.14518v1 [cs.LG])

    [http://arxiv.org/abs/2309.14518](http://arxiv.org/abs/2309.14518)

    本文提出了一种名为Detach-ROCKET的方法，用于时间序列分类中的顺序特征选择。通过利用随机卷积核模型中的大量特征，并使用顺序特征分离方法剪枝非主要特征，提高了模型的可扩展性和泛化能力。

    

    时间序列分类在许多领域中都是必不可少的，如医学、金融、环境科学和制造业，可以实现疾病诊断、异常检测和股价预测等任务。尽管循环神经网络和InceptionTime等机器学习模型在许多应用中取得了成功，但由于训练需求的繁重，可能会面临可扩展性限制。为了解决这个问题，出现了Rocket及其衍生模型等随机卷积核模型，通过利用从时间序列数据中随机生成的大量特征，简化训练并实现最先进的性能。然而，由于其随机性质，生成的大部分特征是冗余或非信息性的，增加了不必要的计算负载并损害了泛化能力。在这里，我们引入了顺序特征分离（SFD）作为一种识别和修剪这些非主要特征的方法。SFD利用模型系数来估计特征的重要性。

    Time series classification is essential in many fields, such as medicine, finance, environmental science, and manufacturing, enabling tasks like disease diagnosis, anomaly detection, and stock price prediction. Machine learning models like Recurrent Neural Networks and InceptionTime, while successful in numerous applications, can face scalability limitations due to intensive training requirements. To address this, random convolutional kernel models such as Rocket and its derivatives have emerged, simplifying training and achieving state-of-the-art performance by utilizing a large number of randomly generated features from time series data. However, due to their random nature, most of the generated features are redundant or non-informative, adding unnecessary computational load and compromising generalization. Here, we introduce Sequential Feature Detachment (SFD) as a method to identify and prune these non-essential features. SFD uses model coefficients to estimate feature importance a
    
[^69]: DeepSpeed Ulysses：用于训练极长序列Transformer模型的系统优化

    DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models. (arXiv:2309.14509v1 [cs.LG])

    [http://arxiv.org/abs/2309.14509](http://arxiv.org/abs/2309.14509)

    本论文介绍了DeepSpeed-Ulysses，一种用于实现具备极长序列长度的高效可扩展LLM训练的新颖方法。

    

    传统的基于Transformer的大型语言模型（LLM）的计算可以通过批量大小、隐藏维度、层数和序列长度来描述。到目前为止，加速LLM训练的系统工作主要集中在前三个维度上：批量大小的数据并行化、隐藏尺寸的张量并行化以及模型深度或层数的流水线并行化。这些被广泛研究的并行形式并不针对长序列Transformer模型进行优化。鉴于长序列LLM在实际应用需求上的重要性，序列并行化引起了重新关注。然而，现有的序列并行化工作受到内存通信效率的限制，限制了它们在长序列大模型上的可扩展性。在这项工作中，我们引入了DeepSpeed-Ulysses，一种新颖、便携且有效的方法，用于实现具备极长序列长度的高效可扩展LLM训练。

    Computation in a typical Transformer-based large language model (LLM) can be characterized by batch size, hidden dimension, number of layers, and sequence length. Until now, system works for accelerating LLM training have focused on the first three dimensions: data parallelism for batch size, tensor parallelism for hidden size and pipeline parallelism for model depth or layers. These widely studied forms of parallelism are not targeted or optimized for long sequence Transformer models. Given practical application needs for long sequence LLM, renewed attentions are being drawn to sequence parallelism. However, existing works in sequence parallelism are constrained by memory-communication inefficiency, limiting their scalability to long sequence large models. In this work, we introduce DeepSpeed-Ulysses, a novel, portable and effective methodology for enabling highly efficient and scalable LLM training with extremely long sequence length. DeepSpeed-Ulysses at its core partitions input da
    
[^70]: 粒子加速器的不确定性感知深度学习

    Uncertainty Aware Deep Learning for Particle Accelerators. (arXiv:2309.14502v1 [cs.LG])

    [http://arxiv.org/abs/2309.14502](http://arxiv.org/abs/2309.14502)

    本文针对粒子加速器的问题，提出了一种不确定性感知的深度学习方法。通过实施距离感知不确定性估计，可以在输入样本与训练数据不相似时检测并提供预测的可信度。该方法在Spallation Neutron Source (SNS)加速器的错误束流预测（分类）和Fermi National Accelerator Lab (FNAL) Booster Accelerator Complex（回归）的代理模型中取得了良好的效果。

    

    标准的深度学习模型对于捕捉复杂系统动态非常理想，但是当输入样本与训练数据不相似时，它们的预测可能会任意不准确。通过实现距离感知不确定性估计，可以检测这些情况并提供与预测相关的信心水平。在本文中，我们使用深度高斯过程近似（DGPA）方法来预测Spallation Neutron Source (SNS)加速器的错误束流（分类），并为Fermi National Accelerator Lab (FNAL) Booster Accelerator Complex（回归）提供不确定性感知的代理模型。

    Standard deep learning models for classification and regression applications are ideal for capturing complex system dynamics. However, their predictions can be arbitrarily inaccurate when the input samples are not similar to the training data. Implementation of distance aware uncertainty estimation can be used to detect these scenarios and provide a level of confidence associated with their predictions. In this paper, we present results from using Deep Gaussian Process Approximation (DGPA) methods for errant beam prediction at Spallation Neutron Source (SNS) accelerator (classification) and we provide an uncertainty aware surrogate model for the Fermi National Accelerator Lab (FNAL) Booster Accelerator Complex (regression).
    
[^71]: Era Splitting.（arXiv:2309.14496v1 [cs.LG]）

    Era Splitting. (arXiv:2309.14496v1 [cs.LG])

    [http://arxiv.org/abs/2309.14496](http://arxiv.org/abs/2309.14496)

    本研究提出了两种新的分裂准则，使得决策树模型能够利用时代信息进行优化，从而将超分布泛化研究中的思想应用于决策树模型。

    

    现实生活中的机器学习问题在时间和空间上会呈现出数据的分布变化。这种行为超出了传统的经验风险最小化范式的范围，该范式假设数据在时间和地点上是独立同分布的。新兴的超分布泛化领域通过将环境或时代信息融入算法中，来应对这个现实。迄今为止，大部分研究都集中在线性模型和/或神经网络上。在本研究中，我们针对决策树模型，包括随机森林和梯度提升决策树，开发了两种新的分裂准则，使得树模型能够利用与每个数据点相关的时代信息，来找到在数据的所有不相交时代中都是最优的切分点，从而将超分布泛化研究中的思想应用于决策树模型。

    Real life machine learning problems exhibit distributional shifts in the data from one time to another or from on place to another. This behavior is beyond the scope of the traditional empirical risk minimization paradigm, which assumes i.i.d. distribution of data over time and across locations. The emerging field of out-of-distribution (OOD) generalization addresses this reality with new theory and algorithms which incorporate environmental, or era-wise information into the algorithms. So far, most research has been focused on linear models and/or neural networks. In this research we develop two new splitting criteria for decision trees, which allow us to apply ideas from OOD generalization research to decision tree models, including random forest and gradient-boosting decision trees. The new splitting criteria use era-wise information associated with each data point to allow tree-based models to find split points that are optimal across all disjoint eras in the data, instead of optim
    
[^72]: 使用角度明可夫斯基$p$-距离分类标记频率

    Classifying token frequencies using angular Minkowski $p$-distance. (arXiv:2309.14495v1 [cs.LG])

    [http://arxiv.org/abs/2309.14495](http://arxiv.org/abs/2309.14495)

    使用角度明可夫斯基$p$-距离可以获得比传统余弦相似度更高的分类性能

    

    角度明可夫斯基$p$-距离是一种使用明可夫斯基$p$-距离替代余弦相似度定义的不相似度测量方法。余弦相似度经常用于包含标记频率的数据集中，而角度明可夫斯基$p$-距离可能在某些任务中是更好的选择。我们在基于20-newsgroups数据集的案例研究中，评估了传统加权最近邻和模糊粗糙最近邻的分类性能。此外，我们还分析了超参数$p$，数据集维数$m$，邻居数量$k$，权重选择和分类器选择之间的关系。我们得出结论：使用合适的$p$值，使用角度明可夫斯基$p$-距离可以获得比传统余弦相似度更高的分类性能。

    Angular Minkowski $p$-distance is a dissimilarity measure that is obtained by replacing Euclidean distance in the definition of cosine dissimilarity with other Minkowski $p$-distances. Cosine dissimilarity is frequently used with datasets containing token frequencies, and angular Minkowski $p$-distance may potentially be an even better choice for certain tasks. In a case study based on the 20-newsgroups dataset, we evaluate clasification performance for classical weighted nearest neighbours, as well as fuzzy rough nearest neighbours. In addition, we analyse the relationship between the hyperparameter $p$, the dimensionality $m$ of the dataset, the number of neighbours $k$, the choice of weights and the choice of classifier. We conclude that it is possible to obtain substantially higher classification performance with angular Minkowski $p$-distance with suitable values for $p$ than with classical cosine dissimilarity.
    
[^73]: 可解释和准确的语音助手及其它领域的自然语言理解

    Explainable and Accurate Natural Language Understanding for Voice Assistants and Beyond. (arXiv:2309.14485v1 [cs.LG])

    [http://arxiv.org/abs/2309.14485](http://arxiv.org/abs/2309.14485)

    这项研究改进了联合NLU模型的准确性，并使其可解释，同时扩展适用于其他分类任务。

    

    联合意图检测和槽填充，也称为联合自然语言理解（NLU），对于智能语音助手非常重要。最近在这个领域的进展主要集中在使用各种技术提高准确性上。可解释性无疑是基于深度学习模型的模型的一个重要方面，包括联合NLU模型。如果没有解释性，它们的决策对外界来说是不透明的，因此容易缺乏用户的信任。因此，为了弥合这一差距，我们将完整的联合NLU模型转化为在细粒度上“内在地”可解释的，同时不影响准确性。此外，通过使完整的联合NLU模型具有可解释性，我们展示了我们的扩展可以成功应用于其他一般分类任务。我们使用情感分析和命名实体识别来证明这一点。

    Joint intent detection and slot filling, which is also termed as joint NLU (Natural Language Understanding) is invaluable for smart voice assistants. Recent advancements in this area have been heavily focusing on improving accuracy using various techniques. Explainability is undoubtedly an important aspect for deep learning-based models including joint NLU models. Without explainability, their decisions are opaque to the outside world and hence, have tendency to lack user trust. Therefore to bridge this gap, we transform the full joint NLU model to be `inherently' explainable at granular levels without compromising on accuracy. Further, as we enable the full joint NLU model explainable, we show that our extension can be successfully used in other general classification tasks. We demonstrate this using sentiment analysis and named entity recognition.
    
[^74]: 揭示深度学习模型在近边区域太阳耀斑预测中的潜力

    Unveiling the Potential of Deep Learning Models for Solar Flare Prediction in Near-Limb Regions. (arXiv:2309.14483v1 [astro-ph.SR])

    [http://arxiv.org/abs/2309.14483](http://arxiv.org/abs/2309.14483)

    本研究评估了使用深度学习模型预测近边区域太阳耀斑的性能，发现基于AlexNet的模型表现出最佳预测敏感性，可为太阳耀斑预测提供潜在的解决方案。

    

    本研究旨在评估深度学习模型在预测24小时内$\geq$M级太阳耀斑的性能，使用每小时采样的全球视线磁团图像，在特别关注常被忽视的位于近边区域（太阳盘的$\pm$70$^{\circ}$之外）对应的耀斑事件。我们使用迁移学习训练了三种知名的深度学习架构--AlexNet、VGG16和ResNet34，并使用真实技能统计值（TSS）和Heidke技能评分（HSS）比较和评估了我们模型的整体性能，并计算了对X类和M类耀斑的中央和近边区域的预测敏感性的召回率。以下是我们研究的主要发现概括：（1）基于AlexNet的模型表现出最高的整体性能，平均TSS为0.53，平均HSS为0.37；（2）此外，召回率的空间分析揭示，对于太阳耀斑的中央和近边区域，AlexNet模型展现出较高的预测敏感性。

    This study aims to evaluate the performance of deep learning models in predicting $\geq$M-class solar flares with a prediction window of 24 hours, using hourly sampled full-disk line-of-sight (LoS) magnetogram images, particularly focusing on the often overlooked flare events corresponding to the near-limb regions (beyond $\pm$70$^{\circ}$ of the solar disk). We trained three well-known deep learning architectures--AlexNet, VGG16, and ResNet34 using transfer learning and compared and evaluated the overall performance of our models using true skill statistics (TSS) and Heidke skill score (HSS) and computed recall scores to understand the prediction sensitivity in central and near-limb regions for both X- and M-class flares. The following points summarize the key findings of our study: (1) The highest overall performance was observed with the AlexNet-based model, which achieved an average TSS$\sim$0.53 and HSS$\sim$0.37; (2) Further, a spatial analysis of recall scores disclosed that for
    
[^75]: LogGPT：通过GPT进行日志异常检测

    LogGPT: Log Anomaly Detection via GPT. (arXiv:2309.14482v1 [cs.LG])

    [http://arxiv.org/abs/2309.14482](http://arxiv.org/abs/2309.14482)

    本论文提出了LogGPT，它是一个使用GPT进行日志异常检测的框架。通过语言建模和强化学习策略，LogGPT能够有效地检测系统日志中的异常情况。

    

    基于日志数据的系统异常检测对于确保计算机系统的安全性和可靠性非常重要。近年来，深度学习模型已被广泛用于日志异常检测。核心思想是将日志序列建模为自然语言，并采用深度时序模型（如LSTM或Transformer）通过语言建模来编码日志序列中的正常模式。然而，语言建模与异常检测之间存在差距，因为通过语言建模损失训练时序模型的目标与异常检测不直接相关。为填补这一差距，我们提出了LogGPT，这是一个使用GPT进行日志异常检测的新框架。首先，我们训练LogGPT根据前序序列预测下一个日志条目。为了进一步提高LogGPT的性能，我们提出了一种新的强化学习策略，专门为日志异常检测任务微调模型。在三个数据集上的实验结果表明。

    Detecting system anomalies based on log data is important for ensuring the security and reliability of computer systems. Recently, deep learning models have been widely used for log anomaly detection. The core idea is to model the log sequences as natural language and adopt deep sequential models, such as LSTM or Transformer, to encode the normal patterns in log sequences via language modeling. However, there is a gap between language modeling and anomaly detection as the objective of training a sequential model via a language modeling loss is not directly related to anomaly detection. To fill up the gap, we propose LogGPT, a novel framework that employs GPT for log anomaly detection. LogGPT is first trained to predict the next log entry based on the preceding sequence. To further enhance the performance of LogGPT, we propose a novel reinforcement learning strategy to finetune the model specifically for the log anomaly detection task. The experimental results on three datasets show tha
    
[^76]: 为连续强化学习适应双Q-Learning

    Adapting Double Q-Learning for Continuous Reinforcement Learning. (arXiv:2309.14471v1 [cs.LG])

    [http://arxiv.org/abs/2309.14471](http://arxiv.org/abs/2309.14471)

    本文介绍了一种新颖的校正偏差方法，通过使用两个组件的混合策略并由分开的网络进行评估，消除了离线策略强化学习算法中的过高估计偏差。在一小组MuJoCo环境中，该方法显示出了有希望接近SOTA的结果。 (校正偏差方法，混合策略，分开的网络评估)

    

    大部分离线策略强化学习算法使用过高估计偏差控制技术。大多数这些技术基于启发式方法，主要解决的是过高估计的结果，而非其根本原因。本文提出了一种新颖的校正偏差的方法，类似于双Q-Learning。我们提出使用一种由两个组成成分构成的混合策略。每个策略成分由分别最大化和评估的网络处理，从而消除了过高估计偏差的基础。我们的方法在一小组MuJoCo环境上展示了令人期待的接近SOTA的结果。

    Majority of off-policy reinforcement learning algorithms use overestimation bias control techniques. Most of these techniques rooted in heuristics, primarily addressing the consequences of overestimation rather than its fundamental origins. In this work we present a novel approach to the bias correction, similar in spirit to Double Q-Learning. We propose using a policy in form of a mixture with two components. Each policy component is maximized and assessed by separate networks, which removes any basis for the overestimation bias. Our approach shows promising near-SOTA results on a small set of MuJoCo environments.
    
[^77]: FARSEC:一种可重现的基于交通摄像头的自动实时车辆速度估计框架

    FARSEC: A Reproducible Framework for Automatic Real-Time Vehicle Speed Estimation Using Traffic Cameras. (arXiv:2309.14468v1 [cs.CV])

    [http://arxiv.org/abs/2309.14468](http://arxiv.org/abs/2309.14468)

    FARSEC是一种可重现的基于交通摄像头的自动实时车辆速度估计框架，可以应对不同数据集的鲁棒性问题，并通过估计道路段长度的新颖技术提高了准确性。

    

    利用交通摄像头估计车辆速度是交通监控和管理的关键任务，可以实现更优化的交通流量、改善道路安全和降低环境影响。现有的研究在这个领域报告了具有竞争力的准确性水平，但其解决方案在不同数据集上缺乏可重现性和鲁棒性。为了解决这个问题，我们提供了一种新颖的框架，用于自动实时车辆速度计算，能够处理来自公开交通摄像头的更多种类的数据，从而实现更强的鲁棒性。我们的模型采用了新颖的技术，通过深度图预测来估计道路段的长度。此外，我们的框架能够自动处理实际条件，如摄像机运动和不同的视频流输入。我们将我们的模型与三个知名模型进行了比较。

    Estimating the speed of vehicles using traffic cameras is a crucial task for traffic surveillance and management, enabling more optimal traffic flow, improved road safety, and lower environmental impact. Transportation-dependent systems, such as for navigation and logistics, have great potential to benefit from reliable speed estimation. While there is prior research in this area reporting competitive accuracy levels, their solutions lack reproducibility and robustness across different datasets. To address this, we provide a novel framework for automatic real-time vehicle speed calculation, which copes with more diverse data from publicly available traffic cameras to achieve greater robustness. Our model employs novel techniques to estimate the length of road segments via depth map prediction. Additionally, our framework is capable of handling realistic conditions such as camera movements and different video stream inputs automatically. We compare our model to three well-known models i
    
[^78]: DefGoalNet：可变形物体操作时的上下文目标学习

    DefGoalNet: Contextual Goal Learning from Demonstrations For Deformable Object Manipulation. (arXiv:2309.14463v1 [cs.RO])

    [http://arxiv.org/abs/2309.14463](http://arxiv.org/abs/2309.14463)

    本文提出了一种名为DefGoalNet的神经网络，通过少量人类示范直接学习可变形物体的目标形状。实验证明，在各种机器人任务中取得了显著的进展。

    

    形状伺服是一种控制物体到达预期目标形状的机器人任务，对于可变形物体的操作具有潜在的应用前景。然而，目标形状的规定成为一个问题。目标形状通常通过繁琐的领域知识工程过程获取，或者通过手动操作物体到达所需形状并捕获该特定时刻的目标形状，这两种方法在各种机器人应用中都不切实际。本文通过开发一种新颖的神经网络DefGoalNet来解决这个问题，该网络可以直接从少量人类示范中学习可变形物体的目标形状。我们在模拟和实际机器人上展示了我们方法的有效性。值得注意的是，在手术撤退任务中，即使仅使用10个示范进行训练，我们的方法的成功率中值也接近90%。这些结果标志着在可变形物体操作中的重要进展。

    Shape servoing, a robotic task dedicated to controlling objects to desired goal shapes, is a promising approach to deformable object manipulation. An issue arises, however, with the reliance on the specification of a goal shape. This goal has been obtained either by a laborious domain knowledge engineering process or by manually manipulating the object into the desired shape and capturing the goal shape at that specific moment, both of which are impractical in various robotic applications. In this paper, we solve this problem by developing a novel neural network DefGoalNet, which learns deformable object goal shapes directly from a small number of human demonstrations. We demonstrate our method's effectiveness on various robotic tasks, both in simulation and on a physical robot. Notably, in the surgical retraction task, even when trained with as few as 10 demonstrations, our method achieves a median success percentage of nearly 90%. These results mark a substantial advancement in enabl
    
[^79]: Skilog：滑雪跳台运动中的智能传感器系统用于性能分析和生物反馈

    Skilog: A Smart Sensor System for Performance Analysis and Biofeedback in Ski Jumping. (arXiv:2309.14455v1 [eess.SP])

    [http://arxiv.org/abs/2309.14455](http://arxiv.org/abs/2309.14455)

    该论文介绍了一种用于滑雪跳台运动中的智能传感器系统，可以在实时中进行性能分析和生物反馈。通过测量滑雪靴鞋垫上的脚压力，系统可以提供给教练改善反馈和运动员即时的行动反馈。

    

    在滑雪跳台运动中，跳跃的重复率低限制了训练的有效性。因此，提高每次单独跳跃的学习率是成功的关键。运动员训练的关键因素之一是运动学习，研究表明反馈方法可以加速运动学习。特别是在速度上坡时，对重心的精细控制至关重要。这是因为实际起跳发生在眨眼间（约300毫秒），因此在速度上坡阶段任何不平衡的身体姿势都会影响飞行。本文提出了一种智能、紧凑、节能的无线传感器系统，用于滑雪跳台运动中的实时性能分析和生物反馈。该系统通过在滑雪靴鞋垫上的三个不同点测量脚压力，以100Hz的频率进行操作。脚压力数据可以直接发送给教练以改善反馈，或者输入到机器学习模型中，使用滑雪靴中的振动马达给运动员提供即时的行动反馈。

    In ski jumping, low repetition rates of jumps limit the effectiveness of training. Thus, increasing learning rate within every single jump is key to success. A critical element of athlete training is motor learning, which has been shown to be accelerated by feedback methods. In particular, a fine-grained control of the center of gravity in the in-run is essential. This is because the actual takeoff occurs within a blink of an eye ($\sim$300ms), thus any unbalanced body posture during the in-run will affect flight. This paper presents a smart, compact, and energy-efficient wireless sensor system for real-time performance analysis and biofeedback during ski jumping. The system operates by gauging foot pressures at three distinct points on the insoles of the ski boot at 100Hz. Foot pressure data can either be directly sent to coaches to improve their feedback, or fed into a ML model to give athletes instantaneous in-action feedback using a vibration motor in the ski boot. In the biofeedba
    
[^80]: 从大规模分子动力学模拟中学习位错动力学的迁移规律

    Learning dislocation dynamics mobility laws from large-scale MD simulations. (arXiv:2309.14450v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2309.14450](http://arxiv.org/abs/2309.14450)

    本文介绍了一种机器学习框架，用于从大规模分子动力学模拟中学习位错动力学的迁移规律。通过将迁移规律建模为图神经网络，并在BCC钨上进行大规模离散位错动力学模拟，我们证明了我们的方法能够准确地重现具有挑战性拉伸/压缩行为。

    

    离散位错动力学（DDD）的计算方法被用作真正的原子尺度晶格位错动力学的粗粒化模型，它已经成为研究由位错的集体行为引起的金属塑性的强大工具。作为一种介观尺度的方法，DDD模型中位错的运动通过迁移规律来预定；迁移规律是指位错线应如何响应驱动力的函数。然而，传统的手工制作的迁移规律的开发可能是一个麻烦的任务，可能涉及具有负面影响的简化。在这里，我们介绍了一种机器学习（ML）框架，以简化数据驱动的迁移规律的开发，这些规律被模拟为在大规模分子动力学（MD）模拟的晶体塑性上训练的图神经网络（GNN）。我们以BCC钨为例说明了我们的方法，并证明了我们在大规模DDS模拟中实现的GNN迁移规律能够准确地重现具有挑战性的拉伸/压缩行为。

    The computational method of discrete dislocation dynamics (DDD), used as a coarse-grained model of true atomistic dynamics of lattice dislocations, has become of powerful tool to study metal plasticity arising from the collective behavior of dislocations. As a mesoscale approach, motion of dislocations in the DDD model is prescribed via the mobility law; a function which specifies how dislocation lines should respond to the driving force. However, the development of traditional hand-crafted mobility laws can be a cumbersome task and may involve detrimental simplifications. Here we introduce a machine-learning (ML) framework to streamline the development of data-driven mobility laws which are modeled as graph neural networks (GNN) trained on large-scale Molecular Dynamics (MD) simulations of crystal plasticity. We illustrate our approach on BCC tungsten and demonstrate that our GNN mobility implemented in large-scale DDD simulations accurately reproduces the challenging tension/compress
    
[^81]: 自恢复提示：基于基础模型和自恢复的通用服务机器人系统

    Self-Recovery Prompting: Promptable General Purpose Service Robot System with Foundation Models and Self-Recovery. (arXiv:2309.14425v1 [cs.RO])

    [http://arxiv.org/abs/2309.14425](http://arxiv.org/abs/2309.14425)

    本文研究开发了一个通用服务机器人系统，该系统可以根据不同任务和环境的变化进行自适应，并通过自恢复机制解决信息不足、计划生成错误和执行失败等问题，实现了任务的成功完成。

    

    通用服务机器人（GPSR）能够在各种环境中执行多种任务，需要一个具有高通用性和适应性的系统来应对不同的任务和环境。本文首先基于多个基础模型开发了一个顶层GPSR系统，用于全球竞赛（RoboCup@Home 2023）。该系统既可以适应多种变化，又可以通过提示每个模型来实现自适应。然后，通过分析所开发系统的性能，我们发现在更加现实的GPSR应用设置中存在三种失败类型：信息不足、错误的计划生成和计划执行失败。我们提出了自恢复提示管道，该管道探索必要的信息，并修改其提示来从失败中恢复。我们通过实验证实，具有自恢复机制的系统可以通过解决各种失败案例来完成任务。供补充的视频可在https://sites.google.com/view/srgpsr上找到。

    A general-purpose service robot (GPSR), which can execute diverse tasks in various environments, requires a system with high generalizability and adaptability to tasks and environments. In this paper, we first developed a top-level GPSR system for worldwide competition (RoboCup@Home 2023) based on multiple foundation models. This system is both generalizable to variations and adaptive by prompting each model. Then, by analyzing the performance of the developed system, we found three types of failure in more realistic GPSR application settings: insufficient information, incorrect plan generation, and plan execution failure. We then propose the self-recovery prompting pipeline, which explores the necessary information and modifies its prompts to recover from failure. We experimentally confirm that the system with the self-recovery mechanism can accomplish tasks by resolving various failure cases. Supplementary videos are available at https://sites.google.com/view/srgpsr .
    
[^82]: 关于嵌入式量子核的表达能力

    On the expressivity of embedding quantum kernels. (arXiv:2309.14419v1 [quant-ph])

    [http://arxiv.org/abs/2309.14419](http://arxiv.org/abs/2309.14419)

    量子核方法是量子和经典机器学习之间最自然的联系之一。本文探讨了嵌入式量子核的表达能力，并得出结论：通过引入计算普适性，任何核函数都可以表示为量子特征映射和嵌入式量子核。

    

    在核方法的背景下，量子核与经典机器学习之间建立了最自然的联系。核方法依赖于内积特征向量，这些特征向量存在于大型特征空间中。量子核通常通过显式构造量子特征态并计算它们的内积来评估，这里称为嵌入式量子核。由于经典核通常在不使用特征向量的情况下进行评估，我们想知道嵌入式量子核的表达能力如何。在这项工作中，我们提出了一个基本问题：是否所有的量子核都可以表达为量子特征态的内积？我们的第一个结果是肯定的：通过调用计算普适性，我们发现对于任何核函数，总是存在对应的量子特征映射和嵌入式量子核。然而，问题更关注的是有效的构造方式。在第二部分中

    One of the most natural connections between quantum and classical machine learning has been established in the context of kernel methods. Kernel methods rely on kernels, which are inner products of feature vectors living in large feature spaces. Quantum kernels are typically evaluated by explicitly constructing quantum feature states and then taking their inner product, here called embedding quantum kernels. Since classical kernels are usually evaluated without using the feature vectors explicitly, we wonder how expressive embedding quantum kernels are. In this work, we raise the fundamental question: can all quantum kernels be expressed as the inner product of quantum feature states? Our first result is positive: Invoking computational universality, we find that for any kernel function there always exists a corresponding quantum feature map and an embedding quantum kernel. The more operational reading of the question is concerned with efficient constructions, however. In a second part
    
[^83]: 基于核的量子学习器和基于Grover算法的量子预处理的可证明优势

    Provable advantages of kernel-based quantum learners and quantum preprocessing based on Grover's algorithm. (arXiv:2309.14406v1 [quant-ph])

    [http://arxiv.org/abs/2309.14406](http://arxiv.org/abs/2309.14406)

    本研究展示了基于核的量子学习器和基于Grover算法的量子预处理的可证明优势，通过应用于模式匹配问题，提供了实际可行的优势，并结合经典分类方法进一步提高分类器性能。

    

    目前正在努力寻找量子学习问题的量子加速。最近，刘逸等人在自然物理杂志上证明了量子支持向量机利用Shor算法的加速可以实现指数级加速。我们在这个结果的基础上进行扩展，利用Grover算法在支持向量机的核中实现加速。为了展示核结构的实用性，我们将其应用于与模式匹配相关的问题，提供了实际可行且可证明的优势。此外，我们还展示了在预处理步骤中结合量子计算和经典分类方法可以进一步提高分类器性能。

    There is an ongoing effort to find quantum speedups for learning problems. Recently, [Y. Liu et al., Nat. Phys. $\textbf{17}$, 1013--1017 (2021)] have proven an exponential speedup for quantum support vector machines by leveraging the speedup of Shor's algorithm. We expand upon this result and identify a speedup utilizing Grover's algorithm in the kernel of a support vector machine. To show the practicality of the kernel structure we apply it to a problem related to pattern matching, providing a practical yet provable advantage. Moreover, we show that combining quantum computation in a preprocessing step with classical methods for classification further improves classifier performance.
    
[^84]: pLMFPPred: 一种准确预测功能肽的新方法，整合了预训练蛋白质语言模型的嵌入和不平衡学习

    pLMFPPred: a novel approach for accurate prediction of functional peptides integrating embedding from pre-trained protein language model and imbalanced learning. (arXiv:2309.14404v1 [q-bio.QM])

    [http://arxiv.org/abs/2309.14404](http://arxiv.org/abs/2309.14404)

    该论文提出了一种名为pLMFPPred的新方法，通过结合预训练蛋白质语言模型的嵌入和不平衡学习技术，准确预测功能肽。实验结果表明，pLMFPPred在预测功能肽方面优于现有的方法。

    

    功能肽具有治疗各种疾病的潜力。它们具有良好的治疗效果和低毒性，是理想的治疗药物。基于人工智能的计算策略可以帮助从蛋白质序列集合中快速识别新的功能肽，并发现它们的不同功能。本文使用基于蛋白质语言模型的嵌入（ESM-2）开发了一个名为pLMFPPred（基于蛋白质语言模型的功能肽预测工具），用于预测功能肽并识别有毒肽。同时，为了缓解数据不平衡问题并降低计算成本，我们引入了SMOTE-TOMEK数据合成采样和Shapley值基于特征选择技术。在经过验证的独立测试集上，pLMFPPred的准确率、接受者操作特征曲线下面积和F1得分分别达到0.974、0.99和0.974。对比实验表明，pLMFPPred在预测功能肽方面优于当前的方法。

    Functional peptides have the potential to treat a variety of diseases. Their good therapeutic efficacy and low toxicity make them ideal therapeutic agents. Artificial intelligence-based computational strategies can help quickly identify new functional peptides from collections of protein sequences and discover their different functions.Using protein language model-based embeddings (ESM-2), we developed a tool called pLMFPPred (Protein Language Model-based Functional Peptide Predictor) for predicting functional peptides and identifying toxic peptides. We also introduced SMOTE-TOMEK data synthesis sampling and Shapley value-based feature selection techniques to relieve data imbalance issues and reduce computational costs. On a validated independent test set, pLMFPPred achieved accuracy, Area under the curve - Receiver Operating Characteristics, and F1-Score values of 0.974, 0.99, and 0.974, respectively. Comparative experiments show that pLMFPPred outperforms current methods for predicti
    
[^85]: DECORAIT - 用于AI训练的去中心化选择加入/退出注册表

    DECORAIT -- DECentralized Opt-in/out Registry for AI Training. (arXiv:2309.14400v1 [cs.CR])

    [http://arxiv.org/abs/2309.14400](http://arxiv.org/abs/2309.14400)

    DECORAIT是一个去中心化的注册表，为内容创作者提供选择加入或退出AI训练的权利，并通过组合分层聚类和on/off-chain存储的方式来追踪GenAI训练数据的来源，以确定训练同意并奖励贡献该数据的创意人。

    

    我们介绍了DECORAIT：一个去中心化的注册表，通过该注册表，内容创作者可以表明他们选择加入或退出AI训练的权利，并为他们的贡献获得奖励。生成式AI（GenAI）使用在从公共来源中抓取的大量数据上训练的AI模型合成图像。这对于希望公开分享其作品而不认可其用于训练的模型和内容创作者来说，是一个数据治理的挑战。此外，确定GenAI训练数据的来源对于创意人来说是重要的，以确保对他们的使用得到公平的认可和奖励。我们报告了DECORAIT的原型，该原型通过探索分层聚类和on/off-chain存储的组合来创建一个可扩展的去中心化注册表，以追踪GenAI训练数据的来源，以确定训练同意并奖励贡献该数据的创意人。DECORAIT将分布式账本技术（DLT）与视觉指纹识别相结合。

    We present DECORAIT; a decentralized registry through which content creators may assert their right to opt in or out of AI training as well as receive reward for their contributions. Generative AI (GenAI) enables images to be synthesized using AI models trained on vast amounts of data scraped from public sources. Model and content creators who may wish to share their work openly without sanctioning its use for training are thus presented with a data governance challenge. Further, establishing the provenance of GenAI training data is important to creatives to ensure fair recognition and reward for their such use. We report a prototype of DECORAIT, which explores hierarchical clustering and a combination of on/off-chain storage to create a scalable decentralized registry to trace the provenance of GenAI training data in order to determine training consent and reward creatives who contribute that data. DECORAIT combines distributed ledger technology (DLT) with visual fingerprinting, lever
    
[^86]: 看见和听到没被说的话：可解释融合的多模态动机性访谈客户行为分类器

    Seeing and hearing what has not been said; A multimodal client behavior classifier in Motivational Interviewing with interpretable fusion. (arXiv:2309.14398v1 [cs.LG])

    [http://arxiv.org/abs/2309.14398](http://arxiv.org/abs/2309.14398)

    本文提出了一个多模态分类器，在动机性访谈中准确区分了变化话语、持续话语和跟随/中立话语三种类别。该分类器利用文本、声调、面部表情和身体表现等多模态特征，并对AnnoMI数据集进行了注释和训练。研究还找到了决策过程中最重要的模态，提供了宝贵的洞察。

    

    动机性访谈（MI）是一种强调合作并鼓励行为改变的治疗方法。为了评估MI对话的质量，可以利用MISC代码将客户话语分类为变化话语、持续话语或跟随/中立话语。MI对话中变化话语的比例与治疗结果呈正相关，因此准确分类客户话语至关重要。本文提出了一个分类器，利用文本、声调、面部表情和身体表现等多模态特征准确区分三个MISC类别（变化话语、持续话语和跟随/中立话语）。为了训练我们的模型，我们对公开可用的AnnoMI数据集进行注释，收集了文本、音频、面部表情和身体表现等多模态信息。此外，我们还确定了决策过程中最重要的模态，提供了宝贵的洞察。

    Motivational Interviewing (MI) is an approach to therapy that emphasizes collaboration and encourages behavioral change. To evaluate the quality of an MI conversation, client utterances can be classified using the MISC code as either change talk, sustain talk, or follow/neutral talk. The proportion of change talk in a MI conversation is positively correlated with therapy outcomes, making accurate classification of client utterances essential. In this paper, we present a classifier that accurately distinguishes between the three MISC classes (change talk, sustain talk, and follow/neutral talk) leveraging multimodal features such as text, prosody, facial expressivity, and body expressivity. To train our model, we perform annotations on the publicly available AnnoMI dataset to collect multimodal information, including text, audio, facial expressivity, and body expressivity. Furthermore, we identify the most important modalities in the decision-making process, providing valuable insights i
    
[^87]: 利用机器学习预测环境对乳腺癌的影响

    Predicting environment effects on breast cancer by implementing machine learning. (arXiv:2309.14397v1 [cs.LG])

    [http://arxiv.org/abs/2309.14397](http://arxiv.org/abs/2309.14397)

    乳腺癌的发展不仅与遗传因素有关，环境因素也起着重要作用。本研究综述了影响乳腺癌风险、发病率和结局的各种环境因素，包括生活方式决策和环境污染物。

    

    乳腺癌逐渐成为造成女性死亡的主要因素，超过了心脏病。虽然遗传因素在乳腺癌的发展中很重要，但新的研究表明环境因素在其发生和发展中也起着重要作用。本研究报告对可能影响乳腺癌风险、发病率和结局的各种环境因素进行了全面的综述。研究从生活方式决策开始，如饮食习惯、运动习惯和饮酒，这些因素可能影响激素失衡和炎症，这是乳腺癌发展的两个重要因素。此外，本研究还探讨了农药、内分泌干扰物（EDCs）和工业废气等环境污染物在乳腺癌发展中扮演的角色，这些物质通过干扰激素信号传导和DNA损伤而与乳腺癌风险增加相关。机器学习算法可用于预测环境对乳腺癌的影响。

    The biggest Breast cancer is increasingly a major factor in female fatalities, overtaking heart disease. While genetic factors are important in the growth of breast cancer, new research indicates that environmental factors also play a substantial role in its occurrence and progression. The literature on the various environmental factors that may affect breast cancer risk, incidence, and outcomes is thoroughly reviewed in this study report. The study starts by looking at how lifestyle decisions, such as eating habits, exercise routines, and alcohol consumption, may affect hormonal imbalances and inflammation, two important factors driving the development of breast cancer. Additionally, it explores the part played by environmental contaminants such pesticides, endocrine-disrupting chemicals (EDCs), and industrial emissions, all of which have been linked to a higher risk of developing breast cancer due to their interference with hormone signaling and DNA damage. Algorithms for machine lea
    
[^88]: 猜测与绘图：语言模型引导的转译

    Guess & Sketch: Language Model Guided Transpilation. (arXiv:2309.14396v1 [cs.SE])

    [http://arxiv.org/abs/2309.14396](http://arxiv.org/abs/2309.14396)

    本论文通过结合概率性神经语言模型和符号化方法，提出了一种语言模型引导的转译方法，用于自动翻译汇编代码程序，以缩短维护遗留软件的时间和工程成本。

    

    维护遗留软件需要大量的软件和系统工程时间。汇编代码程序对于人类来说特别难以分析，因为它们对计算机机器状态需要低级别的控制，并且没有变量名称。现有的传统程序转换器保证正确性，但是它们是针对特定的源语言和目标编程语言进行手工工程设计的。学习式转译，即代码的自动翻译，提供了手动重写和工程努力的替代方案。自动化的符号程序转换方法保证了正确性，但是由于搜索空间指数级增长，很难扩展到较长的程序。它们刚性的基于规则的系统也限制了它们的表达能力，因此它们只能推理出一小部分程序空间。概率性神经语言模型（LMs）为每个输入生成合理的输出，但是以确保正确性为代价。在这项工作中，我们充分利用了LMs和符号化方法的优势。

    Maintaining legacy software requires many software and systems engineering hours. Assembly code programs, which demand low-level control over the computer machine state and have no variable names, are particularly difficult for humans to analyze. Existing conventional program translators guarantee correctness, but are hand-engineered for the source and target programming languages in question. Learned transpilation, i.e. automatic translation of code, offers an alternative to manual re-writing and engineering efforts. Automated symbolic program translation approaches guarantee correctness but struggle to scale to longer programs due to the exponentially large search space. Their rigid rule-based systems also limit their expressivity, so they can only reason about a reduced space of programs. Probabilistic neural language models (LMs) produce plausible outputs for every input, but do so at the cost of guaranteed correctness. In this work, we leverage the strengths of LMs and symbolic so
    
[^89]: 隐性感知在交通优化中的应用：先进的深度强化学习技术

    Implicit Sensing in Traffic Optimization: Advanced Deep Reinforcement Learning Techniques. (arXiv:2309.14395v1 [cs.LG])

    [http://arxiv.org/abs/2309.14395](http://arxiv.org/abs/2309.14395)

    本论文提出了一个基于深度强化学习的集成车辆跟随和变道决策控制系统，旨在解决高速公路上突发路障情况下智能车辆的行车问题。

    

    高速公路上的突然路障由于道路维护、事故和汽车维修等原因是我们几乎每天都会遇到的情况。配备可以获取车辆动态信息（如速度、加速度和位置）的自主驾驶车辆（AV）可以在到达路障之前做出智能决策来变换车道。许多文献研究已经考察了车辆跟随模型和变道模型。然而，只有很少的研究提出了集成的车辆跟随和变道模型，这个模型有潜力模拟实际的驾驶操纵。因此，在本文中，我们提出了一个基于深度强化学习（DRL）的集成车辆跟随和变道决策控制系统来解决这个问题。具体而言，我们考虑了在高速公路上将进行突发施工的情景。我们将情景建模为马尔可夫决策过程（MDP），并采用着名的DQN算法来训练RL代理以制定决策。

    A sudden roadblock on highways due to many reasons such as road maintenance, accidents, and car repair is a common situation we encounter almost daily. Autonomous Vehicles (AVs) equipped with sensors that can acquire vehicle dynamics such as speed, acceleration, and location can make intelligent decisions to change lanes before reaching a roadblock. A number of literature studies have examined car-following models and lane-changing models. However, only a few studies proposed an integrated car-following and lane-changing model, which has the potential to model practical driving maneuvers. Hence, in this paper, we present an integrated car-following and lane-changing decision-control system based on Deep Reinforcement Learning (DRL) to address this issue. Specifically, we consider a scenario where sudden construction work will be carried out along a highway. We model the scenario as a Markov Decision Process (MDP) and employ the well-known DQN algorithm to train the RL agent to make the
    
[^90]: 多噪声扩散模型用于半监督多域翻译

    Multiple Noises in Diffusion Model for Semi-Supervised Multi-Domain Translation. (arXiv:2309.14394v1 [cs.CL])

    [http://arxiv.org/abs/2309.14394](http://arxiv.org/abs/2309.14394)

    本文提出了一种多噪声扩散模型（MDD）用于半监督多域翻译，通过引入噪声级别来对缺失的域进行建模，实现了任意域之间的翻译而不需要训练单独的模型。

    

    域间翻译涉及在给定源域条件下生成目标域样本。大多数现有方法都集中在固定的输入和输出域上，即它们仅适用于特定的配置（例如对于两个域，要么$D_1\rightarrow{}D_2$，要么$D_2\rightarrow{}D_1$）。本文提出了Multi-Domain Diffusion（MDD）方法，这是一种用于半监督多域翻译的条件扩散框架。与以往的方法不同，MDD不需要定义输入和输出域，允许在一组域的任何分区之间进行翻译（例如$(D_1, D_2)\rightarrow{}D_3$，$D_2\rightarrow{}(D_1, D_3)$，$D_3\rightarrow{}D_1$等），而无需为每个域配置训练单独的模型。MDD的关键思想是利用扩散模型的噪声形式，通过为每个域引入一个噪声级别，以自然的方式对缺失的域进行建模。这将传统的翻译问题转化为一个通过噪声建模来解决的问题。

    Domain-to-domain translation involves generating a target domain sample given a condition in the source domain. Most existing methods focus on fixed input and output domains, i.e. they only work for specific configurations (i.e. for two domains, either $D_1\rightarrow{}D_2$ or $D_2\rightarrow{}D_1$). This paper proposes Multi-Domain Diffusion (MDD), a conditional diffusion framework for multi-domain translation in a semi-supervised context. Unlike previous methods, MDD does not require defining input and output domains, allowing translation between any partition of domains within a set (such as $(D_1, D_2)\rightarrow{}D_3$, $D_2\rightarrow{}(D_1, D_3)$, $D_3\rightarrow{}D_1$, etc. for 3 domains), without the need to train separate models for each domain configuration. The key idea behind MDD is to leverage the noise formulation of diffusion models by incorporating one noise level per domain, which allows missing domains to be modeled with noise in a natural way. This transforms the tra
    
[^91]: LLMCarbon: 对大型语言模型的端到端碳足迹建模

    LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models. (arXiv:2309.14393v1 [cs.CL])

    [http://arxiv.org/abs/2309.14393](http://arxiv.org/abs/2309.14393)

    本研究提出了LLMCarbon，一个针对密集型和MoE LLMs设计的端到端碳足迹预测模型，解决了现有工具的限制，并显著提升了估计的准确性。

    

    大型语言模型（LLMs）的碳足迹是一个重要关注点，包括它们的训练、推理、实验和存储过程中的排放，包括运营和固定碳排放。一个重要方面是在LLMs训练之前准确估计其碳影响，这在很大程度上依赖于GPU的使用。现有研究已报告了LLMs训练的碳足迹，但只有一个工具mlco2能够在实际训练之前预测新的神经网络的碳足迹。然而，mlco2存在一些严重的限制。它不能扩展其对密集或专家混合（MoE）LLMs的估计，忽视了关键的架构参数，仅关注GPU，并不能建模固化的碳足迹。为了解决这些问题，我们引入了LLMCarbon，一个为密集型和MoE LLMs设计的端到端碳足迹预测模型。与mlco2相比，LLMCarbon显著增强了准确性。

    The carbon footprint associated with large language models (LLMs) is a significant concern, encompassing emissions from their training, inference, experimentation, and storage processes, including operational and embodied carbon emissions. An essential aspect is accurately estimating the carbon impact of emerging LLMs even before their training, which heavily relies on GPU usage. Existing studies have reported the carbon footprint of LLM training, but only one tool, mlco2, can predict the carbon footprint of new neural networks prior to physical training. However, mlco2 has several serious limitations. It cannot extend its estimation to dense or mixture-of-experts (MoE) LLMs, disregards critical architectural parameters, focuses solely on GPUs, and cannot model embodied carbon footprints. Addressing these gaps, we introduce \textit{LLMCarbon}, an end-to-end carbon footprint projection model designed for both dense and MoE LLMs. Compared to mlco2, LLMCarbon significantly enhances the ac
    
[^92]: 揭示深度学习在基于脑MRI重建中的公平偏见

    Unveiling Fairness Biases in Deep Learning-Based Brain MRI Reconstruction. (arXiv:2309.14392v1 [eess.IV])

    [http://arxiv.org/abs/2309.14392](http://arxiv.org/abs/2309.14392)

    本研究发现基于深度学习的脑MRI重建模型存在性别和年龄子组之间的显著性能偏差，并揭示了数据不平衡和训练歧视不是偏见的主要来源。

    

    深度学习（DL）特别是在MRI重建方面取得了图像保真度的提高和采集时间的减少。在神经成像中，DL方法可以从采样不足的数据中重建出高质量的图像。然而，考虑到民族特征，尤其是在DL算法中考虑公平性是至关重要的。本研究提出了一个基于DL的脑MRI重建模型的公平性分析。该模型利用U-Net架构进行图像重建，并通过实施基线经验风险最小化（ERM）和重新平衡策略来探索公平性的存在和来源。使用图像重建指标评估模型性能。我们的研究结果揭示了性别和年龄子组之间的统计学显著性能偏差。令人惊讶的是，数据不平衡和训练歧视不是偏见的主要来源。这项分析提供了关于DL图像重建中公平性的见解，并旨在改善公正性。

    Deep learning (DL) reconstruction particularly of MRI has led to improvements in image fidelity and reduction of acquisition time. In neuroimaging, DL methods can reconstruct high-quality images from undersampled data. However, it is essential to consider fairness in DL algorithms, particularly in terms of demographic characteristics. This study presents the first fairness analysis in a DL-based brain MRI reconstruction model. The model utilises the U-Net architecture for image reconstruction and explores the presence and sources of unfairness by implementing baseline Empirical Risk Minimisation (ERM) and rebalancing strategies. Model performance is evaluated using image reconstruction metrics. Our findings reveal statistically significant performance biases between the gender and age subgroups. Surprisingly, data imbalance and training discrimination are not the main sources of bias. This analysis provides insights of fairness in DL-based image reconstruction and aims to improve equit
    
[^93]: 一个用于解释面向服务系统的深度强化学习决策的AI聊天机器人

    An AI Chatbot for Explaining Deep Reinforcement Learning Decisions of Service-oriented Systems. (arXiv:2309.14391v1 [cs.LG])

    [http://arxiv.org/abs/2309.14391](http://arxiv.org/abs/2309.14391)

    一个AI聊天机器人被介绍来解释深度强化学习在面向服务系统中的决策过程，通过提供自然语言解释来帮助用户理解和建立信任。

    

    深度强化学习 (Deep RL) 在面向服务系统中应用越来越多，以应对开放世界的假设。深度强化学习已成功应用于动态服务组合、作业调度、卸载以及服务适应等问题。然而，理解深度强化学习的决策过程具有挑战性，因为其学到的决策策略本质上是一个黑盒子。然而，理解深度强化学习的决策过程对于帮助服务开发人员进行调试、支持服务提供商遵守相关法律框架以及帮助服务使用者建立信任是至关重要的。我们引入了Chat4XAI，通过提供自然语言解释来促进对深度强化学习决策过程的理解。与视觉解释相比，自然语言解释的报告优点包括非技术用户更好的可理解性、用户的接受度和信任度提高，以及更高的效率。

    Deep Reinforcement Learning (Deep RL) is increasingly used to cope with the open-world assumption in service-oriented systems. Deep RL was successfully applied to problems such as dynamic service composition, job scheduling, and offloading, as well as service adaptation. While Deep RL offers many benefits, understanding the decision-making of Deep RL is challenging because its learned decision-making policy essentially appears as a black box. Yet, understanding the decision-making of Deep RL is key to help service developers perform debugging, support service providers to comply with relevant legal frameworks, and facilitate service users to build trust. We introduce Chat4XAI to facilitate the understanding of the decision-making of Deep RL by providing natural-language explanations. Compared with visual explanations, the reported benefits of natural-language explanations include better understandability for non-technical users, increased user acceptance and trust, as well as more effi
    
[^94]: 从大规模用户-产品互动时间序列中预测早期流失

    Early Churn Prediction from Large Scale User-Product Interaction Time Series. (arXiv:2309.14390v1 [cs.LG])

    [http://arxiv.org/abs/2309.14390](http://arxiv.org/abs/2309.14390)

    本文通过对历史数据进行全面研究，提出了一种预测用户早期流失的模型，以促进业务决策和行动。

    

    用户流失，在各种面向客户的业务场景中，以结束与企业关系为特征，对经济产生深远的影响。在许多系统对用户的行动中，如促销折扣和留存活动，预测潜在的流失客户是一个主要目标。在变动剧烈的领域，如幻想体育，不可预测的因素，如国际体育赛事，甚至可以影响到常规的消费习惯。因此，虽然交易历史和用户-产品互动对于预测流失是有价值的，但它们需要深入的领域知识和复杂的特征工程。此外，流失预测系统的特征开发可能会消耗大量资源，特别是在为2亿多用户提供服务的生产环境中，推理流水线主要集中在特征工程上。本文通过对历史数据进行全面的研究，旨在创建一个模型来预测客户流失的可能性，促进决策和行动。

    User churn, characterized by customers ending their relationship with a business, has profound economic consequences across various Business-to-Customer scenarios. For numerous system-to-user actions, such as promotional discounts and retention campaigns, predicting potential churners stands as a primary objective. In volatile sectors like fantasy sports, unpredictable factors such as international sports events can influence even regular spending habits. Consequently, while transaction history and user-product interaction are valuable in predicting churn, they demand deep domain knowledge and intricate feature engineering. Additionally, feature development for churn prediction systems can be resource-intensive, particularly in production settings serving 200m+ users, where inference pipelines largely focus on feature engineering. This paper conducts an exhaustive study on predicting user churn using historical data. We aim to create a model forecasting customer churn likelihood, facil
    
[^95]: 通过广度优先搜索和随机查询探索机器人形态空间

    Exploring Robot Morphology Spaces through Breadth-First Search and Random Query. (arXiv:2309.14387v1 [cs.RO])

    [http://arxiv.org/abs/2309.14387](http://arxiv.org/abs/2309.14387)

    通过比较分析广度优先搜索（BFS）和随机查询在模块化机器人脑体共同进化中的影响，本研究发现这两种查询机制对机器人形态的进化和性能具有重要影响。

    

    进化机器人学为设计和进化机器人形态提供了一个强大的框架，尤其在模块化机器人的上下文中。然而，在基因型到表型映射过程中，查询机制的作用往往被忽视。本研究通过对模块化机器人的脑体共同进化中查询机制的比较分析，填补了这一空白。通过使用两种不同的查询机制——广度优先搜索（BFS）和随机查询，在使用CPPN进化机器人形态和使用张量进化机器人控制器的上下文中，并在两种进化框架（拉马克和达尔文系统）中进行测试，本研究调查了它们对进化结果和性能的影响。研究结果表明，这两种查询机制对模块化机器人身体的进化和性能，包括形态智能、多样性和形态特征的影响。本研究认为BFS更加有效和...

    Evolutionary robotics offers a powerful framework for designing and evolving robot morphologies, particularly in the context of modular robots. However, the role of query mechanisms during the genotype-to-phenotype mapping process has been largely overlooked. This research addresses this gap by conducting a comparative analysis of query mechanisms in the brain-body co-evolution of modular robots. Using two different query mechanisms, Breadth-First Search (BFS) and Random Query, within the context of evolving robot morphologies using CPPNs and robot controllers using tensors, and testing them in two evolutionary frameworks, Lamarckian and Darwinian systems, this study investigates their influence on evolutionary outcomes and performance. The findings demonstrate the impact of the two query mechanisms on the evolution and performance of modular robot bodies, including morphological intelligence, diversity, and morphological traits. This study suggests that BFS is both more effective and 
    
[^96]: 采样 - 变分自编码器 - 集成：在可解释人工智能的探索中

    Sampling - Variational Auto Encoder - Ensemble: In the Quest of Explainable Artificial Intelligence. (arXiv:2309.14385v1 [cs.LG])

    [http://arxiv.org/abs/2309.14385](http://arxiv.org/abs/2309.14385)

    本研究通过采样-变分自编码器（VAE）-集成异常检测（SVEAD）框架的实证评估，在可解释人工智能（XAI）领域做出了贡献。研究发现，集成堆叠、VAE和SHAP的结合不仅可以提高模型性能，还提供了一个易于解释的框架。

    

    可解释人工智能（XAI）模型近年来引起了各个应用领域的广泛兴趣。尽管在这个领域取得了显著进展，但目前仍缺乏标准化的方法或途径来理解人工智能模型的输出。为了弥合这一差距，一个系统和有凝聚力的框架已经越来越必要，以整合新的技术，如判别模型和生成模型。本文通过提出一个基于新颖框架的经验评估，即采样 - 变分自编码器（VAE） - 集成异常检测（SVEAD），为XAI的讨论做出了贡献。该框架是一个混合架构，其中VAE与集成堆叠和SHapley可加性解释（SHAP）用于不平衡分类。研究结果表明，集成堆叠、VAE和SHAP的结合不仅可以提高模型性能，而且提供了一个易于解释的框架。本研究还结合排列重要性和内部与外部重要性的SHAP进行了研究。

    Explainable Artificial Intelligence (XAI) models have recently attracted a great deal of interest from a variety of application sectors. Despite significant developments in this area, there are still no standardized methods or approaches for understanding AI model outputs. A systematic and cohesive framework is also increasingly necessary to incorporate new techniques like discriminative and generative models to close the gap. This paper contributes to the discourse on XAI by presenting an empirical evaluation based on a novel framework: Sampling - Variational Auto Encoder (VAE) - Ensemble Anomaly Detection (SVEAD). It is a hybrid architecture where VAE combined with ensemble stacking and SHapley Additive exPlanations are used for imbalanced classification. The finding reveals that combining ensemble stacking, VAE, and SHAP can. not only lead to better model performance but also provide an easily explainable framework. This work has used SHAP combined with Permutation Importance and In
    
[^97]: 通过利用人工智能进行咳嗽诊断: 一项调查

    Towards using Cough for Respiratory Disease Diagnosis by leveraging Artificial Intelligence: A Survey. (arXiv:2309.14383v1 [cs.SD])

    [http://arxiv.org/abs/2309.14383](http://arxiv.org/abs/2309.14383)

    使用人工智能利用咳嗽诊断呼吸道疾病已经成为一个有希望的趋势，通过研究咳嗽特征进行深度学习算法的发展可以可靠准确地检测特定呼吸系统疾病的发作。

    

    咳嗽声音包含着呼吸系统病理形态学改变的众多重要信息。通过研究潜在的咳嗽特征以及疾病诊断，可可靠准确地检测咳嗽事件在恢复医疗实践中发挥不可或缺的作用。人工智能（AI）的近期应用和泛在计算的进步为呼吸道疾病预测开创了有利的趋势和无数未来可能性。特别是，基于机器学习（ML）和深度学习（DL）的咳嗽诊断算法的迅速出现已经开始利用咳嗽特征。大量关于基于咳嗽的AI算法的文献表明，这些模型可以在检测特定呼吸系统疾病的发作方面发挥重要作用。然而，对医疗专家来说，以详尽的方式收集所有相关研究的信息是非常关键的。

    Cough acoustics contain multitudes of vital information about pathomorphological alterations in the respiratory system. Reliable and accurate detection of cough events by investigating the underlying cough latent features and disease diagnosis can play an indispensable role in revitalizing the healthcare practices. The recent application of Artificial Intelligence (AI) and advances of ubiquitous computing for respiratory disease prediction has created an auspicious trend and myriad of future possibilities in the medical domain. In particular, there is an expeditiously emerging trend of Machine learning (ML) and Deep Learning (DL)-based diagnostic algorithms exploiting cough signatures. The enormous body of literature on cough-based AI algorithms demonstrate that these models can play a significant role for detecting the onset of a specific respiratory disease. However, it is pertinent to collect the information from all relevant studies in an exhaustive manner for the medical experts a
    
[^98]: 基于文本分类的方法用于评估和增强建筑法规的机器可解释性

    A Text Classification-Based Approach for Evaluating and Enhancing the Machine Interpretability of Building Codes. (arXiv:2309.14374v1 [cs.CL])

    [http://arxiv.org/abs/2309.14374](http://arxiv.org/abs/2309.14374)

    该研究提出了一种基于文本分类的方法，自动评估和增强建筑法规的机器可解释性。研究考虑了条款和文档层面的机器可解释性，通过引入几个类别进行分类并开发了一个数据集进行模型训练，并开发了一个高效的文本分类模型。

    

    将监管文件或建筑法规解释为可计算机处理的格式对于智能设计和建造建筑和基础设施至关重要。虽然自动化规则解释（ARI）方法已经研究多年，但其中大部分方法都严重依赖于从建筑法规中早期和手动筛选可解释条款。 虽然其中少数方法考虑了从条款和文档层面上的机器可解释性，但这代表了将其转化为可计算机处理格式的潜力。因此，本研究旨在提出一种新的方法，自动评估和增强单个条款和建筑法规的机器可解释性。首先，引入了几个类别，以考虑对规则解释的要求对每个建筑法规条款进行分类，并开发了一个数据集来进行模型训练。然后，基于预训练的领域特定语言的高效文本分类模型被开发出来。

    Interpreting regulatory documents or building codes into computer-processable formats is essential for the intelligent design and construction of buildings and infrastructures. Although automated rule interpretation (ARI) methods have been investigated for years, most of them highly depend on the early and manual filtering of interpretable clauses from a building code. While few of them considered machine interpretability, which represents the potential to be transformed into a computer-processable format, from both clause- and document-level. Therefore, this research aims to propose a novel approach to automatically evaluate and enhance the machine interpretability of single clause and building codes. First, a few categories are introduced to classify each clause in a building code considering the requirements for rule interpretation, and a dataset is developed for model training. Then, an efficient text classification model is developed based on a pretrained domain-specific language 
    
[^99]: 人类转录质量改进

    Human Transcription Quality Improvement. (arXiv:2309.14372v1 [cs.CL])

    [http://arxiv.org/abs/2309.14372](http://arxiv.org/abs/2309.14372)

    本文提出了一种可靠的方法来收集高质量的语音转录数据，通过在标注阶段进行置信度估计的重新处理和在标注后阶段进行自动词错误修正，成功降低了转录词误率（WER），并发现了转录错误对ASR模型性能的强相关性。

    

    高质量的转录数据对于训练自动语音识别（ASR）系统至关重要。然而，现有的行业级数据收集管道对研究人员来说成本高昂，而众包转录的质量较低。在本文中，我们提出了一种可靠的方法来收集语音转录。我们引入了两种机制来改善转录质量：在标注阶段基于置信度估计的重新处理和在标注后阶段的自动词错误修正。我们收集并发布了LibriCrowd - 一个包含100小时英语语音转录的大规模众包数据集。实验表明，转录词误率（WER）降低了50%以上。我们进一步研究了转录错误对ASR模型性能的影响，并发现了强相关性。转录质量的提升使ASR模型的WER相对减少了10%以上。我们发布了数据集和代码，以造福研究界。

    High quality transcription data is crucial for training automatic speech recognition (ASR) systems. However, the existing industry-level data collection pipelines are expensive to researchers, while the quality of crowdsourced transcription is low. In this paper, we propose a reliable method to collect speech transcriptions. We introduce two mechanisms to improve transcription quality: confidence estimation based reprocessing at labeling stage, and automatic word error correction at post-labeling stage. We collect and release LibriCrowd - a large-scale crowdsourced dataset of audio transcriptions on 100 hours of English speech. Experiment shows the Transcription WER is reduced by over 50%. We further investigate the impact of transcription error on ASR model performance and found a strong correlation. The transcription quality improvement provides over 10% relative WER reduction for ASR models. We release the dataset and code to benefit the research community.
    
[^100]: 基于单位权重的单次迭代量子感知器算法用于非理想训练集

    A Unitary Weights Based One-Iteration Quantum Perceptron Algorithm for Non-Ideal Training Sets. (arXiv:2309.14366v1 [quant-ph])

    [http://arxiv.org/abs/2309.14366](http://arxiv.org/abs/2309.14366)

    提出了一种基于单位权重的高效量子感知器算法，能够解决非理想训练集问题并实现一次迭代学习。算法通过计算训练集中总权重矩阵的奇异值分解来使权重矩阵成为单位阵，并能够精确实现任意量子门。与其他量子感知器算法相比，该算法具有更好的适用性、准确性和可用性。同时，该算法的适用性得到了进一步验证，演示了由多个基本量子门构成的量子复合门。

    

    为了解决非理想训练集（即不完整或过完备集）问题并实现一次迭代学习，提出了一种基于单位权重的高效量子感知器算法，通过计算训练集中总权重矩阵的奇异值分解来使权重矩阵成为单位阵。对量子门{H，S，T，CNOT，Toffoli，Fredkin}的示例验证表明，我们的算法能够在一次迭代中准确实现任意量子门。通过对我们的算法和其他量子感知器算法的性能比较，证明了我们算法在适用性、准确性和可用性方面的优势。为了进一步验证我们算法的适用性，还演示了由几个基本量子门构成的量子复合门。

    In order to solve the problem of non-ideal training sets (i.e., the less-complete or over-complete sets) and implement one-iteration learning, a novel efficient quantum perceptron algorithm based on unitary weights is proposed, where the singular value decomposition of the total weight matrix from the training set is calculated to make the weight matrix to be unitary. The example validation of quantum gates {H, S, T, CNOT, Toffoli, Fredkin} shows that our algorithm can accurately implement arbitrary quantum gates within one iteration. The performance comparison between our algorithm and other quantum perceptron algorithms demonstrates the advantages of our algorithm in terms of applicability, accuracy, and availability. For further validating the applicability of our algorithm, a quantum composite gate which consists of several basic quantum gates is also illustrated.
    
[^101]: 针对无监督域适应的领域引导条件扩散模型

    Domain-Guided Conditional Diffusion Model for Unsupervised Domain Adaptation. (arXiv:2309.14360v1 [cs.LG])

    [http://arxiv.org/abs/2309.14360](http://arxiv.org/abs/2309.14360)

    该论文提出了一种领域引导条件扩散模型（DACDM），通过生成高质量和多样性的目标域样本，帮助现有的无监督域适应方法进行更轻松的迁移，从而提高了性能。

    

    深度学习模型在应用于新的应用场景时，受限的可迁移性影响其性能。最近，无监督域适应（UDA）通过学习领域不变特征在解决这个问题上取得了显著进展。然而，现有的UDA方法的性能受到大范围的领域偏移和有限的目标域数据的限制。为了缓解这些问题，我们提出了一种领域引导条件扩散模型（DACDM），用于为目标域生成高保真度和多样性的样本。在所提出的DACDM中，通过引入类别信息，可以控制生成样本的标签，并且在DACDM中还引入了一个领域分类器来指导目标域的生成样本。生成的样本帮助现有的UDA方法更轻松地从源域转移到目标域，从而提高了迁移性能。在各种基准测试上进行的大量实验表明，DACDM带来了较大的改进。

    Limited transferability hinders the performance of deep learning models when applied to new application scenarios. Recently, Unsupervised Domain Adaptation (UDA) has achieved significant progress in addressing this issue via learning domain-invariant features. However, the performance of existing UDA methods is constrained by the large domain shift and limited target domain data. To alleviate these issues, we propose DomAin-guided Conditional Diffusion Model (DACDM) to generate high-fidelity and diversity samples for the target domain. In the proposed DACDM, by introducing class information, the labels of generated samples can be controlled, and a domain classifier is further introduced in DACDM to guide the generated samples for the target domain. The generated samples help existing UDA methods transfer from the source domain to the target domain more easily, thus improving the transfer performance. Extensive experiments on various benchmarks demonstrate that DACDM brings a large impr
    
[^102]: COCO-Counterfactuals:自动构建图像-文本对的反事实例

    COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs. (arXiv:2309.14356v1 [cs.LG])

    [http://arxiv.org/abs/2309.14356](http://arxiv.org/abs/2309.14356)

    COCO-Counterfactuals是一个自动构建图像-文本对的反事实例的框架，通过使用文本到图像扩散模型来自动生成多模态反事实例。通过人工评估，我们验证了COCO-Counterfactuals的质量，并展示了其对于改善域外泛化能力的实用性。

    

    反事实例在自然语言处理(NLP)领域中已证明对于评估和改进语言模型对数据集中的虚假相关性的鲁棒性非常有价值。尽管反事实例在NLP领域具有显著的效用，但由于创建最小反事实变化的图像-文本配对数据的难度，多模态反事实例的研究相对较少。为了解决这一挑战，我们引入了一个可扩展的框架，利用文本到图像扩散模型自动生成反事实例。我们使用这个框架来创建COCO-Counterfactuals，这是一个基于MS-COCO数据集的多模态反事实数据集，包括图像和文本标题的配对。我们通过人工评估验证了COCO-Counterfactuals的质量，并展示了现有的多模态模型在我们的反事实图像-文本配对中面临的挑战。此外，我们展示了COCO-Counterfactuals在改善域外泛化能力方面的实用性。

    Counterfactual examples have proven to be valuable in the field of natural language processing (NLP) for both evaluating and improving the robustness of language models to spurious correlations in datasets. Despite their demonstrated utility for NLP, multimodal counterfactual examples have been relatively unexplored due to the difficulty of creating paired image-text data with minimal counterfactual changes. To address this challenge, we introduce a scalable framework for automatic generation of counterfactual examples using text-to-image diffusion models. We use our framework to create COCO-Counterfactuals, a multimodal counterfactual dataset of paired image and text captions based on the MS-COCO dataset. We validate the quality of COCO-Counterfactuals through human evaluations and show that existing multimodal models are challenged by our counterfactual image-text pairs. Additionally, we demonstrate the usefulness of COCO-Counterfactuals for improving out-of-domain generalization of 
    
[^103]: PopBERT. 检测德国联邦议院中的民粹主义及其主导意识形态

    PopBERT. Detecting populism and its host ideologies in the German Bundestag. (arXiv:2309.14355v1 [cs.CL])

    [http://arxiv.org/abs/2309.14355](http://arxiv.org/abs/2309.14355)

    本文提出了一种可靠、有效且可扩展的方法来衡量民粹主义立场，通过在德国联邦议院的演讲中进行语言标记，训练了一个基于Transformer的模型（PopBERT）来检测和量化民粹主义的核心维度，并确定民粹主义陈述与左翼或右翼主导意识形态的关联。

    

    民粹主义的崛起引起了许多政治学家和从业人员的关注，然而对其潜在语言的检测仍然不完整。本文旨在提供一种可靠、有效且可扩展的方法来衡量民粹主义立场。为此，我们基于德国联邦议院的议会演讲(2013年至2021年)创建了一个带注释的数据集。遵循民粹主义的概念定义，我们将对高尚人民或腐败精英的道德引用标记为民粹主义语言的核心维度。为了进一步确定民粹主义薄意识形态的形成方式，我们注释了民粹主义陈述如何与左翼或右翼的主导意识形态相关。然后，我们训练了一个基于Transformer的模型（PopBERT）作为一个多标签分类器，用于检测和量化每个维度。一系列验证检查显示该模型预测准确性强，具有高质量的面部有效性，与专家调查的党派排名相吻合，并检测出-of-sa

    The rise of populism concerns many political scientists and practitioners, yet the detection of its underlying language remains fragmentary. This paper aims to provide a reliable, valid, and scalable approach to measure populist stances. For that purpose, we created an annotated dataset based on parliamentary speeches of the German Bundestag (2013 to 2021). Following the ideational definition of populism, we label moralizing references to the virtuous people or the corrupt elite as core dimensions of populist language. To identify, in addition, how the thin ideology of populism is thickened, we annotate how populist statements are attached to left-wing or right-wing host ideologies. We then train a transformer-based model (PopBERT) as a multilabel classifier to detect and quantify each dimension. A battery of validation checks reveals that the model has a strong predictive accuracy, provides high qualitative face validity, matches party rankings of expert surveys, and detects out-of-sa
    
[^104]: 通过深度展开分布式ADMM实现有限通信的分布式优化

    Limited Communications Distributed Optimization via Deep Unfolded Distributed ADMM. (arXiv:2309.14353v1 [math.OC])

    [http://arxiv.org/abs/2309.14353](http://arxiv.org/abs/2309.14353)

    该论文提出了展开的D-ADMM算法来解决分布式优化中的通信限制问题，通过深度展开的方法，减少了消息交换的数量并保持了D-ADMM的操作。

    

    分布式优化是在分散式多智能体系统中进行协作推理和决策的基本框架。操作被建模为共同最小化一个共享目标，该目标通常依赖于每个智能体本地收集的观测。分布式优化算法（如常见的D-ADMM）通过迭代地结合本地计算和消息交换来解决这个任务。与分布式优化（特别是D-ADMM）相关的主要挑战之一是它需要大量的通信，即代理之间交换的消息，以达成共识。这使得D-ADMM在功耗、延迟和通道资源方面变得昂贵。在本文中，我们提出了展开的D-ADMM，它采用新兴的深度展开方法，使D-ADMM能够可靠地通过每个智能体事先定义的少量消息进行操作。展开的D-ADMM完全保留D-ADMM的操作，同时利用数据的空间和时间相关性来减少通信成本。

    Distributed optimization is a fundamental framework for collaborative inference and decision making in decentralized multi-agent systems. The operation is modeled as the joint minimization of a shared objective which typically depends on observations gathered locally by each agent. Distributed optimization algorithms, such as the common D-ADMM, tackle this task by iteratively combining local computations and message exchanges. One of the main challenges associated with distributed optimization, and particularly with D-ADMM, is that it requires a large number of communications, i.e., messages exchanged between the agents, to reach consensus. This can make D-ADMM costly in power, latency, and channel resources. In this work we propose unfolded D-ADMM, which follows the emerging deep unfolding methodology to enable D-ADMM to operate reliably with a predefined and small number of messages exchanged by each agent. Unfolded D-ADMM fully preserves the operation of D-ADMM, while leveraging dat
    
[^105]: 使用仿真数据培训卫星测高的神经映射方案

    Training neural mapping schemes for satellite altimetry with simulation data. (arXiv:2309.14350v1 [physics.ao-ph])

    [http://arxiv.org/abs/2309.14350](http://arxiv.org/abs/2309.14350)

    本研究利用仿真数据和卫星测高仪培训了用于海洋表面高度的神经映射方案，并评估了不同海洋模拟数据对性能的影响。

    

    卫星测高与数据同化和最优插值方案结合，深刻提升了我们监测海洋表面动态的能力。最近，深度学习方案已成为解决时空插值问题的吸引人解决方案。然而，实际环境中的卫星测高数据集在海洋表面时空覆盖方面稀缺，这制约了最先进的神经方案在真实案例研究上的训练。在这里，我们利用海洋动力学的模拟和卫星测高仪来训练基于仿真的神经映射方案，以实现海洋表面高度的模拟，并展示其在真实卫星测高数据集上的性能。我们进一步分析了训练阶段使用的海洋模拟数据集对性能的影响。这个实验性分析涵盖了涡流-存在配置到涡流丰富配置的分辨率，使用数据同化和无潮解析模拟的强制模拟与再分析。

    Satellite altimetry combined with data assimilation and optimal interpolation schemes have deeply renewed our ability to monitor sea surface dynamics. Recently, deep learning (DL) schemes have emerged as appealing solutions to address space-time interpolation problems. The scarcity of real altimetry dataset, in terms of space-time coverage of the sea surface, however impedes the training of state-of-the-art neural schemes on real-world case-studies. Here, we leverage both simulations of ocean dynamics and satellite altimeters to train simulation-based neural mapping schemes for the sea surface height and demonstrate their performance for real altimetry datasets. We analyze further how the ocean simulation dataset used during the training phase impacts this performance. This experimental analysis covers both the resolution from eddy-present configurations to eddy-rich ones, forced simulations vs. reanalyses using data assimilation and tide-free vs. tide-resolving simulations. Our benchm
    
[^106]: 企业信用评级：一项调查

    Corporate Credit Rating: A Survey. (arXiv:2309.14349v1 [cs.LG])

    [http://arxiv.org/abs/2309.14349](http://arxiv.org/abs/2309.14349)

    本论文对企业信用评级进行了系统的调查，总结了CCR的发展背景，比较了不同模型的优缺点，并展望了CCR的未来。

    

    企业信用评级（CCR）在当代经济和社会发展过程中扮演着非常重要的角色。如何使用信用评级方法对企业进行评估一直是一个值得讨论的问题。通过国内外相关文献的阅读和研究，本论文对CCR进行了系统的调查。本论文从统计模型、机器学习模型和神经网络模型的三个层面整理了CCR方法发展的背景，总结了CCR的常见数据集，并深入比较了这些模型的优缺点。最后，本论文总结了当前研究存在的问题，并展望了CCR的未来。与现有的CCR综述相比，本论文详细阐述和分析了神经网络模型在这一领域近年来的进展。

    Corporate credit rating (CCR) plays a very important role in the process of contemporary economic and social development. How to use credit rating methods for enterprises has always been a problem worthy of discussion. Through reading and studying the relevant literature at home and abroad, this paper makes a systematic survey of CCR. This paper combs the context of the development of CCR methods from the three levels: statistical models, machine learning models and neural network models, summarizes the common databases of CCR, and deeply compares the advantages and disadvantages of the models. Finally, this paper summarizes the problems existing in the current research and prospects the future of CCR. Compared with the existing review of CCR, this paper expounds and analyzes the progress of neural network model in this field in recent years.
    
[^107]: 通过稳健对齐的LLM抵御对齐破坏攻击

    Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM. (arXiv:2309.14348v1 [cs.CL])

    [http://arxiv.org/abs/2309.14348](http://arxiv.org/abs/2309.14348)

    本文提出了一种稳健对齐的LLM（RA-LLM），用于防御可能发生的对齐破坏攻击。RA-LLM可以直接在现有的对齐LLM上构建，并通过稳健的对齐检查函数来确保其有效性。

    

    最近，大型语言模型（LLMs）取得了显著的进展，并在各个领域得到广泛应用。不幸的是，人们越来越担心LLMs可能被滥用来生成有害或恶意内容。尽管有一系列的研究专注于对齐LLMs与人类价值观，并防止它们生成不适当的内容，但这些对齐通常是脆弱的，并且可以通过对抗优化或手工构建的越狱提示来绕过。在这项工作中，我们介绍了一种稳健对齐的LLM（RA-LLM），以防范潜在的对齐破坏攻击。RA-LLM可以直接构建在现有的对齐LLM上，通过具有稳健对齐检查功能的方法，而无需对原始LLM进行任何昂贵的重新训练或微调。此外，我们还通过理论分析验证了RA-LLM在防御对齐破坏攻击方面的有效性。通过现实世界的实验，

    Recently, Large Language Models (LLMs) have made significant advancements and are now widely used across various domains. Unfortunately, there has been a rising concern that LLMs can be misused to generate harmful or malicious content. Though a line of research has focused on aligning LLMs with human values and preventing them from producing inappropriate content, such alignments are usually vulnerable and can be bypassed by alignment-breaking attacks via adversarially optimized or handcrafted jailbreaking prompts. In this work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against potential alignment-breaking attacks. RA-LLM can be directly constructed upon an existing aligned LLM with a robust alignment checking function, without requiring any expensive retraining or fine-tuning process of the original LLM. Furthermore, we also provide a theoretical analysis for RA-LLM to verify its effectiveness in defending against alignment-breaking attacks. Through real-world experiments
    
[^108]: 一个用于改进动态集成选择方法的后选择算法

    A post-selection algorithm for improving dynamic ensemble selection methods. (arXiv:2309.14307v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.14307](http://arxiv.org/abs/2309.14307)

    提出了一种后选择动态集成选择（PS-DES）方法，通过使用不同的度量指标评估多种DES技术选择的集成，来提高准确性。

    

    动态集成选择（DES）是一种多分类器系统（MCS）方法，旨在在选择阶段为每个查询样本选择一个集成。尽管提出了几种DES方法的建议，但对于不同的问题，没有特定的DES技术是最佳选择。因此，我们假设选择每个查询示例的最佳DES方法可以提高准确性。为了评估这个想法，我们引入了后选择动态集成选择（PS-DES）方法，这是一种后选择方案，使用不同的度量指标评估由几种DES技术选择的集成。实验结果表明，使用准确性作为选择集成的指标，PS-DES的表现优于单个DES技术。PS-DES源代码可在GitHub存储库中获得。

    Dynamic Ensemble Selection (DES) is a Multiple Classifier Systems (MCS) approach that aims to select an ensemble for each query sample during the selection phase. Even with the proposal of several DES approaches, no particular DES technique is the best choice for different problems. Thus, we hypothesize that selecting the best DES approach per query instance can lead to better accuracy. To evaluate this idea, we introduce the Post-Selection Dynamic Ensemble Selection (PS-DES) approach, a post-selection scheme that evaluates ensembles selected by several DES techniques using different metrics. Experimental results show that using accuracy as a metric to select the ensembles, PS-DES performs better than individual DES techniques. PS-DES source code is available in a GitHub repository
    
[^109]: MoDem-V2: 面向真实世界机器人操作的视觉-运动世界模型

    MoDem-V2: Visuo-Motor World Models for Real-World Robot Manipulation. (arXiv:2309.14236v1 [cs.RO] CROSS LISTED)

    [http://arxiv.org/abs/2309.14236](http://arxiv.org/abs/2309.14236)

    MoDem-V2是一个能够在非仪器化的真实世界中直接学习接触丰富操作的系统。

    

    希望在非仪器化的真实世界环境中运行的机器人系统必须通过机载传感器直接感知世界。基于视觉的学习系统旨在通过基于原始像素的隐式对世界的理解，消除环境装置的需求，但仅仅依靠稀疏的视觉奖励信号在接触丰富的高维搜索空间中导航，显著加剧了探索的难度。因此，这种系统的适用性通常局限于模拟或严格工程化的环境，因为在没有明确的状态估计和稠密奖励的指导下，在真实世界中进行代理的探索可能导致不安全行为和重大安全故障。在本研究中，我们分离了这些限制的根本原因，开发了一个名为MoDem-V2的系统，能够直接在非仪器化的真实世界中学习接触丰富的操作。在最新的算法进展的基础上构建，

    Robotic systems that aspire to operate in uninstrumented real-world environments must perceive the world directly via onboard sensing. Vision-based learning systems aim to eliminate the need for environment instrumentation by building an implicit understanding of the world based on raw pixels, but navigating the contact-rich high-dimensional search space from solely sparse visual reward signals significantly exacerbates the challenge of exploration. The applicability of such systems is thus typically restricted to simulated or heavily engineered environments since agent exploration in the real-world without the guidance of explicit state estimation and dense rewards can lead to unsafe behavior and safety faults that are catastrophic. In this study, we isolate the root causes behind these limitations to develop a system, called MoDem-V2, capable of learning contact-rich manipulation directly in the uninstrumented real world. Building on the latest algorithmic advancements in model-based
    
[^110]: 伪标签选择是一个决策问题

    Pseudo Label Selection is a Decision Problem. (arXiv:2309.13926v1 [cs.LG])

    [http://arxiv.org/abs/2309.13926](http://arxiv.org/abs/2309.13926)

    伪标签选择是半监督学习中的一种方法，通过嵌入决策理论，提出了BPLS框架来解决伪标签选择中的确认偏差问题。

    

    伪标签选择是半监督学习中一种简单而有效的方法，它需要一些准则来指导伪标签数据的选择。这些准则被证明可以在实践中工作得相当好。然而，它们的性能往往取决于标记数据上初始模型的拟合情况。早期过拟合可能通过选择具有自信但错误预测的实例（通常被称为确认偏差）而传播到最终模型。在两项最近的工作中，我们证明了伪标签选择（PLS）可以自然地嵌入到决策理论中。这为BPLS铺平了道路，它是一种用于PLS的贝叶斯框架，可以缓解确认偏差的问题。其核心是一种新的选择准则：伪样本和标记数据的后验预测的解析近似。我们通过证明这个“伪POS”的贝叶斯最优性来推导出这个选择准则。

    Pseudo-Labeling is a simple and effective approach to semi-supervised learning. It requires criteria that guide the selection of pseudo-labeled data. The latter have been shown to crucially affect pseudo-labeling's generalization performance. Several such criteria exist and were proven to work reasonably well in practice. However, their performance often depends on the initial model fit on labeled data. Early overfitting can be propagated to the final model by choosing instances with overconfident but wrong predictions, often called confirmation bias. In two recent works, we demonstrate that pseudo-label selection (PLS) can be naturally embedded into decision theory. This paves the way for BPLS, a Bayesian framework for PLS that mitigates the issue of confirmation bias. At its heart is a novel selection criterion: an analytical approximation of the posterior predictive of pseudo-samples and labeled data. We derive this selection criterion by proving Bayes-optimality of this "pseudo pos
    
[^111]: 关于现代量化高效神经网络的校准研究

    On Calibration of Modern Quantized Efficient Neural Networks. (arXiv:2309.13866v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.13866](http://arxiv.org/abs/2309.13866)

    这项研究探索了在不同精度下三种架构和两个数据集的神经网络校准性能，发现校准质量与量化质量相关，并观察到在低精度下性能和校准质量均变差。GhostNet-VGG表现出最高稳健性，温度缩放可以改善量化网络的校准误差。

    

    我们探索了三种架构（ShuffleNetv2、GhostNet-VGG和MobileOne）以及两个数据集（CIFAR-100和PathMNIST）在不同精度下的校准性能。研究发现，校准质量与量化质量密切相关；已有文献证明随着精度降低，性能会变得更差，我们观察到校准质量也存在类似的关联。这在4位激活区间尤为严重。GhostNet-VGG在更低精度下表现出最高的稳健性，我们发现温度缩放可以改善量化网络的校准误差，但需要注意一些细节。我们希望这些初步的观察能够为可解释和可靠的边缘机器学习提供更多机会。

    We explore calibration properties at various precisions for three architectures: ShuffleNetv2, GhostNet-VGG, and MobileOne; and two datasets: CIFAR-100 and PathMNIST. The quality of calibration is observed to track the quantization quality; it is well-documented that performance worsens with lower precision, and we observe a similar correlation with poorer calibration. This becomes especially egregious at 4-bit activation regime. GhostNet-VGG is shown to be the most robust to overall performance drop at lower precision. We find that temperature scaling can improve calibration error for quantized networks, with some caveats. We hope that these preliminary insights can lead to more opportunities for explainable and reliable EdgeML.
    
[^112]: ICU 重新入院预测的可解释机器学习

    Explainable Machine Learning for ICU Readmission Prediction. (arXiv:2309.13781v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.13781](http://arxiv.org/abs/2309.13781)

    本研究提出了一个标准化且可解释的机器学习流程，用于在多中心数据库中预测加护病房患者的再入院情况。

    

    加护病房（ICU）是一个复杂的医院环境，医生的决策对患者的生命构成高风险。必须遵循一条全面的护理路径来减少并发症。在这种环境中，不确定性、竞争性和非计划性的因素增加了统一实施护理路径的困难。再入院是该路径的困难之一，即患者在短时间内再次入住ICU，导致高死亡率和高资源利用率。一些研究尝试通过患者的医疗信息来预测再入院情况。尽管它们在预测再入院时有一定的成功，但这些研究并未对再入院预测进行适当的评估、描述和理解。本研究提出了一个标准化且可解释的机器学习流程，用于在多中心数据库（即包含166,355名患者的eICU队列，200,859名...）

    The intensive care unit (ICU) comprises a complex hospital environment, where decisions made by clinicians have a high level of risk for the patients' lives. A comprehensive care pathway must then be followed to reduce p complications. Uncertain, competing and unplanned aspects within this environment increase the difficulty in uniformly implementing the care pathway. Readmission contributes to this pathway's difficulty, occurring when patients are admitted again to the ICU in a short timeframe, resulting in high mortality rates and high resource utilisation. Several works have tried to predict readmission through patients' medical information. Although they have some level of success while predicting readmission, those works do not properly assess, characterise and understand readmission prediction. This work proposes a standardised and explainable machine learning pipeline to model patient readmission on a multicentric database (i.e., the eICU cohort with 166,355 patients, 200,859 ad
    
[^113]: 对编码-解码架构在双平面X射线到3D形状重建中进行基准测试

    Benchmarking Encoder-Decoder Architectures for Biplanar X-ray to 3D Shape Reconstruction. (arXiv:2309.13587v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2309.13587](http://arxiv.org/abs/2309.13587)

    对于双平面X射线到3D形状重建，我们提出了一个基准测试框架，该框架包括评估与真实临床情况相关的任务，并提供了8个模型的参考实现和6个公共数据集的收集和预处理。通过这个开源平台，我们可以比较不同模型的性能，评估其对真实世界临床情景的适用性。

    

    各种深度学习模型已被提出用于从两个正交（双平面）X射线图像进行3D骨形重建。然而，由于这些模型在不同的解剖、队列和（通常是私有的）数据集上进行评估，它们之间的比较还不清楚。此外，常常优化的基于图像的分割度量，例如Dice分数，在2D-3D骨形重建中估计临床参数的影响尚不为人所知。为了更接近临床转化，我们提出了一个基准测试框架，评估与真实临床情景相关的任务，包括重建断骨、植入物骨，对人群转移的鲁棒性以及估计临床参数的误差。我们的开源平台提供了8个模型的参考实现（其中许多实现以前不可公开获得），API可轻松收集和预处理6个公共数据集，并实现了自动实证评价。

    Various deep learning models have been proposed for 3D bone shape reconstruction from two orthogonal (biplanar) X-ray images. However, it is unclear how these models compare against each other since they are evaluated on different anatomy, cohort and (often privately held) datasets. Moreover, the impact of the commonly optimized image-based segmentation metrics such as dice score on the estimation of clinical parameters relevant in 2D-3D bone shape reconstruction is not well known. To move closer toward clinical translation, we propose a benchmarking framework that evaluates tasks relevant to real-world clinical scenarios, including reconstruction of fractured bones, bones with implants, robustness to population shift, and error in estimating clinical parameters. Our open-source platform provides reference implementations of 8 models (many of whose implementations were not publicly available), APIs to easily collect and preprocess 6 public datasets, and the implementation of automatic 
    
[^114]: 概率权重固定：用于量化的神经网络权重不确定性的大规模训练

    Probabilistic Weight Fixing: Large-scale training of neural network weight uncertainties for quantization. (arXiv:2309.13575v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.13575](http://arxiv.org/abs/2309.13575)

    本文提出了一种基于贝叶斯神经网络和变分松弛的概率框架，用于通过将权重值限制在一组有限值上来减少推理过程中的能量消耗。通过利用权重值的概率分布，提高了噪声鲁棒性和可压缩性。迭代聚类过程展示了超越现有方法的优势。

    

    权重共享量化是一种通过将神经网络的权重限制在一组有限的值上来减少推理过程中能量消耗的技术。然而，现有的权重共享量化方法常常基于权重值本身进行假设，并忽视了权重位置在其中扮演的独特角色。本文提出了一个基于贝叶斯神经网络（BNNs）和变分松弛的概率框架，根据单个权重的位置特定学习不确定性分布来确定可以将哪些权重移动到哪个聚类中心以及移动到什么程度。我们引入了一种新的初始化设置和正则化项，可以在复杂的数据集-模型组合下训练BNNs。通过利用通过概率分布捕捉到的权重值的灵活性，我们提高了噪声的鲁棒性和下游的可压缩性。我们的迭代聚类过程展示了超越现有方法的优越性能。

    Weight-sharing quantization has emerged as a technique to reduce energy expenditure during inference in large neural networks by constraining their weights to a limited set of values. However, existing methods for weight-sharing quantization often make assumptions about the treatment of weights based on value alone that neglect the unique role weight position plays. This paper proposes a probabilistic framework based on Bayesian neural networks (BNNs) and a variational relaxation to identify which weights can be moved to which cluster centre and to what degree based on their individual position-specific learned uncertainty distributions. We introduce a new initialisation setting and a regularisation term which allow for the training of BNNs under complex dataset-model combinations. By leveraging the flexibility of weight values captured through a probability distribution, we enhance noise resilience and downstream compressibility. Our iterative clustering procedure demonstrates superio
    
[^115]: 焦点中的湍流：用BLASTNet 2.0数据基准测量三维体积超分辨率的缩放行为

    Turbulence in Focus: Benchmarking Scaling Behavior of 3D Volumetric Super-Resolution with BLASTNet 2.0 Data. (arXiv:2309.13457v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.13457](http://arxiv.org/abs/2309.13457)

    本研究提出了BLASTNet 2.0，包含三维高保真压缩湍流流动模拟数据，通过对五种深度学习方法的基准测试和神经缩放分析，揭示了模型规模、成本和架构对预测性能的影响。

    

    压缩湍流流动的分析对推进、能源生成和环境相关应用至关重要。本文介绍了BLASTNet 2.0，它是一个包含744个完整域样本来自34个高保真直接数值模拟的2.2TB数据集网络，旨在解决目前三维高保真反应和非反应压缩湍流流动模拟数据有限的问题。利用这些数据，我们基准测试了49种不同的深度学习方法的五个变体，用于改进科学成像、模拟、湍流模型以及计算机视觉应用。我们对这些模型进行了神经缩放分析，以检查不同机器学习（ML）方法的性能，包括两种科学ML技术。我们证明了（i）预测性能可以随模型规模和成本而扩展，（ii）架构尤其对较小的模型有重要影响，以及（iii）b...

    Analysis of compressible turbulent flows is essential for applications related to propulsion, energy generation, and the environment. Here, we present BLASTNet 2.0, a 2.2 TB network-of-datasets containing 744 full-domain samples from 34 high-fidelity direct numerical simulations, which addresses the current limited availability of 3D high-fidelity reacting and non-reacting compressible turbulent flow simulation data. With this data, we benchmark a total of 49 variations of five deep learning approaches for 3D super-resolution - which can be applied for improving scientific imaging, simulations, turbulence models, as well as in computer vision applications. We perform neural scaling analysis on these models to examine the performance of different machine learning (ML) approaches, including two scientific ML techniques. We demonstrate that (i) predictive performance can scale with model size and cost, (ii) architecture matters significantly, especially for smaller models, and (iii) the b
    
[^116]: InvestLM：使用金融领域指导调优的大型语言模型

    InvestLM: A Large Language Model for Investment using Financial Domain Instruction Tuning. (arXiv:2309.13064v1 [q-fin.GN])

    [http://arxiv.org/abs/2309.13064](http://arxiv.org/abs/2309.13064)

    InvestLM是一个通过对金融领域指导数据集进行调优的大型语言模型，具有强大的理解金融文本的能力，并在投资相关问题上提供有帮助的回答。金融专家评价其与最先进的商业模型可媲美，并在金融NLP基准问题上展现了强大的泛化能力。

    

    我们介绍了一种新的金融领域大型语言模型InvestLM，该模型通过精心策划的与金融投资相关的指导数据集对LLaMA-65B进行调优。受到“少即是多”的启发，我们手动策划了一个既小又多样的指导数据集，涵盖了从特许金融分析师（CFA）考试问题到SEC文件和Stackexchange量化金融讨论的广泛金融相关主题。InvestLM表现出良好的理解金融文本的能力，并对投资相关问题提供有帮助的回答。包括对冲基金经理和研究分析师在内的金融专家将InvestLM的回答评价为与最先进的商业模型（GPT-3.5、GPT-4和Claude-2）可媲美。对一组金融NLP基准问题进行零样本评估表明了其强大的泛化能力。从研究角度来看，本研究表明可以使用高质量的领域特定LLM进行调优。

    We present a new financial domain large language model, InvestLM, tuned on LLaMA-65B (Touvron et al., 2023), using a carefully curated instruction dataset related to financial investment. Inspired by less-is-more-for-alignment (Zhou et al., 2023), we manually curate a small yet diverse instruction dataset, covering a wide range of financial related topics, from Chartered Financial Analyst (CFA) exam questions to SEC filings to Stackexchange quantitative finance discussions. InvestLM shows strong capabilities in understanding financial text and provides helpful responses to investment related questions. Financial experts, including hedge fund managers and research analysts, rate InvestLM's response as comparable to those of state-of-the-art commercial models (GPT-3.5, GPT-4 and Claude-2). Zero-shot evaluation on a set of financial NLP benchmarks demonstrates strong generalizability. From a research perspective, this work suggests that a high-quality domain specific LLM can be tuned usin
    
[^117]: 循环时间修订图网络

    Recurrent Temporal Revision Graph Networks. (arXiv:2309.12694v1 [cs.LG])

    [http://arxiv.org/abs/2309.12694](http://arxiv.org/abs/2309.12694)

    该论文提出了一种循环时间修订图网络的新框架，通过使用循环神经网络和逐节点的隐藏状态，将所有历史邻居的信息整合起来，以获得每个节点的完整邻居信息。该框架在理论表现能力和实际应用中都取得了优越的性能。

    

    与静态图相比，时间图能更准确地建模许多现实场景。然而，邻居聚合是图网络的关键构建块，对于时间图来说，目前是从静态图直接拓展而来的。当在这种聚合过程中涉及所有历史邻居时，计算成本可能非常高昂。实际上，通常只涉及最近邻居的一个子集。然而，这种子抽样会导致邻居信息不完整和有偏。为了解决这个限制，我们提出了一个新颖的框架，用于时间邻居聚合，它使用循环神经网络和逐节点的隐藏状态，以获得每个节点的完整邻居信息。我们证明了该框架具有优越的理论表现能力，并在真实世界应用中展示了其最先进的性能。值得注意的是，它实现了显著的+9.6%的改进。

    Temporal graphs offer more accurate modeling of many real-world scenarios than static graphs. However, neighbor aggregation, a critical building block of graph networks, for temporal graphs, is currently straightforwardly extended from that of static graphs. It can be computationally expensive when involving all historical neighbors during such aggregation. In practice, typically only a subset of the most recent neighbors are involved. However, such subsampling leads to incomplete and biased neighbor information. To address this limitation, we propose a novel framework for temporal neighbor aggregation that uses the recurrent neural network with node-wise hidden states to integrate information from all historical neighbors for each node to acquire the complete neighbor information. We demonstrate the superior theoretical expressiveness of the proposed framework as well as its state-of-the-art performance in real-world applications. Notably, it achieves a significant +9.6% improvement o
    
[^118]: 多模态深度学习用于科学成像解释

    Multimodal Deep Learning for Scientific Imaging Interpretation. (arXiv:2309.12460v1 [cs.LG])

    [http://arxiv.org/abs/2309.12460](http://arxiv.org/abs/2309.12460)

    本研究提出了一种多模态深度学习框架，通过模拟人类与扫描电子显微镜图像的交互，利用文本和视觉数据进行精细数据合成和评估。该模型（GlassLLaVA）能够准确解释、识别关键特征和检测以前未见的SEM图像中的缺陷，同时引入了适用于多种科学成像应用的灵活评估指标。

    

    在科学成像领域，解释视觉数据常常需要人类专业知识和对主题材料的深入理解的复杂组合。本研究提出了一种新的方法，通过多模态深度学习框架来模拟并评估与扫描电子显微镜（SEM）图像的人类交互，特别是玻璃材料图像。我们的方法利用从同行评议的文章中收集的文本和视觉数据，进一步借助 GPT-4 的能力进行精细数据合成和评估。尽管存在诸多挑战，如细微的解释和专业数据集的有限可用性，但我们的模型（GlassLLaVA）在制定准确的解释、识别关键特征和检测以前未见的SEM图像中的缺陷方面表现出色。此外，我们引入了适用于多种科学成像应用的灵活评估指标，使得进行综合评估成为可能。

    In the domain of scientific imaging, interpreting visual data often demands an intricate combination of human expertise and deep comprehension of the subject materials. This study presents a novel methodology to linguistically emulate and subsequently evaluate human-like interactions with Scanning Electron Microscopy (SEM) images, specifically of glass materials. Leveraging a multimodal deep learning framework, our approach distills insights from both textual and visual data harvested from peer-reviewed articles, further augmented by the capabilities of GPT-4 for refined data synthesis and evaluation. Despite inherent challenges--such as nuanced interpretations and the limited availability of specialized datasets--our model (GlassLLaVA) excels in crafting accurate interpretations, identifying key features, and detecting defects in previously unseen SEM images. Moreover, we introduce versatile evaluation metrics, suitable for an array of scientific imaging applications, which allows for
    
[^119]: Fairness Hub技术简报: AUC Gap

    Fairness Hub Technical Briefs: AUC Gap. (arXiv:2309.12371v1 [cs.LG])

    [http://arxiv.org/abs/2309.12371](http://arxiv.org/abs/2309.12371)

    本论文介绍了一种称为AUC Gap的指标，它可以测量AI/ML模型在不同子群体中的性能差异，从而实现非二元的公平评估，为实现共同目标提供了基准和策略分享的基础。

    

    为了测量偏见，我们鼓励团队考虑使用AUC Gap：子群体（例如性别、种族、SES、先前知识）的最高和最低测试AUC的绝对差异。它不依赖于AI/ML算法，并捕捉模型在任意数量的子群体中的性能差异，从而实现了非二元的公平评估，例如针对交叉身份群体。LEVI团队在追求在低收入中学中将数学成就翻倍的共同目标时，使用各种AI/ML模型。确保这些模型在许多不同背景下收集的数据集上训练时不引入或放大偏见，对于实现LEVI目标非常重要。我们在此提供了一种灵活且易于计算的模型偏见度量方法，供所有LEVI团队使用，以创建一个共同的基准和分析基础，用于共享不同团队已经采取的策略。

    To measure bias, we encourage teams to consider using AUC Gap: the absolute difference between the highest and lowest test AUC for subgroups (e.g., gender, race, SES, prior knowledge). It is agnostic to the AI/ML algorithm used and it captures the disparity in model performance for any number of subgroups, which enables non-binary fairness assessments such as for intersectional identity groups. The LEVI teams use a wide range of AI/ML models in pursuit of a common goal of doubling math achievement in low-income middle schools. Ensuring that the models, which are trained on datasets collected in many different contexts, do not introduce or amplify biases is important for achieving the LEVI goal. We offer here a versatile and easy-to-compute measure of model bias for all LEVI teams in order to create a common benchmark and an analytical basis for sharing what strategies have worked for different teams.
    
[^120]: 基于机器学习的微型机器学习综述

    A Machine Learning-oriented Survey on Tiny Machine Learning. (arXiv:2309.11932v1 [cs.LG])

    [http://arxiv.org/abs/2309.11932](http://arxiv.org/abs/2309.11932)

    本综述旨在提供关于微型机器学习（TinyML）的学习算法的最新综述，重点关注其在人工智能领域的应用与潜在贡献。

    

    微型机器学习（TinyML）的出现通过推动资源受限的物联网硬件设备与基于学习的软件架构的联合设计，积极革命了人工智能领域。TinyML在第四和第五次工业革命中发挥着重要作用，帮助社会、经济和个人应用有效的人工智能融入计算技术（如智慧城市、汽车和医疗机器人）。由于其多学科性质，TinyML领域已经从许多不同的角度进行了研究：本综述希望提供一个最新的综述，重点关注在基于TinyML的解决方案中的所有学习算法。本综述基于系统综述和元分析的首选报告项（PRISMA）方法流程，实现了系统和完整的文献综述。具体而言，首先我们将研究实现TinyML基础解决方案的三种不同工作流程。

    The emergence of Tiny Machine Learning (TinyML) has positively revolutionized the field of Artificial Intelligence by promoting the joint design of resource-constrained IoT hardware devices and their learning-based software architectures. TinyML carries an essential role within the fourth and fifth industrial revolutions in helping societies, economies, and individuals employ effective AI-infused computing technologies (e.g., smart cities, automotive, and medical robotics). Given its multidisciplinary nature, the field of TinyML has been approached from many different angles: this comprehensive survey wishes to provide an up-to-date overview focused on all the learning algorithms within TinyML-based solutions. The survey is based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodological flow, allowing for a systematic and complete literature survey. In particular, firstly we will examine the three different workflows for implementing a TinyML-bas
    
[^121]: TMac：用于声音事件分类的时态多模态图学习

    TMac: Temporal Multi-Modal Graph Learning for Acoustic Event Classification. (arXiv:2309.11845v1 [cs.SD])

    [http://arxiv.org/abs/2309.11845](http://arxiv.org/abs/2309.11845)

    TMac是一个时态多模态图学习方法，用于声音事件分类。它通过图学习的方式对多模态数据中的时态信息进行建模，提高了声音事件分类的性能。

    

    在这个数字时代，音频视觉数据随处可见，这对于对它们开发的深度学习模型提出了更高的要求。有效处理多模态数据的信息是更好的音频视觉模型的关键。我们观察到这些音频视觉数据自然具有时间属性，例如视频中每一帧的时间信息。更具体地说，这些数据根据音频和视觉线索自然形成多模态，并且严格按照时间顺序进行。这表明，在多模态声音事件建模中，时态信息对于内部和跨模态都很重要。然而，现有的方法独立地处理每个模态特征，仅简单地将它们融合在一起，忽视了时态关系的挖掘，从而导致次优的性能。出于这个动机，我们提出了一种用于声音事件分类的时态多模态图学习方法，称为TMac，通过图学习对这种时态信息进行建模。

    Audiovisual data is everywhere in this digital age, which raises higher requirements for the deep learning models developed on them. To well handle the information of the multi-modal data is the key to a better audiovisual modal. We observe that these audiovisual data naturally have temporal attributes, such as the time information for each frame in the video. More concretely, such data is inherently multi-modal according to both audio and visual cues, which proceed in a strict chronological order. It indicates that temporal information is important in multi-modal acoustic event modeling for both intra- and inter-modal. However, existing methods deal with each modal feature independently and simply fuse them together, which neglects the mining of temporal relation and thus leads to sub-optimal performance. With this motivation, we propose a Temporal Multi-modal graph learning method for Acoustic event Classification, called TMac, by modeling such temporal information via graph learning
    
[^122]: 图中社区检测的综合评述

    A Comprehensive Review of Community Detection in Graphs. (arXiv:2309.11798v1 [cs.SI])

    [http://arxiv.org/abs/2309.11798](http://arxiv.org/abs/2309.11798)

    本综述对图中的社区检测进行了全面回顾。社区结构是真实世界图的重要特征，社区检测方法的研究具有社会学、生物学和计算机科学方面的应用。尽管科学家们做出了努力，但尚未找到一个令人满意的解决方案。本综述介绍了社区结构的概念，各种社区检测方法，以及在各种网络中的实际应用。

    

    复杂网络研究显著促进了我们对真实世界图的社区结构的理解，这是一个具有挑战性的问题，在社会学、生物学和计算机科学中具有应用价值。尽管跨学科科学家社区的努力，但尚未找到一个令人满意的解决方案。本综述详细介绍了图中社区检测的主题，这对于理解复杂系统的组织和功能起着关键的作用。首先，我们介绍社区结构的概念，它指的是将顶点划分为具有强内部连接和较弱连接的集群。然后，我们对各种社区检测方法进行了彻底的阐述，包括我们设计的一种新方法。此外，我们还探讨了社区检测在各种网络中的真实应用。

    The study of complex networks has significantly advanced our understanding of community structures which serves as a crucial feature of real-world graphs. Detecting communities in graphs is a challenging problem with applications in sociology, biology, and computer science. Despite the efforts of an interdisciplinary community of scientists, a satisfactory solution to this problem has not yet been achieved. This review article delves into the topic of community detection in graphs, which serves as a crucial role in understanding the organization and functioning of complex systems. We begin by introducing the concept of community structure, which refers to the arrangement of vertices into clusters, with strong internal connections and weaker connections between clusters. Then, we provide a thorough exposition of various community detection methods, including a new method designed by us. Additionally, we explore real-world applications of community detection in diverse networks. In concl
    
[^123]: 高效的选核激励机制用于联邦学习中的数据共享

    Efficient Core-selecting Incentive Mechanism for Data Sharing in Federated Learning. (arXiv:2309.11722v1 [cs.GT])

    [http://arxiv.org/abs/2309.11722](http://arxiv.org/abs/2309.11722)

    本文介绍了一个联邦学习的数据共享博弈模型，并利用博弈论的方法设计了一个选核激励机制，以激励真实输入数据并促进稳定合作。

    

    联邦学习是一种分布式机器学习系统，利用参与者的数据训练一个改进的全局模型。在联邦学习中，参与者合作训练一个全局模型，并且他们将获得全局模型和支付。理性的参与者试图最大化自己的个体效用，除非他们基于数据质量获得令人满意的支付，否则他们将不会真实地输入高质量的数据。此外，联邦学习受益于参与者的合作贡献。因此，如何建立一个既激励真实输入数据又促进稳定合作的激励机制已成为一个重要的问题。在本文中，我们引入了一个联邦学习的数据共享博弈模型，并运用博弈论的方法来设计一个选核激励机制，利用合作博弈中的一个流行概念，即核心。

    Federated learning is a distributed machine learning system that uses participants' data to train an improved global model. In federated learning, participants cooperatively train a global model, and they will receive the global model and payments. Rational participants try to maximize their individual utility, and they will not input their high-quality data truthfully unless they are provided with satisfactory payments based on their data quality. Furthermore, federated learning benefits from the cooperative contributions of participants. Accordingly, how to establish an incentive mechanism that both incentivizes inputting data truthfully and promotes stable cooperation has become an important issue to consider. In this paper, we introduce a data sharing game model for federated learning and employ game-theoretic approaches to design a core-selecting incentive mechanism by utilizing a popular concept in cooperative games, the core. In federated learning, the core can be empty, resulti
    
[^124]: 对多模态大规模语言模型中的灾难性遗忘进行的研究

    Investigating the Catastrophic Forgetting in Multimodal Large Language Models. (arXiv:2309.10313v1 [cs.CL])

    [http://arxiv.org/abs/2309.10313](http://arxiv.org/abs/2309.10313)

    本论文针对多模态大规模语言模型中的灾难性遗忘问题进行研究，引入了EMT方法来评估灾难性遗忘，并发现在标准图像分类任务上，几乎所有评估的模型都无法保持与视觉编码器相同的性能水平。研究结果表明，早期微调阶段对性能至关重要。

    

    在GPT4的成功之后，多模态大规模语言模型（MLLM）研究引起了广泛关注。这一研究方向侧重于通过微调预训练的LLM和视觉模型来开发通用的LLM。然而，灾难性遗忘，即微调模型无法保持与预训练模型相似的性能水平，仍然是多模态LLM（MLLM）中的一个固有问题。本文介绍了EMT：用于评估MLLM中灾难性遗忘的评估方法，将每个MLLM作为一个图像分类器进行评估。我们首先应用EMT来评估几个开源的微调MLLM，并发现几乎所有评估的MLLM在标准图像分类任务上无法保持与他们的视觉编码器相同的性能水平。此外，我们继续微调LLaVA，一种MLLM，并利用EMT来评估整个微调过程中的性能。有趣的是，我们的结果表明，早期的微调阶段是关键的，过早停止微调可能导致低性能的模型。

    Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-sta
    
[^125]: 傅里叶变换和软阈值的领域泛化方法

    Domain Generalization with Fourier Transform and Soft Thresholding. (arXiv:2309.09866v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2309.09866](http://arxiv.org/abs/2309.09866)

    本研究提出了一种基于傅里叶变换和软阈值的领域泛化方法，用于提高神经网络在不同源上的视网膜底图图像分割任务中的性能。

    

    领域泛化旨在训练模型以适应多个源领域，并实现对未见过的目标领域的良好泛化能力。在许多领域泛化方法中，基于傅里叶变换的方法因为利用傅里叶变换来捕捉数据中的重要模式和规律，从而使模型对领域转移更加鲁棒而备受青睐。主流的基于傅里叶变换的领域泛化方法在源图像和目标图像之间交换傅里叶幅度谱，同时保留相位谱。然而，它忽略了幅度谱中的背景干扰。为了克服这个局限性，我们在傅里叶域引入了一个软阈值函数。我们将这个新设计的算法应用于视网膜底图图像分割，这对于诊断眼科疾病很重要，但由于领域转移，神经网络在不同源上的性能可能会下降。

    Domain generalization aims to train models on multiple source domains so that they can generalize well to unseen target domains. Among many domain generalization methods, Fourier-transform-based domain generalization methods have gained popularity primarily because they exploit the power of Fourier transformation to capture essential patterns and regularities in the data, making the model more robust to domain shifts. The mainstream Fourier-transform-based domain generalization swaps the Fourier amplitude spectrum while preserving the phase spectrum between the source and the target images. However, it neglects background interference in the amplitude spectrum. To overcome this limitation, we introduce a soft-thresholding function in the Fourier domain. We apply this newly designed algorithm to retinal fundus image segmentation, which is important for diagnosing ocular diseases but the neural network's performance can degrade across different sources due to domain shifts. The proposed 
    
[^126]: 阐明扩展反向时间随机微分方程在扩散模型中的解空间

    Elucidating the solution space of extended reverse-time SDE for diffusion models. (arXiv:2309.06169v1 [cs.LG])

    [http://arxiv.org/abs/2309.06169](http://arxiv.org/abs/2309.06169)

    这项工作介绍了扩展反向时间随机微分方程（ER SDE）用于解决扩散模型中的采样问题，并提供了精确解和高阶近似解，并解释了在快速采样方面ODE求解器优于SDE求解器的数学洞察力。

    

    扩散模型在各种生成建模任务中展示出强大的图像生成能力。然而，它们的主要限制在于采样速度较慢，需要通过大型神经网络进行数百或数千次连续函数评估才能生成高质量的图像。从扩散模型中采样可以看作是解相应的随机微分方程（SDE）或常微分方程（ODE）。在这项工作中，我们将采样过程形式化为扩展反向时间 SDE（ER SDE），将之前对ODE和SDE的探索统一起来。利用ER SDE解的半线性结构，我们为VP SDE提供了精确解和任意高阶近似解，为VE SDE提供了高阶近似解。基于ER SDE的解空间，我们揭示了ODE求解器在快速采样方面优于SDE求解器的数学洞察力。此外，我们还揭示了VP SDE求解器与其VE SDE求解器在性能上相当。

    Diffusion models (DMs) demonstrate potent image generation capabilities in various generative modeling tasks. Nevertheless, their primary limitation lies in slow sampling speed, requiring hundreds or thousands of sequential function evaluations through large neural networks to generate high-quality images. Sampling from DMs can be seen as solving corresponding stochastic differential equations (SDEs) or ordinary differential equations (ODEs). In this work, we formulate the sampling process as an extended reverse-time SDE (ER SDE), unifying prior explorations into ODEs and SDEs. Leveraging the semi-linear structure of ER SDE solutions, we offer exact solutions and arbitrarily high-order approximate solutions for VP SDE and VE SDE, respectively. Based on the solution space of the ER SDE, we yield mathematical insights elucidating the superior performance of ODE solvers over SDE solvers in terms of fast sampling. Additionally, we unveil that VP SDE solvers stand on par with their VE SDE c
    
[^127]: 基于时序预测损失的感知流中基于流的主动学习方法

    Stream-based Active Learning by Exploiting Temporal Properties in Perception with Temporal Predicted Loss. (arXiv:2309.05517v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.05517](http://arxiv.org/abs/2309.05517)

    该论文提出了一种利用时序特性的感知流中的基于流的主动学习方法，并引入了一种新的时序预测损失（TPL）方法。通过实验证明，该方法显著改善了选择的多样性，并且与基于不确定性的方法相比，是一种更常见的感知应用中的AL方法。

    

    主动学习（AL）通过智能选择要标记的实例，减少了训练机器学习模型所需的标记数据量。传统的基于池的AL要求所有数据都集中在数据中心，这在深度学习中需要越来越多的数据时可能会面临挑战。然而，在移动设备和机器人（如自动驾驶汽车）上进行感知传感器流的数据过滤后再到达数据中心是可行的。我们在这项工作中利用了这种图像流的时序特性，并提出了一种新的时序预测损失（TPL）方法。为了正确评估基于流的设置，我们引入了GTA V街道和A2D2街道数据集，并将其公开。我们的实验证明了我们的方法在提高选择的多样性方面显著优于基于不确定性的方法。由于基于池的方法在感知应用中更为常见，我们提出了一种比较基于池和基于流的AL的概念。

    Active learning (AL) reduces the amount of labeled data needed to train a machine learning model by intelligently choosing which instances to label. Classic pool-based AL requires all data to be present in a datacenter, which can be challenging with the increasing amounts of data needed in deep learning. However, AL on mobile devices and robots, like autonomous cars, can filter the data from perception sensor streams before reaching the datacenter. We exploited the temporal properties for such image streams in our work and proposed the novel temporal predicted loss (TPL) method. To evaluate the stream-based setting properly, we introduced the GTA V streets and the A2D2 streets dataset and made both publicly available. Our experiments showed that our approach significantly improves the diversity of the selection while being an uncertainty-based method. As pool-based approaches are more common in perception applications, we derived a concept for comparing pool-based and stream-based AL, 
    
[^128]: Spiking Neural Network联合课程学习策略的训练

    Training of Spiking Neural Network joint Curriculum Learning Strategy. (arXiv:2309.04737v1 [cs.LG])

    [http://arxiv.org/abs/2309.04737](http://arxiv.org/abs/2309.04737)

    该论文提出了一种将课程学习引入脉冲神经网络的训练模型，使其更类似于人类学习过程，并提高了其生物解释性。

    

    从简单到复杂，逐渐引入难度的概念是人类学习的自然过程。脉冲神经网络（SNNs）旨在模拟人类信息处理的方式，但目前的SNNs模型将所有样本视为平等，这与人类学习的原则不符，并忽视了SNNs的生物合理性。为了解决这个问题，我们提出了一个将课程学习（CL）引入SNNs的CL-SNN模型，使SNNs更像人类学习，并提供更高的生物解释性。CL是一种训练策略，提倡在逐渐引入更具挑战性的数据之前向模型展示更容易的数据，模拟了人类学习的过程。我们使用具有信心感知的损失来衡量和处理不同难度水平的样本。通过学习不同样本的置信度，模型自动减少了难样本对参数优化的贡献。我们在实验中进行了研究

    Starting with small and simple concepts, and gradually introducing complex and difficult concepts is the natural process of human learning. Spiking Neural Networks (SNNs) aim to mimic the way humans process information, but current SNNs models treat all samples equally, which does not align with the principles of human learning and overlooks the biological plausibility of SNNs. To address this, we propose a CL-SNN model that introduces Curriculum Learning(CL) into SNNs, making SNNs learn more like humans and providing higher biological interpretability. CL is a training strategy that advocates presenting easier data to models before gradually introducing more challenging data, mimicking the human learning process. We use a confidence-aware loss to measure and process the samples with different difficulty levels. By learning the confidence of different samples, the model reduces the contribution of difficult samples to parameter optimization automatically. We conducted experiments on st
    
[^129]: RTLLM：用于大规模语言模型设计RTL生成的开源基准测试

    RTLLM: An Open-Source Benchmark for Design RTL Generation with Large Language Model. (arXiv:2308.05345v1 [cs.LG])

    [http://arxiv.org/abs/2308.05345](http://arxiv.org/abs/2308.05345)

    RTLLM是一个用自然语言指令生成设计RTL的开源基准测试，旨在解决现有工作中目标设计简单且规模小的问题，并提供对基于LLM的解决方案进行设计质量的定量评估。

    

    受到像ChatGPT这样的大型语言模型（LLMs）的最新成功的启发，研究人员开始探索采用LLMs进行敏捷硬件设计，例如基于自然语言指令生成设计RTL。然而，在现有的工作中，目标设计都相对简单且规模较小，并由作者自己提出，这使得在不同的LLMs解决方案之间进行公平比较具有挑战性。此外，许多先前的工作只关注设计的正确性，而没有评估生成的设计RTL的设计质量。在这项工作中，我们提出了一个名为RTLLM的开源基准测试，用于使用自然语言指令生成设计RTL。为了系统评估自动生成的设计RTL，我们总结了三个渐进目标，即语法目标、功能目标和设计质量目标。该基准测试可以自动提供对任何给定基于LLM的解决方案的定量评估。

    Inspired by the recent success of large language models (LLMs) like ChatGPT, researchers start to explore the adoption of LLMs for agile hardware design, such as generating design RTL based on natural-language instructions. However, in existing works, their target designs are all relatively simple and in a small scale, and proposed by the authors themselves, making a fair comparison among different LLM solutions challenging. In addition, many prior works only focus on the design correctness, without evaluating the design qualities of generated design RTL. In this work, we propose an open-source benchmark named RTLLM, for generating design RTL with natural language instructions. To systematically evaluate the auto-generated design RTL, we summarized three progressive goals, named syntax goal, functionality goal, and design quality goal. This benchmark can automatically provide a quantitative evaluation of any given LLM-based solution. Furthermore, we propose an easy-to-use yet surprisin
    
[^130]: 一个Epoch就足够进行多层次超参数优化吗？

    Is One Epoch All You Need For Multi-Fidelity Hyperparameter Optimization?. (arXiv:2307.15422v1 [cs.LG])

    [http://arxiv.org/abs/2307.15422](http://arxiv.org/abs/2307.15422)

    传统的基准线在多层次超参数优化中取得了与其他方法类似的结果，并大幅减少计算成本。研究人员应该使用该基准线并扩大MF-HPO基准测试的多样性。

    

    超参数优化（HPO）对于微调机器学习模型至关重要，但计算成本很高。为了降低成本，多层次超参数优化（MF-HPO）利用学习过程中的中间准确性级别，并在学习早期丢弃低性能模型。我们在经典基准数据上将各种代表性的MF-HPO方法与简单的基准线进行了比较。基准线是在训练了仅一个Epoch后丢弃除Top-K之外的所有模型，然后进一步训练以选择最佳模型。令人惊讶的是，这个基准线与其对应的方法取得了类似的结果，而计算成本减少了一个数量级。在分析基准数据的学习曲线时，我们观察到了几个占主导地位的学习曲线，这解释了我们基准线的成功。这表明研究人员应该（1）在基准测试中始终使用建议的基准线，并且（2）扩大MF-HPO基准测试的多样性，包括更复杂的情况。

    Hyperparameter optimization (HPO) is crucial for fine-tuning machine learning models but can be computationally expensive. To reduce costs, Multi-fidelity HPO (MF-HPO) leverages intermediate accuracy levels in the learning process and discards low-performing models early on. We compared various representative MF-HPO methods against a simple baseline on classical benchmark data. The baseline involved discarding all models except the Top-K after training for only one epoch, followed by further training to select the best model. Surprisingly, this baseline achieved similar results to its counterparts, while requiring an order of magnitude less computation. Upon analyzing the learning curves of the benchmark data, we observed a few dominant learning curves, which explained the success of our baseline. This suggests that researchers should (1) always use the suggested baseline in benchmarks and (2) broaden the diversity of MF-HPO benchmarks to include more complex cases.
    
[^131]: 使用受约束的自编码器学习非线性投影，用于动态系统的降阶建模

    Learning Nonlinear Projections for Reduced-Order Modeling of Dynamical Systems using Constrained Autoencoders. (arXiv:2307.15288v1 [math.DS])

    [http://arxiv.org/abs/2307.15288](http://arxiv.org/abs/2307.15288)

    该论文提出了一种使用受约束的自编码器学习非线性投影的方法，用于动态系统的降阶建模。这种方法可以有效地对非线性动力学系统进行逼近，并解决了在流形附近对瞬态动力学建模时所面临的问题。

    

    最近开发的降阶建模技术旨在通过从数据中学习的低维流形对非线性动态系统进行逼近。这是一种在过渡后条件中对动力学建模的有效方法，其中初始条件和其他干扰的效应已经衰减。然而，对于需要实时控制和预测应用的基于流形的瞬态动力学建模来说，快速动态和非正常敏感机制的影响使得问题变得复杂。为了开始解决这些问题，我们引入了由从数据中学习的约束自编码器神经网络描述的一种参数化非线性投影类。我们的架构使用可逆激活函数和对偶权重矩阵，以确保编码器是解码器的左逆。我们还引入了新的具有动态感知性的成本函数，以促进学习考虑斜投影纤维的能力。

    Recently developed reduced-order modeling techniques aim to approximate nonlinear dynamical systems on low-dimensional manifolds learned from data. This is an effective approach for modeling dynamics in a post-transient regime where the effects of initial conditions and other disturbances have decayed. However, modeling transient dynamics near an underlying manifold, as needed for real-time control and forecasting applications, is complicated by the effects of fast dynamics and nonnormal sensitivity mechanisms. To begin to address these issues, we introduce a parametric class of nonlinear projections described by constrained autoencoder neural networks in which both the manifold and the projection fibers are learned from data. Our architecture uses invertible activation functions and biorthogonal weight matrices to ensure that the encoder is a left inverse of the decoder. We also introduce new dynamics-aware cost functions that promote learning of oblique projection fibers that account
    
[^132]: TinyMetaFed: 高效的用于TinyML的联邦元学习

    TinyMetaFed: Efficient Federated Meta-Learning for TinyML. (arXiv:2307.06822v1 [cs.LG])

    [http://arxiv.org/abs/2307.06822](http://arxiv.org/abs/2307.06822)

    TinyMetaFed是一个适用于TinyML的高效联邦元学习框架，通过协同训练神经网络初始化，在小型设备上能够快速微调，同时实现通信节省和隐私保护。

    

    Tiny Machine Learning (TinyML)领域在使得机器学习在低功耗设备（如微控制器）上实现方面取得了重大进展。这些微型设备的普及引发了一个问题，即聚合它们的知识是否能够使TinyML应用受益。联邦元学习是这个问题的一个有前景的答案，因为它解决了现实世界中标记数据的稀缺性和设备之间的异构数据分布。然而，部署TinyML硬件面临着独特的资源限制，现有方法由于能源、隐私和通信限制而不实用。我们引入了TinyMetaFed，一个适用于TinyML的模型无关的元学习框架。TinyMetaFed促进了神经网络初始化的协同训练，可以在新设备上快速微调。它通过部分本地重构和Top-P%选择性通信提供通信节省和隐私保护，具有计算效果好。

    The field of Tiny Machine Learning (TinyML) has made substantial advancements in democratizing machine learning on low-footprint devices, such as microcontrollers. The prevalence of these miniature devices raises the question of whether aggregating their knowledge can benefit TinyML applications. Federated meta-learning is a promising answer to this question, as it addresses the scarcity of labeled data and heterogeneous data distribution across devices in the real world. However, deploying TinyML hardware faces unique resource constraints, making existing methods impractical due to energy, privacy, and communication limitations. We introduce TinyMetaFed, a model-agnostic meta-learning framework suitable for TinyML. TinyMetaFed facilitates collaborative training of a neural network initialization that can be quickly fine-tuned on new devices. It offers communication savings and privacy protection through partial local reconstruction and Top-P% selective communication, computational eff
    
[^133]: 多任务学习以提高相干光系统中神经网络均衡器的泛化能力

    Multi-Task Learning to Enhance Generazability of Neural Network Equalizers in Coherent Optical Systems. (arXiv:2307.05374v1 [eess.SP])

    [http://arxiv.org/abs/2307.05374](http://arxiv.org/abs/2307.05374)

    首次提出了多任务学习用于提高相干系统中基于神经网络的均衡器的灵活性，并且通过一个"单一"的基于神经网络的均衡器，在不重新训练的情况下，即使在发射功率、符号速率或传输距离变化的情况下，也能将Q因子提高最高达4 dB。

    

    首次提出了多任务学习用于提高相干系统中基于神经网络的均衡器的灵活性。与常规数字时钟恢复方法相比，一个"单一"的基于神经网络的均衡器在不重新训练的情况下，即使在发射功率、符号速率或传输距离变化的情况下，也能将Q因子提高最高达4 dB。

    For the first time, multi-task learning is proposed to improve the flexibility of NN-based equalizers in coherent systems. A "single" NN-based equalizer improves Q-factor by up to 4 dB compared to CDC, without re-training, even with variations in launch power, symbol rate, or transmission distance.
    
[^134]: ACDNet：基于注意力引导的协同决策网络用于有效的药物推荐

    ACDNet: Attention-guided Collaborative Decision Network for Effective Medication Recommendation. (arXiv:2307.03332v1 [cs.LG])

    [http://arxiv.org/abs/2307.03332](http://arxiv.org/abs/2307.03332)

    本文提出了一种基于注意力引导的协同决策网络（ACDNet）用于药物推荐，通过利用注意力机制和Transformer对患者的健康状况和药物记录进行建模，同时采用协同决策框架，从患者药物记录和具体药物之间的相似性出发，有效地个性化推荐药物。

    

    使用电子健康记录（EHR）进行药物推荐是具有挑战性的，因为涉及复杂的医疗数据。当前的方法从患者EHR中提取纵向信息以个性化推荐。然而，现有模型常常缺乏足够的患者表示，并忽视了考虑患者药物记录和具体药物之间相似性的重要性。因此，本文提出了一种基于注意力引导的协同决策网络（ACDNet）用于药物推荐。具体而言，ACDNet利用注意力机制和Transformer在全局和局部层面对患者的健康状况和药物记录进行建模。ACDNet还采用协同决策框架，利用药物记录与药物表示之间的相似性来促进推荐过程。在两个大型医疗数据集MIMIC-III和MIMIC-IV上的实验证明了ACDNet的有效性。

    Medication recommendation using Electronic Health Records (EHR) is challenging due to complex medical data. Current approaches extract longitudinal information from patient EHR to personalize recommendations. However, existing models often lack sufficient patient representation and overlook the importance of considering the similarity between a patient's medication records and specific medicines. Therefore, an Attention-guided Collaborative Decision Network (ACDNet) for medication recommendation is proposed in this paper. Specifically, ACDNet utilizes attention mechanism and Transformer to effectively capture patient health conditions and medication records by modeling their historical visits at both global and local levels. ACDNet also employs a collaborative decision framework, utilizing the similarity between medication records and medicine representation to facilitate the recommendation process. The experimental results on two extensive medical datasets, MIMIC-III and MIMIC-IV, cle
    
[^135]: 从文本生成艺术性的影动图

    Synthesizing Artistic Cinemagraphs from Text. (arXiv:2307.03190v1 [cs.CV])

    [http://arxiv.org/abs/2307.03190](http://arxiv.org/abs/2307.03190)

    本论文介绍了一种通过文本描述来创建艺术性影动图的自动化方法。通过合成图像双胞胎，即一对艺术图像和与之对齐的真实图像，可以同时满足艺术风格和外观的要求并简化动作分析。同时，利用现有数据集可以准确地分割真实图像并预测合理的运动。

    

    我们介绍了一种全自动的方法，通过文本描述来创建艺术性的影动图。在处理虚构元素和艺术风格的提示时，这是一项特别具有挑战性的任务，因为需要解释这些图像的语义和动作的复杂性。现有的单图动画方法在艺术性输入方面存在不足，而最近的基于文本的视频方法常常引入时间不一致性，难以使某些区域保持静态。为了应对这些挑战，我们提出了一种通过单个文本提示合成图像双胞胎的思想，即艺术图像和其与像素对齐的自然外观配对。虽然艺术图像描绘了我们在文本提示中详细描述的风格和外观，但真实的对应图像大大简化了布局和动作分析。利用现有的自然图像和视频数据集，我们可以准确地分割出真实图像并根据语义信息预测出合理的运动。

    We introduce Artistic Cinemagraph, a fully automated method for creating cinemagraphs from text descriptions - an especially challenging task when prompts feature imaginary elements and artistic styles, given the complexity of interpreting the semantics and motions of these images. Existing single-image animation methods fall short on artistic inputs, and recent text-based video methods frequently introduce temporal inconsistencies, struggling to keep certain regions static. To address these challenges, we propose an idea of synthesizing image twins from a single text prompt - a pair of an artistic image and its pixel-aligned corresponding natural-looking twin. While the artistic image depicts the style and appearance detailed in our text prompt, the realistic counterpart greatly simplifies layout and motion analysis. Leveraging existing natural image and video datasets, we can accurately segment the realistic image and predict plausible motion given the semantic information. The predi
    
[^136]: LLQL: 逻辑似然 Q-Learning 用于增强学习

    LLQL: Logistic Likelihood Q-Learning for Reinforcement Learning. (arXiv:2307.02345v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.02345](http://arxiv.org/abs/2307.02345)

    本研究通过研究在线和离线增强学习中 Bellman 近似误差的分布发现，Bellman 误差符合逻辑分布。基于这一发现，本研究提出了一种使用 Logistic 最大似然函数作为替代方法的方案，并通过实验证明了其有效性。

    

    现代增强学习（RL）可以分为在线和离线两种变体。作为在线和离线 RL 的关键方面，当前对 Bellman 方程的研究主要集中在优化技术和性能增强上，而不是探索 Bellman 误差的固有结构特性，如其分布特征。本研究通过对 Bellman 方程进行迭代探索，研究了在线 RL 和离线 RL 中 Bellman 近似误差的分布情况。我们观察到无论是在线 RL 还是离线 RL，Bellman 误差都符合逻辑分布。基于这一发现，本研究采用 Logistic 最大似然函数（LLoss）作为常用的 MSE Loss 的替代方法，假设 Bellman 误差服从正态分布。通过广泛的数值实验验证了我们的假设，在不同的在线和离线环境中得到了验证。

    Modern reinforcement learning (RL) can be categorized into online and offline variants. As a pivotal aspect of both online and offline RL, current research on the Bellman equation revolves primarily around optimization techniques and performance enhancement rather than exploring the inherent structural properties of the Bellman error, such as its distribution characteristics. This study investigates the distribution of the Bellman approximation error in both online and offline settings through iterative exploration of the Bellman equation. We observed that both in online RL and offline RL, the Bellman error conforms to a Logistic distribution. Building upon this discovery, this study employed the Logistics maximum likelihood function (LLoss) as an alternative to the commonly used MSE Loss, assuming that Bellman errors adhere to a normal distribution. We validated our hypotheses through extensive numerical experiments across diverse online and offline environments. In particular, we app
    
[^137]: 通过强化学习的编辑生成安全关键场景

    Safety-Critical Scenario Generation Via Reinforcement Learning Based Editing. (arXiv:2306.14131v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.14131](http://arxiv.org/abs/2306.14131)

    提出了一种基于强化学习的场景生成方法，通过顺序编辑生成安全关键场景，克服了维度挑战，并生成了质量更高的场景。

    

    生成安全关键场景对于测试和验证自动驾驶汽车的安全性至关重要。传统的优化技术在维度诅咒和搜索空间的限制上存在问题。为了解决这些挑战，我们提出了一种基于深度强化学习的方法，通过顺序编辑生成场景，例如添加新的代理或修改现有代理的轨迹。我们的框架采用了包含风险和可行性目标的奖励函数。可行性目标利用生成模型，如变分自动编码器，从训练数据集中学习生成参数的可能性；它惩罚生成不太可能的场景。我们的方法克服了维度挑战，并探索了广泛的安全关键场景。我们的评估表明，与以前的方法相比，所提出的方法生成了质量更高的安全关键场景。

    Generating safety-critical scenarios is essential for testing and verifying the safety of autonomous vehicles. Traditional optimization techniques suffer from the curse of dimensionality and limit the search space to fixed parameter spaces. To address these challenges, we propose a deep reinforcement learning approach that generates scenarios by sequential editing, such as adding new agents or modifying the trajectories of the existing agents. Our framework employs a reward function consisting of both risk and plausibility objectives. The plausibility objective leverages generative models, such as a variational autoencoder, to learn the likelihood of the generated parameters from the training datasets; It penalizes the generation of unlikely scenarios. Our approach overcomes the dimensionality challenge and explores a wide range of safety-critical scenarios. Our evaluation demonstrates that the proposed method generates safety-critical scenarios of higher quality compared with previous
    
[^138]: CompanyKG:一种用于公司相似性量化的大规模异构图

    CompanyKG: A Large-Scale Heterogeneous Graph for Company Similarity Quantification. (arXiv:2306.10649v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2306.10649](http://arxiv.org/abs/2306.10649)

    本研究提出了CompanyKG，一种用于公司相似性量化的大规模异构图数据集。通过丰富的公司特征和关系表示，以及多个评估任务的基准测试，为公司相似性量化方法的综合评估提供了支持。

    

    在投资行业中，对于许多目的包括市场映射、竞争对手分析和并购，进行细粒度公司相似性量化通常是至关重要的。我们提出并发布了一个名为CompanyKG的知识图，用于表示和学习多样化的公司特征和关系。具体而言，1.17百万家公司被表示为节点，丰富了公司描述嵌入; 15种不同的公司间关系导致了5106百万个带权重的边。为了实现对公司相似性量化方法的全面评估，我们设计并编译了三个带有注释测试集的评估任务: 相似性预测、竞争对手检索和相似性排序。我们对11种可重现预测方法进行了广泛的基准测试，分为节点、边和节点+边三组。据我们所知，CompanyKG是第一个大规模的异构图数据集

    In the investment industry, it is often essential to carry out fine-grained company similarity quantification for a range of purposes, including market mapping, competitor analysis, and mergers and acquisitions. We propose and publish a knowledge graph, named CompanyKG, to represent and learn diverse company features and relations. Specifically, 1.17 million companies are represented as nodes enriched with company description embeddings; and 15 different inter-company relations result in 51.06 million weighted edges. To enable a comprehensive assessment of methods for company similarity quantification, we have devised and compiled three evaluation tasks with annotated test sets: similarity prediction, competitor retrieval and similarity ranking. We present extensive benchmarking results for 11 reproducible predictive methods categorized into three groups: node-only, edge-only, and node+edge. To the best of our knowledge, CompanyKG is the first large-scale heterogeneous graph dataset or
    
[^139]: 能否通过 ChatGPT 实现智能交通系统？使用强化学习实现混合交通流控制的案例研究

    Can ChatGPT Enable ITS? The Case of Mixed Traffic Control via Reinforcement Learning. (arXiv:2306.08094v1 [cs.AI])

    [http://arxiv.org/abs/2306.08094](http://arxiv.org/abs/2306.08094)

    本文研究探讨使用 ChatGPT 解决混合交通流控制问题，通过大规模用户研究发现 ChatGPT 在某些环境下能够提高成功策略数量

    

    强化学习在智能交通系统中的应用不断增多，同时也凸显了一些关键问题。本文研究使用大型语言模型 ChatGPT 研究是否可以帮助解决复杂的混合交通流控制问题，通过一个大规模的用户研究，发现 ChatGPT 在某些环境下能够增加成功策略数量

    The surge in Reinforcement Learning (RL) applications in Intelligent Transportation Systems (ITS) has contributed to its growth as well as highlighted key challenges. However, defining objectives of RL agents in traffic control and management tasks, as well as aligning policies with these goals through an effective formulation of Markov Decision Process (MDP), can be challenging and often require domain experts in both RL and ITS. Recent advancements in Large Language Models (LLMs) such as GPT-4 highlight their broad general knowledge, reasoning capabilities, and commonsense priors across various domains. In this work, we conduct a large-scale user study involving 70 participants to investigate whether novices can leverage ChatGPT to solve complex mixed traffic control problems. Three environments are tested, including ring road, bottleneck, and intersection. We find ChatGPT has mixed results. For intersection and bottleneck, ChatGPT increases number of successful policies by 150% and 
    
[^140]: 抗干扰约束学习

    Resilient Constrained Learning. (arXiv:2306.02426v1 [cs.LG])

    [http://arxiv.org/abs/2306.02426](http://arxiv.org/abs/2306.02426)

    本论文提出了一个名为“抗干扰约束学习”的方法来解决在部署机器学习解决方案时需要满足除了准确性以外的多个要求，并以平衡从放宽中获得的性能增益与用户定义的放宽成本之间的关系的方式放松学习约束。

    

    在部署机器学习解决方案时，除了准确性之外，它们必须满足多个要求，如公平性、鲁棒性或安全性。这些要求可以通过使用惩罚来隐式地施加，或者通过基于Lagrangian对偶的约束优化方法来显式地施加。无论哪种方式，指定要求都受到妥协和有限的有关数据的先前知识的影响。此外，它们对性能的影响通常只能通过实际解决学习问题来评估。本文提出了一种约束学习方法，该方法在同时解决学习任务的同时调整要求。为此，它以平衡从放宽中获得的性能增益与用户定义的放宽成本之间的关系的方式放松了学习约束。我们将此方法称为具有弹性的约束学习，这是对用于描述生态系统的术语的一种借鉴。

    When deploying machine learning solutions, they must satisfy multiple requirements beyond accuracy, such as fairness, robustness, or safety. These requirements are imposed during training either implicitly, using penalties, or explicitly, using constrained optimization methods based on Lagrangian duality. Either way, specifying requirements is hindered by the presence of compromises and limited prior knowledge about the data. Furthermore, their impact on performance can often only be evaluated by actually solving the learning problem. This paper presents a constrained learning approach that adapts the requirements while simultaneously solving the learning task. To do so, it relaxes the learning constraints in a way that contemplates how much they affect the task at hand by balancing the performance gains obtained from the relaxation against a user-defined cost of that relaxation. We call this approach resilient constrained learning after the term used to describe ecological systems tha
    
[^141]: 预训练Transformer用于对抗性样本提纯

    Pre-trained transformer for adversarial purification. (arXiv:2306.01762v1 [cs.CR])

    [http://arxiv.org/abs/2306.01762](http://arxiv.org/abs/2306.01762)

    本文提出了一个快速防御对抗性攻击的方案RaPiD（Rapid Plug-in Defender），通过预训练的Transformer微调来提纯对抗样本，使其逼近清洁数据分布，实验结果表明，在有限数据情况下，该方法优于最先进的方法。

    

    随着越来越多的深度神经网络被部署为各种日常服务，它们的可靠性至关重要。深度神经网络容易受到对抗性攻击的影响，其中逃避攻击是最普遍的一种。最近的研究通常通过对抗训练或利用大量清洁数据的知识来增强其健壮性。然而，在实际应用中，重新训练和部署模型需要大量的计算资源，对在线服务造成重大损失。此外，当检测到某种攻击的对抗性例子时，服务提供者只能获得有限的对抗性样本，而大量的清洁数据可能无法获取。针对这些问题，我们提出了一种新的方案，名为RaPiD（Rapid Plug-in Defender），旨在快速防御具有少量干净和对抗性示例限制的原始服务模型的某种攻击。受到预训练模型提供转移学习良好初始化的通用趋势的启发，我们建议通过微调预先训练的Transformer来提纯对抗性样本。预训练的Transformer作为正则化器，鼓励提纯后的对抗性样本接近清晰数据的分布。实验结果表明，RaPiD在防御各种具有限数据的攻击方面优于最先进的方法。

    With more and more deep neural networks being deployed as various daily services, their reliability is essential. It's frightening that deep neural networks are vulnerable and sensitive to adversarial attacks, the most common one of which for the services is evasion-based. Recent works usually strengthen the robustness by adversarial training or leveraging the knowledge of an amount of clean data. However, in practical terms, retraining and redeploying the model need a large computational budget, leading to heavy losses to the online service. In addition, when adversarial examples of a certain attack are detected, only limited adversarial examples are available for the service provider, while much clean data may not be accessible. Given the mentioned problems, we propose a new scenario, RaPiD (Rapid Plug-in Defender), which is to rapidly defend against a certain attack for the frozen original service model with limitations of few clean and adversarial examples. Motivated by the general
    
[^142]: GRD: 一种在强化学习中用于可解释奖励再分配的生成方式

    GRD: A Generative Approach for Interpretable Reward Redistribution in Reinforcement Learning. (arXiv:2305.18427v1 [cs.LG])

    [http://arxiv.org/abs/2305.18427](http://arxiv.org/abs/2305.18427)

    本文提出了一种可解释奖励再分配的方法，通过因果透视建模状态和行动贡献，产生可解释的返回分解。生成返回分解（GRD）框架用于延迟奖励场景中的策略优化。

    

    在强化学习中，一个重要的挑战是确定哪些状态-行动对应该对未来的分步奖励负责。Return Decomposition提供了一种解决方案，通过重新分配观测序列中的奖励来保持策略不变。虽然大多数现有方法都以不可解释的方式构建奖励再分配，但我们建议采用因果透视来明确建模状态和行动的贡献，从而产生可解释的返回分解。本文首先研究了因果生成模型在返回分解中的作用，通过描述马尔可夫奖励和基于轨迹的长期回报的生成过程，并进一步提出了一种名为生成返回分解（GRD）的框架，用于延迟奖励场景中的策略优化。具体而言，GRD首先确定生成过程中不可观测的马尔可夫奖励和因果关系，然后利用确定的因果模型计算可观测奖励的期望，进而提高策略性能。

    A major challenge in reinforcement learning is to determine which state-action pairs are responsible for future rewards that are delayed. Return Decomposition offers a solution by redistributing rewards from observed sequences while preserving policy invariance. While the majority of current approaches construct the reward redistribution in an uninterpretable manner, we propose to explicitly model the contributions of state and action from a causal perspective, resulting in an interpretable return decomposition. In this paper, we start by studying the role of causal generative models in return decomposition by characterizing the generation of Markovian rewards and trajectory-wise long-term return and further propose a framework, called Generative Return Decomposition (GRD), for policy optimization in delayed reward scenarios. Specifically, GRD first identifies the unobservable Markovian rewards and causal relations in the generative process. Then, GRD makes use of the identified causal
    
[^143]: 利用语义先验细化的弱监督视觉-文本对齐

    Weakly-Supervised Visual-Textual Grounding with Semantic Prior Refinement. (arXiv:2305.10913v1 [cs.CV])

    [http://arxiv.org/abs/2305.10913](http://arxiv.org/abs/2305.10913)

    本文提出了一种利用语义先验细化的弱监督视觉-文本对齐方法，仅使用图像-句子对进行学习，其目标是实现实体表示中的区域-短语对应关系，通过联合两个主要模块的输出进行预测。

    

    弱监督视觉-文本对齐的目标是仅利用图像-句子对学习实体表示中的区域-短语对应关系。与监督方法相比，其难度更大，因为无法获得边界框和文本短语的对应关系。因此，我们提出了语义先验细化模型（SPRM），其预测结果是通过组合两个主要模块的输出得到的。第一个未经训练的模块旨在返回文本短语和边界框之间的粗略对齐。第二个训练过的模块由两个子组件组成，用于细化粗略的对齐以提高最终短语-边界框对齐的准确性。该模型的训练目标是最大化图像和句子之间的多模态相似度，同时使同一句子和一个新的不相关的图像的多模态相似度最小化，以在训练过程中最大限度地提高训练效果。我们的方法在两个流行的数据集上展现了最先进的结果。

    Using only image-sentence pairs, weakly-supervised visual-textual grounding aims to learn region-phrase correspondences of the respective entity mentions. Compared to the supervised approach, learning is more difficult since bounding boxes and textual phrases correspondences are unavailable. In light of this, we propose the Semantic Prior Refinement Model (SPRM), whose predictions are obtained by combining the output of two main modules. The first untrained module aims to return a rough alignment between textual phrases and bounding boxes. The second trained module is composed of two sub-components that refine the rough alignment to improve the accuracy of the final phrase-bounding box alignments. The model is trained to maximize the multimodal similarity between an image and a sentence, while minimizing the multimodal similarity of the same sentence and a new unrelated image, carefully selected to help the most during training. Our approach shows state-of-the-art results on two popula
    
[^144]: 将知识蒸馏用于短期到长期轨迹预测

    Distilling Knowledge for Short-to-Long Term Trajectory Prediction. (arXiv:2305.08553v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.08553](http://arxiv.org/abs/2305.08553)

    本文提出了一种新的方法Di-Long，用于解决长期轨迹预测中越来越不确定和不可预测的问题。该方法利用蒸馏短期轨迹模型预测器来指导训练过程中的长期轨迹预测学生网络。学生网络观察短序列并预测长轨迹，教师网络观察更长序列并预测剩余短目标轨迹。

    

    长期轨迹预测是计算机视觉、机器学习和机器人领域中一个重要且具有挑战性的问题。其中一个基本困难在于随着时间范围的增长，轨迹的演变变得越来越不确定和不可预测，从而增加了问题的复杂性。为了克服这个问题，在本文中，我们提出了Di-Long，一种新的方法，它利用蒸馏短期轨迹模型预测器来指导训练过程中的长期轨迹预测学生网络。给定一个包含学生网络允许的观测序列和补充目标序列的总序列长度，我们让学生和教师对同一个完整轨迹定义两个不同但相关的任务：学生观察一个短序列并预测一个长轨迹，而教师观察一个更长的序列并预测剩下的短目标轨迹。

    Long-term trajectory forecasting is an important and challenging problem in the fields of computer vision, machine learning, and robotics. One fundamental difficulty stands in the evolution of the trajectory that becomes more and more uncertain and unpredictable as the time horizon grows, subsequently increasing the complexity of the problem. To overcome this issue, in this paper, we propose Di-Long, a new method that employs the distillation of a short-term trajectory model forecaster that guides a student network for long-term trajectory prediction during the training process. Given a total sequence length that comprehends the allowed observation for the student network and the complementary target sequence, we let the student and the teacher solve two different related tasks defined over the same full trajectory: the student observes a short sequence and predicts a long trajectory, whereas the teacher observes a longer sequence and predicts the remaining short target trajectory. The
    
[^145]: 如何为推荐基础模型索引项目ID

    How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v1 [cs.IR])

    [http://arxiv.org/abs/2305.06569](http://arxiv.org/abs/2305.06569)

    本研究对推荐基础模型的项目索引问题进行了系统检查，提出了一种新的上下文感知索引方法，该方法在项目推荐准确性和文本生成质量方面具有优势。

    

    推荐基础模型将推荐任务转换为自然语言任务，利用大型语言模型（LLM）进行推荐。它通过直接生成建议的项目而不是计算传统推荐模型中每个候选项目的排名得分，简化了推荐管道，避免了多段过滤的问题。为了避免在决定要推荐哪些项目时生成过长的文本，为推荐基础模型创建LLM兼容的项目ID是必要的。本研究系统地研究了推荐基础模型的项目索引问题，以P5为代表的主干模型，并使用各种索引方法复制其结果。我们首先讨论了几种微不足道的项目索引方法（如独立索引、标题索引和随机索引）的问题，并表明它们不适用于推荐基础模型，然后提出了一种新的索引方法，称为上下文感知索引。我们表明，这种索引方法在项目推荐准确性和文本生成质量方面优于其他索引方法。

    Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text when deciding which item(s) to recommend, creating LLM-compatible item IDs is essential for recommendation foundation models. In this study, we systematically examine the item indexing problem for recommendation foundation models, using P5 as the representative backbone model and replicating its results with various indexing methods. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as independent indexing, title indexing, and random inde
    
[^146]: FMG-Net和W-Net：受多重网格启发的医学图像分割深度学习架构

    FMG-Net and W-Net: Multigrid Inspired Deep Learning Architectures For Medical Imaging Segmentation. (arXiv:2304.02725v1 [eess.IV])

    [http://arxiv.org/abs/2304.02725](http://arxiv.org/abs/2304.02725)

    FMG-Net和W-Net是两种受多重网格启发的深度学习架构，能够解决医学图像分割中面临的细节特征和尺度变化的挑战，能够提高肿瘤分割的精度。

    

    准确的医学图像分割对于精确和有效的医疗干预至关重要。然而，尽管卷积神经网络(CNN)在医学图像分割方面取得了成功，但它们仍面临着处理细粒度特征和图像尺度变化的挑战。这些挑战在复杂和具有挑战性的分割任务中特别明显，例如BraTS多标签脑肿瘤分割挑战赛中。在这个任务中，精确地分割不同的肿瘤亚组分，在大小和形状上都有显著变化，仍然是一个重大挑战，即使是最先进的方法也会产生重大错误。因此，我们提出了两种架构，FMG-Net和W-Net，它们将解线性方程组的几何多重网格方法的原理纳入CNN中，以解决这些挑战。我们在BraTS 2020数据集上的实验表明，FMG-Net和W-Net都优于广泛使用的U-Net架构，特别是在分割中的tum的精度方面。

    Accurate medical imaging segmentation is critical for precise and effective medical interventions. However, despite the success of convolutional neural networks (CNNs) in medical image segmentation, they still face challenges in handling fine-scale features and variations in image scales. These challenges are particularly evident in complex and challenging segmentation tasks, such as the BraTS multi-label brain tumor segmentation challenge. In this task, accurately segmenting the various tumor sub-components, which vary significantly in size and shape, remains a significant challenge, with even state-of-the-art methods producing substantial errors. Therefore, we propose two architectures, FMG-Net and W-Net, that incorporate the principles of geometric multigrid methods for solving linear systems of equations into CNNs to address these challenges. Our experiments on the BraTS 2020 dataset demonstrate that both FMG-Net and W-Net outperform the widely used U-Net architecture regarding tum
    
[^147]: 学习稳定和鲁棒的线性参数可变状态空间模型

    Learning Stable and Robust Linear Parameter-Varying State-Space Models. (arXiv:2304.01828v1 [eess.SY])

    [http://arxiv.org/abs/2304.01828](http://arxiv.org/abs/2304.01828)

    该论文提出了稳定和鲁棒的线性参数可变状态空间模型的两种直接参数化方法，训练出的模型具有收缩意义或通过用户定义的值被限制在 Lipschitz 常数内，对进一步的凸分析或控制器设计非常有用。

    

    本文提出了两种直接参数化的稳定和鲁棒线性参数可变状态空间 (LPV-SS) 模型。模型参数化在训练期间保证所有参数值的模型都是稳定的，具有收缩意义，或其 Lipschitz 常数被用户定义的值 $\gamma$ 限制。此外，由于参数化是直接的，因此可以使用无约束优化来训练模型。由于训练出的模型属于 LPV-SS 类，因此它们对进一步的凸分析或控制器设计非常有用。该方法在 LPV 识别问题上展示了出色的效果。

    This paper presents two direct parameterizations of stable and robust linear parameter-varying state-space (LPV-SS) models. The model parametrizations guarantee a priori that for all parameter values during training, the allowed models are stable in the contraction sense or have their Lipschitz constant bounded by a user-defined value $\gamma$. Furthermore, since the parametrizations are direct, the models can be trained using unconstrained optimization. The fact that the trained models are of the LPV-SS class makes them useful for, e.g., further convex analysis or controller design. The effectiveness of the approach is demonstrated on an LPV identification problem.
    
[^148]: 带对手的在线学习：微分包容分析

    Online Learning with Adversaries: A Differential Inclusion Analysis. (arXiv:2304.01525v1 [cs.LG])

    [http://arxiv.org/abs/2304.01525](http://arxiv.org/abs/2304.01525)

    本文提出了第一个能够以几乎确定的方式收敛到 $\mu$ 的异步在线算法，应用了微分包容分析，并提供了两个关键亮点。

    

    我们考虑测量模型 $Y = AX$，其中 $X$ 和 $Y$ 是随机变量，$A$ 是先验已知的高矩阵。在每个时间实例，可以获得 $Y$ 的一个坐标的样本，并且目标是通过这些样本估计 $\mu := \mathbb{E}[X]$。然而，挑战在于：小但未知的 $Y$ 的坐标子集由对手控制，并具有无限的能力：每次查询样本时，他们可以返回任何实数。对于这种对抗性环境，我们提出了第一个能够以几乎确定的方式收敛到 $\mu$ 的异步在线算法。我们使用一种新颖的微分包容基于两个时间尺度的分析来证明这个结果。我们证明的两个关键亮点包括：(a) 使用一种新颖的 Lyapunov 函数来证明 $\mu$ 是我们算法极限动态的唯一全局吸引子，(b) 使用鞅和停时理论来证明我们算法的迭代几乎必定有界。

    We consider the measurement model $Y = AX,$ where $X$ and, hence, $Y$ are random variables and $A$ is an a priori known tall matrix. At each time instance, a sample of one of $Y$'s coordinates is available, and the goal is to estimate $\mu := \mathbb{E}[X]$ via these samples. However, the challenge is that a small but unknown subset of $Y$'s coordinates are controlled by adversaries with infinite power: they can return any real number each time they are queried for a sample. For such an adversarial setting, we propose the first asynchronous online algorithm that converges to $\mu$ almost surely. We prove this result using a novel differential inclusion based two-timescale analysis. Two key highlights of our proof include: (a) the use of a novel Lyapunov function for showing that $\mu$ is the unique global attractor for our algorithm's limiting dynamics, and (b) the use of martingale and stopping time theory to show that our algorithm's iterates are almost surely bounded.
    
[^149]: Borda Regret Minimization for Generalized Linear Dueling Bandits (通用广义线性对抗性排名问题的博尔达后悔最小化算法)

    Borda Regret Minimization for Generalized Linear Dueling Bandits. (arXiv:2303.08816v1 [cs.LG])

    [http://arxiv.org/abs/2303.08816](http://arxiv.org/abs/2303.08816)

    本文解决了通用广义线性对抗性排名问题中的博尔达后悔最小化问题，提出了高度表达力的模型，并使用一种新的“先探索再执行”算法避免了困难的后悔下限。

    

    对抗性排名问题(Dueling bandits)常被用于机器学习应用，如推荐系统和排名问题。本文研究对抗性排名问题中博尔达后悔最小化问题，旨在确定具有最高博尔达得分的项目，并同时最小化累计的后悔。我们提出了一个新的、高度表达力的通用广义线性对抗性排名模型，它包括许多现有模型。 令人惊讶的是，博尔达后悔最小化问题是困难的。 我们证明了渐近时间复杂度的后悔下限是$\Omega(d^{2/3} T^{2/3})$，其中$d$是上下文向量的维数，$T$是时间跨度。为了达到下限，我们提出了一种"先探索再执行"的算法，它具有几乎匹配的上限回归误差$\tilde{O}(d^{2/3} T^{2/3})$。当项目数量$K$很小时，我们的算法可以通过适当选择超参数以达到更小的后悔$\tilde{O}((d\log K)^{1/3}T^{2/3})$。

    Dueling bandits are widely used to model preferential feedback that is prevalent in machine learning applications such as recommendation systems and ranking. In this paper, we study the Borda regret minimization problem for dueling bandits, which aims to identify the item with the highest Borda score while minimizing the cumulative regret. We propose a new and highly expressive generalized linear dueling bandits model, which covers many existing models. Surprisingly, the Borda regret minimization problem turns out to be difficult, as we prove a regret lower bound of order $\Omega(d^{2/3} T^{2/3})$, where $d$ is the dimension of contextual vectors and $T$ is the time horizon. To attain the lower bound, we propose an explore-then-commit type algorithm, which has a nearly matching regret upper bound $\tilde{O}(d^{2/3} T^{2/3})$. When the number of items/arms $K$ is small, our algorithm can achieve a smaller regret $\tilde{O}( (d \log K)^{1/3} T^{2/3})$ with proper choices of hyperparamete
    
[^150]: 具有别名观测的潜在图的快速探索与学习

    Fast exploration and learning of latent graphs with aliased observations. (arXiv:2303.07397v1 [cs.LG])

    [http://arxiv.org/abs/2303.07397](http://arxiv.org/abs/2303.07397)

    本文介绍了一种在具有别名观测的潜在图上，能够显著提高最大化探索效率的政策算法 eFeX，相比于随机策略，该算法能够更快地恢复各种拓扑结构下的图表。

    

    考虑这种场景：一个智能体通过执行操作从一个节点到另一个节点来导航潜在图。所选操作确定了下一个访问节点上的概率分布。在每个节点处，智能体收到一个观测，但该观测不是唯一的，因此它不能唯一地标识节点，这使得问题别名化。本文旨在提供一个政策，该政策约等于最大化探索效率（即在给定的探索预算下如何恢复图表）。在非别名化的情况下，我们展示了相对于现有最先进强化学习基线的改进性能。对于别名化的情况，我们不知道适用的基线，而是展示了在各种拓扑结构下相对于随机策略更快的恢复速度，并且对于具有挑战性的拓扑结构，恢复速度比随机策略快指数倍。我们将该算法称为 eFeX（来自于 efficient exploration 的缩写）。

    Consider this scenario: an agent navigates a latent graph by performing actions that take it from one node to another. The chosen action determines the probability distribution over the next visited node. At each node, the agent receives an observation, but this observation is not unique, so it does not identify the node, making the problem aliased. The purpose of this work is to provide a policy that approximately maximizes exploration efficiency (i.e., how well the graph is recovered for a given exploration budget). In the unaliased case, we show improved performance w.r.t. state-of-the-art reinforcement learning baselines. For the aliased case we are not aware of suitable baselines and instead show faster recovery w.r.t. a random policy for a wide variety of topologies, and exponentially faster recovery than a random policy for challenging topologies. We dub the algorithm eFeX (from eFficient eXploration).
    
[^151]: 基于深度学习的时间序列因果推断量化北极放大的原因

    Quantifying Causes of Arctic Amplification via Deep Learning based Time-series Causal Inference. (arXiv:2303.07122v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.07122](http://arxiv.org/abs/2303.07122)

    该研究提出了一种基于循环神经网络的时间序列因果推断模型TCINet，用于推断大气过程对海冰融化的因果效应。通过实验证明，该模型能够显著提高量化北极海冰融化的主要原因的能力。

    

    北极变暖，也称北极放大，由多种大气和海洋因素导致，但其基础热力因素的详细情况仍不清楚。使用固定治疗效应策略推断大气过程对海冰融化的因果效应会导致不现实的反事实估计。这样的模型也容易受到时间变化的混淆的影响而引起偏差。为了解决这些挑战，我们提出了TCINet - 一种基于循环神经网络的时间序列因果推断模型，以连续治疗方式推断因果关系。通过对合成和观测数据的实验，我们展示了我们的研究如何大大提高量化北极海冰融化的主要原因的能力。

    The warming of the Arctic, also known as Arctic amplification, is led by several atmospheric and oceanic drivers, however, the details of its underlying thermodynamic causes are still unknown. Inferring the causal effects of atmospheric processes on sea ice melt using fixed treatment effect strategies leads to unrealistic counterfactual estimations. Such models are also prone to bias due to time-varying confoundedness. In order to tackle these challenges, we propose TCINet - time-series causal inference model to infer causation under continuous treatment using recurrent neural networks. Through experiments on synthetic and observational data, we show how our research can substantially improve the ability to quantify the leading causes of Arctic sea ice melt.
    
[^152]: 探究有状态防御黑盒对抗样本

    Investigating Stateful Defenses Against Black-Box Adversarial Examples. (arXiv:2303.06280v1 [cs.CR])

    [http://arxiv.org/abs/2303.06280](http://arxiv.org/abs/2303.06280)

    本文探究了有状态防御黑盒对抗样本的方法，提出了一种新的有状态防御模型，可以在CIFAR10数据集上达到82.2％的准确性，在ImageNet数据集上达到76.5％的准确性。

    This paper investigates stateful defenses against black-box adversarial examples and proposes a new stateful defense model that achieves 82.2% accuracy on the CIFAR10 dataset and 76.5% accuracy on the ImageNet dataset.

    防御机器学习（ML）模型免受白盒对抗攻击已被证明极为困难。相反，最近的工作提出了有状态防御，试图防御更受限制的黑盒攻击者。这些防御通过跟踪传入模型查询的历史记录，并拒绝那些可疑地相似的查询来操作。目前最先进的有状态防御Blacklight是在USENIX Security '22上提出的，声称可以防止几乎100％的CIFAR10和ImageNet数据集上的攻击。在本文中，我们观察到攻击者可以通过简单调整现有黑盒攻击的参数，显著降低受Blacklight保护的分类器的准确性（例如，在CIFAR10上从82.2％降至6.4％）。受到这一惊人观察的启发，我们提供了有状态防御的系统化，以了解为什么现有的有状态防御模型会失败。最后，我们提出了一种新的有状态防御模型，该模型在CIFAR10数据集上的准确性为82.2％，在ImageNet数据集上的准确性为76.5％。

    Defending machine-learning (ML) models against white-box adversarial attacks has proven to be extremely difficult. Instead, recent work has proposed stateful defenses in an attempt to defend against a more restricted black-box attacker. These defenses operate by tracking a history of incoming model queries, and rejecting those that are suspiciously similar. The current state-of-the-art stateful defense Blacklight was proposed at USENIX Security '22 and claims to prevent nearly 100% of attacks on both the CIFAR10 and ImageNet datasets. In this paper, we observe that an attacker can significantly reduce the accuracy of a Blacklight-protected classifier (e.g., from 82.2% to 6.4% on CIFAR10) by simply adjusting the parameters of an existing black-box attack. Motivated by this surprising observation, since existing attacks were evaluated by the Blacklight authors, we provide a systematization of stateful defenses to understand why existing stateful defense models fail. Finally, we propose a
    
[^153]: 以ELBOs的加权积分理解扩散目标

    Understanding the Diffusion Objective as a Weighted Integral of ELBOs. (arXiv:2303.00848v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00848](http://arxiv.org/abs/2303.00848)

    本文深入理解了扩散目标，并揭示了加权损失和ELBO目标之间的直接关系。

    

    文献中的扩散模型采用不同的目标进行优化，并且这些目标都是加权损失的特例，其中加权函数指定每个噪声级别的权重。均匀加权对应于最大似然的原则性近似ELBO的最大化。但是实际上，由于更好的样本质量，目前的扩散模型使用非均匀加权。本文揭示了加权损失（带有任何加权）和ELBO目标之间的直接关系。我们展示了加权损失可以被写成一种ELBOs的加权积分形式，其中每个噪声级别都有一个ELBO。如果权重函数是单调的，那么加权损失是一种基于似然的目标：它在简单的数据增强下（即高斯噪声扰动）下最大化ELBO。我们的主要贡献是更深入地理解了扩散目标，但我们还进行了一些比较单调和非单调权重的实验。

    Diffusion models in the literature are optimized with various objectives that are special cases of a weighted loss, where the weighting function specifies the weight per noise level. Uniform weighting corresponds to maximizing the ELBO, a principled approximation of maximum likelihood. In current practice diffusion models are optimized with non-uniform weighting due to better results in terms of sample quality. In this work we expose a direct relationship between the weighted loss (with any weighting) and the ELBO objective.  We show that the weighted loss can be written as a weighted integral of ELBOs, with one ELBO per noise level. If the weighting function is monotonic, then the weighted loss is a likelihood-based objective: it maximizes the ELBO under simple data augmentation, namely Gaussian noise perturbation. Our main contribution is a deeper theoretical understanding of the diffusion objective, but we also performed some experiments comparing monotonic with non-monotonic weight
    
[^154]: 转换的低秩参数化可以帮助张量神经网络实现稳健的泛化

    Transformed Low-Rank Parameterization Can Help Robust Generalization for Tensor Neural Networks. (arXiv:2303.00196v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00196](http://arxiv.org/abs/2303.00196)

    这项研究首次通过推导泛化误差上界回答了转换的低秩参数化如何影响张量神经网络的学习行为，结果显示通过精确的转换低秩参数化压缩的t-NNs可以实现更尖锐的对抗泛化上界。

    

    在数据科学中，实现高效且稳健的多通道数据学习是一项具有挑战性的任务。通过利用转换域中的低秩性，即转换的低秩性，张量奇异值分解（t-SVD）在多通道数据表示方面取得了广泛的成功，并最近扩展到了函数表示，如具有t-乘积层（t-NNs）的神经网络。然而，t-SVD理论上如何影响t-NNs的学习行为仍不清楚。本文第一次通过推导标准和对抗训练的t-NNs的泛化误差上界来回答这个问题。研究结果显示，通过精确的转换低秩参数化压缩的t-NNs可以实现更尖锐的对抗泛化上界。在实践中，尽管t-NNs很少具有完全转换的低秩权重，我们的分析进一步表明，通过使用梯度流（GF）进行对抗性训练，过参数化的t-NNs具有ReLU

    Achieving efficient and robust multi-channel data learning is a challenging task in data science. By exploiting low-rankness in the transformed domain, i.e., transformed low-rankness, tensor Singular Value Decomposition (t-SVD) has achieved extensive success in multi-channel data representation and has recently been extended to function representation such as Neural Networks with t-product layers (t-NNs). However, it still remains unclear how t-SVD theoretically affects the learning behavior of t-NNs. This paper is the first to answer this question by deriving the upper bounds of the generalization error of both standard and adversarially trained t-NNs. It reveals that the t-NNs compressed by exact transformed low-rank parameterization can achieve a sharper adversarial generalization bound. In practice, although t-NNs rarely have exactly transformed low-rank weights, our analysis further shows that by adversarial training with gradient flow (GF), the over-parameterized t-NNs with ReLU 
    
[^155]: 置换等变神经功能网络

    Permutation Equivariant Neural Functionals. (arXiv:2302.14040v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14040](http://arxiv.org/abs/2302.14040)

    本文介绍了置换等变神经功能网络的设计，通过对权重进行置换对称性编码，实现对其他网络权重或梯度进行处理，为学习优化、处理隐式神经表示等应用提供了架构原则。

    

    本文研究了能够处理其他神经网络的权重或梯度的神经网络的设计，我们将其称为神经功能网络（NFN）。尽管具有广泛的潜在应用，包括学习优化、处理隐式神经表示、网络编辑和策略评估，但设计处理其他网络权重的有效架构的统一原则很少。我们通过对称性的视角来设计神经功能，特别是通过关注深度前馈网络权重中出现的置换对称性，因为隐藏层神经元没有固有顺序。我们介绍了一种构建置换等变神经功能的框架，该框架将这些对称性编码为归纳偏差。该框架的关键组成部分是我们通过适当的参数来约束为置换等变的NF-Layers（神经功能层）。

    This work studies the design of neural networks that can process the weights or gradients of other neural networks, which we refer to as neural functional networks (NFNs). Despite a wide range of potential applications, including learned optimization, processing implicit neural representations, network editing, and policy evaluation, there are few unifying principles for designing effective architectures that process the weights of other networks. We approach the design of neural functionals through the lens of symmetry, in particular by focusing on the permutation symmetries that arise in the weights of deep feedforward networks because hidden layer neurons have no inherent order. We introduce a framework for building permutation equivariant neural functionals, whose architectures encode these symmetries as an inductive bias. The key building blocks of this framework are NF-Layers (neural functional layers) that we constrain to be permutation equivariant through an appropriate paramet
    
[^156]: 有导向性的扩散: 通过关注引导实现对象放置的直接控制

    Directed Diffusion: Direct Control of Object Placement through Attention Guidance. (arXiv:2302.13153v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.13153](http://arxiv.org/abs/2302.13153)

    本论文介绍了一种有导向性的扩散方法，通过关注引导在图像中直接控制对象的放置。通过观察提示词的交叉注意力映射，引入优化目标，在特定位置产生“激活”，从而改进了场景组合能力。

    

    文本引导的扩散模型（如DALLE-2、Imagen和稳定扩散）能够仅通过描述所需图像内容的简短文本提示生成各种形式的图像。在许多情况下，这些图像具有非常高的质量。然而，这些模型通常难以组合包含多个关键对象（如指定位置关系中的字符）的场景。在讲述故事中，能够“直接”指导人物和对象的放置，无论是在图像内还是跨图像内，都是至关重要的，这一点在电影和动画理论的文献中得到了认可。在本文中，我们采用一种特别简单的方法来提供所需的指导。基于一个观察结果，即提示词的交叉注意力映射反映出由这些词所表示的对象的空间布局，我们引入了一个优化目标，在这些交叉注意力映射中的期望位置产生“激活”。由此产生的方法是通向泛化的一步。

    Text-guided diffusion models such as DALLE-2, Imagen, and Stable Diffusion are able to generate an effectively endless variety of images given only a short text prompt describing the desired image content. In many cases the images are of very high quality. However, these models often struggle to compose scenes containing several key objects such as characters in specified positional relationships. The missing capability to "direct" the placement of characters and objects both within and across images is crucial in storytelling, as recognized in the literature on film and animation theory. In this work, we take a particularly straightforward approach to providing the needed direction. Drawing on the observation that the cross-attention maps for prompt words reflect the spatial layout of objects denoted by those words, we introduce an optimization objective that produces ``activation'' at desired positions in these cross-attention maps. The resulting approach is a step toward generalizin
    
[^157]: 基于像素的混合交通控制与协调方法

    Mixed Traffic Control and Coordination from Pixels. (arXiv:2302.09167v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2302.09167](http://arxiv.org/abs/2302.09167)

    本研究考虑利用图像观察作为替代方法来进行混合交通控制。

    

    交通拥堵是我们社会中一直存在的问题。传统的交通控制方法在缓解当前拥堵程度方面已经失效，因此研究人员开始探索通过机器人车辆进行交通控制的想法，考虑到不同级别自主性车辆的不断涌现。这引起了混合交通控制的出现，其中机器人车辆通过强化学习算法来调节人驾驶车辆。本研究考虑利用图像观察作为混合交通控制的替代方法：1）图像通过卫星图像、车内摄像系统和交通监控系统普遍存在；2）图像不需要更新现有道路基础设施，并且不需要向可能不愿意配合的人类驾驶员传递信息。

    Traffic congestion is a persistent problem in our society. Existing methods for traffic control have proven futile in alleviating current congestion levels leading researchers to explore ideas with robot vehicles given the increased emergence of vehicles with different levels of autonomy on our roads. This gives rise to mixed traffic control, where robot vehicles regulate human-driven vehicles through reinforcement learning (RL). However, most existing studies use precise observations that involve global information, such as environment outflow, and local information, i.e., vehicle positions and velocities. Obtaining this information requires updating existing road infrastructure with vast sensor environments and communication to potentially unwilling human drivers. We consider image observations as the alternative for mixed traffic control via RL: 1) images are ubiquitous through satellite imagery, in-car camera systems, and traffic monitoring systems; 2) images do not require a compl
    
[^158]: UniPC：一种用于快速采样扩散模型的统一预测-修正框架

    UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models. (arXiv:2302.04867v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04867](http://arxiv.org/abs/2302.04867)

    本文提出了一种称为UniPC的统一预测-修正框架，用于快速采样扩散模型(DPMs)，该框架通过引入统一修正器(UniC)和统一预测器(UniP)，可以显著提高采样质量，尤其是在较少步骤的情况下。

    

    扩散概率模型(DPMs)在高分辨率图像合成方面表现出非常有希望的能力。然而，由于对去噪网络的多次评估，从预训练的DPM中进行采样非常耗时，因此加速DPM采样变得越来越重要。虽然设计出了快速采样器的最新进展，但现有方法仍无法在许多应用中生成令人满意的图像，这些应用更青睐于较少的步骤(如<10)。在本文中，我们开发了一种统一修正器(UniC)，它可以在任何现有的DPM采样器之后应用，提高精确度的阶数而无需额外的模型评估，并推导出一个支持任意阶数的统一预测器(UniP)作为副产品。通过结合UniP和UniC，我们提出了一种称为UniPC的统一预测-修正框架，用于快速采样DPMs，它具有任意阶数的统一解析形式，并且可以显著提高采样的质量，特别是在极端情况下。

    Diffusion probabilistic models (DPMs) have demonstrated a very promising ability in high-resolution image synthesis. However, sampling from a pre-trained DPM is time-consuming due to the multiple evaluations of the denoising network, making it more and more important to accelerate the sampling of DPMs. Despite recent progress in designing fast samplers, existing methods still cannot generate satisfying images in many applications where fewer steps (e.g., $<$10) are favored. In this paper, we develop a unified corrector (UniC) that can be applied after any existing DPM sampler to increase the order of accuracy without extra model evaluations, and derive a unified predictor (UniP) that supports arbitrary order as a byproduct. Combining UniP and UniC, we propose a unified predictor-corrector framework called UniPC for the fast sampling of DPMs, which has a unified analytical form for any order and can significantly improve the sampling quality over previous methods, especially in extremel
    
[^159]: 基于GPU的设备上机器学习推断的私密信息检索

    GPU-based Private Information Retrieval for On-Device Machine Learning Inference. (arXiv:2301.10904v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2301.10904](http://arxiv.org/abs/2301.10904)

    本论文提出了一种基于GPU的私密信息检索的方法，可以在设备上进行机器学习推断，而无需将私密用户数据透露给远程服务器。通过加速私密信息检索并与下游机器学习应用程序共同设计，我们的方法极大地提高了系统吞吐量。

    

    设备上的机器学习推断可以在不将用户数据透露给远程服务器的情况下利用私密用户数据。然而，纯设备上的私密机器学习推断解决方案对于许多依赖于嵌入式表的应用来说是不实际的，这些嵌入式表的大小太大无法存储在设备上。特别是，推荐模型通常使用多个嵌入式表，每个表大约1-10GB的数据，这使得将它们存储在设备上变得不实际。为了克服这个障碍，我们提出了使用私密信息检索（PIR）从服务器有效且私密地检索嵌入式表，而不共享任何私密信息。由于现成的PIR算法通常对于延迟敏感的推断任务来说计算量过大，我们1）提出了一种基于GPU的PIR加速策略，并且2）与下游的机器学习应用程序共同设计PIR，以获得进一步的加速。我们的GPU加速策略将系统吞吐量提高了超过20倍。

    On-device machine learning (ML) inference can enable the use of private user data on user devices without revealing them to remote servers. However, a pure on-device solution to private ML inference is impractical for many applications that rely on embedding tables that are too large to be stored on-device. In particular, recommendation models typically use multiple embedding tables each on the order of 1-10 GBs of data, making them impractical to store on-device. To overcome this barrier, we propose the use of private information retrieval (PIR) to efficiently and privately retrieve embeddings from servers without sharing any private information. As off-the-shelf PIR algorithms are usually too computationally intensive to directly use for latency-sensitive inference tasks, we 1) propose novel GPU-based acceleration of PIR, and 2) co-design PIR with the downstream ML application to obtain further speedup. Our GPU acceleration strategy improves system throughput by more than $20 \times$
    
[^160]: 数据精炼综述

    Data Distillation: A Survey. (arXiv:2301.04272v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.04272](http://arxiv.org/abs/2301.04272)

    这篇综述介绍了数据精炼的概念和方法，以及针对不同数据类型的应用。数据精炼方法可以用于模型训练、推理和架构搜索等场景，以解决使用大型数据集训练模型所带来的问题。

    

    深度学习的流行导致了大量各种各样的数据集的整理。尽管在个别任务上表现接近人类水平，但在大型数据集上训练参数庞大的模型面临多方面的问题，如（a）高模型训练时间；（b）慢的研究迭代；和（c）差的生态可持续性。作为替代方案，数据精炼方法旨在合成简洁的数据摘要，这些摘要可以作为原始数据集的有效替代品，用于模型训练、推理、架构搜索等场景。在本综述中，我们提出了数据精炼的一个形式框架，并提供了现有方法的详细分类。此外，我们还涵盖了针对不同数据类型的数据精炼方法，包括图像、图形和用户-项目交互（推荐系统），同时确定了当前的挑战和未来的研究方向。

    The popularity of deep learning has led to the curation of a vast number of massive and multifarious datasets. Despite having close-to-human performance on individual tasks, training parameter-hungry models on large datasets poses multi-faceted problems such as (a) high model-training time; (b) slow research iteration; and (c) poor eco-sustainability. As an alternative, data distillation approaches aim to synthesize terse data summaries, which can serve as effective drop-in replacements of the original dataset for scenarios like model training, inference, architecture search, etc. In this survey, we present a formal framework for data distillation, along with providing a detailed taxonomy of existing approaches. Additionally, we cover data distillation approaches for different data modalities, namely images, graphs, and user-item interactions (recommender systems), while also identifying current challenges and future research directions.
    
[^161]: VeriX: 面向深度神经网络的可验证解释性研究

    VeriX: Towards Verified Explainability of Deep Neural Networks. (arXiv:2212.01051v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01051](http://arxiv.org/abs/2212.01051)

    VeriX是一个可以产生最优健壮解释和生成反事实的系统，通过使用约束求解技术和基于特征级敏感性排名的启发式方法，我们在图像识别和自主飞机滑行场景中验证了该方法的有效性。

    

    我们提出了VeriX（Verified eXplainability），一个用于生成机器学习模型决策边界上的最优健壮解释和生成反事实的系统。我们使用约束求解技术和基于特征级敏感性排名的启发式方法迭代地构建这样的解释和反事实。我们在图像识别基准测试和自主飞机滑行的实际场景上评估了我们的方法。

    We present VeriX (Verified eXplainability), a system for producing optimal robust explanations and generating counterfactuals along decision boundaries of machine learning models. We build such explanations and counterfactuals iteratively using constraint solving techniques and a heuristic based on feature-level sensitivity ranking. We evaluate our method on image recognition benchmarks and a real-world scenario of autonomous aircraft taxiing.
    
[^162]: 基于确定性点过程的无监督技能发现的统一算法框架

    A Unified Algorithm Framework for Unsupervised Discovery of Skills based on Determinantal Point Process. (arXiv:2212.00211v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.00211](http://arxiv.org/abs/2212.00211)

    本文提出了一个使用确定性点过程（DPP）的统一算法框架，实现了对无监督技能发现的多样性和覆盖性的显式优化。

    

    在强化学习研究中，通过时间抽象学习丰富的技能而无需外部奖励监督是一个前沿问题。现有的工作主要分为两个不同的类别：变分和拉普拉斯基于技能（又称为选项）发现。前者通过互信息损失最大化发现的选项的多样性，但忽视了状态空间的覆盖率，而后者侧重于通过增加探索过程中的连接性来提高选项的覆盖率，但不考虑多样性。本文提出了一个统一的框架，通过对确定性点过程（DPP）的新型使用来量化多样性和覆盖性，并显式优化无监督选项发现的两个目标。具体而言，我们使用状态转换图的拉普拉斯谱定义DPP核矩阵，并使用轨迹中的期望模数作为目标，来捕捉和增强多样性和覆盖率。

    Learning rich skills through temporal abstractions without supervision of external rewards is at the frontier of Reinforcement Learning research. Existing works mainly fall into two distinctive categories: variational and Laplacian-based skill (a.k.a., option) discovery. The former maximizes the diversity of the discovered options through a mutual information loss but overlooks coverage of the state space, while the latter focuses on improving the coverage of options by increasing connectivity during exploration, but does not consider diversity. In this paper, we propose a unified framework that quantifies diversity and coverage through a novel use of the Determinantal Point Process (DPP) and enables unsupervised option discovery explicitly optimizing both objectives. Specifically, we define the DPP kernel matrix with the Laplacian spectrum of the state transition graph and use the expected mode number in the trajectories as the objective to capture and enhance both diversity and cover
    
[^163]: 基于浸润曲面上热核扩散的Geodesic Sinkhorn用于在流形上进行快速准确的最优输运

    Geodesic Sinkhorn for Fast and Accurate Optimal Transport on Manifolds. (arXiv:2211.00805v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.00805](http://arxiv.org/abs/2211.00805)

    Geodesic Sinkhorn是一种基于浸润曲面上热核扩散的方法，用于在流形上快速准确地计算分布之间的最优输运距离。该方法通过使用基于稀疏图拉普拉斯矩阵的切比雪夫多项式来逼近热核，从而实现了$O(n\log n)$的计算复杂度。在实验中，我们将该方法应用于高维单细胞数据的多个分布的几何中心计算。

    

    在数据科学中，高效计算分布之间的最优输运距离变得越来越重要。目前，Sinkhorn方法是这种计算的最先进方法，但需要$O(n^2)$的计算。此外，Sinkhorn方法通常使用数据点之间的欧几里德距离。然而，随着流形结构科学数据的普及，考虑浸润地面距离常常是需要的。在这里，我们通过在浸润曲面上扩散热核来解决这两个问题，提出了Geodesic Sinkhorn方法。值得注意的是，Geodesic Sinkhorn只需要$O(n\log n)$的计算，因为我们使用基于稀疏图拉普拉斯矩阵的切比雪夫多项式来逼近热核。我们将我们的方法应用于计算化疗患者样本的高维单细胞数据的多个分布的几何中心。特别地，我们将几何中心之间的距离定义为这两个几何中心之间的距离。

    Efficient computation of optimal transport distance between distributions is of growing importance in data science. Sinkhorn-based methods are currently the state-of-the-art for such computations, but require $O(n^2)$ computations. In addition, Sinkhorn-based methods commonly use an Euclidean ground distance between datapoints. However, with the prevalence of manifold structured scientific data, it is often desirable to consider geodesic ground distance. Here, we tackle both issues by proposing Geodesic Sinkhorn -- based on diffusing a heat kernel on a manifold graph. Notably, Geodesic Sinkhorn requires only $O(n\log n)$ computation, as we approximate the heat kernel with Chebyshev polynomials based on the sparse graph Laplacian. We apply our method to the computation of barycenters of several distributions of high dimensional single cell data from patient samples undergoing chemotherapy. In particular, we define the barycentric distance as the distance between two such barycenters. Us
    
[^164]: 通过Q-learning解决连续控制问题

    Solving Continuous Control via Q-learning. (arXiv:2210.12566v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12566](http://arxiv.org/abs/2210.12566)

    本研究通过对Q-learning进行简单修改，通过将bang-bang动作离散化与值分解相结合，将单智能体控制视为合作多智能体强化学习来解决连续控制问题，并取得了与最先进的连续actor-critic方法相匹配的性能。

    

    尽管在解决连续控制问题上，使用actor-critic方法取得了巨大的成功，但是简单的critic-only方法如Q-learning在涉及高维动作空间时应用有限。然而，大多数actor-critic方法的成本是增加了复杂性：稳定性启发式、计算要求和更广泛的超参数搜索空间。我们展示了一种对深度Q-learning进行简单修改的方法，大大减轻了这些问题。通过将bang-bang动作离散化与值分解相结合，将单智能体控制视为合作多智能体强化学习（MARL），这种简单的critic-only方法在从特征或像素学习时与最先进的连续actor-critic方法的性能相匹配。我们将合作MARL的经典赌徒问题扩展到了提供直观感觉的，展示了解耦的critics如何利用状态信息协调联合优化，并表现出了出乎意料的强大性能。

    While there has been substantial success for solving continuous control with actor-critic methods, simpler critic-only methods such as Q-learning find limited application in the associated high-dimensional action spaces. However, most actor-critic methods come at the cost of added complexity: heuristics for stabilisation, compute requirements and wider hyperparameter search spaces. We show that a simple modification of deep Q-learning largely alleviates these issues. By combining bang-bang action discretization with value decomposition, framing single-agent control as cooperative multi-agent reinforcement learning (MARL), this simple critic-only approach matches performance of state-of-the-art continuous actor-critic methods when learning from features or pixels. We extend classical bandit examples from cooperative MARL to provide intuition for how decoupled critics leverage state information to coordinate joint optimization, and demonstrate surprisingly strong performance across a var
    
[^165]: 单时间尺度演员-评论家法的有限时间分析

    Finite-time analysis of single-timescale actor-critic. (arXiv:2210.09921v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.09921](http://arxiv.org/abs/2210.09921)

    这项研究提出了一种在线单时间尺度演员-评论家方法，通过线性函数逼近和马尔可夫样本更新，在连续状态空间中找到了一个$\epsilon$-近似的稳定点，并且在样本复杂度为$\widetilde{\mathcal{O}}(\epsilon^{-2})$的情况下证明了其收敛性。

    

    在许多具有挑战性的应用中，演员-评论家方法取得了显着的成功。然而，在最实际的单时间尺度形式下，其有限时间收敛性仍然不够理解。现有的单时间尺度演员-评论家分析工作仅限于简化的i.i.d.采样或表格设置。我们研究了更实际的在线单时间尺度演员-评论家算法，该算法在连续状态空间中，评论家采用线性函数逼近，并在每个演员步骤中使用单个马尔可夫样本进行更新。先前的分析无法在这种具有挑战性的场景中实现收敛。我们证明，在标准假设下，在线单时间尺度演员-评论家方法能够在样本复杂度为$\widetilde{\mathcal{O}}(\epsilon^{-2})$的情况下找到一个$\epsilon$-近似的稳定点，而在i.i.d.采样下，这个复杂度可以进一步改进为$\mathcal{O}(\epsilon^{-2})$。我们的新框架系统地评估了一个

    Actor-critic methods have achieved significant success in many challenging applications. However, its finite-time convergence is still poorly understood in the most practical single-timescale form. Existing works on analyzing single-timescale actor-critic have been limited to i.i.d. sampling or tabular setting for simplicity. We investigate the more practical online single-timescale actor-critic algorithm on continuous state space, where the critic assumes linear function approximation and updates with a single Markovian sample per actor step. Previous analysis has been unable to establish the convergence for such a challenging scenario. We demonstrate that the online single-timescale actor-critic method provably finds an $\epsilon$-approximate stationary point with $\widetilde{\mathcal{O}}(\epsilon^{-2})$ sample complexity under standard assumptions, which can be further improved to $\mathcal{O}(\epsilon^{-2})$ under the i.i.d. sampling. Our novel framework systematically evaluates an
    
[^166]: 环境感知异常检测：忽略风格变化，专注于内容！

    Env-Aware Anomaly Detection: Ignore Style Changes, Stay True to Content!. (arXiv:2210.03103v2 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2210.03103](http://arxiv.org/abs/2210.03103)

    本研究提出了一种环境感知的异常检测方法，通过忽略风格变化，专注于内容，在无监督场景下达到了较好的性能，并通过考虑环境标签提高了对比方法的准确性。

    

    我们引入了一个正式的无监督异常检测任务的规范和基准，应用于分布转移情景中。我们的工作建立在iWildCam数据集的基础上，据我们所知，我们是首次为视觉数据提出这样的方法。我们经验证明，在这种情况下，环境感知的方法比基本的经验风险最小化（ERM）效果更好。我们接下来提出了一种扩展方法，用于生成对比方法的正样本，该方法在训练时考虑了环境标签，将ERM基线得分提高了8.7%。

    We introduce a formalization and benchmark for the unsupervised anomaly detection task in the distribution-shift scenario. Our work builds upon the iWildCam dataset, and, to the best of our knowledge, we are the first to propose such an approach for visual data. We empirically validate that environment-aware methods perform better in such cases when compared with the basic Empirical Risk Minimization (ERM). We next propose an extension for generating positive samples for contrastive methods that considers the environment labels when training, improving the ERM baseline score by 8.7%.
    
[^167]: 极化编码：一种用于处理缺失值分类的简单基线方法

    Polar Encoding: A Simple Baseline Approach for Classification with Missing Values. (arXiv:2210.01905v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01905](http://arxiv.org/abs/2210.01905)

    极化编码是一种用于处理具有缺失值的分类问题的简单基线方法，它能保留缺失信息、无需插补，让决策树自由选择如何处理缺失值。

    

    我们提出了一种称为极化编码的表示方法，用于处理具有缺失值的分类问题中的分类和数值型$[0,1]$值属性。我们认为这是一种很好的基准方法，因为它可以与任何分类算法配合使用，能够保留缺失信息，非常简单易用并且性能良好。与现有的缺失指示方法不同，极化编码不需要插补，确保缺失值与非缺失值等距离，让决策树算法自由选择如何分割缺失值，从而实现了“属性中包含缺失性”（MIA）的实际处理。此外，我们还展示了分类和$[0,1]$值属性可以被看作是一个单一属性类型的特殊情况，对应于经典的重心坐标概念，这提供了极化编码的模糊化形式的自然解释。

    We propose polar encoding, a representation of categorical and numerical $[0,1]$-valued attributes with missing values to be used in a classification context. We argue that this is a good baseline approach, because it can be used with any classification algorithm, preserves missingness information, is very simple to apply and offers good performance. In particular, unlike the existing missing-indicator approach, it does not require imputation, ensures that missing values are equidistant from non-missing values, and lets decision tree algorithms choose how to split missing values, thereby providing a practical realisation of the "missingness incorporated in attributes" (MIA) proposal. Furthermore, we show that categorical and $[0,1]$-valued attributes can be viewed as special cases of a single attribute type, corresponding to the classical concept of barycentric coordinates, and that this offers a natural interpretation of polar encoding as a fuzzified form of one-hot encoding. With an 
    
[^168]: 使用CNN来测试表示成本理论的预测

    Testing predictions of representation cost theory with CNNs. (arXiv:2210.01257v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01257](http://arxiv.org/abs/2210.01257)

    通过理论和实验证明，训练的卷积神经网络（CNNs）对低频信号具有敏感性，这是因为自然图像的频率分布使大部分能量集中在低到中频。

    

    众所周知，经过训练的卷积神经网络（CNNs）对不同频率的信号具有不同的敏感性。特别是，许多实证研究已经记录了CNNs对低频信号的敏感性。在这项工作中，我们通过理论和实验证明，这种观察到的敏感性是自然图像频率分布的结果，已知大部分能量集中在低到中频。我们的理论分析依赖于CNN的层次在频率空间中的表示，这个想法之前曾被用来加速计算和研究网络训练算法的隐式偏差，但据我们所知，尚未在模型鲁棒性领域应用过。

    It is widely acknowledged that trained convolutional neural networks (CNNs) have different levels of sensitivity to signals of different frequency. In particular, a number of empirical studies have documented CNNs sensitivity to low-frequency signals. In this work we show with theory and experiments that this observed sensitivity is a consequence of the frequency distribution of natural images, which is known to have most of its power concentrated in low-to-mid frequencies. Our theoretical analysis relies on representations of the layers of a CNN in frequency space, an idea that has previously been used to accelerate computations and study implicit bias of network training algorithms, but to the best of our knowledge has not been applied in the domain of model robustness.
    
[^169]: 探索鸟瞰视角感知中的挑战：一项综述、评估和方法

    Delving into the Devils of Bird's-eye-view Perception: A Review, Evaluation and Recipe. (arXiv:2209.05324v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.05324](http://arxiv.org/abs/2209.05324)

    本综述文章探讨了鸟瞰视角感知领域的挑战和方法，主要关注了从透视视图到鸟瞰视图的信息转换、地面真值注释获取、特征融合以及整体流程的构建。

    

    在感知任务中，学习鸟瞰视角（BEV）的强大表示正变得越来越流行，并引起了业界和学术界的广泛关注。传统方法中，大多数自动驾驶算法在前方或透视视图中进行检测、分割、跟踪等操作。随着传感器配置越来越复杂，从不同传感器中集成多源信息并以统一的视图表示特征变得非常重要。BEV感知具有几个优势，即以BEV表示周围场景直观且易于融合；以BEV表示物体对于后续的规划和/或控制模块是最理想的。BEV感知的核心问题在于：（a）如何通过从透视视图到BEV的视角转换重建丢失的3D信息；（b）如何在BEV网格中获取地面真值注释；（c）如何构建包含来自不同来源和视图的特征的流程；以及（d）...

    Learning powerful representations in bird's-eye-view (BEV) for perception tasks is trending and drawing extensive attention both from industry and academia. Conventional approaches for most autonomous driving algorithms perform detection, segmentation, tracking, etc., in a front or perspective view. As sensor configurations get more complex, integrating multi-source information from different sensors and representing features in a unified view come of vital importance. BEV perception inherits several advantages, as representing surrounding scenes in BEV is intuitive and fusion-friendly; and representing objects in BEV is most desirable for subsequent modules as in planning and/or control. The core problems for BEV perception lie in (a) how to reconstruct the lost 3D information via view transformation from perspective view to BEV; (b) how to acquire ground truth annotations in BEV grid; (c) how to formulate the pipeline to incorporate features from different sources and views; and (d) 
    
[^170]: E2EG: 使用图拓扑和基于文本的节点属性进行端到端节点分类

    E2EG: End-to-End Node Classification Using Graph Topology and Text-based Node Attributes. (arXiv:2208.04609v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.04609](http://arxiv.org/abs/2208.04609)

    E2EG是一个端到端节点分类模型，通过利用图拓扑和基于文本的节点属性，消除了嵌入和分类两个阶段，引入了主要和辅助分类目标的串行利用，减少了参数数量并提高了使用的便捷性。

    

    利用基于文本的节点属性进行节点分类在现实世界中有许多应用，从学术引用图中预测论文主题到社交媒体网络中用户特征的分类。现有的节点分类框架，如GIANT，使用了两个阶段的流程：首先嵌入图节点的文本属性，然后将得到的嵌入输入节点分类模型。在本文中，我们消除了这两个阶段，并开发了一个建立在GIANT基础上的端到端节点分类模型，称为End-to-End-GIANT（E2EG）。我们的方法中主要分类目标和辅助分类目标的串行利用结果使得模型更加稳健，使得BERT主干可以被一个参数减少25%-40%的蒸馏编码器取代。此外，模型的端到端特性增加了使用的便捷性，因为它避免了为节点分类链接多个模型的需求。与GIANT相比，

    Node classification utilizing text-based node attributes has many real-world applications, ranging from prediction of paper topics in academic citation graphs to classification of user characteristics in social media networks. State-of-the-art node classification frameworks, such as GIANT, use a two-stage pipeline: first embedding the text attributes of graph nodes then feeding the resulting embeddings into a node classification model. In this paper, we eliminate these two stages and develop an end-to-end node classification model that builds upon GIANT, called End-to-End-GIANT (E2EG). The tandem utilization of a main and an auxiliary classification objectives in our approach results in a more robust model, enabling the BERT backbone to be switched out for a distilled encoder with a 25% - 40% reduction in the number of parameters. Moreover, the model's end-to-end nature increases ease of use, as it avoids the need of chaining multiple models for node classification. Compared to a GIANT
    
[^171]: 使用单步 Q-learning 缓解 Actor-Critic 方法中的离策略偏差：一种新的纠正方法。

    Mitigating Off-Policy Bias in Actor-Critic Methods with One-Step Q-learning: A Novel Correction Approach. (arXiv:2208.00755v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.00755](http://arxiv.org/abs/2208.00755)

    本文提出一种新的策略相似度量来缓解离策略学习中的偏差问题，提供了一种自适应的、可扩展的解决方案。

    

    相较于基于策略的对比方法，离策略无模型深度强化学习可以通过重复使用以前收集的数据来提高数据使用效率。然而，当代理的策略和收集到的数据的基本分布之间的偏差增加时，离策略学习变得具有挑战性。尽管已经研究了重要性采样和离策略策略梯度技术来补偿这种偏差，但它们通常需要一系列长轨迹，并导致额外的问题，如消失/爆炸梯度或抛弃许多有用的经验，最终增加了计算复杂性。此外，它们对连续动作域或由确定性深度神经网络逼近的策略的泛化受到严格限制。为了克服这些限制，我们引入了一种新的策略相似度量来缓解连续控制中这种偏差的影响。我们的方法提供了一种自适应的、可扩展的解决方案，用于减轻 Actor-Critic 方法中离政策偏差的影响。

    Compared to on-policy counterparts, off-policy model-free deep reinforcement learning can improve data efficiency by repeatedly using the previously gathered data. However, off-policy learning becomes challenging when the discrepancy between the underlying distributions of the agent's policy and collected data increases. Although the well-studied importance sampling and off-policy policy gradient techniques were proposed to compensate for this discrepancy, they usually require a collection of long trajectories and induce additional problems such as vanishing/exploding gradients or discarding many useful experiences, which eventually increases the computational complexity. Moreover, their generalization to either continuous action domains or policies approximated by deterministic deep neural networks is strictly limited. To overcome these limitations, we introduce a novel policy similarity measure to mitigate the effects of such discrepancy in continuous control. Our method offers an ad
    
[^172]: 大规模超参数优化的异步分散贝叶斯优化方法

    Asynchronous Decentralized Bayesian Optimization for Large Scale Hyperparameter Optimization. (arXiv:2207.00479v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.00479](http://arxiv.org/abs/2207.00479)

    这项研究提出了一种异步分散的贝叶斯优化方法，可以实现大规模超参数优化，并在Polairs超级计算机上展示了模型准确性的改进。

    

    贝叶斯优化是深度神经网络超参数优化的一种有希望的方法，其中每个模型训练可能需要几分钟到几小时的时间。在贝叶斯优化中，采用计算便宜的替代模型来学习参数配置与性能（如准确性）之间的关系。并行贝叶斯优化方法通常采用单个管理器-多个工作者策略，同时评估多个超参数配置。尽管超参数评估时间相当长，但这种集中式方案的开销阻碍了这些方法在大量工作者上的扩展。我们提出了一种异步分散的贝叶斯优化方法，其中每个工作者运行顺序贝叶斯优化，并通过共享存储异步通信其结果。我们将我们的方法扩展到1,920个并行工作者（Polaris超级计算机的完整生产队列），并展示模型准确性的改进。

    Bayesian optimization (BO) is a promising approach for hyperparameter optimization of deep neural networks (DNNs), where each model training can take minutes to hours. In BO, a computationally cheap surrogate model is employed to learn the relationship between parameter configurations and their performance such as accuracy. Parallel BO methods often adopt single manager/multiple workers strategies to evaluate multiple hyperparameter configurations simultaneously. Despite significant hyperparameter evaluation time, the overhead in such centralized schemes prevents these methods to scale on a large number of workers. We present an asynchronous-decentralized BO, wherein each worker runs a sequential BO and asynchronously communicates its results through shared storage. We scale our method without loss of computational efficiency with above 95% of worker's utilization to 1,920 parallel workers (full production queue of the Polaris supercomputer) and demonstrate improvement in model accurac
    
[^173]: 利用语义角色上下文化的视频特征在EPIC-KITCHENS-100多实例文本-视频检索挑战2022中的应用

    Exploiting Semantic Role Contextualized Video Features for Multi-Instance Text-Video Retrieval EPIC-KITCHENS-100 Multi-Instance Retrieval Challenge 2022. (arXiv:2206.14381v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.14381](http://arxiv.org/abs/2206.14381)

    本论文提出了一种在EPIC-KITCHENS-100多实例检索挑战2022中利用语义角色上下文化的视频特征进行文本-视频检索的方法，通过三元损失函数在多个嵌入空间中融合视频和文本特征，超过了强基线，在nDCG和mAP方面获得了较好的排名。

    

    在这篇报告中，我们介绍了我们在EPIC-KITCHENS-100多实例检索挑战2022中的方法。我们首先将句子解析为与动词和名词相对应的语义角色；然后利用自注意力机制在多个嵌入空间中通过三元损失函数利用语义角色上下文化的视频特征和文本特征。我们的方法在标准化折扣累计增益（nDCG）方面超过了强基线，这对于语义相似度更有价值。我们的提交在nDCG排名第三，在mAP排名第四。

    In this report, we present our approach for EPIC-KITCHENS-100 Multi-Instance Retrieval Challenge 2022. We first parse sentences into semantic roles corresponding to verbs and nouns; then utilize self-attentions to exploit semantic role contextualized video features along with textual features via triplet losses in multiple embedding spaces. Our method overpasses the strong baseline in normalized Discounted Cumulative Gain (nDCG), which is more valuable for semantic similarity. Our submission is ranked 3rd for nDCG and ranked 4th for mAP.
    
[^174]: 具备辅助信息的优化

    Optimization with access to auxiliary information. (arXiv:2206.00395v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.00395](http://arxiv.org/abs/2206.00395)

    本文研究了在计算目标函数梯度很昂贵或有限的情况下，给定一些辅助函数的情况下，如何最小化目标函数。作者提出了两种通用的新算法，并证明了这个框架可以受益于目标和辅助信息之间的Hessian相似性假设。

    This paper investigates the fundamental optimization question of minimizing a target function with expensive or limited gradient computation, given access to some auxiliary side function with cheaper or more available gradients. The authors propose two generic new algorithms and prove that this framework can benefit from the Hessian similarity assumption between the target and side information.

    我们研究了基本的优化问题，即在计算目标函数$f(x)$的梯度很昂贵或有限的情况下，给定一些辅助函数$h(x)$的情况下，如何最小化目标函数。这个公式涵盖了许多实际相关的设置，如i）在SGD中重复使用批次，ii）迁移学习，iii）联邦学习，iv）使用压缩模型/丢弃等进行训练。我们提出了两种通用的新算法，适用于所有这些设置，并证明仅使用目标和辅助信息之间的Hessian相似性假设，我们可以从这个框架中受益。

    We investigate the fundamental optimization question of minimizing a target function $f(x)$ whose gradients are expensive to compute or have limited availability, given access to some auxiliary side function $h(x)$ whose gradients are cheap or more available. This formulation captures many settings of practical relevance such as i) re-using batches in SGD, ii) transfer learning, iii) federated learning, iv) training with compressed models/dropout, etc. We propose two generic new algorithms which are applicable in all these settings and prove using only an assumption on the Hessian similarity between the target and side information that we can benefit from this framework.
    
[^175]: NN2Poly：用于深度前馈人工神经网络的多项式表示方法

    NN2Poly: A polynomial representation for deep feed-forward artificial neural networks. (arXiv:2112.11397v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2112.11397](http://arxiv.org/abs/2112.11397)

    本文提出了一种名为NN2Poly的方法，用于生成已经训练好的全连接前馈人工神经网络的精确多项式表示。该方法适用于任意深度的多层感知器（MLP）且计算成本相对较低，能够在回归和分类任务中提供非常准确的逼近结果。

    

    尽管深度学习应用非常成功，但神经网络的可解释性和理论行为仍然是一个开放的研究领域。本文提出NN2Poly：一种理论方法，用于获取一个显式多项式模型，以提供已经训练好的全连接前馈人工神经网络（多层感知器或MLP）的精确表示。这种方法扩展了文献中提出的先前想法，该想法仅限于单隐藏层的网络，并且适用于回归和分类任务的任意深度MLP。本文的目标是通过在每层上使用激活函数的泰勒展开式，然后使用几个组合性质来计算所需多项式的系数，从而实现此目标。作者讨论了此方法的主要计算挑战以及通过引入一些逼近来克服这些挑战的方法，而不会影响其准确性。通过实验验证表明，尽管NN2Poly方法简单且计算成本低，但对于合成和真实数据集，提供非常准确的多项式逼近。

    Interpretability of neural networks and their underlying theoretical behavior remain an open field of study even after the great success of their practical applications, particularly with the emergence of deep learning. In this work, NN2Poly is proposed: a theoretical approach to obtain an explicit polynomial model that provides an accurate representation of an already trained fully-connected feed-forward artificial neural network (a multilayer perceptron or MLP). This approach extends a previous idea proposed in the literature, which was limited to single hidden layer networks, to work with arbitrarily deep MLPs in both regression and classification tasks. The objective of this paper is to achieve this by using a Taylor expansion on the activation function, at each layer, and then using several combinatorial properties to calculate the coefficients of the desired polynomials. Discussion is presented on the main computational challenges of this method, and the way to overcome them by i
    
[^176]: 可解释且交互式的深度多实例学习用于牙齿龋齿在多位X光片中的分类

    Interpretable and Interactive Deep Multiple Instance Learning for Dental Caries Classification in Bitewing X-rays. (arXiv:2112.09694v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2112.09694](http://arxiv.org/abs/2112.09694)

    本研究提出了一种可解释且交互式的深度多实例学习方法，用于在牙齿放射图中的龋齿分类。该方法首先输出局部补丁分类概率的热图，并可根据分割标签进行训练，与现有方法相比表现出了竞争性的性能，并且用户可以解释预测并与模型交互。

    

    我们提出了一种基于深度多实例学习的简单高效的图像分类架构，并将其应用于在牙科放射图中具有挑战性的龋齿检测任务。从技术上讲，我们的方法有两个贡献：首先，即使使用弱的图像级标签进行训练，它也能输出局部补丁分类概率的热图。其次，它适用于从分割标签中学习以指导训练。与现有方法相比，人类用户可以忠实地解释预测，并与模型交互以决定要关注的区域。在一个包含约38k个多位X光片（约316k个牙齿）的大型临床数据集上进行了实验，在与各种基线方法相比表现出了竞争性的性能。当由外部龋齿分割模型指导时，观察到分类和定位性能的显著改善。

    We propose a simple and efficient image classification architecture based on deep multiple instance learning, and apply it to the challenging task of caries detection in dental radiographs. Technically, our approach contributes in two ways: First, it outputs a heatmap of local patch classification probabilities despite being trained with weak image-level labels. Second, it is amenable to learning from segmentation labels to guide training. In contrast to existing methods, the human user can faithfully interpret predictions and interact with the model to decide which regions to attend to. Experiments are conducted on a large clinical dataset of $\sim$38k bitewings ($\sim$316k teeth), where we achieve competitive performance compared to various baselines. When guided by an external caries segmentation model, a significant improvement in classification and localization performance is observed.
    
[^177]: 路径正则化：一种对并行ReLU网络进行凸性和稀疏性引导的正则化方法

    Path Regularization: A Convexity and Sparsity Inducing Regularization for Parallel ReLU Networks. (arXiv:2110.09548v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.09548](http://arxiv.org/abs/2110.09548)

    路径正则化为并行ReLU网络提供了一种简化的凸优化问题，通过群稀疏性引导实现了凸模型，并提出了一个近似算法，在所有数据维度上具备完全多项式时间复杂度。

    

    理解深度神经网络成功背后的基本原理是当前文献中最重要的开放问题之一。为此，我们研究了深度神经网络的训练问题，并引入了一种分析方法来揭示优化景观中隐藏的凸性。我们考虑了深度并行ReLU网络架构，其也包括标准的深度网络和ResNet作为其特例。然后我们表明，基于路径正则化的训练问题可以表示为一个精确的凸优化问题。我们进一步证明等价的凸问题是通过一种群稀疏性引导的规范进行正则化的。因此，路径正则化的并行ReLU网络可以被视为高维中一种简化的凸模型。更重要的是，由于原始的训练问题可能无法在多项式时间内训练，我们提出了一个在所有数据维度上具有完全多项式时间复杂度的近似算法。然后，我们证明了强全局收敛性。

    Understanding the fundamental principles behind the success of deep neural networks is one of the most important open questions in the current literature. To this end, we study the training problem of deep neural networks and introduce an analytic approach to unveil hidden convexity in the optimization landscape. We consider a deep parallel ReLU network architecture, which also includes standard deep networks and ResNets as its special cases. We then show that pathwise regularized training problems can be represented as an exact convex optimization problem. We further prove that the equivalent convex problem is regularized via a group sparsity inducing norm. Thus, a path regularized parallel ReLU network can be viewed as a parsimonious convex model in high dimensions. More importantly, since the original training problem may not be trainable in polynomial-time, we propose an approximate algorithm with a fully polynomial-time complexity in all data dimensions. Then, we prove strong glob
    
[^178]: 深度生存剂量反应函数的连续治疗推荐

    Continuous Treatment Recommendation with Deep Survival Dose Response Function. (arXiv:2108.10453v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2108.10453](http://arxiv.org/abs/2108.10453)

    本论文提出了一个通用公式，称为深度生存剂量反应函数（DeepSDRF），用于解决临床生存数据中的连续治疗推荐问题。通过校正选择偏差，DeepSDRF估计的治疗效果可以用于开发推荐算法。在模拟研究和实际医学数据库上的测试中，DeepSDRF表现出良好的性能。

    

    我们提出了一个在临床生存数据设置中的连续治疗推荐问题的通用公式，称为深度生存剂量反应函数（DeepSDRF）。也就是说，我们考虑从历史数据中仅仅通过观察到的因素（混杂因子）对观察到的治疗和事件发生时间结果都有影响的条件平均剂量反应（CADR）函数的学习问题。从DeepSDRF中估计的治疗效果使我们能够开发具有选择偏差校正的推荐算法。我们比较了基于随机搜索和强化学习的两种推荐方法，并发现在患者结果方面表现相似。我们在大量的模拟研究和eICU研究机构（eRI）数据库上测试了DeepSDRF和相应的推荐器。据我们所知，这是首次在医学背景下使用因果模型来解决观察数据中的连续治疗效应问题。

    We propose a general formulation for continuous treatment recommendation problems in settings with clinical survival data, which we call the Deep Survival Dose Response Function (DeepSDRF). That is, we consider the problem of learning the conditional average dose response (CADR) function solely from historical data in which observed factors (confounders) affect both observed treatment and time-to-event outcomes. The estimated treatment effect from DeepSDRF enables us to develop recommender algorithms with the correction for selection bias. We compared two recommender approaches based on random search and reinforcement learning and found similar performance in terms of patient outcome. We tested the DeepSDRF and the corresponding recommender on extensive simulation studies and the eICU Research Institute (eRI) database. To the best of our knowledge, this is the first time that causal models are used to address the continuous treatment effect with observational data in a medical context.
    
[^179]: 来自自激和相互激发时间序列的因果图发现

    Causal Graph Discovery from Self and Mutually Exciting Time Series. (arXiv:2106.02600v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.02600](http://arxiv.org/abs/2106.02600)

    该论文提出了一个新的因果图发现方法，结合了广义线性结构因果模型和自适应正则化方法，通过解决凸优化问题来恢复因果DAGs，并建立了置信区间以量化不确定性，经过实验证明其在恢复高度可解释的因果DAGs上表现出竞争性能。

    

    我们提出了一个广义线性结构因果模型，并结合一种新颖的数据自适应线性正则化方法，从时间序列中恢复因果有向无环图(DAGs)。通过利用最近开发的随机单调变分不等式(VI)形式，我们将因果发现问题转化为一般的凸优化问题。此外，我们通过解决线性规划问题，建立了一种非渐进性的恢复保证和可量化的不确定性，以确定一系列非线性单调连接函数的置信区间。我们通过广泛的数值实验验证了理论结果的有效性和方法的竞争性能。最重要的是，我们展示了我们的方法在恢复高度可解释的因果DAGs上的有效性，尤其是在Sepsis相关紊乱(SADs)方面，同时实现了与强大的“黑匣子”模型（如XGBoost）相当的预测性能。因此，我们提出的方法在未来进行条件随机场建模和因果推断中的应用具有重要意义。

    We present a generalized linear structural causal model, coupled with a novel data-adaptive linear regularization, to recover causal directed acyclic graphs (DAGs) from time series. By leveraging a recently developed stochastic monotone Variational Inequality (VI) formulation, we cast the causal discovery problem as a general convex optimization. Furthermore, we develop a non-asymptotic recovery guarantee and quantifiable uncertainty by solving a linear program to establish confidence intervals for a wide range of non-linear monotone link functions. We validate our theoretical results and show the competitive performance of our method via extensive numerical experiments. Most importantly, we demonstrate the effectiveness of our approach in recovering highly interpretable causal DAGs over Sepsis Associated Derangements (SADs) while achieving comparable prediction performance to powerful ``black-box'' models such as XGBoost. Thus, the future adoption of our proposed method to conduct con
    
[^180]: 基于深度强化学习和历史驾驶经验的人类化能量管理

    Human-like Energy Management Based on Deep Reinforcement Learning and Historical Driving Experiences. (arXiv:2007.10126v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2007.10126](http://arxiv.org/abs/2007.10126)

    本文提出了一种基于深度强化学习和历史驾驶经验的人类化能量管理框架，通过采用深度确定性策略梯度算法和驾驶数据训练模型，提高了混合动力电动车能量管理的性能。

    

    混合动力电动车的发展依赖于先进高效的能量管理策略（EMS）。本文提出了一种基于深度强化学习方法和采集的历史驾驶数据的人类化能量管理框架。该研究中的混合动力传动系统采用串联-并联拓扑结构，并首先建立了面向控制的模型。然后引入了独特的深度强化学习算法——深度确定性策略梯度（DDPG）。为了提高深度强化学习框架中的功率分配控制，利用动态规划（DP）获得的全局最优控制轨迹作为专家知识来训练DDPG模型。这个操作确保了所提出的控制架构的最优性。此外，利用基于有经验驾驶员的采集的历史驾驶数据来替代基于DP的控制，从而进一步提升能量管理的性能。

    Development of hybrid electric vehicles depends on an advanced and efficient energy management strategy (EMS). With online and real-time requirements in mind, this article presents a human-like energy management framework for hybrid electric vehicles according to deep reinforcement learning methods and collected historical driving data. The hybrid powertrain studied has a series-parallel topology, and its control-oriented modeling is founded first. Then, the distinctive deep reinforcement learning (DRL) algorithm, named deep deterministic policy gradient (DDPG), is introduced. To enhance the derived power split controls in the DRL framework, the global optimal control trajectories obtained from dynamic programming (DP) are regarded as expert knowledge to train the DDPG model. This operation guarantees the optimality of the proposed control architecture. Moreover, the collected historical driving data based on experienced drivers are employed to replace the DP-based controls, and thus c
    

