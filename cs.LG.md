# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [NeuralClothSim: Neural Deformation Fields Meet the Kirchhoff-Love Thin Shell Theory.](http://arxiv.org/abs/2308.12970) | 本文提出了一种新的布料模拟方法 NeuralClothSim，使用薄壳理论和神经变形场进行表面演化，克服了现有布料模拟方法的局限性和挑战，为物理合理的布料模拟提供了一种全新的视角。 |
| [^2] | [Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation.](http://arxiv.org/abs/2308.12968) | Scenimefy是一个通过半监督的图像到图像翻译框架，可以自动从现实世界图像中渲染高质量的动漫场景，并且能够保持一致的语义、明显的风格化和精细的细节。 |
| [^3] | [NeO 360: Neural Fields for Sparse View Synthesis of Outdoor Scenes.](http://arxiv.org/abs/2308.12967) | NeO 360是一种用于稀疏视角合成户外场景的通用方法，通过捕获复杂的真实世界场景分布，使用混合的图像条件式三平面表示，实现了从单个或少数几个图像重建360度场景，并具有更好的效果和表达能力。 |
| [^4] | [Dense Text-to-Image Generation with Attention Modulation.](http://arxiv.org/abs/2308.12964) | 提出了DenseDiffusion方法，该方法可以使预训练的文本到图像模型处理稠密标题并具有场景布局控制，无需额外微调或数据集，提升了图像生成性能。 |
| [^5] | [DLIP: Distilling Language-Image Pre-training.](http://arxiv.org/abs/2308.12956) | 本文提出了DLIP，提取语言-图像预训练的方法，通过对不同模块的架构特性和不同模态的信息传递进行深入研究，探索了如何提取轻量级但性能优越的视觉-语言预训练模型。实验结果显示DLIP能达到最先进的准确性和效率平衡。 |
| [^6] | [BridgeData V2: A Dataset for Robot Learning at Scale.](http://arxiv.org/abs/2308.12952) | BridgeData V2是一个大规模且多样化的机器人操作行为数据集，广泛应用于机器人学习研究。该数据集具备任务和环境变异性，并且兼容多种学习方法，通过实验表明，使用此数据集可以提高模型性能。 |
| [^7] | [Label Budget Allocation in Multi-Task Learning.](http://arxiv.org/abs/2308.12949) | 该论文首次提出并定义了多任务学习中的标签预算分配问题，提出了一种任务自适应预算分配算法，通过估计和最大化新信息的程度来提高多任务学习的性能。 |
| [^8] | [Learning Only On Boundaries: a Physics-Informed Neural operator for Solving Parametric Partial Differential Equations in Complex Geometries.](http://arxiv.org/abs/2308.12939) | 这项研究提出了一种面向物理的神经算子方法，用于在复杂几何参数下解决无标记数据的边界值问题。该方法通过将PDE重新表述为BIEs，并仅在域的边界上训练算子网络，显著加速了训练过程，并且可以处理无界问题。 |
| [^9] | [Low-count Time Series Anomaly Detection.](http://arxiv.org/abs/2308.12925) | 本论文提出了一种解决低计数时间序列异常检测的方法。通过引入新的生成过程来创建包含异常片段的基准数据集，并通过理论和实证分析解释了常用算法在正常和异常片段之间的分布重叠问题。 |
| [^10] | [Diagnosing Infeasible Optimization Problems Using Large Language Models.](http://arxiv.org/abs/2308.12923) | 本文提出了OptiChat，一种基于自然语言的系统，用于诊断不可行的优化模型。它可以帮助从业人员理解和解释不可行的优化模型，无需具备深厚的优化背景知识。 |
| [^11] | [An Efficient Distributed Multi-Agent Reinforcement Learning for EV Charging Network Control.](http://arxiv.org/abs/2308.12921) | 这个论文提出了一种分散式多智能体强化学习的EV充电控制框架，通过保护隐私和减少网络成本来改善充电网络的性能。 |
| [^12] | [Towards Realistic Unsupervised Fine-tuning with CLIP.](http://arxiv.org/abs/2308.12919) | 本论文针对无监督微调中可能出现的未知类别和超出分布范围的问题，提出了一种称为UEO的简单、高效、有效的微调方法，该方法能够同时提高对超出分布样本的检测能力和预定义类别实例的识别能力。 |
| [^13] | [Evaluating the Vulnerabilities in ML systems in terms of adversarial attacks.](http://arxiv.org/abs/2308.12918) | 本研究评估了机器学习系统在面对对抗攻击时的漏洞，并讨论了漏洞可能的原因、对抗攻击与随机化示例的差异以及相关的道德问题。 |
| [^14] | [POLCA: Power Oversubscription in LLM Cloud Providers.](http://arxiv.org/abs/2308.12908) | 本文研究了在大型语言模型（LLM）云服务提供商中的功率超额使用问题。通过对多种LLM及其不同配置的功耗模式进行分析，我们发现在LLM集群中存在显著的功率超额使用机会，这可以提高数据中心的功率效率，并且允许更多的服务器部署，同时减少部署时间。 |
| [^15] | [CDAN: Convolutional Dense Attention-guided Network for Low-light Image Enhancement.](http://arxiv.org/abs/2308.12902) | 本研究提出了一种名为CDAN的卷积稠密注意力引导网络，用于低光图像增强。该网络结合了自编码器架构、卷积和稠密块、注意力机制和跳跃连接，通过专门的后处理阶段进一步改善色彩平衡和对比度。与现有方法相比，在低光图像增强方面取得了显著的进展，展示了在各种具有挑战性的场景中的稳健性。 |
| [^16] | [Unified Data Management and Comprehensive Performance Evaluation for Urban Spatial-Temporal Prediction [Experiment, Analysis & Benchmark].](http://arxiv.org/abs/2308.12899) | 该论文提出了针对城市时空预测的统一数据管理和综合性能评估方法。其贡献包括引入“原子文件”作为统一存储格式，提供了城市时空预测模型技术进展的全面概述，并建立了性能排行榜和鉴定了有潜力的模型和数据集。 |
| [^17] | [Beyond Document Page Classification: Design, Datasets, and Challenges.](http://arxiv.org/abs/2308.12896) | 本文强调了将文档分类基准测试更接近于现实世界应用的需求，通过提出多页文档分类数据集和不同分类任务，以及高效的多页文档表示，来解决现有基准测试不适用于实际完整文档评估的问题。 |
| [^18] | [Collect, Measure, Repeat: Reliability Factors for Responsible AI Data Collection.](http://arxiv.org/abs/2308.12885) | 对于负责任的AI数据收集，需要对数据的质量进行彻底的审查，避免不公平、偏见或不准确的结果。 |
| [^19] | [LCANets++: Robust Audio Classification using Multi-layer Neural Networks with Lateral Competition.](http://arxiv.org/abs/2308.12882) | LCANets++是一种使用多层神经网络和层间竞争的鲁棒性音频分类方法，通过稀疏编码来提高对扰动和对抗攻击的抵抗力。 |
| [^20] | [Easy attention: A simple self-attention mechanism for Transformers.](http://arxiv.org/abs/2308.12874) | 本论文提出了一种名为简易注意力的注意力机制，用于提高Transformer神经网络在混沌系统时间动态预测中的鲁棒性。该方法不依赖于键、查询和softmax，直接将注意力得分作为可学习参数。实验结果表明，该方法在重构和预测混沌系统的时间动态方面比传统的自注意机制和长短期记忆方法更具鲁棒性和简化性。 |
| [^21] | [IPA: Inference Pipeline Adaptation to Achieve High Accuracy and Cost-Efficiency.](http://arxiv.org/abs/2308.12871) | 提出了一种名为IPA的在线深度学习推理管道自适应系统，通过动态配置批处理大小、复制和模型变体，以优化准确性、最小化成本并满足用户定义的延迟要求。 |
| [^22] | [Auto-weighted Bayesian Physics-Informed Neural Networks and robust estimations for multitask inverse problems in pore-scale imaging of dissolution.](http://arxiv.org/abs/2308.12864) | 本文介绍了一种新的数据同化策略，可可靠地处理包含不确定度量化的孔隙尺度反应反问题。该方法结合了数据驱动和物理建模，确保了孔隙尺度模型的可靠校准。 |
| [^23] | [Towards Automated Animal Density Estimation with Acoustic Spatial Capture-Recapture.](http://arxiv.org/abs/2308.12859) | 本论文提出了一种声学空间捕获-再捕获方法，通过结合个体级别的测量特征，实现了自动动物密度估计。 |
| [^24] | [Fast Adversarial Training with Smooth Convergence.](http://arxiv.org/abs/2308.12857) | 本论文提出了一种快速对抗训练方法（FAT），通过引入平滑收敛过程和振荡约束来解决在处理大的扰动预算时出现的灾难性过拟合问题。 |
| [^25] | [Probabilistic load forecasting with Reservoir Computing.](http://arxiv.org/abs/2308.12844) | 这项研究将储层计算作为核心时间序列预测方法，探索了贝叶斯和确定性方法在不确定性估计方面的兼容性，并评估比较了它们在预测准确性、计算资源效率和估计可靠性方面的表现。 |
| [^26] | [Actuator Trajectory Planning for UAVs with Overhead Manipulator using Reinforcement Learning.](http://arxiv.org/abs/2308.12843) | 本文研究了使用强化学习控制无人机顶部机械臂执行器轨迹的方法，并提出了基于时间到碰撞的运动规划模型以绕过障碍物。同时，利用基于模型的Q-learning模型独立追踪和控制机械臂末端执行器的期望轨迹。通过这种方法可以实现一系列在高难度和危险环境中的执行任务。 |
| [^27] | [Short Run Transit Route Planning Decision Support System Using a Deep Learning-Based Weighted Graph.](http://arxiv.org/abs/2308.12828) | 提出了一种基于深度学习的短途公交线路规划决策支持系统，通过调整路线的特定部分，减少时间并提升公共交通服务。利用多样化的数据源，通过预测道路段的延迟值作为边权重，实现了快速路径规划决策。 |
| [^28] | [Prediction without Preclusion: Recourse Verification with Reachable Sets.](http://arxiv.org/abs/2308.12820) | 这项研究引入了一种称为后续验证的正式测试程序，用于检测模型分配固定预测的情况。通过开发可靠的机制，可以确定给定模型是否能为决策对象提供后续措施，从而解决了模型分配固定预测可能带来的问题。该研究还展示了如何在真实世界的数据集中确保后续措施和对抗鲁棒性，并探讨了在贷款数据集中实现后续措施的不可行性。 |
| [^29] | [ICU Mortality Prediction Using Long Short-Term Memory Networks.](http://arxiv.org/abs/2308.12800) | 该论文通过使用长短期记忆网络分析床边监测产生的时间数据，成功构建了用于预测ICU患者死亡率和住院时间的系统。 |
| [^30] | [Job Shop Scheduling Benchmark: Environments and Instances for Learning and Non-learning Methods.](http://arxiv.org/abs/2308.12794) | 这个开源的GitHub仓库为机器调度问题提供了综合基准，包括多种环境和实例，为研究人员和从业者提供了一个集中的中心。 |
| [^31] | [Single-shot Bayesian approximation for neural networks.](http://arxiv.org/abs/2308.12785) | 这篇论文提出了一种单次MC dropout近似方法，以将神经网络转换为贝叶斯变体的神经网络，该方法具有与普通神经网络相同的计算速度，同时保留了贝叶斯变体神经网络提供的不确定度测量。 |
| [^32] | [Intentionally-underestimated Value Function at Terminal State for Temporal-difference Learning with Mis-designed Reward.](http://arxiv.org/abs/2308.12772) | 本文提出了一种方法来解决在时间差异学习中终止状态下存在的奖励设计问题，通过故意低估终止后的值来避免学习错误的策略。 |
| [^33] | [On the Consistency of Average Embeddings for Item Recommendation.](http://arxiv.org/abs/2308.12767) | 本文研究了推荐系统中平均嵌入的一致性，并提出了一种衡量方法。实证结果表明，现实世界的平均嵌入在推荐中一致性较低，为进一步改进现实世界嵌入提供了方向。 |
| [^34] | [IP-UNet: Intensity Projection UNet Architecture for 3D Medical Volume Segmentation.](http://arxiv.org/abs/2308.12761) | IP-UNet是一种用于3D医学体积分割的深度学习方法，通过对3D体积数据的强度投影进行多类别分割，在保留原始分辨率的同时减少内存消耗。 |
| [^35] | [Motion In-Betweening with Phase Manifolds.](http://arxiv.org/abs/2308.12751) | 本论文介绍了一种使用相位流形的动作插帧系统，通过学习相位变量和混合专家神经网络模型来生成目标姿势之间的连续姿势序列，同时可以满足动画师手动修改的姿势和末端效应器作为约束的要求。 |
| [^36] | [Human Comprehensible Active Learning of Genome-Scale Metabolic Networks.](http://arxiv.org/abs/2308.12740) | 这项研究介绍了一种人类可理解的基因组规模代谢网络的主动学习方法，基于归纳逻辑编程(ILP)框架进行逻辑推理，并通过从实验中学习新的逻辑结构，以有效探索假设空间和指导实验设计。 |
| [^37] | [Real-time Detection of AI-Generated Speech for DeepFake Voice Conversion.](http://arxiv.org/abs/2308.12734) | 该研究创建了DEEP-VOICE数据集，并通过统计分析和机器学习模型实现了实时检测AI生成语音的目标，以应对DeepFake语音转换带来的道德和隐私问题。 |
| [^38] | [Out of the Box Thinking: Improving Customer Lifetime Value Modelling via Expert Routing and Game Whale Detection.](http://arxiv.org/abs/2308.12729) | 本文提出了一种新颖的多任务框架ExpLTV，利用游戏鲸鱼的检测来改进客户终身价值预测模型的准确性。 |
| [^39] | [Continuous Reinforcement Learning-based Dynamic Difficulty Adjustment in a Visual Working Memory Game.](http://arxiv.org/abs/2308.12726) | 本文提出了一种基于连续强化学习的动态难度调整方法，用于处理视觉工作记忆游戏中的复杂难度记忆问题。通过根据玩家的得分和上一轮游戏的难度量度来调整游戏难度，该方法在52位受试者的实验中得到了评估和比较。 |
| [^40] | [Solving Forward and Inverse Problems of Contact Mechanics using Physics-Informed Neural Networks.](http://arxiv.org/abs/2308.12716) | 本文研究了使用物理信息神经网络（PINNs）解决接触力学的前向和反向问题，并将不等式约束转化为损失函数中的软约束。实验证明PINNs可以作为PDE求解器、数据增强的前向模型和反向求解器。 |
| [^41] | [Disentanglement Learning via Topology.](http://arxiv.org/abs/2308.12696) | 本文提出了一种通过拓扑损失实现解缠编码的方法，这是第一个提出用于解缠的可微拓扑损失的论文，实验结果表明所提出的方法相对于最新结果改进了解缠得分。 |
| [^42] | [An Efficient Data Analysis Method for Big Data using Multiple-Model Linear Regression.](http://arxiv.org/abs/2308.12691) | 本文介绍了一种使用多模型线性回归（MMLR）的高效数据分析方法，该方法将输入数据集分为子集并构建局部线性回归模型。通过提出的近似算法，该方法在时间复杂性和预测准确度方面都表现出良好的性能。 |
| [^43] | [Match-And-Deform: Time Series Domain Adaptation through Optimal Transport and Temporal Alignment.](http://arxiv.org/abs/2308.12686) | 本文提出了一种名为Match-And-Deform（MAD）的方法，通过最优传输和时间对齐，在时间序列领域自适应问题中找到对应关系并允许时间失真，实验结果表明MAD可以生成对齐领域并最大化网络判别能力的新时间序列表示形式。 |
| [^44] | [LR-XFL: Logical Reasoning-based Explainable Federated Learning.](http://arxiv.org/abs/2308.12681) | LR-XFL是一种基于逻辑推理的可解释联邦学习方法，通过将逻辑规则和模型更新结合起来，实现了对FL模型的解释性提升和加权聚合，并在相关基准测试中取得了较好的效果。 |
| [^45] | [Master-slave Deep Architecture for Top-K Multi-armed Bandits with Non-linear Bandit Feedback and Diversity Constraints.](http://arxiv.org/abs/2308.12680) | 这篇论文提出了一种用于解决具有非线性边际反馈和多样性约束的前K个组合多臂赌博机问题的新型主从架构，通过引入六个从模型和教师学习优化以及策略共训练技术，实现了高效的探索和利用之间的决策权衡。 |
| [^46] | [A Continual Learning Approach for Cross-Domain White Blood Cell Classification.](http://arxiv.org/abs/2308.12679) | 提出了一种基于重听的持续学习方法，用于跨领域白细胞分类，通过选择代表样本来避免灾难性遗忘。 |
| [^47] | [Masked Feature Modelling: Feature Masking for the Unsupervised Pre-training of a Graph Attention Network Block for Bottom-up Video Event Recognition.](http://arxiv.org/abs/2308.12673) | 本文提出了一种新的遮罩特征建模方法(MFM)，用于无监督预训练图注意力网络块。通过利用预训练的视觉分词器来重构视频中对象的遮罩特征，将预训练的GAT块融入到视频事件识别架构ViGAT中，以改善模型的性能。 |
| [^48] | [Optimal data pooling for shared learning in maintenance operations.](http://arxiv.org/abs/2308.12670) | 本文研究了在维护操作中，通过数据共享学习可以显著降低成本的最优数据汇聚方法。 |
| [^49] | [Geodesic Mode Connectivity.](http://arxiv.org/abs/2308.12666) | 本文探讨了测地线模式连通性的现象，并提出了一种算法近似测地线，实现了模式连通性。 |
| [^50] | [Don't Look into the Sun: Adversarial Solarization Attacks on Image Classifiers.](http://arxiv.org/abs/2308.12661) | 通过对图像进行曝光攻击，我们引入了一种简单而有效的方法来评估图像分类器的鲁棒性，并证明该攻击能够显著降低准确性。 |
| [^51] | [APART: Diverse Skill Discovery using All Pairs with Ascending Reward and DropouT.](http://arxiv.org/abs/2308.12649) | 本文提出了一个名为APART的方法，使用全组对判别器、新颖的内在奖励函数和丢弃技术，在无奖励环境中实现了多样化技能的发现。该方法具有更高的效率，并在简单的网格世界环境中发现了所有可能的技能。 |
| [^52] | [The GENEA Challenge 2023: A large scale evaluation of gesture generation models in monadic and dyadic settings.](http://arxiv.org/abs/2308.12646) | GENEA Challenge 2023是一项大规模评估手势生成模型的挑战。参与团队使用相同语音和运动数据集构建了基于语音驱动的手势生成系统，并通过几项用户研究评估了这些系统的人类相似度以及对于发言者和互动者行为的适应性。 |
| [^53] | [Towards Hierarchical Regional Transformer-based Multiple Instance Learning.](http://arxiv.org/abs/2308.12634) | 本文提出了一种基于变压器的多实例学习方法，通过使用区域自注意力机制，融合区域补丁信息以得出滑片级别预测，并通过堆叠区域聚合来分层处理特征。此外，引入了一种方法来聚焦于高关注区域，从而提高预测准确性。这种方法在组织病理学图像分类任务上表现出了显著的性能改进，并为进一步研究提供了有希望的方向。 |
| [^54] | [Uncertainty and Explainable Analysis of Machine Learning Model for Reconstruction of Sonic Slowness Logs.](http://arxiv.org/abs/2308.12625) | 本论文提出了一种利用机器学习模型重建声速测井曲线的方法，并通过NGBoost算法构建了一个能够预测结果及其不确定性的集成学习模型，同时结合了SHAP方法进行模型的可解释性分析。实验结果表明，NGBoost模型在测试中表现良好。 |
| [^55] | [Try with Simpler -- An Evaluation of Improved Principal Component Analysis in Log-based Anomaly Detection.](http://arxiv.org/abs/2308.12612) | 本论文通过改进传统的主成分分析方法，优化了基于日志的异常检测技术，以提高其效果，从而使其与深度学习方法相媲美。 |
| [^56] | [A Greedy Approach for Offering to Telecom Subscribers.](http://arxiv.org/abs/2308.12606) | 本论文提出了一个用于解决套餐优化问题的新颖组合算法，该算法针对电信运营商在选择激励套餐和目标用户时面临的困难进行了解决。 |
| [^57] | [Exploiting Time-Frequency Conformers for Music Audio Enhancement.](http://arxiv.org/abs/2308.12599) | 这项研究提出了一种基于Conformer架构的音频增强系统，通过探索其注意机制并测试性能，实现了在单音乐音频增强方面的最先进性能。 |
| [^58] | [Persistent learning signals and working memory without continuous attractors.](http://arxiv.org/abs/2308.12585) | 本研究发现，在神经动力系统中，除了广泛研究的连续吸引子外，周期性和准周期性吸引子也可以支持学习时间关系。相比于连续吸引子，准周期性吸引子更适合学习产生时间结构化行为，并且对于人工学习系统的设计和生物神经动力学的可观测特征都有重要意义。 |
| [^59] | [LORD: Leveraging Open-Set Recognition with Unknown Data.](http://arxiv.org/abs/2308.12584) | 本文提出了一种名为LORD的框架，通过利用未知数据进行开放式识别。LORD在分类器训练过程中明确地建模开放空间，并通过三种模型无关的训练策略实现了对未知数据识别能力的改进。 |
| [^60] | [A Huber Loss Minimization Approach to Byzantine Robust Federated Learning.](http://arxiv.org/abs/2308.12581) | 本文介绍了一种基于Huber损失最小化的新型聚合器用于拜占庭鲁棒的联邦学习。在独立同分布假设下，该方法具有与现有方法相比的优势，并对非i.i.d数据进行了扩展分析。 |
| [^61] | [Hypergraph Convolutional Networks for Fine-grained ICU Patient Similarity Analysis and Risk Prediction.](http://arxiv.org/abs/2308.12575) | 本研究提出了一种利用超图卷积网络进行ICU患者相似性分析和风险预测的新方法，可以捕捉隐藏的特征结构，并应用于个性化的死亡风险预测。 |
| [^62] | [Conditional Kernel Imitation Learning for Continuous State Environments.](http://arxiv.org/abs/2308.12573) | 本研究以连续状态空间环境为基础，仅凭观察到的行为进行仿真学习，无需访问转移动力学信息、奖励结构，或者任何额外的交互。 |
| [^63] | [Multivariate Time-Series Anomaly Detection with Contaminated Data: Application to Physiological Signals.](http://arxiv.org/abs/2308.12563) | 这个论文介绍了一种针对带有污染数据的多变量时间序列异常检测的新方法，通过去污和变量依赖建模实现了无监督的异常检测，对于实际场景中的异常检测具有重要意义。 |
| [^64] | [Variational Information Pursuit with Large Language and Multimodal Models for Interpretable Predictions.](http://arxiv.org/abs/2308.12562) | 本文提出了一种利用大型语言和多模态模型的变分信息追求(V-IP)框架，通过顺序选择任务相关的可解释查询，实现可解释预测。为了解决数据标注的限制，引入了基础模型(FMs)，使用大型语言模型(LLMs)生成候选可解释概念集，并使用大型多模态模型注释每个数据样本。此方法适用于大规模任务。 |
| [^65] | [Deep Reinforcement Learning-driven Cross-Community Energy Interaction Optimal Scheduling.](http://arxiv.org/abs/2308.12554) | 本文提出了一种基于深度强化学习的跨社区能量交互优化调度模型，通过学习不同社区的负载特性，并基于该知识做出决策，实现综合能量系统的整体优化和调度。 |
| [^66] | [Don't blame Dataset Shift! Shortcut Learning due to Gradients and Cross Entropy.](http://arxiv.org/abs/2308.12553) | 默认-ERM模型通过最大化间隔来优化训练，导致模型更多依赖于捷径而非稳定特征，这对感知任务来说是不合适的。 |
| [^67] | [A Co-training Approach for Noisy Time Series Learning.](http://arxiv.org/abs/2308.12551) | 本研究提出了一种用于嘈杂时间序列学习的协同训练方法，通过多视图学习来减轻数据噪声和损坏的影响，并在多个时间序列基准上超越现有方法。 |
| [^68] | [CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias.](http://arxiv.org/abs/2308.12539) | CALM是一个用于量化语言模型偏见的多任务基准数据集，相比先前数据集更加多样和可靠，能更好地捕捉评估模型偏见所需的语言变化的广度。 |
| [^69] | [FedSoL: Bridging Global Alignment and Local Generality in Federated Learning.](http://arxiv.org/abs/2308.12532) | FedSoL提出了一种联邦学习的方法，该方法旨在解决数据分布不均匀导致性能下降的问题。它通过平衡全局对齐和本地一般性来改善FL的学习效果。 |
| [^70] | [SieveNet: Selecting Point-Based Features for Mesh Networks.](http://arxiv.org/abs/2308.12530) | SieveNet是一种新的网状网络范式，结合了规则拓扑和准确几何信息，通过重网状和点采样的方法提取特征，消除了对手工特征工程的需求，并且可以应用于3D计算机视觉和图形领域。 |
| [^71] | [UNISOUND System for VoxCeleb Speaker Recognition Challenge 2023.](http://arxiv.org/abs/2308.12526) | UNISOUND团队在VoxCeleb Speaker Recognition Challenge 2023中提交的系统通过一致性感知的得分校准方法和大规模ResNet和RepVGG架构的使用，在Track1中获得第一名，在Track2中获得第二名，并实现了较低的minDCF和EER。 |
| [^72] | [Not Only Rewards But Also Constraints: Applications on Legged Robot Locomotion.](http://arxiv.org/abs/2308.12517) | 本文提出了一种新的强化学习框架，为复杂机器人系统训练神经网络控制器。该框架引入了奖励和约束的概念，通过设计高效的策略优化算法来处理约束，以减少计算开销。通过应用于不同腿式机器人的运动控制器训练中，展示了该框架的有效性。 |
| [^73] | [Masked Autoencoders are Efficient Class Incremental Learners.](http://arxiv.org/abs/2308.12510) | 本论文提出了使用掩蔽自编码器(MAEs)作为高效的分类增量学习器，通过重建原始输入图像和学习图像级和嵌入级融合来存储和学习过去任务的表示。实验证实，在CIFAR-100，ImageNet-Subset和ImageNet-Full上，该方法优于现有最先进的方法。 |
| [^74] | [False Information, Bots and Malicious Campaigns: Demystifying Elements of Social Media Manipulations.](http://arxiv.org/abs/2308.12497) | 本文通过综合多学科的见解，对社交媒体操纵的要素进行了综合分析，包括虚假信息、机器人和恶意宣传，并研究了它们之间的相互关系。 |
| [^75] | [Optimizing Neural Network Scale for ECG Classification.](http://arxiv.org/abs/2308.12492) | 该论文研究了缩放卷积神经网络在心电图分类中的应用，通过优化网络规模的关键参数（层深度、通道数和卷积核大小），得出了在ECG分类中较浅的网络、较大的通道数和较小的卷积核尺寸可以获得更好性能的结论。 |
| [^76] | [Fall Detection using Knowledge Distillation Based Long short-term memory for Offline Embedded and Low Power Devices.](http://arxiv.org/abs/2308.12481) | 本文提出了一种成本效益高、低功耗的方法来进行意外摔倒检测，并使用了基于知识蒸馏的LSTM模型来显著提高准确性。该解决方案通过分析各种传感器收集的时间序列数据实现实时检测能力，并采用知识蒸馏技术来提高模型的精度和降低功耗。 |
| [^77] | [Zero-delay Consistent Signal Reconstruction from Streamed Multivariate Time Series.](http://arxiv.org/abs/2308.12459) | 本文介绍了一种从流式多元时间序列中一致地重建信号的方法，同时减少了零延迟信号重建的粗糙度。 (arXiv:2308.12459v1 [eess.SP]) |
| [^78] | [PFL-GAN: When Client Heterogeneity Meets Generative Models in Personalized Federated Learning.](http://arxiv.org/abs/2308.12454) | PFL-GAN是一种在个性化联合学习中解决客户异质性的新型GAN共享和聚合策略，通过学习客户间的相似度并采用加权协同数据聚合方法来实现。 |
| [^79] | [Augmenting medical image classifiers with synthetic data from latent diffusion models.](http://arxiv.org/abs/2308.12453) | 使用潜在扩散模型生成的合成数据可提高医学图像分类器的性能，在数据受限的情况下表现出饱和效果，比添加真实图像获得的性能提升要小得多。 |
| [^80] | [An Intentional Forgetting-Driven Self-Healing Method For Deep Reinforcement Learning Systems.](http://arxiv.org/abs/2308.12445) | 本文提出了一种名为 Dr. DRL 的自愈方法，用于解决深度强化学习系统中的一些效率问题，该方法通过在连续学习中引入有意遗忘的机制来应对环境漂移引起的困扰。 |
| [^81] | [TAI-GAN: Temporally and Anatomically Informed GAN for early-to-late frame conversion in dynamic cardiac PET motion correction.](http://arxiv.org/abs/2308.12443) | TAI-GAN是一种用于动态心脏PET运动校正的生成对抗网络，通过时间和解剖信息感知实现早期到晚期帧的转换。 |
| [^82] | [BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection.](http://arxiv.org/abs/2308.12439) | BaDExpert是一种防御后门攻击的新方法，通过逆向工程提取给定后门模型的后门功能，并生成一个专家模型，该模型只能识别后门输入。可以进一步使用该模型设计高度准确的后门输入检测器。 |
| [^83] | [Deploying Deep Reinforcement Learning Systems: A Taxonomy of Challenges.](http://arxiv.org/abs/2308.12438) | 本文通过对开发者问答论坛Stack Overflow上的帖子进行实证研究，总结了部署深度强化学习系统所面临的挑战，并针对不同的部署平台进行了分类。 |
| [^84] | [Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature.](http://arxiv.org/abs/2308.12420) | 本研究通过NLP分析了ESG主导的DLT研究的演化，通过构建引用网络和命名实体识别任务，对DLT在ESG背景下的发展进行了文献综述。 |
| [^85] | [Machine learning in parameter estimation of nonlinear systems.](http://arxiv.org/abs/2308.12393) | 该论文提出了一种使用神经网络和Huber损失函数进行非线性系统参数估计的新方法，并通过验证实验展示了其精确性和鲁棒性。 |
| [^86] | [FOSA: Full Information Maximum Likelihood (FIML) Optimized Self-Attention Imputation for Missing Data.](http://arxiv.org/abs/2308.12388) | FOSA是一种全信息最大似然 (FIML) 优化的自注意力缺失数据补全方法，通过融合FIML估计和自注意力机制，实现了在准确性、计算效率和适应不同数据结构方面的显著优势。 |
| [^87] | [Inferring gender from name: a large scale performance evaluation study.](http://arxiv.org/abs/2308.12381) | 本研究评估了从姓名中推断性别的性能，该方法在没有性别信息的情况下是一种可行且广泛应用的方法。其重要性在于研究各种科学学科中对性别差异的模式和决定因素进行分析。 |
| [^88] | [Open-set Face Recognition with Neural Ensemble, Maximal Entropy Loss and Feature Augmentation.](http://arxiv.org/abs/2308.12371) | 本文介绍了一种新颖的方法，将一组紧凑的神经网络与基于边缘的成本函数相结合，通过探索附加样本来提高开放集人脸识别的准确性。该方法利用外部数据库获取辅助负样本或通过混合特征增强方法在训练过程中合成建立负样本。实验结果表明，该方法能够提升封闭集的准确性。 |
| [^89] | [SafeAR: Towards Safer Algorithmic Recourse by Risk-Aware Policies.](http://arxiv.org/abs/2308.12367) | 本文提出了一种更安全的算法补救方法（SafeAR），该方法通过考虑风险因素在计算和评估补救措施时，为那些受到机器学习模型决策不利影响的个体提供更可靠的建议。 |
| [^90] | [Renormalizing Diffusion Models.](http://arxiv.org/abs/2308.12355) | 该论文介绍了如何使用扩散模型学习统计学和量子场论的逆规范化群流，为构建用于研究场论的基于机器学习的模型提供了具体框架，并详细说明了这些模型如何定义一类自适应桥接取样器。 |
| [^91] | [Machine Learning Small Molecule Properties in Drug Discovery.](http://arxiv.org/abs/2308.12354) | 这篇综述介绍了机器学习在药物发现中预测小分子性质的方法，并讨论了不同性质的预测和优化挑战以及多目标优化技术的应用。同时评估了提供模型预测理解的技术对药物发现中的决策的重要性。 |
| [^92] | [Improving Generative Model-based Unfolding with Schr\"{o}dinger Bridges.](http://arxiv.org/abs/2308.12351) | 本研究提出了一种使用Schrödinger桥和扩散模型创建的展开方法SBUnfold，它将判别模型和生成模型的优势结合起来。与最先进方法相比，在合成的Z+jets数据集上获得了卓越的性能。 |
| [^93] | [Predicting Drug Solubility Using Different Machine Learning Methods -- Linear Regression Model with Extracted Chemical Features vs Graph Convolutional Neural Network.](http://arxiv.org/abs/2308.12325) | 本研究利用线性回归模型和图卷积神经网络模型预测药物溶解度，其中图卷积神经网络模型表现最好。通过线性回归模型的特征重要性分析，可以了解每个功能团对溶解度的影响。将图卷积神经网络的高性能与线性回归的可解释性相结合是未来工作的方向。 |
| [^94] | [Graph Neural Stochastic Differential Equations.](http://arxiv.org/abs/2308.12316) | 图神经随机微分方程（Graph Neural SDEs）通过将随机性嵌入到图神经常微分方程中，提供了一种评估预测不确定性的方法，尤其在静态和时空背景下的分布外检测方面表现出色。 |
| [^95] | [Trustworthy Representation Learning Across Domains.](http://arxiv.org/abs/2308.12315) | 本论文首次提出了跨领域的可信表示学习框架，通过包括鲁棒性、隐私、公平性和可解释性等概念，对该研究方向进行了全面的文献综述。 |
| [^96] | [Fast Exact NPN Classification with Influence-aided Canonical Form.](http://arxiv.org/abs/2308.12311) | 本文介绍了一种借助布尔影响力进行快速准确的NPN分类的新方法和算法，实验证明影响力在减小转换函数计算时间方面起到了重要作用。 |
| [^97] | [FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning.](http://arxiv.org/abs/2308.12305) | FedDAT是一种在多模态异构联邦学习中进行基础模型微调的方法，通过采用参数高效微调（PEFT）方法来减轻客户端计算负担和通信开销，并解决了数据异构性的问题。 |
| [^98] | [Fat Shattering, Joint Measurability, and PAC Learnability of POVM Hypothesis Classes.](http://arxiv.org/abs/2308.12304) | 我们研究了量子测量类的可学习性，并建立了必要和充分条件，同时给出了对应的样本复杂性上界。我们发现标准ERM未满足统一收敛性的问题，于是提出了一种新的学习规则——去噪ERM，该规则在POVM和概率观测的概念类别中具有普适性并满足统一收敛性的条件。 |
| [^99] | [Inverse Lithography Physics-informed Deep Neural Level Set for Mask Optimization.](http://arxiv.org/abs/2308.12299) | 本文提出了一种反向成像物理知识驱动的深度神经级集（ILDLS）方法用于掩膜优化，并在迭代过程中利用级集为深度学习框架的一部分。 |
| [^100] | [Integer Factorisation, Fermat & Machine Learning on a Classical Computer.](http://arxiv.org/abs/2308.12290) | 这篇论文介绍了一种基于深度学习的整数因式分解概率算法，通过将问题转化为二元分类问题，通过大量合成生成的训练数据，可以在经典计算机上应用，具有实用性和可扩展性。 |
| [^101] | [An Accelerated Block Proximal Framework with Adaptive Momentum for Nonconvex and Nonsmooth Optimization.](http://arxiv.org/abs/2308.12126) | 本论文提出一种使用自适应动量的加速分块近端框架(ABPL+)来解决非凸和非光滑优化问题，并通过增强比较过程解决外推步骤失败的问题。同时，扩展算法适用于更新块变量的任何情况，并证明了算法的可行性和有效性。通过展示序列的导数集为关键点的性质，更明显地证明了算法的优势。 |
| [^102] | [Constrained Stein Variational Trajectory Optimization.](http://arxiv.org/abs/2308.12110) | CSVTO是一种受限斯坦变分轨迹优化算法，它通过斯坦变分梯度下降方法生成多样的约束满足轨迹集合，提高了在具有任意约束的问题中的优化性能和鲁棒性。 |
| [^103] | [A multiobjective continuation method to compute the regularization path of deep neural networks.](http://arxiv.org/abs/2308.12044) | 本文提出了一种多目标延续方法，用于计算深度神经网络的正则化路径，以解决DNNs中稀疏性和数值效率之间的冲突。 |
| [^104] | [On Uniformly Optimal Algorithms for Best Arm Identification in Two-Armed Bandits with Fixed Budget.](http://arxiv.org/abs/2308.12000) | 本文研究了在有限预算的随机二臂赌博机中进行最佳臂选择的问题，并证明不存在比等概率采样算法更好的算法。我们引入了一致稳定算法的概念，并证明任何在所有情况下与等概率采样算法表现一样好的算法必须属于这个类别。这一结果解决了之前的两个未解之谜。 |
| [^105] | [Adversarial Training Using Feedback Loops.](http://arxiv.org/abs/2308.11881) | 本文提出了一种基于反馈控制的新型对抗训练方法，通过将反馈控制纳入神经网络架构中进行训练，以增强DNN对对抗性攻击的鲁棒性。 |
| [^106] | [HypBO: Expert-Guided Chemist-in-the-Loop Bayesian Search for New Materials.](http://arxiv.org/abs/2308.11787) | HypBO是一种利用专家人类知识引导贝叶斯搜索的方法，通过生成改进的样本种子来更快地找到有希望的化学空间区域。 |
| [^107] | [Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models.](http://arxiv.org/abs/2308.11217) | 本论文提出了一种多模态联邦学习框架，利用私有领域数据协同训练大型模型，以实现跨场景的智能服务。在大模型时代，该框架解决了异构数据、模型聚合、性能和成本权衡、数据隐私以及激励机制等方面的挑战。 |
| [^108] | [Wasserstein Geodesic Generator for Conditional Distributions.](http://arxiv.org/abs/2308.10145) | 通过Wasserstein几何生成器学习条件分布，生成给定特定标签的样本。使用最优输运理论提出的方法能学习观察域的条件分布和它们之间的最优输运映射。在人脸图像数据上的实验验证了该方法的有效性。 |
| [^109] | [MoCLIM: Towards Accurate Cancer Subtyping via Multi-Omics Contrastive Learning with Omics-Inference Modeling.](http://arxiv.org/abs/2308.09725) | 本论文介绍了一种名为MoCLIM的多组学对比学习框架，能够在癌症亚型划分中利用多组学数据的潜力，显著提高了数据的拟合度和亚型划分性能。 |
| [^110] | [Multi-fidelity Fourier Neural Operator for Fast Modeling of Large-Scale Geological Carbon Storage.](http://arxiv.org/abs/2308.09113) | 多保真度傅里叶神经算子用于解决大规模地质碳储存问题，通过利用经济性更高的多保真度训练数据集，能够以与高保真度模型相当的准确性进行预测。 |
| [^111] | [Open-set Face Recognition using Ensembles trained on Clustered Data.](http://arxiv.org/abs/2308.07445) | 本研究提出了一种使用在聚类数据上训练的集成模型进行开放集合人脸识别的方法，能够准确识别感兴趣的个体，同时有效处理陌生的面孔。实验结果表明即使在大规模图库中也能取得竞争性的性能。 |
| [^112] | [Natural Language is All a Graph Needs.](http://arxiv.org/abs/2308.07134) | 本论文提出了一种名为InstructGLM的结构化语言模型算法，该算法将大型语言模型与图表学习问题相结合，旨在探索是否可以用语言模型取代图神经网络作为图表的基础模型。 |
| [^113] | [Dealing with Small Annotated Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models.](http://arxiv.org/abs/2308.06534) | 本研究评估了在医学影像领域使用自监督预训练方法的可行性，比较了共同对比学习和掩码自编码器方法在CT扫描卷积模型中的性能。 |
| [^114] | [Pareto Invariant Representation Learning for Multimedia Recommendation.](http://arxiv.org/abs/2308.04706) | 本文介绍了一种名为Pareto Invariant Representation Learning（PaInvRL）的框架，应用于多媒体推荐。该框架通过学习不变表示和变体表示的同时来缓解通用表示引入的错误相关性问题。从IID-OOD多目标优化的角度，PaInvRL减少了错误相关性对用户偏好的影响。 |
| [^115] | [Min-Max Optimization under Delays.](http://arxiv.org/abs/2307.06886) | 在大规模机器学习中，研究了min-max优化在延迟下的性能。对于简单实例，即使是小的延迟也可能导致Extra-gradient算法发散，因此需要对延迟版本的min-max优化算法进行仔细分析。为此，我们证明了在适当的技术假设下，梯度下降-上升算法在延迟情况下的收敛性和性能。 |
| [^116] | [Conditional expectation via compact kernels.](http://arxiv.org/abs/2306.10592) | 本文提出了一种基于紧核的算子理论方法来解决条件期望估计问题，在再生核希尔伯特空间中实现，易于实现，且成功应用于实际问题中。 |
| [^117] | [Graph Ladling: Shockingly Simple Parallel GNN Training without Intermediate Communication.](http://arxiv.org/abs/2306.10466) | 这项工作提出了一种无需中间通信的简化并行GNN训练方法，通过利用模型汤的原理减轻了GNN扩展过程中的内存瓶颈和可训练性问题。 |
| [^118] | [A Survey on Blood Pressure Measurement Technologies: Addressing Potential Sources of Bias.](http://arxiv.org/abs/2306.08451) | 该综述聚焦于带式血压监测技术，强调了由于测量和设备误差、人口统计学数据和体型差异等因素导致的血压测量偏差和方差。研发使用人工智能来纠正误差的新一代带式血压设备是重点发展方向。 |
| [^119] | [Expectation-Complete Graph Representations with Homomorphisms.](http://arxiv.org/abs/2306.05838) | 提出了一种基于同态映射的新型随机图嵌入方法，在期望的多项式时间内计算，能在期望中区分所有非同构图，具有高效的替代能力。 |
| [^120] | [Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers.](http://arxiv.org/abs/2306.04504) | 本文评估了ChatGPT在生物医学任务上的表现，发现在生物数据集训练样本较小时，零样例ChatGPT甚至优于精调生成式变压器模型。由此表明ChatGPT具有在生物医学领域成为有价值工具的潜力。 |
| [^121] | [Minimum intrinsic dimension scaling for entropic optimal transport.](http://arxiv.org/abs/2306.03398) | 该研究提出了最小内在维度缩放现象，在不做出数据分布假设的情况下，可以应用于各种熵优化输运问题中，以达到更优的结果。 |
| [^122] | [Transforming to Yoked Neural Networks to Improve ANN Structure.](http://arxiv.org/abs/2306.02157) | 本文提出了一种叫做YNN的方法，将同一级别的ANN节点连接在一起形成神经模块，解决了普通ANN无法共享信息的缺陷，显著提高了信息传输和性能。 |
| [^123] | [Exact Bayesian Inference on Discrete Models via Probability Generating Functions: A Probabilistic Programming Approach.](http://arxiv.org/abs/2305.17058) | 该论文提出一种精确推理离散统计模型的贝叶斯方法，支持离散采样、连续采样、离散观测、仿射函数、（随机）分支和事件条件。通过概率生成函数实现后验概率、期望、方差和高阶矩的精确计算。该方法性能优于近似蒙特卡洛方法，并避免了近似误差。 |
| [^124] | [LANISTR: Multimodal Learning from Structured and Unstructured Data.](http://arxiv.org/abs/2305.16556) | LANISTR是一个新颖的基于注意力机制的框架，可从结构化和非结构化数据中进行学习，在挑战性数据集上表现优异。 |
| [^125] | [Symplectic model reduction of Hamiltonian systems using data-driven quadratic manifolds.](http://arxiv.org/abs/2305.15490) | 本文提出了两种新方法，使用数据驱动的二次流形对高维哈密顿系统进行正则模型简化，从而使得可以更好地表示问题中固有的低维性，并在超出其训练数据范围的设置中提供更高的准确性。 |
| [^126] | [PruMUX: Augmenting Data Multiplexing with Model Compression.](http://arxiv.org/abs/2305.14706) | PruMUX是一种结合了结构化剪枝和数据复用的方法，可在保持准确性的情况下提高BERT-base模型的吞吐量。Auto-PruMUX可以预测修剪和复用的高性能参数，从而提供了一个实用的自动化工具。 |
| [^127] | [A Survey on Dataset Distillation: Approaches, Applications and Future Directions.](http://arxiv.org/abs/2305.01975) | 数据集蒸馏在机器学习中越来越重要。该方法可以通过合成高信息密度的数据集来支持持续学习、神经架构搜索和隐私保护。这篇综述性调查论文提出了一种分类方法，对现有方法进行了特征化，并系统回顾了数据模态和相关应用，同时总结了挑战并讨论了未来方向。 |
| [^128] | [Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction: A Unified Library and Performance Benchmark.](http://arxiv.org/abs/2304.14343) | 本研究提出了一种称为原子文件的统一空间时间数据存储格式，开发了一个名为LibCity的开源库，重新构建了65个空间时间预测模型，并收集了55个空间时间数据集。同时还提出了城市时空预测模型的性能基准，为这一领域提供了一个可靠的评估工具。 |
| [^129] | [To Compress or Not to Compress -- Self-Supervised Learning and Information Theory: A Review.](http://arxiv.org/abs/2304.09355) | 本文从信息论的角度回顾了各种自监督学习方法，并提出了一个正式的“自监督信息理论学习问题”统一框架。此外，讨论了压缩性和压缩算法在自监督学习中的作用，并凸显了潜在的未来方向。 |
| [^130] | [BadVFL: Backdoor Attacks in Vertical Federated Learning.](http://arxiv.org/abs/2304.08847) | 本文聚焦于竖直联邦学习中的后门攻击的鲁棒性问题，提出了一种新的后门攻击框架BadVFL，可以有效地将后门注入VFL的训练过程中，成功率高并且误分类率很低。 |
| [^131] | [Tackling Face Verification Edge Cases: In-Depth Analysis and Human-Machine Fusion Approach.](http://arxiv.org/abs/2304.08134) | 本文研究了临近边缘案例的面部验证问题，发现结合人机决策可以进一步提高最先进的面部验证系统在各种基准数据集上的性能。 |
| [^132] | [HyperTab: Hypernetwork Approach for Deep Learning on Small Tabular Datasets.](http://arxiv.org/abs/2304.03543) | HyperTab是一种基于超网络结合了随机森林和神经网络优点的小型表格数据深度学习方法，使用每个特定低维视图处理数据，虚拟增加训练样本数量，避免过度拟合。 |
| [^133] | [Synthesize Extremely High-dimensional Longitudinal Electronic Health Records via Hierarchical Autoregressive Language Model.](http://arxiv.org/abs/2304.02169) | 此论文提出了一种名为HALO的方法，它是一个层级自回归模型，可以生成高保真、细粒度电子健康记录数据，而这些数据可以用于训练准确的ML模型，且无需涉及隐私问题。 |
| [^134] | [Conformal Prediction Regions for Time Series using Linear Complementarity Programming.](http://arxiv.org/abs/2304.01075) | 本文提出了一种基于优化的方法，通过将预测误差参数化为多个时间步长，以找到不保守的预测区间，实现在使用学习启用的时间序列预测器进行长期规划和验证。 |
| [^135] | [Demographic Parity Inspector: Fairness Audits via the Explanation Space.](http://arxiv.org/abs/2303.08040) | 这篇论文提出了一种基于解释空间的算法方法，测量分组人口统计学平等的违规情况，并允许我们检查组间歧视的原因，提高了审计公平性的敏感度。 |
| [^136] | [Uni-RXN: A Unified Framework Bridging the Gap between Chemical Reaction Pretraining and Conditional Molecule Generation.](http://arxiv.org/abs/2303.06965) | 本文提出了Uni-RXN框架，在化学反应Pretraining和分子生成任务中都取得了最先进的结果。通过具备化学知识，克服了当前分子生成模型仅依赖少量反应模板的限制，生成质量高、可合成的药物类分子结构。 |
| [^137] | [Feature Unlearning for Pre-trained GANs and VAEs.](http://arxiv.org/abs/2303.05699) | 本文提出了一种从预训练的GAN和VAE模型中消除特定特征的方法，并通过实验证明了方法的有效性。 |
| [^138] | [Quantized Radio Map Estimation Using Tensor and Deep Generative Models.](http://arxiv.org/abs/2303.01770) | 本文提出了一个量化无线电图估计的框架，利用张量和深度生成模型的方法，扩展了现有的无线电图估计方法，并提出了一种基于最大似然估计的框架来实现从高度量化的传感器测量值中恢复无线电地图。 |
| [^139] | [Efficient Sensor Placement from Regression with Sparse Gaussian Processes in Continuous and Discrete Spaces.](http://arxiv.org/abs/2303.00028) | 本文提出了一种用稀疏高斯过程解决连续空间下传感器放置问题的方法，避免离散化环境并降低了计算复杂度，可通过贪婪算法找到好的解决方案。 |
| [^140] | [Breaking the Communication-Privacy-Accuracy Tradeoff with $f$-Differential Privacy.](http://arxiv.org/abs/2302.09624) | 本文研究了具有隐私问题和有限通信能力的多个用户的协作数据分析问题，在局部差分隐私保证的视角下，我们通过推导离散值机制的紧密$f$-差分隐私保证，进一步研究了隐私放大的稀疏化方法。 |
| [^141] | [On the Generalization of PINNs outside the training domain and the Hyperparameters influencing it.](http://arxiv.org/abs/2302.07557) | 本研究对PINN在训练域之外的预测行为进行了实证分析，并评估了算法设置对其泛化能力的影响。结果提供了有见地且有时直观的观点。 |
| [^142] | [Universal Soldier: Using Universal Adversarial Perturbations for Detecting Backdoor Attacks.](http://arxiv.org/abs/2302.00747) | 本论文提出了一种利用通用对抗扰动（UAP）检测后门攻击的方法。通过观察UAPs生成的方式，我们发现后门模型只需要较少的扰动即可欺骗模型，而干净模型需要更多扰动。这一发现可以用来区分干净模型和后门模型。 |
| [^143] | [Anderson Acceleration For Bioinformatics-Based Machine Learning.](http://arxiv.org/abs/2302.00347) | 这项研究在经典机器学习分类器中探索了Anderson加速的有效性，并通过使用AA，达到了显著提高收敛速度和减小训练误差的效果。 |
| [^144] | [BallGAN: 3D-aware Image Synthesis with a Spherical Background.](http://arxiv.org/abs/2301.09091) | BallGAN是一个新颖的3D感知GAN框架，通过将背景近似为球形表面，并使用特定约束来减少背景自由度，可以产生更合理的3D几何。 |
| [^145] | [Unsupervised Manifold Linearizing and Clustering.](http://arxiv.org/abs/2301.01805) | 本文提出了一种无监督的方法，可以同时对接近低维流形并具有线性表示的数据进行聚类和学习。方法可以处理非线性流形，而不需要对样本的聚类成员资格做出假设或对采样密度要求过高。 |
| [^146] | [Algorithmic progress in computer vision.](http://arxiv.org/abs/2212.05153) | 计算机视觉领域的算法进展对进步起到了重要作用，特别是增强计算的算法创新使计算需求减半，速度比摩尔定律相关速度快两倍以上。 |
| [^147] | [FIESTA: Autoencoders for accurate fiber segmentation in tractography.](http://arxiv.org/abs/2212.00143) | FIESTA是一个可靠、稳健、完全自动化且易于半自动校准的流程，基于深度自动编码器可以解剖和完全填充白质束，通过生成采样的方法改善难以追踪的束的分割覆盖率。 |
| [^148] | [Farm-wide virtual load monitoring for offshore wind structures via Bayesian neural networks.](http://arxiv.org/abs/2211.00642) | 该论文介绍了一种基于贝叶斯神经网络的全场虚拟负载监测方案，以解决离岸风电结构监测的不确定性和实际限制问题。 |
| [^149] | [Exact Manifold Gaussian Variational Bayes.](http://arxiv.org/abs/2210.14598) | 我们提出了一种在复杂模型中进行变分推断的优化算法，通过使用自然梯度更新和黎曼流形，我们开发了一种高效的高斯变分推断算法，并验证了其在多个数据集上的性能。 |
| [^150] | [Convergence of the Backward Deep BSDE Method with Applications to Optimal Stopping Problems.](http://arxiv.org/abs/2210.04118) | 本文提出了倒向深度BSDE方法用于解决最优停止问题，并给出了相应的理论分析和误差估计方法。 |
| [^151] | [Improving Sample Quality of Diffusion Models Using Self-Attention Guidance.](http://arxiv.org/abs/2210.00939) | 该论文提出了一种利用自注意力指导的策略来提升扩散模型生成图像的稳定性和质量，具有较高的实用价值。 |
| [^152] | [Individual Privacy Accounting with Gaussian Differential Privacy.](http://arxiv.org/abs/2209.15596) | 本论文介绍了用于个体隐私核算的高斯差分隐私方法，通过对自适应组合随机机制进行仔细分析，为高斯机制提供了最优边界。 |
| [^153] | [Interneurons accelerate learning dynamics in recurrent neural networks for statistical adaptation.](http://arxiv.org/abs/2209.10634) | 这项工作探索了通过内部神经元介导递归通信与直接递归连接相比的计算优势，通过分析连续突触动态和数值模拟，表明具有内部神经元的网络比具有直接递归连接的网络更能抵抗初始化的干扰。 |
| [^154] | [Towards Top-Down Automated Development in Limited Scopes: A Neuro-Symbolic Framework from Expressibles to Executables.](http://arxiv.org/abs/2209.01566) | 本研究提出了一个神经符号框架，从可表达性到可执行性实现了自顶向下的自动化开发。通过建立代码分类法和使用语义金字塔来关联文本数据和代码数据，我们可以改进深度代码生成的效果。 |
| [^155] | [Augmenting Reinforcement Learning with Transformer-based Scene Representation Learning for Decision-making of Autonomous Driving.](http://arxiv.org/abs/2208.12263) | 本研究提出了Scene-Rep Transformer来提升强化学习决策能力，通过改进场景表示编码和顺序预测潜在蒸馏。采用多阶段Transformer编码器建模交互意识和意图意识，并使用顺序潜在Transformer进行自监督学习，加速训练和减少探索空间。 |
| [^156] | [Efficient Adaptive Activation Rounding for Post-Training Quantization.](http://arxiv.org/abs/2208.11945) | 本论文提出了一种高效自适应激活舍入的训练后量化方法，通过调整激活的舍入方案来降低输出误差，并解决了动态激活和运行时开销的挑战。 |
| [^157] | [Unifying Gradients to Improve Real-world Robustness for Deep Networks.](http://arxiv.org/abs/2208.06228) | 通过统一不同数据的梯度来防御基于评分的查询攻击（SQAs），这样SQAs只能探测到一个更弱的攻击方向，保护真实世界的深度神经网络。 |
| [^158] | [A temporally and spatially local spike-based backpropagation algorithm to enable training in hardware.](http://arxiv.org/abs/2207.09755) | 这个论文提出了一种新的基于脉冲神经网络的反向传播算法，可以在硬件中进行训练，解决了脉冲编码训练机制的挑战，并且不需要外部内存和计算访问。 |
| [^159] | [Self-Supervised Training with Autoencoders for Visual Anomaly Detection.](http://arxiv.org/abs/2206.11723) | 本文提出了一种自监督学习方法，通过修改重构误差的方式集中在数据流形上，从而解决深度卷积自编码器在视觉异常检测中容易出现的重构异常信号而导致检测效果不佳的问题。 |
| [^160] | [Test-Time Adaptation for Visual Document Understanding.](http://arxiv.org/abs/2206.07240) | 该论文提出了一种测试时间自适应方法，用于将自监督预训练得到的表示适应到测试时的分布转移。通过利用跨模态自监督学习和伪标签方法，该方法在文档理解任务中实现了显著的改进，并在实体识别、键值提取和文档视觉问答上分别提高了1.89%、3.43%和17.68%。 |
| [^161] | [Efficient-Adam: Communication-Efficient Distributed Adam.](http://arxiv.org/abs/2205.14473) | 这项工作提出了一种名为高效-Adam的通信效率更高的分布式优化方法，在非凸环境下通过双向量化和双向误差反馈策略来降低通信成本，并对其迭代复杂度和通信复杂度进行了分析。 |
| [^162] | [Leveraging Global Binary Masks for Structure Segmentation in Medical Images.](http://arxiv.org/abs/2205.09107) | 这篇论文提出了利用全局二进制掩码来进行医学图像中结构分割的方法，通过利用器官的解剖形状和位置信息的一致性来进行分割。研究了两种情况下的应用，并在脑部和心脏CT图像数据集上进行了验证。 |
| [^163] | [StableDR: Stabilized Doubly Robust Learning for Recommendation on Data Missing Not at Random.](http://arxiv.org/abs/2205.04701) | StableDR是一种稳定的双重稳健学习方法，用于解决推荐系统中数据缺失非随机的问题。通过减少对外推的依赖，StableDR能够同时具有有界的偏差、方差和泛化误差界，在不准确的估计误差和任意小的倾向性下表现出优越性能。 |
| [^164] | [Riemannian Hamiltonian methods for min-max optimization on manifolds.](http://arxiv.org/abs/2204.11418) | 本文研究了流形上的min-max优化问题，并引入了Riemannian Hamiltonian方法作为其代理方法。通过最小化Hamiltonian函数，可以得到所需的min-max鞍点。该方法在geodesic-bilinear优化问题中具有挑战性，但通过解决代理问题可以得到全局最优搜索方向。该方法在多个应用中展示了其有效性。 |
| [^165] | [FlexFringe: Modeling Software Behavior by Learning Probabilistic Automata.](http://arxiv.org/abs/2203.16331) | FlexFringe提供了高效的概率有限自动机学习方法，可用于建模软件行为。该方法在实践中通过实现改进的状态合并策略实现了显著性能提升，并且能够从软件日志中学习可解释的模型，用于异常检测。与基于神经网络的解决方案相比，学习更小更复杂的模型能够提高FlexFringe在异常检测中的性能。 |
| [^166] | [BagPipe: Accelerating Deep Recommendation Model Training.](http://arxiv.org/abs/2202.12429) | 本文提出了BagPipe，一种用于加速深度推荐模型训练的系统。该系统利用嵌入访问的特定结构，通过缓存和预取的方式优化训练，实现了对推荐模型的高效训练。 |
| [^167] | [The Polynomial Method is Universal for Distribution-Free Correlational SQ Learning.](http://arxiv.org/abs/2010.11925) | 多项式方法在无分布相关SQ学习中是通用的，并且是最佳方法。 |
| [^168] | [Near Optimal Adversarial Attack on UCB Bandits.](http://arxiv.org/abs/2008.09312) | 本文提出了一种在对抗攻击下的UCB最优拉臂策略，成本为$\sqrt{\log T}$，并且证明了此攻击策略近乎是最优的。 |
| [^169] | [The SWAX Benchmark: Attacking Biometric Systems with Wax Figures.](http://arxiv.org/abs/1910.09642) | SWAX基准测试引入了一个名为Sense Wax Attack (SWAX)的新数据库，其中包含真实的人类和蜡像图像和视频，用于验证面部欺骗检测。实验表明，尽管近年来取得了一些进展，但高质量的攻击仍然使高级欺骗方法易受攻击。 |

# 详细

[^1]: NeuralClothSim: 神经变形场与Kirchhoff-Love薄壳理论相遇

    NeuralClothSim: Neural Deformation Fields Meet the Kirchhoff-Love Thin Shell Theory. (arXiv:2308.12970v1 [cs.GR])

    [http://arxiv.org/abs/2308.12970](http://arxiv.org/abs/2308.12970)

    本文提出了一种新的布料模拟方法 NeuralClothSim，使用薄壳理论和神经变形场进行表面演化，克服了现有布料模拟方法的局限性和挑战，为物理合理的布料模拟提供了一种全新的视角。

    

    布料模拟是一个广泛研究的问题，在计算机图形学文献中有大量的解决方案。现有的布料模拟器产生符合不同类型边界条件的逼真布料变形。然而，它们的操作原理在几个方面仍然存在局限性：它们在具有固定空间分辨率的显式表面表示上进行操作，执行一系列离散化的更新（限制了它们的时间分辨率），并且需要相对较大的存储空间。此外，通过现有的求解器进行梯度反向传播通常并不直观，这在将其集成到现代神经架构中时造成了额外的挑战。针对上述限制，本文从根本上以一种根本不同的视角来考虑物理合理的布料模拟，并重新思考这个长期存在的问题：我们提出了NeuralClothSim，即一种使用薄壳的新布料模拟方法，其中表面演化通过神经变形场等进行。

    Cloth simulation is an extensively studied problem, with a plethora of solutions available in computer graphics literature. Existing cloth simulators produce realistic cloth deformations that obey different types of boundary conditions. Nevertheless, their operational principle remains limited in several ways: They operate on explicit surface representations with a fixed spatial resolution, perform a series of discretised updates (which bounds their temporal resolution), and require comparably large amounts of storage. Moreover, back-propagating gradients through the existing solvers is often not straightforward, which poses additional challenges when integrating them into modern neural architectures. In response to the limitations mentioned above, this paper takes a fundamentally different perspective on physically-plausible cloth simulation and re-thinks this long-standing problem: We propose NeuralClothSim, i.e., a new cloth simulation approach using thin shells, in which surface ev
    
[^2]: Scenimefy: 通过半监督的图像到图像翻译学习制作动漫场景

    Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation. (arXiv:2308.12968v1 [cs.CV])

    [http://arxiv.org/abs/2308.12968](http://arxiv.org/abs/2308.12968)

    Scenimefy是一个通过半监督的图像到图像翻译框架，可以自动从现实世界图像中渲染高质量的动漫场景，并且能够保持一致的语义、明显的风格化和精细的细节。

    

    从复杂的现实世界图像自动高质量地渲染动漫场景具有重要的实际价值。这一任务的挑战在于场景的复杂性、动漫风格的独特特点以及缺乏用于填补领域差距的高质量数据集。尽管之前的努力有所进展，但是仍然不能满意地保持一致的语义保存、明显的风格化和精细的细节。在这项研究中，我们提出了Scenimefy，一个新颖的半监督图像到图像翻译框架，解决了这些挑战。我们的方法通过具有结构一致性的伪配对数据进行学习，简化了纯无监督设置。伪数据是通过语义约束的StyleGAN唯一地导出的，充分利用了像CLIP这样的丰富模型先验知识。我们进一步应用分割引导的数据选择来获得高质量的伪监督。引入了基于补丁的对比风格损失来改善风格化和细节。

    Automatic high-quality rendering of anime scenes from complex real-world images is of significant practical value. The challenges of this task lie in the complexity of the scenes, the unique features of anime style, and the lack of high-quality datasets to bridge the domain gap. Despite promising attempts, previous efforts are still incompetent in achieving satisfactory results with consistent semantic preservation, evident stylization, and fine details. In this study, we propose Scenimefy, a novel semi-supervised image-to-image translation framework that addresses these challenges. Our approach guides the learning with structure-consistent pseudo paired data, simplifying the pure unsupervised setting. The pseudo data are derived uniquely from a semantic-constrained StyleGAN leveraging rich model priors like CLIP. We further apply segmentation-guided data selection to obtain high-quality pseudo supervision. A patch-wise contrastive style loss is introduced to improve stylization and fi
    
[^3]: NeO 360: 用于稀疏视角合成户外场景的神经场

    NeO 360: Neural Fields for Sparse View Synthesis of Outdoor Scenes. (arXiv:2308.12967v1 [cs.CV])

    [http://arxiv.org/abs/2308.12967](http://arxiv.org/abs/2308.12967)

    NeO 360是一种用于稀疏视角合成户外场景的通用方法，通过捕获复杂的真实世界场景分布，使用混合的图像条件式三平面表示，实现了从单个或少数几个图像重建360度场景，并具有更好的效果和表达能力。

    

    最近的隐式神经表示方法在新视角合成方面取得了很好的结果。然而，现有方法需要在许多视角下进行昂贵的场景优化，从而限制了它们在真实世界无限制的城市环境中的应用，其中感兴趣的对象或背景只从很少的视角观察到。为了解决这个挑战，我们引入了一种名为NeO 360的新方法，用于稀疏视角合成户外场景。NeO 360是一种通用的方法，可以从单个或少数几个位置的RGB图像重建360度场景。我们方法的核心在于捕获复杂的真实世界户外三维场景的分布，并使用一种混合的图像条件式三平面表示，可以从任何世界点查询。我们的表示方法结合了体素和鸟瞰图表示的优点，比每种表示方法更有效和表达能力更强。NeO 360的表示方法允许我们从一个庞大的无限制的三维数据集中进行学习。

    Recent implicit neural representations have shown great results for novel view synthesis. However, existing methods require expensive per-scene optimization from many views hence limiting their application to real-world unbounded urban settings where the objects of interest or backgrounds are observed from very few views. To mitigate this challenge, we introduce a new approach called NeO 360, Neural fields for sparse view synthesis of outdoor scenes. NeO 360 is a generalizable method that reconstructs 360{\deg} scenes from a single or a few posed RGB images. The essence of our approach is in capturing the distribution of complex real-world outdoor 3D scenes and using a hybrid image-conditional triplanar representation that can be queried from any world point. Our representation combines the best of both voxel-based and bird's-eye-view (BEV) representations and is more effective and expressive than each. NeO 360's representation allows us to learn from a large collection of unbounded 3D
    
[^4]: 带有注意力调节的稠密文本到图像生成

    Dense Text-to-Image Generation with Attention Modulation. (arXiv:2308.12964v1 [cs.CV])

    [http://arxiv.org/abs/2308.12964](http://arxiv.org/abs/2308.12964)

    提出了DenseDiffusion方法，该方法可以使预训练的文本到图像模型处理稠密标题并具有场景布局控制，无需额外微调或数据集，提升了图像生成性能。

    

    现有的文本到图像扩散模型在给定稠密标题的情况下很难合成逼真的图像，其中每个文本提示为特定图像区域提供了详细的描述。为了解决这个问题，我们提出了DenseDiffusion，一种无需训练的方法，它能够使预训练的文本到图像模型能够处理这些稠密标题，并同时提供对场景布局的控制。我们首先分析生成图像布局与预训练模型的中间注意力图之间的关系。接下来，我们开发了一种注意力调节方法，根据布局指导将对象引导到特定区域显示。在不需要额外的微调或数据集的情况下，我们在自动评估和人工评估分数方面改进了给定稠密标题的图像生成性能。此外，我们实现了与专门使用布局条件进行训练的模型相似质量的视觉结果。

    Existing text-to-image diffusion models struggle to synthesize realistic images given dense captions, where each text prompt provides a detailed description for a specific image region. To address this, we propose DenseDiffusion, a training-free method that adapts a pre-trained text-to-image model to handle such dense captions while offering control over the scene layout. We first analyze the relationship between generated images' layouts and the pre-trained model's intermediate attention maps. Next, we develop an attention modulation method that guides objects to appear in specific regions according to layout guidance. Without requiring additional fine-tuning or datasets, we improve image generation performance given dense captions regarding both automatic and human evaluation scores. In addition, we achieve similar-quality visual results with models specifically trained with layout conditions.
    
[^5]: DLIP: 提取语言-图像预训练的方法

    DLIP: Distilling Language-Image Pre-training. (arXiv:2308.12956v1 [cs.CV])

    [http://arxiv.org/abs/2308.12956](http://arxiv.org/abs/2308.12956)

    本文提出了DLIP，提取语言-图像预训练的方法，通过对不同模块的架构特性和不同模态的信息传递进行深入研究，探索了如何提取轻量级但性能优越的视觉-语言预训练模型。实验结果显示DLIP能达到最先进的准确性和效率平衡。

    

    视觉-语言预训练 (VLP) 在极重的参数辅助下取得了显著进展，但这也给其在实际应用中的部署带来了挑战。知识提取作为模型压缩中的重要过程被广泛认可。然而，现有的知识提取技术对于VLP的深入调查和分析还不够，并且VLP导向的提取实践指南尚未被探索。在本文中，我们提出了DLIP，一个简单而高效的提取语言-图像预训练框架，通过它我们研究如何提取一个轻量级的VLP模型。具体地，我们从多个维度剖析了模型提取，如不同模块的架构特性和不同模态的信息传递。我们进行了全面的实验，并对如何提取轻量级但性能优越的VLP模型提供了深刻的见解。实验结果表明，DLIP在准确性和效率的权衡上达到了最先进水平。

    Vision-Language Pre-training (VLP) shows remarkable progress with the assistance of extremely heavy parameters, which challenges deployment in real applications. Knowledge distillation is well recognized as the essential procedure in model compression. However, existing knowledge distillation techniques lack an in-depth investigation and analysis of VLP, and practical guidelines for VLP-oriented distillation are still not yet explored. In this paper, we present DLIP, a simple yet efficient Distilling Language-Image Pre-training framework, through which we investigate how to distill a light VLP model. Specifically, we dissect the model distillation from multiple dimensions, such as the architecture characteristics of different modules and the information transfer of different modalities. We conduct comprehensive experiments and provide insights on distilling a light but performant VLP model. Experimental results reveal that DLIP can achieve a state-of-the-art accuracy/efficiency trade-o
    
[^6]: BridgeData V2:一个用于规模化机器人学习的数据集

    BridgeData V2: A Dataset for Robot Learning at Scale. (arXiv:2308.12952v1 [cs.RO])

    [http://arxiv.org/abs/2308.12952](http://arxiv.org/abs/2308.12952)

    BridgeData V2是一个大规模且多样化的机器人操作行为数据集，广泛应用于机器人学习研究。该数据集具备任务和环境变异性，并且兼容多种学习方法，通过实验表明，使用此数据集可以提高模型性能。

    

    我们介绍了BridgeData V2，这是一个大规模且多样化的机器人操作行为数据集，旨在促进规模化机器人学习的研究。BridgeData V2包含了在一个公开可用且成本较低的机器人上收集的60,096个轨迹，覆盖了24个环境。BridgeData V2提供了广泛的任务和环境变异性，使得可以在不同的环境、领域和机构之间进行泛化的技能，使得该数据集成为广大研究人员的有用资源。此外，该数据集与多种开放词汇、多任务学习方法以目标图像或自然语言指令为条件是兼容的。在我们的实验中，我们在我们的数据集上训练了6种最先进的模仿学习和离线强化学习方法，并发现它们在一系列需要不同泛化程度的任务上取得了成功。我们还展示了这些方法的性能随着更多的数据和更高容量的模型而改善，并且通过训练集大小的增加和模型容量的增加获得了更好的表现。

    We introduce BridgeData V2, a large and diverse dataset of robotic manipulation behaviors designed to facilitate research on scalable robot learning. BridgeData V2 contains 60,096 trajectories collected across 24 environments on a publicly available low-cost robot. BridgeData V2 provides extensive task and environment variability, leading to skills that can generalize across environments, domains, and institutions, making the dataset a useful resource for a broad range of researchers. Additionally, the dataset is compatible with a wide variety of open-vocabulary, multi-task learning methods conditioned on goal images or natural language instructions. In our experiments, we train 6 state-of-the-art imitation learning and offline reinforcement learning methods on our dataset, and find that they succeed on a suite of tasks requiring varying amounts of generalization. We also demonstrate that the performance of these methods improves with more data and higher capacity models, and that trai
    
[^7]: 多任务学习中的标签预算分配问题

    Label Budget Allocation in Multi-Task Learning. (arXiv:2308.12949v1 [cs.LG])

    [http://arxiv.org/abs/2308.12949](http://arxiv.org/abs/2308.12949)

    该论文首次提出并定义了多任务学习中的标签预算分配问题，提出了一种任务自适应预算分配算法，通过估计和最大化新信息的程度来提高多任务学习的性能。

    

    数据标记成本常常限制了机器学习系统的性能。在多任务学习中，相关任务之间相互提供信息并提高整体性能，但是标签成本可能在不同任务之间变化。在不同任务中如何分配标签预算（即在标记上投入的金额）以实现最佳的多任务性能? 我们首次提出并正式定义了多任务学习中的标签预算分配问题，并实证表明不同的预算分配策略对其性能有很大影响。我们提出一种任务自适应预算分配算法，以鲁棒地生成适应不同多任务学习设置的最佳预算分配。具体来说，我们通过估计和最大化分配预算所获得的新信息的程度作为多任务学习性能的代理。在PASCAL VOC和Taskonomy上的实验表明了我们方法的有效性。

    The cost of labeling data often limits the performance of machine learning systems. In multi-task learning, related tasks provide information to each other and improve overall performance, but the label cost can vary among tasks. How should the label budget (i.e. the amount of money spent on labeling) be allocated among different tasks to achieve optimal multi-task performance? We are the first to propose and formally define the label budget allocation problem in multi-task learning and to empirically show that different budget allocation strategies make a big difference to its performance. We propose a Task-Adaptive Budget Allocation algorithm to robustly generate the optimal budget allocation adaptive to different multi-task learning settings. Specifically, we estimate and then maximize the extent of new information obtained from the allocated budget as a proxy for multi-task learning performance. Experiments on PASCAL VOC and Taskonomy demonstrate the efficacy of our approach over o
    
[^8]: 在边界学习：一种面向物理的神经算子用于解决复杂几何参数偏微分方程

    Learning Only On Boundaries: a Physics-Informed Neural operator for Solving Parametric Partial Differential Equations in Complex Geometries. (arXiv:2308.12939v1 [cs.LG])

    [http://arxiv.org/abs/2308.12939](http://arxiv.org/abs/2308.12939)

    这项研究提出了一种面向物理的神经算子方法，用于在复杂几何参数下解决无标记数据的边界值问题。该方法通过将PDE重新表述为BIEs，并仅在域的边界上训练算子网络，显著加速了训练过程，并且可以处理无界问题。

    

    最近，深度学习替代方案和神经算子在解决偏微分方程(PDEs)方面显示出潜力。然而，它们通常需要大量的训练数据，并且只限于有界域。在这项工作中，我们提出了一种新颖的面向物理的神经算子方法，用于解决无标记数据的参数化边界值问题。通过将PDE的重新表述为边界积分方程(BIEs)，我们可以仅在域的边界上训练算子网络。这种方法将所需样本点的数量从$O(N^d)$减少到$O(N^{d-1})$，其中$d$为域的维数，从而显著加速了训练过程。此外，我们的方法可以处理无界问题，对于现有的面向物理的神经网络(PINNs)和神经算子来说是无法实现的。我们的数值实验展示了参数化复杂几何体和无界问题的有效性。

    Recently deep learning surrogates and neural operators have shown promise in solving partial differential equations (PDEs). However, they often require a large amount of training data and are limited to bounded domains. In this work, we present a novel physics-informed neural operator method to solve parametrized boundary value problems without labeled data. By reformulating the PDEs into boundary integral equations (BIEs), we can train the operator network solely on the boundary of the domain. This approach reduces the number of required sample points from $O(N^d)$ to $O(N^{d-1})$, where $d$ is the domain's dimension, leading to a significant acceleration of the training process. Additionally, our method can handle unbounded problems, which are unattainable for existing physics-informed neural networks (PINNs) and neural operators. Our numerical experiments show the effectiveness of parametrized complex geometries and unbounded problems.
    
[^9]: 低计数时间序列异常检测

    Low-count Time Series Anomaly Detection. (arXiv:2308.12925v1 [cs.LG])

    [http://arxiv.org/abs/2308.12925](http://arxiv.org/abs/2308.12925)

    本论文提出了一种解决低计数时间序列异常检测的方法。通过引入新的生成过程来创建包含异常片段的基准数据集，并通过理论和实证分析解释了常用算法在正常和异常片段之间的分布重叠问题。

    

    低计数时间序列描述稀疏或间断事件，这在捕获和监控不同数据类型的大规模在线平台中很常见。建模低计数时间序列面临几个挑战，特别是低信噪比（当异常签名无法检测时）和非均匀性能（平均度量指标不能代表局部行为）。当前的时间序列异常检测领域缺乏明确的工具和流程来建模和可靠地检测这些情况下的异常。为了解决这个问题，我们引入了一种新的生成过程，用于创建包含有异常片段的低计数时间序列的基准数据集。通过理论和实证分析的混合，我们的工作解释了常用算法在正常和异常片段之间的分布重叠中遇到的困难。为了减轻这个缺点，我们利用我们的发现来展示如何进行异常检测。

    Low-count time series describe sparse or intermittent events, which are prevalent in large-scale online platforms that capture and monitor diverse data types. Several distinct challenges surface when modelling low-count time series, particularly low signal-to-noise ratios (when anomaly signatures are provably undetectable), and non-uniform performance (when average metrics are not representative of local behaviour). The time series anomaly detection community currently lacks explicit tooling and processes to model and reliably detect anomalies in these settings. We address this gap by introducing a novel generative procedure for creating benchmark datasets comprising of low-count time series with anomalous segments. Via a mixture of theoretical and empirical analysis, our work explains how widely-used algorithms struggle with the distribution overlap between normal and anomalous segments. In order to mitigate this shortcoming, we then leverage our findings to demonstrate how anomaly sc
    
[^10]: 使用大型语言模型诊断不可行的优化问题

    Diagnosing Infeasible Optimization Problems Using Large Language Models. (arXiv:2308.12923v1 [cs.HC])

    [http://arxiv.org/abs/2308.12923](http://arxiv.org/abs/2308.12923)

    本文提出了OptiChat，一种基于自然语言的系统，用于诊断不可行的优化模型。它可以帮助从业人员理解和解释不可行的优化模型，无需具备深厚的优化背景知识。

    

    决策问题可以被表示为数学优化模型，在经济学、工程学与制造业、交通运输和医疗保健等领域有广泛应用。优化模型是在满足一组要求或约束条件的情况下做出最佳决策的数学抽象。部署这些模型的主要障碍之一是帮助从业人员理解和解释这些模型，特别是当它们是不可行的，也就是说没有决策满足所有的约束条件。现有的诊断不可行优化模型的方法往往依赖于专家系统，需要在优化方面具有重要的背景知识。在本文中，我们介绍了OptiChat，这是一种首创的基于自然语言的系统，配备了一个与聊天机器人进行交互式对话的GUI，用于讨论不可行的优化模型。OptiChat可以提供对优化模型的自然语言描述。

    Decision-making problems can be represented as mathematical optimization models, finding wide applications in fields such as economics, engineering and manufacturing, transportation, and health care. Optimization models are mathematical abstractions of the problem of making the best decision while satisfying a set of requirements or constraints. One of the primary barriers to deploying these models in practice is the challenge of helping practitioners understand and interpret such models, particularly when they are infeasible, meaning no decision satisfies all the constraints. Existing methods for diagnosing infeasible optimization models often rely on expert systems, necessitating significant background knowledge in optimization. In this paper, we introduce OptiChat, a first-of-its-kind natural language-based system equipped with a chatbot GUI for engaging in interactive conversations about infeasible optimization models. OptiChat can provide natural language descriptions of the optim
    
[^11]: 一种高效的分布式多智能体强化学习用于电动汽车充电网络控制

    An Efficient Distributed Multi-Agent Reinforcement Learning for EV Charging Network Control. (arXiv:2308.12921v1 [cs.MA])

    [http://arxiv.org/abs/2308.12921](http://arxiv.org/abs/2308.12921)

    这个论文提出了一种分散式多智能体强化学习的EV充电控制框架，通过保护隐私和减少网络成本来改善充电网络的性能。

    

    电动汽车（EV）的普及趋势将显著影响住宅电力需求，导致配电网变压器超负荷风险增加。为了减轻这种风险，迫切需要开发有效的EV充电控制器。目前，大多数EV充电控制器基于集中式方法管理单个EV或一组EV。本文引入了一种分散式多智能体强化学习（MARL）充电框架，优先保护EV车主的隐私。我们采用了集中式训练分散式执行-深度确定性策略梯度（CTDE-DDPG）模式，在训练过程中向用户提供有价值的信息，同时在执行过程中保护隐私。我们的结果表明，CTDE框架通过减少网络成本提高了充电网络的性能。此外，我们还展示了峰值-平均比（PAR）的

    The increasing trend in adopting electric vehicles (EVs) will significantly impact the residential electricity demand, which results in an increased risk of transformer overload in the distribution grid. To mitigate such risks, there are urgent needs to develop effective EV charging controllers. Currently, the majority of the EV charge controllers are based on a centralized approach for managing individual EVs or a group of EVs. In this paper, we introduce a decentralized Multi-agent Reinforcement Learning (MARL) charging framework that prioritizes the preservation of privacy for EV owners. We employ the Centralized Training Decentralized Execution-Deep Deterministic Policy Gradient (CTDE-DDPG) scheme, which provides valuable information to users during training while maintaining privacy during execution. Our results demonstrate that the CTDE framework improves the performance of the charging network by reducing the network costs. Moreover, we show that the Peak-to-Average Ratio (PAR) 
    
[^12]: 用CLIP实现真实的无监督微调

    Towards Realistic Unsupervised Fine-tuning with CLIP. (arXiv:2308.12919v1 [cs.CV])

    [http://arxiv.org/abs/2308.12919](http://arxiv.org/abs/2308.12919)

    本论文针对无监督微调中可能出现的未知类别和超出分布范围的问题，提出了一种称为UEO的简单、高效、有效的微调方法，该方法能够同时提高对超出分布样本的检测能力和预定义类别实例的识别能力。

    

    视觉-语言模型（VLM）如CLIP的出现推动了人们在下游监督学习任务中的应用研究。尽管一些之前的研究探索了CLIP的无监督微调，但它们常常依赖于与真实标签相关的类名等先验知识。本文中，我们探讨了一种真实的无监督微调情景，假设未标记的数据可能包含来自未知类别的超出分布范围的样本。此外，我们强调了在预定义类标签的识别之外，同时提高对超出分布检测能力的重要性。为了解决这个问题，我们提出了一种简单、高效、有效的微调方法，称为Universal Entropy Optimization (UEO)。UEO利用样本级置信度，以近似方式最小化置信实例的条件熵并最大化边缘熵。

    The emergence of vision-language models (VLMs), such as CLIP, has spurred a significant research effort towards their application for downstream supervised learning tasks. Although some previous studies have explored the unsupervised fine-tuning of CLIP, they often rely on prior knowledge in the form of class names associated with ground truth labels. In this paper, we delve into a realistic unsupervised fine-tuning scenario by assuming that the unlabeled data might contain out-of-distribution samples from unknown classes. Furthermore, we emphasize the importance of simultaneously enhancing out-of-distribution detection capabilities alongside the recognition of instances associated with predefined class labels.  To tackle this problem, we present a simple, efficient, and effective fine-tuning approach called Universal Entropy Optimization (UEO). UEO leverages sample-level confidence to approximately minimize the conditional entropy of confident instances and maximize the marginal entro
    
[^13]: 评估机器学习系统在对抗攻击方面的漏洞

    Evaluating the Vulnerabilities in ML systems in terms of adversarial attacks. (arXiv:2308.12918v1 [cs.LG])

    [http://arxiv.org/abs/2308.12918](http://arxiv.org/abs/2308.12918)

    本研究评估了机器学习系统在面对对抗攻击时的漏洞，并讨论了漏洞可能的原因、对抗攻击与随机化示例的差异以及相关的道德问题。

    

    最近出现了一些难以发现的对抗攻击。这些新的对抗攻击方法可能对当前的深度学习网络防御系统构成挑战，并可能影响未来网络攻击的防御。本研究论文作者专注于这个领域。他们探讨了AI系统的漏洞带来的后果。这包括讨论漏洞可能发生的原因，随机化和对抗性示例之间的差异，以及漏洞可能引发的道德问题。此外，当AI系统处于测试阶段并准备进行更广泛的使用时，适当进行针对性的训练是至关重要的。

    There have been recent adversarial attacks that are difficult to find. These new adversarial attacks methods may pose challenges to current deep learning cyber defense systems and could influence the future defense of cyberattacks. The authors focus on this domain in this research paper. They explore the consequences of vulnerabilities in AI systems. This includes discussing how they might arise, differences between randomized and adversarial examples and also potential ethical implications of vulnerabilities. Moreover, it is important to train the AI systems appropriately when they are in testing phase and getting them ready for broader use.
    
[^14]: POLCA：LLM云服务提供商中的功率超额使用

    POLCA: Power Oversubscription in LLM Cloud Providers. (arXiv:2308.12908v1 [cs.DC])

    [http://arxiv.org/abs/2308.12908](http://arxiv.org/abs/2308.12908)

    本文研究了在大型语言模型（LLM）云服务提供商中的功率超额使用问题。通过对多种LLM及其不同配置的功耗模式进行分析，我们发现在LLM集群中存在显著的功率超额使用机会，这可以提高数据中心的功率效率，并且允许更多的服务器部署，同时减少部署时间。

    

    近期大型语言模型（LLM）的创新及其各种用例迅速推高了数据中心GPU的计算能力需求。几家云服务提供商和其他企业已经制定了大规模扩张计划，以支持这些新工作负载。数据中心的关键瓶颈资源之一是电力，而随着LLM模型规模的增大，它们的功耗也越来越高。本文展示了在LLM集群中存在显著的功率超额使用机会。功率超额使用提高了这些数据中心的功率效率，允许每个数据中心部署更多的服务器，并减少了部署时间，因为建设新的数据中心很慢。我们广泛地表征了各种LLM及其配置的功耗模式。我们确定了推理和训练功耗模式之间的差异。根据我们对这些LLM的分析，我们声称平均开销程度的功耗超额使用可以提高数据中心的能效。

    Recent innovation in large language models (LLMs), and their myriad use-cases have rapidly driven up the compute capacity demand for datacenter GPUs. Several cloud providers and other enterprises have made substantial plans of growth in their datacenters to support these new workloads. One of the key bottleneck resources in datacenters is power, and given the increasing model sizes of LLMs, they are becoming increasingly power intensive. In this paper, we show that there is a significant opportunity to oversubscribe power in LLM clusters. Power oversubscription improves the power efficiency of these datacenters, allowing more deployable servers per datacenter, and reduces the deployment time, since building new datacenters is slow.  We extensively characterize the power consumption patterns of a variety of LLMs and their configurations. We identify the differences between the inference and training power consumption patterns. Based on our analysis of these LLMs, we claim that the avera
    
[^15]: CDAN: 用于低光图像增强的卷积稠密注意力引导网络

    CDAN: Convolutional Dense Attention-guided Network for Low-light Image Enhancement. (arXiv:2308.12902v1 [cs.CV])

    [http://arxiv.org/abs/2308.12902](http://arxiv.org/abs/2308.12902)

    本研究提出了一种名为CDAN的卷积稠密注意力引导网络，用于低光图像增强。该网络结合了自编码器架构、卷积和稠密块、注意力机制和跳跃连接，通过专门的后处理阶段进一步改善色彩平衡和对比度。与现有方法相比，在低光图像增强方面取得了显著的进展，展示了在各种具有挑战性的场景中的稳健性。

    

    低光图像以不足的照明为特征，面临清晰度减弱、颜色暗淡和细节减少的挑战。低光图像增强是计算机视觉中的一个重要任务，旨在通过改善亮度、对比度和整体感知质量来纠正这些问题，从而促进准确的分析和解释。本文介绍了一种新颖的解决方案：卷积稠密注意力引导网络（CDAN），用于增强低光图像。CDAN将自编码器架构与卷积和稠密块相结合，配合注意力机制和跳跃连接。该架构确保了有效的信息传递和特征学习。此外，专门的后处理阶段可以进一步改善色彩平衡和对比度。与低光图像增强领域的最新成果相比，我们的方法取得了显著的进展，并展示了在各种具有挑战性的场景中的稳健性。

    Low-light images, characterized by inadequate illumination, pose challenges of diminished clarity, muted colors, and reduced details. Low-light image enhancement, an essential task in computer vision, aims to rectify these issues by improving brightness, contrast, and overall perceptual quality, thereby facilitating accurate analysis and interpretation. This paper introduces the Convolutional Dense Attention-guided Network (CDAN), a novel solution for enhancing low-light images. CDAN integrates an autoencoder-based architecture with convolutional and dense blocks, complemented by an attention mechanism and skip connections. This architecture ensures efficient information propagation and feature learning. Furthermore, a dedicated post-processing phase refines color balance and contrast. Our approach demonstrates notable progress compared to state-of-the-art results in low-light image enhancement, showcasing its robustness across a wide range of challenging scenarios. Our model performs 
    
[^16]: 统一的数据管理和综合性能评估用于城市时空预测[实验，分析和基准]的论文

    Unified Data Management and Comprehensive Performance Evaluation for Urban Spatial-Temporal Prediction [Experiment, Analysis & Benchmark]. (arXiv:2308.12899v1 [cs.LG])

    [http://arxiv.org/abs/2308.12899](http://arxiv.org/abs/2308.12899)

    该论文提出了针对城市时空预测的统一数据管理和综合性能评估方法。其贡献包括引入“原子文件”作为统一存储格式，提供了城市时空预测模型技术进展的全面概述，并建立了性能排行榜和鉴定了有潜力的模型和数据集。

    

    随着深度学习技术的发展和大规模数据集的可用性，城市时空预测领域正在迅速发展。然而，从不同来源和以不同格式存储的多样化城市时空数据的访问和利用仍然存在挑战，而在深度学习模型大量增加的情况下，确定有效的模型结构和组件也是一个挑战。本文解决了这些挑战，并提供了三个重要的贡献。首先，我们引入了“原子文件”，这是一种为城市时空大数据设计的统一存储格式，并在40个不同的数据集上验证了其有效性，简化了数据管理。其次，我们全面概述了城市时空预测模型的技术进展，指导了强大模型的发展。第三，我们使用不同的模型和数据集进行了大量实验，建立了性能排行榜并确定了有潜力的模型和数据集。

    The field of urban spatial-temporal prediction is advancing rapidly with the development of deep learning techniques and the availability of large-scale datasets. However, challenges persist in accessing and utilizing diverse urban spatial-temporal datasets from different sources and stored in different formats, as well as determining effective model structures and components with the proliferation of deep learning models. This work addresses these challenges and provides three significant contributions. Firstly, we introduce "atomic files", a unified storage format designed for urban spatial-temporal big data, and validate its effectiveness on 40 diverse datasets, simplifying data management. Secondly, we present a comprehensive overview of technological advances in urban spatial-temporal prediction models, guiding the development of robust models. Thirdly, we conduct extensive experiments using diverse models and datasets, establishing a performance leaderboard and identifying promis
    
[^17]: 超越文档页分类：设计、数据集和挑战

    Beyond Document Page Classification: Design, Datasets, and Challenges. (arXiv:2308.12896v1 [cs.CV])

    [http://arxiv.org/abs/2308.12896](http://arxiv.org/abs/2308.12896)

    本文强调了将文档分类基准测试更接近于现实世界应用的需求，通过提出多页文档分类数据集和不同分类任务，以及高效的多页文档表示，来解决现有基准测试不适用于实际完整文档评估的问题。

    

    本文强调了将文档分类基准测试更接近于现实世界应用的需求，即在测试数据的性质上（$X$：多通道、多页、多行业；$Y$：类别分布和标签集的多样性）和考虑的分类任务上（$f$：多页文档、页面流和文档捆绑分类，...）。我们确定了公共的多页文档分类数据集的缺乏，并规范了应用场景中产生的不同分类任务，并激发了以高效的多页文档表示为目标的价值。对提出的多页文档分类数据集进行的实验研究表明，当前的基准测试已经变得无关紧要，并需要更新以评估实际中自然发生的完整文档。这个现实情况检查也呼吁更成熟的评估方法，涵盖校准评估、推理复杂性（时间-内存）和一系列现实分散情况。

    This paper highlights the need to bring document classification benchmarking closer to real-world applications, both in the nature of data tested ($X$: multi-channel, multi-paged, multi-industry; $Y$: class distributions and label set variety) and in classification tasks considered ($f$: multi-page document, page stream, and document bundle classification, ...). We identify the lack of public multi-page document classification datasets, formalize different classification tasks arising in application scenarios, and motivate the value of targeting efficient multi-page document representations. An experimental study on proposed multi-page document classification datasets demonstrates that current benchmarks have become irrelevant and need to be updated to evaluate complete documents, as they naturally occur in practice. This reality check also calls for more mature evaluation methodologies, covering calibration evaluation, inference complexity (time-memory), and a range of realistic distr
    
[^18]: 收集，测量，重复：负责任的AI数据收集的可靠性因素

    Collect, Measure, Repeat: Reliability Factors for Responsible AI Data Collection. (arXiv:2308.12885v1 [cs.LG])

    [http://arxiv.org/abs/2308.12885](http://arxiv.org/abs/2308.12885)

    对于负责任的AI数据收集，需要对数据的质量进行彻底的审查，避免不公平、偏见或不准确的结果。

    

    机器学习方法迅速进入我们的日常活动和高风险领域，要求对其公平性和可靠性进行透明和审查。为了评估机器学习模型的健壮性，研究通常会集中在其部署所使用的大规模数据集上，例如创建和维护文件以了解其来源、开发过程和伦理考虑。然而，AI的数据收集通常仍然是一次性的实践，而且经常为特定目的或应用程序收集的数据集会被重复用于其他问题。此外，数据集的注释可能随时间不具有代表性，包含模糊或错误的注释，或者无法跨问题或领域进行泛化。最近的研究表明，这些做法可能导致不公平、偏见或不准确的结果。我们认为，AI的数据收集应该以负责任的方式进行，对数据的质量进行彻底的审查。

    The rapid entry of machine learning approaches in our daily activities and high-stakes domains demands transparency and scrutiny of their fairness and reliability. To help gauge machine learning models' robustness, research typically focuses on the massive datasets used for their deployment, e.g., creating and maintaining documentation for understanding their origin, process of development, and ethical considerations. However, data collection for AI is still typically a one-off practice, and oftentimes datasets collected for a certain purpose or application are reused for a different problem. Additionally, dataset annotations may not be representative over time, contain ambiguous or erroneous annotations, or be unable to generalize across issues or domains. Recent research has shown these practices might lead to unfair, biased, or inaccurate outcomes. We argue that data collection for AI should be performed in a responsible manner where the quality of the data is thoroughly scrutinized
    
[^19]: LCANets++: 使用多层神经网络和层间竞争的鲁棒性音频分类

    LCANets++: Robust Audio Classification using Multi-layer Neural Networks with Lateral Competition. (arXiv:2308.12882v1 [cs.SD])

    [http://arxiv.org/abs/2308.12882](http://arxiv.org/abs/2308.12882)

    LCANets++是一种使用多层神经网络和层间竞争的鲁棒性音频分类方法，通过稀疏编码来提高对扰动和对抗攻击的抵抗力。

    

    音频分类旨在识别音频信号，包括语音命令或声音事件。然而，当前的音频分类器容易受到扰动和对抗攻击的影响。此外，现实世界中的音频分类任务通常受到有限的标记数据的限制。为了填补这些差距，先前的工作在计算机视觉中通过局部竞争算法（LCA）在第一层使用了神经启发式卷积神经网络（CNNs）进行稀疏编码，即LCANets。LCANets通过监督和无监督学习的组合来学习，减少对标记样本的依赖性。受到听觉皮层也是稀疏的事实的启发，我们将LCANets扩展到音频识别任务，并引入了LCANets++，它们是CNNs，通过LCA在多层次上进行稀疏编码。我们证明LCANets++对于扰动（如背景噪声）以及黑盒和白盒攻击（如逃避和破坏）比标准CNN和LCANets更加鲁棒。

    Audio classification aims at recognizing audio signals, including speech commands or sound events. However, current audio classifiers are susceptible to perturbations and adversarial attacks. In addition, real-world audio classification tasks often suffer from limited labeled data. To help bridge these gaps, previous work developed neuro-inspired convolutional neural networks (CNNs) with sparse coding via the Locally Competitive Algorithm (LCA) in the first layer (i.e., LCANets) for computer vision. LCANets learn in a combination of supervised and unsupervised learning, reducing dependency on labeled samples. Motivated by the fact that auditory cortex is also sparse, we extend LCANets to audio recognition tasks and introduce LCANets++, which are CNNs that perform sparse coding in multiple layers via LCA. We demonstrate that LCANets++ are more robust than standard CNNs and LCANets against perturbations, e.g., background noise, as well as black-box and white-box attacks, e.g., evasion an
    
[^20]: 简易注意力：一种用于Transformer的简单自注意机制

    Easy attention: A simple self-attention mechanism for Transformers. (arXiv:2308.12874v1 [cs.LG])

    [http://arxiv.org/abs/2308.12874](http://arxiv.org/abs/2308.12874)

    本论文提出了一种名为简易注意力的注意力机制，用于提高Transformer神经网络在混沌系统时间动态预测中的鲁棒性。该方法不依赖于键、查询和softmax，直接将注意力得分作为可学习参数。实验结果表明，该方法在重构和预测混沌系统的时间动态方面比传统的自注意机制和长短期记忆方法更具鲁棒性和简化性。

    

    为了提高用于混沌系统时间动态预测的Transformer神经网络的鲁棒性，我们提出了一种新颖的注意力机制，称为简易注意力。由于自注意机制仅使用查询和键的内积，因此证明了为了获取捕捉时间序列的长期依赖关系所需的注意力得分，并不需要键、查询和softmax。通过在softmax注意力得分上实施奇异值分解（SVD），我们进一步观察到自注意力在注意力得分的张成空间中压缩了来自查询和键的贡献。因此，我们提出的简易注意力方法直接将注意力得分作为可学习参数。这种方法在重构和预测展现更强鲁棒性和更少复杂性的混沌系统的时间动态时取得了出色的结果，比自注意机制或广泛使用的长短期记忆

    To improve the robustness of transformer neural networks used for temporal-dynamics prediction of chaotic systems, we propose a novel attention mechanism called easy attention. Due to the fact that self attention only makes usage of the inner product of queries and keys, it is demonstrated that the keys, queries and softmax are not necessary for obtaining the attention score required to capture long-term dependencies in temporal sequences. Through implementing singular-value decomposition (SVD) on the softmax attention score, we further observe that the self attention compresses contribution from both queries and keys in the spanned space of the attention score. Therefore, our proposed easy-attention method directly treats the attention scores as learnable parameters. This approach produces excellent results when reconstructing and predicting the temporal dynamics of chaotic systems exhibiting more robustness and less complexity than the self attention or the widely-used long short-ter
    
[^21]: IPA：推理管道自适应以实现高准确性和成本效益

    IPA: Inference Pipeline Adaptation to Achieve High Accuracy and Cost-Efficiency. (arXiv:2308.12871v1 [cs.DC])

    [http://arxiv.org/abs/2308.12871](http://arxiv.org/abs/2308.12871)

    提出了一种名为IPA的在线深度学习推理管道自适应系统，通过动态配置批处理大小、复制和模型变体，以优化准确性、最小化成本并满足用户定义的延迟要求。

    

    在机器学习生产系统中，高效地优化多模型推理管道以实现快速、准确和成本效益的推理是一个关键挑战，考虑到它们对端到端延迟的严格要求。为了简化准确性和成本之间广阔而复杂的权衡空间的探索，提供者通常选择考虑其中之一。然而，挑战在于协调准确性和成本的权衡。为了应对这一挑战并提出一种有效管理推理管道中模型变体的解决方案，我们提出了IPA，一种在线深度学习推理管道自适应系统，它能够有效地利用每个深度学习任务的模型变体。模型变体是同一深度学习任务的预训练模型的不同版本，其资源需求、延迟和准确性有所不同。IPA通过动态配置批处理大小、复制和模型变体来优化准确性、最小化成本并满足用户定义的延迟SLA。

    Efficiently optimizing multi-model inference pipelines for fast, accurate, and cost-effective inference is a crucial challenge in ML production systems, given their tight end-to-end latency requirements. To simplify the exploration of the vast and intricate trade-off space of accuracy and cost in inference pipelines, providers frequently opt to consider one of them. However, the challenge lies in reconciling accuracy and cost trade-offs. To address this challenge and propose a solution to efficiently manage model variants in inference pipelines, we present IPA, an online deep-learning Inference Pipeline Adaptation system that efficiently leverages model variants for each deep learning task. Model variants are different versions of pre-trained models for the same deep learning task with variations in resource requirements, latency, and accuracy. IPA dynamically configures batch size, replication, and model variants to optimize accuracy, minimize costs, and meet user-defined latency SLAs
    
[^22]: 自动加权的贝叶斯物理信息神经网络和鲁棒估计在孔隙尺度溶解图像的多任务反问题中的应用

    Auto-weighted Bayesian Physics-Informed Neural Networks and robust estimations for multitask inverse problems in pore-scale imaging of dissolution. (arXiv:2308.12864v1 [cs.LG])

    [http://arxiv.org/abs/2308.12864](http://arxiv.org/abs/2308.12864)

    本文介绍了一种新的数据同化策略，可可靠地处理包含不确定度量化的孔隙尺度反应反问题。该方法结合了数据驱动和物理建模，确保了孔隙尺度模型的可靠校准。

    

    在这篇文章中，我们提出了一种新的数据同化策略，并展示了这种方法可以使我们能够可靠地处理包含不确定度量化的反应反问题。孔隙尺度的反应流动建模为研究宏观性质在动态过程中的演变提供了宝贵的机会。然而，它们受到相关的X射线微计算高度分辨率成像 (X射线微CT) 过程中的成像限制的影响，导致了性质估计中的差异。动力学参数的评估也面临挑战，因为反应系数是关键参数，其数值范围很广。我们解决了这两个问题，并通过将不确定度量化集成到工作流程中，确保了孔隙尺度模型的可靠校准。当前的方法基于反应反问题的多任务公式，将数据驱动和物理建模相结合。

    In this article, we present a novel data assimilation strategy in pore-scale imaging and demonstrate that this makes it possible to robustly address reactive inverse problems incorporating Uncertainty Quantification (UQ). Pore-scale modeling of reactive flow offers a valuable opportunity to investigate the evolution of macro-scale properties subject to dynamic processes. Yet, they suffer from imaging limitations arising from the associated X-ray microtomography (X-ray microCT) process, which induces discrepancies in the properties estimates. Assessment of the kinetic parameters also raises challenges, as reactive coefficients are critical parameters that can cover a wide range of values. We account for these two issues and ensure reliable calibration of pore-scale modeling, based on dynamical microCT images, by integrating uncertainty quantification in the workflow.  The present method is based on a multitasking formulation of reactive inverse problems combining data-driven and physics
    
[^23]: 采用声学空间捕获-再捕获方法实现自动动物密度估计

    Towards Automated Animal Density Estimation with Acoustic Spatial Capture-Recapture. (arXiv:2308.12859v1 [cs.SD])

    [http://arxiv.org/abs/2308.12859](http://arxiv.org/abs/2308.12859)

    本论文提出了一种声学空间捕获-再捕获方法，通过结合个体级别的测量特征，实现了自动动物密度估计。

    

    无声动物的被动声学监测是一种有效的监测方法，但难以进行视觉调查。数字录音仪可以以较低的成本收集大量的数据，但在这些数据中识别目标物种的声音是非常困难的。机器学习方法通常用于进行声音识别，它们可以快速处理大量的数据，但不能检测到所有的声音，并且会产生一些误报（不属于目标物种的声音）。现有的野生动物种群密度估计方法已经专门设计用于处理这些错误中的第一个，但目前处理误报的方法还不够成熟。它们并没有考虑到个体声音特征，其中一些特征更有可能是误报。我们提出了三种声学空间捕获-再捕获推断方法，将个体级测量特征综合进去。

    Passive acoustic monitoring can be an effective way of monitoring wildlife populations that are acoustically active but difficult to survey visually. Digital recorders allow surveyors to gather large volumes of data at low cost, but identifying target species vocalisations in these data is non-trivial. Machine learning (ML) methods are often used to do the identification. They can process large volumes of data quickly, but they do not detect all vocalisations and they do generate some false positives (vocalisations that are not from the target species). Existing wildlife abundance survey methods have been designed specifically to deal with the first of these mistakes, but current methods of dealing with false positives are not well-developed. They do not take account of features of individual vocalisations, some of which are more likely to be false positives than others. We propose three methods for acoustic spatial capture-recapture inference that integrate individual-level measures o
    
[^24]: 快速对抗训练与平滑收敛

    Fast Adversarial Training with Smooth Convergence. (arXiv:2308.12857v1 [cs.LG])

    [http://arxiv.org/abs/2308.12857](http://arxiv.org/abs/2308.12857)

    本论文提出了一种快速对抗训练方法（FAT），通过引入平滑收敛过程和振荡约束来解决在处理大的扰动预算时出现的灾难性过拟合问题。

    

    快速对抗训练（FAT）有助于提高神经网络的对抗鲁棒性。然而，之前的FAT工作在处理大的扰动预算时遇到了一个重要问题，即在训练过程中，模型的对抗鲁棒性下降到接近零的程度，被称为灾难性过拟合。为了解决这个问题，我们分析了之前FAT工作的训练过程，并观察到灾难性过拟合伴随着收敛损失离群值的出现。因此，我们认为一个适度平滑的收敛过程将是一个稳定的FAT过程，可以解决灾难性过拟合。为了获得平滑的收敛过程，我们提出了一种新的振荡约束（称为ConvergeSmooth），来限制相邻周期之间的损失差异。ConvergeSmooth的收敛步长被引入来平衡收敛和平滑。同样，我们设计了不引入额外超参数的权重集中，除了l.

    Fast adversarial training (FAT) is beneficial for improving the adversarial robustness of neural networks. However, previous FAT work has encountered a significant issue known as catastrophic overfitting when dealing with large perturbation budgets, \ie the adversarial robustness of models declines to near zero during training.  To address this, we analyze the training process of prior FAT work and observe that catastrophic overfitting is accompanied by the appearance of loss convergence outliers.  Therefore, we argue a moderately smooth loss convergence process will be a stable FAT process that solves catastrophic overfitting.  To obtain a smooth loss convergence process, we propose a novel oscillatory constraint (dubbed ConvergeSmooth) to limit the loss difference between adjacent epochs. The convergence stride of ConvergeSmooth is introduced to balance convergence and smoothing. Likewise, we design weight centralization without introducing additional hyperparameters other than the l
    
[^25]: 带有储层计算的概率负载预测

    Probabilistic load forecasting with Reservoir Computing. (arXiv:2308.12844v1 [cs.LG])

    [http://arxiv.org/abs/2308.12844](http://arxiv.org/abs/2308.12844)

    这项研究将储层计算作为核心时间序列预测方法，探索了贝叶斯和确定性方法在不确定性估计方面的兼容性，并评估比较了它们在预测准确性、计算资源效率和估计可靠性方面的表现。

    

    深度学习的一些应用不仅需要提供准确的结果，还需要量化其预测的置信度。电力网络管理就是其中之一：为了避免风险，决策者需要精确可靠的负载预测。因此，仅仅提供点预测是不够的，因此需要采用能提供不确定性量化的方法。本研究以储层计算作为核心的时间序列预测方法，因其计算效率和时间序列预测效果而被选择。虽然储层计算的研究大多集中在点预测上，本研究探究了一些常用不确定性估计方法与储层设置的兼容性。贝叶斯和确定性方法都被评估和比较，涉及的评估指标有预测准确性、计算资源效率和估计的可靠性。

    Some applications of deep learning require not only to provide accurate results but also to quantify the amount of confidence in their prediction. The management of an electric power grid is one of these cases: to avoid risky scenarios, decision-makers need both precise and reliable forecasts of, for example, power loads. For this reason, point forecasts are not enough hence it is necessary to adopt methods that provide an uncertainty quantification.  This work focuses on reservoir computing as the core time series forecasting method, due to its computational efficiency and effectiveness in predicting time series. While the RC literature mostly focused on point forecasting, this work explores the compatibility of some popular uncertainty quantification methods with the reservoir setting. Both Bayesian and deterministic approaches to uncertainty assessment are evaluated and compared in terms of their prediction accuracy, computational resource efficiency and reliability of the estimated
    
[^26]: 使用强化学习的无人机顶部机械臂执行器轨迹规划

    Actuator Trajectory Planning for UAVs with Overhead Manipulator using Reinforcement Learning. (arXiv:2308.12843v1 [cs.RO])

    [http://arxiv.org/abs/2308.12843](http://arxiv.org/abs/2308.12843)

    本文研究了使用强化学习控制无人机顶部机械臂执行器轨迹的方法，并提出了基于时间到碰撞的运动规划模型以绕过障碍物。同时，利用基于模型的Q-learning模型独立追踪和控制机械臂末端执行器的期望轨迹。通过这种方法可以实现一系列在高难度和危险环境中的执行任务。

    

    本文研究了一种空中机械臂系统，即装备有可控制的二自由度臂的无人机，以实现即时执行任务。我们的解决方案基于使用Q-learning方法来控制臂尖端（即末端执行器）的轨迹。我们开发了一个基于时间到碰撞（TTC）的运动规划模型，使四旋翼无人机能够在保证机械臂可达性的同时绕过障碍物航行。此外，我们利用基于模型的Q-learning模型独立追踪和控制机械臂末端执行器的期望轨迹，给定无人机平台的任意基准轨迹。这种组合使得可以执行各种执行任务，如高空焊接、结构监测和修复、电池更换、排水沟清理、摩天大楼清洁和电力线路维护在难以到达和危险的环境中。

    In this paper, we investigate the operation of an aerial manipulator system, namely an Unmanned Aerial Vehicle (UAV) equipped with a controllable arm with two degrees of freedom to carry out actuation tasks on the fly. Our solution is based on employing a Q-learning method to control the trajectory of the tip of the arm, also called \textit{end-effector}. More specifically, we develop a motion planning model based on Time To Collision (TTC), which enables a quadrotor UAV to navigate around obstacles while ensuring the manipulator's reachability. Additionally, we utilize a model-based Q-learning model to independently track and control the desired trajectory of the manipulator's end-effector, given an arbitrary baseline trajectory for the UAV platform. Such a combination enables a variety of actuation tasks such as high-altitude welding, structural monitoring and repair, battery replacement, gutter cleaning, sky scrapper cleaning, and power line maintenance in hard-to-reach and risky en
    
[^27]: 使用基于深度学习的加权图的短途公交线路规划决策支持系统

    Short Run Transit Route Planning Decision Support System Using a Deep Learning-Based Weighted Graph. (arXiv:2308.12828v1 [cs.AI])

    [http://arxiv.org/abs/2308.12828](http://arxiv.org/abs/2308.12828)

    提出了一种基于深度学习的短途公交线路规划决策支持系统，通过调整路线的特定部分，减少时间并提升公共交通服务。利用多样化的数据源，通过预测道路段的延迟值作为边权重，实现了快速路径规划决策。

    

    公共交通路线规划在交通网络设计中起着至关重要的作用，确保乘客获得满意的服务水平。然而，当前的路线规划解决方案依赖于传统的运营研究启发式算法，实施起来耗时，并且缺乏提供快速解决方案的能力。在这里，我们提出了一种新颖的基于深度学习的方法，用于决策支持系统，使公共交通规划者能够快速确定短期路线改进。通过在特定时间段内无缝调整两个站点之间特定路段的路线，我们的方法有效地减少时间并提升公共交通服务。利用GTFS和智能卡数据等多样化的数据源，我们提取特征并将交通网络建模为有向图。通过自监督学习，我们训练了一个深度学习模型，用于预测道路段的延迟值。然后，这些延迟值被用作交通图中的边权重，实现了快速路径规划决策。

    Public transport routing plays a crucial role in transit network design, ensuring a satisfactory level of service for passengers. However, current routing solutions rely on traditional operational research heuristics, which can be time-consuming to implement and lack the ability to provide quick solutions. Here, we propose a novel deep learning-based methodology for a decision support system that enables public transport (PT) planners to identify short-term route improvements rapidly. By seamlessly adjusting specific sections of routes between two stops during specific times of the day, our method effectively reduces times and enhances PT services. Leveraging diverse data sources such as GTFS and smart card data, we extract features and model the transportation network as a directed graph. Using self-supervision, we train a deep learning model for predicting lateness values for road segments.  These lateness values are then utilized as edge weights in the transportation graph, enabling
    
[^28]: 不排除预测：基于可达集的后续验证方法

    Prediction without Preclusion: Recourse Verification with Reachable Sets. (arXiv:2308.12820v1 [cs.LG])

    [http://arxiv.org/abs/2308.12820](http://arxiv.org/abs/2308.12820)

    这项研究引入了一种称为后续验证的正式测试程序，用于检测模型分配固定预测的情况。通过开发可靠的机制，可以确定给定模型是否能为决策对象提供后续措施，从而解决了模型分配固定预测可能带来的问题。该研究还展示了如何在真实世界的数据集中确保后续措施和对抗鲁棒性，并探讨了在贷款数据集中实现后续措施的不可行性。

    

    机器学习模型常被用于决定谁有资格得到贷款、面试或公共福利。标准技术用于构建这些模型时，会使用关于人的特征，但忽视他们的可操作性。因此，模型可能会分配固定的预测，这意味着被拒绝贷款、面试或福利的消费者可能永久被排除在获得信贷、就业或援助的机会之外。在这项工作中，我们引入了一种正式的测试程序来检测分配固定预测的模型，我们称之为后续验证。我们开发了一套机制可靠地确定给定模型是否能提供对决策对象的后续手段，这些手段由用户指定的可操作性约束确定。我们演示了我们的工具如何在真实世界的数据集中确保后续措施和对抗鲁棒性，并利用它们研究了在真实世界的贷款数据集中实现后续措施的不可行性。我们的结果凸显了模型如何无意中分配固定预测，从而永久禁止使用者获得相关权益。

    Machine learning models are often used to decide who will receive a loan, a job interview, or a public benefit. Standard techniques to build these models use features about people but overlook their actionability. In turn, models can assign predictions that are fixed, meaning that consumers who are denied loans, interviews, or benefits may be permanently locked out from access to credit, employment, or assistance. In this work, we introduce a formal testing procedure to flag models that assign fixed predictions that we call recourse verification. We develop machinery to reliably determine if a given model can provide recourse to its decision subjects from a set of user-specified actionability constraints. We demonstrate how our tools can ensure recourse and adversarial robustness in real-world datasets and use them to study the infeasibility of recourse in real-world lending datasets. Our results highlight how models can inadvertently assign fixed predictions that permanently bar acces
    
[^29]: ICU使用长短期记忆网络进行死亡率预测

    ICU Mortality Prediction Using Long Short-Term Memory Networks. (arXiv:2308.12800v1 [cs.LG])

    [http://arxiv.org/abs/2308.12800](http://arxiv.org/abs/2308.12800)

    该论文通过使用长短期记忆网络分析床边监测产生的时间数据，成功构建了用于预测ICU患者死亡率和住院时间的系统。

    

    在重症监护病房 (ICU) 进行的大量床边监测产生了与患者生理相关的复杂时间数据，这为临床数据分析提供了一个复杂的背景。另一方面，识别出这些数据中的时间序列模式可能会提供高度能力来预测临床事件。因此，在这项工作中，我们研究了实施一个自动数据驱动的系统，该系统分析来自电子健康记录 (EHRs) 的大量多变量时间数据，并提取高级信息以早期预测住院死亡率和住院时间。在实践中，我们通过将时间窗口缩短到6小时来研究LSTM网络的适用性，以增强临床任务。实验结果突出了LSTM模型的效率，通过严格的多变量时间序列测量来构建真实世界的预测引擎。

    Extensive bedside monitoring in Intensive Care Units (ICUs) has resulted in complex temporal data regarding patient physiology, which presents an upscale context for clinical data analysis. In the other hand, identifying the time-series patterns within these data may provide a high aptitude to predict clinical events. Hence, we investigate, during this work, the implementation of an automatic data-driven system, which analyzes large amounts of multivariate temporal data derived from Electronic Health Records (EHRs), and extracts high-level information so as to predict in-hospital mortality and Length of Stay (LOS) early. Practically, we investigate the applicability of LSTM network by reducing the time-frame to 6-hour so as to enhance clinical tasks. The experimental results highlight the efficiency of LSTM model with rigorous multivariate time-series measurements for building real-world prediction engines.
    
[^30]: 工作车间调度基准：用于学习和非学习方法的环境和实例

    Job Shop Scheduling Benchmark: Environments and Instances for Learning and Non-learning Methods. (arXiv:2308.12794v1 [cs.AI])

    [http://arxiv.org/abs/2308.12794](http://arxiv.org/abs/2308.12794)

    这个开源的GitHub仓库为机器调度问题提供了综合基准，包括多种环境和实例，为研究人员和从业者提供了一个集中的中心。

    

    我们介绍了一个开源的GitHub仓库，其中包含了广泛的机器调度问题的综合基准，包括工作车间调度（JSP），流水车间调度（FSP），灵活工作车间调度（FJSP），具有装配约束的FJSP（FAJSP），具有序列依赖设置时间的FJSP（FJSP-SDST）和在线FJSP（在线作业到达）。我们的主要目标是为对机器调度挑战感兴趣的研究人员，从业者和爱好者提供一个集中的中心。

    We introduce an open-source GitHub repository containing comprehensive benchmarks for a wide range of machine scheduling problems, including Job Shop Scheduling (JSP), Flow Shop Scheduling (FSP), Flexible Job Shop Scheduling (FJSP), FJSP with Assembly constraints (FAJSP), FJSP with Sequence-Dependent Setup Times (FJSP-SDST), and the online FJSP (with online job arrivals). Our primary goal is to provide a centralized hub for researchers, practitioners, and enthusiasts interested in tackling machine scheduling challenges.
    
[^31]: 单次贝叶斯近似用于神经网络

    Single-shot Bayesian approximation for neural networks. (arXiv:2308.12785v1 [cs.LG])

    [http://arxiv.org/abs/2308.12785](http://arxiv.org/abs/2308.12785)

    这篇论文提出了一种单次MC dropout近似方法，以将神经网络转换为贝叶斯变体的神经网络，该方法具有与普通神经网络相同的计算速度，同时保留了贝叶斯变体神经网络提供的不确定度测量。

    

    深度神经网络以其高预测性能而闻名。然而，当遇到完全新的情况并且没有指示其不确定性时，神经网络很容易产生不可靠的预测。贝叶斯变体的神经网络（BNNs），如蒙特卡洛（MC）dropout BNNs，在提供不确定度测量的同时提高了预测性能。BNNs唯一的缺点是它们在测试时计算时间较长，因为它们依赖于一种采样方法。在这里，我们提出了一种单次MC dropout近似，它保留了BNNs的优点，同时与神经网络一样快。我们的方法基于矩传播（MP），可以在常用的神经网络层（卷积、最大池化、全连接、softmax和dropout层）中解析地近似MC dropout信号的期望值和方差。MP方法可以在不重新训练的情况下将神经网络转换为BNN，只要NN已经使用标准的dropout进行了训练。我们评估了我们的方法。

    Deep neural networks (NNs) are known for their high-prediction performances. However, NNs are prone to yield unreliable predictions when encountering completely new situations without indicating their uncertainty. Bayesian variants of NNs (BNNs), such as Monte Carlo (MC) dropout BNNs, do provide uncertainty measures and simultaneously increase the prediction performance. The only disadvantage of BNNs is their higher computation time during test time because they rely on a sampling approach. Here we present a single-shot MC dropout approximation that preserves the advantages of BNNs while being as fast as NNs. Our approach is based on moment propagation (MP) and allows to analytically approximate the expected value and the variance of the MC dropout signal for commonly used layers in NNs, i.e. convolution, max pooling, dense, softmax, and dropout layers. The MP approach can convert an NN into a BNN without re-training given the NN has been trained with standard dropout. We evaluate our 
    
[^32]: 通过故意低估终止状态的值函数来解决时间差异学习中的错误奖励问题

    Intentionally-underestimated Value Function at Terminal State for Temporal-difference Learning with Mis-designed Reward. (arXiv:2308.12772v1 [cs.RO])

    [http://arxiv.org/abs/2308.12772](http://arxiv.org/abs/2308.12772)

    本文提出了一种方法来解决在时间差异学习中终止状态下存在的奖励设计问题，通过故意低估终止后的值来避免学习错误的策略。

    

    使用强化学习的机器人控制已经变得流行，但是由于安全和节约时间的原因，学习过程通常在一集的中途终止。本研究解决了在这种终止情况下时间差异（TD）学习执行的最常见异常处理问题。也就是说，在终止后通过强制假设值为零，会意外地引起低估或高估，这取决于正常状态下的奖励设计。当任务失败导致一集终止时，由于非预期的高估，失败可能被高度评价，并且可能获取错误的策略。虽然通过注意奖励设计可以避免此问题，但在实际使用中审查终止时的异常处理是TD学习的要点。因此，本文提出了一种方法来故意低估终止后的值，以避免由于非预期的高估而导致的学习失败。

    Robot control using reinforcement learning has become popular, but its learning process generally terminates halfway through an episode for safety and time-saving reasons. This study addresses the problem of the most popular exception handling that temporal-difference (TD) learning performs at such termination. That is, by forcibly assuming zero value after termination, unintentionally implicit underestimation or overestimation occurs, depending on the reward design in the normal states. When the episode is terminated due to task failure, the failure may be highly valued with the unintentional overestimation, and the wrong policy may be acquired. Although this problem can be avoided by paying attention to the reward design, it is essential in practical use of TD learning to review the exception handling at termination. This paper therefore proposes a method to intentionally underestimate the value after termination to avoid learning failures due to the unintentional overestimation. In 
    
[^33]: 关于平均嵌入用于物品推荐的一致性研究

    On the Consistency of Average Embeddings for Item Recommendation. (arXiv:2308.12767v1 [cs.IR])

    [http://arxiv.org/abs/2308.12767](http://arxiv.org/abs/2308.12767)

    本文研究了推荐系统中平均嵌入的一致性，并提出了一种衡量方法。实证结果表明，现实世界的平均嵌入在推荐中一致性较低，为进一步改进现实世界嵌入提供了方向。

    

    推荐系统中一种流行的做法是将物品嵌入进行平均以在同一嵌入空间中代表用户或更高级的概念。本文研究了这种做法的相关性。为此，我们提出了一种期望精度分数，用于衡量平均嵌入与其构建所使用的物品的一致性。我们随后在具有特定假设的理论环境和来自音乐流媒体服务的真实数据上分析了该分数的数学表达式及其经验表现。我们的结果强调了现实世界的平均值在推荐中的一致性较低，为未来研究更好地将现实世界的嵌入与我们理论环境的假设相一致铺平了道路。

    A prevalent practice in recommender systems consists of averaging item embeddings to represent users or higher-level concepts in the same embedding space. This paper investigates the relevance of such a practice. For this purpose, we propose an expected precision score, designed to measure the consistency of an average embedding relative to the items used for its construction. We subsequently analyze the mathematical expression of this score in a theoretical setting with specific assumptions, as well as its empirical behavior on real-world data from music streaming services. Our results emphasize that real-world averages are less consistent for recommendation, which paves the way for future research to better align real-world embeddings with assumptions from our theoretical setting.
    
[^34]: IP-UNet：用于3D医学体积分割的强度投影UNet架构

    IP-UNet: Intensity Projection UNet Architecture for 3D Medical Volume Segmentation. (arXiv:2308.12761v1 [eess.IV])

    [http://arxiv.org/abs/2308.12761](http://arxiv.org/abs/2308.12761)

    IP-UNet是一种用于3D医学体积分割的深度学习方法，通过对3D体积数据的强度投影进行多类别分割，在保留原始分辨率的同时减少内存消耗。

    

    CNN已广泛应用于医学图像分析。然而，有限的内存容量是处理高分辨率3D体积数据的常见缺点之一。通常在处理之前需要对3D体积进行裁剪或缩小，这可能导致分辨率降低，增加类别不平衡，并影响分割算法的性能。在本文中，我们提出了一种称为IP-UNet的端到端深度学习方法。IP-UNet是一种基于UNet的模型，它对3D体积数据的强度投影（IP）进行多类别分割，而不是占用大量内存的3D体积。IP-UNet利用有限的内存能力进行训练，同时保留了原始的3D图像分辨率。我们通过分割准确性和计算成本比较了三种模型的性能：1）使用传统的2D UNet模型对CT扫描图像进行逐层2D分割。2）使用IP-UNet对提取的最大强度投影进行数据融合处理。

    CNNs have been widely applied for medical image analysis. However, limited memory capacity is one of the most common drawbacks of processing high-resolution 3D volumetric data. 3D volumes are usually cropped or downsized first before processing, which can result in a loss of resolution, increase class imbalance, and affect the performance of the segmentation algorithms. In this paper, we propose an end-to-end deep learning approach called IP-UNet. IP-UNet is a UNet-based model that performs multi-class segmentation on Intensity Projection (IP) of 3D volumetric data instead of the memory-consuming 3D volumes. IP-UNet uses limited memory capability for training without losing the original 3D image resolution. We compare the performance of three models in terms of segmentation accuracy and computational cost: 1) Slice-by-slice 2D segmentation of the CT scan images using a conventional 2D UNet model. 2) IP-UNet that operates on data obtained by merging the extracted Maximum Intensity Proje
    
[^35]: 使用相位流形的动作插帧

    Motion In-Betweening with Phase Manifolds. (arXiv:2308.12751v1 [cs.GR])

    [http://arxiv.org/abs/2308.12751](http://arxiv.org/abs/2308.12751)

    本论文介绍了一种使用相位流形的动作插帧系统，通过学习相位变量和混合专家神经网络模型来生成目标姿势之间的连续姿势序列，同时可以满足动画师手动修改的姿势和末端效应器作为约束的要求。

    

    本论文介绍了一种新颖的数据驱动动作插帧系统，通过使用周期自编码器学习的相位变量来达到角色的目标姿势。我们的方法利用混合专家神经网络模型，其中相位以不同的专家权重将动作在空间和时间上进行聚类。每组生成的权重以自回归的方式在当前状态和目标状态之间产生一系列姿势。此外，为了满足动画师手动修改的姿势或某些末端效应器作为动画要达到的约束，实施了一种学习的双向控制方案来满足这些约束。结果表明，使用相位进行动作插帧可以提高插值动作的锐度，并进一步稳定学习过程。此外，使用相位进行动作插帧还可以合成更具挑战性的运动，超越了行走等基本运动。

    This paper introduces a novel data-driven motion in-betweening system to reach target poses of characters by making use of phases variables learned by a Periodic Autoencoder. Our approach utilizes a mixture-of-experts neural network model, in which the phases cluster movements in both space and time with different expert weights. Each generated set of weights then produces a sequence of poses in an autoregressive manner between the current and target state of the character. In addition, to satisfy poses which are manually modified by the animators or where certain end effectors serve as constraints to be reached by the animation, a learned bi-directional control scheme is implemented to satisfy such constraints. The results demonstrate that using phases for motion in-betweening tasks sharpen the interpolated movements, and furthermore stabilizes the learning process. Moreover, using phases for motion in-betweening tasks can also synthesize more challenging movements beyond locomotion b
    
[^36]: 人类可理解的基因组规模代谢网络的主动学习

    Human Comprehensible Active Learning of Genome-Scale Metabolic Networks. (arXiv:2308.12740v1 [cs.AI])

    [http://arxiv.org/abs/2308.12740](http://arxiv.org/abs/2308.12740)

    这项研究介绍了一种人类可理解的基因组规模代谢网络的主动学习方法，基于归纳逻辑编程(ILP)框架进行逻辑推理，并通过从实验中学习新的逻辑结构，以有效探索假设空间和指导实验设计。

    

    合成生物学的一个重要应用是将宿主细胞系统工程化以产生有用的产品。然而，宿主系统规模的增加导致设计空间巨大，并需要大量高昂的验证试验。为了宿主细胞系统的设计-构建-测试-学习（Design-Build-Test-Learn，DBTL）周期，迫切需要一种能有效探索假设空间并指导实验设计的可理解的机器学习方法。我们引入了一种基于归纳逻辑编程（ILP）的新型机器学习框架ILP-iML1515，它通过诱导逻辑推理和从训练实例中积极学习来执行说明性的逻辑推理。与数值模型不同，ILP-iML1515建立在对基因组规模代谢模型的可理解的逻辑表示上，并可以通过从缺乏营养的突变体试验中学习新的逻辑结构来更新模型。ILP-iML1515框架具有高通量模拟能力，并能主动选择实验。

    An important application of Synthetic Biology is the engineering of the host cell system to yield useful products. However, an increase in the scale of the host system leads to huge design space and requires a large number of validation trials with high experimental costs. A comprehensible machine learning approach that efficiently explores the hypothesis space and guides experimental design is urgently needed for the Design-Build-Test-Learn (DBTL) cycle of the host cell system. We introduce a novel machine learning framework ILP-iML1515 based on Inductive Logic Programming (ILP) that performs abductive logical reasoning and actively learns from training examples. In contrast to numerical models, ILP-iML1515 is built on comprehensible logical representations of a genome-scale metabolic model and can update the model by learning new logical structures from auxotrophic mutant trials. The ILP-iML1515 framework 1) allows high-throughput simulations and 2) actively selects experiments that 
    
[^37]: 实时检测AI生成的语音用于DeepFake语音转换

    Real-time Detection of AI-Generated Speech for DeepFake Voice Conversion. (arXiv:2308.12734v1 [cs.SD])

    [http://arxiv.org/abs/2308.12734](http://arxiv.org/abs/2308.12734)

    该研究创建了DEEP-VOICE数据集，并通过统计分析和机器学习模型实现了实时检测AI生成语音的目标，以应对DeepFake语音转换带来的道德和隐私问题。

    

    在语音领域中，生成型AI技术使得语音克隆和实时语音转换成为可能，这带来了一系列潜在的道德问题，包括隐私侵犯和虚假陈述。因此，我们亟需一种能够实时检测AI生成语音的方法来应对DeepFake语音转换。为此，我们创建了DEEP-VOICE数据集，其中包含了八位知名人物的真实语音和他们之间相互转换后的语音。通过对语音真实性进行二分类，通过t检验对时间音频特征进行了统计分析，发现其分布存在显著差异。使用超参数优化来对机器学习模型进行训练，以识别语音的来源。

    There are growing implications surrounding generative AI in the speech domain that enable voice cloning and real-time voice conversion from one individual to another. This technology poses a significant ethical threat and could lead to breaches of privacy and misrepresentation, thus there is an urgent need for real-time detection of AI-generated speech for DeepFake Voice Conversion. To address the above emerging issues, the DEEP-VOICE dataset is generated in this study, comprised of real human speech from eight well-known figures and their speech converted to one another using Retrieval-based Voice Conversion. Presenting as a binary classification problem of whether the speech is real or AI-generated, statistical analysis of temporal audio features through t-testing reveals that there are significantly different distributions. Hyperparameter optimisation is implemented for machine learning models to identify the source of speech. Following the training of 208 individual machine learnin
    
[^38]: 独特思维：通过专家路径选择和游戏鲸鱼检测改进客户终身价值建模

    Out of the Box Thinking: Improving Customer Lifetime Value Modelling via Expert Routing and Game Whale Detection. (arXiv:2308.12729v1 [cs.IR])

    [http://arxiv.org/abs/2308.12729](http://arxiv.org/abs/2308.12729)

    本文提出了一种新颖的多任务框架ExpLTV，利用游戏鲸鱼的检测来改进客户终身价值预测模型的准确性。

    

    客户终身价值（LTV）预测对于试图根据估计价值优化广告投资的移动游戏发行商至关重要。在移动游戏中，部署微交易是一种简单而有效的货币化策略，吸引了一小群在游戏内购买上大量消费的游戏鲸鱼。这种游戏鲸鱼的存在可能阻碍现有LTV预测模型的实用性，因为游戏鲸鱼的购买行为总是表现出与普通用户不同的分布。因此，识别游戏鲸鱼可以为改进LTV预测模型的准确性打开新机会。然而，目前在LTV预测中应用游戏鲸鱼检测的研究很少，现有工作主要针对假设可获得高质量用户特征的长期LTV预测，这在用户获取阶段不适用。在本文中，我们提出了一种新颖的多任务框架ExpLTV。

    Customer lifetime value (LTV) prediction is essential for mobile game publishers trying to optimize the advertising investment for each user acquisition based on the estimated worth. In mobile games, deploying microtransactions is a simple yet effective monetization strategy, which attracts a tiny group of game whales who splurge on in-game purchases. The presence of such game whales may impede the practicality of existing LTV prediction models, since game whales' purchase behaviours always exhibit varied distribution from general users. Consequently, identifying game whales can open up new opportunities to improve the accuracy of LTV prediction models. However, little attention has been paid to applying game whale detection in LTV prediction, and existing works are mainly specialized for the long-term LTV prediction with the assumption that the high-quality user features are available, which is not applicable in the UA stage. In this paper, we propose ExpLTV, a novel multi-task framew
    
[^39]: 基于连续强化学习的视觉工作记忆游戏动态难度调整

    Continuous Reinforcement Learning-based Dynamic Difficulty Adjustment in a Visual Working Memory Game. (arXiv:2308.12726v1 [cs.HC])

    [http://arxiv.org/abs/2308.12726](http://arxiv.org/abs/2308.12726)

    本文提出了一种基于连续强化学习的动态难度调整方法，用于处理视觉工作记忆游戏中的复杂难度记忆问题。通过根据玩家的得分和上一轮游戏的难度量度来调整游戏难度，该方法在52位受试者的实验中得到了评估和比较。

    

    动态难度调整（DDA）是提升玩家在视频游戏中体验的一种有效方法。最近，强化学习（RL）方法已被应用于非竞争性游戏的DDA；尽管如此，它们仅依赖于具有小的搜索空间的离散状态-动作空间。在本文中，我们提出了一种基于连续强化学习的DDA方法，用于处理视觉工作记忆（VWM）游戏中难度记忆的复杂搜索空间。该提出的基于RL的DDA根据玩家的得分和上一轮游戏的难度量度来调整游戏难度。我们定义了一种连续的难度记忆度量。然后，我们将任务难度和难度-得分向量分别作为RL的动作和状态。我们通过一个涉及52位受试者的被试内实验来评估所提出的方法。在玩家的得分和游戏体验度量方面，将所提出的方法与两种基于规则的难度调整方法进行了比较。

    Dynamic Difficulty Adjustment (DDA) is a viable approach to enhance a player's experience in video games. Recently, Reinforcement Learning (RL) methods have been employed for DDA in non-competitive games; nevertheless, they rely solely on discrete state-action space with a small search space. In this paper, we propose a continuous RL-based DDA methodology for a visual working memory (VWM) game to handle the complex search space for the difficulty of memorization. The proposed RL-based DDA tailors game difficulty based on the player's score and game difficulty in the last trial. We defined a continuous metric for the difficulty of memorization. Then, we consider the task difficulty and the vector of difficulty-score as the RL's action and state, respectively. We evaluated the proposed method through a within-subject experiment involving 52 subjects. The proposed approach was compared with two rule-based difficulty adjustment methods in terms of player's score and game experience measure
    
[^40]: 使用物理信息神经网络解决接触力学的正向和反向问题

    Solving Forward and Inverse Problems of Contact Mechanics using Physics-Informed Neural Networks. (arXiv:2308.12716v1 [math.NA])

    [http://arxiv.org/abs/2308.12716](http://arxiv.org/abs/2308.12716)

    本文研究了使用物理信息神经网络（PINNs）解决接触力学的前向和反向问题，并将不等式约束转化为损失函数中的软约束。实验证明PINNs可以作为PDE求解器、数据增强的前向模型和反向求解器。

    

    本文探讨了物理信息神经网络（PINNs）在小变形弹性接触力学的正向和反向问题中的能力。我们采用混合变量形式的PINNs，并通过输出变换增强其能够强制执行迪里切特和诺依曼边界条件作为硬约束。接触问题的不等式约束（即 Karush-Kuhn-Tucker (KKT) 类型条件）通过将其纳入网络训练的损失函数中作为软约束来强制执行。为了形成KKT约束的损失函数贡献，我们研究了现有的用于弹塑性问题的方法，并探索了一种非线性互补问题（NCP）函数，即 Fischer-Burmeister 函数，它在优化方面具有有利的特性。基于赫兹接触问题，我们展示了PINNs可以作为纯偏微分方程（PDE）求解器、数据增强的前向模型和反向求解器。

    This paper explores the ability of physics-informed neural networks (PINNs) to solve forward and inverse problems of contact mechanics for small deformation elasticity. We deploy PINNs in a mixed-variable formulation enhanced by output transformation to enforce Dirichlet and Neumann boundary conditions as hard constraints. Inequality constraints of contact problems, namely Karush-Kuhn-Tucker (KKT) type conditions, are enforced as soft constraints by incorporating them into the loss function during network training. To formulate the loss function contribution of KKT constraints, existing approaches applied to elastoplasticity problems are investigated and we explore a nonlinear complementarity problem (NCP) function, namely Fischer-Burmeister, which possesses advantageous characteristics in terms of optimization. Based on the Hertzian contact problem, we show that PINNs can serve as pure partial differential equation (PDE) solver, as data-enhanced forward model, as inverse solver for pa
    
[^41]: 通过拓扑学习实现解缠编码

    Disentanglement Learning via Topology. (arXiv:2308.12696v1 [cs.LG])

    [http://arxiv.org/abs/2308.12696](http://arxiv.org/abs/2308.12696)

    本文提出了一种通过拓扑损失实现解缠编码的方法，这是第一个提出用于解缠的可微拓扑损失的论文，实验结果表明所提出的方法相对于最新结果改进了解缠得分。

    

    我们提出了TopDis（拓扑解缠），一种通过增加多尺度拓扑损失项学习解缠表示的方法。解缠是数据表示的关键属性，对深度学习模型的可解释性和鲁棒性以及高级认知的实现都非常重要。基于VAE的最新方法通过最小化潜变量的联合分布的总体相关性来实现解缠。我们从分析数据流形的拓扑属性的角度来看待解缠，特别是优化数据流形遍历的拓扑相似性。据我们所知，我们的论文是第一个提出用于解缠的可微拓扑损失的方法。我们的实验结果表明，所提出的拓扑损失相对于最新结果改进了解缠得分，如MIG、FactorVAE得分、SAP得分和DCI解缠得分。我们的方法以无监督的方式工作。

    We propose TopDis (Topological Disentanglement), a method for learning disentangled representations via adding multi-scale topological loss term. Disentanglement is a crucial property of data representations substantial for the explainability and robustness of deep learning models and a step towards high-level cognition. The state-of-the-art method based on VAE minimizes the total correlation of the joint distribution of latent variables. We take a different perspective on disentanglement by analyzing topological properties of data manifolds. In particular, we optimize the topological similarity for data manifolds traversals. To the best of our knowledge, our paper is the first one to propose a differentiable topological loss for disentanglement. Our experiments have shown that the proposed topological loss improves disentanglement scores such as MIG, FactorVAE score, SAP score and DCI disentanglement score with respect to state-of-the-art results. Our method works in an unsupervised m
    
[^42]: 一种使用多模型线性回归的高效大数据分析方法

    An Efficient Data Analysis Method for Big Data using Multiple-Model Linear Regression. (arXiv:2308.12691v1 [cs.LG])

    [http://arxiv.org/abs/2308.12691](http://arxiv.org/abs/2308.12691)

    本文介绍了一种使用多模型线性回归（MMLR）的高效数据分析方法，该方法将输入数据集分为子集并构建局部线性回归模型。通过提出的近似算法，该方法在时间复杂性和预测准确度方面都表现出良好的性能。

    

    本文介绍了一种新的数据分析方法，使用新定义的回归模型多模型线性回归（MMLR），该方法将输入数据集分为子集并构建局部线性回归模型。所提出的数据分析方法比其他基于回归的方法更高效和灵活。本文还提出了一种基于$(\epsilon,\delta)$-估计量的MMLR模型构造的近似算法，并对MMLR算法的正确性和效率给出了数学证明，其时间复杂性与输入数据集的大小成线性关系。本文还在合成和真实数据集上进行了实证实现，算法在许多情况下表现出与现有回归方法可比较的性能，同时提供高预测准确度所需的时间几乎最短。

    This paper introduces a new data analysis method for big data using a newly defined regression model named multiple model linear regression(MMLR), which separates input datasets into subsets and construct local linear regression models of them. The proposed data analysis method is shown to be more efficient and flexible than other regression based methods. This paper also proposes an approximate algorithm to construct MMLR models based on $(\epsilon,\delta)$-estimator, and gives mathematical proofs of the correctness and efficiency of MMLR algorithm, of which the time complexity is linear with respect to the size of input datasets. This paper also empirically implements the method on both synthetic and real-world datasets, the algorithm shows to have comparable performance to existing regression methods in many cases, while it takes almost the shortest time to provide a high prediction accuracy.
    
[^43]: Match-And-Deform: 通过最优传输和时间对齐进行时间序列领域自适应

    Match-And-Deform: Time Series Domain Adaptation through Optimal Transport and Temporal Alignment. (arXiv:2308.12686v1 [cs.LG])

    [http://arxiv.org/abs/2308.12686](http://arxiv.org/abs/2308.12686)

    本文提出了一种名为Match-And-Deform（MAD）的方法，通过最优传输和时间对齐，在时间序列领域自适应问题中找到对应关系并允许时间失真，实验结果表明MAD可以生成对齐领域并最大化网络判别能力的新时间序列表示形式。

    

    尽管通常存在大量的无标签数据，但相关联的标签往往很少。无监督领域自适应问题旨在利用来自源领域的标签来对来自相关但不同的目标领域的数据进行分类。当涉及时间序列时，除了标准特征分布偏移之外，还会出现时间偏移的新难题。在本文中，我们介绍了Match-And-Deform（MAD）方法，该方法旨在在允许时间失真的同时在源时间序列和目标时间序列之间找到对应关系。通过最优传输损失和动态时间扭曲同时对齐了系列。当嵌入到深度神经网络中时，MAD有助于学习时间序列的新表示，既可以对齐领域又可以最大化网络的判别能力。对基准数据集和遥感数据进行的实证研究表明，MAD具有意义。

    While large volumes of unlabeled data are usually available, associated labels are often scarce. The unsupervised domain adaptation problem aims at exploiting labels from a source domain to classify data from a related, yet different, target domain. When time series are at stake, new difficulties arise as temporal shifts may appear in addition to the standard feature distribution shift. In this paper, we introduce the Match-And-Deform (MAD) approach that aims at finding correspondences between the source and target time series while allowing temporal distortions. The associated optimization problem simultaneously aligns the series thanks to an optimal transport loss and the time stamps through dynamic time warping. When embedded into a deep neural network, MAD helps learning new representations of time series that both align the domains and maximize the discriminative power of the network. Empirical studies on benchmark datasets and remote sensing data demonstrate that MAD makes meanin
    
[^44]: LR-XFL: 基于逻辑推理的可解释联邦学习

    LR-XFL: Logical Reasoning-based Explainable Federated Learning. (arXiv:2308.12681v1 [cs.AI])

    [http://arxiv.org/abs/2308.12681](http://arxiv.org/abs/2308.12681)

    LR-XFL是一种基于逻辑推理的可解释联邦学习方法，通过将逻辑规则和模型更新结合起来，实现了对FL模型的解释性提升和加权聚合，并在相关基准测试中取得了较好的效果。

    

    联邦学习 (FL) 是一种新兴的机器学习模型协作训练方法，能够保护数据隐私。隐私保护的需求使得FL模型很难实现全局透明度和可解释性。为了解决这个限制，我们提出了基于逻辑推理的可解释联邦学习 (LR-XFL) 方法，将逻辑推理融入FL中。在LR-XFL中，FL客户端根据其本地数据创建本地逻辑规则，并将其与模型更新一起发送到FL服务器。FL服务器通过适当的逻辑连接符将本地逻辑规则连接起来，该连接符基于客户端数据的属性进行推导，而无需访问原始数据。此外，服务器还根据客户端上传的逻辑规则反映的本地数据的质量，使用权重值对本地模型更新进行聚合。结果显示，LR-XFL在最相关的基准测试中超过1.19％，5.81％和5.41％。

    Federated learning (FL) is an emerging approach for training machine learning models collaboratively while preserving data privacy. The need for privacy protection makes it difficult for FL models to achieve global transparency and explainability. To address this limitation, we incorporate logic-based explanations into FL by proposing the Logical Reasoning-based eXplainable Federated Learning (LR-XFL) approach. Under LR-XFL, FL clients create local logic rules based on their local data and send them, along with model updates, to the FL server. The FL server connects the local logic rules through a proper logical connector that is derived based on properties of client data, without requiring access to the raw data. In addition, the server also aggregates the local model updates with weight values determined by the quality of the clients' local data as reflected by their uploaded logic rules. The results show that LR-XFL outperforms the most relevant baseline by 1.19%, 5.81% and 5.41% in
    
[^45]: 使用主从深度架构解决具有非线性边际反馈和多样性约束的前K个多臂赌博机问题

    Master-slave Deep Architecture for Top-K Multi-armed Bandits with Non-linear Bandit Feedback and Diversity Constraints. (arXiv:2308.12680v1 [cs.LG])

    [http://arxiv.org/abs/2308.12680](http://arxiv.org/abs/2308.12680)

    这篇论文提出了一种用于解决具有非线性边际反馈和多样性约束的前K个组合多臂赌博机问题的新型主从架构，通过引入六个从模型和教师学习优化以及策略共训练技术，实现了高效的探索和利用之间的决策权衡。

    

    我们提出了一种新颖的主从架构，用于解决具有非线性边际反馈和多样性约束的前K个组合多臂赌博机问题，据我们所知，这是第一个在赌博反馈下考虑多样性约束的组合臂赌博机设置。具体而言，为了高效地探索组合和受约束的行动空间，我们引入了六个具有显著优点的从模型，以生成平衡奖励和约束以及效率的多样化样本。此外，我们提出了基于教师学习的优化和策略共训练技术，以提升多个从模型的性能。然后，主模型收集从模型提供的精英样本，并通过基于神经上下文UCB网络估计的最佳样本来做出在探索和利用之间权衡的决策。由于从模型的精心设计，共同训练机制成效显著。

    We propose a novel master-slave architecture to solve the top-$K$ combinatorial multi-armed bandits problem with non-linear bandit feedback and diversity constraints, which, to the best of our knowledge, is the first combinatorial bandits setting considering diversity constraints under bandit feedback. Specifically, to efficiently explore the combinatorial and constrained action space, we introduce six slave models with distinguished merits to generate diversified samples well balancing rewards and constraints as well as efficiency. Moreover, we propose teacher learning based optimization and the policy co-training technique to boost the performance of the multiple slave models. The master model then collects the elite samples provided by the slave models and selects the best sample estimated by a neural contextual UCB-based network to make a decision with a trade-off between exploration and exploitation. Thanks to the elaborate design of slave models, the co-training mechanism among s
    
[^46]: 跨领域白细胞分类的持续学习方法

    A Continual Learning Approach for Cross-Domain White Blood Cell Classification. (arXiv:2308.12679v1 [cs.CV])

    [http://arxiv.org/abs/2308.12679](http://arxiv.org/abs/2308.12679)

    提出了一种基于重听的持续学习方法，用于跨领域白细胞分类，通过选择代表样本来避免灾难性遗忘。

    

    准确分类外周血液中的白细胞对于诊断血液病非常重要。由于临床环境、数据来源和疾病分类不断变化，为了适应实际应用，需要定期更新机器学习分类模型。这些模型在从输入数据流中顺序学习时会受益匪浅，但是当在新数据上微调时，模型可能会遭受灾难性遗忘，导致在之前的任务上性能下降。在这里，我们提出了一种基于重听的持续学习方法，用于白细胞分类中的类增量和领域增量场景。为了选择前一任务中的代表样本，我们采用了基于模型预测的样本选择方法。这包括通过模型的不确定性估计选择最有信心的样本和最具挑战性的样本。

    Accurate classification of white blood cells in peripheral blood is essential for diagnosing hematological diseases. Due to constantly evolving clinical settings, data sources, and disease classifications, it is necessary to update machine learning classification models regularly for practical real-world use. Such models significantly benefit from sequentially learning from incoming data streams without forgetting previously acquired knowledge. However, models can suffer from catastrophic forgetting, causing a drop in performance on previous tasks when fine-tuned on new data. Here, we propose a rehearsal-based continual learning approach for class incremental and domain incremental scenarios in white blood cell classification. To choose representative samples from previous tasks, we employ exemplar set selection based on the model's predictions. This involves selecting the most confident samples and the most challenging samples identified through uncertainty estimation of the model. We
    
[^47]: 遮罩特征建模：无监督预训练图注意力网络块的特征遮罩方法

    Masked Feature Modelling: Feature Masking for the Unsupervised Pre-training of a Graph Attention Network Block for Bottom-up Video Event Recognition. (arXiv:2308.12673v1 [cs.CV])

    [http://arxiv.org/abs/2308.12673](http://arxiv.org/abs/2308.12673)

    本文提出了一种新的遮罩特征建模方法(MFM)，用于无监督预训练图注意力网络块。通过利用预训练的视觉分词器来重构视频中对象的遮罩特征，将预训练的GAT块融入到视频事件识别架构ViGAT中，以改善模型的性能。

    

    本文介绍了一种新颖的方法，即遮罩特征建模(MFM)，用于无监督预训练图注意力网络(GAT)块。MFM利用预训练的视觉分词器来重构视频中对象的遮罩特征，利用MiniKinetics数据集。然后将预训练的GAT块融入到最先进的自底向上有监督视频事件识别架构ViGAT中，以改善模型的起点和整体准确性。在YLI-MED数据集上的实验评估表明，MFM在提高事件识别性能方面具有有效性。

    In this paper, we introduce Masked Feature Modelling (MFM), a novel approach for the unsupervised pre-training of a Graph Attention Network (GAT) block. MFM utilizes a pretrained Visual Tokenizer to reconstruct masked features of objects within a video, leveraging the MiniKinetics dataset. We then incorporate the pre-trained GAT block into a state-of-the-art bottom-up supervised video-event recognition architecture, ViGAT, to improve the model's starting point and overall accuracy. Experimental evaluations on the YLI-MED dataset demonstrate the effectiveness of MFM in improving event recognition performance.
    
[^48]: 维护操作中数据共享学习的最优数据汇聚

    Optimal data pooling for shared learning in maintenance operations. (arXiv:2308.12670v1 [cs.LG])

    [http://arxiv.org/abs/2308.12670](http://arxiv.org/abs/2308.12670)

    本文研究了在维护操作中，通过数据共享学习可以显著降低成本的最优数据汇聚方法。

    

    本文讨论了维护操作中汇聚数据进行共享学习的好处。我们考虑了一组通过先验未知速率相互耦合的泊松退化系统。涉及这些系统的决策问题是高维度马尔可夫决策过程（MDP）。我们提出了一个将这样的MDP分解为二维MDP的分解结果，从而实现结构分析和计算。我们利用这个分解结果证明了数据共享可以相对于不共享时显著降低成本。

    This paper addresses the benefits of pooling data for shared learning in maintenance operations. We consider a set of systems subject to Poisson degradation that are coupled through an a-priori unknown rate. Decision problems involving these systems are high-dimensional Markov decision processes (MDPs). We present a decomposition result that reduces such an MDP to two-dimensional MDPs, enabling structural analyses and computations. We leverage this decomposition to demonstrate that pooling data can lead to significant cost reductions compared to not pooling.
    
[^49]: 测地线模式连通性

    Geodesic Mode Connectivity. (arXiv:2308.12666v1 [cs.LG])

    [http://arxiv.org/abs/2308.12666](http://arxiv.org/abs/2308.12666)

    本文探讨了测地线模式连通性的现象，并提出了一种算法近似测地线，实现了模式连通性。

    

    模式连通性是指训练好的模型之间存在一条低损失路径的现象。我们将这一现象重新解释为信息几何的一部分，其中神经网络被研究为具有曲线几何的参数化分布空间。我们假设这些空间中的最短路径，即测地线，对应于损失景观中的模式连接路径。我们提出了一种近似测地线的算法，并证明其实现了模式连通性。

    Mode connectivity is a phenomenon where trained models are connected by a path of low loss. We reframe this in the context of Information Geometry, where neural networks are studied as spaces of parameterized distributions with curved geometry. We hypothesize that shortest paths in these spaces, known as geodesics, correspond to mode-connecting paths in the loss landscape. We propose an algorithm to approximate geodesics and demonstrate that they achieve mode connectivity.
    
[^50]: 不要望向太阳：对图像分类器的对抗性曝光攻击

    Don't Look into the Sun: Adversarial Solarization Attacks on Image Classifiers. (arXiv:2308.12661v1 [cs.CV])

    [http://arxiv.org/abs/2308.12661](http://arxiv.org/abs/2308.12661)

    通过对图像进行曝光攻击，我们引入了一种简单而有效的方法来评估图像分类器的鲁棒性，并证明该攻击能够显著降低准确性。

    

    评估深度神经网络对于超出分布范围的输入的鲁棒性是至关重要的，特别是在像自动驾驶这样的安全关键领域，但也适用于恶意攻击者可以数字地修改输入以绕过安全保护的安全系统中。然而，设计有效的超出分布测试，在保持准确的标签信息的同时涵盖所有可能的场景，是一项具有挑战性的任务。现有的方法往往需要在攻击的多样性和限制水平之间做出妥协，有时甚至两者兼而有之。作为对图像分类模型更全面的鲁棒性评估的第一步，我们引入了一种基于图像曝光的攻击方法，该方法在概念上简单明了，但又能避免局部范围内的自然图像的全局结构受到损害。通过对多个ImageNet模型进行全面评估，我们证明该攻击能够显著降低准确性，前提是它没有集成到

    Assessing the robustness of deep neural networks against out-of-distribution inputs is crucial, especially in safety-critical domains like autonomous driving, but also in safety systems where malicious actors can digitally alter inputs to circumvent safety guards. However, designing effective out-of-distribution tests that encompass all possible scenarios while preserving accurate label information is a challenging task. Existing methodologies often entail a compromise between variety and constraint levels for attacks and sometimes even both. In a first step towards a more holistic robustness evaluation of image classification models, we introduce an attack method based on image solarization that is conceptually straightforward yet avoids jeopardizing the global structure of natural images independent of the intensity. Through comprehensive evaluations of multiple ImageNet models, we demonstrate the attack's capacity to degrade accuracy significantly, provided it is not integrated into
    
[^51]: APART: 使用有升序奖励和丢弃技术的全组对多样化技能发现

    APART: Diverse Skill Discovery using All Pairs with Ascending Reward and DropouT. (arXiv:2308.12649v1 [cs.LG])

    [http://arxiv.org/abs/2308.12649](http://arxiv.org/abs/2308.12649)

    本文提出了一个名为APART的方法，使用全组对判别器、新颖的内在奖励函数和丢弃技术，在无奖励环境中实现了多样化技能的发现。该方法具有更高的效率，并在简单的网格世界环境中发现了所有可能的技能。

    

    我们研究了在无奖励环境中的多样化技能发现，在简单的网格世界环境中发现所有可能的技能，先前的方法很难成功。这个问题被制定为使用内在奖励和一个经过训练的判别器来相互训练技能以预测给定轨迹的技能。我们的初始解决方案用全组对（all pairs）判别器替换了标准的一对多（softmax）判别器，并结合了一种新颖的内在奖励函数和丢弃（dropout）正则化技术。这种综合方法被命名为APART: 使用有升序奖励和丢弃技术的全组对多样化技能发现。我们证明APART比先前的方法更少样本就能发现网格世界中所有可能的技能。受到APART的实证成功的启发，我们进一步研究了一个更简单的算法，通过改变VIC，重新调整其内在奖励，并调节其softmax温度来实现最大化技能的目标。

    We study diverse skill discovery in reward-free environments, aiming to discover all possible skills in simple grid-world environments where prior methods have struggled to succeed. This problem is formulated as mutual training of skills using an intrinsic reward and a discriminator trained to predict a skill given its trajectory. Our initial solution replaces the standard one-vs-all (softmax) discriminator with a one-vs-one (all pairs) discriminator and combines it with a novel intrinsic reward function and a dropout regularization technique. The combined approach is named APART: Diverse Skill Discovery using All Pairs with Ascending Reward and Dropout. We demonstrate that APART discovers all the possible skills in grid worlds with remarkably fewer samples than previous works. Motivated by the empirical success of APART, we further investigate an even simpler algorithm that achieves maximum skills by altering VIC, rescaling its intrinsic reward, and tuning the temperature of its softm
    
[^52]: GENEA Challenge 2023：单声道和双声道环境中手势生成模型的大规模评估

    The GENEA Challenge 2023: A large scale evaluation of gesture generation models in monadic and dyadic settings. (arXiv:2308.12646v1 [cs.HC])

    [http://arxiv.org/abs/2308.12646](http://arxiv.org/abs/2308.12646)

    GENEA Challenge 2023是一项大规模评估手势生成模型的挑战。参与团队使用相同语音和运动数据集构建了基于语音驱动的手势生成系统，并通过几项用户研究评估了这些系统的人类相似度以及对于发言者和互动者行为的适应性。

    

    本论文报告了GENEA Challenge 2023的情况，参与团队使用相同的语音和运动数据集构建了基于语音驱动的手势生成系统，并进行了共同评估。今年的挑战提供了双人互动的数据，使团队可以根据发言者的语音（文本和音频）以及互动者的语音和动作生成完整的全身动作。我们对12个提交和2个基准模型进行了评估，同时使用了保留的运动捕捉数据进行了几项大规模用户研究。这些研究集中在三个方面：1）动作的人类相似度，2）动作对于发言者自身语音的适应性（同时控制动作的人类相似度），和3）动作对于互动中对话者行为的适应性，使用了一个控制了动作的人类相似度和发言者自身语音的设置。我们发现挑战的提交之间的人类相似度差异很大。

    This paper reports on the GENEA Challenge 2023, in which participating teams built speech-driven gesture-generation systems using the same speech and motion dataset, followed by a joint evaluation. This year's challenge provided data on both sides of a dyadic interaction, allowing teams to generate full-body motion for an agent given its speech (text and audio) and the speech and motion of the interlocutor. We evaluated 12 submissions and 2 baselines together with held-out motion-capture data in several large-scale user studies. The studies focused on three aspects: 1) the human-likeness of the motion, 2) the appropriateness of the motion for the agent's own speech whilst controlling for the human-likeness of the motion, and 3) the appropriateness of the motion for the behaviour of the interlocutor in the interaction, using a setup that controls for both the human-likeness of the motion and the agent's own speech. We found a large span in human-likeness between challenge submissions, w
    
[^53]: 面向分层区域变压器的多实例学习

    Towards Hierarchical Regional Transformer-based Multiple Instance Learning. (arXiv:2308.12634v1 [cs.CV])

    [http://arxiv.org/abs/2308.12634](http://arxiv.org/abs/2308.12634)

    本文提出了一种基于变压器的多实例学习方法，通过使用区域自注意力机制，融合区域补丁信息以得出滑片级别预测，并通过堆叠区域聚合来分层处理特征。此外，引入了一种方法来聚焦于高关注区域，从而提高预测准确性。这种方法在组织病理学图像分类任务上表现出了显著的性能改进，并为进一步研究提供了有希望的方向。

    

    在数字病理学和精确医学中，使用深度多实例学习模型对巨像素组织病理学图像进行分类已成为一项关键任务。本文提出了一种基于变压器的多实例学习方法，该方法用区域性的、受到视觉变压器启发的自注意力机制替代了传统的学习注意力机制。我们提出了一种融合区域补丁信息以得出滑片级别预测的方法，并展示了如何堆叠这种区域聚合以分层地处理不同距离水平上的特征。为了提高预测准确性，特别是对于具有小的局部形态特征的数据集，我们引入了一种方法，在推理期间将图像处理集中在高关注区域。我们的方法能够显著改善两个组织病理学数据集的性能，并指向进一步研究的有希望的方向。

    The classification of gigapixel histopathology images with deep multiple instance learning models has become a critical task in digital pathology and precision medicine. In this work, we propose a Transformer-based multiple instance learning approach that replaces the traditional learned attention mechanism with a regional, Vision Transformer inspired self-attention mechanism. We present a method that fuses regional patch information to derive slide-level predictions and show how this regional aggregation can be stacked to hierarchically process features on different distance levels. To increase predictive accuracy, especially for datasets with small, local morphological features, we introduce a method to focus the image processing on high attention regions during inference. Our approach is able to significantly improve performance over the baseline on two histopathology datasets and points towards promising directions for further research.
    
[^54]: 重建声速测井曲线的机器学习模型的不确定性和可解释性分析

    Uncertainty and Explainable Analysis of Machine Learning Model for Reconstruction of Sonic Slowness Logs. (arXiv:2308.12625v1 [cs.LG])

    [http://arxiv.org/abs/2308.12625](http://arxiv.org/abs/2308.12625)

    本论文提出了一种利用机器学习模型重建声速测井曲线的方法，并通过NGBoost算法构建了一个能够预测结果及其不确定性的集成学习模型，同时结合了SHAP方法进行模型的可解释性分析。实验结果表明，NGBoost模型在测试中表现良好。

    

    测井曲线对于油气田来说是宝贵的信息，它们帮助确定井孔周围地层的岩性以及地下油气储层的位置和储量。然而，在水平或旧井中经常缺少重要的测井曲线，这给现场应用带来了挑战。本文利用SPWLA的2020年机器学习竞赛的数据，旨在利用同一井眼中的其他测井曲线来预测缺失的压缩波慢度和剪切波慢度测井曲线。我们采用NGBoost算法构建了一个集成学习模型，可以预测结果及其不确定性。此外，我们结合了SHAP方法来研究机器学习模型的可解释性。我们将NGBosst模型与其他四种常用的集成学习方法进行了性能比较，包括随机森林、GBDT、XGBoost和LightGBM。结果表明，NGBoost模型在测试中表现良好。

    Logs are valuable information for oil and gas fields as they help to determine the lithology of the formations surrounding the borehole and the location and reserves of subsurface oil and gas reservoirs. However, important logs are often missing in horizontal or old wells, which poses a challenge in field applications. In this paper, we utilize data from the 2020 machine learning competition of the SPWLA, which aims to predict the missing compressional wave slowness and shear wave slowness logs using other logs in the same borehole. We employ the NGBoost algorithm to construct an Ensemble Learning model that can predicate the results as well as their uncertainty. Furthermore, we combine the SHAP method to investigate the interpretability of the machine learning model. We compare the performance of the NGBosst model with four other commonly used Ensemble Learning methods, including Random Forest, GBDT, XGBoost, LightGBM. The results show that the NGBoost model performs well in the testi
    
[^55]: 尝试更简单-改进主成分分析在基于日志的异常检测中的评估

    Try with Simpler -- An Evaluation of Improved Principal Component Analysis in Log-based Anomaly Detection. (arXiv:2308.12612v1 [cs.LG])

    [http://arxiv.org/abs/2308.12612](http://arxiv.org/abs/2308.12612)

    本论文通过改进传统的主成分分析方法，优化了基于日志的异常检测技术，以提高其效果，从而使其与深度学习方法相媲美。

    

    深度学习的快速发展激发了对增强基于日志的异常检测的兴趣。这种方法旨在从日志事件（日志消息模板）中提取意义，并开发先进的深度学习模型以进行异常检测。然而，这些深度学习方法面临着依赖训练数据、标签和计算资源的挑战，因为模型复杂性较高。相比之下，传统的机器学习和数据挖掘技术虽然不太依赖数据，更高效，但比深度学习方法效果较差。为了使基于日志的异常检测更实用，目标是增强传统技术以达到深度学习方法的效果。以前在不同领域（链接Stack Overflow上的问题）的研究表明，经过优化的传统技术可以与最先进的深度学习方法相媲美。受到这一概念的启发，我们进行了一项经验研究。我们通过整合轻量级正则化方法来优化无监督的主成分分析（PCA），一种传统技术。

    The rapid growth of deep learning (DL) has spurred interest in enhancing log-based anomaly detection. This approach aims to extract meaning from log events (log message templates) and develop advanced DL models for anomaly detection. However, these DL methods face challenges like heavy reliance on training data, labels, and computational resources due to model complexity. In contrast, traditional machine learning and data mining techniques are less data-dependent and more efficient but less effective than DL. To make log-based anomaly detection more practical, the goal is to enhance traditional techniques to match DL's effectiveness. Previous research in a different domain (linking questions on Stack Overflow) suggests that optimized traditional techniques can rival state-of-the-art DL methods. Drawing inspiration from this concept, we conducted an empirical study. We optimized the unsupervised PCA (Principal Component Analysis), a traditional technique, by incorporating lightweight se
    
[^56]: 一个用于向电信用户提供的贪心方法

    A Greedy Approach for Offering to Telecom Subscribers. (arXiv:2308.12606v1 [stat.ML])

    [http://arxiv.org/abs/2308.12606](http://arxiv.org/abs/2308.12606)

    本论文提出了一个用于解决套餐优化问题的新颖组合算法，该算法针对电信运营商在选择激励套餐和目标用户时面临的困难进行了解决。

    

    客户保留或减少流失是电信运营商面临的一项具有挑战性的任务。其中一个有效的方法是向用户提供一些有吸引力的激励措施或附加服务或金钱，以保持他们的参与并确保他们在运营商的网络中停留更长时间。通常，运营商会分配一定金额的预算来进行推广活动。这项活动的困难之处在于从庞大的订户群体中选择一组客户，并决定应该向个体提供多少金额，以实现运营商的目标。选择订户和选择提供给被选定订户的套餐可能有多个目标（例如，最大化收入，最小化流失数量）。除了金钱利益，套餐还可以包括额外的数据、短信、手机热点共享等等。这个问题被称为套餐优化。在这篇论文中，我们提出了一种新颖的组合算法来解决套餐优化问题。

    Customer retention or churn prevention is a challenging task of a telecom operator. One of the effective approaches is to offer some attractive incentive or additional services or money to the subscribers for keeping them engaged and make sure they stay in the operator's network for longer time. Often, operators allocate certain amount of monetary budget to carry out the offer campaign. The difficult part of this campaign is the selection of a set of customers from a large subscriber-base and deciding the amount that should be offered to an individual so that operator's objective is achieved. There may be multiple objectives (e.g., maximizing revenue, minimizing number of churns) for selection of subscriber and selection of an offer to the selected subscriber. Apart from monetary benefit, offers may include additional data, SMS, hots-spot tethering, and many more. This problem is known as offer optimization. In this paper, we propose a novel combinatorial algorithm for solving offer op
    
[^57]: 利用时频适配器进行音乐音频增强

    Exploiting Time-Frequency Conformers for Music Audio Enhancement. (arXiv:2308.12599v1 [cs.SD])

    [http://arxiv.org/abs/2308.12599](http://arxiv.org/abs/2308.12599)

    这项研究提出了一种基于Conformer架构的音频增强系统，通过探索其注意机制并测试性能，实现了在单音乐音频增强方面的最先进性能。

    

    随着互联网上视频平台的普及，使用移动设备录制音乐演出已经成为常见现象。然而，这些录音往往受到噪音和混响等问题的影响，对听众的体验产生负面影响。因此，音乐音频增强（以下简称音频增强）的需求不断增长，旨在将受损的音频录音转化为高品质的音乐，以提升听觉体验。为了解决这个问题，我们提出了一种基于Conformer架构的音频增强系统，该架构在语音增强任务中表现出色。我们的方法探索了Conformer的注意机制，并测试其在音频增强任务中的性能，以找到最佳的方法。实验结果表明，我们提出的模型在单音乐音频增强方面达到了最先进的性能。

    With the proliferation of video platforms on the internet, recording musical performances by mobile devices has become commonplace. However, these recordings often suffer from degradation such as noise and reverberation, which negatively impact the listening experience. Consequently, the necessity for music audio enhancement (referred to as music enhancement from this point onward), involving the transformation of degraded audio recordings into pristine high-quality music, has surged to augment the auditory experience. To address this issue, we propose a music enhancement system based on the Conformer architecture that has demonstrated outstanding performance in speech enhancement tasks. Our approach explores the attention mechanisms of the Conformer and examines their performance to discover the best approach for the music enhancement task. Our experimental results show that our proposed model achieves state-of-the-art performance on single-stem music enhancement. Furthermore, our sys
    
[^58]: 没有连续吸引子的持久学习信号和工作记忆

    Persistent learning signals and working memory without continuous attractors. (arXiv:2308.12585v1 [q-bio.NC])

    [http://arxiv.org/abs/2308.12585](http://arxiv.org/abs/2308.12585)

    本研究发现，在神经动力系统中，除了广泛研究的连续吸引子外，周期性和准周期性吸引子也可以支持学习时间关系。相比于连续吸引子，准周期性吸引子更适合学习产生时间结构化行为，并且对于人工学习系统的设计和生物神经动力学的可观测特征都有重要意义。

    

    稳定吸引子结构的神经动力系统，如点吸引子和连续吸引子，被假设为支持需要工作记忆的有意义时间行为的基础。然而，工作记忆可能不支持适应环境时间结构变化所需的有用学习信号。我们展示了除了广泛涉及的连续吸引子外，周期性和准周期性吸引子也可以支持学习任意长的时间关系。与受到微调问题困扰的连续吸引子不同，较少被探索的准周期性吸引子有独特的学习产生时间结构化行为的能力。我们的理论对于人工学习系统的设计具有广泛的影响，并对能够支持时间关联学习和工作记忆的生物神经动力学的可观测特征提出预测。

    Neural dynamical systems with stable attractor structures, such as point attractors and continuous attractors, are hypothesized to underlie meaningful temporal behavior that requires working memory. However, working memory may not support useful learning signals necessary to adapt to changes in the temporal structure of the environment. We show that in addition to the continuous attractors that are widely implicated, periodic and quasi-periodic attractors can also support learning arbitrarily long temporal relationships. Unlike the continuous attractors that suffer from the fine-tuning problem, the less explored quasi-periodic attractors are uniquely qualified for learning to produce temporally structured behavior. Our theory has broad implications for the design of artificial learning systems and makes predictions about observable signatures of biological neural dynamics that can support temporal dependence learning and working memory. Based on our theory, we developed a new initializ
    
[^59]: LORD: 利用未知数据进行开放式识别的方法

    LORD: Leveraging Open-Set Recognition with Unknown Data. (arXiv:2308.12584v1 [cs.CV])

    [http://arxiv.org/abs/2308.12584](http://arxiv.org/abs/2308.12584)

    本文提出了一种名为LORD的框架，通过利用未知数据进行开放式识别。LORD在分类器训练过程中明确地建模开放空间，并通过三种模型无关的训练策略实现了对未知数据识别能力的改进。

    

    处理完全未知数据对于任何部署的分类器都是一种挑战。分类模型通常是在一个静态预定义的数据集上进行训练，对于未分配的开放特征空间一无所知。因此，在推断过程中，它们很难处理超出分布范围的数据。解决这个任务的一种方法是在类别级别上进行开放式识别（OSR）。然而，大多数OSR方法本质上是有限的，因为它们训练的是封闭集分类器，并且只调整了对OSR的下游预测。本文提出了一个名为LORD的框架，通过利用未知数据来实现开放式识别。LORD在分类器训练过程中明确地建模开放空间，并对这种方法进行了系统评估。我们确定了三种模型无关的训练策略，利用了背景数据，并将其应用到已经建立的分类器上。由于LORD的广泛评估协议，我们始终能够展示对未知数据的改进识别能力。这些基准有助于...

    Handling entirely unknown data is a challenge for any deployed classifier. Classification models are typically trained on a static pre-defined dataset and are kept in the dark for the open unassigned feature space. As a result, they struggle to deal with out-of-distribution data during inference. Addressing this task on the class-level is termed open-set recognition (OSR). However, most OSR methods are inherently limited, as they train closed-set classifiers and only adapt the downstream predictions to OSR. This work presents LORD, a framework to Leverage Open-set Recognition by exploiting unknown Data. LORD explicitly models open space during classifier training and provides a systematic evaluation for such approaches. We identify three model-agnostic training strategies that exploit background data and applied them to well-established classifiers. Due to LORD's extensive evaluation protocol, we consistently demonstrate improved recognition of unknown data. The benchmarks facilitate i
    
[^60]: 一种Huber损失最小化方法用于拜占庭鲁棒的联邦学习

    A Huber Loss Minimization Approach to Byzantine Robust Federated Learning. (arXiv:2308.12581v1 [cs.LG])

    [http://arxiv.org/abs/2308.12581](http://arxiv.org/abs/2308.12581)

    本文介绍了一种基于Huber损失最小化的新型聚合器用于拜占庭鲁棒的联邦学习。在独立同分布假设下，该方法具有与现有方法相比的优势，并对非i.i.d数据进行了扩展分析。

    

    联邦学习系统容易受到对抗攻击。为了应对这个问题，我们引入了一种基于Huber损失最小化的新型聚合器，并提供了全面的理论分析。在独立同分布（i.i.d）假设下，与现有方法相比，我们的方法具有几个优势。首先，它对于被攻击客户端比率$\epsilon$具有最优的依赖关系。其次，我们的方法不需要对$\epsilon$有精确的知识。第三，它允许不同的客户端具有不均等的数据大小。然后，我们将分析扩展到包括非i.i.d数据，这意味着客户端具有略有不同的分布。

    Federated learning systems are susceptible to adversarial attacks. To combat this, we introduce a novel aggregator based on Huber loss minimization, and provide a comprehensive theoretical analysis. Under independent and identically distributed (i.i.d) assumption, our approach has several advantages compared to existing methods. Firstly, it has optimal dependence on $\epsilon$, which stands for the ratio of attacked clients. Secondly, our approach does not need precise knowledge of $\epsilon$. Thirdly, it allows different clients to have unequal data sizes. We then broaden our analysis to include non-i.i.d data, such that clients have slightly different distributions.
    
[^61]: 用于细粒度ICU患者相似性分析和风险预测的超图卷积网络

    Hypergraph Convolutional Networks for Fine-grained ICU Patient Similarity Analysis and Risk Prediction. (arXiv:2308.12575v1 [cs.LG])

    [http://arxiv.org/abs/2308.12575](http://arxiv.org/abs/2308.12575)

    本研究提出了一种利用超图卷积网络进行ICU患者相似性分析和风险预测的新方法，可以捕捉隐藏的特征结构，并应用于个性化的死亡风险预测。

    

    重症监护病房（ICU）是医院中最重要的部分之一，用于收治重症患者并提供连续监测和治疗。已经尝试了各种患者预测方法来辅助医疗专业人员进行临床决策。现有方法侧重于使用深度神经网络来衡量患者之间的相似性，以捕捉隐藏的特征结构。然而，更高阶的关系被忽视了，例如患者特征（如诊断代码）以及它们对下游临床预测的因果影响。本文提出了一种新颖的超图卷积网络，允许在超图中表示诊断代码之间的非成对关系，以捕捉隐藏的特征结构，从而可以计算细粒度患者相似性，用于个性化的死亡风险预测。在公开可用的eICU协作研究数据库上进行评估。

    The Intensive Care Unit (ICU) is one of the most important parts of a hospital, which admits critically ill patients and provides continuous monitoring and treatment. Various patient outcome prediction methods have been attempted to assist healthcare professionals in clinical decision-making. Existing methods focus on measuring the similarity between patients using deep neural networks to capture the hidden feature structures. However, the higher-order relationships are ignored, such as patient characteristics (e.g., diagnosis codes) and their causal effects on downstream clinical predictions.  In this paper, we propose a novel Hypergraph Convolutional Network that allows the representation of non-pairwise relationships among diagnosis codes in a hypergraph to capture the hidden feature structures so that fine-grained patient similarity can be calculated for personalized mortality risk prediction. Evaluation using a publicly available eICU Collaborative Research Database indicates that
    
[^62]: 条件核模仿学习在连续状态环境中的应用

    Conditional Kernel Imitation Learning for Continuous State Environments. (arXiv:2308.12573v1 [cs.LG])

    [http://arxiv.org/abs/2308.12573](http://arxiv.org/abs/2308.12573)

    本研究以连续状态空间环境为基础，仅凭观察到的行为进行仿真学习，无需访问转移动力学信息、奖励结构，或者任何额外的交互。

    

    仿真学习（IL）是在更广泛的强化学习（RL）方法中的重要范例。与大多数RL不同，它不假设回馈奖励的可用性。奖励推断和塑形已知是困难且容易出错的方法，特别是当演示数据来自人类专家时。传统方法如行为克隆和逆向强化学习对估计误差非常敏感，这在连续状态空间问题中尤为严重。与此同时，最先进的IL算法将行为策略学习问题转换为分布匹配问题，通常需要额外的在线交互数据才能发挥作用。本文考虑了在连续状态空间环境中仅基于观察到的行为进行仿真学习的问题，不需要访问转移动力学信息、奖励结构，或者最重要的是不需要与环境进行任何额外交互。

    Imitation Learning (IL) is an important paradigm within the broader reinforcement learning (RL) methodology. Unlike most of RL, it does not assume availability of reward-feedback. Reward inference and shaping are known to be difficult and error-prone methods particularly when the demonstration data comes from human experts. Classical methods such as behavioral cloning and inverse reinforcement learning are highly sensitive to estimation errors, a problem that is particularly acute in continuous state space problems. Meanwhile, state-of-the-art IL algorithms convert behavioral policy learning problems into distribution-matching problems which often require additional online interaction data to be effective. In this paper, we consider the problem of imitation learning in continuous state space environments based solely on observed behavior, without access to transition dynamics information, reward structure, or, most importantly, any additional interactions with the environment. Our appr
    
[^63]: 针对带有污染数据的多变量时间序列异常检测：对生理信号的应用

    Multivariate Time-Series Anomaly Detection with Contaminated Data: Application to Physiological Signals. (arXiv:2308.12563v1 [cs.LG])

    [http://arxiv.org/abs/2308.12563](http://arxiv.org/abs/2308.12563)

    这个论文介绍了一种针对带有污染数据的多变量时间序列异常检测的新方法，通过去污和变量依赖建模实现了无监督的异常检测，对于实际场景中的异常检测具有重要意义。

    

    主流无监督异常检测算法通常在学术数据集中表现出色，但由于受到控制实验条件下的清洁训练数据的限制，它们在实际场景下的性能受到了限制。然而，在实际异常检测中，训练数据包含噪声的挑战经常被忽视。本研究在感知时间序列异常检测（TSAD）中深入研究了标签级噪声的领域。本文提出了一种新颖实用的端到端无监督TSAD方法，该方法处理训练数据中包含异常的情况下。该方法称为TSAD-C，其在训练阶段不需要访问异常标签。TSAD-C包括三个模块：一个去污器用于纠正训练数据中存在的异常（也称为噪声），一个变量依赖建模模块用于捕捉去污后数据中的长期内部和跨变量依赖关系，可以视为替代性的异常性度量方法。

    Mainstream unsupervised anomaly detection algorithms often excel in academic datasets, yet their real-world performance is restricted due to the controlled experimental conditions involving clean training data. Addressing the challenge of training with noise, a prevalent issue in practical anomaly detection, is frequently overlooked. In a pioneering endeavor, this study delves into the realm of label-level noise within sensory time-series anomaly detection (TSAD). This paper presents a novel and practical end-to-end unsupervised TSAD when the training data are contaminated with anomalies. The introduced approach, called TSAD-C, is devoid of access to abnormality labels during the training phase. TSAD-C encompasses three modules: a Decontaminator to rectify the abnormalities (aka noise) present in the training data, a Variable Dependency Modeling module to capture both long-term intra- and inter-variable dependencies within the decontaminated data that can be considered as a surrogate o
    
[^64]: 利用大型语言和多模态模型进行可解释预测的变分信息追求

    Variational Information Pursuit with Large Language and Multimodal Models for Interpretable Predictions. (arXiv:2308.12562v1 [cs.LG])

    [http://arxiv.org/abs/2308.12562](http://arxiv.org/abs/2308.12562)

    本文提出了一种利用大型语言和多模态模型的变分信息追求(V-IP)框架，通过顺序选择任务相关的可解释查询，实现可解释预测。为了解决数据标注的限制，引入了基础模型(FMs)，使用大型语言模型(LLMs)生成候选可解释概念集，并使用大型多模态模型注释每个数据样本。此方法适用于大规模任务。

    

    变分信息追求(V-IP)是一个通过顺序选择与任务相关、用户定义和可解释的数据查询来设计可解释预测的框架。虽然这使得预测模型具有内置的可解释性，但将V-IP应用于任何任务都需要具有由领域专家进行密集概念标注的数据样本，限制了V-IP在手动数据注释可行的小规模任务中的应用。本文中，我们通过引入基础模型(FMs)来扩展V-IP框架，以解决这个限制。具体而言，我们使用了一个两步流程，首先利用大型语言模型(LLMs)生成足够大的候选任务相关可解释概念集，然后利用大型多模态模型通过与生成的概念集中的每个概念的语义相似性对每个数据样本进行注释。虽然还有其他可解释设计框架，比如Concept Bot，但这些框架不适合处理大规模任务。

    Variational Information Pursuit (V-IP) is a framework for making interpretable predictions by design by sequentially selecting a short chain of task-relevant, user-defined and interpretable queries about the data that are most informative for the task. While this allows for built-in interpretability in predictive models, applying V-IP to any task requires data samples with dense concept-labeling by domain experts, limiting the application of V-IP to small-scale tasks where manual data annotation is feasible. In this work, we extend the V-IP framework with Foundational Models (FMs) to address this limitation. More specifically, we use a two-step process, by first leveraging Large Language Models (LLMs) to generate a sufficiently large candidate set of task-relevant interpretable concepts, then using Large Multimodal Models to annotate each data sample by semantic similarity with each concept in the generated concept set. While other interpretable-by-design frameworks such as Concept Bot
    
[^65]: 基于深度强化学习的跨社区能量交互优化调度

    Deep Reinforcement Learning-driven Cross-Community Energy Interaction Optimal Scheduling. (arXiv:2308.12554v1 [eess.SY])

    [http://arxiv.org/abs/2308.12554](http://arxiv.org/abs/2308.12554)

    本文提出了一种基于深度强化学习的跨社区能量交互优化调度模型，通过学习不同社区的负载特性，并基于该知识做出决策，实现综合能量系统的整体优化和调度。

    

    为了在不确定的条件下协调各个社区之间的能量交互和多能源子系统之间的能量转换，并实现综合能量系统的整体优化和调度，本文提出了一种综合调度模型，利用多智能体深度强化学习算法学习不同社区的负载特性，并基于该知识做出决策。在该模型中，综合能量系统的调度问题被转化为马尔科夫决策过程，并使用数据驱动的深度强化学习算法解决，从而避免了对多社区和多能源子系统之间复杂能量耦合关系的建模需求。模拟结果表明，所提出的方法能够有效捕捉不同社区的负载特性，并利用其互补特性进行协调。

    In order to coordinate energy interactions among various communities and energy conversions among multi-energy subsystems within the multi-community integrated energy system under uncertain conditions, and achieve overall optimization and scheduling of the comprehensive energy system, this paper proposes a comprehensive scheduling model that utilizes a multi-agent deep reinforcement learning algorithm to learn load characteristics of different communities and make decisions based on this knowledge. In this model, the scheduling problem of the integrated energy system is transformed into a Markov decision process and solved using a data-driven deep reinforcement learning algorithm, which avoids the need for modeling complex energy coupling relationships between multi-communities and multi-energy subsystems. The simulation results show that the proposed method effectively captures the load characteristics of different communities and utilizes their complementary features to coordinate re
    
[^66]: 不要怪数据集转移！梯度和交叉熵导致了捷径学习

    Don't blame Dataset Shift! Shortcut Learning due to Gradients and Cross Entropy. (arXiv:2308.12553v1 [cs.LG])

    [http://arxiv.org/abs/2308.12553](http://arxiv.org/abs/2308.12553)

    默认-ERM模型通过最大化间隔来优化训练，导致模型更多依赖于捷径而非稳定特征，这对感知任务来说是不合适的。

    

    常见对于捷径学习的解释认为捷径在训练分布下改善了预测结果，但在测试分布下却没有改善。因此，通过典型的基于梯度的交叉熵优化训练的模型（我们称其为默认-ERM）利用了这个捷径。然而，即使在训练分布中稳定特征决定了标签而捷径并没有提供额外的信息，比如在感知任务中，默认-ERM仍然表现出了捷径学习。为什么这样的解决方案更受青睐，当可以单独使用稳定特征将默认-ERM的损失驱动为零时？通过研究线性感知任务，我们展示了默认-ERM对于最大化间隔的偏好导致了更多依赖于捷径而非稳定特征的模型，即使没有过度参数化。这一发现表明，默认-ERM的隐性归纳偏好即最大间隔对于感知任务是不合适的。相反，我们提出了一种适合感知任务的归纳偏好。

    Common explanations for shortcut learning assume that the shortcut improves prediction under the training distribution but not in the test distribution. Thus, models trained via the typical gradient-based optimization of cross-entropy, which we call default-ERM, utilize the shortcut. However, even when the stable feature determines the label in the training distribution and the shortcut does not provide any additional information, like in perception tasks, default-ERM still exhibits shortcut learning. Why are such solutions preferred when the loss for default-ERM can be driven to zero using the stable feature alone? By studying a linear perception task, we show that default-ERM's preference for maximizing the margin leads to models that depend more on the shortcut than the stable feature, even without overparameterization. This insight suggests that default-ERM's implicit inductive bias towards max-margin is unsuitable for perception tasks. Instead, we develop an inductive bias toward 
    
[^67]: 一种用于嘈杂时间序列学习的协同训练方法

    A Co-training Approach for Noisy Time Series Learning. (arXiv:2308.12551v1 [cs.LG])

    [http://arxiv.org/abs/2308.12551](http://arxiv.org/abs/2308.12551)

    本研究提出了一种用于嘈杂时间序列学习的协同训练方法，通过多视图学习来减轻数据噪声和损坏的影响，并在多个时间序列基准上超越现有方法。

    

    本文致力于鲁棒的时间序列表示学习。我们的假设是现实世界的时间序列是嘈杂的，并且来自同一时间序列的不同观点的互补信息在分析嘈杂输入时发挥着重要作用。基于此，我们通过两个不同的编码器为输入时间序列创建了两个视图。我们通过协同训练基于对比学习的方式来迭代学习编码器。我们的实验表明，这种协同训练方法可以显著提高性能。特别是，通过利用来自不同视图的互补信息，我们提出的TS-CoT方法可以减轻数据噪声和损坏的影响。在无监督和半监督设置下对四个时间序列基准进行的实证评估表明，TS-CoT优于现有方法。此外，通过微调，TS-CoT学习到的表示可以很好地适用于下游任务。

    In this work, we focus on robust time series representation learning. Our assumption is that real-world time series is noisy and complementary information from different views of the same time series plays an important role while analyzing noisy input. Based on this, we create two views for the input time series through two different encoders. We conduct co-training based contrastive learning iteratively to learn the encoders. Our experiments demonstrate that this co-training approach leads to a significant improvement in performance. Especially, by leveraging the complementary information from different views, our proposed TS-CoT method can mitigate the impact of data noise and corruption. Empirical evaluations on four time series benchmarks in unsupervised and semi-supervised settings reveal that TS-CoT outperforms existing methods. Furthermore, the representations learned by TS-CoT can transfer well to downstream tasks through fine-tuning.
    
[^68]: CALM: 一种用于全面评估语言模型偏见的多任务基准数据集

    CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias. (arXiv:2308.12539v1 [cs.CL])

    [http://arxiv.org/abs/2308.12539](http://arxiv.org/abs/2308.12539)

    CALM是一个用于量化语言模型偏见的多任务基准数据集，相比先前数据集更加多样和可靠，能更好地捕捉评估模型偏见所需的语言变化的广度。

    

    随着语言模型（LMs）的不断增强，量化和比较它们在社会和人口学偏见方面的能力以及潜在的危害变得越来越重要。先前的偏见测量数据集对于人工设计模板的扰动敏感，因此不可靠。为了保证可靠性，我们引入了全面评估语言模型偏见（CALM）的基准数据集，用于量化LMs在三个任务上的偏见。我们整合了来自不同领域（如维基百科和新闻文章）的16个现有数据集，过滤出224个模板，并构建了一个包含78,400个示例的数据集。我们通过平均语义相似性和模板长度的变异程度等指标，比较CALM与先前数据集的多样性，并测试其对细微扰动的敏感性。我们展示了我们的数据集相对于先前数据集更加多样和可靠，因此能更好地捕捉评估模型偏见所需的语言变化的广度。我们评估了20个大型语言模型的偏见。

    As language models (LMs) become increasingly powerful, it is important to quantify and compare them for sociodemographic bias with potential for harm. Prior bias measurement datasets are sensitive to perturbations in their manually designed templates, therefore unreliable. To achieve reliability, we introduce the Comprehensive Assessment of Language Model bias (CALM), a benchmark dataset to quantify bias in LMs across three tasks. We integrate 16 existing datasets across different domains, such as Wikipedia and news articles, to filter 224 templates from which we construct a dataset of 78,400 examples. We compare the diversity of CALM with prior datasets on metrics such as average semantic similarity, and variation in template length, and test the sensitivity to small perturbations. We show that our dataset is more diverse and reliable than previous datasets, thus better capture the breadth of linguistic variation required to reliably evaluate model bias. We evaluate 20 large language 
    
[^69]: FedSoL: 在联邦学习中解决全局对齐和本地一般性的问题

    FedSoL: Bridging Global Alignment and Local Generality in Federated Learning. (arXiv:2308.12532v1 [cs.LG])

    [http://arxiv.org/abs/2308.12532](http://arxiv.org/abs/2308.12532)

    FedSoL提出了一种联邦学习的方法，该方法旨在解决数据分布不均匀导致性能下降的问题。它通过平衡全局对齐和本地一般性来改善FL的学习效果。

    

    联邦学习(Federated Learning, FL)通过聚合来自个体客户端的本地训练模型来构建全局模型。虽然FL可以在保护数据隐私的情况下学习模型，但当客户端数据分布不均匀时，常常导致性能下降。许多先前的FL算法通过引入各种近似约束来解决这个问题。这些约束旨在通过限制局部学习与全局目标的偏离来促进全局对齐。然而，它们本质上通过干扰原始的局部目标而限制了局部学习。最近，出现了一种替代方法来改善本地学习的一般性。通过在平滑的损失空间中获得本地模型，这种方法减轻了客户端不同本地目标之间的冲突。然而，它不能确保稳定的全局对齐，因为本地学习不考虑全局目标。在本研究中，我们提出了联邦学习的稳定性(FedSoL)方法来在FL中解决全局对齐和本地一般性的问题。

    Federated Learning (FL) aggregates locally trained models from individual clients to construct a global model. While FL enables learning a model with data privacy, it often suffers from significant performance degradation when client data distributions are heterogeneous. Many previous FL algorithms have addressed this issue by introducing various proximal restrictions. These restrictions aim to encourage global alignment by constraining the deviation of local learning from the global objective. However, they inherently limit local learning by interfering with the original local objectives. Recently, an alternative approach has emerged to improve local learning generality. By obtaining local models within a smooth loss landscape, this approach mitigates conflicts among different local objectives of the clients. Yet, it does not ensure stable global alignment, as local learning does not take the global objective into account. In this study, we propose Federated Stability on Learning (Fed
    
[^70]: SieveNet：选择基于点的特征用于网状网络

    SieveNet: Selecting Point-Based Features for Mesh Networks. (arXiv:2308.12530v1 [cs.CV])

    [http://arxiv.org/abs/2308.12530](http://arxiv.org/abs/2308.12530)

    SieveNet是一种新的网状网络范式，结合了规则拓扑和准确几何信息，通过重网状和点采样的方法提取特征，消除了对手工特征工程的需求，并且可以应用于3D计算机视觉和图形领域。

    

    网状在三维计算机视觉和图形中得到广泛应用，但它们的不规则拓扑结构给现有神经网络架构的应用带来了挑战。最新的网状神经网络的进展转向了重网状和推动了仅将原始网状作为输入的先驱方法的边界。虽然重网状提供了一种规则的拓扑结构，从而显著促进了网状网络架构的设计，但是从这种重网状代理中提取的特征可能会难以准确保留底层几何性质，从而限制了后续神经网络的容量。为了解决这个问题，我们提出了SieveNet，一种同时考虑规则拓扑和准确几何的新范式。具体而言，该方法利用了从重网状中提取得到的结构化网状拓扑以及原始网状表面上基于畸变感知的点采样得到的准确几何信息。此外，我们的方法消除了对手工特征工程的需求，并且可以利用。

    Meshes are widely used in 3D computer vision and graphics, but their irregular topology poses challenges in applying them to existing neural network architectures. Recent advances in mesh neural networks turn to remeshing and push the boundary of pioneer methods that solely take the raw meshes as input. Although the remeshing offers a regular topology that significantly facilitates the design of mesh network architectures, features extracted from such remeshed proxies may struggle to retain the underlying geometry faithfully, limiting the subsequent neural network's capacity. To address this issue, we propose SieveNet, a novel paradigm that takes into account both the regular topology and the exact geometry. Specifically, this method utilizes structured mesh topology from remeshing and accurate geometric information from distortion-aware point sampling on the surface of the original mesh. Furthermore, our method eliminates the need for hand-crafted feature engineering and can leverage 
    
[^71]: UNISOUND系统对VoxCeleb Speaker Recognition Challenge 2023的贡献

    UNISOUND System for VoxCeleb Speaker Recognition Challenge 2023. (arXiv:2308.12526v1 [eess.AS])

    [http://arxiv.org/abs/2308.12526](http://arxiv.org/abs/2308.12526)

    UNISOUND团队在VoxCeleb Speaker Recognition Challenge 2023中提交的系统通过一致性感知的得分校准方法和大规模ResNet和RepVGG架构的使用，在Track1中获得第一名，在Track2中获得第二名，并实现了较低的minDCF和EER。

    

    本报告描述了UNISOUND在VoxCeleb Speaker Recognition Challenge 2023（VoxSRC 2023）的Track1和Track2中的提交。我们在Track 1和Track 2上提交了相同的系统，该系统仅使用VoxCeleb2-dev进行训练。我们为挑战任务开发了大规模ResNet和RepVGG架构。我们提出了一种一致性感知的得分校准方法，通过一致性测量因子（CMF）利用声音特征的稳定性来提高相似性得分的稳定性。CMF使我们在该挑战中获得了巨大的性能提升。我们最终的系统是六个模型的融合，在VoxSRC 2023的Track1中获得第一名，在Track2中获得第二名。我们的提交的minDCF为0.0855，EER为1.5880%。

    This report describes the UNISOUND submission for Track1 and Track2 of VoxCeleb Speaker Recognition Challenge 2023 (VoxSRC 2023). We submit the same system on Track 1 and Track 2, which is trained with only VoxCeleb2-dev. Large-scale ResNet and RepVGG architectures are developed for the challenge. We propose a consistency-aware score calibration method, which leverages the stability of audio voiceprints in similarity score by a Consistency Measure Factor (CMF). CMF brings a huge performance boost in this challenge. Our final system is a fusion of six models and achieves the first place in Track 1 and second place in Track 2 of VoxSRC 2023. The minDCF of our submission is 0.0855 and the EER is 1.5880%.
    
[^72]: 不仅仅奖励，还有约束：用于腿式机器人运动的应用

    Not Only Rewards But Also Constraints: Applications on Legged Robot Locomotion. (arXiv:2308.12517v1 [cs.RO])

    [http://arxiv.org/abs/2308.12517](http://arxiv.org/abs/2308.12517)

    本文提出了一种新的强化学习框架，为复杂机器人系统训练神经网络控制器。该框架引入了奖励和约束的概念，通过设计高效的策略优化算法来处理约束，以减少计算开销。通过应用于不同腿式机器人的运动控制器训练中，展示了该框架的有效性。

    

    早期的一些研究通过设计神经网络控制器并使用无模型强化学习来训练，展示了复杂机器人系统中令人印象深刻的控制性能。然而，这些具有自然动作风格和高任务性能的出色控制器是通过进行大量奖励工程而开发的，该过程非常费时费力，需要设计大量奖励项并确定合适的奖励系数。在这项工作中，我们提出了一种新的强化学习框架，用于训练同时包含奖励和约束的神经网络控制器。为了让工程师能够适当地反映他们对约束的意图并以最小的计算开销处理它们，我们提出了两种约束类型和一种高效的策略优化算法。该学习框架被应用于训练不同形态和物理属性的几个腿式机器人的运动控制器。

    Several earlier studies have shown impressive control performance in complex robotic systems by designing the controller using a neural network and training it with model-free reinforcement learning. However, these outstanding controllers with natural motion style and high task performance are developed through extensive reward engineering, which is a highly laborious and time-consuming process of designing numerous reward terms and determining suitable reward coefficients. In this work, we propose a novel reinforcement learning framework for training neural network controllers for complex robotic systems consisting of both rewards and constraints. To let the engineers appropriately reflect their intent to constraints and handle them with minimal computation overhead, two constraint types and an efficient policy optimization algorithm are suggested. The learning framework is applied to train locomotion controllers for several legged robots with different morphology and physical attribu
    
[^73]: Masked Autoencoders是高效的分类增量学习器

    Masked Autoencoders are Efficient Class Incremental Learners. (arXiv:2308.12510v1 [cs.CV])

    [http://arxiv.org/abs/2308.12510](http://arxiv.org/abs/2308.12510)

    本论文提出了使用掩蔽自编码器(MAEs)作为高效的分类增量学习器，通过重建原始输入图像和学习图像级和嵌入级融合来存储和学习过去任务的表示。实验证实，在CIFAR-100，ImageNet-Subset和ImageNet-Full上，该方法优于现有最先进的方法。

    

    分类增量学习(CIL)旨在在学习新类别的同时避免对之前知识的灾难性遗忘。我们提出使用掩蔽自编码器(MAEs)作为CIL的高效学习器。MAEs最初是为了通过重建无监督学习来学习有用的表示，并且它们可以轻松地与监督损失结合以用于分类。此外，MAEs可以可靠地从随机选择的图像补丁中重建原始输入图像，我们使用这种方法更高效地存储过去任务的范例用于CIL。我们还提出了双边MAE框架来学习图像级和嵌入级融合，它可以产生更高质量的重建图像和更稳定的表示。我们的实验证实，我们的方法在CIFAR-100，ImageNet-Subset和ImageNet-Full上优于现有最先进的方法。代码可在https://github.com/scok30/MAE-CIL上获取。

    Class Incremental Learning (CIL) aims to sequentially learn new classes while avoiding catastrophic forgetting of previous knowledge. We propose to use Masked Autoencoders (MAEs) as efficient learners for CIL. MAEs were originally designed to learn useful representations through reconstructive unsupervised learning, and they can be easily integrated with a supervised loss for classification. Moreover, MAEs can reliably reconstruct original input images from randomly selected patches, which we use to store exemplars from past tasks more efficiently for CIL. We also propose a bilateral MAE framework to learn from image-level and embedding-level fusion, which produces better-quality reconstructed images and more stable representations. Our experiments confirm that our approach performs better than the state-of-the-art on CIFAR-100, ImageNet-Subset, and ImageNet-Full. The code is available at https://github.com/scok30/MAE-CIL .
    
[^74]: 错误信息，机器人和恶意宣传：揭秘社交媒体操纵的要素

    False Information, Bots and Malicious Campaigns: Demystifying Elements of Social Media Manipulations. (arXiv:2308.12497v1 [cs.SI])

    [http://arxiv.org/abs/2308.12497](http://arxiv.org/abs/2308.12497)

    本文通过综合多学科的见解，对社交媒体操纵的要素进行了综合分析，包括虚假信息、机器人和恶意宣传，并研究了它们之间的相互关系。

    

    虚假信息的快速传播和对在线社交网络的持续操纵攻击，往往是出于政治、意识形态或经济利益，已经影响了在线社交网络的开放性。虽然来自不同学科的研究人员已经调查了在线社交网络中不同操纵引发要素（如了解信息在在线社交网络上的传播或检测账户的自动化行为），但这些工作没有被整合起来以提供对这些要素之间相互关系的全面概述。值得注意的是，以往的研究忽视了用户心理、机器人的普及程度以及它们与虚假信息检测的策略之间的关系。为了填补这一研究空白，本文综合了来自各个学科的见解，提供了对操纵领域的全面分析。通过整合社交媒体操纵的主要要素，包括虚假信息、机器人和恶意宣传，我们广泛地研究了每个社交媒体操纵的方面。

    The rapid spread of false information and persistent manipulation attacks on online social networks (OSNs), often for political, ideological, or financial gain, has affected the openness of OSNs. While researchers from various disciplines have investigated different manipulation-triggering elements of OSNs (such as understanding information diffusion on OSNs or detecting automated behavior of accounts), these works have not been consolidated to present a comprehensive overview of the interconnections among these elements. Notably, user psychology, the prevalence of bots, and their tactics in relation to false information detection have been overlooked in previous research. To address this research gap, this paper synthesizes insights from various disciplines to provide a comprehensive analysis of the manipulation landscape. By integrating the primary elements of social media manipulation (SMM), including false information, bots, and malicious campaigns, we extensively examine each SMM 
    
[^75]: 优化心电图分类中神经网络规模

    Optimizing Neural Network Scale for ECG Classification. (arXiv:2308.12492v1 [cs.LG])

    [http://arxiv.org/abs/2308.12492](http://arxiv.org/abs/2308.12492)

    该论文研究了缩放卷积神经网络在心电图分类中的应用，通过优化网络规模的关键参数（层深度、通道数和卷积核大小），得出了在ECG分类中较浅的网络、较大的通道数和较小的卷积核尺寸可以获得更好性能的结论。

    

    我们研究了缩放卷积神经网络（CNN），具体针对残差神经网络（ResNet），用于分析心电图（ECG）。虽然ECG信号是时间序列数据，但基于CNN的模型在ECG分析中表现出了优于其他不同架构的神经网络的能力。然而，大多数以前的ECG分析研究忽略了网络缩放优化的重要性，而这会显著提高性能。我们通过检查关键参数（包括层深度、通道数和卷积核大小）探索和证明了一种有效的ResNet缩放方法。通过广泛的实验，我们发现在ECG分类中，较浅的网络、较大的通道数和较小的卷积核尺寸可以获得更好的性能。最佳网络规模可能因目标任务而异，但我们的研究结果为获得更高效准确的模型提供了见解。

    We study scaling convolutional neural networks (CNNs), specifically targeting Residual neural networks (ResNet), for analyzing electrocardiograms (ECGs). Although ECG signals are time-series data, CNN-based models have been shown to outperform other neural networks with different architectures in ECG analysis. However, most previous studies in ECG analysis have overlooked the importance of network scaling optimization, which significantly improves performance. We explored and demonstrated an efficient approach to scale ResNet by examining the effects of crucial parameters, including layer depth, the number of channels, and the convolution kernel size. Through extensive experiments, we found that a shallower network, a larger number of channels, and smaller kernel sizes result in better performance for ECG classifications. The optimal network scale might differ depending on the target task, but our findings provide insight into obtaining more efficient and accurate models with fewer com
    
[^76]: 使用基于知识蒸馏的长短期记忆（LSTM）模型的离线嵌入式低功耗装置的摔倒检测

    Fall Detection using Knowledge Distillation Based Long short-term memory for Offline Embedded and Low Power Devices. (arXiv:2308.12481v1 [eess.SP])

    [http://arxiv.org/abs/2308.12481](http://arxiv.org/abs/2308.12481)

    本文提出了一种成本效益高、低功耗的方法来进行意外摔倒检测，并使用了基于知识蒸馏的LSTM模型来显著提高准确性。该解决方案通过分析各种传感器收集的时间序列数据实现实时检测能力，并采用知识蒸馏技术来提高模型的精度和降低功耗。

    

    本文提出了一种成本效益高、低功耗的方法来进行意外摔倒检测，该方法使用了基于知识蒸馏的LSTM模型，以显著提高准确性。主要关注从各种传感器收集的时间序列数据的分析，该解决方案提供实时检测能力，确保及时可靠地识别摔倒事件。作者研究了基于不同传感器的摔倒检测模型，比较了它们的准确率和性能。此外，他们采用了知识蒸馏技术来增强模型的精度，从而得到了消耗更低功率的精确配置。因此，这个提议的解决方案为未来这一关键领域中节能摔倒检测系统的发展提供了一个引人注目的途径。

    This paper presents a cost-effective, low-power approach to unintentional fall detection using knowledge distillation-based LSTM (Long Short-Term Memory) models to significantly improve accuracy. With a primary focus on analyzing time-series data collected from various sensors, the solution offers real-time detection capabilities, ensuring prompt and reliable identification of falls. The authors investigate fall detection models that are based on different sensors, comparing their accuracy rates and performance. Furthermore, they employ the technique of knowledge distillation to enhance the models' precision, resulting in refined accurate configurations that consume lower power. As a result, this proposed solution presents a compelling avenue for the development of energy-efficient fall detection systems for future advancements in this critical domain.
    
[^77]: 从流式多元时间序列中实现零延迟一致信号重建

    Zero-delay Consistent Signal Reconstruction from Streamed Multivariate Time Series. (arXiv:2308.12459v1 [eess.SP])

    [http://arxiv.org/abs/2308.12459](http://arxiv.org/abs/2308.12459)

    本文介绍了一种从流式多元时间序列中一致地重建信号的方法，同时减少了零延迟信号重建的粗糙度。 (arXiv:2308.12459v1 [eess.SP])

    

    数字化现实世界的模拟信号通常涉及时间采样和幅度离散化。后续的信号重建不可避免地会产生一个与幅度分辨率和获取样本的时间密度有关的误差。从实施的角度来看，一致的信号重建方法在采样率增加时已被证明具有有益的误差衰减效果。尽管如此，这些结果是在离线设置下获得的。因此，关于从数据流中进行一致信号重建的方法存在研究空白。本文提出了一种在零延迟响应要求下一致地重建量化间隔的流式多元时间序列的方法。另一方面，先前的工作表明，利用单变量时间序列中的时间依赖性可以减少零延迟信号重建的粗糙度。本工作表明，在多元时间序列中存在着时空依赖性。

    Digitalizing real-world analog signals typically involves sampling in time and discretizing in amplitude. Subsequent signal reconstructions inevitably incur an error that depends on the amplitude resolution and the temporal density of the acquired samples. From an implementation viewpoint, consistent signal reconstruction methods have proven a profitable error-rate decay as the sampling rate increases. Despite that, these results are obtained under offline settings. Therefore, a research gap exists regarding methods for consistent signal reconstruction from data streams. This paper presents a method that consistently reconstructs streamed multivariate time series of quantization intervals under a zero-delay response requirement. On the other hand, previous work has shown that the temporal dependencies within univariate time series can be exploited to reduce the roughness of zero-delay signal reconstructions. This work shows that the spatiotemporal dependencies within multivariate time 
    
[^78]: PFL-GAN：当客户异质性与生成模型相遇在个性化联合学习中

    PFL-GAN: When Client Heterogeneity Meets Generative Models in Personalized Federated Learning. (arXiv:2308.12454v1 [cs.LG])

    [http://arxiv.org/abs/2308.12454](http://arxiv.org/abs/2308.12454)

    PFL-GAN是一种在个性化联合学习中解决客户异质性的新型GAN共享和聚合策略，通过学习客户间的相似度并采用加权协同数据聚合方法来实现。

    

    近期生成学习模型的进展伴随着对基于生成对抗网络（GAN）模型的联合学习（FL）的日益关注。在FL的背景下，GAN可以捕捉底层客户数据结构，并重新生成类似于原始数据分布的样本，而不损害私有原始数据。虽然大多数现有基于GAN的FL工作集中在训练全局模型上，但在客户数据异质性方面，个性化FL（PFL）有时可能更加有效，因为它涉及到不同数据样本分布、特征空间和标签。为了应对GAN-based FL中的客户异质性，我们提出了一种针对PFL的新型GAN共享和聚合策略。所提出的PFL-GAN解决了不同场景下的客户异质性问题。具体而言，我们首先学习客户之间的相似度，然后开发一个加权的协同数据聚合方法。通过在几个知名数据集上进行严格的实验来验证此方法的实际效果。

    Recent advances of generative learning models are accompanied by the growing interest in federated learning (FL) based on generative adversarial network (GAN) models. In the context of FL, GAN can capture the underlying client data structure, and regenerate samples resembling the original data distribution without compromising the private raw data. Although most existing GAN-based FL works focus on training a global model, Personalized FL (PFL) sometimes can be more effective in view of client data heterogeneity in terms of distinct data sample distributions, feature spaces, and labels. To cope with client heterogeneity in GAN-based FL, we propose a novel GAN sharing and aggregation strategy for PFL. The proposed PFL-GAN addresses the client heterogeneity in different scenarios. More specially, we first learn the similarity among clients and then develop an weighted collaborative data aggregation. The empirical results through the rigorous experimentation on several well-known datasets
    
[^79]: 利用潜在扩散模型的合成数据增强医学图像分类器

    Augmenting medical image classifiers with synthetic data from latent diffusion models. (arXiv:2308.12453v1 [cs.CV])

    [http://arxiv.org/abs/2308.12453](http://arxiv.org/abs/2308.12453)

    使用潜在扩散模型生成的合成数据可提高医学图像分类器的性能，在数据受限的情况下表现出饱和效果，比添加真实图像获得的性能提升要小得多。

    

    虽然现在有数百种人工智能算法已获得美国食品和药物管理局的批准或清除，但许多研究显示其一致性泛化或潜在偏差存在不一致，特别是对于少数群体。有人提出生成式人工智能可以减少对真实数据的需求，但其在模型开发中的效用尚不清楚。皮肤疾病是合成图像生成的有用案例研究，因为疾病外观具有多样性，特别是在皮肤色调这一保护属性上。我们展示了潜在扩散模型可以可扩展地生成皮肤疾病图像，并且在数据受限情况下，将这些数据用于模型训练可以提高性能。这种性能提升在合成到真实图像比例超过10：1后达到饱和，并且比添加真实图像所获得的提升要小得多。作为我们分析的一部分，我们生成并分析了一个新的数据集，其中包括458,920个合成图像。

    While hundreds of artificial intelligence (AI) algorithms are now approved or cleared by the US Food and Drugs Administration (FDA), many studies have shown inconsistent generalization or latent bias, particularly for underrepresented populations. Some have proposed that generative AI could reduce the need for real data, but its utility in model development remains unclear. Skin disease serves as a useful case study in synthetic image generation due to the diversity of disease appearance, particularly across the protected attribute of skin tone. Here we show that latent diffusion models can scalably generate images of skin disease and that augmenting model training with these data improves performance in data-limited settings. These performance gains saturate at synthetic-to-real image ratios above 10:1 and are substantially smaller than the gains obtained from adding real images. As part of our analysis, we generate and analyze a new dataset of 458,920 synthetic images produced using 
    
[^80]: 一种基于有意遗忘驱动的自愈深度强化学习系统的方法

    An Intentional Forgetting-Driven Self-Healing Method For Deep Reinforcement Learning Systems. (arXiv:2308.12445v1 [cs.LG])

    [http://arxiv.org/abs/2308.12445](http://arxiv.org/abs/2308.12445)

    本文提出了一种名为 Dr. DRL 的自愈方法，用于解决深度强化学习系统中的一些效率问题，该方法通过在连续学习中引入有意遗忘的机制来应对环境漂移引起的困扰。

    

    深度强化学习 (DRL) 在像 Netflix 和 Facebook 这样的大规模应用中越来越多。和大多数数据驱动系统一样，DRL 系统可能由于环境漂移导致不良行为，而这种漂移经常发生在不断变化的生产环境中。连续学习 (CL) 是自愈方法，用于根据环境条件的变化调整 DRL 代理。然而，大规模的连续变化可能导致生产环境从原始状态偏离。最近的研究表明，这些环境漂移往往导致连续学习进入长时间的自愈周期，甚至无法成功，这是由于灾难性遗忘、温和起始失败和收敛缓慢等效率低下问题引起的。在本文中，我们提出 Dr. DRL，一种对 DRL 系统的有效自愈方法，它将有意遗忘的新颖机制整合到原始的连续学习中以解决其主要问题。Dr. DRL 有意地擦除...

    Deep reinforcement learning (DRL) is increasingly applied in large-scale productions like Netflix and Facebook. As with most data-driven systems, DRL systems can exhibit undesirable behaviors due to environmental drifts, which often occur in constantly-changing production settings. Continual Learning (CL) is the inherent self-healing approach for adapting the DRL agent in response to the environment's conditions shifts. However, successive shifts of considerable magnitude may cause the production environment to drift from its original state. Recent studies have shown that these environmental drifts tend to drive CL into long, or even unsuccessful, healing cycles, which arise from inefficiencies such as catastrophic forgetting, warm-starting failure, and slow convergence. In this paper, we propose Dr. DRL, an effective self-healing approach for DRL systems that integrates a novel mechanism of intentional forgetting into vanilla CL to overcome its main issues. Dr. DRL deliberately erases
    
[^81]: TAI-GAN：时间和解剖信息感知的生成对抗网络，用于动态心脏PET运动校正中的早期到晚期帧转换

    TAI-GAN: Temporally and Anatomically Informed GAN for early-to-late frame conversion in dynamic cardiac PET motion correction. (arXiv:2308.12443v1 [eess.IV])

    [http://arxiv.org/abs/2308.12443](http://arxiv.org/abs/2308.12443)

    TAI-GAN是一种用于动态心脏PET运动校正的生成对抗网络，通过时间和解剖信息感知实现早期到晚期帧的转换。

    

    鲍比铷引物($^{82}$Rb)的快速示踪剂动力学和动态心脏正电子发射断层显像(PET)中帧间分布的高变异性给帧间运动校正提出了显著的挑战，尤其是在传统的基于强度的图像配准技术不适用的早期帧上。作为替代方法，一种有希望的方法是利用生成方法处理示踪剂分布变化，以辅助现有的配准方法。为了改进逐帧配准和参数化量化，我们提出了一种时间和解剖信息感知的生成对抗网络(TAI-GAN)，将早期帧转换为晚期参考帧，使用一个对一的映射。具体地，一个特征逐通道线性调制层编码来自时间示踪剂动力学信息生成的通道参数，粗略的心脏分割带有局部移位作为解剖信息。我们在临床数据上验证了我们提出的方法。

    The rapid tracer kinetics of rubidium-82 ($^{82}$Rb) and high variation of cross-frame distribution in dynamic cardiac positron emission tomography (PET) raise significant challenges for inter-frame motion correction, particularly for the early frames where conventional intensity-based image registration techniques are not applicable. Alternatively, a promising approach utilizes generative methods to handle the tracer distribution changes to assist existing registration methods. To improve frame-wise registration and parametric quantification, we propose a Temporally and Anatomically Informed Generative Adversarial Network (TAI-GAN) to transform the early frames into the late reference frame using an all-to-one mapping. Specifically, a feature-wise linear modulation layer encodes channel-wise parameters generated from temporal tracer kinetics information, and rough cardiac segmentations with local shifts serve as the anatomical information. We validated our proposed method on a clinica
    
[^82]: BaDExpert: 提取后门功能以实现准确的后门输入检测

    BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection. (arXiv:2308.12439v1 [cs.CR])

    [http://arxiv.org/abs/2308.12439](http://arxiv.org/abs/2308.12439)

    BaDExpert是一种防御后门攻击的新方法，通过逆向工程提取给定后门模型的后门功能，并生成一个专家模型，该模型只能识别后门输入。可以进一步使用该模型设计高度准确的后门输入检测器。

    

    我们提出了一种新颖的防御方法，用于对抗深度神经网络（DNN）上的后门攻击，其中对手将恶意行为（后门）秘密地植入DNN中。我们的防御属于后期开发的防御范畴，独立于模型生成的方式。所提出的防御方法基于一种新颖的逆向工程方法，可以直接提取给定后门模型的后门功能并生成一个专家模型。该方法很简单 - 在一小组有意义的错误标记的干净样本上微调后门模型，从而使其忘记正常功能但仍保留后门功能，从而生成一个只能识别后门输入的模型（称为后门专家模型）。基于提取的后门专家模型，我们展示了设计高度准确的后门输入检测器的可行性，在模型推理过程中过滤掉后门输入。进一步通过...

    We present a novel defense, against backdoor attacks on Deep Neural Networks (DNNs), wherein adversaries covertly implant malicious behaviors (backdoors) into DNNs. Our defense falls within the category of post-development defenses that operate independently of how the model was generated. The proposed defense is built upon a novel reverse engineering approach that can directly extract backdoor functionality of a given backdoored model to a backdoor expert model. The approach is straightforward -- finetuning the backdoored model over a small set of intentionally mislabeled clean samples, such that it unlearns the normal functionality while still preserving the backdoor functionality, and thus resulting in a model (dubbed a backdoor expert model) that can only recognize backdoor inputs. Based on the extracted backdoor expert model, we show the feasibility of devising highly accurate backdoor input detectors that filter out the backdoor inputs during model inference. Further augmented by
    
[^83]: 部署深度强化学习系统：挑战分类

    Deploying Deep Reinforcement Learning Systems: A Taxonomy of Challenges. (arXiv:2308.12438v1 [cs.LG])

    [http://arxiv.org/abs/2308.12438](http://arxiv.org/abs/2308.12438)

    本文通过对开发者问答论坛Stack Overflow上的帖子进行实证研究，总结了部署深度强化学习系统所面临的挑战，并针对不同的部署平台进行了分类。

    

    深度强化学习（DRL）利用深度学习（DL）在强化学习中，已经在包括机器人、计算机视觉和电脑游戏等领域显示出显著潜力，以实现人类水平的自主能力。这种潜力引起了学术界和工业界对DRL的热情和日益增长的兴趣。然而，目前社区主要集中在DRL系统开发阶段，对DRL部署关注较少。在本文中，我们对开发者最流行的问答论坛Stack Overflow（SO）进行了一项实证研究，以揭示和了解从业人员在部署DRL系统时遇到的挑战。具体而言，我们按部署平台对相关SO帖子进行了分类：服务器/云、移动/嵌入式系统、浏览器和游戏引擎。经过过滤和手动分析，我们研究了357个有关DRL部署的SO帖子，调查了当前状况，并确定了与部署DRL系统相关的挑战。

    Deep reinforcement learning (DRL), leveraging Deep Learning (DL) in reinforcement learning, has shown significant potential in achieving human-level autonomy in a wide range of domains, including robotics, computer vision, and computer games. This potential justifies the enthusiasm and growing interest in DRL in both academia and industry. However, the community currently focuses mostly on the development phase of DRL systems, with little attention devoted to DRL deployment. In this paper, we propose an empirical study on Stack Overflow (SO), the most popular Q&A forum for developers, to uncover and understand the challenges practitioners faced when deploying DRL systems. Specifically, we categorized relevant SO posts by deployment platforms: server/cloud, mobile/embedded system, browser, and game engine. After filtering and manual analysis, we examined 357 SO posts about DRL deployment, investigated the current state, and identified the challenges related to deploying DRL systems. The
    
[^84]: ESG主导的DLT研究的演化：对文献进行NLP分析

    Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature. (arXiv:2308.12420v1 [cs.IR])

    [http://arxiv.org/abs/2308.12420](http://arxiv.org/abs/2308.12420)

    本研究通过NLP分析了ESG主导的DLT研究的演化，通过构建引用网络和命名实体识别任务，对DLT在ESG背景下的发展进行了文献综述。

    

    分布式账本技术(DLT)迅速发展，需要全面了解其各个组成部分。然而，针对DLT的环境、可持续性和治理(ESG)组成部分的系统文献综述还不足。为填补这一空白，我们选择了107篇种子文献，构建了一个包含63,083个参考文献的引用网络，并将其精炼为24,539篇文献的语料库进行分析。然后，我们根据一个已建立的技术分类法从46篇论文中标记了命名实体，并通过找出DLT的ESG要素来完善这个分类法。利用基于transformer的语言模型，我们对一个预先训练的语言模型进行了细化调整，用于命名实体识别任务，使用我们标记的数据集。我们利用我们调整后的语言模型对语料库进行了精简，得到了505篇关键论文，通过命名实体和时间图分析，促进了对DLT在ESG背景下的演化的文献综述。

    Distributed Ledger Technologies (DLTs) have rapidly evolved, necessitating comprehensive insights into their diverse components. However, a systematic literature review that emphasizes the Environmental, Sustainability, and Governance (ESG) components of DLT remains lacking. To bridge this gap, we selected 107 seed papers to build a citation network of 63,083 references and refined it to a corpus of 24,539 publications for analysis. Then, we labeled the named entities in 46 papers according to twelve top-level categories derived from an established technology taxonomy and enhanced the taxonomy by pinpointing DLT's ESG elements. Leveraging transformer-based language models, we fine-tuned a pre-trained language model for a Named Entity Recognition (NER) task using our labeled dataset. We used our fine-tuned language model to distill the corpus to 505 key papers, facilitating a literature review via named entities and temporal graph analysis on DLT evolution in the context of ESG. Our con
    
[^85]: 机器学习在非线性系统参数估计中的应用

    Machine learning in parameter estimation of nonlinear systems. (arXiv:2308.12393v1 [cs.LG])

    [http://arxiv.org/abs/2308.12393](http://arxiv.org/abs/2308.12393)

    该论文提出了一种使用神经网络和Huber损失函数进行非线性系统参数估计的新方法，并通过验证实验展示了其精确性和鲁棒性。

    

    在科学和工程领域中，准确估计复杂非线性系统的参数至关重要。我们提出了一种使用具有Huber损失函数的神经网络进行参数估计的新方法。该方法利用深度学习的能力来揭示非线性方程中的复杂行为的参数。我们使用合成数据和预定义的模型系统动力学函数来验证我们的方法。通过使用带有噪声时间序列数据训练神经网络，它可以对参数进行精确调整，使其收敛到准确值。我们将我们的方法应用于阻尼振荡子、Van der Pol振荡子、Lotka-Volterra系统和Lorenz系统在乘法噪声下的情况。训练后的神经网络从与潜在动态密切匹配的结果中准确估计参数。通过对比真实轨迹和估计轨迹的可视化，进一步证明了我们的方法的精确性和鲁棒性。我们的研究强调了以Huber损失为指导的神经网络作为一种多功能的方法。

    Accurately estimating parameters in complex nonlinear systems is crucial across scientific and engineering fields. We present a novel approach for parameter estimation using a neural network with the Huber loss function. This method taps into deep learning's abilities to uncover parameters governing intricate behaviors in nonlinear equations. We validate our approach using synthetic data and predefined functions that model system dynamics. By training the neural network with noisy time series data, it fine-tunes the Huber loss function to converge to accurate parameters. We apply our method to damped oscillators, Van der Pol oscillators, Lotka-Volterra systems, and Lorenz systems under multiplicative noise. The trained neural network accurately estimates parameters, evident from closely matching latent dynamics. Comparing true and estimated trajectories visually reinforces our method's precision and robustness. Our study underscores the Huber loss-guided neural network as a versatile t
    
[^86]: FOSA: 全信息最大似然 (FIML) 优化的自注意力缺失数据补全方法

    FOSA: Full Information Maximum Likelihood (FIML) Optimized Self-Attention Imputation for Missing Data. (arXiv:2308.12388v1 [cs.LG])

    [http://arxiv.org/abs/2308.12388](http://arxiv.org/abs/2308.12388)

    FOSA是一种全信息最大似然 (FIML) 优化的自注意力缺失数据补全方法，通过融合FIML估计和自注意力机制，实现了在准确性、计算效率和适应不同数据结构方面的显著优势。

    

    数据补全中，有效地处理缺失值尤为重要，特别是在复杂的数据集中。本论文深入研究了FIML优化自注意力（FOSA）框架，这是一种融合了全信息最大似然（FIML）估计和自注意力神经网络能力的创新方法。我们的方法首先通过FIML对缺失值进行初始估计，然后通过利用自注意力机制来进一步提炼这些估计值。我们在模拟数据集和真实数据集上的全面实验证明了FOSA相对于传统的FIML技术在准确性、计算效率和适应不同数据结构方面的显著优势。有趣的是，即使在结构方程模型（SEM）可能错误规定导致子优的FIML估计的情况下，FOSA自注意力组件的稳健架构能够灵活地纠正和优化补全结果。

    In data imputation, effectively addressing missing values is pivotal, especially in intricate datasets. This paper delves into the FIML Optimized Self-attention (FOSA) framework, an innovative approach that amalgamates the strengths of Full Information Maximum Likelihood (FIML) estimation with the capabilities of self-attention neural networks. Our methodology commences with an initial estimation of missing values via FIML, subsequently refining these estimates by leveraging the self-attention mechanism. Our comprehensive experiments on both simulated and real-world datasets underscore FOSA's pronounced advantages over traditional FIML techniques, encapsulating facets of accuracy, computational efficiency, and adaptability to diverse data structures. Intriguingly, even in scenarios where the Structural Equation Model (SEM) might be mis-specified, leading to suboptimal FIML estimates, the robust architecture of FOSA's self-attention component adeptly rectifies and optimizes the imputati
    
[^87]: 从姓名中推断性别：一个大规模的性能评估研究

    Inferring gender from name: a large scale performance evaluation study. (arXiv:2308.12381v1 [cs.CL])

    [http://arxiv.org/abs/2308.12381](http://arxiv.org/abs/2308.12381)

    本研究评估了从姓名中推断性别的性能，该方法在没有性别信息的情况下是一种可行且广泛应用的方法。其重要性在于研究各种科学学科中对性别差异的模式和决定因素进行分析。

    

    一个人的性别是在进行医学、社会学、政治学和经济学等各种科学学科的研究时一个至关重要的信息。然而，随着大数据的激增，性别信息的获取变得越来越困难。在这种情况下，研究人员需要从可获得的信息中，主要是从人名中推断性别。尽管通过姓名来推断性别可能引发一些伦理问题，但缺乏可行的替代方法意味着研究人员不得不使用这种方法，当目标使手段合理时-在大多数这类研究中，目标是研究性别差异的模式和决定因素。姓名到性别推断的必要性产生了一个越来越多的算法方法和软件产品的领域。这些方法已经在世界各地的学术界、工业界、政府和非政府组织中使用。

    A person's gender is a crucial piece of information when performing research across a wide range of scientific disciplines, such as medicine, sociology, political science, and economics, to name a few. However, in increasing instances, especially given the proliferation of big data, gender information is not readily available. In such cases researchers need to infer gender from readily available information, primarily from persons' names. While inferring gender from name may raise some ethical questions, the lack of viable alternatives means that researchers have to resort to such approaches when the goal justifies the means - in the majority of such studies the goal is to examine patterns and determinants of gender disparities. The necessity of name-to-gender inference has generated an ever-growing domain of algorithmic approaches and software products. These approaches have been used throughout the world in academia, industry, governmental and non-governmental organizations. Neverthe
    
[^88]: 带有神经集合、最大熵损失和特征增强的开放集人脸识别

    Open-set Face Recognition with Neural Ensemble, Maximal Entropy Loss and Feature Augmentation. (arXiv:2308.12371v1 [cs.CV])

    [http://arxiv.org/abs/2308.12371](http://arxiv.org/abs/2308.12371)

    本文介绍了一种新颖的方法，将一组紧凑的神经网络与基于边缘的成本函数相结合，通过探索附加样本来提高开放集人脸识别的准确性。该方法利用外部数据库获取辅助负样本或通过混合特征增强方法在训练过程中合成建立负样本。实验结果表明，该方法能够提升封闭集的准确性。

    

    开放集人脸识别是指生物识别系统对所有已存在主体的知识不完整的情况。因此，人们期望它们能够防止将未注册主体的人脸样本识别为先前注册的身份。这种监视列表的背景增加了一个艰巨的要求，要求主要关注感兴趣的主体，从而排除不相关的人脸。为此，本文引入了一种新的方法，该方法将一组紧凑的神经网络与基于边缘的成本函数相结合，该函数探索附加样本。辅助负样本可以从外部数据库中获得，或者在训练时通过新的混合特征增强方法在表示层次上进行合成构建。预先在大型人脸数据集上训练的深度神经网络作为初步特征提取模块。我们在知名的LFW和IJB-C数据集上进行了实验证明，该方法能够提升封闭集的

    Open-set face recognition refers to a scenario in which biometric systems have incomplete knowledge of all existing subjects. Therefore, they are expected to prevent face samples of unregistered subjects from being identified as previously enrolled identities. This watchlist context adds an arduous requirement that calls for the dismissal of irrelevant faces by focusing mainly on subjects of interest. As a response, this work introduces a novel method that associates an ensemble of compact neural networks with a margin-based cost function that explores additional samples. Supplementary negative samples can be obtained from external databases or synthetically built at the representation level in training time with a new mix-up feature augmentation approach. Deep neural networks pre-trained on large face datasets serve as the preliminary feature extraction module. We carry out experiments on well-known LFW and IJB-C datasets where results show that the approach is able to boost closed an
    
[^89]: SafeAR: 通过风险感知策略实现更安全的算法补偿

    SafeAR: Towards Safer Algorithmic Recourse by Risk-Aware Policies. (arXiv:2308.12367v1 [cs.LG])

    [http://arxiv.org/abs/2308.12367](http://arxiv.org/abs/2308.12367)

    本文提出了一种更安全的算法补救方法（SafeAR），该方法通过考虑风险因素在计算和评估补救措施时，为那些受到机器学习模型决策不利影响的个体提供更可靠的建议。

    

    随着机器学习模型在金融和医疗等关键领域的广泛使用，为那些受到机器学习模型决策不利影响的个体提供补救措施的需求变得更加重要；个体应该获得改善自身情况和获得有利决策的建议。之前关于顺序算法补救的工作——推荐一系列变化——主要关注行动的可行性，并使用特征变化的接近程度确定行动成本。然而，未考虑特征变化的不确定性和补救中高于平均成本的风险。如果补救措施可能（以一定概率）导致更糟糕的情况，而恢复需要付出非常高的代价，那将是不可取的。在计算和评估补救措施时，必须考虑风险。我们将考虑了这种风险因素计算出的补救措施称为更安全的算法补救（SafeAR）。

    With the growing use of machine learning (ML) models in critical domains such as finance and healthcare, the need to offer recourse for those adversely affected by the decisions of ML models has become more important; individuals ought to be provided with recommendations on actions to take for improving their situation and thus receive a favorable decision. Prior work on sequential algorithmic recourse -- which recommends a series of changes -- focuses on action feasibility and uses the proximity of feature changes to determine action costs. However, the uncertainties of feature changes and the risk of higher than average costs in recourse have not been considered. It is undesirable if a recourse could (with some probability) result in a worse situation from which recovery requires an extremely high cost. It is essential to incorporate risks when computing and evaluating recourse. We call the recourse computed with such risk considerations as Safer Algorithmic Recourse (SafeAR). The ob
    
[^90]: 规范化扩散模型

    Renormalizing Diffusion Models. (arXiv:2308.12355v1 [hep-th])

    [http://arxiv.org/abs/2308.12355](http://arxiv.org/abs/2308.12355)

    该论文介绍了如何使用扩散模型学习统计学和量子场论的逆规范化群流，为构建用于研究场论的基于机器学习的模型提供了具体框架，并详细说明了这些模型如何定义一类自适应桥接取样器。

    

    我们解释了如何使用扩散模型学习统计学和量子场论的逆规范化群流。扩散模型是一类机器学习模型，通过学习将数据添加噪声的逆过程，生成复杂分布的样本，例如自然图像的分布。非微扰规范化群方案可以自然地写成场空间中的扩散过程。我们将这些观察结果结合到一个具体的框架中，用于构建用于研究场论的基于机器学习的模型，其中模型学习到一个明确指定的规范化群方案的逆过程。我们详细说明了这些模型如何定义格点场论的自适应桥接（或平行淬火）取样器的一类。由于规范化群方案具有物理意义，我们提供了明确比较不同方案的指导方案。

    We explain how to use diffusion models to learn inverse renormalization group flows of statistical and quantum field theories. Diffusion models are a class of machine learning models which have been used to generate samples from complex distributions, such as the distribution of natural images, by learning the inverse process to a diffusion process which adds noise to the data until the distribution of the data is pure noise. Nonperturbative renormalization group schemes can naturally be written as diffusion processes in the space of fields. We combine these observations in a concrete framework for building ML-based models for studying field theories, in which the models learn the inverse process to an explicitly-specified renormalization group scheme. We detail how these models define a class of adaptive bridge (or parallel tempering) samplers for lattice field theory. Because renormalization group schemes have a physical meaning, we provide explicit prescriptions for how to compare r
    
[^91]: 机器学习在药物发现中预测小分子性质的研究

    Machine Learning Small Molecule Properties in Drug Discovery. (arXiv:2308.12354v1 [q-bio.BM])

    [http://arxiv.org/abs/2308.12354](http://arxiv.org/abs/2308.12354)

    这篇综述介绍了机器学习在药物发现中预测小分子性质的方法，并讨论了不同性质的预测和优化挑战以及多目标优化技术的应用。同时评估了提供模型预测理解的技术对药物发现中的决策的重要性。

    

    机器学习（ML）是一种在药物发现中预测小分子性质的有前景的方法。本文综述了近年来为此目的引入的各种ML方法。我们审查了一系列性质，包括结合亲和力、溶解度和ADMET（吸收、分布、代谢、排泄和毒性）。我们讨论了现有的流行数据集和分子描述符和嵌入方法，如化学指纹和基于图的神经网络。我们还强调了在药物发现的命中优选和优化引导阶段预测和优化多个性质的挑战，并简要探讨了可能用于平衡不同性质的多目标优化技术。最后，我们评估了提供模型预测理解的技术，特别是在药物发现中关键决策的情况下。总的来说，本综述提供了对机器学习在药物发现中预测小分子性质方面的洞察。

    Machine learning (ML) is a promising approach for predicting small molecule properties in drug discovery. Here, we provide a comprehensive overview of various ML methods introduced for this purpose in recent years. We review a wide range of properties, including binding affinities, solubility, and ADMET (Absorption, Distribution, Metabolism, Excretion, and Toxicity). We discuss existing popular datasets and molecular descriptors and embeddings, such as chemical fingerprints and graph-based neural networks. We highlight also challenges of predicting and optimizing multiple properties during hit-to-lead and lead optimization stages of drug discovery and explore briefly possible multi-objective optimization techniques that can be used to balance diverse properties while optimizing lead candidates. Finally, techniques to provide an understanding of model predictions, especially for critical decision-making in drug discovery are assessed. Overall, this review provides insights into the land
    
[^92]: 用Schrödinger桥改进基于生成模型的展开

    Improving Generative Model-based Unfolding with Schr\"{o}dinger Bridges. (arXiv:2308.12351v1 [hep-ph])

    [http://arxiv.org/abs/2308.12351](http://arxiv.org/abs/2308.12351)

    本研究提出了一种使用Schrödinger桥和扩散模型创建的展开方法SBUnfold，它将判别模型和生成模型的优势结合起来。与最先进方法相比，在合成的Z+jets数据集上获得了卓越的性能。

    

    基于机器学习的展开已经实现了无bin和高维微分截面测量。这个研究领域出现了两种主要方法：一种基于判别模型，一种基于生成模型。判别模型的主要优势在于，它们学习了对起始模拟的小修正，而生成模型在数据稀疏的相空间区域具有更好的扩展性。我们提出使用Schrödinger桥和扩散模型创建SBUnfold，一种将判别模型和生成模型的优势结合起来的展开方法。SBUnfold的关键特点是它的生成模型将一组事件映射到另一组事件，而无需通过已知的概率密度（与标准扩散模型和标准扩散模型不同）。我们展示了SBUnfold在合成的Z+jets数据集上与最先进方法相比的出色性能。

    Machine learning-based unfolding has enabled unbinned and high-dimensional differential cross section measurements. Two main approaches have emerged in this research area: one based on discriminative models and one based on generative models. The main advantage of discriminative models is that they learn a small correction to a starting simulation while generative models scale better to regions of phase space with little data. We propose to use Schroedinger Bridges and diffusion models to create SBUnfold, an unfolding approach that combines the strengths of both discriminative and generative models. The key feature of SBUnfold is that its generative model maps one set of events into another without having to go through a known probability density as is the case for normalizing flows and standard diffusion models. We show that SBUnfold achieves excellent performance compared to state of the art methods on a synthetic Z+jets dataset.
    
[^93]: 使用不同机器学习方法来预测药物溶解度--提取化学特征的线性回归模型与图卷积神经网络模型

    Predicting Drug Solubility Using Different Machine Learning Methods -- Linear Regression Model with Extracted Chemical Features vs Graph Convolutional Neural Network. (arXiv:2308.12325v1 [q-bio.QM])

    [http://arxiv.org/abs/2308.12325](http://arxiv.org/abs/2308.12325)

    本研究利用线性回归模型和图卷积神经网络模型预测药物溶解度，其中图卷积神经网络模型表现最好。通过线性回归模型的特征重要性分析，可以了解每个功能团对溶解度的影响。将图卷积神经网络的高性能与线性回归的可解释性相结合是未来工作的方向。

    

    预测给定分子的溶解度是制药行业中重要的任务，因此是一个广泛研究的课题。本研究利用现代计算资源的优势重新探讨了这个问题。我们在多个实验数据集上应用了两种机器学习模型，即线性回归模型和图卷积神经网络模型。两种方法都能做出合理的预测，而图卷积神经网络模型表现最好。然而，当前的图卷积神经网络模型是一个黑盒子，而线性回归模型的特征重要性分析提供了更多有关底层化学影响的洞察。利用线性回归模型，我们展示了每个功能团对整体溶解度的影响。最终，在设计新药时，了解化学结构如何影响化学性质是至关重要的。未来的工作应该致力于将图卷积神经网络的高性能与线性回归的可解释性相结合，释放出新的优势。

    Predicting the solubility of given molecules is an important task in the pharmaceutical industry, and consequently this is a well-studied topic. In this research, we revisited this problem with the advantage of modern computing resources. We applied two machine learning models, a linear regression model and a graph convolutional neural network model, on multiple experimental datasets. Both methods can make reasonable predictions while the GCNN model had the best performance. However, the current GCNN model is a black box, while feature importance analysis from the linear regression model offers more insights into the underlying chemical influences. Using the linear regression model, we show how each functional group affects the overall solubility. Ultimately, knowing how chemical structure influences chemical properties is crucial when designing new drugs. Future work should aim to combine the high performance of GCNNs with the interpretability of linear regression, unlocking new advan
    
[^94]: 图神经随机微分方程

    Graph Neural Stochastic Differential Equations. (arXiv:2308.12316v1 [cs.LG])

    [http://arxiv.org/abs/2308.12316](http://arxiv.org/abs/2308.12316)

    图神经随机微分方程（Graph Neural SDEs）通过将随机性嵌入到图神经常微分方程中，提供了一种评估预测不确定性的方法，尤其在静态和时空背景下的分布外检测方面表现出色。

    

    我们提出了一种新颖的模型图神经随机微分方程（Graph Neural Stochastic Differential Equations，Graph Neural SDEs）。这种技术通过将布朗运动引入数据表示中，将随机性嵌入到图神经常微分方程（Graph Neural Ordinary Differential Equations，Graph Neural ODEs）中。这种嵌入使得可以评估预测的不确定性，这是目前的模型经常忽视的一个重要方面。在我们的框架中，我们重点介绍了“潜在图神经SDE”变体，并证明了其有效性。通过实证研究，我们发现潜在图神经SDE在置信度预测方面优于常规模型如图卷积网络和图神经ODEs，尤其在处理静态和时空背景下的分布外检测方面表现出色。

    We present a novel model Graph Neural Stochastic Differential Equations (Graph Neural SDEs). This technique enhances the Graph Neural Ordinary Differential Equations (Graph Neural ODEs) by embedding randomness into data representation using Brownian motion. This inclusion allows for the assessment of prediction uncertainty, a crucial aspect frequently missed in current models. In our framework, we spotlight the \textit{Latent Graph Neural SDE} variant, demonstrating its effectiveness. Through empirical studies, we find that Latent Graph Neural SDEs surpass conventional models like Graph Convolutional Networks and Graph Neural ODEs, especially in confidence prediction, making them superior in handling out-of-distribution detection across both static and spatio-temporal contexts.
    
[^95]: 跨领域的可信表示学习

    Trustworthy Representation Learning Across Domains. (arXiv:2308.12315v1 [cs.LG])

    [http://arxiv.org/abs/2308.12315](http://arxiv.org/abs/2308.12315)

    本论文首次提出了跨领域的可信表示学习框架，通过包括鲁棒性、隐私、公平性和可解释性等概念，对该研究方向进行了全面的文献综述。

    

    随着人工智能系统在我们日常生活和人类社会中取得显著的性能，人们既享受到了这些技术带来的好处，也面临因这些系统而引发的许多社会问题。为了使人工智能系统足够好并且可信，已经进行了大量研究，建立了可信人工智能系统的指南。机器学习是人工智能系统中最重要的部分之一，而表示学习是机器学习中的基础技术。如何使表示学习在现实世界的应用中具有可信度，例如跨领域场景，对于机器学习和人工智能系统领域都是非常有价值和必要的。在可信人工智能的概念启发下，我们提出了第一个跨领域的可信表示学习框架，包括了鲁棒性、隐私、公平性和可解释性这四个概念，对这个研究方向进行了全面的文献综述。

    As AI systems have obtained significant performance to be deployed widely in our daily live and human society, people both enjoy the benefits brought by these technologies and suffer many social issues induced by these systems. To make AI systems good enough and trustworthy, plenty of researches have been done to build guidelines for trustworthy AI systems. Machine learning is one of the most important parts for AI systems and representation learning is the fundamental technology in machine learning. How to make the representation learning trustworthy in real-world application, e.g., cross domain scenarios, is very valuable and necessary for both machine learning and AI system fields. Inspired by the concepts in trustworthy AI, we proposed the first trustworthy representation learning across domains framework which includes four concepts, i.e, robustness, privacy, fairness, and explainability, to give a comprehensive literature review on this research direction. Specifically, we first 
    
[^96]: 借助影响辅助的规范形式进行快速准确的NPN分类

    Fast Exact NPN Classification with Influence-aided Canonical Form. (arXiv:2308.12311v1 [cs.LG])

    [http://arxiv.org/abs/2308.12311](http://arxiv.org/abs/2308.12311)

    本文介绍了一种借助布尔影响力进行快速准确的NPN分类的新方法和算法，实验证明影响力在减小转换函数计算时间方面起到了重要作用。

    

    NPN分类在数字电路的综合和验证中有许多应用。基于规范形式的方法是最常用的方法，首先设计一个作为NPN等价类代表的规范形式，然后根据规范形式计算转换函数。大多数工作使用变量对称性和几个标记，主要基于余子式，来简化规范形式的构建和计算。本文通过引入布尔函数分析中的布尔影响力，描述了一种新颖的规范形式及其计算算法，我们展示了影响力与NPN分类的输入否定无关、输入置换依赖以及具有其他结构信息，因此它是加速NPN分类的重要因素。实验结果证明，影响力在减小转换函数计算时间方面起到了重要作用。

    NPN classification has many applications in the synthesis and verification of digital circuits. The canonical-form-based method is the most common approach, designing a canonical form as representative for the NPN equivalence class first and then computing the transformation function according to the canonical form. Most works use variable symmetries and several signatures, mainly based on the cofactor, to simplify the canonical form construction and computation. This paper describes a novel canonical form and its computation algorithm by introducing Boolean influence to NPN classification, which is a basic concept in analysis of Boolean functions. We show that influence is input-negation-independent, input-permutation-dependent, and has other structural information than previous signatures for NPN classification. Therefore, it is a significant ingredient in speeding up NPN classification. Experimental results prove that influence plays an important role in reducing the transformation 
    
[^97]: FedDAT: 一种多模态异构联邦学习中基础模型微调的方法

    FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning. (arXiv:2308.12305v1 [cs.LG])

    [http://arxiv.org/abs/2308.12305](http://arxiv.org/abs/2308.12305)

    FedDAT是一种在多模态异构联邦学习中进行基础模型微调的方法，通过采用参数高效微调（PEFT）方法来减轻客户端计算负担和通信开销，并解决了数据异构性的问题。

    

    近年来，在多模态学习中，基础模型取得了显著的进展。这些模型通常配备了数百万（或数十亿）个参数，需要大量数据进行微调。然而，由于不同的隐私法规，从不同领域收集和集中训练数据变得具有挑战性。联邦学习（FL）作为一种有前途的解决方案出现，可以让多个客户端在不集中其本地数据的情况下共同训练神经网络。为了减轻客户端的计算负担和通信开销，之前的工作已经采用了参数高效微调（PEFT）方法用于FL。在此过程中，只有少量模型参数在联邦通信期间进行优化和传输。然而，大多数之前的工作都专注于单一形态，并忽略了一种常见现象，即客户端之间存在数据异构性。因此，在本文中，我们提出了一个微调框架，用于解决多模态异构联邦学习中的问题。

    Recently, foundation models have exhibited remarkable advancements in multi-modal learning. These models, equipped with millions (or billions) of parameters, typically require a substantial amount of data for finetuning. However, collecting and centralizing training data from diverse sectors becomes challenging due to distinct privacy regulations. Federated Learning (FL) emerges as a promising solution, enabling multiple clients to collaboratively train neural networks without centralizing their local data. To alleviate client computation burdens and communication overheads, previous works have adapted Parameter-efficient Finetuning (PEFT) methods for FL. Hereby, only a small fraction of the model parameters are optimized and communicated during federated communications. Nevertheless, most previous works have focused on a single modality and neglected one common phenomenon, i.e., the presence of data heterogeneity across the clients. Therefore, in this work, we propose a finetuning fra
    
[^98]: 脂肪碎化、联合可测性和POVM假设类的PAC可学习性

    Fat Shattering, Joint Measurability, and PAC Learnability of POVM Hypothesis Classes. (arXiv:2308.12304v1 [stat.ML])

    [http://arxiv.org/abs/2308.12304](http://arxiv.org/abs/2308.12304)

    我们研究了量子测量类的可学习性，并建立了必要和充分条件，同时给出了对应的样本复杂性上界。我们发现标准ERM未满足统一收敛性的问题，于是提出了一种新的学习规则——去噪ERM，该规则在POVM和概率观测的概念类别中具有普适性并满足统一收敛性的条件。

    

    通过建立匹配的必要和充分条件，并给出相应的样本复杂性上界，我们对量子测量类的可学习性进行了刻画，其中学习器仅能接触到已准备好的量子态。我们首先探究了先前作品中关于该设置的结果。我们发现，在一些可学习类别中，先前作品中定义的经验风险与经典理论中的定义相一致，但未能满足统一收敛性质。此外，我们发现，先前作品中对VC维度广义上界的推广常常是无穷的，即使对于有限维的POVM类别也是如此。为了克服标准ERM未能满足统一收敛性的问题，我们提出了一种新的学习规则——去噪ERM。我们证明了对于POVM和概率观测的概念类别，这是一种通用的学习规则，并给出了它满足统一收敛性的条件。

    We characterize learnability for quantum measurement classes by establishing matching necessary and sufficient conditions for their PAC learnability, along with corresponding sample complexity bounds, in the setting where the learner is given access only to prepared quantum states. We first probe the results from previous works on this setting. We show that the empirical risk defined in previous works and matching the definition in the classical theory fails to satisfy the uniform convergence property enjoyed in the classical setting for some learnable classes. Moreover, we show that VC dimension generalization upper bounds in previous work are frequently infinite, even for finite-dimensional POVM classes. To surmount the failure of the standard ERM to satisfy uniform convergence, we define a new learning rule -- denoised ERM. We show this to be a universal learning rule for POVM and probabilistically observed concept classes, and the condition for it to satisfy uniform convergence is 
    
[^99]: 反向成像物理知识驱动的深度神经级集用于掩膜优化

    Inverse Lithography Physics-informed Deep Neural Level Set for Mask Optimization. (arXiv:2308.12299v1 [eess.IV])

    [http://arxiv.org/abs/2308.12299](http://arxiv.org/abs/2308.12299)

    本文提出了一种反向成像物理知识驱动的深度神经级集（ILDLS）方法用于掩膜优化，并在迭代过程中利用级集为深度学习框架的一部分。

    

    随着集成电路特征尺寸的不断减小，光学邻近修正（OPC）已成为保证光刻过程中高印刷性的关键分辨率增强技术。近年来，基于级集的反向成像技术（ILT）作为一种有前途的OPC解决方案，展示了其强大的图案保真度，特别是在先进工艺中。然而，ILT的巨大计算时间消耗限制了其主要用于修正部分层和热点区域。深度学习（DL）方法在加速ILT方面显示出巨大潜力。然而，缺乏反向成像领域知识限制了基于DL的算法在工艺窗口（PW）增强等方面的能力。本文提出了一种反向成像物理知识驱动的深度神经级集（ILDLS）方法来进行掩膜优化。该方法利用级集为DL框架中的一层，并在迭代中进行。

    As the feature size of integrated circuits continues to decrease, optical proximity correction (OPC) has emerged as a crucial resolution enhancement technology for ensuring high printability in the lithography process. Recently, level set-based inverse lithography technology (ILT) has drawn considerable attention as a promising OPC solution, showcasing its powerful pattern fidelity, especially in advanced process. However, massive computational time consumption of ILT limits its applicability to mainly correcting partial layers and hotspot regions. Deep learning (DL) methods have shown great potential in accelerating ILT. However, lack of domain knowledge of inverse lithography limits the ability of DL-based algorithms in process window (PW) enhancement and etc. In this paper, we propose an inverse lithography physics-informed deep neural level set (ILDLS) approach for mask optimization. This approach utilizes level set based-ILT as a layer within the DL framework and iteratively condu
    
[^100]: 整数因式分解、费马和机器学习在经典计算机上的应用

    Integer Factorisation, Fermat & Machine Learning on a Classical Computer. (arXiv:2308.12290v1 [cs.LG])

    [http://arxiv.org/abs/2308.12290](http://arxiv.org/abs/2308.12290)

    这篇论文介绍了一种基于深度学习的整数因式分解概率算法，通过将问题转化为二元分类问题，通过大量合成生成的训练数据，可以在经典计算机上应用，具有实用性和可扩展性。

    

    本文描述了一种基于深度学习的概率算法用于整数因式分解。我们使用劳伦斯对费马因式分解算法的扩展，将整数因式分解问题转化为二元分类问题。为了解决分类问题，在能够生成大型伪随机素数的便利性基础上，我们合成生成了一组训练数据。我们将介绍该算法，总结一些实验，分析这些实验的不足之处，并呼吁其他人重现、验证并探索是否可以改进这种方法，使其成为一种实用的可扩展的因式分解算法。

    In this paper we describe a deep learning--based probabilistic algorithm for integer factorisation. We use Lawrence's extension of Fermat's factorisation algorithm to reduce the integer factorisation problem to a binary classification problem. To address the classification problem, based on the ease of generating large pseudo--random primes, a corpus of training data, as large as needed, is synthetically generated. We will introduce the algorithm, summarise some experiments, analyse where these experiments fall short, and finally put out a call to others to reproduce, verify and see if this approach can be improved to a point where it becomes a practical, scalable factorisation algorithm.
    
[^101]: 使用自适应动量的加速分块近端框架用于非凸和非光滑优化

    An Accelerated Block Proximal Framework with Adaptive Momentum for Nonconvex and Nonsmooth Optimization. (arXiv:2308.12126v1 [math.OC])

    [http://arxiv.org/abs/2308.12126](http://arxiv.org/abs/2308.12126)

    本论文提出一种使用自适应动量的加速分块近端框架(ABPL+)来解决非凸和非光滑优化问题，并通过增强比较过程解决外推步骤失败的问题。同时，扩展算法适用于更新块变量的任何情况，并证明了算法的可行性和有效性。通过展示序列的导数集为关键点的性质，更明显地证明了算法的优势。

    

    我们提出了一种使用自适应动量的加速分块近端线性框架（ABPL+）用于非凸和非光滑优化。我们分析了一些算法中的外推步骤失败的潜在原因，并通过增强比较过程来解决这个问题，该过程评估了我们算法中近端梯度步骤与线性外推步骤之间的权衡。此外，我们将算法扩展到涉及使用正整数更新块变量的任何情况，允许每个周期随机重新排列变量块的更新顺序。此外，在一些温和假设下，我们证明了ABPL+可以在严格限定外推参数和步长的情况下单调地减小函数值，并展示了以随机顺序更新这些块的可行性和有效性。我们还通过更明显直观地展示由我们算法生成的序列的导数集是关键点的性质。

    We propose an accelerated block proximal linear framework with adaptive momentum (ABPL$^+$) for nonconvex and nonsmooth optimization. We analyze the potential causes of the extrapolation step failing in some algorithms, and resolve this issue by enhancing the comparison process that evaluates the trade-off between the proximal gradient step and the linear extrapolation step in our algorithm. Furthermore, we extends our algorithm to any scenario involving updating block variables with positive integers, allowing each cycle to randomly shuffle the update order of the variable blocks. Additionally, under mild assumptions, we prove that ABPL$^+$ can monotonically decrease the function value without strictly restricting the extrapolation parameters and step size, demonstrates the viability and effectiveness of updating these blocks in a random order, and we also more obviously and intuitively demonstrate that the derivative set of the sequence generated by our algorithm is a critical point 
    
[^102]: 受限斯坦变分轨迹优化

    Constrained Stein Variational Trajectory Optimization. (arXiv:2308.12110v1 [cs.RO] CROSS LISTED)

    [http://arxiv.org/abs/2308.12110](http://arxiv.org/abs/2308.12110)

    CSVTO是一种受限斯坦变分轨迹优化算法，它通过斯坦变分梯度下降方法生成多样的约束满足轨迹集合，提高了在具有任意约束的问题中的优化性能和鲁棒性。

    

    我们提出了一种受限斯坦变分轨迹优化（CSVTO）算法，用于在一组轨迹上进行带约束的轨迹优化。我们将受限轨迹优化视为一种新颖的对轨迹分布约束的函数最小化形式，避免将约束视为目标函数的惩罚，从而使我们能够生成多样的满足约束的轨迹集合。我们的方法使用斯坦变分梯度下降（SVGD）寻找一组粒子，近似表示一个低成本轨迹的分布，并遵守约束。CSVTO适用于具有任意等式和不等式约束的问题，并包括一种新颖的粒子重新采样步骤来避免局部最小值。通过明确生成多样的轨迹集合，CSVTO能够更好地避免不良的局部最小值，并且对初始化更具鲁棒性。我们证明，CSVTO在具有高度约束的挑战性问题上优于基线方法。

    We present Constrained Stein Variational Trajectory Optimization (CSVTO), an algorithm for performing trajectory optimization with constraints on a set of trajectories in parallel. We frame constrained trajectory optimization as a novel form of constrained functional minimization over trajectory distributions, which avoids treating the constraints as a penalty in the objective and allows us to generate diverse sets of constraint-satisfying trajectories. Our method uses Stein Variational Gradient Descent (SVGD) to find a set of particles that approximates a distribution over low-cost trajectories while obeying constraints. CSVTO is applicable to problems with arbitrary equality and inequality constraints and includes a novel particle resampling step to escape local minima. By explicitly generating diverse sets of trajectories, CSVTO is better able to avoid poor local minima and is more robust to initialization. We demonstrate that CSVTO outperforms baselines in challenging highly-constr
    
[^103]: 用于计算深度神经网络正则化路径的多目标延续方法

    A multiobjective continuation method to compute the regularization path of deep neural networks. (arXiv:2308.12044v1 [cs.LG])

    [http://arxiv.org/abs/2308.12044](http://arxiv.org/abs/2308.12044)

    本文提出了一种多目标延续方法，用于计算深度神经网络的正则化路径，以解决DNNs中稀疏性和数值效率之间的冲突。

    

    稀疏性是深度神经网络(DNNs)中非常理想的特征，因为它确保了数值效率，提高了模型的可解释性(由于相关特征的数量较少)和鲁棒性。在基于线性模型的机器学习方法中，众所周知在$\ell^1$范数(即零权重)的最稀疏解和非正则化解之间存在一条连接路径，这条路径被称为正则化路径。最近，通过将经验损失和稀疏性($\ell^1$范数)作为两个冲突的标准，并解决由此产生的多目标优化问题，首次尝试将正则化路径的概念扩展到DNNs。然而，由于$\ell^1$范数的不光滑性和参数数量的高度，从计算的角度来看，这种方法并不是很有效。为了克服这个限制，我们提出了一种算法，可以近似计算整个帕累托曲线

    Sparsity is a highly desired feature in deep neural networks (DNNs) since it ensures numerical efficiency, improves the interpretability of models (due to the smaller number of relevant features), and robustness. In machine learning approaches based on linear models, it is well known that there exists a connecting path between the sparsest solution in terms of the $\ell^1$ norm (i.e., zero weights) and the non-regularized solution, which is called the regularization path. Very recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the resulting multiobjective optimization problem. However, due to the non-smoothness of the $\ell^1$ norm and the high number of parameters, this approach is not very efficient from a computational perspective. To overcome this limitation, we present an algorithm that allows for the approximation of the entire Pareto
    
[^104]: 有关在有限预算二臂赌博机中进行最佳臂选择的统一最优算法研究

    On Uniformly Optimal Algorithms for Best Arm Identification in Two-Armed Bandits with Fixed Budget. (arXiv:2308.12000v1 [stat.ML])

    [http://arxiv.org/abs/2308.12000](http://arxiv.org/abs/2308.12000)

    本文研究了在有限预算的随机二臂赌博机中进行最佳臂选择的问题，并证明不存在比等概率采样算法更好的算法。我们引入了一致稳定算法的概念，并证明任何在所有情况下与等概率采样算法表现一样好的算法必须属于这个类别。这一结果解决了之前的两个未解之谜。

    

    本文研究了在具有伯努利奖励的随机二臂赌博机中，使用有限预算进行最佳臂选择的问题。我们证明令人惊讶的是，不存在一个算法可以在所有情况下与等概率采样算法表现一样好（该算法被称为“均匀采样”算法），并且在至少一个情况下明显优于该算法。简而言之，不存在比均匀采样算法更好的算法。为了证明这一结果，我们引入了“一致”和“稳定”算法的自然类，并且证明了任何算法要在所有情况下与均匀采样算法表现一样好，必须属于这个类别。通过导出满足任何一致且稳定算法的错误率的下界，并证明均匀采样算法与此下界相匹配，我们完成了证明过程。我们的结果解决了\cite{qin2022open}中提出的两个未解之谜。

    We study the problem of best-arm identification with fixed budget in stochastic two-arm bandits with Bernoulli rewards. We prove that surprisingly, there is no algorithm that (i) performs as well as the algorithm sampling each arm equally (this algorithm is referred to as the {\it uniform sampling} algorithm) on all instances, and that (ii) strictly outperforms this algorithm on at least one instance. In short, there is no algorithm better than the uniform sampling algorithm. Towards this result, we introduce the natural class of {\it consistent} and {\it stable} algorithms, and show that any algorithm that performs as well as the uniform sampling algorithm on all instances belongs to this class. The proof is completed by deriving a lower bound on the error rate satisfied by any consistent and stable algorithm, and by showing that the uniform sampling algorithm matches this lower bound. Our results provide a solution to the two open problems presented in \cite{qin2022open}.
    
[^105]: 使用反馈循环的对抗训练

    Adversarial Training Using Feedback Loops. (arXiv:2308.11881v1 [cs.LG])

    [http://arxiv.org/abs/2308.11881](http://arxiv.org/abs/2308.11881)

    本文提出了一种基于反馈控制的新型对抗训练方法，通过将反馈控制纳入神经网络架构中进行训练，以增强DNN对对抗性攻击的鲁棒性。

    

    深度神经网络(DNN)由于能够准确地学习非常复杂的输入-输出关系而在许多领域得到了广泛应用。尽管DNN准确性高且使用广泛，但由于泛化能力有限，它们对对抗性攻击非常敏感。为了未来在这个领域取得进展，构建对任何数据点的扰动都具有抵抗能力的DNN是至关重要的。过去提出了许多使用网络的一阶导数信息来增强DNN的技术。本文提出了一种基于控制论的新型增强方法。提出了一种将反馈控制纳入神经网络架构的神经网络架构，称为反馈神经网络。控制器本身是一个神经网络，通过使用常规和对抗性数据进行训练，以稳定系统的输出。基于反馈控制架构的新型对抗训练方法称为反馈循环对抗训练(FLAT)。

    Deep neural networks (DNN) have found wide applicability in numerous fields due to their ability to accurately learn very complex input-output relations. Despite their accuracy and extensive use, DNNs are highly susceptible to adversarial attacks due to limited generalizability. For future progress in the field, it is essential to build DNNs that are robust to any kind of perturbations to the data points. In the past, many techniques have been proposed to robustify DNNs using first-order derivative information of the network.  This paper proposes a new robustification approach based on control theory. A neural network architecture that incorporates feedback control, named Feedback Neural Networks, is proposed. The controller is itself a neural network, which is trained using regular and adversarial data such as to stabilize the system outputs. The novel adversarial training approach based on the feedback control architecture is called Feedback Looped Adversarial Training (FLAT). Numeri
    
[^106]: HypBO: 专家引导下的化学家参与的贝叶斯搜索新材料论文

    HypBO: Expert-Guided Chemist-in-the-Loop Bayesian Search for New Materials. (arXiv:2308.11787v1 [cs.LG])

    [http://arxiv.org/abs/2308.11787](http://arxiv.org/abs/2308.11787)

    HypBO是一种利用专家人类知识引导贝叶斯搜索的方法，通过生成改进的样本种子来更快地找到有希望的化学空间区域。

    

    机器人和自动化可以大大加速解决材料发现等难以解决的多变量科学问题，但可用的搜索空间可能非常庞大。贝叶斯优化已经成为一种受欢迎的样本高效优化引擎，在没有目标函数或属性的解析形式被知道的任务中获得了成功。在这里，我们利用专家人类知识以假设的形式，更快地将贝叶斯搜索引导到有希望的化学空间区域。先前的方法使用从现有实验测量得到的潜在分布，这对于新的未开发的科学任务是不可行的。此外，这样的分布无法捕捉精细的假设。我们提出的方法，称为HypBO，利用专家人类假设生成改进的样本种子。不太有希望的种子自动折扣，而有希望的种子用于增加代理模型数据，从而实现更具信息的采样。

    Robotics and automation offer massive accelerations for solving intractable, multivariate scientific problems such as materials discovery, but the available search spaces can be dauntingly large. Bayesian optimization (BO) has emerged as a popular sample-efficient optimization engine, thriving in tasks where no analytic form of the target function/property is known. Here we exploit expert human knowledge in the form of hypotheses to direct Bayesian searches more quickly to promising regions of chemical space. Previous methods have used underlying distributions derived from existing experimental measurements, which is unfeasible for new, unexplored scientific tasks. Also, such distributions cannot capture intricate hypotheses. Our proposed method, which we call HypBO, uses expert human hypotheses to generate an improved seed of samples. Unpromising seeds are automatically discounted, while promising seeds are used to augment the surrogate model data, thus achieving better-informed sampl
    
[^107]: 大模型时代中的联邦学习：针对特定领域的多模态大模型

    Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models. (arXiv:2308.11217v1 [cs.LG])

    [http://arxiv.org/abs/2308.11217](http://arxiv.org/abs/2308.11217)

    本论文提出了一种多模态联邦学习框架，利用私有领域数据协同训练大型模型，以实现跨场景的智能服务。在大模型时代，该框架解决了异构数据、模型聚合、性能和成本权衡、数据隐私以及激励机制等方面的挑战。

    

    多模态数据能够全面感知和识别物理世界，已成为通往通用人工智能的重要路径。然而，在公共数据集上训练的多模态大模型在特定工业领域的性能往往不理想。本文提出了一种多模态联邦学习框架，可以使多个企业利用私有领域数据协同训练大型模型，实现跨场景的智能服务。作者深入探讨了大模型时代联邦学习的智能基础和目标的战略转变，以及在异构数据、模型聚合、性能和成本权衡、数据隐私和激励机制方面面临的新挑战。本文详细介绍了领先企业在城市安全运营管理方面贡献多模态数据和专家知识的案例研究，包括分布式部署和高效性能的实现。

    Multimodal data, which can comprehensively perceive and recognize the physical world, has become an essential path towards general artificial intelligence. However, multimodal large models trained on public datasets often underperform in specific industrial domains. This paper proposes a multimodal federated learning framework that enables multiple enterprises to utilize private domain data to collaboratively train large models for vertical domains, achieving intelligent services across scenarios. The authors discuss in-depth the strategic transformation of federated learning in terms of intelligence foundation and objectives in the era of big model, as well as the new challenges faced in heterogeneous data, model aggregation, performance and cost trade-off, data privacy, and incentive mechanism. The paper elaborates a case study of leading enterprises contributing multimodal data and expert knowledge to city safety operation management , including distributed deployment and efficient 
    
[^108]: Wasserstein几何生成器用于条件分布

    Wasserstein Geodesic Generator for Conditional Distributions. (arXiv:2308.10145v1 [stat.ML])

    [http://arxiv.org/abs/2308.10145](http://arxiv.org/abs/2308.10145)

    通过Wasserstein几何生成器学习条件分布，生成给定特定标签的样本。使用最优输运理论提出的方法能学习观察域的条件分布和它们之间的最优输运映射。在人脸图像数据上的实验验证了该方法的有效性。

    

    生成给定特定标签的样本需要估计条件分布。我们推导出条件分布之间Wasserstein距离的可处理的上界，以建立学习条件分布的理论基础。基于这一结果，我们提出了一种新颖的条件生成算法，其中条件分布完全由由统计距离定义的度量空间来表征。我们利用最优输运理论来提出了Wasserstein几何生成器，一种学习Wasserstein几何的新的条件生成器。所提出的方法学习观察域的条件分布和它们之间的最优输运映射。给定两个观察域标签，未观察到的中间域的条件分布位于给定的条件分布之间的Wasserstein几何中。在以光照条件为域标签的人脸图像上的实验证明了所提出方法的有效性。

    Generating samples given a specific label requires estimating conditional distributions. We derive a tractable upper bound of the Wasserstein distance between conditional distributions to lay the theoretical groundwork to learn conditional distributions. Based on this result, we propose a novel conditional generation algorithm where conditional distributions are fully characterized by a metric space defined by a statistical distance. We employ optimal transport theory to propose the \textit{Wasserstein geodesic generator}, a new conditional generator that learns the Wasserstein geodesic. The proposed method learns both conditional distributions for observed domains and optimal transport maps between them. The conditional distributions given unobserved intermediate domains are on the Wasserstein geodesic between conditional distributions given two observed domain labels. Experiments on face images with light conditions as domain labels demonstrate the efficacy of the proposed method.
    
[^109]: MoCLIM: 用多组学对比学习和组学推理建模实现准确的癌症亚型划分

    MoCLIM: Towards Accurate Cancer Subtyping via Multi-Omics Contrastive Learning with Omics-Inference Modeling. (arXiv:2308.09725v1 [q-bio.GN])

    [http://arxiv.org/abs/2308.09725](http://arxiv.org/abs/2308.09725)

    本论文介绍了一种名为MoCLIM的多组学对比学习框架，能够在癌症亚型划分中利用多组学数据的潜力，显著提高了数据的拟合度和亚型划分性能。

    

    精准医学的目标是建立癌症亚型的生化机制与疾病之间的因果关系。基于组学的癌症亚型划分已经成为一种革命性的方法，因为不同级别的组学记录了癌症中多步骤过程的生化产物。本文旨在充分利用多组学数据的潜力来改善癌症亚型划分结果，因此开发了MoCLIM，一种表示学习框架。MoCLIM独立地从不同的组学模式中提取有信息量的特征。通过对不同组学模式之间的对比学习所得到的统一表示，在给定癌症情况下，我们能够将亚型很好地聚类到较低的潜空间中。这种对比可以被解释为在生物网络中观察到的组际推理的投影。在六个癌症数据集上的实验结果表明，我们的方法在较少的高维癌症数据拟合和亚型划分性能方面显著改善。

    Precision medicine fundamentally aims to establish causality between dysregulated biochemical mechanisms and cancer subtypes. Omics-based cancer subtyping has emerged as a revolutionary approach, as different level of omics records the biochemical products of multistep processes in cancers. This paper focuses on fully exploiting the potential of multi-omics data to improve cancer subtyping outcomes, and hence developed MoCLIM, a representation learning framework. MoCLIM independently extracts the informative features from distinct omics modalities. Using a unified representation informed by contrastive learning of different omics modalities, we can well-cluster the subtypes, given cancer, into a lower latent space. This contrast can be interpreted as a projection of inter-omics inference observed in biological networks. Experimental results on six cancer datasets demonstrate that our approach significantly improves data fit and subtyping performance in fewer high-dimensional cancer ins
    
[^110]: 多保真度傅里叶神经算子用于快速建模大规模地质碳储存

    Multi-fidelity Fourier Neural Operator for Fast Modeling of Large-Scale Geological Carbon Storage. (arXiv:2308.09113v1 [stat.ML])

    [http://arxiv.org/abs/2308.09113](http://arxiv.org/abs/2308.09113)

    多保真度傅里叶神经算子用于解决大规模地质碳储存问题，通过利用经济性更高的多保真度训练数据集，能够以与高保真度模型相当的准确性进行预测。

    

    深度学习的代理模型已广泛应用于地质碳储存（GCS）问题，以加快预测储压和二氧化碳云层移动。然而，由于高计算成本，大规模三维问题的可用训练数据始终有限。因此，我们提出使用多保真度傅里叶神经算子来解决大规模GCS问题，利用更具经济性的多保真度训练数据集。傅里叶神经算子具有良好的网格不变性，简化了不同离散数据集之间的迁移学习过程。我们首先在一个GCS储层模型上进行模型有效性测试，该模型被划分为110,000个网格单元。多保真度模型的预测准确度可与高保真度模型的训练进行比较。

    Deep learning-based surrogate models have been widely applied in geological carbon storage (GCS) problems to accelerate the prediction of reservoir pressure and CO2 plume migration. Large amounts of data from physics-based numerical simulators are required to train a model to accurately predict the complex physical behaviors associated with this process. In practice, the available training data are always limited in large-scale 3D problems due to the high computational cost. Therefore, we propose to use a multi-fidelity Fourier Neural Operator to solve large-scale GCS problems with more affordable multi-fidelity training datasets. The Fourier Neural Operator has a desirable grid-invariant property, which simplifies the transfer learning procedure between datasets with different discretization. We first test the model efficacy on a GCS reservoir model being discretized into 110k grid cells. The multi-fidelity model can predict with accuracy comparable to a high-fidelity model trained wi
    
[^111]: 使用在聚类数据上训练的集成模型进行开放集合人脸识别

    Open-set Face Recognition using Ensembles trained on Clustered Data. (arXiv:2308.07445v1 [cs.CV])

    [http://arxiv.org/abs/2308.07445](http://arxiv.org/abs/2308.07445)

    本研究提出了一种使用在聚类数据上训练的集成模型进行开放集合人脸识别的方法，能够准确识别感兴趣的个体，同时有效处理陌生的面孔。实验结果表明即使在大规模图库中也能取得竞争性的性能。

    

    开放集合人脸识别描述了在测试时出现未知的主题，而在训练阶段未见过。它不仅需要准确识别感兴趣的个体的方法，还需要有效处理陌生的面孔的方法。本文详细介绍了一个可扩展的开放集合人脸识别方法，适用于包含数百和数千个主题的图库。它由聚类和一组二进制学习算法组成，用于估计查询人脸样本是否属于人脸图库，并检索其正确的身份。该方法选择最合适的图库主题，并使用集成模型改善预测性能。我们在知名的LFW和YTF基准测试上进行实验。结果表明，即使针对可扩展性，也可以达到竞争性的性能。

    Open-set face recognition describes a scenario where unknown subjects, unseen during the training stage, appear on test time. Not only it requires methods that accurately identify individuals of interest, but also demands approaches that effectively deal with unfamiliar faces. This work details a scalable open-set face identification approach to galleries composed of hundreds and thousands of subjects. It is composed of clustering and an ensemble of binary learning algorithms that estimates when query face samples belong to the face gallery and then retrieves their correct identity. The approach selects the most suitable gallery subjects and uses the ensemble to improve prediction performance. We carry out experiments on well-known LFW and YTF benchmarks. Results show that competitive performance can be achieved even when targeting scalability.
    
[^112]: 自然语言是图表所需要的全部内容

    Natural Language is All a Graph Needs. (arXiv:2308.07134v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.07134](http://arxiv.org/abs/2308.07134)

    本论文提出了一种名为InstructGLM的结构化语言模型算法，该算法将大型语言模型与图表学习问题相结合，旨在探索是否可以用语言模型取代图神经网络作为图表的基础模型。

    

    大规模预训练语言模型的出现，如ChatGPT，已经在人工智能的各个研究领域中引起了革命。基于Transformer的大型语言模型（LLMs）逐渐取代了CNN和RNN，将计算机视觉和自然语言处理领域统一起来。与相对独立存在的数据（如图像、视频或文本）相比，图表是一种包含丰富结构和关系信息的数据类型。同时，作为最具表现力的媒介之一，自然语言在描述复杂结构方面表现出色。然而，将图表学习问题纳入生成式语言建模框架的现有工作仍然非常有限。随着大型语言模型的重要性不断增长，探索LLMs是否也可以替代GNNs成为图表的基础模型变得至关重要。在本文中，我们提出了InstructGLM（结构化语言模型）算法，系统地设计高度可扩展的模型来处理图表学习问题。

    The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scal
    
[^113]: 解决医学影像深度学习中的小型注释数据集问题：对比共同对比学习和掩码自编码器方法在CT扫描卷积模型中的自监督预训练的评估

    Dealing with Small Annotated Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models. (arXiv:2308.06534v1 [cs.CV])

    [http://arxiv.org/abs/2308.06534](http://arxiv.org/abs/2308.06534)

    本研究评估了在医学影像领域使用自监督预训练方法的可行性，比较了共同对比学习和掩码自编码器方法在CT扫描卷积模型中的性能。

    

    医学影像中的深度学习有潜力减少诊断错误的风险、减轻放射科医生的工作负担并加速确诊。训练这样的深度学习模型需要大型且准确的数据集，并且需要为所有训练样本提供注释。然而，在医学影像领域，由于注释的高复杂性、受限的获取方式或疾病的罕见性，特定任务的注释数据集通常很小。为了应对这一挑战，深度学习模型可以使用自监督学习领域的方法，在没有注释的大型图像数据集上进行预训练。在预训练之后，小型的已注释数据集就足以对模型进行特定任务的微调，即所谓的“下游任务”。医学影像中最流行的自监督预训练方法基于共同对比学习。然而，最近的自然图像处理研究表明掩码自编码器方法具有很大的潜力。本研究比较了二者在CT扫描卷积模型中的性能。

    Deep learning in medical imaging has the potential to minimize the risk of diagnostic errors, reduce radiologist workload, and accelerate diagnosis. Training such deep learning models requires large and accurate datasets, with annotations for all training samples. However, in the medical imaging domain, annotated datasets for specific tasks are often small due to the high complexity of annotations, limited access, or the rarity of diseases. To address this challenge, deep learning models can be pre-trained on large image datasets without annotations using methods from the field of self-supervised learning. After pre-training, small annotated datasets are sufficient to fine-tune the models for a specific task, the so-called ``downstream task". The most popular self-supervised pre-training approaches in medical imaging are based on contrastive learning. However, recent studies in natural image processing indicate a strong potential for masked autoencoder approaches. Our work compares sta
    
[^114]: Pareto不变表示学习在多媒体推荐中的应用

    Pareto Invariant Representation Learning for Multimedia Recommendation. (arXiv:2308.04706v1 [cs.IR])

    [http://arxiv.org/abs/2308.04706](http://arxiv.org/abs/2308.04706)

    本文介绍了一种名为Pareto Invariant Representation Learning（PaInvRL）的框架，应用于多媒体推荐。该框架通过学习不变表示和变体表示的同时来缓解通用表示引入的错误相关性问题。从IID-OOD多目标优化的角度，PaInvRL减少了错误相关性对用户偏好的影响。

    

    多媒体推荐涉及个性化排序任务，通常使用通用编码器表示多媒体内容。然而，这些通用表示引入了错误的相关性，无法揭示用户的真实偏好。现有的工作尝试通过学习不变表示来缓解这个问题，但忽视了独立同分布（IID）和非分布（OOD）广义化之间的平衡。本文提出了一个名为Pareto Invariant Representation Learning（PaInvRL）的框架，从IID-OOD多目标优化的角度减少了错误相关性的影响，同时学习不变表示（吸引用户注意的内在因素）和变体表示（其他因素）。具体而言，PaInvRL包括三个迭代执行的模块：（i）非同质识别模块，用于识别反映分布转移

    Multimedia recommendation involves personalized ranking tasks, where multimedia content is usually represented using a generic encoder. However, these generic representations introduce spurious correlations that fail to reveal users' true preferences. Existing works attempt to alleviate this problem by learning invariant representations, but overlook the balance between independent and identically distributed (IID) and out-of-distribution (OOD) generalization. In this paper, we propose a framework called Pareto Invariant Representation Learning (PaInvRL) to mitigate the impact of spurious correlations from an IID-OOD multi-objective optimization perspective, by learning invariant representations (intrinsic factors that attract user attention) and variant representations (other factors) simultaneously. Specifically, PaInvRL includes three iteratively executed modules: (i) heterogeneous identification module, which identifies the heterogeneous environments to reflect distributional shift
    
[^115]: Min-Max优化在延迟下的研究

    Min-Max Optimization under Delays. (arXiv:2307.06886v1 [cs.LG])

    [http://arxiv.org/abs/2307.06886](http://arxiv.org/abs/2307.06886)

    在大规模机器学习中，研究了min-max优化在延迟下的性能。对于简单实例，即使是小的延迟也可能导致Extra-gradient算法发散，因此需要对延迟版本的min-max优化算法进行仔细分析。为此，我们证明了在适当的技术假设下，梯度下降-上升算法在延迟情况下的收敛性和性能。

    

    在通信起重要作用的大规模机器学习问题中，延迟和异步是不可避免的。因此，一些研究团队广泛分析了具有延迟梯度的随机优化问题。但据我们所知，尚无类似的理论可用于min-max优化，这个话题由于在对抗鲁棒性、博弈论和强化学习中的应用而越来越受关注。针对这一差距，我们对带有延迟梯度更新的标准min-max优化算法的性能进行了研究。首先，我们（经验性地）展示了即使是小的延迟也可能导致像Extra-gradient (EG) 这样的杰出算法在简单实例上发散，而在没有延迟的情况下EG可以保证收敛。因此，我们的经验研究表明有必要对延迟版本的min-max优化算法进行仔细分析。相应地，在适当的技术假设下，我们证明了梯度下降 - 上升 (GDA)算法在延迟情况下的收敛性和性能。

    Delays and asynchrony are inevitable in large-scale machine-learning problems where communication plays a key role. As such, several works have extensively analyzed stochastic optimization with delayed gradients. However, as far as we are aware, no analogous theory is available for min-max optimization, a topic that has gained recent popularity due to applications in adversarial robustness, game theory, and reinforcement learning. Motivated by this gap, we examine the performance of standard min-max optimization algorithms with delayed gradient updates. First, we show (empirically) that even small delays can cause prominent algorithms like Extra-gradient (\texttt{EG}) to diverge on simple instances for which \texttt{EG} guarantees convergence in the absence of delays. Our empirical study thus suggests the need for a careful analysis of delayed versions of min-max optimization algorithms. Accordingly, under suitable technical assumptions, we prove that Gradient Descent-Ascent (\texttt{G
    
[^116]: 基于紧核的条件期望估计

    Conditional expectation via compact kernels. (arXiv:2306.10592v1 [stat.ML])

    [http://arxiv.org/abs/2306.10592](http://arxiv.org/abs/2306.10592)

    本文提出了一种基于紧核的算子理论方法来解决条件期望估计问题，在再生核希尔伯特空间中实现，易于实现，且成功应用于实际问题中。

    

    去噪、条件期望和流形学习任务通常可以在寻找两个随机变量积的条件期望的公共环境下表述。本文针对这个更一般的问题，描述了一种算子理论方法来估计条件期望。核积分算子被用作紧致化工具，将估计问题设置为在再生核希尔伯特空间中的线性逆问题。该方程的解被证明对数值逼近是稳定的，从而确保了数据驱动实现的收敛性。总体技术易于实现，还展示了其在一些实际问题中的成功应用。

    The separate tasks of denoising, conditional expectation and manifold learning can often be posed in a common setting of finding the conditional expectations arising from a product of two random variables. This paper focuses on this more general problem and describes an operator theoretic approach to estimating the conditional expectation. Kernel integral operators are used as a compactification tool, to set up the estimation problem as a linear inverse problem in a reproducing kernel Hilbert space. This equation is shown to have solutions that are stable to numerical approximation, thus guaranteeing the convergence of data-driven implementations. The overall technique is easy to implement, and their successful application to some real-world problems are also shown.
    
[^117]: 图加载：令人震惊的简化并行GNN训练，无需中间通信

    Graph Ladling: Shockingly Simple Parallel GNN Training without Intermediate Communication. (arXiv:2306.10466v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10466](http://arxiv.org/abs/2306.10466)

    这项工作提出了一种无需中间通信的简化并行GNN训练方法，通过利用模型汤的原理减轻了GNN扩展过程中的内存瓶颈和可训练性问题。

    

    图在各处都是存在的，而GNN是一类用于图学习的强大神经网络。尽管它们很受欢迎，但通过加深或加宽来扩展GNN还存在着梯度不健康、过度平滑、信息压缩等普遍问题，常常导致次标准的性能。在这项工作中，我们有兴趣探索一种有原则的方法，以不加深或加宽的方式扩展GNN的容量，从而提高其在多个小型和大型图上的性能。受最近引人注目的模型汤现象的启发，该现象表明多个大型预训练模型的微调权重可以合并成更好的最小值，我们提出利用模型汤的基本原理来减轻GNN扩展过程中的内存瓶颈和可训练性问题。更具体地说，我们建议不加深或加宽当前的GNN，而是提出了一种专为GNN定制的以数据为中心的模型汤视角，即构建功能强大的

    Graphs are omnipresent and GNNs are a powerful family of neural networks for learning over graphs. Despite their popularity, scaling GNNs either by deepening or widening suffers from prevalent issues of unhealthy gradients, over-smoothening, information squashing, which often lead to sub-standard performance. In this work, we are interested in exploring a principled way to scale GNNs capacity without deepening or widening, which can improve its performance across multiple small and large graphs. Motivated by the recent intriguing phenomenon of model soups, which suggest that fine-tuned weights of multiple large-language pre-trained models can be merged to a better minima, we argue to exploit the fundamentals of model soups to mitigate the aforementioned issues of memory bottleneck and trainability during GNNs scaling. More specifically, we propose not to deepen or widen current GNNs, but instead present a data-centric perspective of model soups tailored for GNNs, i.e., to build powerfu
    
[^118]: 血压测量技术综述：解决潜在的偏差来源

    A Survey on Blood Pressure Measurement Technologies: Addressing Potential Sources of Bias. (arXiv:2306.08451v2 [physics.med-ph] UPDATED)

    [http://arxiv.org/abs/2306.08451](http://arxiv.org/abs/2306.08451)

    该综述聚焦于带式血压监测技术，强调了由于测量和设备误差、人口统计学数据和体型差异等因素导致的血压测量偏差和方差。研发使用人工智能来纠正误差的新一代带式血压设备是重点发展方向。

    

    临床和流动场景中的定期血压（BP）监测在预防、诊断、治疗和管理心血管疾病中起着至关重要的作用。最近，流动式BP测量设备的广泛采用主要是由于高血压的普遍增加以及其相关风险和临床状况。最近的指南建议将定期BP监测作为常规临床访视甚至在家里进行。 这种增加的BP测量技术利用带式测压方法也带来了重要的关注点，涉及各种设置下报告的BP值的准确性。在这项综述中，我们重点介绍了基于带式测压技术的BP测量方法如何由于测量和设备误差、人口统计学数据和体型差异等因素导致显著偏差和方差。在这些固有的偏差情况下，发展一种新一代的基于带式测压设备，其使用人工智能技术来纠正误差是非常重要的。

    Regular blood pressure (BP) monitoring in clinical and ambulatory settings plays a crucial role in the prevention, diagnosis, treatment, and management of cardiovascular diseases. Recently, the widespread adoption of ambulatory BP measurement devices has been driven predominantly by the increased prevalence of hypertension and its associated risks and clinical conditions. Recent guidelines advocate for regular BP monitoring as part of regular clinical visits or even at home. This increased utilization of BP measurement technologies has brought up significant concerns, regarding the accuracy of reported BP values across settings.  In this survey, focusing mainly on cuff-based BP monitoring technologies, we highlight how BP measurements can demonstrate substantial biases and variances due to factors such as measurement and device errors, demographics, and body habitus. With these inherent biases, the development of a new generation of cuff-based BP devices which use artificial-intelligen
    
[^119]: 基于同态映射的期望完全图表示

    Expectation-Complete Graph Representations with Homomorphisms. (arXiv:2306.05838v1 [cs.LG])

    [http://arxiv.org/abs/2306.05838](http://arxiv.org/abs/2306.05838)

    提出了一种基于同态映射的新型随机图嵌入方法，在期望的多项式时间内计算，能在期望中区分所有非同构图，具有高效的替代能力。

    

    我们研究了一种新颖的随机图嵌入，可以在期望的多项式时间内计算，并且能够在期望中区分所有非同构图。以前的图嵌入具有有限的表达能力，并且要么不能区分所有图，要么不能有效地计算每个图。为了能够在图上近似任意函数，我们对具有递增资源的高效替代方案感兴趣。我们的方法基于 Lov\'asz 对通过同态计数的无限维向量的图同构的表征。我们的实证评估显示了在几个基准图学习任务上具有竞争力的结果。

    We investigate novel random graph embeddings that can be computed in expected polynomial time and that are able to distinguish all non-isomorphic graphs in expectation. Previous graph embeddings have limited expressiveness and either cannot distinguish all graphs or cannot be computed efficiently for every graph. To be able to approximate arbitrary functions on graphs, we are interested in efficient alternatives that become arbitrarily expressive with increasing resources. Our approach is based on Lov\'asz' characterisation of graph isomorphism through an infinite dimensional vector of homomorphism counts. Our empirical evaluation shows competitive results on several benchmark graph learning tasks.
    
[^120]: 在生物医学任务上评估ChatGPT：与精调生成式变压器的零样例比较。

    Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers. (arXiv:2306.04504v1 [cs.CL])

    [http://arxiv.org/abs/2306.04504](http://arxiv.org/abs/2306.04504)

    本文评估了ChatGPT在生物医学任务上的表现，发现在生物数据集训练样本较小时，零样例ChatGPT甚至优于精调生成式变压器模型。由此表明ChatGPT具有在生物医学领域成为有价值工具的潜力。

    

    ChatGPT是OpenAI开发的大型语言模型。尽管其在各种任务上表现出色，但先前的工作尚未研究其在生物医学领域的能力。因此，本文旨在评估ChatGPT在各种基准生物医学任务上的性能，如关系提取、文档分类、问答和摘要。据我们所知，这是首次对ChatGPT在生物医学领域进行全面评估的工作。有趣的是，在训练集较小的生物医学数据集中，基于我们的评估结果，零样例ChatGPT甚至优于先进的精调生成式变压器模型，如BioGPT和BioBART。这表明ChatGPT在大型文本语料库上的预训练使其在生物医学领域具有相当的专业性。我们的发现表明，ChatGPT在生物医学领域具有成为各种任务的有价值工具的潜力。

    ChatGPT is a large language model developed by OpenAI. Despite its impressive performance across various tasks, no prior work has investigated its capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of ChatGPT on various benchmark biomedical tasks, such as relation extraction, document classification, question answering, and summarization. To the best of our knowledge, this is the first work that conducts an extensive evaluation of ChatGPT in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generative transformer models, such as BioGPT and BioBART. This suggests that ChatGPT's pre-training on large text corpora makes it quite specialized even in the biomedical domain. Our findings demonstrate that ChatGPT has the potential to be a valuable tool for various tasks in the biomedical domain that la
    
[^121]: 熵优化输运的最小内在维度缩放

    Minimum intrinsic dimension scaling for entropic optimal transport. (arXiv:2306.03398v1 [math.ST])

    [http://arxiv.org/abs/2306.03398](http://arxiv.org/abs/2306.03398)

    该研究提出了最小内在维度缩放现象，在不做出数据分布假设的情况下，可以应用于各种熵优化输运问题中，以达到更优的结果。

    

    受流形假说的启发，我们对熵优化输运进行精细的统计界限设计，该界限对数据的内在维度敏感，并仅在单一距离尺度下衡量盲区内在维度。我们称之为最小内在维度缩放（MID scaling）现象，它表明仅有这些单尺度内在维度的最小值才控制收敛速度。我们的结果显示，MID scaling是一个通用现象，可以应用于不同的熵优化输运问题，不需要对数据分布进行假设。当一个分布集中在流形上时，我们的结果有更强的对应物。

    Motivated by the manifold hypothesis, which states that data with a high extrinsic dimension may yet have a low intrinsic dimension, we develop refined statistical bounds for entropic optimal transport that are sensitive to the intrinsic dimension of the data. Our bounds involve a robust notion of intrinsic dimension, measured at only a single distance scale depending on the regularization parameter, and show that it is only the minimum of these single-scale intrinsic dimensions which governs the rate of convergence. We call this the Minimum Intrinsic Dimension scaling (MID scaling) phenomenon, and establish MID scaling with no assumptions on the data distributions so long as the cost is bounded and Lipschitz, and for various entropic optimal transport quantities beyond just values, with stronger analogs when one distribution is supported on a manifold. Our results significantly advance the theoretical state of the art by showing that MID scaling is a generic phenomenon, and provide th
    
[^122]: 将神经网络转化为Yoked神经网络以改进ANN结构

    Transforming to Yoked Neural Networks to Improve ANN Structure. (arXiv:2306.02157v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02157](http://arxiv.org/abs/2306.02157)

    本文提出了一种叫做YNN的方法，将同一级别的ANN节点连接在一起形成神经模块，解决了普通ANN无法共享信息的缺陷，显著提高了信息传输和性能。

    

    大部分已经存在的经典人工神经网络（ANN）都被设计成树形结构以模拟神经网络。本文认为，树形结构的连接不足以描述神经网络。同一级别的节点不能连接在一起，即这些神经元不能相互共享信息，这是ANN的一个重大缺陷。我们提出了一种方法，即为同一级别的节点建立双向完全图，将同一级别的节点连接到一起形成神经模块。我们把我们的模型称为YNN。YNN显著促进了信息传输，明显有助于提高该方法的性能。我们的YNN可以更好地模拟神经网络，相比其他ANN方法有着明显的优势。

    Most existing classical artificial neural networks (ANN) are designed as a tree structure to imitate neural networks. In this paper, we argue that the connectivity of a tree is not sufficient to characterize a neural network. The nodes of the same level of a tree cannot be connected with each other, i.e., these neural unit cannot share information with each other, which is a major drawback of ANN. Although ANN has been significantly improved in recent years to more complex structures, such as the directed acyclic graph (DAG), these methods also have unidirectional and acyclic bias for ANN. In this paper, we propose a method to build a bidirectional complete graph for the nodes in the same level of an ANN, which yokes the nodes of the same level to formulate a neural module. We call our model as YNN in short. YNN promotes the information transfer significantly which obviously helps in improving the performance of the method. Our YNN can imitate neural networks much better compared with 
    
[^123]: 通过概率生成函数的贝叶斯离散模型精确推理：概率编程方法

    Exact Bayesian Inference on Discrete Models via Probability Generating Functions: A Probabilistic Programming Approach. (arXiv:2305.17058v1 [cs.PL])

    [http://arxiv.org/abs/2305.17058](http://arxiv.org/abs/2305.17058)

    该论文提出一种精确推理离散统计模型的贝叶斯方法，支持离散采样、连续采样、离散观测、仿射函数、（随机）分支和事件条件。通过概率生成函数实现后验概率、期望、方差和高阶矩的精确计算。该方法性能优于近似蒙特卡洛方法，并避免了近似误差。

    

    我们提出了一种离散统计模型的精确贝叶斯推理方法，即使是对于无限支持和连续先验也可以找到准确的解决方案。为了表达这样的模型，我们引入了一种支持离散和连续采样、离散观测、仿射函数、（随机）分支和事件条件的概率编程语言。我们的关键工具是概率生成函数：它们提供了定义程序的分布的紧凑闭合形式表示，从而实现了后验概率、期望、方差和高阶矩的精确计算。我们的推理方法是可证明正确的、完全自动化的，使用自动微分（特别是泰勒多项式），但不需要计算机代数。我们的实验表明，它在一系列真实世界的例子中的性能与近似蒙特卡洛方法竞争，同时避免了近似误差。

    We present an exact Bayesian inference method for discrete statistical models, which can find exact solutions to many discrete inference problems, even with infinite support and continuous priors. To express such models, we introduce a probabilistic programming language that supports discrete and continuous sampling, discrete observations, affine functions, (stochastic) branching, and conditioning on events. Our key tool is probability generating functions: they provide a compact closed-form representation of distributions that are definable by programs, thus enabling the exact computation of posterior probabilities, expectation, variance, and higher moments. Our inference method is provably correct, fully automated and uses automatic differentiation (specifically, Taylor polynomials), but does not require computer algebra. Our experiments show that its performance on a range of real-world examples is competitive with approximate Monte Carlo methods, while avoiding approximation errors
    
[^124]: LANISTR：从结构化和非结构化数据中进行多模态学习

    LANISTR: Multimodal Learning from Structured and Unstructured Data. (arXiv:2305.16556v1 [cs.LG])

    [http://arxiv.org/abs/2305.16556](http://arxiv.org/abs/2305.16556)

    LANISTR是一个新颖的基于注意力机制的框架，可从结构化和非结构化数据中进行学习，在挑战性数据集上表现优异。

    

    多模态的大规模预训练已经在处理非结构化数据（包括文本、图像、音频和视频）方面展现了令人瞩目的性能提升。但是，现实世界中最常见的情况是结构化（包括表格和时间序列）和非结构化数据的结合，但这一领域尚未得到充分的研究。为此，我们提出了LANISTR，这是一个新颖的基于注意力机制的框架，用于从语言、图像和结构化数据中进行学习。我们引入了一个新的多模态融合模块，并采用基于相似性的多模态遮罩损失，使得LANISTR能够在大规模多模态数据中学习跨模态关系，并在训练和测试时处理缺失的模态。在两个公开可用的具有挑战性的数据集MIMIC-IV和Amazon Product Review上，与最先进的多模态模型相比，LANISTR分别达到了6.47%（AUROC）和高达17.69%（准确度）的绝对提升，并显示出更强的泛化能力。

    Multimodal large-scale pretraining has shown impressive performance gains for unstructured data including language, image, audio, and video. Yet, the scenario most prominent in real-world applications is the existence of combination of structured (including tabular and time-series) and unstructured data, and this has so far been understudied. Towards this end, we propose LANISTR, a novel attention-based framework to learn from LANguage, Image, and STRuctured data. We introduce a new multimodal fusion module with a similarity-based multimodal masking loss that enables LANISTR to learn cross-modal relations from large-scale multimodal data with missing modalities during training and test time. On two publicly available challenging datasets, MIMIC-IV and Amazon Product Review, LANISTR achieves absolute improvements of 6.47% (AUROC) and up to 17.69% (accuracy), respectively, compared to the state-of-the-art multimodal models while showing superior generalization capabilities.
    
[^125]: 使用数据驱动的二次流形进行哈密顿系统正则模型简化

    Symplectic model reduction of Hamiltonian systems using data-driven quadratic manifolds. (arXiv:2305.15490v1 [math.NA])

    [http://arxiv.org/abs/2305.15490](http://arxiv.org/abs/2305.15490)

    本文提出了两种新方法，使用数据驱动的二次流形对高维哈密顿系统进行正则模型简化，从而使得可以更好地表示问题中固有的低维性，并在超出其训练数据范围的设置中提供更高的准确性。

    

    本文提出了两种新方法，使用数据驱动的二次流形对高维哈密顿系统进行正则模型简化。传统的正则模型简化方法采用线性正则子空间表示高维系统状态的减少维度坐标系。虽然这些近似考虑了哈密顿系统正则性质，但是近似的线性性使得无法达到很高的准确性。我们基于最近开发的二次流形提出了两种不同的模型简化方法，每一种方法都具有独特的优点和局限性。在状态近似中加入二次项，这是所提出的方法的核心，使我们能够更好地表示问题中固有的低维性。这两种方法在超出其训练数据范围的设置中发出预测时都是有效的，同时提供了更高的准确性。

    This work presents two novel approaches for the symplectic model reduction of high-dimensional Hamiltonian systems using data-driven quadratic manifolds. Classical symplectic model reduction approaches employ linear symplectic subspaces for representing the high-dimensional system states in a reduced-dimensional coordinate system. While these approximations respect the symplectic nature of Hamiltonian systems, the linearity of the approximation imposes a fundamental limitation to the accuracy that can be achieved. We propose two different model reduction methods based on recently developed quadratic manifolds, each presenting its own advantages and limitations. The addition of quadratic terms in the state approximation, which sits at the heart of the proposed methodologies, enables us to better represent intrinsic low-dimensionality in the problem at hand. Both approaches are effective for issuing predictions in settings well outside the range of their training data while providing mor
    
[^126]: PruMUX：利用模型压缩增强数据复用

    PruMUX: Augmenting Data Multiplexing with Model Compression. (arXiv:2305.14706v1 [cs.LG])

    [http://arxiv.org/abs/2305.14706](http://arxiv.org/abs/2305.14706)

    PruMUX是一种结合了结构化剪枝和数据复用的方法，可在保持准确性的情况下提高BERT-base模型的吞吐量。Auto-PruMUX可以预测修剪和复用的高性能参数，从而提供了一个实用的自动化工具。

    

    随着语言模型的不断扩大，提高其推理效率变得至关重要。 先前的研究调查了模型修剪，知识蒸馏和数据复用等技术，以增加模型的吞吐量而不损失准确性。 在本文中，我们将结构化剪枝和数据复用两种方法结合起来，以增强两种方法获得的加速优势。 我们的方法PruMUX在保持准确性阈值为80％到74％的情况下，与BERT-base模型相比，可获得7.5-29.5倍的吞吐量提高。 我们进一步研究了两种技术中不同参数（例如稀疏性和复用因子）的各种组合，以提供有关准确性和吞吐量之间权衡的综合分析。 然后，我们提出了Auto-PruMUX，这是一种元模型，可以在给定期望的精度损失预算的情况下，预测修剪和复用的高性能参数，从而提供了一个实用的自动化工具。

    As language models increase in size by the day, methods for efficient inference are critical to leveraging their capabilities for various applications. Prior work has investigated techniques like model pruning, knowledge distillation, and data multiplexing to increase model throughput without sacrificing accuracy. In this paper, we combine two such methods -structured pruning and data multiplexing -- to compound the speedup gains obtained by either method. Our approach, PruMUX, obtains up to 7.5-29.5X throughput improvement over BERT-base model with accuracy threshold from 80% to 74%. We further study various combinations of parameters (such as sparsity and multiplexing factor) in the two techniques to provide a comprehensive analysis of the tradeoff between accuracy and throughput in the resulting models. We then propose Auto-PruMUX, a meta-level model that can predict the high-performance parameters for pruning and multiplexing given a desired accuracy loss budget, providing a prac
    
[^127]: 数据集蒸馏研究综述: 方法、应用和未来方向

    A Survey on Dataset Distillation: Approaches, Applications and Future Directions. (arXiv:2305.01975v1 [cs.LG])

    [http://arxiv.org/abs/2305.01975](http://arxiv.org/abs/2305.01975)

    数据集蒸馏在机器学习中越来越重要。该方法可以通过合成高信息密度的数据集来支持持续学习、神经架构搜索和隐私保护。这篇综述性调查论文提出了一种分类方法，对现有方法进行了特征化，并系统回顾了数据模态和相关应用，同时总结了挑战并讨论了未来方向。

    

    随着训练集的不断增长和训练最先进的模型成本越来越高，数据集蒸馏在机器学习中越来越受到关注。通过合成高信息密度的数据集，数据集蒸馏提供了一系列潜在应用，包括支持持续学习、神经架构搜索和隐私保护。尽管近年来有了一些进展，但我们缺乏对方法和应用的全面理解。我们的调查旨在填补这一空白，首先提出一种数据集蒸馏的分类方法，对现有方法进行特征化，然后系统地回顾数据模态和相关应用。此外，我们总结了挑战并讨论了这一研究领域的未来方向。

    Dataset distillation is attracting more attention in machine learning as training sets continue to grow and the cost of training state-of-the-art models becomes increasingly high. By synthesizing datasets with high information density, dataset distillation offers a range of potential applications, including support for continual learning, neural architecture search, and privacy protection. Despite recent advances, we lack a holistic understanding of the approaches and applications. Our survey aims to bridge this gap by first proposing a taxonomy of dataset distillation, characterizing existing approaches, and then systematically reviewing the data modalities, and related applications. In addition, we summarize the challenges and discuss future directions for this field of research.
    
[^128]: 实现高效和全面的城市时空预测：一个统一的库和性能基准

    Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction: A Unified Library and Performance Benchmark. (arXiv:2304.14343v1 [cs.LG])

    [http://arxiv.org/abs/2304.14343](http://arxiv.org/abs/2304.14343)

    本研究提出了一种称为原子文件的统一空间时间数据存储格式，开发了一个名为LibCity的开源库，重新构建了65个空间时间预测模型，并收集了55个空间时间数据集。同时还提出了城市时空预测模型的性能基准，为这一领域提供了一个可靠的评估工具。

    

    随着深度学习技术的不断推进和城市时空数据的积累，越来越多的深度学习模型被提出来解决城市时空预测问题。然而，现有领域存在许多限制，包括开放数据以各种格式存在，使用困难，极少数论文公开其代码和数据，以及开源模型经常使用不同的框架和平台，使得比较具有挑战性。迫切需要一个统一的框架来实施和评估这些方法。为解决这些问题，我们提供了一个城市时空预测的综合评估，并提出了一种称为原子文件的统一空间时间数据存储格式。我们还提出了一个名为LibCity的开源库，为研究人员提供了一个可靠的实验工具和一个方便的开发框架。在这个库中，我们已经重新构建了65个空间时间预测模型，并收集了55个空间时间数据集。此外，我们还引入了一个城市时空预测模型性能基准，包括效率和有效性度量，以进行公平比较。在这个基准上的实验结果证明了我们提出的统一库和基准的有用性和有效性。

    As deep learning technology advances and more urban spatial-temporal data accumulates, an increasing number of deep learning models are being proposed to solve urban spatial-temporal prediction problems. However, there are limitations in the existing field, including open-source data being in various formats and difficult to use, few papers making their code and data openly available, and open-source models often using different frameworks and platforms, making comparisons challenging. A standardized framework is urgently needed to implement and evaluate these methods. To address these issues, we provide a comprehensive review of urban spatial-temporal prediction and propose a unified storage format for spatial-temporal data called atomic files. We also propose LibCity, an open-source library that offers researchers a credible experimental tool and a convenient development framework. In this library, we have reproduced 65 spatial-temporal prediction models and collected 55 spatial-temp
    
[^129]: 压缩与否——自监督学习与信息论:一篇综述

    To Compress or Not to Compress -- Self-Supervised Learning and Information Theory: A Review. (arXiv:2304.09355v1 [cs.LG])

    [http://arxiv.org/abs/2304.09355](http://arxiv.org/abs/2304.09355)

    本文从信息论的角度回顾了各种自监督学习方法，并提出了一个正式的“自监督信息理论学习问题”统一框架。此外，讨论了压缩性和压缩算法在自监督学习中的作用，并凸显了潜在的未来方向。

    

    深度神经网络在监督学习任务中表现出了卓越的性能，但需要大量的标注数据。自监督学习提供了一个替代范例，使得模型可以在没有明确标签的情况下学习。信息论在理解和优化深度神经网络方面起着关键作用。特别地，信息瓶颈原则被应用于在监督设置中优化压缩和相关信息保存之间的权衡。然而，自监督学习中的最佳信息目标仍然不清楚。在本文中，我们从信息论的角度回顾了各种自监督学习方法，并提出了一个正式的“自监督信息理论学习问题”统一框架。我们将现有研究融合成一个一致的框架，研究了最近的自监督方法，并确定了研究机会和挑战。此外，我们还讨论了压缩性和压缩算法在自监督学习中的作用，并凸显了潜在的未来方向。

    Deep neural networks have demonstrated remarkable performance in supervised learning tasks but require large amounts of labeled data. Self-supervised learning offers an alternative paradigm, enabling the model to learn from data without explicit labels. Information theory has been instrumental in understanding and optimizing deep neural networks. Specifically, the information bottleneck principle has been applied to optimize the trade-off between compression and relevant information preservation in supervised settings. However, the optimal information objective in self-supervised learning remains unclear. In this paper, we review various approaches to self-supervised learning from an information-theoretic standpoint and present a unified framework that formalizes the \textit{self-supervised information-theoretic learning problem}. We integrate existing research into a coherent framework, examine recent self-supervised methods, and identify research opportunities and challenges. Moreove
    
[^130]: BadVFL: 竖直联邦学习中的后门攻击

    BadVFL: Backdoor Attacks in Vertical Federated Learning. (arXiv:2304.08847v1 [cs.LG])

    [http://arxiv.org/abs/2304.08847](http://arxiv.org/abs/2304.08847)

    本文聚焦于竖直联邦学习中的后门攻击的鲁棒性问题，提出了一种新的后门攻击框架BadVFL，可以有效地将后门注入VFL的训练过程中，成功率高并且误分类率很低。

    

    联邦学习（FL）使多个参与者可以在不共享其数据的情况下协作地训练机器学习模型；相反，他们在本地训练自己的模型，并将更新发送到中央服务器进行聚合。根据数据在参与者之间的分布方式，FL可以分为水平（HFL）和竖直（VFL）。在VFL中，参与者共享相同的训练实例集，但仅托管整个特征空间的不同和非重叠子集。而在HFL中，每个参与者共享相同的特征集，而训练集被分为本地拥有的训练数据子集。尽管VFL越来越多地应用于金融欺诈检测等应用程序中，但很少有工作分析其安全性。本文重点研究VFL的鲁棒性，特别是后门攻击，其中对手试图在训练过程中操纵聚合模型以触发错误分类。在VFL上执行后门攻击可以创建严重的安全和隐私问题，因为它可以允许攻击者有针对性地控制模型预测的结果。为此，我们提出了一种新的VFL后门攻击框架，称为BadVFL，它可以有效地将后门注入VFL的训练过程中。我们的方法不仅考虑数据的特征，还适应于VFL中使用的不同中心度量。在三个数据集上进行的大量实验证明了我们的方法在保持干净数据的低失真的同时实现了高攻击成功率的有效性。

    Federated learning (FL) enables multiple parties to collaboratively train a machine learning model without sharing their data; rather, they train their own model locally and send updates to a central server for aggregation. Depending on how the data is distributed among the participants, FL can be classified into Horizontal (HFL) and Vertical (VFL). In VFL, the participants share the same set of training instances but only host a different and non-overlapping subset of the whole feature space. Whereas in HFL, each participant shares the same set of features while the training set is split into locally owned training data subsets.  VFL is increasingly used in applications like financial fraud detection; nonetheless, very little work has analyzed its security. In this paper, we focus on robustness in VFL, in particular, on backdoor attacks, whereby an adversary attempts to manipulate the aggregate model during the training process to trigger misclassifications. Performing backdoor attack
    
[^131]: 解决面部验证边缘案例：深度分析和人机融合方法

    Tackling Face Verification Edge Cases: In-Depth Analysis and Human-Machine Fusion Approach. (arXiv:2304.08134v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2304.08134](http://arxiv.org/abs/2304.08134)

    本文研究了临近边缘案例的面部验证问题，发现结合人机决策可以进一步提高最先进的面部验证系统在各种基准数据集上的性能。

    

    目前，面部识别系统在几个数据集上已经超过了人类表现。然而，仍然存在一些机器无法正确分类的边缘案例。本文研究了机器和人操作员在面部验证任务中的组合效应。首先，我们仔细研究了几个最先进模型的边缘案例，以发现常见数据集的挑战性设置。然后，我们对这些选定任务中的60个参与者进行了一项人类研究，并提供了广泛的分析。最后，我们展示了将机器和人类决策结合起来，可以进一步提高最先进的面部验证系统在各种基准数据集上的性能。代码和数据可在GitHub上公开获取。

    Nowadays, face recognition systems surpass human performance on several datasets. However, there are still edge cases that the machine can't correctly classify. This paper investigates the effect of a combination of machine and human operators in the face verification task. First, we look closer at the edge cases for several state-of-the-art models to discover common datasets' challenging settings. Then, we conduct a study with 60 participants on these selected tasks with humans and provide an extensive analysis. Finally, we demonstrate that combining machine and human decisions can further improve the performance of state-of-the-art face verification systems on various benchmark datasets. Code and data are publicly available on GitHub.
    
[^132]: HyperTab: 基于超网络的小型表格数据深度学习方法

    HyperTab: Hypernetwork Approach for Deep Learning on Small Tabular Datasets. (arXiv:2304.03543v1 [cs.LG])

    [http://arxiv.org/abs/2304.03543](http://arxiv.org/abs/2304.03543)

    HyperTab是一种基于超网络结合了随机森林和神经网络优点的小型表格数据深度学习方法，使用每个特定低维视图处理数据，虚拟增加训练样本数量，避免过度拟合。

    

    深度学习在许多领域取得了惊人的表现，例如计算机视觉和自然语言处理，但它在表格数据集上相对传统浅层方法的优势仍然值得商榷。在小型数据集（小于1k个样本）上超过树状集成（如XGBoost或随机森林）的表现尤其具有挑战性。为了解决这个问题，我们引入了HyperTab，这是一种基于超网络解决表格数据集小样本问题的方法。通过将随机森林和神经网络的优点结合起来，HyperTab生成了一个神经网络集合，其中每个目标模型专门处理数据的特定低维视图。由于每个视图扮演数据增强的角色，我们在保持可训练参数数量不变的情况下，虚拟增加了训练样本数量，从而避免了过度拟合。我们对40多个大小不同的表格数据集对HyperTab进行了评估。

    Deep learning has achieved impressive performance in many domains, such as computer vision and natural language processing, but its advantage over classical shallow methods on tabular datasets remains questionable. It is especially challenging to surpass the performance of tree-like ensembles, such as XGBoost or Random Forests, on small-sized datasets (less than 1k samples). To tackle this challenge, we introduce HyperTab, a hypernetwork-based approach to solving small sample problems on tabular datasets. By combining the advantages of Random Forests and neural networks, HyperTab generates an ensemble of neural networks, where each target model is specialized to process a specific lower-dimensional view of the data. Since each view plays the role of data augmentation, we virtually increase the number of training samples while keeping the number of trainable parameters unchanged, which prevents model overfitting. We evaluated HyperTab on more than 40 tabular datasets of a varying number
    
[^133]: 基于层级自回归语言模型合成极高维长期电子健康记录

    Synthesize Extremely High-dimensional Longitudinal Electronic Health Records via Hierarchical Autoregressive Language Model. (arXiv:2304.02169v1 [cs.LG])

    [http://arxiv.org/abs/2304.02169](http://arxiv.org/abs/2304.02169)

    此论文提出了一种名为HALO的方法，它是一个层级自回归模型，可以生成高保真、细粒度电子健康记录数据，而这些数据可以用于训练准确的ML模型，且无需涉及隐私问题。

    

    合成的电子健康记录(EHRs)能够在机器学习(ML)和统计分析中作为真实EHRs的替代品，既真实又保护隐私。然而，由于高维数据的内在复杂性，以其原始高度维形式生成高保真、细粒度电子健康记录(EHR)数据对现有方法构成了挑战。本文提出了一种名为“Hierarchical Autoregressive Language mOdel (HALO)”的方法，用于生成纵向高维EHR数据，该方法保留了真实EHR的统计特性，可以用于训练准确的ML模型而不涉及隐私问题。我们的HALO方法被设计为一个层级自回归模型，生成一组针对医学代码、临床就诊和病人记录的概率密度函数，可以在其原始未聚合形式下生成真实的EHR数据，无需进行变量选择或聚合。此外，我们的模型还产生大量的随机样本，以提供复杂度较低但仍有意义的EHR数据。

    Synthetic electronic health records (EHRs) that are both realistic and preserve privacy can serve as an alternative to real EHRs for machine learning (ML) modeling and statistical analysis. However, generating high-fidelity and granular electronic health record (EHR) data in its original, highly-dimensional form poses challenges for existing methods due to the complexities inherent in high-dimensional data. In this paper, we propose Hierarchical Autoregressive Language mOdel (HALO) for generating longitudinal high-dimensional EHR, which preserve the statistical properties of real EHR and can be used to train accurate ML models without privacy concerns. Our HALO method, designed as a hierarchical autoregressive model, generates a probability density function of medical codes, clinical visits, and patient records, allowing for the generation of realistic EHR data in its original, unaggregated form without the need for variable selection or aggregation. Additionally, our model also produc
    
[^134]: 使用线性互补编程的时序符合预测区间

    Conformal Prediction Regions for Time Series using Linear Complementarity Programming. (arXiv:2304.01075v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2304.01075](http://arxiv.org/abs/2304.01075)

    本文提出了一种基于优化的方法，通过将预测误差参数化为多个时间步长，以找到不保守的预测区间，实现在使用学习启用的时间序列预测器进行长期规划和验证。

    

    符合预测是一种用于生成机器学习模型预测区间的统计工具，其具有高概率有效性。然而，将符合预测应用于时间序列数据会导致保守的预测区间。本文提出了一种基于优化的方法来减少这种保守性，以便在使用学习启用的时间序列预测器进行长期规划和验证。我们将预测误差参数化为多个时间步长，通过对额外数据集上的参数进行优化，找到了不保守的预测区间。我们表明，该问题可以被看作是一个混合整数线性互补规划（MILCP），我们将其放宽为一个线性互补规划（LCP）。

    Conformal prediction is a statistical tool for producing prediction regions of machine learning models that are valid with high probability. However, applying conformal prediction to time series data leads to conservative prediction regions. In fact, to obtain prediction regions over $T$ time steps with confidence $1-\delta$, {previous works require that each individual prediction region is valid} with confidence $1-\delta/T$. We propose an optimization-based method for reducing this conservatism to enable long horizon planning and verification when using learning-enabled time series predictors. Instead of considering prediction errors individually at each time step, we consider a parameterized prediction error over multiple time steps. By optimizing the parameters over an additional dataset, we find prediction regions that are not conservative. We show that this problem can be cast as a mixed integer linear complementarity program (MILCP), which we then relax into a linear complementa
    
[^135]: 《人口统计平等检查员：通过解释空间进行公平审核》

    Demographic Parity Inspector: Fairness Audits via the Explanation Space. (arXiv:2303.08040v1 [cs.LG])

    [http://arxiv.org/abs/2303.08040](http://arxiv.org/abs/2303.08040)

    这篇论文提出了一种基于解释空间的算法方法，测量分组人口统计学平等的违规情况，并允许我们检查组间歧视的原因，提高了审计公平性的敏感度。

    

    即使具有最好的意图，机器学习方法也可能延续、放大甚至创造社会偏见。衡量机器学习模型的歧视性（非歧视性）的方法已被提出。然而，导致歧视效果的受保护属性的代理仍然是一个具有挑战性的问题。我们提出了一种新的算法方法，它可以测量分组人口统计学平等的违规情况，并允许我们检查组间歧视的原因。我们的方法依赖于一种新颖的思想，即基于解释空间对模型对受保护属性的依赖度进行测量，解释空间是一种提供比输入数据或预测分布的原始空间更敏感审计的信息空间，从而允许断言理论上的人口统计审核保证。我们提供了数学分析、合成样例和实际数据的实验评估。我们还发布了一个开源的Pytorch实现和一个易于使用的Web应用程序。

    Even if deployed with the best intentions, machine learning methods can perpetuate, amplify or even create social biases. Measures of (un-)fairness have been proposed as a way to gauge the (non-)discriminatory nature of machine learning models. However, proxies of protected attributes causing discriminatory effects remain challenging to address. In this work, we propose a new algorithmic approach that measures group-wise demographic parity violations and allows us to inspect the causes of inter-group discrimination. Our method relies on the novel idea of measuring the dependence of a model on the protected attribute based on the explanation space, an informative space that allows for more sensitive audits than the primary space of input data or prediction distributions, and allowing for the assertion of theoretical demographic parity auditing guarantees. We provide a mathematical analysis, synthetic examples, and experimental evaluation of real-world data. We release an open-source Pyt
    
[^136]: Uni-RXN: 一种统一的框架，弥合化学反应Pretraining和条件分子生成之间的差距

    Uni-RXN: A Unified Framework Bridging the Gap between Chemical Reaction Pretraining and Conditional Molecule Generation. (arXiv:2303.06965v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06965](http://arxiv.org/abs/2303.06965)

    本文提出了Uni-RXN框架，在化学反应Pretraining和分子生成任务中都取得了最先进的结果。通过具备化学知识，克服了当前分子生成模型仅依赖少量反应模板的限制，生成质量高、可合成的药物类分子结构。

    

    化学反应是药物设计和有机化学研究的基本构建模块。近年来，对于一个可以有效捕捉化学反应基本规则的大规模深度学习框架的需求不断增长。在本文中，我们提出了一个统一的框架，解决了反应表示学习和分子生成任务，允许更整体的方法。受有机化学机制的启发，我们开发了一种新的预训练框架，使我们能够将归纳偏见纳入模型。我们的框架在具有挑战性的下游任务上取得了最先进的结果。通过具备化学知识，该框架可应用于基于反应的生成模型，克服了当前分子生成模型仅依赖少量反应模板的限制。在广泛的实验中，我们的模型生成了高质量的可合成药物类分子结构。

    Chemical reactions are the fundamental building blocks of drug design and organic chemistry research. In recent years, there has been a growing need for a large-scale deep-learning framework that can efficiently capture the basic rules of chemical reactions. In this paper, we have proposed a unified framework that addresses both the reaction representation learning and molecule generation tasks, which allows for a more holistic approach. Inspired by the organic chemistry mechanism, we develop a novel pretraining framework that enables us to incorporate inductive biases into the model. Our framework achieves state-of-the-art results on challenging downstream tasks. By possessing chemical knowledge, this framework can be applied to reaction-based generative models, overcoming the limitations of current molecule generation models that rely on a small number of reaction templates. In the extensive experiments, our model generates synthesizable drug-like structures of high quality. Overall,
    
[^137]: 预训练GAN和VAE的特征消除

    Feature Unlearning for Pre-trained GANs and VAEs. (arXiv:2303.05699v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.05699](http://arxiv.org/abs/2303.05699)

    本文提出了一种从预训练的GAN和VAE模型中消除特定特征的方法，并通过实验证明了方法的有效性。

    

    我们解决了从预训练的图像生成模型（GAN和VAE）中消除特征的问题。与常见的消除任务不同，我们的目标是从预训练的生成模型中消除特定的特征，例如面部图像中的发型。由于目标特征仅出现在图像的局部区域中，从预训练模型中消除整个图像可能导致失去图像剩余区域中的其他细节。为了指定要消除的特征，我们收集包含目标特征的随机生成图像。然后，我们识别与目标特征对应的潜在表示，并使用表示来微调预训练模型。通过对MNIST和CelebA数据集进行实验，我们展示了成功删除目标特征同时保持原始模型的可信度。进一步的对抗性攻击实验证明了消除后的模型的鲁棒性。

    We tackle the problem of feature unlearning from a pre-trained image generative model: GANs and VAEs. Unlike a common unlearning task where an unlearning target is a subset of the training set, we aim to unlearn a specific feature, such as hairstyle from facial images, from the pre-trained generative models. As the target feature is only presented in a local region of an image, unlearning the entire image from the pre-trained model may result in losing other details in the remaining region of the image. To specify which features to unlearn, we collect randomly generated images that contain the target features. We then identify a latent representation corresponding to the target feature and then use the representation to fine-tune the pre-trained model. Through experiments on MNIST and CelebA datasets, we show that target features are successfully removed while keeping the fidelity of the original models. Further experiments with an adversarial attack show that the unlearned model is mo
    
[^138]: 使用张量和深度生成模型的量化无线电图估计

    Quantized Radio Map Estimation Using Tensor and Deep Generative Models. (arXiv:2303.01770v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2303.01770](http://arxiv.org/abs/2303.01770)

    本文提出了一个量化无线电图估计的框架，利用张量和深度生成模型的方法，扩展了现有的无线电图估计方法，并提出了一种基于最大似然估计的框架来实现从高度量化的传感器测量值中恢复无线电地图。

    

    频谱制图（SC），也称为无线电图估计（RME），旨在从有限的传感器测量数据中制作多领域（例如频率和空间）无线电功率传播图。最近的研究表明，使用低维模型（如块状张量分解（BTD）模型和某些深度生成模型（DGM）），可以通过理论保证恢复无线电地图。然而，这些现有的可证明SC方法假设传感器将实值（全分辨率）测量值发送至融合中心，这是不现实的。本文提出了一个量化SC框架，将基于BTD和基于DGM的SC推广到使用高度量化的传感器测量值的情况。提出了一个基于最大似然估计（MLE）的SC框架，在高斯量化器的作用下对无线电地图的可恢复性进行了刻画。

    Spectrum cartography (SC), also known as radio map estimation (RME), aims at crafting multi-domain (e.g., frequency and space) radio power propagation maps from limited sensor measurements. While early methods often lacked theoretical support, recent works have demonstrated that radio maps can be provably recovered using low-dimensional models -- such as the block-term tensor decomposition (BTD) model and certain deep generative models (DGMs) -- of the high-dimensional multi-domain radio signals. However, these existing provable SC approaches assume that sensors send real-valued (full-resolution) measurements to the fusion center, which is unrealistic. This work puts forth a quantized SC framework that generalizes the BTD and DGM-based SC to scenarios where heavily quantized sensor measurements are used. A maximum likelihood estimation (MLE)-based SC framework under a Gaussian quantizer is proposed. Recoverability of the radio map using the MLE criterion are characterized under realist
    
[^139]: 基于稀疏高斯过程的连续和离散空间的回归传感器放置优化

    Efficient Sensor Placement from Regression with Sparse Gaussian Processes in Continuous and Discrete Spaces. (arXiv:2303.00028v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.00028](http://arxiv.org/abs/2303.00028)

    本文提出了一种用稀疏高斯过程解决连续空间下传感器放置问题的方法，避免离散化环境并降低了计算复杂度，可通过贪婪算法找到好的解决方案。

    

    本文提出了一种基于稀疏高斯过程方法的传感器放置方案，用于监测温度、降水等空间（或时空）相关现象。与现有的基于高斯过程的传感器放置方法不同，我们将已知内核函数参数的稀疏高斯过程拟合到环境中随机采样的未标记位置，并通过学习得到的诱导点来解决连续空间的传感器放置问题。使用稀疏高斯过程避免了对环境进行离散化，并将计算复杂度从立方级别降低到线性级别。在候选传感器放置点集合的限制下，我们可以使用贪婪顺序选择算法来找到较好的解决方案。

    We present a novel approach based on sparse Gaussian processes (SGPs) to address the sensor placement problem for monitoring spatially (or spatiotemporally) correlated phenomena such as temperature and precipitation. Existing Gaussian process (GP) based sensor placement approaches use GPs with known kernel function parameters to model a phenomenon and subsequently optimize the sensor locations in a discretized representation of the environment. In our approach, we fit an SGP with known kernel function parameters to randomly sampled unlabeled locations in the environment and show that the learned inducing points of the SGP inherently solve the sensor placement problem in continuous spaces. Using SGPs avoids discretizing the environment and reduces the computation cost from cubic to linear complexity. When restricted to a candidate set of sensor placement locations, we can use greedy sequential selection algorithms on the SGP's optimization bound to find good solutions. We also present a
    
[^140]: 通过$f$-差分隐私打破通信-隐私-准确性的权衡

    Breaking the Communication-Privacy-Accuracy Tradeoff with $f$-Differential Privacy. (arXiv:2302.09624v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2302.09624](http://arxiv.org/abs/2302.09624)

    本文研究了具有隐私问题和有限通信能力的多个用户的协作数据分析问题，在局部差分隐私保证的视角下，我们通过推导离散值机制的紧密$f$-差分隐私保证，进一步研究了隐私放大的稀疏化方法。

    

    本文考虑了一个联邦数据分析问题，其中服务器协调具有隐私问题和有限通信能力的多个用户的协作数据分析。通常采用的压缩方案在改善通信效率的同时引入了局部数据的信息损失，但这些离散值机制是否提供了任何隐私保护仍然是一个开放问题。在本文中，我们通过$f$-差分隐私（DP）的视角研究了具有有限输出空间的离散值机制的局部差分隐私保证。具体而言，我们通过推导出各种离散值机制的紧密$f$-差分隐私保证，包括用于隐私保护的二项噪声和二项机制以及用于数据压缩的基于符号的方法，进一步研究了稀疏化对隐私的放大，并提出了解决的方法。

    We consider a federated data analytics problem in which a server coordinates the collaborative data analysis of multiple users with privacy concerns and limited communication capability. The commonly adopted compression schemes introduce information loss into local data while improving communication efficiency, and it remains an open problem whether such discrete-valued mechanisms provide any privacy protection. In this paper, we study the local differential privacy guarantees of discrete-valued mechanisms with finite output space through the lens of $f$-differential privacy (DP). More specifically, we advance the existing literature by deriving tight $f$-DP guarantees for a variety of discrete-valued mechanisms, including the binomial noise and the binomial mechanisms that are proposed for privacy preservation, and the sign-based methods that are proposed for data compression, in closed-form expressions. We further investigate the amplification in privacy by sparsification and propose
    
[^141]: 关于PINNs在训练域之外的泛化及其影响的超参数

    On the Generalization of PINNs outside the training domain and the Hyperparameters influencing it. (arXiv:2302.07557v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07557](http://arxiv.org/abs/2302.07557)

    本研究对PINN在训练域之外的预测行为进行了实证分析，并评估了算法设置对其泛化能力的影响。结果提供了有见地且有时直观的观点。

    

    物理知识驱动神经网络（PINNs）是一种神经网络架构，通过训练来模拟微分方程的解，而无需解决数据。由于其灵活和有前景的设置，PINNs目前在科学文献中得到广泛应用。然而，目前可用的研究很少提供实际的研究，旨在更好地定量理解这种架构及其功能。在本文中，我们对PINN在其训练域之外的预测行为进行了实证分析。主要目标是研究PINN在提供一致预测的情况下的场景。随后，我们评估了PINNs的算法设置是否会影响其泛化能力，并展示了对预测的相应影响。本研究的结果提供了有见地且有时直观的观点，对于这种架构的相关性可能是非常重要的。

    Physics-Informed Neural Networks (PINNs) are Neural Network architectures trained to emulate solutions of differential equations without the necessity of solution data. They are currently ubiquitous in the scientific literature due to their flexible and promising settings. However, very little of the available research provides practical studies that aim for a better quantitative understanding of such architecture and its functioning. In this paper, we perform an empirical analysis of the behavior of PINN predictions outside their training domain. The primary goal is to investigate the scenarios in which a PINN can provide consistent predictions outside the training area. Thereinafter, we assess whether the algorithmic setup of PINNs can influence their potential for generalization and showcase the respective effect on the prediction. The results obtained in this study returns insightful and at times counterintuitive perspectives which can be highly relevant for architectures which com
    
[^142]: 通用士兵：利用通用对抗扰动来检测后门攻击

    Universal Soldier: Using Universal Adversarial Perturbations for Detecting Backdoor Attacks. (arXiv:2302.00747v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00747](http://arxiv.org/abs/2302.00747)

    本论文提出了一种利用通用对抗扰动（UAP）检测后门攻击的方法。通过观察UAPs生成的方式，我们发现后门模型只需要较少的扰动即可欺骗模型，而干净模型需要更多扰动。这一发现可以用来区分干净模型和后门模型。

    

    深度学习模型在许多机器学习任务中表现出色，但是它们面临着诸如对抗示例和中毒（后门）攻击等与安全相关的问题。深度学习模型可能通过使用带有后门数据的训练或修改内部网络参数来中毒。然后，当接收到干净输入时，后门模型表现如预期，但是当接收到带有预先设计的"触发器"的后门输入时，它会错误分类。不幸的是，如果没有事先了解触发器，很难区分干净模型和后门模型。本文提出了一种后门检测方法，利用一种特殊类型的对抗攻击——通用对抗扰动（UAP）及其与后门触发器的相似之处。我们观察到一个直观的现象：从后门模型生成的UAP比从干净模型生成的UAP需要更少的扰动来引导模型误导。后门模型的UAP倾向于利用网络漏洞。

    Deep learning models achieve excellent performance in numerous machine learning tasks. Yet, they suffer from security-related issues such as adversarial examples and poisoning (backdoor) attacks. A deep learning model may be poisoned by training with backdoored data or by modifying inner network parameters. Then, a backdoored model performs as expected when receiving a clean input, but it misclassifies when receiving a backdoored input stamped with a pre-designed pattern called "trigger". Unfortunately, it is difficult to distinguish between clean and backdoored models without prior knowledge of the trigger. This paper proposes a backdoor detection method by utilizing a special type of adversarial attack, universal adversarial perturbation (UAP), and its similarities with a backdoor trigger. We observe an intuitive phenomenon: UAPs generated from backdoored models need fewer perturbations to mislead the model than UAPs from clean models. UAPs of backdoored models tend to exploit the sh
    
[^143]: 基于生物信息学的机器学习中的Anderson加速

    Anderson Acceleration For Bioinformatics-Based Machine Learning. (arXiv:2302.00347v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00347](http://arxiv.org/abs/2302.00347)

    这项研究在经典机器学习分类器中探索了Anderson加速的有效性，并通过使用AA，达到了显著提高收敛速度和减小训练误差的效果。

    

    Anderson加速（AA）是一种用于加快迭代算法收敛速度的著名方法，应用于深度学习和优化等各个领域。尽管在这些领域中AA很受欢迎，但其在经典机器学习分类器中的有效性尚未得到彻底研究。尤其是对于表格数据，深度学习模型面临着独特的挑战，而经典机器学习模型在这些场景中表现更好。然而，这些模型的收敛性分析受到了有限的关注。为了填补这一研究空白，我们实现了一种支持向量机（SVM）分类器变种，结合了AA以加速收敛。我们评估了在多个生物学领域的数据集上使用和不使用Anderson加速的SVM的性能，并证明在迭代次数增加时，使用AA显著提高了收敛速度，并减小了训练误差。我们的研究结果提供了一个。

    Anderson acceleration (AA) is a well-known method for accelerating the convergence of iterative algorithms, with applications in various fields including deep learning and optimization. Despite its popularity in these areas, the effectiveness of AA in classical machine learning classifiers has not been thoroughly studied. Tabular data, in particular, presents a unique challenge for deep learning models, and classical machine learning models are known to perform better in these scenarios. However, the convergence analysis of these models has received limited attention. To address this gap in research, we implement a support vector machine (SVM) classifier variant that incorporates AA to speed up convergence. We evaluate the performance of our SVM with and without Anderson acceleration on several datasets from the biology domain and demonstrate that the use of AA significantly improves convergence and reduces the training loss as the number of iterations increases. Our findings provide a
    
[^144]: BallGAN: 带有球形背景的3D感知图像合成

    BallGAN: 3D-aware Image Synthesis with a Spherical Background. (arXiv:2301.09091v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.09091](http://arxiv.org/abs/2301.09091)

    BallGAN是一个新颖的3D感知GAN框架，通过将背景近似为球形表面，并使用特定约束来减少背景自由度，可以产生更合理的3D几何。

    

    3D感知的生成对抗网络（GAN）旨在合成逼真的3D场景，以便可以以任意角度进行渲染以产生图像。尽管以前的方法可以产生逼真的图像，但它们在训练不稳定或存在不自然的3D几何解决方案方面存在问题。我们假设3D几何在约束不足的情况下是不确定的，即仅将其分类为真实图像对于鉴别器来说是不够的。为了解决这个问题，我们提出将背景近似为球形表面，并将场景表示为放置在球体中的前景和薄球形背景的联合。这样可以减少背景场的自由度。因此，我们修改了体渲染方程，并加入了专用的约束，设计了一种名为BallGAN的新型3D感知GAN框架。 BallGAN具有以下多个优点。1）它产生了更合理的3D几何；场景在不同视角下的图像具有更好的光度。

    3D-aware GANs aim to synthesize realistic 3D scenes such that they can be rendered in arbitrary perspectives to produce images. Although previous methods produce realistic images, they suffer from unstable training or degenerate solutions where the 3D geometry is unnatural. We hypothesize that the 3D geometry is underdetermined due to the insufficient constraint, i.e., being classified as real image to the discriminator is not enough. To solve this problem, we propose to approximate the background as a spherical surface and represent a scene as a union of the foreground placed in the sphere and the thin spherical background. It reduces the degree of freedom in the background field. Accordingly, we modify the volume rendering equation and incorporate dedicated constraints to design a novel 3D-aware GAN framework named BallGAN. BallGAN has multiple advantages as follows. 1) It produces more reasonable 3D geometry; the images of a scene across different viewpoints have better photometric 
    
[^145]: 无监督的流形线性化和聚类

    Unsupervised Manifold Linearizing and Clustering. (arXiv:2301.01805v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.01805](http://arxiv.org/abs/2301.01805)

    本文提出了一种无监督的方法，可以同时对接近低维流形并具有线性表示的数据进行聚类和学习。方法可以处理非线性流形，而不需要对样本的聚类成员资格做出假设或对采样密度要求过高。

    

    我们考虑同时对接近低维流形并具有线性表示的数据进行聚类和学习的问题，这是机器学习和计算机视觉中的一个基本任务。当假设流形是线性子空间时，这归结为经典的子空间聚类问题，在过去的二十年中得到了广泛研究。然而，许多现实世界的数据集（如自然图像）不能很好地用线性子空间近似。另一方面，许多研究尝试学习数据的适当变换，使得数据从一组非线性流形映射到一组线性子空间（将同一流形上的点映射到同一子空间）。然而，许多现有方法存在诸如假设已知样本对聚类的成员资格、要求高采样密度或在理论上被证明学习的是平凡表示的局限性。

    We consider the problem of simultaneously clustering and learning a linear representation of data lying close to a union of low-dimensional manifolds, a fundamental task in machine learning and computer vision. When the manifolds are assumed to be linear subspaces, this reduces to the classical problem of subspace clustering, which has been studied extensively over the past two decades. Unfortunately, many real-world datasets such as natural images can not be well approximated by linear subspaces. On the other hand, numerous works have attempted to learn an appropriate transformation of the data, such that data is mapped from a union of general non-linear manifolds to a union of linear subspaces (with points from the same manifold being mapped to the same subspace). However, many existing works have limitations such as assuming knowledge of the membership of samples to clusters, requiring high sampling density, or being shown theoretically to learn trivial representations. In this pape
    
[^146]: 计算机视觉中的算法进展

    Algorithmic progress in computer vision. (arXiv:2212.05153v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.05153](http://arxiv.org/abs/2212.05153)

    计算机视觉领域的算法进展对进步起到了重要作用，特别是增强计算的算法创新使计算需求减半，速度比摩尔定律相关速度快两倍以上。

    

    我们研究了在ImageNet上的图像分类中的算法进展，这是计算机视觉领域最著名的测试平台之一。我们估计了一个模型，并根据神经缩放定律的工作推断了进展的分解，包括计算、数据和算法的缩放。使用Shapley值来归因性能改进，我们发现算法改进在计算机视觉进展中的作用与计算的缩放差不多重要。我们的估计表明，算法创新主要以增强计算的算法进步的形式出现（使研究人员能够在较少计算资源的情况下获得更好的性能），而不是增加数据的算法进步。我们发现，增强计算的算法创新的速度比通常与摩尔定律相关的速度更快两倍以上。特别是，我们估计每九个月计算增强的创新将使计算需求减半（95％的置信区间为4到25个月）。

    We investigate algorithmic progress in image classification on ImageNet, perhaps the most well-known test bed for computer vision. We estimate a model, informed by work on neural scaling laws, and infer a decomposition of progress into the scaling of compute, data, and algorithms. Using Shapley values to attribute performance improvements, we find that algorithmic improvements have been roughly as important as the scaling of compute for progress computer vision. Our estimates indicate that algorithmic innovations mostly take the form of compute-augmenting algorithmic advances (which enable researchers to get better performance from less compute), not data-augmenting algorithmic advances. We find that compute-augmenting algorithmic advances are made at a pace more than twice as fast as the rate usually associated with Moore's law. In particular, we estimate that compute-augmenting innovations halve compute requirements every nine months (95\% confidence interval: 4 to 25 months).
    
[^147]: FIESTA：用于精确纤维分割的自动编码器在道路图中的应用

    FIESTA: Autoencoders for accurate fiber segmentation in tractography. (arXiv:2212.00143v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.00143](http://arxiv.org/abs/2212.00143)

    FIESTA是一个可靠、稳健、完全自动化且易于半自动校准的流程，基于深度自动编码器可以解剖和完全填充白质束，通过生成采样的方法改善难以追踪的束的分割覆盖率。

    

    白质束分割是现代道路图研究脑结构连接性的基石，适用于神经疾病、神经外科和衰老等领域。在本研究中，我们提出了FIESTA（使用自动编码器进行纤维分割的FIbEr Segmentation in Tractography），这是一个可靠、稳健、完全自动化且易于半自动校准的流程，基于深度自动编码器可以解剖和完全填充白质束。该流程建立在之前的工作基础上，这些工作证明了自动编码器在道路图中的流线过滤、束分割和流线生成方面的成功应用。我们提出的方法通过使用主体束和准则束的潜空间样本生成来恢复难以追踪的束，从而改善束分割的覆盖率。使用基于自动编码器建模和对比学习相结合的方法学习了流线的潜空间。使用标准空间（MNI）中的束的分割图作为准则图。

    White matter bundle segmentation is a cornerstone of modern tractography to study the brain's structural connectivity in domains such as neurological disorders, neurosurgery, and aging. In this study, we present FIESTA (FIbEr Segmentation in Tractography using Autoencoders), a reliable and robust, fully automated, and easily semi-automatically calibrated pipeline based on deep autoencoders that can dissect and fully populate white matter bundles. This pipeline is built upon previous works that demonstrated how autoencoders can be used successfully for streamline filtering, bundle segmentation, and streamline generation in tractography. Our proposed method improves bundle segmentation coverage by recovering hard-to-track bundles with generative sampling through the latent space seeding of the subject bundle and the atlas bundle. A latent space of streamlines is learned using autoencoder-based modeling combined with contrastive learning. Using an atlas of bundles in standard space (MNI),
    
[^148]: 基于贝叶斯神经网络的离岸风电结构全场虚拟负载监测

    Farm-wide virtual load monitoring for offshore wind structures via Bayesian neural networks. (arXiv:2211.00642v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.00642](http://arxiv.org/abs/2211.00642)

    该论文介绍了一种基于贝叶斯神经网络的全场虚拟负载监测方案，以解决离岸风电结构监测的不确定性和实际限制问题。

    

    离岸风电结构在运行寿命内会受到退化机制的影响。即使通过基于物理的退化模型可以估计结构元件的退化演变，但过程中涉及的不确定性阻碍了生命周期管理决策的选择。在这种情况下，通过高效的监测系统收集相关信息，可以减少不确定性，最终推动更优化的生命周期决策。然而，在整个风电场的所有风力涡轮上实施完整的监测仪器可能由于实际和经济限制而变得不可行。此外，某些负载监测系统在海洋环境暴露几年后经常发生故障。为解决上述问题，由一个领导型风力涡轮指导的全场虚拟负载监测方案提供了一种有吸引力的解决方案。

    Offshore wind structures are subject to deterioration mechanisms throughout their operational lifetime. Even if the deterioration evolution of structural elements can be estimated through physics-based deterioration models, the uncertainties involved in the process hurdle the selection of lifecycle management decisions. In this scenario, the collection of relevant information through an efficient monitoring system enables the reduction of uncertainties, ultimately driving more optimal lifecycle decisions. However, a full monitoring instrumentation implemented on all wind turbines in a farm might become unfeasible due to practical and economical constraints. Besides, certain load monitoring systems often become defective after a few years of marine environment exposure. Addressing the aforementioned concerns, a farm-wide virtual load monitoring scheme directed by a fleet-leader wind turbine offers an attractive solution. Fetched with data retrieved from a fully-instrumented wind turbine
    
[^149]: 确切的流形高斯变分贝叶斯

    Exact Manifold Gaussian Variational Bayes. (arXiv:2210.14598v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.14598](http://arxiv.org/abs/2210.14598)

    我们提出了一种在复杂模型中进行变分推断的优化算法，通过使用自然梯度更新和黎曼流形，我们开发了一种高效的高斯变分推断算法，并验证了其在多个数据集上的性能。

    

    我们提出了一种用于复杂模型中变分推断（VI）的优化算法。我们的方法依赖于自然梯度更新，其中变分空间是一个黎曼流形。我们开发了一个高效的高斯变分推断算法，以隐式满足变分协方差矩阵的正定约束。我们的确切流形高斯变分贝叶斯（EMGVB）提供了精确但简单的更新规则，并且易于实现。由于其黑盒性质，EMGVB成为复杂模型中即插即用的解决方案。通过在不同统计、计量和深度学习模型上使用五个数据集，我们对我们的可行性方法进行了实证验证，并与基准方法进行了性能讨论。

    We propose an optimization algorithm for Variational Inference (VI) in complex models. Our approach relies on natural gradient updates where the variational space is a Riemann manifold. We develop an efficient algorithm for Gaussian Variational Inference that implicitly satisfies the positive definite constraint on the variational covariance matrix. Our Exact manifold Gaussian Variational Bayes (EMGVB) provides exact but simple update rules and is straightforward to implement. Due to its black-box nature, EMGVB stands as a ready-to-use solution for VI in complex models. Over five datasets, we empirically validate our feasible approach on different statistical, econometric, and deep learning models, discussing its performance with respect to baseline methods.
    
[^150]: 倒向深度BSDE方法的收敛性及其在最优停止问题中的应用

    Convergence of the Backward Deep BSDE Method with Applications to Optimal Stopping Problems. (arXiv:2210.04118v3 [math.PR] UPDATED)

    [http://arxiv.org/abs/2210.04118](http://arxiv.org/abs/2210.04118)

    本文提出了倒向深度BSDE方法用于解决最优停止问题，并给出了相应的理论分析和误差估计方法。

    

    最优停止问题是金融市场中的核心问题之一，具有广泛的应用，如定价美式和百慕达期权。深度BSDE方法在解决高维前后向随机微分方程（FBSDEs）方面展现出了强大的能力，并且激发了许多应用。然而，该方法以正向方式解决倒向随机微分方程（BSDEs），不能用于通常需要向后运行BSDE的最优停止问题。为了克服这一困难，一篇最近的论文提出了倒向深度BSDE方法来解决最优停止问题。在本文中，我们提供了倒向深度BSDE方法的严格理论。具体来说，我们推导了后验误差估计，即通过训练损失函数可以限制数值解的误差；并且我们提供了一-

    The optimal stopping problem is one of the core problems in financial markets, with broad applications such as pricing American and Bermudan options. The deep BSDE method [Han, Jentzen and E, PNAS, 115(34):8505-8510, 2018] has shown great power in solving high-dimensional forward-backward stochastic differential equations (FBSDEs), and inspired many applications. However, the method solves backward stochastic differential equations (BSDEs) in a forward manner, which can not be used for optimal stopping problems that in general require running BSDE backwardly. To overcome this difficulty, a recent paper [Wang, Chen, Sudjianto, Liu and Shen, arXiv:1807.06622, 2018] proposed the backward deep BSDE method to solve the optimal stopping problem. In this paper, we provide the rigorous theory for the backward deep BSDE method. Specifically, 1. We derive the a posteriori error estimation, i.e., the error of the numerical solution can be bounded by the training loss function; and; 2. We give an 
    
[^151]: 利用自注意力指导提高扩散模型的样本质量

    Improving Sample Quality of Diffusion Models Using Self-Attention Guidance. (arXiv:2210.00939v5 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.00939](http://arxiv.org/abs/2210.00939)

    该论文提出了一种利用自注意力指导的策略来提升扩散模型生成图像的稳定性和质量，具有较高的实用价值。

    

    去噪扩散模型以其出色的生成质量和多样性受到关注。这种成功很大程度上归因于使用分类或文本条件的扩散指导方法，如分类器和无分类器指导。在本文中，我们提出了一个更全面的视角，超越了传统的指导方法。从这个广义的视角出发，我们引入了新的无条件和无监督的策略来提高生成图像的质量。作为一种简单的解决方案，模糊指导改善了中间样本的适用性，使得扩散模型能够以适度的指导尺度生成更高质量的样本。在此基础上，自注意力指导（SAG）利用了扩散模型的中间自注意力映射来增强它们的稳定性和效果。具体而言，SAG在每次迭代中仅对扩散模型关注的区域进行对抗性模糊处理。

    Denoising diffusion models (DDMs) have attracted attention for their exceptional generation quality and diversity. This success is largely attributed to the use of class- or text-conditional diffusion guidance methods, such as classifier and classifier-free guidance. In this paper, we present a more comprehensive perspective that goes beyond the traditional guidance methods. From this generalized perspective, we introduce novel condition- and training-free strategies to enhance the quality of generated images. As a simple solution, blur guidance improves the suitability of intermediate samples for their fine-scale information and structures, enabling diffusion models to generate higher quality samples with a moderate guidance scale. Improving upon this, Self-Attention Guidance (SAG) uses the intermediate self-attention maps of diffusion models to enhance their stability and efficacy. Specifically, SAG adversarially blurs only the regions that diffusion models attend to at each iteratio
    
[^152]: 用高斯机器隐私实现个体隐私核算

    Individual Privacy Accounting with Gaussian Differential Privacy. (arXiv:2209.15596v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2209.15596](http://arxiv.org/abs/2209.15596)

    本论文介绍了用于个体隐私核算的高斯差分隐私方法，通过对自适应组合随机机制进行仔细分析，为高斯机制提供了最优边界。

    

    个体隐私核算能够为参与分析的每个参与者个别地限制差分隐私损失。这通常是有意义的，因为个体隐私损失往往比考虑每次数据访问的最坏情况边界所示的差分隐私边界要小得多。为了以有原则的方式核算个体隐私损失，我们需要一种针对自适应组合随机机制的隐私核算方法，其中在给定的数据访问中所产生的损失允许比最坏情况损失要小。费尔德曼和兹尔尼克（2021）已对Rényi差分隐私（RDP）进行了这种分析，但尚未应用于所谓的最优隐私核算方法。我们通过使用高斯差分隐私进行仔细分析，为此方向迈出了第一步，高斯差分隐私为最多功能的差分隐私机制之一提供了最优边界。这种方法基于...

    Individual privacy accounting enables bounding differential privacy (DP) loss individually for each participant involved in the analysis. This can be informative as often the individual privacy losses are considerably smaller than those indicated by the DP bounds that are based on considering worst-case bounds at each data access. In order to account for the individual privacy losses in a principled manner, we need a privacy accountant for adaptive compositions of randomised mechanisms, where the loss incurred at a given data access is allowed to be smaller than the worst-case loss. This kind of analysis has been carried out for the R\'enyi differential privacy (RDP) by Feldman and Zrnic (2021), however not yet for the so-called optimal privacy accountants. We make first steps in this direction by providing a careful analysis using the Gaussian differential privacy which gives optimal bounds for the Gaussian mechanism, one of the most versatile DP mechanisms. This approach is based on 
    
[^153]: 在统计自适应中，内部神经元加速了递归神经网络的学习动态

    Interneurons accelerate learning dynamics in recurrent neural networks for statistical adaptation. (arXiv:2209.10634v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2209.10634](http://arxiv.org/abs/2209.10634)

    这项工作探索了通过内部神经元介导递归通信与直接递归连接相比的计算优势，通过分析连续突触动态和数值模拟，表明具有内部神经元的网络比具有直接递归连接的网络更能抵抗初始化的干扰。

    

    大脑中的早期感知系统快速适应波动的输入统计，这需要神经元之间的递归通信。在这项工作中，我们探索了通过内部神经元介导递归通信与直接递归连接相比的计算优势。为此，我们考虑了两种数学可追踪的递归线性神经网络，它们对输入进行统计白化——一种具有直接递归连接，另一种具有介导递归通信的内部神经元。通过分析相应的连续突触动态并对网络进行数值模拟，我们表明具有内部神经元的网络比具有直接递归连接的网络更能抵抗初始化的干扰，即内部神经元网络的突触动态的收敛时间（或者直接递归连接的网络）呈对数尺度。

    Early sensory systems in the brain rapidly adapt to fluctuating input statistics, which requires recurrent communication between neurons. Mechanistically, such recurrent communication is often indirect and mediated by local interneurons. In this work, we explore the computational benefits of mediating recurrent communication via interneurons compared with direct recurrent connections. To this end, we consider two mathematically tractable recurrent linear neural networks that statistically whiten their inputs -- one with direct recurrent connections and the other with interneurons that mediate recurrent communication. By analyzing the corresponding continuous synaptic dynamics and numerically simulating the networks, we show that the network with interneurons is more robust to initialization than the network with direct recurrent connections in the sense that the convergence time for the synaptic dynamics in the network with interneurons (resp. direct recurrent connections) scales logar
    
[^154]: 从可表达性到可执行性：在有限范围内实现自顶向下的自动化开发的神经符号框架

    Towards Top-Down Automated Development in Limited Scopes: A Neuro-Symbolic Framework from Expressibles to Executables. (arXiv:2209.01566v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2209.01566](http://arxiv.org/abs/2209.01566)

    本研究提出了一个神经符号框架，从可表达性到可执行性实现了自顶向下的自动化开发。通过建立代码分类法和使用语义金字塔来关联文本数据和代码数据，我们可以改进深度代码生成的效果。

    

    深度代码生成是软件工程中深度学习的一个主题，采用神经模型为预期功能生成代码。由于端到端神经方法缺乏领域知识和软件层次意识，它们在项目级任务上表现不佳。为了系统地探索代码生成的潜在改进，我们让其参与从“可表达性”到“可执行性”的自顶向下开发，这在有限的范围内是可能的。在这个过程中，它从大量的样本、特征和知识中受益。作为基础，我们建议在代码数据上建立一个分类法，即代码分类法，利用代码信息的分类。此外，我们引入了一个三层语义金字塔(SP)来关联文本数据和代码数据。它识别不同抽象层次的信息，从而引入了关于开发的领域知识，并揭示了软件的层次结构。

    Deep code generation is a topic of deep learning for software engineering (DL4SE), which adopts neural models to generate code for the intended functions. Since end-to-end neural methods lack domain knowledge and software hierarchy awareness, they tend to perform poorly w.r.t project-level tasks. To systematically explore the potential improvements of code generation, we let it participate in the whole top-down development from \emph{expressibles} to \emph{executables}, which is possible in limited scopes. In the process, it benefits from massive samples, features, and knowledge. As the foundation, we suggest building a taxonomy on code data, namely code taxonomy, leveraging the categorization of code information. Moreover, we introduce a three-layer semantic pyramid (SP) to associate text data and code data. It identifies the information of different abstraction levels, and thus introduces the domain knowledge on development and reveals the hierarchy of software. Furthermore, we propo
    
[^155]: 使用基于Transformer的场景表示学习增强强化学习用于自动驾驶决策

    Augmenting Reinforcement Learning with Transformer-based Scene Representation Learning for Decision-making of Autonomous Driving. (arXiv:2208.12263v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.12263](http://arxiv.org/abs/2208.12263)

    本研究提出了Scene-Rep Transformer来提升强化学习决策能力，通过改进场景表示编码和顺序预测潜在蒸馏。采用多阶段Transformer编码器建模交互意识和意图意识，并使用顺序潜在Transformer进行自监督学习，加速训练和减少探索空间。

    

    城市自动驾驶的决策是具有挑战性的，由于交通参与者的随机性和道路结构的复杂性。尽管基于强化学习（RL）的决策方案在处理城市驾驶场景方面很有前景，但它的采样效率低且适应性差。在本文中，我们提出了Scene-Rep Transformer来改善RL决策能力，通过更好的场景表示编码和顺序预测潜在蒸馏。具体而言，我们构建了一个多阶段Transformer（MST）编码器，用于建模自车与其邻居之间的交互意识以及代理者与候选路径之间的意图意识。我们采用了一个具有自监督学习目标的顺序潜在Transformer（SLT），将未来的预测信息蒸馏到潜在的场景表示中，以减少探索空间并加快训练。

    Decision-making for urban autonomous driving is challenging due to the stochastic nature of interactive traffic participants and the complexity of road structures. Although reinforcement learning (RL)-based decision-making scheme is promising to handle urban driving scenarios, it suffers from low sample efficiency and poor adaptability. In this paper, we propose Scene-Rep Transformer to improve the RL decision-making capabilities with better scene representation encoding and sequential predictive latent distillation. Specifically, a multi-stage Transformer (MST) encoder is constructed to model not only the interaction awareness between the ego vehicle and its neighbors but also intention awareness between the agents and their candidate routes. A sequential latent Transformer (SLT) with self-supervised learning objectives is employed to distill the future predictive information into the latent scene representation, in order to reduce the exploration space and speed up training. The fina
    
[^156]: 高效自适应激活舍入用于训练后量化

    Efficient Adaptive Activation Rounding for Post-Training Quantization. (arXiv:2208.11945v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.11945](http://arxiv.org/abs/2208.11945)

    本论文提出了一种高效自适应激活舍入的训练后量化方法，通过调整激活的舍入方案来降低输出误差，并解决了动态激活和运行时开销的挑战。

    

    由于在部署量化神经网络方面的便利性，训练后量化受到越来越多的关注。尽管最近的研究表明最近舍入仍然是DNN量化的主流方法，但它在权重量化中表现出了次优的性质。他们提出通过利用输出误差而不是传统的权重量化误差来优化权重舍入方案。我们的研究发现相似的舍入挑战也适用于激活量化。尽管很容易推广，但挑战在于激活的动态性。需要针对不同的激活进行自适应舍入，并且该方法会导致运行时开销。为了解决这个问题，我们提出了一种新的AQuant量化框架，通过调整激活的舍入方案来降低输出误差。与使用常数舍入边界0.5的最近舍入操作不同，我们使边界成为激活的函数。

    Post-training quantization attracts increasing attention due to its convenience in deploying quantized neural networks. Although rounding-to-nearest remains the prevailing method for DNN quantization, prior research has demonstrated its suboptimal nature when applied to weight quantization. They propose optimizing weight rounding schemes by leveraging output error rather than the traditional weight quantization error. Our study reveals that similar rounding challenges also extend to activation quantization. Despite the easy generalization, the challenges lie in the dynamic nature of activation. Adaptive rounding is expected for varying activations and the method is subjected to runtime overhead. To tackle this, we propose the AQuant quantization framework with a novel perspective to reduce output error by adjusting rounding schemes of activations. Instead of using the constant rounding border 0.5 of the rounding-to-nearest operation, we make the border become a function w.r.t. the acti
    
[^157]: 将梯度统一化以提高深度网络的真实世界鲁棒性

    Unifying Gradients to Improve Real-world Robustness for Deep Networks. (arXiv:2208.06228v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2208.06228](http://arxiv.org/abs/2208.06228)

    通过统一不同数据的梯度来防御基于评分的查询攻击（SQAs），这样SQAs只能探测到一个更弱的攻击方向，保护真实世界的深度神经网络。

    

    深度神经网络（DNN）的广泛应用对它们的真实世界鲁棒性提出了更多关注，即DNN是否能够抵抗黑盒对抗攻击，其中基于评分的查询攻击（SQAs）最具威胁性，因为它们只能通过访问模型输出有效地攻击受害网络。抵御SQAs需要对输出进行轻微但巧妙的变化，因为用户与SQAs共享相同的输出信息。在本文中，我们提出了一种通过统一不同数据的梯度来进行真实世界防御的方法，使得SQAs只能探测到一个更弱的攻击方向，这个攻击方向对于不同样本是相似的。由于这种统一的攻击扰动被验证为比输入特定的扰动更不具侵略性，UniG通过指示攻击者一个扭曲且信息较少的攻击方向来保护真实世界的DNN。我们通过一个可插拔的Hadamard乘积模块高效实现了UniG。

    The wide application of deep neural networks (DNNs) demands an increasing amount of attention to their real-world robustness, i.e., whether a DNN resists black-box adversarial attacks, among which score-based query attacks (SQAs) are most threatening since they can effectively hurt a victim network with the only access to model outputs. Defending against SQAs requires a slight but artful variation of outputs due to the service purpose for users, who share the same output information with SQAs. In this paper, we propose a real-world defense by Unifying Gradients (UniG) of different data so that SQAs could only probe a much weaker attack direction that is similar for different samples. Since such universal attack perturbations have been validated as less aggressive than the input-specific perturbations, UniG protects real-world DNNs by indicating attackers a twisted and less informative attack direction. We implement UniG efficiently by a Hadamard product module which is plug-and-play. A
    
[^158]: 一种时间和空间局部的基于脉冲的反向传播算法，以实现硬件中的训练

    A temporally and spatially local spike-based backpropagation algorithm to enable training in hardware. (arXiv:2207.09755v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2207.09755](http://arxiv.org/abs/2207.09755)

    这个论文提出了一种新的基于脉冲神经网络的反向传播算法，可以在硬件中进行训练，解决了脉冲编码训练机制的挑战，并且不需要外部内存和计算访问。

    

    脉冲神经网络（SNN）已成为一种用于分类任务的硬件高效架构。脉冲编码的挑战在于缺乏完全使用脉冲进行训练的通用机制。已经有几次尝试采用非脉冲人工神经网络（ANN）中使用的强大反向传播（BP）技术：（1）可以通过外部计算的数值梯度来训练SNN；（2）朝向前/后传递的相位结合了近似的基于脉冲的反向传播，利用了脉冲时序依赖可塑性（STDP）。然而，这种相位之间的信息传递，用于梯度和权重更新计算，需要外部内存和计算访问。这对于标准的神经形态硬件实现是一个挑战。在本文中，我们提出了一种基于随机SNN的反向传播（SSNN-BP）算法，利用复合神经元同时计算...

    Spiking Neural Networks (SNNs) have emerged as a hardware efficient architecture for classification tasks. The challenge of spike-based encoding has been the lack of a universal training mechanism performed entirely using spikes. There have been several attempts to adopt the powerful backpropagation (BP) technique used in non-spiking artificial neural networks (ANN): (1) SNNs can be trained by externally computed numerical gradients. (2) A major advancement towards native spike-based learning has been the use of approximate Backpropagation using spike-time dependent plasticity (STDP) with phased forward/backward passes. However, the transfer of information between such phases for gradient and weight update calculation necessitates external memory and computational access. This is a challenge for standard neuromorphic hardware implementations. In this paper, we propose a stochastic SNN based Back-Prop (SSNN-BP) algorithm that utilizes a composite neuron to simultaneously compute the for
    
[^159]: 自编码器的自监督训练用于视觉异常检测

    Self-Supervised Training with Autoencoders for Visual Anomaly Detection. (arXiv:2206.11723v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.11723](http://arxiv.org/abs/2206.11723)

    本文提出了一种自监督学习方法，通过修改重构误差的方式集中在数据流形上，从而解决深度卷积自编码器在视觉异常检测中容易出现的重构异常信号而导致检测效果不佳的问题。

    

    深度卷积自编码器提供了一种有效的工具，可以以无监督的方式学习非线性降维。最近，它们已被用于视觉领域的异常检测任务。通过使用无异常的样本优化重构误差，普遍认为相应的网络应在应用阶段未能准确重构异常区域。这个目标通常通过控制网络的容量来解决，要么通过减少瓶颈层的大小，要么通过对其激活施加稀疏约束。然而，这两种技术都没有明确惩罚异常信号的重构，通常导致检测效果不佳。我们通过自适应自监督学习方法来解决这个问题，这种方法允许在训练过程中使用判别信息，通过修改的重构误差集中在数据流形上。这使得模型能够产生局部一致的重构结果。

    Deep convolutional autoencoders provide an effective tool for learning non-linear dimensionality reduction in an unsupervised way. Recently, they have been used for the task of anomaly detection in the visual domain. By optimising for the reconstruction error using anomaly-free examples, the common belief is that a corresponding network should fail to accurately reconstruct anomalous regions in the application phase. This goal is typically addressed by controlling the capacity of the network by either reducing the size of the bottleneck layer or enforcing sparsity constraints on its activations. However, neither of these techniques does explicitly penalize reconstruction of anomalous signals often resulting in poor detection. We tackle this problem by adapting a self-supervised learning regime, which allows to use discriminative information during training focusing on the data manifold by means of a modified reconstruction error. This regularizes the model to produce locally consistent
    
[^160]: 测试时间自适应视觉文档理解

    Test-Time Adaptation for Visual Document Understanding. (arXiv:2206.07240v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.07240](http://arxiv.org/abs/2206.07240)

    该论文提出了一种测试时间自适应方法，用于将自监督预训练得到的表示适应到测试时的分布转移。通过利用跨模态自监督学习和伪标签方法，该方法在文档理解任务中实现了显著的改进，并在实体识别、键值提取和文档视觉问答上分别提高了1.89%、3.43%和17.68%。

    

    对于视觉文档理解（VDU），自监督预训练已被证明能够成功生成具有可传递性的表示，然而，在测试时间对这种表示进行有效的适应仍然是一个未被探索的领域。我们提出了DocTTA，一种用于文档的新型测试时间适应方法，该方法使用无标签的目标文档数据进行无源域适应。DocTTA利用跨模态自监督学习和伪标签方法，将在“源”域上学到的模型适应到未标记的“目标”域。我们使用现有的公共数据集为各种VDU任务引入了新的基准，包括实体识别、键值提取和文档视觉问答。相比于源模型性能，DocTTA在这些任务上表现出了显著的改进，分别提高了1.89%（F1分数）、3.43%（F1分数）和17.68%（ANLS分数）。

    For visual document understanding (VDU), self-supervised pretraining has been shown to successfully generate transferable representations, yet, effective adaptation of such representations to distribution shifts at test-time remains to be an unexplored area. We propose DocTTA, a novel test-time adaptation method for documents, that does source-free domain adaptation using unlabeled target document data. DocTTA leverages cross-modality self-supervised learning via masked visual language modeling, as well as pseudo labeling to adapt models learned on a \textit{source} domain to an unlabeled \textit{target} domain at test time. We introduce new benchmarks using existing public datasets for various VDU tasks, including entity recognition, key-value extraction, and document visual question answering. DocTTA shows significant improvements on these compared to the source model performance, up to 1.89\% in (F1 score), 3.43\% (F1 score), and 17.68\% (ANLS score), respectively. Our benchmark dat
    
[^161]: 高效Adam：高效通信的分布式Adam

    Efficient-Adam: Communication-Efficient Distributed Adam. (arXiv:2205.14473v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14473](http://arxiv.org/abs/2205.14473)

    这项工作提出了一种名为高效-Adam的通信效率更高的分布式优化方法，在非凸环境下通过双向量化和双向误差反馈策略来降低通信成本，并对其迭代复杂度和通信复杂度进行了分析。

    

    分布式自适应随机梯度方法广泛应用于大规模非凸优化，如深度学习模型训练。然而，在非凸环境中很少有关于找到ε-稳定点的通信复杂性分析。在这项工作中，我们提出了一种新颖的高效通信的分布式Adam，用于随机非凸优化的参数服务器模型，称为"高效-Adam"。具体来说，我们将双向量化方案纳入高效-Adam，以减少工作者和服务器之间的通信成本。同时，我们采用双向误差反馈策略，分别减小了双向量化对服务器和工作者的偏差影响。此外，我们还建立了一类量化算子的高效-Adam的迭代复杂度，并进一步刻画了服务器和工作者之间的通信复杂度，当ε-稳定点满足时。

    Distributed adaptive stochastic gradient methods have been widely used for large-scale nonconvex optimization, such as training deep learning models. However, their communication complexity on finding $\varepsilon$-stationary points has rarely been analyzed in the nonconvex setting. In this work, we present a novel communication-efficient distributed Adam in the parameter-server model for stochastic nonconvex optimization, dubbed {\em Efficient-Adam}. Specifically, we incorporate a two-way quantization scheme into Efficient-Adam to reduce the communication cost between the workers and server. Simultaneously, we adopt a two-way error feedback strategy to reduce the biases caused by the two-way quantization on both the server and workers, respectively. In addition, we establish the iteration complexity for the proposed Efficient-Adam with a class of quantization operators, and further characterize its communication complexity between the server and workers when an $\varepsilon$-stationar
    
[^162]: 在医学图像中利用全局二进制掩码进行结构分割

    Leveraging Global Binary Masks for Structure Segmentation in Medical Images. (arXiv:2205.09107v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2205.09107](http://arxiv.org/abs/2205.09107)

    这篇论文提出了利用全局二进制掩码来进行医学图像中结构分割的方法，通过利用器官的解剖形状和位置信息的一致性来进行分割。研究了两种情况下的应用，并在脑部和心脏CT图像数据集上进行了验证。

    

    对于医学图像分割的深度学习模型受到输入图像强度变化的影响很大，并且由于主要利用像素强度信息进行推理，缺乏泛化能力。获取足够的训练数据是限制模型应用的另一个挑战。我们提出利用医学图像中器官的解剖形状和位置信息的一致性。我们引入了一个框架，通过全局二进制掩码来利用重复出现的解剖模式进行器官分割。研究了两种情况：1）全局二进制掩码是模型(U-Net)的唯一输入，强制进行器官的定位和形状信息编码进行分割。2）将全局二进制掩码作为附加通道，并用作位置和形状线索以减少训练数据的稀缺性。将两个包含脑部和心脏CT图像及其地面实况的数据集分为(26:10:10)和(12:3:5)进行训练、验证和测试。

    Deep learning (DL) models for medical image segmentation are highly influenced by intensity variations of input images and lack generalization due to primarily utilizing pixels' intensity information for inference. Acquiring sufficient training data is another challenge limiting models' applications. We proposed to leverage the consistency of organs' anatomical shape and position information in medical images. We introduced a framework leveraging recurring anatomical patterns through global binary masks for organ segmentation. Two scenarios were studied.1) Global binary masks were the only model's (i.e. U-Net) input, forcing exclusively encoding organs' position and shape information for segmentation/localization.2) Global binary masks were incorporated as an additional channel functioning as position/shape clues to mitigate training data scarcity. Two datasets of the brain and heart CT images with their ground-truth were split into (26:10:10) and (12:3:5) for training, validation, and
    
[^163]: StableDR:稳定的双重稳健学习方法用于数据缺失非随机的推荐系统

    StableDR: Stabilized Doubly Robust Learning for Recommendation on Data Missing Not at Random. (arXiv:2205.04701v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.04701](http://arxiv.org/abs/2205.04701)

    StableDR是一种稳定的双重稳健学习方法，用于解决推荐系统中数据缺失非随机的问题。通过减少对外推的依赖，StableDR能够同时具有有界的偏差、方差和泛化误差界，在不准确的估计误差和任意小的倾向性下表现出优越性能。

    

    在推荐系统中，用户倾向于选择自己喜欢的物品进行评价，这导致了数据缺失非随机的问题，在对预测模型进行无偏评估和学习时带来了很大挑战。目前，双重稳健（DR）方法已经得到广泛研究，并展示出优越的性能。然而，在本文中，我们展示了DR方法的不稳定性以及对极小的倾向性具有无界偏差、方差和泛化界限的问题。此外，DR更多地依赖外推，这会导致次优的性能。为了解决以上问题，我们提出了一种稳定的双重稳健（StableDR）学习方法，对外推的依赖较弱。理论分析表明，在不准确的估计误差和任意小的倾向性下，StableDR同时具有有界的偏差、方差和泛化误差界。此外，我们还提出了一种针对StableDR的新型学习方法来更新估计值。

    In recommender systems, users always choose the favorite items to rate, which leads to data missing not at random and poses a great challenge for unbiased evaluation and learning of prediction models. Currently, the doubly robust (DR) methods have been widely studied and demonstrate superior performance. However, in this paper, we show that DR methods are unstable and have unbounded bias, variance, and generalization bounds to extremely small propensities. Moreover, the fact that DR relies more on extrapolation will lead to suboptimal performance. To address the above limitations while retaining double robustness, we propose a stabilized doubly robust (StableDR) learning approach with a weaker reliance on extrapolation. Theoretical analysis shows that StableDR has bounded bias, variance, and generalization error bound simultaneously under inaccurate imputed errors and arbitrarily small propensities. In addition, we propose a novel learning approach for StableDR that updates the imputat
    
[^164]: 流形上的Riemannian Hamiltonian方法用于min-max优化问题

    Riemannian Hamiltonian methods for min-max optimization on manifolds. (arXiv:2204.11418v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2204.11418](http://arxiv.org/abs/2204.11418)

    本文研究了流形上的min-max优化问题，并引入了Riemannian Hamiltonian方法作为其代理方法。通过最小化Hamiltonian函数，可以得到所需的min-max鞍点。该方法在geodesic-bilinear优化问题中具有挑战性，但通过解决代理问题可以得到全局最优搜索方向。该方法在多个应用中展示了其有效性。

    

    本文研究了流形上的min-max优化问题。我们引入了一个Riemannian Hamiltonian函数，其最小化作为解决原始min-max问题的代理。在Riemannian Polyak-{\L}ojasiewicz条件下，其最小值对应于所需的min-max鞍点。我们还提供了满足此条件的情况。特别是对于geodesic-bilinear优化，在解决代理问题的情况下，可以得到正确的全局最优搜索方向，而在min-max形式化中变得具有挑战性。为了最小化Hamiltonian函数，我们提出了Riemannian Hamiltonian方法（RHM）并提出了它们的收敛性分析。我们将RHM扩展到包括共识正则化和随机设置。我们通过应用如子空间鲁棒Wasserstein距离、神经网络的鲁棒训练和生成对抗网络等来说明所提出的RHM的有效性。

    In this paper, we study min-max optimization problems on Riemannian manifolds. We introduce a Riemannian Hamiltonian function, minimization of which serves as a proxy for solving the original min-max problems. Under the Riemannian Polyak--{\L}ojasiewicz condition on the Hamiltonian function, its minimizer corresponds to the desired min-max saddle point. We also provide cases where this condition is satisfied. For geodesic-bilinear optimization in particular, solving the proxy problem leads to the correct search direction towards global optimality, which becomes challenging with the min-max formulation. To minimize the Hamiltonian function, we propose Riemannian Hamiltonian methods (RHM) and present their convergence analyses. We extend RHM to include consensus regularization and to the stochastic setting. We illustrate the efficacy of the proposed RHM in applications such as subspace robust Wasserstein distance, robust training of neural networks, and generative adversarial networks.
    
[^165]: FlexFringe:通过学习概率有限自动机来建模软件行为

    FlexFringe: Modeling Software Behavior by Learning Probabilistic Automata. (arXiv:2203.16331v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.16331](http://arxiv.org/abs/2203.16331)

    FlexFringe提供了高效的概率有限自动机学习方法，可用于建模软件行为。该方法在实践中通过实现改进的状态合并策略实现了显著性能提升，并且能够从软件日志中学习可解释的模型，用于异常检测。与基于神经网络的解决方案相比，学习更小更复杂的模型能够提高FlexFringe在异常检测中的性能。

    

    我们介绍了FlexFringe中可用的概率确定性有限自动机学习方法的高效实现。这些实现了众所周知的状态合并策略，包括几种修改以提高它们在实践中的性能。我们通过实验证明这些算法能够获得有竞争力的结果，并在默认实现上实现了显著的改进。我们还展示了如何使用FlexFringe从软件日志中学习可解释的模型，并将其用于异常检测。虽然这些模型较难解释，但我们展示了学习更小、更复杂的模型如何提高FlexFringe在异常检测中的性能，优于基于神经网络的现有解决方案。

    We present the efficient implementations of probabilistic deterministic finite automaton learning methods available in FlexFringe. These implement well-known strategies for state-merging including several modifications to improve their performance in practice. We show experimentally that these algorithms obtain competitive results and significant improvements over a default implementation. We also demonstrate how to use FlexFringe to learn interpretable models from software logs and use these for anomaly detection. Although less interpretable, we show that learning smaller more convoluted models improves the performance of FlexFringe on anomaly detection, outperforming an existing solution based on neural nets.
    
[^166]: BagPipe：加速深度推荐模型训练

    BagPipe: Accelerating Deep Recommendation Model Training. (arXiv:2202.12429v3 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2202.12429](http://arxiv.org/abs/2202.12429)

    本文提出了BagPipe，一种用于加速深度推荐模型训练的系统。该系统利用嵌入访问的特定结构，通过缓存和预取的方式优化训练，实现了对推荐模型的高效训练。

    

    基于深度学习的推荐模型（DLRM）广泛应用于几个关键的商业应用。高效地训练这种推荐模型具有挑战性，因为它们包含数十亿个基于嵌入的参数，从嵌入访问中导致了显著的开销。通过对现有的DLRM训练系统进行分析，我们观察到约75％的迭代时间用于嵌入访问和模型同步。本文的关键见解是嵌入访问具有特定的结构，可以用于加速训练。我们观察到嵌入访问具有严重的偏斜性，约1％的嵌入表示了超过92％的总访问量。此外，我们观察到在离线训练期间，我们可以预测未来批次来确定将在将来的何时迭代需要哪些嵌入。基于这些见解，我们开发了Bagpipe，一种用于训练深度推荐模型的系统，该系统利用缓存和预取来优化训练。

    Deep learning based recommendation models (DLRM) are widely used in several business critical applications. Training such recommendation models efficiently is challenging because they contain billions of embedding-based parameters, leading to significant overheads from embedding access. By profiling existing systems for DLRM training, we observe that around 75\% of the iteration time is spent on embedding access and model synchronization. Our key insight in this paper is that embedding access has a specific structure which can be used to accelerate training. We observe that embedding accesses are heavily skewed, with around 1\% of embeddings representing more than 92\% of total accesses. Further, we observe that during offline training we can lookahead at future batches to determine exactly which embeddings will be needed at what iteration in the future. Based on these insights, we develop Bagpipe, a system for training deep recommendation models that uses caching and prefetching to ov
    
[^167]: 多项式方法在无分布相关SQ学习中是通用的

    The Polynomial Method is Universal for Distribution-Free Correlational SQ Learning. (arXiv:2010.11925v3 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2010.11925](http://arxiv.org/abs/2010.11925)

    多项式方法在无分布相关SQ学习中是通用的，并且是最佳方法。

    

    我们考虑PAC模型和agnostic模型中的布尔函数类的无分布学习问题。我们推广Malach和Shalev-Shwartz（2022年）的优秀工作，给出了学习DNF公式的紧凑型SQ（CSQ）下界，我们证明了对于任何函数类，阈值或近似度的下界直接导致PAC或agnostic学习的CSQ下界。虽然通过结合Feldman（2008、2012）和Sherstov（2008、2011）的先前结果，这些下界可以隐含地得出，但据我们所知，我们给出的精确陈述以此形式尚未出现过。此外，我们的证明方法简单且基本独立。这些下界与PAC或agnostic学习中使用阈值或近似度的SQ模型的相应正面结果相匹配，从这个意义上说，这些结果表明多项式方法是一种无分布CSQ学习的通用且最佳方法。

    We consider the problem of distribution-free learning for Boolean function classes in the PAC and agnostic models. Generalizing a beautiful work of Malach and Shalev-Shwartz (2022) that gave tight correlational SQ (CSQ) lower bounds for learning DNF formulas, we give new proofs that lower bounds on the threshold or approximate degree of any function class directly imply CSQ lower bounds for PAC or agnostic learning respectively. While such bounds implicitly follow by combining prior results by Feldman (2008, 2012) and Sherstov (2008, 2011), to our knowledge the precise statements we give had not appeared in this form before. Moreover, our proofs are simple and largely self-contained.  These lower bounds match corresponding positive results using upper bounds on the threshold or approximate degree in the SQ model for PAC or agnostic learning, and in this sense these results show that the polynomial method is a universal, best-possible approach for distribution-free CSQ learning.
    
[^168]: UCB Bandits在对抗攻击中的近乎最优攻击策略

    Near Optimal Adversarial Attack on UCB Bandits. (arXiv:2008.09312v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2008.09312](http://arxiv.org/abs/2008.09312)

    本文提出了一种在对抗攻击下的UCB最优拉臂策略，成本为$\sqrt{\log T}$，并且证明了此攻击策略近乎是最优的。

    

    本文考虑了一种随机多臂赌博问题，其中奖励受到对抗性破坏。我们提出了一种新颖的攻击策略，通过操作UCB原则来拉动一些非最优目标臂$T-o(T)$次，累积成本的标度为$\sqrt{\log T}$，其中$T$为回合数。我们还证明了累积攻击成本的第一个下界。我们的下界与我们的上界匹配，除了$\log\log T$因子，表明我们的攻击策略近乎是最优的。

    We consider a stochastic multi-arm bandit problem where rewards are subject to adversarial corruption. We propose a novel attack strategy that manipulates a UCB principle into pulling some non-optimal target arm $T - o(T)$ times with a cumulative cost that scales as $\sqrt{\log T}$, where $T$ is the number of rounds. We also prove the first lower bound on the cumulative attack cost. Our lower bound matches our upper bound up to $\log \log T$ factors, showing our attack to be near optimal.
    
[^169]: SWAX基准测试：用蜡像攻击生物识别系统

    The SWAX Benchmark: Attacking Biometric Systems with Wax Figures. (arXiv:1910.09642v1 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/1910.09642](http://arxiv.org/abs/1910.09642)

    SWAX基准测试引入了一个名为Sense Wax Attack (SWAX)的新数据库，其中包含真实的人类和蜡像图像和视频，用于验证面部欺骗检测。实验表明，尽管近年来取得了一些进展，但高质量的攻击仍然使高级欺骗方法易受攻击。

    

    面部欺骗攻击是指入侵者试图冒充携带有盈利认证凭据的人。由于移动设备、高安全区域等对生物识别认证的需求增加，这一问题成为一个热门话题。本研究介绍了一个名为Sense Wax Attack (SWAX)的新数据库，包含真实的人类和蜡像图像和视频，用于验证面部欺骗检测问题。该数据集包含1800多张人脸图像和110个55人/蜡像的视频，分为训练集、验证集和测试集，包含大量的表情、光照和姿势变化。通过与基准方法进行的实验表明，尽管近年来取得了一些进展，但高质量的攻击仍然使高级欺骗方法易受攻击。

    A face spoofing attack occurs when an intruder attempts to impersonate someone who carries a gainful authentication clearance. It is a trending topic due to the increasing demand for biometric authentication on mobile devices, high-security areas, among others. This work introduces a new database named Sense Wax Attack dataset (SWAX), comprised of real human and wax figure images and videos that endorse the problem of face spoofing detection. The dataset consists of more than 1800 face images and 110 videos of 55 people/waxworks, arranged in training, validation and test sets with a large range in expression, illumination and pose variations. Experiments performed with baseline methods show that despite the progress in recent years, advanced spoofing methods are still vulnerable to high-quality violation attempts.
    

