# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning](https://arxiv.org/abs/2402.15506) | AgentOhana提供了一种统一数据和训练流水线的综合解决方案，有助于克服使用大型语言模型（LLMs）进行智能体任务时的挑战。 |
| [^2] | [Co-Supervised Learning: Improving Weak-to-Strong Generalization with Hierarchical Mixture of Experts](https://arxiv.org/abs/2402.15505) | 提出了一种通过使用分层专家混合模型来改善弱到强泛化的协同监督学习方法，利用一组多样化的专家教师共同监督强大的学生模型。 |
| [^3] | [Mechanics-Informed Autoencoder Enables Automated Detection and Localization of Unforeseen Structural Damage](https://arxiv.org/abs/2402.15492) | 该研究提出了一种新颖的"部署和忘记"方法，结合廉价传感器和机械信息自编码器，实现了对结构损伤的自动检测和定位，仅需学习3小时数据即可自主识别和定位不同类型的未知损伤。 |
| [^4] | [A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges, and Future Trends](https://arxiv.org/abs/2402.15490) | 深度学习中的卷积应用广泛，有许多类型的CNNs可满足特定需求，通过比较分析不同类型的CNNs，可以更好地了解它们的优势和劣势，并促进未来新架构的发展。 |
| [^5] | [RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation](https://arxiv.org/abs/2402.15487) | 本文提出了交互式场景探索任务，通过自主探索环境生成了动作条件化场景图，捕捉了环境的结构 |
| [^6] | [Transformers are Expressive, But Are They Expressive Enough for Regression?](https://arxiv.org/abs/2402.15478) | Transformer在逼近连续函数方面存在困难，是否真正是通用函数逼近器仍有待考证 |
| [^7] | [Debiasing Machine Learning Models by Using Weakly Supervised Learning](https://arxiv.org/abs/2402.15477) | 提出了一种针对连续敏感变量的偏见消除策略，基于内生性概念并采用弱监督学习方法，无需对预测模型做任何假设 |
| [^8] | [Leveraging Domain Knowledge for Efficient Reward Modelling in RLHF: A Case-Study in E-Commerce Opinion Summarization](https://arxiv.org/abs/2402.15473) | 提出了一种方法，在RLHF中利用领域知识来降低训练奖励模型所需的大量人类偏好注释数量。 |
| [^9] | [FAIR: Filtering of Automatically Induced Rules](https://arxiv.org/abs/2402.15472) | 该论文提出了一种通过次模块目标函数过滤大量自动诱导规则的算法 FAIR，以解决机器学习算法训练中缺乏大量已注释数据的问题。 |
| [^10] | [Repetition Improves Language Model Embeddings](https://arxiv.org/abs/2402.15449) | 回声嵌入方法通过重复输入来提取信息，解决了自回归模型无法包含后续令牌信息的限制，实验结果表明其能够最大程度充分利用高质量的语言模型进行嵌入。 |
| [^11] | [Unleashing the Power of Imbalanced Modality Information for Multi-modal Knowledge Graph Completion](https://arxiv.org/abs/2402.15444) | 提出了自适应多模态融合和模态对抗训练（AdaMF-MAT）方法，以解决多模态知识图完成中存在的模态信息不平衡问题，发挥不平衡模态信息的力量。 |
| [^12] | [Active Few-Shot Fine-Tuning](https://arxiv.org/abs/2402.15441) | 该论文提出了ITL方法来实现主动少样本微调，通过最大化对下游任务的信息获取，从而在大型神经网络的微调中取得了显著的改进。 |
| [^13] | [Universal Lower Bounds and Optimal Rates: Achieving Minimax Clustering Error in Sub-Exponential Mixture Models](https://arxiv.org/abs/2402.15432) | 本文在混合模型中建立了一个通用下界，通过Chernoff散度来表达，将其拓展到具有次指数尾部的混合模型，并证明了迭代算法在这些混合模型中实现了最佳误差率 |
| [^14] | [Hierarchical Invariance for Robust and Interpretable Vision Tasks at Larger Scales](https://arxiv.org/abs/2402.15430) | 通过分层不变性构建稳健且可解释的视觉系统，克服了不变性表示在较大规模的视觉任务中可区分性有限的问题。 |
| [^15] | [ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion Models against Stochastic Perturbation](https://arxiv.org/abs/2402.15429) | 本研究引入了概率概念的文本到图像扩散模型鲁棒性，并建立了一个名为ProTIP的高效框架用于评估其统计保证，解决了生成过程的高计算成本和对抗性样本判断困难的问题 |
| [^16] | [A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models](https://arxiv.org/abs/2402.15422) | 本研究探讨了使用大型语言模型基于医生笔记生成患者总结的潜力，通过严格的标记协议和医学专家标记实验发现，在无幻觉数据上进行微调能有效减少幻觉的生成，并保留相关信息。 |
| [^17] | [PREDILECT: Preferences Delineated with Zero-Shot Language-based Reasoning in Reinforcement Learning](https://arxiv.org/abs/2402.15420) | 本文提出了一种在强化学习中利用零样本语言推理来划分偏好的方法，通过扩展查询信息并重新定义奖励学习目标，提高了样本效率。 |
| [^18] | [The Impact of LoRA on the Emergence of Clusters in Transformers](https://arxiv.org/abs/2402.15415) | 本文利用转换器数学框架探讨了LoRA算法对Token聚类结构动态的影响，发现在不同参数下，修改后的注意力矩阵动态的聚类表现出较长时间的显著差异，但仍在短时间内保持密切相似。 |
| [^19] | [Does Combining Parameter-efficient Modules Improve Few-shot Transfer Accuracy?](https://arxiv.org/abs/2402.15414) | 本文探索了LoRA模块的可组合性，并发现在少样本情景下，无论是均匀组成还是学习组成，对视觉和语言模型进行组合都可以提高对未知下游任务的泛化能力。 |
| [^20] | [G-RepsNet: A Fast and General Construction of Equivariant Networks for Arbitrary Matrix Groups](https://arxiv.org/abs/2402.15413) | G-RepsNet 是一种轻量级的等变网络，用于任意矩阵群，通过在神经网络的隐藏层中使用张量表示和简单廉价的张量操作，实现了具有表现力的通用等变网络。 |
| [^21] | [Optimisic Information Directed Sampling](https://arxiv.org/abs/2402.15411) | 提出了一种名为乐观信息导向采样的算法模板, 结合了贝叶斯理论和最坏情形理论，能够实现类似于贝叶斯方法的实例相关遗憾保证，但无需贝叶斯假设。 |
| [^22] | [Lasso with Latents: Efficient Estimation, Covariate Rescaling, and Computational-Statistical Gaps](https://arxiv.org/abs/2402.15409) | 在处理拥有潜在变量的稀疏线性回归问题时，通过对协变量进行异质缩放，Lasso方法可以获得强有力的估计保证。 |
| [^23] | [Conformalized-DeepONet: A Distribution-Free Framework for Uncertainty Quantification in Deep Operator Networks](https://arxiv.org/abs/2402.15406) | 采用conformal prediction框架为Deep Operator Network中的回归问题提供了具有覆盖保证的置信区间，同时引入Quantile-DeepONet进一步提高了不确定性量化的效果。 |
| [^24] | [United We Pretrain, Divided We Fail! Representation Learning for Time Series by Pretraining on 75 Datasets at Once](https://arxiv.org/abs/2402.15404) | 引入了一种新的自监督对比预训练方法，通过在多个未标记和多样化的时间序列数据集上学习一个编码，证明了这种方法在低数据情境下优于监督训练和其他自监督预训练方法，颠覆了常见认知，时间序列可以从多个数据集中学习 |
| [^25] | [Grasp, See and Place: Efficient Unknown Object Rearrangement with Policy Structure Prior](https://arxiv.org/abs/2402.15402) | 该论文提出了一种具有策略结构先验的高效未知物体重新排列系统，通过内外环的学习，实现了抓取、观察和放置在感知噪声中的优化。 |
| [^26] | [Distributionally Robust Off-Dynamics Reinforcement Learning: Provable Efficiency with Linear Function Approximation](https://arxiv.org/abs/2402.15399) | 研究了离策略强化学习中的在线分布鲁棒马尔可夫决策过程，设计了可以去除非线性和避免误差传播的$d$-矩形不确定性集合，并提出了第一个针对离策略具有线性函数逼近的在线DRMDP算法，并且具有高效性。 |
| [^27] | [TransFlower: An Explainable Transformer-Based Model with Flow-to-Flow Attention for Commuting Flow Prediction](https://arxiv.org/abs/2402.15398) | TransFlower模型是一种基于Transformer的可解释模型，采用Flow-to-Flow注意力来预测城市通勤模式，以解决深度学习模型准确性和可解释性之间的权衡问题。 |
| [^28] | [NeuralThink: Algorithm Synthesis that Extrapolates in General Tasks](https://arxiv.org/abs/2402.15393) | NeuralThink 是一种新的递归架构，可以一贯地对对称和不对称任务进行外推，相较于之前的最先进的深度思维架构在稳定地从较小的训练规模对大观测进行外推方面表现出更好的性能。 |
| [^29] | [Offline Inverse RL: New Solution Concepts and Provably Efficient Algorithms](https://arxiv.org/abs/2402.15392) | 该论文提出了一种新的可行奖励集概念，以应对离线设定的机会和限制，并分析了其估计的复杂性。 |
| [^30] | [Genie: Generative Interactive Environments](https://arxiv.org/abs/2402.15391) | Genie是第一个经过无监督训练的生成交互环境，可生成各种动作可控的虚拟世界，并提供了学习潜在行动空间以训练代理程序模仿未见视频行为的可能性。 |
| [^31] | [Explorations of Self-Repair in Language Models](https://arxiv.org/abs/2402.15390) | 自修复现象存在于各种模型家族和尺寸上，但在完整的训练分布上是不完美和嘈杂的，有两种机制可促成自修复，包括最终LayerNorm缩放因子的变化和实现反擦除的稀疏神经元集。 |
| [^32] | [Outlier detection by ensembling uncertainty with negative objectness](https://arxiv.org/abs/2402.15374) | 提出一种利用不确定性和负对象性集成的异常检测方法，通过直接预测K+1个logits并在密集预测结构中嵌入，可独立检测异常值。 |
| [^33] | [Dual Encoder: Exploiting the Potential of Syntactic and Semantic for Aspect Sentiment Triplet Extraction](https://arxiv.org/abs/2402.15370) | 提出了一种双编码器模型（D2E2S），结合了BERT通道和增强型LSTM通道来最大化单词间的句法和语义关系，引入了异构特征交互模块用于捕获复杂互动和动态选择重要节点。 |
| [^34] | [Efficient semi-supervised inference for logistic regression under case-control studies](https://arxiv.org/abs/2402.15365) | 针对病例-对照研究的逻辑回归半监督推断，利用未标记数据可以识别截距参数，解决了截距参数在半监督学习中不可辨识的问题。 |
| [^35] | [All Thresholds Barred: Direct Estimation of Call Density in Bioacoustic Data](https://arxiv.org/abs/2402.15360) | 直接估计生物声学数据中的呼叫密度而不考虑分类器分数，从而消除阈值选择引起的偏差计数。 |
| [^36] | [Streaming Gaussian Dirichlet Random Fields for Spatial Predictions of High Dimensional Categorical Observations](https://arxiv.org/abs/2402.15359) | 该研究提出了一种新的流式高斯狄利克雷随机场模型，能够高效地处理高维分类观测数据，在空间预测中取得更准确的预测结果，并实现了有效的信息路径规划。 |
| [^37] | [On normalization-equivariance properties of supervised and unsupervised denoising methods: a survey](https://arxiv.org/abs/2402.15352) | 本文对图像去噪的监督和无监督学习方法进行了调查，着重关注归一化等变性质，突出了方法的原理和局限性 |
| [^38] | [AutoMMLab: Automatically Generating Deployable Models from Language Instructions for Computer Vision Tasks](https://arxiv.org/abs/2402.15351) | AutoMMLab是一个通用的LLM增强AutoML系统，通过用户的语言指令来自动化计算机视觉任务的整个模型生成工作流程，使非专家个体更容易构建特定任务的模型。 |
| [^39] | [Farsight: Fostering Responsible AI Awareness During AI Application Prototyping](https://arxiv.org/abs/2402.15350) | Farsight是一个新颖的实地交互工具，帮助人们在设计AI应用原型时识别潜在危害，用户研究表明使用Farsight后，AI原型设计者能够更好地独立识别与提示相关的潜在危害。 |
| [^40] | [Information-Theoretic Safe Bayesian Optimization](https://arxiv.org/abs/2402.15347) | 提出了一种信息论安全探索准则，结合贝叶斯优化收益函数，形成了一种新颖的安全贝叶斯优化选择准则。 |
| [^41] | [Fourier Basis Density Model](https://arxiv.org/abs/2402.15345) | 引入了一种基于受限Fourier基的轻量级、灵活且端到端可训练的概率密度模型，能够有效逼近各种多模态1维密度，表现优于传统的深度因式模型，同时在学习压缩任务中展示了其实用性。 |
| [^42] | [Iteration and Stochastic First-order Oracle Complexities of Stochastic Gradient Descent using Constant and Decaying Learning Rates](https://arxiv.org/abs/2402.15344) | 研究了在深度学习中使用固定或递减学习率的SGD进行非凸优化时，批量大小与迭代和SFO复杂度之间的关系，并指出使用关键批量大小的SGD可以最小化SFO复杂度 |
| [^43] | [NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data](https://arxiv.org/abs/2402.15343) | 利用LLM注释数据进行实体识别编码器预训练，创建了NuNER，一种专门用于命名实体识别任务的紧凑语言表示模型，可以在少样本学习领域胜过相似大小的基础模型，并与更大的LLMs竞争。 |
| [^44] | [Ranking Entities along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies](https://arxiv.org/abs/2402.15337) | 本研究通过使用LLMs探索概念空间维度，提出了一种新颖的实体排名方法，并分析其在感知和主观特征上的转移能力。 |
| [^45] | [Low-Rank Representations Meets Deep Unfolding: A Generalized and Interpretable Network for Hyperspectral Anomaly Detection](https://arxiv.org/abs/2402.15335) | 提出了一个广义和可解释的HAD网络，通过深度展开可学习字典的LRR模型，称为LRR-Net$^+$，可以更通用地光谱解耦背景结构和目标特性，并消除由重要干扰目标引入的偏差。同时，还构建了用于改进HAD算法鲁棒性的新基准数据集AIR-HAD。 |
| [^46] | [Categorical Deep Learning: An Algebraic Theory of Architectures](https://arxiv.org/abs/2402.15332) | 提出了一种关于深度学习架构的代数理论，应用范畴论构建了一个桥梁，有效地涵盖了神经网络设计的不同风格，同时自然地编码了计算机科学和自动机理论中的许多标准结构。 |
| [^47] | [Towards Principled Task Grouping for Multi-Task Learning](https://arxiv.org/abs/2402.15328) | 提出了一种为多任务学习建立基于原则的任务分组方法，该方法在理论和实践上具有优势，通过灵活的数学规划形式解决了资源约束问题。 |
| [^48] | [Understanding Oversmoothing in Diffusion-Based GNNs From the Perspective of Operator Semigroup Theory](https://arxiv.org/abs/2402.15326) | 本文研究了扩散型GNN中的过度平滑问题，通过算子半群理论严格证明了过度平滑与扩散算子的遍历性密切相关，提出了一个更普遍和理论上基础的减轻过度平滑问题的方法，并提供了概率解释。 |
| [^49] | [Shapley Value Based Multi-Agent Reinforcement Learning: Theory, Method and Its Application to Energy Network](https://arxiv.org/abs/2402.15324) | 通过合作博弈论，本论文将Shapley值和凸博弈概念扩展到马尔可夫决策过程，用于解决多智能体强化学习中的信用分配问题。 |
| [^50] | [OpenSUN3D: 1st Workshop Challenge on Open-Vocabulary 3D Scene Understanding](https://arxiv.org/abs/2402.15321) | 提供了OpenSUN3D研讨会上针对开放词汇3D场景理解的挑战概述，包括挑战数据集、评估方法和获胜方法的简要描述 |
| [^51] | [GPTVQ: The Blessing of Dimensionality for LLM Quantization](https://arxiv.org/abs/2402.15319) | 通过增加量化维度，GPTVQ方法在大型语言模型的量化中取得了新的最优结果，不仅显著改善了大小与准确性的权衡，还提高了处理效率。 |
| [^52] | [On Minimal Depth in Neural Networks](https://arxiv.org/abs/2402.15315) | 本研究研究了神经网络中关于最小深度的问题，特别关注了ReLU神经网络的表达能力和最小深度与CPWL函数的关系。 |
| [^53] | [ArabianGPT: Native Arabic GPT-based Large Language](https://arxiv.org/abs/2402.15313) | 提出了ArabianGPT，这是一系列专门为阿拉伯语设计的基于Transformer的模型，包括大小和复杂性不同的ArabianGPT-0.1B和ArabianGPT-0.3B，帮助弥补了本土阿拉伯语大型语言模型的不足。 |
| [^54] | [Counterfactual Generation with Identifiability Guarantees](https://arxiv.org/abs/2402.15309) | 反事实生成面临着配对数据稀缺和标注信息有限等挑战，现有方法依赖过度简化的假设，但复杂数据分布下这些假设可能不成立。 |
| [^55] | [Representing Online Handwriting for Recognition in Large Vision-Language Models](https://arxiv.org/abs/2402.15307) | 本文研究了在大型视觉语言模型中进行在线手写识别，提出了一种新颖的数字墨水(tokenized representation)表示方法，将手写呈现为文本序列和图像，取得了可与现有方法媲美的结果。 |
| [^56] | [Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models](https://arxiv.org/abs/2402.15301) | 利用大型语言模型和检索增强生成技术，提出了一种新方法用于从科学文献中推断因果关系，以解决传统方法受限于数据收集偏见和个体知识的问题。 |
| [^57] | [Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding](https://arxiv.org/abs/2402.15300) | CLIP相似性作为更强大和更稳健的幻觉指标，研究提出了CLIP引导解码（CGD）方法，在大型视觉-语言模型中有效减少对象幻觉。 |
| [^58] | [Semi-supervised Counting via Pixel-by-pixel Density Distribution Modelling](https://arxiv.org/abs/2402.15297) | 提出了一种基于像素级密度分布建模的半监督人群计数方法，通过匹配损失、密度标记和自监督学习机制，取得了明显优于竞争对手的效果 |
| [^59] | [A Survey of Music Generation in the Context of Interaction](https://arxiv.org/abs/2402.15294) | 该论文调查了音乐生成领域的现状，针对机器学习在音乐创作中的应用进行了综合评估，探讨了当前研究的主要焦点以及存在的挑战。 |
| [^60] | [Linear Dynamics-embedded Neural Network for Long-Sequence Modeling](https://arxiv.org/abs/2402.15290) | 提出了一种名为嵌入线性动力学的神经网络（LDNN），通过引入连续状态空间模型的属性和优化策略，实现了在长序列任务中具有少量参数、灵活推断和高效训练，最终在长距离竞技场上取得了有效且领先的性能。 |
| [^61] | [Let's Rectify Step by Step: Improving Aspect-based Sentiment Analysis with Diffusion Models](https://arxiv.org/abs/2402.15289) | 提出了DiffusionABSA，一种新颖的扩散模型，可以逐步提取方面，并通过识别途中的上下文信息来改进基于方面的情感分析。 |
| [^62] | [Real-Time FPGA Demonstrator of ANN-Based Equalization for Optical Communications](https://arxiv.org/abs/2402.15288) | 该论文提出了一个实时执行光通信系统均衡的高吞吐量FPGA演示器，采用基于ANN的方法。 |
| [^63] | [Generative Modelling with Tensor Train approximations of Hamilton--Jacobi--Bellman equations](https://arxiv.org/abs/2402.15285) | 使用张量矩阵逼近哈密尔顿-雅各比-贝尔曼方程，提出了一种在生成建模中解决HJB方程的新方法，该方法无需样本，不依赖于归一化常数，并能避免维数灾难。 |
| [^64] | [Spatiotemporal Observer Design for Predictive Learning of High-Dimensional Data](https://arxiv.org/abs/2402.15284) | 本文设计了一个名为“时空观察者”的观察者理论引导的深度学习架构，为高维数据的预测学习提供了泛化误差界限和收敛保证，并引入了动态正则化以更好地学习系统动态。 |
| [^65] | [When in Doubt, Think Slow: Iterative Reasoning with Latent Imagination](https://arxiv.org/abs/2402.15283) | 通过在决策时应用迭代推理来微调推断的代理状态，能够在视觉3D导航任务中取得一致的性能改进，并在部分可观察环境中得到更好的表现。 |
| [^66] | [Neural Implicit Swept Volume Models for Fast Collision Detection](https://arxiv.org/abs/2402.15281) | 提出了一种新颖的神经隐式扫描体模型，能够连续表示任意运动，并结合了深度学习速度和几何碰撞检查的准确性保证。 |
| [^67] | [Classification Under Strategic Self-Selection](https://arxiv.org/abs/2402.15274) | 用户在学习分类器后决定是否参与的战略自我选择对学习效果和自我选择人口构成产生了影响，我们提出了一种可优化的可微分框架。 |
| [^68] | [Optimized Deployment of Deep Neural Networks for Visual Pose Estimation on Nano-drones](https://arxiv.org/abs/2402.15273) | 提出了一种用于纳米无人机上视觉姿态估计任务的深度神经网络优化部署流水线，利用神经架构搜索算法进行探索，并通过新型软件内核在off-the-shelf纳米无人机上部署，将推断延迟缩短了3.22倍 |
| [^69] | [Smoothed Graph Contrastive Learning via Seamless Proximity Integration](https://arxiv.org/abs/2402.15270) | SGCL模型通过三种不同的平滑技术调整对比损失中节点对的惩罚，从而形成具有接近度感知的正样本和负样本 |
| [^70] | [MemoryPrompt: A Light Wrapper to Improve Context Tracking in Pre-trained Language Models](https://arxiv.org/abs/2402.15268) | MemoryPrompt方法通过引入辅助循环网络，将信息传递给语言模型，从而改进了预训练语言模型在上下文跟踪方面的性能，避免了灾难性遗忘现象。 |
| [^71] | [Calibration of Deep Learning Classification Models in fNIRS](https://arxiv.org/abs/2402.15266) | 在fNIRS领域，我们提出将校准整合到模型中以评估其可靠性，结果显示许多现有模型的校准性能不佳。 |
| [^72] | [Dynamic Memory Based Adaptive Optimization](https://arxiv.org/abs/2402.15262) | 提出了一种称为“回顾式学习法律修正”的通用方法，用于计算内存单元的动态变化线性组合，能够在具有线性更新规则和小内存的优化器中取得优于经典优化器的性能。 |
| [^73] | [Open Ad Hoc Teamwork with Cooperative Game Theory](https://arxiv.org/abs/2402.15259) | 提出了采用合作博弈论解释开放式即兴团队合作中联合Q值表示的新理论，为进一步发展这一研究方向和应用提供了新思路 |
| [^74] | [High Resolution Guitar Transcription via Domain Adaptation](https://arxiv.org/abs/2402.15258) | 通过域自适应实现了高分辨率吉他转录的新方法，并在零样本情况下在GuitarSet上取得了最先进的转录结果。 |
| [^75] | [Optimal Transport for Structure Learning Under Missing Data](https://arxiv.org/abs/2402.15255) | 提出了一种基于最优输运的得分算法，用于从缺失数据中学习因果结构，通过将结构学习视为密度拟合问题，并通过最小化与观测数据分布之间的沃尔仑斯坦距离来找到导致观测数据分布的因果模型 |
| [^76] | [A Bargaining-based Approach for Feature Trading in Vertical Federated Learning](https://arxiv.org/abs/2402.15247) | 该研究提出了一种基于谈判的垂直联邦学习特征交易方法，以促进经济高效的交易 |
| [^77] | [GS-EMA: Integrating Gradient Surgery Exponential Moving Average with Boundary-Aware Contrastive Learning for Enhanced Domain Generalization in Aneurysm Segmentation](https://arxiv.org/abs/2402.15239) | 提出了一种新颖的领域泛化策略，结合梯度手术指数移动平均（GS-EMA）优化技术和边界感知对比学习（BACL），提高了动脉瘤分割的鲁棒性和准确性 |
| [^78] | [Unsupervised Domain Adaptation for Brain Vessel Segmentation through Transwarp Contrastive Learning](https://arxiv.org/abs/2402.15237) | 本文提出了一个简单而强大的对比学习框架，用于无监督领域自适应，可以改善医学影像分析中脑血管分割的性能。 |
| [^79] | [Classification of compact radio sources in the Galactic plane with supervised machine learning](https://arxiv.org/abs/2402.15232) | 本研究使用射电和红外图像，结合20,000张图像数据集进行监督机器学习，实现对银河平面中紧凑射电源的分类。 |
| [^80] | [Which Model to Transfer? A Survey on Transferability Estimation](https://arxiv.org/abs/2402.15231) | 转移学习方法关注利用源预训练模型或数据解决目标任务，模型可转移性评估旨在提出适用度量标准，这项研究对该领域进展进行了综述并分类为两种模式。 |
| [^81] | [Fixed Random Classifier Rearrangement for Continual Learning](https://arxiv.org/abs/2402.15227) | 提出了一种名为固定随机分类器重排（FRCR）的两阶段持续学习算法，通过替换可学习的分类器为固定的随机分类器，在不影响网络性能的情况下，约束了等价的单分类器的范数。 |
| [^82] | [ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition](https://arxiv.org/abs/2402.15220) | ChunkAttention是一种前缀感知的自注意力模块，通过将键/值张量分解为较小的块并结构化到辅助前缀树中，实现了在运行时改善内存利用率的KV缓存，同时设计了两阶段分区算法以提高自注意力计算中的数据局部性。 |
| [^83] | [Statistical Agnostic Regression: a machine learning method to validate regression models](https://arxiv.org/abs/2402.15213) | 本文提出了一种新的方法，统计无关地评估了线性回归模型，并评估了ML估计在检测方面的表现。 |
| [^84] | [Bidirectional Uncertainty-Based Active Learning for Open Set Annotation](https://arxiv.org/abs/2402.15198) | 本文提出了一种双向不确定性主动学习（BUAL）框架，通过随机标签负学习方法和双向不确定性采样策略，旨在同时筛选既可能属于已知类别又高度信息丰富的示例，解决了在开放集场景下确定最有价值示例的挑战。 |
| [^85] | [Safety Optimized Reinforcement Learning via Multi-Objective Policy Optimization](https://arxiv.org/abs/2402.15197) | 本文提出了一种基于多目标策略优化框架的安全强化学习算法，通过安全评论家塑造环境奖励函数，使得策略可以同时朝着最优性和安全性优化，相较于传统方法，该算法无需约束策略搜索空间，实现了安全性和最优性之间的自然权衡。 |
| [^86] | [The AffectToolbox: Affect Analysis for Everyone](https://arxiv.org/abs/2402.15195) | AffectToolbox是一个新型软件系统，旨在为开发情感敏感研究和原型的研究人员提供支持，无需编程知识即可可靠分析用户情感状态，实现多模情感识别和融合评估。 |
| [^87] | [Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized Control](https://arxiv.org/abs/2402.15194) | 扩散模型的微调方法可以通过最大化奖励函数的价值来以目标导向方式进行微调，但可能会面临奖励崩溃的挑战。 |
| [^88] | [Biomedical Entity Linking as Multiple Choice Question Answering](https://arxiv.org/abs/2402.15189) | 提出了一种新颖的模型BioELQA，将生物医学实体链接看作是多项选择问答，通过使用快速检索器获得候选实体，实现了更好的实体链接效果。 |
| [^89] | [Parameter-Free Algorithms for Performative Regret Minimization under Decision-Dependent Distributions](https://arxiv.org/abs/2402.15188) | 该论文提出了一种针对决策相关分布下的执行风险最小化问题的无参数乐观优化方法，显著改进了利普希茨自助式方法，实验结果显示该方法在数值上的优越性。 |
| [^90] | [GraphEdit: Large Language Models for Graph Structure Learning](https://arxiv.org/abs/2402.15183) | 本研究提出了一种名为GraphEdit的方法，利用大型语言模型（LLMs）学习复杂的图结构化数据中的节点关系，通过在图结构上进行指导调整，增强LLMs的推理能力，从而提高图结构学习的可靠性。 |
| [^91] | [Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement](https://arxiv.org/abs/2402.15180) | 提出了一种通过自我完善和格式化改进LMs对抗越狱攻击的方法，即使在非安全对齐的LMs中也具有出色的安全性，同时降低攻击成功率。 |
| [^92] | [Advancing Parameter Efficiency in Fine-tuning via Representation Editing](https://arxiv.org/abs/2402.15179) | RED通过表示编辑显著降低了可训练参数数量，实现了与完全参数微调和其他PEFT方法相当或更好的结果 |
| [^93] | [Unified View of Grokking, Double Descent and Emergent Abilities: A Perspective from Circuits Competition](https://arxiv.org/abs/2402.15175) | 提供了一个全面框架，统一解释了Grokking、双重下降和新兴能力这三种现象，着重探讨了记忆和泛化电路之间的竞争。 |
| [^94] | [Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed Zeroth-Order Optimizer](https://arxiv.org/abs/2402.15173) | 提出了HiZOO，一种对角Hessian信息的零阶优化器，以增强LLMs微调过程中的模型收敛速度和准确性 |
| [^95] | [Attention-Guided Masked Autoencoders For Learning Image Representations](https://arxiv.org/abs/2402.15172) | 通过引入注意力引导的损失函数，该论文提出了一种新的遮罩自编码器，可以更好地学习图像中的对象表示，相比普通MAE表现出更好的性能。 |
| [^96] | [Covariance-Adaptive Least-Squares Algorithm for Stochastic Combinatorial Semi-Bandits](https://arxiv.org/abs/2402.15171) | 提出了一种协方差自适应的最小二乘算法，利用在线估计协方差结构，相对于基于代理方差的算法获得改进的遗憾上界，特别在协方差系数全为非负时，能有效地利用半臂反馈，并在各种参数设置下表现优异。 |
| [^97] | [The Surprising Effectiveness of Skip-Tuning in Diffusion Sampling](https://arxiv.org/abs/2402.15170) | 跳跃调谐是一种简单而惊人有效的训练方法，可以提高扩散采样中UNet模型的性能，并突破了ODE采样器的限制 |
| [^98] | [Convergence Analysis of Split Federated Learning on Heterogeneous Data](https://arxiv.org/abs/2402.15166) | 本文填补了分裂联邦学习在各异数据上收敛分析的空白，提供了针对强凸和一般凸目标的SFL收敛分析，收敛速率分别为$O(1/T)$和$O(1/\sqrt[3]{T})。 |
| [^99] | [EasyRL4Rec: A User-Friendly Code Library for Reinforcement Learning Based Recommender Systems](https://arxiv.org/abs/2402.15164) | EasyRL4Rec是一个面向基于强化学习的推荐系统的用户友好和高效库，提供了多样化的RL环境、全面的核心模块、一致的评估标准和定制解决方案，旨在帮助简化模型开发并改善长期用户参与度。 |
| [^100] | [Studying the Impact of Stochasticity on the Evaluation of Deep Neural Networks for Forest-Fire Prediction](https://arxiv.org/abs/2402.15163) | 该论文首次系统研究了在随机假设下评估深度神经网络用于森林火灾预测的影响，发现评估对统计的忠实度是在高度随机场景下的可靠替代方法。 |
| [^101] | [Entity-level Factual Adaptiveness of Fine-tuning based Abstractive Summarization Models](https://arxiv.org/abs/2402.15162) | 分析了基于微调的摘要模型在处理知识冲突时的实体级事实适应性，并提出了一种反事实数据增强方法，实验结果表明该方法增强了事实适应性，同时保持了事实一致性。 |
| [^102] | [Spatially-Aware Transformer Memory for Embodied Agents](https://arxiv.org/abs/2402.15160) | 本文探讨了利用包含空间信息的面向空间感知变压器模型，以改善记忆利用效率。 |
| [^103] | [Machine Unlearning of Pre-trained Large Language Models](https://arxiv.org/abs/2402.15159) | 本研究在大型语言模型（LLMs）中探讨了“被遗忘权”的概念，提出机器遗忘作为解决方案，并在预训练模型中建立了全面的遗忘框架及高效的遗忘方法，同时提供了改进超参数鲁棒性以及高效调整超参数的指南。 |
| [^104] | [Self-Adaptive Reconstruction with Contrastive Learning for Unsupervised Sentence Embeddings](https://arxiv.org/abs/2402.15153) | 提出了一种新颖的Self-Adaptive Reconstruction Contrastive Sentence Embeddings（SARCSE）框架，利用自动编码器重建句子中的所有令牌，以帮助模型保留更多细粒度语义，并提出了一种自适应重建损失来缓解对令牌频率的偏见。 |
| [^105] | [On the Duality Between Sharpness-Aware Minimization and Adversarial Training](https://arxiv.org/abs/2402.15152) | 通过研究Sharpness-Aware最小化(SAM)和对抗训练(AT)之间的对偶性，发现单独使用SAM可以提高对抗性能。 |
| [^106] | [TREC: APT Tactic / Technique Recognition via Few-Shot Provenance Subgraph Learning](https://arxiv.org/abs/2402.15147) | 本文提出了TREC，首次尝试通过少样本追溯子图学习实现对APT攻击活动中战术/技术的识别，弥补了现有基于规则的方法无法识别细粒度APT技术和变种APT攻击的不足。 |
| [^107] | [Convergence Analysis of Blurring Mean Shift](https://arxiv.org/abs/2402.15146) | 本文通过将BMS算法解释为优化过程，提供了模糊均值漂移算法的收敛性质分析，相较于现有结果，本研究不仅覆盖数据收敛到单一点的情况，还提供了多点收敛的收敛保证，展示了算法收敛速度快。 |
| [^108] | [The Cost of Parallelizing Boosting](https://arxiv.org/abs/2402.15145) | 我们研究了并行化弱到强Boosting算法学习的成本，证明了即使是对Boosting的轻微并行化也需要在训练复杂度上呈指数级增长。 |
| [^109] | [PUAD: Frustratingly Simple Method for Robust Anomaly Detection](https://arxiv.org/abs/2402.15143) | 提出了一种在特征空间上利用分布检测方法来检测逻辑异常的简单方法 |
| [^110] | [A note on the adjoint method for neural ordinary differential equation network](https://arxiv.org/abs/2402.15141) | 该论文对神经常微分方程网络的伴随方法进行了深入探讨，发现传统的伴随形式与反向传播结果并不等价，同时指出了什么情况下伴随形式会产生与反向传播相同结果。 |
| [^111] | [Deep Coupling Network For Multivariate Time Series Forecasting](https://arxiv.org/abs/2402.15134) | 重新审视多元时间序列的序内和序间关系，提出了一种用于预测的深度耦合网络，可以同时捕捉多阶序内和序间的复杂耦合。 |
| [^112] | [Improving Sentence Embeddings with an Automatically Generated NLI Dataset](https://arxiv.org/abs/2402.15132) | 通过自动生成的NLI数据集改进句子嵌入，实验结果表明该方法在STS任务中表现出色，优于现有方法。 |
| [^113] | [Multi-Armed Bandits with Abstention](https://arxiv.org/abs/2402.15127) | 提出了一个扩展的多臂赌博机问题，引入了弃权选项，并成功设计和分析了算法，实现了渐近和米迷诺下最优。 |
| [^114] | [Accelerating Convergence of Stein Variational Gradient Descent via Deep Unfolding](https://arxiv.org/abs/2402.15125) | 通过深度展开技术，本文提出的可训练SVGD算法加速了其收敛速度，相比传统SVGD变体表现出更快的收敛速度。 |
| [^115] | [Fine-tuning CLIP Text Encoders with Two-step Paraphrasing](https://arxiv.org/abs/2402.15120) | 通过两步重述生成过程对CLIP模型进行微调，以增强对释义的表示能力。 |
| [^116] | [Physics-constrained polynomial chaos expansion for scientific machine learning and uncertainty quantification](https://arxiv.org/abs/2402.15115) | 提出一种物理约束的多项式混沌展开方法，将科学机器学习与不确定性量化无缝集成，有效地实现SciML任务中的不确定性量化和在UQ任务中利用SciML提高不确定性评估。 |
| [^117] | [MSPipe: Efficient Temporal GNN Training via Staleness-aware Pipeline](https://arxiv.org/abs/2402.15113) | 提出了MSPipe，一个通用而高效的MTGNNs框架，实现了最大化训练吞吐量同时保持模型准确性 |
| [^118] | [Chu-ko-nu: A Reliable, Efficient, and Anonymously Authentication-Enabled Realization for Multi-Round Secure Aggregation in Federated Learning](https://arxiv.org/abs/2402.15111) | Chu-ko-nu提出了一种更可靠和匿名认证的多轮安全聚合方案，通过重新分配秘密密钥组件来突破了共享传输的概率P限制 |
| [^119] | [Machine Unlearning by Suppressing Sample Contribution](https://arxiv.org/abs/2402.15109) | 本文提出了一种机器遗忘方法，通过最小化输入敏感度来抑制遗忘数据的贡献，并在实验中表现出优异的性能。 |
| [^120] | [Sampling-based Distributed Training with Message Passing Neural Network](https://arxiv.org/abs/2402.15106) | 该论文介绍了一种基于采样和分布式训练的消息传递神经网络（MPNN），能够有效解决边缘图神经网络在节点数量增加时的扩展挑战。 |
| [^121] | [Trajectory-wise Iterative Reinforcement Learning Framework for Auto-bidding](https://arxiv.org/abs/2402.15102) | 自动广告竞标中使用了一种新的迭代离线强化学习框架，有效缓解了传统RL算法在在线环境下性能下降的问题。 |
| [^122] | [Studying LLM Performance on Closed- and Open-source Data](https://arxiv.org/abs/2402.15100) | 本文研究了LLM在闭源和开源数据上的性能表现，发现在闭源软件数据上，C#的性能变化不大。 |
| [^123] | [Learning solution operators of PDEs defined on varying domains via MIONet](https://arxiv.org/abs/2402.15097) | 通过MIONet学习定义在不同域上的PDE的解算子，实现了解映射的学习，包括各种参数的变化，结果为进一步处理度量空间的逼近理论提供了洞见。 |
| [^124] | [Multimodal Transformer With a Low-Computational-Cost Guarantee](https://arxiv.org/abs/2402.15096) | 提出了一种低成本多模态Transformer (LoCoMT) 机制，通过分配不同的多模态注意力模式给每个注意力头，实现了在减少计算成本的同时最小化性能损失 |
| [^125] | [The Umeyama algorithm for matching correlated Gaussian geometric models in the low-dimensional regime](https://arxiv.org/abs/2402.15095) | 该论文研究了匹配通过潜在节点排列相关的两个高斯几何模型的问题，为低维情况下的精确和几乎精确恢复建立了信息阈值，并进行了梅山算法的数值实验。 |
| [^126] | [AttributionBench: How Hard is Automatic Attribution Evaluation?](https://arxiv.org/abs/2402.15089) | AttributionBench是一个综合基准，揭示了自动归因评估的挑战，即使对于最先进的语言模型也只能达到80%的准确率。 |
| [^127] | [PEMT: Multi-Task Correlation Guided Mixture-of-Experts Enables Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2402.15082) | PEMT 是一种基于多任务迁移学习的参数高效微调框架，通过扩展专家混合框架，以权重组合捕获源任务上训练的适配器，从而有效利用任务特定知识和源目标任务之间的相关性。 |
| [^128] | [Cost-Adaptive Recourse Recommendation by Adaptive Preference Elicitation](https://arxiv.org/abs/2402.15073) | 本文提出了一种集成偏好学习的两步方法，用于生成成本自适应的补救建议，通过设计问答框架和利用两种方法生成补救来考虑主体的不完整信息。 |
| [^129] | [Enhancing One-Shot Federated Learning Through Data and Ensemble Co-Boosting](https://arxiv.org/abs/2402.15070) | 在本论文中，提出了一种名为Co-Boosting的新型框架，通过对合成数据和集成模型相互增强，促进了一次性联邦学习的发展。 |
| [^130] | [Gotcha! Don't trick me with unanswerable questions! Self-aligning Large Language Models for Responding to Unknown Questions](https://arxiv.org/abs/2402.15062) | 提出了一种自我调整方法，利用大型语言模型增强回答未知问题的能力，包括拒绝回答并解释未知问题无法回答的原因。 |
| [^131] | [Fine-tuning Large Language Models for Domain-specific Machine Translation](https://arxiv.org/abs/2402.15061) | 提出了一种名为LlamaIT的基于提示的微调方法，用于领域特定机器翻译任务，解决了大型语言模型在领域特定机器翻译中遇到的挑战。 |
| [^132] | [Mixup Barcodes: Quantifying Geometric-Topological Interactions between Point Clouds](https://arxiv.org/abs/2402.15058) | 提出了一种名为混合条形码的新方法，利用标准持久同调与图像持久同调结合，可以量化任意维度两个点集之间的几何-拓扑相互作用，以及引入简单的统计量来量化这种相互作用的复杂性。 |
| [^133] | [Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions](https://arxiv.org/abs/2402.15055) | 该研究探究了Transformer中注意力头和MLP之间的相互作用，并揭示了特定上下文下激活特定token预测的机制，从而阐明在LLMs中注意力如何促成依赖上下文的专门化处理。 |
| [^134] | [Nonlinear Bayesian optimal experimental design using logarithmic Sobolev inequalities](https://arxiv.org/abs/2402.15053) | 使用对数Sobolev不等式构造的MI下限的贪婪方法在非线性模型优化设计中表现出色 |
| [^135] | [Fiducial Focus Augmentation for Facial Landmark Detection](https://arxiv.org/abs/2402.15044) | 提出了一种专为人脸标记检测任务设计的新型图像增强技术，通过连体架构训练机制和DCCA损失来实现对面部结构理解的提高 |
| [^136] | [KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models](https://arxiv.org/abs/2402.15043) | 该论文引入了KIEval，一种知识引导式交互评估框架，通过LLM-powered "interactor"角色实现动态的抗污染评估 |
| [^137] | [Descripci\'on autom\'atica de secciones delgadas de rocas: una aplicaci\'on Web](https://arxiv.org/abs/2402.15039) | 该论文提出使用人工智能技术结合计算机视觉和自然语言处理，从岩石薄片图像生成文本和语音描述。 |
| [^138] | [Dynamics-Guided Diffusion Model for Robot Manipulator Design](https://arxiv.org/abs/2402.15038) | 该论文提出了动态引导扩散模型，利用共享的动力学网络为不同操作任务生成 manipulator 几何设计，通过设计目标构建的梯度引导手指几何设计的完善过程。 |
| [^139] | [Practice Makes Perfect: Planning to Learn Skill Parameter Policies](https://arxiv.org/abs/2402.15025) | 机器人应该通过估计技能的能力，推断通过练习能力的提升，并将其放置在任务分配中，以选择要练习的技能来最大化未来任务成功的预期。 |
| [^140] | [Probabilistically-sound beam search with masked language models](https://arxiv.org/abs/2402.15020) | 提出了在掩码语言模型上进行束搜索的概率健壮方法，表明其在多个领域中优于传统方法。 |
| [^141] | [Consistency-Guided Temperature Scaling Using Style and Content Information for Out-of-Domain Calibration](https://arxiv.org/abs/2402.15019) | 提出了一种新的一致性引导温度缩放（CTS）策略，通过提供源域数据样本之间的相互监督，显著增强了域外（OOD）校准性能。 |
| [^142] | [Unintended Impacts of LLM Alignment on Global Representation](https://arxiv.org/abs/2402.15018) | 对大型语言模型（LLMs）进行用户偏好对齐可能会导致英语方言和全球意见之间的差异，但也提高了多种语言的能力。 |
| [^143] | [Towards Few-Shot Adaptation of Foundation Models via Multitask Finetuning](https://arxiv.org/abs/2402.15017) | 多任务微调的方法通过在基础模型上对相关任务进行微调，然后适应限制标签数的目标任务，能够降低目标任务中的误差，并提出了一种实用的任务选择算法。 |
| [^144] | [How Important Is Tokenization in French Medical Masked Language Models?](https://arxiv.org/abs/2402.15010) | 子词标记化成为自然语言处理领域的主流标准，但其成功因素，如不同任务和语言的最佳分割粒度、数据源对标记工具的影响以及形态信息在印欧语言中的作用，仍不明确。 |
| [^145] | [opp/ai: Optimistic Privacy-Preserving AI on Blockchain](https://arxiv.org/abs/2402.15006) | opp/ai框架结合了zkML的隐私和opML的效率，为区块链上的AI服务提供了平衡的隐私保护和计算效率。 |
| [^146] | [Comparison of Machine Learning Classification Algorithms and Application to the Framingham Heart Study](https://arxiv.org/abs/2402.15005) | 该研究通过弗莱明汉姆心脏病数据作为案例研究，比较了八种机器学习分类算法在不同训练/测试场景下的预测性能，发现极端梯度提升和支持向量机在训练不平衡数据时存在缺陷。 |
| [^147] | [Divide-or-Conquer? Which Part Should You Distill Your LLM?](https://arxiv.org/abs/2402.15000) | 本文提出了一种将推理任务分解为问题分解阶段和问题解决阶段的策略，发现问题分解阶段相比问题解决更容易提炼为较小模型，并证实该策略胜过单阶段解决方案。 |
| [^148] | [tinyBenchmarks: evaluating LLMs with fewer examples](https://arxiv.org/abs/2402.14992) | 本文研究了减少评估LLMs性能所需的评估次数的策略，并展示了在小规模示例上可以准确估计LLMs在多种基准测试上的性能。 |
| [^149] | [Quantum Theory and Application of Contextual Optimal Transport](https://arxiv.org/abs/2402.14991) | 提出了一种首创的量子计算公式，用于情境化输送计划的摊销优化，并通过预测背景情境中药物剂量参数化的细胞类型分布的变化来验证方法，展示了捕捉剂量引起的细胞分布变化的能力。 |
| [^150] | [Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data](https://arxiv.org/abs/2402.14989) | 神经常微分方程（Neural ODEs）的扩展——神经随机微分方程（Neural SDEs）在处理不规则时间序列数据中的稳定性和性能方面提出了重要指导，需要谨慎设计漂移和扩散函数以保持稳定性。 |
| [^151] | [Verifiable Boosted Tree Ensembles](https://arxiv.org/abs/2402.14988) | 本研究将可验证学习从基本集成方法扩展到高级提升树集成，提出了一个伪多项式时间算法来验证鲁棒性，对基于$L_p$-范数的攻击者具有出色的性能。 |
| [^152] | [On the Performance of Empirical Risk Minimization with Smoothed Data](https://arxiv.org/abs/2402.14987) | 在数据是良好指定和平滑的情况下，对于经验风险最小化（ERM）与平方损失的性能，当类是可从 iid 数据中学习时，ERM能够实现次线性误差。 |
| [^153] | [Privacy-Enhancing Collaborative Information Sharing through Federated Learning -- A Case of the Insurance Industry](https://arxiv.org/abs/2402.14983) | 通过联邦学习实现跨多个保险行业数据集学习单一模型的方法，解决了数据隐私保护问题，提高了赔偿损失模型的预测准确性。 |
| [^154] | [Human Brain Exhibits Distinct Patterns When Listening to Fake Versus Real Audio: Preliminary Evidence](https://arxiv.org/abs/2402.14982) | 人类大脑对真实和虚假音频有不同的反应模式，与深度伪造音频检测算法不同，这为深度伪造音频检测等领域的未来研究方向提供了重要的初步证据。 |
| [^155] | [Comparative Analysis of Data Preprocessing Methods, Feature Selection Techniques and Machine Learning Models for Improved Classification and Regression Performance on Imbalanced Genetic Data](https://arxiv.org/abs/2402.14980) | 研究比较了数据预处理、特征选择和模型选择对训练遗传数据集上模型性能的影响，发现异常值和倾斜会影响模型性能。 |
| [^156] | [Optimizing Language Models for Human Preferences is a Causal Inference Problem](https://arxiv.org/abs/2402.14979) | 本文首次提出将语言模型优化视为一个因果问题，提出了因果偏好优化方法并通过双重稳健CPO(DR-CPO)降低了替代目标的方差。 |
| [^157] | [Mudjacking: Patching Backdoor Vulnerabilities in Foundation Models](https://arxiv.org/abs/2402.14977) | Mudjacking提出了第一个方法来修补基础模型以消除后门漏洞，通过优化问题和梯度下降方法解决此问题。 |
| [^158] | [Unsupervised Domain Adaptation within Deep Foundation Latent Spaces](https://arxiv.org/abs/2402.14976) | 提出了一种在深度基础潜空间内进行无监督领域自适应的方法，通过分析和定性解释，展示了该方法可以优于现有基线，并显示了尚未解决的局限性。 |
| [^159] | [Towards Spatially-Lucid AI Classification in Non-Euclidean Space: An Application for MxIF Oncology Data](https://arxiv.org/abs/2402.14974) | 发展了一个使用空间集合框架的分类器，可以根据点的排列在非欧几里得空间中区分两个类别，对于肿瘤学等应用具有重要意义 |
| [^160] | [GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data](https://arxiv.org/abs/2402.14973) | 提出了一种名为GenCeption的新型MLLM评估框架，可以仅利用单模态数据评估跨模态语义一致性，并有效反映模型产生幻觉的倾向，具有较强的相关性和潜力于流行的MLLM基准结果。 |
| [^161] | [Smoothness Adaptive Hypothesis Transfer Learning](https://arxiv.org/abs/2402.14966) | 本文提出了光滑自适应迁移学习（SATL）算法，通过在两个阶段均采用高斯核，使估计器能够适应目标/源及其偏移函数的未知光滑性。 |
| [^162] | [Reinforcement Learning with Elastic Time Steps](https://arxiv.org/abs/2402.14961) | SEAC是一种弹性时间步长的离策略演员-评论家算法，通过可变持续时间的时间步长，使代理能够根据情况改变控制频率，在模拟环境中表现优异。 |
| [^163] | [The Common Stability Mechanism behind most Self-Supervised Learning Approaches](https://arxiv.org/abs/2402.14957) | 自监督学习方法的共同稳定机制是通过引入有用的归纳偏差来学习有意义的视觉表示，并避免坍缩，这些方法尽管有不同的公式，但隐式优化了类似的目标函数。 |
| [^164] | [In-Context Learning of a Linear Transformer Block: Benefits of the MLP Component and One-Step GD Initialization](https://arxiv.org/abs/2402.14951) | 该论文研究了结合线性注意力和线性MLP组件的线性Transformer块在上下文学习中的性能，证明了其在线性回归任务中几乎可以达到贝叶斯最优风险，并且与一步梯度下降估计器有对应关系。 |
| [^165] | [Enhancing Power Quality Event Classification with AI Transformer Models](https://arxiv.org/abs/2402.14949) | 本论文提出利用启用注意力的Transformer作为工具，通过深度学习框架来准确分类考虑测量噪声、直流偏移和电压信号幅度频率变化等条件下的电能质量事件，无需额外特征提取，精度高于其他学习技术。 |
| [^166] | [Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach](https://arxiv.org/abs/2402.14948) | 提出了基于课程的正无标记学习 CuPUL 方法，能够显著降低嘈杂标签的影响，胜过现有方法 |
| [^167] | [SoK: Analyzing Adversarial Examples: A Framework to Study Adversary Knowledge](https://arxiv.org/abs/2402.14937) | 提出一个理论框架来研究对手知识，通过对抗性样本游戏标准化攻击，对图像分类领域的最新攻击进行分类，从而系统地总结了对手知识，得出新的结论。 |
| [^168] | [Federated Fairness without Access to Sensitive Groups](https://arxiv.org/abs/2402.14929) | 提出了一种不依赖于预定义敏感群体定义或额外标签的方法，通过一个超参数实现公平性和效用之间的权衡，保证任何足够大的人群子集能获得至少最低效用性能。 |
| [^169] | [Learning Inverse Kinodynamics for Autonomous Vehicle Drifting](https://arxiv.org/abs/2402.14928) | 通过数据驱动的方法学习小型自动车的运动学模型，特别是为了实现高速圆形导航和自主漂移，帮助车辆学习世界状态并避开障碍物。 |
| [^170] | [Boosting gets full Attention for Relational Learning](https://arxiv.org/abs/2402.14926) | 引入了适合结构化数据的注意力机制，与树模型结合，用于（梯度）提升的训练。 |
| [^171] | [Efficient Unbiased Sparsification](https://arxiv.org/abs/2402.14925) | 该论文描述了对于排列不变或者可加可分的分裂函数，高效的无偏稀疏化特征。 |
| [^172] | [Practical Insights into Knowledge Distillation for Pre-Trained Models](https://arxiv.org/abs/2402.14922) | 研究对知识蒸馏在预训练模型中的应用进行了深入比较，包括优化的温度和权重参数的调整，以及数据分区KD，揭示了最有效的知识蒸馏策略。 |
| [^173] | [MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases](https://arxiv.org/abs/2402.14905) | MobileLLM通过优化模型架构，采用深度和瘦身结构、嵌入共享和分组查询注意机制，实现了2.7%/4.3%的准确率提升，并提出了一种无需增加模型大小且仅有极小延迟开销的块状权重共享方法 |
| [^174] | [Watermarking Makes Language Models Radioactive](https://arxiv.org/abs/2402.14904) | 本文研究了LLM生成文本的放射性，表明使用数字水印训练数据能更容易检测到，同时也展示了即使只有很少比例的水印训练文本，仍可以高置信度地检测出使用数字水印进行微调的情况。 |
| [^175] | [Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs](https://arxiv.org/abs/2402.14903) | 本研究探讨了在大型语言模型中对输入文本进行tokenization对数值推理的影响，发现采用从右到左的tokenization方式可显著提高算术任务的性能表现。 |
| [^176] | [Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images](https://arxiv.org/abs/2402.14899) | 该研究评估了多模态LLMs在采用串联推理时的对抗鲁棒性，发现串联推理在一定程度上提高了对抗性鲁棒性，但引入了一种新的停止推理攻击技术成功规避了这种增强。 |
| [^177] | [Chain-of-Thought Unfaithfulness as Disguised Accuracy](https://arxiv.org/abs/2402.14897) | 了解Chain-of-Thought生成与大语言模型内部计算的一致程度对于决定是否信任模型输出至关重要，研究发现模型大小与忠实度之间存在着特定关系，并且发现130亿参数模型表现出更高的忠实度。 |
| [^178] | [Data Augmentation is Dead, Long Live Data Augmentation](https://arxiv.org/abs/2402.14895) | 数据增强不过是更好地微调模型，零唁态和少样本数据生成可提高性能 |
| [^179] | [Data-Driven Ground-Fault Location Method in Distribution Power System With Distributed Generation](https://arxiv.org/abs/2402.14894) | 提出了一种基于数据驱动的接地故障定位方法，通过离散小波变换和人工神经网络分析处理数据，实现了对配电系统中故障的准确预测 |
| [^180] | [Novelty Detection on Radio Astronomy Data using Signatures](https://arxiv.org/abs/2402.14892) | 提出了一个新的半监督框架SigNova，用于检测射电天文数据中的异常值，采用特征转换提取摘要统计信息并计算新颖性评分，以识别偏离预期行为的观察范围。 |
| [^181] | [Vygotsky Distance: Measure for Benchmark Task Similarity](https://arxiv.org/abs/2402.14890) | 论文提出了一种基于相对性能而非任务属性的相似性度量方法，即“维果茨基距离”，可帮助减少评估任务数量并保持高验证质量。 |
| [^182] | [Efficient data selection employing Semantic Similarity-based Graph Structures for model training](https://arxiv.org/abs/2402.14888) | 提出了一种基于语义相似性的图结构的高效数据选择机制，可在不经过计算密集型模型或其他密集的预处理转换的情况下，用于模型训练。 |
| [^183] | [Applying Reinforcement Learning to Optimize Traffic Light Cycles](https://arxiv.org/abs/2402.14886) | 提出了将强化学习应用于交通信号灯循环优化，实验证明能显著减少紧急停车次数，降低交通拥堵，改善交通流。 |
| [^184] | [Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning](https://arxiv.org/abs/2402.14883) | 提出了一种名为“双I水印”的水印方法，通过引入两种backdoor数据范例并利用LLM的学习能力，有效地保护了LLM微调定制模型的版权。 |
| [^185] | [Deep Generative Model-based Synthesis of Four-bar Linkage Mechanisms with Target Conditions](https://arxiv.org/abs/2402.14882) | 提出了基于深度生成模型的方法，利用条件生成对抗网络生成满足运动学和准静态要求的多连杆四连杆机构。 |
| [^186] | [Energy-efficiency Limits on Training AI Systems using Learning-in-Memory](https://arxiv.org/abs/2402.14878) | 该论文提出了使用内存中学习的方法训练AI系统时的能效限制，并推导了新的理论下限。 |
| [^187] | [Machine-learning prediction of tipping and collapse of the Atlantic Meridional Overturning Circulation](https://arxiv.org/abs/2402.14877) | 该研究开发了一种机器学习方法，用于预测在嘈杂的动力系统中具有时间变化参数的倾覆，包括预测大西洋经向翻转环流的倾覆和崩溃。 |
| [^188] | [What's in a Name? Auditing Large Language Models for Race and Gender Bias](https://arxiv.org/abs/2402.14875) | 调查发现，大型语言模型存在种族和性别偏见，尤其对与黑人女性相关的名字表现最不利。审计在模型部署和实施时的重要性得到强调。 |
| [^189] | [Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation](https://arxiv.org/abs/2402.14874) | 该研究提出了一种叫做蒸馏对比解码（DCD）的方法，通过结合对比提示与蒸馏技术，有效提升了大型语言模型（LLM）在推理任务上的性能表现，超过了传统的对比解码方法，并在多个基准数据集上取得了显著成果。 |
| [^190] | [Effects of term weighting approach with and without stop words removing on Arabic text classification](https://arxiv.org/abs/2402.14867) | 本研究比较不同的加权特征方法（二元和词频加权）在文本分类中使用和不使用停用词时的影响，通过评估准确性、召回率、精确度和F-度量值，结果表明停用词的处理方式对文本分类结果具有重要影响。 |
| [^191] | [APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models](https://arxiv.org/abs/2402.14866) | APTQ提出了针对大型语言模型的注意力感知后训练混合精度量化方法，在保持模型性能的同时，超越了先前的量化方法，并在零-shot任务上达到了最先进的准确率 |
| [^192] | [DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents](https://arxiv.org/abs/2402.14865) | 本文提出了一种基于心理测量学思想的元探测代理（MPA）动态评估协议，用于评估大型语言模型（LLMs）的能力。 |
| [^193] | [SISSA: Real-time Monitoring of Hardware Functional Safety and Cybersecurity with In-vehicle SOME/IP Ethernet Traffic](https://arxiv.org/abs/2402.14862) | SISSA提出了基于SOME/IP通信流量的方法，用于建模和分析车载功能安全和网络安全，解决了SOME/IP缺乏安全架构的问题，包括硬件故障和五种潜在攻击。 |
| [^194] | [CloudNine: Analyzing Meteorological Observation Impact on Weather Prediction Using Explainable Graph Neural Networks](https://arxiv.org/abs/2402.14861) | 提出了一个名为“CloudNine”的系统，利用可解释图神经网络分析气象观测对特定天气预测的影响 |
| [^195] | [Ranking Large Language Models without Ground Truth](https://arxiv.org/abs/2402.14860) | 不需要基准实况或参考响应的条件下，通过考虑模型的三元组来排名大型语言模型，并提出了两种排名方法。 |
| [^196] | [The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative](https://arxiv.org/abs/2402.14859) | 这里是中文总结出的一句话要点: 论文探讨了在MLLM社会中通过单个操作员间接影响其他代理生成恶意内容的新型漏洞。 |
| [^197] | [HumanEval on Latest GPT Models -- 2024](https://arxiv.org/abs/2402.14852) | 使用最新的GPT-4模型在程序合成方面取得显著进展，通过在HumanEval任务中展示了在零样本Python代码生成中的竞争性性能和更多多步骤范式综合。 |
| [^198] | [Asynchronous and Segmented Bidirectional Encoding for NMT](https://arxiv.org/abs/2402.14849) | 本文介绍了一种基于Transformer的改进模型，引入了异步和分段的双向编码策略，以提高神经机器翻译的效率和准确性。 |
| [^199] | [Deep learning-driven scheduling algorithm for a single machine problem minimizing the total tardiness](https://arxiv.org/abs/2402.14847) | 本文提出了一种基于深度学习的单机问题调度算法，通过神经网络估计最佳问题划分方式以最小化总滞后。 |
| [^200] | [Stick to your Role! Stability of Personal Values Expressed in Large Language Models](https://arxiv.org/abs/2402.14846) | 本文提出研究在大型语言模型中个人价值在不同背景下的表达稳定性，通过模拟对话的方式进行评估，对19个LLMs进行比较研究。 |
| [^201] | [Purifying Large Language Models by Ensembling a Small Language Model](https://arxiv.org/abs/2402.14845) | 通过将大型语言模型与小型语言模型集成，可以有效净化大型语言模型，保持其性能并减轻版权侵权、数据污染和隐私侵犯等问题 |
| [^202] | [The New Era of Dynamic Pricing: Synergizing Supervised Learning and Quadratic Programming](https://arxiv.org/abs/2402.14844) | 结合监督学习和二次规划优化汽车租赁行业的动态定价模型，以提高利润率。 |
| [^203] | [Text Diffusion with Reinforced Conditioning](https://arxiv.org/abs/2402.14843) | 提出了一种名为TREC的文本扩散模型，通过强化调节和时间感知方差缩放解决了现有文本扩散模型在训练过程中自我调节的退化和训练与采样不一致的问题，展示了其在不同序列生成任务中的竞争力。 |
| [^204] | [RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic Features for Distinguishing AI-Generated and Human-Written Texts](https://arxiv.org/abs/2402.14838) | 该研究探究了语义和句法两个方面用于区分AI生成文本和人类撰写文本的问题，并提出了一个高准确度的AI模型，在M4数据集上表现出较好的性能。 |
| [^205] | [An Empirical Categorization of Prompting Techniques for Large Language Models: A Practitioner's Guide](https://arxiv.org/abs/2402.14837) | 编制了一个全面的大型语言模型提示技术清单，并建立了一个跨学科的分类框架，以帮助从业者更有效地利用这些技术。 |
| [^206] | [MIKE: A New Benchmark for Fine-grained Multimodal Entity Knowledge Editing](https://arxiv.org/abs/2402.14835) | MIKE是一个针对细粒度多模态实体知识编辑的全面基准和数据集，突破了现有基准主要侧重于粗粒度知识的局限性，引入了新的知识编辑形式以评估编辑效率。 |
| [^207] | [CliqueParcel: An Approach For Batching LLM Prompts That Jointly Optimizes Efficiency And Faithfulness](https://arxiv.org/abs/2402.14833) | CliqueParcel提出了一种通过提示批处理来提高LLM效率的方法，旨在在推理过程中同时确保准确性和最小化与原始输出的偏差，解决了折价输出问题。 |
| [^208] | [Optimizing Uterine Synchronization Analysis in Pregnancy and Labor through Window Selection and Node Optimization](https://arxiv.org/abs/2402.14827) | 通过窗口方法和节点优化分析EHG信号，提出一种新方法来优化妊娠和分娩中的子宫同步分析。 |
| [^209] | [Deepfake Detection and the Impact of Limited Computing Capabilities](https://arxiv.org/abs/2402.14825) | 本研究旨在通过分析在有限计算资源情况下的深度学习技术的适用性以及探索提高效率的方法，解决了深度伪造检测的问题。 |
| [^210] | [Bringing Generative AI to Adaptive Learning in Education](https://arxiv.org/abs/2402.14601) | 生成式人工智能技术与自适应学习概念的交叉研究将对教育中下一阶段学习格式的发展做出重要贡献。 |
| [^211] | [OmniPred: Language Models as Universal Regressors](https://arxiv.org/abs/2402.14547) | 本文提出了OmniPred框架，用于训练语言模型作为通用的端到端回归器，实验证明，在多个任务上训练时，语言模型能够显著优于传统回归模型。 |
| [^212] | [Parallelized Midpoint Randomization for Langevin Monte Carlo](https://arxiv.org/abs/2402.14434) | 探索在能够进行梯度平行评估的框架中的抽样问题，提出了并行化的随机中点方法，并通过新技术导出了对抽样和目标密度之间Wasserstein距离的上界，量化了并行处理单元带来的运行时改进。 |
| [^213] | [Uncertainty-driven and Adversarial Calibration Learning for Epicardial Adipose Tissue Segmentation](https://arxiv.org/abs/2402.14349) | 提出了一种基于不确定性驱动和对抗校准学习的心外脂肪组织分割方法，通过特征潜空间多级监督网络(SPDNet)，增强分割以更准确估计EAT体积 |
| [^214] | [Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion](https://arxiv.org/abs/2402.14285) | 介绍了一种用于符号音乐生成的不可微分规则引导的新方法，引入了可以与之即插即用的高时间分辨率潜在扩散架构，对音乐质量取得了显著进步 |
| [^215] | [Content Conditional Debiasing for Fair Text Embedding](https://arxiv.org/abs/2402.14208) | 通过在内容条件下确保敏感属性与文本嵌入之间的条件独立性，我们提出了一种可以改善公平性的新方法，在保持效用的同时，解决了缺乏适当训练数据的问题。 |
| [^216] | [Neural Networks and Friction: Slide, Hold, Learn](https://arxiv.org/abs/2402.14148) | 循环神经网络利用GRU架构学习合成数据中复杂摩擦定律动力学，展示了机器学习模型在理解和模拟摩擦过程物理的潜力。 |
| [^217] | [Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions](https://arxiv.org/abs/2402.13777) | 深度生成模型在离线策略学习中展现了巨大潜力，本文提供了首个系统性综述，涵盖了五种主流深度生成模型及其应用。 |
| [^218] | [DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based Graph Continual Learning](https://arxiv.org/abs/2402.13711) | DSLR提出了一种基于覆盖范围的多样性方法，以解决基于重播的图持续学习中回放节点过于集中导致过拟合和灾难性遗忘的问题。 |
| [^219] | [Explainable Classification Techniques for Quantum Dot Device Measurements](https://arxiv.org/abs/2402.13699) | 提出了一种基于合成数据的的可解释特征技术，利用可解释性提升机（EBMs）实现了在量子点调谐中较高的可解释性和准确性。 |
| [^220] | [KetGPT -- Dataset Augmentation of Quantum Circuits using Transformers](https://arxiv.org/abs/2402.13352) | 该研究利用Transformer机器学习架构生成“看起来真实”的量子电路，以增强现有的量子电路数据集。 |
| [^221] | [DeepCode AI Fix: Fixing Security Vulnerabilities with Large Language Models](https://arxiv.org/abs/2402.13291) | 使用大型语言模型修复复杂语义bug，通过新的查询和微调方法来解决长距离代码关系学习的挑战。 |
| [^222] | [MLSTL-WSN: Machine Learning-based Intrusion Detection using SMOTETomek in WSNs](https://arxiv.org/abs/2402.13277) | 提出了一种将机器学习技术与Synthetic Minority Oversampling Technique Tomek Link (SMOTE-TomekLink)算法相结合的创新入侵检测方法 |
| [^223] | [SzCORE: A Seizure Community Open-source Research Evaluation framework for the validation of EEG-based automated seizure detection algorithms](https://arxiv.org/abs/2402.13005) | 提出了SzCORE框架，用于验证基于脑电图的自动癫痫检测算法，旨在标准化验证方法，包括数据集、文件格式、输入内容、性能度量等。 |
| [^224] | [Improving Deep Generative Models on Many-To-One Image-to-Image Translation](https://arxiv.org/abs/2402.12531) | 介绍了一种新的非对称框架，可改进现有深度生成模型在多对一图像到图像翻译上的效果，并在 StarGAN V2 上展示了其性能优化。 |
| [^225] | [Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data](https://arxiv.org/abs/2402.12424) | 本研究探讨了LLM在解释表格数据方面的有效性，比较了文本和图像表格表示对LLM性能的影响，为在表格相关任务上有效使用LLM提供了见解。 |
| [^226] | [End-to-end Supervised Prediction of Arbitrary-size Graphs with Partially-Masked Fused Gromov-Wasserstein Matching](https://arxiv.org/abs/2402.12269) | 提出了利用部分掩码融合的Gromov-Wasserstein匹配进行任意大小图的端对端监督预测方法，并展示了其在不同任务上相比竞争者更高的效率和多功能性。 |
| [^227] | [Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models](https://arxiv.org/abs/2402.11641) | 本文提出了一种利用大规模语言模型设计多功能图学习方法的新概念原型，重点关注“在哪里”和“如何”的角度。 |
| [^228] | [Robust agents learn causal world models](https://arxiv.org/abs/2402.10877) | 智能体必须学习因果模型才能在广泛的分布转变下达到后悔界限，这对迁移学习和因果推断等研究领域有重要影响。 |
| [^229] | [Regret Minimization in Stackelberg Games with Side Information](https://arxiv.org/abs/2402.08576) | 这篇论文研究了侧信息中的Stackelberg博弈，提出了一种方法来解决现实中玩家之间信息交流不充分的情况，并且证明了在这种情况下后悔最小化是有效的。 |
| [^230] | [Score-based generative models break the curse of dimensionality in learning a family of sub-Gaussian probability distributions](https://arxiv.org/abs/2402.08082) | 基于分数的生成模型在学习一个子高斯概率分布族中突破了维数灾难，通过分数匹配生成的分布以维度无关的速率逼近目标分布的总变差。 |
| [^231] | [Online Sequential Decision-Making with Unknown Delays](https://arxiv.org/abs/2402.07703) | 本文提出了在在线顺序决策中处理未知延迟问题的三个延迟算法族，并提供了相应的遗憾界限。 |
| [^232] | [Do Large Code Models Understand Programming Concepts? A Black-box Approach](https://arxiv.org/abs/2402.05980) | 本文使用反事实分析框架评估了十个大型代码模型对四种编程概念的理解情况，发现当前模型缺乏对数据流和控制流等概念的理解。 |
| [^233] | [SDEMG: Score-based Diffusion Model for Surface Electromyographic Signal Denoising](https://arxiv.org/abs/2402.03808) | SDEMG是一种基于得分的扩散模型，用于表面肌电信号去噪。实验证明，SDEMG胜过了其他比较方法。 |
| [^234] | [CPT: Competence-progressive Training Strategy for Few-shot Node Classification](https://arxiv.org/abs/2402.00450) | CPT是一种新颖的两阶段课程学习方法，弥补了传统元学习方法在少样本节点分类上的困难。它使用能力递进的训练策略来提高元学习器的效果和稳定性。 |
| [^235] | [Graph Transformers without Positional Encodings](https://arxiv.org/abs/2401.17791) | 本文介绍了一种不需要位置编码的图变压器模型，该模型通过注意机制本身包含图结构信息，并通过实验证明了其有效性。 |
| [^236] | [Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators](https://arxiv.org/abs/2401.17548) | 本文提出了一种新方法，LIFT，通过利用通道相关性和领先指标，为多元时间序列预测提供准确的预测。LIFT方法可以无缝与任意时间序列预测方法协作，大量实验证明了其有效性。 |
| [^237] | [Structured Probabilistic Coding](https://arxiv.org/abs/2312.13933) | 结构化概率编码（SPC）是一种新的监督式表示学习框架，通过编码和预测任务的信息来学习紧凑且信息丰富的表示，提高语言模型的泛化能力和语言理解能力，并通过结构化正则化实现更好的覆盖率。 |
| [^238] | [Federated Learning with Extremely Noisy Clients via Negative Distillation](https://arxiv.org/abs/2312.12703) | 通过负蒸馏方法，本研究针对极端嘈杂客户的问题，提出了一种通过减少在嘈杂数据上训练的客户权重来优化模型性能的解决方案。 |
| [^239] | [Revisiting the Role of Label Smoothing in Enhanced Text Sentiment Classification](https://arxiv.org/abs/2312.06522) | 通过在文本情感分类中进行深入分析，发现标签平滑可以加速深度模型的收敛，并使不同标签的样本更容易区分 |
| [^240] | [Remembering to Be Fair: Non-Markovian Fairness in Sequential Decision Making](https://arxiv.org/abs/2312.04772) | 本文研究了在序贯决策过程中的非马尔可夫公平性，发现公平往往取决于历史，需要在过程中的不同时间点进行评估。 |
| [^241] | [Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian Perspective](https://arxiv.org/abs/2312.01957) | 本文提出了一种将RLAIF解释为贝叶斯推断的方法，通过经过精炼的自我批评对LLM的输出进行精炼，为获得微调模型提供了一种可行且廉价的替代方案。 |
| [^242] | [Age-Based Scheduling for Mobile Edge Computing: A Deep Reinforcement Learning Approach](https://arxiv.org/abs/2312.00279) | 基于深度强化学习的移动边缘计算调度方案提出了一种新的信息时代定义，通过最小化信息时代来改善应用程序性能。 |
| [^243] | [InteRACT: Transformer Models for Human Intent Prediction Conditioned on Robot Actions](https://arxiv.org/abs/2311.12943) | InteRACT通过在大型人类-人类数据集上预训练条件意图预测模型，并在小型人机数据集上微调，解决了人机交互中的先有鸡还是先有蛋问题。 |
| [^244] | [Adversarial Preference Optimization](https://arxiv.org/abs/2311.08045) | 提出了一种对抗偏好优化（APO）框架，实现了在没有额外注释的情况下，通过对抗学习自适应于生成分布差距。 |
| [^245] | [Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation](https://arxiv.org/abs/2311.04254) | 引入一种名为“一切的思考”（XoT）的新型思考促进方法，借助预训练的强化学习和蒙特卡洛树搜索（MCTS）将外部领域知识融入思想，从而提高大型语言模型（LLMs）的能力，使其可以高效地推广到未知问题。 |
| [^246] | [Joint Problems in Learning Multiple Dynamical Systems](https://arxiv.org/abs/2311.02181) | 聚类时间序列的新问题，提出联合划分轨迹集并学习每个部分的线性动态系统模型，以最小化所有模型的最大误差 |
| [^247] | [Generative Artificial Intelligence in Healthcare: Ethical Considerations and Assessment Checklist](https://arxiv.org/abs/2311.02107) | 对医疗保健中的生成人工智能（GenAI）进行了伦理讨论的范围审查，提出了一份检查表，以推动伦理讨论的全面评估和透明记录。 |
| [^248] | [UniTime: A Language-Empowered Unified Model for Cross-Domain Time Series Forecasting](https://arxiv.org/abs/2310.09751) | UniTime提出了一种语言增强的跨领域时间序列预测统一模型，旨在应对数据特征差异、数据区分困难和收敛速度不同等挑战 |
| [^249] | [Exploring Memorization in Fine-tuned Language Models](https://arxiv.org/abs/2310.06714) | 在微调语言模型过程中，该研究首次全面分析了不同任务中模型的记忆现象，发现了记忆在各种微调任务中表现出显著的差异，并通过稀疏编码理论解释了这种任务差异性。 |
| [^250] | [Fleet Learning via Policy Merging](https://arxiv.org/abs/2310.01362) | 本文研究了通过策略合并解决机器人群体学习中的数据存储和传输问题，并提出了一种基于循环神经网络的分布式学习方法。该方法能够在Meta-World环境中将50个任务的策略行为整合，并在大多数训练任务上表现良好。 |
| [^251] | [Spear and Shield: Adversarial Attacks and Defense Methods for Model-Based Link Prediction on Continuous-Time Dynamic Graphs](https://arxiv.org/abs/2308.10779) | 该论文提出了一种简单而有效的T-SPEAR对抗攻击方法，重点研究了时间图神经网络(TGNN)在连续时间动态图上的链路预测中的脆弱性。 |
| [^252] | [FedDefender: Backdoor Attack Defense in Federated Learning](https://arxiv.org/abs/2307.08672) | FedDefender是一种针对联邦学习中有针对性的中毒攻击的防御机制，通过差分测试来识别潜在包含后门的恶意客户，有效降低攻击成功率到10%。 |
| [^253] | [Oversmoothing: A Nightmare for Graph Contrastive Learning?](https://arxiv.org/abs/2306.02117) | 对于图对比学习（GCL），研究表明增加网络深度会导致过度平滑，包括深度表示和浅层，提出了BlockGCL解决这一问题 |
| [^254] | [SparDL: Distributed Deep Learning Training with Efficient Sparse Communication](https://arxiv.org/abs/2304.00737) | SparDL提出了一种高效稀疏通信框架，使用Spar-Reduce-Scatter和Spar-All-Gather算法来解决稀疏梯度累积困境，避免依赖低效通信算法和额外传输步骤。 |
| [^255] | [Inversion dynamics of class manifolds in deep learning reveals tradeoffs underlying generalisation](https://arxiv.org/abs/2303.05161) | 优化动态发现了在学习不同标签的数据点流形时如何平衡类别分离和特征不变性之间的对立倾向，通过非单调趋势展现了这种权衡现象。 |
| [^256] | [Feature Selection with Annealing for Forecasting Financial Time Series](https://arxiv.org/abs/2303.02223) | 本研究首次在金融领域研究了特征选择与退火方法，为预测金融时间序列提供了有效的解决方案。 |
| [^257] | [Sharp Lower Bounds on Interpolation by Deep ReLU Neural Networks at Irregularly Spaced Data](https://arxiv.org/abs/2302.00834) | 该论文研究了深度ReLU神经网络在不规则间隔数据上的插值问题，证明了在数据点间距指数级小的情况下需要$\Omega(N)$个参数，同时指出现有的位提取技术无法应用于这种情况。 |
| [^258] | [FedDebug: Systematic Debugging for Federated Learning Applications](https://arxiv.org/abs/2301.03553) | FedDebug设计了一个系统化的故障定位框架，通过两个新领域推动了FL调试。FedDebug通过利用记录和重现技术，实现了对FL中实时协同训练的交互式调试。 |
| [^259] | [Predicting Properties of Quantum Systems with Conditional Generative Models](https://arxiv.org/abs/2211.16943) | 使用条件生成模型同时表示一系列状态，从测量中学习不同量子态的共享结构，可以预测基态的任意局部性质，无需为新的可观测量进行进一步训练 |
| [^260] | [Centaur: Federated Learning for Constrained Edge Devices](https://arxiv.org/abs/2211.04175) | Centaur提出了面向受限边缘设备的联邦学习框架，通过数据选择方案和基于分区的训练算法，实现了超限制设备在大型神经网络的高效参与，相比本地训练能获得更高准确性和节约能量。 |
| [^261] | [Simultaneous off-the-grid learning of mixtures issued from a continuous dictionary](https://arxiv.org/abs/2210.16311) | 本文提出了一种名为Group-Nonlinear-Lasso的方法，可以同时估计混合物中的线性系数和特征的非线性参数，并使用证明函数对预测误差提供了高概率界限。 |
| [^262] | [DMODE: Differential Monocular Object Distance Estimation Module without Class Specific Information](https://arxiv.org/abs/2210.12596) | DMODE是一种无需物体类别信息的单目目标距离估计方法，通过融合物体大小变化和摄像头运动来实现对各种目标检测和未知物体的适应，解决了单目距离估计中缺乏参考点和对象特定线索的挑战。 |
| [^263] | [Improving Data Quality with Training Dynamics of Gradient Boosting Decision Trees](https://arxiv.org/abs/2210.11327) | 本文提出了一种基于梯度提升决策树的训练动态来评估每个训练实例行为的方法，针对包含大部分表格化或结构化数据的数据集，相较于自信学习、直接启发式和健壮提升算法，取得了最佳结果。 |
| [^264] | [Experimental Design for Multi-Channel Imaging via Task-Driven Feature Selection](https://arxiv.org/abs/2210.06891) | 提出了一种基于任务驱动特征选择的多通道成像实验设计方法，通过优化设计和训练机器学习模型执行用户指定的图像分析任务。 |
| [^265] | [Interventional Causal Representation Learning](https://arxiv.org/abs/2209.11924) | 干预数据有助于因果表示学习，可以通过干预数据中潜在因素支持的几何特征来识别潜在的因果因素。 |
| [^266] | [GNNInterpreter: A Probabilistic Generative Model-Level Explanation for Graph Neural Networks](https://arxiv.org/abs/2209.07924) | 提出了GNNInterpreter，一种用于解释图神经网络高级决策过程的模型级解释方法，通过学习概率生成图分布来揭示GNN模型内部工作机制。 |
| [^267] | [FP8 Quantization: The Power of the Exponent](https://arxiv.org/abs/2208.09225) | 本文深入研究了FP8格式对神经网络推理的好处，提出了在不同设置中选择尾数和指数位数的方法，并证明了FP8格式在实际网络中的表现更好，尤其对于后训练量化来说优于INT8格式。 |
| [^268] | [On Hypothesis Transfer Learning of Functional Linear Models](https://arxiv.org/abs/2206.04277) | 该研究在函数线性回归下探讨了迁移学习，提出了使用RKHS距离衡量任务相似性，并提出了两种算法来处理迁移，一种需要已知正源，另一种利用聚合技术实现无源信息的稳健传输。同时建立了学习问题的下界，并证明了算法的上界。 |
| [^269] | [Hyperbolic Hierarchical Knowledge Graph Embeddings for Link Prediction in Low Dimensions](https://arxiv.org/abs/2204.13704) | 提出了一种新颖的KGE模型，名为HypH，利用超几何空间嵌入分层数据，以提高知识图中链接预测的性能 |
| [^270] | [Cluster Algebras: Network Science and Machine Learning](https://arxiv.org/abs/2203.13847) | 本研究通过网络科学和机器学习技术对簇代数进行了研究，发现在簇代数的交换图中存在一种优雅的对称性，可成功使用种子数据对簇代数进行分类，准确性超过0.9。 |
| [^271] | [Bernstein Flows for Flexible Posteriors in Variational Bayes](https://arxiv.org/abs/2202.05650) | 该论文提出了一种名为伯恩斯块流变分推断（BF-VI）的方法，能够灵活逼近复杂的多元后验，在实验中表现优于其他VI方法。 |
| [^272] | [Causal Discovery from Conditionally Stationary Time Series](https://arxiv.org/abs/2110.06257) | 该论文提出了一种State-Dependent Causal Inference（SDCI）方法，可以处理一类宽泛的非平稳时间序列，成功地回复出潜在的因果依赖关系。 |
| [^273] | [Adversarial Examples Detection with Bayesian Neural Network](https://arxiv.org/abs/2105.08620) | 提出了一种基于贝叶斯神经网络的新框架，利用随机性模拟隐藏层输出分布，从而改善对抗样本检测性能。 |
| [^274] | [User-friendly guarantees for the Langevin Monte Carlo with inaccurate gradient](https://arxiv.org/abs/1710.00095) | 该论文分析了具有不准确梯度的 Langevin Monte Carlo 算法的采样问题，并在Wasserstein-2距离中提出了改进的误差保证。 |
| [^275] | [Adaptive Deep Learning for Efficient Visual Pose Estimation aboard Ultra-low-power Nano-drones.](http://arxiv.org/abs/2401.15236) | 本研究提出了一种自适应深度学习机制，用于在超低功耗纳米无人机上进行高效的视觉姿态估计。通过将两种具有不同性能和成本权衡的卷积神经网络与自适应分类模块结合使用，我们能够根据计算资源的可用性选择合适的网络以实现高效的姿态估计。 |
| [^276] | [Estimation of partially known Gaussian graphical models with score-based structural priors.](http://arxiv.org/abs/2401.14340) | 本论文提出了一种基于得分结构先验的算法，用于估计部分已知高斯图模型。通过使用图神经网络来估计图的得分函数，我们可以在生成样本时利用退火朗格维能扩散，从而更准确地估计后验分布。数值实验表明，我们的方法具有明显的优势。 |
| [^277] | [Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach.](http://arxiv.org/abs/2401.12686) | 这篇论文提出了一种在稀疏图上学习平均场对局博弈的新方法，通过引入图形扩展的概念，解决了现有方法对于稀疏网络拓扑结构的限制。 |
| [^278] | [GNNShap: Fast and Accurate GNN Explanations using Shapley Values.](http://arxiv.org/abs/2401.04829) | GNNShap是一种使用Shapley值的解释方法，能够快速而准确地解释图神经网络的预测结果。相较于其他方法，GNNShap通过抽样、并行化计算等技术提高了解释速度和精细度。 |
| [^279] | [Machine unlearning through fine-grained model parameters perturbation.](http://arxiv.org/abs/2401.04385) | 本文提出了一种精细的机器去学习策略，通过细粒度模型参数的扰动来实现用户隐私保护，同时保持可控的计算成本。采用遗忘率和记忆保留率等新的指标来评估去学习效果和模型泛化能力。 |
| [^280] | [Neural Causal Abstractions.](http://arxiv.org/abs/2401.02602) | 本文提出了一种新的神经因果抽象方法，通过聚类变量和其域，用于解决真实因果推断任务中的挑战，并通过神经因果模型实现了学习和应用。 |
| [^281] | [Diffusion Models for Reinforcement Learning: A Survey.](http://arxiv.org/abs/2311.01223) | 强化学习中的扩散模型已经成为一种突出的生成模型，通过在样本质量和训练稳定性方面的优势改进了强化学习解决方案。该综述提供了这一新兴领域发展的概述，并探讨了扩散模型在强化学习中的分类法和应用。 |
| [^282] | [Alquist 5.0: Dialogue Trees Meet Generative Models. A Novel Approach for Enhancing SocialBot Conversations.](http://arxiv.org/abs/2310.16119) | Alquist 5.0是一种新的SocialBot系统，通过将对话树和生成模型相结合，以及引入NRG Barista和支持多模式设备，提高了用户对话体验，并保持了共情和知识型对话能力。 |
| [^283] | [Almost Equivariance via Lie Algebra Convolutions.](http://arxiv.org/abs/2310.13164) | 本文研究了几乎等变性的主题，并提供了一个不同于现有定义的几乎等变性定义，并通过利用李群的李代数给出了在模型中编码几乎等变性的实用方法。 |
| [^284] | [ProbTS: A Unified Toolkit to Probe Deep Time-series Forecasting.](http://arxiv.org/abs/2310.07446) | ProbTS是一个统一的工具包，用于协同和比较定制神经架构和深度生成模型在时间序列预测中的方法，揭示了它们的特点、优势和需要进一步研究的领域。 |
| [^285] | [CFDBench: A Comprehensive Benchmark for Machine Learning Methods in Fluid Dynamics.](http://arxiv.org/abs/2310.05963) | 这个论文介绍了CFDBench，这是一个针对计算流体动力学中四个经典问题的基准测试。它包含了不同边界条件、流体物理特性和域几何的数据，能帮助评估深度学习方法在解决物理问题中的表现。 |
| [^286] | [LARA: A Light and Anti-overfitting Retraining Approach for Unsupervised Anomaly Detection.](http://arxiv.org/abs/2310.05668) | LARA是一种轻量级且抗过拟合的无监督异常检测再训练方法，它将重新训练过程形式化为一个凸问题，并设计了一个反思模块以利用历史数据，同时数学证明了在微调后可以获得更好的性能。 |
| [^287] | [A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence Classes with the same Skeleton.](http://arxiv.org/abs/2310.04218) | 本文提出了一个固定参数可处理算法，用于计数具有相同骨架的马尔可夫等价类。 |
| [^288] | [Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection.](http://arxiv.org/abs/2310.02861) | 《Rayleigh Quotient Graph Neural Networks用于图级异常检测的研究》提出使用Rayleigh Quotient作为驱动因素，通过探索图的固有光谱特征来实现图级异常检测。 |
| [^289] | [DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models.](http://arxiv.org/abs/2310.00902) | DataInf是一种高效的影响力近似方法，特别适用于大规模生成型AI模型，相比现有方法在计算和内存效率上有明显优势。 |
| [^290] | [Convolutional Deep Kernel Machines.](http://arxiv.org/abs/2309.09814) | 这项研究介绍了一种称为卷积深度核机器的新型核方法，该方法纯粹使用核而不使用特征，通过高效的跨域诱导点近似方案和多种模型变体的设计，达到了在MNIST、CIFAR-10和CIFAR-100上接近甚至超过其他方法的测试准确率。 |
| [^291] | [Constructing Indoor Region-based Radio Map without Location Labels.](http://arxiv.org/abs/2308.16759) | 本文提出了一种无需位置标签的基于区域的无线电地图构建方法，该方法利用接收信号强度（RSS）测量数据，并通过一个综合的分割和聚类算法实现了全局最优解的匹配。 |
| [^292] | [The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection.](http://arxiv.org/abs/2308.12215) | 本研究通过虚假信息检测为例，检查了机器学习在信任与安全问题中学术与实践之间的脱节，并发现了文献中存在的严重不足之处，包括任务不符合在线服务面临的挑战、数据集和模型评估不真实、评估不独立于模型训练等。在此基础上，提出了评估机器学习应用于信任与安全问题的建议。 |
| [^293] | [Size Lowerbounds for Deep Operator Networks.](http://arxiv.org/abs/2308.06338) | 本文建立了深度算子网络的数据依赖性大小下界，并证明了在解决偏微分方程时，支路网络和主干网络的共同输出维度需要与数据点数量按照Ω(√n)的比例扩展，并且为了获得更低的训练误差，训练数据的大小可能需要与共同输出维度按照二次比例关系扩展。 |
| [^294] | [Simulation-based inference using surjective sequential neural likelihood estimation.](http://arxiv.org/abs/2308.01054) | 我们提出了一种使用全射序列神经似然估计（SSNL）进行基于仿真的推断的新方法，在模型中无法计算似然函数并且只能使用模拟器生成数据的情况下，SSNL通过拟合降维的全射归一化流模型，并将其作为替代似然函数，解决了先前基于似然方法在高维数据集中遇到的问题，并在各种实验中展示了其优越性能。 |
| [^295] | [Quantum Convolutional Neural Networks with Interaction Layers for Classification of Classical Data.](http://arxiv.org/abs/2307.11792) | 本文介绍了一种引入了三量子位相互作用的新型交互层的量子卷积网络，增加了网络的表达能力和纠缠能力，用于对图像和一维数据进行分类。 |
| [^296] | [FMT: Removing Backdoor Feature Maps via Feature Map Testing in Deep Neural Networks.](http://arxiv.org/abs/2307.11565) | 这项工作中提出了一种名为FMT的防御策略，通过检测并移除训练用于从输入中提取后门信息的后门特征图，从而有效防止后门攻击。 |
| [^297] | [Learning Decentralized Partially Observable Mean Field Control for Artificial Collective Behavior.](http://arxiv.org/abs/2307.06175) | 本文提出了一种分散式部分可观察的场均控制模型（Dec-POMFC），用于解决多智能体强化学习中的分散化、部分可观察和可扩展性等挑战。该模型可将问题简化为可解决的单智能体马尔可夫决策过程，为实现人工集体行为提供了解决方案。 |
| [^298] | [Variance-Covariance Regularization Improves Representation Learning.](http://arxiv.org/abs/2306.13292) | 提出了方差-协方差正则化方法，旨在促进学习网络特征的多样性，改善表示学习和迁移学习的性能。 |
| [^299] | [Loss Functions for Behavioral Game Theory.](http://arxiv.org/abs/2306.04778) | 本文研究了行为博弈论家在损失函数选择上的差异，并构建了一组满足特定公理的损失函数集合，其中平方L2误差是实践中唯一可接受的损失函数，并建议行为博弈论家继续使用它。 |
| [^300] | [ConceptBed: Evaluating Concept Learning Abilities of Text-to-Image Diffusion Models.](http://arxiv.org/abs/2306.04695) | 本文提出ConceptBed数据集和评估指标CCD，用于评估文本到图像模型的概念学习和合成能力。 |
| [^301] | [Efficient Recruitment Strategy for Collaborative Mobile Crowd Sensing Based on GCN Trustworthiness Prediction.](http://arxiv.org/abs/2306.04366) | 本文提出了一种基于GCN可信度预测的协同移动群感知的高效招募策略，通过捕获工人之间的非对称信任关系和工人能力来实现有效的任务分配，优于现有方法。 |
| [^302] | [Quick-Tune: Quickly Learning Which Pretrained Model to Finetune and How.](http://arxiv.org/abs/2306.03828) | 本文提出一种方法快速选择最佳的预训练模型和微调超参数，通过生成大规模元数据集并元学习多保真度性能预测器，并在学习新数据集时使用该预测器进行超参数优化，可以快速实现此目标。 |
| [^303] | [Human-Aligned Calibration for AI-Assisted Decision Making.](http://arxiv.org/abs/2306.00074) | 本文通过引入一种基于主动询问决策者个人偏好的置信度构造方法，解决了现有置信度对于决策者信任决策的不准确问题，从而提高决策的准确性和效率。 |
| [^304] | [Tailoring Instructions to Student's Learning Levels Boosts Knowledge Distillation.](http://arxiv.org/abs/2305.09651) | 本文提出了一种个性化指导的学习技术，称为LGTM，其利用蒸馏效应选择样本以增强学生的泛化能力，在GLUE基准测试的6个文本分类任务中优于10个常见的知识蒸馏基线算法。 |
| [^305] | [Learning-Augmented Online Packet Scheduling with Deadlines.](http://arxiv.org/abs/2305.07164) | 本研究提出了一种学习增强的在线数据包调度算法，能有效解决网络缓冲区管理问题。 |
| [^306] | [Robust Implicit Regularization via Weight Normalization.](http://arxiv.org/abs/2305.05448) | 本文提出了使用权重规范化的梯度下降作为过度参数化模型的鲁棒隐式正则化方法，实现了对欧几里德范数较低的参数的隐式偏好，并建立了一个统一框架来解决线性模型和神经网络之间的隐式正则化隔阂。 |
| [^307] | [Unlocking the Power of Open Set : A New Perspective for Open-set Noisy Label Learning.](http://arxiv.org/abs/2305.04203) | 本文提出了一种新的两步对比学习方法CECL，通过利用开放集合示例的有用信息来处理两种类型的标签噪声。该方法在几个基准数据集上得到了验证。 |
| [^308] | [Learning Action Embeddings for Off-Policy Evaluation.](http://arxiv.org/abs/2305.03954) | 本论文探讨了从记录数据中学习动作嵌入，以减少在大型动作空间中反向倾向评分（IPS）估计器的方差，同时提高离线评估的准确性。 |
| [^309] | [Out-of-Domain Intent Detection Considering Multi-turn Dialogue Contexts.](http://arxiv.org/abs/2305.03237) | 本文提出了一个上下文感知的OOD意图检测框架（Caro），用于模拟OOD意图检测任务中的多轮对话上下文，并在提取稳健的表示时删除与意图检测无关的多余信息。Caro在多个标准数据集上表现出最先进的性能，并超越了先前方法。 |
| [^310] | [Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca.](http://arxiv.org/abs/2304.08177) | 这篇论文提出了一种方法，通过扩展LLaMA现有的词汇表，增加了20,000个中文标记，从而提高其编码效率和对汉语语义的理解能力，并在中文数据上进行二次预训练和精细调整模型，以改善LLaMA对中文的理解和生成能力。 |
| [^311] | [CMOS + stochastic nanomagnets: heterogeneous computers for probabilistic inference and learning.](http://arxiv.org/abs/2304.05949) | 本文展示了如何将基于随机磁隧道结（sMTJ）的概率比特（p位）与多功能可编程门阵列（FPGA）相结合，设计出一种能源高效的异构CMOS + X（X = sMTJ）原型，其成功地执行了概率推理和异步Boltzmann学习。 |
| [^312] | [Training Language Models with Language Feedback at Scale.](http://arxiv.org/abs/2303.16755) | 本文提出一种新方法，即利用更丰富的语言反馈进行模仿学习，通过三个迭代步骤对语言模型进行训练以生成更符合人类偏好的输出。 |
| [^313] | [Improving Code Generation by Training with Natural Language Feedback.](http://arxiv.org/abs/2303.16749) | 该论文提出了一种新算法ILF，通过从自然语言反馈中进行学习来显著提高代码生成模型的性能，即使只有少量反馈，也可以获得很好的效果。 |
| [^314] | [Contrastive Learning Is Spectral Clustering On Similarity Graph.](http://arxiv.org/abs/2303.15103) | 本文通过证明标准InfoNCE损失下的对比学习等同于相似性图上的谱聚类，揭示了这种方法的内在等价性，并进一步将这种分析扩展到CLIP模型，提出新的核混合损失函数。 |
| [^315] | [EdgeServe: An Execution Layer for Decentralized Prediction.](http://arxiv.org/abs/2303.08028) | EdgeServe 是一种为去中心化预测而设计的机器学习系统，通过低延迟的消息代理程序将数据路由到可以提供预测的节点。它具有一系列新颖的优化，可以在计算、通信和准确性之间进行折衷。在多摄像机物体跟踪，网络入侵检测和人类活动识别等三个去中心化预测任务中，EdgeServe 展现了很好的性能。 |
| [^316] | [Archetypal Analysis++: Rethinking the Initialization Strategy.](http://arxiv.org/abs/2301.13748) | 本文提出了一种针对原型分析的概率初始化策略 AA ++，能够在13个不同大小和维度的实际数据集上表现优异。 |
| [^317] | [Zero-shot causal learning.](http://arxiv.org/abs/2301.12292) | 无先验因果学习是一个解决预测新型干预措施个性化影响的框架，并通过元学习对任务的处理达成了目的，能够将干预措施的知识传输到未见过的干预措施中，并在合成和真实数据集上表现出了优越性能。 |
| [^318] | [Covert Channel Attack to Federated Learning Systems.](http://arxiv.org/abs/2104.10561) | 这篇论文介绍了一种针对联邦学习系统的新型攻击模型，通过在联邦训练期间污染全局模型实现隐蔽通信，而不影响模型性能。 |
| [^319] | [Efficient Data-Driven Optimization with Noisy Data.](http://arxiv.org/abs/2102.04363) | 本文研究了在已知噪声源的情况下的数据驱动处方问题，并导出了在这个噪声情况下的高效数据驱动形式，并指出它们具有熵最优传输解释。 |

# 详细

[^1]: AgentOhana：为有效智能体学习设计统一数据和训练流水线

    AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning

    [https://arxiv.org/abs/2402.15506](https://arxiv.org/abs/2402.15506)

    AgentOhana提供了一种统一数据和训练流水线的综合解决方案，有助于克服使用大型语言模型（LLMs）进行智能体任务时的挑战。

    

    由大型语言模型（LLMs）提供支持的自主智能体引起了重大研究关注。然而，充分利用LLMs的潜力进行基于智能体的任务面临困难，这是由于具有多轮轨迹的多样化数据源的异构性。在本文中，我们介绍AgentOhana作为解决这些挑战的综合解决方案。AgentOhana从不同环境中聚合智能体轨迹，涵盖了各种情景。它精心地将这些轨迹标准化和统一到一致的格式中，简化了为智能体训练优化的通用数据加载器的创建。通过数据统一，我们的训练流水线在不同数据源之间保持平衡，并在数据集划分和模型训练过程中保持设备之间的独立随机性。此外，我们还介绍了xLAM-v0.1，一个大动作模式

    arXiv:2402.15506v1 Announce Type: new  Abstract: Autonomous agents powered by large language models (LLMs) have garnered significant research attention. However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories. In this paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address these challenges. \textit{AgentOhana} aggregates agent trajectories from distinct environments, spanning a wide array of scenarios. It meticulously standardizes and unifies these trajectories into a consistent format, streamlining the creation of a generic data loader optimized for agent training. Leveraging the data unification, our training pipeline maintains equilibrium across different data sources and preserves independent randomness across devices during dataset partitioning and model training. Additionally, we present \textbf{xLAM-v0.1}, a large action mode
    
[^2]: Co-Supervised Learning: Improving Weak-to-Strong Generalization with Hierarchical Mixture of Experts

    Co-Supervised Learning: Improving Weak-to-Strong Generalization with Hierarchical Mixture of Experts

    [https://arxiv.org/abs/2402.15505](https://arxiv.org/abs/2402.15505)

    提出了一种通过使用分层专家混合模型来改善弱到强泛化的协同监督学习方法，利用一组多样化的专家教师共同监督强大的学生模型。

    

    强有力的模型经过在互联网规模数据上的预训练后，由于缺乏胜任的监督者，在引导其行为时可能会变得困难。最近的研究表明，尽管存在监督噪声，一个强大的学生模型在针对特定目标进行微调后可能会超越其弱教师。然而，这种从弱到强的泛化效果仍然有限，特别是在存在巨大能力差距的情况下。在本文中，我们提出通过利用一组多样化的专家教师，而不是单一的通才教师，共同监督强大的学生模型来应对这一挑战。我们的方法类似于传统的分层专家混合模型，其中包含两个针对协同监督的组件：(i)我们逐步交替进行学生训练和教师分配，利用强大学生模型的增长来识别可能的监督方式；(ii)我们谨慎地强化教师-学生和局部-全局的一致性。

    arXiv:2402.15505v1 Announce Type: cross  Abstract: Steering the behavior of a strong model pre-trained on internet-scale data can be difficult due to the scarcity of competent supervisors. Recent studies reveal that, despite supervisory noises, a strong student model may surpass its weak teacher when fine-tuned on specific objectives. Yet, the effectiveness of such weak-to-strong generalization remains limited, especially in the presence of large capability gaps. In this paper, we propose to address this challenge by harnessing a diverse set of specialized teachers, instead of a single generalist one, that collectively supervises the strong student. Our approach resembles the classical hierarchical mixture of experts, with two components tailored for co-supervision: (i) we progressively alternate student training and teacher assignment, leveraging the growth of the strong student to identify plausible supervisions; (ii) we conservatively enforce teacher-student and local-global consist
    
[^3]: 机械信息自编码器实现自动检测和定位意外的结构损伤

    Mechanics-Informed Autoencoder Enables Automated Detection and Localization of Unforeseen Structural Damage

    [https://arxiv.org/abs/2402.15492](https://arxiv.org/abs/2402.15492)

    该研究提出了一种新颖的"部署和忘记"方法，结合廉价传感器和机械信息自编码器，实现了对结构损伤的自动检测和定位，仅需学习3小时数据即可自主识别和定位不同类型的未知损伤。

    

    结构健康监测（SHM）对于确保建筑物和桥梁等结构的安全性和长寿命至关重要。本文提出了一种基于机械信息自编码器的新颖的“部署和忘记”方法，用于自动检测和定位结构中的损伤。这种方法基于廉价传感器的全 pass 学习和机械信息自编码器的协同组合，在仅学习了 3 小时的数据后，就能自主检测和定位不同类型的意外损伤。

    arXiv:2402.15492v1 Announce Type: new  Abstract: Structural health monitoring (SHM) is vital for ensuring the safety and longevity of structures like buildings and bridges. As the volume and scale of structures and the impact of their failure continue to grow, there is a dire need for SHM techniques that are scalable, inexpensive, operate passively without human intervention, and customized for each mechanical structure without the need for complex baseline models. We present a novel "deploy-and-forget" approach for automated detection and localization of damages in structures. It is based on a synergistic combination of fully passive measurements from inexpensive sensors and a mechanics-informed autoencoder. Once deployed, our solution continuously learns and adapts a bespoke baseline model for each structure, learning from its undamaged state's response characteristics. After learning from just 3 hours of data, it can autonomously detect and localize different types of unforeseen dam
    
[^4]: 深度学习中卷积的全面调查：应用、挑战和未来趋势

    A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges, and Future Trends

    [https://arxiv.org/abs/2402.15490](https://arxiv.org/abs/2402.15490)

    深度学习中的卷积应用广泛，有许多类型的CNNs可满足特定需求，通过比较分析不同类型的CNNs，可以更好地了解它们的优势和劣势，并促进未来新架构的发展。

    

    在当今数字时代，卷积神经网络 (CNNs) 在计算机视觉任务中广泛应用，如图像分类、物体检测和图像分割。有许多类型的CNNs旨在满足特定需求和要求，包括1D、2D和3D CNNs，以及扩张的、分组的、注意力的、深度可分的卷积和NAS等。每种类型的CNN具有其独特的结构和特点，使其适用于特定任务。深入了解并对这些不同类型的CNN进行比较分析是至关重要的，以了解它们的优势和劣势。此外，研究每种类型CNN的性能、限制和实际应用可以帮助未来开发新的改进架构。我们还探讨了研究人员用于研究或开发的平台和框架。

    arXiv:2402.15490v1 Announce Type: new  Abstract: In today's digital age, Convolutional Neural Networks (CNNs), a subset of Deep Learning (DL), are widely used for various computer vision tasks such as image classification, object detection, and image segmentation. There are numerous types of CNNs designed to meet specific needs and requirements, including 1D, 2D, and 3D CNNs, as well as dilated, grouped, attention, depthwise convolutions, and NAS, among others. Each type of CNN has its unique structure and characteristics, making it suitable for specific tasks. It's crucial to gain a thorough understanding and perform a comparative analysis of these different CNN types to understand their strengths and weaknesses. Furthermore, studying the performance, limitations, and practical applications of each type of CNN can aid in the development of new and improved architectures in the future. We also dive into the platforms and frameworks that researchers utilize for their research or develop
    
[^5]: RoboEXP: 通过交互式探索实现动作条件化场景图用于机器人操作

    RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation

    [https://arxiv.org/abs/2402.15487](https://arxiv.org/abs/2402.15487)

    本文提出了交互式场景探索任务，通过自主探索环境生成了动作条件化场景图，捕捉了环境的结构

    

    机器人需要探索周围环境以适应并应对未知环境中的任务。本文介绍了交互式场景探索的新任务，其中机器人自主探索环境并生成一个捕捉基础环境结构的动作条件化场景图（ACSG）

    arXiv:2402.15487v1 Announce Type: cross  Abstract: Robots need to explore their surroundings to adapt to and tackle tasks in unknown environments. Prior work has proposed building scene graphs of the environment but typically assumes that the environment is static, omitting regions that require active interactions. This severely limits their ability to handle more complex tasks in household and office environments: before setting up a table, robots must explore drawers and cabinets to locate all utensils and condiments. In this work, we introduce the novel task of interactive scene exploration, wherein robots autonomously explore environments and produce an action-conditioned scene graph (ACSG) that captures the structure of the underlying environment. The ACSG accounts for both low-level information, such as geometry and semantics, and high-level information, such as the action-conditioned relationships between different entities in the scene. To this end, we present the Robotic Explo
    
[^6]: Transformer是表现力强大的，但是对于回归任务来说表现力足够吗？

    Transformers are Expressive, But Are They Expressive Enough for Regression?

    [https://arxiv.org/abs/2402.15478](https://arxiv.org/abs/2402.15478)

    Transformer在逼近连续函数方面存在困难，是否真正是通用函数逼近器仍有待考证

    

    Transformer已成为自然语言处理中至关重要的技术，在机器翻译和摘要等应用中表现出色。随着它们的广泛应用，一些研究尝试分析Transformer的表现力。神经网络的表现力指的是它能够逼近的函数类。一个神经网络是完全表现力的，如果它可以充当通用函数逼近器。我们尝试分析Transformer的表现力。与现有观点相反，我们的研究结果表明，Transformer在可靠逼近连续函数方面存在困难，依赖于具有可观区间的分段常数逼近。关键问题是：“Transformer是否真正是通用函数逼近器？”为了解决这个问题，我们进行了彻底的调查，通过实验提供理论见解和支持证据。我们的贡献包括了一个理论分析……（摘要未完整）

    arXiv:2402.15478v1 Announce Type: new  Abstract: Transformers have become pivotal in Natural Language Processing, demonstrating remarkable success in applications like Machine Translation and Summarization. Given their widespread adoption, several works have attempted to analyze the expressivity of Transformers. Expressivity of a neural network is the class of functions it can approximate. A neural network is fully expressive if it can act as a universal function approximator. We attempt to analyze the same for Transformers. Contrary to existing claims, our findings reveal that Transformers struggle to reliably approximate continuous functions, relying on piecewise constant approximations with sizable intervals. The central question emerges as: "\textit{Are Transformers truly Universal Function Approximators}?" To address this, we conduct a thorough investigation, providing theoretical insights and supporting evidence through experiments. Our contributions include a theoretical analysi
    
[^7]: 通过使用弱监督学习来消除机器学习模型的偏见

    Debiasing Machine Learning Models by Using Weakly Supervised Learning

    [https://arxiv.org/abs/2402.15477](https://arxiv.org/abs/2402.15477)

    提出了一种针对连续敏感变量的偏见消除策略，基于内生性概念并采用弱监督学习方法，无需对预测模型做任何假设

    

    我们在算法决策的偏见消除问题中探讨了一个情况，即算法的输出和敏感变量均为连续的情形。大部分先前的工作处理的是离散的敏感变量，这意味着偏见是针对由标签定义的人群子集进行测量的，而忽略了算法偏见的重要情况，即敏感变量是连续的。典型的例子是关于年龄或财务状况而做出的不公平决策。在我们的工作中，我们提出了一种针对连续敏感变量的偏见消除策略，该策略基于计量经济学领域的内生性概念。除了解决这个新问题，我们的偏见消除策略还是一种弱监督学习方法，它要求数据的一小部分可以以公平的方式进行测量。这种方法在模型上是无关的，即它不对预测模型进行任何假设。

    arXiv:2402.15477v1 Announce Type: new  Abstract: We tackle the problem of bias mitigation of algorithmic decisions in a setting where both the output of the algorithm and the sensitive variable are continuous. Most of prior work deals with discrete sensitive variables, meaning that the biases are measured for subgroups of persons defined by a label, leaving out important algorithmic bias cases, where the sensitive variable is continuous. Typical examples are unfair decisions made with respect to the age or the financial status. In our work, we then propose a bias mitigation strategy for continuous sensitive variables, based on the notion of endogeneity which comes from the field of econometrics. In addition to solve this new problem, our bias mitigation strategy is a weakly supervised learning method which requires that a small portion of the data can be measured in a fair manner. It is model agnostic, in the sense that it does not make any hypothesis on the prediction model. It also m
    
[^8]: 利用领域知识在RLHF中高效建模奖励：电子商务意见摘要的案例研究

    Leveraging Domain Knowledge for Efficient Reward Modelling in RLHF: A Case-Study in E-Commerce Opinion Summarization

    [https://arxiv.org/abs/2402.15473](https://arxiv.org/abs/2402.15473)

    提出了一种方法，在RLHF中利用领域知识来降低训练奖励模型所需的大量人类偏好注释数量。

    

    从人类反馈中进行强化学习（RLHF）已成为引导语言模型（LMs）朝向人类价值/目标的主导策略。该策略的关键在于使用一个能够反映与人类相关的潜在奖励模型的奖励模型（{$\varphi$}）。虽然这一策略已被证明是有效的，但训练方法需要大量人类偏好注释（通常数量级为数万）来训练{$\varphi$}。如果奖励模型可以被普遍使用，这种大规模偏好注释是可以实现的。然而，人类价值/目标是主观的，并且取决于任务的性质。这对于收集下游应用程序的多样化偏好构成挑战。为了解决这个问题，我们提出了一种新颖的方法，将领域知识融入{$\varphi$}中，从而减少所需注释的大小。我们在电子商务意见摘要中验证了我们的方法，具有显著的

    arXiv:2402.15473v1 Announce Type: new  Abstract: Reinforcement Learning from Human Feedback (RLHF) has become a dominating strategy in steering Language Models (LMs) towards human values/goals. The key to the strategy is employing a reward model ({$\varphi$}) which can reflect a latent reward model with humans. While this strategy has proven to be effective, the training methodology requires a lot of human preference annotation (usually of the order of tens of thousands) to train {$\varphi$}. Such large-scale preference annotations can be achievable if the reward model can be ubiquitously used. However, human values/goals are subjective and depend on the nature of the task. This poses a challenge in collecting diverse preferences for downstream applications. To address this, we propose a novel methodology to infuse domain knowledge into {$\varphi$}, which reduces the size of preference annotation required. We validate our approach in E-Commerce Opinion Summarization, with a significant
    
[^9]: FAIR：自动诱导规则的过滤

    FAIR: Filtering of Automatically Induced Rules

    [https://arxiv.org/abs/2402.15472](https://arxiv.org/abs/2402.15472)

    该论文提出了一种通过次模块目标函数过滤大量自动诱导规则的算法 FAIR，以解决机器学习算法训练中缺乏大量已注释数据的问题。

    

    大量已注释数据的可用性可能是成功训练机器学习算法的关键瓶颈，特别是当应用于多样化的领域时。弱监督提供了一个有希望的替代方案，通过加速使用特定领域规则创建带标签的训练数据。然而，它要求用户编写多样化且高质量的规则集，以将标签分配给未标记数据。自动规则诱导（ARI）方法通过从少量带标签集上的特征自动创建规则并从中过滤出最终一组规则来避开这个问题。在ARI方法中，关键步骤是从大量自动创建的规则中过滤出一组高质量有用的规则子集。在本文中，我们提出一种算法（自动诱导规则的过滤），使用考虑到次模块目标函数的方法从大量自动诱导规则中过滤出规则。

    arXiv:2402.15472v1 Announce Type: new  Abstract: The availability of large annotated data can be a critical bottleneck in training machine learning algorithms successfully, especially when applied to diverse domains. Weak supervision offers a promising alternative by accelerating the creation of labeled training data using domain-specific rules. However, it requires users to write a diverse set of high-quality rules to assign labels to the unlabeled data. Automatic Rule Induction (ARI) approaches circumvent this problem by automatically creating rules from features on a small labeled set and filtering a final set of rules from them. In the ARI approach, the crucial step is to filter out a set of a high-quality useful subset of rules from the large set of automatically created rules. In this paper, we propose an algorithm (Filtering of Automatically Induced Rules) to filter rules from a large number of automatically induced rules using submodular objective functions that account for the
    
[^10]: 重复改善语言模型嵌入

    Repetition Improves Language Model Embeddings

    [https://arxiv.org/abs/2402.15449](https://arxiv.org/abs/2402.15449)

    回声嵌入方法通过重复输入来提取信息，解决了自回归模型无法包含后续令牌信息的限制，实验结果表明其能够最大程度充分利用高质量的语言模型进行嵌入。

    

    最近改进从自回归大型语言模型（LLMs）中提取文本嵌入的方法主要集中在改进数据、骨干预训练语言模型或通过指令改进任务差异化上。在这项工作中，我们解决了自回归模型的一个架构限制：令牌嵌入不能包含来自输入中后续令牌的信息。为了解决这一限制，我们提出了一种简单的方法，“回声嵌入”，其中我们在上下文中将输入重复两次，并从第二次出现中提取嵌入。我们展示了早期令牌的回声嵌入可以编码关于后续令牌的信息，从而使我们能够最大程度地利用高质量的LLMs进行嵌入。在MTEB排行榜上，回声嵌入在零射击中比经典嵌入提高了超过9%，在微调时提高了约0.7%。使用Mistral-7B模型的回声嵌入实现了与当前最先进模型的比较。

    arXiv:2402.15449v1 Announce Type: new  Abstract: Recent approaches to improving the extraction of text embeddings from autoregressive large language models (LLMs) have largely focused on improvements to data, backbone pretrained language models, or improving task-differentiation via instructions. In this work, we address an architectural limitation of autoregressive models: token embeddings cannot contain information from tokens that appear later in the input. To address this limitation, we propose a simple approach, "echo embeddings," in which we repeat the input twice in context and extract embeddings from the second occurrence. We show that echo embeddings of early tokens can encode information about later tokens, allowing us to maximally leverage high-quality LLMs for embeddings. On the MTEB leaderboard, echo embeddings improve over classical embeddings by over 9% zero-shot and by around 0.7% when fine-tuned. Echo embeddings with a Mistral-7B model achieve state-of-the-art compared
    
[^11]: 发挥不平衡模态信息在多模态知识图完成中的力量

    Unleashing the Power of Imbalanced Modality Information for Multi-modal Knowledge Graph Completion

    [https://arxiv.org/abs/2402.15444](https://arxiv.org/abs/2402.15444)

    提出了自适应多模态融合和模态对抗训练（AdaMF-MAT）方法，以解决多模态知识图完成中存在的模态信息不平衡问题，发挥不平衡模态信息的力量。

    

    多模态知识图完成（MMKGC）旨在通过将实体的结构、视觉和文本信息纳入判别模型来预测多模态知识图中缺失的三元组。来自不同模态的信息将共同工作以衡量三元组的可能性。现有的MMKGC方法忽视了实体之间模态信息不平衡的问题，导致模态融合不足以及对原始模态信息的低效利用。为解决上述问题，我们提出了自适应多模态融合和模态对抗训练（AdaMF-MAT），以发挥不平衡模态信息在MMKGC中的力量。AdaMF-MAT通过自适应模态权重实现多模态融合，并通过模态对抗训练生成对抗样本，以增强不平衡模态信息。我们的方法是MMKGC模型和训练的协同设计。

    arXiv:2402.15444v1 Announce Type: new  Abstract: Multi-modal knowledge graph completion (MMKGC) aims to predict the missing triples in the multi-modal knowledge graphs by incorporating structural, visual, and textual information of entities into the discriminant models. The information from different modalities will work together to measure the triple plausibility. Existing MMKGC methods overlook the imbalance problem of modality information among entities, resulting in inadequate modal fusion and inefficient utilization of the raw modality information. To address the mentioned problems, we propose Adaptive Multi-modal Fusion and Modality Adversarial Training (AdaMF-MAT) to unleash the power of imbalanced modality information for MMKGC. AdaMF-MAT achieves multi-modal fusion with adaptive modality weights and further generates adversarial samples by modality-adversarial training to enhance the imbalanced modality information. Our approach is a co-design of the MMKGC model and training s
    
[^12]: 主动少样本微调

    Active Few-Shot Fine-Tuning

    [https://arxiv.org/abs/2402.15441](https://arxiv.org/abs/2402.15441)

    该论文提出了ITL方法来实现主动少样本微调，通过最大化对下游任务的信息获取，从而在大型神经网络的微调中取得了显著的改进。

    

    我们研究了大型神经网络对下游任务进行主动少样本微调。我们表明少样本微调是传统主动学习和转导主动学习的泛化实例，我们提出了信息基于转导学习（ITL）的方法，该方法自适应地进行采样以最大化获得对指定下游任务的信息。在一般正则性假设下，我们证明ITL均匀收敛到可从可访问数据获取的最小可能的不确定性。据我们所知，我们是首批推导出这种泛化界限的人，这对于主动学习可能是具有独立意义的。我们将ITL应用于大型神经网络的少样本微调中，结果显示ITL明显改进了现有技术。

    arXiv:2402.15441v1 Announce Type: cross  Abstract: We study the active few-shot fine-tuning of large neural networks to downstream tasks. We show that few-shot fine-tuning is an instance of a generalization of classical active learning, transductive active learning, and we propose ITL, short for information-based transductive learning, an approach which samples adaptively to maximize the information gained about specified downstream tasks. Under general regularity assumptions, we prove that ITL converges uniformly to the smallest possible uncertainty obtainable from the accessible data. To the best of our knowledge, we are the first to derive generalization bounds of this kind, and they may be of independent interest for active learning. We apply ITL to the few-shot fine-tuning of large neural networks and show that ITL substantially improves upon the state-of-the-art.
    
[^13]: 在次指数混合模型中实现极小化聚类误差：通用下界和最佳速率

    Universal Lower Bounds and Optimal Rates: Achieving Minimax Clustering Error in Sub-Exponential Mixture Models

    [https://arxiv.org/abs/2402.15432](https://arxiv.org/abs/2402.15432)

    本文在混合模型中建立了一个通用下界，通过Chernoff散度来表达，将其拓展到具有次指数尾部的混合模型，并证明了迭代算法在这些混合模型中实现了最佳误差率

    

    聚类是无监督机器学习中的一个关键挑战，通常通过混合模型的视角来研究。在高斯和次高斯混合模型中恢复聚类标签的最佳误差率涉及到特定的信噪比。简单的迭代算法，如Lloyd算法，可以达到这个最佳误差率。在本文中，我们首先为任何混合模型中的误差率建立了一个通用下界，通过Chernoff散度来表达，这是一个比信噪比更通用的模型信息度量。然后我们证明了迭代算法在混合模型中实现了这个下界，特别强调了具有拉普拉斯分布误差的位置-尺度混合。此外，针对更适合由泊松或负二项混合模型建模的数据集，我们研究了其分布属于指数族的混合模型。

    arXiv:2402.15432v1 Announce Type: cross  Abstract: Clustering is a pivotal challenge in unsupervised machine learning and is often investigated through the lens of mixture models. The optimal error rate for recovering cluster labels in Gaussian and sub-Gaussian mixture models involves ad hoc signal-to-noise ratios. Simple iterative algorithms, such as Lloyd's algorithm, attain this optimal error rate. In this paper, we first establish a universal lower bound for the error rate in clustering any mixture model, expressed through a Chernoff divergence, a more versatile measure of model information than signal-to-noise ratios. We then demonstrate that iterative algorithms attain this lower bound in mixture models with sub-exponential tails, notably emphasizing location-scale mixtures featuring Laplace-distributed errors. Additionally, for datasets better modelled by Poisson or Negative Binomial mixtures, we study mixture models whose distributions belong to an exponential family. In such m
    
[^14]: 较大尺度下用于稳健且可解释视觉任务的分层不变性

    Hierarchical Invariance for Robust and Interpretable Vision Tasks at Larger Scales

    [https://arxiv.org/abs/2402.15430](https://arxiv.org/abs/2402.15430)

    通过分层不变性构建稳健且可解释的视觉系统，克服了不变性表示在较大规模的视觉任务中可区分性有限的问题。

    

    开发稳健且可解释的视觉系统是迈向可靠人工智能的关键一步。在这方面，一个有前途的范式是考虑在基本图像表示中嵌入任务所需的不变结构，例如几何不变性。然而，这种不变的表示通常表现出有限的可区分性，限制了它们在更大规模的可靠视觉任务中的应用。针对这个开放性问题，我们从理论、实践和应用的角度对分层不变性进行了系统研究。在理论层面上，我们展示了如何以类似于卷积神经网络（CNN）的分层体系结构但以完全可解释的方式构建超完备不变性。我们提供了通用蓝图、具体定义、不变特性和数值实现。在实践层面上，我们讨论了如何定制不变性以适应更大规模的视觉任务。

    arXiv:2402.15430v1 Announce Type: cross  Abstract: Developing robust and interpretable vision systems is a crucial step towards trustworthy artificial intelligence. In this regard, a promising paradigm considers embedding task-required invariant structures, e.g., geometric invariance, in the fundamental image representation. However, such invariant representations typically exhibit limited discriminability, limiting their applications in larger-scale trustworthy vision tasks. For this open problem, we conduct a systematic investigation of hierarchical invariance, exploring this topic from theoretical, practical, and application perspectives. At the theoretical level, we show how to construct over-complete invariants with a Convolutional Neural Networks (CNN)-like hierarchical architecture yet in a fully interpretable manner. The general blueprint, specific definitions, invariant properties, and numerical implementations are provided. At the practical level, we discuss how to customize 
    
[^15]: ProTIP：针对文本到图像扩散模型抗随机扰动的概率鲁棒性验证

    ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion Models against Stochastic Perturbation

    [https://arxiv.org/abs/2402.15429](https://arxiv.org/abs/2402.15429)

    本研究引入了概率概念的文本到图像扩散模型鲁棒性，并建立了一个名为ProTIP的高效框架用于评估其统计保证，解决了生成过程的高计算成本和对抗性样本判断困难的问题

    

    文本到图像（T2I）扩散模型（DMs）展现了在简单文本描述基础上生成高质量图像的印象能力。然而，与许多深度学习（DL）模型一样，DMs存在缺乏鲁棒性的问题。在评估T2I DMs的鲁棒性时，存在以二元或最坏情况问题解方面的尝试，但无法回答模型在存在对抗性样本（AE）时的总体鲁棒性如何。本研究首先引入了T2I DMs鲁棒性的概率概念；然后建立了一个名为ProTIP的高效框架，用于具有统计保证的评估。主要挑战源自：i）生成过程的高计算成本；和ii）确定扰动输入是否为AE涉及比较两个输出分布，这与其他DL任务（如分类）不同，其中AE是在标签错误预测时被识别的。为解决这些挑战，

    arXiv:2402.15429v1 Announce Type: cross  Abstract: Text-to-Image (T2I) Diffusion Models (DMs) have shown impressive abilities in generating high-quality images based on simple text descriptions. However, as is common with many Deep Learning (DL) models, DMs are subject to a lack of robustness. While there are attempts to evaluate the robustness of T2I DMs as a binary or worst-case problem, they cannot answer how robust in general the model is whenever an adversarial example (AE) can be found. In this study, we first introduce a probabilistic notion of T2I DMs' robustness; and then establish an efficient framework, ProTIP, to evaluate it with statistical guarantees. The main challenges stem from: i) the high computational cost of the generation process; and ii) determining if a perturbed input is an AE involves comparing two output distributions, which is fundamentally harder compared to other DL tasks like classification where an AE is identified upon misprediction of labels. To tackle
    
[^16]: 用大型语言模型生成忠实且高质量的病人总结的数据中心方法

    A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models

    [https://arxiv.org/abs/2402.15422](https://arxiv.org/abs/2402.15422)

    本研究探讨了使用大型语言模型基于医生笔记生成患者总结的潜力，通过严格的标记协议和医学专家标记实验发现，在无幻觉数据上进行微调能有效减少幻觉的生成，并保留相关信息。

    

    患者经常面临难以理解其住院情况的困难，而医护人员资源有限以提供解释。在这项工作中，我们研究了大型语言模型基于医生笔记生成患者总结的潜力，并研究了训练数据对生成总结的忠实性和质量的影响。为此，我们开发了严格的标记协议用于幻觉，让两位医学专家标记了100个真实总结和100个生成的总结。我们展示了在无幻觉数据进行微调可以有效地减少Llama 2每个总结的幻觉从2.60降低到1.55，同时保留相关信息。虽然效果仍然存在，但当使用五个例子提示GPT-4时，该效果要小得多（0.70降至0.40）。我们还对无幻觉和改进的训练数据进行了定性评估。即使在幻觉自由数据下，GPT-4也展现出非常好的结果。

    arXiv:2402.15422v1 Announce Type: cross  Abstract: Patients often face difficulties in understanding their hospitalizations, while healthcare workers have limited resources to provide explanations. In this work, we investigate the potential of large language models to generate patient summaries based on doctors' notes and study the effect of training data on the faithfulness and quality of the generated summaries. To this end, we develop a rigorous labeling protocol for hallucinations, and have two medical experts annotate 100 real-world summaries and 100 generated summaries. We show that fine-tuning on hallucination-free data effectively reduces hallucinations from 2.60 to 1.55 per summary for Llama 2, while preserving relevant information. Although the effect is still present, it is much smaller for GPT-4 when prompted with five examples (0.70 to 0.40). We also conduct a qualitative evaluation using hallucination-free and improved training data. GPT-4 shows very good results even in 
    
[^17]: PREDILECT：在强化学习中利用零样本语言推理划分偏好

    PREDILECT: Preferences Delineated with Zero-Shot Language-based Reasoning in Reinforcement Learning

    [https://arxiv.org/abs/2402.15420](https://arxiv.org/abs/2402.15420)

    本文提出了一种在强化学习中利用零样本语言推理来划分偏好的方法，通过扩展查询信息并重新定义奖励学习目标，提高了样本效率。

    

    基于偏好的强化学习已经成为机器人学习中的一个新领域，在这个领域中，人类通过对不同状态-动作序列表达偏好来塑造机器人行为。然而，为机器人制定现实政策需要人类对大量查询的响应。本工作通过扩展每个查询收集的信息，包含偏好和可选文本提示，来解决样本效率挑战。为了实现这一目标，我们利用大型语言模型(LLM)的零样本能力来从人类提供的文本中进行推理。为了适应额外的查询信息，我们重新定义了奖励学习目标，包含灵活的重点 —— 包含相对高信息量且与零射样本传递的特征相关的状态-动作对。在仿真场景和实际场景中，我们展示了我们方法的有效性。

    arXiv:2402.15420v1 Announce Type: cross  Abstract: Preference-based reinforcement learning (RL) has emerged as a new field in robot learning, where humans play a pivotal role in shaping robot behavior by expressing preferences on different sequences of state-action pairs. However, formulating realistic policies for robots demands responses from humans to an extensive array of queries. In this work, we approach the sample-efficiency challenge by expanding the information collected per query to contain both preferences and optional text prompting. To accomplish this, we leverage the zero-shot capabilities of a large language model (LLM) to reason from the text provided by humans. To accommodate the additional query information, we reformulate the reward learning objectives to contain flexible highlights -- state-action pairs that contain relatively high information and are related to the features processed in a zero-shot fashion from a pretrained LLM. In both a simulated scenario and a u
    
[^18]: LoRA对转换器中聚类的影响

    The Impact of LoRA on the Emergence of Clusters in Transformers

    [https://arxiv.org/abs/2402.15415](https://arxiv.org/abs/2402.15415)

    本文利用转换器数学框架探讨了LoRA算法对Token聚类结构动态的影响，发现在不同参数下，修改后的注意力矩阵动态的聚类表现出较长时间的显著差异，但仍在短时间内保持密切相似。

    

    在本文中，我们利用\citet{sander2022sinkformers,geshkovski2023emergence,geshkovski2023mathematical}提出的转换器数学框架，探讨注意力参数和初始标记值的变化如何影响标记聚类的结构动态。我们的分析表明，虽然修改后的注意力矩阵动态中的聚类可能在较长时间内与原始聚类差异显著，但在较短时间间隔内，它们在参数差异的影响下仍保持密切相似。这项工作通过LoRA算法\cite{hu2021lora,peft}的实际应用，为微调领域做出了贡献，增进了我们对LoRA增强的Transformer模型行为的理解。

    arXiv:2402.15415v1 Announce Type: new  Abstract: In this paper, we employ the mathematical framework on Transformers developed by \citet{sander2022sinkformers,geshkovski2023emergence,geshkovski2023mathematical} to explore how variations in attention parameters and initial token values impact the structural dynamics of token clusters. Our analysis demonstrates that while the clusters within a modified attention matrix dynamics can exhibit significant divergence from the original over extended periods, they maintain close similarities over shorter intervals, depending on the parameter differences. This work contributes to the fine-tuning field through practical applications to the LoRA algorithm \cite{hu2021lora,peft}, enhancing our understanding of the behavior of LoRA-enhanced Transformer models.
    
[^19]: 组合高效参数模块是否改善小样本迁移精度？

    Does Combining Parameter-efficient Modules Improve Few-shot Transfer Accuracy?

    [https://arxiv.org/abs/2402.15414](https://arxiv.org/abs/2402.15414)

    本文探索了LoRA模块的可组合性，并发现在少样本情景下，无论是均匀组成还是学习组成，对视觉和语言模型进行组合都可以提高对未知下游任务的泛化能力。

    

    参数高效微调被视为在下游任务上高效微调大型语言和视觉模型的标准。特别是，低秩参数适应性的高效性促进了数百个定制LoRA模块的创建和共享，每个模块都是在各种不同下游任务的数据上训练而成。在本文中，我们探索了LoRA模块的可组合性，研究了合并这些预训练模块是否增强了对未知下游任务的泛化能力。我们的研究涉及评估两种方法：（a）均匀组成，包括将上游LoRA模块均匀加权平均，以及（b）学习组成，我们在每个上游模块上学习权重，并执行加权平均。我们在视觉和语言模型上的实验结果表明，在少样本情景中，即只有有限数量的样本可用于下游任务时，无论是均匀组成还是学习组成

    arXiv:2402.15414v1 Announce Type: new  Abstract: Parameter-efficient fine-tuning stands as the standard for efficiently fine-tuning large language and vision models on downstream tasks. Specifically, the efficiency of low-rank adaptation has facilitated the creation and sharing of hundreds of custom LoRA modules, each trained on distinct data from various downstream tasks. In this paper, we explore the composability of LoRA modules, examining if combining these pre-trained modules enhances generalization to unseen downstream tasks. Our investigation involves evaluating two approaches: (a) uniform composition, involving averaging upstream LoRA modules with equal weights, and (b) learned composition, where we learn the weights for each upstream module and perform weighted averaging. Our experimental results on both vision and language models reveal that in few-shot settings, where only a limited number of samples are available for the downstream task, both uniform and learned composition
    
[^20]: G-RepsNet：一种快速而通用的构建用于任意矩阵群的等变网络

    G-RepsNet: A Fast and General Construction of Equivariant Networks for Arbitrary Matrix Groups

    [https://arxiv.org/abs/2402.15413](https://arxiv.org/abs/2402.15413)

    G-RepsNet 是一种轻量级的等变网络，用于任意矩阵群，通过在神经网络的隐藏层中使用张量表示和简单廉价的张量操作，实现了具有表现力的通用等变网络。

    

    组等变性是一种强大的归纳偏差，在各种深度学习任务中非常有用。然而，为一般群和域构建高效的等变网络是困难的。最近Finzi等人（2021）的工作直接解决了任意矩阵群的等变性约束，获得了等变MLP（EMLP）。但这种方法在扩展方面效果不佳，而在深度学习中，扩展是至关重要的。在这里，我们引入了Group Representation Networks（G-RepsNets），一种轻量级的等变网络，用于任意矩阵群，其中特征使用张量多项式表示。我们设计的关键直觉是，在神经网络的隐藏层中使用张量表示以及简单廉价的张量操作可以导致具有表现力的通用等变网络。我们发现G-RepsNet在具有O(5)、O(1, 3)和O(3)对称性的几项任务中与EMLP竞争力强。

    arXiv:2402.15413v1 Announce Type: new  Abstract: Group equivariance is a strong inductive bias useful in a wide range of deep learning tasks. However, constructing efficient equivariant networks for general groups and domains is difficult. Recent work by Finzi et al. (2021) directly solves the equivariance constraint for arbitrary matrix groups to obtain equivariant MLPs (EMLPs). But this method does not scale well and scaling is crucial in deep learning. Here, we introduce Group Representation Networks (G-RepsNets), a lightweight equivariant network for arbitrary matrix groups with features represented using tensor polynomials. The key intuition for our design is that using tensor representations in the hidden layers of a neural network along with simple inexpensive tensor operations can lead to expressive universal equivariant networks. We find G-RepsNet to be competitive to EMLP on several tasks with group symmetries such as O(5), O(1, 3), and O(3) with scalars, vectors, and second-
    
[^21]: 乐观信息导向采样

    Optimisic Information Directed Sampling

    [https://arxiv.org/abs/2402.15411](https://arxiv.org/abs/2402.15411)

    提出了一种名为乐观信息导向采样的算法模板, 结合了贝叶斯理论和最坏情形理论，能够实现类似于贝叶斯方法的实例相关遗憾保证，但无需贝叶斯假设。

    

    我们研究在上下文强盗问题中的在线学习问题，其中损失函数被假定属于已知的参数函数类。我们提出了一个新的分析框架，它在贝叶斯理论和基于决策估计系数的最坏情形理论之间架起了桥梁。汲取这两方面的工作，我们提出了一种名为乐观信息导向采样的算法模板，并展示它能够实现类似于经典贝叶斯IDS方法可实现的实例相关遗憾保证，但却无需任何贝叶斯假设。我们分析的关键技术创新是引入了一个乐观的替代模型用于遗憾，并将其用于定义一个基于频率的基于Russo和Van Roy (2018)的信息比，以及一个...

    arXiv:2402.15411v1 Announce Type: new  Abstract: We study the problem of online learning in contextual bandit problems where the loss function is assumed to belong to a known parametric function class. We propose a new analytic framework for this setting that bridges the Bayesian theory of information-directed sampling due to Russo and Van Roy (2018) and the worst-case theory of Foster, Kakade, Qian, and Rakhlin (2021) based on the decision-estimation coefficient. Drawing from both lines of work, we propose a algorithmic template called Optimistic Information-Directed Sampling and show that it can achieve instance-dependent regret guarantees similar to the ones achievable by the classic Bayesian IDS method, but with the major advantage of not requiring any Bayesian assumptions. The key technical innovation of our analysis is introducing an optimistic surrogate model for the regret and using it to define a frequentist version of the Information Ratio of Russo and Van Roy (2018), and a l
    
[^22]: 拥有潜在变量的Lasso：高效估计、协变量重新缩放和计算统计差距

    Lasso with Latents: Efficient Estimation, Covariate Rescaling, and Computational-Statistical Gaps

    [https://arxiv.org/abs/2402.15409](https://arxiv.org/abs/2402.15409)

    在处理拥有潜在变量的稀疏线性回归问题时，通过对协变量进行异质缩放，Lasso方法可以获得强有力的估计保证。

    

    众所周知，当感兴趣的协变量之间存在强相关性时，Lasso的统计性能会显著下降。特别是，与计算效率低下的备选方案如最佳子集选择相比，Lasso的预测误差会变得严重严重。由于在稀疏线性回归问题中存在一个被普遍猜测的计算统计权衡，通常不可能一般性地减小这一差距。在这项工作中，我们提出了一个自然的稀疏线性回归设置，其中协变量之间的强相关性来自未观察到的潜在变量。在这种设定下，我们分析了由强相关性引起的问题，并设计了一个令人惊讶地简单的修复方法。虽然标准化协变量的Lasso失败了，但有一种异质缩放的协变量，Lasso将突然获得对估计的强有力保证。此外，我们设计了一个简单而高效的程序

    arXiv:2402.15409v1 Announce Type: cross  Abstract: It is well-known that the statistical performance of Lasso can suffer significantly when the covariates of interest have strong correlations. In particular, the prediction error of Lasso becomes much worse than computationally inefficient alternatives like Best Subset Selection. Due to a large conjectured computational-statistical tradeoff in the problem of sparse linear regression, it may be impossible to close this gap in general.   In this work, we propose a natural sparse linear regression setting where strong correlations between covariates arise from unobserved latent variables. In this setting, we analyze the problem caused by strong correlations and design a surprisingly simple fix. While Lasso with standard normalization of covariates fails, there exists a heterogeneous scaling of the covariates with which Lasso will suddenly obtain strong provable guarantees for estimation. Moreover, we design a simple, efficient procedure fo
    
[^23]: Conformalized-DeepONet: 深度算子网络中的无分布不确定性量化框架

    Conformalized-DeepONet: A Distribution-Free Framework for Uncertainty Quantification in Deep Operator Networks

    [https://arxiv.org/abs/2402.15406](https://arxiv.org/abs/2402.15406)

    采用conformal prediction框架为Deep Operator Network中的回归问题提供了具有覆盖保证的置信区间，同时引入Quantile-DeepONet进一步提高了不确定性量化的效果。

    

    本文中，我们采用了一种无分布不确定性量化 (UQ) 框架——conformal prediction，用于在Deep Operator Network (DeepONet) 回归中获得具有覆盖保证的置信区间。我们通过使用split conformal prediction 加强了作者先前提出的不确定性量化框架 (B-DeepONet 和 Prob-DeepONet)。通过将conformal prediction 与我们的Prob- 和B-DeepONets 结合，我们有效地通过为DeepONet 预测生成严格置信区间来量化不确定性。此外，我们设计了一种新颖的Quantile-DeepONet，它允许更自然地使用split conformal prediction。我们将这种无分布有效不确定性量化框架称为split conformal Quantile-DeepONet 回归。最后，我们使用各种普通的、偏微分方程数值示例展示了所提出方法的有效性。

    arXiv:2402.15406v1 Announce Type: new  Abstract: In this paper, we adopt conformal prediction, a distribution-free uncertainty quantification (UQ) framework, to obtain confidence prediction intervals with coverage guarantees for Deep Operator Network (DeepONet) regression. Initially, we enhance the uncertainty quantification frameworks (B-DeepONet and Prob-DeepONet) previously proposed by the authors by using split conformal prediction. By combining conformal prediction with our Prob- and B-DeepONets, we effectively quantify uncertainty by generating rigorous confidence intervals for DeepONet prediction. Additionally, we design a novel Quantile-DeepONet that allows for a more natural use of split conformal prediction. We refer to this distribution-free effective uncertainty quantification framework as split conformal Quantile-DeepONet regression. Finally, we demonstrate the effectiveness of the proposed methods using various ordinary, partial differential equation numerical examples, a
    
[^24]: 聚而预训练，分而不胜！通过同时在75个数据集上预训练进行时间序列的表示学习

    United We Pretrain, Divided We Fail! Representation Learning for Time Series by Pretraining on 75 Datasets at Once

    [https://arxiv.org/abs/2402.15404](https://arxiv.org/abs/2402.15404)

    引入了一种新的自监督对比预训练方法，通过在多个未标记和多样化的时间序列数据集上学习一个编码，证明了这种方法在低数据情境下优于监督训练和其他自监督预训练方法，颠覆了常见认知，时间序列可以从多个数据集中学习

    

    在自然语言处理和视觉领域，预训练被用于学习有效的表示。然而，由于来源和目标之间可能存在的不匹配，预训练的成功并不容易应用于时间序列。事实上，普遍认为多数据集预训练在时间序列中并不奏效！相反地，我们引入了一种新的自监督对比预训练方法，可以从许多未标记和多样化的时间序列数据集中学习一个编码，然后可以在多个目标域（例如分类）中重复使用单一学到的表示。具体来说，我们提出了XD-MixUp插值方法和Soft插值上下文对比（SICC）损失。经验证，这优于在低数据情况下进行微调时的监督训练和其他自监督预训练方法。这一事实证明了常见看法的错误：我们实际上可以从多个时间序列数据集中进行学习。

    arXiv:2402.15404v1 Announce Type: new  Abstract: In natural language processing and vision, pretraining is utilized to learn effective representations. Unfortunately, the success of pretraining does not easily carry over to time series due to potential mismatch between sources and target. Actually, common belief is that multi-dataset pretraining does not work for time series! Au contraire, we introduce a new self-supervised contrastive pretraining approach to learn one encoding from many unlabeled and diverse time series datasets, so that the single learned representation can then be reused in several target domains for, say, classification. Specifically, we propose the XD-MixUp interpolation method and the Soft Interpolation Contextual Contrasting (SICC) loss. Empirically, this outperforms both supervised training and other self-supervised pretraining methods when finetuning on low-data regimes. This disproves the common belief: We can actually learn from multiple time series datasets
    
[^25]: 抓取、观察和放置：具有策略结构先验的高效未知物体重新排列

    Grasp, See and Place: Efficient Unknown Object Rearrangement with Policy Structure Prior

    [https://arxiv.org/abs/2402.15402](https://arxiv.org/abs/2402.15402)

    该论文提出了一种具有策略结构先验的高效未知物体重新排列系统，通过内外环的学习，实现了抓取、观察和放置在感知噪声中的优化。

    

    我们关注未知物体重新排列任务，即机器人应重新配置物体到由RGB-D图像指定的期望目标配置中。最近的研究通过整合基于学习的感知模块来探索未知物体重新排列系统。然而，它们对感知误差敏感，并且较少关注任务级性能。本文旨在开发一个有效的系统，用于在感知噪声中重新排列未知物体。我们在理论上揭示了噪声感知如何以分离的方式影响抓取和放置，并展示这样的分离结构不容易改善任务的最优性。我们提出了具有分离结构作为先验的GSP，一个双环系统。对于内环，我们学习主动观察策略以提高放置的感知。对于外环，我们学习一个抓取策略，意识到物体匹配和抓取能力。

    arXiv:2402.15402v1 Announce Type: cross  Abstract: We focus on the task of unknown object rearrangement, where a robot is supposed to re-configure the objects into a desired goal configuration specified by an RGB-D image. Recent works explore unknown object rearrangement systems by incorporating learning-based perception modules. However, they are sensitive to perception error, and pay less attention to task-level performance. In this paper, we aim to develop an effective system for unknown object rearrangement amidst perception noise. We theoretically reveal the noisy perception impacts grasp and place in a decoupled way, and show such a decoupled structure is non-trivial to improve task optimality. We propose GSP, a dual-loop system with the decoupled structure as prior. For the inner loop, we learn an active seeing policy for self-confident object matching to improve the perception of place. For the outer loop, we learn a grasp policy aware of object matching and grasp capability gu
    
[^26]: 分布鲁棒的离策略强化学习：具有线性函数逼近的高效性证明

    Distributionally Robust Off-Dynamics Reinforcement Learning: Provable Efficiency with Linear Function Approximation

    [https://arxiv.org/abs/2402.15399](https://arxiv.org/abs/2402.15399)

    研究了离策略强化学习中的在线分布鲁棒马尔可夫决策过程，设计了可以去除非线性和避免误差传播的$d$-矩形不确定性集合，并提出了第一个针对离策略具有线性函数逼近的在线DRMDP算法，并且具有高效性。

    

    我们研究离策略强化学习（RL），其中策略在源域上进行训练，然后部署到不同的目标域。我们旨在通过在线分布鲁棒马尔可夫决策过程（DRMDPs）来解决这个问题，其中学习算法在与源域交互时，寻求在源域转移核的不确定性集合内的最差动态下的最佳性能。我们首次研究了离策略RL中具有函数逼近的在线DRMDPs。我们发现DRMDPs的对偶形式可能会引入非线性，即使名义转移核是线性的，也会导致误差传播。通过设计一个$d$-矩形不确定性集合，使用总变差距离，我们去除了这种额外的非线性，避免了误差传播。然后我们引入了DR-LSVI-UCB，这是第一个针对离策略具有线性函数逼近的在线DRMDP算法，并且具有可证明的高效性。

    arXiv:2402.15399v1 Announce Type: new  Abstract: We study off-dynamics Reinforcement Learning (RL), where the policy is trained on a source domain and deployed to a distinct target domain. We aim to solve this problem via online distributionally robust Markov decision processes (DRMDPs), where the learning algorithm actively interacts with the source domain while seeking the optimal performance under the worst possible dynamics that is within an uncertainty set of the source domain's transition kernel. We provide the first study on online DRMDPs with function approximation for off-dynamics RL. We find that DRMDPs' dual formulation can induce nonlinearity, even when the nominal transition kernel is linear, leading to error propagation. By designing a $d$-rectangular uncertainty set using the total variation distance, we remove this additional nonlinearity and bypass the error propagation. We then introduce DR-LSVI-UCB, the first provably efficient online DRMDP algorithm for off-dynamics
    
[^27]: TransFlower: 一种基于Transformer的模型，使用Flow-to-Flow注意力进行通勤流预测

    TransFlower: An Explainable Transformer-Based Model with Flow-to-Flow Attention for Commuting Flow Prediction

    [https://arxiv.org/abs/2402.15398](https://arxiv.org/abs/2402.15398)

    TransFlower模型是一种基于Transformer的可解释模型，采用Flow-to-Flow注意力来预测城市通勤模式，以解决深度学习模型准确性和可解释性之间的权衡问题。

    

    理解城市规划和通勤流之间的联系对指导城市发展和政策制定至关重要。这项研究跨越计算机科学和城市研究，解决了整合这些领域及其不同关注点的挑战。传统的城市研究方法，如引力和辐射模型，通常在复杂情况下表现不佳，因为它们对多个变量的处理有限，并且依赖于过于简化且不现实的假设，如空间各向同性。虽然深度学习模型提供了更高的准确性，但它们的黑盒特性在性能和可解释性之间存在权衡，这两者对于分析通勤流等复杂社会现象至关重要。为了解决这个问题，我们引入了TransFlower，一种可解释的、基于Transformer的模型，采用Flow-to-Flow注意力来预测城市通勤模式。它具有一个带有各向异性感知的地理空间编码器

    arXiv:2402.15398v1 Announce Type: cross  Abstract: Understanding the link between urban planning and commuting flows is crucial for guiding urban development and policymaking. This research, bridging computer science and urban studies, addresses the challenge of integrating these fields with their distinct focuses. Traditional urban studies methods, like the gravity and radiation models, often underperform in complex scenarios due to their limited handling of multiple variables and reliance on overly simplistic and unrealistic assumptions, such as spatial isotropy. While deep learning models offer improved accuracy, their black-box nature poses a trade-off between performance and explainability -- both vital for analyzing complex societal phenomena like commuting flows. To address this, we introduce TransFlower, an explainable, transformer-based model employing flow-to-flow attention to predict urban commuting patterns. It features a geospatial encoder with an anisotropy-aware relative
    
[^28]: NeuralThink: 在一般任务中进行外推的算法综合

    NeuralThink: Algorithm Synthesis that Extrapolates in General Tasks

    [https://arxiv.org/abs/2402.15393](https://arxiv.org/abs/2402.15393)

    NeuralThink 是一种新的递归架构，可以一贯地对对称和不对称任务进行外推，相较于之前的最先进的深度思维架构在稳定地从较小的训练规模对大观测进行外推方面表现出更好的性能。

    

    虽然机器学习方法擅长模式识别，但在可扩展的算法方式上处理复杂的推理任务时仍然面临困难。最近的深度思维方法展现了学习可以外推的算法的潜力：在较小的环境中学习并在较大的环境中执行学到的算法。然而，这些工作局限于对称任务，即输入和输出的维度相同。为了填补这一空白，我们提出了 NeuralThink，一种新的递归架构，可以一贯地对对称和不对称任务进行外推，其中输入和输出的维度不同。我们提供了一个新颖的不对称任务外推基准。我们展示了 NeuralThink 在稳定地从较小的训练规模对大观测进行外推方面一直优于之前的最先进的深度思维架构。

    arXiv:2402.15393v1 Announce Type: cross  Abstract: While machine learning methods excel at pattern recognition, they struggle with complex reasoning tasks in a scalable, algorithmic manner. Recent Deep Thinking methods show promise in learning algorithms that extrapolate: learning in smaller environments and executing the learned algorithm in larger environments. However, these works are limited to symmetrical tasks, where the input and output dimensionalities are the same. To address this gap, we propose NeuralThink, a new recurrent architecture that can consistently extrapolate to both symmetrical and asymmetrical tasks, where the dimensionality of the input and output are different. We contribute with a novel benchmark of asymmetrical tasks for extrapolation. We show that NeuralThink consistently outperforms the prior state-of-the-art Deep Thinking architectures, in regards to stable extrapolation to large observations from smaller training sizes.
    
[^29]: 离线逆强化学习：新的解决方案概念和可证明高效算法

    Offline Inverse RL: New Solution Concepts and Provably Efficient Algorithms

    [https://arxiv.org/abs/2402.15392](https://arxiv.org/abs/2402.15392)

    该论文提出了一种新的可行奖励集概念，以应对离线设定的机会和限制，并分析了其估计的复杂性。

    

    逆强化学习（IRL）旨在从行为演示中恢复专家代理的奖励函数。目前已经逐渐以估计可行奖励集合作为IRL的新框架，将选择单一奖励推迟。然而，迄今为止，现有的制定和算法解决方案主要针对在线设置提出，并达到分析，这在大多数实际应用中明显不现实，在那里离线数据集更为普遍。本文引入了一个捕捉离线环境机遇和限制的可行奖励集概念，并分析了其估计的复杂性。

    arXiv:2402.15392v1 Announce Type: new  Abstract: Inverse reinforcement learning (IRL) aims to recover the reward function of an expert agent from demonstrations of behavior. It is well known that the IRL problem is fundamentally ill-posed, i.e., many reward functions can explain the demonstrations. For this reason, IRL has been recently reframed in terms of estimating the feasible reward set, thus, postponing the selection of a single reward. However, so far, the available formulations and algorithmic solutions have been proposed and analyzed mainly for the online setting, where the learner can interact with the environment and query the expert at will. This is clearly unrealistic in most practical applications, where the availability of an offline dataset is a much more common scenario. In this paper, we introduce a novel notion of feasible reward set capturing the opportunities and limitations of the offline setting and we analyze the complexity of its estimation. This requires the i
    
[^30]: Genie：生成交互环境

    Genie: Generative Interactive Environments

    [https://arxiv.org/abs/2402.15391](https://arxiv.org/abs/2402.15391)

    Genie是第一个经过无监督训练的生成交互环境，可生成各种动作可控的虚拟世界，并提供了学习潜在行动空间以训练代理程序模仿未见视频行为的可能性。

    

    我们介绍了Genie，这是第一个通过无监督方式从未标记的互联网视频中训练而成的生成式交互环境。该模型可以被提示生成通过文本、合成图像、照片甚至素描描述的无限种类的动作可控虚拟世界。拥有110亿个参数的Genie可以被视为基础世界模型。其由时空视频标记器、自回归动力学模型以及简单且可扩展的潜在行动模型组成。尽管训练过程中没有使用任何地面真值行动标签或其他通常在世界模型文献中找到的领域特定要求，但Genie使用户可以基于逐帧基础在生成的环境中行动。进一步，所得到的学习潜在行动空间有助于训练代理程序模仿来自未见视频的行为，为未来培训通用代理铺平了道路。

    arXiv:2402.15391v1 Announce Type: cross  Abstract: We introduce Genie, the first generative interactive environment trained in an unsupervised manner from unlabelled Internet videos. The model can be prompted to generate an endless variety of action-controllable virtual worlds described through text, synthetic images, photographs, and even sketches. At 11B parameters, Genie can be considered a foundation world model. It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model. Genie enables users to act in the generated environments on a frame-by-frame basis despite training without any ground-truth action labels or other domain-specific requirements typically found in the world model literature. Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future.
    
[^31]: 在语言模型中自修复的探索

    Explorations of Self-Repair in Language Models

    [https://arxiv.org/abs/2402.15390](https://arxiv.org/abs/2402.15390)

    自修复现象存在于各种模型家族和尺寸上，但在完整的训练分布上是不完美和嘈杂的，有两种机制可促成自修复，包括最终LayerNorm缩放因子的变化和实现反擦除的稀疏神经元集。

    

    先前研究狭窄分布的可解释性发现了自修复现象，即如果剥离大型语言模型中的组件，后续组件会改变其行为以进行补偿。我们的工作基于这些过去的文献，展示了当在完整的训练分布上剥离单个注意力头时，自修复存在于各种模型家族和尺寸上。我们进一步表明，在完整的训练分布上，自修复是不完美的，因为头部的原始直接效果并未完全恢复，并且是嘈杂的，因为自修复程度在不同提示之间显著变化（有时超过原始效果）。我们强调了促成自修复的两种不同机制，包括最终LayerNorm缩放因子的变化（可修复直接效果的30%）以及实现反擦除的稀疏神经元集。

    arXiv:2402.15390v1 Announce Type: cross  Abstract: Prior interpretability research studying narrow distributions has preliminarily identified self-repair, a phenomena where if components in large language models are ablated, later components will change their behavior to compensate. Our work builds off this past literature, demonstrating that self-repair exists on a variety of models families and sizes when ablating individual attention heads on the full training distribution. We further show that on the full training distribution self-repair is imperfect, as the original direct effect of the head is not fully restored, and noisy, since the degree of self-repair varies significantly across different prompts (sometimes overcorrecting beyond the original effect). We highlight two different mechanisms that contribute to self-repair, including changes in the final LayerNorm scaling factor (which can repair up to 30% of the direct effect) and sparse sets of neurons implementing Anti-Erasure
    
[^32]: 利用不确定性和负对象性集成的异常检测

    Outlier detection by ensembling uncertainty with negative objectness

    [https://arxiv.org/abs/2402.15374](https://arxiv.org/abs/2402.15374)

    提出一种利用不确定性和负对象性集成的异常检测方法，通过直接预测K+1个logits并在密集预测结构中嵌入，可独立检测异常值。

    

    异常检测是监督式视觉识别中关键的功能。现有的大多数方法通过鼓励标准封闭集模型在负训练数据中产生低置信度预测来获得最佳结果。然而，这种方法混淆了预测不确定性和对负类别的识别。因此，我们重新考虑了直接预测K+1个logits，这些logits对应于K个基本真实类别和一个异常类别。这种设置允许我们制定一种新奇的异常得分，作为分布内不确定性和异常类别的后验的集合，我们称之为负对象性。现在，异常值可以通过高预测不确定性或与负数据相似之处独立检测。我们将我们的方法嵌入到一个密集预测结构中，该结构具有K+2个类别的掩码级别识别。训练过程鼓励新颖的K+2-th类别去学习

    arXiv:2402.15374v1 Announce Type: cross  Abstract: Outlier detection is an essential capability in safety-critical applications of supervised visual recognition. Most of the existing methods deliver best results by encouraging standard closed-set models to produce low-confidence predictions in negative training data. However, that approach conflates prediction uncertainty with recognition of the negative class. We therefore reconsider direct prediction of K+1 logits that correspond to K groundtruth classes and one outlier class. This setup allows us to formulate a novel anomaly score as an ensemble of in-distribution uncertainty and the posterior of the outlier class which we term negative objectness. Now outliers can be independently detected due to i) high prediction uncertainty or ii) similarity with negative data. We embed our method into a dense prediction architecture with mask-level recognition over K+2 classes. The training procedure encourages the novel K+2-th class to learn n
    
[^33]: 双编码器：利用句法和语义潜力进行方面情感三元组提取

    Dual Encoder: Exploiting the Potential of Syntactic and Semantic for Aspect Sentiment Triplet Extraction

    [https://arxiv.org/abs/2402.15370](https://arxiv.org/abs/2402.15370)

    提出了一种双编码器模型（D2E2S），结合了BERT通道和增强型LSTM通道来最大化单词间的句法和语义关系，引入了异构特征交互模块用于捕获复杂互动和动态选择重要节点。

    

    方面情感三元组提取（ASTE）是精细情感分析中的一个新兴任务。最近的研究使用图神经网络（GNN）来建模三元组元素固有的句法-语义关系。然而，他们尚未充分发挥ASTE任务中句法和语义信息的巨大潜力。在这项工作中，我们提出了一种\emph{双编码器：利用句法和语义潜力}模型（D2E2S），最大化单词间的句法和语义关系。具体而言，我们的模型利用双通道编码器，其中包括一个BERT通道来捕捉语义信息，以及一个增强型LSTM通道用于全面捕捉句法信息。随后，我们介绍了异构特征交互模块，以捕获依赖句法与注意力语义之间的复杂互动，并动态选择重要节点。我们利用这些模块的协同作用

    arXiv:2402.15370v1 Announce Type: cross  Abstract: Aspect Sentiment Triple Extraction (ASTE) is an emerging task in fine-grained sentiment analysis. Recent studies have employed Graph Neural Networks (GNN) to model the syntax-semantic relationships inherent in triplet elements. However, they have yet to fully tap into the vast potential of syntactic and semantic information within the ASTE task. In this work, we propose a \emph{Dual Encoder: Exploiting the potential of Syntactic and Semantic} model (D2E2S), which maximizes the syntactic and semantic relationships among words. Specifically, our model utilizes a dual-channel encoder with a BERT channel to capture semantic information, and an enhanced LSTM channel for comprehensive syntactic information capture. Subsequently, we introduce the heterogeneous feature interaction module to capture intricate interactions between dependency syntax and attention semantics, and to dynamically select vital nodes. We leverage the synergy of these m
    
[^34]: 针对病例-对照研究的逻辑回归半监督推断的高效方法

    Efficient semi-supervised inference for logistic regression under case-control studies

    [https://arxiv.org/abs/2402.15365](https://arxiv.org/abs/2402.15365)

    针对病例-对照研究的逻辑回归半监督推断，利用未标记数据可以识别截距参数，解决了截距参数在半监督学习中不可辨识的问题。

    

    半监督学习在统计学和机器学习中越来越受到关注。在半监督学习设置中，收集了一个带有结果和协变量的标记数据集，以及一个仅包含协变量的未标记数据集。我们考虑了在半监督设置中的推断问题，在这种设置下，标记数据中的结果是二进制的，经过病例-对照抽样的方式收集标记数据。病例-对照抽样是一种有效的抽样方案，可减轻二进制数据中的不平衡结构。在逻辑模型假设下，病例-对照数据仍然可以为回归模型的斜率参数提供一致的估计量。然而，截距参数是不可辨识的。因此，不能从病例-对照数据中估计边际比例。我们发现，在有未标记数据的情况下，可以在半监督学习设置中识别截距参数。

    arXiv:2402.15365v1 Announce Type: cross  Abstract: Semi-supervised learning has received increasingly attention in statistics and machine learning. In semi-supervised learning settings, a labeled data set with both outcomes and covariates and an unlabeled data set with covariates only are collected. We consider an inference problem in semi-supervised settings where the outcome in the labeled data is binary and the labeled data is collected by case-control sampling. Case-control sampling is an effective sampling scheme for alleviating imbalance structure in binary data. Under the logistic model assumption, case-control data can still provide consistent estimator for the slope parameter of the regression model. However, the intercept parameter is not identifiable. Consequently, the marginal case proportion cannot be estimated from case-control data. We find out that with the availability of the unlabeled data, the intercept parameter can be identified in semi-supervised learning setting.
    
[^35]: 禁止使用所有阈值：生物声学数据中呼叫密度的直接估计

    All Thresholds Barred: Direct Estimation of Call Density in Bioacoustic Data

    [https://arxiv.org/abs/2402.15360](https://arxiv.org/abs/2402.15360)

    直接估计生物声学数据中的呼叫密度而不考虑分类器分数，从而消除阈值选择引起的偏差计数。

    

    无线电学报：2402.15360v1 类型：交叉 摘要：被动声学监测(PAM)研究产生成千上万小时的音频，可以用于监测特定动物种群、进行广泛的生物多样性调查、检测盗猎等威胁，等等。用于物种识别的机器学习分类器越来越多地用于处理生物声学调查生成的大量音频，加快了分析速度并增加了PAM作为管理工具的效用。在常见做法中，对分类器输出分数应用阈值，并将超过阈值的分数聚合成检测计数。阈值的选择产生了有偏见的鸣叫计数，这些鸣叫计数受到在数据集子集间可能变化的误报/漏报率的影响。在这项工作中，我们倡导直接估计通话密度：包含目标鸣声的检测窗口的比例，而不考虑分类器分数。我们的方法致力于实现一种理想的

    arXiv:2402.15360v1 Announce Type: cross  Abstract: Passive acoustic monitoring (PAM) studies generate thousands of hours of audio, which may be used to monitor specific animal populations, conduct broad biodiversity surveys, detect threats such as poachers, and more. Machine learning classifiers for species identification are increasingly being used to process the vast amount of audio generated by bioacoustic surveys, expediting analysis and increasing the utility of PAM as a management tool. In common practice, a threshold is applied to classifier output scores, and scores above the threshold are aggregated into a detection count. The choice of threshold produces biased counts of vocalizations, which are subject to false positive/negative rates that may vary across subsets of the dataset. In this work, we advocate for directly estimating call density: The proportion of detection windows containing the target vocalization, regardless of classifier score. Our approach targets a desirabl
    
[^36]: 流式高斯狄利克雷随机场用于高维分类观测的空间预测

    Streaming Gaussian Dirichlet Random Fields for Spatial Predictions of High Dimensional Categorical Observations

    [https://arxiv.org/abs/2402.15359](https://arxiv.org/abs/2402.15359)

    该研究提出了一种新的流式高斯狄利克雷随机场模型，能够高效地处理高维分类观测数据，在空间预测中取得更准确的预测结果，并实现了有效的信息路径规划。

    

    我们提出了流式高斯狄利克雷随机场（S-GDRF）模型，这是一种用于建模流式空间分布稀疏、高维分类观测的新方法。所提出的方法高效地学习时空数据中的全局和局部模式，允许快速推断和查询，具有有界的时间复杂度。通过使用经过神经网络分类的高分辨率浮游生物图像数据系列，我们展示了该方法相对于变分高斯过程（VGP）能够做出更准确预测，并从流式分类数据中学习观测的预测分布。S-GDRF为有效地在高维分类观测上进行信息化路径规划打开了大门，而这在此前是不可行的。

    arXiv:2402.15359v1 Announce Type: cross  Abstract: We present the Streaming Gaussian Dirichlet Random Field (S-GDRF) model, a novel approach for modeling a stream of spatiotemporally distributed, sparse, high-dimensional categorical observations. The proposed approach efficiently learns global and local patterns in spatiotemporal data, allowing for fast inference and querying with a bounded time complexity. Using a high-resolution data series of plankton images classified with a neural network, we demonstrate the ability of the approach to make more accurate predictions compared to a Variational Gaussian Process (VGP), and to learn a predictive distribution of observations from streaming categorical data. S-GDRFs open the door to enabling efficient informative path planning over high-dimensional categorical observations, which until now has not been feasible.
    
[^37]: 对监督和无监督去噪方法的归一化等变性质进行调查

    On normalization-equivariance properties of supervised and unsupervised denoising methods: a survey

    [https://arxiv.org/abs/2402.15352](https://arxiv.org/abs/2402.15352)

    本文对图像去噪的监督和无监督学习方法进行了调查，着重关注归一化等变性质，突出了方法的原理和局限性

    

    图像去噪可能是图像处理中最古老且仍然是最活跃的研究课题之一。在过去几十年中引入了许多方法论概念，并随着卷积神经网络和监督深度学习的出现在近年来显著提高了性能。本文提出了对图像去噪的监督和无监督学习方法进行全面调查，对此演变过程中阐明的主要原则进行分类，特别关注监督学习的最新发展。它被构想为一个以当前方法为框架的教程，提供对文献中最有效方法的原理和局限性的见解，突出了许多方法之间的共同特征。最后，我们重点关注了归一化等变性质，这一性质令人惊讶地并没有得到保证。

    arXiv:2402.15352v1 Announce Type: cross  Abstract: Image denoising is probably the oldest and still one of the most active research topic in image processing. Many methodological concepts have been introduced in the past decades and have improved performances significantly in recent years, especially with the emergence of convolutional neural networks and supervised deep learning. In this paper, we propose a survey of guided tour of supervised and unsupervised learning methods for image denoising, classifying the main principles elaborated during this evolution, with a particular concern given to recent developments in supervised learning. It is conceived as a tutorial organizing in a comprehensive framework current approaches. We give insights on the rationales and limitations of the most performant methods in the literature, and we highlight the common features between many of them. Finally, we focus on on the normalization equivariance properties that is surprisingly not guaranteed 
    
[^38]: AutoMMLab：从语言指令自动生成可部署模型用于计算机视觉任务

    AutoMMLab: Automatically Generating Deployable Models from Language Instructions for Computer Vision Tasks

    [https://arxiv.org/abs/2402.15351](https://arxiv.org/abs/2402.15351)

    AutoMMLab是一个通用的LLM增强AutoML系统，通过用户的语言指令来自动化计算机视觉任务的整个模型生成工作流程，使非专家个体更容易构建特定任务的模型。

    

    arXiv:2402.15351v1 公告类型：新 提要：自动化机器学习（AutoML）是一组旨在自动化机器学习开发过程的技术。虽然传统的AutoML方法已成功应用于模型开发的几个关键步骤（例如超参数优化），但缺乏一个可以自动化整个端到端模型生成工作流程的AutoML系统。为了填补这一空白，我们提出了AutoMMLab，这是一个通用的LLM增强AutoML系统，按照用户的语言指令来自动化计算机视觉任务的整个模型生成工作流程。所提出的AutoMMLab系统有效地利用LLM作为连接AutoML和OpenMMLab社区的桥梁，使非专家个体能够通过用户友好的语言界面轻松构建特定任务的模型。具体地，我们提出RU-LLaMA来理解用户的请求并安排整个流水线，并提出一种基于LLM的超参数优化器 c

    arXiv:2402.15351v1 Announce Type: new  Abstract: Automated machine learning (AutoML) is a collection of techniques designed to automate the machine learning development process. While traditional AutoML approaches have been successfully applied in several critical steps of model development (e.g. hyperparameter optimization), there lacks a AutoML system that automates the entire end-to-end model production workflow. To fill this blank, we present AutoMMLab, a general-purpose LLM-empowered AutoML system that follows user's language instructions to automate the whole model production workflow for computer vision tasks. The proposed AutoMMLab system effectively employs LLMs as the bridge to connect AutoML and OpenMMLab community, empowering non-expert individuals to easily build task-specific models via a user-friendly language interface. Specifically, we propose RU-LLaMA to understand users' request and schedule the whole pipeline, and propose a novel LLM-based hyperparameter optimizer c
    
[^39]: Farsight：在AI应用原型设计过程中培养负责任的AI意识

    Farsight: Fostering Responsible AI Awareness During AI Application Prototyping

    [https://arxiv.org/abs/2402.15350](https://arxiv.org/abs/2402.15350)

    Farsight是一个新颖的实地交互工具，帮助人们在设计AI应用原型时识别潜在危害，用户研究表明使用Farsight后，AI原型设计者能够更好地独立识别与提示相关的潜在危害。

    

    大型语言模型（LLM）的提示驱动界面使得原型设计和构建AI应用比以往任何时候都更容易。然而，识别可能在AI应用中出现的潜在危害仍然是一个挑战，特别是在基于提示的原型设计过程中。为了解决这一问题，我们提出了一种新颖的实地交互工具Farsight，帮助人们识别他们正在设计原型的AI应用中可能出现的潜在危害。根据用户的提示，Farsight突出显示了与相关AI事件有关的新闻文章，并允许用户探索和编辑LLM生成的用例、利益相关者和危害。我们报告了与10位AI原型设计者进行的共同设计研究的设计见解，以及与42位AI原型设计者进行的用户研究结果。在使用Farsight后，我们用户研究中的AI原型设计者能够更好地独立识别与提示相关的潜在危害，并发现我们的工具比现有资源更有用且更易于使用。

    arXiv:2402.15350v1 Announce Type: cross  Abstract: Prompt-based interfaces for Large Language Models (LLMs) have made prototyping and building AI-powered applications easier than ever before. However, identifying potential harms that may arise from AI applications remains a challenge, particularly during prompt-based prototyping. To address this, we present Farsight, a novel in situ interactive tool that helps people identify potential harms from the AI applications they are prototyping. Based on a user's prompt, Farsight highlights news articles about relevant AI incidents and allows users to explore and edit LLM-generated use cases, stakeholders, and harms. We report design insights from a co-design study with 10 AI prototypers and findings from a user study with 42 AI prototypers. After using Farsight, AI prototypers in our user study are better able to independently identify potential harms associated with a prompt and find our tool more useful and usable than existing resources. T
    
[^40]: 信息论安全贝叶斯优化

    Information-Theoretic Safe Bayesian Optimization

    [https://arxiv.org/abs/2402.15347](https://arxiv.org/abs/2402.15347)

    提出了一种信息论安全探索准则，结合贝叶斯优化收益函数，形成了一种新颖的安全贝叶斯优化选择准则。

    

    我们考虑了一个顺序决策任务，其目标是在不评估违反先验未知（安全）约束的参数的情况下优化未知函数。一个常见的方法是在未知函数上放置高斯过程先验，并且仅允许在高概率安全区域内进行评估。大多数当前方法依赖于对域的离散化，并且不能直接扩展到连续情况。此外，它们利用约束的规则假设的方式引入了一个额外的关键超参数。在本文中，我们提出了一个信息论安全探索准则，该准则直接利用GP后验来识别最具信息的安全参数进行评估。将这一探索准则与众所周知的贝叶斯优化收益函数结合起来，产生了一种新颖的安全贝叶斯优化选择准则。

    arXiv:2402.15347v1 Announce Type: cross  Abstract: We consider a sequential decision making task, where the goal is to optimize an unknown function without evaluating parameters that violate an a~priori unknown (safety) constraint. A common approach is to place a Gaussian process prior on the unknown functions and allow evaluations only in regions that are safe with high probability. Most current methods rely on a discretization of the domain and cannot be directly extended to the continuous case. Moreover, the way in which they exploit regularity assumptions about the constraint introduces an additional critical hyperparameter. In this paper, we propose an information-theoretic safe exploration criterion that directly exploits the GP posterior to identify the most informative safe parameters to evaluate. The combination of this exploration criterion with a well known Bayesian optimization acquisition function yields a novel safe Bayesian optimization selection criterion. Our approach 
    
[^41]: Fourier基密度模型

    Fourier Basis Density Model

    [https://arxiv.org/abs/2402.15345](https://arxiv.org/abs/2402.15345)

    引入了一种基于受限Fourier基的轻量级、灵活且端到端可训练的概率密度模型，能够有效逼近各种多模态1维密度，表现优于传统的深度因式模型，同时在学习压缩任务中展示了其实用性。

    

    我们引入了一种轻量级、灵活且端到端可训练的概率密度模型，其由一个受限的Fourier基参数化。我们评估了该模型在逼近一系列多模态1维密度方面的表现，这些密度通常很难拟合。与[1]中引入的深度因式模型相比，我们的模型在类似的计算预算下实现了更低的交叉熵。此外，我们还在一个玩具压缩任务上评估了我们的方法，展示了其在学习压缩中的实用性。

    arXiv:2402.15345v1 Announce Type: new  Abstract: We introduce a lightweight, flexible and end-to-end trainable probability density model parameterized by a constrained Fourier basis. We assess its performance at approximating a range of multi-modal 1D densities, which are generally difficult to fit. In comparison to the deep factorized model introduced in [1], our model achieves a lower cross entropy at a similar computational budget. In addition, we also evaluate our method on a toy compression task, demonstrating its utility in learned compression.
    
[^42]: 使用固定和递减学习率的随机梯度下降的迭代和随机一阶预言者复杂度

    Iteration and Stochastic First-order Oracle Complexities of Stochastic Gradient Descent using Constant and Decaying Learning Rates

    [https://arxiv.org/abs/2402.15344](https://arxiv.org/abs/2402.15344)

    研究了在深度学习中使用固定或递减学习率的SGD进行非凸优化时，批量大小与迭代和SFO复杂度之间的关系，并指出使用关键批量大小的SGD可以最小化SFO复杂度

    

    随机梯度下降（SGD）的性能取决于学习率和批量大小，影响训练所需的迭代次数和随机一阶预言者（SFO）复杂度。先前的数值结果表明，对于使用固定学习率的SGD，随着批量大小的增加，训练所需的迭代次数减少，并且SFO复杂度在关键批量大小时最小化，一旦批量大小超过该大小后增加。本文研究了在深度学习中使用固定或递减学习率的SGD进行非凸优化时，批量大小与所需迭代和SFO复杂度之间的关系，并表明使用关键批量大小的SGD可以最小化SFO复杂度。

    arXiv:2402.15344v1 Announce Type: cross  Abstract: The performance of stochastic gradient descent (SGD), which is the simplest first-order optimizer for training deep neural networks, depends on not only the learning rate but also the batch size. They both affect the number of iterations and the stochastic first-order oracle (SFO) complexity needed for training. In particular, the previous numerical results indicated that, for SGD using a constant learning rate, the number of iterations needed for training decreases when the batch size increases, and the SFO complexity needed for training is minimized at a critical batch size and that it increases once the batch size exceeds that size. Here, we study the relationship between batch size and the iteration and SFO complexities needed for nonconvex optimization in deep learning with SGD using constant or decaying learning rates and show that SGD using the critical batch size minimizes the SFO complexity. We also provide numerical compariso
    
[^43]: NuNER: 利用LLM注释数据进行实体识别编码器预训练

    NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data

    [https://arxiv.org/abs/2402.15343](https://arxiv.org/abs/2402.15343)

    利用LLM注释数据进行实体识别编码器预训练，创建了NuNER，一种专门用于命名实体识别任务的紧凑语言表示模型，可以在少样本学习领域胜过相似大小的基础模型，并与更大的LLMs竞争。

    

    大型语言模型（LLMs）展现出在数据标注方面令人印象深刻的能力，为解决经典的自然语言处理问题提供了新的途径。本文展示了如何利用LLMs创建NuNER，这是一个专门针对命名实体识别（NER）任务的紧凑语言表示模型。NuNER可以被微调以以高效的方式解决下游的NER问题，在少样本学习领域胜过相似大小的基础模型，并与更大的LLMs竞争。我们发现预训练数据集的大小和实体类型的多样性是取得良好性能的关键。我们将NuNER视为最近被LLMs解锁的更广泛的特定任务基础模型家族的一员。

    arXiv:2402.15343v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown impressive abilities in data annotation, opening the way for new approaches to solve classic NLP problems. In this paper, we show how to use LLMs to create NuNER, a compact language representation model specialized in the Named Entity Recognition (NER) task. NuNER can be fine-tuned to solve downstream NER problems in a data-efficient way, outperforming similar-sized foundation models in the few-shot regime and competing with much larger LLMs. We find that the size and entity-type diversity of the pre-training dataset are key to achieving good performance. We view NuNER as a member of the broader family of task-specific foundation models, recently unlocked by LLMs.
    
[^44]: 使用LLMs沿着概念空间维度对实体进行排名：微调策略分析

    Ranking Entities along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies

    [https://arxiv.org/abs/2402.15337](https://arxiv.org/abs/2402.15337)

    本研究通过使用LLMs探索概念空间维度，提出了一种新颖的实体排名方法，并分析其在感知和主观特征上的转移能力。

    

    概念空间以实体的原始语义特征表示。这种表示非常有价值，但学习起来非常困难，特别是在建模感知和主观特征时。从大型语言模型（LLMs）中提炼概念空间最近出现为一种有前途的策略。然而，现有工作仅限于使用相对简单的零样本策略探查预训练的LLMs。我们特别关注根据给定的概念空间维度对实体进行排名的任务。不幸的是，由于概念空间维度的真实排名很少见，我们无法直接在这个任务上微调LLMs。因此，我们使用更容易获得的特征作为训练数据，并分析由此产生的模型的排名能力是否能转移到感知和主观特征。我们发现在某种程度上确实是这种情况，但是未完成的句子。

    arXiv:2402.15337v1 Announce Type: new  Abstract: Conceptual spaces represent entities in terms of their primitive semantic features. Such representations are highly valuable but they are notoriously difficult to learn, especially when it comes to modelling perceptual and subjective features. Distilling conceptual spaces from Large Language Models (LLMs) has recently emerged as a promising strategy. However, existing work has been limited to probing pre-trained LLMs using relatively simple zero-shot strategies. We focus in particular on the task of ranking entities according to a given conceptual space dimension. Unfortunately, we cannot directly fine-tune LLMs on this task, because ground truth rankings for conceptual space dimensions are rare. We therefore use more readily available features as training data and analyse whether the ranking capabilities of the resulting models transfer to perceptual and subjective features. We find that this is indeed the case, to some extent, but havi
    
[^45]: 低秩表示遇上深度展开：用于高光谱异常检测的广义和可解释网络

    Low-Rank Representations Meets Deep Unfolding: A Generalized and Interpretable Network for Hyperspectral Anomaly Detection

    [https://arxiv.org/abs/2402.15335](https://arxiv.org/abs/2402.15335)

    提出了一个广义和可解释的HAD网络，通过深度展开可学习字典的LRR模型，称为LRR-Net$^+$，可以更通用地光谱解耦背景结构和目标特性，并消除由重要干扰目标引入的偏差。同时，还构建了用于改进HAD算法鲁棒性的新基准数据集AIR-HAD。

    

    当前高光谱异常检测（HAD）基准数据集存在分辨率低、背景简单、检测数据规模小等问题。这些因素也限制了众所周知的低秩表示（LRR）模型在背景和目标特征分离方面的鲁棒性以及对手动参数选择的依赖性能。基于此，我们建立了一组新的用于改善复杂场景下HAD算法鲁棒性的HAD基准数据集，简称AIR-HAD。因此，我们提出了一个广义和可解释的HAD网络，通过深度展开可学习字典的LRR模型来完成，命名为LRR-Net$^+$，能够更通用地光谱解耦背景结构和目标特性，并同时消除由重要干扰目标引入的偏差。此外，LRR-Net$^+$集成了交替方向法的解决过程

    arXiv:2402.15335v1 Announce Type: cross  Abstract: Current hyperspectral anomaly detection (HAD) benchmark datasets suffer from low resolution, simple background, and small size of the detection data. These factors also limit the performance of the well-known low-rank representation (LRR) models in terms of robustness on the separation of background and target features and the reliance on manual parameter selection. To this end, we build a new set of HAD benchmark datasets for improving the robustness of the HAD algorithm in complex scenarios, AIR-HAD for short. Accordingly, we propose a generalized and interpretable HAD network by deeply unfolding a dictionary-learnable LLR model, named LRR-Net$^+$, which is capable of spectrally decoupling the background structure and object properties in a more generalized fashion and eliminating the bias introduced by vital interference targets concurrently. In addition, LRR-Net$^+$ integrates the solution process of the Alternating Direction Metho
    
[^46]: 分类深度学习：一种关于架构的代数理论

    Categorical Deep Learning: An Algebraic Theory of Architectures

    [https://arxiv.org/abs/2402.15332](https://arxiv.org/abs/2402.15332)

    提出了一种关于深度学习架构的代数理论，应用范畴论构建了一个桥梁，有效地涵盖了神经网络设计的不同风格，同时自然地编码了计算机科学和自动机理论中的许多标准结构。

    

    我们提出了一个关于指定和研究深度学习架构的通用框架的立场。我们认为到目前为止关于这一领域的关键尝试缺乏一种一致的桥梁，能够指定模型必须满足的约束并规定它们的实现方式。专注于构建这样一个桥梁，我们建议应用范畴论——准确地说，单子值于参数映射的二范畴的通用代数——作为一种单一理论，优雅地包含了神经网络设计的这两种风格。为了支持我们的观点，我们展示了这一理论如何恢复由几何深度学习导致的约束，以及从神经网络不同领域的多种架构（如RNNs）的实现。我们还展示了这一理论如何自然地编码了计算机科学和自动机理论中的许多标准结构。

    arXiv:2402.15332v1 Announce Type: cross  Abstract: We present our position on the elusive quest for a general-purpose framework for specifying and studying deep learning architectures. Our opinion is that the key attempts made so far lack a coherent bridge between specifying constraints which models must satisfy and specifying their implementations. Focusing on building a such a bridge, we propose to apply category theory -- precisely, the universal algebra of monads valued in a 2-category of parametric maps -- as a single theory elegantly subsuming both of these flavours of neural network design. To defend our position, we show how this theory recovers constraints induced by geometric deep learning, as well as implementations of many architectures drawn from the diverse landscape of neural networks, such as RNNs. We also illustrate how the theory naturally encodes many standard constructs in computer science and automata theory.
    
[^47]: 为多任务学习建立基于原则的任务分组方法

    Towards Principled Task Grouping for Multi-Task Learning

    [https://arxiv.org/abs/2402.15328](https://arxiv.org/abs/2402.15328)

    提出了一种为多任务学习建立基于原则的任务分组方法，该方法在理论和实践上具有优势，通过灵活的数学规划形式解决了资源约束问题。

    

    本文提出了一种新颖的多任务学习（MTL）中任务分组的方法，超越了现有方法，解决了关键的理论和实际限制。与之前的研究不同，我们的方法提供了一个更具理论基础的方法，不依赖于构建转移增益的限制性假设。我们还提出了一种灵活的数学规划形式，可以适应各种资源约束，从而增强了其多功能性。在各种领域进行的实验结果，包括计算机视觉数据集、组合优化基准和时间序列任务，证明了我们的方法相对于广泛的基线方法的优越性，验证了其在MTL中的有效性和普适性。

    arXiv:2402.15328v1 Announce Type: new  Abstract: This paper presents a novel approach to task grouping in Multitask Learning (MTL), advancing beyond existing methods by addressing key theoretical and practical limitations. Unlike prior studies, our approach offers a more theoretically grounded method that does not rely on restrictive assumptions for constructing transfer gains. We also propose a flexible mathematical programming formulation which can accommodate a wide spectrum of resource constraints, thus enhancing its versatility. Experimental results across diverse domains, including computer vision datasets, combinatorial optimization benchmarks and time series tasks, demonstrate the superiority of our method over extensive baselines, validating its effectiveness and general applicability in MTL.
    
[^48]: 从算子半群理论的视角理解扩散型GNN中的过度平滑问题

    Understanding Oversmoothing in Diffusion-Based GNNs From the Perspective of Operator Semigroup Theory

    [https://arxiv.org/abs/2402.15326](https://arxiv.org/abs/2402.15326)

    本文研究了扩散型GNN中的过度平滑问题，通过算子半群理论严格证明了过度平滑与扩散算子的遍历性密切相关，提出了一个更普遍和理论上基础的减轻过度平滑问题的方法，并提供了概率解释。

    

    本文从算子半群理论的角度对扩散型图神经网络（GNN）中的过度平滑问题进行了全新研究。与基于随机游走分析或粒子系统的现有方法不同，我们通过算子半群理论来解决这一问题。这一理论框架使我们能够严格证明过度平滑与扩散算子的遍历性息息相关。这一发现进一步提出了一个通用且温和的遍历性破坏条件，包括先前提出的各种特定解决方案，从而提出了一个更普遍和理论上基础的方法来减轻扩散型GNN中的过度平滑问题。此外，我们还提出了对我们理论的概率解释，与之前的研究建立了联系，拓宽了理论视野。我们的实验结果表明，这一遍历性破坏项有效地减少了以迪利克雷能量衡量的过度平滑，并达到了模拟

    arXiv:2402.15326v1 Announce Type: new  Abstract: This paper presents a novel study of the oversmoothing issue in diffusion-based Graph Neural Networks (GNNs). Diverging from extant approaches grounded in random walk analysis or particle systems, we approach this problem through operator semigroup theory. This theoretical framework allows us to rigorously prove that oversmoothing is intrinsically linked to the ergodicity of the diffusion operator. This finding further poses a general and mild ergodicity-breaking condition, encompassing the various specific solutions previously offered, thereby presenting a more universal and theoretically grounded approach to mitigating oversmoothing in diffusion-based GNNs. Additionally, we offer a probabilistic interpretation of our theory, forging a link with prior works and broadening the theoretical horizon. Our experimental results reveal that this ergodicity-breaking term effectively mitigates oversmoothing measured by Dirichlet energy, and simul
    
[^49]: 基于Shapley值的多智能体强化学习：理论、方法及其在能源网络中的应用

    Shapley Value Based Multi-Agent Reinforcement Learning: Theory, Method and Its Application to Energy Network

    [https://arxiv.org/abs/2402.15324](https://arxiv.org/abs/2402.15324)

    通过合作博弈论，本论文将Shapley值和凸博弈概念扩展到马尔可夫决策过程，用于解决多智能体强化学习中的信用分配问题。

    

    arXiv:2402.15324v1 公告类型：交叉摘要：多智能体强化学习是人工智能和机器学习领域快速发展的领域。其中一个重要问题是如何在多智能体系统中进行信用分配。已经设计了许多通过多智能体强化学习算法进行信用分配的方案。尽管这些信用分配方案已被证明对改善多智能体强化学习的性能有用，但大多数是启发式设计的，缺乏严格的理论基础，因此无法理解智能体如何合作。在这篇论文中，我们旨在通过合作博弈论研究多智能体强化学习中的信用分配基础。我们首先将一个称为凸博弈的游戏模型和一个称为Shapley值的支付分配方案在合作博弈论中扩展到马尔可夫决策过程，命名为马尔可夫凸博弈和马氏S

    arXiv:2402.15324v1 Announce Type: cross  Abstract: Multi-agent reinforcement learning is an area of rapid advancement in artificial intelligence and machine learning. One of the important questions to be answered is how to conduct credit assignment in a multi-agent system. There have been many schemes designed to conduct credit assignment by multi-agent reinforcement learning algorithms. Although these credit assignment schemes have been proved useful in improving the performance of multi-agent reinforcement learning, most of them are designed heuristically without a rigorous theoretic basis and therefore infeasible to understand how agents cooperate. In this thesis, we aim at investigating the foundation of credit assignment in multi-agent reinforcement learning via cooperative game theory. We first extend a game model called convex game and a payoff distribution scheme called Shapley value in cooperative game theory to Markov decision process, named as Markov convex game and Markov S
    
[^50]: OpenSUN3D: 开放词汇的3D场景理解第一次研讨会挑战

    OpenSUN3D: 1st Workshop Challenge on Open-Vocabulary 3D Scene Understanding

    [https://arxiv.org/abs/2402.15321](https://arxiv.org/abs/2402.15321)

    提供了OpenSUN3D研讨会上针对开放词汇3D场景理解的挑战概述，包括挑战数据集、评估方法和获胜方法的简要描述

    

    这份报告概述了在2023年ICCV会议上举办的OpenSUN3D Workshop关于开放词汇3D场景理解的挑战。该研讨会系列的目标是为开放词汇3D场景理解任务提供探索和讨论平台，包括但不限于分割、检测和映射。我们提供了研讨会上举办的挑战概述，展示了挑战数据集、评估方法以及获胜方法的简要描述。更多详情请参阅https://opensun3d.github.io/index_iccv23.html。

    arXiv:2402.15321v1 Announce Type: cross  Abstract: This report provides an overview of the challenge hosted at the OpenSUN3D Workshop on Open-Vocabulary 3D Scene Understanding held in conjunction with ICCV 2023. The goal of this workshop series is to provide a platform for exploration and discussion of open-vocabulary 3D scene understanding tasks, including but not limited to segmentation, detection and mapping. We provide an overview of the challenge hosted at the workshop, present the challenge dataset, the evaluation methodology, and brief descriptions of the winning methods. For additional details, please see https://opensun3d.github.io/index_iccv23.html.
    
[^51]: GPTVQ：LLM量化中维度的福音

    GPTVQ: The Blessing of Dimensionality for LLM Quantization

    [https://arxiv.org/abs/2402.15319](https://arxiv.org/abs/2402.15319)

    通过增加量化维度，GPTVQ方法在大型语言模型的量化中取得了新的最优结果，不仅显著改善了大小与准确性的权衡，还提高了处理效率。

    

    在这项工作中，我们展示了通过增加量化维度可以显著改善神经网络量化的大小与准确性权衡。我们提出了GPTVQ方法，这是一种新的快速后训练向量量化（VQ）方法，适用于大型语言模型（LLMs）。我们的方法交替进行一个或多个列的量化，并使用来自每层输出重建MSE的Hessian信息来更新其余未量化的权重。量化码书使用一种高效的数据感知版本的EM算法进行初始化。然后，通过使用整数量化和基于SVD的压缩进一步压缩码书。GPTVQ在诸如Llama-v2和Mistral等各种LLMs上建立了新的最新技术，大小与准确性之间的权衡。此外，我们的方法高效：在单个H100上，处理一个Llamav2-70B需要3至11小时。

    arXiv:2402.15319v1 Announce Type: cross  Abstract: In this work we show that the size versus accuracy trade-off of neural network quantization can be significantly improved by increasing the quantization dimensionality. We propose the GPTVQ method, a new fast method for post-training vector quantization (VQ) that scales well to Large Language Models (LLMs). Our method interleaves quantization of one or more columns with updates to the remaining unquantized weights, using information from the Hessian of the per-layer output reconstruction MSE. Quantization codebooks are initialized using an efficient data-aware version of the EM algorithm. The codebooks are then updated, and further compressed by using integer quantization and SVD-based compression. GPTVQ establishes a new state-of-the art in the size vs accuracy trade-offs on a wide range of LLMs such as Llama-v2 and Mistral. Furthermore, our method is efficient: on a single H100 it takes between 3 and 11 hours to process a Llamav2-70B
    
[^52]: 关于神经网络中的最小深度

    On Minimal Depth in Neural Networks

    [https://arxiv.org/abs/2402.15315](https://arxiv.org/abs/2402.15315)

    本研究研究了神经网络中关于最小深度的问题，特别关注了ReLU神经网络的表达能力和最小深度与CPWL函数的关系。

    

    通过对ReLU神经网络表达能力以及与表示任何连续分段线性函数（CPWL）所需的最小深度相关的猜想的关系进行研究，本研究探讨了神经网络的表达能力特性。研究重点包括对求和和最大运算的最小深度表示，以及对多面体神经网络的探索。实验结果表明，对于求和运算，我们建立了关于操作数最小深度的充分条件以找到运算的最小深度。相反，关于最大运算，我们提供了全面的例子，证明仅依赖于操作数深度的充分条件，并不会暗示运算的最小深度。研究还考察了凸CPWL函数之间的最小深度关系。

    arXiv:2402.15315v1 Announce Type: new  Abstract: A characterization of the representability of neural networks is relevant to comprehend their success in artificial intelligence. This study investigate two topics on ReLU neural network expressivity and their connection with a conjecture related to the minimum depth required for representing any continuous piecewise linear function (CPWL). The topics are the minimal depth representation of the sum and max operations, as well as the exploration of polytope neural networks. For the sum operation, we establish a sufficient condition on the minimal depth of the operands to find the minimal depth of the operation. In contrast, regarding the max operation, a comprehensive set of examples is presented, demonstrating that no sufficient conditions, depending solely on the depth of the operands, would imply a minimal depth for the operation. The study also examine the minimal depth relationship between convex CPWL functions. On polytope neural ne
    
[^53]: ArabianGPT：基于原生阿拉伯语的大型语言模型

    ArabianGPT: Native Arabic GPT-based Large Language

    [https://arxiv.org/abs/2402.15313](https://arxiv.org/abs/2402.15313)

    提出了ArabianGPT，这是一系列专门为阿拉伯语设计的基于Transformer的模型，包括大小和复杂性不同的ArabianGPT-0.1B和ArabianGPT-0.3B，帮助弥补了本土阿拉伯语大型语言模型的不足。

    

    英语和拉丁语为主导的大型语言模型（LLMs）的主导地位导致了本土阿拉伯语LLMs的显著不足。本文提出ArabianGPT，这是一系列基于Transformer的模型，专门为阿拉伯语设计而成。这些模型包括ArabianGPT-0.1B和ArabianGPT-0.3B，大小和复杂性不同，与阿拉伯语的微妙语言特征相契合。

    arXiv:2402.15313v1 Announce Type: cross  Abstract: The predominance of English and Latin-based large language models (LLMs) has led to a notable deficit in native Arabic LLMs. This discrepancy is accentuated by the prevalent inclusion of English tokens in existing Arabic models, detracting from their efficacy in processing native Arabic's intricate morphology and syntax. Consequently, there is a theoretical and practical imperative for developing LLMs predominantly focused on Arabic linguistic elements. To address this gap, this paper proposes ArabianGPT, a series of transformer-based models within the ArabianLLM suite designed explicitly for Arabic. These models, including ArabianGPT-0.1B and ArabianGPT-0.3B, vary in size and complexity, aligning with the nuanced linguistic characteristics of Arabic. The AraNizer tokenizer, integral to these models, addresses the unique morphological aspects of Arabic script, ensuring more accurate text processing. Empirical results from fine-tuning t
    
[^54]: 具有可识别性保证的反事实生成

    Counterfactual Generation with Identifiability Guarantees

    [https://arxiv.org/abs/2402.15309](https://arxiv.org/abs/2402.15309)

    反事实生成面临着配对数据稀缺和标注信息有限等挑战，现有方法依赖过度简化的假设，但复杂数据分布下这些假设可能不成立。

    

    反事实生成是各种机器学习任务的核心，包括图像转换和可控文本生成。这一生成过程通常需要识别潜在的分解表征，如内容和风格，这些表征潜在地支撑着观察到的数据。然而，当面临配对数据和标注信息的稀缺时，情况变得更具挑战性。现有的分解方法关键依赖于过度简化的假设，比如假设内容和风格变量独立，以识别潜在变量，尽管这样的假设可能并不适用于复杂的数据分布。例如，食物评论往往涉及“美味”等词语，而电影评论通常含有“惊心动魄”等词语表示同样的积极情感。当数据从多个领域采样时，由于内容和风格之间的相互关系可能会显著变化，这个问题会变得更加严重。

    arXiv:2402.15309v1 Announce Type: cross  Abstract: Counterfactual generation lies at the core of various machine learning tasks, including image translation and controllable text generation. This generation process usually requires the identification of the disentangled latent representations, such as content and style, that underlie the observed data. However, it becomes more challenging when faced with a scarcity of paired data and labeling information. Existing disentangled methods crucially rely on oversimplified assumptions, such as assuming independent content and style variables, to identify the latent variables, even though such assumptions may not hold for complex data distributions. For instance, food reviews tend to involve words like tasty, whereas movie reviews commonly contain words such as thrilling for the same positive sentiment. This problem is exacerbated when data are sampled from multiple domains since the dependence between content and style may vary significantly
    
[^55]: 在大型视觉语言模型中表示在线手写识别

    Representing Online Handwriting for Recognition in Large Vision-Language Models

    [https://arxiv.org/abs/2402.15307](https://arxiv.org/abs/2402.15307)

    本文研究了在大型视觉语言模型中进行在线手写识别，提出了一种新颖的数字墨水(tokenized representation)表示方法，将手写呈现为文本序列和图像，取得了可与现有方法媲美的结果。

    

    随着带有触摸屏和触控笔的平板电脑的普及，将手写转换为文本的关键功能得以实现，实现了搜索、索引和人工智能辅助。与此同时，视觉语言模型(VLMs)现在已成为图像理解的首选解决方案，这要归功于它们在各种任务上的最先进性能，以及训练、微调和推理的统一方法的简洁性。虽然VLMs在基于图像的任务上表现出色，但在朴素应用时(即将手写呈现为图像并执行光学字符识别(OCR))在手写识别方面效果不佳。在本文中，我们研究了VLMs中的在线手写识别，超越了朴素的OCR。我们提出了一种将数字墨水(在线手写)的新型标记化表示，其中包括作为文本的一系列按时间顺序排列的笔画，以及作为图像。我们展示了这种表示得到的结果与

    arXiv:2402.15307v1 Announce Type: cross  Abstract: The adoption of tablets with touchscreens and styluses is increasing, and a key feature is converting handwriting to text, enabling search, indexing, and AI assistance. Meanwhile, vision-language models (VLMs) are now the go-to solution for image understanding, thanks to both their state-of-the-art performance across a variety of tasks and the simplicity of a unified approach to training, fine-tuning, and inference. While VLMs obtain high performance on image-based tasks, they perform poorly on handwriting recognition when applied naively, i.e., by rendering handwriting as an image and performing optical character recognition (OCR). In this paper, we study online handwriting recognition with VLMs, going beyond naive OCR. We propose a novel tokenized representation of digital ink (online handwriting) that includes both a time-ordered sequence of strokes as text, and as image. We show that this representation yields results comparable to
    
[^56]: 基于大型语言模型的检索增强生成的因果图发现

    Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models

    [https://arxiv.org/abs/2402.15301](https://arxiv.org/abs/2402.15301)

    利用大型语言模型和检索增强生成技术，提出了一种新方法用于从科学文献中推断因果关系，以解决传统方法受限于数据收集偏见和个体知识的问题。

    

    因果图恢复在因果推断领域至关重要。传统方法通常是基于知识或统计估计，受数据收集偏见和个体关于影响变量之间关系的知识的限制。大型语言模型（LLMs）的进步为解决这些问题提供了机会。我们提出了一种利用大量科学文献中所包含的知识推导一般因果图恢复任务中的因果关系的新方法。该方法利用基于检索增强生成（RAG）的LLMs系统地分析和提取来自广泛研究论文集的相关信息。我们的方法首先从汇总的文献中检索相关文本片段。然后，LLM被用来识别和标记因素之间的潜在关联。最后，我们给出了一个...

    arXiv:2402.15301v1 Announce Type: new  Abstract: Causal graph recovery is essential in the field of causal inference. Traditional methods are typically knowledge-based or statistical estimation-based, which are limited by data collection biases and individuals' knowledge about factors affecting the relations between variables of interests. The advance of large language models (LLMs) provides opportunities to address these problems. We propose a novel method that utilizes the extensive knowledge contained within a large corpus of scientific literature to deduce causal relationships in general causal graph recovery tasks. This method leverages Retrieval Augmented-Generation (RAG) based LLMs to systematically analyze and extract pertinent information from a comprehensive collection of research papers. Our method first retrieves relevant text chunks from the aggregated literature. Then, the LLM is tasked with identifying and labelling potential associations between factors. Finally, we giv
    
[^57]: 见证为信：通过CLIP引导解码缓解大型视觉-语言模型中的幻觉

    Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding

    [https://arxiv.org/abs/2402.15300](https://arxiv.org/abs/2402.15300)

    CLIP相似性作为更强大和更稳健的幻觉指标，研究提出了CLIP引导解码（CGD）方法，在大型视觉-语言模型中有效减少对象幻觉。

    

    大型视觉-语言模型(LVLMs)容易出现对象幻觉，即生成的文本包含不存在的对象，严重限制了它们的可靠性和实用性。我们首先对句子级LVLM幻觉进行实证分析，发现与图像的CLIP相似性作为一个比单词可能性更强大、更稳健的幻觉指示器。基于这一发现，我们提出了CLIP引导解码（CGD）方法，这是一种简单但有效的无需训练的方法，用于减少解码时的对象幻觉。CGD利用CLIP来引导模型的解码过程，通过增强生成文本与图像的视觉联系。实验表明，CGD有效地减轻了对象幻觉。

    arXiv:2402.15300v1 Announce Type: cross  Abstract: Large Vision-Language Models (LVLMs) are susceptible to object hallucinations, an issue in which their generated text contains non-existent objects, greatly limiting their reliability and practicality. Current approaches often rely on the model's token likelihoods or other internal information, instruction tuning on additional datasets, or incorporating complex external tools. We first perform empirical analysis on sentence-level LVLM hallucination, finding that CLIP similarity to the image acts as a stronger and more robust indicator of hallucination compared to token likelihoods. Motivated by this, we introduce our CLIP-Guided Decoding (CGD) approach, a straightforward but effective training-free approach to reduce object hallucination at decoding time. CGD uses CLIP to guide the model's decoding process by enhancing visual grounding of generated text with the image. Experiments demonstrate that CGD effectively mitigates object hallu
    
[^58]: 基于逐像素密度分布建模的半监督计数

    Semi-supervised Counting via Pixel-by-pixel Density Distribution Modelling

    [https://arxiv.org/abs/2402.15297](https://arxiv.org/abs/2402.15297)

    提出了一种基于像素级密度分布建模的半监督人群计数方法，通过匹配损失、密度标记和自监督学习机制，取得了明显优于竞争对手的效果

    

    本文关注半监督人群计数，仅有少量标记的训练数据可用。我们将像素级密度值建模为概率分布，而不是单一确定的值进行回归。在此基础上，我们提出了一种半监督人群计数模型。首先，我们设计了一个逐像素分布匹配损失，用于衡量预测和真实密度分布之间的差异；其次，我们通过使用密度标记来增强变压器解码器，使解码器的前向针对不同密度区间有所专门化；第三，我们设计了交错一致性自监督学习机制，以有效地从未标记的数据中学习。对四个数据集进行了大量实验，结果表明我们的方法在各种标记比例设置下明显优于竞争对手。代码将发布在 https://g

    arXiv:2402.15297v1 Announce Type: cross  Abstract: This paper focuses on semi-supervised crowd counting, where only a small portion of the training data are labeled. We formulate the pixel-wise density value to regress as a probability distribution, instead of a single deterministic value. On this basis, we propose a semi-supervised crowd-counting model. Firstly, we design a pixel-wise distribution matching loss to measure the differences in the pixel-wise density distributions between the prediction and the ground truth; Secondly, we enhance the transformer decoder by using density tokens to specialize the forwards of decoders w.r.t. different density intervals; Thirdly, we design the interleaving consistency self-supervised learning mechanism to learn from unlabeled data efficiently. Extensive experiments on four datasets are performed to show that our method clearly outperforms the competitors by a large margin under various labeled ratio settings. Code will be released at https://g
    
[^59]: 在互动背景下音乐生成的调查

    A Survey of Music Generation in the Context of Interaction

    [https://arxiv.org/abs/2402.15294](https://arxiv.org/abs/2402.15294)

    该论文调查了音乐生成领域的现状，针对机器学习在音乐创作中的应用进行了综合评估，探讨了当前研究的主要焦点以及存在的挑战。

    

    近年来，机器学习，特别是生成对抗神经网络（GANs）和基于注意力机制的神经网络（transformers），已成功用于创作和生成音乐，包括旋律和复调作品。当前研究主要集中在风格复制（例如生成巴赫风格赋格曲）或风格转移（例如古典到爵士）上，基于大量录制或转录的音乐，这也允许相当直接的“表现”评估。然而，大多数这些模型不适合通过实时互动进行人机共创，也不清楚这些模型和生成的作品将如何评估。本文全面审查了音乐表示、特征分析、启发式算法、统计和参数建模，以及人为和自动评估措施，同时讨论了哪些方法和模型似乎最为重要。

    arXiv:2402.15294v1 Announce Type: cross  Abstract: In recent years, machine learning, and in particular generative adversarial neural networks (GANs) and attention-based neural networks (transformers), have been successfully used to compose and generate music, both melodies and polyphonic pieces. Current research focuses foremost on style replication (eg. generating a Bach-style chorale) or style transfer (eg. classical to jazz) based on large amounts of recorded or transcribed music, which in turn also allows for fairly straight-forward "performance" evaluation. However, most of these models are not suitable for human-machine co-creation through live interaction, neither is clear, how such models and resulting creations would be evaluated. This article presents a thorough review of music representation, feature analysis, heuristic algorithms, statistical and parametric modelling, and human and automatic evaluation measures, along with a discussion of which approaches and models seem m
    
[^60]: 嵌入线性动力学的神经网络用于长序列建模

    Linear Dynamics-embedded Neural Network for Long-Sequence Modeling

    [https://arxiv.org/abs/2402.15290](https://arxiv.org/abs/2402.15290)

    提出了一种名为嵌入线性动力学的神经网络（LDNN），通过引入连续状态空间模型的属性和优化策略，实现了在长序列任务中具有少量参数、灵活推断和高效训练，最终在长距离竞技场上取得了有效且领先的性能。

    

    由于现有模型在长序列建模中性能和计算效率之间的权衡成为瓶颈，受到控制理论中具有多输入多输出的连续状态空间模型（SSMs）启发，我们提出了一种名为嵌入线性动力学的神经网络（LDNN）的新型神经网络。 SSM的连续、离散和卷积属性使LDNN具有少量参数、灵活的推断和在长序列任务中高效训练的特点。 我们开发了两种有效策略，对角化和“解耦然后快速傅立叶变换（FFT）”，以将卷积的时间复杂度从$O(LNH\max\{L, N\})$降低到$O(LN\max\{H, \log L\})$。 我们通过双向非因果和多头设置进一步改进了LDNN，以适应更广泛的应用范围。 对长距离竞技场（LRA）的大量实验表明了LDNN的有效性和最先进的性能。

    arXiv:2402.15290v1 Announce Type: cross  Abstract: The trade-off between performance and computational efficiency in long-sequence modeling becomes a bottleneck for existing models. Inspired by the continuous state space models (SSMs) with multi-input and multi-output in control theory, we propose a new neural network called Linear Dynamics-embedded Neural Network (LDNN). SSMs' continuous, discrete, and convolutional properties enable LDNN to have few parameters, flexible inference, and efficient training in long-sequence tasks. Two efficient strategies, diagonalization and $'\text{Disentanglement then Fast Fourier Transform (FFT)}'$, are developed to reduce the time complexity of convolution from $O(LNH\max\{L, N\})$ to $O(LN\max \{H, \log L\})$. We further improve LDNN through bidirectional noncausal and multi-head settings to accommodate a broader range of applications. Extensive experiments on the Long Range Arena (LRA) demonstrate the effectiveness and state-of-the-art performance
    
[^61]: 逐步矫正：用扩散模型改进基于方面的情感分析

    Let's Rectify Step by Step: Improving Aspect-based Sentiment Analysis with Diffusion Models

    [https://arxiv.org/abs/2402.15289](https://arxiv.org/abs/2402.15289)

    提出了DiffusionABSA，一种新颖的扩散模型，可以逐步提取方面，并通过识别途中的上下文信息来改进基于方面的情感分析。

    

    基于方面的情感分析（ABSA）在预测文本中与识别方面相关的情感极性方面扮演着关键角色。然而，ABSA中的一个值得注意的挑战在于精确确定方面的边界（起始和结束索引），特别是对于长表达式，这是由于用户的口语表达。我们提出了DiffusionABSA，一种为ABSA量身定制的新颖扩散模型，该模型逐步提取方面。特别地，DiffusionABSA在训练过程中逐渐向方面术语添加噪音，随后学习一个逆向逐渐恢复这些术语的去噪过程。为了估计边界，我们设计了一个通过语法感知的时间注意机制增强的去噪神经网络，以按时间顺序捕捉方面与周围文本之间的相互作用。对八个基准数据集进行的实证评估突显了DiffusionABSA的明显优势。

    arXiv:2402.15289v1 Announce Type: cross  Abstract: Aspect-Based Sentiment Analysis (ABSA) stands as a crucial task in predicting the sentiment polarity associated with identified aspects within text. However, a notable challenge in ABSA lies in precisely determining the aspects' boundaries (start and end indices), especially for long ones, due to users' colloquial expressions. We propose DiffusionABSA, a novel diffusion model tailored for ABSA, which extracts the aspects progressively step by step. Particularly, DiffusionABSA gradually adds noise to the aspect terms in the training process, subsequently learning a denoising process that progressively restores these terms in a reverse manner. To estimate the boundaries, we design a denoising neural network enhanced by a syntax-aware temporal attention mechanism to chronologically capture the interplay between aspects and surrounding text. Empirical evaluations conducted on eight benchmark datasets underscore the compelling advantages of
    
[^62]: 光通信中基于ANN的均衡的实时FPGA演示

    Real-Time FPGA Demonstrator of ANN-Based Equalization for Optical Communications

    [https://arxiv.org/abs/2402.15288](https://arxiv.org/abs/2402.15288)

    该论文提出了一个实时执行光通信系统均衡的高吞吐量FPGA演示器，采用基于ANN的方法。

    

    在这项工作中，我们提出了一个高吞吐量的现场可编程门阵列（FPGA）演示器，用于基于人工神经网络（ANN）的均衡器。该均衡器实时执行并展示了在一个30 GBd，两级脉冲振幅调制（PAM2）光通信系统中的均衡。

    arXiv:2402.15288v1 Announce Type: cross  Abstract: In this work, we present a high-throughput field programmable gate array (FPGA) demonstrator of an artificial neural network (ANN)-based equalizer. The equalization is performed and illustrated in real-time for a 30 GBd, two-level pulse amplitude modulation (PAM2) optical communication system.
    
[^63]: 使用张量矩阵逼近哈密尔顿-雅各比-贝尔曼方程的生成建模

    Generative Modelling with Tensor Train approximations of Hamilton--Jacobi--Bellman equations

    [https://arxiv.org/abs/2402.15285](https://arxiv.org/abs/2402.15285)

    使用张量矩阵逼近哈密尔顿-雅各比-贝尔曼方程，提出了一种在生成建模中解决HJB方程的新方法，该方法无需样本，不依赖于归一化常数，并能避免维数灾难。

    

    从概率密度中进行采样在不确定性量化（UQ）和生成建模（GM）等领域中是一项常见挑战。 在GM中，特别流行的采样工具是依赖于Ornstein-Uhlenbeck正向过程的对数密度的逆时间扩散过程。 在Berner等人[2022]中，作者指出这些对数密度可以通过解决源自随机最优控制的哈密尔顿-雅各比-贝尔曼（HJB）方程来获得。 虽然这个HJB方程通常使用间接方法来处理，比如政策迭代和对神经网络这样的黑匣子架构进行无监督训练，我们提出通过直接时间积分来解决HJB方程，使用张量矩阵（TT）格式的压缩多项式进行空间离散化。 这种方法没有样本需求，不依赖于归一化常数，并且可以避免维数灾难。

    arXiv:2402.15285v1 Announce Type: cross  Abstract: Sampling from probability densities is a common challenge in fields such as Uncertainty Quantification (UQ) and Generative Modelling (GM). In GM in particular, the use of reverse-time diffusion processes depending on the log-densities of Ornstein-Uhlenbeck forward processes are a popular sampling tool. In Berner et al. [2022] the authors point out that these log-densities can be obtained by solution of a \textit{Hamilton-Jacobi-Bellman} (HJB) equation known from stochastic optimal control. While this HJB equation is usually treated with indirect methods such as policy iteration and unsupervised training of black-box architectures like Neural Networks, we propose instead to solve the HJB equation by direct time integration, using compressed polynomials represented in the Tensor Train (TT) format for spatial discretization. Crucially, this method is sample-free, agnostic to normalization constants and can avoid the curse of dimensionalit
    
[^64]: 高维数据预测学习的时空观察者设计

    Spatiotemporal Observer Design for Predictive Learning of High-Dimensional Data

    [https://arxiv.org/abs/2402.15284](https://arxiv.org/abs/2402.15284)

    本文设计了一个名为“时空观察者”的观察者理论引导的深度学习架构，为高维数据的预测学习提供了泛化误差界限和收敛保证，并引入了动态正则化以更好地学习系统动态。

    

    虽然基于深度学习的方法在时空预测学习中取得了巨大成功，但这些模型的框架主要是基于直觉设计的。如何进行具有理论保证的时空预测仍然是一个具有挑战性的问题。在这项工作中，我们通过将动态系统的领域知识应用于深度学习模型的框架设计，来解决这个问题。我们设计了一个名为“时空观察者”的观察者理论引导的深度学习架构，用于高维数据的预测学习。提出的框架的特点是双重的：首先，它为时空预测提供了泛化误差界限和收敛保证；其次，在训练过程中引入了动态正则化，使模型能够更好地学习系统动态。进一步的实验结果表明，该框架能够捕捉时空动态并进行准确预测。

    arXiv:2402.15284v1 Announce Type: cross  Abstract: Although deep learning-based methods have shown great success in spatiotemporal predictive learning, the framework of those models is designed mainly by intuition. How to make spatiotemporal forecasting with theoretical guarantees is still a challenging issue. In this work, we tackle this problem by applying domain knowledge from the dynamical system to the framework design of deep learning models. An observer theory-guided deep learning architecture, called Spatiotemporal Observer, is designed for predictive learning of high dimensional data. The characteristics of the proposed framework are twofold: firstly, it provides the generalization error bound and convergence guarantee for spatiotemporal prediction; secondly, dynamical regularization is introduced to enable the model to learn system dynamics better during training. Further experimental results show that this framework could capture the spatiotemporal dynamics and make accurate
    
[^65]: 犹豫时，要慢慢思考：具有潜在想象力的迭代推理

    When in Doubt, Think Slow: Iterative Reasoning with Latent Imagination

    [https://arxiv.org/abs/2402.15283](https://arxiv.org/abs/2402.15283)

    通过在决策时应用迭代推理来微调推断的代理状态，能够在视觉3D导航任务中取得一致的性能改进，并在部分可观察环境中得到更好的表现。

    

    在一个陌生的环境中，基于模型的强化学习代理可能会受到其世界模型准确性的限制。在这项工作中，我们提出了一种改进这类代理性能的新颖、无需训练的方法，与规划和学习分开。我们通过在决策时应用迭代推理来进行微调，以基于未来状态表示的连贯性来改进推断的代理状态。我们的方法在应用于视觉3D导航任务时，在重构精度和任务性能方面实现了一致的改进。我们进一步展示，在部分可观察环境中考虑更多的未来状态会进一步提高代理的性能，但在完全可观察的环境中不会。最后，我们证明训练预评估较少的代理从我们的方法中获益最多。

    arXiv:2402.15283v1 Announce Type: cross  Abstract: In an unfamiliar setting, a model-based reinforcement learning agent can be limited by the accuracy of its world model. In this work, we present a novel, training-free approach to improving the performance of such agents separately from planning and learning. We do so by applying iterative inference at decision-time, to fine-tune the inferred agent states based on the coherence of future state representations. Our approach achieves a consistent improvement in both reconstruction accuracy and task performance when applied to visual 3D navigation tasks. We go on to show that considering more future states further improves the performance of the agent in partially-observable environments, but not in a fully-observable one. Finally, we demonstrate that agents with less training pre-evaluation benefit most from our approach.
    
[^66]: 神经隐式扫描体模型用于快速碰撞检测

    Neural Implicit Swept Volume Models for Fast Collision Detection

    [https://arxiv.org/abs/2402.15281](https://arxiv.org/abs/2402.15281)

    提出了一种新颖的神经隐式扫描体模型，能够连续表示任意运动，并结合了深度学习速度和几何碰撞检查的准确性保证。

    

    碰撞检测是运动规划中最耗时的操作之一。因此，越来越多的人开始探索利用机器学习技术加速碰撞检测和基于采样的运动规划。最近的一系列研究侧重于利用机器人几何体或机器人运动的扫描体的神经有符号距离函数。在此基础上，我们提出了一种新颖的神经隐式扫描体模型，首次连续表示由起始和目标配置参数化的任意运动。这使得可以快速计算任务空间中任意点到机器人运动的有符号距离。此外，我们提出了一种算法，将基于深度学习的有符号距离计算的速度与几何碰撞检查的强大准确性保证相结合。我们在模拟和真实世界的机器人实验中验证了我们的方法，并证明了…

    arXiv:2402.15281v1 Announce Type: cross  Abstract: Collision detection is one of the most time-consuming operations during motion planning. Thus, there is an increasing interest in exploring machine learning techniques to speed up collision detection and sampling-based motion planning. A recent line of research focuses on utilizing neural signed distance functions of either the robot geometry or the swept volume of the robot motion. Building on this, we present a novel neural implicit swept volume model that is the first to continuously represent arbitrary motions parameterized by their start and goal configurations. This allows to quickly compute signed distances for any point in the task space to the robot motion. Further, we present an algorithm combining the speed of the deep learning-based signed distance computations with the strong accuracy guarantees of geometric collision checkers. We validate our approach in simulated and real-world robotic experiments, and demonstrate that i
    
[^67]: 在战略自我选择下的分类

    Classification Under Strategic Self-Selection

    [https://arxiv.org/abs/2402.15274](https://arxiv.org/abs/2402.15274)

    用户在学习分类器后决定是否参与的战略自我选择对学习效果和自我选择人口构成产生了影响，我们提出了一种可优化的可微分框架。

    

    当用户可以从某些预测中获益时，他们往往会采取战略行动以获得有利的预测结果。大多数关于战略分类的研究都考虑用户行为表现为特征修改，而我们研究了一种新颖的情况，在这种情况下，用户在学习分类器的情况下决定是否参与（或不参与）。为了学习增加战略意识的方法，我们研究了自我选择对学习的影响，以及学习对自我选择人口构成的影响。然后，我们提出了一个可在自我选择行为下学习的可微分框架，可以有效优化。我们最后通过对真实数据和模拟行为进行实验，这些实验既可以补充我们的分析，又可以展示我们方法的实用性。

    arXiv:2402.15274v1 Announce Type: new  Abstract: When users stand to gain from certain predictions, they are prone to act strategically to obtain favorable predictive outcomes. Whereas most works on strategic classification consider user actions that manifest as feature modifications, we study a novel setting in which users decide -- in response to the learned classifier -- whether to at all participate (or not). For learning approaches of increasing strategic awareness, we study the effects of self-selection on learning, and the implications of learning on the composition of the self-selected population. We then propose a differentiable framework for learning under self-selective behavior, which can be optimized effectively. We conclude with experiments on real data and simulated behavior that both complement our analysis and demonstrate the utility of our approach.
    
[^68]: 基于纳米无人机的视觉姿态估计的深度神经网络优化部署

    Optimized Deployment of Deep Neural Networks for Visual Pose Estimation on Nano-drones

    [https://arxiv.org/abs/2402.15273](https://arxiv.org/abs/2402.15273)

    提出了一种用于纳米无人机上视觉姿态估计任务的深度神经网络优化部署流水线，利用神经架构搜索算法进行探索，并通过新型软件内核在off-the-shelf纳米无人机上部署，将推断延迟缩短了3.22倍

    

    小型自主无人驾驶飞行器（UAVs）因其小巧的尺寸越来越受欢迎，能够执行诸如室内导航或人员监控等新任务。然而，它们的尺寸和简单的电子设备在实现先进的机载智能方面提出了严峻挑战。本文提出了一种新的用于利用深度神经网络（DNNs）进行视觉姿态估计任务的自动优化流水线。该流水线利用两种不同的神经架构搜索（NAS）算法，以在DNN结构空间中进行广泛的基于复杂性的探索。然后，获得的网络部署在一款配备并行超低功耗SoC的现成纳米无人机上，利用一组用于高效执行关键DNN层序列的新型软件内核。我们的结果改进了最新技术，将推断延迟在等误差情况下缩短了最多3.22倍。

    arXiv:2402.15273v1 Announce Type: cross  Abstract: Miniaturized autonomous unmanned aerial vehicles (UAVs) are gaining popularity due to their small size, enabling new tasks such as indoor navigation or people monitoring. Nonetheless, their size and simple electronics pose severe challenges in implementing advanced onboard intelligence. This work proposes a new automatic optimization pipeline for visual pose estimation tasks using Deep Neural Networks (DNNs). The pipeline leverages two different Neural Architecture Search (NAS) algorithms to pursue a vast complexity-driven exploration in the DNNs' architectural space. The obtained networks are then deployed on an off-the-shelf nano-drone equipped with a parallel ultra-low power System-on-Chip leveraging a set of novel software kernels for the efficient fused execution of critical DNN layer sequences. Our results improve the state-of-the-art reducing inference latency by up to 3.22x at iso-error.
    
[^69]: 通过无缝接近度整合的平滑图对比学习

    Smoothed Graph Contrastive Learning via Seamless Proximity Integration

    [https://arxiv.org/abs/2402.15270](https://arxiv.org/abs/2402.15270)

    SGCL模型通过三种不同的平滑技术调整对比损失中节点对的惩罚，从而形成具有接近度感知的正样本和负样本

    

    图对比学习（GCL）通过将节点对归类为正样本和负样本来对齐节点表示，其选择过程通常依赖于在两个增强图中建立对应关系。传统的GCL方法在对比损失中统一地融入负样本，导致负节点被平等对待，而不考虑它们与真正正样本的接近程度。本文提出了一种平滑图对比学习模型（SGCL），利用增强图的几何结构来在对比损失中注入与正负样本相关的接近度信息，从而显著规范化学习过程。所提出的SGCL通过整合三种不同的平滑技术调整对比损失中节点对的惩罚，形成了具有接近度感知的正样本和负样本。

    arXiv:2402.15270v1 Announce Type: cross  Abstract: Graph contrastive learning (GCL) aligns node representations by classifying node pairs into positives and negatives using a selection process that typically relies on establishing correspondences within two augmented graphs. The conventional GCL approaches incorporate negative samples uniformly in the contrastive loss, resulting in the equal treatment negative nodes, regardless of their proximity to the true positive. In this paper, we present a Smoothed Graph Contrastive Learning model (SGCL), which leverages the geometric structure of augmented graphs to inject proximity information associated with positive/negative pairs in the contrastive loss, thus significantly regularizing the learning process. The proposed SGCL adjusts the penalties associated with node pairs in the contrastive loss by incorporating three distinct smoothing techniques that result in proximity aware positives and negatives. To enhance scalability for large-scale
    
[^70]: MemoryPrompt: 一种改进预训练语言模型上下文跟踪的轻量封装方法

    MemoryPrompt: A Light Wrapper to Improve Context Tracking in Pre-trained Language Models

    [https://arxiv.org/abs/2402.15268](https://arxiv.org/abs/2402.15268)

    MemoryPrompt方法通过引入辅助循环网络，将信息传递给语言模型，从而改进了预训练语言模型在上下文跟踪方面的性能，避免了灾难性遗忘现象。

    

    基于Transformer的语言模型通过大型硬编码输入窗口跟踪上下文信息。我们引入MemoryPrompt，一种更精简的方法，其中语言模型由一个小的辅助循环网络补充，通过在其常规输入之前添加一系列向量（类似于软提示）将信息传递给语言模型，而无需需要对语言模型进行微调。在对一个旨在检测语言模型跟踪多个事实更新能力的任务上进行测试时，MemoryPrompt增强的语言模型优于那些可以访问完整输入历史记录的更大型语言模型。我们还在一个长距离对话数据集上测试了MemoryPrompt，在该数据集上，其性能与在整个对话历史记录上进行条件处理的模型相当。在这两个实验中，我们还观察到，与完全微调方法不同，MemoryPrompt在适应新任务时不会出现灾难性遗忘，因此不会破坏非专家能力。

    arXiv:2402.15268v1 Announce Type: cross  Abstract: Transformer-based language models (LMs) track contextual information through large, hard-coded input windows. We introduce MemoryPrompt, a leaner approach in which the LM is complemented by a small auxiliary recurrent network that passes information to the LM by prefixing its regular input with a sequence of vectors, akin to soft prompts, without requiring LM finetuning. Tested on a task designed to probe a LM's ability to keep track of multiple fact updates, a MemoryPrompt-augmented LM outperforms much larger LMs that have access to the full input history. We also test MemoryPrompt on a long-distance dialogue dataset, where its performance is comparable to that of a model conditioned on the entire conversation history. In both experiments we also observe that, unlike full-finetuning approaches, MemoryPrompt does not suffer from catastrophic forgetting when adapted to new tasks, thus not disrupting the generalist capabilities of the un
    
[^71]: fNIRS中深度学习分类模型的校准

    Calibration of Deep Learning Classification Models in fNIRS

    [https://arxiv.org/abs/2402.15266](https://arxiv.org/abs/2402.15266)

    在fNIRS领域，我们提出将校准整合到模型中以评估其可靠性，结果显示许多现有模型的校准性能不佳。

    

    功能性近红外光谱（fNIRS）是用于监测脑活动的宝贵无创工具。与意识活动相关的fNIRS数据分类对于推进我们对大脑的理解和促进脑机接口（BCI）的发展具有重要意义。许多研究人员转向深度学习来解决fNIRS数据中固有的分类挑战，因为它具有很强的泛化能力和鲁棒性。在fNIRS的应用中，可靠性非常重要，置信度可靠性的数学表达式之一就是校准。然而，许多研究人员忽视了校准这个重要问题。为了解决这一空白，我们提出了将校准融入fNIRS领域，并评估现有模型的可靠性。令人惊讶的是，我们的研究结果表明，许多提出的模型在校准性能方面表现不佳。为了推动fNIRS领域的校准发展，我们提出 ...

    arXiv:2402.15266v1 Announce Type: new  Abstract: Functional near-infrared spectroscopy (fNIRS) is a valuable non-invasive tool for monitoring brain activity. The classification of fNIRS data in relation to conscious activity holds significance for advancing our understanding of the brain and facilitating the development of brain-computer interfaces (BCI). Many researchers have turned to deep learning to tackle the classification challenges inherent in fNIRS data due to its strong generalization and robustness. In the application of fNIRS, reliability is really important, and one mathematical formulation of the reliability of confidence is calibration. However, many researchers overlook the important issue of calibration. To address this gap, we propose integrating calibration into fNIRS field and assess the reliability of existing models. Surprisingly, our results indicate poor calibration performance in many proposed models. To advance calibration development in the fNIRS field, we su
    
[^72]: 基于动态内存的自适应优化

    Dynamic Memory Based Adaptive Optimization

    [https://arxiv.org/abs/2402.15262](https://arxiv.org/abs/2402.15262)

    提出了一种称为“回顾式学习法律修正”的通用方法，用于计算内存单元的动态变化线性组合，能够在具有线性更新规则和小内存的优化器中取得优于经典优化器的性能。

    

    将优化器定义为在参数空间中存储$k$个动态变化向量的具有内存$k$的优化器。经典的SGD优化器具有内存$0$，动量SGD优化器具有$1$，Adam优化器具有$2$。本文探讨了以下问题：优化器如何利用更多内存单元？应该在其中存储哪些信息？如何将它们用于学习步骤？作为最后一个问题的方法，我们介绍了一种称为“回顾式学习法律修正”或简称RLLC的通用方法。该方法旨在计算内存单元的动态变化线性组合（称为学习法则），这些内存单元本身可能会任意演变。我们在内存单元具有线性更新规则和小内存（$\leq 4$内存单元）的优化器上展示了RLLC。我们的实验表明，在各种标准问题中，这些优化器表现优于上述三种经典优化器。我们得出结论，RLLC是一种有前途的方法。

    arXiv:2402.15262v1 Announce Type: cross  Abstract: Define an optimizer as having memory $k$ if it stores $k$ dynamically changing vectors in the parameter space. Classical SGD has memory $0$, momentum SGD optimizer has $1$ and Adam optimizer has $2$. We address the following questions: How can optimizers make use of more memory units? What information should be stored in them? How to use them for the learning steps? As an approach to the last question, we introduce a general method called "Retrospective Learning Law Correction" or shortly RLLC. This method is designed to calculate a dynamically varying linear combination (called learning law) of memory units, which themselves may evolve arbitrarily. We demonstrate RLLC on optimizers whose memory units have linear update rules and small memory ($\leq 4$ memory units). Our experiments show that in a variety of standard problems, these optimizers outperform the above mentioned three classical optimizers. We conclude that RLLC is a promisi
    
[^73]: 采用合作博弈论的开放式即兴团队合作

    Open Ad Hoc Teamwork with Cooperative Game Theory

    [https://arxiv.org/abs/2402.15259](https://arxiv.org/abs/2402.15259)

    提出了采用合作博弈论解释开放式即兴团队合作中联合Q值表示的新理论，为进一步发展这一研究方向和应用提供了新思路

    

    即兴团队合作面临着一个具有挑战性的问题，需要设计一个能够与队友协作但没有先前协调或联合训练的智能体。开放式即兴团队合作进一步复杂化了这一挑战，考虑了具有不断变化的队友数量的环境，即开放式团队。现有解决这一问题的最先进方法是基于图神经网络的策略学习（GPL），利用了图神经网络的泛化能力来处理无限数量的智能体，有效应对开放式团队。GPL的性能优于其他方法，但其联合Q值表示对解释造成了挑战，阻碍了进一步发展这一研究方向和应用。本文建立了一种新的理论，从合作博弈论的角度为GPL中采用的联合Q值表示提供了一种解释。基于我们的理论，我们提出了一种基于

    arXiv:2402.15259v1 Announce Type: cross  Abstract: Ad hoc teamwork poses a challenging problem, requiring the design of an agent to collaborate with teammates without prior coordination or joint training. Open ad hoc teamwork further complicates this challenge by considering environments with a changing number of teammates, referred to as open teams. The state-of-the-art solution to this problem is graph-based policy learning (GPL), leveraging the generalizability of graph neural networks to handle an unrestricted number of agents and effectively address open teams. GPL's performance is superior to other methods, but its joint Q-value representation presents challenges for interpretation, hindering further development of this research line and applicability. In this paper, we establish a new theory to give an interpretation for the joint Q-value representation employed in GPL, from the perspective of cooperative game theory. Building on our theory, we propose a novel algorithm based on
    
[^74]: 高分辨率吉他转录的域自适应

    High Resolution Guitar Transcription via Domain Adaptation

    [https://arxiv.org/abs/2402.15258](https://arxiv.org/abs/2402.15258)

    通过域自适应实现了高分辨率吉他转录的新方法，并在零样本情况下在GuitarSet上取得了最先进的转录结果。

    

    自动音乐转录（AMT）对于钢琴已经取得了很高的准确性，这得益于MAESTRO和MAPS等大型高质量数据集的可用性，但其他乐器的可比数据集尚不可用。最近的研究表明，将乐谱与转录模型的激活对准可以为除钢琴以外的乐器生成高质量的AMT训练数据。本文专注于吉他，对使用商用乐谱-音频对数据集进行训练的方法进行了改进。我们提出使用高分辨率钢琴转录模型来训练新的吉他转录模型。最终模型在零样本情况下在GuitarSet上获得了最先进的转录结果，优于先前发表的方法。

    arXiv:2402.15258v1 Announce Type: cross  Abstract: Automatic music transcription (AMT) has achieved high accuracy for piano due to the availability of large, high-quality datasets such as MAESTRO and MAPS, but comparable datasets are not yet available for other instruments. In recent work, however, it has been demonstrated that aligning scores to transcription model activations can produce high quality AMT training data for instruments other than piano. Focusing on the guitar, we refine this approach to training on score data using a dataset of commercially available score-audio pairs. We propose the use of a high-resolution piano transcription model to train a new guitar transcription model. The resulting model obtains state-of-the-art transcription results on GuitarSet in a zero-shot context, improving on previously published methods.
    
[^75]: 缺失数据下的结构学习的最优输运

    Optimal Transport for Structure Learning Under Missing Data

    [https://arxiv.org/abs/2402.15255](https://arxiv.org/abs/2402.15255)

    提出了一种基于最优输运的得分算法，用于从缺失数据中学习因果结构，通过将结构学习视为密度拟合问题，并通过最小化与观测数据分布之间的沃尔仑斯坦距离来找到导致观测数据分布的因果模型

    

    在缺失数据的情况下进行因果发现会引入鸡生蛋问题。虽然目标是恢复真实的因果结构，但鲁棒的插补需要考虑变量之间的依赖性或更好地因果关系。仅仅用现有的插补方法填充缺失值，然后在完整数据上应用结构学习被证明是次优的。为此，本文提出了一种基于最优输运的基于得分的算法，用于从缺失数据中学习因果结构。这种最优输运的观点不同于现有基于EM的基于得分方法。我们将结构学习投影为密度拟合问题，其目标是找到引起观测数据分布和观测数据之间的沃尔仑斯坦距离的因果模型。通过大量的模拟和实际数据实验，我们的框架...

    arXiv:2402.15255v1 Announce Type: cross  Abstract: Causal discovery in the presence of missing data introduces a chicken-and-egg dilemma. While the goal is to recover the true causal structure, robust imputation requires considering the dependencies or preferably causal relations among variables. Merely filling in missing values with existing imputation methods and subsequently applying structure learning on the complete data is empirical shown to be sub-optimal. To this end, we propose in this paper a score-based algorithm, based on optimal transport, for learning causal structure from missing data. This optimal transport viewpoint diverges from existing score-based approaches that are dominantly based on EM. We project structure learning as a density fitting problem, where the goal is to find the causal model that induces a distribution of minimum Wasserstein distance with the distribution over the observed data. Through extensive simulations and real-data experiments, our framework 
    
[^76]: 一种基于谈判的垂直联邦学习特征交易方法

    A Bargaining-based Approach for Feature Trading in Vertical Federated Learning

    [https://arxiv.org/abs/2402.15247](https://arxiv.org/abs/2402.15247)

    该研究提出了一种基于谈判的垂直联邦学习特征交易方法，以促进经济高效的交易

    

    垂直联邦学习（VFL）已经成为一种流行的机器学习范式，可以在保护数据隐私的同时，实现跨数据和任务方对相同用户集的模型训练。在生产环境中，VFL通常涉及一个任务方和一个数据方。公平和经济高效的特征交易对VFL的商业化至关重要，其中任务方被视为购买数据方特征的数据消费者。然而，当前的VFL特征交易做法通常将数据方的数据整体定价，并假定交易发生在执行VFL之前。忽略交易特征产生的性能增益可能导致不公平支付和过度支付问题。在本研究中，我们提出了一种基于谈判的VFL特征交易方法，以促进经济高效的交易。我们的模型结合了基于性能增益的定价，

    arXiv:2402.15247v1 Announce Type: cross  Abstract: Vertical Federated Learning (VFL) has emerged as a popular machine learning paradigm, enabling model training across the data and the task parties with different features about the same user set while preserving data privacy. In production environment, VFL usually involves one task party and one data party. Fair and economically efficient feature trading is crucial to the commercialization of VFL, where the task party is considered as the data consumer who buys the data party's features. However, current VFL feature trading practices often price the data party's data as a whole and assume transactions occur prior to the performing VFL. Neglecting the performance gains resulting from traded features may lead to underpayment and overpayment issues. In this study, we propose a bargaining-based feature trading approach in VFL to encourage economically efficient transactions. Our model incorporates performance gain-based pricing, taking int
    
[^77]: GS-EMA：将梯度手术指数移动平均与边界感知对比学习相结合，以增强在动脉瘤分割中的领域泛化

    GS-EMA: Integrating Gradient Surgery Exponential Moving Average with Boundary-Aware Contrastive Learning for Enhanced Domain Generalization in Aneurysm Segmentation

    [https://arxiv.org/abs/2402.15239](https://arxiv.org/abs/2402.15239)

    提出了一种新颖的领域泛化策略，结合梯度手术指数移动平均（GS-EMA）优化技术和边界感知对比学习（BACL），提高了动脉瘤分割的鲁棒性和准确性

    

    自动分割脑动脉瘤对于准确诊断和治疗规划至关重要。在来自各种医疗机构的三维旋转血管造影（3DRA）数据中，面临着显著的领域转移和类别不平衡，这使得任务变得具有挑战性。为了解决这些问题，我们提出了一种新颖的领域泛化策略，采用梯度手术指数移动平均（GS-EMA）优化技术与边界感知对比学习（BACL）相结合。我们的方法在于能够适应新的、看不见的领域，通过学习领域不变的特征，从而提高在各种临床数据集中的动脉瘤分割的鲁棒性和准确性。结果表明，我们提出的方法可以提取更多的领域不变特征。

    arXiv:2402.15239v1 Announce Type: cross  Abstract: The automated segmentation of cerebral aneurysms is pivotal for accurate diagnosis and treatment planning. Confronted with significant domain shifts and class imbalance in 3D Rotational Angiography (3DRA) data from various medical institutions, the task becomes challenging. These shifts include differences in image appearance, intensity distribution, resolution, and aneurysm size, all of which complicate the segmentation process. To tackle these issues, we propose a novel domain generalization strategy that employs gradient surgery exponential moving average (GS-EMA) optimization technique coupled with boundary-aware contrastive learning (BACL). Our approach is distinct in its ability to adapt to new, unseen domains by learning domain-invariant features, thereby improving the robustness and accuracy of aneurysm segmentation across diverse clinical datasets. The results demonstrate that our proposed approach can extract more domain-inva
    
[^78]: 通过Transwarp对比学习进行脑血管分割的无监督领域自适应

    Unsupervised Domain Adaptation for Brain Vessel Segmentation through Transwarp Contrastive Learning

    [https://arxiv.org/abs/2402.15237](https://arxiv.org/abs/2402.15237)

    本文提出了一个简单而强大的对比学习框架，用于无监督领域自适应，可以改善医学影像分析中脑血管分割的性能。

    

    无监督领域自适应（UDA）旨在将标记源分布与未标记目标分布对齐，以获得领域不变的预测模型。本文提出了一个简单而强大的对比学习框架，用于UDA，以缩小标记源和未标记目标分布之间的领域间差距。我们的方法在脑血管数据集上得到验证。实验结果表明，我们的方法可以从标记的3DRA模态数据中学习潜在特征，并改善未标记的MRA模态数据中的血管分割性能。

    arXiv:2402.15237v1 Announce Type: cross  Abstract: Unsupervised domain adaptation (UDA) aims to align the labelled source distribution with the unlabelled target distribution to obtain domain-invariant predictive models. Since cross-modality medical data exhibit significant intra and inter-domain shifts and most are unlabelled, UDA is more important while challenging in medical image analysis. This paper proposes a simple yet potent contrastive learning framework for UDA to narrow the inter-domain gap between labelled source and unlabelled target distribution. Our method is validated on cerebral vessel datasets. Experimental results show that our approach can learn latent features from labelled 3DRA modality data and improve vessel segmentation performance in unlabelled MRA modality data.
    
[^79]: 用监督机器学习对银河平面的紧凑射电源进行分类

    Classification of compact radio sources in the Galactic plane with supervised machine learning

    [https://arxiv.org/abs/2402.15232](https://arxiv.org/abs/2402.15232)

    本研究使用射电和红外图像，结合20,000张图像数据集进行监督机器学习，实现对银河平面中紧凑射电源的分类。

    

    从处理过的数据产品中生成科学准备就绪的数据是未来Square Kilometre Array（SKA）及其前身射电连续波普查面临的主要挑战之一，这是由于预期的数据量和实现高度自动化处理的需求。本文着重于使用射电和红外图像对银河平面中的紧凑射电源进行分类。为此，我们产生了一个包含约20,000张来自过去射电和红外调查以及使用澳大利亚SKA Pathfinder（ASKAP）进行的试验调查的不同天文类别的紧凑源图像的策划数据集。还获得了部分数据的射电谱指数信息。然后在产生的数据集上训练了两个不同的分类器。

    arXiv:2402.15232v1 Announce Type: cross  Abstract: Generation of science-ready data from processed data products is one of the major challenges in next-generation radio continuum surveys with the Square Kilometre Array (SKA) and its precursors, due to the expected data volume and the need to achieve a high degree of automated processing. Source extraction, characterization, and classification are the major stages involved in this process. In this work we focus on the classification of compact radio sources in the Galactic plane using both radio and infrared images as inputs. To this aim, we produced a curated dataset of ~20,000 images of compact sources of different astronomical classes, obtained from past radio and infrared surveys, and novel radio data from pilot surveys carried out with the Australian SKA Pathfinder (ASKAP). Radio spectral index information was also obtained for a subset of the data. We then trained two different classifiers on the produced dataset. The first model 
    
[^80]: 需要转移哪种模型？关于转移性评估的调查

    Which Model to Transfer? A Survey on Transferability Estimation

    [https://arxiv.org/abs/2402.15231](https://arxiv.org/abs/2402.15231)

    转移学习方法关注利用源预训练模型或数据解决目标任务，模型可转移性评估旨在提出适用度量标准，这项研究对该领域进展进行了综述并分类为两种模式。

    

    转移学习方法致力于利用现有源预训练模型或数据集中的相关知识来解决下游目标任务。随着当前可用预训练模型的规模和数量的增加，事先评估它们是否适用于特定目标任务变得至关重要。模型可转移性评估是一个新兴且不断发展的领域，旨在提出一个度量标准来量化这种适用性，而无需对其进行单独训练，这在计算上是不可行的。尽管最近已经有很多对这一领域的广泛研究进展，但它们都有自定义术语定义和实验设置。在这项调查中，我们首次对该领域的现有进展进行了综述，并将其分类为两个不同领域：无源模型可转移性评估和有源依赖模型可转移性评估。每个类别都经过系统定义，

    arXiv:2402.15231v1 Announce Type: new  Abstract: Transfer learning methods endeavor to leverage relevant knowledge from existing source pre-trained models or datasets to solve downstream target tasks. With the increase in the scale and quantity of available pre-trained models nowadays, it becomes critical to assess in advance whether they are suitable for a specific target task. Model transferability estimation is an emerging and growing area of interest, aiming to propose a metric to quantify this suitability without training them individually, which is computationally prohibitive. Despite extensive recent advances already devoted to this area, they have custom terminological definitions and experimental settings. In this survey, we present the first review of existing advances in this area and categorize them into two separate realms: source-free model transferability estimation and source-dependent model transferability estimation. Each category is systematically defined, accompanie
    
[^81]: 固定随机分类器重排在持续学习中的应用

    Fixed Random Classifier Rearrangement for Continual Learning

    [https://arxiv.org/abs/2402.15227](https://arxiv.org/abs/2402.15227)

    提出了一种名为固定随机分类器重排（FRCR）的两阶段持续学习算法，通过替换可学习的分类器为固定的随机分类器，在不影响网络性能的情况下，约束了等价的单分类器的范数。

    

    随着数据的爆炸性增长，神经网络的持续学习能力变得越来越重要。由于灾难性遗忘，神经网络在学习新任务后不可避免地会忘记旧任务的知识。在视觉分类场景中，缓解遗忘的常见做法是限制主干网络，然而分类器的影响被低估了。本文分析了模型在顺序二元分类任务中的预测变化，并发现等价单分类器的范数显著影响遗忘水平。基于这一结论，我们提出了一个名为固定随机分类器重排（Fixed Random Classifier Rearrangement，FRCR）的两阶段持续学习算法。在第一阶段，FRCR用固定的随机分类器替换可学习的分类器，约束了等价的单分类器的范数，而不影响网络的性能。

    arXiv:2402.15227v1 Announce Type: cross  Abstract: With the explosive growth of data, continual learning capability is increasingly important for neural networks. Due to catastrophic forgetting, neural networks inevitably forget the knowledge of old tasks after learning new ones. In visual classification scenario, a common practice of alleviating the forgetting is to constrain the backbone. However, the impact of classifiers is underestimated. In this paper, we analyze the variation of model predictions in sequential binary classification tasks and find that the norm of the equivalent one-class classifiers significantly affects the forgetting level. Based on this conclusion, we propose a two-stage continual learning algorithm named Fixed Random Classifier Rearrangement (FRCR). In first stage, FRCR replaces the learnable classifiers with fixed random classifiers, constraining the norm of the equivalent one-class classifiers without affecting the performance of the network. In second sta
    
[^82]: ChunkAttention: 具有前缀感知KV缓存和两阶段分区的高效自注意力

    ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition

    [https://arxiv.org/abs/2402.15220](https://arxiv.org/abs/2402.15220)

    ChunkAttention是一种前缀感知的自注意力模块，通过将键/值张量分解为较小的块并结构化到辅助前缀树中，实现了在运行时改善内存利用率的KV缓存，同时设计了两阶段分区算法以提高自注意力计算中的数据局部性。

    

    自注意力是大型语言模型（LLMs）的重要组成部分，但对于长序列来说是推理延迟的一个显著来源。在多租户LLMs服务场景中，通过利用多个LLM请求在前缀中共享系统提示的概率，可以优化自注意力的计算和内存操作成本。本文介绍了ChunkAttention，一种具有前缀感知的自注意力模块，可以在运行时检测多个请求之间匹配的提示前缀，并共享它们的键/值张量以改进KV缓存的内存利用率。这是通过将整体键/值张量分解为较小的块，并将它们结构化到辅助前缀树中来实现的。因此，在基于前缀树的KV缓存之上，我们设计了一个高效的自注意力内核，其中实现了两阶段分区算法，以改善自注意力计算中的数据局部性。

    arXiv:2402.15220v1 Announce Type: cross  Abstract: Self-attention is an essential component of large language models(LLMs) but a significant source of inference latency for long sequences. In multi-tenant LLMs serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the p
    
[^83]: 统计无偏回归：一种用于验证回归模型的机器学习方法

    Statistical Agnostic Regression: a machine learning method to validate regression models

    [https://arxiv.org/abs/2402.15213](https://arxiv.org/abs/2402.15213)

    本文提出了一种新的方法，统计无关地评估了线性回归模型，并评估了ML估计在检测方面的表现。

    

    回归分析是统计建模中的一个核心主题，旨在估计因变量（通常称为响应变量）与一个或多个自变量（即解释变量）之间的关系。线性回归是迄今为止在预测、预测或因果推断等多个研究领域执行此任务的最流行方法。除了解决线性回归问题的各种传统方法外，如普通最小二乘法、岭回归或套索回归——这些方法往往是更高级机器学习（ML）技术的基础——后者已成功地应用在这种场景中，但没有对统计显著性进行正式定义。最多，基于经验测量（如残差或准确度）进行置换或基于经典分析，以反映ML估计对检测的更高能力。本文介绍了一种新的方法，该方法统计无关地评估了线性回归模型，并对ML估计在检测方面的表现进行了评估。

    arXiv:2402.15213v1 Announce Type: cross  Abstract: Regression analysis is a central topic in statistical modeling, aiming to estimate the relationships between a dependent variable, commonly referred to as the response variable, and one or more independent variables, i.e., explanatory variables. Linear regression is by far the most popular method for performing this task in several fields of research, such as prediction, forecasting, or causal inference. Beyond various classical methods to solve linear regression problems, such as Ordinary Least Squares, Ridge, or Lasso regressions - which are often the foundation for more advanced machine learning (ML) techniques - the latter have been successfully applied in this scenario without a formal definition of statistical significance. At most, permutation or classical analyses based on empirical measures (e.g., residuals or accuracy) have been conducted to reflect the greater ability of ML estimations for detection. In this paper, we introd
    
[^84]: 开放集注释的双向不确定性主动学习

    Bidirectional Uncertainty-Based Active Learning for Open Set Annotation

    [https://arxiv.org/abs/2402.15198](https://arxiv.org/abs/2402.15198)

    本文提出了一种双向不确定性主动学习（BUAL）框架，通过随机标签负学习方法和双向不确定性采样策略，旨在同时筛选既可能属于已知类别又高度信息丰富的示例，解决了在开放集场景下确定最有价值示例的挑战。

    

    开放集场景下的主动学习(AL)面临着一个新挑战，即确定在一个包含已知和未知类别数据的未标记数据池中识别最有价值的示例。传统方法优先选择置信度较低的信息量丰富的示例，存在误选置信度同样较低的未知类别示例的风险。最近的方法更青睐最有可能属于已知类别的示例，但存在选取已经掌握的简单示例的风险。在本文中，我们尝试查询既有可能来自已知类别又高度信息丰富的示例，并提出了一种\textit{双向不确定性主动学习}（BUAL）框架。具体来说，我们首先通过我们提出的\textit{随机标签负学习}方法将未知类别示例推向高置信度预测区域。然后，我们提出一种\textit{双向不确定性采样}策略...

    arXiv:2402.15198v1 Announce Type: new  Abstract: Active learning (AL) in open set scenarios presents a novel challenge of identifying the most valuable examples in an unlabeled data pool that comprises data from both known and unknown classes. Traditional methods prioritize selecting informative examples with low confidence, with the risk of mistakenly selecting unknown-class examples with similarly low confidence. Recent methods favor the most probable known-class examples, with the risk of picking simple already mastered examples. In this paper, we attempt to query examples that are both likely from known classes and highly informative, and propose a \textit{Bidirectional Uncertainty-based Active Learning} (BUAL) framework. Specifically, we achieve this by first pushing the unknown class examples toward regions with high-confidence predictions with our proposed \textit{Random Label Negative Learning} method. Then, we propose a \textit{Bidirectional Uncertainty sampling} strategy by j
    
[^85]: 安全优化的多目标策略优化强化学习

    Safety Optimized Reinforcement Learning via Multi-Objective Policy Optimization

    [https://arxiv.org/abs/2402.15197](https://arxiv.org/abs/2402.15197)

    本文提出了一种基于多目标策略优化框架的安全强化学习算法，通过安全评论家塑造环境奖励函数，使得策略可以同时朝着最优性和安全性优化，相较于传统方法，该算法无需约束策略搜索空间，实现了安全性和最优性之间的自然权衡。

    

    安全强化学习（安全RL）指的是一类技术，旨在防止RL算法在试错决策和探索过程中违反约束。本文介绍了一种基于多目标策略优化框架制定的新型无模型安全RL算法，其中策略同时朝着最优性和安全性进行优化。通过使用安全评论家来塑造环境奖励函数，从而实现最优性。相较于传统安全RL算法，安全优化RL（SORL）算法的优势在于省略了对策略搜索空间的约束需要。这使得SORL能够在不受严格搜索空间约束的情况下找到安全性和最优性之间的自然权衡，而无需因严格搜索空间约束而在安全性或最优性方面性能受损。通过对SORL的理论分析，我们提出了一种co

    arXiv:2402.15197v1 Announce Type: cross  Abstract: Safe reinforcement learning (Safe RL) refers to a class of techniques that aim to prevent RL algorithms from violating constraints in the process of decision-making and exploration during trial and error. In this paper, a novel model-free Safe RL algorithm, formulated based on the multi-objective policy optimization framework is introduced where the policy is optimized towards optimality and safety, simultaneously. The optimality is achieved by the environment reward function that is subsequently shaped using a safety critic. The advantage of the Safety Optimized RL (SORL) algorithm compared to the traditional Safe RL algorithms is that it omits the need to constrain the policy search space. This allows SORL to find a natural tradeoff between safety and optimality without compromising the performance in terms of either safety or optimality due to strict search space constraints. Through our theoretical analysis of SORL, we propose a co
    
[^86]: AffectToolbox：每个人的情感分析

    The AffectToolbox: Affect Analysis for Everyone

    [https://arxiv.org/abs/2402.15195](https://arxiv.org/abs/2402.15195)

    AffectToolbox是一个新型软件系统，旨在为开发情感敏感研究和原型的研究人员提供支持，无需编程知识即可可靠分析用户情感状态，实现多模情感识别和融合评估。

    

    在情感计算领域，研究不断以快速的速度前进，用户对用户友好工具的需求变得越来越显而易见。在本文中，我们介绍了AffectToolbox，这是一个旨在支持研究人员开发情感敏感研究和原型的新型软件系统。该系统旨在解决现有框架所提出的挑战，这些框架通常需要深入的编程知识，并主要面向高级用户或熟练开发人员。为了促进易用性，AffectToolbox不需要编程知识，并通过易于访问的图形用户界面提供其功能，可可靠地分析用户的情感状态。其架构涵盖了多种模型，用于在多种情感渠道和情感方式上进行情绪识别，以及一个复杂的融合系统，将多模态评估合并为统一结果。整个系统是公开的

    arXiv:2402.15195v1 Announce Type: cross  Abstract: In the field of affective computing, where research continually advances at a rapid pace, the demand for user-friendly tools has become increasingly apparent. In this paper, we present the AffectToolbox, a novel software system that aims to support researchers in developing affect-sensitive studies and prototypes. The proposed system addresses the challenges posed by existing frameworks, which often require profound programming knowledge and cater primarily to power-users or skilled developers. Aiming to facilitate ease of use, the AffectToolbox requires no programming knowledge and offers its functionality to reliably analyze the affective state of users through an accessible graphical user interface. The architecture encompasses a variety of models for emotion recognition on multiple affective channels and modalities, as well as an elaborate fusion system to merge multi-modal assessments into a unified result. The entire system is op
    
[^87]: 连续时间扩散模型的微调作为熵正则化控制

    Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized Control

    [https://arxiv.org/abs/2402.15194](https://arxiv.org/abs/2402.15194)

    扩散模型的微调方法可以通过最大化奖励函数的价值来以目标导向方式进行微调，但可能会面临奖励崩溃的挑战。

    

    扩散模型在捕捉复杂数据分布方面表现出色，例如自然图像和蛋白质的分布。虽然扩散模型经过训练可代表训练数据集中的分布，但我们通常更关注其他属性，例如生成图像的美学质量或生成蛋白质的功能属性。扩散模型可以通过最大化某些奖励函数的价值（例如图像的美学质量）以目标导向的方式进行微调。然而，这些方法可能会导致样本多样性减少，与训练数据分布出现显著偏差，甚至由于利用不完美的奖励函数而导致样本质量较差。在许多实际应用中奖励函数是用于近似真实“真实”奖励的学习模型时，最后一个问题经常会产生。这些挑战总称为“奖励崩溃”。

    arXiv:2402.15194v1 Announce Type: cross  Abstract: Diffusion models excel at capturing complex data distributions, such as those of natural images and proteins. While diffusion models are trained to represent the distribution in the training dataset, we often are more concerned with other properties, such as the aesthetic quality of the generated images or the functional properties of generated proteins. Diffusion models can be finetuned in a goal-directed way by maximizing the value of some reward function (e.g., the aesthetic quality of an image). However, these approaches may lead to reduced sample diversity, significant deviations from the training data distribution, and even poor sample quality due to the exploitation of an imperfect reward function. The last issue often occurs when the reward function is a learned model meant to approximate a ground-truth "genuine" reward, as is the case in many practical applications. These challenges, collectively termed "reward collapse," pose
    
[^88]: 将生物医学实体链接视为多项选择问答

    Biomedical Entity Linking as Multiple Choice Question Answering

    [https://arxiv.org/abs/2402.15189](https://arxiv.org/abs/2402.15189)

    提出了一种新颖的模型BioELQA，将生物医学实体链接看作是多项选择问答，通过使用快速检索器获得候选实体，实现了更好的实体链接效果。

    

    尽管预训练语言模型在生物医学实体链接（BioEL）方面取得了显著进展，但对于细粒度和长尾实体仍然存在挑战。为了解决这些挑战，我们提出了BioELQA，这是一种将生物医学实体链接视为多项选择问答的新颖模型。BioELQA首先利用快速检索器获得候选实体，将提及和候选实体共同呈现给生成器，然后输出与其选定实体相关的预测符号。这种公式使得不同候选实体之间的明确比较成为可能，从而捕捉了提及和实体之间以及实体之间的精细交互。为了改善长尾实体的泛化能力，我们检索相似的已标记训练实例作为线索，并将输入与检索实例连接到生成器。广泛的实验结果表明，BioELQA的表现优于统计结果。

    arXiv:2402.15189v1 Announce Type: cross  Abstract: Although biomedical entity linking (BioEL) has made significant progress with pre-trained language models, challenges still exist for fine-grained and long-tailed entities. To address these challenges, we present BioELQA, a novel model that treats Biomedical Entity Linking as Multiple Choice Question Answering. BioELQA first obtains candidate entities with a fast retriever, jointly presents the mention and candidate entities to a generator, and then outputs the predicted symbol associated with its chosen entity. This formulation enables explicit comparison of different candidate entities, thus capturing fine-grained interactions between mentions and entities, as well as among entities themselves. To improve generalization for long-tailed entities, we retrieve similar labeled training instances as clues and concatenate the input with retrieved instances for the generator. Extensive experimental results show that BioELQA outperforms stat
    
[^89]: 无参数算法在决策相关分布下进行执行风险最小化

    Parameter-Free Algorithms for Performative Regret Minimization under Decision-Dependent Distributions

    [https://arxiv.org/abs/2402.15188](https://arxiv.org/abs/2402.15188)

    该论文提出了一种针对决策相关分布下的执行风险最小化问题的无参数乐观优化方法，显著改进了利普希茨自助式方法，实验结果显示该方法在数值上的优越性。

    

    这篇论文研究了执行风险最小化，即在决策相关分布下的随机优化的一种形式。我们考虑了执行风险可能是非凸的一般情况，针对这种情况我们开发了高效的无参数乐观优化方法。我们的算法在许多方面显著改进了现有的利普希茨自助式方法。具体来说，我们的框架不需要对分布映射的敏感性参数和损失函数的利普希茨常数进行了解。这使得我们的框架在实践中具有优势，同时具有有效的基于乐观优化的树搜索机制。我们提供了实验结果，证明了我们的算法在数值上优于现有方法和其他黑盒乐观优化方法。

    arXiv:2402.15188v1 Announce Type: new  Abstract: This paper studies performative risk minimization, a formulation of stochastic optimization under decision-dependent distributions. We consider the general case where the performative risk can be non-convex, for which we develop efficient parameter-free optimistic optimization-based methods. Our algorithms significantly improve upon the existing Lipschitz bandit-based method in many aspects. In particular, our framework does not require knowledge about the sensitivity parameter of the distribution map and the Lipshitz constant of the loss function. This makes our framework practically favorable, together with the efficient optimistic optimization-based tree-search mechanism. We provide experimental results that demonstrate the numerical superiority of our algorithms over the existing method and other black-box optimistic optimization methods.
    
[^90]: GraphEdit：用于图结构学习的大型语言模型

    GraphEdit: Large Language Models for Graph Structure Learning

    [https://arxiv.org/abs/2402.15183](https://arxiv.org/abs/2402.15183)

    本研究提出了一种名为GraphEdit的方法，利用大型语言模型（LLMs）学习复杂的图结构化数据中的节点关系，通过在图结构上进行指导调整，增强LLMs的推理能力，从而提高图结构学习的可靠性。

    

    图结构学习（GSL）致力于通过生成新颖的图结构来捕捉图结构数据中节点之间的固有依赖性和相互作用。本文提出了一种名为GraphEdit的方法，利用大型语言模型（LLMs）学习图结构化数据中复杂的节点关系。通过在图结构上进行指导调整，增强LLMs的推理能力，我们旨在克服显式图结构信息带来的挑战，并提高图结构学习的可靠性。

    arXiv:2402.15183v1 Announce Type: cross  Abstract: Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies and interactions among nodes in graph-structured data by generating novel graph structures. Graph Neural Networks (GNNs) have emerged as promising GSL solutions, utilizing recursive message passing to encode node-wise inter-dependencies. However, many existing GSL methods heavily depend on explicit graph structural information as supervision signals, leaving them susceptible to challenges such as data noise and sparsity. In this work, we propose GraphEdit, an approach that leverages large language models (LLMs) to learn complex node relationships in graph-structured data. By enhancing the reasoning capabilities of LLMs through instruction-tuning over graph structures, we aim to overcome the limitations associated with explicit graph structural information and enhance the reliability of graph structure learning. Our approach not only effectively denoises noisy co
    
[^91]: 打破Breakout: 用自我完善重新定义LM对抗越狱攻击的防御

    Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement

    [https://arxiv.org/abs/2402.15180](https://arxiv.org/abs/2402.15180)

    提出了一种通过自我完善和格式化改进LMs对抗越狱攻击的方法，即使在非安全对齐的LMs中也具有出色的安全性，同时降低攻击成功率。

    

    警告：本文包含可能引起不快的冒犯性词语。语言模型（LMs）容易被利用进行恶意滥用。对LM进行安全对齐的训练非常复杂，使得难以立即应对快速发展的攻击，如越狱攻击。我们提出了一种通过格式自我完善的方法，即使在非安全对齐的LMs中也能实现出色的安全性，并将我们的方法与几种防御基线进行评估，表明这是针对越狱攻击最安全的无训练方法。此外，我们提出了一种改进自我完善过程效率的格式化方法，同时在较少迭代中降低攻击成功率。我们还观察到非安全对齐的LM在安全任务中表现优于安全对齐的LM，因为它们给出更有用且更安全的回复。总之，我们的发现能够在较少的计算成本下实现更少的安全风险。

    arXiv:2402.15180v1 Announce Type: cross  Abstract: Caution: This paper includes offensive words that could potentially cause unpleasantness. Language models (LMs) are vulnerable to exploitation for adversarial misuse. Training LMs for safety alignment is extensive and makes it hard to respond to fast-developing attacks immediately, such as jailbreaks. We propose self-refine with formatting that achieves outstanding safety even in non-safety-aligned LMs and evaluate our method alongside several defense baselines, demonstrating that it is the safest training-free method against jailbreak attacks. Additionally, we proposed a formatting method that improves the efficiency of the self-refine process while reducing attack success rates in fewer iterations. We've also observed that non-safety-aligned LMs outperform safety-aligned LMs in safety tasks by giving more helpful and safe responses. In conclusion, our findings can achieve less safety risk with fewer computational costs, allowing non-
    
[^92]: 通过表示编辑推进微调中的参数效率

    Advancing Parameter Efficiency in Fine-tuning via Representation Editing

    [https://arxiv.org/abs/2402.15179](https://arxiv.org/abs/2402.15179)

    RED通过表示编辑显著降低了可训练参数数量，实现了与完全参数微调和其他PEFT方法相当或更好的结果

    

    参数有效微调（PEFT）因其能够在仅更新可训练参数的一个小子集时达到竞争性结果而受到了重视。在解决这些挑战问题中，我们提出了一种新颖的微调神经模型的方法，称为表示编辑（RED），其扩放和偏置每一层产生的表示。与完全参数微调相比，RED将可训练参数数量降低了$25,700$倍，并与LoRA相比降低了32倍。值得注意的是，RED实现了与完全参数微调和其他PEFT方法相当或更好的结果。对不同架构和规模的模型进行了大量实验。

    arXiv:2402.15179v1 Announce Type: cross  Abstract: Parameter Efficient Fine-Tuning (PEFT) has gained significant attention for its ability to achieve competitive results while updating only a small subset of trainable parameters. Despite the promising performance of current PEFT methods, they present challenges in hyperparameter selection, such as determining the rank of LoRA or Adapter, or specifying the length of soft prompts. In addressing these challenges, we propose a novel approach to fine-tuning neural models, termed Representation EDiting (RED), which scales and biases the representation produced at each layer. RED substantially reduces the number of trainable parameters by a factor of $25,700$ compared to full parameter fine-tuning, and by a factor of $32$ compared to LoRA. Remarkably, RED achieves comparable or superior results to full parameter fine-tuning and other PEFT methods. Extensive experiments were conducted across models of varying architectures and scales, includin
    
[^93]: 统一理解Grokking、双降和新兴能力：来自电路竞争视角的观点

    Unified View of Grokking, Double Descent and Emergent Abilities: A Perspective from Circuits Competition

    [https://arxiv.org/abs/2402.15175](https://arxiv.org/abs/2402.15175)

    提供了一个全面框架，统一解释了Grokking、双重下降和新兴能力这三种现象，着重探讨了记忆和泛化电路之间的竞争。

    

    最近的研究揭示了深度学习中一些有趣的现象，如Grokking、双重下降以及大型语言模型中的新兴能力，这些挑战了人类的直觉，并对神经模型的更深入理解至关重要。在本文中，我们提出了一个全面的框架，提供了对这三种现象的统一观点，重点关注记忆和泛化电路之间的竞争。这种方法最初用于解释Grokking，我们在工作中将其扩展到了更广泛的模型大小和训练数据量。我们的框架勾勒出了四种不同的训练动态，每种都取决于模型大小和训练数据数量的不同组合。利用这一框架，我们对双重下降现象进行了详细分析，并提出了两个关于其发生的可验证预测，均得到我们实验结果的支持。此外，我们扩展了我们的框架

    arXiv:2402.15175v1 Announce Type: new  Abstract: Recent studies have uncovered intriguing phenomena in deep learning, such as grokking, double descent, and emergent abilities in large language models, which challenge human intuition and are crucial for a deeper understanding of neural models. In this paper, we present a comprehensive framework that provides a unified view of these three phenomena, focusing on the competition between memorization and generalization circuits. This approach, initially employed to explain grokking, is extended in our work to encompass a wider range of model sizes and training data volumes. Our framework delineates four distinct training dynamics, each depending on varying combinations of model size and training data quantity. Utilizing this framework, we provide a detailed analysis of the double descent phenomenon and propose two verifiable predictions regarding its occurrence, both substantiated by our experimental results. Moreover, we expand our framewo
    
[^94]: 无痛人工大语言模型的二阶微调：一种基于Hessian信息的零阶优化器

    Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed Zeroth-Order Optimizer

    [https://arxiv.org/abs/2402.15173](https://arxiv.org/abs/2402.15173)

    提出了HiZOO，一种对角Hessian信息的零阶优化器，以增强LLMs微调过程中的模型收敛速度和准确性

    

    通过背向传播过程对大型语言模型（LLMs）进行微调，通常需要昂贵的GPU内存。最近的研究转向使用零阶优化器进行微调，通过两次前向传递显著节省内存。然而，这些优化器受不同维度之间参数曲率的异质性困扰。在这项工作中，我们提出了HiZOO，一种对角Hessian信息的零阶优化器，这是第一项利用对角Hessian增强零阶优化器进行LLMs微调的工作。HiZOO避免了昂贵的内存成本，并且每步只增加了一个前向传递。对各种模型（350M〜66B参数）进行的大量实验表明，HiZOO提高了模型收敛速度，显著减少了训练步骤，并有效提高了模型准确性。此外，我们可视化了HiZOO在测试函数上的优化轨迹，

    arXiv:2402.15173v1 Announce Type: new  Abstract: Fine-tuning large language models (LLMs) with classic first-order optimizers entails prohibitive GPU memory due to the backpropagation process. Recent works have turned to zeroth-order optimizers for fine-tuning, which save substantial memory by using two forward passes. However, these optimizers are plagued by the heterogeneity of parameter curvatures across different dimensions. In this work, we propose HiZOO, a diagonal Hessian informed zeroth-order optimizer which is the first work to leverage the diagonal Hessian to enhance zeroth-order optimizer for fine-tuning LLMs. What's more, HiZOO avoids the expensive memory cost and only increases one forward pass per step. Extensive experiments on various models (350M~66B parameters) indicate that HiZOO improves model convergence, significantly reducing training steps and effectively enhancing model accuracy. Moreover, we visualize the optimization trajectories of HiZOO on test functions, il
    
[^95]: 基于注意力引导的遮罩自编码器用于学习图像表示

    Attention-Guided Masked Autoencoders For Learning Image Representations

    [https://arxiv.org/abs/2402.15172](https://arxiv.org/abs/2402.15172)

    通过引入注意力引导的损失函数，该论文提出了一种新的遮罩自编码器，可以更好地学习图像中的对象表示，相比普通MAE表现出更好的性能。

    

    Masked autoencoders (MAEs)已经被证明是一种强大的方法，用于计算机视觉任务的无监督预训练。我们提出通过一个注意力引导的损失函数来通知重建过程。通过利用无监督目标发现的进展，我们获得了场景的注意力图，将其用于损失函数中，增强了对重建相关对象的重点重建，从而有效地激励模型学习更注重对象的表示，同时不会损害已建立的遮罩策略。我们的评估表明，我们的预训练模型学习了比普通MAE更好的潜在表示，证明了在几个基准测试上通过改进的线性探测和k-NN分类结果，同时使ViTs对不同背景更具鲁棒性。

    arXiv:2402.15172v1 Announce Type: cross  Abstract: Masked autoencoders (MAEs) have established themselves as a powerful method for unsupervised pre-training for computer vision tasks. While vanilla MAEs put equal emphasis on reconstructing the individual parts of the image, we propose to inform the reconstruction process through an attention-guided loss function. By leveraging advances in unsupervised object discovery, we obtain an attention map of the scene which we employ in the loss function to put increased emphasis on reconstructing relevant objects, thus effectively incentivizing the model to learn more object-focused representations without compromising the established masking strategy. Our evaluations show that our pre-trained models learn better latent representations than the vanilla MAE, demonstrated by improved linear probing and k-NN classification results on several benchmarks while at the same time making ViTs more robust against varying backgrounds.
    
[^96]: 用于随机组合半臂老虎机的协方差自适应最小二乘算法

    Covariance-Adaptive Least-Squares Algorithm for Stochastic Combinatorial Semi-Bandits

    [https://arxiv.org/abs/2402.15171](https://arxiv.org/abs/2402.15171)

    提出了一种协方差自适应的最小二乘算法，利用在线估计协方差结构，相对于基于代理方差的算法获得改进的遗憾上界，特别在协方差系数全为非负时，能有效地利用半臂反馈，并在各种参数设置下表现优异。

    

    我们解决了随机组合半臂老虎机问题，其中玩家可以从包含d个基本项的P个子集中进行选择。大多数现有算法（如CUCB、ESCB、OLS-UCB）需要对奖励分布有先验知识，比如子高斯代理-方差的上界，这很难准确估计。在这项工作中，我们设计了OLS-UCB的方差自适应版本，依赖于协方差结构的在线估计。在实际设置中，估计协方差矩阵的系数要容易得多，并且相对于基于代理方差的算法，导致改进的遗憾上界。当协方差系数全为非负时，我们展示了我们的方法有效地利用了半臂反馈，并且可以明显优于老虎机反馈方法，在指数级别P≫d以及P≤d的情况下，这一点并不来自大多数现有分析。

    arXiv:2402.15171v1 Announce Type: new  Abstract: We address the problem of stochastic combinatorial semi-bandits, where a player can select from P subsets of a set containing d base items. Most existing algorithms (e.g. CUCB, ESCB, OLS-UCB) require prior knowledge on the reward distribution, like an upper bound on a sub-Gaussian proxy-variance, which is hard to estimate tightly. In this work, we design a variance-adaptive version of OLS-UCB, relying on an online estimation of the covariance structure. Estimating the coefficients of a covariance matrix is much more manageable in practical settings and results in improved regret upper bounds compared to proxy variance-based algorithms. When covariance coefficients are all non-negative, we show that our approach efficiently leverages the semi-bandit feedback and provably outperforms bandit feedback approaches, not only in exponential regimes where P $\gg$ d but also when P $\le$ d, which is not straightforward from most existing analyses.
    
[^97]: 跳跃调谐在扩散采样中的惊人有效性

    The Surprising Effectiveness of Skip-Tuning in Diffusion Sampling

    [https://arxiv.org/abs/2402.15170](https://arxiv.org/abs/2402.15170)

    跳跃调谐是一种简单而惊人有效的训练方法，可以提高扩散采样中UNet模型的性能，并突破了ODE采样器的限制

    

    随着UNet架构的整合，扩散概率模型已经成为图像生成任务中的一个主导力量。UNet中的一个关键设计是编码器和解码器块之间的跳跃连接。尽管已经证明跳跃连接可以提高训练稳定性和模型性能，我们发现这样的捷径可能限制了变换的复杂性。随着采样步骤减少，生成过程和UNet的作用更接近于从高斯分布向目标的推进转换，为网络的复杂性提出了挑战。为了解决这一挑战，我们提出了Skip-Tuning，一种简单而令人惊讶地有效的基于跳过连接的无需训练的调整方法。我们的方法可以在ImageNet 64上使用19个NFE（1.75）为预训练的EDM实现100%的FID改进，突破了ODE采样器的限制，不论采样步骤如何。

    arXiv:2402.15170v1 Announce Type: cross  Abstract: With the incorporation of the UNet architecture, diffusion probabilistic models have become a dominant force in image generation tasks. One key design in UNet is the skip connections between the encoder and decoder blocks. Although skip connections have been shown to improve training stability and model performance, we reveal that such shortcuts can be a limiting factor for the complexity of the transformation. As the sampling steps decrease, the generation process and the role of the UNet get closer to the push-forward transformations from Gaussian distribution to the target, posing a challenge for the network's complexity. To address this challenge, we propose Skip-Tuning, a simple yet surprisingly effective training-free tuning method on the skip connections. Our method can achieve 100% FID improvement for pretrained EDM on ImageNet 64 with only 19 NFEs (1.75), breaking the limit of ODE samplers regardless of sampling steps. Surpris
    
[^98]: 分布式异构数据上的分裂联邦学习的收敛分析

    Convergence Analysis of Split Federated Learning on Heterogeneous Data

    [https://arxiv.org/abs/2402.15166](https://arxiv.org/abs/2402.15166)

    本文填补了分裂联邦学习在各异数据上收敛分析的空白，提供了针对强凸和一般凸目标的SFL收敛分析，收敛速率分别为$O(1/T)$和$O(1/\sqrt[3]{T})。

    

    分裂联邦学习（SFL）是一种最近的分布式方法，用于在多个客户端之间进行协作模型训练。在SFL中，全局模型通常被分为两部分，其中客户端以并行联邦方式训练一部分，主服务器训练另一部分。尽管最近关于SFL算法发展的研究很多，但SFL的收敛分析在文献中还未有提及，本文旨在弥补这一空白。对SFL进行分析可能比对联邦学习（FL）的分析更具挑战性，这是由于客户端和主服务器之间可能存在双速更新。我们提供了针对异构数据上强凸和一般凸目标的SFL收敛分析。收敛速率分别为$O(1/T)$和$O(1/\sqrt[3]{T})$，其中$T$表示SFL训练的总轮数。我们进一步将分析扩展到非凸目标和一些客户端可能在训练过程中不可用的情况。

    arXiv:2402.15166v1 Announce Type: cross  Abstract: Split federated learning (SFL) is a recent distributed approach for collaborative model training among multiple clients. In SFL, a global model is typically split into two parts, where clients train one part in a parallel federated manner, and a main server trains the other. Despite the recent research on SFL algorithm development, the convergence analysis of SFL is missing in the literature, and this paper aims to fill this gap. The analysis of SFL can be more challenging than that of federated learning (FL), due to the potential dual-paced updates at the clients and the main server. We provide convergence analysis of SFL for strongly convex and general convex objectives on heterogeneous data. The convergence rates are $O(1/T)$ and $O(1/\sqrt[3]{T})$, respectively, where $T$ denotes the total number of rounds for SFL training. We further extend the analysis to non-convex objectives and where some clients may be unavailable during trai
    
[^99]: EasyRL4Rec：面向基于强化学习的推荐系统的用户友好代码库

    EasyRL4Rec: A User-Friendly Code Library for Reinforcement Learning Based Recommender Systems

    [https://arxiv.org/abs/2402.15164](https://arxiv.org/abs/2402.15164)

    EasyRL4Rec是一个面向基于强化学习的推荐系统的用户友好和高效库，提供了多样化的RL环境、全面的核心模块、一致的评估标准和定制解决方案，旨在帮助简化模型开发并改善长期用户参与度。

    

    强化学习（RL）-基础的推荐系统（RSs）越来越被认可其提高长期用户参与度的能力。然而，这个领域面临挑战，如缺乏易用的框架、评估标准不一致以及复制以前的工作的复杂性。为解决这些障碍，我们提出了EasyRL4Rec，一个专为基于RL的RSs量身定制的用户友好和高效的库。EasyRL4Rec具有基于五个广泛使用的公共数据集构建的轻量级、多样化的RL环境，并配备了全面的核心模块，提供丰富的选项来简化模型的开发。它建立了一致的评估标准，重点关注长期影响，并引入了针对推荐系统定制的状态建模和行为表示的定制解决方案。此外，我们分享了通过与当前方法进行的大量实验获得的宝贵见解。EasyRL4Rec旨在促进

    arXiv:2402.15164v1 Announce Type: cross  Abstract: Reinforcement Learning (RL)-Based Recommender Systems (RSs) are increasingly recognized for their ability to improve long-term user engagement. Yet, the field grapples with challenges such as the absence of accessible frameworks, inconsistent evaluation standards, and the complexity of replicating prior work. Addressing these obstacles, we present EasyRL4Rec, a user-friendly and efficient library tailored for RL-based RSs. EasyRL4Rec features lightweight, diverse RL environments built on five widely-used public datasets, and is equipped with comprehensive core modules that offer rich options to ease the development of models. It establishes consistent evaluation criteria with a focus on long-term impacts and introduces customized solutions for state modeling and action representation tailored to recommender systems. Additionally, we share valuable insights gained from extensive experiments with current methods. EasyRL4Rec aims to facil
    
[^100]: 研究随机性对深度神经网络在森林火灾预测中评估的影响

    Studying the Impact of Stochasticity on the Evaluation of Deep Neural Networks for Forest-Fire Prediction

    [https://arxiv.org/abs/2402.15163](https://arxiv.org/abs/2402.15163)

    该论文首次系统研究了在随机假设下评估深度神经网络用于森林火灾预测的影响，发现评估对统计的忠实度是在高度随机场景下的可靠替代方法。

    

    本文首次系统研究了在随机假设下评估深度神经网络（DNNs）用于离散动力系统，重点关注野火预测。我们开发了一个框架来研究随机性对两类评估指标的影响：基于分类的指标，评估对观察地面真相（GT）的忠实度，以及适当的得分规则，测试对统计的忠实度。我们的研究结果表明，在高度随机的情况下，评估对统计的忠实度是一个可靠的替代方案。我们将我们的分析扩展到现实世界的森林火灾数据，突显了传统森林火灾预测评估方法中的局限性，并建议可解释的适用于随机性的替代方法。

    arXiv:2402.15163v1 Announce Type: cross  Abstract: This paper presents the first systematic study of the evaluation of Deep Neural Networks (DNNs) for discrete dynamical systems under stochastic assumptions, with a focus on wildfire prediction. We develop a framework to study the impact of stochasticity on two classes of evaluation metrics: classification-based metrics, which assess fidelity to observed ground truth (GT), and proper scoring rules, which test fidelity-to-statistic. Our findings reveal that evaluating for fidelity-to-statistic is a reliable alternative in highly stochastic scenarios. We extend our analysis to real-world wildfire data, highlighting limitations in traditional wildfire prediction evaluation methods, and suggest interpretable stochasticity-compatible alternatives.
    
[^101]: 基于微调的抽象式摘要模型的实体级事实适应性

    Entity-level Factual Adaptiveness of Fine-tuning based Abstractive Summarization Models

    [https://arxiv.org/abs/2402.15162](https://arxiv.org/abs/2402.15162)

    分析了基于微调的摘要模型在处理知识冲突时的实体级事实适应性，并提出了一种反事实数据增强方法，实验结果表明该方法增强了事实适应性，同时保持了事实一致性。

    

    抽象式摘要模型在处理参数化知识与输入文档中的知识冲突时，往往生成事实不一致的内容。本文分析了基于微调的摘要模型对知识冲突的鲁棒性，即我们称之为事实适应性。我们利用预训练语言模型构建评估集，并发现事实适应性与原始数据集上的事实一致性并非强相关。此外，我们引入了一种可控的反事实数据增强方法，其中增强数据中的知识冲突程度是可调节的。我们在两个预训练语言模型（PEGASUS 和 BART）和两个微调数据集（XSum 和 CNN/DailyMail）上的实验结果表明，我们的方法增强了事实适应性，同时在原始数据集上实现了与对比方法相当的事实一致性。

    arXiv:2402.15162v1 Announce Type: cross  Abstract: Abstractive summarization models often generate factually inconsistent content particularly when the parametric knowledge of the model conflicts with the knowledge in the input document. In this paper, we analyze the robustness of fine-tuning based summarization models to the knowledge conflict, which we call factual adaptiveness. We utilize pre-trained language models to construct evaluation sets and find that factual adaptiveness is not strongly correlated with factual consistency on original datasets. Furthermore, we introduce a controllable counterfactual data augmentation method where the degree of knowledge conflict within the augmented data can be adjustable. Our experimental results on two pre-trained language models (PEGASUS and BART) and two fine-tuning datasets (XSum and CNN/DailyMail) demonstrate that our method enhances factual adaptiveness while achieving factual consistency on original datasets on par with the contrastiv
    
[^102]: 面向空间感知的变压器记忆体用于体验代理

    Spatially-Aware Transformer Memory for Embodied Agents

    [https://arxiv.org/abs/2402.15160](https://arxiv.org/abs/2402.15160)

    本文探讨了利用包含空间信息的面向空间感知变压器模型，以改善记忆利用效率。

    

    情节记忆在各种认知过程中起着至关重要的作用，比如能够在头脑中回忆过去事件的能力。虽然认知科学强调空间上下文在情节记忆的形成和检索中的重要性，但当前实现人工智能系统中情节记忆的主要方法是通过存储时间顺序体验的变压器，这忽略了空间维度。因此，目前尚不清楚如何将基础结构扩展到除了仅有时间顺序之外的空间轴，并由此能够获得哪些好处。为了解决这个问题，本文探讨了利用包含空间信息的面向空间感知变压器模型。这些模型使得可以创建考虑时间和空间维度的场所中心情节记忆。采用这种方法，我们证明记忆利用效率可以得到提高，导致增强

    arXiv:2402.15160v1 Announce Type: cross  Abstract: Episodic memory plays a crucial role in various cognitive processes, such as the ability to mentally recall past events. While cognitive science emphasizes the significance of spatial context in the formation and retrieval of episodic memory, the current primary approach to implementing episodic memory in AI systems is through transformers that store temporally ordered experiences, which overlooks the spatial dimension. As a result, it is unclear how the underlying structure could be extended to incorporate the spatial axis beyond temporal order alone and thereby what benefits can be obtained. To address this, this paper explores the use of Spatially-Aware Transformer models that incorporate spatial information. These models enable the creation of place-centric episodic memory that considers both temporal and spatial dimensions. Adopting this approach, we demonstrate that memory utilization efficiency can be improved, leading to enhanc
    
[^103]: 面向预训练大型语言模型的机器遗忘

    Machine Unlearning of Pre-trained Large Language Models

    [https://arxiv.org/abs/2402.15159](https://arxiv.org/abs/2402.15159)

    本研究在大型语言模型（LLMs）中探讨了“被遗忘权”的概念，提出机器遗忘作为解决方案，并在预训练模型中建立了全面的遗忘框架及高效的遗忘方法，同时提供了改进超参数鲁棒性以及高效调整超参数的指南。

    

    本研究探讨了大型语言模型（LLMs）背景下“被遗忘权”的概念。我们以机器遗忘作为一个关键解决方案，重点关注预训练模型——一个明显缺乏研究的领域。我们在预训练LLMs中勾勒了一个全面的机器遗忘框架，包括对七种不同遗忘方法的批判性分析。通过使用来自arXiv、书籍和GitHub的策划数据集进行严格评估，我们建立了一个有力的机器遗忘性能基准，表明这些方法的计算效率比重新训练高出 $10^5$ 倍以上。我们的结果表明，在分布数据上将梯度上升与梯度下降结合可以改善超参数的鲁棒性。我们还提供了关于在遗忘过程中进行高效超参数调整的详细指南。我们的研究推动了有关伦理人工智能实践的讨论，提供了

    arXiv:2402.15159v1 Announce Type: cross  Abstract: This study investigates the concept of the `right to be forgotten' within the context of large language models (LLMs). We explore machine unlearning as a pivotal solution, with a focus on pre-trained models--a notably under-researched area. Our research delineates a comprehensive framework for machine unlearning in pre-trained LLMs, encompassing a critical analysis of seven diverse unlearning methods. Through rigorous evaluation using curated datasets from arXiv, books, and GitHub, we establish a robust benchmark for unlearning performance, demonstrating that these methods are over $10^5$ times more computationally efficient than retraining. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering
    
[^104]: 自适应对比学习的无监督句子嵌入自适应重建

    Self-Adaptive Reconstruction with Contrastive Learning for Unsupervised Sentence Embeddings

    [https://arxiv.org/abs/2402.15153](https://arxiv.org/abs/2402.15153)

    提出了一种新颖的Self-Adaptive Reconstruction Contrastive Sentence Embeddings（SARCSE）框架，利用自动编码器重建句子中的所有令牌，以帮助模型保留更多细粒度语义，并提出了一种自适应重建损失来缓解对令牌频率的偏见。

    

    无监督句子嵌入任务旨在将句子转换为语义向量表示。大多数先前的工作直接使用从预训练语言模型派生的句子表示。然而，由于预训练语言模型中的令牌偏差，模型无法捕捉句子中的细粒度语义，导致预测能力不佳。为解决这一问题，我们提出了一种新颖的自适应重建对比句子嵌入（SARCSE）框架，该框架利用自动编码器重建句子中的所有令牌，帮助模型在聚合令牌过程中保留更多细粒度语义。此外，我们提出了一种自适应重建损失来缓解对令牌频率的偏见。实验结果表明，与强基准SimCSE相比，SARCSE在7个STS任务上取得了显著的改进。

    arXiv:2402.15153v1 Announce Type: new  Abstract: Unsupervised sentence embeddings task aims to convert sentences to semantic vector representations. Most previous works directly use the sentence representations derived from pretrained language models. However, due to the token bias in pretrained language models, the models can not capture the fine-grained semantics in sentences, which leads to poor predictions. To address this issue, we propose a novel Self-Adaptive Reconstruction Contrastive Sentence Embeddings (SARCSE) framework, which reconstructs all tokens in sentences with an AutoEncoder to help the model to preserve more fine-grained semantics during tokens aggregating. In addition, we proposed a self-adaptive reconstruction loss to alleviate the token bias towards frequency. Experimental results show that SARCSE gains significant improvements compared with the strong baseline SimCSE on the 7 STS tasks.
    
[^105]: 关于Sharpness-Aware最小化和对抗训练之间的对偶性

    On the Duality Between Sharpness-Aware Minimization and Adversarial Training

    [https://arxiv.org/abs/2402.15152](https://arxiv.org/abs/2402.15152)

    通过研究Sharpness-Aware最小化(SAM)和对抗训练(AT)之间的对偶性，发现单独使用SAM可以提高对抗性能。

    

    对抗训练(Adversarial Training, AT)在训练过程中对输入样本进行对抗性扰动，被认为是对抗攻击中最有效的防御之一，但不可避免地存在一种基本的权衡，即必然会降低干净准确性。与对样本进行扰动不同，Sharpness-Aware最小化(SAM)在训练过程中对模型权重进行扰动，以寻找更平坦的损失曲面并提高泛化性能。然而，由于SAM旨在提高干净准确性，其在增强对抗稳健性方面的有效性尚未被探索。在本研究中，考虑到SAM和AT之间的对偶性，我们调查了从SAM中派生的对抗稳健性。有趣的是，我们发现单独使用SAM可以提高对抗稳健性。为了理解SAM的这种意外特性，我们首先提供了关于SAM如何隐式学习更鲁棒特征的经验和理论的见解，并进行了全面的实验。

    arXiv:2402.15152v1 Announce Type: cross  Abstract: Adversarial Training (AT), which adversarially perturb the input samples during training, has been acknowledged as one of the most effective defenses against adversarial attacks, yet suffers from a fundamental tradeoff that inevitably decreases clean accuracy. Instead of perturbing the samples, Sharpness-Aware Minimization (SAM) perturbs the model weights during training to find a more flat loss landscape and improve generalization. However, as SAM is designed for better clean accuracy, its effectiveness in enhancing adversarial robustness remains unexplored. In this work, considering the duality between SAM and AT, we investigate the adversarial robustness derived from SAM. Intriguingly, we find that using SAM alone can improve adversarial robustness. To understand this unexpected property of SAM, we first provide empirical and theoretical insights into how SAM can implicitly learn more robust features, and conduct comprehensive exper
    
[^106]: TREC: 通过少样本追溯子图学习实现APT战术/技术识别

    TREC: APT Tactic / Technique Recognition via Few-Shot Provenance Subgraph Learning

    [https://arxiv.org/abs/2402.15147](https://arxiv.org/abs/2402.15147)

    本文提出了TREC，首次尝试通过少样本追溯子图学习实现对APT攻击活动中战术/技术的识别，弥补了现有基于规则的方法无法识别细粒度APT技术和变种APT攻击的不足。

    

    APT（高级持续性威胁）具有持久性、隐秘性和多样性等特征，是对网络基础设施的最大威胁之一。 为应对此威胁，现有研究利用溯源图来捕捉主机中系统实体之间的复杂关系，实现有效的APT检测。 与大多数现有工作只检测单个攻击事件不同，理解组织和完成APT攻击活动所应用的战术/技术（例如，Kill-Chain、ATT&CK）对安全运营更为重要。 现有研究尝试手动设计一组规则，将低级系统事件映射到高级APT战术/技术。 但基于规则的方法粗粒度且缺乏泛化能力，因此只能识别APT战术，无法辨识细粒度的APT技术和突变的APT攻击。 本文提出了TREC，这是首个尝试认知AP

    arXiv:2402.15147v1 Announce Type: cross  Abstract: APT (Advanced Persistent Threat) with the characteristics of persistence, stealth, and diversity is one of the greatest threats against cyber-infrastructure. As a countermeasure, existing studies leverage provenance graphs to capture the complex relations between system entities in a host for effective APT detection. In addition to detecting single attack events as most existing work does, understanding the tactics / techniques (e.g., Kill-Chain, ATT&CK) applied to organize and accomplish the APT attack campaign is more important for security operations. Existing studies try to manually design a set of rules to map low-level system events to high-level APT tactics / techniques. However, the rule based methods are coarse-grained and lack generalization ability, thus they can only recognize APT tactics and cannot identify fine-grained APT techniques and mutant APT attacks. In this paper, we propose TREC, the first attempt to recognize AP
    
[^107]: 模糊均值漂移的收敛分析

    Convergence Analysis of Blurring Mean Shift

    [https://arxiv.org/abs/2402.15146](https://arxiv.org/abs/2402.15146)

    本文通过将BMS算法解释为优化过程，提供了模糊均值漂移算法的收敛性质分析，相较于现有结果，本研究不仅覆盖数据收敛到单一点的情况，还提供了多点收敛的收敛保证，展示了算法收敛速度快。

    

    模糊均值漂移（BMS）算法是均值漂移算法的一种变体，是一种基于核的迭代方法，用于数据聚类，数据点根据它们通过迭代模糊的收敛点进行聚类。本文通过利用将BMS算法解释为优化过程来分析BMS算法的收敛性质，这在现有的收敛研究中是已知的，但却被少有利用。现有结果仅适用于多维数据的收敛性质，只涵盖了所有模糊数据点序列收敛到单一点的情况，而本研究提供了一个收敛保证，即使这些序列可以收敛到多个点，从而产生多个簇。该研究还通过进一步利用收敛点的几何特征表明，BMS算法的收敛速度快。

    arXiv:2402.15146v1 Announce Type: new  Abstract: Blurring mean shift (BMS) algorithm, a variant of the mean shift algorithm, is a kernel-based iterative method for data clustering, where data points are clustered according to their convergent points via iterative blurring. In this paper, we analyze convergence properties of the BMS algorithm by leveraging its interpretation as an optimization procedure, which is known but has been underutilized in existing convergence studies. Whereas existing results on convergence properties applicable to multi-dimensional data only cover the case where all the blurred data point sequences converge to a single point, this study provides a convergence guarantee even when those sequences can converge to multiple points, yielding multiple clusters. This study also shows that the convergence of the BMS algorithm is fast by further leveraging geometrical characterization of the convergent points.
    
[^108]: 平行Boosting的成本

    The Cost of Parallelizing Boosting

    [https://arxiv.org/abs/2402.15145](https://arxiv.org/abs/2402.15145)

    我们研究了并行化弱到强Boosting算法学习的成本，证明了即使是对Boosting的轻微并行化也需要在训练复杂度上呈指数级增长。

    

    我们研究了并行化弱到强Boosting算法学习的成本，延续了Karbasi和Larsen最近的工作。我们的主要结果是双重的：首先，我们证明了一个紧密的下界，表明即使是对Boosting的轻微并行化也需要在训练复杂度上呈指数级增长。具体来说，设$\gamma$为弱学习器优于随机猜测的优势。著名的\textsc{AdaBoost}算法通过与弱学习器进行$\tilde{O}(1 / \gamma^2)$轮交互来产生准确的假设，其中每轮都在多项式时间内运行。Karbasi和Larsen表明“显著”的并行化必须导致指数级增长：任何Boosting算法要么与弱学习器交互$\Omega(1 / \gamma)$轮，要么在训练复杂度上出现$\exp(d/\gamma)$的增长，其中$d$是假设类的VC维度。我们通过展示任何Boosti

    arXiv:2402.15145v1 Announce Type: new  Abstract: We study the cost of parallelizing weak-to-strong boosting algorithms for learning, following the recent work of Karbasi and Larsen. Our main results are two-fold:   - First, we prove a tight lower bound, showing that even "slight" parallelization of boosting requires an exponential blow-up in the complexity of training.   Specifically, let $\gamma$ be the weak learner's advantage over random guessing. The famous \textsc{AdaBoost} algorithm produces an accurate hypothesis by interacting with the weak learner for $\tilde{O}(1 / \gamma^2)$ rounds where each round runs in polynomial time.   Karbasi and Larsen showed that "significant" parallelization must incur exponential blow-up: Any boosting algorithm either interacts with the weak learner for $\Omega(1 / \gamma)$ rounds or incurs an $\exp(d / \gamma)$ blow-up in the complexity of training, where $d$ is the VC dimension of the hypothesis class. We close the gap by showing that any boosti
    
[^109]: PUAD：稳健异常检测的简单方法

    PUAD: Frustratingly Simple Method for Robust Anomaly Detection

    [https://arxiv.org/abs/2402.15143](https://arxiv.org/abs/2402.15143)

    提出了一种在特征空间上利用分布检测方法来检测逻辑异常的简单方法

    

    开发准确快速的异常检测模型是实时计算机视觉应用中的重要任务。目前有很多研究致力于开发单一模型，用于检测结构性或逻辑性异常，这两者本质上是不同的。现有方法中，大多数暗含异常可以通过识别异常位置来表示。然而，我们认为如错误的物体数量等逻辑异常，不能很好地被空间特征图所表示，需要另一种方法。此外，我们专注于利用特征空间上的分布检测方法来检测逻辑异常的可能性，该方法聚合了特征图的空间信息。作为演示，我们提出了一种将简单的特征空间分布检测方法与最先进的基于重建的方法相结合的方法。

    arXiv:2402.15143v1 Announce Type: cross  Abstract: Developing an accurate and fast anomaly detection model is an important task in real-time computer vision applications. There has been much research to develop a single model that detects either structural or logical anomalies, which are inherently distinct. The majority of the existing approaches implicitly assume that the anomaly can be represented by identifying the anomalous location. However, we argue that logical anomalies, such as the wrong number of objects, can not be well-represented by the spatial feature maps and require an alternative approach. In addition, we focused on the possibility of detecting logical anomalies by using an out-of-distribution detection approach on the feature space, which aggregates the spatial information of the feature map. As a demonstration, we propose a method that incorporates a simple out-of-distribution detection method on the feature space against state-of-the-art reconstruction-based approa
    
[^110]: 关于神经常微分方程网络的伴随方法的注解

    A note on the adjoint method for neural ordinary differential equation network

    [https://arxiv.org/abs/2402.15141](https://arxiv.org/abs/2402.15141)

    该论文对神经常微分方程网络的伴随方法进行了深入探讨，发现传统的伴随形式与反向传播结果并不等价，同时指出了什么情况下伴随形式会产生与反向传播相同结果。

    

    利用扰动和算子伴随方法，严格给出了正确的伴随形式。从推导中，我们得出以下结果：1) 损失梯度不是一个常微分方程，而是一个积分，我们展示了原因；2) 传统的伴随形式与反向传播结果不等价。3) 伴随算子分析表明，只有当离散伴随算子与离散神经常微分方程具有相同方案时，伴随形式才能给出与BP相同的结果。

    arXiv:2402.15141v1 Announce Type: cross  Abstract: Perturbation and operator adjoint method are used to give the right adjoint form rigourously. From the derivation, we can have following results: 1) The loss gradient is not an ODE, it is an integral and we shows the reason; 2) The traditional adjoint form is not equivalent with the back propagation results. 3) The adjoint operator analysis shows that if and only if the discrete adjoint has the same scheme with the discrete neural ODE, the adjoint form would give the same results as BP does.
    
[^111]: 多元时间序列预测的深度耦合网络

    Deep Coupling Network For Multivariate Time Series Forecasting

    [https://arxiv.org/abs/2402.15134](https://arxiv.org/abs/2402.15134)

    重新审视多元时间序列的序内和序间关系，提出了一种用于预测的深度耦合网络，可以同时捕捉多阶序内和序间的复杂耦合。

    

    多元时间序列（MTS）预测在许多实际应用中至关重要。为了实现准确的MTS预测，同时考虑时间序列数据中的序内和序间关系是至关重要的。然而，先前的工作通常分别建模序内和序间关系，并忽略了存在于时间序列数据内部和之间的多阶交互，这严重影响了预测的准确性。在本文中，我们从互信息的角度重新审视序内和序间关系，并相应地构建了一个专门捕捉复杂多阶序内和序间耦合的全面关系学习机制。基于这一机制，我们提出了一种新颖的用于MTS预测的深度耦合网络，称为DeepCN，它包括一个专用于明确探索多阶序内和间的耦合机制的耦合机制。

    arXiv:2402.15134v1 Announce Type: cross  Abstract: Multivariate time series (MTS) forecasting is crucial in many real-world applications. To achieve accurate MTS forecasting, it is essential to simultaneously consider both intra- and inter-series relationships among time series data. However, previous work has typically modeled intra- and inter-series relationships separately and has disregarded multi-order interactions present within and between time series data, which can seriously degrade forecasting accuracy. In this paper, we reexamine intra- and inter-series relationships from the perspective of mutual information and accordingly construct a comprehensive relationship learning mechanism tailored to simultaneously capture the intricate multi-order intra- and inter-series couplings. Based on the mechanism, we propose a novel deep coupling network for MTS forecasting, named DeepCN, which consists of a coupling mechanism dedicated to explicitly exploring the multi-order intra- and in
    
[^112]: 通过自动生成的NLI数据集改进句子嵌入

    Improving Sentence Embeddings with an Automatically Generated NLI Dataset

    [https://arxiv.org/abs/2402.15132](https://arxiv.org/abs/2402.15132)

    通过自动生成的NLI数据集改进句子嵌入，实验结果表明该方法在STS任务中表现出色，优于现有方法。

    

    基于解码器的大型语言模型在自然语言处理的许多任务中表现出了很高的性能。这在句子嵌入学习中同样成立，其中基于解码器的模型PromptEOL 在语义文本相似性（STS）任务中取得了最佳表现。然而，PromptEOL 在很大程度上利用了对自然语言推理（NLI）数据集的手动标注进行微调。我们旨在通过使用LLM自动生成的NLI数据集来改进在无监督设置下学习的句子嵌入，并将其用于微调PromptEOL。在STS任务的实验中，提出的方法在人类评估方面达到了82.21的平均Spearman等级相关系数，从而优于现有方法而无需使用大规模手动注释的数据集。

    arXiv:2402.15132v1 Announce Type: new  Abstract: Decoder-based large language models (LLMs) have shown high performance on many tasks in natural language processing. This is also true for sentence embedding learning, where a decoder-based model, PromptEOL, has achieved the best performance on semantic textual similarity (STS) tasks. However, PromptEOL makes great use of fine-tuning with a manually annotated natural language inference (NLI) dataset. We aim to improve sentence embeddings learned in an unsupervised setting by automatically generating an NLI dataset with an LLM and using it to fine-tune PromptEOL. In experiments on STS tasks, the proposed method achieved an average Spearman's rank correlation coefficient of 82.21 with respect to human evaluation, thus outperforming existing methods without using large, manually annotated datasets.
    
[^113]: 具有弃权选项的多臂赌博机问题

    Multi-Armed Bandits with Abstention

    [https://arxiv.org/abs/2402.15127](https://arxiv.org/abs/2402.15127)

    提出了一个扩展的多臂赌博机问题，引入了弃权选项，并成功设计和分析了算法，实现了渐近和米迷诺下最优。

    

    我们介绍了一个新颖的多臂赌博机问题扩展，其中包含了额外的战略元素：弃权选项。在这个增强框架中，代理不仅需要在每个时间步选择一个臂，还可以选择在观察之前放弃接受随机瞬时奖励。当选择弃权时，代理要么遭受固定的后悔，要么获得一定的奖励保证。鉴于这种额外的复杂性，我们探讨是否可以开发出既渐近又米迷诺下最优的有效算法。我们通过设计和分析算法来回答这个问题，这些算法的后悔满足相应的信息理论下限。我们的研究为弃权选项的好处提供了有价值的数量化见解，为在其他具有这种选项的在线决策问题中进一步探索奠定了基础。

    arXiv:2402.15127v1 Announce Type: new  Abstract: We introduce a novel extension of the canonical multi-armed bandit problem that incorporates an additional strategic element: abstention. In this enhanced framework, the agent is not only tasked with selecting an arm at each time step, but also has the option to abstain from accepting the stochastic instantaneous reward before observing it. When opting for abstention, the agent either suffers a fixed regret or gains a guaranteed reward. Given this added layer of complexity, we ask whether we can develop efficient algorithms that are both asymptotically and minimax optimal. We answer this question affirmatively by designing and analyzing algorithms whose regrets meet their corresponding information-theoretic lower bounds. Our results offer valuable quantitative insights into the benefits of the abstention option, laying the groundwork for further exploration in other online decision-making problems with such an option. Numerical results f
    
[^114]: 通过深度展开加速斯坦变分梯度下降的收敛速度

    Accelerating Convergence of Stein Variational Gradient Descent via Deep Unfolding

    [https://arxiv.org/abs/2402.15125](https://arxiv.org/abs/2402.15125)

    通过深度展开技术，本文提出的可训练SVGD算法加速了其收敛速度，相比传统SVGD变体表现出更快的收敛速度。

    

    Stein变分梯度下降（SVGD）是一种著名的基于粒子的变分推断方法，用于对目标分布进行采样。SVGD已经引起了人们的兴趣，应用于贝叶斯推理等机器学习技术。本文提出了将一种名为深度展开的深度学习技术融入SVGD的新型可训练算法。这种方法促进了对SVGD的内部参数进行学习，从而加速了其收敛速度。为了评估所提出的可训练SVGD算法，我们对三项任务进行了数值模拟：对一维高斯混合进行采样，进行贝叶斯逻辑回归以及学习贝叶斯神经网络。结果表明，我们提出的算法比SVGD的传统变体表现出更快的收敛速度。

    arXiv:2402.15125v1 Announce Type: new  Abstract: Stein variational gradient descent (SVGD) is a prominent particle-based variational inference method used for sampling a target distribution. SVGD has attracted interest for application in machine-learning techniques such as Bayesian inference. In this paper, we propose novel trainable algorithms that incorporate a deep-learning technique called deep unfolding,into SVGD. This approach facilitates the learning of the internal parameters of SVGD, thereby accelerating its convergence speed. To evaluate the proposed trainable SVGD algorithms, we conducted numerical simulations of three tasks: sampling a one-dimensional Gaussian mixture, performing Bayesian logistic regression, and learning Bayesian neural networks. The results show that our proposed algorithms exhibit faster convergence than the conventional variants of SVGD.
    
[^115]: 使用两步重述对CLIP文本编码器进行微调

    Fine-tuning CLIP Text Encoders with Two-step Paraphrasing

    [https://arxiv.org/abs/2402.15120](https://arxiv.org/abs/2402.15120)

    通过两步重述生成过程对CLIP模型进行微调，以增强对释义的表示能力。

    

    Contrastive language-image pre-training (CLIP)模型在各种视觉-语言任务中表现出色，如文本到图像检索，其中模型需要有效处理自然语言输入以产生准确的视觉输出。然而，当前模型在处理输入查询中的语言变化（如释义）方面仍然面临限制，这使得难以处理现实应用中用户查询的广泛范围。在这项研究中，我们引入了一种简单的微调方法，以增强CLIP模型对释义的表示。我们的方法涉及一个两步释义生成过程，我们通过利用大型语言模型从网页规模的图像标题中自动创建两类释义。随后，我们通过使用这些生成的释义来微调CLIP文本编码器，同时冻结图像编码器。我们的结果模型，...

    arXiv:2402.15120v1 Announce Type: cross  Abstract: Contrastive language-image pre-training (CLIP) models have demonstrated considerable success across various vision-language tasks, such as text-to-image retrieval, where the model is required to effectively process natural language input to produce an accurate visual output. However, current models still face limitations in dealing with linguistic variations in input queries, such as paraphrases, making it challenging to handle a broad range of user queries in real-world applications. In this study, we introduce a straightforward fine-tuning approach to enhance the representations of CLIP models for paraphrases. Our approach involves a two-step paraphrase generation process, where we automatically create two categories of paraphrases from web-scale image captions by leveraging large language models. Subsequently, we fine-tune the CLIP text encoder using these generated paraphrases while freezing the image encoder. Our resulting model, 
    
[^116]: 物理约束的多项式混沌展开用于科学机器学习和不确定性量化

    Physics-constrained polynomial chaos expansion for scientific machine learning and uncertainty quantification

    [https://arxiv.org/abs/2402.15115](https://arxiv.org/abs/2402.15115)

    提出一种物理约束的多项式混沌展开方法，将科学机器学习与不确定性量化无缝集成，有效地实现SciML任务中的不确定性量化和在UQ任务中利用SciML提高不确定性评估。

    

    我们提出了一种新颖的物理约束的多项式混沌展开作为一种替代建模方法，能够执行科学机器学习（SciML）和不确定性量化（UQ）任务。所提出的方法具有独特的能力：将SciML与UQ无缝集成，从而能够有效地量化SciML任务中的不确定性，并利用SciML来改善UQ相关任务中的不确定性评估。该替代模型可以有效地纳入多种物理约束，如支配偏微分方程（PDEs）及其相关的初始和边界条件约束，不等式型约束（如单调性，凸性，非负性等），以及在训练过程中添加额外先验信息以辅助有限数据。这确保了物理上合理的预测，并显著减少了昂贵计算的需求。

    arXiv:2402.15115v1 Announce Type: cross  Abstract: We present a novel physics-constrained polynomial chaos expansion as a surrogate modeling method capable of performing both scientific machine learning (SciML) and uncertainty quantification (UQ) tasks. The proposed method possesses a unique capability: it seamlessly integrates SciML into UQ and vice versa, which allows it to quantify the uncertainties in SciML tasks effectively and leverage SciML for improved uncertainty assessment during UQ-related tasks. The proposed surrogate model can effectively incorporate a variety of physical constraints, such as governing partial differential equations (PDEs) with associated initial and boundary conditions constraints, inequality-type constraints (e.g., monotonicity, convexity, non-negativity, among others), and additional a priori information in the training process to supplement limited data. This ensures physically realistic predictions and significantly reduces the need for expensive comp
    
[^117]: MSPipe: 通过意识到陈旧性的管道实现高效的时间性GNN训练

    MSPipe: Efficient Temporal GNN Training via Staleness-aware Pipeline

    [https://arxiv.org/abs/2402.15113](https://arxiv.org/abs/2402.15113)

    提出了MSPipe，一个通用而高效的MTGNNs框架，实现了最大化训练吞吐量同时保持模型准确性

    

    记忆型时间性图神经网络（MTGNNs）是一类利用节点记忆模块捕获和保留长期时间依赖关系的时间性图神经网络，相对于无记忆的对应网络具有卓越的性能。然而，在MTGNNs中，为了获取最新的信息，记忆模块的迭代读取和更新过程需要遵循时间依赖关系，这引入了显著的开销并限制了训练吞吐量。现有静态GNNs的优化不适用于MTGNNs，因为两者在训练范式、模型架构和缺乏记忆模块上存在差异。此外，它们并未有效地解决时间依赖带来的挑战，使其对MTGNN训练无效。在本文中，我们提出了MSPipe，这是一个通用而高效的MTGNNs框架，可以最大化训练吞吐量同时保持模型准确性。

    arXiv:2402.15113v1 Announce Type: new  Abstract: Memory-based Temporal Graph Neural Networks (MTGNNs) are a class of temporal graph neural networks that utilize a node memory module to capture and retain long-term temporal dependencies, leading to superior performance compared to memory-less counterparts. However, the iterative reading and updating process of the memory module in MTGNNs to obtain up-to-date information needs to follow the temporal dependencies. This introduces significant overhead and limits training throughput. Existing optimizations for static GNNs are not directly applicable to MTGNNs due to differences in training paradigm, model architecture, and the absence of a memory module. Moreover, they do not effectively address the challenges posed by temporal dependencies, making them ineffective for MTGNN training. In this paper, we propose MSPipe, a general and efficient framework for MTGNNs that maximizes training throughput while maintaining model accuracy. Our design
    
[^118]: Chu-ko-nu：一种可靠、高效且支持匿名认证的多轮安全聚合实现，用于联邦学习中

    Chu-ko-nu: A Reliable, Efficient, and Anonymously Authentication-Enabled Realization for Multi-Round Secure Aggregation in Federated Learning

    [https://arxiv.org/abs/2402.15111](https://arxiv.org/abs/2402.15111)

    Chu-ko-nu提出了一种更可靠和匿名认证的多轮安全聚合方案，通过重新分配秘密密钥组件来突破了共享传输的概率P限制

    

    安全聚合使得联邦学习（FL）能够通过本地梯度更新对客户端进行协作训练，而无需暴露原始数据。然而，现有的安全聚合方案每轮必须执行昂贵的刷新设置，因为每个客户端需要在不同轮次建立新的独立于输入的密钥。最新研究Flamingo（S&P 2023）设计了一种基于共享传输的可重复使用密钥，以支持服务器持续执行多轮聚合。然而，它提出的共享传输机制仅能以P概率实现，具有有限的可靠性。为解决上述问题，我们提出了一种更可靠且支持匿名认证的名为Chu-ko-nu的多轮安全聚合方案。具体来说，在共享传输方面，Chu-ko-nu通过补充秘密密钥组件的重新分配过程，突破了概率P的限制。

    arXiv:2402.15111v1 Announce Type: cross  Abstract: Secure aggregation enables federated learning (FL) to perform collaborative training of clients from local gradient updates without exposing raw data. However, existing secure aggregation schemes inevitably perform an expensive fresh setup per round because each client needs to establish fresh input-independent secrets over different rounds. The latest research, Flamingo (S&P 2023), designed a share-transfer-based reusable secret key to support the server continuously performing multiple rounds of aggregation. Nevertheless, the share transfer mechanism it proposed can only be achieved with P probability, which has limited reliability. To tackle the aforementioned problems, we propose a more reliable and anonymously authenticated scheme called Chu-ko-nu for multi-round secure aggregation. Specifically, in terms of share transfer, Chu-ko-nu breaks the probability P barrier by supplementing a redistribution process of secret key component
    
[^119]: 抑制样本贡献的机器遗忘

    Machine Unlearning by Suppressing Sample Contribution

    [https://arxiv.org/abs/2402.15109](https://arxiv.org/abs/2402.15109)

    本文提出了一种机器遗忘方法，通过最小化输入敏感度来抑制遗忘数据的贡献，并在实验中表现出优异的性能。

    

    机器遗忘（MU）是指从经过良好训练的模型中删除数据，这在实践中非常重要，因为涉及“被遗忘的权利”。本文从训练数据和未见数据对模型贡献的基本区别入手：训练数据对最终模型有贡献，而未见数据没有。我们理论上发现输入敏感度可以近似衡量贡献，并实际设计了一种算法，称为MU-Mis（通过最小化输入敏感度进行机器遗忘），来抑制遗忘数据的贡献。实验结果表明，MU-Mis明显优于最先进的MU方法。此外，MU-Mis与MU的应用更加密切，因为它不需要使用剩余数据。

    arXiv:2402.15109v1 Announce Type: new  Abstract: Machine Unlearning (MU) is to forget data from a well-trained model, which is practically important due to the "right to be forgotten". In this paper, we start from the fundamental distinction between training data and unseen data on their contribution to the model: the training data contributes to the final model while the unseen data does not. We theoretically discover that the input sensitivity can approximately measure the contribution and practically design an algorithm, called MU-Mis (machine unlearning via minimizing input sensitivity), to suppress the contribution of the forgetting data. Experimental results demonstrate that MU-Mis outperforms state-of-the-art MU methods significantly. Additionally, MU-Mis aligns more closely with the application of MU as it does not require the use of remaining data.
    
[^120]: 基于采样的消息传递神经网络分布式训练

    Sampling-based Distributed Training with Message Passing Neural Network

    [https://arxiv.org/abs/2402.15106](https://arxiv.org/abs/2402.15106)

    该论文介绍了一种基于采样和分布式训练的消息传递神经网络（MPNN），能够有效解决边缘图神经网络在节点数量增加时的扩展挑战。

    

    在这项研究中，我们介绍了一种基于域分解的消息传递神经网络（MPNN）分布式训练和推断方法。我们的目标是解决随着节点数量增加而扩展边缘图神经网络的挑战。通过我们的分布式训练方法，结合Nystrom-近似采样技术，我们提出了一种可扩展的图神经网络，称为DS-MPNN（其中D和S分别代表分布式和采样），能够扩展到$O(10^5)$个节点。我们在两个案例上验证了我们的采样和分布式训练方法：（a）Darcy流数据集和（b）2-D机翼的稳态RANS模拟，提供了与单GPU实现和基于节点的图卷积网络（GCNs）的比较。DS-MPNN模型表现出与单GPU实现相当的准确性，能够容纳比单个GPU实现更多数量的节点。

    arXiv:2402.15106v1 Announce Type: new  Abstract: In this study, we introduce a domain-decomposition-based distributed training and inference approach for message-passing neural networks (MPNN). Our objective is to address the challenge of scaling edge-based graph neural networks as the number of nodes increases. Through our distributed training approach, coupled with Nystr\"om-approximation sampling techniques, we present a scalable graph neural network, referred to as DS-MPNN (D and S standing for distributed and sampled, respectively), capable of scaling up to $O(10^5)$ nodes. We validate our sampling and distributed training approach on two cases: (a) a Darcy flow dataset and (b) steady RANS simulations of 2-D airfoils, providing comparisons with both single-GPU implementation and node-based graph convolution networks (GCNs). The DS-MPNN model demonstrates comparable accuracy to single-GPU implementation, can accommodate a significantly larger number of nodes compared to the single-
    
[^121]: 轨迹式迭代强化学习框架用于自动竞标

    Trajectory-wise Iterative Reinforcement Learning Framework for Auto-bidding

    [https://arxiv.org/abs/2402.15102](https://arxiv.org/abs/2402.15102)

    自动广告竞标中使用了一种新的迭代离线强化学习框架，有效缓解了传统RL算法在在线环境下性能下降的问题。

    

    在在线广告中，广告主参与广告竞拍以获取广告机会，通常是通过需求方平台(DSPs)提供的自动竞标工具。目前的自动竞标算法通常采用强化学习（RL）。然而，由于安全性问题，大多数基于RL的自动竞标策略是在模拟环境中进行训练的，在在线环境中部署会导致性能下降。为了缩小这一差距，我们可以并行部署多个自动竞标代理以收集大量交互数据集。然后，可以利用离线RL算法训练新策略。训练后的策略随后可以部署以进行进一步的数据收集，从而形成一个迭代训练框架，我们将其称为迭代离线RL。在这项工作中，我们确定了这种迭代离线RL框架的性能瓶颈，其根源在于由于内在原因而导致的探索和利用的低效问题。

    arXiv:2402.15102v1 Announce Type: cross  Abstract: In online advertising, advertisers participate in ad auctions to acquire ad opportunities, often by utilizing auto-bidding tools provided by demand-side platforms (DSPs). The current auto-bidding algorithms typically employ reinforcement learning (RL). However, due to safety concerns, most RL-based auto-bidding policies are trained in simulation, leading to a performance degradation when deployed in online environments. To narrow this gap, we can deploy multiple auto-bidding agents in parallel to collect a large interaction dataset. Offline RL algorithms can then be utilized to train a new policy. The trained policy can subsequently be deployed for further data collection, resulting in an iterative training framework, which we refer to as iterative offline RL. In this work, we identify the performance bottleneck of this iterative offline RL framework, which originates from the ineffective exploration and exploitation caused by the inhe
    
[^122]: 研究LLM在闭源和开源数据上的性能表现

    Studying LLM Performance on Closed- and Open-source Data

    [https://arxiv.org/abs/2402.15100](https://arxiv.org/abs/2402.15100)

    本文研究了LLM在闭源和开源数据上的性能表现，发现在闭源软件数据上，C#的性能变化不大。

    

    大型语言模型（LLMs）在软件工程实践中得到广泛应用。这些模型对数据需求极高，主要是在具有宽松许可的开源（OSS）代码上进行训练。然而，实际使用中，很多软件开发仍然发生在盈利/专有领域，其中正在开发的代码不是也从未在公共领域中。因此，许多开发人员在LLMs在不熟悉正在开发的代码的情况下进行工作和使用。在这样的情况下，LLMs的性能是否和在OSS代码上一样好呢？若不是，有什么不同？当性能不同时，可能的原因是什么，是否有解决方法？本文使用来自微软的专有闭源软件数据作为研究对象，大部分专有代码采用C#和C++。我们发现C#的性能从OSS --> proprietary变化不大。

    arXiv:2402.15100v1 Announce Type: cross  Abstract: Large Language models (LLMs) are finding wide use in software engineering practice. These models are extremely data-hungry, and are largely trained on open-source (OSS) code distributed with permissive licenses. In terms of actual use however, a great deal of software development still occurs in the for-profit/proprietary sphere, where the code under development is not, and never has been, in the public domain; thus, many developers, do their work, and use LLMs, in settings where the models may not be as familiar with the code under development. In such settings, do LLMs work as well as they do for OSS code? If not, what are the differences? When performance differs, what are the possible causes, and are there work-arounds? In this paper, we examine this issue using proprietary, closed-source software data from Microsoft, where most proprietary code is in C# and C++. We find that performance for C# changes little from OSS --> proprieta
    
[^123]: 通过MIONet学习定义在不同域上的PDE的解算子

    Learning solution operators of PDEs defined on varying domains via MIONet

    [https://arxiv.org/abs/2402.15097](https://arxiv.org/abs/2402.15097)

    通过MIONet学习定义在不同域上的PDE的解算子，实现了解映射的学习，包括各种参数的变化，结果为进一步处理度量空间的逼近理论提供了洞见。

    

    在这项工作中，我们提出了一种方法，通过MIONet学习定义在不同域上的PDE的解算子，并在理论上证明了这种方法。我们首先将MIONet的逼近理论扩展到进一步处理度量空间，建立MIONet可以在度量空间中逼近具有多个输入的映射。随后，我们构建了一个包含一些适当区域的集合，并为这个集合提供了一个度量，从而使其成为一个度量空间，满足MIONet的逼近条件。基于理论基础，我们能够学习PDE的解映射，其中包括各种参数的变化，包括微分算子的参数，右手边项，边界条件以及域。举例来说，我们对2D泊松方程进行了实验，其中域和右手边项是变化的。结果提供了洞见。

    arXiv:2402.15097v1 Announce Type: new  Abstract: In this work, we propose a method to learn the solution operators of PDEs defined on varying domains via MIONet, and theoretically justify this method. We first extend the approximation theory of MIONet to further deal with metric spaces, establishing that MIONet can approximate mappings with multiple inputs in metric spaces. Subsequently, we construct a set consisting of some appropriate regions and provide a metric on this set thus make it a metric space, which satisfies the approximation condition of MIONet. Building upon the theoretical foundation, we are able to learn the solution mapping of a PDE with all the parameters varying, including the parameters of the differential operator, the right-hand side term, the boundary condition, as well as the domain. Without loss of generality, we for example perform the experiments for 2-d Poisson equations, where the domains and the right-hand side terms are varying. The results provide insig
    
[^124]: 具有低计算成本保证的多模态Transformer

    Multimodal Transformer With a Low-Computational-Cost Guarantee

    [https://arxiv.org/abs/2402.15096](https://arxiv.org/abs/2402.15096)

    提出了一种低成本多模态Transformer (LoCoMT) 机制，通过分配不同的多模态注意力模式给每个注意力头，实现了在减少计算成本的同时最小化性能损失

    

    基于Transformer的模型显著提高了各种多模态理解任务的性能，如视觉问题回答和动作识别。然而，多模态Transformer在多头注意力机制的复杂度方面存在问题，特别是随着模态数量的增加，存在二次复杂度。为了解决这个问题，我们引入了低成本多模态Transformer（LoCoMT），这是一种旨在在训练和推断过程中减少计算成本的新型多模态注意力机制，并且能够实现最小性能损失。具体地，通过为每个注意力头分配不同的多模态注意力模式，LoCoMT能够灵活控制多模态信号，并在理论上确保相对于现有多模态Transformer变体减少计算成本。在两个多模态数据集Audioset和MedVidCL上的实验结果表明，LoCoMT不仅减少了GFLOPs，

    arXiv:2402.15096v1 Announce Type: new  Abstract: Transformer-based models have significantly improved performance across a range of multimodal understanding tasks, such as visual question answering and action recognition. However, multimodal Transformers significantly suffer from a quadratic complexity of the multi-head attention with the input sequence length, especially as the number of modalities increases. To address this, we introduce Low-Cost Multimodal Transformer (LoCoMT), a novel multimodal attention mechanism that aims to reduce computational cost during training and inference with minimal performance loss. Specifically, by assigning different multimodal attention patterns to each attention head, LoCoMT can flexibly control multimodal signals and theoretically ensures a reduced computational cost compared to existing multimodal Transformer variants. Experimental results on two multimodal datasets, namely Audioset and MedVidCL demonstrate that LoCoMT not only reduces GFLOPs bu
    
[^125]: 用于匹配低维情况下的相关高斯几何模型的梅山算法

    The Umeyama algorithm for matching correlated Gaussian geometric models in the low-dimensional regime

    [https://arxiv.org/abs/2402.15095](https://arxiv.org/abs/2402.15095)

    该论文研究了匹配通过潜在节点排列相关的两个高斯几何模型的问题，为低维情况下的精确和几乎精确恢复建立了信息阈值，并进行了梅山算法的数值实验。

    

    受到匹配两个相关的随机几何图的问题的启发，我们研究了通过潜在节点排列相关的两个高斯几何模型的匹配问题。具体来说，给定$\{1,\ldots,n\}$上的一个未知排列$\pi^*$，以及给定$n$对在$\mathbb{R}^d$中通过噪声参数$\sigma$相关的高斯向量$\{X_{\pi^*(i)},Y_i\}$，我们考虑具有边权重$A_{i,j}=\langle X_i,X_j \rangle$，$B_{i,j}=\langle Y_i,Y_j \rangle$的两种类型的(相关的)加权完全图。目标是基于观察到的矩阵$A$和$B$恢复隐藏的顶点对应$\pi^*$。在维数为$d=O(\log n)$的低维情况下，Wang, Wu, Xu和Yolou [WWXY22+]建立了匹配相关的高斯几何模型中精确和几乎精确恢复的信息阈值。他们还对经典的梅山算法进行了数值实验。

    arXiv:2402.15095v1 Announce Type: cross  Abstract: Motivated by the problem of matching two correlated random geometric graphs, we study the problem of matching two Gaussian geometric models correlated through a latent node permutation. Specifically, given an unknown permutation $\pi^*$ on $\{1,\ldots,n\}$ and given $n$ i.i.d. pairs of correlated Gaussian vectors $\{X_{\pi^*(i)},Y_i\}$ in $\mathbb{R}^d$ with noise parameter $\sigma$, we consider two types of (correlated) weighted complete graphs with edge weights given by $A_{i,j}=\langle X_i,X_j \rangle$, $B_{i,j}=\langle Y_i,Y_j \rangle$. The goal is to recover the hidden vertex correspondence $\pi^*$ based on the observed matrices $A$ and $B$. For the low-dimensional regime where $d=O(\log n)$, Wang, Wu, Xu, and Yolou [WWXY22+] established the information thresholds for exact and almost exact recovery in matching correlated Gaussian geometric models. They also conducted numerical experiments for the classical Umeyama algorithm. In o
    
[^126]: AttributionBench：自动归因评估有多难？

    AttributionBench: How Hard is Automatic Attribution Evaluation?

    [https://arxiv.org/abs/2402.15089](https://arxiv.org/abs/2402.15089)

    AttributionBench是一个综合基准，揭示了自动归因评估的挑战，即使对于最先进的语言模型也只能达到80%的准确率。

    

    现代生成式搜索引擎通过提供引用证据增强了大型语言模型（LLM）响应的可靠性。然而，评估答案的归因，即生成响应中的每个声明是否都得到其引用证据的充分支持，仍然是一个未解决的问题。传统上依赖于昂贵的人工评估的这种验证强调了对自动归因评估方法的迫切需求。为了填补这种方法缺乏标准化基准的差距，我们提出了AttributionBench，这是一个综合性基准，由各种现有的归因数据集编制而成。我们在AttributionBench上的大量实验揭示了自动归因评估面临的挑战，即使对于最先进的LLM也是如此。具体而言，我们的发现表明，即使是经过优化的GPT-3.5在二元分类公式下也只能达到约80%的宏F1分数。更 than 300 error c

    arXiv:2402.15089v1 Announce Type: cross  Abstract: Modern generative search engines enhance the reliability of large language model (LLM) responses by providing cited evidence. However, evaluating the answer's attribution, i.e., whether every claim within the generated responses is fully supported by its cited evidence, remains an open problem. This verification, traditionally dependent on costly human evaluation, underscores the urgent need for automatic attribution evaluation methods. To bridge the gap in the absence of standardized benchmarks for these methods, we present AttributionBench, a comprehensive benchmark compiled from various existing attribution datasets. Our extensive experiments on AttributionBench reveal the challenges of automatic attribution evaluation, even for state-of-the-art LLMs. Specifically, our findings show that even a fine-tuned GPT-3.5 only achieves around 80% macro-F1 under a binary classification formulation. A detailed analysis of more than 300 error c
    
[^127]: PEMT: 多任务相关性引导的专家混合模型实现了参数高效的迁移学习

    PEMT: Multi-Task Correlation Guided Mixture-of-Experts Enables Parameter-Efficient Transfer Learning

    [https://arxiv.org/abs/2402.15082](https://arxiv.org/abs/2402.15082)

    PEMT 是一种基于多任务迁移学习的参数高效微调框架，通过扩展专家混合框架，以权重组合捕获源任务上训练的适配器，从而有效利用任务特定知识和源目标任务之间的相关性。

    

    针对参数高效微调（PEFT）作为将预训练语言模型有效地适应各种任务的方法已经崛起。最近，人们对从一个或多个任务转移知识到下游目标任务以实现性能提升产生了越来越浓厚的兴趣。然而，当前方法通常要么在各个任务上训练适配器，要么从源任务中提取共享知识，未能充分利用任务特定知识和源任务与目标任务之间的相关性。为了克服这些限制，我们提出了PEMT，这是一种基于多任务迁移学习的创新参数高效微调框架。PEMT将专家混合（MoE）框架扩展为源任务上训练的适配器的加权组合以捕获可转移知识。这些权重由一个门控单元确定，利用任务之间的相关性来测量目标任务和每个源任务之间的相关性。

    arXiv:2402.15082v1 Announce Type: new  Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as an effective method for adapting pre-trained language models to various tasks efficiently. Recently, there has been a growing interest in transferring knowledge from one or multiple tasks to the downstream target task to achieve performance improvements. However, current approaches typically either train adapters on individual tasks or distill shared knowledge from source tasks, failing to fully exploit task-specific knowledge and the correlation between source and target tasks. To overcome these limitations, we propose PEMT, a novel parameter-efficient fine-tuning framework based on multi-task transfer learning. PEMT extends the mixture-of-experts (MoE) framework to capture the transferable knowledge as a weighted combination of adapters trained on source tasks. These weights are determined by a gated unit, measuring the correlation between the target and each source task using task 
    
[^128]: 通过自适应偏好引导实现成本自适应补救建议

    Cost-Adaptive Recourse Recommendation by Adaptive Preference Elicitation

    [https://arxiv.org/abs/2402.15073](https://arxiv.org/abs/2402.15073)

    本文提出了一种集成偏好学习的两步方法，用于生成成本自适应的补救建议，通过设计问答框架和利用两种方法生成补救来考虑主体的不完整信息。

    

    arXiv:2402.15073v1 公告类型：新的 摘要：算法性补救建议是向主体推荐一种成本高效的行动，以扭转不利的机器学习分类决策。文献中大多数现有方法在生成补救建议时都是基于对成本函数的完全了解这一假设。在现实世界的实践中，主体可能具有不同的偏好，导致对主体的基础成本函数存在不完整的信息。本文提出了一种将偏好学习整合到补救产生问题中的两步方法。在第一步中，我们设计了一个问答框架，以顺序地改进主体的马氏矩阵成本的置信区间。然后，我们通过利用基于梯度和基于图的成本自适应补救两种方法来生成补救建议，确保有效性的同时考虑成本矩阵的整个置信区间。数值评估证明了我们的方法相对于当前最先进的方法的优势。

    arXiv:2402.15073v1 Announce Type: new  Abstract: Algorithmic recourse recommends a cost-efficient action to a subject to reverse an unfavorable machine learning classification decision. Most existing methods in the literature generate recourse under the assumption of complete knowledge about the cost function. In real-world practice, subjects could have distinct preferences, leading to incomplete information about the underlying cost function of the subject. This paper proposes a two-step approach integrating preference learning into the recourse generation problem. In the first step, we design a question-answering framework to refine the confidence set of the Mahalanobis matrix cost of the subject sequentially. Then, we generate recourse by utilizing two methods: gradient-based and graph-based cost-adaptive recourse that ensures validity while considering the whole confidence set of the cost matrix. The numerical evaluation demonstrates the benefits of our approach over state-of-the-a
    
[^129]: 通过数据和集成协同增强增强一次性联邦学习

    Enhancing One-Shot Federated Learning Through Data and Ensemble Co-Boosting

    [https://arxiv.org/abs/2402.15070](https://arxiv.org/abs/2402.15070)

    在本论文中，提出了一种名为Co-Boosting的新型框架，通过对合成数据和集成模型相互增强，促进了一次性联邦学习的发展。

    

    一次性联邦学习（OFL）已经成为一种有前途的学习范式，通过单一通信轮次实现全局服务器模型的训练。在OFL中，服务器模型通过从所有客户端模型（集成）中提炼知识进行聚合，客户端模型也负责合成用于提炼的样本。在这方面，先进的作品表明，服务器模型的性能与合成数据的质量和集成模型密切相关。为了推广OFL，我们引入了一个新颖的框架，Co-Boosting，其中合成数据和集成模型相互逐步增强对方。具体而言，Co-Boosting利用当前的集成模型以对抗方式合成更高质量的样本。这些困难样本然后被用来通过调整每个客户端模型的集成权重来提升集成模型的质量。因此，Co-Boosting周期性地...

    arXiv:2402.15070v1 Announce Type: new  Abstract: One-shot Federated Learning (OFL) has become a promising learning paradigm, enabling the training of a global server model via a single communication round. In OFL, the server model is aggregated by distilling knowledge from all client models (the ensemble), which are also responsible for synthesizing samples for distillation. In this regard, advanced works show that the performance of the server model is intrinsically related to the quality of the synthesized data and the ensemble model. To promote OFL, we introduce a novel framework, Co-Boosting, in which synthesized data and the ensemble model mutually enhance each other progressively. Specifically, Co-Boosting leverages the current ensemble model to synthesize higher-quality samples in an adversarial manner. These hard samples are then employed to promote the quality of the ensemble model by adjusting the ensembling weights for each client model. Consequently, Co-Boosting periodicall
    
[^130]: 别耍花招！大型语言模型自我调整以回答未知问题

    Gotcha! Don't trick me with unanswerable questions! Self-aligning Large Language Models for Responding to Unknown Questions

    [https://arxiv.org/abs/2402.15062](https://arxiv.org/abs/2402.15062)

    提出了一种自我调整方法，利用大型语言模型增强回答未知问题的能力，包括拒绝回答并解释未知问题无法回答的原因。

    

    尽管大型语言模型（LLMs）具有出色的回答问题能力，但它们在问题没有明确答案时往往表现出相当程度的自信过度。为了避免向这些未知问题提供虚构答案，现有研究通常探讨拒绝回答这些问题的方法。在这项工作中，我们提出了一种新颖且可扩展的自我调整方法，利用LLM本身来增强其对不同类型未知问题的回应能力，不仅能够拒绝回答，还能够解释未知问题无法回答的原因。具体来说，Self-Align方法首先采用两阶段类感知自我增强方法生成大量未知问题-回应数据。然后，我们进行差异驱动的自我整理，选择合格数据对LLM本身进行微调，以调整对未知问题的响应。

    arXiv:2402.15062v1 Announce Type: new  Abstract: Despite the remarkable abilities of Large Language Models (LLMs) to answer questions, they often display a considerable level of overconfidence even when the question does not have a definitive answer. To avoid providing hallucinated answers to these unknown questions, existing studies typically investigate approaches to refusing to answer these questions. In this work, we propose a novel and scalable self-alignment method to utilize the LLM itself to enhance its response-ability to different types of unknown questions, being capable of not only refusing to answer but also providing explanation to the unanswerability of unknown questions. Specifically, the Self-Align method first employ a two-stage class-aware self-augmentation approach to generate a large amount of unknown question-response data. Then we conduct disparity-driven self-curation to select qualified data for fine-tuning the LLM itself for aligning the responses to unknown q
    
[^131]: 针对领域特定机器翻译的大型语言模型微调

    Fine-tuning Large Language Models for Domain-specific Machine Translation

    [https://arxiv.org/abs/2402.15061](https://arxiv.org/abs/2402.15061)

    提出了一种名为LlamaIT的基于提示的微调方法，用于领域特定机器翻译任务，解决了大型语言模型在领域特定机器翻译中遇到的挑战。

    

    大型语言模型（LLMs）在机器翻译（MT）领域取得了重要进展。然而，它们在领域特定MT中的潜力尚未得到充分探索。当前基于LLMs的MT系统仍然面临一些挑战。为了解决这些挑战，本文提出了一种名为LlamaIT的基于提示的微调方法，以有效高效地为领域特定MT任务微调通用LLM。

    arXiv:2402.15061v1 Announce Type: new  Abstract: Large language models (LLMs) have made significant progress in machine translation (MT). However, their potential in domain-specific MT remains under-explored. Current LLM-based MT systems still face several challenges. First, for LLMs with in-context learning, their effectiveness is highly sensitive to input translation examples, and processing them can increase inference costs. They often require extra post-processing due to over-generation. Second, LLMs with fine-tuning on domain-specific data often require high training costs for domain adaptation, and may weaken the zero-shot MT capabilities of LLMs due to over-specialization. The aforementioned methods can struggle to translate rare words in domain transfer scenarios. To address these challenges, this paper proposes a prompt-oriented fine-tuning method, denoted as LlamaIT, to effectively and efficiently fine-tune a general-purpose LLM for domain-specific MT tasks. First, we constru
    
[^132]: 混合条形码：量化点云之间的几何-拓扑相互作用

    Mixup Barcodes: Quantifying Geometric-Topological Interactions between Point Clouds

    [https://arxiv.org/abs/2402.15058](https://arxiv.org/abs/2402.15058)

    提出了一种名为混合条形码的新方法，利用标准持久同调与图像持久同调结合，可以量化任意维度两个点集之间的几何-拓扑相互作用，以及引入简单的统计量来量化这种相互作用的复杂性。

    

    我们将标准持久同调与图像持久同调相结合，定义了一种新颖的表征形状和它们之间相互作用的方法。具体而言，我们介绍了：（1）混合条形码，捕捉任意维度两个点集之间的几何-拓扑相互作用（混合）；（2）简单的总混合和总百分比混合统计量，作为一个单一数字来量化相互作用的复杂性；（3）一个用于操作上述工具的软件工具。作为一个概念验证，我们将该工具应用到一个源自机器学习的问题上。具体地，我们研究了不同类别嵌入的可分离性。结果表明，拓扑混合是一种用于表征低维和高维数据交互的有效方法。与持久同调的典型用法相比，这个新工具对于拓扑特征的几何位置更为敏感，这通常是可取的。

    arXiv:2402.15058v1 Announce Type: cross  Abstract: We combine standard persistent homology with image persistent homology to define a novel way of characterizing shapes and interactions between them. In particular, we introduce: (1) a mixup barcode, which captures geometric-topological interactions (mixup) between two point sets in arbitrary dimension; (2) simple summary statistics, total mixup and total percentage mixup, which quantify the complexity of the interactions as a single number; (3) a software tool for playing with the above.   As a proof of concept, we apply this tool to a problem arising from machine learning. In particular, we study the disentanglement in embeddings of different classes. The results suggest that topological mixup is a useful method for characterizing interactions for low and high-dimensional data. Compared to the typical usage of persistent homology, the new tool is sensitive to the geometric locations of the topological features, which is often desirabl
    
[^133]: 在Transformer中解释上下文查找：探究注意力-MLP交互

    Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions

    [https://arxiv.org/abs/2402.15055](https://arxiv.org/abs/2402.15055)

    该研究探究了Transformer中注意力头和MLP之间的相互作用，并揭示了特定上下文下激活特定token预测的机制，从而阐明在LLMs中注意力如何促成依赖上下文的专门化处理。

    

    在本文中，我们研究了注意力头和Multilayer Perceptron中专门预测特定token的"next-token"神经元之间的相互作用。通过促使像GPT-4这样的LLM解释这些模型内部，我们可以阐明激活某些next-token神经元的注意力机制。我们的分析确定了识别与预测特定token相关的上下文的attention heads，通过残差连接激活相关联的神经元。我们专注于在较早的层中始终激活相同next-token神经元的attention heads。探索这些不同的激活模式揭示了为不同语言上下文专门化的头与生成某些tokens相关联。总体而言，我们的方法结合了神经解释和探测孤立的组件，以阐明注意力如何使LLMs中的依赖上下文的专门处理成为可能。

    arXiv:2402.15055v1 Announce Type: cross  Abstract: In this paper, we investigate the interplay between attention heads and specialized "next-token" neurons in the Multilayer Perceptron that predict specific tokens. By prompting an LLM like GPT-4 to explain these model internals, we can elucidate attention mechanisms that activate certain next-token neurons. Our analysis identifies attention heads that recognize contexts relevant to predicting a particular token, activating the associated neuron through the residual connection. We focus specifically on heads in earlier layers consistently activating the same next-token neuron across similar prompts. Exploring these differential activation patterns reveals that heads that specialize for distinct linguistic contexts are tied to generating certain tokens. Overall, our method combines neural explanations and probing isolated components to illuminate how attention enables context-dependent, specialized processing in LLMs.
    
[^134]: 使用对数Sobolev不等式的非线性贝叶斯最优实验设计

    Nonlinear Bayesian optimal experimental design using logarithmic Sobolev inequalities

    [https://arxiv.org/abs/2402.15053](https://arxiv.org/abs/2402.15053)

    使用对数Sobolev不等式构造的MI下限的贪婪方法在非线性模型优化设计中表现出色

    

    我们研究了从一个较大的候选池中选择$k$个实验的问题，目标是最大化所选子集与基础参数之间的互信息（MI）。由于组合优化问题的复杂性以及在非线性/非高斯设置中评估MI的困难性，找到确切解决方案在计算上是昂贵的。我们提出了基于通过对数Sobolev不等式构造的新的计算廉价的MI下限的贪婪方法。我们证明，在包括具有非加性噪声的非线性模型的最优设计在内的各种设置中，我们的方法优于随机选择策略、高斯逼近和嵌套蒙特卡洛（NMC）MI估算器。

    arXiv:2402.15053v1 Announce Type: cross  Abstract: We study the problem of selecting $k$ experiments from a larger candidate pool, where the goal is to maximize mutual information (MI) between the selected subset and the underlying parameters. Finding the exact solution is to this combinatorial optimization problem is computationally costly, not only due to the complexity of the combinatorial search but also the difficulty of evaluating MI in nonlinear/non-Gaussian settings. We propose greedy approaches based on new computationally inexpensive lower bounds for MI, constructed via log-Sobolev inequalities. We demonstrate that our method outperforms random selection strategies, Gaussian approximations, and nested Monte Carlo (NMC) estimators of MI in various settings, including optimal design for nonlinear models with non-additive noise.
    
[^135]: 人脸标记检测的信任焦点增强

    Fiducial Focus Augmentation for Facial Landmark Detection

    [https://arxiv.org/abs/2402.15044](https://arxiv.org/abs/2402.15044)

    提出了一种专为人脸标记检测任务设计的新型图像增强技术，通过连体架构训练机制和DCCA损失来实现对面部结构理解的提高

    

    深度学习方法在人脸标记检测（FLD）任务的表现上取得了显著的改进。然而，在具有挑战性的环境中检测标记，如头部姿势变化、夸张表情或不均匀照明，仍然是一个挑战，这是由于高度变化和样本不足所导致的。为了解决这一问题，我们提出了一种专为FLD任务设计的新型图像增强技术，以增强模型对面部结构的理解。为了有效利用新提出的增强技术，我们采用基于连体架构的训练机制，使用基于深度规范相关分析（DCCA）的损失来实现从输入图像的两个不同视图中的高级特征表示的集体学习。

    arXiv:2402.15044v1 Announce Type: cross  Abstract: Deep learning methods have led to significant improvements in the performance on the facial landmark detection (FLD) task. However, detecting landmarks in challenging settings, such as head pose changes, exaggerated expressions, or uneven illumination, continue to remain a challenge due to high variability and insufficient samples. This inadequacy can be attributed to the model's inability to effectively acquire appropriate facial structure information from the input images. To address this, we propose a novel image augmentation technique specifically designed for the FLD task to enhance the model's understanding of facial structures. To effectively utilize the newly proposed augmentation technique, we employ a Siamese architecture-based training mechanism with a Deep Canonical Correlation Analysis (DCCA)-based loss to achieve collective learning of high-level feature representations from two different views of the input images. Furthe
    
[^136]: KIEval：面向大型语言模型的知识引导式交互评估框架

    KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models

    [https://arxiv.org/abs/2402.15043](https://arxiv.org/abs/2402.15043)

    该论文引入了KIEval，一种知识引导式交互评估框架，通过LLM-powered "interactor"角色实现动态的抗污染评估

    

    大型语言模型（LLMs）的自动评估方法受到数据污染的影响，导致对其有效性的评估被夸大。现有的策略旨在检测受污染的文本，但侧重于量化污染程度而非准确衡量模型性能。本文介绍了KIEval，这是一种知识引导式交互评估框架，首次引入了LLM驱动的“交互者”角色，实现了动态抗污染评估。从涉及特定领域知识的常规LLM基准问题开始，KIEval利用动态生成的、多轮、以知识为重点的对话，以确定模型的响应是否仅是基准答案的回忆，还是表明了深入理解并能在更复杂的对话中应用知识。在五个数据集上对七个领先的LLM进行了大量实验证实了KI

    arXiv:2402.15043v1 Announce Type: cross  Abstract: Automatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inflated assessments of their effectiveness. Existing strategies, which aim to detect contaminated texts, focus on quantifying contamination status instead of accurately gauging model performance. In this paper, we introduce KIEval, a Knowledge-grounded Interactive Evaluation framework, which incorporates an LLM-powered "interactor" role for the first time to accomplish a dynamic contamination-resilient evaluation. Starting with a question in a conventional LLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically generated, multi-round, and knowledge-focused dialogues to determine whether a model's response is merely a recall of benchmark answers or demonstrates a deep comprehension to apply knowledge in more complex conversations. Extensive experiments on seven leading LLMs across five datasets validate KI
    
[^137]: 自动描述岩石薄片：一个Web应用的应用

    Descripci\'on autom\'atica de secciones delgadas de rocas: una aplicaci\'on Web

    [https://arxiv.org/abs/2402.15039](https://arxiv.org/abs/2402.15039)

    该论文提出使用人工智能技术结合计算机视觉和自然语言处理，从岩石薄片图像生成文本和语音描述。

    

    确定和表征各种岩石类型是地质学以及矿业、石油、环境、工业和建筑等领域的基本活动之一。传统上, 人类专家负责使用原位采集的岩石样本或在实验室准备的样本分析和解释有关类型、组成、质地、形状和其他属性的细节。结果因经验而主观, 除了消耗大量时间和精力。本提议利用人工智能技术结合计算机视觉和自然语言处理，从岩石薄片图像生成文本和语音描述。我们构建了图像及其相应文本描述的数据集，用于训练将由EfficientNetB7提取的图像相关特征与生成的文本描述相关联的模型。

    arXiv:2402.15039v1 Announce Type: cross  Abstract: The identification and characterization of various rock types is one of the fundamental activities for geology and related areas such as mining, petroleum, environment, industry and construction. Traditionally, a human specialist is responsible for analyzing and explaining details about the type, composition, texture, shape and other properties using rock samples collected in-situ or prepared in a laboratory. The results become subjective based on experience, in addition to consuming a large investment of time and effort. The present proposal uses artificial intelligence techniques combining computer vision and natural language processing to generate a textual and verbal description from a thin section image of rock. We build a dataset of images and their respective textual descriptions for the training of a model that associates the relevant features of the image extracted by EfficientNetB7 with the textual description generated by a 
    
[^138]: 动态引导扩散模型用于机器人 manipulator 设计

    Dynamics-Guided Diffusion Model for Robot Manipulator Design

    [https://arxiv.org/abs/2402.15038](https://arxiv.org/abs/2402.15038)

    该论文提出了动态引导扩散模型，利用共享的动力学网络为不同操作任务生成 manipulator 几何设计，通过设计目标构建的梯度引导手指几何设计的完善过程。

    

    我们提出了一个名为动态引导扩散模型的数据驱动框架，用于为给定操作任务生成 manipulator 几何设计。与为每个任务训练不同的设计模型不同，我们的方法采用一个跨任务共享的学习动力学网络。对于新的操作任务，我们首先将其分解为一组称为目标相互作用配置文件的个别运动目标，其中每个个别运动可以由共享的动力学网络建模。从目标和预测的相互作用配置文件构建的设计目标为任务的手指几何设计提供了梯度引导。这个设计过程被执行为一种分类器引导的扩散过程，其中设计目标作为分类器引导。我们在只使用开环平行夹爪运动的无传感器设置下，在各种操作任务上评估了我们的框架。

    arXiv:2402.15038v1 Announce Type: cross  Abstract: We present Dynamics-Guided Diffusion Model, a data-driven framework for generating manipulator geometry designs for a given manipulation task. Instead of training different design models for each task, our approach employs a learned dynamics network shared across tasks. For a new manipulation task, we first decompose it into a collection of individual motion targets which we call target interaction profile, where each individual motion can be modeled by the shared dynamics network. The design objective constructed from the target and predicted interaction profiles provides a gradient to guide the refinement of finger geometry for the task. This refinement process is executed as a classifier-guided diffusion process, where the design objective acts as the classifier guidance. We evaluate our framework on various manipulation tasks, under the sensor-less setting using only an open-loop parallel jaw motion. Our generated designs outperfor
    
[^139]: 练习方能致完美：规划学习技能参数策略

    Practice Makes Perfect: Planning to Learn Skill Parameter Policies

    [https://arxiv.org/abs/2402.15025](https://arxiv.org/abs/2402.15025)

    机器人应该通过估计技能的能力，推断通过练习能力的提升，并将其放置在任务分配中，以选择要练习的技能来最大化未来任务成功的预期。

    

    一个有效的机器人决策方法是将参数化技能序列起来，考虑到机器人最初会配备一系列参数化技能库、一个AI规划器用于根据目标对技能进行排序，并且具有用于选择技能参数的非常普遍的先验分布。一旦部署，机器人应该通过将其技能参数选择策略专门化到其环境中的特定对象、目标和约束，来迅速自主地学习以提高其性能。本文侧重于主动学习问题，即选择要练习的技能以最大化未来任务成功的预期。我们提出机器人应该估计每个技能的能力，推断能力（即“通过练习能力会提升多少？”），并将该技能置于任务分配中。

    arXiv:2402.15025v1 Announce Type: cross  Abstract: One promising approach towards effective robot decision making in complex, long-horizon tasks is to sequence together parameterized skills. We consider a setting where a robot is initially equipped with (1) a library of parameterized skills, (2) an AI planner for sequencing together the skills given a goal, and (3) a very general prior distribution for selecting skill parameters. Once deployed, the robot should rapidly and autonomously learn to improve its performance by specializing its skill parameter selection policy to the particular objects, goals, and constraints in its environment. In this work, we focus on the active learning problem of choosing which skills to practice to maximize expected future task success. We propose that the robot should estimate the competence of each skill, extrapolate the competence (asking: "how much would the competence improve through practice?"), and situate the skill in the task distribution throu
    
[^140]: 具有掩码语言模型的概率健壮束搜索

    Probabilistically-sound beam search with masked language models

    [https://arxiv.org/abs/2402.15020](https://arxiv.org/abs/2402.15020)

    提出了在掩码语言模型上进行束搜索的概率健壮方法，表明其在多个领域中优于传统方法。

    

    具有掩码语言模型（MLMs）的束搜索存在挑战，部分原因是由于序列的联合概率分布不像自回归模型那样readily available。然而，估算这样的分布在许多领域中具有应用，包括蛋白工程和古代文本恢复。我们提出了一种具有概率健壮性的使用MLMs进行束搜索的方法。首先，我们阐明了在哪些条件下使用标准束搜索对MLMs执行文本填充在理论上是可靠的。当这些条件失败时，我们提供了一种具有概率健壮性的修改，而且无需额外的计算复杂性，并且证明在预期条件下它优于前述的束搜索。然后，我们提出了比较多个领域中几种使用MLMs进行填充的方法的经验结果。

    arXiv:2402.15020v1 Announce Type: cross  Abstract: Beam search with masked language models (MLMs) is challenging in part because joint probability distributions over sequences are not readily available, unlike for autoregressive models. Nevertheless, estimating such distributions has applications in many domains, including protein engineering and ancient text restoration. We present probabilistically-sound methods for beam search with MLMs. First, we clarify the conditions under which it is theoretically sound to perform text infilling with MLMs using standard beam search. When these conditions fail, we provide a probabilistically-sound modification with no additional computational complexity and demonstrate that it is superior to the aforementioned beam search in the expected conditions. We then present empirical results comparing several infilling approaches with MLMs across several domains.
    
[^141]: 使用样式和内容信息的一致性引导温度缩放用于域外校准

    Consistency-Guided Temperature Scaling Using Style and Content Information for Out-of-Domain Calibration

    [https://arxiv.org/abs/2402.15019](https://arxiv.org/abs/2402.15019)

    提出了一种新的一致性引导温度缩放（CTS）策略，通过提供源域数据样本之间的相互监督，显著增强了域外（OOD）校准性能。

    

    近年来，关于深度神经网络对领域转移的鲁棒性越来越受到关注。然而，大多数现有研究都集中在提高模型的准确性上，而不是校准性能，而后者是值得信赖的AI系统的另一个重要要求。温度缩放（TS）作为一种可以保持准确性的事后校准方法，在领域内环境中已被证明是有效的，但在领域外（OOD）却不是，因为事先很难获取未见领域的验证集。在本文中，我们提出了一种新的温度缩放策略，一致性引导温度缩放（CTS），通过提供源域数据样本之间的相互监督，可以显著提高OOD校准性能。受到我们的观察到的发现，由于不一致的样本预测导致的过度自信是OOD校准的主要障碍，我们提出了一种新的校准策略。

    arXiv:2402.15019v1 Announce Type: cross  Abstract: Research interests in the robustness of deep neural networks against domain shifts have been rapidly increasing in recent years. Most existing works, however, focus on improving the accuracy of the model, not the calibration performance which is another important requirement for trustworthy AI systems. Temperature scaling (TS), an accuracy-preserving post-hoc calibration method, has been proven to be effective in in-domain settings, but not in out-of-domain (OOD) due to the difficulty in obtaining a validation set for the unseen domain beforehand. In this paper, we propose consistency-guided temperature scaling (CTS), a new temperature scaling strategy that can significantly enhance the OOD calibration performance by providing mutual supervision among data samples in the source domains. Motivated by our observation that over-confidence stemming from inconsistent sample predictions is the main obstacle to OOD calibration, we propose to 
    
[^142]: LLM对全球表示的意外影响

    Unintended Impacts of LLM Alignment on Global Representation

    [https://arxiv.org/abs/2402.15018](https://arxiv.org/abs/2402.15018)

    对大型语言模型（LLMs）进行用户偏好对齐可能会导致英语方言和全球意见之间的差异，但也提高了多种语言的能力。

    

    在为面向用户的应用程序部署之前，开发人员通过各种程序（如从人类反馈中学习强化学习（RLHF）和直接偏好优化（DPO））将大型语言模型（LLMs）与用户偏好进行对齐。目前对这些程序的评估侧重于遵循指导、推理和真实性的基准。然而，人类偏好并非普遍，对特定偏好集进行对齐可能会产生意外影响。我们探讨了对三个全球表示维度：英语方言、多语言能力和全球各国意见的影响。我们的结果显示，当前的对齐程序在英语方言和全球意见之间产生差异。我们发现对齐提高了多种语言的能力。最后，我们讨论了导致这些意外影响的设计决策，并为更公平的建议。

    arXiv:2402.15018v1 Announce Type: new  Abstract: Before being deployed for user-facing applications, developers align Large Language Models (LLMs) to user preferences through a variety of procedures, such as Reinforcement Learning From Human Feedback (RLHF) and Direct Preference Optimization (DPO). Current evaluations of these procedures focus on benchmarks of instruction following, reasoning, and truthfulness. However, human preferences are not universal, and aligning to specific preference sets may have unintended effects. We explore how alignment impacts performance along three axes of global representation: English dialects, multilingualism, and opinions from and about countries worldwide. Our results show that current alignment procedures create disparities between English dialects and global opinions. We find alignment improves capabilities in several languages. We conclude by discussing design decisions that led to these unintended impacts and recommendations for more equitable 
    
[^143]: 通过多任务微调实现基础模型的少样本适应

    Towards Few-Shot Adaptation of Foundation Models via Multitask Finetuning

    [https://arxiv.org/abs/2402.15017](https://arxiv.org/abs/2402.15017)

    多任务微调的方法通过在基础模型上对相关任务进行微调，然后适应限制标签数的目标任务，能够降低目标任务中的误差，并提出了一种实用的任务选择算法。

    

    基础模型已经成为许多人工智能问题的有力工具。尽管基础模型取得了巨大成功，但有效地适应新任务，特别是那些数据标签有限的任务，仍然是一个开放问题，并且缺乏理论理解。最近在视觉和自然语言处理领域取得成功的一种新兴解决方案是，在基础模型上对一系列相关任务进行微调，然后再适应具有有限标记样本的目标任务。本文研究了这种多任务微调方法的理论验证。我们的理论分析表明，通过一个多样化的相关任务集，这种多任务微调可以降低目标任务中的误差，与直接适应相同预训练模型相比。我们通过多样性和一致性指标量化了微调任务和目标任务之间的关系，并进一步提出了一个实用的任务选择算法。

    arXiv:2402.15017v1 Announce Type: cross  Abstract: Foundation models have emerged as a powerful tool for many AI problems. Despite the tremendous success of foundation models, effective adaptation to new tasks, particularly those with limited labels, remains an open question and lacks theoretical understanding. An emerging solution with recent success in vision and NLP involves finetuning a foundation model on a selection of relevant tasks, before its adaptation to a target task with limited labeled samples. In this paper, we study the theoretical justification of this multitask finetuning approach. Our theoretical analysis reveals that with a diverse set of related tasks, this multitask finetuning leads to reduced error in the target task, in comparison to directly adapting the same pretrained model. We quantify the relationship between finetuning tasks and target tasks by diversity and consistency metrics, and further propose a practical task selection algorithm. We substantiate our 
    
[^144]: 法语医用口罩语言模型中的标记化有多重要？

    How Important Is Tokenization in French Medical Masked Language Models?

    [https://arxiv.org/abs/2402.15010](https://arxiv.org/abs/2402.15010)

    子词标记化成为自然语言处理领域的主流标准，但其成功因素，如不同任务和语言的最佳分割粒度、数据源对标记工具的影响以及形态信息在印欧语言中的作用，仍不明确。

    

    近年来，基于子词的标记化已成为自然语言处理（NLP）领域中的主流标准，主要是由于预训练语言模型的广泛应用。然而，导致其成功的确切因素，如不同任务和语言的最佳分割粒度，数据源对标记工具的影响以及形态信息在印欧语言中的作用，仍然不够清楚。这在生物医学术语方面尤为重要，其特点是具有管理形态素组合的特定规则。

    arXiv:2402.15010v1 Announce Type: cross  Abstract: Subword tokenization has become the prevailing standard in the field of natural language processing (NLP) over recent years, primarily due to the widespread utilization of pre-trained language models. This shift began with Byte-Pair Encoding (BPE) and was later followed by the adoption of SentencePiece and WordPiece. While subword tokenization consistently outperforms character and word-level tokenization, the precise factors contributing to its success remain unclear. Key aspects such as the optimal segmentation granularity for diverse tasks and languages, the influence of data sources on tokenizers, and the role of morphological information in Indo-European languages remain insufficiently explored. This is particularly pertinent for biomedical terminology, characterized by specific rules governing morpheme combinations. Despite the agglutinative nature of biomedical terminology, existing language models do not explicitly incorporate 
    
[^145]: opp/ai：基于区块链的乐观隐私保护人工智能

    opp/ai: Optimistic Privacy-Preserving AI on Blockchain

    [https://arxiv.org/abs/2402.15006](https://arxiv.org/abs/2402.15006)

    opp/ai框架结合了zkML的隐私和opML的效率，为区块链上的AI服务提供了平衡的隐私保护和计算效率。

    

    人工智能（AI）和区块链技术的融合正在重塑数字世界，为区块链平台上提供了去中心化、安全和高效的AI服务。然而，AI对区块链的高计算需求引发了隐私和效率方面的重大担忧。乐观隐私保护人工智能（opp/ai）框架被引入作为解决这些问题的开创性方案，在隐私保护和计算效率之间取得平衡。该框架将Zero-Knowledge Machine Learning（zkML）用于隐私保护，将Optimistic Machine Learning（opML）用于提高效率，打造了针对区块链AI服务的混合模型。本研究介绍了opp/ai框架，深入探讨了zkML的隐私功能，并评估了该框架在不同场景下的性能和适应性。

    arXiv:2402.15006v1 Announce Type: cross  Abstract: The convergence of Artificial Intelligence (AI) and blockchain technology is reshaping the digital world, offering decentralized, secure, and efficient AI services on blockchain platforms. Despite the promise, the high computational demands of AI on blockchain raise significant privacy and efficiency concerns. The Optimistic Privacy-Preserving AI (opp/ai) framework is introduced as a pioneering solution to these issues, striking a balance between privacy protection and computational efficiency. The framework integrates Zero-Knowledge Machine Learning (zkML) for privacy with Optimistic Machine Learning (opML) for efficiency, creating a hybrid model tailored for blockchain AI services. This study presents the opp/ai framework, delves into the privacy features of zkML, and assesses the framework's performance and adaptability across different scenarios.
    
[^146]: 机器学习分类算法的比较及其在弗莱明汉姆心脏研究中的应用

    Comparison of Machine Learning Classification Algorithms and Application to the Framingham Heart Study

    [https://arxiv.org/abs/2402.15005](https://arxiv.org/abs/2402.15005)

    该研究通过弗莱明汉姆心脏病数据作为案例研究，比较了八种机器学习分类算法在不同训练/测试场景下的预测性能，发现极端梯度提升和支持向量机在训练不平衡数据时存在缺陷。

    

    在医疗保健中使用机器学习算法可能会放大社会不公正和健康不平等。本研究针对机器学习分类算法在开发和部署过程中出现的一些一般化障碍，使用弗莱明汉姆冠心病数据作为案例研究，展示了如何有效地选择概率截断以将回归模型转换为分类器。我们比较了八种机器学习分类算法在四种训练/测试场景下的预测性能的抽样分布，以测试它们的一般化能力和延续偏见的潜力。我们发现，极端梯度提升和支持向量机在训练不平衡数据时存在缺陷。

    arXiv:2402.15005v1 Announce Type: new  Abstract: The use of machine learning algorithms in healthcare can amplify social injustices and health inequities. While the exacerbation of biases can occur and compound during the problem selection, data collection, and outcome definition, this research pertains to some generalizability impediments that occur during the development and the post-deployment of machine learning classification algorithms. Using the Framingham coronary heart disease data as a case study, we show how to effectively select a probability cutoff to convert a regression model for a dichotomous variable into a classifier. We then compare the sampling distribution of the predictive performance of eight machine learning classification algorithms under four training/testing scenarios to test their generalizability and their potential to perpetuate biases. We show that both the Extreme Gradient Boosting, and Support Vector Machine are flawed when trained on an unbalanced data
    
[^147]: 划分还是征服？你应该提炼LLM的哪一部分？

    Divide-or-Conquer? Which Part Should You Distill Your LLM?

    [https://arxiv.org/abs/2402.15000](https://arxiv.org/abs/2402.15000)

    本文提出了一种将推理任务分解为问题分解阶段和问题解决阶段的策略，发现问题分解阶段相比问题解决更容易提炼为较小模型，并证实该策略胜过单阶段解决方案。

    

    最近的研究表明，大型语言模型（LLMs）在被鼓励先解决主要任务的子任务时可以更好地解决推理任务。本文设计了一种类似的策略，将推理任务分解为问题分解阶段和问题解决阶段，并展示该策略能够胜过单阶段解决方案。此外，我们假设与解决问题相比，分解阶段更容易被提炼为较小的模型，因为后者需要大量的领域知识，而前者只需要学习一般的问题解决策略。我们提出了提炼这两种能力的方法，并评估了它们对推理结果和推理成本的影响。我们发现我们可以提炼问题分解阶段，并同时在任务、数据集和模型之间实现良好的泛化。然而，要提炼问题解决阶段就更困难了。

    arXiv:2402.15000v1 Announce Type: new  Abstract: Recent methods have demonstrated that Large Language Models (LLMs) can solve reasoning tasks better when they are encouraged to solve subtasks of the main task first. In this paper we devise a similar strategy that breaks down reasoning tasks into a problem decomposition phase and a problem solving phase and show that the strategy is able to outperform a single stage solution. Further, we hypothesize that the decomposition should be easier to distill into a smaller model compared to the problem solving because the latter requires large amounts of domain knowledge while the former only requires learning general problem solving strategies. We propose methods to distill these two capabilities and evaluate their impact on reasoning outcomes and inference cost. We find that we can distill the problem decomposition phase and at the same time achieve good generalization across tasks, datasets, and models. However, it is harder to distill the pr
    
[^148]: 小型基准测试：用更少的示例评估LLM

    tinyBenchmarks: evaluating LLMs with fewer examples

    [https://arxiv.org/abs/2402.14992](https://arxiv.org/abs/2402.14992)

    本文研究了减少评估LLMs性能所需的评估次数的策略，并展示了在小规模示例上可以准确估计LLMs在多种基准测试上的性能。

    

    大型语言模型（LLMs）的多功能性导致创建了多种基准测试，彻底测试各种语言模型的能力。这些基准测试包含成千上万个示例，使得评估LLMs非常昂贵。本文研究了减少评估LLMs性能所需的评估次数的策略。例如，我们展示了要准确估计LLMs在MMLU上的性能（一个包含14K个示例的流行多选问答基准测试），只需要在100个精心挑选的示例上评估这个LLMs。我们发布了评估工具和流行基准测试的微型版本：Open LLM Leaderboard、MMLU、HELM和AlpacaEval 2.0。我们的实证分析表明，这些工具和微型基准测试足以可靠且高效地重现原始评估结果。

    arXiv:2402.14992v1 Announce Type: cross  Abstract: The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models' abilities. These benchmarks consist of tens of thousands of examples making evaluation of LLMs very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an LLM on several key benchmarks. For example, we show that to accurately estimate the performance of an LLM on MMLU, a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results.
    
[^149]: 量子理论与情境最优输运的应用

    Quantum Theory and Application of Contextual Optimal Transport

    [https://arxiv.org/abs/2402.14991](https://arxiv.org/abs/2402.14991)

    提出了一种首创的量子计算公式，用于情境化输送计划的摊销优化，并通过预测背景情境中药物剂量参数化的细胞类型分布的变化来验证方法，展示了捕捉剂量引起的细胞分布变化的能力。

    

    最优输运（Optimal Transport，OT）推动了机器学习在许多领域的应用。在测量数据（$\mu$，$\nu$）与上下文变量 $p_i$ 耦合的情况下，我们可以努力学习一个可以通过可能看不见的上下文参数化的全局输运映射。现有方法利用神经最优输运，并在很大程度上依赖于Brenier定理。在这里，我们提出了一种首创的量子计算公式，用于情境化输送计划的摊销优化。我们利用双随机矩阵和酉算符之间的直接联系，从而找到了最优输运和量子计算之间的自然联系。我们通过对合成和真实数据的验证，预测通过药物剂量参数化的细胞类型分布的变化作为背景情境。我们将我们的方法与几个基准进行了比较，结果显示我们的方法可以捕捉到剂量引起的细胞分布变化，甚至在一定程度上。

    arXiv:2402.14991v1 Announce Type: new  Abstract: Optimal Transport (OT) has fueled machine learning (ML) applications across many domains. In cases where paired data measurements ($\mu$, $\nu$) are coupled to a context variable $p_i$ , one may aspire to learn a global transportation map that can be parameterized through a potentially unseen con-text. Existing approaches utilize Neural OT and largely rely on Brenier's theorem. Here, we propose a first-of-its-kind quantum computing formulation for amortized optimization of contextualized transportation plans. We exploit a direct link between doubly stochastic matrices and unitary operators thus finding a natural connection between OT and quantum computation. We verify our method on synthetic and real data, by predicting variations in cell type distributions parameterized through drug dosage as context. Our comparisons to several baselines reveal that our method can capture dose-induced variations in cell distributions, even to some exten
    
[^150]: 分析不规则时间序列数据中的稳定神经随机微分方程

    Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data

    [https://arxiv.org/abs/2402.14989](https://arxiv.org/abs/2402.14989)

    神经常微分方程（Neural ODEs）的扩展——神经随机微分方程（Neural SDEs）在处理不规则时间序列数据中的稳定性和性能方面提出了重要指导，需要谨慎设计漂移和扩散函数以保持稳定性。

    

    实际时间序列数据中的不规则采样间隔和缺失值对于假设一致间隔和完整数据的传统方法构成挑战。神经常微分方程（Neural ODEs）提供了一种替代方法，利用神经网络与常微分方程求解器结合，通过参数化向量场学习连续潜在表示。神经随机微分方程（Neural SDEs）通过引入扩散项扩展了神经常微分方程，然而在处理不规则间隔和缺失值时，这种添加并不是微不足道的。因此，仔细设计漂移和扩散函数对于保持稳定性和增强性能至关重要，而粗心的选择可能导致出现没有强解、随机破坏或不稳定的Euler离散化等不利的性质，显著影响神经随机微分方程的性能。

    arXiv:2402.14989v1 Announce Type: cross  Abstract: Irregular sampling intervals and missing values in real-world time series data present challenges for conventional methods that assume consistent intervals and complete data. Neural Ordinary Differential Equations (Neural ODEs) offer an alternative approach, utilizing neural networks combined with ODE solvers to learn continuous latent representations through parameterized vector fields. Neural Stochastic Differential Equations (Neural SDEs) extend Neural ODEs by incorporating a diffusion term, although this addition is not trivial, particularly when addressing irregular intervals and missing values. Consequently, careful design of drift and diffusion functions is crucial for maintaining stability and enhancing performance, while incautious choices can result in adverse properties such as the absence of strong solutions, stochastic destabilization, or unstable Euler discretizations, significantly affecting Neural SDEs' performance. In 
    
[^151]: 可验证的提升树集成

    Verifiable Boosted Tree Ensembles

    [https://arxiv.org/abs/2402.14988](https://arxiv.org/abs/2402.14988)

    本研究将可验证学习从基本集成方法扩展到高级提升树集成，提出了一个伪多项式时间算法来验证鲁棒性，对基于$L_p$-范数的攻击者具有出色的性能。

    

    可验证学习倡导训练易于进行高效安全验证的机器学习模型。先前的研究表明，特定类的决策树集成，即称为大广泛集成，可以在多项式时间内针对任何基于范数的攻击者进行鲁棒性验证。本研究将可验证学习从基本集成方法（即硬多数投票）扩展到高级提升树集成，比如那些使用XGBoost或LightGBM训练的集成。我们的正式结果表明，在考虑基于$L_\infty$-范数的攻击者时，鲁棒性验证可以在多项式时间内实现，但对于其他基于范数的攻击者来说仍然是NP难的。尽管如此，我们提出了一个伪多项式时间算法来验证针对基于$L_p$-范数的攻击者的鲁棒性，其中$p \in \mathbb{N} \cup \{0\}$，在实践中具有出色的性能。我们的实验评估表明

    arXiv:2402.14988v1 Announce Type: new  Abstract: Verifiable learning advocates for training machine learning models amenable to efficient security verification. Prior research demonstrated that specific classes of decision tree ensembles -- called large-spread ensembles -- allow for robustness verification in polynomial time against any norm-based attacker. This study expands prior work on verifiable learning from basic ensemble methods (i.e., hard majority voting) to advanced boosted tree ensembles, such as those trained using XGBoost or LightGBM. Our formal results indicate that robustness verification is achievable in polynomial time when considering attackers based on the $L_\infty$-norm, but remains NP-hard for other norm-based attackers. Nevertheless, we present a pseudo-polynomial time algorithm to verify robustness against attackers based on the $L_p$-norm for any $p \in \mathbb{N} \cup \{0\}$, which in practice grants excellent performance. Our experimental evaluation shows th
    
[^152]: 在平滑数据上的经验风险最小化性能研究

    On the Performance of Empirical Risk Minimization with Smoothed Data

    [https://arxiv.org/abs/2402.14987](https://arxiv.org/abs/2402.14987)

    在数据是良好指定和平滑的情况下，对于经验风险最小化（ERM）与平方损失的性能，当类是可从 iid 数据中学习时，ERM能够实现次线性误差。

    

    为了避开在序贯决策中的统计和计算困难结果，最近的工作考虑了平滑的在线学习，其中假设每个时间点的数据分布在给定历史条件下相对于基础度量具有有界的似然比。虽然先前的工作已经证明了平滑性的好处，但它们要么假设基础度量对学习者是已知的，要么提出的算法在特殊情况下仅适用于计算效率低下。本研究研究了更一般的设置，即基础度量对学习者是\emph{未知}的情况，特别关注在数据明确定和平滑的情况下，当数据是良好指定时经验风险最小化（ERM）与平方损失的性能。我们展示，在这种设置下，只要类是可从iid数据中学习的，ERM就能够实现次线性误差；特别是，当数据是iid时，ERM实现的错误尺度为$\tilde O(

    arXiv:2402.14987v1 Announce Type: cross  Abstract: In order to circumvent statistical and computational hardness results in sequential decision-making, recent work has considered smoothed online learning, where the distribution of data at each time is assumed to have bounded likeliehood ratio with respect to a base measure when conditioned on the history. While previous works have demonstrated the benefits of smoothness, they have either assumed that the base measure is known to the learner or have presented computationally inefficient algorithms applying only in special cases. This work investigates the more general setting where the base measure is \emph{unknown} to the learner, focusing in particular on the performance of Empirical Risk Minimization (ERM) with square loss when the data are well-specified and smooth. We show that in this setting, ERM is able to achieve sublinear error whenever a class is learnable with iid data; in particular, ERM achieves error scaling as $\tilde O(
    
[^153]: 通过联邦学习增强隐私保护的协作信息共享--以保险行业为例

    Privacy-Enhancing Collaborative Information Sharing through Federated Learning -- A Case of the Insurance Industry

    [https://arxiv.org/abs/2402.14983](https://arxiv.org/abs/2402.14983)

    通过联邦学习实现跨多个保险行业数据集学习单一模型的方法，解决了数据隐私保护问题，提高了赔偿损失模型的预测准确性。

    

    这份报告展示了利用联邦学习（FL）的价值，通过在多个保险行业数据集之间学习单一模型而无需将数据集从一家公司分享给另一家公司，以改进赔偿损失建模的好处。FL的应用解决了两个最紧迫的问题：数据量有限和数据种类繁多，这些问题是由隐私顾虑、赔偿事件的罕见性、缺乏信息性评级因素等引起的。在每一轮FL中，合作者使用他们的本地私有数据计算模型的改进，这些见解被结合起来更新一个全局模型。这种见解的聚合相对于在每个合作者单独训练的模型，可以增加对赔偿损失预测的有效性。关键是，这种方法使得机器学习协作无需原始数据离开计算基础设施。

    arXiv:2402.14983v1 Announce Type: new  Abstract: The report demonstrates the benefits (in terms of improved claims loss modeling) of harnessing the value of Federated Learning (FL) to learn a single model across multiple insurance industry datasets without requiring the datasets themselves to be shared from one company to another. The application of FL addresses two of the most pressing concerns: limited data volume and data variety, which are caused by privacy concerns, the rarity of claim events, the lack of informative rating factors, etc.. During each round of FL, collaborators compute improvements on the model using their local private data, and these insights are combined to update a global model. Such aggregation of insights allows for an increase to the effectiveness in forecasting claims losses compared to models individually trained at each collaborator. Critically, this approach enables machine learning collaboration without the need for raw data to leave the compute infrast
    
[^154]: 人类大脑在听取真实和虚假音频时展现出不同模式：初步证据

    Human Brain Exhibits Distinct Patterns When Listening to Fake Versus Real Audio: Preliminary Evidence

    [https://arxiv.org/abs/2402.14982](https://arxiv.org/abs/2402.14982)

    人类大脑对真实和虚假音频有不同的反应模式，与深度伪造音频检测算法不同，这为深度伪造音频检测等领域的未来研究方向提供了重要的初步证据。

    

    本文研究了人类听取真实和虚假音频时大脑活动的变化。我们的初步结果表明，一种最先进的深度伪造音频检测算法所学习的表示，并没有显示出真实和虚假音频之间的清晰不同模式。相反，人类大脑活动，通过 EEG 测量，在个体接触虚假与真实音频时显示出不同的模式。这些初步证据为未来在深度伪造音频检测等领域提供了研究方向。

    arXiv:2402.14982v1 Announce Type: cross  Abstract: In this paper we study the variations in human brain activity when listening to real and fake audio. Our preliminary results suggest that the representations learned by a state-of-the-art deepfake audio detection algorithm, do not exhibit clear distinct patterns between real and fake audio. In contrast, human brain activity, as measured by EEG, displays distinct patterns when individuals are exposed to fake versus real audio. This preliminary evidence enables future research directions in areas such as deepfake audio detection.
    
[^155]: 数据预处理方法、特征选择技术和机器学习模型在不平衡遗传数据上提高分类和回归性能的比较分析

    Comparative Analysis of Data Preprocessing Methods, Feature Selection Techniques and Machine Learning Models for Improved Classification and Regression Performance on Imbalanced Genetic Data

    [https://arxiv.org/abs/2402.14980](https://arxiv.org/abs/2402.14980)

    研究比较了数据预处理、特征选择和模型选择对训练遗传数据集上模型性能的影响，发现异常值和倾斜会影响模型性能。

    

    基因组测序技术的快速发展导致了大量基因组数据的收集。研究人员可能有兴趣在这些数据上使用机器学习模型来预测基因突变的致病性或临床意义。然而，许多遗传数据集包含不平衡的目标变量，这给机器学习模型带来挑战：在回归任务中观察结果倾斜/不平衡，在分类任务中类别不平衡。遗传数据集通常具有高基数和倾斜的预测变量，这进一步增加了挑战。我们旨在研究数据预处理、特征选择技术和模型选择对训练在这些数据集上的模型性能的影响。我们使用5折交叉验证测量性能，并比较不同技术组合下的平均r平方和准确率指标。我们发现预测变量或目标变量中的异常值/倾斜会对模型的性能产生影响。

    arXiv:2402.14980v1 Announce Type: cross  Abstract: Rapid advancements in genome sequencing have led to the collection of vast amounts of genomics data. Researchers may be interested in using machine learning models on such data to predict the pathogenicity or clinical significance of a genetic mutation. However, many genetic datasets contain imbalanced target variables that pose challenges to machine learning models: observations are skewed/imbalanced in regression tasks or class-imbalanced in classification tasks. Genetic datasets are also often high-cardinal and contain skewed predictor variables, which poses further challenges. We aimed to investigate the effects of data preprocessing, feature selection techniques, and model selection on the performance of models trained on these datasets. We measured performance with 5-fold cross-validation and compared averaged r-squared and accuracy metrics across different combinations of techniques. We found that outliers/skew in predictor or t
    
[^156]: 优化语言模型以符合人类偏好是一个因果推断问题

    Optimizing Language Models for Human Preferences is a Causal Inference Problem

    [https://arxiv.org/abs/2402.14979](https://arxiv.org/abs/2402.14979)

    本文首次提出将语言模型优化视为一个因果问题，提出了因果偏好优化方法并通过双重稳健CPO(DR-CPO)降低了替代目标的方差。

    

    随着大型语言模型(LLMs)在学术和商业领域的广泛应用，越来越多的人对允许语言模型生成符合人类偏好文本的方法产生了兴趣。本文首次探索了从直接结果数据集中针对人类偏好进行语言模型优化，其中每个样本由一段文本和一个衡量读者响应的相关数值结果组成。我们首次提出应将语言模型优化视为一个因果问题，以确保模型正确学习文本与结果之间的关系。我们正式化了这个因果语言优化问题，并开发了一种方法--因果偏好优化(CPO)--来解决该问题的无偏替代目标。我们进一步使用双重稳健的CPO(DR-CPO)扩展CPO，降低了替代目标的方差，同时保留了明显强有力的保证。

    arXiv:2402.14979v1 Announce Type: cross  Abstract: As large language models (LLMs) see greater use in academic and commercial settings, there is increasing interest in methods that allow language models to generate texts aligned with human preferences. In this paper, we present an initial exploration of language model optimization for human preferences from direct outcome datasets, where each sample consists of a text and an associated numerical outcome measuring the reader's response. We first propose that language model optimization should be viewed as a causal problem to ensure that the model correctly learns the relationship between the text and the outcome. We formalize this causal language optimization problem, and we develop a method--causal preference optimization (CPO)--that solves an unbiased surrogate objective for the problem. We further extend CPO with doubly robust CPO (DR-CPO), which reduces the variance of the surrogate objective while retaining provably strong guarante
    
[^157]: Mudjacking：修补基础模型中的后门漏洞

    Mudjacking: Patching Backdoor Vulnerabilities in Foundation Models

    [https://arxiv.org/abs/2402.14977](https://arxiv.org/abs/2402.14977)

    Mudjacking提出了第一个方法来修补基础模型以消除后门漏洞，通过优化问题和梯度下降方法解决此问题。

    

    基础模型已成为人工智能生态系统的支柱。特别是，基础模型可用作通用特征提取器，用于构建各种下游分类器。然而，基础模型容易遭受后门攻击，而后门基础模型是人工智能生态系统的单点故障，例如，多个下游分类器同时继承后门漏洞。在本研究中，我们提出了Mudjacking，这是第一个修补基础模型以消除后门的方法。具体来说，一旦部署了带有后门的基础模型，并检测到嵌入误分类触发器的输入，Mudjacking会调整基础模型的参数以消除后门。我们将修补基础模型形式化为优化问题，并提出了一种基于梯度下降的方法来解决它。我们在视觉和语言基础模型上评估了Mudjacking，使用了十一个基准数据集。

    arXiv:2402.14977v1 Announce Type: cross  Abstract: Foundation model has become the backbone of the AI ecosystem. In particular, a foundation model can be used as a general-purpose feature extractor to build various downstream classifiers. However, foundation models are vulnerable to backdoor attacks and a backdoored foundation model is a single-point-of-failure of the AI ecosystem, e.g., multiple downstream classifiers inherit the backdoor vulnerabilities simultaneously. In this work, we propose Mudjacking, the first method to patch foundation models to remove backdoors. Specifically, given a misclassified trigger-embedded input detected after a backdoored foundation model is deployed, Mudjacking adjusts the parameters of the foundation model to remove the backdoor. We formulate patching a foundation model as an optimization problem and propose a gradient descent based method to solve it. We evaluate Mudjacking on both vision and language foundation models, eleven benchmark datasets, f
    
[^158]: 在深度基础潜空间内的无监督领域自适应

    Unsupervised Domain Adaptation within Deep Foundation Latent Spaces

    [https://arxiv.org/abs/2402.14976](https://arxiv.org/abs/2402.14976)

    提出了一种在深度基础潜空间内进行无监督领域自适应的方法，通过分析和定性解释，展示了该方法可以优于现有基线，并显示了尚未解决的局限性。

    

    基于Vision Transformer的基础模型，如ViT或Dino-V2，旨在解决无需或很少微调特征的问题。通过一组原型网络的设置，我们分析了这种基础模型在无需在源域或目标域进行微调情况下，能够解决无监督领域自适应的程度。通过定量分析以及对决策过程的定性解释，我们证明了建议方法可以改进现有基线，并展示了这种方法的局限性有待解决。

    arXiv:2402.14976v1 Announce Type: cross  Abstract: The vision transformer-based foundation models, such as ViT or Dino-V2, are aimed at solving problems with little or no finetuning of features. Using a setting of prototypical networks, we analyse to what extent such foundation models can solve unsupervised domain adaptation without finetuning over the source or target domain. Through quantitative analysis, as well as qualitative interpretations of decision making, we demonstrate that the suggested method can improve upon existing baselines, as well as showcase the limitations of such approach yet to be solved.
    
[^159]: 在非欧几里得空间中实现空间透明的AI分类：MxIF肿瘤数据应用

    Towards Spatially-Lucid AI Classification in Non-Euclidean Space: An Application for MxIF Oncology Data

    [https://arxiv.org/abs/2402.14974](https://arxiv.org/abs/2402.14974)

    发展了一个使用空间集合框架的分类器，可以根据点的排列在非欧几里得空间中区分两个类别，对于肿瘤学等应用具有重要意义

    

    针对来自不同地点类型的多类别点集，我们的目标是开发一个空间透明的分类器，可以根据点的排列区分两个类别。这个问题对于许多应用非常重要，比如肿瘤学，用于分析免疫-肿瘤关系和设计新的免疫治疗方法。这个问题具有挑战性，因为需要考虑空间变化和可解释性需求。以前提出的技术要求密集的训练数据，或者在处理单个地点类型内的显著空间变异性方面能力有限。最重要的是，这些深度神经网络（DNN）方法没有设计用于在非欧几里得空间中工作，特别是点集。现有的非欧几里得DNN方法局限于一刀切的方法。我们探索了一种空间集合框架，明确使用不同的训练策略，包括加权距离学习率和空间域自适应。

    arXiv:2402.14974v1 Announce Type: cross  Abstract: Given multi-category point sets from different place-types, our goal is to develop a spatially-lucid classifier that can distinguish between two classes based on the arrangements of their points. This problem is important for many applications, such as oncology, for analyzing immune-tumor relationships and designing new immunotherapies. It is challenging due to spatial variability and interpretability needs. Previously proposed techniques require dense training data or have limited ability to handle significant spatial variability within a single place-type. Most importantly, these deep neural network (DNN) approaches are not designed to work in non-Euclidean space, particularly point sets. Existing non-Euclidean DNN methods are limited to one-size-fits-all approaches. We explore a spatial ensemble framework that explicitly uses different training strategies, including weighted-distance learning rate and spatial domain adaptation, on v
    
[^160]: GenCeption：使用未标记的单模态数据评估多模态LLM

    GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data

    [https://arxiv.org/abs/2402.14973](https://arxiv.org/abs/2402.14973)

    提出了一种名为GenCeption的新型MLLM评估框架，可以仅利用单模态数据评估跨模态语义一致性，并有效反映模型产生幻觉的倾向，具有较强的相关性和潜力于流行的MLLM基准结果。

    

    多模态大型语言模型（MLLMs）通常使用昂贵的带标注的多模态基准进行评估。然而，这些基准通常难以跟上MLLM评估的快速发展要求。我们提出了GenCeption，这是一个新颖的无需注释的MLLM评估框架，仅需要单模态数据来评估跨模态语义一致性，并反映出模型产生幻觉的倾向。类似于流行的DrawCeption游戏，GenCeption从一个非文本样本开始，并经历一系列迭代的描述和生成步骤。迭代之间的语义漂移使用GC@T指标进行量化。我们的实证发现验证了GenCeption的有效性，并显示出与流行的MLLM基准结果的强相关性。GenCeption可以通过利用普遍存在且以前未见的单模态数据来扩展，以减轻训练数据的污染。

    arXiv:2402.14973v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) are commonly evaluated using costly annotated multimodal benchmarks. However, these benchmarks often struggle to keep pace with the rapidly advancing requirements of MLLM evaluation. We propose GenCeption, a novel and annotation-free MLLM evaluation framework that merely requires unimodal data to assess inter-modality semantic coherence and inversely reflects the models' inclination to hallucinate. Analogous to the popular DrawCeption game, GenCeption initiates with a non-textual sample and undergoes a series of iterative description and generation steps. Semantic drift across iterations is quantified using the GC@T metric. Our empirical findings validate GenCeption's efficacy, showing strong correlations with popular MLLM benchmarking results. GenCeption may be extended to mitigate training data contamination by utilizing ubiquitous, previously unseen unimodal data.
    
[^161]: 光滑自适应假设迁移学习

    Smoothness Adaptive Hypothesis Transfer Learning

    [https://arxiv.org/abs/2402.14966](https://arxiv.org/abs/2402.14966)

    本文提出了光滑自适应迁移学习（SATL）算法，通过在两个阶段均采用高斯核，使估计器能够适应目标/源及其偏移函数的未知光滑性。

    

    许多现有的基于核的两阶段假设迁移学习算法在不同阶段均采用相同的核正则化，并依赖于函数的已知光滑性来实现最优性。因此，在实践中，它们未能适应目标/源及其偏移之间的变化和未知光滑性。本文通过提出光滑自适应迁移学习（SATL），一个基于两阶段核岭回归（KRR）的算法，解决了这些问题。我们首先证明，在目标专用KRR学习中采用错误指定的固定带宽高斯核可以实现极小化最优性，并推导出一种适应未知Sobolev光滑性的自适应过程。利用这些结果，SATL在两阶段均采用高斯核，以使估计量能够适应目标/源及其偏移函数的未知光滑性。我们推导了学习问题在过量风险中的极小值下限，并表明

    arXiv:2402.14966v1 Announce Type: cross  Abstract: Many existing two-phase kernel-based hypothesis transfer learning algorithms employ the same kernel regularization across phases and rely on the known smoothness of functions to obtain optimality. Therefore, they fail to adapt to the varying and unknown smoothness between the target/source and their offset in practice. In this paper, we address these problems by proposing Smoothness Adaptive Transfer Learning (SATL), a two-phase kernel ridge regression(KRR)-based algorithm. We first prove that employing the misspecified fixed bandwidth Gaussian kernel in target-only KRR learning can achieve minimax optimality and derive an adaptive procedure to the unknown Sobolev smoothness. Leveraging these results, SATL employs Gaussian kernels in both phases so that the estimators can adapt to the unknown smoothness of the target/source and their offset function. We derive the minimax lower bound of the learning problem in excess risk and show that
    
[^162]: 弹性时间步长的强化学习

    Reinforcement Learning with Elastic Time Steps

    [https://arxiv.org/abs/2402.14961](https://arxiv.org/abs/2402.14961)

    SEAC是一种弹性时间步长的离策略演员-评论家算法，通过可变持续时间的时间步长，使代理能够根据情况改变控制频率，在模拟环境中表现优异。

    

    传统的强化学习（RL）算法通常应用于机器人学习以以固定控制频率执行动作的控制器。鉴于RL算法的离散性质，它们对控制频率的选择的影响视而不见：找到正确的控制频率可能很困难，错误往往会导致过度使用计算资源甚至导致无法收敛。我们提出了软弹性演员-评论家（SEAC）, 一种新颖的离策略演员-评论家算法来解决这个问题。SEAC实现了弹性时间步长，即具有已知变化持续时间的时间步长，允许代理根据情况改变其控制频率。在实践中，SEAC仅在必要时应用控制，最小化计算资源和数据使用。我们在模拟环境中评估了SEAC在牛顿运动学迷宫导航任务和三维赛车视频游戏Trackmania中的能力。SEAC在表现上优于SAC基线。

    arXiv:2402.14961v1 Announce Type: cross  Abstract: Traditional Reinforcement Learning (RL) algorithms are usually applied in robotics to learn controllers that act with a fixed control rate. Given the discrete nature of RL algorithms, they are oblivious to the effects of the choice of control rate: finding the correct control rate can be difficult and mistakes often result in excessive use of computing resources or even lack of convergence.   We propose Soft Elastic Actor-Critic (SEAC), a novel off-policy actor-critic algorithm to address this issue. SEAC implements elastic time steps, time steps with a known, variable duration, which allow the agent to change its control frequency to adapt to the situation. In practice, SEAC applies control only when necessary, minimizing computational resources and data usage.   We evaluate SEAC's capabilities in simulation in a Newtonian kinematics maze navigation task and on a 3D racing video game, Trackmania. SEAC outperforms the SAC baseline in t
    
[^163]: 大多数自监督学习方法背后的共同稳定机制

    The Common Stability Mechanism behind most Self-Supervised Learning Approaches

    [https://arxiv.org/abs/2402.14957](https://arxiv.org/abs/2402.14957)

    自监督学习方法的共同稳定机制是通过引入有用的归纳偏差来学习有意义的视觉表示，并避免坍缩，这些方法尽管有不同的公式，但隐式优化了类似的目标函数。

    

    过去几年见证了自监督学习（SSL）领域的巨大进展，其成功归功于在学习过程中引入了有用的归纳偏差，以学习有意义的视觉表示，同时避免坍缩。本文提供了一个框架来解释这些不同SSL技术的稳定机制，讨论了像SimCLR这样的对比技术、像BYOL、SWAV、SimSiam、Barlow Twins和DINO这样的非对比技术的工作机制，并提出了一个论点，即尽管有不同的公式，这些方法隐式优化了类似的目标函数，即最小化幅度。

    arXiv:2402.14957v1 Announce Type: cross  Abstract: Last couple of years have witnessed a tremendous progress in self-supervised learning (SSL), the success of which can be attributed to the introduction of useful inductive biases in the learning process to learn meaningful visual representations while avoiding collapse. These inductive biases and constraints manifest themselves in the form of different optimization formulations in the SSL techniques, e.g. by utilizing negative examples in a contrastive formulation, or exponential moving average and predictor in BYOL and SimSiam. In this paper, we provide a framework to explain the stability mechanism of these different SSL techniques: i) we discuss the working mechanism of contrastive techniques like SimCLR, non-contrastive techniques like BYOL, SWAV, SimSiam, Barlow Twins, and DINO; ii) we provide an argument that despite different formulations these methods implicitly optimize a similar objective function, i.e. minimizing the magnitu
    
[^164]: 一个线性Transformer块的上下文学习：MLP组件和一步GD初始化的优势

    In-Context Learning of a Linear Transformer Block: Benefits of the MLP Component and One-Step GD Initialization

    [https://arxiv.org/abs/2402.14951](https://arxiv.org/abs/2402.14951)

    该论文研究了结合线性注意力和线性MLP组件的线性Transformer块在上下文学习中的性能，证明了其在线性回归任务中几乎可以达到贝叶斯最优风险，并且与一步梯度下降估计器有对应关系。

    

    我们研究了结合线性注意力组件和线性多层感知器（MLP）组件的线性Transformer块（LTB）的上下文学习（ICL）能力。对于具有高斯先验和非零均值的线性回归的ICL，我们表明LTB可以实现几乎贝叶斯最优的ICL风险。相比之下，仅使用线性注意力必须产生不可避免的附加近似误差。此外，我们建立了LTB与具有可学习初始化的一步梯度下降估计器（$\mathsf{GD}-\mathbf{\beta}$）之间的对应关系，从每个$\mathsf{GD}-\mathbf{\beta}$估计器可以通过LTB估计器实现，到最小化类内ICL风险的每个最优LTB估计器实际上是一个$\mathsf{GD}-\mathbf{\beta}$估计器。最后，我们表明$\mathsf{GD}-\mathbf{\beta}$估计器可以通过梯度优化高效地优化。

    arXiv:2402.14951v1 Announce Type: cross  Abstract: We study the \emph{in-context learning} (ICL) ability of a \emph{Linear Transformer Block} (LTB) that combines a linear attention component and a linear multi-layer perceptron (MLP) component. For ICL of linear regression with a Gaussian prior and a \emph{non-zero mean}, we show that LTB can achieve nearly Bayes optimal ICL risk. In contrast, using only linear attention must incur an irreducible additive approximation error. Furthermore, we establish a correspondence between LTB and one-step gradient descent estimators with learnable initialization ($\mathsf{GD}\text{-}\mathbf{\beta}$), in the sense that every $\mathsf{GD}\text{-}\mathbf{\beta}$ estimator can be implemented by an LTB estimator and every optimal LTB estimator that minimizes the in-class ICL risk is effectively a $\mathsf{GD}\text{-}\mathbf{\beta}$ estimator. Finally, we show that $\mathsf{GD}\text{-}\mathbf{\beta}$ estimators can be efficiently optimized with gradient f
    
[^165]: 利用AI Transformer模型增强电能质量事件分类

    Enhancing Power Quality Event Classification with AI Transformer Models

    [https://arxiv.org/abs/2402.14949](https://arxiv.org/abs/2402.14949)

    本论文提出利用启用注意力的Transformer作为工具，通过深度学习框架来准确分类考虑测量噪声、直流偏移和电压信号幅度频率变化等条件下的电能质量事件，无需额外特征提取，精度高于其他学习技术。

    

    最近，人们越来越感兴趣利用机器学习来精确分类电能质量事件（PQEs）。然而，大多数研究都是在假设理想情况下进行的，而现实中我们可能会遇到测量噪声、直流偏移以及电压信号幅度和频率的变化。本文在先前使用深度学习进行PQE分类工作的基础上，提出了一个利用启用注意力的Transformer作为工具来准确分类考虑上述因素的PQE的深度学习框架。所提出的框架可以直接在电压信号上操作，无需单独进行特征提取或计算阶段。我们的结果表明，所提出的框架优于最近提出的基于学习的技术。它可以在上述条件下精确分类PQEs，准确率在99.81%至91.43%之间变化。

    arXiv:2402.14949v1 Announce Type: new  Abstract: Recently, there has been a growing interest in utilizing machine learning for accurate classification of power quality events (PQEs). However, most of these studies are performed assuming an ideal situation, while in reality, we can have measurement noise, DC offset, and variations in the voltage signal's amplitude and frequency. Building on the prior PQE classification works using deep learning, this paper proposes a deep-learning framework that leverages attention-enabled Transformers as a tool to accurately classify PQEs under the aforementioned considerations. The proposed framework can operate directly on the voltage signals with no need for a separate feature extraction or calculation phase. Our results show that the proposed framework outperforms recently proposed learning-based techniques. It can accurately classify PQEs under the aforementioned conditions with an accuracy varying between 99.81%$-$91.43% depending on the signal-t
    
[^166]: 重新审视远程监督命名实体识别：一个新的基准和简单方法

    Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach

    [https://arxiv.org/abs/2402.14948](https://arxiv.org/abs/2402.14948)

    提出了基于课程的正无标记学习 CuPUL 方法，能够显著降低嘈杂标签的影响，胜过现有方法

    

    本文深入探讨了在远程监督（DS-NER）框架下的命名实体识别（NER），主要挑战在于标签质量受到误差的影响，如假阳性、假阴性和正向类型错误。我们批判性地评估了当前DS-NER方法的有效性，使用了一个名为QTL的真实世界基准数据集，揭示它们的性能往往不符合预期。为了解决标签噪声普遍问题，我们引入了一种简单而有效的方法，基于课程的正无标记学习（CuPUL），在训练过程中策略性地从“易”和更清洁的样本开始，以增强模型对嘈杂样本的韧性。我们的实证结果突出了CuPUL减少嘈杂标签影响并胜过现有方法的能力。

    arXiv:2402.14948v1 Announce Type: new  Abstract: This paper delves into Named Entity Recognition (NER) under the framework of Distant Supervision (DS-NER), where the main challenge lies in the compromised quality of labels due to inherent errors such as false positives, false negatives, and positive type errors. We critically assess the efficacy of current DS-NER methodologies using a real-world benchmark dataset named QTL, revealing that their performance often does not meet expectations. To tackle the prevalent issue of label noise, we introduce a simple yet effective approach, Curriculum-based Positive-Unlabeled Learning CuPUL, which strategically starts on "easy" and cleaner samples during the training process to enhance model resilience to noisy samples. Our empirical results highlight the capability of CuPUL to significantly reduce the impact of noisy labels and outperform existing methods.
    
[^167]: SoK: 分析对抗性样本：研究对手知识的框架

    SoK: Analyzing Adversarial Examples: A Framework to Study Adversary Knowledge

    [https://arxiv.org/abs/2402.14937](https://arxiv.org/abs/2402.14937)

    提出一个理论框架来研究对手知识，通过对抗性样本游戏标准化攻击，对图像分类领域的最新攻击进行分类，从而系统地总结了对手知识，得出新的结论。

    

    对抗性样本是恶意输入到机器学习模型中的内容，会引发误分类。这种类型的攻击已经研究了近十年，我们发现在发起攻击时对对手知识的研究和形式化存在缺乏。这导致了一个复杂的攻击研究领域，具有难以比较的威胁模型和攻击方式。我们专注于图像分类领域，并提供了一个受启发于序理论工作的框架来研究对手知识。我们提出了一个对抗性样本游戏，受到密码游戏的启发，用以标准化攻击。我们调查了图像分类领域的最新攻击，并在我们的框架中对攻击者的知识进行分类。通过这种系统化方法，我们汇编了结果，既确认了关于对手知识的现有观点，例如关于被攻击模型信息的有效性，也让我们得出了关于对抗性样本的新结论。

    arXiv:2402.14937v1 Announce Type: new  Abstract: Adversarial examples are malicious inputs to machine learning models that trigger a misclassification. This type of attack has been studied for close to a decade, and we find that there is a lack of study and formalization of adversary knowledge when mounting attacks. This has yielded a complex space of attack research with hard-to-compare threat models and attacks. We focus on the image classification domain and provide a theoretical framework to study adversary knowledge inspired by work in order theory. We present an adversarial example game, inspired by cryptographic games, to standardize attacks. We survey recent attacks in the image classification domain and classify their adversary's knowledge in our framework. From this systematization, we compile results that both confirm existing beliefs about adversary knowledge, such as the potency of information about the attacked model as well as allow us to derive new conclusions on the di
    
[^168]: 在没有访问敏感群体的情况下实现联邦公平性

    Federated Fairness without Access to Sensitive Groups

    [https://arxiv.org/abs/2402.14929](https://arxiv.org/abs/2402.14929)

    提出了一种不依赖于预定义敏感群体定义或额外标签的方法，通过一个超参数实现公平性和效用之间的权衡，保证任何足够大的人群子集能获得至少最低效用性能。

    

    当前联邦学习中关于群体公平性的方法都假设在训练期间存在预定义和标记的敏感群体。然而，由于从新兴法规到受保护群体的动态和位置依赖性等多种因素，这一假设在许多实际情况下可能不合适。在这项工作中，我们提出了一种新的方法来保证群体公平性，不依赖于任何预定义的敏感群体的定义或额外的标签。我们的目标允许联邦学习学习一个帕累托有效的全局模型，确保最坏情况下的群体公平性，并且通过一个超参数，实现公平性和效用之间的权衡，仅受到群体大小约束。这意味着任何足够大的人群子集都保证能从模型中获得至少的最低效用性能。所提出的目标涵盖了现有方法作为特殊案例，

    arXiv:2402.14929v1 Announce Type: cross  Abstract: Current approaches to group fairness in federated learning assume the existence of predefined and labeled sensitive groups during training. However, due to factors ranging from emerging regulations to dynamics and location-dependency of protected groups, this assumption may be unsuitable in many real-world scenarios. In this work, we propose a new approach to guarantee group fairness that does not rely on any predefined definition of sensitive groups or additional labels. Our objective allows the federation to learn a Pareto efficient global model ensuring worst-case group fairness and it enables, via a single hyper-parameter, trade-offs between fairness and utility, subject only to a group size constraint. This implies that any sufficiently large subset of the population is guaranteed to receive at least a minimum level of utility performance from the model. The proposed objective encompasses existing approaches as special cases, such
    
[^169]: 学习逆运动学以实现自动车漂移

    Learning Inverse Kinodynamics for Autonomous Vehicle Drifting

    [https://arxiv.org/abs/2402.14928](https://arxiv.org/abs/2402.14928)

    通过数据驱动的方法学习小型自动车的运动学模型，特别是为了实现高速圆形导航和自主漂移，帮助车辆学习世界状态并避开障碍物。

    

    在这项工作中，我们探索了一种基于数据驱动的学习方法，用于学习小型自动车的运动学模型，并观察其对运动规划特别是自主漂移的影响。在现实世界中执行运动规划时，存在许多导致错误的原因，计划的内容通常与实际汽车上执行的内容不同。基于惯性测量和执行命令学习动力学规划器可以帮助我们学习世界状态。在我们的情况下，我们将目光转向漂移领域；漂移是一种复杂的演练，需要足够平滑的表面、足够高的速度和速度的急剧变化。我们尝试学习这些漂移演练的动力学模型，并尝试减小车辆的侧滑。我们的方法能够学习高速圆形航行的运动学模型，并能够通过校正自主高速漂移上的障碍物来避免障碍物。

    arXiv:2402.14928v1 Announce Type: cross  Abstract: In this work, we explore a data-driven learning-based approach to learning the kinodynamic model of a small autonomous vehicle, and observe the effect it has on motion planning, specifically autonomous drifting. When executing a motion plan in the real world, there are numerous causes for error, and what is planned is often not what is executed on the actual car. Learning a kinodynamic planner based off of inertial measurements and executed commands can help us learn the world state. In our case, we look towards the realm of drifting; it is a complex maneuver that requires a smooth enough surface, high enough speed, and a drastic change in velocity. We attempt to learn the kinodynamic model for these drifting maneuvers, and attempt to tighten the slip of the car. Our approach is able to learn a kinodynamic model for high-speed circular navigation, and is able to avoid obstacles on an autonomous drift at high speed by correcting an exec
    
[^170]: 提升关系学习的全注意力机制

    Boosting gets full Attention for Relational Learning

    [https://arxiv.org/abs/2402.14926](https://arxiv.org/abs/2402.14926)

    引入了适合结构化数据的注意力机制，与树模型结合，用于（梯度）提升的训练。

    

    在基准监督机器学习中，常常会遇到平面化的表格数据，即由一个$m \times d$（行，列）文件组成，但现实世界中有很多情况是由一组带有结构关系的表格描述的观察数据。本文引入了一个适合于结构化数据的注意力机制，与树模型结合在（梯度）提升的训练背景下。

    arXiv:2402.14926v1 Announce Type: new  Abstract: More often than not in benchmark supervised ML, tabular data is flat, i.e. consists of a single $m \times d$ (rows, columns) file, but cases abound in the real world where observations are described by a set of tables with structural relationships. Neural nets-based deep models are a classical fit to incorporate general topological dependence among description features (pixels, words, etc.), but their suboptimality to tree-based models on tabular data is still well documented. In this paper, we introduce an attention mechanism for structured data that blends well with tree-based models in the training context of (gradient) boosting. Each aggregated model is a tree whose training involves two steps: first, simple tabular models are learned descending tables in a top-down fashion with boosting's class residuals on tables' features. Second, what has been learned progresses back bottom-up via attention and aggregation mechanisms, progressive
    
[^171]: 高效无偏稀疏化

    Efficient Unbiased Sparsification

    [https://arxiv.org/abs/2402.14925](https://arxiv.org/abs/2402.14925)

    该论文描述了对于排列不变或者可加可分的分裂函数，高效的无偏稀疏化特征。

    

    一个向量$p\in \mathbb{R}^n$的无偏$m$-稀疏化是一个具有平均值为$p$，最多有$m<n$个非零坐标的随机向量$Q\in \mathbb{R}^n。 无偏稀疏化可以压缩原始向量而不引入偏差；它出现在各种情境中，比如联邦学习和采样稀疏概率分布。 理想情况下，无偏稀疏化还应该最小化一个度量$Q$与原始$p$之间距离有多远的分裂函数$\mathsf{Div}(Q,p)$的期望值。 如果$Q$在这个意义上是最优的，那么我们称之为高效。 我们的主要结果描述了对于既是排列不变又是可加可分的分裂函数的高效无偏稀疏化。 令人惊讶的是，排列不变分裂函数的表征对于分裂函数的选择是健壮的，也就是说，我们针对平方欧氏距离的最优$Q$的类与我们的类重合了op

    arXiv:2402.14925v1 Announce Type: cross  Abstract: An unbiased $m$-sparsification of a vector $p\in \mathbb{R}^n$ is a random vector $Q\in \mathbb{R}^n$ with mean $p$ that has at most $m<n$ nonzero coordinates. Unbiased sparsification compresses the original vector without introducing bias; it arises in various contexts, such as in federated learning and sampling sparse probability distributions. Ideally, unbiased sparsification should also minimize the expected value of a divergence function $\mathsf{Div}(Q,p)$ that measures how far away $Q$ is from the original $p$. If $Q$ is optimal in this sense, then we call it efficient. Our main results describe efficient unbiased sparsifications for divergences that are either permutation-invariant or additively separable. Surprisingly, the characterization for permutation-invariant divergences is robust to the choice of divergence function, in the sense that our class of optimal $Q$ for squared Euclidean distance coincides with our class of op
    
[^172]: 针对预训练模型的知识蒸馏的实践见解

    Practical Insights into Knowledge Distillation for Pre-Trained Models

    [https://arxiv.org/abs/2402.14922](https://arxiv.org/abs/2402.14922)

    研究对知识蒸馏在预训练模型中的应用进行了深入比较，包括优化的温度和权重参数的调整，以及数据分区KD，揭示了最有效的知识蒸馏策略。

    

    这项研究探讨了在预训练模型中对知识蒸馏（KD）过程的增强，这是知识传输中一个新兴领域，并对分布式训练和联邦学习环境产生重要影响。尽管采用了许多知识蒸馏方法来在预训练模型之间传递知识，但在这些场景中了解知识蒸馏的应用仍然缺乏全面的理解。我们的研究对多种知识蒸馏技术进行了广泛比较，包括标准KD、经过优化温度和权重参数调整的KD、深度相互学习以及数据分区KD。我们评估这些方法在不同数据分布策略下的表现，以确定每种方法最有效的情境。通过详细研究超参数调整，结合广泛的网格搜索评估来获取信息

    arXiv:2402.14922v1 Announce Type: cross  Abstract: This research investigates the enhancement of knowledge distillation (KD) processes in pre-trained models, an emerging field in knowledge transfer with significant implications for distributed training and federated learning environments. These environments benefit from reduced communication demands and accommodate various model architectures. Despite the adoption of numerous KD approaches for transferring knowledge among pre-trained models, a comprehensive understanding of KD's application in these scenarios is lacking. Our study conducts an extensive comparison of multiple KD techniques, including standard KD, tuned KD (via optimized temperature and weight parameters), deep mutual learning, and data partitioning KD. We assess these methods across various data distribution strategies to identify the most effective contexts for each. Through detailed examination of hyperparameter tuning, informed by extensive grid search evaluations, w
    
[^173]: MobileLLM：优化亚十亿参数语言模型以用于设备端应用

    MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases

    [https://arxiv.org/abs/2402.14905](https://arxiv.org/abs/2402.14905)

    MobileLLM通过优化模型架构，采用深度和瘦身结构、嵌入共享和分组查询注意机制，实现了2.7%/4.3%的准确率提升，并提出了一种无需增加模型大小且仅有极小延迟开销的块状权重共享方法

    

    本文解决了移动设备上高效的大型语言模型(LLMs)的迫切需求问题，这是由于云成本和延迟问题不断增加所导致的。我们专注于设计具有不到十亿参数的顶级LLMs，这是移动部署的实际选择。与普遍的观点相反，强调数据和参数数量在确定模型质量方面的关键作用，我们的研究强调了亚十亿规模LLMs的模型架构的重要性。利用深度和瘦身结构，再加上嵌入共享和分组查询注意机制，我们建立了一个强大的基准网络，称为MobileLLM，其在将近125M/350M先进模型上分别获得了惊人的2.7%/4.3%的准确率提升。此外，我们提出了一种立即的块状权重共享方法，不增加模型大小，且仅具有极小的延迟开销。由此产生的模型被命名为MobileLLM-L

    arXiv:2402.14905v1 Announce Type: cross  Abstract: This paper addresses the growing need for efficient large language models (LLMs) on mobile devices, driven by increasing cloud costs and latency concerns. We focus on designing top-quality LLMs with fewer than a billion parameters, a practical choice for mobile deployment. Contrary to prevailing belief emphasizing the pivotal role of data and parameter quantity in determining model quality, our investigation underscores the significance of model architecture for sub-billion scale LLMs. Leveraging deep and thin architectures, coupled with embedding sharing and grouped-query attention mechanisms, we establish a strong baseline network denoted as MobileLLM, which attains a remarkable 2.7%/4.3% accuracy boost over preceding 125M/350M state-of-the-art models. Additionally, we propose an immediate block-wise weight sharing approach with no increase in model size and only marginal latency overhead. The resultant models, denoted as MobileLLM-L
    
[^174]: 数字水印使语言模型具有放射性

    Watermarking Makes Language Models Radioactive

    [https://arxiv.org/abs/2402.14904](https://arxiv.org/abs/2402.14904)

    本文研究了LLM生成文本的放射性，表明使用数字水印训练数据能更容易检测到，同时也展示了即使只有很少比例的水印训练文本，仍可以高置信度地检测出使用数字水印进行微调的情况。

    

    本文研究了LLM生成的文本的放射性，即是否可以检测到这种输入被用作训练数据。传统方法如成员推断可以以一定水平的准确性进行这种检测。我们表明，带有数字水印的训练数据留下的痕迹比成员推断更容易检测且更可靠。我们将污染水平与水印的鲁棒性、在训练集中的比例和微调过程联系起来。特别是我们展示，即使只有5％的训练文本被数字水印标记，训练在带有数字水印的合成指令上仍然可以具有高置信度（p值<1e-5）被检测到。因此，原本设计用于检测机器生成文本的LLM水印技术，使我们能够轻松确定带有数字水印的LLM的输出是否被用来对另一个LLM进行微调。

    arXiv:2402.14904v1 Announce Type: cross  Abstract: This paper investigates the radioactivity of LLM-generated texts, i.e. whether it is possible to detect that such input was used as training data. Conventional methods like membership inference can carry out this detection with some level of accuracy. We show that watermarked training data leaves traces easier to detect and much more reliable than membership inference. We link the contamination level to the watermark robustness, its proportion in the training set, and the fine-tuning process. We notably demonstrate that training on watermarked synthetic instructions can be detected with high confidence (p-value < 1e-5) even when as little as 5% of training text is watermarked. Thus, LLM watermarking, originally designed for detecting machine-generated text, gives the ability to easily identify if the outputs of a watermarked LLM were used to fine-tune another LLM.
    
[^175]: Tokenization计数：Tokenization对前沿LLMs中算术的影响

    Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs

    [https://arxiv.org/abs/2402.14903](https://arxiv.org/abs/2402.14903)

    本研究探讨了在大型语言模型中对输入文本进行tokenization对数值推理的影响，发现采用从右到左的tokenization方式可显著提高算术任务的性能表现。

    

    Tokenization，即将输入文本分成输入token的过程，是大型语言模型（LLM）管道中经常被忽视的一个方面，可能是有用的或有害的归纳偏差的来源。在历史上，LLMs倾向于使用字节对编码，而没有考虑特定的输入领域。随着LLMs用于推理的增加，各种特定于数字的tokenization方案得到了采用，像LLaMa和PaLM这样的流行模型选择了单个数字tokenization，而GPT-3.5和GPT-4为每个1位、2位和3位数字都有单独的token。在这项工作中，我们通过算术任务研究这种选择对数值推理的影响。我们考虑了GPT-3.5和-4的从左到右和从右到左的tokenization，发现从右到左的tokenization（在推理时通过逗号分离数字）导致了大幅提高的性能。此外，我们发现在使用标准的从左到右tokenization时模型存在误差

    arXiv:2402.14903v1 Announce Type: new  Abstract: Tokenization, the division of input text into input tokens, is an often overlooked aspect of the large language model (LLM) pipeline and could be the source of useful or harmful inductive biases. Historically, LLMs have relied on byte pair encoding, without care to specific input domains. With the increased use of LLMs for reasoning, various number-specific tokenization schemes have been adopted, with popular models like LLaMa and PaLM opting for single-digit tokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and 3-digit numbers. In this work, we study the effect this choice has on numerical reasoning through the use of arithmetic tasks. We consider left-to-right and right-to-left tokenization for GPT-3.5 and -4, finding that right-to-left tokenization (enforced by comma separating numbers at inference time) leads to largely improved performance. Furthermore, we find that model errors when using standard left-to-r
    
[^176]: 停止推理！当多模态LLMs与串联推理遇到对抗性图像

    Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images

    [https://arxiv.org/abs/2402.14899](https://arxiv.org/abs/2402.14899)

    该研究评估了多模态LLMs在采用串联推理时的对抗鲁棒性，发现串联推理在一定程度上提高了对抗性鲁棒性，但引入了一种新的停止推理攻击技术成功规避了这种增强。

    

    最近，多模态LLMs（MLLMs）展示了很强的理解图像的能力。然而，像传统视觉模型一样，它们仍然容易受到对抗性图像的攻击。与此同时，串联推理（CoT）已经被广泛应用在MLLMs上，不仅提高了模型的性能，而且通过提供中间推理步骤来增强模型的可解释性。然而，目前还缺乏关于MLLMs在CoT下的对抗鲁棒性的研究，以及在MLLMs用对抗性图像推断错误答案时推理的合理性。我们的研究评估了采用CoT推理时MLLMs的对抗鲁棒性，发现CoT在一定程度上提高了对抗性鲁棒性，抵抗了已有的攻击方法。此外，我们引入了一种新的停止推理攻击技术，可以有效地规避CoT引起的鲁棒性增强。最后，我们展示了CoT推理的变化。

    arXiv:2402.14899v1 Announce Type: cross  Abstract: Recently, Multimodal LLMs (MLLMs) have shown a great ability to understand images. However, like traditional vision models, they are still vulnerable to adversarial images. Meanwhile, Chain-of-Thought (CoT) reasoning has been widely explored on MLLMs, which not only improves model's performance, but also enhances model's explainability by giving intermediate reasoning steps. Nevertheless, there is still a lack of study regarding MLLMs' adversarial robustness with CoT and an understanding of what the rationale looks like when MLLMs infer wrong answers with adversarial images. Our research evaluates the adversarial robustness of MLLMs when employing CoT reasoning, finding that CoT marginally improves adversarial robustness against existing attack methods. Moreover, we introduce a novel stop-reasoning attack technique that effectively bypasses the CoT-induced robustness enhancements. Finally, we demonstrate the alterations in CoT reasonin
    
[^177]: Chain-of-Thought不忠诚作为伪装的准确性

    Chain-of-Thought Unfaithfulness as Disguised Accuracy

    [https://arxiv.org/abs/2402.14897](https://arxiv.org/abs/2402.14897)

    了解Chain-of-Thought生成与大语言模型内部计算的一致程度对于决定是否信任模型输出至关重要，研究发现模型大小与忠实度之间存在着特定关系，并且发现130亿参数模型表现出更高的忠实度。

    

    了解Chain-of-Thought (CoT)生成与大语言模型(LLM)内部计算的一致程度对于决定是否信任LLM的输出至关重要。作为CoT忠实度的代理，arXiv:2307.13702提出了一个度量模型依赖其CoT生成答案的指标。在一个专有模型系列中，他们发现LLM表现出模型大小与其忠实度测量之间的缩放-反向缩放关系，并且130亿参数模型相比于尺寸介于8.1亿到1750亿参数之间的模型表现出增加的忠实度。我们评估这些结果是否作为所有LLM的特性泛化。我们使用三种不同系列的模型复制他们的实验设置，并在特定条件下，成功复制了他们报告的CoT忠实度的缩放趋势。然而，我们发现简单的改变设定会导致这些模式在多大程度上重复。

    arXiv:2402.14897v1 Announce Type: cross  Abstract: Understanding the extent to which Chain-of-Thought (CoT) generations align with a large language model's (LLM) internal computations is critical for deciding whether to trust an LLM's output. As a proxy for CoT faithfulness, arXiv:2307.13702 propose a metric that measures a model's dependence on its CoT for producing an answer. Within a single family of proprietary models, they find that LLMs exhibit a scaling-then-inverse-scaling relationship between model size and their measure of faithfulness, and that a 13 billion parameter model exhibits increased faithfulness compared to models ranging from 810 million to 175 billion parameters in size. We evaluate whether these results generalize as a property of all LLMs. We replicate their experimental setup with three different families of models and, under specific conditions, successfully reproduce the scaling trends for CoT faithfulness they report. However, we discover that simply changin
    
[^178]: 数据增强已死，数据增强万岁

    Data Augmentation is Dead, Long Live Data Augmentation

    [https://arxiv.org/abs/2402.14895](https://arxiv.org/abs/2402.14895)

    数据增强不过是更好地微调模型，零唁态和少样本数据生成可提高性能

    

    文本数据增强（DA）是一个繁荣的研究领域，不断提出新颖的技术来创建人工数据，已经在小数据环境中表现出很高的效率，至少对于文本分类任务而言。在本文中，我们质疑这些结果，表明经典的数据增强只是一种更好地进行微调的方式，并且在应用数据增强之前花更多时间进行微调会抵消其效果。这是一个重要的贡献，因为它回答了最近几年留下的几个问题，即：哪种DA技术表现最佳（只要它们生成的数据与训练集足够接近，不会损害训练），为什么DA表现出积极的结果（简化网络训练）。此外，我们还展示了通过对话代理（如ChatGPT或LLama2）零唁态和少样本数据生成可以提高性能，从而得出了结论，此法可以提高模型性能。

    arXiv:2402.14895v1 Announce Type: cross  Abstract: Textual data augmentation (DA) is a prolific field of study where novel techniques to create artificial data are regularly proposed, and that has demonstrated great efficiency on small data settings, at least for text classification tasks. In this paper, we challenge those results, showing that classical data augmentation is simply a way of performing better fine-tuning, and that spending more time fine-tuning before applying data augmentation negates its effect. This is a significant contribution as it answers several questions that were left open in recent years, namely~: which DA technique performs best (all of them as long as they generate data close enough to the training set as to not impair training) and why did DA show positive results (facilitates training of network). We furthermore show that zero and few-shot data generation via conversational agents such as ChatGPT or LLama2 can increase performances, concluding that this f
    
[^179]: 基于数据驱动的配电网分布式发电接地故障定位方法

    Data-Driven Ground-Fault Location Method in Distribution Power System With Distributed Generation

    [https://arxiv.org/abs/2402.14894](https://arxiv.org/abs/2402.14894)

    提出了一种基于数据驱动的接地故障定位方法，通过离散小波变换和人工神经网络分析处理数据，实现了对配电系统中故障的准确预测

    

    最近可再生能源在配电级别的增加引入了多方向功率流，使得过时的传统故障定位技术难以适用。为此，需要开发新的方法以确保快速准确的故障定位，从而增强电力系统可靠性。本文提出了一种针对配电系统的基于数据驱动的接地故障定位方法。在Matlab/Simulink中建模了一个11节点 20 kV的电力系统，用于模拟接地故障。在不同位置和不同系统运行状态下产生了故障。然后，使用离散小波变换分析系统变电站的时域故障三相电压。最终利用处理后的数据的统计量训练人工神经网络(ANN)来找到计算电压特征和故障之间的映射。具体而言，三个ANNs可以预测故障

    arXiv:2402.14894v1 Announce Type: cross  Abstract: The recent increase in renewable energy penetration at the distribution level introduces a multi-directional power flow that outdated traditional fault location techniques. To this extent, the development of new methods is needed to ensure fast and accurate fault localization and, hence, strengthen power system reliability. This paper proposes a data-driven ground fault location method for the power distribution system. An 11-bus 20 kV power system is modeled in Matlab/Simulink to simulate ground faults. The faults are generated at different locations and under various system operational states. Time-domain faulted three-phase voltages at the system substation are then analyzed with discrete wavelet transform. Statistical quantities of the processed data are eventually used to train an Artificial Neural Network (ANN) to find a mapping between computed voltage features and faults. Specifically, three ANNs allow the prediction of faulted
    
[^180]: 利用特征对射电天文数据进行新颖性检测

    Novelty Detection on Radio Astronomy Data using Signatures

    [https://arxiv.org/abs/2402.14892](https://arxiv.org/abs/2402.14892)

    提出了一个新的半监督框架SigNova，用于检测射电天文数据中的异常值，采用特征转换提取摘要统计信息并计算新颖性评分，以识别偏离预期行为的观察范围。

    

    我们引入了SigNova，一个新的半监督框架，用于检测流数据中的异常值。尽管我们最初的例子侧重于在射电天文学领域内检测数字信号中的射频干扰（RFI），但重要的是要注意，SigNova的适用范围扩展到任何类型的流数据。该框架由三个主要组件组成。首先，我们使用特征转换从观测序列中提取一组规范的摘要统计信息。这使我们能够将可变长度的可见性样本表示为有限维特征向量。其次，每个特征向量被分配一个新颖性评分，计算为到无RFI训练集中最近邻的马氏距离。通过设定这些分数的阈值，我们识别出偏离无RFI可见性样本预期行为的观察范围，而不依赖于严格的分布假设。

    arXiv:2402.14892v1 Announce Type: cross  Abstract: We introduce SigNova, a new semi-supervised framework for detecting anomalies in streamed data. While our initial examples focus on detecting radio-frequency interference (RFI) in digitized signals within the field of radio astronomy, it is important to note that SigNova's applicability extends to any type of streamed data. The framework comprises three primary components. Firstly, we use the signature transform to extract a canonical collection of summary statistics from observational sequences. This allows us to represent variable-length visibility samples as finite-dimensional feature vectors. Secondly, each feature vector is assigned a novelty score, calculated as the Mahalanobis distance to its nearest neighbor in an RFI-free training set. By thresholding these scores we identify observation ranges that deviate from the expected behavior of RFI-free visibility samples without relying on stringent distributional assumptions. Thirdl
    
[^181]: Vygotsky Distance: 用于基准任务相似性的度量方法

    Vygotsky Distance: Measure for Benchmark Task Similarity

    [https://arxiv.org/abs/2402.14890](https://arxiv.org/abs/2402.14890)

    论文提出了一种基于相对性能而非任务属性的相似性度量方法，即“维果茨基距离”，可帮助减少评估任务数量并保持高验证质量。

    

    论文介绍了一种理论工具和实践算法来计算基准任务之间的相似性，称之为"维果茨基距离"。这种相似性度量的核心思想是基于“学生”在给定任务上的相对表现，而不是基于任务本身的属性。如果两个任务在维果茨基距离上彼此接近，模型在这些任务上 tend to have similar relative performance。因此，通过了解任务之间的维果茨基距离，可以显著减少评估任务数量，同时保持高验证质量。

    arXiv:2402.14890v1 Announce Type: cross  Abstract: Evaluation plays a significant role in modern natural language processing. Most modern NLP benchmarks consist of arbitrary sets of tasks that neither guarantee any generalization potential for the model once applied outside the test set nor try to minimize the resource consumption needed for model evaluation. This paper presents a theoretical instrument and a practical algorithm to calculate similarity between benchmark tasks, we call this similarity measure "Vygotsky distance". The core idea of this similarity measure is that it is based on relative performance of the "students" on a given task, rather that on the properties of the task itself. If two tasks are close to each other in terms of Vygotsky distance the models tend to have similar relative performance on them. Thus knowing Vygotsky distance between tasks one can significantly reduce the number of evaluation tasks while maintaining a high validation quality. Experiments on v
    
[^182]: 利用基于语义相似性的图结构进行高效数据选择用于模型训练

    Efficient data selection employing Semantic Similarity-based Graph Structures for model training

    [https://arxiv.org/abs/2402.14888](https://arxiv.org/abs/2402.14888)

    提出了一种基于语义相似性的图结构的高效数据选择机制，可在不经过计算密集型模型或其他密集的预处理转换的情况下，用于模型训练。

    

    自然语言处理（NLP）领域的最新发展凸显了模型准确捕捉文本信息所需大量数据的必要性。这引发了关于训练此类模型所需的计算资源和时间的担忧。本文介绍了一种称为“SeSaME”的数据选择机制，它仅基于文本信息进行高效的数据采样，无需通过计算密集型模型或其他密集的预处理转换。

    arXiv:2402.14888v1 Announce Type: cross  Abstract: Recent developments in natural language processing (NLP) have highlighted the need for substantial amounts of data for models to capture textual information accurately. This raises concerns regarding the computational resources and time required for training such models. This paper introduces Semantics for data SAliency in Model performance Estimation (SeSaME). It is an efficient data sampling mechanism solely based on textual information without passing the data through a compute-heavy model or other intensive pre-processing transformations. The application of this approach is demonstrated in the use case of low-resource automated speech recognition (ASR) models, which excessively rely on text-to-speech (TTS) calls when using augmented data. SeSaME learns to categorize new incoming data points into speech recognition difficulty buckets by employing semantic similarity-based graph structures and discrete ASR information from homophilou
    
[^183]: 将强化学习应用于优化交通信号灯循环

    Applying Reinforcement Learning to Optimize Traffic Light Cycles

    [https://arxiv.org/abs/2402.14886](https://arxiv.org/abs/2402.14886)

    提出了将强化学习应用于交通信号灯循环优化，实验证明能显著减少紧急停车次数，降低交通拥堵，改善交通流。

    

    交通信号灯循环的手动优化是一项复杂且耗时的任务，需要开发自动化解决方案。本文提出了将强化学习应用于实时优化交通信号灯循环。我们通过使用模拟城市移动模拟器进行案例研究，训练了一个深度Q网络算法。实验结果显示平均紧急停车次数减少了44.16%，显示了我们方法减少交通拥堵、改善交通流的潜力。此外，我们还讨论了未来研究的途径和对强化学习模型的改进。

    arXiv:2402.14886v1 Announce Type: cross  Abstract: Manual optimization of traffic light cycles is a complex and time-consuming task, necessitating the development of automated solutions. In this paper, we propose the application of reinforcement learning to optimize traffic light cycles in real-time. We present a case study using the Simulation Urban Mobility simulator to train a Deep Q-Network algorithm. The experimental results showed 44.16% decrease in the average number of Emergency stops, showing the potential of our approach to reduce traffic congestion and improve traffic flow. Furthermore, we discuss avenues for future research and enhancements to the reinforcement learning model.
    
[^184]: 双I水印：保护LLM微调模型版权

    Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning

    [https://arxiv.org/abs/2402.14883](https://arxiv.org/abs/2402.14883)

    提出了一种名为“双I水印”的水印方法，通过引入两种backdoor数据范例并利用LLM的学习能力，有效地保护了LLM微调定制模型的版权。

    

    为了支持各种应用，业主经常通过LLM所有者或云服务器提供的API对预训练的LLM进行微调，以获取定制模型。然而，这一过程存在着模型被滥用的风险，可能会给业主带来严重的经济后果。因此，在LLM微调过程中保护这些定制模型的版权已成为紧迫的实际需求，但现有的解决方案有限。为了解决这一紧迫问题，我们提出了一种名为“双I水印”的新型水印方法。具体地，基于指导微调数据，引入了两种backdoor数据范例，分别在指令和输入中触发。通过利用LLM的学习能力将定制的后门样本纳入数据集，所提出的方法有效地注入了特定的水印。

    arXiv:2402.14883v1 Announce Type: cross  Abstract: To support various applications, business owners often seek the customized models that are obtained by fine-tuning a pre-trained LLM through the API provided by LLM owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during LLM fine-tuning has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named "Double-I watermark". Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging LLM's learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermar
    
[^185]: 基于深度生成模型的满足目标条件的四连杆机构合成

    Deep Generative Model-based Synthesis of Four-bar Linkage Mechanisms with Target Conditions

    [https://arxiv.org/abs/2402.14882](https://arxiv.org/abs/2402.14882)

    提出了基于深度生成模型的方法，利用条件生成对抗网络生成满足运动学和准静态要求的多连杆四连杆机构。

    

    机构是各种机械系统中设计用于执行特定任务的关键组件。然而，设计满足特定运动学或准静态要求的机构是一项具有挑战性的任务。本文提出了基于深度学习的生成模型，用于生成满足运动学和准静态要求的多连杆四连杆机构。所提出的模型基于有条件生成对抗网络(cGAN)，并经过针对机构合成的修改，其训练目的是学习机构的要求与连杆长度之间的关系。结果表明，该方法可以成功合成满足要求的四连杆机构。

    arXiv:2402.14882v1 Announce Type: cross  Abstract: Mechanisms are essential components designed to perform specific tasks in various mechanical systems. However, designing a mechanism that satisfies certain kinematic or quasi-static requirements is a challenging task. The kinematic requirements may include the workspace of a mechanism, while the quasi-static requirements of a mechanism may include its torque transmission, which refers to the ability of the mechanism to transfer power and torque effectively. In this paper, we propose a deep learning-based generative model for generating multiple crank-rocker four-bar linkage mechanisms that satisfy both the kinematic and quasi-static requirements aforementioned. The proposed model is based on a conditional generative adversarial network (cGAN) with modifications for mechanism synthesis, which is trained to learn the relationship between the requirements of a mechanism with respect to linkage lengths. The results demonstrate that the pro
    
[^186]: 使用内存中学习的方法训练AI系统的能效限制

    Energy-efficiency Limits on Training AI Systems using Learning-in-Memory

    [https://arxiv.org/abs/2402.14878](https://arxiv.org/abs/2402.14878)

    该论文提出了使用内存中学习的方法训练AI系统时的能效限制，并推导了新的理论下限。

    

    arXiv:2402.14878v1 公告类型: cross 摘要: 内存中学习（LIM）是一种最近提出的范Paradigm，旨在克服训练机器学习系统中的基本内存瓶颈。虽然计算于内存（CIM）方法可以解决所谓的内存墙问题（即由于重复内存读取访问而消耗的能量），但它们对于以训练所需的精度重复内存写入时消耗的能量（更新墙）是不可知的，并且它们不考虑在短期和长期记忆之间传输信息时所消耗的能量（整合墙）。LIM范式提出，如果物理内存的能量屏障被自适应调制，使得存储器更新和整合的动态与梯度下降训练AI模型的Lyapunov动态相匹配，那么这些瓶颈也可以被克服。在本文中，我们推导了使用不同LIM应用程序训练AI系统时的能耗的新理论下限。

    arXiv:2402.14878v1 Announce Type: cross  Abstract: Learning-in-memory (LIM) is a recently proposed paradigm to overcome fundamental memory bottlenecks in training machine learning systems. While compute-in-memory (CIM) approaches can address the so-called memory-wall (i.e. energy dissipated due to repeated memory read access) they are agnostic to the energy dissipated due to repeated memory writes at the precision required for training (the update-wall), and they don't account for the energy dissipated when transferring information between short-term and long-term memories (the consolidation-wall). The LIM paradigm proposes that these bottlenecks, too, can be overcome if the energy barrier of physical memories is adaptively modulated such that the dynamics of memory updates and consolidation match the Lyapunov dynamics of gradient-descent training of an AI model. In this paper, we derive new theoretical lower bounds on energy dissipation when training AI systems using different LIM app
    
[^187]: 机器学习预测大西洋经向翻转环流的倾覆和崩溃

    Machine-learning prediction of tipping and collapse of the Atlantic Meridional Overturning Circulation

    [https://arxiv.org/abs/2402.14877](https://arxiv.org/abs/2402.14877)

    该研究开发了一种机器学习方法，用于预测在嘈杂的动力系统中具有时间变化参数的倾覆，包括预测大西洋经向翻转环流的倾覆和崩溃。

    

    大西洋经向翻转环流(AMOC)的最新研究引发了对其潜在倾覆的担忧，这是由于气候变化导致北大西洋淡水输入增加的一个临界点。预测的崩溃时间窗口大约在本世纪中叶，最早可能在大约两年后开始。更一般地，对系统从一个稳定平衡状态转变到另一个稳定状态的临界点的预测对于广泛领域都是相关的。我们开发了一种机器学习方法，用于预测嘈杂的动力系统中具有时间变化参数的倾覆，并在多个系统上进行测试，包括AMOC、生态网络、电力系统和气候模型。对于AMOC，我们基于模拟指纹数据和海表温度的真实数据进行预测，将潜在倾覆的时间窗口置于

    arXiv:2402.14877v1 Announce Type: cross  Abstract: Recent research on the Atlantic Meridional Overturning Circulation (AMOC) raised concern about its potential collapse through a tipping point due to the climate-change caused increase in the freshwater input into the North Atlantic. The predicted time window of collapse is centered about the middle of the century and the earliest possible start is approximately two years from now. More generally, anticipating a tipping point at which the system transitions from one stable steady state to another is relevant to a broad range of fields. We develop a machine-learning approach to predicting tipping in noisy dynamical systems with a time-varying parameter and test it on a number of systems including the AMOC, ecological networks, an electrical power system, and a climate model. For the AMOC, our prediction based on simulated fingerprint data and real data of the sea surface temperature places the time window of a potential collapse between 
    
[^188]: 名字的含义是什么？审计大型语言模型中的种族和性别偏见

    What's in a Name? Auditing Large Language Models for Race and Gender Bias

    [https://arxiv.org/abs/2402.14875](https://arxiv.org/abs/2402.14875)

    调查发现，大型语言模型存在种族和性别偏见，尤其对与黑人女性相关的名字表现最不利。审计在模型部署和实施时的重要性得到强调。

    

    我们采用审计设计来调查最先进的大型语言模型中的偏见，包括GPT-4。在我们的研究中，我们引发模型在各种情景下为个人提供建议，比如在购车谈判或选举结果预测过程中。我们发现该建议系统性地对与种族少数群体和女性常见相关的名字产生不利影响。与黑人女性相关的名字得到的结果最不利。这些偏见在42个提示模板和多个模型中都是一致的，表明这是一个系统性问题，而不是孤立事件。在提示中提供数值、与决策相关的锚点可以成功抵消偏见，而定性细节的影响并不一致，甚至可能会加剧差异。我们的研究结果强调了在语言模型部署和实施时进行审计的重要性，以减轻其潜在影响。

    arXiv:2402.14875v1 Announce Type: cross  Abstract: We employ an audit design to investigate biases in state-of-the-art large language models, including GPT-4. In our study, we elicit prompt the models for advice regarding an individual across a variety of scenarios, such as during car purchase negotiations or election outcome predictions. We find that the advice systematically disadvantages names that are commonly associated with racial minorities and women. Names associated with Black women receive the least advantageous outcomes. The biases are consistent across 42 prompt templates and several models, indicating a systemic issue rather than isolated incidents. While providing numerical, decision-relevant anchors in the prompt can successfully counteract the biases, qualitative details have inconsistent effects and may even increase disparities. Our findings underscore the importance of conducting audits at the point of LLM deployment and implementation to mitigate their potential for
    
[^189]: 蒸馏对比解码：利用对比解码和蒸馏提升LLM的推理能力

    Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation

    [https://arxiv.org/abs/2402.14874](https://arxiv.org/abs/2402.14874)

    该研究提出了一种叫做蒸馏对比解码（DCD）的方法，通过结合对比提示与蒸馏技术，有效提升了大型语言模型（LLM）在推理任务上的性能表现，超过了传统的对比解码方法，并在多个基准数据集上取得了显著成果。

    

    我们提出了一种称为蒸馏对比解码（DCD）的简单方法，以增强大型语言模型（LLMs）在推理过程中的推理能力。与先前依赖于较小的业余模型或隐藏状态差异分析的方法不同，DCD采用了对比式思维引导和先进的蒸馏技术，包括Dropout和量化。这种方法有效地解决了对比解码（CD）的局限性，后者通常需要专家和业余模型，从而增加计算资源需求。通过将对比提示与蒸馏相结合，DCD消除了对业余模型的需求并减少了内存使用。我们的评估表明，DCD显著增强了LLM在各种推理基准测试中的性能，在GSM8K和StrategyQA数据集中均超过了CD和现有方法。

    arXiv:2402.14874v1 Announce Type: cross  Abstract: We propose a straightforward approach called Distillation Contrastive Decoding (DCD) to enhance the reasoning capabilities of Large Language Models (LLMs) during inference. In contrast to previous approaches that relied on smaller amateur models or analysis of hidden state differences, DCD employs Contrastive Chain-of-thought Prompting and advanced distillation techniques, including Dropout and Quantization. This approach effectively addresses the limitations of Contrastive Decoding (CD), which typically requires both an expert and an amateur model, thus increasing computational resource demands. By integrating contrastive prompts with distillation, DCD obviates the need for an amateur model and reduces memory usage. Our evaluations demonstrate that DCD significantly enhances LLM performance across a range of reasoning benchmarks, surpassing both CD and existing methods in the GSM8K and StrategyQA datasets.
    
[^190]: 使用和不使用停用词对阿拉伯文本分类的加权方法的影响

    Effects of term weighting approach with and without stop words removing on Arabic text classification

    [https://arxiv.org/abs/2402.14867](https://arxiv.org/abs/2402.14867)

    本研究比较不同的加权特征方法（二元和词频加权）在文本分类中使用和不使用停用词时的影响，通过评估准确性、召回率、精确度和F-度量值，结果表明停用词的处理方式对文本分类结果具有重要影响。

    

    分类文本是一种将文档分类为预先建立的群组的方法。在分类之前，文本文档必须以适合数据挖掘所使用的算法的方式进行准备和表示。因此，文献中已经创建了许多术语加权策略来增强文本分类算法的功能性。本研究比较了二元和词频加权特征方法对文本分类方法的影响，一次删除停用词和不删除停用词。为了评估先前特征加权方法对分类结果的影响，我们使用了一个包含322份文档的阿拉伯数据集，分为六个主题（农业、经济、健康、政治、科学和体育），每个主题包含50份文档，唯独健康类别除外。

    arXiv:2402.14867v1 Announce Type: cross  Abstract: Classifying text is a method for categorizing documents into pre-established groups. Text documents must be prepared and represented in a way that is appropriate for the algorithms used for data mining prior to classification. As a result, a number of term weighting strategies have been created in the literature to enhance text categorization algorithms' functionality. This study compares the effects of Binary and Term frequency weighting feature methodologies on the text's classification method when stop words are eliminated once and when they are not. In recognition of assessing the effects of prior weighting of features approaches on classification results in terms of accuracy, recall, precision, and F-measure values, we used an Arabic data set made up of 322 documents divided into six main topics (agriculture, economy, health, politics, science, and sport), each of which contains 50 documents, with the exception of the health categ
    
[^191]: APTQ: 针对大型语言模型的注意力感知后训练混合精度量化

    APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models

    [https://arxiv.org/abs/2402.14866](https://arxiv.org/abs/2402.14866)

    APTQ提出了针对大型语言模型的注意力感知后训练混合精度量化方法，在保持模型性能的同时，超越了先前的量化方法，并在零-shot任务上达到了最先进的准确率

    

    大型语言模型（LLMs）极大地推动了自然语言处理范式。然而，高计算负载和巨大的模型尺寸对在边缘设备上部署构成了巨大挑战。为此，我们提出了针对LLMs的APTQ（Attention-aware Post-Training Mixed-Precision Quantization），该方法不仅考虑了每层权重的二阶信息，而且首次考虑了注意力输出对整个模型的非线性影响。我们利用Hessian迹作为混合精度量化的敏感度度量，确保经过理性的精度降低能保持模型性能。实验表明，APTQ超越了先前的量化方法，在C4数据集中以平均4位宽度获得5.22困惑度，几乎等效于全精度。此外，APTQ在LLaMa-7B和LLaMa-1中以平均3.8位宽度达到了68.24％和70.48％的最先进零-shot准确率。

    arXiv:2402.14866v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have greatly advanced the natural language processing paradigm. However, the high computational load and huge model sizes pose a grand challenge for deployment on edge devices. To this end, we propose APTQ (Attention-aware Post-Training Mixed-Precision Quantization) for LLMs, which considers not only the second-order information of each layer's weights, but also, for the first time, the nonlinear effect of attention outputs on the entire model. We leverage the Hessian trace as a sensitivity metric for mixed-precision quantization, ensuring an informed precision reduction that retains model performance. Experiments show APTQ surpasses previous quantization methods, achieving an average of 4 bit width a 5.22 perplexity nearly equivalent to full precision in the C4 dataset. In addition, APTQ attains state-of-the-art zero-shot accuracy of 68.24\% and 70.48\% at an average bitwidth of 3.8 in LLaMa-7B and LLaMa-1
    
[^192]: DyVal 2: 元探测代理动态评估大型语言模型

    DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents

    [https://arxiv.org/abs/2402.14865](https://arxiv.org/abs/2402.14865)

    本文提出了一种基于心理测量学思想的元探测代理（MPA）动态评估协议，用于评估大型语言模型（LLMs）的能力。

    

    大型语言模型（LLMs）的评估引起了社区的极大关注，因为存在数据污染问题。现有工作设计了使用针对特定任务的明确定义算法的评估协议，这些协议无法轻松扩展到不同的场景。此外，当前的评估基准只能提供整体基准结果，不能支持对LLMs能力进行细粒度和多方面的分析。在本文中，我们提出了元探测代理（MPA），这是一种受心理测量学启发的通用动态评估协议，用于评估LLMs。 MPA 是 DyVal 2 的关键组件，自然地扩展了先前的 DyVal。 MPA 设计了探测和评判代理，以自动将原始评估问题转化为一个新问题，遵循心理测量理论在三个基本认知能力上的应用: 语言理解、问题解决和领域知识。

    arXiv:2402.14865v1 Announce Type: cross  Abstract: Evaluation of large language models (LLMs) has raised great concerns in the community due to the issue of data contamination. Existing work designed evaluation protocols using well-defined algorithms for specific tasks, which cannot be easily extended to diverse scenarios. Moreover, current evaluation benchmarks can only provide the overall benchmark results and cannot support a fine-grained and multifaceted analysis of LLMs' abilities. In this paper, we propose meta probing agents (MPA), a general dynamic evaluation protocol inspired by psychometrics to evaluate LLMs. MPA is the key component of DyVal 2, which naturally extends the previous DyVal~\citep{zhu2023dyval}. MPA designs the probing and judging agents to automatically transform an original evaluation problem into a new one following psychometric theory on three basic cognitive abilities: language understanding, problem solving, and domain knowledge. These basic abilities are 
    
[^193]: SISSA: 实时监控车载SOME/IP以太网流量的硬件功能安全和网络安全

    SISSA: Real-time Monitoring of Hardware Functional Safety and Cybersecurity with In-vehicle SOME/IP Ethernet Traffic

    [https://arxiv.org/abs/2402.14862](https://arxiv.org/abs/2402.14862)

    SISSA提出了基于SOME/IP通信流量的方法，用于建模和分析车载功能安全和网络安全，解决了SOME/IP缺乏安全架构的问题，包括硬件故障和五种潜在攻击。

    

    Scalable Service-Oriented Middleware over IP（SOME/IP）是汽车开放系统架构（AUTOSAR）中的以太网通信标准协议，促进ECU与ECU之间通过IP堆栈进行通信。然而，SOME/IP缺乏健壮的安全架构，容易受到潜在攻击。此外，ECU的随机硬件故障会破坏SOME/IP通信。本文提出了SISSA，一种基于SOME/IP通信流量的方法，用于建模和分析车载功能安全和网络安全。具体而言，SISSA使用Weibull分布对硬件故障进行建模，并解决包括分布式拒绝服务、中间人和异常通信流程在内的五种潜在SOME/IP通信攻击，假设恶意用户访问车载网络。随后，SISSA设计了一系列具有不同主干结构的深度学习模型，用于从SOM中提取特征。

    arXiv:2402.14862v1 Announce Type: cross  Abstract: Scalable service-Oriented Middleware over IP (SOME/IP) is an Ethernet communication standard protocol in the Automotive Open System Architecture (AUTOSAR), promoting ECU-to-ECU communication over the IP stack. However, SOME/IP lacks a robust security architecture, making it susceptible to potential attacks. Besides, random hardware failure of ECU will disrupt SOME/IP communication. In this paper, we propose SISSA, a SOME/IP communication traffic-based approach for modeling and analyzing in-vehicle functional safety and cyber security. Specifically, SISSA models hardware failures with the Weibull distribution and addresses five potential attacks on SOME/IP communication, including Distributed Denial-of-Services, Man-in-the-Middle, and abnormal communication processes, assuming a malicious user accesses the in-vehicle network. Subsequently, SISSA designs a series of deep learning models with various backbones to extract features from SOM
    
[^194]: CloudNine：使用可解释图神经网络分析气象观测对天气预测的影响

    CloudNine: Analyzing Meteorological Observation Impact on Weather Prediction Using Explainable Graph Neural Networks

    [https://arxiv.org/abs/2402.14861](https://arxiv.org/abs/2402.14861)

    提出了一个名为“CloudNine”的系统，利用可解释图神经网络分析气象观测对特定天气预测的影响

    

    气象观测对天气预报的影响取决于传感器类型、位置、时间和其他环境因素。因此，定量分析观测影响对天气预报系统的有效和高效发展至关重要。为解决这些问题，我们提出了一个名为“CloudNine”的新系统，基于可解释图神经网络（XGNNs）分析单个观测对特定预测的影响。将基于XGNN的大气状态估计模型与数值天气预报模型相结合，提供一个网络应用程序，可在地球系统的三维空间中搜索观测。

    arXiv:2402.14861v1 Announce Type: cross  Abstract: The impact of meteorological observations on weather forecasting varies with sensor type, location, time, and other environmental factors. Thus, quantitative analysis of observation impacts is crucial for effective and efficient development of weather forecasting systems. However, the existing impact analysis methods are difficult to be widely applied due to their high dependencies on specific forecasting systems. Also, they cannot provide observation impacts at multiple spatio-temporal scales, only global impacts of observation types. To address these issues, we present a novel system called ``CloudNine,'' which allows analysis of individual observations' impacts on specific predictions based on explainable graph neural networks (XGNNs). Combining an XGNN-based atmospheric state estimation model with a numerical weather prediction model, we provide a web application to search for observations in the 3D space of the Earth system and to
    
[^195]: 在没有基准实况的情况下对大型语言模型进行排名

    Ranking Large Language Models without Ground Truth

    [https://arxiv.org/abs/2402.14860](https://arxiv.org/abs/2402.14860)

    不需要基准实况或参考响应的条件下，通过考虑模型的三元组来排名大型语言模型，并提出了两种排名方法。

    

    随着大型语言模型（LLMs）的普及和影响力的增强，评估和排名LLMs已成为一个重要问题。现有的评估方法要么需要获取昂贵的人类响应，要么使用LLMs成对地互相评估，这可能不够可靠。本文提供了一个新的视角，在给定一组提示数据集（比如问题、说明等）和一组LLMs的情况下，我们在没有任何基准实况或参考响应的情况下对它们进行排名。受到现实生活的启发，其中专家和有知识的人都能识别一个新手，我们的主要思路是考虑模型的三元组，其中每个模型评估其他两个模型，能够以很高的概率正确识别最差的模型。我们还分析了我们的想法并提供了成功的充分条件。通过反复应用这一想法，我们提出了两种对LLMs进行排名的方法。

    arXiv:2402.14860v1 Announce Type: cross  Abstract: Evaluation and ranking of large language models (LLMs) has become an important problem with the proliferation of these models and their impact. Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable. In this paper, we provide a novel perspective where, given a dataset of prompts (viz. questions, instructions, etc.) and a set of LLMs, we rank them without access to any ground truth or reference responses. Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability. We also analyze our idea and provide sufficient conditions for it to succeed. Applying this idea repeatedly, we propose two methods to rank LLMs. In experiments on different generati
    
[^196]: 内在的狼：通过MLLM操作员向MLLM社会中渗入恶意

    The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative

    [https://arxiv.org/abs/2402.14859](https://arxiv.org/abs/2402.14859)

    这里是中文总结出的一句话要点: 论文探讨了在MLLM社会中通过单个操作员间接影响其他代理生成恶意内容的新型漏洞。

    

    由于其前所未有的处理和响应各种数据类型的能力，多模大型语言模型（MLLMs）不断定义人工通用智能（AGI）的新边界。随着这些先进的生成模型越来越多地形成用于复杂任务的协作网络，这些系统的完整性和安全性至关重要。我们的论文《内在的狼》探讨了MLLM社会中的一种新型漏洞 - 恶意内容的间接传播。与直接为MLLM生成有害输出不同，我们的研究展示了一个单个MLLM代理如何被微妙地影响，以生成再次诱使社会中其他MLLM代理输出恶意内容的提示。这种微妙而强有力的间接影响方法标志着与MLLM相关的安全风险的显著升级。我们的发现表明，即使几乎没有或是根本没有访问MLLM参数，一个MLLM代理，当

    arXiv:2402.14859v1 Announce Type: cross  Abstract: Due to their unprecedented ability to process and respond to various types of data, Multimodal Large Language Models (MLLMs) are constantly defining the new boundary of Artificial General Intelligence (AGI). As these advanced generative models increasingly form collaborative networks for complex tasks, the integrity and security of these systems are crucial. Our paper, ``The Wolf Within'', explores a novel vulnerability in MLLM societies - the indirect propagation of malicious content. Unlike direct harmful output generation for MLLMs, our research demonstrates how a single MLLM agent can be subtly influenced to generate prompts that, in turn, induce other MLLM agents in the society to output malicious content. This subtle, yet potent method of indirect influence marks a significant escalation in the security risks associated with MLLMs. Our findings reveal that, with minimal or even no access to MLLMs' parameters, an MLLM agent, when 
    
[^197]: 最新GPT模型上的HumanEval -- 2024

    HumanEval on Latest GPT Models -- 2024

    [https://arxiv.org/abs/2402.14852](https://arxiv.org/abs/2402.14852)

    使用最新的GPT-4模型在程序合成方面取得显著进展，通过在HumanEval任务中展示了在零样本Python代码生成中的竞争性性能和更多多步骤范式综合。

    

    在2023年，我们正在使用最新的GPT-4模型来推进程序合成。这些大型语言模型显著改进了这一目的的最新技术。为了使这些进展更易于访问，我们创建了一个将这些模型连接到Human Eval的存储库。该数据集最初是为与名为CODEGEN的语言模型在自然语言和编程语言数据上使用而开发的。通过展示这些经过训练的模型在与以前的最先进解决方案相比在HumanEval任务上零样本Python代码生成中的竞争性性能，展示了这些训练模型的效用。此外，这为开发更多的多步骤范式综合创造了可能。这一基准测试包含160个多样化的问题集，这些问题集被分解成多步提示，我们的分析表明这显著改进了单轮输入上的程序综合。所有代码均以开源方式发布在https://github.com/daniel442li/gpt-human-eval。

    arXiv:2402.14852v1 Announce Type: cross  Abstract: In 2023, we are using the latest models of GPT-4 to advance program synthesis. The large language models have significantly improved the state-of-the-art for this purpose. To make these advancements more accessible, we have created a repository that connects these models to Huamn Eval. This dataset was initally developed to be used with a language model called CODEGEN on natural and programming language data. The utility of these trained models is showcased by demonstrating their competitive performance in zero-shot Python code generation on HumanEval tasks compared to previous state-of-the-art solutions. Additionally, this gives way to developing more multi-step paradigm synthesis. This benchmark features 160 diverse problem sets factorized into multistep prompts that our analysis shows significantly improves program synthesis over single-turn inputs. All code is open source at https://github.com/daniel442li/gpt-human-eval .
    
[^198]: 异步和分段的双向编码对神经机器翻译的影响

    Asynchronous and Segmented Bidirectional Encoding for NMT

    [https://arxiv.org/abs/2402.14849](https://arxiv.org/abs/2402.14849)

    本文介绍了一种基于Transformer的改进模型，引入了异步和分段的双向编码策略，以提高神经机器翻译的效率和准确性。

    

    随着神经机器翻译(NMT)的迅速发展，提高翻译效率和质量已成为研究的焦点。本文提出了一种基于Transformer的改进模型，实施了异步和分段的双向解码策略，旨在提高翻译效率和准确性。与传统的从左到右或从右到左的单向翻译相比，我们的方法在处理长句时表现出更高的效率和更好的翻译质量。在IWSLT2017数据集上的实验结果验证了我们方法在加速翻译和提高准确性方面的有效性，尤其是超越了传统的单向翻译。

    arXiv:2402.14849v1 Announce Type: cross  Abstract: With the rapid advancement of Neural Machine Translation (NMT), enhancing translation efficiency and quality has become a focal point of research. Despite the commendable performance of general models such as the Transformer in various aspects, they still fall short in processing long sentences and fully leveraging bidirectional contextual information. This paper introduces an improved model based on the Transformer, implementing an asynchronous and segmented bidirectional decoding strategy aimed at elevating translation efficiency and accuracy. Compared to traditional unidirectional translations from left-to-right or right-to-left, our method demonstrates heightened efficiency and improved translation quality, particularly in handling long sentences. Experimental results on the IWSLT2017 dataset confirm the effectiveness of our approach in accelerating translation and increasing accuracy, especially surpassing traditional unidirection
    
[^199]: 基于深度学习的单机问题最小化总滞后的调度算法

    Deep learning-driven scheduling algorithm for a single machine problem minimizing the total tardiness

    [https://arxiv.org/abs/2402.14847](https://arxiv.org/abs/2402.14847)

    本文提出了一种基于深度学习的单机问题调度算法，通过神经网络估计最佳问题划分方式以最小化总滞后。

    

    在本文中，我们研究了使用深度学习方法来解决一个著名的NP难题单机调度问题，目标是最小化总滞后。我们提出了一个深度神经网络，它作为一个多项式时间估计量来用于基于Lawler分解和Della Croce等人提出的对称分解的单遍调度算法。实质上，神经网络通过估计问题划分为子问题的最佳方式来引导算法。本文还描述了一种生成训练数据集的新方法，加快了训练数据集的生成速度，减少了解决方案的平均最优性差距。实验结果表明，我们的机器学习驱动方法能够高效地从训练阶段概括信息到更大规模的实例。即使在训练阶段使用的实例从75

    arXiv:2402.14847v1 Announce Type: cross  Abstract: In this paper, we investigate the use of the deep learning method for solving a well-known NP-hard single machine scheduling problem with the objective of minimizing the total tardiness. We propose a deep neural network that acts as a polynomial-time estimator of the criterion value used in a single-pass scheduling algorithm based on Lawler's decomposition and symmetric decomposition proposed by Della Croce et al. Essentially, the neural network guides the algorithm by estimating the best splitting of the problem into subproblems. The paper also describes a new method for generating the training data set, which speeds up the training dataset generation and reduces the average optimality gap of solutions. The experimental results show that our machine learning-driven approach can efficiently generalize information from the training phase to significantly larger instances. Even though the instances used in the training phase have from 75
    
[^200]: 坚持你的角色！个人价值在大型语言模型中的稳定性

    Stick to your Role! Stability of Personal Values Expressed in Large Language Models

    [https://arxiv.org/abs/2402.14846](https://arxiv.org/abs/2402.14846)

    本文提出研究在大型语言模型中个人价值在不同背景下的表达稳定性，通过模拟对话的方式进行评估，对19个LLMs进行比较研究。

    

    通过基准测试或心理问卷的标准方式研究大型语言模型(LLMs)是提供许多来源于类似最小背景的不同查询（例如多项选择问题）。然而，由于LLM高度依赖于背景，因此从这种最小背景评估中得出的结论可能对模型在部署中的行为（在那里它将暴露于许多新背景）的说明很少。我们认为，依赖于背景的特性应该作为LLM比较的另一个维度来研究，而不是其他维度，如认知能力、知识或模型大小。在本文中，我们提出了一个关于在不同背景下（模拟对不同话题的对话）价值表达稳定性的案例研究，并使用标准心理学问卷（PVQ）和行为下游任务进行测量。我们考虑了来自五个家族的19个开源LLM。借鉴心理学方法，我们研究了等级稳定性。

    arXiv:2402.14846v1 Announce Type: cross  Abstract: The standard way to study Large Language Models (LLMs) through benchmarks or psychology questionnaires is to provide many different queries from similar minimal contexts (e.g. multiple choice questions). However, due to LLM's highly context-dependent nature, conclusions from such minimal-context evaluations may be little informative about the model's behavior in deployment (where it will be exposed to many new contexts). We argue that context-dependence should be studied as another dimension of LLM comparison alongside others such as cognitive abilities, knowledge, or model size. In this paper, we present a case-study about the stability of value expression over different contexts (simulated conversations on different topics), and as measured using a standard psychology questionnaire (PVQ) and a behavioral downstream task. We consider 19 open-sourced LLMs from five families. Reusing methods from psychology, we study Rank-order stabilit
    
[^201]: 通过集成小型语言模型来净化大型语言模型

    Purifying Large Language Models by Ensembling a Small Language Model

    [https://arxiv.org/abs/2402.14845](https://arxiv.org/abs/2402.14845)

    通过将大型语言模型与小型语言模型集成，可以有效净化大型语言模型，保持其性能并减轻版权侵权、数据污染和隐私侵犯等问题

    

    大型语言模型（LLMs）的成功很大程度上取决于从外部（不受信任）来源收集丰富的训练数据。尽管已经付出了大量努力进行数据清洗和精心策划，但已有报道显示构建良好的LLMs存在版权侵权、数据污染和/或隐私侵犯问题，这将阻碍LLMs的实际部署。在本研究中，我们提出了一种简单易行的方法，通过将LLMs与良性小语言模型（SLMs）集成来净化LLMs免受未经筛选数据带来的负面影响。除了理论保证外，我们进行了全面实验，从经验证实，LLMs与SLMs集成可以有效保持LLMs的性能，同时减轻版权侵权、数据污染和隐私侵犯等问题。

    arXiv:2402.14845v1 Announce Type: cross  Abstract: The emerging success of large language models (LLMs) heavily relies on collecting abundant training data from external (untrusted) sources. Despite substantial efforts devoted to data cleaning and curation, well-constructed LLMs have been reported to suffer from copyright infringement, data poisoning, and/or privacy violations, which would impede practical deployment of LLMs. In this study, we propose a simple and easily implementable method for purifying LLMs from the negative effects caused by uncurated data, namely, through ensembling LLMs with benign and small language models (SLMs). Aside from theoretical guarantees, we perform comprehensive experiments to empirically confirm the efficacy of ensembling LLMs with SLMs, which can effectively preserve the performance of LLMs while mitigating issues such as copyright infringement, data poisoning, and privacy violations.
    
[^202]: 动态定价的新时代：协同监督学习与二次规划

    The New Era of Dynamic Pricing: Synergizing Supervised Learning and Quadratic Programming

    [https://arxiv.org/abs/2402.14844](https://arxiv.org/abs/2402.14844)

    结合监督学习和二次规划优化汽车租赁行业的动态定价模型，以提高利润率。

    

    在本文中，我们探讨了监督学习和二次规划的新颖组合，用于在汽车租赁行业中优化动态定价模型。我们利用价格弹性的动态建模，借助普通最小二乘（OLS）指标，如P值、同方差性、误差正态性进行优化。当这些指标的基本假设成立时，它们对指导二次规划代理至关重要。该程序旨在为给定的有限目标集优化利润率。

    arXiv:2402.14844v1 Announce Type: cross  Abstract: In this paper, we explore a novel combination of supervised learning and quadratic programming to refine dynamic pricing models in the car rental industry. We utilize dynamic modeling of price elasticity, informed by ordinary least squares (OLS) metrics such as p-values, homoscedasticity, error normality. These metrics, when their underlying assumptions hold, are integral in guiding a quadratic programming agent. The program is tasked with optimizing margin for a given finite set target.
    
[^203]: 具有强化调节的文本扩散模型

    Text Diffusion with Reinforced Conditioning

    [https://arxiv.org/abs/2402.14843](https://arxiv.org/abs/2402.14843)

    提出了一种名为TREC的文本扩散模型，通过强化调节和时间感知方差缩放解决了现有文本扩散模型在训练过程中自我调节的退化和训练与采样不一致的问题，展示了其在不同序列生成任务中的竞争力。

    

    扩散模型在生成高质量图像、视频和音频方面表现出色，由于在迭代改进中的适应性，它们对实现更好的非自回归序列生成具有潜力。然而，由于处理语言的离散性的挑战，现有的文本扩散模型在性能方面仍然存在不足。本文对文本扩散模型进行了彻底分析，并揭示了两个重要限制：训练过程中自我调节的退化和训练与采样之间的不一致性。在我们的发现的启发下，我们提出了一个名为TREC的新型文本扩散模型，通过强化调节缓解了退化问题，通过时间感知方差缩放解决了不一致性。我们的大量实验证明了TREC在自回归、非自回归和扩散基线中的竞争力。此外，定性分析显示...

    arXiv:2402.14843v1 Announce Type: cross  Abstract: Diffusion models have demonstrated exceptional capability in generating high-quality images, videos, and audio. Due to their adaptiveness in iterative refinement, they provide a strong potential for achieving better non-autoregressive sequence generation. However, existing text diffusion models still fall short in their performance due to a challenge in handling the discreteness of language. This paper thoroughly analyzes text diffusion models and uncovers two significant limitations: degradation of self-conditioning during training and misalignment between training and sampling. Motivated by our findings, we propose a novel Text Diffusion model called TREC, which mitigates the degradation with Reinforced Conditioning and the misalignment by Time-Aware Variance Scaling. Our extensive experiments demonstrate the competitiveness of TREC against autoregressive, non-autoregressive, and diffusion baselines. Moreover, qualitative analysis sh
    
[^204]: RFBES在SemEval-2024任务8中的应用：探究用于区分AI生成和人类撰写文本的句法和语义特征

    RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic Features for Distinguishing AI-Generated and Human-Written Texts

    [https://arxiv.org/abs/2402.14838](https://arxiv.org/abs/2402.14838)

    该研究探究了语义和句法两个方面用于区分AI生成文本和人类撰写文本的问题，并提出了一个高准确度的AI模型，在M4数据集上表现出较好的性能。

    

    最近，大型语言模型（LLMs）的使用越来越广泛，并且LLMs已被用于在不同语言和不同任务中生成文本。此外，由于谷歌和OpenAI等知名公司的参与，LLMs现在更易获得，人们可以轻松使用它们。然而，一个重要问题是如何检测AI生成的文本与人类撰写的文本区别。本文从语义和句法两个方面探讨了AI生成文本检测问题。最终，我们提出了一个AI模型，可以在M4数据集上高准确度区分AI生成文本和人类撰写文本，无论是多语言还是单语任务。根据我们的结果，使用语义方法对于检测更有帮助。然而，在句法方法上还有很大改进空间，这将是未来工作的一个良好途径。

    arXiv:2402.14838v1 Announce Type: cross  Abstract: Nowadays, the usage of Large Language Models (LLMs) has increased, and LLMs have been used to generate texts in different languages and for different tasks. Additionally, due to the participation of remarkable companies such as Google and OpenAI, LLMs are now more accessible, and people can easily use them. However, an important issue is how we can detect AI-generated texts from human-written ones. In this article, we have investigated the problem of AI-generated text detection from two different aspects: semantics and syntax. Finally, we presented an AI model that can distinguish AI-generated texts from human-written ones with high accuracy on both multilingual and monolingual tasks using the M4 dataset. According to our results, using a semantic approach would be more helpful for detection. However, there is a lot of room for improvement in the syntactic approach, and it would be a good approach for future work.
    
[^205]: 大型语言模型提示技术的实证分类：从业者指南

    An Empirical Categorization of Prompting Techniques for Large Language Models: A Practitioner's Guide

    [https://arxiv.org/abs/2402.14837](https://arxiv.org/abs/2402.14837)

    编制了一个全面的大型语言模型提示技术清单，并建立了一个跨学科的分类框架，以帮助从业者更有效地利用这些技术。

    

    由于大型语言模型（LLMs）的快速发展，最近用提示语来编程这些模型引起了人们的极大关注。然而，现有提示工程技术的数量庞大，对于希望利用这些工具的从业者来说，这构成了一个令人难以应对的挑战。为了最有效地利用LLMs，编制一个全面的提示技术清单并建立一个标准化的跨学科分类框架是很重要的。本调查研究了一些最知名的提示技术，从学术和实践角度对它们进行了分类，分为七个不同的类别。我们概述了每个类别，旨在澄清它们的独特贡献，并展示它们在真实世界示例中的实际应用，以为同行从业者提供一个结构化框架，帮助他们理解和归类提示技术。

    arXiv:2402.14837v1 Announce Type: cross  Abstract: Due to rapid advancements in the development of Large Language Models (LLMs), programming these models with prompts has recently gained significant attention. However, the sheer number of available prompt engineering techniques creates an overwhelming landscape for practitioners looking to utilize these tools. For the most efficient and effective use of LLMs, it is important to compile a comprehensive list of prompting techniques and establish a standardized, interdisciplinary categorization framework. In this survey, we examine some of the most well-known prompting techniques from both academic and practical viewpoints and classify them into seven distinct categories. We present an overview of each category, aiming to clarify their unique contributions and showcase their practical applications in real-world examples in order to equip fellow practitioners with a structured framework for understanding and categorizing prompting techniqu
    
[^206]: MIKE：细粒度多模态实体知识编辑的新基准

    MIKE: A New Benchmark for Fine-grained Multimodal Entity Knowledge Editing

    [https://arxiv.org/abs/2402.14835](https://arxiv.org/abs/2402.14835)

    MIKE是一个针对细粒度多模态实体知识编辑的全面基准和数据集，突破了现有基准主要侧重于粗粒度知识的局限性，引入了新的知识编辑形式以评估编辑效率。

    

    多模态知识编辑是增强多模态大语言模型（MLLMs）功能的重要进展。尽管其潜力巨大，但当前的基准主要集中在粗粒度知识上，细粒度多模态实体知识的复杂性大多未被探索。为了弥补这一差距，我们引入了MIKE，这是一个专门为细粒度多模态实体知识编辑设计的全面基准和数据集。

    arXiv:2402.14835v1 Announce Type: cross  Abstract: Multimodal knowledge editing represents a critical advancement in enhancing the capabilities of Multimodal Large Language Models (MLLMs). Despite its potential, current benchmarks predominantly focus on coarse-grained knowledge, leaving the intricacies of fine-grained (FG) multimodal entity knowledge largely unexplored. This gap presents a notable challenge, as FG entity recognition is pivotal for the practical deployment and effectiveness of MLLMs in diverse real-world scenarios. To bridge this gap, we introduce MIKE, a comprehensive benchmark and dataset specifically designed for the FG multimodal entity knowledge editing. MIKE encompasses a suite of tasks tailored to assess different perspectives, including Vanilla Name Answering, Entity-Level Caption, and Complex-Scenario Recognition. In addition, a new form of knowledge editing, Multi-step Editing, is introduced to evaluate the editing efficiency. Through our extensive evaluations
    
[^207]: CliqueParcel：一种同时优化效率和忠实度的批处理LLM提示的方法

    CliqueParcel: An Approach For Batching LLM Prompts That Jointly Optimizes Efficiency And Faithfulness

    [https://arxiv.org/abs/2402.14833](https://arxiv.org/abs/2402.14833)

    CliqueParcel提出了一种通过提示批处理来提高LLM效率的方法，旨在在推理过程中同时确保准确性和最小化与原始输出的偏差，解决了折价输出问题。

    

    大型语言模型（LLM）在最近的研究中变得至关重要。然而，在推理过程中，LLM仍然需要大量资源。本文提出了CliqueParcel，一种旨在通过提示批处理来提高LLM效率的方法。现有的优化推理效率的策略通常会对输出质量进行妥协，导致折价输出问题。这个问题可能导致准确性降低或输出缺乏细节。CliqueParcel是我们对这一挑战的回应。在确保准确性和最小化与原始输出的偏差（即忠实度）的情况下，我们的方法在推理过程中显著提高了效率。为了奠定基础，我们首先通过排除由于长度缩短而导致的运行时间减少来重新定义效率测量标准。然后，我们提供了效率和忠实度之间的全面权衡，以阐明“折价输出”问题的本质。

    arXiv:2402.14833v1 Announce Type: cross  Abstract: Large language models (LLMs) have become pivotal in recent research. However, during the inference process, LLMs still require substantial resources. In this paper, we propose CliqueParcel, a method designed to improve the efficiency of LLMs via prompt batching. Existing strategies to optimize inference efficiency often compromise on output quality, leading to a discounted output problem. This issue might result in reduced accuracy or outputs that are less detailed. CliqueParcel is our answer to this challenge. While ensuring accuracy and minimizing deviations from the original outputs (i.e., faithfulness), our method significantly improves efficiency during inference.   To lay the groundwork, we first redefine efficiency measurements by excluding the reduction in running time due to shorter lengths. Then, we provide a comprehensive trade-off between efficiency and faithfulness to clarify the nature of the 'discounted output' problem. 
    
[^208]: 通过窗口选择和节点优化优化妊娠和分娩中的子宫同步分析

    Optimizing Uterine Synchronization Analysis in Pregnancy and Labor through Window Selection and Node Optimization

    [https://arxiv.org/abs/2402.14827](https://arxiv.org/abs/2402.14827)

    通过窗口方法和节点优化分析EHG信号，提出一种新方法来优化妊娠和分娩中的子宫同步分析。

    

    早产是全球5岁以下儿童死亡的主要原因。本文通过分析记录在劳动和妊娠期间母亲腹部的EHG信号，提出了一种新的方法来解决这一问题。EHG信号反映了诱导子宫肌电机收缩的电活动。由于EHG信号被认为是非平稳信号，并且我们预期在收缩过程中连接性会发生改变，我们应用了窗口方法在实际信号上，以帮助我们识别用于分类的最佳窗口和最佳节点的数据。

    arXiv:2402.14827v1 Announce Type: cross  Abstract: Preterm labor (PL) has globally become the leading cause of death in children under the age of 5 years. To address this problem, this paper will provide a new approach by analyzing the EHG signals, which are recorded on the abdomen of the mother during labor and pregnancy. The EHG signal reflects the electrical activity that induces the mechanical contraction of the myometrium. Because EHGs are known to be non-stationary signals, and because we anticipate connectivity to alter during contraction, we applied the windowing approach on real signals to help us identify the best windows and the best nodes with the most significant data to be used for classification. The suggested pipeline includes i) divide the 16 EHG signals that are recorded from the abdomen of pregnant women in N windows; ii) apply the connectivity matrices on each window; iii) apply the Graph theory-based measures on the connectivity matrices on each window; iv) apply t
    
[^209]: 深度伪造检测及有限计算能力的影响

    Deepfake Detection and the Impact of Limited Computing Capabilities

    [https://arxiv.org/abs/2402.14825](https://arxiv.org/abs/2402.14825)

    本研究旨在通过分析在有限计算资源情况下的深度学习技术的适用性以及探索提高效率的方法，解决了深度伪造检测的问题。

    

    技术和人工智能的快速发展使得deepfakes成为一种越来越复杂和具有挑战性的识别技术。为了确保信息的准确性，控制虚假信息和大规模操纵，发现并开发能够检测伪造视频的人工智能模型至关重要。本研究旨在解决在有限计算资源场景下不同数据集中深度伪造的检测问题。目标是分析不同深度学习技术在这些限制条件下的适用性，并探索提高效率的可能方法。

    arXiv:2402.14825v1 Announce Type: cross  Abstract: The rapid development of technologies and artificial intelligence makes deepfakes an increasingly sophisticated and challenging-to-identify technique. To ensure the accuracy of information and control misinformation and mass manipulation, it is of paramount importance to discover and develop artificial intelligence models that enable the generic detection of forged videos. This work aims to address the detection of deepfakes across various existing datasets in a scenario with limited computing resources. The goal is to analyze the applicability of different deep learning techniques under these restrictions and explore possible approaches to enhance their efficiency.
    
[^210]: 将生成式人工智能引入教育中的自适应学习

    Bringing Generative AI to Adaptive Learning in Education

    [https://arxiv.org/abs/2402.14601](https://arxiv.org/abs/2402.14601)

    生成式人工智能技术与自适应学习概念的交叉研究将对教育中下一阶段学习格式的发展做出重要贡献。

    

    最近生成式人工智能技术的激增，如大型语言模型和扩散模型，推动了人工智能在科学、金融和教育等各个领域的应用发展。与此同时，自适应学习这一概念在教育领域引起了极大关注，并证明其在提高学生学习效率方面的有效性。在本立场论文中，我们旨在探讨将生成式人工智能与自适应学习概念结合起来的交叉研究。通过讨论这一领域的好处、挑战和潜力，我们认为这种结合将为教育中下一阶段学习形式的发展做出重要贡献。

    arXiv:2402.14601v1 Announce Type: cross  Abstract: The recent surge in generative AI technologies, such as large language models and diffusion models, have boosted the development of AI applications in various domains, including science, finance, and education. Concurrently, adaptive learning, a concept that has gained substantial interest in the educational sphere, has proven its efficacy in enhancing students' learning efficiency. In this position paper, we aim to shed light on the intersectional studies of these two methods, which combine generative AI with adaptive learning concepts. By presenting discussions about the benefits, challenges, and potentials in this field, we argue that this union will contribute significantly to the development of the next stage learning format in education.
    
[^211]: OmniPred：语言模型作为通用回归器

    OmniPred: Language Models as Universal Regressors

    [https://arxiv.org/abs/2402.14547](https://arxiv.org/abs/2402.14547)

    本文提出了OmniPred框架，用于训练语言模型作为通用的端到端回归器，实验证明，在多个任务上训练时，语言模型能够显著优于传统回归模型。

    

    在实验设计的广阔领域中，回归一直是一个强大的工具，可以准确预测系统或模型在给定一组参数的情况下的结果指标，但传统上只限于适用于特定任务的方法。在本文中，我们提出了OmniPred，这是一个用于训练语言模型作为通用端到端回归器的框架，使用来自多样真实世界实验的$(x,y)$评估数据。通过使用源自Google Vizier的数据，这是世界上最大的黑盒优化数据库之一，我们的大量实验表明，仅通过数学参数和值的文本表示，语言模型能够进行非常精确的数值回归，如果有机会训练多个任务，则可以显著优于传统的回归模型。

    arXiv:2402.14547v1 Announce Type: cross  Abstract: Over the broad landscape of experimental design, regression has been a powerful tool to accurately predict the outcome metrics of a system or model given a set of parameters, but has been traditionally restricted to methods which are only applicable to a specific task. In this paper, we propose OmniPred, a framework for training language models as universal end-to-end regressors over $(x,y)$ evaluation data from diverse real world experiments. Using data sourced from Google Vizier, one of the largest blackbox optimization databases in the world, our extensive experiments demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks, can significantly outperform traditional regression models.
    
[^212]: 并行中点随机化的 Langevin Monte Carlo

    Parallelized Midpoint Randomization for Langevin Monte Carlo

    [https://arxiv.org/abs/2402.14434](https://arxiv.org/abs/2402.14434)

    探索在能够进行梯度平行评估的框架中的抽样问题，提出了并行化的随机中点方法，并通过新技术导出了对抽样和目标密度之间Wasserstein距离的上界，量化了并行处理单元带来的运行时改进。

    

    我们探讨了在可以进行梯度的平行评估的框架中的抽样问题。我们的研究重点放在由平滑和强log-凹密度表征的目标分布上。我们重新审视了并行化的随机中点方法，并运用最近开发用于分析其纯顺序版本的证明技术。利用这些技术，我们得出了抽样和目标密度之间的Wasserstein距离的上界。这些界限量化了通过利用并行处理单元所实现的运行时改进，这可能是相当可观的。

    arXiv:2402.14434v1 Announce Type: cross  Abstract: We explore the sampling problem within the framework where parallel evaluations of the gradient of the log-density are feasible. Our investigation focuses on target distributions characterized by smooth and strongly log-concave densities. We revisit the parallelized randomized midpoint method and employ proof techniques recently developed for analyzing its purely sequential version. Leveraging these techniques, we derive upper bounds on the Wasserstein distance between the sampling and target densities. These bounds quantify the runtime improvement achieved by utilizing parallel processing units, which can be considerable.
    
[^213]: 基于不确定性驱动和对抗校准学习的心外脂肪组织分割

    Uncertainty-driven and Adversarial Calibration Learning for Epicardial Adipose Tissue Segmentation

    [https://arxiv.org/abs/2402.14349](https://arxiv.org/abs/2402.14349)

    提出了一种基于不确定性驱动和对抗校准学习的心外脂肪组织分割方法，通过特征潜空间多级监督网络(SPDNet)，增强分割以更准确估计EAT体积

    

    心外脂肪组织(EAT)是一种可以分泌大量脂联素从而影响心肌和冠状动脉的内脏脂肪。EAT的体积和密度可以作为独立风险标记的测量标准，通过非侵入性磁共振图像测量体积是评估EAT的最佳方法。然而，由于EAT与心包积液之间对比度低以及运动伪影的存在，分割EAT是具有挑战性的。本文提出了一种新颖的特征潜空间多级监督网络(SPDNet)，采用基于不确定性驱动和对抗校准学习以增强分割，以更准确地估计EAT体积。网络首先通过在特征潜空间中将不确定性建模为高斯分布来解决由于开放式医疗环境中医学图像的低质量或超出分布范围而导致EAT边缘模糊的问题，通过使用其贝叶斯估计作为正则化

    arXiv:2402.14349v1 Announce Type: cross  Abstract: Epicardial adipose tissue (EAT) is a type of visceral fat that can secrete large amounts of adipokines to affect the myocardium and coronary arteries. EAT volume and density can be used as independent risk markers measurement of volume by noninvasive magnetic resonance images is the best method of assessing EAT. However, segmenting EAT is challenging due to the low contrast between EAT and pericardial effusion and the presence of motion artifacts. we propose a novel feature latent space multilevel supervision network (SPDNet) with uncertainty-driven and adversarial calibration learning to enhance segmentation for more accurate EAT volume estimation. The network first addresses the blurring of EAT edges due to the medical images in the open medical environments with low quality or out-of-distribution by modeling the uncertainty as a Gaussian distribution in the feature latent space, which using its Bayesian estimation as a regularizatio
    
[^214]: 具有不可微分规则引导扩散的符号音乐生成

    Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion

    [https://arxiv.org/abs/2402.14285](https://arxiv.org/abs/2402.14285)

    介绍了一种用于符号音乐生成的不可微分规则引导的新方法，引入了可以与之即插即用的高时间分辨率潜在扩散架构，对音乐质量取得了显著进步

    

    我们研究了符号音乐生成的问题（例如生成钢琴卷谱），技术重点放在不可微分规则引导上。音乐规则通常以符号形式表达在音符特征上，如音符密度或和弦进行，许多规则是不可微分的，这在使用它们进行引导扩散时存在挑战。我们提出了一种新颖的引导方法，称为随机控制引导（SCG），它仅需要对规则函数进行前向评估，可以与预训练的扩散模型以即插即用的方式一起工作，从而首次实现了对不可微分规则的无训练引导。此外，我们引入了一种用于符号音乐生成的高时间分辨率潜在扩散架构，可以与SCG以即插即用的方式组合。与符号音乐生成中的标准强基线相比，该框架在音乐质量方面展示了明显的进展

    arXiv:2402.14285v1 Announce Type: cross  Abstract: We study the problem of symbolic music generation (e.g., generating piano rolls), with a technical focus on non-differentiable rule guidance. Musical rules are often expressed in symbolic form on note characteristics, such as note density or chord progression, many of which are non-differentiable which pose a challenge when using them for guided diffusion. We propose Stochastic Control Guidance (SCG), a novel guidance method that only requires forward evaluation of rule functions that can work with pre-trained diffusion models in a plug-and-play way, thus achieving training-free guidance for non-differentiable rules for the first time. Additionally, we introduce a latent diffusion architecture for symbolic music generation with high time resolution, which can be composed with SCG in a plug-and-play fashion. Compared to standard strong baselines in symbolic music generation, this framework demonstrates marked advancements in music quali
    
[^215]: 面向公平文本嵌入的内容条件去偏方法

    Content Conditional Debiasing for Fair Text Embedding

    [https://arxiv.org/abs/2402.14208](https://arxiv.org/abs/2402.14208)

    通过在内容条件下确保敏感属性与文本嵌入之间的条件独立性，我们提出了一种可以改善公平性的新方法，在保持效用的同时，解决了缺乏适当训练数据的问题。

    

    在自然语言处理（NLP）中，减轻机器学习模型中的偏见引起了越来越多的关注。然而，只有少数研究集中在公平的文本嵌入上，这对实际应用至关重要且具有挑战性。本文提出了一种学习公平文本嵌入的新方法。我们通过确保在内容条件下敏感属性与文本嵌入之间的条件独立性来实现公平性，同时保持效用权衡。具体来说，我们强制要求具有不同敏感属性但相同内容的文本的嵌入与其对应中立文本的嵌入保持相同的距离。此外，我们通过使用大型语言模型（LLMs）将文本增强为不同的敏感组，来解决缺乏适当训练数据的问题。我们广泛的评估表明，我们的方法有效地提高了公平性同时保持了嵌入的效用。

    arXiv:2402.14208v1 Announce Type: cross  Abstract: Mitigating biases in machine learning models has gained increasing attention in Natural Language Processing (NLP). Yet, only a few studies focus on fair text embeddings, which are crucial yet challenging for real-world applications. In this paper, we propose a novel method for learning fair text embeddings. We achieve fairness while maintaining utility trade-off by ensuring conditional independence between sensitive attributes and text embeddings conditioned on the content. Specifically, we enforce that embeddings of texts with different sensitive attributes but identical content maintain the same distance toward the embedding of their corresponding neutral text. Furthermore, we address the issue of lacking proper training data by using Large Language Models (LLMs) to augment texts into different sensitive groups. Our extensive evaluations demonstrate that our approach effectively improves fairness while preserving the utility of embed
    
[^216]: 神经网络与摩擦：滑动、保持、学习

    Neural Networks and Friction: Slide, Hold, Learn

    [https://arxiv.org/abs/2402.14148](https://arxiv.org/abs/2402.14148)

    循环神经网络利用GRU架构学习合成数据中复杂摩擦定律动力学，展示了机器学习模型在理解和模拟摩擦过程物理的潜力。

    

    本研究表明，利用门控循环单元（GRU）架构的循环神经网络（RNNs）具有学习合成数据中速率与状态摩擦定律复杂动力学的能力。用于训练网络的数据通过应用传统速率与状态摩擦方程结合状态演化老化定律生成。我们方法的一个新颖之处在于制定一个损失函数，该函数明确考虑训练过程中的初始条件、直接效应以及状态变量的演变。研究发现，具有GRU架构的RNN能够有效学习预测摩擦系数由于速度跳跃而产生的变化，展示了机器学习模型在理解和模拟摩擦过程物理的潜力。

    arXiv:2402.14148v1 Announce Type: cross  Abstract: In this study, it is demonstrated that Recurrent Neural Networks (RNNs), specifically those utilizing Gated Recurrent Unit (GRU) architecture, possess the capability to learn the complex dynamics of rate-and-state friction laws from synthetic data. The data employed for training the network is generated through the application of traditional rate-and-state friction equations coupled with the aging law for state evolution. A novel aspect of our approach is the formulation of a loss function that explicitly accounts for initial conditions, the direct effect, and the evolution of state variables during training. It is found that the RNN, with its GRU architecture, effectively learns to predict changes in the friction coefficient resulting from velocity jumps, thereby showcasing the potential of machine learning models in understanding and simulating the physics of frictional processes.
    
[^217]: 离线策略学习的深度生成模型：教程、调查和未来方向展望

    Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions

    [https://arxiv.org/abs/2402.13777](https://arxiv.org/abs/2402.13777)

    深度生成模型在离线策略学习中展现了巨大潜力，本文提供了首个系统性综述，涵盖了五种主流深度生成模型及其应用。

    

    深度生成模型(DGMs)在各个领域展示了巨大成功，特别是在使用从离线数据训练的模型生成文本、图像和视频方面。类似地，基于数据驱动的决策和机器人控制也需要从离线数据中学习一个生成函数作为策略或政策。在这种情况下，将深度生成模型应用于离线策略学习展现出巨大潜力，许多研究在这个方向上进行了探索。然而，这一领域仍然缺乏全面的评估，因此不同分支的发展相对独立。因此，我们提供了深度生成模型在离线策略学习应用方面的第一次系统性综述。具体而言，我们涵盖了五种主流深度生成模型，包括变分自动编码器、生成对抗网络、归一化流、变压器和扩散模型，以及它们的应用。

    arXiv:2402.13777v1 Announce Type: cross  Abstract: Deep generative models (DGMs) have demonstrated great success across various domains, particularly in generating texts, images, and videos using models trained from offline data. Similarly, data-driven decision-making and robotic control also necessitate learning a generator function from the offline data to serve as the strategy or policy. In this case, applying deep generative models in offline policy learning exhibits great potential, and numerous studies have explored in this direction. However, this field still lacks a comprehensive review and so developments of different branches are relatively independent. Thus, we provide the first systematic review on the applications of deep generative models for offline policy learning. In particular, we cover five mainstream deep generative models, including Variational Auto-Encoders, Generative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion Models, and their applicati
    
[^218]: DSLR：多样性增强和结构学习用于基于重播的图持续学习

    DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based Graph Continual Learning

    [https://arxiv.org/abs/2402.13711](https://arxiv.org/abs/2402.13711)

    DSLR提出了一种基于覆盖范围的多样性方法，以解决基于重播的图持续学习中回放节点过于集中导致过拟合和灾难性遗忘的问题。

    

    我们研究了基于重播方法中回放缓冲区对图持续学习（GCL）方法的影响。现有的基于重播的GCL方法为每个类别选择最具代表性的节点并将它们存储在重播缓冲区中，以供在训练后续任务时使用。然而，我们发现，仅考虑每个回放节点的类别代表性会使回放节点集中在每个类别的中心周围，可能存在过拟合于位于那些区域的节点的风险，从而加剧灾难性遗忘。此外，由于基于重播方法严重依赖于少数回放节点来保留从先前任务中获得的知识，涉及在模型训练中具有不相关邻居的回放节点可能对模型性能产生显着的负面影响。在本文中，我们提出了一种名为DSLR的GCL模型，具体来说，我们设计了一种基于覆盖范围的多样性（CD）

    arXiv:2402.13711v1 Announce Type: cross  Abstract: We investigate the replay buffer in rehearsal-based approaches for graph continual learning (GCL) methods. Existing rehearsal-based GCL methods select the most representative nodes for each class and store them in a replay buffer for later use in training subsequent tasks. However, we discovered that considering only the class representativeness of each replayed node makes the replayed nodes to be concentrated around the center of each class, incurring a potential risk of overfitting to nodes residing in those regions, which aggravates catastrophic forgetting. Moreover, as the rehearsal-based approach heavily relies on a few replayed nodes to retain knowledge obtained from previous tasks, involving the replayed nodes that have irrelevant neighbors in the model training may have a significant detrimental impact on model performance. In this paper, we propose a GCL model named DSLR, specifically, we devise a coverage-based diversity (CD)
    
[^219]: 可解释的量子点器件测量分类技术

    Explainable Classification Techniques for Quantum Dot Device Measurements

    [https://arxiv.org/abs/2402.13699](https://arxiv.org/abs/2402.13699)

    提出了一种基于合成数据的的可解释特征技术，利用可解释性提升机（EBMs）实现了在量子点调谐中较高的可解释性和准确性。

    

    在物理科学中，对图像数据的稳健特征表示需求增加：图像采集，在广义上指二维数据，现在在许多领域广泛应用，包括我们在此考虑的量子信息科学。虽然在这些情况下广泛使用传统图像特征，但它们的使用正在迅速被神经网络技术所取代，后者往往以牺牲可解释性为代价换取高准确性。为了弥合这种权衡，我们提出了一种基于合成数据的技术，可以产生可解释的特征。我们利用可解释性提升机（EBMs）展示，这种方法提供了卓越的可解释性，并且不会降低准确性。具体而言，我们展示了在量子点调谐的背景下，这种技术带来了实质性的益处，当前发展阶段需要人类干预。

    arXiv:2402.13699v1 Announce Type: cross  Abstract: In the physical sciences, there is an increased need for robust feature representations of image data: image acquisition, in the generalized sense of two-dimensional data, is now widespread across a large number of fields, including quantum information science, which we consider here. While traditional image features are widely utilized in such cases, their use is rapidly being supplanted by Neural Network-based techniques that often sacrifice explainability in exchange for high accuracy. To ameliorate this trade-off, we propose a synthetic data-based technique that results in explainable features. We show, using Explainable Boosting Machines (EBMs), that this method offers superior explainability without sacrificing accuracy. Specifically, we show that there is a meaningful benefit to this technique in the context of quantum dot tuning, where human intervention is necessary at the current stage of development.
    
[^220]: KetGPT -- 使用Transformer对量子电路进行数据增强

    KetGPT -- Dataset Augmentation of Quantum Circuits using Transformers

    [https://arxiv.org/abs/2402.13352](https://arxiv.org/abs/2402.13352)

    该研究利用Transformer机器学习架构生成“看起来真实”的量子电路，以增强现有的量子电路数据集。

    

    量子算法，表示为量子电路，可用作评估量子系统性能的基准。现有数据集在规模和多样性方面存在限制，在该领域广泛使用，导致研究人员使用随机生成的电路。然而，随机电路并不是代表性基准，因为它们缺乏量子系统制造的真实量子算法的固有属性。这种缺乏“有用”的量子基准构成了推动量子编译器和硬件开发与比较的挑战。本研究旨在通过使用Transformer机器学习架构生成我们称之为“看起来真实”的电路，以增强现有的量子电路数据集。为此，我们引入了KetGPT，一种以OpenQASM语言生成合成电路的工具，其结构是基于推导自量子电路的

    arXiv:2402.13352v1 Announce Type: cross  Abstract: Quantum algorithms, represented as quantum circuits, can be used as benchmarks for assessing the performance of quantum systems. Existing datasets, widely utilized in the field, suffer from limitations in size and versatility, leading researchers to employ randomly generated circuits. Random circuits are, however, not representative benchmarks as they lack the inherent properties of real quantum algorithms for which the quantum systems are manufactured. This shortage of `useful' quantum benchmarks poses a challenge to advancing the development and comparison of quantum compilers and hardware.   This research aims to enhance the existing quantum circuit datasets by generating what we refer to as `realistic-looking' circuits by employing the Transformer machine learning architecture. For this purpose, we introduce KetGPT, a tool that generates synthetic circuits in OpenQASM language, whose structure is based on quantum circuits derived f
    
[^221]: DeepCode AI Fix: 使用大型语言模型修复安全漏洞

    DeepCode AI Fix: Fixing Security Vulnerabilities with Large Language Models

    [https://arxiv.org/abs/2402.13291](https://arxiv.org/abs/2402.13291)

    使用大型语言模型修复复杂语义bug，通过新的查询和微调方法来解决长距离代码关系学习的挑战。

    

    自动程序修复领域能引起广泛关注，但尽管有大量研究工作，创建一个对于复杂语义错误（如安全漏洞）效果良好的系统仍然很困难。解决这一挑战的一个有前途的方向是利用越来越多地用于解决各种编程任务的大型语言模型（LLMs）。本文研究了LLMs在解决代码修复任务中的有效性。我们表明这个任务很困难，因为它要求模型学习长距离的代码关系，这是一个天然依赖大量训练数据的任务。同时，为复杂程序错误和其对应修复创建一个大型且干净的数据集并不是一个简单的事情。我们提出了一种方法来应对这些挑战，通过一种新的查询和微调LLMs的方法。这个想法是利用程序分析来限制LLMs的关注度。

    arXiv:2402.13291v1 Announce Type: cross  Abstract: The automated program repair field has attracted substantial interest over the years, but despite significant research efforts, creating a system that works well for complex semantic bugs such as security vulnerabilities has proven difficult. A promising direction to solve this challenge is by leveraging large language models (LLMs), which are increasingly used to solve various programming tasks. In this paper, we investigate the effectiveness of LLMs for solving code-repair task. We show that the task is difficult as it requires the model to learn long-range code relationships, a task that inherently relies on extensive amounts of training data. At the same time, creating a large, clean dataset for complex program bugs and their corresponding fixes is non-trivial. We propose a technique to address these challenges with a new approach for querying and fine-tuning LLMs. The idea is to use program analysis to limit the LLM's attention me
    
[^222]: 使用SMOTETomek在WSNs中基于机器学习的入侵检测

    MLSTL-WSN: Machine Learning-based Intrusion Detection using SMOTETomek in WSNs

    [https://arxiv.org/abs/2402.13277](https://arxiv.org/abs/2402.13277)

    提出了一种将机器学习技术与Synthetic Minority Oversampling Technique Tomek Link (SMOTE-TomekLink)算法相结合的创新入侵检测方法

    

    无线传感器网络(WSNs)在基础设施中发挥着关键作用，包括固定和移动传感器。这些传感器自组织并建立多跳连接进行通信，共同感知、收集、处理和传输有关周围环境的数据。尽管它们的重要性，WSNs面临着可能破坏功能的快速和有害的攻击。现有的WSN入侵检测方法遇到了低检测率、计算开销和误报警的挑战。这些问题源于传感器节点资源约束、数据冗余以及网络内高相关性。为了解决这些挑战，我们提出了一种创新的入侵检测方法，该方法将机器学习技术与Synthetic Minority Oversampling Technique Tomek Link (SMOTE-TomekLink)算法相结合。这种融合合成了少数实例并消除了Tomek链接，结果

    arXiv:2402.13277v1 Announce Type: cross  Abstract: Wireless Sensor Networks (WSNs) play a pivotal role as infrastructures, encompassing both stationary and mobile sensors. These sensors self-organize and establish multi-hop connections for communication, collectively sensing, gathering, processing, and transmitting data about their surroundings. Despite their significance, WSNs face rapid and detrimental attacks that can disrupt functionality. Existing intrusion detection methods for WSNs encounter challenges such as low detection rates, computational overhead, and false alarms. These issues stem from sensor node resource constraints, data redundancy, and high correlation within the network. To address these challenges, we propose an innovative intrusion detection approach that integrates Machine Learning (ML) techniques with the Synthetic Minority Oversampling Technique Tomek Link (SMOTE-TomekLink) algorithm. This blend synthesizes minority instances and eliminates Tomek links, result
    
[^223]: SzCORE：用于验证基于脑电图的自动癫痫检测算法的癫痫社区开源研究评估框架

    SzCORE: A Seizure Community Open-source Research Evaluation framework for the validation of EEG-based automated seizure detection algorithms

    [https://arxiv.org/abs/2402.13005](https://arxiv.org/abs/2402.13005)

    提出了SzCORE框架，用于验证基于脑电图的自动癫痫检测算法，旨在标准化验证方法，包括数据集、文件格式、输入内容、性能度量等。

    

    随着家庭和长期脑电图监测的增加，基于脑电图的高质量自动癫痫检测算法的需求变得更加迫切。这些算法验证方法的异质性影响了报告的结果，并使全面评估和比较变得具有挑战性。该异质性主要涉及数据集的选择、评估方法和性能度量等方面。本文提出了一个统一的框架，旨在建立EEG基础癫痫检测算法验证的标准化。基于现有指南和建议，该框架引入了一组关于数据集、文件格式、EEG数据输入内容、癫痫注释输入和输出、交叉验证策略以及性能度量的建议和标准。

    arXiv:2402.13005v1 Announce Type: cross  Abstract: The need for high-quality automated seizure detection algorithms based on electroencephalography (EEG) becomes ever more pressing with the increasing use of ambulatory and long-term EEG monitoring. Heterogeneity in validation methods of these algorithms influences the reported results and makes comprehensive evaluation and comparison challenging. This heterogeneity concerns in particular the choice of datasets, evaluation methodologies, and performance metrics. In this paper, we propose a unified framework designed to establish standardization in the validation of EEG-based seizure detection algorithms. Based on existing guidelines and recommendations, the framework introduces a set of recommendations and standards related to datasets, file formats, EEG data input content, seizure annotation input and output, cross-validation strategies, and performance metrics. We also propose the 10-20 seizure detection benchmark, a machine-learning 
    
[^224]: 在多对一图像到图像翻译上改进深度生成模型

    Improving Deep Generative Models on Many-To-One Image-to-Image Translation

    [https://arxiv.org/abs/2402.12531](https://arxiv.org/abs/2402.12531)

    介绍了一种新的非对称框架，可改进现有深度生成模型在多对一图像到图像翻译上的效果，并在 StarGAN V2 上展示了其性能优化。

    

    arXiv:2402.12531v1 通告类型: 跨 针对图像到图像翻译中的多个应用，已应用深度生成模型。 生成对抗网络和扩散模型展示了令人印象深刻的结果，在这些任务上取得了新的最先进结果。 大多数方法在数据集中的不同领域之间具有对称设置。 这些方法假设所有领域都具有多个模态或仅一个模态。 但是，许多数据集存在两个域之间的多对一关系。 在这项工作中，我们首先介绍了一个Colorized MNIST数据集和一个Color-Recall分数，它可以为在多对一翻译上评估模型提供一个简单的基准。 然后，我们引入了一个新的非对称框架，以改进现有的深度生成模型在多对一图像到图像翻译上的表现。 我们将这个框架应用到 StarGAN V2 上，并表明在无监督和半监督设置中，这个新模型的性能得到了提升。

    arXiv:2402.12531v1 Announce Type: cross  Abstract: Deep generative models have been applied to multiple applications in image- to-image translation. Generative Adversarial Networks and Diffusion Models have presented impressive results, setting new state-of-the-art results on these tasks. Most methods have symmetric setups across the different domains in a dataset. These methods assume that all domains have either multiple modalities or only one modality. However, there are many datasets that have a many-to-one relationship between two domains. In this work, we first introduce a Colorized MNIST dataset and a Color-Recall score that can provide a simple benchmark for evaluating models on many-to-one translation. We then introduce a new asymmetric framework to improve existing deep generative models on many-to-one image-to- image translation. We apply this framework to StarGAN V2 and show that in both unsupervised and semi-supervised settings, the performance of this new model improves o
    
[^225]: 表格作为图片？探讨LLM在多模态表格数据表示上的优势和局限性

    Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data

    [https://arxiv.org/abs/2402.12424](https://arxiv.org/abs/2402.12424)

    本研究探讨了LLM在解释表格数据方面的有效性，比较了文本和图像表格表示对LLM性能的影响，为在表格相关任务上有效使用LLM提供了见解。

    

    在本文中，我们通过不同的提示策略和数据格式研究了各种LLM在解释表格数据方面的有效性。我们的分析涵盖了六个针对与表格相关任务的基准，如问答和事实核查。我们首次介绍了LLM在基于图像的表格表示上的表现评估。具体地，我们比较了五种基于文本和三种基于图像的表格表示，展示了表示和提示对LLM性能的影响。我们的研究为在表格相关任务上有效使用LLM提供了见解。

    arXiv:2402.12424v1 Announce Type: cross  Abstract: In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking. We introduce for the first time the assessment of LLMs' performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the influence of representation and prompting on LLM performance. Our study provides insights into the effective use of LLMs on table-related tasks.
    
[^226]: 利用部分掩码融合的Gromov-Wasserstein匹配进行任意大小图的端对端监督预测

    End-to-end Supervised Prediction of Arbitrary-size Graphs with Partially-Masked Fused Gromov-Wasserstein Matching

    [https://arxiv.org/abs/2402.12269](https://arxiv.org/abs/2402.12269)

    提出了利用部分掩码融合的Gromov-Wasserstein匹配进行任意大小图的端对端监督预测方法，并展示了其在不同任务上相比竞争者更高的效率和多功能性。

    

    我们提出了一种新颖的基于端到端深度学习的监督图预测（SGP）方法。我们引入了一种原始的基于最优输运（OT）的损失，部分掩码融合的Gromov-Wasserstein损失（PM-FGW），可以直接利用图表示，比如邻接和特征矩阵。PM-FGW具有SGP的所有理想属性：节点排列不变性，可微分性，通过比较它们的填充表示以及它们的掩码向量处理不同大小的图。此外，我们提出了一个灵活的基于transformer的架构，可以轻松适应不同类型的输入数据。在实验部分，三个不同的任务，一个新颖且具有挑战性的合成数据集（image2graph）和两个真实任务，图像到地图和指纹到分子 - 展示了该方法相比竞争者的效率和多功能性。

    arXiv:2402.12269v1 Announce Type: new  Abstract: We present a novel end-to-end deep learning-based approach for Supervised Graph Prediction (SGP). We introduce an original Optimal Transport (OT)-based loss, the Partially-Masked Fused Gromov-Wasserstein loss (PM-FGW), that allows to directly leverage graph representations such as adjacency and feature matrices. PM-FGW exhibits all the desirable properties for SGP: it is node permutation invariant, sub-differentiable and handles graphs of different sizes by comparing their padded representations as well as their masking vectors. Moreover, we present a flexible transformer-based architecture that easily adapts to different types of input data. In the experimental section, three different tasks, a novel and challenging synthetic dataset (image2graph) and two real-world tasks, image2map and fingerprint2molecule - showcase the efficiency and versatility of the approach compared to competitors.
    
[^227]: 从大规模语言模型的视角探索多功能图学习方法

    Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models

    [https://arxiv.org/abs/2402.11641](https://arxiv.org/abs/2402.11641)

    本文提出了一种利用大规模语言模型设计多功能图学习方法的新概念原型，重点关注“在哪里”和“如何”的角度。

    

    图结构数据是常用的，并在现实世界中有广泛的应用场景。面对多样的学习任务、图领域和复杂的图学习过程，传统的设计多功能图学习方法对人类专家提出挑战。本文提出了一个新颖的概念原型，用于设计具有大规模语言模型（LLMs）的多功能图学习方法，特别关注“在哪里”和“如何”的角度。从“在哪里”的角度，我们总结了四个关键的图学习过程，包括任务定义、图数据特征工程、模型选择与优化、部署与服务。然后，我们探讨了LLMs在这些过程中的应用场景。在“如何”的角度，

    arXiv:2402.11641v1 Announce Type: new  Abstract: Graph-structured data are the commonly used and have wide application scenarios in the real world. For these diverse applications, the vast variety of learning tasks, graph domains, and complex graph learning procedures present challenges for human experts when designing versatile graph learning approaches. Facing these challenges, large language models (LLMs) offer a potential solution due to the extensive knowledge and the human-like intelligence. This paper proposes a novel conceptual prototype for designing versatile graph learning methods with LLMs, with a particular focus on the ``where'' and ``how'' perspectives. From the ``where'' perspective, we summarize four key graph learning procedures, including task definition, graph data feature engineering, model selection and optimization, deployment and serving. We then explore the application scenarios of LLMs in these procedures across a wider spectrum. In the ``how'' perspective, we
    
[^228]: 强健的智能体学习因果世界模型

    Robust agents learn causal world models

    [https://arxiv.org/abs/2402.10877](https://arxiv.org/abs/2402.10877)

    智能体必须学习因果模型才能在广泛的分布转变下达到后悔界限，这对迁移学习和因果推断等研究领域有重要影响。

    

    一直有人假设因果推理在强健且具有通用智能中起着基础作用，然而不清楚智能体是否必须学习因果模型才能推广到新的领域，或者其他归纳偏差是否足够。我们回答了这个问题，表明任何能够在大量分布转变下满足后悔界限的智能体必须学习数据生成过程的近似因果模型，对于优化智能体来说，该近似模型会收敛到真实的因果模型。我们讨论了这一结果对于多个研究领域，包括迁移学习和因果推断的影响。

    arXiv:2402.10877v1 Announce Type: new  Abstract: It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound under a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.
    
[^229]: 侧信息中的Stackelberg博弈中的后悔最小化

    Regret Minimization in Stackelberg Games with Side Information

    [https://arxiv.org/abs/2402.08576](https://arxiv.org/abs/2402.08576)

    这篇论文研究了侧信息中的Stackelberg博弈，提出了一种方法来解决现实中玩家之间信息交流不充分的情况，并且证明了在这种情况下后悔最小化是有效的。

    

    在最基本的情况下，Stackelberg博弈是一个双人博弈，其中领导者承诺一种（混合）策略，追随者做出最佳反应。在过去的十年中，Stackelberg博弈算法是算法博弈论的最大成功之一，因为Stackelberg博弈的算法已经在许多现实世界的领域中被应用，包括机场安全、反盗猎和网络犯罪预防。然而，这些算法通常未能考虑到每个玩家可用的额外信息（例如交通模式，天气条件，网络拥塞），这是现实的显著特征，可能会显著影响到两个玩家的最优策略。我们将这样的情况形式化为带有侧信息的Stackelberg博弈，其中两个玩家在进行游戏之前都观察到一个外部环境。然后，领导者承诺一种（可能依赖于上下文的）策略，追随者对领导者的策略和上下文都做出最佳反应。

    In its most basic form, a Stackelberg game is a two-player game in which a leader commits to a (mixed) strategy, and a follower best-responds. Stackelberg games are perhaps one of the biggest success stories of algorithmic game theory over the last decade, as algorithms for playing in Stackelberg games have been deployed in many real-world domains including airport security, anti-poaching efforts, and cyber-crime prevention. However, these algorithms often fail to take into consideration the additional information available to each player (e.g. traffic patterns, weather conditions, network congestion), a salient feature of reality which may significantly affect both players' optimal strategies. We formalize such settings as Stackelberg games with side information, in which both players observe an external context before playing. The leader then commits to a (possibly context-dependent) strategy, and the follower best-responds to both the leader's strategy and the context. We focus on t
    
[^230]: 基于分数的生成模型在学习一个子高斯概率分布族中突破了维数灾难

    Score-based generative models break the curse of dimensionality in learning a family of sub-Gaussian probability distributions

    [https://arxiv.org/abs/2402.08082](https://arxiv.org/abs/2402.08082)

    基于分数的生成模型在学习一个子高斯概率分布族中突破了维数灾难，通过分数匹配生成的分布以维度无关的速率逼近目标分布的总变差。

    

    尽管基于分数的生成模型（SGMs）在巨大的图像生成任务中取得了显著的成功，但它们的数学基础仍然有限。在本文中，我们分析了SGMs在学习一个子高斯概率分布族中的近似和泛化。我们引入了一种关于概率分布复杂性的概念，即相对密度与标准高斯测度的相对密度。我们证明，如果对数相对密度可以通过神经网络进行局部逼近，并且网络参数可以适当地受限，那么通过经验分数匹配生成的分布以维度无关的速率逼近目标分布的总变差。我们通过示例说明了我们的理论，其中包括某些高斯混合分布。我们证明的一个关键点是推导出与正向过程相关的真实得分函数的维度无关的深度神经网络逼近速率。

    While score-based generative models (SGMs) have achieved remarkable success in enormous image generation tasks, their mathematical foundations are still limited. In this paper, we analyze the approximation and generalization of SGMs in learning a family of sub-Gaussian probability distributions. We introduce a notion of complexity for probability distributions in terms of their relative density with respect to the standard Gaussian measure. We prove that if the log-relative density can be locally approximated by a neural network whose parameters can be suitably bounded, then the distribution generated by empirical score matching approximates the target distribution in total variation with a dimension-independent rate. We illustrate our theory through examples, which include certain mixtures of Gaussians. An essential ingredient of our proof is to derive a dimension-free deep neural network approximation rate for the true score function associated with the forward process, which is inte
    
[^231]: 在线顺序决策中的未知延迟问题

    Online Sequential Decision-Making with Unknown Delays

    [https://arxiv.org/abs/2402.07703](https://arxiv.org/abs/2402.07703)

    本文提出了在在线顺序决策中处理未知延迟问题的三个延迟算法族，并提供了相应的遗憾界限。

    

    在在线顺序决策领域，我们利用在线凸优化（OCO）框架解决了具有延迟的问题，其中决策的反馈可能以未知延迟到达。与之前仅限于欧几里得范数和梯度信息的研究不同，我们提出了三个基于近似解的延迟算法族，处理不同类型的接收反馈。我们提出的算法是多功能且适用于通用范数。具体地，我们引入了一系列针对具有完整损失函数信息反馈的延迟规范化领导算法族，一系列针对具有梯度信息反馈的延迟镜像下降算法族，以及一系列针对相应决策点损失函数梯度值信息反馈的简化延迟镜像下降算法族。对于每种类型的算法，我们提供了相应的遗憾界限。

    In the field of online sequential decision-making, we address the problem with delays utilizing the framework of online convex optimization (OCO), where the feedback of a decision can arrive with an unknown delay. Unlike previous research that is limited to Euclidean norm and gradient information, we propose three families of delayed algorithms based on approximate solutions to handle different types of received feedback. Our proposed algorithms are versatile and applicable to universal norms. Specifically, we introduce a family of Follow the Delayed Regularized Leader algorithms for feedback with full information on the loss function, a family of Delayed Mirror Descent algorithms for feedback with gradient information on the loss function and a family of Simplified Delayed Mirror Descent algorithms for feedback with the value information of the loss function's gradients at corresponding decision points. For each type of algorithm, we provide corresponding regret bounds under cases of 
    
[^232]: 大型代码模型是否理解编程概念？一种黑盒方法探究

    Do Large Code Models Understand Programming Concepts? A Black-box Approach

    [https://arxiv.org/abs/2402.05980](https://arxiv.org/abs/2402.05980)

    本文使用反事实分析框架评估了十个大型代码模型对四种编程概念的理解情况，发现当前模型缺乏对数据流和控制流等概念的理解。

    

    大型语言模型在文本生成方面的成功也使其在代码生成和编码任务方面表现更好。虽然有很多工作展示了它们在代码补全和编辑等任务上的出色性能，但为什么它们能够成功还不清楚。我们通过探索自回归模型对底层程序的逻辑结构理解程度，来填补这一差距。我们提出了用于编程概念谓词的反事实分析（CACP）作为一种反事实测试框架，以评估大型代码模型是否理解编程概念。只通过黑盒访问模型，我们使用CACP评估了十个流行的大型代码模型对四个不同编程概念的理解情况。我们的研究结果表明，当前模型缺乏对数据流和控制流等概念的理解。

    Large Language Models' success on text generation has also made them better at code generation and coding tasks. While a lot of work has demonstrated their remarkable performance on tasks such as code completion and editing, it is still unclear as to why. We help bridge this gap by exploring to what degree auto-regressive models understand the logical constructs of the underlying programs. We propose Counterfactual Analysis for Programming Concept Predicates (CACP) as a counterfactual testing framework to evaluate whether Large Code Models understand programming concepts. With only black-box access to the model, we use CACP to evaluate ten popular Large Code Models for four different programming concepts. Our findings suggest that current models lack understanding of concepts such as data flow and control flow.
    
[^233]: SDEMG: 基于得分的扩散模型用于表面肌电信号去噪

    SDEMG: Score-based Diffusion Model for Surface Electromyographic Signal Denoising

    [https://arxiv.org/abs/2402.03808](https://arxiv.org/abs/2402.03808)

    SDEMG是一种基于得分的扩散模型，用于表面肌电信号去噪。实验证明，SDEMG胜过了其他比较方法。

    

    当被监测的肌肉靠近心脏时，表面肌电图（sEMG）记录可能受到心电图（ECG）信号的影响。一些现有方法使用基于信号处理的方法，如高通滤波器和模板减法，而一些方法则通过求取映射函数从带有ECG干扰的sEMG（噪声sEMG）中恢复出干净的sEMG信号。最近引入了一种著名的生成模型，即基于得分的扩散模型，用于通过噪声输入数据生成高质量和准确的样本。在本研究中，我们提出了一种新颖的方法，称为SDEMG，用作sEMG信号去噪的基于得分的扩散模型。为了评估所提出的SDEMG方法，我们使用来自开放接源的Non-Invasive Adaptive Prosthetics数据库的数据以及来自MIT-BIH Normal Sinus Rhythm数据库的ECG信号进行了降噪实验。实验结果表明，SDEMG胜过了比较方法。

    Surface electromyography (sEMG) recordings can be influenced by electrocardiogram (ECG) signals when the muscle being monitored is close to the heart. Several existing methods use signal-processing-based approaches, such as high-pass filter and template subtraction, while some derive mapping functions to restore clean sEMG signals from noisy sEMG (sEMG with ECG interference). Recently, the score-based diffusion model, a renowned generative model, has been introduced to generate high-quality and accurate samples with noisy input data. In this study, we proposed a novel approach, termed SDEMG, as a score-based diffusion model for sEMG signal denoising. To evaluate the proposed SDEMG approach, we conduct experiments to reduce noise in sEMG signals, employing data from an openly accessible source, the Non-Invasive Adaptive Prosthetics database, along with ECG signals from the MIT-BIH Normal Sinus Rhythm Database. The experiment result indicates that SDEMG outperformed comparative methods a
    
[^234]: CPT: 应用于少样本节点分类的能 力递进式训练策略

    CPT: Competence-progressive Training Strategy for Few-shot Node Classification

    [https://arxiv.org/abs/2402.00450](https://arxiv.org/abs/2402.00450)

    CPT是一种新颖的两阶段课程学习方法，弥补了传统元学习方法在少样本节点分类上的困难。它使用能力递进的训练策略来提高元学习器的效果和稳定性。

    

    图神经网络（GNNs）在节点分类方面取得了显著的进展，但其成功仍然依赖于训练数据中每个类别有足够的标记节点。现实世界中的图数据通常呈现出长尾分布，标签稀疏，强调了GNN在少样本节点分类中的重要性，即使用有限的数据对节点进行分类。传统的情节元学习方法在这个领域显示出了潜力，但它们面临着固有的限制：随机和均匀任务分配可能导致模型收敛到次优解，忽视了任务的难度水平。这可能导致元学习器过早地面临复杂任务，阻碍了正常的学习。理想情况下，元学习器应该从简单概念开始，逐渐进入更复杂的概念，就像人类学习一样。因此，我们引入了CPT，一种新颖的两阶段课程学习方法，将任务难度与元学习器的递进能力相匹配，增强了元学习的效果和稳定性。

    Graph Neural Networks (GNNs) have made significant advancements in node classification, but their success relies on sufficient labeled nodes per class in the training data. Real-world graph data often exhibits a long-tail distribution with sparse labels, emphasizing the importance of GNNs' ability in few-shot node classification, which entails categorizing nodes with limited data. Traditional episodic meta-learning approaches have shown promise in this domain, but they face an inherent limitation: it might lead the model to converge to suboptimal solutions because of random and uniform task assignment, ignoring task difficulty levels. This could lead the meta-learner to face complex tasks too soon, hindering proper learning. Ideally, the meta-learner should start with simple concepts and advance to more complex ones, like human learning. So, we introduce CPT, a novel two-stage curriculum learning method that aligns task difficulty with the meta-learner's progressive competence, enhanci
    
[^235]: 不带位置编码的图变压器

    Graph Transformers without Positional Encodings

    [https://arxiv.org/abs/2401.17791](https://arxiv.org/abs/2401.17791)

    本文介绍了一种不需要位置编码的图变压器模型，该模型通过注意机制本身包含图结构信息，并通过实验证明了其有效性。

    

    最近，用于图表示学习的变压器越来越受欢迎，在各种数据集上取得了最先进的性能，无论是单独使用还是与消息传递图神经网络（MP-GNN）结合。将图归纳偏见融入天然与结构无关的变压器架构中，以结构或位置编码（PEs）的形式，是实现这些令人印象深刻的结果的关键。然而，设计这样的编码是棘手的，人们已经提出了不同的尝试来设计这样的编码，包括拉普拉斯特征向量、相对随机行走概率（RRWP）、空间编码、中心度编码、边缘编码等。在这项工作中，我们认为这些编码可能根本不需要，只要注意机制本身包含有关图结构的信息。我们介绍了Eigenformer，它使用一种新颖的谱感知注意机制，了解图的拉普拉斯谱，并通过实验证明

    Recently, Transformers for graph representation learning have become increasingly popular, achieving state-of-the-art performance on a wide-variety of datasets, either alone or in combination with message-passing graph neural networks (MP-GNNs). Infusing graph inductive-biases in the innately structure-agnostic transformer architecture in the form of structural or positional encodings (PEs) is key to achieving these impressive results. However, designing such encodings is tricky and disparate attempts have been made to engineer such encodings including Laplacian eigenvectors, relative random-walk probabilities (RRWP), spatial encodings, centrality encodings, edge encodings etc. In this work, we argue that such encodings may not be required at all, provided the attention mechanism itself incorporates information about the graph structure. We introduce Eigenformer, which uses a novel spectrum-aware attention mechanism cognizant of the Laplacian spectrum of the graph, and empirically show
    
[^236]: 重新思考多元时间序列预测的通道相关性：从领先指标中学习

    Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators

    [https://arxiv.org/abs/2401.17548](https://arxiv.org/abs/2401.17548)

    本文提出了一种新方法，LIFT，通过利用通道相关性和领先指标，为多元时间序列预测提供准确的预测。LIFT方法可以无缝与任意时间序列预测方法协作，大量实验证明了其有效性。

    

    最近，独立于通道的方法在多元时间序列（MTS）预测中取得了最先进的性能。尽管这些方法减少了过拟合的风险，但它们错过了利用通道相关性进行准确预测的潜在机会。我们认为，在变量之间存在局部平稳的领先-滞后关系，即一些滞后变量在短时间内可能遵循领先指标。利用这种通道相关性是有益的，因为领先指标提供了先进信息，可以用来减少滞后变量的预测难度。在本文中，我们提出了一种名为LIFT的新方法，该方法首先在每个时间步骤高效地估计领先指标及其领先步骤，然后巧妙地允许滞后变量利用来自领先指标的先进信息。LIFT作为一个插件，可以与任意时间序列预测方法无缝协作。进行了大量实验证明了LIFT方法的有效性。

    Recently, channel-independent methods have achieved state-of-the-art performance in multivariate time series (MTS) forecasting. Despite reducing overfitting risks, these methods miss potential opportunities in utilizing channel dependence for accurate predictions. We argue that there exist locally stationary lead-lag relationships between variates, i.e., some lagged variates may follow the leading indicators within a short time period. Exploiting such channel dependence is beneficial since leading indicators offer advance information that can be used to reduce the forecasting difficulty of the lagged variates. In this paper, we propose a new method named LIFT that first efficiently estimates leading indicators and their leading steps at each time step and then judiciously allows the lagged variates to utilize the advance information from leading indicators. LIFT plays as a plugin that can be seamlessly collaborated with arbitrary time series forecasting methods. Extensive experiments o
    
[^237]: 结构化概率编码

    Structured Probabilistic Coding

    [https://arxiv.org/abs/2312.13933](https://arxiv.org/abs/2312.13933)

    结构化概率编码（SPC）是一种新的监督式表示学习框架，通过编码和预测任务的信息来学习紧凑且信息丰富的表示，提高语言模型的泛化能力和语言理解能力，并通过结构化正则化实现更好的覆盖率。

    

    本论文提出了一种新的监督式表示学习框架，即结构化概率编码（SPC），用于从与目标任务相关的输入中学习紧凑和信息丰富的表示。SPC是一种仅有编码器的概率编码技术，具有来自目标空间的结构化正则化。它可以提高预训练语言模型的泛化能力，以实现更好的语言理解。具体而言，我们的概率编码在一个模块中同时进行信息编码和任务预测，以更充分地利用输入数据中的有效信息。它使用输出空间的变分推断来减少随机性和不确定性。此外，为了更好地控制概率表示的学习过程，在潜在空间中提出了结构化正则化，以促进类别之间的均匀性。通过正则化项，SPC可以保持潜在编码的高斯结构，并实现更好的覆盖率。

    This paper presents a new supervised representation learning framework, namely structured probabilistic coding (SPC), to learn compact and informative representations from input related to the target task. SPC is an encoder-only probabilistic coding technology with a structured regularization from the target space. It can enhance the generalization ability of pre-trained language models for better language understanding. Specifically, our probabilistic coding simultaneously performs information encoding and task prediction in one module to more fully utilize the effective information from input data. It uses variational inference in the output space to reduce randomness and uncertainty. Besides, to better control the learning process of probabilistic representations, a structured regularization is proposed to promote uniformity across classes in the latent space. With the regularization term, SPC can preserve the Gaussian structure of the latent code and achieve better coverage of the 
    
[^238]: 通过负蒸馏针对极端嘈杂客户的联邦学习

    Federated Learning with Extremely Noisy Clients via Negative Distillation

    [https://arxiv.org/abs/2312.12703](https://arxiv.org/abs/2312.12703)

    通过负蒸馏方法，本研究针对极端嘈杂客户的问题，提出了一种通过减少在嘈杂数据上训练的客户权重来优化模型性能的解决方案。

    

    联邦学习已经在合作训练深度模型方面取得了显著成功，但通常在处理嘈杂标签时存在困难。先进的作品提出通过一种假设较弱标签嘈杂性的重新加权策略来解决标签噪声问题。然而，由于高度污染的客户，在许多真实世界的联邦学习场景中，这种假设可能被违反，导致极端嘈杂比例，例如大于90%。为了解决极端嘈杂客户的问题，我们研究了重新加权策略的鲁棒性，得出了一种悲观结论：减少在嘈杂数据上训练的客户的权重胜过重新加权策略。为了利用在嘈杂客户上训练的模型，我们提出了一种新方法，称为负蒸馏（FedNed）。FedNed首先识别嘈杂客户，并采用而不是丢弃这些嘈杂客户，以知识蒸馏的方式进行训练。特别是，被识别为嘈杂的客户需要使用

    arXiv:2312.12703v2 Announce Type: replace  Abstract: Federated learning (FL) has shown remarkable success in cooperatively training deep models, while typically struggling with noisy labels. Advanced works propose to tackle label noise by a re-weighting strategy with a strong assumption, i.e., mild label noise. However, it may be violated in many real-world FL scenarios because of highly contaminated clients, resulting in extreme noise ratios, e.g., $>$90%. To tackle extremely noisy clients, we study the robustness of the re-weighting strategy, showing a pessimistic conclusion: minimizing the weight of clients trained over noisy data outperforms re-weighting strategies. To leverage models trained on noisy clients, we propose a novel approach, called negative distillation (FedNed). FedNed first identifies noisy clients and employs rather than discards the noisy clients in a knowledge distillation manner. In particular, clients identified as noisy ones are required to train models using 
    
[^239]: 重新审视标签平滑在增强文本情感分类中的作用

    Revisiting the Role of Label Smoothing in Enhanced Text Sentiment Classification

    [https://arxiv.org/abs/2312.06522](https://arxiv.org/abs/2312.06522)

    通过在文本情感分类中进行深入分析，发现标签平滑可以加速深度模型的收敛，并使不同标签的样本更容易区分

    

    标签平滑是一种广泛应用于各个领域的技术，如文本分类、图像分类和语音识别，以有效对抗模型过拟合而闻名。然而，对于标签平滑如何增强文本情感分类的细致分析却很少。为了填补这一空白，本文在八个文本情感分类数据集和三种深度学习架构（TextCNN、BERT和RoBERTa）以及两种学习方案下进行了一系列深入分析。通过调整平滑参数，我们可以在每个模型架构的几乎所有数据集上实现性能提升。我们进一步研究了标签平滑的好处，发现标签平滑可以加速深度模型的收敛，并使不同标签的样本更容易区分。

    arXiv:2312.06522v2 Announce Type: replace-cross  Abstract: Label smoothing is a widely used technique in various domains, such as text classification, image classification and speech recognition, known for effectively combating model overfitting. However, there is little fine-grained analysis on how label smoothing enhances text sentiment classification. To fill in the gap, this article performs a set of in-depth analyses on eight datasets for text sentiment classification and three deep learning architectures: TextCNN, BERT, and RoBERTa, under two learning schemes: training from scratch and fine-tuning. By tuning the smoothing parameters, we can achieve improved performance on almost all datasets for each model architecture. We further investigate the benefits of label smoothing, finding that label smoothing can accelerate the convergence of deep models and make samples of different labels easily distinguishable.
    
[^240]: 在序贯决策中记住公平：非马尔可夫公平性

    Remembering to Be Fair: Non-Markovian Fairness in Sequential Decision Making

    [https://arxiv.org/abs/2312.04772](https://arxiv.org/abs/2312.04772)

    本文研究了在序贯决策过程中的非马尔可夫公平性，发现公平往往取决于历史，需要在过程中的不同时间点进行评估。

    

    公平的决策制定在很大程度上是针对单一决策进行研究的。本文在多个利益相关者可能受到决策结果影响的情况下，研究了顺序决策中的公平概念。我们观察到，公平往往取决于顺序决策过程的历史，从这个意义上讲，它是固有的非马尔可夫性。我们进一步观察到，公平通常需要在过程中的某个时间点进行评估，而不仅仅是在过程结束时。为了推进我们对这类公平性问题的理解，我们探讨了顺序决策背景下非马尔可夫公平的概念。我们确定了非马尔可夫公平的属性，包括长期公平性、任意时刻公平性、周期性公平性和有界公平性等概念。我们进一步探讨了非马尔可夫公平性和记忆之间的相互作用，以及这如何支持制定公平政策。

    arXiv:2312.04772v3 Announce Type: replace  Abstract: Fair decision making has largely been studied with respect to a single decision. In this paper we investigate the notion of fairness in the context of sequential decision making where multiple stakeholders can be affected by the outcomes of decisions. We observe that fairness often depends on the history of the sequential decision-making process, and in this sense that it is inherently non-Markovian. We further observe that fairness often needs to be assessed at time points within the process, not just at the end of the process. To advance our understanding of this class of fairness problems, we explore the notion of non-Markovian fairness in the context of sequential decision making. We identify properties of non-Markovian fairness, including notions of long-term, anytime, periodic, and bounded fairness. We further explore the interplay between non-Markovian fairness and memory, and how this can support construction of fair policies
    
[^241]: 使用合成数据的经过精炼的LLMs自我批评：一种贝叶斯视角

    Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian Perspective

    [https://arxiv.org/abs/2312.01957](https://arxiv.org/abs/2312.01957)

    本文提出了一种将RLAIF解释为贝叶斯推断的方法，通过经过精炼的自我批评对LLM的输出进行精炼，为获得微调模型提供了一种可行且廉价的替代方案。

    

    本文提出了将RLAIF解释为贝叶斯推断的方法，通过引入经过精炼的自我批评(dSC)，该方法通过Gibbs采样器对LLM的输出进行精炼，然后将其蒸馏成一个微调模型。只需要合成数据，dSC在涉及安全性、情感和隐私控制的实验中得到了应用，表明它可以作为对齐LLMs的一种可行且廉价的替代方案。代码在\url{https://github.com/vicgalle/distilled-self-critique}上发布。

    arXiv:2312.01957v2 Announce Type: replace  Abstract: This paper proposes an interpretation of RLAIF as Bayesian inference by introducing distilled Self-Critique (dSC), which refines the outputs of a LLM through a Gibbs sampler that is later distilled into a fine-tuned model. Only requiring synthetic data, dSC is exercised in experiments regarding safety, sentiment, and privacy control, showing it can be a viable and cheap alternative to align LLMs. Code released at \url{https://github.com/vicgalle/distilled-self-critique}.
    
[^242]: 基于年龄的移动边缘计算调度：一种深度强化学习方法

    Age-Based Scheduling for Mobile Edge Computing: A Deep Reinforcement Learning Approach

    [https://arxiv.org/abs/2312.00279](https://arxiv.org/abs/2312.00279)

    基于深度强化学习的移动边缘计算调度方案提出了一种新的信息时代定义，通过最小化信息时代来改善应用程序性能。

    

    随着移动边缘计算（MEC）的快速发展，各种实时应用程序已经部署，造福于人们的日常生活。这些应用程序的性能在很大程度上取决于收集的环境信息的新鲜度，这可以通过其信息时代（AoI）来衡量。在传统AoI的定义中，假定状态信息可以被积极采样并直接使用。然而，对于许多MEC启用的应用程序，期望的状态信息以事件驱动的方式更新，并需要数据处理。为了更好地满足这些应用程序的需求，我们提出了一个新的AoI定义，并基于重新定义的AoI，为MEC系统制定了一个在线AoI最小化问题。值得注意的是，该问题可以被解释为马尔可夫决策过程（MDP），从而使其可以通过强化学习（RL）算法来解决。然而，传统的RL算法存在不足

    arXiv:2312.00279v2 Announce Type: replace  Abstract: With the rapid development of Mobile Edge Computing (MEC), various real-time applications have been deployed to benefit people's daily lives. The performance of these applications relies heavily on the freshness of collected environmental information, which can be quantified by its Age of Information (AoI). In the traditional definition of AoI, it is assumed that the status information can be actively sampled and directly used. However, for many MEC-enabled applications, the desired status information is updated in an event-driven manner and necessitates data processing. To better serve these applications, we propose a new definition of AoI and, based on the redefined AoI, we formulate an online AoI minimization problem for MEC systems. Notably, the problem can be interpreted as a Markov Decision Process (MDP), thus enabling its solution through Reinforcement Learning (RL) algorithms. Nevertheless, the traditional RL algorithms are d
    
[^243]: InteRACT：基于机器人动作的人类意图预测的Transformer模型

    InteRACT: Transformer Models for Human Intent Prediction Conditioned on Robot Actions

    [https://arxiv.org/abs/2311.12943](https://arxiv.org/abs/2311.12943)

    InteRACT通过在大型人类-人类数据集上预训练条件意图预测模型，并在小型人机数据集上微调，解决了人机交互中的先有鸡还是先有蛋问题。

    

    在协作的人机操纵中，机器人必须预测人类意图并相应调整其行动，以平稳执行任务。然而，人类的意图反过来又取决于机器人采取的动作，造成了一个先有鸡还是先有蛋的问题。先前的方法忽略了这种相互依赖关系，而是训练独立于机器人行动的边际意图预测模型。这是因为在缺乏配对的人机交互数据集的情况下，训练条件模型是困难的。我们能否转而利用更容易获取的大规模人类-人类交互数据？我们的关键见解是利用人类和机器人行动之间的对应关系，实现从人类-人类到人类-机器人数据的迁移学习。我们提出了一种新颖的架构InteRACT，该架构在大型人类-人类数据集上预训练条件意图预测模型，并在小型人机数据集上进行微调。我们在一组真实世界的协作数据上进行评估。

    arXiv:2311.12943v2 Announce Type: replace-cross  Abstract: In collaborative human-robot manipulation, a robot must predict human intents and adapt its actions accordingly to smoothly execute tasks. However, the human's intent in turn depends on actions the robot takes, creating a chicken-or-egg problem. Prior methods ignore such inter-dependency and instead train marginal intent prediction models independent of robot actions. This is because training conditional models is hard given a lack of paired human-robot interaction datasets. Can we instead leverage large-scale human-human interaction data that is more easily accessible? Our key insight is to exploit a correspondence between human and robot actions that enables transfer learning from human-human to human-robot data. We propose a novel architecture, InteRACT, that pre-trains a conditional intent prediction model on large human-human datasets and fine-tunes on a small human-robot dataset. We evaluate on a set of real-world collabo
    
[^244]: 对抗偏好优化

    Adversarial Preference Optimization

    [https://arxiv.org/abs/2311.08045](https://arxiv.org/abs/2311.08045)

    提出了一种对抗偏好优化（APO）框架，实现了在没有额外注释的情况下，通过对抗学习自适应于生成分布差距。

    

    人类偏好调整是提高大型语言模型（LLMs）交互质量的关键。现有的对齐方法依赖于手动注释的偏好数据来指导LLM的优化方向。然而，在实践中，持续更新LLMs会导致模型生成样本与人类首选响应之间存在分布差距，这阻碍了模型微调的效率。为了缓解这个问题，先前的方法需要在生成的样本上额外进行偏好注释，以适应转移分布，这需要大量的注释资源。针对更高效的人类偏好优化，我们提出了一种对抗偏好优化（APO）框架，其中LLM代理和偏好模型通过极小-极大博弈交替更新。在没有额外注释的情况下，我们的APO方法可以通过对抗学习自适应于生成分布差距。

    arXiv:2311.08045v2 Announce Type: replace-cross  Abstract: Human preference alignment is essential to improve the interaction quality of large language models (LLMs). Existing aligning methods depend on manually annotated preference data to guide the LLM optimization directions. However, in practice, continuously updating LLMs raises a distribution gap between model-generated samples and human-preferred responses, which hinders model fine-tuning efficiency. To mitigate this issue, previous methods require additional preference annotation on generated samples to adapt the shifted distribution, which consumes a large amount of annotation resources. Targeting more efficient human preference optimization, we propose an adversarial preference optimization (APO) framework, where the LLM agent and the preference model update alternatively via a min-max game. Without additional annotation, our APO method can make a self-adaption to the generation distribution gap through the adversarial learni
    
[^245]: 一切的思考：打破彭罗斯三角定律以生成思想

    Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation

    [https://arxiv.org/abs/2311.04254](https://arxiv.org/abs/2311.04254)

    引入一种名为“一切的思考”（XoT）的新型思考促进方法，借助预训练的强化学习和蒙特卡洛树搜索（MCTS）将外部领域知识融入思想，从而提高大型语言模型（LLMs）的能力，使其可以高效地推广到未知问题。

    

    最近大型语言模型（LLMs）的进展通过将复杂问题分解为更易处理的语言序列（即“思想”）彻底改变了决策。一个有效的思想设计应该考虑三个关键视角：性能、效率和灵活性。然而，现有的思想最多只能体现这些属性中的两个。为了解决这些限制，我们引入了一种名为“一切的思考”（XoT）的新型思考促进方法，以打破现有思考范式的“彭罗斯三角定律”。XoT利用预训练的强化学习和蒙特卡洛树搜索（MCTS）将外部领域知识融入思想中，从而增强LLMs的能力，并使其能够高效地推广到未见问题。通过利用MCTS-LLM协作思考修订框架，这种方法自主生产高质量的综合认知。

    arXiv:2311.04254v3 Announce Type: replace  Abstract: Recent advancements in Large Language Models (LLMs) have revolutionized decision-making by breaking down complex problems into more manageable language sequences referred to as "thoughts". An effective thought design should consider three key perspectives: performance, efficiency, and flexibility. However, existing thought can at most exhibit two of these attributes. To address these limitations, we introduce a novel thought prompting approach called "Everything of Thoughts" (XoT) to defy the law of "Penrose triangle of existing thought paradigms. XoT leverages pretrained reinforcement learning and Monte Carlo Tree Search (MCTS) to incorporate external domain knowledge into thoughts, thereby enhancing LLMs' capabilities and enabling them to generalize to unseen problems efficiently. Through the utilization of the MCTS-LLM collaborative thought revision framework, this approach autonomously produces high-quality comprehensive cognitiv
    
[^246]: 学习多个动态系统中的联合问题

    Joint Problems in Learning Multiple Dynamical Systems

    [https://arxiv.org/abs/2311.02181](https://arxiv.org/abs/2311.02181)

    聚类时间序列的新问题，提出联合划分轨迹集并学习每个部分的线性动态系统模型，以最小化所有模型的最大误差

    

    时间序列的聚类是一个经过充分研究的问题，其应用范围从通过代谢产物浓度获得的定量个性化代谢模型到量子信息理论中的状态判别。我们考虑了一个变种，即给定一组轨迹和一些部分，我们联合划分轨迹集并学习每个部分的线性动态系统（LDS）模型，以使得所有模型的最大误差最小化。我们提出了全局收敛的方法和EM启发式算法，并附上了有前景的计算结果。

    arXiv:2311.02181v2 Announce Type: replace-cross  Abstract: Clustering of time series is a well-studied problem, with applications ranging from quantitative, personalized models of metabolism obtained from metabolite concentrations to state discrimination in quantum information theory. We consider a variant, where given a set of trajectories and a number of parts, we jointly partition the set of trajectories and learn linear dynamical system (LDS) models for each part, so as to minimize the maximum error across all the models. We present globally convergent methods and EM heuristics, accompanied by promising computational results.
    
[^247]: 卫生保健中的生成人工智能：伦理考虑与评估检查表

    Generative Artificial Intelligence in Healthcare: Ethical Considerations and Assessment Checklist

    [https://arxiv.org/abs/2311.02107](https://arxiv.org/abs/2311.02107)

    对医疗保健中的生成人工智能（GenAI）进行了伦理讨论的范围审查，提出了一份检查表，以推动伦理讨论的全面评估和透明记录。

    

    ChatGPT等新兴技术基于生成人工智能（GenAI）的广泛应用引起了对潜在伦理问题的关注，特别是在高风险应用领域，如医疗保健，但伦理讨论尚未转化为可操作的解决方案。此外，正在进行的伦理讨论常常忽视其他类型的GenAI，这些GenAI已被用于合成数据（例如图像）进行研究和实际目的，从而解决了一些伦理问题并暴露了其他问题。我们进行了一项关于医疗保健中GenAI伦理讨论的范围审查，以全面分析当前研究中的差距，并进一步提议通过制定一份检查表来减少这些差距，以全面评估和透明记录GenAI研究中的伦理讨论。这份检查表可以轻松整合到当前的同行评审和发布系统中，以增强GenAI研究。

    arXiv:2311.02107v2 Announce Type: replace-cross  Abstract: The widespread use of ChatGPT and other emerging technology powered by generative artificial intelligence (GenAI) has drawn much attention to potential ethical issues, especially in high-stakes applications such as healthcare, but ethical discussions are yet to translate into operationalisable solutions. Furthermore, ongoing ethical discussions often neglect other types of GenAI that have been used to synthesise data (e.g., images) for research and practical purposes, which resolved some ethical issues and exposed others. We conduct a scoping review of ethical discussions on GenAI in healthcare to comprehensively analyse gaps in the current research, and further propose to reduce the gaps by developing a checklist for comprehensive assessment and transparent documentation of ethical discussions in GenAI research. The checklist can be readily integrated into the current peer review and publication system to enhance GenAI researc
    
[^248]: UniTime：一种语言增强的跨领域时间序列预测统一模型

    UniTime: A Language-Empowered Unified Model for Cross-Domain Time Series Forecasting

    [https://arxiv.org/abs/2310.09751](https://arxiv.org/abs/2310.09751)

    UniTime提出了一种语言增强的跨领域时间序列预测统一模型，旨在应对数据特征差异、数据区分困难和收敛速度不同等挑战

    

    多元时间序列预测在当代网络技术中起着至关重要的作用。与为特定时间序列应用领域创建专用模型的传统方法不同，本研究倡导跨越领域边界的统一模型范式。然而，学习一个有效的跨领域模型面临以下挑战。首先，不同领域在数据特征上存在差异，例如变量数量，这给现有模型带来了困难，因为这些因素上强加了不灵活的约束。其次，模型可能在区分来自不同领域的数据时遇到困难，导致在我们的评估中表现不佳。第三，时间序列领域的不同收敛速度也可能导致实证性能受损。为解决这些问题，我们提出了UniTime，用于有效的跨领域时间序列学习。具体而言，UniTime可以

    arXiv:2310.09751v3 Announce Type: replace  Abstract: Multivariate time series forecasting plays a pivotal role in contemporary web technologies. In contrast to conventional methods that involve creating dedicated models for specific time series application domains, this research advocates for a unified model paradigm that transcends domain boundaries. However, learning an effective cross-domain model presents the following challenges. First, various domains exhibit disparities in data characteristics, e.g., the number of variables, posing hurdles for existing models that impose inflexible constraints on these factors. Second, the model may encounter difficulties in distinguishing data from various domains, leading to suboptimal performance in our assessments. Third, the diverse convergence rates of time series domains can also result in compromised empirical performance. To address these issues, we propose UniTime for effective cross-domain time series learning. Concretely, UniTime can
    
[^249]: 探索微调语言模型中的记忆能力

    Exploring Memorization in Fine-tuned Language Models

    [https://arxiv.org/abs/2310.06714](https://arxiv.org/abs/2310.06714)

    在微调语言模型过程中，该研究首次全面分析了不同任务中模型的记忆现象，发现了记忆在各种微调任务中表现出显著的差异，并通过稀疏编码理论解释了这种任务差异性。

    

    大型语言模型（LLMs）展现出在各种任务中的巨大能力，但同时也表现出对训练数据的记忆，引起了巨大的隐私和版权担忧。 在此工作中，我们进行了首次全面分析，探讨了在各种任务中微调语言模型（LMs）时的记忆现象。 我们使用开源和我们自己的微调LMs进行了研究，结果表明在不同微调任务中，记忆呈现出较强的差异性。 我们通过稀疏编码理论提供了对这种任务差异性的直观解释，并揭示了记忆和注意力分数之间的强相关性。

    arXiv:2310.06714v2 Announce Type: replace  Abstract: Large language models (LLMs) have shown great capabilities in various tasks but also exhibited memorization of training data, raising tremendous privacy and copyright concerns. While prior works have studied memorization during pre-training, the exploration of memorization during fine-tuning is rather limited. Compared to pre-training, fine-tuning typically involves more sensitive data and diverse objectives, thus may bring distinct privacy risks and unique memorization behaviors. In this work, we conduct the first comprehensive analysis to explore language models' (LMs) memorization during fine-tuning across tasks. Our studies with open-sourced and our own fine-tuned LMs across various tasks indicate that memorization presents a strong disparity among different fine-tuning tasks. We provide an intuitive explanation of this task disparity via sparse coding theory and unveil a strong correlation between memorization and attention scor
    
[^250]: 通过策略合并实现舰队学习

    Fleet Learning via Policy Merging

    [https://arxiv.org/abs/2310.01362](https://arxiv.org/abs/2310.01362)

    本文研究了通过策略合并解决机器人群体学习中的数据存储和传输问题，并提出了一种基于循环神经网络的分布式学习方法。该方法能够在Meta-World环境中将50个任务的策略行为整合，并在大多数训练任务上表现良好。

    

    机器人群体通过与环境互动产生的大量异构流数据存储或传输上的困难，机器人团队需要通过在不同环境中的异构经验来共同获得多样化的技能。本文研究了从这种分布式异构数据集中进行策略合并作为潜在解决方案的问题。为了在舰队环境中高效合并策略，我们提出了FLEET-MERGE，一种基于循环神经网络参数化控制策略的分布式学习实例，考虑了参数化控制策略中的排列不变性。我们表明，FLEET-MERGE在Meta-World环境中对50个任务进行训练的策略行为进行了整合，并且几乎在所有训练任务上表现良好。

    arXiv:2310.01362v2 Announce Type: replace-cross  Abstract: Fleets of robots ingest massive amounts of heterogeneous streaming data silos generated by interacting with their environments, far more than what can be stored or transmitted with ease. At the same time, teams of robots should co-acquire diverse skills through their heterogeneous experiences in varied settings. How can we enable such fleet-level learning without having to transmit or centralize fleet-scale data? In this paper, we investigate policy merging (PoMe) from such distributed heterogeneous datasets as a potential solution. To efficiently merge policies in the fleet setting, we propose FLEET-MERGE, an instantiation of distributed learning that accounts for the permutation invariance that arises when parameterizing the control policies with recurrent neural networks. We show that FLEET-MERGE consolidates the behavior of policies trained on 50 tasks in the Meta-World environment, with good performance on nearly all train
    
[^251]: 矛与盾：针对连续时间动态图模型的对抗攻击与防御方法

    Spear and Shield: Adversarial Attacks and Defense Methods for Model-Based Link Prediction on Continuous-Time Dynamic Graphs

    [https://arxiv.org/abs/2308.10779](https://arxiv.org/abs/2308.10779)

    该论文提出了一种简单而有效的T-SPEAR对抗攻击方法，重点研究了时间图神经网络(TGNN)在连续时间动态图上的链路预测中的脆弱性。

    

    实际世界中的图是动态的，不断随着新的交互而发展，比如金融网络中的金融交易。时间图神经网络(TGNN)已经被开发出来，以有效地捕捉动态图中不断演变的模式。虽然这些模型表现出了它们的优越性，并被广泛应用于各个重要领域，但它们对抗性攻击的脆弱性仍然鲜为人知。在本文中，我们提出了T-SPEAR，这是一种简单而有效的针对连续时间动态图上的链路预测的对抗攻击方法，重点研究了TGNN的脆弱性。具体而言，在一个针对链路预测的TGNN受害模型的训练过程之前，我们向数据注入边缘扰动，这些扰动在我们提出的四个约束条件方面是不可察觉的，但足以导致受害模型的故障。此外，我们提出了一种强大的训练方法。

    arXiv:2308.10779v2 Announce Type: replace  Abstract: Real-world graphs are dynamic, constantly evolving with new interactions, such as financial transactions in financial networks. Temporal Graph Neural Networks (TGNNs) have been developed to effectively capture the evolving patterns in dynamic graphs. While these models have demonstrated their superiority, being widely adopted in various important fields, their vulnerabilities against adversarial attacks remain largely unexplored. In this paper, we propose T-SPEAR, a simple and effective adversarial attack method for link prediction on continuous-time dynamic graphs, focusing on investigating the vulnerabilities of TGNNs. Specifically, before the training procedure of a victim model, which is a TGNN for link prediction, we inject edge perturbations to the data that are unnoticeable in terms of the four constraints we propose, and yet effective enough to cause malfunction of the victim model. Moreover, we propose a robust training appr
    
[^252]: FedDefender：联邦学习中的后门攻击防御

    FedDefender: Backdoor Attack Defense in Federated Learning

    [https://arxiv.org/abs/2307.08672](https://arxiv.org/abs/2307.08672)

    FedDefender是一种针对联邦学习中有针对性的中毒攻击的防御机制，通过差分测试来识别潜在包含后门的恶意客户，有效降低攻击成功率到10%。

    

    Federated Learning (FL)是一种隐私保护的分布式机器学习技术，它使得个体客户（例如用户参与者、边缘设备或组织）能够在安全环境中基于本地数据训练模型，然后与聚合器共享训练模型以协作构建全局模型。在这项工作中，我们提出了FedDefender，一种针对联邦学习中有针对性的中毒攻击的防御机制，它利用差分测试。我们提出的方法对相同输入的客户模型的神经元激活进行指纹识别，并利用差分测试来识别潜在包含后门的恶意客户。我们使用MNIST和FashionMNIST数据集以及20个和30个客户对FedDefender进行评估，结果表明，FedDefender有效地缓解了此类攻击，将攻击成功率（ASR）降低到10%，而不会恶化全局模型的性能。

    arXiv:2307.08672v2 Announce Type: replace-cross  Abstract: Federated Learning (FL) is a privacy-preserving distributed machine learning technique that enables individual clients (e.g., user participants, edge devices, or organizations) to train a model on their local data in a secure environment and then share the trained model with an aggregator to build a global model collaboratively. In this work, we propose FedDefender, a defense mechanism against targeted poisoning attacks in FL by leveraging differential testing. Our proposed method fingerprints the neuron activations of clients' models on the same input and uses differential testing to identify a potentially malicious client containing a backdoor. We evaluate FedDefender using MNIST and FashionMNIST datasets with 20 and 30 clients, and our results demonstrate that FedDefender effectively mitigates such attacks, reducing the attack success rate (ASR) to 10\% without deteriorating the global model performance.
    
[^253]: 过度平滑：图对比学习的噩梦？

    Oversmoothing: A Nightmare for Graph Contrastive Learning?

    [https://arxiv.org/abs/2306.02117](https://arxiv.org/abs/2306.02117)

    对于图对比学习（GCL），研究表明增加网络深度会导致过度平滑，包括深度表示和浅层，提出了BlockGCL解决这一问题

    

    过度平滑是图神经网络（GNNs）中常见的现象，即网络深度的增加导致性能下降。图对比学习（GCL）正日益成为利用大量未标记图数据的一种有前途的方式。作为GNNs和对比学习的融合，尚不清楚GCL是否会继承GNNs的过度平滑缺陷。本文从过度平滑的角度对GCL进行了基础分析。我们通过实验证明，在GCL中增加网络深度也会导致它们的深度表示过度平滑，而且令人惊讶的是浅层也会出现这种现象。我们将这种现象在GCL中称为“长距离饥饿”，即深度网络中的较低层由于缺乏来自监督的充分指导而遭受退化。根据我们的研究结果，我们提出了BlockGCL，这是一个非常简单但有效的块wi

    arXiv:2306.02117v2 Announce Type: replace-cross  Abstract: Oversmoothing is a common phenomenon observed in graph neural networks (GNNs), in which an increase in the network depth leads to a deterioration in their performance. Graph contrastive learning (GCL) is emerging as a promising way of leveraging vast unlabeled graph data. As a marriage between GNNs and contrastive learning, it remains unclear whether GCL inherits the same oversmoothing defect from GNNs. This work undertakes a fundamental analysis of GCL from the perspective of oversmoothing on the first hand. We demonstrate empirically that increasing network depth in GCL also leads to oversmoothing in their deep representations, and surprisingly, the shallow ones. We refer to this phenomenon in GCL as `long-range starvation', wherein lower layers in deep networks suffer from degradation due to the lack of sufficient guidance from supervision. Based on our findings, we present BlockGCL, a remarkably simple yet effective blockwi
    
[^254]: SparDL：高效稀疏通信的分布式深度学习训练

    SparDL: Distributed Deep Learning Training with Efficient Sparse Communication

    [https://arxiv.org/abs/2304.00737](https://arxiv.org/abs/2304.00737)

    SparDL提出了一种高效稀疏通信框架，使用Spar-Reduce-Scatter和Spar-All-Gather算法来解决稀疏梯度累积困境，避免依赖低效通信算法和额外传输步骤。

    

    近年来，Top-k稀疏化被广泛应用于减少分布式深度学习中的通信量，然而由于稀疏梯度累积（SGA）困境，Top-k稀疏化的性能仍然存在局限性。为了应对SGA困境，一些方法被提出，然而即使最先进的方法也存在一些缺陷，例如依赖低效的通信算法，需要额外的传输步骤。受现有方法局限性的启发，我们提出了一种新颖高效的稀疏通信框架，称为SparDL。具体来说，SparDL使用了Spar-Reduce-Scatter算法，基于高效的Reduce-Scatter模型处理SGA困境而不需要额外的通信操作。此外，为了进一步降低延迟成本并提高SparDL的效率，我们提出了Spar-All-Gather算法。

    arXiv:2304.00737v2 Announce Type: replace  Abstract: Top-k sparsification has recently been widely used to reduce the communication volume in distributed deep learning. However, due to the Sparse Gradient Accumulation (SGA) dilemma, the performance of top-k sparsification still has limitations. Recently, a few methods have been put forward to handle the SGA dilemma. Regrettably, even the state-of-the-art method suffers from several drawbacks, e.g., it relies on an inefficient communication algorithm and requires extra transmission steps. Motivated by the limitations of existing methods, we propose a novel efficient sparse communication framework, called SparDL. Specifically, SparDL uses the Spar-Reduce-Scatter algorithm, which is based on an efficient Reduce-Scatter model, to handle the SGA dilemma without additional communication operations. Besides, to further reduce the latency cost and improve the efficiency of SparDL, we propose the Spar-All-Gather algorithm. Moreover, we propose 
    
[^255]: 深度学习中类别流形的反演动力学揭示了潜在泛化的权衡

    Inversion dynamics of class manifolds in deep learning reveals tradeoffs underlying generalisation

    [https://arxiv.org/abs/2303.05161](https://arxiv.org/abs/2303.05161)

    优化动态发现了在学习不同标签的数据点流形时如何平衡类别分离和特征不变性之间的对立倾向，通过非单调趋势展现了这种权衡现象。

    

    为了在分类问题中实现接近零训练误差，前馈网络的层必须解开具有不同标签的数据点的流形，以促进区分。然而，过度的类别分离可能导致过拟合，因为良好的泛化需要学习不变特征，这涉及一定程度的混杂。我们报告了数值实验，展示了优化动态如何找到平衡这些对立倾向的表示，并呈现出非单调趋势。快速分离阶段之后，更慢的重新排列（在数据集和架构间保持一致）增加了类别混杂性。反演时的训练误差在子采样、网络初始化和优化器之间保持稳定，这将其特征化为仅取决于数据结构（及架构上的微弱属性）。

    arXiv:2303.05161v2 Announce Type: replace  Abstract: To achieve near-zero training error in a classification problem, the layers of a feed-forward network have to disentangle the manifolds of data points with different labels, to facilitate the discrimination. However, excessive class separation can bring to overfitting since good generalisation requires learning invariant features, which involve some level of entanglement. We report on numerical experiments showing how the optimisation dynamics finds representations that balance these opposing tendencies with a non-monotonic trend. After a fast segregation phase, a slower rearrangement (conserved across data sets and architectures) increases the class entanglement.The training error at the inversion is stable under subsampling, and across network initialisations and optimisers, which characterises it as a property solely of the data structure and (very weakly) of the architecture. The inversion is the manifestation of tradeoffs elicit
    
[^256]: 特征选择与退火用于预测金融时间序列

    Feature Selection with Annealing for Forecasting Financial Time Series

    [https://arxiv.org/abs/2303.02223](https://arxiv.org/abs/2303.02223)

    本研究首次在金融领域研究了特征选择与退火方法，为预测金融时间序列提供了有效的解决方案。

    

    股票市场和加密货币的预测对投资者非常重要，因为他们渴望改进购买或持有策略，以增加盈利。本研究提供了一种基于机器学习模型的战略输入输出特征映射技术，用于预测金融时间序列的全面方法，以减少复杂性。

    arXiv:2303.02223v3 Announce Type: replace  Abstract: Stock market and cryptocurrency forecasting is very important to investors as they aspire to achieve even the slightest improvement to their buy or hold strategies so that they may increase profitability. However, obtaining accurate and reliable predictions is challenging, noting that accuracy does not equate to reliability, especially when financial time-series forecasting is applied owing to its complex and chaotic tendencies. To mitigate this complexity, this study provides a comprehensive method for forecasting financial time series based on tactical input output feature mapping techniques using machine learning (ML) models. During the prediction process, selecting the relevant indicators is vital to obtaining the desired results. In the financial field, limited attention has been paid to this problem with ML solutions. We investigate the use of feature selection with annealing (FSA) for the first time in this field, and we apply
    
[^257]: 用于不规则间隔数据的深度ReLU神经网络插值的尖锐下界

    Sharp Lower Bounds on Interpolation by Deep ReLU Neural Networks at Irregularly Spaced Data

    [https://arxiv.org/abs/2302.00834](https://arxiv.org/abs/2302.00834)

    该论文研究了深度ReLU神经网络在不规则间隔数据上的插值问题，证明了在数据点间距指数级小的情况下需要$\Omega(N)$个参数，同时指出现有的位提取技术无法应用于这种情况。

    

    我们研究了深度ReLU神经网络的插值能力。具体来说，我们考虑深度ReLU网络如何在单位球中的$N$个数据点上进行值的插值，这些点之间相距$\delta$。我们表明在$\delta$在$N$指数级小的区域中需要$\Omega(N)$个参数，这给出了该区域的尖锐结果，因为$O(N)$个参数总是足够的。 这也表明用于证明VC维度下界的位提取技术无法应用于不规则间隔的数据点。最后，作为应用，我们给出了深度ReLU神经网络在嵌入端点处为Sobolev空间实现的近似速率的下界。

    arXiv:2302.00834v2 Announce Type: replace  Abstract: We study the interpolation power of deep ReLU neural networks. Specifically, we consider the question of how efficiently, in terms of the number of parameters, deep ReLU networks can interpolate values at $N$ datapoints in the unit ball which are separated by a distance $\delta$. We show that $\Omega(N)$ parameters are required in the regime where $\delta$ is exponentially small in $N$, which gives the sharp result in this regime since $O(N)$ parameters are always sufficient. This also shows that the bit-extraction technique used to prove lower bounds on the VC dimension cannot be applied to irregularly spaced datapoints. Finally, as an application we give a lower bound on the approximation rates that deep ReLU neural networks can achieve for Sobolev spaces at the embedding endpoint.
    
[^258]: FedDebug: 面向联邦学习应用的系统化调试

    FedDebug: Systematic Debugging for Federated Learning Applications

    [https://arxiv.org/abs/2301.03553](https://arxiv.org/abs/2301.03553)

    FedDebug设计了一个系统化的故障定位框架，通过两个新领域推动了FL调试。FedDebug通过利用记录和重现技术，实现了对FL中实时协同训练的交互式调试。

    

    在联邦学习（FL）中，客户端独立训练本地模型并与中央聚合器共享，以构建全局模型。无法访问客户端数据和协同训练使FL在涉及数据隐私的应用中具有吸引力，如医学成像。然而，这些FL特征为调试带来了前所未有的挑战。当全局模型性能下降时，识别责任轮次和客户端是一个主要问题。开发者借助使用子客户端的试错调试，希望提高全局模型的准确性或让未来的FL轮次重新调整模型，这些过程耗时且成本高。

    arXiv:2301.03553v2 Announce Type: replace-cross  Abstract: In Federated Learning (FL), clients independently train local models and share them with a central aggregator to build a global model. Impermissibility to access clients' data and collaborative training make FL appealing for applications with data-privacy concerns, such as medical imaging. However, these FL characteristics pose unprecedented challenges for debugging. When a global model's performance deteriorates, identifying the responsible rounds and clients is a major pain point. Developers resort to trial-and-error debugging with subsets of clients, hoping to increase the global model's accuracy or let future FL rounds retune the model, which are time-consuming and costly.   We design a systematic fault localization framework, FedDebug, that advances the FL debugging on two novel fronts. First, FedDebug enables interactive debugging of realtime collaborative training in FL by leveraging record and replay techniques to const
    
[^259]: 使用条件生成模型预测量子系统的性质

    Predicting Properties of Quantum Systems with Conditional Generative Models

    [https://arxiv.org/abs/2211.16943](https://arxiv.org/abs/2211.16943)

    使用条件生成模型同时表示一系列状态，从测量中学习不同量子态的共享结构，可以预测基态的任意局部性质，无需为新的可观测量进行进一步训练

    

    机器学习最近作为一个强大的工具出现，用于预测量子多体系统的性质。对于许多间隙哈密顿量的基态，生成模型可以从单个量子态的测量中学习，精确重构出状态，从而预测局部可观测量。另外，分类和回归模型可以通过学习不同但相关状态的测量来预测局部可观测量。在这项工作中，我们结合了两种方法的优点，提出了使用条件生成模型来同时表示一系列状态，从测量中学习不同量子态的共享结构。训练好的模型使我们能够预测基态的任意局部性质，甚至对于未包含在训练数据中的状态，也无需为新的可观测量进行进一步训练。我们首先在二维随机海森堡模型上对我们的方法进行了数值验证。

    arXiv:2211.16943v2 Announce Type: replace-cross  Abstract: Machine learning has emerged recently as a powerful tool for predicting properties of quantum many-body systems. For many ground states of gapped Hamiltonians, generative models can learn from measurements of a single quantum state to reconstruct the state accurately enough to predict local observables. Alternatively, classification and regression models can predict local observables by learning from measurements on different but related states. In this work, we combine the benefits of both approaches and propose the use of conditional generative models to simultaneously represent a family of states, learning shared structures of different quantum states from measurements. The trained model enables us to predict arbitrary local properties of ground states, even for states not included in the training data, without necessitating further training for new observables. We first numerically validate our approach on 2D random Heisenb
    
[^260]: Centaur: 面向受限边缘设备的联邦学习

    Centaur: Federated Learning for Constrained Edge Devices

    [https://arxiv.org/abs/2211.04175](https://arxiv.org/abs/2211.04175)

    Centaur提出了面向受限边缘设备的联邦学习框架，通过数据选择方案和基于分区的训练算法，实现了超限制设备在大型神经网络的高效参与，相比本地训练能获得更高准确性和节约能量。

    

    联邦学习（FL）促进了在边缘设备上的新应用，尤其是对于可穿戴和物联网设备。这些设备捕获大量多样化的数据，但它们受到内存、计算、功耗和连接性约束，这些约束阻碍了它们参与FL。我们提出Centaur，一个多层FL框架，使超限制的设备能够高效地参与大型神经网络的FL。Centaur结合了两个主要的想法：（i）数据选择方案选择一部分样本加速学习，以及（ii）一个基于分区的训练算法，整合同一用户拥有的受限和强大设备。在四个基准神经网络和三个数据集上的评估显示，Centaur相比于在受限设备上的本地训练，能够获得约10\%更高的准确性，平均能节约约58\%的能量。我们的实验结果也表明了Centaur在处理时的卓越效率。

    arXiv:2211.04175v3 Announce Type: replace  Abstract: Federated learning (FL) facilitates new applications at the edge, especially for wearable and Internet-of-Thing devices. Such devices capture a large and diverse amount of data, but they have memory, compute, power, and connectivity constraints which hinder their participation in FL. We propose Centaur, a multitier FL framework, enabling ultra-constrained devices to efficiently participate in FL on large neural nets. Centaur combines two major ideas: (i) a data selection scheme to choose a portion of samples that accelerates the learning, and (ii) a partition-based training algorithm that integrates both constrained and powerful devices owned by the same user. Evaluations, on four benchmark neural nets and three datasets, show that Centaur gains ~10\% higher accuracy than local training on constrained devices with ~58\% energy saving on average. Our experimental results also demonstrate the superior efficiency of Centaur when dealing
    
[^261]: 来自连续字典的混合物的离散学习

    Simultaneous off-the-grid learning of mixtures issued from a continuous dictionary

    [https://arxiv.org/abs/2210.16311](https://arxiv.org/abs/2210.16311)

    本文提出了一种名为Group-Nonlinear-Lasso的方法，可以同时估计混合物中的线性系数和特征的非线性参数，并使用证明函数对预测误差提供了高概率界限。

    

    在本文中，我们观察了一组信号，可能是一个连续信号，受到噪声的干扰。每个信号是由一个未知数量的特征混合而成，这些特征属于一个连续字典。连续字典由一个实非线性参数进行参数化。我们假设这些信号共享一个基本结构，假定每个信号的活跃特征包含在一个有限稀疏集合中。我们提出了正则化优化问题，同时估计混合物中的线性系数和特征的非线性参数。优化问题由数据保真度项和$(\ell_1,L^p)$-惩罚项组成。我们称其解为Group-Nonlinear-Lasso，并使用证明函数对预测误差提供了高概率界限。借鉴最近关于离散学习方法几何性质的研究，我们表明只要特定参数满足条件，就可以构造这样的函数。

    arXiv:2210.16311v2 Announce Type: replace-cross  Abstract: In this paper we observe a set, possibly a continuum, of signals corrupted by noise. Each signal is a finite mixture of an unknown number of features belonging to a continuous dictionary. The continuous dictionary is parametrized by a real non-linear parameter. We shall assume that the signals share an underlying structure by assuming that each signal has its active features included in a finite and sparse set. We formulate regularized optimization problem to estimate simultaneously the linear coefficients in the mixtures and the non-linear parameters of the features. The optimization problem is composed of a data fidelity term and a $(\ell_1,L^p)$-penalty. We call its solution the Group-Nonlinear-Lasso and provide high probability bounds on the prediction error using certificate functions. Following recent works on the geometry of off-the-grid methods, we show that such functions can be constructed provided the parameters of t
    
[^262]: DMODE: 无需特定类别信息的单目目标距离估计模块

    DMODE: Differential Monocular Object Distance Estimation Module without Class Specific Information

    [https://arxiv.org/abs/2210.12596](https://arxiv.org/abs/2210.12596)

    DMODE是一种无需物体类别信息的单目目标距离估计方法，通过融合物体大小变化和摄像头运动来实现对各种目标检测和未知物体的适应，解决了单目距离估计中缺乏参考点和对象特定线索的挑战。

    

    利用单个摄像头测量物体距离是一种经济高效的替代方案，而不需要立体视觉和激光雷达。尽管文献中已经探讨了单目距离估计，但大多数现有技术依赖于物体类别知识以实现高性能。在缺乏这些情境数据的情况下，单目距离估计变得更具挑战性，缺乏参考点和物体特定线索。然而，这些线索可能会误导与范围广泛变化或对抗情况下的对象，这是面向对象不可知距离估计的一个具有挑战性的方面。本文提出了DMODE，一种不需要物体类别知识的单目距离估计类别不可知方法。DMODE通过融合物体随时间变化的大小波动和摄像头运动来估计物体的距离，使其能够适应各种目标检测器和未知物体，从而解决这些挑战。

    arXiv:2210.12596v2 Announce Type: replace-cross  Abstract: Utilizing a single camera for measuring object distances is a cost-effective alternative to stereo-vision and LiDAR. Although monocular distance estimation has been explored in the literature, most existing techniques rely on object class knowledge to achieve high performance. Without this contextual data, monocular distance estimation becomes more challenging, lacking reference points and object-specific cues. However, these cues can be misleading for objects with wide-range variation or adversarial situations, which is a challenging aspect of object-agnostic distance estimation. In this paper, we propose DMODE, a class-agnostic method for monocular distance estimation that does not require object class knowledge. DMODE estimates an object's distance by fusing its fluctuation in size over time with the camera's motion, making it adaptable to various object detectors and unknown objects, thus addressing these challenges. We eva
    
[^263]: 利用梯度提升决策树的训练动态提高数据质量

    Improving Data Quality with Training Dynamics of Gradient Boosting Decision Trees

    [https://arxiv.org/abs/2210.11327](https://arxiv.org/abs/2210.11327)

    本文提出了一种基于梯度提升决策树的训练动态来评估每个训练实例行为的方法，针对包含大部分表格化或结构化数据的数据集，相较于自信学习、直接启发式和健壮提升算法，取得了最佳结果。

    

    真实世界的数据集中常常包含有错误标记的实例，这会影响模型的性能，尤其是在泛化超出分布范围时。同时，每个示例对学习过程可能有不同的贡献。这促使研究者更好地理解数据实例在模型中对好指标的贡献角色。本文提出了一种基于梯度提升决策树（GBDTs）训练动态计算的度量来评估每个训练实例行为的方法。我们专注于包含大部分表格化或结构化数据的数据集，对于这类数据集，决策树集成在性能方面仍处于领先地位。与自信学习、直接启发式和健壮提升算法相比，我们的方法在整体上取得了最佳结果。我们展示了在检测嘈杂标签以清理数据集、改进模型指标方面的结果。

    arXiv:2210.11327v2 Announce Type: replace  Abstract: Real world datasets contain incorrectly labeled instances that hamper the performance of the model and, in particular, the ability to generalize out of distribution. Also, each example might have different contribution towards learning. This motivates studies to better understanding of the role of data instances with respect to their contribution in good metrics in models. In this paper we propose a method based on metrics computed from training dynamics of Gradient Boosting Decision Trees (GBDTs) to assess the behavior of each training example. We focus on datasets containing mostly tabular or structured data, for which the use of Decision Trees ensembles are still the state-of-the-art in terms of performance. Our methods achieved the best results overall when compared with confident learning, direct heuristics and a robust boosting algorithm. We show results on detecting noisy labels in order clean datasets, improving models' metri
    
[^264]: 基于任务驱动特征选择的多通道成像实验设计

    Experimental Design for Multi-Channel Imaging via Task-Driven Feature Selection

    [https://arxiv.org/abs/2210.06891](https://arxiv.org/abs/2210.06891)

    提出了一种基于任务驱动特征选择的多通道成像实验设计方法，通过优化设计和训练机器学习模型执行用户指定的图像分析任务。

    

    本文提出了一种数据驱动的、任务特定的实验设计范式，旨在缩短采集时间、降低成本、加速成像设备的部署。当前实验设计方法主要集中在模型参数估计上，并要求对特定模型进行规范，而在成像领域，其他任务可能驱动设计。此外，这种方法常常在真实世界的成像应用中导致难以求解的优化问题。本文提出了一种新的实验设计范式，同时优化设计（图像通道集）并训练一个机器学习模型来执行用户指定的图像分析任务。该方法在测量空间上密集采样数据（许多图像通道）进行了少量采集，然后识别一个预先指定尺寸的最佳支持任务的通道子集。

    arXiv:2210.06891v3 Announce Type: replace-cross  Abstract: This paper presents a data-driven, task-specific paradigm for experimental design, to shorten acquisition time, reduce costs, and accelerate the deployment of imaging devices. Current approaches in experimental design focus on model-parameter estimation and require specification of a particular model, whereas in imaging, other tasks may drive the design. Furthermore, such approaches often lead to intractable optimization problems in real-world imaging applications. Here we present a new paradigm for experimental design that simultaneously optimizes the design (set of image channels) and trains a machine-learning model to execute a user-specified image-analysis task. The approach obtains data densely-sampled over the measurement space (many image channels) for a small number of acquisitions, then identifies a subset of channels of prespecified size that best supports the task. We propose a method: TADRED for TAsk-DRiven Experime
    
[^265]: 干预因果表示学习

    Interventional Causal Representation Learning

    [https://arxiv.org/abs/2209.11924](https://arxiv.org/abs/2209.11924)

    干预数据有助于因果表示学习，可以通过干预数据中潜在因素支持的几何特征来识别潜在的因果因素。

    

    因果表示学习旨在从低级感官数据中提取高级潜在因素。大多数现有方法依赖于观测数据和结构假设（如条件独立性）来识别潜在因素。然而，干预数据在各种应用中普遍存在。干预数据能否促进因果表示学习？本文探讨了这个问题。关键观察是，干预数据通常携带潜在因素支持的几何特征（即每个潜在因素可能采取的值）。举例来说，当潜在因素存在因果联系时，干预可以打破干预潜在因素支持和它们祖先之间的依赖关系。利用这一事实，我们证明在获得完美$do$干预数据后，可以确定潜在的因果因素，而且能够实现区块仿射识别。

    arXiv:2209.11924v4 Announce Type: replace-cross  Abstract: Causal representation learning seeks to extract high-level latent factors from low-level sensory data. Most existing methods rely on observational data and structural assumptions (e.g., conditional independence) to identify the latent factors. However, interventional data is prevalent across applications. Can interventional data facilitate causal representation learning? We explore this question in this paper. The key observation is that interventional data often carries geometric signatures of the latent factors' support (i.e. what values each latent can possibly take). For example, when the latent factors are causally connected, interventions can break the dependency between the intervened latents' support and their ancestors'. Leveraging this fact, we prove that the latent causal factors can be identified up to permutation and scaling given data from perfect $do$ interventions. Moreover, we can achieve block affine identific
    
[^266]: GNNInterpreter：图神经网络的生成模型级解释

    GNNInterpreter: A Probabilistic Generative Model-Level Explanation for Graph Neural Networks

    [https://arxiv.org/abs/2209.07924](https://arxiv.org/abs/2209.07924)

    提出了GNNInterpreter，一种用于解释图神经网络高级决策过程的模型级解释方法，通过学习概率生成图分布来揭示GNN模型内部工作机制。

    

    最近，图神经网络（GNNs）显著提升了在图上的机器学习任务的性能。然而，这一技术突破使人们产生了疑问：GNN是如何做出决策的，我们能否高度信任其预测？在一些关键领域，如生物医学，做出错误决策可能带来严重后果，因此在应用之前解释GNN的内部工作机制至关重要。本文提出了一种适用于遵循消息传递方案的不同GNN的模型不可知的模型级解释方法GNNInterpreter，来解释GNN模型的高级决策过程。具体而言，GNNInterpreter通过优化一种新颖的目标函数学习一个能够产生GNN在做出某个预测时试图检测到的最具辨识性图模式的概率生成图分布。

    arXiv:2209.07924v4 Announce Type: replace-cross  Abstract: Recently, Graph Neural Networks (GNNs) have significantly advanced the performance of machine learning tasks on graphs. However, this technological breakthrough makes people wonder: how does a GNN make such decisions, and can we trust its prediction with high confidence? When it comes to some critical fields, such as biomedicine, where making wrong decisions can have severe consequences, it is crucial to interpret the inner working mechanisms of GNNs before applying them. In this paper, we propose a model-agnostic model-level explanation method for different GNNs that follow the message passing scheme, GNNInterpreter, to explain the high-level decision-making process of the GNN model. More specifically, GNNInterpreter learns a probabilistic generative graph distribution that produces the most discriminative graph pattern the GNN tries to detect when making a certain prediction by optimizing a novel objective function specifical
    
[^267]: FP8量化：指数的力量

    FP8 Quantization: The Power of the Exponent

    [https://arxiv.org/abs/2208.09225](https://arxiv.org/abs/2208.09225)

    本文深入研究了FP8格式对神经网络推理的好处，提出了在不同设置中选择尾数和指数位数的方法，并证明了FP8格式在实际网络中的表现更好，尤其对于后训练量化来说优于INT8格式。

    

    当将神经网络量化以进行高效推理时，低比特整数是效率的首选格式。然而，低比特浮点数具有额外的自由度，可以将一些比特分配到指数刻度上。本文深入研究了浮点格式对神经网络推理的这种好处。我们详细介绍了FP8格式的选择，包括尾数和指数位数的重要选择，并分析了在哪些设置中这些选择会带来更好的性能。然后我们展示了这些发现如何转化为真实网络，提供了FP8模拟的高效实现，以及一种新算法，可以学习FP8格式中的比例参数和指数位数。我们的主要结论是，在对各种网络进行后训练量化时，FP8格式优于INT8格式。

    arXiv:2208.09225v2 Announce Type: replace  Abstract: When quantizing neural networks for efficient inference, low-bit integers are the go-to format for efficiency. However, low-bit floating point numbers have an extra degree of freedom, assigning some bits to work on an exponential scale instead. This paper in-depth investigates this benefit of the floating point format for neural network inference. We detail the choices that can be made for the FP8 format, including the important choice of the number of bits for the mantissa and exponent, and show analytically in which settings these choices give better performance. Then we show how these findings translate to real networks, provide an efficient implementation for FP8 simulation, and a new algorithm that enables the learning of both the scale parameters and the number of exponent bits in the FP8 format. Our chief conclusion is that when doing post-training quantization for a wide range of networks, the FP8 format is better than INT8 i
    
[^268]: 关于函数线性模型假设迁移学习的研究

    On Hypothesis Transfer Learning of Functional Linear Models

    [https://arxiv.org/abs/2206.04277](https://arxiv.org/abs/2206.04277)

    该研究在函数线性回归下探讨了迁移学习，提出了使用RKHS距离衡量任务相似性，并提出了两种算法来处理迁移，一种需要已知正源，另一种利用聚合技术实现无源信息的稳健传输。同时建立了学习问题的下界，并证明了算法的上界。

    

    我们研究了在再生核希尔伯特空间（RKHS）框架下的函数线性回归（FLR）的迁移学习（TL），观察到现有高维线性回归中的TL技术与基于截断的FLR方法不兼容，因为函数数据在本质上是无限维的，并由平滑的基础过程生成。我们使用RKHS距离来衡量任务之间的相似性，允许传输的信息类型与所施加的RKHS的属性相关联。基于假设偏移迁移学习范式，提出了两种算法：一种在已知正源时进行传输，另一种利用聚合技术实现无需先验信息的稳健传输。我们为这个学习问题建立了下界，并展示了所提出的算法享有匹配的渐近上界。

    arXiv:2206.04277v4 Announce Type: replace-cross  Abstract: We study the transfer learning (TL) for the functional linear regression (FLR) under the Reproducing Kernel Hilbert Space (RKHS) framework, observing the TL techniques in existing high-dimensional linear regression is not compatible with the truncation-based FLR methods as functional data are intrinsically infinite-dimensional and generated by smooth underlying processes. We measure the similarity across tasks using RKHS distance, allowing the type of information being transferred tied to the properties of the imposed RKHS. Building on the hypothesis offset transfer learning paradigm, two algorithms are proposed: one conducts the transfer when positive sources are known, while the other leverages aggregation techniques to achieve robust transfer without prior information about the sources. We establish lower bounds for this learning problem and show the proposed algorithms enjoy a matching asymptotic upper bound. These analyses
    
[^269]: 超几何分层知识图嵌入的低维链接预测

    Hyperbolic Hierarchical Knowledge Graph Embeddings for Link Prediction in Low Dimensions

    [https://arxiv.org/abs/2204.13704](https://arxiv.org/abs/2204.13704)

    提出了一种新颖的KGE模型，名为HypH，利用超几何空间嵌入分层数据，以提高知识图中链接预测的性能

    

    知识图嵌入（KGE）已被证实是推断知识图（KGs）中缺失链接的强大方法，它们通常将实体映射到欧几里得空间，并将关系视为实体的转换。 最近，一些欧几里得KGE方法已经增强，以建模KGs中常见的语义层次结构，提高链接预测性能。 为了嵌入分层数据，超几何空间已经成为传统欧几里得空间的一种有前途的替代方法，具有高度保真度和较低的内存消耗。 与欧几里得空间不同，超几何空间提供了无数可供选择的曲率。 但是，现有的超几何KGE方法难以手动获取最佳曲率设置，从而限制了它们有效地对语义层次结构进行建模的能力。 为解决这一限制，我们提出了一种名为$\textbf{Hyp}$erbolic $\textbf{H}$ierarchical $\textb

    arXiv:2204.13704v2 Announce Type: replace-cross  Abstract: Knowledge graph embeddings (KGE) have been validated as powerful methods for inferring missing links in knowledge graphs (KGs) that they typically map entities into Euclidean space and treat relations as transformations of entities. Recently, some Euclidean KGE methods have been enhanced to model semantic hierarchies commonly found in KGs, improving the performance of link prediction. To embed hierarchical data, hyperbolic space has emerged as a promising alternative to traditional Euclidean space, offering high fidelity and lower memory consumption. Unlike Euclidean, hyperbolic space provides countless curvatures to choose from. However, it is difficult for existing hyperbolic KGE methods to obtain the optimal curvature settings manually, thereby limiting their ability to effectively model semantic hierarchies. To address this limitation, we propose a novel KGE model called $\textbf{Hyp}$erbolic $\textbf{H}$ierarchical $\textb
    
[^270]: 簇代数：网络科学与机器学习

    Cluster Algebras: Network Science and Machine Learning

    [https://arxiv.org/abs/2203.13847](https://arxiv.org/abs/2203.13847)

    本研究通过网络科学和机器学习技术对簇代数进行了研究，发现在簇代数的交换图中存在一种优雅的对称性，可成功使用种子数据对簇代数进行分类，准确性超过0.9。

    

    簇代数最近在数学和物理学中变得非常重要。在这项工作中，我们通过现代数据科学的视角对其进行了研究，具体采用了网络科学和机器学习的技术。我们将网络分析方法应用于不同突变类型的簇代数的交换图中。分析表明，当以不通过识别簇之间的置换等价性来表示图时，在凝聚图嵌入中会出现优雅的对称性。对于秩不超过5的有限Dynkin类型代数，计算了与该对称性相关的种子数量和关联的准经稳图的比率，并推测了更高等级的情况。简单的机器学习技术成功地学习了使用种子数据对簇代数进行分类。该学习性能在相同突变类型和不同类型的代数之间的准确性上超过了0.9。

    arXiv:2203.13847v2 Announce Type: replace-cross  Abstract: Cluster algebras have recently become an important player in mathematics and physics. In this work, we investigate them through the lens of modern data science, specifically with techniques from network science and machine learning. Network analysis methods are applied to the exchange graphs for cluster algebras of varying mutation types. The analysis indicates that when the graphs are represented without identifying by permutation equivalence between clusters an elegant symmetry emerges in the quiver exchange graph embedding. The ratio between number of seeds and number of quivers associated to this symmetry is computed for finite Dynkin type algebras up to rank 5, and conjectured for higher ranks. Simple machine learning techniques successfully learn to classify cluster algebras using the data of seeds. The learning performance exceeds 0.9 accuracies between algebras of the same mutation type and between types, as well as rel
    
[^271]: 变分贝叶斯中的柔性后验的伯恩斯块流

    Bernstein Flows for Flexible Posteriors in Variational Bayes

    [https://arxiv.org/abs/2202.05650](https://arxiv.org/abs/2202.05650)

    该论文提出了一种名为伯恩斯块流变分推断（BF-VI）的方法，能够灵活逼近复杂的多元后验，在实验中表现优于其他VI方法。

    

    变分推断（VI）是一种通过优化来近似难以计算后验的技术。与MCMC相比，VI可以扩展到许多观测。然而，在复杂后验的情况下，现有的VI方法通常产生令人不满意的后验近似。本文提出了伯恩斯块流变分推断（BF-VI），这是一种稳健且易于使用的方法，足够灵活以逼近复杂的多元后验。BF-VI结合了归一化流和基于伯恩斯多项式的转换模型的思想。在基准实验中，我们将BF-VI解与准确的后验、MCMC解以及基于归一化流的VI等现有VI方法进行了比较。我们发现在低维模型中，BF-VI可以准确逼近真实后验；而在高维模型中，BF-VI优于其他VI方法。此外，我们利用BF-VI针对半结构化Mela开发了一个贝叶斯模型。

    arXiv:2202.05650v2 Announce Type: replace-cross  Abstract: Variational inference (VI) is a technique to approximate difficult to compute posteriors by optimization. In contrast to MCMC, VI scales to many observations. In the case of complex posteriors, however, state-of-the-art VI approaches often yield unsatisfactory posterior approximations. This paper presents Bernstein flow variational inference (BF-VI), a robust and easy-to-use method, flexible enough to approximate complex multivariate posteriors. BF-VI combines ideas from normalizing flows and Bernstein polynomial-based transformation models. In benchmark experiments, we compare BF-VI solutions with exact posteriors, MCMC solutions, and state-of-the-art VI methods including normalizing flow based VI. We show for low-dimensional models that BF-VI accurately approximates the true posterior; in higher-dimensional models, BF-VI outperforms other VI methods. Further, we develop with BF-VI a Bayesian model for the semi-structured Mela
    
[^272]: 从有条件平稳时间序列中进行因果发现

    Causal Discovery from Conditionally Stationary Time Series

    [https://arxiv.org/abs/2110.06257](https://arxiv.org/abs/2110.06257)

    该论文提出了一种State-Dependent Causal Inference（SDCI）方法，可以处理一类宽泛的非平稳时间序列，成功地回复出潜在的因果依赖关系。

    

    因果发现，即从观测数据推断潜在的因果关系，已被证明对AI系统具有极大挑战。在时间序列建模背景下，传统的因果发现方法主要考虑具有完全观测变量和/或来自平稳时间序列的数据的受限场景。我们开发了一种因果发现方法来处理一类宽泛的非平稳时间序列，即在条件上是平稳的条件平稳时间序列，其中非平稳行为被建模为在一组（可能是隐藏的）状态变量上的平稳性。命名为State-Dependent Causal Inference（SDCI），我们的方法能够可证地回复出潜在的因果依赖关系，证明在完全观察到的状态下，并在存在隐藏状态时经验性地实现。后者通过对合成线性系统和非线性粒子相互作用数据的实验进行验证，SDCI实现了优于基线因果发现方法的性能。

    arXiv:2110.06257v2 Announce Type: replace  Abstract: Causal discovery, i.e., inferring underlying causal relationships from observational data, has been shown to be highly challenging for AI systems. In time series modeling context, traditional causal discovery methods mainly consider constrained scenarios with fully observed variables and/or data from stationary time-series. We develop a causal discovery approach to handle a wide class of non-stationary time-series that are conditionally stationary, where the non-stationary behaviour is modeled as stationarity conditioned on a set of (possibly hidden) state variables. Named State-Dependent Causal Inference (SDCI), our approach is able to recover the underlying causal dependencies, provably with fully-observed states and empirically with hidden states. The latter is confirmed by experiments on synthetic linear system and nonlinear particle interaction data, where SDCI achieves superior performance over baseline causal discovery methods
    
[^273]: 具有贝叶斯神经网络的对抗样本检测

    Adversarial Examples Detection with Bayesian Neural Network

    [https://arxiv.org/abs/2105.08620](https://arxiv.org/abs/2105.08620)

    提出了一种基于贝叶斯神经网络的新框架，利用随机性模拟隐藏层输出分布，从而改善对抗样本检测性能。

    

    在本文中，我们提出了一个新的框架来检测对抗样本，其灵感来源于这样的观察结果：随机组件可以提高预测器的平滑性，使得更容易模拟深度神经网络的输出分布。基于这些观察结果，我们提出了一种新颖的贝叶斯对抗样本检测器，简称为BATer，以提高对抗样本检测的性能。具体而言，我们研究了自然样本和对抗样本之间隐藏层输出的分布差异，并建议使用贝叶斯神经网络的随机性来模拟隐藏层输出分布，并利用分布的离散性来检测对抗样本。

    arXiv:2105.08620v3 Announce Type: replace-cross  Abstract: In this paper, we propose a new framework to detect adversarial examples motivated by the observations that random components can improve the smoothness of predictors and make it easier to simulate the output distribution of a deep neural network. With these observations, we propose a novel Bayesian adversarial example detector, short for BATer, to improve the performance of adversarial example detection. Specifically, we study the distributional difference of hidden layer output between natural and adversarial examples, and propose to use the randomness of the Bayesian neural network to simulate hidden layer output distribution and leverage the distribution dispersion to detect adversarial examples. The advantage of a Bayesian neural network is that the output is stochastic while a deep neural network without random components does not have such characteristics. Empirical results on several benchmark datasets against popular a
    
[^274]: 具有不准确梯度的 Langevin Monte Carlo 的用户友好保证

    User-friendly guarantees for the Langevin Monte Carlo with inaccurate gradient

    [https://arxiv.org/abs/1710.00095](https://arxiv.org/abs/1710.00095)

    该论文分析了具有不准确梯度的 Langevin Monte Carlo 算法的采样问题，并在Wasserstein-2距离中提出了改进的误差保证。

    

    在本文中，我们研究了从已知光滑且强对数凹函数的概率密度函数中采样的问题。我们分析了基于(高度过阻尼) Langevin 扩散的离散化的近似采样方法，并建立了在Wasserstein-2距离中测量的误差保证。我们在三个方向上改进或扩展了最新结果。首先，我们对优化的不定步长一阶 Langevin Monte Carlo(LMC)算法的误差给出了上界。这个结果的优点是不受时间限制(我们无需事先知道目标精度)，并且在对应的常数步长结果基础上提升了对数因子。其次，我们研究了当无法准确评估对数密度梯度，但可以获得前述梯度的近似时的情况。

    arXiv:1710.00095v4 Announce Type: replace-cross  Abstract: In this paper, we study the problem of sampling from a given probability density function that is known to be smooth and strongly log-concave. We analyze several methods of approximate sampling based on discretizations of the (highly overdamped) Langevin diffusion and establish guarantees on its error measured in the Wasserstein-2 distance. Our guarantees improve or extend the state-of-the-art results in three directions. First, we provide an upper bound on the error of the first-order Langevin Monte Carlo (LMC) algorithm with optimized varying step-size. This result has the advantage of being horizon free (we do not need to know in advance the target precision) and to improve by a logarithmic factor the corresponding result for the constant step-size. Second, we study the case where accurate evaluations of the gradient of the log-density are unavailable, but one can have access to approximations of the aforementioned gradient.
    
[^275]: 自适应深度学习用于超低功耗纳米无人机上的高效视觉姿态估计

    Adaptive Deep Learning for Efficient Visual Pose Estimation aboard Ultra-low-power Nano-drones. (arXiv:2401.15236v1 [cs.CV])

    [http://arxiv.org/abs/2401.15236](http://arxiv.org/abs/2401.15236)

    本研究提出了一种自适应深度学习机制，用于在超低功耗纳米无人机上进行高效的视觉姿态估计。通过将两种具有不同性能和成本权衡的卷积神经网络与自适应分类模块结合使用，我们能够根据计算资源的可用性选择合适的网络以实现高效的姿态估计。

    

    小于10厘米直径的纳米无人机由于其适用于较大的飞行无人机无法到达的狭窄环境和人类附近的特点，正变得越来越受关注。然而，其微小的外形也带来了一个主要的缺点：超限的内存和处理器用于其感知流程的机载执行。因此，基于轻量级深度学习的方法越来越受欢迎，强调计算效率和节能的重要性，因为这可以决定一个完全工作的闭环系统和一个失败的闭环系统之间的区别。在本研究中，为了最大限度地利用纳米无人机上极其有限的资源，我们提出了一种新的自适应深度学习机制，用于高效执行基于视觉的人体姿态估计任务。我们结合了两种具有不同回归性能与计算成本折衷的最新卷积神经网络（CNN）。通过将这些CNN与一个自适应分类模块组合起来，我们能够根据计算资源的可用性自适应地选择合适的网络以实现高效的姿态估计。

    Sub-10cm diameter nano-drones are gaining momentum thanks to their applicability in scenarios prevented to bigger flying drones, such as in narrow environments and close to humans. However, their tiny form factor also brings their major drawback: ultra-constrained memory and processors for the onboard execution of their perception pipelines. Therefore, lightweight deep learning-based approaches are becoming increasingly popular, stressing how computational efficiency and energy-saving are paramount as they can make the difference between a fully working closed-loop system and a failing one. In this work, to maximize the exploitation of the ultra-limited resources aboard nano-drones, we present a novel adaptive deep learning-based mechanism for the efficient execution of a vision-based human pose estimation task. We leverage two State-of-the-Art (SoA) convolutional neural networks (CNNs) with different regression performance vs. computational costs trade-offs. By combining these CNNs wi
    
[^276]: 基于得分结构先验的部分已知高斯图模型估计

    Estimation of partially known Gaussian graphical models with score-based structural priors. (arXiv:2401.14340v1 [stat.ML])

    [http://arxiv.org/abs/2401.14340](http://arxiv.org/abs/2401.14340)

    本论文提出了一种基于得分结构先验的算法，用于估计部分已知高斯图模型。通过使用图神经网络来估计图的得分函数，我们可以在生成样本时利用退火朗格维能扩散，从而更准确地估计后验分布。数值实验表明，我们的方法具有明显的优势。

    

    我们提出了一种新的算法，用于支持估计部分已知的高斯图模型，并且结合了关于底层图的先验信息。与传统方法相比，传统方法使用点估计方法基于最大似然或最大后验准则，并使用（简单的）精度矩阵先验来提供点估计。我们考虑对图进行先验，并依赖退火朗格维能扩散从后验分布中生成样本。由于朗格维能采样器需要访问底层图先验的得分函数，因此我们使用图神经网络来有效地从图数据集（事先可用或从已知分布生成）估计得分。数值实验证明了我们方法的优势。

    We propose a novel algorithm for the support estimation of partially known Gaussian graphical models that incorporates prior information about the underlying graph. In contrast to classical approaches that provide a point estimate based on a maximum likelihood or a maximum a posteriori criterion using (simple) priors on the precision matrix, we consider a prior on the graph and rely on annealed Langevin diffusion to generate samples from the posterior distribution. Since the Langevin sampler requires access to the score function of the underlying graph prior, we use graph neural networks to effectively estimate the score from a graph dataset (either available beforehand or generated from a known distribution). Numerical experiments demonstrate the benefits of our approach.
    
[^277]: 在稀疏图上学习平均场对局博弈：一种混合图形扩展方法

    Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach. (arXiv:2401.12686v1 [cs.MA])

    [http://arxiv.org/abs/2401.12686](http://arxiv.org/abs/2401.12686)

    这篇论文提出了一种在稀疏图上学习平均场对局博弈的新方法，通过引入图形扩展的概念，解决了现有方法对于稀疏网络拓扑结构的限制。

    

    学习大规模代理群体的行为是许多研究领域中的重要任务。虽然多代理强化学习（MARL）领域在解决这些系统方面取得了重要进展，但对于许多代理的解决方案通常在计算上是不可行的，且缺乏理论保证。平均场对局博弈（MFGs）解决了这两个问题，并且可以扩展到包括代理之间的网络结构的图形平均场对局博弈（GMFGs）。尽管具有诸多优点，但GMFGs的现实世界应用受到图形只能捕捉密集图的限制。由于大多数实验证明的网络显示出一定程度的稀疏性，例如幂律图，因此GMFG框架无法捕捉这些网络拓扑结构。因此，我们提出了一种新颖的图形对局博弈（GXMFGs）的概念，它建立在图论概念图形扩展（graphexes）基础上。图形扩展是稀疏图序列的极限对象，还具有其他一些理想特性，如sma

    Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the sma
    
[^278]: GNNShap: 使用Shapley值快速而准确解释GNN的论文

    GNNShap: Fast and Accurate GNN Explanations using Shapley Values. (arXiv:2401.04829v1 [cs.LG])

    [http://arxiv.org/abs/2401.04829](http://arxiv.org/abs/2401.04829)

    GNNShap是一种使用Shapley值的解释方法，能够快速而准确地解释图神经网络的预测结果。相较于其他方法，GNNShap通过抽样、并行化计算等技术提高了解释速度和精细度。

    

    图神经网络(GNN)是一种在科学领域中具有广泛应用的图机器学习模型。然而，GNN被认为是黑盒模型，很难理解模型如何进行预测。基于博弈论的Shapley值方法在其他领域中被广泛应用于解释模型，但在图领域中研究较少。一些研究已经提出了基于Shapley值的GNN解释方法，然而它们存在一些限制：它们只考虑了有限的样本来近似Shapley值；有些方法主要关注小和大的联盟大小，并且它们比其他解释方法慢了一个数量级，使得它们在中等规模的图中无法应用。在这项工作中，我们提出了GNNShap，它提供边的解释，因为它们对图提供了更自然和精细的解释。我们通过对所有联盟大小进行抽样、在GPU上并行抽样和加速模型等方面克服了这些限制。

    Graph neural networks (GNNs) are popular machine learning models for graphs with many applications across scientific domains. However, GNNs are considered black box models, and it is challenging to understand how the model makes predictions. Game theory-based Shapley value approaches are popular explanation methods in other domains but are not well-studied for graphs. Some studies have proposed Shapley value-based GNN explanations, yet they have several limitations: they consider limited samples to approximate Shapley values; some mainly focus on small and large coalition sizes, and they are an order of magnitude slower than other explanation methods, making them inapplicable to even moderate-size graphs. In this work, we propose GNNShap, which provides explanations for edges since they provide more natural explanations for graphs and more fine-grained explanations. We overcome the limitations by sampling from all coalition sizes, parallelizing the sampling on GPUs, and speeding up mod
    
[^279]: 通过细粒度模型参数扰动实现机器去学习

    Machine unlearning through fine-grained model parameters perturbation. (arXiv:2401.04385v1 [cs.LG])

    [http://arxiv.org/abs/2401.04385](http://arxiv.org/abs/2401.04385)

    本文提出了一种精细的机器去学习策略，通过细粒度模型参数的扰动来实现用户隐私保护，同时保持可控的计算成本。采用遗忘率和记忆保留率等新的指标来评估去学习效果和模型泛化能力。

    

    机器去学习技术涉及到撤销数据记录和减小该数据对训练模型的影响，从而帮助实现用户隐私保护目标，但会带来显著的计算成本。基于参数扰动的权重去学习是一种通用方法，但通常涉及到全局修改参数。我们提出了精细的Top-K和Random-k参数扰动不精确机器去学习策略，以满足隐私需求同时保持计算成本可控。为了展示我们策略的有效性，我们还解决了评估机器去学习效果的挑战，考虑了模型在去学习和剩余数据上的广义性能。为了更好地评估去学习效果和模型泛化能力，我们提出了新的指标，即遗忘率和记忆保留率。然而，对于不精确的机器去学习，现有的指标无法对去学习程度进行准确量化。

    Machine unlearning techniques, which involve retracting data records and reducing influence of said data on trained models, help with the user privacy protection objective but incur significant computational costs. Weight perturbation-based unlearning is a general approach, but it typically involves globally modifying the parameters. We propose fine-grained Top-K and Random-k parameters perturbed inexact machine unlearning strategies that address the privacy needs while keeping the computational costs tractable.  In order to demonstrate the efficacy of our strategies we also tackle the challenge of evaluating the effectiveness of machine unlearning by considering the model's generalization performance across both unlearning and remaining data. To better assess the unlearning effect and model generalization, we propose novel metrics, namely, the forgetting rate and memory retention rate. However, for inexact machine unlearning, current metrics are inadequate in quantifying the degree of
    
[^280]: 神经因果抽象

    Neural Causal Abstractions. (arXiv:2401.02602v1 [cs.LG])

    [http://arxiv.org/abs/2401.02602](http://arxiv.org/abs/2401.02602)

    本文提出了一种新的神经因果抽象方法，通过聚类变量和其域，用于解决真实因果推断任务中的挑战，并通过神经因果模型实现了学习和应用。

    

    人类理解世界中的因果关系以及将信息压缩成抽象概念的能力是人类智慧的两个标志性特征。这两个主题在文献中被统称为因果抽象理论同时进行研究。在实践中，如何在真实的因果推断任务中充分利用抽象理论仍然是一个开放的问题，因为真实机制是未知的，只有有限的数据可用。在本文中，我们通过对变量及其域进行聚类，开发了一种新的因果抽象家族。这种方法改进和概括了之前的抽象概念，以更好地适应Pearl的因果层次结构引发的个体因果分布。我们证明了在实际场景中通过神经因果模型（Xia等，2021）可以学得这样的抽象概念，从而能够利用深度学习技术解决各种具有挑战性的因果推断任务。

    The abilities of humans to understand the world in terms of cause and effect relationships, as well as to compress information into abstract concepts, are two hallmark features of human intelligence. These two topics have been studied in tandem in the literature under the rubric of causal abstractions theory. In practice, it remains an open problem how to best leverage abstraction theory in real-world causal inference tasks, where the true mechanisms are unknown and only limited data is available. In this paper, we develop a new family of causal abstractions by clustering variables and their domains. This approach refines and generalizes previous notions of abstractions to better accommodate individual causal distributions that are spawned by Pearl's causal hierarchy. We show that such abstractions are learnable in practical settings through Neural Causal Models (Xia et al., 2021), enabling the use of the deep learning toolkit to solve various challenging causal inference tasks -- iden
    
[^281]: 强化学习的扩散模型: 一份综述

    Diffusion Models for Reinforcement Learning: A Survey. (arXiv:2311.01223v1 [cs.LG])

    [http://arxiv.org/abs/2311.01223](http://arxiv.org/abs/2311.01223)

    强化学习中的扩散模型已经成为一种突出的生成模型，通过在样本质量和训练稳定性方面的优势改进了强化学习解决方案。该综述提供了这一新兴领域发展的概述，并探讨了扩散模型在强化学习中的分类法和应用。

    

    扩散模型作为一种突出的生成模型类别已经出现，超越了以往方法在样本质量和训练稳定性方面的优势。最近的研究表明，扩散模型在改进强化学习（RL）解决方案方面具有优势，包括作为轨迹规划器、表达能力丰富的策略类别、数据合成器等。本综述旨在提供该新兴领域发展的概述，并希望能启发新的研究方向。首先，我们审查了当前RL算法遇到的一些挑战。然后，我们根据扩散模型在RL中所扮演的角色，提出了现有方法的分类法，并探讨了如何解决现有挑战。我们进一步概述了扩散模型在各种与RL相关任务中的成功应用，并讨论了当前方法的局限性。最后，我们总结了这项综述，并提出了对未来研究方向的见解，重点是提高模型性能和应用扩散模型的方法。

    Diffusion models have emerged as a prominent class of generative models, surpassing previous methods regarding sample quality and training stability. Recent works have shown the advantages of diffusion models in improving reinforcement learning (RL) solutions, including as trajectory planners, expressive policy classes, data synthesizers, etc. This survey aims to provide an overview of the advancements in this emerging field and hopes to inspire new avenues of research. First, we examine several challenges encountered by current RL algorithms. Then, we present a taxonomy of existing methods based on the roles played by diffusion models in RL and explore how the existing challenges are addressed. We further outline successful applications of diffusion models in various RL-related tasks while discussing the limitations of current approaches. Finally, we conclude the survey and offer insights into future research directions, focusing on enhancing model performance and applying diffusion m
    
[^282]: Alquist 5.0：对话树与生成模型相结合。增强SocialBot对话的一种新方法。

    Alquist 5.0: Dialogue Trees Meet Generative Models. A Novel Approach for Enhancing SocialBot Conversations. (arXiv:2310.16119v1 [cs.LG])

    [http://arxiv.org/abs/2310.16119](http://arxiv.org/abs/2310.16119)

    Alquist 5.0是一种新的SocialBot系统，通过将对话树和生成模型相结合，以及引入NRG Barista和支持多模式设备，提高了用户对话体验，并保持了共情和知识型对话能力。

    

    我们介绍了我们的SocialBot- Alquist 5.0-，该系统是为Alexa Prize SocialBot大挑战5开发的。在我们系统的前几个版本基础上，我们引入了NRG Barista，并概述了将Barista整合到我们的SocialBot中的几种创新方法，从而改善了整体的对话体验。此外，我们还扩展了我们的SocialBot以支持多模式设备。本文提供了关于Alquist 5.0开发的见解，该系统在满足用户不断变化的期望的同时，保持了对各种主题的共情和知识型对话能力。

    We present our SocialBot -- Alquist~5.0 -- developed for the Alexa Prize SocialBot Grand Challenge~5. Building upon previous versions of our system, we introduce the NRG Barista and outline several innovative approaches for integrating Barista into our SocialBot, improving the overall conversational experience. Additionally, we extend our SocialBot to support multimodal devices. This paper offers insights into the development of Alquist~5.0, which meets evolving user expectations while maintaining empathetic and knowledgeable conversational abilities across diverse topics.
    
[^283]: 几乎等变性通过李代数卷积

    Almost Equivariance via Lie Algebra Convolutions. (arXiv:2310.13164v1 [cs.LG])

    [http://arxiv.org/abs/2310.13164](http://arxiv.org/abs/2310.13164)

    本文研究了几乎等变性的主题，并提供了一个不同于现有定义的几乎等变性定义，并通过利用李群的李代数给出了在模型中编码几乎等变性的实用方法。

    

    最近，在机器学习中，模型相对于群作用的等变性已成为一个重要的研究课题。然而，赋予一个架构具体的群等变性对模型所期望看到的数据变换类型施加了强大的先验。严格等变模型强制执行对称性，但真实世界的数据并不总是符合这样的严格等变性，可能是因为数据中的噪声或仅编码了近似或部分对称性的潜在物理定律。在这种情况下，严格等变性的先验实际上可能过于强大，导致模型在真实数据上表现不佳。因此，在这项工作中，我们研究了一个相关的主题，即几乎等变性。我们提供了一个与当前文献中现有定义不同的几乎等变性定义，并通过利用李群的李代数给出了在模型中编码几乎等变性的实用方法。

    Recently, the equivariance of models with respect to a group action has become an important topic of research in machine learning. However, imbuing an architecture with a specific group equivariance imposes a strong prior on the types of data transformations that the model expects to see. While strictly-equivariant models enforce symmetries, real-world data does not always conform to such strict equivariances, be it due to noise in the data or underlying physical laws that encode only approximate or partial symmetries. In such cases, the prior of strict equivariance can actually prove too strong and cause models to underperform on real-world data. Therefore, in this work we study a closely related topic, that of almost equivariance. We provide a definition of almost equivariance that differs from those extant in the current literature and give a practical method for encoding almost equivariance in models by appealing to the Lie algebra of a Lie group. Specifically, we define Lie algebr
    
[^284]: ProbTS：一种用于探索深度时间序列预测的统一工具包。

    ProbTS: A Unified Toolkit to Probe Deep Time-series Forecasting. (arXiv:2310.07446v1 [cs.LG])

    [http://arxiv.org/abs/2310.07446](http://arxiv.org/abs/2310.07446)

    ProbTS是一个统一的工具包，用于协同和比较定制神经架构和深度生成模型在时间序列预测中的方法，揭示了它们的特点、优势和需要进一步研究的领域。

    

    时间序列预测在各个领域的各种应用中起着关键作用。随着深度学习的发展，这个领域分化成了两个显著的分支：一个专注于为时间序列定制特定的神经架构，另一个利用先进的深度生成模型进行概率预测。虽然这两个分支都取得了显著的进展，但它们在数据情景、方法论焦点和解码方案上的差异提出了深入而未被探索的研究问题。为了填补这一知识鸿沟，我们引入了ProbTS，这是一个创新的工具包，旨在协同和比较这两个不同的分支。ProbTS具备统一的数据模块、模块化的模型模块和全面的评估器模块，使我们能够重新审视和基准测试这两个分支的领先方法。通过ProbTS的审查，突显了它们各自的特点、相对优势和劣势，以及需要进一步研究的领域。

    Time-series forecasting serves as a linchpin in a myriad of applications, spanning various domains. With the growth of deep learning, this arena has bifurcated into two salient branches: one focuses on crafting specific neural architectures tailored for time series, and the other harnesses advanced deep generative models for probabilistic forecasting. While both branches have made significant progress, their differences across data scenarios, methodological focuses, and decoding schemes pose profound, yet unexplored, research questions. To bridge this knowledge chasm, we introduce ProbTS, a pioneering toolkit developed to synergize and compare these two distinct branches. Endowed with a unified data module, a modularized model module, and a comprehensive evaluator module, ProbTS allows us to revisit and benchmark leading methods from both branches. The scrutiny with ProbTS highlights their distinct characteristics, relative strengths and weaknesses, and areas that need further explorat
    
[^285]: CFDBench：流体动力学中机器学习方法的综合基准测试

    CFDBench: A Comprehensive Benchmark for Machine Learning Methods in Fluid Dynamics. (arXiv:2310.05963v1 [cs.LG])

    [http://arxiv.org/abs/2310.05963](http://arxiv.org/abs/2310.05963)

    这个论文介绍了CFDBench，这是一个针对计算流体动力学中四个经典问题的基准测试。它包含了不同边界条件、流体物理特性和域几何的数据，能帮助评估深度学习方法在解决物理问题中的表现。

    

    近年来，将深度学习应用于解决物理问题已经引起了广泛关注。数据驱动的深度学习方法可以生成能学习解决整个偏微分方程系统的算子。然而，现有方法仅在简单的流动方程（如Burger方程）上进行评估，并且仅考虑了对不同初始条件的泛化能力。本文构建了CFDBench，一个针对计算流体力学（CFD）中四个经典问题的基准测试：驱动腔流动、圆管中的层流边界层流动、通过台阶的坝流动和周期性的卡门涡街流动。每个流动问题都包括具有不同边界条件、流体物理特性和域几何的数据。与现有数据集相比，CFDBench具有以下优势：（1）综合。它包含常用的物理参数，如速度、压力和腔体比例。（2）真实。非常适合深度学习解决方案。

    In recent years, applying deep learning to solve physics problems has attracted much attention. Data-driven deep learning methods produce operators that can learn solutions to the whole system of partial differential equations. However, the existing methods are only evaluated on simple flow equations (e.g., Burger's equation), and only consider the generalization ability on different initial conditions. In this paper, we construct CFDBench, a benchmark with four classic problems in computational fluid dynamics (CFD): lid-driven cavity flow, laminar boundary layer flow in circular tubes, dam flows through the steps, and periodic Karman vortex street. Each flow problem includes data with different boundary conditions, fluid physical properties, and domain geometry. Compared to existing datasets, the advantages of CFDBench are (1) comprehensive. It contains common physical parameters such as velocity, pressure, and cavity fraction. (2) realistic. It is very suitable for deep learning solu
    
[^286]: LARA：一种轻量级且抗过拟合的无监督异常检测再训练方法

    LARA: A Light and Anti-overfitting Retraining Approach for Unsupervised Anomaly Detection. (arXiv:2310.05668v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.05668](http://arxiv.org/abs/2310.05668)

    LARA是一种轻量级且抗过拟合的无监督异常检测再训练方法，它将重新训练过程形式化为一个凸问题，并设计了一个反思模块以利用历史数据，同时数学证明了在微调后可以获得更好的性能。

    

    当前大部分异常检测模型都假设正常模式始终保持不变。然而，Web服务的正常模式经常发生剧烈变化。在这种变化之后，使用旧分布数据训练的模型已经过时。每次都重新训练整个模型是昂贵的。此外，在正常模式变化开始时，新分布的观察数据不足。用有限的数据对大型神经网络模型进行重新训练容易过拟合。因此，我们提出了一种轻量级且抗过拟合的再训练方法（LARA），用于基于深度变分自编码器的时间序列异常检测方法（VAEs）。本工作旨在提出三个新颖的贡献：1）将重新训练过程形式化为一个凸问题，并能够以快速收敛以及防止过拟合；2）设计了一个反思模块，可以利用历史数据而无需储存它们；3）数学证明了在微调后可以获得更好的性能。

    Most of current anomaly detection models assume that the normal pattern remains same all the time. However, the normal patterns of Web services change dramatically and frequently. The model trained on old-distribution data is outdated after such changes. Retraining the whole model every time is expensive. Besides, at the beginning of normal pattern changes, there is not enough observation data from the new distribution. Retraining a large neural network model with limited data is vulnerable to overfitting. Thus, we propose a Light and Anti-overfitting Retraining Approach (LARA) for deep variational auto-encoder based time series anomaly detection methods (VAEs). This work aims to make three novel contributions: 1) the retraining process is formulated as a convex problem and can converge at a fast rate as well as prevent overfitting; 2) designing a ruminate block, which leverages the historical data without the need to store them; 3) mathematically proving that when fine-tuning the late
    
[^287]: 一个可计数具有相同骨架的马尔可夫等价类的固定参数可处理算法

    A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence Classes with the same Skeleton. (arXiv:2310.04218v1 [cs.DS])

    [http://arxiv.org/abs/2310.04218](http://arxiv.org/abs/2310.04218)

    本文提出了一个固定参数可处理算法，用于计数具有相同骨架的马尔可夫等价类。

    

    因果有向无环图（也称为贝叶斯网络）是编码随机变量之间条件依赖关系的流行工具。在因果有向无环图中，随机变量被建模为有向图中的顶点，并且规定每个随机变量在给定其父节点的情况下与其祖先节点无关。然而，对于同一组随机变量上的两个不同的因果有向无环图可以准确编码相同的一组条件依赖关系。这样的因果有向无环图被称为马尔可夫等价，马尔可夫等价的因果有向无环图的等价类被称为马尔可夫等价类（MEC）。在过去几十年中，对于MEC已经创建了一些美丽的组合特征，并且已知，特别是在同一MEC中的所有因果有向无环图必须具有相同的“骨架”（底层无向图）和v-结构（形式为$a\rightarrow b \leftarrow c$的诱导子图）。这些组合特征还提出了几个自然的算法问题。

    Causal DAGs (also known as Bayesian networks) are a popular tool for encoding conditional dependencies between random variables. In a causal DAG, the random variables are modeled as vertices in the DAG, and it is stipulated that every random variable is independent of its ancestors conditioned on its parents. It is possible, however, for two different causal DAGs on the same set of random variables to encode exactly the same set of conditional dependencies. Such causal DAGs are said to be Markov equivalent, and equivalence classes of Markov equivalent DAGs are known as Markov Equivalent Classes (MECs). Beautiful combinatorial characterizations of MECs have been developed in the past few decades, and it is known, in particular that all DAGs in the same MEC must have the same ''skeleton'' (underlying undirected graph) and v-structures (induced subgraph of the form $a\rightarrow b \leftarrow c$).  These combinatorial characterizations also suggest several natural algorithmic questions. On
    
[^288]: Rayleigh Quotient Graph Neural Networks用于图级异常检测的研究

    Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection. (arXiv:2310.02861v1 [cs.LG])

    [http://arxiv.org/abs/2310.02861](http://arxiv.org/abs/2310.02861)

    《Rayleigh Quotient Graph Neural Networks用于图级异常检测的研究》提出使用Rayleigh Quotient作为驱动因素，通过探索图的固有光谱特征来实现图级异常检测。

    

    图级异常检测在癌症诊断和酶预测等领域中广泛应用。然而，现有方法无法捕捉到图异常的潜在属性，导致框架设计不可解释和性能不令人满意。在本文中，我们退一步重新研究了异常和正常图之间的光谱差异。我们的主要观察表明，这两个类之间的累计光谱能量存在显著差异。此外，我们证明了图信号的累计光谱能量可以用其瑞利商表示，这表明瑞利商是图异常属性的一个驱动因素。受此启发，我们提出了Rayleigh Quotient Graph Neural Network（RQGNN），这是第一个用于图级异常检测的光谱GNN，为探索异常图的固有光谱特征提供了新的视角。

    Graph-level anomaly detection has gained significant attention as it finds many applications in various domains, such as cancer diagnosis and enzyme prediction. However, existing methods fail to capture the underlying properties of graph anomalies, resulting in unexplainable framework design and unsatisfying performance. In this paper, we take a step back and re-investigate the spectral differences between anomalous and normal graphs. Our main observation shows a significant disparity in the accumulated spectral energy between these two classes. Moreover, we prove that the accumulated spectral energy of the graph signal can be represented by its Rayleigh Quotient, indicating that the Rayleigh Quotient is a driving factor behind the anomalous properties of graphs. Motivated by this, we propose Rayleigh Quotient Graph Neural Network (RQGNN), the first spectral GNN for graph-level anomaly detection, providing a new perspective on exploring the inherent spectral features of anomalous graph
    
[^289]: DataInf：在LLMs和扩散模型中高效估计数据影响力

    DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models. (arXiv:2310.00902v1 [cs.LG])

    [http://arxiv.org/abs/2310.00902](http://arxiv.org/abs/2310.00902)

    DataInf是一种高效的影响力近似方法，特别适用于大规模生成型AI模型，相比现有方法在计算和内存效率上有明显优势。

    

    量化训练数据点的影响力对于理解机器学习模型的输出和提高AI管道的透明度至关重要。影响函数是一种原则性和流行的数据归属方法，但其计算成本使其难以使用。这个问题在大型语言模型和文本到图像模型的设置中更加突出。在这项工作中，我们提出了DataInf，一种高效的影响力近似方法，适用于大规模生成型AI模型。通过利用易于计算的闭式表达式，DataInf在计算和内存效率方面优于现有的影响计算算法。我们的理论分析表明，DataInf特别适用于诸如LoRA的参数有效微调技术。通过系统的实证评估，我们展示了DataInf能够准确地近似影响分数，并且比现有方法快几个数量级。

    Quantifying the impact of training data points is crucial for understanding the outputs of machine learning models and for improving the transparency of the AI pipeline. The influence function is a principled and popular data attribution method, but its computational cost often makes it challenging to use. This issue becomes more pronounced in the setting of large language models and text-to-image models. In this work, we propose DataInf, an efficient influence approximation method that is practical for large-scale generative AI models. Leveraging an easy-to-compute closed-form expression, DataInf outperforms existing influence computation algorithms in terms of computational and memory efficiency. Our theoretical analysis shows that DataInf is particularly well-suited for parameter-efficient fine-tuning techniques such as LoRA. Through systematic empirical evaluations, we show that DataInf accurately approximates influence scores and is orders of magnitude faster than existing methods
    
[^290]: 卷积深度核机器

    Convolutional Deep Kernel Machines. (arXiv:2309.09814v1 [stat.ML])

    [http://arxiv.org/abs/2309.09814](http://arxiv.org/abs/2309.09814)

    这项研究介绍了一种称为卷积深度核机器的新型核方法，该方法纯粹使用核而不使用特征，通过高效的跨域诱导点近似方案和多种模型变体的设计，达到了在MNIST、CIFAR-10和CIFAR-100上接近甚至超过其他方法的测试准确率。

    

    深度核机器(DKMs)是一种最近引入的具有其他深度模型灵活性的核方法，包括深度神经网络和深度高斯过程。DKMs纯粹使用核，而不使用特征，因此与其他方法（从神经网络到深度核学习甚至深度高斯过程）不同，后者都使用特征作为基本组成部分。在这里，我们引入了卷积DKMs，并配以一种高效的跨域诱导点近似方案。此外，我们还开发并实验评估了许多模型变体，包括9种不同类型的为卷积DKMs设计的归一化方法，两种似然函数和两种不同类型的顶层。尽管只在约28个GPU小时内训练（比完全的NNGP / NTK / Myrtle kernel快1-2个数量级），但得到的模型在MNIST上实现了约99％的测试准确性，在CIFAR-10上为92％，在CIFAR-100上为71％，同时达到可比较的性能。

    Deep kernel machines (DKMs) are a recently introduced kernel method with the flexibility of other deep models including deep NNs and deep Gaussian processes. DKMs work purely with kernels, never with features, and are therefore different from other methods ranging from NNs to deep kernel learning and even deep Gaussian processes, which all use features as a fundamental component. Here, we introduce convolutional DKMs, along with an efficient inter-domain inducing point approximation scheme. Further, we develop and experimentally assess a number of model variants, including 9 different types of normalisation designed for the convolutional DKMs, two likelihoods, and two different types of top-layer. The resulting models achieve around 99% test accuracy on MNIST, 92% on CIFAR-10 and 71% on CIFAR-100, despite training in only around 28 GPU hours, 1-2 orders of magnitude faster than full NNGP / NTK / Myrtle kernels, whilst achieving comparable performance.
    
[^291]: 无需位置标签构建室内基于区域的无线电地图

    Constructing Indoor Region-based Radio Map without Location Labels. (arXiv:2308.16759v1 [cs.LG])

    [http://arxiv.org/abs/2308.16759](http://arxiv.org/abs/2308.16759)

    本文提出了一种无需位置标签的基于区域的无线电地图构建方法，该方法利用接收信号强度（RSS）测量数据，并通过一个综合的分割和聚类算法实现了全局最优解的匹配。

    

    无线电地图的构建需要大量带有位置标签的无线电测量数据，这给部署成本带来了很高的压力。本文提出了一种基于区域的无线电地图构建方法，它利用接收信号强度（RSS）测量数据而无需位置标签。构建过程基于从一个设备上盲目收集到的RSS测量数据，该设备在室内区域中的各个区域中恰好访问一次，但没有记录脚印和时间戳。主要挑战是将RSS数据聚类，并将聚类与物理区域进行匹配。由于多径和噪声的存在，传统的聚类算法无法有效处理RSS数据，因为RSS数据自然而然地呈现为非聚类的形式。本文构建了一个带有顺序先验的信号子空间模型用于处理RSS数据，并开发了一种综合分割和聚类算法，在特殊情况下证明能够找到全局最优解。此外，使用基于图的算法将聚类数据与物理区域进行匹配。

    Radio map construction requires a large amount of radio measurement data with location labels, which imposes a high deployment cost. This paper develops a region-based radio map from received signal strength (RSS) measurements without location labels. The construction is based on a set of blindly collected RSS measurement data from a device that visits each region in an indoor area exactly once, where the footprints and timestamps are not recorded. The main challenge is to cluster the RSS data and match clusters with the physical regions. Classical clustering algorithms fail to work as the RSS data naturally appears as non-clustered due to multipaths and noise. In this paper, a signal subspace model with a sequential prior is constructed for the RSS data, and an integrated segmentation and clustering algorithm is developed, which is shown to find the globally optimal solution in a special case. Furthermore, the clustered data is matched with the physical regions using a graph-based app
    
[^292]: 机器学习在信任与安全方面的挑战：一个针对虚假信息检测的案例研究

    The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection. (arXiv:2308.12215v1 [cs.LG])

    [http://arxiv.org/abs/2308.12215](http://arxiv.org/abs/2308.12215)

    本研究通过虚假信息检测为例，检查了机器学习在信任与安全问题中学术与实践之间的脱节，并发现了文献中存在的严重不足之处，包括任务不符合在线服务面临的挑战、数据集和模型评估不真实、评估不独立于模型训练等。在此基础上，提出了评估机器学习应用于信任与安全问题的建议。

    

    我们使用虚假信息检测作为案例研究，检查了在将机器学习应用于信任与安全问题上学术和实践之间的脱节。我们对该领域中270篇广受引用的论文进行了自动检测虚假信息的文献系统化，并对子集中的论文进行了数据和代码的可用性、设计失误、可复现性和泛化性等方面的研究。我们发现文献中存在严重的不足之处，这对所声称的性能和实用性提出了质疑。检测任务通常与在线服务真正面临的挑战有本质上的区别。数据集和模型评估通常不代表现实世界的情景，而且评估往往不独立于模型训练。数据和代码的可用性很差。模型在领域外的数据上泛化能力不强。基于这些结果，我们提出了评估机器学习应用于信任与安全问题的建议。

    We examine the disconnect between scholarship and practice in applying machine learning to trust and safety problems, using misinformation detection as a case study. We systematize literature on automated detection of misinformation across a corpus of 270 well-cited papers in the field. We then examine subsets of papers for data and code availability, design missteps, reproducibility, and generalizability. We find significant shortcomings in the literature that call into question claimed performance and practicality. Detection tasks are often meaningfully distinct from the challenges that online services actually face. Datasets and model evaluation are often non-representative of real-world contexts, and evaluation frequently is not independent of model training. Data and code availability is poor. Models do not generalize well to out-of-domain data. Based on these results, we offer recommendations for evaluating machine learning applications to trust and safety problems. Our aim is fo
    
[^293]: 深度算子网络的大小下界

    Size Lowerbounds for Deep Operator Networks. (arXiv:2308.06338v1 [cs.LG])

    [http://arxiv.org/abs/2308.06338](http://arxiv.org/abs/2308.06338)

    本文建立了深度算子网络的数据依赖性大小下界，并证明了在解决偏微分方程时，支路网络和主干网络的共同输出维度需要与数据点数量按照Ω(√n)的比例扩展，并且为了获得更低的训练误差，训练数据的大小可能需要与共同输出维度按照二次比例关系扩展。

    

    深度算子网络是一种在无限维度中解决回归问题和一次解决一类偏微分方程组的流行范式。本文旨在建立一种首次依赖于数据的深度算子网络大小下界，以便能够在噪声数据上减小经验误差。具体而言，我们证明了为了获得低训练误差，需要将支路网络和主干网络的共同输出维度与数据点数量n按照Ω(√n)的比例扩展。这启发了我们在解决对流-扩散-反应偏微分方程时对深度算子网络进行的实验，我们证明了在固定模型大小的情况下，利用这种共同输出维度的增加，可以单调降低训练误差，而训练数据的大小可能需要与之呈二次比例关系。

    Deep Operator Networks are an increasingly popular paradigm for solving regression in infinite dimensions and hence solve families of PDEs in one shot. In this work, we aim to establish a first-of-its-kind data-dependent lowerbound on the size of DeepONets required for them to be able to reduce empirical error on noisy data. In particular, we show that for low training errors to be obtained on $n$ data points it is necessary that the common output dimension of the branch and the trunk net be scaling as $\Omega \left ( {\sqrt{n}} \right )$. This inspires our experiments with DeepONets solving the advection-diffusion-reaction PDE, where we demonstrate the possibility that at a fixed model size, to leverage increase in this common output dimension and get monotonic lowering of training error, the size of the training data might necessarily need to scale quadratically with it.
    
[^294]: 使用全射序列神经似然估计进行基于仿真的推断

    Simulation-based inference using surjective sequential neural likelihood estimation. (arXiv:2308.01054v1 [stat.ML])

    [http://arxiv.org/abs/2308.01054](http://arxiv.org/abs/2308.01054)

    我们提出了一种使用全射序列神经似然估计（SSNL）进行基于仿真的推断的新方法，在模型中无法计算似然函数并且只能使用模拟器生成数据的情况下，SSNL通过拟合降维的全射归一化流模型，并将其作为替代似然函数，解决了先前基于似然方法在高维数据集中遇到的问题，并在各种实验中展示了其优越性能。

    

    我们提出了全射序列神经似然（SSNL）估计方法，这是一种在模型中无法计算似然函数并且只能使用可以生成合成数据的模拟器时进行基于仿真的推断的新方法。SSNL拟合一个降维的全射归一化流模型，并将其用作替代似然函数，从而可以使用传统的贝叶斯推断方法，包括马尔科夫链蒙特卡罗方法或变分推断。通过将数据嵌入到低维空间中，SSNL解决了先前基于似然方法在应用于高维数据集时遇到的几个问题，例如包含无信息数据维度或位于较低维流形上的数据。我们对SSNL在各种实验中进行了评估，并表明它通常优于在基于仿真推断中使用的现代方法，例如在一项来自天体物理学的具有挑战性的真实世界例子上对磁场模型的建模。

    We present Surjective Sequential Neural Likelihood (SSNL) estimation, a novel method for simulation-based inference in models where the evaluation of the likelihood function is not tractable and only a simulator that can generate synthetic data is available. SSNL fits a dimensionality-reducing surjective normalizing flow model and uses it as a surrogate likelihood function which allows for conventional Bayesian inference using either Markov chain Monte Carlo methods or variational inference. By embedding the data in a low-dimensional space, SSNL solves several issues previous likelihood-based methods had when applied to high-dimensional data sets that, for instance, contain non-informative data dimensions or lie along a lower-dimensional manifold. We evaluate SSNL on a wide variety of experiments and show that it generally outperforms contemporary methods used in simulation-based inference, for instance, on a challenging real-world example from astrophysics which models the magnetic fi
    
[^295]: 具有交互层的量子卷积神经网络用于经典数据的分类

    Quantum Convolutional Neural Networks with Interaction Layers for Classification of Classical Data. (arXiv:2307.11792v1 [quant-ph])

    [http://arxiv.org/abs/2307.11792](http://arxiv.org/abs/2307.11792)

    本文介绍了一种引入了三量子位相互作用的新型交互层的量子卷积网络，增加了网络的表达能力和纠缠能力，用于对图像和一维数据进行分类。

    

    由于量子计算机具有异常的计算能力，量子机器学习（QML）引起了广泛关注。在不久的将来，几乎没有错误的量子计算机的承诺之下，对量子神经网络中多量子位相互作用的影响进行广泛研究非常重要。本文介绍了一种引入了三量子位相互作用的新型交互层的量子卷积网络，增加了网络的表达能力和纠缠能力，用于对图像和一维数据进行分类。该方法在三个公开可用的数据集MNIST、Fashion MNIST和Iris数据集上进行了测试，用于进行二元和多类别分类，并发现超越了现有最先进方法的性能。

    Quantum Machine Learning (QML) has come into the limelight due to the exceptional computational abilities of quantum computers. With the promises of near error-free quantum computers in the not-so-distant future, it is important that the effect of multi-qubit interactions on quantum neural networks is studied extensively. This paper introduces a Quantum Convolutional Network with novel Interaction layers exploiting three-qubit interactions increasing the network's expressibility and entangling capability, for classifying both image and one-dimensional data. The proposed approach is tested on three publicly available datasets namely MNIST, Fashion MNIST, and Iris datasets, to perform binary and multiclass classifications and is found to supersede the performance of the existing state-of-the-art methods.
    
[^296]: FMT: 通过特征图测试在深度神经网络中移除后门特征图

    FMT: Removing Backdoor Feature Maps via Feature Map Testing in Deep Neural Networks. (arXiv:2307.11565v1 [cs.LG])

    [http://arxiv.org/abs/2307.11565](http://arxiv.org/abs/2307.11565)

    这项工作中提出了一种名为FMT的防御策略，通过检测并移除训练用于从输入中提取后门信息的后门特征图，从而有效防止后门攻击。

    

    深度神经网络广泛应用于许多关键应用，如自动驾驶车辆和医学诊断。然而，它们的安全性受到后门攻击的威胁，后门攻击是通过向特定训练数据中添加人工模式实现的。现有的防御策略主要集中在使用逆向工程来复现攻击者生成的后门触发器，并通过将触发器添加到输入中并使用真实标签微调模型来修复DNN模型。然而，一旦攻击者生成的触发器复杂而且不可见，防御者就无法成功复现触发器。因此，由于触发器没有被有效去除，DNN模型将无法被修复。在这项工作中，我们提出了特征图测试（FMT）方法。与现有的防御策略不同，它们专注于复现后门触发器，FMT试图检测训练用于从输入中提取后门信息的后门特征图。

    Deep neural networks have been widely used in many critical applications, such as autonomous vehicles and medical diagnosis. However, their security is threatened by backdoor attack, which is achieved by adding artificial patterns to specific training data. Existing defense strategies primarily focus on using reverse engineering to reproduce the backdoor trigger generated by attackers and subsequently repair the DNN model by adding the trigger into inputs and fine-tuning the model with ground-truth labels. However, once the trigger generated by the attackers is complex and invisible, the defender can not successfully reproduce the trigger. Consequently, the DNN model will not be repaired since the trigger is not effectively removed.  In this work, we propose Feature Map Testing~(FMT). Different from existing defense strategies, which focus on reproducing backdoor triggers, FMT tries to detect the backdoor feature maps, which are trained to extract backdoor information from the inputs. 
    
[^297]: 学习分散式部分可观察场均控制来实现人工集体行为

    Learning Decentralized Partially Observable Mean Field Control for Artificial Collective Behavior. (arXiv:2307.06175v1 [cs.LG])

    [http://arxiv.org/abs/2307.06175](http://arxiv.org/abs/2307.06175)

    本文提出了一种分散式部分可观察的场均控制模型（Dec-POMFC），用于解决多智能体强化学习中的分散化、部分可观察和可扩展性等挑战。该模型可将问题简化为可解决的单智能体马尔可夫决策过程，为实现人工集体行为提供了解决方案。

    

    近期，强化学习方法在各个领域取得了成功。然而，多智能体强化学习在分散化、部分可观察以及面对众多智能体时仍然存在挑战。与此同时，集体行为要解决上述问题，并且对于许多最前沿的应用，如活动物质物理、自组织系统、舆论动态以及生物或机器人群体来说非常重要。在这篇论文中，我们通过提出新的分散式部分可观察场均控制（Dec-POMFC）模型，实现了代理在部分信息下的分散行为，这是一类允许将问题简化为可解决的单智能体马尔可夫决策过程（MDP）的排列不变代理的广泛问题，以及单智能体强化学习解决方案。

    Recent reinforcement learning (RL) methods have achieved success in various domains. However, multi-agent RL (MARL) remains a challenge in terms of decentralization, partial observability and scalability to many agents. Meanwhile, collective behavior requires resolution of the aforementioned challenges, and remains of importance to many state-of-the-art applications such as active matter physics, self-organizing systems, opinion dynamics, and biological or robotic swarms. Here, MARL via mean field control (MFC) offers a potential solution to scalability, but fails to consider decentralized and partially observable systems. In this paper, we enable decentralized behavior of agents under partial information by proposing novel models for decentralized partially observable MFC (Dec-POMFC), a broad class of problems with permutation-invariant agents allowing for reduction to tractable single-agent Markov decision processes (MDP) with single-agent RL solution. We provide rigorous theoretical
    
[^298]: 方差-协方差正则化改进表示学习

    Variance-Covariance Regularization Improves Representation Learning. (arXiv:2306.13292v1 [cs.LG])

    [http://arxiv.org/abs/2306.13292](http://arxiv.org/abs/2306.13292)

    提出了方差-协方差正则化方法，旨在促进学习网络特征的多样性，改善表示学习和迁移学习的性能。

    

    迁移学习已成为机器学习领域的一个关键方法，能够将从一个领域获得的知识应用于提高后续任务的性能。然而，缺乏关于这些后续任务的足够信息，强有力的迁移学习方法要求在初始预训练阶段捕获各种特征。然而，最近的研究表明，在没有足够的正则化的情况下，网络往往会集中于主要减少预训练损失函数的特征。这种趋势可能导致不充分的特征学习和目标任务的受损泛化能力。为了解决这个问题，我们提出了方差-协方差正则化（VCR）技术，旨在促进学习网络特征的多样性。借鉴最近自监督学习方法的进展，我们的方法促进了表现出高方差和高相关性的学习表示。

    Transfer learning has emerged as a key approach in the machine learning domain, enabling the application of knowledge derived from one domain to improve performance on subsequent tasks. Given the often limited information about these subsequent tasks, a strong transfer learning approach calls for the model to capture a diverse range of features during the initial pretraining stage. However, recent research suggests that, without sufficient regularization, the network tends to concentrate on features that primarily reduce the pretraining loss function. This tendency can result in inadequate feature learning and impaired generalization capability for target tasks. To address this issue, we propose Variance-Covariance Regularization (VCR), a regularization technique aimed at fostering diversity in the learned network features. Drawing inspiration from recent advancements in the self-supervised learning approach, our approach promotes learned representations that exhibit high variance and 
    
[^299]: 行为博弈论的损失函数研究

    Loss Functions for Behavioral Game Theory. (arXiv:2306.04778v1 [cs.LG])

    [http://arxiv.org/abs/2306.04778](http://arxiv.org/abs/2306.04778)

    本文研究了行为博弈论家在损失函数选择上的差异，并构建了一组满足特定公理的损失函数集合，其中平方L2误差是实践中唯一可接受的损失函数，并建议行为博弈论家继续使用它。

    

    行为博弈论家们使用实验数据评估人类行为的预测模型，但是他们在损失函数的选择上存在很大的差异，错误率、负对数似然、交叉熵、Brier得分和L2误差都是常见的选择。本文试图提供一个原则性的答案，解决哪些损失函数适用于这个任务的问题，并规范化了认为损失函数应该满足的准则。我们构建了一组损失函数，称为“对角线有界Bregman散度”，满足所有这些公理，并包括平方L2误差。实际上，平方L2误差是相对常用的唯一可接受的损失函数。因此，我们建议行为博弈论家继续使用它。

    Behavioral game theorists all use experimental data to evaluate predictive models of human behavior. However, they differ greatly in their choice of loss function for these evaluations, with error rate, negative log-likelihood, cross-entropy, Brier score, and L2 error all being common choices. We attempt to offer a principled answer to the question of which loss functions make sense for this task, formalizing desiderata that we argue loss functions should satisfy. We construct a family of loss functions, which we dub "diagonal bounded Bregman divergences", that satisfy all of these axioms and includes the squared L2 error. In fact, the squared L2 error is the only acceptable loss that is relatively commonly used in practice; we thus recommend its continued use to behavioral game theorists.
    
[^300]: ConceptBed: 评估文本到图像扩散模型的概念学习能力

    ConceptBed: Evaluating Concept Learning Abilities of Text-to-Image Diffusion Models. (arXiv:2306.04695v1 [cs.CV])

    [http://arxiv.org/abs/2306.04695](http://arxiv.org/abs/2306.04695)

    本文提出ConceptBed数据集和评估指标CCD，用于评估文本到图像模型的概念学习和合成能力。

    

    理解视觉概念并从图像中复制和组合这些概念的能力是计算机视觉的一个核心目标。最近文本到图像（T2I）模型的进展使得通过学习大量图像及其描述来生成高清晰度和逼真的图像质量成为可能。然而，评估T2I模型的重点在于照片般的真实感和有限的视觉理解定性量度。为了量化T2I模型在学习和合成新的视觉概念方面的能力，我们引入了ConceptBed，一个包含284个独特视觉概念、5K个独特概念组合和33K个组合文本提示的大规模数据集。除了数据集，我们提出了一个评估指标Concept Confidence Deviation（CCD），它利用oracle概念分类器的置信度来衡量T2I生成器生成的概念与地面真实图像中包含的概念之间的对齐度。我们评估的视觉概念是对象或者...

    The ability to understand visual concepts and replicate and compose these concepts from images is a central goal for computer vision. Recent advances in text-to-image (T2I) models have lead to high definition and realistic image quality generation by learning from large databases of images and their descriptions. However, the evaluation of T2I models has focused on photorealism and limited qualitative measures of visual understanding. To quantify the ability of T2I models in learning and synthesizing novel visual concepts, we introduce ConceptBed, a large-scale dataset that consists of 284 unique visual concepts, 5K unique concept compositions, and 33K composite text prompts. Along with the dataset, we propose an evaluation metric, Concept Confidence Deviation (CCD), that uses the confidence of oracle concept classifiers to measure the alignment between concepts generated by T2I generators and concepts contained in ground truth images. We evaluate visual concepts that are either object
    
[^301]: 基于GCN可信度预测的协同移动群感知的高效招募策略

    Efficient Recruitment Strategy for Collaborative Mobile Crowd Sensing Based on GCN Trustworthiness Prediction. (arXiv:2306.04366v1 [cs.SI])

    [http://arxiv.org/abs/2306.04366](http://arxiv.org/abs/2306.04366)

    本文提出了一种基于GCN可信度预测的协同移动群感知的高效招募策略，通过捕获工人之间的非对称信任关系和工人能力来实现有效的任务分配，优于现有方法。

    

    协同移动群感知可以通过促进任务感知的团队合作来提高数据质量和覆盖范围，而工人招募则代表着一个复杂的多目标优化问题。现有策略主要关注工人本身的特征，忽略了工人之间的非对称信任关系，从而影响了任务效用评估的合理性。为解决这个问题，本文首先使用Mini-Batch K-Means聚类算法和边缘服务器来实现高效的分布式工人招募。利用历史数据和任务要求获得工人的能力类型和距离。使用工人社交网络中的信任导向图输入至图卷积网络（GCN）框架进行训练，捕获工人之间的非对称信任关系。通过工人之间的高信任值，防止CMCS场景下的隐私泄露。最终，利用预测的信任和工人能力构建了一个无向招募图，以实现有效的任务分配。实验结果表明，与现有方法相比，这种招募方法在招募准确度、任务完成时间和能量消耗方面表现优异。

    Collaborative Mobile Crowd Sensing (CMCS) enhances data quality and coverage by promoting teamwork in task sensing, with worker recruitment representing a complex multi-objective optimization problem. Existing strategies mainly focus on the characteristics of workers themselves, neglecting the asymmetric trust relationships between them, which affects the rationality of task utility evaluation. To address this, this paper first employs the Mini-Batch K-Means clustering algorithm and deploys edge servers to enable efficient distributed worker recruitment. Historical data and task requirements are utilized to obtain workers' ability types and distances. A trust-directed graph in the worker's social network is input into the Graph Convolutional Network (GCN) framework for training, capturing asymmetric trustworthiness between worker pairs. Privacy leakage is prevented in CMCS scenarios through high trust values between workers. Ultimately, an undirected recruitment graph is constructed us
    
[^302]: Quick-Tune：快速学习应该使用哪个预训练模型以及如何微调它

    Quick-Tune: Quickly Learning Which Pretrained Model to Finetune and How. (arXiv:2306.03828v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.03828](http://arxiv.org/abs/2306.03828)

    本文提出一种方法快速选择最佳的预训练模型和微调超参数，通过生成大规模元数据集并元学习多保真度性能预测器，并在学习新数据集时使用该预测器进行超参数优化，可以快速实现此目标。

    

    随着预训练模型数量的不断增加，机器学习从业者不断面临一个问题：应该使用哪个预训练模型以及该如何微调它以适应新的数据集。本文提出了一种方法，联合搜索最佳的预训练模型和微调超参数。我们的方法通过评估超过20k个超参数配置在87个数据集上微调24个预训练图像分类模型来生成大规模元数据集，并在其学习曲线上元学习多保真度性能预测器，以用于快速超参数优化。我们的实证研究表明，我们的方法能够快速选择一个准确的预训练模型并找到它的最佳超参数。

    With the ever-increasing number of pretrained models, machine learning practitioners are continuously faced with which pretrained model to use, and how to finetune it for a new dataset. In this paper, we propose a methodology that jointly searches for the optimal pretrained model and the hyperparameters for finetuning it. Our method transfers knowledge about the performance of many pretrained models with multiple hyperparameter configurations on a series of datasets. To this aim, we evaluated over 20k hyperparameter configurations for finetuning 24 pretrained image classification models on 87 datasets to generate a large-scale meta-dataset. We meta-learn a multi-fidelity performance predictor on the learning curves of this meta-dataset and use it for fast hyperparameter optimization on new datasets. We empirically demonstrate that our resulting approach can quickly select an accurate pretrained model for a new dataset together with its optimal hyperparameters.
    
[^303]: 人类对齐校准用于AI辅助决策制定

    Human-Aligned Calibration for AI-Assisted Decision Making. (arXiv:2306.00074v1 [cs.LG])

    [http://arxiv.org/abs/2306.00074](http://arxiv.org/abs/2306.00074)

    本文通过引入一种基于主动询问决策者个人偏好的置信度构造方法，解决了现有置信度对于决策者信任决策的不准确问题，从而提高决策的准确性和效率。

    

    当使用二元分类器提供决策支持时，它通常提供标签预测和置信度值。然后，决策者应使用置信度值来校准对预测的信任程度。在这种情况下，人们经常认为置信度值应对预测标签与实际标签匹配的概率进行良好校准的估计。然而，多条实证证据表明，决策者难以使用这些置信度值很好地确定何时信任预测。本文的目标首先是理解为什么，然后研究如何构建更有用的置信度值。我们首先认为，在广泛类的效用函数中，存在数据分布，对于这些分布，理性决策者通常难以使用以上置信度值发现最佳决策政策——最佳的决策者需要人类对齐。然后，我们引入了一种基于主动询问决策者他们在所面临的二元分类任务的决策上的个人偏好的新方法来构造置信度值。我们表明，该方法产生的置信度值比使用标准置信度度量导致更好的决策。

    Whenever a binary classifier is used to provide decision support, it typically provides both a label prediction and a confidence value. Then, the decision maker is supposed to use the confidence value to calibrate how much to trust the prediction. In this context, it has been often argued that the confidence value should correspond to a well calibrated estimate of the probability that the predicted label matches the ground truth label. However, multiple lines of empirical evidence suggest that decision makers have difficulties at developing a good sense on when to trust a prediction using these confidence values. In this paper, our goal is first to understand why and then investigate how to construct more useful confidence values. We first argue that, for a broad class of utility functions, there exist data distributions for which a rational decision maker is, in general, unlikely to discover the optimal decision policy using the above confidence values -- an optimal decision maker wou
    
[^304]: 个性化指导有助于知识蒸馏

    Tailoring Instructions to Student's Learning Levels Boosts Knowledge Distillation. (arXiv:2305.09651v1 [cs.CL])

    [http://arxiv.org/abs/2305.09651](http://arxiv.org/abs/2305.09651)

    本文提出了一种个性化指导的学习技术，称为LGTM，其利用蒸馏效应选择样本以增强学生的泛化能力，在GLUE基准测试的6个文本分类任务中优于10个常见的知识蒸馏基线算法。

    

    先前研究表明，能力超群的教师模型并不一定能够让学生水平得到提升，这凸显了当前教师培训实践和有效知识传授之间的不一致性。为了提高教师培训过程的指导效果，本文引入了蒸馏效应的概念，以确定每个训练样本对学生泛化能力的影响。我们提出了一种名为学好教师很重要（LGTM）的有效训练技术，以将蒸馏效应纳入教师的学习过程中。通过优先选择可能提升学生泛化能力的样本，我们的LGTM在GLUE基准测试的6个文本分类任务中优于10个常见的知识蒸馏基线算法。

    It has been commonly observed that a teacher model with superior performance does not necessarily result in a stronger student, highlighting a discrepancy between current teacher training practices and effective knowledge transfer. In order to enhance the guidance of the teacher training process, we introduce the concept of distillation influence to determine the impact of distillation from each training sample on the student's generalization ability. In this paper, we propose Learning Good Teacher Matters (LGTM), an efficient training technique for incorporating distillation influence into the teacher's learning process. By prioritizing samples that are likely to enhance the student's generalization ability, our LGTM outperforms 10 common knowledge distillation baselines on 6 text classification tasks in the GLUE benchmark.
    
[^305]: 学习增强的在线数据包调度算法

    Learning-Augmented Online Packet Scheduling with Deadlines. (arXiv:2305.07164v1 [cs.DS])

    [http://arxiv.org/abs/2305.07164](http://arxiv.org/abs/2305.07164)

    本研究提出了一种学习增强的在线数据包调度算法，能有效解决网络缓冲区管理问题。

    

    现代网络的目标是优先处理关键流量并有效地管理流量。这需要适当的缓冲区管理以防止丢失重要流量，同时最小化对非关键流量的影响。因此，算法的目标是控制每步要传输哪些数据包、哪些数据包要丢弃。在本研究中，我们提出了一种学习增强的在线数据包调度算法，并提供了一种新的算法框架来应对预测问题。我们证明了，当预测误差很小时，我们的算法会提高竞争比率，同时仍保持有界的竞争比率。

    The modern network aims to prioritize critical traffic over non-critical traffic and effectively manage traffic flow. This necessitates proper buffer management to prevent the loss of crucial traffic while minimizing the impact on non-critical traffic. Therefore, the algorithm's objective is to control which packets to transmit and which to discard at each step. In this study, we initiate the learning-augmented online packet scheduling with deadlines and provide a novel algorithmic framework to cope with the prediction. We show that when the prediction error is small, our algorithm improves the competitive ratio while still maintaining a bounded competitive ratio, regardless of the prediction error.
    
[^306]: 借助权重规范化的鲁棒性隐式正则化

    Robust Implicit Regularization via Weight Normalization. (arXiv:2305.05448v1 [cs.LG])

    [http://arxiv.org/abs/2305.05448](http://arxiv.org/abs/2305.05448)

    本文提出了使用权重规范化的梯度下降作为过度参数化模型的鲁棒隐式正则化方法，实现了对欧几里德范数较低的参数的隐式偏好，并建立了一个统一框架来解决线性模型和神经网络之间的隐式正则化隔阂。

    

    过度参数化的模型可能有许多插值解; 隐式正则化是指特定优化方法对众多插值解之一的隐含喜好。已经建立的工作表明，（随机）梯度下降在用于训练深度线性网络时倾向于具有低秩和/或稀疏解的隐式偏差，从某种程度上解释了为什么通过梯度下降训练的过度参数化神经网络模型在实践中具有良好的泛化性能。然而，现有的平方损失目标理论通常需要可训练权重的非常小的初始化，这与实践中为了更快的收敛和更好的泛化性能而初始化的更大规模的权重矛盾。在本文中，我们旨在通过纳入并分析采用权重规范化的梯度下降来弥合这一差距，其中权重向量以极坐标参数化，导致自然的权重归一化。我们表明，在过度参数化的线性回归模型的设置中，采用权重规范化的梯度下降对欧几里德范数较低的权重向量具有隐式正则化作用。此外，我们建立了一个新颖的统一框架，将权重规范化的隐式偏差与非线性模型的经验范数正则化联系起来，从而弥合了线性模型和神经网络之间的差距。

    Overparameterized models may have many interpolating solutions; implicit regularization refers to the hidden preference of a particular optimization method towards a certain interpolating solution among the many. A by now established line of work has shown that (stochastic) gradient descent tends to have an implicit bias towards low rank and/or sparse solutions when used to train deep linear networks, explaining to some extent why overparameterized neural network models trained by gradient descent tend to have good generalization performance in practice. However, existing theory for square-loss objectives often requires very small initialization of the trainable weights, which is at odds with the larger scale at which weights are initialized in practice for faster convergence and better generalization performance. In this paper, we aim to close this gap by incorporating and analyzing gradient descent with weight normalization, where the weight vector is reparamterized in terms of polar
    
[^307]: 开放集合学习的新视角：解锁开放集合的力量

    Unlocking the Power of Open Set : A New Perspective for Open-set Noisy Label Learning. (arXiv:2305.04203v1 [cs.LG])

    [http://arxiv.org/abs/2305.04203](http://arxiv.org/abs/2305.04203)

    本文提出了一种新的两步对比学习方法CECL，通过利用开放集合示例的有用信息来处理两种类型的标签噪声。该方法在几个基准数据集上得到了验证。

    

    学习嘈杂的数据一直受到人们的关注，其中大多数方法集中在封闭集的标签噪声上。然而，在现实世界中更常见的情况是同时存在开放集合和封闭集合的噪声。现有方法通常通过为每种类型设计特定的策略来区分和处理这两种类型的标签噪声。然而，在许多现实世界的情况下，识别开放集合示例可能是具有挑战性的，特别是当数据集已经严重损坏时。本文对模型面对开放集合示例时的行为进行了探索，并发现部分开放集合示例逐渐融入某些已知类别，这有利于已知类别的分离。在这种现象的推动下，我们提出了一种新的两步对比学习方法CECL，通过利用开放集合示例的有用信息来处理两种类型的标签噪声。具体而言，我们在对比学习框架中将一些开放集合示例作为负例，可以有效地增强模型对开放集合噪声的鲁棒性。在几个基准数据集上进行的实验证明了我们方法处理开放集合和封闭集合标签噪声的有效性。

    Learning from noisy data has attracted much attention, where most methods focus on closed-set label noise. However, a more common scenario in the real world is the presence of both open-set and closed-set noise. Existing methods typically identify and handle these two types of label noise separately by designing a specific strategy for each type. However, in many real-world scenarios, it would be challenging to identify open-set examples, especially when the dataset has been severely corrupted. Unlike the previous works, we explore how models behave when faced open-set examples, and find that a part of open-set examples gradually get integrated into certain known classes, which is beneficial for the seperation among known classes. Motivated by the phenomenon, in this paper, we propose a novel two-step contrastive learning method called CECL, which aims to deal with both types of label noise by exploiting the useful information of open-set examples. Specifically, we incorporate some ope
    
[^308]: 学习动作嵌入以进行离线评估

    Learning Action Embeddings for Off-Policy Evaluation. (arXiv:2305.03954v1 [cs.LG])

    [http://arxiv.org/abs/2305.03954](http://arxiv.org/abs/2305.03954)

    本论文探讨了从记录数据中学习动作嵌入，以减少在大型动作空间中反向倾向评分（IPS）估计器的方差，同时提高离线评估的准确性。

    

    离线评估（OPE）方法使我们能够使用由不同策略收集的记录数据来计算策略的预期奖励。 OPE是运行昂贵的在线A / B测试的可行选择：它可以加快新策略的开发，并降低向客户暴露次优治疗的风险。然而，当动作数量很大或记录策略未充分探索某些操作时，基于反向倾向评分（IPS）的现有估计器可能具有高甚至无限方差。Saito和Joachims提出使用动作嵌入的边际IPS（MIPS），从而在大型动作空间中降低IPS的方差。 MIPS假设从业者可以定义良好的动作嵌入，但在许多实际应用中很难做到这一点。在这项工作中，我们探讨从记录数据中学习动作嵌入。特别地，我们使用已经训练好的奖励模型的中间输出来定义动作嵌入，然后将其用于MIPS估计器中。

    Off-policy evaluation (OPE) methods allow us to compute the expected reward of a policy by using the logged data collected by a different policy. OPE is a viable alternative to running expensive online A/B tests: it can speed up the development of new policies, and reduces the risk of exposing customers to suboptimal treatments. However, when the number of actions is large, or certain actions are under-explored by the logging policy, existing estimators based on inverse-propensity scoring (IPS) can have a high or even infinite variance. Saito and Joachims (arXiv:2202.06317v2 [cs.LG]) propose marginalized IPS (MIPS) that uses action embeddings instead, which reduces the variance of IPS in large action spaces. MIPS assumes that good action embeddings can be defined by the practitioner, which is difficult to do in many real-world applications. In this work, we explore learning action embeddings from logged data. In particular, we use intermediate outputs of a trained reward model to defin
    
[^309]: 考虑多轮对话上下文的领域外意图检测

    Out-of-Domain Intent Detection Considering Multi-turn Dialogue Contexts. (arXiv:2305.03237v1 [cs.CL])

    [http://arxiv.org/abs/2305.03237](http://arxiv.org/abs/2305.03237)

    本文提出了一个上下文感知的OOD意图检测框架（Caro），用于模拟OOD意图检测任务中的多轮对话上下文，并在提取稳健的表示时删除与意图检测无关的多余信息。Caro在多个标准数据集上表现出最先进的性能，并超越了先前方法。

    

    领域外（OOD）意图检测对于实用的对话系统非常重要，通常需要考虑多轮对话上下文。然而，大多数先前的OOD意图检测方法仅限于单轮对话。在本文中，我们介绍了一个上下文感知的OOD意图检测（Caro）框架，用于对OOD意图检测任务中的多轮上下文进行建模。具体地，我们遵循信息瓶颈原则从多轮对话上下文中提取稳健的表示。每个输入样本构建了两个不同的视角，使用多视图信息瓶颈损失删除与意图检测无关的多余信息。此外，我们还探索了在Caro中利用未标记的数据。引入了一个两阶段训练过程来从这些未标记的数据中挖掘OOD样本，并使用自举方法用这些OOD样本来训练生成的模型。全面的实验表明，Caro在OOD意图检测任务的几个基准数据集上建立了最先进的性能，并超越了仅考虑单轮上下文的先前方法。

    Out-of-Domain (OOD) intent detection is vital for practical dialogue systems, and it usually requires considering multi-turn dialogue contexts. However, most previous OOD intent detection approaches are limited to single dialogue turns. In this paper, we introduce a context-aware OOD intent detection (Caro) framework to model multi-turn contexts in OOD intent detection tasks. Specifically, we follow the information bottleneck principle to extract robust representations from multi-turn dialogue contexts. Two different views are constructed for each input sample and the superfluous information not related to intent detection is removed using a multi-view information bottleneck loss. Moreover, we also explore utilizing unlabeled data in Caro. A two-stage training process is introduced to mine OOD samples from these unlabeled data, and these OOD samples are used to train the resulting model with a bootstrapping approach. Comprehensive experiments demonstrate that Caro establishes state-of-
    
[^310]: 中文LLaMA和Alpaca的高效有效的文本编码

    Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca. (arXiv:2304.08177v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.08177](http://arxiv.org/abs/2304.08177)

    这篇论文提出了一种方法，通过扩展LLaMA现有的词汇表，增加了20,000个中文标记，从而提高其编码效率和对汉语语义的理解能力，并在中文数据上进行二次预训练和精细调整模型，以改善LLaMA对中文的理解和生成能力。

    

    大型语言模型（LLM）已经彻底改变了自然语言处理研究，并显示出朝着人工通用智能（AGI）的有希望的进展。然而，训练和部署LLM的高成本对透明、可访问的学术研究构成了重大障碍。在这篇论文中，我们提出了一种方法，通过扩展LLaMA现有的词汇表，增加了20,000个中文标记，从而提高其编码效率和对汉语语义的理解能力，并在中文数据上进行二次预训练和精细调整模型，以便更好地理解和生成中文文本及其指令。

    Large Language Models (LLMs), such as ChatGPT and GPT-4, have dramatically transformed natural language processing research and shown promising strides towards Artificial General Intelligence (AGI). Nonetheless, the high costs associated with training and deploying LLMs present substantial obstacles to transparent, accessible academic research. While several large language models, such as LLaMA, have been open-sourced by the community, these predominantly focus on English corpora, limiting their usefulness for other languages. In this paper, we propose a method to augment LLaMA with capabilities for understanding and generating Chinese text and its ability to follow instructions. We achieve this by extending LLaMA's existing vocabulary with an additional 20,000 Chinese tokens, thereby improving its encoding efficiency and semantic understanding of Chinese. We further incorporate secondary pre-training using Chinese data and fine-tune the model with Chinese instruction datasets, signifi
    
[^311]: CMOS + 随机纳米磁体：概率推理与学习异构计算机

    CMOS + stochastic nanomagnets: heterogeneous computers for probabilistic inference and learning. (arXiv:2304.05949v1 [cond-mat.mes-hall])

    [http://arxiv.org/abs/2304.05949](http://arxiv.org/abs/2304.05949)

    本文展示了如何将基于随机磁隧道结（sMTJ）的概率比特（p位）与多功能可编程门阵列（FPGA）相结合，设计出一种能源高效的异构CMOS + X（X = sMTJ）原型，其成功地执行了概率推理和异步Boltzmann学习。

    

    随着摩尔定律的放缓，利用新兴的纳米技术（X）增强互补金属氧化物半导体（CMOS）晶体管变得越来越重要。本文展示了如何将基于随机磁隧道结（sMTJ）的概率比特（p位）与多功能可编程门阵列（FPGA）相结合，设计出一种能源高效的异构CMOS + X（X = sMTJ）原型。尽管sMTJs设备间存在差异，我们的异构计算机成功地执行了概率推理和异步Boltzmann学习。使用CMOS预测流程设计套件（PDK）进行全面比较，数字CMOS-based p-bits模拟高质量随机性需要超过10,000个晶体管，每生成一个随机数的能量比使用只消耗2fJ的sMTJ-based p-bits高约两个数量级。我们的方法的缩放和集成版本可以显着推进概率性的推理。

    With the slowing down of Moore's law, augmenting complementary-metal-oxide semiconductor (CMOS) transistors with emerging nanotechnologies (X) is becoming increasingly important. In this paper, we demonstrate how stochastic magnetic tunnel junction (sMTJ)-based probabilistic bits, or p-bits, can be combined with versatile Field Programmable Gate Arrays (FPGA) to design an energy-efficient, heterogeneous CMOS + X (X = sMTJ) prototype. Our heterogeneous computer successfully performs probabilistic inference and asynchronous Boltzmann learning despite device-to-device variations in sMTJs. A comprehensive comparison using a CMOS predictive process design kit (PDK) reveals that digital CMOS-based p-bits emulating high-quality randomness use over 10,000 transistors with the energy per generated random number being roughly two orders of magnitude greater than the sMTJ-based p-bits that dissipate only 2 fJ. Scaled and integrated versions of our approach can significantly advance probabilistic 
    
[^312]: 使用语言反馈规模化训练语言模型

    Training Language Models with Language Feedback at Scale. (arXiv:2303.16755v1 [cs.CL])

    [http://arxiv.org/abs/2303.16755](http://arxiv.org/abs/2303.16755)

    本文提出一种新方法，即利用更丰富的语言反馈进行模仿学习，通过三个迭代步骤对语言模型进行训练以生成更符合人类偏好的输出。

    

    预训练的语言模型经常生成不符合人类偏好的输出，例如有害的文本或事实不正确的摘要。最近的研究尝试通过学习一种简单的人类反馈形式（即模型生成输出之间的比较）来解决这些问题。但是，比较反馈只能传达有限的关于人类偏好的信息。在本文中，我们介绍了一种新的方法——使用语言反馈进行模仿学习（ILF），它利用了更丰富的语言反馈。ILF由三个迭代步骤组成：第一步，根据输入，初始LM输出和反馈对语言模型进行调节以生成改进。第二步，选择最多反馈的改进。第三步，微调语言模型，以最大化在给定输入的情况下选择的改进的可能性。我们在理论上证明了ILF可以被看作是贝叶斯推断，类似于从人类反馈中进行强化学习。我们还评估了ILF在各种基准测试中的性能。

    Pretrained language models often generate outputs that are not in line with human preferences, such as harmful text or factually incorrect summaries. Recent work approaches the above issues by learning from a simple form of human feedback: comparisons between pairs of model-generated outputs. However, comparison feedback only conveys limited information about human preferences. In this paper, we introduce Imitation learning from Language Feedback (ILF), a new approach that utilizes more informative language feedback. ILF consists of three steps that are applied iteratively: first, conditioning the language model on the input, an initial LM output, and feedback to generate refinements. Second, selecting the refinement incorporating the most feedback. Third, finetuning the language model to maximize the likelihood of the chosen refinement given the input. We show theoretically that ILF can be viewed as Bayesian Inference, similar to Reinforcement Learning from human feedback. We evaluate
    
[^313]: 利用自然语言反馈进行代码生成的改进

    Improving Code Generation by Training with Natural Language Feedback. (arXiv:2303.16749v1 [cs.SE])

    [http://arxiv.org/abs/2303.16749](http://arxiv.org/abs/2303.16749)

    该论文提出了一种新算法ILF，通过从自然语言反馈中进行学习来显著提高代码生成模型的性能，即使只有少量反馈，也可以获得很好的效果。

    

    预先训练好的大型语言模型（LLM）在推理时使用自然语言反馈的潜力是最近的一个令人兴奋的发展。我们在此基础上提出一种名为Language Feedback（ILF）的算法，用于从自然语言反馈中进行学习。ILF在训练期间仅需要少量的人工编写反馈，并且在测试时不需要相同的反馈，因此使用起来既方便又高效。此外，我们进一步证明ILF可以被视为最小化与基准分布的KL散度的一种形式，并在神经程序合成任务上进行了概念验证。我们使用ILF在Mostly Basic Python Problems(MBPP)基准测试上将Codegen-Mono 6.1B模型的pass @ 1覆盖率相对提高了38%（绝对提高了10%），胜过了在MBPP上微调和在人类修复的程序上微调的模型。总的来说，我们的结果表明，即使只有少量反馈，从人类编写的自然语言反馈中进行学习也可以显著改进代码生成模型。

    The potential for pre-trained large language models (LLMs) to use natural language feedback at inference time has been an exciting recent development. We build upon this observation by formalizing an algorithm for learning from natural language feedback at training time instead, which we call Imitation learning from Language Feedback (ILF). ILF requires only a small amount of human-written feedback during training and does not require the same feedback at test time, making it both user-friendly and sample-efficient. We further show that ILF can be seen as a form of minimizing the KL divergence to the ground truth distribution and demonstrate a proof-of-concept on a neural program synthesis task. We use ILF to improve a Codegen-Mono 6.1B model's pass@1 rate by 38% relative (and 10% absolute) on the Mostly Basic Python Problems (MBPP) benchmark, outperforming both fine-tuning on MBPP and fine-tuning on repaired programs written by humans. Overall, our results suggest that learning from h
    
[^314]: 对比学习是相似性图谱上的谱聚类

    Contrastive Learning Is Spectral Clustering On Similarity Graph. (arXiv:2303.15103v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.15103](http://arxiv.org/abs/2303.15103)

    本文通过证明标准InfoNCE损失下的对比学习等同于相似性图上的谱聚类，揭示了这种方法的内在等价性，并进一步将这种分析扩展到CLIP模型，提出新的核混合损失函数。

    

    对比学习是一种强大的自监督学习方法，但我们对其运作原理和原因的理论理解有限。本文通过证明标准InfoNCE损失下的对比学习等同于相似性图上的谱聚类，揭示了这种方法的内在等价性。利用这种等价性作为基石，我们将分析扩展到CLIP模型，并严格描述多模态对象如何被嵌入到一起。在理论洞见的推动下，我们引入了核混合损失，结合新颖的核函数，在多个视觉数据集上优于标准高斯核。

    Contrastive learning is a powerful self-supervised learning method, but we have a limited theoretical understanding of how it works and why it works. In this paper, we prove that contrastive learning with the standard InfoNCE loss is equivalent to spectral clustering on the similarity graph. Using this equivalence as the building block, we extend our analysis to the CLIP model and rigorously characterize how similar multi-modal objects are embedded together. Motivated by our theoretical insights, we introduce the kernel mixture loss, incorporating novel kernel functions that outperform the standard Gaussian kernel on several vision datasets.
    
[^315]: EdgeServe:一种为去中心化预测而设计的执行层

    EdgeServe: An Execution Layer for Decentralized Prediction. (arXiv:2303.08028v1 [cs.DB])

    [http://arxiv.org/abs/2303.08028](http://arxiv.org/abs/2303.08028)

    EdgeServe 是一种为去中心化预测而设计的机器学习系统，通过低延迟的消息代理程序将数据路由到可以提供预测的节点。它具有一系列新颖的优化，可以在计算、通信和准确性之间进行折衷。在多摄像机物体跟踪，网络入侵检测和人类活动识别等三个去中心化预测任务中，EdgeServe 展现了很好的性能。

    

    机器学习任务的相关特征可能来自于网络中不同节点收集的数据源。这种问题被称之为去中心化预测，并在数据路由、计算布局和时间同步方面带来了许多有趣的系统挑战。本文提出了一种名为EdgeServe的机器学习系统，可以为去中心化预测提供服务。 EdgeServe 依赖于一个低延迟的消息代理程序，通过网络路由数据到可以提供预测的节点。EdgeServe 依赖一系列新颖的优化，可以在计算、通信和准确性之间进行折衷。我们在三个去中心化预测任务中评估了EdgeServe：（1）多摄像机物体跟踪，（2）网络入侵检测和（3）人类活动识别。

    The relevant features for a machine learning task may be aggregated from data sources collected on different nodes in a network. This problem, which we call decentralized prediction, creates a number of interesting systems challenges in managing data routing, placing computation, and time-synchronization. This paper presents EdgeServe, a machine learning system that can serve decentralized predictions. EdgeServe relies on a low-latency message broker to route data through a network to nodes that can serve predictions. EdgeServe relies on a series of novel optimizations that can tradeoff computation, communication, and accuracy. We evaluate EdgeServe on three decentralized prediction tasks: (1) multi-camera object tracking, (2) network intrusion detection, and (3) human activity recognition.
    
[^316]: 原型分析++：重新思考初始化策略

    Archetypal Analysis++: Rethinking the Initialization Strategy. (arXiv:2301.13748v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13748](http://arxiv.org/abs/2301.13748)

    本文提出了一种针对原型分析的概率初始化策略 AA ++，能够在13个不同大小和维度的实际数据集上表现优异。

    

    原型分析是一种带有凸性约束的矩阵分解方法。由于局部最小值的存在，好的初始化非常重要，但是经常使用的初始化方法要么产生次优的起始点，要么容易陷入不良的局部最小值。在本文中，我们提出了原型分析++（AA ++），这是一种针对原型分析的概率初始化策略，它根据点对目标的影响顺序地进行采样，类似于$k$-means++。实际上，我们认为$k$-means++已近逼近了所提出的初始化方法。此外，我们建议将$k$-means++的高效蒙特卡罗近似方法应用于AA++。在对13个不同大小和维度的实际数据集进行广泛的实证评估并考虑两个预处理策略的情况下，我们表明AA++几乎总是优于所有的基线方法，包括最常用的方法。

    Archetypal analysis is a matrix factorization method with convexity constraints. Due to local minima, a good initialization is essential, but frequently used initialization methods yield either sub-optimal starting points or are prone to get stuck in poor local minima. In this paper, we propose archetypal analysis++ (AA++), a probabilistic initialization strategy for archetypal analysis that sequentially samples points based on their influence on the objective, similar to $k$-means++. In fact, we argue that $k$-means++ already approximates the proposed initialization method. Furthermore, we suggest to adapt an efficient Monte Carlo approximation of $k$-means++ to AA++. In an extensive empirical evaluation of 13 real-world data sets of varying sizes and dimensionalities and considering two pre-processing strategies, we show that AA++ nearly always outperforms all baselines, including the most frequently used ones.
    
[^317]: 无先验因果学习

    Zero-shot causal learning. (arXiv:2301.12292v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12292](http://arxiv.org/abs/2301.12292)

    无先验因果学习是一个解决预测新型干预措施个性化影响的框架，并通过元学习对任务的处理达成了目的，能够将干预措施的知识传输到未见过的干预措施中，并在合成和真实数据集上表现出了优越性能。

    

    在个性化医疗、公共政策和在线营销等领域，预测不同干预措施对特定个体的因果影响非常重要。预测现有干预措施的影响有许多方法，这些方法基于接受过干预措施的个体的历史数据。然而，在许多场景中，预测新型干预措施的影响也很重要，这些方法无法解决。在这里，我们考虑了无先验因果学习：预测新型干预措施的个性化影响。我们提出了CaML，这是一个因果元学习框架，它将每个干预措施的个性化预测效果作为一个任务来进行处理。CaML在数千个任务中训练单一的元模型，每个任务都是通过抽样生成一个干预措施及其接收者和非接收者来构建的。通过利用干预信息（例如，药物的属性）和个体特征（例如，特定个体的医疗记录），CaML学习如何将已观察到的干预措施的知识有效地传输给未见过的干预措施。我们在合成和真实数据集上展示了我们方法的有效性，展示了该方法具有推广到未见过干预措施并胜过现有方法的能力。

    Predicting how different interventions will causally affect a specific individual is important in a variety of domains such as personalized medicine, public policy, and online marketing. There are a large number of methods to predict the effect of an existing intervention based on historical data from individuals who received it. However, in many settings it is important to predict the effects of novel interventions (\emph{e.g.}, a newly invented drug), which these methods do not address. Here, we consider zero-shot causal learning: predicting the personalized effects of a novel intervention. We propose CaML, a causal meta-learning framework which formulates the personalized prediction of each intervention's effect as a task. CaML trains a single meta-model across thousands of tasks, each constructed by sampling an intervention, along with its recipients and nonrecipients. By leveraging both intervention information (\emph{e.g.}, a drug's attributes) and individual features~(\emph{e.g.
    
[^318]: 对联邦学习系统的隐蔽信道攻击

    Covert Channel Attack to Federated Learning Systems. (arXiv:2104.10561v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2104.10561](http://arxiv.org/abs/2104.10561)

    这篇论文介绍了一种针对联邦学习系统的新型攻击模型，通过在联邦训练期间污染全局模型实现隐蔽通信，而不影响模型性能。

    

    联邦学习通过在众多边缘客户端之间分布模型训练，超越了传统的集中式机器学习。这些客户端合作训练一个全局模型，而不会泄露他们的本地私有训练数据。然后，全局模型在所有参与者之间共享，用于本地预测。本文提出了一个新颖的攻击模型，旨在将联邦学习系统转化为隐蔽信道，实现隐秘的通信基础设施。主要思想是，在联邦训练期间，恶意发送者可以通过提交专门构造的样本来污染全局模型。虽然模型污染对其他参与者影响几乎可以忽略不计，也不会改变整体模型性能，但它可以被恶意接收者观察到，并用于传输一个比特位。

    Federated learning (FL) goes beyond traditional, centralized machine learning by distributing model training among a large collection of edge clients. These clients cooperatively train a global, e.g., cloud-hosted, model without disclosing their local, private training data. The global model is then shared among all the participants which use it for local predictions. In this paper, we put forward a novel attacker model aiming at turning FL systems into covert channels to implement a stealth communication infrastructure. The main intuition is that, during federated training, a malicious sender can poison the global model by submitting purposely crafted examples. Although the effect of the model poisoning is negligible to other participants, and does not alter the overall model performance, it can be observed by a malicious receiver and used to transmit a single bit.
    
[^319]: 高效的数据驱动优化与有噪数据

    Efficient Data-Driven Optimization with Noisy Data. (arXiv:2102.04363v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2102.04363](http://arxiv.org/abs/2102.04363)

    本文研究了在已知噪声源的情况下的数据驱动处方问题，并导出了在这个噪声情况下的高效数据驱动形式，并指出它们具有熵最优传输解释。

    

    在决策者面临的实际情况下，大多数情况下，可用于决策的数据都受到一定程度的测量噪声的影响。本文研究了在已知噪声源的情况下的数据驱动处方问题，并导出了在这个噪声情况下的高效数据驱动形式，并指出它们具有熵最优传输解释。最后，通过利用Strassen的经典表示结果，我们证明了这些有效的鲁棒形式在几种有趣的设置下是可行的。

    Classical Kullback-Leibler or entropic distances are known to enjoy certain desirable statistical properties in the context of decision-making with noiseless data. However, in most practical situations the data available to a decision maker is subject to a certain amount of measurement noise. We hence study here data-driven prescription problems in which the data is corrupted by a known noise source. We derive efficient data-driven formulations in this noisy regime and indicate that they enjoy an entropic optimal transport interpretation. Finally, we show that these efficient robust formulations are tractable in several interesting settings by exploiting a classical representation result by Strassen.
    

