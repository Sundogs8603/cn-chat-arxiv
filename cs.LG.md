# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [LagrangeBench: A Lagrangian Fluid Mechanics Benchmarking Suite.](http://arxiv.org/abs/2309.16342) | LagrangeBench是第一个针对拉格朗日粒子问题的基准测试套件，提供了七个新的流体力学数据集（包括不同维度和物理特性），以及高效的API和已建立的图神经网络的JAX实现。 |
| [^2] | [ADGym: Design Choices for Deep Anomaly Detection.](http://arxiv.org/abs/2309.15376) | ADGym是一款针对深度异常检测的设计选择的综合评估和自动选择平台。 |
| [^3] | [Tempo Adaption in Non-stationary Reinforcement Learning.](http://arxiv.org/abs/2309.14989) | 该论文解决了非平稳强化学习中"时间同步"问题，通过考虑墙钟时间而不是情节进展来实现对环境变化的适应。 |
| [^4] | [ODE-based Recurrent Model-free Reinforcement Learning for POMDPs.](http://arxiv.org/abs/2309.14078) | 本论文提出了一种基于ODE的循环模型结合无模型强化学习来解决部分可观察的马尔可夫决策过程（POMDPs）。实验结果表明该方法在PO连续控制和元强化学习任务中表现出了良好的效果，并且对于不规则观察具有鲁棒性。 |
| [^5] | [FeCAM: Exploiting the Heterogeneity of Class Distributions in Exemplar-Free Continual Learning.](http://arxiv.org/abs/2309.14062) | 本文针对免去样本的增量式学习（CIL）中的异质性类别分布问题，使用原型网络和改进的各向异性马哈拉诺比斯距离进行特征分类和建模，有效解决了非恒定数据学习中的特征分布异质性挑战。 |
| [^6] | [Physics-Driven ML-Based Modelling for Correcting Inverse Estimation.](http://arxiv.org/abs/2309.13985) | 本文提出了一种基于物理驱动的机器学习模型，用于检测和纠正在科学和工程领域中的状态估计问题。其中，该模型通过优化算法来减少模拟成本，并通过两个生成模型近似候选状态的概率分布，以提高估计的准确性和效率。 |
| [^7] | [Provable Training for Graph Contrastive Learning.](http://arxiv.org/abs/2309.13944) | 图对比学习中，我们发现训练存在不平衡的问题，为此我们提出了“节点紧凑性”度量来指导训练。 |
| [^8] | [Robust Distributed Learning: Tight Error Bounds and Breakdown Point under Data Heterogeneity.](http://arxiv.org/abs/2309.13591) | 在数据异构性下，鲁棒性分布式学习算法在更实际的数据异构性模型下得到了改进，在学习误差下界和断点分析方面取得了新的结果。 |
| [^9] | [Turbulence in Focus: Benchmarking Scaling Behavior of 3D Volumetric Super-Resolution with BLASTNet 2.0 Data.](http://arxiv.org/abs/2309.13457) | 本研究提出了BLASTNet 2.0，包含三维高保真压缩湍流流动模拟数据，通过对五种深度学习方法的基准测试和神经缩放分析，揭示了模型规模、成本和架构对预测性能的影响。 |
| [^10] | [Posterior Contraction Rates for Mat\'ern Gaussian Processes on Riemannian Manifolds.](http://arxiv.org/abs/2309.10918) | 该论文研究了定义在紧致Riemannian流形上的内在Matern高斯过程和外在过程之间的收缩速率，并发现它们的速率在适当匹配平滑参数的情况下是相等的。 |
| [^11] | [Clustered Multi-Agent Linear Bandits.](http://arxiv.org/abs/2309.08710) | 本文研究了集群化的多智能体线性赌博机问题，提出了一种新颖的算法，通过智能体之间的协作来加速优化问题。通过理论分析和实证评估，证明了算法在遗憾最小化和聚类质量上的有效性。 |
| [^12] | [Virchow: A Million-Slide Digital Pathology Foundation Model.](http://arxiv.org/abs/2309.07778) | Virchow是一个数百万参数的深度神经网络基础模型，通过在数百万张全数字病理学切片图像上进行自监督学习训练，有效解决了计算病理学任务中数据不足的问题，并在多个下游任务上超越了最先进的系统。 |
| [^13] | [Bayesian Dynamic DAG Learning: Application in Discovering Dynamic Effective Connectome of Brain.](http://arxiv.org/abs/2309.07080) | 本文介绍了贝叶斯动态有向无环图学习方法（BDyMA）来解决在发现大脑动态效应连接组中的两个主要挑战。该方法通过无约束框架实现更准确的结果和更稀疏的网络结构，使其特别适用于提取动态效应连接组。 |
| [^14] | [ATTA: Anomaly-aware Test-Time Adaptation for Out-of-Distribution Detection in Segmentation.](http://arxiv.org/abs/2309.05994) | 本文提出了一种双层场外分布检测框架来处理领域转移和语义转移问题。该框架利用全局低级特征和密集高级特征图来适应模型到未见领域，并增强模型在检测新类别方面的能力。 |
| [^15] | [Learning Zero-Sum Linear Quadratic Games with Improved Sample Complexity.](http://arxiv.org/abs/2309.04272) | 这项研究提出了改进样本复杂性的零和线性二次博弈，并发现了自然策略梯度方法的隐式正则化属性。在无模型参数知识的情况下，他们还提出了第一个多项式样本复杂性算法来达到Nash均衡。 |
| [^16] | [Pareto Frontiers in Neural Feature Learning: Data, Compute, Width, and Luck.](http://arxiv.org/abs/2309.03800) | 本研究研究了深度学习算法设计中的微妙选择，特别关注计算统计差距。通过理论和实验，发现稀疏初始化和增加网络宽度可以提高样本效率，并且合成稀疏奇偶任务可以作为真实问题的代理。 |
| [^17] | [Superclustering by finding statistically significant separable groups of optimal gaussian clusters.](http://arxiv.org/abs/2309.02623) | 本文提出了一种算法，通过找到统计显著可分离的最佳高斯簇的分组，实现超聚类。算法具有三个阶段，包括表示数据集为高斯混合分布-簇、使用马氏距离估计簇之间的距离和大小以及将簇组合为超簇。算法的创新点在于引入了矩阵质量准则，并通过自动选择合适的统计显著性水平来确定最佳超簇数量。 |
| [^18] | [Effective Multi-Graph Neural Networks for Illicit Account Detection on Cryptocurrency Transaction Networks.](http://arxiv.org/abs/2309.02460) | 本文介绍了一种新颖的多图神经网络模型DIAM，用于有效地检测加密货币交易网络上的非法账户。该模型通过自动学习节点表示并保留平行边的内在交易模式，在大型交易网络中取得了良好的效果。 |
| [^19] | [Stabilize to Act: Learning to Coordinate for Bimanual Manipulation.](http://arxiv.org/abs/2309.01087) | 通过借鉴人类的角色分配方法，我们提出了一种稳定行动的框架，其中一个手臂用于固定物体，另一个手臂用于执行任务。通过学习到的稳定分类器和行动策略，我们实现了这一框架并对其进行了评估。 |
| [^20] | [Iterative Multi-granular Image Editing using Diffusion Models.](http://arxiv.org/abs/2309.00613) | 本研究提出了一种名为EMILIE的迭代多粒度图像编辑器，通过重新利用预训练的扩散模型来实现迭代编辑，并引入梯度控制操作来实现对图像编辑的粒度控制。 |
| [^21] | [Diffusion Variational Autoencoder for Tackling Stochasticity in Multi-Step Regression Stock Price Prediction.](http://arxiv.org/abs/2309.00073) | 本论文提出了一种利用扩散变分自编码器来解决多步回归股票价格预测中的随机性问题的方法。 |
| [^22] | [NAS-X: Neural Adaptive Smoothing via Twisting.](http://arxiv.org/abs/2308.14864) | NAS-X是一种基于扭曲的神经自适应平滑方法，通过重新加权的唤醒-睡眠算法来学习和推断顺序潜变量模型，并在离散和连续任务中取得了优于先前方法的推断和参数恢复效果。 |
| [^23] | [Towards Accelerated Model Training via Bayesian Data Selection.](http://arxiv.org/abs/2308.10544) | 通过对数据的贝叶斯选择和零样本预测器的利用，提出了一种更合理、高效且易于实现的模型训练方法，动态选取地在线批量训练样本，减少噪声和不平衡带来的影响。 |
| [^24] | [An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM.](http://arxiv.org/abs/2308.06828) | 本研究提出了一种集成Electra Transformer、GloVe和LSTM模型的创新问题分类方法，通过在TREC数据集上进行严格测试，证明了融合不同技术可以获得更优越的结果。 |
| [^25] | [A deep learning framework for multi-scale models based on physics-informed neural networks.](http://arxiv.org/abs/2308.06672) | 本文提出了一个基于物理约束神经网络的新框架，用于解决具有不同数量级损失项的多尺度问题。通过重新构建损失函数，并应用不同数量的幂运算，使各个损失项在数量级上大致相等。同时提供了一种分组正则化策略来应对在不同子领域中显着变化的问题。 |
| [^26] | [Toward a Better Understanding of Loss Functions for Collaborative Filtering.](http://arxiv.org/abs/2308.06091) | 现有研究已经表明，通过改进对齐和均匀性设计的损失函数可以实现显著的性能提升。本文提出了一种新的损失函数，称为MAWU，它考虑了数据集的独特模式。 |
| [^27] | [Homophily-enhanced Structure Learning for Graph Clustering.](http://arxiv.org/abs/2308.05309) | 提出了一种名为HoLe的方法，通过在图结构中增强同类性可以显著改善图聚类任务的性能。 |
| [^28] | [Deep Learning based Image Watermarking: A Brief Survey.](http://arxiv.org/abs/2308.04603) | 该论文调查了基于深度学习的图像水印技术的最新研究进展，将其分为嵌入器-提取器联合训练、深度网络作为特征变换和混合方案。对每个类别的研究方向进行了分析和总结，并讨论了未来的研究方向。 |
| [^29] | [Toward Transparent Sequence Models with Model-Based Tree Markov Model.](http://arxiv.org/abs/2307.15367) | 本研究引入了基于模型的树马尔可夫模型（MOB-HSMM），用于解决复杂黑盒机器学习模型应用于序列数据时的可解释性问题。通过从深度神经网络中蒸馏的知识，实现了提高预测性能的同时提供清晰解释的目标。实验结果表明通过将LSTM学习到的顺序模式转移到MOB树中，可以进一步提高MOB树的性能，并利用MOB-HSMM将MOB树与隐马尔可夫模型（HSMM）整合，实现了潜在和可解释的序列的发现。 |
| [^30] | [Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems.](http://arxiv.org/abs/2307.12975) | 该论文研究了基于偏好的政策学习方法在离线情境多臂赌博问题中的优势，并通过改进建模和分析，证明了这一方法相比其他政策学习方法具有更低的次优性。 |
| [^31] | [Uncertainty-aware Grounded Action Transformation towards Sim-to-Real Transfer for Traffic Signal Control.](http://arxiv.org/abs/2307.12388) | 本文提出了UGAT方法，通过在模拟环境中动态转换具有不确定性的行动，实现了从模拟环境到真实环境的策略转移，显著提高了在真实世界中的性能。 |
| [^32] | [HIQL: Offline Goal-Conditioned RL with Latent States as Actions.](http://arxiv.org/abs/2307.11949) | 本文提出了一个基于离线数据的目标导向强化学习的分层算法，通过利用目标达成问题的结构，使用一个无动作的价值函数学习了两个策略，从而在学习过程中更有效地利用离线数据。 |
| [^33] | [Beyond Black-Box Advice: Learning-Augmented Algorithms for MDPs with Q-Value Predictions.](http://arxiv.org/abs/2307.10524) | 该论文研究了在具有不可信的机器学习建议的单轨迹时间变化的MDP中一致性和鲁棒性之间的权衡，并证明了利用Q值建议可以获得接近最优的性能保证，并改进了仅使用黑盒建议的情况。 |
| [^34] | [Neural Image Compression: Generalization, Robustness, and Spectral Biases.](http://arxiv.org/abs/2307.08657) | 本论文将神经图像压缩的泛化性能、鲁棒性和光谱偏差进行了研究，提出了评估图像压缩方法在未见分布下性能的基准套件，并引入了一种光谱启发式的检测工具来深入了解这些方法引入的错误。 |
| [^35] | [SBMLtoODEjax: Efficient Simulation and Optimization of Biological Network Models in JAX.](http://arxiv.org/abs/2307.08452) | SBMLtoODEjax是一个轻量级的工具，利用机器学习在JAX中高效模拟和优化生物网络模型，用于理解生物系统的多样行为和优化干预。 |
| [^36] | [Complexity Matters: Rethinking the Latent Space for Generative Modeling.](http://arxiv.org/abs/2307.08283) | 本研究从模型复杂性的角度重新思考生成建模的潜在空间，提出了一种新的潜在与数据分布之间的“距离”，并通过该距离的最小化来优化生成器的复杂性。 |
| [^37] | [CatBoost Versus XGBoost and LightGBM: Developing Enhanced Predictive Models for Zero-Inflated Insurance Claim Data.](http://arxiv.org/abs/2307.07771) | 本文比较了CatBoost、XGBoost和LightGBM三种流行的梯度提升库在处理零膨胀保险理赔数据上的效果，并通过对两个不同数据集的分析，证明了CatBoost是最适合训练保险理赔数据和拟合精算频率模型的库。 |
| [^38] | [Embracing the chaos: analysis and diagnosis of numerical instability in variational flows.](http://arxiv.org/abs/2307.06957) | 本文研究了数值不稳定性对变分流中采样、密度评估和ELBO估计的可靠性的影响。通过理论保证和实验验证，我们发现尽管存在严重的数值不稳定性，变分流产生的结果在应用中常常足够准确。 |
| [^39] | [A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media.](http://arxiv.org/abs/2307.06775) | 本研究创建了一个多模态深度学习模型，将文本和视觉数据相结合，能够准确识别社交媒体上的促进饮食紊乱的内容。最有效的模型是RoBERTa自然语言处理模型和MaxViT图像分类模型的融合模型，准确率和F1分数分别达到95.9%和0.959。 |
| [^40] | [Instruction Mining: High-Quality Instruction Data Selection for Large Language Models.](http://arxiv.org/abs/2307.06290) | 本文提出了InstructMining，一种用于选择高质量指令数据的线性规则，以增强大语言模型的解释和响应指令能力。通过特定的自然语言指标建模，研究结果表明，即使只有少量高质量的指令跟随数据，语言模型也可以进行良好的微调。 |
| [^41] | [Differentially Private Statistical Inference through $\beta$-Divergence One Posterior Sampling.](http://arxiv.org/abs/2307.05194) | 通过对数据生成过程和模型之间的$\beta$-分解进行后验采样，我们提出了$\beta$D-Bayes，一种能够实现差分机器学习的方法。 |
| [^42] | [Enhancing Adversarial Robustness via Score-Based Optimization.](http://arxiv.org/abs/2307.04333) | 本文介绍了一种名为ScoreOpt的新型对抗防御方案，该方案通过优化对抗样本来提高模型的鲁棒性，实验证明该方法在多个数据集上优于现有的对抗防御方法。 |
| [^43] | [Text Descriptions are Compressive and Invariant Representations for Visual Learning.](http://arxiv.org/abs/2307.04317) | 这项研究提出了一种新的方法，通过生成多个视觉特征描述并将其转化为视觉特征嵌入，实现了在少样本学习环境中的优异性能表现。 |
| [^44] | [Optimal Learners for Realizable Regression: PAC Learning and Online Learning.](http://arxiv.org/abs/2307.03848) | 本论文研究了可实现回归问题的PAC学习和在线学习的统计复杂度，并提出了对于可学习性的必要条件和充分条件。 |
| [^45] | [Goal-Conditioned Predictive Coding as an Implicit Planner for Offline Reinforcement Learning.](http://arxiv.org/abs/2307.03406) | 本论文研究了将决策制定视为离线收集的轨迹的监督学习问题，并探究了序列建模在轨迹压缩和策略学习中的作用。通过引入目标条件预测编码（GCPC），本文提出了一种能够产生强大轨迹表示并实现高性能策略的方法。 |
| [^46] | [Conditional independence testing under model misspecification.](http://arxiv.org/abs/2307.02520) | 该论文研究了模型错误下的条件独立性检验，在这种情况下提出了新的近似或上界来衡量基于回归的测试的测试误差，并引入了一种新颖的基于回归的CI检验方法RBPT，对模型错误具有鲁棒性。 |
| [^47] | [Fast Optimal Transport through Sliced Wasserstein Generalized Geodesics.](http://arxiv.org/abs/2307.01770) | 本文提出了一种快速计算最优输运的方法，通过切片Wasserstein广义测地线进行近似，得到了一个基于一维最优投影的代理距离min-SWGG，并提供了相关的传输计划。这种方法具有较低的计算复杂度，适用于优化算法。 |
| [^48] | [Adaptive Principal Component Regression with Applications to Panel Data.](http://arxiv.org/abs/2307.01357) | 本文提出了自适应主成分回归方法，并在面板数据中的应用中获得了均匀有限样本保证。该方法可以用于面板数据中的实验设计，特别是当干预方案是自适应分配的情况。 |
| [^49] | [FedCP: Separating Feature Information for Personalized Federated Learning via Conditional Policy.](http://arxiv.org/abs/2307.01217) | 提出了一种名为FedCP的个性化联邦学习方法，通过生成条件策略分离特征中的全局信息和个性化信息，并分别进行处理。实验证明该方法在计算机视觉和自然语言处理领域的性能超过了十一种最先进的方法，最高可提高6.69%。 |
| [^50] | [Coupled Gradient Flows for Strategic Non-Local Distribution Shift.](http://arxiv.org/abs/2307.01166) | 该论文提出了一种框架，用于分析现实世界系统中的分布偏移动态，并且捕捉了学习算法与其应用的分布之间的反馈循环。通过耦合偏微分方程模型，考虑了战略性反应、非局部内生人口互动和其他外生分布偏移来源，以实现对时间上细微变化的捕捉。研究证明了在合作设置和竞争设置中，当算法通过梯度下降进行重新训练时，重新训练过程渐近收敛到一个稳定状态。 |
| [^51] | [BuildingsBench: A Large-Scale Dataset of 900K Buildings and Benchmark for Short-Term Load Forecasting.](http://arxiv.org/abs/2307.00142) | 本文提出了BuildingsBench，这是一个包含900K座建筑物的大规模数据集，旨在解决短期负荷预测中数据集不足的问题。通过该数据集，我们进行了两个任务的基准评估，并发现经过合成预训练的模型具有良好的泛化能力。 |
| [^52] | [Generalization Limits of Graph Neural Networks in Identity Effects Learning.](http://arxiv.org/abs/2307.00134) | 本研究在学习身份效应的背景下，分析了图神经网络在泛化属性和基本限制方面的新性质，以及在两个字母的单词案例中的具体应用。 |
| [^53] | [On the Exploitability of Instruction Tuning.](http://arxiv.org/abs/2306.17194) | 该论文研究了如何利用指令调整技术来改变模型行为的问题，并提出了一种自动数据注入的方法AutoPoison。实验结果表明，通过少量的训练数据毒化，对手能够改变模型的行为。 |
| [^54] | [Realistic Synthetic Financial Transactions for Anti-Money Laundering Models.](http://arxiv.org/abs/2306.16424) | 本文提供了一个逼真的合成金融交易数据集生成器和一组合成的反洗钱数据集，以满足训练模型和推进领域发展的需求。 |
| [^55] | [Learning non-Markovian Decision-Making from State-only Sequences.](http://arxiv.org/abs/2306.15156) | 本文提出了一种从仅状态序列学习非马尔科夫决策的方法，通过深度生成建模和最大似然估计实现基于模型的模仿。学习的模型能够实现“推理式决策”，并在路径规划任务中展示了有效性。 |
| [^56] | [InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback.](http://arxiv.org/abs/2306.14898) | InterCode是一个交互式编码的标准化和基准测试框架，它使用执行反馈作为观察，并提供了安全可重现的执行环境，可以用于开发新的交互式代码生成方法。 |
| [^57] | [Accelerating Molecular Graph Neural Networks via Knowledge Distillation.](http://arxiv.org/abs/2306.14818) | 本文以知识蒸馏为基础，旨在加速分子图神经网络。通过开发知识蒸馏策略，我们成功地加速了分子GNNs，并在能量和力预测任务中取得了更高的预测准确性。 |
| [^58] | [Design from Policies: Conservative Test-Time Adaptation for Offline Policy Optimization.](http://arxiv.org/abs/2306.14479) | 本文提出了一种非迭代的双层范式，将迭代式双层线下强化学习解耦，并回答了传递信息、安全优化和同时进行外层优化的核心问题。 |
| [^59] | [Human-in-the-Loop Optimization for Deep Stimulus Encoding in Visual Prostheses.](http://arxiv.org/abs/2306.13104) | 本研究提出了一种“人类介入的视觉前列腺深度刺激编码优化”的方法，通过反演前向模型、实时优化编码参数等手段，显著提高了感知质量。 |
| [^60] | [Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective.](http://arxiv.org/abs/2306.13092) | SRe$^2$L是一种数据集压缩方法，可以处理不同规模的数据集、模型体系结构和图像分辨率，具有低训练成本和内存消耗以及扩展到任意评估网络体系结构的能力，在Tiny-ImageNet和ImageNet-1K数据集上实现了最佳性能，并超越了现有的最先进方法。 |
| [^61] | [Should I Stop or Should I Go: Early Stopping with Heterogeneous Populations.](http://arxiv.org/abs/2306.11839) | 本文提出了针对于异质种群有害实验的早期停止方法CLASH，使用因果机器学习可以有效提前停止临床试验和A/B测试。 |
| [^62] | [Sampling from Gaussian Process Posteriors using Stochastic Gradient Descent.](http://arxiv.org/abs/2306.11589) | 本文探索了使用随机梯度下降算法从高斯过程后验中采样的方法，该方法计算高效且能在足够覆盖数据的区域和足够远离数据的区域中产生接近真实后验的预测分布。 |
| [^63] | [Quilt-1M: One Million Image-Text Pairs for Histopathology.](http://arxiv.org/abs/2306.11207) | 本文介绍了一个名为 Quilt-1M 的癌症组织学图像和文字对的百万数据集，并利用 YouTube 上的专家医生教程视频为主要来源。这个数据集将使得癌症组织学领域的表征学习取得类似于其他领域的进展。 |
| [^64] | [Simplifying and Empowering Transformers for Large-Graph Representations.](http://arxiv.org/abs/2306.10759) | 本文通过实验证明，在大型图上使用一层注意力即可获得令人惊讶的竞争性能，挑战了在语言和视觉任务中复杂模型的应用。这促使我们重新思考在大型图上设计Transformer的理念，以提高可扩展性。 |
| [^65] | [Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis.](http://arxiv.org/abs/2306.10168) | 本研究提出了一种新的方法，通过比较神经网络系统的动力学特征来判断它们是否利用了相同的内部过程进行计算。 |
| [^66] | [Block-State Transformer.](http://arxiv.org/abs/2306.09539) | 本文提出了一种名为块状态变换器（BST）的混合神经网络层，结合了状态空间模型和块变换器，旨在在语言建模任务中提高性能和可扩展性。实验证明，该模型在语言建模困惑度上优于类似的基于Transformer的架构，并且可以推广到更长的序列。此外，在层级别上具有超过十倍的速度提升。 |
| [^67] | [Residual Q-Learning: Offline and Online Policy Customization without Value.](http://arxiv.org/abs/2306.09526) | 该研究提出了一种新方法，使用动态控制残差的 Q 学习来进行离线和在线的策略定制，无需使用价值函数。 |
| [^68] | [Class-Conditional Conformal Prediction With Many Classes.](http://arxiv.org/abs/2306.09335) | 提出了一种叫做聚类符合性预测的方法，可以在多类条件下提供类别条件符合性预测，针对多个类别的图像数据集中经验评估结果表明其优于现有方法。 |
| [^69] | [Neural Fields with Hard Constraints of Arbitrary Differential Order.](http://arxiv.org/abs/2306.08943) | 本文提出了一种名为约束神经场的方法，用于在神经网络中实施任意微分阶的硬约束。通过应用线性算子到神经场及其导数，我们能够解决标准模型在受限制情况下遇到的问题，并在各种实际应用中验证了该方法的有效性。 |
| [^70] | [MMD-FUSE: Learning and Combining Kernels for Two-Sample Testing Without Data Splitting.](http://arxiv.org/abs/2306.08777) | 本文提出了MMD-FUSE方法，通过适应内核集合最大化基于MMD的双样本检验功率，避免数据分割，并在低维合成数据和高维实际数据上证明了其适用性和功率超过现有最先进的核检验方法。 |
| [^71] | [LargeST: A Benchmark Dataset for Large-Scale Traffic Forecasting.](http://arxiv.org/abs/2306.08259) | LargeST数据集是一个更为现实和具有挑战性的交通预测基准，包括8600个传感器、覆盖5年时间和包括细致元数据。 |
| [^72] | [Unified Off-Policy Learning to Rank: a Reinforcement Learning Perspective.](http://arxiv.org/abs/2306.07528) | 本文提出了点击模型不可知的统一非同策略学习排序（CUOLR）方法，通过离线强化学习（RL）直接学习最优排名，可以轻松地应用于各种点击模型。 |
| [^73] | [A Holistic Approach to Unifying Automatic Concept Extraction and Concept Importance Estimation.](http://arxiv.org/abs/2306.07304) | 本文提出了一个全面的理论框架，来统一定义和澄清自动概念提取和概念重要性评估，进而提供新的评估指标以实现对这些方法的比较以及推导关于这种方法的最优性的理论保证。 |
| [^74] | [Neural Injective Functions for Multisets, Measures and Graphs via a Finite Witness Theorem.](http://arxiv.org/abs/2306.06529) | 本文介绍了一种通过使用解析的非多项式激活函数，基于神经网络的矩来定义可逆多重集合函数的方法，并通过有限证据定理证明了其有效性。该方法在多重集合和图的机器学习中具有重要的应用价值。 |
| [^75] | [ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer.](http://arxiv.org/abs/2306.06446) | ShiftAddViT通过使用位移和加法等多种乘法原语对ViT进行重新参数化，实现了减少乘法操作的高效视觉变换器模型，可以在GPU上实现端到端的推理加速，无需从头训练。 |
| [^76] | [Decision Stacks: Flexible Reinforcement Learning via Modular Generative Models.](http://arxiv.org/abs/2306.06253) | 决策堆叠是一种灵活的生成框架，将目标条件化策略代理分解为3个生成模块，并通过教师强制并行学习，可以用于设计单个模块以考虑架构偏差、优化目标和动态、跨领域的可转移性和推理速度。在连续控制基准测试中具有灵活性和可扩展性。 |
| [^77] | [Strategic Apple Tasting.](http://arxiv.org/abs/2306.06250) | 本篇论文介绍了一种带有苹果品尝反馈的在线学习问题，该问题需要使用一种学习算法以实现战略遗憾。研究结果表明，我们提出的算法的战略遗憾近似于最佳速率。 |
| [^78] | [FLSL: Feature-level Self-supervised Learning.](http://arxiv.org/abs/2306.06203) | 本文提出FLSL方法，采用Transformer进行联合嵌入和聚类，适合于内视图和外视图特征聚类。实验证明该方法在语义类簇表达方面取得显著改进。 |
| [^79] | [PoET: A generative model of protein families as sequences-of-sequences.](http://arxiv.org/abs/2306.06156) | PoET是一个模型，能够生成任何蛋白质家族的一系列相关蛋白质序列，可以用作检索增强语言模型生成和评分任何修改 |
| [^80] | [Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks.](http://arxiv.org/abs/2306.06155) | 本文提出了一种连续时间网络表示学习框架，涵盖核平滑的强度函数估计、最小化强度重构误差的投影学习和归纳构造节点表示。这种表示保留了网络结构和时间一致性。 |
| [^81] | [Prodigy: An Expeditiously Adaptive Parameter-Free Learner.](http://arxiv.org/abs/2306.06101) | 本文提出了一种基于自适应算法(如Adagrad和Adam)的学习率估计方法Prodigy和Resetting，可以快速且正确地估计到达解决方案所需的距离D，从而提高了模型的收敛速度，并在多个数据集和模型上进行了测试，实验表明该方法优于D-Adaptation并可达到手动调整Adam的测试准确度值。 |
| [^82] | [Advancing Counterfactual Inference through Quantile Regression.](http://arxiv.org/abs/2306.05751) | 本文提出一种基于分位数回归的反事实推断方法，旨在用于缺乏因果模型和直接条件分布估计的情况，并能提供估计结果的泛化能力和泛化误差上界。 |
| [^83] | [MC-NN: An End-to-End Multi-Channel Neural Network Approach for Predicting Influenza A Virus Hosts and Antigenic Types.](http://arxiv.org/abs/2306.05587) | 提出了一种利用多通道神经网络模型预测流感A病毒宿主和抗原亚型的方法，数据显示多通道神经网络模型具有较高的准确性和预测能力。 |
| [^84] | [Bayesian Optimisation of Functions on Graphs.](http://arxiv.org/abs/2306.05304) | 本论文提出了一种在通用的大规模和潜在未知图上定义函数的贝叶斯优化算法，并通过学习适当的图内核，适应目标函数行为。 |
| [^85] | [Factorized Contrastive Learning: Going Beyond Multi-view Redundancy.](http://arxiv.org/abs/2306.05268) | 本论文提出了FactorCL，一种新的多模态表示学习方法，不仅考虑跨模态共享信息，还能捕捉跨模态唯一的任务相关信息。 |
| [^86] | [Robust Learning with Progressive Data Expansion Against Spurious Correlation.](http://arxiv.org/abs/2306.04949) | 本文通过理论分析和实验证明了，在存在虚假特征的情况下，数据组的不平衡和易于学习的虚假特征可能导致模型学习虚假特征。作者提出了一种新的训练算法PDE，它逐步扩展训练数据的大小可以提高了模型鲁棒性，有效地增强了其最劣组性能，实验证实在合成和真实世界的基准数据集上的超越了R模型等其它模型。 |
| [^87] | [Exact Optimality of Communication-Privacy-Utility Tradeoffs in Distributed Mean Estimation.](http://arxiv.org/abs/2306.04924) | 本研究针对均值估计问题，探讨了在通信和本地差分隐私约束下的精确最优方法，提出了利用旋转对称的共享随机码书，并通过$k$-closest编码实现了随机旋转的单纯形$c$的精确最优。 |
| [^88] | [Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards Simpler Subnetworks.](http://arxiv.org/abs/2306.04251) | SGD在训练过度表达的网络时，会随机地将动态吸引到更简单的子网络，这种随机吸引性能够提高泛化能力。 |
| [^89] | [LEACE: Perfect linear concept erasure in closed form.](http://arxiv.org/abs/2306.03819) | 本文介绍了一种闭合形式的方法LEACE，可在删除指定特征的同时尽可能少地改变表示，并可证明防止所有线性分类器检测到概念。作者用“概念擦除”这一新方法将其应用于大型语言模型，在测量语言模型对词性的依赖性和减少BERT嵌入中的性别偏差任务中得出良好表现。 |
| [^90] | [FAMO: Fast Adaptive Multitask Optimization.](http://arxiv.org/abs/2306.03792) | FAMO是一种快速自适应多任务优化方法，通过动态加权方式实现平衡的任务损失减少，相比最先进的梯度操作技术具有相似或更优的性能。 |
| [^91] | [State Regularized Policy Optimization on Data with Dynamics Shift.](http://arxiv.org/abs/2306.03552) | 本文提出了一种叫做 SRPO (状态规范化策略优化) 的算法，该算法利用训练数据中的稳态分布来规范新环境中的策略，在处理具有不同动态的多个环境时表现优异。 |
| [^92] | [Random Distribution Shift in Refugee Placement: Strategies for Building Robust Models.](http://arxiv.org/abs/2306.02948) | 本文研究了难民安置中的随机分布转移问题，并提出并比较了三种建模策略，最终发现混合方法具有较强鲁棒性。 |
| [^93] | [Towards Anytime Classification in Early-Exit Architectures by Enforcing Conditional Monotonicity.](http://arxiv.org/abs/2306.02652) | 本文提出了一种在Early-Exit网络中实现条件单调性的方法，将深度模型转化为真正的随时分类器。 |
| [^94] | [LambdaBeam: Neural Program Search with Higher-Order Functions and Lambdas.](http://arxiv.org/abs/2306.02049) | LambdaBeam是一种神经程序搜索算法，通过构建任意lambda函数，并将其作为参数传递给高阶函数来解决先前神经搜索合成更长且更通用程序的限制。 |
| [^95] | [Uncovering the Hidden Dynamics of Video Self-supervised Learning under Distribution Shifts.](http://arxiv.org/abs/2306.02014) | 本文全面研究了视频自监督学习方法在不同形式的分布转移下的行为，揭示了一系列有趣发现和行为。 |
| [^96] | [Auditing for Human Expertise.](http://arxiv.org/abs/2306.01646) | 人类专家的价值超出了算法可捕捉范围，我们可以用一个简单的程序测试这个问题。 |
| [^97] | [Exposing Attention Glitches with Flip-Flop Language Modeling.](http://arxiv.org/abs/2306.00946) | 本论文揭示了语言模型中注意力故障的现象，并通过引入翻转-翻转语言建模来分析这个问题。研究发现，Transformer FFLMs经常出现推理错误。 |
| [^98] | [Interaction Measures, Partition Lattices and Kernel Tests for High-Order Interactions.](http://arxiv.org/abs/2306.00904) | 本文提出了一种用于高阶相互作用的测量方法和基于核的测试方法，并与格理论建立了数学联系，为增强相互作用模型提供了方法。 |
| [^99] | [Nonparametric Identifiability of Causal Representations from Unknown Interventions.](http://arxiv.org/abs/2306.00542) | 本文提出了一种新方法用于从未知干预数据中推断非参数因果表达式学习，并且证明了在两个因果变量的基本设置中，无法消除一些由干预数据引起的歧义问题。 |
| [^100] | [Addressing Negative Transfer in Diffusion Models.](http://arxiv.org/abs/2306.00354) | 本文从多任务学习角度出发，研究了扩散训练中的负迁移现象，提出了利用正则化技术增强扩散训练的方法，以减轻负迁移并提高去噪任务的性能。 |
| [^101] | [Diffusion Brush: A Latent Diffusion Model-based Editing Tool for AI-generated Images.](http://arxiv.org/abs/2306.00219) | 本论文提出了一种名为扩散画笔的基于潜在扩散模型的AI图像微调工具，可以有效地根据目标区域修改AI合成图像并保留原始上下文。与其他最先进的图像修复技术进行比较，该方法在用户研究中表现出更好的可用性和有效性。 |
| [^102] | [Diffused Redundancy in Pre-trained Representations.](http://arxiv.org/abs/2306.00183) | 本文发现预训练表示中存在一定程度的扩散冗余，即随机选择一部分神经元与全部神经元相似度高且性能相似，可用于降低实际部署中的计算和内存成本，同时保持相当的性能水平。 |
| [^103] | [Inconsistency, Instability, and Generalization Gap of Deep Neural Network Training.](http://arxiv.org/abs/2306.00169) | 本文的理论分析与实证研究表明，深度神经网络训练中模型输出的不一致性和不稳定性可以作为估计泛化间隙的重要指标，消除不一致性的算法能够提高模型性能。 |
| [^104] | [Improving CLIP Training with Language Rewrites.](http://arxiv.org/abs/2305.20088) | 本文介绍了一种名为Language augmented CLIP（LaCLIP）的方法，通过语言重写来增强CLIP训练。利用大型语言模型的能力，重新书写与每个图像关联的文本描述，以增加多样性，同时保留原始的关键概念和意义。 |
| [^105] | [Latent Exploration for Reinforcement Learning.](http://arxiv.org/abs/2305.20065) | 本文提出了一种名为Lattice的方法，通过向策略网络的潜在状态中注入时间相关性噪声，来解决强化学习中多作用器系统存在的探索问题。 |
| [^106] | [UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures.](http://arxiv.org/abs/2305.20054) | 本文提出了一种名为UNSSOR的算法，通过利用过渡训练混合物来实现无监督神经语音分离。该算法利用每个混合信号作为约束，将估计的说话者图像加起来等于混合信号，从而缩小解的范围。实验结果表明，UNSSOR算法在语音分离效果和性能方面取得了显著的提升。 |
| [^107] | [Spectal Harmonics: Bridging Spectral Embedding and Matrix Completion in Self-Supervised Learning.](http://arxiv.org/abs/2305.19818) | 该论文通过从拉普拉斯算子的角度出发，将自监督学习方法中增广过程所产生的归纳偏置与低秩矩阵补全问题相联系，提供了对现代自监督学习方法收敛性和下游性能的理论分析。 |
| [^108] | [The Tunnel Effect: Building Data Representations in Deep Neural Networks.](http://arxiv.org/abs/2305.19753) | 本文研究表明，深度神经网络中存在一种名为“隧道”的现象，它在网络的训练早期就出现，并且对最终的数据表示起到了压缩作用。其中，初始层构建了线性可分表示形式，而随后的层压缩这些表示形式并对整体性能影响不大。然而，隧道会削弱网络在超出分布的泛化性能。 |
| [^109] | [Replicability in Reinforcement Learning.](http://arxiv.org/abs/2305.19562) | 这篇论文研究了在强化学习中的可复制性，提出了可复制算法和松弛可复制算法，并给出了相应的时间和样本复杂度，这对于RL算法设计以及未来的可复制性研究具有影响。 |
| [^110] | [Joint Bayesian Inference of Graphical Structure and Parameters with a Single Generative Flow Network.](http://arxiv.org/abs/2305.19366) | 本文提出了在单一生成流网络中联合建模贝叶斯网络结构和参数的方法，包括非离散样本空间，提高了贝叶斯网络局部概率模型的灵活性。 |
| [^111] | [NetHack is Hard to Hack.](http://arxiv.org/abs/2305.19240) | 本文研究了神经策略学习在NetHack中的性能差距，并分析了获胜的符号代理，提出了动作层级的优势、神经架构的改进以及强化学习与模仿学习的整合等方面可能提升性能的方法。 |
| [^112] | [Compression with Bayesian Implicit Neural Representations.](http://arxiv.org/abs/2305.19185) | 该论文提出了一种用Bayesian隐式神经表示来压缩数据的方法，通过最小化 $\beta$-ELBO 直接优化码-失真性能，并通过调整 $\beta$ 来针对给定的网络结构实现不同的码-失真平衡。 |
| [^113] | [LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images.](http://arxiv.org/abs/2305.19164) | 本文提出了一种自动化算法LANCE，通过生成语言引导的对抗性测试图像来压力测试视觉模型，实现了在不改变模型权重的情况下增加多样、逼真且具有挑战性的测试图像。通过对多种预训练模型的性能进行基准测试，发现模型性能显著下降，并且通过分析模型对不同类型编辑的敏感性，揭示了ImageNet中先前未知的类别层次模型偏差。 |
| [^114] | [Neural Sculpting: Uncovering hierarchically modular task structure through pruning and network analysis.](http://arxiv.org/abs/2305.18402) | 本文提出了一种名为“神经雕塑”的方法，该方法通过神经网络的修剪和分析生成的图形结构来揭示任务的子函数的层次结构。该方法在布尔任务上得到了有效验证。 |
| [^115] | [On the impact of activation and normalization in obtaining isometric embeddings at initialization.](http://arxiv.org/abs/2305.18399) | 本论文研究了深度神经网络中的 Gram 矩阵结构，证明了激活函数和层规范化结合使用可以在初始化时偏向指数级深度等距，从而弥补了现有理论的空白。 |
| [^116] | [Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks.](http://arxiv.org/abs/2305.18395) | 本论文提出了一种KARD方法，可以通过向小型语言模型中加入从外部知识库检索到的增强知识来解决知识密集型推理任务中小型语言模型记忆能力有限的问题。 |
| [^117] | [Emergent Modularity in Pre-trained Transformers.](http://arxiv.org/abs/2305.18390) | 本论文研究了预训练Transformers中的自发模块化现象，发现神经元可以进行功能专业化，并通过聚类建立起模块化结构，此结构可被有效扰动。 |
| [^118] | [Beyond Confidence: Reliable Models Should Also Consider Atypicality.](http://arxiv.org/abs/2305.18262) | 研究发现，模型的可靠性不能仅仅依靠置信度，还应考虑预测样本或类别的非典型性，特别是对于非典型输入或类别，模型预测更过于自信且准确性较低。利用这些发现，将非典型性纳入模型可以提高不确定性量化和性能。 |
| [^119] | [Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making.](http://arxiv.org/abs/2305.17588) | 该论文介绍了一种名为SUFO的框架，用于增强微调的变压器模型的可解释性。该框架利用多种分析和可视化技术，解决了模型信任和可解释性的关键问题，并通过案例研究验证了其有效性。 |
| [^120] | [Auditing Fairness by Betting.](http://arxiv.org/abs/2305.17570) | 本文提供了一种通过赌博的方式进行公平性审计的方法，相比之前的方法，这种方法具有更高的实用性和效率，能够对不断产生的数据进行连续的监控，并处理因分布漂移导致的公平性问题。 |
| [^121] | [Causal Component Analysis.](http://arxiv.org/abs/2305.17225) | 本文介绍了一个中间问题：因果成分分析(CauCA)，它是独立成分分析(ICA)和因果表示学习(CRL)的泛化和特例，其目标是学习解混函数和因果机制，预设了因果图的知识。 |
| [^122] | [Three Towers: Flexible Contrastive Learning with Pretrained Image Models.](http://arxiv.org/abs/2305.16999) | 本文提出了 3T 方法，即在视觉语言模型中引入预训练的图像分类器，从而提高对比学习的灵活性。3T 同时受益于预训练嵌入和对比训练，并在实验证明对检索任务和分类问题均取得了有竞争力的性能。 |
| [^123] | [On Evaluating Adversarial Robustness of Large Vision-Language Models.](http://arxiv.org/abs/2305.16934) | 该论文提出在最真实和高风险的情境中评估大型视觉语言模型的对抗鲁棒性。作者首先构建有针对性的对抗样本，然后将其转移到其他模型中进行评估，并观察到黑盒查询可以改进效果。 |
| [^124] | [Accelerating Value Iteration with Anchoring.](http://arxiv.org/abs/2305.16569) | 本文提出了一种加速值迭代算法Anc-VI，采用了锚定机制，可加速Bellman一致性和最优性算子的计算。对于$\gamma\approx 1$或$\gamma=1$，Anc-VI速度为$\mathcal{O}(1/k)$，比标准VI更快。 |
| [^125] | [Tree-Based Diffusion Schr\"odinger Bridge with Applications to Wasserstein Barycenters.](http://arxiv.org/abs/2305.16557) | 本文介绍了一种基于树的扩散薛定谔桥算法(TreeDSB)来解决多元最优输运(mOT)的问题，并可以应用于高维设置如图像插值和贝叶斯融合。 |
| [^126] | [Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer.](http://arxiv.org/abs/2305.16380) | 本文分析了1层Transformer在下一个标记预测任务中的SGD训练动态，证明了自我关注层充当了“区分性扫描算法”，从而逐步关注到相关标记并排除不相关的标记，总结相关信息在编码表示中。同时研究了标记频率、上下文和初始化自我关注层等对Transformer性能的影响。 |
| [^127] | [Sharpness-Aware Minimization Leads to Low-Rank Features.](http://arxiv.org/abs/2305.16292) | 锐度感知最小化方法SAM在神经网络训练中不仅能提升泛化性能，还可以降低特征的秩，适用于不同网络架构和任务类型。 |
| [^128] | [DoWG Unleashed: An Efficient Universal Parameter-Free Gradient Descent Method.](http://arxiv.org/abs/2305.16284) | 本文提出了一种名为DoWG的无参数梯度下降方法，它是第一个既高效又通用的算法，能够自适应于平稳和非平稳问题，并且无需回溯搜索过程。 |
| [^129] | [Incentivizing Honesty among Competitors in Collaborative Learning and Optimization.](http://arxiv.org/abs/2305.16272) | 这项研究提出了一个模型来描述在协作学习中竞争对手的不诚实行为，提出了机制来激励诚实沟通，并确保学习质量与全面合作相当。 |
| [^130] | [Trans-Dimensional Generative Modeling via Jump Diffusion Models.](http://arxiv.org/abs/2305.16261) | 本文通过跳跃扩散模型实现了一种跨维度生成建模方法，并证明其在处理不同维度数据时具有更好的兼容性和插值能力。 |
| [^131] | [Koopman Kernel Regression.](http://arxiv.org/abs/2305.16215) | 提出了一种基于Koopman核的回归方法，用于预测非线性动力系统的时间演变。该方法在机器人操作，视频预测和交通预测等各种应用中均有优异表现，并具有可证明的学习理论保证。 |
| [^132] | [Accelerated Methods for Riemannian Min-Max Optimization Ensuring Bounded Geometric Penalties.](http://arxiv.org/abs/2305.16186) | 本文研究了Riemannian流形上的优化问题，设计了加速方法，并介绍了几何凸优化的新结果。通过去除关于迭代在预定紧致集合内的假设，完善了之前的工作。 |
| [^133] | [Strategic Data Sharing between Competitors.](http://arxiv.org/abs/2305.16052) | 这项研究介绍了竞争对手之间战略数据共享的问题，提出了一个分析框架，研究了市场条件对数据共享激励的影响。 |
| [^134] | [How to Turn Your Knowledge Graph Embeddings into Generative Models via Probabilistic Circuits.](http://arxiv.org/abs/2305.15944) | 本文提出了将知识图嵌入模型转化为生成模型的方法，允许精确MLE学习、有效抽样新的三元组以及保证逻辑约束，获得了比原始KGEs更具伸缩性的性能。 |
| [^135] | [Differentially Private Latent Diffusion Models.](http://arxiv.org/abs/2305.15759) | 本文提出使用差分隐私训练潜在扩散模型(LDMs)，通过预训练自编码器将高维像素空间转变为低维潜在空间实现更高效快速的DMs训练，并且通过只微调注意力模块减少了可训练参数的数量。 |
| [^136] | [Characterizing Out-of-Distribution Error via Optimal Transport.](http://arxiv.org/abs/2305.15640) | 本论文提出了一种基于最优输运理论的新方法 - 置信最优输运(COT)，并且引入了基于经验的变体 - 带门限的置信最优输运(COTT)，它们能够更精确地估计模型的性能，特别是在面对伪标签转移误差时。 |
| [^137] | [On the Minimax Regret for Online Learning with Feedback Graphs.](http://arxiv.org/abs/2305.15383) | 本论文改进了具有强可观察无向反馈图的在线学习遗憾的上下界，并提出了一个适用于任意$\alpha$情况下的改进上界，该上界与强盗案例和专家案例下界相匹配，并中间插值，证明过程使用了特定值$q \in [1/2, 1)$随$\alpha$变化的FTRL和$q$-Tsallis熵的方法。 |
| [^138] | [Black-Box Variational Inference Converges.](http://arxiv.org/abs/2305.15349) | 通过对黑盒变分推断（BBVI）的分析，发现一些常见的算法设计选择可能会导致次优收敛速率，但使用带有近端随机梯度下降的BBVI可以实现最强收敛率保证。 |
| [^139] | [Training Energy-Based Normalizing Flow with Score-Matching Objectives.](http://arxiv.org/abs/2305.15267) | 本文提出一种新的基于能量的归一化流模型（EBFlow），通过得分匹配目标优化使其训练更高效，同时开发一些技术增强EBFlow的训练稳定性和实证表现。 |
| [^140] | [Momentum Provably Improves Error Feedback!.](http://arxiv.org/abs/2305.15155) | 这项研究证明动量可以改善机器学习中的误差反馈问题，并提出了一个简单的解决方案，解决了批次大小过大的问题。 |
| [^141] | [Beyond Individual Input for Deep Anomaly Detection on Tabular Data.](http://arxiv.org/abs/2305.15121) | 本文提出了一种采用非参数转换器的深度异常检测方法，能够捕捉表格数据中特征与特征之间以及样本与样本之间的依赖关系，并在广泛的表格数据集上取得了比现有最先进方法更优的性能。 |
| [^142] | [Block-local learning with probabilistic latent representations.](http://arxiv.org/abs/2305.14974) | 该论文提出了一种使用概率潜在表示的块局部学习方法，通过将深度神经网络分为块并引入反馈网络，在解决锁定和权重传输问题的同时，实现了高效的并行化和水平扩展。 |
| [^143] | [Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4.](http://arxiv.org/abs/2305.14928) | 本研究提出利用泛化、不确定性和最新的大型语言模型，寻求解决假新闻问题。实验证明GPT-4在多语言环境下表现优于之前的方法。研究还探讨了泛化和不确定性处理技术，并在其他语言模型、温度、提示、版本控制、可解释性和网络检索方面取得了实际见解和未来研究方向。此外，研究还发布了新颖的英法配对假新闻数据集LIAR-New，为信息真实性评估提供了可行性标签。 |
| [^144] | [Text encoders bottleneck compositionality in contrastive vision-language models.](http://arxiv.org/abs/2305.14897) | 本研究发现，在对比视觉-语言模型中，使用单个向量表示标语的文本编码器在处理更加复杂的输入时表现不佳，但某些文本编码器的性能明显优于其他编码器。仅基于文本的恢复性能能够预测多模态匹配性能。 |
| [^145] | [A Joint Time-frequency Domain Transformer for Multivariate Time Series Forecasting.](http://arxiv.org/abs/2305.14649) | 本文提出了一种称为JTFT的方法，可以在提取时间依赖性的同时有效地减小计算需求，适用于多元时间序列预测，并具有线性复杂度。同时，采用低秩注意层以有效捕获跨维度的依赖关系，并在六个真实世界数据集上展示了比现有方法更好的表现。 |
| [^146] | [Discriminative calibration.](http://arxiv.org/abs/2305.14593) | 这篇论文提出了一种替代基于排序的模拟校准（SBC）的灵活分类方法，该方法可以从数据中学习测试统计量，并计算出从分类准确度中计算出的误校准发散度度量，具有更高的统计功效，可以解决多重检验的挑战。 |
| [^147] | [Optimal Preconditioning and Fisher Adaptive Langevin Sampling.](http://arxiv.org/abs/2305.14442) | 通过最优预条件和费舍尔自适应 Langevin 采样，提出了一种计算有效且在高维中非常强健的自适应 MCMC 方案。 |
| [^148] | [Equivariant Neural Simulators for Stochastic Spatiotemporal Dynamics.](http://arxiv.org/abs/2305.14286) | 本研究提出了一种称为EPNS的等变概率神经模拟框架，可以在系统演化中生成等变分布，并在随机时空动态方面表现出色。 |
| [^149] | [Hierarchical Prompting Assists Large Language Model on Web Navigation.](http://arxiv.org/abs/2305.14257) | 这项研究提出了一种分层提示方法来解决大规模语言模型在处理复杂观察的交互决策任务中的困难。研究表明该方法在网络导航中的效果优于先前最先进的提示机制，具有广泛的适用性。 |
| [^150] | [Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization.](http://arxiv.org/abs/2305.14152) | 本文提出了一种称为PEQA的方法，结合了参数高效微调和量化LLM的优点。通过仅更新量化尺度，PEQA可以高效地微调压缩大型语言模型，并减少内存开销。 |
| [^151] | [Towards Understanding the Dynamics of Gaussian--Stein Variational Gradient Descent.](http://arxiv.org/abs/2305.14076) | 本文探究了高斯-斯坦变分梯度下降动态性。对于从高斯目标中采样，只要初始值是高斯的，具有双线性核的SVGD动态将保持高斯状态。当目标函数呈现出强对数凹性时，证明了均场高斯-SVGD动态会线性收敛于KL散度下最接近目标高斯分布。在有限粒子设置中，存在对均场极限的时间微步一致收敛以及线性收敛至目标高斯分布。 |
| [^152] | [SNEkhorn: Dimension Reduction with Symmetric Entropic Affinities.](http://arxiv.org/abs/2305.13797) | 提出了一种新的对称化方法用于熵亲和力下的降维算法，能够有效解决对称化过程中的熵和随机性问题。 |
| [^153] | [Understanding Multi-phase Optimization Dynamics and Rich Nonlinear Behaviors of ReLU Networks.](http://arxiv.org/abs/2305.12467) | 本文对于ReLU神经网络通过Gradient Flow训练的两层模型在线性可分数据上进行了完整的理论分析，揭示了整个训练过程中的四个不同阶段，显示出一个从简化到复杂的学习趋势。 |
| [^154] | [Uniform-in-Time Wasserstein Stability Bounds for (Noisy) Stochastic Gradient Descent.](http://arxiv.org/abs/2305.12056) | 本文通过建立学习理论和应用概率之间的联系，提出了一种证明随机优化算法Wasserstein稳定性界限的统一指南，并在随机梯度下降上验证了该方法的有效性，包括强凸损失和带添加噪声的非凸损失。 |
| [^155] | [Differentially Private Online Item Pricing.](http://arxiv.org/abs/2305.11362) | 本文介绍了一种差分隐私算法，可在保护买家隐私的同时实现重复、不限供应物品拍卖中的收益最大化，是第一个提供隐私保证的$O(\sqrt{T}\log{T})$亏损子线性的方法。 |
| [^156] | [Attacks on Online Learners: a Teacher-Student Analysis.](http://arxiv.org/abs/2305.11132) | 本文利用控制理论的视角研究了在线学习环境下可能遭受到的标签扰动攻击情况，得出攻击强度超过临界阈值时学习准确率将出现不连续转变的结论，并验证了理论在复杂结构学习器上的适用性。 |
| [^157] | [Language Models Meet World Models: Embodied Experiences Enhance Language Models.](http://arxiv.org/abs/2305.10626) | 本文提出一种新的增强语言模型的方法——将其与世界模型相结合，通过有目的的规划和随机探索获得丰富的实体经验进行微调, 以提高其在物理环境下的推理和行为能力，并在语言基准上保持或提高性能。 |
| [^158] | [Statistical Knowledge Assessment for Generative Language Models.](http://arxiv.org/abs/2305.10519) | 本文介绍了一个统计知识评估框架，用于评估生成式语言模型（GLMs）的知识输出。通过对14种GLMs进行综合比较，发现知识遵循缩放定律，而对指令遵循数据进行微调可能会损害模型的生成能力。 |
| [^159] | [The Adversarial Consistency of Surrogate Risks for Binary Classification.](http://arxiv.org/abs/2305.09956) | 本文研究了二分类中代理风险的对抗一致性，并给出了代理损失函数集合的特征化，结果表明敌对一致代理的类与标准设置相比小得多。 |
| [^160] | [Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques.](http://arxiv.org/abs/2305.07116) | 本文研究了隐私增强技术对机器学习模型准确性和能量消耗的影响，发现k-匿名化训练的模型准确性较低，但能源消耗较少，合成数据是一种有潜力的选择，因为它能够在能源消耗更少的情况下取得可比的准确性。 |
| [^161] | [Information Design in Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2305.06807) | 本文探究了多智能体强化学习中的信息设计问题及其挑战，提出了“马尔科夫信令博弈”的概念。 |
| [^162] | [Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation.](http://arxiv.org/abs/2305.05803) | 本论文提出了一种通过利用Segment Anything Model (SAM)增强Class Activation Maps (CAM)生成高质量伪标签的方法来解决弱监督语义分割中CAM的局部激活和虚假激活的限制问题。 |
| [^163] | [NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports.](http://arxiv.org/abs/2305.03598) | 本文提出了一种面向临床试验报告的自然语言推理模型，通过检索支持事实来确定自然语言陈述和CTR之间的推理关系。 |
| [^164] | [Demystifying Softmax Gating in Gaussian Mixture of Experts.](http://arxiv.org/abs/2305.03288) | 本文提出了新的参数Vononoi损失函数并建立了MLE的收敛速度来解决高斯混合专家模型中的Softmax门控问题，研究表明该门控与高斯分布中的专家函数通过偏微分方程相互作用，是一个复杂依赖关系。 |
| [^165] | [An Adaptive Algorithm for Learning with Unknown Distribution Drift.](http://arxiv.org/abs/2305.02252) | 一种适应未知分布漂移的学习算法，相对于当前分布在不需要先验知识的情况下，学习一个函数族，且误差几乎与预先知道漂移大小的学习算法相同。此外，由于该算法适应数据，因此可以保证比依赖于漂移宽松限制的算法具有更好的学习效果。 |
| [^166] | [Synergies Between Federated Learning and O-RAN: Towards an Elastic Virtualized Architecture for Multiple Distributed Machine Learning Services.](http://arxiv.org/abs/2305.02109) | 本文研究了联邦学习在现代无线网络下的挑战，提出了一种方法称为动态多服务联邦学习（DMS-FL）来解决这个问题。同时，还提出了一种名为弹性虚拟化联邦学习（EV-FL）的分布式机器学习架构，来支持DMS-FL中的设计要求。 |
| [^167] | [Domain Agnostic Fourier Neural Operators.](http://arxiv.org/abs/2305.00478) | 介绍了一种新的神经算子架构 DAFNO，可以学习带有不规则几何和不断变化的域的代理。通过将平滑化的特征函数纳入 FNOs 的积分层架构中，并利用 FFT 来实现快速计算，以明确的方式将几何信息编码到架构中，DAFNO 相对于基线神经算子模型具有最先进的精度。 |
| [^168] | [Exploring the Effectiveness of Large Language Models in Generating Unit Tests.](http://arxiv.org/abs/2305.00418) | 本文研究了三种生成模型在单元测试生成方面的效果，并发现在不经过微调的情况下，它们的覆盖率较低且存在测试味道问题。 |
| [^169] | [Towards Automated Circuit Discovery for Mechanistic Interpretability.](http://arxiv.org/abs/2304.14997) | 该论文提出了一种新算法 ACDC，可以自动识别网络中的重要单元，从而实现机制可解释性自动电路发现。 |
| [^170] | [Towards Understanding Feature Learning in Out-of-Distribution Generalization.](http://arxiv.org/abs/2304.11327) | 研究发现，ERM本质上同时学习了具有误导性的特征和不变特征，在ERM预训练期间学习到的特征质量影响了最终的OOD性能，未能捕获所有潜在的有用特征将限制最终的OOD性能。 |
| [^171] | [Learning Dictionaries from Physical-Based Interpolation for Water Network Leak Localization.](http://arxiv.org/abs/2304.10932) | 本文提出一种基于物理插值和字典学习的泄漏定位方法，应用于Modena案例得到了优于现有技术的结果。 |
| [^172] | [Learning Sample Difficulty from Pre-trained Models for Reliable Prediction.](http://arxiv.org/abs/2304.10127) | 该论文介绍了如何使用预训练模型通过熵正则化来计算训练样本的难度，并根据样本难度惩罚过于自信的预测，从而提高模型的准确性和不确定性校准。 |
| [^173] | [Stochastic Distributed Optimization under Average Second-order Similarity: Algorithms and Analysis.](http://arxiv.org/abs/2304.07504) | 本文提出了两种新算法SVRS和AccSVRS，针对分布式优化问题，实现了卓越的通信复杂度。其中，AccSVRS算法实现了完全无平滑性，通信复杂度更是优于现有算法。 |
| [^174] | [Perceptual Quality Assessment of Face Video Compression: A Benchmark and An Effective Method.](http://arxiv.org/abs/2304.07056) | 本文介绍了大规模压缩面部视频质量评估（CFVQA）数据库，用于系统地了解面部视频感知质量和多样化压缩失真。生成式编码方法被确定为具有合理的感知码率失真折衷的有前途的替代方法，利用面部视频的统计先验知识。 |
| [^175] | [Bridging Action Space Mismatch in Learning from Demonstrations.](http://arxiv.org/abs/2304.03833) | 本文提出了一种解决学习演示中行为空间不匹配问题的框架，可以通过其他形态有显着不同的代理的演示进行训练，并且可以从次优演示中学习。 |
| [^176] | [Reliable Learning for Test-time Attacks and Distribution Shift.](http://arxiv.org/abs/2304.03370) | 本文提出了可靠的学习方法以抵御测试时攻击和分布偏移，在测试时引入了新的可靠性保障方法，确保预测结果正确。同时，该学习方法能够适应任意测试点，具有非常好的可靠性。 |
| [^177] | [Making AI Less "Thirsty": Uncovering and Addressing the Secret Water Footprint of AI Models.](http://arxiv.org/abs/2304.03271) | 本论文揭示以及提出了解决人工智能模型巨大水足迹的方法，因为其淡水消耗已经引起国际社会的重视，并且AI模型应该承担社会责任，做出面对水危机的表率。 |
| [^178] | [Query lower bounds for log-concave sampling.](http://arxiv.org/abs/2304.02599) | 该论文研究了对数凹采样的查询下界，在强对数凹和对数光滑分布中采样需要 $\Omega(\log \kappa)$ 查询，在采样高斯分布中需要 $\widetilde \Omega(\min(\sqrt\kappa \log d, d))$ 查询。 |
| [^179] | [EduceLab-Scrolls: Verifiable Recovery of Text from Herculaneum Papyri using X-ray CT.](http://arxiv.org/abs/2304.02084) | 该论文介绍了使用X射线CT图像揭示赫库兰尼姆纸草卷隐藏文本的软件管道和数据集。他们运用了机器学习和几何框架的方法检测“不可见”的碳墨，达到了人类专家标记者难以达到的效果。 |
| [^180] | [Diffusion map particle systems for generative modeling.](http://arxiv.org/abs/2304.00200) | 本文提出一种新型扩散映射粒子系统(DMPS)，可以用于高效生成建模，实验表明在包含流形结构的合成数据集上取得了比其他方法更好的效果。 |
| [^181] | [Fides: A Generative Framework for Result Validation of Outsourced Machine Learning Workloads via TEE.](http://arxiv.org/abs/2304.00083) | 本文提出了一种名为Fides的框架，用于实时验证外协的机器学习工作负载。该框架采用贪心蒸馏迁移学习技术，可动态蒸馏并优化验证模型，以验证对应的服务模型。此外，该框架还能在可信执行环境中运行，以提供更高的安全性和隐私性保证。 |
| [^182] | [Data-driven abstractions via adaptive refinements and a Kantorovich metric [extended version].](http://arxiv.org/abs/2303.17618) | 我们提出了一种基于自适应细化和康托洛维奇度量的智能且可扩展的动态系统抽象技术，并且定义了一种马尔可夫链之间的度量用作损失函数。我们的方法具有更好的计算复杂度。 |
| [^183] | [Pgx: Hardware-accelerated parallel game simulation for reinforcement learning.](http://arxiv.org/abs/2303.17503) | Pgx是一个用JAX编写的游戏模拟器集合，具有强化学习硬件加速能力，支持并行执行，速度比现有的强化学习库快10倍。 它实现了Backgammon，Shogi和Go等基准测试游戏。 |
| [^184] | [Efficient distributed representations beyond negative sampling.](http://arxiv.org/abs/2303.17475) | 本文介绍了一种高效的分布式表示（嵌入）学习方法，通过线性时间估计softmax归一化常数来实现学习过程，该方法优于负采样方法并在多项测试中验证了其有效性。 |
| [^185] | [Optimal approximation of $C^k$-functions using shallow complex-valued neural networks.](http://arxiv.org/abs/2303.16813) | 本文证明了对于$C^k$（在实变量意义下）的函数，使用具有单层隐藏层和$m$个神经元的神经网络可以以错误率$m^{-k/(2n)}$将其逼近。此外，如果选取权值$\sigma_j,b_j\in\mathbb{C}$和$\rho_j\in\mathbb{C}^n$对$f$连续，那么获得的逼近速率是最优的。 |
| [^186] | [Probabilistic inverse optimal control with local linearization for non-linear partially observable systems.](http://arxiv.org/abs/2303.16698) | 本文介绍了一种针对非线性部分可观测系统的局部线性化概率逆优化控制方法，可用于特征化顺序决策任务中的行为，并且具有广泛的适用性。 |
| [^187] | [PDExplain: Contextual Modeling of PDEs in the Wild.](http://arxiv.org/abs/2303.15827) | 我们提出了PDExplain，一种解释性的方法来解决偏微分方程。该算法能够通过提供少量样本的方式，预测未来时间步的PDE解，极大地协助了建立物理科学中基于数据的现象建模。 |
| [^188] | [The Quantization Model of Neural Scaling.](http://arxiv.org/abs/2303.13506) | 该论文提出了神经网络缩放的量化模型，通过将神经网络学习到的功能分为量子并对量子进行递减使用频率的顺序学习，可以解释损失缩放定律，并自动发现语言模型的多样能力。 |
| [^189] | [Object-Centric Slot Diffusion.](http://arxiv.org/abs/2303.10834) | 基于对象中心的槽扩散是一种将扩散模型整合到对象中心学习的新颖方法，它使用Latent Slot Diffusion (LSD)模型替换了传统的槽解码器，同时也是第一个能够无监督进行组合条件扩散的模型。 |
| [^190] | [A New Policy Iteration Algorithm For Reinforcement Learning in Zero-Sum Markov Games.](http://arxiv.org/abs/2303.09716) | 本文提出了一种适用于零和马尔可夫博弈的简单但有效的策略迭代算法。 |
| [^191] | [Identifying Label Errors in Object Detection Datasets by Loss Inspection.](http://arxiv.org/abs/2303.06999) | 本研究首次引入了一个用于目标检测数据集上标签错误检测的基准以及一种标签错误检测方法和几种基线方法。研究模拟了四种不同类型的随机引入的标签错误，并提出了一种通过损失检查来检测这些错误的方法。 |
| [^192] | [Transformer-based Planning for Symbolic Regression.](http://arxiv.org/abs/2303.06833) | 该论文提出了一种基于Transformer和蒙特卡罗树搜索的符号回归规划策略TPSR，可以将非可微的反馈作为知识的外部来源融入到方程生成过程中，提高方程生成的准确性和复杂性。 |
| [^193] | [Clustering with minimum spanning trees: How good can it be?.](http://arxiv.org/abs/2303.05679) | 本文研究了使用最小生成树（MST）进行分区数据聚类任务的意义程度，并发现MST方法在总体上具有很强的竞争力。此外，通过回顾、研究、扩展和推广现有的MST-based划分方案，我们提出了一些新的和值得注意的方法。总体上，Genie和信息论方法往往优于其他非MST算法，在某些情况下MST方法可能不如其他算法。 |
| [^194] | [Towards better traffic volume estimation: Tackling both underdetermined and non-equilibrium problems via a correlation-adaptive graph convolution network.](http://arxiv.org/abs/2303.05660) | 本研究提出基于图卷积网络的方法，解决交通量估计中的不确定和非平衡问题，实现准确的全面交通量估计。 |
| [^195] | [Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning.](http://arxiv.org/abs/2303.05479) | 本文介绍了一种计算机辅助脱机强化学习方法Cal-QL，该方法能够从脱机数据中学习一个保守的值函数初始化，使得在在线微调时同时保障了快速和性能。 |
| [^196] | [A General Theory of Correct, Incorrect, and Extrinsic Equivariance.](http://arxiv.org/abs/2303.04745) | 该论文提出了一个关于正确、错误和外在等变性的普遍理论，通过逐点定义量化了函数表现的每种类型等变性的程度，并研究了不正确或外在对称性对模型错误的影响。实验证实了这些结果。 (230字符) |
| [^197] | [Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction.](http://arxiv.org/abs/2303.04132) | 本文展示了利用不对称性进行合成训练数据生成的方法，可以针对结构化输出问题产生大规模、高质量的数据。在封闭信息提取任务中，我们合成生成了一个包含180万数据点的数据集，并利用此数据集对小型模型进行微调，取得了远超先前领先技术的成果。 |
| [^198] | [On Momentum-Based Gradient Methods for Bilevel Optimization with Nonconvex Lower-Level.](http://arxiv.org/abs/2303.03944) | 本文研究了一类非凸双层优化问题，并提出了一种基于动量的梯度双层方法(MGBiO)来解决这些确定性问题，同时提出了一类基于动量的随机梯度双层方法(MSGBiO和VR-MSGBiO)来解决这些随机问题。通过收敛分析，证明了MGBiO方法具有收敛性。 |
| [^199] | [Client-specific Property Inference against Secure Aggregation in Federated Learning.](http://arxiv.org/abs/2303.03908) | 针对联邦学习中的安全聚合，本研究提出了一种针对客户特定属性推断的解决方案。目前的防御方法包括差分隐私和安全聚合，但它们都存在一定的缺陷。因此，本研究旨在提供一种更有效的保护隐私攻击的方法。 |
| [^200] | [Learning to Influence Human Behavior with Offline Reinforcement Learning.](http://arxiv.org/abs/2303.02265) | 本文提出了一种通过离线强化学习学习影响人类行为的方法，可以提高人类在协作任务中的表现。 |
| [^201] | [Summary Statistic Privacy in Data Sharing.](http://arxiv.org/abs/2303.02014) | 这项研究关注在数据共享中保护摘要统计隐私的问题，提出了衡量隐私风险的度量标准，并证明了隐私和失真之间的权衡存在下界，并提出了一类适用于不同数据分布的量化机制，该机制在某些情况下与下界匹配，最终在实际数据集上展示了更好的隐私-失真权衡的效果。 |
| [^202] | [Revisiting Adversarial Training for ImageNet: Architectures, Training and Generalization across Threat Models.](http://arxiv.org/abs/2303.01870) | 本论文重新研究了在ImageNet上的对抗训练，发现通过轻微改变架构和训练方案可显著提高模型的鲁棒性和泛化能力。修改后的ConvNeXt在已见威胁模型下获得了最鲁棒的结果，而ViT + ConvStem在未见威胁模型下的泛化效果最好。 |
| [^203] | [GlucoSynth: Generating Differentially-Private Synthetic Glucose Traces.](http://arxiv.org/abs/2303.01621) | 本文提出了一种新颖的GAN框架-GlucoSynth，采用差分隐私机制来生成合成血糖轨迹，保证数据隐私安全的同时能生成高质量的合成数据. |
| [^204] | [SHAP-IQ: Unified Approximation of any-order Shapley Interactions.](http://arxiv.org/abs/2303.01179) | 提出了一种名为SHAP-IQ的新方法，用于计算任意阶Shapley互动，并提供了逼近质量的理论保证和方差估计。该方法在计算成本和逼近质量方面优于现有方法。 |
| [^205] | [Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework.](http://arxiv.org/abs/2302.12247) | 通过引入信息分解框架，我们提供了一种量化和建模多模态交互的方法，通过PID统计量来度量输入模态与输出任务之间的冗余度、独特性和协同性，并引入了两个新的PID统计估计器。 |
| [^206] | [Offline Reinforcement Learning for Mixture-of-Expert Dialogue Management.](http://arxiv.org/abs/2302.10850) | 本论文研究了离线强化学习在混合专家对话管理中的应用。通过利用最新的混合专家语言模型（MoE-LMs），我们开发了针对对话规划的强化学习算法，显著减少了动作空间的大小，并提高了对话管理的有效性。 |
| [^207] | [On Calibrating Diffusion Probabilistic Models.](http://arxiv.org/abs/2302.10688) | 本文发现了数据分数随机反向过程是一个鞅，提出了一种简单的方法，用于校准任意预先训练的DPM，有效减小模型的得分匹配损失，增加模型似然的下限，并提供了一般校准指南。 |
| [^208] | [Towards Unbounded Machine Unlearning.](http://arxiv.org/abs/2302.09880) | 本文是第一篇研究不同应用（偏见消除、混淆解决、隐私保护）遗忘问题的论文，提出了适用于不同应用的遗忘定义和指标，并提出了SCRUB，一种在不同应用的遗忘质量度量上始终是顶级表现者的算法。 |
| [^209] | [Stochastic Approximation Approaches to Group Distributionally Robust Optimization.](http://arxiv.org/abs/2302.09267) | 本文提出了一种随机逼近法，用于组分布式鲁棒优化，该算法利用在线学习技术，将每轮所需的样本数从$m$个降至$1$个，同时保持相同的样本复杂度。 |
| [^210] | [Fast Temporal Wavelet Graph Neural Networks.](http://arxiv.org/abs/2302.08643) | 本论文提出了一种快速时域小波图神经网络(FTWGNN)，用于可靠和及时地对人脑和交通网络进行预测。实验结果表明，该方法是高效的。 |
| [^211] | [Bounding Training Data Reconstruction in DP-SGD.](http://arxiv.org/abs/2302.07225) | 本文在DP-SGD的上下文中研究了如何设置隐私参数以保护免受训练数据重构攻击，并提供了相关攻击的上限和匹配的攻击方式。 |
| [^212] | [Near-Optimal Non-Convex Stochastic Optimization under Generalized Smoothness.](http://arxiv.org/abs/2302.06032) | 本文提出了一种在广义光滑性条件下的近似最优非凸随机优化算法解决了大批量大小和只基于预期速率的收敛界限两个问题。 |
| [^213] | [Near-optimal learning with average H\"older smoothness.](http://arxiv.org/abs/2302.06005) | 通过推广平均Lipschitz平滑性到Hölder平滑性，得到了关于平均Hölder平滑性的上下风险界，最优的下界对数因子最多差一个，提供了独立的学习算法。 |
| [^214] | [Towards Multi-User Activity Recognition through Facilitated Training Data and Deep Learning for Human-Robot Collaboration Applications.](http://arxiv.org/abs/2302.05763) | 本研究通过收集单个用户数据并在后处理中合并数据的方法，实现了多用户活动的识别，有望用于人机协作领域。 |
| [^215] | [Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels.](http://arxiv.org/abs/2302.05666) | 本文提出了Jaccard度量损失（JMLs）来优化Jaccard指数，该损失在软标签下仍然有效。 |
| [^216] | [Star-Shaped Denoising Diffusion Probabilistic Models.](http://arxiv.org/abs/2302.05259) | 创新点在于提出了一种非马尔可夫扩散噪声过程的星形降噪扩散概率模型，能够广泛适用于指数族中的多种分布，特别适用于约束流形上的数据。 |
| [^217] | [Outlier-Robust Gromov-Wasserstein for Graph Data.](http://arxiv.org/abs/2302.04610) | 本论文提出了一种针对图数据的异常稳健Gromov-Wasserstein方法（RGW），通过引入乐观扰动的边际约束和使用Bregman近端交替线性化最小化算法，解决了GW距离对异常值敏感的问题。 |
| [^218] | [Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US.](http://arxiv.org/abs/2302.02560) | 本研究提出了一种神经网络方法，利用其理论基础和实施的可行性，从而估计连续暴露/治疗的分布对政策相关结果的因果效应。我们将此方法应用于包含6800万个个体和2700万个美国境内死亡事件的数据中，通过评估美国国家环境保护局（EPA）对PM2.5的国家环境空气质量标准（NAAQS）进行修订后的健康效益。 |
| [^219] | [Nonparametric Density Estimation under Distribution Drift.](http://arxiv.org/abs/2302.02460) | 该论文研究了非参数密度估计在分布漂移下的问题，证明了紧密极小风险界，并推广了先前关于对漂移的无知学习的结果。 |
| [^220] | [Effective Robustness against Natural Distribution Shifts for Models with Different Training Data.](http://arxiv.org/abs/2302.01381) | 本论文提出了一种用于评估和比较在不同训练数据上训练的模型有效鲁棒性的新指标，通过控制所有模型的训练分布的多个ID测试集准确性，提供更准确估计的有效鲁棒性。这有助于解释先前工作中使用ImageNet的CLIP样式零样本模型所展示出的令人惊讶的有效鲁棒性提升。 |
| [^221] | [The contextual lasso: Sparse linear models via deep neural networks.](http://arxiv.org/abs/2302.00878) | 本论文提出了一种新的统计估计器——上下文套索，可以通过深度神经网络的方法解决解释性和拟合能力的矛盾问题，实现对可解释特征的稀疏拟合，并且稀疏模式和系数会随着上下文特征的变化而发生变化。 |
| [^222] | [Test-Time Amendment with a Coarse Classifier for Fine-Grained Classification.](http://arxiv.org/abs/2302.00368) | 本文研究了精细分类中减小错误严重性的问题，提出了一种使用粗粒度分类器进行测试期修正的方法。通过利用标签层级来提高精细分类性能，在iNaturalist-19和tieredImageNet-H数据集上实现了最先进的结果，并在半监督设置中取得了显著的准确率提升。 |
| [^223] | [The geometry of hidden representations of large transformer models.](http://arxiv.org/abs/2302.00294) | 大型Transformer模型中的隐藏表示具有类似的几何和统计特性，随着层级的移动，它们在最初的几层中变得高维，然后在中间层中显著收缩，在模型的最后部分，保持恒定或形成第二个浅峰。在第一个峰值结束时，数据集的语义信息被更好地表达。 |
| [^224] | [Oracle Complexity of Single-Loop Switching Subgradient Methods for Non-Smooth Weakly Convex Functional Constrained Optimization.](http://arxiv.org/abs/2301.13314) | 本文分析了一种交替次梯度法用于非凸优化问题的Oracle复杂度，其中约束函数为凸或弱凸，在只使用单环路的情况下取得了相同的复杂度，并可应用于非光滑问题。 |
| [^225] | [Scaling in Depth: Unlocking Robustness Certification on ImageNet.](http://arxiv.org/abs/2301.12549) | 本文提出了一些新策略和方法解决了证明深度网络稳健性的难点，引入了Linear ResNet架构和Efficient Margin MAximization损失函数，最终实现了新的最先进稳健准确性。 |
| [^226] | [Kernelized Cumulants: Beyond Kernel Mean Embeddings.](http://arxiv.org/abs/2301.12466) | 本文通过核技巧将累计量扩展到再生核希尔伯特空间（RKHS），提供了一组新的通用统计量。超越一阶具有几个优势，并且在计算上具有相同的复杂度和最小的开销。 |
| [^227] | [Neural Relation Graph: A Unified Framework for Identifying Label Noise and Outlier Data.](http://arxiv.org/abs/2301.12321) | 本研究提出了一种利用数据的关系结构识别标签错误和异常数据的统一方法，并提供了可视化工具，在大规模图像、语音和语言领域任务中表现良好。 |
| [^228] | [Alignment with human representations supports robust few-shot learning.](http://arxiv.org/abs/2301.11990) | 论文提出少样本学习的表现与人类表征的一致性存在U形关系，并通过计算机视觉模型的实验进行了验证。高度对齐的模型更加鲁棒，对数据的利用更加有效，但与人类对齐并非必要条件。 |
| [^229] | [Characterization and Learning of Causal Graphs with Small Conditioning Sets.](http://arxiv.org/abs/2301.09028) | 本研究提出了一种使用小的条件集来表征和学习因果图的方法，用于解决约束性因果发现算法在数据有限和条件集较大时的困难。我们定义了k-马尔可夫等价的概念，该概念在不能利用所有条件独立性语句时仍然适用。 |
| [^230] | [Demystify Problem-Dependent Power of Quantum Neural Networks on Multi-Class Classification.](http://arxiv.org/abs/2301.01597) | 本研究揭示了基于问题的量子神经网络在多类分类任务上的功效，发现训练损失主导着其性能，与深度神经分类器的双峰风险曲线相反。此外，发现最优量子神经分类器与Helstrom边界和等角紧框之间存在内在联系，并提出了一种基于最小角度的优化方法。 |
| [^231] | [Invertible normalizing flow neural networks by JKO scheme.](http://arxiv.org/abs/2212.14424) | 本文提出了一种基于JKO方案的可逆归一化流神经网络，通过按块进行残差块的训练，减少了内存负载和深度流网络训练的难度。并且通过自适应时间重新参数化的流网络，在概率空间中逐步细化轨迹，从而提高了模型的训练效率和准确性。 |
| [^232] | [Doubly Smoothed GDA: Global Convergent Algorithm for Constrained Nonconvex-Nonconcave Minimax Optimization.](http://arxiv.org/abs/2212.12978) | 本文提出了一种双重平滑梯度下降上升法 (DSGDA)，该算法可以应用于非凸-非凹极小极大优化，并且能够全局收敛并消除极限环。在一定条件下，DSGDA 的迭代复杂度达到了文献中单循环算法的最佳结果。 |
| [^233] | [Physics-Informed Gaussian Process Regression Generalizes Linear PDE Solvers.](http://arxiv.org/abs/2212.12474) | 本文使用物理学知识指导的高斯过程回归方法，解决线性偏微分方程求解器无法量化近似误差的问题。 |
| [^234] | [A note on the smallest eigenvalue of the empirical covariance of causal Gaussian processes.](http://arxiv.org/abs/2212.09508) | 本研究提出了一个简单的证明方法，用于约束因果高斯过程中经验协方差的最小特征值。同时，我们还建立了一个因果分解，用于建立高斯二次形式的单侧尾部不等式。我们的结果对于向量自回归的最小二乘识别具有性能保证。 |
| [^235] | [On the Sensitivity of Reward Inference to Misspecified Human Models.](http://arxiv.org/abs/2212.04717) | 本研究从理论和实证角度研究了奖励推断对错误人类模型的敏感性，并发现存在可能构建小的对抗性偏差的情况。 |
| [^236] | [Confidence-Conditioned Value Functions for Offline Reinforcement Learning.](http://arxiv.org/abs/2212.04607) | 本文提出了一种条件值函数的学习方法，该方法在离线强化学习中处理了数据集和学习策略之间的分布偏移，并可以在训练和评估时根据保守程度动态选择策略。 |
| [^237] | [Integration of Pre-trained Protein Language Models into Geometric Deep Learning Networks.](http://arxiv.org/abs/2212.03447) | 将预训练的蛋白质语言模型与几何深度学习网络相结合，提高了几何网络的表征能力，并在多个蛋白质学习任务上取得了良好的性能。 |
| [^238] | [One Risk to Rule Them All: Addressing Distributional Shift in Offline Reinforcement Learning via Risk-Aversion.](http://arxiv.org/abs/2212.00124) | 本研究提出了一种基于风险规避机制的离线强化学习方法，同时解决了避免分布偏移和避免灾难性结果的风险问题，并取得了明显的改进。 |
| [^239] | [Approximating Intersections and Differences Between Linear Statistical Shape Models Using Markov Chain Monte Carlo.](http://arxiv.org/abs/2211.16314) | 本文提出了一种使用马尔可夫链蒙特卡洛方法近似计算线性统计形状模型之间相交和差异的新方法。该方法能够对比两个模型的相交空间和集合差异，并生成新的统计形状模型。 |
| [^240] | [Data-Driven Network Neuroscience: On Data Collection and Benchmark.](http://arxiv.org/abs/2211.12421) | 本文提供了一个全面且高质量的人脑功能网络数据集，用于神经科学、机器学习和图分析的交叉研究。它利用解剖学和功能性磁共振成像来理解人脑的功能连接，并具有识别潜在神经退行性疾病的重要性。通过利用机器学习和图分析研究大脑的网络形式，能够预测这些疾病的早期发生。脑网络以图形方式表示，保留了丰富的结构和位置信息，传统的检查方法无法捕捉到这些信息。然而，缺乏公开可访问的脑网络数据限制了数据驱动的研究。 |
| [^241] | [MelHuBERT: A simplified HuBERT on Mel spectrograms.](http://arxiv.org/abs/2211.09944) | MelHuBERT是基于Mel频谱图的简化版HuBERT模型，通过改进损失函数、输入表示和多阶段训练，在语音识别方面取得了有利表现，节省了31.2%的预训练时间和33.5%的计算资源。 |
| [^242] | [Towards Good Practices in Evaluating Transfer Adversarial Attacks.](http://arxiv.org/abs/2211.09565) | 本文提出了良好的实践来解决现有评估中的限制，首次全面评估了转移对抗攻击，并发现了新的攻击特点和最佳超参数。 |
| [^243] | [Transformers over Directed Acyclic Graphs.](http://arxiv.org/abs/2210.13148) | 本文研究了在有向无环图上的Transformer。通过改进注意机制和位置编码，本方法在多种任务上表现出了很好的效果。 |
| [^244] | [On Many-Actions Policy Gradient.](http://arxiv.org/abs/2210.13011) | 本论文研究了具有多个动作样本的随机策略梯度的方差问题，并提出了一种基于动态模型的多动作采样方法，实现了更高的样本效率和更高的回报。 |
| [^245] | [Investigating Neuron Disturbing in Fusing Heterogeneous Neural Networks.](http://arxiv.org/abs/2210.12974) | 该论文提出了研究异构神经网络中神经干扰现象的理论分析和实验验证，并提出了一种通过自适应选择本地模型来执行预测的方法。实验结果表明，该方法在数据异质性方面更加鲁棒。 |
| [^246] | [On Cross-Domain Pre-Trained Language Models for Clinical Text Mining: How Do They Perform on Data-Constrained Fine-Tuning?.](http://arxiv.org/abs/2210.12770) | 本研究探讨了在临床自然语言处理任务中，将从一般领域或相关领域数据预训练的迁移学习模型微调到特定任务上的有效性。实验结果显示，微调的语言模型相对于从头开始学习的模型在命名实体识别任务上取得了更好的性能。 |
| [^247] | [Unbiased constrained sampling with Self-Concordant Barrier Hamiltonian Monte Carlo.](http://arxiv.org/abs/2210.11925) | 本文提出了一种名为BHMC的新的蒙特卡罗采样算法，能够从定义了约束的黎曼流形中进行无偏采样，其中包含一种新的过滤步骤involution checking step。 |
| [^248] | [Rethinking Bias Mitigation: Fairer Architectures Make for Fairer Face Recognition.](http://arxiv.org/abs/2210.09943) | 通过进行公平性的神经架构搜索，我们发现偏见是神经网络架构本身固有的，而不仅仅是训练数据的影响。我们的方法在人脸识别等难题上取得了比其他方法更好的准确性和公平性。 |
| [^249] | [Online Convex Optimization with Unbounded Memory.](http://arxiv.org/abs/2210.09903) | 本论文提出了一种新的在线凸优化框架，可以处理决策历史的长期依赖关系，并介绍了用于量化依赖程度的$p$-有效内存容量的概念。 |
| [^250] | [ConSpec: honing in on critical steps for rapid learning and generalization in RL.](http://arxiv.org/abs/2210.05845) | ConSpec是一个新的强化学习算法，通过离线对比学习来确定任务中的关键步骤，实现快速学习和泛化。该算法通过学习关键步骤的原型，并在当前状态匹配时提供内在奖励，具有快速识别关键步骤和可解释的信用分配的优势。 |
| [^251] | [SimSCOOD: Systematic Analysis of Out-of-Distribution Generalization in Fine-tuned Source Code Models.](http://arxiv.org/abs/2210.04802) | 本文是第一个系统研究超分布泛化问题的方法，通过模拟不同维度的源代码数据属性和微调方法，在不同场景中研究了模型的行为。 |
| [^252] | [A Spectral Approach to Item Response Theory.](http://arxiv.org/abs/2210.04317) | 本文提出了一种基于谱方法的项目反应理论算法，通过计算马尔科夫链的平稳分布来估计模型参数，具有良好的优化性能和有限样本误差保证。 |
| [^253] | [VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment.](http://arxiv.org/abs/2210.04135) | VoLTA是一种采用弱监督对齐策略的视觉语言Transformer模型，通过在本地特征上进行图像和文本的对齐，实现了细粒度的图像理解，无需昂贵的边界框标注。 |
| [^254] | [Learnware: Small Models Do Big.](http://arxiv.org/abs/2210.03647) | Learnware范式旨在帮助用户充分利用小型模型，实现超越原始目的的事情，以解决当前机器学习技术面临的数据量大、训练难度高、数据安全等问题。 |
| [^255] | [Many-body Approximation for Non-negative Tensors.](http://arxiv.org/abs/2209.15338) | 提出了一种名为多体逼近的方法来分解非负张量，通过能量建模来避免全局优化和目标秩选择的困难，可通过考虑模式之间的交互进行全局优化; 在许多任务中都展示了其有效性。 |
| [^256] | [Hyperbolic VAE via Latent Gaussian Distributions.](http://arxiv.org/abs/2209.15217) | 这项研究提出了一种通过使用高斯流形的潜空间来改进变分自编码器(VAE)的方法。实验证明，这种GM-VAE方法在密度估计和模型驱动的强化学习任务中表现出色，具有较强的数值稳定性。 |
| [^257] | [Private Stochastic Optimization With Large Worst-Case Lipschitz Parameter: Optimal Rates for (Non-Smooth) Convex Losses and Extension to Non-Convex Losses.](http://arxiv.org/abs/2209.07403) | 本论文研究了具有大的最坏情况Lipschitz参数的差分隐私随机优化问题，并提供了一种不依赖于统一Lipschitz参数的接近最优的过量风险界限方法。 |
| [^258] | [Studying Drowsiness Detection Performance while Driving through Scalable Machine Learning Models using Electroencephalography.](http://arxiv.org/abs/2209.04048) | 本研究通过使用脑电图和机器学习的智能框架，在驾驶场景中检测驾驶员嗜睡状态。结果表明，随机森林（RF）是性能最佳的模型，相比支持向量机（SVM）有更高的f1分数。 |
| [^259] | [Learning Invariant Representations under General Interventions on the Response.](http://arxiv.org/abs/2208.10027) | 本文研究了在响应被干预的情况下学习不变表示的问题，提出了不变匹配属性（IMP）作为一种捕捉干预关系的方法。 |
| [^260] | [Federated Learning for Medical Applications: A Taxonomy, Current Trends, Challenges, and Future Research Directions.](http://arxiv.org/abs/2208.03392) | 本文调查了联邦学习在医疗应用中的分类、当前趋势、挑战和未来研究方向。该调查强调了联邦学习在保护隐私和解决安全问题方面的重要性。 |
| [^261] | [Dive into Machine Learning Algorithms for Influenza Virus Host Prediction with Hemagglutinin Sequences.](http://arxiv.org/abs/2207.13842) | 本研究使用机器学习算法，以血凝素序列为基础，利用位置特异性评分矩阵和词嵌入的方法，通过多种评估指标在不同分类水平上评估了机器学习算法，并发现5-grams-transformer神经网络是最有效的算法，可以准确预测流感病毒序列的起源。 |
| [^262] | [Language models show human-like content effects on reasoning tasks.](http://arxiv.org/abs/2207.07051) | 本研究探讨了语言模型在逻辑推理任务中是否像人类一样通过混入内容来影响答案，结果发现大型语言模型的先验期望能够捕捉到这种特征。 |
| [^263] | [What is Flagged in Uncertainty Quantification? Latent Density Models for Uncertainty Categorization.](http://arxiv.org/abs/2207.05161) | 本论文提出了一种分类框架，用于对不确定性量化方法标记的不确定示例进行分类。通过引入混淆密度矩阵，并将示例分为分布外、边界和高分布误分类区域的三类，本研究为评估不同的不确定性量化方法提供了新的视角和评估基准。 |
| [^264] | [Robustness Evaluation of Deep Unsupervised Learning Algorithms for Intrusion Detection Systems.](http://arxiv.org/abs/2207.03576) | 本论文评估了六种最新的深度学习算法在受污染数据上的鲁棒性，结果表明这些算法对于数据污染非常敏感，强调了开发新模型时对于数据扰动的自我防御的重要性。 |
| [^265] | [Fairness and Bias in Robot Learning.](http://arxiv.org/abs/2207.03444) | 本文从技术、伦理和法律角度出发，首次调查了机器人学习中的公平性问题。提出了偏见来源的分类法和由其引起的歧视类型，并探讨了不同机器人学习领域中不公平结果的场景和缓解策略。 |
| [^266] | [Semi-supervised Contrastive Outlier removal for Pseudo Expectation Maximization (SCOPE).](http://arxiv.org/abs/2206.14261) | SCOPE 提出了一种名为半监督对比异常值剔除的伪最大期望化方法，通过抑制混淆错误来提高半监督学习的性能。 |
| [^267] | [Interactive Visual Reasoning under Uncertainty.](http://arxiv.org/abs/2206.09203) | 该论文介绍了一个名为IVRE的交互式环境，在不确定性下评估人工智能代理的推理能力。研究人员通过在IVRE中设置不确定的动作-效果对，要求代理确定对象的角色，并鼓励代理基于观察提出有效且高效的实验来验证假设。 |
| [^268] | [Evaluating and Inducing Personality in Pre-trained Language Models.](http://arxiv.org/abs/2206.07550) | 本文提出了Machine Personality Inventory (MPI)数据集用于评估机器个性，通过评估，证明了预训练语言模型(LLM)具有个性。进一步提出了Personality Prompting (P^2)方法，用于以可控的方式诱导LLM具有特定个性。 |
| [^269] | [Complex Locomotion Skill Learning via Differentiable Physics.](http://arxiv.org/abs/2206.02341) | 本研究提出了一个实用的学习框架，通过可微分物理实现了统一的神经网络控制器，使其具备更高复杂性和多样性。与强化学习相比，该框架的学习效果更好且收敛速度更快。 |
| [^270] | [Augmentation-Aware Self-Supervision for Data-Efficient GAN Training.](http://arxiv.org/abs/2205.15677) | 本文提出一种增强感知的自监督判别器用于对生成数据及其增强参数的预测，从而提高判别器的表现和生成模型的性能，实现了数据有效的 GAN 训练。 |
| [^271] | [DELTA: Diverse Client Sampling for Fasting Federated Learning.](http://arxiv.org/abs/2205.13925) | DELTA 提出了一个无偏抽样方案来减少部分客户端参与所引起的方差，以缓解现有抽样方法可能导致性能下降的问题，它考虑了客户端的多样性和局部方差的影响，并选择具有全局模型更新所需有价值信息的代表性客户端。实验结果表明，DELTA 可以优于其他无偏抽样方案并加速模型收敛速度。 |
| [^272] | [INSPIRE: Distributed Bayesian Optimization for ImproviNg SPatIal REuse in Dense WLANs.](http://arxiv.org/abs/2204.10184) | INSPIRE是一种面向密集WLAN中空间复用的分布式贝叶斯优化解决方案，通过无线传输参数的优化，提高WLAN的性能。 |
| [^273] | [On Parametric Optimal Execution and Machine Learning Surrogates.](http://arxiv.org/abs/2204.08581) | 本论文研究了离散时间中具有瞬时价格影响和随机弹性的最优订单执行问题。首先，我们扩展了线性瞬时价格影响情况下的最优策略，然后开发了一个基于动态规划和深度学习的数值算法来处理非线性瞬时价格影响情况。我们的方法利用神经网络替代品在参数化学习中具有灵活可伸缩性，对于精确校准价格影响和弹性等参数非常重要。 |
| [^274] | [Speculative Decoding: Lossless Speedup of Autoregressive Translation.](http://arxiv.org/abs/2203.16487) | Speculative Decoding是一种新型解码范式，结合了自回归翻译（AT）和非自回归翻译（NAT）的优势，提供了无损加速的翻译方法。在每个解码步骤中，它推测性地预测下一个标记，并使用验证模型确保翻译结果与AT完全相同。通过推测解码和验证的协作，实现了更快的解码速度，同时保持翻译质量不变。实验证明，原始的SpecDec与AT贪婪解码的结果完全相同。 |
| [^275] | [Treatment Learning Causal Transformer for Noisy Image Classification.](http://arxiv.org/abs/2203.15529) | 本研究提出了一种处理学习因果Transformer（TLT）架构，用于噪声图像分类。通过将"存在噪声"的信息作为处理输入，并联合估计处理效果，TLT能够提高预测准确性，并使用潜在生成模型估计鲁棒的特征表示。 |
| [^276] | [Geodesic Multi-Modal Mixup for Robust Fine-Tuning.](http://arxiv.org/abs/2203.03897) | 本文研究了CLIP模型的多模态嵌入质量，并发现其统一性和对齐性不足，限制了嵌入的传递性和鲁棒性。为了解决这个问题，我们提出了一种新的鲁棒微调方法，通过高度几何多模型混合生成难负样本，并对模型进行微调。 |
| [^277] | [Distributionally Robust Bayesian Optimization with $\phi$-divergences.](http://arxiv.org/abs/2203.02128) | 本研究提出了一种基于$\phi$-离散度的分布鲁棒贝叶斯优化算法。 |
| [^278] | [Improving Molecular Representation Learning with Metric Learning-enhanced Optimal Transport.](http://arxiv.org/abs/2202.06208) | 本文提出了一种基于最优传输的算法MROT，用于改进分子表示学习和增强其泛化能力。实验证明，MROT在化学性质预测和材料吸附选择等任务中显著优于现有模型，具有加速发现具有期望性质的新物质的潜力。 |
| [^279] | [Leveraging Bitstream Metadata for Fast, Accurate, Generalized Compressed Video Quality Enhancement.](http://arxiv.org/abs/2202.00011) | 本文提出了一种利用视频比特流元数据进行深度学习的压缩视频质量增强方法，在实现更高吞吐量的同时提高了压缩视频的还原准确性，具有重要的应用价值。 |
| [^280] | [Learning with Subset Stacking.](http://arxiv.org/abs/2112.06251) | 提出了一种新的回归算法LESS，通过生成以随机点为中心的子集并训练局部预测器，然后以新颖的方式组合预测器得到整体预测器。在多个数据集上测试表明，LESS是一种有竞争力且高效的监督学习方法。 |
| [^281] | [CubeTR: Learning to Solve The Rubiks Cube Using Transformers.](http://arxiv.org/abs/2111.06036) | CubeTR提出了一种使用Transformer进行强化学习解决魔方问题的方法，并通过关注长序列动作和解决稀疏奖励的问题，实现了从任意起始状态学习如何解决魔方问题，并能够生成接近专业人员解决方案长度的解决方案。 |
| [^282] | [Lipschitz Bandits with Batched Feedback.](http://arxiv.org/abs/2110.09722) | 本文研究了带有批量反馈的Lipschitz贪婪问题，并提出了一种名为BLiN的新颖算法，该算法可最优地解决该问题。算法在理论上达到了最优遗憾率，并仅使用最小的通信成本。 |
| [^283] | [Neural Model Reprogramming with Similarity Based Mapping for Low-Resource Spoken Command Classification.](http://arxiv.org/abs/2110.03894) | 本文提出了一种基于相似性映射的神经模型重编程方法，用于低资源口语命令分类。实验证明，在有限的数据条件下，该方法在阿拉伯语和立陶宛语命令数据集上表现优于当前最先进的结果。 |
| [^284] | [FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Medical Imaging.](http://arxiv.org/abs/2109.09658) | 本论文介绍了一系列从经验、共识和最佳实践中提炼出的指导原则，旨在引领医学影像中值得信赖的人工智能的发展，提高信任、安全性和应用水平。 |
| [^285] | [Medical Profile Model: Scientific and Practical Applications in Healthcare.](http://arxiv.org/abs/2107.03913) | 本论文提出了一种医疗档案模型，利用Transformer神经网络进行无监督学习，将患者病史表示为疾病的时间序列，并包含人口统计参数，可以成功将医疗知识转移到其他领域。模型训练于超过一百万名患者的数据集，表明在诊断预测任务中具有明显优势。另外，还展示了两个基于该档案模型的应用：一种揭示与疾病相关的假设的新颖方法和从档案模型中提取的患者嵌入。 |
| [^286] | [An XAI Approach to Deep Learning Models in the Detection of DCIS.](http://arxiv.org/abs/2106.14186) | 该研究证明了XAI可用于证明辅助人工智能系统的可行性，有效地应用于医疗领域。 |
| [^287] | [On the Role of Entropy-based Loss for Learning Causal Structures with Continuous Optimization.](http://arxiv.org/abs/2106.02835) | 发现因果结构的任务是重要且具有挑战性的，在连续优化中使用的最小二乘损失函数受限于高斯噪声假设。在本研究中，我们提出了一种理论上与任何噪声分布一致的基于熵的损失函数来克服这一限制。 |
| [^288] | [Reservoir Computing with Magnetic Thin Films.](http://arxiv.org/abs/2101.12700) | 本研究探索了三种磁性薄膜的储层计算，该计算通过利用系统的内部动力学实现非线性投影，提升了模式识别和时间序列分析等任务的性能。 |
| [^289] | [Continuous Conditional Generative Adversarial Networks: Novel Empirical Losses and Label Input Mechanisms.](http://arxiv.org/abs/2011.07466) | 本文提出了连续条件生成对抗网络（CcGAN），首个用于基于连续标量条件的图像生成的生成模型。通过重新构建经验cGAN损失和提出新的标签输入方法，解决了在回归标签条件生成中存在的问题。 |
| [^290] | [BUSTLE: Bottom-Up Program Synthesis Through Learning-Guided Exploration.](http://arxiv.org/abs/2007.14381) | 这种论文提出了一种利用学习指导自下而上搜索的程序合成方法，通过训练模型优先考虑中间值的组合，利用中间程序的语义信息和具体值的特性进行灵活的解决方案生成。 |
| [^291] | [BSDAR: Beam Search Decoding with Attention Reward in Neural Keyphrase Generation.](http://arxiv.org/abs/1909.09485) | 本研究提出了一种基于注意力奖励的束搜索解码策略，用于解决神经关键词生成中的序列长度偏差和束多样性问题，该方法显著提高了生成关键词的解码性能。 |
| [^292] | [Testing Robustness Against Unforeseen Adversaries.](http://arxiv.org/abs/1908.08016) | 该论文提出了18种新的对抗攻击，并使用这些攻击创建了一个用于评估对各种未预料到的对手的鲁棒性的新基准。作者还发现了一系列防御策略，可以帮助克服训练期间未考虑到的对手的泛化差距。该研究的结果将为研究现实世界最坏情况下的鲁棒性提供有用工具，促进开发更强大的防御措施。 |
| [^293] | [Weighted bandits or: How bandits learn distorted values that are not expected.](http://arxiv.org/abs/1611.10283) | 本论文研究了带有扭曲概率的随机多臂赌博机问题，并提出了以UCB算法为基础、考虑了奖励扭曲并具有次线性后悔的算法。 |

# 详细

[^1]: LagrangeBench: 一种拉格朗日流体力学基准测试套件

    LagrangeBench: A Lagrangian Fluid Mechanics Benchmarking Suite. (arXiv:2309.16342v1 [cs.LG])

    [http://arxiv.org/abs/2309.16342](http://arxiv.org/abs/2309.16342)

    LagrangeBench是第一个针对拉格朗日粒子问题的基准测试套件，提供了七个新的流体力学数据集（包括不同维度和物理特性），以及高效的API和已建立的图神经网络的JAX实现。

    

    机器学习在各种科学应用中成功应用于基于网格的偏微分方程建模。然而，基于拉格朗日粒子离散化的学习PDE求解器，在涉及自由表面或复杂物理问题时仍然很少被探索。我们提出了LagrangeBench，这是针对拉格朗日粒子问题的第一个基准测试套件，重点是时间粗粒化。特别地，我们的贡献是：(a)使用平滑粒子流体动力学（SPH）方法生成的七个新的流体力学数据集（其中四个是2D的，三个是3D的），包括了Taylor-Green涡旋、驱动上盖、反Poiseuille流动和断坝等不同物理特性，如固体壁相互作用或自由表面，(b)高效的基于JAX的API，配备不同的近期训练策略和邻居搜索例程，以及(c)已建立的图神经网络（GNNs）如GNS和SEGNN的JAX实现与基准结果。最后，为了度量...

    Machine learning has been successfully applied to grid-based PDE modeling in various scientific applications. However, learned PDE solvers based on Lagrangian particle discretizations, which are the preferred approach to problems with free surfaces or complex physics, remain largely unexplored. We present LagrangeBench, the first benchmarking suite for Lagrangian particle problems, focusing on temporal coarse-graining. In particular, our contribution is: (a) seven new fluid mechanics datasets (four in 2D and three in 3D) generated with the Smoothed Particle Hydrodynamics (SPH) method including the Taylor-Green vortex, lid-driven cavity, reverse Poiseuille flow, and dam break, each of which includes different physics like solid wall interactions or free surface, (b) efficient JAX-based API with various recent training strategies and neighbors search routine, and (c) JAX implementation of established Graph Neural Networks (GNNs) like GNS and SEGNN with baseline results. Finally, to measu
    
[^2]: ADGym：深度异常检测的设计选择

    ADGym: Design Choices for Deep Anomaly Detection. (arXiv:2309.15376v1 [cs.LG])

    [http://arxiv.org/abs/2309.15376](http://arxiv.org/abs/2309.15376)

    ADGym是一款针对深度异常检测的设计选择的综合评估和自动选择平台。

    

    深度学习（DL）技术最近被应用于异常检测（AD），在金融、医疗服务和云计算等领域取得了成功的成果。然而，目前的研究往往将深度AD算法作为一个整体进行评估，未能理解个别设计选择（如损失函数和网络架构）的贡献。因此，原始步骤（如预处理）的重要性可能被新颖的损失函数和架构所掩盖。在本文中，我们通过提出两个问题来解决这些疏漏：（i）深度AD方法的哪些组成部分（即设计选择）在检测异常方面是至关重要的？（ii）我们如何通过自动选择最佳设计选择来构建针对特定数据集的定制AD算法，而不是依赖通用的、预先存在的解决方案？为此，我们介绍了ADGym，这是第一个用于全面评估和自动选择AD的平台。

    Deep learning (DL) techniques have recently been applied to anomaly detection (AD), yielding successful outcomes in areas such as finance, medical services, and cloud computing. However, much of the current research evaluates a deep AD algorithm holistically, failing to understand the contributions of individual design choices like loss functions and network architectures. Consequently, the importance of prerequisite steps, such as preprocessing, might be overshadowed by the spotlight on novel loss functions and architectures. In this paper, we address these oversights by posing two questions: (i) Which components (i.e., design choices) of deep AD methods are pivotal in detecting anomalies? (ii) How can we construct tailored AD algorithms for specific datasets by selecting the best design choices automatically, rather than relying on generic, pre-existing solutions? To this end, we introduce ADGym, the first platform designed for comprehensive evaluation and automatic selection of AD d
    
[^3]: 非平稳强化学习中的节奏适应

    Tempo Adaption in Non-stationary Reinforcement Learning. (arXiv:2309.14989v1 [cs.LG])

    [http://arxiv.org/abs/2309.14989](http://arxiv.org/abs/2309.14989)

    该论文解决了非平稳强化学习中"时间同步"问题，通过考虑墙钟时间而不是情节进展来实现对环境变化的适应。

    

    首先我们提出并解决了非平稳强化学习中的“时间同步”问题，这是阻碍其在真实世界应用中的一个关键因素。现实中，环境的变化是按照墙钟时间（$\mathfrak{t}$）而不是按照情节进展（$k$）发生的，其中墙钟时间表示固定持续时间$\mathfrak{t} \in [0, T]$内实际流逝的时间。在现有的工作中，在情节$k$时，智能体生成一个轨迹并训练一个策略，然后转入情节$k+1$。然而，在时间不同步的环境下，智能体在时间$\mathfrak{t}_k$分配$\Delta \mathfrak{t}$用于轨迹生成和训练，然后在$\mathfrak{t}_{k+1}=\mathfrak{t}_{k}+\Delta \mathfrak{t}$时刻转入下一个情节。尽管情节总数固定（$K$），智能体根据相互作用时间的选择（$\mathfrak{t}_1,\mathfrak{t}_2,...,\mathfrak{t}_K$）积累不同的轨迹。

    We first raise and tackle ``time synchronization'' issue between the agent and the environment in non-stationary reinforcement learning (RL), a crucial factor hindering its real-world applications. In reality, environmental changes occur over wall-clock time ($\mathfrak{t}$) rather than episode progress ($k$), where wall-clock time signifies the actual elapsed time within the fixed duration $\mathfrak{t} \in [0, T]$. In existing works, at episode $k$, the agent rollouts a trajectory and trains a policy before transitioning to episode $k+1$. In the context of the time-desynchronized environment, however, the agent at time $\mathfrak{t}_k$ allocates $\Delta \mathfrak{t}$ for trajectory generation and training, subsequently moves to the next episode at $\mathfrak{t}_{k+1}=\mathfrak{t}_{k}+\Delta \mathfrak{t}$. Despite a fixed total episode ($K$), the agent accumulates different trajectories influenced by the choice of \textit{interaction times} ($\mathfrak{t}_1,\mathfrak{t}_2,...,\mathfra
    
[^4]: 基于ODE的无模型反馈强化学习用于POMDPs

    ODE-based Recurrent Model-free Reinforcement Learning for POMDPs. (arXiv:2309.14078v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.14078](http://arxiv.org/abs/2309.14078)

    本论文提出了一种基于ODE的循环模型结合无模型强化学习来解决部分可观察的马尔可夫决策过程（POMDPs）。实验结果表明该方法在PO连续控制和元强化学习任务中表现出了良好的效果，并且对于不规则观察具有鲁棒性。

    

    神经常微分方程（ODEs）被广泛认为是建模物理机制的标准，它们有助于在未知的物理或生物环境中进行近似推断。在部分可观测（PO）环境中，如何从原始观测中推断看不见的信息困扰着代理人。通过使用具有紧凑上下文的循环策略，基于上下文的强化学习提供了一种灵活的方法来从历史转换中提取不可观察的信息。为了帮助代理人提取更多与动态相关的信息，我们提出了一种新颖的基于ODE的循环模型，并结合了无模型的强化学习框架来解决部分可观察的马尔可夫决策过程（POMDPs）。我们通过各种PO连续控制和元强化学习任务实验验证了我们方法的有效性。此外，我们的实验表明，由于ODEs具有建模不规则性的能力，我们的方法对于不规则观察具有鲁棒性。

    Neural ordinary differential equations (ODEs) are widely recognized as the standard for modeling physical mechanisms, which help to perform approximate inference in unknown physical or biological environments. In partially observable (PO) environments, how to infer unseen information from raw observations puzzled the agents. By using a recurrent policy with a compact context, context-based reinforcement learning provides a flexible way to extract unobservable information from historical transitions. To help the agent extract more dynamics-related information, we present a novel ODE-based recurrent model combines with model-free reinforcement learning (RL) framework to solve partially observable Markov decision processes (POMDPs). We experimentally demonstrate the efficacy of our methods across various PO continuous control and meta-RL tasks. Furthermore, our experiments illustrate that our method is robust against irregular observations, owing to the ability of ODEs to model irregularl
    
[^5]: FeCAM：在免去样本的连续学习中利用类别分布的异质性

    FeCAM: Exploiting the Heterogeneity of Class Distributions in Exemplar-Free Continual Learning. (arXiv:2309.14062v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.14062](http://arxiv.org/abs/2309.14062)

    本文针对免去样本的增量式学习（CIL）中的异质性类别分布问题，使用原型网络和改进的各向异性马哈拉诺比斯距离进行特征分类和建模，有效解决了非恒定数据学习中的特征分布异质性挑战。

    

    免去样本的增量式学习（CIL）面临着许多挑战，因为它禁止了来自先前任务的数据回顾，从而导致了灾难性遗忘。最近的增量学习方法通过在第一个任务之后冻结特征提取器来学习分类器，已经引起了广泛关注。在本文中，我们探索了用于CIL的原型网络，该网络使用冻结的特征提取器生成新的类别原型，并根据到原型的欧氏距离对特征进行分类。通过对类别特征分布进行分析，我们发现基于欧氏度量的分类对于联合训练的特征是成功的。然而，当从非恒定数据中学习时，我们观察到欧氏度量是次优的，并且特征分布是异质的。为了解决这个挑战，我们重新审视了用于CIL的各向异性马哈拉诺比斯距离。此外，我们通过实验证明了建模特征协方差关系的重要性。

    Exemplar-free class-incremental learning (CIL) poses several challenges since it prohibits the rehearsal of data from previous tasks and thus suffers from catastrophic forgetting. Recent approaches to incrementally learning the classifier by freezing the feature extractor after the first task have gained much attention. In this paper, we explore prototypical networks for CIL, which generate new class prototypes using the frozen feature extractor and classify the features based on the Euclidean distance to the prototypes. In an analysis of the feature distributions of classes, we show that classification based on Euclidean metrics is successful for jointly trained features. However, when learning from non-stationary data, we observe that the Euclidean metric is suboptimal and that feature distributions are heterogeneous. To address this challenge, we revisit the anisotropic Mahalanobis distance for CIL. In addition, we empirically show that modeling the feature covariance relations is b
    
[^6]: 基于物理驱动的机器学习模型用于修正逆问题估计

    Physics-Driven ML-Based Modelling for Correcting Inverse Estimation. (arXiv:2309.13985v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.13985](http://arxiv.org/abs/2309.13985)

    本文提出了一种基于物理驱动的机器学习模型，用于检测和纠正在科学和工程领域中的状态估计问题。其中，该模型通过优化算法来减少模拟成本，并通过两个生成模型近似候选状态的概率分布，以提高估计的准确性和效率。

    

    在科学和工程领域部署机器学习估计器时，避免失败的估计是至关重要的，因为它可能会产生灾难性后果，例如在航空发动机设计中。本文重点在于利用模拟和受物理定律指导的性能指标，在采用SAE逆问题时检测和纠正失败的状态估计。我们建议在其物理模型误差超过可行阈值时标记机器学习估计，并提出了一种新方法GEESE来通过优化来纠正，旨在提供低误差和高效性。GEESE的关键设计包括（1）混合代理误差模型，以提供快速的误差估计以减少模拟成本，并实现基于梯度的误差反馈传播，以及（2）两个生成模型，以近似候选状态的概率分布，用于模拟开发和探索行为。这三个模型是协同的。

    When deploying machine learning estimators in science and engineering (SAE) domains, it is critical to avoid failed estimations that can have disastrous consequences, e.g., in aero engine design. This work focuses on detecting and correcting failed state estimations before adopting them in SAE inverse problems, by utilizing simulations and performance metrics guided by physical laws. We suggest to flag a machine learning estimation when its physical model error exceeds a feasible threshold, and propose a novel approach, GEESE, to correct it through optimization, aiming at delivering both low error and high efficiency. The key designs of GEESE include (1) a hybrid surrogate error model to provide fast error estimations to reduce simulation cost and to enable gradient based backpropagation of error feedback, and (2) two generative models to approximate the probability distributions of the candidate states for simulating the exploitation and exploration behaviours. All three models are co
    
[^7]: 图对比学习的可证明训练方法

    Provable Training for Graph Contrastive Learning. (arXiv:2309.13944v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.13944](http://arxiv.org/abs/2309.13944)

    图对比学习中，我们发现训练存在不平衡的问题，为此我们提出了“节点紧凑性”度量来指导训练。

    

    图对比学习（GCL）已经成为一种从增强图中学习节点嵌入而无需标签的流行训练方法。尽管最大化正节点对之间的相似性并最小化负节点对之间的相似性的关键原则已经得到确认，但仍存在一些基本问题。考虑到复杂的图结构，是否有一些节点始终按照这一原则进行良好训练，即使在不同的图增强方法下也是如此？还是有一些节点更有可能在图增强中未经训练，并违反这一原则？如何区分这些节点并进一步指导GCL的训练？为了回答这些问题，我们首先提出了实验证据，表明GCL的训练在所有节点上确实存在不平衡。为了解决这个问题，我们提出了度量“节点紧凑性”，它是节点遵循GCL原则与增强范围相关的下界。我们进一步导出了

    Graph Contrastive Learning (GCL) has emerged as a popular training approach for learning node embeddings from augmented graphs without labels. Despite the key principle that maximizing the similarity between positive node pairs while minimizing it between negative node pairs is well established, some fundamental problems are still unclear. Considering the complex graph structure, are some nodes consistently well-trained and following this principle even with different graph augmentations? Or are there some nodes more likely to be untrained across graph augmentations and violate the principle? How to distinguish these nodes and further guide the training of GCL? To answer these questions, we first present experimental evidence showing that the training of GCL is indeed imbalanced across all nodes. To address this problem, we propose the metric "node compactness", which is the lower bound of how a node follows the GCL principle related to the range of augmentations. We further derive the
    
[^8]: 鲁棒性分布式学习：在数据异构性下的严格误差界和断点分析

    Robust Distributed Learning: Tight Error Bounds and Breakdown Point under Data Heterogeneity. (arXiv:2309.13591v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.13591](http://arxiv.org/abs/2309.13591)

    在数据异构性下，鲁棒性分布式学习算法在更实际的数据异构性模型下得到了改进，在学习误差下界和断点分析方面取得了新的结果。

    

    鲁棒性分布式学习算法在数据均匀性的情况下与实际观测吻合，但在实际场景中的数据异构性下，现有的学习误差下界基本上是空洞的，并且与实际观测明显不匹配。本文考虑了更加实际的数据异构性模型，称为（G，B）-梯度差异度，并证明它涵盖了比现有理论更多的学习问题。特别地，我们展示了在数据异构性下的断点分析低于经典的1/2。我们还证明了任何分布式学习算法的学习误差的新下界。我们导出了分布式梯度下降的鲁棒变种的匹配上界，并通过实验证明了我们的分析结果。

    The theory underlying robust distributed learning algorithms, designed to resist adversarial machines, matches empirical observations when data is homogeneous. Under data heterogeneity however, which is the norm in practical scenarios, established lower bounds on the learning error are essentially vacuous and greatly mismatch empirical observations. This is because the heterogeneity model considered is too restrictive and does not cover basic learning tasks such as least-squares regression. We consider in this paper a more realistic heterogeneity model, namely (G,B)-gradient dissimilarity, and show that it covers a larger class of learning problems than existing theory. Notably, we show that the breakdown point under heterogeneity is lower than the classical fraction 1/2. We also prove a new lower bound on the learning error of any distributed learning algorithm. We derive a matching upper bound for a robust variant of distributed gradient descent, and empirically show that our analysi
    
[^9]: 焦点中的湍流：用BLASTNet 2.0数据基准测量三维体积超分辨率的缩放行为

    Turbulence in Focus: Benchmarking Scaling Behavior of 3D Volumetric Super-Resolution with BLASTNet 2.0 Data. (arXiv:2309.13457v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.13457](http://arxiv.org/abs/2309.13457)

    本研究提出了BLASTNet 2.0，包含三维高保真压缩湍流流动模拟数据，通过对五种深度学习方法的基准测试和神经缩放分析，揭示了模型规模、成本和架构对预测性能的影响。

    

    压缩湍流流动的分析对推进、能源生成和环境相关应用至关重要。本文介绍了BLASTNet 2.0，它是一个包含744个完整域样本来自34个高保真直接数值模拟的2.2TB数据集网络，旨在解决目前三维高保真反应和非反应压缩湍流流动模拟数据有限的问题。利用这些数据，我们基准测试了49种不同的深度学习方法的五个变体，用于改进科学成像、模拟、湍流模型以及计算机视觉应用。我们对这些模型进行了神经缩放分析，以检查不同机器学习（ML）方法的性能，包括两种科学ML技术。我们证明了（i）预测性能可以随模型规模和成本而扩展，（ii）架构尤其对较小的模型有重要影响，以及（iii）b...

    Analysis of compressible turbulent flows is essential for applications related to propulsion, energy generation, and the environment. Here, we present BLASTNet 2.0, a 2.2 TB network-of-datasets containing 744 full-domain samples from 34 high-fidelity direct numerical simulations, which addresses the current limited availability of 3D high-fidelity reacting and non-reacting compressible turbulent flow simulation data. With this data, we benchmark a total of 49 variations of five deep learning approaches for 3D super-resolution - which can be applied for improving scientific imaging, simulations, turbulence models, as well as in computer vision applications. We perform neural scaling analysis on these models to examine the performance of different machine learning (ML) approaches, including two scientific ML techniques. We demonstrate that (i) predictive performance can scale with model size and cost, (ii) architecture matters significantly, especially for smaller models, and (iii) the b
    
[^10]: Riemannian流形上Matern高斯过程的后验收缩速率

    Posterior Contraction Rates for Mat\'ern Gaussian Processes on Riemannian Manifolds. (arXiv:2309.10918v1 [stat.ML])

    [http://arxiv.org/abs/2309.10918](http://arxiv.org/abs/2309.10918)

    该论文研究了定义在紧致Riemannian流形上的内在Matern高斯过程和外在过程之间的收缩速率，并发现它们的速率在适当匹配平滑参数的情况下是相等的。

    

    高斯过程在许多依赖于不确定性量化的机器学习应用中被使用。最近，已经开发了在几何设置下处理这些模型的计算工具，例如，当输入位于Riemannian流形上时。这引出了一个问题：这些内在模型在理论上是否可以证明相比于将所有相关量嵌入到$\mathbb{R}^d$并使用普通欧几里德高斯过程的限制，可以带来更好的性能？为了研究这个问题，我们证明了定义在紧致Riemannian流形上的内在Matern高斯过程的最优收缩速率。我们还通过流形和环境Sobolev空间之间的迹和扩展定理证明了外在过程的类似速率：令人惊讶的是，所得到的速率与内在过程的速率相符，前提是它们的平滑参数适当匹配。我们在一些实证数据上进行了对这些速率的演示。

    Gaussian processes are used in many machine learning applications that rely on uncertainty quantification. Recently, computational tools for working with these models in geometric settings, such as when inputs lie on a Riemannian manifold, have been developed. This raises the question: can these intrinsic models be shown theoretically to lead to better performance, compared to simply embedding all relevant quantities into $\mathbb{R}^d$ and using the restriction of an ordinary Euclidean Gaussian process? To study this, we prove optimal contraction rates for intrinsic Mat\'ern Gaussian processes defined on compact Riemannian manifolds. We also prove analogous rates for extrinsic processes using trace and extension theorems between manifold and ambient Sobolev spaces: somewhat surprisingly, the rates obtained turn out to coincide with those of the intrinsic processes, provided that their smoothness parameters are matched appropriately. We illustrate these rates empirically on a number of
    
[^11]: 集群化的多智能体线性赌博机

    Clustered Multi-Agent Linear Bandits. (arXiv:2309.08710v1 [cs.LG])

    [http://arxiv.org/abs/2309.08710](http://arxiv.org/abs/2309.08710)

    本文研究了集群化的多智能体线性赌博机问题，提出了一种新颖的算法，通过智能体之间的协作来加速优化问题。通过理论分析和实证评估，证明了算法在遗憾最小化和聚类质量上的有效性。

    

    本文针对多智能体线性随机赌博问题的一个特定实例，即集群化的多智能体线性赌博机进行了研究。在这个设置中，我们提出了一种新颖的算法，通过智能体之间的有效协作来加速整体优化问题。在这一贡献中，网络控制器负责估计网络的基本集群结构并优化同一组中智能体之间的经验分享。我们对遗憾最小化问题和聚类质量进行了理论分析。通过对合成数据和真实数据进行与最先进算法的实证评估，我们证明了我们方法的有效性：我们的算法显著改善了遗憾最小化，并成功恢复了真实的基本集群划分。

    We address in this paper a particular instance of the multi-agent linear stochastic bandit problem, called clustered multi-agent linear bandits. In this setting, we propose a novel algorithm leveraging an efficient collaboration between the agents in order to accelerate the overall optimization problem. In this contribution, a network controller is responsible for estimating the underlying cluster structure of the network and optimizing the experiences sharing among agents within the same groups. We provide a theoretical analysis for both the regret minimization problem and the clustering quality. Through empirical evaluation against state-of-the-art algorithms on both synthetic and real data, we demonstrate the effectiveness of our approach: our algorithm significantly improves regret minimization while managing to recover the true underlying cluster partitioning.
    
[^12]: Virchow: 数百万张全数字病理学基础模型

    Virchow: A Million-Slide Digital Pathology Foundation Model. (arXiv:2309.07778v1 [eess.IV])

    [http://arxiv.org/abs/2309.07778](http://arxiv.org/abs/2309.07778)

    Virchow是一个数百万参数的深度神经网络基础模型，通过在数百万张全数字病理学切片图像上进行自监督学习训练，有效解决了计算病理学任务中数据不足的问题，并在多个下游任务上超越了最先进的系统。

    

    计算病理学利用人工智能通过分析全数字切片图像实现精准医学和决策支持系统，有潜力彻底改变癌症的诊断和治疗。然而，实现这个目标的一个主要挑战是对于许多特定的计算病理学任务，数据量不足以进行开发。为了应对这个挑战，我们创建了Virchow，一个632百万参数的深度神经网络基础模型，用于计算病理学。通过自监督学习，Virchow在1.5百万个不同组织样本的苏木精和伊红染色全数字切片图像上进行训练，这比之前的研究数据量大得多。在包括瓦片级全癌检测和亚型以及幻灯片级生物标志物预测在内的下游任务上，Virchow在来自与预训练数据相同人群的内部数据集和外部公开数据集上均胜过最先进的系统。

    Computational pathology uses artificial intelligence to enable precision medicine and decision support systems through the analysis of whole slide images. It has the potential to revolutionize the diagnosis and treatment of cancer. However, a major challenge to this objective is that for many specific computational pathology tasks the amount of data is inadequate for development. To address this challenge, we created Virchow, a 632 million parameter deep neural network foundation model for computational pathology. Using self-supervised learning, Virchow is trained on 1.5 million hematoxylin and eosin stained whole slide images from diverse tissue groups, which is orders of magnitude more data than previous works. When evaluated on downstream tasks including tile-level pan-cancer detection and subtyping and slide-level biomarker prediction, Virchow outperforms state-of-the-art systems both on internal datasets drawn from the same population as the pretraining data as well as external pu
    
[^13]: 贝叶斯动态有向无环图学习：在发现大脑动态效应连接组中的应用

    Bayesian Dynamic DAG Learning: Application in Discovering Dynamic Effective Connectome of Brain. (arXiv:2309.07080v1 [q-bio.NC])

    [http://arxiv.org/abs/2309.07080](http://arxiv.org/abs/2309.07080)

    本文介绍了贝叶斯动态有向无环图学习方法（BDyMA）来解决在发现大脑动态效应连接组中的两个主要挑战。该方法通过无约束框架实现更准确的结果和更稀疏的网络结构，使其特别适用于提取动态效应连接组。

    

    通过提取动态效应连接组（DEC）可以揭示大脑的复杂机制。最近，基于评分的有向无环图（DAG）发现方法在提取因果结构和推断有效连接方面表现出显著改进。然而，通过这些方法学习DEC仍然面临两个主要挑战：一个是高维动态DAG发现方法的根本无能力，另一个是fMRI数据质量低下。在本文中，我们引入了基于M-矩阵无环特性的贝叶斯动态DAG学习（BDyMA）方法来解决发现DEC中的挑战。所提出的动态因果模型使我们能够发现双向边缘。利用BDyMA方法中的无约束框架在检测高维网络方面可以获得更准确的结果，实现更稀疏的结果，使其特别适用于提取DEC。

    Understanding the complex mechanisms of the brain can be unraveled by extracting the Dynamic Effective Connectome (DEC). Recently, score-based Directed Acyclic Graph (DAG) discovery methods have shown significant improvements in extracting the causal structure and inferring effective connectivity. However, learning DEC through these methods still faces two main challenges: one with the fundamental impotence of high-dimensional dynamic DAG discovery methods and the other with the low quality of fMRI data. In this paper, we introduce Bayesian Dynamic DAG learning with M-matrices Acyclicity characterization \textbf{(BDyMA)} method to address the challenges in discovering DEC. The presented dynamic causal model enables us to discover bidirected edges as well. Leveraging an unconstrained framework in the BDyMA method leads to more accurate results in detecting high-dimensional networks, achieving sparser outcomes, making it particularly suitable for extracting DEC. Additionally, the score f
    
[^14]: ATTA: 一种针对分割中的区分场外分布检测的异常感知的测试时适应方法

    ATTA: Anomaly-aware Test-Time Adaptation for Out-of-Distribution Detection in Segmentation. (arXiv:2309.05994v1 [cs.CV])

    [http://arxiv.org/abs/2309.05994](http://arxiv.org/abs/2309.05994)

    本文提出了一种双层场外分布检测框架来处理领域转移和语义转移问题。该框架利用全局低级特征和密集高级特征图来适应模型到未见领域，并增强模型在检测新类别方面的能力。

    

    最近在稠密场外分布检测方面的进展主要集中在训练和测试数据集具有相似领域的情况下，假设它们之间不存在领域转移。然而，实际情况下常常存在领域转移，并且显著影响现有场外分布检测模型的准确性。在这项工作中，我们提出了一个双层场外分布检测框架，同时处理领域转移和语义转移。第一层利用全局低级特征区分图像中是否存在领域转移，而第二层利用密集高级特征图识别具有语义转移的像素。通过这种方式，我们可以有选择地适应模型到未见领域，并增强模型在检测新类别方面的能力。我们在几个场外分割基准上验证了我们提出的方法的有效性，包括 those with significant domain shifts and those without。

    Recent advancements in dense out-of-distribution (OOD) detection have primarily focused on scenarios where the training and testing datasets share a similar domain, with the assumption that no domain shift exists between them. However, in real-world situations, domain shift often exits and significantly affects the accuracy of existing out-of-distribution (OOD) detection models. In this work, we propose a dual-level OOD detection framework to handle domain shift and semantic shift jointly. The first level distinguishes whether domain shift exists in the image by leveraging global low-level features, while the second level identifies pixels with semantic shift by utilizing dense high-level feature maps. In this way, we can selectively adapt the model to unseen domains as well as enhance model's capacity in detecting novel classes. We validate the efficacy of our proposed method on several OOD segmentation benchmarks, including those with significant domain shifts and those without, obse
    
[^15]: 学习改进样本复杂性的零和线性二次博弈

    Learning Zero-Sum Linear Quadratic Games with Improved Sample Complexity. (arXiv:2309.04272v1 [eess.SY])

    [http://arxiv.org/abs/2309.04272](http://arxiv.org/abs/2309.04272)

    这项研究提出了改进样本复杂性的零和线性二次博弈，并发现了自然策略梯度方法的隐式正则化属性。在无模型参数知识的情况下，他们还提出了第一个多项式样本复杂性算法来达到Nash均衡。

    

    零和线性二次（LQ）博弈在最优控制中是基础性的，可以用于（i）风险敏感或鲁棒控制的动态博弈形式，或者（ii）作为连续状态-控制空间中两个竞争智能体的多智能体强化学习的基准设置。与广泛研究的单智能体线性二次调节器问题不同，零和LQ博弈涉及解决一个具有缺乏强制性的目标函数的具有挑战性的非凸非凹最小-最大问题。最近，张等人发现了自然策略梯度方法的隐式正则化属性，这对于安全关键的控制系统非常重要，因为它在学习过程中保持了控制器的鲁棒性。此外，在没有模型参数知识的模型无关设置中，张等人提出了第一个多项式样本复杂性算法，以达到Nash均衡的ε-邻域，同时保持理想的隐式正则化属性。

    Zero-sum Linear Quadratic (LQ) games are fundamental in optimal control and can be used (i) as a dynamic game formulation for risk-sensitive or robust control, or (ii) as a benchmark setting for multi-agent reinforcement learning with two competing agents in continuous state-control spaces. In contrast to the well-studied single-agent linear quadratic regulator problem, zero-sum LQ games entail solving a challenging nonconvex-nonconcave min-max problem with an objective function that lacks coercivity. Recently, Zhang et al. discovered an implicit regularization property of natural policy gradient methods which is crucial for safety-critical control systems since it preserves the robustness of the controller during learning. Moreover, in the model-free setting where the knowledge of model parameters is not available, Zhang et al. proposed the first polynomial sample complexity algorithm to reach an $\epsilon$-neighborhood of the Nash equilibrium while maintaining the desirable implicit 
    
[^16]: 神经特征学习中的帕累托前沿：数据、计算、宽度和运气

    Pareto Frontiers in Neural Feature Learning: Data, Compute, Width, and Luck. (arXiv:2309.03800v1 [cs.LG])

    [http://arxiv.org/abs/2309.03800](http://arxiv.org/abs/2309.03800)

    本研究研究了深度学习算法设计中的微妙选择，特别关注计算统计差距。通过理论和实验，发现稀疏初始化和增加网络宽度可以提高样本效率，并且合成稀疏奇偶任务可以作为真实问题的代理。

    

    本研究探讨了在计算统计差距存在的情况下，深度学习中微妙的算法设计选择。我们首先考虑了离线稀疏奇偶学习，这是一个有关多层感知器梯度训练的监督分类问题，其具有统计查询下界。这个下界可以解释为多资源的权衡前沿：成功学习只有在一个足够丰富（大型模型）、知识渊博（大规模数据集）、耐心（训练迭代次数多）或幸运（随机猜测次数多）的情况下才能发生。我们通过理论和实验表明，在这种情况下，稀疏初始化和增加网络宽度可以显著提高样本效率。在这里，宽度起到了并行搜索的作用：它增加了找到“幸运神经元”的概率，这些神经元可以更高效地学习稀疏特征。最后，我们表明合成稀疏奇偶任务可以作为真实问题的代理。

    This work investigates the nuanced algorithm design choices for deep learning in the presence of computational-statistical gaps. We begin by considering offline sparse parity learning, a supervised classification problem which admits a statistical query lower bound for gradient-based training of a multilayer perceptron. This lower bound can be interpreted as a multi-resource tradeoff frontier: successful learning can only occur if one is sufficiently rich (large model), knowledgeable (large dataset), patient (many training iterations), or lucky (many random guesses). We show, theoretically and experimentally, that sparse initialization and increasing network width yield significant improvements in sample efficiency in this setting. Here, width plays the role of parallel search: it amplifies the probability of finding "lottery ticket" neurons, which learn sparse features more sample-efficiently. Finally, we show that the synthetic sparse parity task can be useful as a proxy for real pro
    
[^17]: 通过找到显著可分离的最佳高斯簇的分组，实现超聚类

    Superclustering by finding statistically significant separable groups of optimal gaussian clusters. (arXiv:2309.02623v1 [cs.LG])

    [http://arxiv.org/abs/2309.02623](http://arxiv.org/abs/2309.02623)

    本文提出了一种算法，通过找到统计显著可分离的最佳高斯簇的分组，实现超聚类。算法具有三个阶段，包括表示数据集为高斯混合分布-簇、使用马氏距离估计簇之间的距离和大小以及将簇组合为超簇。算法的创新点在于引入了矩阵质量准则，并通过自动选择合适的统计显著性水平来确定最佳超簇数量。

    

    本文提出了一种通过将数据集的最佳高斯簇分组成统计可分离的超簇的算法。算法包括三个阶段：将数据集表示为高斯混合分布-簇，其数量基于BIC准则的最小值确定；使用马氏距离估计簇之间的距离和簇的大小；使用DBSCAN方法将得到的簇组合成超簇，通过找到其超参数（最大距离）在最大数量的超簇情况下，提供引入的矩阵质量准则的最大值。矩阵质量准则对应于所有发现的超簇中具有统计显著分离的超簇所占比例。该算法只有一个超参数-统计显著性水平，并且可以自动为其选择合适的值。

    The paper presents the algorithm for clustering a dataset by grouping the optimal, from the point of view of the BIC criterion, number of Gaussian clusters into the optimal, from the point of view of their statistical separability, superclusters.  The algorithm consists of three stages: representation of the dataset as a mixture of Gaussian distributions - clusters, which number is determined based on the minimum of the BIC criterion; using the Mahalanobis distance, to estimate the distances between the clusters and cluster sizes; combining the resulting clusters into superclusters using the DBSCAN method by finding its hyperparameter (maximum distance) providing maximum value of introduced matrix quality criterion at maximum number of superclusters. The matrix quality criterion corresponds to the proportion of statistically significant separated superclusters among all found superclusters.  The algorithm has only one hyperparameter - statistical significance level, and automatically d
    
[^18]: 有效的多图神经网络用于加密货币交易网络上的非法账户检测

    Effective Multi-Graph Neural Networks for Illicit Account Detection on Cryptocurrency Transaction Networks. (arXiv:2309.02460v1 [cs.LG])

    [http://arxiv.org/abs/2309.02460](http://arxiv.org/abs/2309.02460)

    本文介绍了一种新颖的多图神经网络模型DIAM，用于有效地检测加密货币交易网络上的非法账户。该模型通过自动学习节点表示并保留平行边的内在交易模式，在大型交易网络中取得了良好的效果。

    

    我们研究了在线金融市场中日益重要的加密货币交易网络上的非法账户检测。在加密货币上的非法活动激增导致了普通用户数十亿的损失。现有的解决方案要么依赖于繁琐的特征工程来获得手工特征，要么不能充分利用加密货币交易数据中丰富的语义信息，从而导致亚优化的性能。在本文中，我们将非法账户检测问题定义为带有边属性的有向多图上的分类任务，并提出了DIAM，一种新颖的多图神经网络模型，用于在大型交易网络上有效地检测非法账户。首先，DIAM包含一个Edge2Seq模块，通过同时考虑边属性和有向边序列依赖关系，自动学习有效的节点表示，保留平行边的内在交易模式。然后利用t

    We study illicit account detection on transaction networks of cryptocurrencies that are increasi_testngly important in online financial markets. The surge of illicit activities on cryptocurrencies has resulted in billions of losses from normal users. Existing solutions either rely on tedious feature engineering to get handcrafted features, or are inadequate to fully utilize the rich semantics of cryptocurrency transaction data, and consequently, yield sub-optimal performance. In this paper, we formulate the illicit account detection problem as a classification task over directed multigraphs with edge attributes, and present DIAM, a novel multi-graph neural network model to effectively detect illicit accounts on large transaction networks. First, DIAM includes an Edge2Seq module that automatically learns effective node representations preserving intrinsic transaction patterns of parallel edges, by considering both edge attributes and directed edge sequence dependencies. Then utilizing t
    
[^19]: 稳定行动：学习协调双手操作的方法

    Stabilize to Act: Learning to Coordinate for Bimanual Manipulation. (arXiv:2309.01087v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2309.01087](http://arxiv.org/abs/2309.01087)

    通过借鉴人类的角色分配方法，我们提出了一种稳定行动的框架，其中一个手臂用于固定物体，另一个手臂用于执行任务。通过学习到的稳定分类器和行动策略，我们实现了这一框架并对其进行了评估。

    

    在真实世界中进行丰富和灵巧的操作的关键是能够协调控制两只手。然而，虽然双手机器人系统带来了巨大的前景，但构建双臂自主系统的控制策略却面临难题。其中一个困难是双手动作空间的高维度，这给基于模型和数据驱动的方法增加了复杂性。我们从人类身上得到启发，提出了一种新的角色分配框架来应对这一挑战：一个稳定的臂将物体固定在一个位置上，简化环境，而一个活动的臂则执行任务。我们利用学习到的稳定分类器来实施 BimanUal Dexterity from Stabilization (BUDS) 框架，该分类器交替更新学习到的稳定位置以保持环境稳定，并利用示范学习到的行动策略完成任务。我们对BUDS进行了四种不同的双手任务评估。

    Key to rich, dexterous manipulation in the real world is the ability to coordinate control across two hands. However, while the promise afforded by bimanual robotic systems is immense, constructing control policies for dual arm autonomous systems brings inherent difficulties. One such difficulty is the high-dimensionality of the bimanual action space, which adds complexity to both model-based and data-driven methods. We counteract this challenge by drawing inspiration from humans to propose a novel role assignment framework: a stabilizing arm holds an object in place to simplify the environment while an acting arm executes the task. We instantiate this framework with BimanUal Dexterity from Stabilization (BUDS), which uses a learned restabilizing classifier to alternate between updating a learned stabilization position to keep the environment unchanged, and accomplishing the task with an acting policy learned from demonstrations. We evaluate BUDS on four bimanual tasks of varying compl
    
[^20]: 使用扩散模型的多粒度迭代图像编辑

    Iterative Multi-granular Image Editing using Diffusion Models. (arXiv:2309.00613v1 [cs.CV])

    [http://arxiv.org/abs/2309.00613](http://arxiv.org/abs/2309.00613)

    本研究提出了一种名为EMILIE的迭代多粒度图像编辑器，通过重新利用预训练的扩散模型来实现迭代编辑，并引入梯度控制操作来实现对图像编辑的粒度控制。

    

    最近，文本引导的图像合成的进展极大地改变了创意专业人员生成艺术和审美上令人愉悦的视觉资产的方式。为了充分支持这样的创意努力，该过程应具备以下能力：1）迭代地编辑生成的图像，2）控制所需变化的空间范围（全局、局部或介于两者之间）。我们将这个实用的问题设定正式化为迭代多粒度编辑。虽然在图像合成和编辑方面，基于扩散模型取得了重大进展，但它们都是一次性操作（即没有迭代编辑能力），并且不能自然产生多粒度控制（即涵盖从局部到全局编辑的全谱）。为了克服这些缺点，我们提出了EMILIE：迭代多粒度图像编辑器。EMILIE引入了一种新颖的潜在迭代策略，利用预训练的扩散模型来促进迭代编辑。同时，还引入了梯度控制操作来实现对所需变化的粒度控制。

    Recent advances in text-guided image synthesis has dramatically changed how creative professionals generate artistic and aesthetically pleasing visual assets. To fully support such creative endeavors, the process should possess the ability to: 1) iteratively edit the generations and 2) control the spatial reach of desired changes (global, local or anything in between). We formalize this pragmatic problem setting as Iterative Multi-granular Editing. While there has been substantial progress with diffusion-based models for image synthesis and editing, they are all one shot (i.e., no iterative editing capabilities) and do not naturally yield multi-granular control (i.e., covering the full spectrum of local-to-global edits). To overcome these drawbacks, we propose EMILIE: Iterative Multi-granular Image Editor. EMILIE introduces a novel latent iteration strategy, which re-purposes a pre-trained diffusion model to facilitate iterative editing. This is complemented by a gradient control opera
    
[^21]: 利用扩散变分自编码器解决多步回归股票价格预测中的随机性问题

    Diffusion Variational Autoencoder for Tackling Stochasticity in Multi-Step Regression Stock Price Prediction. (arXiv:2309.00073v1 [q-fin.ST])

    [http://arxiv.org/abs/2309.00073](http://arxiv.org/abs/2309.00073)

    本论文提出了一种利用扩散变分自编码器来解决多步回归股票价格预测中的随机性问题的方法。

    

    长期内的多步股票价格预测对于预测波动性至关重要，使得金融机构能够定价和对冲衍生品，并让银行量化其交易簿中的风险。此外，大多数金融监管机构还要求机构投资者有几天的流动性期限从其风险资产中退出，以避免对市场价格产生实质性影响。然而，由于股票数据具有高度随机性，多步股票价格预测的任务很具挑战性。目前解决这个问题的方法主要针对单步基于分类的预测，并且在表征表达力方面有限。随着目标价格序列的引入，这个问题在测试时变得越来越困难，因为目标价格序列中也包含随机噪声，降低了泛化能力。为了解决这些问题，我们结合了深层分层变分自编码器(VAE)和扩散概率技术。

    Multi-step stock price prediction over a long-term horizon is crucial for forecasting its volatility, allowing financial institutions to price and hedge derivatives, and banks to quantify the risk in their trading books. Additionally, most financial regulators also require a liquidity horizon of several days for institutional investors to exit their risky assets, in order to not materially affect market prices. However, the task of multi-step stock price prediction is challenging, given the highly stochastic nature of stock data. Current solutions to tackle this problem are mostly designed for single-step, classification-based predictions, and are limited to low representation expressiveness. The problem also gets progressively harder with the introduction of the target price sequence, which also contains stochastic noise and reduces generalizability at test-time. To tackle these issues, we combine a deep hierarchical variational-autoencoder (VAE) and diffusion probabilistic techniques
    
[^22]: NAS-X: 基于扭曲的神经自适应平滑方法

    NAS-X: Neural Adaptive Smoothing via Twisting. (arXiv:2308.14864v1 [cs.LG])

    [http://arxiv.org/abs/2308.14864](http://arxiv.org/abs/2308.14864)

    NAS-X是一种基于扭曲的神经自适应平滑方法，通过重新加权的唤醒-睡眠算法来学习和推断顺序潜变量模型，并在离散和连续任务中取得了优于先前方法的推断和参数恢复效果。

    

    本文提出了一种名为NAS-X的神经自适应平滑方法，该方法基于重新加权的唤醒-睡眠算法进行顺序潜变量模型的学习和推断。NAS-X适用于离散和连续潜变量，并利用平滑SMC方法来拟合比传统的重新加权唤醒-睡眠方法更广泛的模型。我们在离散和连续任务上测试了NAS-X，并发现在推断和参数恢复方面，它明显优于先前的变分和基于重新加权唤醒-睡眠方法。

    We present Neural Adaptive Smoothing via Twisting (NAS-X), a method for learning and inference in sequential latent variable models based on reweighted wake-sleep (RWS). NAS-X works with both discrete and continuous latent variables, and leverages smoothing SMC to fit a broader range of models than traditional RWS methods. We test NAS-X on discrete and continuous tasks and find that it substantially outperforms previous variational and RWS-based methods in inference and parameter recovery.
    
[^23]: 较快的模型训练之路: 基于贝叶斯数据选择的方法

    Towards Accelerated Model Training via Bayesian Data Selection. (arXiv:2308.10544v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.10544](http://arxiv.org/abs/2308.10544)

    通过对数据的贝叶斯选择和零样本预测器的利用，提出了一种更合理、高效且易于实现的模型训练方法，动态选取地在线批量训练样本，减少噪声和不平衡带来的影响。

    

    实际场景中的错误标记、重复或有偏数据可能导致训练时间延长甚至阻碍模型收敛。传统方法优先考虑简单或困难样本，缺乏同时处理多样情况的灵活性。最近的研究提出了一种更合理的数据选择原则，通过检查数据对模型的泛化损失的影响。然而，其实际应用依赖于不太可靠的近似方法和额外的holdout数据。本文通过利用轻量级的贝叶斯方法，并结合基于大规模预训练模型构建的零样本预测器来解决这些问题。所得到的算法高效且易于实现。我们在具有大量数据噪声和不平衡性的在线批量选择场景下进行了广泛的实证研究，并观察到与竞争基线相比更高的训练效率。值得注意的是，在具有挑战性的WebVision基准测试上，我们的方法能够达到较快的训练速度。

    Mislabeled, duplicated, or biased data in real-world scenarios can lead to prolonged training and even hinder model convergence. Traditional solutions prioritizing easy or hard samples lack the flexibility to handle such a variety simultaneously. Recent work has proposed a more reasonable data selection principle by examining the data's impact on the model's generalization loss. However, its practical adoption relies on less principled approximations and additional holdout data. This work solves these problems by leveraging a lightweight Bayesian treatment and incorporating off-the-shelf zero-shot predictors built on large-scale pre-trained models. The resulting algorithm is efficient and easy to implement. We perform extensive empirical studies on challenging benchmarks with considerable data noise and imbalance in the online batch selection scenario, and observe superior training efficiency over competitive baselines. Notably, on the challenging WebVision benchmark, our method can ac
    
[^24]: 问题分类的集成方法：融合Electra Transformer、GloVe和LSTM

    An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM. (arXiv:2308.06828v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.06828](http://arxiv.org/abs/2308.06828)

    本研究提出了一种集成Electra Transformer、GloVe和LSTM模型的创新问题分类方法，通过在TREC数据集上进行严格测试，证明了融合不同技术可以获得更优越的结果。

    

    自然语言处理（NLP）已经成为理解和生成人类语言的关键技术，它在机器翻译、情感分析等任务中扮演着重要角色，尤其是在问题分类方面。作为自然语言处理的子领域，问题分类专注于确定所需信息的类型，这是问题回答系统等下游应用的基本步骤。本研究提出了一种创新的问题分类集成方法，将Electra、GloVe和LSTM模型的优势相结合。该模型在著名的TREC数据集上进行了严格测试，展示了如何整合这些不同技术可以得到更优越的结果。Electra提供了基于transformer的复杂语言理解能力，GloVe提供了全局向量表示以捕捉词级语义，LSTM则贡献了序列学习能力以建模长期依赖关系。

    Natural Language Processing (NLP) has emerged as a crucial technology for understanding and generating human language, playing an essential role in tasks such as machine translation, sentiment analysis, and more pertinently, question classification. As a subfield within NLP, question classification focuses on determining the type of information being sought, a fundamental step for downstream applications like question answering systems. This study presents an innovative ensemble approach for question classification, combining the strengths of Electra, GloVe, and LSTM models. Rigorously tested on the well-regarded TREC dataset, the model demonstrates how the integration of these disparate technologies can lead to superior results. Electra brings in its transformer-based capabilities for complex language understanding, GloVe offers global vector representations for capturing word-level semantics, and LSTM contributes its sequence learning abilities to model long-term dependencies. By fus
    
[^25]: 基于物理约束神经网络的多尺度模型的深度学习框架

    A deep learning framework for multi-scale models based on physics-informed neural networks. (arXiv:2308.06672v1 [cs.LG])

    [http://arxiv.org/abs/2308.06672](http://arxiv.org/abs/2308.06672)

    本文提出了一个基于物理约束神经网络的新框架，用于解决具有不同数量级损失项的多尺度问题。通过重新构建损失函数，并应用不同数量的幂运算，使各个损失项在数量级上大致相等。同时提供了一种分组正则化策略来应对在不同子领域中显着变化的问题。

    

    物理约束神经网络（PINN）将深度神经网络与偏微分方程（PDE）的解相结合，创建了一个新的有希望的研究领域，用于数值求解PDE。面对一类包含不同数量级损失项的多尺度问题，对于标准PINN方法来说，获得可用的预测是具有挑战性的。在本文中，我们提出了一个通过重构损失函数来解决多尺度问题的新框架。该框架基于标准PINN方法，并通过对不同量级的损失项应用不同数量的幂运算来修改标准PINN方法的损失函数，从而使构成损失函数的各个损失项在数量级上大致相等。此外，我们提出了一种分组正则化策略，该策略可以很好地处理在不同子领域中显着变化的问题。

    Physics-informed neural networks (PINN) combine deep neural networks with the solution of partial differential equations (PDEs), creating a new and promising research area for numerically solving PDEs. Faced with a class of multi-scale problems that include loss terms of different orders of magnitude in the loss function, it is challenging for standard PINN methods to obtain an available prediction. In this paper, we propose a new framework for solving multi-scale problems by reconstructing the loss function. The framework is based on the standard PINN method, and it modifies the loss function of the standard PINN method by applying different numbers of power operations to the loss terms of different magnitudes, so that the individual loss terms composing the loss function have approximately the same order of magnitude among themselves. In addition, we give a grouping regularization strategy, and this strategy can deal well with the problem which varies significantly in different subdo
    
[^26]: 对协同过滤丢失函数的更好理解

    Toward a Better Understanding of Loss Functions for Collaborative Filtering. (arXiv:2308.06091v1 [cs.IR])

    [http://arxiv.org/abs/2308.06091](http://arxiv.org/abs/2308.06091)

    现有研究已经表明，通过改进对齐和均匀性设计的损失函数可以实现显著的性能提升。本文提出了一种新的损失函数，称为MAWU，它考虑了数据集的独特模式。

    

    协同过滤（CF）是现代推荐系统中的关键技术。CF模型的学习过程通常由三个组件组成：交互编码器、损失函数和负采样。尽管许多现有研究已经提出了各种CF模型来设计复杂的交互编码器，但最近的工作表明，简单地重新制定损失函数可以实现显著的性能提升。本文深入分析了现有损失函数之间的关系。我们的数学分析揭示了先前的损失函数可以解释为对齐和均匀性函数：（i）对齐匹配用户和物品表示，（ii）均匀性分散用户和物品分布。受到这个分析的启示，我们提出了一种改进对齐和均匀性设计的损失函数，考虑到数据集的独特模式，称为Margin-aware Alignment and Weighted Uniformity（MAWU）。MAWU的关键创新是

    Collaborative filtering (CF) is a pivotal technique in modern recommender systems. The learning process of CF models typically consists of three components: interaction encoder, loss function, and negative sampling. Although many existing studies have proposed various CF models to design sophisticated interaction encoders, recent work shows that simply reformulating the loss functions can achieve significant performance gains. This paper delves into analyzing the relationship among existing loss functions. Our mathematical analysis reveals that the previous loss functions can be interpreted as alignment and uniformity functions: (i) the alignment matches user and item representations, and (ii) the uniformity disperses user and item distributions. Inspired by this analysis, we propose a novel loss function that improves the design of alignment and uniformity considering the unique patterns of datasets called Margin-aware Alignment and Weighted Uniformity (MAWU). The key novelty of MAWU 
    
[^27]: 图聚类的同类性增强结构学习

    Homophily-enhanced Structure Learning for Graph Clustering. (arXiv:2308.05309v1 [cs.LG])

    [http://arxiv.org/abs/2308.05309](http://arxiv.org/abs/2308.05309)

    提出了一种名为HoLe的方法，通过在图结构中增强同类性可以显著改善图聚类任务的性能。

    

    图聚类是图分析中的一个基本任务，在利用图神经网络（GNNs）方面的最新进展已经取得了令人印象深刻的成果。尽管现有的基于GNN的图聚类方法取得了成功，但它们往往忽视了图结构的质量，这是由于现实世界图的稀疏性和多样性所固有的，从而导致了次优的性能。图结构学习可以通过添加缺失的连接和删除错误的连接来优化输入图。然而，以往的图结构学习工作主要集中在有监督的设置上，并且由于缺乏真实标签，不能直接应用于我们的特定聚类任务。为了弥补这个差距，我们提出了一种新颖的方法，称为同类性增强结构学习图聚类（HoLe）。我们的动机源于观察到，微妙地增强图结构中的同类性程度可以显著提升GNNs的性能。

    Graph clustering is a fundamental task in graph analysis, and recent advances in utilizing graph neural networks (GNNs) have shown impressive results. Despite the success of existing GNN-based graph clustering methods, they often overlook the quality of graph structure, which is inherent in real-world graphs due to their sparse and multifarious nature, leading to subpar performance. Graph structure learning allows refining the input graph by adding missing links and removing spurious connections. However, previous endeavors in graph structure learning have predominantly centered around supervised settings, and cannot be directly applied to our specific clustering tasks due to the absence of ground-truth labels. To bridge the gap, we propose a novel method called \textbf{ho}mophily-enhanced structure \textbf{le}arning for graph clustering (HoLe). Our motivation stems from the observation that subtly enhancing the degree of homophily within the graph structure can significantly improve G
    
[^28]: 基于深度学习的图像水印技术：简要调查

    Deep Learning based Image Watermarking: A Brief Survey. (arXiv:2308.04603v1 [cs.MM])

    [http://arxiv.org/abs/2308.04603](http://arxiv.org/abs/2308.04603)

    该论文调查了基于深度学习的图像水印技术的最新研究进展，将其分为嵌入器-提取器联合训练、深度网络作为特征变换和混合方案。对每个类别的研究方向进行了分析和总结，并讨论了未来的研究方向。

    

    图像水印是指在一张封面图像中秘密嵌入和提取水印以保护图像的行为。近年来，基于深度学习的图像水印技术层出不穷。为了研究最新的技术，本调查将前沿的基于深度学习的图像水印技术分为嵌入器-提取器联合训练、深度网络作为特征变换和混合方案。还分析和总结了每个类别中的研究方向。此外，还讨论了潜在的未来研究方向，展望未来的研究。

    The act of secretly embedding and extracting a watermark on a cover image to protect it is known as image watermarking. In recent years, deep learning-based image watermarking techniques have been emerging one after another. To study the state-of-the-art, this survey categorizes cutting-edge deep learning-based image watermarking techniques into Embedder-Extractor Joint Training, Deep Networks as a Feature Transformation, and Hybrid schemes. Research directions in each category are also analyzed and summarized. Additionally, potential future research directions are discussed to envision future studies.
    
[^29]: 通过基于模型的树马尔可夫模型，实现透明的序列模型

    Toward Transparent Sequence Models with Model-Based Tree Markov Model. (arXiv:2307.15367v1 [cs.LG])

    [http://arxiv.org/abs/2307.15367](http://arxiv.org/abs/2307.15367)

    本研究引入了基于模型的树马尔可夫模型（MOB-HSMM），用于解决复杂黑盒机器学习模型应用于序列数据时的可解释性问题。通过从深度神经网络中蒸馏的知识，实现了提高预测性能的同时提供清晰解释的目标。实验结果表明通过将LSTM学习到的顺序模式转移到MOB树中，可以进一步提高MOB树的性能，并利用MOB-HSMM将MOB树与隐马尔可夫模型（HSMM）整合，实现了潜在和可解释的序列的发现。

    

    本研究解决了应用于序列数据的复杂、黑盒机器学习模型的可解释性问题。我们引入了基于模型的树隐马尔可夫模型（MOB-HSMM），这是一个固有可解释性的模型，旨在检测高死亡风险事件，并发现与死亡风险相关的隐藏模式。该模型利用从深度神经网络（DNN）中蒸馏的知识，提高预测性能的同时提供清晰的解释。我们的实验结果表明，通过使用LSTM学习顺序模式，进而将其转移给MOB树，可以提高基于模型的树（MOB树）的性能。将MOB树与基于模型的隐马尔可夫模型（HSMM）集成在MOB-HSMM中，可以使用可用信息揭示潜在的和可解释的序列。

    In this study, we address the interpretability issue in complex, black-box Machine Learning models applied to sequence data. We introduce the Model-Based tree Hidden Semi-Markov Model (MOB-HSMM), an inherently interpretable model aimed at detecting high mortality risk events and discovering hidden patterns associated with the mortality risk in Intensive Care Units (ICU). This model leverages knowledge distilled from Deep Neural Networks (DNN) to enhance predictive performance while offering clear explanations. Our experimental results indicate the improved performance of Model-Based trees (MOB trees) via employing LSTM for learning sequential patterns, which are then transferred to MOB trees. Integrating MOB trees with the Hidden Semi-Markov Model (HSMM) in the MOB-HSMM enables uncovering potential and explainable sequences using available information.
    
[^30]: 从人类偏好中学习的政策在情境多臂赌博问题中的可证明优势

    Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems. (arXiv:2307.12975v1 [cs.LG])

    [http://arxiv.org/abs/2307.12975](http://arxiv.org/abs/2307.12975)

    该论文研究了基于偏好的政策学习方法在离线情境多臂赌博问题中的优势，并通过改进建模和分析，证明了这一方法相比其他政策学习方法具有更低的次优性。

    

    在决策问题中，奖励工程是一个关键的任务。在实践中，往往不存在明显的奖励函数选择。因此，一种常见的方法是在训练过程中引入人类反馈，并利用这种反馈来学习奖励函数。在使用人类反馈的所有政策学习方法中，基于偏好的方法在最近的实证应用中取得了显著的成功，如InstructGPT。在这项工作中，我们开发了一个理论，可以证明在离线情境多臂赌博问题中，基于偏好的方法具有显著的优势。具体而言，我们改进了在人类评分样本上运行政策学习方法的建模和次优性分析。然后，我们将其与基于偏好的方法的次优性保证进行比较，并表明基于偏好的方法享有更低的次优性。

    A crucial task in decision-making problems is reward engineering. It is common in practice that no obvious choice of reward function exists. Thus, a popular approach is to introduce human feedback during training and leverage such feedback to learn a reward function. Among all policy learning methods that use human feedback, preference-based methods have demonstrated substantial success in recent empirical applications such as InstructGPT. In this work, we develop a theory that provably shows the benefits of preference-based methods in offline contextual bandits. In particular, we improve the modeling and suboptimality analysis for running policy learning methods on human-scored samples directly. Then, we compare it with the suboptimality guarantees of preference-based methods and show that preference-based methods enjoy lower suboptimality.
    
[^31]: 面向交通信号控制的不确定性感知基于实例的行动转换的模拟到实际转移

    Uncertainty-aware Grounded Action Transformation towards Sim-to-Real Transfer for Traffic Signal Control. (arXiv:2307.12388v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.12388](http://arxiv.org/abs/2307.12388)

    本文提出了UGAT方法，通过在模拟环境中动态转换具有不确定性的行动，实现了从模拟环境到真实环境的策略转移，显著提高了在真实世界中的性能。

    

    交通信号控制（TSC）是一个影响数百万人日常生活的复杂而重要的任务。强化学习（RL）在优化交通信号控制方面取得了有希望的结果，但当前基于RL的TSC方法主要在模拟环境中训练，存在模拟和真实世界之间性能差距的问题。本文提出了一种模拟到实际环境转移的方法，称为UGAT，通过在模拟中动态转换具有不确定性的行动，以减轻转移动态的领域差距，将在模拟环境中训练的学习策略转移到真实环境中。我们在模拟交通环境中评估了我们的方法，并表明它显著提高了转移后的RL策略在真实世界中的性能。

    Traffic signal control (TSC) is a complex and important task that affects the daily lives of millions of people. Reinforcement Learning (RL) has shown promising results in optimizing traffic signal control, but current RL-based TSC methods are mainly trained in simulation and suffer from the performance gap between simulation and the real world. In this paper, we propose a simulation-to-real-world (sim-to-real) transfer approach called UGAT, which transfers a learned policy trained from a simulated environment to a real-world environment by dynamically transforming actions in the simulation with uncertainty to mitigate the domain gap of transition dynamics. We evaluate our method on a simulated traffic environment and show that it significantly improves the performance of the transferred RL policy in the real world.
    
[^32]: HIQL: 以潜在状态作为动作的离线目标导向强化学习

    HIQL: Offline Goal-Conditioned RL with Latent States as Actions. (arXiv:2307.11949v1 [cs.LG])

    [http://arxiv.org/abs/2307.11949](http://arxiv.org/abs/2307.11949)

    本文提出了一个基于离线数据的目标导向强化学习的分层算法，通过利用目标达成问题的结构，使用一个无动作的价值函数学习了两个策略，从而在学习过程中更有效地利用离线数据。

    

    无监督预训练最近已成为计算机视觉和自然语言处理的基石。在强化学习中，目标导向强化学习可以潜在地利用大量未标记的（无奖励）数据，提供类似于自我监督的方法。然而，构建有效的目标导向强化学习算法并直接从多样化的离线数据中进行学习是具有挑战性的，因为准确估计远期目标的价值函数很困难。然而，目标达成问题表现出一定的结构，即达到远期目标需要首先通过较近子目标。这种结构非常有用，因为评估邻近目标的动作质量通常比更远目标容易。基于这一思想，我们提出了一个基于离线数据的目标导向强化学习的分层算法。利用一个没有动作的价值函数，我们学习了两个策略，允许我们利用这种结构：一个高层策略

    Unsupervised pre-training has recently become the bedrock for computer vision and natural language processing. In reinforcement learning (RL), goal-conditioned RL can potentially provide an analogous self-supervised approach for making use of large quantities of unlabeled (reward-free) data. However, building effective algorithms for goal-conditioned RL that can learn directly from diverse offline data is challenging, because it is hard to accurately estimate the exact value function for faraway goals. Nonetheless, goal-reaching problems exhibit structure, such that reaching distant goals entails first passing through closer subgoals. This structure can be very useful, as assessing the quality of actions for nearby goals is typically easier than for more distant goals. Based on this idea, we propose a hierarchical algorithm for goal-conditioned RL from offline data. Using one action-free value function, we learn two policies that allow us to exploit this structure: a high-level policy 
    
[^33]: 超越黑盒建议: 基于学习的增强算法用于具有Q值预测的MDPs

    Beyond Black-Box Advice: Learning-Augmented Algorithms for MDPs with Q-Value Predictions. (arXiv:2307.10524v1 [cs.LG])

    [http://arxiv.org/abs/2307.10524](http://arxiv.org/abs/2307.10524)

    该论文研究了在具有不可信的机器学习建议的单轨迹时间变化的MDP中一致性和鲁棒性之间的权衡，并证明了利用Q值建议可以获得接近最优的性能保证，并改进了仅使用黑盒建议的情况。

    

    我们研究了在单轨迹时间变化的马尔科夫决策过程(MDP)中一致性和鲁棒性之间的权衡，该过程具有不可信的机器学习建议。我们的工作不同于常规方法，不再将建议视为来自黑盒来源，而是考虑到有关如何生成建议的其他信息。我们证明了在包括连续和离散状态/动作空间的一般MDP模型下给出的Q值建议的一种新型一致性和鲁棒性权衡。我们的结果表明，利用Q值建议可以动态追求机器学习建议和稳健基线中较优的那个，从而产生接近最优的性能保证，并且改进了仅使用黑盒建议所能获得的结果。

    We study the tradeoff between consistency and robustness in the context of a single-trajectory time-varying Markov Decision Process (MDP) with untrusted machine-learned advice. Our work departs from the typical approach of treating advice as coming from black-box sources by instead considering a setting where additional information about how the advice is generated is available. We prove a first-of-its-kind consistency and robustness tradeoff given Q-value advice under a general MDP model that includes both continuous and discrete state/action spaces. Our results highlight that utilizing Q-value advice enables dynamic pursuit of the better of machine-learned advice and a robust baseline, thus result in near-optimal performance guarantees, which provably improves what can be obtained solely with black-box advice.
    
[^34]: 神经图像压缩：泛化性能、鲁棒性和光谱偏差

    Neural Image Compression: Generalization, Robustness, and Spectral Biases. (arXiv:2307.08657v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2307.08657](http://arxiv.org/abs/2307.08657)

    本论文将神经图像压缩的泛化性能、鲁棒性和光谱偏差进行了研究，提出了评估图像压缩方法在未见分布下性能的基准套件，并引入了一种光谱启发式的检测工具来深入了解这些方法引入的错误。

    

    最近神经图像压缩（NIC）的进展已经产生了一些开始超越经典编解码器的模型。尽管这引起了越来越多关于在实际应用中使用NIC的兴奋，但任何机器学习系统在实际环境中的成功应用都要求它在部署时具有泛化性能（和鲁棒性）。不幸的是，目前的研究缺乏全面的数据集和信息工具来评估和理解NIC在实际环境中的性能。为了弥补这个关键的差距，首先，本文提出了一个全面的基准套件来评估图像压缩方法在分布上未见情况下的性能。具体而言，我们通过向流行的CLIC和Kodak基准测试中引入15个损坏类型，提供了CLIC-C和Kodak-C两个新增数据集。接下来，我们提出了一种光谱启发式的检测工具，以深入了解图像压缩方法引入的错误以及它们在未见分布下的性能。然后，我们进行了详细的性能比较研究。

    Recent advances in neural image compression (NIC) have produced models that are starting to outperform classic codecs. While this has led to growing excitement about using NIC in real-world applications, the successful adoption of any machine learning system in the wild requires it to generalize (and be robust) to unseen distribution shifts at deployment. Unfortunately, current research lacks comprehensive datasets and informative tools to evaluate and understand NIC performance in real-world settings. To bridge this crucial gap, first, this paper presents a comprehensive benchmark suite to evaluate the out-of-distribution (OOD) performance of image compression methods. Specifically, we provide CLIC-C and Kodak-C by introducing 15 corruptions to the popular CLIC and Kodak benchmarks. Next, we propose spectrally-inspired inspection tools to gain deeper insight into errors introduced by image compression methods as well as their OOD performance. We then carry out a detailed performance c
    
[^35]: SBMLtoODEjax: 在JAX中高效模拟和优化生物网络模型

    SBMLtoODEjax: Efficient Simulation and Optimization of Biological Network Models in JAX. (arXiv:2307.08452v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2307.08452](http://arxiv.org/abs/2307.08452)

    SBMLtoODEjax是一个轻量级的工具，利用机器学习在JAX中高效模拟和优化生物网络模型，用于理解生物系统的多样行为和优化干预。

    

    生物工程和生物医学的进展要求深入了解生物系统的动态行为，从蛋白质途径到复杂的细胞过程。基因调控网络和蛋白质途径等生物网络是胚胎发育和生理过程的关键驱动因素。理解它们的多样行为对于解决包括癌症在内的疾病以及工程新的生物构建是至关重要的。尽管在系统生物学标记语言（SBML）中有大量的数学模型可用，但研究人员在探索完整的行为谱系和优化干预以有效塑造这些行为方面面临着重大挑战。现有的设计用于生物网络模型模拟的工具并没有针对促进网络动力学上的干预或促进自动发现而定制。本文利用机器学习（ML）的最新进展，介绍了SBMLtoODEjax，一个轻量级的工具，可以在JAX中进行生物网络模型的模拟和优化。

    Advances in bioengineering and biomedicine demand a deep understanding of the dynamic behavior of biological systems, ranging from protein pathways to complex cellular processes. Biological networks like gene regulatory networks and protein pathways are key drivers of embryogenesis and physiological processes. Comprehending their diverse behaviors is essential for tackling diseases, including cancer, as well as for engineering novel biological constructs. Despite the availability of extensive mathematical models represented in Systems Biology Markup Language (SBML), researchers face significant challenges in exploring the full spectrum of behaviors and optimizing interventions to efficiently shape those behaviors. Existing tools designed for simulation of biological network models are not tailored to facilitate interventions on network dynamics nor to facilitate automated discovery. Leveraging recent developments in machine learning (ML), this paper introduces SBMLtoODEjax, a lightweig
    
[^36]: 复杂性至关重要：重新思考生成建模的潜在空间

    Complexity Matters: Rethinking the Latent Space for Generative Modeling. (arXiv:2307.08283v1 [cs.LG])

    [http://arxiv.org/abs/2307.08283](http://arxiv.org/abs/2307.08283)

    本研究从模型复杂性的角度重新思考生成建模的潜在空间，提出了一种新的潜在与数据分布之间的“距离”，并通过该距离的最小化来优化生成器的复杂性。

    

    在生成建模中，许多成功的方法利用低维潜在空间，例如，稳定扩散模型通过编码器引导的潜在空间生成图像，并通过配对的解码器进行生成。尽管潜在空间的选择在实践中非常重要，但确定最优选择和识别过程仍不清楚。在本研究中，我们旨在从模型复杂性的角度重新思考潜在空间，来揭示这个未被充分探索的话题。我们的调查从经典的生成对抗网络（GANs）开始。受到GAN训练目标的启发，我们提出了一种新的潜在与数据分布之间的“距离”，其最小化与生成器的复杂性最小化相一致。这个距离的最小化者被描述为能够最有效地利用生成器容量的最佳数据相关的潜在。然后，我们考虑通过编码器网络对这样的潜在分布进行参数化，并提出了一个方法...

    In generative modeling, numerous successful approaches leverage a low-dimensional latent space, e.g., Stable Diffusion models the latent space induced by an encoder and generates images through a paired decoder. Although the selection of the latent space is empirically pivotal, determining the optimal choice and the process of identifying it remain unclear. In this study, we aim to shed light on this under-explored topic by rethinking the latent space from the perspective of model complexity. Our investigation starts with the classic generative adversarial networks (GANs). Inspired by the GAN training objective, we propose a novel "distance" between the latent and data distributions, whose minimization coincides with that of the generator complexity. The minimizer of this distance is characterized as the optimal data-dependent latent that most effectively capitalizes on the generator's capacity. Then, we consider parameterizing such a latent distribution by an encoder network and propo
    
[^37]: CatBoost对比XGBoost和LightGBM：开发增强的零膨胀保险理赔数据预测模型

    CatBoost Versus XGBoost and LightGBM: Developing Enhanced Predictive Models for Zero-Inflated Insurance Claim Data. (arXiv:2307.07771v1 [cs.LG])

    [http://arxiv.org/abs/2307.07771](http://arxiv.org/abs/2307.07771)

    本文比较了CatBoost、XGBoost和LightGBM三种流行的梯度提升库在处理零膨胀保险理赔数据上的效果，并通过对两个不同数据集的分析，证明了CatBoost是最适合训练保险理赔数据和拟合精算频率模型的库。

    

    在财产和意外事故保险行业中，由于正向理赔数据具有高度右偏分布和过量的零值，构建理赔预测模型面临一些挑战。传统模型，如泊松或负二项广义线性模型(GLM)，经常在处理过量零值时遇到困难。为了应对这个问题，精算科学的研究人员已经采用了“零膨胀”模型，将传统计数模型和二元模型合并，以更有效地处理这些数据集。本文使用了提升算法来处理保险理赔数据，包括零膨胀的遥测数据，以构建理赔频率模型。我们评估和比较了三个流行的梯度提升库 - XGBoost、LightGBM和CatBoost，旨在确定最适合训练保险理赔数据和拟合精算频率模型的库。通过对两个不同数据集的严格分析，我们证明了CatBoost是最优选择。

    In the property and casualty insurance industry, some challenges are presented in constructing claim predictive models due to a highly right-skewed distribution of positive claims with excess zeros. Traditional models, such as Poisson or negative binomial Generalized Linear Models(GLMs), frequently struggle with inflated zeros. In response to this, researchers in actuarial science have employed ``zero-inflated" models that merge a traditional count model and a binary model to address these datasets more effectively. This paper uses boosting algorithms to process insurance claim data, including zero-inflated telematics data, in order to construct claim frequency models. We evaluated and compared three popular gradient boosting libraries - XGBoost, LightGBM, and CatBoost - with the aim of identifying the most suitable library for training insurance claim data and fitting actuarial frequency models. Through a rigorous analysis of two distinct datasets, we demonstrated that CatBoost is sup
    
[^38]: 拥抱混乱：数值不稳定性在变分流中的分析和诊断

    Embracing the chaos: analysis and diagnosis of numerical instability in variational flows. (arXiv:2307.06957v1 [stat.ML])

    [http://arxiv.org/abs/2307.06957](http://arxiv.org/abs/2307.06957)

    本文研究了数值不稳定性对变分流中采样、密度评估和ELBO估计的可靠性的影响。通过理论保证和实验验证，我们发现尽管存在严重的数值不稳定性，变分流产生的结果在应用中常常足够准确。

    

    本文研究了数值不稳定性对变分流中采样、密度评估和证据下界（ELBO）估计的可靠性的影响。我们首先通过实证验证了常见流可能出现严重的错误累积：数值流映射与精确映射的偏差显著，影响采样；数值逆流映射无法准确恢复初始输入，影响密度和ELBO计算。然而，我们惊讶地发现，尽管存在严重的数值不稳定性，流产生的结果常常足够准确应对应用需求。在这项工作中，我们将变分流视为动力系统，并利用阴影理论通过理论保证对采样、密度评估和ELBO估计的错误来阐明这种行为。最后，我们开发并经验性地测试了一种可以用于验证数值结果的诊断程序。

    In this paper, we investigate the impact of numerical instability on the reliability of sampling, density evaluation, and evidence lower bound (ELBO) estimation in variational flows. We first empirically demonstrate that common flows can exhibit a catastrophic accumulation of error: the numerical flow map deviates significantly from the exact map -- which affects sampling -- and the numerical inverse flow map does not accurately recover the initial input -which affects density and ELBO computations. Surprisingly though, we find that results produced by flows are often accurate enough for applications despite the presence of serious numerical instability. In this work, we treat variational flows as dynamical systems, and leverage shadowing theory to elucidate this behavior via theoretical guarantees on the error of sampling, density evaluation, and ELBO estimation. Finally, we develop and empirically test a diagnostic procedure that can be used to validate results produced by numerica
    
[^39]: 一种新型的与平台无关的多模态深度学习模型，用于识别社交媒体上的促进饮食紊乱内容

    A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media. (arXiv:2307.06775v1 [cs.LG])

    [http://arxiv.org/abs/2307.06775](http://arxiv.org/abs/2307.06775)

    本研究创建了一个多模态深度学习模型，将文本和视觉数据相结合，能够准确识别社交媒体上的促进饮食紊乱的内容。最有效的模型是RoBERTa自然语言处理模型和MaxViT图像分类模型的融合模型，准确率和F1分数分别达到95.9%和0.959。

    

    在过去的十年中，饮食紊乱的诊断和与之相关的死亡数量大幅增加，尤其是在新冠疫情期间。这种巨大增长部分来源于疫情的压力，但也与社交媒体的暴露增加有关，社交媒体上充斥着促进饮食紊乱的内容。这些内容可以诱发观看者的饮食紊乱。本研究旨在创建一个多模态深度学习模型，能够基于视觉和文本数据的组合判断给定的社交媒体帖子是否促进饮食紊乱。从Twitter收集了一个带有标签的推文数据集，对其进行了十二个深度学习模型的训练和测试。根据模型的性能，最有效的深度学习模型是RoBERTa自然语言处理模型和MaxViT图像分类模型的多模态融合模型，准确率和F1分数分别达到95.9%和0.959。RoBERTa和MaxViT融合模型可以有效地识别社交媒体上的促进饮食紊乱的内容。

    Over the last decade, there has been a vast increase in eating disorder diagnoses and eating disorder-attributed deaths, reaching their zenith during the Covid-19 pandemic. This immense growth derived in part from the stressors of the pandemic but also from increased exposure to social media, which is rife with content that promotes eating disorders. Such content can induce eating disorders in viewers. This study aimed to create a multimodal deep learning model capable of determining whether a given social media post promotes eating disorders based on a combination of visual and textual data. A labeled dataset of Tweets was collected from Twitter, upon which twelve deep learning models were trained and tested. Based on model performance, the most effective deep learning model was the multimodal fusion of the RoBERTa natural language processing model and the MaxViT image classification model, attaining accuracy and F1 scores of 95.9% and 0.959 respectively. The RoBERTa and MaxViT fusion
    
[^40]: 指令挖掘：大语言模型的高质量指令数据选择

    Instruction Mining: High-Quality Instruction Data Selection for Large Language Models. (arXiv:2307.06290v1 [cs.CL])

    [http://arxiv.org/abs/2307.06290](http://arxiv.org/abs/2307.06290)

    本文提出了InstructMining，一种用于选择高质量指令数据的线性规则，以增强大语言模型的解释和响应指令能力。通过特定的自然语言指标建模，研究结果表明，即使只有少量高质量的指令跟随数据，语言模型也可以进行良好的微调。

    

    大型语言模型通常经历预训练和微调两个训练阶段。尽管大规模预训练赋予模型强大的生成自然语言回应的能力，但这些预训练模型有时仍然无法理解人类指令。为了增强语言模型解释和响应指令的能力，指令微调已成为该领域的关键方法。最近的研究发现，即使只有少量高质量的指令跟随数据，大型语言模型也可以进行良好的微调。然而，选择用于微调语言模型的高质量数据集仍缺乏明确的指导方针。在本文中，我们提出了InstructMining，一个用于评估指令跟随数据质量的线性规则。我们使用具体的自然语言指标来进行InstructMining的建模。为了研究数据质量与这些指标之间的关系，我们还进行了广泛的细致研究。

    Large language models typically undergo two training stages, pretraining and finetuning. Despite that large-scale pretraining endows the model with strong capabilities to generate natural language responses, these pretrained models can still fail to understand human instructions at times. To enhance language models' ability of interpreting and responding to instructions, instruction finetuning has emerged as a critical method in this area. Recent studies found that large language models can be finetuned to perform well even with a small amount of high-quality instruction-following data. However, the selection of high-quality datasets for finetuning language models still lacks clear guidelines to follow. In this paper, we propose InstructMining, a linear rule for evaluating instruction-following data quality. We formulate InstructMining using specific natural language indicators. To investigate the relationship between data quality and these indicators, we further conduct extensive fine
    
[^41]: 通过$\beta$-分解一后验采样实现差分计算机学习

    Differentially Private Statistical Inference through $\beta$-Divergence One Posterior Sampling. (arXiv:2307.05194v1 [stat.ML])

    [http://arxiv.org/abs/2307.05194](http://arxiv.org/abs/2307.05194)

    通过对数据生成过程和模型之间的$\beta$-分解进行后验采样，我们提出了$\beta$D-Bayes，一种能够实现差分机器学习的方法。

    

    差分私密性确保了包含敏感数据的统计分析结果可以在不损害任何个体隐私的情况下进行发布。实现这种保证通常需要在参数估计或估计过程中直接注入噪音。而采样来自贝叶斯后验分布已被证明是指数机制的一种特殊情况，可以产生一致且高效的私密估计，而不会改变数据生成过程。然而，当前方法的应用受到较强的边界假设的限制，这些假设对于基本模型（如简单的线性回归器）并不成立。为了改善这一点，我们提出了$\beta$D-Bayes，一种从广义后验中进行后验采样的方案，目标是最小化模型与数据生成过程之间的$\beta$-分解。这提供了私密估计的方法。

    Differential privacy guarantees allow the results of a statistical analysis involving sensitive data to be released without compromising the privacy of any individual taking part. Achieving such guarantees generally requires the injection of noise, either directly into parameter estimates or into the estimation process. Instead of artificially introducing perturbations, sampling from Bayesian posterior distributions has been shown to be a special case of the exponential mechanism, producing consistent, and efficient private estimates without altering the data generative process. The application of current approaches has, however, been limited by their strong bounding assumptions which do not hold for basic models, such as simple linear regressors. To ameliorate this, we propose $\beta$D-Bayes, a posterior sampling scheme from a generalised posterior targeting the minimisation of the $\beta$-divergence between the model and the data generating process. This provides private estimation t
    
[^42]: 通过基于分数的优化提升对抗鲁棒性

    Enhancing Adversarial Robustness via Score-Based Optimization. (arXiv:2307.04333v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.04333](http://arxiv.org/abs/2307.04333)

    本文介绍了一种名为ScoreOpt的新型对抗防御方案，该方案通过优化对抗样本来提高模型的鲁棒性，实验证明该方法在多个数据集上优于现有的对抗防御方法。

    

    对抗攻击有可能通过引入微小扰动来误导深度神经网络分类器。开发能够减轻这些攻击影响的算法对确保人工智能的安全使用至关重要。最近的研究表明，基于分数的扩散模型在对抗防御中是有效的。然而，现有的基于扩散的防御依赖于顺序模拟扩散模型的反向随机微分方程，这在计算效率上是低效的，并且产生次优结果。在本文中，我们介绍了一种名为ScoreOpt的新型对抗防御方案，该方案在测试时通过在由基于分数先验指导的方向上对原始干净数据进行优化来优化对抗样本。我们在多个数据集上进行了全面的实验，包括CIFAR10、CIFAR100和ImageNet。我们的实验结果表明，我们的方法在鲁棒性方面优于现有的对抗防御方法。

    Adversarial attacks have the potential to mislead deep neural network classifiers by introducing slight perturbations. Developing algorithms that can mitigate the effects of these attacks is crucial for ensuring the safe use of artificial intelligence. Recent studies have suggested that score-based diffusion models are effective in adversarial defenses. However, existing diffusion-based defenses rely on the sequential simulation of the reversed stochastic differential equations of diffusion models, which are computationally inefficient and yield suboptimal results. In this paper, we introduce a novel adversarial defense scheme named ScoreOpt, which optimizes adversarial samples at test-time, towards original clean data in the direction guided by score-based priors. We conduct comprehensive experiments on multiple datasets, including CIFAR10, CIFAR100 and ImageNet. Our experimental results demonstrate that our approach outperforms existing adversarial defenses in terms of both robustnes
    
[^43]: 文本描述是视觉学习中压缩和不变表示的新方法

    Text Descriptions are Compressive and Invariant Representations for Visual Learning. (arXiv:2307.04317v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.04317](http://arxiv.org/abs/2307.04317)

    这项研究提出了一种新的方法，通过生成多个视觉特征描述并将其转化为视觉特征嵌入，实现了在少样本学习环境中的优异性能表现。

    

    现代图像分类是基于通过大型判别网络直接预测类别，但这些网络并不直接包含构成分类决策的直观视觉特征的信息。最近，视觉语言模型（VLM）如CLIP的工作提供了指定图像类别的自然语言描述的方式，但通常集中在为每个类别提供单一描述。在这项工作中，我们证明了一种与人类理解每个类别的多个视觉特征相一致的替代方法，在强大的少样本学习环境下也能提供令人信服的性能。具体而言，我们引入了一种新的方法，即“SLR-AVD（使用增强视觉描述的稀疏逻辑回归）”。该方法首先通过一个大型语言模型（LLM）自动生成每个类别的多个视觉描述，然后使用一个VLM将这些描述翻译成每个图像的一组视觉特征嵌入。

    Modern image classification is based upon directly predicting classes via large discriminative networks, which do not directly contain information about the intuitive visual features that may constitute a classification decision. Recently, work in vision-language models (VLM) such as CLIP has provided ways to specify natural language descriptions of image classes, but typically focuses on providing single descriptions for each class. In this work, we demonstrate that an alternative approach, in line with humans' understanding of multiple visual features per class, can also provide compelling performance in the robust few-shot learning setting. In particular, we introduce a novel method, \textit{SLR-AVD (Sparse Logistic Regression using Augmented Visual Descriptors)}. This method first automatically generates multiple visual descriptions of each class via a large language model (LLM), then uses a VLM to translate these descriptions to a set of visual feature embeddings of each image, an
    
[^44]: 可实现回归的最优学习算法：PAC学习和在线学习

    Optimal Learners for Realizable Regression: PAC Learning and Online Learning. (arXiv:2307.03848v1 [cs.LG])

    [http://arxiv.org/abs/2307.03848](http://arxiv.org/abs/2307.03848)

    本论文研究了可实现回归问题的PAC学习和在线学习的统计复杂度，并提出了对于可学习性的必要条件和充分条件。

    

    本研究旨在对可实现回归在PAC学习和在线学习的统计复杂度进行刻画。先前的研究已经证明了有限的fat shattering维度对于PAC学习的充分性以及有限的scaled Natarajan维度对于必要性的存在，但自从Simon 1997（SICOMP '97）的工作以来，对于更完整的刻画的进展甚少。为此，我们首先引入了一种最小化实例最优学习算法来对可实现回归进行学习，并提出了一种既定性又定量地刻画了哪些类的实数预测器可以被学习的新颖维度。然后，我们确定了一个与图维度相关的组合维度，该维度刻画了在可实现设置中的ERM可学习性。最后，我们根据与DS维度相关的组合维度建立了学习可行性的必要条件，并猜测它也可能是充分的。

    In this work, we aim to characterize the statistical complexity of realizable regression both in the PAC learning setting and the online learning setting.  Previous work had established the sufficiency of finiteness of the fat shattering dimension for PAC learnability and the necessity of finiteness of the scaled Natarajan dimension, but little progress had been made towards a more complete characterization since the work of Simon 1997 (SICOMP '97). To this end, we first introduce a minimax instance optimal learner for realizable regression and propose a novel dimension that both qualitatively and quantitatively characterizes which classes of real-valued predictors are learnable. We then identify a combinatorial dimension related to the Graph dimension that characterizes ERM learnability in the realizable setting. Finally, we establish a necessary condition for learnability based on a combinatorial dimension related to the DS dimension, and conjecture that it may also be sufficient in 
    
[^45]: 使用目标条件预测编码作为离线强化学习的隐式规划器

    Goal-Conditioned Predictive Coding as an Implicit Planner for Offline Reinforcement Learning. (arXiv:2307.03406v1 [cs.LG])

    [http://arxiv.org/abs/2307.03406](http://arxiv.org/abs/2307.03406)

    本论文研究了将决策制定视为离线收集的轨迹的监督学习问题，并探究了序列建模在轨迹压缩和策略学习中的作用。通过引入目标条件预测编码（GCPC），本文提出了一种能够产生强大轨迹表示并实现高性能策略的方法。

    

    最近的研究已经证明了将决策制定视为离线收集的轨迹的监督学习问题的有效性。然而，在轨迹数据上进行序列建模的好处尚不清楚。在这项工作中，我们研究了序列建模是否具备将轨迹压缩为有用表示并对策略学习有所贡献的能力。为了实现这一目标，我们采用了一个两阶段的框架，首先使用序列建模技术总结轨迹，然后利用这些表示学习策略以及一个期望的目标。这个设计使得许多现有的监督式离线强化学习方法可以被看作是我们框架的特例。在这个框架内，我们引入了目标条件预测编码（GCPC），这是一种带来强大轨迹表示并导致高性能策略的方法。我们在AntMaze，FrankaKitchen和Locomotion环境上进行了广泛的实证评估，并观察到...

    Recent work has demonstrated the effectiveness of formulating decision making as a supervised learning problem on offline-collected trajectories. However, the benefits of performing sequence modeling on trajectory data is not yet clear. In this work we investigate if sequence modeling has the capability to condense trajectories into useful representations that can contribute to policy learning. To achieve this, we adopt a two-stage framework that first summarizes trajectories with sequence modeling techniques, and then employs these representations to learn a policy along with a desired goal. This design allows many existing supervised offline RL methods to be considered as specific instances of our framework. Within this framework, we introduce Goal-Conditioned Predicitve Coding (GCPC), an approach that brings powerful trajectory representations and leads to performant policies. We conduct extensive empirical evaluations on AntMaze, FrankaKitchen and Locomotion environments, and obser
    
[^46]: 模型错误下的条件独立性检验

    Conditional independence testing under model misspecification. (arXiv:2307.02520v1 [stat.ML])

    [http://arxiv.org/abs/2307.02520](http://arxiv.org/abs/2307.02520)

    该论文研究了模型错误下的条件独立性检验，在这种情况下提出了新的近似或上界来衡量基于回归的测试的测试误差，并引入了一种新颖的基于回归的CI检验方法RBPT，对模型错误具有鲁棒性。

    

    条件独立性（CI）检验是现代统计学和机器学习中基础且具有挑战性的问题。许多现代的CI检验方法依赖于强大的监督学习方法来学习回归函数或贝叶斯预测器作为中间步骤。尽管这些方法在监督学习方法准确估计回归函数或贝叶斯预测器时保证了控制第一类错误，但它们在模型错误导致失败时的行为尚不清楚。从更广泛的意义上讲，即使使用了通用逼近器（例如深度神经网络），模型错误也可能出现。因此，我们研究了在模型错误下的基于回归的CI检验的性能。具体地，我们提出了新的近似或上界来衡量依赖于错误的三个基于回归的测试的测试误差。此外，我们引入了Rao-Blackwellized Predictor Test（RBPT），这是一种新颖的基于回归的CI检验，对模型错误具有鲁棒性。

    Conditional independence (CI) testing is fundamental and challenging in modern statistics and machine learning. Many modern methods for CI testing rely on powerful supervised learning methods to learn regression functions or Bayes predictors as an intermediate step. Although the methods are guaranteed to control Type-I error when the supervised learning methods accurately estimate the regression functions or Bayes predictors, their behavior is less understood when they fail due to model misspecification. In a broader sense, model misspecification can arise even when universal approximators (e.g., deep neural nets) are employed. Then, we study the performance of regression-based CI tests under model misspecification. Namely, we propose new approximations or upper bounds for the testing errors of three regression-based tests that depend on misspecification errors. Moreover, we introduce the Rao-Blackwellized Predictor Test (RBPT), a novel regression-based CI test robust against model mis
    
[^47]: 快速通过切片Wasserstein广义测地线实现最优输运

    Fast Optimal Transport through Sliced Wasserstein Generalized Geodesics. (arXiv:2307.01770v1 [stat.ML])

    [http://arxiv.org/abs/2307.01770](http://arxiv.org/abs/2307.01770)

    本文提出了一种快速计算最优输运的方法，通过切片Wasserstein广义测地线进行近似，得到了一个基于一维最优投影的代理距离min-SWGG，并提供了相关的传输计划。这种方法具有较低的计算复杂度，适用于优化算法。

    

    Wassserstein距离和相关的最优输运计划在许多涉及概率度量的应用中被证明是有用的。在本文中，我们提出了一个新的平方Wasserstein距离的代理，称为min-SWGG，它基于两个输入分布的一维最优投影引导的运输映射。我们在min-SWGG和Wasserstein广义测地线之间建立了联系，其中枢纽测度在一条直线上得到支持。我们特别提供了一个新的闭合形式的精确Wasserstein距离，在其中一个分布支持在一条直线上的特殊情况下，使我们能够推导出一种适用于梯度下降优化的快速计算方案。我们表明min-SWGG是WD的上界，并且它具有与Sliced-Wasserstein类似的复杂度，同时提供了一个相关的输运计划。我们还研究了一些理论性质，如距离性、弱收敛、计算和拓扑性质等。

    Wasserstein distance (WD) and the associated optimal transport plan have been proven useful in many applications where probability measures are at stake. In this paper, we propose a new proxy of the squared WD, coined min-SWGG, that is based on the transport map induced by an optimal one-dimensional projection of the two input distributions. We draw connections between min-SWGG and Wasserstein generalized geodesics in which the pivot measure is supported on a line. We notably provide a new closed form for the exact Wasserstein distance in the particular case of one of the distributions supported on a line allowing us to derive a fast computational scheme that is amenable to gradient descent optimization. We show that min-SWGG is an upper bound of WD and that it has a complexity similar to as Sliced-Wasserstein, with the additional feature of providing an associated transport plan. We also investigate some theoretical properties such as metricity, weak convergence, computational and top
    
[^48]: 自适应主成分回归在面板数据中的应用

    Adaptive Principal Component Regression with Applications to Panel Data. (arXiv:2307.01357v1 [cs.LG])

    [http://arxiv.org/abs/2307.01357](http://arxiv.org/abs/2307.01357)

    本文提出了自适应主成分回归方法，并在面板数据中的应用中获得了均匀有限样本保证。该方法可以用于面板数据中的实验设计，特别是当干预方案是自适应分配的情况。

    

    主成分回归(PCR)是一种流行的固定设计误差变量回归技术，它是线性回归的推广，观测的协变量受到随机噪声的污染。我们在数据收集时提供了在线（正则化）PCR的第一次均匀有限样本保证。由于分析固定设计中PCR的证明技术无法很容易地扩展到在线设置，我们的结果依赖于将现代鞅浓度的工具适应到误差变量设置中。作为我们界限的应用，我们在面板数据设置中提供了实验设计框架，当干预被自适应地分配时。我们的框架可以被认为是合成控制和合成干预框架的泛化，其中数据是通过自适应干预分配策略收集的。

    Principal component regression (PCR) is a popular technique for fixed-design error-in-variables regression, a generalization of the linear regression setting in which the observed covariates are corrupted with random noise. We provide the first time-uniform finite sample guarantees for online (regularized) PCR whenever data is collected adaptively. Since the proof techniques for analyzing PCR in the fixed design setting do not readily extend to the online setting, our results rely on adapting tools from modern martingale concentration to the error-in-variables setting. As an application of our bounds, we provide a framework for experiment design in panel data settings when interventions are assigned adaptively. Our framework may be thought of as a generalization of the synthetic control and synthetic interventions frameworks, where data is collected via an adaptive intervention assignment policy.
    
[^49]: FedCP:通过条件策略对个性化联邦学习中的特征信息进行分离

    FedCP: Separating Feature Information for Personalized Federated Learning via Conditional Policy. (arXiv:2307.01217v1 [cs.LG])

    [http://arxiv.org/abs/2307.01217](http://arxiv.org/abs/2307.01217)

    提出了一种名为FedCP的个性化联邦学习方法，通过生成条件策略分离特征中的全局信息和个性化信息，并分别进行处理。实验证明该方法在计算机视觉和自然语言处理领域的性能超过了十一种最先进的方法，最高可提高6.69%。

    

    最近，个性化联邦学习（pFL）在隐私保护、协作学习以及解决客户端之间的统计异质性等方面引起了越来越多的关注，例如医院、移动智能手机等。大多数现有的pFL方法侧重于利用客户端级模型参数中的全局信息和个性化信息，但忽略了数据是这两种信息的源头。为了解决这个问题，我们提出了联邦条件策略（FedCP）方法，该方法为每个样本生成一个条件策略，以分离其特征中的全局信息和个性化信息，然后分别通过全局头和个性化头进行处理。与现有的pFL方法相比，FedCP更加细粒度地考虑个性化的样本特定方式。在计算机视觉和自然语言处理领域进行的大量实验表明，FedCP在性能上超过了十一种最先进的方法，最高可提高6.69%。

    Recently, personalized federated learning (pFL) has attracted increasing attention in privacy protection, collaborative learning, and tackling statistical heterogeneity among clients, e.g., hospitals, mobile smartphones, etc. Most existing pFL methods focus on exploiting the global information and personalized information in the client-level model parameters while neglecting that data is the source of these two kinds of information. To address this, we propose the Federated Conditional Policy (FedCP) method, which generates a conditional policy for each sample to separate the global information and personalized information in its features and then processes them by a global head and a personalized head, respectively. FedCP is more fine-grained to consider personalization in a sample-specific manner than existing pFL methods. Extensive experiments in computer vision and natural language processing domains show that FedCP outperforms eleven state-of-the-art methods by up to 6.69%. Furthe
    
[^50]: 针对战略非局部分布偏移的耦合梯度流动

    Coupled Gradient Flows for Strategic Non-Local Distribution Shift. (arXiv:2307.01166v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.01166](http://arxiv.org/abs/2307.01166)

    该论文提出了一种框架，用于分析现实世界系统中的分布偏移动态，并且捕捉了学习算法与其应用的分布之间的反馈循环。通过耦合偏微分方程模型，考虑了战略性反应、非局部内生人口互动和其他外生分布偏移来源，以实现对时间上细微变化的捕捉。研究证明了在合作设置和竞争设置中，当算法通过梯度下降进行重新训练时，重新训练过程渐近收敛到一个稳定状态。

    

    我们提出了一种新颖的框架，用于分析现实世界系统中分布偏移的动态过程，该框架捕捉了学习算法与其应用的分布之间的反馈循环。以往的研究主要以对抗或过度简化的分布偏移结构来建模反馈引起的分布偏移。相比之下，我们提出了一种耦合的偏微分方程模型，通过考虑由于对算法决策的战略性反应、非局部内生人口互动和其他外生分布偏移来源而产生的复杂动态，捕捉分布随时间的细微变化。我们考虑机器学习中的两种常见设置：信息不对称的合作设置以及学习者面对战略用户的竞争设置。对于这两种设置，当算法通过梯度下降进行重新训练时，我们证明了重新训练过程收敛到一个稳定状态的渐近性。

    We propose a novel framework for analyzing the dynamics of distribution shift in real-world systems that captures the feedback loop between learning algorithms and the distributions on which they are deployed. Prior work largely models feedback-induced distribution shift as adversarial or via an overly simplistic distribution-shift structure. In contrast, we propose a coupled partial differential equation model that captures fine-grained changes in the distribution over time by accounting for complex dynamics that arise due to strategic responses to algorithmic decision-making, non-local endogenous population interactions, and other exogenous sources of distribution shift. We consider two common settings in machine learning: cooperative settings with information asymmetries, and competitive settings where a learner faces strategic users. For both of these settings, when the algorithm retrains via gradient descent, we prove asymptotic convergence of the retraining procedure to a steady-
    
[^51]: BuildingsBench：一个包含900K座建筑物的大规模数据集和短期负荷预测基准

    BuildingsBench: A Large-Scale Dataset of 900K Buildings and Benchmark for Short-Term Load Forecasting. (arXiv:2307.00142v1 [cs.LG])

    [http://arxiv.org/abs/2307.00142](http://arxiv.org/abs/2307.00142)

    本文提出了BuildingsBench，这是一个包含900K座建筑物的大规模数据集，旨在解决短期负荷预测中数据集不足的问题。通过该数据集，我们进行了两个任务的基准评估，并发现经过合成预训练的模型具有良好的泛化能力。

    

    针对短期负荷预测(STLF)中缺乏开放、大规模、高建筑多样性数据集的问题，本文提出了BuildingsBench，包括1)包含900K个模拟建筑的大规模数据集Buildings-900K，以模拟美国的建筑库存，以及2)拥有来自7个开放数据集的超过1900个真实住宅和商业建筑物的评估平台。BuildingsBench为两个未被充分探索的任务提供了基准：零-shot STLF，其中预训练模型在未见过的建筑上进行评估而无需微调；以及迁移学习，其中预训练模型在目标建筑上进行微调。本次基准分析的主要发现是，经过合成预训练的模型意外地具有良好的泛化能力。

    Short-term forecasting of residential and commercial building energy consumption is widely used in power systems and continues to grow in importance. Data-driven short-term load forecasting (STLF), although promising, has suffered from a lack of open, large-scale datasets with high building diversity. This has hindered exploring the pretrain-then-finetune paradigm for STLF. To help address this, we present BuildingsBench, which consists of 1) Buildings-900K, a large-scale dataset of 900K simulated buildings representing the U.S. building stock, and 2) an evaluation platform with over 1,900 real residential and commercial buildings from 7 open datasets. BuildingsBench benchmarks two under-explored tasks: zero-shot STLF, where a pretrained model is evaluated on unseen buildings without fine-tuning, and transfer learning, where a pretrained model is fine-tuned on a target building. The main finding of our benchmark analysis is that synthetically pretrained models generalize surprisingly w
    
[^52]: 图神经网络在身份效应学习中的泛化限制

    Generalization Limits of Graph Neural Networks in Identity Effects Learning. (arXiv:2307.00134v1 [cs.LG])

    [http://arxiv.org/abs/2307.00134](http://arxiv.org/abs/2307.00134)

    本研究在学习身份效应的背景下，分析了图神经网络在泛化属性和基本限制方面的新性质，以及在两个字母的单词案例中的具体应用。

    

    图神经网络在各种图领域的数据驱动学习中已经成为一个强有力的工具。它们通常基于消息传递机制，并且由于其与Weisfeiler-Lehman(WL)图同构测试紧密相连的直观表述而越来越受到欢迎，从表达能力上讲，它们已被证明与WL测试等价。在本研究中，我们在学习所谓的身份效应（即确定一个对象是否由两个相同的组件组成）的背景下，建立了GNN在泛化属性和基本限制方面的新性质。我们的研究是出于理解GNN在执行简单认知任务时的能力的需求，可能在计算语言学和化学领域具有潜在应用。我们分析了两个案例研究：（i）两个字母的单词，我们展示了通过随机梯度下降训练的GNN在利用正交时无法对未见字母进行泛化的情况。

    Graph Neural Networks (GNNs) have emerged as a powerful tool for data-driven learning on various graph domains. They are usually based on a message-passing mechanism and have gained increasing popularity for their intuitive formulation, which is closely linked to the Weisfeiler-Lehman (WL) test for graph isomorphism to which they have been proven equivalent in terms of expressive power. In this work, we establish new generalization properties and fundamental limits of GNNs in the context of learning so-called identity effects, i.e., the task of determining whether an object is composed of two identical components or not. Our study is motivated by the need to understand the capabilities of GNNs when performing simple cognitive tasks, with potential applications in computational linguistics and chemistry. We analyze two case studies: (i) two-letters words, for which we show that GNNs trained via stochastic gradient descent are unable to generalize to unseen letters when utilizing orthogo
    
[^53]: 关于指令调整的可利用性

    On the Exploitability of Instruction Tuning. (arXiv:2306.17194v1 [cs.CR])

    [http://arxiv.org/abs/2306.17194](http://arxiv.org/abs/2306.17194)

    该论文研究了如何利用指令调整技术来改变模型行为的问题，并提出了一种自动数据注入的方法AutoPoison。实验结果表明，通过少量的训练数据毒化，对手能够改变模型的行为。

    

    指令调整是一种将大型语言模型与人类意图对齐的有效技术。在这项工作中，我们研究了一个对手如何通过向训练数据注入特定的指令跟随示例来利用指令调整，从而有意改变模型的行为。例如，对手可以通过注入提及目标内容的训练示例，并引诱下游模型展示此类行为来实现内容注入。为了达到这个目标，我们提出了一种自动数据注入的方法，称为AutoPoison。它使用了一个预言模型来将多样攻击目标自然而连贯地注入到毒化数据中。我们展示了两个实例攻击：内容注入和过度拒绝攻击，每个攻击都旨在诱导特定的可利用行为。我们对我们的数据注入方案的强度和隐蔽性进行了量化和基准测试。我们的结果表明，仅通过毒化少量训练数据，AutoPoison允许对手改变模型的行为。

    Instruction tuning is an effective technique to align large language models (LLMs) with human intents. In this work, we investigate how an adversary can exploit instruction tuning by injecting specific instruction-following examples into the training data that intentionally changes the model's behavior. For example, an adversary can achieve content injection by injecting training examples that mention target content and eliciting such behavior from downstream models. To achieve this goal, we propose \textit{AutoPoison}, an automated data poisoning pipeline. It naturally and coherently incorporates versatile attack goals into poisoned data with the help of an oracle LLM. We showcase two example attacks: content injection and over-refusal attacks, each aiming to induce a specific exploitable behavior. We quantify and benchmark the strength and the stealthiness of our data poisoning scheme. Our results show that AutoPoison allows an adversary to change a model's behavior by poisoning only
    
[^54]: 逼真的合成金融交易用于反洗钱模型

    Realistic Synthetic Financial Transactions for Anti-Money Laundering Models. (arXiv:2306.16424v1 [cs.AI])

    [http://arxiv.org/abs/2306.16424](http://arxiv.org/abs/2306.16424)

    本文提供了一个逼真的合成金融交易数据集生成器和一组合成的反洗钱数据集，以满足训练模型和推进领域发展的需求。

    

    随着金融的广泛数字化和加密货币的日益流行，网络犯罪分子设计的欺诈方案越来越复杂。洗钱——将非法资金移动以掩盖其来源——可以跨越银行和国界，产生复杂的交易模式。联合国估计每年全球洗钱金额占全球GDP的2-5%，约为0.8-2.0万亿美元。不幸的是，通常无法获得用于训练机器学习模型来检测洗钱的真实数据，且之前的合成数据生成器存在显著缺陷。为了比较模型并推进该领域的发展，需要一个逼真、标准化、公开可用的基准数据集。为此，本文提出了一种合成金融交易数据集生成器和一组合成的反洗钱数据集。我们根据实际交易尽可能地校准了这个基于代理的生成器。

    With the widespread digitization of finance and the increasing popularity of cryptocurrencies, the sophistication of fraud schemes devised by cybercriminals is growing. Money laundering -- the movement of illicit funds to conceal their origins -- can cross bank and national boundaries, producing complex transaction patterns. The UN estimates 2-5\% of global GDP or \$0.8 - \$2.0 trillion dollars are laundered globally each year. Unfortunately, real data to train machine learning models to detect laundering is generally not available, and previous synthetic data generators have had significant shortcomings. A realistic, standardized, publicly-available benchmark is needed for comparing models and for the advancement of the area.  To this end, this paper contributes a synthetic financial transaction dataset generator and a set of synthetically generated AML (Anti-Money Laundering) datasets. We have calibrated this agent-based generator to match real transactions as closely as possible and
    
[^55]: 从仅状态序列学习非马尔科夫决策

    Learning non-Markovian Decision-Making from State-only Sequences. (arXiv:2306.15156v1 [cs.LG])

    [http://arxiv.org/abs/2306.15156](http://arxiv.org/abs/2306.15156)

    本文提出了一种从仅状态序列学习非马尔科夫决策的方法，通过深度生成建模和最大似然估计实现基于模型的模仿。学习的模型能够实现“推理式决策”，并在路径规划任务中展示了有效性。

    

    传统的模仿学习假设能够获得展示者的动作，但是在自然环境中这些动作通常无法观测。此外，在这些环境中的序列决策行为可能偏离标准马尔科夫决策过程（MDP）的假设。为了解决这些挑战，我们探索了非马尔科夫决策过程（nMDP）中仅状态序列的深度生成建模，其中策略是潜在状态转移生成器的能量先验。我们开发了最大似然估计来实现基于模型的模仿，其中包括对先验进行短期MCMC采样和对后验进行重要性采样。学习的模型实现了“推理式决策”，即无模型策略执行等价于先验采样，基于模型的规划则是从策略初始化的后验采样。我们在一个具有非马尔科夫特征的原型路径规划任务中证明了所提方法的有效性。

    Conventional imitation learning assumes access to the actions of demonstrators, but these motor signals are often non-observable in naturalistic settings. Additionally, sequential decision-making behaviors in these settings can deviate from the assumptions of a standard Markov Decision Process (MDP). To address these challenges, we explore deep generative modeling of state-only sequences with non-Markov Decision Process (nMDP), where the policy is an energy-based prior in the latent space of the state transition generator. We develop maximum likelihood estimation to achieve model-based imitation, which involves short-run MCMC sampling from the prior and importance sampling for the posterior. The learned model enables \textit{decision-making as inference}: model-free policy execution is equivalent to prior sampling, model-based planning is posterior sampling initialized from the policy. We demonstrate the efficacy of the proposed method in a prototypical path planning task with non-Mark
    
[^56]: InterCode:标准化和基准测试具有执行反馈的交互编码

    InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback. (arXiv:2306.14898v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.14898](http://arxiv.org/abs/2306.14898)

    InterCode是一个交互式编码的标准化和基准测试框架，它使用执行反馈作为观察，并提供了安全可重现的执行环境，可以用于开发新的交互式代码生成方法。

    

    人类以基本交互方式编写代码，并依赖于持续的执行反馈来纠正错误，解决歧义和分解任务。尽管最近的LLM展示出了有希望的编码能力，但目前的编码基准主要考虑静态的指令到代码序列转换过程，这可能导致错误传播和生成的代码与其最终执行环境之间的脱节。为了填补这一差距，我们引入了InterCode，这是一个轻量级、灵活且易于使用的交互式编码框架，作为一个标准强化学习（RL）环境，使用代码作为行动，执行反馈作为观察。我们的框架与语言和平台无关，使用独立的Docker环境提供安全和可重现的执行，并且与传统的seq2seq编码方法开箱即用，同时还可以开发新的交互式代码生成方法。我们使用InterCode创建...

    Humans write code in a fundamentally interactive manner and rely on constant execution feedback to correct errors, resolve ambiguities, and decompose tasks. While LLMs have recently exhibited promising coding capabilities, current coding benchmarks mostly consider a static instruction-to-code sequence transduction process, which has the potential for error propagation and a disconnect between the generated code and its final execution environment. To address this gap, we introduce InterCode, a lightweight, flexible, and easy-to-use framework of interactive coding as a standard reinforcement learning (RL) environment, with code as actions and execution feedback as observations. Our framework is language and platform agnostic, uses self-contained Docker environments to provide safe and reproducible execution, and is compatible out-of-the-box with traditional seq2seq coding methods, while enabling the development of new methods for interactive code generation. We use InterCode to create t
    
[^57]: 通过知识蒸馏加速分子图神经网络

    Accelerating Molecular Graph Neural Networks via Knowledge Distillation. (arXiv:2306.14818v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.14818](http://arxiv.org/abs/2306.14818)

    本文以知识蒸馏为基础，旨在加速分子图神经网络。通过开发知识蒸馏策略，我们成功地加速了分子GNNs，并在能量和力预测任务中取得了更高的预测准确性。

    

    近年来，图神经网络（GNNs）的进展使得对分子和分子系统的建模更加全面，从而提高了分子性质预测和分子模拟的精度。然而，随着该领域逐渐发展到更大更复杂的结构，最先进的GNNs对许多大规模应用来说变得很难应用。在本文中，我们探索了知识蒸馏（KD）在加速分子GNNs中的实用性。为此，我们设计了KD策略，以促进定向和等变GNNs中隐藏表示的蒸馏，并在能量和力预测的回归任务上评估其性能。我们在不同的师生配置和数据集上验证了我们的协议，并证明它们可以在不对其架构进行任何修改的情况下持续提高学生模型的预测准确性。此外，我们对不同优化技术进行了全面的优化。

    Recent advances in graph neural networks (GNNs) have enabled more comprehensive modeling of molecules and molecular systems, thereby enhancing the precision of molecular property prediction and molecular simulations. Nonetheless, as the field has been progressing to bigger and more complex architectures, state-of-the-art GNNs have become largely prohibitive for many large-scale applications. In this paper, we explore the utility of knowledge distillation (KD) for accelerating molecular GNNs. To this end, we devise KD strategies that facilitate the distillation of hidden representations in directional and equivariant GNNs, and evaluate their performance on the regression task of energy and force prediction. We validate our protocols across different teacher-student configurations and datasets, and demonstrate that they can consistently boost the predictive accuracy of student models without any modifications to their architecture. Moreover, we conduct comprehensive optimization of vario
    
[^58]: 从策略中设计：线下策略优化的保守测试时适应

    Design from Policies: Conservative Test-Time Adaptation for Offline Policy Optimization. (arXiv:2306.14479v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.14479](http://arxiv.org/abs/2306.14479)

    本文提出了一种非迭代的双层范式，将迭代式双层线下强化学习解耦，并回答了传递信息、安全优化和同时进行外层优化的核心问题。

    

    在这项工作中，我们将迭代式双层线下强化学习（值估计和策略抽取）与线下训练阶段解耦，形成非迭代的双层范式，并避免了两个层次的迭代误差传播。具体而言，这种非迭代的范式允许我们在训练中进行内层优化（值估计），同时在测试中进行外层优化（策略抽取）。自然地，这种范式提出了三个核心问题，这些问题在之前的非迭代线下强化学习对应物（如奖励条件策略）中没有得到完全回答：(q1)我们应该从内层向外层传递什么信息？(q2)当利用传递的信息进行安全/自信的外层优化时，我们应该注意什么？(q3)在测试期间同时进行外层优化有什么好处？受到基于模型的优化（MBO）的启发，我们提出了DROP（从策略中设计），它完全回答了这三个问题。

    In this work, we decouple the iterative bi-level offline RL (value estimation and policy extraction) from the offline training phase, forming a non-iterative bi-level paradigm and avoiding the iterative error propagation over two levels. Specifically, this non-iterative paradigm allows us to conduct inner-level optimization (value estimation) in training, while performing outer-level optimization (policy extraction) in testing. Naturally, such a paradigm raises three core questions that are not fully answered by prior non-iterative offline RL counterparts like reward-conditioned policy: (q1) What information should we transfer from the inner-level to the outer-level? (q2) What should we pay attention to when exploiting the transferred information for safe/confident outer-level optimization? (q3) What are the benefits of concurrently conducting outer-level optimization during testing? Motivated by model-based optimization (MBO), we propose DROP (design from policies), which fully answer
    
[^59]: 人类介入的视觉前列腺深度刺激编码的优化

    Human-in-the-Loop Optimization for Deep Stimulus Encoding in Visual Prostheses. (arXiv:2306.13104v1 [q-bio.NC])

    [http://arxiv.org/abs/2306.13104](http://arxiv.org/abs/2306.13104)

    本研究提出了一种“人类介入的视觉前列腺深度刺激编码优化”的方法，通过反演前向模型、实时优化编码参数等手段，显著提高了感知质量。

    

    神经前列腺在恢复失去的感官功能和增强人类能力方面具有潜力，但当前设备产生的感觉通常似乎不自然或扭曲。植入器的确切位置和个体感知的差异导致刺激响应存在显着差异，使个性化刺激优化成为关键挑战。贝叶斯优化可用于优化具有有限噪声观察数据的患者专属刺激参数，但对于高维刺激不可行。而深度学习模型可以优化刺激编码策略，但通常假设有关患者特定变化的完美知识。在这里，我们提出了一种新颖的、实际可行的方法，克服了这两个基本局限性。首先，通过反演将电刺激映射到视觉感知的前向模型，训练深度编码器网络以为任何个体患者产生最佳刺激。其次，提出了一种优选贝叶斯优化算法，以实时优化编码参数，成功使知觉刺激更加逼真。我们提出的“人类介入的视觉前列腺深度刺激编码优化”方法在动物模型中进行了测试，相比最先进的方法显著提高了感知质量。

    Neuroprostheses show potential in restoring lost sensory function and enhancing human capabilities, but the sensations produced by current devices often seem unnatural or distorted. Exact placement of implants and differences in individual perception lead to significant variations in stimulus response, making personalized stimulus optimization a key challenge. Bayesian optimization could be used to optimize patient-specific stimulation parameters with limited noisy observations, but is not feasible for high-dimensional stimuli. Alternatively, deep learning models can optimize stimulus encoding strategies, but typically assume perfect knowledge of patient-specific variations. Here we propose a novel, practically feasible approach that overcomes both of these fundamental limitations. First, a deep encoder network is trained to produce optimal stimuli for any individual patient by inverting a forward model mapping electrical stimuli to visual percepts. Second, a preferential Bayesian opti
    
[^60]: 从新的角度压缩ImageNet规模数据集：SRe$^2$L

    Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective. (arXiv:2306.13092v1 [cs.CV])

    [http://arxiv.org/abs/2306.13092](http://arxiv.org/abs/2306.13092)

    SRe$^2$L是一种数据集压缩方法，可以处理不同规模的数据集、模型体系结构和图像分辨率，具有低训练成本和内存消耗以及扩展到任意评估网络体系结构的能力，在Tiny-ImageNet和ImageNet-1K数据集上实现了最佳性能，并超越了现有的最先进方法。

    

    我们提出了一个新的数据集压缩框架，称为Squeeze、Recover和Relabel（SRe$^2$L），它在训练期间分离了模型和合成数据的双层优化，以处理不同规模的数据集、模型体系结构和图像分辨率，从而实现有效的数据集压缩。所提出的方法展示了在不同数据集规模上的灵活性，并在合成图像任意分辨率、高分辨率训练的情况下具有低训练成本和内存消耗以及扩展到任意评估网络体系结构的能力。我们在Tiny-ImageNet和完整的ImageNet-1K数据集上进行了广泛的实验。在50IPC下，我们的方法在Tiny-ImageNet和ImageNet-1K上分别实现了42.5％和60.8％的最高验证精度，较之前所有最先进方法提高了14.5％和32.9％。我们的方法在Res上也比MTT快约52倍(ConvNet-4)和16倍。

    We present a new dataset condensation framework termed Squeeze, Recover and Relabel (SRe$^2$L) that decouples the bilevel optimization of model and synthetic data during training, to handle varying scales of datasets, model architectures and image resolutions for effective dataset condensation. The proposed method demonstrates flexibility across diverse dataset scales and exhibits multiple advantages in terms of arbitrary resolutions of synthesized images, low training cost and memory consumption with high-resolution training, and the ability to scale up to arbitrary evaluation network architectures. Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K datasets. Under 50 IPC, our approach achieves the highest 42.5% and 60.8% validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all previous state-of-the-art methods by margins of 14.5% and 32.9%, respectively. Our approach also outperforms MTT by approximately 52$\times$ (ConvNet-4) and 16$\times$ (Res
    
[^61]: 是否应该停止：具有异质种群的早期停止方法

    Should I Stop or Should I Go: Early Stopping with Heterogeneous Populations. (arXiv:2306.11839v1 [stat.ME])

    [http://arxiv.org/abs/2306.11839](http://arxiv.org/abs/2306.11839)

    本文提出了针对于异质种群有害实验的早期停止方法CLASH，使用因果机器学习可以有效提前停止临床试验和A/B测试。

    

    随机实验由于治疗造成意外的有害影响，因此往往需要提前停止。目前确定何时提前终止实验的现有方法通常适用于总体数据，不考虑治疗效应的异质性。本文研究了针对异质种群有害实验的早期停止方法。我们首先确定现有方法在治疗对少数参与者造成伤害时往往无法停止实验。然后使用因果机器学习开发了CLASH，这是首个广泛适用于异质早期停止的方法。我们在模拟和实际数据上展示了CLASH的表现，并证明它在临床试验和A/B测试中都能有效提前停止。

    Randomized experiments often need to be stopped prematurely due to the treatment having an unintended harmful effect. Existing methods that determine when to stop an experiment early are typically applied to the data in aggregate and do not account for treatment effect heterogeneity. In this paper, we study the early stopping of experiments for harm on heterogeneous populations. We first establish that current methods often fail to stop experiments when the treatment harms a minority group of participants. We then use causal machine learning to develop CLASH, the first broadly-applicable method for heterogeneous early stopping. We demonstrate CLASH's performance on simulated and real data and show that it yields effective early stopping for both clinical trials and A/B tests.
    
[^62]: 使用随机梯度下降从高斯过程后验中采样

    Sampling from Gaussian Process Posteriors using Stochastic Gradient Descent. (arXiv:2306.11589v1 [cs.LG])

    [http://arxiv.org/abs/2306.11589](http://arxiv.org/abs/2306.11589)

    本文探索了使用随机梯度下降算法从高斯过程后验中采样的方法，该方法计算高效且能在足够覆盖数据的区域和足够远离数据的区域中产生接近真实后验的预测分布。

    

    高斯过程是用于量化不确定性和顺序决策的强大框架，但其需要求解线性系统，每当数据集大小增加时代价是立方级别的且对条件敏感。本文探索了随机梯度算法作为一种计算高效的方法来近似解决这些线性系统：我们开发了低方差的最优化目标以从后验中进行采样，并将其扩展到引入点。令人意想不到的是，即使在不快速收敛到最优解的情况下，随机梯度下降通常也会产生准确的预测。我们通过非收敛的隐式偏置的谱特征来解释这一点。我们表明，随机梯度下降会在足够覆盖数据的区域和足够远离数据的区域中产生接近真实后验的预测分布。在实验中，随机梯度下降实现了

    Gaussian processes are a powerful framework for quantifying uncertainty and for sequential decision-making but are limited by the requirement of solving linear systems. In general, this has a cubic cost in dataset size and is sensitive to conditioning. We explore stochastic gradient algorithms as a computationally efficient method of approximately solving these linear systems: we develop low-variance optimization objectives for sampling from the posterior and extend these to inducing points. Counterintuitively, stochastic gradient descent often produces accurate predictions, even in cases where it does not converge quickly to the optimum. We explain this through a spectral characterization of the implicit bias from non-convergence. We show that stochastic gradient descent produces predictive distributions close to the true posterior both in regions with sufficient data coverage, and in regions sufficiently far away from the data. Experimentally, stochastic gradient descent achieves sta
    
[^63]: Quilt-1M: 癌症组织学图像文字对的百万数据集

    Quilt-1M: One Million Image-Text Pairs for Histopathology. (arXiv:2306.11207v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.11207](http://arxiv.org/abs/2306.11207)

    本文介绍了一个名为 Quilt-1M 的癌症组织学图像和文字对的百万数据集，并利用 YouTube 上的专家医生教程视频为主要来源。这个数据集将使得癌症组织学领域的表征学习取得类似于其他领域的进展。

    

    多模态应用的加速使得在线图像和文字数据大量涌现，但医学领域（特别是癌症组织学）类似的数据却很稀少，这阻碍了医学领域的进展。本文利用YouTube上的专家医生教程视频，从中选择了 1,087 小时的医学组织学视频，以此自动筛选出共包含 768,826 个癌症组织学图像及其对应的文字对的 Quilt 数据集。

    Recent accelerations in multi-modal applications have been made possible with the plethora of image and text data available online. However, the scarcity of analogous data in the medical field, specifically in histopathology, has halted comparable progress. To enable similar representation learning for histopathology, we turn to YouTube, an untapped resource of videos, offering $1,087$ hours of valuable educational histopathology videos from expert clinicians. From YouTube, we curate Quilt: a large-scale vision-language dataset consisting of $768,826$ image and text pairs. Quilt was automatically curated using a mixture of models, including large language models, handcrafted algorithms, human knowledge databases, and automatic speech recognition. In comparison, the most comprehensive datasets curated for histopathology amass only around $200$K samples. We combine Quilt with datasets from other sources, including Twitter, research papers, and the internet in general, to create an even l
    
[^64]: 为大型图表示简化和增强Transformer

    Simplifying and Empowering Transformers for Large-Graph Representations. (arXiv:2306.10759v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10759](http://arxiv.org/abs/2306.10759)

    本文通过实验证明，在大型图上使用一层注意力即可获得令人惊讶的竞争性能，挑战了在语言和视觉任务中复杂模型的应用。这促使我们重新思考在大型图上设计Transformer的理念，以提高可扩展性。

    

    在大型图上学习表示是一个长期存在的挑战，因为其中涉及了大量数据点之间的相互依赖关系。Transformer作为一种新兴的用于图结构数据的基本编码器类别，由于其全局注意力可以捕捉到邻节点之外的所有对影响，因此在小型图上表现出了有希望的性能。尽管如此，现有方法往往继承了Transformer在语言和视觉任务中的思想，并通过堆叠深层多头注意力来采用复杂的模型。本文通过关于节点属性预测基准的实验证明，即使只使用一层注意力也能在节点数量从千级到十亿级的范围内带来令人惊讶的竞争性能。这鼓励我们重新思考在大型图上设计Transformer的理念，其中全局注意力是一个阻碍可扩展性的计算开销。我们将提出的方案称为简化图Transformer。

    Learning representations on large-sized graphs is a long-standing challenge due to the inter-dependence nature involved in massive data points. Transformers, as an emerging class of foundation encoders for graph-structured data, have shown promising performance on small graphs due to its global attention capable of capturing all-pair influence beyond neighboring nodes. Even so, existing approaches tend to inherit the spirit of Transformers in language and vision tasks, and embrace complicated models by stacking deep multi-head attentions. In this paper, we critically demonstrate that even using a one-layer attention can bring up surprisingly competitive performance across node property prediction benchmarks where node numbers range from thousand-level to billion-level. This encourages us to rethink the design philosophy for Transformers on large graphs, where the global attention is a computation overhead hindering the scalability. We frame the proposed scheme as Simplified Graph Trans
    
[^65]: 超越几何：使用动力相似性分析比较神经回路计算中的计算时间结构

    Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis. (arXiv:2306.10168v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2306.10168](http://arxiv.org/abs/2306.10168)

    本研究提出了一种新的方法，通过比较神经网络系统的动力学特征来判断它们是否利用了相同的内部过程进行计算。

    

    我们如何判断两个神经网络是否在特定计算中利用了相同的内部过程？这个问题对神经科学和机器学习的多个子领域都很重要，包括神经人工智能、机械解释性和脑机接口。比较神经网络的标准方法注重潜在状态的空间几何。然而，在循环网络中，计算是在神经动力学的层面上实现的，它们与几何没有简单的一对一映射关系。为了弥合这个差距，我们引入了一种新的相似度度量方法，它在动力学的层面上比较两个系统。我们的方法包括两个组成部分：使用最近数据驱动的动力系统理论的发展，我们学习一个能够准确捕捉原始非线性动力学核心特征的高维线性系统。接下来，我们通过一种新的Procrustes分析的扩展方法比较这些线性近似，该方法考虑了向量场的影响。

    How can we tell whether two neural networks are utilizing the same internal processes for a particular computation? This question is pertinent for multiple subfields of both neuroscience and machine learning, including neuroAI, mechanistic interpretability, and brain-machine interfaces. Standard approaches for comparing neural networks focus on the spatial geometry of latent states. Yet in recurrent networks, computations are implemented at the level of neural dynamics, which do not have a simple one-to-one mapping with geometry. To bridge this gap, we introduce a novel similarity metric that compares two systems at the level of their dynamics. Our method incorporates two components: Using recent advances in data-driven dynamical systems theory, we learn a high-dimensional linear system that accurately captures core features of the original nonlinear dynamics. Next, we compare these linear approximations via a novel extension of Procrustes Analysis that accounts for how vector fields c
    
[^66]: 块状态变换器

    Block-State Transformer. (arXiv:2306.09539v1 [cs.CL])

    [http://arxiv.org/abs/2306.09539](http://arxiv.org/abs/2306.09539)

    本文提出了一种名为块状态变换器（BST）的混合神经网络层，结合了状态空间模型和块变换器，旨在在语言建模任务中提高性能和可扩展性。实验证明，该模型在语言建模困惑度上优于类似的基于Transformer的架构，并且可以推广到更长的序列。此外，在层级别上具有超过十倍的速度提升。

    

    状态空间模型（SSM）在需要建模长期依赖性并且需要高效扩展到长序列的任务中显示出了惊人的效果。尽管最初是为连续信号设计的，但SSM在视觉和音频等许多任务中表现出了优异的性能；然而，在语言建模任务中，SSM仍然落后于Transformers的性能。在这项工作中，我们提出了一个名为块状态变换器（BST）的混合层，它在内部组合了一个用于长距离上下文化的SSM子层和一个用于短期序列表示的块变换器子层。我们研究了三种不同的、完全可并行的集成SSM和块注意力的变体。我们证明了我们的模型在语言建模的困惑度上优于类似的基于Transformer的架构，并且可以推广到更长的序列。此外，块状态变换器在层级别上具有超过十倍的速度提升。

    State space models (SSMs) have shown impressive results on tasks that require modeling long-range dependencies and efficiently scale to long sequences owing to their subquadratic runtime complexity. Originally designed for continuous signals, SSMs have shown superior performance on a plethora of tasks, in vision and audio; however, SSMs still lag Transformer performance in Language Modeling tasks. In this work, we propose a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences. We study three different, and completely parallelizable, variants that integrate SSMs and block-wise attention. We show that our model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences. In addition, the Block-State Transformer demonstrates more than tenfold increase in speed at the layer level compa
    
[^67]: 残差 Q 学习：无需价值的在线和离线策略定制

    Residual Q-Learning: Offline and Online Policy Customization without Value. (arXiv:2306.09526v1 [cs.LG])

    [http://arxiv.org/abs/2306.09526](http://arxiv.org/abs/2306.09526)

    该研究提出了一种新方法，使用动态控制残差的 Q 学习来进行离线和在线的策略定制，无需使用价值函数。

    

    模仿学习是一种广泛使用的框架，适用于从演示中学习模仿行为。当手工制作奖励函数困难或目标是模仿人类专家行为时，这种方法特别有吸引力。但是，学习的模仿策略只能遵循演示中的行为。在应用模仿策略时，我们可能需要根据不同的下游任务要求定制策略行为。同时，我们仍希望定制的策略保持其模仿性质。为此，我们提出了一种新的问题设置，称为策略定制。它将学习任务定义为训练一种策略，该策略继承先前策略的特性，同时满足目标下游任务强加的一些附加要求。我们提出了一种新颖和有原则的方法来解释和确定两个任务目标之间的权衡。具体而言，我们制定了一种动态控制残差的 Q 学习方法，该方法可以在不使用价值函数的情况下进行在线和离线策略定制。

    Imitation Learning (IL) is a widely used framework for learning imitative behavior from demonstrations. It is especially appealing for solving complex real-world tasks where handcrafting reward function is difficult, or when the goal is to mimic human expert behavior. However, the learned imitative policy can only follow the behavior in the demonstration. When applying the imitative policy, we may need to customize the policy behavior to meet different requirements coming from diverse downstream tasks. Meanwhile, we still want the customized policy to maintain its imitative nature. To this end, we formulate a new problem setting called policy customization. It defines the learning task as training a policy that inherits the characteristics of the prior policy while satisfying some additional requirements imposed by a target downstream task. We propose a novel and principled approach to interpret and determine the trade-off between the two task objectives. Specifically, we formulate the
    
[^68]: 多类条件下的类别条件符合性预测

    Class-Conditional Conformal Prediction With Many Classes. (arXiv:2306.09335v1 [stat.ML])

    [http://arxiv.org/abs/2306.09335](http://arxiv.org/abs/2306.09335)

    提出了一种叫做聚类符合性预测的方法，可以在多类条件下提供类别条件符合性预测，针对多个类别的图像数据集中经验评估结果表明其优于现有方法。

    

    标准符合性预测方法提供边缘覆盖保证，这意味着对于一个随机的测试点，符合性预测集合以用户选择的概率包含真实标签。在许多分类问题中，我们希望获得更强的保证——对于特定类别的测试点，预测集以相同的用户选择概率包含真实标签。现有的符合性预测方法在每个类别有限的标记数据的情况下表现不佳，而这在实际应用中往往是大量类别的情况。我们提出了一种称为聚类符合性预测的方法，它将具有“相似”符合性分数的类别聚类在一起，然后在聚类级别上执行符合性预测。在针对多个（多达1000）类别的四个图像数据集的经验评估中，我们发现，聚类符合性通常在类条件覆盖和集合方面优于现有方法。

    Standard conformal prediction methods provide a marginal coverage guarantee, which means that for a random test point, the conformal prediction set contains the true label with a user-chosen probability. In many classification problems, we would like to obtain a stronger guarantee -- that for test points of a specific class, the prediction set contains the true label with the same user-chosen probability. Existing conformal prediction methods do not work well when there is a limited amount of labeled data per class, as is often the case in real applications where the number of classes is large. We propose a method called clustered conformal prediction, which clusters together classes that have "similar" conformal scores and then performs conformal prediction at the cluster level. Based on empirical evaluation across four image data sets with many (up to 1000) classes, we find that clustered conformal typically outperforms existing methods in terms of class-conditional coverage and set 
    
[^69]: 具有任意微分阶硬约束的神经场

    Neural Fields with Hard Constraints of Arbitrary Differential Order. (arXiv:2306.08943v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.08943](http://arxiv.org/abs/2306.08943)

    本文提出了一种名为约束神经场的方法，用于在神经网络中实施任意微分阶的硬约束。通过应用线性算子到神经场及其导数，我们能够解决标准模型在受限制情况下遇到的问题，并在各种实际应用中验证了该方法的有效性。

    

    尽管深度学习技术在解决各种优化问题方面变得非常流行，但在优化过程中强制施加硬约束的方法，特别是在深度神经网络上，仍然不太成熟。受到网格无约束插值和其在科学计算中的光谱色散方法的丰富文献启发，我们开发了一系列用于在神经场上强制施加硬约束的方法，我们将其称为约束神经场（CNF）。约束可以指定为应用于神经场及其导数的线性算子。我们还设计了特定的模型表示和训练策略，用于解决标准模型可能遇到的困难，如系统的条件、内存消耗和在受限制时网络的容量。我们的方法在各种实际应用中得到了验证。此外，我们还开发了一个能够实现高效模型和协作的框架。

    While deep learning techniques have become extremely popular for solving a broad range of optimization problems, methods to enforce hard constraints during optimization, particularly on deep neural networks, remain underdeveloped. Inspired by the rich literature on meshless interpolation and its extension to spectral collocation methods in scientific computing, we develop a series of approaches for enforcing hard constraints on neural fields, which we refer to as Constrained Neural Fields (CNF). The constraints can be specified as a linear operator applied to the neural field and its derivatives. We also design specific model representations and training strategies for problems where standard models may encounter difficulties, such as conditioning of the system, memory consumption, and capacity of the network when being constrained. Our approaches are demonstrated in a wide range of real-world applications. Additionally, we develop a framework that enables highly efficient model and co
    
[^70]: MMD-FUSE: 在不分割数据的情况下学习和组合内核进行双样本检验

    MMD-FUSE: Learning and Combining Kernels for Two-Sample Testing Without Data Splitting. (arXiv:2306.08777v1 [stat.ML])

    [http://arxiv.org/abs/2306.08777](http://arxiv.org/abs/2306.08777)

    本文提出了MMD-FUSE方法，通过适应内核集合最大化基于MMD的双样本检验功率，避免数据分割，并在低维合成数据和高维实际数据上证明了其适用性和功率超过现有最先进的核检验方法。

    

    本文提出了一种新的统计方法，通过适应定义该方法的内核集合，最大化基于最大平均偏差（MMD）的双样本检验的功率。 对于有限集合，这就缩小了通过加权软最大值组合（标准化的）每个内核下的MMD值。 对于零假设和备择假设，证明了我们提出的统计量的指数浓度上限。 我们进一步展示了如何通过数据依赖但与排列独立的方式选择这些内核，在一个经过良好校准的测试中避免数据分割。 这种技术更广泛地适用于基于一般排列的MMD测试，并且包括使用使用自编码器等无监督模型学习的深度内核。 我们强调了我们的MMD-FUSE测试在合成低维数据和现实世界高维数据方面的适用性，并比较了其功率表现与当前最先进的内核检验。

    We propose novel statistics which maximise the power of a two-sample test based on the Maximum Mean Discrepancy (MMD), by adapting over the set of kernels used in defining it. For finite sets, this reduces to combining (normalised) MMD values under each of these kernels via a weighted soft maximum. Exponential concentration bounds are proved for our proposed statistics under the null and alternative. We further show how these kernels can be chosen in a data-dependent but permutation-independent way, in a well-calibrated test, avoiding data splitting. This technique applies more broadly to general permutation-based MMD testing, and includes the use of deep kernels with features learnt using unsupervised models such as auto-encoders. We highlight the applicability of our MMD-FUSE test on both synthetic low-dimensional and real-world high-dimensional data, and compare its performance in terms of power against current state-of-the-art kernel tests.
    
[^71]: LargeST: 一个面向大规模交通预测的基准数据集

    LargeST: A Benchmark Dataset for Large-Scale Traffic Forecasting. (arXiv:2306.08259v1 [cs.LG])

    [http://arxiv.org/abs/2306.08259](http://arxiv.org/abs/2306.08259)

    LargeST数据集是一个更为现实和具有挑战性的交通预测基准，包括8600个传感器、覆盖5年时间和包括细致元数据。

    

    交通预测在智慧城市项目中扮演着至关重要的角色，并通过深度学习捕捉交通数据的非线性模式取得了显著的进展。然而，目前公共数据集上取得的有前途的结果可能不适用于实际场景，因为这些数据集存在局限性。为了解决这些问题，我们引入了一个名为LargeST的基准数据集，包括8600个传感器、覆盖5年时间和包括细致元数据。我们使用LargeST进行深入数据分析并演示了它相对于现有数据集而言是一个更为现实和具有挑战性的交通预测基准。

    Traffic forecasting plays a critical role in smart city initiatives and has experienced significant advancements thanks to the power of deep learning in capturing non-linear patterns of traffic data. However, the promising results achieved on current public datasets may not be applicable to practical scenarios due to limitations within these datasets. First, the limited sizes of them may not reflect the real-world scale of traffic networks. Second, the temporal coverage of these datasets is typically short, posing hurdles in studying long-term patterns and acquiring sufficient samples for training deep models. Third, these datasets often lack adequate metadata for sensors, which compromises the reliability and interpretability of the data. To mitigate these limitations, we introduce the LargeST benchmark dataset. It encompasses a total number of 8,600 sensors with a 5-year time coverage and includes comprehensive metadata. Using LargeST, we perform in-depth data analysis to extract dat
    
[^72]: 统一的非同策略学习排序：强化学习视角

    Unified Off-Policy Learning to Rank: a Reinforcement Learning Perspective. (arXiv:2306.07528v1 [cs.LG])

    [http://arxiv.org/abs/2306.07528](http://arxiv.org/abs/2306.07528)

    本文提出了点击模型不可知的统一非同策略学习排序（CUOLR）方法，通过离线强化学习（RL）直接学习最优排名，可以轻松地应用于各种点击模型。

    

    非同策略学习排序（LTR）旨在通过已部署的记录策略收集的数据优化排名器。然而，现有的非同策略学习排序方法经常对用户如何生成点击数据即点击模型进行假设，因此需要根据不同的点击模型专门调整他们的方法。在本文中，我们将排名过程在一般随机点击模型下统一为马尔可夫决策过程（MDP），通过离线强化学习（RL），可以直接学习最优排名。在此基础上，我们利用离线RL技术进行非同策略LTR，并提出点击模型不可知的统一非同策略学习排序（CUOLR）方法，该方法可以轻松地应用于各种点击模型。通过对MDP的专门制定，我们证明了离线RL算法可以适应各种点击模型，而无需复杂的去偏倚技术和先验知识。在各种大规模数据集上的实验结果都证明了我们方法的有效性。

    Off-policy Learning to Rank (LTR) aims to optimize a ranker from data collected by a deployed logging policy. However, existing off-policy learning to rank methods often make strong assumptions about how users generate the click data, i.e., the click model, and hence need to tailor their methods specifically under different click models. In this paper, we unified the ranking process under general stochastic click models as a Markov Decision Process (MDP), and the optimal ranking could be learned with offline reinforcement learning (RL) directly. Building upon this, we leverage offline RL techniques for off-policy LTR and propose the Click Model-Agnostic Unified Off-policy Learning to Rank (CUOLR) method, which could be easily applied to a wide range of click models. Through a dedicated formulation of the MDP, we show that offline RL algorithms can adapt to various click models without complex debiasing techniques and prior knowledge of the model. Results on various large-scale datasets
    
[^73]: 一种统一自动概念提取和概念重要性评估的全面方法

    A Holistic Approach to Unifying Automatic Concept Extraction and Concept Importance Estimation. (arXiv:2306.07304v1 [cs.LG])

    [http://arxiv.org/abs/2306.07304](http://arxiv.org/abs/2306.07304)

    本文提出了一个全面的理论框架，来统一定义和澄清自动概念提取和概念重要性评估，进而提供新的评估指标以实现对这些方法的比较以及推导关于这种方法的最优性的理论保证。

    

    近年来，基于概念的方法成为了一些最有前途的可解释方法，帮助我们解释人工神经网络（ANN）的决策。这些方法试图在两个关键步骤中发现被隐藏在ANN激活的复杂模式中的可理解的视觉“概念”：（1）概念提取，（2）重要性评估。虽然这两个步骤是各种方法之间共同的，但它们的具体实现都有所不同。在这里，我们介绍了一个统一的理论框架，全面定义和澄清了这两个步骤。该框架具有几个优点，它允许我们：（i）提出新的评估指标来比较不同的概念提取方法；（ii）利用现代归因方法和评估指标来扩展和系统地评估最先进的基于概念的方法和重要性评估技术；（iii）推导关于这种方法的最优性的理论保证。

    In recent years, concept-based approaches have emerged as some of the most promising explainability methods to help us interpret the decisions of Artificial Neural Networks (ANNs). These methods seek to discover intelligible visual 'concepts' buried within the complex patterns of ANN activations in two key steps: (1) concept extraction followed by (2) importance estimation. While these two steps are shared across methods, they all differ in their specific implementations. Here, we introduce a unifying theoretical framework that comprehensively defines and clarifies these two steps. This framework offers several advantages as it allows us: (i) to propose new evaluation metrics for comparing different concept extraction approaches; (ii) to leverage modern attribution methods and evaluation metrics to extend and systematically evaluate state-of-the-art concept-based approaches and importance estimation techniques; (iii) to derive theoretical guarantees regarding the optimality of such met
    
[^74]: 通过有限证据定理，用于多重集合、度量和图的神经可逆函数

    Neural Injective Functions for Multisets, Measures and Graphs via a Finite Witness Theorem. (arXiv:2306.06529v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06529](http://arxiv.org/abs/2306.06529)

    本文介绍了一种通过使用解析的非多项式激活函数，基于神经网络的矩来定义可逆多重集合函数的方法，并通过有限证据定理证明了其有效性。该方法在多重集合和图的机器学习中具有重要的应用价值。

    

    可逆多重集合函数在多重集合和图的机器学习理论研究中起着关键作用。然而，目前理论上考虑的可证明可逆多重集合函数通常依赖于多项式矩，而实际中使用的多重集合函数依赖于尚未研究过在多重集合上的可逆神经矩。在本文中，我们通过展示神经网络的矩确实定义了可逆多重集合函数，前提是使用了一个解析的非多项式激活函数，从而弥合了这一差距。我们的理论所需的矩数量基本上是最优的，最多相差一个乘法因子为二。为了证明这一结果，我们提出并证明了一个独立引人注目的“有限证据定理”。作为我们主要定理的推论，我们推导了关于多重集合和度量函数的新近似结果，并得到了图神经网络的新的分离结果。

    Injective multiset functions have a key role in the theoretical study of machine learning on multisets and graphs. Yet, there remains a gap between the provably injective multiset functions considered in theory, which typically rely on polynomial moments, and the multiset functions used in practice, which rely on $\textit{neural moments}$ $\unicode{x2014}$ whose injectivity on multisets has not been studied to date.  In this paper, we bridge this gap by showing that moments of neural networks do define injective multiset functions, provided that an analytic non-polynomial activation is used. The number of moments required by our theory is optimal essentially up to a multiplicative factor of two. To prove this result, we state and prove a $\textit{finite witness theorem}$, which is of independent interest.  As a corollary to our main theorem, we derive new approximation results for functions on multisets and measures, and new separation results for graph neural networks. We also provide
    
[^75]: ShiftAddViT：多种乘法原语混合实现高效的视觉变换器

    ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer. (arXiv:2306.06446v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06446](http://arxiv.org/abs/2306.06446)

    ShiftAddViT通过使用位移和加法等多种乘法原语对ViT进行重新参数化，实现了减少乘法操作的高效视觉变换器模型，可以在GPU上实现端到端的推理加速，无需从头训练。

    

    视觉变换器（ViT）展示了令人印象深刻的性能，并成为多个视觉任务的统一骨干。但是，ViTs中的注意力和多层感知器（MLPs）由于密集的乘法而不够高效，导致训练和推理代价高昂。为此，我们提出了一种将预训练的ViT以多种乘法原语（例如位移和加法）重新参数化的方法，以实现全新类型的减少乘法的模型，称为ShiftAddViT，旨在实现GPU上的端到端推理加速，无需从头开始训练。具体而言，我们将查询和键映射为汉明空间中的二进制码之后，采用加法核对查询、键和值之间的MatMul进行重新参数化。剩余的MLPs或线性层则采用位移核进行重新参数化。我们利用TVM在GPU上实施并优化这些定制核，以实现实际硬件部署。我们发现，这种重新参数化方法可以显著提高推理速度，而无需从头开始训练。

    Vision Transformers (ViTs) have shown impressive performance and have become a unified backbone for multiple vision tasks. But both attention and multi-layer perceptions (MLPs) in ViTs are not efficient enough due to dense multiplications, resulting in costly training and inference. To this end, we propose to reparameterize the pre-trained ViT with a mixture of multiplication primitives, e.g., bitwise shifts and additions, towards a new type of multiplication-reduced model, dubbed $\textbf{ShiftAddViT}$, which aims for end-to-end inference speedups on GPUs without the need of training from scratch. Specifically, all $\texttt{MatMuls}$ among queries, keys, and values are reparameterized by additive kernels, after mapping queries and keys to binary codes in Hamming space. The remaining MLPs or linear layers are then reparameterized by shift kernels. We utilize TVM to implement and optimize those customized kernels for practical hardware deployment on GPUs. We find that such a reparameter
    
[^76]: 通过模块化生成模型实现灵活的强化学习决策堆叠

    Decision Stacks: Flexible Reinforcement Learning via Modular Generative Models. (arXiv:2306.06253v1 [cs.LG])

    [http://arxiv.org/abs/2306.06253](http://arxiv.org/abs/2306.06253)

    决策堆叠是一种灵活的生成框架，将目标条件化策略代理分解为3个生成模块，并通过教师强制并行学习，可以用于设计单个模块以考虑架构偏差、优化目标和动态、跨领域的可转移性和推理速度。在连续控制基准测试中具有灵活性和可扩展性。

    

    强化学习是一种有吸引力的模型，可以推理顺序决策制定的几个不同方面，如指定复杂目标、规划未来观察和行动，以及批评其实用性。然而，这些能力的综合集成在保持最大表达能力的同时允许进行模型选择以实现高效的学习和推理，这构成了竞争性的算法挑战。我们提出了决策堆叠（Decision Stacks），这是一个生成框架，将目标条件化策略代理分解为3个生成模块。这些模块通过独立的生成模型模拟了观测、奖励和行动的时间演变，可以通过教师强制并行学习。我们的框架保证了表达能力和灵活性，在设计单个模块以考虑关键因素（如架构偏差、优化目标和动态、跨领域的可转移性和推理速度）方面具有优势。我们对一系列连续控制基准进行的实证结果表明，决策堆叠提供了一种灵活且可扩展的替代最先进的基于模型和基于模型的强化学习方法。

    Reinforcement learning presents an attractive paradigm to reason about several distinct aspects of sequential decision making, such as specifying complex goals, planning future observations and actions, and critiquing their utilities. However, the combined integration of these capabilities poses competing algorithmic challenges in retaining maximal expressivity while allowing for flexibility in modeling choices for efficient learning and inference. We present Decision Stacks, a generative framework that decomposes goal-conditioned policy agents into 3 generative modules. These modules simulate the temporal evolution of observations, rewards, and actions via independent generative models that can be learned in parallel via teacher forcing. Our framework guarantees both expressivity and flexibility in designing individual modules to account for key factors such as architectural bias, optimization objective and dynamics, transferrability across domains, and inference speed. Our empirical 
    
[^77]: 战略性苹果品尝：带有一面性反馈的在线学习问题

    Strategic Apple Tasting. (arXiv:2306.06250v1 [cs.GT])

    [http://arxiv.org/abs/2306.06250](http://arxiv.org/abs/2306.06250)

    本篇论文介绍了一种带有苹果品尝反馈的在线学习问题，该问题需要使用一种学习算法以实现战略遗憾。研究结果表明，我们提出的算法的战略遗憾近似于最佳速率。

    

    在高风险领域中，算法决策往往涉及将决策分配给具有策略性修改其算法输入动机的代理。除了应对激励因素外，在许多感兴趣的领域（例如贷款和招聘）中，决策者只观察到在分配积极决策给代理时的回馈。我们将这种反馈称为苹果品尝（或单向反馈）。我们将这一情境形式化为带有苹果品尝反馈的在线学习问题，其中一个负责人决策一系列 $T$ 个代理，每个代理由可被策略性修改的上下文表示。我们的目标是在代理揭示其上下文时获得亚线性的战略遗憾，即如果代理在揭示其上下文时是真实的，则将负责人的表现与后见之明的最佳固定策略进行比较。我们的主要结果是一种学习算法，在这种情况下产生 $\tilde{\mathcal{O}}(\sqrt{T})$ 的战略遗憾，与更一般类型的反馈所知的最佳速率相匹配。

    Algorithmic decision-making in high-stakes domains often involves assigning decisions to agents with incentives to strategically modify their input to the algorithm. In addition to dealing with incentives, in many domains of interest (e.g. lending and hiring) the decision-maker only observes feedback regarding their policy for rounds in which they assign a positive decision to the agent; this type of feedback is often referred to as apple tasting (or one-sided) feedback. We formalize this setting as an online learning problem with apple-tasting feedback where a principal makes decisions about a sequence of $T$ agents, each of which is represented by a context that may be strategically modified. Our goal is to achieve sublinear strategic regret, which compares the performance of the principal to that of the best fixed policy in hindsight, if the agents were truthful when revealing their contexts. Our main result is a learning algorithm which incurs $\tilde{\mathcal{O}}(\sqrt{T})$ strate
    
[^78]: 特征级自监督学习方法FLSL

    FLSL: Feature-level Self-supervised Learning. (arXiv:2306.06203v1 [cs.LG])

    [http://arxiv.org/abs/2306.06203](http://arxiv.org/abs/2306.06203)

    本文提出FLSL方法，采用Transformer进行联合嵌入和聚类，适合于内视图和外视图特征聚类。实验证明该方法在语义类簇表达方面取得显著改进。

    

    当前的自监督学习方法（如SimCLR、DINO、VICReg、MOCOv3）主要针对实例级别的表示，不适用于密集预测任务，例如对象检测和分割。本文针对这个问题，首次展示了Vision Transformers（ViT）的基础均值漂移聚类过程能够良好地与自然图像语义（例如物体和场景）对齐。通过采用Transformer进行联合嵌入和聚类，我们提出了一种两级特征聚类的自监督学习方法，称为特征级自监督学习（FLSL）。我们提出了FLSL问题的正式定义，并从均值漂移和k-means的角度构建目标。实验证明，FLSL促进了显著的语义类簇表示，并学习了一种适合于内视图和外视图特征聚类的嵌入方案。FLSL的运用取得了显著改进。

    Current self-supervised learning (SSL) methods (e.g., SimCLR, DINO, VICReg, MOCOv3) target primarily on representations at instance level and do not generalize well to dense prediction tasks, such as object detection and segmentation. Towards aligning SSL with dense predictions, this paper demonstrates for the first time the underlying mean-shift clustering process of Vision Transformers (ViT), which aligns well with natural image semantics (e.g., a world of objects and stuffs). By employing transformer for joint embedding and clustering, we propose a two-level feature clustering SSL method, coined Feature-Level Self-supervised Learning (FLSL). We present the formal definition of the FLSL problem and construct the objectives from the mean-shift and k-means perspectives. We show that FLSL promotes remarkable semantic cluster representations and learns an embedding scheme amenable to intra-view and inter-view feature clustering. Experiments show that FLSL yields significant improvements 
    
[^79]: PoET: 一种将蛋白质家族看作序列的生成模型

    PoET: A generative model of protein families as sequences-of-sequences. (arXiv:2306.06156v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.06156](http://arxiv.org/abs/2306.06156)

    PoET是一个模型，能够生成任何蛋白质家族的一系列相关蛋白质序列，可以用作检索增强语言模型生成和评分任何修改

    

    生成式蛋白质语言模型是设计具有所需功能的新蛋白质的自然方法。然而，当前的模型要么难以指导其生成特定类型的蛋白质，要么必须在特定类型的蛋白质家族的大型多重序列比对上进行训练，这使得它们无法从家族之间的迁移学习中受益。为了解决这个问题，我们提出了蛋白质进化变换器（PoET），这是一种全蛋白质家族自回归生成模型，学习在数千万个天然蛋白质序列簇之间生成一系列相关的蛋白质序列。PoET可以作为一个检索增强语言模型，在任何感兴趣的蛋白质家族条件下生成和评分任意修改，而且可以从短序列长度进行外推，在小家族中也能很好地泛化。这是通过独特的Transformer层实现的；我们模拟了令牌s

    Generative protein language models are a natural way to design new proteins with desired functions. However, current models are either difficult to direct to produce a protein from a specific family of interest, or must be trained on a large multiple sequence alignment (MSA) from the specific family of interest, making them unable to benefit from transfer learning across families. To address this, we propose $\textbf{P}$r$\textbf{o}$tein $\textbf{E}$volutionary $\textbf{T}$ransformer (PoET), an autoregressive generative model of whole protein families that learns to generate sets of related proteins as sequences-of-sequences across tens of millions of natural protein sequence clusters. PoET can be used as a retrieval-augmented language model to generate and score arbitrary modifications conditioned on any protein family of interest, and can extrapolate from short context lengths to generalize well even for small families. This is enabled by a unique Transformer layer; we model tokens s
    
[^80]: 强度轮廓投影：用于动态网络的连续时间表示学习框架。

    Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks. (arXiv:2306.06155v1 [cs.LG])

    [http://arxiv.org/abs/2306.06155](http://arxiv.org/abs/2306.06155)

    本文提出了一种连续时间网络表示学习框架，涵盖核平滑的强度函数估计、最小化强度重构误差的投影学习和归纳构造节点表示。这种表示保留了网络结构和时间一致性。

    

    我们提出了一种名为“强度轮廓投影”的新算法框架，用于学习动态网络节点的连续时间表示，该动态网络由节点集和在连续时间内发生的瞬时交互事件的集合所特征化。我们的框架包括三个阶段：通过核平滑等方法估计节点对之间交互的强度函数；学习一个最小化某种强度重构误差的投影；通过学习的投影归纳构造出不断发展的节点表示。我们展示了我们的表示保留了网络的基本结构，并具有时间一致性，这意味着节点表示可以在不同的时间点上进行有意义的比较。同时，我们也构建了估计理论来阐明平滑作为偏差方差折衷的作用，并展示了如何随着信噪比的增加而减少平滑程度以获得更好的性能。

    We present a new algorithmic framework, Intensity Profile Projection, for learning continuous-time representations of the nodes of a dynamic network, characterised by a node set and a collection of instantaneous interaction events which occur in continuous time. Our framework consists of three stages: estimating the intensity functions underlying the interactions between pairs of nodes, e.g. via kernel smoothing; learning a projection which minimises a notion of intensity reconstruction error; and inductively constructing evolving node representations via the learned projection. We show that our representations preserve the underlying structure of the network, and are temporally coherent, meaning that node representations can be meaningfully compared at different points in time. We develop estimation theory which elucidates the role of smoothing as a bias-variance trade-off, and shows how we can reduce smoothing as the signal-to-noise ratio increases on account of the algorithm `borrow
    
[^81]: Prodigy: 一种快速自适应零参数学习算法

    Prodigy: An Expeditiously Adaptive Parameter-Free Learner. (arXiv:2306.06101v1 [cs.LG])

    [http://arxiv.org/abs/2306.06101](http://arxiv.org/abs/2306.06101)

    本文提出了一种基于自适应算法(如Adagrad和Adam)的学习率估计方法Prodigy和Resetting，可以快速且正确地估计到达解决方案所需的距离D，从而提高了模型的收敛速度，并在多个数据集和模型上进行了测试，实验表明该方法优于D-Adaptation并可达到手动调整Adam的测试准确度值。

    

    本文研究自适应算法(如Adagrad和Adam)中的学习率估计问题，描述了两种技术Prodigy和Resetting，可以证明地估计到达解决方案所需的距离D，以便最优设置学习率。我们的技术是基于学习率自由的D-Adaptation方法的修改，并通过$O(\sqrt{\log(D/d_0)})$的因子提高了D-Adaptation的收敛速度，其中$d_0$是$D$的初始估计值。我们在12个常见的逻辑回归基准数据集、在CIFAR10上训练的VGG11和ResNet-50、在Imagenet上训练的ViT、在IWSLT14上训练的LSTM、在Criteo数据集上训练的DLRM、在Knee MRI数据集上的VarNet，以及在BookWiki上训练的RoBERTa和GPT transformer上测试了我们的方法。我们的实验结果表明，我们的方法始终优于D-Adaptation，并达到手动调整Adam的测试准确度值。

    We consider the problem of estimating the learning rate in adaptive methods, such as Adagrad and Adam. We describe two techniques, Prodigy and Resetting, to provably estimate the distance to the solution $D$, which is needed to set the learning rate optimally. Our techniques are modifications of the D-Adaptation method for learning-rate-free learning. Our methods improve upon the convergence rate of D-Adaptation by a factor of $O(\sqrt{\log(D/d_0)})$, where $d_0$ is the initial estimate of $D$. We test our methods on 12 common logistic-regression benchmark datasets, VGG11 and ResNet-50 training on CIFAR10, ViT training on Imagenet, LSTM training on IWSLT14, DLRM training on Criteo dataset, VarNet on Knee MRI dataset, as well as RoBERTa and GPT transformer training on BookWiki. Our experimental results show that our approaches consistently outperform D-Adaptation and reach test accuracy values close to that of hand-tuned Adam.
    
[^82]: 通过分位数回归推进反事实推断

    Advancing Counterfactual Inference through Quantile Regression. (arXiv:2306.05751v1 [cs.LG])

    [http://arxiv.org/abs/2306.05751](http://arxiv.org/abs/2306.05751)

    本文提出一种基于分位数回归的反事实推断方法，旨在用于缺乏因果模型和直接条件分布估计的情况，并能提供估计结果的泛化能力和泛化误差上界。

    

    应对反事实“假设”问题的能力对于理解和利用因果影响至关重要。传统的反事实推断通常假定存在结构性因果模型。然而，在实践中，这样的因果模型通常是未知的，甚至不可辨识的。本文旨在基于（学习到的）定性因果结构和观测数据，不需要给定因果模型甚至不需要直接估计条件分布，就能进行可靠的反事实推断。我们使用神经网络将反事实推理重新转化为一个扩展分位数回归问题。这种方法在统计上比现有方法更有效，并且进一步使得估计的反事实结果对未见数据具有一定的泛化能力，并提供了泛化误差的上界。多个数据集上的实验结果强烈支持我们的理论贡献。

    The capacity to address counterfactual "what if" inquiries is crucial for understanding and making use of causal influences. Traditional counterfactual inference usually assumes a structural causal model is available. However, in practice, such a causal model is often unknown and may not be identifiable. This paper aims to perform reliable counterfactual inference based on the (learned) qualitative causal structure and observational data, without a given causal model or even directly estimating conditional distributions. We re-cast counterfactual reasoning as an extended quantile regression problem using neural networks. The approach is statistically more efficient than existing ones, and further makes it possible to develop the generalization ability of the estimated counterfactual outcome to unseen data and provide an upper bound on the generalization error. Experiment results on multiple datasets strongly support our theoretical claims.
    
[^83]: MC-NN：一种端到端的多通道神经网络方法，用于预测流感病毒宿主和抗原类型。

    MC-NN: An End-to-End Multi-Channel Neural Network Approach for Predicting Influenza A Virus Hosts and Antigenic Types. (arXiv:2306.05587v1 [cs.LG])

    [http://arxiv.org/abs/2306.05587](http://arxiv.org/abs/2306.05587)

    提出了一种利用多通道神经网络模型预测流感A病毒宿主和抗原亚型的方法，数据显示多通道神经网络模型具有较高的准确性和预测能力。

    

    流感对公共卫生构成重大威胁，特别是对老年人、儿童和患有潜在疾病的人来说更为严重。严重病况的发生，如肺炎，凸显了预防流感传播的重要性。准确而具有成本效益的预测流感A病毒的宿主和抗原亚型对于应对这一问题至关重要，特别是在资源有限的地区。在本研究中，我们提出了一种多通道神经网络模型，用于从血凝素和神经氨酸酶蛋白序列预测流感A病毒的宿主和抗原亚型。我们的模型是在一个完整蛋白质序列的全面数据集上进行训练的，并在各种完整和不完整序列的测试数据集上进行评估。结果表明，使用多通道神经网络来预测来自完整和部分蛋白质序列的流感A病毒的宿主和抗原亚型具有潜力和实用性。

    Influenza poses a significant threat to public health, particularly among the elderly, young children, and people with underlying dis-eases. The manifestation of severe conditions, such as pneumonia, highlights the importance of preventing the spread of influenza. An accurate and cost-effective prediction of the host and antigenic sub-types of influenza A viruses is essential to addressing this issue, particularly in resource-constrained regions. In this study, we propose a multi-channel neural network model to predict the host and antigenic subtypes of influenza A viruses from hemagglutinin and neuraminidase protein sequences. Our model was trained on a comprehensive data set of complete protein sequences and evaluated on various test data sets of complete and incomplete sequences. The results demonstrate the potential and practicality of using multi-channel neural networks in predicting the host and antigenic subtypes of influenza A viruses from both full and partial protein sequence
    
[^84]: 图上函数的贝叶斯优化

    Bayesian Optimisation of Functions on Graphs. (arXiv:2306.05304v1 [cs.LG])

    [http://arxiv.org/abs/2306.05304](http://arxiv.org/abs/2306.05304)

    本论文提出了一种在通用的大规模和潜在未知图上定义函数的贝叶斯优化算法，并通过学习适当的图内核，适应目标函数行为。

    

    图结构数据的不断涌现推动了在图节点集上定义函数的优化任务。传统的图搜索算法可用于此，但它们可能样本效率低下，并且不利用关于函数值的信息；另一方面，贝叶斯优化是一类有前途的黑盒求解器，具有更高的样本效率，但它很少被应用于这样的新颖设置。为了填补这一空白，我们提出了一种新颖的贝叶斯优化框架，该框架优化在通用，大规模和潜在的未知图上定义的函数。通过学习适当的图内核，我们的框架具有适应目标函数行为的优点。局部建模方法进一步保证了我们方法的效率。在合成和真实世界图上的大量实验表明了所提出的优化框架的有效性。

    The increasing availability of graph-structured data motivates the task of optimising over functions defined on the node set of graphs. Traditional graph search algorithms can be applied in this case, but they may be sample-inefficient and do not make use of information about the function values; on the other hand, Bayesian optimisation is a class of promising black-box solvers with superior sample efficiency, but it has been scarcely been applied to such novel setups. To fill this gap, we propose a novel Bayesian optimisation framework that optimises over functions defined on generic, large-scale and potentially unknown graphs. Through the learning of suitable kernels on graphs, our framework has the advantage of adapting to the behaviour of the target function. The local modelling approach further guarantees the efficiency of our method. Extensive experiments on both synthetic and real-world graphs demonstrate the effectiveness of the proposed optimisation framework.
    
[^85]: 分解对比学习：超越多视角冗余

    Factorized Contrastive Learning: Going Beyond Multi-view Redundancy. (arXiv:2306.05268v1 [cs.LG])

    [http://arxiv.org/abs/2306.05268](http://arxiv.org/abs/2306.05268)

    本论文提出了FactorCL，一种新的多模态表示学习方法，不仅考虑跨模态共享信息，还能捕捉跨模态唯一的任务相关信息。

    

    在广泛的多模态任务中，对比学习已成为一种特别吸引人的方法，因为它可以成功地学习具有丰富未标记数据的表示，只需配对信息（例如，图像标题或视频音频对）。这些方法的基础是多视角冗余的假设——跨模态间共享信息对于下游任务是必要且足够的。然而，在许多现实世界的情况下，任务相关信息也包含在跨模态唯一区域中：一种仅存在于一个模态中但与任务仍然相关的信息。如何学习自我监督的多模态表示以捕获与下游任务相关的共享和唯一信息？本文提出了一种新的多模态表示学习方法FactorCL，以超越多视角冗余。FactorCL的基础是三个新的贡献：（1）将任务相关信息分解为共享和唯一表示，（2）限制共享和唯一成分之间的交互，（3）使用因子正则化促进表示学习。

    In a wide range of multimodal tasks, contrastive learning has become a particularly appealing approach since it can successfully learn representations from abundant unlabeled data with only pairing information (e.g., image-caption or video-audio pairs). Underpinning these approaches is the assumption of multi-view redundancy - that shared information between modalities is necessary and sufficient for downstream tasks. However, in many real-world settings, task-relevant information is also contained in modality-unique regions: information that is only present in one modality but still relevant to the task. How can we learn self-supervised multimodal representations to capture both shared and unique information relevant to downstream tasks? This paper proposes FactorCL, a new multimodal representation learning method to go beyond multi-view redundancy. FactorCL is built from three new contributions: (1) factorizing task-relevant information into shared and unique representations, (2) cap
    
[^86]: 通过逐步扩展数据对抗虚假相关性的鲁棒学习

    Robust Learning with Progressive Data Expansion Against Spurious Correlation. (arXiv:2306.04949v1 [cs.LG])

    [http://arxiv.org/abs/2306.04949](http://arxiv.org/abs/2306.04949)

    本文通过理论分析和实验证明了，在存在虚假特征的情况下，数据组的不平衡和易于学习的虚假特征可能导致模型学习虚假特征。作者提出了一种新的训练算法PDE，它逐步扩展训练数据的大小可以提高了模型鲁棒性，有效地增强了其最劣组性能，实验证实在合成和真实世界的基准数据集上的超越了R模型等其它模型。

    

    虽然深度学习模型在各种任务中表现出了卓越的性能，但它们易于学习与真实标签无关的虚假特征，而不是真正与标签相关的核心特征。在本文中，我们在已有线性模型分析的基础上，从理论上检查了存在虚假特征时两层非线性卷积神经网络的学习过程。我们的分析表明，数据组不平衡和易于学习的虚假特征可能在学习过程中导致虚假特征的支配。基于此，我们提出了一种名为PDE的新的训练算法，该算法有效地增强了模型的鲁棒性，以获得更好的最差组性能。PDE从一组平衡的训练数据子集开始，并逐步扩展其大小以促进核心特征的学习。在合成和真实世界基准数据集上的实验证明了我们的方法在模型（例如R）上的优异性能。

    While deep learning models have shown remarkable performance in various tasks, they are susceptible to learning non-generalizable spurious features rather than the core features that are genuinely correlated to the true label. In this paper, beyond existing analyses of linear models, we theoretically examine the learning process of a two-layer nonlinear convolutional neural network in the presence of spurious features. Our analysis suggests that imbalanced data groups and easily learnable spurious features can lead to the dominance of spurious features during the learning process. In light of this, we propose a new training algorithm called PDE that efficiently enhances the model's robustness for a better worst-group performance. PDE begins with a group-balanced subset of training data and progressively expands it to facilitate the learning of the core features. Experiments on synthetic and real-world benchmark datasets confirm the superior performance of our method on models such as R
    
[^87]: 分布式均值估计中的通信隐私效用权衡的精确最优性研究

    Exact Optimality of Communication-Privacy-Utility Tradeoffs in Distributed Mean Estimation. (arXiv:2306.04924v1 [cs.LG])

    [http://arxiv.org/abs/2306.04924](http://arxiv.org/abs/2306.04924)

    本研究针对均值估计问题，探讨了在通信和本地差分隐私约束下的精确最优方法，提出了利用旋转对称的共享随机码书，并通过$k$-closest编码实现了随机旋转的单纯形$c$的精确最优。

    

    本文研究了在通信和本地差分隐私约束下的均值估计问题。虽然以前的研究已经提出了相同问题的\emph{阶}-最优算法（即当我们花费更多比特时渐进最优），但在非渐进设置下仍然没有实现\emph{精确}最优性。在本文中，我们迈出了一步，描述了在共享随机性存在的情况下的\emph{精确}-最优方法，并确定了几个\emph{精确}最优的必要条件。我们证明了其中一个必要条件是利用旋转对称的共享随机码书。基于此，我们提出了一种随机化机制，其中码书是随机旋转的单纯形——满足\emph{精确}-最优码书的必要属性。该机制基于我们证明的$k$最近编码，对于随机旋转的单纯形$c$来说是\emph{精确}-最优的。

    We study the mean estimation problem under communication and local differential privacy constraints. While previous work has proposed \emph{order}-optimal algorithms for the same problem (i.e., asymptotically optimal as we spend more bits), \emph{exact} optimality (in the non-asymptotic setting) still has not been achieved. In this work, we take a step towards characterizing the \emph{exact}-optimal approach in the presence of shared randomness (a random variable shared between the server and the user) and identify several necessary conditions for \emph{exact} optimality. We prove that one of the necessary conditions is to utilize a rotationally symmetric shared random codebook. Based on this, we propose a randomization mechanism where the codebook is a randomly rotated simplex -- satisfying the necessary properties of the \emph{exact}-optimal codebook. The proposed mechanism is based on a $k$-closest encoding which we prove to be \emph{exact}-optimal for the randomly rotated simplex c
    
[^88]: 随机坍缩：如何利用梯度噪声使SGD动态趋向更简单的子网络

    Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards Simpler Subnetworks. (arXiv:2306.04251v1 [cs.LG])

    [http://arxiv.org/abs/2306.04251](http://arxiv.org/abs/2306.04251)

    SGD在训练过度表达的网络时，会随机地将动态吸引到更简单的子网络，这种随机吸引性能够提高泛化能力。

    

    本文揭示了随机梯度下降（SGD）的一个强烈隐式偏好，它将过度表达的网络驱动到更简单的子网络，从而大大减少了独立参数的数量，并提高了泛化能力。为了揭示这个偏好，我们识别了不变集，或者说是SGD未修改的参数空间的子集。我们专注于两类不变集，它们对应于现代架构中常见的更简单的子网络。我们的分析揭示了SGD在这些简单不变集方面具有随机吸引性的特性。我们根据损失景观在不变集周围的曲率和随机梯度引入的噪声之间的竞争建立了一种随机吸引性的充分条件。值得注意的是，我们发现增加噪声水平会增强吸引力，导致与鞍点或训练损失的局部极大值相关的吸引不变集的出现。

    In this work, we reveal a strong implicit bias of stochastic gradient descent (SGD) that drives overly expressive networks to much simpler subnetworks, thereby dramatically reducing the number of independent parameters, and improving generalization. To reveal this bias, we identify invariant sets, or subsets of parameter space that remain unmodified by SGD. We focus on two classes of invariant sets that correspond to simpler subnetworks and commonly appear in modern architectures. Our analysis uncovers that SGD exhibits a property of stochastic attractivity towards these simpler invariant sets. We establish a sufficient condition for stochastic attractivity based on a competition between the loss landscape's curvature around the invariant set and the noise introduced by stochastic gradients. Remarkably, we find that an increased level of noise strengthens attractivity, leading to the emergence of attractive invariant sets associated with saddle-points or local maxima of the train loss.
    
[^89]: LEACE：闭合形式中的完美线性概念擦除

    LEACE: Perfect linear concept erasure in closed form. (arXiv:2306.03819v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.03819](http://arxiv.org/abs/2306.03819)

    本文介绍了一种闭合形式的方法LEACE，可在删除指定特征的同时尽可能少地改变表示，并可证明防止所有线性分类器检测到概念。作者用“概念擦除”这一新方法将其应用于大型语言模型，在测量语言模型对词性的依赖性和减少BERT嵌入中的性别偏差任务中得出良好表现。

    

    概念擦除旨在从表征中删除指定的特征。它可以提高公平性（例如，防止分类器使用性别或种族）和可解释性（例如，删除概念以观察模型行为的变化）。我们引入了LEAst-squares概念擦除（LEACE），这是一种闭合形式的方法，可证明防止所有线性分类器检测到概念，同时尽可能地改变表示，如广泛类别的范数所测量的那样。我们使用名为“概念擦除”的新方法将LEACE应用于大型语言模型，擦除每个层中的目标概念信息。我们在两个任务上展示了我们的方法：测量语言模型对词性信息的依赖性，以及减少BERT嵌入中的性别偏差。代码可在https://github.com/EleutherAI/concept-erasure上找到。

    Concept erasure aims to remove specified features from a representation. It can improve fairness (e.g. preventing a classifier from using gender or race) and interpretability (e.g. removing a concept to observe changes in model behavior). We introduce LEAst-squares Concept Erasure (LEACE), a closed-form method which provably prevents all linear classifiers from detecting a concept while changing the representation as little as possible, as measured by a broad class of norms. We apply LEACE to large language models with a novel procedure called "concept scrubbing," which erases target concept information from every layer in the network. We demonstrate our method on two tasks: measuring the reliance of language models on part-of-speech information, and reducing gender bias in BERT embeddings. Code is available at https://github.com/EleutherAI/concept-erasure.
    
[^90]: FAMO：快速自适应多任务优化

    FAMO: Fast Adaptive Multitask Optimization. (arXiv:2306.03792v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.03792](http://arxiv.org/abs/2306.03792)

    FAMO是一种快速自适应多任务优化方法，通过动态加权方式实现平衡的任务损失减少，相比最先进的梯度操作技术具有相似或更优的性能。

    

    人工智能的一个长久的目标是创建能够通过多任务学习从多样化数据中学习多个不同任务的通用代理。然而，在实践中，对所有任务的平均损失应用梯度下降可能会由于某些任务的严重欠优化而导致较差的多任务性能。以往的方法通过操纵任务梯度以获得更平衡的损失减少，但需要存储和计算所有任务的梯度（时间和空间复杂度为O(k)，其中k是任务数量），限制了它们在大规模场景中的使用。在这项工作中，我们引入了快速自适应多任务优化（FAMO），一种动态加权方法，以O(1)的时间和空间复杂度以平衡的方式减少任务损失。我们进行了一系列广泛的实验，涵盖了多任务监督学习和强化学习问题。我们的结果表明，FAMO在性能上达到了与最先进的梯度操作技术相媲美或更优的水平。

    One of the grand enduring goals of AI is to create generalist agents that can learn multiple different tasks from diverse data via multitask learning (MTL). However, in practice, applying gradient descent (GD) on the average loss across all tasks may yield poor multitask performance due to severe under-optimization of certain tasks. Previous approaches that manipulate task gradients for a more balanced loss decrease require storing and computing all task gradients ($\mathcal{O}(k)$ space and time where $k$ is the number of tasks), limiting their use in large-scale scenarios. In this work, we introduce Fast Adaptive Multitask Optimization FAMO, a dynamic weighting method that decreases task losses in a balanced way using $\mathcal{O}(1)$ space and time. We conduct an extensive set of experiments covering multi-task supervised and reinforcement learning problems. Our results indicate that FAMO achieves comparable or superior performance to state-of-the-art gradient manipulation technique
    
[^91]: 数据中动态偏移的状态规范化策略优化

    State Regularized Policy Optimization on Data with Dynamics Shift. (arXiv:2306.03552v1 [cs.LG])

    [http://arxiv.org/abs/2306.03552](http://arxiv.org/abs/2306.03552)

    本文提出了一种叫做 SRPO (状态规范化策略优化) 的算法，该算法利用训练数据中的稳态分布来规范新环境中的策略，在处理具有不同动态的多个环境时表现优异。

    

    在许多实际场景中，强化学习算法使用的数据受到动态偏移的影响，即具有不同的环境动态。目前的大多数方法通过训练上下文编码器来识别环境参数来解决这个问题。根据其环境参数将带有动态漂移的数据分开以训练相应的策略。然而，这些方法可能会出现样本效率低下的问题，因为数据是“特定场景”使用的，针对某个环境训练的策略不能从收集在其他具有不同动态的所有其他环境中的数据中受益。本文发现，在许多具有相似结构和不同动态的环境中，最优策略具有类似的稳态分布。我们利用这种特性，并从具有动态漂移的数据中学习稳态分布，以实现高效的数据重用。这种分布用于规范新环境中训练的策略，导致了 SRPO（状态规范化策略优化）算法的出现。实验结果表明，SRPO 在具有动态偏移的任务上显著优于现有的方法。

    In many real-world scenarios, Reinforcement Learning (RL) algorithms are trained on data with dynamics shift, i.e., with different underlying environment dynamics. A majority of current methods address such issue by training context encoders to identify environment parameters. Data with dynamics shift are separated according to their environment parameters to train the corresponding policy. However, these methods can be sample inefficient as data are used \textit{ad hoc}, and policies trained for one dynamics cannot benefit from data collected in all other environments with different dynamics. In this paper, we find that in many environments with similar structures and different dynamics, optimal policies have similar stationary state distributions. We exploit such property and learn the stationary state distribution from data with dynamics shift for efficient data reuse. Such distribution is used to regularize the policy trained in a new environment, leading to the SRPO (\textbf{S}tat
    
[^92]: 难民安置中的随机分布转移: 建立健壮模型的策略。

    Random Distribution Shift in Refugee Placement: Strategies for Building Robust Models. (arXiv:2306.02948v1 [stat.ML])

    [http://arxiv.org/abs/2306.02948](http://arxiv.org/abs/2306.02948)

    本文研究了难民安置中的随机分布转移问题，并提出并比较了三种建模策略，最终发现混合方法具有较强鲁棒性。

    

    近年来，算法分配难民和寻求庇护者到主机国家的地点已经引起了关注，在美国和瑞士实施。这些方法使用过去抵达的数据生成可以用于匹配家庭到位置的机器学习模型（与分配算法一起使用），目标是最大化政策相关的整合结果，如在一定时间后的就业状态。现有的实现和研究通过直接预测政策结果来训练模型，并将这些预测用于分配过程。然而，这种方法的优点，特别是在非稳态环境下，尚未被先前探讨。本研究提出并比较了三种不同的建模策略：上述的标准方法、使用更新数据和代理结果的方法以及混合方法。我们证明混合方法在分布转移和弱代理关系方面具有鲁棒性-

    Algorithmic assignment of refugees and asylum seekers to locations within host countries has gained attention in recent years, with implementations in the US and Switzerland. These approaches use data on past arrivals to generate machine learning models that can be used (along with assignment algorithms) to match families to locations, with the goal of maximizing a policy-relevant integration outcome such as employment status after a certain duration. Existing implementations and research train models to predict the policy outcome directly, and use these predictions in the assignment procedure. However, the merits of this approach, particularly in non-stationary settings, has not been previously explored. This study proposes and compares three different modeling strategies: the standard approach described above, an approach that uses newer data and proxy outcomes, and a hybrid approach. We show that the hybrid approach is robust to both distribution shift and weak proxy relationships -
    
[^93]: 通过强制条件单调性在Early-Exit结构中实现随时分类

    Towards Anytime Classification in Early-Exit Architectures by Enforcing Conditional Monotonicity. (arXiv:2306.02652v1 [cs.LG])

    [http://arxiv.org/abs/2306.02652](http://arxiv.org/abs/2306.02652)

    本文提出了一种在Early-Exit网络中实现条件单调性的方法，将深度模型转化为真正的随时分类器。

    

    现代预测模型通常部署在计算预算动态的环境中。随时算法非常适用于这种环境，因为它们在计算的任何时候都可以输出预测值，其质量是计算时间的函数。由于其能够在网络各个阶段提供中间预测结果的能力，Early-Exit神经网络在随时计算的背景下引起了人们的关注。然而，我们证明当前的Early-Exit网络并不直接适用于任何时候的设置，因为单个数据点的预测质量不能保证随着计算时间的增加而提高。为了解决这个缺陷，我们提出了一种优雅的事后修改，基于专家乘积，鼓励Early-Exit网络逐渐变得自信。这赋予了我们的深度模型条件单调性的特性——这是实现真正随时分类的重要基石。

    Modern predictive models are often deployed to environments in which computational budgets are dynamic. Anytime algorithms are well-suited to such environments as, at any point during computation, they can output a prediction whose quality is a function of computation time. Early-exit neural networks have garnered attention in the context of anytime computation due to their capability to provide intermediate predictions at various stages throughout the network. However, we demonstrate that current early-exit networks are not directly applicable to anytime settings, as the quality of predictions for individual data points is not guaranteed to improve with longer computation. To address this shortcoming, we propose an elegant post-hoc modification, based on the Product-of-Experts, that encourages an early-exit network to become gradually confident. This gives our deep models the property of conditional monotonicity in the prediction quality -- an essential stepping stone towards truly an
    
[^94]: LambdaBeam：具有高阶函数和Lambda的神经程序搜索

    LambdaBeam: Neural Program Search with Higher-Order Functions and Lambdas. (arXiv:2306.02049v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02049](http://arxiv.org/abs/2306.02049)

    LambdaBeam是一种神经程序搜索算法，通过构建任意lambda函数，并将其作为参数传递给高阶函数来解决先前神经搜索合成更长且更通用程序的限制。

    

    搜索是程序合成中的一种重要技术，它允许根据执行结果采用自适应策略，例如专注于特定搜索方向。几项先前的研究已经证明神经模型在引导程序合成搜索方面是有效的。然而，这些方法的一个常见缺点是无法处理迭代循环、高阶函数或lambda函数，从而限制了先前的神经搜索合成更长且更通用的程序。为解决这一问题，我们设计了一种名为LambdaBeam的搜索算法，可以构建在给定DSL内组合操作的任意lambda函数。我们创建了lambda函数执行行为的语义向量表示，并训练了一个神经策略网络，在搜索过程中选择要构建的lambda函数，并将其作为参数传递给高阶函数执行循环计算。我们的实验表明，LambdaBeam在神经、符号和LLM方法上表现出色。

    Search is an important technique in program synthesis that allows for adaptive strategies such as focusing on particular search directions based on execution results. Several prior works have demonstrated that neural models are effective at guiding program synthesis searches. However, a common drawback of those approaches is the inability to handle iterative loops, higher-order functions, or lambda functions, thus limiting prior neural searches from synthesizing longer and more general programs. We address this gap by designing a search algorithm called LambdaBeam that can construct arbitrary lambda functions that compose operations within a given DSL. We create semantic vector representations of the execution behavior of the lambda functions and train a neural policy network to choose which lambdas to construct during search, and pass them as arguments to higher-order functions to perform looping computations. Our experiments show that LambdaBeam outperforms neural, symbolic, and LLM-
    
[^95]: 揭示视频自监督学习在分布转移下的隐藏动力学

    Uncovering the Hidden Dynamics of Video Self-supervised Learning under Distribution Shifts. (arXiv:2306.02014v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.02014](http://arxiv.org/abs/2306.02014)

    本文全面研究了视频自监督学习方法在不同形式的分布转移下的行为，揭示了一系列有趣发现和行为。

    

    近年来，视频自监督学习（VSSL）取得了显著进展。然而，这些模型在不同形式的分布转移下的确切行为和动态尚未得知。在本文中，我们全面研究了六种流行的自监督方法（v-SimCLR、v-MoCo、v-BYOL、v-SimSiam、v-DINO、v-MAE）对于各种自然分布转移的行为，即（i）上下文转移、（ii）视角转移、（iii）行为者转移、（iv）来源转移、（v）对未知类别的泛化能力（零样本）和（vi）开放集识别。为了进行这项广泛的研究，我们精心构建了一个测试基准，其中包含17个基准对，使用现有的公共数据集和一系列评估协议来对不同方法进行压力测试。我们的研究揭示了VSSL方法的一系列有趣发现和有趣行为。例如，我们观察到尽管视频模型通常

    Video self-supervised learning (VSSL) has made significant progress in recent years. However, the exact behavior and dynamics of these models under different forms of distribution shift are not yet known. In this paper, we comprehensively study the behavior of six popular self-supervised methods (v-SimCLR, v-MoCo, v-BYOL, v-SimSiam, v-DINO, v-MAE) in response to various forms of natural distribution shift, i.e., (i) context shift, (ii) viewpoint shift, (iii) actor shift, (iv) source shift, (v) generalizability to unknown classes (zero-shot), and (vi) open-set recognition. To perform this extensive study, we carefully craft a test bed consisting of 17 in-distribution and out-of-distribution benchmark pairs using available public datasets and a series of evaluation protocols to stress-test the different methods under the intended shifts. Our study uncovers a series of intriguing findings and interesting behaviors of VSSL methods. For instance, we observe that while video models generally
    
[^96]: 人类专家审核研究

    Auditing for Human Expertise. (arXiv:2306.01646v1 [stat.ML])

    [http://arxiv.org/abs/2306.01646](http://arxiv.org/abs/2306.01646)

    人类专家的价值超出了算法可捕捉范围，我们可以用一个简单的程序测试这个问题。

    

    高风险预测任务（例如患者诊断）通常由接受培训的人类专家处理。在这些设置中，自动化的一个常见问题是，专家可能运用很难建模的直觉，并且/或者可以获取信息（例如与患者的交谈），这些信息对于算法来说是不可用的。这引发了一个自然的问题，人类专家是否增加了无法被算法预测器捕捉到的价值。我们开发了一个统计框架，可以将这个问题提出为一个自然的假设检验。正如我们的框架所强调的那样，检测人类专业知识比简单比较专家预测准确性与特定学习算法做出的准确性更加微妙。而是提出了一个简单的程序，测试专家预测是否在“特征”可用而条件下是否与感兴趣的结果统计上独立。因此，我们测试的拒绝表明了人类专业知识确实增加了超出算法可捕捉范围的价值。

    High-stakes prediction tasks (e.g., patient diagnosis) are often handled by trained human experts. A common source of concern about automation in these settings is that experts may exercise intuition that is difficult to model and/or have access to information (e.g., conversations with a patient) that is simply unavailable to a would-be algorithm. This raises a natural question whether human experts add value which could not be captured by an algorithmic predictor. We develop a statistical framework under which we can pose this question as a natural hypothesis test. Indeed, as our framework highlights, detecting human expertise is more subtle than simply comparing the accuracy of expert predictions to those made by a particular learning algorithm. Instead, we propose a simple procedure which tests whether expert predictions are statistically independent from the outcomes of interest after conditioning on the available inputs (`features'). A rejection of our test thus suggests that huma
    
[^97]: 揭示Attention故障的翻转-翻转语言建模

    Exposing Attention Glitches with Flip-Flop Language Modeling. (arXiv:2306.00946v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.00946](http://arxiv.org/abs/2306.00946)

    本论文揭示了语言模型中注意力故障的现象，并通过引入翻转-翻转语言建模来分析这个问题。研究发现，Transformer FFLMs经常出现推理错误。

    

    为什么大型语言模型有时会输出事实错误并表现出错误的推理？这些模型的脆弱性，特别是在执行长链推理时，目前似乎是为了它们能够精确地综合知识、语用和抽象思维而必须付出的代价。为了理解这个根本未解决的问题，本研究确定并分析了注意力故障现象，其中Transformer架构的归纳性偏见间歇性地未能捕捉到稳健的推理。为了隔离这个问题，我们引入了翻转-翻转语言建模（FFLM），这是一组参数化合成基准，旨在探索神经语言模型的外推行为。这个简单的生成任务要求模型在长程依赖关系中复制二进制符号，忽略中间的标记。我们发现Transformer FFLMs在推理错误方面存在着长尾现象，其中一些我们可以

    Why do large language models sometimes output factual inaccuracies and exhibit erroneous reasoning? The brittleness of these models, particularly when executing long chains of reasoning, currently seems to be an inevitable price to pay for their advanced capabilities of coherently synthesizing knowledge, pragmatics, and abstract thought. Towards making sense of this fundamentally unsolved problem, this work identifies and analyzes the phenomenon of attention glitches, in which the Transformer architecture's inductive biases intermittently fail to capture robust reasoning. To isolate the issue, we introduce flip-flop language modeling (FFLM), a parametric family of synthetic benchmarks designed to probe the extrapolative behavior of neural language models. This simple generative task requires a model to copy binary symbols over long-range dependencies, ignoring the tokens in between. We find that Transformer FFLMs suffer from a long tail of sporadic reasoning errors, some of which we ca
    
[^98]: 相互作用测量，分区格和核测试用于高阶相互作用

    Interaction Measures, Partition Lattices and Kernel Tests for High-Order Interactions. (arXiv:2306.00904v1 [stat.ML])

    [http://arxiv.org/abs/2306.00904](http://arxiv.org/abs/2306.00904)

    本文提出了一种用于高阶相互作用的测量方法和基于核的测试方法，并与格理论建立了数学联系，为增强相互作用模型提供了方法。

    

    仅依赖于成对关系的模型往往无法捕捉到各种领域（如社会经济、生态或生物医学系统）中找到的复杂多变量数据的完整统计结构。两个以上变量组之间的非平凡依赖关系在这些系统的分析和建模中可以发挥重要作用，但从数据中提取这样的高阶相互作用仍然具有挑战性。本文引入了一系列$d$-order ($d \geq 2$)相互作用测量，依次包括可能的联合概率分布分解，并定义了非参数、基于核的测试，以系统地确定$d$-order相互作用的统计显着性。同时，我们建立了与格理论的数学联系，阐明了相互作用度量的导出及其复合排列测试的涵义；澄清了单纯复合体与核矩阵中心化的联系；并提供了一种增强相互作用模型的方法。

    Models that rely solely on pairwise relationships often fail to capture the complete statistical structure of the complex multivariate data found in diverse domains, such as socio-economic, ecological, or biomedical systems. Non-trivial dependencies between groups of more than two variables can play a significant role in the analysis and modelling of such systems, yet extracting such high-order interactions from data remains challenging. Here, we introduce a hierarchy of $d$-order ($d \geq 2$) interaction measures, increasingly inclusive of possible factorisations of the joint probability distribution, and define non-parametric, kernel-based tests to establish systematically the statistical significance of $d$-order interactions. We also establish mathematical links with lattice theory, which elucidate the derivation of the interaction measures and their composite permutation tests; clarify the connection of simplicial complexes with kernel matrix centring; and provide a means to enhan
    
[^99]: 未知干预的因果表达式的非参数识别

    Nonparametric Identifiability of Causal Representations from Unknown Interventions. (arXiv:2306.00542v1 [stat.ML])

    [http://arxiv.org/abs/2306.00542](http://arxiv.org/abs/2306.00542)

    本文提出了一种新方法用于从未知干预数据中推断非参数因果表达式学习，并且证明了在两个因果变量的基本设置中，无法消除一些由干预数据引起的歧义问题。

    

    我们研究因果表达式学习，即从变量的高维函数（“混合物”）中推断潜在的因果变量及其因果关系的任务。以前的工作依赖于弱监督，如反事实的干预观察或时间结构；对混合函数或潜在因果模型施加限制，如线性；或需要部分了解生成过程，如因果图或干预目标。我们考虑到因果模型和混合函数都是非参数的一般情况。学习信号采用来自基础因果模型中未知干预的多个数据集或环境的形式。我们的目标是将地面真实潜变量及其因果图鉴定出来，同时解决一组从干预数据无法消除的歧义问题。我们研究了两个因果变量的基本设置，并证明了...

    We study causal representation learning, the task of inferring latent causal variables and their causal relations from high-dimensional functions ("mixtures") of the variables. Prior work relies on weak supervision, in the form of counterfactual pre- and post-intervention views or temporal structure; places restrictive assumptions, such as linearity, on the mixing function or latent causal model; or requires partial knowledge of the generative process, such as the causal graph or the intervention targets. We instead consider the general setting in which both the causal model and the mixing function are nonparametric. The learning signal takes the form of multiple datasets, or environments, arising from unknown interventions in the underlying causal model. Our goal is to identify both the ground truth latents and their causal graph up to a set of ambiguities which we show to be irresolvable from interventional data. We study the fundamental setting of two causal variables and prove that
    
[^100]: 解决扩散模型中的负迁移问题

    Addressing Negative Transfer in Diffusion Models. (arXiv:2306.00354v1 [cs.CV])

    [http://arxiv.org/abs/2306.00354](http://arxiv.org/abs/2306.00354)

    本文从多任务学习角度出发，研究了扩散训练中的负迁移现象，提出了利用正则化技术增强扩散训练的方法，以减轻负迁移并提高去噪任务的性能。

    

    基于扩散的生成模型在各个领域都取得了显著的成功。它在同时涵盖不同噪声水平的去噪任务上训练模型，代表了一种多任务学习（MTL）的形式。然而，从MTL的角度分析和改善扩散模型仍然未被充分探索。特别地，MTL有时会导致众所周知的$\textit{负迁移}$现象，这种现象是由于任务之间存在冲突而导致某些任务的性能降低。本文旨在从MTL的角度分析扩散训练，提出了两个关键观察：$\textbf{(O1)}$ 随着噪声水平之间的差距加大，去噪任务之间的任务亲和力减弱， $\textbf{(O2)}$ 在扩散训练的背景下，负迁移也可能会出现。基于这些观察结果，我们的目标是通过减轻负迁移来增强扩散训练。为了实现这一目标，我们提出了利用现有的MTL方法、具体是正则化技术，来鼓励任务特定的特征提取并减少任务干扰。实验结果表明，我们提出的方法有效地减轻了负迁移，提高了扩散模型在一系列去噪任务上的性能。

    Diffusion-based generative models have achieved remarkable success in various domains. It trains a model on denoising tasks that encompass different noise levels simultaneously, representing a form of multi-task learning (MTL). However, analyzing and improving diffusion models from an MTL perspective remains under-explored. In particular, MTL can sometimes lead to the well-known phenomenon of $\textit{negative transfer}$, which results in the performance degradation of certain tasks due to conflicts between tasks. In this paper, we aim to analyze diffusion training from an MTL standpoint, presenting two key observations: $\textbf{(O1)}$ the task affinity between denoising tasks diminishes as the gap between noise levels widens, and $\textbf{(O2)}$ negative transfer can arise even in the context of diffusion training. Building upon these observations, our objective is to enhance diffusion training by mitigating negative transfer. To achieve this, we propose leveraging existing MTL metho
    
[^101]: 扩散画笔：基于潜在扩散模型的AI生成图像编辑工具

    Diffusion Brush: A Latent Diffusion Model-based Editing Tool for AI-generated Images. (arXiv:2306.00219v1 [cs.CV])

    [http://arxiv.org/abs/2306.00219](http://arxiv.org/abs/2306.00219)

    本论文提出了一种名为扩散画笔的基于潜在扩散模型的AI图像微调工具，可以有效地根据目标区域修改AI合成图像并保留原始上下文。与其他最先进的图像修复技术进行比较，该方法在用户研究中表现出更好的可用性和有效性。

    

    文本到图像的生成模型在生成高质量图像方面取得了显著的进展。然而，由于模型限制，生成的图像经常包含不良的伪影或其他错误。现有的微调生成图像的技术要么耗时（手动编辑），要么产生不够完美的结果（修补），要么会导致整体图像产生意想不到的变化（变体选择和提示微调）。本文提出了一种名为扩散画笔的基于潜在扩散模型的AI图像微调工具，可以有效地微调AI合成图像中所需的区域。我们的方法在反向扩散过程中在目标区域引入了新的随机噪声模式，使模型能够在保留其他区域原始上下文的同时，高效地对指定区域进行更改。我们通过艺术家进行的用户研究评估了我们方法的可用性和有效性，将我们的技术与其他最先进的图像修复技术进行了比较。

    Text-to-image generative models have made remarkable advancements in generating high-quality images. However, generated images often contain undesirable artifacts or other errors due to model limitations. Existing techniques to fine-tune generated images are time-consuming (manual editing), produce poorly-integrated results (inpainting), or result in unexpected changes across the entire image (variation selection and prompt fine-tuning). In this work, we present Diffusion Brush, a Latent Diffusion Model-based (LDM) tool to efficiently fine-tune desired regions within an AI-synthesized image. Our method introduces new random noise patterns at targeted regions during the reverse diffusion process, enabling the model to efficiently make changes to the specified regions while preserving the original context for the rest of the image. We evaluate our method's usability and effectiveness through a user study with artists, comparing our technique against other state-of-the-art image inpaintin
    
[^102]: 预训练表示中的扩散冗余

    Diffused Redundancy in Pre-trained Representations. (arXiv:2306.00183v1 [cs.LG])

    [http://arxiv.org/abs/2306.00183](http://arxiv.org/abs/2306.00183)

    本文发现预训练表示中存在一定程度的扩散冗余，即随机选择一部分神经元与全部神经元相似度高且性能相似，可用于降低实际部署中的计算和内存成本，同时保持相当的性能水平。

    

    在大型数据集上预训练神经网络获得的表示已被越来越多地成功应用于各种下游任务中。在本文中，我们更加深入地研究了这种预训练表示中的特征是如何被编码的。我们发现，在给定层中学到的表示展现出一定程度的扩散冗余，即对于超过一个阈值大小的任何随机子集神经元，都与完整层具有很高的相似度，并且在各种下游任务中能够表现出与整个层相似的性能。我们在各种不同的神经架构（包括CNN和Transformer）上进行了实验，使用了ImageNet1k和ImageNet21k进行预训练，并评估了各种下游任务，如图像分类、目标检测和自然语言处理。我们的实验结果表明，可以利用预训练表示中的冗余来降低在实际部署中使用这些模型的计算和内存成本，同时仍然保持相当的性能水平。

    Representations learned by pre-training a neural network on a large dataset are increasingly used successfully to perform a variety of downstream tasks. In this work, we take a closer look at how features are encoded in such pre-trained representations. We find that learned representations in a given layer exhibit a degree of diffuse redundancy, i.e., any randomly chosen subset of neurons in the layer that is larger than a threshold size shares a large degree of similarity with the full layer and is able to perform similarly as the whole layer on a variety of downstream tasks. For example, a linear probe trained on $20\%$ of randomly picked neurons from a ResNet50 pre-trained on ImageNet1k achieves an accuracy within $5\%$ of a linear probe trained on the full layer of neurons for downstream CIFAR10 classification. We conduct experiments on different neural architectures (including CNNs and Transformers) pre-trained on both ImageNet1k and ImageNet21k and evaluate a variety of downstrea
    
[^103]: 深度神经网络训练的不一致性、不稳定性和泛化差距

    Inconsistency, Instability, and Generalization Gap of Deep Neural Network Training. (arXiv:2306.00169v1 [cs.LG])

    [http://arxiv.org/abs/2306.00169](http://arxiv.org/abs/2306.00169)

    本文的理论分析与实证研究表明，深度神经网络训练中模型输出的不一致性和不稳定性可以作为估计泛化间隙的重要指标，消除不一致性的算法能够提高模型性能。

    

    随着深度神经网络具有很高的表现力，寻找具有小泛化差距的解决方案变得很重要（即在训练数据和未见数据之间的性能差异）。本文通过关注训练的随机性，首先提出了一个理论分析，其中泛化间隙的界限取决于我们称之为模型输出的不一致性和不稳定性，这可以在未标记的数据上进行估计。我们的实证研究基于这个分析，表明在各种设置中，不稳定性和不一致性强烈预示着泛化间隙。特别地，我们的发现表明，不一致性比损失变化的锐度更可靠地指示着泛化间隙。此外，我们还展示了消除不一致性的算法可以带来更优异的表现。这些结果还为现有的方法提供了理论基础，如共同蒸馏和集成。

    As deep neural networks are highly expressive, it is important to find solutions with small generalization gap (the difference between the performance on the training data and unseen data). Focusing on the stochastic nature of training, we first present a theoretical analysis in which the bound of generalization gap depends on what we call inconsistency and instability of model outputs, which can be estimated on unlabeled data. Our empirical study based on this analysis shows that instability and inconsistency are strongly predictive of generalization gap in various settings. In particular, our finding indicates that inconsistency is a more reliable indicator of generalization gap than the sharpness of the loss landscape. Furthermore, we show that algorithmic reduction of inconsistency leads to superior performance. The results also provide a theoretical basis for existing methods such as co-distillation and ensemble.
    
[^104]: 改进CLIP训练的语言重写方法

    Improving CLIP Training with Language Rewrites. (arXiv:2305.20088v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.20088](http://arxiv.org/abs/2305.20088)

    本文介绍了一种名为Language augmented CLIP（LaCLIP）的方法，通过语言重写来增强CLIP训练。利用大型语言模型的能力，重新书写与每个图像关联的文本描述，以增加多样性，同时保留原始的关键概念和意义。

    

    对比语言-图像预训练（CLIP）是使用成对的图像和文本数据进行训练可转移视觉模型的最有效和可扩展的方法之一。CLIP模型使用对比损失进行训练，通常依赖于数据增强来防止过拟合和捷径问题。然而，在CLIP训练范式中，数据增强仅应用于图像输入，而语言输入在整个训练过程中保持不变，限制了多样文本对相同图像的暴露。本文介绍了Language augmented CLIP（LaCLIP），一种简单而高效的方法，通过语言重写来增强CLIP训练。利用大型语言模型的上下文学习能力，我们重新书写与每个图像关联的文本描述。这些重新书写的文本在句子结构和词汇方面呈现多样性，同时保留了原始关键概念和意义。在训练过程中，LaCLIP随机地。

    Contrastive Language-Image Pre-training (CLIP) stands as one of the most effective and scalable methods for training transferable vision models using paired image and text data. CLIP models are trained using contrastive loss, which typically relies on data augmentations to prevent overfitting and shortcuts. However, in the CLIP training paradigm, data augmentations are exclusively applied to image inputs, while language inputs remain unchanged throughout the entire training process, limiting the exposure of diverse texts to the same image. In this paper, we introduce Language augmented CLIP (LaCLIP), a simple yet highly effective approach to enhance CLIP training through language rewrites. Leveraging the in-context learning capability of large language models, we rewrite the text descriptions associated with each image. These rewritten texts exhibit diversity in sentence structure and vocabulary while preserving the original key concepts and meanings. During training, LaCLIP randomly s
    
[^105]: 强化学习中的潜在探索

    Latent Exploration for Reinforcement Learning. (arXiv:2305.20065v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2305.20065](http://arxiv.org/abs/2305.20065)

    本文提出了一种名为Lattice的方法，通过向策略网络的潜在状态中注入时间相关性噪声，来解决强化学习中多作用器系统存在的探索问题。

    

    在强化学习中，智能体通过探索和与环境互动来学习策略。由于维度灾难，学习将高维感知输入映射到运动输出的策略尤其具有挑战性。在训练过程中，最先进的方法（如SAC，PPO等）通过对作用力施加独立的高斯噪声来探索环境。尽管这种非结构化的探索方法在许多任务中证明成功，但对于过动作系统来说可能不够优化。当多个作用器（如马达或肌肉）驱动行为时，不相关的扰动可能会减少彼此的影响，或以与任务无关的方式修改行为。虽然已经存在通过引入动作扰动之间的时间相关性来解决这个问题的方法，但忽视了跨作用器之间的相关性。在本文中，我们提出了潜在时间相关探索（Lattice），一种将时间相关噪声注入到策略网络的潜在状态中的方法。

    In Reinforcement Learning, agents learn policies by exploring and interacting with the environment. Due to the curse of dimensionality, learning policies that map high-dimensional sensory input to motor output is particularly challenging. During training, state of the art methods (SAC, PPO, etc.) explore the environment by perturbing the actuation with independent Gaussian noise. While this unstructured exploration has proven successful in numerous tasks, it can be suboptimal for overactuated systems. When multiple actuators, such as motors or muscles, drive behavior, uncorrelated perturbations risk diminishing each other's effect, or modifying the behavior in a task-irrelevant way. While solutions to introduce time correlation across action perturbations exist, introducing correlation across actuators has been largely ignored. Here, we propose LATent TIme-Correlated Exploration (Lattice), a method to inject temporally-correlated noise into the latent state of the policy network, which
    
[^106]: UNSSOR: 通过利用过度训练混合物实现无监督神经语音分离

    UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures. (arXiv:2305.20054v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2305.20054](http://arxiv.org/abs/2305.20054)

    本文提出了一种名为UNSSOR的算法，通过利用过渡训练混合物来实现无监督神经语音分离。该算法利用每个混合信号作为约束，将估计的说话者图像加起来等于混合信号，从而缩小解的范围。实验结果表明，UNSSOR算法在语音分离效果和性能方面取得了显著的提升。

    

    在多个同时说话者的混响条件下，每个麦克风获取到不同位置的多个说话者混合信号。在过渡条件下，即麦克风数量超过说话者数量的情况下，我们可以通过将每个混合信号作为约束来缩小到说话者图像的解，并实现无监督语音分离（即估计的麦克风上的说话者图像应该加起来等于混合信号）。基于这个的理解，我们提出了一种名为UNSSOR的算法，通过利用过度训练混合物实现无监督神经语音分离。在每个训练步骤中，我们将输入混合物送入深度神经网络（DNN）以为每个说话者产生一个中间估计，对估计进行线性滤波，并优化损失，以使每个麦克风上的所有说话者的滤波估计相加等于混合信号，满足上述约束。我们证明这一方法可以显著提高语音分离效果和性能。

    In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over-determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for $\textbf{u}$nsupervised $\textbf{n}$eural $\textbf{s}$peech $\textbf{s}$eparation by leveraging $\textbf{o}$ver-determined training mixtu$\textbf{r}$es. At each training step, we feed an input mixture to a deep neural network (DNN) to produce an intermediate estimate for each speaker, linearly filter the estimates, and optimize a loss so that, at each microphone, the filtered estimates of all the speakers can add up to the mixture to satisfy the above constraint. We show that this l
    
[^107]: 光谱谐波：在自监督学习中连接谱嵌入和矩阵补全

    Spectal Harmonics: Bridging Spectral Embedding and Matrix Completion in Self-Supervised Learning. (arXiv:2305.19818v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19818](http://arxiv.org/abs/2305.19818)

    该论文通过从拉普拉斯算子的角度出发，将自监督学习方法中增广过程所产生的归纳偏置与低秩矩阵补全问题相联系，提供了对现代自监督学习方法收敛性和下游性能的理论分析。

    

    自监督方法由于其在学习表示时似乎启发式的方法，即在没有明显的标签监督的情况下尊重数据语义的学习表示方法，已经受到了广泛的关注。现在已经有了大量的文献发表，试图建立起对现代自监督学习方法中使用的一系列损失的连贯且理论上有根据的理解。在本文中，我们试图从拉普拉斯算子的角度提供一个理解，并将增广过程所产生的归纳偏置与低秩矩阵补全问题相连接。为此，我们利用低秩矩阵补全的结果，对现代自监督学习方法的收敛性以及影响其下游性能的一个关键属性进行了理论分析。

    Self-supervised methods received tremendous attention thanks to their seemingly heuristic approach to learning representations that respect the semantics of the data without any apparent supervision in the form of labels. A growing body of literature is already being published in an attempt to build a coherent and theoretically grounded understanding of the workings of a zoo of losses used in modern self-supervised representation learning methods. In this paper, we attempt to provide an understanding from the perspective of a Laplace operator and connect the inductive bias stemming from the augmentation process to a low-rank matrix completion problem. To this end, we leverage the results from low-rank matrix completion to provide theoretical analysis on the convergence of modern SSL methods and a key property that affects their downstream performance.
    
[^108]: 隧道效应：深度神经网络中的数据表示构建

    The Tunnel Effect: Building Data Representations in Deep Neural Networks. (arXiv:2305.19753v1 [cs.LG])

    [http://arxiv.org/abs/2305.19753](http://arxiv.org/abs/2305.19753)

    本文研究表明，深度神经网络中存在一种名为“隧道”的现象，它在网络的训练早期就出现，并且对最终的数据表示起到了压缩作用。其中，初始层构建了线性可分表示形式，而随后的层压缩这些表示形式并对整体性能影响不大。然而，隧道会削弱网络在超出分布的泛化性能。

    

    深度神经网络以其在各种任务上的卓越表现而闻名，人们普遍认为更深的网络隐含着对更复杂数据表示的理解。本文表明，训练有素的用于监督图像分类的深度网络分为两个不同的部分，它们对最终数据表示的形成起着不同的作用：最初的层构建了线性可分的表示形式，而随后的层（我们称之为“隧道”）则压缩这些表示形式，并对整体性能影响不大。我们通过全面的实证研究探讨了隧道的行为，发现它会在训练过程中的早期出现，隧道的深度取决于网络容量与任务复杂度之间的关系。此外，我们表明，隧道会削弱网络在超出分布的泛化性能，并讨论了这对于持续学习的影响。

    Deep neural networks are widely known for their remarkable effectiveness across various tasks, with the consensus that deeper networks implicitly learn more complex data representations. This paper shows that sufficiently deep networks trained for supervised image classification split into two distinct parts that contribute to the resulting data representations differently. The initial layers create linearly-separable representations, while the subsequent layers, which we refer to as \textit{the tunnel}, compress these representations and have a minimal impact on the overall performance. We explore the tunnel's behavior through comprehensive empirical studies, highlighting that it emerges early in the training process. Its depth depends on the relation between the network's capacity and task complexity. Furthermore, we show that the tunnel degrades out-of-distribution generalization and discuss its implications for continual learning.
    
[^109]: 强化学习中的可复现性研究

    Replicability in Reinforcement Learning. (arXiv:2305.19562v1 [cs.LG])

    [http://arxiv.org/abs/2305.19562](http://arxiv.org/abs/2305.19562)

    这篇论文研究了在强化学习中的可复制性，提出了可复制算法和松弛可复制算法，并给出了相应的时间和样本复杂度，这对于RL算法设计以及未来的可复制性研究具有影响。

    

    我们在强化学习 (RL) 的背景下，将可复现性作为算法属性进行了数学研究。我们关注的是具有生成模型访问权的带折扣表格MDP的基本设置。受Impagliazzo等人 [2022]的启发，如果在内部随机性相同时，RL算法在从生成器抽取的两个独立和同分布的样本上执行两次并输出完全相同的策略，则表示该RL算法是可复制的。我们首先提供一个有效的$\rho$-可复制算法，用于$(\varepsilon,\delta)$-最优策略估计，其样本和时间复杂度为 $\widetilde O\left(\frac{N^3\cdot\log(1/\delta)}{(1-\gamma)^5\cdot\varepsilon^2\cdot\rho^2}\right)$，其中$N$是状态-动作对的数量。然后，对于确定性算法的子类，我们提供了 $ \Omega\left(\frac {N^3}{(1-\gamma)^3\cdot\varepsilon^2\cdot\rho^2}\right) $ 阶的下限。接下来，我们研究了Kalavasis等人[2019]提出的可复制性的松弛版本，其中仅要求算法的输出接近复制算法的输出，而不是相同。我们提供了一种有效算法，其时间和样本复杂度为 $\widetilde O\left(\frac{N^5\cdot\log(1/\delta)}{(1-\gamma)^9\cdot\varepsilon^4\cdot\rho^2}\right)$，用于$(\varepsilon,\delta)$意义下的可复制性，这比先前与相关问题的界限更好。最后，我们讨论了我们的结果对RL算法设计和可重复性研究的未来方向的影响。

    We initiate the mathematical study of replicability as an algorithmic property in the context of reinforcement learning (RL). We focus on the fundamental setting of discounted tabular MDPs with access to a generative model. Inspired by Impagliazzo et al. [2022], we say that an RL algorithm is replicable if, with high probability, it outputs the exact same policy after two executions on i.i.d. samples drawn from the generator when its internal randomness is the same. We first provide an efficient $\rho$-replicable algorithm for $(\varepsilon, \delta)$-optimal policy estimation with sample and time complexity $\widetilde O\left(\frac{N^3\cdot\log(1/\delta)}{(1-\gamma)^5\cdot\varepsilon^2\cdot\rho^2}\right)$, where $N$ is the number of state-action pairs. Next, for the subclass of deterministic algorithms, we provide a lower bound of order $\Omega\left(\frac{N^3}{(1-\gamma)^3\cdot\varepsilon^2\cdot\rho^2}\right)$. Then, we study a relaxed version of replicability proposed by Kalavasis et 
    
[^110]: 单一生成流网络中的图结构与参数的联合贝叶斯推理

    Joint Bayesian Inference of Graphical Structure and Parameters with a Single Generative Flow Network. (arXiv:2305.19366v1 [cs.LG])

    [http://arxiv.org/abs/2305.19366](http://arxiv.org/abs/2305.19366)

    本文提出了在单一生成流网络中联合建模贝叶斯网络结构和参数的方法，包括非离散样本空间，提高了贝叶斯网络局部概率模型的灵活性。

    

    生成流网络是一类对离散和结构化样本空间进行建模的生成模型。先前的研究已将其应用于推断给定观测数据的贝叶斯网络的有向无环图（DAG）的边缘后验分布。本文基于最近的研究进展，在非离散样本空间上将此框架扩展到联合后验分布的建模，不仅包括贝叶斯网络的结构，还考虑了其条件概率分布的参数。

    Generative Flow Networks (GFlowNets), a class of generative models over discrete and structured sample spaces, have been previously applied to the problem of inferring the marginal posterior distribution over the directed acyclic graph (DAG) of a Bayesian Network, given a dataset of observations. Based on recent advances extending this framework to non-discrete sample spaces, we propose in this paper to approximate the joint posterior over not only the structure of a Bayesian Network, but also the parameters of its conditional probability distributions. We use a single GFlowNet whose sampling policy follows a two-phase process: the DAG is first generated sequentially one edge at a time, and then the corresponding parameters are picked once the full structure is known. Since the parameters are included in the posterior distribution, this leaves more flexibility for the local probability models of the Bayesian Network, making our approach applicable even to non-linear models parametrized
    
[^111]: NetHack很难被黑客入侵。

    NetHack is Hard to Hack. (arXiv:2305.19240v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19240](http://arxiv.org/abs/2305.19240)

    本文研究了神经策略学习在NetHack中的性能差距，并分析了获胜的符号代理，提出了动作层级的优势、神经架构的改进以及强化学习与模仿学习的整合等方面可能提升性能的方法。

    

    神经策略学习方法在各种控制问题中取得了显著的成果，从Atari游戏到模拟运动。然而，在长视野任务中，尤其是在具有多模态观测的开放式环境中，比如流行的地牢探险游戏NetHack中，这些方法都面临困难。有趣的是，NeurIPS 2021 NetHack挑战赛表明，符号代理在中位游戏得分上超过神经方法四倍。在本文中，我们深入探讨了这种性能差距背后的原因，并对NetHack的神经策略学习进行了广泛研究。为了进行这项研究，我们分析了获胜的符号代理，并扩展了其代码库以跟踪内部策略选择，以生成其中一个最大的可用演示数据集。利用这个数据集，我们研究了以下几个方面：(i) 动作层级的优势；(ii) 神经架构的改进；以及 (iii) 强化学习与模仿学习的整合。

    Neural policy learning methods have achieved remarkable results in various control problems, ranging from Atari games to simulated locomotion. However, these methods struggle in long-horizon tasks, especially in open-ended environments with multi-modal observations, such as the popular dungeon-crawler game, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that symbolic agents outperformed neural approaches by over four times in median game score. In this paper, we delve into the reasons behind this performance gap and present an extensive study on neural policy learning for NetHack. To conduct this study, we analyze the winning symbolic agent, extending its codebase to track internal strategy selection in order to generate one of the largest available demonstration datasets. Utilizing this dataset, we examine (i) the advantages of an action hierarchy; (ii) enhancements in neural architecture; and (iii) the integration of reinforcement learning with imitation learning.
    
[^112]: Bayesian隐式神经表示下的压缩

    Compression with Bayesian Implicit Neural Representations. (arXiv:2305.19185v1 [cs.LG])

    [http://arxiv.org/abs/2305.19185](http://arxiv.org/abs/2305.19185)

    该论文提出了一种用Bayesian隐式神经表示来压缩数据的方法，通过最小化 $\beta$-ELBO 直接优化码-失真性能，并通过调整 $\beta$ 来针对给定的网络结构实现不同的码-失真平衡。

    

    许多常见类型的数据可以表示为将坐标映射到信号值的函数，例如图像中的像素位置到RGB值。基于这个观点，可以通过对数据的功能表示进行超拟合，然后编码网络权重来压缩数据。然而，大多数当前的解决方案都效率低下，因为将精度量化到低比特会大幅降低重构质量。为解决这个问题，我们提出了过度拟合变分贝叶斯神经网络来压缩近似后验权重样本，而不是量化和熵编码它。该策略通过最小化 $\beta$-ELBO 直接优化码-失真性能，并通过调整 $\beta$ 来针对给定的网络结构实现不同的码-失真平衡。此外，我们引入了一种学习先验权重分布的迭代算法，并采用主动尺寸调整来进一步提高效率。

    Many common types of data can be represented as functions that map coordinates to signal values, such as pixel locations to RGB values in the case of an image. Based on this view, data can be compressed by overfitting a compact neural network to its functional representation and then encoding the network weights. However, most current solutions for this are inefficient, as quantization to low-bit precision substantially degrades the reconstruction quality. To address this issue, we propose overfitting variational Bayesian neural networks to the data and compressing an approximate posterior weight sample using relative entropy coding instead of quantizing and entropy coding it. This strategy enables direct optimization of the rate-distortion performance by minimizing the $\beta$-ELBO, and target different rate-distortion trade-offs for a given network architecture by adjusting $\beta$. Moreover, we introduce an iterative algorithm for learning prior weight distributions and employ a pro
    
[^113]: LANCE：通过生成语言引导的对抗性图像对视觉模型进行压力测试

    LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images. (arXiv:2305.19164v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.19164](http://arxiv.org/abs/2305.19164)

    本文提出了一种自动化算法LANCE，通过生成语言引导的对抗性测试图像来压力测试视觉模型，实现了在不改变模型权重的情况下增加多样、逼真且具有挑战性的测试图像。通过对多种预训练模型的性能进行基准测试，发现模型性能显著下降，并且通过分析模型对不同类型编辑的敏感性，揭示了ImageNet中先前未知的类别层次模型偏差。

    

    我们提出了一种自动化算法，通过生成语言引导的对抗性测试图像（LANCE）来对训练过的视觉模型进行压力测试。我们的方法借鉴了最近语言建模和基于文本编辑的图像处理的进展，在不改变模型权重的情况下，用一套多样，逼真且具有挑战性的测试图像增加了一个IID测试集合。我们在我们生成的数据上对多种预训练模型的性能进行了基准测试，并观察到了显著而一致的性能下降。我们进一步分析了模型对不同类型编辑的敏感性，并展示了其在揭示ImageNet中先前未知的类别层次模型偏差方面的适用性。代码可在https://github.com/virajprabhu/lance找到。

    We propose an automated algorithm to stress-test a trained visual model by generating language-guided counterfactual test images (LANCE). Our method leverages recent progress in large language modeling and text-based image editing to augment an IID test set with a suite of diverse, realistic, and challenging test images without altering model weights. We benchmark the performance of a diverse set of pre-trained models on our generated data and observe significant and consistent performance drops. We further analyze model sensitivity across different types of edits, and demonstrate its applicability at surfacing previously unknown class-level model biases in ImageNet. Code is available at https://github.com/virajprabhu/lance.
    
[^114]: 神经雕塑：通过修剪和网络分析揭示分层模块化任务结构

    Neural Sculpting: Uncovering hierarchically modular task structure through pruning and network analysis. (arXiv:2305.18402v1 [cs.LG])

    [http://arxiv.org/abs/2305.18402](http://arxiv.org/abs/2305.18402)

    本文提出了一种名为“神经雕塑”的方法，该方法通过神经网络的修剪和分析生成的图形结构来揭示任务的子函数的层次结构。该方法在布尔任务上得到了有效验证。

    

    自然目标函数和任务通常表现为分层模块化，可以将其分解为更简单的子函数以分层组织。这些子函数具有两个重要特征：它们有一组不同的输入（输入可分离性），并且在更高层次中作为输入被重用（可重复使用性）。以往的研究已经确立了分层模块化神经网络的优点，包括学习效率、泛化、多任务学习和可转移性。但是，对于给定的任务，如何识别潜在的子函数及其分层结构仍然具有挑战性。本文提出了一种名为“神经雕塑”的方法，该方法涉及神经网络的修剪和分析生成的图形结构，以揭示子函数的层次结构。我们在几个基准布尔任务上证明了神经雕塑的有效性，并表明它可以准确地识别任务的潜在模块化结构。此外，我们证明，修剪后的网络具有更好的泛化能力，更容易被人类解释。我们的方法也可以扩展到现实任务中，为复杂问题的潜在模块化结构提供洞察。

    Natural target functions and tasks typically exhibit hierarchical modularity - they can be broken down into simpler sub-functions that are organized in a hierarchy. Such sub-functions have two important features: they have a distinct set of inputs (input-separability) and they are reused as inputs higher in the hierarchy (reusability). Previous studies have established that hierarchically modular neural networks, which are inherently sparse, offer benefits such as learning efficiency, generalization, multi-task learning, and transferability. However, identifying the underlying sub-functions and their hierarchical structure for a given task can be challenging. The high-level question in this work is: if we learn a task using a sufficiently deep neural network, how can we uncover the underlying hierarchy of sub-functions in that task? As a starting point, we examine the domain of Boolean functions, where it is easier to determine whether a task is hierarchically modular. We propose an ap
    
[^115]: 关于激活函数和规范化对初始化等距嵌入的影响

    On the impact of activation and normalization in obtaining isometric embeddings at initialization. (arXiv:2305.18399v1 [cs.LG])

    [http://arxiv.org/abs/2305.18399](http://arxiv.org/abs/2305.18399)

    本论文研究了深度神经网络中的 Gram 矩阵结构，证明了激活函数和层规范化结合使用可以在初始化时偏向指数级深度等距，从而弥补了现有理论的空白。

    

    本文探讨了深度神经网络中倒数第二个 Gram 矩阵的结构，该矩阵包含与一批输入对应的输出之间的成对内积。在几种架构中，观察到在初始化时该 Gram 矩阵会随着深度变得退化，从而严重减缓训练速度。规范化层如批处理规范化或层规范化，在防止秩崩溃问题方面起着关键作用。然而现有的理论结果无法全面覆盖广泛用于 transformer 中的层规范化和有限深度下规范化的量化偏差。为了解决这个问题，我们证明了在初始化时，结合激活函数层使用的层规范化可以使多层感知机的 Gram 矩阵偏向指数级深度等距，并使用激活函数的 Hermite 展开来量化这个速度，从而填补了现有理论的空白。

    In this paper, we explore the structure of the penultimate Gram matrix in deep neural networks, which contains the pairwise inner products of outputs corresponding to a batch of inputs. In several architectures it has been observed that this Gram matrix becomes degenerate with depth at initialization, which dramatically slows training. Normalization layers, such as batch or layer normalization, play a pivotal role in preventing the rank collapse issue. Despite promising advances, the existing theoretical results (i) do not extend to layer normalization, which is widely used in transformers, (ii) can not characterize the bias of normalization quantitatively at finite depth.  To bridge this gap, we provide a proof that layer normalization, in conjunction with activation layers, biases the Gram matrix of a multilayer perceptron towards isometry at an exponential rate with depth at initialization. We quantify this rate using the Hermite expansion of the activation function, highlighting th
    
[^116]: 知识增强的推理蒸馏：面向知识密集型任务的小型语言模型

    Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks. (arXiv:2305.18395v1 [cs.CL])

    [http://arxiv.org/abs/2305.18395](http://arxiv.org/abs/2305.18395)

    本论文提出了一种KARD方法，可以通过向小型语言模型中加入从外部知识库检索到的增强知识来解决知识密集型推理任务中小型语言模型记忆能力有限的问题。

    

    大型语言模型在需要复合知识理解的知识密集型推理任务中表现出了良好的性能。但是，由于计算要求高且涉及数据隐私，将此类模型部署到现实世界的应用中可能会具有挑战性。以往的研究专注于通过微调具有标记数据或蒸馏大型语言模型来构建任务特定的小型语言模型，但是由于小型语言模型在记忆所需知识方面的能力有限，这些方法不适用于知识密集型推理任务。在理论分析的基础上，我们提出了一种名为知识增强的推理蒸馏 (KARD) 的新方法，该方法微调小型语言模型以生成从外部知识库检索到的增强知识的依据。此外，我们还提出了一个神经重排器，用于获得与依据生成相关的文档。我们实证表明，KARD在三项知识密集型任务上显着优于以前的方法，并且在模型尺寸相同的情况下可以达到与LLMs可比较的结果。

    Large Language Models (LLMs) have shown promising performance in knowledge-intensive reasoning tasks that require a compound understanding of knowledge. However, deployment of the LLMs in real-world applications can be challenging due to their high computational requirements and concerns on data privacy. Previous studies have focused on building task-specific small language models (LMs) by fine-tuning them with labeled data or distilling LLMs. However, these approaches are ill-suited for knowledge-intensive reasoning tasks due to the limited capacity of small LMs in memorizing the knowledge required. Motivated by our theoretical analysis on memorization, we propose Knowledge-Augmented Reasoning Distillation (KARD), a novel method that fine-tunes small LMs to generate rationales with augmented knowledge retrieved from an external knowledge base. Moreover, we further propose a neural reranker to obtain documents relevant to rationale generation. We empirically show that KARD significantl
    
[^117]: 预训练Transformers中的自发模块化

    Emergent Modularity in Pre-trained Transformers. (arXiv:2305.18390v1 [cs.CL])

    [http://arxiv.org/abs/2305.18390](http://arxiv.org/abs/2305.18390)

    本论文研究了预训练Transformers中的自发模块化现象，发现神经元可以进行功能专业化，并通过聚类建立起模块化结构，此结构可被有效扰动。

    

    本论文研究了预训练Transformers中的模块化特征，这是人脑中常见的特点，被认为对于普遍智能至关重要。本文主要考虑了模块化的两个主要特征：（1）神经元的功能专业化：我们评估了每个神经元是否主要专业化于某一功能，结果表明是的。（2）基于功能聚类的神经元分组：我们探究了将神经元按功能分组的结构寻找方法，每个模块均为其相应功能工作。鉴于可能存在的大量结构，我们将重点放在了分层专家模型身上，并将神经元划分为专家，通常为不同的输入激活不同的专家。实验结果表明存在功能专家，聚集了某一功能的神经元。此外，扰动功能专家的激活显著影响了相应的f键

    This work examines the presence of modularity in pre-trained Transformers, a feature commonly found in human brains and thought to be vital for general intelligence. In analogy to human brains, we consider two main characteristics of modularity: (1) functional specialization of neurons: we evaluate whether each neuron is mainly specialized in a certain function, and find that the answer is yes. (2) function-based neuron grouping: we explore finding a structure that groups neurons into modules by function, and each module works for its corresponding function. Given the enormous amount of possible structures, we focus on Mixture-of-Experts as a promising candidate, which partitions neurons into experts and usually activates different experts for different inputs. Experimental results show that there are functional experts, where clustered are the neurons specialized in a certain function. Moreover, perturbing the activations of functional experts significantly affects the corresponding f
    
[^118]: 超越置信度：可靠模型还应考虑非典型性

    Beyond Confidence: Reliable Models Should Also Consider Atypicality. (arXiv:2305.18262v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18262](http://arxiv.org/abs/2305.18262)

    研究发现，模型的可靠性不能仅仅依靠置信度，还应考虑预测样本或类别的非典型性，特别是对于非典型输入或类别，模型预测更过于自信且准确性较低。利用这些发现，将非典型性纳入模型可以提高不确定性量化和性能。

    

    大多数机器学习模型能够提供置信度以预测结果，然而，置信度无法完全理解预测的可靠性。例如，当输入在训练数据集中没有很好的表示或者输入 inherently 易混淆时，模型可能会给出较低的置信度。本研究探讨了样本或类别的非典型性与模型预测可靠性之间的关系。我们首先证明了非典型性与误校准和准确性之间的强相关性。具体而言，我们实证表明对于非典型的输入或非典型的类别，预测更加过于自信且准确性较低。利用这些发现，我们展示了如何将非典型性纳入不确定性量化和鉴别性神经网络以及大型语言模型的性能提升。在一个案例研究中，我们展示了利用非典型性如何提高不同肤色群体的皮肤病变分类器的性能。

    While most machine learning models can provide confidence in their predictions, confidence is insufficient to understand a prediction's reliability. For instance, the model may have a low confidence prediction if the input is not well-represented in the training dataset or if the input is inherently ambiguous. In this work, we investigate the relationship between how atypical(rare) a sample or a class is and the reliability of a model's predictions. We first demonstrate that atypicality is strongly related to miscalibration and accuracy. In particular, we empirically show that predictions for atypical inputs or atypical classes are more overconfident and have lower accuracy. Using these insights, we show incorporating atypicality improves uncertainty quantification and model performance for discriminative neural networks and large language models. In a case study, we show that using atypicality improves the performance of a skin lesion classifier across different skin tone groups witho
    
[^119]: 诊断变压器：揭示临床决策中的特征空间。 (arXiv:2305.17588v2 [cs.CL] UPDATED)

    Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making. (arXiv:2305.17588v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17588](http://arxiv.org/abs/2305.17588)

    该论文介绍了一种名为SUFO的框架，用于增强微调的变压器模型的可解释性。该框架利用多种分析和可视化技术，解决了模型信任和可解释性的关键问题，并通过案例研究验证了其有效性。

    

    在医学等高风险领域，为了建立信任和确保安全，模型的可解释性至关重要，而使用有限的临床记录对预训练的变压器进行微调以辅助临床决策。我们引入了一种名为SUFO的系统框架，该框架增强了微调的变压器特征空间的可解释性。SUFO利用一系列分析和可视化技术，包括监督探索、无监督相似性分析、特征动态和异常值分析，来解决关于模型信任和可解释性的关键问题。我们进行了一个案例研究，研究了预训练数据对真实世界病理分类任务的影响，并在MedNLI上验证了我们的发现。我们评估了五个110M规模的预训练变压器模型，分为通用领域（BERT, TNLR）、混合领域（BioBERT, Clinical BioBERT）和领域特定（PubMedBERT）组。我们的SUFO分析揭示了：(1)

    Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial, especially in high-stakes domains like medicine, to establish trust and ensure safety, which requires human engagement. We introduce SUFO, a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. SUFO utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis to address key questions about model trust and interpretability. We conduct a case study investigating the impact of pre-training data where we focus on real-world pathology classification tasks, and validate our findings on MedNLI. We evaluate five 110M-sized pre-trained transformer models, categorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical BioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal that: (1
    
[^120]: 通过赌博进行公平性审计

    Auditing Fairness by Betting. (arXiv:2305.17570v1 [stat.ML])

    [http://arxiv.org/abs/2305.17570](http://arxiv.org/abs/2305.17570)

    本文提供了一种通过赌博的方式进行公平性审计的方法，相比之前的方法，这种方法具有更高的实用性和效率，能够对不断产生的数据进行连续的监控，并处理因分布漂移导致的公平性问题。

    

    我们提供了实用、高效、非参数方法，用于审计已部署的分类和回归模型的公平性。相比之前依赖于固定样本量的方法，我们的方法是序贯的，并允许对不断产生的数据进行连续的监控，因此非常适用于跟踪现实世界系统的公平性。我们也允许数据通过概率策略进行收集，而不是从人口中均匀采样。这使得审计可以在为其他目的收集的数据上进行。此外，该策略可以随时间改变，并且不同的子人群可以使用不同的策略。最后，我们的方法可以处理因模型变更或基础人群变更导致的分布漂移。我们的方法基于最近关于 anytime-valid 推断和博弈统计学的进展，尤其是"通过赌博进行测试"框架。这些联系确保了我们的方法具有可解释性、快速和提供统计保证。

    We provide practical, efficient, and nonparametric methods for auditing the fairness of deployed classification and regression models. Whereas previous work relies on a fixed-sample size, our methods are sequential and allow for the continuous monitoring of incoming data, making them highly amenable to tracking the fairness of real-world systems. We also allow the data to be collected by a probabilistic policy as opposed to sampled uniformly from the population. This enables auditing to be conducted on data gathered for another purpose. Moreover, this policy may change over time and different policies may be used on different subpopulations. Finally, our methods can handle distribution shift resulting from either changes to the model or changes in the underlying population. Our approach is based on recent progress in anytime-valid inference and game-theoretic statistics-the "testing by betting" framework in particular. These connections ensure that our methods are interpretable, fast, 
    
[^121]: 因果成分分析

    Causal Component Analysis. (arXiv:2305.17225v1 [stat.ML])

    [http://arxiv.org/abs/2305.17225](http://arxiv.org/abs/2305.17225)

    本文介绍了一个中间问题：因果成分分析(CauCA)，它是独立成分分析(ICA)和因果表示学习(CRL)的泛化和特例，其目标是学习解混函数和因果机制，预设了因果图的知识。

    

    独立成分分析(ICA)的目标是从混合观测到的变量中恢复独立的潜在变量。而因果表示学习(CRL)的目标是推断因果关系强相关性的潜在变量，以及编码它们的因果关系的未知图。我们引入了一个中间问题，称为因果成分分析(CauCA)。CauCA可以被看作是ICA的一种推广，对潜在成分之间的因果依赖建模，也是CRL的一个特例。与CRL不同的是，它预设了因果图的知识，仅关注于学习解混函数和因果机制。所有关于CauCA回收基础真相的不可能结果也适用于CRL，而可能性结果可以作为扩展CRL的基础。我们将从对潜在因果变量实施不同类型干预的多个数据集中表征CauCA的可识别性。

    Independent Component Analysis (ICA) aims to recover independent latent variables from observed mixtures thereof. Causal Representation Learning (CRL) aims instead to infer causally related (thus often statistically dependent) latent variables, together with the unknown graph encoding their causal relationships. We introduce an intermediate problem termed Causal Component Analysis (CauCA). CauCA can be viewed as a generalization of ICA, modelling the causal dependence among the latent components, and as a special case of CRL. In contrast to CRL, it presupposes knowledge of the causal graph, focusing solely on learning the unmixing function and the causal mechanisms. Any impossibility results regarding the recovery of the ground truth in CauCA also apply for CRL, while possibility results may serve as a stepping stone for extensions to CRL. We characterize CauCA identifiability from multiple datasets generated through different types of interventions on the latent causal variables. As a
    
[^122]: 三塔：利用预训练图像模型进行灵活的对比学习

    Three Towers: Flexible Contrastive Learning with Pretrained Image Models. (arXiv:2305.16999v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.16999](http://arxiv.org/abs/2305.16999)

    本文提出了 3T 方法，即在视觉语言模型中引入预训练的图像分类器，从而提高对比学习的灵活性。3T 同时受益于预训练嵌入和对比训练，并在实验证明对检索任务和分类问题均取得了有竞争力的性能。

    

    本文提出了一种名为“三塔（3T）”的灵活方法，通过将预先训练的图像分类器纳入对比学习，改进了视觉语言模型的对比学习。与通常从头开始训练对比模型不同，最近的 LiT（Zhai 等人，2022）表明了使用预训练分类器嵌入的性能提升。但是，LiT 直接用冻结的嵌入替换图像塔，排除了对图像塔进行对比训练的任何潜在好处。通过 3T，我们提出了一种更灵活的策略，允许图像塔同时受益于预训练嵌入和对比训练。为了实现这一点，我们引入了第三个塔，其中包含冻结的预训练嵌入，并鼓励该第三个塔与主要的图像-文本塔之间的对齐。在实验证明，3T 在检索任务上始终优于 LiT 和 CLIP 风格的从头开始对比学习基线。对于分类问题，3T 在从头开始基线的基础上可靠地改善，虽然在某些数据集上表现不及 LiT，但仍然实现了有竞争力的性能。总的来说，本方法凸显了将预训练分类器注入到视觉语言模型中的有效性，并提供了一种更灵活的利用它们的方法。

    We introduce Three Towers (3T), a flexible method to improve the contrastive learning of vision-language models by incorporating pretrained image classifiers. While contrastive models are usually trained from scratch, LiT (Zhai et al., 2022) has recently shown performance gains from using pretrained classifier embeddings. However, LiT directly replaces the image tower with the frozen embeddings, excluding any potential benefits of contrastively training the image tower. With 3T, we propose a more flexible strategy that allows the image tower to benefit from both pretrained embeddings and contrastive training. To achieve this, we introduce a third tower that contains the frozen pretrained embeddings, and we encourage alignment between this third tower and the main image-text towers. Empirically, 3T consistently improves over LiT and the CLIP-style from-scratch baseline for retrieval tasks. For classification, 3T reliably improves over the from-scratch baseline, and while it underperform
    
[^123]: 评估大型视觉语言模型的对抗鲁棒性

    On Evaluating Adversarial Robustness of Large Vision-Language Models. (arXiv:2305.16934v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.16934](http://arxiv.org/abs/2305.16934)

    该论文提出在最真实和高风险的情境中评估大型视觉语言模型的对抗鲁棒性。作者首先构建有针对性的对抗样本，然后将其转移到其他模型中进行评估，并观察到黑盒查询可以改进效果。

    

    大型视觉语言模型（VLMs）如GPT-4在生成响应方面取得了前所未有的性能，尤其是在视觉输入方面，使得交互更有创造力和适应性，而不仅仅是大型语言模型如ChatGPT。然而，多模态生成加剧了安全性问题，因为对手可以通过微妙地操纵最易受攻击的模态（例如视觉）成功避开整个系统。为此，我们提出在最真实和高风险的情境下评估开源大型VLMs的鲁棒性，其中对手只能黑盒访问系统，并试图欺骗模型返回目标响应。具体而言，我们首先针对预训练模型（如CLIP和BLIP）构建有针对性的对抗样本，然后将这些对抗样本转移到其他VLMs（如MiniGPT-4、LLaVA、UniDiffuser、BLIP-2和Img2Prompt）。此外，我们观察到，在这些VLMs上进行黑盒查询可以进一步提高效果。

    Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented performance in response generation, especially with visual inputs, enabling more creative and adaptable interaction than large language models such as ChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision). To this end, we propose evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses. In particular, we first craft targeted adversarial examples against pretrained models such as CLIP and BLIP, and then transfer these adversarial examples to other VLMs such as MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we observe that black-box queries on these VLMs can further improve the effectiveness
    
[^124]: 基于锚定机制的值迭代加速算法

    Accelerating Value Iteration with Anchoring. (arXiv:2305.16569v1 [cs.LG])

    [http://arxiv.org/abs/2305.16569](http://arxiv.org/abs/2305.16569)

    本文提出了一种加速值迭代算法Anc-VI，采用了锚定机制，可加速Bellman一致性和最优性算子的计算。对于$\gamma\approx 1$或$\gamma=1$，Anc-VI速度为$\mathcal{O}(1/k)$，比标准VI更快。

    

    值迭代(Value Iteration, VI)是现代强化学习领域中理论和实践的基础，已知其收敛速度为$\mathcal{O}(\gamma^k)$，其中$\gamma$是折扣因子。然而，在VI设置中的最优速度尚未确定，寻求一种通用的加速机制一直是一个未解决的问题。本文提出了第一个基于锚定机制的VI加速算法，称为Anc-VI。不同于Nesterov的加速方法，Anc-VI可以加速Bellman一致性和最优性算子，还比标准VI更快地减少Bellman误差。尤其是，对于$\gamma\approx 1$或甚至$\gamma=1$，Anc-VI呈现出$\mathcal{O}(1/k)$的速度，而标准VI在$\gamma\ge 1-1/k$时的速度为$\mathcal{O}(1)$，其中$k$是迭代次数。我们还提供了与上界匹配的复杂性下界，除了一个常数因子$4$，从而证明了Anc-VI的加速速度的最优性。最后，我们在实验中证明了Anc-VI的有效性。

    Value Iteration (VI) is foundational to the theory and practice of modern reinforcement learning, and it is known to converge at a $\mathcal{O}(\gamma^k)$-rate, where $\gamma$ is the discount factor. Surprisingly, however, the optimal rate for the VI setup was not known, and finding a general acceleration mechanism has been an open problem. In this paper, we present the first accelerated VI for both the Bellman consistency and optimality operators. Our method, called Anc-VI, is based on an \emph{anchoring} mechanism (distinct from Nesterov's acceleration), and it reduces the Bellman error faster than standard VI. In particular, Anc-VI exhibits a $\mathcal{O}(1/k)$-rate for $\gamma\approx 1$ or even $\gamma=1$, while standard VI has rate $\mathcal{O}(1)$ for $\gamma\ge 1-1/k$, where $k$ is the iteration count. We also provide a complexity lower bound matching the upper bound up to a constant factor of $4$, thereby establishing optimality of the accelerated rate of Anc-VI. Finally, we sh
    
[^125]: 基于树的扩散薛定谔桥算法在Wasserstein重心中的应用

    Tree-Based Diffusion Schr\"odinger Bridge with Applications to Wasserstein Barycenters. (arXiv:2305.16557v1 [stat.ML])

    [http://arxiv.org/abs/2305.16557](http://arxiv.org/abs/2305.16557)

    本文介绍了一种基于树的扩散薛定谔桥算法(TreeDSB)来解决多元最优输运(mOT)的问题，并可以应用于高维设置如图像插值和贝叶斯融合。

    

    多元最优输运(mOT)是最优输运(OT)的一种推广，其旨在最小化成本函数相对于某些预先指定的边际分布的积分。本文考虑了一个树形二次成本的熵版本，即一种可以写作树节点之间成对成本函数之和的函数。为了解决这个问题，我们开发了Tree-based Diffusion Schr\"odinger Bridge(TreeDSB)，这是扩展了扩散薛定谔桥(DSB)算法的算法。TreeDSB对应于多元Sinkhorn算法的动态连续状态空间。我们方法的一个显著应用是计算Wasserstein重心，它可以被重新转化为基于星形树的mOT问题的解决方案。我们证明了我们的方法可以应用于高维设置，如图像插值和贝叶斯融合。

    Multi-marginal Optimal Transport (mOT), a generalization of OT, aims at minimizing the integral of a cost function with respect to a distribution with some prescribed marginals. In this paper, we consider an entropic version of mOT with a tree-structured quadratic cost, i.e., a function that can be written as a sum of pairwise cost functions between the nodes of a tree. To address this problem, we develop Tree-based Diffusion Schr\"odinger Bridge (TreeDSB), an extension of the Diffusion Schr\"odinger Bridge (DSB) algorithm. TreeDSB corresponds to a dynamic and continuous state-space counterpart of the multimarginal Sinkhorn algorithm. A notable use case of our methodology is to compute Wasserstein barycenters which can be recast as the solution of a mOT problem on a star-shaped tree. We demonstrate that our methodology can be applied in high-dimensional settings such as image interpolation and Bayesian fusion.
    
[^126]: 扫描与拍照：理解1层Transformer中的训练动态和标记组成

    Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer. (arXiv:2305.16380v1 [cs.CL])

    [http://arxiv.org/abs/2305.16380](http://arxiv.org/abs/2305.16380)

    本文分析了1层Transformer在下一个标记预测任务中的SGD训练动态，证明了自我关注层充当了“区分性扫描算法”，从而逐步关注到相关标记并排除不相关的标记，总结相关信息在编码表示中。同时研究了标记频率、上下文和初始化自我关注层等对Transformer性能的影响。

    

    Transformer架构在多个研究领域表现出了惊人的性能，并成为许多神经网络模型的基础。然而，我们对其如何工作的理解仍然有限。特别是，通过简单的预测性损失，表示如何从梯度训练动态中出现仍然是一个谜。在本文中，针对具有一个自我关注层和一个解码器层的1层Transformer，我们以数学严谨的方式分析其在下一个标记预测任务中的SGD训练动态。我们打开了自我关注层组合输入标记的动态过程的黑盒子，并揭示了底层归纳偏差的本质。具体而言，在没有位置编码、长输入序列和解码器层学习速度快于自我关注层的假设下，我们证明了自我关注层充当了“区分性扫描算法”：从均匀注意力开始，它逐渐关注到相关标记，排除不相关的标记，直到所有相关信息被扫描并总结在编码表示中。我们的分析还显示了标记频率和上下文如何影响注意权重，以及自我关注层初始化如何影响收敛速度。

    Transformer architecture has shown impressive performance in multiple research domains and has become the backbone of many neural network models. However, there is limited understanding on how it works. In particular, with a simple predictive loss, how the representation emerges from the gradient \emph{training dynamics} remains a mystery. In this paper, for 1-layer transformer with one self-attention layer plus one decoder layer, we analyze its SGD training dynamics for the task of next token prediction in a mathematically rigorous manner. We open the black box of the dynamic process of how the self-attention layer combines input tokens, and reveal the nature of underlying inductive bias. More specifically, with the assumption (a) no positional encoding, (b) long input sequence, and (c) the decoder layer learns faster than the self-attention layer, we prove that self-attention acts as a \emph{discriminative scanning algorithm}: starting from uniform attention, it gradually attends mor
    
[^127]: 锐度感知最小化导致低秩特征

    Sharpness-Aware Minimization Leads to Low-Rank Features. (arXiv:2305.16292v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.16292](http://arxiv.org/abs/2305.16292)

    锐度感知最小化方法SAM在神经网络训练中不仅能提升泛化性能，还可以降低特征的秩，适用于不同网络架构和任务类型。

    

    锐度感知最小化（SAM）是一种最小化神经网络训练损失锐度的方法。尽管其泛化性能提升已被广泛认可并成为主要动机，我们发现了SAM的另一个有趣效果：在神经网络的不同层级上发生特征秩降低。我们展示了这种低秩效果的广泛性：适用于全连接网络、卷积网络、视觉变换器等不同体系结构，适用于回归、分类、语言图像对比训练等不同目标。为了更好地理解这一现象，我们在一个简单的两层网络中提供了低秩特征产生的机制性理解。我们观察到SAM将大量的激活完全修剪掉，直接导致秩的降低。我们在理论上验证了这一效果，并检查了它在深度网络中也可能发生，尽管整体秩会有略微的增加。

    Sharpness-aware minimization (SAM) is a recently proposed method that minimizes the sharpness of the training loss of a neural network. While its generalization improvement is well-known and is the primary motivation, we uncover an additional intriguing effect of SAM: reduction of the feature rank which happens at different layers of a neural network. We show that this low-rank effect occurs very broadly: for different architectures such as fully-connected networks, convolutional networks, vision transformers and for different objectives such as regression, classification, language-image contrastive training. To better understand this phenomenon, we provide a mechanistic understanding of how low-rank features arise in a simple two-layer network. We observe that a significant number of activations gets entirely pruned by SAM which directly contributes to the rank reduction. We confirm this effect theoretically and check that it can also occur in deep networks, although the overall rank 
    
[^128]: DoWG展示：一种高效的通用无参数梯度下降方法

    DoWG Unleashed: An Efficient Universal Parameter-Free Gradient Descent Method. (arXiv:2305.16284v1 [cs.LG])

    [http://arxiv.org/abs/2305.16284](http://arxiv.org/abs/2305.16284)

    本文提出了一种名为DoWG的无参数梯度下降方法，它是第一个既高效又通用的算法，能够自适应于平稳和非平稳问题，并且无需回溯搜索过程。

    

    本文提出了一种新的易于实现的无参数梯度优化器：DoWG（Weighted Gradients的距离）。我们证明了该方法是高效的——在不调整任何参数的情况下，匹配优化凸优化中最优调的梯度下降的收敛速度，直到对数因子，并且是通用的——自动适应平滑和非平滑问题。与AdaGrad，Adam或DoG等流行算法计算平方梯度的运行平均值不同，DoWG保持运行平均值的一种新的基于距离的加权版本，这对于实现所需的性质至关重要。据我们所知，DoWG是第一个不需要回溯搜索过程的无参数，高效和通用算法。它还是第一个适应于平稳优化的无参数AdaGrad样式算法。为了补充我们的理论，我们还通过实验证明DoWG在稳定的边缘训练，并证明其在实践中的有效性。

    This paper proposes a new easy-to-implement parameter-free gradient-based optimizer: DoWG (Distance over Weighted Gradients). We prove that DoWG is efficient -- matching the convergence rate of optimally tuned gradient descent in convex optimization up to a logarithmic factor without tuning any parameters, and universal -- automatically adapting to both smooth and nonsmooth problems. While popular algorithms such as AdaGrad, Adam, or DoG compute a running average of the squared gradients, DoWG maintains a new distance-based weighted version of the running average, which is crucial to achieve the desired properties. To our best knowledge, DoWG is the first parameter-free, efficient, and universal algorithm that does not require backtracking search procedures. It is also the first parameter-free AdaGrad style algorithm that adapts to smooth optimization. To complement our theory, we also show empirically that DoWG trains at the edge of stability, and validate its effectiveness on practic
    
[^129]: 在协同学习和优化中激励竞争对手诚实行为的研究

    Incentivizing Honesty among Competitors in Collaborative Learning and Optimization. (arXiv:2305.16272v1 [cs.LG])

    [http://arxiv.org/abs/2305.16272](http://arxiv.org/abs/2305.16272)

    这项研究提出了一个模型来描述在协作学习中竞争对手的不诚实行为，提出了机制来激励诚实沟通，并确保学习质量与全面合作相当。

    

    协同学习技术能够让机器学习模型的训练比仅利用单一数据源的模型效果更好。然而，在许多情况下，潜在的参与者是下游任务中的竞争对手，如每个都希望通过提供最佳推荐来吸引客户的公司。这可能会激励不诚实的更新，损害其他参与者的模型，从而可能破坏协作的好处。在这项工作中，我们制定了一个模型来描述这种交互，并在该框架内研究了两个学习任务：单轮均值估计和强凸目标的多轮 SGD。对于一类自然的参与者行为，我们发现理性的客户会被激励强烈地操纵他们的更新，从而防止学习。然后，我们提出了机制来激励诚实沟通，并确保学习质量与全面合作相当。最后，我们通过实验证明了这一点。

    Collaborative learning techniques have the potential to enable training machine learning models that are superior to models trained on a single entity's data. However, in many cases, potential participants in such collaborative schemes are competitors on a downstream task, such as firms that each aim to attract customers by providing the best recommendations. This can incentivize dishonest updates that damage other participants' models, potentially undermining the benefits of collaboration. In this work, we formulate a game that models such interactions and study two learning tasks within this framework: single-round mean estimation and multi-round SGD on strongly-convex objectives. For a natural class of player actions, we show that rational clients are incentivized to strongly manipulate their updates, preventing learning. We then propose mechanisms that incentivize honest communication and ensure learning quality comparable to full cooperation. Lastly, we empirically demonstrate the
    
[^130]: 跳跃扩散模型实现的跨维度生成建模

    Trans-Dimensional Generative Modeling via Jump Diffusion Models. (arXiv:2305.16261v1 [stat.ML])

    [http://arxiv.org/abs/2305.16261](http://arxiv.org/abs/2305.16261)

    本文通过跳跃扩散模型实现了一种跨维度生成建模方法，并证明其在处理不同维度数据时具有更好的兼容性和插值能力。

    

    本文提出了一种新的生成模型，通过联合建模每个数据点的状态和尺寸，自然地处理不同维度的数据。该生成过程被定义为在不同维度空间之间进行跳跃扩散的过程。我们首先定义了一个破坏尺寸的前向噪声过程，然后推导出一个创造尺寸的逆向生成过程，以及一个新颖的证据下界训练目标来学习逼近该生成过程。通过模拟我们学习到的逆向生成过程的近似值，可以有效地联合生成状态值和尺寸，从而提供一种处理不同维度数据的有效方法。我们在分子和视频数据集上展示了我们的方法，相较于固定维度的模型，我们报告了更好的与测试时扩散引导插值任务兼容性和改进的插值能力。

    We propose a new class of generative models that naturally handle data of varying dimensionality by jointly modeling the state and dimension of each datapoint. The generative process is formulated as a jump diffusion process that makes jumps between different dimensional spaces. We first define a dimension destroying forward noising process, before deriving the dimension creating time-reversed generative process along with a novel evidence lower bound training objective for learning to approximate it. Simulating our learned approximation to the time-reversed generative process then provides an effective way of sampling data of varying dimensionality by jointly generating state values and dimensions. We demonstrate our approach on molecular and video datasets of varying dimensionality, reporting better compatibility with test-time diffusion guidance imputation tasks and improved interpolation capabilities versus fixed dimensional models that generate state values and dimensions separate
    
[^131]: Koopman核回归

    Koopman Kernel Regression. (arXiv:2305.16215v1 [cs.LG])

    [http://arxiv.org/abs/2305.16215](http://arxiv.org/abs/2305.16215)

    提出了一种基于Koopman核的回归方法，用于预测非线性动力系统的时间演变。该方法在机器人操作，视频预测和交通预测等各种应用中均有优异表现，并具有可证明的学习理论保证。

    

    许多决策制定的机器学习方法，如强化学习，依赖于模拟器或预测模型来预测感兴趣的量的时间演变，例如智能体的状态或策略的奖励。这些复杂现象的预测通常由高度非线性的动力系统描述，使得它们在基于优化的决策制定中的使用具有挑战性。Koopman算子理论通过通过线性动态系统描述预测来解决这个问题。这使得系统分析和长期预测变得简单--只涉及矩阵乘法。然而，将其转化为线性系统通常是非平凡的和未知的，需要基于学习的方法。虽然存在各种方法，但它们通常缺乏关键的学习理论保证，因此所获得的模型在数据和维度增加时的行为通常不清楚。通过提出一种新颖的基于Koopman核的回归方法，我们解决了上述挑战，该方法直接从历史观察中学习到未来预测在Koopman算子空间中的映射。我们的方法享有可证明的学习理论保证，并在广泛的应用中与现有的最先进方法匹配（或优于），包括机器人操作，视频预测和交通预测。

    Many machine learning approaches for decision making, such as reinforcement learning, rely on simulators or predictive models to forecast the time-evolution of quantities of interest, e.g., the state of an agent or the reward of a policy. Forecasts of such complex phenomena are commonly described by highly nonlinear dynamical systems, making their use in optimization-based decision-making challenging. Koopman operator theory offers a beneficial paradigm for addressing this problem by characterizing forecasts via linear dynamical systems. This makes system analysis and long-term predictions simple -- involving only matrix multiplications. However, the transformation to a linear system is generally non-trivial and unknown, requiring learning-based approaches. While there exists a variety of approaches, they usually lack crucial learning-theoretic guarantees, such that the behavior of the obtained models with increasing data and dimensionality is often unclear. We address the aforemention
    
[^132]: 用于保证有界几何惩罚的Riemannian Min-Max优化加速方法

    Accelerated Methods for Riemannian Min-Max Optimization Ensuring Bounded Geometric Penalties. (arXiv:2305.16186v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2305.16186](http://arxiv.org/abs/2305.16186)

    本文研究了Riemannian流形上的优化问题，设计了加速方法，并介绍了几何凸优化的新结果。通过去除关于迭代在预定紧致集合内的假设，完善了之前的工作。

    

    本文研究形式为$\min_x \max_y f(x, y)$的优化问题，其中$f(x, y)$定义在乘积Riemannian流形$\mathcal{M} \times \mathcal{N}$上，并且在$x$方向上$\mu_x$-强测地凸（g-convex），在$y$方向上$\mu_y$-强g-凹，其中$\mu_x, \mu_y \geq 0$。当$f$是$(L_x, L_y, L_{xy})$-平滑的，并且$\mathcal{M}$，$\mathcal{N}$是Hadamard流形时，我们设计了加速方法。为了达到这个目的，我们介绍了几何凸优化的新结果，并展示了度量投影Riemannian梯度下降的全局线性收敛性，并通过减小几何常数改进了现有的加速方法。此外，我们通过去除一种关于迭代在预定紧致集合内的假设，完成了两个之前应用于Riemannian min-max情况的工作的分析。

    In this work, we study optimization problems of the form $\min_x \max_y f(x, y)$, where $f(x, y)$ is defined on a product Riemannian manifold $\mathcal{M} \times \mathcal{N}$ and is $\mu_x$-strongly geodesically convex (g-convex) in $x$ and $\mu_y$-strongly g-concave in $y$, for $\mu_x, \mu_y \geq 0$. We design accelerated methods when $f$ is $(L_x, L_y, L_{xy})$-smooth and $\mathcal{M}$, $\mathcal{N}$ are Hadamard. To that aim we introduce new g-convex optimization results, of independent interest: we show global linear convergence for metric-projected Riemannian gradient descent and improve existing accelerated methods by reducing geometric constants. Additionally, we complete the analysis of two previous works applying to the Riemannian min-max case by removing an assumption about iterates staying in a pre-specified compact set.
    
[^133]: 竞争对手之间的战略数据共享

    Strategic Data Sharing between Competitors. (arXiv:2305.16052v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.16052](http://arxiv.org/abs/2305.16052)

    这项研究介绍了竞争对手之间战略数据共享的问题，提出了一个分析框架，研究了市场条件对数据共享激励的影响。

    

    近年来，协作学习技术取得了显著进展，可以实现多个组织之间的私密模型训练。然而，企业在考虑与竞争对手共享数据时面临着困境——虽然协作可以改善公司的机器学习模型，但也可能使竞争对手受益，从而降低利润。在这项工作中，我们引入了一个用于分析这种数据共享权衡的通用框架。该框架包括三个组成部分，分别代表企业的生产决策、额外数据对模型质量的影响以及数据共享协商过程。然后，我们研究了基于经济理论中的传统市场模型的框架实例，以确定影响协作激励的关键因素。我们的研究结果表明市场条件对数据共享激励具有深远影响。特别是，我们发现市场竞争的减少，即企业之间的相似性。

    Collaborative learning techniques have significantly advanced in recent years, enabling private model training across multiple organizations. Despite this opportunity, firms face a dilemma when considering data sharing with competitors -- while collaboration can improve a company's machine learning model, it may also benefit competitors and hence reduce profits. In this work, we introduce a general framework for analyzing this data-sharing trade-off. The framework consists of three components, representing the firms' production decisions, the effect of additional data on model quality, and the data-sharing negotiation process, respectively. We then study an instantiation of the framework, based on a conventional market model from economic theory, to identify key factors that affect collaboration incentives. Our findings indicate a profound impact of market conditions on the data-sharing incentives. In particular, we find that reduced competition, in terms of the similarities between th
    
[^134]: 如何通过概率电路将您的知识图嵌入转化为生成模型

    How to Turn Your Knowledge Graph Embeddings into Generative Models via Probabilistic Circuits. (arXiv:2305.15944v1 [cs.LG])

    [http://arxiv.org/abs/2305.15944](http://arxiv.org/abs/2305.15944)

    本文提出了将知识图嵌入模型转化为生成模型的方法，允许精确MLE学习、有效抽样新的三元组以及保证逻辑约束，获得了比原始KGEs更具伸缩性的性能。

    

    一些成功的知识图嵌入模型可用作基于能量的模型，而这篇论文解释了这些模型的得分函数，将其重新解释成为电路形式--这是一种允许有效边际化的约束计算图。然后，我们设计了两个方法来获得有效的生成电路模型，其中一个方法是将其激活限制为非负数，另一个方法是将其输出平方。我们的解释不会影响到预测节点连边模型的性能，但电路框架使得MLE的精确学习、新三元组的有效抽样以及保证逻辑约束得以满足成为可能。此外，我们的模型在拥有数百万个实体的图上比原始的KGEs更具伸缩性。

    Some of the most successful knowledge graph embedding (KGE) models for link prediction -- CP, RESCAL, TuckER, ComplEx -- can be interpreted as energy-based models. Under this perspective they are not amenable for exact maximum-likelihood estimation (MLE), sampling and struggle to integrate logical constraints. This work re-interprets the score functions of these KGEs as circuits -- constrained computational graphs allowing efficient marginalisation. Then, we design two recipes to obtain efficient generative circuit models by either restricting their activations to be non-negative or squaring their outputs. Our interpretation comes with little or no loss of performance for link prediction, while the circuits framework unlocks exact learning by MLE, efficient sampling of new triples, and guarantee that logical constraints are satisfied by design. Furthermore, our models scale more gracefully than the original KGEs on graphs with millions of entities.
    
[^135]: 差分隐私潜在扩散模型

    Differentially Private Latent Diffusion Models. (arXiv:2305.15759v1 [stat.ML])

    [http://arxiv.org/abs/2305.15759](http://arxiv.org/abs/2305.15759)

    本文提出使用差分隐私训练潜在扩散模型(LDMs)，通过预训练自编码器将高维像素空间转变为低维潜在空间实现更高效快速的DMs训练，并且通过只微调注意力模块减少了可训练参数的数量。

    

    扩散模型(DMs)被广泛用于生成高质量图像数据集。然而，由于它们直接在高维像素空间中运行，DMs的优化计算成本高，需要长时间的训练。这导致由于差分隐私的可组合性属性，大量噪音注入到差分隐私学习过程中。为了解决这个挑战，我们提出使用差分隐私训练潜在扩散模型(LDMs)。LDMs使用强大的预训练自编码器将高维像素空间减少到更低维的潜在空间，使训练DMs更加高效和快速。与[Ghalebikesabi等人，2023]预先用公共数据预训练DMs，然后再用隐私数据进行微调不同，我们仅微调LDMs中不同层的注意力模块以获得隐私敏感数据，相对于整个DM微调，可减少大约96%的可训练参数数量。

    Diffusion models (DMs) are widely used for generating high-quality image datasets. However, since they operate directly in the high-dimensional pixel space, optimization of DMs is computationally expensive, requiring long training times. This contributes to large amounts of noise being injected into the differentially private learning process, due to the composability property of differential privacy. To address this challenge, we propose training Latent Diffusion Models (LDMs) with differential privacy. LDMs use powerful pre-trained autoencoders to reduce the high-dimensional pixel space to a much lower-dimensional latent space, making training DMs more efficient and fast. Unlike [Ghalebikesabi et al., 2023] that pre-trains DMs with public data then fine-tunes them with private data, we fine-tune only the attention modules of LDMs at varying layers with privacy-sensitive data, reducing the number of trainable parameters by approximately 96% compared to fine-tuning the entire DM. We te
    
[^136]: 通过最优输运表征区分于分布误差

    Characterizing Out-of-Distribution Error via Optimal Transport. (arXiv:2305.15640v1 [cs.LG])

    [http://arxiv.org/abs/2305.15640](http://arxiv.org/abs/2305.15640)

    本论文提出了一种基于最优输运理论的新方法 - 置信最优输运(COT)，并且引入了基于经验的变体 - 带门限的置信最优输运(COTT)，它们能够更精确地估计模型的性能，特别是在面对伪标签转移误差时。

    

    在机器学习部署中，没在分布(out-of-distribution)的数据对模型提出了严峻的挑战，因此预测模型在没标签的o

    Out-of-distribution (OOD) data poses serious challenges in deployed machine learning models, so methods of predicting a model's performance on OOD data without labels are important for machine learning safety. While a number of methods have been proposed by prior work, they often underestimate the actual error, sometimes by a large margin, which greatly impacts their applicability to real tasks. In this work, we identify pseudo-label shift, or the difference between the predicted and true OOD label distributions, as a key indicator to this underestimation. Based on this observation, we introduce a novel method for estimating model performance by leveraging optimal transport theory, Confidence Optimal Transport (COT), and show that it provably provides more robust error estimates in the presence of pseudo-label shift. Additionally, we introduce an empirically-motivated variant of COT, Confidence Optimal Transport with Thresholding (COTT), which applies thresholding to the individual tra
    
[^137]: 在具有反馈图的在线学习中的Minimax遗憾

    On the Minimax Regret for Online Learning with Feedback Graphs. (arXiv:2305.15383v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15383](http://arxiv.org/abs/2305.15383)

    本论文改进了具有强可观察无向反馈图的在线学习遗憾的上下界，并提出了一个适用于任意$\alpha$情况下的改进上界，该上界与强盗案例和专家案例下界相匹配，并中间插值，证明过程使用了特定值$q \in [1/2, 1)$随$\alpha$变化的FTRL和$q$-Tsallis熵的方法。

    

    在这项工作中，我们改进了具有强可观察无向反馈图的在线学习遗憾的上下界。该问题的已知最优上界为$\mathcal {O}\bigl(\sqrt{\alpha T\ln K}\bigr)$，其中$K$是行动数量，$\alpha$是图的独立数，$T$是时间范围。 $\sqrt{\ln K}$因子在$\alpha=1$（专家案例）时被认为是必要的。另一方面，当$\alpha=K$（强盗案例）时，Minimax率为$\Theta\bigl(\sqrt{KT}\bigr)$，并且已知对于任何$\alpha$，下界为$\Omega\bigl(\sqrt{\alpha T}\bigr)$。我们的改进上界$\mathcal {O}\bigl(\sqrt{\alpha T(1+\ln(K/\alpha))}\bigr)$适用于任何$\alpha$，与强盗和专家的下界相匹配，并插值中间案例。为了证明这个结果，我们使用了具有特定值$q \in [1/2, 1)$随$\alpha$变化的FTRL和$q$-Tsallis熵的方法。

    In this work, we improve on the upper and lower bounds for the regret of online learning with strongly observable undirected feedback graphs. The best known upper bound for this problem is $\mathcal{O}\bigl(\sqrt{\alpha T\ln K}\bigr)$, where $K$ is the number of actions, $\alpha$ is the independence number of the graph, and $T$ is the time horizon. The $\sqrt{\ln K}$ factor is known to be necessary when $\alpha = 1$ (the experts case). On the other hand, when $\alpha = K$ (the bandits case), the minimax rate is known to be $\Theta\bigl(\sqrt{KT}\bigr)$, and a lower bound $\Omega\bigl(\sqrt{\alpha T}\bigr)$ is known to hold for any $\alpha$. Our improved upper bound $\mathcal{O}\bigl(\sqrt{\alpha T(1+\ln(K/\alpha))}\bigr)$ holds for any $\alpha$ and matches the lower bounds for bandits and experts, while interpolating intermediate cases. To prove this result, we use FTRL with $q$-Tsallis entropy for a carefully chosen value of $q \in [1/2, 1)$ that varies with $\alpha$. The analysis of 
    
[^138]: 黑盒变分推断收敛性分析

    Black-Box Variational Inference Converges. (arXiv:2305.15349v1 [cs.LG])

    [http://arxiv.org/abs/2305.15349](http://arxiv.org/abs/2305.15349)

    通过对黑盒变分推断（BBVI）的分析，发现一些常见的算法设计选择可能会导致次优收敛速率，但使用带有近端随机梯度下降的BBVI可以实现最强收敛率保证。

    

    我们提供了第一个完整的黑盒变分推断（BBVI）的收敛保证，也称为蒙特卡罗变分推断。尽管早期的研究只针对简化版本的BBVI进行了研究（例如，有界域、有界支持、仅针对尺度进行优化等），但我们的设置不需要任何这样的算法修改。我们的结果适用于对数平滑后验密度，无论是否强对数凹性以及位置-尺度变分族。此外，我们的分析揭示出了一些常见的算法设计选择，特别是变分近似尺度的非线性参数化，可能会导致次优收敛速率。幸运的是，运行带有近端随机梯度下降的BBVI可以纠正这些限制，从而实现已知的最强收敛率保证。我们通过将近端SGD与其他标准的BBVI实现进行比较，验证了这一理论结论在大规模数据集上的有效性。

    We provide the first convergence guarantee for full black-box variational inference (BBVI), also known as Monte Carlo variational inference. While preliminary investigations worked on simplified versions of BBVI (e.g., bounded domain, bounded support, only optimizing for the scale, and such), our setup does not need any such algorithmic modifications. Our results hold for log-smooth posterior densities with and without strong log-concavity and the location-scale variational family. Also, our analysis reveals that certain algorithm design choices commonly employed in practice, particularly, nonlinear parameterizations of the scale of the variational approximation, can result in suboptimal convergence rates. Fortunately, running BBVI with proximal stochastic gradient descent fixes these limitations, and thus achieves the strongest known convergence rate guarantees. We evaluate this theoretical insight by comparing proximal SGD against other standard implementations of BBVI on large-scale
    
[^139]: 训练基于能量的归一化流模型的得分匹配目标

    Training Energy-Based Normalizing Flow with Score-Matching Objectives. (arXiv:2305.15267v1 [cs.LG])

    [http://arxiv.org/abs/2305.15267](http://arxiv.org/abs/2305.15267)

    本文提出一种新的基于能量的归一化流模型（EBFlow），通过得分匹配目标优化使其训练更高效，同时开发一些技术增强EBFlow的训练稳定性和实证表现。

    

    本文建立了流模型和能量模型参数化之间的联系，并提出了一种新的基于能量的归一化流建模方法（EBFlow）。我们展示了通过得分匹配目标优化EBFlow，可以完全避开线性变换的雅可比行列式计算。这使得EBFlow在构建基于流的模型时使用任意线性层，而不会使每个训练迭代的计算时间复杂度从$\mathcal{O}(D^2L)$增加到$\mathcal{O}(D^3L)$，其中$L$为层数，$D$为输入维度。这使得EBFlow的训练比常用的最大似然训练方法更高效。除了减少运行时间外，我们通过基于分值匹配方法的分析开发了一些技术，以增强EBFlow的训练稳定性和实证表现。

    In this paper, we establish a connection between the parameterization of flow-based and energy-based generative models, and present a new flow-based modeling approach called energy-based normalizing flow (EBFlow). We demonstrate that by optimizing EBFlow with score-matching objectives, the computation of Jacobian determinants for linear transformations can be entirely bypassed. This feature enables the use of arbitrary linear layers in the construction of flow-based models without increasing the computational time complexity of each training iteration from $\mathcal{O}(D^2L)$ to $\mathcal{O}(D^3L)$ for an $L$-layered model that accepts $D$-dimensional inputs. This makes the training of EBFlow more efficient than the commonly-adopted maximum likelihood training method. In addition to the reduction in runtime, we enhance the training stability and empirical performance of EBFlow through a number of techniques developed based on our analysis on the score-matching methods. The experimental
    
[^140]: 动量被证明可以改善误差反馈！

    Momentum Provably Improves Error Feedback!. (arXiv:2305.15155v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15155](http://arxiv.org/abs/2305.15155)

    这项研究证明动量可以改善机器学习中的误差反馈问题，并提出了一个简单的解决方案，解决了批次大小过大的问题。

    

    由于在分布式环境中训练机器学习模型时存在较高的通信开销，现代算法总是依赖于有损压缩通信。然而，如果不加处理，压缩引起的误差会传播，并且可能导致严重的不稳定行为，包括指数级发散。大约十年前，Seide等人[2014]提出了一种称为EF14的错误反馈（EF）机制，作为缓解这个问题的极其有效的启发式方法。然而，尽管在过去十年中，EF领域在算法和理论方面有了稳定的进展，但我们对问题的理解还远未完善。在本文中，我们解决了其中一个最紧迫的问题。特别地，在经典的非凸设置中，所有已知的EF变种都依赖于非常大的批次大小才能收敛，这在实践中可能是禁止的。我们提出了一个令人惊讶地简单的解决方案，从理论和实践上消除了这个问题：将Polyak的动量应用到最新的i

    Due to the high communication overhead when training machine learning models in a distributed environment, modern algorithms invariably rely on lossy communication compression. However, when untreated, the errors caused by compression propagate, and can lead to severely unstable behavior, including exponential divergence. Almost a decade ago, Seide et al [2014] proposed an error feedback (EF) mechanism, which we refer to as EF14, as an immensely effective heuristic for mitigating this issue. However, despite steady algorithmic and theoretical advances in the EF field in the last decade, our understanding is far from complete. In this work we address one of the most pressing issues. In particular, in the canonical nonconvex setting, all known variants of EF rely on very large batch sizes to converge, which can be prohibitive in practice. We propose a surprisingly simple fix which removes this issue both theoretically, and in practice: the application of Polyak's momentum to the latest i
    
[^141]: 在表格数据上进行深度异常检测的超越个体输入

    Beyond Individual Input for Deep Anomaly Detection on Tabular Data. (arXiv:2305.15121v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15121](http://arxiv.org/abs/2305.15121)

    本文提出了一种采用非参数转换器的深度异常检测方法，能够捕捉表格数据中特征与特征之间以及样本与样本之间的依赖关系，并在广泛的表格数据集上取得了比现有最先进方法更优的性能。

    

    异常检测在金融、医疗和网络安全等各个领域都至关重要。在本文中，我们提出了一种新颖的基于非参数转换器（NPTs）的表格数据深度异常检测方法，以捕捉特征与特征之间以及样本与样本之间的依赖关系。在基于重构的框架中，我们训练NPT来重构正常样本的遮蔽特征。以非参数化方式，在推理过程中利用整个训练集，并利用模型在生成异常得分时重构遮蔽特征的能力。据我们所知，我们提出的方法是首个成功结合特征之间和样本之间依赖关系进行表格数据异常检测的方法。我们在31个表格数据集的广泛基准测试中评估了我们的方法，并证明我们的方法在F1得分和AUROC方面优于现有的最先进方法。

    Anomaly detection is crucial in various domains, such as finance, healthcare, and cybersecurity. In this paper, we propose a novel deep anomaly detection method for tabular data that leverages Non-Parametric Transformers (NPTs), a model initially proposed for supervised tasks, to capture both feature-feature and sample-sample dependencies. In a reconstruction-based framework, we train the NPT to reconstruct masked features of normal samples. In a non-parametric fashion, we leverage the whole training set during inference and use the model's ability to reconstruct the masked features during to generate an anomaly score. To the best of our knowledge, our proposed method is the first to successfully combine feature-feature and sample-sample dependencies for anomaly detection on tabular datasets. We evaluate our method on an extensive benchmark of 31 tabular datasets and demonstrate that our approach outperforms existing state-of-the-art methods based on the F1-score and AUROC by a signifi
    
[^142]: 使用概率潜在表示的块局部学习

    Block-local learning with probabilistic latent representations. (arXiv:2305.14974v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.14974](http://arxiv.org/abs/2305.14974)

    该论文提出了一种使用概率潜在表示的块局部学习方法，通过将深度神经网络分为块并引入反馈网络，在解决锁定和权重传输问题的同时，实现了高效的并行化和水平扩展。

    

    广泛使用的反向传播算法需要通过网络进行顺序更新，导致了锁定问题。此外，反向传播依赖于前向权重矩阵的转置来计算更新，导致了网络中的权重传输问题。锁定和权重传输问题阻碍了训练过程的高效并行化和水平扩展。我们提出了一种解决这些问题并扩展大型模型训练的新方法。我们的方法将深度神经网络分为块，并引入了一个反馈网络，从目标向后传播信息以提供辅助的局部损失。前向和后向传播可以并行进行，并且使用不同的权重集，解决了锁定和权重传输的问题。我们的方法源于一种统计解释的训练方法，将网络块的输出激活视为概率参数。

    The ubiquitous backpropagation algorithm requires sequential updates through the network introducing a locking problem. In addition, back-propagation relies on the transpose of forward weight matrices to compute updates, introducing a weight transport problem across the network. Locking and weight transport are problems because they prevent efficient parallelization and horizontal scaling of the training process. We propose a new method to address both these problems and scale up the training of large models. Our method works by dividing a deep neural network into blocks and introduces a feedback network that propagates the information from the targets backwards to provide auxiliary local losses. Forward and backward propagation can operate in parallel and with different sets of weights, addressing the problems of locking and weight transport. Our approach derives from a statistical interpretation of training that treats output activations of network blocks as parameters of probability
    
[^143]: 迈向可靠的假新闻缓解：泛化，不确定性和GPT-4

    Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4. (arXiv:2305.14928v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14928](http://arxiv.org/abs/2305.14928)

    本研究提出利用泛化、不确定性和最新的大型语言模型，寻求解决假新闻问题。实验证明GPT-4在多语言环境下表现优于之前的方法。研究还探讨了泛化和不确定性处理技术，并在其他语言模型、温度、提示、版本控制、可解释性和网络检索方面取得了实际见解和未来研究方向。此外，研究还发布了新颖的英法配对假新闻数据集LIAR-New，为信息真实性评估提供了可行性标签。

    

    假新闻构成了一个重要的社会挑战，目前的方法尚未找到有效的解决方案。我们提出关注泛化，不确定性以及如何利用最新的大型语言模型，以便在无法完美分类的情况下创建更实用的工具来评估信息真实性。我们首先证明了GPT-4在多个设定和语言中可以胜过之前的方法。接下来，我们探索泛化，揭示了GPT-4和RoBERTa-large在失效模式上的差异。第三，我们提出了处理不确定性的技术，可以检测到不可能的例子并显著改进结果。我们还讨论了其他语言模型，温度，提示，版本控制，可解释性和网络检索的结果，每个结果都提供了实际的见解和未来研究的方向。最后，我们发布了具有新颖的英法配对假新闻数据和可行性标签的LIAR-New数据集。

    Misinformation poses a critical societal challenge, and current approaches have yet to produce an effective solution. We propose focusing on generalization, uncertainty, and how to leverage recent large language models, in order to create more practical tools to evaluate information veracity in contexts where perfect classification is impossible. We first demonstrate that GPT-4 can outperform prior methods in multiple settings and languages. Next, we explore generalization, revealing that GPT-4 and RoBERTa-large exhibit differences in failure modes. Third, we propose techniques to handle uncertainty that can detect impossible examples and strongly improve outcomes. We also discuss results on other language models, temperature, prompting, versioning, explainability, and web retrieval, each one providing practical insights and directions for future research. Finally, we publish the LIAR-New dataset with novel paired English and French misinformation data and Possibility labels that indic
    
[^144]: 文本编码器限制了对比视觉-语言模型的组合性能

    Text encoders bottleneck compositionality in contrastive vision-language models. (arXiv:2305.14897v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14897](http://arxiv.org/abs/2305.14897)

    本研究发现，在对比视觉-语言模型中，使用单个向量表示标语的文本编码器在处理更加复杂的输入时表现不佳，但某些文本编码器的性能明显优于其他编码器。仅基于文本的恢复性能能够预测多模态匹配性能。

    

    高性能的视觉-语言模型（VL）如CLIP使用单一向量表示标题。在这个瓶颈中失去了多少关于语言的信息？我们首先策划了CompPrompts，这是一组越来越复杂的图像标题，VL模型应该能够捕捉到（例如，单个对象，到对象+属性，到多个互动对象）。然后，我们训练了仅基于文本的恢复探针，旨在从几个VL模型生成的单一向量文本表示中重建标题。这种方法不需要图像，相对于之前的工作，使我们能够在更广泛的场景上进行测试。我们发现：1）CLIP的文本编码器在更复杂的输入上表现不佳，包括对象关系、属性-对象关联、计数和否定；2）一些文本编码器比其他编码器要好得多；3）仅基于文本的恢复性能预测了ControlledImCaps上的多模态匹配性能：这是我们收集和发布的一个新的评估基准。

    Performant vision-language (VL) models like CLIP represent captions using a single vector. How much information about language is lost in this bottleneck? We first curate CompPrompts, a set of increasingly compositional image captions that VL models should be able to capture (e.g., single object, to object+property, to multiple interacting objects). Then, we train text-only recovery probes that aim to reconstruct captions from single-vector text representations produced by several VL models. This approach does not require images, allowing us to test on a broader range of scenes compared to prior work. We find that: 1) CLIP's text encoder falls short on more compositional inputs, including object relationships, attribute-object association, counting, and negations; 2) some text encoders work significantly better than others; and 3) text-only recovery performance predicts multi-modal matching performance on ControlledImCaps: a new evaluation benchmark we collect and release consisting of
    
[^145]: 一种用于多元时间序列预测的联合时频域变换器

    A Joint Time-frequency Domain Transformer for Multivariate Time Series Forecasting. (arXiv:2305.14649v1 [cs.LG])

    [http://arxiv.org/abs/2305.14649](http://arxiv.org/abs/2305.14649)

    本文提出了一种称为JTFT的方法，可以在提取时间依赖性的同时有效地减小计算需求，适用于多元时间序列预测，并具有线性复杂度。同时，采用低秩注意层以有效捕获跨维度的依赖关系，并在六个真实世界数据集上展示了比现有方法更好的表现。

    

    本文介绍了一种称为联合时频域变换器的方法，旨在提高多元时间序列的预测性能，同时最小化计算需求。该方法利用可学习频率的稀疏性，在频域有效地提取时间依赖性。除了频率域表示外，最近的有限数量的数据点也被直接编码在时间域中，以增强学习局部关系并减轻非平稳性的不利影响。JTFT具有线性复杂度，因为内部表示的长度保持独立于输入序列长度。此外，提出了一个低秩注意层，以有效捕获跨维度的依赖关系，并防止由于时间和通道建模的纠缠而导致性能降级。 对六个真实世界数据集进行的实验表明，JTFT优于现有的状态-of-the-art方法。

    To enhance predicting performance while minimizing computational demands, this paper introduces a joint time-frequency domain Transformer (JTFT) for multivariate forecasting. The method exploits the sparsity of time series in the frequency domain using a small number of learnable frequencies to extract temporal dependencies effectively. Alongside the frequency domain representation, a fixed number of the most recent data points are directly encoded in the time domain, bolstering the learning of local relationships and mitigating the adverse effects of non-stationarity. JTFT achieves linear complexity since the length of the internal representation remains independent of the input sequence length. Additionally, a low-rank attention layer is proposed to efficiently capture cross-dimensional dependencies and prevent performance degradation due to the entanglement of temporal and channel-wise modeling. Experiments conducted on six real-world datasets demonstrate that JTFT outperforms state
    
[^146]: 判别校准

    Discriminative calibration. (arXiv:2305.14593v1 [stat.ML])

    [http://arxiv.org/abs/2305.14593](http://arxiv.org/abs/2305.14593)

    这篇论文提出了一种替代基于排序的模拟校准（SBC）的灵活分类方法，该方法可以从数据中学习测试统计量，并计算出从分类准确度中计算出的误校准发散度度量，具有更高的统计功效，可以解决多重检验的挑战。

    

    为了检验贝叶斯计算的准确性，常常使用基于排序的模拟校准（SBC）。然而，SBC 存在一些缺点：测试统计量略显随意，交互性难以检查，多重检验是一个挑战，并且得到的 P 值不是一种发散度度量。我们提出用一种灵活的分类方法替换边缘排序检验，该方法可以从数据中学习测试统计量。该度量通常具有比 SBC 排名检验更高的统计功效，并返回从分类准确度计算出的可解释的误校准发散度度量。此方法可以与不同的数据生成过程一起使用，以应对无需似然推断或传统推断方法（如马尔科夫链蒙特卡罗或变分推断）。我们使用神经网络和统计学启发式特征演示了一种自动化实现，并用数值和真实数据实验验证了该方法。

    To check the accuracy of Bayesian computations, it is common to use rank-based simulation-based calibration (SBC). However, SBC has drawbacks: The test statistic is somewhat ad-hoc, interactions are difficult to examine, multiple testing is a challenge, and the resulting p-value is not a divergence metric. We propose to replace the marginal rank test with a flexible classification approach that learns test statistics from data. This measure typically has a higher statistical power than the SBC rank test and returns an interpretable divergence measure of miscalibration, computed from classification accuracy. This approach can be used with different data generating processes to address likelihood-free inference or traditional inference methods like Markov chain Monte Carlo or variational inference. We illustrate an automated implementation using neural networks and statistically-inspired features, and validate the method with numerical and real data experiments.
    
[^147]: 最优预条件和费舍尔自适应 Langevin 采样

    Optimal Preconditioning and Fisher Adaptive Langevin Sampling. (arXiv:2305.14442v1 [stat.ML])

    [http://arxiv.org/abs/2305.14442](http://arxiv.org/abs/2305.14442)

    通过最优预条件和费舍尔自适应 Langevin 采样，提出了一种计算有效且在高维中非常强健的自适应 MCMC 方案。

    

    我们通过分析最大化预期平方跳跃距离，为 Langevin 扩散定义了最优预条件。这导致最优预条件为反费舍尔信息协方差矩阵，其中协方差矩阵是在目标下平均对数目标梯度的外积。我们将此结果应用于 Metropolis 调整 Langevin 算法 (MALA)，并推导出一种从算法运行产生的梯度历史中学习预条件的计算有效的自适应 MCMC 方案。我们在几个实验中展示了所提出的算法在高维中非常强健，并且明显优于其他方法，包括使用标准自适应 MCMC 学习预条件和位置相关的 Riemann 流形 MALA 采样器的密切相关的自适应 MALA 方案。

    We define an optimal preconditioning for the Langevin diffusion by analytically maximizing the expected squared jumped distance. This yields as the optimal preconditioning an inverse Fisher information covariance matrix, where the covariance matrix is computed as the outer product of log target gradients averaged under the target. We apply this result to the Metropolis adjusted Langevin algorithm (MALA) and derive a computationally efficient adaptive MCMC scheme that learns the preconditioning from the history of gradients produced as the algorithm runs. We show in several experiments that the proposed algorithm is very robust in high dimensions and significantly outperforms other methods, including a closely related adaptive MALA scheme that learns the preconditioning with standard adaptive MCMC as well as the position-dependent Riemannian manifold MALA sampler.
    
[^148]: 等变神经模拟器用于随机时空动态

    Equivariant Neural Simulators for Stochastic Spatiotemporal Dynamics. (arXiv:2305.14286v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.14286](http://arxiv.org/abs/2305.14286)

    本研究提出了一种称为EPNS的等变概率神经模拟框架，可以在系统演化中生成等变分布，并在随机时空动态方面表现出色。

    

    神经网络正在成为可扩展的数据驱动高维动态系统模拟工具，特别是在数值方法不可行或计算昂贵的情况下。值得注意的是，已经证明在确定性神经模拟器中引入域对称性可以大大提高其精确性、样本效率和参数效率。然而，为了将对称性纳入可以模拟随机现象的概率神经模拟器中，我们需要一个能够生成等变轨迹分布而不是等变函数逼近的模型。在本文中，我们提出了等变概率神经模拟（EPNS），这是一个用于等变分布系统演化的自回归概率建模框架。我们使用EPNS设计了一个用于随机N体系统和随机细胞动力学的模型。我们的结果表明，EPNS在p方面比现有的基于神经网络的方法表现出色。

    Neural networks are emerging as a tool for scalable data-driven simulation of high-dimensional dynamical systems, especially in settings where numerical methods are infeasible or computationally expensive. Notably, it has been shown that incorporating domain symmetries in deterministic neural simulators can substantially improve their accuracy, sample efficiency, and parameter efficiency. However, to incorporate symmetries in probabilistic neural simulators that can simulate stochastic phenomena, we need a model that produces equivariant distributions over trajectories, rather than equivariant function approximations. In this paper, we propose Equivariant Probabilistic Neural Simulation (EPNS), a framework for autoregressive probabilistic modeling of equivariant distributions over system evolutions. We use EPNS to design models for a stochastic n-body system and stochastic cellular dynamics. Our results show that EPNS considerably outperforms existing neural network-based methods for p
    
[^149]: 分层提示提升大规模语言模型在网络导航中的应用

    Hierarchical Prompting Assists Large Language Model on Web Navigation. (arXiv:2305.14257v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14257](http://arxiv.org/abs/2305.14257)

    这项研究提出了一种分层提示方法来解决大规模语言模型在处理复杂观察的交互决策任务中的困难。研究表明该方法在网络导航中的效果优于先前最先进的提示机制，具有广泛的适用性。

    

    大规模语言模型（LLMs）在处理交互决策任务中的复杂观察时遇到困难。为了解决这个问题，我们提出了一种简单的分层提示方法。不同于以往总是把\emph{完整}观察（例如网页）放到提示中的提示方法，我们提出首先构建一个与动作相关的\emph{压缩}和\emph{相关}的观察，并使用专门的\summ提示。然后，\actor提示根据总结的观察预测下一个动作。尽管我们的方法具有广泛的适用性，但我们尤其展示了它在复杂的网络导航领域的有效性，其中完整的观察通常包含冗余和无关信息。我们的方法在任务成功率上优于先前最先进的提示机制6.2\%，展示了其在具有长时间观察轨迹的交互决策任务中的潜力。

    Large language models (LLMs) struggle on processing complicated observations in interactive decision making tasks. To alleviate this issue, we propose a simple hierarchical prompting approach. Diverging from previous prompting approaches that always put the \emph{full} observation~(\eg a web page) to the prompt, we propose to first construct an action-aware observation which is more \emph{condensed} and \emph{relevant} with a dedicated \summ prompt. The \actor prompt then predicts the next action based on the summarized observation. While our method has broad applicability, we particularly demonstrate its efficacy in the complex domain of web navigation where a full observation often contains redundant and irrelevant information. Our approach outperforms the previous state-of-the-art prompting mechanis by 6.2\% on task success rate, demonstrating its potential on interactive decision making tasks with long observation traces.
    
[^150]: 通过子4位整数量化实现压缩大型语言模型的内存高效微调

    Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization. (arXiv:2305.14152v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.14152](http://arxiv.org/abs/2305.14152)

    本文提出了一种称为PEQA的方法，结合了参数高效微调和量化LLM的优点。通过仅更新量化尺度，PEQA可以高效地微调压缩大型语言模型，并减少内存开销。

    

    大型语言模型（LLM）面临着在微调和部署过程中由于其高内存需求和计算成本而带来的挑战。虽然参数高效微调（PEFT）方法旨在减少微调过程中优化器状态的内存使用量，但预训练LLM权重本身的大小仍然是一个紧迫的问题。尽管量化技术被广泛提出来缓解内存需求和加快LLM推理速度，但大多数这些技术都是针对部署阶段。为了弥合这一差距，本文提出了参数高效和量化感知适应（PEQA）-一种简单而有效的方法，将PEFT的优点与量化LLM结合起来。通过仅更新量化尺度，PEQA可以直接应用于量化LLM，确保平稳的任务转换。与现有的PEFT方法并行，PEQA显着减少了与优化器状态相关的内存开销。此外，它还充分利用了

    Large language models (LLMs) face the challenges in fine-tuning and deployment due to their high memory demands and computational costs. While parameter-efficient fine-tuning (PEFT) methods aim to reduce the memory usage of the optimizer state during fine-tuning, the inherent size of pre-trained LLM weights continues to be a pressing concern. Even though quantization techniques are widely proposed to ease memory demands and accelerate LLM inference, most of these techniques are geared towards the deployment phase. To bridge this gap, this paper presents Parameter-Efficient and Quantization-aware Adaptation (PEQA) - a simple yet effective method that combines the advantages of PEFT with quantized LLMs. By updating solely the quantization scales, PEQA can be directly applied to quantized LLMs, ensuring seamless task transitions. Parallel to existing PEFT methods, PEQA significantly reduces the memory overhead associated with the optimizer state. Furthermore, it leverages the advantages o
    
[^151]: 关于高斯-斯坦变分梯度下降动态性的探究

    Towards Understanding the Dynamics of Gaussian--Stein Variational Gradient Descent. (arXiv:2305.14076v1 [math.ST])

    [http://arxiv.org/abs/2305.14076](http://arxiv.org/abs/2305.14076)

    本文探究了高斯-斯坦变分梯度下降动态性。对于从高斯目标中采样，只要初始值是高斯的，具有双线性核的SVGD动态将保持高斯状态。当目标函数呈现出强对数凹性时，证明了均场高斯-SVGD动态会线性收敛于KL散度下最接近目标高斯分布。在有限粒子设置中，存在对均场极限的时间微步一致收敛以及线性收敛至目标高斯分布。

    

    Stein Variational Gradient Descent (SVGD)是一种非参数基于粒子的确定性采样算法。尽管其被广泛使用，但理解SVGD的理论属性一直是一个具有挑战性的问题。对于从高斯目标中采样，只要初始值是高斯的，具有双线性核的SVGD动态将保持高斯状态。受此事实的启发，我们通过双线性核将SVGD投影到高斯分布族中，即高斯变分推断 (GVI) 与 SVGD。我们通过考虑均场 PDE 和离散粒子系统，提供了一个完整的图像。当目标函数呈现出强对数凹性时，证明了均场高斯-SVGD动态会线性收敛于KL散度下最接近目标高斯分布。在有限粒子设置中，存在对均场极限的时间微步一致收敛以及线性收敛至目标高斯分布。我们的分析基于一个新的代数恒等式，该等式将目标高斯分布的费希尔信息矩阵与粒子均匀分布的费希尔信息矩阵相关联。这个等式为我们提供了透视 GVI with SVGD 在均场和粒子设置中的动态性的统一视角。

    Stein Variational Gradient Descent (SVGD) is a nonparametric particle-based deterministic sampling algorithm. Despite its wide usage, understanding the theoretical properties of SVGD has remained a challenging problem. For sampling from a Gaussian target, the SVGD dynamics with a bilinear kernel will remain Gaussian as long as the initializer is Gaussian. Inspired by this fact, we undertake a detailed theoretical study of the Gaussian-SVGD, i.e., SVGD projected to the family of Gaussian distributions via the bilinear kernel, or equivalently Gaussian variational inference (GVI) with SVGD. We present a complete picture by considering both the mean-field PDE and discrete particle systems. When the target is strongly log-concave, the mean-field Gaussian-SVGD dynamics is proven to converge linearly to the Gaussian distribution closest to the target in KL divergence. In the finite-particle setting, there is both uniform in time convergence to the mean-field limit and linear convergence in ti
    
[^152]: SNEkhorn: 对称熵亲和力下的降维方法

    SNEkhorn: Dimension Reduction with Symmetric Entropic Affinities. (arXiv:2305.13797v1 [cs.LG])

    [http://arxiv.org/abs/2305.13797](http://arxiv.org/abs/2305.13797)

    提出了一种新的对称化方法用于熵亲和力下的降维算法，能够有效解决对称化过程中的熵和随机性问题。

    

    许多机器学习方法都依赖于加权图来编码数据集中样本间的相似性。熵亲和力（EAs）是这类图的特例，它通常用于流形学习算法 t-SNE 中。为了保证对不同采样密度的数据具有鲁棒性，EAs 按一定方式对每个样本分配一个核带宽参数，以使得亲和力矩阵中每一行的熵都保持在一个特定的指数参数下。EAs具有不对称性和按行随机性，但是在经过启发式对称化方法处理后，又被用于降维。本文发现了EAs的一种新颖的优化形式，视其作为最优传输问题，实现了自然的对称化，并且可用双重上升法高效计算。由此得到的亲和力矩阵有效地避免了对称化所带来的熵和随机性问题。

    Many approaches in machine learning rely on a weighted graph to encode the similarities between samples in a dataset. Entropic affinities (EAs), which are notably used in the popular Dimensionality Reduction (DR) algorithm t-SNE, are particular instances of such graphs. To ensure robustness to heterogeneous sampling densities, EAs assign a kernel bandwidth parameter to every sample in such a way that the entropy of each row in the affinity matrix is kept constant at a specific value, whose exponential is known as perplexity. EAs are inherently asymmetric and row-wise stochastic, but they are used in DR approaches after undergoing heuristic symmetrization methods that violate both the row-wise constant entropy and stochasticity properties. In this work, we uncover a novel characterization of EA as an optimal transport problem, allowing a natural symmetrization that can be computed efficiently using dual ascent. The corresponding novel affinity matrix derives advantages from symmetric do
    
[^153]: 理解ReLU网络的多阶段优化动态和丰富的非线性行为

    Understanding Multi-phase Optimization Dynamics and Rich Nonlinear Behaviors of ReLU Networks. (arXiv:2305.12467v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.12467](http://arxiv.org/abs/2305.12467)

    本文对于ReLU神经网络通过Gradient Flow训练的两层模型在线性可分数据上进行了完整的理论分析，揭示了整个训练过程中的四个不同阶段，显示出一个从简化到复杂的学习趋势。

    

    ReLU神经网络的训练过程经常表现出复杂的非线性现象。模型的非线性和损失的非凸性为理论分析带来了重大挑战。本文对通过Gradient Flow训练的二层ReLU网络在线性可分数据上进行了完整的理论描述。在这种特定的设置下，我们的分析捕获了从随机初始化到最终收敛的整个优化过程。尽管我们研究的模型和数据相对简单，但我们揭示了整个训练过程中的四个不同阶段，显示出一个从简化到复杂的学习趋势。特定的非线性行为也可以被精确地识别和理论上捕获，例如...

    The training process of ReLU neural networks often exhibits complicated nonlinear phenomena. The nonlinearity of models and non-convexity of loss pose significant challenges for theoretical analysis. Therefore, most previous theoretical works on the optimization dynamics of neural networks focus either on local analysis (like the end of training) or approximate linear models (like Neural Tangent Kernel). In this work, we conduct a complete theoretical characterization of the training process of a two-layer ReLU network trained by Gradient Flow on a linearly separable data. In this specific setting, our analysis captures the whole optimization process starting from random initialization to final convergence. Despite the relatively simple model and data that we studied, we reveal four different phases from the whole training process showing a general simplifying-to-complicating learning trend. Specific nonlinear behaviors can also be precisely identified and captured theoretically, such 
    
[^154]: （带噪声的）随机梯度下降的时间均匀Wasserstein稳定性界限

    Uniform-in-Time Wasserstein Stability Bounds for (Noisy) Stochastic Gradient Descent. (arXiv:2305.12056v1 [stat.ML])

    [http://arxiv.org/abs/2305.12056](http://arxiv.org/abs/2305.12056)

    本文通过建立学习理论和应用概率之间的联系，提出了一种证明随机优化算法Wasserstein稳定性界限的统一指南，并在随机梯度下降上验证了该方法的有效性，包括强凸损失和带添加噪声的非凸损失。

    

    算法稳定性是一个重要的概念，对于推导实践算法的泛化界限已被证明是有用的。过去十年已经见证了不同损失函数所应用的不同算法的稳定性界限的增加。虽然这些界限照亮了优化算法的各种属性，但每个案例的分析通常需要不同的证明技术和显著不同的数学工具。在本研究中，我们在学习理论和应用概率之间建立了新的联系，并介绍了一种证明随机优化算法的Wasserstein稳定性界限的统一指南。我们在随机梯度下降（SGD）上阐述了我们的方法，并获得了强凸损失和带添加噪声的非凸损失的时间均匀稳定性界限（即，界限不随迭代次数增加而增加），在这些情况下，我们恢复了与先前文献相似的结果或将它们扩展到更广泛。

    Algorithmic stability is an important notion that has proven powerful for deriving generalization bounds for practical algorithms. The last decade has witnessed an increasing number of stability bounds for different algorithms applied on different classes of loss functions. While these bounds have illuminated various properties of optimization algorithms, the analysis of each case typically required a different proof technique with significantly different mathematical tools. In this study, we make a novel connection between learning theory and applied probability and introduce a unified guideline for proving Wasserstein stability bounds for stochastic optimization algorithms. We illustrate our approach on stochastic gradient descent (SGD) and we obtain time-uniform stability bounds (i.e., the bound does not increase with the number of iterations) for strongly convex losses and non-convex losses with additive noise, where we recover similar results to the prior art or extend them to mor
    
[^155]: 差分隐私在线物品定价

    Differentially Private Online Item Pricing. (arXiv:2305.11362v1 [cs.GT])

    [http://arxiv.org/abs/2305.11362](http://arxiv.org/abs/2305.11362)

    本文介绍了一种差分隐私算法，可在保护买家隐私的同时实现重复、不限供应物品拍卖中的收益最大化，是第一个提供隐私保证的$O(\sqrt{T}\log{T})$亏损子线性的方法。

    

    本文探讨了在保护买方隐私的同时，实现重复、不限供应物品拍卖中的收益最大化问题。我们提出了一种新颖的算法，它与买方的输入对（商品选择和出价）具有差分隐私性质。值得注意的是，我们的算法是第一个提供隐私保证的$O(\sqrt{T}\log{T})$亏损子线性(regret)的方法。我们的方法基于指数权重元算法，通过小的随机扰动缓解了收益函数不连续的问题。由于与指数机制的结构相似，因此我们的方法固有地保证了差分隐私。我们还将我们的算法扩展到逐轮策略性出价的情况。内在的差分隐私性质使我们能够在最小修改的情况下适应这种情况，并确保其亏损子线性。

    This work addresses the problem of revenue maximization in a repeated, unlimited supply item-pricing auction while preserving buyer privacy. We present a novel algorithm that provides differential privacy with respect to the buyer's input pair: item selection and bid. Notably, our algorithm is the first to offer a sublinear $O(\sqrt{T}\log{T})$ regret with a privacy guarantee. Our method is based on an exponential weights meta-algorithm, and we mitigate the issue of discontinuities in revenue functions via small random perturbations. As a result of its structural similarity to the exponential mechanism, our method inherently secures differential privacy. We also extend our algorithm to accommodate scenarios where buyers strategically bid over successive rounds. The inherent differential privacy allows us to adapt our algorithm with minimal modification to ensure a sublinear regret in this setting.
    
[^156]: 在线学习者的攻击：一项教师-学生分析

    Attacks on Online Learners: a Teacher-Student Analysis. (arXiv:2305.11132v1 [stat.ML])

    [http://arxiv.org/abs/2305.11132](http://arxiv.org/abs/2305.11132)

    本文利用控制理论的视角研究了在线学习环境下可能遭受到的标签扰动攻击情况，得出攻击强度超过临界阈值时学习准确率将出现不连续转变的结论，并验证了理论在复杂结构学习器上的适用性。

    

    机器学习模型通常容易受到对抗性攻击：数据的微小扰动可能会使模型的预测结果产生灾难性的影响。虽然大量的文献研究了对已经预先训练的模型进行测试时的攻击情况，但在线学习环境下的攻击情况却鲜有研究。本文使用控制理论的视角研究了在线学习者可能存在的标签扰动攻击情况，考虑了不同的攻击策略，并针对简单线性学习器的稳态获得了分析结果。这些结果可以证明，当攻击强度超过临界阈值时，学习器的准确率会出现不连续的转变。然后我们使用真实数据对复杂结构的学习器进行了实证分析，验证了理论分析的洞见并揭示了遭受攻击的学习器的新行为。

    Machine learning models are famously vulnerable to adversarial attacks: small ad-hoc perturbations of the data that can catastrophically alter the model predictions. While a large literature has studied the case of test-time attacks on pre-trained models, the important case of attacks in an online learning setting has received little attention so far. In this work, we use a control-theoretical perspective to study the scenario where an attacker may perturb data labels to manipulate the learning dynamics of an online learner. We perform a theoretical analysis of the problem in a teacher-student setup, considering different attack strategies, and obtaining analytical results for the steady state of simple linear learners. These results enable us to prove that a discontinuous transition in the learner's accuracy occurs when the attack strength exceeds a critical threshold. We then study empirically attacks on learners with complex architectures using real data, confirming the insights of 
    
[^157]: 语言模型遇见世界模型：实体经验增强语言模型

    Language Models Meet World Models: Embodied Experiences Enhance Language Models. (arXiv:2305.10626v1 [cs.CL])

    [http://arxiv.org/abs/2305.10626](http://arxiv.org/abs/2305.10626)

    本文提出一种新的增强语言模型的方法——将其与世界模型相结合，通过有目的的规划和随机探索获得丰富的实体经验进行微调, 以提高其在物理环境下的推理和行为能力，并在语言基准上保持或提高性能。

    

    虽然大型语言模型 (LMs) 在许多任务上表现出了非凡的能力，但它们常常在处理物理环境下的简单推理和规划问题时遇到困难，例如理解物体永恒或规划家庭活动。这种限制源于 LM 仅受书面语言训练，缺少必要的实体知识和技能。本文提出了一种新的增强 LM 的方法，即将其与世界模型相结合进行微调，以获得多样化的实体知识，同时保持其一般语言能力。本方法在世界模型中部署一个融入实体经验的代理，特别是一个模拟物理世界的仿真器(VirtualHome)，通过有目的的规划和随机探索获得多样化的实体经验。然后，利用这些经验微调 LM ，以教授在物理世界中的各种推理和行为能力，例如规划和完成目标、物体永恒和跟踪等。此外，我们相信我们的方法可以轻松扩展以利用其他模拟器，包括机器人或自动驾驶汽车。我们证明了我们的方法显着提高了 LM 在一系列物理推理任务中的性能，同时保留并经常提高了它们在语言基准上的表现。

    While large language models (LMs) have shown remarkable capabilities across numerous tasks, they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills. In this paper, we propose a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities. Our approach deploys an embodied agent in a world model, particularly a simulator of the physical world (VirtualHome), and acquires a diverse set of embodied experiences through both goal-oriented planning and random exploration. These experiences are then used to finetune LMs to teach diverse abilities of reasoning and acting in the physical world, e.g., planning and completing goals, object permanence and tracking, etc. Moreover, it is
    
[^158]: 生成式语言模型的统计知识评估

    Statistical Knowledge Assessment for Generative Language Models. (arXiv:2305.10519v1 [cs.CL])

    [http://arxiv.org/abs/2305.10519](http://arxiv.org/abs/2305.10519)

    本文介绍了一个统计知识评估框架，用于评估生成式语言模型（GLMs）的知识输出。通过对14种GLMs进行综合比较，发现知识遵循缩放定律，而对指令遵循数据进行微调可能会损害模型的生成能力。

    

    生成式语言模型（GLMs）展示了存储事实知识和高效回答查询的能力。但是，给定不同的提示，GLM是否始终生成事实正确的答案？本文介绍了一个由潜变量和KaRR度量指导的统计知识评估框架，该度量通过计算模型在各种文本形式上的连续概率量化其知识。我们使用我们的框架对14种GLM的知识进行了全面比较，包括LLaMA、Alpaca、OPT和其他模型。我们的统计知识评估涵盖了600种关系类型，并显示出与人类评估的强相关性（0.43 Kendall's $\tau$）。我们的发现揭示了具有相同支架结构的GLM的知识遵循缩放定律，并且在指令遵循数据上进行的微调可能会损害模型持续生成事实正确的文本的能力。

    Generative Language Models (GLMs) have demonstrated capabilities to store factual knowledge and answer queries efficiently. Given varying prompts, does a GLM consistently generate factually correct answers? In this paper, we introduce a statistical knowledge assessment framework guided by latent variables and the KaRR metric, which quantifies a model's knowledge by computing its continuous probability across diverse text forms. We conduct a comprehensive comparison of knowledge across 14 GLMs using our framework, including LLaMA, Alpaca, OPT, and others. Our statistical knowledge assessment encompasses 600 relation types and exhibits a strong correlation (0.43 Kendall's $\tau$) with human evaluation. Our findings reveal that the knowledge in GLMs with the same backbone architecture adheres to the scaling law, and that tuning on instruction-following data may compromise the model's ability to generate factually correct text consistently.
    
[^159]: 二分类中代理风险的对抗一致性

    The Adversarial Consistency of Surrogate Risks for Binary Classification. (arXiv:2305.09956v1 [cs.LG])

    [http://arxiv.org/abs/2305.09956](http://arxiv.org/abs/2305.09956)

    本文研究了二分类中代理风险的对抗一致性，并给出了代理损失函数集合的特征化，结果表明敌对一致代理的类与标准设置相比小得多。

    

    我们研究用于鲁棒二分类的代理风险的一致性。常见的做法是通过对抗性训练来学习鲁棒分类器，该方法试图在每个示例可以在小球内被恶意损坏的情况下最小化期望的$0$-$1$损失。我们给出了一种简单而完整的代理损失函数集的特征化，这些集是“一致”的，即可以替换$0$-$1$损失而不影响原始对抗风险的最小化序列的任何数据分布。我们还证明了用于$\rho$-margin损失的对抗一致性的量化版本。我们的结果显示，与标准设置相比，敌对一致代理的类明显较小，在标准设置中，许多常见的代理都被认为是一致的。

    We study the consistency of surrogate risks for robust binary classification. It is common to learn robust classifiers by adversarial training, which seeks to minimize the expected $0$-$1$ loss when each example can be maliciously corrupted within a small ball. We give a simple and complete characterization of the set of surrogate loss functions that are \emph{consistent}, i.e., that can replace the $0$-$1$ loss without affecting the minimizing sequences of the original adversarial risk, for any data distribution. We also prove a quantitative version of adversarial consistency for the $\rho$-margin loss. Our results reveal that the class of adversarially consistent surrogates is substantially smaller than in the standard setting, where many common surrogates are known to be consistent.
    
[^160]: k-匿名和合成数据技术的能量成本和机器学习准确性影响。

    Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques. (arXiv:2305.07116v1 [cs.LG])

    [http://arxiv.org/abs/2305.07116](http://arxiv.org/abs/2305.07116)

    本文研究了隐私增强技术对机器学习模型准确性和能量消耗的影响，发现k-匿名化训练的模型准确性较低，但能源消耗较少，合成数据是一种有潜力的选择，因为它能够在能源消耗更少的情况下取得可比的准确性。

    

    为了解决与隐私和气候变化有关的愈发增长的社会关切，欧盟颁布了《通用数据保护条例》(GDPR)并承诺了绿色协议。大量研究探究了运用匿名数据集训练机器学习模型的能效和准确性。最近的研究开始探究隐私增强技术（PET）对机器学习模型的能量消耗和准确性的影响，重点关注k-匿名。由于合成数据越来越受欢迎，因此本文分析了两个阶段的能量消耗和准确性：a）将隐私增强技术应用于相关数据集，b）在相关隐私增强数据集上训练模型。我们使用两种隐私增强技术：k-匿名化（使用泛化和抑制）和合成数据，以及三种机器学习模型。每个模型都在每个隐私增强数据集上进行训练。结果显示，在经过k-匿名化的数据上训练的模型具有较低的准确性，并且消耗的能量较少，与在非匿名化数据上训练的模型相比。然而，k-匿名化过程中消耗的能量非常可观，在评估其有用性时必须将其考虑在内。合成数据证明是一种有前途的选择，因为它在消耗更少能源的情况下实现了与非匿名化数据可比的准确性。

    To address increasing societal concerns regarding privacy and climate, the EU adopted the General Data Protection Regulation (GDPR) and committed to the Green Deal. Considerable research studied the energy efficiency of software and the accuracy of machine learning models trained on anonymised data sets. Recent work began exploring the impact of privacy-enhancing techniques (PET) on both the energy consumption and accuracy of the machine learning models, focusing on k-anonymity. As synthetic data is becoming an increasingly popular PET, this paper analyses the energy consumption and accuracy of two phases: a) applying privacy-enhancing techniques to the concerned data set, b) training the models on the concerned privacy-enhanced data set. We use two privacy-enhancing techniques: k-anonymisation (using generalisation and suppression) and synthetic data, and three machine-learning models. Each model is trained on each privacy-enhanced data set. Our results show that models trained on k-a
    
[^161]: 多智能体强化学习中的信息设计

    Information Design in Multi-Agent Reinforcement Learning. (arXiv:2305.06807v1 [cs.GT])

    [http://arxiv.org/abs/2305.06807](http://arxiv.org/abs/2305.06807)

    本文探究了多智能体强化学习中的信息设计问题及其挑战，提出了“马尔科夫信令博弈”的概念。

    

    强化学习（RL）模仿人类和动物与环境交互的方式。然而实际环境中存在其他有自己目标的智能体，它们会适应地与自己相互作用。因此，为了在这些环境中成功，自主智能体需要影响其他智能体以使它们的行为更有益。信息设计是影响其他智能体行为的一种方法。本文探讨了针对一组RL代理的信息设计问题，并提出了“马尔科夫信令博弈”的概念。

    Reinforcement learning (RL) mimics how humans and animals interact with the environment. The setting is somewhat idealized because, in actual tasks, other agents in the environment have their own goals and behave adaptively to the ego agent. To thrive in those environments, the agent needs to influence other agents so their actions become more helpful and less harmful. Research in computational economics distills two ways to influence others directly: by providing tangible goods (mechanism design) and by providing information (information design). This work investigates information design problems for a group of RL agents. The main challenges are two-fold. One is the information provided will immediately affect the transition of the agent trajectories, which introduces additional non-stationarity. The other is the information can be ignored, so the sender must provide information that the receivers are willing to respect. We formulate the Markov signaling game, and develop the notions 
    
[^162]: 基于Segment Anything Model (SAM)增强伪标签的弱监督语义分割

    Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation. (arXiv:2305.05803v1 [cs.CV])

    [http://arxiv.org/abs/2305.05803](http://arxiv.org/abs/2305.05803)

    本论文提出了一种通过利用Segment Anything Model (SAM)增强Class Activation Maps (CAM)生成高质量伪标签的方法来解决弱监督语义分割中CAM的局部激活和虚假激活的限制问题。

    

    仅使用图像级别的监督的弱监督语义分割(WSSS)由于其与像素级注释相比的低成本而越来越受到关注。大多数现有方法依赖于类激活图(CAM)生成像素级的伪标签进行监督训练。然而，众所周知，CAM经常遭受局部激活的限制-只激活最具区分性的部分，而不是整个对象区域和虚假的激活-不必要地激活物体周围的背景。本研究引入了一种简单而有效的方法来解决这些限制，即利用最近发布的Segment Anything Model (SAM)增强CAM生成高质量的伪标签。SAM是一个分割基础模型，展示了将图像分割成段落的强零-shot能力，但缺乏这些区域的语义标签。为了解决这个问题，我们采用特定类别的伪标签作为信号来选择m。

    Weakly Supervised Semantic Segmentation (WSSS) with only image-level supervision has garnered increasing attention due to its low annotation cost compared to pixel-level annotation. Most existing methods rely on Class Activation Maps (CAM) to generate pixel-level pseudo labels for supervised training. However, it is well known that CAM often suffers from partial activation -- activating the most discriminative part instead of the entire object area, and false activation -- unnecessarily activating the background around the object. In this study, we introduce a simple yet effective approach to address these limitations by harnessing the recently released Segment Anything Model (SAM) to generate higher-quality pseudo labels with CAM. SAM is a segmentation foundation model that demonstrates strong zero-shot ability in partitioning images into segments but lacks semantic labels for these regions. To circumvent this, we employ pseudo labels for a specific class as the signal to select the m
    
[^163]: NLI4CT：面向临床试验报告的多证据自然语言推理

    NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports. (arXiv:2305.03598v1 [cs.CL])

    [http://arxiv.org/abs/2305.03598](http://arxiv.org/abs/2305.03598)

    本文提出了一种面向临床试验报告的自然语言推理模型，通过检索支持事实来确定自然语言陈述和CTR之间的推理关系。

    

    如何解释和检索用于支持临床决策的医学证据？多年来，积累下来的临床试验报告包含了发展个性化医学所必需的信息。然而，为了找到最佳的实验治疗证据，手动检查超过400,000个临床试验报告是实际上不可行的。自然语言推理（NLI）提供了一个潜在的解决方案，通过允许可扩展计算文本蕴含关系。然而，现有的NLI模型在生物医学语料库上表现不佳，之前发布的数据集无法捕捉CTR推理的全部复杂性。在本文中，我们提出了一种新的资源，以推进关于CTR推理的NLI研究。该资源包括两个主要任务。首先，确定自然语言陈述和CTR之间的推理关系。其次，检索支持事实以证明预测的关系。我们提供了NLI4CT，一个基于CTR的语料库。

    How can we interpret and retrieve medical evidence to support clinical decisions? Clinical trial reports (CTR) amassed over the years contain indispensable information for the development of personalized medicine. However, it is practically infeasible to manually inspect over 400,000+ clinical trial reports in order to find the best evidence for experimental treatments. Natural Language Inference (NLI) offers a potential solution to this problem, by allowing the scalable computation of textual entailment. However, existing NLI models perform poorly on biomedical corpora, and previously published datasets fail to capture the full complexity of inference over CTRs. In this work, we present a novel resource to advance research on NLI for reasoning on CTRs. The resource includes two main tasks. Firstly, to determine the inference relation between a natural language statement, and a CTR. Secondly, to retrieve supporting facts to justify the predicted relation. We provide NLI4CT, a corpus of
    
[^164]: 解密高斯混合专家模型中的Softmax门控问题

    Demystifying Softmax Gating in Gaussian Mixture of Experts. (arXiv:2305.03288v1 [stat.ML])

    [http://arxiv.org/abs/2305.03288](http://arxiv.org/abs/2305.03288)

    本文提出了新的参数Vononoi损失函数并建立了MLE的收敛速度来解决高斯混合专家模型中的Softmax门控问题，研究表明该门控与高斯分布中的专家函数通过偏微分方程相互作用，是一个复杂依赖关系。

    

    理解Softmax门控高斯混合专家模型的参数估计一直是文献中长期未解决的问题。这主要是由于三个基本理论挑战与Softmax门控相关：（i）只能识别参数的平移；（ii）Softmax门控和高斯分布中专家函数之间通过偏微分方程的内在相互作用；（iii）Softmax门控高斯混合专家模型的条件密度的分子和分母之间的复杂依赖关系。我们通过提出新的参数Vononoi损失函数并建立MLE的收敛速度来解决这些挑战，用于解决这些模型的参数估计。当专家数量未知且超额指定时，我们的发现表明MLE的速率与一组多项式方程的可解性问题有关。

    Understanding parameter estimation of softmax gating Gaussian mixture of experts has remained a long-standing open problem in the literature. It is mainly due to three fundamental theoretical challenges associated with the softmax gating: (i) the identifiability only up to the translation of the parameters; (ii) the intrinsic interaction via partial differential equation between the softmax gating and the expert functions in Gaussian distribution; (iii) the complex dependence between the numerator and denominator of the conditional density of softmax gating Gaussian mixture of experts. We resolve these challenges by proposing novel Vononoi loss functions among parameters and establishing the convergence rates of the maximum likelihood estimator (MLE) for solving parameter estimation in these models. When the number of experts is unknown and over-specified, our findings show a connection between the rate of MLE and a solvability problem of a system of polynomial equations.
    
[^165]: 一种适应未知分布漂移的学习算法

    An Adaptive Algorithm for Learning with Unknown Distribution Drift. (arXiv:2305.02252v1 [cs.LG])

    [http://arxiv.org/abs/2305.02252](http://arxiv.org/abs/2305.02252)

    一种适应未知分布漂移的学习算法，相对于当前分布在不需要先验知识的情况下，学习一个函数族，且误差几乎与预先知道漂移大小的学习算法相同。此外，由于该算法适应数据，因此可以保证比依赖于漂移宽松限制的算法具有更好的学习效果。

    

    我们开发和分析了一种学习未知分布漂移的通用技术。给定一个从漂移分布的最后$T$步中独立观测到的序列，我们的算法在$T$时刻不加区分地学习一个函数族，相对于当前分布。与以前的工作不同，我们的技术不需要关于漂移大小的先验知识。相反，该算法适应样本数据。在不明确估计漂移的情况下，该算法学习的函数族的误差几乎与预先知道漂移大小的学习算法相同。此外，由于我们的算法适应数据，它可以保证比依赖于漂移宽松限制的算法具有更好的学习效果。

    We develop and analyze a general technique for learning with an unknown distribution drift. Given a sequence of independent observations from the last $T$ steps of a drifting distribution, our algorithm agnostically learns a family of functions with respect to the current distribution at time $T$. Unlike previous work, our technique does not require prior knowledge about the magnitude of the drift. Instead, the algorithm adapts to the sample data. Without explicitly estimating the drift, the algorithm learns a family of functions with almost the same error as a learning algorithm that knows the magnitude of the drift in advance. Furthermore, since our algorithm adapts to the data, it can guarantee a better learning error than an algorithm that relies on loose bounds on the drift.
    
[^166]: 联邦学习与O-RAN的协同：面向多个分布式机器学习服务的弹性虚拟化架构

    Synergies Between Federated Learning and O-RAN: Towards an Elastic Virtualized Architecture for Multiple Distributed Machine Learning Services. (arXiv:2305.02109v1 [cs.NI])

    [http://arxiv.org/abs/2305.02109](http://arxiv.org/abs/2305.02109)

    本文研究了联邦学习在现代无线网络下的挑战，提出了一种方法称为动态多服务联邦学习（DMS-FL）来解决这个问题。同时，还提出了一种名为弹性虚拟化联邦学习（EV-FL）的分布式机器学习架构，来支持DMS-FL中的设计要求。

    

    联邦学习是最流行的分布式机器学习技术，但是在现代无线网络中实现联邦学习面临着许多挑战，主要包括网络条件的动态性、系统中多个联邦学习服务/任务的并存以及联邦学习服务与其他网络服务的并行执行等。针对这些挑战，本文提出了一种名为动态多服务联邦学习（DMS-FL）的联邦学习泛型架构，并通过提出一种新的分布式机器学习架构——弹性虚拟化联邦学习（EV-FL）来解决DMS-FL中的三个未探索的设计问题。

    Federated learning (FL) is the most popular distributed machine learning technique. However, implementation of FL over modern wireless networks faces key challenges caused by (i) dynamics of the network conditions, (ii) coexistence of multiple FL services/tasks in the system, and (iii) concurrent execution of FL services with other network services, which are not jointly considered in prior works. Motivated by these challenges, we introduce a generic FL paradigm over next-generation (NextG) networks, called dynamic multi-service FL (DMS-FL). We identify three unexplored design considerations in DMS-FL: (i) FL service operator accumulation, (ii) wireless resource fragmentation, and (iii) signal strength fluctuations. We take the first steps towards addressing these design considerations through proposing a novel distributed ML architecture called elastic virtualized FL (EV-FL). EV-FL unleashes the full potential of Open RAN (O-RAN) systems and introduces an elastic resource provisioning
    
[^167]: 域不可知傅里叶神经算子

    Domain Agnostic Fourier Neural Operators. (arXiv:2305.00478v1 [cs.LG])

    [http://arxiv.org/abs/2305.00478](http://arxiv.org/abs/2305.00478)

    介绍了一种新的神经算子架构 DAFNO，可以学习带有不规则几何和不断变化的域的代理。通过将平滑化的特征函数纳入 FNOs 的积分层架构中，并利用 FFT 来实现快速计算，以明确的方式将几何信息编码到架构中，DAFNO 相对于基线神经算子模型具有最先进的精度。

    

    傅里叶神经算子（FNOs）能够学习在函数空间之间高度非线性的映射，最近已成为学习复杂物理系统响应的热门工具。然而，为了实现良好的精度和效率，FNOs 依赖于快速傅里叶变换 (FFT)，该变换仅限于矩形域上的建模问题。为了消除这样的限制，允许 FFT 在不规则几何以及拓扑变化中使用，我们引入了域不可知傅里叶神经算子 (DAFNO)，一种用于学习带有不规则几何和不断变化的域的代理的新的神经算子架构。关键思想是将平滑化的特征函数纳入 FNOs 的积分层架构中，并利用 FFT 来实现快速计算，以便以明确的方式将几何信息编码到架构中。在我们的实证评估中，DAFNO 相对于基线神经算子模型具有最先进的精度。

    Fourier neural operators (FNOs) can learn highly nonlinear mappings between function spaces, and have recently become a popular tool for learning responses of complex physical systems. However, to achieve good accuracy and efficiency, FNOs rely on the Fast Fourier transform (FFT), which is restricted to modeling problems on rectangular domains. To lift such a restriction and permit FFT on irregular geometries as well as topology changes, we introduce domain agnostic Fourier neural operator (DAFNO), a novel neural operator architecture for learning surrogates with irregular geometries and evolving domains. The key idea is to incorporate a smoothed characteristic function in the integral layer architecture of FNOs, and leverage FFT to achieve rapid computations, in such a way that the geometric information is explicitly encoded in the architecture. In our empirical evaluation, DAFNO has achieved state-of-the-art accuracy as compared to baseline neural operator models on two benchmark dat
    
[^168]: 探究大型语言模型在生成单元测试方面的有效性

    Exploring the Effectiveness of Large Language Models in Generating Unit Tests. (arXiv:2305.00418v1 [cs.SE])

    [http://arxiv.org/abs/2305.00418](http://arxiv.org/abs/2305.00418)

    本文研究了三种生成模型在单元测试生成方面的效果，并发现在不经过微调的情况下，它们的覆盖率较低且存在测试味道问题。

    

    代码生成模型可以通过使用代码注释、现有代码或两者的组合来生成代码。本文调查了三个生成模型（CodeGen、Codex和GPT-3.5）在没有微调的情况下是否能够成功用于生成单元测试的效果。研究中使用了两个基准（HumanEval和Evosuite SF110）来调查环境生成对单元测试生成过程的影响。我们根据编译率、测试正确性、覆盖率和测试味道来评估模型。我们发现，Codex模型在HumanEval数据集上取得了超过80%的覆盖率，但在EvoSuite SF110基准中没有一个模型超过2%的覆盖率。生成的测试还存在测试味道问题，比如重复的断言和空测试。

    A code generation model generates code by taking a prompt from a code comment, existing code, or a combination of both. Although code generation models (e.g., GitHub Copilot) are increasingly being adopted in practice, it is unclear whether they can successfully be used for unit test generation without fine-tuning. To fill this gap, we investigated how well three generative models (CodeGen, Codex, and GPT-3.5) can generate test cases. We used two benchmarks (HumanEval and Evosuite SF110) to investigate the context generation's effect in the unit test generation process. We evaluated the models based on compilation rates, test correctness, coverage, and test smells. We found that the Codex model achieved above 80% coverage for the HumanEval dataset, but no model had more than 2% coverage for the EvoSuite SF110 benchmark. The generated tests also suffered from test smells, such as Duplicated Asserts and Empty Tests.
    
[^169]: 实现机制可解释性自动电路发现

    Towards Automated Circuit Discovery for Mechanistic Interpretability. (arXiv:2304.14997v1 [cs.LG])

    [http://arxiv.org/abs/2304.14997](http://arxiv.org/abs/2304.14997)

    该论文提出了一种新算法 ACDC，可以自动识别网络中的重要单元，从而实现机制可解释性自动电路发现。

    

    最近的机制可解释性研究倒推了变形金刚模型的非平凡行为。这些发现需要大量的工作和研究者的直觉，这使得应用相同的方法来了解当前模型所展示的复杂行为变得困难。然而，这些发现的核心工作流程非常相似。研究人员创建一个数据集和度量，诱发所需的模型行为，将网络分为适当的抽象单元，替换这些单元的激活以确定哪些参与了行为，然后解释这些单元实施的功能。通过改变数据集、度量和待研究的单元，研究人员可以理解每个神经网络区域的功能和它们组成的电路。本文提出了一种新算法，自动电路发现（ACDC），以自动识别网络中的重要单元。

    Recent work in mechanistic interpretability has reverse-engineered nontrivial behaviors of transformer models. These contributions required considerable effort and researcher intuition, which makes it difficult to apply the same methods to understand the complex behavior that current models display. At their core however, the workflow for these discoveries is surprisingly similar. Researchers create a data set and metric that elicit the desired model behavior, subdivide the network into appropriate abstract units, replace activations of those units to identify which are involved in the behavior, and then interpret the functions that these units implement. By varying the data set, metric, and units under investigation, researchers can understand the functionality of each neural network region and the circuits they compose. This work proposes a novel algorithm, Automatic Circuit DisCovery (ACDC), to automate the identification of the important units in the network. Given a model's comput
    
[^170]: 探索外部分布广义化中的特征学习

    Towards Understanding Feature Learning in Out-of-Distribution Generalization. (arXiv:2304.11327v1 [cs.LG])

    [http://arxiv.org/abs/2304.11327](http://arxiv.org/abs/2304.11327)

    研究发现，ERM本质上同时学习了具有误导性的特征和不变特征，在ERM预训练期间学习到的特征质量影响了最终的OOD性能，未能捕获所有潜在的有用特征将限制最终的OOD性能。

    

    对于外部分布（OOD）广义化的失败，常见的解释是使用经验风险最小化（ERM）模型学习到具有误导性的特征而不是期望的不变特征。然而，最近的几项研究挑战了这种解释，发现深度网络可能已经学到了足够好的特征进行OOD广义化。这场辩论扩展到了许多OOD广义化任务的训练或微调神经网络的内部组织和OOD性能相关性中。为了理解这些似乎相互矛盾的现象，我们进行了理论研究，发现ERM本质上同时学习了具有误导性的特征和不变特征。另一方面，在ERM预训练期间学习到的特征质量显著影响了最终的OOD性能，因为OOD对象很少学习到新功能。未能在预训练期间捕获所有潜在的有用特征将进一步限制最终的OOD性能。

    A common explanation for the failure of out-of-distribution (OOD) generalization is that the model trained with empirical risk minimization (ERM) learns spurious features instead of the desired invariant features. However, several recent studies challenged this explanation and found that deep networks may have already learned sufficiently good features for OOD generalization. The debate extends to the in-distribution and OOD performance correlations along with training or fine-tuning neural nets across a variety of OOD generalization tasks. To understand these seemingly contradicting phenomena, we conduct a theoretical investigation and find that ERM essentially learns both spurious features and invariant features. On the other hand, the quality of learned features during ERM pre-training significantly affects the final OOD performance, as OOD objectives rarely learn new features. Failing to capture all the underlying useful features during pre-training will further limit the final OOD
    
[^171]: 基于物理插值的水网泄漏定位中的字典学习

    Learning Dictionaries from Physical-Based Interpolation for Water Network Leak Localization. (arXiv:2304.10932v1 [eess.SY])

    [http://arxiv.org/abs/2304.10932](http://arxiv.org/abs/2304.10932)

    本文提出一种基于物理插值和字典学习的泄漏定位方法，应用于Modena案例得到了优于现有技术的结果。

    

    本文介绍了一种基于状态估计和学习的泄漏定位方法。第一个阶段由插值方案处理，第二个阶段考虑字典学习。新提出的插值技术利用了水力连接的物理学原理，连接相邻节点的液压头。另外，残差直接被插值而不是液压头值。将所提出的方法应用于一个著名案例(Modena)，结果表明，新的插值方法在插值误差(考虑状态和残差估计)和后验定位方面都优于现有技术。

    This article presents a leak localization methodology based on state estimation and learning. The first is handled by an interpolation scheme, whereas dictionary learning is considered for the second stage. The novel proposed interpolation technique exploits the physics of the interconnections between hydraulic heads of neighboring nodes in water distribution networks. Additionally, residuals are directly interpolated instead of hydraulic head values. The results of applying the proposed method to a well-known case study (Modena) demonstrated the improvements of the new interpolation method with respect to a state-of-the-art approach, both in terms of interpolation error (considering state and residual estimation) and posterior localization.
    
[^172]: 从预训练模型中学习样本难度以提高模型可靠性

    Learning Sample Difficulty from Pre-trained Models for Reliable Prediction. (arXiv:2304.10127v1 [cs.LG])

    [http://arxiv.org/abs/2304.10127](http://arxiv.org/abs/2304.10127)

    该论文介绍了如何使用预训练模型通过熵正则化来计算训练样本的难度，并根据样本难度惩罚过于自信的预测，从而提高模型的准确性和不确定性校准。

    

    大规模的预训练模型在各种场景和应用中取得了显著的成功，但如何利用它们来提高下游模型的预测可靠性仍未得到充分探索。此外，现代神经网络发现在固有样本难度和数据不确定性方面表现不佳，做出过于自信的预测。为了解决这个问题，我们提出使用大规模预训练模型以样本难度感知的熵正则化来指导下游模型的训练。预训练模型通过大规模数据集的训练，不会过度拟合下游训练集，使我们能够通过特征空间高斯建模和相对马氏距离的计算来测量每个训练样本的难度。重要的是，通过根据样本的难度自适应地惩罚过于自信的预测，我们同时提高各种具有挑战性的基准测试上的准确性和不确定性校准。

    Large-scale pre-trained models have achieved remarkable success in a variety of scenarios and applications, but how to leverage them to improve the prediction reliability of downstream models is undesirably under-explored. Moreover, modern neural networks have been found to be poorly calibrated and make overconfident predictions regardless of inherent sample difficulty and data uncertainty. To address this issue, we propose to utilize large-scale pre-trained models to guide downstream model training with sample difficulty-aware entropy regularization. Pre-trained models that have been exposed to large-scale datasets and do not overfit the downstream training classes enable us to measure each training sample difficulty via feature-space Gaussian modeling and relative Mahalanobis distance computation. Importantly, by adaptively penalizing overconfident prediction based on the sample's difficulty, we simultaneously improve accuracy and uncertainty calibration on various challenging benchm
    
[^173]: 基于平均二阶相似性的随机分布式优化：算法与分析

    Stochastic Distributed Optimization under Average Second-order Similarity: Algorithms and Analysis. (arXiv:2304.07504v1 [cs.LG])

    [http://arxiv.org/abs/2304.07504](http://arxiv.org/abs/2304.07504)

    本文提出了两种新算法SVRS和AccSVRS，针对分布式优化问题，实现了卓越的通信复杂度。其中，AccSVRS算法实现了完全无平滑性，通信复杂度更是优于现有算法。

    

    本文研究了具有$n$个客户端的有限和分布式优化问题，满足流行的$\delta$-相似性条件和$\mu$-强凸性。我们提出了两种新算法：SVRS和AccSVRS，启发自先前的工作。非加速的SVRS方法结合了梯度滑动和方差缩减技术，实现了卓越的通信复杂度$\tilde{\gO}(n {+} \sqrt{n}\delta/\mu)$，与现有的非加速算法相比有所提高。应用Katyusha X提出的框架，我们还建立了一个名为AccSVRS的直接加速实际版本，其完全无平滑性，通信复杂度为$\tilde{\gO}(n {+} n^{3/4}\sqrt{\delta/\mu})$，在病态情况下优于现有算法。此外，我们展示了一种接近匹配的下界，以验证我们的AccSVRS方法的紧密程度。

    We study finite-sum distributed optimization problems with $n$-clients under popular $\delta$-similarity condition and $\mu$-strong convexity. We propose two new algorithms: SVRS and AccSVRS motivated by previous works. The non-accelerated SVRS method combines the techniques of gradient-sliding and variance reduction, which achieves superior communication complexity $\tilde{\gO}(n {+} \sqrt{n}\delta/\mu)$ compared to existing non-accelerated algorithms. Applying the framework proposed in Katyusha X, we also build a direct accelerated practical version named AccSVRS with totally smoothness-free $\tilde{\gO}(n {+} n^{3/4}\sqrt{\delta/\mu})$ communication complexity that improves upon existing algorithms on ill-conditioning cases. Furthermore, we show a nearly matched lower bound to verify the tightness of our AccSVRS method.
    
[^174]: 面部视频压缩的感知质量评估：基准和有效方法

    Perceptual Quality Assessment of Face Video Compression: A Benchmark and An Effective Method. (arXiv:2304.07056v1 [eess.IV])

    [http://arxiv.org/abs/2304.07056](http://arxiv.org/abs/2304.07056)

    本文介绍了大规模压缩面部视频质量评估（CFVQA）数据库，用于系统地了解面部视频感知质量和多样化压缩失真。生成式编码方法被确定为具有合理的感知码率失真折衷的有前途的替代方法，利用面部视频的统计先验知识。

    

    近年来，对面部视频压缩的需求呈指数级增长，人工智能的成功使得超出了传统的混合视频编码范围。生成式编码方法被确定为具有合理的感知码率失真折衷的有前途的替代方法，利用面部视频的统计先验知识。然而，空间和时间域中扭曲类型的极大多样性，从传统的混合编码框架到生成模型，给压缩面部视频质量评估（VQA）带来了巨大挑战。在本文中，我们介绍了大规模压缩面部视频质量评估（CFVQA）数据库，这是系统地了解面部视频感知质量和多样化压缩失真的第一次尝试。该数据库包含 3,240 个压缩的面部视频片段，涵盖多个压缩级别，这些片段来自 135 个源视频，具有多样性。

    Recent years have witnessed an exponential increase in the demand for face video compression, and the success of artificial intelligence has expanded the boundaries beyond traditional hybrid video coding. Generative coding approaches have been identified as promising alternatives with reasonable perceptual rate-distortion trade-offs, leveraging the statistical priors of face videos. However, the great diversity of distortion types in spatial and temporal domains, ranging from the traditional hybrid coding frameworks to generative models, present grand challenges in compressed face video quality assessment (VQA). In this paper, we introduce the large-scale Compressed Face Video Quality Assessment (CFVQA) database, which is the first attempt to systematically understand the perceptual quality and diversified compression distortions in face videos. The database contains 3,240 compressed face video clips in multiple compression levels, which are derived from 135 source videos with diversif
    
[^175]: 学习演示中的行为空间匹配问题

    Bridging Action Space Mismatch in Learning from Demonstrations. (arXiv:2304.03833v1 [cs.RO])

    [http://arxiv.org/abs/2304.03833](http://arxiv.org/abs/2304.03833)

    本文提出了一种解决学习演示中行为空间不匹配问题的框架，可以通过其他形态有显着不同的代理的演示进行训练，并且可以从次优演示中学习。

    

    学习演示的方法可以通过教师演示达到特定的目的，但是当教师用于演示的行为空间与学生不同时，就会出现行为空间不匹配的问题。本文提出了一种框架，即模仿学习中的形态适应（MAIL），旨在帮助学生代理根据其他形态显着不同的代理的演示进行训练。 MAIL可以从次优演示中学习，只要这些演示提供了一些指引，以达到预期的解决方案。作者在具有挑战性的家庭布料操作任务上演示了MAIL，并介绍了一个新的DRY CLOTH任务 - 在三维空间中带有障碍物的布料操作任务。

    Learning from demonstrations (LfD) methods guide learning agents to a desired solution using demonstrations from a teacher. While some LfD methods can handle small mismatches in the action spaces of the teacher and student, here we address the case where the teacher demonstrates the task in an action space that can be substantially different from that of the student -- thereby inducing a large action space mismatch. We bridge this gap with a framework, Morphological Adaptation in Imitation Learning (MAIL), that allows training an agent from demonstrations by other agents with significantly different morphologies (from the student or each other). MAIL is able to learn from suboptimal demonstrations, so long as they provide some guidance towards a desired solution. We demonstrate MAIL on challenging household cloth manipulation tasks and introduce a new DRY CLOTH task -- cloth manipulation in 3D task with obstacles. In these tasks, we train a visual control policy for a robot with one en
    
[^176]: 可靠的学习方法应对测试时攻击与分布偏移

    Reliable Learning for Test-time Attacks and Distribution Shift. (arXiv:2304.03370v1 [cs.LG])

    [http://arxiv.org/abs/2304.03370](http://arxiv.org/abs/2304.03370)

    本文提出了可靠的学习方法以抵御测试时攻击和分布偏移，在测试时引入了新的可靠性保障方法，确保预测结果正确。同时，该学习方法能够适应任意测试点，具有非常好的可靠性。

    

    机器学习算法经常被用于即使经过精心获得的训练数据也无法准确捕捉的环境中，这既可能是由于测试时的“对抗性”攻击，也可能是因为“自然”的数据分布偏移。针对测试时攻击，我们提出并分析一种新颖的稳健性可靠性保证方法，要求学习器输出一个可靠半径 $\eta$ 的预测结果，意味着只要对手没有扰动测试点超过距离 $\eta$，它的预测结果就是正确的。我们提供了在任意测试点上都能输出最佳可靠性半径的最优学习器，并且特征化了可靠区域即可达到给定可靠性半径的点集。我们还分析了在分布偏移下的可靠学习方法，其中测试点可能来自于一个与训练分布不同的任意分布 $Q$。

    Machine learning algorithms are often used in environments which are not captured accurately even by the most carefully obtained training data, either due to the possibility of `adversarial' test-time attacks, or on account of `natural' distribution shift. For test-time attacks, we introduce and analyze a novel robust reliability guarantee, which requires a learner to output predictions along with a reliability radius $\eta$, with the meaning that its prediction is guaranteed to be correct as long as the adversary has not perturbed the test point farther than a distance $\eta$. We provide learners that are optimal in the sense that they always output the best possible reliability radius on any test point, and we characterize the reliable region, i.e. the set of points where a given reliability radius is attainable. We additionally analyze reliable learners under distribution shift, where the test points may come from an arbitrary distribution Q different from the training distribution 
    
[^177]: 使AI“口渴”减少的方法：揭示和解决AI模型的秘密水消耗

    Making AI Less "Thirsty": Uncovering and Addressing the Secret Water Footprint of AI Models. (arXiv:2304.03271v1 [cs.LG])

    [http://arxiv.org/abs/2304.03271](http://arxiv.org/abs/2304.03271)

    本论文揭示以及提出了解决人工智能模型巨大水足迹的方法，因为其淡水消耗已经引起国际社会的重视，并且AI模型应该承担社会责任，做出面对水危机的表率。

    

    人工智能（AI）模型的碳足迹不断增长，特别是像GPT-3和GPT-4这样的大型模型，已经受到公众的关注。然而，同等重要且巨大的AI模型水印尚未引起人们的注意。例如，在微软最先进的美国数据中心中训练GPT-3可以直接消耗70万升清洁淡水（相当于生产370辆宝马汽车或320辆特斯拉电动汽车），如果在微软的亚洲数据中心进行训练，这个水消耗量将增加三倍，但这样的信息一直被保密。这极其令人担忧，因为淡水短缺已成为在人口迅速增长、水资源减少和老化的水基础设施的背景下，我们所有人面临的最紧迫的挑战之一。为了应对全球水资源的挑战，人工智能模型可以，而且应该，承担社会责任，以身作则解决自己的问题。

    The growing carbon footprint of artificial intelligence (AI) models, especially large ones such as GPT-3 and GPT-4, has been undergoing public scrutiny. Unfortunately, however, the equally important and enormous water footprint of AI models has remained under the radar. For example, training GPT-3 in Microsoft's state-of-the-art U.S. data centers can directly consume 700,000 liters of clean freshwater (enough for producing 370 BMW cars or 320 Tesla electric vehicles) and the water consumption would have been tripled if training were done in Microsoft's Asian data centers, but such information has been kept as a secret. This is extremely concerning, as freshwater scarcity has become one of the most pressing challenges shared by all of us in the wake of the rapidly growing population, depleting water resources, and aging water infrastructures. To respond to the global water challenges, AI models can, and also should, take social responsibility and lead by example by addressing their own 
    
[^178]: 对数凹采样的查询下界

    Query lower bounds for log-concave sampling. (arXiv:2304.02599v1 [math.ST])

    [http://arxiv.org/abs/2304.02599](http://arxiv.org/abs/2304.02599)

    该论文研究了对数凹采样的查询下界，在强对数凹和对数光滑分布中采样需要 $\Omega(\log \kappa)$ 查询，在采样高斯分布中需要 $\widetilde \Omega(\min(\sqrt\kappa \log d, d))$ 查询。

    

    最近几年，对数凹采样在算法方面取得了显著的进展，但相应的证明此任务的下界的问题仍然很难，以前只知道在一维中存在较小的下界。在这项工作中，我们建立了以下查询下界：（1）在维度 $d\ge 2$中从强对数凹和对数光滑分布中采样需要 $\Omega(\log \kappa)$ 查询，这在任何固定维度上都是最优的，（2）从高斯分布中采样需要 $\widetilde \Omega(\min(\sqrt\kappa \log d, d))$ 查询（因此也适用于在维数 $d$ 中采样一般的对数凹和光滑分布），这对于高斯类几乎是最优的。这里 $\kappa$ 是目标分布的条件数。我们的证明依赖于（1）一种多尺度构造，受到了关于谐振分析中的Kakeya猜想的工作的启发，以及（2）一种新颖的约简，证明了块Krylov算法在此问题中是最佳的。

    Log-concave sampling has witnessed remarkable algorithmic advances in recent years, but the corresponding problem of proving lower bounds for this task has remained elusive, with lower bounds previously known only in dimension one. In this work, we establish the following query lower bounds: (1) sampling from strongly log-concave and log-smooth distributions in dimension $d\ge 2$ requires $\Omega(\log \kappa)$ queries, which is sharp in any constant dimension, and (2) sampling from Gaussians in dimension $d$ (hence also from general log-concave and log-smooth distributions in dimension $d$) requires $\widetilde \Omega(\min(\sqrt\kappa \log d, d))$ queries, which is nearly sharp for the class of Gaussians. Here $\kappa$ denotes the condition number of the target distribution. Our proofs rely upon (1) a multiscale construction inspired by work on the Kakeya conjecture in harmonic analysis, and (2) a novel reduction that demonstrates that block Krylov algorithms are optimal for this probl
    
[^179]: 《EduceLab-Scrolls：利用X射线CT从赫库兰尼姆纸草卷中可验证地恢复文本》

    EduceLab-Scrolls: Verifiable Recovery of Text from Herculaneum Papyri using X-ray CT. (arXiv:2304.02084v1 [cs.CV])

    [http://arxiv.org/abs/2304.02084](http://arxiv.org/abs/2304.02084)

    该论文介绍了使用X射线CT图像揭示赫库兰尼姆纸草卷隐藏文本的软件管道和数据集。他们运用了机器学习和几何框架的方法检测“不可见”的碳墨，达到了人类专家标记者难以达到的效果。

    

    我们提出了一个用于揭示赫库兰尼姆纸草卷隐藏文本的完整软件管道。这个增强的虚拟展开流水线将机器学习与一种新颖的几何框架相结合，将三维和二维图像联系起来。我们还提出了EduceLab-Scrolls，这是一个全面的开放数据集，代表了二十年来对这个问题的研究努力。EduceLab-Scrolls包含了一组小碎片和完整卷轴的体积X射线CT图像。该数据集还包含用于监督训练油墨检测模型的二维图像标签。通过将卷轴碎片的频谱照片与相同碎片的X射线CT图像对准，从而创建了一个可机器学习的图像空间和模态之间的映射。这种对准允许有监督地学习检测X射线CT中“隐形”碳墨的任务，即使对于人类专家标记者来说也是“不可能”的任务。据我们所知，这是第一个对齐数据集。

    We present a complete software pipeline for revealing the hidden texts of the Herculaneum papyri using X-ray CT images. This enhanced virtual unwrapping pipeline combines machine learning with a novel geometric framework linking 3D and 2D images. We also present EduceLab-Scrolls, a comprehensive open dataset representing two decades of research effort on this problem. EduceLab-Scrolls contains a set of volumetric X-ray CT images of both small fragments and intact, rolled scrolls. The dataset also contains 2D image labels that are used in the supervised training of an ink detection model. Labeling is enabled by aligning spectral photography of scroll fragments with X-ray CT images of the same fragments, thus creating a machine-learnable mapping between image spaces and modalities. This alignment permits supervised learning for the detection of "invisible" carbon ink in X-ray CT, a task that is "impossible" even for human expert labelers. To our knowledge, this is the first aligned datas
    
[^180]: 基于扩散映射的粒子系统用于生成模型

    Diffusion map particle systems for generative modeling. (arXiv:2304.00200v1 [stat.ML])

    [http://arxiv.org/abs/2304.00200](http://arxiv.org/abs/2304.00200)

    本文提出一种新型扩散映射粒子系统(DMPS)，可以用于高效生成建模，实验表明在包含流形结构的合成数据集上取得了比其他方法更好的效果。

    

    本文提出了一种新颖的扩散映射粒子系统(DMPS)，用于生成建模，该方法基于扩散映射和Laplacian调整的Wasserstein梯度下降（LAWGD）。扩散映射被用来从样本中近似Langevin扩散过程的生成器，从而学习潜在的数据生成流形。另一方面，LAWGD能够在合适的核函数选择下高效地从目标分布中抽样，我们在这里通过扩散映射计算生成器的谱逼近来构造核函数。数值实验表明，我们的方法在包括具有流形结构的合成数据集上优于其他方法。

    We propose a novel diffusion map particle system (DMPS) for generative modeling, based on diffusion maps and Laplacian-adjusted Wasserstein gradient descent (LAWGD). Diffusion maps are used to approximate the generator of the Langevin diffusion process from samples, and hence to learn the underlying data-generating manifold. On the other hand, LAWGD enables efficient sampling from the target distribution given a suitable choice of kernel, which we construct here via a spectral approximation of the generator, computed with diffusion maps. Numerical experiments show that our method outperforms others on synthetic datasets, including examples with manifold structure.
    
[^181]: Fides：一种利用安全执行环境对机器学习工作负载进行结果验证的生成框架

    Fides: A Generative Framework for Result Validation of Outsourced Machine Learning Workloads via TEE. (arXiv:2304.00083v1 [cs.CR])

    [http://arxiv.org/abs/2304.00083](http://arxiv.org/abs/2304.00083)

    本文提出了一种名为Fides的框架，用于实时验证外协的机器学习工作负载。该框架采用贪心蒸馏迁移学习技术，可动态蒸馏并优化验证模型，以验证对应的服务模型。此外，该框架还能在可信执行环境中运行，以提供更高的安全性和隐私性保证。

    

    机器学习在敏感领域的部署导致了对其安全性和隐私性的重视，现有解决方案，如多方计算和基于证明的系统，给实时应用带来了很大的计算开销。本文提出了一个名为Fides的框架，用于实时验证外协的机器学习工作负载，其中采用新颖且高效的贪心蒸馏迁移学习技术，实现一种实时验证模型来较少地消耗空间和计算能力，同时运行在可信执行环境中。

    The growing popularity of Machine Learning (ML) has led to its deployment in various sensitive domains, which has resulted in significant research focused on ML security and privacy. However, in some applications, such as autonomous driving, integrity verification of the outsourced ML workload is more critical-a facet that has not received much attention. Existing solutions, such as multi-party computation and proof-based systems, impose significant computation overhead, which makes them unfit for real-time applications. We propose Fides, a novel framework for real-time validation of outsourced ML workloads. Fides features a novel and efficient distillation technique-Greedy Distillation Transfer Learning-that dynamically distills and fine-tunes a space and compute-efficient verification model for verifying the corresponding service model while running inside a trusted execution environment. Fides features a client-side attack detection model that uses statistical analysis and divergenc
    
[^182]: 基于自适应细化和康托洛维奇度量的数据驱动抽象（扩展版）

    Data-driven abstractions via adaptive refinements and a Kantorovich metric [extended version]. (arXiv:2303.17618v1 [cs.LG])

    [http://arxiv.org/abs/2303.17618](http://arxiv.org/abs/2303.17618)

    我们提出了一种基于自适应细化和康托洛维奇度量的智能且可扩展的动态系统抽象技术，并且定义了一种马尔可夫链之间的度量用作损失函数。我们的方法具有更好的计算复杂度。

    

    我们介绍了一种智能且可扩展的动态系统抽象自适应细化技术。我们的技术依赖于根据未来输出的观察将状态空间划分。然而，这种知识是动态地以不对称的方式构建的。为了学习最优结构，我们定义了马尔可夫链之间的康托洛维奇度量，并将其用作损失函数。我们的技术适用于数据驱动的框架，但不受限于此。我们还研究了马尔可夫链之间上述度量的性质，我们认为这可能具有更广泛的应用。我们提出了一种近似计算该度量的算法，并且我们展示了我们的方法比使用传统的线性规划技术具有更好的计算复杂度。

    We introduce an adaptive refinement procedure for smart, and scalable abstraction of dynamical systems. Our technique relies on partitioning the state space depending on the observation of future outputs. However, this knowledge is dynamically constructed in an adaptive, asymmetric way. In order to learn the optimal structure, we define a Kantorovich-inspired metric between Markov chains, and we use it as a loss function. Our technique is prone to data-driven frameworks, but not restricted to.  We also study properties of the above mentioned metric between Markov chains, which we believe could be of application for wider purpose. We propose an algorithm to approximate it, and we show that our method yields a much better computational complexity than using classical linear programming techniques.
    
[^183]: Pgx:强化学习硬件加速的并行游戏模拟器

    Pgx: Hardware-accelerated parallel game simulation for reinforcement learning. (arXiv:2303.17503v1 [cs.AI])

    [http://arxiv.org/abs/2303.17503](http://arxiv.org/abs/2303.17503)

    Pgx是一个用JAX编写的游戏模拟器集合，具有强化学习硬件加速能力，支持并行执行，速度比现有的强化学习库快10倍。 它实现了Backgammon，Shogi和Go等基准测试游戏。

    

    我们提出了Pgx，这是一个用JAX编写的棋盘游戏模拟器集合。由于JAX的自动向量化和即时编译功能，Pgx易于在GPU/TPU加速器上进行大规模并行执行。我们发现，在单个A100 GPU上的Pgx模拟比现有的强化学习库快10倍。Pgx实现了被认为是人工智能研究中至关重要的基准测试的游戏，如Backgammon，Shogi和Go。 Pgx可在https://github.com/sotetsuk/pgx获得。

    We propose Pgx, a collection of board game simulators written in JAX. Thanks to auto-vectorization and Just-In-Time compilation of JAX, Pgx scales easily to thousands of parallel execution on GPU/TPU accelerators. We found that the simulation of Pgx on a single A100 GPU is 10x faster than that of existing reinforcement learning libraries. Pgx implements games considered vital benchmarks in artificial intelligence research, such as Backgammon, Shogi, and Go. Pgx is available at https://github.com/sotetsuk/pgx.
    
[^184]: 超越负采样的高效分布式表示方法

    Efficient distributed representations beyond negative sampling. (arXiv:2303.17475v1 [cs.LG])

    [http://arxiv.org/abs/2303.17475](http://arxiv.org/abs/2303.17475)

    本文介绍了一种高效的分布式表示（嵌入）学习方法，通过线性时间估计softmax归一化常数来实现学习过程，该方法优于负采样方法并在多项测试中验证了其有效性。

    

    本文介绍了一种高效的学习分布式表示（也称为嵌入）的方法。该方法通过最小化一个类似于Word2Vec算法中引入并在多个工作中采用的目标函数来实现。优化计算的瓶颈是softmax归一化常数的计算，这需要与样本大小呈二次比例的操作数。这种复杂度不适用于大型数据集，所以负采样是一个常见的解决方法，可以在与样本大小线性相关的时间内获得分布式表示。然而，负采样会改变损失函数，因此解决的是与最初提出的不同的优化问题。我们的贡献在于展示如何通过线性时间估计softmax归一化常数，从而设计了一种有效的优化策略来学习分布式表示。我们使用不同的数据集进行测试，并展示了我们的方法在嵌入质量和训练时间方面优于负采样。

    This article describes an efficient method to learn distributed representations, also known as embeddings. This is accomplished minimizing an objective function similar to the one introduced in the Word2Vec algorithm and later adopted in several works. The optimization computational bottleneck is the calculation of the softmax normalization constants for which a number of operations scaling quadratically with the sample size is required. This complexity is unsuited for large datasets and negative sampling is a popular workaround, allowing one to obtain distributed representations in linear time with respect to the sample size. Negative sampling consists, however, in a change of the loss function and hence solves a different optimization problem from the one originally proposed. Our contribution is to show that the sotfmax normalization constants can be estimated in linear time, allowing us to design an efficient optimization strategy to learn distributed representations. We test our ap
    
[^185]: 浅层复值神经网络对$C^k$-函数的最优逼近

    Optimal approximation of $C^k$-functions using shallow complex-valued neural networks. (arXiv:2303.16813v1 [math.FA])

    [http://arxiv.org/abs/2303.16813](http://arxiv.org/abs/2303.16813)

    本文证明了对于$C^k$（在实变量意义下）的函数，使用具有单层隐藏层和$m$个神经元的神经网络可以以错误率$m^{-k/(2n)}$将其逼近。此外，如果选取权值$\sigma_j,b_j\in\mathbb{C}$和$\rho_j\in\mathbb{C}^n$对$f$连续，那么获得的逼近速率是最优的。

    

    本文证明了使用浅层复值神经网络对复立方体上$C^k$（在实变量意义下）的函数进行逼近的量化结果。具体而言，我们考虑具有单层隐藏层和$m$个神经元的神经网络，即形如$z \mapsto \sum_{j=1}^m \sigma_j \cdot \phi\big(\rho_j^T z + b_j\big)$的网络，并且证明了可以使用这种形式的函数逼近$C^k \left(\Omega_n;\mathbb{C}\right)$中的任何函数，当$m\to\infty$时误差为$m^{-k/(2n)}$.此外，我们还证明选取权值$\sigma_j,b_j\in\mathbb{C}$和$\rho_j\in\mathbb{C}^n$对$f$连续并且在这种连续性假设下获得的逼近速率是最优的。

    We prove a quantitative result for the approximation of functions of regularity $C^k$ (in the sense of real variables) defined on the complex cube $\Omega_n := [-1,1]^n +i[-1,1]^n\subseteq \mathbb{C}^n$ using shallow complex-valued neural networks. Precisely, we consider neural networks with a single hidden layer and $m$ neurons, i.e., networks of the form $z \mapsto \sum_{j=1}^m \sigma_j \cdot \phi\big(\rho_j^T z + b_j\big)$ and show that one can approximate every function in $C^k \left( \Omega_n; \mathbb{C}\right)$ using a function of that form with error of the order $m^{-k/(2n)}$ as $m \to \infty$, provided that the activation function $\phi: \mathbb{C} \to \mathbb{C}$ is smooth but not polyharmonic on some non-empty open set. Furthermore, we show that the selection of the weights $\sigma_j, b_j \in \mathbb{C}$ and $\rho_j \in \mathbb{C}^n$ is continuous with respect to $f$ and prove that the derived rate of approximation is optimal under this continuity assumption. We also discuss
    
[^186]: 针对非线性部分可观测系统的局部线性化概率逆优化控制方法

    Probabilistic inverse optimal control with local linearization for non-linear partially observable systems. (arXiv:2303.16698v1 [cs.LG])

    [http://arxiv.org/abs/2303.16698](http://arxiv.org/abs/2303.16698)

    本文介绍了一种针对非线性部分可观测系统的局部线性化概率逆优化控制方法，可用于特征化顺序决策任务中的行为，并且具有广泛的适用性。

    

    逆优化控制方法可以用于特征化顺序决策任务中的行为。然而，大多数现有的工作要求已知控制信号，或者仅限于完全可观测或线性系统。本文介绍了一种概率逆优化控制方法，用于非线性随机系统的丢失控制信号和部分可观测性，该方法统一了现有方法。通过使用代理的感觉和控制系统的噪声特征的显式模型以及局部线性化技术，我们推导出了模型参数的近似似然函数，可以在单个正向传递中计算。我们在随机和部分可观测版本的经典控制任务，导航任务和手动达到任务上评估了我们提出的方法。该方法具有广泛的适用性，可用于模仿学习到感觉运动神经科学。

    Inverse optimal control methods can be used to characterize behavior in sequential decision-making tasks. Most existing work, however, requires the control signals to be known, or is limited to fully-observable or linear systems. This paper introduces a probabilistic approach to inverse optimal control for stochastic non-linear systems with missing control signals and partial observability that unifies existing approaches. By using an explicit model of the noise characteristics of the sensory and control systems of the agent in conjunction with local linearization techniques, we derive an approximate likelihood for the model parameters, which can be computed within a single forward pass. We evaluate our proposed method on stochastic and partially observable version of classic control tasks, a navigation task, and a manual reaching task. The proposed method has broad applicability, ranging from imitation learning to sensorimotor neuroscience.
    
[^187]: PDExplain：PDEs 在实际应用中的情境建模

    PDExplain: Contextual Modeling of PDEs in the Wild. (arXiv:2303.15827v1 [cs.LG])

    [http://arxiv.org/abs/2303.15827](http://arxiv.org/abs/2303.15827)

    我们提出了PDExplain，一种解释性的方法来解决偏微分方程。该算法能够通过提供少量样本的方式，预测未来时间步的PDE解，极大地协助了建立物理科学中基于数据的现象建模。

    

    我们提出了一种解释性的方法PDExplain用于解决偏微分方程。在训练阶段，我们的方法通过一个操作员定义的PDE家族的数据以及这个家族的一般形式进行馈送。在推断阶段，提供了一个从现象中收集到的最小样本，其中样本与 PDE 家族相关，但不一定属于训练阶段看到的具体 PDE 集合。我们展示了算法如何预测未来时间步的PDE解。此外，我们的方法提供了PDE的可解释形式，这种特征可以协助通过物理科学数据来对现象进行建模。为了验证我们的方法，我们进行了大量实验，考察了其在预测误差和可解释性方面的质量。

    We propose an explainable method for solving Partial Differential Equations by using a contextual scheme called PDExplain. During the training phase, our method is fed with data collected from an operator-defined family of PDEs accompanied by the general form of this family. In the inference phase, a minimal sample collected from a phenomenon is provided, where the sample is related to the PDE family but not necessarily to the set of specific PDEs seen in the training phase. We show how our algorithm can predict the PDE solution for future timesteps. Moreover, our method provides an explainable form of the PDE, a trait that can assist in modelling phenomena based on data in physical sciences. To verify our method, we conduct extensive experimentation, examining its quality both in terms of prediction error and explainability.
    
[^188]: 神经网络缩放的量化模型

    The Quantization Model of Neural Scaling. (arXiv:2303.13506v1 [cs.LG])

    [http://arxiv.org/abs/2303.13506](http://arxiv.org/abs/2303.13506)

    该论文提出了神经网络缩放的量化模型，通过将神经网络学习到的功能分为量子并对量子进行递减使用频率的顺序学习，可以解释损失缩放定律，并自动发现语言模型的多样能力。

    

    我们提出了神经网络缩放定律的量化模型，解释了观察到的损失函数随着模型和数据规模的幂律下降以及随着规模的增加出现新能力的突然突破。我们从所谓的“量化假设”中推导出这个模型，其中学习到的神经网络功能被量化为离散块（“量子”）。我们在降序学习频率中学习量子，并表明当量子被以递减使用频率的顺序学习时，在使用频率中使用幂律可以解释观察到的损失缩放定律。我们在玩具数据集上验证了这个预测，然后研究了大型语言模型的缩放曲线如何分解。使用语言模型的内部，我们自动发现多样的模型能力（量子），并发现对应子问题的分布与我们理论预测的神经缩放指数产生了兼容性证据。

    We propose the $\textit{Quantization Model}$ of neural scaling laws, explaining both the observed power law dropoff of loss with model and data size, and also the sudden emergence of new capabilities with scale. We derive this model from what we call the $\textit{Quantization Hypothesis}$, where learned network capabilities are quantized into discrete chunks ($\textit{quanta}$). We show that when quanta are learned in order of decreasing use frequency, then a power law in use frequencies explains observed power law scaling of loss. We validate this prediction on toy datasets, then study how scaling curves decompose for large language models. Using language model internals, we auto-discover diverse model capabilities (quanta) and find tentative evidence that the distribution over corresponding subproblems in the prediction of natural text is compatible with the power law predicted from the neural scaling exponent as predicted from our theory.
    
[^189]: 基于对象中心的槽扩散

    Object-Centric Slot Diffusion. (arXiv:2303.10834v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.10834](http://arxiv.org/abs/2303.10834)

    基于对象中心的槽扩散是一种将扩散模型整合到对象中心学习的新颖方法，它使用Latent Slot Diffusion (LSD)模型替换了传统的槽解码器，同时也是第一个能够无监督进行组合条件扩散的模型。

    

    最近基于Transformer的图像生成模型在处理复杂场景的对象中心学习中取得了成功，这突显了强大的图像生成器的重要性。然而，尽管扩散模型在图像生成方面具有较高的表达能力，但它们在对象中心学习中的整合在这个领域中仍然较少探索。在本文中，我们探讨了将扩散模型整合到对象中心学习中的可行性和潜力，并研究了这种方法的优点和缺点。我们引入了Latent Slot Diffusion (LSD)，这是一种新颖的模型，它具有两个目标：首先，它是第一个将传统的槽解码器替换为以对象槽为条件的潜在扩散模型的对象中心学习模型；其次，它也是第一个不需要像文本这样的监督注释而能够无监督地进行组合条件扩散的模型。通过对各种对象中心任务的实验，包括首次在FFHQ数据集中的应用。

    The recent success of transformer-based image generative models in object-centric learning highlights the importance of powerful image generators for handling complex scenes. However, despite the high expressiveness of diffusion models in image generation, their integration into object-centric learning remains largely unexplored in this domain. In this paper, we explore the feasibility and potential of integrating diffusion models into object-centric learning and investigate the pros and cons of this approach. We introduce Latent Slot Diffusion (LSD), a novel model that serves dual purposes: it is the first object-centric learning model to replace conventional slot decoders with a latent diffusion model conditioned on object slots, and it is also the first unsupervised compositional conditional diffusion model that operates without the need for supervised annotations like text. Through experiments on various object-centric tasks, including the first application of the FFHQ dataset in t
    
[^190]: 零和马尔可夫博弈中强化学习的新政策迭代算法

    A New Policy Iteration Algorithm For Reinforcement Learning in Zero-Sum Markov Games. (arXiv:2303.09716v1 [cs.LG])

    [http://arxiv.org/abs/2303.09716](http://arxiv.org/abs/2303.09716)

    本文提出了一种适用于零和马尔可夫博弈的简单但有效的策略迭代算法。

    

    许多基于模型的强化学习算法可以被视为具有两个阶段: 学习阶段和规划阶段。在标准MDPs情况下，可以使用价值迭代或策略迭代来解决学习问题。但在零和马尔可夫博弈的情况下，没有有效的策略迭代算法，以前的尝试都有局限性。本文提出了一种简单的策略迭代变体，能够有效地解决这个问题。

    Many model-based reinforcement learning (RL) algorithms can be viewed as having two phases that are iteratively implemented: a learning phase where the model is approximately learned and a planning phase where the learned model is used to derive a policy. In the case of standard MDPs, the learning problem can be solved using either value iteration or policy iteration. However, in the case of zero-sum Markov games, there is no efficient policy iteration algorithm; e.g., it has been shown in Hansen et al. (2013) that one has to solve Omega(1/(1-alpha)) MDPs, where alpha is the discount factor, to implement the only known convergent version of policy iteration. Another algorithm for Markov zero-sum games, called naive policy iteration, is easy to implement but is only provably convergent under very restrictive assumptions. Prior attempts to fix naive policy iteration algorithm have several limitations. Here, we show that a simple variant of naive policy iteration for games converges, and 
    
[^191]: 通过损失检查在目标检测数据集中识别标签错误

    Identifying Label Errors in Object Detection Datasets by Loss Inspection. (arXiv:2303.06999v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.06999](http://arxiv.org/abs/2303.06999)

    本研究首次引入了一个用于目标检测数据集上标签错误检测的基准以及一种标签错误检测方法和几种基线方法。研究模拟了四种不同类型的随机引入的标签错误，并提出了一种通过损失检查来检测这些错误的方法。

    

    为监督目标检测的标签数据集进行标注是一个枯燥且耗时的任务。在注释过程中很容易引入错误，并且在审核过程中可能会被忽视，导致准确度低下的基准和基于噪声标签训练的深度神经网络性能降低。在本研究中，我们首次引入了一个用于目标检测数据集上的标签错误检测方法的基准以及一个标签错误检测方法和一些基线方法。我们在已经标记良好的目标检测数据集的训练集和测试集中模拟了四种不同类型的随机引入的标签错误。对于我们的标签错误检测方法，我们假设已经提供了一个两阶段目标检测器，并考虑两个阶段的分类损失和回归损失的总和。这些损失基于预测和包括模拟标签错误的噪声标签进行计算，旨在检测后者。我们将我们的方法与三个基线进行了比较：一个无深度学习的朴素方法，目标检测器的...

    Labeling datasets for supervised object detection is a dull and time-consuming task. Errors can be easily introduced during annotation and overlooked during review, yielding inaccurate benchmarks and performance degradation of deep neural networks trained on noisy labels. In this work, we for the first time introduce a benchmark for label error detection methods on object detection datasets as well as a label error detection method and a number of baselines. We simulate four different types of randomly introduced label errors on train and test sets of well-labeled object detection datasets. For our label error detection method we assume a two-stage object detector to be given and consider the sum of both stages' classification and regression losses. The losses are computed with respect to the predictions and the noisy labels including simulated label errors, aiming at detecting the latter. We compare our method to three baselines: a naive one without deep learning, the object detector'
    
[^192]: 基于Transformer的符号回归规划

    Transformer-based Planning for Symbolic Regression. (arXiv:2303.06833v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06833](http://arxiv.org/abs/2303.06833)

    该论文提出了一种基于Transformer和蒙特卡罗树搜索的符号回归规划策略TPSR，可以将非可微的反馈作为知识的外部来源融入到方程生成过程中，提高方程生成的准确性和复杂性。

    

    符号回归是机器学习中一项具有挑战性的任务，它涉及基于函数值查找其数学表达式。最近，符号回归的一些进展表明，预训练的基于Transformer的模型对于生成方程序列是有效的，这些模型从合成数据集的大规模预训练中获益，并在推理时间方面比基于GP的方法具有显著优势。然而，这些模型关注的是借鉴文本生成的监督预训练目标，而忽略了方程的特定目标，如准确性和复杂性。为了解决这个问题，我们提出了TPSR，一种基于Transformer的符号回归规划策略，将蒙特卡罗树搜索融入到Transformer解码过程中。与传统的解码策略不同，TPSR允许将非可微的反馈（如拟合准确性和复杂性）作为知识的外部来源融入到方程生成过程中。

    Symbolic regression (SR) is a challenging task in machine learning that involves finding a mathematical expression for a function based on its values. Recent advancements in SR have demonstrated the efficacy of pretrained transformer-based models for generating equations as sequences, which benefit from large-scale pretraining on synthetic datasets and offer considerable advantages over GP-based methods in terms of inference time. However, these models focus on supervised pretraining goals borrowed from text generation and ignore equation-specific objectives like accuracy and complexity. To address this, we propose TPSR, a Transformer-based Planning strategy for Symbolic Regression that incorporates Monte Carlo Tree Search into the transformer decoding process. TPSR, as opposed to conventional decoding strategies, allows for the integration of non-differentiable feedback, such as fitting accuracy and complexity, as external sources of knowledge into the equation generation process. Ext
    
[^193]: 使用最小生成树进行聚类：能有多好？

    Clustering with minimum spanning trees: How good can it be?. (arXiv:2303.05679v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.05679](http://arxiv.org/abs/2303.05679)

    本文研究了使用最小生成树（MST）进行分区数据聚类任务的意义程度，并发现MST方法在总体上具有很强的竞争力。此外，通过回顾、研究、扩展和推广现有的MST-based划分方案，我们提出了一些新的和值得注意的方法。总体上，Genie和信息论方法往往优于其他非MST算法，在某些情况下MST方法可能不如其他算法。

    

    最小生成树（MST）在许多模式识别任务中可以提供方便的数据集表示，并且计算相对较快。本文中，我们量化了MST在低维空间的分区数据聚类任务中的意义程度。通过识别最佳（oracle）算法与大量基准数据的专家标签之间的一致性上限，我们发现MST方法在总体上具有很强的竞争力。接下来，我们不是提出另一个只在有限的示例上表现良好的算法，而是回顾、研究、扩展和推广现有的最新MST-based划分方案。这导致了一些新的和值得注意的方法。总体上，Genie和信息论方法往往优于非MST算法，如k-means，高斯混合，谱聚类，Birch，基于密度和经典层次聚类程序。尽管如此，我们还是发现MST方法在某些情况下可能不如其他算法。

    Minimum spanning trees (MSTs) provide a convenient representation of datasets in numerous pattern recognition activities. Moreover, they are relatively fast to compute. In this paper, we quantify the extent to which they can be meaningful in partitional data clustering tasks in low-dimensional spaces. By identifying the upper bounds for the agreement between the best (oracle) algorithm and the expert labels from a large battery of benchmark data, we discover that MST methods are overall very competitive. Next, instead of proposing yet another algorithm that performs well on a limited set of examples, we review, study, extend, and generalise existing, state-of-the-art MST-based partitioning schemes. This leads to a few new and noteworthy approaches. Overall, Genie and the information-theoretic methods often outperform the non-MST algorithms such as k-means, Gaussian mixtures, spectral clustering, Birch, density-based, and classical hierarchical agglomerative procedures. Nevertheless, we
    
[^194]: 基于自适应相关图卷积网络，实现交通量估计更好的性能：解决不确定和非平衡问题

    Towards better traffic volume estimation: Tackling both underdetermined and non-equilibrium problems via a correlation-adaptive graph convolution network. (arXiv:2303.05660v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.05660](http://arxiv.org/abs/2303.05660)

    本研究提出基于图卷积网络的方法，解决交通量估计中的不确定和非平衡问题，实现准确的全面交通量估计。

    

    交通量是交通管理和控制提供细粒度信息不可或缺的因素。然而，由于交通传感器的有限部署，获取全面的交通量信息并不容易。现有研究主要集中在提高特定方法的整体估计准确性上，忽略了交通量估计的基本挑战，因此在一些关键任务上表现较差。本文研究了交通量估计中的两个关键问题: (1) 由未检测到的行动引起的不确定交通流，以及 (2) 由拥堵传播引起的非平衡交通流。我们提出了一种基于图形的深度学习方法，可以提供数据驱动的、无模型的和相关自适应方法来解决上述问题，并进行准确的全面交通量估计。特别地，为了量化交通速度和流量之间的动态和非线性关系，本文介绍了用于建立交通流图的相关图卷积网络。

    Traffic volume is an indispensable ingredient to provide fine-grained information for traffic management and control. However, due to limited deployment of traffic sensors, obtaining full-scale volume information is far from easy. Existing works on this topic primarily focus on improving the overall estimation accuracy of a particular method and ignore the underlying challenges of volume estimation, thereby having inferior performances on some critical tasks. This paper studies two key problems with regard to traffic volume estimation: (1) underdetermined traffic flows caused by undetected movements, and (2) non-equilibrium traffic flows arise from congestion propagation. Here we demonstrate a graph-based deep learning method that can offer a data-driven, model-free and correlation adaptive approach to tackle the above issues and perform accurate network-wide traffic volume estimation. Particularly, in order to quantify the dynamic and nonlinear relationships between traffic speed and 
    
[^195]: Cal-QL: 计算机辅助脱机强化学习预先训练用于高效在线微调

    Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning. (arXiv:2303.05479v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.05479](http://arxiv.org/abs/2303.05479)

    本文介绍了一种计算机辅助脱机强化学习方法Cal-QL，该方法能够从脱机数据中学习一个保守的值函数初始化，使得在在线微调时同时保障了快速和性能。

    

    脱机强化学习方法可以用来从现有数据集中获取策略初始化并通过有限互动进行快速在线微调。然而，现有的脱机强化学习方法在在线微调中表现较差。本文研究了保守脱机强化学习方法中的微调问题，并设计了一种方法，可以从脱机数据中学习到有效的初始化，并使其具备快速的在线微调功能。我们的方法，即Cal-QL，通过学习一个保守的值函数初始化，低估从脱机数据中学到的策略的价值，同时确保学习到的Q值在合理的范围内。实验结果表明，我们的方法在多个基准环境中具有显著的性能优势，并且也能在真实机器人问题中进行有效的在线微调。

    A compelling use case of offline reinforcement learning (RL) is to obtain a policy initialization from existing datasets followed by fast online fine-tuning with limited interaction. However, existing offline RL methods tend to behave poorly during fine-tuning. In this paper, we study the fine-tuning problem in the context of conservative offline RL methods and we devise an approach for learning an effective initialization from offline data that also enables fast online fine-tuning capabilities. Our approach, calibrated Q-learning (Cal-QL), accomplishes this by learning a conservative value function initialization that underestimates the value of the learned policy from offline data, while also ensuring that the learned Q-values are at a reasonable scale. We refer to this property as calibration, and define it formally as providing a lower bound on the true value function of the learned policy and an upper bound on the value of some other (suboptimal) reference policy, which may simply
    
[^196]: 一个关于正确、错误和外在等变性的普遍理论

    A General Theory of Correct, Incorrect, and Extrinsic Equivariance. (arXiv:2303.04745v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04745](http://arxiv.org/abs/2303.04745)

    该论文提出了一个关于正确、错误和外在等变性的普遍理论，通过逐点定义量化了函数表现的每种类型等变性的程度，并研究了不正确或外在对称性对模型错误的影响。实验证实了这些结果。 (230字符)

    

    尽管等变机器学习在许多任务中证明是有效的，但成功很大程度上依赖于假设地面真相函数在整个域上是对称的，与等变神经网络的对称性匹配。等变学习文献中缺少的一块是在域中仅部分存在对称性时等变网络的分析。在这项工作中，我们提出了一个适用于这种情况的普遍理论。我们提出了正确、错误和外在等变性的逐点定义，这使我们能够连续地量化函数显示的每种类型等变性的程度。然后，我们研究了不正确或外在对称性的各种程度对模型错误的影响。我们证明了在部分不正确对称性的分类或回归设置中不变或等变网络存在错误的下界。我们还分析了外在等变性的潜在有害影响。实验证实了这些结果在三种不同的实验中。

    Although equivariant machine learning has proven effective at many tasks, success depends heavily on the assumption that the ground truth function is symmetric over the entire domain matching the symmetry in an equivariant neural network. A missing piece in the equivariant learning literature is the analysis of equivariant networks when symmetry exists only partially in the domain. In this work, we present a general theory for such a situation. We propose pointwise definitions of correct, incorrect, and extrinsic equivariance, which allow us to quantify continuously the degree of each type of equivariance a function displays. We then study the impact of various degrees of incorrect or extrinsic symmetry on model error. We prove error lower bounds for invariant or equivariant networks in classification or regression settings with partially incorrect symmetry. We also analyze the potentially harmful effects of extrinsic equivariance. Experiments validate these results in three different 
    
[^197]: 利用不对称性进行合成训练数据生成：SynthIE和信息提取案例

    Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction. (arXiv:2303.04132v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.04132](http://arxiv.org/abs/2303.04132)

    本文展示了利用不对称性进行合成训练数据生成的方法，可以针对结构化输出问题产生大规模、高质量的数据。在封闭信息提取任务中，我们合成生成了一个包含180万数据点的数据集，并利用此数据集对小型模型进行微调，取得了远超先前领先技术的成果。

    

    大型语言模型（LLM）在合成数据生成方面有着巨大的潜力。这项工作表明，即使对于LLM无法直接解决的任务，也可以合成生成有用的数据：对于具有结构化输出的问题，可以提示LLM在反向方向上执行任务，通过为目标输出结构生成合理的输入文本。利用任务困难度的不对称性，可以生成大规模、高质量的复杂任务数据。我们在封闭信息提取方面展示了这种方法的有效性，该领域难以收集到真实数据，至今没有令人满意的数据集存在。我们合成生成了一个包含180万数据点的数据集，并在人工评估中证明其与现有数据集相比具有更好的质量，并利用该数据集对小型模型（220M和770M参数）进行微调，这些模型被称为SynthIE，以远远超过先前领先技术的水平（具有相同的模型大小）。

    Large language models (LLMs) have great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by LLMs: for problems with structured outputs, it is possible to prompt an LLM to perform the task in the reverse direction, by generating plausible input text for a target output structure. Leveraging this asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex tasks. We demonstrate the effectiveness of this approach on closed information extraction, where collecting ground-truth data is challenging, and no satisfactory dataset exists to date. We synthetically generate a dataset of 1.8M data points, establish its superior quality compared to existing datasets in a human evaluation, and use it to finetune small models (220M and 770M parameters), termed SynthIE, that outperform the prior state of the art (with equal model size) by a substantial margin of 
    
[^198]: 基于动量的梯度方法在非凸下层双层优化中的应用

    On Momentum-Based Gradient Methods for Bilevel Optimization with Nonconvex Lower-Level. (arXiv:2303.03944v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2303.03944](http://arxiv.org/abs/2303.03944)

    本文研究了一类非凸双层优化问题，并提出了一种基于动量的梯度双层方法(MGBiO)来解决这些确定性问题，同时提出了一类基于动量的随机梯度双层方法(MSGBiO和VR-MSGBiO)来解决这些随机问题。通过收敛分析，证明了MGBiO方法具有收敛性。

    

    双层优化是一种常用的两级层次优化方法，已广泛应用于许多机器学习任务，如超参数学习、元学习和持续学习。然而，在下层问题为非凸时，双层方法的研究还不够充分。为此，本文研究了一类非凸双层优化问题，其中上层问题和下层问题均为非凸问题，并且下层问题满足Polyak-Lojasiewicz (PL)条件。我们提出了一种高效的基于动量的梯度双层方法(MGBiO)来解决这些确定性问题。同时，我们提出了一类高效的基于动量的随机梯度双层方法(MSGBiO和VR-MSGBiO)来解决这些随机问题。此外，我们为我们的方法提供了一个有用的收敛分析框架。具体而言，在一些温和的条件下，我们证明了MGBiO方法具有收敛性。

    Bilevel optimization is a popular two-level hierarchical optimization, which has been widely applied to many machine learning tasks such as hyperparameter learning, meta learning and continual learning. Although many bilevel optimization methods recently have been developed, the bilevel methods are not well studied when the lower-level problem is nonconvex. To fill this gap, in the paper, we study a class of nonconvex bilevel optimization problems, where both upper-level and lower-level problems are nonconvex, and the lower-level problem satisfies Polyak-{\L}ojasiewicz (PL) condition. We propose an efficient momentum-based gradient bilevel method (MGBiO) to solve these deterministic problems. Meanwhile, we propose a class of efficient momentum-based stochastic gradient bilevel methods (MSGBiO and VR-MSGBiO) to solve these stochastic problems. Moreover, we provide a useful convergence analysis framework for our methods. Specifically, under some mild conditions, we prove that our MGBiO m
    
[^199]: 针对联邦学习中安全聚合的客户特定属性推断

    Client-specific Property Inference against Secure Aggregation in Federated Learning. (arXiv:2303.03908v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2303.03908](http://arxiv.org/abs/2303.03908)

    针对联邦学习中的安全聚合，本研究提出了一种针对客户特定属性推断的解决方案。目前的防御方法包括差分隐私和安全聚合，但它们都存在一定的缺陷。因此，本研究旨在提供一种更有效的保护隐私攻击的方法。

    

    联邦学习已成为一种广泛使用的范例，用于在不同参与者之间协作训练共同的模型，并通过协调训练的中央服务器进行协调。尽管在联邦训练期间仅交换模型参数或其他模型更新，而不是参与者的数据，但许多攻击表明仍然有可能推断出敏感信息，如成员身份、属性或参与者数据的完全重建。尽管差分隐私被认为是一种有效的保护隐私攻击的解决方案，但也因其对效用的负面影响而受到批评。另一个可能的防御是使用安全聚合，它允许服务器仅访问聚合的更新，而不是每个单独的更新，并且由于不会降低模型质量，因此更具吸引力。然而，仅仅结合由每轮中不同组合的客户生成的聚合更新可能仍然存在隐私泄露。

    Federated learning has become a widely used paradigm for collaboratively training a common model among different participants with the help of a central server that coordinates the training. Although only the model parameters or other model updates are exchanged during the federated training instead of the participant's data, many attacks have shown that it is still possible to infer sensitive information such as membership, property, or outright reconstruction of participant data. Although differential privacy is considered an effective solution to protect against privacy attacks, it is also criticized for its negative effect on utility. Another possible defense is to use secure aggregation which allows the server to only access the aggregated update instead of each individual one, and it is often more appealing because it does not degrade model quality. However, combining only the aggregated updates, which are generated by a different composition of clients in every round, may still 
    
[^200]: 通过离线强化学习学习影响人类行为

    Learning to Influence Human Behavior with Offline Reinforcement Learning. (arXiv:2303.02265v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.02265](http://arxiv.org/abs/2303.02265)

    本文提出了一种通过离线强化学习学习影响人类行为的方法，可以提高人类在协作任务中的表现。

    This paper proposes a method of learning to influence human behavior through offline reinforcement learning, which can improve human performance in collaborative tasks.

    在现实世界中，学习代理与人类互动是最复杂的设置之一，因为人类往往由于复杂的偏见而表现出次优的、不可预测的行为。在这种情况下与人类互动的代理最终会影响这些人所采取的行动。我们的目标是使代理能够利用这种影响来提高人类在协作任务中的表现，随着任务的展开。与以前的工作不同，我们不假设与人员进行在线培训（这往往太昂贵和不安全），也不假设有高保真度环境模拟器的访问权限。我们的想法是，通过采用各种先前观察到的人类-人类交互数据并将其标记为任务奖励，离线强化学习（RL）可以学习组合行为的组件，并发现导致更理想的人类行为的行动。首先，我们展示了离线RL可以学习策略来影响和改善人类行为，尽管这些策略可能与人类的期望不同。

    In the real world, some of the most complex settings for learned agents involve interaction with humans, who often exhibit suboptimal, unpredictable behavior due to sophisticated biases. Agents that interact with people in such settings end up influencing the actions that these people take. Our goal in this work is to enable agents to leverage that influence to improve the human's performance in collaborative tasks, as the task unfolds. Unlike prior work, we do not assume online training with people (which tends to be too expensive and unsafe), nor access to a high fidelity simulator of the environment. Our idea is that by taking a variety of previously observed human-human interaction data and labeling it with the task reward, offline reinforcement learning (RL) can learn to combine components of behavior, and uncover actions that lead to more desirable human actions. First, we show that offline RL can learn strategies to influence and improve human behavior, despite those strategies 
    
[^201]: 数据共享中的摘要统计隐私

    Summary Statistic Privacy in Data Sharing. (arXiv:2303.02014v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2303.02014](http://arxiv.org/abs/2303.02014)

    这项研究关注在数据共享中保护摘要统计隐私的问题，提出了衡量隐私风险的度量标准，并证明了隐私和失真之间的权衡存在下界，并提出了一类适用于不同数据分布的量化机制，该机制在某些情况下与下界匹配，最终在实际数据集上展示了更好的隐私-失真权衡的效果。

    

    我们研究了一个数据持有者希望与接收者共享数据，同时又不透露数据分布的某些摘要统计信息（如平均值，标准差）的情景。通过将数据通过随机化机制传递，实现了这一目标。我们提出了摘要统计隐私，这是一种用于量化此类机制的隐私风险的度量标准，基于对于攻击者在某个阈值内猜测分布秘密的最坏情况概率。将失真定义为真实数据与发布数据之间的最坏情况Wasserstein-1距离，我们证明了隐私和失真之间的权衡的下界。然后，我们提出了一类可以适应不同数据分布的量化机制。我们证明，该量化机制的隐私-失真权衡在某些情况下与我们的下界匹配，最多相差一些较小的常数因子。最后，我们在实际数据集上展示了所提出的量化机制实现更好的隐私-失真权衡。

    We study a setting where a data holder wishes to share data with a receiver, without revealing certain summary statistics of the data distribution (e.g., mean, standard deviation). It achieves this by passing the data through a randomization mechanism. We propose summary statistic privacy, a metric for quantifying the privacy risk of such a mechanism based on the worst-case probability of an adversary guessing the distributional secret within some threshold. Defining distortion as a worst-case Wasserstein-1 distance between the real and released data, we prove lower bounds on the tradeoff between privacy and distortion. We then propose a class of quantization mechanisms that can be adapted to different data distributions. We show that the quantization mechanism's privacy-distortion tradeoff matches our lower bounds under certain regimes, up to small constant factors. Finally, we demonstrate on real-world datasets that the proposed quantization mechanisms achieve better privacy-distorti
    
[^202]: 对于ImageNet的对抗训练再探：架构、训练和跨威胁模型的泛化

    Revisiting Adversarial Training for ImageNet: Architectures, Training and Generalization across Threat Models. (arXiv:2303.01870v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.01870](http://arxiv.org/abs/2303.01870)

    本论文重新研究了在ImageNet上的对抗训练，发现通过轻微改变架构和训练方案可显著提高模型的鲁棒性和泛化能力。修改后的ConvNeXt在已见威胁模型下获得了最鲁棒的结果，而ViT + ConvStem在未见威胁模型下的泛化效果最好。

    

    虽然对于ResNet架构和低分辨率数据集如CIFAR来说，对抗训练已有广泛研究，但对于ImageNet而言，了解较少。鉴于最近有关transformers是否比卷积网络更坚固的争论，我们重新研究了在ImageNet上的对抗训练，并比较了ViTs和ConvNeXts的性能。大量实验证明，架构的微小改变，尤其是用ConvStem替换PatchStem以及训练方案，对所获得的鲁棒性有显著影响。这些改变不仅提高了在已见$\ell_\infty$威胁模型下的鲁棒性，而且更进一步改善了对未见$\ell_1/\ell_2$攻击的泛化能力。我们修改后的ConvNeXt，在不同模型参数和FLOPs范围内获得了最鲁棒的$\ell_\infty$模型，而我们的ViT + ConvStem在未见威胁模型上实现了最佳的泛化效果。

    While adversarial training has been extensively studied for ResNet architectures and low resolution datasets like CIFAR, much less is known for ImageNet. Given the recent debate about whether transformers are more robust than convnets, we revisit adversarial training on ImageNet comparing ViTs and ConvNeXts. Extensive experiments show that minor changes in architecture, most notably replacing PatchStem with ConvStem, and training scheme have a significant impact on the achieved robustness. These changes not only increase robustness in the seen $\ell_\infty$-threat model, but even more so improve generalization to unseen $\ell_1/\ell_2$-attacks. Our modified ConvNeXt, ConvNeXt + ConvStem, yields the most robust $\ell_\infty$-models across different ranges of model parameters and FLOPs, while our ViT + ConvStem yields the best generalization to unseen threat models.
    
[^203]: GlucoSynth：生成差分私有合成血糖轨迹

    GlucoSynth: Generating Differentially-Private Synthetic Glucose Traces. (arXiv:2303.01621v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01621](http://arxiv.org/abs/2303.01621)

    本文提出了一种新颖的GAN框架-GlucoSynth，采用差分隐私机制来生成合成血糖轨迹，保证数据隐私安全的同时能生成高质量的合成数据.

    

    本论文研究生成高质量、私有合成血糖轨迹的问题，这个任务可推广到许多其他时间序列数据。现有的时间序列数据合成方法，如使用生成对抗网络（GANs）的方法，无法捕捉血糖数据的先天特征，也无法在不严重降低合成数据效用的情况下提供任何形式隐私保证。本文提出了GlucoSynth，一种新颖的保护隐私的GAN框架，可用于生成合成血糖轨迹。我们方法的核心思想是在考虑时序动态的同时，保留轨迹中motif（血糖事件）之间的关系。我们的框架采用差分隐私机制，提供了强有力的形式隐私保证。我们使用120万条血糖轨迹进行了全面评估；GlucoSynth在生成高质量合成数据方面表现优异。

    We focus on the problem of generating high-quality, private synthetic glucose traces, a task generalizable to many other time series sources. Existing methods for time series data synthesis, such as those using Generative Adversarial Networks (GANs), are not able to capture the innate characteristics of glucose data and cannot provide any formal privacy guarantees without severely degrading the utility of the synthetic data. In this paper we present GlucoSynth, a novel privacy-preserving GAN framework to generate synthetic glucose traces. The core intuition behind our approach is to conserve relationships amongst motifs (glucose events) within the traces, in addition to temporal dynamics. Our framework incorporates differential privacy mechanisms to provide strong formal privacy guarantees. We provide a comprehensive evaluation on the real-world utility of the data using 1.2 million glucose traces; GlucoSynth outperforms all previous methods in its ability to generate high-quality synt
    
[^204]: SHAP-IQ: 任意阶Shapley interaction的统一逼近方法

    SHAP-IQ: Unified Approximation of any-order Shapley Interactions. (arXiv:2303.01179v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01179](http://arxiv.org/abs/2303.01179)

    提出了一种名为SHAP-IQ的新方法，用于计算任意阶Shapley互动，并提供了逼近质量的理论保证和方差估计。该方法在计算成本和逼近质量方面优于现有方法。

    

    在可解释的人工智能（XAI）研究中，Shapley值（SV）通常被应用于确定任何黑盒模型的特征重要性得分。 Shapley interaction indices将SV扩展为定义任意阶特征相互作用得分。定义独特的Shapley interaction index是一个开放性研究问题，迄今为止已经提出了三个定义，其不同之处在于所选择的公理。此外，每个定义都需要特定的逼近技术。在这里，我们提出了基于采样的有效逼近方法SHAPley Interaction Quantification（SHAP-IQ），以计算任意基数交互指数（CII）的Shapley互动。即满足线性、对称和虚拟公理的交互指数。SHAP-IQ基于一种新颖的表示方法，与现有方法相比，我们为其逼近质量提供了理论保证，以及点估计的方差估计。对于SV的特殊情况，我们的逼近方法与精确计算一致。进行了数值实验，以证明我们的方法在几个合成和实际数据集上的有效性，并显示SHAP-IQ在计算成本和逼近质量方面优于现有方法。

    Predominately in explainable artificial intelligence (XAI) research, the Shapley value (SV) is applied to determine feature importance scores for any black box model. Shapley interaction indices extend the SV to define any-order feature interaction scores. Defining a unique Shapley interaction index is an open research question and, so far, three definitions have been proposed, which differ by their choice of axioms. Moreover, each definition requires a specific approximation technique. Here, we propose SHAPley Interaction Quantification (SHAP-IQ), an efficient sampling-based approximator to compute Shapley interactions for arbitrary cardinal interaction indices (CII), i.e. interaction indices that satisfy the linearity, symmetry and dummy axiom. SHAP-IQ is based on a novel representation and, in contrast to existing methods, we provide theoretical guarantees for its approximation quality, as well as estimates for the variance of the point estimates. For the special case of SV, our app
    
[^205]: 量化和建模多模态交互：一种信息分解框架

    Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework. (arXiv:2302.12247v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12247](http://arxiv.org/abs/2302.12247)

    通过引入信息分解框架，我们提供了一种量化和建模多模态交互的方法，通过PID统计量来度量输入模态与输出任务之间的冗余度、独特性和协同性，并引入了两个新的PID统计估计器。

    

    对于解决多模态任务所需的交互如何进行量化？最适合捕捉这些交互的多模态模型是什么？为了回答这些问题，我们提出了一种信息论方法来量化输入模态与输出任务之间的冗余度、独特性和协同性。我们将这三个衡量标准称为多模态分布（或简称PID）的PID统计量，并引入了两个新的PID统计估计器，适用于高维分布。为了验证PID估计，我们在已知PID的合成数据集和大规模多模态基准测试集上进行了大量实验。

    The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations
    
[^206]: 离线强化学习用于混合专家对话管理

    Offline Reinforcement Learning for Mixture-of-Expert Dialogue Management. (arXiv:2302.10850v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10850](http://arxiv.org/abs/2302.10850)

    本论文研究了离线强化学习在混合专家对话管理中的应用。通过利用最新的混合专家语言模型（MoE-LMs），我们开发了针对对话规划的强化学习算法，显著减少了动作空间的大小，并提高了对话管理的有效性。

    

    强化学习（RL）在开发对话管理（DM）代理，实现非目标导向，进行富有内容的对话，最大化用户满意度方面表现出了巨大的潜力。尽管强化学习和语言模型（LMs）最近取得了进展，但使用强化学习驱动的对话聊天机器人仍然具有挑战性，部分原因是强化学习需要在线探索以有效学习，而收集新颖的人机交互可能既昂贵又不安全。这个问题在面对这些算法的组合动作空间时变得更为严重，因为大多数语言模型代理以词级别生成响应。我们开发了多种针对对话规划的强化学习算法，利用最新的混合专家语言模型（MoE-LMs） - 一种捕捉多样语义，生成反映不同意图的话语的模型，适用于多轮对话管理。通过利用MoE-LM结构，我们的方法显著减少了行动空间的大小，并提高了基于强化学习的对话管理的有效性。

    Reinforcement learning (RL) has shown great promise for developing dialogue management (DM) agents that are non-myopic, conduct rich conversations, and maximize overall user satisfaction. Despite recent developments in RL and language models (LMs), using RL to power conversational chatbots remains challenging, in part because RL requires online exploration to learn effectively, whereas collecting novel human-bot interactions can be expensive and unsafe. This issue is exacerbated by the combinatorial action spaces facing these algorithms, as most LM agents generate responses at the word level. We develop a variety of RL algorithms, specialized to dialogue planning, that leverage recent Mixture-of-Expert Language Models (MoE-LMs) -- models that capture diverse semantics, generate utterances reflecting different intents, and are amenable for multi-turn DM. By exploiting MoE-LM structure, our methods significantly reduce the size of the action space and improve the efficacy of RL-based DM.
    
[^207]: 关于校准扩散概率模型

    On Calibrating Diffusion Probabilistic Models. (arXiv:2302.10688v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10688](http://arxiv.org/abs/2302.10688)

    本文发现了数据分数随机反向过程是一个鞅，提出了一种简单的方法，用于校准任意预先训练的DPM，有效减小模型的得分匹配损失，增加模型似然的下限，并提供了一般校准指南。

    

    最近，扩散概率模型（DPM）在各种生成性任务中取得了有希望的结果。一个典型的DPM框架包括一个逐渐扩散数据分布的正向过程和一个从时间相关数据分数中恢复数据分布的随机反向过程。本文观察到数据分数的随机反向过程是一个鞅，从中可以导出数据分数的集中界和随机停止定理。然后，我们发现一种简单的方法，用于校准任意预先训练的DPM，以减小得分匹配损失，并因此增加模型似然的下限。我们提供了各种模型参数化下的一般校准指南。我们的校准方法仅执行一次，并且可以重复使用所得到的模型进行采样。我们在多个数据集上进行实验，以经验性地验证我们的提议。我们的代码位于https://github.com/thudzj/Cal。

    Recently, diffusion probabilistic models (DPMs) have achieved promising results in diverse generative tasks. A typical DPM framework includes a forward process that gradually diffuses the data distribution and a reverse process that recovers the data distribution from time-dependent data scores. In this work, we observe that the stochastic reverse process of data scores is a martingale, from which concentration bounds and the optional stopping theorem for data scores can be derived. Then, we discover a simple way for calibrating an arbitrary pretrained DPM, with which the score matching loss can be reduced and the lower bounds of model likelihood can consequently be increased. We provide general calibration guidelines under various model parametrizations. Our calibration method is performed only once and the resulting models can be used repeatedly for sampling. We conduct experiments on multiple datasets to empirically validate our proposal. Our code is at https://github.com/thudzj/Cal
    
[^208]: 迈向无界机器遗忘

    Towards Unbounded Machine Unlearning. (arXiv:2302.09880v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09880](http://arxiv.org/abs/2302.09880)

    本文是第一篇研究不同应用（偏见消除、混淆解决、隐私保护）遗忘问题的论文，提出了适用于不同应用的遗忘定义和指标，并提出了SCRUB，一种在不同应用的遗忘质量度量上始终是顶级表现者的算法。

    

    深度机器遗忘是指从训练好的神经网络中“移除”其训练集的一个子集的问题。这个问题非常及时，并且有许多应用，包括解除偏见（RB）、消除混淆（RC）（由训练模型中的错误标签数据引起），以及允许用户行使“被遗忘权”以保护用户隐私（UP）的关键任务。本文是我们所知的第一篇研究不同应用（RB、RC、UP）的遗忘问题的论文，我们认为每个应用都有自己的忘记需求、忘记定义和与忘记质量相关的指标。对于UP，我们提出了一种用于遗忘的新颖适应性强的成员推断攻击。我们还提出了SCRUB，一种新颖的遗忘算法，在RB、RC和UP的不同应用相关度量指标上始终是忘记质量的顶级表现者。同时，SCRUB还在衡量模式的度量指标上始终是顶级表现者。

    Deep machine unlearning is the problem of `removing' from a trained neural network a subset of its training set. This problem is very timely and has many applications, including the key tasks of removing biases (RB), resolving confusion (RC) (caused by mislabelled data in trained models), as well as allowing users to exercise their `right to be forgotten' to protect User Privacy (UP). This paper is the first, to our knowledge, to study unlearning for different applications (RB, RC, UP), with the view that each has its own desiderata, definitions for `forgetting' and associated metrics for forget quality. For UP, we propose a novel adaptation of a strong Membership Inference Attack for unlearning. We also propose SCRUB, a novel unlearning algorithm, which is the only method that is consistently a top performer for forget quality across the different application-dependent metrics for RB, RC, and UP. At the same time, SCRUB is also consistently a top performer on metrics that measure mode
    
[^209]: 随机逼近法用于组分布式鲁棒优化

    Stochastic Approximation Approaches to Group Distributionally Robust Optimization. (arXiv:2302.09267v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09267](http://arxiv.org/abs/2302.09267)

    本文提出了一种随机逼近法，用于组分布式鲁棒优化，该算法利用在线学习技术，将每轮所需的样本数从$m$个降至$1$个，同时保持相同的样本复杂度。

    

    本文研究组分布式鲁棒优化（GDRO），目的是学习一个能在$m$个不同分布上表现良好的模型。首先，我们将GDRO建模为随机凸凹鞍点问题，并证明使用$m$个样本的随机镜像下降法(SMD)，能够实现$O(m(\log m)/\epsilon ^2)$个样本的复杂度，以找到一个$\epsilon$-最优解，这与$\Omega(m/\epsilon ^2)$的下界想匹配，除了一个对数因子。接下来，我们利用在线学习技术，将每轮所需的样本数从$m$个降至$1$个，同时保持相同的样本复杂度。具体而言，我们将GDRO构造为一个双人博弈，其中一个玩家简单地执行SMD，另一个执行一种用于非明显多臂老虎机的在线算法。接下来，我们考虑一个更实际的情况，即可以从每个分布中绘制的样本数量不同，并提出一种新的公式。

    This paper investigates group distributionally robust optimization (GDRO), with the purpose to learn a model that performs well over $m$ different distributions. First, we formulate GDRO as a stochastic convex-concave saddle-point problem, and demonstrate that stochastic mirror descent (SMD), using $m$ samples in each iteration, achieves an $O(m (\log m)/\epsilon^2)$ sample complexity for finding an $\epsilon$-optimal solution, which matches the $\Omega(m/\epsilon^2)$ lower bound up to a logarithmic factor. Then, we make use of techniques from online learning to reduce the number of samples required in each round from $m$ to $1$, keeping the same sample complexity. Specifically, we cast GDRO as a two-players game where one player simply performs SMD and the other executes an online algorithm for non-oblivious multi-armed bandits. Next, we consider a more practical scenario where the number of samples that can be drawn from each distribution is different, and propose a novel formulation
    
[^210]: 快速时域小波图神经网络

    Fast Temporal Wavelet Graph Neural Networks. (arXiv:2302.08643v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08643](http://arxiv.org/abs/2302.08643)

    本论文提出了一种快速时域小波图神经网络(FTWGNN)，用于可靠和及时地对人脑和交通网络进行预测。实验结果表明，该方法是高效的。

    

    时空信号预测在众多领域中起着重要作用，尤其是在神经科学和交通领域。由于网络的高度复杂的空间结构和非线性的时态动态，这个任务具有挑战性。为了方便可靠和及时地对人脑和交通网络进行预测，我们提出了一种快速时域小波图神经网络(FTWGNN)，它对基于时序数据和基础图结构的学习任务既高效又节省内存，这要归功于多分辨率分析和离散空间小波理论的理论基础。我们采用多分辨率矩阵分解(MMF) (Kondor et al., 2014)来分解高密度图结构，并计算相应的稀疏小波基，从而构建快速小波卷积作为我们新型架构的核心。在真实世界的PEMS-BAY、METR-LA交通数据集和AJILE12 ECoG数据集上的实验结果表明，FTWGNN是高效的。

    Spatio-temporal signals forecasting plays an important role in numerous domains, especially in neuroscience and transportation. The task is challenging due to the highly intricate spatial structure, as well as the non-linear temporal dynamics of the network. To facilitate reliable and timely forecast for the human brain and traffic networks, we propose the Fast Temporal Wavelet Graph Neural Networks (FTWGNN) that is both time- and memory-efficient for learning tasks on timeseries data with the underlying graph structure, thanks to the theories of multiresolution analysis and wavelet theory on discrete spaces. We employ Multiresolution Matrix Factorization (MMF) (Kondor et al., 2014) to factorize the highly dense graph structure and compute the corresponding sparse wavelet basis that allows us to construct fast wavelet convolution as the backbone of our novel architecture. Experimental results on real-world PEMS-BAY, METR-LA traffic datasets and AJILE12 ECoG dataset show that FTWGNN is 
    
[^211]: DP-SGD中的训练数据重构界限研究

    Bounding Training Data Reconstruction in DP-SGD. (arXiv:2302.07225v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2302.07225](http://arxiv.org/abs/2302.07225)

    本文在DP-SGD的上下文中研究了如何设置隐私参数以保护免受训练数据重构攻击，并提供了相关攻击的上限和匹配的攻击方式。

    

    差分隐私训练通常被解释为对抗成员推断攻击的保护。最近的研究发现，如果只需要保护免受训练数据重构攻击的威胁，那么私有模型的效用可以改善，因为保护免受这些更有野心的攻击需要更少的噪声。本文在DP-SGD的上下文中进一步研究了这一问题，并提供了针对DP-SGD的任何重建攻击成功的上限以及与我们边界预测相匹配的攻击。这两个结果为设置DP-SGD的隐私参数以保护免受重建攻击开辟了细致的研究方向。

    Differentially private training offers a protection which is usually interpreted as a guarantee against membership inference attacks. By proxy, this guarantee extends to other threats like reconstruction attacks attempting to extract complete training examples. Recent works provide evidence that if one does not need to protect against membership attacks but instead only wants to protect against training data reconstruction, then utility of private models can be improved because less noise is required to protect against these more ambitious attacks. We investigate this further in the context of DP-SGD, a standard algorithm for private deep learning, and provide an upper bound on the success of any reconstruction attack against DP-SGD together with an attack that empirically matches the predictions of our bound. Together, these two results open the door to fine-grained investigations on how to set the privacy parameters of DP-SGD in practice to protect against reconstruction attacks. Fin
    
[^212]: 近似最优的非凸随机优化在广义光滑性下

    Near-Optimal Non-Convex Stochastic Optimization under Generalized Smoothness. (arXiv:2302.06032v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06032](http://arxiv.org/abs/2302.06032)

    本文提出了一种在广义光滑性条件下的近似最优非凸随机优化算法解决了大批量大小和只基于预期速率的收敛界限两个问题。

    

    广义光滑条件，$(L_{0},L_{1})$-光滑性，在许多优化问题中都更加现实，这通过经验和理论证据都得到了证明。最近的两项工作建立了$O(\epsilon^{-3})$的样本复杂度，以获得$O(\epsilon)$-稳定点。然而，这两者都需要一个大批量的大小，其数量级是$\mathrm{poly}(\epsilon^{-1})$，这不仅在计算上很负担，也不适用于流式应用。此外，这些现有的收敛界限只对预期速率进行了建立，这是不足的，因为它们在单次运行时没有提供有用的性能保证。在这项工作中，我们通过重新审视STORM算法的一个简单变体来同时解决前两个问题。具体来说，在$(L_{0},L_{1})$-光滑性和仿射型噪声下，我们建立了第一个近似最优的高概率样本复杂度，为$O(\log(1/(\delta\epsilon))\epsilon^{-3})$，其中$\delta\i

    The generalized smooth condition, $(L_{0},L_{1})$-smoothness, has triggered people's interest since it is more realistic in many optimization problems shown by both empirical and theoretical evidence. Two recent works established the $O(\epsilon^{-3})$ sample complexity to obtain an $O(\epsilon)$-stationary point. However, both require a large batch size on the order of $\mathrm{ploy}(\epsilon^{-1})$, which is not only computationally burdensome but also unsuitable for streaming applications. Additionally, these existing convergence bounds are established only for the expected rate, which is inadequate as they do not supply a useful performance guarantee on a single run. In this work, we solve the prior two problems simultaneously by revisiting a simple variant of the STORM algorithm. Specifically, under the $(L_{0},L_{1})$-smoothness and affine-type noises, we establish the first near-optimal $O(\log(1/(\delta\epsilon))\epsilon^{-3})$ high-probability sample complexity where $\delta\i
    
[^213]: 平均Hölder平滑度下的近似最优学习

    Near-optimal learning with average H\"older smoothness. (arXiv:2302.06005v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06005](http://arxiv.org/abs/2302.06005)

    通过推广平均Lipschitz平滑性到Hölder平滑性，得到了关于平均Hölder平滑性的上下风险界，最优的下界对数因子最多差一个，提供了独立的学习算法。

    

    我们将Ashlagi等人（COLT 2021）提出的平均Lipschitz平滑性概念推广到Hölder平滑性，并证明了关于平均Hölder平滑性的上下风险界，这些界的速率甚至在平均Lipschitz平滑性的特殊情况下也优于之前已知界。此外，我们的下界在可实现情况下是最优的，最多差一个对数因子，从而建立了极小值率。从算法的角度来看，由于我们对平均平滑度的定义是针对未知的基础分布的，因此学习者没有函数类的显式表示，无法执行ERM。尽管如此，我们提供了独立的学习算法。

    We generalize the notion of average Lipschitz smoothness proposed by Ashlagi et al. (COLT 2021) by extending it to H\"older smoothness. This measure of the "effective smoothness" of a function is sensitive to the underlying distribution and can be dramatically smaller than its classic "worst-case H\"older constant. We consider both the realizable and the agnostic (noisy) regression settings, proving upper and lower risk bounds in terms of the average H\"older smoothness; these rates improve upon both previously known rates even in the special case of average Lipschitz smoothness. Moreover, our lower bound is tight in the realizable setting up to log factors, thus we establish the minimax rate. From an algorithmic perspective, since our notion of average smoothness is defined with respect to the unknown underlying distribution, the learner does not have an explicit representation of the function class, hence is unable to execute ERM. Nevertheless, we provide distinct learning algorithms
    
[^214]: 基于深度学习和辅助训练数据的多用户活动识别实现人机协作

    Towards Multi-User Activity Recognition through Facilitated Training Data and Deep Learning for Human-Robot Collaboration Applications. (arXiv:2302.05763v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05763](http://arxiv.org/abs/2302.05763)

    本研究通过收集单个用户数据并在后处理中合并数据的方法，实现了多用户活动的识别，有望用于人机协作领域。

    

    人机交互研究逐渐关注多方面场景，即机器人与多个人用户同时交互的场景。 然而，在人机协作方面，研究仍处于早期阶段。处理此类合作的机器学习技术需要的数据比典型的人机交互设置中更不可行。本研究提出了非二元人机协作应用的并行任务场景，并提议一种替代方法来收集与多用户活动相关的数据，即收集与单个用户相关的数据并在后处理中合并它们，以减少产生成双设置录制的努力。收集了单个用户的活动三维骨架姿势并将它们合并成一对来验证该语句，随后，这些数据点被用于分别训练由LSTM网络和VAE 混合而成的模型。

    Human-robot interaction (HRI) research is progressively addressing multi-party scenarios, where a robot interacts with more than one human user at the same time. Conversely, research is still at an early stage for human-robot collaboration. The use of machine learning techniques to handle such type of collaboration requires data that are less feasible to produce than in a typical HRC setup. This work outlines scenarios of concurrent tasks for non-dyadic HRC applications. Based upon these concepts, this study also proposes an alternative way of gathering data regarding multi-user activity, by collecting data related to single users and merging them in post-processing, to reduce the effort involved in producing recordings of pair settings. To validate this statement, 3D skeleton poses of activity of single users were collected and merged in pairs. After this, such datapoints were used to separately train a long short-term memory (LSTM) network and a variational autoencoder (VAE) composed
    
[^215]: Jaccard度量损失：使用软标签优化Jaccard指数

    Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels. (arXiv:2302.05666v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.05666](http://arxiv.org/abs/2302.05666)

    本文提出了Jaccard度量损失（JMLs）来优化Jaccard指数，该损失在软标签下仍然有效。

    

    IoU损失是直接优化Jaccard指数的替代品。在语义分割中，将IoU损失作为损失函数的一部分，与仅优化像素损失（如交叉熵损失）相比，对于Jaccard指数测量表现更好。最显着的IoU损失是软Jaccard损失和Lovasz-Softmax损失。然而，这些损失与机器学习中普遍存在的软标签不兼容。在本文中，我们提出了Jaccard度量损失（JMLs），它们在标准设置下与软标签兼容，与软Jaccard损失相同。使用JMLs，我们研究了两种最流行的软标签用例：标签平滑和知识蒸馏。在三个语义分割数据集（Cityscapes、PASCAL VOC和DeepGlobe Land）上，我们的实验表明，与交叉熵损失相比，我们的简单方法显著提高了性能，并且在DeepGlobe Land数据集上超过了最先进的方法。

    IoU losses are surrogates that directly optimize the Jaccard index. In semantic segmentation, leveraging IoU losses as part of the loss function is shown to perform better with respect to the Jaccard index measure than optimizing pixel-wise losses such as the cross-entropy loss alone. The most notable IoU losses are the soft Jaccard loss and the Lovasz-Softmax loss. However, these losses are incompatible with soft labels which are ubiquitous in machine learning. In this paper, we propose Jaccard metric losses (JMLs), which are identical to the soft Jaccard loss in a standard setting with hard labels, but are compatible with soft labels. With JMLs, we study two of the most popular use cases of soft labels: label smoothing and knowledge distillation. With a variety of architectures, our experiments show significant improvements over the cross-entropy loss on three semantic segmentation datasets (Cityscapes, PASCAL VOC and DeepGlobe Land), and our simple approach outperforms state-of-the-
    
[^216]: 星形降噪扩散概率模型

    Star-Shaped Denoising Diffusion Probabilistic Models. (arXiv:2302.05259v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.05259](http://arxiv.org/abs/2302.05259)

    创新点在于提出了一种非马尔可夫扩散噪声过程的星形降噪扩散概率模型，能够广泛适用于指数族中的多种分布，特别适用于约束流形上的数据。

    

    基于降噪扩散概率模型（DDPM）的方法已经成为生成模型中无处不在的工具。但是，它们大多局限于高斯和离散扩散过程。我们提出了星形降噪扩散概率模型（SS-DDPM），一种具有非马尔可夫扩散噪声过程的模型。在高斯分布的情况下，该模型等效于马尔可夫DDPM。然而，它可以定义和适用于任意噪声分布，并且对于落在指数族中的广泛分布，它采用了高效的训练和采样算法。我们提供了一个简单的配方，用于设计具有Beta，von Mises-Fisher，Dirichlet，Wishart等分布的扩散样式模型，当数据位于约束流形上时特别有用，例如单位球，正半定矩阵的空间，概率单纯形等。我们在不同的设置中评估了该模型，并发现它很有竞争力。

    Methods based on Denoising Diffusion Probabilistic Models (DDPM) became a ubiquitous tool in generative modeling. However, they are mostly limited to Gaussian and discrete diffusion processes. We propose Star-Shaped Denoising Diffusion Probabilistic Models (SS-DDPM), a model with a non-Markovian diffusion-like noising process. In the case of Gaussian distributions, this model is equivalent to Markovian DDPMs. However, it can be defined and applied with arbitrary noising distributions, and admits efficient training and sampling algorithms for a wide range of distributions that lie in the exponential family. We provide a simple recipe for designing diffusion-like models with distributions like Beta, von Mises--Fisher, Dirichlet, Wishart and others, which can be especially useful when data lies on a constrained manifold such as the unit sphere, the space of positive semi-definite matrices, the probabilistic simplex, etc. We evaluate the model in different settings and find it competitive 
    
[^217]: 针对图数据的异常稳健Gromov-Wasserstein方法

    Outlier-Robust Gromov-Wasserstein for Graph Data. (arXiv:2302.04610v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04610](http://arxiv.org/abs/2302.04610)

    本论文提出了一种针对图数据的异常稳健Gromov-Wasserstein方法（RGW），通过引入乐观扰动的边际约束和使用Bregman近端交替线性化最小化算法，解决了GW距离对异常值敏感的问题。

    

    Gromov-Wasserstein（GW）距离是一种在不同度量空间上比较和对齐概率分布的强大工具。最近，GW已成为广泛应用于图学习任务中对齐异构数据的主要建模技术。然而，已知GW距离对异常值非常敏感，如果在目标函数中将异常值与其他样本赋予相同的权重，可能会导致较大的不准确性。为了缓解这个问题，我们引入了一种新的、稳健的GW距离称为RGW。RGW在基于Kullback-Leibler散度的模糊集合中引入了乐观扰动的边际约束。为了更方便地在实践中使用RGW的好处，我们利用Bregman近端交替线性化最小化算法开发了一个计算高效且理论可证的过程。通过大量实验证实了我们的理论结果，并展示了RGW的有效性。

    Gromov-Wasserstein (GW) distance is a powerful tool for comparing and aligning probability distributions supported on different metric spaces. Recently, GW has become the main modeling technique for aligning heterogeneous data for a wide range of graph learning tasks. However, the GW distance is known to be highly sensitive to outliers, which can result in large inaccuracies if the outliers are given the same weight as other samples in the objective function. To mitigate this issue, we introduce a new and robust version of the GW distance called RGW. RGW features optimistically perturbed marginal constraints within a Kullback-Leibler divergence-based ambiguity set. To make the benefits of RGW more accessible in practice, we develop a computationally efficient and theoretically provable procedure using Bregman proximal alternating linearized minimization algorithm. Through extensive experimentation, we validate our theoretical results and demonstrate the effectiveness of RGW on real-wor
    
[^218]: 神经网络在因果估计中的应用: 在美国评估更严格的空气质量标准的健康效益

    Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US. (arXiv:2302.02560v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02560](http://arxiv.org/abs/2302.02560)

    本研究提出了一种神经网络方法，利用其理论基础和实施的可行性，从而估计连续暴露/治疗的分布对政策相关结果的因果效应。我们将此方法应用于包含6800万个个体和2700万个美国境内死亡事件的数据中，通过评估美国国家环境保护局（EPA）对PM2.5的国家环境空气质量标准（NAAQS）进行修订后的健康效益。

    

    在政策研究中，估计连续性暴露/治疗的分布对感兴趣的结果的因果效应是最关键的分析任务之一。我们称之为偏移-响应函数（SRF）估计问题。现有的涉及强健因果效应估计器的神经网络方法缺乏理论保证和实际实现，用于SRF估计。受公共卫生中的关键政策问题的启发，我们开发了一种神经网络方法及其理论基础，以提供具有强健性和效率保证的SRF估计。然后，我们将我们的方法应用于包含6800万个个体和2700万个美国境内死亡事件的数据中，以估计将美国国家环境保护局（EPA）最近提议从12 μg/m³改为9 μg/m³的PM2.5的美国国家环境空气质量标准（NAAQS）的修订对结果的因果效应。我们的目标是首次估计

    In policy research, one of the most critical analytic tasks is to estimate the causal effect of a policy-relevant shift to the distribution of a continuous exposure/treatment on an outcome of interest. We call this problem shift-response function (SRF) estimation. Existing neural network methods involving robust causal-effect estimators lack theoretical guarantees and practical implementations for SRF estimation. Motivated by a key policy-relevant question in public health, we develop a neural network method and its theoretical underpinnings to estimate SRFs with robustness and efficiency guarantees. We then apply our method to data consisting of 68 million individuals and 27 million deaths across the U.S. to estimate the causal effect from revising the US National Ambient Air Quality Standards (NAAQS) for PM 2.5 from 12 $\mu g/m^3$ to 9 $\mu g/m^3$. This change has been recently proposed by the US Environmental Protection Agency (EPA). Our goal is to estimate, for the first time, the 
    
[^219]: 非参数密度估计在分布漂移下的研究

    Nonparametric Density Estimation under Distribution Drift. (arXiv:2302.02460v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02460](http://arxiv.org/abs/2302.02460)

    该论文研究了非参数密度估计在分布漂移下的问题，证明了紧密极小风险界，并推广了先前关于对漂移的无知学习的结果。

    

    我们研究了非驻点漂移设置下的非参数密度估计。给定来自一系列随时间逐渐变化的分布的独立样本序列，目标是计算当前分布的最佳估计。我们证明了离散和连续平滑密度的紧密极小风险界，其中极小值是对所有可能估计的最小值，而极大值是对满足漂移约束的所有可能分布的最大值。我们的技术适用于广泛的漂移模型，并推广了先前关于对漂移的无知学习的结果。

    We study nonparametric density estimation in non-stationary drift settings. Given a sequence of independent samples taken from a distribution that gradually changes in time, the goal is to compute the best estimate for the current distribution. We prove tight minimax risk bounds for both discrete and continuous smooth densities, where the minimum is over all possible estimates and the maximum is over all possible distributions that satisfy the drift constraints. Our technique handles a broad class of drift models, and generalizes previous results on agnostic learning under drift.
    
[^220]: 与不同训练数据的模型的自然分布偏移的有效鲁棒性

    Effective Robustness against Natural Distribution Shifts for Models with Different Training Data. (arXiv:2302.01381v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01381](http://arxiv.org/abs/2302.01381)

    本论文提出了一种用于评估和比较在不同训练数据上训练的模型有效鲁棒性的新指标，通过控制所有模型的训练分布的多个ID测试集准确性，提供更准确估计的有效鲁棒性。这有助于解释先前工作中使用ImageNet的CLIP样式零样本模型所展示出的令人惊讶的有效鲁棒性提升。

    

    "有效鲁棒性"衡量了超出由于在分布（ID）性能预测的额外的离域（OOD）鲁棒性。现有的有效鲁棒性评估通常使用单个测试集（如ImageNet）来评估ID准确性。当评估在不同数据分布上训练的模型（例如，在ImageNet上训练的模型与在LAION上进行零样本语言-图像预训练的模型进行比较）时，这会产生问题。在本文中，我们提出了一种新的评估指标来评估和比较在不同数据上训练的模型的有效鲁棒性。为此，我们控制了所有评估模型的训练分布所涵盖的多个ID测试集上的准确性。我们的新评估指标在存在具有不同训练数据的模型时能够更好地估计有效鲁棒性。它还可以解释先前工作中使用ImageNet的CLIP样式零样本模型展现出的令人惊讶的有效鲁棒性增益。

    "Effective robustness" measures the extra out-of-distribution (OOD) robustness beyond what can be predicted from the in-distribution (ID) performance. Existing effective robustness evaluations typically use a single test set such as ImageNet to evaluate the ID accuracy. This becomes problematic when evaluating models trained on different data distributions, e.g., comparing models trained on ImageNet vs. zero-shot language-image pre-trained models trained on LAION. In this paper, we propose a new evaluation metric to evaluate and compare the effective robustness of models trained on different data. To do this, we control for the accuracy on multiple ID test sets that cover the training distributions for all the evaluated models. Our new evaluation metric provides a better estimate of effective robustness when there are models with different training data. It may also explain the surprising effective robustness gains of zero-shot CLIP-like models exhibited in prior works that used ImageN
    
[^221]: 上下文套索：通过深度神经网络的方法实现稀疏线性模型

    The contextual lasso: Sparse linear models via deep neural networks. (arXiv:2302.00878v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.00878](http://arxiv.org/abs/2302.00878)

    本论文提出了一种新的统计估计器——上下文套索，可以通过深度神经网络的方法解决解释性和拟合能力的矛盾问题，实现对可解释特征的稀疏拟合，并且稀疏模式和系数会随着上下文特征的变化而发生变化。

    

    稀疏线性模型是可解释机器学习的黄金标准工具，本论文通过使用深度神经网络对稀疏线性模型进行改进，实现了可解释性和强大的拟合能力。上下文套索是一种新的统计估计器，它将输入特征分成可解释特征和上下文特征两组，并对可解释特征进行稀疏拟合，同时其稀疏模式和系数会随着上下文特征的变化而发生变化，这个过程通过深度神经网络无需参数地进行学习。

    Sparse linear models are a gold standard tool for interpretable machine learning, a field of emerging importance as predictive models permeate decision-making in many domains. Unfortunately, sparse linear models are far less flexible as functions of their input features than black-box models like deep neural networks. With this capability gap in mind, we study a not-uncommon situation where the input features dichotomize into two groups: explanatory features, which are candidates for inclusion as variables in an interpretable model, and contextual features, which select from the candidate variables and determine their effects. This dichotomy leads us to the contextual lasso, a new statistical estimator that fits a sparse linear model to the explanatory features such that the sparsity pattern and coefficients vary as a function of the contextual features. The fitting process learns this function nonparametrically via a deep neural network. To attain sparse coefficients, we train the net
    
[^222]: 使用粗粒度分类器进行精细分类的测试期修正

    Test-Time Amendment with a Coarse Classifier for Fine-Grained Classification. (arXiv:2302.00368v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.00368](http://arxiv.org/abs/2302.00368)

    本文研究了精细分类中减小错误严重性的问题，提出了一种使用粗粒度分类器进行测试期修正的方法。通过利用标签层级来提高精细分类性能，在iNaturalist-19和tieredImageNet-H数据集上实现了最先进的结果，并在半监督设置中取得了显著的准确率提升。

    

    我们研究了减小精细分类错误严重性的问题。精细分类常常具有挑战性，主要是因为需要领域专业知识进行准确标注。相比较而言，人类在进行粗粒度分类时特别擅长，因为这需要相对较低的专业水平。为此，我们提出了一种名为Hierarchical Ensembles (HiE)的后期纠正方法，利用标签层级来改善在测试期间使用粗粒度预测的精细分类性能。通过仅要求叶节点的父节点，我们的方法显著减少了平均错误严重性，同时在iNaturalist-19和tieredImageNet-H数据集上提高了top-1准确率，实现了这两个基准测试的新的最先进结果。我们还研究了我们的方法在半监督设置中的有效性。我们的方法在显著降低错误严重性的同时，带来了显著的top-1准确率提升。

    We investigate the problem of reducing mistake severity for fine-grained classification. Fine-grained classification can be challenging, mainly due to the requirement of domain expertise for accurate annotation. However, humans are particularly adept at performing coarse classification as it requires relatively low levels of expertise. To this end, we present a novel approach for Post-Hoc Correction called Hierarchical Ensembles (HiE) that utilizes label hierarchy to improve the performance of fine-grained classification at test-time using the coarse-grained predictions. By only requiring the parents of leaf nodes, our method significantly reduces avg. mistake severity while improving top-1 accuracy on the iNaturalist-19 and tieredImageNet-H datasets, achieving a new state-of-the-art on both benchmarks. We also investigate the efficacy of our approach in the semi-supervised setting. Our approach brings notable gains in top-1 accuracy while significantly decreasing the severity of mista
    
[^223]: 大型Transformer模型的隐藏表示的几何学

    The geometry of hidden representations of large transformer models. (arXiv:2302.00294v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00294](http://arxiv.org/abs/2302.00294)

    大型Transformer模型中的隐藏表示具有类似的几何和统计特性，随着层级的移动，它们在最初的几层中变得高维，然后在中间层中显著收缩，在模型的最后部分，保持恒定或形成第二个浅峰。在第一个峰值结束时，数据集的语义信息被更好地表达。

    

    大型Transformer模型是用于自监督数据分析的强大架构，可以处理包括蛋白质序列、图像和文本在内的各种数据类型。在这些模型中，数据集的语义结构通过一个表示与下一个表示之间的一系列变换而出现。我们表征了这些表示的几何和统计特性，以及它们在层级移动时的变化。通过分析内在维度（ID）和邻居组成，我们发现在训练在蛋白质语言任务和图像重建任务上的Transformer模型中，表示以相似的方式演化。在最初的几层中，数据流形扩展，变得高维，然后在中间层中显著收缩。在模型的最后部分，ID保持大致恒定或形成第二个浅峰。我们展示了数据集的语义信息在第一个峰值结束时更好地表达，这一现象可以被观察到。

    Large transformers are powerful architectures used for self-supervised data analysis across various data types, including protein sequences, images, and text. In these models, the semantic structure of the dataset emerges from a sequence of transformations between one representation and the next. We characterize the geometric and statistical properties of these representations and how they change as we move through the layers. By analyzing the intrinsic dimension (ID) and neighbor composition, we find that the representations evolve similarly in transformers trained on protein language tasks and image reconstruction tasks. In the first layers, the data manifold expands, becoming high-dimensional, and then contracts significantly in the intermediate layers. In the last part of the model, the ID remains approximately constant or forms a second shallow peak. We show that the semantic information of the dataset is better expressed at the end of the first peak, and this phenomenon can be ob
    
[^224]: 单环路交替次梯度法求解非光滑弱凸函数约束优化问题的Oracle复杂度分析

    Oracle Complexity of Single-Loop Switching Subgradient Methods for Non-Smooth Weakly Convex Functional Constrained Optimization. (arXiv:2301.13314v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2301.13314](http://arxiv.org/abs/2301.13314)

    本文分析了一种交替次梯度法用于非凸优化问题的Oracle复杂度，其中约束函数为凸或弱凸，在只使用单环路的情况下取得了相同的复杂度，并可应用于非光滑问题。

    

    本文研究了一类弱凸且约束为凸或弱凸的非凸约束优化问题。本文考虑了经典的交替次梯度法求解此类问题，它是一种直观易实现的一阶方法，但目前只有在凸优化问题中已知其Oracle复杂性分析。本文提出了第一项针对非凸问题的交替次梯度法Oracle复杂度分析，针对的问题是求得几乎最优解。本文分别对约束为凸和弱凸的情形进行讨论。与现有的双环路方法相比，交替次梯度法可应用于非光滑问题，仅使用单环路即可实现相同的复杂度，从而节省了内部迭代次数的调整。

    We consider a non-convex constrained optimization problem, where the objective function is weakly convex and the constraint function is either convex or weakly convex. To solve this problem, we consider the classical switching subgradient method, which is an intuitive and easily implementable first-order method whose oracle complexity was only known for convex problems. This paper provides the first analysis on the oracle complexity of the switching subgradient method for finding a nearly stationary point of non-convex problems. Our results are derived separately for convex and weakly convex constraints. Compared to existing approaches, especially the double-loop methods, the switching gradient method can be applied to non-smooth problems and achieves the same complexity using only a single loop, which saves the effort on tuning the number of inner iterations.
    
[^225]: 深度尺度：在ImageNet上实现稳健性认证

    Scaling in Depth: Unlocking Robustness Certification on ImageNet. (arXiv:2301.12549v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12549](http://arxiv.org/abs/2301.12549)

    本文提出了一些新策略和方法解决了证明深度网络稳健性的难点，引入了Linear ResNet架构和Efficient Margin MAximization损失函数，最终实现了新的最先进稳健准确性。

    

    尽管基于Lipschitz方法在确定性保证下能够实现稳健深度学习的承诺，但目前最先进的结果仅限于对低维数据，例如CIFAR-10的前馈卷积网络（ConvNet）。本文研究了将可证明的稳健训练扩展到更大、更深模型的策略。证明深度网络的一个关键挑战是计算ResNet和ViT体系结构中的残差块的Lipschitz界的高效方法。我们展示了用于常规ResNet的Lipschitz常数边界的快速方法往往不准确，并展示了如何通过设计新的残差块来解决这个问题，从而实现了\emph{Linear ResNet} (LiResNet)架构。然后，我们介绍了\emph{Efficient Margin MAximization} (EMMA)损失函数，通过同时惩罚来自\emph{所有}类别的最坏情况对抗性示例稳定稳健训练。这些贡献共同产生了新的\emph{最先进}的稳健准确性。

    Despite the promise of Lipschitz-based methods for provably-robust deep learning with deterministic guarantees, current state-of-the-art results are limited to feed-forward Convolutional Networks (ConvNets) on low-dimensional data, such as CIFAR-10. This paper investigates strategies for expanding certifiably robust training to larger, deeper models. A key challenge in certifying deep networks is efficient calculation of the Lipschitz bound for residual blocks found in ResNet and ViT architectures. We show that fast ways of bounding the Lipschitz constant for conventional ResNets are loose, and show how to address this by designing a new residual block, leading to the \emph{Linear ResNet} (LiResNet) architecture. We then introduce \emph{Efficient Margin MAximization} (EMMA), a loss function that stabilizes robust training by simultaneously penalizing worst-case adversarial examples from \emph{all} classes. Together, these contributions yield new \emph{state-of-the-art} robust accuracy 
    
[^226]: 核化累计量：超越核均值嵌入

    Kernelized Cumulants: Beyond Kernel Mean Embeddings. (arXiv:2301.12466v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.12466](http://arxiv.org/abs/2301.12466)

    本文通过核技巧将累计量扩展到再生核希尔伯特空间（RKHS），提供了一组新的通用统计量。超越一阶具有几个优势，并且在计算上具有相同的复杂度和最小的开销。

    

    在$d$维实数空间中，众所周知，累计量是一种替代矩的方法，可以以较低的方差估计达到相同的目标。本文利用张量代数的工具将累计量扩展到再生核希尔伯特空间（RKHS），并通过核技巧证明了它们在计算上是可行的。这些核化累计量提供了一组新的通用统计量；经典的最大均值差异和希尔伯特-施密特独立性准则是我们一般构造中的一阶对象。我们在理论上和实证上（使用合成、环境和流量数据分析）论证了超越一阶具有几个优势，并且在我们的实验中可以以相同的计算复杂度和最小的额外开销实现。

    In $\mathbb R^d$, it is well-known that cumulants provide an alternative to moments that can achieve the same goals with numerous benefits such as lower variance estimators. In this paper we extend cumulants to reproducing kernel Hilbert spaces (RKHS) using tools from tensor algebras and show that they are computationally tractable by a kernel trick. These kernelized cumulants provide a new set of all-purpose statistics; the classical maximum mean discrepancy and Hilbert-Schmidt independence criterion arise as the degree one objects in our general construction. We argue both theoretically and empirically (on synthetic, environmental, and traffic data analysis) that going beyond degree one has several advantages and can be achieved with the same computational complexity and minimal overhead in our experiments.
    
[^227]: 神经关系图：识别标签噪音和异常数据的统一框架

    Neural Relation Graph: A Unified Framework for Identifying Label Noise and Outlier Data. (arXiv:2301.12321v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12321](http://arxiv.org/abs/2301.12321)

    本研究提出了一种利用数据的关系结构识别标签错误和异常数据的统一方法，并提供了可视化工具，在大规模图像、语音和语言领域任务中表现良好。

    

    诊断和清理数据是构建健壮的机器学习系统的关键步骤。但是，由于存在复杂问题，如标签错误、欠表示和异常值，因此在具有真实世界分布的大规模数据集中识别问题具有挑战性。在本文中，我们提出了一种利用特征嵌入空间中数据的关系结构这一被忽视的信息来源，来识别有问题的数据的统一方法。为此，我们提出了基于数据的关系图结构来检测标签错误和异常数据的可扩展和有效的算法。我们进一步引入了一种可视化工具，提供特征嵌入空间中数据点的上下文信息，作为交互式诊断数据的有效工具。我们在大规模图像、语音和语言领域任务中评估了我们方法的标签错误和离群值/分布外（OOD）检测性能。

    Diagnosing and cleaning data is a crucial step for building robust machine learning systems. However, identifying problems within large-scale datasets with real-world distributions is challenging due to the presence of complex issues such as label errors, under-representation, and outliers. In this paper, we propose a unified approach for identifying the problematic data by utilizing a largely ignored source of information: a relational structure of data in the feature-embedded space. To this end, we present scalable and effective algorithms for detecting label errors and outlier data based on the relational graph structure of data. We further introduce a visualization tool that provides contextual information of a data point in the feature-embedded space, serving as an effective tool for interactively diagnosing data. We evaluate the label error and outlier/out-of-distribution (OOD) detection performances of our approach on the large-scale image, speech, and language domain tasks, inc
    
[^228]: 与人类表征的一致性支持鲁棒的少样本学习

    Alignment with human representations supports robust few-shot learning. (arXiv:2301.11990v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11990](http://arxiv.org/abs/2301.11990)

    论文提出少样本学习的表现与人类表征的一致性存在U形关系，并通过计算机视觉模型的实验进行了验证。高度对齐的模型更加鲁棒，对数据的利用更加有效，但与人类对齐并非必要条件。

    

    我们是否应该关心AI系统是否具有与人类相似的世界表征？我们提供了一个信息论分析，建议在少样本学习任务的表现度与人类表征的一致性之间应该存在一个U形关系。我们通过对491个计算机视觉模型的性能分析验证了这个预测的可行性，并且表明高度对齐的模型更加鲁棒于对抗攻击和域偏移。我们的结果表明，与人类对齐往往是模型有效利用有限数据、鲁棒性 以及泛化能力的充分但不必要条件。

    Should we care whether AI systems have representations of the world that are similar to those of humans? We provide an information-theoretic analysis that suggests that there should be a U-shaped relationship between the degree of representational alignment with humans and performance on few-shot learning tasks. We confirm this prediction empirically, finding such a relationship in an analysis of the performance of 491 computer vision models. We also show that highly-aligned models are more robust to both adversarial attacks and domain shifts. Our results suggest that human-alignment is often a sufficient, but not necessary, condition for models to make effective use of limited data, be robust, and generalize well.
    
[^229]: 用小的条件集表征和学习因果图

    Characterization and Learning of Causal Graphs with Small Conditioning Sets. (arXiv:2301.09028v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.09028](http://arxiv.org/abs/2301.09028)

    本研究提出了一种使用小的条件集来表征和学习因果图的方法，用于解决约束性因果发现算法在数据有限和条件集较大时的困难。我们定义了k-马尔可夫等价的概念，该概念在不能利用所有条件独立性语句时仍然适用。

    

    约束性因果发现算法通过系统地测试数据中观察到的条件独立性来学习因果图的一部分结构。这些算法，如PC算法及其变体，依赖于由Pearl提出的所谓因果图等价类的图形表征。然而，当数据有限时，约束性因果发现算法往往面临困难，因为条件独立性检验很快失去统计能力，尤其是当条件集很大时。为了解决这个问题，我们提出使用条件独立性检验，在鲁棒的因果发现中将条件集的大小上限设置为某个整数 k。然而，现有的因果图等价类的图形表征在我们不能利用所有的条件独立性语句时不适用。我们首先定义了 k-马尔可夫等价的概念：如果两个因果图得到相同的条件独立性语句，它们是 k-马尔可夫等价的。

    Constraint-based causal discovery algorithms learn part of the causal graph structure by systematically testing conditional independences observed in the data. These algorithms, such as the PC algorithm and its variants, rely on graphical characterizations of the so-called equivalence class of causal graphs proposed by Pearl. However, constraint-based causal discovery algorithms struggle when data is limited since conditional independence tests quickly lose their statistical power, especially when the conditioning set is large. To address this, we propose using conditional independence tests where the size of the conditioning set is upper bounded by some integer $k$ for robust causal discovery. The existing graphical characterizations of the equivalence classes of causal graphs are not applicable when we cannot leverage all the conditional independence statements. We first define the notion of $k$-Markov equivalence: Two causal graphs are $k$-Markov equivalent if they entail the same c
    
[^230]: 揭示基于问题的量子神经网络在多类分类上的功效

    Demystify Problem-Dependent Power of Quantum Neural Networks on Multi-Class Classification. (arXiv:2301.01597v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2301.01597](http://arxiv.org/abs/2301.01597)

    本研究揭示了基于问题的量子神经网络在多类分类任务上的功效，发现训练损失主导着其性能，与深度神经分类器的双峰风险曲线相反。此外，发现最优量子神经分类器与Helstrom边界和等角紧框之间存在内在联系，并提出了一种基于最小角度的优化方法。

    

    量子神经网络（QNNs）已成为理解物理世界的重要工具，但它们的优势和局限性尚未完全理解。一些使用特定编码方法的QNNs可以通过经典代理有效地模拟，而具有量子记忆的其他QNNs可能比经典分类器表现更好。在这里，我们系统地调查了量子神经分类器（QCs）在多类分类任务上的问题相关能力。通过对期望风险的分析，该指标综合考虑了分类器的训练损失和泛化误差，我们发现了两个关键发现：首先，训练损失主导着功效，而不是泛化能力；第二，QCs经历U形风险曲线，与深度神经分类器的双峰风险曲线相反。我们还揭示了最优QCs与Helstrom边界和等角紧框之间的内在联系。基于这些发现，我们提出了一种方法，其中我们通过量子待测试样本与最优QCs之间的最小角度，实现了基于问题的量子神经分类器的优化。

    Quantum neural networks (QNNs) have become an important tool for understanding the physical world, but their advantages and limitations are not fully understood. Some QNNs with specific encoding methods can be efficiently simulated by classical surrogates, while others with quantum memory may perform better than classical classifiers. Here we systematically investigate the problem-dependent power of quantum neural classifiers (QCs) on multi-class classification tasks. Through the analysis of expected risk, a measure that weighs the training loss and the generalization error of a classifier jointly, we identify two key findings: first, the training loss dominates the power rather than the generalization ability; second, QCs undergo a U-shaped risk curve, in contrast to the double-descent risk curve of deep neural classifiers. We also reveal the intrinsic connection between optimal QCs and the Helstrom bound and the equiangular tight frame. Using these findings, we propose a method that 
    
[^231]: 基于JKO方案的可逆归一化流神经网络

    Invertible normalizing flow neural networks by JKO scheme. (arXiv:2212.14424v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2212.14424](http://arxiv.org/abs/2212.14424)

    本文提出了一种基于JKO方案的可逆归一化流神经网络，通过按块进行残差块的训练，减少了内存负载和深度流网络训练的难度。并且通过自适应时间重新参数化的流网络，在概率空间中逐步细化轨迹，从而提高了模型的训练效率和准确性。

    

    归一化流是一类用于高效采样和密度估计的深度生成模型。实际中，流通常表示为一系列可逆的神经网络模块链; 为了便于训练，现有的工作对流轨迹进行了正则化，并设计了特殊的网络架构。本文提出了受Jordan-Kinderleherer-Otto (JKO)方案启发的神经ODE流网络，它允许有效地按块进行残差块的训练，无需采样SDE轨迹或分数匹配或变分学习的内循环。由于JKO方案展开了梯度流的动态，所提出的模型自然地逐个堆叠残差网络块，降低了内存负载和进行端到端深度流网络训练的难度。我们还开发了自适应时间重新参数化的流网络，通过在概率空间中逐步细化轨迹，提高了模型的训练效率和准确性。

    Normalizing flow is a class of deep generative models for efficient sampling and density estimation. In practice, the flow often appears as a chain of invertible neural network blocks; to facilitate training, existing works have regularized flow trajectories and designed special network architectures. The current paper develops a neural ODE flow network inspired by the Jordan-Kinderleherer-Otto (JKO) scheme, which allows efficient block-wise training of the residual blocks without sampling SDE trajectories or inner loops of score matching or variational learning. As the JKO scheme unfolds the dynamic of gradient flow, the proposed model naturally stacks residual network blocks one by one, reducing the memory load and difficulty in performing end-to-end deep flow network training. We also develop adaptive time reparameterization of the flow network with a progressive refinement of the trajectory in probability space, which improves the model training efficiency and accuracy in practice.
    
[^232]: 双重平滑GDA：用于非凸-非凹极小极大优化的全局收敛算法

    Doubly Smoothed GDA: Global Convergent Algorithm for Constrained Nonconvex-Nonconcave Minimax Optimization. (arXiv:2212.12978v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2212.12978](http://arxiv.org/abs/2212.12978)

    本文提出了一种双重平滑梯度下降上升法 (DSGDA)，该算法可以应用于非凸-非凹极小极大优化，并且能够全局收敛并消除极限环。在一定条件下，DSGDA 的迭代复杂度达到了文献中单循环算法的最佳结果。

    

    非凸-非凹极小极大优化近年来受到了广泛的关注，其在机器学习中具有广泛的应用。然而，大多数现有算法不能保证全局收敛，甚至会遭受极限环的困扰。为了解决这个问题，我们提出了一种新颖的单循环算法，称为双重平滑梯度下降上升法 (DSGDA)，它能够自然地平衡原始与对偶更新，并且将极其具有挑战性的非凸-非凹例子中的极限环消除，包括 Forsaken，Bilinearly-coupled minimax，Sixth-order polynomial 和 PolarGame。我们进一步证明，在一个单侧的 $\theta\in(0,1)$ Kurdyka-\L{}ojasiewicz条件（或凸原始/凹对偶函数）下，DSGDA 可以找到一个游戏平衡点，并且具有迭代复杂度 $\mathcal{O}(\epsilon^{-2\max\{2\theta,1\}})$（或 $\mathcal{O}(\epsilon^{-4})$），这些与文献中单循环算法的最佳结果相匹配。

    Nonconvex-nonconcave minimax optimization has received intense attention over the last decade due to its broad applications in machine learning. Unfortunately, most existing algorithms cannot be guaranteed to converge globally and even suffer from limit cycles. To address this issue, we propose a novel single-loop algorithm called doubly smoothed gradient descent ascent method (DSGDA), which naturally balances the primal and dual updates. The proposed DSGDA can get rid of limit cycles in various challenging nonconvex-nonconcave examples in the literature, including Forsaken, Bilinearly-coupled minimax, Sixth-order polynomial, and PolarGame. We further show that under an one-sided Kurdyka-\L{}ojasiewicz condition with exponent $\theta\in(0,1)$ (resp. convex primal/concave dual function), DSGDA can find a game-stationary point with an iteration complexity of $\mathcal{O}(\epsilon^{-2\max\{2\theta,1\}})$ (resp. $\mathcal{O}(\epsilon^{-4})$). These match the best results for single-loop al
    
[^233]: 物理学知识指导的高斯过程回归应用于解决线性偏微分方程

    Physics-Informed Gaussian Process Regression Generalizes Linear PDE Solvers. (arXiv:2212.12474v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.12474](http://arxiv.org/abs/2212.12474)

    本文使用物理学知识指导的高斯过程回归方法，解决线性偏微分方程求解器无法量化近似误差的问题。

    

    线性偏微分方程是一类重要且广泛应用的机械模型，描述了物理过程，例如热传导、电磁学和波传播等。实践中，通常使用基于离散化的专门数值方法来解决偏微分方程。这些求解器通常使用未知模型参数的估计值以及如果可用的话，物理测量值用于初始化。这些求解器经常嵌入到具有下游应用的更大的科学模型中，因此误差量化起着关键作用。然而，经典的偏微分方程求解器忽略参数和测量不确定性，可能无法产生一致性的估计值，以用于计算其固有的逼近误差。本文通过将求解线性偏微分方程解释为物理学知识指导的高斯过程回归来解决这个问题。我们的框架基于高斯过程推理定理的一个关键推广，该定理适用于通过任意界面进行观察的情况。

    Linear partial differential equations (PDEs) are an important, widely applied class of mechanistic models, describing physical processes such as heat transfer, electromagnetism, and wave propagation. In practice, specialized numerical methods based on discretization are used to solve PDEs. They generally use an estimate of the unknown model parameters and, if available, physical measurements for initialization. Such solvers are often embedded into larger scientific models with a downstream application and thus error quantification plays a key role. However, by ignoring parameter and measurement uncertainty, classical PDE solvers may fail to produce consistent estimates of their inherent approximation error. In this work, we approach this problem in a principled fashion by interpreting solving linear PDEs as physics-informed Gaussian process (GP) regression. Our framework is based on a key generalization of the Gaussian process inference theorem to observations made via an arbitrary bou
    
[^234]: 关于因果高斯过程经验协方差最小特征值的研究

    A note on the smallest eigenvalue of the empirical covariance of causal Gaussian processes. (arXiv:2212.09508v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2212.09508](http://arxiv.org/abs/2212.09508)

    本研究提出了一个简单的证明方法，用于约束因果高斯过程中经验协方差的最小特征值。同时，我们还建立了一个因果分解，用于建立高斯二次形式的单侧尾部不等式。我们的结果对于向量自回归的最小二乘识别具有性能保证。

    

    我们提出了一个简单的证明方法，用于约束因果高斯过程中经验协方差的最小特征值。在证明过程中，我们使用了因果分解来建立高斯二次形式的单侧尾部不等式。我们的证明只使用了关于高斯分布和并集界的基本事实。最后，我们给出了一个例子，为向量自回归的最小二乘识别提供了性能保证。

    We present a simple proof for bounding the smallest eigenvalue of the empirical covariance in a causal Gaussian process. Along the way, we establish a one-sided tail inequality for Gaussian quadratic forms using a causal decomposition. Our proof only uses elementary facts about the Gaussian distribution and the union bound. We conclude with an example in which we provide a performance guarantee for least squares identification of a vector autoregression.
    
[^235]: 关于奖励推断对错误人类模型的敏感性

    On the Sensitivity of Reward Inference to Misspecified Human Models. (arXiv:2212.04717v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.04717](http://arxiv.org/abs/2212.04717)

    本研究从理论和实证角度研究了奖励推断对错误人类模型的敏感性，并发现存在可能构建小的对抗性偏差的情况。

    

    从人类行为中推断奖励函数是与价值对齐密切相关的内容 - 确保人工智能的目标与我们人类实际想要的一致。但这需要建立人类的行为模型。经过几十年的认知科学、神经科学和行为经济学研究，获得准确的人类模型仍然是一个开放的研究课题。这引出了一个问题：模型的准确性对于奖励推断的准确性有多重要？一方面，如果模型中存在小错误就会导致推断的灾难性错误，那么奖励学习的整个框架似乎注定失败，因为我们永远无法拥有完美的人类行为模型。另一方面，如果随着模型的改进，可以保证奖励的准确性也会提高，这将证明在模型方面做更多工作的益处。我们在理论和实证方面研究了这个问题。我们确实展示了构建小的对抗性偏差是可能的

    Inferring reward functions from human behavior is at the center of value alignment - aligning AI objectives with what we, humans, actually want. But doing so relies on models of how humans behave given their objectives. After decades of research in cognitive science, neuroscience, and behavioral economics, obtaining accurate human models remains an open research topic. This begs the question: how accurate do these models need to be in order for the reward inference to be accurate? On the one hand, if small errors in the model can lead to catastrophic error in inference, the entire framework of reward learning seems ill-fated, as we will never have perfect models of human behavior. On the other hand, if as our models improve, we can have a guarantee that reward accuracy also improves, this would show the benefit of more work on the modeling side. We study this question both theoretically and empirically. We do show that it is unfortunately possible to construct small adversarial biases 
    
[^236]: 离线强化学习中的置信度条件值函数

    Confidence-Conditioned Value Functions for Offline Reinforcement Learning. (arXiv:2212.04607v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.04607](http://arxiv.org/abs/2212.04607)

    本文提出了一种条件值函数的学习方法，该方法在离线强化学习中处理了数据集和学习策略之间的分布偏移，并可以在训练和评估时根据保守程度动态选择策略。

    

    离线强化学习（RL）承诺能够仅使用现有的静态数据集学习有效的策略，而不需要任何昂贵的在线交互。为了做到这一点，离线RL方法必须处理数据集和学习策略之间的分布偏移。最常见的方法是学习保守或下限值函数，它们低估了超出分布的行为的回报。然而，这种方法存在一个显著缺点：在这些值函数上优化的策略只能根据固定的、可能是次优的保守程度来行为。然而，如果我们能够在训练时学习不同保守程度的策略，并设计一种方法在评估时动态选择其中之一，这个问题就可以得到缓解。为此，在这项工作中，我们提出学习附加条件的值函数，这些值函数依赖于保守程度，我们将其称为置信度条件值函数。

    Offline reinforcement learning (RL) promises the ability to learn effective policies solely using existing, static datasets, without any costly online interaction. To do so, offline RL methods must handle distributional shift between the dataset and the learned policy. The most common approach is to learn conservative, or lower-bound, value functions, which underestimate the return of out-of-distribution (OOD) actions. However, such methods exhibit one notable drawback: policies optimized on such value functions can only behave according to a fixed, possibly suboptimal, degree of conservatism. However, this can be alleviated if we instead are able to learn policies for varying degrees of conservatism at training time and devise a method to dynamically choose one of them during evaluation. To do so, in this work, we propose learning value functions that additionally condition on the degree of conservatism, which we dub confidence-conditioned value functions. We derive a new form of a Be
    
[^237]: 将预训练蛋白质语言模型集成到几何深度学习网络中

    Integration of Pre-trained Protein Language Models into Geometric Deep Learning Networks. (arXiv:2212.03447v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.03447](http://arxiv.org/abs/2212.03447)

    将预训练的蛋白质语言模型与几何深度学习网络相结合，提高了几何网络的表征能力，并在多个蛋白质学习任务上取得了良好的性能。

    

    几何深度学习在非欧几里得领域取得了巨大成功，学习大型生物分子的三维结构正在成为一个新兴的研究领域。然而，由于结构数据数量有限，其有效性受到很大限制。与此同时，针对丰富的一维序列训练的蛋白质语言模型在各种应用中展现出了不断增长的能力。之前的研究中有几个尝试将这些不同的蛋白质模态组合起来，以提升几何神经网络的表征能力，但未能给出对其优势的全面理解。本研究将经过良好训练的蛋白质语言模型的知识集成到几种最先进的几何网络中，并评估了多种蛋白质表征学习基准，包括蛋白质相互作用预测、模型质量评估、蛋白质相互作用刚体对接和结合亲和力预测。我们的发现

    Geometric deep learning has recently achieved great success in non-Euclidean domains, and learning on 3D structures of large biomolecules is emerging as a distinct research area. However, its efficacy is largely constrained due to the limited quantity of structural data. Meanwhile, protein language models trained on substantial 1D sequences have shown burgeoning capabilities with scale in a broad range of applications. Several previous studies consider combining these different protein modalities to promote the representation power of geometric neural networks, but fail to present a comprehensive understanding of their benefits. In this work, we integrate the knowledge learned by well-trained protein language models into several state-of-the-art geometric networks and evaluate a variety of protein representation learning benchmarks, including protein-protein interface prediction, model quality assessment, protein-protein rigid-body docking, and binding affinity prediction. Our findings
    
[^238]: 一种解决离线强化学习分布偏移的风险规避方法

    One Risk to Rule Them All: Addressing Distributional Shift in Offline Reinforcement Learning via Risk-Aversion. (arXiv:2212.00124v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.00124](http://arxiv.org/abs/2212.00124)

    本研究提出了一种基于风险规避机制的离线强化学习方法，同时解决了避免分布偏移和避免灾难性结果的风险问题，并取得了明显的改进。

    

    离线强化学习（RL）适用于在线探索不可行的安全关键领域。在这种领域中，决策应考虑到灾难性结果的风险。换句话说，决策应该是风险规避的。离线RL的另一个挑战是避免分布偏移，即确保策略访问的状态-操作对靠近数据集中的状态-操作对。以往的研究将离线RL技术（以避免分布偏移）与风险敏感型RL算法（以实现风险规避）相结合。在本文中，我们提出了将风险规避机制作为同时解决这两个问题的方法。我们提出一种基于模型的方法，使用模型集合来估计认知不确定性和随机不确定性。我们训练了一个风险规避的策略，避免高不确定性的行为。对认知不确定性的风险规避可以防止分布偏移，因为避免了数据集中未涵盖的区域。我们在几个标准基准测试中展示了这种方法的有效性，并显示出比先前处理这些挑战的工作明显的改进。

    Offline reinforcement learning (RL) is suitable for safety-critical domains where online exploration is not feasible. In such domains, decision-making should take into consideration the risk of catastrophic outcomes. In other words, decision-making should be risk-averse. An additional challenge of offline RL is avoiding distributional shift, i.e. ensuring that state-action pairs visited by the policy remain near those in the dataset. Previous works on risk in offline RL combine offline RL techniques (to avoid distributional shift), with risk-sensitive RL algorithms (to achieve risk-aversion). In this work, we propose risk-aversion as a mechanism to jointly address both of these issues. We propose a model-based approach, and use an ensemble of models to estimate epistemic uncertainty, in addition to aleatoric uncertainty. We train a policy that is risk-averse, and avoids high uncertainty actions. Risk-aversion to epistemic uncertainty prevents distributional shift, as areas not covered 
    
[^239]: 使用马尔可夫链蒙特卡洛方法，近似计算线性统计形状模型之间的相交和差异

    Approximating Intersections and Differences Between Linear Statistical Shape Models Using Markov Chain Monte Carlo. (arXiv:2211.16314v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.16314](http://arxiv.org/abs/2211.16314)

    本文提出了一种使用马尔可夫链蒙特卡洛方法近似计算线性统计形状模型之间相交和差异的新方法。该方法能够对比两个模型的相交空间和集合差异，并生成新的统计形状模型。

    

    到目前为止，对统计形状模型 (SSMs) 的比较通常仅基于性能，通过简单的度量指标（如紧致性、泛化性或特异性）进行。实际形状空间之间的相似性或差异无法可视化或量化。在本文中，我们提出了一种新方法，通过计算模型所跨越的（超椭球形）可允许形状域之间的近似相交空间和集合差异，对比两个线性SSM在密集对应上进行定性比较。为此，我们使用马尔可夫链蒙特卡洛方法近似计算处于相交空间中的形状分布，并随后应用主成分分析 (PCA) 对后验样本进行处理，最终得到相交空间的新SSM。我们以类似的方式估计线性SSM之间的差异；然而，在这种情况下，所得空间不再是凸的，我们不再应用PCA，而是使用后验样本进行处理。

    To date, the comparison of Statistical Shape Models (SSMs) is often solely performance-based, carried out by means of simplistic metrics such as compactness, generalization, or specificity. Any similarities or differences between the actual shape spaces can neither be visualized nor quantified. In this paper, we present a new method to qualitatively compare two linear SSMs in dense correspondence by computing approximate intersection spaces and set-theoretic differences between the (hyper-ellipsoidal) allowable shape domains spanned by the models. To this end, we approximate the distribution of shapes lying in the intersection space using Markov chain Monte Carlo and subsequently apply Principal Component Analysis (PCA) to the posterior samples, eventually yielding a new SSM of the intersection space. We estimate differences between linear SSMs in a similar manner; here, however, the resulting spaces are no longer convex and we do not apply PCA but instead use the posterior samples for
    
[^240]: 基于数据驱动的网络神经科学：关于数据收集与基准的研究

    Data-Driven Network Neuroscience: On Data Collection and Benchmark. (arXiv:2211.12421v3 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2211.12421](http://arxiv.org/abs/2211.12421)

    本文提供了一个全面且高质量的人脑功能网络数据集，用于神经科学、机器学习和图分析的交叉研究。它利用解剖学和功能性磁共振成像来理解人脑的功能连接，并具有识别潜在神经退行性疾病的重要性。通过利用机器学习和图分析研究大脑的网络形式，能够预测这些疾病的早期发生。脑网络以图形方式表示，保留了丰富的结构和位置信息，传统的检查方法无法捕捉到这些信息。然而，缺乏公开可访问的脑网络数据限制了数据驱动的研究。

    

    本文提供了一份全面且高质量的人脑功能网络数据集，用于神经科学、机器学习和图分析的交叉研究。解剖学和功能性磁共振成像被用于理解人脑的功能连接，并且在识别阿尔茨海默氏症、帕金森症和自闭症等潜在的神经退行性疾病方面尤为重要。最近，利用机器学习和图分析研究以脑网络的形式来研究大脑的方法变得越来越流行，特别是用于预测这些疾病的早期发生。作为一个图形表示的脑网络保留了丰富的结构和位置信息，传统的检查方法无法捕捉到这些信息。然而，缺乏公开可访问的脑网络数据限制了研究人员进行数据驱动的探索。其中主要的困难在于复杂的领域特定的预处理步骤。

    This paper presents a comprehensive and quality collection of functional human brain \emph{network} data for potential research in the intersection of neuroscience, machine learning, and graph analytics. Anatomical and functional MRI images have been used to understand the functional connectivity of the human brain and are particularly important in identifying underlying neurodegenerative conditions such as Alzheimer's, Parkinson's, and Autism. Recently, the study of the brain in the form of brain networks using machine learning and graph analytics has become increasingly popular, especially to predict the early onset of these conditions. A brain network, represented as a graph, retains rich structural and positional information that traditional examination methods are unable to capture. However, the lack of publicly accessible brain network data prevents researchers from data-driven explorations. One of the main difficulties lies in the complicated domain-specific preprocessing steps 
    
[^241]: MelHuBERT: 一种基于Mel频谱图的简化HuBERT模型

    MelHuBERT: A simplified HuBERT on Mel spectrograms. (arXiv:2211.09944v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.09944](http://arxiv.org/abs/2211.09944)

    MelHuBERT是基于Mel频谱图的简化版HuBERT模型，通过改进损失函数、输入表示和多阶段训练，在语音识别方面取得了有利表现，节省了31.2%的预训练时间和33.5%的计算资源。

    

    自监督模型在学习语音表示方面取得了巨大的成功，可以推广到各种下游任务。然而，大多数自监督模型需要大量的计算资源和多个GPU来进行训练，从而严重限制了自监督学习的发展。为了减少训练的计算量，我们重新审视了HuBERT的训练方法，这是一个非常成功的自监督模型。我们改进并简化了几个关键组成部分，包括损失函数、输入表示和多阶段训练。我们的模型MelHuBERT在音素识别、说话人识别和自动语音识别方面均能取得较好的性能，同时节省了31.2%的预训练时间，或等效地每秒语音节省了33.5%的MACs。代码和预训练模型可在https://github.com/nervjack2/MelHuBERT中获得。

    Self-supervised models have had great success in learning speech representations that can generalize to various downstream tasks. However, most self-supervised models require a large amount of compute and multiple GPUs to train, significantly hampering the development of self-supervised learning. In an attempt to reduce the computation of training, we revisit the training of HuBERT, a highly successful self-supervised model. We improve and simplify several key components, including the loss function, input representation, and training in multiple stages. Our model, MelHuBERT, is able to achieve favorable performance on phone recognition, speaker identification, and automatic speech recognition against HuBERT, while saving 31.2% of the pre-training time, or equivalently 33.5% MACs per one second speech. The code and pre-trained models are available in https://github.com/nervjack2/MelHuBERT.
    
[^242]: 在评估转移对抗攻击中的良好实践

    Towards Good Practices in Evaluating Transfer Adversarial Attacks. (arXiv:2211.09565v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2211.09565](http://arxiv.org/abs/2211.09565)

    本文提出了良好的实践来解决现有评估中的限制，首次全面评估了转移对抗攻击，并发现了新的攻击特点和最佳超参数。

    

    转移对抗攻击在真实世界、黑盒子场景中引发了关键的安全问题。然而，由于现有评估中存在两个常见限制，这个领域的实际进展很难评估。首先，不同方法往往没有进行系统和公正的一对一比较评估。其次，只评估了可转移性，而另一个关键的攻击属性——隐蔽性则被大部分忽视了。在这项工作中，我们设计了良好的实践来解决这些限制，并且我们对转移攻击进行了首次全面的评估，涵盖了对ImageNet上9种防御措施的23种代表性攻击。特别地，我们提出将现有攻击分为五大类，这样可以进行系统的类别分析。这些分析得出了新的发现，甚至挑战了现有的知识，并且有助于确定我们的攻击全面评估中的最佳攻击超参数。我们还特别注意了隐蔽性方面。

    Transfer adversarial attacks raise critical security concerns in real-world, black-box scenarios. However, the actual progress of this field is difficult to assess due to two common limitations in existing evaluations. First, different methods are often not systematically and fairly evaluated in a one-to-one comparison. Second, only transferability is evaluated but another key attack property, stealthiness, is largely overlooked. In this work, we design good practices to address these limitations, and we present the first comprehensive evaluation of transfer attacks, covering 23 representative attacks against 9 defenses on ImageNet. In particular, we propose to categorize existing attacks into five categories, which enables our systematic category-wise analyses. These analyses lead to new findings that even challenge existing knowledge and also help determine the optimal attack hyperparameters for our attack-wise comprehensive evaluation. We also pay particular attention to stealthines
    
[^243]: 在有向无环图上的Transformer

    Transformers over Directed Acyclic Graphs. (arXiv:2210.13148v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13148](http://arxiv.org/abs/2210.13148)

    本文研究了在有向无环图上的Transformer。通过改进注意机制和位置编码，本方法在多种任务上表现出了很好的效果。

    

    最近，Transformer模型在图表示学习中变得流行起来，因为它们有能力学习超出常规图神经网络捕捉到的复杂关系。主要的研究问题是如何将图的结构偏差注入到Transformer的架构中，并针对有向无环图（DAGs）提出了一些适应性的架构改进：（1）一个比常规Transformer的二次复杂度更高效的注意机制，同时忠实地捕捉了DAGs的结构，（2）一个对DAG的偏序进行位置编码，补充了前者。我们对我们的方法在各种类型的任务上进行了严格的评估，从对源代码图的分类到对引用网络中的节点，结果显示它在两个重要的任务上是有效的。

    Transformer models have recently gained popularity in graph representation learning as they have the potential to learn complex relationships beyond the ones captured by regular graph neural networks. The main research question is how to inject the structural bias of graphs into the transformer architecture, and several proposals have been made for undirected molecular graphs and, recently, also for larger network graphs. In this paper, we study transformers over directed acyclic graphs (DAGs) and propose architecture adaptations tailored to DAGs: (1) An attention mechanism that is considerably more efficient than the regular quadratic complexity of transformers and at the same time faithfully captures the DAG structure, and (2) a positional encoding of the DAG's partial order, complementing the former. We rigorously evaluate our approach over various types of tasks, ranging from classifying source code graphs to nodes in citation networks, and show that it is effective in two importan
    
[^244]: 论多动作策略梯度

    On Many-Actions Policy Gradient. (arXiv:2210.13011v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13011](http://arxiv.org/abs/2210.13011)

    本论文研究了具有多个动作样本的随机策略梯度的方差问题，并提出了一种基于动态模型的多动作采样方法，实现了更高的样本效率和更高的回报。

    

    我们研究了具有多个动作样本的随机策略梯度（SPG）的方差。我们得出了一个多动作最优性条件，它决定了当与比例扩展轨迹的单动作代理相比，多动作SPG产生比较低的方差。我们提出了一种称为基于模型的多动作（MBMA）的方法，在SPG背景下利用动态模型进行多动作采样，以解决现有多动作SPG实现所涉及的问题，并在模型模拟的回合中提供与SPG相当的偏差和方差。我们发现，MBMA的偏差和方差结构与理论预测的相匹配。因此，在一系列连续动作环境中，MBMA与无模型，多动作和基于模型的策略梯度基线相比，实现了更高的样本效率和更高的回报。

    We study the variance of stochastic policy gradients (SPGs) with many action samples per state. We derive a many-actions optimality condition, which determines when many-actions SPG yields lower variance as compared to a single-action agent with proportionally extended trajectory. We propose Model-Based Many-Actions (MBMA), an approach leveraging dynamics models for many-actions sampling in the context of SPG. MBMA addresses issues associated with existing implementations of many-actions SPG and yields lower bias and comparable variance to SPG estimated from states in model-simulated rollouts. We find that MBMA bias and variance structure matches that predicted by theory. As a result, MBMA achieves improved sample efficiency and higher returns on a range of continuous action environments as compared to model-free, many-actions, and model-based on-policy SPG baselines.
    
[^245]: 研究异构神经网络中的神经干扰

    Investigating Neuron Disturbing in Fusing Heterogeneous Neural Networks. (arXiv:2210.12974v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12974](http://arxiv.org/abs/2210.12974)

    该论文提出了研究异构神经网络中神经干扰现象的理论分析和实验验证，并提出了一种通过自适应选择本地模型来执行预测的方法。实验结果表明，该方法在数据异质性方面更加鲁棒。

    

    将在不同位置上训练的深度学习模型融合成一个全局模型是联邦学习的直接实现。尽管当前的模型融合方法在融合几乎相同结构的神经网络时在实验上是有效的，但它们很少被进行理论分析。本文从贝叶斯观点结合来自客户端的数据异质性和神经网络的特性，揭示了神经元干扰的现象，即异构本地模型的神经元相互干扰。此外，为了验证我们的发现，我们提出了一种通过自适应选择本地模型来执行预测的实验方法，称为AMS，以排除神经元干扰并融合神经网络。实验结果表明，AMS在数据异质性方面比一般的模型融合和集成方法更加鲁棒。

    Fusing deep learning models trained on separately located clients into a global model in a one-shot communication round is a straightforward implementation of Federated Learning. Although current model fusion methods are shown experimentally valid in fusing neural networks with almost identical architectures, they are rarely theoretically analyzed. In this paper, we reveal the phenomenon of neuron disturbing, where neurons from heterogeneous local models interfere with each other mutually. We give detailed explanations from a Bayesian viewpoint combining the data heterogeneity among clients and properties of neural networks. Furthermore, to validate our findings, we propose an experimental method that excludes neuron disturbing and fuses neural networks via adaptively selecting a local model, called AMS, to execute the prediction according to the input. The experiments demonstrate that AMS is more robust in data heterogeneity than general model fusion and ensemble methods. This implies
    
[^246]: 关于临床文本挖掘的跨领域预训练语言模型：在数据受限微调中它们表现如何？

    On Cross-Domain Pre-Trained Language Models for Clinical Text Mining: How Do They Perform on Data-Constrained Fine-Tuning?. (arXiv:2210.12770v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.12770](http://arxiv.org/abs/2210.12770)

    本研究探讨了在临床自然语言处理任务中，将从一般领域或相关领域数据预训练的迁移学习模型微调到特定任务上的有效性。实验结果显示，微调的语言模型相对于从头开始学习的模型在命名实体识别任务上取得了更好的性能。

    

    在自然语言处理领域，使用从一般或相关领域数据预训练的大型语言模型（LLMs）来将其微调到特定领域和任务上，并使用新任务中可用的有限资源进行微调，一直以来都是一个流行的实践。在本研究中，我们重新考虑了这种假设，并在临床自然语言处理领域进行了研究，具体是在药物及其相关属性的命名实体识别任务上。我们比较了从头开始学习的Transformer模型和通过微调BERT-based LLMs（包括BERT-base、BioBERT和ClinicalBERT）进行微调的模型。我们还对这些模型及其扩展模型与带有CRF层的连续学习进行了比较。我们使用n2c2-2018共享任务数据进行模型开发和评估。实验结果表明：1）CRF层对所有神经模型都起到了积极的影响；2）在使用宏平均F1对BIO-strict跨度级别进行评估时，微调的LLMs获得了0.83+的得分，而从头开始学习的TransformerCRF模型得分为0.78+，证明了微调模型的优势。

    Fine-tuning Large Language Models (LLMs) pre-trained from general or related domain data to a specific domain and task using a limited amount of resources available in the new task has been a popular practice in NLP fields. In this work, we re-visit this assumption, and carry out investigation in clinical NLP, specifically named-entity recognition on Drugs and their related Attributes. We compare Transformer models that are learned from scratch to fine-tuning BERT-based LLMs including BERT-base, BioBERT, and ClinicalBERT. We also investigate the comparison of such models and their extended models with a CRF layer for continuous learning. We use n2c2-2018 shared task data for model development and evaluations. The experimental outcomes show that 1) the CRF layer makes a difference for all neural models; 2) on BIO-strict span level evaluation using macro-average F1, while the fine-tuned LLMs achieved scores 0.83+, the TransformerCRF model learned from scratch achieved 0.78+ demonstrating
    
[^247]: 自共轭障碍哈密尔顿蒙特卡洛的无偏约束采样

    Unbiased constrained sampling with Self-Concordant Barrier Hamiltonian Monte Carlo. (arXiv:2210.11925v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.11925](http://arxiv.org/abs/2210.11925)

    本文提出了一种名为BHMC的新的蒙特卡罗采样算法，能够从定义了约束的黎曼流形中进行无偏采样，其中包含一种新的过滤步骤involution checking step。

    

    本文提出了障碍哈密尔顿蒙特卡罗(BHMC)，它是HMC算法的一种变体，旨在从带有自共轭障碍度量的流形中的Gibbs分布π中进行采样。该方法依赖于包含度量的Hamiltonian动力学。因此，它包含定义流形的约束，并能够利用其底层几何形状。然而，相应的Hamilton动力学是通过不可分离的常微分方程来定义的，与欧几里得情况相反。这意味着将HMC推广到黎曼流形中会产生不可避免的偏差。为解决这个问题，我们提出了一种新的过滤步骤，称为“involution检查步骤”。该步骤在两个BHMC版本——连续BHMC(c-BHMC)和数值BHMC(n-BHMC)中实现。我们的主要结果表明，这两个新算法生成可逆Markov链且无偏。

    In this paper, we propose Barrier Hamiltonian Monte Carlo (BHMC), a version of the HMC algorithm which aims at sampling from a Gibbs distribution $\pi$ on a manifold $\mathrm{M}$, endowed with a Hessian metric $\mathfrak{g}$ derived from a self-concordant barrier. Our method relies on Hamiltonian dynamics which comprises $\mathfrak{g}$. Therefore, it incorporates the constraints defining $\mathrm{M}$ and is able to exploit its underlying geometry. However, the corresponding Hamiltonian dynamics is defined via non separable Ordinary Differential Equations (ODEs) in contrast to the Euclidean case. It implies unavoidable bias in existing generalization of HMC to Riemannian manifolds. In this paper, we propose a new filter step, called "involution checking step", to address this problem. This step is implemented in two versions of BHMC, coined continuous BHMC (c-BHMC) and numerical BHMC (n-BHMC) respectively. Our main results establish that these two new algorithms generate reversible Mark
    
[^248]: 重新思考偏见缓解：更公平的架构实现更公平的人脸识别

    Rethinking Bias Mitigation: Fairer Architectures Make for Fairer Face Recognition. (arXiv:2210.09943v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.09943](http://arxiv.org/abs/2210.09943)

    通过进行公平性的神经架构搜索，我们发现偏见是神经网络架构本身固有的，而不仅仅是训练数据的影响。我们的方法在人脸识别等难题上取得了比其他方法更好的准确性和公平性。

    

    人脸识别系统广泛应用于执法等安全关键应用中，但它们在性别和种族等社会人口统计维度上存在偏见。传统观点认为，模型偏见源于有偏的训练数据。因此，以往关于偏见缓解的工作主要集中在预处理训练数据、在训练过程中添加惩罚项以防止偏见影响模型，或对预测结果进行后处理以消除偏见，但这些方法在人脸识别等难题上的成功有限。在我们的工作中，我们发现偏见实际上根源于神经网络架构本身。基于这一重新定义，我们首次进行了公平性的神经架构搜索，同时进行了超参数搜索。我们的搜索输出了一系列在准确性和公平性方面均优于其他高性能架构和现有偏见缓解方法的模型。

    Face recognition systems are widely deployed in safety-critical applications, including law enforcement, yet they exhibit bias across a range of socio-demographic dimensions, such as gender and race. Conventional wisdom dictates that model biases arise from biased training data. As a consequence, previous works on bias mitigation largely focused on pre-processing the training data, adding penalties to prevent bias from effecting the model during training, or post-processing predictions to debias them, yet these approaches have shown limited success on hard problems such as face recognition. In our work, we discover that biases are actually inherent to neural network architectures themselves. Following this reframing, we conduct the first neural architecture search for fairness, jointly with a search for hyperparameters. Our search outputs a suite of models which Pareto-dominate all other high-performance architectures and existing bias mitigation methods in terms of accuracy and fairne
    
[^249]: 具有无限制内存的在线凸优化

    Online Convex Optimization with Unbounded Memory. (arXiv:2210.09903v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.09903](http://arxiv.org/abs/2210.09903)

    本论文提出了一种新的在线凸优化框架，可以处理决策历史的长期依赖关系，并介绍了用于量化依赖程度的$p$-有效内存容量的概念。

    

    在线凸优化（OCO）是在线学习中广泛使用的框架。然而，在很多应用中，学习者的损失不仅取决于当前的决策，还取决于直到那个时间点的所有决策历史。本文引入了一种OCO的扩展框架，“具有无限制内存的在线凸优化”，来捕捉对过去决策的长期依赖关系，并介绍了$p$-有效内存容量的概念，$H_p$，它量化了$p$阶影响的最大值。

    Online convex optimization (OCO) is a widely used framework in online learning. In each round, the learner chooses a decision in a convex set and an adversary chooses a convex loss function, and then the learner suffers the loss associated with their current decision. However, in many applications the learner's loss depends not only on the current decision but on the entire history of decisions until that point. The OCO framework and its existing generalizations do not capture this, and they can only be applied to many settings of interest after a long series of approximation arguments. They also leave open the question of whether the dependence on memory is tight because there are no non-trivial lower bounds. In this work we introduce a generalization of the OCO framework, ``Online Convex Optimization with Unbounded Memory'', that captures long-term dependence on past decisions. We introduce the notion of $p$-effective memory capacity, $H_p$, that quantifies the maximum influence of p
    
[^250]: ConSpec: 突出强化学习中的关键步骤，实现快速学习和泛化

    ConSpec: honing in on critical steps for rapid learning and generalization in RL. (arXiv:2210.05845v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.05845](http://arxiv.org/abs/2210.05845)

    ConSpec是一个新的强化学习算法，通过离线对比学习来确定任务中的关键步骤，实现快速学习和泛化。该算法通过学习关键步骤的原型，并在当前状态匹配时提供内在奖励，具有快速识别关键步骤和可解释的信用分配的优势。

    

    在现实生活中，成功往往取决于多个关键步骤，这些步骤在时间上相距较远，与最终奖励也相距甚远。传统的强化学习方法在信用分配方面依赖Bellman方程，很难识别这些关键步骤。本文提出了一种新的强化学习算法，使用离线对比学习来确定关键步骤。这个算法被称为对比内省（ConSpec），可以添加到任何现有的强化学习算法中。ConSpec通过一种新颖的对比损失学习任务中的关键步骤的原型，并在当前状态与这些原型之一匹配时提供内在奖励。ConSpec中的原型在信用分配方面具有两个关键优势：（1）它们使得能够迅速识别所有关键步骤；（2）它们以容易解释的方式实现这一点，使得在感觉特征改变时可以进行超出分布的泛化。与其他当代的强化学习方法不同，

    In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on critical steps. This algorithm, which we call contrastive introspection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of these prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (1) They enable rapid identification of all the critical steps. (2) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL
    
[^251]: SimSCOOD: Fine-tuned源代码模型的超分布泛化的系统分析

    SimSCOOD: Systematic Analysis of Out-of-Distribution Generalization in Fine-tuned Source Code Models. (arXiv:2210.04802v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2210.04802](http://arxiv.org/abs/2210.04802)

    本文是第一个系统研究超分布泛化问题的方法，通过模拟不同维度的源代码数据属性和微调方法，在不同场景中研究了模型的行为。

    

    大型代码数据集已经越来越容易地用于预训练源代码模型。然而，对于微调阶段来说，获取代表性的训练数据以充分覆盖特定下游任务的代码分布仍然具有挑战性，原因是任务特定性和有限的标注资源。此外，微调预训练模型可能会导致遗忘以前获得的预训练知识。这些问题导致了超分布泛化问题，即模型的推理行为出现意外情况，这尚未进行系统研究。在本文中，我们提出了第一个系统方法，模拟了不同维度源代码数据属性的各种超分布场景，并研究了这些场景中微调模型的行为。我们研究了不同微调方法（包括全微调和低秩适应微调方法）下模型的行为。我们在各个系统上进行了全面分析。

    Large code datasets have become increasingly accessible for pre-training source code models. However, for the fine-tuning phase, obtaining representative training data that fully covers the code distribution for specific downstream tasks remains challenging due to the task-specific nature and limited labeling resources. Moreover, fine-tuning pretrained models can result in forgetting previously acquired pre-training knowledge. These lead to out-of-distribution (OOD) generalization issues with unexpected model inference behaviors that have not been systematically studied yet. In this paper, we contribute the first systematic approach that simulates various OOD scenarios along different dimensions of source code data properties and study the fine-tuned model behaviors in such scenarios. We investigate the behaviors of models under different fine-tuning methodologies, including full fine-tuning and Low-Rank Adaptation (LoRA) fine-tuning methods. Our comprehensive analysis, conducted on fo
    
[^252]: 基于谱方法的项目反应理论

    A Spectral Approach to Item Response Theory. (arXiv:2210.04317v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.04317](http://arxiv.org/abs/2210.04317)

    本文提出了一种基于谱方法的项目反应理论算法，通过计算马尔科夫链的平稳分布来估计模型参数，具有良好的优化性能和有限样本误差保证。

    

    Rasch模型是项目反应理论中最基础的模型之一，广泛应用于教育测试和推荐系统。本文提出了一种新的项目估计算法，核心是计算在项目-项目图上定义的马尔科夫链的平稳分布。

    The Rasch model is one of the most fundamental models in \emph{item response theory} and has wide-ranging applications from education testing to recommendation systems. In a universe with $n$ users and $m$ items, the Rasch model assumes that the binary response $X_{li} \in \{0,1\}$ of a user $l$ with parameter $\theta^*_l$ to an item $i$ with parameter $\beta^*_i$ (e.g., a user likes a movie, a student correctly solves a problem) is distributed as $\Pr(X_{li}=1) = 1/(1 + \exp{-(\theta^*_l - \beta^*_i)})$. In this paper, we propose a \emph{new item estimation} algorithm for this celebrated model (i.e., to estimate $\beta^*$). The core of our algorithm is the computation of the stationary distribution of a Markov chain defined on an item-item graph. We complement our algorithmic contributions with finite-sample error guarantees, the first of their kind in the literature, showing that our algorithm is consistent and enjoys favorable optimality properties. We discuss practical modification
    
[^253]: VoLTA: 弱监督本地特征对齐的视觉语言Transformer

    VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment. (arXiv:2210.04135v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.04135](http://arxiv.org/abs/2210.04135)

    VoLTA是一种采用弱监督对齐策略的视觉语言Transformer模型，通过在本地特征上进行图像和文本的对齐，实现了细粒度的图像理解，无需昂贵的边界框标注。

    

    近期研究表明，视觉语言预训练（VLP）在各种单模态和多模态下游应用中非常有效。然而，大多数现有的端到端VLP方法利用高分辨率的图像-文本框数据在精细化的区域级任务（如目标检测、分割和指代表达理解）上表现良好。不幸的是，这样的高分辨率图像配以准确的边界框标注昂贵且难以大规模采集和使用。在这项工作中，我们提出了VoLTA（使用弱监督本地特征对齐的视觉语言Transformer），这是一种新的VLP范式，只使用图像-标题数据，但可以实现细粒度区域级图像理解，避免使用昂贵的框标注。VoLTA采用基于图优化传输的弱监督本地图像块和文本标记对齐，以生成一个显式、自标准化和可解释的低级匹配准则。

    Vision-language pre-training (VLP) has recently proven highly effective for various uni- and multi-modal downstream applications. However, most existing end-to-end VLP methods use high-resolution image-text box data to perform well on fine-grained region-level tasks, such as object detection, segmentation, and referring expression comprehension. Unfortunately, such high-resolution images with accurate bounding box annotations are expensive to collect and use for supervision at scale. In this work, we propose VoLTA (Vision-Language Transformer with weakly-supervised local-feature Alignment), a new VLP paradigm that only utilizes image-caption data but achieves fine-grained region-level image understanding, eliminating the use of expensive box annotations. VoLTA adopts graph optimal transport-based weakly-supervised alignment on local image patches and text tokens to germinate an explicit, self-normalized, and interpretable low-level matching criterion. In addition, VoLTA pushes multi-mo
    
[^254]: Learnware: 小模型实现大作为

    Learnware: Small Models Do Big. (arXiv:2210.03647v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.03647](http://arxiv.org/abs/2210.03647)

    Learnware范式旨在帮助用户充分利用小型模型，实现超越原始目的的事情，以解决当前机器学习技术面临的数据量大、训练难度高、数据安全等问题。

    

    当前机器学习技术存在训练数据量大、训练技能高、连续学习难、遗忘风险大、数据隐私和专有信息泄露等问题，而过去的大模型范式虽然在自然语言处理和计算机视觉应用中取得了惊人的结果，但并未解决这些问题，反而成为严重的碳排放源。该文概述了Learnware范式，让用户不需要从头构建机器学习模型，希望利用小型模型可以实现超越原始目的的事情，其中关键是规范，可以使训练的模型得到充分鉴别。

    There are complaints about current machine learning techniques such as the requirement of a huge amount of training data and proficient training skills, the difficulty of continual learning, the risk of catastrophic forgetting, the leaking of data privacy/proprietary, etc. Most research efforts have been focusing on one of those concerned issues separately, paying less attention to the fact that most issues are entangled in practice. The prevailing big model paradigm, which has achieved impressive results in natural language processing and computer vision applications, has not yet addressed those issues, whereas becoming a serious source of carbon emissions. This article offers an overview of the learnware paradigm, which attempts to enable users not need to build machine learning models from scratch, with the hope of reusing small models to do things even beyond their original purposes, where the key ingredient is the specification which enables a trained model to be adequately identi
    
[^255]: 非负张量的多体逼近

    Many-body Approximation for Non-negative Tensors. (arXiv:2209.15338v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.15338](http://arxiv.org/abs/2209.15338)

    提出了一种名为多体逼近的方法来分解非负张量，通过能量建模来避免全局优化和目标秩选择的困难，可通过考虑模式之间的交互进行全局优化; 在许多任务中都展示了其有效性。

    

    我们提出了一种替代方法来分解非负张量，称为多体逼近。传统的分解方法假设表示具有低秩性，导致全局优化和目标秩选择的困难。我们通过张量的能量建模避免了这些问题，其中张量和其模式分别对应于概率分布和随机变量。我们的模型可以通过考虑模式之间的交互来进行全局优化，可以比秩更直观地进行调整。此外，我们将模式之间的相互作用可视化为张量网络，揭示了多体逼近和低秩逼近之间的非平凡关系。我们在张量完成和逼近中展示了我们方法的有效性。

    We present an alternative approach to decompose non-negative tensors, called many-body approximation. Traditional decomposition methods assume low-rankness in the representation, resulting in difficulties in global optimization and target rank selection. We avoid these problems by energy-based modeling of tensors, where a tensor and its mode correspond to a probability distribution and a random variable, respectively. Our model can be globally optimized in terms of the KL divergence minimization by taking the interaction between variables, i.e. modes, into account that can be tuned more intuitively than ranks. Furthermore, we visualize interactions between modes as tensor networks and reveal a nontrivial relationship between many-body approximation and low-rank approximation. We demonstrate the effectiveness of our approach in tensor completion and approximation.
    
[^256]: 通过隐变量高斯分布的双曲 VAE

    Hyperbolic VAE via Latent Gaussian Distributions. (arXiv:2209.15217v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.15217](http://arxiv.org/abs/2209.15217)

    这项研究提出了一种通过使用高斯流形的潜空间来改进变分自编码器(VAE)的方法。实验证明，这种GM-VAE方法在密度估计和模型驱动的强化学习任务中表现出色，具有较强的数值稳定性。

    

    我们提出了一种高斯流形变分自编码器(GM-VAE)，其潜空间由一组高斯分布组成。已知一维高斯分布集合在 Fisher 信息度量下形成了一个双曲空间，我们称之为高斯流形。为了学习具有高斯流形的 VAE，我们提出了基于 Kullback-Leibler 散度的伪高斯流形正态分布，它是对平方 Fisher-Rao 距离的局部近似，用于定义潜空间上的密度。在实验中，我们展示了 GM-VAE 在两个不同任务上的有效性：图像数据集密度估计和模型驱动的强化学习中的环境建模。GM-VAE 在密度估计任务上超越了其他双曲和欧几里得 VAE 的变体，并在模型驱动的强化学习中展现了竞争性的性能。我们观察到我们的模型提供了强大的数值稳定性，解决了一个常见的限制问题。

    We propose a Gaussian manifold variational auto-encoder (GM-VAE) whose latent space consists of a set of Gaussian distributions. It is known that the set of the univariate Gaussian distributions with the Fisher information metric form a hyperbolic space, which we call a Gaussian manifold. To learn the VAE endowed with the Gaussian manifolds, we propose a pseudo-Gaussian manifold normal distribution based on the Kullback-Leibler divergence, a local approximation of the squared Fisher-Rao distance, to define a density over the latent space. In experiments, we demonstrate the efficacy of GM-VAE on two different tasks: density estimation of image datasets and environment modeling in model-based reinforcement learning. GM-VAE outperforms the other variants of hyperbolicand Euclidean-VAEs on density estimation tasks and shows competitive performance in model-based reinforcement learning. We observe that our model provides strong numerical stability, addressing a common limitation reported 
    
[^257]: 具有大的最坏情况Lipschitz参数的私有随机优化：（非光滑）凸损失的最优速率及其对非凸损失的扩展

    Private Stochastic Optimization With Large Worst-Case Lipschitz Parameter: Optimal Rates for (Non-Smooth) Convex Losses and Extension to Non-Convex Losses. (arXiv:2209.07403v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.07403](http://arxiv.org/abs/2209.07403)

    本论文研究了具有大的最坏情况Lipschitz参数的差分隐私随机优化问题，并提供了一种不依赖于统一Lipschitz参数的接近最优的过量风险界限方法。

    

    我们研究了具有最坏情况Lipschitz参数可能非常大的损失函数的差分隐私（DP）随机优化（SO）。迄今为止，大部分关于DP SO的工作都假设损失在所有数据点上是均匀Lipschitz连续的（即随机梯度在所有数据点上都有界）。虽然这种假设很方便，但通常会导致悲观的过量风险界限。在许多实际问题中，由于异常值，损失在所有数据点上的最坏情况（统一）Lipschitz参数可能非常大。在这种情况下，DP SO的误差界限与损失的最坏情况Lipschitz参数成比例，将会是空洞的。为了解决这些限制，本工作提供了一种接近最优的过量风险界限，不依赖于损失的统一Lipschitz参数。在最近的工作（Wang等人，2020; Kamath等人，2022）的基础上，我们假设随机梯度具有有界的k阶矩

    We study differentially private (DP) stochastic optimization (SO) with loss functions whose worst-case Lipschitz parameter over all data points may be extremely large. To date, the vast majority of work on DP SO assumes that the loss is uniformly Lipschitz continuous over data (i.e. stochastic gradients are uniformly bounded over all data points). While this assumption is convenient, it often leads to pessimistic excess risk bounds. In many practical problems, the worst-case (uniform) Lipschitz parameter of the loss over all data points may be extremely large due to outliers. In such cases, the error bounds for DP SO, which scale with the worst-case Lipschitz parameter of the loss, are vacuous. To address these limitations, this work provides near-optimal excess risk bounds that do not depend on the uniform Lipschitz parameter of the loss. Building on a recent line of work (Wang et al., 2020; Kamath et al., 2022), we assume that stochastic gradients have bounded $k$-th order moments fo
    
[^258]: 使用可扩展的机器学习模型和脑电图研究驾驶中的嗜睡检测性能

    Studying Drowsiness Detection Performance while Driving through Scalable Machine Learning Models using Electroencephalography. (arXiv:2209.04048v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2209.04048](http://arxiv.org/abs/2209.04048)

    本研究通过使用脑电图和机器学习的智能框架，在驾驶场景中检测驾驶员嗜睡状态。结果表明，随机森林（RF）是性能最佳的模型，相比支持向量机（SVM）有更高的f1分数。

    

    背景 / 引言：驾驶员嗜睡是一个重要问题，也是交通事故的主要原因之一。认知神经科学和计算机科学的进展使得可以利用脑-计算机接口（BCIs）和机器学习（ML）检测驾驶员的嗜睡状态。然而，文献中缺乏对使用不同机器学习算法进行综合评估的嗜睡检测性能，并且有必要研究适用于被试群体的可扩展机器学习模型的性能。- 方法：为了解决这些限制，本研究提出了一种智能框架，利用脑-计算机接口和基于脑电图的特征，用于检测驾驶场景中的嗜睡状态。使用SEED-VIG数据集评估了最佳性能模型在个体和群体上的表现。- 结果：结果显示，随机森林（RF）在个体驾驶员上的性能优于其他在文献中使用的模型，如支持向量机（SVM），具有78％的f1分数。

    - Background / Introduction: Driver drowsiness is a significant concern and one of the leading causes of traffic accidents. Advances in cognitive neuroscience and computer science have enabled the detection of drivers' drowsiness using Brain-Computer Interfaces (BCIs) and Machine Learning (ML). However, the literature lacks a comprehensive evaluation of drowsiness detection performance using a heterogeneous set of ML algorithms, and it is necessary to study the performance of scalable ML models suitable for groups of subjects. - Methods: To address these limitations, this work presents an intelligent framework employing BCIs and features based on electroencephalography for detecting drowsiness in driving scenarios. The SEED-VIG dataset is used to evaluate the best-performing models for individual subjects and groups. - Results: Results show that Random Forest (RF) outperformed other models used in the literature, such as Support Vector Machine (SVM), with a 78% f1-score for individual 
    
[^259]: 在响应上进行一般干预的学习不变表示

    Learning Invariant Representations under General Interventions on the Response. (arXiv:2208.10027v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2208.10027](http://arxiv.org/abs/2208.10027)

    本文研究了在响应被干预的情况下学习不变表示的问题，提出了不变匹配属性（IMP）作为一种捕捉干预关系的方法。

    

    如今，从不同环境中收集特征和响应对的观察变得越来越常见。因此，由于分布的变化，我们必须将学习到的预测器应用于具有不同分布的数据。一种有原则的方法是采用结构因果模型来描述训练和测试模型，遵循不变性原则，即给定预测器的条件分布在不同环境中保持不变。然而，在实际情况下，这个原则可能被违反，特别是在响应被干预的情况下。一个自然的问题是，在未知环境中是否仍然可能识别其他形式的不变性以促进预测。为了阐明这种具有挑战性的场景，我们专注于线性结构因果模型（SCMs）并引入不变匹配属性（IMP），这是通过附加特征来捕捉干预的一个明确关系，从而导致另一种形式的不变性。

    It has become increasingly common nowadays to collect observations of feature and response pairs from different environments. As a consequence, one has to apply learned predictors to data with a different distribution due to distribution shifts. One principled approach is to adopt the structural causal models to describe training and test models, following the invariance principle which says that the conditional distribution of the response given its predictors remains the same across environments. However, this principle might be violated in practical settings when the response is intervened. A natural question is whether it is still possible to identify other forms of invariance to facilitate prediction in unseen environments. To shed light on this challenging scenario, we focus on linear structural causal models (SCMs) and introduce invariant matching property (IMP), an explicit relation to capture interventions through an additional feature, leading to an alternative form of invari
    
[^260]: 用于医疗应用的联邦学习：分类、当前趋势、挑战和未来研究方向

    Federated Learning for Medical Applications: A Taxonomy, Current Trends, Challenges, and Future Research Directions. (arXiv:2208.03392v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.03392](http://arxiv.org/abs/2208.03392)

    本文调查了联邦学习在医疗应用中的分类、当前趋势、挑战和未来研究方向。该调查强调了联邦学习在保护隐私和解决安全问题方面的重要性。

    

    随着物联网、人工智能和机器学习/深度学习算法的出现，基于数据驱动的医学应用领域已成为设计强大且可扩展的诊断和预测模型的有望途径。因此，数据驱动的医学应用领域受到了学术界和工业界的广泛关注，在改善医疗服务质量方面取得了显著进展。尽管取得了这些进展，但人们普遍面临着采用基于人工智能的医疗应用的巨大挑战，包括满足安全、隐私和服务质量标准的艰巨任务。联邦学习的最新发展使得在分布式环境中训练复杂的机器学习模型成为可能，并已成为一个活跃的研究领域，特别是在分散的边缘网络中处理医学数据以保护隐私和解决安全问题。为此，本调查论文重点介绍了当前和未来的研究方向。

    With the advent of the IoT, AI and ML/DL algorithms, the landscape of data-driven medical applications has emerged as a promising avenue for designing robust and scalable diagnostic and prognostic models from medical data. Consequently, the realm of data-driven medical applications has garnered significant attention spanning academia and industry, ushering in marked enhancements in healthcare delivery quality. Despite these strides, the adoption of AI-driven medical applications remains hindered by formidable challenges, including the arduous task of meeting security, privacy, and quality of service (QoS) standards. Recent developments in federated learning have made it possible to train complex machine-learned models in a distributed manner and has become an active research domain, particularly processing the medical data at the edge of the network in a decentralized way to preserve privacy and address security concerns. To this end, this survey paper highlights the current and future
    
[^261]: 使用机器学习算法研究血凝素序列在流感病毒宿主预测中的应用

    Dive into Machine Learning Algorithms for Influenza Virus Host Prediction with Hemagglutinin Sequences. (arXiv:2207.13842v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.13842](http://arxiv.org/abs/2207.13842)

    本研究使用机器学习算法，以血凝素序列为基础，利用位置特异性评分矩阵和词嵌入的方法，通过多种评估指标在不同分类水平上评估了机器学习算法，并发现5-grams-transformer神经网络是最有效的算法，可以准确预测流感病毒序列的起源。

    

    流感病毒突变迅速，对公众健康，特别是脆弱群体构成威胁。在历史上，甲型流感病毒在不同物种之间引发过大流行。确定病毒的起源至关重要，以防止疫情的传播。最近，越来越多的人对使用机器学习算法进行病毒序列的快速准确预测产生了兴趣。本研究使用真实测试数据集和各种评估指标以不同分类水平评估机器学习算法。由于血凝素是免疫反应中的主要蛋白质，只使用血凝素序列，并以位置特异性评分矩阵和词嵌入表示。结果表明，5-grams-transformer神经网络是预测病毒序列起源最有效的算法，在较高分类水平上大约有99.54％的AUCPR，98.01％的F1得分和96.60％的MCC。

    Influenza viruses mutate rapidly and can pose a threat to public health, especially to those in vulnerable groups. Throughout history, influenza A viruses have caused pandemics between different species. It is important to identify the origin of a virus in order to prevent the spread of an outbreak. Recently, there has been increasing interest in using machine learning algorithms to provide fast and accurate predictions for viral sequences. In this study, real testing data sets and a variety of evaluation metrics were used to evaluate machine learning algorithms at different taxonomic levels. As hemagglutinin is the major protein in the immune response, only hemagglutinin sequences were used and represented by position-specific scoring matrix and word embedding. The results suggest that the 5-grams-transformer neural network is the most effective algorithm for predicting viral sequence origins, with approximately 99.54% AUCPR, 98.01% F1 score and 96.60% MCC at a higher classification l
    
[^262]: 语言模型显示对推理任务具有类似人类的内容效应

    Language models show human-like content effects on reasoning tasks. (arXiv:2207.07051v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.07051](http://arxiv.org/abs/2207.07051)

    本研究探讨了语言模型在逻辑推理任务中是否像人类一样通过混入内容来影响答案，结果发现大型语言模型的先验期望能够捕捉到这种特征。

    

    抽象推理是智能系统的关键能力。大型语言模型在抽象推理任务上实现了高于随机的性能，但存在许多不完善之处。然而，人类的抽象推理也是不完美的。例如，人类推理受到我们对真实世界的知识和信念的影响，并表现出显著的“内容效应”；当问题的语义内容支持正确的逻辑推理时，人类更可靠地进行推理。这些内容纠缠的推理模式在关于人类智能基本性质的争论中起着核心作用。在这里，我们研究了语言模型是否以类似的方式混入内容来回答逻辑问题，这些语言模型的先验期望捕捉了一些人类知识的特征。我们在三个逻辑推理任务上探索了这个问题：自然语言推理、判断三段论的逻辑有效性和Wason选择任务。我们评估了最先进的大型语言模型的性能。

    Abstract reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections. However, human abstract reasoning is also imperfect. For example, human reasoning is affected by our real-world knowledge and beliefs, and shows notable "content effects"; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns play a central role in debates about the fundamental nature of human intelligence. Here, we investigate whether language models $\unicode{x2014}$ whose prior expectations capture some aspects of human knowledge $\unicode{x2014}$ similarly mix content into their answers to logical problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art large 
    
[^263]: 不确定性量化中的标记是什么？潜在密度模型用于不确定性分类

    What is Flagged in Uncertainty Quantification? Latent Density Models for Uncertainty Categorization. (arXiv:2207.05161v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.05161](http://arxiv.org/abs/2207.05161)

    本论文提出了一种分类框架，用于对不确定性量化方法标记的不确定示例进行分类。通过引入混淆密度矩阵，并将示例分为分布外、边界和高分布误分类区域的三类，本研究为评估不同的不确定性量化方法提供了新的视角和评估基准。

    

    不确定性量化（UQ）对于创建可信赖的机器学习模型至关重要。近年来，出现了许多能够标记可疑样本的UQ方法，然而，这些方法究竟识别了什么内容往往不清楚。在这项工作中，我们提出了一个框架，用于对分类任务中被UQ方法标记为不确定的示例进行分类。我们引入了混淆密度矩阵——基于核的误分类密度的近似，并将其用于将被给定不确定性方法识别为可疑样本的示例分为三类：分布外（OOD）样本、边界（Bnd）样本和处于高分布误分类区域（IDM）的示例。通过大量实验，我们证明了我们的框架为评估不同不确定性量化方法之间的差异提供了一种新的独特视角，从而形成了一个有价值的评估基准。

    Uncertainty Quantification (UQ) is essential for creating trustworthy machine learning models. Recent years have seen a steep rise in UQ methods that can flag suspicious examples, however, it is often unclear what exactly these methods identify. In this work, we propose a framework for categorizing uncertain examples flagged by UQ methods in classification tasks. We introduce the confusion density matrix -- a kernel-based approximation of the misclassification density -- and use this to categorize suspicious examples identified by a given uncertainty method into three classes: out-of-distribution (OOD) examples, boundary (Bnd) examples, and examples in regions of high in-distribution misclassification (IDM). Through extensive experiments, we show that our framework provides a new and distinct perspective for assessing differences between uncertainty quantification methods, thereby forming a valuable assessment benchmark.
    
[^264]: 深度无监督学习算法对入侵检测系统的鲁棒性评估

    Robustness Evaluation of Deep Unsupervised Learning Algorithms for Intrusion Detection Systems. (arXiv:2207.03576v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2207.03576](http://arxiv.org/abs/2207.03576)

    本论文评估了六种最新的深度学习算法在受污染数据上的鲁棒性，结果表明这些算法对于数据污染非常敏感，强调了开发新模型时对于数据扰动的自我防御的重要性。

    

    最近，深度学习在包括计算机视觉、自然语言处理和网络安全等多个领域取得了进展。机器学习已经被证明是建立安全计算机网络的潜在工具，尤其是基于异常检测的入侵检测系统。与启发式方法相比，越来越多的网络安全领域开始广泛采用机器学习方法，因为它们能够直接从数据中进行学习。数据对于机器学习系统的发展至关重要，但也成为了攻击者的潜在目标。数据毒化或污染基本上是一种通过数据欺骗机器学习模型的常用技术之一。本文评估了六种最新的深度学习算法在受污染数据上的鲁棒性。我们的实验表明，本研究中使用的最先进算法对数据污染敏感，同时也揭示了在开发新模型时自我防御对于数据扰动的重要性。

    Recently, advances in deep learning have been observed in various fields, including computer vision, natural language processing, and cybersecurity. Machine learning (ML) has demonstrated its ability as a potential tool for anomaly detection-based intrusion detection systems to build secure computer networks. Increasingly, ML approaches are widely adopted than heuristic approaches for cybersecurity because they learn directly from data. Data is critical for the development of ML systems, and becomes potential targets for attackers. Basically, data poisoning or contamination is one of the most common techniques used to fool ML models through data. This paper evaluates the robustness of six recent deep learning algorithms for intrusion detection on contaminated data. Our experiments suggest that the state-of-the-art algorithms used in this study are sensitive to data contamination and reveal the importance of self-defense against data perturbation when developing novel models, especially
    
[^265]: 机器人学习中的公平性和偏见

    Fairness and Bias in Robot Learning. (arXiv:2207.03444v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2207.03444](http://arxiv.org/abs/2207.03444)

    本文从技术、伦理和法律角度出发，首次调查了机器人学习中的公平性问题。提出了偏见来源的分类法和由其引起的歧视类型，并探讨了不同机器人学习领域中不公平结果的场景和缓解策略。

    

    机器学习显著增强了机器人的能力，使它们能够在人类环境中执行各种任务并适应我们不确定的真实世界。最近各个机器学习领域的研究强调了考虑公平性的重要性，以确保这些算法不会重复人类的偏见，并因此导致具有歧视性的结果。随着机器人学习系统在我们日常生活中执行越来越多的任务，了解这种偏见的影响以防止对某些人群产生意外行为至关重要。在这项工作中，我们从技术、伦理和法律挑战的跨学科角度提出了机器人学习中公平性的首次调查。我们提出了偏见来源的分类法和由它们引起的歧视类型。通过不同机器人学习领域的示例，我们探讨了不公平结果的场景和缓解策略。

    Machine learning has significantly enhanced the abilities of robots, enabling them to perform a wide range of tasks in human environments and adapt to our uncertain real world. Recent works in various machine learning domains have highlighted the importance of accounting for fairness to ensure that these algorithms do not reproduce human biases and consequently lead to discriminatory outcomes. With robot learning systems increasingly performing more and more tasks in our everyday lives, it is crucial to understand the influence of such biases to prevent unintended behavior toward certain groups of people. In this work, we present the first survey on fairness in robot learning from an interdisciplinary perspective spanning technical, ethical, and legal challenges. We propose a taxonomy for sources of bias and the resulting types of discrimination due to them. Using examples from different robot learning domains, we examine scenarios of unfair outcomes and strategies to mitigate them. We
    
[^266]: 半监督对比异常值剔除的伪最大期望化方法（SCOPE）

    Semi-supervised Contrastive Outlier removal for Pseudo Expectation Maximization (SCOPE). (arXiv:2206.14261v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.14261](http://arxiv.org/abs/2206.14261)

    SCOPE 提出了一种名为半监督对比异常值剔除的伪最大期望化方法，通过抑制混淆错误来提高半监督学习的性能。

    

    半监督学习是将少量标记数据集与较大的未标记数据集相结合，训练出准确的预测模型的问题。已经开发了许多半监督深度学习方法，包括伪标签、一致性正则化和对比学习技术。然而，伪标签方法容易受到混淆的影响，在早期迭代中，错误的伪标签被认为是真实标签，导致模型加强其先前的偏见，进而无法很好地推广到强大的预测性能。我们提出了一种新方法，通过一个我们称之为半监督对比异常值剔除的伪最大期望化方法（SCOPE）来抑制混淆错误。像基本的伪标签方法一样，SCOPE与最大期望化（EM）相关，EM是一种潜变量框架，可以扩展到理解聚类假设的深度半监督算法。但是，与基本的方法不同。

    Semi-supervised learning is the problem of training an accurate predictive model by combining a small labeled dataset with a presumably much larger unlabeled dataset. Many methods for semi-supervised deep learning have been developed, including pseudolabeling, consistency regularization, and contrastive learning techniques. Pseudolabeling methods however are highly susceptible to confounding, in which erroneous pseudolabels are assumed to be true labels in early iterations, thereby causing the model to reinforce its prior biases and thereby fail to generalize to strong predictive performance. We present a new approach to suppress confounding errors through a method we describe as Semi-supervised Contrastive Outlier removal for Pseudo Expectation Maximization (SCOPE). Like basic pseudolabeling, SCOPE is related to Expectation Maximization (EM), a latent variable framework which can be extended toward understanding cluster-assumption deep semi-supervised algorithms. However, unlike basic
    
[^267]: 《不确定性下的交互式视觉推理》

    Interactive Visual Reasoning under Uncertainty. (arXiv:2206.09203v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2206.09203](http://arxiv.org/abs/2206.09203)

    该论文介绍了一个名为IVRE的交互式环境，在不确定性下评估人工智能代理的推理能力。研究人员通过在IVRE中设置不确定的动作-效果对，要求代理确定对象的角色，并鼓励代理基于观察提出有效且高效的实验来验证假设。

    

    人类的基本认知能力之一是通过生成假设并通过积极试验来迅速解决不确定性。当遇到伴随着模糊的因果关系的新现象时，人类对数据提出假设，通过观察进行推理，通过实验来测试他们的理论，并在不一致出现时修正命题。这些迭代过程持续到底层机制变得清晰为止。在这项工作中，我们设计了一个名为IVRE（读作"ivory"）的环境，用于评估人工智能代理在不确定性下的推理能力。IVRE是一个交互式环境，围绕Blicket检测的丰富场景展开。IVRE中的代理被放置在具有各种模糊的动作-效果对的环境中，并被要求确定每个对象的角色。他们被鼓励基于观察提出有效且高效的实验来验证他们的假设，并积极收集新的信息。

    One of the fundamental cognitive abilities of humans is to quickly resolve uncertainty by generating hypotheses and testing them via active trials. Encountering a novel phenomenon accompanied by ambiguous cause-effect relationships, humans make hypotheses against data, conduct inferences from observation, test their theory via experimentation, and correct the proposition if inconsistency arises. These iterative processes persist until the underlying mechanism becomes clear. In this work, we devise the IVRE (pronounced as "ivory") environment for evaluating artificial agents' reasoning ability under uncertainty. IVRE is an interactive environment featuring rich scenarios centered around Blicket detection. Agents in IVRE are placed into environments with various ambiguous action-effect pairs and asked to determine each object's role. They are encouraged to propose effective and efficient experiments to validate their hypotheses based on observations and actively gather new information. T
    
[^268]: 评估和诱导预训练语言模型的个性

    Evaluating and Inducing Personality in Pre-trained Language Models. (arXiv:2206.07550v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.07550](http://arxiv.org/abs/2206.07550)

    本文提出了Machine Personality Inventory (MPI)数据集用于评估机器个性，通过评估，证明了预训练语言模型(LLM)具有个性。进一步提出了Personality Prompting (P^2)方法，用于以可控的方式诱导LLM具有特定个性。

    

    个性起源于哲学探索，关注个体在思考、情感和行为方面的差异。为了构建能够与人类日常合作的社交机器，我们想知道：现有的大型语言模型(LLMs)是否拥有与人类类似的个性？如果是，我们如何评估它们？进一步地，在此评估框架的基础上，如何以可控的方式诱导具有特定个性的语言模型？为回答这些问题，我们提出了机器个性库(Machine Personality Inventory, MPI)数据集，用于评估机器的个性。MPI遵循标准化的个性测试，基于五因素人格理论和人格评估库建立。通过用MPI系统地评估LLM，我们提供了第一个证据，证明了LLM的个性。我们进一步设计了一种个性提示(Personality Prompting, P^2)方法，以可控的方式诱导LLMs具有特定的个性。

    Originating as a philosophical quest, the study of personality concerns how individuals differ in thinking, feeling, and behaving. Towards building social machines that work with humans on a daily basis, we are motivated to ask: Do existing Large Language Models (LLMs) possess personalities akin to their human counterparts? If so, how can we evaluate them? Further, given this evaluation framework, how can we induce a particular personality in a controllable fashion? To answer these three questions, we propose the Machine Personality Inventory (MPI) dataset for evaluating the machine personality; MPI follows standardized personality tests, built upon the Big Five Personality Factors (Big Five) theory and personality assessment inventories. By systematically evaluating LLMs with MPI, we provide the first piece of evidence showing the existence of personality in LLMs. We further devise a Personality Prompting (P^2) method to induce LLMs with a specific personality in a controllable manner
    
[^269]: 通过可微分物理进行复杂运动技能学习

    Complex Locomotion Skill Learning via Differentiable Physics. (arXiv:2206.02341v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2206.02341](http://arxiv.org/abs/2206.02341)

    本研究提出了一个实用的学习框架，通过可微分物理实现了统一的神经网络控制器，使其具备更高复杂性和多样性。与强化学习相比，该框架的学习效果更好且收敛速度更快。

    

    可微分物理能够通过有效的基于梯度的优化来获得神经网络控制器。然而，现有的工作通常只能提供具有有限能力和泛化能力的神经网络控制器。我们提出了一个实用的学习框架，可以输出统一的具有更高复杂性和多样性的神经网络控制器。为了系统地提高训练的鲁棒性和效率，我们对基准方法进行了一系列改进，包括周期激活函数和定制损失函数。此外，我们发现批处理和Adam优化器在训练复杂运动任务时非常有效。我们在可微分的质点弹簧和材料点法（MPM）模拟中评估了我们的框架，进行了具有挑战性的运动任务和多个机器人设计。实验表明，基于可微分物理的学习框架比强化学习具有更好的结果，并且收敛速度更快。

    Differentiable physics enables efficient gradient-based optimizations of neural network (NN) controllers. However, existing work typically only delivers NN controllers with limited capability and generalizability. We present a practical learning framework that outputs unified NN controllers capable of tasks with significantly improved complexity and diversity. To systematically improve training robustness and efficiency, we investigated a suite of improvements over the baseline approach, including periodic activation functions, and tailored loss functions. In addition, we find our adoption of batching and an Adam optimizer effective in training complex locomotion tasks. We evaluate our framework on differentiable mass-spring and material point method (MPM) simulations, with challenging locomotion tasks and multiple robot designs. Experiments show that our learning framework, based on differentiable physics, delivers better results than reinforcement learning and converges much faster. 
    
[^270]: 数据有效的 GAN 训练中的自我监督增强技术

    Augmentation-Aware Self-Supervision for Data-Efficient GAN Training. (arXiv:2205.15677v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.15677](http://arxiv.org/abs/2205.15677)

    本文提出一种增强感知的自监督判别器用于对生成数据及其增强参数的预测，从而提高判别器的表现和生成模型的性能，实现了数据有效的 GAN 训练。

    

    有限数据情况下训练生成式对抗网络（GAN）是具有挑战性的，因为判别器容易过拟合。先前提出的可微增强技术改善了GAN训练的数据效率。但是，增强技术隐式地引入了不良不变性因素，因为它忽略了由数据转换引起的标签空间语义变化，这可能限制了判别器的表示学习能力，并最终影响生成模型的表现。为了减轻不变性的负面影响，同时继承数据增强的好处，我们提出了一种新的增强感知的自监督判别器，该判别器可以预测增强数据的参数。特别地，真实数据和生成数据的预测目标在训练过程中需要区别开来。我们还鼓励生成器对抗地生成其增强参数可以被判别器准确预测的数据，从而获得更多信息量和更高效的判别器，提高生成模型的性能。多个数据集上的实验表明，我们的方法在数据有效的 GAN 训练中实现了最先进的性能。

    Training generative adversarial networks (GANs) with limited data is challenging because discriminator is prone to overfitting. Previously proposed differentiable augmentation demonstrates improved data efficiency of training GANs. However, the augmentation implicitly introduces undesired invariance to augmentation for the discriminator since it ignores the change of semantics in the label space caused by data transformation, which may limit the representation learning ability of the discriminator and ultimately affect the generative modeling performance of the generator. To mitigate the negative impact of invariance while inheriting the benefits of data augmentation, we propose a novel augmentation-aware self-supervised discriminator that predicts the augmentation parameter of the augmented data. Particularly, the prediction targets of real data and generated data are required to be distinguished since they are different during training. We further encourage the generator to adversari
    
[^271]: DELTA: 多样化客户抽样用于快速联邦学习

    DELTA: Diverse Client Sampling for Fasting Federated Learning. (arXiv:2205.13925v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.13925](http://arxiv.org/abs/2205.13925)

    DELTA 提出了一个无偏抽样方案来减少部分客户端参与所引起的方差，以缓解现有抽样方法可能导致性能下降的问题，它考虑了客户端的多样性和局部方差的影响，并选择具有全局模型更新所需有价值信息的代表性客户端。实验结果表明，DELTA 可以优于其他无偏抽样方案并加速模型收敛速度。

    

    在联邦学习中，部分客户端参与已被广泛应用以高效减少通信负担。然而，不合适的客户端抽样方案可能导致选择出不具代表性子集，从而导致模型更新的显著方差和收敛速度的减慢。现有的抽样方法可能会有偏差，或者可以进一步优化以实现更快的收敛。本文介绍了 DELTA，这是一种设计用于缓解这些问题的无偏抽样方案。DELTA 刻画了客户端的多样性和局部方差的影响，并选择具有全局模型更新所需有价值信息的代表性客户端。此外，DELTA 是一种经过证明的最优无偏抽样方案，在减少由部分客户端参与引起的方差方面优于其他无偏抽样方案。此外，为解决全客户端梯度依赖性，我们提供了一个实用版本的 DELTA，它取决于可用客户端的信息。

    Partial client participation has been widely adopted in Federated Learning (FL) to reduce the communication burden efficiently. However, an inadequate client sampling scheme can lead to the selection of unrepresentative subsets, resulting in significant variance in model updates and slowed convergence. Existing sampling methods are either biased or can be further optimized for faster convergence.In this paper, we present DELTA, an unbiased sampling scheme designed to alleviate these issues. DELTA characterizes the effects of client diversity and local variance, and samples representative clients with valuable information for global model updates. In addition, DELTA is a proven optimal unbiased sampling scheme that minimizes variance caused by partial client participation and outperforms other unbiased sampling schemes in terms of convergence. Furthermore, to address full-client gradient dependence,we provide a practical version of DELTA depending on the available clients' information, 
    
[^272]: INSPIRE：面向密集WLAN中空间复用的分布式贝叶斯优化

    INSPIRE: Distributed Bayesian Optimization for ImproviNg SPatIal REuse in Dense WLANs. (arXiv:2204.10184v3 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2204.10184](http://arxiv.org/abs/2204.10184)

    INSPIRE是一种面向密集WLAN中空间复用的分布式贝叶斯优化解决方案，通过无线传输参数的优化，提高WLAN的性能。

    

    WLANs已经取代有线网络成为连接设备到互联网的主要手段，但由于无线电频谱空间有限，容易出现性能问题。为此，IEEE 802.11ax和随后的修正案旨在通过允许无线传输中两个关键参数（传输功率和灵敏度阈值）的动态更新，提高无线信道的空间复用。本文提出了一种名为INSPIRE的分布式解决方案，它基于高斯过程执行本地贝叶斯优化，以提高WLAN中的空间复用。INSPIRE不对WLAN的拓扑做出明确假设，青睐无线接入点的无私行为，使其找到适当的传输功率和灵敏度阈值参数配置，以实现WLAN的“最大利益”。我们使用ns-3模拟器和两个示例表明了INSPIRE相对于其他最先进策略的优越性。

    WLANs, which have overtaken wired networks to become the primary means of connecting devices to the Internet, are prone to performance issues due to the scarcity of space in the radio spectrum. As a response, IEEE 802.11ax and subsequent amendments aim at increasing the spatial reuse of a radio channel by allowing the dynamic update of two key parameters in wireless transmission: the transmission power (TX_POWER) and the sensitivity threshold (OBSS_PD). In this paper, we present INSPIRE, a distributed solution performing local Bayesian optimizations based on Gaussian processes to improve the spatial reuse in WLANs. INSPIRE makes no explicit assumptions about the topology of WLANs and favors altruistic behaviors of the access points, leading them to find adequate configurations of their TX_POWER and OBSS_PD parameters for the "greater good" of the WLANs. We demonstrate the superiority of INSPIRE over other state-of-the-art strategies using the ns-3 simulator and two examples inspired by
    
[^273]: 关于参数化最优执行和机器学习替代品的研究

    On Parametric Optimal Execution and Machine Learning Surrogates. (arXiv:2204.08581v3 [q-fin.TR] UPDATED)

    [http://arxiv.org/abs/2204.08581](http://arxiv.org/abs/2204.08581)

    本论文研究了离散时间中具有瞬时价格影响和随机弹性的最优订单执行问题。首先，我们扩展了线性瞬时价格影响情况下的最优策略，然后开发了一个基于动态规划和深度学习的数值算法来处理非线性瞬时价格影响情况。我们的方法利用神经网络替代品在参数化学习中具有灵活可伸缩性，对于精确校准价格影响和弹性等参数非常重要。

    

    我们研究了具有瞬时价格影响和随机弹性的离散时间最优订单执行问题。首先，在线性瞬时价格影响的情况下，我们推导出最优策略的闭式递归公式，扩展了Obizhaeva和Wang（2013年，《金融市场杂志》）的确定性结果。其次，我们基于动态规划和深度学习开发了一个数值算法，用于处理Bouchaud等人（2004年，《量化金融》）提出的非线性瞬时价格影响的情况。具体而言，我们利用了一个演员-评论家框架，构建了两个神经网络（NN）的替代品，用于价值函数和反馈控制。NN功能逼近器的灵活可伸缩性使得参数化学习成为可能，即将几个模型或市场参数作为输入空间的一部分。精确校准价格影响、弹性等是极具挑战性的，因此了解执行策略对这些参数的敏感度至关重要。

    We investigate optimal order execution problems in discrete time with instantaneous price impact and stochastic resilience. First, in the setting of linear transient price impact we derive a closed-form recursion for the optimal strategy, extending the deterministic results from Obizhaeva and Wang (J Financial Markets, 2013). Second, we develop a numerical algorithm based on dynamic programming and deep learning for the case of nonlinear transient price impact as proposed by Bouchaud et al. (Quant. Finance, 2004). Specifically, we utilize an actor-critic framework that constructs two neural-network (NN) surrogates for the value function and the feedback control. The flexible scalability of NN functional approximators enables parametric learning, i.e., incorporating several model or market parameters as part of the input space. Precise calibration of price impact, resilience, etc., is known to be extremely challenging and hence it is critical to understand sensitivity of the execution p
    
[^274]: Speculative Decoding: 无损加速自回归翻译

    Speculative Decoding: Lossless Speedup of Autoregressive Translation. (arXiv:2203.16487v5 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2203.16487](http://arxiv.org/abs/2203.16487)

    Speculative Decoding是一种新型解码范式，结合了自回归翻译（AT）和非自回归翻译（NAT）的优势，提供了无损加速的翻译方法。在每个解码步骤中，它推测性地预测下一个标记，并使用验证模型确保翻译结果与AT完全相同。通过推测解码和验证的协作，实现了更快的解码速度，同时保持翻译质量不变。实验证明，原始的SpecDec与AT贪婪解码的结果完全相同。

    

    与之前一些牺牲翻译质量加速自回归翻译（AT）的工作不同，我们提出了Speculative Decoding（SpecDec）-一种受计算机体系结构中的推测执行启发的新型解码范式，它结合了AT和非自回归翻译（NAT）的各自优势，实现了在翻译过程中的无损加速。在每个解码步骤中，SpecDec首先使用NAT模型推测性地预测（即解码）下一个k个标记，然后使用AT模型验证这些标记，只有通过验证的预测标记才会被接受作为解码结果，以确保其翻译结果与AT完全相同。NAT的推测和AT的验证之间的协作使得解码速度大大提高，同时不损失翻译质量，这是由于推测解码所支持的并行计算。我们在4个标准WMT翻译基准上进行实验，并证实原始的SpecDec与AT贪婪解码的结果完全相同，速度提高了约 $k$倍。

    Different from some previous work accelerating autoregressive translation (AT) at the sacrifice of quality, we propose Speculative Decoding (SpecDec) -a novel decoding paradigm inspired by speculative execution in computer architecture, which combines respective advantages of AT and non-autoregressive translation (NAT) for lossless speedup of translation. At each decoding step, SpecDec first speculatively drafts (i.e. decodes) next $k$ tokens with an NAT model and then verifies them with an AT model, where only the drafted tokens passing the verification are accepted as decoded tokens for guaranteeing its translation result is exactly the same as AT. The collaboration of NAT drafting and AT verification leads to a much higher decoding speed without quality loss due to parallel computing enabled by speculative decoding.  We conduct experiments in 4 standard WMT translation benchmarks and confirm the vanilla SpecDec yields exactly the same results as AT greedy decoding with an around $
    
[^275]: 处理学习因果Transformer用于噪声图像分类

    Treatment Learning Causal Transformer for Noisy Image Classification. (arXiv:2203.15529v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.15529](http://arxiv.org/abs/2203.15529)

    本研究提出了一种处理学习因果Transformer（TLT）架构，用于噪声图像分类。通过将"存在噪声"的信息作为处理输入，并联合估计处理效果，TLT能够提高预测准确性，并使用潜在生成模型估计鲁棒的特征表示。

    

    当前顶级的基于深度学习（DL）的视觉模型主要基于探索和利用训练数据样本和其关联标签之间的内在相关性。然而，已知的一个实际挑战是它们在"噪声"数据对抗下性能下降，噪声数据由于不同情况引起，比如虚假相关性、无关背景、领域转移和对抗攻击。在这项工作中，我们将"存在噪声"的二进制信息作为处理输入到图像分类任务中，通过联合估计处理效果来提高预测准确性。受因果变分推断的启发，我们提出了一种基于Transformer的架构，称为处理学习因果Transformer（TLT），它使用潜在的生成模型从当前观测输入中估计鲁棒的特征表示以进行噪声图像分类。根据估计的噪声水平（建模为二进制处理因子），TLT分配相应的推断

    Current top-notch deep learning (DL) based vision models are primarily based on exploring and exploiting the inherent correlations between training data samples and their associated labels. However, a known practical challenge is their degraded performance against "noisy" data, induced by different circumstances such as spurious correlations, irrelevant contexts, domain shift, and adversarial attacks. In this work, we incorporate this binary information of "existence of noise" as treatment into image classification tasks to improve prediction accuracy by jointly estimating their treatment effects. Motivated from causal variational inference, we propose a transformer-based architecture, Treatment Learning Causal Transformer (TLT), that uses a latent generative model to estimate robust feature representations from current observational input for noise image classification. Depending on the estimated noise level (modeled as a binary treatment factor), TLT assigns the corresponding inferen
    
[^276]: 高度几何多模型混合用于鲁棒微调

    Geodesic Multi-Modal Mixup for Robust Fine-Tuning. (arXiv:2203.03897v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.03897](http://arxiv.org/abs/2203.03897)

    本文研究了CLIP模型的多模态嵌入质量，并发现其统一性和对齐性不足，限制了嵌入的传递性和鲁棒性。为了解决这个问题，我们提出了一种新的鲁棒微调方法，通过高度几何多模型混合生成难负样本，并对模型进行微调。

    

    预训练的多模型模型，如CLIP，在各种应用中提供可转移的嵌入，并显示出有希望的结果。然而，对学习到的多模型嵌入的分析相对较少，嵌入的可转移性有待改进。在这项工作中，我们观察到CLIP为两种不同的模态保留了分离的嵌入子空间，并通过统一对齐的视角对其进行了调查，以衡量学习表示的质量。理论上和实证上，我们展示了即使在微调之后，CLIP仍然保持着较差的统一性和对齐性。这种缺乏对齐和统一性可能限制了嵌入的传递性和鲁棒性。为此，我们设计了一种新的用于鲁棒表示的微调方法，提供更好的对齐和统一性。首先，我们提出了一种高度几何多模型混合方法，将图像和文本的嵌入混合在一起，在超球面上生成难负样本。然后，我们对模型进行鲁棒微调。

    Pre-trained multi-modal models, such as CLIP, provide transferable embeddings and show promising results in diverse applications. However, the analysis of learned multi-modal embeddings is relatively unexplored, and the embedding transferability can be improved. In this work, we observe that CLIP holds separated embedding subspaces for two different modalities, and then we investigate it through the lens of uniformity-alignment to measure the quality of learned representation. Both theoretically and empirically, we show that CLIP retains poor uniformity and alignment even after fine-tuning. Such a lack of alignment and uniformity might restrict the transferability and robustness of embeddings. To this end, we devise a new fine-tuning method for robust representation equipping better alignment and uniformity. First, we propose a Geodesic Multi-Modal Mixup that mixes the embeddings of image and text to generate hard negative samples on the hypersphere. Then, we fine-tune the model on har
    
[^277]: 基于$\phi$-离散度的分布鲁棒贝叶斯优化

    Distributionally Robust Bayesian Optimization with $\phi$-divergences. (arXiv:2203.02128v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.02128](http://arxiv.org/abs/2203.02128)

    本研究提出了一种基于$\phi$-离散度的分布鲁棒贝叶斯优化算法。

    

    鲁棒性研究因其在面对不确定性的许多系统中不可避免而受到广泛关注。其中一个例子是贝叶斯优化，它面临着多方面的不确定性，但仅有少量的研究致力于这个方向。在现有研究的基础上，我们提出了一种基于$\phi$-离散度的分布鲁棒贝叶斯优化算法。

    The study of robustness has received much attention due to its inevitability in data-driven settings where many systems face uncertainty. One such example of concern is Bayesian Optimization (BO), where uncertainty is multi-faceted, yet there only exists a limited number of works dedicated to this direction. In particular, there is the work of Kirschner et al. (2020), which bridges the existing literature of Distributionally Robust Optimization (DRO) by casting the BO problem from the lens of DRO. While this work is pioneering, it admittedly suffers from various practical shortcomings such as finite contexts assumptions, leaving behind the main question Can one devise a computationally tractable algorithm for solving this DRO-BO problem? In this work, we tackle this question to a large degree of generality by considering robustness against data-shift in $\phi$-divergences, which subsumes many popular choices, such as the $\chi^2$-divergence, Total Variation, and the extant Kullback-Lei
    
[^278]: 用度量学习增强的最优传输改进分子表示学习

    Improving Molecular Representation Learning with Metric Learning-enhanced Optimal Transport. (arXiv:2202.06208v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.06208](http://arxiv.org/abs/2202.06208)

    本文提出了一种基于最优传输的算法MROT，用于改进分子表示学习和增强其泛化能力。实验证明，MROT在化学性质预测和材料吸附选择等任务中显著优于现有模型，具有加速发现具有期望性质的新物质的潜力。

    

    在许多化学和生物应用中，训练数据通常有限或异构。现有的化学和材料科学的机器学习模型未能考虑训练领域之外的泛化能力。在本文中，我们开发了一种基于最优传输的新算法MROT，用于增强分子回归问题的泛化能力。MROT通过测量一种新的领域距离度量和传输计划上的后验方差正则化，来学习数据的连续标签，以弥合化学领域的差距。在下游任务中，我们考虑了无监督和半监督设置中的基本化学回归任务，包括化学性质预测和材料吸附选择。大量实验表明，MROT显著优于现有模型，在加速发现具有期望性质的新物质方面具有潜在的潜力。

    Training data are usually limited or heterogeneous in many chemical and biological applications. Existing machine learning models for chemistry and materials science fail to consider generalizing beyond training domains. In this article, we develop a novel optimal transport-based algorithm termed MROT to enhance their generalization capability for molecular regression problems. MROT learns a continuous label of the data by measuring a new metric of domain distances and a posterior variance regularization over the transport plan to bridge the chemical domain gap. Among downstream tasks, we consider basic chemical regression tasks in unsupervised and semi-supervised settings, including chemical property prediction and materials adsorption selection. Extensive experiments show that MROT significantly outperforms state-of-the-art models, showing promising potential in accelerating the discovery of new substances with desired properties.
    
[^279]: 利用比特流元数据快速、准确、泛化地压缩视频质量增强

    Leveraging Bitstream Metadata for Fast, Accurate, Generalized Compressed Video Quality Enhancement. (arXiv:2202.00011v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2202.00011](http://arxiv.org/abs/2202.00011)

    本文提出了一种利用视频比特流元数据进行深度学习的压缩视频质量增强方法，在实现更高吞吐量的同时提高了压缩视频的还原准确性，具有重要的应用价值。

    

    视频压缩是现代互联网的核心功能，涵盖从社交媒体到视频会议等技术。虽然视频压缩不断成熟，但在许多压缩设置中，质量损失仍然很明显。然而，这些设置对于在带宽受限或不稳定的连接上高效传输视频具有重要应用。在本文中，我们开发了一个深度学习架构，能够利用嵌入视频比特流中的底层结构和动态信息，为压缩视频恢复细节。我们展示了这种方法相比于以前的压缩校正方法提高了恢复准确性，并且在实现更高吞吐量的同时，与最近的基于深度学习的视频压缩方法在速率-失真比上具有竞争力。此外，我们在模型上进行量化数据条件化，这可轻松地从比特流中获取。这使得我们的单一模型能够处理各种不同的压缩设置。

    Video compression is a central feature of the modern internet powering technologies from social media to video conferencing. While video compression continues to mature, for many compression settings, quality loss is still noticeable. These settings nevertheless have important applications to the efficient transmission of videos over bandwidth constrained or otherwise unstable connections. In this work, we develop a deep learning architecture capable of restoring detail to compressed videos which leverages the underlying structure and motion information embedded in the video bitstream. We show that this improves restoration accuracy compared to prior compression correction methods and is competitive when compared with recent deep-learning-based video compression methods on rate-distortion while achieving higher throughput. Furthermore, we condition our model on quantization data which is readily available in the bitstream. This allows our single model to handle a variety of different c
    
[^280]: 学习与子集叠加

    Learning with Subset Stacking. (arXiv:2112.06251v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.06251](http://arxiv.org/abs/2112.06251)

    提出了一种新的回归算法LESS，通过生成以随机点为中心的子集并训练局部预测器，然后以新颖的方式组合预测器得到整体预测器。在多个数据集上测试表明，LESS是一种有竞争力且高效的监督学习方法。

    

    我们提出了一种新的回归算法，该算法从一组输入-输出对中进行学习。我们的算法适用于输入变量与输出变量之间的关系在预测空间中表现出异质行为的群体。该算法首先生成以输入空间中的随机点为中心的子集，然后为每个子集训练一个局部预测器。然后这些预测器以一种新颖的方式组合在一起，形成一个整体预测器。我们将此算法称为“学习与子集叠加”或LESS，因为它类似于叠加回归器的方法。我们将LESS与多个数据集上的最先进方法进行测试性能比较。我们的比较结果表明，LESS是一种有竞争力的监督学习方法。此外，我们观察到LESS在计算时间上也非常高效，并且可以直接进行并行实现。

    We propose a new regression algorithm that learns from a set of input-output pairs. Our algorithm is designed for populations where the relation between the input variables and the output variable exhibits a heterogeneous behavior across the predictor space. The algorithm starts with generating subsets that are concentrated around random points in the input space. This is followed by training a local predictor for each subset. Those predictors are then combined in a novel way to yield an overall predictor. We call this algorithm ``LEarning with Subset Stacking'' or LESS, due to its resemblance to the method of stacking regressors. We compare the testing performance of LESS with state-of-the-art methods on several datasets. Our comparison shows that LESS is a competitive supervised learning method. Moreover, we observe that LESS is also efficient in terms of computation time and it allows a straightforward parallel implementation.
    
[^281]: CubeTR: 使用Transformer学习解决魔方问题

    CubeTR: Learning to Solve The Rubiks Cube Using Transformers. (arXiv:2111.06036v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.06036](http://arxiv.org/abs/2111.06036)

    CubeTR提出了一种使用Transformer进行强化学习解决魔方问题的方法，并通过关注长序列动作和解决稀疏奖励的问题，实现了从任意起始状态学习如何解决魔方问题，并能够生成接近专业人员解决方案长度的解决方案。

    

    自从Transformer首次出现以来，已经成功地应用于计算机视觉到自然语言处理等各种领域。最近提出将Transformer应用于强化学习，将其重新定义为序列建模问题。与其他常见的强化学习问题相比，魔方问题具有独特的挑战。魔方有着数以千万计的可能组合，但只有一个解决的状态，这导致了极度稀疏的奖励信号。提出的模型CubeTR关注于长序列的动作，并解决了稀疏奖励的问题。CubeTR能够从任意起始状态学习如何解决魔方问题，经过移动规范化后，生成的解决方案长度预期将非常接近专业人员使用的算法给出的解决方案长度。CubeTR为学习算法在更高维度魔方上的泛化能力提供了深入洞见。

    Since its first appearance, transformers have been successfully used in wide ranging domains from computer vision to natural language processing. Application of transformers in Reinforcement Learning by reformulating it as a sequence modelling problem was proposed only recently. Compared to other commonly explored reinforcement learning problems, the Rubiks cube poses a unique set of challenges. The Rubiks cube has a single solved state for quintillions of possible configurations which leads to extremely sparse rewards. The proposed model CubeTR attends to longer sequences of actions and addresses the problem of sparse rewards. CubeTR learns how to solve the Rubiks cube from arbitrary starting states without any human prior, and after move regularisation, the lengths of solutions generated by it are expected to be very close to those given by algorithms used by expert human solvers. CubeTR provides insights to the generalisability of learning algorithms to higher dimensional cubes and 
    
[^282]: 带批量反馈的Lipschitz贪婪算法

    Lipschitz Bandits with Batched Feedback. (arXiv:2110.09722v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.09722](http://arxiv.org/abs/2110.09722)

    本文研究了带有批量反馈的Lipschitz贪婪问题，并提出了一种名为BLiN的新颖算法，该算法可最优地解决该问题。算法在理论上达到了最优遗憾率，并仅使用最小的通信成本。

    

    本文研究带有批量反馈的Lipschitz贪婪问题，其中期望奖励是Lipschitz的，奖励观测结果以批量形式传达给玩家。我们引入了一种新颖的面向算法设计的方法，称为BLiN（Batched Lipschitz Narrowing），可以最优地解决这个问题。具体来说，我们证明了对于Lipschitz奖励的带有$d_z$倍缩放维度的$T$步问题，我们的算法在理论上达到了最优（除对数因子外）的遗憾率$ \widetilde{\mathcal{O}}\left(T^{\frac{d_z+1}{d_z+2}}\right)$，仅使用$ \mathcal{O} \left( \log\log T\right) $批次。我们还为这个问题提供了复杂性分析。我们的理论下界意味着任何算法要达到最优遗憾，必须使用$\Omega(\log\log T)$批次。因此，BLiN使用最小的通信实现了最优遗憾率（除对数因子外）。

    In this paper, we study Lipschitz bandit problems with batched feedback, where the expected reward is Lipschitz and the reward observations are communicated to the player in batches. We introduce a novel landscape-aware algorithm, called Batched Lipschitz Narrowing (BLiN), that optimally solves this problem. Specifically, we show that for a $T$-step problem with Lipschitz reward of zooming dimension $d_z$, our algorithm achieves theoretically optimal (up to logarithmic factors) regret rate $\widetilde{\mathcal{O}}\left(T^{\frac{d_z+1}{d_z+2}}\right)$ using only $ \mathcal{O} \left( \log\log T\right) $ batches. We also provide complexity analysis for this problem. Our theoretical lower bound implies that $\Omega(\log\log T)$ batches are necessary for any algorithm to achieve the optimal regret. Thus, BLiN achieves optimal regret rate (up to logarithmic factors) using minimal communication.
    
[^283]: 基于相似性映射的神经模型重编程用于低资源口语命令分类

    Neural Model Reprogramming with Similarity Based Mapping for Low-Resource Spoken Command Classification. (arXiv:2110.03894v4 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2110.03894](http://arxiv.org/abs/2110.03894)

    本文提出了一种基于相似性映射的神经模型重编程方法，用于低资源口语命令分类。实验证明，在有限的数据条件下，该方法在阿拉伯语和立陶宛语命令数据集上表现优于当前最先进的结果。

    

    本研究提出了一种新颖的对抗重编程（AR）方法，用于低资源口语命令识别（SCR），并构建了一个AR-SCR系统。AR过程旨在修改来自目标领域的声学信号，以重新调整预训练的SCR模型，从而实现重编程。为了解决源域和目标域之间的标签不匹配问题，并进一步提高AR的稳定性，我们提出了一种新颖的基于相似性的标签映射技术来对齐类别。此外，将迁移学习（TL）技术与原始AR过程相结合，以提高模型的适应能力。我们在三个低资源SCR数据集上评估了提出的AR-SCR系统，包括阿拉伯语、立陶宛语和言语障碍性普通话。实验结果表明，在大规模英语数据集上训练的预训练AM的基础上，提出的AR-SCR系统在阿拉伯语和立陶宛语命令数据集上表现优于当前最先进的结果，且仅使用有限数量的数据。

    In this study, we propose a novel adversarial reprogramming (AR) approach for low-resource spoken command recognition (SCR), and build an AR-SCR system. The AR procedure aims to modify the acoustic signals (from the target domain) to repurpose a pretrained SCR model (from the source domain). To solve the label mismatches between source and target domains, and further improve the stability of AR, we propose a novel similarity-based label mapping technique to align classes. In addition, the transfer learning (TL) technique is combined with the original AR process to improve the model adaptation capability. We evaluate the proposed AR-SCR system on three low-resource SCR datasets, including Arabic, Lithuanian, and dysarthric Mandarin speech. Experimental results show that with a pretrained AM trained on a large-scale English dataset, the proposed AR-SCR system outperforms the current state-of-the-art results on Arabic and Lithuanian speech commands datasets, with only a limited amount of 
    
[^284]: FUTURE-AI:医学影像中值得信赖的人工智能的指导原则和共识建议

    FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Medical Imaging. (arXiv:2109.09658v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2109.09658](http://arxiv.org/abs/2109.09658)

    本论文介绍了一系列从经验、共识和最佳实践中提炼出的指导原则，旨在引领医学影像中值得信赖的人工智能的发展，提高信任、安全性和应用水平。

    

    近年来，人工智能和临床系统生成的大量数据的结合，推动了医学影像领域整个价值链上的成像人工智能解决方案的发展，包括图像重建、医学图像分割、基于图像的诊断和治疗规划。尽管医学影像中的人工智能取得了成功并有着巨大的潜力，但许多利益相关者担心成像人工智能解决方案的潜在风险和伦理问题，认为其复杂、不透明、难以理解、难以应用和难以在关键临床应用中建立信任。尽管存在这些担忧和风险，但目前尚没有具体的指导原则和最佳实践来引导未来医学影像中人工智能的发展以增加信任、安全性和采用。为了弥补这一空白，本文提出了从积累的经验、共识和最佳实践中精选出的指导原则。

    The recent advancements in artificial intelligence (AI) combined with the extensive amount of data generated by today's clinical systems, has led to the development of imaging AI solutions across the whole value chain of medical imaging, including image reconstruction, medical image segmentation, image-based diagnosis and treatment planning. Notwithstanding the successes and future potential of AI in medical imaging, many stakeholders are concerned of the potential risks and ethical implications of imaging AI solutions, which are perceived as complex, opaque, and difficult to comprehend, utilise, and trust in critical clinical applications. Despite these concerns and risks, there are currently no concrete guidelines and best practices for guiding future AI developments in medical imaging towards increased trust, safety and adoption. To bridge this gap, this paper introduces a careful selection of guiding principles drawn from the accumulated experiences, consensus, and best practices f
    
[^285]: 医疗档案模型：在医疗保健中的科学和实践应用

    Medical Profile Model: Scientific and Practical Applications in Healthcare. (arXiv:2107.03913v3 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2107.03913](http://arxiv.org/abs/2107.03913)

    本论文提出了一种医疗档案模型，利用Transformer神经网络进行无监督学习，将患者病史表示为疾病的时间序列，并包含人口统计参数，可以成功将医疗知识转移到其他领域。模型训练于超过一百万名患者的数据集，表明在诊断预测任务中具有明显优势。另外，还展示了两个基于该档案模型的应用：一种揭示与疾病相关的假设的新颖方法和从档案模型中提取的患者嵌入。

    

    本文研究了电子健康记录的表示学习问题。我们将患者病史表示为疾病的时间序列，使用基于Transformer的神经网络模型进行无监督学习。此外，嵌入空间还包括人口统计参数，使得可以创建广义的患者个人档案，并成功将医疗知识转移到其他领域。这种医疗档案模型已在超过一百万名患者的数据集上进行了训练。详细的模型分析及与最先进方法的比较表明，在诊断预测任务中具有明显优势。此外，我们展示了基于开发的档案模型的两个应用：首先，一种新颖的Harbinger疾病发现方法，可以揭示与疾病相关的假设，并在流行病学研究设计方面具有潜在益处。其次，从档案模型中提取的患者嵌入用于...

    The paper researches the problem of representation learning for electronic health records. We present the patient histories as temporal sequences of diseases for which embeddings are learned in an unsupervised setup with a transformer-based neural network model. Additionally the embedding space includes demographic parameters which allow the creation of generalized patient profiles and successful transfer of medical knowledge to other domains. The training of such a medical profile model has been performed on a dataset of more than one million patients. Detailed model analysis and its comparison with the state-of-the-art method show its clear advantage in the diagnosis prediction task. Further, we show two applications based on the developed profile model. First, a novel Harbinger Disease Discovery method allowing to reveal disease associated hypotheses and potentially are beneficial in the design of epidemiological studies. Second, the patient embeddings extracted from the profile mod
    
[^286]: 一种应用XAI方法检测DCIS的深度学习模型

    An XAI Approach to Deep Learning Models in the Detection of DCIS. (arXiv:2106.14186v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2106.14186](http://arxiv.org/abs/2106.14186)

    该研究证明了XAI可用于证明辅助人工智能系统的可行性，有效地应用于医疗领域。

    

    结果表明，XAI确实可以作为概念验证的方法，对临床社区内辅助人工智能系统的实施进行讨论。

    The results showed that XAI could indeed be used as a proof of concept to begin discussions on the implementation of assistive AI systems within the clinical community.
    
[^287]: 论连续优化下基于熵损失函数在学习因果结构中的作用

    On the Role of Entropy-based Loss for Learning Causal Structures with Continuous Optimization. (arXiv:2106.02835v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.02835](http://arxiv.org/abs/2106.02835)

    发现因果结构的任务是重要且具有挑战性的，在连续优化中使用的最小二乘损失函数受限于高斯噪声假设。在本研究中，我们提出了一种理论上与任何噪声分布一致的基于熵的损失函数来克服这一限制。

    

    因果发现是许多科学领域中重要且具有挑战性的任务。最近，一种名为NOTEARS的非组合有向无环约束方法将因果结构学习问题转化为使用最小二乘损失的连续优化问题。虽然最小二乘损失函数在标准高斯噪声假设下是合理的，但如果该假设不成立，它会受到限制。在这项工作中，我们从理论上证明了高斯噪声假设的违反将阻碍因果方向的识别，使因果方向完全由因果强度以及线性情况下噪声方差和非线性情况下强非高斯噪声确定。因此，我们提出了一种更一般的基于熵的损失函数，在任何噪声分布下，在理论上与似然分数一致。我们对合成数据和真实世界数据进行了大量的实证评估。

    Causal discovery from observational data is an important but challenging task in many scientific fields. Recently, a method with non-combinatorial directed acyclic constraint, called NOTEARS, formulates the causal structure learning problem as a continuous optimization problem using least-square loss. Though the least-square loss function is well justified under the standard Gaussian noise assumption, it is limited if the assumption does not hold. In this work, we theoretically show that the violation of the Gaussian noise assumption will hinder the causal direction identification, making the causal orientation fully determined by the causal strength as well as the variances of noises in the linear case and by the strong non-Gaussian noises in the nonlinear case. Consequently, we propose a more general entropy-based loss that is theoretically consistent with the likelihood score under any noise distribution. We run extensive empirical evaluations on both synthetic data and real-world d
    
[^288]: 磁性薄膜的储层计算

    Reservoir Computing with Magnetic Thin Films. (arXiv:2101.12700v2 [cs.ET] UPDATED)

    [http://arxiv.org/abs/2101.12700](http://arxiv.org/abs/2101.12700)

    本研究探索了三种磁性薄膜的储层计算，该计算通过利用系统的内部动力学实现非线性投影，提升了模式识别和时间序列分析等任务的性能。

    

    人工智能的进步是受到受大脑启发的技术推动的，但这些技术的功能和能效远远不及生物系统。受神经网络的非线性动力学启发，新的非传统计算硬件出现了，具有利用自然现象和提高效率的潜力，类似于生物系统。物理储层计算通过各种非传统系统（包括光学和记忆电阻系统）展示了这一点。储层计算通过利用系统的内部动力学将任务输入进行非线性投影到高维特征空间中。然后，经过训练的输出层结合这些特征来执行识别模式和时间序列分析等任务。尽管取得了一些进展，但在不进行外部信号处理的情况下实现最先进的性能仍具有挑战性。

    Advances in artificial intelligence are driven by technologies inspired by the brain, but these technologies are orders of magnitude less powerful and energy efficient than biological systems. Inspired by the nonlinear dynamics of neural networks, new unconventional computing hardware has emerged with the potential to exploit natural phenomena and gain efficiency, in a similar manner to biological systems. Physical reservoir computing demonstrates this with a variety of unconventional systems, from optical-based to memristive systems. Reservoir computers provide a nonlinear projection of the task input into a high-dimensional feature space by exploiting the system's internal dynamics. A trained readout layer then combines features to perform tasks, such as pattern recognition and time-series analysis. Despite progress, achieving state-of-the-art performance without external signal processing to the reservoir remains challenging. Here we perform an initial exploration of three magnetic 
    
[^289]: 连续条件生成对抗网络：创新的经验损失和标签输入机制

    Continuous Conditional Generative Adversarial Networks: Novel Empirical Losses and Label Input Mechanisms. (arXiv:2011.07466v9 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2011.07466](http://arxiv.org/abs/2011.07466)

    本文提出了连续条件生成对抗网络（CcGAN），首个用于基于连续标量条件的图像生成的生成模型。通过重新构建经验cGAN损失和提出新的标签输入方法，解决了在回归标签条件生成中存在的问题。

    

    本文提出了连续条件生成对抗网络（CcGAN），这是首个用于基于连续标量条件（称为回归标签）的图像生成的生成模型。现有的条件GAN（cGAN）主要设计用于分类条件（例如类标签）；对于回归标签的条件生成则在数学上有所不同，引发了两个基本问题：（P1）由于某些回归标签可能没有真实图像，最小化现有的经验cGAN损失（也称为经验cGAN损失）在实践中通常不起作用；（P2）由于回归标签是连续的且无限多，传统的标签输入方法不适用。所提出的CcGAN通过分别（S1）重新构建现有的经验cGAN损失以适应连续场景；以及（S2）提出一种简单的标签输入（NLI）方法和一种改进的标签输入（ILI）方法将回归标签融入模型，解决了上述问题。

    This work proposes the continuous conditional generative adversarial network (CcGAN), the first generative model for image generation conditional on continuous, scalar conditions (termed regression labels). Existing conditional GANs (cGANs) are mainly designed for categorical conditions (eg, class labels); conditioning on regression labels is mathematically distinct and raises two fundamental problems:(P1) Since there may be very few (even zero) real images for some regression labels, minimizing existing empirical versions of cGAN losses (aka empirical cGAN losses) often fails in practice;(P2) Since regression labels are scalar and infinitely many, conventional label input methods are not applicable. The proposed CcGAN solves the above problems, respectively, by (S1) reformulating existing empirical cGAN losses to be appropriate for the continuous scenario; and (S2) proposing a naive label input (NLI) method and an improved label input (ILI) method to incorporate regression labels into
    
[^290]: BUSTLE: 通过学习指导的自下而上程序合成

    BUSTLE: Bottom-Up Program Synthesis Through Learning-Guided Exploration. (arXiv:2007.14381v3 [cs.PL] CROSS LISTED)

    [http://arxiv.org/abs/2007.14381](http://arxiv.org/abs/2007.14381)

    这种论文提出了一种利用学习指导自下而上搜索的程序合成方法，通过训练模型优先考虑中间值的组合，利用中间程序的语义信息和具体值的特性进行灵活的解决方案生成。

    

    程序合成的挑战在于在庞大的程序空间中进行搜索的困难。人类程序员通常通过编写子程序并分析其中间结果来以适当的方式组合它们来解决编写复杂程序的任务。在此直觉的基础上，我们提出了一种新的合成方法，利用学习来引导自下而上的程序搜索。具体而言，我们训练一个模型，在给定一组输入-输出示例时，通过学习优先考虑中间值的组合来指导搜索。这是一个强大的组合，因为有几个 emergent properties。首先，在自下而上的搜索中，可以执行中间程序，为神经网络提供语义信息。其次，鉴于这些执行的具体值，我们可以利用基于最近关于属性签名的工作的丰富特性。最后，自下而上的搜索使系统在生成解决方案的顺序上具有相当大的灵活性。

    Program synthesis is challenging largely because of the difficulty of search in a large space of programs. Human programmers routinely tackle the task of writing complex programs by writing sub-programs and then analyzing their intermediate results to compose them in appropriate ways. Motivated by this intuition, we present a new synthesis approach that leverages learning to guide a bottom-up search over programs. In particular, we train a model to prioritize compositions of intermediate values during search conditioned on a given set of input-output examples. This is a powerful combination because of several emergent properties. First, in bottom-up search, intermediate programs can be executed, providing semantic information to the neural network. Second, given the concrete values from those executions, we can exploit rich features based on recent work on property signatures. Finally, bottom-up search allows the system substantial flexibility in what order to generate the solution, al
    
[^291]: BSDAR: 基于注意力奖励的神经关键词生成中的束搜索解码

    BSDAR: Beam Search Decoding with Attention Reward in Neural Keyphrase Generation. (arXiv:1909.09485v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/1909.09485](http://arxiv.org/abs/1909.09485)

    本研究提出了一种基于注意力奖励的束搜索解码策略，用于解决神经关键词生成中的序列长度偏差和束多样性问题，该方法显著提高了生成关键词的解码性能。

    

    本研究主要研究神经关键词生成中的两个常见解码问题：序列长度偏差和束多样性。为了解决这些问题，我们引入了一种基于词级和ngram级奖励函数的束搜索解码策略，以在测试时约束和优化Seq2Seq推理过程。结果表明，我们简单的提案可以克服算法对较短和几乎相同的序列的偏好，从而显著提高生成源文本中存在和不存在的关键词的解码性能。

    This study mainly investigates two common decoding problems in neural keyphrase generation: sequence length bias and beam diversity. To tackle the problems, we introduce a beam search decoding strategy based on word-level and ngram-level reward function to constrain and refine Seq2Seq inference at test time. Results show that our simple proposal can overcome the algorithm bias to shorter and nearly identical sequences, resulting in a significant improvement of the decoding performance on generating keyphrases that are present and absent in source text.
    
[^292]: 针对未预料到的对手测试鲁棒性

    Testing Robustness Against Unforeseen Adversaries. (arXiv:1908.08016v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1908.08016](http://arxiv.org/abs/1908.08016)

    该论文提出了18种新的对抗攻击，并使用这些攻击创建了一个用于评估对各种未预料到的对手的鲁棒性的新基准。作者还发现了一系列防御策略，可以帮助克服训练期间未考虑到的对手的泛化差距。该研究的结果将为研究现实世界最坏情况下的鲁棒性提供有用工具，促进开发更强大的防御措施。

    

    在考虑现实世界的对抗环境时，防御者在训练期间不太可能对所有可能的对手进行训练，并且对手很可能使用逼真的对抗扭曲，而不限于小的L_p约束扰动。为了缩小研究和现实之间的差距，我们介绍了18种新的对抗攻击，并使用它们创建了ImageNet-UA，这是一个用于评估模型对各种未预料到的对手的鲁棒性的新基准。我们利用这个基准来识别一系列能够帮助克服这种泛化差距的防御策略，发现了可以提高对未预料到的攻击的鲁棒性的技术的丰富空间。我们希望ImageNet-UA的更多样性和逼真性将成为那些研究现实世界最坏情况的鲁棒性的人的有用工具，从而促进开发能够在训练期间看不到的攻击中进行泛化的更强大的防御措施。

    When considering real-world adversarial settings, defenders are unlikely to have access to the full range of deployment-time adversaries during training, and adversaries are likely to use realistic adversarial distortions that will not be limited to small L_p-constrained perturbations. To narrow in on this discrepancy between research and reality we introduce eighteen novel adversarial attacks, which we use to create ImageNet-UA, a new benchmark for evaluating model robustness against a wide range of unforeseen adversaries. We make use of our benchmark to identify a range of defense strategies which can help overcome this generalization gap, finding a rich space of techniques which can improve unforeseen robustness. We hope the greater variety and realism of ImageNet-UA will make it a useful tool for those working on real-world worst-case robustness, enabling development of more robust defenses which can generalize beyond attacks seen during training.
    
[^293]: 加权赌博机或者：赌博机如何学习预期之外的扭曲价值

    Weighted bandits or: How bandits learn distorted values that are not expected. (arXiv:1611.10283v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1611.10283](http://arxiv.org/abs/1611.10283)

    本论文研究了带有扭曲概率的随机多臂赌博机问题，并提出了以UCB算法为基础、考虑了奖励扭曲并具有次线性后悔的算法。

    

    受到用于解释常见偏离传统预期价值偏好的人类决策模型的启发，我们提出了两个带有扭曲概率的随机多臂赌博机问题：经典的K臂赌博机和线性参数化赌博机设置。我们在对多臂赌博机的后悔最小化和最佳臂识别框架下研究了上述问题。对于K臂赌博机以及线性赌博机问题的后悔最小化设置，我们提出了受到上置信界(UCB)算法启发、包含奖励扭曲并且具有次线性后悔的算法。对于K臂赌博机设置，我们得出了对我们提出的算法的预期后悔的上界，然后我们证明了一个匹配的下界，以验证我们算法的次线性优化顺序。对于线性参数化设置，我们的算法实现了一个后悔上界，该上界是次线性的。

    Motivated by models of human decision making proposed to explain commonly observed deviations from conventional expected value preferences, we formulate two stochastic multi-armed bandit problems with distorted probabilities on the reward distributions: the classic $K$-armed bandit and the linearly parameterized bandit settings. We consider the aforementioned problems in the regret minimization as well as best arm identification framework for multi-armed bandits. For the regret minimization setting in $K$-armed as well as linear bandit problems, we propose algorithms that are inspired by Upper Confidence Bound (UCB) algorithms, incorporate reward distortions, and exhibit sublinear regret. For the $K$-armed bandit setting, we derive an upper bound on the expected regret for our proposed algorithm, and then we prove a matching lower bound to establish the order-optimality of our algorithm. For the linearly parameterized setting, our algorithm achieves a regret upper bound that is of the 
    

