# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [3D Diffusion Policy](https://arxiv.org/abs/2403.03954) | 3D扩散策略（DP3）是一种新颖的视觉模仿学习方法，通过将3D视觉表示的强大性结合到扩散策略中，成功解决了学习复杂技能所需大量人类演示的问题。 |
| [^2] | [Stop Regressing: Training Value Functions via Classification for Scalable Deep RL](https://arxiv.org/abs/2403.03950) | 通过使用分类代替回归训练值函数，本文提出了一种简单方法来改善深度强化学习的性能和可扩展性 |
| [^3] | [Reconciling Reality through Simulation: A Real-to-Sim-to-Real Approach for Robust Manipulation](https://arxiv.org/abs/2403.03949) | 该论文提出了一种名为RialTo的系统，通过在“数字孪生”模拟环境中进行强化学习来稳健化真实世界的模仿学习策略，以实现在不需要大量不安全真实世界数据采集或广泛人类监督的情况下学习性能优越、稳健的策略。 |
| [^4] | [SPEAR:Exact Gradient Inversion of Batches in Federated Learning](https://arxiv.org/abs/2403.03945) | 该论文提出了第一个能够精确重构批量$b >1$的算法，在联邦学习中解决了梯度反演攻击的问题。 |
| [^5] | [The Heuristic Core: Understanding Subnetwork Generalization in Pretrained Language Models](https://arxiv.org/abs/2403.03942) | 在预训练语言模型中，发现所有子网络都共享一个注意力头集合，被称为启发式核心，这可能是造成子网络泛化差异的原因。 |
| [^6] | [GUIDE: Guidance-based Incremental Learning with Diffusion Models](https://arxiv.org/abs/2403.03938) | GUIDE利用扩散模型和分类器引导技术，针对持续训练模型遗忘的信息产生复习示例，显著减少了灾难性遗忘。 |
| [^7] | [Extreme Precipitation Nowcasting using Transformer-based Generative Models](https://arxiv.org/abs/2403.03929) | 使用基于Transformer的生成模型NowcastingGPT-EVL在极端降水预测中表现出优越性能，尤其在处理极端降水事件时。 |
| [^8] | [Black-Box $k$-to-$1$-PCA Reductions: Theory and Applications](https://arxiv.org/abs/2403.03905) | 我们的主要贡献是对于$k$-PCA算法中近似参数退化的边界得到了显著更为精确的界限 |
| [^9] | [DART: Implicit Doppler Tomography for Radar Novel View Synthesis](https://arxiv.org/abs/2403.03896) | DART提出了一种基于雷达特定物理学的隐式多普勒层析成像方法，通过创建反射率和透射率的呈现流水线，实现了优越的雷达距离-多普勒图像合成。 |
| [^10] | [Joint multi-task learning improves weakly-supervised biomarker prediction in computational pathology](https://arxiv.org/abs/2403.03891) | 使用弱监督联合多任务学习的新方法，在计算病理学中的生物标记预测中实现了+7.7％的性能改进 |
| [^11] | [Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation](https://arxiv.org/abs/2403.03890) | 这项研究提出了一种分层扩散策略（HDP）用于多任务机器人操作，通过将操纵策略分解为分层结构，同时解决长期任务规划和生成细粒度的低层动作，同时提出了一种新颖的机器人运动学感知目标条件控制代理（RK-Diffuser）。 |
| [^12] | [Latent Dataset Distillation with Diffusion Models](https://arxiv.org/abs/2403.03881) | 这项研究提出了使用扩散模型进行潜在数据集蒸馏（LD3M），结合潜在空间中的扩散和数据集蒸馏的方法，以解决不同模型架构导致准确性下降和生成高分辨率图像的挑战。 |
| [^13] | [Graph neural network outputs are almost surely asymptotically constant](https://arxiv.org/abs/2403.03880) | 研究表明，图神经网络的输出将渐近于一个常数函数，并限制了这些分类器的统一表达能力。 |
| [^14] | [Decoupled Vertical Federated Learning for Practical Training on Vertically Partitioned Data](https://arxiv.org/abs/2403.03871) | 提出了Decoupled VFL（DVFL），一种面向VFL的分段学习方法，实现了分散聚合和隔离，从而提高了容错性。 |
| [^15] | [Learning to Decode Collaboratively with Multiple Language Models](https://arxiv.org/abs/2403.03870) | 学习了一种协作解码方法，通过在标记级别交错生成来教授多个大型语言模型协作，无需直接监督，在特定任务中融合每个模型的专业知识，提高了联合系统的性能。 |
| [^16] | [On the Origins of Linear Representations in Large Language Models](https://arxiv.org/abs/2403.03867) | 本文研究了大型语言模型中线性表示的起源，通过引入简单的潜变量模型，并展示了梯度下降的隐性偏差与下一个标记预测目标共同促进了概念的线性表示。 |
| [^17] | [Designing Informative Metrics for Few-Shot Example Selection](https://arxiv.org/abs/2403.03861) | 提出了一种基于复杂度的提示选择方法，用于将示例与测试句子的句法-语义复杂度对齐，在少样本NER任务中取得了显著的性能提升。 |
| [^18] | [Public-data Assisted Private Stochastic Optimization: Power and Limitations](https://arxiv.org/abs/2403.03856) | 该研究研究了公共数据辅助的私有差分隐私算法在随机凸优化问题中的限制和能力，展示出简单策略在这一领域的最优性。 |
| [^19] | [Accelerating Convergence of Score-Based Diffusion Models, Provably](https://arxiv.org/abs/2403.03852) | 设计了新颖的无需训练的算法，以加速流行的确定性和随机采样器，改进了确定性采样器的收敛速率至$O(1/{T}^2)$，提升了随机采样器的收敛速率至$O(1/T)$。 |
| [^20] | [Conformal prediction for multi-dimensional time series by ellipsoidal sets](https://arxiv.org/abs/2403.03850) | 开发了一种名为$\texttt{MultiDimSPCI}$的顺序CP方法，用于在多元时间序列中构建预测区域，具有更小的预测区域和有效的覆盖。 |
| [^21] | [MedMamba: Vision Mamba for Medical Image Classification](https://arxiv.org/abs/2403.03849) | 提出了Vision Mamba用于医学图像分类，结合了卷积层的局部特征提取能力和SSM捕捉长距离依赖性的能力。 |
| [^22] | [Dexterous Legged Locomotion in Confined 3D Spaces with Reinforcement Learning](https://arxiv.org/abs/2403.03848) | 这项研究提出了在受限的三维空间中以目标导向为基础的端到端学习方法，以解决灵巧四肢运动和强化学习之间的挑战。 |
| [^23] | [On the Effectiveness of Distillation in Mitigating Backdoors in Pre-trained Encoder](https://arxiv.org/abs/2403.03846) | 研究了如何利用蒸馏从受污染的预训练编码器中提取良性知识，将其传递给新编码器，成功降低攻击成功率，并探讨了蒸馏的核心组件对性能的影响。 |
| [^24] | [Feature Selection as Deep Sequential Generative Learning](https://arxiv.org/abs/2403.03838) | 将特征选择视为深度顺序生成学习任务，通过开发深度变分转换模型，可以精炼特征选择知识并学习嵌入空间，实现特征子集的生成决策序列。 |
| [^25] | [Cobweb: An Incremental and Hierarchical Model of Human-Like Category Learning](https://arxiv.org/abs/2403.03835) | Cobweb是一种类似人类类别学习系统，采用类别效用度量构建分层组织的类似树状结构，能够捕捉心理效应并在单一模型中展现出实例和原型学习的灵活性，为将来研究人类类别学习提供了基础。 |
| [^26] | [Linear and nonlinear system identification under $\ell_1$- and group-Lasso regularization via L-BFGS-B](https://arxiv.org/abs/2403.03827) | 本文提出了一种基于L-BFGS-B算法的方法，可用于在$\ell_1$和group-Lasso正则化下识别线性和非线性系统，相比传统线性子空间方法，该方法在结果、损失和正则化项使用的通用性以及数值稳定性方面通常提供更好的表现，并且可以广泛应用于各种参数化非线性状态空间模型的识别。 |
| [^27] | [Targeted Variance Reduction: Robust Bayesian Optimization of Black-Box Simulators with Noise Parameters](https://arxiv.org/abs/2403.03816) | 该论文提出了一种针对具有噪声参数的黑盒模拟器的鲁棒贝叶斯优化方法，在优化过程中联合考虑控制参数和不确定性参数，以有效减少方差并充分利用控制与噪声的交互作用。 |
| [^28] | [ProbSAINT: Probabilistic Tabular Regression for Used Car Pricing](https://arxiv.org/abs/2403.03812) | ProbSAINT是一种提出了一种原则性方法来量化其价格预测的不确定性，并且提供了与最先进的提升技术相媲美的准确点预测的模型。 |
| [^29] | [Incentivized Learning in Principal-Agent Bandit Games](https://arxiv.org/abs/2403.03811) | 本文考虑了一个重复的委托-代理赌博游戏，在其中委托方通过提供激励以影响代理方的决策，目标是迭代学习激励策略最大化效用，提出了关于委托方后悔的几乎最优学习算法，支撑理论保证通过数值实验。 |
| [^30] | [Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks](https://arxiv.org/abs/2403.03792) | 通过学习方法生成的执行触发器可以比当前手工制作的触发器更加有效，并表现出形状、属性和功能上的固有灵活性。 |
| [^31] | [KG-TREAT: Pre-training for Treatment Effect Estimation by Synergizing Patient Data with Knowledge Graphs](https://arxiv.org/abs/2403.03791) | KG-TREAT框架通过协同患者数据和知识图谱，引入双重关注的知识图谱和深度双层注意力协同方法，实现了治疗相关因素和结果相关因素的独立编码，表现优于现有方法。 |
| [^32] | [A machine learning workflow to address credit default prediction](https://arxiv.org/abs/2403.03785) | 提出了一种基于工作流程的机器学习方法，用于改进信用违约预测，旨在评估借款人违约信用义务概率，利用多种技术优势解决该任务。 |
| [^33] | [Neural Architecture Search using Particle Swarm and Ant Colony Optimization](https://arxiv.org/abs/2403.03781) | 本研究开发了一种集成用于图像分类的神经结构搜索开源工具系统（OpenNAS），通过采用粒子群和蚁群优化等元启发式方法，自动生成卷积神经网络架构，以显著提高模型准确性。 |
| [^34] | [ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport](https://arxiv.org/abs/2403.03777) | 通过期望回归正则化，本论文提出了一种新的神经优化传输（NOT）训练程序扩展，能够有效地估计最优输运方案，并使学习变得稳定。 |
| [^35] | [Verified Training for Counterfactual Explanation Robustness under Data Shift](https://arxiv.org/abs/2403.03773) | 本文引入了VeriTraCER，一种联合训练分类器和解释器的方法，以显式考虑生成的CEs对小型模型偏移的健壮性。 |
| [^36] | [AcceleratedLiNGAM: Learning Causal DAGs at the speed of GPUs](https://arxiv.org/abs/2403.03772) | 通过有效地并行化现有的因果发现方法，本研究实现了对数千个维度的扩展，特别是将LiNGAM方法并行化，获得了多达32倍的加速。 |
| [^37] | [Joint Sparsity Pattern Learning Based Channel Estimation for Massive MIMO-OTFS Systems](https://arxiv.org/abs/2403.03771) | 提出了一种基于联合稀疏模式学习的信道估计方案，通过利用时延-多普勒-角域信道的潜在联合稀疏性，将信道估计问题转化为稀疏恢复问题，并利用尖峰和板块先验模型以及正交匹配追踪方法，取得了优于现有基线方案的性能提升。 |
| [^38] | [DeepCRE: Revolutionizing Drug R&D with Cutting-Edge Computational Models](https://arxiv.org/abs/2403.03768) | DeepCRE是一种新型的计算模型，在患者级别CRE性能上平均提高了17.7％，在指示级别CRE增加了5倍，并成功确定了六个具有显着优势的药物候选者。 |
| [^39] | [Predicting the Temperature Dependence of Surfactant CMCs Using Graph Neural Networks](https://arxiv.org/abs/2403.03767) | 本研究开发了一个用于预测表面活性剂温度依赖临界胶束浓度的图神经网络模型，填补了现有模型忽视温度依赖性的空白。 |
| [^40] | [Parameterized quantum comb and simpler circuits for reversing unknown qubit-unitary operations](https://arxiv.org/abs/2403.03761) | 通过优化参数化量子电路，我们开发了一个简化的协议，用于逆转未知量子比特酉操作，将辅助比特开销减少到3，显示了量子梳结构的实用性和PQComb在解决复杂量子任务中的潜力。 |
| [^41] | [SUPClust: Active Learning at the Boundaries](https://arxiv.org/abs/2403.03741) | 提出了一种名为SUPClust的新型主动学习方法，旨在识别类别之间的决策边界上的点，通过标记这些点来优化模型预测的性能。 |
| [^42] | [A&B BNN: Add&Bit-Operation-Only Hardware-Friendly Binary Neural Network](https://arxiv.org/abs/2403.03739) | A&B BNN 提出了一种只使用加和位操作的硬件友好二值神经网络，通过引入掩码层和量化 RPReLU 结构，能够更高效地进行计算，并在CIFA数据集上取得了良好的实验结果。 |
| [^43] | [Probabilistic Topic Modelling with Transformer Representations](https://arxiv.org/abs/2403.03737) | 提出了Transformer-Representation神经主题模型（TNTM），结合了transformer嵌入空间中主题表示的优势和概率建模以及变分自动编码器（VAE）框架，实现了主题建模的强大和多功能性 |
| [^44] | [Unifying Generation and Compression: Ultra-low bitrate Image Coding Via Multi-stage Transformer](https://arxiv.org/abs/2403.03736) | 通过引入统一图像生成-压缩（UIGC）范式，本文提出了一种新的多阶段Transformer框架，结合了生成和压缩的过程，以有效利用学习到的先验进行熵估计。 |
| [^45] | [Learning 3D object-centric representation through prediction](https://arxiv.org/abs/2403.03730) | 通过预测未来场景，该研究开发了一种网络架构，同时学习对象分割、3D位置推断和深度感知，从而以类似人类婴儿的约束条件学习物体的表示方式 |
| [^46] | [Bridging Diversity and Uncertainty in Active learning with Self-Supervised Pre-Training](https://arxiv.org/abs/2403.03728) | 通过引入TCM启发式方法，本研究在主动学习中成功结合了多样性采样和不确定性采样策略，解决了冷启动问题并在各种数据水平上表现出色。 |
| [^47] | [Diffusion on language model embeddings for protein sequence generation](https://arxiv.org/abs/2403.03726) | 使用DiMA模型，在蛋白语言模型嵌入进行扩散来生成氨基酸序列，比传统解决方案表现更好，并通过设计选择的影响来量化其优越性能。 |
| [^48] | [Online model error correction with neural networks: application to the Integrated Forecasting System](https://arxiv.org/abs/2403.03702) | 使用神经网络为欧洲中程气象中心的集成预测系统开发模型误差校正，以解决机器学习天气预报模型在表示动力平衡和适用于数据同化实验方面的挑战。 |
| [^49] | [Model Parallelism on Distributed Infrastructure: A Literature Review from Theory to LLM Case-Studies](https://arxiv.org/abs/2403.03699) | 这项研究从理论到LLM案例研究综述了分布式基础设施上的模型并行性，探讨了模型并行性类型、挑战和现代用例。神经网络可以通过操作内部和操作之间并行化，但实施挑战包括操作图的最佳划分。 |
| [^50] | [Towards Controllable Time Series Generation](https://arxiv.org/abs/2403.03698) | 提出了 Controllable Time Series (CTS) 框架，通过解耦映射过程来实现对复杂交互模式的精确学习，从而创新了针对可控时间序列生成 (CTSG) 的方法。 |
| [^51] | [Spectral Phase Transition and Optimal PCA in Block-Structured Spiked models](https://arxiv.org/abs/2403.03695) | 该论文提出了一种针对不均匀尖峰维格纳模型的最优光谱方法，通过对转化矩阵的深入严格分析，在最佳阈值处实现了异常值和正重叠的相位转换。 |
| [^52] | [Simplified PCNet with Robustness](https://arxiv.org/abs/2403.03676) | 本文简化了PCNet并增强了其鲁棒性，通过扩展滤波器阶数并减少参数，以及实现适应性邻域大小的变体，提高了模型对图结构扰动或对抗性攻击的鲁棒性。 |
| [^53] | [Learning Adversarial MDPs with Stochastic Hard Constraints](https://arxiv.org/abs/2403.03672) | 本论文首次研究了涉及对抗损失和硬约束的CMDP，在两种不同情形下设计了具有次线性遗憾的算法，填补了先前研究中对这一问题的空白。 |
| [^54] | [CDC: A Simple Framework for Complex Data Clustering](https://arxiv.org/abs/2403.03670) | 提出了一个简单但有效的复杂数据聚类框架（CDC），能够以线性复杂度高效处理不同类型的数据，并通过图滤波器和高质量锚点来融合几何结构和属性信息，具有很高的集群能力。 |
| [^55] | [Spectral Algorithms on Manifolds through Diffusion](https://arxiv.org/abs/2403.03669) | 本文提出了一种新的视角，将输入数据视为嵌入到更高维欧几里得空间中的低维流形，并研究了在RKHS中谱算法的收敛性能，特别是热核生成的扩散空间，通过积分算子技术推导了关于广义范数的紧收敛上界，使估计器在强意义下收敛到目标函数。 |
| [^56] | [Provable Filter for Real-world Graph Clustering](https://arxiv.org/abs/2403.03666) | 提出了一种可证实的方案，针对实际世界图聚类问题，在处理同源和异源图时表现出色，并构建了低通和高通滤波器来捕捉全面信息。 |
| [^57] | [Environmental Insights: Democratizing Access to Ambient Air Pollution Data and Predictive Analytics with an Open-Source Python Package](https://arxiv.org/abs/2403.03664) | 这个开源Python软件包“Environmental Insights”旨在使大众能够轻松获取环境空气污染数据，进行预测分析，以及通过动态可视化提高用户参与度。 |
| [^58] | [Robust Graph Structure Learning under Heterophily](https://arxiv.org/abs/2403.03659) | 提出了一种稳健图结构学习方法，能够从异质数据中获得高质量图，以用于下游任务 |
| [^59] | [A Survey on Applications of Reinforcement Learning in Spatial Resource Allocation](https://arxiv.org/abs/2403.03643) | 运用强化学习解决空间资源分配问题的新方法具有快速解决方法收敛和强大的模型泛化能力等优势，为这一问题领域提供了新的视角。 |
| [^60] | [Generative Active Learning with Variational Autoencoder for Radiology Data Generation in Veterinary Medicine](https://arxiv.org/abs/2403.03642) | 提出了一种基于变分自动编码器的生成式主动学习框架，用于缓解兽医学CAD系统可靠数据不足的问题，并在心脏增大X光数据集上取得了显著效果。 |
| [^61] | [SheetAgent: A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models](https://arxiv.org/abs/2403.03636) | SheetAgent是一种利用大型语言模型进行电子表格推理和操作的通用代理，提供了处理复杂现实任务的解决方案 |
| [^62] | [Tackling Missing Values in Probabilistic Wind Power Forecasting: A Generative Approach](https://arxiv.org/abs/2403.03631) | 本文提出了一种新的概率风力发电预测方法，通过生成模型估计特征和目标的联合分布，同时预测所有未知值，避免了预处理环节，在连续排名概率得分方面比传统方法表现更优。 |
| [^63] | [GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D Scene Understanding](https://arxiv.org/abs/2403.03608) | GSNeRF通过引入图像语义，实现了对未知场景的新视图图像和相关语义地图的生成，并在图像和语义渲染方面取得了改进性能。 |
| [^64] | [Enhancing Price Prediction in Cryptocurrency Using Transformer Neural Network and Technical Indicators](https://arxiv.org/abs/2403.03606) | 该研究提出了通过引入Transformer神经网络和技术指标来提升加密货币价格预测的方法，该方法对于捕获时间动态和提取重要特征具有显著的优势 |
| [^65] | [Learning Invariant Representations of Graph Neural Networks via Cluster Generalization](https://arxiv.org/abs/2403.03599) | 该论文提出了一种Cluster Information Transfer (CIT) 机制，可以为图神经网络学习不变表示，从而提高对具有结构转移的各种未知测试图的泛化能力。 |
| [^66] | [DeepEclipse: How to Break White-Box DNN-Watermarking Schemes](https://arxiv.org/abs/2403.03590) | DeepEclipse提出了一种新颖统一的框架，用于移除白盒DNN水印，并采用与现有方法显著不同的混淆技术。 |
| [^67] | [Active Adaptive Experimental Design for Treatment Effect Estimation with Covariate Choices](https://arxiv.org/abs/2403.03589) | 该研究提出了一种更有效地估计处理效应的活跃自适应实验设计方法，通过优化协变量密度和倾向得分来降低渐近方差。 |
| [^68] | [RouteExplainer: An Explanation Framework for Vehicle Routing Problem](https://arxiv.org/abs/2403.03585) | 提出了一种车辆路径问题的解释框架RouteExplainer，实现了边的影响解释和意图推断，通过大型语言模型生成解释文本，并在多个VRP上进行了量化评估 |
| [^69] | [Enhancing ASD detection accuracy: a combined approach of machine learning and deep learning models with natural language processing](https://arxiv.org/abs/2403.03581) | 该研究利用机器学习和深度学习模型结合自然语言处理技术，成功提高了ASD检测准确率，尤其在儿童中具有重要意义。 |
| [^70] | [On Transfer in Classification: How Well do Subsets of Classes Generalize?](https://arxiv.org/abs/2403.03569) | 本文的创新之处在于建立了一个部分有序集，用以代表哪些类别子集可以推广到其他类别，并探索了类别子集如何提高测试性能以及少样本学习中迁移的重要性。 |
| [^71] | [Multimodal Anomaly Detection based on Deep Auto-Encoder for Object Slip Perception of Mobile Manipulation Robots](https://arxiv.org/abs/2403.03563) | 提出了一个基于深度自编码器的多模态异常检测方法，用于移动操作机器人的物体滑移感知，能够整合来自不同机器人传感器的数据流并识别异常。 |
| [^72] | [Efficient Algorithms for Empirical Group Distributional Robust Optimization and Beyond](https://arxiv.org/abs/2403.03562) | 该研究提出了一种高效算法，用于解决群分布鲁棒优化问题，通过两级有限和凹凸最小最大优化结构和随机方差减小镜像Prox算法，实现对所有组的方差减少，并支持非恒定学习率。 |
| [^73] | [Population-aware Online Mirror Descent for Mean-Field Games by Deep Reinforcement Learning](https://arxiv.org/abs/2403.03552) | 本文提出了一种通过深度强化学习算法实现基于人口的纳什均衡的方法，通过设计额外的内循环重播缓冲区，智能体可以有效地学习从任何分布实现纳什均衡，从而在多代理系统中展现出更好的收敛特性。 |
| [^74] | [Low-Dose CT Image Reconstruction by Fine-Tuning a UNet Pretrained for Gaussian Denoising for the Downstream Task of Image Enhancement](https://arxiv.org/abs/2403.03551) | 提出了一种通过精调UNet进行低剂量CT图像重建的方法，其中第二阶段的训练策略为CT图像增强阶段。 |
| [^75] | [Diffusion-based Generative Prior for Low-Complexity MIMO Channel Estimation](https://arxiv.org/abs/2403.03545) | 提出了一种基于扩散模型的低复杂度MIMO信道估计器，通过设计轻量级CNN和位置嵌入的学习方法，实现了在稀疏角域中对信道分布进行估计，并采用避免随机重采样的估计策略，以及截断反向扩散步骤的策略，取得了低复杂度和内存开销的效果。 |
| [^76] | [DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training](https://arxiv.org/abs/2403.03542) | 本文提出了一种新的自回归去噪预训练策略，可以更稳定、更高效地在PDE数据上进行预训练，并且通过基于傅里叶注意力的模型架构设计，实现了在大规模预训练中轻松扩展模型，该模型在多个PDE数据集上取得了SOTA表现。 |
| [^77] | [Gadolinium dose reduction for brain MRI using conditional deep learning](https://arxiv.org/abs/2403.03539) | 使用条件深度学习技术，本研究通过利用对比信号提高归纳图像质量，解决了钆剂在脑部MRI中用量减少的挑战 |
| [^78] | [Task Attribute Distance for Few-Shot Learning: Theoretical Analysis and Applications](https://arxiv.org/abs/2403.03535) | 本文介绍了基于任务属性距离（TAD）的度量方法，用于量化训练和新任务之间的关系，以及该关系对不同模型在新任务上适应困难的影响。 |
| [^79] | [FingerNet: EEG Decoding of A Fine Motor Imagery with Finger-tapping Task Based on A Deep Neural Network](https://arxiv.org/abs/2403.03526) | FingerNet是一个专门用于细致手指想象运动分类的网络，能够从EEG信号中提取空间和时间特征，在同一只手内的分类准确度方面表现显著优越。 |
| [^80] | [Non-verbal information in spontaneous speech - towards a new framework of analysis](https://arxiv.org/abs/2403.03522) | 这项研究提出了一个分析框架和技术验证概念，用于对言语中的非言语信号进行分类，并将其与含义关联起来，从而为探索表达实现多层韵律事件的大型数据提供了一种方法。 |
| [^81] | [Probing the Robustness of Time-series Forecasting Models with CounterfacTS](https://arxiv.org/abs/2403.03508) | 提出了CounterfacTS工具，通过反事实探究深度学习模型在时间序列预测中的鲁棒性。 |
| [^82] | [GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection](https://arxiv.org/abs/2403.03507) | GaLore提出了一种名为Gradient Low-Rank Projection (GaLore)的训练策略，相比于一般低秩适应方法，它能够实现更高效的LLM训练，大幅降低内存使用同时保持性能。 |
| [^83] | [A Teacher-Free Graph Knowledge Distillation Framework with Dual Self-Distillation](https://arxiv.org/abs/2403.03483) | 本文提出了一种无教师图自蒸馏框架，不需要教师模型或GNN，仅基于MLP，有助于缩小学术和工业之间的差距。 |
| [^84] | [Inverse-Free Fast Natural Gradient Descent Method for Deep Learning](https://arxiv.org/abs/2403.03473) | 本文提出一种快速自然梯度下降（FNGD）方法，在深度学习中仅需要在第一个时代计算逆运算，避免了迭代求逆操作。 |
| [^85] | [Boosting Meta-Training with Base Class Information for Few-Shot Learning](https://arxiv.org/abs/2403.03472) | 提出了一个端到端的训练范式，通过增强元训练的基础类别信息，解决了Meta-Baseline两个训练阶段固有冲突的问题 |
| [^86] | [Self-Attention Empowered Graph Convolutional Network for Structure Learning and Node Embedding](https://arxiv.org/abs/2403.03465) | 本文提出了一种新颖的图学习框架GCN-SA，通过自注意力机制设计稳定有效的图结构学习模块，可以捕捉长程依赖关系和低同质性图中节点的节点级表示学习。 |
| [^87] | [Interactive Continual Learning Architecture for Long-Term Personalization of Home Service Robots](https://arxiv.org/abs/2403.03462) | 提出了一种新颖的交互式持续学习架构，旨在通过人机交互在家庭环境中持续学习语义知识。 |
| [^88] | [TGPT-PINN: Nonlinear model reduction with transformed GPT-PINNs](https://arxiv.org/abs/2403.03459) | TGPT-PINN通过引入转换层和捕获冲击损失函数组件，在PINNs框架中实现了非线性模型的缩减，有效解决了具有参数依赖性间断的问题。 |
| [^89] | [Slot Abstractors: Toward Scalable Abstract Visual Reasoning](https://arxiv.org/abs/2403.03458) | 提出了槽抽象化器，结合槽方法和关系归纳偏见，实现了可扩展的抽象视觉推理。 |
| [^90] | [Learning Constrained Optimization with Deep Augmented Lagrangian Methods](https://arxiv.org/abs/2403.03454) | 本文提出了一种使用深度增广拉格朗日方法的学习受限制优化的方法，通过训练机器学习模型直接预测对偶解估计，并构建原始估计，从而实现对偶可行解对，同时迭代向原始可行性，模拟对偶上升方法。 |
| [^91] | [SalienTime: User-driven Selection of Salient Time Steps for Large-Scale Geospatial Data Visualization](https://arxiv.org/abs/2403.03449) | 通过需要研究，建立了对显著时间步骤的定义，提出了一种利用自动编码器和动态规划的新方法来促进用户驱动的时间选择，结合结构特征和统计变化以实现更灵活的选择 |
| [^92] | [Kernel Correlation-Dissimilarity for Multiple Kernel k-Means Clustering](https://arxiv.org/abs/2403.03448) | 通过系统地整合核相关性和不相似度，我们提出了一种新方法，为多核k均值聚类提供了更有效的信息提取和改善聚类性能。 |
| [^93] | [Uncertainty quantification for deeponets with ensemble kalman inversion](https://arxiv.org/abs/2403.03444) | 使用集合卡尔曼反演方法针对DeepONets提出了一种高效的不确定性量化推断方法。 |
| [^94] | [An AI-enabled Agent-Based Model and Its Application in Measles Outbreak Simulation for New Zealand](https://arxiv.org/abs/2403.03434) | 通过将图神经网络（GNN）和长短期记忆（LSTM）网络与传统ABM相结合，开发了一种可微分的基于Agent的模型，成功应用于模拟新西兰2019年麻疹爆发，深入洞察传染病爆发的动态。 |
| [^95] | [Single Transit Detection In Kepler With Machine Learning And Onboard Spacecraft Diagnostics](https://arxiv.org/abs/2403.03427) | 使用卷积神经网络和航天器诊断技术，在Kepler数据集中实现了可靠地检测长轨道周期外行星的单次变迁，潜在地发现了新的行星。 |
| [^96] | [Sculpting Molecules in 3D: A Flexible Substructure Aware Framework for Text-Oriented Molecular Optimization](https://arxiv.org/abs/2403.03425) | 提出了一种通过多模态引导生成/优化任务解决分子设计问题的创新方法。 |
| [^97] | [LEAD: Learning Decomposition for Source-free Universal Domain Adaptation](https://arxiv.org/abs/2403.03421) | 提出了LEAD方法，通过将特征分解为源已知和未知组件来识别目标私有数据。 |
| [^98] | [Leveraging The Finite States of Emotion Processing to Study Late-Life Mental Health](https://arxiv.org/abs/2403.03414) | 通过隐藏马尔可夫模型和有限状态自动机的应用，可以更全面地理解晚年心理健康中的系统级动态。 |
| [^99] | [Advancing Out-of-Distribution Detection through Data Purification and Dynamic Activation Function Design](https://arxiv.org/abs/2403.03412) | 通过引入OOD-R数据集合和噪声过滤技术，我们的工作在神经网络中增强异常样本的检测和管理，以提高模型的健壮性和可靠性。 |
| [^100] | [Prediction Of Cryptocurrency Prices Using LSTM, SVM And Polynomial Regression](https://arxiv.org/abs/2403.03410) | 该论文使用LSTM、SVM和多项式回归算法对加密货币价格进行预测，通过性能比较来确定最佳算法模型 |
| [^101] | [An EnKF-LSTM Assimilation Algorithm for Crop Growth Model](https://arxiv.org/abs/2403.03406) | 本文提出了一种EnKF-LSTM数据同化方法，通过结合集合卡尔曼滤波器和LSTM神经网络，提高了作物生长预测的准确性。 |
| [^102] | [BAIT: Benchmarking (Embedding) Architectures for Interactive Theorem-Proving](https://arxiv.org/abs/2403.03401) | BAIT框架提供了公平且简化的ITP学习方法比较，发现Structure Aware Transformers在公式嵌入问题上表现出色。 |
| [^103] | [CoRMF: Criticality-Ordered Recurrent Mean Field Ising Solver](https://arxiv.org/abs/2403.03391) | CoRMF是一种基于RNN的高效伊辛模型求解器，利用关键有序自旋序列和循环神经网络来实现变分均场和 RNN 之间的统一，从而实现了对通常难以处理的伊辛模型的高效探索。 |
| [^104] | [Performance Evaluation of Semi-supervised Learning Frameworks for Multi-Class Weed Detection](https://arxiv.org/abs/2403.03390) | 半监督学习方法在多类杂草检测中展现出很好的性能，为解决传统监督学习方法中需要大量手工标注数据集的缺点提供了可持续替代方案。 |
| [^105] | [Complexity Matters: Dynamics of Feature Learning in the Presence of Spurious Correlations](https://arxiv.org/abs/2403.03375) | 在本文中，通过提出一个基于布尔函数分析的合成数据集，研究了在虚假相关性条件下特征学习动态，发现了虚假相关性或虚假特征的强度会影响核心特征学习速度，虚假特征和核心特征的学习阶段不总是可分开，并且虚假特征即使在一段时间后也不会被遗忘。 |
| [^106] | [TartanAviation: Image, Speech, and ADS-B Trajectory Datasets for Terminal Airspace Operations](https://arxiv.org/abs/2403.03372) | 这个论文介绍了TartanAviation数据集，包含图像、语音和ADS-B轨迹数据，对机场环境进行了全面观察，为AI和机器学习应用提供了重要资源 |
| [^107] | [Leveraging Federated Learning for Automatic Detection of Clopidogrel Treatment Failures](https://arxiv.org/abs/2403.03368) | 本研究利用联邦学习策略处理氯吡格雷治疗失败检测，结果显示联邦学习方法可以显著缩小性能差距，强调了其在该领域的潜力。 |
| [^108] | [Level Set Teleportation: An Optimization Perspective](https://arxiv.org/abs/2403.03362) | 该论文从优化的角度研究了水平集传输，证明了在某些条件下水平集传输可以加速梯度方法的收敛速度，且提出了一种只需要Hessian-vector products的方法验证了该技术在实践中的有效性。 |
| [^109] | [Chained Information-Theoretic bounds and Tight Regret Rate for Linear Bandit Problems](https://arxiv.org/abs/2403.03361) | 本文研究了线性赌博问题中Thompson-Sampling算法变体的贝叶斯遗憾，通过使用链接论证建立了具有度量动作空间的新界限，为$d$维线性赌博问题提供了$O(d\sqrt{T})$的严格率。 |
| [^110] | [RACE-SM: Reinforcement Learning Based Autonomous Control for Social On-Ramp Merging](https://arxiv.org/abs/2403.03359) | 该论文提出了一种基于强化学习的自主控制模型，专注于并行式匝道合流，考虑了道路上其他车辆的影响，并提出了新颖的激励函数。 |
| [^111] | [Hypothesis Spaces for Deep Learning](https://arxiv.org/abs/2403.03353) | 本文介绍了一种应用深度神经网络的深度学习假设空间，并构建了一个再生核Banach空间，研究了正则化学习和最小插值问题，证明了学习模型的解可以表示为线性组合。 |
| [^112] | [Solution Simplex Clustering for Heterogeneous Federated Learning](https://arxiv.org/abs/2403.03333) | 提出了Solution Simplex Clustered Federated Learning（SosicFL），通过学习解决方案单纯形的思想，为每个客户端分配单一区域，从而同时实现了学习本地和全局模型的目标。 |
| [^113] | [An Ensemble Framework for Explainable Geospatial Machine Learning Models](https://arxiv.org/abs/2403.03328) | 介绍了一个集成框架，结合了本地空间加权方案、可解释人工智能（XAI）和尖端机器学习技术，以提高地理空间机器学习模型的可解释性和准确性。 |
| [^114] | [Deep Configuration Performance Learning: A Systematic Survey and Taxonomy](https://arxiv.org/abs/2403.03322) | 性能是可配置软件系统行为的关键属性，本文针对深度学习在可配置软件性能学习方面进行了全面的调查与分类研究。 |
| [^115] | [Collision Avoidance Verification of Multiagent Systems with Learned Policies](https://arxiv.org/abs/2403.03314) | 该论文提出了一种基于向后可达性的方法，用于验证多智能体神经反馈环路的碰撞避免属性，通过解决一系列混合整数线性规划计算相对反投影集，并且该逐对方法可并行化，随着智能体数量的增加能够很好地扩展 |
| [^116] | [Graph Learning for Parameter Prediction of Quantum Approximate Optimization Algorithm](https://arxiv.org/abs/2403.03310) | 本研究利用图神经网络为量子近似优化算法提供参数预测，实现了在减少量子计算资源开销的同时增强算法效果。 |
| [^117] | [Mad Libs Are All You Need: Augmenting Cross-Domain Document-Level Event Argument Data](https://arxiv.org/abs/2403.03304) | Mad Libs 提供的数据增强方法 MLA 在跨领域文档级事件参数数据提取上取得了显著的性能提升，平均提高了 2.6 个 F1 分数。同时，在零和少样本事件角色方面相比无增强基线，提高了 3.9 和 5.2 个百分点。 |
| [^118] | [Proper vs Improper Quantum PAC learning](https://arxiv.org/abs/2403.03295) | 量子Coupon Collector问题的发现展示了正确与不正确学习在样本复杂度方面的差异，为量子PAC学习提供了新的见解。 |
| [^119] | [Averaging Rate Scheduler for Decentralized Learning on Heterogeneous Data](https://arxiv.org/abs/2403.03292) | 提出了一种平均化速率调度的方法来减少分散式学习中异构性的影响，并通过实验证明其相比于传统方法的优越性（提高了约3%测试准确性）。 |
| [^120] | [Credibility-Aware Multi-Modal Fusion Using Probabilistic Circuits](https://arxiv.org/abs/2403.03281) | 提出了一种使用概率电路进行可信度感知的多模态融合方法，在维持竞争性能的同时能够可靠推断可信度。 |
| [^121] | [ARNN: Attentive Recurrent Neural Network for Multi-channel EEG Signals to Identify Epileptic Seizures](https://arxiv.org/abs/2403.03276) | ARNN提出了一种注意力循环神经网络，用于处理多通道脑电图信号，具有线性复杂度和并行计算，结合注意力和LSTM gate的优势，并避免了它们的缺点。 |
| [^122] | [From Noise to Signal: Unveiling Treatment Effects from Digital Health Data through Pharmacology-Informed Neural-SDE](https://arxiv.org/abs/2403.03274) | 通过药理学启发的神经随机微分方程模型，有效地识别数字健康数据中的治疗效果和学习因果关系，从而实现反事实能力。 |
| [^123] | [DINOv2 based Self Supervised Learning For Few Shot Medical Image Segmentation](https://arxiv.org/abs/2403.03273) | 本研究结合了ALPNet的优势和DINOv2的特征提取能力，提出了一种新颖的少样本医学图像分割方法，不仅提升了性能，也为更多创新铺平了道路。 |
| [^124] | [Neural Network Learning and Quantum Gravity](https://arxiv.org/abs/2403.03245) | 研究旨在探索神经网络学习如何帮助发现弦景观中一致性理论的新特性，以及其统计可学习性质。 |
| [^125] | [Triple/Debiased Lasso for Statistical Inference of Conditional Average Treatment Effects](https://arxiv.org/abs/2403.03240) | 研究提出了一种三重/去偏Lasso方法，用于统计推断条件平均处理效应，不要求直接假设稀疏性，有效估计了线性模型之间的差异。 |
| [^126] | [Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling](https://arxiv.org/abs/2403.03234) | Caduceus 是第一个支持反向互补性并具有双向性的长范围 DNA 语言模型，通过引入预训练和微调策略，在下游基准测试中表现优异。 |
| [^127] | [From Displacements to Distributions: A Machine-Learning Enabled Framework for Quantifying Uncertainties in Parameters of Computational Models](https://arxiv.org/abs/2403.03233) | 该论文提出了一种机器学习框架，可以量化计算模型参数不确定性的两种来源，通过结合数据一致性和学习不确定量框架，可以有效处理测量误差和兴趣数量映射问题 |
| [^128] | [Machine and deep learning methods for predicting 3D genome organization](https://arxiv.org/abs/2403.03231) | 该论文总结了机器学习方法作为一种替代方式，用于获取缺失的3D染色质相互作用和/或提高分辨率的情况 |
| [^129] | [Embracing Uncertainty Flexibility: Harnessing a Supervised Tree Kernel to Empower Ensemble Modelling for 2D Echocardiography-Based Prediction of Right Ventricular Volume](https://arxiv.org/abs/2403.03229) | 提出了一种利用监督树核强化集成模型，预测右心室容积的二维超声心动图的方法，并通过不确定性得分增强了预测表现，该方法在小规模数据集上表现出较好的概率和点预测性能。 |
| [^130] | [Reinforcement Learning Jazz Improvisation: When Music Meets Game Theory](https://arxiv.org/abs/2403.03224) | 介绍了一个新颖的数学博弈论模型用于研究爵士即兴演奏，探索不同的随机即兴策略和其在即兴演奏中的配对表现，发现最有效的策略对是逐步改变和和弦跟随强化学习。 |
| [^131] | [Exact Enforcement of Temporal Continuity in Sequential Physics-Informed Neural Networks](https://arxiv.org/abs/2403.03223) | 精确执⾏时间连续性是本论⽂的⼀项重要创新，简化了解决时间相关问题动态⾏为预测困难的挑战。 |
| [^132] | [Knowledge-guided EEG Representation Learning](https://arxiv.org/abs/2403.03222) | 提出了一个知识引导的EEG自监督学习模型，通过使用基于状态空间的深度学习架构，实现了稳健的性能和显著的参数效率。 |
| [^133] | [FedHCDR: Federated Cross-Domain Recommendation with Hypergraph Signal Decoupling](https://arxiv.org/abs/2403.02630) | 该研究提出了FedHCDR框架，通过超图信号解耦的方式解决了联邦跨领域推荐中不同领域数据异质性的问题。 |
| [^134] | [Sample Efficient Myopic Exploration Through Multitask Reinforcement Learning with Diverse Tasks](https://arxiv.org/abs/2403.01636) | 通过研究发现，当代理在多样化任务上进行训练时，具有短视探索设计的通用策略共享算法可以在多任务强化学习中显著提高样本效率。 |
| [^135] | [Less is More: Hop-Wise Graph Attention for Scalable and Generalizable Learning on Circuits](https://arxiv.org/abs/2403.01317) | 提出了一种名为HOGA的基于注意力的模型，能够在电路中以可扩展和通用的方式学习电路表示，通过跳数特征和门控自注意力模块的方式，实现了对不同电路结构的自适应学习，并可以进行高效的分布式训练。 |
| [^136] | [Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy](https://arxiv.org/abs/2402.19379) | 该研究通过将十二个LLMs组成的LLM集成方法与925名人类预测者的群体预测进行比较，发现LLM群体优于简单的无信息基准，并在统计上等效于人类群体。 |
| [^137] | [Verification of Neural Networks' Global Robustness](https://arxiv.org/abs/2402.19322) | 提出了一种新的全局鲁棒性属性，旨在找到分类器的最小全局鲁棒边界，并引入了VHAGaR，一个用于计算此边界的验证器。 |
| [^138] | [Estimation and Deconvolution of Second Order Cyclostationary Signals](https://arxiv.org/abs/2402.19290) | 该方法解决了盲去卷积和估计带噪声二阶循环平稳信号时间波形的双重问题，通过证明去卷积滤波器存在，消除信号的TF效应，实现高精度算法，有潜力改善机器学习模型的训练。 |
| [^139] | [Learning with Language-Guided State Abstractions](https://arxiv.org/abs/2402.18759) | 利用自然语言和语言模型引导的方法，实现自动构建适用于未见任务的状态表示，有助于高维观测空间中泛化策略学习。 |
| [^140] | [Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards](https://arxiv.org/abs/2402.18571) | 提出了方向偏好对齐（DPA）框架，通过多目标奖励模拟不同偏好配置，以实现用户相关的偏好控制。 |
| [^141] | [Structure-Guided Adversarial Training of Diffusion Models](https://arxiv.org/abs/2402.17563) | SADM通过对抗训练的方式引入结构指导，使得模型能够学习每个训练批次中样本之间的流形结构。 |
| [^142] | [A prior Estimates for Deep Residual Network in Continuous-time Reinforcement Learning](https://arxiv.org/abs/2402.16899) | 本研究针对连续时间控制问题，提出了一种可以直接分析Bellman最优损失\emph{先验}泛化误差的方法，避免了有界性假设，并通过最大算子的分解方法实现了损失函数的转换。 |
| [^143] | [Expressive Whole-Body Control for Humanoid Robots](https://arxiv.org/abs/2402.16796) | 我们提出了一种表现全身控制（Exbody）方法，通过在人类大小的机器人上学习全身控制策略，使其能够模仿人类动作，并通过在模拟环境中训练和Sim2Real转移，实现控制人形机器人以不同风格行走、摇晃等。 |
| [^144] | [Scalable Robust Sparse Principal Component Analysis](https://arxiv.org/abs/2402.16712) | 本文提出了一个优化框架，可在稀疏稳健的情况下估计一维子空间，通过引入线性松弛方法和新颖的拟合程序，实现了全局最优的稳健稀疏子空间，具有多项式时间效率且可扩展性强。 |
| [^145] | [Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing](https://arxiv.org/abs/2402.16627) | 提出了一种新颖且通用的上下文化扩散模型（ContextDiff），通过在正向和逆向过程中融入文本条件和视觉样本之间的交互和对齐，以便在视觉生成中更准确地传达文本语义 |
| [^146] | [Pfeed: Generating near real-time personalized feeds using precomputed embedding similarities](https://arxiv.org/abs/2402.16073) | 使用预计算的嵌入相似性生成个性化信息流，提高了电子商务平台上的客户参与度和体验，转化率提升4.9％。 |
| [^147] | [Brant-2: Foundation Model for Brain Signals](https://arxiv.org/abs/2402.10251) | Brant-2是脑信号领域最大的基础模型，相比于Brant，它不仅对数据变化和建模尺度具有稳健性，还能适用于更广泛范围的脑神经数据。 |
| [^148] | [Prompt Learning on Temporal Interaction Graphs](https://arxiv.org/abs/2402.06326) | 这个论文提出了一种在时间交互图上进行提示学习的方法，以解决当前模型在预训练和下游预测阶段所面临的时间差异和语义差异的问题。 |
| [^149] | [Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction](https://arxiv.org/abs/2402.04154) | 本论文探索了为智能体提供增强形式的任务指导，使其能够理解游戏指导并实现"读玩游戏"的能力。通过将多模态指导调优的成功应用于视觉任务中的强化学习任务，构建了一组... (内容太长，无法继续显示) |
| [^150] | [Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining](https://arxiv.org/abs/2402.03302) | Swin-UMamba是一种以Mamba为基础的新型UNet模型，通过结合局部特征和全局依赖的多尺度信息来实现准确的医学图像分割。与现有方法相比，Swin-UMamba具有更高的准确性、较低的内存消耗和更少的计算负担，并充分利用了预训练的威力。 |
| [^151] | [GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models](https://arxiv.org/abs/2402.03299) | 本论文提出了一个通过角色扮演的系统，可以生成自然语言越狱，用于测试大型语言模型的指南遵循情况。系统通过收集现有越狱并将其组织成知识图来生成新的越狱，证明了其高效性和有效性。 |
| [^152] | [Markov Persuasion Processes: Learning to Persuade from Scratch](https://arxiv.org/abs/2402.03077) | 这篇论文提出了马尔可夫说服过程模型，用于捕捉发送者和接收者顺序交互的情景。论文解决了现有模型中的问题，提供了针对发送者没有环境知识的解决方案。通过设计学习算法，证明了算法的性能。这个方法的总结要点是提出了马尔可夫说服过程模型，并提出了针对没有环境知识的发送者的学习算法。 |
| [^153] | [LOCOST: State-Space Models for Long Document Abstractive Summarization](https://arxiv.org/abs/2401.17919) | LOCOST是一种基于状态空间模型的编码器-解码器架构，用于处理长文档的抽象摘要生成。与基于稀疏注意模式的最先进模型相比，LOCOST具有更低的计算复杂度，并且能够在训练和推断期间节省大量内存。在评估中，LOCOST在长文档摘要化任务上达到了93-96%的性能水平，并且能够处理超过600K个标记的输入文本。 |
| [^154] | [MedLM: Exploring Language Models for Medical Question Answering Systems](https://arxiv.org/abs/2401.11389) | 本研究比较了用于医疗问答的通用和医学特定的精炼语言模型的表现，以填补领域特定任务中这些模型性能的研究空白。 |
| [^155] | [SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding](https://arxiv.org/abs/2401.09340) | 本研究通过系统性地扩展室内环境中的3D视觉-语言学习，提出了首个百万规模的3D视觉-语言数据集SceneVerse，以解决3D视觉-语言对齐面临的几个重要挑战。 |
| [^156] | [Who Said What? An Automated Approach to Analyzing Speech in Preschool Classrooms](https://arxiv.org/abs/2401.07342) | 提出了一个自动化框架，使用儿童和教师佩戴的录音设备记录言语，实现说话者分类和转录，与人类专家对比结果表明，框架整体准确率达到0.76，为学前教室言语分析提供了新思路 |
| [^157] | [Understanding Distributed Representations of Concepts in Deep Neural Networks without Supervision](https://arxiv.org/abs/2312.17285) | 本文提出了一种无监督方法，通过选择主要神经元来发现概念的分布表示，可以用于识别数据中的未标记子类和检测误分类的原因。 |
| [^158] | [Parameterized Projected Bellman Operator](https://arxiv.org/abs/2312.12869) | 本论文提出了一种基于学习的近似贝尔曼算子的新方法，以解决近似值迭代算法中样本不确定性和计算复杂度的问题。 |
| [^159] | [TSRNet: Simple Framework for Real-time ECG Anomaly Detection with Multimodal Time and Spectrogram Restoration Network](https://arxiv.org/abs/2312.10187) | 本论文提出了TSRNet框架，利用多模态时间和谱图恢复网络进行实时心电图异常检测，通过恢复正常ECG数据训练，结合时间序列和时频领域，实现了对心电图信号异常的检测。 |
| [^160] | [Large-scale Training of Foundation Models for Wearable Biosignals](https://arxiv.org/abs/2312.05409) | 利用自监督学习和大规模可穿戴设备数据，本研究训练了基础模型用于衡量光电容积描记（PPG）和心电图信号，以解决医学数据集规模较小的难题。 |
| [^161] | [Fair Text-to-Image Diffusion via Fair Mapping](https://arxiv.org/abs/2311.17695) | 本文提出了一种通过Fair Mapping控制模型提示来修改文本到图像扩散模型，实现公平图像生成的方法，具有高效性和能够生成相对平衡人口统计结果的优势。 |
| [^162] | [Ensemble sampling for linear bandits: small ensembles suffice](https://arxiv.org/abs/2311.08376) | 该论文对随机线性赌臂环境中的集成抽样进行了首次实用和严格的分析，展示了在标准假设下，采用规模为$d \log T$的集成抽样可以获得接近$\sqrt{T}$阶的后悔，而不需要集成大小与$T$线性扩展。 |
| [^163] | [Exploration via linearly perturbed loss minimisation](https://arxiv.org/abs/2311.07565) | 提出了一种名为EVILL的随机探索方法，通过解决线性扰动的负对数似然函数的极小化问题来工作，提供了关于随机奖励扰动产生良好赌博算法的简洁解释，并在实践中展示了与汤普森抽样风格参数扰动方法性能相匹配的能力 |
| [^164] | [From Coupled Oscillators to Graph Neural Networks: Reducing Over-smoothing via a Kuramoto Model-based Approach](https://arxiv.org/abs/2311.03260) | 提出了一种新型的连续深度图神经网络KuramotoGNN，通过采用Kuramoto模型来减轻GNN中的过度平滑现象，实现节点特征的差异化，取得了优于基线GNN和现有方法的实验效果。 |
| [^165] | [What's In My Big Data?](https://arxiv.org/abs/2310.20707) | 通过提出的平台和分析方法，我们揭示和比较了大型文本语料库的内容，发现了关于语料库内容的几个令人惊讶且以前未被记录的发现。 |
| [^166] | [Transformer-based nowcasting of radar composites from satellite images for severe weather](https://arxiv.org/abs/2310.19515) | 该论文提出了一种基于Transformer的模型，利用卫星数据对地面雷达图像序列进行即时预报，能够有效弥合地面和空间观测之间的差距，提高天气预测的准确性。 |
| [^167] | [Continual Driving Policy Optimization with Closed-Loop Individualized Curricula](https://arxiv.org/abs/2309.14209) | 开发了连续驾驶政策优化框架，提出了闭环个性化课程（CLIC）概念，允许重复利用广泛场景来迭代改进自主驾驶车辆模型。 |
| [^168] | [Neural Koopman prior for data assimilation](https://arxiv.org/abs/2309.05317) | 本文介绍了一种利用神经Koopman先验进行数据同化的方法，将动态系统嵌入到潜在空间，使得对其动态进行线性描述，并展示了长期连续重构和自监督学习的潜力。 |
| [^169] | [The Normal Distributions Indistinguishability Spectrum and its Application to Privacy-Preserving Machine Learning](https://arxiv.org/abs/2309.01243) | 本文提出了正态分布不可区分性谱定理 (NDIS Theorem)，旨在利用查询本身的随机性改进随机化机器学习查询的差分隐私机制。 |
| [^170] | [PURL: Safe and Effective Sanitization of Link Decoration](https://arxiv.org/abs/2308.03417) | 使用机器学习方法的PURL能够安全有效地清洁链接装饰，显着优于现有对策，而且对常见规避技术具有很强的鲁棒性。 |
| [^171] | [THC: Accelerating Distributed Deep Learning Using Tensor Homomorphic Compression](https://arxiv.org/abs/2302.08545) | 引入了Tensor Homomorphic Compression (THC)，一种新颖的双向压缩框架，可以加速分布式深度学习中的模型训练 |
| [^172] | [Planted Bipartite Graph Detection](https://arxiv.org/abs/2302.03658) | 该论文研究了在随机图中检测隐藏的种植二分图子图的问题，并对其统计和计算障碍进行了表征。 |
| [^173] | [Exact Fractional Inference via Re-Parametrization & Interpolation between Tree-Re-Weighted- and Belief Propagation- Algorithms](https://arxiv.org/abs/2301.10369) | 通过构建$\lambda$-分数全同维数，实现了树重新加权和信念传播算法之间的插值，确保在铁磁性情况下，存在“精确”$\lambda_*$使得计算的配分函数$Z=Z^{(\lambda_*)}$。 |
| [^174] | [Spatio-Temporal Self-Supervised Learning for Traffic Flow Prediction](https://arxiv.org/abs/2212.04475) | 提出了一种新的时空自监督学习（ST-SSL）交通预测框架，通过辅助自监督学习范式增强交通模式表示，既反映空间异质性又反映时间异质性。 |
| [^175] | [Decision-making with Speculative Opponent Models](https://arxiv.org/abs/2211.11940) | 提出了一种使用纯粹局部信息实现推测对手建模的多智能体分布式演员-评论家算法，能够帮助受控代理做出决策。 |
| [^176] | [SemSegDepth: A Combined Model for Semantic Segmentation and Depth Completion](https://arxiv.org/abs/2209.00381) | 在本文中，我们提出了一个新的端到端模型，用于同时进行语义分割和深度完成。我们的方法结合了语义分割和深度完成任务，在多任务网络中有效提高了每个任务的性能。 |
| [^177] | [STDEN: Towards Physics-Guided Neural Networks for Traffic Flow Prediction](https://arxiv.org/abs/2209.00225) | STDEN是一种将交通流动力学的物理机制转化为深度学习模型的物理指导神经网络，旨在弥合纯数据驱动和物理驱动方法之间的差距。 |
| [^178] | [Differentially Private Generalized Linear Models Revisited](https://arxiv.org/abs/2205.03014) | 研究了具有凸损失的线性预测器的$(\epsilon,\delta)$-差分隐私学习问题，为两个损失函数子类提供了结果，并在所有参数上展示了基本紧密的上界和下界。 |
| [^179] | [Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer.](http://arxiv.org/abs/2310.20172) | 提出了一种叫做CBS-GPT的生成预训练Transformer模型，用于紧凑二进制系统波形生成，在预测准确性上达到了较高的准确率，并且具有显著的解释性能。 |
| [^180] | [Leveraging Ensemble Diversity for Robust Self-Training in the Presence of Sample Selection Bias.](http://arxiv.org/abs/2310.14814) | 本文提出了一种在样本选择偏差存在的情况下，利用集成多样性进行鲁棒的自训练的方法，并引入了一种新的自信度度量方法-$\mathcal{T}$-相似度。实验证明该方法在三种不同伪标签策略下具有良好的效果。 |
| [^181] | [Scalable Neural Network Kernels.](http://arxiv.org/abs/2310.13225) | 可扩展神经网络内核（SNNKs）是一种替代常规前馈层的方法，能够近似实现常规前馈层的功能，但具有更优的计算特性。通过将内核与参数-输入向量的点积联系起来，SNNKs能够有效地解开参数与输入之间的联系，从而模拟复杂关系。此外，我们还引入了神经网络捆绑过程，将SNNKs应用于深度神经网络压缩，进一步提高了压缩效果。最终捆绑网络甚至可以绕过反向传播，通过显式公式求解最优参数。 |
| [^182] | [Residual Multi-Fidelity Neural Network Computing.](http://arxiv.org/abs/2310.03572) | 本研究提出了一种残差多保真计算框架，通过使用多保真信息构建神经网络代理模型，解决了低保真和高保真计算模型之间的相关性建模问题。这种方法训练了两个神经网络，利用残差函数进行模型训练，最终得到了高保真替代模型。 |
| [^183] | [Distribution-Free Statistical Dispersion Control for Societal Applications.](http://arxiv.org/abs/2309.13786) | 提出了一个简单而灵活的框架，用于处理具有社会意义的无分布统计离散度控制，可以应用于高风险应用。 |
| [^184] | [Methods for generating and evaluating synthetic longitudinal patient data: a systematic review.](http://arxiv.org/abs/2309.12380) | 本文对生成和评估合成纵向患者数据的方法进行了系统综述，以解决医学领域中数据使用和隐私保护的问题。 |
| [^185] | [Projected Task-Specific Layers for Multi-Task Reinforcement Learning.](http://arxiv.org/abs/2309.08776) | 本研究提出了一种新的架构，Projected Task-Specific Layers (PTSL)，通过任务特定的层来表达共享和可变的任务信息，成功解决了多任务强化学习中的推广和干扰问题。 |
| [^186] | [Structure-Preserving Transformers for Sequences of SPD Matrices.](http://arxiv.org/abs/2309.07579) | 本文介绍了一种保持序列的对称正定矩阵的黎曼几何特性的结构保持变压器机制，并将其应用于自动睡眠分期，取得了高水平的阶段性能。 |
| [^187] | [HarvestNet: A Dataset for Detecting Smallholder Farming Activity Using Harvest Piles and Remote Sensing.](http://arxiv.org/abs/2308.12061) | 该论文介绍了一种新的方法，通过检测全球小农户系统中常见的收获堆来映射农田的存在。作者提供了HarvestNet数据集，该数据集由专家知识和卫星图像收集而来，可用于在埃塞俄比亚的特定地区进行农田映射。实验结果表明，作者的最佳模型在手动标记数据上具有约80%的分类性能。 |
| [^188] | [xxMD: Benchmarking Neural Force Fields Using Extended Dynamics beyond Equilibrium.](http://arxiv.org/abs/2308.11155) | 在神经力场模型中，常用的MD17数据集对于表示经历化学反应的系统不足。为了解决这一问题，我们引入了xxMD数据集，该数据集采样自扩展激发态分子动力学，包含了能量和力的信息。 |
| [^189] | [Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models.](http://arxiv.org/abs/2308.09778) | 本文旨在研究多模态视觉语言模型在理解空间关系方面的能力，提出了细粒度组合的空间关系基础，并采用自底向上的方法评估空间关系推理任务的性能。 |
| [^190] | [AbDiffuser: Full-Atom Generation of In-Vitro Functioning Antibodies.](http://arxiv.org/abs/2308.05027) | AbDiffuser是一个物理性扩散模型，用于联合生成抗体的三维结构和序列。该方法利用领域知识和基于物理的约束改善蛋白质扩散，处理序列长度变化，并能够生成与参考集合的序列和结构特性密切匹配的抗体。实验结果表明，AbDiffuser能够生成高水平表达的抗体，其中57.1%的设计选择是紧密结合剂。 |
| [^191] | [Linear Convergence Bounds for Diffusion Models via Stochastic Localization.](http://arxiv.org/abs/2308.03686) | 通过随机定位方法，我们提供了一种解决扩散模型中线性收敛界限问题的方法，并证明了这种方法可以在有限二阶矩条件下达到可接受的精度。 |
| [^192] | [VQGraph: Graph Vector-Quantization for Bridging GNNs and MLPs.](http://arxiv.org/abs/2308.02117) | VQGraph是一个框架，通过学习一个强大的图形表示空间，用于连接GNN和MLPs。它采用矢量量化变分自编码器（VQ-VAE）的编码器作为结构感知图标记器，有效地表示底层图的多样化局部结构。通过 VQGraph，可以实现从GNN到MLP的知识转移。 |
| [^193] | [Interpretable Stereotype Identification through Reasoning.](http://arxiv.org/abs/2308.00071) | 本研究通过使用推理方法，在零射击刻板印象识别中取得了重要的进展，并发现推理的性能增益远远超过模型规模扩展的增益。推理不仅提高了准确性，还提高了决策的可解释性。 |
| [^194] | [Multigrid-Augmented Deep Learning for the Helmholtz Equation: Better Scalability with Compact Implicit Layers.](http://arxiv.org/abs/2306.17486) | 通过结合多网格求解器和卷积神经网络，该论文提出了一种用于解决离散异质Helmholtz方程的迭代深度学习方法，在可伸缩性和求解速度上优于传统方法。其中的三个主要创新包括引入隐式层来解决CNN中的视野问题、改进CNN预条件技术以提高性能，并提出了一种多尺度训练方法使网络能够处理不同尺寸的问题。 |
| [^195] | [Ordinal Potential-based Player Rating.](http://arxiv.org/abs/2306.05366) | 该论文提出了一种使用序数势函数的玩家评级方法，通过可逆映射的嵌套计算，能够保持传递性。 |
| [^196] | [High-Fidelity Image Compression with Score-based Generative Models.](http://arxiv.org/abs/2305.18231) | 本文提出了一种基于分数的生成模型的两阶段方法，该方法在图像压缩领域取得了显著的表现，实验证明该方法在一定比特率下能够提高图像的感知质量。 |
| [^197] | [Massively Scalable Inverse Reinforcement Learning in Google Maps.](http://arxiv.org/abs/2305.11290) | 本文提出了一种新的逆强化学习算法（RHIP），通过图压缩、并行化和基于主特征向量的问题初始化解决了全球规模的MDPs、大型数据集和高度参数化的模型的问题，在谷歌地图中实现了16-24%的全球路线质量改进。 |
| [^198] | [A Lightweight CNN-Transformer Model for Learning Traveling Salesman Problems.](http://arxiv.org/abs/2305.01883) | 本文提出了一种轻量级CNN-Transformer模型，使用CNN嵌入层和部分自注意力，能更好学习输入数据中的空间特征并消除冗余。实验表明该模型在解决旅行商问题方面表现出更好的性能，例如TSP解决方案质量和GPU内存使用方面。 |
| [^199] | [ClusterNet: A Perception-Based Clustering Model for Scattered Data.](http://arxiv.org/abs/2304.14185) | 这项工作介绍了ClusterNet，一种基于感知的分布式数据聚类模型，利用大规模数据集和基于点的深度学习模型，反映人类感知的聚类可分性。 |
| [^200] | [SIFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency.](http://arxiv.org/abs/2303.11525) | 本研究提出了一种名为SIFT的方法，用于提高深度神经网络的训练效率、准确性和表示能力，通过稀疏等FLOP转换，缩短训练时间。 |
| [^201] | [Treat Different Negatives Differently: Enriching Loss Functions with Domain and Range Constraints for Link Prediction.](http://arxiv.org/abs/2303.00286) | 通过引入领域和范围约束，我们提出了基于语义的损失函数来区分不同质量的负样本，实验证明在链接预测任务上有效。 |
| [^202] | [A Reduction-based Framework for Sequential Decision Making with Delayed Feedback.](http://arxiv.org/abs/2302.01477) | 我们提出了一个基于规约的框架，可以将任何多批次算法转化为处理顺序决策中的随机延迟的高效算法。我们不仅在赌博机、表格型MDPs和表格型MGs方面取得了与现有结果相匹配或改进的成果，还首次对顺序决策中的延迟与函数逼近进行了研究。 |
| [^203] | [AMPNet: Attention as Message Passing for Graph Neural Networks.](http://arxiv.org/abs/2210.09475) | AMPNet是一种用于图神经网络的基于注意力的消息传递层，能够对节点进行逐个特征编码，并通过跨节点注意力模型特征级别的交互。在实际生物系统上进行广泛基准测试表明，AMPNet在fMRI信号重建方面优于现有基准，并通过案例研究验证了其发现有意义的特征级别交互的能力。 |
| [^204] | [Efficient Quantum Agnostic Improper Learning of Decision Trees.](http://arxiv.org/abs/2210.00212) | 本文提出了第一个无需成员查询在多项式时间内学习大小为$t$的决策树的算法，并成功量化了Kalai和Kanade的不知性增强算法，得到了第一个高效的量子不知性增强算法。 |
| [^205] | [Application of Quantum Density Matrix in Classical Question Answering and Classical Image Classification.](http://arxiv.org/abs/2203.11155) | 该论文将量子密度矩阵应用于经典问答和图像分类中，证明了其可以提高任务的效率，尤其在图像分类中取得了优秀的性能表现。 |

# 详细

[^1]: 3D扩散策略

    3D Diffusion Policy

    [https://arxiv.org/abs/2403.03954](https://arxiv.org/abs/2403.03954)

    3D扩散策略（DP3）是一种新颖的视觉模仿学习方法，通过将3D视觉表示的强大性结合到扩散策略中，成功解决了学习复杂技能所需大量人类演示的问题。

    

    模仿学习为教授机器人灵巧技能提供了一种高效的方式；然而，学习复杂而具有通用性的技能通常需要大量的人类演示。为了解决这一具有挑战性的问题，我们提出了3D扩散策略（DP3），这是一种将3D视觉表示的强大性融入到扩散策略中的新颖视觉模仿学习方法，扩散策略是一类有条件的动作生成模型。DP3的核心设计是利用一个紧凑的3D视觉表示，该表示是从稀疏点云中提取出来的，使用高效的点编码器。在我们涵盖了72个仿真任务的实验中，DP3仅需要10个演示就可以成功处理大多数任务，并且比基线模型提高了55.3%。在4个真实机器人任务中，DP3表现出了高成功率的精确控制，每项任务仅需40次演示即可成功率为85%，在不同领域展现了出色的泛化能力。

    arXiv:2403.03954v1 Announce Type: cross  Abstract: Imitation learning provides an efficient way to teach robots dexterous skills; however, learning complex skills robustly and generalizablely usually consumes large amounts of human demonstrations. To tackle this challenging problem, we present 3D Diffusion Policy (DP3), a novel visual imitation learning approach that incorporates the power of 3D visual representations into diffusion policies, a class of conditional action generative models. The core design of DP3 is the utilization of a compact 3D visual representation, extracted from sparse point clouds with an efficient point encoder. In our experiments involving 72 simulation tasks, DP3 successfully handles most tasks with just 10 demonstrations and surpasses baselines with a 55.3% relative improvement. In 4 real robot tasks, DP3 demonstrates precise control with a high success rate of 85%, given only 40 demonstrations of each task, and shows excellent generalization abilities in di
    
[^2]: 停止回归：通过分类训练值函数实现可扩展的深度强化学习

    Stop Regressing: Training Value Functions via Classification for Scalable Deep RL

    [https://arxiv.org/abs/2403.03950](https://arxiv.org/abs/2403.03950)

    通过使用分类代替回归训练值函数，本文提出了一种简单方法来改善深度强化学习的性能和可扩展性

    

    值函数是深度强化学习（RL）的核心组件。这些通过神经网络参数化的函数，使用均方误差回归目标进行训练，以匹配自举目标值。然而，将使用回归的价值型RL方法扩展到大型网络，如高容量的Transformers，已被证明是具有挑战性的。本文观察到了这一差异，探讨了通过使用分类而不是回归来训练值函数是否也可以简单地提高深度RL的可扩展性。我们证明，使用分类交叉熵训练的值函数在各种领域中显著提高了性能和可扩展性。这些领域包括：在Atari 2600游戏上使用SoftMoEs进行单一任务RL。

    arXiv:2403.03950v1 Announce Type: cross  Abstract: Value functions are a central component of deep reinforcement learning (RL). These functions, parameterized by neural networks, are trained using a mean squared error regression objective to match bootstrapped target values. However, scaling value-based RL methods that use regression to large networks, such as high-capacity Transformers, has proven challenging. This difficulty is in stark contrast to supervised learning: by leveraging a cross-entropy classification loss, supervised methods have scaled reliably to massive networks. Observing this discrepancy, in this paper, we investigate whether the scalability of deep RL can also be improved simply by using classification in place of regression for training value functions. We demonstrate that value functions trained with categorical cross-entropy significantly improves performance and scalability in a variety of domains. These include: single-task RL on Atari 2600 games with SoftMoEs
    
[^3]: 通过模拟调和现实：一种用于稳健操作的实-模-实方法

    Reconciling Reality through Simulation: A Real-to-Sim-to-Real Approach for Robust Manipulation

    [https://arxiv.org/abs/2403.03949](https://arxiv.org/abs/2403.03949)

    该论文提出了一种名为RialTo的系统，通过在“数字孪生”模拟环境中进行强化学习来稳健化真实世界的模仿学习策略，以实现在不需要大量不安全真实世界数据采集或广泛人类监督的情况下学习性能优越、稳健的策略。

    

    仿真学习方法需要大量人类监督来学习对物体姿势变化、物理干扰和视觉扰动鲁棒的策略。另一方面，强化学习可以自主探索环境以学习稳健行为，但可能需要大量不安全的真实世界数据采集。为了在没有不安全真实世界数据采集或广泛人类监督的负担下学习性能优越、稳健的策略，我们提出了RialTo，一个通过在即将从少量真实世界数据构建的“数字孪生”模拟环境中进行强化学习来稳健化真实世界的模仿学习策略的系统。为了实现这种实-模-实流水线，RialTo提出了一个易于使用的接口，用于快速扫描和构建真实世界环境的数字孪生。我们还引入了一种新颖的“反向提炼”过程，用于给真实世界演示带来

    arXiv:2403.03949v1 Announce Type: cross  Abstract: Imitation learning methods need significant human supervision to learn policies robust to changes in object poses, physical disturbances, and visual distractors. Reinforcement learning, on the other hand, can explore the environment autonomously to learn robust behaviors but may require impractical amounts of unsafe real-world data collection. To learn performant, robust policies without the burden of unsafe real-world data collection or extensive human supervision, we propose RialTo, a system for robustifying real-world imitation learning policies via reinforcement learning in "digital twin" simulation environments constructed on the fly from small amounts of real-world data. To enable this real-to-sim-to-real pipeline, RialTo proposes an easy-to-use interface for quickly scanning and constructing digital twins of real-world environments. We also introduce a novel "inverse distillation" procedure for bringing real-world demonstrations
    
[^4]: SPEAR：联邦学习中批量精确梯度反演

    SPEAR:Exact Gradient Inversion of Batches in Federated Learning

    [https://arxiv.org/abs/2403.03945](https://arxiv.org/abs/2403.03945)

    该论文提出了第一个能够精确重构批量$b >1$的算法，在联邦学习中解决了梯度反演攻击的问题。

    

    联邦学习是一种流行的协作机器学习框架，在这个框架中，多个客户端仅与服务器共享他们本地数据的梯度更新，而不是实际数据。不幸的是，最近发现梯度反演攻击可以从这些共享的梯度中重构出数据。现有的攻击只能在重要的诚实但好奇设置中对批量大小为$b=1$的数据进行精确重构，对于更大的批量只能进行近似重构。在这项工作中，我们提出了\emph{第一个准确重建批量$b >1$的算法}。这种方法结合了对梯度显式低秩结构的数学见解和基于采样的算法。关键的是，我们利用ReLU诱导的梯度稀疏性，精确地过滤掉大量错误的样本，使最终的重建步骤可行。我们为全连接提供了高效的GPU实现

    arXiv:2403.03945v1 Announce Type: new  Abstract: Federated learning is a popular framework for collaborative machine learning where multiple clients only share gradient updates on their local data with the server and not the actual data. Unfortunately, it was recently shown that gradient inversion attacks can reconstruct this data from these shared gradients. Existing attacks enable exact reconstruction only for a batch size of $b=1$ in the important honest-but-curious setting, with larger batches permitting only approximate reconstruction. In this work, we propose \emph{the first algorithm reconstructing whole batches with $b >1$ exactly}. This approach combines mathematical insights into the explicit low-rank structure of gradients with a sampling-based algorithm. Crucially, we leverage ReLU-induced gradient sparsity to precisely filter out large numbers of incorrect samples, making a final reconstruction step tractable. We provide an efficient GPU implementation for fully connected 
    
[^5]: 《启发式核心：理解预训练语言模型中的子网络泛化》

    The Heuristic Core: Understanding Subnetwork Generalization in Pretrained Language Models

    [https://arxiv.org/abs/2403.03942](https://arxiv.org/abs/2403.03942)

    在预训练语言模型中，发现所有子网络都共享一个注意力头集合，被称为启发式核心，这可能是造成子网络泛化差异的原因。

    

    先前的研究发现，经过不同随机种子微调的预训练语言模型（LMs）可以实现类似的领域内性能，但在句法泛化测试中泛化不同。在这项工作中，我们展示出即使在单一模型内部，我们也可以找到执行类似领域内操作但泛化差异巨大的多个子网络。为了更好地理解这些现象，我们调查是否可以用“竞争性子网络”来理解它们：模型最初表示各种不同算法，对应于不同的子网络，当最终收敛到其中一个时泛化发生。这一解释已被用于解释简单算法任务中的泛化。我们发现，并非找到竞争性子网络，而是所有子网络 -- 无论它们是否泛化 -- 都共享一组注意力头，我们将其称为启发式核心。进一步分析表明，它可能是造成不同子网络泛化的原因。

    arXiv:2403.03942v1 Announce Type: new  Abstract: Prior work has found that pretrained language models (LMs) fine-tuned with different random seeds can achieve similar in-domain performance but generalize differently on tests of syntactic generalization. In this work, we show that, even within a single model, we can find multiple subnetworks that perform similarly in-domain, but generalize vastly differently. To better understand these phenomena, we investigate if they can be understood in terms of "competing subnetworks": the model initially represents a variety of distinct algorithms, corresponding to different subnetworks, and generalization occurs when it ultimately converges to one. This explanation has been used to account for generalization in simple algorithmic tasks. Instead of finding competing subnetworks, we find that all subnetworks -- whether they generalize or not -- share a set of attention heads, which we refer to as the heuristic core. Further analysis suggests that th
    
[^6]: GUIDE：基于引导的扩散模型增量学习

    GUIDE: Guidance-based Incremental Learning with Diffusion Models

    [https://arxiv.org/abs/2403.03938](https://arxiv.org/abs/2403.03938)

    GUIDE利用扩散模型和分类器引导技术，针对持续训练模型遗忘的信息产生复习示例，显著减少了灾难性遗忘。

    

    我们引入了GUIDE，一种新颖的持续学习方法，该方法指导扩散模型对有被遗忘风险的样本进行复习。现有的生成策略通过从生成模型中随机抽取复习样本来对抗灾难性遗忘。这种方法与基于缓冲区的方法相矛盾，其中采样策略起着重要作用。我们提出通过将扩散模型与分类器引导技术相结合，产生专门针对持续训练模型遗忘信息的复习示例，以弥合这一差距。这种方法使得能够从先前任务分布中生成样本，这些样本在最近遇到的类别情境下更有可能被误分类。我们的实验结果表明，GUIDE显著减少了灾难性遗忘，优于传统的随机抽样方法，并在持续学习方面超过了最近的先进方法。

    arXiv:2403.03938v1 Announce Type: new  Abstract: We introduce GUIDE, a novel continual learning approach that directs diffusion models to rehearse samples at risk of being forgotten. Existing generative strategies combat catastrophic forgetting by randomly sampling rehearsal examples from a generative model. Such an approach contradicts buffer-based approaches where sampling strategy plays an important role. We propose to bridge this gap by integrating diffusion models with classifier guidance techniques to produce rehearsal examples specifically targeting information forgotten by a continuously trained model. This approach enables the generation of samples from preceding task distributions, which are more likely to be misclassified in the context of recently encountered classes. Our experimental results show that GUIDE significantly reduces catastrophic forgetting, outperforming conventional random sampling approaches and surpassing recent state-of-the-art methods in continual learnin
    
[^7]: 使用基于Transformer的生成模型进行极端降水即时预报

    Extreme Precipitation Nowcasting using Transformer-based Generative Models

    [https://arxiv.org/abs/2403.03929](https://arxiv.org/abs/2403.03929)

    使用基于Transformer的生成模型NowcastingGPT-EVL在极端降水预测中表现出优越性能，尤其在处理极端降水事件时。

    

    本文提出了一种创新的方法，通过使用基于Transformer的生成模型，即带有极值损失（EVL）正则化的NowcastingGPT，进行极端降水即时预报。利用来自荷兰皇家气象研究所（KNMI）的全面数据集，我们的研究重点是高精度地预测短期降水。我们引入了一种新颖的计算EVL的方法，而不是假定固定的极端表示，从而解决了当前模型在捕捉极端天气事件方面的局限性。我们展示了定性和定量分析，证明了所提议的NowcastingGPT-EVL在生成准确的降水预报方面的优越性能，特别是在处理极端降水事件时。代码可在\url{https://github.com/Cmeo97/NowcastingGPT}上找到。

    arXiv:2403.03929v1 Announce Type: cross  Abstract: This paper presents an innovative approach to extreme precipitation nowcasting by employing Transformer-based generative models, namely NowcastingGPT with Extreme Value Loss (EVL) regularization. Leveraging a comprehensive dataset from the Royal Netherlands Meteorological Institute (KNMI), our study focuses on predicting short-term precipitation with high accuracy. We introduce a novel method for computing EVL without assuming fixed extreme representations, addressing the limitations of current models in capturing extreme weather events. We present both qualitative and quantitative analyses, demonstrating the superior performance of the proposed NowcastingGPT-EVL in generating accurate precipitation forecasts, especially when dealing with extreme precipitation events. The code is available at \url{https://github.com/Cmeo97/NowcastingGPT}.
    
[^8]: 黑盒$k$-to-$1$-PCA降维：理论与应用

    Black-Box $k$-to-$1$-PCA Reductions: Theory and Applications

    [https://arxiv.org/abs/2403.03905](https://arxiv.org/abs/2403.03905)

    我们的主要贡献是对于$k$-PCA算法中近似参数退化的边界得到了显著更为精确的界限

    

    $k$-主成分分析（$k$-PCA）问题是一种基本的算法原语，在数据分析和降维应用中被广泛使用。在统计环境中，$k$-PCA的目标是识别一个分布的协方差矩阵的顶部特征空间，我们只能通过样本隐式访问这个矩阵。受这些隐式设置的启发，我们分析黑盒缩减方法作为设计$k$-PCA算法的框架，其中我们通过黑盒$1$-PCA预言模拟对未知目标矩阵的访问，该预言返回一个近似的顶部特征向量，根据两个流行的近似概念。尽管这种黑盒方法可能是设计$k$-PCA算法中最自然的基于降维的方法，这种方法，即通过递归调用$1$-PCA预言调用了$k$次，以前很难理解。

    arXiv:2403.03905v1 Announce Type: cross  Abstract: The $k$-principal component analysis ($k$-PCA) problem is a fundamental algorithmic primitive that is widely-used in data analysis and dimensionality reduction applications. In statistical settings, the goal of $k$-PCA is to identify a top eigenspace of the covariance matrix of a distribution, which we only have implicit access to via samples. Motivated by these implicit settings, we analyze black-box deflation methods as a framework for designing $k$-PCA algorithms, where we model access to the unknown target matrix via a black-box $1$-PCA oracle which returns an approximate top eigenvector, under two popular notions of approximation. Despite being arguably the most natural reduction-based approach to $k$-PCA algorithm design, such black-box methods, which recursively call a $1$-PCA oracle $k$ times, were previously poorly-understood.   Our main contribution is significantly sharper bounds on the approximation parameter degradation of
    
[^9]: DART：雷达新视图合成的隐式多普勒层析成像

    DART: Implicit Doppler Tomography for Radar Novel View Synthesis

    [https://arxiv.org/abs/2403.03896](https://arxiv.org/abs/2403.03896)

    DART提出了一种基于雷达特定物理学的隐式多普勒层析成像方法，通过创建反射率和透射率的呈现流水线，实现了优越的雷达距离-多普勒图像合成。

    

    模拟是射频系统设计人员的宝贵工具，可以快速原型化各种用于成像、目标检测、分类和跟踪的算法。本文提出了DART - 多普勒辅助雷达层析成像，这是一种受神经辐射场启发的方法，利用雷达特定的物理学来创建一个基于反射率和透射率的呈现流水线，用于距离-多普勒图像。然后，我们通过构建一个自定义数据收集平台，并与基于激光雷达的定位一起收集了一个新颖的雷达数据集以及准确的位置和瞬时速度测量来评估DART。与最先进的基线方法相比，DART合成了更优越的雷达距离-多普勒图像。

    arXiv:2403.03896v1 Announce Type: cross  Abstract: Simulation is an invaluable tool for radio-frequency system designers that enables rapid prototyping of various algorithms for imaging, target detection, classification, and tracking. However, simulating realistic radar scans is a challenging task that requires an accurate model of the scene, radio frequency material properties, and a corresponding radar synthesis function. Rather than specifying these models explicitly, we propose DART - Doppler Aided Radar Tomography, a Neural Radiance Field-inspired method which uses radar-specific physics to create a reflectance and transmittance-based rendering pipeline for range-Doppler images. We then evaluate DART by constructing a custom data collection platform and collecting a novel radar dataset together with accurate position and instantaneous velocity measurements from lidar-based localization. In comparison to state-of-the-art baselines, DART synthesizes superior radar range-Doppler imag
    
[^10]: 联合多任务学习改进计算病理学中弱监督生物标记预测

    Joint multi-task learning improves weakly-supervised biomarker prediction in computational pathology

    [https://arxiv.org/abs/2403.03891](https://arxiv.org/abs/2403.03891)

    使用弱监督联合多任务学习的新方法，在计算病理学中的生物标记预测中实现了+7.7％的性能改进

    

    深度学习（DL）可以在弱监督设置中直接从数字化的癌症组织学中预测生物标记。最近，基于回归的DL预测连续生物标记引起了越来越多的关注。尽管如此，临床决策通常需要分类结果。因此，我们开发了一种弱监督联合多任务Transformer架构，该架构已在四个公共患者队列上进行了训练和评估，用于预测两个关键的预测性生物标记，微卫星不稳定性（MSI）和同源重组缺陷（HRD），并进行了与肿瘤微环境相关的辅助回归任务的训练。此外，我们在计算病理学中进行了16种任务平衡的弱监督联合多任务学习的综合基准测试。使用我们的新方法，我们的性能优于最先进的受试者工作特征下的面积+7.7％

    arXiv:2403.03891v1 Announce Type: cross  Abstract: Deep Learning (DL) can predict biomarkers directly from digitized cancer histology in a weakly-supervised setting. Recently, the prediction of continuous biomarkers through regression-based DL has seen an increasing interest. Nonetheless, clinical decision making often requires a categorical outcome. Consequently, we developed a weakly-supervised joint multi-task Transformer architecture which has been trained and evaluated on four public patient cohorts for the prediction of two key predictive biomarkers, microsatellite instability (MSI) and homologous recombination deficiency (HRD), trained with auxiliary regression tasks related to the tumor microenvironment. Moreover, we perform a comprehensive benchmark of 16 approaches of task balancing for weakly-supervised joint multi-task learning in computational pathology. Using our novel approach, we improve over the state-of-the-art area under the receiver operating characteristic by +7.7%
    
[^11]: 分层扩散策略用于考虑运动学的多任务机器人操作

    Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation

    [https://arxiv.org/abs/2403.03890](https://arxiv.org/abs/2403.03890)

    这项研究提出了一种分层扩散策略（HDP）用于多任务机器人操作，通过将操纵策略分解为分层结构，同时解决长期任务规划和生成细粒度的低层动作，同时提出了一种新颖的机器人运动学感知目标条件控制代理（RK-Diffuser）。

    

    本文介绍了分层扩散策略（HDP），这是一种用于多任务机器人操作的分层代理。HDP将操纵策略分解为分层结构：高层任务规划代理预测远端最佳末端执行器姿势（NBP），低层目标条件扩散策略生成最佳运动轨迹。这种分解的策略表示使HDP能够同时解决长期任务规划和生成细粒度的低层动作。为了生成符合机器人运动学约束的上下文感知运动轨迹，我们提出了一种新颖的运动学感知目标条件控制代理，机器人运动学扩散器（RK-Diffuser）。具体来说，RK-Diffuser学习生成末端执行器姿势和关节位置轨迹，并将精确但缺乏运动学意识的末端执行器姿势扩散器提炼为运动学感知但不太精确的关节位置。

    arXiv:2403.03890v1 Announce Type: cross  Abstract: This paper introduces Hierarchical Diffusion Policy (HDP), a hierarchical agent for multi-task robotic manipulation. HDP factorises a manipulation policy into a hierarchical structure: a high-level task-planning agent which predicts a distant next-best end-effector pose (NBP), and a low-level goal-conditioned diffusion policy which generates optimal motion trajectories. The factorised policy representation allows HDP to tackle both long-horizon task planning while generating fine-grained low-level actions. To generate context-aware motion trajectories while satisfying robot kinematics constraints, we present a novel kinematics-aware goal-conditioned control agent, Robot Kinematics Diffuser (RK-Diffuser). Specifically, RK-Diffuser learns to generate both the end-effector pose and joint position trajectories, and distill the accurate but kinematics-unaware end-effector pose diffuser to the kinematics-aware but less accurate joint positio
    
[^12]: 使用扩散模型进行潜在数据集蒸馏

    Latent Dataset Distillation with Diffusion Models

    [https://arxiv.org/abs/2403.03881](https://arxiv.org/abs/2403.03881)

    这项研究提出了使用扩散模型进行潜在数据集蒸馏（LD3M），结合潜在空间中的扩散和数据集蒸馏的方法，以解决不同模型架构导致准确性下降和生成高分辨率图像的挑战。

    

    机器学习的有效性传统上依赖于越来越大的数据集的可用性。然而，大型数据集带来存储挑战，并且包含一些非影响力样本，在训练过程中可以被忽略而不影响模型最终的准确性。为了应对这些限制，出现了将数据集信息蒸馏成一组压缩样本（合成样本），即蒸馏数据集的概念。其中一个关键方面是选择用于连接原始和合成数据集的架构（通常是ConvNet）。然而，如果所使用的模型架构与蒸馏过程中使用的模型不同，则最终准确性会降低。另一个挑战是生成高分辨率图像，例如128x128及更高。

    arXiv:2403.03881v1 Announce Type: cross  Abstract: The efficacy of machine learning has traditionally relied on the availability of increasingly larger datasets. However, large datasets pose storage challenges and contain non-influential samples, which could be ignored during training without impacting the final accuracy of the model. In response to these limitations, the concept of distilling the information on a dataset into a condensed set of (synthetic) samples, namely a distilled dataset, emerged. One crucial aspect is the selected architecture (usually ConvNet) for linking the original and synthetic datasets. However, the final accuracy is lower if the employed model architecture differs from the model used during distillation. Another challenge is the generation of high-resolution images, e.g., 128x128 and higher. In this paper, we propose Latent Dataset Distillation with Diffusion Models (LD3M) that combine diffusion in latent space with dataset distillation to tackle both chal
    
[^13]: 图神经网络的输出几乎肯定是渐近常数

    Graph neural network outputs are almost surely asymptotically constant

    [https://arxiv.org/abs/2403.03880](https://arxiv.org/abs/2403.03880)

    研究表明，图神经网络的输出将渐近于一个常数函数，并限制了这些分类器的统一表达能力。

    

    图神经网络（GNNs）是各种图学习任务中主要的架构。我们通过研究GNN的概率分类器在从某个随机图模型中绘制的更大图上应用时预测如何演变，提出了GNN表达能力的新角度。我们展示了输出收敛到一个常数函数，这个函数上限了这些分类器可以统一表达的内容。这种收敛现象适用于非常广泛的GNN类别，包括先进模型，其中的聚合包括平均值和基于注意力的图转换器机制。我们的结果适用于各种随机图模型，包括（稀疏的）Erd\H{o}s-R\'enyi模型和随机块模型。我们通过实证验证这些发现，观察到收敛现象已经在相对适中规模的图中显现。

    arXiv:2403.03880v1 Announce Type: new  Abstract: Graph neural networks (GNNs) are the predominant architectures for a variety of learning tasks on graphs. We present a new angle on the expressive power of GNNs by studying how the predictions of a GNN probabilistic classifier evolve as we apply it on larger graphs drawn from some random graph model. We show that the output converges to a constant function, which upper-bounds what these classifiers can express uniformly. This convergence phenomenon applies to a very wide class of GNNs, including state of the art models, with aggregates including mean and the attention-based mechanism of graph transformers. Our results apply to a broad class of random graph models, including the (sparse) Erd\H{o}s-R\'enyi model and the stochastic block model. We empirically validate these findings, observing that the convergence phenomenon already manifests itself on graphs of relatively modest size.
    
[^14]: 面向垂直分区数据的解耦式垂直联邦学习，用于实际训练

    Decoupled Vertical Federated Learning for Practical Training on Vertically Partitioned Data

    [https://arxiv.org/abs/2403.03871](https://arxiv.org/abs/2403.03871)

    提出了Decoupled VFL（DVFL），一种面向VFL的分段学习方法，实现了分散聚合和隔离，从而提高了容错性。

    

    垂直联邦学习（VFL）是一种新兴的分布式机器学习范式，其中共同实体的不同特征所有者合作学习全局模型而无需共享数据。在VFL中，主机客户端拥有每个实体的数据标签，并基于所有客户端的中间本地表示学习最终表示。因此，主机是一个单点故障，标签反馈可以被恶意客户端用来推断私有特征。要求所有参与者在整个训练过程中保持活跃和值得信赖通常是不切实际的，在受控环境之外完全不可行。我们提出了一种面向VFL的分段学习方法Decoupled VFL（DVFL）。通过在各自的目标上训练每个模型，DVFL允许特征学习和标签监督之间的分散聚合和隔离。具有这些属性，DVFL具有容错性。

    arXiv:2403.03871v1 Announce Type: new  Abstract: Vertical Federated Learning (VFL) is an emergent distributed machine learning paradigm wherein owners of disjoint features of a common set of entities collaborate to learn a global model without sharing data. In VFL, a host client owns data labels for each entity and learns a final representation based on intermediate local representations from all guest clients. Therefore, the host is a single point of failure and label feedback can be used by malicious guest clients to infer private features. Requiring all participants to remain active and trustworthy throughout the entire training process is generally impractical and altogether infeasible outside of controlled environments. We propose Decoupled VFL (DVFL), a blockwise learning approach to VFL. By training each model on its own objective, DVFL allows for decentralized aggregation and isolation between feature learning and label supervision. With these properties, DVFL is fault tolerant
    
[^15]: 学习多个语言模型的协作解码方法

    Learning to Decode Collaboratively with Multiple Language Models

    [https://arxiv.org/abs/2403.03870](https://arxiv.org/abs/2403.03870)

    学习了一种协作解码方法，通过在标记级别交错生成来教授多个大型语言模型协作，无需直接监督，在特定任务中融合每个模型的专业知识，提高了联合系统的性能。

    

    我们提出了一种方法，通过在标记级别交错它们的生成来教授多个大型语言模型（LLM）协作。我们将下一个标记由哪个LLM生成的决策建模为潜变量。通过在我们的潜变量模型下优化训练集的边际似然，基础LLM自动学习何时生成自身以及何时调用其中一个“助手”语言模型来生成，而无需直接监督。在解码过程中进行标记级协作允许融合每个模型的专业知识，以符合特定任务的方式。我们的协作解码在跨领域设置中特别有用，其中通用基础LLM学会调用领域专家模型。在执行指令、领域特定问答和推理任务时，我们展示了联合系统的性能优于单独模型。通过对学习到的潜

    arXiv:2403.03870v1 Announce Type: new  Abstract: We propose a method to teach multiple large language models (LLM) to collaborate by interleaving their generations at the token level. We model the decision of which LLM generates the next token as a latent variable. By optimizing the marginal likelihood of a training set under our latent variable model, the base LLM automatically learns when to generate itself and when to call on one of the ``assistant'' language models to generate, all without direct supervision. Token-level collaboration during decoding allows for a fusion of each model's expertise in a manner tailored to the specific task at hand. Our collaborative decoding is especially useful in cross-domain settings where a generalist base LLM learns to invoke domain expert models. On instruction-following, domain-specific QA, and reasoning tasks, we show that the performance of the joint system exceeds that of the individual models. Through qualitative analysis of the learned lat
    
[^16]: 论大型语言模型中线性表示的起源

    On the Origins of Linear Representations in Large Language Models

    [https://arxiv.org/abs/2403.03867](https://arxiv.org/abs/2403.03867)

    本文研究了大型语言模型中线性表示的起源，通过引入简单的潜变量模型，并展示了梯度下降的隐性偏差与下一个标记预测目标共同促进了概念的线性表示。

    

    近期研究表明，大型语言模型的表示空间中编码了高层语义概念是"线性"的。本文研究了这种线性表示的起源。为此，我们引入了一个简单的潜变量模型来抽象和形式化下一个标记预测的概念动态。我们利用这种形式化展示了下一个标记预测目标（具有交叉熵的softmax）和梯度下降的隐性偏差共同促进了概念的线性表示。实验表明，当学习与潜变量模型匹配的数据时，线性表示会出现，从而确认这种简单结构已足以产生线性表示。我们还使用 LLaMA-2 大型语言模型验证了这一理论的部分预测，证明了简化模型提供了可推广的见解。

    arXiv:2403.03867v1 Announce Type: new  Abstract: Recent works have argued that high-level semantic concepts are encoded "linearly" in the representation space of large language models. In this work, we study the origins of such linear representations. To that end, we introduce a simple latent variable model to abstract and formalize the concept dynamics of the next token prediction. We use this formalism to show that the next token prediction objective (softmax with cross-entropy) and the implicit bias of gradient descent together promote the linear representation of concepts. Experiments show that linear representations emerge when learning from data matching the latent variable model, confirming that this simple structure already suffices to yield linear representations. We additionally confirm some predictions of the theory using the LLaMA-2 large language model, giving evidence that the simplified model yields generalizable insights.
    
[^17]: 为少样本示例选择设计信息度量

    Designing Informative Metrics for Few-Shot Example Selection

    [https://arxiv.org/abs/2403.03861](https://arxiv.org/abs/2403.03861)

    提出了一种基于复杂度的提示选择方法，用于将示例与测试句子的句法-语义复杂度对齐，在少样本NER任务中取得了显著的性能提升。

    

    预训练语言模型（PLMs）在提供适当格式的示例时展现出了卓越的少样本学习能力。然而，选择“最佳”示例仍然是一个未解决的挑战。我们提出了一种基于复杂度的提示选择方法，适用于序列标注任务。该方法避免了训练一个专门用于选择示例的模型，而是使用特定的度量标准来对齐测试句子和示例的句法-语义复杂度。我们使用句子和单词级别的度量标准，将示例的复杂度与考虑中的（测试）句子进行匹配。我们的结果表明，我们的方法能够从PLMs中提取出更好的性能：在少样本NER上实现了最先进的性能，在CoNLL2003数据集上对GPT-4的F1分数实现了5%的绝对改善。我们还在像GPT-j-6B这样的较小模型中看到了高达28.85个点（F1/Acc.）的显著增益。

    arXiv:2403.03861v1 Announce Type: new  Abstract: Pretrained language models (PLMs) have shown remarkable few-shot learning capabilities when provided with properly formatted examples. However, selecting the "best" examples remains an open challenge. We propose a complexity-based prompt selection approach for sequence tagging tasks. This approach avoids the training of a dedicated model for selection of examples, and instead uses certain metrics to align the syntactico-semantic complexity of test sentences and examples. We use both sentence- and word-level metrics to match the complexity of examples to the (test) sentence being considered. Our results demonstrate that our approach extracts greater performance from PLMs: it achieves state-of-the-art performance on few-shot NER, achieving a 5% absolute improvement in F1 score on the CoNLL2003 dataset for GPT-4. We also see large gains of upto 28.85 points (F1/Acc.) in smaller models like GPT-j-6B.
    
[^18]: 公共数据辅助下的私有随机优化：动力和限制

    Public-data Assisted Private Stochastic Optimization: Power and Limitations

    [https://arxiv.org/abs/2403.03856](https://arxiv.org/abs/2403.03856)

    该研究研究了公共数据辅助的私有差分隐私算法在随机凸优化问题中的限制和能力，展示出简单策略在这一领域的最优性。

    

    我们研究了公共数据辅助的差分隐私（PA-DP）算法的限制和能力。具体来说，我们关注具有标记或未标记公共数据的随机凸优化（SCO）问题。对于完整/标记的公共数据，我们表明任何$(\epsilon,\delta)$-PA-DP都具有超出风险$\tilde{\Omega}\big(\min\big\{\frac{1}{\sqrt{n_{\text{pub}}}},\frac{1}{\sqrt{n}}+\frac{\sqrt{d}}{n\epsilon} \big\} \big)$，其中$d$是维数，${n_{\text{pub}}}$是公共样本数量，${n_{\text{priv}}}$是私有样本数量，$n={n_{\text{pub}}}+{n_{\text{priv}}}$. 这些下界是通过我们对PA-DP均值估计的新下界建立的，其形式相似。在常数因素的影响下，这些下界表明将所有数据视为私有或丢弃私有数据的简单策略是最优的。我们还研究了具有未标记公共数据的PA-DP监督学习。

    arXiv:2403.03856v1 Announce Type: new  Abstract: We study the limits and capability of public-data assisted differentially private (PA-DP) algorithms. Specifically, we focus on the problem of stochastic convex optimization (SCO) with either labeled or unlabeled public data. For complete/labeled public data, we show that any $(\epsilon,\delta)$-PA-DP has excess risk $\tilde{\Omega}\big(\min\big\{\frac{1}{\sqrt{n_{\text{pub}}}},\frac{1}{\sqrt{n}}+\frac{\sqrt{d}}{n\epsilon} \big\} \big)$, where $d$ is the dimension, ${n_{\text{pub}}}$ is the number of public samples, ${n_{\text{priv}}}$ is the number of private samples, and $n={n_{\text{pub}}}+{n_{\text{priv}}}$. These lower bounds are established via our new lower bounds for PA-DP mean estimation, which are of a similar form. Up to constant factors, these lower bounds show that the simple strategy of either treating all data as private or discarding the private data, is optimal. We also study PA-DP supervised learning with \textit{unlabe
    
[^19]: 加速基于分数的扩散模型的收敛性，有保证

    Accelerating Convergence of Score-Based Diffusion Models, Provably

    [https://arxiv.org/abs/2403.03852](https://arxiv.org/abs/2403.03852)

    设计了新颖的无需训练的算法，以加速流行的确定性和随机采样器，改进了确定性采样器的收敛速率至$O(1/{T}^2)$，提升了随机采样器的收敛速率至$O(1/T)$。

    

    基于分数的扩散模型在实践中取得了显著的经验性能，但通常由于在采样阶段需要进行大量函数评估而导致采样速度较慢。尽管近年来一系列工作致力于加速扩散生成建模，但加速技术的理论基础仍然严重有限。在本文中，我们设计了新颖的无需训练的算法来加速流行的确定性（即DDIM）和随机（即DDPM）采样器。我们的加速确定性采样器以$O(1/{T}^2)$的速率收敛，其中$T$为步数，改进了DDIM采样器的$O(1/T)$速率；而我们的加速随机采样器以$O(1/T)$的速率收敛，优于DDPM采样器的$O(1/\sqrt{T})$速率。我们算法的设计利用了更高阶逼近的见解，并具有类似于流行的高阶ODE求解器的直觉。

    arXiv:2403.03852v1 Announce Type: cross  Abstract: Score-based diffusion models, while achieving remarkable empirical performance, often suffer from low sampling speed, due to extensive function evaluations needed during the sampling phase. Despite a flurry of recent activities towards speeding up diffusion generative modeling in practice, theoretical underpinnings for acceleration techniques remain severely limited. In this paper, we design novel training-free algorithms to accelerate popular deterministic (i.e., DDIM) and stochastic (i.e., DDPM) samplers. Our accelerated deterministic sampler converges at a rate $O(1/{T}^2)$ with $T$ the number of steps, improving upon the $O(1/T)$ rate for the DDIM sampler; and our accelerated stochastic sampler converges at a rate $O(1/T)$, outperforming the rate $O(1/\sqrt{T})$ for the DDPM sampler. The design of our algorithms leverages insights from higher-order approximation, and shares similar intuitions as popular high-order ODE solvers like 
    
[^20]: 利用椭球集进行多维时间序列的合规预测

    Conformal prediction for multi-dimensional time series by ellipsoidal sets

    [https://arxiv.org/abs/2403.03850](https://arxiv.org/abs/2403.03850)

    开发了一种名为$\texttt{MultiDimSPCI}$的顺序CP方法，用于在多元时间序列中构建预测区域，具有更小的预测区域和有效的覆盖。

    

    合规预测（CP）因其无需假设分布、不受模型限制且在理论上可靠而成为一种流行的不确定性量化方法。对于监督学习中的预测问题，大多数CP方法专注于为单变量响应构建预测区间。在本文中，我们开发了一种名为$\texttt{MultiDimSPCI}$的顺序CP方法，用于为多元响应构建预测区域，特别是在不可交换的多元时间序列环境中。在理论上，我们估计了条件覆盖间隙的有限样本高概率界限。在实证方面，我们证明了$\texttt{MultiDimSPCI}$在各种多元时间序列上保持有效覆盖，同时产生比CP和非CP基线更小的预测区域。

    arXiv:2403.03850v1 Announce Type: cross  Abstract: Conformal prediction (CP) has been a popular method for uncertainty quantification because it is distribution-free, model-agnostic, and theoretically sound. For forecasting problems in supervised learning, most CP methods focus on building prediction intervals for univariate responses. In this work, we develop a sequential CP method called $\texttt{MultiDimSPCI}$ that builds prediction regions for a multivariate response, especially in the context of multivariate time series, which are not exchangeable. Theoretically, we estimate finite-sample high-probability bounds on the conditional coverage gap. Empirically, we demonstrate that $\texttt{MultiDimSPCI}$ maintains valid coverage on a wide range of multivariate time series while producing smaller prediction regions than CP and non-CP baselines.
    
[^21]: MedMamba: 用于医学图像分类的Vision Mamba

    MedMamba: Vision Mamba for Medical Image Classification

    [https://arxiv.org/abs/2403.03849](https://arxiv.org/abs/2403.03849)

    提出了Vision Mamba用于医学图像分类，结合了卷积层的局部特征提取能力和SSM捕捉长距离依赖性的能力。

    

    医学图像分类是计算机视觉领域中非常基础和关键的任务。近年来，基于CNN和Transformer的模型被广泛应用于分类各种医学图像。不幸的是，CNN在长距离建模能力方面存在局限，无法有效提取医学图像中的细粒度特征，而Transformers受到二次计算复杂度的阻碍。最近的研究表明，由Mamba表示的状态空间模型（SSM）可以高效地建模长距离交互作用同时保持线性计算复杂度。受此启发，我们提出了用于医学图像分类的Vision Mamba（MedMamba）。更具体地，我们引入了一个新颖的Conv-SSM模块，将卷积层的局部特征提取能力与SSM捕捉长距离依赖性的能力结合在一起。为了展示MedMamba的潜力，我们进行了

    arXiv:2403.03849v1 Announce Type: cross  Abstract: Medical image classification is a very fundamental and crucial task in the field of computer vision. These years, CNN-based and Transformer-based models are widely used in classifying various medical images. Unfortunately, The limitation of CNNs in long-range modeling capabilities prevent them from effectively extracting fine-grained features in medical images , while Transformers are hampered by their quadratic computational complexity. Recent research has shown that the state space model (SSM) represented by Mamba can efficiently model long-range interactions while maintaining linear computational complexity. Inspired by this, we propose Vision Mamba for medical image classification (MedMamba). More specifically, we introduce a novel Conv-SSM module, which combines the local feature extraction ability of convolutional layers with the ability of SSM to capture long-range dependency. To demonstrate the potential of MedMamba, we conduct
    
[^22]: 三维受限空间中的灵巧四肢运动与强化学习

    Dexterous Legged Locomotion in Confined 3D Spaces with Reinforcement Learning

    [https://arxiv.org/abs/2403.03848](https://arxiv.org/abs/2403.03848)

    这项研究提出了在受限的三维空间中以目标导向为基础的端到端学习方法，以解决灵巧四肢运动和强化学习之间的挑战。

    

    利用深度强化学习（RL）的运动控制器的最新进展在面对具有挑战性的地形时取得了令人印象深刻的结果，如崎岖的岩石、非刚性地面和滑溜的表面。然而，尽管这些控制器主要解决了机器人底部的挑战，但相对较少的研究研究了在受限的三维空间中的四肢移动性，比如狭窄的隧道或不规则的空洞，这些空间施加了全方位的约束。现有基于RL的方法产生的周期性步态模式学习参数化的运动技能，这些运动技能具有运动参数，如速度和身体高度，可能不足以使机器人在具有挑战性的受限三维空间中导航，需要具备敏捷的三维障碍物回避和强大的四肢运动。相反，我们建议从在受限的三维空间中的目标导航端到端地学习运动技能。

    arXiv:2403.03848v1 Announce Type: cross  Abstract: Recent advances of locomotion controllers utilizing deep reinforcement learning (RL) have yielded impressive results in terms of achieving rapid and robust locomotion across challenging terrain, such as rugged rocks, non-rigid ground, and slippery surfaces. However, while these controllers primarily address challenges underneath the robot, relatively little research has investigated legged mobility through confined 3D spaces, such as narrow tunnels or irregular voids, which impose all-around constraints. The cyclic gait patterns resulted from existing RL-based methods to learn parameterized locomotion skills characterized by motion parameters, such as velocity and body height, may not be adequate to navigate robots through challenging confined 3D spaces, requiring both agile 3D obstacle avoidance and robust legged locomotion. Instead, we propose to learn locomotion skills end-to-end from goal-oriented navigation in confined 3D spaces. 
    
[^23]: 在缓解预训练编码器中后门问题中蒸馏的有效性研究

    On the Effectiveness of Distillation in Mitigating Backdoors in Pre-trained Encoder

    [https://arxiv.org/abs/2403.03846](https://arxiv.org/abs/2403.03846)

    研究了如何利用蒸馏从受污染的预训练编码器中提取良性知识，将其传递给新编码器，成功降低攻击成功率，并探讨了蒸馏的核心组件对性能的影响。

    

    在本文中，我们研究了一种用于SSL中防御受污染编码器的方法，叫做蒸馏，这个方法最初是用于监督学习中的防御机制。蒸馏旨在从给定模型（称为教师网络）中提炼知识，并将其传递给另一个模型（称为学生网络）。我们现在使用它从受污染的预训练编码器中提炼良性知识，并将其传递给一个新编码器，从而得到一个干净的预训练编码器。具体来说，我们对蒸馏对抗受污染编码器的有效性和性能进行了实证研究。我们使用了两种最先进的针对预训练图像编码器的后门攻击方法和四个常用的图像分类数据集，实验结果表明，蒸馏可以将攻击成功率从80.87%降低到27.51%，而准确率下降了6.35%。此外，我们研究了蒸馏的三个核心组件对性能的影响：教师网络、学生网络和

    arXiv:2403.03846v1 Announce Type: new  Abstract: In this paper, we study a defense against poisoned encoders in SSL called distillation, which is a defense used in supervised learning originally. Distillation aims to distill knowledge from a given model (a.k.a the teacher net) and transfer it to another (a.k.a the student net). Now, we use it to distill benign knowledge from poisoned pre-trained encoders and transfer it to a new encoder, resulting in a clean pre-trained encoder. In particular, we conduct an empirical study on the effectiveness and performance of distillation against poisoned encoders. Using two state-of-the-art backdoor attacks against pre-trained image encoders and four commonly used image classification datasets, our experimental results show that distillation can reduce attack success rate from 80.87% to 27.51% while suffering a 6.35% loss in accuracy. Moreover, we investigate the impact of three core components of distillation on performance: teacher net, student n
    
[^24]: 特征选择作为深度顺序生成学习

    Feature Selection as Deep Sequential Generative Learning

    [https://arxiv.org/abs/2403.03838](https://arxiv.org/abs/2403.03838)

    将特征选择视为深度顺序生成学习任务，通过开发深度变分转换模型，可以精炼特征选择知识并学习嵌入空间，实现特征子集的生成决策序列。

    

    特征选择旨在确定最具图案区分特征子集。我们提出将所选特征子集视为选择决策令牌序列，将特征选择重新构想为一种深度顺序生成学习任务，精炼特征知识并生成决策序列。

    arXiv:2403.03838v1 Announce Type: new  Abstract: Feature selection aims to identify the most pattern-discriminative feature subset. In prior literature, filter (e.g., backward elimination) and embedded (e.g., Lasso) methods have hyperparameters (e.g., top-K, score thresholding) and tie to specific models, thus, hard to generalize; wrapper methods search a feature subset in a huge discrete space and is computationally costly. To transform the way of feature selection, we regard a selected feature subset as a selection decision token sequence and reformulate feature selection as a deep sequential generative learning task that distills feature knowledge and generates decision sequences. Our method includes three steps: (1) We develop a deep variational transformer model over a joint of sequential reconstruction, variational, and performance evaluator losses. Our model can distill feature selection knowledge and learn a continuous embedding space to map feature selection decision sequences
    
[^25]: Cobweb：一种增量和分层式的人类类别学习模型

    Cobweb: An Incremental and Hierarchical Model of Human-Like Category Learning

    [https://arxiv.org/abs/2403.03835](https://arxiv.org/abs/2403.03835)

    Cobweb是一种类似人类类别学习系统，采用类别效用度量构建分层组织的类似树状结构，能够捕捉心理效应并在单一模型中展现出实例和原型学习的灵活性，为将来研究人类类别学习提供了基础。

    

    Cobweb是一种类似人类的类别学习系统，与其他增量分类模型不同的是，它利用类别效用度量构建分层组织的类似树状结构。先前的研究表明，Cobweb能够捕捉心理效应，如基本水平、典型性和扇形效应。然而，对Cobweb作为人类分类模型的更广泛评估仍然缺乏。本研究填补了这一空白。它确定了Cobweb与经典的人类类别学习效应的一致性。还探讨了Cobweb展现出在单一模型中既有实例又有原型学习的灵活性。这些发现为将来研究Cobweb作为人类类别学习的综合模型奠定了基础。

    arXiv:2403.03835v1 Announce Type: cross  Abstract: Cobweb, a human like category learning system, differs from other incremental categorization models in constructing hierarchically organized cognitive tree-like structures using the category utility measure. Prior studies have shown that Cobweb can capture psychological effects such as the basic level, typicality, and fan effects. However, a broader evaluation of Cobweb as a model of human categorization remains lacking. The current study addresses this gap. It establishes Cobweb's alignment with classical human category learning effects. It also explores Cobweb's flexibility to exhibit both exemplar and prototype like learning within a single model. These findings set the stage for future research on Cobweb as a comprehensive model of human category learning.
    
[^26]: 在L-BFGS-B算法的$\ell_1$和group-Lasso正则化下进行线性和非线性系统识别

    Linear and nonlinear system identification under $\ell_1$- and group-Lasso regularization via L-BFGS-B

    [https://arxiv.org/abs/2403.03827](https://arxiv.org/abs/2403.03827)

    本文提出了一种基于L-BFGS-B算法的方法，可用于在$\ell_1$和group-Lasso正则化下识别线性和非线性系统，相比传统线性子空间方法，该方法在结果、损失和正则化项使用的通用性以及数值稳定性方面通常提供更好的表现，并且可以广泛应用于各种参数化非线性状态空间模型的识别。

    

    在本文中，我们提出了一种基于L-BFGS-B算法的方法，用于识别可能在$\ell_1$和group-Lasso正则化下的线性和非线性离散时间状态空间模型。针对线性模型的识别，我们展示了与经典线性子空间方法相比，该方法通常提供更好的结果，在损失和正则化项的使用方面更加通用，也在数值上更加稳定。该方法不仅丰富了现有的线性系统识别工具集，还可以应用于识别包括循环神经网络在内的非常广泛的参数化非线性状态空间模型。我们在合成和实验数据集上演示了该方法，并将其应用于解决Weigand等人（2022年）提出的具有挑战性的工业机器人基准的非线性多输入/多输出系统识别。

    arXiv:2403.03827v1 Announce Type: cross  Abstract: In this paper, we propose an approach for identifying linear and nonlinear discrete-time state-space models, possibly under $\ell_1$- and group-Lasso regularization, based on the L-BFGS-B algorithm. For the identification of linear models, we show that, compared to classical linear subspace methods, the approach often provides better results, is much more general in terms of the loss and regularization terms used, and is also more stable from a numerical point of view. The proposed method not only enriches the existing set of linear system identification tools but can be also applied to identifying a very broad class of parametric nonlinear state-space models, including recurrent neural networks. We illustrate the approach on synthetic and experimental datasets and apply it to solve the challenging industrial robot benchmark for nonlinear multi-input/multi-output system identification proposed by Weigand et al. (2022). A Python impleme
    
[^27]: 针对方差减少：具有噪声参数的黑盒模拟器的鲁棒贝叶斯优化

    Targeted Variance Reduction: Robust Bayesian Optimization of Black-Box Simulators with Noise Parameters

    [https://arxiv.org/abs/2403.03816](https://arxiv.org/abs/2403.03816)

    该论文提出了一种针对具有噪声参数的黑盒模拟器的鲁棒贝叶斯优化方法，在优化过程中联合考虑控制参数和不确定性参数，以有效减少方差并充分利用控制与噪声的交互作用。

    

    在许多科学应用中，需要优化控制参数$\mathbf{x}$的黑盒模拟器。在这些应用中，模拟器通常采用形式$f(\mathbf{x},\boldsymbol{\theta})$，其中$\boldsymbol{\theta}$是实践中不确定的参数。鲁棒优化的目标是优化期望$\mathbb{E}[f(\mathbf{x},\boldsymbol{\Theta})]$，其中$\boldsymbol{\Theta} \sim \mathcal{P}$是模拟$\boldsymbol{\theta}$不确定性的随机变量。为此，现有的黑盒方法通常采用两阶段方法来选择下一个点$(\mathbf{x},\boldsymbol{\theta})$，其中$\mathbf{x}$和$\boldsymbol{\theta}$通过不同的收获函数分别优化。因此，这些方法未对$(\mathbf{x},\boldsymbol{\theta})$进行联合获取，可能无法充分利用控制与噪声相互作用实现有效的鲁棒优化。

    arXiv:2403.03816v1 Announce Type: cross  Abstract: The optimization of a black-box simulator over control parameters $\mathbf{x}$ arises in a myriad of scientific applications. In such applications, the simulator often takes the form $f(\mathbf{x},\boldsymbol{\theta})$, where $\boldsymbol{\theta}$ are parameters that are uncertain in practice. Robust optimization aims to optimize the objective $\mathbb{E}[f(\mathbf{x},\boldsymbol{\Theta})]$, where $\boldsymbol{\Theta} \sim \mathcal{P}$ is a random variable that models uncertainty on $\boldsymbol{\theta}$. For this, existing black-box methods typically employ a two-stage approach for selecting the next point $(\mathbf{x},\boldsymbol{\theta})$, where $\mathbf{x}$ and $\boldsymbol{\theta}$ are optimized separately via different acquisition functions. As such, these approaches do not employ a joint acquisition over $(\mathbf{x},\boldsymbol{\theta})$, and thus may fail to fully exploit control-to-noise interactions for effective robust opti
    
[^28]: ProbSAINT：概率表格回归用于二手车定价

    ProbSAINT: Probabilistic Tabular Regression for Used Car Pricing

    [https://arxiv.org/abs/2403.03812](https://arxiv.org/abs/2403.03812)

    ProbSAINT是一种提出了一种原则性方法来量化其价格预测的不确定性，并且提供了与最先进的提升技术相媲美的准确点预测的模型。

    

    二手车定价是汽车行业的关键领域，受许多经济因素和市场动态的影响。随着在线市场的激增和二手车需求的增加，准确的定价将使买家和卖家受益，确保公平交易。然而，当前向使用机器学习的自动定价算法的转变需要理解模型不确定性，特别是标记模型不确定的预测的能力。虽然最近的文献提出使用提升算法或基于最近邻的方法进行迅速和精准的价格预测，但用这些算法封装模型不确定性面临着复杂的挑战。我们介绍了ProbSAINT，这是一种模型，提供了一个原则性的方法来量化其价格预测的不确定性，以及与最先进的提升技术相媲美的准确点预测。

    arXiv:2403.03812v1 Announce Type: cross  Abstract: Used car pricing is a critical aspect of the automotive industry, influenced by many economic factors and market dynamics. With the recent surge in online marketplaces and increased demand for used cars, accurate pricing would benefit both buyers and sellers by ensuring fair transactions. However, the transition towards automated pricing algorithms using machine learning necessitates the comprehension of model uncertainties, specifically the ability to flag predictions that the model is unsure about. Although recent literature proposes the use of boosting algorithms or nearest neighbor-based approaches for swift and precise price predictions, encapsulating model uncertainties with such algorithms presents a complex challenge. We introduce ProbSAINT, a model that offers a principled approach for uncertainty quantification of its price predictions, along with accurate point predictions that are comparable to state-of-the-art boosting tec
    
[^29]: 委托-代理赌博游戏中的激励学习

    Incentivized Learning in Principal-Agent Bandit Games

    [https://arxiv.org/abs/2403.03811](https://arxiv.org/abs/2403.03811)

    本文考虑了一个重复的委托-代理赌博游戏，在其中委托方通过提供激励以影响代理方的决策，目标是迭代学习激励策略最大化效用，提出了关于委托方后悔的几乎最优学习算法，支撑理论保证通过数值实验。

    

    本文考虑了一个重复的委托-代理赌博游戏，委托方只能通过代理与环境互动。委托方和代理方的目标不一致，选择行动的权利仅归代理方所有。然而，委托方可以通过提供激励来影响代理方的决策，激励将计入其奖励之中。委托方的目标是迭代学习一种激励策略，以最大化其总效用。这一框架扩展了传统的赌博问题，并受到了几个实际应用的启发，如医疗保健或生态税收，传统的机制设计理论往往忽视了问题的学习方面。我们在多臂和线性背景设置下提出了关于委托方后悔的几乎最优（相对于一个时域 T 的）学习算法。最后，我们通过数值实验支持我们的理论保证。

    arXiv:2403.03811v1 Announce Type: cross  Abstract: This work considers a repeated principal-agent bandit game, where the principal can only interact with her environment through the agent. The principal and the agent have misaligned objectives and the choice of action is only left to the agent. However, the principal can influence the agent's decisions by offering incentives which add up to his rewards. The principal aims to iteratively learn an incentive policy to maximize her own total utility. This framework extends usual bandit problems and is motivated by several practical applications, such as healthcare or ecological taxation, where traditionally used mechanism design theories often overlook the learning aspect of the problem. We present nearly optimal (with respect to a horizon $T$) learning algorithms for the principal's regret in both multi-armed and linear contextual settings. Finally, we support our theoretical guarantees through numerical experiments.
    
[^30]: 神经执行：学习执行触发器用于提示注入攻击

    Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks

    [https://arxiv.org/abs/2403.03792](https://arxiv.org/abs/2403.03792)

    通过学习方法生成的执行触发器可以比当前手工制作的触发器更加有效，并表现出形状、属性和功能上的固有灵活性。

    

    我们引入了一种名为神经执行（Neural Exec）的新型提示注入攻击。与依赖手工制作的字符串（例如“忽略先前的指令并...”）的已知攻击不同，我们展示了将创建执行触发器概念化为可微分搜索问题，并使用基于学习的方法自动生成执行触发器的可能性。我们的结果表明，一个积极进取的对手可以伪造出不仅比当前手工制作的触发器更加有效，而且在形状、属性和功能上表现出固有灵活性的触发器。在这方面，我们展示了攻击者可以设计和生成能够在经历多阶段预处理流水线的情况下持久存在的神经执行（Neural Exec），例如在基于检索增强生成（RAG）的应用中。更为关键的是，我们的发现表明，攻击者可以生成与任何已知攻击明显不同的形式和形状的触发器，绕过...

    arXiv:2403.03792v1 Announce Type: cross  Abstract: We introduce a new family of prompt injection attacks, termed Neural Exec. Unlike known attacks that rely on handcrafted strings (e.g., "Ignore previous instructions and..."), we show that it is possible to conceptualize the creation of execution triggers as a differentiable search problem and use learning-based methods to autonomously generate them.   Our results demonstrate that a motivated adversary can forge triggers that are not only drastically more effective than current handcrafted ones but also exhibit inherent flexibility in shape, properties, and functionality. In this direction, we show that an attacker can design and generate Neural Execs capable of persisting through multi-stage preprocessing pipelines, such as in the case of Retrieval-Augmented Generation (RAG)-based applications. More critically, our findings show that attackers can produce triggers that deviate markedly in form and shape from any known attack, sidestep
    
[^31]: KG-TREAT: 通过在患者数据和知识图谱之间进行协同作用进行治疗效果估计的预训练

    KG-TREAT: Pre-training for Treatment Effect Estimation by Synergizing Patient Data with Knowledge Graphs

    [https://arxiv.org/abs/2403.03791](https://arxiv.org/abs/2403.03791)

    KG-TREAT框架通过协同患者数据和知识图谱，引入双重关注的知识图谱和深度双层注意力协同方法，实现了治疗相关因素和结果相关因素的独立编码，表现优于现有方法。

    

    治疗效果估计（TEE）是确定各种治疗对患者结果影响的任务。KG-TREAT引入了一种新颖的预训练和微调框架，通过将大规模观察性患者数据与生物医学知识图谱（KGs）进行协同以增强TEE，以解决有限标记数据依赖和稀疏和高维观察性患者数据带来的挑战。

    arXiv:2403.03791v1 Announce Type: cross  Abstract: Treatment effect estimation (TEE) is the task of determining the impact of various treatments on patient outcomes. Current TEE methods fall short due to reliance on limited labeled data and challenges posed by sparse and high-dimensional observational patient data. To address the challenges, we introduce a novel pre-training and fine-tuning framework, KG-TREAT, which synergizes large-scale observational patient data with biomedical knowledge graphs (KGs) to enhance TEE. Unlike previous approaches, KG-TREAT constructs dual-focus KGs and integrates a deep bi-level attention synergy method for in-depth information fusion, enabling distinct encoding of treatment-covariate and outcome-covariate relationships. KG-TREAT also incorporates two pre-training tasks to ensure a thorough grounding and contextualization of patient data and KGs. Evaluation on four downstream TEE tasks shows KG-TREAT's superiority over existing methods, with an average
    
[^32]: 一种解决信用违约预测问题的机器学习工作流程

    A machine learning workflow to address credit default prediction

    [https://arxiv.org/abs/2403.03785](https://arxiv.org/abs/2403.03785)

    提出了一种基于工作流程的机器学习方法，用于改进信用违约预测，旨在评估借款人违约信用义务概率，利用多种技术优势解决该任务。

    

    由于近年来对金融科技（FinTech）的兴趣持续增加，像信用违约预测（CDP）这样的应用正受到重视。在这方面，CDP在评估个人和企业的信用价值方面发挥着至关重要的作用，使贷款人能够就贷款批准和风险管理做出明智决策。在本文中，我们提出了一个基于工作流程的方法来改进CDP，这指的是评估借款人违约其信用义务的概率的任务。该工作流程由多个步骤组成，每个步骤旨在利用机器学习管道中不同技术的优势，从而最好地解决CDP任务。我们采用了一种全面系统的方法，从使用WOE（Weight of Evidence）编码进行数据预处理开始，这是一种确保在一次性数据缩放中剔除异常值、处理缺失值的技术。

    arXiv:2403.03785v1 Announce Type: cross  Abstract: Due to the recent increase in interest in Financial Technology (FinTech), applications like credit default prediction (CDP) are gaining significant industrial and academic attention. In this regard, CDP plays a crucial role in assessing the creditworthiness of individuals and businesses, enabling lenders to make informed decisions regarding loan approvals and risk management. In this paper, we propose a workflow-based approach to improve CDP, which refers to the task of assessing the probability that a borrower will default on his or her credit obligations. The workflow consists of multiple steps, each designed to leverage the strengths of different techniques featured in machine learning pipelines and, thus best solve the CDP task. We employ a comprehensive and systematic approach starting with data preprocessing using Weight of Evidence encoding, a technique that ensures in a single-shot data scaling by removing outliers, handling mi
    
[^33]: 使用粒子群和蚁群优化的神经结构搜索

    Neural Architecture Search using Particle Swarm and Ant Colony Optimization

    [https://arxiv.org/abs/2403.03781](https://arxiv.org/abs/2403.03781)

    本研究开发了一种集成用于图像分类的神经结构搜索开源工具系统（OpenNAS），通过采用粒子群和蚁群优化等元启发式方法，自动生成卷积神经网络架构，以显著提高模型准确性。

    

    神经网络模型具有许多超参数，必须选择其结构。这对于初学者来说是一个沉重的负担，他们需要选择哪种结构以及分配给参数的值。在大多数情况下，使用默认的超参数和结构。通过评估多种结构，可以显著提高模型准确性。一种称为神经结构搜索（NAS）的过程可以自动评估大量这样的结构。作为本研究的一部分，已开发了一个集成用于图像分类的神经结构搜索开源工具系统（OpenNAS）。OpenNAS接受任何灰度或RGB图像数据集，并根据一系列元启发式方法使用AutoKeras，迁移学习或Swarm Intelligence (SI)方法生成卷积神经网络（CNN）架构。

    arXiv:2403.03781v1 Announce Type: cross  Abstract: Neural network models have a number of hyperparameters that must be chosen along with their architecture. This can be a heavy burden on a novice user, choosing which architecture and what values to assign to parameters. In most cases, default hyperparameters and architectures are used. Significant improvements to model accuracy can be achieved through the evaluation of multiple architectures. A process known as Neural Architecture Search (NAS) may be applied to automatically evaluate a large number of such architectures. A system integrating open source tools for Neural Architecture Search (OpenNAS), in the classification of images, has been developed as part of this research. OpenNAS takes any dataset of grayscale, or RBG images, and generates Convolutional Neural Network (CNN) architectures based on a range of metaheuristics using either an AutoKeras, a transfer learning or a Swarm Intelligence (SI) approach. Particle Swarm Optimizat
    
[^34]: ENOT：期望回归用于神经优化传输的快速和准确训练

    ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport

    [https://arxiv.org/abs/2403.03777](https://arxiv.org/abs/2403.03777)

    通过期望回归正则化，本论文提出了一种新的神经优化传输（NOT）训练程序扩展，能够有效地估计最优输运方案，并使学习变得稳定。

    

    我们提出了一种新的神经优化传输（NOT）训练程序扩展，通过特定的共轭势正则化能够准确和高效地估计最优输运方案。现有NOT求解器的主要瓶颈在于找到共轭算子（即c-transform）的接近精确近似的过程，这要么通过优化最小-最大目标，要么通过计算密集型的对初始近似预测的精细调整来完成。我们通过提出一种新的、在期望回归形式上强制适应性条件于学习对偶势的理论上合理化损失来解决这两个问题。这样的正则化提供了可能共轭势分布的上限估计，并使学习变得稳定，消除了对额外广泛微调的需求。我们正式证明了我们的方法的效率。

    arXiv:2403.03777v1 Announce Type: cross  Abstract: We present a new extension for Neural Optimal Transport (NOT) training procedure, capable of accurately and efficiently estimating optimal transportation plan via specific regularisation on conjugate potentials. The main bottleneck of existing NOT solvers is associated with the procedure of finding a near-exact approximation of the conjugate operator (i.e., the c-transform), which is done either by optimizing over maximin objectives or by the computationally-intensive fine-tuning of the initial approximated prediction. We resolve both issues by proposing a new, theoretically justified loss in the form of expectile regularization that enforces binding conditions on the learning dual potentials. Such a regularization provides the upper bound estimation over the distribution of possible conjugate potentials and makes the learning stable, eliminating the need for additional extensive finetuning. We formally justify the efficiency of our me
    
[^35]: 针对数据偏移下反事实解释健壮性的验证训练

    Verified Training for Counterfactual Explanation Robustness under Data Shift

    [https://arxiv.org/abs/2403.03773](https://arxiv.org/abs/2403.03773)

    本文引入了VeriTraCER，一种联合训练分类器和解释器的方法，以显式考虑生成的CEs对小型模型偏移的健壮性。

    

    反事实解释（CEs）通过描述对输入的哪些更改是必要的，以将其预测改变为所需类别，提高了机器学习模型的可解释性。这些解释通常用于指导用户的行动，例如，通过描述一个贷款申请被拒绝的用户如何在未来可以获批贷款。现有方法通过专注于单个固定模型生成CEs，并未对CEs的未来有效性提供任何正式保证。当模型定期更新以考虑数据偏移时，如果生成的CEs对偏移不具有健壮性，用户的行动可能不再对其预测产生期望的影响。本文介绍了VeriTraCER，一种联合训练分类器和解释器的方法，明确考虑生成的CEs对小型模型偏移的健壮性。VeriTraCER通过一个精心设计的损失函数进行优化，确保验证

    arXiv:2403.03773v1 Announce Type: new  Abstract: Counterfactual explanations (CEs) enhance the interpretability of machine learning models by describing what changes to an input are necessary to change its prediction to a desired class. These explanations are commonly used to guide users' actions, e.g., by describing how a user whose loan application was denied can be approved for a loan in the future. Existing approaches generate CEs by focusing on a single, fixed model, and do not provide any formal guarantees on the CEs' future validity. When models are updated periodically to account for data shift, if the generated CEs are not robust to the shifts, users' actions may no longer have the desired impacts on their predictions. This paper introduces VeriTraCER, an approach that jointly trains a classifier and an explainer to explicitly consider the robustness of the generated CEs to small model shifts. VeriTraCER optimizes over a carefully designed loss function that ensures the verifi
    
[^36]: 加速LiNGAM: 以GPU速度学习因果DAGs

    AcceleratedLiNGAM: Learning Causal DAGs at the speed of GPUs

    [https://arxiv.org/abs/2403.03772](https://arxiv.org/abs/2403.03772)

    通过有效地并行化现有的因果发现方法，本研究实现了对数千个维度的扩展，特别是将LiNGAM方法并行化，获得了多达32倍的加速。

    

    现有基于组合优化或搜索的因果发现方法速度较慢，限制了它们在大规模数据集上的应用。最近的一些方法尝试通过连续优化的结构学习来解决这一限制，但迄今为止这些方法并未提供统计保证。本文通过有效地并行化现有的因果发现方法，展示我们实际上可以将它们扩展到数千个维度，使其在规模更大的问题上变得实用。具体而言，我们对LiNGAM方法进行了并行化，这种方法与变量数量成二次关系，与现有的顺序实现相比，我们在基准数据集上获得了多达32倍的加速。特别是，我们专注于DirectLiNGAM中的因果排序子过程，并实现了GPU核心以加速它。这使我们能够将DirectLiNGAM应用于因果推断。

    arXiv:2403.03772v1 Announce Type: new  Abstract: Existing causal discovery methods based on combinatorial optimization or search are slow, prohibiting their application on large-scale datasets. In response, more recent methods attempt to address this limitation by formulating causal discovery as structure learning with continuous optimization but such approaches thus far provide no statistical guarantees. In this paper, we show that by efficiently parallelizing existing causal discovery methods, we can in fact scale them to thousands of dimensions, making them practical for substantially larger-scale problems. In particular, we parallelize the LiNGAM method, which is quadratic in the number of variables, obtaining up to a 32-fold speed-up on benchmark datasets when compared with existing sequential implementations. Specifically, we focus on the causal ordering subprocedure in DirectLiNGAM and implement GPU kernels to accelerate it. This allows us to apply DirectLiNGAM to causal inferen
    
[^37]: 基于联合稀疏模式学习的大规模MIMO-OTFS系统信道估计

    Joint Sparsity Pattern Learning Based Channel Estimation for Massive MIMO-OTFS Systems

    [https://arxiv.org/abs/2403.03771](https://arxiv.org/abs/2403.03771)

    提出了一种基于联合稀疏模式学习的信道估计方案，通过利用时延-多普勒-角域信道的潜在联合稀疏性，将信道估计问题转化为稀疏恢复问题，并利用尖峰和板块先验模型以及正交匹配追踪方法，取得了优于现有基线方案的性能提升。

    

    我们提出了一种基于联合稀疏模式学习（JSPL）的信道估计方案，用于大规模多输入多输出（MIMO）正交时频空（OTFS）调制辅助系统。通过利用时延-多普勒-角（DDA）域信道的潜在联合稀疏性，将信道估计问题转化为稀疏恢复问题。为了解决这个问题，我们首先应用尖峰和板块先验模型来迭代地估计信道矩阵的支持集，引入依赖于所识别支持集的高精度参数更新规则进行迭代。然后通过正交匹配追踪（OMP）方法估计支持集对应的信道元素的具体值。我们的仿真结果和分析表明，所提出的JSPL信道估计方案在性能上优于代表性的最新基线方案。

    arXiv:2403.03771v1 Announce Type: cross  Abstract: We propose a channel estimation scheme based on joint sparsity pattern learning (JSPL) for massive multi-input multi-output (MIMO) orthogonal time-frequency-space (OTFS) modulation aided systems. By exploiting the potential joint sparsity of the delay-Doppler-angle (DDA) domain channel, the channel estimation problem is transformed into a sparse recovery problem. To solve it, we first apply the spike and slab prior model to iteratively estimate the support set of the channel matrix, and a higher-accuracy parameter update rule relying on the identified support set is introduced into the iteration. Then the specific values of the channel elements corresponding to the support set are estimated by the orthogonal matching pursuit (OMP) method. Both our simulation results and analysis demonstrate that the proposed JSPL channel estimation scheme achieves an improved performance over the representative state-of-the-art baseline schemes, despit
    
[^38]: DeepCRE：利用尖端计算模型改革药物研发

    DeepCRE: Revolutionizing Drug R&D with Cutting-Edge Computational Models

    [https://arxiv.org/abs/2403.03768](https://arxiv.org/abs/2403.03768)

    DeepCRE是一种新型的计算模型，在患者级别CRE性能上平均提高了17.7％，在指示级别CRE增加了5倍，并成功确定了六个具有显着优势的药物候选者。

    

    arXiv:2403.03768v1 公告类型：新摘要：药物开发领域和治疗应用领域都面临着重大挑战。治疗领域需要更多的治疗选择，同时大量有前景的临床前药物在临床试验中失败。一个原因是在药物开发的后期阶段交叉药物反应评估（CRE）的不足。尽管计算机模拟的CRE模型为解决这一问题提供了一种解决方案，但现有方法学要么局限于早期开发阶段，要么缺乏对全面CRE分析的能力。在这里，我们介绍了一种名为DeepCRE的新型计算模型，并展示了DeepCRE在推动治疗发现和发展方面的潜力。DeepCRE通过实现患者级别CRE平均性能提高17.7\%，指示级别CRE增加了5倍，优于现有最佳模型。此外，DeepCRE已经确定了六个显示出明显更大优势的药物候选者。

    arXiv:2403.03768v1 Announce Type: new  Abstract: The field of pharmaceutical development and therapeutic application both face substantial challenges. Therapeutic domain calls for more treatment alternatives while numerous promising pre-clinical drugs fail in clinical trails. One of the reasons is the inadequacy of Cross-drug Response Evaluation (CRE) during the late stage of drug development. Although in-silico CRE models offer a solution to this problem, existing methodologies are either limited to early development stages or lack the capacity for a comprehensive CRE analysis. Herein, we introduce a novel computational model named DeepCRE and present the potential of DeepCRE in advancing therapeutic discovery and development. DeepCRE outperforms the existing best models by achieving an average performance improvement of 17.7\% in patient-level CRE, and a 5-fold increase in indication-level CRE. Furthermore, DeepCRE has identified six drug candidates that show significantly greater ef
    
[^39]: 使用图神经网络预测表面活性剂临界胶束浓度的温度依赖性

    Predicting the Temperature Dependence of Surfactant CMCs Using Graph Neural Networks

    [https://arxiv.org/abs/2403.03767](https://arxiv.org/abs/2403.03767)

    本研究开发了一个用于预测表面活性剂温度依赖临界胶束浓度的图神经网络模型，填补了现有模型忽视温度依赖性的空白。

    

    表面活性剂分子的临界胶束浓度（CMC）是工业中表面活性剂应用中的基本性质。最近，经典的QSPR和图神经网络（GNN）作为一种深度学习技术已成功应用于预测室温下表面活性剂的CMC。然而，这些模型尚未考虑CMC的温度依赖性，这对于实际应用非常重要。本文开发了一个用于预测表面活性剂温度依赖CMC的GNN模型。我们从公共来源收集了大约1400个不同温度下的各类表面活性剂（即离子性，非离子性和带电离子性）的数据点。我们测试了模型的预测质量，分别考虑以下情况：i）当训练模型时至少有一个不同温度下的表面活性剂CMC数据存在于训练集中，和ii）当模型的训练集中没有表面活性剂的CMC数据，即泛化到未见数据的情况。

    arXiv:2403.03767v1 Announce Type: cross  Abstract: The critical micelle concentration (CMC) of surfactant molecules is an essential property for surfactant applications in industry. Recently, classical QSPR and Graph Neural Networks (GNNs), a deep learning technique, have been successfully applied to predict the CMC of surfactants at room temperature. However, these models have not yet considered the temperature dependency of the CMC, which is highly relevant for practical applications. We herein develop a GNN model for temperature-dependent CMC prediction of surfactants. We collect about 1400 data points from public sources for all surfactant classes, i.e., ionic, nonionic, and zwitterionic, at multiple temperatures. We test the predictive quality of the model for following scenarios: i) when CMC data for surfactants are present in the training of the model in at least one different temperature, and ii) CMC data for surfactants are not present in the training, i.e., generalizing to un
    
[^40]: 参数化量子梳和简化电路用于逆转未知量子比特-酉操作

    Parameterized quantum comb and simpler circuits for reversing unknown qubit-unitary operations

    [https://arxiv.org/abs/2403.03761](https://arxiv.org/abs/2403.03761)

    通过优化参数化量子电路，我们开发了一个简化的协议，用于逆转未知量子比特酉操作，将辅助比特开销减少到3，显示了量子梳结构的实用性和PQComb在解决复杂量子任务中的潜力。

    

    Quantum comb是量子信息处理中表征复杂量子协议的重要工具。在这项工作中，我们引入了PQComb，一个利用参数化量子电路探索量子梳在一般量子过程转换任务及其他方面能力的框架。通过优化PQComb进行未知酉演化的时间反漞模拟，我们开发出了一种更简单的未知量子比特酉反演协议，将比现有方法[Yoshida, Soeda, Murao, PRL 131, 120602, 2023]的辅助比特开销从6减少到3。这展示了量子梳结构的实用性，展示了PQComb在解决复杂量子任务方面的潜力。我们的结果为PQComb在量子计算和量子信息中更广泛的应用铺平了道路，强调了它在解决量子机器学习中的多样问题时的多功能性。

    arXiv:2403.03761v1 Announce Type: cross  Abstract: Quantum comb is an essential tool for characterizing complex quantum protocols in quantum information processing. In this work, we introduce PQComb, a framework leveraging parameterized quantum circuits to explore the capabilities of quantum combs for general quantum process transformation tasks and beyond. By optimizing PQComb for time-reversal simulations of unknown unitary evolutions, we develop a simpler protocol for unknown qubit unitary inversion that reduces the ancilla qubit overhead from 6 to 3 compared to the existing method in [Yoshida, Soeda, Murao, PRL 131, 120602, 2023]. This demonstrates the utility of quantum comb structures and showcases PQComb's potential for solving complex quantum tasks. Our results pave the way for broader PQComb applications in quantum computing and quantum information, emphasizing its versatility for tackling diverse problems in quantum machine learning.
    
[^41]: SUPClust: 边界处的主动学习

    SUPClust: Active Learning at the Boundaries

    [https://arxiv.org/abs/2403.03741](https://arxiv.org/abs/2403.03741)

    提出了一种名为SUPClust的新型主动学习方法，旨在识别类别之间的决策边界上的点，通过标记这些点来优化模型预测的性能。

    

    主动学习是一种机器学习范式，旨在在获取标记数据昂贵的情况下优化模型性能。本文提出了一种名为SUPClust的新型主动学习方法，旨在识别类别之间的决策边界上的点。通过针对这些点，SUPClust旨在收集对于精细化模型对复杂决策区域的预测最具信息量的信息。我们在实验证明，标记这些点会导致强大的模型性能。即使在存在强烈类别不平衡的情况下，也观察到了这种改进。

    arXiv:2403.03741v1 Announce Type: cross  Abstract: Active learning is a machine learning paradigm designed to optimize model performance in a setting where labeled data is expensive to acquire. In this work, we propose a novel active learning method called SUPClust that seeks to identify points at the decision boundary between classes. By targeting these points, SUPClust aims to gather information that is most informative for refining the model's prediction of complex decision regions. We demonstrate experimentally that labeling these points leads to strong model performance. This improvement is observed even in scenarios characterized by strong class imbalance.
    
[^42]: A&B BNN: A&B BNN：仅使用加和位操作的硬件友好的二值神经网络

    A&B BNN: Add&Bit-Operation-Only Hardware-Friendly Binary Neural Network

    [https://arxiv.org/abs/2403.03739](https://arxiv.org/abs/2403.03739)

    A&B BNN 提出了一种只使用加和位操作的硬件友好二值神经网络，通过引入掩码层和量化 RPReLU 结构，能够更高效地进行计算，并在CIFA数据集上取得了良好的实验结果。

    

    二值神经网络利用1位量化的权重和激活来减少模型的存储需求和计算负担。然而，先进的二值架构仍然包含数百万个低效且对硬件不友好的全精度乘法操作。A&B BNN 提出了直接移除传统 BNN 中的部分乘法操作，并用相同数量的位操作替换剩余部分，引入了基于无归一化网络架构的掩码层和量化 RPReLU 结构。掩码层可以通过利用 BNN 的内在特征以及简单的数学变换在推断期间将其移除，以避免相关的乘法操作。量化 RPReLU 结构通过将其斜率限制为2的整数幂，实现更高效的位操作。实验结果在CIFA数据集上达到了92.30%、69.35%和66.89%的准确率。

    arXiv:2403.03739v1 Announce Type: cross  Abstract: Binary neural networks utilize 1-bit quantized weights and activations to reduce both the model's storage demands and computational burden. However, advanced binary architectures still incorporate millions of inefficient and nonhardware-friendly full-precision multiplication operations. A&B BNN is proposed to directly remove part of the multiplication operations in a traditional BNN and replace the rest with an equal number of bit operations, introducing the mask layer and the quantized RPReLU structure based on the normalizer-free network architecture. The mask layer can be removed during inference by leveraging the intrinsic characteristics of BNN with straightforward mathematical transformations to avoid the associated multiplication operations. The quantized RPReLU structure enables more efficient bit operations by constraining its slope to be integer powers of 2. Experimental results achieved 92.30%, 69.35%, and 66.89% on the CIFA
    
[^43]: 使用Transformer表示的概率主题建模

    Probabilistic Topic Modelling with Transformer Representations

    [https://arxiv.org/abs/2403.03737](https://arxiv.org/abs/2403.03737)

    提出了Transformer-Representation神经主题模型（TNTM），结合了transformer嵌入空间中主题表示的优势和概率建模以及变分自动编码器（VAE）框架，实现了主题建模的强大和多功能性

    

    主题建模在过去的十年中大多由贝叶斯图模型主导。然而，随着Transformer在自然语言处理中的兴起，一些依赖于transformer嵌入空间中简单聚类方法的成功模型已经出现并巩固了主题作为嵌入向量聚类的概念。我们提出了Transformer-Representation神经主题模型（TNTM），结合了transformer嵌入空间中主题表示的优势和概率建模。因此，这种方法将基于transformer嵌入的强大多功能主题概念与完全概率建模统一起来，如Latent Dirichlet Allocation（LDA）等模型。我们利用变分自动编码器（VAE）框架改进推理速度和建模灵活性。实验结果显示我们提出的模型与各种模型达到了类似的结果。

    arXiv:2403.03737v1 Announce Type: cross  Abstract: Topic modelling was mostly dominated by Bayesian graphical models during the last decade. With the rise of transformers in Natural Language Processing, however, several successful models that rely on straightforward clustering approaches in transformer-based embedding spaces have emerged and consolidated the notion of topics as clusters of embedding vectors. We propose the Transformer-Representation Neural Topic Model (TNTM), which combines the benefits of topic representations in transformer-based embedding spaces and probabilistic modelling. Therefore, this approach unifies the powerful and versatile notion of topics based on transformer embeddings with fully probabilistic modelling, as in models such as Latent Dirichlet Allocation (LDA). We utilize the variational autoencoder (VAE) framework for improved inference speed and modelling flexibility. Experimental results show that our proposed model achieves results on par with various 
    
[^44]: 通过多阶段Transformer实现统一生成和压缩：超低比特率图像编码

    Unifying Generation and Compression: Ultra-low bitrate Image Coding Via Multi-stage Transformer

    [https://arxiv.org/abs/2403.03736](https://arxiv.org/abs/2403.03736)

    通过引入统一图像生成-压缩（UIGC）范式，本文提出了一种新的多阶段Transformer框架，结合了生成和压缩的过程，以有效利用学习到的先验进行熵估计。

    

    近年来生成式压缩技术的进展显著提高了压缩数据的感知质量。然而，这些进展主要集中在产生高频细节，往往忽视了生成模型捕捉图像内容先验分布的能力，从而阻碍了在极端压缩场景（<0.05 bpp）中进一步降低比特率。在预测性语言模型对无损压缩的能力的启发下，本文引入了一种新颖的统一图像生成-压缩（UIGC）范式，将生成和压缩的过程结合在一起。UIGC框架的一个关键特征是采用向量量化（VQ）图像模型进行标记化，以及一个多阶段Transformer，旨在利用空间上下文信息来建模先验分布。因此，这种双重用途的框架有效地利用了学习到的先验进行熵估计。

    arXiv:2403.03736v1 Announce Type: cross  Abstract: Recent progress in generative compression technology has significantly improved the perceptual quality of compressed data. However, these advancements primarily focus on producing high-frequency details, often overlooking the ability of generative models to capture the prior distribution of image content, thus impeding further bitrate reduction in extreme compression scenarios (<0.05 bpp). Motivated by the capabilities of predictive language models for lossless compression, this paper introduces a novel Unified Image Generation-Compression (UIGC) paradigm, merging the processes of generation and compression. A key feature of the UIGC framework is the adoption of vector-quantized (VQ) image models for tokenization, alongside a multi-stage transformer designed to exploit spatial contextual information for modeling the prior distribution. As such, the dual-purpose framework effectively utilizes the learned prior for entropy estimation and
    
[^45]: 通过预测学习三维物体中心表示

    Learning 3D object-centric representation through prediction

    [https://arxiv.org/abs/2403.03730](https://arxiv.org/abs/2403.03730)

    通过预测未来场景，该研究开发了一种网络架构，同时学习对象分割、3D位置推断和深度感知，从而以类似人类婴儿的约束条件学习物体的表示方式

    

    作为人类核心知识的一部分，对象的表示是支持高层概念和符号推理的心理表示的基本构件。尽管人类能够在3D环境中无需监督地感知对象，但缺乏能够以类似于人类婴儿面临的相似约束条件学习相同能力的模型。为此，我们开发了一种新颖的网络架构，同时学习以下能力：1) 从离散图像中分割对象，2) 推断它们的3D位置，以及3) 感知深度，而仅使用了直接可用于大脑训练数据，即图像序列和自我运动。核心思想是将对象视为视觉输入的潜在原因，大脑利用这些原因做出对未来场景的有效预测。这导致对象表示作为学习预测的基本副产品被学习。

    arXiv:2403.03730v1 Announce Type: cross  Abstract: As part of human core knowledge, the representation of objects is the building block of mental representation that supports high-level concepts and symbolic reasoning. While humans develop the ability of perceiving objects situated in 3D environments without supervision, models that learn the same set of abilities with similar constraints faced by human infants are lacking. Towards this end, we developed a novel network architecture that simultaneously learns to 1) segment objects from discrete images, 2) infer their 3D locations, and 3) perceive depth, all while using only information directly available to the brain as training data, namely: sequences of images and self-motion. The core idea is treating objects as latent causes of visual input which the brain uses to make efficient predictions of future scenes. This results in object representations being learned as an essential byproduct of learning to predict.
    
[^46]: 通过自监督预训练在主动学习中弥合多样性与不确定性

    Bridging Diversity and Uncertainty in Active learning with Self-Supervised Pre-Training

    [https://arxiv.org/abs/2403.03728](https://arxiv.org/abs/2403.03728)

    通过引入TCM启发式方法，本研究在主动学习中成功结合了多样性采样和不确定性采样策略，解决了冷启动问题并在各种数据水平上表现出色。

    

    本研究探讨了在主动学习中集成基于多样性和基于不确定性的采样策略，特别是在自监督预训练模型的背景下。我们引入了一个称为TCM的简单启发式方法，可以缓解冷启动问题，同时在各种数据水平上保持强大性能。通过首先应用TypiClust进行多样性采样，随后过渡到使用Margin进行不确定性采样，我们的方法有效地结合了两种策略的优势。我们的实验表明，TCM在低数据和高数据情况下始终优于现有方法。

    arXiv:2403.03728v1 Announce Type: cross  Abstract: This study addresses the integration of diversity-based and uncertainty-based sampling strategies in active learning, particularly within the context of self-supervised pre-trained models. We introduce a straightforward heuristic called TCM that mitigates the cold start problem while maintaining strong performance across various data levels. By initially applying TypiClust for diversity sampling and subsequently transitioning to uncertainty sampling with Margin, our approach effectively combines the strengths of both strategies. Our experiments demonstrate that TCM consistently outperforms existing methods across various datasets in both low and high data regimes.
    
[^47]: 蛋白质序列生成的语言模型嵌入扩散

    Diffusion on language model embeddings for protein sequence generation

    [https://arxiv.org/abs/2403.03726](https://arxiv.org/abs/2403.03726)

    使用DiMA模型，在蛋白语言模型嵌入进行扩散来生成氨基酸序列，比传统解决方案表现更好，并通过设计选择的影响来量化其优越性能。

    

    蛋白设计需要对蛋白质宇宙固有复杂性的深入了解。尽管许多工作倾向于有条件的生成或专注于特定蛋白质家族，但无条件生成的基础任务仍未得到充分探索和重视。在这里，我们探索这个关键领域，引入了DiMA，这是一个利用从蛋白语言模型ESM-2衍生的嵌入进行连续扩散以生成氨基酸序列的模型。DiMA超越了包括自回归变换器和离散扩散模型在内的主要解决方案，我们定量地说明了导致其卓越性能的设计选择所带来的影响。我们使用各种指标跨多种形式广泛评估生成序列的质量、多样性、分布相似性和生物相关性。我们的方法始终产生新颖、多样化的蛋白质序列，精准

    arXiv:2403.03726v1 Announce Type: cross  Abstract: Protein design requires a deep understanding of the inherent complexities of the protein universe. While many efforts lean towards conditional generation or focus on specific families of proteins, the foundational task of unconditional generation remains underexplored and undervalued. Here, we explore this pivotal domain, introducing DiMA, a model that leverages continuous diffusion on embeddings derived from the protein language model, ESM-2, to generate amino acid sequences. DiMA surpasses leading solutions, including autoregressive transformer-based and discrete diffusion models, and we quantitatively illustrate the impact of the design choices that lead to its superior performance. We extensively evaluate the quality, diversity, distribution similarity, and biological relevance of the generated sequences using multiple metrics across various modalities. Our approach consistently produces novel, diverse protein sequences that accura
    
[^48]: 在线模型误差校正与神经网络: 应用于集成预测系统

    Online model error correction with neural networks: application to the Integrated Forecasting System

    [https://arxiv.org/abs/2403.03702](https://arxiv.org/abs/2403.03702)

    使用神经网络为欧洲中程气象中心的集成预测系统开发模型误差校正，以解决机器学习天气预报模型在表示动力平衡和适用于数据同化实验方面的挑战。

    

    最近几年，在全球数值天气预报模型的完全数据驱动开发方面取得了显著进展。这些机器学习天气预报模型具有其优势，尤其是准确性和较低的计算需求，但也存在其弱点：它们难以表示基本动力平衡，并且远未适用于资料同化实验。混合建模出现为解决这些限制的一种有希望的方法。混合模型将基于物理的核心组件与统计组件（通常是神经网络）集成在一起，以增强预测能力。在本文中，我们提出使用神经网络为欧洲中程气象中心的运行集成预测系统（IFS）开发模型误差校正。神经网络最初会离线进行预训练，使用大量运行分析数据集

    arXiv:2403.03702v1 Announce Type: cross  Abstract: In recent years, there has been significant progress in the development of fully data-driven global numerical weather prediction models. These machine learning weather prediction models have their strength, notably accuracy and low computational requirements, but also their weakness: they struggle to represent fundamental dynamical balances, and they are far from being suitable for data assimilation experiments. Hybrid modelling emerges as a promising approach to address these limitations. Hybrid models integrate a physics-based core component with a statistical component, typically a neural network, to enhance prediction capabilities. In this article, we propose to develop a model error correction for the operational Integrated Forecasting System (IFS) of the European Centre for Medium-Range Weather Forecasts using a neural network. The neural network is initially pre-trained offline using a large dataset of operational analyses and a
    
[^49]: 分布式基础设施上的模型并行性：从理论到LLM案例研究的文献综述

    Model Parallelism on Distributed Infrastructure: A Literature Review from Theory to LLM Case-Studies

    [https://arxiv.org/abs/2403.03699](https://arxiv.org/abs/2403.03699)

    这项研究从理论到LLM案例研究综述了分布式基础设施上的模型并行性，探讨了模型并行性类型、挑战和现代用例。神经网络可以通过操作内部和操作之间并行化，但实施挑战包括操作图的最佳划分。

    

    神经网络已经成为机器学习的基石。随着这些网络变得越来越复杂的趋势持续进行，用于训练和部署的基础硬件和软件基础设施也在不断发展。在这项调查中，我们回答了三个研究问题：“存在哪些模型并行性类型？”，“模型并行性的挑战是什么？”，以及“模型并行性的现代用例是什么？” 我们通过研究神经网络的并行化方式以及将其表达为操作图来回答第一个问题，同时探索可用的维度。神经网络可以并行化的维度包括操作内部并行和操作之间并行。我们通过收集和列出并行化类型的实施挑战以及操作图的最佳划分问题来回答第二个问题。最后，我们通过收集和列出并行化如何应用在m

    arXiv:2403.03699v1 Announce Type: cross  Abstract: Neural networks have become a cornerstone of machine learning. As the trend for these to get more and more complex continues, so does the underlying hardware and software infrastructure for training and deployment. In this survey we answer three research questions: "What types of model parallelism exist?", "What are the challenges of model parallelism?", and "What is a modern use-case of model parallelism?" We answer the first question by looking at how neural networks can be parallelised and expressing these as operator graphs while exploring the available dimensions. The dimensions along which neural networks can be parallelised are intra-operator and inter-operator. We answer the second question by collecting and listing both implementation challenges for the types of parallelism, as well as the problem of optimally partitioning the operator graph. We answer the last question by collecting and listing how parallelism is applied in m
    
[^50]: 朝向可控时间序列生成

    Towards Controllable Time Series Generation

    [https://arxiv.org/abs/2403.03698](https://arxiv.org/abs/2403.03698)

    提出了 Controllable Time Series (CTS) 框架，通过解耦映射过程来实现对复杂交互模式的精确学习，从而创新了针对可控时间序列生成 (CTSG) 的方法。

    

    时间序列生成（TSG）已经成为合成准确反映现实世界时间序列数据的关键技术，在许多应用中变得不可或缺。尽管TSG取得了显著进展，但其有效性经常取决于具有大型训练数据集。这种依赖性在数据稀缺的情况下，特别是在处理罕见或独特条件时，构成了一个重大挑战。为了应对这些挑战，我们探索了一个新问题，即可控时间序列生成（CTSG），旨在产生能够适应各种外部条件的合成时间序列，从而解决数据稀缺问题。

    arXiv:2403.03698v1 Announce Type: cross  Abstract: Time Series Generation (TSG) has emerged as a pivotal technique in synthesizing data that accurately mirrors real-world time series, becoming indispensable in numerous applications. Despite significant advancements in TSG, its efficacy frequently hinges on having large training datasets. This dependency presents a substantial challenge in data-scarce scenarios, especially when dealing with rare or unique conditions. To confront these challenges, we explore a new problem of Controllable Time Series Generation (CTSG), aiming to produce synthetic time series that can adapt to various external conditions, thereby tackling the data scarcity issue.   In this paper, we propose \textbf{C}ontrollable \textbf{T}ime \textbf{S}eries (\textsf{CTS}), an innovative VAE-agnostic framework tailored for CTSG. A key feature of \textsf{CTS} is that it decouples the mapping process from standard VAE training, enabling precise learning of a complex interpla
    
[^51]: 具有块结构尖峰模型中的光谱相位转换和最优PCA

    Spectral Phase Transition and Optimal PCA in Block-Structured Spiked models

    [https://arxiv.org/abs/2403.03695](https://arxiv.org/abs/2403.03695)

    该论文提出了一种针对不均匀尖峰维格纳模型的最优光谱方法，通过对转化矩阵的深入严格分析，在最佳阈值处实现了异常值和正重叠的相位转换。

    

    我们讨论了不均匀尖峰维格纳模型，这是最近引入的一个理论框架，用于研究各种学习场景中的结构噪声，通过随机矩阵理论的棱镜，特别关注其光谱特性。我们的主要目标是找到一种最优的光谱方法，并将在不均匀，块结构的维格纳模型中有名的\cite{BBP}（BBP）相位转换准则扩展到我们的情况。我们对一个转换矩阵进行了彻底的严格分析，并展示了出现1）在极限光谱分布的群外的异常值和2）相关特征向量与信号之间的正重叠的转变正好发生在最佳阈值处，使得所提出的光谱方法在不均匀维格纳问题的迭代方法类中是最优的。

    arXiv:2403.03695v1 Announce Type: cross  Abstract: We discuss the inhomogeneous spiked Wigner model, a theoretical framework recently introduced to study structured noise in various learning scenarios, through the prism of random matrix theory, with a specific focus on its spectral properties. Our primary objective is to find an optimal spectral method and to extend the celebrated \cite{BBP} (BBP) phase transition criterion -- well-known in the homogeneous case -- to our inhomogeneous, block-structured, Wigner model. We provide a thorough rigorous analysis of a transformed matrix and show that the transition for the appearance of 1) an outlier outside the bulk of the limiting spectral distribution and 2) a positive overlap between the associated eigenvector and the signal, occurs precisely at the optimal threshold, making the proposed spectral method optimal within the class of iterative methods for the inhomogeneous Wigner problem.
    
[^52]: 具有鲁棒性的简化PCNet

    Simplified PCNet with Robustness

    [https://arxiv.org/abs/2403.03676](https://arxiv.org/abs/2403.03676)

    本文简化了PCNet并增强了其鲁棒性，通过扩展滤波器阶数并减少参数，以及实现适应性邻域大小的变体，提高了模型对图结构扰动或对抗性攻击的鲁棒性。

    

    图神经网络（GNNs）因在学习同构或异构图的表示方面取得成功而备受关注。然而，它们在泛化到具有不同同态程度的现实世界图时表现不佳。为此，前作中提出的Possion-Charlier Network（PCNet）允许从异态到同态学习图表示。尽管PCNet缓解了异态问题，但在进一步提高有效性和效率方面仍存在一些挑战。本文简化了PCNet并增强了其鲁棒性。我们首先将滤波器阶数扩展到连续值并减少其参数。我们实现了两种具有自适应邻域大小的变体。理论分析显示我们的模型对于图结构扰动或对抗性攻击具有鲁棒性。通过在代表同构和异构图的各种数据集上进行半监督学习任务，我们验证了我们的方法。

    arXiv:2403.03676v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have garnered significant attention for their success in learning the representation of homophilic or heterophilic graphs. However, they cannot generalize well to real-world graphs with different levels of homophily. In response, the Possion-Charlier Network (PCNet) \cite{li2024pc}, the previous work, allows graph representation to be learned from heterophily to homophily. Although PCNet alleviates the heterophily issue, there remain some challenges in further improving the efficacy and efficiency. In this paper, we simplify PCNet and enhance its robustness. We first extend the filter order to continuous values and reduce its parameters. Two variants with adaptive neighborhood sizes are implemented. Theoretical analysis shows our model's robustness to graph structure perturbations or adversarial attacks. We validate our approach through semi-supervised learning tasks on various datasets representing both homo
    
[^53]: 在具有随机硬约束的对抗MDP中学习

    Learning Adversarial MDPs with Stochastic Hard Constraints

    [https://arxiv.org/abs/2403.03672](https://arxiv.org/abs/2403.03672)

    本论文首次研究了涉及对抗损失和硬约束的CMDP，在两种不同情形下设计了具有次线性遗憾的算法，填补了先前研究中对这一问题的空白。

    

    我们研究带有对抗损失和随机硬约束的受限马尔可夫决策过程（CMDP）中的在线学习问题。我们考虑两种不同的情形。在第一种情形中，我们解决了一般CMDP问题，设计了一个算法，实现了次线性遗憾和累积正约束违反。在第二种情形中，在一个政策严格满足约束存在且为学习者所了解的温和假设下，我们设计了一个算法，实现了次线性遗憾，同时确保在每一轮中约束以高概率得到满足。据我们所知，我们的工作是第一个研究既涉及对抗损失又涉及硬约束的CMDP的工作。实际上，先前的研究要么集中在更弱的软约束上--允许正违反来抵消负违反--要么局限于随机损失。因此，我们的算法可以处理一般的非统计

    arXiv:2403.03672v1 Announce Type: new  Abstract: We study online learning problems in constrained Markov decision processes (CMDPs) with adversarial losses and stochastic hard constraints. We consider two different scenarios. In the first one, we address general CMDPs, where we design an algorithm that attains sublinear regret and cumulative positive constraints violation. In the second scenario, under the mild assumption that a policy strictly satisfying the constraints exists and is known to the learner, we design an algorithm that achieves sublinear regret while ensuring that the constraints are satisfied at every episode with high probability. To the best of our knowledge, our work is the first to study CMDPs involving both adversarial losses and hard constraints. Indeed, previous works either focus on much weaker soft constraints--allowing for positive violation to cancel out negative ones--or are restricted to stochastic losses. Thus, our algorithms can deal with general non-stat
    
[^54]: CDC：复杂数据聚类的简单框架

    CDC: A Simple Framework for Complex Data Clustering

    [https://arxiv.org/abs/2403.03670](https://arxiv.org/abs/2403.03670)

    提出了一个简单但有效的复杂数据聚类框架（CDC），能够以线性复杂度高效处理不同类型的数据，并通过图滤波器和高质量锚点来融合几何结构和属性信息，具有很高的集群能力。

    

    在当今数据驱动的数字时代，收集到的数据量以及复杂度（如多视图、非欧几里得和多关联性）正在呈指数甚至更快地增长。聚类无监督地从数据中提取有效知识，在实践中非常有用。然而，现有方法独立开发，处理一个特定挑战，牺牲其他挑战。在这项工作中，我们提出了一个简单但有效的复杂数据聚类（CDC）框架，可以以线性复杂度高效处理不同类型的数据。我们首先利用图滤波器融合几何结构和属性信息。然后通过一种新颖的保存相似性的正则化器自适应学习高质量锚点来降低复杂度。我们从理论和实验上说明了我们提出的方法的集群能力。特别是，我们将CDC部署到规模为111M的图数据中。

    arXiv:2403.03670v1 Announce Type: new  Abstract: In today's data-driven digital era, the amount as well as complexity, such as multi-view, non-Euclidean, and multi-relational, of the collected data are growing exponentially or even faster. Clustering, which unsupervisely extracts valid knowledge from data, is extremely useful in practice. However, existing methods are independently developed to handle one particular challenge at the expense of the others. In this work, we propose a simple but effective framework for complex data clustering (CDC) that can efficiently process different types of data with linear complexity. We first utilize graph filtering to fuse geometry structure and attribute information. We then reduce the complexity with high-quality anchors that are adaptively learned via a novel similarity-preserving regularizer. We illustrate the cluster-ability of our proposed method theoretically and experimentally. In particular, we deploy CDC to graph data of size 111M.
    
[^55]: 通过扩散在流形上的谱算法

    Spectral Algorithms on Manifolds through Diffusion

    [https://arxiv.org/abs/2403.03669](https://arxiv.org/abs/2403.03669)

    本文提出了一种新的视角，将输入数据视为嵌入到更高维欧几里得空间中的低维流形，并研究了在RKHS中谱算法的收敛性能，特别是热核生成的扩散空间，通过积分算子技术推导了关于广义范数的紧收敛上界，使估计器在强意义下收敛到目标函数。

    

    在重现核希尔伯特空间（RKHS）中应用的谱算法的现有研究主要集中在一般核函数上，经常忽略输入特征空间的固有结构。我们的论文引入了一个新的视角，主张输入数据位于一个嵌入到更高维欧几里得空间中的低维流形内。我们研究了RKHS中谱算法的收敛性能，特别是那些由热核生成的，被称为扩散空间的空间。通过结合输入的流形结构，我们采用积分算子技术推导了关于广义范数的紧收敛上界，这表明估计器在强意义下收敛到目标函数，意味着函数本身及其导数同时收敛。这些界提供了两个重要优势：首先，它们是完全连续的。

    arXiv:2403.03669v1 Announce Type: cross  Abstract: The existing research on spectral algorithms, applied within a Reproducing Kernel Hilbert Space (RKHS), has primarily focused on general kernel functions, often neglecting the inherent structure of the input feature space. Our paper introduces a new perspective, asserting that input data are situated within a low-dimensional manifold embedded in a higher-dimensional Euclidean space. We study the convergence performance of spectral algorithms in the RKHSs, specifically those generated by the heat kernels, known as diffusion spaces. Incorporating the manifold structure of the input, we employ integral operator techniques to derive tight convergence upper bounds concerning generalized norms, which indicates that the estimators converge to the target function in strong sense, entailing the simultaneous convergence of the function itself and its derivatives. These bounds offer two significant advantages: firstly, they are exclusively contin
    
[^56]: 可证滤波器用于现实世界图聚类

    Provable Filter for Real-world Graph Clustering

    [https://arxiv.org/abs/2403.03666](https://arxiv.org/abs/2403.03666)

    提出了一种可证实的方案，针对实际世界图聚类问题，在处理同源和异源图时表现出色，并构建了低通和高通滤波器来捕捉全面信息。

    

    图聚类是一个重要的无监督问题，已经被证明对图神经网络（GNNs）的进展更具抵抗力。此外，几乎所有的聚类方法都专注于同源图，忽略异源性。这严重限制了它们在实践中的适用性，因为现实世界图展现出结构不一致，不能简单地被归类为同源性和异源性。因此，迫切需要一种处理实际图的原则性方法。为了填补这一空白，我们提供了一个具有理论支持的新颖解决方案。有趣的是，我们发现大多数同源和异源边可以基于邻居信息被正确识别。受到这一发现的启发，我们构建了两个分别高度同源和异源的图。它们用于构建低通和高通滤波器以捕捉整体信息。重要的特征进一步由挤压-激励块增强。

    arXiv:2403.03666v1 Announce Type: new  Abstract: Graph clustering, an important unsupervised problem, has been shown to be more resistant to advances in Graph Neural Networks (GNNs). In addition, almost all clustering methods focus on homophilic graphs and ignore heterophily. This significantly limits their applicability in practice, since real-world graphs exhibit a structural disparity and cannot simply be classified as homophily and heterophily. Thus, a principled way to handle practical graphs is urgently needed. To fill this gap, we provide a novel solution with theoretical support. Interestingly, we find that most homophilic and heterophilic edges can be correctly identified on the basis of neighbor information. Motivated by this finding, we construct two graphs that are highly homophilic and heterophilic, respectively. They are used to build low-pass and high-pass filters to capture holistic information. Important features are further enhanced by the squeeze-and-excitation block
    
[^57]: 环境洞见: 用开源Python软件包实现大众获取环境空气污染数据和预测分析

    Environmental Insights: Democratizing Access to Ambient Air Pollution Data and Predictive Analytics with an Open-Source Python Package

    [https://arxiv.org/abs/2403.03664](https://arxiv.org/abs/2403.03664)

    这个开源Python软件包“Environmental Insights”旨在使大众能够轻松获取环境空气污染数据，进行预测分析，以及通过动态可视化提高用户参与度。

    

    环境空气污染是一个普遍存在的问题，对人类健康、生态系统生命力和经济结构产生广泛影响。利用环境空气污染浓度数据，研究人员可以进行全面分析，揭示空气污染对社会各方面的多方面影响。为此，我们推出了“Environmental Insights”，这是一个旨在使人们能够轻松获取空气污染浓度数据的开源Python软件包。该工具使用户可以轻松检索历史空气污染数据，并使用机器学习模型预测未来潜在条件。此外，“Environmental Insights”还包括一套旨在促进分析结果传播和通过动态可视化增强用户参与的工具组。这一全面的方法确保该软件包满足了希望探索和了解空气污染趋势及其对环境的影响的个人的多样需求。

    arXiv:2403.03664v1 Announce Type: cross  Abstract: Ambient air pollution is a pervasive issue with wide-ranging effects on human health, ecosystem vitality, and economic structures. Utilizing data on ambient air pollution concentrations, researchers can perform comprehensive analyses to uncover the multifaceted impacts of air pollution across society. To this end, we introduce Environmental Insights, an open-source Python package designed to democratize access to air pollution concentration data. This tool enables users to easily retrieve historical air pollution data and employ a Machine Learning model for forecasting potential future conditions. Moreover, Environmental Insights includes a suite of tools aimed at facilitating the dissemination of analytical findings and enhancing user engagement through dynamic visualizations. This comprehensive approach ensures that the package caters to the diverse needs of individuals looking to explore and understand air pollution trends and their
    
[^58]: 在异质环境下的稳健图结构学习

    Robust Graph Structure Learning under Heterophily

    [https://arxiv.org/abs/2403.03659](https://arxiv.org/abs/2403.03659)

    提出了一种稳健图结构学习方法，能够从异质数据中获得高质量图，以用于下游任务

    

    图是一种基本的数学结构，用于描述不同对象之间的关系，在各种学习任务中被广泛应用。大多数方法隐式地假设给定的图是准确和完整的。然而，真实数据不可避免地是嘈杂和稀疏的，这将导致较差的结果。尽管最近的图表示学习方法取得了显著的成功，但它们本质上假设图是同质的，并且很大程度上忽略了异质性，即大多数连接节点来自不同的类别。在这方面，我们提出了一种新颖的稳健图结构学习方法，从异质数据中获得高质量的图以用于下游任务。我们首先应用一个高通滤波器，通过将结构信息编码到节点特征中，使每个节点与其邻居更具有区别性。然后，我们学习一个具有自适应范数的稳健图，用于描述不同噪声水平。随后，我们提出

    arXiv:2403.03659v1 Announce Type: new  Abstract: Graph is a fundamental mathematical structure in characterizing relations between different objects and has been widely used on various learning tasks. Most methods implicitly assume a given graph to be accurate and complete. However, real data is inevitably noisy and sparse, which will lead to inferior results. Despite the remarkable success of recent graph representation learning methods, they inherently presume that the graph is homophilic, and largely overlook heterophily, where most connected nodes are from different classes. In this regard, we propose a novel robust graph structure learning method to achieve a high-quality graph from heterophilic data for downstream tasks. We first apply a high-pass filter to make each node more distinctive from its neighbors by encoding structure information into the node features. Then, we learn a robust graph with an adaptive norm characterizing different levels of noise. Afterwards, we propose 
    
[^59]: 强化学习在空间资源分配中的应用调查

    A Survey on Applications of Reinforcement Learning in Spatial Resource Allocation

    [https://arxiv.org/abs/2403.03643](https://arxiv.org/abs/2403.03643)

    运用强化学习解决空间资源分配问题的新方法具有快速解决方法收敛和强大的模型泛化能力等优势，为这一问题领域提供了新的视角。

    

    空间资源分配的挑战在交通运输、工业和日常生活等各个领域普遍存在。随着现实世界问题规模不断扩大以及对实时解决方案的需求增加，传统算法面临着巨大的计算压力，难以实现最佳效率和实时能力。近年来，随着计算机计算能力的不断提升，强化学习在诸如围棋和机器人领域取得了显著成就，展示了其强大的学习和序贯决策能力。鉴于这些进展，近年来出现了大量运用强化学习解决空间资源分配问题的新方法。这些方法具有快速解决方法收敛和强大的模型泛化能力等优势，为解决空间资源分配问题提供了新的视角。

    arXiv:2403.03643v1 Announce Type: cross  Abstract: The challenge of spatial resource allocation is pervasive across various domains such as transportation, industry, and daily life. As the scale of real-world issues continues to expand and demands for real-time solutions increase, traditional algorithms face significant computational pressures, struggling to achieve optimal efficiency and real-time capabilities. In recent years, with the escalating computational power of computers, the remarkable achievements of reinforcement learning in domains like Go and robotics have demonstrated its robust learning and sequential decision-making capabilities. Given these advancements, there has been a surge in novel methods employing reinforcement learning to tackle spatial resource allocation problems. These methods exhibit advantages such as rapid solution convergence and strong model generalization abilities, offering a new perspective on resolving spatial resource allocation problems. Therefor
    
[^60]: 用变分自动编码器生成宠物医学放射数据的生成式主动学习

    Generative Active Learning with Variational Autoencoder for Radiology Data Generation in Veterinary Medicine

    [https://arxiv.org/abs/2403.03642](https://arxiv.org/abs/2403.03642)

    提出了一种基于变分自动编码器的生成式主动学习框架，用于缓解兽医学CAD系统可靠数据不足的问题，并在心脏增大X光数据集上取得了显著效果。

    

    最近，随着人们对宠物健康的兴趣增加，兽医学中计算辅助诊断（CAD）系统的需求也在增加。在兽医学CAD的发展中，由于放射数据不足而停滞不前。为了克服这一挑战，我们提出了一个基于变分自动编码器的生成式主动学习框架。这种方法旨在缓解兽医学CAD系统可靠数据的稀缺性。本研究利用包含心脏增大X光数据的数据集。在去除注释并标准化图像后，我们使用了一个数据增强框架，其中包括数据生成阶段和用于过滤生成数据的查询阶段。实验结果表明，随着通过该框架生成的数据被添加到生成模型的训练数据中，frechet入侵距离在X光上持续从84.14下降到50.75。

    arXiv:2403.03642v1 Announce Type: cross  Abstract: Recently, with increasing interest in pet healthcare, the demand for computer-aided diagnosis (CAD) systems in veterinary medicine has increased. The development of veterinary CAD has stagnated due to a lack of sufficient radiology data. To overcome the challenge, we propose a generative active learning framework based on a variational autoencoder. This approach aims to alleviate the scarcity of reliable data for CAD systems in veterinary medicine. This study utilizes datasets comprising cardiomegaly radiograph data. After removing annotations and standardizing images, we employed a framework for data augmentation, which consists of a data generation phase and a query phase for filtering the generated data. The experimental results revealed that as the data generated through this framework was added to the training data of the generative model, the frechet inception distance consistently decreased from 84.14 to 50.75 on the radiograph.
    
[^61]: SheetAgent：通过大型语言模型进行电子表格推理和操作的通用代理

    SheetAgent: A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models

    [https://arxiv.org/abs/2403.03636](https://arxiv.org/abs/2403.03636)

    SheetAgent是一种利用大型语言模型进行电子表格推理和操作的通用代理，提供了处理复杂现实任务的解决方案

    

    电子表格操作广泛存在于大多数日常工作中，并显著提高了工作效率。最近尝试使用大型语言模型(LLM)进行自动电子表格操作，但尚未在存在推理挑战的复杂和现实任务中进行探究（例如，具有多步推理和模糊要求的长视野操作）。为了弥合与真实世界要求之间的差距，我们引入了$\textbf{SheetRM}$，一个特点是长视野和多类任务的基准，具有推理相关操纵，由真实挑战引起。为了缓解以上挑战，我们进一步提出了$\textbf{SheetAgent}$，一种利用LLMs能力的新型自主代理。SheetAgent由三个协作模块组成：$\textit{Planner}$、$\textit{Informer}$和$\textit{Retriever}$，实现了对电子表格的高级推理和准确操作，而不需人类

    arXiv:2403.03636v1 Announce Type: new  Abstract: Spreadsheet manipulation is widely existing in most daily works and significantly improves working efficiency. Large language model (LLM) has been recently attempted for automatic spreadsheet manipulation but has not yet been investigated in complicated and realistic tasks where reasoning challenges exist (e.g., long horizon manipulation with multi-step reasoning and ambiguous requirements). To bridge the gap with the real-world requirements, we introduce $\textbf{SheetRM}$, a benchmark featuring long-horizon and multi-category tasks with reasoning-dependent manipulation caused by real-life challenges. To mitigate the above challenges, we further propose $\textbf{SheetAgent}$, a novel autonomous agent that utilizes the power of LLMs. SheetAgent consists of three collaborative modules: $\textit{Planner}$, $\textit{Informer}$, and $\textit{Retriever}$, achieving both advanced reasoning and accurate manipulation over spreadsheets without hu
    
[^62]: 处理概率风力发电预测中的缺失值：一种生成方法

    Tackling Missing Values in Probabilistic Wind Power Forecasting: A Generative Approach

    [https://arxiv.org/abs/2403.03631](https://arxiv.org/abs/2403.03631)

    本文提出了一种新的概率风力发电预测方法，通过生成模型估计特征和目标的联合分布，同时预测所有未知值，避免了预处理环节，在连续排名概率得分方面比传统方法表现更优。

    

    机器学习技术已成功应用于概率风力发电预测。然而，由于传感器故障等原因导致数据集中存在缺失值的问题长期以来被忽视。尽管通常在模型估计和预测之前通过插补缺失值来解决这个问题是很自然的，但我们建议将缺失值和预测目标视为同等重要，并基于观测值同时预测所有未知值。本文通过基于生成模型估计特征和目标的联合分布，提出了一种有效的概率预测方法。这种方法无需预处理，避免引入潜在的错误。与传统的“插补，然后预测”流程相比，该方法在连续排名概率得分方面表现更好。

    arXiv:2403.03631v1 Announce Type: new  Abstract: Machine learning techniques have been successfully used in probabilistic wind power forecasting. However, the issue of missing values within datasets due to sensor failure, for instance, has been overlooked for a long time. Although it is natural to consider addressing this issue by imputing missing values before model estimation and forecasting, we suggest treating missing values and forecasting targets indifferently and predicting all unknown values simultaneously based on observations. In this paper, we offer an efficient probabilistic forecasting approach by estimating the joint distribution of features and targets based on a generative model. It is free of preprocessing, and thus avoids introducing potential errors. Compared with the traditional "impute, then predict" pipeline, the proposed approach achieves better performance in terms of continuous ranked probability score.
    
[^63]: GSNeRF: 增强了3D场景理解的通用语义神经辐射场

    GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D Scene Understanding

    [https://arxiv.org/abs/2403.03608](https://arxiv.org/abs/2403.03608)

    GSNeRF通过引入图像语义，实现了对未知场景的新视图图像和相关语义地图的生成，并在图像和语义渲染方面取得了改进性能。

    

    在这项工作中，我们引入了一种通用的语义神经辐射场（GSNeRF），它独特地将图像语义纳入合成过程，因此可以为未知场景生成新视图图像和相关的语义地图。我们的GSNeRF由两个阶段组成：语义地理推理和深度引导可视渲染。前者能够观察多视图图像输入，从场景中提取语义和几何特征。在后者的指导下，根据生成的图像几何信息，进行了具有改进性能的图像和语义渲染。我们的实验证实了GSNeRF在新视图图像和语义分割合成方面优于先前工作，并且证明了我们的采样策略对于可视渲染的有效性。

    arXiv:2403.03608v1 Announce Type: cross  Abstract: Utilizing multi-view inputs to synthesize novel-view images, Neural Radiance Fields (NeRF) have emerged as a popular research topic in 3D vision. In this work, we introduce a Generalizable Semantic Neural Radiance Field (GSNeRF), which uniquely takes image semantics into the synthesis process so that both novel view images and the associated semantic maps can be produced for unseen scenes. Our GSNeRF is composed of two stages: Semantic Geo-Reasoning and Depth-Guided Visual rendering. The former is able to observe multi-view image inputs to extract semantic and geometry features from a scene. Guided by the resulting image geometry information, the latter performs both image and semantic rendering with improved performances. Our experiments not only confirm that GSNeRF performs favorably against prior works on both novel-view image and semantic segmentation synthesis but the effectiveness of our sampling strategy for visual rendering is 
    
[^64]: 通过Transformer神经网络和技术指标提升加密货币价格预测

    Enhancing Price Prediction in Cryptocurrency Using Transformer Neural Network and Technical Indicators

    [https://arxiv.org/abs/2403.03606](https://arxiv.org/abs/2403.03606)

    该研究提出了通过引入Transformer神经网络和技术指标来提升加密货币价格预测的方法，该方法对于捕获时间动态和提取重要特征具有显著的优势

    

    本研究提出了一种创新的方法，用于预测加密货币时间序列，特别关注比特币、以太坊和莱特币。该方法整合了技术指标、Performer神经网络和BiLSTM（双向长短期记忆网络）来捕获时间动态并从原始加密货币数据中提取重要特征。技术指标的应用有助于提取复杂模式、动量、波动性和趋势。Performer神经网络使用快速关注正交随机特征（FAVOR+）相对于传统的Transformer模型中的多头注意力机制，展现出更优越的计算效率和可伸缩性。此外，将BiLSTM集成到前馈网络中增强了模型捕获数据时间动态的能力，向前和向后两个方向进行处理。

    arXiv:2403.03606v1 Announce Type: cross  Abstract: This study presents an innovative approach for predicting cryptocurrency time series, specifically focusing on Bitcoin, Ethereum, and Litecoin. The methodology integrates the use of technical indicators, a Performer neural network, and BiLSTM (Bidirectional Long Short-Term Memory) to capture temporal dynamics and extract significant features from raw cryptocurrency data. The application of technical indicators, such facilitates the extraction of intricate patterns, momentum, volatility, and trends. The Performer neural network, employing Fast Attention Via positive Orthogonal Random features (FAVOR+), has demonstrated superior computational efficiency and scalability compared to the traditional Multi-head attention mechanism in Transformer models. Additionally, the integration of BiLSTM in the feedforward network enhances the model's capacity to capture temporal dynamics in the data, processing it in both forward and backward direction
    
[^65]: 通过集群泛化学习图神经网络的不变表示

    Learning Invariant Representations of Graph Neural Networks via Cluster Generalization

    [https://arxiv.org/abs/2403.03599](https://arxiv.org/abs/2403.03599)

    该论文提出了一种Cluster Information Transfer (CIT) 机制，可以为图神经网络学习不变表示，从而提高对具有结构转移的各种未知测试图的泛化能力。

    

    图神经网络(GNNs)在建模图结构数据中变得越来越流行，因为它们能够通过聚合局部结构信息来学习节点表示。然而，人们普遍认为测试图的结构可能不同于训练图的结构，导致了结构转移。在本文中，我们实验性地发现，当结构转移发生时，GNNs的性能显著下降，这表明学习的模型可能偏向于特定的结构模式。为了解决这一挑战，我们提出了Cluster Information Transfer (CIT) 机制（代码可在https://github.com/BUPT-GAMMA/CITGNN找到），该机制可以为GNNs学习不变表示，从而提高它们对具有结构转移的各种未知测试图的泛化能力。CIT 机制通过将不同的集群信息与节点结合在一起，同时保留了节点之间的关系。

    arXiv:2403.03599v1 Announce Type: new  Abstract: Graph neural networks (GNNs) have become increasingly popular in modeling graph-structured data due to their ability to learn node representations by aggregating local structure information. However, it is widely acknowledged that the test graph structure may differ from the training graph structure, resulting in a structure shift. In this paper, we experimentally find that the performance of GNNs drops significantly when the structure shift happens, suggesting that the learned models may be biased towards specific structure patterns. To address this challenge, we propose the Cluster Information Transfer (CIT) mechanism (Code available at https://github.com/BUPT-GAMMA/CITGNN), which can learn invariant representations for GNNs, thereby improving their generalization ability to various and unknown test graphs with structure shift. The CIT mechanism achieves this by combining different cluster information with the nodes while preserving th
    
[^66]: DeepEclipse: 如何破解白盒DNN水印方案

    DeepEclipse: How to Break White-Box DNN-Watermarking Schemes

    [https://arxiv.org/abs/2403.03590](https://arxiv.org/abs/2403.03590)

    DeepEclipse提出了一种新颖统一的框架，用于移除白盒DNN水印，并采用与现有方法显著不同的混淆技术。

    

    深度学习（DL）模型在数字转型中变得至关重要，因此引起了对其知识产权的关注。不同的水印技术已被开发用于保护深度神经网络（DNNs）免受知识产权侵权，创造出一个竞争激烈的DNN水印和去水印方法领域。主要的水印方案使用白盒技术，涉及通过向特定DNN层添加唯一标识来修改权重。另一方面，对白盒水印的现有攻击通常需要了解特定部署的水印方案或访问基础数据以进行进一步训练和微调。我们提出DeepEclipse，一个新颖且统一的旨在移除白盒水印的框架。我们提出的混淆技术与现有白盒水印去除方案有显著区别。DeepEclipse可以在没有事先了解的情况下规避水印检测。

    arXiv:2403.03590v1 Announce Type: cross  Abstract: Deep Learning (DL) models have become crucial in digital transformation, thus raising concerns about their intellectual property rights. Different watermarking techniques have been developed to protect Deep Neural Networks (DNNs) from IP infringement, creating a competitive field for DNN watermarking and removal methods. The predominant watermarking schemes use white-box techniques, which involve modifying weights by adding a unique signature to specific DNN layers. On the other hand, existing attacks on white-box watermarking usually require knowledge of the specific deployed watermarking scheme or access to the underlying data for further training and fine-tuning. We propose DeepEclipse, a novel and unified framework designed to remove white-box watermarks. We present obfuscation techniques that significantly differ from the existing white-box watermarking removal schemes. DeepEclipse can evade watermark detection without prior knowl
    
[^67]: 用于处理因变量选择的活跃自适应实验设计的处理效应估计

    Active Adaptive Experimental Design for Treatment Effect Estimation with Covariate Choices

    [https://arxiv.org/abs/2403.03589](https://arxiv.org/abs/2403.03589)

    该研究提出了一种更有效地估计处理效应的活跃自适应实验设计方法，通过优化协变量密度和倾向得分来降低渐近方差。

    

    这项研究设计了一个自适应实验，用于高效地估计平均处理效应（ATEs）。我们考虑了一个自适应实验，其中实验者按顺序从由实验者决定的协变量密度中抽样一个实验单元，并分配一种处理。在分配处理后，实验者立即观察相应的结果。在实验结束时，实验者利用收集的样本估算出一个ATE。实验者的目标是通过较小的渐近方差估计ATE。现有研究已经设计了一些能够自适应优化倾向得分（处理分配概率）的实验。作为这种方法的一个概括，我们提出了一个框架，该框架下实验者优化协变量密度以及倾向得分，并发现优化协变量密度和倾向得分比仅优化倾向得分可以减少渐近方差更多的情况。

    arXiv:2403.03589v1 Announce Type: cross  Abstract: This study designs an adaptive experiment for efficiently estimating average treatment effect (ATEs). We consider an adaptive experiment where an experimenter sequentially samples an experimental unit from a covariate density decided by the experimenter and assigns a treatment. After assigning a treatment, the experimenter observes the corresponding outcome immediately. At the end of the experiment, the experimenter estimates an ATE using gathered samples. The objective of the experimenter is to estimate the ATE with a smaller asymptotic variance. Existing studies have designed experiments that adaptively optimize the propensity score (treatment-assignment probability). As a generalization of such an approach, we propose a framework under which an experimenter optimizes the covariate density, as well as the propensity score, and find that optimizing both covariate density and propensity score reduces the asymptotic variance more than o
    
[^68]: RouteExplainer：一种车辆路径问题的解释框架

    RouteExplainer: An Explanation Framework for Vehicle Routing Problem

    [https://arxiv.org/abs/2403.03585](https://arxiv.org/abs/2403.03585)

    提出了一种车辆路径问题的解释框架RouteExplainer，实现了边的影响解释和意图推断，通过大型语言模型生成解释文本，并在多个VRP上进行了量化评估

    

    车辆路径问题（VRP）是一个广泛研究的组合优化问题，已应用于各种实际问题。尽管对VRP的可解释性对于改善实际VRP应用中的可靠性和互动性至关重要，但这个领域仍未被探索。本文提出了RouteExplainer，一种事后解释框架，用于解释在生成路径中每条边的影响。我们的框架通过重新思考路径为动作序列并基于动作影响模型扩展对VRP的反事实解释来实现这一点。为了增强解释，我们额外提出了一个边界分类器，推断每个边界的意图，一个用于训练边界分类器的损失函数，以及通过大型语言模型（LLMs）生成解释文本。我们在四种不同的VRP上定量评估了我们的边界分类器。结果表明，在保持计算速度的同时

    arXiv:2403.03585v1 Announce Type: cross  Abstract: The Vehicle Routing Problem (VRP) is a widely studied combinatorial optimization problem and has been applied to various practical problems. While the explainability for VRP is significant for improving the reliability and interactivity in practical VRP applications, it remains unexplored. In this paper, we propose RouteExplainer, a post-hoc explanation framework that explains the influence of each edge in a generated route. Our framework realizes this by rethinking a route as the sequence of actions and extending counterfactual explanations based on the action influence model to VRP. To enhance the explanation, we additionally propose an edge classifier that infers the intentions of each edge, a loss function to train the edge classifier, and explanation-text generation by Large Language Models (LLMs). We quantitatively evaluate our edge classifier on four different VRPs. The results demonstrate its rapid computation while maintaining
    
[^69]: 提高ASD检测准确性：机器学习和深度学习模型与自然语言处理的综合方法

    Enhancing ASD detection accuracy: a combined approach of machine learning and deep learning models with natural language processing

    [https://arxiv.org/abs/2403.03581](https://arxiv.org/abs/2403.03581)

    该研究利用机器学习和深度学习模型结合自然语言处理技术，成功提高了ASD检测准确率，尤其在儿童中具有重要意义。

    

    arXiv:2403.03581v1 公告类型：新摘要：目的：我们的研究探讨了利用人工智能（AI）来诊断自闭症谱系障碍（ASD）。它专注于利用机器学习（ML）和深度学习（DL）从社交媒体上的文本输入中检测ASD，解决传统ASD诊断中的挑战。方法：我们使用自然语言处理（NLP）、ML和DL模型（包括决策树、XGB、KNN、RNN、LSTM、Bi-LSTM、BERT和BERTweet）分析了404,627条tweets，根据ASD或非ASD作者对其进行分类。其中90,000条tweets用于模型训练和测试。结果：我们的AI模型表现出较高的准确性，成功率达到88％，可以识别出来自ASD患者的文本。结论：该研究展示了AI在改善ASD诊断方面的潜力，特别是在儿童中，突显了早期检测的重要性。

    arXiv:2403.03581v1 Announce Type: new  Abstract: Purpose: Our study explored the use of artificial intelligence (AI) to diagnose autism spectrum disorder (ASD). It focused on machine learning (ML) and deep learning (DL) to detect ASD from text inputs on social media, addressing challenges in traditional ASD diagnosis.   Methods: We used natural language processing (NLP), ML, and DL models (including decision trees, XGB, KNN, RNN, LSTM, Bi-LSTM, BERT, and BERTweet) to analyze 404,627 tweets, classifying them based on ASD or non-ASD authors. A subset of 90,000 tweets was used for model training and testing.   Results: Our AI models showed high accuracy, with an 88% success rate in identifying texts from individuals with ASD.   Conclusion: The study demonstrates AI's potential in improving ASD diagnosis, especially in children, highlighting the importance of early detection.
    
[^70]: 在分类任务中的迁移：子类别的泛化能力如何？

    On Transfer in Classification: How Well do Subsets of Classes Generalize?

    [https://arxiv.org/abs/2403.03569](https://arxiv.org/abs/2403.03569)

    本文的创新之处在于建立了一个部分有序集，用以代表哪些类别子集可以推广到其他类别，并探索了类别子集如何提高测试性能以及少样本学习中迁移的重要性。

    

    在分类任务中，通常会发现在给定一组类别上训练的模型可以推广到以前未见过的类别，这表明了学习超越初始任务的能力。这种能力通常在迁移学习的背景下得到利用，其中预训练模型可以用于处理新类别，无论是否进行微调。令人惊讶的是，很少有论文探讨这一现象背后的理论根基。在这项工作中，我们试图为类别集之间的可迁移性奠定这样一个理论框架的基础。具体来说，我们建立了一个类别子集的部分有序集。这个工具可以表示哪个类别子集可以推广到其他类别。在更实际的情境中，我们探讨了我们的框架预测哪个类别子集可以在对所有类别进行测试时实现最佳性能的能力。我们还探讨了少样本学习，其中转移是至关重要的。

    arXiv:2403.03569v1 Announce Type: new  Abstract: In classification, it is usual to observe that models trained on a given set of classes can generalize to previously unseen ones, suggesting the ability to learn beyond the initial task. This ability is often leveraged in the context of transfer learning where a pretrained model can be used to process new classes, with or without fine tuning. Surprisingly, there are a few papers looking at the theoretical roots beyond this phenomenon. In this work, we are interested in laying the foundations of such a theoretical framework for transferability between sets of classes. Namely, we establish a partially ordered set of subsets of classes. This tool allows to represent which subset of classes can generalize to others. In a more practical setting, we explore the ability of our framework to predict which subset of classes can lead to the best performance when testing on all of them. We also explore few-shot learning, where transfer is the golden
    
[^71]: 基于深度自编码器的移动操作机器人物体滑移感知的多模态异常检测

    Multimodal Anomaly Detection based on Deep Auto-Encoder for Object Slip Perception of Mobile Manipulation Robots

    [https://arxiv.org/abs/2403.03563](https://arxiv.org/abs/2403.03563)

    提出了一个基于深度自编码器的多模态异常检测方法，用于移动操作机器人的物体滑移感知，能够整合来自不同机器人传感器的数据流并识别异常。

    

    物体滑移感知对于移动操作机器人在动态实际环境中可靠执行操作任务至关重要。传统的机器臂滑移感知方法使用触觉或视觉传感器。然而，移动机器人仍然需要处理传感器信号中的噪声，这是由机器人在不断变化的环境中移动造成的。为了解决这一问题，我们提出了一种利用基于深度自编码器模型的多感知数据的异常检测方法。所提出的框架整合了从各种机器人传感器收集的异构数据流，包括RGB和深度摄像机、麦克风以及力矩传感器。综合数据用于训练深度自编码器，以构建表示多模态数据的隐变量，指示正常状态。通过测量训练编码器的隐变量值与输入值之间的差异来识别异常。

    arXiv:2403.03563v1 Announce Type: cross  Abstract: Object slip perception is essential for mobile manipulation robots to perform manipulation tasks reliably in the dynamic real-world. Traditional approaches to robot arms' slip perception use tactile or vision sensors. However, mobile robots still have to deal with noise in their sensor signals caused by the robot's movement in a changing environment. To solve this problem, we present an anomaly detection method that utilizes multisensory data based on a deep autoencoder model. The proposed framework integrates heterogeneous data streams collected from various robot sensors, including RGB and depth cameras, a microphone, and a force-torque sensor. The integrated data is used to train a deep autoencoder to construct latent representations of the multisensory data that indicate the normal status. Anomalies can then be identified by error scores measured by the difference between the trained encoder's latent values and the latent values of
    
[^72]: 高效算法用于经验群分布鲁棒优化及更多

    Efficient Algorithms for Empirical Group Distributional Robust Optimization and Beyond

    [https://arxiv.org/abs/2403.03562](https://arxiv.org/abs/2403.03562)

    该研究提出了一种高效算法，用于解决群分布鲁棒优化问题，通过两级有限和凹凸最小最大优化结构和随机方差减小镜像Prox算法，实现对所有组的方差减少，并支持非恒定学习率。

    

    我们研究了群分布鲁棒优化（GDRO）的经验对应问题，旨在最小化$m$个不同组中的最大经验风险。我们将经验GDRO表述为$\textit{两级}$有限和凹凸最小最大优化问题，并开发了一种随机方差减小镜像Prox算法。与现有方法不同，我们通过逐组抽样技术构建了随机梯度，并为所有组执行方差减少，充分利用了经验GDRO的$\textit{两级}$有限和结构。此外，我们通过一个一索引偏移加权平均来计算快照和镜像快照点，这使我们与朴素的遍历平均方法有所不同。我们的算法还支持非恒定学习率，这与现有文献不同。我们建立了期望和高概率收敛保证，展示出$\m

    arXiv:2403.03562v1 Announce Type: new  Abstract: We investigate the empirical counterpart of group distributionally robust optimization (GDRO), which aims to minimize the maximal empirical risk across $m$ distinct groups. We formulate empirical GDRO as a $\textit{two-level}$ finite-sum convex-concave minimax optimization problem and develop a stochastic variance reduced mirror prox algorithm. Unlike existing methods, we construct the stochastic gradient by per-group sampling technique and perform variance reduction for all groups, which fully exploits the $\textit{two-level}$ finite-sum structure of empirical GDRO. Furthermore, we compute the snapshot and mirror snapshot point by a one-index-shifted weighted average, which distinguishes us from the naive ergodic average. Our algorithm also supports non-constant learning rates, which is different from existing literature. We establish convergence guarantees both in expectation and with high probability, demonstrating a complexity of $\m
    
[^73]: 通过深度强化学习的人口感知在线镜像下降解均场博弈

    Population-aware Online Mirror Descent for Mean-Field Games by Deep Reinforcement Learning

    [https://arxiv.org/abs/2403.03552](https://arxiv.org/abs/2403.03552)

    本文提出了一种通过深度强化学习算法实现基于人口的纳什均衡的方法，通过设计额外的内循环重播缓冲区，智能体可以有效地学习从任何分布实现纳什均衡，从而在多代理系统中展现出更好的收敛特性。

    

    均场博弈（MFGs）具有处理大规模多智体系统的能力，但在MFGs中学习纳什均衡仍然是一项具有挑战性的任务。本文提出了一种深度强化学习（DRL）算法，通过Munchausen RL和在线镜像下降的启发，实现了基于人口的纳什均衡，无需对历史进行平均或抽样。通过设计额外的内循环重播缓冲区，智能体可以有效地学习从任何分布实现纳什均衡，减轻了灾难性遗忘的影响。由此产生的策略可以应用于各种初始分布。对四个经典示例的数值实验表明，我们的算法具有比SOTA算法更好的收敛特性，特别是基于人口的策略的DRL版本虚构博弈。

    arXiv:2403.03552v1 Announce Type: cross  Abstract: Mean Field Games (MFGs) have the ability to handle large-scale multi-agent systems, but learning Nash equilibria in MFGs remains a challenging task. In this paper, we propose a deep reinforcement learning (DRL) algorithm that achieves population-dependent Nash equilibrium without the need for averaging or sampling from history, inspired by Munchausen RL and Online Mirror Descent. Through the design of an additional inner-loop replay buffer, the agents can effectively learn to achieve Nash equilibrium from any distribution, mitigating catastrophic forgetting. The resulting policy can be applied to various initial distributions. Numerical experiments on four canonical examples demonstrate our algorithm has better convergence properties than SOTA algorithms, in particular a DRL version of Fictitious Play for population-dependent policies.
    
[^74]: 通过微调预先为高斯降噪而训练的UNet进行低剂量CT图像重建，用于图像增强的下游任务

    Low-Dose CT Image Reconstruction by Fine-Tuning a UNet Pretrained for Gaussian Denoising for the Downstream Task of Image Enhancement

    [https://arxiv.org/abs/2403.03551](https://arxiv.org/abs/2403.03551)

    提出了一种通过精调UNet进行低剂量CT图像重建的方法，其中第二阶段的训练策略为CT图像增强阶段。

    

    计算机断层扫描（CT）是一种广泛使用的医学成像模态，由于其基于电离辐射，因此希望尽量减少辐射剂量。然而，降低辐射剂量会导致图像质量下降，从低剂量CT（LDCT）数据重建仍然是一个具有挑战性的任务，值得进行研究。根据LoDoPaB-CT基准，许多最先进的方法使用涉及UNet型架构的流程。具体来说，排名第一的方法ItNet使用包括滤波反投影（FBP）、在CT数据上训练的UNet和迭代细化步骤的三阶段流程。在本文中，我们提出了一种更简单的两阶段方法。第一阶段也使用了FBP，而新颖之处在于第二阶段的训练策略，特点是CT图像增强阶段。我们方法的关键点在于神经网络是预训练的。

    arXiv:2403.03551v1 Announce Type: cross  Abstract: Computed Tomography (CT) is a widely used medical imaging modality, and as it is based on ionizing radiation, it is desirable to minimize the radiation dose. However, a reduced radiation dose comes with reduced image quality, and reconstruction from low-dose CT (LDCT) data is still a challenging task which is subject to research. According to the LoDoPaB-CT benchmark, a benchmark for LDCT reconstruction, many state-of-the-art methods use pipelines involving UNet-type architectures. Specifically the top ranking method, ItNet, employs a three-stage process involving filtered backprojection (FBP), a UNet trained on CT data, and an iterative refinement step. In this paper, we propose a less complex two-stage method. The first stage also employs FBP, while the novelty lies in the training strategy for the second stage, characterized as the CT image enhancement stage. The crucial point of our approach is that the neural network is pretrained
    
[^75]: 基于扩散模型的低复杂度MIMO信道估计的生成先验

    Diffusion-based Generative Prior for Low-Complexity MIMO Channel Estimation

    [https://arxiv.org/abs/2403.03545](https://arxiv.org/abs/2403.03545)

    提出了一种基于扩散模型的低复杂度MIMO信道估计器，通过设计轻量级CNN和位置嵌入的学习方法，实现了在稀疏角域中对信道分布进行估计，并采用避免随机重采样的估计策略，以及截断反向扩散步骤的策略，取得了低复杂度和内存开销的效果。

    

    本文提出了一种基于扩散模型（DMs）的新型信道估计器，这是目前评价最高的生成模型之一。与利用生成先验的相关工作相反，设计了一个轻量级的卷积神经网络（CNN），该网络通过学习稀疏角域中的信噪比（SNR）信息的位置嵌入来设计了一个轻量级的卷积神经网络（CNN）。结合一个避免随机重采样的估计策略，并截断用于解释比给定导频观察更低信噪比的反向扩散步骤，导致的DM信道估计器具有低复杂性和内存开销。 数值结果表明，与利用生成先验的最先进信道估计器相比，DM估计器表现更好。

    arXiv:2403.03545v1 Announce Type: cross  Abstract: This work proposes a novel channel estimator based on diffusion models (DMs), one of the currently top-rated generative models. Contrary to related works utilizing generative priors, a lightweight convolutional neural network (CNN) with positional embedding of the signal-to-noise ratio (SNR) information is designed by learning the channel distribution in the sparse angular domain. Combined with an estimation strategy that avoids stochastic resampling and truncates reverse diffusion steps that account for lower SNR than the given pilot observation, the resulting DM estimator has both low complexity and memory overhead. Numerical results exhibit better performance than state-of-the-art channel estimators utilizing generative priors.
    
[^76]: DPOT: 自回归去噪运算器变换器用于大规模PDE预训练

    DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training

    [https://arxiv.org/abs/2403.03542](https://arxiv.org/abs/2403.03542)

    本文提出了一种新的自回归去噪预训练策略，可以更稳定、更高效地在PDE数据上进行预训练，并且通过基于傅里叶注意力的模型架构设计，实现了在大规模预训练中轻松扩展模型，该模型在多个PDE数据集上取得了SOTA表现。

    

    预训练已经被研究用来提高在数据稀缺环境中训练神经算子的效率和性能。然而，由于偏微分方程（PDE）数据的固有复杂性和多样性，如长轨迹、多个尺度和不同维度，它在很大程度上还处于起步阶段。在本文中，我们提出了一种新的自回归去噪预训练策略，这种策略能够更稳定、更高效地在PDE数据上进行预训练，并且可以泛化到各种下游任务。此外，通过基于傅里叶注意力的灵活可扩展模型架构的设计，我们可以轻松地将模型扩展到大规模预训练。我们在10+个PDE数据集上训练了具有超过0.5B参数的PDE基础模型，包括超过100k轨迹。大量实验证明我们在这些基准上取得了SOTA，并验证了我们的模型对显著提升性能的强大泛化能力。

    arXiv:2403.03542v1 Announce Type: new  Abstract: Pre-training has been investigated to improve the efficiency and performance of training neural operators in data-scarce settings. However, it is largely in its infancy due to the inherent complexity and diversity, such as long trajectories, multiple scales and varying dimensions of partial differential equations (PDEs) data. In this paper, we present a new auto-regressive denoising pre-training strategy, which allows for more stable and efficient pre-training on PDE data and generalizes to various downstream tasks. Moreover, by designing a flexible and scalable model architecture based on Fourier attention, we can easily scale up the model for large-scale pre-training. We train our PDE foundation model with up to 0.5B parameters on 10+ PDE datasets with more than 100k trajectories. Extensive experiments show that we achieve SOTA on these benchmarks and validate the strong generalizability of our model to significantly enhance performanc
    
[^77]: 使用条件深度学习减少脑部MRI中的钆剂用量

    Gadolinium dose reduction for brain MRI using conditional deep learning

    [https://arxiv.org/abs/2403.03539](https://arxiv.org/abs/2403.03539)

    使用条件深度学习技术，本研究通过利用对比信号提高归纳图像质量，解决了钆剂在脑部MRI中用量减少的挑战

    

    最近，提出了基于深度学习（DL）的方法来计算减少钆基造影剂（GBCAs）以减轻不良副作用同时保持诊断价值。目前，这些方法面临的两个主要挑战是准确预测对比增强和合成逼真图像。本研究通过利用预对比和后对比图像对的减法图像中编码的对比信号来解决这两个挑战。为了避免合成任何噪音或伪影，并仅专注于从低剂量减法图像中提取和增强对比信号，我们使用无噪声的标准剂量减法图像作为目标来训练我们的DL模型。因此，我们的模型仅预测对比增强信号；从而实现超出标准剂量的图像合成。此外，我们还借鉴了最近扩散模型的嵌入思想。

    arXiv:2403.03539v1 Announce Type: cross  Abstract: Recently, deep learning (DL)-based methods have been proposed for the computational reduction of gadolinium-based contrast agents (GBCAs) to mitigate adverse side effects while preserving diagnostic value. Currently, the two main challenges for these approaches are the accurate prediction of contrast enhancement and the synthesis of realistic images. In this work, we address both challenges by utilizing the contrast signal encoded in the subtraction images of pre-contrast and post-contrast image pairs. To avoid the synthesis of any noise or artifacts and solely focus on contrast signal extraction and enhancement from low-dose subtraction images, we train our DL model using noise-free standard-dose subtraction images as targets. As a result, our model predicts the contrast enhancement signal only; thereby enabling synthesization of images beyond the standard dose. Furthermore, we adapt the embedding idea of recent diffusion-based models
    
[^78]: 任务属性距离用于少样本学习：理论分析和应用

    Task Attribute Distance for Few-Shot Learning: Theoretical Analysis and Applications

    [https://arxiv.org/abs/2403.03535](https://arxiv.org/abs/2403.03535)

    本文介绍了基于任务属性距离（TAD）的度量方法，用于量化训练和新任务之间的关系，以及该关系对不同模型在新任务上适应困难的影响。

    

    少样本学习（FSL）旨在通过利用从\emph{相关}训练任务中积累的经验，用极少标记样本学习新任务。本文通过探讨两个关键问题来理解FSL：（1）如何量化\emph{训练}和\emph{新}任务之间的关系？（2）这种关系如何影响不同模型对新任务的\emph{适应困难}？为了回答这两个问题，我们引入了基于属性的任务属性距离（TAD）作为度量任务相关性的指标。与许多现有度量不同，TAD是与模型无关的，使其适用于不同的FSL模型。然后，我们利用TAD指标建立了任务相关性和任务适应困难之间的理论联系。通过在新任务上推导广义化错误界限，我们发现了TAD如何度量FSL模型在新任务上的适应困难。为了验证我们的TAD指标和th

    arXiv:2403.03535v1 Announce Type: cross  Abstract: Few-shot learning (FSL) aims to learn novel tasks with very few labeled samples by leveraging experience from \emph{related} training tasks. In this paper, we try to understand FSL by delving into two key questions: (1) How to quantify the relationship between \emph{training} and \emph{novel} tasks? (2) How does the relationship affect the \emph{adaptation difficulty} on novel tasks for different models? To answer the two questions, we introduce Task Attribute Distance (TAD) built upon attributes as a metric to quantify the task relatedness. Unlike many existing metrics, TAD is model-agnostic, making it applicable to different FSL models. Then, we utilize TAD metric to establish a theoretical connection between task relatedness and task adaptation difficulty. By deriving the generalization error bound on a novel task, we discover how TAD measures the adaptation difficulty on novel tasks for FSL models. To validate our TAD metric and th
    
[^79]: FingerNet: 基于深度神经网络的手指敲击任务的 EEG 解码

    FingerNet: EEG Decoding of A Fine Motor Imagery with Finger-tapping Task Based on A Deep Neural Network

    [https://arxiv.org/abs/2403.03526](https://arxiv.org/abs/2403.03526)

    FingerNet是一个专门用于细致手指想象运动分类的网络，能够从EEG信号中提取空间和时间特征，在同一只手内的分类准确度方面表现显著优越。

    

    大脑计算机接口（BCI）技术促进了人脑与计算机之间的交流，主要利用脑电图（EEG）信号来识别人类意图。尽管针对瘫痪患者已开发了基于EEG的BCI系统，但正在进行的研究探索了语言想象和运动想象（MI）系统。本研究介绍了 FingerNet，这是一种专门用于精细MI分类的网络，偏离了传统的粗糙MI研究。所提出的 FingerNet 可以从 EEG 信号中提取空间和时间特征，提高了在同一只手内的分类准确度。实验结果表明，在分类五个手指敲击任务（拇指、食指、中指、无名指和小指运动）方面性能显示出显著较高的准确度。与传统基准模型 EEGNet 和 DeepConvNet 相比，FingerNet 表现出卓越的性能。

    arXiv:2403.03526v1 Announce Type: cross  Abstract: Brain-computer interface (BCI) technology facilitates communication between the human brain and computers, primarily utilizing electroencephalography (EEG) signals to discern human intentions. Although EEG-based BCI systems have been developed for paralysis individuals, ongoing studies explore systems for speech imagery and motor imagery (MI). This study introduces FingerNet, a specialized network for fine MI classification, departing from conventional gross MI studies. The proposed FingerNet could extract spatial and temporal features from EEG signals, improving classification accuracy within the same hand. The experimental results demonstrated that performance showed significantly higher accuracy in classifying five finger-tapping tasks, encompassing thumb, index, middle, ring, and little finger movements. FingerNet demonstrated dominant performance compared to the conventional baseline models, EEGNet and DeepConvNet. The average acc
    
[^80]: 自发性言语中的非言语信息 - 朝着新的分析框架

    Non-verbal information in spontaneous speech - towards a new framework of analysis

    [https://arxiv.org/abs/2403.03522](https://arxiv.org/abs/2403.03522)

    这项研究提出了一个分析框架和技术验证概念，用于对言语中的非言语信号进行分类，并将其与含义关联起来，从而为探索表达实现多层韵律事件的大型数据提供了一种方法。

    

    言语中的非言语信号是由韵律编码的，携带的信息范围从对话行为到态度和情感。尽管其重要性，掌握掌声结构的原则仍未得到充分理解。本文提出了一个分析框架和技术验证概念，用于对韵律信号进行分类，并将其与含义关联起来。该框架解释了多层韵律事件的表层表示。作为实施的第一步，我们提出了一个分类过程，可以解开三个级别的韵律现象。它依赖于微调预训练的语音识别模型，实现同时的多类别/多标签检测。它可以概括各种各样的自发数据，在与人类注释相当或优于的情况下执行。除了对韵律的标准化形式化外，解开韵律模式还可以指导

    arXiv:2403.03522v1 Announce Type: cross  Abstract: Non-verbal signals in speech are encoded by prosody and carry information that ranges from conversation action to attitude and emotion. Despite its importance, the principles that govern prosodic structure are not yet adequately understood. This paper offers an analytical schema and a technological proof-of-concept for the categorization of prosodic signals and their association with meaning. The schema interprets surface-representations of multi-layered prosodic events. As a first step towards implementation, we present a classification process that disentangles prosodic phenomena of three orders. It relies on fine-tuning a pre-trained speech recognition model, enabling the simultaneous multi-class/multi-label detection. It generalizes over a large variety of spontaneous data, performing on a par with, or superior to, human annotation. In addition to a standardized formalization of prosody, disentangling prosodic patterns can direct a
    
[^81]: 探究使用CounterfacTS探究时间序列预测模型的鲁棒性

    Probing the Robustness of Time-series Forecasting Models with CounterfacTS

    [https://arxiv.org/abs/2403.03508](https://arxiv.org/abs/2403.03508)

    提出了CounterfacTS工具，通过反事实探究深度学习模型在时间序列预测中的鲁棒性。

    

    机器学习模型应用于时间序列预测时面临的一个常见问题是数据分布的时间演化（即概念漂移）。由于大多数训练数据没有反映这些变化，模型在新的分布场景下表现出很差，因此，此类事件的影响事前无法可靠地预测。我们提出并公开发布CounterfacTS，这是一个通过反事实探究深度学习模型在时间序列预测任务中鲁棒性的工具。CounterfacTS具有用户友好的界面，允许用户可视化、比较和量化时间序列数据及其预测结果，适用于多个数据集和深度学习模型。此外，用户可以对时间序列应用各种变换，并以可解释的方式探索预测结果的变化。通过示例案例，我们演示了CounterfacTS如何用于：

    arXiv:2403.03508v1 Announce Type: new  Abstract: A common issue for machine learning models applied to time-series forecasting is the temporal evolution of the data distributions (i.e., concept drift). Because most of the training data does not reflect such changes, the models present poor performance on the new out-of-distribution scenarios and, therefore, the impact of such events cannot be reliably anticipated ahead of time. We present and publicly release CounterfacTS, a tool to probe the robustness of deep learning models in time-series forecasting tasks via counterfactuals. CounterfacTS has a user-friendly interface that allows the user to visualize, compare and quantify time series data and their forecasts, for a number of datasets and deep learning models. Furthermore, the user can apply various transformations to the time series and explore the resulting changes in the forecasts in an interpretable manner. Through example cases, we illustrate how CounterfacTS can be used to i)
    
[^82]: GaLore: 通过梯度低秩投影实现高效的LLM训练

    GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection

    [https://arxiv.org/abs/2403.03507](https://arxiv.org/abs/2403.03507)

    GaLore提出了一种名为Gradient Low-Rank Projection (GaLore)的训练策略，相比于一般低秩适应方法，它能够实现更高效的LLM训练，大幅降低内存使用同时保持性能。

    

    训练大型语言模型(LLMs)存在显着的内存挑战，主要是由于权重和优化器状态的不断增加。通常的内存减少方法，如低秩适应（LoRA），在每个层中向冻结的预训练权重添加可训练的低秩矩阵，从而减少了可训练的参数和优化器状态。然而，这样的方法通常在预训练和微调阶段的表现都不如完整秩权重的训练，因为它们将参数搜索限制在低秩子空间并改变了训练动态，而且可能需要完整秩的热启动。在这项工作中，我们提出了Gradient Low-Rank Projection (GaLore)，这是一种训练策略，允许完全参数学习，但比LoRA等常见低秩适应方法更节省内存。我们的方法在优化器状态上将内存使用降低了高达65.5%，同时保持了预训练和精调的效率和性能。

    arXiv:2403.03507v1 Announce Type: new  Abstract: Training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states. However, such approaches typically underperform training with full-rank weights in both pre-training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA. Our approach reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for pre-tra
    
[^83]: 一种双自蒸馏的无教师图知识蒸馏框架

    A Teacher-Free Graph Knowledge Distillation Framework with Dual Self-Distillation

    [https://arxiv.org/abs/2403.03483](https://arxiv.org/abs/2403.03483)

    本文提出了一种无教师图自蒸馏框架，不需要教师模型或GNN，仅基于MLP，有助于缩小学术和工业之间的差距。

    

    近年来，使用图神经网络（GNNs）处理与图相关的任务取得了巨大成功。尽管在学术上取得了巨大成功，但多层感知器（MLPs）仍然是实际工业应用的主要工具。本文发现，无论是教师还是GNN对图知识蒸馏都不是必需的。我们提出了一种无教师图自蒸馏（TGS）框架，既不需要教师模型也不需要GNNs，在训练和推理过程中都不需要。更重要的是，所提出的TGS框架纯粹基于MLP，其中结构信息仅被暗示。

    arXiv:2403.03483v1 Announce Type: new  Abstract: Recent years have witnessed great success in handling graph-related tasks with Graph Neural Networks (GNNs). Despite their great academic success, Multi-Layer Perceptrons (MLPs) remain the primary workhorse for practical industrial applications. One reason for such an academic-industry gap is the neighborhood-fetching latency incurred by data dependency in GNNs. To reduce their gaps, Graph Knowledge Distillation (GKD) is proposed, usually based on a standard teacher-student architecture, to distill knowledge from a large teacher GNN into a lightweight student GNN or MLP. However, we found in this paper that neither teachers nor GNNs are necessary for graph knowledge distillation. We propose a Teacher-Free Graph Self-Distillation (TGS) framework that does not require any teacher model or GNNs during both training and inference. More importantly, the proposed TGS framework is purely based on MLPs, where structural information is only impli
    
[^84]: 无逆矩阵快速自然梯度下降方法用于深度学习

    Inverse-Free Fast Natural Gradient Descent Method for Deep Learning

    [https://arxiv.org/abs/2403.03473](https://arxiv.org/abs/2403.03473)

    本文提出一种快速自然梯度下降（FNGD）方法，在深度学习中仅需要在第一个时代计算逆运算，避免了迭代求逆操作。

    

    二阶方法通过包含二阶导数或统计量可以比一阶方法收敛得更快，但由于计算效率低，它们在深度学习中很少被使用。为了解决这个问题，现有的许多解决方案都集中在减小需要求逆的矩阵的大小。然而，仍然需要在每次迭代中执行求逆操作。本文提出了一种快速自然梯度下降（FNGD）方法，只需在第一个时代计算逆运算。首先，我们将自然梯度下降（NGD）的梯度预处理公式重构为使用Sherman-Morrison-Woodbury公式的每个样本梯度的加权和。基于此，为了避免涉及计算系数的迭代逆操作，这些加权系数在整个时代共享而不影响经验性能。FNGD将NGD近似为f

    arXiv:2403.03473v1 Announce Type: new  Abstract: Second-order methods can converge much faster than first-order methods by incorporating second-order derivates or statistics, but they are far less prevalent in deep learning due to their computational inefficiency. To handle this, many of the existing solutions focus on reducing the size of the matrix to be inverted. However, it is still needed to perform the inverse operator in each iteration. In this paper, we present a fast natural gradient descent (FNGD) method, which only requires computing the inverse during the first epoch. Firstly, we reformulate the gradient preconditioning formula in the natural gradient descent (NGD) as a weighted sum of per-sample gradients using the Sherman-Morrison-Woodbury formula. Building upon this, to avoid the iterative inverse operation involved in computing coefficients, the weighted coefficients are shared across epochs without affecting the empirical performance.   FNGD approximates the NGD as a f
    
[^85]: 通过基础类别信息增强元训练，用于少样本学习

    Boosting Meta-Training with Base Class Information for Few-Shot Learning

    [https://arxiv.org/abs/2403.03472](https://arxiv.org/abs/2403.03472)

    提出了一个端到端的训练范式，通过增强元训练的基础类别信息，解决了Meta-Baseline两个训练阶段固有冲突的问题

    

    少样本学习是机器学习中的一项具有挑战性的任务，旨在学习一个能够适应识别具有有限标记示例的新类别的分类器。元学习已经成为少样本学习的重要框架。最初的训练框架是一个任务级学习方法，如模型无关的元学习（MAML）和原型网络。最近提出的培训范式称为元Baseline，它由序贯预训练和元训练阶段组成，取得了最先进的性能。然而，作为一种非端到端的训练方法，表明元训练阶段只能在预训练完成后开始，Meta-Baseline由于两个训练阶段的固有冲突而导致更高的训练成本和次优的性能。为了解决这些限制，我们提出了一个由两个交替循环组成的端到端训练范式。在外循环中，我们计算cr

    arXiv:2403.03472v1 Announce Type: new  Abstract: Few-shot learning, a challenging task in machine learning, aims to learn a classifier adaptable to recognize new, unseen classes with limited labeled examples. Meta-learning has emerged as a prominent framework for few-shot learning. Its training framework is originally a task-level learning method, such as Model-Agnostic Meta-Learning (MAML) and Prototypical Networks. And a recently proposed training paradigm called Meta-Baseline, which consists of sequential pre-training and meta-training stages, gains state-of-the-art performance. However, as a non-end-to-end training method, indicating the meta-training stage can only begin after the completion of pre-training, Meta-Baseline suffers from higher training cost and suboptimal performance due to the inherent conflicts of the two training stages. To address these limitations, we propose an end-to-end training paradigm consisting of two alternative loops. In the outer loop, we calculate cr
    
[^86]: 自注意力增强的图卷积网络用于结构学习和节点嵌入

    Self-Attention Empowered Graph Convolutional Network for Structure Learning and Node Embedding

    [https://arxiv.org/abs/2403.03465](https://arxiv.org/abs/2403.03465)

    本文提出了一种新颖的图学习框架GCN-SA，通过自注意力机制设计稳定有效的图结构学习模块，可以捕捉长程依赖关系和低同质性图中节点的节点级表示学习。

    

    在图结构数据的表示学习中，许多流行的图神经网络（GNNs）未能捕捉长程依赖关系，导致性能下降。此外，当关注的图表征为异质性（低同质性）时，这种弱点会被放大。为解决这一问题，本文提出了一种新颖的图学习框架，名为具有自注意力的图卷积网络（GCN-SA）。所提出的方案在节点级表示学习中表现出卓越的泛化能力。所提出的GCN-SA包含两个增强对应于边和节点特征。对于边，我们利用自注意力机制设计了一个稳定且有效的图结构学习模块，能够捕捉任意一对节点之间的内部相关性。这种图结构学习模块可以从整个图中为每个节点识别可靠的邻居。关于节点特征，我们修改...

    arXiv:2403.03465v1 Announce Type: new  Abstract: In representation learning on graph-structured data, many popular graph neural networks (GNNs) fail to capture long-range dependencies, leading to performance degradation. Furthermore, this weakness is magnified when the concerned graph is characterized by heterophily (low homophily). To solve this issue, this paper proposes a novel graph learning framework called the graph convolutional network with self-attention (GCN-SA). The proposed scheme exhibits an exceptional generalization capability in node-level representation learning. The proposed GCN-SA contains two enhancements corresponding to edges and node features. For edges, we utilize a self-attention mechanism to design a stable and effective graph-structure-learning module that can capture the internal correlation between any pair of nodes. This graph-structure-learning module can identify reliable neighbors for each node from the entire graph. Regarding the node features, we modi
    
[^87]: 用于长期个性化家庭服务机器人的交互式持续学习架构

    Interactive Continual Learning Architecture for Long-Term Personalization of Home Service Robots

    [https://arxiv.org/abs/2403.03462](https://arxiv.org/abs/2403.03462)

    提出了一种新颖的交互式持续学习架构，旨在通过人机交互在家庭环境中持续学习语义知识。

    

    为了在非结构化家庭环境中执行辅助任务，机器人必须学习和推理环境的语义知识。尽管语义推理架构的发展出现了复苏，但这些方法假定所有训练数据是预先提供的。然而，每个用户的环境是独特的，并且可以随着时间的推移而持续变化，这使得这些方法不适用于个性化家庭服务机器人。尽管持续学习的研究开发了可以随时间学习和适应的方法，但这些方法大多在静态图像数据集上的对象分类狭义语境中进行了测试。本文结合了持续学习、语义推理和交互式机器学习文献中的思想，开发了一种新颖的交互式持续学习架构，用于通过人机交互在家庭环境中持续学习语义知识。

    arXiv:2403.03462v1 Announce Type: cross  Abstract: For robots to perform assistive tasks in unstructured home environments, they must learn and reason on the semantic knowledge of the environments. Despite a resurgence in the development of semantic reasoning architectures, these methods assume that all the training data is available a priori. However, each user's environment is unique and can continue to change over time, which makes these methods unsuitable for personalized home service robots. Although research in continual learning develops methods that can learn and adapt over time, most of these methods are tested in the narrow context of object classification on static image datasets. In this paper, we combine ideas from continual learning, semantic reasoning, and interactive machine learning literature and develop a novel interactive continual learning architecture for continual learning of semantic knowledge in a home environment through human-robot interaction. The architectu
    
[^88]: TGPT-PINN：具有转换GPT-PINNs的非线性模型缩减

    TGPT-PINN: Nonlinear model reduction with transformed GPT-PINNs

    [https://arxiv.org/abs/2403.03459](https://arxiv.org/abs/2403.03459)

    TGPT-PINN通过引入转换层和捕获冲击损失函数组件，在PINNs框架中实现了非线性模型的缩减，有效解决了具有参数依赖性间断的问题。

    

    我们引入了转换生成预训练物理信息神经网络（TGPT-PINN），用于在MOR整合PINNs框架中完成传输主导偏微分方程的非线性模型阶减。在最近开发的GPT-PINN的基础上，GPT-PINN是一种实现基于快照的模型缩减的网络结构设计，我们设计并测试了一种新颖的非线性模型减范例，可以有效处理具有参数依赖性间断的问题。通过纳入捕获冲击的损失函数分量以及参数依赖的变换层，TGPT-PINN克服了传输主导区域中线性模型减的限制。我们通过几个非平凡参数偏微分方程在PINNs框架中展示了这种新的非线性模型缩减能力。

    arXiv:2403.03459v1 Announce Type: cross  Abstract: We introduce the Transformed Generative Pre-Trained Physics-Informed Neural Networks (TGPT-PINN) for accomplishing nonlinear model order reduction (MOR) of transport-dominated partial differential equations in an MOR-integrating PINNs framework. Building on the recent development of the GPT-PINN that is a network-of-networks design achieving snapshot-based model reduction, we design and test a novel paradigm for nonlinear model reduction that can effectively tackle problems with parameter-dependent discontinuities. Through incorporation of a shock-capturing loss function component as well as a parameter-dependent transform layer, the TGPT-PINN overcomes the limitations of linear model reduction in the transport-dominated regime. We demonstrate this new capability for nonlinear model reduction in the PINNs framework by several nontrivial parametric partial differential equations.
    
[^89]: 槽抽象化器：迈向可扩展的抽象视觉推理

    Slot Abstractors: Toward Scalable Abstract Visual Reasoning

    [https://arxiv.org/abs/2403.03458](https://arxiv.org/abs/2403.03458)

    提出了槽抽象化器，结合槽方法和关系归纳偏见，实现了可扩展的抽象视觉推理。

    

    抽象视觉推理是一种人类特有的能力，允许识别从对象特征中抽象出的关系模式，并将这些模式系统化地推广到未曾见过的问题。最近的研究表明，在涉及多个对象输入的视觉推理任务中，通过使用基于槽的方法提取以对象为中心的表示并具有强归纳偏见的关系抽象，实现了强大的系统化推广。然而，这种方法仅限于包含单一规则的问题，并且不适用于包含大量对象的视觉推理问题。其他最近的工作提出了抽象化器，这是Transformer的扩展，融合了强大的关系归纳偏见，从而继承了Transformer的可扩展性和多头架构，但尚未展示如何将此方法应用到

    arXiv:2403.03458v1 Announce Type: cross  Abstract: Abstract visual reasoning is a characteristically human ability, allowing the identification of relational patterns that are abstracted away from object features, and the systematic generalization of those patterns to unseen problems. Recent work has demonstrated strong systematic generalization in visual reasoning tasks involving multi-object inputs, through the integration of slot-based methods used for extracting object-centric representations coupled with strong inductive biases for relational abstraction. However, this approach was limited to problems containing a single rule, and was not scalable to visual reasoning problems containing a large number of objects. Other recent work proposed Abstractors, an extension of Transformers that incorporates strong relational inductive biases, thereby inheriting the Transformer's scalability and multi-head architecture, but it has yet to be demonstrated how this approach might be applied to
    
[^90]: 使用深度增广拉格朗日方法学习受限制优化

    Learning Constrained Optimization with Deep Augmented Lagrangian Methods

    [https://arxiv.org/abs/2403.03454](https://arxiv.org/abs/2403.03454)

    本文提出了一种使用深度增广拉格朗日方法的学习受限制优化的方法，通过训练机器学习模型直接预测对偶解估计，并构建原始估计，从而实现对偶可行解对，同时迭代向原始可行性，模拟对偶上升方法。

    

    学习优化（LtO）是一个问题设置，在此设置中，一个机器学习（ML）模型被训练成模拟一个受限制优化求解器。学习产生最优和符合复杂约束的解决方案是一项困难的任务，但通常可以通过将输入空间限制为一组相关问题的分布来实现。大多数LtO方法侧重于直接学习原始问题的解决方案，并应用校正方案或损失函数惩罚来鼓励可行性。本文提出了一种替代方法，即训练ML模型直接预测对偶解估计，从而构建原始估计以形成对偶可行解对。这使得能够进行端到端训练方案，在这种方案中对偶目标被最大化作为损失函数，解决方案估计向原始可行性迭代，模拟对偶上升方法。

    arXiv:2403.03454v1 Announce Type: new  Abstract: Learning to Optimize (LtO) is a problem setting in which a machine learning (ML) model is trained to emulate a constrained optimization solver. Learning to produce optimal and feasible solutions subject to complex constraints is a difficult task, but is often made possible by restricting the input space to a limited distribution of related problems. Most LtO methods focus on directly learning solutions to the primal problem, and applying correction schemes or loss function penalties to encourage feasibility. This paper proposes an alternative approach, in which the ML model is trained instead to predict dual solution estimates directly, from which primal estimates are constructed to form dual-feasible solution pairs. This enables an end-to-end training scheme is which the dual objective is maximized as a loss function, and solution estimates iterate toward primal feasibility, emulating a Dual Ascent method. First it is shown that the poo
    
[^91]: SalienTime: 用户驱动的大规模地理空间数据可视化中显著时间步长的选择

    SalienTime: User-driven Selection of Salient Time Steps for Large-Scale Geospatial Data Visualization

    [https://arxiv.org/abs/2403.03449](https://arxiv.org/abs/2403.03449)

    通过需要研究，建立了对显著时间步骤的定义，提出了一种利用自动编码器和动态规划的新方法来促进用户驱动的时间选择，结合结构特征和统计变化以实现更灵活的选择

    

    地理空间临时数据的庞大性来自物理监视器和仿真模型，给有效数据访问带来挑战，通常导致网络数据门户中繁琐的时间选择体验。因此，选择一个时间步骤子集以进行优先可视化和预加载是非常可取的。本文通过与领域专家进行广泛的需求研究，建立了对显著时间步骤的多方面定义，以了解他们的工作流程。在此基础上，我们提出了一种利用自动编码器和动态规划来促进用户驱动时间选择的新方法。结构特征，统计变化和距离惩罚被纳入以进行更灵活的选择。用户指定的优先级，空间区域和聚合被用来结合不同的视角。我们设计并实施了一个基于网络的接口，以实现高效

    arXiv:2403.03449v1 Announce Type: cross  Abstract: The voluminous nature of geospatial temporal data from physical monitors and simulation models poses challenges to efficient data access, often resulting in cumbersome temporal selection experiences in web-based data portals. Thus, selecting a subset of time steps for prioritized visualization and pre-loading is highly desirable. Addressing this issue, this paper establishes a multifaceted definition of salient time steps via extensive need-finding studies with domain experts to understand their workflows. Building on this, we propose a novel approach that leverages autoencoders and dynamic programming to facilitate user-driven temporal selections. Structural features, statistical variations, and distance penalties are incorporated to make more flexible selections. User-specified priorities, spatial regions, and aggregations are used to combine different perspectives. We design and implement a web-based interface to enable efficient an
    
[^92]: 多核k均值聚类的核相关-不相似度

    Kernel Correlation-Dissimilarity for Multiple Kernel k-Means Clustering

    [https://arxiv.org/abs/2403.03448](https://arxiv.org/abs/2403.03448)

    通过系统地整合核相关性和不相似度，我们提出了一种新方法，为多核k均值聚类提供了更有效的信息提取和改善聚类性能。

    

    多核k均值（MKKM）算法的主要目标是通过优化基础核矩阵来提取非线性信息并实现最佳聚类。当前的方法通过利用多个核之间的相关性或不相似性来增强信息多样性并减少冗余。然而，仅依赖于单一度量（如相关性或不相似性）来定义核关系会引入偏见和不完整的表征。因此，这种限制会妨碍有效信息提取，最终影响聚类性能。为了解决这一挑战，我们提出了一种新颖的方法，系统地整合了核相关性和不相似度。我们的方法全面捕捉了核关系，促进了更有效的分类信息提取，提高了聚类性能。

    arXiv:2403.03448v1 Announce Type: new  Abstract: The main objective of the Multiple Kernel k-Means (MKKM) algorithm is to extract non-linear information and achieve optimal clustering by optimizing base kernel matrices. Current methods enhance information diversity and reduce redundancy by exploiting interdependencies among multiple kernels based on correlations or dissimilarities. Nevertheless, relying solely on a single metric, such as correlation or dissimilarity, to define kernel relationships introduces bias and incomplete characterization. Consequently, this limitation hinders efficient information extraction, ultimately compromising clustering performance. To tackle this challenge, we introduce a novel method that systematically integrates both kernel correlation and dissimilarity. Our approach comprehensively captures kernel relationships, facilitating more efficient classification information extraction and improving clustering performance. By emphasizing the coherence between
    
[^93]: 使用集合卡尔曼反演进行DeepONets的不确定性量化

    Uncertainty quantification for deeponets with ensemble kalman inversion

    [https://arxiv.org/abs/2403.03444](https://arxiv.org/abs/2403.03444)

    使用集合卡尔曼反演方法针对DeepONets提出了一种高效的不确定性量化推断方法。

    

    近年来，操作员学习，特别是DeepONet，因其高效地学习跨不同领域的输入和输出函数之间的复杂映射而受到广泛关注。在有限且带噪声数据的实际场景中，访问DeepONet预测中的不确定性变得至关重要，特别是在使命关键或安全关键应用中。现有方法要么计算密集，要么产生令人不满意的不确定性量化，为DeepONets量身定制高效且信息丰富的不确定性量化（UQ）技术留下了空间。在这项工作中，我们提出了一种利用集合卡尔曼反演（EKI）方法的新型推断方法，用于操作员学习的高效UQ。EKI以其无导数、噪声抗干扰和高度可并行化的特性而闻名，已经证明了在面向物理的神经网络的UQ中的优势。

    arXiv:2403.03444v1 Announce Type: cross  Abstract: In recent years, operator learning, particularly the DeepONet, has received much attention for efficiently learning complex mappings between input and output functions across diverse fields. However, in practical scenarios with limited and noisy data, accessing the uncertainty in DeepONet predictions becomes essential, especially in mission-critical or safety-critical applications. Existing methods, either computationally intensive or yielding unsatisfactory uncertainty quantification, leave room for developing efficient and informative uncertainty quantification (UQ) techniques tailored for DeepONets. In this work, we proposed a novel inference approach for efficient UQ for operator learning by harnessing the power of the Ensemble Kalman Inversion (EKI) approach. EKI, known for its derivative-free, noise-robust, and highly parallelizable feature, has demonstrated its advantages for UQ for physics-informed neural networks [28]. Our inn
    
[^94]: 一种AI启用的基于Agent的模型及其在新西兰麻疹爆发模拟中的应用

    An AI-enabled Agent-Based Model and Its Application in Measles Outbreak Simulation for New Zealand

    [https://arxiv.org/abs/2403.03434](https://arxiv.org/abs/2403.03434)

    通过将图神经网络（GNN）和长短期记忆（LSTM）网络与传统ABM相结合，开发了一种可微分的基于Agent的模型，成功应用于模拟新西兰2019年麻疹爆发，深入洞察传染病爆发的动态。

    

    代理模型（ABMs）已经成为研究复杂社会互动的强大工具，特别是在公共卫生和传染病调查的背景下。为了增强传统ABM，实现自动化模型校准并减少扩展模型所需的计算资源，我们开发了一个通过耦合图神经网络（GNN）和长短期记忆（LSTM）网络来进行张量化和可微分的基于Agent的模型。该模型被用于研究2019年发生在新西兰的麻疹爆发，展示了在高峰期重复病例中准确模拟爆发动态的有希望能力。本文表明，通过利用最新的人工智能（AI）技术和传统ABMs的能力，我们可以更深入地了解传染病爆发的动态，从而帮助我们做出更好的决策。

    arXiv:2403.03434v1 Announce Type: cross  Abstract: Agent Based Models (ABMs) have emerged as a powerful tool for investigating complex social interactions, particularly in the context of public health and infectious disease investigation. In an effort to enhance the conventional ABM, enabling automated model calibration and reducing the computational resources needed for scaling up the model, we have developed a tensorized and differentiable agent-based model by coupling Graph Neural Network (GNN) and Long Short-Term Memory (LSTM) network. The model was employed to investigate the 2019 measles outbreak occurred in New Zealand, demonstrating a promising ability to accurately simulate the outbreak dynamics, particularly during the peak period of repeated cases. This paper shows that by leveraging the latest Artificial Intelligence (AI) technology and the capabilities of traditional ABMs, we gain deeper insights into the dynamics of infectious disease outbreaks. This, in turn, helps us ma
    
[^95]: 用机器学习和航天器上的诊断技术在Kepler中单次变迁检测

    Single Transit Detection In Kepler With Machine Learning And Onboard Spacecraft Diagnostics

    [https://arxiv.org/abs/2403.03427](https://arxiv.org/abs/2403.03427)

    使用卷积神经网络和航天器诊断技术，在Kepler数据集中实现了可靠地检测长轨道周期外行星的单次变迁，潜在地发现了新的行星。

    

    发现长轨道周期的外行星需要可靠地检测单个变迁，而无需关于系统的其他信息。像相位折叠光变曲线和径向速度数据的周期图分析这样的技术对于轨道周期较短的行星更敏感，导致在长周期下的行星发现不足。我们提出了一种新颖的技术，利用集成的卷积神经网络结合Kepler上的航天器诊断来对光变曲线中的变迁进行分类。我们创建了一个流程来恢复单个变迁的位置，以及轨道行星的周期，保持超过80%的变迁恢复灵敏度，达到800天的轨道周期。我们的神经网络流程有潜力在Kepler数据集中发现额外的行星，并且关键地，在η-地球区域内。我们报告了此流程的首个候选对象，KOI 1271。

    arXiv:2403.03427v1 Announce Type: cross  Abstract: Exoplanet discovery at long orbital periods requires reliably detecting individual transits without additional information about the system. Techniques like phase-folding of light curves and periodogram analysis of radial velocity data are more sensitive to planets with shorter orbital periods, leaving a dearth of planet discoveries at long periods. We present a novel technique using an ensemble of Convolutional Neural Networks incorporating the onboard spacecraft diagnostics of \emph{Kepler} to classify transits within a light curve. We create a pipeline to recover the location of individual transits, and the period of the orbiting planet, which maintains $>80\%$ transit recovery sensitivity out to an 800-day orbital period. Our neural network pipeline has the potential to discover additional planets in the \emph{Kepler} dataset, and crucially, within the $\eta$-Earth regime. We report our first candidate from this pipeline, KOI 1271.
    
[^96]: 在3D中塑造分子：面向文本的分子优化灵活子结构感知框架

    Sculpting Molecules in 3D: A Flexible Substructure Aware Framework for Text-Oriented Molecular Optimization

    [https://arxiv.org/abs/2403.03425](https://arxiv.org/abs/2403.03425)

    提出了一种通过多模态引导生成/优化任务解决分子设计问题的创新方法。

    

    通过将深度学习，特别是AI-Generated Content，与从第一原理计算中得出的高质量数据相结合，已经成为改变科学研究格局的一种有前途的途径。然而，设计既包含多模态先验知识又具有关键和复杂性的分子药物或材料的挑战依然是一项关键而复杂的工作。本文提出了一种创新方法来解决这一逆设计问题，将其构造为一种多模态导向生成/优化任务。我们提出的解决方案涉及一个面向文本-结构对齐的对称扩散框架，用于实现分子生成/优化任务，即3DToMolo.

    arXiv:2403.03425v1 Announce Type: new  Abstract: The integration of deep learning, particularly AI-Generated Content, with high-quality data derived from ab initio calculations has emerged as a promising avenue for transforming the landscape of scientific research. However, the challenge of designing molecular drugs or materials that incorporate multi-modality prior knowledge remains a critical and complex undertaking. Specifically, achieving a practical molecular design necessitates not only meeting the diversity requirements but also addressing structural and textural constraints with various symmetries outlined by domain experts. In this article, we present an innovative approach to tackle this inverse design problem by formulating it as a multi-modality guidance generation/optimization task. Our proposed solution involves a textural-structure alignment symmetric diffusion framework for the implementation of molecular generation/optimization tasks, namely 3DToMolo. 3DToMolo aims to 
    
[^97]: LEAD：学习分解用于无源通用领域自适应

    LEAD: Learning Decomposition for Source-free Universal Domain Adaptation

    [https://arxiv.org/abs/2403.03421](https://arxiv.org/abs/2403.03421)

    提出了LEAD方法，通过将特征分解为源已知和未知组件来识别目标私有数据。

    

    通用领域自适应（UniDA）旨在实现在存在协变量和标签转移的情况下进行知识转移。 最近，无源通用领域自适应（SF-UniDA）已经出现，旨在实现UniDA而无需访问源数据，这更实用由于数据保护政策。 本文提出了一个新的LEArning Decomposition（LEAD）的想法，通过将特征分解为源已知和未知组件来识别目标私有数据。

    arXiv:2403.03421v1 Announce Type: cross  Abstract: Universal Domain Adaptation (UniDA) targets knowledge transfer in the presence of both covariate and label shifts. Recently, Source-free Universal Domain Adaptation (SF-UniDA) has emerged to achieve UniDA without access to source data, which tends to be more practical due to data protection policies. The main challenge lies in determining whether covariate-shifted samples belong to target-private unknown categories. Existing methods tackle this either through hand-crafted thresholding or by developing time-consuming iterative clustering strategies. In this paper, we propose a new idea of LEArning Decomposition (LEAD), which decouples features into source-known and -unknown components to identify target-private data. Technically, LEAD initially leverages the orthogonal decomposition analysis for feature decomposition. Then, LEAD builds instance-level decision boundaries to adaptively identify target-private data. Extensive experiments a
    
[^98]: 利用有限情绪处理状态研究晚年心理健康

    Leveraging The Finite States of Emotion Processing to Study Late-Life Mental Health

    [https://arxiv.org/abs/2403.03414](https://arxiv.org/abs/2403.03414)

    通过隐藏马尔可夫模型和有限状态自动机的应用，可以更全面地理解晚年心理健康中的系统级动态。

    

    传统的心理健康研究方法采用广义线性模型(GLM)描述观察到的心理行为测量数据(问卷摘要分数)的纵向动态。类似地，GLM也被应用于表征神经生物测量数据(区域fMRI信号)与感知刺激或其他区域信号之间的关系。虽然这些方法对于探索那些构造的独立信号之间的线性相关性(即，总分或fMRI信号)是有用的，但这些经典框架在提供关于可观察变化背后的系统级动态的洞察方面存在不足。隐藏马尔可夫模型(HMM)是一种统计模型，使我们能够描述多个可观测构造之间的顺序关系，并且当通过有限状态自动机(FSA)的视角应用时，可以提供一个更综合和直观的框架来建模和理解。

    arXiv:2403.03414v1 Announce Type: new  Abstract: Traditional approaches in mental health research apply General Linear Models (GLM) to describe the longitudinal dynamics of observed psycho-behavioral measurements (questionnaire summary scores). Similarly, GLMs are also applied to characterize relationships between neurobiological measurements (regional fMRI signals) and perceptual stimuli or other regional signals. While these methods are useful for exploring linear correlations among the isolated signals of those constructs (i.e., summary scores or fMRI signals), these classical frameworks fall short in providing insights into the comprehensive system-level dynamics underlying observable changes. Hidden Markov Models (HMM) are a statistical model that enable us to describe the sequential relations among multiple observable constructs, and when applied through the lens of Finite State Automata (FSA), can provide a more integrated and intuitive framework for modeling and understanding t
    
[^99]: 通过数据净化和动态激活函数设计推动异常样本检测的进步

    Advancing Out-of-Distribution Detection through Data Purification and Dynamic Activation Function Design

    [https://arxiv.org/abs/2403.03412](https://arxiv.org/abs/2403.03412)

    通过引入OOD-R数据集合和噪声过滤技术，我们的工作在神经网络中增强异常样本的检测和管理，以提高模型的健壮性和可靠性。

    

    在机器学习和深度学习的动态领域中，模型的健壮性和可靠性至关重要，尤其是在关键的现实世界应用中。在这个领域中的一个基本挑战是管理异常样本，这大大增加了模型误分类和不确定性的风险。我们的工作通过增强神经网络中异常样本的检测和管理来应对这一挑战。我们引入了OOD-R（异常样本矫正），这是一个经过精心筛选的具有增强噪声减少性能的开源数据集合。现有OOD数据集中的内部分布（ID）噪声可能导致检测算法的准确评估。基于这一认识，OOD-R采用噪声过滤技术来改进数据集，确保对OOD检测算法进行更准确和可靠的评估。这种方法不仅提高了数据的整体质量，还有助于提高

    arXiv:2403.03412v1 Announce Type: new  Abstract: In the dynamic realms of machine learning and deep learning, the robustness and reliability of models are paramount, especially in critical real-world applications. A fundamental challenge in this sphere is managing Out-of-Distribution (OOD) samples, significantly increasing the risks of model misclassification and uncertainty. Our work addresses this challenge by enhancing the detection and management of OOD samples in neural networks. We introduce OOD-R (Out-of-Distribution-Rectified), a meticulously curated collection of open-source datasets with enhanced noise reduction properties. In-Distribution (ID) noise in existing OOD datasets can lead to inaccurate evaluation of detection algorithms. Recognizing this, OOD-R incorporates noise filtering technologies to refine the datasets, ensuring a more accurate and reliable evaluation of OOD detection algorithms. This approach not only improves the overall quality of data but also aids in be
    
[^100]: 使用LSTM、SVM和多项式回归预测加密货币价格

    Prediction Of Cryptocurrency Prices Using LSTM, SVM And Polynomial Regression

    [https://arxiv.org/abs/2403.03410](https://arxiv.org/abs/2403.03410)

    该论文使用LSTM、SVM和多项式回归算法对加密货币价格进行预测，通过性能比较来确定最佳算法模型

    

    arXiv:2403.03410v1 公告类型: 新的 摘要: 信息技术的快速发展，特别是互联网，为用户提供了一种快速简便的获取信息的方式。随着互联网服务带来的便利，许多最初投资于黄金和贵金属的个人现在正在转向数字货币的数字投资。然而，投资加密货币充满了不确定性，并且每天都存在波动。这种风险给拥有硬币的投资者带来了重大挑战，可能导致巨额投资损失。这些加密货币的价值不确定性是硬币投资领域的一个重要问题。预测是用于预测这些加密货币未来价值的方法之一。通过利用长短期记忆模型、支持向量机和多项式回归算法进行预测，进行绩效比较以确定哪种算法模型最为有效

    arXiv:2403.03410v1 Announce Type: new  Abstract: The rapid development of information technology, especially the Internet, has facilitated users with a quick and easy way to seek information. With these convenience offered by internet services, many individuals who initially invested in gold and precious metals are now shifting into digital investments in form of cryptocurrencies. However, investments in crypto coins are filled with uncertainties and fluctuation in daily basis. This risk posed as significant challenges for coin investors that could result in substantial investment losses. The uncertainty of the value of these crypto coins is a critical issue in the field of coin investment. Forecasting, is one of the methods used to predict the future value of these crypto coins. By utilizing the models of Long Short Term Memory, Support Vector Machine, and Polynomial Regression algorithm for forecasting, a performance comparison is conducted to determine which algorithm model is most 
    
[^101]: 一种用于作物生长模型的EnKF-LSTM同化算法

    An EnKF-LSTM Assimilation Algorithm for Crop Growth Model

    [https://arxiv.org/abs/2403.03406](https://arxiv.org/abs/2403.03406)

    本文提出了一种EnKF-LSTM数据同化方法，通过结合集合卡尔曼滤波器和LSTM神经网络，提高了作物生长预测的准确性。

    

    准确及时地预测作物生长对于确保作物产量具有重要意义，研究人员开发了几种用于预测作物生长的模型。然而，作物模型得到的模拟结果与实际结果之间存在较大差异，因此本文提出将模拟结果与采集的作物数据结合进行数据同化，从而提高预测的准确性。本文提出了一种EnKF-LSTM数据同化方法，通过结合集合卡尔曼滤波器和LSTM神经网络，有效避免了现有数据同化方法的过度拟合问题，并消除了测量数据的不确定性。利用传感器设备收集的数据集对所提出的EnKF-LSTM方法进行了验证，并与其他数据同化方法进行了比较。

    arXiv:2403.03406v1 Announce Type: new  Abstract: Accurate and timely prediction of crop growth is of great significance to ensure crop yields and researchers have developed several crop models for the prediction of crop growth. However, there are large difference between the simulation results obtained by the crop models and the actual results, thus in this paper, we proposed to combine the simulation results with the collected crop data for data assimilation so that the accuracy of prediction will be improved. In this paper, an EnKF-LSTM data assimilation method for various crops is proposed by combining ensemble Kalman filter and LSTM neural network, which effectively avoids the overfitting problem of existing data assimilation methods and eliminates the uncertainty of the measured data. The verification of the proposed EnKF-LSTM method and the comparison of the proposed method with other data assimilation methods were performed using datasets collected by sensor equipment deployed o
    
[^102]: BAIT: 交互定理证明嵌入架构的基准测试

    BAIT: Benchmarking (Embedding) Architectures for Interactive Theorem-Proving

    [https://arxiv.org/abs/2403.03401](https://arxiv.org/abs/2403.03401)

    BAIT框架提供了公平且简化的ITP学习方法比较，发现Structure Aware Transformers在公式嵌入问题上表现出色。

    

    人工智能定理证明催生了大量基准测试和方法论，尤其是在交互式定理证明领域。该领域的研究分散，采用多种方法散布在几个交互式定理证明系统中。这给方法的比较带来了重大挑战，因为这些方法通常复杂且难以复制。为了解决这一问题，我们提出了BAIT，一个用于公平和简化比较ITP学习方法的框架。我们通过全面比较展示了BAIT的能力，涵盖了应用于公式嵌入问题的最先进架构在多个ITP基准测试中的表现。我们发现结构感知变压器表现特别出色，改进了与原问题集相关的技术。BAIT还允许我们评估建立在交互环境中的系统的端到端证明性能。这种统一的视角揭示

    arXiv:2403.03401v1 Announce Type: new  Abstract: Artificial Intelligence for Theorem Proving has given rise to a plethora of benchmarks and methodologies, particularly in Interactive Theorem Proving (ITP). Research in the area is fragmented, with a diverse set of approaches being spread across several ITP systems. This presents a significant challenge to the comparison of methods, which are often complex and difficult to replicate. Addressing this, we present BAIT, a framework for fair and streamlined comparison of learning approaches in ITP. We demonstrate BAIT's capabilities with an in-depth comparison, across several ITP benchmarks, of state-of-the-art architectures applied to the problem of formula embedding. We find that Structure Aware Transformers perform particularly well, improving on techniques associated with the original problem sets. BAIT also allows us to assess the end-to-end proving performance of systems built on interactive environments. This unified perspective revea
    
[^103]: CoRMF: 临界有序循环均场伊辛求解器

    CoRMF: Criticality-Ordered Recurrent Mean Field Ising Solver

    [https://arxiv.org/abs/2403.03391](https://arxiv.org/abs/2403.03391)

    CoRMF是一种基于RNN的高效伊辛模型求解器，利用关键有序自旋序列和循环神经网络来实现变分均场和 RNN 之间的统一，从而实现了对通常难以处理的伊辛模型的高效探索。

    

    我们提出了一种基于RNN的高效伊辛模型求解器，称为Criticality-ordered Recurrent Mean Field (CoRMF)，用于前向伊辛问题。在其核心部分，通过贪婪算法对N个自旋的伊辛模型进行了关键有序自旋序列的引入，从而可以利用自回归均场因子分解，并通过循环神经网络(RNNs)进行优化。我们的方法具有两个显著特点：(i)通过利用底层伊辛图的近似树结构，新获得的关键性顺序使变分均场和RNN之间得以统一，从而允许有效地利用概率推断来探究通常难以处理的伊辛模型;(ii)它具有良好的模块化、独立于模型而又足够表达能力，因此可以完全适用于任何前向伊辛推理问题，而且工作量极小。计算上，通过使用一种方差减少

    arXiv:2403.03391v1 Announce Type: cross  Abstract: We propose an RNN-based efficient Ising model solver, the Criticality-ordered Recurrent Mean Field (CoRMF), for forward Ising problems. In its core, a criticality-ordered spin sequence of an $N$-spin Ising model is introduced by sorting mission-critical edges with greedy algorithm, such that an autoregressive mean-field factorization can be utilized and optimized with Recurrent Neural Networks (RNNs). Our method has two notable characteristics: (i) by leveraging the approximated tree structure of the underlying Ising graph, the newly-obtained criticality order enables the unification between variational mean-field and RNN, allowing the generally intractable Ising model to be efficiently probed with probabilistic inference; (ii) it is well-modulized, model-independent while at the same time expressive enough, and hence fully applicable to any forward Ising inference problems with minimal effort. Computationally, by using a variance-redu
    
[^104]: 半监督学习框架在多类杂草检测中的性能评估

    Performance Evaluation of Semi-supervised Learning Frameworks for Multi-Class Weed Detection

    [https://arxiv.org/abs/2403.03390](https://arxiv.org/abs/2403.03390)

    半监督学习方法在多类杂草检测中展现出很好的性能，为解决传统监督学习方法中需要大量手工标注数据集的缺点提供了可持续替代方案。

    

    有效的杂草控制在优化农作物产量和提高农产品质量中起着至关重要的作用。然而，对除草剂的依赖不仅对环境构成严重威胁，还会促使抗草草出现。近年来，ML和DL所带来的精准除草管理的进展为此提供了可持续的替代方案。尽管取得了巨大进展，但现有算法主要基于监督学习方法，通常需要具有手工标注的大规模数据集，这是耗时且劳动密集的。因此，在广泛的计算机视觉领域中，尤其是半监督学习方法，已经引起了人们的广泛关注，并展现出了很好的性能。这些方法旨在利用少量标记数据样本以及大量未标记样本来开发高性能的模型。

    arXiv:2403.03390v1 Announce Type: cross  Abstract: Effective weed control plays a crucial role in optimizing crop yield and enhancing agricultural product quality. However, the reliance on herbicide application not only poses a critical threat to the environment but also promotes the emergence of resistant weeds. Fortunately, recent advances in precision weed management enabled by ML and DL provide a sustainable alternative. Despite great progress, existing algorithms are mainly developed based on supervised learning approaches, which typically demand large-scale datasets with manual-labeled annotations, which is time-consuming and labor-intensive. As such, label-efficient learning methods, especially semi-supervised learning, have gained increased attention in the broader domain of computer vision and have demonstrated promising performance. These methods aim to utilize a small number of labeled data samples along with a great number of unlabeled samples to develop high-performing mod
    
[^105]: 复杂性至关重要：在存在虚假相关性的情况下特征学习的动态

    Complexity Matters: Dynamics of Feature Learning in the Presence of Spurious Correlations

    [https://arxiv.org/abs/2403.03375](https://arxiv.org/abs/2403.03375)

    在本文中，通过提出一个基于布尔函数分析的合成数据集，研究了在虚假相关性条件下特征学习动态，发现了虚假相关性或虚假特征的强度会影响核心特征学习速度，虚假特征和核心特征的学习阶段不总是可分开，并且虚假特征即使在一段时间后也不会被遗忘。

    

    现有研究经常将虚假特征定义为在神经网络优化中“更容易”学习的内容，但它们相对简单性的影响仍未被充分探讨。此外，他们主要关注的是最终性能，而不是特征学习的学习动态。在本文中，我们提出了一个理论框架和相关的基于布尔函数分析的合成数据集，允许对虚假特征的相对复杂性（与核心特征相比）和相关性强度（相对于标签）进行细致控制，以研究在虚假相关性下特征学习的动态。我们的设置揭示了几个有趣的现象：（1）更强的虚假相关性或更简单的虚假特征会减慢核心特征的学习速度，（2）虚假特征和核心特征的学习阶段并非总是可以被分开，（3）虚假特征即使在一段时间之后也不会被遗忘。

    arXiv:2403.03375v1 Announce Type: new  Abstract: Existing research often posits spurious features as "easier" to learn than core features in neural network optimization, but the impact of their relative simplicity remains under-explored. Moreover they mainly focus on the end performance intead of the learning dynamics of feature learning. In this paper, we propose a theoretical framework and associated synthetic dataset grounded in boolean function analysis which allows for fine-grained control on the relative complexity (compared to core features) and correlation strength (with respect to the label) of spurious features to study the dynamics of feature learning under spurious correlation. Our setup uncovers several interesting phenomenon: (1) stronger spurious correlations or simpler spurious features slow down the rate of learning for the core features, (2) learning phases of spurious features and core features are not always separable, (3) spurious features are not forgotten even af
    
[^106]: TartanAviation：用于终端领域空域作业的图像、语音和ADS-B轨迹数据集

    TartanAviation: Image, Speech, and ADS-B Trajectory Datasets for Terminal Airspace Operations

    [https://arxiv.org/abs/2403.03372](https://arxiv.org/abs/2403.03372)

    这个论文介绍了TartanAviation数据集，包含图像、语音和ADS-B轨迹数据，对机场环境进行了全面观察，为AI和机器学习应用提供了重要资源

    

    我们介绍了TartanAviation，一个专注于终端区域空域作业的开源多模态数据集。TartanAviation通过在机场边界内安装设备同时收集图像、语音和ADS-B轨迹数据，提供了对机场环境的整体视图。这些数据集在多个月内收集于有塔楼和无塔楼的机场，以捕捉飞机作业、季节、飞机类型和天气条件的多样性。总共，TartanAviation提供了310万张图像、3374小时的空中交通管制语音数据以及661天的ADS-B轨迹数据。数据经过过滤、处理和验证，创建了一个精心策划的数据集。除了数据集外，我们还开源了用于收集和预处理数据集的代码库，进一步增强了可访问性和可用性。我们相信这个数据集有许多潜在的用途，特别是在允许AI和m的情况下会非常重要

    arXiv:2403.03372v1 Announce Type: new  Abstract: We introduce TartanAviation, an open-source multi-modal dataset focused on terminal-area airspace operations. TartanAviation provides a holistic view of the airport environment by concurrently collecting image, speech, and ADS-B trajectory data using setups installed inside airport boundaries. The datasets were collected at both towered and non-towered airfields across multiple months to capture diversity in aircraft operations, seasons, aircraft types, and weather conditions. In total, TartanAviation provides 3.1M images, 3374 hours of Air Traffic Control speech data, and 661 days of ADS-B trajectory data. The data was filtered, processed, and validated to create a curated dataset. In addition to the dataset, we also open-source the code-base used to collect and pre-process the dataset, further enhancing accessibility and usability. We believe this dataset has many potential use cases and would be particularly vital in allowing AI and m
    
[^107]: 利用联邦学习进行自动检测氯吡格雷治疗失败

    Leveraging Federated Learning for Automatic Detection of Clopidogrel Treatment Failures

    [https://arxiv.org/abs/2403.03368](https://arxiv.org/abs/2403.03368)

    本研究利用联邦学习策略处理氯吡格雷治疗失败检测，结果显示联邦学习方法可以显著缩小性能差距，强调了其在该领域的潜力。

    

    氯吡格雷是一种广泛使用的抗血小板药物，其有效性在个体之间存在显著差异，必须开发精确的预测模型以优化患者护理。在这项研究中，我们利用联邦学习策略来处理氯吡格雷治疗失败的检测。我们的研究利用多个医疗机构的协作力量，使它们能够共同训练机器学习模型，同时保护敏感的患者数据。利用涵盖广泛且多样化人口的英国生物银行数据集，我们基于地理中心对数据进行分区，并评估了联邦学习的性能。我们的结果表明，虽然集中式训练可以获得更高的曲线下面积（AUC）值和更快的收敛速度，但联邦学习方法可以显著缩小这种性能差距。我们的发现强调了联邦学习在应对氯吡格雷治疗失败问题中的潜力。

    arXiv:2403.03368v1 Announce Type: new  Abstract: The effectiveness of clopidogrel, a widely used antiplatelet medication, varies significantly among individuals, necessitating the development of precise predictive models to optimize patient care. In this study, we leverage federated learning strategies to address clopidogrel treatment failure detection. Our research harnesses the collaborative power of multiple healthcare institutions, allowing them to jointly train machine learning models while safeguarding sensitive patient data. Utilizing the UK Biobank dataset, which encompasses a vast and diverse population, we partitioned the data based on geographic centers and evaluated the performance of federated learning. Our results show that while centralized training achieves higher Area Under the Curve (AUC) values and faster convergence, federated learning approaches can substantially narrow this performance gap. Our findings underscore the potential of federated learning in addressing 
    
[^108]: 水平集传输：优化的视角

    Level Set Teleportation: An Optimization Perspective

    [https://arxiv.org/abs/2403.03362](https://arxiv.org/abs/2403.03362)

    该论文从优化的角度研究了水平集传输，证明了在某些条件下水平集传输可以加速梯度方法的收敛速度，且提出了一种只需要Hessian-vector products的方法验证了该技术在实践中的有效性。

    

    我们研究水平集传输，这是一种优化子程序，旨在通过在目标函数的水平集上最大化梯度范数来加速梯度方法。由于下降引理暗示梯度下降（GD）使目标函数按梯度的平方范数下降，水平集传输最大化了这一步进保证。对于满足Hessian稳定性的凸函数，我们证明了GD与水平集传输获得了综合的次线性/线性收敛速度，这比在最优性差距较小时标准GD要快得多。与标准（强）凸设置形成鲜明对比的是，我们展示了水平集传输既不改善也不恶化收敛速度。为了实际评估传输，我们开发了一种只需要Hessian-向量乘积的投影梯度类型方法。我们使用这种方法显示，如果提供了梯度访问权限，水平集传输梯度方法具有更快的收敛速度。

    arXiv:2403.03362v1 Announce Type: new  Abstract: We study level set teleportation, an optimization sub-routine which seeks to accelerate gradient methods by maximizing the gradient norm on a level-set of the objective function. Since the descent lemma implies that gradient descent (GD) decreases the objective proportional to the squared norm of the gradient, level-set teleportation maximizes this one-step progress guarantee. For convex functions satisfying Hessian stability, we prove that GD with level-set teleportation obtains a combined sub-linear/linear convergence rate which is strictly faster than standard GD when the optimality gap is small. This is in sharp contrast to the standard (strongly) convex setting, where we show level-set teleportation neither improves nor worsens convergence rates. To evaluate teleportation in practice, we develop a projected-gradient-type method requiring only Hessian-vector products. We use this method to show that gradient methods with access to a 
    
[^109]: 链式信息论界限和线性赌博问题的严格遗憾率

    Chained Information-Theoretic bounds and Tight Regret Rate for Linear Bandit Problems

    [https://arxiv.org/abs/2403.03361](https://arxiv.org/abs/2403.03361)

    本文研究了线性赌博问题中Thompson-Sampling算法变体的贝叶斯遗憾，通过使用链接论证建立了具有度量动作空间的新界限，为$d$维线性赌博问题提供了$O(d\sqrt{T})$的严格率。

    

    本文研究了一种Thompson-Sampling算法变体在赌博问题中的贝叶斯遗憾。它基于[Russo and Van Roy，2015]的信息论框架，更具体地，基于[Dong and Van Roy，2020]的率-失真分析，在那里他们证明了$d$维线性赌博设置的遗憾率为$O(d\sqrt{T \log(T)})$的界限。我们专注于具有度量动作空间的赌博问题，并使用链接论证建立了依赖于动作空间度量熵的新界限，针对Thompson-Sampling的一个变体。在奖励的适当连续性假设下，我们的界限为$d$维线性赌博问题提供了$O(d\sqrt{T})$的严格率。

    arXiv:2403.03361v1 Announce Type: cross  Abstract: This paper studies the Bayesian regret of a variant of the Thompson-Sampling algorithm for bandit problems. It builds upon the information-theoretic framework of [Russo and Van Roy, 2015] and, more specifically, on the rate-distortion analysis from [Dong and Van Roy, 2020], where they proved a bound with regret rate of $O(d\sqrt{T \log(T)})$ for the $d$-dimensional linear bandit setting. We focus on bandit problems with a metric action space and, using a chaining argument, we establish new bounds that depend on the metric entropy of the action space for a variant of Thompson-Sampling.   Under suitable continuity assumption of the rewards, our bound offers a tight rate of $O(d\sqrt{T})$ for $d$-dimensional linear bandit problems.
    
[^110]: RACE-SM:基于强化学习的社交式匝道合流自主控制

    RACE-SM: Reinforcement Learning Based Autonomous Control for Social On-Ramp Merging

    [https://arxiv.org/abs/2403.03359](https://arxiv.org/abs/2403.03359)

    该论文提出了一种基于强化学习的自主控制模型，专注于并行式匝道合流，考虑了道路上其他车辆的影响，并提出了新颖的激励函数。

    

    自主并行式匝道合流在人控车辆交通中仍然是自主车辆控制中存在的问题。现有非学习型车辆控制解决方案主要依赖规则和优化，但这些方法往往面临重大挑战。最近深度强化学习的进展展现了希望，并受到了重要学术关注，然而现有的基于学习的方法对其他高速公路车辆关注不足，且经常依赖不准确的道路交通假设。此外，并行式情况很少被考虑。提出了一种新颖的学习模型，用于加速和变道决策制定，该模型明确考虑了对于车辆本身及其周围车辆（可能合作或不合作）的效用，以产生符合社会规范的行为。这种新颖的奖励函数利用社交

    arXiv:2403.03359v1 Announce Type: new  Abstract: Autonomous parallel-style on-ramp merging in human controlled traffic continues to be an existing issue for autonomous vehicle control. Existing non-learning based solutions for vehicle control rely on rules and optimization primarily. These methods have been seen to present significant challenges. Recent advancements in Deep Reinforcement Learning have shown promise and have received significant academic interest however the available learning based approaches show inadequate attention to other highway vehicles and often rely on inaccurate road traffic assumptions. In addition, the parallel-style case is rarely considered. A novel learning based model for acceleration and lane change decision making that explicitly considers the utility to both the ego vehicle and its surrounding vehicles which may be cooperative or uncooperative to produce behaviour that is socially acceptable is proposed. The novel reward function makes use of Social 
    
[^111]: 深度学习的假设空间

    Hypothesis Spaces for Deep Learning

    [https://arxiv.org/abs/2403.03353](https://arxiv.org/abs/2403.03353)

    本文介绍了一种应用深度神经网络的深度学习假设空间，并构建了一个再生核Banach空间，研究了正则化学习和最小插值问题，证明了学习模型的解可以表示为线性组合。

    

    本文介绍了一种应用深度神经网络（DNNs）的深度学习假设空间。通过将DNN视为两个变量的函数，即物理变量和参数变量，我们考虑了DNNs的原始集合，参数变量位于由DNNs的权重矩阵和偏置决定的一组深度和宽度中。然后在弱*拓扑中完成原始DNN集合的线性跨度，以构建一个物理变量函数的Banach空间。我们证明所构造的Banach空间是一个再生核Banach空间（RKBS），并构造其再生核。通过为学习模型的解建立表达定理，我们研究了两个学习模型，正则化学习和最小插值问题在结果RKBS中。表达定理揭示了这些学习模型的解可以表示为线性组合

    arXiv:2403.03353v1 Announce Type: cross  Abstract: This paper introduces a hypothesis space for deep learning that employs deep neural networks (DNNs). By treating a DNN as a function of two variables, the physical variable and parameter variable, we consider the primitive set of the DNNs for the parameter variable located in a set of the weight matrices and biases determined by a prescribed depth and widths of the DNNs. We then complete the linear span of the primitive DNN set in a weak* topology to construct a Banach space of functions of the physical variable. We prove that the Banach space so constructed is a reproducing kernel Banach space (RKBS) and construct its reproducing kernel. We investigate two learning models, regularized learning and minimum interpolation problem in the resulting RKBS, by establishing representer theorems for solutions of the learning models. The representer theorems unfold that solutions of these learning models can be expressed as linear combination of
    
[^112]: Solution Simplex Clustering for Heterogeneous Federated Learning

    Solution Simplex Clustering for Heterogeneous Federated Learning

    [https://arxiv.org/abs/2403.03333](https://arxiv.org/abs/2403.03333)

    提出了Solution Simplex Clustered Federated Learning（SosicFL），通过学习解决方案单纯形的思想，为每个客户端分配单一区域，从而同时实现了学习本地和全局模型的目标。

    

    我们针对联邦学习（FL）中的一个主要挑战提出了解决方案，即在高度异构的客户分布下实现良好的性能。这种困难部分源于两个看似矛盾的目标：通过聚合来自客户端的信息来学习一个通用模型，以及学习应适应每个本地分布的本地个性化模型。在这项工作中，我们提出了Solution Simplex Clustered Federated Learning（SosicFL）来消除这种矛盾。基于学习解决方案单纯形的最新思想，SosicFL为每个客户端分配一个单纯形中的子区域，并执行FL来学习一个通用解决方案单纯形。这使得客户端模型在解决方案单纯形的自由度范围内具有其特征，同时实现了学习一个全局通用模型的目标。我们的实验证明，SosicFL改善了性能，并加速了全局和训练过程。

    arXiv:2403.03333v1 Announce Type: new  Abstract: We tackle a major challenge in federated learning (FL) -- achieving good performance under highly heterogeneous client distributions. The difficulty partially arises from two seemingly contradictory goals: learning a common model by aggregating the information from clients, and learning local personalized models that should be adapted to each local distribution. In this work, we propose Solution Simplex Clustered Federated Learning (SosicFL) for dissolving such contradiction. Based on the recent ideas of learning solution simplices, SosicFL assigns a subregion in a simplex to each client, and performs FL to learn a common solution simplex. This allows the client models to possess their characteristics within the degrees of freedom in the solution simplex, and at the same time achieves the goal of learning a global common model. Our experiments show that SosicFL improves the performance and accelerates the training process for global and 
    
[^113]: 一个用于可解释地理空间机器学习模型的集成框架

    An Ensemble Framework for Explainable Geospatial Machine Learning Models

    [https://arxiv.org/abs/2403.03328](https://arxiv.org/abs/2403.03328)

    介绍了一个集成框架，结合了本地空间加权方案、可解释人工智能（XAI）和尖端机器学习技术，以提高地理空间机器学习模型的可解释性和准确性。

    

    分析空间变化效应在地理分析中至关重要。然而，由于地理数据的复杂性和非线性，准确捕捉和解释这种变异性是具有挑战性的。在这里，我们介绍了一个集成框架，结合了本地空间加权方案、可解释人工智能（XAI）和尖端机器学习技术，以弥合传统地理分析模型和通用机器学习方法之间的差距。通过对合成数据集的测试，验证了该框架通过阐明空间变异性，提高了地理回归和分类预测的可解释性和准确性。它显著提高了预测精度，提供了一种理解空间现象的新方法。

    arXiv:2403.03328v1 Announce Type: new  Abstract: Analyzing spatial varying effect is pivotal in geographic analysis. Yet, accurately capturing and interpreting this variability is challenging due to the complexity and non-linearity of geospatial data. Herein, we introduce an integrated framework that merges local spatial weighting scheme, Explainable Artificial Intelligence (XAI), and cutting-edge machine learning technologies to bridge the gap between traditional geographic analysis models and general machine learning approaches. Through tests on synthetic datasets, this framework is verified to enhance the interpretability and accuracy of predictions in both geographic regression and classification by elucidating spatial variability. It significantly boosts prediction precision, offering a novel approach to understanding spatial phenomena.
    
[^114]: 深度配置性能学习：一项系统性调查与分类

    Deep Configuration Performance Learning: A Systematic Survey and Taxonomy

    [https://arxiv.org/abs/2403.03322](https://arxiv.org/abs/2403.03322)

    性能是可配置软件系统行为的关键属性，本文针对深度学习在可配置软件性能学习方面进行了全面的调查与分类研究。

    

    性能可以说是反映可配置软件系统行为的最关键属性。然而，随着现代软件规模和复杂性不断增加，对各种配置如何影响性能进行建模和预测成为软件维护中的主要挑战之一。因此，性能通常是在没有对软件系统有透彻了解的情况下建模的，主要依赖数据，这正好符合深度学习的目的。在这篇论文中，我们专注于深度学习在可配置软件性能学习方面进行了全面的回顾，涵盖了948篇来自六个索引服务的论文，基于此提取并分析了85篇主要论文。我们的结果总结了配置数据如何准备，深度配置性能学习模型如何构建，以及该模型如何进行评估等关键主题和统计信息。

    arXiv:2403.03322v1 Announce Type: cross  Abstract: Performance is arguably the most crucial attribute that reflects the behavior of a configurable software system. However, given the increasing scale and complexity of modern software, modeling and predicting how various configurations can impact performance becomes one of the major challenges in software maintenance. As such, performance is often modeled without having a thorough knowledge of the software system, but relying mainly on data, which fits precisely with the purpose of deep learning.   In this paper, we conduct a comprehensive review exclusively on the topic of deep learning for performance learning of configurable software, covering 948 searched papers spanning six indexing services, based on which 85 primary papers were extracted and analyzed. Our results summarize the key topics and statistics on how the configuration data is prepared; how the deep configuration performance learning model is built; how the model is evalu
    
[^115]: 基于学习策略的多智能体系统碰撞避免验证

    Collision Avoidance Verification of Multiagent Systems with Learned Policies

    [https://arxiv.org/abs/2403.03314](https://arxiv.org/abs/2403.03314)

    该论文提出了一种基于向后可达性的方法，用于验证多智能体神经反馈环路的碰撞避免属性，通过解决一系列混合整数线性规划计算相对反投影集，并且该逐对方法可并行化，随着智能体数量的增加能够很好地扩展

    

    对于许多多智能体控制问题，神经网络（NN）已经实现了有前途的新能力。然而，许多这些系统缺乏正式的保证（例如，碰撞避免、鲁棒性），这阻止了在安全关键环境中利用这些进展。鉴于现有技术不能处理多于一个智能体的情况，为了填补这一研究空白，本论文提出了一种用于验证多智能体神经反馈环路（MA-NFLs）的碰撞避免属性的基于向后可达性的方法。通过给定每个智能体的动力学模型和训练控制策略，所提出的算法通过离线为每对智能体解决一系列混合整数线性规划（MILPs）来计算相对反投影集。我们的逐对方法可并行化，从而可以很好地随着智能体数量的增加而扩展，并且我们考虑了状态。

    arXiv:2403.03314v1 Announce Type: cross  Abstract: For many multiagent control problems, neural networks (NNs) have enabled promising new capabilities. However, many of these systems lack formal guarantees (e.g., collision avoidance, robustness), which prevents leveraging these advances in safety-critical settings. While there is recent work on formal verification of NN-controlled systems, most existing techniques cannot handle scenarios with more than one agent. To address this research gap, this paper presents a backward reachability-based approach for verifying the collision avoidance properties of Multi-Agent Neural Feedback Loops (MA-NFLs). Given the dynamics models and trained control policies of each agent, the proposed algorithm computes relative backprojection sets by solving a series of Mixed Integer Linear Programs (MILPs) offline for each pair of agents. Our pair-wise approach is parallelizable and thus scales well with increasing number of agents, and we account for state 
    
[^116]: 图学习用于量子近似优化算法参数预测

    Graph Learning for Parameter Prediction of Quantum Approximate Optimization Algorithm

    [https://arxiv.org/abs/2403.03310](https://arxiv.org/abs/2403.03310)

    本研究利用图神经网络为量子近似优化算法提供参数预测，实现了在减少量子计算资源开销的同时增强算法效果。

    

    近年来，量子计算已经成为组合优化领域中的一股变革力量，提供了处理长期以来挑战经典计算方法的复杂问题的新方法。其中，量子近似优化算法（QAOA）以其潜力在高效解决Max-Cut问题上脱颖而出。然而，由于目前量子计算资源的限制，实际应用面临挑战。我们的工作优化了QAOA初始化，使用图神经网络（GNN）作为热启动技术。这牺牲了在经典计算机上的可负担的计算资源，以减少量子计算资源开销，增强了QAOA的效果。对各种GNN架构进行的实验表明了我们框架的适应性和稳定性，凸显了量子算法和GNN之间的协同作用。

    arXiv:2403.03310v1 Announce Type: cross  Abstract: In recent years, quantum computing has emerged as a transformative force in the field of combinatorial optimization, offering novel approaches to tackling complex problems that have long challenged classical computational methods. Among these, the Quantum Approximate Optimization Algorithm (QAOA) stands out for its potential to efficiently solve the Max-Cut problem, a quintessential example of combinatorial optimization. However, practical application faces challenges due to current limitations on quantum computational resource. Our work optimizes QAOA initialization, using Graph Neural Networks (GNN) as a warm-start technique. This sacrifices affordable computational resource on classical computer to reduce quantum computational resource overhead, enhancing QAOA's effectiveness. Experiments with various GNN architectures demonstrate the adaptability and stability of our framework, highlighting the synergy between quantum algorithms an
    
[^117]: 所需只是 Mad Libs: 增强跨领域文档级事件参数数据

    Mad Libs Are All You Need: Augmenting Cross-Domain Document-Level Event Argument Data

    [https://arxiv.org/abs/2403.03304](https://arxiv.org/abs/2403.03304)

    Mad Libs 提供的数据增强方法 MLA 在跨领域文档级事件参数数据提取上取得了显著的性能提升，平均提高了 2.6 个 F1 分数。同时，在零和少样本事件角色方面相比无增强基线，提高了 3.9 和 5.2 个百分点。

    

    文档级事件参数提取（DocEAE）是一个极其困难的信息提取问题，尤其在低资源跨领域设置中存在着显著的局限性。为了解决这个问题，我们引入了 Mad Lib Aug（MLA），这是一个新颖的生成式 DocEAE 数据增强框架。我们的方法利用了 Mad Libs 的直觉，即作为一种热门游戏中使用的分类掩码文档可以被 LLMs 生成并解答，从而为 DocEAE 生成数据。使用 MLA，我们的整体 F1 分数平均改进了 2.6 个百分点。此外，与无增强基线相比，该方法在零和少样本事件角色方面在所有实验中平均提高了 3.9 和 5.2 个百分点。

    arXiv:2403.03304v1 Announce Type: new  Abstract: Document-Level Event Argument Extraction (DocEAE) is an extremely difficult information extraction problem -- with significant limitations in low-resource cross-domain settings. To address this problem, we introduce Mad Lib Aug (MLA), a novel generative DocEAE data augmentation framework. Our approach leverages the intuition that Mad Libs, which are categorically masked documents used as a part of a popular game, can be generated and solved by LLMs to produce data for DocEAE. Using MLA, we achieve a 2.6-point average improvement in overall F1 score. Moreover, this approach achieves a 3.9 and 5.2 point average increase in zero and few-shot event roles compared to augmentation-free baselines across all experiments.   To better facilitate analysis of cross-domain DocEAE, we additionally introduce a new metric, Role-Depth F1 (RDF1), which uses statistical depth to identify roles in the target domain which are semantic outliers with respect t
    
[^118]: 正确与不正确的量子PAC学习

    Proper vs Improper Quantum PAC learning

    [https://arxiv.org/abs/2403.03295](https://arxiv.org/abs/2403.03295)

    量子Coupon Collector问题的发现展示了正确与不正确学习在样本复杂度方面的差异，为量子PAC学习提供了新的见解。

    

    学习的PAC模型中的一个基本问题是正确学习是否比不正确学习更困难。在经典情况下，存在着VC维度为$d$的概念类别，对于错误率为$\epsilon$的正确学习而言，它们的样本复杂度是$\Omega\left(\frac d\epsilon\log\frac1\epsilon\right)$，而对于不正确学习则是O$\!\left(\frac d\epsilon\right)$。其中一个例子涉及到了优惠券收集问题。受量子样本的正确与不正确学习效率启发，Arunachalam、Belovs、Childs、Kothari、Rosmanis和de Wolf (TQC 2020)研究了一个类似的问题，即量子优惠券收集问题。令人好奇的是，他们发现对于学习大小为$k$的$[n]$的子集，该问题的样本复杂度为$\Theta(k\log\min\{k,n-k+1\})$，与优惠券收集问题的$\Theta(k\log k)$复杂度形成对比。这事实上否定了可能存在的

    arXiv:2403.03295v1 Announce Type: cross  Abstract: A basic question in the PAC model of learning is whether proper learning is harder than improper learning. In the classical case, there are examples of concept classes with VC dimension $d$ that have sample complexity $\Omega\left(\frac d\epsilon\log\frac1\epsilon\right)$ for proper learning with error $\epsilon$, while the complexity for improper learning is O$\!\left(\frac d\epsilon\right)$. One such example arises from the Coupon Collector problem.   Motivated by the efficiency of proper versus improper learning with quantum samples, Arunachalam, Belovs, Childs, Kothari, Rosmanis, and de Wolf (TQC 2020) studied an analogue, the Quantum Coupon Collector problem. Curiously, they discovered that for learning size $k$ subsets of $[n]$ the problem has sample complexity $\Theta(k\log\min\{k,n-k+1\})$, in contrast with the complexity of $\Theta(k\log k)$ for Coupon Collector. This effectively negates the possibility of a separation between
    
[^119]: 用于异构数据下的分散式学习的平均化速率调度器

    Averaging Rate Scheduler for Decentralized Learning on Heterogeneous Data

    [https://arxiv.org/abs/2403.03292](https://arxiv.org/abs/2403.03292)

    提出了一种平均化速率调度的方法来减少分散式学习中异构性的影响，并通过实验证明其相比于传统方法的优越性（提高了约3%测试准确性）。

    

    最先进的分散式学习算法通常要求数据分布是独立同分布（IID）的。然而，在实际场景中，代理之间的数据分布可能存在显著的异构性。在这项工作中，我们提出了平均化速率调度作为一种简单而有效的方法，以减少分散式学习中异构性的影响。我们的实验表明，与采用恒定平均化速率的传统方法相比，所提出的方法在测试准确性上表现优越（提高了约3%）。

    arXiv:2403.03292v1 Announce Type: new  Abstract: State-of-the-art decentralized learning algorithms typically require the data distribution to be Independent and Identically Distributed (IID). However, in practical scenarios, the data distribution across the agents can have significant heterogeneity. In this work, we propose averaging rate scheduling as a simple yet effective way to reduce the impact of heterogeneity in decentralized learning. Our experiments illustrate the superiority of the proposed method (~3% improvement in test accuracy) compared to the conventional approach of employing a constant averaging rate.
    
[^120]: 使用概率电路进行可信度感知的多模态融合

    Credibility-Aware Multi-Modal Fusion Using Probabilistic Circuits

    [https://arxiv.org/abs/2403.03281](https://arxiv.org/abs/2403.03281)

    提出了一种使用概率电路进行可信度感知的多模态融合方法，在维持竞争性能的同时能够可靠推断可信度。

    

    我们考虑了针对辨别学习的迟到多模态融合问题。受到需要理解每个数据源可靠性的嘈杂的多源领域的启发，我们探讨了在多模态融合中的可信度概念。我们提出了一种使用概率电路（PCs）来结合个体模态上的预测分布的组合函数。我们还定义了一种概率度量来评估每个模态的可信度，通过PC上的推理查询。我们的实验评估表明，我们的融合方法能够可靠地推断可信度，并且与最先进技术保持竞争性能。

    arXiv:2403.03281v1 Announce Type: cross  Abstract: We consider the problem of late multi-modal fusion for discriminative learning. Motivated by noisy, multi-source domains that require understanding the reliability of each data source, we explore the notion of credibility in the context of multi-modal fusion. We propose a combination function that uses probabilistic circuits (PCs) to combine predictive distributions over individual modalities. We also define a probabilistic measure to evaluate the credibility of each modality via inference queries over the PC. Our experimental evaluation demonstrates that our fusion method can reliably infer credibility while maintaining competitive performance with the state-of-the-art.
    
[^121]: ARNN: 用于识别癫痫发作的多通道脑电图信号的注意力循环神经网络

    ARNN: Attentive Recurrent Neural Network for Multi-channel EEG Signals to Identify Epileptic Seizures

    [https://arxiv.org/abs/2403.03276](https://arxiv.org/abs/2403.03276)

    ARNN提出了一种注意力循环神经网络，用于处理多通道脑电图信号，具有线性复杂度和并行计算，结合注意力和LSTM gate的优势，并避免了它们的缺点。

    

    我们提出了一种注意力循环神经网络（ARNN），其沿着序列循环应用注意力层，并且具有与序列长度相关的线性复杂度。该模型在多通道脑电图信号上运行，而不是单通道信号，并利用并行计算。在该模型中，注意力层是一种计算单元，可以有效地应用自注意力机制和交叉注意力机制来计算一组广泛数量的状态向量和输入信号的递归函数。我们的架构在某种程度上受到了注意力层和长短期记忆（LSTM）单元的启发，并使用长短风格门，但通过多个阶段将这种典型单元扩展到多通道脑电图信号的并行化。它继承了注意力层和LSTM门的优势，同时避免了它们各自的缺点。我们通过对异质实验进行了广泛的模型有效性评估。

    arXiv:2403.03276v1 Announce Type: cross  Abstract: We proposed an Attentive Recurrent Neural Network (ARNN), which recurrently applies attention layers along a sequence and has linear complexity with respect to the sequence length. The proposed model operates on multi-channel EEG signals rather than single channel signals and leverages parallel computation. In this cell, the attention layer is a computational unit that efficiently applies self-attention and cross-attention mechanisms to compute a recurrent function over a wide number of state vectors and input signals. Our architecture is inspired in part by the attention layer and long short-term memory (LSTM) cells, and it uses long-short style gates, but it scales this typical cell up by several orders to parallelize for multi-channel EEG signals. It inherits the advantages of attention layers and LSTM gate while avoiding their respective drawbacks. We evaluated the model effectiveness through extensive experiments with heterogeneou
    
[^122]: 从噪音到信号：通过药理学启发的神经随机微分方程解密数字健康数据中的治疗效果

    From Noise to Signal: Unveiling Treatment Effects from Digital Health Data through Pharmacology-Informed Neural-SDE

    [https://arxiv.org/abs/2403.03274](https://arxiv.org/abs/2403.03274)

    通过药理学启发的神经随机微分方程模型，有效地识别数字健康数据中的治疗效果和学习因果关系，从而实现反事实能力。

    

    数字健康技术（DHT），如可穿戴设备，提供个性化、持续、实时监测患者的能力。这些技术正在为新疗法和个性化医学的发展做出贡献。从这些技术中获取洞察力需要适当的建模技术来捕捉疾病状态中的临床相关变化。这些设备产生的数据具有随机性，可能存在缺失元素，并表现出相当大的个体间变异性，因此很难使用传统的纵向建模技术进行分析。我们提出了一种能够应对这些挑战的新颖药理学启发的神经随机微分方程（SDE）模型。通过合成数据，我们演示了我们的方法在识别治疗效果和从随机数据中学习因果关系方面的有效性，从而实现反事实能力。

    arXiv:2403.03274v1 Announce Type: cross  Abstract: Digital health technologies (DHT), such as wearable devices, provide personalized, continuous, and real-time monitoring of patient. These technologies are contributing to the development of novel therapies and personalized medicine. Gaining insight from these technologies requires appropriate modeling techniques to capture clinically-relevant changes in disease state. The data generated from these devices is characterized by being stochastic in nature, may have missing elements, and exhibits considerable inter-individual variability - thereby making it difficult to analyze using traditional longitudinal modeling techniques. We present a novel pharmacology-informed neural stochastic differential equation (SDE) model capable of addressing these challenges. Using synthetic data, we demonstrate that our approach is effective in identifying treatment effects and learning causal relationships from stochastic data, thereby enabling counterfac
    
[^123]: 基于DINOv2的自监督学习用于少样本医学图像分割

    DINOv2 based Self Supervised Learning For Few Shot Medical Image Segmentation

    [https://arxiv.org/abs/2403.03273](https://arxiv.org/abs/2403.03273)

    本研究结合了ALPNet的优势和DINOv2的特征提取能力，提出了一种新颖的少样本医学图像分割方法，不仅提升了性能，也为更多创新铺平了道路。

    

    深度学习模型已经成为医学图像分割的基石，但其有效性取决于大量手动标记的数据集的可用性，其适应性到未知类别仍然是一个挑战。少样本分割（FSS）通过赋予模型从有限标记示例中学习新类别的能力，提供了一个有前途的解决方案。一种领先的FSS方法是ALPNet，它比较查询图像和少量可用支持分割图像之间的特征。关于使用ALPNet的一个关键问题是如何设计其特征。在这项工作中，我们深入探讨了使用DINOv2的特征的潜力，DINOv2是计算机视觉中一种基础的自监督学习模型。利用ALPNet的优势并利用DINOv2的特征提取功能，我们提出了一种新颖的少样本分割方法，不仅提高了性能，还为更多创新铺平了道路。

    arXiv:2403.03273v1 Announce Type: cross  Abstract: Deep learning models have emerged as the cornerstone of medical image segmentation, but their efficacy hinges on the availability of extensive manually labeled datasets and their adaptability to unforeseen categories remains a challenge. Few-shot segmentation (FSS) offers a promising solution by endowing models with the capacity to learn novel classes from limited labeled examples. A leading method for FSS is ALPNet, which compares features between the query image and the few available support segmented images. A key question about using ALPNet is how to design its features. In this work, we delve into the potential of using features from DINOv2, which is a foundational self-supervised learning model in computer vision. Leveraging the strengths of ALPNet and harnessing the feature extraction capabilities of DINOv2, we present a novel approach to few-shot segmentation that not only enhances performance but also paves the way for more ro
    
[^124]: 神经网络学习与量子引力

    Neural Network Learning and Quantum Gravity

    [https://arxiv.org/abs/2403.03245](https://arxiv.org/abs/2403.03245)

    研究旨在探索神经网络学习如何帮助发现弦景观中一致性理论的新特性，以及其统计可学习性质。

    

    源自弦论的低能有效场论领域太过广阔，难以系统性地探索。然而，弦景观的这片广袤之地可能是应用机器学习技术的肥沃土壤。利用神经网络学习可以推断出那些一致性理论应该具备的新颖、未发现的特性，或检验关于其声称特征的猜想性陈述。本文旨在描述用神经网络学习能够探索弦景观的程度。我们的分析受近期研究的启发，表明弦景观以其底层温和的、O-极小结构而表现出有限性属性。实际上，利用这些结果，我们说明弦论的任何低能有效理论都具备某种统计可学习性质。因此，一些…

    arXiv:2403.03245v1 Announce Type: cross  Abstract: The landscape of low-energy effective field theories stemming from string theory is too vast for a systematic exploration. However, the meadows of the string landscape may be fertile ground for the application of machine learning techniques. Employing neural network learning may allow for inferring novel, undiscovered properties that consistent theories in the landscape should possess, or checking conjectural statements about alleged characteristics thereof. The aim of this work is to describe to what extent the string landscape can be explored with neural network-based learning. Our analysis is motivated by recent studies that show that the string landscape is characterized by finiteness properties, emerging from its underlying tame, o-minimal structures. Indeed, employing these results, we illustrate that any low-energy effective theory of string theory is endowed with certain statistical learnability properties. Consequently, severa
    
[^125]: 用于统计推断的三重/去偏Lasso：条件平均处理效应

    Triple/Debiased Lasso for Statistical Inference of Conditional Average Treatment Effects

    [https://arxiv.org/abs/2403.03240](https://arxiv.org/abs/2403.03240)

    研究提出了一种三重/去偏Lasso方法，用于统计推断条件平均处理效应，不要求直接假设稀疏性，有效估计了线性模型之间的差异。

    

    本研究调查了关于条件平均处理效应（CATEs）的估计和统计推断，CATEs作为表示个性化因果效应的一个指标已经引起了关注。在我们的数据生成过程中，我们假设与二值处理相关联的结果采用线性模型，并将CATE定义为这些线性模型的预期结果之间的差异。本研究允许线性模型是高维的，我们的兴趣在于对CATE进行一致估计和统计推断。在高维线性回归中，一种典型的方法是假设稀疏性。但是，在我们的研究中，我们不直接假设稀疏性。相反，我们仅在线性模型的差异中考虑稀疏性。我们首先使用双重稳健估计器来近似这种差异，然后用Lasso正则化将差异回归到协变量上。尽管这种回归估计量是

    arXiv:2403.03240v1 Announce Type: cross  Abstract: This study investigates the estimation and the statistical inference about Conditional Average Treatment Effects (CATEs), which have garnered attention as a metric representing individualized causal effects. In our data-generating process, we assume linear models for the outcomes associated with binary treatments and define the CATE as a difference between the expected outcomes of these linear models. This study allows the linear models to be high-dimensional, and our interest lies in consistent estimation and statistical inference for the CATE. In high-dimensional linear regression, one typical approach is to assume sparsity. However, in our study, we do not assume sparsity directly. Instead, we consider sparsity only in the difference of the linear models. We first use a doubly robust estimator to approximate this difference and then regress the difference on covariates with Lasso regularization. Although this regression estimator is
    
[^126]: Caduceus: 双向等变长范围 DNA 序列建模

    Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling

    [https://arxiv.org/abs/2403.03234](https://arxiv.org/abs/2403.03234)

    Caduceus 是第一个支持反向互补性并具有双向性的长范围 DNA 语言模型，通过引入预训练和微调策略，在下游基准测试中表现优异。

    

    大规模序列建模引发了快速进展，现在扩展至生物学和基因组学。然而，建模基因组序列引入了挑战，如需要建模长范围片段相互作用，基因组上游和下游区域的影响，以及 DNA 的反向互补性（RC）。在这里，我们提出了一个受这些挑战启发的架构，它基于长范围的 Mamba 块，将其扩展为支持双向性的 BiMamba 组件，以及支持 RC 等变性的 MambaDNA 块。我们以 MambaDNA 为基础，创造了 Caduceus，第一个支持 RC 等变性的双向长范围 DNA 语言模型系列，我们介绍了预训练和微调策略，生成了 Caduceus DNA 基础模型。Caduceus 在下游基准测试中胜过以前的长范围模型；在具有挑战性的长范围变异效应预测任务上，Caduceus 超越了该任务。

    arXiv:2403.03234v1 Announce Type: cross  Abstract: Large-scale sequence modeling has sparked rapid advances that now extend into biology and genomics. However, modeling genomic sequences introduces challenges such as the need to model long-range token interactions, the effects of upstream and downstream regions of the genome, and the reverse complementarity (RC) of DNA. Here, we propose an architecture motivated by these challenges that builds off the long-range Mamba block, and extends it to a BiMamba component that supports bi-directionality, and to a MambaDNA block that additionally supports RC equivariance. We use MambaDNA as the basis of Caduceus, the first family of RC equivariant bi-directional long-range DNA language models, and we introduce pre-training and fine-tuning strategies that yield Caduceus DNA foundation models. Caduceus outperforms previous long-range models on downstream benchmarks; on a challenging long-range variant effect prediction task, Caduceus exceeds the pe
    
[^127]: 从位移到分布：一种用于量化计算模型参数不确定性的机器学习框架

    From Displacements to Distributions: A Machine-Learning Enabled Framework for Quantifying Uncertainties in Parameters of Computational Models

    [https://arxiv.org/abs/2403.03233](https://arxiv.org/abs/2403.03233)

    该论文提出了一种机器学习框架，可以量化计算模型参数不确定性的两种来源，通过结合数据一致性和学习不确定量框架，可以有效处理测量误差和兴趣数量映射问题

    

    这项工作提出了结合两种框架的新扩展，用于量化工程系统建模中的混合不确定性源，即aleatoric（即不可减少的）和epistemic（即可减少的）。数据一致性（DC）框架提出了一个逆问题和解决方案，以量化以给定量化兴趣图定的回拉和推进测度的aleatoric不确定性。不幸的是，预先指定的兴趣数量映射并不总是在与系统输出相关的数据收集之前是可用的。数据本身经常受到测量误差（即epistemic不确定性）的污染，这使得指定一个有用的兴趣数量映射的过程变得复杂。学习不确定量（LUQ）框架定义了一个正式的三步机器学习启用过程，用于将嘈杂数据集转化为学习到的兴趣的映射样本，以启用基于DC的反演。我们在LUQ中开发了一个强大的过滤步骤。

    arXiv:2403.03233v1 Announce Type: cross  Abstract: This work presents novel extensions for combining two frameworks for quantifying both aleatoric (i.e., irreducible) and epistemic (i.e., reducible) sources of uncertainties in the modeling of engineered systems. The data-consistent (DC) framework poses an inverse problem and solution for quantifying aleatoric uncertainties in terms of pullback and push-forward measures for a given Quantity of Interest (QoI) map. Unfortunately, a pre-specified QoI map is not always available a priori to the collection of data associated with system outputs. The data themselves are often polluted with measurement errors (i.e., epistemic uncertainties), which complicates the process of specifying a useful QoI. The Learning Uncertain Quantities (LUQ) framework defines a formal three-step machine-learning enabled process for transforming noisy datasets into samples of a learned QoI map to enable DC-based inversion. We develop a robust filtering step in LUQ 
    
[^128]: 用于预测3D基因组组织的机器学习和深度学习方法

    Machine and deep learning methods for predicting 3D genome organization

    [https://arxiv.org/abs/2403.03231](https://arxiv.org/abs/2403.03231)

    该论文总结了机器学习方法作为一种替代方式，用于获取缺失的3D染色质相互作用和/或提高分辨率的情况

    

    三维（3D）染色质相互作用，如增强子-启动子相互作用（EPIs），环状构象域（loops），拓扑相关域（TADs）和A/B区域通过调控基因表达在各种细胞过程中发挥关键作用。最近发展的染色质构象捕获技术使得即使使用单个细胞也能对各种3D结构进行全基因组分析。然而，由于技术、工具和数据分辨率低的差异，当前的3D结构目录仍然不完整且不可靠。机器学习方法作为一种替代方式出现，用于获取缺失的3D相互作用和/或改善分辨率。这些方法通常使用基因组注释数据（ChIP-seq，DNAse-seq等），DNA序列信息（k-mers，转录因子结合位点（TFBS）模体）和其他基因组特性来学习基因组特征与染色质相互作用之间的关联。

    arXiv:2403.03231v1 Announce Type: cross  Abstract: Three-Dimensional (3D) chromatin interactions, such as enhancer-promoter interactions (EPIs), loops, Topologically Associating Domains (TADs), and A/B compartments play critical roles in a wide range of cellular processes by regulating gene expression. Recent development of chromatin conformation capture technologies has enabled genome-wide profiling of various 3D structures, even with single cells. However, current catalogs of 3D structures remain incomplete and unreliable due to differences in technology, tools, and low data resolution. Machine learning methods have emerged as an alternative to obtain missing 3D interactions and/or improve resolution. Such methods frequently use genome annotation data (ChIP-seq, DNAse-seq, etc.), DNA sequencing information (k-mers, Transcription Factor Binding Site (TFBS) motifs), and other genomic properties to learn the associations between genomic features and chromatin interactions. In this revie
    
[^129]: 拥抱不确定性灵活性：利用监督树核强化集成模型，预测右心室容积的二维超声心动图

    Embracing Uncertainty Flexibility: Harnessing a Supervised Tree Kernel to Empower Ensemble Modelling for 2D Echocardiography-Based Prediction of Right Ventricular Volume

    [https://arxiv.org/abs/2403.03229](https://arxiv.org/abs/2403.03229)

    提出了一种利用监督树核强化集成模型，预测右心室容积的二维超声心动图的方法，并通过不确定性得分增强了预测表现，该方法在小规模数据集上表现出较好的概率和点预测性能。

    

    右心室（RV）功能恶化在许多情况下都能强力预测临床结果。为了增强使用广泛可用的二维超声心动图（2DE）的表格数据量化RV容积的集成回归方法的临床部署，我们建议将体积预测与不确定性得分相结合。为此，我们采用一种基于实例的方法，该方法使用学习的树结构来识别目标实例周围的最近训练样本，然后使用多种分布类型来更灵活地建模输出。所提出的框架的概率和点预测性能在一个相对小规模的数据集上进行评估，包括100个舒张末和收缩末RV容积。点性能的参考值来自MRI。结果表明，我们的灵活方法在概率和点预测性能上表现出更好的表现。

    arXiv:2403.03229v1 Announce Type: cross  Abstract: The right ventricular (RV) function deterioration strongly predicts clinical outcomes in numerous circumstances. To boost the clinical deployment of ensemble regression methods that quantify RV volumes using tabular data from the widely available two-dimensional echocardiography (2DE), we propose to complement the volume predictions with uncertainty scores. To this end, we employ an instance-based method which uses the learned tree structure to identify the nearest training samples to a target instance and then uses a number of distribution types to more flexibly model the output. The probabilistic and point-prediction performances of the proposed framework are evaluated on a relatively small-scale dataset, comprising 100 end-diastolic and end-systolic RV volumes. The reference values for point performance were obtained from MRI. The results demonstrate that our flexible approach yields improved probabilistic and point performances ove
    
[^130]: 强化学习爵士即兴演奏：音乐遇上博弈论

    Reinforcement Learning Jazz Improvisation: When Music Meets Game Theory

    [https://arxiv.org/abs/2403.03224](https://arxiv.org/abs/2403.03224)

    介绍了一个新颖的数学博弈论模型用于研究爵士即兴演奏，探索不同的随机即兴策略和其在即兴演奏中的配对表现，发现最有效的策略对是逐步改变和和弦跟随强化学习。

    

    音乐的现场表演总是迷人的，即兴演奏的不可预测性是由于音乐家之间的动态关系和与观众的互动。爵士即兴演奏是一个特别值得从理论视角进一步研究的例子。本文引入了一个新颖的数学博弈论模型来研究爵士即兴演奏，为研究音乐理论和即兴演奏方法学提供了一个框架。我们使用计算建模，主要是强化学习，来探索不同的随机即兴策略及其在即兴演奏中的配对表现。我们发现，最有效的策略对是一种对最近收益作出反应的策略（逐步改变），配合强化学习策略，其仅限于给定和弦中的音符（和弦跟随强化学习）。相反，一种对伙伴的上一个音符作出反应，并试图与之和谐的策略（和谐P

    arXiv:2403.03224v1 Announce Type: cross  Abstract: Live performances of music are always charming, with the unpredictability of improvisation due to the dynamic between musicians and interactions with the audience. Jazz improvisation is a particularly noteworthy example for further investigation from a theoretical perspective. Here, we introduce a novel mathematical game theory model for jazz improvisation, providing a framework for studying music theory and improvisational methodologies. We use computational modeling, mainly reinforcement learning, to explore diverse stochastic improvisational strategies and their paired performance on improvisation. We find that the most effective strategy pair is a strategy that reacts to the most recent payoff (Stepwise Changes) with a reinforcement learning strategy limited to notes in the given chord (Chord-Following Reinforcement Learning). Conversely, a strategy that reacts to the partner's last note and attempts to harmonize with it (Harmony P
    
[^131]: 在顺序物理信息神经网络中精确执⾏时间连续性

    Exact Enforcement of Temporal Continuity in Sequential Physics-Informed Neural Networks

    [https://arxiv.org/abs/2403.03223](https://arxiv.org/abs/2403.03223)

    精确执⾏时间连续性是本论⽂的⼀项重要创新，简化了解决时间相关问题动态⾏为预测困难的挑战。

    

    科学计算中深度学习方法的应⽤代表了⼯程问题解决⽅法的潜在范式转变。最显著的发展之⼀是物理信息神经⽹络（PINNs），其中神经⽹络被训练以满⾜偏微分⽅程（PDEs）和/或观察数据。尽管此⽅法有希望，但标准版本已被证明在准确预测时间相关问题的动态⾏为⽅⾯存在困难。为了解决这⼀挑战，已经提出⽅法将时间域分解为多个段，每个段中使⽤⼀个不同的神经⽹络，并直接在最⼩化问题的损失函数中将它们之间的连续性纳⼊其中。在本⼯作中，我们引⼊⼀种通过解析解精确强制实现连续性的⽅法。这种⽅法简单易⽤，能够消除

    arXiv:2403.03223v1 Announce Type: new  Abstract: The use of deep learning methods in scientific computing represents a potential paradigm shift in engineering problem solving. One of the most prominent developments is Physics-Informed Neural Networks (PINNs), in which neural networks are trained to satisfy partial differential equations (PDEs) and/or observed data. While this method shows promise, the standard version has been shown to struggle in accurately predicting the dynamic behavior of time-dependent problems. To address this challenge, methods have been proposed that decompose the time domain into multiple segments, employing a distinct neural network in each segment and directly incorporating continuity between them in the loss function of the minimization problem. In this work we introduce a method to exactly enforce continuity between successive time segments via a solution ansatz. This hard constrained sequential PINN (HCS-PINN) method is simple to implement and eliminates 
    
[^132]: 知识引导的EEG表示学习

    Knowledge-guided EEG Representation Learning

    [https://arxiv.org/abs/2403.03222](https://arxiv.org/abs/2403.03222)

    提出了一个知识引导的EEG自监督学习模型，通过使用基于状态空间的深度学习架构，实现了稳健的性能和显著的参数效率。

    

    arXiv:2403.03222v1 公告类型:跨领域 摘要:自监督学习在音频、视觉和语音等多媒体领域取得了令人瞩目的成果。由于这些情景中标记数据的稀缺性，这种范式对于生物信号领域同样重要，甚至更重要。利用大规模未标记数据来学习稳健的表示能够帮助提高生物信号上许多推断任务的性能。考虑到多媒体模态和生物信号之间固有的领域差异，为自监督学习建立的传统目标可能无法很好地转化到这一领域。因此，有必要将这些方法调整到生物信号分析中。在这项工作中，我们提出了一个基于状态空间的深度学习架构的自监督EEG模型，该模型通过提出一种新颖的知识引导的预训练目标来提供稳健的性能和显著的参数效率。

    arXiv:2403.03222v1 Announce Type: cross  Abstract: Self-supervised learning has produced impressive results in multimedia domains of audio, vision and speech. This paradigm is equally, if not more, relevant for the domain of biosignals, owing to the scarcity of labelled data in such scenarios. The ability to leverage large-scale unlabelled data to learn robust representations could help improve the performance of numerous inference tasks on biosignals. Given the inherent domain differences between multimedia modalities and biosignals, the established objectives for self-supervised learning may not translate well to this domain. Hence, there is an unmet need to adapt these methods to biosignal analysis. In this work we propose a self-supervised model for EEG, which provides robust performance and remarkable parameter efficiency by using state space-based deep learning architecture. We also propose a novel knowledge-guided pre-training objective that accounts for the idiosyncrasies of th
    
[^133]: FedHCDR: 具有超图信号解耦的联邦跨领域推荐

    FedHCDR: Federated Cross-Domain Recommendation with Hypergraph Signal Decoupling

    [https://arxiv.org/abs/2403.02630](https://arxiv.org/abs/2403.02630)

    该研究提出了FedHCDR框架，通过超图信号解耦的方式解决了联邦跨领域推荐中不同领域数据异质性的问题。

    

    近年来，跨领域推荐（CDR）备受关注，利用来自多个领域的用户数据来增强推荐性能。然而，当前的CDR方法需要跨领域共享用户数据，违反了《通用数据保护条例》（GDPR）。因此，已提出了许多联邦跨领域推荐（FedCDR）方法。然而，不同领域间的数据异质性不可避免地影响了联邦学习的整体性能。在这项研究中，我们提出了FedHCDR，一种具有超图信号解耦的新型联邦跨领域推荐框架。具体地，为了解决不同领域之间的数据异质性，我们引入一种称为超图信号解耦（HSD）的方法，将用户特征解耦为领域独有和领域共享特征。该方法采用高通和低通超图滤波器来进行解耦。

    arXiv:2403.02630v1 Announce Type: new  Abstract: In recent years, Cross-Domain Recommendation (CDR) has drawn significant attention, which utilizes user data from multiple domains to enhance the recommendation performance. However, current CDR methods require sharing user data across domains, thereby violating the General Data Protection Regulation (GDPR). Consequently, numerous approaches have been proposed for Federated Cross-Domain Recommendation (FedCDR). Nevertheless, the data heterogeneity across different domains inevitably influences the overall performance of federated learning. In this study, we propose FedHCDR, a novel Federated Cross-Domain Recommendation framework with Hypergraph signal decoupling. Specifically, to address the data heterogeneity across domains, we introduce an approach called hypergraph signal decoupling (HSD) to decouple the user features into domain-exclusive and domain-shared features. The approach employs high-pass and low-pass hypergraph filters to de
    
[^134]: 通过多任务强化学习实现高效的短视探索

    Sample Efficient Myopic Exploration Through Multitask Reinforcement Learning with Diverse Tasks

    [https://arxiv.org/abs/2403.01636](https://arxiv.org/abs/2403.01636)

    通过研究发现，当代理在多样化任务上进行训练时，具有短视探索设计的通用策略共享算法可以在多任务强化学习中显著提高样本效率。

    

    多任务强化学习（MTRL）方法在许多重要的强化学习（RL）任务中应用广泛，但近期MTRL理论的进展主要集中在通过假设任务间共享结构来提高统计效率，对于RL中至关重要的探索这一关键方面却大多被忽视。本文通过展示，当代理在足够多样化的任务集上训练时，具有短视探索设计（如$\epsilon$-贪心）的通用策略共享算法可以在MTRL中具有高样本效率，从我们所知，这是对“探索收益”在MTRL中的首次理论证明，也有助于解释短视探索在实践中应用广泛的成功。为了验证多样性的作用，我们在合成机器人控制任务上进行了实验证明。

    arXiv:2403.01636v1 Announce Type: cross  Abstract: Multitask Reinforcement Learning (MTRL) approaches have gained increasing attention for its wide applications in many important Reinforcement Learning (RL) tasks. However, while recent advancements in MTRL theory have focused on the improved statistical efficiency by assuming a shared structure across tasks, exploration--a crucial aspect of RL--has been largely overlooked. This paper addresses this gap by showing that when an agent is trained on a sufficiently diverse set of tasks, a generic policy-sharing algorithm with myopic exploration design like $\epsilon$-greedy that are inefficient in general can be sample-efficient for MTRL. To the best of our knowledge, this is the first theoretical demonstration of the "exploration benefits" of MTRL. It may also shed light on the enigmatic success of the wide applications of myopic exploration in practice. To validate the role of diversity, we conduct experiments on synthetic robotic control
    
[^135]: 少即是多：面向可扩展和通用学习的跳数图注意力在电路上的应用

    Less is More: Hop-Wise Graph Attention for Scalable and Generalizable Learning on Circuits

    [https://arxiv.org/abs/2403.01317](https://arxiv.org/abs/2403.01317)

    提出了一种名为HOGA的基于注意力的模型，能够在电路中以可扩展和通用的方式学习电路表示，通过跳数特征和门控自注意力模块的方式，实现了对不同电路结构的自适应学习，并可以进行高效的分布式训练。

    

    虽然图神经网络（GNNs）在各种电子设计自动化（EDA）任务中学习电路表示方面变得流行，但当应用于大图时，它们面临可扩展性挑战，并且对新设计的泛化能力有限。这些限制使它们在解决大规模复杂电路问题时不太实用。在这项工作中，我们提出了HOGA，一种新颖的基于注意力的模型，用于以可扩展和通用的方式学习电路表示。HOGA首先在模型训练之前针对每个节点计算跳数特征。随后，跳数特征仅用于通过门控自注意力模块生成节点表示，该模块自适应地学习不同跳数之间的重要特征，而不涉及图拓扑。因此，HOGA能够适应不同电路之间的各种结构，并可以以分布式的方式高效训练。

    arXiv:2403.01317v1 Announce Type: new  Abstract: While graph neural networks (GNNs) have gained popularity for learning circuit representations in various electronic design automation (EDA) tasks, they face challenges in scalability when applied to large graphs and exhibit limited generalizability to new designs. These limitations make them less practical for addressing large-scale, complex circuit problems. In this work we propose HOGA, a novel attention-based model for learning circuit representations in a scalable and generalizable manner. HOGA first computes hop-wise features per node prior to model training. Subsequently, the hop-wise features are solely used to produce node representations through a gated self-attention module, which adaptively learns important features among different hops without involving the graph topology. As a result, HOGA is adaptive to various structures across different circuits and can be efficiently trained in a distributed manner. To demonstrate the e
    
[^136]: 硅谷人群的智慧：LLM集成预测能力达到人群准确率水平

    Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy

    [https://arxiv.org/abs/2402.19379](https://arxiv.org/abs/2402.19379)

    该研究通过将十二个LLMs组成的LLM集成方法与925名人类预测者的群体预测进行比较，发现LLM群体优于简单的无信息基准，并在统计上等效于人类群体。

    

    实践中人类预测准确性依赖于“群体智慧”效应，即通过聚合一群个体预测者的预测可以显著提高对未来事件的预测。过去关于大型语言模型（LLMs）预测能力的研究表明，作为个体预测者的前沿LLMs表现不佳，与人类群体预测比赛的黄金标准相比。我们通过使用一个由十二个LLMs组成的LLM集成方法，扩展了研究。我们将31个二元问题的聚合LLM预测与一个来自三个月预测比赛的925名人类预测者的群体预测进行比较。我们的主要分析表明，LLM群体的表现优于简单的无信息基准，并在统计上等效于人类群体。我们还观察到一种顺从效应，平均模型预测明显高于50%，尽管几乎是平等的。

    arXiv:2402.19379v1 Announce Type: cross  Abstract: Human forecasting accuracy in practice relies on the 'wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters. Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate. In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of twelve LLMs. We compare the aggregated LLM predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament. Our main analysis shows that the LLM crowd outperforms a simple no-information benchmark and is statistically equivalent to the human crowd. We also observe an acquiescence effect, with mean model predictions being significantly above 50%, despite an almost even
    
[^137]: 神经网络全局鲁棒性的验证

    Verification of Neural Networks' Global Robustness

    [https://arxiv.org/abs/2402.19322](https://arxiv.org/abs/2402.19322)

    提出了一种新的全局鲁棒性属性，旨在找到分类器的最小全局鲁棒边界，并引入了VHAGaR，一个用于计算此边界的验证器。

    

    神经网络在各种应用中取得了成功，但也容易受到对抗性攻击的影响。为了展示网络分类器的安全性，引入了许多验证器来推理给定输入对给定扰动的局部鲁棒性。尽管成功，局部鲁棒性不能推广到未见过的输入。一些工作分析全局鲁棒性属性，然而，它们都无法提供网络分类器在不改变分类的情况下的精确保证。在这项工作中，我们提出了一种针对分类器的新全局鲁棒性属性，旨在找到最小的全局稳健边界，其自然地扩展了用于分类器的流行的局部鲁棒性属性。我们引入了VHAGaR，一个用于计算此边界的可随时使用的验证器。VHAGaR依赖于三个主要思想：将问题编码为混合整数规划，并通过识别由于每个局部更改而产生的依赖关系来修剪搜索空间。

    arXiv:2402.19322v1 Announce Type: new  Abstract: Neural networks are successful in various applications but are also susceptible to adversarial attacks. To show the safety of network classifiers, many verifiers have been introduced to reason about the local robustness of a given input to a given perturbation. While successful, local robustness cannot generalize to unseen inputs. Several works analyze global robustness properties, however, neither can provide a precise guarantee about the cases where a network classifier does not change its classification. In this work, we propose a new global robustness property for classifiers aiming at finding the minimal globally robust bound, which naturally extends the popular local robustness property for classifiers. We introduce VHAGaR, an anytime verifier for computing this bound. VHAGaR relies on three main ideas: encoding the problem as a mixed-integer programming and pruning the search space by identifying dependencies stemming from the per
    
[^138]: 二阶循环平稳信号估计和去卷积

    Estimation and Deconvolution of Second Order Cyclostationary Signals

    [https://arxiv.org/abs/2402.19290](https://arxiv.org/abs/2402.19290)

    该方法解决了盲去卷积和估计带噪声二阶循环平稳信号时间波形的双重问题，通过证明去卷积滤波器存在，消除信号的TF效应，实现高精度算法，有潜力改善机器学习模型的训练。

    

    这种方法解决了盲去卷积和估计通过传输函数(TF)传输到传感器的带噪声二阶循环平稳(CS2)信号的时间波形的双重问题。我们已经证明，去卷积滤波器存在并消除了统计特性随时间变化的信号上的TF效应。这种方法是盲目的，即不需要关于信号或TF的先前知识。模拟表明，算法在各种信号类型、TF和信噪比(SNR)下具有高精度。在本研究中，CS2信号族被限制为确定性周期函数和白噪声的乘积。此外，这种方法有潜力改善机器学习模型的训练，其中需要聚合来自相同系统但具有不同TF的信号。

    arXiv:2402.19290v1 Announce Type: new  Abstract: This method solves the dual problem of blind deconvolution and estimation of the time waveform of noisy second-order cyclo-stationary (CS2) signals that traverse a Transfer Function (TF) en route to a sensor. We have proven that the deconvolution filter exists and eliminates the TF effect from signals whose statistics vary over time. This method is blind, meaning it does not require prior knowledge about the signals or TF. Simulations demonstrate the algorithm high precision across various signal types, TFs, and Signal-to-Noise Ratios (SNRs). In this study, the CS2 signals family is restricted to the product of a deterministic periodic function and white noise. Furthermore, this method has the potential to improve the training of Machine Learning models where the aggregation of signals from identical systems but with different TFs is required.
    
[^139]: 基于语言引导的状态抽象学习

    Learning with Language-Guided State Abstractions

    [https://arxiv.org/abs/2402.18759](https://arxiv.org/abs/2402.18759)

    利用自然语言和语言模型引导的方法，实现自动构建适用于未见任务的状态表示，有助于高维观测空间中泛化策略学习。

    

    我们描述了一个利用自然语言设计状态抽象用于模仿学习的框架。在高维观测空间中实现泛化策略学习的关键在于精心设计的状态表示，这可以将环境中的重要特征展现出来并隐藏不相关的特征。这些状态表示通常是手动指定的，或者是从其他繁重的标记过程中导出的。我们的方法LGA（语言引导的抽象）利用自然语言监督和语言模型的背景知识的结合自动构建适用于未见任务的状态表示。在LGA中，用户首先使用自然语言提供目标任务的（可能是不完整的）描述；接下来，一个预训练的语言模型将这个任务描述转化为掩盖不相关特征的状态抽象函数；最后，使用少量演示数据训练一个模仿策略。

    arXiv:2402.18759v1 Announce Type: cross  Abstract: We describe a framework for using natural language to design state abstractions for imitation learning. Generalizable policy learning in high-dimensional observation spaces is facilitated by well-designed state representations, which can surface important features of an environment and hide irrelevant ones. These state representations are typically manually specified, or derived from other labor-intensive labeling procedures. Our method, LGA (language-guided abstraction), uses a combination of natural language supervision and background knowledge from language models (LMs) to automatically build state representations tailored to unseen tasks. In LGA, a user first provides a (possibly incomplete) description of a target task in natural language; next, a pre-trained LM translates this task description into a state abstraction function that masks out irrelevant features; finally, an imitation policy is trained using a small number of demo
    
[^140]: 用于满足多样用户偏好的算术控制LLMs：具有多目标奖励的方向偏好对齐

    Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards

    [https://arxiv.org/abs/2402.18571](https://arxiv.org/abs/2402.18571)

    提出了方向偏好对齐（DPA）框架，通过多目标奖励模拟不同偏好配置，以实现用户相关的偏好控制。

    

    针对大型语言模型（LLMs）的精细控制仍然是一个重要挑战，阻碍了它们适应各种用户需求。本文提出了方向偏好对齐（DPA）框架，通过多目标奖励建模来表示多样化的偏好配置，将用户偏好建模为奖励空间中的方向（即单位向量）以实现用户相关的偏好控制。

    arXiv:2402.18571v1 Announce Type: cross  Abstract: Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles. Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2. This method enjoys a better performance
    
[^141]: 结构引导的扩散模型对抗训练

    Structure-Guided Adversarial Training of Diffusion Models

    [https://arxiv.org/abs/2402.17563](https://arxiv.org/abs/2402.17563)

    SADM通过对抗训练的方式引入结构指导，使得模型能够学习每个训练批次中样本之间的流形结构。

    

    扩散模型在各种生成应用中展现出卓越的功效。为了解决现有模型主要侧重于最小化加权去噪评分匹配损失以进行数据分布建模的训练局限性，我们引入了结构引导的扩散模型对抗训练（SADM）。在这种开创性方法中，我们强迫模型在每个训练批次中学习样本之间的流形结构。

    arXiv:2402.17563v2 Announce Type: cross  Abstract: Diffusion models have demonstrated exceptional efficacy in various generative applications. While existing models focus on minimizing a weighted sum of denoising score matching losses for data distribution modeling, their training primarily emphasizes instance-level optimization, overlooking valuable structural information within each mini-batch, indicative of pair-wise relationships among samples. To address this limitation, we introduce Structure-guided Adversarial training of Diffusion Models (SADM). In this pioneering approach, we compel the model to learn manifold structures between samples in each training batch. To ensure the model captures authentic manifold structures in the data distribution, we advocate adversarial training of the diffusion generator against a novel structure discriminator in a minimax game, distinguishing real manifold structures from the generated ones. SADM substantially improves existing diffusion transf
    
[^142]: 连续时间强化学习中深度残差网络的\emph{先验估计}

    A prior Estimates for Deep Residual Network in Continuous-time Reinforcement Learning

    [https://arxiv.org/abs/2402.16899](https://arxiv.org/abs/2402.16899)

    本研究针对连续时间控制问题，提出了一种可以直接分析Bellman最优损失\emph{先验}泛化误差的方法，避免了有界性假设，并通过最大算子的分解方法实现了损失函数的转换。

    

    深度强化学习在许多大规模实际应用中表现出色。然而，现有的性能分析忽略了连续时间控制问题的独特特征，无法直接估计Bellman最优损失的泛化误差，并且需要一个有界性假设。我们的工作侧重于连续时间控制问题，并提出了一种适用于所有满足半群和Lipschitz性质的问题的方法。在该方法下，我们能够直接分析Bellman最优损失的\emph{先验}泛化误差。该方法的核心在于损失函数的两次转换。为了完成转换，我们提出了最大算子的分解方法。此外，这个分析方法不需要有界性假设。最终我们维得到了一个没有“维度诅咒”的\emph{先验}泛化误差。

    arXiv:2402.16899v1 Announce Type: cross  Abstract: Deep reinforcement learning excels in numerous large-scale practical applications. However, existing performance analyses ignores the unique characteristics of continuous-time control problems, is unable to directly estimate the generalization error of the Bellman optimal loss and require a boundedness assumption. Our work focuses on continuous-time control problems and proposes a method that is applicable to all such problems where the transition function satisfies semi-group and Lipschitz properties. Under this method, we can directly analyze the \emph{a priori} generalization error of the Bellman optimal loss. The core of this method lies in two transformations of the loss function. To complete the transformation, we propose a decomposition method for the maximum operator. Additionally, this analysis method does not require a boundedness assumption. Finally, we obtain an \emph{a priori} generalization error without the curse of dime
    
[^143]: 人形机器人的表现全身控制

    Expressive Whole-Body Control for Humanoid Robots

    [https://arxiv.org/abs/2402.16796](https://arxiv.org/abs/2402.16796)

    我们提出了一种表现全身控制（Exbody）方法，通过在人类大小的机器人上学习全身控制策略，使其能够模仿人类动作，并通过在模拟环境中训练和Sim2Real转移，实现控制人形机器人以不同风格行走、摇晃等。

    

    我们提出在真实世界中让人形机器人生成丰富、多样和富有表现力的动作。我们建议在一个人类大小的机器人上学习一个全身控制策略，以尽可能逼真地模仿人类动作。为了训练这样一个策略，我们在强化学习框架中利用了来自图形学界的大规模人体动作捕捉数据。然而，直接使用动作捕捉数据集进行模仿学习在真实人形机器人上并不奏效，因为在自由度和物理能力方面存在较大差距。我们的方法——表现全身控制(Exbody)通过鼓励上半身模仿参考运动来解决这个问题，同时松开对其两条腿的模仿约束，只要求它们稳固地遵循给定的速度。通过在模拟环境中训练和Sim2Real转移，我们的策略能够控制人形机器人以不同风格行走，摇晃等。

    arXiv:2402.16796v2 Announce Type: replace-cross  Abstract: Can we enable humanoid robots to generate rich, diverse, and expressive motions in the real world? We propose to learn a whole-body control policy on a human-sized robot to mimic human motions as realistic as possible. To train such a policy, we leverage the large-scale human motion capture data from the graphics community in a Reinforcement Learning framework. However, directly performing imitation learning with the motion capture dataset would not work on the real humanoid robot, given the large gap in degrees of freedom and physical capabilities. Our method Expressive Whole-Body Control (Exbody) tackles this problem by encouraging the upper humanoid body to imitate a reference motion, while relaxing the imitation constraint on its two legs and only requiring them to follow a given velocity robustly. With training in simulation and Sim2Real transfer, our policy can control a humanoid robot to walk in different styles, shake h
    
[^144]: 可扩展的稳健稀疏主成分分析

    Scalable Robust Sparse Principal Component Analysis

    [https://arxiv.org/abs/2402.16712](https://arxiv.org/abs/2402.16712)

    本文提出了一个优化框架，可在稀疏稳健的情况下估计一维子空间，通过引入线性松弛方法和新颖的拟合程序，实现了全局最优的稳健稀疏子空间，具有多项式时间效率且可扩展性强。

    

    在这项工作中，我们提出了一个优化框架来估计稀疏稳健的一维子空间。我们的目标是最小化表示误差和l1范数准则下的惩罚。鉴于问题是NP难的，我们引入了一种基于线性松弛的方法。此外，我们提出了一种利用简单比例和排序技术的新型拟合程序。所提出的算法展示了$O(n^2 m \log n)$的最坏时间复杂度，并且在某些情况下，实现了稀疏稳健子空间的全局最优，从而展示了多项式时间效率。与现有方法相比，所提出的算法找到具有最低不一致性的子空间，提供了在稀疏性和拟合之间更平滑的权衡。其架构具有可扩展性，对于2000x2000的矩阵，计算速度相较CPU版本提升了16倍。此外，这种方法...

    arXiv:2402.16712v1 Announce Type: new  Abstract: In this work, we propose an optimization framework for estimating a sparse robust one-dimensional subspace. Our objective is to minimize both the representation error and the penalty, in terms of the l1-norm criterion. Given that the problem is NP-hard, we introduce a linear relaxation-based approach. Additionally, we present a novel fitting procedure, utilizing simple ratios and sorting techniques. The proposed algorithm demonstrates a worst-case time complexity of $O(n^2 m \log n)$ and, in certain instances, achieves global optimality for the sparse robust subspace, thereby exhibiting polynomial time efficiency. Compared to extant methodologies, the proposed algorithm finds the subspace with the lowest discordance, offering a smoother trade-off between sparsity and fit. Its architecture affords scalability, evidenced by a 16-fold improvement in computational speeds for matrices of 2000x2000 over CPU version. Furthermore, this method is
    
[^145]: 文本引导下的跨模态上下文扩散模型用于视觉生成与编辑

    Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing

    [https://arxiv.org/abs/2402.16627](https://arxiv.org/abs/2402.16627)

    提出了一种新颖且通用的上下文化扩散模型（ContextDiff），通过在正向和逆向过程中融入文本条件和视觉样本之间的交互和对齐，以便在视觉生成中更准确地传达文本语义

    

    有条件的扩散模型在高保真度文本引导的视觉生成和编辑中展现出卓越的性能。然而，当前的文本引导视觉扩散模型主要集中于将文本-视觉关系独占地融入到逆过程中，往往忽略了它们在正向过程中的相关性。这种正反过程之间的不一致可能限制了在视觉合成结果中精确传达文本语义。为了解决这个问题，我们提出了一种新颖且通用的上下文化扩散模型（ContextDiff），通过将跨模态上下文包含文本条件和视觉样本之间的交互和对齐融入到正向和逆向过程中。我们将这个上下文传播到两个过程中的所有时间步，以调整它们的轨迹，从而促进跨模态条件建模。我们将我们的上下文化扩散推广到DDPMs和...

    arXiv:2402.16627v2 Announce Type: cross  Abstract: Conditional diffusion models have exhibited superior performance in high-fidelity text-guided visual generation and editing. Nevertheless, prevailing text-guided visual diffusion models primarily focus on incorporating text-visual relationships exclusively into the reverse process, often disregarding their relevance in the forward process. This inconsistency between forward and reverse processes may limit the precise conveyance of textual semantics in visual synthesis results. To address this issue, we propose a novel and general contextualized diffusion model (ContextDiff) by incorporating the cross-modal context encompassing interactions and alignments between text condition and visual sample into forward and reverse processes. We propagate this context to all timesteps in the two processes to adapt their trajectories, thereby facilitating cross-modal conditional modeling. We generalize our contextualized diffusion to both DDPMs and 
    
[^146]: 使用预计算的嵌入相似性生成几乎实时个性化信息流

    Pfeed: Generating near real-time personalized feeds using precomputed embedding similarities

    [https://arxiv.org/abs/2402.16073](https://arxiv.org/abs/2402.16073)

    使用预计算的嵌入相似性生成个性化信息流，提高了电子商务平台上的客户参与度和体验，转化率提升4.9％。

    

    在个性化推荐系统中，通常使用嵌入来编码用户动作和项目，然后在嵌入空间中进行检索，使用近似最近邻搜索。然而，这种方法可能会导致两个挑战：1）用户嵌入可能限制所捕获的兴趣多样性，2）保持它们最新需要昂贵的实时基础设施。本文提出了一种在实际工业环境中克服这些挑战的方法。该方法动态更新客户配置文件，并每两分钟组成一个信息流，利用预计算的嵌入及其各自的相似性。我们在荷兰和比利时最大的电子商务平台之一Bol上测试并部署了这种方法，该方法提高了客户参与度和体验，导致转化率显著提高了4.9％。

    arXiv:2402.16073v1 Announce Type: cross  Abstract: In personalized recommender systems, embeddings are often used to encode customer actions and items, and retrieval is then performed in the embedding space using approximate nearest neighbor search. However, this approach can lead to two challenges: 1) user embeddings can restrict the diversity of interests captured and 2) the need to keep them up-to-date requires an expensive, real-time infrastructure. In this paper, we propose a method that overcomes these challenges in a practical, industrial setting. The method dynamically updates customer profiles and composes a feed every two minutes, employing precomputed embeddings and their respective similarities. We tested and deployed this method to personalise promotional items at Bol, one of the largest e-commerce platforms of the Netherlands and Belgium. The method enhanced customer engagement and experience, leading to a significant 4.9% uplift in conversions.
    
[^147]: Brant-2：脑信号基础模型

    Brant-2: Foundation Model for Brain Signals

    [https://arxiv.org/abs/2402.10251](https://arxiv.org/abs/2402.10251)

    Brant-2是脑信号领域最大的基础模型，相比于Brant，它不仅对数据变化和建模尺度具有稳健性，还能适用于更广泛范围的脑神经数据。

    

    基础模型受益于在大量未标记数据上进行预训练，并且在少量标记数据的情况下能够在各种应用中表现出色。这种模型在分析脑信号方面特别有效，因为这一领域涵盖了众多应用场景，并且进行大规模注释是成本高昂的。在这项工作中，我们提出了脑信号领域最大的基础模型，Brant-2。与用于颅内神经信号的基础模型Brant相比，Brant-2不仅对数据变化和建模尺度表现出稳健性，而且可以应用于更广泛范围的脑神经数据。通过在大量任务上进行实验，我们展示了Brant-2对脑信号中各种应用场景的适应性。进一步分析揭示了Brant-2的可扩展性，验证了每个组件的有效性，并展示了我们模型保持的能力。

    arXiv:2402.10251v1 Announce Type: cross  Abstract: Foundational models benefit from pre-training on large amounts of unlabeled data and enable strong performance in a wide variety of applications with a small amount of labeled data. Such models can be particularly effective in analyzing brain signals, as this field encompasses numerous application scenarios, and it is costly to perform large-scale annotation. In this work, we present the largest foundation model in brain signals, Brant-2. Compared to Brant, a foundation model designed for intracranial neural signals, Brant-2 not only exhibits robustness towards data variations and modeling scales but also can be applied to a broader range of brain neural data. By experimenting on an extensive range of tasks, we demonstrate that Brant-2 is adaptive to various application scenarios in brain signals. Further analyses reveal the scalability of the Brant-2, validate each component's effectiveness, and showcase our model's ability to maintai
    
[^148]: 时间交互图上的提示学习

    Prompt Learning on Temporal Interaction Graphs

    [https://arxiv.org/abs/2402.06326](https://arxiv.org/abs/2402.06326)

    这个论文提出了一种在时间交互图上进行提示学习的方法，以解决当前模型在预训练和下游预测阶段所面临的时间差异和语义差异的问题。

    

    时间交互图(TIGs)被广泛用于表示真实世界系统。为了促进在TIGs上的表示学习，研究人员提出了一系列的TIG模型。然而，这些模型在“预训练，预测”训练范式中依然面临着两个难题。首先，预训练和推理数据之间的时间差异严重削弱了模型在动态演化数据上进行遥远未来预测的适用性。其次，预文本任务和下游任务之间的语义差异阻碍了它们在实际应用中的使用，因为它们在应用场景中很难对齐其学习和预测能力。

    Temporal Interaction Graphs (TIGs) are widely utilized to represent real-world systems. To facilitate representation learning on TIGs, researchers have proposed a series of TIG models. However, these models are still facing two tough gaps between the pre-training and downstream predictions in their ``pre-train, predict'' training paradigm. First, the temporal discrepancy between the pre-training and inference data severely undermines the models' applicability in distant future predictions on the dynamically evolving data. Second, the semantic divergence between pretext and downstream tasks hinders their practical applications, as they struggle to align with their learning and prediction capabilities across application scenarios.   Recently, the ``pre-train, prompt'' paradigm has emerged as a lightweight mechanism for model generalization. Applying this paradigm is a potential solution to solve the aforementioned challenges. However, the adaptation of this paradigm to TIGs is not straig
    
[^149]: 《读玩游戏（R2-Play）: 多模态游戏指导下的决策 Transformer》

    Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction

    [https://arxiv.org/abs/2402.04154](https://arxiv.org/abs/2402.04154)

    本论文探索了为智能体提供增强形式的任务指导，使其能够理解游戏指导并实现"读玩游戏"的能力。通过将多模态指导调优的成功应用于视觉任务中的强化学习任务，构建了一组... (内容太长，无法继续显示)

    

    在人工智能领域，开发一款通用智能体一直是一个长期的目标。先前的研究利用来自各种任务的大量离线数据集，在强化学习的多任务场景中表现出了出色的性能。然而，这些工作在扩展到新任务方面面临挑战。最近的方法将文本指导或视觉轨迹整合到决策网络中，提供任务特定的上下文提示，代表了一个有前途的方向。然而，观察到仅依赖于文本指导或视觉轨迹对于准确传达任务的上下文信息是不足够的。本文探索了增强智能体任务指导的形式，使其能够理解游戏指导，从而实现"读玩游戏"的能力。受到多模态指导调优在视觉任务中的成功启发，我们将基于视觉的强化学习任务视为一个长期视觉任务，并构建了一组... (内容太长，无法继续显示)

    Developing a generalist agent is a longstanding objective in artificial intelligence. Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning.However, these works encounter challenges in extending their capabilities to new tasks.Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction.However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks.This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a "read-to-play" capability.Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set 
    
[^150]: Swin-UMamba：以Mamba为基础的具有ImageNet预训练的UNet模型

    Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining

    [https://arxiv.org/abs/2402.03302](https://arxiv.org/abs/2402.03302)

    Swin-UMamba是一种以Mamba为基础的新型UNet模型，通过结合局部特征和全局依赖的多尺度信息来实现准确的医学图像分割。与现有方法相比，Swin-UMamba具有更高的准确性、较低的内存消耗和更少的计算负担，并充分利用了预训练的威力。

    

    准确的医学图像分割需要整合从局部特征到全局依赖的多尺度信息。然而，现有方法很难建模长距离的全局信息，卷积神经网络受到其局部感受野的限制，而视觉变换器的注意力机制受到高二次复杂性的影响。最近，基于Mamba的模型因其在长序列建模方面的出色能力而受到广泛关注。几项研究表明，这些模型在各种任务中能够胜过流行的视觉模型，提供更高的准确性、更低的内存消耗和更少的计算负担。然而，现有的基于Mamba的模型大多是从头开始训练，没有充分利用预训练的威力，而预训练已被证明对于高效的医学图像分析非常有效。本文介绍了一种新颖的基于Mamba的模型Swin-UMamba，专为医学图像分割任务而设计。

    Accurate medical image segmentation demands the integration of multi-scale information, spanning from local features to global dependencies. However, it is challenging for existing methods to model long-range global information, where convolutional neural networks (CNNs) are constrained by their local receptive fields, and vision transformers (ViTs) suffer from high quadratic complexity of their attention mechanism. Recently, Mamba-based models have gained great attention for their impressive ability in long sequence modeling. Several studies have demonstrated that these models can outperform popular vision models in various tasks, offering higher accuracy, lower memory consumption, and less computational burden. However, existing Mamba-based models are mostly trained from scratch and do not explore the power of pretraining, which has been proven to be quite effective for data-efficient medical image analysis. This paper introduces a novel Mamba-based model, Swin-UMamba, designed speci
    
[^151]: GUARD: 通过角色扮演生成自然语言越狱来测试大型语言模型遵循指南的合规性

    GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models

    [https://arxiv.org/abs/2402.03299](https://arxiv.org/abs/2402.03299)

    本论文提出了一个通过角色扮演的系统，可以生成自然语言越狱，用于测试大型语言模型的指南遵循情况。系统通过收集现有越狱并将其组织成知识图来生成新的越狱，证明了其高效性和有效性。

    

    发现绕过大型语言模型（LLM）的安全过滤和有害回应的"越狱"已经鼓励社区采取安全措施。其中一个主要的安全措施是在发布之前用越狱主动测试LLM。因此，这样的测试将需要一种能够大规模且高效地生成越狱的方法。本文在追随一种新颖而直观的策略下，以人类生成的方式来生成越狱。我们提出了一个角色扮演系统，将四种不同角色分配给用户LLM，以便协作生成新的越狱。此外，我们收集现有的越狱，并通过句子逐句进行聚类频率和语义模式的划分，将它们分成不同的独立特征。我们将这些特征组织成一个知识图，使其更易于访问和检索。我们的角色系统将利用这个知识图来生成新的越狱，证明了其有效性。

    The discovery of "jailbreaks" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effec
    
[^152]: 马尔可夫说服过程：从零开始学会说服

    Markov Persuasion Processes: Learning to Persuade from Scratch

    [https://arxiv.org/abs/2402.03077](https://arxiv.org/abs/2402.03077)

    这篇论文提出了马尔可夫说服过程模型，用于捕捉发送者和接收者顺序交互的情景。论文解决了现有模型中的问题，提供了针对发送者没有环境知识的解决方案。通过设计学习算法，证明了算法的性能。这个方法的总结要点是提出了马尔可夫说服过程模型，并提出了针对没有环境知识的发送者的学习算法。

    

    在贝叶斯说服中，一个消息灵通的发送者可以策略性地向接收者透露信息，以说服他们采取期望的行动。最近，越来越多的关注点集中在发送者和接收者顺序交互的情境中。最近，引入了马尔可夫说服过程（MPPs）来捕捉在马尔可夫环境中，发送者面对一系列短视接收者的顺序情景。迄今为止，在文献中研究的MPPs存在一些问题，这些问题阻碍了它们在实践中的充分运作，例如，它们假设发送者知道接收者的奖励。我们通过处理发送者对环境没有任何了解的MPPs，解决了这些问题。我们设计了一个学习算法，用于发送者的部分反馈。我们证明了它的悔恨与最佳信息披露策略之间的差异以次线性增长，就像学习过程中累计的说服力损失一样。

    In Bayesian persuasion, an informed sender strategically discloses information to a receiver so as to persuade them to undertake desirable actions. Recently, a growing attention has been devoted to settings in which sender and receivers interact sequentially. Recently, Markov persuasion processes (MPPs) have been introduced to capture sequential scenarios where a sender faces a stream of myopic receivers in a Markovian environment. The MPPs studied so far in the literature suffer from issues that prevent them from being fully operational in practice, e.g., they assume that the sender knows receivers' rewards. We fix such issues by addressing MPPs where the sender has no knowledge about the environment. We design a learning algorithm for the sender, working with partial feedback. We prove that its regret with respect to an optimal information-disclosure policy grows sublinearly in the number of episodes, as it is the case for the loss in persuasiveness cumulated while learning. Moreover
    
[^153]: LOCOST: 长文档抽象摘要化的状态空间模型

    LOCOST: State-Space Models for Long Document Abstractive Summarization

    [https://arxiv.org/abs/2401.17919](https://arxiv.org/abs/2401.17919)

    LOCOST是一种基于状态空间模型的编码器-解码器架构，用于处理长文档的抽象摘要生成。与基于稀疏注意模式的最先进模型相比，LOCOST具有更低的计算复杂度，并且能够在训练和推断期间节省大量内存。在评估中，LOCOST在长文档摘要化任务上达到了93-96%的性能水平，并且能够处理超过600K个标记的输入文本。

    

    状态空间模型是编码长序列和捕捉长期依赖的低复杂度替代方案，我们提出了LOCOST：一种基于状态空间模型的编码器-解码器架构，用于具有长上下文输入的条件文本生成。这种架构的计算复杂度为O（L log L），可以处理比基于稀疏注意模式的最先进模型更长的序列。我们在一系列长文档抽象摘要化任务上评估了我们的模型。该模型在性能水平上达到了与相同大小的最优稀疏变压器相当的93-96%，同时在训练期间节省了高达50%的内存，在推断期间节省了高达87%的内存。此外，LOCOST有效地处理超过600K个标记的输入文本，为完整书摘要化设定了新的最新结果，并为长输入处理开辟了新的视角。

    State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of $O(L \log L)$, this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.
    
[^154]: MedLM: 探索用于医疗问答系统的语言模型

    MedLM: Exploring Language Models for Medical Question Answering Systems

    [https://arxiv.org/abs/2401.11389](https://arxiv.org/abs/2401.11389)

    本研究比较了用于医疗问答的通用和医学特定的精炼语言模型的表现，以填补领域特定任务中这些模型性能的研究空白。

    

    面对迅速扩大的在线医学文献，自动化系统用于聚合和总结信息对于医疗保健专业人员和患者变得日益关键。大型语言模型（LLM）以其先进的生成能力在各种自然语言处理任务中显示出潜力，它们在医疗领域的潜力，特别是在封闭式生成问答方面，是显著的。然而，这些模型在医学问答等领域特定任务中的性能仍然较少被探索。本研究旨在通过比较通用和专门用于医学的精炼语言模型在医疗问答中的表现来填补这一空白。我们旨在评估微调领域特定语言模型的效果，并比较不同类型语言模型的性能。本研究将探讨这些模型的可靠性、比较性能和在医疗问答背景下的有效性等关键问题。

    arXiv:2401.11389v2 Announce Type: replace-cross  Abstract: In the face of rapidly expanding online medical literature, automated systems for aggregating and summarizing information are becoming increasingly crucial for healthcare professionals and patients. Large Language Models (LLMs), with their advanced generative capabilities, have shown promise in various NLP tasks, and their potential in the healthcare domain, particularly for Closed-Book Generative QnA, is significant. However, the performance of these models in domain-specific tasks such as medical Q&A remains largely unexplored. This study aims to fill this gap by comparing the performance of general and medical-specific distilled LMs for medical Q&A. We aim to evaluate the effectiveness of fine-tuning domain-specific LMs and compare the performance of different families of Language Models. The study will address critical questions about these models' reliability, comparative performance, and effectiveness in the context of me
    
[^155]: SceneVerse：为基于场景的场景理解扩展3D视觉-语言学习

    SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding

    [https://arxiv.org/abs/2401.09340](https://arxiv.org/abs/2401.09340)

    本研究通过系统性地扩展室内环境中的3D视觉-语言学习，提出了首个百万规模的3D视觉-语言数据集SceneVerse，以解决3D视觉-语言对齐面临的几个重要挑战。

    

    3D视觉-语言对齐，即将语言与3D物理环境对齐，是发展具身体能力的智能体的基石。与2D领域最近的进展相比，将语言与3D场景对齐面临着几个重要挑战：（i）3D场景固有复杂性，由于多样的物体配置、丰富的属性和错综复杂的关系；（ii）支持基于场景学习的配对3D视觉-语言数据的稀缺性；以及（iii）缺乏从基于场景的3D数据中提炼知识的统一学习框架。在这项工作中，我们旨在通过系统地扩展室内环境中的3D视觉-语言学习，从而解决3D视觉-语言领域中的这三大挑战。我们介绍首个百万规模的3D视觉-语言数据集SceneVerse，包含约68K个3D室内场景，包括250万个视觉语言

    arXiv:2401.09340v2 Announce Type: replace-cross  Abstract: 3D vision-language grounding, which focuses on aligning language with the 3D physical environment, stands as a cornerstone in the development of embodied agents. In comparison to recent advancements in the 2D domain, grounding language in 3D scenes faces several significant challenges: (i) the inherent complexity of 3D scenes due to the diverse object configurations, their rich attributes, and intricate relationships; (ii) the scarcity of paired 3D vision-language data to support grounded learning; and (iii) the absence of a unified learning framework to distill knowledge from grounded 3D data. In this work, we aim to address these three major challenges in 3D vision-language by examining the potential of systematically upscaling 3D vision-language learning in indoor environments. We introduce the first million-scale 3D vision-language dataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising 2.5M vision-langu
    
[^156]: 谁说了什么？分析学前课堂言语的自动化方法

    Who Said What? An Automated Approach to Analyzing Speech in Preschool Classrooms

    [https://arxiv.org/abs/2401.07342](https://arxiv.org/abs/2401.07342)

    提出了一个自动化框架，使用儿童和教师佩戴的录音设备记录言语，实现说话者分类和转录，与人类专家对比结果表明，框架整体准确率达到0.76，为学前教室言语分析提供了新思路

    

    年幼儿童在喧闹的学前课堂中度过大部分清醒时间。在这种环境中，孩子与教师之间的言语互动对他们的语言结果至关重要，但手动转录这些互动是困难的。本文提出了一个使用儿童和教师佩戴的录音设备录音的自动化框架，该框架使用开源软件对说话者进行分类（ALICE），并转录他们的话语（Whisper）。我们将框架的结果与人类专家对110分钟教室录音（包括来自儿童话筒的85分钟（4名儿童）和来自教师话筒的25分钟（2名教师））的结果进行了比较。总体一致比例，即正确分类的教师和儿童话语的比例为0.76，矫正的kappa为0.50，加权F1为0.76。教师和儿童的话语的词误率

    arXiv:2401.07342v2 Announce Type: replace-cross  Abstract: Young children spend substantial portions of their waking hours in noisy preschool classrooms. In these environments, children's vocal interactions with teachers are critical contributors to their language outcomes, but manually transcribing these interactions is prohibitive. Using audio from child- and teacher-worn recorders, we propose an automated framework that uses open source software both to classify speakers (ALICE) and to transcribe their utterances (Whisper). We compare results from our framework to those from a human expert for 110 minutes of classroom recordings, including 85 minutes from child-word microphones (n=4 children) and 25 minutes from teacher-worn microphones (n=2 teachers). The overall proportion of agreement, that is, the proportion of correctly classified teacher and child utterances, was .76, with an error-corrected kappa of .50 and a weighted F1 of .76. The word error rate for both teacher and child 
    
[^157]: 无监督学习下理解深度神经网络中概念的分布表示

    Understanding Distributed Representations of Concepts in Deep Neural Networks without Supervision

    [https://arxiv.org/abs/2312.17285](https://arxiv.org/abs/2312.17285)

    本文提出了一种无监督方法，通过选择主要神经元来发现概念的分布表示，可以用于识别数据中的未标记子类和检测误分类的原因。

    

    理解深度学习分类器学习的概念的中间表示对解释模型的一般行为至关重要。现有揭示学习概念的方法通常依赖于人类监督，例如预定义的概念集或分割过程。本文提出了一种新颖的无监督方法，通过选择主要子集的神经元来发现概念的分布表示。我们的实证结果表明，具有类似神经元激活状态的实例往往共享一致的概念。根据观察，所提出的方法选择构建可解释区域的主要神经元，即涵盖特征空间中具有一致概念的实例的放松决策区域（RDR）。它可用于识别数据中的未标记子类并检测误分类的原因。此外，我们的方法可应用于

    arXiv:2312.17285v2 Announce Type: replace-cross  Abstract: Understanding intermediate representations of the concepts learned by deep learning classifiers is indispensable for interpreting general model behaviors. Existing approaches to reveal learned concepts often rely on human supervision, such as pre-defined concept sets or segmentation processes. In this paper, we propose a novel unsupervised method for discovering distributed representations of concepts by selecting a principal subset of neurons. Our empirical findings demonstrate that instances with similar neuron activation states tend to share coherent concepts. Based on the observations, the proposed method selects principal neurons that construct an interpretable region, namely a Relaxed Decision Region (RDR), encompassing instances with coherent concepts in the feature space. It can be utilized to identify unlabeled subclasses within data and to detect the causes of misclassifications. Furthermore, the applicability of our 
    
[^158]: 参数化投影贝尔曼算子

    Parameterized Projected Bellman Operator

    [https://arxiv.org/abs/2312.12869](https://arxiv.org/abs/2312.12869)

    本论文提出了一种基于学习的近似贝尔曼算子的新方法，以解决近似值迭代算法中样本不确定性和计算复杂度的问题。

    

    近似值迭代（AVI）是一类用于强化学习（RL）的算法家族，旨在获得最优值函数的近似。通常，AVI算法采用迭代过程，每个步骤包括（i）贝尔曼算子的应用和（ii）投影步骤到考虑的函数空间中。众所周知，贝尔曼算子利用转移样本，这些样本强烈影响其行为，因为无信息的样本可能导致可忽略的更新或长时间的绕行，而计算密集的投影步骤进一步加剧了这些不利影响。为了解决这些问题，我们提出了一种新颖的替代方法，该方法采用学习的方式得到贝尔曼算子的近似版本，而不是像AVI方法那样通过样本进行估计。通过这种方式，我们能够（i）在转移样本之间进行泛化，（ii）避免计算密集的投影步骤。因此，我们称我们的新算子为"projec"算子。

    Approximate value iteration (AVI) is a family of algorithms for reinforcement learning (RL) that aims to obtain an approximation of the optimal value function. Generally, AVI algorithms implement an iterated procedure where each step consists of (i) an application of the Bellman operator and (ii) a projection step into a considered function space. Notoriously, the Bellman operator leverages transition samples, which strongly determine its behavior, as uninformative samples can result in negligible updates or long detours, whose detrimental effects are further exacerbated by the computationally intensive projection step. To address these issues, we propose a novel alternative approach based on learning an approximate version of the Bellman operator rather than estimating it through samples as in AVI approaches. This way, we are able to (i) generalize across transition samples and (ii) avoid the computationally intensive projection step. For this reason, we call our novel operator projec
    
[^159]: TSRNet：多模态时间和谱图恢复网络实时心电图异常检测的简单框架

    TSRNet: Simple Framework for Real-time ECG Anomaly Detection with Multimodal Time and Spectrogram Restoration Network

    [https://arxiv.org/abs/2312.10187](https://arxiv.org/abs/2312.10187)

    本论文提出了TSRNet框架，利用多模态时间和谱图恢复网络进行实时心电图异常检测，通过恢复正常ECG数据训练，结合时间序列和时频领域，实现了对心电图信号异常的检测。

    

    心电图（ECG）是一种有价值的信号，用于评估心脏健康的各个方面，如心率和节律。它在识别心脏状况和检测ECG数据中的异常方面起着关键作用。然而，区分正常和异常ECG信号可能是一项具有挑战性的任务。在本文中，我们提出了一种利用异常检测来识别不健康状况的方法，仅使用正常ECG数据进行训练。此外，为了增强可用信息并构建一个强大的系统，我们建议考虑ECG信号的时间序列和时频领域两个方面。因此，我们引入了一种专门用于检测ECG信号异常的名为多模态时间和谱图恢复网络（TSRNet）的网络。TSRNet属于基于恢复的异常检测类别，灵感来源于时间序列和时频领域。

    arXiv:2312.10187v2 Announce Type: replace-cross  Abstract: The electrocardiogram (ECG) is a valuable signal used to assess various aspects of heart health, such as heart rate and rhythm. It plays a crucial role in identifying cardiac conditions and detecting anomalies in ECG data. However, distinguishing between normal and abnormal ECG signals can be a challenging task. In this paper, we propose an approach that leverages anomaly detection to identify unhealthy conditions using solely normal ECG data for training. Furthermore, to enhance the information available and build a robust system, we suggest considering both the time series and time-frequency domain aspects of the ECG signal. As a result, we introduce a specialized network called the Multimodal Time and Spectrogram Restoration Network (TSRNet) designed specifically for detecting anomalies in ECG signals. TSRNet falls into the category of restoration-based anomaly detection and draws inspiration from both the time series and sp
    
[^160]: 可穿戴生物信号基础模型的大规模训练

    Large-scale Training of Foundation Models for Wearable Biosignals

    [https://arxiv.org/abs/2312.05409](https://arxiv.org/abs/2312.05409)

    利用自监督学习和大规模可穿戴设备数据，本研究训练了基础模型用于衡量光电容积描记（PPG）和心电图信号，以解决医学数据集规模较小的难题。

    

    跟踪生物信号对于监测健康状况并预防严重医学状况的发展至关重要。如今，可穿戴设备可以方便地记录各种生物信号，从而有机会在不干扰日常生活的情况下监测健康状况。虽然可穿戴设备被广泛使用且存在数字生物标志物，但缺乏带有注释医学标签的筛选数据，阻碍了开发衡量常见健康状况的新生物标志物。事实上，与其他领域相比，医学数据集通常较小，这是开发生物信号神经网络模型的障碍。为解决这一挑战，我们利用在知情同意下从大规模纵向Apple心脏和运动研究（AHMS）中收集的未标记传感器数据，采用自监督学习，为两种常见生物信号（光电容积描记（PPG）和心电图）

    arXiv:2312.05409v2 Announce Type: replace-cross  Abstract: Tracking biosignals is crucial for monitoring wellness and preempting the development of severe medical conditions. Today, wearable devices can conveniently record various biosignals, creating the opportunity to monitor health status without disruption to one's daily routine. Despite widespread use of wearable devices and existing digital biomarkers, the absence of curated data with annotated medical labels hinders the development of new biomarkers to measure common health conditions. In fact, medical datasets are usually small in comparison to other domains, which is an obstacle for developing neural network models for biosignals. To address this challenge, we have employed self-supervised learning using the unlabeled sensor data collected under informed consent from the large longitudinal Apple Heart and Movement Study (AHMS) to train foundation models for two common biosignals: photoplethysmography (PPG) and electrocardiogra
    
[^161]: 通过公平映射实现公平文本到图像扩散

    Fair Text-to-Image Diffusion via Fair Mapping

    [https://arxiv.org/abs/2311.17695](https://arxiv.org/abs/2311.17695)

    本文提出了一种通过Fair Mapping控制模型提示来修改文本到图像扩散模型，实现公平图像生成的方法，具有高效性和能够生成相对平衡人口统计结果的优势。

    

    在本文中，我们解决了现有文本到图像扩散模型在生成与人类相关描述时出现人口统计上公平结果的局限性。这些模型经常难以将目标语言环境与社会文化偏见分离开，导致生成偏见图像。为了克服这一挑战，我们提出了一种灵活、与模型无关且轻量级的方法Fair Mapping，通过控制提示来修改预训练的文本到图像扩散模型，从而实现公平图像生成。我们方法的一个关键优势在于其高效性。它只需要以低计算成本更新少量参数的额外线性网络。通过开发一个将条件嵌入映射到去偏空间的线性网络，我们能够根据指定的文本条件生成相对平衡的人口统计结果。

    arXiv:2311.17695v2 Announce Type: replace-cross  Abstract: In this paper, we address the limitations of existing text-to-image diffusion models in generating demographically fair results when given human-related descriptions. These models often struggle to disentangle the target language context from sociocultural biases, resulting in biased image generation. To overcome this challenge, we propose Fair Mapping, a flexible, model-agnostic, and lightweight approach that modifies a pre-trained text-to-image diffusion model by controlling the prompt to achieve fair image generation. One key advantage of our approach is its high efficiency. It only requires updating an additional linear network with few parameters at a low computational cost. By developing a linear network that maps conditioning embeddings into a debiased space, we enable the generation of relatively balanced demographic results based on the specified text condition. With comprehensive experiments on face image generation, 
    
[^162]: 线性赌臂的集成抽样：小集成足矣

    Ensemble sampling for linear bandits: small ensembles suffice

    [https://arxiv.org/abs/2311.08376](https://arxiv.org/abs/2311.08376)

    该论文对随机线性赌臂环境中的集成抽样进行了首次实用和严格的分析，展示了在标准假设下，采用规模为$d \log T$的集成抽样可以获得接近$\sqrt{T}$阶的后悔，而不需要集成大小与$T$线性扩展。

    

    我们首次对随机线性赌臂设定下的集成抽样进行了有用且严谨的分析。特别地，我们展示了在标准假设下，对于一个具有交互作用时间跨度$T$的$d$维随机线性赌臂，采用集成大小为$\smash{d \log T}$的集成抽样，遭受的后悔最多为$\smash{(d \log T)^{5/2} \sqrt{T}}$阶。我们的结果是在任何结构化环境中第一个不要求集成大小与$T$线性扩展的结果，这使得集成抽样失去意义，同时获得了接近$\smash{\sqrt{T}}$阶的后悔。我们的结果也是第一个允许无限动作集的结果。

    arXiv:2311.08376v2 Announce Type: replace-cross  Abstract: We provide the first useful and rigorous analysis of ensemble sampling for the stochastic linear bandit setting. In particular, we show that, under standard assumptions, for a $d$-dimensional stochastic linear bandit with an interaction horizon $T$, ensemble sampling with an ensemble of size of order $\smash{d \log T}$ incurs regret at most of the order $\smash{(d \log T)^{5/2} \sqrt{T}}$. Ours is the first result in any structured setting not to require the size of the ensemble to scale linearly with $T$ -- which defeats the purpose of ensemble sampling -- while obtaining near $\smash{\sqrt{T}}$ order regret. Ours is also the first result that allows infinite action sets.
    
[^163]: 通过线性扰动的损失最小化来进行探索

    Exploration via linearly perturbed loss minimisation

    [https://arxiv.org/abs/2311.07565](https://arxiv.org/abs/2311.07565)

    提出了一种名为EVILL的随机探索方法，通过解决线性扰动的负对数似然函数的极小化问题来工作，提供了关于随机奖励扰动产生良好赌博算法的简洁解释，并在实践中展示了与汤普森抽样风格参数扰动方法性能相匹配的能力

    

    我们引入了一种称为通过线性损失扰动进行探索（EVILL）的随机探索方法，用于结构化随机赌博问题，该方法通过解决线性扰动正则化负对数似然函数的极小化问题来工作。我们展示，对于广义线性赌博问题，EVILL可以简化为扰动历史探索（PHE），一种通过在随机扰动奖励上进行训练来进行探索的方法。通过这样做，我们对随机奖励扰动何时以及为何产生良好的赌博算法提供了简单干净的解释。我们提出了先前PHE类型方法中不含的数据相关扰动，使EVILL能够在理论和实践中与汤普森抽样风格参数扰动方法的性能相匹配。此外，我们展示了一个超出广义线性赌博的例子，其中PHE导致不一致的估计，从而导致线性后悔，而EVILL则保持表现。

    arXiv:2311.07565v2 Announce Type: replace  Abstract: We introduce exploration via linear loss perturbations (EVILL), a randomised exploration method for structured stochastic bandit problems that works by solving for the minimiser of a linearly perturbed regularised negative log-likelihood function. We show that, for the case of generalised linear bandits, EVILL reduces to perturbed history exploration (PHE), a method where exploration is done by training on randomly perturbed rewards. In doing so, we provide a simple and clean explanation of when and why random reward perturbations give rise to good bandit algorithms. We propose data-dependent perturbations not present in previous PHE-type methods that allow EVILL to match the performance of Thompson-sampling-style parameter-perturbation methods, both in theory and in practice. Moreover, we show an example outside generalised linear bandits where PHE leads to inconsistent estimates, and thus linear regret, while EVILL remains performa
    
[^164]: 从耦合振荡器到图神经网络：基于Kuramoto模型的方法减轻过度平滑现象

    From Coupled Oscillators to Graph Neural Networks: Reducing Over-smoothing via a Kuramoto Model-based Approach

    [https://arxiv.org/abs/2311.03260](https://arxiv.org/abs/2311.03260)

    提出了一种新型的连续深度图神经网络KuramotoGNN，通过采用Kuramoto模型来减轻GNN中的过度平滑现象，实现节点特征的差异化，取得了优于基线GNN和现有方法的实验效果。

    

    我们提出了Kuramoto图神经网络（KuramotoGNN），一种新颖的连续深度图神经网络（GNN），它采用Kuramoto模型来缓解过度平滑现象，即随着层数增加，GNN中节点特征变得难以区分的问题。Kuramoto模型捕捉了非线性耦合振荡器的同步行为。从耦合振荡器的视角，我们首先展示了Kuramoto模型与基本GNN之间的联系，然后说明了GNN中的过度平滑现象可以被解释为Kuramoto模型中的相位同步。KuramotoGNN用频率同步取代了这种相位同步，以防止节点特征收敛到一起，同时使系统达到稳定的同步状态。我们在各种实验中验证了KuramotoGNN在减少过度平滑方面相对于基线GNN和现有方法的优势。

    arXiv:2311.03260v2 Announce Type: replace-cross  Abstract: We propose the Kuramoto Graph Neural Network (KuramotoGNN), a novel class of continuous-depth graph neural networks (GNNs) that employs the Kuramoto model to mitigate the over-smoothing phenomenon, in which node features in GNNs become indistinguishable as the number of layers increases. The Kuramoto model captures the synchronization behavior of non-linear coupled oscillators. Under the view of coupled oscillators, we first show the connection between Kuramoto model and basic GNN and then over-smoothing phenomenon in GNNs can be interpreted as phase synchronization in Kuramoto model. The KuramotoGNN replaces this phase synchronization with frequency synchronization to prevent the node features from converging into each other while allowing the system to reach a stable synchronized state. We experimentally verify the advantages of the KuramotoGNN over the baseline GNNs and existing methods in reducing over-smoothing on various 
    
[^165]: 我的大数据中有什么？

    What's In My Big Data?

    [https://arxiv.org/abs/2310.20707](https://arxiv.org/abs/2310.20707)

    通过提出的平台和分析方法，我们揭示和比较了大型文本语料库的内容，发现了关于语料库内容的几个令人惊讶且以前未被记录的发现。

    

    大型文本语料库是语言模型的基础。然而，我们对这些语料库的内容，包括一般统计信息、质量、社会因素和评估数据（污染）的理解有限。在这项工作中，我们提出了“What's In My Big Data?”（WIMBD），这是一个平台和一组十六个分析方法，使我们能够揭示和比较大型文本语料库的内容。WIMBD基于两种基本能力——计数和搜索——在规模上进行，这使我们能够在标准计算节点上分析超过35TB的数据。我们将WIMBD应用于用于训练流行语言模型的十个不同语料库，包括C4、The Pile和RedPajama。我们的分析揭示了关于这些语料库的几个令人惊讶且以前未记录的发现，包括重复内容、合成内容、低质量内容、个人可识别信息、有毒语言和基准污染的高流行率。

    arXiv:2310.20707v2 Announce Type: replace  Abstract: Large text corpora are the backbone of language models. However, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination). In this work, we propose What's In My Big Data? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities -- count and search -- at scale, which allows us to analyze more than 35 terabytes on a standard compute node. We apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama. Our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. For instance,
    
[^166]: 基于Transformer的卫星图像对雷达复合物的即时预报，用于严重天气

    Transformer-based nowcasting of radar composites from satellite images for severe weather

    [https://arxiv.org/abs/2310.19515](https://arxiv.org/abs/2310.19515)

    该论文提出了一种基于Transformer的模型，利用卫星数据对地面雷达图像序列进行即时预报，能够有效弥合地面和空间观测之间的差距，提高天气预测的准确性。

    

    天气雷达数据对于即时预报至关重要，并且是数值天气预测模型的一个组成部分。尽管天气雷达数据提供了高分辨率的宝贵信息，但其基于地面的特性限制了其可用性，这阻碍了大规模应用。相比之下，气象卫星覆盖了更大的区域，但分辨率更低。然而，随着数据驱动方法和地球同步卫星上现代传感器的快速发展，新的机会正在出现，以弥合地面和空间观测之间的差距，最终实现更准确的高技能天气预报。在这里，我们提出了一种基于Transformer模型的方法，利用卫星数据对地面雷达图像序列进行即时预报，提前两小时。该模型在反映严重天气条件的数据集上进行训练，预测出现在不同天气现象下的雷达场。

    arXiv:2310.19515v2 Announce Type: replace-cross  Abstract: Weather radar data are critical for nowcasting and an integral component of numerical weather prediction models. While weather radar data provide valuable information at high resolution, their ground-based nature limits their availability, which impedes large-scale applications. In contrast, meteorological satellites cover larger domains but with coarser resolution. However, with the rapid advancements in data-driven methodologies and modern sensors aboard geostationary satellites, new opportunities are emerging to bridge the gap between ground- and space-based observations, ultimately leading to more skillful weather prediction with high accuracy. Here, we present a Transformer-based model for nowcasting ground-based radar image sequences using satellite data up to two hours lead time. Trained on a dataset reflecting severe weather conditions, the model predicts radar fields occurring under different weather phenomena and show
    
[^167]: 具有闭环个性化课程的连续驾驶政策优化

    Continual Driving Policy Optimization with Closed-Loop Individualized Curricula

    [https://arxiv.org/abs/2309.14209](https://arxiv.org/abs/2309.14209)

    开发了连续驾驶政策优化框架，提出了闭环个性化课程（CLIC）概念，允许重复利用广泛场景来迭代改进自主驾驶车辆模型。

    

    自主驾驶车辆（AV）的安全一直是一个长期的头等关注点，根源于长尾自然驾驶分布中罕见的安全关键场景的缺失。为了解决这一挑战，出现了大量基于场景的自动驾驶研究，重点是生成高风险驾驶场景并将它们应用于对AV模型进行安全关键测试。然而，有限的工作探讨了重复利用这些广泛场景来迭代改进AV模型。此外，从具有不同行为的其他AV模型收集的巨大场景库中滤出可传递信息以改进当前AV仍然是难以解决的且具有挑战性。因此，我们开发了一个具有闭环个性化课程（CLIC）特点的连续驾驶政策优化框架，我们将其分解为一组标准化的子模块。

    arXiv:2309.14209v3 Announce Type: replace-cross  Abstract: The safety of autonomous vehicles (AV) has been a long-standing top concern, stemming from the absence of rare and safety-critical scenarios in the long-tail naturalistic driving distribution. To tackle this challenge, a surge of research in scenario-based autonomous driving has emerged, with a focus on generating high-risk driving scenarios and applying them to conduct safety-critical testing of AV models. However, limited work has been explored on the reuse of these extensive scenarios to iteratively improve AV models. Moreover, it remains intractable and challenging to filter through gigantic scenario libraries collected from other AV models with distinct behaviors, attempting to extract transferable information for current AV improvement. Therefore, we develop a continual driving policy optimization framework featuring Closed-Loop Individualized Curricula (CLIC), which we factorize into a set of standardized sub-modules for
    
[^168]: 数据同化的神经Koopman先验

    Neural Koopman prior for data assimilation

    [https://arxiv.org/abs/2309.05317](https://arxiv.org/abs/2309.05317)

    本文介绍了一种利用神经Koopman先验进行数据同化的方法，将动态系统嵌入到潜在空间，使得对其动态进行线性描述，并展示了长期连续重构和自监督学习的潜力。

    

    随着大规模数据集、计算能力和诸如自动微分和表达能力强的神经网络架构等工具的日益普及，序贯数据现在经常以数据驱动的方式处理，通过从观测数据训练动力学模型。尽管神经网络经常被视为不可解释的黑盒架构，但它们仍然可以受益于数据的物理先验和数学知识。在本文中，我们使用了一种神经网络架构，该架构利用长期以来已知的Koopman算子理论，将动力系统嵌入到潜在空间中，其中它们的动态可以被线性描述，从而呈现出许多吸引人的特性。我们引入了使该模型能够进行长期连续重构的方法，即使在数据呈不规则采样时间序列的困难情境中也可以顺利进行。我们还展示了自监督学习的潜力，因为我们展示

    arXiv:2309.05317v2 Announce Type: replace  Abstract: With the increasing availability of large scale datasets, computational power and tools like automatic differentiation and expressive neural network architectures, sequential data are now often treated in a data-driven way, with a dynamical model trained from the observation data. While neural networks are often seen as uninterpretable black-box architectures, they can still benefit from physical priors on the data and from mathematical knowledge. In this paper, we use a neural network architecture which leverages the long-known Koopman operator theory to embed dynamical systems in latent spaces where their dynamics can be described linearly, enabling a number of appealing features. We introduce methods that enable to train such a model for long-term continuous reconstruction, even in difficult contexts where the data comes in irregularly-sampled time series. The potential for self-supervised learning is also demonstrated, as we show
    
[^169]: 正态分布不可区分性谱及其在隐私保护机器学习中的应用

    The Normal Distributions Indistinguishability Spectrum and its Application to Privacy-Preserving Machine Learning

    [https://arxiv.org/abs/2309.01243](https://arxiv.org/abs/2309.01243)

    本文提出了正态分布不可区分性谱定理 (NDIS Theorem)，旨在利用查询本身的随机性改进随机化机器学习查询的差分隐私机制。

    

    要实现差分隐私(DP)，通常需要随机化基础查询的输出。在大数据分析中，人们经常使用随机化草图/聚合算法来使处理高维数据变得可行。直观地，这样的机器学习(ML)算法应该提供一些固有的隐私性，但现有的大部分DP机制并没有利用这种固有的随机性，导致潜在的多余噪音。我们工作的动机问题是：(如何)可以通过利用查询本身的随机性来提高随机化ML查询的DP机制的效用？为了给出积极的答案，我们证明了正态分布不可区分性谱定理(简称为NDIS定理)，这是一个具有深远实际影响的理论结果。总的来说，NDIS是一个用于$(\epsilon,\delta)$-不可区分性谱(简称为$

    arXiv:2309.01243v2 Announce Type: replace-cross  Abstract: To achieve differential privacy (DP) one typically randomizes the output of the underlying query. In big data analytics, one often uses randomized sketching/aggregation algorithms to make processing high-dimensional data tractable. Intuitively, such machine learning (ML) algorithms should provide some inherent privacy, yet most if not all existing DP mechanisms do not leverage this inherent randomness, resulting in potentially redundant noising.   The motivating question of our work is:   (How) can we improve the utility of DP mechanisms for randomized ML queries, by leveraging the randomness of the query itself?   Towards a (positive) answer, we prove the Normal Distributions Indistinguishability Spectrum Theorem (in short, NDIS Theorem), a theoretical result with far-reaching practical implications. In a nutshell, NDIS is a closed-form analytic computation for the $(\epsilon,\delta)$-indistinguishability-spectrum (in short, $
    
[^170]: PURL: 链接装饰的安全有效清洁

    PURL: Safe and Effective Sanitization of Link Decoration

    [https://arxiv.org/abs/2308.03417](https://arxiv.org/abs/2308.03417)

    使用机器学习方法的PURL能够安全有效地清洁链接装饰，显着优于现有对策，而且对常见规避技术具有很强的鲁棒性。

    

    尽管以隐私为重点的浏览器已经采取了阻止第三方cookie和减少浏览器指纹识别的措施，但仍然不断出现可以绕过现有对策的新型跟踪技术。鉴于跟踪器需要通过链接装饰从客户端向服务器端共享信息，无论他们采用何种跟踪技术，一个有前途的方法是检测和清洁装饰链接中的跟踪信息。为此，我们提出了PURL（发音为purel-l），这是一种利用网页执行的跨层图表示来安全有效地清洁链接装饰的机器学习方法。我们的评估显示，PURL在准确性和减少网站破坏方面明显优于现有的对策，并且对常见的规避技术具有很强的鲁棒性。在一部分前一百万网站上部署PURL的结果显示，链接装饰被滥用用于跟踪。

    arXiv:2308.03417v2 Announce Type: replace-cross  Abstract: While privacy-focused browsers have taken steps to block third-party cookies and mitigate browser fingerprinting, novel tracking techniques that can bypass existing countermeasures continue to emerge. Since trackers need to share information from the client-side to the server-side through link decoration regardless of the tracking technique they employ, a promising orthogonal approach is to detect and sanitize tracking information in decorated links. To this end, we present PURL (pronounced purel-l), a machine-learning approach that leverages a cross-layer graph representation of webpage execution to safely and effectively sanitize link decoration. Our evaluation shows that PURL significantly outperforms existing countermeasures in terms of accuracy and reducing website breakage while being robust to common evasion techniques. PURL's deployment on a sample of top-million websites shows that link decoration is abused for trackin
    
[^171]: THC：使用张量同态压缩加速分布式深度学习

    THC: Accelerating Distributed Deep Learning Using Tensor Homomorphic Compression

    [https://arxiv.org/abs/2302.08545](https://arxiv.org/abs/2302.08545)

    引入了Tensor Homomorphic Compression (THC)，一种新颖的双向压缩框架，可以加速分布式深度学习中的模型训练

    

    深度神经网络（DNNs）已经成为必要用例（如图像分类、计算机视觉和自然语言处理）的事实标准。随着DNNs和数据集变得越来越大，它们需要在越来越大的集群上进行分布式训练。 主要瓶颈是由工作者在每轮基础上交换模型更新（即梯度）产生的通信开销。 为了解决这一瓶颈并加速训练，一个广泛部署的方法是压缩。 但是，先前的部署通常只是在每个方向上使用单方向梯度压缩方案来应用双向压缩方案。 这导致参数服务器上的显着计算开销和压缩误差增加，从而导致训练时间更长和准确性更低。 我们介绍了张量同态压缩（THC），这是一种新颖的双向压缩框架，能够直接聚合

    arXiv:2302.08545v2 Announce Type: replace-cross  Abstract: Deep neural networks (DNNs) are the de facto standard for essential use cases, such as image classification, computer vision, and natural language processing. As DNNs and datasets get larger, they require distributed training on increasingly larger clusters. A main bottleneck is the resulting communication overhead where workers exchange model updates (i.e., gradients) on a per-round basis. To address this bottleneck and accelerate training, a widely-deployed approach is compression. However, previous deployments often apply bi-directional compression schemes by simply using a uni-directional gradient compression scheme in each direction. This results in significant computational overheads at the parameter server and increased compression error, leading to longer training and lower accuracy. We introduce Tensor Homomorphic Compression (THC), a novel bi-directional compression framework that enables the direct aggregation of com
    
[^172]: 发现种植的二分图检测

    Planted Bipartite Graph Detection

    [https://arxiv.org/abs/2302.03658](https://arxiv.org/abs/2302.03658)

    该论文研究了在随机图中检测隐藏的种植二分图子图的问题，并对其统计和计算障碍进行了表征。

    

    我们考虑在给定的随机图中检测隐藏的二分图子图的任务。这被规划为一个假设检验问题，在零假设下，图是具有$n$个顶点和边密度$q$的Erd\H{o}s-R\'{e}nyi随机图的一个实现。在备择假设下，存在一个种植的$k_{\mathsf{R}} \times k_{\mathsf{L}}$二分图子图，边密度为$p>q$。我们对这个问题的统计和计算障碍进行了表征。具体地，我们得出了信息论下界，并设计和分析匹配这些下界的最优算法，在密集区域和稀疏区域都是如此，其中$p,q = \Theta\left(1\right)$在密集区域，$p,q = \Theta\left(n^{-\alpha}\right), \alpha \in \left(0,2\right]$在稀疏区域。我们还考虑了多项式时间内的测试问题。与类似结构化高维问题一样，我们的模型经历了"easy-hard-impossible"阶段

    arXiv:2302.03658v2 Announce Type: replace-cross  Abstract: We consider the task of detecting a hidden bipartite subgraph in a given random graph. This is formulated as a hypothesis testing problem, under the null hypothesis, the graph is a realization of an Erd\H{o}s-R\'{e}nyi random graph over $n$ vertices with edge density $q$. Under the alternative, there exists a planted $k_{\mathsf{R}} \times k_{\mathsf{L}}$ bipartite subgraph with edge density $p>q$. We characterize the statistical and computational barriers for this problem. Specifically, we derive information-theoretic lower bounds, and design and analyze optimal algorithms matching those bounds, in both the dense regime, where $p,q = \Theta\left(1\right)$, and the sparse regime where $p,q = \Theta\left(n^{-\alpha}\right), \alpha \in \left(0,2\right]$. We also consider the problem of testing in polynomial-time. As is customary in similar structured high-dimensional problems, our model undergoes an "easy-hard-impossible" phase t
    
[^173]: 精确分数推断：重新参数化及树重新加权和信念传播算法之间的插值

    Exact Fractional Inference via Re-Parametrization & Interpolation between Tree-Re-Weighted- and Belief Propagation- Algorithms

    [https://arxiv.org/abs/2301.10369](https://arxiv.org/abs/2301.10369)

    通过构建$\lambda$-分数全同维数，实现了树重新加权和信念传播算法之间的插值，确保在铁磁性情况下，存在“精确”$\lambda_*$使得计算的配分函数$Z=Z^{(\lambda_*)}$。

    

    推断工作--计算Ising模型在N个“自旋”组成的图上的配分函数$Z$所需的工作--很可能随着N呈指数增长。高效的变分方法，如信念传播（BP）和树重新加权（TRW）算法，通过计算最小化各自（BP或TRW）自由能的$Z$来近似计算$Z$。我们通过构建一个$\lambda$-分数全同维数，$Z^{(\lambda)}$，其中$\lambda=0$和$\lambda=1$分别对应于TRW和BP的近似，且$Z^{(\lambda)}$随$\lambda$单调减少。此外，这种分数方案保证在吸引力（铁磁性）情况下$Z^{(TRW)}\geq Z^{(\lambda)}\geq Z^{(BP)}$，并且存在一个唯一的（“精确”）$\lambda_*$，使得$Z=Z^{(\lambda_*)}$。通过推广\citep {wainwright_tree-based_2002}的重新参数化方法和\citep {chertkov_loop_2006}的环级数方法，我们展示了如何进行e

    arXiv:2301.10369v2 Announce Type: replace  Abstract: Inference efforts -- required to compute partition function, $Z$, of an Ising model over a graph of $N$ ``spins" -- are most likely exponential in $N$. Efficient variational methods, such as Belief Propagation (BP) and Tree Re-Weighted (TRW) algorithms, compute $Z$ approximately minimizing respective (BP- or TRW-) free energy. We generalize the variational scheme building a $\lambda$-fractional-homotopy, $Z^{(\lambda)}$, where $\lambda=0$ and $\lambda=1$ correspond to TRW- and BP-approximations, respectively, and $Z^{(\lambda)}$ decreases with $\lambda$ monotonically. Moreover, this fractional scheme guarantees that in the attractive (ferromagnetic) case $Z^{(TRW)}\geq Z^{(\lambda)}\geq Z^{(BP)}$, and there exists a unique (``exact") $\lambda_*$ such that, $Z=Z^{(\lambda_*)}$. Generalizing the re-parametrization approach of \citep{wainwright_tree-based_2002} and the loop series approach of \citep{chertkov_loop_2006}, we show how to e
    
[^174]: 交通流量预测的时空自监督学习

    Spatio-Temporal Self-Supervised Learning for Traffic Flow Prediction

    [https://arxiv.org/abs/2212.04475](https://arxiv.org/abs/2212.04475)

    提出了一种新的时空自监督学习（ST-SSL）交通预测框架，通过辅助自监督学习范式增强交通模式表示，既反映空间异质性又反映时间异质性。

    

    城市范围内在不同时间段对交通流量进行稳健预测在智能交通系统中起着至关重要的作用。尽管先前的工作已经为建模时空相关性做出了巨大努力，但现有方法仍然存在两个关键限制：i) 大多数模型在预测所有区域的流量时没有考虑空间异质性，即不同区域可能具有倾斜的交通流量分布。 ii) 这些模型未能捕捉由于时间变化的交通模式而引起的时间异质性，因为它们通常使用一个共享参数化空间来模拟所有时间段的时间相关性。为解决这些挑战，我们提出了一种新的时空自监督学习（ST-SSL）交通预测框架，通过辅助自监督学习范式增强交通模式表示，使其既反映空间异质性又反映时间异质性。

    arXiv:2212.04475v2 Announce Type: replace-cross  Abstract: Robust prediction of citywide traffic flows at different time periods plays a crucial role in intelligent transportation systems. While previous work has made great efforts to model spatio-temporal correlations, existing methods still suffer from two key limitations: i) Most models collectively predict all regions' flows without accounting for spatial heterogeneity, i.e., different regions may have skewed traffic flow distributions. ii) These models fail to capture the temporal heterogeneity induced by time-varying traffic patterns, as they typically model temporal correlations with a shared parameterized space for all time periods. To tackle these challenges, we propose a novel Spatio-Temporal Self-Supervised Learning (ST-SSL) traffic prediction framework which enhances the traffic pattern representations to be reflective of both spatial and temporal heterogeneity, with auxiliary self-supervised learning paradigms. Specificall
    
[^175]: 用推测对手模型进行决策

    Decision-making with Speculative Opponent Models

    [https://arxiv.org/abs/2211.11940](https://arxiv.org/abs/2211.11940)

    提出了一种使用纯粹局部信息实现推测对手建模的多智能体分布式演员-评论家算法，能够帮助受控代理做出决策。

    

    对手建模通过构建其他代理的模型，使受控代理的决策受益。现有方法通常假设可以访问对手的观察和行为，但当对手的行为不可观察或难以获得时，这是不可行的。我们提出了一种新颖的多智能体分布式演员-评论家算法，通过纯粹的局部信息（即受控代理的观察、行为和奖励）实现推测对手建模。具体而言，演员维持对对手的推测信念，我们称之为推测对手模型，以使用局部观察来预测对手的动作，并相应地做出决策。此外，分布式评论家模型政策的回报分布。它反映了演员的质量，因此可以指导演员所依赖的推测对手模型的训练。大量实验证实了我们的方法成功地...

    arXiv:2211.11940v2 Announce Type: replace  Abstract: Opponent modeling has benefited a controlled agent's decision-making by constructing models of other agents. Existing methods commonly assume access to opponents' observations and actions, which is infeasible when opponents' behaviors are unobservable or hard to obtain. We propose a novel multi-agent distributional actor-critic algorithm to achieve speculative opponent modeling with purely local information (i.e., the controlled agent's observations, actions, and rewards). Specifically, the actor maintains a speculated belief of the opponents, which we call the speculative opponent models, to predict opponent actions using local observations and makes decisions accordingly. Further, the distributional critic models the return distribution of the policy. It reflects the quality of the actor and thus can guide the training of the speculative opponent model that the actor relies on. Extensive experiments confirm that our method successf
    
[^176]: SemSegDepth: 一个集成模型用于语义分割和深度完成

    SemSegDepth: A Combined Model for Semantic Segmentation and Depth Completion

    [https://arxiv.org/abs/2209.00381](https://arxiv.org/abs/2209.00381)

    在本文中，我们提出了一个新的端到端模型，用于同时进行语义分割和深度完成。我们的方法结合了语义分割和深度完成任务，在多任务网络中有效提高了每个任务的性能。

    

    综合场景理解对于自主机器的性能至关重要。本文提出了一个新的端到端模型，用于同时进行语义分割和深度完成。我们的方法依赖于RGB和稀疏深度作为模型的输入，并产生密集深度图和相应的语义分割图像。它包括一个特征提取器，一个深度完成分支，一个语义分割分支以及一个联合分支，进一步同时处理语义和深度信息。在Virtual KITTI 2数据集上进行的实验证明，并提供了进一步的证据，即在多任务网络中结合语义分割和深度完成任务可以有效提高每个任务的性能。

    arXiv:2209.00381v2 Announce Type: replace-cross  Abstract: Holistic scene understanding is pivotal for the performance of autonomous machines. In this paper we propose a new end-to-end model for performing semantic segmentation and depth completion jointly. The vast majority of recent approaches have developed semantic segmentation and depth completion as independent tasks. Our approach relies on RGB and sparse depth as inputs to our model and produces a dense depth map and the corresponding semantic segmentation image. It consists of a feature extractor, a depth completion branch, a semantic segmentation branch and a joint branch which further processes semantic and depth information altogether. The experiments done on Virtual KITTI 2 dataset, demonstrate and provide further evidence, that combining both tasks, semantic segmentation and depth completion, in a multi-task network can effectively improve the performance of each task. Code is available at https://github.com/juanb09111/sem
    
[^177]: STDEN:基于物理指导的神经网络用于交通流量预测

    STDEN: Towards Physics-Guided Neural Networks for Traffic Flow Prediction

    [https://arxiv.org/abs/2209.00225](https://arxiv.org/abs/2209.00225)

    STDEN是一种将交通流动力学的物理机制转化为深度学习模型的物理指导神经网络，旨在弥合纯数据驱动和物理驱动方法之间的差距。

    

    高性能交通流量预测模型设计是智能交通系统的核心技术，是工业界和学术界长期以来一直面临的挑战。缺乏物理原理与数据驱动模型之间的整合是限制该领域发展的重要原因。为了弥合纯数据驱动和物理驱动方法之间的差距，我们提出了一种名为Spatio-Temporal Differential Equation Network (STDEN)的物理指导深度学习模型，将交通流动力学的物理机制转化为深度学习模型的形式。

    arXiv:2209.00225v2 Announce Type: replace  Abstract: High-performance traffic flow prediction model designing, a core technology of Intelligent Transportation System, is a long-standing but still challenging task for industrial and academic communities. The lack of integration between physical principles and data-driven models is an important reason for limiting the development of this field. In the literature, physics-based methods can usually provide a clear interpretation of the dynamic process of traffic flow systems but are with limited accuracy, while data-driven methods, especially deep learning with black-box structures, can achieve improved performance but can not be fully trusted due to lack of a reasonable physical basis. To bridge the gap between purely data-driven and physics-driven approaches, we propose a physics-guided deep learning model named Spatio-Temporal Differential Equation Network (STDEN), which casts the physical mechanism of traffic flow dynamics into a deep 
    
[^178]: 重新审视差分隐私广义线性模型

    Differentially Private Generalized Linear Models Revisited

    [https://arxiv.org/abs/2205.03014](https://arxiv.org/abs/2205.03014)

    研究了具有凸损失的线性预测器的$(\epsilon,\delta)$-差分隐私学习问题，为两个损失函数子类提供了结果，并在所有参数上展示了基本紧密的上界和下界。

    

    我们研究了具有凸损失的线性预测器的$(\epsilon,\delta)$-差分隐私学习问题。我们针对两个损失函数子类提供了结果。第一种情况是当损失是光滑且非负但不一定利普希兹时（如平方损失）。对于这种情况，我们建立了关于过量总体风险的上界，为$\tilde{O}\left(\frac{\Vert w^*\Vert}{\sqrt{n}} + \min\left\{\frac{\Vert w^* \Vert^2}{(n\epsilon)^{2/3}},\frac{\sqrt{d}\Vert w^*\Vert^2}{n\epsilon}\right\}\right)$，其中$n$是样本数，$d$是问题的维度，$w^*$是总体风险的最小化者。除了对$\Vert w^\ast\Vert$的依赖之外，我们的界限在所有参数上基本上是紧密的。特别地，我们展示了一个$\tilde{\Omega}\left(\frac{1}{\sqrt{n}} + {\min\left\{\frac{\Vert w^*\Vert^{4/3}}{(n\epsilon)^{2/3}}, \frac{\sqrt{d}\Vert w^*\Vert}{n\epsilon}\right\}}$的下界。

    arXiv:2205.03014v2 Announce Type: replace  Abstract: We study the problem of $(\epsilon,\delta)$-differentially private learning of linear predictors with convex losses. We provide results for two subclasses of loss functions. The first case is when the loss is smooth and non-negative but not necessarily Lipschitz (such as the squared loss). For this case, we establish an upper bound on the excess population risk of $\tilde{O}\left(\frac{\Vert w^*\Vert}{\sqrt{n}} + \min\left\{\frac{\Vert w^* \Vert^2}{(n\epsilon)^{2/3}},\frac{\sqrt{d}\Vert w^*\Vert^2}{n\epsilon}\right\}\right)$, where $n$ is the number of samples, $d$ is the dimension of the problem, and $w^*$ is the minimizer of the population risk. Apart from the dependence on $\Vert w^\ast\Vert$, our bound is essentially tight in all parameters. In particular, we show a lower bound of $\tilde{\Omega}\left(\frac{1}{\sqrt{n}} + {\min\left\{\frac{\Vert w^*\Vert^{4/3}}{(n\epsilon)^{2/3}}, \frac{\sqrt{d}\Vert w^*\Vert}{n\epsilon}\right\}}
    
[^179]: 用生成预训练Transformer生成紧凑二进制系统波形

    Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer. (arXiv:2310.20172v1 [gr-qc])

    [http://arxiv.org/abs/2310.20172](http://arxiv.org/abs/2310.20172)

    提出了一种叫做CBS-GPT的生成预训练Transformer模型，用于紧凑二进制系统波形生成，在预测准确性上达到了较高的准确率，并且具有显著的解释性能。

    

    空间引力波探测是未来十年最受期待的引力波探测项目之一，将探测到丰富的紧凑二进制系统。然而，对于空间引力波波形的精确预测仍未被探索。为了解决探测器响应和二代时延干涉（TDI 2.0）引起的波形复杂性增加而带来的数据处理困难，提出了一种名为CBS-GPT（Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer）的可解释预训练大模型。对于紧凑二进制系统波形，训练了三个模型来预测超大质量黑洞二进制（MBHB）、极端质量比融合（EMRIs）和星系二进制（GB）的波形，分别实现了98%、91%和99%的预测准确性。CBS-GPT模型具有显著的解释性，其隐藏参数能够有效捕捉波形的复杂信息，即使是复杂的不连续。

    Space-based gravitational wave detection is one of the most anticipated gravitational wave (GW) detection projects in the next decade, which will detect abundant compact binary systems. However, the precise prediction of space GW waveforms remains unexplored. To solve the data processing difficulty in the increasing waveform complexity caused by detectors' response and second-generation time-delay interferometry (TDI 2.0), an interpretable pre-trained large model named CBS-GPT (Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer) is proposed. For compact binary system waveforms, three models were trained to predict the waveforms of massive black hole binary (MBHB), extreme mass-ratio inspirals (EMRIs), and galactic binary (GB), achieving prediction accuracies of 98%, 91%, and 99%, respectively. The CBS-GPT model exhibits notable interpretability, with its hidden parameters effectively capturing the intricate information of waveforms, even with complex ins
    
[^180]: 在样本选择偏差存在的情况下，利用集成多样性进行鲁棒的自训练

    Leveraging Ensemble Diversity for Robust Self-Training in the Presence of Sample Selection Bias. (arXiv:2310.14814v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.14814](http://arxiv.org/abs/2310.14814)

    本文提出了一种在样本选择偏差存在的情况下，利用集成多样性进行鲁棒的自训练的方法，并引入了一种新的自信度度量方法-$\mathcal{T}$-相似度。实验证明该方法在三种不同伪标签策略下具有良好的效果。

    

    自训练是半监督学习中一种众所周知的方法。它包括对模型自信度高的未标记数据进行伪标签分配，并将其视为标记样本进行处理。对于神经网络，通常使用softmax预测概率作为自信度度量，尽管已知它们对错误预测也过于自信。当数据标注受到某种约束时，这种现象尤为明显，即样本选择偏差存在。为了解决这个问题，我们提出了一种新的自信度度量方法，称为$\mathcal{T}$-相似度，它基于线性分类器的集成预测多样性。我们通过研究稳定点并描述单个成员的多样性与其性能之间的关系来提供我们方法的理论分析。我们通过对三种不同伪标签策略的实验验证了我们自信度度量的好处。

    Self-training is a well-known approach for semi-supervised learning. It consists of iteratively assigning pseudo-labels to unlabeled data for which the model is confident and treating them as labeled examples. For neural networks, softmax prediction probabilities are often used as a confidence measure, despite the fact that they are known to be overconfident, even for wrong predictions. This phenomenon is particularly intensified in the presence of sample selection bias, i.e., when data labeling is subject to some constraint. To address this issue, we propose a novel confidence measure, called $\mathcal{T}$-similarity, built upon the prediction diversity of an ensemble of linear classifiers. We provide the theoretical analysis of our approach by studying stationary points and describing the relationship between the diversity of the individual members and their performance. We empirically demonstrate the benefit of our confidence measure for three different pseudo-labeling policies on c
    
[^181]: 可扩展的神经网络内核

    Scalable Neural Network Kernels. (arXiv:2310.13225v1 [cs.LG])

    [http://arxiv.org/abs/2310.13225](http://arxiv.org/abs/2310.13225)

    可扩展神经网络内核（SNNKs）是一种替代常规前馈层的方法，能够近似实现常规前馈层的功能，但具有更优的计算特性。通过将内核与参数-输入向量的点积联系起来，SNNKs能够有效地解开参数与输入之间的联系，从而模拟复杂关系。此外，我们还引入了神经网络捆绑过程，将SNNKs应用于深度神经网络压缩，进一步提高了压缩效果。最终捆绑网络甚至可以绕过反向传播，通过显式公式求解最优参数。

    

    我们引入了可扩展神经网络内核（SNNKs）的概念，这是常规前馈层（FFLs）的替代品，能够近似实现后者，但具有有利的计算属性。SNNKs有效地解开了FFL中参数与输入之间的联系，并通过点积内核在最终计算中连接它们。它们也更加表达力强，能够模拟复杂关系，超出参数-输入向量的函数范围。我们还引入了神经网络捆绑过程，将SNNKs应用于压缩深度神经网络结构，从而获得额外的压缩效益。在极端情况下，它导致完全捆绑网络，其最优参数可以通过多个损失函数（例如均方误差）的显式公式来表示，从而有可能绕过反向传播。作为我们分析的副产品，我们引入了普遍性机制的机制。

    We introduce the concept of scalable neural network kernels (SNNKs), the replacements of regular feedforward layers (FFLs), capable of approximating the latter, but with favorable computational properties. SNNKs effectively disentangle the inputs from the parameters of the neural network in the FFL, only to connect them in the final computation via the dot-product kernel. They are also strictly more expressive, as allowing to model complicated relationships beyond the functions of the dot-products of parameter-input vectors. We also introduce the neural network bundling process that applies SNNKs to compactify deep neural network architectures, resulting in additional compression gains. In its extreme version, it leads to the fully bundled network whose optimal parameters can be expressed via explicit formulae for several loss functions (e.g. mean squared error), opening a possibility to bypass backpropagation. As a by-product of our analysis, we introduce the mechanism of the universa
    
[^182]: 多保真神经网络计算的残差方法

    Residual Multi-Fidelity Neural Network Computing. (arXiv:2310.03572v1 [cs.LG])

    [http://arxiv.org/abs/2310.03572](http://arxiv.org/abs/2310.03572)

    本研究提出了一种残差多保真计算框架，通过使用多保真信息构建神经网络代理模型，解决了低保真和高保真计算模型之间的相关性建模问题。这种方法训练了两个神经网络，利用残差函数进行模型训练，最终得到了高保真替代模型。

    

    在本研究中，我们考虑使用多保真信息构建神经网络代理模型的一般问题。给定一个廉价的低保真和一个昂贵的高保真计算模型，我们提出了一个残差多保真计算框架，将模型之间的相关性建模为一个残差函数，这是一个可能非线性的1）模型共享的输入空间和低保真模型输出之间的映射，以及2）两个模型输出之间的差异。为了实现这一点，我们训练了两个神经网络来协同工作。第一个网络在少量的高保真和低保真数据上学习残差函数。一旦训练完成，这个网络被用来生成额外的合成高保真数据，用于训练第二个网络。一旦训练完成，第二个网络作为我们对高保真感兴趣的量的替代模型。我们提供了三个数值例子来证明这种方法的能力。

    In this work, we consider the general problem of constructing a neural network surrogate model using multi-fidelity information. Given an inexpensive low-fidelity and an expensive high-fidelity computational model, we present a residual multi-fidelity computational framework that formulates the correlation between models as a residual function, a possibly non-linear mapping between 1) the shared input space of the models together with the low-fidelity model output and 2) the discrepancy between the two model outputs. To accomplish this, we train two neural networks to work in concert. The first network learns the residual function on a small set of high-fidelity and low-fidelity data. Once trained, this network is used to generate additional synthetic high-fidelity data, which is used in the training of a second network. This second network, once trained, acts as our surrogate for the high-fidelity quantity of interest. We present three numerical examples to demonstrate the power of th
    
[^183]: 社会应用的无分布统计离散度控制

    Distribution-Free Statistical Dispersion Control for Societal Applications. (arXiv:2309.13786v1 [cs.LG])

    [http://arxiv.org/abs/2309.13786](http://arxiv.org/abs/2309.13786)

    提出了一个简单而灵活的框架，用于处理具有社会意义的无分布统计离散度控制，可以应用于高风险应用。

    

    在负责任的机器学习中，对模型性能的显式有限样本统计保证是一个重要因素。之前的研究主要关注于界定预测器的期望损失或者个体预测将承受的损失值在一个指定范围内的概率。然而，对于许多高风险应用而言，理解和控制损失分布的离散度，或者说人群中不同个体对算法决策的影响程度是至关重要的。我们开始研究具有社会意义的无分布统计离散度控制，并提出了一个简单但灵活的框架，可以处理比以前的工作更丰富的统计功能类。我们通过在有毒评论检测、医学影像和电影推荐等实验中验证了我们的方法。

    Explicit finite-sample statistical guarantees on model performance are an important ingredient in responsible machine learning. Previous work has focused mainly on bounding either the expected loss of a predictor or the probability that an individual prediction will incur a loss value in a specified range. However, for many high-stakes applications, it is crucial to understand and control the dispersion of a loss distribution, or the extent to which different members of a population experience unequal effects of algorithmic decisions. We initiate the study of distribution-free control of statistical dispersion measures with societal implications and propose a simple yet flexible framework that allows us to handle a much richer class of statistical functionals beyond previous work. Our methods are verified through experiments in toxic comment detection, medical imaging, and film recommendation.
    
[^184]: 生成和评估合成纵向患者数据的方法：一项系统综述

    Methods for generating and evaluating synthetic longitudinal patient data: a systematic review. (arXiv:2309.12380v1 [stat.ME])

    [http://arxiv.org/abs/2309.12380](http://arxiv.org/abs/2309.12380)

    本文对生成和评估合成纵向患者数据的方法进行了系统综述，以解决医学领域中数据使用和隐私保护的问题。

    

    近年来数据的迅猛增长促进了各种统计和深度学习技术的发展和应用，加快了研究和开发活动。然而，并非所有行业都能从数据的增加中同等受益，部分原因是由于数据使用和隐私规定的法律限制，例如医学领域。为了解决这个问题，提出了各种统计披露和隐私保护方法，包括使用合成数据生成。合成数据是基于一些现有数据生成的，目的是尽可能地复制它们，并充当真实敏感数据的代理。本文对生成和评估合成纵向患者数据的方法进行了系统综述，这是医学领域中一种常见的数据类型。该综述遵循PRISMA指南，并涵盖了自2022年底以来的五个数据库的文献。本文描述了17种方法，从传统方法到深度学习方法。

    The proliferation of data in recent years has led to the advancement and utilization of various statistical and deep learning techniques, thus expediting research and development activities. However, not all industries have benefited equally from the surge in data availability, partly due to legal restrictions on data usage and privacy regulations, such as in medicine. To address this issue, various statistical disclosure and privacy-preserving methods have been proposed, including the use of synthetic data generation. Synthetic data are generated based on some existing data, with the aim of replicating them as closely as possible and acting as a proxy for real sensitive data. This paper presents a systematic review of methods for generating and evaluating synthetic longitudinal patient data, a prevalent data type in medicine. The review adheres to the PRISMA guidelines and covers literature from five databases until the end of 2022. The paper describes 17 methods, ranging from traditi
    
[^185]: 多任务强化学习中的Projected Task-Specific Layers

    Projected Task-Specific Layers for Multi-Task Reinforcement Learning. (arXiv:2309.08776v1 [cs.LG])

    [http://arxiv.org/abs/2309.08776](http://arxiv.org/abs/2309.08776)

    本研究提出了一种新的架构，Projected Task-Specific Layers (PTSL)，通过任务特定的层来表达共享和可变的任务信息，成功解决了多任务强化学习中的推广和干扰问题。

    

    多任务强化学习可以使机器人在家庭和工作场所的各种操作任务中实现规模化。然而，从一个任务推广到另一个任务并减轻负面任务干扰仍然是一个挑战。成功地在任务之间共享信息并取得良好效果将取决于对任务底层结构的有效捕捉。在这项工作中，我们介绍了一种新的架构，即Projected Task-Specific Layers（PTSL），它通过任务特定的层，通过稠密的任务特定的修正来更好地表达共享和可变的任务信息。然后，我们展示了我们的模型在Meta-World的MT10和MT50基准中（包括Sawyer机器人臂上的10个和50个目标条件任务）的表现优于现有技术水平。

    Multi-task reinforcement learning could enable robots to scale across a wide variety of manipulation tasks in homes and workplaces. However, generalizing from one task to another and mitigating negative task interference still remains a challenge. Addressing this challenge by successfully sharing information across tasks will depend on how well the structure underlying the tasks is captured. In this work, we introduce our new architecture, Projected Task-Specific Layers (PTSL), that leverages a common policy with dense task-specific corrections through task-specific layers to better express shared and variable task information. We then show that our model outperforms the state of the art on the MT10 and MT50 benchmarks of Meta-World consisting of 10 and 50 goal-conditioned tasks for a Sawyer arm.
    
[^186]: 保持结构的变压器用于序列的SPD矩阵

    Structure-Preserving Transformers for Sequences of SPD Matrices. (arXiv:2309.07579v1 [cs.LG])

    [http://arxiv.org/abs/2309.07579](http://arxiv.org/abs/2309.07579)

    本文介绍了一种保持序列的对称正定矩阵的黎曼几何特性的结构保持变压器机制，并将其应用于自动睡眠分期，取得了高水平的阶段性能。

    

    近年来，基于变压器的自注意力机制已成功应用于各种上下文相关的数据类型的分析，从文本到图像等，包括非欧几里得几何的数据。本文提出了一种这样的机制，用于分类序列的对称正定矩阵，并在整个分析过程中保持它们的黎曼几何特性。我们将我们的方法应用于来自标准数据集中的脑电图协方差矩阵序列的自动睡眠分期，取得了高水平的阶段性能。

    In recent years, Transformer-based auto-attention mechanisms have been successfully applied to the analysis of a variety of context-reliant data types, from texts to images and beyond, including data from non-Euclidean geometries. In this paper, we present such a mechanism, designed to classify sequences of Symmetric Positive Definite matrices while preserving their Riemannian geometry throughout the analysis. We apply our method to automatic sleep staging on timeseries of EEG-derived covariance matrices from a standard dataset, obtaining high levels of stage-wise performance.
    
[^187]: HarvestNet：利用收获堆和遥感技术检测小农户农业活动的数据集

    HarvestNet: A Dataset for Detecting Smallholder Farming Activity Using Harvest Piles and Remote Sensing. (arXiv:2308.12061v1 [cs.CV])

    [http://arxiv.org/abs/2308.12061](http://arxiv.org/abs/2308.12061)

    该论文介绍了一种新的方法，通过检测全球小农户系统中常见的收获堆来映射农田的存在。作者提供了HarvestNet数据集，该数据集由专家知识和卫星图像收集而来，可用于在埃塞俄比亚的特定地区进行农田映射。实验结果表明，作者的最佳模型在手动标记数据上具有约80%的分类性能。

    

    小型农场在发展中国家的生产土地中所占比例很大。在撒哈拉以南非洲等地区，80%的农场都很小（面积小于2公顷），映射小农户的农田是追踪作物生产力等可持续发展措施的重要组成部分。然而，小农场的外观多样且微妙，传统的农田映射方法的效果受到限制。在这里，我们引入了一个基于检测遍布全球许多小农户系统的收获堆的新方法。我们提供了HarvestNet数据集，用于在埃塞俄比亚的提格雷和阿姆哈拉地区的2020-2023年间映射农田的存在，该数据集利用专家知识和卫星图像收集，共有7k个手动标记的图像和2k个地面收集标签。我们还针对现有技术在遥感领域进行了基准测试，我们的最佳模型在手动标记数据上具有约80%的分类性能。

    Small farms contribute to a large share of the productive land in developing countries. In regions such as sub-Saharan Africa, where 80% of farms are small (under 2 ha in size), the task of mapping smallholder cropland is an important part of tracking sustainability measures such as crop productivity. However, the visually diverse and nuanced appearance of small farms has limited the effectiveness of traditional approaches to cropland mapping. Here we introduce a new approach based on the detection of harvest piles characteristic of many smallholder systems throughout the world. We present HarvestNet, a dataset for mapping the presence of farms in the Ethiopian regions of Tigray and Amhara during 2020-2023, collected using expert knowledge and satellite images, totaling 7k hand-labeled images and 2k ground collected labels. We also benchmark a set of baselines including SOTA models in remote sensing with our best models having around 80% classification performance on hand labelled data
    
[^188]: 通过超出平衡状态的扩展动力学性能评估神经力场

    xxMD: Benchmarking Neural Force Fields Using Extended Dynamics beyond Equilibrium. (arXiv:2308.11155v1 [cs.LG])

    [http://arxiv.org/abs/2308.11155](http://arxiv.org/abs/2308.11155)

    在神经力场模型中，常用的MD17数据集对于表示经历化学反应的系统不足。为了解决这一问题，我们引入了xxMD数据集，该数据集采样自扩展激发态分子动力学，包含了能量和力的信息。

    

    神经力场已成为计算化学中的重要模型，取代了从头算的分子动力学中的量子化学计算。目前对神经力场的主要评估基准是MD17数据集及其后续扩展。这些数据集主要包含来自基态势能面平衡区域的几何结构，采样自直接绝热动力学。然而，许多化学反应涉及到较大的分子变形，特别是键断裂。我们展示了MD17数据集中内坐标和能量的约束分布，凸显了其在表示经历化学反应的系统方面的不足。为了解决这种采样限制，我们引入了xxMD（扩展激发态分子动力学）数据集，从非绝热动力学中派生。该数据集包含了从多参考波函数理论和密度泛函中确定的能量和力。

    Neural force fields (NFFs) have gained prominence in computational chemistry as surrogate models, superseding quantum-chemistry calculations in ab initio molecular dynamics. The prevalent benchmark for NFFs has been the MD17 dataset and its subsequent extension. These datasets predominantly comprise geometries from the equilibrium region of the ground electronic state potential energy surface, sampling from direct adiabatic dynamics. However, many chemical reactions entail significant molecular deformations, notably bond breaking. We demonstrate the constrained distribution of internal coordinates and energies in the MD17 datasets, underscoring their inadequacy for representing systems undergoing chemical reactions. Addressing this sampling limitation, we introduce the xxMD (Extended Excited-state Molecular Dynamics) dataset, derived from non-adiabatic dynamics. This dataset encompasses energies and forces ascertained from both multireference wave function theory and density functional
    
[^189]: 追求多模态视觉语言模型中的基于实际的视觉空间推理

    Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models. (arXiv:2308.09778v1 [cs.CV])

    [http://arxiv.org/abs/2308.09778](http://arxiv.org/abs/2308.09778)

    本文旨在研究多模态视觉语言模型在理解空间关系方面的能力，提出了细粒度组合的空间关系基础，并采用自底向上的方法评估空间关系推理任务的性能。

    

    随着大规模视觉和语言模型（VLMs）的进展，评估它们在各种视觉推理任务（如计数、指涉表达和一般的视觉问题回答）上的表现变得越来越重要。本文的重点是研究这些模型理解空间关系的能力。先前，人们尝试使用图像-文本匹配（Liu, Emerson, and Collier 2022) 或视觉问题回答任务来处理此问题，但都表现出性能不佳并且与人类性能存在较大差距。为了更好地理解差距，我们提出了细粒度组合的空间关系基础，并提出了一种自底向上的方法来对空间从句进行排名并评估空间关系推理任务的性能。我们建议通过结合和地面化物体对应的名词短语和它们的位置的证据来计算空间从句的最终排名。我们在代表性的视觉语言模型上展示了这种方法。

    With the advances in large scale vision-and-language models (VLMs) it is of interest to assess their performance on various visual reasoning tasks such as counting, referring expressions and general visual question answering. The focus of this work is to study the ability of these models to understanding spatial relations. Previously, this has been tackled using image-text matching (Liu, Emerson, and Collier 2022) or visual question answering task, both showing poor performance and a large gap compared to human performance. To better understand the gap, we present fine-grained compositional grounding of spatial relationships and propose a bottom up approach for ranking spatial clauses and evaluating the performance of spatial relationship reasoning task. We propose to combine the evidence from grounding noun phrases corresponding to objects and their locations to compute the final rank of the spatial clause. We demonstrate the approach on representative vision-language models (Tan and 
    
[^190]: AbDiffuser：体外功能抗体的全原子生成

    AbDiffuser: Full-Atom Generation of In-Vitro Functioning Antibodies. (arXiv:2308.05027v1 [q-bio.BM])

    [http://arxiv.org/abs/2308.05027](http://arxiv.org/abs/2308.05027)

    AbDiffuser是一个物理性扩散模型，用于联合生成抗体的三维结构和序列。该方法利用领域知识和基于物理的约束改善蛋白质扩散，处理序列长度变化，并能够生成与参考集合的序列和结构特性密切匹配的抗体。实验结果表明，AbDiffuser能够生成高水平表达的抗体，其中57.1%的设计选择是紧密结合剂。

    

    我们介绍了一个名为AbDiffuser的等变物理性扩散模型，用于联合生成抗体的三维结构和序列。AbDiffuser建立在一种新的蛋白质结构表示上，依赖于一种针对齐位蛋白的新型架构，并利用强扩散先验改善去噪过程。我们的方法通过利用领域知识和基于物理的约束改善了蛋白质扩散；处理序列长度变化；并将内存复杂性降低一个数量级，实现了骨架和侧链的生成。我们在体内和体外验证了AbDiffuser。数值实验展示了AbDiffuser生成与参考集合的序列和结构特性密切匹配的抗体的能力。实验室实验证实，发现的16种HER2抗体均以高水平表达，并且57.1%的设计选择是紧密结合剂。

    We introduce AbDiffuser, an equivariant and physics-informed diffusion model for the joint generation of antibody 3D structures and sequences. AbDiffuser is built on top of a new representation of protein structure, relies on a novel architecture for aligned proteins, and utilizes strong diffusion priors to improve the denoising process. Our approach improves protein diffusion by taking advantage of domain knowledge and physics-based constraints; handles sequence-length changes; and reduces memory complexity by an order of magnitude enabling backbone and side chain generation. We validate AbDiffuser in silico and in vitro. Numerical experiments showcase the ability of AbDiffuser to generate antibodies that closely track the sequence and structural properties of a reference set. Laboratory experiments confirm that all 16 HER2 antibodies discovered were expressed at high levels and that 57.1% of selected designs were tight binders.
    
[^191]: 通过随机定位方法获得扩散模型的线性收敛界限

    Linear Convergence Bounds for Diffusion Models via Stochastic Localization. (arXiv:2308.03686v1 [stat.ML])

    [http://arxiv.org/abs/2308.03686](http://arxiv.org/abs/2308.03686)

    通过随机定位方法，我们提供了一种解决扩散模型中线性收敛界限问题的方法，并证明了这种方法可以在有限二阶矩条件下达到可接受的精度。

    

    扩散模型是从高维数据分布中生成近似样本的有效方法。最近的一些研究结果提供了关于这种模型的收敛速度的多项式界限，假设$L^2$准确的得分估计器。然而，到目前为止，已知的最佳界限要么对数据维度是超线性的，要么需要强平滑性假设。我们提供了第一个假设只需要数据分布有有限二阶矩的收敛界限，这些界限对于数据维度是线性的（乘以对数因子）。我们证明了扩散模型最多需要$\tilde O(\frac{d \log^2(1/\delta)}{\varepsilon^2})$步，就可以将带有方差为$\delta$的高斯噪声损坏的任意数据分布在Kullback--Leibler散度下近似到$\varepsilon^2$。我们的证明依赖于前人的Girsanov方法。我们引入了对于反向SD离散化误差的精细处理。

    Diffusion models are a powerful method for generating approximate samples from high-dimensional data distributions. Several recent results have provided polynomial bounds on the convergence rate of such models, assuming $L^2$-accurate score estimators. However, up until now the best known such bounds were either superlinear in the data dimension or required strong smoothness assumptions. We provide the first convergence bounds which are linear in the data dimension (up to logarithmic factors) assuming only finite second moments of the data distribution. We show that diffusion models require at most $\tilde O(\frac{d \log^2(1/\delta)}{\varepsilon^2})$ steps to approximate an arbitrary data distribution on $\mathbb{R}^d$ corrupted with Gaussian noise of variance $\delta$ to within $\varepsilon^2$ in Kullback--Leibler divergence. Our proof builds on the Girsanov-based methods of previous works. We introduce a refined treatment of the error arising from the discretization of the reverse SD
    
[^192]: VQGraph: 图形向量量化用于连接GNN和MLPs

    VQGraph: Graph Vector-Quantization for Bridging GNNs and MLPs. (arXiv:2308.02117v1 [cs.LG])

    [http://arxiv.org/abs/2308.02117](http://arxiv.org/abs/2308.02117)

    VQGraph是一个框架，通过学习一个强大的图形表示空间，用于连接GNN和MLPs。它采用矢量量化变分自编码器（VQ-VAE）的编码器作为结构感知图标记器，有效地表示底层图的多样化局部结构。通过 VQGraph，可以实现从GNN到MLP的知识转移。

    

    图神经网络（GNNs）进行信息传递，聚合局部邻居以更新节点表示。这种信息传递导致在实际的延迟约束应用程序中存在可扩展性问题。为了解决这个问题，最近的方法采用知识蒸馏（KD）通过模仿GNN的输出来学习计算效率高的多层感知机（MLP）。然而，现有的GNN表示空间可能不足以表示底层图的多样化局部结构，这限制了从GNN到MLP的知识转移。在这里，我们提出了一个新颖的框架VQGraph，用于学习一个强大的图形表示空间，用于连接GNN和MLPs。我们采用一种变体的矢量量化变分自编码器（VQ-VAE）的编码器作为结构感知图标记器，它将多样化的局部结构节点明确表示为大量离散令牌，并构成一个有意义的代码书。配备了学习的代码书，我们提出

    Graph Neural Networks (GNNs) conduct message passing which aggregates local neighbors to update node representations. Such message passing leads to scalability issues in practical latency-constrained applications. To address this issue, recent methods adopt knowledge distillation (KD) to learn computationally-efficient multi-layer perceptron (MLP) by mimicking the output of GNN. However, the existing GNN representation space may not be expressive enough for representing diverse local structures of the underlying graph, which limits the knowledge transfer from GNN to MLP. Here we present a novel framework VQGraph to learn a powerful graph representation space for bridging GNNs and MLPs. We adopt the encoder of a variant of a vector-quantized variational autoencoder (VQ-VAE) as a structure-aware graph tokenizer, which explicitly represents the nodes of diverse local structures as numerous discrete tokens and constitutes a meaningful codebook. Equipped with the learned codebook, we propos
    
[^193]: 可解释的推理方法用于刻板印象识别

    Interpretable Stereotype Identification through Reasoning. (arXiv:2308.00071v1 [cs.CL])

    [http://arxiv.org/abs/2308.00071](http://arxiv.org/abs/2308.00071)

    本研究通过使用推理方法，在零射击刻板印象识别中取得了重要的进展，并发现推理的性能增益远远超过模型规模扩展的增益。推理不仅提高了准确性，还提高了决策的可解释性。

    

    鉴于语言模型训练使用了包含固有偏见的大量数据集，可能会不经意地持续系统性歧视，因此，审查和解决语言模型中的偏见变得至关重要，将公平性整合到它们的发展中，以确保这些模型具有公正和无偏的特性。在这项工作中，我们展示了基于Vicuna-13B-v1.3的零射击刻板印象识别中推理的重要性。尽管我们观察到从13B到33B的规模扩展会提高准确性，但我们表明推理的性能增益远远超过规模扩展的增益。我们的研究结果表明，推理可能是使LLMs在刻板印象等领域任务上超越规模定律的关键因素。此外，通过对选定的推理追踪进行定性分析，我们突出显示了推理不仅提高了准确性，还提高了决策的可解释性。

    Given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination. Consequently, it becomes essential to examine and address biases in language models, integrating fairness into their development to ensure these models are equitable and free from bias. In this work, we demonstrate the importance of reasoning in zero-shot stereotype identification based on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from 13B to 33B, we show that the performance gain from reasoning significantly exceeds the gain from scaling up. Our findings suggest that reasoning could be a key factor that enables LLMs to trescend the scaling law on out-of-domain tasks such as stereotype identification. Additionally, through a qualitative analysis of select reasoning traces, we highlight how reasoning enhances not just accuracy but also the interpretability of the decision.
    
[^194]: 用于Helmholtz方程的多网格增强深度学习方法：通过紧致隐式层提高可伸缩性

    Multigrid-Augmented Deep Learning for the Helmholtz Equation: Better Scalability with Compact Implicit Layers. (arXiv:2306.17486v1 [cs.LG])

    [http://arxiv.org/abs/2306.17486](http://arxiv.org/abs/2306.17486)

    通过结合多网格求解器和卷积神经网络，该论文提出了一种用于解决离散异质Helmholtz方程的迭代深度学习方法，在可伸缩性和求解速度上优于传统方法。其中的三个主要创新包括引入隐式层来解决CNN中的视野问题、改进CNN预条件技术以提高性能，并提出了一种多尺度训练方法使网络能够处理不同尺寸的问题。

    

    我们提出了一种基于深度学习的迭代方法来解决离散异质Helmholtz方程在高波数下的问题。通过将经典的迭代多网格求解器和卷积神经网络（CNN）与预条件技术结合起来，我们得到了一个更快且可伸缩性更好的学习型神经求解器，相比标准的多网格求解器更优。我们的方法在先前这类神经方法的基础上提出了三个主要贡献。首先，我们构建了一个多层U-Net-like编码器-求解器CNN，其中在U-Net的最粗糙网格上包含一个隐式层，卷积核被反转。这种方法缓解了CNN中的视野问题，并允许更好的可伸缩性。其次，我们在参数数量、计算时间和收敛速度方面改进了先前的CNN预条件器。第三，我们提出了一种多尺度训练方法，使网络能够扩展到之前未见过的尺寸问题，同时仍保持合理的训练过程。我们的编码器

    We present a deep learning-based iterative approach to solve the discrete heterogeneous Helmholtz equation for high wavenumbers. Combining classical iterative multigrid solvers and convolutional neural networks (CNNs) via preconditioning, we obtain a learned neural solver that is faster and scales better than a standard multigrid solver. Our approach offers three main contributions over previous neural methods of this kind. First, we construct a multilevel U-Net-like encoder-solver CNN with an implicit layer on the coarsest grid of the U-Net, where convolution kernels are inverted. This alleviates the field of view problem in CNNs and allows better scalability. Second, we improve upon the previous CNN preconditioner in terms of the number of parameters, computation time, and convergence rates. Third, we propose a multiscale training approach that enables the network to scale to problems of previously unseen dimensions while still maintaining a reasonable training procedure. Our encoder
    
[^195]: 序数势函数的玩家评级方法

    Ordinal Potential-based Player Rating. (arXiv:2306.05366v1 [cs.GT])

    [http://arxiv.org/abs/2306.05366](http://arxiv.org/abs/2306.05366)

    该论文提出了一种使用序数势函数的玩家评级方法，通过可逆映射的嵌套计算，能够保持传递性。

    

    如果对于任意纯策略$x$、$y$和$z$，如果$x$比$y$更好，$y$比$z$更好，则$x$比$z$更好，则两个对称的零和博弈是可传递的。最近观察到，Elo评级未能保持策略之间的传递关系，因此不能正确提取游戏的传递组件。我们的第一个贡献是表明当在正确的空间中计算Elo评级时，Elo评级确实能够保持传递性。具体而言，我们首先使用合适的可逆映射$\varphi$将游戏应用于$\varphi$，然后计算Elo评级，最后通过应用$\varphi^{-1}$回到原始空间。我们将可传递游戏的表征为势游戏的一个弱变体，其势函数是加性可分离的。利用这一洞见，我们引入了传递序数的概念，即将可传递游戏的收益转化为其差异所需的最小可逆映射数。

    A two-player symmetric zero-sum game is transitive if for any pure strategies $x$, $y$, $z$, if $x$ is better than $y$, and $y$ is better than $z$, then $x$ is better than $z$. It was recently observed that the Elo rating fails at preserving transitive relations among strategies and therefore cannot correctly extract the transitive component of a game. Our first contribution is to show that the Elo rating actually does preserve transitivity when computed in the right space. Precisely, using a suitable invertible mapping $\varphi$, we first apply $\varphi$ to the game, then compute Elo ratings, then go back to the original space by applying $\varphi^{-1}$. We provide a characterization of transitive games as a weak variant of ordinal potential games with additively separable potential functions. Leveraging this insight, we introduce the concept of transitivity order, the minimum number of invertible mappings required to transform the payoff of a transitive game into (differences of) its
    
[^196]: 基于分数的生成模型的高保真图像压缩

    High-Fidelity Image Compression with Score-based Generative Models. (arXiv:2305.18231v1 [eess.IV])

    [http://arxiv.org/abs/2305.18231](http://arxiv.org/abs/2305.18231)

    本文提出了一种基于分数的生成模型的两阶段方法，该方法在图像压缩领域取得了显著的表现，实验证明该方法在一定比特率下能够提高图像的感知质量。

    

    尽管扩散生成模型在文本到图像生成中取得了巨大的成功，但在图像压缩领域复制这个成功却很困难。在本文中，我们展示了扩散模型可以显著提高在给定比特率下的感知质量，通过 FID 分数评估，表现超越了 PO-ELIC 和 HiFiC 的现有方法。我们通过一个简单但在理论上有动机的两阶段方法实现了这一点，该方法结合了以 MSE 为目标的自动编码器和一个进一步基于分数的解码器。然而，正如我们将展示的那样，实现细节很重要，最佳设计决策可能与典型的文本到图像模型有很大不同。

    Despite the tremendous success of diffusion generative models in text-to-image generation, replicating this success in the domain of image compression has proven difficult. In this paper, we demonstrate that diffusion can significantly improve perceptual quality at a given bit-rate, outperforming state-of-the-art approaches PO-ELIC and HiFiC as measured by FID score. This is achieved using a simple but theoretically motivated two-stage approach combining an autoencoder targeting MSE followed by a further score-based decoder. However, as we will show, implementation details matter and the optimal design decisions can differ greatly from typical text-to-image models.
    
[^197]: 谷歌地图中的大规模可扩展逆强化学习

    Massively Scalable Inverse Reinforcement Learning in Google Maps. (arXiv:2305.11290v1 [cs.LG])

    [http://arxiv.org/abs/2305.11290](http://arxiv.org/abs/2305.11290)

    本文提出了一种新的逆强化学习算法（RHIP），通过图压缩、并行化和基于主特征向量的问题初始化解决了全球规模的MDPs、大型数据集和高度参数化的模型的问题，在谷歌地图中实现了16-24%的全球路线质量改进。

    

    优化人类潜在偏好是路线推荐中的巨大挑战，全球可扩展解决方案仍是一个未解决的问题。尽管过去的研究为逆强化学习的应用创建了越来越通用的解决方案，但这些解决方案尚未成功扩展到世界规模的MDP、大型数据集和高度参数化的模型，分别涉及数亿个状态、轨迹和参数。本文通过一系列的改进，聚焦于图压缩、并行化和基于主特征向量的问题初始化，突破以往的限制。我们引入了逆向规划递进地平面(RHIP)，它可以概括现有的工作，并通过其规划水平控制关键性能平衡。我们的策略在全球路线质量方面实现了16-24%的改进，就我们所知，它是迄今为止实现在真实世界环境下的最大的逆强化学习示例。我们的结果显示了更好的导航行为和路径规划。

    Optimizing for humans' latent preferences is a grand challenge in route recommendation, where globally-scalable solutions remain an open problem. Although past work created increasingly general solutions for the application of inverse reinforcement learning (IRL), these have not been successfully scaled to world-sized MDPs, large datasets, and highly parameterized models; respectively hundreds of millions of states, trajectories, and parameters. In this work, we surpass previous limitations through a series of advancements focused on graph compression, parallelization, and problem initialization based on dominant eigenvectors. We introduce Receding Horizon Inverse Planning (RHIP), which generalizes existing work and enables control of key performance trade-offs via its planning horizon. Our policy achieves a 16-24% improvement in global route quality, and, to our knowledge, represents the largest instance of IRL in a real-world setting to date. Our results show critical benefits to mor
    
[^198]: 一种轻量级CNN-Transformer模型用于学习旅行商问题

    A Lightweight CNN-Transformer Model for Learning Traveling Salesman Problems. (arXiv:2305.01883v1 [cs.LG])

    [http://arxiv.org/abs/2305.01883](http://arxiv.org/abs/2305.01883)

    本文提出了一种轻量级CNN-Transformer模型，使用CNN嵌入层和部分自注意力，能更好学习输入数据中的空间特征并消除冗余。实验表明该模型在解决旅行商问题方面表现出更好的性能，例如TSP解决方案质量和GPU内存使用方面。

    

    基于Transformer的模型即使在大规模旅行商问题（TSPs）中也表现出最先进的性能。然而，它们基于全连接的注意模型，存在大量的计算复杂性和GPU内存使用。我们提出了一种基于CNN嵌入层和部分自注意力的轻量级CNN-Transformer模型。与标准Transformer模型相比，我们的CNN-Transformer模型能够更好地学习输入数据中的空间特征。它还使用提出的部分自注意力消除了全连接注意模型中的相当数量的冗余。实验表明，与其他最先进的基于Transformer的模型相比，所提出的模型在TSP解决方案质量、GPU内存使用和推理时间方面都表现出更好的性能。我们的模型使用GPU内存约少20％，推理时间比其他最先进的基于Transformer的模型快45％。我们的代码公开可用，网址为https://g

    Transformer-based models show state-of-the-art performance even for large-scale Traveling Salesman Problems (TSPs). However, they are based on fully-connected attention models and suffer from large computational complexity and GPU memory usage. We propose a lightweight CNN-Transformer model based on a CNN embedding layer and partial self-attention. Our CNN-Transformer model is able to better learn spatial features from input data using a CNN embedding layer compared with the standard Transformer models. It also removes considerable redundancy in fully connected attention models using the proposed partial self-attention. Experiments show that the proposed model outperforms other state-of-the-art Transformer-based models in terms of TSP solution quality, GPU memory usage, and inference time. Our model consumes approximately 20% less GPU memory usage and has 45% faster inference time compared with other state-of-the-art Transformer-based models. Our code is publicly available at https://g
    
[^199]: ClusterNet：一种基于感知的分布式数据聚类模型

    ClusterNet: A Perception-Based Clustering Model for Scattered Data. (arXiv:2304.14185v1 [cs.LG])

    [http://arxiv.org/abs/2304.14185](http://arxiv.org/abs/2304.14185)

    这项工作介绍了ClusterNet，一种基于感知的分布式数据聚类模型，利用大规模数据集和基于点的深度学习模型，反映人类感知的聚类可分性。

    

    散点图中的聚类分离是一个通常由广泛使用的聚类技术（例如k-means或DBSCAN）来解决的任务。然而，由于这些算法基于非感知度量，它们的输出经常不能反映出人类聚类感知。为了弥合人类聚类感知和机器计算聚类之间的差距，我们提出了一种直接处理分布式数据的学习策略。为了在这些数据上学习感知聚类分离，我们进行了一项众包大规模数据集的工作，其中包括384个人群工作者对双变量数据的7,320个点聚类从属进行了标记。基于这些数据，我们能够训练ClusterNet，这是一个基于点的深度学习模型，被训练成反映人类感知的聚类可分性。为了在人类注释的数据上训练ClusterNet，我们省略了在2D画布上渲染散点图，而是使用了一个PointNet++架构，使其能够直接推理点云。在这项工作中，我们建立了一种基于感知的分布式数据聚类模型，ClusterNet。

    Cluster separation in scatterplots is a task that is typically tackled by widely used clustering techniques, such as for instance k-means or DBSCAN. However, as these algorithms are based on non-perceptual metrics, their output often does not reflect human cluster perception. To bridge the gap between human cluster perception and machine-computed clusters, we propose a learning strategy which directly operates on scattered data. To learn perceptual cluster separation on this data, we crowdsourced a large scale dataset, consisting of 7,320 point-wise cluster affiliations for bivariate data, which has been labeled by 384 human crowd workers. Based on this data, we were able to train ClusterNet, a point-based deep learning model, trained to reflect human perception of cluster separability. In order to train ClusterNet on human annotated data, we omit rendering scatterplots on a 2D canvas, but rather use a PointNet++ architecture enabling inference on point clouds directly. In this work, w
    
[^200]: SIFT: 稀疏等FLOP转换以最大限度提高训练效率

    SIFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency. (arXiv:2303.11525v1 [cs.LG])

    [http://arxiv.org/abs/2303.11525](http://arxiv.org/abs/2303.11525)

    本研究提出了一种名为SIFT的方法，用于提高深度神经网络的训练效率、准确性和表示能力，通过稀疏等FLOP转换，缩短训练时间。

    

    最近的研究探索了使用权重稀疏性来改善深度神经网络（DNN）的训练效率（与训练FLOPS相关的测试准确性）。 这些工作旨在减少训练FLOP，但使用稀疏权重进行训练通常会导致准确性损失或需要更长的训练周期，使得结果的训练效率不够清晰。 相比之下，我们专注于使用稀疏性提高准确性，同时使用与密集模型相同的FLOPS，并通过更高的准确性展示训练效率提高。 在本文中，我们介绍了SIFT，一组用作密集层的即插即用替代品来提高其表示能力和FLOP效率的稀疏等FLOP转换。 每个转换都由一个单一参数（稀疏级别）参数化，并提供更大的搜索空间以找到最佳的稀疏掩膜。

    Recent works have explored the use of weight sparsity to improve the training efficiency (test accuracy w.r.t training FLOPs) of deep neural networks (DNNs). These works aim to reduce training FLOPs but training with sparse weights often leads to accuracy loss or requires longer train schedules, making the resulting training efficiency less clear. In contrast, we focus on using sparsity to increase accuracy while using the same FLOPS as the dense model and show training efficiency gains through higher accuracy. In this work, we introduce SIFT, a family of Sparse Iso-FLOP Transformations which are used as drop-in replacements for dense layers to improve their representational capacity and FLOP efficiency. Each transformation is parameterized by a single parameter (sparsity level) and provides a larger search space to find optimal sparse masks. Without changing any training hyperparameters, replacing dense layers with SIFT leads to significant improvements across computer vision (CV) and
    
[^201]: 针对链接预测，对待不同的负样本有差异性：利用领域和范围约束丰富损失函数

    Treat Different Negatives Differently: Enriching Loss Functions with Domain and Range Constraints for Link Prediction. (arXiv:2303.00286v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00286](http://arxiv.org/abs/2303.00286)

    通过引入领域和范围约束，我们提出了基于语义的损失函数来区分不同质量的负样本，实验证明在链接预测任务上有效。

    

    知识图谱嵌入模型（KGEMs）用于与知识图谱（KGs）相关的各种任务，包括链接预测。它们使用考虑了一批得分三元组及其相应标签的损失函数进行训练。传统方法认为三元组的标签要么为真，要么为假。然而，最近的研究表明，并非所有的负样本应该被平等对待。与这一最近的假设一致，我们认为基于领域和范围约束在语义上有效的负样本可能是高质量的负样本。因此，损失函数应该将它们与语义上无效的负样本区别对待。为此，我们针对链接预测的三个主要损失函数提出了基于语义的版本。通过广泛和受控的实验设置，我们展示了所提出的损失函数在三个具有不同模式的公共基准KG上系统地提供了令人满意的结果。

    Knowledge graph embedding models (KGEMs) are used for various tasks related to knowledge graphs (KGs), including link prediction. They are trained with loss functions that are computed considering a batch of scored triples and their corresponding labels. Traditional approaches consider the label of a triple to be either true or false. However, recent works suggest that all negative triples should not be valued equally. In line with this recent assumption, we posit that negative triples that are semantically valid w.r.t. domain and range constraints might be high-quality negative triples. As such, loss functions should treat them differently from semantically invalid negative ones. To this aim, we propose semantic-driven versions for the three main loss functions for link prediction. In an extensive and controlled experimental setting, we show that the proposed loss functions systematically provide satisfying results on three public benchmark KGs underpinned with different schemas, whic
    
[^202]: 基于规约的延迟反馈顺序决策框架

    A Reduction-based Framework for Sequential Decision Making with Delayed Feedback. (arXiv:2302.01477v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01477](http://arxiv.org/abs/2302.01477)

    我们提出了一个基于规约的框架，可以将任何多批次算法转化为处理顺序决策中的随机延迟的高效算法。我们不仅在赌博机、表格型MDPs和表格型MGs方面取得了与现有结果相匹配或改进的成果，还首次对顺序决策中的延迟与函数逼近进行了研究。

    

    我们研究了一般多智能体顺序决策中的随机延迟反馈，包括赌博机问题、单智能体马尔可夫决策过程（MDPs）和马尔可夫博弈（MGs）。我们提出了一种新颖的基于规约的框架，将任何多批次算法转化为能处理顺序决策中的随机延迟的高效算法。通过将不同的多批次算法插入我们的框架中，我们提供了几个示例，证明我们的框架不仅匹配或改进了现有的赌博机、表格型MDPs和表格型MGs的结果，还首次对顺序决策中的延迟与函数逼近进行了研究。总之，我们为多智能体顺序决策中的延迟反馈提供了一套完整的尖锐结果。

    We study stochastic delayed feedback in general multi-agent sequential decision making, which includes bandits, single-agent Markov decision processes (MDPs), and Markov games (MGs). We propose a novel reduction-based framework, which turns any multi-batched algorithm for sequential decision making with instantaneous feedback into a sample-efficient algorithm that can handle stochastic delays in sequential decision making. By plugging different multi-batched algorithms into our framework, we provide several examples demonstrating that our framework not only matches or improves existing results for bandits, tabular MDPs, and tabular MGs, but also provides the first line of studies on delays in sequential decision making with function approximation. In summary, we provide a complete set of sharp results for multi-agent sequential decision making with delayed feedback.
    
[^203]: AMPNet: 基于注意力的消息传递用于图神经网络

    AMPNet: Attention as Message Passing for Graph Neural Networks. (arXiv:2210.09475v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.09475](http://arxiv.org/abs/2210.09475)

    AMPNet是一种用于图神经网络的基于注意力的消息传递层，能够对节点进行逐个特征编码，并通过跨节点注意力模型特征级别的交互。在实际生物系统上进行广泛基准测试表明，AMPNet在fMRI信号重建方面优于现有基准，并通过案例研究验证了其发现有意义的特征级别交互的能力。

    

    图神经网络（GNNs）已成为一种强大的用于图结构数据的表示学习框架。传统GNNs的一个关键限制是将每个节点表示为一个单一的特征向量，可能忽视了关于个体节点特征的复杂细节。我们提出了一种基于注意力的消息传递层AMPNet，用于GNNs，它对节点进行逐个特征编码，并在消息传递步骤中通过跨节点注意力模型特征级别的交互。我们通过在真实生物系统（如fMRI脑活动记录和空间基因组数据）上进行广泛的基准测试，证明了AMPNet的能力，它在fMRI信号重建方面相比现有基准提高了20％，在添加位置嵌入后又进一步提高了8％。最后，我们通过对生物系统的案例研究验证了AMPNet发现有意义的特征级别交互的能力。我们预计我们的架构将被广泛应用于图神经网络的研究中。

    Graph Neural Networks (GNNs) have emerged as a powerful representation learning framework for graph-structured data. A key limitation of conventional GNNs is their representation of each node with a singular feature vector, potentially overlooking intricate details about individual node features. Here, we propose an Attention-based Message-Passing layer for GNNs (AMPNet) that encodes individual features per node and models feature-level interactions through cross-node attention during message-passing steps. We demonstrate the abilities of AMPNet through extensive benchmarking on real-world biological systems such as fMRI brain activity recordings and spatial genomic data, improving over existing baselines by 20% on fMRI signal reconstruction, and further improving another 8% with positional embedding added. Finally, we validate the ability of AMPNet to uncover meaningful feature-level interactions through case studies on biological systems. We anticipate that our architecture will be h
    
[^204]: 高效量子不可知不当决策树学习

    Efficient Quantum Agnostic Improper Learning of Decision Trees. (arXiv:2210.00212v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2210.00212](http://arxiv.org/abs/2210.00212)

    本文提出了第一个无需成员查询在多项式时间内学习大小为$t$的决策树的算法，并成功量化了Kalai和Kanade的不知性增强算法，得到了第一个高效的量子不知性增强算法。

    

    不知性设置是最类似于学习对抗噪声的PAC模型的最困难的泛化。在本文中，我们提出了一个Poly$(n,t,{\frac{1}{\varepsilon}})$量子算法，用于在不知性设置中无需成员查询即可学习大小为$t$的决策树，并且实例间具有均匀边际。我们的算法是第一个在多项式时间内学习决策树的算法（经典或量子），且无需成员查询。我们展示了如何通过设计量子版本的Goldreich-Levin算法，使用高度偏置的函数预言机来构建量子不知性弱学习器。我们展示了如何量化Kalai和Kanade（NIPS 2009）的不知性增强算法，以获得第一个高效的量子不知性增强算法。我们的量子增强算法在适应性量子增强算法中，所有弱学习器偏差依赖性方面都具有多项式改进，同时保留了在$V$中的标准加速度。

    The agnostic setting is the hardest generalization of the PAC model since it is akin to learning with adversarial noise. In this paper, we give a poly$(n,t,{\frac{1}{\varepsilon}})$ quantum algorithm for learning size $t$ decision trees with uniform marginal over instances, in the agnostic setting, without membership queries. Our algorithm is the first algorithm (classical or quantum) for learning decision trees in polynomial time without membership queries. We show how to construct a quantum agnostic weak learner by designing a quantum version of the classical Goldreich-Levin algorithm that works with strongly biased function oracles. We show how to quantize the agnostic boosting algorithm by Kalai and Kanade (NIPS 2009) to obtain the first efficient quantum agnostic boosting algorithm. Our quantum boosting algorithm has a polynomial improvement in the dependence of the bias of the weak learner over all adaptive quantum boosting algorithms while retaining the standard speedup in the V
    
[^205]: 量子密度矩阵在经典问答和图像分类中的应用

    Application of Quantum Density Matrix in Classical Question Answering and Classical Image Classification. (arXiv:2203.11155v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2203.11155](http://arxiv.org/abs/2203.11155)

    该论文将量子密度矩阵应用于经典问答和图像分类中，证明了其可以提高任务的效率，尤其在图像分类中取得了优秀的性能表现。

    

    量子密度矩阵可表示整个量子系统的全部信息，将密度矩阵用于经典问答任务可以更加有效地实现问题回答。本论文设计了一种基于LSTM的新机制，以应对输入为矩阵的情况，并将该机制应用于卷积神经网络进行QA问题的求解，同时也证明了量子密度矩阵可以增强经典图像分类中的特征信息和特征之间的关系。实验结果表明，该新框架在CIFAR-10数据集上的性能优于传统的基于CNN的分类方法。

    Quantum density matrix represents all the information of the entire quantum system, and novel models of meaning employing density matrices naturally model linguistic phenomena such as hyponymy and linguistic ambiguity, among others in quantum question answering tasks. Naturally, we argue that applying the quantum density matrix into classical Question Answering (QA) tasks can show more effective performance. Specifically, we (i) design a new mechanism based on Long Short-Term Memory (LSTM) to accommodate the case when the inputs are matrixes; (ii) apply the new mechanism to QA problems with Convolutional Neural Network (CNN) and gain the LSTM-based QA model with the quantum density matrix. Experiments of our new model on TREC-QA and WIKI-QA data sets show encouraging results. Similarly, we argue that the quantum density matrix can also enhance the image feature information and the relationship between the features for the classical image classification. Thus, we (i) combine density mat
    

