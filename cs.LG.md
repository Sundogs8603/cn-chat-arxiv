# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Hierarchical Randomized Smoothing.](http://arxiv.org/abs/2310.16221) | 分层随机平滑是一种在复杂数据上进行鲁棒性认证的解决方案，通过只在一个对象的子集上添加随机噪声，以更有针对性的方式提供了更强的鲁棒性保证和高准确性。 |
| [^2] | [Convergence of Sign-based Random Reshuffling Algorithms for Nonconvex Optimization.](http://arxiv.org/abs/2310.15976) | 该论文通过证明signSGD算法在非凸优化问题中的随机重排（SignRR）的收敛性，弥补了现有分析中的缺陷，提出了SignRVR和SignRVM算法，并且都以较快的收敛速度收敛于全局最优解。 |
| [^3] | [Predicting Transcription Factor Binding Sites using Transformer based Capsule Network.](http://arxiv.org/abs/2310.15202) | 本文提出了一种基于Transformer的胶囊网络DNABERT-Cap，用于预测转录因子结合位点。该模型在预测中利用了双向编码器、胶囊层、卷积和双向长短时记忆层的特征，并通过对这些特征的联合优化构建了转录因子结合位点的预测器。 |
| [^4] | [Relearning Forgotten Knowledge: on Forgetting, Overfit and Training-Free Ensembles of DNNs.](http://arxiv.org/abs/2310.11094) | 本论文介绍一种新的评估过拟合的得分，该得分通过监测深度模型在验证数据上的遗忘速率来衡量。实证结果发现，尽管整体上泛化性能得到改善，但在数据空间的某些区域中，泛化性能可能会下降。这一观察结果有助于澄清关于深度神经网络中过拟合的困惑情况。 |
| [^5] | [United We Stand: Using Epoch-wise Agreement of Ensembles to Combat Overfit.](http://arxiv.org/abs/2310.11077) | 本论文提出了一种新的深度网络集成分类器，通过分析和实证发现过拟合时分类器之间的方差增加，基于此构建了一种通过整个训练过程中最具一致性的预测结果来对抗过拟合的方法。在实验中表明，这种方法在多个图像和文本分类任务上的表现优于传统的集成方法。 |
| [^6] | [Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis.](http://arxiv.org/abs/2310.10477) | 该论文介绍了一种基于错误分析的对齐策略，通过暴露大型语言模型的错误输出并进行评估，以理解内部原因。通过这种方法，有毒回应可以转化为模型对齐的指导调谐语料，从而提高模型的安全性并训练其进行自我批评。 |
| [^7] | [Can We Edit Multimodal Large Language Models?.](http://arxiv.org/abs/2310.08475) | 本文提出了编辑多模式大型语言模型（MLLMs）的挑战，并构建了一个新的基准用于评估和比较不同编辑方法的效果。实验结果表明，编辑多模式LLMs仍然存在困难，但这项工作为NLP社区提供了宝贵的见解。 |
| [^8] | [DSAC-T: Distributional Soft Actor-Critic with Three Refinements.](http://arxiv.org/abs/2310.05858) | 本论文介绍了DSAC-T，通过评论者梯度调整、双值分布学习和基于方差的目标回报裁剪等三个改进对标准DSAC进行了改进，解决了标准DSAC存在的不稳定学习过程和对任务特定奖励缩放的问题，提高了算法的性能和适应性。 |
| [^9] | [Attributing Learned Concepts in Neural Networks to Training Data.](http://arxiv.org/abs/2310.03149) | 通过将数据归因方法与概念探测方法相结合，研究了神经网络中学习到的概念与训练数据的关系，并发现概念的位置和稀疏性并不完全依赖于少量特定示例。 |
| [^10] | [Ophiuchus: Scalable Modeling of Protein Structures through Hierarchical Coarse-graining SO(3)-Equivariant Autoencoders.](http://arxiv.org/abs/2310.02508) | Ophiuchus是一个通过分层粗粒化SO(3)-等变自编码器对蛋白质结构进行可扩展建模的模型，它能在高分辨率下操作所有重原子，同时捕捉到结构的重复和分层模式。 |
| [^11] | [Mini-BEHAVIOR: A Procedurally Generated Benchmark for Long-horizon Decision-Making in Embodied AI.](http://arxiv.org/abs/2310.01824) | Mini-BEHAVIOR是一个面向具身人工智能的基准，旨在挑战智能体解决类似于日常挑战的复杂活动，并通过过程生成实现了无限的任务变化和对开放式学习的支持。 |
| [^12] | [Bayesian Design Principles for Frequentist Sequential Learning.](http://arxiv.org/abs/2310.00806) | 该论文提出了一种贝叶斯设计原则用于优化频率式序贯学习问题的通用理论，通过生成“算法信念”并使用贝叶斯后验进行决策，实现了无先验的贝叶斯类型算法在对抗性环境中的泛化和优化应用。 |
| [^13] | [Data is often loadable in short depth: Quantum circuits from tensor networks for finance, images, fluids, and proteins.](http://arxiv.org/abs/2309.13108) | 使用基于张量网络的电路编译方法AMLET，该方法可以解决量子电路加载经典数据的“输入问题”。作者在金融、图像、流体和蛋白质领域进行了广泛的数值实验，并展示了该方法的有效性。 |
| [^14] | [Early diagnosis of autism spectrum disorder using machine learning approaches.](http://arxiv.org/abs/2309.11646) | 本文利用机器学习方法旨在提供自闭症谱系障碍(ASD)的早期诊断，并通过研究不同算法寻找最显著的特征，自动化诊断过程。 |
| [^15] | [PromptTTS++: Controlling Speaker Identity in Prompt-Based Text-to-Speech Using Natural Language Descriptions.](http://arxiv.org/abs/2309.08140) | PromptTTS++是一种基于提示的文本转语音系统，可以使用自然语言描述控制说话者身份。与现有研究不同，该方法利用说话者提示来学习自然语言描述与声学特征的映射。 |
| [^16] | [Random postprocessing for combinatorial Bayesian optimization.](http://arxiv.org/abs/2309.02842) | 针对组合贝叶斯优化，我们研究了一种随机后处理方法，严格禁止数据集中的重复样本，结果表明此方法显著减少了顺序步骤数，特别是在最大后验估计的情况下，为解决高维问题中贝叶斯优化的收敛速度慢提供了一种简单但通用的策略。 |
| [^17] | [An Adaptive Tangent Feature Perspective of Neural Networks.](http://arxiv.org/abs/2308.15478) | 本研究提出了一个切向特征视角的框架，通过线性变换和结构正则化优化神经网络的特征学习，从而更好地理解神经网络的特征学习过程。在实验证明了该方法在回归问题中的有效性，并提供了对核对齐现象的细致分析。 |
| [^18] | [Region-Disentangled Diffusion Model for High-Fidelity PPG-to-ECG Translation.](http://arxiv.org/abs/2308.13568) | 这项工作引入了区域解耦扩散模型 (RDDM)，用于将PPG转换为ECG信号。RDDM通过选择性地向感兴趣区域添加噪声来捕捉ECG信号的复杂时间动态。 |
| [^19] | [Audio Generation with Multiple Conditional Diffusion Model.](http://arxiv.org/abs/2308.11940) | 本论文提出了一种使用多条件扩散模型进行音频生成的方法。通过引入内容和风格等额外条件，增强了现有模型的可控性。这种方法可以精确控制生成音频的时间顺序、音高和能量。由于缺乏合适的数据集和评估指标，作者整合了现有数据集并进行了实验验证。 |
| [^20] | [Bounded P-values in Parametric Programming-based Selective Inference.](http://arxiv.org/abs/2307.11351) | 本研究提出了一种降低参数规划选择性推断计算成本的方法，通过计算p值的上界和下界来保证所需精度。 |
| [^21] | [PreDiff: Precipitation Nowcasting with Latent Diffusion Models.](http://arxiv.org/abs/2307.10422) | 本论文提出了PreDiff方法，使用条件隐式扩散模型进行降水近期预测。同时，引入显式知识控制机制以满足特定领域的物理约束。 |
| [^22] | [Adaptive Topological Feature via Persistent Homology: Filtration Learning for Point Clouds.](http://arxiv.org/abs/2307.09259) | 本文提出了一种通过使用神经网络自适应学习滤波器的方法，用于增强点云机器学习方法的准确性。同时，还提出了一种具有等变性的神经网络架构，以使得持久同调具有等变性。 |
| [^23] | [Deep learning for dynamic graphs: models and benchmarks.](http://arxiv.org/abs/2307.06104) | 本文对深度学习动态图领域进行了调查，总结了学习时间和空间信息的最新优势，并对最流行的方法进行了公平的性能比较，为评估新架构和方法建立了一个可靠的基准模型。 |
| [^24] | [Look, Remember and Reason: Visual Reasoning with Grounded Rationales.](http://arxiv.org/abs/2306.17778) | 在这项研究中，我们借鉴了人类视觉问题解决的方法，通过三个步骤（看、记住、推理）逐步提取视觉信息来解决复杂的视觉推理问题，从而使大型语言模型能够有效解决这些问题。 |
| [^25] | [Enhancing training of physics-informed neural networks using domain-decomposition based preconditioning strategies.](http://arxiv.org/abs/2306.17648) | 这项研究提出了一种基于域分解的预条件策略，用于增强物理信息神经网络的训练。该方法通过非线性预条件器改进了L-BFGS优化器的收敛性，并提供了更准确的偏微分方程解。加性预条件器还具有并行性，为模型并行提供了新的方法。 |
| [^26] | [Safe Model-Based Multi-Agent Mean-Field Reinforcement Learning.](http://arxiv.org/abs/2306.17052) | 本文提出了Safe-$\text{M}^3$-UCRL算法，通过使用模型中的认知不确定性和对数障碍方法，实现了在未知转移动态情况下达到安全策略的优化，成功解决了大规模多智能体协调问题。 |
| [^27] | [Leveraging Locality and Robustness to Achieve Massively Scalable Gaussian Process Regression.](http://arxiv.org/abs/2306.14731) | 该研究提出了一种新的思路，通过探索 GPnn 的鲁棒性和极限行为实现大规模高斯过程回归，即使在出现重大小错误的情况下只需要花费少量的工作进行参数估计即可实现高 MSE 准确性。同时，该研究成功解决了加性噪声方差带来的不确定度校准和 NLL 准确性问题。 |
| [^28] | [Active Sparse Conversations for Improved Audio-Visual Embodied Navigation.](http://arxiv.org/abs/2306.04047) | 本文提出了CAVEN - 一种具有对话功能的音频视觉导航代理，能够向人类/神谕提出导航问题并处理神谕回答以协助自主导航。该系统基于多模态分层强化学习方法，并使用三个低级策略进行引导。 |
| [^29] | [Russo-Ukrainian War: Prediction and explanation of Twitter suspension.](http://arxiv.org/abs/2306.03502) | 本研究分析了Twitter封禁机制，揭示了存在的政策违规、宣传、垃圾邮件等问题，并发现拥有更多粉丝的账户更可能被封禁。这些发现可以让Twitter和其他社交网络改进其内容过滤机制。 |
| [^30] | [SimFBO: Towards Simple, Flexible and Communication-efficient Federated Bilevel Learning.](http://arxiv.org/abs/2305.19442) | SimFBO和其ShroFBO变体提出了一个简单、灵活且通信高效的FBO框架，可以应用于元学习和超参数优化任务。 |
| [^31] | [Embedding Inequalities for Barron-type Spaces.](http://arxiv.org/abs/2305.19082) | 本文测量了Barron空间和谱Barron空间之间的关系，并提供了嵌入不等式。 |
| [^32] | [Understanding Multi-phase Optimization Dynamics and Rich Nonlinear Behaviors of ReLU Networks.](http://arxiv.org/abs/2305.12467) | 本文对于ReLU神经网络通过Gradient Flow训练的两层模型在线性可分数据上进行了完整的理论分析，揭示了整个训练过程中的四个不同阶段，显示出一个从简化到复杂的学习趋势。 |
| [^33] | [Dynamic Gradient Balancing for Enhanced Adversarial Attacks on Multi-Task Models.](http://arxiv.org/abs/2305.12066) | 本文提出了动态梯度平衡攻击（DGBA）框架来攻击多任务模型，并通过实验回答了多任务模型的对抗攻击的安全性、多任务攻击和对抗训练是否增强多任务模型的鲁棒性等安全研究问题。 |
| [^34] | [Protein Complex Invariant Embedding with Cross-Gate MLP is A One-Shot Antibody Designer.](http://arxiv.org/abs/2305.09480) | 本文提出了一种深度生成模型，可以一次性地共同设计抗体CDR的1D序列和3D结构，解决几何建模和低效推断的问题。 |
| [^35] | [Designing Discontinuities.](http://arxiv.org/abs/2305.08559) | 本文通过一种量化理论方法优化不连续变量的设计，以平衡效应大小的增益和损失，并开发了一种计算效率高的强化学习算法。 |
| [^36] | [A Survey on Out-of-Distribution Detection in NLP.](http://arxiv.org/abs/2305.03236) | 这篇论文首次综述了最新的OOD检测方法在自然语言处理中的应用。根据算法使用的数据，将方法分成三类，并介绍了相关数据集、应用和度量方法。 |
| [^37] | [It is all about where you start: Text-to-image generation with seed selection.](http://arxiv.org/abs/2304.14530) | 该论文研究了文本到图像生成中训练数据不平衡对模型的影响，并提出了一种高效的方法：在噪声空间中选择适当的生成种子。该方法能够正确生成罕见的概念，而不需要重新训练模型。 |
| [^38] | [Heterogeneous-Agent Reinforcement Learning.](http://arxiv.org/abs/2304.09870) | 提出了一种异构智能体强化学习（HARL）算法，解决了协作多智能体强化学习中的参数共享限制，同时通过引入多智能体优势分解引理和序列更新方案，建立了异构智能体信任区域学习（HATRL）算法及其易处理的逼近方式 HATRPO 和 HAPPO。此外，发现了一种名为异构智能体镜像学习（HAML）的新型框架，加强了对HATRPO和HAPPO的理论保证。 |
| [^39] | [Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models.](http://arxiv.org/abs/2304.08818) | 本文提出了一种高分辨率视频合成方法，通过引入时间维度并在图像序列上微调扩散模型，实现了对实际驾驶数据的模拟和创意内容创作的的良好效果。 |
| [^40] | [ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation.](http://arxiv.org/abs/2304.05977) | ImageReward是一种通用的文本到图像生成的人类喜好奖励模型，它可以通过收集专家的比较数据集来解决生成模型的问题，并且在人类评估中表现出色，有望成为一种用于评估和改进文本到图像合成的自动度量标准。 |
| [^41] | [Transfer Learning Across Heterogeneous Features For Efficient Tensor Program Generation.](http://arxiv.org/abs/2304.05430) | 本研究提出了适用于异构特征的迁移学习方法，在新的目标硬件上通过学习联合神经网络和硬件特征，解决了张量程序生成的自动调整问题。 |
| [^42] | [NeBLa: Neural Beer-Lambert for 3D Reconstruction of Oral Structures from Panoramic Radiographs.](http://arxiv.org/abs/2304.04027) | 该论文提出了一个新的框架：NeBLa，可以从全景放射线图中通过神经啤酒-兰伯特法重建精确的3D口腔结构模型。 |
| [^43] | [Identification of Negative Transfers in Multitask Learning Using Surrogate Models.](http://arxiv.org/abs/2303.14582) | 本文提出了一种通过代理建模来解决多任务学习中负迁移问题的方法，能够识别哪些源任务的子集会对目标任务有帮助。 |
| [^44] | [Towards provably efficient quantum algorithms for large-scale machine-learning models.](http://arxiv.org/abs/2303.03428) | 本论文提出了一种可能的针对通用（随机）梯度下降算法的高效量子解决方案，只要模型足够耗散和稀疏，具有小的学习率，并且可以缩放至 $O(T^2 \times \text{polylog}(n))$。在实践中，证明了在稀疏训练的情况下，量子计算可以显著提高效率。 |
| [^45] | [MDF-Net for Abnormality Detection by Fusing X-Rays with Clinical Data.](http://arxiv.org/abs/2302.13390) | 本研究提出了一种多模态融合方法MDF-Net，将临床数据与胸部X射线图像相结合，成功地提高了疾病定位的性能表现。 |
| [^46] | [How to Trust Your Diffusion Model: A Convex Optimization Approach to Conformal Risk Control.](http://arxiv.org/abs/2302.03791) | 本文提出了一种风险控制预测集（RCPS）程序的推广，称为$K$-RCPS，它允许为任何扩散模型提供逐个校准的未来样本间隔，并控制相对于基准真实图像的某种风险概念，同时保持最小平均区间长度。 |
| [^47] | [Editing Language Model-based Knowledge Graph Embeddings.](http://arxiv.org/abs/2301.10405) | 本文提出了一种新的任务——编辑基于语言模型的知识图谱嵌入，旨在实现对KG嵌入的数据高效和快速更新。针对这一任务，提出了一个简单而强大的方案——KGEditor，可以更好地更新特定事实而不影响其余部分的性能。 |
| [^48] | [gRoMA: a Tool for Measuring Deep Neural Networks Global Robustness.](http://arxiv.org/abs/2301.02288) | gRoMA是一种衡量DNN全局鲁棒性的创新工具，采用概率验证方法评估特定输出类别遭受到对抗性输入的概率。该工具可运行于预训练的黑盒分类模型上，并对整个模型和每个输入样本产生鲁棒性测量结果。 |
| [^49] | [One-shot domain adaptation in video-based assessment of surgical skills.](http://arxiv.org/abs/2301.00812) | 本文提出了一种元学习模型A-VBANet，可以通过一次性学习提供领域不可知的手术技能分类，成功地适应了模拟任务和腹腔镜胆囊切除术，为基于视频的手术技能评估提供了领域不可知程序。 |
| [^50] | [SHAP-XRT: The Shapley Value Meets Conditional Independence Testing.](http://arxiv.org/abs/2207.07038) | 本文展示了Shapley值解释方法与条件独立性测试之间的紧密关系，并介绍了一种基于条件随机化测试的测试过程SHAP-XRT，用于二分类问题中证明了Shapley值的边际贡献。 |
| [^51] | [Augmentation-Aware Self-Supervision for Data-Efficient GAN Training.](http://arxiv.org/abs/2205.15677) | 本文提出一种增强感知的自监督判别器用于对生成数据及其增强参数的预测，从而提高判别器的表现和生成模型的性能，实现了数据有效的 GAN 训练。 |
| [^52] | [Policy design in experiments with unknown interference.](http://arxiv.org/abs/2011.08174) | 本文研究了如何在有限数量的大簇中进行实验设计，估计和推断最大福利政策，并提出单波实验和多波实验的方法来解决溢出效应问题。 |
| [^53] | [Gasper: GrAph Signal ProcEssing in R.](http://arxiv.org/abs/2007.10642) | Gasper是一个在R中进行图形信号处理的包，同时提供了与SuiteSparse矩阵集合的接口。 |

# 详细

[^1]: 分层随机平滑

    Hierarchical Randomized Smoothing. (arXiv:2310.16221v1 [cs.LG])

    [http://arxiv.org/abs/2310.16221](http://arxiv.org/abs/2310.16221)

    分层随机平滑是一种在复杂数据上进行鲁棒性认证的解决方案，通过只在一个对象的子集上添加随机噪声，以更有针对性的方式提供了更强的鲁棒性保证和高准确性。

    

    真实世界的数据是复杂的，通常由可分解为多个实体的对象组成（例如，将图像分解为像素，将图形分解为相互连接的节点）。随机平滑是一种强大的框架，可以使模型在其输入的微小变化上具有证明的鲁棒性-通过在分类之前随机添加噪声来保证多数投票的鲁棒性。然而，当对手不是任意干扰整个对象（例如图像），而是对象的某个实体的子集（例如像素）时，通过随机平滑对这种复杂数据进行鲁棒性认证是具有挑战性的。作为解决方案，我们引入了分层随机平滑：我们通过仅在随机选择的实体子集上添加随机噪声来部分平滑对象。通过以比现有方法更有针对性的方式添加噪声，我们获得更强的鲁棒性保证，同时保持高准确性。我们使用不同的噪声分布初始化分层平滑，得到了新的鲁棒性保证。

    Real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). Randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs - by guaranteeing robustness of the majority vote when randomly adding noise before classification. Yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). As a solution, we introduce hierarchical randomized smoothing: We partially smooth objects by adding random noise only on a randomly selected subset of their entities. By adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. We initialize hierarchical smoothing using different noising distributions, yielding novel robustness
    
[^2]: 非凸优化的基于符号随机重排算法的收敛性研究

    Convergence of Sign-based Random Reshuffling Algorithms for Nonconvex Optimization. (arXiv:2310.15976v1 [cs.LG])

    [http://arxiv.org/abs/2310.15976](http://arxiv.org/abs/2310.15976)

    该论文通过证明signSGD算法在非凸优化问题中的随机重排（SignRR）的收敛性，弥补了现有分析中的缺陷，提出了SignRVR和SignRVM算法，并且都以较快的收敛速度收敛于全局最优解。

    

    由于其通信效率较高，signSGD在非凸优化中很受欢迎。然而，现有对signSGD的分析基于假设每次迭代中的数据都是有放回采样的，这与实际实现中数据的随机重排和顺序馈送进算法的情况相矛盾。为了弥补这一差距，我们证明了signSGD在非凸优化中的随机重排（SignRR）的首个收敛结果。给定数据集大小$n$，数据迭代次数$T$，和随机梯度的方差限制$\sigma^2$，我们证明了SignRR的收敛速度与signSGD相同，为$O(\log(nT)/\sqrt{nT} + \|\sigma\|_1)$ \citep{bernstein2018signsgd}。接着，我们还提出了 SignRVR 和 SignRVM，分别利用了方差约减梯度和动量更新，都以$O(\log(nT)/\sqrt{nT})$的速度收敛。与signSGD的分析不同，我们的结果不需要每次迭代中极大的批次大小与同等数量的梯度进行比较。

    signSGD is popular in nonconvex optimization due to its communication efficiency. Yet, existing analyses of signSGD rely on assuming that data are sampled with replacement in each iteration, contradicting the practical implementation where data are randomly reshuffled and sequentially fed into the algorithm. We bridge this gap by proving the first convergence result of signSGD with random reshuffling (SignRR) for nonconvex optimization. Given the dataset size $n$, the number of epochs of data passes $T$, and the variance bound of a stochastic gradient $\sigma^2$, we show that SignRR has the same convergence rate $O(\log(nT)/\sqrt{nT} + \|\sigma\|_1)$ as signSGD \citep{bernstein2018signsgd}. We then present SignRVR and SignRVM, which leverage variance-reduced gradients and momentum updates respectively, both converging at $O(\log(nT)/\sqrt{nT})$. In contrast with the analysis of signSGD, our results do not require an extremely large batch size in each iteration to be of the same order a
    
[^3]: 基于Transformer的胶囊网络预测转录因子结合位点

    Predicting Transcription Factor Binding Sites using Transformer based Capsule Network. (arXiv:2310.15202v1 [q-bio.GN])

    [http://arxiv.org/abs/2310.15202](http://arxiv.org/abs/2310.15202)

    本文提出了一种基于Transformer的胶囊网络DNABERT-Cap，用于预测转录因子结合位点。该模型在预测中利用了双向编码器、胶囊层、卷积和双向长短时记忆层的特征，并通过对这些特征的联合优化构建了转录因子结合位点的预测器。

    

    预测转录因子的结合位点对于理解它们如何调控基因表达以及如何通过治疗手段进行调节非常重要。尽管在过去几年里已经有相当多的工作针对这个问题进行了研究，但仍然有改进的空间。在这方面，本文提出了一种基于Transformer的胶囊网络DNABERT-Cap，用于利用ChIP-seq数据集挖掘预测转录因子结合位点。DNABERT-Cap是一个双向编码器，经过大量基因组DNA序列的预训练，并通过胶囊层进行最终预测。所提出的模型通过对包含双向编码器、胶囊层、卷积和双向长短时记忆层特征的联合优化，构建了转录因子结合位点的预测器。为了评估所提方法的效率，我们使用了五个细胞系的基准ChIP-seq数据集，包括A54。

    Prediction of binding sites for transcription factors is important to understand how they regulate gene expression and how this regulation can be modulated for therapeutic purposes. Although in the past few years there are significant works addressing this issue, there is still space for improvement. In this regard, a transformer based capsule network viz. DNABERT-Cap is proposed in this work to predict transcription factor binding sites mining ChIP-seq datasets. DNABERT-Cap is a bidirectional encoder pre-trained with large number of genomic DNA sequences, empowered with a capsule layer responsible for the final prediction. The proposed model builds a predictor for transcription factor binding sites using the joint optimisation of features encompassing both bidirectional encoder and capsule layer, along with convolutional and bidirectional long-short term memory layers. To evaluate the efficiency of the proposed approach, we use a benchmark ChIP-seq datasets of five cell lines viz. A54
    
[^4]: 重学已遗忘知识：关于遗忘，过拟合和无需训练的深度神经网络集成

    Relearning Forgotten Knowledge: on Forgetting, Overfit and Training-Free Ensembles of DNNs. (arXiv:2310.11094v1 [cs.LG])

    [http://arxiv.org/abs/2310.11094](http://arxiv.org/abs/2310.11094)

    本论文介绍一种新的评估过拟合的得分，该得分通过监测深度模型在验证数据上的遗忘速率来衡量。实证结果发现，尽管整体上泛化性能得到改善，但在数据空间的某些区域中，泛化性能可能会下降。这一观察结果有助于澄清关于深度神经网络中过拟合的困惑情况。

    

    深度神经网络中过拟合的不经常发生令人困惑。理论预测，随着模型变得更大，它们最终应该变得过度适应某个特定的训练集，从而导致泛化下降。然而，在图像分类的实证结果中，增加深度模型的训练时间或使用更大的模型几乎从不损害泛化。这是因为我们衡量过拟合的方式太过有限吗？在这里，我们引入了一种新的评估过拟合程度的得分，该得分监测深度模型在验证数据上的遗忘速率。这个分数表明，尽管整体上泛化性能得到改善，但在数据空间的某些区域中，泛化性能可能会下降。当用这种方式测量时，我们展示了过拟合可以在验证精度降低和不降低的情况下发生，并且可能比以前认为的更常见。这一观察结果可能有助于澄清前述的困惑局面。

    The infrequent occurrence of overfit in deep neural networks is perplexing. On the one hand, theory predicts that as models get larger they should eventually become too specialized for a specific training set, with ensuing decrease in generalization. In contrast, empirical results in image classification indicate that increasing the training time of deep models or using bigger models almost never hurts generalization. Is it because the way we measure overfit is too limited? Here, we introduce a novel score for quantifying overfit, which monitors the forgetting rate of deep models on validation data. Presumably, this score indicates that even while generalization improves overall, there are certain regions of the data space where it deteriorates. When thus measured, we show that overfit can occur with and without a decrease in validation accuracy, and may be more common than previously appreciated. This observation may help to clarify the aforementioned confusing picture. We use our obs
    
[^5]: 团结一致：利用分时一致性集合来对抗过拟合

    United We Stand: Using Epoch-wise Agreement of Ensembles to Combat Overfit. (arXiv:2310.11077v1 [cs.LG])

    [http://arxiv.org/abs/2310.11077](http://arxiv.org/abs/2310.11077)

    本论文提出了一种新的深度网络集成分类器，通过分析和实证发现过拟合时分类器之间的方差增加，基于此构建了一种通过整个训练过程中最具一致性的预测结果来对抗过拟合的方法。在实验中表明，这种方法在多个图像和文本分类任务上的表现优于传统的集成方法。

    

    深度神经网络已经成为解决许多图像分类任务的首选方法，主要是因为它们可以拟合原始图像上定义的非常复杂的函数。这种强大学习器的缺点是过拟合训练集的危险，导致泛化能力差，通常通过正则化和训练的“提前停止”来避免。在本文中，我们提出了一种新的深度网络集成分类器，它对抗过拟合非常有效。我们首先对回归模型进行理论分析，证明了当发生过拟合时，分类器之间的方差增加，这一点已在常用的深度网络中得到了实证。在这些结果的指导下，我们构建了一种新的基于集成的预测方法，旨在对抗过拟合，其中预测结果是通过整个训练过程中最具一致性的预测结果确定的。在多个图像和文本分类数据集上，我们展示了当常规集成遭受过拟合时，我们的方法能够更好地应对。

    Deep neural networks have become the method of choice for solving many image classification tasks, largely because they can fit very complex functions defined over raw images. The downside of such powerful learners is the danger of overfitting the training set, leading to poor generalization, which is usually avoided by regularization and "early stopping" of the training. In this paper, we propose a new deep network ensemble classifier that is very effective against overfit. We begin with the theoretical analysis of a regression model, whose predictions - that the variance among classifiers increases when overfit occurs - is demonstrated empirically in deep networks in common use. Guided by these results, we construct a new ensemble-based prediction method designed to combat overfit, where the prediction is determined by the most consensual prediction throughout the training. On multiple image and text classification datasets, we show that when regular ensembles suffer from overfit, ou
    
[^6]: 从挫折中获得智慧：通过错误分析对齐大型语言模型

    Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis. (arXiv:2310.10477v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.10477](http://arxiv.org/abs/2310.10477)

    该论文介绍了一种基于错误分析的对齐策略，通过暴露大型语言模型的错误输出并进行评估，以理解内部原因。通过这种方法，有毒回应可以转化为模型对齐的指导调谐语料，从而提高模型的安全性并训练其进行自我批评。

    

    大型语言模型（LLMs）的快速发展既带来了机遇，也带来了挑战，特别是在意外生成有害和有毒回应方面。传统的对齐方法致力于引导LLMs朝着期望的性能发展并保护它们免受恶意内容的侵害，而本研究提出了一种基于错误分析的全新对齐策略，通过有意暴露LLMs的缺陷输出并进行深入评估，以完全理解内部原因，通过自然语言分析。因此，有毒回应可以转化为模型对齐的指导调谐语料，LLMs不仅可以避免生成有缺陷的回应，还可以训练其进行自我批评，发挥其辨别有毒内容的内在能力。实验结果表明，所提出的方法在安全指令遵循方面优于传统的对齐技术，同时还保持了卓越的效率。

    The rapid advancement of large language models (LLMs) presents both opportunities and challenges, particularly concerning unintentional generation of harmful and toxic responses. While the traditional alignment methods strive to steer LLMs towards desired performance and shield them from malicious content, this study proposes a novel alignment strategy rooted in mistake analysis by exposing LLMs to flawed outputs purposefully and then conducting a thorough assessment to fully comprehend internal reasons via natural language analysis. Thus, toxic responses can be transformed into instruction tuning corpus for model alignment, and LLMs can not only be deterred from generating flawed responses but also trained to self-criticize, leveraging its innate ability to discriminate toxic content. Experimental results demonstrate that the proposed method outperforms conventional alignment techniques for safety instruction following, while maintaining superior efficiency.
    
[^7]: 我们能编辑多模式大型语言模型吗？

    Can We Edit Multimodal Large Language Models?. (arXiv:2310.08475v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.08475](http://arxiv.org/abs/2310.08475)

    本文提出了编辑多模式大型语言模型（MLLMs）的挑战，并构建了一个新的基准用于评估和比较不同编辑方法的效果。实验结果表明，编辑多模式LLMs仍然存在困难，但这项工作为NLP社区提供了宝贵的见解。

    

    本文关注编辑多模式大型语言模型（MLLMs）。与编辑单模式LLMs相比，多模式模型的编辑更具挑战性，需要更高级别的审查和慎重考虑。为了促进这一领域的研究，我们构建了一个新的基准，称为MMEdit，用于编辑多模式LLMs，并建立了一套创新的度量标准进行评估。我们进行了包括各种模型编辑基线的综合实验，并分析了编辑多模式LLMs的不同组件的影响。根据经验，我们发现之前的基线在某种程度上可以实现编辑多模式LLMs，但效果仍然不理想，表明这个任务可能存在的困难。我们希望我们的工作能为NLP社区提供见解。代码和数据集可在https://github.com/zjunlp/EasyEdit获取。

    In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.
    
[^8]: DSAC-T: 带有三个改进的分布式软角色扮演者—评论者

    DSAC-T: Distributional Soft Actor-Critic with Three Refinements. (arXiv:2310.05858v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.05858](http://arxiv.org/abs/2310.05858)

    本论文介绍了DSAC-T，通过评论者梯度调整、双值分布学习和基于方差的目标回报裁剪等三个改进对标准DSAC进行了改进，解决了标准DSAC存在的不稳定学习过程和对任务特定奖励缩放的问题，提高了算法的性能和适应性。

    

    强化学习在处理复杂的决策和控制任务方面已经被证明非常有效。然而，常见的无模型的强化学习方法往往面临严重的性能下降问题，这是由于众所周知的过估计问题所引起的。作为对这个问题的回应，我们最近引入了一种离线策略的强化学习算法，称为分布式软角色扮演者评论者（DSAC或DSAC-v1），它通过学习连续的高斯值分布来有效提高值估计的准确性。然而，标准DSAC也存在一些缺点，包括时而不稳定的学习过程和对任务特定的奖励缩放的需求，这可能会阻碍其在一些特殊任务中的整体性能和适应性。本文进一步引入了三个对标准DSAC的重要改进，以解决这些问题。这些改进包括评论者梯度调整、双值分布学习和基于方差的目标回报裁剪。修改后的强化学习算法称为DSAC-T。

    Reinforcement learning (RL) has proven to be highly effective in tackling complex decision-making and control tasks. However, prevalent model-free RL methods often face severe performance degradation due to the well-known overestimation issue. In response to this problem, we recently introduced an off-policy RL algorithm, called distributional soft actor-critic (DSAC or DSAC-v1), which can effectively improve the value estimation accuracy by learning a continuous Gaussian value distribution. Nonetheless, standard DSAC has its own shortcomings, including occasionally unstable learning processes and needs for task-specific reward scaling, which may hinder its overall performance and adaptability in some special tasks. This paper further introduces three important refinements to standard DSAC in order to address these shortcomings. These refinements consist of critic gradient adjusting, twin value distribution learning, and variance-based target return clipping. The modified RL algorithm 
    
[^9]: 将神经网络中学习到的概念归因于训练数据

    Attributing Learned Concepts in Neural Networks to Training Data. (arXiv:2310.03149v1 [cs.LG])

    [http://arxiv.org/abs/2310.03149](http://arxiv.org/abs/2310.03149)

    通过将数据归因方法与概念探测方法相结合，研究了神经网络中学习到的概念与训练数据的关系，并发现概念的位置和稀疏性并不完全依赖于少量特定示例。

    

    现在有大量的证据表明，深度学习模型学习到了某些可解释的人类特征，作为其对数据的内部表示的一部分。由于拥有正确（或错误）的概念对于可信赖的机器学习系统至关重要，自然而然地我们想要知道在给定层次上，模型原始训练集中的哪些输入对于学习一个概念最为重要。为了回答这个问题，我们将数据归因方法与探测模型学习到的概念的方法相结合。通过在一系列网络层次上训练网络和探测模型，并使用最近开发的用于大规模数据归因的TRAK方法，我们对两个概念数据集进行训练网络和探测模型的集合。我们发现一些证据表明，通过移除对一个概念具有最高归因的前10000张图像并重新训练模型，概念在网络中的位置以及概念的探测稀疏性并没有发生改变。这表明，与依赖于少量特定示例不同，用于确定概念的特征具有较高的独立性。

    By now there is substantial evidence that deep learning models learn certain human-interpretable features as part of their internal representations of data. As having the right (or wrong) concepts is critical to trustworthy machine learning systems, it is natural to ask which inputs from the model's original training set were most important for learning a concept at a given layer. To answer this, we combine data attribution methods with methods for probing the concepts learned by a model. Training network and probe ensembles for two concept datasets on a range of network layers, we use the recently developed TRAK method for large-scale data attribution. We find some evidence for convergence, where removing the 10,000 top attributing images for a concept and retraining the model does not change the location of the concept in the network nor the probing sparsity of the concept. This suggests that rather than being highly dependent on a few specific examples, the features that inform the 
    
[^10]: Ophiuchus: 通过分层粗粒化SO(3)-等变自编码器对蛋白质结构进行可扩展建模

    Ophiuchus: Scalable Modeling of Protein Structures through Hierarchical Coarse-graining SO(3)-Equivariant Autoencoders. (arXiv:2310.02508v1 [cs.LG])

    [http://arxiv.org/abs/2310.02508](http://arxiv.org/abs/2310.02508)

    Ophiuchus是一个通过分层粗粒化SO(3)-等变自编码器对蛋白质结构进行可扩展建模的模型，它能在高分辨率下操作所有重原子，同时捕捉到结构的重复和分层模式。

    

    天然蛋白质的三维原生态状态显示出重复和分层模式。然而，传统的基于图的蛋白质结构建模通常局限于在单一精细化分辨率内操作，并且缺乏捕捉高级构建模块的中间神经网络架构。我们通过引入Ophiuchus来填补这个差距，它是一个SO(3)-等变粗粒化模型，可以高效地操作标准蛋白质残基的所有重原子，并同时尊重它们的相关对称性。我们的模型与当前采用图模型的方法不同，而是专注于局部卷积粗化，以在对数线性长度复杂度下模拟序列模体之间的相互作用。我们使用PDB单体的连续片段对Ophiuchus进行训练，研究其在不同压缩率下的重构能力。我们检查学习到的潜空间，并展示其在构象插值中的快速使用，将插值轨迹与结构进行比较。

    Three-dimensional native states of natural proteins display recurring and hierarchical patterns. Yet, traditional graph-based modeling of protein structures is often limited to operate within a single fine-grained resolution, and lacks hourglass neural architectures to learn those high-level building blocks. We narrow this gap by introducing Ophiuchus, an SO(3)-equivariant coarse-graining model that efficiently operates on all heavy atoms of standard protein residues, while respecting their relevant symmetries. Our model departs from current approaches that employ graph modeling, instead focusing on local convolutional coarsening to model sequence-motif interactions in log-linear length complexity. We train Ophiuchus on contiguous fragments of PDB monomers, investigating its reconstruction capabilities across different compression rates. We examine the learned latent space and demonstrate its prompt usage in conformational interpolation, comparing interpolated trajectories to structure
    
[^11]: Mini-BEHAVIOR：面向具身人工智能的长期决策制定的过程生成基准

    Mini-BEHAVIOR: A Procedurally Generated Benchmark for Long-horizon Decision-Making in Embodied AI. (arXiv:2310.01824v1 [cs.AI])

    [http://arxiv.org/abs/2310.01824](http://arxiv.org/abs/2310.01824)

    Mini-BEHAVIOR是一个面向具身人工智能的基准，旨在挑战智能体解决类似于日常挑战的复杂活动，并通过过程生成实现了无限的任务变化和对开放式学习的支持。

    

    我们提出了Mini-BEHAVIOR，一种新颖的面向具身人工智能的基准，挑战智能体利用推理和决策技能解决类似于日常人类挑战的复杂活动。Mini-BEHAVIOR环境是一个快速，现实的Gridworld环境，既具有快速原型设计和易用性的优点，同时也保留了复杂具身人工智能基准中符号级的物理现实感和复杂性。我们引入了关键特性，如过程生成，以实现无限的任务变化和对开放式学习的支持。Mini-BEHAVIOR提供了原始BEHAVIOR基准中各种家务任务的实现，以及用于数据收集和强化学习代理训练的入门代码。总之，Mini-BEHAVIOR为评估具身人工智能中的决策制定和规划解决方案提供了一个快速、开放式的基准。它作为研究的用户友好的入口点，促进了评估和发展过程。

    We present Mini-BEHAVIOR, a novel benchmark for embodied AI that challenges agents to use reasoning and decision-making skills to solve complex activities that resemble everyday human challenges. The Mini-BEHAVIOR environment is a fast, realistic Gridworld environment that offers the benefits of rapid prototyping and ease of use while preserving a symbolic level of physical realism and complexity found in complex embodied AI benchmarks. We introduce key features such as procedural generation, to enable the creation of countless task variations and support open-ended learning. Mini-BEHAVIOR provides implementations of various household tasks from the original BEHAVIOR benchmark, along with starter code for data collection and reinforcement learning agent training. In essence, Mini-BEHAVIOR offers a fast, open-ended benchmark for evaluating decision-making and planning solutions in embodied AI. It serves as a user-friendly entry point for research and facilitates the evaluation and devel
    
[^12]: 贝叶斯设计原则用于频率式序贯学习问题

    Bayesian Design Principles for Frequentist Sequential Learning. (arXiv:2310.00806v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00806](http://arxiv.org/abs/2310.00806)

    该论文提出了一种贝叶斯设计原则用于优化频率式序贯学习问题的通用理论，通过生成“算法信念”并使用贝叶斯后验进行决策，实现了无先验的贝叶斯类型算法在对抗性环境中的泛化和优化应用。

    

    我们提出了一种通用理论，用于优化频率误差求和的序贯学习问题，可以通过统一的贝叶斯原则得到高效的强化学习和赌博机算法。我们提出了一种新颖的优化方法，在每一轮生成“算法信念”，并使用贝叶斯后验进行决策。我们提出的优化目标是创建“算法信念”，我们将其称为“算法信息比”，有效地表征了任何算法的频率误差的内在复杂度度量。据我们所知，这是第一种将贝叶斯式算法无先验地并且在对抗性环境中以通用和最优方式应用的系统方法。此外，这些算法简单且通常容易实现。作为一个重要应用，我们提出了一种新的多臂赌博机算法，在随机、对抗和非...

    We develop a general theory to optimize the frequentist regret for sequential learning problems, where efficient bandit and reinforcement learning algorithms can be derived from unified Bayesian principles. We propose a novel optimization approach to generate "algorithmic beliefs" at each round, and use Bayesian posteriors to make decisions. The optimization objective to create "algorithmic beliefs," which we term "Algorithmic Information Ratio," represents an intrinsic complexity measure that effectively characterizes the frequentist regret of any algorithm. To the best of our knowledge, this is the first systematical approach to make Bayesian-type algorithms prior-free and applicable to adversarial settings, in a generic and optimal manner. Moreover, the algorithms are simple and often efficient to implement. As a major application, we present a novel algorithm for multi-armed bandits that achieves the "best-of-all-worlds" empirical performance in the stochastic, adversarial, and non
    
[^13]: 数据加载通常具有短深度：基于张量网络的金融、图像、流体和蛋白质量子电路

    Data is often loadable in short depth: Quantum circuits from tensor networks for finance, images, fluids, and proteins. (arXiv:2309.13108v1 [quant-ph])

    [http://arxiv.org/abs/2309.13108](http://arxiv.org/abs/2309.13108)

    使用基于张量网络的电路编译方法AMLET，该方法可以解决量子电路加载经典数据的“输入问题”。作者在金融、图像、流体和蛋白质领域进行了广泛的数值实验，并展示了该方法的有效性。

    

    尽管在开发用于研究经典数据集的量子算法方面取得了显著进展，但简单加载经典数据的成本是实现量子优势的障碍。当使用振幅编码时，加载任意经典向量需要与比特数成指数关系的电路深度。在这里，我们通过两个贡献来解决这个“输入问题”。首先，我们引入了一种基于张量网络（TN）理论的电路编译方法。我们的方法——AMLET（自动多层加载器利用TNs）——通过精心构建特定的TN拓扑结构，并可以根据需要调整电路深度。其次，我们对来自金融、图像、流体力学和蛋白质四个不同领域的真实经典数据进行了数值实验。据我们所知，这是迄今为止关于将经典数据加载到量子计算机中的最广泛的数值分析。与这一领域最近的其他工作一致，所需的

    Though there has been substantial progress in developing quantum algorithms to study classical datasets, the cost of simply loading classical data is an obstacle to quantum advantage. When the amplitude encoding is used, loading an arbitrary classical vector requires up to exponential circuit depths with respect to the number of qubits. Here, we address this ``input problem'' with two contributions. First, we introduce a circuit compilation method based on tensor network (TN) theory. Our method -- AMLET (Automatic Multi-layer Loader Exploiting TNs) -- proceeds via careful construction of a specific TN topology and can be tailored to arbitrary circuit depths. Second, we perform numerical experiments on real-world classical data from four distinct areas: finance, images, fluid mechanics, and proteins. To the best of our knowledge, this is the broadest numerical analysis to date of loading classical data into a quantum computer. Consistent with other recent work in this area, the required
    
[^14]: 早期诊断自闭症谱系障碍的机器学习方法

    Early diagnosis of autism spectrum disorder using machine learning approaches. (arXiv:2309.11646v1 [cs.LG])

    [http://arxiv.org/abs/2309.11646](http://arxiv.org/abs/2309.11646)

    本文利用机器学习方法旨在提供自闭症谱系障碍(ASD)的早期诊断，并通过研究不同算法寻找最显著的特征，自动化诊断过程。

    

    自闭症谱系障碍(ASD)是一种神经系统疾病，表现为社交互动困难、语言沟通困难和重复行为。这些困难的严重程度各不相同，被诊断为ASD的人面临着独特的挑战。尽早识别和处理ASD可以促进该疾病的改善。近年来，机器学习驱动的智能诊断作为传统临床方法的补充出现，旨在解决传统方法耗时且昂贵的潜在缺点。本文利用不同的机器学习算法寻找ASD的最显著特征并自动化诊断过程。我们研究了六种分类模型，以找到最适合识别ASD的模型，同时还研究了五种流行的聚类方法，以对这些ASD数据集进行有意义的洞察。

    Autistic Spectrum Disorder (ASD) is a neurological disease characterized by difficulties with social interaction, communication, and repetitive activities. The severity of these difficulties varies, and those with this diagnosis face unique challenges. While its primary origin lies in genetics, identifying and addressing it early can contribute to the enhancement of the condition. In recent years, machine learning-driven intelligent diagnosis has emerged as a supplement to conventional clinical approaches, aiming to address the potential drawbacks of time-consuming and costly traditional methods. In this work, we utilize different machine learning algorithms to find the most significant traits responsible for ASD and to automate the diagnostic process. We study six classification models to see which model works best to identify ASD and also study five popular clustering methods to get a meaningful insight of these ASD datasets. To find the best classifier for these binary datasets, we 
    
[^15]: PromptTTS++：使用自然语言描述控制提示式文本转语音中的说话者身份

    PromptTTS++: Controlling Speaker Identity in Prompt-Based Text-to-Speech Using Natural Language Descriptions. (arXiv:2309.08140v1 [eess.AS])

    [http://arxiv.org/abs/2309.08140](http://arxiv.org/abs/2309.08140)

    PromptTTS++是一种基于提示的文本转语音系统，可以使用自然语言描述控制说话者身份。与现有研究不同，该方法利用说话者提示来学习自然语言描述与声学特征的映射。

    

    我们提出了PromptTTS++，一种基于提示的文本转语音（TTS）合成系统，它允许使用自然语言描述来控制说话者身份。为了在基于提示的TTS框架中控制说话者身份，我们引入了说话者提示的概念，该提示描述了语音特征（如中性、年轻、老年和沉闷），旨在与说话风格大致独立。由于目前没有包含说话者提示的大规模数据集，我们首先使用LibriTTS-R语料库构建了一个基于手动注释的说话者提示数据集。然后，我们采用基于扩散的声学模型与混合密度网络来建模训练数据中的多样化说话者因素。与之前仅依赖样式提示的研究不同，样式提示仅描述了说话者个性化的有限方面，如音调、说话速度和能量，我们的方法利用额外的说话者提示来有效地学习从自然语言描述到声学特征的映射。

    We propose PromptTTS++, a prompt-based text-to-speech (TTS) synthesis system that allows control over speaker identity using natural language descriptions. To control speaker identity within the prompt-based TTS framework, we introduce the concept of speaker prompt, which describes voice characteristics (e.g., gender-neutral, young, old, and muffled) designed to be approximately independent of speaking style. Since there is no large-scale dataset containing speaker prompts, we first construct a dataset based on the LibriTTS-R corpus with manually annotated speaker prompts. We then employ a diffusion-based acoustic model with mixture density networks to model diverse speaker factors in the training data. Unlike previous studies that rely on style prompts describing only a limited aspect of speaker individuality, such as pitch, speaking speed, and energy, our method utilizes an additional speaker prompt to effectively learn the mapping from natural language descriptions to the acoustic f
    
[^16]: 针对组合贝叶斯优化的随机后处理方法

    Random postprocessing for combinatorial Bayesian optimization. (arXiv:2309.02842v1 [cs.LG])

    [http://arxiv.org/abs/2309.02842](http://arxiv.org/abs/2309.02842)

    针对组合贝叶斯优化，我们研究了一种随机后处理方法，严格禁止数据集中的重复样本，结果表明此方法显著减少了顺序步骤数，特别是在最大后验估计的情况下，为解决高维问题中贝叶斯优化的收敛速度慢提供了一种简单但通用的策略。

    

    基于模型的顺序方法用于离散的“黑盒”优化问题，包括贝叶斯优化技术，通常会对给定的目标函数访问多次相同的点，导致需要很多步骤才能找到全局最优解。在这里，我们对贝叶斯优化中的一种后处理方法进行了数值研究，该方法严格禁止数据集中的重复样本。我们发现后处理方法显著减少了找到全局最优解所需的顺序步骤数，特别是当采样函数是最大后验估计时。我们的结果为解决高维问题中贝叶斯优化的收敛速度慢提供了一种简单但通用的策略。

    Model-based sequential approaches to discrete "black-box" optimization, including Bayesian optimization techniques, often access the same points multiple times for a given objective function in interest, resulting in many steps to find the global optimum. Here, we numerically study the effect of a postprocessing method on Bayesian optimization that strictly prohibits duplicated samples in the dataset. We find the postprocessing method significantly reduces the number of sequential steps to find the global optimum, especially when the acquisition function is of maximum a posterior estimation. Our results provide a simple but general strategy to solve the slow convergence of Bayesian optimization for high-dimensional problems.
    
[^17]: 神经网络的自适应切向特征视角

    An Adaptive Tangent Feature Perspective of Neural Networks. (arXiv:2308.15478v1 [cs.LG])

    [http://arxiv.org/abs/2308.15478](http://arxiv.org/abs/2308.15478)

    本研究提出了一个切向特征视角的框架，通过线性变换和结构正则化优化神经网络的特征学习，从而更好地理解神经网络的特征学习过程。在实验证明了该方法在回归问题中的有效性，并提供了对核对齐现象的细致分析。

    

    为了更好地理解神经网络中的特征学习，我们提出了一个框架来理解在切向特征空间中的线性模型，其中特征在训练过程中可以进行转换。我们考虑特征的线性变换，从而通过双线性插值约束在参数和变换上进行联合优化。我们证明了这个优化问题与具有结构化正则化的等价线性约束优化问题具有近似低秩解的关系。通过将其应用于神经网络结构，我们对特征以及核函数的变化获得了更深入的理解，为当目标函数在切向特征上表征不好时的核对齐现象提供了额外的细微差别。除了在简单回归问题上验证我们的理论观察结果外，我们还通过实验证明了切向特征分类的自适应特征实现方法。

    In order to better understand feature learning in neural networks, we propose a framework for understanding linear models in tangent feature space where the features are allowed to be transformed during training. We consider linear transformations of features, resulting in a joint optimization over parameters and transformations with a bilinear interpolation constraint. We show that this optimization problem has an equivalent linearly constrained optimization with structured regularization that encourages approximately low rank solutions. Specializing to neural network structure, we gain insights into how the features and thus the kernel function change, providing additional nuance to the phenomenon of kernel alignment when the target function is poorly represented using tangent features. In addition to verifying our theoretical observations in real neural networks on a simple regression problem, we empirically show that an adaptive feature implementation of tangent feature classificat
    
[^18]: 区域解耦扩散模型用于高保真度PPG到ECG的转换

    Region-Disentangled Diffusion Model for High-Fidelity PPG-to-ECG Translation. (arXiv:2308.13568v1 [eess.SP])

    [http://arxiv.org/abs/2308.13568](http://arxiv.org/abs/2308.13568)

    这项工作引入了区域解耦扩散模型 (RDDM)，用于将PPG转换为ECG信号。RDDM通过选择性地向感兴趣区域添加噪声来捕捉ECG信号的复杂时间动态。

    

    心血管疾病的高发率需要便捷且经济有效的连续心脏监测工具。尽管心电图（ECG）是黄金标准，但连续监测仍然是一个挑战，因此人们开始探索光电容容积脉搏图（PPG），这是一种更基本的可用于消费者可穿戴设备的替代方案。这种想法最近引起了将PPG转化为ECG信号的兴趣。在这项工作中，我们引入了区域解耦扩散模型（RDDM），这是一种设计用于捕捉ECG复杂时间动态的新颖扩散模型。传统的扩散模型如去噪扩散概率模型（DDPM）在捕捉这种细微差别时面临困难，因为整个信号上的噪声添加过程是不加选择的。我们提出的RDDM通过将噪声有选择地添加到感兴趣区域（ROI）（如ECG信号中的QRS复合体）的新颖正向过程和反向过程来克服此类限制。

    The high prevalence of cardiovascular diseases (CVDs) calls for accessible and cost-effective continuous cardiac monitoring tools. Despite Electrocardiography (ECG) being the gold standard, continuous monitoring remains a challenge, leading to the exploration of Photoplethysmography (PPG), a promising but more basic alternative available in consumer wearables. This notion has recently spurred interest in translating PPG to ECG signals. In this work, we introduce Region-Disentangled Diffusion Model (RDDM), a novel diffusion model designed to capture the complex temporal dynamics of ECG. Traditional Diffusion models like Denoising Diffusion Probabilistic Models (DDPM) face challenges in capturing such nuances due to the indiscriminate noise addition process across the entire signal. Our proposed RDDM overcomes such limitations by incorporating a novel forward process that selectively adds noise to specific regions of interest (ROI) such as QRS complex in ECG signals, and a reverse proces
    
[^19]: 使用多条件扩散模型进行音频生成

    Audio Generation with Multiple Conditional Diffusion Model. (arXiv:2308.11940v1 [cs.SD])

    [http://arxiv.org/abs/2308.11940](http://arxiv.org/abs/2308.11940)

    本论文提出了一种使用多条件扩散模型进行音频生成的方法。通过引入内容和风格等额外条件，增强了现有模型的可控性。这种方法可以精确控制生成音频的时间顺序、音高和能量。由于缺乏合适的数据集和评估指标，作者整合了现有数据集并进行了实验验证。

    

    基于文本的音频生成模型有其局限性，因为它们无法包含音频中的所有信息，仅依靠文本会导致受控性受限。为了解决这个问题，我们提出了一种新颖的模型，通过引入额外的条件（包括内容（时间戳）和风格（音高曲线和能量曲线））作为文本的补充，增强了现有预训练文本到音频模型的可控性。这种方法实现了对生成音频的时间顺序、音高和能量的精细控制。为了保持生成的多样性，我们使用一个可训练的控制条件编码器，该编码器由一个大型语言模型增强，并使用一个可训练的融合网络来编码和融合额外的条件，同时保持预训练文本到音频模型的权重不变。由于缺乏合适的数据集和评估指标，我们将现有数据集整合为一个新的数据集，包括音频和相应的条件，并使用一个可训练的控制条件编码器，该编码器由一个大型语言模型增强，并使用一个可训练的融合网络来编码和融合额外的条件，同时保持预训练文本到音频模型的权重不变。

    Text-based audio generation models have limitations as they cannot encompass all the information in audio, leading to restricted controllability when relying solely on text. To address this issue, we propose a novel model that enhances the controllability of existing pre-trained text-to-audio models by incorporating additional conditions including content (timestamp) and style (pitch contour and energy contour) as supplements to the text. This approach achieves fine-grained control over the temporal order, pitch, and energy of generated audio. To preserve the diversity of generation, we employ a trainable control condition encoder that is enhanced by a large language model and a trainable Fusion-Net to encode and fuse the additional conditions while keeping the weights of the pre-trained text-to-audio model frozen. Due to the lack of suitable datasets and evaluation metrics, we consolidate existing datasets into a new dataset comprising the audio and corresponding conditions and use a 
    
[^20]: 参数规划的选择性推断中的有界P值

    Bounded P-values in Parametric Programming-based Selective Inference. (arXiv:2307.11351v1 [stat.ML])

    [http://arxiv.org/abs/2307.11351](http://arxiv.org/abs/2307.11351)

    本研究提出了一种降低参数规划选择性推断计算成本的方法，通过计算p值的上界和下界来保证所需精度。

    

    选择性推断（SI）作为一种适用于数据驱动的假设检验的有前景的框架，一直受到研究关注。SI的基本思想是在一个假设被选中的事件的条件下进行推断。为了进行SI，必须以可追踪的形式对这个事件进行描述。当选择事件难以描述时，可以引入额外的条件以使其可处理。这些额外的条件往往会导致功效的损失，这一问题被称为过度条件化。基于参数规划的SI（PP-based SI）被提出作为解决过度条件化问题的一种方法。PP-based SI的主要问题是由于需要完全地探索数据空间而导致计算成本高。本研究引入了一种降低计算成本的过程，同时保证所需精度，通过提出计算p值的上界和下界的方法。我们还提出了三种类型的搜索策略。

    Selective inference (SI) has been actively studied as a promising framework for statistical hypothesis testing for data-driven hypotheses. The basic idea of SI is to make inferences conditional on an event that a hypothesis is selected. In order to perform SI, this event must be characterized in a traceable form. When selection event is too difficult to characterize, additional conditions are introduced for tractability. This additional conditions often causes the loss of power, and this issue is referred to as over-conditioning. Parametric programming-based SI (PP-based SI) has been proposed as one way to address the over-conditioning issue. The main problem of PP-based SI is its high computational cost due to the need to exhaustively explore the data space. In this study, we introduce a procedure to reduce the computational cost while guaranteeing the desired precision, by proposing a method to compute the upper and lower bounds of p-values. We also proposed three types of search str
    
[^21]: PreDiff: 使用隐式扩散模型进行降水近期预测

    PreDiff: Precipitation Nowcasting with Latent Diffusion Models. (arXiv:2307.10422v1 [cs.LG])

    [http://arxiv.org/abs/2307.10422](http://arxiv.org/abs/2307.10422)

    本论文提出了PreDiff方法，使用条件隐式扩散模型进行降水近期预测。同时，引入显式知识控制机制以满足特定领域的物理约束。

    

    传统上，地球系统预测主要依赖于复杂的物理模型，这些模型计算量大且需要领域专业知识。在过去的十年中，时空地球观测数据的空前增加使得使用深度学习技术的数据驱动预测模型成为可能。这些模型在不同的地球系统预测任务中显示出有希望的效果，但是它们要么难以处理不确定性，要么忽视特定领域的先验知识，导致预测结果模糊或产生物理上不合理的预测。为了解决这些限制，我们提出了一种概率时空预测的两阶段流程：1）我们开发了一种名为PreDiff的条件隐式扩散模型，能够进行概率预测；2）我们融入了一种显式知识控制机制，以使预测符合特定领域的物理约束。这是通过在每个去噪步骤中估计与所施加约束的偏差来实现的。

    Earth system forecasting has traditionally relied on complex physical models that are computationally expensive and require significant domain expertise. In the past decade, the unprecedented increase in spatiotemporal Earth observation data has enabled data-driven forecasting models using deep learning techniques. These models have shown promise for diverse Earth system forecasting tasks but either struggle with handling uncertainty or neglect domain-specific prior knowledge, resulting in averaging possible futures to blurred forecasts or generating physically implausible predictions. To address these limitations, we propose a two-stage pipeline for probabilistic spatiotemporal forecasting: 1) We develop PreDiff, a conditional latent diffusion model capable of probabilistic forecasts. 2) We incorporate an explicit knowledge control mechanism to align forecasts with domain-specific physical constraints. This is achieved by estimating the deviation from imposed constraints at each denoi
    
[^22]: 基于持久同调的自适应拓扑特征：点云的滤波学习

    Adaptive Topological Feature via Persistent Homology: Filtration Learning for Point Clouds. (arXiv:2307.09259v1 [cs.LG])

    [http://arxiv.org/abs/2307.09259](http://arxiv.org/abs/2307.09259)

    本文提出了一种通过使用神经网络自适应学习滤波器的方法，用于增强点云机器学习方法的准确性。同时，还提出了一种具有等变性的神经网络架构，以使得持久同调具有等变性。

    

    点云的机器学习在形状识别和材料科学等领域得到了广泛关注。为了提高这些机器学习方法的准确性，通常会引入全局拓扑特征，这些特征通常通过持久同调提取。在对点云进行持久同调计算时，我们需要选择一个点云的滤波器，即一个逐渐增加的空间序列。由于机器学习方法与持久同调的结合受到滤波器选择的影响很大，因此我们需要根据数据和任务进行调整。本文提出了一种使用神经网络自适应学习滤波器的框架。为了使得得到的持久同调具有等变性，在此基础上提出了一种具有等变性的神经网络架构。此外，我们还在理论上展示了有限维近似的结果。

    Machine learning for point clouds has been attracting much attention, with many applications in various fields, such as shape recognition and material science. To enhance the accuracy of such machine learning methods, it is known to be effective to incorporate global topological features, which are typically extracted by persistent homology. In the calculation of persistent homology for a point cloud, we need to choose a filtration for the point clouds, an increasing sequence of spaces. Because the performance of machine learning methods combined with persistent homology is highly affected by the choice of a filtration, we need to tune it depending on data and tasks. In this paper, we propose a framework that learns a filtration adaptively with the use of neural networks. In order to make the resulting persistent homology isometry-invariant, we develop a neural network architecture with such invariance. Additionally, we theoretically show a finite-dimensional approximation result that 
    
[^23]: 深度学习动态图：模型与基准

    Deep learning for dynamic graphs: models and benchmarks. (arXiv:2307.06104v1 [cs.LG])

    [http://arxiv.org/abs/2307.06104](http://arxiv.org/abs/2307.06104)

    本文对深度学习动态图领域进行了调查，总结了学习时间和空间信息的最新优势，并对最流行的方法进行了公平的性能比较，为评估新架构和方法建立了一个可靠的基准模型。

    

    近年来，在深度图网络（DGNs）的研究中取得了巨大进展，推动了图上学习的领域成熟发展。尽管这个研究领域正在快速增长，但仍然存在一些尚未解决的重要挑战。特别地，急需使DGNs适用于实时系统中随时间推移不断演化的预测任务。为促进动态图领域的研究，首先，我们调查了学习时间和空间信息的最新优势，并提供了动态图表示学习领域的当前最新概览。其次，我们对最流行的方法进行了公平的性能比较，通过严格的模型选择和评估，为评估新架构和方法建立了一个可靠的基准模型。

    Recent progress in research on Deep Graph Networks (DGNs) has led to a maturation of the domain of learning on graphs. Despite the growth of this research field, there are still important challenges that are yet unsolved. Specifically, there is an urge of making DGNs suitable for predictive tasks on realworld systems of interconnected entities, which evolve over time. With the aim of fostering research in the domain of dynamic graphs, at first, we survey recent advantages in learning both temporal and spatial information, providing a comprehensive overview of the current state-of-the-art in the domain of representation learning for dynamic graphs. Secondly, we conduct a fair performance comparison among the most popular proposed approaches, leveraging rigorous model selection and assessment for all the methods, thus establishing a sound baseline for evaluating new architectures and approaches
    
[^24]: 看看、记住和推理：基于机理的视觉推理

    Look, Remember and Reason: Visual Reasoning with Grounded Rationales. (arXiv:2306.17778v1 [cs.CV])

    [http://arxiv.org/abs/2306.17778](http://arxiv.org/abs/2306.17778)

    在这项研究中，我们借鉴了人类视觉问题解决的方法，通过三个步骤（看、记住、推理）逐步提取视觉信息来解决复杂的视觉推理问题，从而使大型语言模型能够有效解决这些问题。

    

    近期，大型语言模型在各种推理任务上展示了与人类水平的表现。然而，这些模型在进行复杂的视觉推理方面的能力尚未得到详细研究。在许多视觉推理任务中，一个关键挑战是需要将视觉信息紧密融合到推理过程中。我们提出通过借鉴人类视觉问题解决的方法来解决这个挑战，这个方法依赖于多种低级视觉能力。它通常可以被看作是“看，记住，推理”的三个步骤过程：通过逐步进行低级视觉过程提取视觉信息，直到得出最终答案。我们遵循相同的范例，通过最小的架构更改，使现有的大型语言模型能够解决视觉推理问题。为此，我们引入了基于视觉输入的原理，允许我们集成低级视觉能力，如对象识别。

    Large language models have recently shown human level performance on a variety of reasoning tasks. However, the ability of these models to perform complex visual reasoning has not been studied in detail yet. A key challenge in many visual reasoning tasks is that the visual information needs to be tightly integrated in the reasoning process. We propose to address this challenge by drawing inspiration from human visual problem solving which depends on a variety of low-level visual capabilities. It can often be cast as the three step-process of ``Look, Remember, Reason'': visual information is incrementally extracted using low-level visual routines in a step-by-step fashion until a final answer is reached. We follow the same paradigm to enable existing large language models, with minimal changes to the architecture, to solve visual reasoning problems. To this end, we introduce rationales over the visual input that allow us to integrate low-level visual capabilities, such as object recogni
    
[^25]: 使用基于域分解的预条件策略增强物理信息神经网络的训练

    Enhancing training of physics-informed neural networks using domain-decomposition based preconditioning strategies. (arXiv:2306.17648v1 [math.NA])

    [http://arxiv.org/abs/2306.17648](http://arxiv.org/abs/2306.17648)

    这项研究提出了一种基于域分解的预条件策略，用于增强物理信息神经网络的训练。该方法通过非线性预条件器改进了L-BFGS优化器的收敛性，并提供了更准确的偏微分方程解。加性预条件器还具有并行性，为模型并行提供了新的方法。

    

    我们提出了一种增强物理信息神经网络（PINNs）训练的方法。为此，我们引入了非线性加性和乘性预条件策略，用于广泛使用的L-BFGS优化器。非线性预条件器是通过利用Schwarz域分解框架构建的，其中网络的参数以逐层方式进行分解。通过一系列数值实验，我们证明了加性和乘性预条件器都能显著改善标准L-BFGS优化器的收敛性，同时提供了更准确的偏微分方程解。此外，加性预条件器本质上是并行的，因此为一种新的模型并行方法提供了可能。

    We propose to enhance the training of physics-informed neural networks (PINNs). To this aim, we introduce nonlinear additive and multiplicative preconditioning strategies for the widely used L-BFGS optimizer. The nonlinear preconditioners are constructed by utilizing the Schwarz domain-decomposition framework, where the parameters of the network are decomposed in a layer-wise manner. Through a series of numerical experiments, we demonstrate that both, additive and multiplicative preconditioners significantly improve the convergence of the standard L-BFGS optimizer, while providing more accurate solutions of the underlying partial differential equations. Moreover, the additive preconditioner is inherently parallel, thus giving rise to a novel approach to model parallelism.
    
[^26]: 安全的基于模型的多智能体均场强化学习

    Safe Model-Based Multi-Agent Mean-Field Reinforcement Learning. (arXiv:2306.17052v1 [cs.LG])

    [http://arxiv.org/abs/2306.17052](http://arxiv.org/abs/2306.17052)

    本文提出了Safe-$\text{M}^3$-UCRL算法，通过使用模型中的认知不确定性和对数障碍方法，实现了在未知转移动态情况下达到安全策略的优化，成功解决了大规模多智能体协调问题。

    

    许多应用，比如共享交通，需要协调大量的智能体。均场强化学习通过优化代表性智能体的策略来应对由此带来的可扩展性挑战。在本文中，我们解决了一个重要的泛化问题，即智能体分布存在全局约束的情况（例如需要满足容量约束或最小覆盖要求）。我们提出了Safe-$\text{M}^3$-UCRL，这是第一个能够在未知转移动态的情况下实现安全策略的基于模型的算法。作为一个关键因素，它在保证悲观约束满足的同时，利用转移模型中的认知不确定性来使用对数障碍方法确保高概率。我们在许多共享交通运营商面临的车辆重定位问题上展示了Safe-$\text{M}^3$-UCRL，并通过基于深圳出租车轨迹数据的仿真评估其性能。我们的算法能够有效满足关键需求。

    Many applications, e.g., in shared mobility, require coordinating a large number of agents. Mean-field reinforcement learning addresses the resulting scalability challenge by optimizing the policy of a representative agent. In this paper, we address an important generalization where there exist global constraints on the distribution of agents (e.g., requiring capacity constraints or minimum coverage requirements to be met). We propose Safe-$\text{M}^3$-UCRL, the first model-based algorithm that attains safe policies even in the case of unknown transition dynamics. As a key ingredient, it uses epistemic uncertainty in the transition model within a log-barrier approach to ensure pessimistic constraints satisfaction with high probability. We showcase Safe-$\text{M}^3$-UCRL on the vehicle repositioning problem faced by many shared mobility operators and evaluate its performance through simulations built on Shenzhen taxi trajectory data. Our algorithm effectively meets the demand in critica
    
[^27]: 利用本地性和鲁棒性实现大规模高斯过程回归

    Leveraging Locality and Robustness to Achieve Massively Scalable Gaussian Process Regression. (arXiv:2306.14731v1 [stat.ML])

    [http://arxiv.org/abs/2306.14731](http://arxiv.org/abs/2306.14731)

    该研究提出了一种新的思路，通过探索 GPnn 的鲁棒性和极限行为实现大规模高斯过程回归，即使在出现重大小错误的情况下只需要花费少量的工作进行参数估计即可实现高 MSE 准确性。同时，该研究成功解决了加性噪声方差带来的不确定度校准和 NLL 准确性问题。

    

    高斯过程回归所提供的精确预测和原则性不确定性测量会产生 O(n^3) 的成本，这对于现代大规模应用来说是难以承受的。因此，出现了大量关于计算效率的研究。我们通过探索 GP 最近邻预测(GPnn) 的鲁棒性和极限行为引入了一种新的视角。我们通过理论和模拟证明，随着数据量 n 的增加，估计参数和 GP 模型假设的准确性对 GPnn 预测准确性的影响逐渐减小。因此，为了实现高 MSE 准确性，即使在出现重大错误的情况下, 只需要花费少量的工作进行参数估计即可。相比之下，随着 n 趋近于无穷大，我们发现不确定度校准和 NLL 仍对一个参数敏感，即加性噪声方差；但我们证明可以纠正这种不准确性，并实现良好的不确定度校准和 NLL。

    The accurate predictions and principled uncertainty measures provided by GP regression incur O(n^3) cost which is prohibitive for modern-day large-scale applications. This has motivated extensive work on computationally efficient approximations. We introduce a new perspective by exploring robustness properties and limiting behaviour of GP nearest-neighbour (GPnn) prediction. We demonstrate through theory and simulation that as the data-size n increases, accuracy of estimated parameters and GP model assumptions become increasingly irrelevant to GPnn predictive accuracy. Consequently, it is sufficient to spend small amounts of work on parameter estimation in order to achieve high MSE accuracy, even in the presence of gross misspecification. In contrast, as n tends to infinity, uncertainty calibration and NLL are shown to remain sensitive to just one parameter, the additive noise-variance; but we show that this source of inaccuracy can be corrected for, thereby achieving both well-calibra
    
[^28]: 用于改进视听融合导航的主动稀疏对话

    Active Sparse Conversations for Improved Audio-Visual Embodied Navigation. (arXiv:2306.04047v1 [cs.CV])

    [http://arxiv.org/abs/2306.04047](http://arxiv.org/abs/2306.04047)

    本文提出了CAVEN - 一种具有对话功能的音频视觉导航代理，能够向人类/神谕提出导航问题并处理神谕回答以协助自主导航。该系统基于多模态分层强化学习方法，并使用三个低级策略进行引导。

    

    为了高效地导航到一个听觉目标，一个具有固定自主权的实体必须不仅要有能力有效地使用视听线索, 而且还要有能力在不牺牲自主性的情况下主动寻求人类/神谕的帮助，例如，当不确定导航到哪里寻找嘈杂或间歇性听觉目标时。因此，我们提出了CAVEN-一种具有对话功能的音频视觉导航代理，能够向人类/神谕提出导航问题并处理神谕的自由形式自然语言回答。在CAVEN的核心是一个多模态分层强化学习(RL)设置，它配备了一个高级策略，该策略经过训练，可以在每一步从三个低级策略中选择一个，即：(i)使用视听线索进行导航，或(ii)向神谕提出问题并接收短或详细的回答，或(iii)提问普遍问题(当不确定该问什么时)并获得指导

    Efficient navigation towards an audio-goal necessitates an embodied agent to not only possess the ability to use audio-visual cues effectively, but also be equipped to actively (but occasionally) seek human/oracle assistance without sacrificing autonomy, e.g., when it is uncertain of where to navigate towards locating a noisy or sporadic audio goal. To this end, we present CAVEN -- a conversational audio-visual embodied navigation agent that is capable of posing navigation questions to a human/oracle and processing the oracle responses; both in free-form natural language. At the core of CAVEN is a multimodal hierarchical reinforcement learning (RL) setup that is equipped with a high-level policy that is trained to choose from one of three low-level policies (at every step), namely: (i) to navigate using audio-visual cues, or (ii) to frame a question to the oracle and receive a short or detailed response, or (iii) ask generic questions (when unsure of what to ask) and receive instructio
    
[^29]: 俄乌战争：预测和解释Twitter的封禁

    Russo-Ukrainian War: Prediction and explanation of Twitter suspension. (arXiv:2306.03502v1 [cs.SI])

    [http://arxiv.org/abs/2306.03502](http://arxiv.org/abs/2306.03502)

    本研究分析了Twitter封禁机制，揭示了存在的政策违规、宣传、垃圾邮件等问题，并发现拥有更多粉丝的账户更可能被封禁。这些发现可以让Twitter和其他社交网络改进其内容过滤机制。

    

    2022年2月24日，俄罗斯入侵乌克兰，开始了现在已知的俄乌战争，并在社交媒体上引发了在线话语。Twitter作为最受欢迎的社交网络之一，以其开放和民主的特点，在其庞大的用户群中实现了透明的讨论。不幸的是，这往往会导致Twitter的政策违规、宣传、滥用行为、侵犯公民权利，因此导致用户账户被封禁和删除。本研究着重探讨了Twitter的封禁机制，并分析了可能导致账户被封禁的共享内容和用户账户的特征。为此，我们利用Twitter API获得了包含107.7M条推文的数据集，来自980万用户。我们提取了被封禁账户的共享内容类别，并通过提取文本嵌入和余弦相似性聚类来解释其特征。我们的研究结果揭示了一些滥用Twitter政策标准的骗子活动、垃圾邮件和宣传活动。此外，我们发现相对于粉丝数较少的账户，拥有更多粉丝的账户更有可能被封禁。这些发现可以为Twitter和其他社交网络改进其内容过滤机制，最小化有害内容的传播提供有用的参考。

    On 24 February 2022, Russia invaded Ukraine, starting what is now known as the Russo-Ukrainian War, initiating an online discourse on social media. Twitter as one of the most popular SNs, with an open and democratic character, enables a transparent discussion among its large user base. Unfortunately, this often leads to Twitter's policy violations, propaganda, abusive actions, civil integrity violation, and consequently to user accounts' suspension and deletion. This study focuses on the Twitter suspension mechanism and the analysis of shared content and features of the user accounts that may lead to this. Toward this goal, we have obtained a dataset containing 107.7M tweets, originating from 9.8 million users, using Twitter API. We extract the categories of shared content of the suspended accounts and explain their characteristics, through the extraction of text embeddings in junction with cosine similarity clustering. Our results reveal scam campaigns taking advantage of trending top
    
[^30]: SimFBO：简单、灵活且通信高效的联邦双层学习

    SimFBO: Towards Simple, Flexible and Communication-efficient Federated Bilevel Learning. (arXiv:2305.19442v1 [cs.LG])

    [http://arxiv.org/abs/2305.19442](http://arxiv.org/abs/2305.19442)

    SimFBO和其ShroFBO变体提出了一个简单、灵活且通信高效的FBO框架，可以应用于元学习和超参数优化任务。

    

    近来，由于元学习、微调、超参数调整等领域中嵌套优化结构的出现，联邦双层优化（FBO）在机器学习和边缘计算中显示了巨大的潜力。然而，现有的FBO算法往往涉及复杂的计算，并需要每次迭代多个子循环，每个子循环包含多个通信轮。在本文中，我们提出了一个名为SimFBO的简单灵活的FBO框架，它易于实现，不需要子循环，并包括一种广义的服务器端聚合和更新以提高通信效率。我们进一步提出了系统级异构鲁棒FBO（ShroFBO）作为SimFBO的变体，其对本地计算的异构有更强的鲁棒性。我们证明了在部分客户端参与和无替换的客户端采样下，SimFBO和ShroFBO可以实现线性收敛加速，同时改进了样本和通信复杂度。实验证明了它们在图像分类数据集的元学习和真实世界数据集上的超参数优化任务中的有效性。

    Federated bilevel optimization (FBO) has shown great potential recently in machine learning and edge computing due to the emerging nested optimization structure in meta-learning, fine-tuning, hyperparameter tuning, etc. However, existing FBO algorithms often involve complicated computations and require multiple sub-loops per iteration, each of which contains a number of communication rounds. In this paper, we propose a simple and flexible FBO framework named SimFBO, which is easy to implement without sub-loops, and includes a generalized server-side aggregation and update for improving communication efficiency. We further propose System-level heterogeneity robust FBO (ShroFBO) as a variant of SimFBO with stronger resilience to heterogeneous local computation. We show that SimFBO and ShroFBO provably achieve a linear convergence speedup with partial client participation and client sampling without replacement, as well as improved sample and communication complexities. Experiments demons
    
[^31]: Barron型空间的嵌入不等式

    Embedding Inequalities for Barron-type Spaces. (arXiv:2305.19082v1 [stat.ML])

    [http://arxiv.org/abs/2305.19082](http://arxiv.org/abs/2305.19082)

    本文测量了Barron空间和谱Barron空间之间的关系，并提供了嵌入不等式。

    

    深度学习理论中的一个基本问题是理解高维条件下两层神经网络的逼近和泛化性质。为了解决这个问题，研究人员引入了Barron空间$\mathcal{B}_s(\Omega)$和谱Barron空间$\mathcal{F}_s(\Omega)$，其中指数$s$表征了这些空间中函数的平滑性，$\Omega\subset\mathbb{R}^d$表示输入域。然而，两种类型的Barron空间之间的关系仍不清楚。本文通过以下不等式建立了这些空间之间的连续嵌入：对于任意$\delta\in(0,1),s\in\mathbb{N}^{+}$和$f:\Omega \mapsto \mathbb{R}$，都有\[ \delta\gamma^{\delta-s}_{\Omega}\|f\|_{\mathcal{F}_{s-\delta}(\Omega)}\lesssim_s \|f\|_{\mathcal{B}_s(\Omega)}\lesssim_s \|f\|_{\mathcal{F}_{s+1}(\Omega)}, \]其中$\gamma_{\Omega}=\sup_{\|v\|_2=1,x\in\Omega}|v^Tx|$，$\lesssim_s$表示仅与平滑参数$s$有关的常数。

    One of the fundamental problems in deep learning theory is understanding the approximation and generalization properties of two-layer neural networks in high dimensions. In order to tackle this issue, researchers have introduced the Barron space $\mathcal{B}_s(\Omega)$ and the spectral Barron space $\mathcal{F}_s(\Omega)$, where the index $s$ characterizes the smoothness of functions within these spaces and $\Omega\subset\mathbb{R}^d$ represents the input domain. However, it is still not clear what is the relationship between the two types of Barron spaces. In this paper, we establish continuous embeddings between these spaces as implied by the following inequality: for any $\delta\in (0,1), s\in \mathbb{N}^{+}$ and $f: \Omega \mapsto\mathbb{R}$, it holds that \[ \delta\gamma^{\delta-s}_{\Omega}\|f\|_{\mathcal{F}_{s-\delta}(\Omega)}\lesssim_s \|f\|_{\mathcal{B}_s(\Omega)}\lesssim_s \|f\|_{\mathcal{F}_{s+1}(\Omega)}, \] where $\gamma_{\Omega}=\sup_{\|v\|_2=1,x\in\Omega}|v^Tx|$ and notab
    
[^32]: 理解ReLU网络的多阶段优化动态和丰富的非线性行为

    Understanding Multi-phase Optimization Dynamics and Rich Nonlinear Behaviors of ReLU Networks. (arXiv:2305.12467v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.12467](http://arxiv.org/abs/2305.12467)

    本文对于ReLU神经网络通过Gradient Flow训练的两层模型在线性可分数据上进行了完整的理论分析，揭示了整个训练过程中的四个不同阶段，显示出一个从简化到复杂的学习趋势。

    

    ReLU神经网络的训练过程经常表现出复杂的非线性现象。模型的非线性和损失的非凸性为理论分析带来了重大挑战。本文对通过Gradient Flow训练的二层ReLU网络在线性可分数据上进行了完整的理论描述。在这种特定的设置下，我们的分析捕获了从随机初始化到最终收敛的整个优化过程。尽管我们研究的模型和数据相对简单，但我们揭示了整个训练过程中的四个不同阶段，显示出一个从简化到复杂的学习趋势。特定的非线性行为也可以被精确地识别和理论上捕获，例如...

    The training process of ReLU neural networks often exhibits complicated nonlinear phenomena. The nonlinearity of models and non-convexity of loss pose significant challenges for theoretical analysis. Therefore, most previous theoretical works on the optimization dynamics of neural networks focus either on local analysis (like the end of training) or approximate linear models (like Neural Tangent Kernel). In this work, we conduct a complete theoretical characterization of the training process of a two-layer ReLU network trained by Gradient Flow on a linearly separable data. In this specific setting, our analysis captures the whole optimization process starting from random initialization to final convergence. Despite the relatively simple model and data that we studied, we reveal four different phases from the whole training process showing a general simplifying-to-complicating learning trend. Specific nonlinear behaviors can also be precisely identified and captured theoretically, such 
    
[^33]: 多任务模型的动态梯度平衡增强对抗攻击

    Dynamic Gradient Balancing for Enhanced Adversarial Attacks on Multi-Task Models. (arXiv:2305.12066v1 [cs.LG])

    [http://arxiv.org/abs/2305.12066](http://arxiv.org/abs/2305.12066)

    本文提出了动态梯度平衡攻击（DGBA）框架来攻击多任务模型，并通过实验回答了多任务模型的对抗攻击的安全性、多任务攻击和对抗训练是否增强多任务模型的鲁棒性等安全研究问题。

    

    多任务学习 (MTL) 创建了一个名为多任务模型的单个机器学习模型，可以同时执行多个任务。虽然单任务分类器的安全性得到了广泛的研究，但对于多任务模型，存在着几个关键的安全性研究问题，包括: 1）多任务模型对单任务对抗机器学习攻击的安全性如何？2）能否设计对抗性攻击来同时攻击多个任务？ 3）任务共享和对抗训练是否增加了多任务模型对对抗攻击的鲁棒性？本文通过仔细分析和严格的实验回答了这些问题。首先，我们开发了单任务白盒攻击的初级转化并分析了其固有缺陷。然后，我们提出了一种新的攻击框架，动态梯度平衡攻击（DGBA）。我们的框架把攻击多任务模型的问题作为一种基于平均相对损失变化的优化问题。

    Multi-task learning (MTL) creates a single machine learning model called multi-task model to simultaneously perform multiple tasks. Although the security of single task classifiers has been extensively studied, there are several critical security research questions for multi-task models including 1) How secure are multi-task models to single task adversarial machine learning attacks, 2) Can adversarial attacks be designed to attack multiple tasks simultaneously, and 3) Does task sharing and adversarial training increase multi-task model robustness to adversarial attacks? In this paper, we answer these questions through careful analysis and rigorous experimentation. First, we develop na\"ive adaptation of single-task white-box attacks and analyze their inherent drawbacks. We then propose a novel attack framework, Dynamic Gradient Balancing Attack (DGBA). Our framework poses the problem of attacking a multi-task model as an optimization problem based on averaged relative loss change, whi
    
[^34]: 交叉门控多层感知机下的蛋白质复合物不变嵌入是一种一次性抗体设计器

    Protein Complex Invariant Embedding with Cross-Gate MLP is A One-Shot Antibody Designer. (arXiv:2305.09480v1 [q-bio.BM])

    [http://arxiv.org/abs/2305.09480](http://arxiv.org/abs/2305.09480)

    本文提出了一种深度生成模型，可以一次性地共同设计抗体CDR的1D序列和3D结构，解决几何建模和低效推断的问题。

    

    抗体是由免疫系统产生的针对外来物质或抗原的重要蛋白质。抗体的特异性由其互补决定区（CDR）决定，CDR位于抗体链的可变区域中，形成与抗原结合的位点。以往的研究利用复杂的技术生成CDR，但它们遭受了几何建模不足的问题。此外，常见的迭代精化策略导致了低效的推断。本文提出了一种深度生成模型，可以一次性地共同设计CDR的1D序列和3D结构。为了实现这一目标，我们将抗体CDR设计分为两个阶段：（i）蛋白质结构的几何建模和（ii）序列结构共学习。我们开发了一种蛋白质复合物不变嵌入，可捕捉蛋白质骨架原子（包括Cα、N、C和O原子）之间的内部和外部组分相互作用，以实现全面的几何建模。

    Antibodies are crucial proteins produced by the immune system in response to foreign substances or antigens. The specificity of an antibody is determined by its complementarity-determining regions (CDRs), which are located in the variable domains of the antibody chains and form the antigen-binding site. Previous studies have utilized complex techniques to generate CDRs, but they suffer from inadequate geometric modeling. Moreover, the common iterative refinement strategies lead to an inefficient inference. In this paper, we propose a deep generative model that can co-design 1D sequences and 3D structures of CDRs in a one-shot manner. To achieve this, we decouple the antibody CDR design into two stages: (i) geometric modeling of protein structures and (ii) sequence-structure co-learning. We develop a protein complex invariant embedding that captures both intra- and inter-component interactions among the backbone atoms including C$\alpha$, N, C, and O atoms to achieve comprehensive geome
    
[^35]: 设计不连续性

    Designing Discontinuities. (arXiv:2305.08559v1 [cs.IT])

    [http://arxiv.org/abs/2305.08559](http://arxiv.org/abs/2305.08559)

    本文通过一种量化理论方法优化不连续变量的设计，以平衡效应大小的增益和损失，并开发了一种计算效率高的强化学习算法。

    

    不连续性可以是相当任意的，但也会在社会系统中产生重大影响。事实上，它们的任意性是为什么它们被用于推断在许多情况下变量之间的因果关系。计量经济学中的回归不连续性假定存在一个不连续的变量，将人口分成不同的部分，以估计给定现象的因果效应。在这里，我们考虑为给定的不连续变量设计分区以优化以前使用回归不连续性研究过的某种效果。为此，我们提出了一种量化理论方法来优化感兴趣的效果，首先学习给定不连续变量的因果效应大小，然后应用动态规划来优化不连续性的量化设计，以平衡增益和损失的效应大小。我们还开发了一种计算效率高的强化学习算法，用于形成动态规划公式。

    Discontinuities can be fairly arbitrary but also cause a significant impact on outcomes in social systems. Indeed, their arbitrariness is why they have been used to infer causal relationships among variables in numerous settings. Regression discontinuity from econometrics assumes the existence of a discontinuous variable that splits the population into distinct partitions to estimate the causal effects of a given phenomenon. Here we consider the design of partitions for a given discontinuous variable to optimize a certain effect previously studied using regression discontinuity. To do so, we propose a quantization-theoretic approach to optimize the effect of interest, first learning the causal effect size of a given discontinuous variable and then applying dynamic programming for optimal quantization design of discontinuities that balance the gain and loss in the effect size. We also develop a computationally-efficient reinforcement learning algorithm for the dynamic programming formul
    
[^36]: 自然语言处理中基于外部分布检测的综述

    A Survey on Out-of-Distribution Detection in NLP. (arXiv:2305.03236v1 [cs.CL])

    [http://arxiv.org/abs/2305.03236](http://arxiv.org/abs/2305.03236)

    这篇论文首次综述了最新的OOD检测方法在自然语言处理中的应用。根据算法使用的数据，将方法分成三类，并介绍了相关数据集、应用和度量方法。

    

    在现实世界中，基于外部分布（OOD）的检测对于机器学习系统的可靠和安全的部署至关重要。过去几年取得了极大进展。本文重点关注自然语言处理方法，并首次综述了OOD检测方面的最新进展。首先，我们给出OOD检测的正式定义，并讨论了几个相关领域。然后，根据算法使用的数据，将最近的算法分成三类：（1）可用OOD数据，（2）OOD数据不可用+内部分布（ID）标签可用，（3）OOD数据不可用+ID标签不可用。第三，介绍数据集、应用和度量方法。最后，总结现有工作并提出潜在的未来研究课题。

    Out-of-distribution (OOD) detection is essential for the reliable and safe deployment of machine learning systems in the real world. Great progress has been made over the past years. This paper presents the first review of recent advances in OOD detection with a particular focus on natural language processing approaches. First, we provide a formal definition of OOD detection and discuss several related fields. We then categorize recent algorithms into three classes according to the data they used: (1) OOD data available, (2) OOD data unavailable + in-distribution (ID) label available, and (3) OOD data unavailable + ID label unavailable. Third, we introduce datasets, applications, and metrics. Finally, we summarize existing work and present potential future research topics.
    
[^37]: 文本到图像生成中种子选择的重要性

    It is all about where you start: Text-to-image generation with seed selection. (arXiv:2304.14530v1 [cs.CV])

    [http://arxiv.org/abs/2304.14530](http://arxiv.org/abs/2304.14530)

    该论文研究了文本到图像生成中训练数据不平衡对模型的影响，并提出了一种高效的方法：在噪声空间中选择适当的生成种子。该方法能够正确生成罕见的概念，而不需要重新训练模型。

    

    文本到图像扩散模型可以在新的组合和场景中合成大量的概念。然而，它们仍然在生成不常见的概念、罕见的不寻常组合或结构化概念（如手掌）方面有困难。它们的限制部分是由于训练数据的长尾性：网络爬取的数据集严重不平衡，导致模型在分布尾部的概念上表现不足。在这里，我们表征了不平衡训练数据对文本到图像模型的影响，并提出了一个解决方案。我们展示了通过在噪声空间中精心选择适当的生成种子，可以正确生成罕见的概念，这一技术被称为SeedSelect。SeedSelect是高效的，不需要重新训练扩散模型。我们在一系列问题上评估了SeedSelect的效益。首先，在少样本语义数据增强中，我们为少样本和长尾基准生成了语义正确的图像。我们展示了分类

    Text-to-image diffusion models can synthesize a large variety of concepts in new compositions and scenarios. However, they still struggle with generating uncommon concepts, rare unusual combinations, or structured concepts like hand palms. Their limitation is partly due to the long-tail nature of their training data: web-crawled data sets are strongly unbalanced, causing models to under-represent concepts from the tail of the distribution. Here we characterize the effect of unbalanced training data on text-to-image models and offer a remedy. We show that rare concepts can be correctly generated by carefully selecting suitable generation seeds in the noise space, a technique that we call SeedSelect. SeedSelect is efficient and does not require retraining the diffusion model. We evaluate the benefit of SeedSelect on a series of problems. First, in few-shot semantic data augmentation, where we generate semantically correct images for few-shot and long-tail benchmarks. We show classificati
    
[^38]: 异构智能体强化学习

    Heterogeneous-Agent Reinforcement Learning. (arXiv:2304.09870v1 [cs.LG])

    [http://arxiv.org/abs/2304.09870](http://arxiv.org/abs/2304.09870)

    提出了一种异构智能体强化学习（HARL）算法，解决了协作多智能体强化学习中的参数共享限制，同时通过引入多智能体优势分解引理和序列更新方案，建立了异构智能体信任区域学习（HATRL）算法及其易处理的逼近方式 HATRPO 和 HAPPO。此外，发现了一种名为异构智能体镜像学习（HAML）的新型框架，加强了对HATRPO和HAPPO的理论保证。

    

    协作多智能体强化学习（MARL）在人工智能研究中越来越受欢迎，然而，许多研究仍然严重依赖于智能体之间的参数共享，这将它们限制在同质异构智能体设置下，从而导致训练不稳定和缺乏收敛保证。为了在一般的异构智能体设置下实现有效的协作，我们提出了解决上述问题的异构智能体强化学习（HARL）算法。我们的发现核心是多智能体优势分解引理和序列更新方案。基于这些，我们开发了经过验证的无参数共享约束的异构智能体信任区域学习（HATRL）算法，并通过易处理的逼近方式得出了HATRPO和HAPPO。此外，我们发现了一种名为异构智能体镜像学习（HAML）的新型框架，它加强了对HATRPO和HAPPO的理论保证。

    The necessity for cooperation among intelligent machines has popularised cooperative multi-agent reinforcement learning (MARL) in AI research. However, many research endeavours heavily rely on parameter sharing among agents, which confines them to only homogeneous-agent setting and leads to training instability and lack of convergence guarantees. To achieve effective cooperation in the general heterogeneous-agent setting, we propose Heterogeneous-Agent Reinforcement Learning (HARL) algorithms that resolve the aforementioned issues. Central to our findings are the multi-agent advantage decomposition lemma and the sequential update scheme. Based on these, we develop the provably correct Heterogeneous-Agent Trust Region Learning (HATRL) that is free of parameter-sharing constraint, and derive HATRPO and HAPPO by tractable approximations. Furthermore, we discover a novel framework named Heterogeneous-Agent Mirror Learning (HAML), which strengthens theoretical guarantees for HATRPO and HAPP
    
[^39]: 将潜变量对齐：使用潜扩散模型进行高分辨率视频合成

    Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models. (arXiv:2304.08818v1 [cs.CV])

    [http://arxiv.org/abs/2304.08818](http://arxiv.org/abs/2304.08818)

    本文提出了一种高分辨率视频合成方法，通过引入时间维度并在图像序列上微调扩散模型，实现了对实际驾驶数据的模拟和创意内容创作的的良好效果。

    

    潜扩散模型（LDM）通过在压缩的低维潜空间中训练扩散模型，实现高质量的图像合成，同时避免了过多的计算需求。本文将LDM应用于高分辨率视频生成，这是一项特别资源密集型的任务。我们首先对单独的图像进行预训练，然后通过在潜空间扩散模型中引入时间维度，并在编码的图像序列（即视频）上进行微调，将生成器从图像生成器转换为视频生成器。同样，我们在时间上对齐扩散模型上采样器，将其转化为时间一致性的视频超分辨率模型。我们关注两个相关的实际应用：野外驾驶数据的模拟和文本到视频建模的创意内容创作。特别地，我们在分辨率为512 x 1024的真实驾驶视频上验证了我们的视频LDM，并取得了最先进的性能。

    Latent Diffusion Models (LDMs) enable high-quality image synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower-dimensional latent space. Here, we apply the LDM paradigm to high-resolution video generation, a particularly resource-intensive task. We first pre-train an LDM on images only; then, we turn the image generator into a video generator by introducing a temporal dimension to the latent space diffusion model and fine-tuning on encoded image sequences, i.e., videos. Similarly, we temporally align diffusion model upsamplers, turning them into temporally consistent video super resolution models. We focus on two relevant real-world applications: Simulation of in-the-wild driving data and creative content creation with text-to-video modeling. In particular, we validate our Video LDM on real driving videos of resolution 512 x 1024, achieving state-of-the-art performance. Furthermore, our approach can easily leverage off-the-shelf pre-trai
    
[^40]: ImageReward：学习和评估文本到图像生成的人类喜好

    ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation. (arXiv:2304.05977v1 [cs.CV])

    [http://arxiv.org/abs/2304.05977](http://arxiv.org/abs/2304.05977)

    ImageReward是一种通用的文本到图像生成的人类喜好奖励模型，它可以通过收集专家的比较数据集来解决生成模型的问题，并且在人类评估中表现出色，有望成为一种用于评估和改进文本到图像合成的自动度量标准。

    

    本文提出一种通用的文本到图像生成人类喜好奖励模型ImageReward，旨在解决生成模型中存在的各种问题，并使其与人类价值和偏好保持一致。该奖励模型的训练基于我们的系统注释流程，其中包括评分和排名组件，迄今已收集了137k的专家比较数据集。在人类评估中，ImageReward的表现优于现有的评分方法（例如比CLIP高38.6\%），因此它是一种有前途的用于评估和改进文本到图像合成的自动度量标准。该奖励模型通过\texttt {image-reward}程序包公开提供，网址为\url{https://github.com/THUDM/ImageReward}。

    We present ImageReward -- the first general-purpose text-to-image human preference reward model -- to address various prevalent issues in generative models and align them with human values and preferences. Its training is based on our systematic annotation pipeline that covers both the rating and ranking components, collecting a dataset of 137k expert comparisons to date. In human evaluation, ImageReward outperforms existing scoring methods (e.g., CLIP by 38.6\%), making it a promising automatic metric for evaluating and improving text-to-image synthesis. The reward model is publicly available via the \texttt{image-reward} package at \url{https://github.com/THUDM/ImageReward}.
    
[^41]: 适用于异构特征的迁移学习，实现高效的张量程序生成

    Transfer Learning Across Heterogeneous Features For Efficient Tensor Program Generation. (arXiv:2304.05430v1 [cs.PL])

    [http://arxiv.org/abs/2304.05430](http://arxiv.org/abs/2304.05430)

    本研究提出了适用于异构特征的迁移学习方法，在新的目标硬件上通过学习联合神经网络和硬件特征，解决了张量程序生成的自动调整问题。

    

    改进张量程序生成需要在目标硬件上为给定程序搜索各种可能的程序转换组合，以优化张量程序的执行。由于庞大的搜索空间和指数级别的变换组合，自动调整张量程序的生成变得更加困难，尤其是当需要面对异构的目标时。本研究旨在通过学习联合神经网络和硬件特征，并将它们转移到新的目标硬件上，从而解决这些问题。我们广泛研究现有的最先进数据集TenSet，在测试集分割策略上进行比较分析，并提出优化数据集的方法。我们采用注意力启发式方法，为调整张量程序提供支持，使它们能够融入神经网络和硬件特定特征。我们的方法能够将数据集的基线精简高达45％，而不会影响Pairwise Comparis。

    Tuning tensor program generation involves searching for various possible program transformation combinations for a given program on target hardware to optimize the tensor program execution. It is already a complex process because of the massive search space and exponential combinations of transformations make auto-tuning tensor program generation more challenging, especially when we have a heterogeneous target. In this research, we attempt to address these problems by learning the joint neural network and hardware features and transferring them to the new target hardware. We extensively study the existing state-of-the-art dataset, TenSet, perform comparative analysis on the test split strategies and propose methodologies to prune the dataset. We adopt an attention-inspired approach for tuning the tensor programs enabling them to embed neural network and hardware-specific features. Our approach could prune the dataset up to 45\% of the baseline without compromising the Pairwise Comparis
    
[^42]: NeBLa: 使用神经啤酒-兰伯特法从全景放射线图中重建口腔结构的三维模型

    NeBLa: Neural Beer-Lambert for 3D Reconstruction of Oral Structures from Panoramic Radiographs. (arXiv:2304.04027v1 [eess.IV])

    [http://arxiv.org/abs/2304.04027](http://arxiv.org/abs/2304.04027)

    该论文提出了一个新的框架：NeBLa，可以从全景放射线图中通过神经啤酒-兰伯特法重建精确的3D口腔结构模型。

    

    全景X线片（全景放射线图，PX）是常用于牙科检查的成像模式。然而，与3D锥形束计算机断层扫描（CBCT）相比，PX的适用性有限，因为PX只提供口腔结构的二维扁平图像。在本文中，我们提出了一个新的框架，用于从真实的PX图像估计3D口腔结构。由于PX和CBCT数据的匹配不多，我们在训练时使用了从CBCT模拟的PX，但在推理时使用了真实的全景放射线片。我们提出了一种新的光线采样方法，受到全景放射线成像原理的启发，利用啤酒-兰伯特定律导出渲染函数生成模拟全景放射线图。我们的模型由三个部分组成：转换模块，生成模块和精炼模块。转换模块将真实的全景放射线图转换为模拟的训练图像风格。生成模块利用射线采样方法得到的模拟全景放射线图约束下的输入图像生成3D结构。精炼模块改善了3D结构的平滑性和一致性。实验结果表明，我们提出的方法可以从全景放射线片提供的有限信息中生成精确的3D牙科模型。

    Panoramic radiography (panoramic X-ray, PX) is a widely used imaging modality for dental examination. However, its applicability is limited as compared to 3D Cone-beam computed tomography (CBCT), because PX only provides 2D flattened images of the oral structure. In this paper, we propose a new framework which estimates 3D oral structure from real-world PX images. Since there are not many matching PX and CBCT data, we used simulated PX from CBCT for training, however, we used real-world panoramic radiographs at the inference time. We propose a new ray-sampling method to make simulated panoramic radiographs inspired by the principle of panoramic radiography along with the rendering function derived from the Beer-Lambert law. Our model consists of three parts: translation module, generation module, and refinement module. The translation module changes the real-world panoramic radiograph to the simulated training image style. The generation module makes the 3D structure from the input ima
    
[^43]: 利用代理模型识别多任务学习中的负迁移

    Identification of Negative Transfers in Multitask Learning Using Surrogate Models. (arXiv:2303.14582v1 [cs.LG])

    [http://arxiv.org/abs/2303.14582](http://arxiv.org/abs/2303.14582)

    本文提出了一种通过代理建模来解决多任务学习中负迁移问题的方法，能够识别哪些源任务的子集会对目标任务有帮助。

    

    多任务学习广泛应用于通过增加多个相关源任务来训练低资源目标任务。然而，将所有源任务与目标任务简单组合并不总是能提高目标任务的预测性能，因为会存在负迁移。因此，多任务学习的一个关键问题是识别哪些源任务的子集会对目标任务有益。这个问题在计算上很具有挑战性，因为子集的数量随着源任务的数量呈指数级增长。在本文中，我们介绍了一种通过代理建模来解决此问题的有效方法。在代理建模中，我们对源任务进行采样（随机），并预先计算它们的多任务学习表现；然后，我们用线性回归模型来逼近预先计算的表现，该模型也可用于预测未采样的子集的表现。我们在几个合成示例和一个现实世界的多语言情感分析任务上证明了我们方法的有效性。

    Multitask learning is widely used in practice to train a low-resource target task by augmenting it with multiple related source tasks. Yet, naively combining all the source tasks with a target task does not always improve the prediction performance for the target task due to negative transfers. Thus, a critical problem in multitask learning is identifying subsets of source tasks that would benefit the target task. This problem is computationally challenging since the number of subsets grows exponentially with the number of source tasks; efficient heuristics for subset selection does not always capture the relationship between task subsets and multitask learning performances. In this paper, we introduce an efficient procedure to address this problem via surrogate modeling. In surrogate modeling, we sample (random) subsets of source tasks and precompute their multitask learning performances; Then, we approximate the precomputed performances with a linear regression model that can also be
    
[^44]: 面向大规模机器学习模型的可证明高效量子算法

    Towards provably efficient quantum algorithms for large-scale machine-learning models. (arXiv:2303.03428v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2303.03428](http://arxiv.org/abs/2303.03428)

    本论文提出了一种可能的针对通用（随机）梯度下降算法的高效量子解决方案，只要模型足够耗散和稀疏，具有小的学习率，并且可以缩放至 $O(T^2 \times \text{polylog}(n))$。在实践中，证明了在稀疏训练的情况下，量子计算可以显著提高效率。

    

    大型机器学习模型是人工智能的革命性技术，其瓶颈包括巨大的计算开销、功耗和时间，既用于预训练，也用于微调过程。本研究表明，容错量子计算可能会针对通用（随机）梯度下降算法提供可证明的高效解决方案，其缩放为 $\mathcal{O}(T^2 \times \text{polylog}(n))$，其中 $n$ 是模型的大小，$T$ 是训练中的迭代次数，只要模型足够耗散和稀疏，并具有较小的学习率。基于早期用于耗散微分方程的高效量子算法，我们发现并证明了类似的算法可用于（随机）梯度下降，这是机器学习的主要算法。在实践中，我们对拥有从700万到1.03亿个参数的大型机器学习模型进行了基准测试。我们发现，在稀疏训练的情况下，量子计算显然可以在一定程度上提高效率。

    Large machine learning models are revolutionary technologies of artificial intelligence whose bottlenecks include huge computational expenses, power, and time used both in the pre-training and fine-tuning process. In this work, we show that fault-tolerant quantum computing could possibly provide provably efficient resolutions for generic (stochastic) gradient descent algorithms, scaling as $\mathcal{O}(T^2 \times \text{polylog}(n))$, where $n$ is the size of the models and $T$ is the number of iterations in the training, as long as the models are both sufficiently dissipative and sparse, with small learning rates. Based on earlier efficient quantum algorithms for dissipative differential equations, we find and prove that similar algorithms work for (stochastic) gradient descent, the primary algorithm for machine learning. In practice, we benchmark instances of large machine learning models from 7 million to 103 million parameters. We find that, in the context of sparse training, a quan
    
[^45]: MDF-Net：结合X射线与临床数据用于异常检测的方法

    MDF-Net for Abnormality Detection by Fusing X-Rays with Clinical Data. (arXiv:2302.13390v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2302.13390](http://arxiv.org/abs/2302.13390)

    本研究提出了一种多模态融合方法MDF-Net，将临床数据与胸部X射线图像相结合，成功地提高了疾病定位的性能表现。

    

    本研究探讨将患者的临床信息加入深度学习分类器以提高胸部X射线图像疾病定位性能的影响。当前的分类器在使用胸部X射线图像时具有高性能，但我们对放射科医生的访谈表明，临床数据对于解释图像和做出正确的诊断来说非常重要，并具有很高的信息量。我们提出了一种新颖的架构，该架构由两种融合方法组成，使得模型能够同时处理患者的临床数据（结构化数据）和胸部X射线（图像数据）。由于这两种数据模态在不同的维度空间中，因此我们提出了一种空间排列策略空间化，以便在Mask R-CNN模型中促进多模态学习过程。我们使用包含多种数据模态的MIMIC-Eye数据集进行了广泛的实验评估：MIMIC-CXR（胸部X射线图像）、MIMIC IV-ED（患者的临床数据）和REFLACX（评估注释）。实验结果表明，我们提出的MDF-Net相比当前最先进方法具有更优异的性能。

    This study investigates the effects of including patients' clinical information on the performance of deep learning (DL) classifiers for disease location in chest X-ray images. Although current classifiers achieve high performance using chest X-ray images alone, our interviews with radiologists indicate that clinical data is highly informative and essential for interpreting images and making proper diagnoses.  In this work, we propose a novel architecture consisting of two fusion methods that enable the model to simultaneously process patients' clinical data (structured data) and chest X-rays (image data). Since these data modalities are in different dimensional spaces, we propose a spatial arrangement strategy, spatialization, to facilitate the multimodal learning process in a Mask R-CNN model. We performed an extensive experimental evaluation using MIMIC-Eye, a dataset comprising modalities: MIMIC-CXR (chest X-ray images), MIMIC IV-ED (patients' clinical data), and REFLACX (annotatio
    
[^46]: 如何信任您的扩散模型：一种凸优化方法应对符合风险控制的因式分解模型

    How to Trust Your Diffusion Model: A Convex Optimization Approach to Conformal Risk Control. (arXiv:2302.03791v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.03791](http://arxiv.org/abs/2302.03791)

    本文提出了一种风险控制预测集（RCPS）程序的推广，称为$K$-RCPS，它允许为任何扩散模型提供逐个校准的未来样本间隔，并控制相对于基准真实图像的某种风险概念，同时保持最小平均区间长度。

    

    基于分数的生成建模方法，简称扩散模型，在多个重要领域和任务中继续增长。尽管它们提供了来自经验分布的高质量和多样化样本，但在其负责任地用于关键场景方面的可靠性和可信度仍存在重要问题。收敛预测是一种现代工具，用于为任何黑盒子预测器构建有限样本、分布自由的不确定性保证。在这项工作中，我们专注于图像到图像回归任务，并提出了一种风险控制预测集（RCPS）程序的推广，我们称之为$K$-RCPS，它允许$(i)$为任何扩散模型提供逐个校准的未来样本间隔，并$(ii)$控制相对于基准真实图像的某种风险概念，同时保持最小平均区间长度。与现有的收敛风险控制过程不同，我们的过程依靠一种新型的凸优化公式，使其具有计算效率和易于实现的特点。我们在几个图像到图像回归任务上使用得分为基础的生成建模方法来说明我们的程序的有效性，展示了高度校准和良好控制的预测间隔。

    Score-based generative modeling, informally referred to as diffusion models, continue to grow in popularity across several important domains and tasks. While they provide high-quality and diverse samples from empirical distributions, important questions remain on the reliability and trustworthiness of these sampling procedures for their responsible use in critical scenarios. Conformal prediction is a modern tool to construct finite-sample, distribution-free uncertainty guarantees for any black-box predictor. In this work, we focus on image-to-image regression tasks and we present a generalization of the Risk-Controlling Prediction Sets (RCPS) procedure, that we term $K$-RCPS, which allows to $(i)$ provide entrywise calibrated intervals for future samples of any diffusion model, and $(ii)$ control a certain notion of risk with respect to a ground truth image with minimal mean interval length. Differently from existing conformal risk control procedures, ours relies on a novel convex opti
    
[^47]: 基于语言模型的知识图谱嵌入编辑

    Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.10405](http://arxiv.org/abs/2301.10405)

    本文提出了一种新的任务——编辑基于语言模型的知识图谱嵌入，旨在实现对KG嵌入的数据高效和快速更新。针对这一任务，提出了一个简单而强大的方案——KGEditor，可以更好地更新特定事实而不影响其余部分的性能。

    

    近几十年来，使用语言模型进行知识图谱（KG）嵌入已经取得了实证成功。但是，基于语言模型的KG嵌入通常作为静态工件部署，修改起来具有挑战性，需要重新训练。为了解决这个问题，本文提出了一种新的任务，即编辑基于语言模型的KG嵌入。该任务旨在实现对KG嵌入的数据高效和快速更新，而不影响其余部分的性能。我们构建了四个新数据集：E-FB15k237、A-FB15k237、E-WN18RR 和 A-WN18RR，并评估了几种知识编辑基线，证明了之前的模型处理该任务的能力有限。我们进一步提出了一个简单但强大的基线——KGEditor，它利用超网络的附加参数层来编辑/添加事实。全面的实验结果表明，当更新特定事实而不影响其余部分的性能时，KGEditor 的表现更好。

    Recently decades have witnessed the empirical success of framing Knowledge Graph (KG) embeddings via language models. However, language model-based KG embeddings are usually deployed as static artifacts, which are challenging to modify without re-training after deployment. To address this issue, we propose a new task of editing language model-based KG embeddings in this paper. The proposed task aims to enable data-efficient and fast updates to KG embeddings without damaging the performance of the rest. We build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge editing baselines demonstrating the limited ability of previous models to handle the proposed challenging task. We further propose a simple yet strong baseline dubbed KGEditor, which utilizes additional parametric layers of the hyper network to edit/add facts. Comprehensive experimental results demonstrate that KGEditor can perform better when updating specific facts while not affec
    
[^48]: gRoMA: 一种衡量深度神经网络全局鲁棒性的工具

    gRoMA: a Tool for Measuring Deep Neural Networks Global Robustness. (arXiv:2301.02288v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.02288](http://arxiv.org/abs/2301.02288)

    gRoMA是一种衡量DNN全局鲁棒性的创新工具，采用概率验证方法评估特定输出类别遭受到对抗性输入的概率。该工具可运行于预训练的黑盒分类模型上，并对整个模型和每个输入样本产生鲁棒性测量结果。

    

    深度神经网络（DNN）是前沿技术的代表，在各种复杂任务中取得了显著的表现。然而，将它们应用于安全关键系统（如航空或汽车领域）时，由于对抗性输入（即可能导致DNN犯错的输入扰动）的威胁，存在重大挑战。多项研究表明即便是现代DNN也容易受到对抗性输入的影响，因此必须测量并降低这种风险才能在安全关键系统中部署DNN。在这里，我们提出了一种创新且可扩展的工具gRoMA（全局鲁棒性测量和评估），它实现了一种概率验证方法来测量DNN的全局分类鲁棒性。具体而言，gRoMA测量特定输出类别遇到对抗性输入的概率。我们的工具基于预训练的黑盒分类模型，产生整个模型和每个输入样本的鲁棒性测量结果。我们通过测量多个最先进的DNN在热门图像数据集上的鲁棒性并分析结果，证明了我们的工具的有效性。

    Deep neural networks (DNNs) are at the forefront of cutting-edge technology, and have been achieving remarkable performance in a variety of complex tasks. Nevertheless, their integration into safety-critical systems, such as in the aerospace or automotive domains, poses a significant challenge due to the threat of adversarial inputs: perturbations in inputs that might cause the DNN to make grievous mistakes. Multiple studies have demonstrated that even modern DNNs are susceptible to adversarial inputs; and this risk must thus be measured and mitigated to allow the deployment of DNNs in safety-critical systems.  Here, we present gRoMA (global Robustness Measurement and Assessment), an innovative and scalable tool that implements a probabilistic verification approach to measure the global categorial robustness of a DNN. Specifically, gRoMA measures the probability of encountering adversarial inputs for a specific output category. Our tool operates on pre-trained, black-box classification
    
[^49]: 一次性领域自适应在基于视频的手术技能评估中的应用

    One-shot domain adaptation in video-based assessment of surgical skills. (arXiv:2301.00812v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.00812](http://arxiv.org/abs/2301.00812)

    本文提出了一种元学习模型A-VBANet，可以通过一次性学习提供领域不可知的手术技能分类，成功地适应了模拟任务和腹腔镜胆囊切除术，为基于视频的手术技能评估提供了领域不可知程序。

    This paper proposes a meta-learning model, A-VBANet, that can deliver domain-agnostic surgical skill classification via one-shot learning. The model successfully adapts to simulated tasks and laparoscopic cholecystectomy, providing a domain-agnostic procedure for video-based assessment of surgical skills.

    深度学习已经实现了手术技能的自动和客观评估。然而，深度学习模型需要大量数据，并且受限于其训练领域。这阻止了它们过渡到数据有限的新任务。因此，领域自适应对于在现实生活中实现深度学习至关重要。在这里，我们提出了一种元学习模型A-VBANet，它可以通过一次性学习提供领域不可知的手术技能分类。我们在五个腹腔镜和机器人手术模拟器上开发了A-VBANet。此外，我们在腹腔镜胆囊切除术的手术室视频上进行了测试。我们的模型成功地适应了模拟任务，准确率高达99.5%（一次性）和99.9%（少量样本），在腹腔镜胆囊切除术中的准确率为89.7%。我们首次提供了基于视频的手术技能评估的领域不可知程序。这种方法的一个重要影响是它允许使用来自手术模拟器的数据来评估手术表现。

    Deep Learning (DL) has achieved automatic and objective assessment of surgical skills. However, DL models are data-hungry and restricted to their training domain. This prevents them from transitioning to new tasks where data is limited. Hence, domain adaptation is crucial to implement DL in real life. Here, we propose a meta-learning model, A-VBANet, that can deliver domain-agnostic surgical skill classification via one-shot learning. We develop the A-VBANet on five laparoscopic and robotic surgical simulators. Additionally, we test it on operating room (OR) videos of laparoscopic cholecystectomy. Our model successfully adapts with accuracies up to 99.5% in one-shot and 99.9% in few-shot settings for simulated tasks and 89.7% for laparoscopic cholecystectomy. For the first time, we provide a domain-agnostic procedure for video-based assessment of surgical skills. A significant implication of this approach is that it allows the use of data from surgical simulators to assess performance 
    
[^50]: SHAP-XRT: Shapley Value遇上条件独立性测试

    SHAP-XRT: The Shapley Value Meets Conditional Independence Testing. (arXiv:2207.07038v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.07038](http://arxiv.org/abs/2207.07038)

    本文展示了Shapley值解释方法与条件独立性测试之间的紧密关系，并介绍了一种基于条件随机化测试的测试过程SHAP-XRT，用于二分类问题中证明了Shapley值的边际贡献。

    

    人工神经网络的复杂性引发了对其在现实场景中的可靠性、可信度和公平性的关注。Shapley值是机器学习模型最流行的解释方法之一。从统计学的角度来看，特征重要性是通过条件独立性来定义的。到目前为止，这两种解释方法和特征重要性的方法被认为是分开的。本文展示了Shapley值解释方法与条件独立性测试之间的紧密关系。我们引入了SHAPley-E$\textbf{X}$planation $\textbf{R}$andomization $\textbf{T}$est (SHAP-XRT)，一种受条件随机化测试(CRT)启发的测试过程，用于检验特定概念的局部（在样本上的）条件独立性。通过SHAP-XRT，在二分类问题中证明了Shapley值的边际贡献。

    The complex nature of artificial neural networks raises concerns on their reliability, trustworthiness, and fairness in real-world scenarios. The Shapley value -- a solution concept from game theory -- is one of the most popular explanation methods for machine learning models. More traditionally, from a statistical perspective, feature importance is defined in terms of conditional independence. So far, these two approaches to interpretability and feature importance have been considered separate and distinct. In this work, we show that Shapley-based explanation methods and conditional independence testing are closely related. We introduce the $\textbf{SHAP}$ley-E$\textbf{X}$planation $\textbf{R}$andomization $\textbf{T}$est (SHAP-XRT), a testing procedure inspired by the Conditional Randomization Test (CRT) for a specific notion of local (i.e., on a sample) conditional independence. With it, we prove that for binary classification problems, the marginal contributions in the Shapley valu
    
[^51]: 数据有效的 GAN 训练中的自我监督增强技术

    Augmentation-Aware Self-Supervision for Data-Efficient GAN Training. (arXiv:2205.15677v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.15677](http://arxiv.org/abs/2205.15677)

    本文提出一种增强感知的自监督判别器用于对生成数据及其增强参数的预测，从而提高判别器的表现和生成模型的性能，实现了数据有效的 GAN 训练。

    

    有限数据情况下训练生成式对抗网络（GAN）是具有挑战性的，因为判别器容易过拟合。先前提出的可微增强技术改善了GAN训练的数据效率。但是，增强技术隐式地引入了不良不变性因素，因为它忽略了由数据转换引起的标签空间语义变化，这可能限制了判别器的表示学习能力，并最终影响生成模型的表现。为了减轻不变性的负面影响，同时继承数据增强的好处，我们提出了一种新的增强感知的自监督判别器，该判别器可以预测增强数据的参数。特别地，真实数据和生成数据的预测目标在训练过程中需要区别开来。我们还鼓励生成器对抗地生成其增强参数可以被判别器准确预测的数据，从而获得更多信息量和更高效的判别器，提高生成模型的性能。多个数据集上的实验表明，我们的方法在数据有效的 GAN 训练中实现了最先进的性能。

    Training generative adversarial networks (GANs) with limited data is challenging because discriminator is prone to overfitting. Previously proposed differentiable augmentation demonstrates improved data efficiency of training GANs. However, the augmentation implicitly introduces undesired invariance to augmentation for the discriminator since it ignores the change of semantics in the label space caused by data transformation, which may limit the representation learning ability of the discriminator and ultimately affect the generative modeling performance of the generator. To mitigate the negative impact of invariance while inheriting the benefits of data augmentation, we propose a novel augmentation-aware self-supervised discriminator that predicts the augmentation parameter of the augmented data. Particularly, the prediction targets of real data and generated data are required to be distinguished since they are different during training. We further encourage the generator to adversari
    
[^52]: 未知干扰实验中的政策设计

    Policy design in experiments with unknown interference. (arXiv:2011.08174v7 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2011.08174](http://arxiv.org/abs/2011.08174)

    本文研究了如何在有限数量的大簇中进行实验设计，估计和推断最大福利政策，并提出单波实验和多波实验的方法来解决溢出效应问题。

    

    本文研究了在溢出效应存在的情况下估计和推断最大福利政策的实验设计。将单元组织成有限数量的大簇，并在每个簇内以未知的方式相互作用。作为第一项贡献，本文提出了一种单波实验，通过在簇对间仔细变化随机化，考虑溢出效应估计治疗概率变化的边际效应。利用这个边际效应，文章提出了一个检验政策最优性的测试。作为第二项贡献，本文设计了一个多波实验，估计治疗规则并最大化福利。本文对最大可达福利于所估计政策评估下福利之间的差异给出了强有力的小样本保证。作者在根据现有关于信息传播和现金转移计划的实验模拟和大规模现场实验中提供了这种方法的特性。

    This paper studies experimental designs for estimation and inference on welfare-maximizing policies in the presence of spillover effects. Units are organized into a finite number of large clusters and interact in unknown ways within each cluster. As a first contribution, I introduce a single-wave experiment that, by carefully varying the randomization across cluster pairs, estimates the marginal effect of a change in treatment probabilities, taking spillover effects into account. Using the marginal effect, I propose a test for policy optimality. As a second contribution, I design a multiple-wave experiment to estimate treatment rules and maximize welfare. I derive strong small-sample guarantees on the difference between the maximum attainable welfare and the welfare evaluated at the estimated policy. I illustrate the method's properties in simulations calibrated to existing experiments on information diffusion and cash-transfer programs, and in a large scale field experiment implemente
    
[^53]: Gasper：在R中进行图形信号处理

    Gasper: GrAph Signal ProcEssing in R. (arXiv:2007.10642v4 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2007.10642](http://arxiv.org/abs/2007.10642)

    Gasper是一个在R中进行图形信号处理的包，同时提供了与SuiteSparse矩阵集合的接口。

    

    我们介绍了关于使用R语言的gasper包的简短教程。Gasper是一个专门用于图形信号处理的包，还提供了与SuiteSparse矩阵集合的接口。

    We present a short tutorial on to the use of the \proglang{R} \pkg{gasper} package. Gasper is a package dedicated to signal processing on graphs. It also provides an interface to the SuiteSparse Matrix Collection.
    

