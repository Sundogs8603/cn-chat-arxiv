# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds](https://arxiv.org/abs/2404.02866) | 通过添加噪声到最后层的激活来保护隐私，使用HCR界限可量化保护机密性的可信度 |
| [^2] | [Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks](https://arxiv.org/abs/2404.02151) | 展示了对齐的LLM对简单自适应越狱攻击不具有鲁棒性，并成功实现了在多个模型上几乎100%的攻击成功率，同时还介绍了对于不公开logprobs的模型如何进行越狱以及如何在受污染的模型中查找木马字符串的方法。 |
| [^3] | [Preventing Model Collapse in Gaussian Process Latent Variable Models](https://arxiv.org/abs/2404.01697) | 本文通过理论分析投影方差对高斯过程潜变量模型的影响，以及集成了谱混合（SM）核和可微随机傅立叶特征（RFF）核逼近来解决核灵活性不足问题，从而防止模型崩溃。 |
| [^4] | [Evaluating Text-to-Visual Generation with Image-to-Text Generation](https://arxiv.org/abs/2404.01291) | 引入VQAScore，利用视觉问答模型生成对齐分数，取得了在图像文本对齐基准上的最先进结果 |
| [^5] | [Mixing Artificial and Natural Intelligence: From Statistical Mechanics to AI and Back to Turbulence](https://arxiv.org/abs/2403.17993) | 人工智能对通过创新性使用深度神经网络推动湍流减少的拉格朗日模型具有重要影响，为AI和湍流研究之间紧密交织的未来铺平道路。 |
| [^6] | [Foundation Models for Time Series Analysis: A Tutorial and Survey](https://arxiv.org/abs/2403.14735) | Foundation Models为时间序列分析带来创新，利用预训练或微调的模型来获得具体定制的广义知识，提升了实践中多个下游任务的效果。 |
| [^7] | [Convergence of Kinetic Langevin Monte Carlo on Lie groups](https://arxiv.org/abs/2403.12012) | 提出了一个基于Lie群的动力学Langevin Monte Carlo采样算法，通过添加噪声和精细离散化实现了Lie群结构的保持，并在W2距离下证明了连续动力学和离散采样器的指数收敛性。 |
| [^8] | [Learning Useful Representations of Recurrent Neural Network Weight Matrices](https://arxiv.org/abs/2403.11998) | 提出了机械主义和功能主义两种方法以学习递归神经网络(RNN)权重的有用表示，并发展了框架来生成有助于确定RNN行为的丰富表示 |
| [^9] | [LSKNet: A Foundation Lightweight Backbone for Remote Sensing](https://arxiv.org/abs/2403.11735) | LSKNet是一种轻量级的大型选择核网络骨干，能动态调整其较大的空间感受野，以更好地模拟遥感场景中各种对象的远程上下文。 |
| [^10] | [Finite-Time Error Analysis of Soft Q-Learning: Switching System Approach](https://arxiv.org/abs/2403.06366) | 本文通过切换系统模型，针对软Q-learning算法进行了有限时间误差分析，为两种软Q-learning算法导出了新颖的误差界限。 |
| [^11] | [Addressing Shortcomings in Fair Graph Learning Datasets: Towards a New Benchmark](https://arxiv.org/abs/2403.06017) | 通过引入满足广泛要求的合成、半合成和真实世界数据集，解决了公平图学习中数据集信息不足的问题。 |
| [^12] | [HistGen: Histopathology Report Generation via Local-Global Feature Encoding and Cross-modal Context Interaction](https://arxiv.org/abs/2403.05396) | HistGen是一个通过本地-全局特征编码和跨模态上下文交互来生成组织病理学报告的框架，提供了第一个用于评估的基准数据集。 |
| [^13] | [Electrocardiogram Instruction Tuning for Report Generation](https://arxiv.org/abs/2403.04945) | 提出了Multimodal ECG Instruction Tuning（MEIT）框架，首次尝试使用LLMs和多模态指导解决ECG报告生成问题，并在两个大规模ECG数据集上进行了广泛的实验评估其优越性。 |
| [^14] | [Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference](https://arxiv.org/abs/2403.04082) | 通过对比学习学到的时间序列数据表示遵循高斯马尔可夫链，从而启用规划和推断 |
| [^15] | [Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve](https://arxiv.org/abs/2403.02310) | 引入了一种有效的LLM推理调度程序Sarathi-Serve，通过分块预装填技术平衡了GPU计算饱和和单个标记处理的挑战，实现了高吞吐量和低延迟。 |
| [^16] | [EEG2Rep: Enhancing Self-supervised EEG Representation Through Informative Masked Inputs](https://arxiv.org/abs/2402.17772) | EEG2Rep通过在潜在表示空间中预测遮蔽输入和使用新的语义子... |
| [^17] | [Generative Pretrained Hierarchical Transformer for Time Series Forecasting](https://arxiv.org/abs/2402.16516) | 提出一种名为GPHT的新型生成式预训练分层Transformer架构，通过构建混合数据集来预训练模型，解决了时间序列预测中数据集限制和时间依赖性忽视的问题 |
| [^18] | [Equivariant Frames and the Impossibility of Continuous Canonicalization](https://arxiv.org/abs/2402.16077) | 对于常用的群，研究揭示出没有有效的可计算的框架选择能够保持被平均函数的连续性，但本研究提出了加权框架这一解决方案。 |
| [^19] | [A Theoretical Result on the Inductive Bias of RNN Language Models](https://arxiv.org/abs/2402.15814) | RNN语言模型能够有效表示更大类别的语言模型，具有有界堆栈和广义堆栈更新函数，类似于保留固定数量符号记忆并使用简单更新机制的自动机。 |
| [^20] | [Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models](https://arxiv.org/abs/2402.15301) | 利用大型语言模型和检索增强生成技术，提出了一种新方法用于从科学文献中推断因果关系，以解决传统方法受限于数据收集偏见和个体知识的问题。 |
| [^21] | [Pseudorandom Error-Correcting Codes](https://arxiv.org/abs/2402.09370) | 我们构建了一种伪随机纠错码，它对替换和删除错误具有鲁棒性，并且可以高效解码。我们还使用伪随机码提出了一种对语言模型输出进行水印处理的方案，该方案对裁剪和随机替换、删除具有恒定的鲁棒性。 |
| [^22] | [Concept-1K: A Novel Benchmark for Instance Incremental Learning](https://arxiv.org/abs/2402.08526) | 我们提出了一种名为Concept-1K的具有挑战性的实例增量学习（IIL）场景和数据集，揭示了十亿参数的PLM仍然遭受灾难性遗忘，影响因素包括模型规模、预训练和缓冲区大小。现有的IL方法和LoRA技术无法满足性能需求。我们的研究为探索和缓解PLM中的遗忘问题提供了新的场景。 |
| [^23] | [LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation](https://arxiv.org/abs/2402.07721) | 本文提出了LoRA-drop方法，通过分析LoRA输出评估参数的重要性，并且保留重要层的LoRA，其余层共享相同参数。实验结果表明LoRA-drop有很好的效果。 |
| [^24] | [On Differentially Private Subspace Estimation Without Distributional Assumptions](https://arxiv.org/abs/2402.06465) | 本论文研究了在没有分布假设的情况下，差分隐私子空间估计的问题。通过使用少量的数据点，可以私密地识别出低维结构，避免了高维度的代价。 |
| [^25] | [Mitigating Privacy Risk in Membership Inference by Convex-Concave Loss](https://arxiv.org/abs/2402.05453) | 本论文提出了一种凸凹损失函数的方法，通过梯度下降实现训练损失的高方差，从而降低会员推断攻击中的隐私风险。 |
| [^26] | [Navigating Complexity: Toward Lossless Graph Condensation via Expanding Window Matching](https://arxiv.org/abs/2402.05011) | 本文通过连接先前被忽视的监督信号的方式，首次尝试实现无损图谱精简，以解决现有方法无法准确复制原始图谱的问题。 |
| [^27] | [Online Cascade Learning for Efficient Inference over Streams](https://arxiv.org/abs/2402.04513) | 这项研究提出了在线级联学习的方法，通过学习一个“级联”模型，从容量较低的模型到强大的语言模型，以及推迟策略，可以在流数据处理中同时保证准确性和降低推理成本。 |
| [^28] | [Retrieve to Explain: Evidence-driven Predictions with Language Models](https://arxiv.org/abs/2402.04068) | 检索以解释（R2E）是一种基于语言模型的检索方法，通过使用Shapley值确定证据的相对重要性，从而在黑盒模型中提供了可解释性，通过应用于药物靶点鉴定任务中，R2E模型在预测临床试验结果方面优于传统基因学方法。 |
| [^29] | [Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models](https://arxiv.org/abs/2402.02207) | 该论文研究了大规模语言模型在安全微调过程中存在的问题，并提出了一种可行的解决方案。他们首先策划了一个包含各种有害类别的视觉语言安全指令遵循数据集VLGuard，并通过实验证明将该数据集集成到视觉语言微调中或进行事后微调，可以有效地对齐VLLMs的安全性。这对模型的有益性几乎没有影响甚至有所提升。这项研究的贡献是提供了一个有价值的资源，用于对现有VLLMs进行安全测试、训练新模型或保护预训练VLLMs。 |
| [^30] | [Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents](https://arxiv.org/abs/2402.00798) | 本文提出了一种将自然语言和形式语言整合的“正式-LLM”框架，用于解决现有LLM智能体无法控制的计划生成问题。实验证明，该框架在提高生成计划性能和确保可控性方面取得了显著改进。 |
| [^31] | [Benefits of Transformer: In-Context Learning in Linear Regression Tasks with Unstructured Data](https://arxiv.org/abs/2402.00743) | 本研究通过线性回归任务的实验研究了Transformer在非结构化数据中的上下文学习能力，并解释了其中的关键组件。 |
| [^32] | [A Single Graph Convolution Is All You Need: Efficient Grayscale Image Classification](https://arxiv.org/abs/2402.00564) | 本论文提出了一种高效的灰度图像分类方法，通过将图像视为矢量，并使用单个图卷积层进行处理，提高了分类模型的准确性和稳定性。 |
| [^33] | [Positional Encoding Helps Recurrent Neural Networks Handle a Large Vocabulary](https://arxiv.org/abs/2402.00236) | 本研究探讨了位置编码在循环神经网络中的作用，发现即使与RNN结合使用，位置编码仍然有效，尤其是在处理大词汇量和多样观察结果时。这为使用输入驱动和自主时间表示的组合研究提供了新的方向，同时研究结果也对神经元振荡的生物学意义提供了讨论。 |
| [^34] | [Towards Understanding Variants of Invariant Risk Minimization through the Lens of Calibration](https://arxiv.org/abs/2401.17541) | 本研究通过比较分析使用不同近似方法的不变风险最小化（IRM）技术，以期望校准误差（ECE）作为关键指标，观察到基于信息瓶颈的IRM在压缩关键特征上表现最佳。 |
| [^35] | [How Graph Neural Networks Learn: Lessons from Training Dynamics](https://arxiv.org/abs/2310.05105) | 图神经网络的优化过程中涉及核-图对齐现象，从优化角度解释了学到的函数何时和为何泛化，有助于理解其在异源图上的限制。 |
| [^36] | [Segmentation and Characterization of Macerated Fibers and Vessels Using Deep Learning.](http://arxiv.org/abs/2401.16937) | 这项工作开发了一种利用深度学习进行浸软纤维和导管分割和特征提取的自动方法，并且在显微镜图像中取得了快速而准确的分割效果。 |
| [^37] | [Self-Supervised Learning in Event Sequences: A Comparative Study and Hybrid Approach of Generative Modeling and Contrastive Learning.](http://arxiv.org/abs/2401.15935) | 本研究通过比较研究和混合方法，调查了事件序列的自我监督学习技术，并引入了一种新的方法，将生成模型和对比嵌入进行对齐。结果显示，这种对齐模型在各种任务上表现优越，为预测事件序列中的信息提供了潜在的好处。 |
| [^38] | [Prompt Weight Experiments for LLM Instruction Fine-Tuning.](http://arxiv.org/abs/2401.13586) | LLM指令微调中，对于短提示完成数据集，提示词标记分类损失加权（PLW）与性能呈负二次关系，而长提示完成数据集则不受PLW影响。 |
| [^39] | [Memorization in Self-Supervised Learning Improves Downstream Generalization.](http://arxiv.org/abs/2401.12233) | 自监督学习中的记忆化问题一直是一个挑战，本文提出了SSLMem框架，用于定义自监督学习中的记忆化，并通过实证分析证明了在大规模数据集和强数据增强的情况下，记忆化仍然存在。 |
| [^40] | [Spatial-Temporal Large Language Model for Traffic Prediction.](http://arxiv.org/abs/2401.10134) | 本文提出了一种空间-时间大语言模型（ST-LLM）用于交通预测，通过参数扩展和预训练来提高预测准确性，并利用空间-时间嵌入模块学习标记的空间位置和全局时间表示。 |
| [^41] | [Uncertainty Quantification on Clinical Trial Outcome Prediction.](http://arxiv.org/abs/2401.03482) | 本研究将不确定性量化方法应用于临床试验结果预测，提高模型对微妙差异的识别能力，从而改善其整体性能。 |
| [^42] | [Privacy-Preserving Neural Graph Databases.](http://arxiv.org/abs/2312.15591) | 隐私保护的神经图数据库结合了图数据库和神经网络的优势，能够高效存储、检索和分析图结构数据。然而，这种能力也带来了潜在的隐私风险。 |
| [^43] | [Respiratory Anomaly Detection using Reflected Infrared Light-wave Signals.](http://arxiv.org/abs/2311.01367) | 利用反射红外光波信号，开发了一种利用低成本光源和传感器进行非接触呼吸异常检测的方法，实现了96.6%的平均准确率，并能够检测到错误数据。 |
| [^44] | [Compressed representation of brain genetic transcription.](http://arxiv.org/abs/2310.16113) | 本文研究了大脑基因转录的压缩表示方法，通过比较不同的线性和非线性方法，评估了它们在重建、解剖和预测方面的性能。 |
| [^45] | [Language Models as Zero-Shot Trajectory Generators.](http://arxiv.org/abs/2310.11604) | 本文研究了使用大型语言模型（LLMs）作为零-shot轨迹生成器的可能性。通过给予LLM物体检测和分割视觉模型的访问权限，研究人员发现LLMs能够直接预测操作技能中的末端执行器姿态序列，并在26个真实世界的语言任务中取得了良好效果。这一研究突破了对LLMs在机器人技术中的限制，揭示了LLMs确实具有对操作任务的理解能力。 |
| [^46] | [Large Language Models Are Zero-Shot Time Series Forecasters.](http://arxiv.org/abs/2310.07820) | 大型语言模型（LLMs）如GPT-3和LLaMA-2能够令人惊讶地零-shot外推时间序列，其性能可与或超过专门设计的时间序列模型的性能相媲美。这是因为LLMs具有自然地表示多模态分布的能力，并且具有与许多时间序列的重复季节趋势特征相一致的简单性和重复性偏好。 |
| [^47] | [Accelerating optimization over the space of probability measures.](http://arxiv.org/abs/2310.04006) | 本研究研究了在概率测度空间中加速优化的问题，提出了一种类似于欧几里得空间中基于矩方法的哈密顿流方法，并证明了其可以达到任意高阶的收敛速度。 |
| [^48] | [Segment Anything Model is a Good Teacher for Local Feature Learning.](http://arxiv.org/abs/2309.16992) | 本文提出了使用Segment Anything Model (SAM)作为教师来指导本地特征学习，通过像素语义关系蒸馏和弱监督对比学习两种技术，实现了在有限数据集上的更高性能表现。 |
| [^49] | [Structure and Gradient Dynamics Near Global Minima of Two-layer Neural Networks.](http://arxiv.org/abs/2309.00508) | 本论文通过分析两层神经网络在全局最小值附近的结构和梯度动力学，揭示了其泛化能力较强的原因。 |
| [^50] | [Clustering Without an Eigengap.](http://arxiv.org/abs/2308.15642) | 这个论文介绍了在随机块模型中进行图聚类的新算法，能够恢复大聚类，无论其他聚类的大小，并且对中等大小的聚类提出了新的技术挑战。 |
| [^51] | [Promoting Exploration in Memory-Augmented Adam using Critical Momenta.](http://arxiv.org/abs/2307.09638) | 本研究提出了一种记忆增强型Adam方法，通过使用关键动量项的缓冲区来促进对更平坦最小值的探索。实验证明，该方法在标准的监督语言建模和图像分类任务中提高了几种Adam变体的性能。 |
| [^52] | [CatBoost Versus XGBoost and LightGBM: Developing Enhanced Predictive Models for Zero-Inflated Insurance Claim Data.](http://arxiv.org/abs/2307.07771) | 本文比较了CatBoost、XGBoost和LightGBM三种流行的梯度提升库在处理零膨胀保险理赔数据上的效果，并通过对两个不同数据集的分析，证明了CatBoost是最适合训练保险理赔数据和拟合精算频率模型的库。 |
| [^53] | [Zero-Shot Neural Architecture Search: Challenges, Solutions, and Opportunities.](http://arxiv.org/abs/2307.01998) | 零代价神经架构搜索是一种不需要训练的方法，其核心思想是设计能够预测网络精确度的代理。本文综述了最新的零代价神经架构搜索方法，并在硬件感知和硬件无感知的NAS场景中展示了其有效性。 |
| [^54] | [SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation.](http://arxiv.org/abs/2307.01646) | 本文提出了一种新的图生成扩散模型SwinGNN，通过使用高效的2-WL消息传递网络和移动窗口自注意力，以及结合关键的训练和采样技术，显著提高了图生成样本的质量，并引入了随机置换的后处理技巧转换生成的图形统计量。 |
| [^55] | [Balanced Filtering via Non-Disclosive Proxies.](http://arxiv.org/abs/2306.15083) | 本文研究了在群组成员资格不可用或被禁止使用时，如何以非泄露方式收集平衡的数据样本。通过使用代理函数和抽样概率，实现了对个体样本的分类和选择，同时保护个体样本的敏感群组成员资格不被泄露。 |
| [^56] | [State-wise Constrained Policy Optimization.](http://arxiv.org/abs/2306.12594) | 本文提出了一种新的通用策略搜索算法，State-wise Constrained Policy Optimization (SCPO)，可用于处理状态限制约束下的强化学习，具有良好的期望状态约束保证和最坏安全违反的有界性。 |
| [^57] | [On Faking a Nash Equilibrium.](http://arxiv.org/abs/2306.08041) | 本文研究多智能体强化学习中的数据污染攻击，提出了唯一纳什集的概念，并设计了一个线性规划方案来计算最优污染攻击策略。 |
| [^58] | [GPT-FL: Generative Pre-trained Model-Assisted Federated Learning.](http://arxiv.org/abs/2306.02210) | GPT-FL是一种生成预训练模型辅助的联邦学习框架，通过生成多样化的合成数据并结合私有客户端数据进行训练，它在模型准确性、通信效率和客户端采样效率等方面优于最先进的方法。在FL训练中，由合成数据生成的下游模型对于控制梯度多样性的方向起着关键作用，提高了收敛速度，并显著提升了准确性。 |
| [^59] | [Embeddings between Barron spaces with higher order activation functions.](http://arxiv.org/abs/2305.15839) | 本文研究了不同激活函数的Barron空间之间的嵌入，并证明了Barron空间的层次结构类似于Sobolev空间$H^m$。其中，修正功率单位激活函数在这个研究中特别重要。 |
| [^60] | [An Unsupervised Method for Estimating Class Separability of Datasets with Application to LLMs Fine-Tuning.](http://arxiv.org/abs/2305.15016) | 这个论文提出了一种无监督方法，通过利用数据流形的拓扑特征估计数据的类别可分性，该方法与有监督度量具有相关性，可以用于半监督和感知学习。同时，在语言模型微调中应用该方法用于自动停止准则。 |
| [^61] | [Expressiveness Remarks for Denoising Diffusion Models and Samplers.](http://arxiv.org/abs/2305.09605) | 本文在漫扩扩散模型和采样器方面进行了表达能力的研究，通过将已知的神经网络逼近结果扩展到漫扩扩散模型和采样器来实现。 |
| [^62] | [Characterizing the Users, Challenges, and Visualization Needs of Knowledge Graphs in Practice.](http://arxiv.org/abs/2304.01311) | 本研究通过访谈19位知识图谱（KG）实践者，发现KG构建者需求架构执行程序，KG分析师需要可自定义查询构建器，KG消费者需要领域特定可视化，并指出在实践中实施KG需要技术和社交方面的解决方案。 |
| [^63] | [Fixed Design Analysis of Regularization-Based Continual Learning.](http://arxiv.org/abs/2303.10263) | 该论文分析了固定设计下的正则化连续学习问题，提出算法在遗忘和不妥协之间存在权衡，可能会存在灾难性遗忘问题。 |
| [^64] | [Gaussian Process on the Product of Directional Manifolds.](http://arxiv.org/abs/2303.06799) | 本文提出了一种在超环面上建立高斯过程的方法，并使用内在的核相关模型进行学习，以定义在超环面上的向量值函数。通过使用 HvM-based GP 进行数据驱动递归定位，数值结果表明，在跟踪精度方面，该方法具有优势。 |
| [^65] | [Exploring Numerical Priors for Low-Rank Tensor Completion with Generalized CP Decomposition.](http://arxiv.org/abs/2302.05881) | 本文提出了一种新的方法框架GCDTC，利用数值先验和广义CP分解实现了更高的低秩张量补全精度；同时介绍了一个算法SPTC，作为该框架的一个实现。在实验中，该方法表现出比现有技术更好的性能。 |
| [^66] | [Joint Representations for Reinforcement Learning with Multiple Sensors.](http://arxiv.org/abs/2302.05342) | 本文提出了一种基于重构和对比损失的方法来对多传感器的输入进行联合表示学习，证明了这种方法在完成复杂任务时具有良好的效果。 |
| [^67] | [Deep Proxy Causal Learning and its Application to Confounded Bandit Policy Evaluation.](http://arxiv.org/abs/2106.03907) | 本论文提出了一种深度代理因果学习（PCL）方法，用于在存在混淆因素的情况下估计治疗对结果的因果效应。通过构建治疗和代理之间的模型，并利用该模型在给定代理的情况下学习治疗对结果的影响，PCL可以保证恢复真实的因果效应。作者还提出了一种名为深度特征代理变量方法（DFPV）的新方法，用于处理高维和非线性复杂关系的情况，并表明DFPV在合成基准测试中的性能优于最先进的PCL方法。 |

# 详细

[^1]: 通过Hammersley-Chapman-Robbins界限保证机密性

    Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds

    [https://arxiv.org/abs/2404.02866](https://arxiv.org/abs/2404.02866)

    通过添加噪声到最后层的激活来保护隐私，使用HCR界限可量化保护机密性的可信度

    

    在深度神经网络推断过程中通过向最后几层的激活添加噪声来保护隐私是可能的。这些层中的激活被称为“特征”（少见的称为“嵌入”或“特征嵌入”）。添加的噪声有助于防止从嘈杂的特征中重建输入。通过对所有可能的无偏估计量的方差进行下限估计，量化了由此添加的噪声产生的机密性。经典不等式Hammersley和Chapman以及Robbins提供便利的、可计算的界限-- HCR界限。数值实验表明，对于包含10个类别的图像分类数据集“MNIST”和“CIFAR-10”，HCR界限在小型神经网络上表现良好。HCR界限似乎单独无法保证

    arXiv:2404.02866v1 Announce Type: new  Abstract: Protecting privacy during inference with deep neural networks is possible by adding noise to the activations in the last layers prior to the final classifiers or other task-specific layers. The activations in such layers are known as "features" (or, less commonly, as "embeddings" or "feature embeddings"). The added noise helps prevent reconstruction of the inputs from the noisy features. Lower bounding the variance of every possible unbiased estimator of the inputs quantifies the confidentiality arising from such added noise. Convenient, computationally tractable bounds are available from classic inequalities of Hammersley and of Chapman and Robbins -- the HCR bounds. Numerical experiments indicate that the HCR bounds are on the precipice of being effectual for small neural nets with the data sets, "MNIST" and "CIFAR-10," which contain 10 classes each for image classification. The HCR bounds appear to be insufficient on their own to guar
    
[^2]: 用简单自适应攻击越狱功能对齐的LLM

    Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks

    [https://arxiv.org/abs/2404.02151](https://arxiv.org/abs/2404.02151)

    展示了对齐的LLM对简单自适应越狱攻击不具有鲁棒性，并成功实现了在多个模型上几乎100%的攻击成功率，同时还介绍了对于不公开logprobs的模型如何进行越狱以及如何在受污染的模型中查找木马字符串的方法。

    

    我们展示了即使是最新的安全对齐的LLM也不具有抵抗简单自适应越狱攻击的稳健性。首先，我们展示了如何成功利用对logprobs的访问进行越狱：我们最初设计了一个对抗性提示模板（有时会适应目标LLM），然后我们在后缀上应用随机搜索以最大化目标logprob（例如token“Sure”），可能会进行多次重启。通过这种方式，我们实现了对GPT-3.5/4、Llama-2-Chat-7B/13B/70B、Gemma-7B和针对GCG攻击进行对抗训练的HarmBench上的R2D2等几乎100%的攻击成功率--根据GPT-4的评判。我们还展示了如何通过转移或预填充攻击以100%的成功率对所有不暴露logprobs的Claude模型进行越狱。此外，我们展示了如何在受污染的模型中使用对一组受限制的token执行随机搜索以查找木马字符串的方法--这项任务与许多其他任务共享相同的属性。

    arXiv:2404.02151v1 Announce Type: cross  Abstract: We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize the target logprob (e.g., of the token "Sure"), potentially with multiple restarts. In this way, we achieve nearly 100\% attack success rate -- according to GPT-4 as a judge -- on GPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with 100\% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many s
    
[^3]: 防止高斯过程潜变量模型中的模型崩溃

    Preventing Model Collapse in Gaussian Process Latent Variable Models

    [https://arxiv.org/abs/2404.01697](https://arxiv.org/abs/2404.01697)

    本文通过理论分析投影方差对高斯过程潜变量模型的影响，以及集成了谱混合（SM）核和可微随机傅立叶特征（RFF）核逼近来解决核灵活性不足问题，从而防止模型崩溃。

    

    Gaussian process latent variable models (GPLVMs)是一类多才多艺的无监督学习模型，通常用于降维。然而，用GPLVMs对数据建模时常见的挑战包括核灵活性不足和投影噪声选择不当，导致了一种以模糊潜变量表示为主要特征的模型崩溃，这种表示不反映数据的潜在结构。本文首先从理论上通过线性GPLVM的视角研究了投影方差对模型崩溃的影响。其次，通过集成谱混合（SM）核和可微随机傅立叶特征（RFF）核逼近，解决了由于核灵活性不足导致的模型崩溃问题，从而保证了通过现成的自动微分工具实现学习核参数的计算可扩展性和效率。

    arXiv:2404.01697v1 Announce Type: cross  Abstract: Gaussian process latent variable models (GPLVMs) are a versatile family of unsupervised learning models, commonly used for dimensionality reduction. However, common challenges in modeling data with GPLVMs include inadequate kernel flexibility and improper selection of the projection noise, which leads to a type of model collapse characterized primarily by vague latent representations that do not reflect the underlying structure of the data. This paper addresses these issues by, first, theoretically examining the impact of the projection variance on model collapse through the lens of a linear GPLVM. Second, we address the problem of model collapse due to inadequate kernel flexibility by integrating the spectral mixture (SM) kernel and a differentiable random Fourier feature (RFF) kernel approximation, which ensures computational scalability and efficiency through off-the-shelf automatic differentiation tools for learning the kernel hype
    
[^4]: 评估文本到图像生成与图像到文本生成

    Evaluating Text-to-Visual Generation with Image-to-Text Generation

    [https://arxiv.org/abs/2404.01291](https://arxiv.org/abs/2404.01291)

    引入VQAScore，利用视觉问答模型生成对齐分数，取得了在图像文本对齐基准上的最先进结果

    

    尽管生成式人工智能取得了显著进展，但由于缺乏有效的度量标准和标准化基准，综合评估仍然具有挑战性。为了解决这个问题，我们引入了VQAScore，使用视觉问答（VQA）模型通过计算对简单的“这幅图表现出了'{文本}'吗？”问题的“是”答案的概率来生成对齐分数。尽管比先前的方法更简单，但使用现成模型计算的VQAScore在许多（8个）图像文本对齐基准上产生了最先进的结果。

    arXiv:2404.01291v1 Announce Type: cross  Abstract: Despite significant progress in generative AI, comprehensive evaluation remains challenging because of the lack of effective metrics and standardized benchmarks. For instance, the widely-used CLIPScore measures the alignment between a (generated) image and text prompt, but it fails to produce reliable scores for complex prompts involving compositions of objects, attributes, and relations. One reason is that text encoders of CLIP can notoriously act as a "bag of words", conflating prompts such as "the horse is eating the grass" with "the grass is eating the horse". To address this, we introduce the VQAScore, which uses a visual-question-answering (VQA) model to produce an alignment score by computing the probability of a "Yes" answer to a simple "Does this figure show '{text}'?" question. Though simpler than prior art, VQAScore computed with off-the-shelf models produces state-of-the-art results across many (8) image-text alignment benc
    
[^5]: 将人工智能与自然智能相融合：从统计力学到人工智能再到湍流

    Mixing Artificial and Natural Intelligence: From Statistical Mechanics to AI and Back to Turbulence

    [https://arxiv.org/abs/2403.17993](https://arxiv.org/abs/2403.17993)

    人工智能对通过创新性使用深度神经网络推动湍流减少的拉格朗日模型具有重要影响，为AI和湍流研究之间紧密交织的未来铺平道路。

    

    这篇论文反思了人工智能在科学研究中的未来角色，特别关注了湍流研究，并通过根植于非平衡统计力学的扩散模型来检验人工智能的发展，强调了人工智能通过创新性地利用深度神经网络推动减少的拉格朗日湍流模型的重要影响。此外，论文审查了湍流研究中的各种其他人工智能应用，并概述了在人工智能和统计流体力学的同时发展中的潜在挑战和机会。

    arXiv:2403.17993v1 Announce Type: cross  Abstract: The paper reflects on the future role of AI in scientific research, with a special focus on turbulence studies, and examines the evolution of AI, particularly through Diffusion Models rooted in non-equilibrium statistical mechanics. It underscores the significant impact of AI on advancing reduced, Lagrangian models of turbulence through innovative use of deep neural networks. Additionally, the paper reviews various other AI applications in turbulence research and outlines potential challenges and opportunities in the concurrent advancement of AI and statistical hydrodynamics. This discussion sets the stage for a future where AI and turbulence research are intricately intertwined, leading to more profound insights and advancements in both fields.
    
[^6]: 基于Foundation Models的时间序列分析：教程与调查

    Foundation Models for Time Series Analysis: A Tutorial and Survey

    [https://arxiv.org/abs/2403.14735](https://arxiv.org/abs/2403.14735)

    Foundation Models为时间序列分析带来创新，利用预训练或微调的模型来获得具体定制的广义知识，提升了实践中多个下游任务的效果。

    

    时间序列分析作为数据挖掘领域的焦点，是提取有价值见解的基石，对众多实际应用至关重要。最近Foundation Models（FMs）的发展根本性地改变了时间序列分析模型设计的范式，提升了实践中各种下游任务的效果。这些创新方法通常利用预训练或微调的FMs，以获取专门为时间序列分析量身定制的广义知识。在本调查中，我们旨在提供Foundation Models用于时间序列分析的全面和最新概述。尽管先前的调查主要集中在Foundation Models在时间序列分析中的应用或管道方面，但它们往往缺乏深入了解阐明Foundation Models如何受益时间序列分析的基本机制。为弥补这一空白，我们的调查采用了以模型为中心的分类

    arXiv:2403.14735v1 Announce Type: new  Abstract: Time series analysis stands as a focal point within the data mining community, serving as a cornerstone for extracting valuable insights crucial to a myriad of real-world applications. Recent advancements in Foundation Models (FMs) have fundamentally reshaped the paradigm of model design for time series analysis, boosting various downstream tasks in practice. These innovative approaches often leverage pre-trained or fine-tuned FMs to harness generalized knowledge tailored specifically for time series analysis. In this survey, we aim to furnish a comprehensive and up-to-date overview of FMs for time series analysis. While prior surveys have predominantly focused on either the application or the pipeline aspects of FMs in time series analysis, they have often lacked an in-depth understanding of the underlying mechanisms that elucidate why and how FMs benefit time series analysis. To address this gap, our survey adopts a model-centric class
    
[^7]: 基于Lie群的动力学Langevin Monte Carlo算法的收敛性

    Convergence of Kinetic Langevin Monte Carlo on Lie groups

    [https://arxiv.org/abs/2403.12012](https://arxiv.org/abs/2403.12012)

    提出了一个基于Lie群的动力学Langevin Monte Carlo采样算法，通过添加噪声和精细离散化实现了Lie群结构的保持，并在W2距离下证明了连续动力学和离散采样器的指数收敛性。

    

    最近，基于变分优化和左平凡化等技术构建了一个明确的、基于动量的动力学系统，用于优化定义在Lie群上的函数。我们适当地为优化动力学添加可处理的噪声，将其转化为采样动力学，利用动量变量是欧几里得的这一有利特性，尽管潜在函数存在于流形上。然后，我们通过精心离散化导致的动力学采样动力学提出了一个Lie群MCMC采样器。这种离散化完全保持了Lie群结构。在W2距离下，分别对连续动力学和离散采样器证明了指数收敛性，其中只需要Lie群的紧致性和潜在函数的测地L-光滑性。据我们所知，这是对动力学Langevin算法的第一个收敛性结果。

    arXiv:2403.12012v1 Announce Type: cross  Abstract: Explicit, momentum-based dynamics for optimizing functions defined on Lie groups was recently constructed, based on techniques such as variational optimization and left trivialization. We appropriately add tractable noise to the optimization dynamics to turn it into a sampling dynamics, leveraging the advantageous feature that the momentum variable is Euclidean despite that the potential function lives on a manifold. We then propose a Lie-group MCMC sampler, by delicately discretizing the resulting kinetic-Langevin-type sampling dynamics. The Lie group structure is exactly preserved by this discretization. Exponential convergence with explicit convergence rate for both the continuous dynamics and the discrete sampler are then proved under W2 distance. Only compactness of the Lie group and geodesically L-smoothness of the potential function are needed. To the best of our knowledge, this is the first convergence result for kinetic Langev
    
[^8]: 学习递归神经网络权重矩阵的有用表示

    Learning Useful Representations of Recurrent Neural Network Weight Matrices

    [https://arxiv.org/abs/2403.11998](https://arxiv.org/abs/2403.11998)

    提出了机械主义和功能主义两种方法以学习递归神经网络(RNN)权重的有用表示，并发展了框架来生成有助于确定RNN行为的丰富表示

    

    递归神经网络(RNNs)是通用的并行串行计算机。 RNN的程序是其权重矩阵。 如何学习有助于RNN分析以及下游任务的RNN权重的有用表示？ 尽管机械主义方法直接查看一些RNN的权重来预测其行为，功能主义方法分析其整体功能--具体来说是其输入输出映射。 我们考虑了几种适用于RNN权重的机械主义方法，并为RNN引入了置换等变的深度权重空间层。我们的两种新颖的功能主义方法通过“询问”输入而从RNN权重中提取信息。 我们开发了一个理论框架，证明了功能主义方法能够产生有助于确定RNN行为的丰富表示的条件。 我们创建并发布了第一个两个“模型动物园”数据集，用于RNN权重表示

    arXiv:2403.11998v1 Announce Type: new  Abstract: Recurrent Neural Networks (RNNs) are general-purpose parallel-sequential computers. The program of an RNN is its weight matrix. How to learn useful representations of RNN weights that facilitate RNN analysis as well as downstream tasks? While the mechanistic approach directly looks at some RNN's weights to predict its behavior, the functionalist approach analyzes its overall functionality -- specifically, its input-output mapping. We consider several mechanistic approaches for RNN weights and adapt the permutation equivariant Deep Weight Space layer for RNNs. Our two novel functionalist approaches extract information from RNN weights by 'interrogating' the RNN through probing inputs. We develop a theoretical framework that demonstrates conditions under which the functionalist approach can generate rich representations that help determine RNN behavior. We create and release the first two 'model zoo' datasets for RNN weight representation 
    
[^9]: LSKNet：一种用于遥感的轻量级基础架构

    LSKNet: A Foundation Lightweight Backbone for Remote Sensing

    [https://arxiv.org/abs/2403.11735](https://arxiv.org/abs/2403.11735)

    LSKNet是一种轻量级的大型选择核网络骨干，能动态调整其较大的空间感受野，以更好地模拟遥感场景中各种对象的远程上下文。

    

    遥感图像由于其固有的复杂性对下游任务提出了独特的挑战。尽管已经有大量研究致力于遥感分类、目标检测和语义分割，但其中大多数研究都忽视了嵌入在遥感场景中的宝贵先验知识。这些先验知识可能会很有用，因为在没有参考足够长程上下文的情况下，遥感对象可能会被错误识别，而这可以因不同对象而异。本文考虑了这些先验知识，并提出了一种轻量级的大型选择核网络（LSKNet）骨干网络。LSKNet可以动态调整其较大的空间感受野，以更好地模拟遥感场景中各种对象的远距离上下文。据我们所知，先前尚未在遥感图像中探索过大型和选择性核机制。我们的轻量级方法没有太多复杂性。

    arXiv:2403.11735v1 Announce Type: cross  Abstract: Remote sensing images pose distinct challenges for downstream tasks due to their inherent complexity. While a considerable amount of research has been dedicated to remote sensing classification, object detection and semantic segmentation, most of these studies have overlooked the valuable prior knowledge embedded within remote sensing scenarios. Such prior knowledge can be useful because remote sensing objects may be mistakenly recognized without referencing a sufficiently long-range context, which can vary for different objects. This paper considers these priors and proposes a lightweight Large Selective Kernel Network (LSKNet) backbone. LSKNet can dynamically adjust its large spatial receptive field to better model the ranging context of various objects in remote sensing scenarios. To our knowledge, large and selective kernel mechanisms have not been previously explored in remote sensing images. Without bells and whistles, our lightw
    
[^10]: 软Q-learning的有限时间误差分析：切换系统方法

    Finite-Time Error Analysis of Soft Q-Learning: Switching System Approach

    [https://arxiv.org/abs/2403.06366](https://arxiv.org/abs/2403.06366)

    本文通过切换系统模型，针对软Q-learning算法进行了有限时间误差分析，为两种软Q-learning算法导出了新颖的误差界限。

    

    Soft Q-learning是Q-learning的一种变体，旨在解决熵正则化马尔可夫决策问题，其中代理的目标是最大化熵正则化值函数。尽管在经验上取得成功，但迄今为止对软Q-learning的理论研究有限。本文旨在提供对软Q-learning算法的新颖和统一的有限时间、控制论分析。我们专注于两种类型的软Q-learning算法：一种利用对数和指数运算子，另一种采用玻尔兹曼运算子。通过使用动态切换系统模型，我们为两种软Q-learning算法推导出了新颖的有限时间误差界限。我们希望我们的分析能够通过与切换系统模型建立联系来加深对软Q-learning的当前理解，甚至为其他强化学习算法的有限时间分析的新框架铺平道路。

    arXiv:2403.06366v1 Announce Type: new  Abstract: Soft Q-learning is a variation of Q-learning designed to solve entropy regularized Markov decision problems where an agent aims to maximize the entropy regularized value function. Despite its empirical success, there have been limited theoretical studies of soft Q-learning to date. This paper aims to offer a novel and unified finite-time, control-theoretic analysis of soft Q-learning algorithms. We focus on two types of soft Q-learning algorithms: one utilizing the log-sum-exp operator and the other employing the Boltzmann operator. By using dynamical switching system models, we derive novel finite-time error bounds for both soft Q-learning algorithms. We hope that our analysis will deepen the current understanding of soft Q-learning by establishing connections with switching system models and may even pave the way for new frameworks in the finite-time analysis of other reinforcement learning algorithms.
    
[^11]: 弥补公平图学习数据集的不足：走向一个新的基准

    Addressing Shortcomings in Fair Graph Learning Datasets: Towards a New Benchmark

    [https://arxiv.org/abs/2403.06017](https://arxiv.org/abs/2403.06017)

    通过引入满足广泛要求的合成、半合成和真实世界数据集，解决了公平图学习中数据集信息不足的问题。

    

    公平图学习在许多实际应用中起着关键作用。最近，提出了许多公平图学习方法；然而，它们的评估往往依赖于构造不佳的半合成数据集或低标准的真实世界数据集。在这种情况下，即使是基本的多层感知机（MLP）也可以在效用和公平性方面胜过图神经网络（GNNs）。在这项工作中，我们说明许多数据集未能提供有意义的边缘信息，这可能挑战使用图结构在这些问题中的必要性。为解决这些问题，我们开发并引入了一个充分满足广泛要求的合成、半合成和真实世界数据集集合。这些数据集经过精心设计，包括对于模型公平评价至关重要的相关图结构和偏差信息。所提出的合成和半合成数据集提供了灵活性...

    arXiv:2403.06017v1 Announce Type: new  Abstract: Fair graph learning plays a pivotal role in numerous practical applications. Recently, many fair graph learning methods have been proposed; however, their evaluation often relies on poorly constructed semi-synthetic datasets or substandard real-world datasets. In such cases, even a basic Multilayer Perceptron (MLP) can outperform Graph Neural Networks (GNNs) in both utility and fairness. In this work, we illustrate that many datasets fail to provide meaningful information in the edges, which may challenge the necessity of using graph structures in these problems. To address these issues, we develop and introduce a collection of synthetic, semi-synthetic, and real-world datasets that fulfill a broad spectrum of requirements. These datasets are thoughtfully designed to include relevant graph structures and bias information crucial for the fair evaluation of models. The proposed synthetic and semi-synthetic datasets offer the flexibility to
    
[^12]: HistGen：通过本地-全局特征编码和跨模态上下文交互生成组织病理学报告

    HistGen: Histopathology Report Generation via Local-Global Feature Encoding and Cross-modal Context Interaction

    [https://arxiv.org/abs/2403.05396](https://arxiv.org/abs/2403.05396)

    HistGen是一个通过本地-全局特征编码和跨模态上下文交互来生成组织病理学报告的框架，提供了第一个用于评估的基准数据集。

    

    组织病理学在癌症诊断中扮演着黄金标准的角色，临床报告在解释和理解这一过程中至关重要，在指导癌症治疗和患者护理方面起着关键作用。深度学习对组织病理学报告生成的自动化将极大提升临床效率，并减轻病理学家在报告撰写方面的劳动强度和耗时负担。为追求这一进步，作者引入了HistGen，这是一个多实例学习增强的组织病理学报告生成框架，并提供第一个用于评估的基准数据集。HistGen受诊断和报告撰写工作流程的启发，具有两个精心设计的模块，旨在通过对齐整张切片图像（WSIs）和诊断报告，从本地和全局粒度提升报告生成。为实现这一目标，开发了一个本地-全局分层编码器，用于有效地从区域中聚合视觉特征。

    arXiv:2403.05396v1 Announce Type: cross  Abstract: Histopathology serves as the gold standard in cancer diagnosis, with clinical reports being vital in interpreting and understanding this process, guiding cancer treatment and patient care. The automation of histopathology report generation with deep learning stands to significantly enhance clinical efficiency and lessen the labor-intensive, time-consuming burden on pathologists in report writing. In pursuit of this advancement, we introduce HistGen, a multiple instance learning-empowered framework for histopathology report generation together with the first benchmark dataset for evaluation. Inspired by diagnostic and report-writing workflows, HistGen features two delicately designed modules, aiming to boost report generation by aligning whole slide images (WSIs) and diagnostic reports from local and global granularity. To achieve this, a local-global hierarchical encoder is developed for efficient visual feature aggregation from a regi
    
[^13]: 为报告生成调优心电图指导

    Electrocardiogram Instruction Tuning for Report Generation

    [https://arxiv.org/abs/2403.04945](https://arxiv.org/abs/2403.04945)

    提出了Multimodal ECG Instruction Tuning（MEIT）框架，首次尝试使用LLMs和多模态指导解决ECG报告生成问题，并在两个大规模ECG数据集上进行了广泛的实验评估其优越性。

    

    心电图（ECG）作为心脏病情监测的主要非侵入性诊断工具，对于协助临床医生至关重要。最近的研究集中在使用ECG数据对心脏病情进行分类，但忽略了ECG报告生成，这不仅耗时，而且需要临床专业知识。为了自动化ECG报告生成并确保其多功能性，我们提出了Multimodal ECG Instruction Tuning（MEIT）框架，这是\textit{首次}尝试使用LLMs和多模态指导来解决ECG报告生成问题。为了促进未来的研究，我们建立了一个基准来评估MEIT在两个大规模ECG数据集上使用各种LLM骨干的表现。我们的方法独特地对齐了ECG信号和报告的表示，并进行了大量实验来评估MEIT与九个开源LLMs，使用了超过80万个ECG报告。MEIT的结果凸显了其优越性。

    arXiv:2403.04945v1 Announce Type: new  Abstract: Electrocardiogram (ECG) serves as the primary non-invasive diagnostic tool for cardiac conditions monitoring, are crucial in assisting clinicians. Recent studies have concentrated on classifying cardiac conditions using ECG data but have overlooked ECG report generation, which is not only time-consuming but also requires clinical expertise. To automate ECG report generation and ensure its versatility, we propose the Multimodal ECG Instruction Tuning (MEIT) framework, the \textit{first} attempt to tackle ECG report generation with LLMs and multimodal instructions. To facilitate future research, we establish a benchmark to evaluate MEIT with various LLMs backbones across two large-scale ECG datasets. Our approach uniquely aligns the representations of the ECG signal and the report, and we conduct extensive experiments to benchmark MEIT with nine open source LLMs, using more than 800,000 ECG reports. MEIT's results underscore the superior p
    
[^14]: 通过插值进行推断：对比表示可证明启用规划和推断

    Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference

    [https://arxiv.org/abs/2403.04082](https://arxiv.org/abs/2403.04082)

    通过对比学习学到的时间序列数据表示遵循高斯马尔可夫链，从而启用规划和推断

    

    给定时间序列数据，我们如何回答诸如“未来会发生什么？”和“我们是如何到达这里的？”这类概率推断问题在观测值为高维时具有挑战性。本文展示了这些问题如何通过学习表示的紧凑闭式解决方案。关键思想是将对比学习的变体应用于时间序列数据。之前的工作已经表明，通过对比学习学到的表示编码了概率比。通过将之前的工作扩展以表明表示的边际分布是高斯分布，我们随后证明表示的联合分布也是高斯分布。这些结果共同表明，通过时间对比学习学到的表示遵循高斯马尔可夫链，一种图形模型，其中对表示进行的推断（例如预测、规划）对应于反演低维分布。

    arXiv:2403.04082v1 Announce Type: new  Abstract: Given time series data, how can we answer questions like "what will happen in the future?" and "how did we get here?" These sorts of probabilistic inference questions are challenging when observations are high-dimensional. In this paper, we show how these questions can have compact, closed form solutions in terms of learned representations. The key idea is to apply a variant of contrastive learning to time series data. Prior work already shows that the representations learned by contrastive learning encode a probability ratio. By extending prior work to show that the marginal distribution over representations is Gaussian, we can then prove that joint distribution of representations is also Gaussian. Taken together, these results show that representations learned via temporal contrastive learning follow a Gauss-Markov chain, a graphical model where inference (e.g., prediction, planning) over representations corresponds to inverting a low-
    
[^15]: 在LLM推理中平衡吞吐量和延迟权衡的研究：Sarathi-Serve方法

    Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve

    [https://arxiv.org/abs/2403.02310](https://arxiv.org/abs/2403.02310)

    引入了一种有效的LLM推理调度程序Sarathi-Serve，通过分块预装填技术平衡了GPU计算饱和和单个标记处理的挑战，实现了高吞吐量和低延迟。

    

    每个LLM服务请求经历两个阶段。首先是prefill阶段，处理整个输入提示以生成一个输出标记；第二个是decode阶段，逐个生成其余的输出标记。Prefill迭代具有较高的延迟，但由于输入提示的并行处理，可以使GPU计算饱和。相比之下，decode迭代具有较低的延迟，但也仅使用较低的计算资源，因为每个请求只处理一个标记。这使得对解码来说批处理非常有效，因此对整体吞吐量也很有效。然而，批量处理多个请求会导致prefill和decode迭代交错进行，这使得在实现高吞吐量和低延迟之间的平衡变得具有挑战性。我们引入了一个高效的LLM推理调度程序Sarathi-Serve，灵感来自我们最初为优化Sarathi的吞吐量提出的技术。Sarathi-Serve利用了从Sarathi中引入的分块prefill技术。

    arXiv:2403.02310v1 Announce Type: new  Abstract: Each LLM serving request goes through two phases. The first is prefill which processes the entire input prompt to produce one output token and the second is decode which generates the rest of output tokens, one-at-a-time. Prefill iterations have high latency but saturate GPU compute due to parallel processing of the input prompt. In contrast, decode iterations have low latency but also low compute utilization because a decode iteration processes only a single token per request. This makes batching highly effective for decodes and consequently for overall throughput. However, batching multiple requests leads to an interleaving of prefill and decode iterations which makes it challenging to achieve both high throughput and low latency.   We introduce an efficient LLM inference scheduler Sarathi-Serve inspired by the techniques we originally proposed for optimizing throughput in Sarathi. Sarathi-Serve leverages chunked-prefills from Sarathi 
    
[^16]: EEG2Rep：通过信息化遮蔽输入增强自监督脑电图表示

    EEG2Rep: Enhancing Self-supervised EEG Representation Through Informative Masked Inputs

    [https://arxiv.org/abs/2402.17772](https://arxiv.org/abs/2402.17772)

    EEG2Rep通过在潜在表示空间中预测遮蔽输入和使用新的语义子...

    

    脑电图（EEG）表示学习的自监督方法面临EEG数据固有的三个特定挑战：（1）低信噪比挑战学到的表示质量，（2）振幅范围广，从非常小到相对较大，由于诸如受试者间变异性等因素，风险导致模型被高振幅范围主导，和（3）连续值序列中缺乏明确分割，可能导致信息较少的表示。为了解决这些挑战，我们引入了EEG2Rep，一种用于从EEG进行自监督表示学习的自预测方法。EEG2Rep的两个核心新颖组成部分如下：1）EEG2Rep不是学习从原始EEG预测遮蔽输入，而是学习在潜在表示空间中预测遮蔽输入，2）EEG2Rep不使用传统的遮蔽方法，而是使用一个新的语义子

    arXiv:2402.17772v1 Announce Type: cross  Abstract: Self-supervised approaches for electroencephalography (EEG) representation learning face three specific challenges inherent to EEG data: (1) The low signal-to-noise ratio which challenges the quality of the representation learned, (2) The wide range of amplitudes from very small to relatively large due to factors such as the inter-subject variability, risks the models to be dominated by higher amplitude ranges, and (3) The absence of explicit segmentation in the continuous-valued sequences which can result in less informative representations. To address these challenges, we introduce EEG2Rep, a self-prediction approach for self-supervised representation learning from EEG. Two core novel components of EEG2Rep are as follows: 1) Instead of learning to predict the masked input from raw EEG, EEG2Rep learns to predict masked input in latent representation space, and 2) Instead of conventional masking methods, EEG2Rep uses a new semantic sub
    
[^17]: 针对时间序列预测的生成式预训练分层Transformer

    Generative Pretrained Hierarchical Transformer for Time Series Forecasting

    [https://arxiv.org/abs/2402.16516](https://arxiv.org/abs/2402.16516)

    提出一种名为GPHT的新型生成式预训练分层Transformer架构，通过构建混合数据集来预训练模型，解决了时间序列预测中数据集限制和时间依赖性忽视的问题

    

    最近的研究致力于通过引入先进的网络架构和自监督预训练策略来提高时间序列预测的准确性。然而，现有方法仍存在两个关键缺陷。首先，这些方法通常依赖于单一数据集进行训练，由于训练数据的规模受限，限制了模型的泛化能力。其次，广泛采用一步生成模式，这需要定制化的预测头部，忽略了输出序列中的时间依赖性，同时在不同的时间跨度设置下会导致增加的训练成本。为了解决这些问题，我们提出了一种新颖的用于预测的生成式预训练分层Transformer架构，命名为GPHT。GPHT中有两个关键设计方面。

    arXiv:2402.16516v1 Announce Type: new  Abstract: Recent efforts have been dedicated to enhancing time series forecasting accuracy by introducing advanced network architectures and self-supervised pretraining strategies. Nevertheless, existing approaches still exhibit two critical drawbacks. Firstly, these methods often rely on a single dataset for training, limiting the model's generalizability due to the restricted scale of the training data. Secondly, the one-step generation schema is widely followed, which necessitates a customized forecasting head and overlooks the temporal dependencies in the output series, and also leads to increased training costs under different horizon length settings.   To address these issues, we propose a novel generative pretrained hierarchical transformer architecture for forecasting, named GPHT. There are two aspects of key designs in GPHT. On the one hand, we advocate for constructing a mixed dataset for pretraining our model, comprising various dataset
    
[^18]: 等变框架与连续规范化的不可能性

    Equivariant Frames and the Impossibility of Continuous Canonicalization

    [https://arxiv.org/abs/2402.16077](https://arxiv.org/abs/2402.16077)

    对于常用的群，研究揭示出没有有效的可计算的框架选择能够保持被平均函数的连续性，但本研究提出了加权框架这一解决方案。

    

    规范化提供了一种与架构无关的方法来强制保持等变性，近期广受关注的泛化方法如框架平均化成为一种轻量且灵活的等变架构替代方案。 最近的研究发现，使用概率框架能够带来实证效益，这些框架学习群元素上的加权分布。 本文提供了关于这一现象的强有力理论证据：对于常用的群，没有有效的可计算的框架选择能够保持被平均函数的连续性。 换句话说，非加权的框架平均可以将一个平滑的、非对称的函数转变为一个不连续、对称的函数。 为了解决这一基本的鲁棒性问题，我们正式定义并构建了\emph{加权}框架，据证明能够保持连续性，并通过构建高效连续的加权框架展示了它们的实用性。

    arXiv:2402.16077v1 Announce Type: new  Abstract: Canonicalization provides an architecture-agnostic method for enforcing equivariance, with generalizations such as frame-averaging recently gaining prominence as a lightweight and flexible alternative to equivariant architectures. Recent works have found an empirical benefit to using probabilistic frames instead, which learn weighted distributions over group elements. In this work, we provide strong theoretical justification for this phenomenon: for commonly-used groups, there is no efficiently computable choice of frame that preserves continuity of the function being averaged. In other words, unweighted frame-averaging can turn a smooth, non-symmetric function into a discontinuous, symmetric function. To address this fundamental robustness problem, we formally define and construct \emph{weighted} frames, which provably preserve continuity, and demonstrate their utility by constructing efficient and continuous weighted frames for the act
    
[^19]: RNN语言模型归纳偏差的一个理论结果

    A Theoretical Result on the Inductive Bias of RNN Language Models

    [https://arxiv.org/abs/2402.15814](https://arxiv.org/abs/2402.15814)

    RNN语言模型能够有效表示更大类别的语言模型，具有有界堆栈和广义堆栈更新函数，类似于保留固定数量符号记忆并使用简单更新机制的自动机。

    

    最近Hewitt等人（2020）的工作提出了对循环神经网络（RNNs）作为语言模型（LMs）的经验成功可能性的一个解释。 它显示RNNs可以有效地表示在人类语言中普遍存在的有界分层结构。 这表明RNNs的成功可能与它们建模层次结构的能力有关。 然而，对Hewitt等人（2020）构造的更详细检查表明，它不限于分层LMs，这引出了RNNs可以有效表示哪些\emph{其他类型} LMs的问题。 为此，我们概括他们的构造以展示RNNs可以有效表示更大类别的LMs：可以通过带有有界堆栈和广义堆栈更新函数的下推自动机表示的那些。 这类似于一个保留固定数量符号记忆并使用简单更新机制更新记忆的自动机。

    arXiv:2402.15814v1 Announce Type: new  Abstract: Recent work by Hewitt et al. (2020) provides a possible interpretation of the empirical success of recurrent neural networks (RNNs) as language models (LMs).   It shows that RNNs can efficiently represent bounded hierarchical structures that are prevalent in human language.   This suggests that RNNs' success might be linked to their ability to model hierarchy.   However, a closer inspection of Hewitt et al.'s (2020) construction shows that it is not limited to hierarchical LMs, posing the question of what \emph{other classes} of LMs can be efficiently represented by RNNs.   To this end, we generalize their construction to show that RNNs can efficiently represent a larger class of LMs: Those that can be represented by a pushdown automaton with a bounded stack and a generalized stack update function.   This is analogous to an automaton that keeps a memory of a fixed number of symbols and updates the memory with a simple update mechanism.  
    
[^20]: 基于大型语言模型的检索增强生成的因果图发现

    Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models

    [https://arxiv.org/abs/2402.15301](https://arxiv.org/abs/2402.15301)

    利用大型语言模型和检索增强生成技术，提出了一种新方法用于从科学文献中推断因果关系，以解决传统方法受限于数据收集偏见和个体知识的问题。

    

    因果图恢复在因果推断领域至关重要。传统方法通常是基于知识或统计估计，受数据收集偏见和个体关于影响变量之间关系的知识的限制。大型语言模型（LLMs）的进步为解决这些问题提供了机会。我们提出了一种利用大量科学文献中所包含的知识推导一般因果图恢复任务中的因果关系的新方法。该方法利用基于检索增强生成（RAG）的LLMs系统地分析和提取来自广泛研究论文集的相关信息。我们的方法首先从汇总的文献中检索相关文本片段。然后，LLM被用来识别和标记因素之间的潜在关联。最后，我们给出了一个...

    arXiv:2402.15301v1 Announce Type: new  Abstract: Causal graph recovery is essential in the field of causal inference. Traditional methods are typically knowledge-based or statistical estimation-based, which are limited by data collection biases and individuals' knowledge about factors affecting the relations between variables of interests. The advance of large language models (LLMs) provides opportunities to address these problems. We propose a novel method that utilizes the extensive knowledge contained within a large corpus of scientific literature to deduce causal relationships in general causal graph recovery tasks. This method leverages Retrieval Augmented-Generation (RAG) based LLMs to systematically analyze and extract pertinent information from a comprehensive collection of research papers. Our method first retrieves relevant text chunks from the aggregated literature. Then, the LLM is tasked with identifying and labelling potential associations between factors. Finally, we giv
    
[^21]: 伪随机纠错码

    Pseudorandom Error-Correcting Codes

    [https://arxiv.org/abs/2402.09370](https://arxiv.org/abs/2402.09370)

    我们构建了一种伪随机纠错码，它对替换和删除错误具有鲁棒性，并且可以高效解码。我们还使用伪随机码提出了一种对语言模型输出进行水印处理的方案，该方案对裁剪和随机替换、删除具有恒定的鲁棒性。

    

    我们构建了伪随机纠错码（或简称为伪随机码），它们是具有以下特性的纠错码：对于任何计算受限的对手来说，任意多个编码词都是伪随机的。通过解码密钥，可以高效地纠正有错误的编码词。我们构建了对替换错误和删除错误具有强鲁棒性的伪随机码，其中伪随机性基于标准密码学假设。具体而言，伪随机性基于LPN问题的$2^{O(\sqrt{n})}$困难程度，或者基于LPN问题和低密度下的插入异或问题的多项式困难程度。

    arXiv:2402.09370v1 Announce Type: cross Abstract: We construct pseudorandom error-correcting codes (or simply pseudorandom codes), which are error-correcting codes with the property that any polynomial number of codewords are pseudorandom to any computationally-bounded adversary. Efficient decoding of corrupted codewords is possible with the help of a decoding key.   We build pseudorandom codes that are robust to substitution and deletion errors, where pseudorandomness rests on standard cryptographic assumptions. Specifically, pseudorandomness is based on either $2^{O(\sqrt{n})}$-hardness of LPN, or polynomial hardness of LPN and the planted XOR problem at low density.   As our primary application of pseudorandom codes, we present an undetectable watermarking scheme for outputs of language models that is robust to cropping and a constant rate of random substitutions and deletions. The watermark is undetectable in the sense that any number of samples of watermarked text are computationa
    
[^22]: Concept-1K：一种用于实例增量学习的新型基准

    Concept-1K: A Novel Benchmark for Instance Incremental Learning

    [https://arxiv.org/abs/2402.08526](https://arxiv.org/abs/2402.08526)

    我们提出了一种名为Concept-1K的具有挑战性的实例增量学习（IIL）场景和数据集，揭示了十亿参数的PLM仍然遭受灾难性遗忘，影响因素包括模型规模、预训练和缓冲区大小。现有的IL方法和LoRA技术无法满足性能需求。我们的研究为探索和缓解PLM中的遗忘问题提供了新的场景。

    

    增量学习（IL）对于实现神经网络中的人类级智能至关重要。然而，现有的IL场景和数据集无法评估PLM中的遗忘，使人误以为PLM不会遭受灾难性遗忘。为此，我们提出了一种具有挑战性的IL场景，称为实例增量学习（IIL），以及一个支持数量级更大的IL步骤的新数据集Concept-1K。基于对Concept-1K的实验，我们揭示了十亿参数的PLM仍然遭受着灾难性遗忘，并且遗忘受模型规模、预训练和缓冲区大小的影响。此外，现有的IL方法和一种流行的微调技术LoRA都未能达到令人满意的性能。我们的研究为未来研究提供了一个新的场景，探索PLM的灾难性遗忘，并鼓励设计更强大的技术以减轻PLM中的遗忘问题。

    Incremental learning (IL) is essential to realize the human-level intelligence in the neural network. However, existing IL scenarios and datasets are unqualified for assessing forgetting in PLMs, giving an illusion that PLMs do not suffer from catastrophic forgetting. To this end, we propose a challenging IL scenario called instance-incremental learning (IIL) and a novel dataset called Concept-1K, which supports an order of magnitude larger IL steps. Based on the experiments on Concept-1K, we reveal that billion-parameter PLMs still suffer from catastrophic forgetting, and the forgetting is affected by both model scale, pretraining, and buffer size. Furthermore, existing IL methods and a popular finetuning technique, LoRA, fail to achieve satisfactory performance. Our study provides a novel scenario for future studies to explore the catastrophic forgetting of PLMs and encourage more powerful techniques to be designed for alleviating the forgetting in PLMs. The data, code and scripts ar
    
[^23]: LoRA-drop：基于输出评估的高效LoRA参数剪枝

    LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation

    [https://arxiv.org/abs/2402.07721](https://arxiv.org/abs/2402.07721)

    本文提出了LoRA-drop方法，通过分析LoRA输出评估参数的重要性，并且保留重要层的LoRA，其余层共享相同参数。实验结果表明LoRA-drop有很好的效果。

    

    低秩适应（LoRA）为每个层引入辅助参数，以在有限的计算资源下微调预训练模型。但是，当扩展到更大的模型时，仍然面临资源消耗的挑战。先前的研究通过评估不同层的LoRA参数的重要性来采用剪枝技术来解决这个问题。然而，这些努力只分析了参数的特征以评估其重要性。事实上，与参数和数据相关的LoRA的输出是直接影响冻结模型的因素。为此，我们提出了LoRA-drop，通过分析LoRA输出来评估参数的重要性。我们保留重要层的LoRA，而其他层的LoRA共享相同的参数。在NLU和NLG任务上进行了充分的实验，证明了LoRA-drop的有效性。

    Low-Rank Adaptation (LoRA) introduces auxiliary parameters for each layer to fine-tune the pre-trained model under limited computing resources. But it still faces challenges of resource consumption when scaling up to larger models. Previous studies employ pruning techniques by evaluating the importance of LoRA parameters for different layers to address the problem. However, these efforts only analyzed parameter features to evaluate their importance. Indeed, the output of LoRA related to the parameters and data is the factor that directly impacts the frozen model. To this end, we propose LoRA-drop which evaluates the importance of the parameters by analyzing the LoRA output. We retain LoRA for important layers and the LoRA of the other layers share the same parameters. Abundant experiments on NLU and NLG tasks demonstrate the effectiveness of LoRA-drop.
    
[^24]: 关于无分布假设的差分隐私子空间估计

    On Differentially Private Subspace Estimation Without Distributional Assumptions

    [https://arxiv.org/abs/2402.06465](https://arxiv.org/abs/2402.06465)

    本论文研究了在没有分布假设的情况下，差分隐私子空间估计的问题。通过使用少量的数据点，可以私密地识别出低维结构，避免了高维度的代价。

    

    隐私数据分析面临着一个被称为维数诅咒的重大挑战，导致了成本的增加。然而，许多数据集具有固有的低维结构。例如，在梯度下降优化过程中，梯度经常位于一个低维子空间附近。如果可以使用少量点私密地识别出这种低维结构，就可以避免因高维度而支付隐私和准确性的代价。

    Private data analysis faces a significant challenge known as the curse of dimensionality, leading to increased costs. However, many datasets possess an inherent low-dimensional structure. For instance, during optimization via gradient descent, the gradients frequently reside near a low-dimensional subspace. If the low-dimensional structure could be privately identified using a small amount of points, we could avoid paying (in terms of privacy and accuracy) for the high ambient dimension.   On the negative side, Dwork, Talwar, Thakurta, and Zhang (STOC 2014) proved that privately estimating subspaces, in general, requires an amount of points that depends on the dimension. But Singhal and Steinke (NeurIPS 2021) bypassed this limitation by considering points that are i.i.d. samples from a Gaussian distribution whose covariance matrix has a certain eigenvalue gap. Yet, it was still left unclear whether we could provide similar upper bounds without distributional assumptions and whether we 
    
[^25]: 通过凸凹损失函数降低会员推断中的隐私风险

    Mitigating Privacy Risk in Membership Inference by Convex-Concave Loss

    [https://arxiv.org/abs/2402.05453](https://arxiv.org/abs/2402.05453)

    本论文提出了一种凸凹损失函数的方法，通过梯度下降实现训练损失的高方差，从而降低会员推断攻击中的隐私风险。

    

    机器学习模型容易受到会员推断攻击（MIAs），即推断样本是否在训练集中。现有工作利用梯度上升来增大训练数据的损失方差，缓解隐私风险。然而，向相反方向优化可能导致模型参数在局部最小值附近振荡，导致不稳定和次优性能。在本研究中，我们提出了一种新的方法——凸凹损失函数，在梯度下降的过程中实现了训练损失分布的高方差。我们的方法受到理论分析的启发，凸损失函数在训练过程中倾向于减少损失方差。因此，我们在CCL的背后的关键思想是通过凹函数项减小损失函数的凸性。使用CCL训练的神经网络产生训练数据的高方差损失，加强了对MIAs的防御。大量实验证实了CCL的卓越性能，实现了最新的平衡。

    Machine learning models are susceptible to membership inference attacks (MIAs), which aim to infer whether a sample is in the training set. Existing work utilizes gradient ascent to enlarge the loss variance of training data, alleviating the privacy risk. However, optimizing toward a reverse direction may cause the model parameters to oscillate near local minima, leading to instability and suboptimal performance. In this work, we propose a novel method -- Convex-Concave Loss, which enables a high variance of training loss distribution by gradient descent. Our method is motivated by the theoretical analysis that convex losses tend to decrease the loss variance during training. Thus, our key idea behind CCL is to reduce the convexity of loss functions with a concave term. Trained with CCL, neural networks produce losses with high variance for training data, reinforcing the defense against MIAs. Extensive experiments demonstrate the superiority of CCL, achieving state-of-the-art balance i
    
[^26]: 导航复杂性：通过扩展窗口匹配实现无损图谱精简

    Navigating Complexity: Toward Lossless Graph Condensation via Expanding Window Matching

    [https://arxiv.org/abs/2402.05011](https://arxiv.org/abs/2402.05011)

    本文通过连接先前被忽视的监督信号的方式，首次尝试实现无损图谱精简，以解决现有方法无法准确复制原始图谱的问题。

    

    图谱精简旨在通过合成紧凑的图谱来减少大规模图谱数据集的大小，同时不损失在其上训练的图神经网络（GNNs）的性能，这为减少训练GNNs的计算成本提供了启示。然而，现有方法往往无法准确复制某些数据集的原始图谱，从而未能实现无损精简的目标。为了理解这一现象，我们调查了潜在的原因，并揭示了先前最先进的轨迹匹配方法在优化精简图谱时提供了来自原始图谱的偏倚和受限的监督信号。这严重限制了精简图谱的规模和功效。在本文中，我们首次尝试通过连接先前被忽视的监督信号来实现无损图谱精简。

    Graph condensation aims to reduce the size of a large-scale graph dataset by synthesizing a compact counterpart without sacrificing the performance of Graph Neural Networks (GNNs) trained on it, which has shed light on reducing the computational cost for training GNNs. Nevertheless, existing methods often fall short of accurately replicating the original graph for certain datasets, thereby failing to achieve the objective of lossless condensation. To understand this phenomenon, we investigate the potential reasons and reveal that the previous state-of-the-art trajectory matching method provides biased and restricted supervision signals from the original graph when optimizing the condensed one. This significantly limits both the scale and efficacy of the condensed graph. In this paper, we make the first attempt toward \textit{lossless graph condensation} by bridging the previously neglected supervision signals. Specifically, we employ a curriculum learning strategy to train expert traje
    
[^27]: 在流数据上进行高效推理的在线级联学习

    Online Cascade Learning for Efficient Inference over Streams

    [https://arxiv.org/abs/2402.04513](https://arxiv.org/abs/2402.04513)

    这项研究提出了在线级联学习的方法，通过学习一个“级联”模型，从容量较低的模型到强大的语言模型，以及推迟策略，可以在流数据处理中同时保证准确性和降低推理成本。

    

    大型语言模型 (LLM) 在回答关于数据流的复杂查询方面具有天然的优势，但是 LLM 推理的高计算成本使得它们在许多任务中不可行。我们提出了在线级联学习，这是首个解决这一挑战的方法。这里的目标是学习一个“级联”模型，从容量较低的模型（如逻辑回归器）开始，到强大的 LLM 结束，并配备一个决定在给定输入上使用哪个模型的推迟策略。我们将在线学习级联的任务公式化为一个模仿学习问题，并为该问题提供了无遗憾算法。在四个基准测试中的实验结果显示，我们的方法在准确性上与 LLM 相当，同时将推理成本削减了多达 90%，突显了它在流处理中的效能和适应能力。

    Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks. We propose online cascade learning, the first approach to addressing this challenge. The objective here is to learn a "cascade" of models, starting with lower-capacity models (such as logistic regressors) and ending with a powerful LLM, along with a deferral policy that determines the model that is used on a given input. We formulate the task of learning cascades online as an imitation-learning problem and give a no-regret algorithm for the problem. Experimental results across four benchmarks show that our method parallels LLMs in accuracy while cutting down inference costs by as much as 90%, underscoring its efficacy and adaptability in stream processing.
    
[^28]: 检索以解释：基于语言模型的证据驱动预测

    Retrieve to Explain: Evidence-driven Predictions with Language Models

    [https://arxiv.org/abs/2402.04068](https://arxiv.org/abs/2402.04068)

    检索以解释（R2E）是一种基于语言模型的检索方法，通过使用Shapley值确定证据的相对重要性，从而在黑盒模型中提供了可解释性，通过应用于药物靶点鉴定任务中，R2E模型在预测临床试验结果方面优于传统基因学方法。

    

    机器学习模型，尤其是语言模型，往往难以深入分析。黑盒模型可能掩盖了模型训练中的问题和有害偏差。对于人机协作过程来说，不透明的预测可能导致缺乏信任，限制模型的影响，即使模型的性能很好。为了解决这些问题，我们引入了检索以解释（Retrieve to Explain，简称R2E）。R2E是一种基于检索的语言模型，根据文档语料库中的证据，使用Shapley值来确定证据对最终预测的相对重要性，并根据自然语言模板将结构化数据纳入其中。R2E能够在不重新训练的情况下适应新的证据，并且能够通过模板化将结构化数据纳入到自然语言中。我们在通过分析已发表的科学文献进行药物靶点鉴定的实际案例中进行了评估，结果显示该模型在预测临床试验结果方面优于行业标准的基因学方法。

    Machine learning models, particularly language models, are notoriously difficult to introspect. Black-box models can mask both issues in model training and harmful biases. For human-in-the-loop processes, opaque predictions can drive lack of trust, limiting a model's impact even when it performs effectively. To address these issues, we introduce Retrieve to Explain (R2E). R2E is a retrieval-based language model that prioritizes amongst a pre-defined set of possible answers to a research question based on the evidence in a document corpus, using Shapley values to identify the relative importance of pieces of evidence to the final prediction. R2E can adapt to new evidence without retraining, and incorporate structured data through templating into natural language. We assess on the use case of drug target identification from published scientific literature, where we show that the model outperforms an industry-standard genetics-based approach on predicting clinical trial outcomes.
    
[^29]: 安全微调几乎零成本：大规模语言模型视觉基线

    Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models

    [https://arxiv.org/abs/2402.02207](https://arxiv.org/abs/2402.02207)

    该论文研究了大规模语言模型在安全微调过程中存在的问题，并提出了一种可行的解决方案。他们首先策划了一个包含各种有害类别的视觉语言安全指令遵循数据集VLGuard，并通过实验证明将该数据集集成到视觉语言微调中或进行事后微调，可以有效地对齐VLLMs的安全性。这对模型的有益性几乎没有影响甚至有所提升。这项研究的贡献是提供了一个有价值的资源，用于对现有VLLMs进行安全测试、训练新模型或保护预训练VLLMs。

    

    当前的大规模语言模型（VLLMs）具有非凡的能力，但容易生成有害内容，并且容易受到简单越狱攻击的影响。我们的初步分析发现，这是由于在视觉语言指令微调过程中存在有害数据，并且VLLM的微调会导致对底层LLM之前学习的安全对齐性的遗忘。为了解决这个问题，我们首先策划了一个包含各种有害类别的视觉语言安全指令遵循数据集VLGuard。我们的实验表明，将该数据集集成到标准的视觉语言微调中或将其用于事后微调，可以有效地对齐VLLMs的安全性。这种对齐是在对模型的有益性几乎没有影响甚至有所提升的情况下实现的。我们安全微调数据集的多样性使其成为对现有VLLMs进行安全测试，训练新模型或保护预训练VLLMs的宝贵资源。

    Current vision large language models (VLLMs) exhibit remarkable capabilities yet are prone to generate harmful content and are vulnerable to even the simplest jailbreaking attacks. Our initial analysis finds that this is due to the presence of harmful data during vision-language instruction fine-tuning, and that VLLM fine-tuning can cause forgetting of safety alignment previously learned by the underpinning LLM. To address this issue, we first curate a vision-language safe instruction-following dataset VLGuard covering various harmful categories. Our experiments demonstrate that integrating this dataset into standard vision-language fine-tuning or utilizing it for post-hoc fine-tuning effectively safety aligns VLLMs. This alignment is achieved with minimal impact on, or even enhancement of, the models' helpfulness. The versatility of our safety fine-tuning dataset makes it a valuable resource for safety-testing existing VLLMs, training new models or safeguarding pre-trained VLLMs. Empi
    
[^30]: 正式-LLM：将形式语言和自然语言集成于可控的LLM智能体中

    Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents

    [https://arxiv.org/abs/2402.00798](https://arxiv.org/abs/2402.00798)

    本文提出了一种将自然语言和形式语言整合的“正式-LLM”框架，用于解决现有LLM智能体无法控制的计划生成问题。实验证明，该框架在提高生成计划性能和确保可控性方面取得了显著改进。

    

    最近，对于大型语言模型（LLMs）的进展使得人工智能智能体能够自动生成和执行解决复杂任务的多步计划。然而，由于LLM的内容生成过程几乎无法控制，当前的LLM智能体经常生成无效或不可执行的计划，这损害了生成计划的性能并破坏了用户对LLM智能体的信任。为应对这个问题，本文提出了一种新颖的“正式-LLM”框架，用于LLM智能体，通过将自然语言的表达力和形式语言的精确性进行整合。具体而言，该框架允许人类用户将他们对计划过程的要求或约束表达为自动机。然后，在自动机的监督下，使用基于堆栈的LLM计划生成过程来确保生成的计划满足约束条件，从而使计划过程可控。我们在基准任务和实际的真实任务上进行了实验，并且obtained significant improvements over existing LLM-based agents, demonstrating the effectiveness and controllability of the proposed Formal-LLM framework.

    Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and o
    
[^31]: Transformer的好处：在非结构化数据的线性回归任务中的上下文学习

    Benefits of Transformer: In-Context Learning in Linear Regression Tasks with Unstructured Data

    [https://arxiv.org/abs/2402.00743](https://arxiv.org/abs/2402.00743)

    本研究通过线性回归任务的实验研究了Transformer在非结构化数据中的上下文学习能力，并解释了其中的关键组件。

    

    实践中观察到，基于Transformer的模型在推理阶段能够学习上下文中的概念。现有的文献，例如\citet{zhang2023trained,huang2023context}对这种上下文学习能力提供了理论解释，但是他们假设每个样本的输入$x_i$和输出$y_i$都被嵌入到相同的令牌中（即结构化数据）。然而，在现实中，它们呈现为两个令牌（即非结构化数据\cite{wibisono2023role}）。在这种情况下，本文进行了线性回归任务的实验，研究了Transformer架构的好处，并提供了一些相应的理论直觉，解释了为什么Transformer可以从非结构化数据中学习。我们研究了在Transformer中起到上下文学习作用的确切组件。特别地，我们观察到（1）带有两层softmax（自我）注意力和前瞻性注意力掩码的Transformer可以从提示中学习，如果$y_i$在令牌中。

    In practice, it is observed that transformer-based models can learn concepts in context in the inference stage. While existing literature, e.g., \citet{zhang2023trained,huang2023context}, provide theoretical explanations on this in-context learning ability, they assume the input $x_i$ and the output $y_i$ for each sample are embedded in the same token (i.e., structured data). However, in reality, they are presented in two tokens (i.e., unstructured data \cite{wibisono2023role}). In this case, this paper conducts experiments in linear regression tasks to study the benefits of the architecture of transformers and provides some corresponding theoretical intuitions to explain why the transformer can learn from unstructured data. We study the exact components in a transformer that facilitate the in-context learning. In particular, we observe that (1) a transformer with two layers of softmax (self-)attentions with look-ahead attention mask can learn from the prompt if $y_i$ is in the token n
    
[^32]: 一次图卷积就够了：高效灰度图像分类

    A Single Graph Convolution Is All You Need: Efficient Grayscale Image Classification

    [https://arxiv.org/abs/2402.00564](https://arxiv.org/abs/2402.00564)

    本论文提出了一种高效的灰度图像分类方法，通过将图像视为矢量，并使用单个图卷积层进行处理，提高了分类模型的准确性和稳定性。

    

    图像分类器通常依赖于卷积神经网络(CNN)来完成任务，而CNN相比于多层感知机(MLP)更加庞大，这在实时应用中可能会带来问题。此外，许多图像分类模型适用于RGB和灰度数据集，但仅仅使用灰度图像的分类器相对较少见。灰度图像分类具有广泛的应用，包括但不限于医学图像分类和合成孔径雷达(SAR)自动目标识别(ATR)。因此，我们提出了一种使用图像的矢量化视图的新型灰度(单通道)图像分类方法。我们通过将图像视为矢量，并将问题设置为灰度图像分类问题，充分利用了MLP的轻量级特性。我们发现，批次级别使用单个图卷积层可以提高模型的准确性并减小性能的差异。此外，我们开发了定制的准确率估计方法。

    Image classifiers often rely on convolutional neural networks (CNN) for their tasks, which are inherently more heavyweight than multilayer perceptrons (MLPs), which can be problematic in real-time applications. Additionally, many image classification models work on both RGB and grayscale datasets. Classifiers that operate solely on grayscale images are much less common. Grayscale image classification has diverse applications, including but not limited to medical image classification and synthetic aperture radar (SAR) automatic target recognition (ATR). Thus, we present a novel grayscale (single channel) image classification approach using a vectorized view of images. We exploit the lightweightness of MLPs by viewing images as a vector and reducing our problem setting to the grayscale image classification setting. We find that using a single graph convolutional layer batch-wise increases accuracy and reduces variance in the performance of our model. Moreover, we develop a customized acc
    
[^33]: 位置编码有助于循环神经网络处理大词汇量

    Positional Encoding Helps Recurrent Neural Networks Handle a Large Vocabulary

    [https://arxiv.org/abs/2402.00236](https://arxiv.org/abs/2402.00236)

    本研究探讨了位置编码在循环神经网络中的作用，发现即使与RNN结合使用，位置编码仍然有效，尤其是在处理大词汇量和多样观察结果时。这为使用输入驱动和自主时间表示的组合研究提供了新的方向，同时研究结果也对神经元振荡的生物学意义提供了讨论。

    

    本研究讨论了位置编码在利用合成基准测试的循环神经网络（RNN）中的影响。位置编码将时间序列中的数据点“时间戳化”，并补充了Transformer神经网络的能力，后者缺乏表示数据顺序的内在机制。相反，RNN可以自己对数据点进行时间编码，使得它们对位置编码的使用似乎是“冗余”的。然而，经验研究表明，即使与RNN结合使用，位置编码的有效性仍然很高，特别是用于处理产生多样观察结果的大词汇量。这些发现为循环神经网络上的新的研究方向铺平了道路，涉及输入驱动和自主时间表示的组合。此外，本研究还讨论了计算/模拟结果的生物学意义，考虑到位置编码的正弦实现与神经元振荡之间的关联。

    This study discusses the effects of positional encoding on recurrent neural networks (RNNs) utilizing synthetic benchmarks. Positional encoding "time-stamps" data points in time series and complements the capabilities of Transformer neural networks, which lack an inherent mechanism for representing the data order. By contrast, RNNs can encode the temporal information of data points on their own, rendering their use of positional encoding seemingly "redundant". Nonetheless, empirical investigations reveal the effectiveness of positional encoding even when coupled with RNNs, specifically for handling a large vocabulary that yields diverse observations. These findings pave the way for a new line of research on RNNs, concerning the combination of input-driven and autonomous time representation. Additionally, biological implications of the computational/simulational results are discussed, in the light of the affinity between the sinusoidal implementation of positional encoding and neural os
    
[^34]: 透过校准的视角理解不变风险最小化的变体

    Towards Understanding Variants of Invariant Risk Minimization through the Lens of Calibration

    [https://arxiv.org/abs/2401.17541](https://arxiv.org/abs/2401.17541)

    本研究通过比较分析使用不同近似方法的不变风险最小化（IRM）技术，以期望校准误差（ECE）作为关键指标，观察到基于信息瓶颈的IRM在压缩关键特征上表现最佳。

    

    传统的机器学习模型假设训练和测试数据是独立且同分布的。然而，在现实世界的应用中，测试分布往往与训练不同。这个问题被称为越域泛化，在常规模型面临挑战。不变风险最小化（IRM）作为一个解决方案出现，旨在识别在不同环境中保持不变的特征，以增强越域鲁棒性。然而，IRM的复杂性，特别是其双层优化，导致了各种近似方法的开发。我们的研究调查了这些近似IRM技术，使用期望校准误差（ECE）作为关键指标。ECE可以衡量模型预测的可靠性，它是衡量模型是否有效捕捉到环境不变特征的指标。通过对具有分布变化的数据集进行比较分析，我们观察到基于信息瓶颈的IRM在压缩了...（接下部分摘要超过200字，提取前200字）

    Machine learning models traditionally assume that training and test data are independently and identically distributed. However, in real-world applications, the test distribution often differs from training. This problem, known as out-of-distribution generalization, challenges conventional models. Invariant Risk Minimization (IRM) emerges as a solution, aiming to identify features invariant across different environments to enhance out-of-distribution robustness. However, IRM's complexity, particularly its bi-level optimization, has led to the development of various approximate methods. Our study investigates these approximate IRM techniques, employing the Expected Calibration Error (ECE) as a key metric. ECE, which measures the reliability of model prediction, serves as an indicator of whether models effectively capture environment-invariant features. Through a comparative analysis of datasets with distributional shifts, we observe that Information Bottleneck-based IRM, which condenses
    
[^35]: 图神经网络是如何学习的：来自训练动态的启示

    How Graph Neural Networks Learn: Lessons from Training Dynamics

    [https://arxiv.org/abs/2310.05105](https://arxiv.org/abs/2310.05105)

    图神经网络的优化过程中涉及核-图对齐现象，从优化角度解释了学到的函数何时和为何泛化，有助于理解其在异源图上的限制。

    

    在深度学习中，一个长期以来的目标是以更易解释的方式表征黑盒模型的学习行为。对于图神经网络（GNNs），在正式化它们可以表示的函数方面已经取得了相当大的进展，但在优化过程中GNNs是否会学习到期望的函数仍不太清楚。为了填补这一空白，我们研究了它们在函数空间中的训练动态。特别是，我们发现通过梯度下降优化GNNs隐式利用图结构来更新学到的函数。这种现象被称为核-图对齐，已经经验性和理论上得到了验证。这种来自优化角度的新分析框架能够解释了何时以及为什么学习到的GNN函数泛化，这对于它们在异源图上的限制具有相关性。从实用的角度看，它也提供了对于GNNs如何学习函数的洞察。

    arXiv:2310.05105v2 Announce Type: replace  Abstract: A long-standing goal in deep learning has been to characterize the learning behavior of black-box models in a more interpretable manner. For graph neural networks (GNNs), considerable advances have been made in formalizing what functions they can represent, but whether GNNs will learn desired functions during the optimization process remains less clear. To fill this gap, we study their training dynamics in function space. In particular, we find that the optimization of GNNs through gradient descent implicitly leverages the graph structure to update the learned function. This phenomenon is dubbed as kernel-graph alignment, which has been empirically and theoretically corroborated. This new analytical framework from the optimization perspective enables interpretable explanations of when and why the learned GNN functions generalize, which are relevant to their limitations on heterophilic graphs. From a practical standpoint, it also prov
    
[^36]: 利用深度学习对浸软纤维和导管进行分割和特征提取

    Segmentation and Characterization of Macerated Fibers and Vessels Using Deep Learning. (arXiv:2401.16937v1 [cs.CV])

    [http://arxiv.org/abs/2401.16937](http://arxiv.org/abs/2401.16937)

    这项工作开发了一种利用深度学习进行浸软纤维和导管分割和特征提取的自动方法，并且在显微镜图像中取得了快速而准确的分割效果。

    

    目的：木材由纤维和导管等不同种类的细胞组成，这些细胞的形状、大小和排列对于理解木材样本至关重要。通常，这涉及将样本浸泡在溶液中以分离细胞，然后将它们分散在载玻片上，用显微镜进行广域成像，捕捉数千个细胞。然而，这些细胞在图像中经常聚集和重叠，使用标准图像处理方法进行分割变得困难且耗时。结果：在这项工作中，我们开发了一种自动深度学习分割方法，利用一阶YOLOv8模型来快速而准确地对显微镜图像中的纤维和导管进行分割和特征提取。该模型可以分析32640 x 25920像素的图像，并展示出有效的细胞检测和分割，达到78%的mAP_0.5-0.95。为了评估模型的鲁棒性，我们对经过基因改造的纤维进行了研究。

    Purpose: Wood comprises different cell types, such as fibers and vessels, defining its properties. Studying their shape, size, and arrangement in microscopic images is crucial for understanding wood samples. Typically, this involves macerating (soaking) samples in a solution to separate cells, then spreading them on slides for imaging with a microscope that covers a wide area, capturing thousands of cells. However, these cells often cluster and overlap in images, making the segmentation difficult and time-consuming using standard image-processing methods. Results: In this work, we develop an automatic deep learning segmentation approach that utilizes the one-stage YOLOv8 model for fast and accurate fiber and vessel segmentation and characterization in microscopy images. The model can analyze 32640 x 25920 pixels images and demonstrate effective cell detection and segmentation, achieving a mAP_0.5-0.95 of 78 %. To assess the model's robustness, we examined fibers from a genetically modi
    
[^37]: 事件序列的自我监督学习：生成建模和对比学习的比较研究和混合方法的应用

    Self-Supervised Learning in Event Sequences: A Comparative Study and Hybrid Approach of Generative Modeling and Contrastive Learning. (arXiv:2401.15935v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.15935](http://arxiv.org/abs/2401.15935)

    本研究通过比较研究和混合方法，调查了事件序列的自我监督学习技术，并引入了一种新的方法，将生成模型和对比嵌入进行对齐。结果显示，这种对齐模型在各种任务上表现优越，为预测事件序列中的信息提供了潜在的好处。

    

    本研究调查了获取事件序列表示的自我监督学习技术。这是各种应用中的关键模态，包括但不限于银行、电子商务和医疗保健。我们对自我监督学习中的生成模型和对比方法进行了全面的研究，并分别应用了它们。我们发现没有一种绝对优越的方法。因此，我们探讨了结合这些方法的潜在好处。为了实现这个目标，我们引入了一种新的方法，将生成模型和对比嵌入作为不同的模态进行对齐，从当代多模态研究中汲取灵感。生成模型和对比方法通常被视为互斥的，因此存在它们的联合探索的空白。我们的结果表明，这种对齐模型在至少与现有方法持平，并且在各种任务上更加普适。此外，我们证明了自我监督学习在预测事件序列中包含的信息方面的潜力。

    This study investigates self-supervised learning techniques to obtain representations of Event Sequences. It is a key modality in various applications, including but not limited to banking, e-commerce, and healthcare.  We perform a comprehensive study of generative and contrastive approaches in self-supervised learning, applying them both independently. We find that there is no single supreme method. Consequently, we explore the potential benefits of combining these approaches. To achieve this goal, we introduce a novel method that aligns generative and contrastive embeddings as distinct modalities, drawing inspiration from contemporary multimodal research.  Generative and contrastive approaches are often treated as mutually exclusive, leaving a gap for their combined exploration. Our results demonstrate that this aligned model performs at least on par with, and mostly surpasses, existing methods and is more universal across a variety of tasks. Furthermore, we demonstrate that self-sup
    
[^38]: LLM指令微调中的提示权重实验

    Prompt Weight Experiments for LLM Instruction Fine-Tuning. (arXiv:2401.13586v1 [cs.LG])

    [http://arxiv.org/abs/2401.13586](http://arxiv.org/abs/2401.13586)

    LLM指令微调中，对于短提示完成数据集，提示词标记分类损失加权（PLW）与性能呈负二次关系，而长提示完成数据集则不受PLW影响。

    

    我们进行了一项小型研究，分析了提示词标记分类损失加权（PLW）如何影响在指令任务上进行微调的7B大小的LLaMA模型的性能。我们使用多个指令数据集重现了斯坦福大学的Alpaca实验，其中包括LLaMA 1和LLaMA 2。我们发现，在我们的短提示完成数据集上微调的模型与PLW之间存在负二次关系，而在长提示完成数据集上微调的模型不受PLW的影响。

    We present a small study analyzing how prompt token classification loss weighting (PLW) affects the performance of 7B-size LLaMA models fine-tuned on instruction tasks. We recreated Stanford's Alpaca experiment with both LLaMA 1 and LLaMA 2 using multiple instruction datasets. We found that models fine-tuned on our short-completion dataset have a negative quadratic relationship with PLW while models fine-tuned on long-completion datasets were unaffected by PLW.
    
[^39]: 自监督学习中的记忆化提高了下游概括能力

    Memorization in Self-Supervised Learning Improves Downstream Generalization. (arXiv:2401.12233v1 [cs.LG])

    [http://arxiv.org/abs/2401.12233](http://arxiv.org/abs/2401.12233)

    自监督学习中的记忆化问题一直是一个挑战，本文提出了SSLMem框架，用于定义自监督学习中的记忆化，并通过实证分析证明了在大规模数据集和强数据增强的情况下，记忆化仍然存在。

    

    自监督学习（SSL）最近因其在无标签数据上训练高性能编码器的能力而受到重视，这些数据通常来源于互联网的抓取。然而，经验证据表明，SSL编码器会记忆其训练数据的私人信息，并在推理时泄露这些信息。现有的监督学习记忆化的理论定义依赖于标签，因此无法适用于SSL。为了填补这一空白，我们提出了SSLMem，一个在SSL内定义记忆化的框架。我们的定义通过比较训练在这些数据点上的编码器和未被训练在这些数据点上的编码器返回的数据点和他们的增强视图的表示的对齐差异。通过对不同编码器架构和数据集的综合实证分析，我们强调了即使SSL依赖于大型数据集和强大的数据增强，这都是监督学习中作为正则化手段的已知技术，记忆化仍然存在。

    Self-supervised learning (SSL) has recently received significant attention due to its ability to train high-performance encoders purely on unlabeled data-often scraped from the internet. This data can still be sensitive and empirical evidence suggests that SSL encoders memorize private information of their training data and can disclose them at inference time. Since existing theoretical definitions of memorization from supervised learning rely on labels, they do not transfer to SSL. To address this gap, we propose SSLMem, a framework for defining memorization within SSL. Our definition compares the difference in alignment of representations for data points and their augmented views returned by both encoders that were trained on these data points and encoders that were not. Through comprehensive empirical analysis on diverse encoder architectures and datasets we highlight that even though SSL relies on large datasets and strong augmentations-both known in supervised learning as regulari
    
[^40]: 空间-时间大语言模型用于交通预测

    Spatial-Temporal Large Language Model for Traffic Prediction. (arXiv:2401.10134v1 [cs.LG])

    [http://arxiv.org/abs/2401.10134](http://arxiv.org/abs/2401.10134)

    本文提出了一种空间-时间大语言模型（ST-LLM）用于交通预测，通过参数扩展和预训练来提高预测准确性，并利用空间-时间嵌入模块学习标记的空间位置和全局时间表示。

    

    交通预测是智能交通系统的关键组成部分，它通过使用历史数据来预测特定位置的未来交通情况。尽管现有的交通预测模型通常强调开发复杂的神经网络结构，但它们的准确性并未相应提高。最近，大型语言模型（LLMs）在时间序列分析方面显示出了出色的能力。与现有模型不同，LLMs主要通过参数扩展和广泛的预训练来进步，同时保持其基本结构。本文提出了一种空间-时间大语言模型（ST-LLM）用于交通预测。具体而言，ST-LLM将每个位置的时间步长定义为标记，并结合空间-时间嵌入模块来学习标记的空间位置和全局时间表示。然后，这些表示被融合以为每个标记提供统一的空间和时间信息。

    Traffic prediction, a critical component for intelligent transportation systems, endeavors to foresee future traffic at specific locations using historical data. Although existing traffic prediction models often emphasize developing complex neural network structures, their accuracy has not seen improvements accordingly. Recently, Large Language Models (LLMs) have shown outstanding capabilities in time series analysis. Differing from existing models, LLMs progress mainly through parameter expansion and extensive pre-training while maintaining their fundamental structures. In this paper, we propose a Spatial-Temporal Large Language Model (ST-LLM) for traffic prediction. Specifically, ST-LLM redefines the timesteps at each location as tokens and incorporates a spatial-temporal embedding module to learn the spatial location and global temporal representations of tokens. Then these representations are fused to provide each token with unified spatial and temporal information. Furthermore, we
    
[^41]: 临床试验结果预测中的不确定性量化

    Uncertainty Quantification on Clinical Trial Outcome Prediction. (arXiv:2401.03482v1 [cs.LG])

    [http://arxiv.org/abs/2401.03482](http://arxiv.org/abs/2401.03482)

    本研究将不确定性量化方法应用于临床试验结果预测，提高模型对微妙差异的识别能力，从而改善其整体性能。

    

    不确定性量化在机器学习的不同领域中的重要性日益被认识到。准确评估模型预测的不确定性可以帮助研究人员和从业人员更深入地理解和增加信心。这在医学诊断和药物发现领域尤为重要，因为可靠的预测直接影响研究质量和患者健康。本文提出将不确定性量化纳入临床试验结果预测中。我们的主要目标是提高模型辨别微妙差异的能力，从而显著改善其整体性能。我们采用了一种选择性分类方法来实现我们的目标，并将其与层次交互网络(HINT)无缝集成，HINT是临床试验预测建模的最前沿。选择性分类涵盖了一系列不确定性量化方法，使模型能够保留信息以供进一步分析。

    The importance of uncertainty quantification is increasingly recognized in the diverse field of machine learning. Accurately assessing model prediction uncertainty can help provide deeper understanding and confidence for researchers and practitioners. This is especially critical in medical diagnosis and drug discovery areas, where reliable predictions directly impact research quality and patient health.  In this paper, we proposed incorporating uncertainty quantification into clinical trial outcome predictions. Our main goal is to enhance the model's ability to discern nuanced differences, thereby significantly improving its overall performance.  We have adopted a selective classification approach to fulfill our objective, integrating it seamlessly with the Hierarchical Interaction Network (HINT), which is at the forefront of clinical trial prediction modeling. Selective classification, encompassing a spectrum of methods for uncertainty quantification, empowers the model to withhold de
    
[^42]: 隐私保护的神经图数据库

    Privacy-Preserving Neural Graph Databases. (arXiv:2312.15591v2 [cs.DB] UPDATED)

    [http://arxiv.org/abs/2312.15591](http://arxiv.org/abs/2312.15591)

    隐私保护的神经图数据库结合了图数据库和神经网络的优势，能够高效存储、检索和分析图结构数据。然而，这种能力也带来了潜在的隐私风险。

    

    在大数据和快速发展的信息系统时代，高效准确地检索数据变得日益重要。神经图数据库（NGDB）是一种强大的范式，将图数据库（图形数据库）和神经网络的优势相结合，实现了对图结构数据的高效存储、检索和分析。神经嵌入存储和复杂神经逻辑查询回答为NGDB提供了泛化能力。当图形不完整时，神经图数据库可以通过提取潜在模式和表示来填补图结构中的空缺，揭示隐藏的关系并实现准确的查询回答。然而，这种能力也带来了潜在的隐私风险，因为恶意攻击者可以使用精心设计的组合查询推断出更多敏感信息，例如通过比较图数据库中Turing奖得主的答案集。

    In the era of big data and rapidly evolving information systems, efficient and accurate data retrieval has become increasingly crucial. Neural graph databases (NGDBs) have emerged as a powerful paradigm that combines the strengths of graph databases (graph DBs) and neural networks to enable efficient storage, retrieval, and analysis of graph-structured data. The usage of neural embedding storage and complex neural logical query answering provides NGDBs with generalization ability. When the graph is incomplete, by extracting latent patterns and representations, neural graph databases can fill gaps in the graph structure, revealing hidden relationships and enabling accurate query answering. Nevertheless, this capability comes with inherent trade-offs, as it introduces additional privacy risks to the database. Malicious attackers can infer more sensitive information in the database using well-designed combinatorial queries, such as by comparing the answer sets of where Turing Award winner
    
[^43]: 利用反射红外光波信号进行呼吸异常检测

    Respiratory Anomaly Detection using Reflected Infrared Light-wave Signals. (arXiv:2311.01367v1 [eess.SP])

    [http://arxiv.org/abs/2311.01367](http://arxiv.org/abs/2311.01367)

    利用反射红外光波信号，开发了一种利用低成本光源和传感器进行非接触呼吸异常检测的方法，实现了96.6%的平均准确率，并能够检测到错误数据。

    

    在这项研究中，我们提出了一种利用机械机器人胸部反射的非相干光波信号进行非接触呼吸异常检测的方法。与现有的雷达和摄像头感应系统相比，这项技术只使用了低成本的普遍光源（如红外发光二极管）和传感器（如光电检测器）。这个光波感应（LWS）系统通过测量机器人胸部反射的光强变化来识别不同的呼吸异常，在0.5米至1.5米范围内。该异常检测模型使用机器学习能够以96.6%的平均准确率分类7种不同类型的呼吸数据。该模型还可以检测到系统收集的不包含呼吸信息的错误数据。开发的系统可以作为智能、非接触和隐蔽的呼吸监测方法，在家庭或医疗机构中使用。

    In this study, we present a non-contact respiratory anomaly detection method using incoherent light-wave signals reflected from the chest of a mechanical robot that can breathe like human beings. In comparison to existing radar and camera-based sensing systems for vitals monitoring, this technology uses only a low-cost ubiquitous light source (e.g., infrared light emitting diode) and sensor (e.g., photodetector). This light-wave sensing (LWS) system recognizes different breathing anomalies from the variations of light intensity reflected from the chest of the robot within a 0.5m-1.5m range. The anomaly detection model demonstrates up to 96.6% average accuracy in classifying 7 different types of breathing data using machine learning. The model can also detect faulty data collected by the system that does not contain breathing information. The developed system can be utilized at home or healthcare facilities as a smart, non-contact and discreet respiration monitoring method.
    
[^44]: 大脑基因转录的压缩表示

    Compressed representation of brain genetic transcription. (arXiv:2310.16113v1 [cs.LG])

    [http://arxiv.org/abs/2310.16113](http://arxiv.org/abs/2310.16113)

    本文研究了大脑基因转录的压缩表示方法，通过比较不同的线性和非线性方法，评估了它们在重建、解剖和预测方面的性能。

    

    大脑的结构过于复杂，无法直观地进行观察，需要使用压缩表示将其变化投影到紧凑、可导航的空间中。在高维数据（如基因表达）中，尤其具有挑战性，其中解剖和转录模式的联合复杂性要求最大压缩。目前的实践是使用标准的主成分分析（PCA），其计算效率受到限制，尤其在大压缩比下表现力有限。本研究利用全脑体素级Allen大脑图谱转录数据，系统比较了基于最广泛支持的线性和非线性方法（PCA，核PCA，非负矩阵分解（NMF），t-随机邻居嵌入（t-SNE），统一流形逼近和投影（UMAP），深度自编码）的压缩表示，量化重建保真度，解剖连贯性和预测效果。

    The architecture of the brain is too complex to be intuitively surveyable without the use of compressed representations that project its variation into a compact, navigable space. The task is especially challenging with high-dimensional data, such as gene expression, where the joint complexity of anatomical and transcriptional patterns demands maximum compression. Established practice is to use standard principal component analysis (PCA), whose computational felicity is offset by limited expressivity, especially at great compression ratios. Employing whole-brain, voxel-wise Allen Brain Atlas transcription data, here we systematically compare compressed representations based on the most widely supported linear and non-linear methods-PCA, kernel PCA, non-negative matrix factorization (NMF), t-stochastic neighbour embedding (t-SNE), uniform manifold approximation and projection (UMAP), and deep auto-encoding-quantifying reconstruction fidelity, anatomical coherence, and predictive utility
    
[^45]: 语言模型作为零-shot轨迹生成器

    Language Models as Zero-Shot Trajectory Generators. (arXiv:2310.11604v1 [cs.RO])

    [http://arxiv.org/abs/2310.11604](http://arxiv.org/abs/2310.11604)

    本文研究了使用大型语言模型（LLMs）作为零-shot轨迹生成器的可能性。通过给予LLM物体检测和分割视觉模型的访问权限，研究人员发现LLMs能够直接预测操作技能中的末端执行器姿态序列，并在26个真实世界的语言任务中取得了良好效果。这一研究突破了对LLMs在机器人技术中的限制，揭示了LLMs确实具有对操作任务的理解能力。

    

    近期研究表明，大型语言模型（LLMs）在给予低级技能选择时能够作为机器人的高级规划器。然而，通常认为LLMs不具备足够的知识来用于低级轨迹生成。在本研究中，我们详细探讨了这种假设，并调查了当给予LLM（GPT-4）仅能访问物体检测和分割视觉模型时，它能否直接预测一系列密集的末端执行器姿态用于操作技能。我们研究了一个单一的任务不可知提示，没有任何上下文示例、运动原语或外部轨迹优化器，它在26个真实世界的基于语言的任务中的表现，如“打开瓶盖”和“用海绵擦拭盘子”，以及我们调查了这个提示中哪些设计选择最有效。我们的结论突破了对LLMs在机器人技术上的限制，并首次揭示了LLMs确实具有对操作任务的理解能力。

    Large Language Models (LLMs) have recently shown promise as high-level planners for robots when given access to a selection of low-level skills. However, it is often assumed that LLMs do not possess sufficient knowledge to be used for the low-level trajectories themselves. In this work, we address this assumption thoroughly, and investigate if an LLM (GPT-4) can directly predict a dense sequence of end-effector poses for manipulation skills, when given access to only object detection and segmentation vision models. We study how well a single task-agnostic prompt, without any in-context examples, motion primitives, or external trajectory optimisers, can perform across 26 real-world language-based tasks, such as "open the bottle cap" and "wipe the plate with the sponge", and we investigate which design choices in this prompt are the most effective. Our conclusions raise the assumed limit of LLMs for robotics, and we reveal for the first time that LLMs do indeed possess an understanding o
    
[^46]: 大型语言模型能够进行零-shot时间序列预测

    Large Language Models Are Zero-Shot Time Series Forecasters. (arXiv:2310.07820v1 [cs.LG])

    [http://arxiv.org/abs/2310.07820](http://arxiv.org/abs/2310.07820)

    大型语言模型（LLMs）如GPT-3和LLaMA-2能够令人惊讶地零-shot外推时间序列，其性能可与或超过专门设计的时间序列模型的性能相媲美。这是因为LLMs具有自然地表示多模态分布的能力，并且具有与许多时间序列的重复季节趋势特征相一致的简单性和重复性偏好。

    

    通过将时间序列编码为一系列数字，我们可以将时间序列预测视为文本中的下一个标记预测。在开发这种方法时，我们发现大型语言模型（LLMs）例如GPT-3和LLaMA-2可以令人惊讶地零-shot外推时间序列，其性能可与或超过针对下游任务训练的专门设计的时间序列模型的性能相媲美。为了促进这种性能，我们提出了有效标记化时间序列数据并将离散分布转换为高度灵活的连续值密度的方法。我们认为，LLMs在时间序列中的成功源于它们自然地表示多模态分布的能力，结合了简单性和重复性的偏见，这与许多时间序列的重复季节趋势等显著特征相一致。我们还展示了LLMs如何通过非数字文本处理缺失数据，以及如何适应文本附加信息。

    By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side in
    
[^47]: 在概率测度空间中加速优化

    Accelerating optimization over the space of probability measures. (arXiv:2310.04006v1 [math.OC])

    [http://arxiv.org/abs/2310.04006](http://arxiv.org/abs/2310.04006)

    本研究研究了在概率测度空间中加速优化的问题，提出了一种类似于欧几里得空间中基于矩方法的哈密顿流方法，并证明了其可以达到任意高阶的收敛速度。

    

    梯度优化方法的加速是一个非常实用和理论上有意义的问题，特别是在机器学习应用中。大多数研究都集中在欧几里得空间的优化上，但考虑到在许多机器学习问题中需要在概率测度空间上进行优化，研究这种情况下的加速梯度方法是很有意义的。为此，我们引入了一种类似于欧几里得空间中基于矩方法的哈密顿流方法。我们证明了基于这种方法的算法可以达到任意高阶的收敛速度。数值实例证明了我们的论断。

    Acceleration of gradient-based optimization methods is an issue of significant practical and theoretical interest, particularly in machine learning applications. Most research has focused on optimization over Euclidean spaces, but given the need to optimize over spaces of probability measures in many machine learning problems, it is of interest to investigate accelerated gradient methods in this context too. To this end, we introduce a Hamiltonian-flow approach that is analogous to moment-based approaches in Euclidean space. We demonstrate that algorithms based on this approach can achieve convergence rates of arbitrarily high order. Numerical examples illustrate our claim.
    
[^48]: Segment Anything Model对本地特征学习具有良好的教导作用

    Segment Anything Model is a Good Teacher for Local Feature Learning. (arXiv:2309.16992v1 [cs.CV])

    [http://arxiv.org/abs/2309.16992](http://arxiv.org/abs/2309.16992)

    本文提出了使用Segment Anything Model (SAM)作为教师来指导本地特征学习，通过像素语义关系蒸馏和弱监督对比学习两种技术，实现了在有限数据集上的更高性能表现。

    

    本地特征的检测和描述在许多计算机视觉任务中起着重要作用，旨在检测和描述“任何场景”和“任何下游任务”的关键点。数据驱动的本地特征学习方法需要依赖于像素级一致性进行训练，这在大规模获得方面具有挑战性，从而阻碍了进一步的性能提升。在本文中，我们提出了SAMFeat来引入SAM（segment anything model）作为教师来指导本地特征学习，从而在有限的数据集上激发更高的性能。为此，首先，我们构建了一个像素语义关系蒸馏（PSRD）的辅助任务，将SAM编码器学习到的类别不可知的语义信息通过特征关系蒸馏到本地特征学习网络中，以提高通过语义区分改善本地特征描述的能力。其次，我们开发了一种称为弱监督对比学习的技术

    Local feature detection and description play an important role in many computer vision tasks, which are designed to detect and describe keypoints in "any scene" and "any downstream task". Data-driven local feature learning methods need to rely on pixel-level correspondence for training, which is challenging to acquire at scale, thus hindering further improvements in performance. In this paper, we propose SAMFeat to introduce SAM (segment anything model), a fundamental model trained on 11 million images, as a teacher to guide local feature learning and thus inspire higher performance on limited datasets. To do so, first, we construct an auxiliary task of Pixel Semantic Relational Distillation (PSRD), which distillates feature relations with category-agnostic semantic information learned by the SAM encoder into a local feature learning network, to improve local feature description using semantic discrimination. Second, we develop a technique called Weakly Supervised Contrastive Learning 
    
[^49]: 两层神经网络全局最小值附近的结构和梯度动力学

    Structure and Gradient Dynamics Near Global Minima of Two-layer Neural Networks. (arXiv:2309.00508v1 [cs.LG])

    [http://arxiv.org/abs/2309.00508](http://arxiv.org/abs/2309.00508)

    本论文通过分析两层神经网络在全局最小值附近的结构和梯度动力学，揭示了其泛化能力较强的原因。

    

    在温和的假设下，我们研究了两层神经网络在全局最小值附近的损失函数表面的结构，确定了能够实现完美泛化的参数集，并完整描述了其周围的梯度流动态。通过新颖的技术，我们揭示了复杂的损失函数表面的一些简单方面，并揭示了模型、目标函数、样本和初始化对训练动力学的不同影响。基于这些结果，我们还解释了为什么（过度参数化的）神经网络可以很好地泛化。

    Under mild assumptions, we investigate the structure of loss landscape of two-layer neural networks near global minima, determine the set of parameters which give perfect generalization, and fully characterize the gradient flows around it. With novel techniques, our work uncovers some simple aspects of the complicated loss landscape and reveals how model, target function, samples and initialization affect the training dynamics differently. Based on these results, we also explain why (overparametrized) neural networks could generalize well.
    
[^50]: 无需特征间隔的聚类方法

    Clustering Without an Eigengap. (arXiv:2308.15642v1 [cs.LG])

    [http://arxiv.org/abs/2308.15642](http://arxiv.org/abs/2308.15642)

    这个论文介绍了在随机块模型中进行图聚类的新算法，能够恢复大聚类，无论其他聚类的大小，并且对中等大小的聚类提出了新的技术挑战。

    

    我们在随机块模型（SBM）中研究了具有大聚类和小不可恢复聚类的图聚类问题。之前的方法要么不允许小于$ o（\sqrt {n}）$大小的小聚类，要么要求最小可恢复聚类和最大不可恢复聚类之间存在大小间隔。我们提供了一个基于半定规划（SDP）的算法，它消除了这些要求，并可以确定地恢复大聚类，而不考虑其他聚类的大小。中等大小的聚类对分析提出了独特的挑战，因为它们接近恢复阈值，非常敏感于小的噪声扰动，不允许闭合形式的候选解决方案。我们开发了新颖的技术，包括leave-one-out风格的论证，即使去掉一行噪声也可能大幅改变SDP解决方案，仍然可以控制SDP解决方案与噪声向量之间的相关性。

    We study graph clustering in the Stochastic Block Model (SBM) in the presence of both large clusters and small, unrecoverable clusters. Previous approaches achieving exact recovery do not allow any small clusters of size $o(\sqrt{n})$, or require a size gap between the smallest recovered cluster and the largest non-recovered cluster. We provide an algorithm based on semidefinite programming (SDP) which removes these requirements and provably recovers large clusters regardless of the remaining cluster sizes. Mid-sized clusters pose unique challenges to the analysis, since their proximity to the recovery threshold makes them highly sensitive to small noise perturbations and precludes a closed-form candidate solution. We develop novel techniques, including a leave-one-out-style argument which controls the correlation between SDP solutions and noise vectors even when the removal of one row of noise can drastically change the SDP solution. We also develop improved eigenvalue perturbation bo
    
[^51]: 通过关键阶段促进记忆增强型Adam的探索

    Promoting Exploration in Memory-Augmented Adam using Critical Momenta. (arXiv:2307.09638v1 [cs.LG])

    [http://arxiv.org/abs/2307.09638](http://arxiv.org/abs/2307.09638)

    本研究提出了一种记忆增强型Adam方法，通过使用关键动量项的缓冲区来促进对更平坦最小值的探索。实验证明，该方法在标准的监督语言建模和图像分类任务中提高了几种Adam变体的性能。

    

    自适应梯度优化器，特别是Adam，在训练大规模深度学习模型中发挥了重要作用。这种优化器的优势在于其快速收敛性，同时对超参数的选择更加鲁棒。然而，它们通常比非自适应方法泛化效果更差。最近的研究将这种性能差距归因于选择平坦最小值：自适应方法倾向于在损失函数曲面中更尖锐的盆地中寻找解决方案，从而损害了泛化能力。为了克服这个问题，我们提出了一种新的记忆增强型Adam方法，在训练过程中使用关键动量项的缓冲区来促进对更平坦最小值的探索。直观地说，缓冲区的使用使得优化器如果盆地的吸引范围不够宽，就会超出其范围。我们经验性地证明了我们的方法在标准的监督语言建模和图像分类任务上提高了几种Adam变体的性能。

    Adaptive gradient-based optimizers, particularly Adam, have left their mark in training large-scale deep learning models. The strength of such optimizers is that they exhibit fast convergence while being more robust to hyperparameter choice. However, they often generalize worse than non-adaptive methods. Recent studies have tied this performance gap to flat minima selection: adaptive methods tend to find solutions in sharper basins of the loss landscape, which in turn hurts generalization. To overcome this issue, we propose a new memory-augmented version of Adam that promotes exploration towards flatter minima by using a buffer of critical momentum terms during training. Intuitively, the use of the buffer makes the optimizer overshoot outside the basin of attraction if it is not wide enough. We empirically show that our method improves the performance of several variants of Adam on standard supervised language modelling and image classification tasks.
    
[^52]: CatBoost对比XGBoost和LightGBM：开发增强的零膨胀保险理赔数据预测模型

    CatBoost Versus XGBoost and LightGBM: Developing Enhanced Predictive Models for Zero-Inflated Insurance Claim Data. (arXiv:2307.07771v1 [cs.LG])

    [http://arxiv.org/abs/2307.07771](http://arxiv.org/abs/2307.07771)

    本文比较了CatBoost、XGBoost和LightGBM三种流行的梯度提升库在处理零膨胀保险理赔数据上的效果，并通过对两个不同数据集的分析，证明了CatBoost是最适合训练保险理赔数据和拟合精算频率模型的库。

    

    在财产和意外事故保险行业中，由于正向理赔数据具有高度右偏分布和过量的零值，构建理赔预测模型面临一些挑战。传统模型，如泊松或负二项广义线性模型(GLM)，经常在处理过量零值时遇到困难。为了应对这个问题，精算科学的研究人员已经采用了“零膨胀”模型，将传统计数模型和二元模型合并，以更有效地处理这些数据集。本文使用了提升算法来处理保险理赔数据，包括零膨胀的遥测数据，以构建理赔频率模型。我们评估和比较了三个流行的梯度提升库 - XGBoost、LightGBM和CatBoost，旨在确定最适合训练保险理赔数据和拟合精算频率模型的库。通过对两个不同数据集的严格分析，我们证明了CatBoost是最优选择。

    In the property and casualty insurance industry, some challenges are presented in constructing claim predictive models due to a highly right-skewed distribution of positive claims with excess zeros. Traditional models, such as Poisson or negative binomial Generalized Linear Models(GLMs), frequently struggle with inflated zeros. In response to this, researchers in actuarial science have employed ``zero-inflated" models that merge a traditional count model and a binary model to address these datasets more effectively. This paper uses boosting algorithms to process insurance claim data, including zero-inflated telematics data, in order to construct claim frequency models. We evaluated and compared three popular gradient boosting libraries - XGBoost, LightGBM, and CatBoost - with the aim of identifying the most suitable library for training insurance claim data and fitting actuarial frequency models. Through a rigorous analysis of two distinct datasets, we demonstrated that CatBoost is sup
    
[^53]: 零代价神经架构搜索：挑战、解决方案和机遇

    Zero-Shot Neural Architecture Search: Challenges, Solutions, and Opportunities. (arXiv:2307.01998v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.01998](http://arxiv.org/abs/2307.01998)

    零代价神经架构搜索是一种不需要训练的方法，其核心思想是设计能够预测网络精确度的代理。本文综述了最新的零代价神经架构搜索方法，并在硬件感知和硬件无感知的NAS场景中展示了其有效性。

    

    最近，零代价（或无需训练）神经架构搜索（NAS）方法被提出来将NAS从昂贵的训练过程中解放出来。零代价NAS方法的关键思想是设计能够预测某些给定网络精确度的代理，而无需训练网络参数。到目前为止，已经提出的代理通常受到深度学习的理论理解的最新进展的启发，并在几个数据集和NAS基准测试上显示出了巨大潜力。本文旨在全面审查和比较最先进的零代价NAS方法，重点关注它们对硬件的意识。为此，我们首先回顾主流的零代价代理并讨论它们的理论基础。然后，我们通过大规模实验比较这些零代价代理，并展示它们在硬件感知和硬件无感知的NAS场景中的有效性。最后，我们指出了设计更好的代理的几个有前途的思路。

    Recently, zero-shot (or training-free) Neural Architecture Search (NAS) approaches have been proposed to liberate NAS from the expensive training process. The key idea behind zero-shot NAS approaches is to design proxies that can predict the accuracy of some given networks without training the network parameters. The proxies proposed so far are usually inspired by recent progress in theoretical understanding of deep learning and have shown great potential on several datasets and NAS benchmarks. This paper aims to comprehensively review and compare the state-of-the-art (SOTA) zero-shot NAS approaches, with an emphasis on their hardware awareness. To this end, we first review the mainstream zero-shot proxies and discuss their theoretical underpinnings. We then compare these zero-shot proxies through large-scale experiments and demonstrate their effectiveness in both hardware-aware and hardware-oblivious NAS scenarios. Finally, we point out several promising ideas to design better proxies
    
[^54]: SwinGNN:重新思考在图生成的扩散模型中的置换不变性

    SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation. (arXiv:2307.01646v1 [cs.LG])

    [http://arxiv.org/abs/2307.01646](http://arxiv.org/abs/2307.01646)

    本文提出了一种新的图生成扩散模型SwinGNN，通过使用高效的2-WL消息传递网络和移动窗口自注意力，以及结合关键的训练和采样技术，显著提高了图生成样本的质量，并引入了随机置换的后处理技巧转换生成的图形统计量。

    

    基于置换等变网络的扩散模型可以学习图数据的置换不变分布。然而，相对于非不变模型，我们发现这些不变模型遇到了更大的学习挑战，因为1）它们的目标分布更具模态性；2）它们的最优一步去噪得分是具有更多成分的高斯混合物的得分函数。受到这个分析的启发，我们提出了一种非不变的扩散模型，称为“SwinGNN”，它采用了一种高效的边到边的2-WL消息传递网络，并利用SwinTransformers中的移动窗口自注意力。此外，通过系统性的实验和剖析，我们确定了几种关键的训练和采样技术，显著提高了图生成样本的质量。最后，我们引入了一种简单的后处理技巧，即随机置换生成的图，可以证明将任何图转换成图形统计量。

    Diffusion models based on permutation-equivariant networks can learn permutation-invariant distributions for graph data. However, in comparison to their non-invariant counterparts, we have found that these invariant models encounter greater learning challenges since 1) their effective target distributions exhibit more modes; 2) their optimal one-step denoising scores are the score functions of Gaussian mixtures with more components. Motivated by this analysis, we propose a non-invariant diffusion model, called $\textit{SwinGNN}$, which employs an efficient edge-to-edge 2-WL message passing network and utilizes shifted window based self-attention inspired by SwinTransformers. Further, through systematic ablations, we identify several critical training and sampling techniques that significantly improve the sample quality of graph generation. At last, we introduce a simple post-processing trick, $\textit{i.e.}$, randomly permuting the generated graphs, which provably converts any graph ge
    
[^55]: 平衡过滤使用非泄露代理

    Balanced Filtering via Non-Disclosive Proxies. (arXiv:2306.15083v1 [cs.LG])

    [http://arxiv.org/abs/2306.15083](http://arxiv.org/abs/2306.15083)

    本文研究了在群组成员资格不可用或被禁止使用时，如何以非泄露方式收集平衡的数据样本。通过使用代理函数和抽样概率，实现了对个体样本的分类和选择，同时保护个体样本的敏感群组成员资格不被泄露。

    

    当群组成员资格在收集时不可用或被禁止使用时，我们研究了非泄露方式收集与敏感群组平衡的数据样本的问题。具体而言，我们的收集机制不会比基本比率能够确定的任何个体样本的群组成员资格更多地透露相关信息。为了做到这一点，我们采用了公平流程的观点，即学习者可以使用少量的标记数据训练代理函数，这个代理函数以后可以用于这个过滤任务。然后，我们将代理函数的范围与抽样概率相关联；给定一个新的候选样本，我们使用代理函数对其进行分类，然后根据与其代理分类对应的抽样概率选择它作为我们的样本。重要的是，我们要求代理分类本身不会透露任何个体样本的敏感群组成员资格的重要信息（即，它应该是非泄露的）。

    We study the problem of non-disclosively collecting a sample of data that is balanced with respect to sensitive groups when group membership is unavailable or prohibited from use at collection time. Specifically, our collection mechanism does not reveal significantly more about group membership of any individual sample than can be ascertained from base rates alone. To do this, we adopt a fairness pipeline perspective, in which a learner can use a small set of labeled data to train a proxy function that can later be used for this filtering task. We then associate the range of the proxy function with sampling probabilities; given a new candidate, we classify it using our proxy function, and then select it for our sample with probability proportional to the sampling probability corresponding to its proxy classification. Importantly, we require that the proxy classification itself not reveal significant information about the sensitive group membership of any individual sample (i.e., it sho
    
[^56]: 约束下的策略优化：State-wise Constrained Policy Optimization

    State-wise Constrained Policy Optimization. (arXiv:2306.12594v1 [cs.LG])

    [http://arxiv.org/abs/2306.12594](http://arxiv.org/abs/2306.12594)

    本文提出了一种新的通用策略搜索算法，State-wise Constrained Policy Optimization (SCPO)，可用于处理状态限制约束下的强化学习，具有良好的期望状态约束保证和最坏安全违反的有界性。

    

    强化学习算法在模拟环境中已经取得了巨大的成功，但是在实际问题中应用仍然面临着重大挑战，其中安全性是一个主要问题。特别地，对于许多具有挑战性的任务，例如自动驾驶和机器人操作，强制执行状态限制是十分必要的。然而，现有的约束马尔可夫决策过程（CMDP）框架下的安全强化学习算法并没有考虑状态约束。为填补这一空白，我们提出了State-wise Constrained Policy Optimization（SCPO），这是第一个旨在处理状态限制的通用策略搜索算法。SCPO能够在期望上保证状态约束的满足。特别地，我们引入了最大马尔可夫决策过程框架，并证明了在SCPO下最坏的安全违反是有界的。我们在大量训练神经网络策略时展示了该方法的有效性。

    Reinforcement Learning (RL) algorithms have shown tremendous success in simulation environments, but their application to real-world problems faces significant challenges, with safety being a major concern. In particular, enforcing state-wise constraints is essential for many challenging tasks such as autonomous driving and robot manipulation. However, existing safe RL algorithms under the framework of Constrained Markov Decision Process (CMDP) do not consider state-wise constraints. To address this gap, we propose State-wise Constrained Policy Optimization (SCPO), the first general-purpose policy search algorithm for state-wise constrained reinforcement learning. SCPO provides guarantees for state-wise constraint satisfaction in expectation. In particular, we introduce the framework of Maximum Markov Decision Process, and prove that the worst-case safety violation is bounded under SCPO. We demonstrate the effectiveness of our approach on training neural network policies for extensive 
    
[^57]: 关于伪造纳什均衡的研究

    On Faking a Nash Equilibrium. (arXiv:2306.08041v1 [cs.MA])

    [http://arxiv.org/abs/2306.08041](http://arxiv.org/abs/2306.08041)

    本文研究多智能体强化学习中的数据污染攻击，提出了唯一纳什集的概念，并设计了一个线性规划方案来计算最优污染攻击策略。

    

    本文研究了多智能体强化学习中的数据污染攻击，攻击者试图更改数据集以安装（潜在虚假的）唯一马尔可夫完美纳什均衡点(Nash equilibrium)。我们提出了唯一纳什集的概念，即由其Q函数规定的游戏的集合，其具有唯一的联合策略作为唯一的纳什均衡点。唯一纳什集对于污染攻击非常重要，因为只有当数据污染使所有合理的游戏都在其中时，攻击才成功。唯一纳什集将常用于逆强化学习中的奖励多面体推广到多智能体强化学习中。对于零和马尔科夫博弈，逆纳什集以及由数据引起的合理游戏集都是Q函数空间中的多面体。我们提出了一个线性规划方案以有效地计算最优的污染攻击策略。我们的工作为设计更加鲁棒的多智能体强化学习算法之前必要的步骤揭示了离线MARL数据污染攻击结构的一些特点。

    We characterize offline data poisoning attacks on Multi-Agent Reinforcement Learning (MARL), where an attacker may change a data set in an attempt to install a (potentially fictitious) unique Markov-perfect Nash equilibrium. We propose the unique Nash set, namely the set of games, specified by their Q functions, with a specific joint policy being the unique Nash equilibrium. The unique Nash set is central to poisoning attacks because the attack is successful if and only if data poisoning pushes all plausible games inside it. The unique Nash set generalizes the reward polytope commonly used in inverse reinforcement learning to MARL. For zero-sum Markov games, both the inverse Nash set and the set of plausible games induced by data are polytopes in the Q function space. We exhibit a linear program to efficiently compute the optimal poisoning attack. Our work sheds light on the structure of data poisoning attacks on offline MARL, a necessary step before one can design more robust MARL alg
    
[^58]: GPT-FL: 生成预训练模型辅助的联邦学习

    GPT-FL: Generative Pre-trained Model-Assisted Federated Learning. (arXiv:2306.02210v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02210](http://arxiv.org/abs/2306.02210)

    GPT-FL是一种生成预训练模型辅助的联邦学习框架，通过生成多样化的合成数据并结合私有客户端数据进行训练，它在模型准确性、通信效率和客户端采样效率等方面优于最先进的方法。在FL训练中，由合成数据生成的下游模型对于控制梯度多样性的方向起着关键作用，提高了收敛速度，并显著提升了准确性。

    

    在这项工作中，我们提出了GPT-FL，一种生成预训练模型辅助的联邦学习（FL）框架。GPT-FL利用生成预训练模型生成多样化的合成数据。这些生成的数据用于在服务器上训练下游模型，然后在标准FL框架下使用私有客户端数据进行微调。我们展示了GPT-FL在模型测试准确性、通信效率和客户端采样效率方面始终优于最先进的FL方法。通过全面的消融分析，我们发现在FL训练过程中，由合成数据生成的下游模型对于控制梯度多样性的方向起着关键作用，这提高了收敛速度，并对观察到的GPT-FL的显著准确性提升做出了贡献。此外，无论目标数据是否在预训练生成模型的领域内或外，GPT-FL始终实现了显著的性能提升。

    In this work, we propose GPT-FL, a generative pre-trained model-assisted federated learning (FL) framework. At its core, GPT-FL leverages generative pre-trained models to generate diversified synthetic data. These generated data are used to train a downstream model on the server, which is then fine-tuned with private client data under the standard FL framework. We show that GPT-FL consistently outperforms state-of-the-art FL methods in terms of model test accuracy, communication efficiency, and client sampling efficiency. Through comprehensive ablation analysis, we discover that the downstream model generated by synthetic data plays a crucial role in controlling the direction of gradient diversity during FL training, which enhances convergence speed and contributes to the notable accuracy boost observed with GPT-FL. Also, regardless of whether the target data falls within or outside the domain of the pre-trained generative model, GPT-FL consistently achieves significant performance gai
    
[^59]: 具有高阶激活函数的Barron空间之间的嵌入

    Embeddings between Barron spaces with higher order activation functions. (arXiv:2305.15839v1 [stat.ML])

    [http://arxiv.org/abs/2305.15839](http://arxiv.org/abs/2305.15839)

    本文研究了不同激活函数的Barron空间之间的嵌入，并证明了Barron空间的层次结构类似于Sobolev空间$H^m$。其中，修正功率单位激活函数在这个研究中特别重要。

    

    无限宽浅层神经网络的逼近性质很大程度上取决于激活函数的选择。为了了解这种影响，我们研究了具有不同激活函数的Barron空间之间的嵌入。通过提供用于表示函数$f$的测量$\mu$上的推进映射来证明这些嵌入。一种特别感兴趣的激活函数是给定为$\operatorname{RePU}_s(x)=\max(0,x)^s$的修正功率单位($\operatorname{RePU}$)。对于许多常用的激活函数，可以使用众所周知的泰勒余项定理构造推进映射，这使我们能够证明相关Barron空间嵌入到具有$\operatorname{RePU}$作为激活函数的Barron空间中。此外，与$\operatorname{RePU}_s$相关的Barron空间具有类似于Sobolev空间$H^m$的分层结构。

    The approximation properties of infinitely wide shallow neural networks heavily depend on the choice of the activation function. To understand this influence, we study embeddings between Barron spaces with different activation functions. These embeddings are proven by providing push-forward maps on the measures $\mu$ used to represent functions $f$. An activation function of particular interest is the rectified power unit ($\operatorname{RePU}$) given by $\operatorname{RePU}_s(x)=\max(0,x)^s$. For many commonly used activation functions, the well-known Taylor remainder theorem can be used to construct a push-forward map, which allows us to prove the embedding of the associated Barron space into a Barron space with a $\operatorname{RePU}$ as activation function. Moreover, the Barron spaces associated with the $\operatorname{RePU}_s$ have a hierarchical structure similar to the Sobolev spaces $H^m$.
    
[^60]: 一种无监督方法用于估计数据集的类别可分性，并应用到LLMs的微调

    An Unsupervised Method for Estimating Class Separability of Datasets with Application to LLMs Fine-Tuning. (arXiv:2305.15016v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15016](http://arxiv.org/abs/2305.15016)

    这个论文提出了一种无监督方法，通过利用数据流形的拓扑特征估计数据的类别可分性，该方法与有监督度量具有相关性，可以用于半监督和感知学习。同时，在语言模型微调中应用该方法用于自动停止准则。

    

    本论文提出了一种无监督方法，利用数据流形的拓扑特征来估计数据的类别可分性，而不需要标签信息。在本论文中，对几个数据集进行的实验表明，所提出的方法估计的类别可分性与需要标签信息的监督度量（如Fisher判别比率（FDR）和分类器的交叉验证）之间存在清晰的相关性和一致性。这可以实现从有标签和无标签数据中学习的学习范式，如半监督学习和感知学习。当我们有限的有标签数据和相对较大的无标签数据集可以用来增强学习过程时，这将特别有用。该方法在无监督的设置下，通过监测嵌入空间流形的类别可分性，实现了语言模型微调和自动停止准则的应用。

    This paper proposes an unsupervised method that leverages topological characteristics of data manifolds to estimate class separability of the data without requiring labels. Experiments conducted in this paper on several datasets demonstrate a clear correlation and consistency between the class separability estimated by the proposed method with supervised metrics like Fisher Discriminant Ratio~(FDR) and cross-validation of a classifier, which both require labels. This can enable implementing learning paradigms aimed at learning from both labeled and unlabeled data, like semi-supervised and transductive learning. This would be particularly useful when we have limited labeled data and a relatively large unlabeled dataset that can be used to enhance the learning process. The proposed method is implemented for language model fine-tuning with automated stopping criterion by monitoring class separability of the embedding-space manifold in an unsupervised setting. The proposed methodology has 
    
[^61]: 漫扩扩散模型和采样器的表达能力研究

    Expressiveness Remarks for Denoising Diffusion Models and Samplers. (arXiv:2305.09605v1 [stat.ML])

    [http://arxiv.org/abs/2305.09605](http://arxiv.org/abs/2305.09605)

    本文在漫扩扩散模型和采样器方面进行了表达能力的研究，通过将已知的神经网络逼近结果扩展到漫扩扩散模型和采样器来实现。

    

    漫扩扩散模型是一类生成模型，在许多领域最近已经取得了最先进的结果。通过漫扩过程逐渐向数据中添加噪声，将数据分布转化为高斯分布。然后，通过模拟该漫扩的时间反演的逼近来获取生成模型的样本，刚开始这个漫扩模拟的初始值是高斯样本。最近的研究探索了将漫扩模型适应于采样和推断任务。本文基于众所周知的与F\"ollmer漂移类似的随机控制联系，将针对F\"ollmer漂移的已知神经网络逼近结果扩展到漫扩扩散模型和采样器。

    Denoising diffusion models are a class of generative models which have recently achieved state-of-the-art results across many domains. Gradual noise is added to the data using a diffusion process, which transforms the data distribution into a Gaussian. Samples from the generative model are then obtained by simulating an approximation of the time reversal of this diffusion initialized by Gaussian samples. Recent research has explored adapting diffusion models for sampling and inference tasks. In this paper, we leverage known connections to stochastic control akin to the F\"ollmer drift to extend established neural network approximation results for the F\"ollmer drift to denoising diffusion models and samplers.
    
[^62]: 实践中知识图谱用户、挑战和可视化需求的特征化研究

    Characterizing the Users, Challenges, and Visualization Needs of Knowledge Graphs in Practice. (arXiv:2304.01311v1 [cs.HC])

    [http://arxiv.org/abs/2304.01311](http://arxiv.org/abs/2304.01311)

    本研究通过访谈19位知识图谱（KG）实践者，发现KG构建者需求架构执行程序，KG分析师需要可自定义查询构建器，KG消费者需要领域特定可视化，并指出在实践中实施KG需要技术和社交方面的解决方案。

    

    本研究通过对19位来自企业和学术环境下、涉及各种用例的知识图谱（KG）实践者的访谈，提出了KG实践者在创建、探索和分析KG时遇到的重要挑战，这些挑战可以通过可视化设计来缓解。我们的研究发现，KG实践者可以分为三类：KG构建者、分析师和消费者，每个人都有自己的专业知识和需求。我们发现，KG构建者可以从架构执行程序中获益，而KG分析师需要提供中间查询结果的可自定义查询构建器。对于KG消费者，我们确定节点链接图的效力不足，并需要定制的领域特定可视化来促进KG的采用和理解。最后，我们发现，在实践中有效地实施KG需要不仅技术上的，还有社交上的解决方案，而这些解决方案目前并未被当前的工具、技术和最佳实践所考虑。

    This study presents insights from interviews with nineteen Knowledge Graph (KG) practitioners who work in both enterprise and academic settings on a wide variety of use cases. Through this study, we identify critical challenges experienced by KG practitioners when creating, exploring, and analyzing KGs that could be alleviated through visualization design. Our findings reveal three major personas among KG practitioners - KG Builders, Analysts, and Consumers - each of whom have their own distinct expertise and needs. We discover that KG Builders would benefit from schema enforcers, while KG Analysts need customizable query builders that provide interim query results. For KG Consumers, we identify a lack of efficacy for node-link diagrams, and the need for tailored domain-specific visualizations to promote KG adoption and comprehension. Lastly, we find that implementing KGs effectively in practice requires both technical and social solutions that are not addressed with current tools, tec
    
[^63]: 固定设计下正则化连续学习的分析

    Fixed Design Analysis of Regularization-Based Continual Learning. (arXiv:2303.10263v1 [cs.LG])

    [http://arxiv.org/abs/2303.10263](http://arxiv.org/abs/2303.10263)

    该论文分析了固定设计下的正则化连续学习问题，提出算法在遗忘和不妥协之间存在权衡，可能会存在灾难性遗忘问题。

    

    我们考虑一个连续学习问题，其中有两个线性回归任务，特征向量被假定为固定的，标签被假定为随机变量。我们考虑一个$\ell_2$-正则化的连续学习算法，它计算一个普通最小二乘参数来拟合第一个数据集，然后计算另一个参数，在$\ell_2$-正则化约束下，拟合第二个数据集并输出第二个参数。对于这个算法，我们提供了两个任务的平均风险的严格界限。我们的风险界限揭示了$\ell_2$-正则化连续学习算法中一个可证明的遗忘和不妥协的权衡关系：使用大的正则化参数，算法输出比较不会遗忘第一个任务的信息，但不愿从第二个任务中提取新信息；反之亦然。我们的结果表明，具有不相似任务的连续学习可能会发生灾难性的遗忘。

    We consider a continual learning (CL) problem with two linear regression tasks in the fixed design setting, where the feature vectors are assumed fixed and the labels are assumed to be random variables. We consider an $\ell_2$-regularized CL algorithm, which computes an Ordinary Least Squares parameter to fit the first dataset, then computes another parameter that fits the second dataset under an $\ell_2$-regularization penalizing its deviation from the first parameter, and outputs the second parameter. For this algorithm, we provide tight bounds on the average risk over the two tasks. Our risk bounds reveal a provable trade-off between forgetting and intransigence of the $\ell_2$-regularized CL algorithm: with a large regularization parameter, the algorithm output forgets less information about the first task but is intransigent to extract new information from the second task; and vice versa. Our results suggest that catastrophic forgetting could happen for CL with dissimilar tasks (u
    
[^64]: 基于方向流形积的高斯过程研究

    Gaussian Process on the Product of Directional Manifolds. (arXiv:2303.06799v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06799](http://arxiv.org/abs/2303.06799)

    本文提出了一种在超环面上建立高斯过程的方法，并使用内在的核相关模型进行学习，以定义在超环面上的向量值函数。通过使用 HvM-based GP 进行数据驱动递归定位，数值结果表明，在跟踪精度方面，该方法具有优势。

    

    本文提出了一种在方向流形积上建立高斯过程（GPs）的方法，并引入了基于 von Mises 分布的循环核。在此基础上，我们提出了所谓的超环维 von Mises（HvM）核，以考虑循环关联组件来建立超环面上的高斯过程。通过使用内在的核相关模型，运用多输出 GP 回归进行学习，用于定义在超环面上的向量值函数。为运行时关键应用程序提供了超参数优化的分析导数。为了评估所提出的方法，我们合成了基于距离的传感器网络，并采用 HvM-based GP 进行数据驱动递归定位。数值结果表明，与参数模型和基于传统核设计的高斯过程相比，HvM-based GP 实现了更优的跟踪精度。

    We introduce a principled study on establishing Gaussian processes (GPs) with inputs on the product of directional manifolds. A circular kernel is first presented according to the von Mises distribution. Based thereon, the so-called hypertoroidal von Mises (HvM) kernel is proposed to establish GPs on hypertori with consideration of correlational circular components. The proposed HvM kernel is demonstrated with multi-output GP regression for learning vector-valued functions defined on hypertori using the intrinsic coregionalization model. Analytical derivatives in hyperparameter optimization are provided for runtime-critical applications. For evaluation, we synthesize a ranging-based sensor network and employ the HvM-based GPs for data-driven recursive localization. The numerical results show that the HvM-based GP achieves superior tracking accuracy compared to parametric model and GPs based on conventional kernel designs.
    
[^65]: 探索基于数值先验的广义CP分解低秩张量补全算法

    Exploring Numerical Priors for Low-Rank Tensor Completion with Generalized CP Decomposition. (arXiv:2302.05881v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.05881](http://arxiv.org/abs/2302.05881)

    本文提出了一种新的方法框架GCDTC，利用数值先验和广义CP分解实现了更高的低秩张量补全精度；同时介绍了一个算法SPTC，作为该框架的一个实现。在实验中，该方法表现出比现有技术更好的性能。

    

    张量补全在计算机视觉、数据分析和信号处理等领域中具有重要意义。最近，低秩张量补全这一类别的方法得到了广泛研究，对补全张量施加低秩结构。虽然这些方法取得了巨大成功，但尚未考虑到张量元素的数值先验信息。忽略数值先验将导致丢失关于数据的重要信息，因此阻止算法达到最优精度。本研究试图构建一个新的方法框架，名为GCDTC（广义CP分解张量补全），以利用数值先验并实现更高的张量补全精度。在这个新引入的框架中，将广义的CP分解应用于低秩张量补全。本文还提出了一种名为SPTC（平滑泊松张量补全）的算法，用于非负整数张量补全，作为GCDTC框架的一个实现。通过对合成和真实世界数据集的大量实验，证明所提出的方法相比于现有技术具有更优的张量补全性能。

    Tensor completion is important to many areas such as computer vision, data analysis, and signal processing. Enforcing low-rank structures on completed tensors, a category of methods known as low-rank tensor completion has recently been studied extensively. While such methods attained great success, none considered exploiting numerical priors of tensor elements. Ignoring numerical priors causes loss of important information regarding the data, and therefore prevents the algorithms from reaching optimal accuracy. This work attempts to construct a new methodological framework called GCDTC (Generalized CP Decomposition Tensor Completion) for leveraging numerical priors and achieving higher accuracy in tensor completion. In this newly introduced framework, a generalized form of CP Decomposition is applied to low-rank tensor completion. This paper also proposes an algorithm known as SPTC (Smooth Poisson Tensor Completion) for nonnegative integer tensor completion as an instantiation of the G
    
[^66]: 多传感器强化学习的联合表示

    Joint Representations for Reinforcement Learning with Multiple Sensors. (arXiv:2302.05342v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05342](http://arxiv.org/abs/2302.05342)

    本文提出了一种基于重构和对比损失的方法来对多传感器的输入进行联合表示学习，证明了这种方法在完成复杂任务时具有良好的效果。

    

    在强化学习中有效地结合多个传感器的输入是一个待解决的问题。尽管存在许多自监督表示学习方法来提高基于图像的强化学习的性能和样本复杂性，但它们通常忽略其他可用信息，如机器人本体感知。然而，利用这种本体感知进行表示学习可以帮助算法聚焦于相关方面，并指导其寻找更好的表示。在本文中，我们通过基于递归状态空间模型，从多个传感器中对强化学习的表示学习进行了系统分析。我们提出了一种基于重构和对比损失的组合方法，使我们能够为每个传感器模态选择最合适的方法。我们证明了联合表示的好处，特别是对于每个模态具有不同损失函数的无模型和模型基础强化学习，以完成复杂的任务，包括图像包含分散的视觉信息或缺少足够的上下文线索的任务。

    Combining inputs from multiple sensor modalities effectively in reinforcement learning (RL) is an open problem. While many self-supervised representation learning approaches exist to improve performance and sample complexity for image-based RL, they usually neglect other available information, such as robot proprioception. However, using this proprioception for representation learning can help algorithms to focus on relevant aspects and guide them toward finding better representations. In this work, we systematically analyze representation learning for RL from multiple sensors by building on Recurrent State Space Models. We propose a combination of reconstruction-based and contrastive losses, which allows us to choose the most appropriate method for each sensor modality. We demonstrate the benefits of joint representations, particularly with distinct loss functions for each modality, for model-free and model-based RL on complex tasks. Those include tasks where the images contain distra
    
[^67]: 深度代理因果学习及其在混淆赌博策略评估中的应用

    Deep Proxy Causal Learning and its Application to Confounded Bandit Policy Evaluation. (arXiv:2106.03907v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.03907](http://arxiv.org/abs/2106.03907)

    本论文提出了一种深度代理因果学习（PCL）方法，用于在存在混淆因素的情况下估计治疗对结果的因果效应。通过构建治疗和代理之间的模型，并利用该模型在给定代理的情况下学习治疗对结果的影响，PCL可以保证恢复真实的因果效应。作者还提出了一种名为深度特征代理变量方法（DFPV）的新方法，用于处理高维和非线性复杂关系的情况，并表明DFPV在合成基准测试中的性能优于最先进的PCL方法。

    

    代理因果学习（PCL）是一种在存在未观察到的混淆因素时，利用代理（结构化侧面信息）估计治疗对结果的因果效应的方法。这是通过两阶段回归实现的：在第一阶段，我们建模治疗和代理之间的关系；在第二阶段，我们利用这个模型来学习在给定代理提供的上下文下，治疗对结果的影响。PCL在可识别条件下保证恢复真实的因果效应。我们提出了一种新的PCL方法，深度特征代理变量方法（DFPV），以解决代理、治疗和结果为高维且具有非线性复杂关系的情况，如深度神经网络特征表示。我们表明DFPV在具有挑战性的合成基准测试中优于最近的最先进的PCL方法，包括涉及高维图像数据的设置。此外，我们还展示了PCL的应用...

    Proxy causal learning (PCL) is a method for estimating the causal effect of treatments on outcomes in the presence of unobserved confounding, using proxies (structured side information) for the confounder. This is achieved via two-stage regression: in the first stage, we model relations among the treatment and proxies; in the second stage, we use this model to learn the effect of treatment on the outcome, given the context provided by the proxies. PCL guarantees recovery of the true causal effect, subject to identifiability conditions. We propose a novel method for PCL, the deep feature proxy variable method (DFPV), to address the case where the proxies, treatments, and outcomes are high-dimensional and have nonlinear complex relationships, as represented by deep neural network features. We show that DFPV outperforms recent state-of-the-art PCL methods on challenging synthetic benchmarks, including settings involving high dimensional image data. Furthermore, we show that PCL can be app
    

