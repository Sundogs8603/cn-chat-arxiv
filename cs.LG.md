# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Boosting Unsupervised Contrastive Learning Using Diffusion-Based Data Augmentation From Scratch.](http://arxiv.org/abs/2309.07909) | 本文介绍了一种新颖且高效的基于扩散的数据增强技术DiffAug，通过扩散步骤实现增强数据和原始数据共享平滑的潜空间，旨在提高无监督对比学习的效果和效率。 |
| [^2] | [Physically Plausible Full-Body Hand-Object Interaction Synthesis.](http://arxiv.org/abs/2309.07907) | 提出了一种基于物理的方法，用于合成全身环境中的灵巧手物交互。方法采用强化学习和物理仿真，通过学习先验技能来控制手物交互，解决了数据驱动方法的局限性。 |
| [^3] | [Improving physics-informed DeepONets with hard constraints.](http://arxiv.org/abs/2309.07899) | 本研究提出了一种改进的物理信息深度学习策略，消除了对初始条件的学习需求，并确保在多次应用时得到的函数是连续的。 |
| [^4] | [Choosing a Proxy Metric from Past Experiments.](http://arxiv.org/abs/2309.07893) | 本文介绍了一种新的统计框架，用于在同质种群的随机实验中选择最优的代理指标。该方法通过投资组合优化问题将最优代理指标的构建进行了归约，并对观察到的治疗效果进行了降噪处理。 |
| [^5] | [A Novel Local-Global Feature Fusion Framework for Body-weight Exercise Recognition with Pressure Mapping Sensors.](http://arxiv.org/abs/2309.07888) | 这项研究提出了一种新颖的基于局部全局特征融合框架的体重运动识别方法，利用压力传感器和图像处理技术结合局部和全局特征，在考虑物理约束的情况下，以11%的提升改善了运动识别的性能。 |
| [^6] | [Some notes concerning a generalized KMM-type optimization method for density ratio estimation.](http://arxiv.org/abs/2309.07887) | 本文介绍了一种用于密度比估计的广义KMM类型优化算法，通过构建适当的损失函数，扩展了现有方法以适应更一般的情况，并包括对训练数据和测试数据子集相对于密度比的估计。 |
| [^7] | [Beta Diffusion.](http://arxiv.org/abs/2309.07867) | beta扩散是一种新型生成模型方法，通过引入去掩盖和去噪的技术，利用缩放和偏移的beta分布进行乘法转换，实现在有界范围内生成数据。相比于传统的基于扩散的生成模型，它通过KL散度上界进行优化，证明了效果更好。 |
| [^8] | [Identifying the Group-Theoretic Structure of Machine-Learned Symmetries.](http://arxiv.org/abs/2309.07860) | 本研究提出了一种用于检查和识别机器学习对称性群论结构的方法。通过设计损失函数，可以在对称性发现的深度学习阶段或后处理阶段探测子代数结构，并应用于粒子物理学中的SU(3)和SU(5)非阿贝尔规范对称性的自发破缺。 |
| [^9] | [Learning to Warm-Start Fixed-Point Optimization Algorithms.](http://arxiv.org/abs/2309.07835) | 该论文提出了一个机器学习框架，用于为固定点优化算法提供预热。该框架使用神经网络预测问题参数的预热起始点，并通过固定点迭代来优化结果。实验结果表明，该方法可以显著减少迭代次数。 |
| [^10] | [Directed Scattering for Knowledge Graph-based Cellular Signaling Analysis.](http://arxiv.org/abs/2309.07813) | 本论文提出了一种新的框架，称为有向散射自编码器（DSAE），用于基于知识图谱的细胞信号分析。该框架结合了有向版本的几何散射变换、自编码器的非线性降维特性和双曲空间的几何属性，可以学习到细胞信号网络中的潜在分层信息，并在细胞信号网络推断等科学任务中取得了优异的性能。 |
| [^11] | [Text Classification of Cancer Clinical Trial Eligibility Criteria.](http://arxiv.org/abs/2309.07812) | 本文研究了癌症临床试验中常见的排除标准，通过应用文本分类方法和预训练的BERT模型，证明了自动分类排除标准的可行性，并展示了专门为临床试验设计的预训练语言模型的价值。 |
| [^12] | [Communication Efficient Private Federated Learning Using Dithering.](http://arxiv.org/abs/2309.07809) | 本文提出了一种使用抖动实现高效通信的隐私联邦学习方法，通过在客户端使用减法抖动的量化方案，可以复制正常噪声添加过程，实现与完整精度梯度方法相当的准确性，并大幅减少所需通信量。 |
| [^13] | [What Matters to Enhance Traffic Rule Compliance of Imitation Learning for Automated Driving.](http://arxiv.org/abs/2309.07808) | 本文提出了一种基于惩罚的模仿学习方法P-CSG，结合语义生成传感器融合技术，以提高端到端自动驾驶的整体性能，并解决了交通规则遵守和传感器感知问题。 |
| [^14] | [Improving Multimodal Classification of Social Media Posts by Leveraging Image-Text Auxiliary tasks.](http://arxiv.org/abs/2309.07794) | 本研究通过引入图像-文本辅助任务，有效地提高了社交媒体帖子的多模态分类，通过两个辅助损失函数对图像-文本表示进行调整，捕捉底层依赖关系和语义对应关系，实现了一致的改进。 |
| [^15] | [Virchow: A Million-Slide Digital Pathology Foundation Model.](http://arxiv.org/abs/2309.07778) | Virchow是一个数百万参数的深度神经网络基础模型，通过在数百万张全数字病理学切片图像上进行自监督学习训练，有效解决了计算病理学任务中数据不足的问题，并在多个下游任务上超越了最先进的系统。 |
| [^16] | [Variational Quantum Linear Solver enhanced Quantum Support Vector Machine.](http://arxiv.org/abs/2309.07770) | 提出了一种变分量子线性求解器增强的量子支持向量机算法，能够在嘈杂中间规模量子设备上进行可扩展的分类任务。算法利用变分量子线性求解器解决了最小二乘支持向量机的线性方程组，并在Iris数据集上进行了验证。通过经典计算和量子计算的策略性组合，算法在不同维度的特征空间中表现出了实用性和有效性。 |
| [^17] | [PRE: Vision-Language Prompt Learning with Reparameterization Encoder.](http://arxiv.org/abs/2309.07760) | 这项工作提出了一种名为PRE的方法，通过重新参数化编码器来增强可学习提示的泛化能力，从而解决了大型预训练视觉-语言模型中手动提示工程的挑战。 |
| [^18] | [Interpretability is in the Mind of the Beholder: A Causal Framework for Human-interpretable Representation Learning.](http://arxiv.org/abs/2309.07742) | 可解释性表示学习的关键挑战是如何在人类因素中进行建模和操作。本论文提出了一种数学框架，用于获取可解释的表示，适用于事后解释者和基于概念的神经网络。 |
| [^19] | [Understanding Vector-Valued Neural Networks and Their Relationship with Real and Hypercomplex-Valued Neural Networks.](http://arxiv.org/abs/2309.07716) | 本文介绍了向量值神经网络（V-nets）的广泛框架，并解释了它们与超复值神经网络以及传统神经网络的关系。 |
| [^20] | [Market-GAN: Adding Control to Financial Market Data Generation with Semantic Context.](http://arxiv.org/abs/2309.07708) | 本研究通过提出具有市场动态、股票代码和历史状态作为上下文的上下文市场数据集以及使用条件生成对抗网络（GAN）来实现对金融数据生成的控制。 |
| [^21] | [Causal Entropy and Information Gain for Measuring Causal Control.](http://arxiv.org/abs/2309.07703) | 本文提出了一种考虑因果结构的信息论量，用于评估某个特定结果变量的因果重要性，解决了因果可解释性的挑战。 |
| [^22] | [Tree of Uncertain Thoughts Reasoning for Large Language Models.](http://arxiv.org/abs/2309.07694) | 本研究提出了一种针对大型语言模型的推理框架——不确定思维树（TouT），它通过利用蒙特卡洛丢弃来量化中间步骤上的本地不确定性，提高了模型生成响应的精确性。 |
| [^23] | [A DenseNet-based method for decoding auditory spatial attention with EEG.](http://arxiv.org/abs/2309.07690) | 本论文提出了一种基于DenseNet的EEG辅助听觉空间注意力解码方法，该方法充分利用了EEG电极的空间分布，并通过深度卷积神经网络提取了时空特征，实现了较高的解码精度。 |
| [^24] | [deepFDEnet: A Novel Neural Network Architecture for Solving Fractional Differential Equations.](http://arxiv.org/abs/2309.07684) | deepFDEnet是一种新型神经网络架构，能够准确求解各种形式的分数阶微分方程。 |
| [^25] | [Benchmarking machine learning models for quantum state classification.](http://arxiv.org/abs/2309.07679) | 本文对应用于真实量子设备的多种分类技术进行基准测试，以开发一个模型来区分基态和激发态。 |
| [^26] | [Goal Space Abstraction in Hierarchical Reinforcement Learning via Set-Based Reachability Analysis.](http://arxiv.org/abs/2309.07675) | 本文提出了一种基于集合的可达性分析方法，通过 emergent representation 实现目标空间抽象，在分层强化学习中自主发现符号目标表示，并引入封建HRL算法来同时学习目标表示和分层策略。 |
| [^27] | [Physics-constrained robust learning of open-form PDEs from limited and noisy data.](http://arxiv.org/abs/2309.07672) | 该论文提出了一种名为R-DISCOVER的框架，能够从有限且有噪声的数据中稳健地揭示开放式偏微分方程。该框架通过符号表示和强化学习引导下的混合PDE生成器，以及神经网络预测模型来实现。实验结果表明该方法能够高效地发现和嵌入PDE，并选择表现最佳的PDE进行系统响应预测。 |
| [^28] | [Federated Dataset Dictionary Learning for Multi-Source Domain Adaptation.](http://arxiv.org/abs/2309.07670) | 本文提出了一种用于联邦领域自适应的方法，通过字典学习经验分布来解决客户端间分布偏移和部分无标签数据的问题。该方法通过设计协作通信协议和聚合操作，保护了客户端数据隐私，并成功在目标领域生成了标记数据。 |
| [^29] | [Multi-Source Domain Adaptation meets Dataset Distillation through Dataset Dictionary Learning.](http://arxiv.org/abs/2309.07666) | 本文介绍了一个新问题，称为多源领域适应通过数据集字典学习遇见数据集蒸馏（MSDA-DD），通过适应先前的方法以及分配匹配方法，我们实现了仅使用每类1个样本即可实现最先进的适应性能。 |
| [^30] | [Dataset Size Dependence of Rate-Distortion Curve and Threshold of Posterior Collapse in Linear VAE.](http://arxiv.org/abs/2309.07663) | 本文通过分析在高维限制下的最简化的VAE，提出了一个闭式表达式，评估了beta与VAE中数据集大小、后验坍缩和率失真曲线之间的关系。结果显示，随着beta的增加，产生较大的广义误差平台，并且选择一个小于特定阈值的beta值可以提高模型性能。 |
| [^31] | [Feature Engineering in Learning-to-Rank for Community Question Answering Task.](http://arxiv.org/abs/2309.07610) | 本文研究了在社区问答任务中学习到排名中的特征工程的几个方面。首先，引入了基于BERT的特征，捕捉语义相似性；其次，结合问题和答案两种类型的特征；第三，通过经验性研究探索了不同排名算法。 |
| [^32] | [Learning Quasi-Static 3D Models of Markerless Deformable Linear Objects for Bimanual Robotic Manipulation.](http://arxiv.org/abs/2309.07609) | 本文提出了一种基于Transformer架构的新的学习模型，通过缩放方法实现了不同长度的可变形一维物体的更高精度。此外，引入的数据增强技术大大提高了数据驱动模型的预测性能，甚至简单的多层感知机也能达到接近最先进性能的水平。 |
| [^33] | [Turning Dross Into Gold Loss: is BERT4Rec really better than SASRec?.](http://arxiv.org/abs/2309.07602) | 在比较推荐系统中的两种模型SASRec和BERT4Rec时，我们的研究发现，如果两个模型都使用相同的损失函数进行训练，SASRec在质量和训练速度方面表现明显优于BERT4Rec。同时，我们还发现，即使使用负采样，SASRec仍然能够有效训练并优于BERT4Rec，但需要更多的负样本。 |
| [^34] | [Detecting Misinformation with LLM-Predicted Credibility Signals and Weak Supervision.](http://arxiv.org/abs/2309.07601) | 本文研究了使用大型语言模型和弱监督的方式来检测虚假信息，证明了这种方法在两个数据集上的效果优于当前最先进的分类器。 |
| [^35] | [Statistically Valid Variable Importance Assessment through Conditional Permutations.](http://arxiv.org/abs/2309.07593) | 本论文提出了一种通过条件置换进行统计有效的变量重要性评估的方法。实证结果表明，这种方法克服了标准置换重要性的局限性，并在深度神经网络中表现出最高的准确性。 |
| [^36] | [Structure-Preserving Transformers for Sequences of SPD Matrices.](http://arxiv.org/abs/2309.07579) | 本文介绍了一种保持序列的对称正定矩阵的黎曼几何特性的结构保持变压器机制，并将其应用于自动睡眠分期，取得了高水平的阶段性能。 |
| [^37] | [Equivariant Data Augmentation for Generalization in Offline Reinforcement Learning.](http://arxiv.org/abs/2309.07578) | 这个论文提出了一种用于离线强化学习中泛化的等变数据增强方法，并通过扩展等变集合并增强数据集来提高策略的测试性能。 |
| [^38] | [Naturalistic Robot Arm Trajectory Generation via Representation Learning.](http://arxiv.org/abs/2309.07550) | 本文通过自监督学习方法使用自回归时空图神经网络，通过模仿人类示范者的运动轨迹来生成自然的机器人臂运动轨迹，为轮椅坐式辅助机器人提供更可预测和更类似人类的运动。 |
| [^39] | [Proximal Bellman mappings for reinforcement learning and their application to robust adaptive filtering.](http://arxiv.org/abs/2309.07548) | 本文提出了一种新的近似贝尔曼映射类，并在强化学习和鲁棒自适应滤波中应用。这些映射具有丰富的设计自由度，并可以解决线性自适应滤波中选择指数的问题。数值测试表明这种方法具有卓越的性能。 |
| [^40] | [VerilogEval: Evaluating Large Language Models for Verilog Code Generation.](http://arxiv.org/abs/2309.07544) | 本文提出了一个专门用于评估大型语言模型在Verilog代码生成中的性能的基准框架，并提供了一个包含156个问题的综合评估数据集。通过与黄金解决方案进行比较，可以自动测试Verilog代码的功能正确性。通过使用LLM生成的合成问题-代码对进行监督微调，可以改善预训练语言模型的Verilog代码生成能力。 |
| [^41] | [Adaptive approximation of monotone functions.](http://arxiv.org/abs/2309.07530) | 这项研究解决了通过顺序查询函数值来逼近非递减函数的问题，并引入了一个通用算法GreedyBox，该算法对于任意函数都能实现最佳的样本复杂度。 |
| [^42] | [Learning Beyond Similarities: Incorporating Dissimilarities between Positive Pairs in Self-Supervised Time Series Learning.](http://arxiv.org/abs/2309.07526) | 本文提出了一种自监督学习方法（DEBS），通过融合正例对之间的不相似性，超越了简单的相似性，用于心电图(ECG)信号的心房颤动(AFib)检测。使用DEBS可以提高10%的检测准确度，强调了利用不相似性来编码时间序列数据的动态特征。 |
| [^43] | [Massively-Parallel Heat Map Sorting and Applications To Explainable Clustering.](http://arxiv.org/abs/2309.07486) | 本研究针对热图排序问题在保留簇的同时重新排列和合并点和维度，并提供了一个高度并行的固定参数算法。通过实证比较，我们发现该算法在应用于电子邮件和计算机网络等领域时，与k-means和DBSCAN的效果相当。 |
| [^44] | [Improved Auto-Encoding using Deterministic Projected Belief Networks.](http://arxiv.org/abs/2309.07481) | 本研究中，我们利用确定性投影置信网络（D-PBN）结合可训练的复合激活函数（TCAs），对自动编码器进行了改进，实验结果表明，相比于传统自动编码器，包括变分自动编码器，具有TCAs的D-PBN自动编码器表现更好。 |
| [^45] | [Direct Text to Speech Translation System using Acoustic Units.](http://arxiv.org/abs/2309.07478) | 本文提出了一种使用离散声学单元的直接文本到语音翻译系统，通过结合语音编码器和聚类算法提取声学单元，并使用编码器-解码器架构预测和生成语音。实验证明，该系统在多数评估的语言对上表现出竞争性能，并且使用更多语言预训练模型进行初始化会带来显著的改进。 |
| [^46] | [Detecting Unknown Attacks in IoT Environments: An Open Set Classifier for Enhanced Network Intrusion Detection.](http://arxiv.org/abs/2309.07461) | 这项研究介绍了一个针对物联网环境定制的网络入侵检测系统的开放集分类器框架，利用图像表示和堆叠子聚类技术来识别未知攻击。 |
| [^47] | [SC-MAD: Mixtures of Higher-order Networks for Data Augmentation.](http://arxiv.org/abs/2309.07453) | 本论文提出了一种名为SC-MAD的方法，利用混合高阶网络对数据进行增强。通过线性和非线性混合机制返回现有标记样本的混合物，以及一种凸聚类混合方法用于描述多个复形复杂网络之间的数据驱动关系。这种方法在复形复杂分类中表现出良好的性能。 |
| [^48] | [Is Solving Graph Neural Tangent Kernel Equivalent to Training Graph Neural Network?.](http://arxiv.org/abs/2309.07452) | 本文探讨了解决图神经切向核是否等价于训练图神经网络的问题，提供了三个新的理论结果。 |
| [^49] | [TensorFlow Chaotic Prediction and Blow Up.](http://arxiv.org/abs/2309.07450) | 该论文使用TensorFlow库来预测高维非线性系统的混沌动态，发现了TensorFlow库的长期预测行为有失控的问题。 |
| [^50] | [A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time.](http://arxiv.org/abs/2309.07418) | 在本文中，我们使用张量和SVM Trick重新构建了单层LLM注意力机制，并提供了对应的优化视角。我们的工作可以在矩阵乘法时间内解决注意力回归问题。 |
| [^51] | [Advancing Regular Language Reasoning in Linear Recurrent Neural Networks.](http://arxiv.org/abs/2309.07412) | 通过分析现有的线性循环神经网络（LRNN），我们提出了一种新的LRNN模型，该模型配备了一个块对角且输入相关的转移矩阵，并且是唯一一个能够在正则语言任务上进行长度外推的LRNN。 |
| [^52] | [Semi-supervised Domain Adaptation on Graphs with Contrastive Learning and Minimax Entropy.](http://arxiv.org/abs/2309.07402) | 提出了一种名为SemiGCL的方法，使用图对比学习和最小最大熵训练来解决图上的半监督领域适应问题，该方法通过对比学得表示来生成信息丰富的节点表示，并使用对抗优化减小域差异。在实验中取得了良好的性能。 |
| [^53] | [Semantic Adversarial Attacks via Diffusion Models.](http://arxiv.org/abs/2309.07398) | 本文提出了一个用于生成语义对抗攻击的框架，并使用扩散模型中的潜在空间中的语义信息。在该框架中，有两个变种方法：语义转换方法(ST)，通过微调潜在空间和/或扩散模型本身来生成图像；潜在屏蔽方法(LM)，利用另一目标图像屏蔽潜在空间，并使用基于局部反向传播的解释方法。实验结果验证了该框架的有效性。 |
| [^54] | [EnCodecMAE: Leveraging neural codecs for universal audio representation learning.](http://arxiv.org/abs/2309.07391) | 本文提出了一种称为EnCodecMAE的方法，利用神经编解码器EnCodec生成离散目标，用于基于遮蔽自动编码器（MAE）学习通用音频模型。通过在涵盖语音、音乐和环境声音的多个音频任务上的评估，发现EnCodecMAE达到了与领先的音频表示模型相当甚至更好的性能。 |
| [^55] | [Rates of Convergence in Certain Native Spaces of Approximations used in Reinforcement Learning.](http://arxiv.org/abs/2309.07383) | 本文研究了在强化学习中出现的值函数近似在特定本地空间中的收敛速度，提出了运用算子方程进行离线近似的方法，并通过有限维近似空间中的功率函数得到了值函数近似误差的上界。这些结果改进和细化了值函数近似的收敛性。 |
| [^56] | [Beta quantile regression for robust estimation of uncertainty in the presence of outliers.](http://arxiv.org/abs/2309.07374) | 本研究提出了一种鲁棒的分位数回归方法，通过融合鲁棒散度的概念，解决了分位数回归对异常值特征敏感的问题。 |
| [^57] | [The kernel-balanced equation for deep neural networks.](http://arxiv.org/abs/2309.07367) | 本文提出了深度神经网络的核平衡方程，解释了数据集分布估计中的不稳定性和尺度机制。网络的输出是数据集的局部平均，平均的尺度随着训练逐渐减小并导致不稳定性。 |
| [^58] | [Hodge-Aware Contrastive Learning.](http://arxiv.org/abs/2309.07364) | 本文提出了一种基于豪奇分解的对比学习方法，用于处理单纯复合体数据，并生成具有特定谱信息的嵌入。通过编码数据不变性和设计合适的增强方法，以及重新权衡负样本的重要性，我们得到了一个反映谱信息的嵌入空间。 |
| [^59] | [Tackling the dimensions in imaging genetics with CLUB-PLS.](http://arxiv.org/abs/2309.07352) | 本文介绍了一种名为Cluster-Bootstrap PLS（CLUB-PLS）的基于偏最小二乘的框架，用于解决影像遗传学中的维度问题。该框架可以处理两个领域的大量输入维度和大样本量，并使用聚类自助法提供稳健的统计数据。 |
| [^60] | [Efficient Learning of PDEs via Taylor Expansion and Sparse Decomposition into Value and Fourier Domains.](http://arxiv.org/abs/2309.07344) | 通过Reel算法，利用随机投影和稀疏分解在值域和傅里叶域中，高效学习偏微分方程，加快了科学发现的速度。 |
| [^61] | [Efficient quantum recurrent reinforcement learning via quantum reservoir computing.](http://arxiv.org/abs/2309.07339) | 本研究提出了一种高效的量子循环强化学习方法，通过构建利用基于量子循环神经网络的储水池的QRL代理，解决了QRL与QRNN的低效训练问题。通过数值模拟验证了这种方法的有效性，并在标准基准测试中展示了其潜力。 |
| [^62] | [Reliability-based cleaning of noisy training labels with inductive conformal prediction in multi-modal biomedical data mining.](http://arxiv.org/abs/2309.07332) | 创新点：提出了一种基于可靠性的训练数据清洗方法，利用归纳性符合预测来纠正噪声训练数据中的错误标记和异常值。该方法在多个分类任务中验证有效性，并显著提升了分类性能。 |
| [^63] | [Traveling Words: A Geometric Interpretation of Transformers.](http://arxiv.org/abs/2309.07315) | 本文提出了一种几何视角来解释变压器的内部机制，主要贡献在于阐明了层归一化如何限制潜在特征并在超球面上塑造注意力机制，通过探测预训练的GPT-2模型验证了该视角的有效性，并提供了对变压器的直观理解。 |
| [^64] | [User Training with Error Augmentation for Electromyogram-based Gesture Classification.](http://arxiv.org/abs/2309.07289) | 该论文研究了一种基于错误增强的肌电信号手势分类的用户训练系统，实验结果表明，相对于基线，使用修改反馈的条件下能够显著提高手势分类准确性和类别区分度。 |
| [^65] | [Unbiased Face Synthesis With Diffusion Models: Are We There Yet?.](http://arxiv.org/abs/2309.07277) | 本文研究了生成模型在人脸生成领域的功效和不足之处。通过定性和定量方法，我们提出了一个框架来审计基于社会属性条件生成的人脸的特征。我们发现人脸图像生成存在几个限制，包括对文本提示的忠实度、人口统计差异和分布偏移。 |
| [^66] | [Safe and Accelerated Deep Reinforcement Learning-based O-RAN Slicing: A Hybrid Transfer Learning Approach.](http://arxiv.org/abs/2309.07265) | 本文提出了一种混合迁移学习方法，用于解决在O-RAN网络中使用DRL算法进行闭环控制时遇到的收敛速度慢和性能不稳定的问题。 |
| [^67] | [Simultaneous inference for generalized linear models with unmeasured confounders.](http://arxiv.org/abs/2309.07261) | 本文研究了存在混淆效应时的广义线性模型的大规模假设检验问题，并提出了一种利用正交结构和线性投影的统计估计和推断框架，解决了由于未测混淆因素引起的偏差问题。 |
| [^68] | [Solving Recurrence Relations using Machine Learning, with Application to Cost Analysis.](http://arxiv.org/abs/2309.07259) | 该论文提出了一种使用机器学习解决递归关系的方法，旨在解决当前递归求解工具的瓶颈问题，并应用于成本分析中。方法基于机器学习稀疏回归技术猜测闭式函数，并结合SMT-so来求解任意约束递归关系。 |
| [^69] | [All you need is spin: SU(2) equivariant variational quantum circuits based on spin networks.](http://arxiv.org/abs/2309.07250) | 本文提出使用自旋网络构建SU(2)等价量子电路，通过编码群结构来限制优化空间， 具有旋转对称性，比其他已知的构造更直接实现在量子硬件上。 |
| [^70] | [Autotuning Apache TVM-based Scientific Applications Using Bayesian Optimization.](http://arxiv.org/abs/2309.07235) | 本文提出了一种基于贝叶斯优化的TVM自动调优框架，使用TVM张量表达语言实现了LU分解、Cholesky分解和3mm等线性代数核。在GPU集群上的实验结果表明，该框架在大多数情况下优于传统的AutoTVM框架。 |
| [^71] | [EarthPT: a foundation model for Earth Observation.](http://arxiv.org/abs/2309.07207) | EarthPT是一个地球观测的预训练transformer模型，能够准确预测未来地表反射值，并提供有意义的嵌入信息用于下游任务。 |
| [^72] | [Latent Representation and Simulation of Markov Processes via Time-Lagged Information Bottleneck.](http://arxiv.org/abs/2309.07200) | 本文介绍了一种通过时间滞后信息瓶颈的方法，将复杂系统映射到简化表示空间并模拟时间上的大跳跃。实验证明该方法能够准确模拟原始过程的统计特性和动力学，优于现有的时间滞后降维方法。 |
| [^73] | [Mitigating Adversarial Attacks in Federated Learning with Trusted Execution Environments.](http://arxiv.org/abs/2309.07197) | 在联邦学习中，通过使用受信任的执行环境，可以减轻被入侵节点利用对抗性攻击来探测模型的风险。这可以避免危险的现实世界场景，如被篡改的交通标志导致自动驾驶车辆错误识别，或者被污染的本地数据集。 |
| [^74] | [A Robust SINDy Approach by Combining Neural Networks and an Integral Form.](http://arxiv.org/abs/2309.07193) | 本文提出了一种通过结合神经网络和积分形式的方法实现鲁棒的SINDy方法，能够从嘈杂和稀缺的数据中发现非线性控制方程。 |
| [^75] | [The effect of data augmentation and 3D-CNN depth on Alzheimer's Disease detection.](http://arxiv.org/abs/2309.07192) | 本研究通过严格遵守数据处理、实验设计和模型评估的最佳实践来确保可重复和可靠的机器学习。针对阿尔茨海默病检测问题，我们研究了不同的数据增强技术和模型复杂度对整体性能的影响，并采用了交叉验证和多次训练试验来补偿数据稀缺性和初始随机参数。 |
| [^76] | [Learning From Drift: Federated Learning on Non-IID Data via Drift Regularization.](http://arxiv.org/abs/2309.07189) | 通过漂移正则化，我们提出了学习漂移（LfD）方法，它能在非独立同分布数据上有效地训练模型，防止性能下降。 |
| [^77] | [Predicting Survival Time of Ball Bearings in the Presence of Censoring.](http://arxiv.org/abs/2309.07188) | 本文提出了一种新的方法，使用生存分析来预测滚珠轴承的失效时间，并且通过分析频域数据和训练多个生存模型来实现。模型可以以时间为基础进行风险的概率预测，并允许比较不同组轴承的生存函数。 |
| [^78] | [Multi-step prediction of chlorophyll concentration based on Adaptive Graph-Temporal Convolutional Network with Series Decomposition.](http://arxiv.org/abs/2309.07187) | 本文提出了一种基于自适应图时空卷积网络和序列分解的多步叶绿素浓度预测模型，可以有效挖掘数据中的非线性特征，对环境保护和水产养殖具有重要意义。 |
| [^79] | [Audio-Based Classification of Respiratory Diseases using Advanced Signal Processing and Machine Learning for Assistive Diagnosis Support.](http://arxiv.org/abs/2309.07183) | 本研究利用声音数据训练多个机器学习模型来对呼吸疾病进行分类。方法采用经验模态分解和频谱分析来提取与心血管和呼吸模式相关的生理信号，并通过特征提取和预测建模实现快速筛查和诊断支持。 |
| [^80] | [Sleep Stage Classification Using a Pre-trained Deep Learning Model.](http://arxiv.org/abs/2309.07182) | 本研究提出了一种名为"EEGMobile"的机器学习模型，在睡眠阶段分类中取得了优于其他模型的准确率，特别在N1阶段表现更佳。 |
| [^81] | [The Grand Illusion: The Myth of Software Portability and Implications for ML Progress.](http://arxiv.org/abs/2309.07181) | 这项研究通过大规模研究了流行的机器学习框架在不同硬件上的可移植性，发现它们在转移到其他硬件上时可能会丢失超过40%的关键功能。这种不可移植性对机器学习创新的速度产生了负面影响。 |
| [^82] | [CloudBrain-NMR: An Intelligent Cloud Computing Platform for NMR Spectroscopy Processing, Reconstruction and Analysis.](http://arxiv.org/abs/2309.07178) | CloudBrain-NMR是一个智能云计算平台，用于NMR光谱处理和分析，通过在线访问，无需用户端安装任何程序，使用并行计算来加快处理时间。 |
| [^83] | [Optimal and Fair Encouragement Policy Evaluation and Learning.](http://arxiv.org/abs/2309.07176) | 本研究探讨了在关键领域中针对鼓励政策的最优和公平评估以及学习的问题，研究发现在人类不遵循治疗建议的情况下，最优策略规则只是建议。同时，针对治疗的异质性和公平考虑因素，决策者的权衡和决策规则也会发生变化。在社会服务领域，研究显示存在一个使用差距问题，那些最有可能受益的人却无法获得这些益服务。 |
| [^84] | [MELAGE: A purely python based Neuroimaging software (Neonatal).](http://arxiv.org/abs/2309.07175) | MELAGE是一款纯Python基于新生儿神经影像软件，具有卓越的适应性和丰富的功能。其核心特点是利用深度学习模块的半自动脑部提取工具，可精确高效地提取MRI和3D超声数据中的脑结构。该软件在图像分析、深度学习算法集成以及医学影像领域中具有广泛的应用前景。 |
| [^85] | [HurriCast: An Automatic Framework Using Machine Learning and Statistical Modeling for Hurricane Forecasting.](http://arxiv.org/abs/2309.07174) | 本研究提出了HurriCast，一种使用机器学习和统计建模的自动化框架，通过组合ARIMA模型和K-MEANS算法以更好地捕捉飓风趋势，并结合Autoencoder进行改进的飓风模拟，从而有效模拟历史飓风行为并提供详细的未来预测。这项研究通过利用全面且有选择性的数据集，丰富了对飓风模式的理解，并为风险管理策略提供了可操作的见解。 |
| [^86] | [Using Unsupervised and Supervised Learning and Digital Twin for Deep Convective Ice Storm Classification.](http://arxiv.org/abs/2309.07173) | 在这篇论文中，我们使用无监督学习、有监督学习和数字孪生技术来对深对流冰风暴进行分类。我们通过生成模拟的前瞻辐射计数据和“科学”隐藏变量的地球大气数字孪生技术来训练一个分类器，并且通过对科学隐藏变量进行K均值聚类来生成自动标记的数据集。 |
| [^87] | [Exploring Large Language Models for Ontology Alignment.](http://arxiv.org/abs/2309.07172) | 本文研究了大型语言模型在本体对齐中的应用，并发现它们有潜力在谨慎的框架和提示设计下超越现有的本体对齐系统。 |
| [^88] | [Overview of Human Activity Recognition Using Sensor Data.](http://arxiv.org/abs/2309.07170) | 本研究对传感器数据在人体活动识别中的应用进行了综述，总结了传感器数据的使用和HAR技术的应用，提出了几种常见的机器学习方法，并探讨了HAR面临的挑战。 |
| [^89] | [Frequency Convergence of Complexon Shift Operators.](http://arxiv.org/abs/2309.07169) | 本文研究了拓扑信号处理中复合子的可转移性，通过构造边际复合子和复合移位算子，研究其特征值和特征向量，并证明了复合子收敛时对应的复合移位算子的特征值会收敛到极限复合子的特征值。这些结果拓展了图信号处理框架。 |
| [^90] | [Goal Space Abstraction in Hierarchical Reinforcement Learning via Reachability Analysis.](http://arxiv.org/abs/2309.07168) | 本文介绍了一种通过可达性分析在分层强化学习中对目标空间进行抽象的方法，以自动发现符号目标表示。该方法通过将具有相似任务角色的环境状态集合在一起的紧密关联表示来发现子目标，在导航任务中表现出可解释性和数据效率。 |
| [^91] | [Systematic Review of Experimental Paradigms and Deep Neural Networks for Electroencephalography-Based Cognitive Workload Detection.](http://arxiv.org/abs/2309.07163) | 本文对基于EEG的认知负荷估计进行了系统综述，发现DNN在离线和实时分类方面都表现出色，但研究中较少采用可解释的神经网络模型。 |
| [^92] | [A Strong and Simple Deep Learning Baseline for BCI MI Decoding.](http://arxiv.org/abs/2309.07159) | 本论文提出了一种强大且简单的深度学习基线EEG-SimpleConv，用于BCI中的运动想象解码。与其他方法相比，EEG-SimpleConv表现至少同样好或更高效，具有强大的知识传递能力，推理时间较低。 |
| [^93] | [Compressed Real Numbers for AI: a case-study using a RISC-V CPU.](http://arxiv.org/abs/2309.07158) | 本文针对AI中的深度神经网络（DNN）使用较低精度进行训练，探讨了16位和8位压缩格式的应用。通过在计算之前解压压缩操作数，可以提高带宽使用和缓存效率。 |
| [^94] | [Distribution Grid Line Outage Identification with Unknown Pattern and Performance Guarantee.](http://arxiv.org/abs/2309.07157) | 本研究提出了一种实用而稳健的检测方法，只利用电压幅值，无需相角或功率流数据，通过数据驱动的方法学习中断后分布的参数，并通过添加Bregman散度约束来解决可行性问题，证明可以学习到最优参数。 |
| [^95] | [A Deep Dive into Sleep: Single-Channel EEG-Based Sleep Stage Classification with Model Interpretability.](http://arxiv.org/abs/2309.07156) | 本文提出了一种基于单通道脑电图的睡眠阶段分类方法，使用SE-Resnet-Bi-LSTM架构进行睡眠阶段的分类，并进行了模型解释性分析。在三个不同的数据集上进行了全面评估，取得了显著的准确率和宏F1得分。 |
| [^96] | [Finding Influencers in Complex Networks: An Effective Deep Reinforcement Learning Approach.](http://arxiv.org/abs/2309.07153) | 本文提出了一种有效的深度强化学习模型，通过将图神经网络作为编码器、强化学习作为解码器，实现了在复杂网络中寻找影响者的任务上的优越性能，超过了传统的最佳影响力最大化算法。 |
| [^97] | [Decoding visual brain representations from electroencephalography through Knowledge Distillation and latent diffusion models.](http://arxiv.org/abs/2309.07149) | 本研究提出了一种创新方法，利用脑电图数据解码人脑中的视觉表示。通过将EEG数据转换为频谱图并使用卷积神经网络进行训练，结合基于知识蒸馏的图像分类教师网络，我们的模型在图像分类和重建任务上表现出色。 |
| [^98] | [DGSD: Dynamical Graph Self-Distillation for EEG-Based Auditory Spatial Attention Detection.](http://arxiv.org/abs/2309.07147) | 本文提出了一种基于动态图自蒸馏的方法，用于处理具有非欧几里得特征的 EEG 信号，并提高听觉空间注意力检测的性能。 |
| [^99] | [ETP: Learning Transferable ECG Representations via ECG-Text Pre-training.](http://arxiv.org/abs/2309.07145) | 本论文介绍了ECG-Text预训练（ETP）框架，它通过将ECG信号与文本报告对齐，实现了跨模态ECG特征学习。ETP在线性评估和零样本分类任务中表现出色，并展示了其在跨模态ECG特征学习方面的鲁棒性和可迁移性。 |
| [^100] | [Design of Recognition and Evaluation System for Table Tennis Players' Motor Skills Based on Artificial Intelligence.](http://arxiv.org/abs/2309.07141) | 本研究基于人工智能，设计了一种用于识别和评估乒乓球运动员动作技能的系统，通过改进可穿戴设备，并利用特征工程、降维和不同评估指标的损失函数实现了动作的模式识别和层次化评估。 |
| [^101] | [Short-term power load forecasting method based on CNN-SAEDN-Res.](http://arxiv.org/abs/2309.07140) | 提出了一种基于CNN-SAEDN-Res的短期功率负荷预测方法，通过结合卷积神经网络、自注意力编码器-解码器网络和残差修正技术，能够有效处理带有非时序因素的负荷数据并提高预测精度。 |
| [^102] | [Self-Supervised Blind Source Separation via Multi-Encoder Autoencoders.](http://arxiv.org/abs/2309.07138) | 本论文提出了一种基于多编码器自编码器和自监督学习的方法，用于解决盲源分离问题。通过训练网络进行输入解码和重构，然后利用编码掩蔽技术进行源推断，同时引入路径分离损失以促进稀疏性。 |
| [^103] | [Bringing PDEs to JAX with forward and reverse modes automatic differentiation.](http://arxiv.org/abs/2309.07137) | 这篇论文介绍了如何将偏微分方程引入到JAX库中，通过使用Firedrake有限元库的接口来实现。而使用正向和逆向模式的自动微分计算方法，可以高效地组合有限元求解器和可微程序。 |
| [^104] | [Masked Transformer for Electrocardiogram Classification.](http://arxiv.org/abs/2309.07136) | 提出了一种基于掩码Transformer的ECG分类方法，命名为MTECG，扩展了掩码自动编码器在ECG时间序列上的应用，该方法在广泛的掩码比例下表现稳定良好，并进行了消融实验验证了重构目标的波动性、训练计划长度、逐层学习率衰减和DropPath率的重要性。 |
| [^105] | [EpiDeNet: An Energy-Efficient Approach to Seizure Detection for Embedded Systems.](http://arxiv.org/abs/2309.07135) | EpiDeNet是一种节能性癫痫检测方法，通过引入新的轻量级癫痫检测网络和结合灵敏度和特异度的损失函数(SSWCE)，成功应对了严重不平衡的数据集挑战，并在两个不同的数据集上实现了高达91.16％和92.00％的癫痫事件检测准确率。 |
| [^106] | [Entropy-based machine learning model for diagnosis and monitoring of Parkinson's Disease in smart IoT environment.](http://arxiv.org/abs/2309.07134) | 基于熵的机器学习模型在智能物联网环境中利用静息状态脑电信号进行帕金森病的诊断和监测。通过计算不同类型的熵，我们发现模糊熵在诊断和监测PD方面表现最佳。通过使用特定的特征组合，我们实现了高准确度的PD诊断。最重要的脑电信号频率范围和通道位置也被确定。 |
| [^107] | [Using wearable device-based machine learning models to autonomously identify older adults with poor cognition.](http://arxiv.org/abs/2309.07133) | 通过使用可穿戴设备的机器学习模型，可以在老年人正常生活条件下连续监测其认知水平，并能够预测认知能力较差的情况，为早期干预提供了替代方法。 |
| [^108] | [Optimal transport distances for directed, weighted graphs: a case study with cell-cell communication networks.](http://arxiv.org/abs/2309.07030) | 本文提出了两种基于最优输运的距离度量，用于比较有向图，并通过仿真图数据和单细胞RNA-seq数据推断的实际细胞间通讯图对其相对表现进行了评估。 |
| [^109] | [Uncertainty-aware Traffic Prediction under Missing Data.](http://arxiv.org/abs/2309.06800) | 本研究提出了一种考虑不确定性的交通预测方法，可以处理缺失数据和测量不确定性，并适用于风险敏感任务和决策导向问题。 |
| [^110] | [Deep Nonparametric Convexified Filtering for Computational Photography, Image Synthesis and Adversarial Defense.](http://arxiv.org/abs/2309.06724) | 该论文提出了一种使用深度非参数凸化滤波（DNCF）的通用框架，用于计算摄影领域中的图像恢复。DNCF具有强大的泛化性和对抗性图像处理的鲁棒性，同时能够实现实时的对抗性图像分类网络防御。 |
| [^111] | [Hybrid Algorithm Selection and Hyperparameter Tuning on Distributed Machine Learning Resources: A Hierarchical Agent-based Approach.](http://arxiv.org/abs/2309.06604) | 本文提出了一种基于代理的层级机器学习平台，用于选择分布式组织的机器学习算法并同时调整其超参数。该方法具有可伸缩性、灵活性和鲁棒性，并支持自动化和协同的功能。 |
| [^112] | [Correcting sampling biases via importancereweighting for spatial modeling.](http://arxiv.org/abs/2309.04824) | 通过重要性重新加权修正采样偏差的空间建模方法，可以有效解决分布偏移问题，实现目标错误的无偏估计。该方法在人工数据实验中表现出优势，使预测误差从7%降至2%，且样本量越大效果越好。 |
| [^113] | [Self-optimizing Feature Generation via Categorical Hashing Representation and Hierarchical Reinforcement Crossing.](http://arxiv.org/abs/2309.04612) | 通过分类哈希表示和分层强化交叉，提出了一种自优化特征生成的通用框架，解决了现有系统中的一些挑战，包括有意义、稳健和高效的生成。 |
| [^114] | [A Deep Learning Method for Sensitivity Enhancement of Deuterium Metabolic Imaging (DMI).](http://arxiv.org/abs/2309.04100) | 本研究提出了一种用于增强DMI敏感性的深度学习方法，通过训练卷积神经网络来估计低信噪比和失真的DMI FID的代谢物浓度，并通过MRI的边缘保护正则化进一步提高估计精度。实验结果显示，该方法在提高参数质量方面具有潜在的改进效果。 |
| [^115] | [eDKM: An Efficient and Accurate Train-time Weight Clustering for Large Language Models.](http://arxiv.org/abs/2309.00964) | 本文提出了一种内存高效的DKM实现，即eDKM，用于大型语言模型的高效准确的训练时权重聚类方法。它通过减小DKM的内存占用，解决了LLM压缩中的训练开销问题。 |
| [^116] | [GBE-MLZSL: A Group Bi-Enhancement Framework for Multi-Label Zero-Shot Learning.](http://arxiv.org/abs/2309.00923) | GBE-MLZSL是一种用于多标签零样本学习的群体双增强框架，通过整合局部和全局特征，解决了在推理未见类时模型失去主要组成部分把握的问题。 |
| [^117] | [DoRA: Domain-Based Self-Supervised Learning Framework for Low-Resource Real Estate Appraisal.](http://arxiv.org/abs/2309.00855) | DoRA是一种基于领域的低资源房地产评估自监督学习框架，通过学习未标记的房地产数据集合来减少主观性，并且融入领域知识。 |
| [^118] | [When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-making.](http://arxiv.org/abs/2308.11721) | 这项研究分析了一种特定类型的人工和算法合作，对于多个噪音模型来说，将选择项目的子集大小$k$设置在$[2, n-1]$范围内能够最大化最终选择最佳项目的概率。 |
| [^119] | [Machine Learning-Assisted Discovery of Novel Reactor Designs via CFD-Coupled Multi-fidelity Bayesian Optimisation.](http://arxiv.org/abs/2308.08841) | 本研究通过应用多精度贝叶斯优化方法，结合参数化网格化和模拟，提出了两种新的螺旋管参数化方法，用于发现新的反应器设计。这种方法能够处理高维度和复杂的优化问题，并克服了没有梯度的非局部优化困难。 |
| [^120] | [Neural Categorical Priors for Physics-Based Character Control.](http://arxiv.org/abs/2308.07200) | 本文提出了一种新的基于神经分类先验的学习框架，用于控制基于物理的角色，利用强化学习和离散信息瓶颈，生成高质量逼真的行为。 |
| [^121] | [Learning nonparametric DAGs with incremental information via high-order HSIC.](http://arxiv.org/abs/2308.05969) | 本文提出了一个基于高阶HSIC的方法，在学习Bayesian网络中解决了局部变量同时具有直接和间接依赖关系的问题，通过确定子集和两阶段算法来进行局部修正，取得了良好的效果。 |
| [^122] | [Transferable Graph Neural Fingerprint Models for Quick Response to Future Bio-Threats.](http://arxiv.org/abs/2308.01921) | 该论文提出了一种可转移的图神经指纹模型，用于快速应对未来的生物威胁。通过利用包含30万种候选药物和23个冠状病毒蛋白靶的COVID-19药物对接数据集，训练了高通量虚拟COVID-19药物筛选的图神经指纹模型。与传统指纹方法相比，该模型在对接得分上具有较高的预测准确性，并且提出了可转移的图神经指纹方法，能够适用于未知的靶点。 |
| [^123] | [Benchmarking Jetson Edge Devices with an End-to-end Video-based Anomaly Detection System.](http://arxiv.org/abs/2307.16834) | 本论文实现了一个端到端的视频异常检测系统，通过从监控视频输入进行犯罪现场异常检测，并在多个Jetson边缘设备上部署和运行。这是对Jetson平台在深度学习算法执行方面性能的基准测试分析的创新。 |
| [^124] | [GOKU-UI: Ubiquitous Inference through Attention and Multiple Shooting for Continuous-time Generative Models.](http://arxiv.org/abs/2307.05735) | GOKU-UI是一种普适推理的SciML生成模型，通过关注机制和多射击训练策略，在包括随机微分方程在内的各种微分方程中展现出了优异的性能表现。它具有显著的数据效率，并在合成和实证数据集上优于所有基线模型。 |
| [^125] | [Generating Parametric BRDFs from Natural Language Descriptions.](http://arxiv.org/abs/2306.15679) | 这项研究开发了一个模型，可以根据描述性的文本提示生成参数化的BRDFs，为艺术性地创作3D环境提供了一种新的方法。 |
| [^126] | [EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought.](http://arxiv.org/abs/2305.15021) | EmbodiedGPT是一种端到端的多模态基础模型，通过思维链预训练的方式，赋予具有多模态理解和执行能力的实体代理人。 |
| [^127] | [SpikeCP: Delay-Adaptive Reliable Spiking Neural Networks via Conformal Prediction.](http://arxiv.org/abs/2305.11322) | 这篇论文提出了一种新的脉冲神经网络模型，能够通过极限预测实现自适应的推断延迟，从而节约能源与提高可靠性。 |
| [^128] | [Deep Spatiotemporal Clustering: A Temporal Clustering Approach for Multi-dimensional Climate Data.](http://arxiv.org/abs/2304.14541) | 该论文提出了一种新颖的算法Deep Spatiotemporal Clustering (DSC) ，用于使用无监督深度学习方法进行高维时空数据的时间聚类，DSC利用自动编码器集成CNN-RNN层学习时空数据的潜在表示，并优化聚类损失和数据重建损失以改善聚类分配和非线性重建能力。 |
| [^129] | [TempEE: Temporal-Spatial Parallel Transformer for Radar Echo Extrapolation Beyond Auto-Regression.](http://arxiv.org/abs/2304.14131) | 本文提出了一个新型的雷达回波外推算法TempEE，该算法利用时空相关特征和Transformer技术，通过从多帧回波图像中提取特征，准确地表示了降水的非平稳运动过程，克服了传统算法的局限性，在三个真实数据集上表现出色。 |
| [^130] | [Evaluation of Parameter-based Attacks against Embedded Neural Networks with Laser Injection.](http://arxiv.org/abs/2304.12876) | 本研究首次成功地在32位Cortex-M微控制器上使用激光注入进行了比特翻转攻击，强调了典型深度神经网络的缺乏强健性。 |
| [^131] | [Contrastive Tuning: A Little Help to Make Masked Autoencoders Forget.](http://arxiv.org/abs/2304.10520) | 本文提出了一种新方法：掩码自编码器对比调整(MAE-CT)，利用最近邻对比学习（NNCLR）在标记数据较少的情况下实现下游分类，该方法可以将图像的丰富特征聚类成对象的语义聚类。 |
| [^132] | [A Latent Space Theory for Emergent Abilities in Large Language Models.](http://arxiv.org/abs/2304.09960) | 本文探讨了大规模语言模型中的贝叶斯推断和稀疏联合分布，证明了LLMs能够完成语言理解、上下文学习、思路启发以及有效指令微调的新兴能力。 |
| [^133] | [Weighted Siamese Network to Predict the Time to Onset of Alzheimer's Disease from MRI Images.](http://arxiv.org/abs/2304.07097) | 本文提出使用加权连体网络对进展性MCI患者进行序数分类，以预测他们距离严重AD阶段的距离，从而实现更准确的早期检测。 |
| [^134] | [Conformal Regression in Calorie Prediction for Team Jumbo-Visma.](http://arxiv.org/abs/2304.03778) | 本文提出一种新的符合性回归方法，通过预测动力和速度来为自行车比赛中的每个骑手提供卡路里需求的估计。 |
| [^135] | [Domain Generalization for Crop Segmentation with Knowledge Distillation.](http://arxiv.org/abs/2304.01029) | 本文针对作物分割问题提出了一种使用知识蒸馏的方法来增强域泛化能力，通过将来自源域的模型集合的知识传递给学生模型，实现了对新的作物和环境条件的泛化处理。 |
| [^136] | [Fixed points of arbitrarily deep 1-dimensional neural networks.](http://arxiv.org/abs/2303.12814) | 本研究发现，具有对数S型激活函数的任意深度的一维神经网络最多只有三个不动点，为深度神经网络的应用和理论之间构建了一个必要的桥梁。 |
| [^137] | [MAHTM: A Multi-Agent Framework for Hierarchical Transactive Microgrids.](http://arxiv.org/abs/2303.08447) | 该论文提出了一个多智能体强化学习框架，用于管理微电网中的能源交易。该框架通过最小化碳足迹，同时平衡可再生能源和传统能源的消费和生产，并考虑能源变化。 |
| [^138] | [Kernel Conditional Moment Constraints for Confounding Robust Inference.](http://arxiv.org/abs/2302.13348) | 本文提出了一种应对混淆因素的核条件矩约束方法，通过利用核方法得到的条件矩约束的近似估计器，实现了对政策评估的尖锐下界估计，并能对经典边际敏感度模型进行新颖扩展。 |
| [^139] | [Deep reinforced learning heuristic tested on spin-glass ground states: The larger picture.](http://arxiv.org/abs/2302.10848) | 该研究提出了一种基于深度强化学习的启发式方法，用于增强组合优化过程，并在自旋玻璃基态问题上取得了改进结果。研究结果表明，相对于传统方法如模拟退火或并行退火，强化学习方法在提供相当质量的结果之前减少了运行时间。 |
| [^140] | [Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks.](http://arxiv.org/abs/2302.07260) | 本文提出了一个基于带随机先验的神经网络的深度学习框架用于高维输出的贝叶斯优化，可有效地处理全局优化问题，即使在高维度向量空间或无限维函数空间中也能近似功能关系。 |
| [^141] | [On a continuous time model of gradient descent dynamics and instability in deep learning.](http://arxiv.org/abs/2302.01952) | 本文提出了一个连续时间流模型——主要流（PF），通过对Hessian矩阵特征分解的依赖，捕捉到了梯度下降中的发散和振荡行为、逃逸局部极小值和鞍点的连续性流，并解释了深度学习中的稳定边缘现象。通过对不稳定性的新理解，提出了一种学习率适应方法，可以控制训练稳定性和测试集评估性能之间的权衡。 |
| [^142] | [Reasoning with Language Model Prompting: A Survey.](http://arxiv.org/abs/2212.09597) | 本文提供了使用语言模型提示进行推理的前沿研究综合调查。讨论了新兴推理能力出现的潜在原因，并提供系统资源帮助初学者。 |
| [^143] | [TargetCall: Eliminating the Wasted Computation in Basecalling via Pre-Basecalling Filtering.](http://arxiv.org/abs/2212.04953) | TargetCall通过预基调过滤，消除了basecalling中的浪费计算，提高了基因组分析流程的效率。 |
| [^144] | [Reward is not Necessary: How to Create a Compositional Self-Preserving Agent for Life-Long Learning.](http://arxiv.org/abs/2211.10851) | 这项研究表明，我们可以使用内在动机衡量标准而不依赖于奖励来创建一个具有自我保护能力的智能体。 |
| [^145] | [Efficient Spatially Sparse Inference for Conditional GANs and Diffusion Models.](http://arxiv.org/abs/2211.02048) | 提出了空间稀疏推理（SSI）的通用技术，该技术选择性地为编辑区域执行计算并加速各种生成模型，包括条件GAN和扩散模型。通过缓存和重复使用原始图像的特征图，我们将卷积滤波器稀疏地应用于编辑区域，并在未编辑的区域中重复使用缓存特征，从而通过约$1\%$的区域编辑来减少计算资源的浪费。 |
| [^146] | [ConSpec: honing in on critical steps for rapid learning and generalization in RL.](http://arxiv.org/abs/2210.05845) | ConSpec是一个新的强化学习算法，通过离线对比学习来确定任务中的关键步骤，实现快速学习和泛化。该算法通过学习关键步骤的原型，并在当前状态匹配时提供内在奖励，具有快速识别关键步骤和可解释的信用分配的优势。 |
| [^147] | [BAFFLE: Backdoor Attack in Offline Reinforcement Learning.](http://arxiv.org/abs/2210.04688) | 本文研究离线增强学习中的后门攻击，通过向数据中添加扰动，使得智能体在注入触发器的观测值上采取低奖励动作，从而提出了BAFFLE方法。 |
| [^148] | [LambdaKG: A Library for Pre-trained Language Model-Based Knowledge Graph Embeddings.](http://arxiv.org/abs/2210.00305) | LambdaKG是一个基于预训练语言模型的知识图谱嵌入库，提供了多个预训练语言模型和支持多种任务，如知识图谱补全、问答、推荐和知识探索。 |
| [^149] | [TrojViT: Trojan Insertion in Vision Transformers.](http://arxiv.org/abs/2208.13049) | 本文提出了一种针对视觉Transformer的隐秘实用的后门攻击方法TrojViT，通过补丁级触发器将一些易受攻击位组成的参数建立为特洛伊木马。 |
| [^150] | [Gaussian Process Surrogate Models for Neural Networks.](http://arxiv.org/abs/2208.06028) | 通过利用高斯过程构建神经网络的代理模型，我们解决了深度学习系统行为预测和架构算法选择的问题，并展示了模型在捕捉现象、确定关键数据点和预测泛化能力方面的实际应用价值。 |
| [^151] | [Machine Learning and Computer Vision Techniques in Bee Monitoring Applications.](http://arxiv.org/abs/2208.00085) | 本文介绍了机器学习和计算机视觉在蜜蜂监测中的最新应用，展示了自动化蜜蜂计数算法的潜力，并希望能够激发其他科学家的灵感和兴趣。 |
| [^152] | [An Adaptive Federated Relevance Framework for Spatial Temporal Graph Learning.](http://arxiv.org/abs/2206.03420) | 该论文提出了一种适应性联邦相关框架，用于处理时空数据中的复杂预测任务，并解决了同时融合空间信息的相互依赖性和动态的时间变化的挑战。 |
| [^153] | [Meta-Learning Regrasping Strategies for Physical-Agnostic Objects.](http://arxiv.org/abs/2205.11110) | 本研究提出了一种元学习算法，ConDex，用于自主识别具有未知物理属性的非均匀对象，并实现精确的抓取点估计。与现有方法相比，ConDex在性能上表现出优势，并且生成了两个新的对象数据集用于进一步研究。 |
| [^154] | [Model-free Learning of Regions of Attraction via Recurrent Sets.](http://arxiv.org/abs/2204.10372) | 提出了一种无模型学习渐近稳定平衡点吸引域的方法，通过学习满足循环包含性概念的集合，并利用循环性质计算吸引域的内部逼近。 |
| [^155] | [An Optimal Control Method to Compute the Most Likely Transition Path for Stochastic Dynamical Systems with Jumps.](http://arxiv.org/abs/2203.16874) | 本论文提出了一种用于计算具有跳跃性的随机动力学系统最可能转换路径的最优控制方法，通过解决一个最优控制问题和运用神经网络方法来克服相关速率函数无法明确表示的挑战。 |
| [^156] | [Model Reprogramming: Resource-Efficient Cross-Domain Machine Learning.](http://arxiv.org/abs/2202.10629) | 模型重新编程是一种资源高效的跨领域机器学习方法，通过重新利用和重用预训练模型，无需模型细调即可在目标领域解决任务。这种方法在许多应用中优于迁移学习和从头训练。 |
| [^157] | [Random Feature Amplification: Feature Learning and Generalization in Neural Networks.](http://arxiv.org/abs/2202.07626) | 本文通过分析在随机初始化后通过逻辑损失函数进行梯度下降训练的两层ReLU网络在特征学习过程中的表征，证明了对于具有二元标签的由XOR-like函数生成的数据，尽管线性分类器在该分布上无法更好地工作，但该网络的泛化误差接近于标签噪声率。通过一种新颖的证明技巧，揭示了初始化时大多数神经元作为随机特征，随后通过梯度下降动力学将这些弱的随机特征放大为有用的特征。 |
| [^158] | [Benign Overfitting without Linearity: Neural Network Classifiers Trained by Gradient Descent for Noisy Linear Data.](http://arxiv.org/abs/2202.05928) | 本文研究了使用梯度下降训练的神经网络在泛化时能够很好应对噪声数据的良性过拟合现象。研究表明，在特定条件下，神经网络能够将训练误差降至零并完美地适应带有噪声标签的数据，并同时达到最优的测试误差。 |
| [^159] | [PolicyCleanse: Backdoor Detection and Mitigation in Reinforcement Learning.](http://arxiv.org/abs/2202.03609) | 本文提出了PolicyCleanse方法，用于检测和缓解多智能体强化学习系统中的后门攻击。该方法基于激活的特洛伊智能体累积奖励的下降特性进行检测，并尝试缓解其特洛伊行为。 |
| [^160] | [Speeding up Learning Quantum States through Group Equivariant Convolutional Quantum Ans\"atze.](http://arxiv.org/abs/2112.07611) | 通过建立$S_n$-等变卷积量子Ansatze，我们证明了其能够在具有SU($d$)对称性的广泛量子机器学习问题中生成任意幺正矩阵，同时验证了4-local SU($d$)对称幺正矩阵的可实现性。 |
| [^161] | [Pareto Adversarial Robustness: Balancing Spatial Robustness and Sensitivity-based Robustness.](http://arxiv.org/abs/2111.01996) | 本文提出了Pareto Adversarial Robustness策略，通过整合空间鲁棒性方法和基于敏感性的鲁棒性，实现了通用对抗鲁棒性。同时，从鲁棒表示的角度提供了自然准确性、敏感性鲁棒性和空间鲁棒性之间的关系。 |
| [^162] | [Discrete Acoustic Space for an Efficient Sampling in Neural Text-To-Speech.](http://arxiv.org/abs/2110.12539) | 我们提出了一种使用拆分向量量化变分自编码器(SVQ-VAE)架构的离散声学空间，相比于之前的架构，该模型既保留了使用话语级限制的好处，又具有足够小的离散潜在空间以从文本进行高效预测。在实验中，我们证明了SVQ-VAE在自然度方面明显优于其他模型，并且可从文本进行预测，减少了合成和录音之间的差距。 |
| [^163] | [Joint Community Detection and Rotational Synchronization via Semidefinite Programming.](http://arxiv.org/abs/2105.06031) | 本文提出了一种通过半定规划来联合社区检测和旋转同步的方法，在存在异构数据的情况下，能够准确恢复出旋转和聚类身份，并通过数值实验证明了其有效性和准确恢复性的相变现象。 |
| [^164] | [BAARD: Blocking Adversarial Examples by Testing for Applicability, Reliability and Decidability.](http://arxiv.org/abs/2105.00495) | 该论文提出了一种新的对抗性样本检测方法，借鉴了化学信息学中的适用性域概念，通过判断样本是否在分类器的域内来阻挡对抗性样本。 |
| [^165] | [Survival Estimation for Missing not at Random Censoring Indicators based on Copula Models.](http://arxiv.org/abs/2009.01726) | 本论文提出了一种基于Copula模型的新的估计器，用于处理缺失非随机截尾指标下的条件生存函数估计问题。通过模拟实验和真实数据分析，我们验证了该估计器的实用性。 |
| [^166] | [On the complexity of finding a local minimizer of a quadratic function over a polytope.](http://arxiv.org/abs/2008.05558) | 除非P=NP，否则不存在一个多项式时间算法，能够找到一个在欧几里得距离$c^n$（对于任意常数$c \ge 0$）内的二次函数在多面体上的局部最小化点。 |
| [^167] | [On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems.](http://arxiv.org/abs/1906.00331) | 该论文提出了一种用于解决非凸-凹极小极大问题的梯度下降算法，并进行了复杂性分析，证明了该算法可以高效地找到函数的稳定点。该研究对于非凸-凹极小极大问题的非渐进分析是首次进行的。 |

# 详细

[^1]: 从头开始利用基于扩散的数据增强方法来提升无监督对比学习

    Boosting Unsupervised Contrastive Learning Using Diffusion-Based Data Augmentation From Scratch. (arXiv:2309.07909v1 [cs.LG])

    [http://arxiv.org/abs/2309.07909](http://arxiv.org/abs/2309.07909)

    本文介绍了一种新颖且高效的基于扩散的数据增强技术DiffAug，通过扩散步骤实现增强数据和原始数据共享平滑的潜空间，旨在提高无监督对比学习的效果和效率。

    

    无监督对比学习方法近年来取得了显著的改进，尤其是通过数据增强策略产生稳健且具有泛化能力的表示。然而，现有的数据增强方法往往过于依赖先验知识或外部数据，限制了它们的有效性和效率。此外，大多数现有的数据增强策略在转换到其他研究领域时应用受限，特别是在科学相关数据的情况下。这种限制来源于这些领域中先验知识和标记数据的匮乏。为了解决这些问题，我们引入了一种新颖而高效的基于扩散的数据增强技术——DiffAug。DiffAug通过扩散步骤确保增强数据和原始数据共享一个平滑的潜空间。与传统方法不同，DiffAug首先对数据进行增强，使增强数据和原始数据在潜空间上平滑。

    Unsupervised contrastive learning methods have recently seen significant improvements, particularly through data augmentation strategies that aim to produce robust and generalizable representations. However, prevailing data augmentation methods, whether hand designed or based on foundation models, tend to rely heavily on prior knowledge or external data. This dependence often compromises their effectiveness and efficiency. Furthermore, the applicability of most existing data augmentation strategies is limited when transitioning to other research domains, especially science-related data. This limitation stems from the paucity of prior knowledge and labeled data available in these domains. To address these challenges, we introduce DiffAug-a novel and efficient Diffusion-based data Augmentation technique. DiffAug aims to ensure that the augmented and original data share a smoothed latent space, which is achieved through diffusion steps. Uniquely, unlike traditional methods, DiffAug first 
    
[^2]: 可行的全身手物交互合成方法

    Physically Plausible Full-Body Hand-Object Interaction Synthesis. (arXiv:2309.07907v1 [cs.RO])

    [http://arxiv.org/abs/2309.07907](http://arxiv.org/abs/2309.07907)

    提出了一种基于物理的方法，用于合成全身环境中的灵巧手物交互。方法采用强化学习和物理仿真，通过学习先验技能来控制手物交互，解决了数据驱动方法的局限性。

    

    我们提出了一种基于物理的方法，用于在全身环境中合成灵巧的手物交互。虽然最近的进展在处理人物-物体交互的具体方面取得了一些成果，但综合性的基于物理的方法仍然是一个挑战。现有方法通常专注于交互过程的孤立部分，并依赖于数据驱动技术，可能会产生伪影。相反，我们提出的方法采用了强化学习（RL）和物理仿真来缓解数据驱动方法的局限性。通过一个分层框架，我们首先在解耦的环境中学习身体和手部运动的技能先验。通用的技能先验将潜在的技能嵌入解码为底层部分的运动。然后，一个高层策略在这些预训练的潜空间中控制手物交互，受到握持和3D目标轨迹跟踪的任务目标的指导。它是使用一种新的奖励函数进行训练的，该函数结合了一种

    We propose a physics-based method for synthesizing dexterous hand-object interactions in a full-body setting. While recent advancements have addressed specific facets of human-object interactions, a comprehensive physics-based approach remains a challenge. Existing methods often focus on isolated segments of the interaction process and rely on data-driven techniques that may result in artifacts. In contrast, our proposed method embraces reinforcement learning (RL) and physics simulation to mitigate the limitations of data-driven approaches. Through a hierarchical framework, we first learn skill priors for both body and hand movements in a decoupled setting. The generic skill priors learn to decode a latent skill embedding into the motion of the underlying part. A high-level policy then controls hand-object interactions in these pretrained latent spaces, guided by task objectives of grasping and 3D target trajectory following. It is trained using a novel reward function that combines an
    
[^3]: 改进具有硬约束的物理信息DeepONets

    Improving physics-informed DeepONets with hard constraints. (arXiv:2309.07899v1 [cs.LG])

    [http://arxiv.org/abs/2309.07899](http://arxiv.org/abs/2309.07899)

    本研究提出了一种改进的物理信息深度学习策略，消除了对初始条件的学习需求，并确保在多次应用时得到的函数是连续的。

    

    当前的物理信息神经网络（标准或操作符）仍然依赖于准确地学习所解决系统的初始条件。相比之下，标准的数值方法在不需要学习这些条件的情况下演化这些初始条件。在这项研究中，我们提出改进当前的物理信息深度学习策略，使得不需要学习初始条件，并且将其准确地表示在预测的解中。此外，该方法保证当将DeepONet多次应用于时间步长解上时，得到的函数是连续的。

    Current physics-informed (standard or operator) neural networks still rely on accurately learning the initial conditions of the system they are solving. In contrast, standard numerical methods evolve such initial conditions without needing to learn these. In this study, we propose to improve current physics-informed deep learning strategies such that initial conditions do not need to be learned and are represented exactly in the predicted solution. Moreover, this method guarantees that when a DeepONet is applied multiple times to time step a solution, the resulting function is continuous.
    
[^4]: 从过去的实验中选择代理指标

    Choosing a Proxy Metric from Past Experiments. (arXiv:2309.07893v1 [stat.ME])

    [http://arxiv.org/abs/2309.07893](http://arxiv.org/abs/2309.07893)

    本文介绍了一种新的统计框架，用于在同质种群的随机实验中选择最优的代理指标。该方法通过投资组合优化问题将最优代理指标的构建进行了归约，并对观察到的治疗效果进行了降噪处理。

    

    在许多随机实验中，往往很难或不可行地测量长期指标（即感兴趣的主要结果）。这些长期指标往往反应变化较慢，且噪声较大，使得在短期实验中难以准确估计。一种常见的替代方法是测量几个短期代理指标，希望它们能够紧密追踪长期指标，从而在近期有效地指导决策。我们引入了一个新的统计框架，用于定义和构建一个适用于同质种群随机实验的最优代理指标。我们的方法首先将给定实验中最优代理指标的构建归约为一个投资组合优化问题，该问题取决于考虑中实验的真实潜在治疗效果和噪声水平。然后我们对长期指标和一组代理的观察到的治疗效果进行降噪处理。

    In many randomized experiments, the treatment effect of the long-term metric (i.e. the primary outcome of interest) is often difficult or infeasible to measure. Such long-term metrics are often slow to react to changes and sufficiently noisy they are challenging to faithfully estimate in short-horizon experiments. A common alternative is to measure several short-term proxy metrics in the hope they closely track the long-term metric -- so they can be used to effectively guide decision-making in the near-term. We introduce a new statistical framework to both define and construct an optimal proxy metric for use in a homogeneous population of randomized experiments. Our procedure first reduces the construction of an optimal proxy metric in a given experiment to a portfolio optimization problem which depends on the true latent treatment effects and noise level of experiment under consideration. We then denoise the observed treatment effects of the long-term metric and a set of proxies in a 
    
[^5]: 一种新颖的基于局部全局特征融合框架的体重运动识别方法与压力传感器

    A Novel Local-Global Feature Fusion Framework for Body-weight Exercise Recognition with Pressure Mapping Sensors. (arXiv:2309.07888v1 [cs.CV])

    [http://arxiv.org/abs/2309.07888](http://arxiv.org/abs/2309.07888)

    这项研究提出了一种新颖的基于局部全局特征融合框架的体重运动识别方法，利用压力传感器和图像处理技术结合局部和全局特征，在考虑物理约束的情况下，以11%的提升改善了运动识别的性能。

    

    我们提出了一种新颖的基于局部全局特征融合框架的体重运动识别方法，利用基于地面的动态压力图。相比于现有主要集中于全局特征提取的深度神经网络研究，我们的方法旨在利用图像处理技术和YOLO目标检测来结合局部和全局特征，定位不同身体部位的压力分布并考虑物理约束。我们提出的局部特征提取方法生成了两组高级局部特征，包括裁剪后的压力分布图与数值特征，如角度方向、垫子上的位置和压力面积。此外，我们采用了知识蒸馏来规范化保留全局特征提取的知识，并提高运动识别的性能。实验结果表明，我们的方法在运动识别的F1得分上取得了明显的11%改善。

    We present a novel local-global feature fusion framework for body-weight exercise recognition with floor-based dynamic pressure maps. One step further from the existing studies using deep neural networks mainly focusing on global feature extraction, the proposed framework aims to combine local and global features using image processing techniques and the YOLO object detection to localize pressure profiles from different body parts and consider physical constraints. The proposed local feature extraction method generates two sets of high-level local features consisting of cropped pressure mapping and numerical features such as angular orientation, location on the mat, and pressure area. In addition, we adopt a knowledge distillation for regularization to preserve the knowledge of the global feature extraction and improve the performance of the exercise recognition. Our experimental results demonstrate a notable 11 percent improvement in F1 score for exercise recognition while preserving 
    
[^6]: 关于一种广义KMM类型密度比估计方法的一些注记

    Some notes concerning a generalized KMM-type optimization method for density ratio estimation. (arXiv:2309.07887v1 [cs.LG])

    [http://arxiv.org/abs/2309.07887](http://arxiv.org/abs/2309.07887)

    本文介绍了一种用于密度比估计的广义KMM类型优化算法，通过构建适当的损失函数，扩展了现有方法以适应更一般的情况，并包括对训练数据和测试数据子集相对于密度比的估计。

    

    在本文中，我们引入了用于密度比估计任务的新优化算法。更具体地说，我们考虑使用构建合适的损失函数来扩展著名的KMM方法，以涵盖更一般的情况，包括对训练数据和测试数据子集相对于密度比进行估计。相关代码可以在https://github.com/CDAlecsa/Generalized-KMM找到。

    In the present paper we introduce new optimization algorithms for the task of density ratio estimation. More precisely, we consider extending the well-known KMM method using the construction of a suitable loss function, in order to encompass more general situations involving the estimation of density ratio with respect to subsets of the training data and test data, respectively. The associated codes can be found at https://github.com/CDAlecsa/Generalized-KMM.
    
[^7]: Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])

    Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])

    [http://arxiv.org/abs/2309.07867](http://arxiv.org/abs/2309.07867)

    beta扩散是一种新型生成模型方法，通过引入去掩盖和去噪的技术，利用缩放和偏移的beta分布进行乘法转换，实现在有界范围内生成数据。相比于传统的基于扩散的生成模型，它通过KL散度上界进行优化，证明了效果更好。

    

    我们引入了beta扩散，一种将去掩盖和去噪集成到一起的新型生成建模方法，用于在有界范围内生成数据。使用了缩放和偏移的beta分布，beta扩散利用了随时间的乘法转换来创建正向和反向的扩散过程，同时维持着正向边缘分布和反向条件分布，给定任意时间点的数据。与传统的基于扩散的生成模型不同，传统模型依赖于加性高斯噪声和重新加权的证据下界（ELBO），beta扩散是乘法的，并且通过从KL散度的凸性推导出来的KL散度上界（KLUB）进行优化。我们证明了所提出的KLUB相对于负ELBO来说对于优化beta扩散更加有效，负ELBO也可以作为相同KL散度的KLUB，只是其两个参数交换了位置。beta扩散的损失函数以Bregman散度为指标来表示。

    We introduce beta diffusion, a novel generative modeling method that integrates demasking and denoising to generate data within bounded ranges. Using scaled and shifted beta distributions, beta diffusion utilizes multiplicative transitions over time to create both forward and reverse diffusion processes, maintaining beta distributions in both the forward marginals and the reverse conditionals, given the data at any point in time. Unlike traditional diffusion-based generative models relying on additive Gaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion is multiplicative and optimized with KL-divergence upper bounds (KLUBs) derived from the convexity of the KL divergence. We demonstrate that the proposed KLUBs are more effective for optimizing beta diffusion compared to negative ELBOs, which can also be derived as the KLUBs of the same KL divergence with its two arguments swapped. The loss function of beta diffusion, expressed in terms of Bregman divergence, furt
    
[^8]: 识别机器学习对称性的群论结构

    Identifying the Group-Theoretic Structure of Machine-Learned Symmetries. (arXiv:2309.07860v1 [hep-ph])

    [http://arxiv.org/abs/2309.07860](http://arxiv.org/abs/2309.07860)

    本研究提出了一种用于检查和识别机器学习对称性群论结构的方法。通过设计损失函数，可以在对称性发现的深度学习阶段或后处理阶段探测子代数结构，并应用于粒子物理学中的SU(3)和SU(5)非阿贝尔规范对称性的自发破缺。

    

    最近，深度学习成功地用于推导保持重要物理量不变的对称性变换。这些技术完全不考虑对已发现的对称性进行识别，将其推迟到后续阶段。在本文中，我们提出了一种检查和识别机器学习对称性的群论结构的方法。我们设计了损失函数，可以在对称性发现的深度学习阶段或后续后处理阶段探测子代数结构的方式。我们通过U(n)李群家族的例子说明了这些新方法，获得相应的子代数分解。作为粒子物理学的应用，我们展示了在非阿贝尔规范对称性如SU(3)和SU(5)自发破缺后识别残余对称性。

    Deep learning was recently successfully used in deriving symmetry transformations that preserve important physics quantities. Being completely agnostic, these techniques postpone the identification of the discovered symmetries to a later stage. In this letter we propose methods for examining and identifying the group-theoretic structure of such machine-learned symmetries. We design loss functions which probe the subalgebra structure either during the deep learning stage of symmetry discovery or in a subsequent post-processing stage. We illustrate the new methods with examples from the U(n) Lie group family, obtaining the respective subalgebra decompositions. As an application to particle physics, we demonstrate the identification of the residual symmetries after the spontaneous breaking of non-Abelian gauge symmetries like SU(3) and SU(5) which are commonly used in model building.
    
[^9]: 学习如何为固定点优化算法预热

    Learning to Warm-Start Fixed-Point Optimization Algorithms. (arXiv:2309.07835v1 [math.OC])

    [http://arxiv.org/abs/2309.07835](http://arxiv.org/abs/2309.07835)

    该论文提出了一个机器学习框架，用于为固定点优化算法提供预热。该框架使用神经网络预测问题参数的预热起始点，并通过固定点迭代来优化结果。实验结果表明，该方法可以显著减少迭代次数。

    

    我们引入了一个机器学习框架来为固定点优化算法进行预热。我们的架构由一个神经网络组成，将问题参数映射到预热起始点，然后进行一定数量的固定点迭代。我们提出了两个损失函数，旨在最小化固定点残差或与基准解的距离。通过这种方式，神经网络预测了预热起始点，最终目标是最小化下游损失。我们架构的一个重要特点是其灵活性，可以为运行任意步数的固定点算法预测预热起始点，而不仅限于它训练的步数。我们为常见的固定点算子类（收缩、线性收敛和平均值）提供了PAC-Bayes泛化界限。将此框架应用于控制、统计和信号处理的众所周知的应用中，我们观察到了数量显著减少。

    We introduce a machine-learning framework to warm-start fixed-point optimization algorithms. Our architecture consists of a neural network mapping problem parameters to warm starts, followed by a predefined number of fixed-point iterations. We propose two loss functions designed to either minimize the fixed-point residual or the distance to a ground truth solution. In this way, the neural network predicts warm starts with the end-to-end goal of minimizing the downstream loss. An important feature of our architecture is its flexibility, in that it can predict a warm start for fixed-point algorithms run for any number of steps, without being limited to the number of steps it has been trained on. We provide PAC-Bayes generalization bounds on unseen data for common classes of fixed-point operators: contractive, linearly convergent, and averaged. Applying this framework to well-known applications in control, statistics, and signal processing, we observe a significant reduction in the number
    
[^10]: 基于知识图谱的细胞信号分析的有向散射

    Directed Scattering for Knowledge Graph-based Cellular Signaling Analysis. (arXiv:2309.07813v1 [cs.LG])

    [http://arxiv.org/abs/2309.07813](http://arxiv.org/abs/2309.07813)

    本论文提出了一种新的框架，称为有向散射自编码器（DSAE），用于基于知识图谱的细胞信号分析。该框架结合了有向版本的几何散射变换、自编码器的非线性降维特性和双曲空间的几何属性，可以学习到细胞信号网络中的潜在分层信息，并在细胞信号网络推断等科学任务中取得了优异的性能。

    

    有向图是许多现象的自然模型，特别是科学知识图谱，如分子相互作用或化学反应网络，用于定义细胞信号关系。在这些情况下，源节点通常具有与汇节点不同的生物物理性质。由于它们的有序和单向关系，许多此类网络还具有分层和多尺度结构。然而，大多数机器学习方法在执行节点级和边级任务时并不考虑这些属性，因此对于细胞信号网络推断等科学任务的利用效果不佳。我们提出了一种新的框架，称为有向散射自编码器（DSAE），它使用有向版本的几何散射变换，结合自编码器的非线性降维特性和双曲空间的几何属性，来学习潜在的分层信息。我们证明了这种方法的性能优于当前方法。

    Directed graphs are a natural model for many phenomena, in particular scientific knowledge graphs such as molecular interaction or chemical reaction networks that define cellular signaling relationships. In these situations, source nodes typically have distinct biophysical properties from sinks. Due to their ordered and unidirectional relationships, many such networks also have hierarchical and multiscale structure. However, the majority of methods performing node- and edge-level tasks in machine learning do not take these properties into account, and thus have not been leveraged effectively for scientific tasks such as cellular signaling network inference. We propose a new framework called Directed Scattering Autoencoder (DSAE) which uses a directed version of a geometric scattering transform, combined with the non-linear dimensionality reduction properties of an autoencoder and the geometric properties of the hyperbolic space to learn latent hierarchies. We show this method outperfor
    
[^11]: 癌症临床试验资格标准的文本分类

    Text Classification of Cancer Clinical Trial Eligibility Criteria. (arXiv:2309.07812v1 [cs.CL])

    [http://arxiv.org/abs/2309.07812](http://arxiv.org/abs/2309.07812)

    本文研究了癌症临床试验中常见的排除标准，通过应用文本分类方法和预训练的BERT模型，证明了自动分类排除标准的可行性，并展示了专门为临床试验设计的预训练语言模型的价值。

    

    由于试验资格标准以自然语言形式陈述，因此自动确定患者是否符合试验资格是一项复杂的任务。解决该问题的一个潜在方法是使用文本分类方法对常见类型的资格标准进行处理。本研究关注癌症试验中的七个常见排除标准：先前恶性肿瘤、人类免疫缺陷病毒、乙肝病毒、丙肝病毒、精神疾病、药物/物质滥用和自身免疫疾病。我们的数据集包含764个带有这些排除标准注释的三期癌症试验。我们尝试了常见的transformer模型以及一个新的预训练的临床试验BERT模型。我们的结果表明，自动分类常见的排除标准是可行的。此外，我们展示了一种专门针对临床试验的预训练语言模型的价值，该模型在所有标准中表现出最高的平均性能。

    Automatic identification of clinical trials for which a patient is eligible is complicated by the fact that trial eligibility is stated in natural language. A potential solution to this problem is to employ text classification methods for common types of eligibility criteria. In this study, we focus on seven common exclusion criteria in cancer trials: prior malignancy, human immunodeficiency virus, hepatitis B, hepatitis C, psychiatric illness, drug/substance abuse, and autoimmune illness. Our dataset consists of 764 phase III cancer trials with these exclusions annotated at the trial level. We experiment with common transformer models as well as a new pre-trained clinical trial BERT model. Our results demonstrate the feasibility of automatically classifying common exclusion criteria. Additionally, we demonstrate the value of a pre-trained language model specifically for clinical trials, which yields the highest average performance across all criteria.
    
[^12]: 使用抖动实现高效通信的隐私联邦学习

    Communication Efficient Private Federated Learning Using Dithering. (arXiv:2309.07809v1 [cs.LG])

    [http://arxiv.org/abs/2309.07809](http://arxiv.org/abs/2309.07809)

    本文提出了一种使用抖动实现高效通信的隐私联邦学习方法，通过在客户端使用减法抖动的量化方案，可以复制正常噪声添加过程，实现与完整精度梯度方法相当的准确性，并大幅减少所需通信量。

    

    在联邦学习中，保护隐私和确保高效通信是一个基本挑战。本文在可信的聚合器模型中解决了这一挑战，并提出了一种实现同时达到两个目标的解决方案。我们展示了在客户端使用基于减法抖动的量化方案可以有效地复制聚合器中的正常噪声添加过程。这意味着我们可以在与其他客户端相比保证相同水平的差分隐私的同时，大大减少所需的通信量，而不是传输完整精度的梯度并使用中心噪声添加。我们还通过实验证明，我们提出的方法的准确性与完整精度梯度方法相匹配。

    The task of preserving privacy while ensuring efficient communication is a fundamental challenge in federated learning. In this work, we tackle this challenge in the trusted aggregator model, and propose a solution that achieves both objectives simultaneously. We show that employing a quantization scheme based on subtractive dithering at the clients can effectively replicate the normal noise addition process at the aggregator. This implies that we can guarantee the same level of differential privacy against other clients while substantially reducing the amount of communication required, as opposed to transmitting full precision gradients and using central noise addition. We also experimentally demonstrate that the accuracy of our proposed approach matches that of the full precision gradient method.
    
[^13]: 提升模仿学习用于自动驾驶的交通规则遵守的关键因素

    What Matters to Enhance Traffic Rule Compliance of Imitation Learning for Automated Driving. (arXiv:2309.07808v1 [cs.CV])

    [http://arxiv.org/abs/2309.07808](http://arxiv.org/abs/2309.07808)

    本文提出了一种基于惩罚的模仿学习方法P-CSG，结合语义生成传感器融合技术，以提高端到端自动驾驶的整体性能，并解决了交通规则遵守和传感器感知问题。

    

    最近越来越多的研究关注于全端到端的自动驾驶技术，在这种技术中，整个驾驶流程被替换为一个简单的神经网络，由于其结构简单和推理时间快，因此变得非常吸引人。尽管这种方法大大减少了驾驶流程中的组件，但其简单性也导致解释性问题和安全问题。训练得到的策略并不总是符合交通规则，同时也很难发现其错误的原因，因为缺乏中间输出。同时，传感器对于自动驾驶的安全性和可行性也至关重要，可以帮助感知复杂驾驶场景下的周围环境。本文提出了一种全新的基于惩罚的模仿学习方法P-CSG，结合语义生成传感器融合技术，以提高端到端自动驾驶的整体性能。我们对模型的性能进行了评估。

    More research attention has recently been given to end-to-end autonomous driving technologies where the entire driving pipeline is replaced with a single neural network because of its simpler structure and faster inference time. Despite this appealing approach largely reducing the components in driving pipeline, its simplicity also leads to interpretability problems and safety issues arXiv:2003.06404. The trained policy is not always compliant with the traffic rules and it is also hard to discover the reason for the misbehavior because of the lack of intermediate outputs. Meanwhile, Sensors are also critical to autonomous driving's security and feasibility to perceive the surrounding environment under complex driving scenarios. In this paper, we proposed P-CSG, a novel penalty-based imitation learning approach with cross semantics generation sensor fusion technologies to increase the overall performance of End-to-End Autonomous Driving. We conducted an assessment of our model's perform
    
[^14]: 通过利用图像-文本辅助任务提高社交媒体帖子的多模态分类

    Improving Multimodal Classification of Social Media Posts by Leveraging Image-Text Auxiliary tasks. (arXiv:2309.07794v1 [cs.CL])

    [http://arxiv.org/abs/2309.07794](http://arxiv.org/abs/2309.07794)

    本研究通过引入图像-文本辅助任务，有效地提高了社交媒体帖子的多模态分类，通过两个辅助损失函数对图像-文本表示进行调整，捕捉底层依赖关系和语义对应关系，实现了一致的改进。

    

    有效地利用社交媒体帖子中的多模态信息对情感分析、讽刺检测和仇恨言论分类等多个下游任务至关重要。然而，由于匹配的图像-文本对中存在隐藏或互补信息的独特跨模态语义，将文本和图像信息结合起来是具有挑战性的。在本研究中，我们旨在通过在微调任何预训练的多模态模型时联合使用两个辅助损失函数来直接建模这一问题。图像-文本对比（ITC）将一篇帖子的图像-文本表示更加靠近，并将其与其他帖子分离开来，捕捉底层依赖关系。图像-文本匹配（ITM）通过惩罚不相关的对来促进理解图像和文本之间的语义对应关系。我们将这些目标与五个多模态模型相结合，证明了在四个热门社交媒体数据集上的一致改进。

    Effectively leveraging multimodal information from social media posts is essential to various downstream tasks such as sentiment analysis, sarcasm detection and hate speech classification. However, combining text and image information is challenging because of the idiosyncratic cross-modal semantics with hidden or complementary information present in matching image-text pairs. In this work, we aim to directly model this by proposing the use of two auxiliary losses jointly with the main task when fine-tuning any pre-trained multimodal model. Image-Text Contrastive (ITC) brings image-text representations of a post closer together and separates them from different posts, capturing underlying dependencies. Image-Text Matching (ITM) facilitates the understanding of semantic correspondence between images and text by penalizing unrelated pairs. We combine these objectives with five multimodal models, demonstrating consistent improvements across four popular social media datasets. Furthermore,
    
[^15]: Virchow: 数百万张全数字病理学基础模型

    Virchow: A Million-Slide Digital Pathology Foundation Model. (arXiv:2309.07778v1 [eess.IV])

    [http://arxiv.org/abs/2309.07778](http://arxiv.org/abs/2309.07778)

    Virchow是一个数百万参数的深度神经网络基础模型，通过在数百万张全数字病理学切片图像上进行自监督学习训练，有效解决了计算病理学任务中数据不足的问题，并在多个下游任务上超越了最先进的系统。

    

    计算病理学利用人工智能通过分析全数字切片图像实现精准医学和决策支持系统，有潜力彻底改变癌症的诊断和治疗。然而，实现这个目标的一个主要挑战是对于许多特定的计算病理学任务，数据量不足以进行开发。为了应对这个挑战，我们创建了Virchow，一个632百万参数的深度神经网络基础模型，用于计算病理学。通过自监督学习，Virchow在1.5百万个不同组织样本的苏木精和伊红染色全数字切片图像上进行训练，这比之前的研究数据量大得多。在包括瓦片级全癌检测和亚型以及幻灯片级生物标志物预测在内的下游任务上，Virchow在来自与预训练数据相同人群的内部数据集和外部公开数据集上均胜过最先进的系统。

    Computational pathology uses artificial intelligence to enable precision medicine and decision support systems through the analysis of whole slide images. It has the potential to revolutionize the diagnosis and treatment of cancer. However, a major challenge to this objective is that for many specific computational pathology tasks the amount of data is inadequate for development. To address this challenge, we created Virchow, a 632 million parameter deep neural network foundation model for computational pathology. Using self-supervised learning, Virchow is trained on 1.5 million hematoxylin and eosin stained whole slide images from diverse tissue groups, which is orders of magnitude more data than previous works. When evaluated on downstream tasks including tile-level pan-cancer detection and subtyping and slide-level biomarker prediction, Virchow outperforms state-of-the-art systems both on internal datasets drawn from the same population as the pretraining data as well as external pu
    
[^16]: 变分量子线性求解器增强的量子支持向量机

    Variational Quantum Linear Solver enhanced Quantum Support Vector Machine. (arXiv:2309.07770v1 [quant-ph])

    [http://arxiv.org/abs/2309.07770](http://arxiv.org/abs/2309.07770)

    提出了一种变分量子线性求解器增强的量子支持向量机算法，能够在嘈杂中间规模量子设备上进行可扩展的分类任务。算法利用变分量子线性求解器解决了最小二乘支持向量机的线性方程组，并在Iris数据集上进行了验证。通过经典计算和量子计算的策略性组合，算法在不同维度的特征空间中表现出了实用性和有效性。

    

    量子支持向量机在使用量子资源进行监督学习任务（如分类）中起着重要作用。然而，当前的方法在嘈杂中间规模量子（NISQ）设备的可扩展性方面存在严重限制。在本研究中，我们提出了一种新的方法，称为变分量子线性求解器（VQLS）增强的量子支持向量机。这是基于我们利用变分量子线性求解器在NISQ设备上解决最小二乘SVM的线性方程组的思想构建的。我们通过对Iris数据集的大量数值实验评估了我们方法的实现，该数据集包含三个不同的鸢尾植物物种。基于此，我们通过构建一个能够在从一维到七维的特征空间中进行分类的分类器，探索了我们算法的实用性和有效性。此外，通过策略性地利用经典计算和量子计算对各种子程序进行优化，我们的算法在NISQ设备上实现了可扩展的QSVM。

    Quantum Support Vector Machines (QSVM) play a vital role in using quantum resources for supervised machine learning tasks, such as classification. However, current methods are strongly limited in terms of scalability on Noisy Intermediate Scale Quantum (NISQ) devices. In this work, we propose a novel approach called the Variational Quantum Linear Solver (VQLS) enhanced QSVM. This is built upon our idea of utilizing the variational quantum linear solver to solve system of linear equations of a least squares-SVM on a NISQ device. The implementation of our approach is evaluated by an extensive series of numerical experiments with the Iris dataset, which consists of three distinct iris plant species. Based on this, we explore the practicality and effectiveness of our algorithm by constructing a classifier capable of classification in a feature space ranging from one to seven dimensions. Furthermore, by strategically exploiting both classical and quantum computing for various subroutines of
    
[^17]: PRE: 视觉-语言提示学习与重新参数化编码器

    PRE: Vision-Language Prompt Learning with Reparameterization Encoder. (arXiv:2309.07760v1 [cs.CV])

    [http://arxiv.org/abs/2309.07760](http://arxiv.org/abs/2309.07760)

    这项工作提出了一种名为PRE的方法，通过重新参数化编码器来增强可学习提示的泛化能力，从而解决了大型预训练视觉-语言模型中手动提示工程的挑战。

    

    大型预训练的视觉-语言模型（如CLIP）已经展示出在零样本迁移任务中具有巨大潜力。然而，为了达到最佳性能，需要手动选择提示以改进下游图像分布和文本类描述之间的对齐。这种手动提示工程是将这些模型部署到实践中的主要挑战，因为它需要领域专业知识并且非常耗时。为了避免复杂的提示工程，最近的CoOp工作引入了在视觉领域使用可控文本标记的提示学习概念。虽然CoOp可以在手动提示上取得显著改进，但其学到的上下文在同一数据集中更广泛的未见类别中的泛化能力较差。在这项工作中，我们提出了一种名为Prompt Learning with Reparameterization Encoder (PRE) 的简单高效的方法，改进了可学习提示的泛化能力。

    Large pre-trained vision-language models such as CLIP have demonstrated great potential in zero-shot transferability to downstream tasks. However, to attain optimal performance, the manual selection of prompts is necessary to improve alignment between the downstream image distribution and the textual class descriptions. This manual prompt engineering is the major challenge for deploying such models in practice since it requires domain expertise and is extremely time-consuming. To avoid non-trivial prompt engineering, recent work Context Optimization (CoOp) introduced the concept of prompt learning to the vision domain using learnable textual tokens. While CoOp can achieve substantial improvements over manual prompts, its learned context is worse generalizable to wider unseen classes within the same dataset. In this work, we present Prompt Learning with Reparameterization Encoder (PRE) - a simple and efficient method that enhances the generalization ability of the learnable prompt to un
    
[^18]: 解读存在于观察者的心中：用于可解释性表示学习的因果框架

    Interpretability is in the Mind of the Beholder: A Causal Framework for Human-interpretable Representation Learning. (arXiv:2309.07742v1 [cs.LG])

    [http://arxiv.org/abs/2309.07742](http://arxiv.org/abs/2309.07742)

    可解释性表示学习的关键挑战是如何在人类因素中进行建模和操作。本论文提出了一种数学框架，用于获取可解释的表示，适用于事后解释者和基于概念的神经网络。

    

    可解释的人工智能的重点正在从以输入特征为基础定义的解释，转向以从数据中学习的可解释概念为基础的解释。然而，如何可靠地获得这些概念仍然基本不清楚。缺少对概念可解释性的一致性概念，导致事后解释者和基于概念的神经网络使用的概念通过多种相互不兼容的策略获得。至关重要的是，这些策略中大多数忽视了人类问题面：一个表示只有在被接收的人类能够理解的程度上才能理解。在人类可解释表示学习（HRL）中，关键挑战是如何对这个人类因素进行建模和操作。在这项工作中，我们提出了一种数学框架，用于获取适用于事后解释者和基于概念的神经网络的可解释表示。我们对HRL的形式化建模...

    Focus in Explainable AI is shifting from explanations defined in terms of low-level elements, such as input features, to explanations encoded in terms of interpretable concepts learned from data. How to reliably acquire such concepts is, however, still fundamentally unclear. An agreed-upon notion of concept interpretability is missing, with the result that concepts used by both post-hoc explainers and concept-based neural networks are acquired through a variety of mutually incompatible strategies. Critically, most of these neglect the human side of the problem: a representation is understandable only insofar as it can be understood by the human at the receiving end. The key challenge in Human-interpretable Representation Learning (HRL) is how to model and operationalize this human element. In this work, we propose a mathematical framework for acquiring interpretable representations suitable for both post-hoc explainers and concept-based neural networks. Our formalization of HRL builds 
    
[^19]: 理解向量值神经网络及其与实数和超复值神经网络的关系

    Understanding Vector-Valued Neural Networks and Their Relationship with Real and Hypercomplex-Valued Neural Networks. (arXiv:2309.07716v1 [cs.LG])

    [http://arxiv.org/abs/2309.07716](http://arxiv.org/abs/2309.07716)

    本文介绍了向量值神经网络（V-nets）的广泛框架，并解释了它们与超复值神经网络以及传统神经网络的关系。

    

    尽管深度学习模型在多维信号和图像处理方面有许多成功的应用，但大多数传统神经网络处理由（多维）实数数组表示的数据。特征通道之间的互相关通常被期望从训练数据中学习，这需要大量的参数和仔细的训练。相反，向量值神经网络被设计成处理向量数组，并自然地考虑特征通道之间的互相关。因此，它们通常具有更少的参数，通常比传统神经网络具有更强的训练能力。本文旨在提出一个广泛的向量值神经网络框架，称为V-nets。在这个背景下，超复值神经网络被视为具有额外代数属性的向量值模型。此外，本文解释了向量值神经网络与传统神经网络之间的关系。

    Despite the many successful applications of deep learning models for multidimensional signal and image processing, most traditional neural networks process data represented by (multidimensional) arrays of real numbers. The intercorrelation between feature channels is usually expected to be learned from the training data, requiring numerous parameters and careful training. In contrast, vector-valued neural networks are conceived to process arrays of vectors and naturally consider the intercorrelation between feature channels. Consequently, they usually have fewer parameters and often undergo more robust training than traditional neural networks. This paper aims to present a broad framework for vector-valued neural networks, referred to as V-nets. In this context, hypercomplex-valued neural networks are regarded as vector-valued models with additional algebraic properties. Furthermore, this paper explains the relationship between vector-valued and traditional neural networks. Precisely, 
    
[^20]: 添加语义上下文的控制力量，为金融市场数据生成引入Market-GAN

    Market-GAN: Adding Control to Financial Market Data Generation with Semantic Context. (arXiv:2309.07708v1 [cs.LG])

    [http://arxiv.org/abs/2309.07708](http://arxiv.org/abs/2309.07708)

    本研究通过提出具有市场动态、股票代码和历史状态作为上下文的上下文市场数据集以及使用条件生成对抗网络（GAN）来实现对金融数据生成的控制。

    

    金融模拟器在提升预测准确性、管理风险和促进战略金融决策方面发挥着重要作用。尽管已经开发了金融市场模拟方法，但现有的框架常常难以适应专门的模拟上下文。我们将挑战归结为：i）当前的金融数据集不包含上下文标签；ii）当前的技术没有设计用于生成具有上下文控制的金融数据，与其他模态相比，这要求更高的精度；iii）由于金融数据的非平稳、噪声性质，生成与上下文对齐、高保真度的数据存在困难。为了解决这些挑战，我们的贡献是：i）提出了具有市场动态、股票代码和历史状态作为上下文的上下文市场数据集，利用线性回归和动态时间扭曲聚类结合的市场动态建模方法提取市场动态；ii）我们预先准备了Market-GAN模型，该模型通过使用条件生成对抗网络（GAN）的方法以及编码上下文向量的方式来实现对金融数据生成的控制。

    Financial simulators play an important role in enhancing forecasting accuracy, managing risks, and fostering strategic financial decision-making. Despite the development of financial market simulation methodologies, existing frameworks often struggle with adapting to specialized simulation context. We pinpoint the challenges as i) current financial datasets do not contain context labels; ii) current techniques are not designed to generate financial data with context as control, which demands greater precision compared to other modalities; iii) the inherent difficulties in generating context-aligned, high-fidelity data given the non-stationary, noisy nature of financial data. To address these challenges, our contributions are: i) we proposed the Contextual Market Dataset with market dynamics, stock ticker, and history state as context, leveraging a market dynamics modeling method that combines linear regression and Dynamic Time Warping clustering to extract market dynamics; ii) we prese
    
[^21]: 测量因果控制的因果熵和信息增益

    Causal Entropy and Information Gain for Measuring Causal Control. (arXiv:2309.07703v1 [cs.LG])

    [http://arxiv.org/abs/2309.07703](http://arxiv.org/abs/2309.07703)

    本文提出了一种考虑因果结构的信息论量，用于评估某个特定结果变量的因果重要性，解决了因果可解释性的挑战。

    

    人工智能模型和方法通常缺乏因果可解释性。尽管解释性机器学习（IML）方法取得了进展，但它们经常将重要性赋予那些对结果变量没有因果影响的特征。在模型训练之前或之后，选择因果相关的特征将提供一种解决方案。利用信息论量进行特征选择的方法在识别统计相关特征方面非常成功。然而，它们所基于的信息论量不包含因果关系，因此在这种情况下不适用。为了解决这个挑战，本文提出了能够考虑系统因果结构的信息论量，可以用于评估某个给定结果变量的因果重要性。具体来说，我们引入了因果熵和因果互信息的因果版本。

    Artificial intelligence models and methods commonly lack causal interpretability. Despite the advancements in interpretable machine learning (IML) methods, they frequently assign importance to features which lack causal influence on the outcome variable. Selecting causally relevant features among those identified as relevant by these methods, or even before model training, would offer a solution. Feature selection methods utilizing information theoretical quantities have been successful in identifying statistically relevant features. However, the information theoretical quantities they are based on do not incorporate causality, rendering them unsuitable for such scenarios. To address this challenge, this article proposes information theoretical quantities that incorporate the causal structure of the system, which can be used to evaluate causal importance of features for some given outcome variable. Specifically, we introduce causal versions of entropy and mutual information, termed cau
    
[^22]: 不确定思维树：大型语言模型的推理方法

    Tree of Uncertain Thoughts Reasoning for Large Language Models. (arXiv:2309.07694v1 [cs.CL])

    [http://arxiv.org/abs/2309.07694](http://arxiv.org/abs/2309.07694)

    本研究提出了一种针对大型语言模型的推理框架——不确定思维树（TouT），它通过利用蒙特卡洛丢弃来量化中间步骤上的本地不确定性，提高了模型生成响应的精确性。

    

    尽管最近引入的不确定思维树（Tree of Thoughts, ToT）在允许大型语言模型（LLMs）通过预见和回溯进行全局决策方面取得了进展，但它忽视了中间决策点或“思维”中的固有局部不确定性。这些固有的局部不确定性，由于LLMs潜在的多样性响应能力，成为推理过程中的重要问题。为了解决这一关键差距，我们引入了不确定思维树（Tree of Uncertain Thoughts, TouT）-一种针对LLMs设计的推理框架。我们的TouT有效地利用了蒙特卡洛丢弃(Monte Carlo Dropout)来量化与LLMs在这些中间步骤上的不同本地响应相关的不确定性得分。通过将这种本地不确定性量化与全局搜索算法结合起来，TouT提高了模型生成响应的精确性。我们通过在两个苛刻的规划任务上进行严格实验证明了我们的方法：24点游戏和迷你填字游戏。

    While the recently introduced Tree of Thoughts (ToT) has heralded advancements in allowing Large Language Models (LLMs) to reason through foresight and backtracking for global decision-making, it has overlooked the inherent local uncertainties in intermediate decision points or "thoughts". These local uncertainties, intrinsic to LLMs given their potential for diverse responses, remain a significant concern in the reasoning process. Addressing this pivotal gap, we introduce the Tree of Uncertain Thoughts (TouT) - a reasoning framework tailored for LLMs. Our TouT effectively leverages Monte Carlo Dropout to quantify uncertainty scores associated with LLMs' diverse local responses at these intermediate steps. By marrying this local uncertainty quantification with global search algorithms, TouT enhances the model's precision in response generation. We substantiate our approach with rigorous experiments on two demanding planning tasks: Game of 24 and Mini Crosswords. The empirical evidence 
    
[^23]: 基于DenseNet的EEG辅助听觉空间注意力解码方法

    A DenseNet-based method for decoding auditory spatial attention with EEG. (arXiv:2309.07690v1 [eess.SP])

    [http://arxiv.org/abs/2309.07690](http://arxiv.org/abs/2309.07690)

    本论文提出了一种基于DenseNet的EEG辅助听觉空间注意力解码方法，该方法充分利用了EEG电极的空间分布，并通过深度卷积神经网络提取了时空特征，实现了较高的解码精度。

    

    听觉空间注意力检测（ASAD）旨在利用EEG在多人演讲者环境中解码被注意的空间位置。ASAD方法受到大脑皮层神经元在处理听觉空间注意力时的侧化启发，并在神经记录的听觉注意力解码（AAD）任务中显示出有希望的性能。在以往的ASAD方法中，尚未充分利用EEG电极的空间分布，这可能限制了这些方法的性能。在本研究中，通过将原始EEG信道转换为二维空间拓扑图，将EEG数据转换为包含时空信息的三维排列。然后使用三维深度卷积神经网络（DenseNet-3D）来提取所关注位置的神经表征的时空特征。结果表明，所提出的方法比传统方法实现了更高的解码精度。

    Auditory spatial attention detection (ASAD) aims to decode the attended spatial location with EEG in a multiple-speaker setting. ASAD methods are inspired by the brain lateralization of cortical neural responses during the processing of auditory spatial attention, and show promising performance for the task of auditory attention decoding (AAD) with neural recordings. In the previous ASAD methods, the spatial distribution of EEG electrodes is not fully exploited, which may limit the performance of these methods. In the present work, by transforming the original EEG channels into a two-dimensional (2D) spatial topological map, the EEG data is transformed into a three-dimensional (3D) arrangement containing spatial-temporal information. And then a 3D deep convolutional neural network (DenseNet-3D) is used to extract temporal and spatial features of the neural representation for the attended locations. The results show that the proposed method achieves higher decoding accuracy than the sta
    
[^24]: deepFDEnet: 一个用于求解分数阶微分方程的新型神经网络架构

    deepFDEnet: A Novel Neural Network Architecture for Solving Fractional Differential Equations. (arXiv:2309.07684v1 [cs.LG])

    [http://arxiv.org/abs/2309.07684](http://arxiv.org/abs/2309.07684)

    deepFDEnet是一种新型神经网络架构，能够准确求解各种形式的分数阶微分方程。

    

    本研究的主要目标是提出一种新颖的深度神经网络架构，能够准确地求解分数阶微分方程。该设计使用了高斯积分规则和$L_1$离散化技术。在每个方程中，使用深度神经网络近似未知函数。研究还对三个形式的分数阶微分方程进行了检验，以突出该方法的多样性：分数阶常微分方程、分数阶积分微分方程和分数阶偏微分方程。结果表明，所提出的架构能够以极高的精度解决不同形式的分数阶微分方程。

    The primary goal of this research is to propose a novel architecture for a deep neural network that can solve fractional differential equations accurately. A Gaussian integration rule and a $L_1$ discretization technique are used in the proposed design. In each equation, a deep neural network is used to approximate the unknown function. Three forms of fractional differential equations have been examined to highlight the method's versatility: a fractional ordinary differential equation, a fractional order integrodifferential equation, and a fractional order partial differential equation. The results show that the proposed architecture solves different forms of fractional differential equations with excellent precision.
    
[^25]: 用于量子态分类的机器学习模型基准测试

    Benchmarking machine learning models for quantum state classification. (arXiv:2309.07679v1 [quant-ph])

    [http://arxiv.org/abs/2309.07679](http://arxiv.org/abs/2309.07679)

    本文对应用于真实量子设备的多种分类技术进行基准测试，以开发一个模型来区分基态和激发态。

    

    量子计算是一个快速发展的领域，其中信息通过称为量子比特的二级量子态进行处理。由于噪声和退相干现象，当前的量子比特物理实现需要经过精细的校准，包括不同的实验。在不同的表征实验中，关键步骤之一是开发一个模型，通过区分基态和激发态来对测量的态进行分类。在本文中，我们对应用于真实量子设备的多种分类技术进行基准测试。

    Quantum computing is a growing field where the information is processed by two-levels quantum states known as qubits. Current physical realizations of qubits require a careful calibration, composed by different experiments, due to noise and decoherence phenomena. Among the different characterization experiments, a crucial step is to develop a model to classify the measured state by discriminating the ground state from the excited state. In this proceedings we benchmark multiple classification techniques applied to real quantum devices.
    
[^26]: 通过基于集合的可达性分析，在分层强化学习中实现目标空间抽象

    Goal Space Abstraction in Hierarchical Reinforcement Learning via Set-Based Reachability Analysis. (arXiv:2309.07675v1 [cs.LG])

    [http://arxiv.org/abs/2309.07675](http://arxiv.org/abs/2309.07675)

    本文提出了一种基于集合的可达性分析方法，通过 emergent representation 实现目标空间抽象，在分层强化学习中自主发现符号目标表示，并引入封建HRL算法来同时学习目标表示和分层策略。

    

    开放式学习通过使用符号方法进行目标表示而获益良多，因为它们提供了一种结构化知识以进行高效和可传递的学习。然而，现有的依赖符号推理的分层强化学习(HRL)方法通常受限于需要手动设置目标表示。自主发现符号目标表示的挑战在于它必须保留关键信息，例如环境动力学。在本文中，我们提出了一种通过新出现的表示来实现目标发现的发展机制，该表示将具有类似任务中的角色的环境状态集合进行抽象（即分组）。我们引入了一个同时学习目标表示和分层策略的封建HRL算法。该算法使用神经网络的符号可达性分析来近似状态集合之间的过渡关系，并改进目标表示。

    Open-ended learning benefits immensely from the use of symbolic methods for goal representation as they offer ways to structure knowledge for efficient and transferable learning. However, the existing Hierarchical Reinforcement Learning (HRL) approaches relying on symbolic reasoning are often limited as they require a manual goal representation. The challenge in autonomously discovering a symbolic goal representation is that it must preserve critical information, such as the environment dynamics. In this paper, we propose a developmental mechanism for goal discovery via an emergent representation that abstracts (i.e., groups together) sets of environment states that have similar roles in the task. We introduce a Feudal HRL algorithm that concurrently learns both the goal representation and a hierarchical policy. The algorithm uses symbolic reachability analysis for neural networks to approximate the transition relation among sets of states and to refine the goal representation. We eval
    
[^27]: 从有限且有噪声的数据中基于物理约束的稳健学习开放式偏微分方程

    Physics-constrained robust learning of open-form PDEs from limited and noisy data. (arXiv:2309.07672v1 [cs.LG])

    [http://arxiv.org/abs/2309.07672](http://arxiv.org/abs/2309.07672)

    该论文提出了一种名为R-DISCOVER的框架，能够从有限且有噪声的数据中稳健地揭示开放式偏微分方程。该框架通过符号表示和强化学习引导下的混合PDE生成器，以及神经网络预测模型来实现。实验结果表明该方法能够高效地发现和嵌入PDE，并选择表现最佳的PDE进行系统响应预测。

    

    揭示非线性动态系统的基本控制方程在遇到噪声观测和没有现成先验知识的情况下仍然是一个巨大的挑战。本研究提出了R-DISCOVER，这是一个旨在从有限和有噪声的数据中稳健地揭示开放式偏微分方程的框架。该框架通过两个交替更新过程进行操作：发现和嵌入。发现阶段利用符号表示和强化学习（RL）引导下的混合PDE生成器，高效地产生具有树结构的多样化的开放式偏微分方程。基于神经网络的预测模型适应系统响应并作为生成的PDE的奖励评估器。利用拟合效果较好的PDE通过RL方法进行迭代优化生成器，并通过无参数稳定度指标选择表现最佳的PDE。嵌入阶段将最初从发现过程中确定的PDE与逼近真实系统进行频谱分析。

    Unveiling the underlying governing equations of nonlinear dynamic systems remains a significant challenge, especially when encountering noisy observations and no prior knowledge available. This study proposes R-DISCOVER, a framework designed to robustly uncover open-form partial differential equations (PDEs) from limited and noisy data. The framework operates through two alternating update processes: discovering and embedding. The discovering phase employs symbolic representation and a reinforcement learning (RL)-guided hybrid PDE generator to efficiently produce diverse open-form PDEs with tree structures. A neural network-based predictive model fits the system response and serves as the reward evaluator for the generated PDEs. PDEs with superior fits are utilized to iteratively optimize the generator via the RL method and the best-performing PDE is selected by a parameter-free stability metric. The embedding phase integrates the initially identified PDE from the discovering process a
    
[^28]: 多源领域自适应的联邦数据集字典学习

    Federated Dataset Dictionary Learning for Multi-Source Domain Adaptation. (arXiv:2309.07670v1 [cs.LG])

    [http://arxiv.org/abs/2309.07670](http://arxiv.org/abs/2309.07670)

    本文提出了一种用于联邦领域自适应的方法，通过字典学习经验分布来解决客户端间分布偏移和部分无标签数据的问题。该方法通过设计协作通信协议和聚合操作，保护了客户端数据隐私，并成功在目标领域生成了标记数据。

    

    本文提出了一种联邦领域自适应的方法，该方法在客户端中存在分布偏移且部分客户端具有无标签的数据。所提出的框架FedDaDiL通过字典学习经验分布来解决这一挑战。在我们的设置中，客户端的分布代表着特定的领域，而FedDaDiL则共同训练了一个联邦经验分布字典。具体而言，我们在数据集字典学习框架上设计了协作通信协议和聚合操作。所选择的协议保护了客户端的数据隐私，相比于集中式方法提高了整体隐私性。我们通过对Caltech-Office、TEP和CWRU基准数据集进行了大量实验证明了我们的方法成功地在目标领域生成了标记数据。此外，我们还将我们的方法与其集中式方法和其他联邦领域基准进行了比较。

    In this article, we propose an approach for federated domain adaptation, a setting where distributional shift exists among clients and some have unlabeled data. The proposed framework, FedDaDiL, tackles the resulting challenge through dictionary learning of empirical distributions. In our setting, clients' distributions represent particular domains, and FedDaDiL collectively trains a federated dictionary of empirical distributions. In particular, we build upon the Dataset Dictionary Learning framework by designing collaborative communication protocols and aggregation operations. The chosen protocols keep clients' data private, thus enhancing overall privacy compared to its centralized counterpart. We empirically demonstrate that our approach successfully generates labeled data on the target domain with extensive experiments on (i) Caltech-Office, (ii) TEP, and (iii) CWRU benchmarks. Furthermore, we compare our method to its centralized counterpart and other benchmarks in federated doma
    
[^29]: 多源领域适应通过数据集字典学习遇见数据集蒸馏

    Multi-Source Domain Adaptation meets Dataset Distillation through Dataset Dictionary Learning. (arXiv:2309.07666v1 [cs.LG])

    [http://arxiv.org/abs/2309.07666](http://arxiv.org/abs/2309.07666)

    本文介绍了一个新问题，称为多源领域适应通过数据集字典学习遇见数据集蒸馏（MSDA-DD），通过适应先前的方法以及分配匹配方法，我们实现了仅使用每类1个样本即可实现最先进的适应性能。

    

    本文考虑了机器学习中两个问题的交集：多源领域适应（MSDA）和数据集蒸馏（DD）。一方面，前者考虑了将多个异质的标注源领域适应到一个未标注的目标领域。另一方面，后者攻击了关于合成一个包含有关数据集的所有信息的小摘要的问题。因此，我们提出了一个新问题称为MSDA-DD。为了解决这个问题，我们适应了MSDA文献中的先前作品，如Wasserstein Barycenter Transport和数据集字典学习，以及DD方法Distribution Matching。我们在四个基准测试集上对这个新问题进行了彻底的实验（Caltech-Office 10， Tennessee-Eastman Process， Continuous Stirred Tank Reactor和Case Western Reserve University），在这些实验中，我们展示了即使每类只有1个样本，我们也达到了最先进的适应性能。

    In this paper, we consider the intersection of two problems in machine learning: Multi-Source Domain Adaptation (MSDA) and Dataset Distillation (DD). On the one hand, the first considers adapting multiple heterogeneous labeled source domains to an unlabeled target domain. On the other hand, the second attacks the problem of synthesizing a small summary containing all the information about the datasets. We thus consider a new problem called MSDA-DD. To solve it, we adapt previous works in the MSDA literature, such as Wasserstein Barycenter Transport and Dataset Dictionary Learning, as well as DD method Distribution Matching. We thoroughly experiment with this novel problem on four benchmarks (Caltech-Office 10, Tennessee-Eastman Process, Continuous Stirred Tank Reactor, and Case Western Reserve University), where we show that, even with as little as 1 sample per class, one achieves state-of-the-art adaptation performance.
    
[^30]: 线性变分自编码器中数据集大小对率失真曲线和后验坍缩阈值的影响

    Dataset Size Dependence of Rate-Distortion Curve and Threshold of Posterior Collapse in Linear VAE. (arXiv:2309.07663v1 [stat.ML])

    [http://arxiv.org/abs/2309.07663](http://arxiv.org/abs/2309.07663)

    本文通过分析在高维限制下的最简化的VAE，提出了一个闭式表达式，评估了beta与VAE中数据集大小、后验坍缩和率失真曲线之间的关系。结果显示，随着beta的增加，产生较大的广义误差平台，并且选择一个小于特定阈值的beta值可以提高模型性能。

    

    在变分自编码器（VAE）中，变分后验经常与先验密切吻合，这被称为后验坍缩，影响了表示学习的质量。为了缓解这个问题，VAE中引入了一个可调节的超参数beta。本文通过在高维限制下分析最简化的VAE，提出了一个闭式表达式，评估了beta与VAE中数据集大小、后验坍缩和率失真曲线之间的关系。这些结果表明，一个较大的beta会产生一个长的广义误差平台。随着beta的增加，平台的长度延长，超过一定的阈值后变为无穷。这意味着与通常的正则化参数不同，beta的选择可能会导致后验坍缩，而与数据集大小无关。因此，beta是一个需要谨慎调整的风险参数。此外，考虑到数据集大小对率失真曲线的依赖性，我们发现存在一个与数据集大小相关的阈值，选择小于这个阈值的beta值可以提高模型的性能。

    In the Variational Autoencoder (VAE), the variational posterior often aligns closely with the prior, which is known as posterior collapse and hinders the quality of representation learning. To mitigate this problem, an adjustable hyperparameter beta has been introduced in the VAE. This paper presents a closed-form expression to assess the relationship between the beta in VAE, the dataset size, the posterior collapse, and the rate-distortion curve by analyzing a minimal VAE in a high-dimensional limit. These results clarify that a long plateau in the generalization error emerges with a relatively larger beta. As the beta increases, the length of the plateau extends and then becomes infinite beyond a certain beta threshold. This implies that the choice of beta, unlike the usual regularization parameters, can induce posterior collapse regardless of the dataset size. Thus, beta is a risky parameter that requires careful tuning. Furthermore, considering the dataset-size dependence on the ra
    
[^31]: 学习到排名中的特征工程在社区问答任务中的应用

    Feature Engineering in Learning-to-Rank for Community Question Answering Task. (arXiv:2309.07610v1 [cs.LG])

    [http://arxiv.org/abs/2309.07610](http://arxiv.org/abs/2309.07610)

    本文研究了在社区问答任务中学习到排名中的特征工程的几个方面。首先，引入了基于BERT的特征，捕捉语义相似性；其次，结合问题和答案两种类型的特征；第三，通过经验性研究探索了不同排名算法。

    

    社区问答（CQA）论坛是基于互联网的平台，用户在这里提出问题，其他专家用户试图提供解决方案。许多CQA论坛，如Quora，Stackoverflow，Yahoo！Answer，StackExchange等都有大量用户生成的数据。这些数据在自动化的CQA排名系统中得到利用，以回应用户的查询，呈现类似的问题（和答案）。在这项工作中，我们经验性地调查了该领域的一些方面。首先，除了传统的特征如TF-IDF、BM25等，我们引入了基于BERT的特征，捕捉问题和答案之间的语义相似性。其次，大部分现有研究工作都集中在仅从问题部分提取的特征上，尚未广泛探索从答案中提取的特征。我们以线性方式结合了两种类型的特征。第三，使用我们提出的概念，我们对不同排名算法进行了经验性的研究。

    Community question answering (CQA) forums are Internet-based platforms where users ask questions about a topic and other expert users try to provide solutions. Many CQA forums such as Quora, Stackoverflow, Yahoo!Answer, StackExchange exist with a lot of user-generated data. These data are leveraged in automated CQA ranking systems where similar questions (and answers) are presented in response to the query of the user. In this work, we empirically investigate a few aspects of this domain. Firstly, in addition to traditional features like TF-IDF, BM25 etc., we introduce a BERT-based feature that captures the semantic similarity between the question and answer. Secondly, most of the existing research works have focused on features extracted only from the question part; features extracted from answers have not been explored extensively. We combine both types of features in a linear fashion. Thirdly, using our proposed concepts, we conduct an empirical investigation with different rank-lea
    
[^32]: 学习非标记可变形一维物体的准静态3D模型，用于双手机器人操作

    Learning Quasi-Static 3D Models of Markerless Deformable Linear Objects for Bimanual Robotic Manipulation. (arXiv:2309.07609v1 [cs.RO])

    [http://arxiv.org/abs/2309.07609](http://arxiv.org/abs/2309.07609)

    本文提出了一种基于Transformer架构的新的学习模型，通过缩放方法实现了不同长度的可变形一维物体的更高精度。此外，引入的数据增强技术大大提高了数据驱动模型的预测性能，甚至简单的多层感知机也能达到接近最先进性能的水平。

    

    可变形一维物体（DLOs）的机器人操作是一个重要且具有挑战性的任务，在许多实际应用中都很重要。传统的基于模型的方法需要一个准确的模型来捕捉机器人运动对DLO变形的影响。如今，基于数据驱动的模型在质量和计算时间之间提供了最好的权衡。本文分析了几种基于学习的DLO三维模型，并提出了一种基于Transformer架构的新模型，通过提出的缩放方法，在不同长度的DLO上实现了更高的精度。此外，我们引入了一种数据增强技术，提高了几乎所有考虑的DLO数据驱动模型的预测性能。借助这种技术，即使是简单的多层感知机（MLP）也能达到接近最先进性能的水平，而且计算速度更快。在实验中，我们比较了这些学习的3D模型的性能。

    The robotic manipulation of Deformable Linear Objects (DLOs) is a vital and challenging task that is important in many practical applications. Classical model-based approaches to this problem require an accurate model to capture how robot motions affect the deformation of the DLO. Nowadays, data-driven models offer the best tradeoff between quality and computation time. This paper analyzes several learning-based 3D models of the DLO and proposes a new one based on the Transformer architecture that achieves superior accuracy, even on the DLOs of different lengths, thanks to the proposed scaling method. Moreover, we introduce a data augmentation technique, which improves the prediction performance of almost all considered DLO data-driven models. Thanks to this technique, even a simple Multilayer Perceptron (MLP) achieves close to state-of-the-art performance while being significantly faster to evaluate. In the experiments, we compare the performance of the learning-based 3D models of the
    
[^33]: 将废料变为黄金的损失：BERT4Rec真的比SASRec更好吗？

    Turning Dross Into Gold Loss: is BERT4Rec really better than SASRec?. (arXiv:2309.07602v1 [cs.IR])

    [http://arxiv.org/abs/2309.07602](http://arxiv.org/abs/2309.07602)

    在比较推荐系统中的两种模型SASRec和BERT4Rec时，我们的研究发现，如果两个模型都使用相同的损失函数进行训练，SASRec在质量和训练速度方面表现明显优于BERT4Rec。同时，我们还发现，即使使用负采样，SASRec仍然能够有效训练并优于BERT4Rec，但需要更多的负样本。

    

    最近，在推荐系统领域，顺序推荐和下一个项目预测任务越来越受欢迎。目前，基于Transformer的模型SASRec和BERT4Rec是两种最先进的基准模型。在过去的几年中，有很多发表的论文比较了这两个算法并提出了新的最先进模型。在大多数论文中，BERT4Rec的性能优于SASRec。但是，BERT4Rec对所有项目使用交叉熵，而SASRec使用负采样对一个正样本和一个负样本计算二元交叉熵损失。在我们的工作中，我们展示了如果两个模型都使用BERT4Rec所用的损失进行训练，那么SASRec在质量和训练速度方面将明显优于BERT4Rec。此外，我们还展示了即使使用负采样，SASRec仍然能够有效训练并优于BERT4Rec，但负样本的数量应该比BERT4Rec要大得多。

    Recently sequential recommendations and next-item prediction task has become increasingly popular in the field of recommender systems. Currently, two state-of-the-art baselines are Transformer-based models SASRec and BERT4Rec. Over the past few years, there have been quite a few publications comparing these two algorithms and proposing new state-of-the-art models. In most of the publications, BERT4Rec achieves better performance than SASRec. But BERT4Rec uses cross-entropy over softmax for all items, while SASRec uses negative sampling and calculates binary cross-entropy loss for one positive and one negative item. In our work, we show that if both models are trained with the same loss, which is used by BERT4Rec, then SASRec will significantly outperform BERT4Rec both in terms of quality and training speed. In addition, we show that SASRec could be effectively trained with negative sampling and still outperform BERT4Rec, but the number of negative examples should be much larger than on
    
[^34]: 使用LLM预测的可信度信号和弱监督检测虚假信息

    Detecting Misinformation with LLM-Predicted Credibility Signals and Weak Supervision. (arXiv:2309.07601v1 [cs.CL])

    [http://arxiv.org/abs/2309.07601](http://arxiv.org/abs/2309.07601)

    本文研究了使用大型语言模型和弱监督的方式来检测虚假信息，证明了这种方法在两个数据集上的效果优于当前最先进的分类器。

    

    可信度信号代表了记者和事实核查员通常用来评估在线内容真实性的一系列启发式方法。然而，自动化可信度信号提取的任务非常具有挑战性，因为它需要训练高准确率的特定信号提取器，而目前没有足够大的数据集对所有可信度信号进行注释。本文研究了是否可以有效地用一组18个可信度信号来提示大型语言模型（LLMs），以产生每个信号的弱标签。然后，我们使用弱监督的方式对这些潜在的噪声标签进行聚合，以预测内容的真实性。我们证明了我们的方法，即结合了零-shot LLM可信度信号标注和弱监督的方法，在两个虚假信息数据集上优于最先进的分类器，而没有使用任何训练标签。

    Credibility signals represent a wide range of heuristics that are typically used by journalists and fact-checkers to assess the veracity of online content. Automating the task of credibility signal extraction, however, is very challenging as it requires high-accuracy signal-specific extractors to be trained, while there are currently no sufficiently large datasets annotated with all credibility signals. This paper investigates whether large language models (LLMs) can be prompted effectively with a set of 18 credibility signals to produce weak labels for each signal. We then aggregate these potentially noisy labels using weak supervision in order to predict content veracity. We demonstrate that our approach, which combines zero-shot LLM credibility signal labeling and weak supervision, outperforms state-of-the-art classifiers on two misinformation datasets without using any ground-truth labels for training. We also analyse the contribution of the individual credibility signals towards p
    
[^35]: 通过条件置换进行统计有效的变量重要性评估

    Statistically Valid Variable Importance Assessment through Conditional Permutations. (arXiv:2309.07593v1 [cs.LG])

    [http://arxiv.org/abs/2309.07593](http://arxiv.org/abs/2309.07593)

    本论文提出了一种通过条件置换进行统计有效的变量重要性评估的方法。实证结果表明，这种方法克服了标准置换重要性的局限性，并在深度神经网络中表现出最高的准确性。

    

    在使用复杂学习器（如深度神经网络）处理大规模数据时，变量重要性评估已成为机器学习应用中的关键步骤。目前，基于移除的重要性评估是参考方法，特别是在需要统计保证来验证变量包含性时。通常，它们使用变量置换方案来实现。然而，这些方法在存在协变量之间的相关性时容易将不重要的变量误识别为重要变量。本文提出了一种系统方法来研究条件置换重要性（Conditional Permutation Importance，CPI），它是模型无关且计算效率高的方法，并提供了基准测试，用于评估当前最先进的变量重要性估计器。理论和实证结果表明，CPI通过提供准确的I型错误控制，克服了标准置换重要性的局限性。当与深度神经网络一起使用时，CPI始终显示出最高的准确性。

    Variable importance assessment has become a crucial step in machine-learning applications when using complex learners, such as deep neural networks, on large-scale data. Removal-based importance assessment is currently the reference approach, particularly when statistical guarantees are sought to justify variable inclusion. It is often implemented with variable permutation schemes. On the flip side, these approaches risk misidentifying unimportant variables as important in the presence of correlations among covariates. Here we develop a systematic approach for studying Conditional Permutation Importance (CPI) that is model agnostic and computationally lean, as well as reusable benchmarks of state-of-the-art variable importance estimators. We show theoretically and empirically that $\textit{CPI}$ overcomes the limitations of standard permutation importance by providing accurate type-I error control. When used with a deep neural network, $\textit{CPI}$ consistently showed top accuracy ac
    
[^36]: 保持结构的变压器用于序列的SPD矩阵

    Structure-Preserving Transformers for Sequences of SPD Matrices. (arXiv:2309.07579v1 [cs.LG])

    [http://arxiv.org/abs/2309.07579](http://arxiv.org/abs/2309.07579)

    本文介绍了一种保持序列的对称正定矩阵的黎曼几何特性的结构保持变压器机制，并将其应用于自动睡眠分期，取得了高水平的阶段性能。

    

    近年来，基于变压器的自注意力机制已成功应用于各种上下文相关的数据类型的分析，从文本到图像等，包括非欧几里得几何的数据。本文提出了一种这样的机制，用于分类序列的对称正定矩阵，并在整个分析过程中保持它们的黎曼几何特性。我们将我们的方法应用于来自标准数据集中的脑电图协方差矩阵序列的自动睡眠分期，取得了高水平的阶段性能。

    In recent years, Transformer-based auto-attention mechanisms have been successfully applied to the analysis of a variety of context-reliant data types, from texts to images and beyond, including data from non-Euclidean geometries. In this paper, we present such a mechanism, designed to classify sequences of Symmetric Positive Definite matrices while preserving their Riemannian geometry throughout the analysis. We apply our method to automatic sleep staging on timeseries of EEG-derived covariance matrices from a standard dataset, obtaining high levels of stage-wise performance.
    
[^37]: Equivariant Data Augmentation for Generalization in Offline Reinforcement Learning.（离线强化学习中用于泛化的等变数据增强）

    Equivariant Data Augmentation for Generalization in Offline Reinforcement Learning. (arXiv:2309.07578v1 [cs.LG])

    [http://arxiv.org/abs/2309.07578](http://arxiv.org/abs/2309.07578)

    这个论文提出了一种用于离线强化学习中泛化的等变数据增强方法，并通过扩展等变集合并增强数据集来提高策略的测试性能。

    

    我们提出了一种新颖的方法来解决离线强化学习中的泛化挑战，其中智能体从固定数据集中学习而无需与环境进行额外交互。具体而言，我们旨在提高智能体对于超出分布范围的目标的泛化能力。为实现这一目标，我们提出了一种学习动力学模型并检查其是否等变于固定类型的变换，即状态空间中的平移。然后，我们使用一个熵正则化器来扩展等变集并通过所得到的转换样本对数据集进行增强。最后，我们基于增强的数据集使用一个现成的离线强化学习算法离线学习一个新的策略。我们的实验结果表明，我们的方法能够极大地提高策略在考虑的环境中的测试性能。

    We present a novel approach to address the challenge of generalization in offline reinforcement learning (RL), where the agent learns from a fixed dataset without any additional interaction with the environment. Specifically, we aim to improve the agent's ability to generalize to out-of-distribution goals. To achieve this, we propose to learn a dynamics model and check if it is equivariant with respect to a fixed type of transformation, namely translations in the state space. We then use an entropy regularizer to increase the equivariant set and augment the dataset with the resulting transformed samples. Finally, we learn a new policy offline based on the augmented dataset, with an off-the-shelf offline RL algorithm. Our experimental results demonstrate that our approach can greatly improve the test performance of the policy on the considered environments.
    
[^38]: 通过表示学习实现自然的机器人臂运动轨迹生成

    Naturalistic Robot Arm Trajectory Generation via Representation Learning. (arXiv:2309.07550v1 [cs.RO])

    [http://arxiv.org/abs/2309.07550](http://arxiv.org/abs/2309.07550)

    本文通过自监督学习方法使用自回归时空图神经网络，通过模仿人类示范者的运动轨迹来生成自然的机器人臂运动轨迹，为轮椅坐式辅助机器人提供更可预测和更类似人类的运动。

    

    在家庭环境中集成机械臂机器人需要更可预测和更类似人类的机器人运动，尤其对于能够支持瘫痪人群独立性的轮椅坐式辅助机器人而言更为重要。通过模仿人类示范者来生成自然的运动轨迹是一种方法。本文探讨了一种自监督模仿学习方法，使用自回归时空图神经网络来执行辅助饮水任务。我们通过可穿戴IMU传感器捕捉到的人臂运动数据作为无动作任务演示来学习多样的人体运动轨迹数据。利用多个参与者的观测到的臂部运动数据来为UR5e机器人臂生成自然且功能性的饮水运动轨迹。

    The integration of manipulator robots in household environments suggests a need for more predictable and human-like robot motion. This holds especially true for wheelchair-mounted assistive robots that can support the independence of people with paralysis. One method of generating naturalistic motion trajectories is via the imitation of human demonstrators. This paper explores a self-supervised imitation learning method using an autoregressive spatio-temporal graph neural network for an assistive drinking task. We address learning from diverse human motion trajectory data that were captured via wearable IMU sensors on a human arm as the action-free task demonstrations. Observed arm motion data from several participants is used to generate natural and functional drinking motion trajectories for a UR5e robot arm.
    
[^39]: 强化学习中的近似贝尔曼映射及其在鲁棒自适应滤波中的应用

    Proximal Bellman mappings for reinforcement learning and their application to robust adaptive filtering. (arXiv:2309.07548v1 [eess.SP])

    [http://arxiv.org/abs/2309.07548](http://arxiv.org/abs/2309.07548)

    本文提出了一种新的近似贝尔曼映射类，并在强化学习和鲁棒自适应滤波中应用。这些映射具有丰富的设计自由度，并可以解决线性自适应滤波中选择指数的问题。数值测试表明这种方法具有卓越的性能。

    

    本文旨在通过引入一类新颖的近似贝尔曼映射，探讨强化学习（RL）的算法理论核心。这些映射定义在重复核希尔伯特空间（RKHS）中，通过充分利用RKHS的近似性质和内积，不论折扣因子的值如何，都属于强大的希尔伯特（Firmly）非扩张映射家族，并具有丰富的设计自由度，甚至可以模拟经典贝尔曼映射的属性，为新的RL设计铺平道路。在提出的映射类上构建了一种近似策略迭代方案，以解决在线选择"最佳"指数$p$的问题，从而在线性自适应滤波中抵抗异常值而无需训练数据和异常值的统计属性。合成数据上的数值测试展示了所提方法的卓越性能。

    This paper aims at the algorithmic/theoretical core of reinforcement learning (RL) by introducing the novel class of proximal Bellman mappings. These mappings are defined in reproducing kernel Hilbert spaces (RKHSs), to benefit from the rich approximation properties and inner product of RKHSs, they are shown to belong to the powerful Hilbertian family of (firmly) nonexpansive mappings, regardless of the values of their discount factors, and possess ample degrees of design freedom to even reproduce attributes of the classical Bellman mappings and to pave the way for novel RL designs. An approximate policy-iteration scheme is built on the proposed class of mappings to solve the problem of selecting online, at every time instance, the "optimal" exponent $p$ in a $p$-norm loss to combat outliers in linear adaptive filtering, without training data and any knowledge on the statistical properties of the outliers. Numerical tests on synthetic data showcase the superior performance of the propo
    
[^40]: VerilogEval：评估大型语言模型在Verilog代码生成中的性能

    VerilogEval: Evaluating Large Language Models for Verilog Code Generation. (arXiv:2309.07544v1 [cs.LG])

    [http://arxiv.org/abs/2309.07544](http://arxiv.org/abs/2309.07544)

    本文提出了一个专门用于评估大型语言模型在Verilog代码生成中的性能的基准框架，并提供了一个包含156个问题的综合评估数据集。通过与黄金解决方案进行比较，可以自动测试Verilog代码的功能正确性。通过使用LLM生成的合成问题-代码对进行监督微调，可以改善预训练语言模型的Verilog代码生成能力。

    

    大型语言模型（LLMs）的日益普及为它们在各个领域的应用铺平了道路。本文提出了一个特别针对Verilog代码生成性能评估的基准框架，用于硬件设计和验证。我们提供了一个包含来自Verilog教学网站HDLBits的156个问题的综合评估数据集。该评估集包含了各种Verilog代码生成任务，从简单的组合电路到复杂的有限状态机。可以通过将生成的设计的瞬态仿真输出与黄金解决方案进行比较，自动测试Verilog代码完成的功能正确性。我们还证明了预训练语言模型的Verilog代码生成能力可以通过使用LLM生成的合成问题-代码对进行监督微调来改善。

    The increasing popularity of large language models (LLMs) has paved the way for their application in diverse domains. This paper proposes a benchmarking framework tailored specifically for evaluating LLM performance in the context of Verilog code generation for hardware design and verification. We present a comprehensive evaluation dataset consisting of 156 problems from the Verilog instructional website HDLBits. The evaluation set consists of a diverse set of Verilog code generation tasks, ranging from simple combinational circuits to complex finite state machines. The Verilog code completions can be automatically tested for functional correctness by comparing the transient simulation outputs of the generated design with a golden solution. We also demonstrate that the Verilog code generation capability of pretrained language models could be improved with supervised fine-tuning by bootstrapping with LLM generated synthetic problem-code pairs.
    
[^41]: 自适应逼近单调函数

    Adaptive approximation of monotone functions. (arXiv:2309.07530v1 [cs.LG])

    [http://arxiv.org/abs/2309.07530](http://arxiv.org/abs/2309.07530)

    这项研究解决了通过顺序查询函数值来逼近非递减函数的问题，并引入了一个通用算法GreedyBox，该算法对于任意函数都能实现最佳的样本复杂度。

    

    我们研究了在已知紧实数区间X、Y和已知概率测度μ下，通过顺序查询函数值来逼近非递减函数$f:\mathcal{X} \to \mathcal{Y}$在$L^p(\mu)$范数下的经典问题。对于任意函数$f$，我们给出了算法需要保证在停止后误差小于ε的逼近$\hat{f}$所需的最小评估次数的特征。与所有$f$上均一致适用的最坏情况结果不同，我们的复杂度度量是依赖于每个特定函数$f$的。为了解决这个问题，我们引入了GreedyBox，这是一个对数值积分的原始算法Novak(1992)的推广。我们证明了无论对于任何函数$f$，GreedyBox都能实现最优的样本复杂度，仅差对数因子。此外，我们还揭示了关于分段光滑函数的结果。也许预料之中的是，GreedyBox的$L^p(\mu)$误差减小得更快。

    We study the classical problem of approximating a non-decreasing function $f: \mathcal{X} \to \mathcal{Y}$ in $L^p(\mu)$ norm by sequentially querying its values, for known compact real intervals $\mathcal{X}$, $\mathcal{Y}$ and a known probability measure $\mu$ on $\cX$. For any function~$f$ we characterize the minimum number of evaluations of $f$ that algorithms need to guarantee an approximation $\hat{f}$ with an $L^p(\mu)$ error below $\epsilon$ after stopping. Unlike worst-case results that hold uniformly over all $f$, our complexity measure is dependent on each specific function $f$. To address this problem, we introduce GreedyBox, a generalization of an algorithm originally proposed by Novak (1992) for numerical integration. We prove that GreedyBox achieves an optimal sample complexity for any function $f$, up to logarithmic factors. Additionally, we uncover results regarding piecewise-smooth functions. Perhaps as expected, the $L^p(\mu)$ error of GreedyBox decreases much faster
    
[^42]: 超越相似性的学习：在自监督时间序列学习中融合正例对的不相似性

    Learning Beyond Similarities: Incorporating Dissimilarities between Positive Pairs in Self-Supervised Time Series Learning. (arXiv:2309.07526v1 [cs.LG])

    [http://arxiv.org/abs/2309.07526](http://arxiv.org/abs/2309.07526)

    本文提出了一种自监督学习方法（DEBS），通过融合正例对之间的不相似性，超越了简单的相似性，用于心电图(ECG)信号的心房颤动(AFib)检测。使用DEBS可以提高10%的检测准确度，强调了利用不相似性来编码时间序列数据的动态特征。

    

    通过识别连续输入之间的相似性，自监督学习方法在时间序列分析中展示了其编码时间数据固有静态特征的有效性。然而，对相似性的独家关注可能导致忽视对模拟心血管疾病建模至关重要的动态属性的表示。本文介绍了一种名为Distilled Encoding Beyond Similarities (DEBS)的自监督学习方法，通过整合正例对之间的不相似性，超越了简单的相似性。该框架被应用于心电图(ECG)信号，提高了不同被试中房颤(AFib)检测准确度的10%。DEBS强调通过在优化过程中利用不相似性来编码时间序列数据的动态特征，实现更精细的表示的潜力。总的来说，本研究开拓了一种在自监督学习中利用正例对不相似性的方法，有助于提高时间序列数据的编码质量。

    By identifying similarities between successive inputs, Self-Supervised Learning (SSL) methods for time series analysis have demonstrated their effectiveness in encoding the inherent static characteristics of temporal data. However, an exclusive emphasis on similarities might result in representations that overlook the dynamic attributes critical for modeling cardiovascular diseases within a confined subject cohort. Introducing Distilled Encoding Beyond Similarities (DEBS), this paper pioneers an SSL approach that transcends mere similarities by integrating dissimilarities among positive pairs. The framework is applied to electrocardiogram (ECG) signals, leading to a notable enhancement of +10\% in the detection accuracy of Atrial Fibrillation (AFib) across diverse subjects. DEBS underscores the potential of attaining a more refined representation by encoding the dynamic characteristics of time series data, tapping into dissimilarities during the optimization process. Broadly, the strat
    
[^43]: 高度并行的热图排序及其在可解释聚类中的应用

    Massively-Parallel Heat Map Sorting and Applications To Explainable Clustering. (arXiv:2309.07486v1 [cs.DS])

    [http://arxiv.org/abs/2309.07486](http://arxiv.org/abs/2309.07486)

    本研究针对热图排序问题在保留簇的同时重新排列和合并点和维度，并提供了一个高度并行的固定参数算法。通过实证比较，我们发现该算法在应用于电子邮件和计算机网络等领域时，与k-means和DBSCAN的效果相当。

    

    给定具有$k$个标签的点集，我们将热图排序问题定义为在保留簇（标签）的同时重新排列和合并点和维度。如果一个簇保持连通，即没有被分裂成多个簇且没有两个簇被合并，则称该簇被保留。我们证明了该问题是NP-hard的，并在高度并行计算模型中提供了一个有固定次数轮的固定参数算法，其中每个机器具有亚线性内存，总内存为线性。对于问题的NP-hard特殊情况，我们提供了一个近似算法。我们通过局部敏感哈希的降维方法，基于电子邮件和计算机网络的有向和无向图，经验性地将我们的算法与k-means和基于密度的聚类（DBSCAN）进行了比较。

    Given a set of points labeled with $k$ labels, we introduce the heat map sorting problem as reordering and merging the points and dimensions while preserving the clusters (labels). A cluster is preserved if it remains connected, i.e., if it is not split into several clusters and no two clusters are merged.  We prove the problem is NP-hard and we give a fixed-parameter algorithm with a constant number of rounds in the massively parallel computation model, where each machine has a sublinear memory and the total memory of the machines is linear. We give an approximation algorithm for a NP-hard special case of the problem. We empirically compare our algorithm with k-means and density-based clustering (DBSCAN) using a dimensionality reduction via locality-sensitive hashing on several directed and undirected graphs of email and computer networks.
    
[^44]: 使用确定性投影置信网络提升自动编码

    Improved Auto-Encoding using Deterministic Projected Belief Networks. (arXiv:2309.07481v1 [cs.LG])

    [http://arxiv.org/abs/2309.07481](http://arxiv.org/abs/2309.07481)

    本研究中，我们利用确定性投影置信网络（D-PBN）结合可训练的复合激活函数（TCAs），对自动编码器进行了改进，实验结果表明，相比于传统自动编码器，包括变分自动编码器，具有TCAs的D-PBN自动编码器表现更好。

    

    本文利用确定性投影置信网络（D-PBN）的独特性质，充分利用可训练的复合激活函数（TCAs）。D-PBN是一种通过向后传播神经网络来运行的自动编码器。TCAs是具有复杂单调递增形状的激活函数，可以改变数据的分布，使后续的线性变换更加有效。因为D-PBN是通过向后传播运行的，所以在重建过程中，TCAs被反转，恢复数据的原始分布，从而充分利用给定的TCAs进行分析和重建。本文表明，具有TCAs的D-PBN自动编码器可以显著优于标准的自动编码器，包括变分自动编码器。

    In this paper, we exploit the unique properties of a deterministic projected belief network (D-PBN) to take full advantage of trainable compound activation functions (TCAs). A D-PBN is a type of auto-encoder that operates by "backing up" through a feed-forward neural network. TCAs are activation functions with complex monotonic-increasing shapes that change the distribution of the data so that the linear transformation that follows is more effective. Because a D-PBN operates by "backing up", the TCAs are inverted in the reconstruction process, restoring the original distribution of the data, thus taking advantage of a given TCA in both analysis and reconstruction. In this paper, we show that a D-PBN auto-encoder with TCAs can significantly out-perform standard auto-encoders including variational auto-encoders.
    
[^45]: 使用声学单元的直接文本到语音翻译系统

    Direct Text to Speech Translation System using Acoustic Units. (arXiv:2309.07478v1 [cs.CL])

    [http://arxiv.org/abs/2309.07478](http://arxiv.org/abs/2309.07478)

    本文提出了一种使用离散声学单元的直接文本到语音翻译系统，通过结合语音编码器和聚类算法提取声学单元，并使用编码器-解码器架构预测和生成语音。实验证明，该系统在多数评估的语言对上表现出竞争性能，并且使用更多语言预训练模型进行初始化会带来显著的改进。

    

    本文提出了一种使用离散声学单元的直接文本到语音翻译系统。该框架使用不同源语言的文本作为输入，在不需要该语言的文本转录的情况下生成目标语言的语音。受到以前直接语音到语音翻译系统中声学单元的成功启发，我们使用相同的流水线通过结合语音编码器和聚类算法提取声学单元。一旦获得单元，就会训练一个编码器-解码器架构来预测它们。然后，一个声码器从单元生成语音。我们的直接文本到语音翻译方法在使用两个不同的文本mBART模型作为初始化的新CVSS语料库上进行了测试。所提出的系统在大多数评估的语言对上表现出竞争性能。此外，结果表明，当使用更多语言预训练模型来初始化我们提出的架构时，有显着的改进。

    This paper proposes a direct text to speech translation system using discrete acoustic units. This framework employs text in different source languages as input to generate speech in the target language without the need for text transcriptions in this language. Motivated by the success of acoustic units in previous works for direct speech to speech translation systems, we use the same pipeline to extract the acoustic units using a speech encoder combined with a clustering algorithm. Once units are obtained, an encoder-decoder architecture is trained to predict them. Then a vocoder generates speech from units. Our approach for direct text to speech translation was tested on the new CVSS corpus with two different text mBART models employed as initialisation. The systems presented report competitive performance for most of the language pairs evaluated. Besides, results show a remarkable improvement when initialising our proposed architecture with a model pre-trained with more languages.
    
[^46]: 在物联网环境中检测未知攻击: 一种用于增强网络入侵检测的开放集分类器

    Detecting Unknown Attacks in IoT Environments: An Open Set Classifier for Enhanced Network Intrusion Detection. (arXiv:2309.07461v1 [cs.CR])

    [http://arxiv.org/abs/2309.07461](http://arxiv.org/abs/2309.07461)

    这项研究介绍了一个针对物联网环境定制的网络入侵检测系统的开放集分类器框架，利用图像表示和堆叠子聚类技术来识别未知攻击。

    

    物联网设备在生活的各个方面都得到了广泛的应用，这引入了互联的时代，创造了新的网络安全挑战，并强调了强大的入侵检测系统的需求。然而，传统的安全系统是基于封闭世界视角设计的，往往面临与不断发展的威胁环境中新的、陌生的攻击相处理的挑战。在本文中，我们介绍了一个框架，旨在解决物联网环境下网络入侵检测系统（NIDS）中的开放集识别（OSR）问题。我们的框架利用基于图像的数据表示，从网络流量中提取空间和时间模式。此外，我们还集成了堆叠和子聚类技术，通过有效地建模复杂和多样化的良性行为，实现对未知攻击的识别。

    The widespread integration of Internet of Things (IoT) devices across all facets of life has ushered in an era of interconnectedness, creating new avenues for cybersecurity challenges and underscoring the need for robust intrusion detection systems. However, traditional security systems are designed with a closed-world perspective and often face challenges in dealing with the ever-evolving threat landscape, where new and unfamiliar attacks are constantly emerging. In this paper, we introduce a framework aimed at mitigating the open set recognition (OSR) problem in the realm of Network Intrusion Detection Systems (NIDS) tailored for IoT environments. Our framework capitalizes on image-based representations of packet-level data, extracting spatial and temporal patterns from network traffic. Additionally, we integrate stacking and sub-clustering techniques, enabling the identification of unknown attacks by effectively modeling the complex and diverse nature of benign behavior. The empiric
    
[^47]: SC-MAD: 混合高阶网络用于数据增强

    SC-MAD: Mixtures of Higher-order Networks for Data Augmentation. (arXiv:2309.07453v1 [stat.ML])

    [http://arxiv.org/abs/2309.07453](http://arxiv.org/abs/2309.07453)

    本论文提出了一种名为SC-MAD的方法，利用混合高阶网络对数据进行增强。通过线性和非线性混合机制返回现有标记样本的混合物，以及一种凸聚类混合方法用于描述多个复形复杂网络之间的数据驱动关系。这种方法在复形复杂分类中表现出良好的性能。

    

    众多复杂系统中存在着多维交互，这促使我们将基于图的成对连接拓展到高阶关系。特别地，复形复杂网络已经启发了基于复形复杂模型的图神经网络的泛化。在这种系统上的学习需要大量的数据，而这些数据可能难以获取或者成本高昂。我们提出了通过线性和非线性混合机制对复形复杂网络进行数据增强的方法，返回现有标记样本的混合物。除了传统的成对混合，我们还提出了一种凸聚类混合方法，用于描述多个复形复杂网络之间的数据驱动关系。我们在理论上证明了生成的合成复形复杂网络在同态密度方面插值了现有数据。我们的方法在合成和真实数据集上进行了复形复杂分类的实证研究。

    The myriad complex systems with multiway interactions motivate the extension of graph-based pairwise connections to higher-order relations. In particular, the simplicial complex has inspired generalizations of graph neural networks (GNNs) to simplicial complex-based models. Learning on such systems requires large amounts of data, which can be expensive or impossible to obtain. We propose data augmentation of simplicial complexes through both linear and nonlinear mixup mechanisms that return mixtures of existing labeled samples. In addition to traditional pairwise mixup, we present a convex clustering mixup approach for a data-driven relationship among several simplicial complexes. We theoretically demonstrate that the resultant synthetic simplicial complexes interpolate among existing data with respect to homomorphism densities. Our method is demonstrated on both synthetic and real-world datasets for simplicial complex classification.
    
[^48]: 解决图神经切向核是否等价于训练图神经网络？

    Is Solving Graph Neural Tangent Kernel Equivalent to Training Graph Neural Network?. (arXiv:2309.07452v1 [cs.LG])

    [http://arxiv.org/abs/2309.07452](http://arxiv.org/abs/2309.07452)

    本文探讨了解决图神经切向核是否等价于训练图神经网络的问题，提供了三个新的理论结果。

    

    理解深度学习原理的一个新趋势是通过神经切向核（NTK）来解释深度学习的工作原理。NTK是一种等价于使用梯度下降训练多层无限宽神经网络的核方法。NTK在理论深度学习中是一个重要的进展，因为它允许研究人员使用传统的数学工具来分析深度神经网络的性质，并从理论角度解释各种神经网络技术。在图学习中，NTK的一种自然推广是图神经切向核（GNTK），研究人员已经提出了GNTK的公式，并从实证上表明该核方法在各种生物信息学数据集上可以达到与GNN相似的准确性。现在剩下的问题是，解决GNTK回归是否等价于使用梯度下降训练无限宽多层GNN。在本文中，我们提供了三个新的理论结果。

    A rising trend in theoretical deep learning is to understand why deep learning works through Neural Tangent Kernel (NTK) [jgh18], a kernel method that is equivalent to using gradient descent to train a multi-layer infinitely-wide neural network. NTK is a major step forward in the theoretical deep learning because it allows researchers to use traditional mathematical tools to analyze properties of deep neural networks and to explain various neural network techniques from a theoretical view. A natural extension of NTK on graph learning is \textit{Graph Neural Tangent Kernel (GNTK)}, and researchers have already provide GNTK formulation for graph-level regression and show empirically that this kernel method can achieve similar accuracy as GNNs on various bioinformatics datasets [dhs+19]. The remaining question now is whether solving GNTK regression is equivalent to training an infinite-wide multi-layer GNN using gradient descent. In this paper, we provide three new theoretical results. Fi
    
[^49]: TensorFlow混沌预测与失控

    TensorFlow Chaotic Prediction and Blow Up. (arXiv:2309.07450v1 [cs.LG])

    [http://arxiv.org/abs/2309.07450](http://arxiv.org/abs/2309.07450)

    该论文使用TensorFlow库来预测高维非线性系统的混沌动态，发现了TensorFlow库的长期预测行为有失控的问题。

    

    预测混沌系统的动力学是神经网络和机器学习的一项最具挑战性的任务之一。在这里，我们旨在预测一个高维非线性系统的时空混沌动态。在我们的尝试中，我们使用了代表深度神经网络训练和预测的TensorFlow库。虽然我们的结果令人鼓舞，并且显示出所考虑系统的动力学可以在短时间内被预测，但我们也间接发现了TensorFlow库的一个意外和不可取的行为。具体而言，由于TensorFlow库的非确定性行为，系统的长期预测行为迅速恶化和崩溃。在这里，我们提供了短期预测能力的数值证据，以及长期可预测性的崩溃。

    Predicting the dynamics of chaotic systems is one of the most challenging tasks for neural networks, and machine learning in general. Here we aim to predict the spatiotemporal chaotic dynamics of a high-dimensional non-linear system. In our attempt we use the TensorFlow library, representing the state of the art for deep neural networks training and prediction. While our results are encouraging, and show that the dynamics of the considered system can be predicted for short time, we also indirectly discovered an unexpected and undesirable behavior of the TensorFlow library. More specifically, the longer term prediction of the system's chaotic behavior quickly deteriorates and blows up due to the nondeterministic behavior of the TensorFlow library. Here we provide numerical evidence of the short time prediction ability, and of the longer term predictability blow up.
    
[^50]: 在张量和SVM Trick基础上重新构建单层LLM注意力机制，并在矩阵乘法时间内解决它的快速优化视角

    A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time. (arXiv:2309.07418v1 [cs.DS])

    [http://arxiv.org/abs/2309.07418](http://arxiv.org/abs/2309.07418)

    在本文中，我们使用张量和SVM Trick重新构建了单层LLM注意力机制，并提供了对应的优化视角。我们的工作可以在矩阵乘法时间内解决注意力回归问题。

    

    大型语言模型(LLMs)在我们日常生活的各个方面都发挥着重要作用。解决LLMs中的注意力回归是优化LLMs的基本任务。本文致力于为一层注意力网络目标函数 $L(X,Y) = \sum_{j_0 = 1}^n \sum_{i_0 = 1}^d ( \langle \langle \exp( \mathsf{A}_{j_0} x ) , {\bf 1}_n \rangle^{-1} \exp( \mathsf{A}_{j_0} x ), A_{3} Y_{*,i_0} \rangle - b_{j_0,i_0} )^2$ 提供可证明的保证。这里 $\mathsf{A} \in \mathbb{R}^{n^2 \times d^2}$ 是$A_1 \in \mathbb{R}^{n \times d}$ 和 $A_2 \in \mathbb{R}^{n \times d}$ 的Kronecker积。$A_3$ 是$\mathbb{R}^{n \times d}$ 中的一个矩阵，$\mathsf{A}_{j_0} \in \mathbb{R}^{n \times d^2}$ 是$\mathsf{A}$ 的第$j_0$个块。$X, Y \in \mathbb{R}^{d \times d}$ 是我们要学习的变量。$B \in \mathbb{R}^{n \times d}$ 和 $b_{j_0,i_0} \in \mathbb{R}$ 是$B$ 的第$j_0$行和第$i_0$列的一个元素，$Y_{*,i_0} \in \mathbb{R}^d$ 是$Y$ 的第$i_0$列。

    Large language models (LLMs) have played a pivotal role in revolutionizing various facets of our daily existence. Solving attention regression is a fundamental task in optimizing LLMs. In this work, we focus on giving a provable guarantee for the one-layer attention network objective function $L(X,Y) = \sum_{j_0 = 1}^n \sum_{i_0 = 1}^d ( \langle \langle \exp( \mathsf{A}_{j_0} x ) , {\bf 1}_n \rangle^{-1} \exp( \mathsf{A}_{j_0} x ), A_{3} Y_{*,i_0} \rangle - b_{j_0,i_0} )^2$. Here $\mathsf{A} \in \mathbb{R}^{n^2 \times d^2}$ is Kronecker product between $A_1 \in \mathbb{R}^{n \times d}$ and $A_2 \in \mathbb{R}^{n \times d}$. $A_3$ is a matrix in $\mathbb{R}^{n \times d}$, $\mathsf{A}_{j_0} \in \mathbb{R}^{n \times d^2}$ is the $j_0$-th block of $\mathsf{A}$. The $X, Y \in \mathbb{R}^{d \times d}$ are variables we want to learn. $B \in \mathbb{R}^{n \times d}$ and $b_{j_0,i_0} \in \mathbb{R}$ is one entry at $j_0$-th row and $i_0$-th column of $B$, $Y_{*,i_0} \in \mathbb{R}^d$ is the $i_
    
[^51]: 在线性循环神经网络中推进正则语言推理

    Advancing Regular Language Reasoning in Linear Recurrent Neural Networks. (arXiv:2309.07412v1 [cs.CL])

    [http://arxiv.org/abs/2309.07412](http://arxiv.org/abs/2309.07412)

    通过分析现有的线性循环神经网络（LRNN），我们提出了一种新的LRNN模型，该模型配备了一个块对角且输入相关的转移矩阵，并且是唯一一个能够在正则语言任务上进行长度外推的LRNN。

    

    在最近的研究中，线性循环神经网络（LRNN）在自然语言建模和长程建模中取得了与Transformer相当的性能，同时提供了快速的并行训练和恒定的推理成本。在对LRNN重新产生兴趣的同时，我们研究它们是否能够学习训练序列中的隐藏规则，例如正则语言的语法结构。我们对一些已有的LRNN进行理论分析，发现它们在正则语言上存在限制。在这个分析的基础上，我们提出了一种新的LRNN，配备了一个块对角和输入相关的转移矩阵。实验表明，所提出的模型是唯一一个能够在正则语言任务（如求和、偶数对、模运算等）上进行长度外推的LRNN。

    In recent studies, linear recurrent neural networks (LRNNs) have achieved Transformer-level performance in natural language modeling and long-range modeling while offering rapid parallel training and constant inference costs. With the resurged interest in LRNNs, we study whether they can learn the hidden rules in training sequences, such as the grammatical structures of regular language. We theoretically analyze some existing LRNNs and discover their limitations on regular language. Motivated by the analysis, we propose a new LRNN equipped with a block-diagonal and input-dependent transition matrix. Experiments suggest that the proposed model is the only LRNN that can perform length extrapolation on regular language tasks such as Sum, Even Pair, and Modular Arithmetic.
    
[^52]: 使用对比学习和最小最大熵的图上半监督领域适应

    Semi-supervised Domain Adaptation on Graphs with Contrastive Learning and Minimax Entropy. (arXiv:2309.07402v1 [cs.LG])

    [http://arxiv.org/abs/2309.07402](http://arxiv.org/abs/2309.07402)

    提出了一种名为SemiGCL的方法，使用图对比学习和最小最大熵训练来解决图上的半监督领域适应问题，该方法通过对比学得表示来生成信息丰富的节点表示，并使用对抗优化减小域差异。在实验中取得了良好的性能。

    

    由于数据标记成本高昂，图中的标签稀缺在现实世界中经常遇到。为此，图上的半监督领域适应(SSDA)旨在利用标记源图的知识来帮助有限标签的目标图中的节点分类。SSDA任务需要克服源图和目标图之间的域差异。然而，到目前为止，现有的用于跨图节点分类的方法尚未正式考虑这个具有挑战性的研究问题。为了解决图上的SSDA问题，提出了一种名为SemiGCL的新方法，它获益于图对比学习和最小最大熵训练。SemiGCL通过对比从图的局部和全局视图中学到的表示来生成信息丰富的节点表示。此外，SemiGCL通过目标图中未标记节点的熵损失进行对抗优化，以减小域差异。在基准数据集上的实验结果表明，SemiGCL在图上的SSDA任务中取得了良好的性能。

    Label scarcity in a graph is frequently encountered in real-world applications due to the high cost of data labeling. To this end, semi-supervised domain adaptation (SSDA) on graphs aims to leverage the knowledge of a labeled source graph to aid in node classification on a target graph with limited labels. SSDA tasks need to overcome the domain gap between the source and target graphs. However, to date, this challenging research problem has yet to be formally considered by the existing approaches designed for cross-graph node classification. To tackle the SSDA problem on graphs, a novel method called SemiGCL is proposed, which benefits from graph contrastive learning and minimax entropy training. SemiGCL generates informative node representations by contrasting the representations learned from a graph's local and global views. Additionally, SemiGCL is adversarially optimized with the entropy loss of unlabeled target nodes to reduce domain divergence. Experimental results on benchmark d
    
[^53]: 通过扩散模型实现语义对抗攻击

    Semantic Adversarial Attacks via Diffusion Models. (arXiv:2309.07398v1 [cs.CV])

    [http://arxiv.org/abs/2309.07398](http://arxiv.org/abs/2309.07398)

    本文提出了一个用于生成语义对抗攻击的框架，并使用扩散模型中的潜在空间中的语义信息。在该框架中，有两个变种方法：语义转换方法(ST)，通过微调潜在空间和/或扩散模型本身来生成图像；潜在屏蔽方法(LM)，利用另一目标图像屏蔽潜在空间，并使用基于局部反向传播的解释方法。实验结果验证了该框架的有效性。

    

    传统的对抗攻击主要通过在像素空间中添加对抗性扰动来操纵干净的样本。相比之下，语义对抗攻击更加关注改变干净样本的语义属性，例如颜色、上下文和特征，在实际世界中更具可行性。在本文中，我们提出了一个框架，通过利用最近的扩散模型，能够快速生成语义对抗攻击，因为语义信息包含在训练有素的扩散模型的潜在空间中。然后，该框架有两个变种：1) 语义转换方法(ST)通过微调生成图像的潜在空间和/或扩散模型本身；2) 潜在屏蔽方法(LM)使用另一目标图像屏蔽潜在空间，并使用基于局部反向传播的解释方法。此外，ST方法可在白盒或黑盒设置中应用。在CelebA-HQ和AFHQ数据集上进行了大量实验。

    Traditional adversarial attacks concentrate on manipulating clean examples in the pixel space by adding adversarial perturbations. By contrast, semantic adversarial attacks focus on changing semantic attributes of clean examples, such as color, context, and features, which are more feasible in the real world. In this paper, we propose a framework to quickly generate a semantic adversarial attack by leveraging recent diffusion models since semantic information is included in the latent space of well-trained diffusion models. Then there are two variants of this framework: 1) the Semantic Transformation (ST) approach fine-tunes the latent space of the generated image and/or the diffusion model itself; 2) the Latent Masking (LM) approach masks the latent space with another target image and local backpropagation-based interpretation methods. Additionally, the ST approach can be applied in either white-box or black-box settings. Extensive experiments are conducted on CelebA-HQ and AFHQ datas
    
[^54]: EnCodecMAE: 利用神经编解码器进行通用音频表示学习

    EnCodecMAE: Leveraging neural codecs for universal audio representation learning. (arXiv:2309.07391v1 [cs.SD])

    [http://arxiv.org/abs/2309.07391](http://arxiv.org/abs/2309.07391)

    本文提出了一种称为EnCodecMAE的方法，利用神经编解码器EnCodec生成离散目标，用于基于遮蔽自动编码器（MAE）学习通用音频模型。通过在涵盖语音、音乐和环境声音的多个音频任务上的评估，发现EnCodecMAE达到了与领先的音频表示模型相当甚至更好的性能。

    

    通用音频表示学习的目标是获得可以用于涉及语音、音乐或环境声音的各种后续任务的基础模型。为了解决这个问题，通常使用受自监督模型（如BERT）启发的方法，并将其应用于音频。这些模型依赖于文本的离散性质，因此采用这种方法来处理音频需要改变学习目标或将音频信号映射到一组离散类别。在这项工作中，我们探索了使用神经音频编解码器EnCodec生成用于基于遮蔽自动编码器（MAE）学习通用音频模型的离散目标的方法。我们评估了该方法，称之为EnCodecMAE，在涵盖语音、音乐和环境声音的广泛音频任务上，其性能相当或优于领先的音频表示模型。

    The goal of universal audio representation learning is to obtain foundational models that can be used for a variety of downstream tasks involving speech, music or environmental sounds. To approach this problem, methods inspired by self-supervised models from NLP, like BERT, are often used and adapted to audio. These models rely on the discrete nature of text, hence adopting this type of approach for audio processing requires either a change in the learning objective or mapping the audio signal to a set of discrete classes. In this work, we explore the use of EnCodec, a neural audio codec, to generate discrete targets for learning an universal audio model based on a masked autoencoder (MAE). We evaluate this approach, which we call EncodecMAE, on a wide range of audio tasks spanning speech, music and environmental sounds, achieving performances comparable or better than leading audio representation models.
    
[^55]: 在强化学习中使用的近似的某些本地空间中的收敛速度

    Rates of Convergence in Certain Native Spaces of Approximations used in Reinforcement Learning. (arXiv:2309.07383v1 [eess.SY])

    [http://arxiv.org/abs/2309.07383](http://arxiv.org/abs/2309.07383)

    本文研究了在强化学习中出现的值函数近似在特定本地空间中的收敛速度，提出了运用算子方程进行离线近似的方法，并通过有限维近似空间中的功率函数得到了值函数近似误差的上界。这些结果改进和细化了值函数近似的收敛性。

    

    本文研究了在一组再生核希尔伯特空间（RKHS）$H(\Omega)$中出现的一些值函数近似的收敛速度。通过在特定类的本地空间中构建一个最优控制问题，得到了离线近似的算子方程的强收敛速度，这个算子方程出现在策略迭代中。利用有限维近似空间$H_N$在本地空间$H(\Omega)$中的功率函数$\Pwr_{H,N}$，得到了值函数近似误差的显式上界。这些上界具有几何性质，并对值函数近似的收敛性有了一些改进和细化。

    This paper studies convergence rates for some value function approximations that arise in a collection of reproducing kernel Hilbert spaces (RKHS) $H(\Omega)$. By casting an optimal control problem in a specific class of native spaces, strong rates of convergence are derived for the operator equation that enables offline approximations that appear in policy iteration. Explicit upper bounds on error in value function approximations are derived in terms of power function $\Pwr_{H,N}$ for the space of finite dimensional approximants $H_N$ in the native space $H(\Omega)$. These bounds are geometric in nature and refine some well-known, now classical results concerning convergence of approximations of value functions.
    
[^56]: 异常值存在下的鲁棒估计不确定性的Beta分位数回归

    Beta quantile regression for robust estimation of uncertainty in the presence of outliers. (arXiv:2309.07374v1 [cs.LG])

    [http://arxiv.org/abs/2309.07374](http://arxiv.org/abs/2309.07374)

    本研究提出了一种鲁棒的分位数回归方法，通过融合鲁棒散度的概念，解决了分位数回归对异常值特征敏感的问题。

    

    分位数回归可以用于估计深度神经网络中的推断不确定性，并生成预测区间。在临床诊断等关键应用中，量化不确定性特别重要，因为在确定疾病状况和制定适当治疗方案时，对不确定性进行真实评估至关重要。分位数回归模型最常见的应用是在无法指定参数似然的情况下。尽管分位数回归对异常值的响应观察具有较强的鲁棒性，但对异常值的协变观察（特征）敏感。异常值特征可能会影响深度学习回归问题的性能，如风格转换、图像重建和深度异常检测，潜在地导致误导性结论。为了解决这个问题，我们提出了一种鲁棒的分位数回归解决方案，它融合了鲁棒散度的概念。

    Quantile Regression (QR) can be used to estimate aleatoric uncertainty in deep neural networks and can generate prediction intervals. Quantifying uncertainty is particularly important in critical applications such as clinical diagnosis, where a realistic assessment of uncertainty is essential in determining disease status and planning the appropriate treatment. The most common application of quantile regression models is in cases where the parametric likelihood cannot be specified. Although quantile regression is quite robust to outlier response observations, it can be sensitive to outlier covariate observations (features). Outlier features can compromise the performance of deep learning regression problems such as style translation, image reconstruction, and deep anomaly detection, potentially leading to misleading conclusions. To address this problem, we propose a robust solution for quantile regression that incorporates concepts from robust divergence. We compare the performance of 
    
[^57]: 深度神经网络的核平衡方程

    The kernel-balanced equation for deep neural networks. (arXiv:2309.07367v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2309.07367](http://arxiv.org/abs/2309.07367)

    本文提出了深度神经网络的核平衡方程，解释了数据集分布估计中的不稳定性和尺度机制。网络的输出是数据集的局部平均，平均的尺度随着训练逐渐减小并导致不稳定性。

    

    在过去十年中，深度神经网络已经展现出许多有益的应用。通过对有限数据集进行训练，网络可以获得广义函数。广义化程度是数据空间中的近似尺度的实现。特别地，在数据集复杂时，尺度不明确。在本文中，我们考虑了一个用于数据集分布估计的网络。我们展示了估计是不稳定的，并且不稳定性取决于数据密度和训练持续时间。我们推导了核平衡方程，它给出了解的简短现象学描述。该方程告诉我们不稳定性的原因和尺度的机制。网络的输出作为预测是数据集的局部平均，平均的尺度沿着方程确定。尺度在训练过程中逐渐减小，最终导致不稳定性。

    Deep neural networks have shown many fruitful applications in this decade. A network can get the generalized function through training with a finite dataset. The degree of generalization is a realization of the proximity scale in the data space. Specifically, the scale is not clear if the dataset is complicated. Here we consider a network for the distribution estimation of the dataset. We show the estimation is unstable and the instability depends on the data density and training duration. We derive the kernel-balanced equation, which gives a short phenomenological description of the solution. The equation tells us the reason for the instability and the mechanism of the scale. The network outputs a local average of the dataset as a prediction and the scale of averaging is determined along the equation. The scale gradually decreases along training and finally results in instability in our case.
    
[^58]: Hodge-Aware Contrastive Learning（基于豪奇分解的对比学习）

    Hodge-Aware Contrastive Learning. (arXiv:2309.07364v1 [cs.LG])

    [http://arxiv.org/abs/2309.07364](http://arxiv.org/abs/2309.07364)

    本文提出了一种基于豪奇分解的对比学习方法，用于处理单纯复合体数据，并生成具有特定谱信息的嵌入。通过编码数据不变性和设计合适的增强方法，以及重新权衡负样本的重要性，我们得到了一个反映谱信息的嵌入空间。

    

    单纯复合体在对具有多向依赖关系的数据进行建模方面表现出很好的效果，例如在网络的边缘上定义的数据或其他高阶结构内的数据。通过豪奇分解，可以将其谱分解为三个可解释的子空间，这在许多应用中都具有基础性。我们利用这种分解来开发一种对比自我监督学习方法，用于处理单纯复合体数据并生成蕴含特定谱信息的嵌入。具体地，我们通过单纯神经网络来编码相关数据的不变性，并设计出具有适当谱特性的增强方法，以生成正对比示例。此外，我们通过考虑负样本的豪奇分量与锚点相似性，重新权衡对比损失中负样本的重要性。通过加强较少相似实例之间的分离，我们获得了一个反映谱信息的嵌入空间。

    Simplicial complexes prove effective in modeling data with multiway dependencies, such as data defined along the edges of networks or within other higher-order structures. Their spectrum can be decomposed into three interpretable subspaces via the Hodge decomposition, resulting foundational in numerous applications. We leverage this decomposition to develop a contrastive self-supervised learning approach for processing simplicial data and generating embeddings that encapsulate specific spectral information.Specifically, we encode the pertinent data invariances through simplicial neural networks and devise augmentations that yield positive contrastive examples with suitable spectral properties for downstream tasks. Additionally, we reweight the significance of negative examples in the contrastive loss, considering the similarity of their Hodge components to the anchor. By encouraging a stronger separation among less similar instances, we obtain an embedding space that reflects the spect
    
[^59]: 使用CLUB-PLS解决影像遗传学中的维度问题

    Tackling the dimensions in imaging genetics with CLUB-PLS. (arXiv:2309.07352v1 [q-bio.GN])

    [http://arxiv.org/abs/2309.07352](http://arxiv.org/abs/2309.07352)

    本文介绍了一种名为Cluster-Bootstrap PLS（CLUB-PLS）的基于偏最小二乘的框架，用于解决影像遗传学中的维度问题。该框架可以处理两个领域的大量输入维度和大样本量，并使用聚类自助法提供稳健的统计数据。

    

    影像遗传学和类似领域的一个主要挑战是将一个领域（如遗传数据）中的高维数据与另一个领域（如脑成像数据）中的高维数据进行关联。该领域的标准方法是通过遗传因素和成像表型进行大规模单变量分析。这意味着对每个预定义的成像测量执行一次全基因组关联研究（GWAS）。尽管这种方法非常成功，但它的一个缺点是表型必须事先定义好。因此，不被限制在预选感兴趣区域内或反映更大的整个大脑模式的效应很容易被忽视。在这项工作中，我们引入了一种基于偏最小二乘（PLS）的框架，称为Cluster-Bootstrap PLS（CLUB-PLS），它可以同时处理两个领域的大量输入维度以及大样本量。该框架的一个关键因素是使用聚类自助法为单个输入特征提供稳健的统计数据。

    A major challenge in imaging genetics and similar fields is to link high-dimensional data in one domain, e.g., genetic data, to high dimensional data in a second domain, e.g., brain imaging data. The standard approach in the area are mass univariate analyses across genetic factors and imaging phenotypes. That entails executing one genome-wide association study (GWAS) for each pre-defined imaging measure. Although this approach has been tremendously successful, one shortcoming is that phenotypes must be pre-defined. Consequently, effects that are not confined to pre-selected regions of interest or that reflect larger brain-wide patterns can easily be missed. In this work we introduce a Partial Least Squares (PLS)-based framework, which we term Cluster-Bootstrap PLS (CLUB-PLS), that can work with large input dimensions in both domains as well as with large sample sizes. One key factor of the framework is to use cluster bootstrap to provide robust statistics for single input features in b
    
[^60]: 通过泰勒展开和稀疏分解在值域和傅里叶域中高效学习偏微分方程

    Efficient Learning of PDEs via Taylor Expansion and Sparse Decomposition into Value and Fourier Domains. (arXiv:2309.07344v1 [cs.LG])

    [http://arxiv.org/abs/2309.07344](http://arxiv.org/abs/2309.07344)

    通过Reel算法，利用随机投影和稀疏分解在值域和傅里叶域中，高效学习偏微分方程，加快了科学发现的速度。

    

    从实验数据中加速学习偏微分方程(PDEs)将加快科学发现的速度。先前的随机算法利用PDE更新的稀疏性加速。然而，这些方法只适用于在值域中具有稀疏特征的一类可分解的PDEs。我们提出了Reel，通过随机投影加速学习PDEs，具有更广泛的适用性。Reel利用稀疏性将密集更新分解为值域和频域中的稀疏更新。这种分解使得在更新源由跨越大区域渐变的项(频域稀疏)以及在少数“界面”区域集中快速更新的情况下能够进行高效学习(值域稀疏)。然后，应用随机投影来压缩稀疏信号进行学习。为了扩展模型的适用性，还使用了泰勒级数展开。

    Accelerating the learning of Partial Differential Equations (PDEs) from experimental data will speed up the pace of scientific discovery. Previous randomized algorithms exploit sparsity in PDE updates for acceleration. However such methods are applicable to a limited class of decomposable PDEs, which have sparse features in the value domain. We propose Reel, which accelerates the learning of PDEs via random projection and has much broader applicability. Reel exploits the sparsity by decomposing dense updates into sparse ones in both the value and frequency domains. This decomposition enables efficient learning when the source of the updates consists of gradually changing terms across large areas (sparse in the frequency domain) in addition to a few rapid updates concentrated in a small set of "interfacial" regions (sparse in the value domain). Random projection is then applied to compress the sparse signals for learning. To expand the model applicability, Taylor series expansion is use
    
[^61]: 高效的量子循环强化学习：基于量子储水池计算

    Efficient quantum recurrent reinforcement learning via quantum reservoir computing. (arXiv:2309.07339v1 [quant-ph])

    [http://arxiv.org/abs/2309.07339](http://arxiv.org/abs/2309.07339)

    本研究提出了一种高效的量子循环强化学习方法，通过构建利用基于量子循环神经网络的储水池的QRL代理，解决了QRL与QRNN的低效训练问题。通过数值模拟验证了这种方法的有效性，并在标准基准测试中展示了其潜力。

    

    量子强化学习（QRL）已经成为解决顺序决策任务的框架，并展示了量子优势的实证结果。一个值得注意的发展是通过量子循环神经网络（QRNN）来处理部分可观察环境等内存密集任务。然而，QRL模型结合QRNN面临着挑战，包括QRL与QRNN的低效训练，因为QRNN中的梯度计算既耗费计算资源又耗时。本研究通过构建利用基于QRNN的储水池的QRL代理来解决这一挑战，具体采用量子长短时记忆（QLSTM）。QLSTM参数是随机初始化并固定不变的。该模型使用异步优势演员-评论家（A3C）算法进行训练。通过数值模拟，我们验证了QLSTM-Reservoir RL框架的有效性。其性能在标准基准测试上得到了评估，证明了其潜力。

    Quantum reinforcement learning (QRL) has emerged as a framework to solve sequential decision-making tasks, showcasing empirical quantum advantages. A notable development is through quantum recurrent neural networks (QRNNs) for memory-intensive tasks such as partially observable environments. However, QRL models incorporating QRNN encounter challenges such as inefficient training of QRL with QRNN, given that the computation of gradients in QRNN is both computationally expensive and time-consuming. This work presents a novel approach to address this challenge by constructing QRL agents utilizing QRNN-based reservoirs, specifically employing quantum long short-term memory (QLSTM). QLSTM parameters are randomly initialized and fixed without training. The model is trained using the asynchronous advantage actor-aritic (A3C) algorithm. Through numerical simulations, we validate the efficacy of our QLSTM-Reservoir RL framework. Its performance is assessed on standard benchmarks, demonstrating 
    
[^62]: 多模态生物医学数据挖掘中的基于可靠性的噪声训练标签清洗方法与归纳性符合预测

    Reliability-based cleaning of noisy training labels with inductive conformal prediction in multi-modal biomedical data mining. (arXiv:2309.07332v1 [cs.LG])

    [http://arxiv.org/abs/2309.07332](http://arxiv.org/abs/2309.07332)

    创新点：提出了一种基于可靠性的训练数据清洗方法，利用归纳性符合预测来纠正噪声训练数据中的错误标记和异常值。该方法在多个分类任务中验证有效性，并显著提升了分类性能。

    

    准确标记生物医学数据是一个挑战。传统的半监督学习方法通常没有充分利用可用的未标记数据。为了解决这个问题，我们提出了一种新的基于可靠性的训练数据清洗方法，采用了归纳性符合预测（ICP）。该方法利用一小部分准确标记的训练数据，并利用ICP计算的可靠性指标来纠正海量噪声训练数据中的错误标记和异常值。该方法的有效性在三个分类任务中得到了验证：使用标题和摘要对药物诱导的肝损伤（DILI）文献进行过滤，通过CT影像学和电子健康记录预测COVID-19患者的重症监护病房（ICU）入院情况，以及使用RNA测序数据对乳腺癌进行亚型分型。通过标签排列引入了不同程度的训练标签噪声。结果显示分类性能显著提升：准确度提高

    Accurately labeling biomedical data presents a challenge. Traditional semi-supervised learning methods often under-utilize available unlabeled data. To address this, we propose a novel reliability-based training data cleaning method employing inductive conformal prediction (ICP). This method capitalizes on a small set of accurately labeled training data and leverages ICP-calculated reliability metrics to rectify mislabeled data and outliers within vast quantities of noisy training data. The efficacy of the method is validated across three classification tasks within distinct modalities: filtering drug-induced-liver-injury (DILI) literature with title and abstract, predicting ICU admission of COVID-19 patients through CT radiomics and electronic health records, and subtyping breast cancer using RNA-sequencing data. Varying levels of noise to the training labels were introduced through label permutation. Results show significant enhancements in classification performance: accuracy enhanc
    
[^63]: 旅行词：一种变压器的几何解释。

    Traveling Words: A Geometric Interpretation of Transformers. (arXiv:2309.07315v1 [cs.CL])

    [http://arxiv.org/abs/2309.07315](http://arxiv.org/abs/2309.07315)

    本文提出了一种几何视角来解释变压器的内部机制，主要贡献在于阐明了层归一化如何限制潜在特征并在超球面上塑造注意力机制，通过探测预训练的GPT-2模型验证了该视角的有效性，并提供了对变压器的直观理解。

    

    变压器在自然语言处理领域取得了显著的进展，但理解其内部机制仍然是一个挑战。本文介绍了一种新颖的几何视角，阐明了变压器操作的内部机制。我们的主要贡献是说明了层归一化如何将潜在特征限制在一个超球面上，从而使注意力能够在该表面上塑造单词的语义表示。这种几何视点无缝地连接了迭代改进和上下文嵌入等已知属性。我们通过探测一个预训练的124M参数的GPT-2模型验证了我们的见解。我们的发现揭示了早期层中清晰的查询-键注意力模式，并在更深的层次上建立在先前关于注意头的专门性的观察基础上。利用这些几何见解，我们提出了对变压器的直观理解，将其描绘为塑造轨迹的过程。

    Transformers have significantly advanced the field of natural language processing, but comprehending their internal mechanisms remains a challenge. In this paper, we introduce a novel geometric perspective that elucidates the inner mechanisms of transformer operations. Our primary contribution is illustrating how layer normalization confines the latent features to a hyper-sphere, subsequently enabling attention to mold the semantic representation of words on this surface. This geometric viewpoint seamlessly connects established properties such as iterative refinement and contextual embeddings. We validate our insights by probing a pre-trained 124M parameter GPT-2 model. Our findings reveal clear query-key attention patterns in early layers and build upon prior observations regarding the subject-specific nature of attention heads at deeper layers. Harnessing these geometric insights, we present an intuitive understanding of transformers, depicting them as processes that model the trajec
    
[^64]: 基于错误增强的肌电信号手势分类的用户训练

    User Training with Error Augmentation for Electromyogram-based Gesture Classification. (arXiv:2309.07289v1 [cs.HC])

    [http://arxiv.org/abs/2309.07289](http://arxiv.org/abs/2309.07289)

    该论文研究了一种基于错误增强的肌电信号手势分类的用户训练系统，实验结果表明，相对于基线，使用修改反馈的条件下能够显著提高手势分类准确性和类别区分度。

    

    我们设计并测试了一个实时控制用户界面的系统，通过从腕带配置的八个电极中提取表面肌电活动（sEMG）。sEMG数据被实时流入一个机器学习算法，用于手势的实时分类。在初始模型校准后，参与者在人类学习阶段中被提供了三种类型的反馈：真实反馈，在其中预测的手势分类算法的概率被显示而不进行任何改动；修改反馈，在其中我们对这些概率进行了错误的隐藏增强处理；和无反馈。然后通过一系列的迷你游戏评估了用户的表现，要求被试使用八个手势来操作游戏角色完成任务。实验结果表明，与基线相比，修改反馈条件下的准确性和手势类别区分度显著提高。

    We designed and tested a system for real-time control of a user interface by extracting surface electromyographic (sEMG) activity from eight electrodes in a wrist-band configuration. sEMG data were streamed into a machine-learning algorithm that classified hand gestures in real-time. After an initial model calibration, participants were presented with one of three types of feedback during a human-learning stage: veridical feedback, in which predicted probabilities from the gesture classification algorithm were displayed without alteration, modified feedback, in which we applied a hidden augmentation of error to these probabilities, and no feedback. User performance was then evaluated in a series of minigames, in which subjects were required to use eight gestures to manipulate their game avatar to complete a task. Experimental results indicated that, relative to baseline, the modified feedback condition led to significantly improved accuracy and improved gesture class separation. These 
    
[^65]: 无偏的扩散模型生成人脸：我们已经到达了吗？

    Unbiased Face Synthesis With Diffusion Models: Are We There Yet?. (arXiv:2309.07277v1 [cs.CV])

    [http://arxiv.org/abs/2309.07277](http://arxiv.org/abs/2309.07277)

    本文研究了生成模型在人脸生成领域的功效和不足之处。通过定性和定量方法，我们提出了一个框架来审计基于社会属性条件生成的人脸的特征。我们发现人脸图像生成存在几个限制，包括对文本提示的忠实度、人口统计差异和分布偏移。

    

    文本到图像扩散模型因其前所未有的图像生成能力而广受欢迎。特别是，它们生成和修改人脸的能力已经推动了对使用生成的人脸图像进行训练数据增强和模型性能评估的研究。本文研究了生成模型在人脸生成领域的功效和不足之处。通过使用基于嵌入度量和用户研究等定性和定量方法，我们提出了一个框架来审计基于一组社会属性条件生成的人脸的特征。我们在最先进的文本到图像扩散模型生成的人脸上应用了我们的框架。我们发现人脸图像生成存在几个限制，包括对文本提示的忠实度、人口统计差异和分布偏移。此外，我们提出了一个分析模型，以深入了解训练数据的影响方式。

    Text-to-image diffusion models have achieved widespread popularity due to their unprecedented image generation capability. In particular, their ability to synthesize and modify human faces has spurred research into using generated face images in both training data augmentation and model performance assessments. In this paper, we study the efficacy and shortcomings of generative models in the context of face generation. Utilizing a combination of qualitative and quantitative measures, including embedding-based metrics and user studies, we present a framework to audit the characteristics of generated faces conditioned on a set of social attributes. We applied our framework on faces generated through state-of-the-art text-to-image diffusion models. We identify several limitations of face image generation that include faithfulness to the text prompt, demographic disparities, and distributional shifts. Furthermore, we present an analytical model that provides insights into how training data
    
[^66]: 安全且加速的基于深度强化学习的O-RAN切片: 一种混合迁移学习方法

    Safe and Accelerated Deep Reinforcement Learning-based O-RAN Slicing: A Hybrid Transfer Learning Approach. (arXiv:2309.07265v1 [cs.NI])

    [http://arxiv.org/abs/2309.07265](http://arxiv.org/abs/2309.07265)

    本文提出了一种混合迁移学习方法，用于解决在O-RAN网络中使用DRL算法进行闭环控制时遇到的收敛速度慢和性能不稳定的问题。

    

    开放无线接入网络（O-RAN）架构支持智能网络控制算法作为其核心能力之一。数据驱动应用程序利用这些算法通过无线接入网络智能控制器（RIC）来优化无线接入网络（RAN）功能。在O-RAN文献中，深度强化学习（DRL）算法是解决动态无线资源管理问题的主要方法之一。然而，尽管O-RAN RIC引入了诸多好处，但在真实网络部署中，DRL算法的实际采用却落后。这主要是因为DRL代理在部署和面对之前未见过的网络条件时收敛速度慢、性能不稳定。在本文中，我们通过将迁移学习（TL）作为O-RAN功能的DRL基于闭环控制的训练和部署流程的核心组成部分来解决这些挑战。为此，我们提出并设计了一个混合TL辅助的方法

    The open radio access network (O-RAN) architecture supports intelligent network control algorithms as one of its core capabilities. Data-driven applications incorporate such algorithms to optimize radio access network (RAN) functions via RAN intelligent controllers (RICs). Deep reinforcement learning (DRL) algorithms are among the main approaches adopted in the O-RAN literature to solve dynamic radio resource management problems. However, despite the benefits introduced by the O-RAN RICs, the practical adoption of DRL algorithms in real network deployments falls behind. This is primarily due to the slow convergence and unstable performance exhibited by DRL agents upon deployment and when facing previously unseen network conditions. In this paper, we address these challenges by proposing transfer learning (TL) as a core component of the training and deployment workflows for the DRL-based closed-loop control of O-RAN functionalities. To this end, we propose and design a hybrid TL-aided a
    
[^67]: 具有未测混淆因素的广义线性模型的同时推断

    Simultaneous inference for generalized linear models with unmeasured confounders. (arXiv:2309.07261v1 [stat.ME])

    [http://arxiv.org/abs/2309.07261](http://arxiv.org/abs/2309.07261)

    本文研究了存在混淆效应时的广义线性模型的大规模假设检验问题，并提出了一种利用正交结构和线性投影的统计估计和推断框架，解决了由于未测混淆因素引起的偏差问题。

    

    在基因组研究中，常常进行成千上万个同时假设检验，以确定差异表达的基因。然而，由于存在未测混淆因素，许多标准统计方法可能存在严重的偏差。本文研究了存在混淆效应时的多元广义线性模型的大规模假设检验问题。在任意混淆机制下，我们提出了一个统一的统计估计和推断方法，利用正交结构并将线性投影整合到三个关键阶段中。首先，利用多元响应变量分离边际和不相关的混淆效应，恢复混淆系数的列空间。随后，利用$\ell_1$正则化进行稀疏性估计，并强加正交性限制于混淆系数，联合估计潜在因子和主要效应。最后，我们结合投影和加权偏差校正步骤。

    Tens of thousands of simultaneous hypothesis tests are routinely performed in genomic studies to identify differentially expressed genes. However, due to unmeasured confounders, many standard statistical approaches may be substantially biased. This paper investigates the large-scale hypothesis testing problem for multivariate generalized linear models in the presence of confounding effects. Under arbitrary confounding mechanisms, we propose a unified statistical estimation and inference framework that harnesses orthogonal structures and integrates linear projections into three key stages. It first leverages multivariate responses to separate marginal and uncorrelated confounding effects, recovering the confounding coefficients' column space. Subsequently, latent factors and primary effects are jointly estimated, utilizing $\ell_1$-regularization for sparsity while imposing orthogonality onto confounding coefficients. Finally, we incorporate projected and weighted bias-correction steps 
    
[^68]: 使用机器学习解决递归关系，以及在成本分析中的应用

    Solving Recurrence Relations using Machine Learning, with Application to Cost Analysis. (arXiv:2309.07259v1 [cs.PL])

    [http://arxiv.org/abs/2309.07259](http://arxiv.org/abs/2309.07259)

    该论文提出了一种使用机器学习解决递归关系的方法，旨在解决当前递归求解工具的瓶颈问题，并应用于成本分析中。方法基于机器学习稀疏回归技术猜测闭式函数，并结合SMT-so来求解任意约束递归关系。

    

    自动静态成本分析推断关于程序资源使用情况的信息，而不需要使用具体数据，并将这些信息作为输入数据大小的函数呈现。逻辑程序（和其他语言）的大多数分析工具基于建立表示谓词计算成本（上界）的递归关系，并解决它们以找到等价于（或上界）的闭式函数。当前工具中的递归求解是一个瓶颈：在分析过程中出现的许多递归式无法使用当前求解器（如计算机代数系统）进行求解，因此需要针对不同类别的递归式开发特定方法。我们通过开发一种新颖的通用方法来解决任意约束递归关系的挑战，该方法利用机器学习稀疏回归技术猜测候选的闭式函数，并结合SMT-so来求解。

    Automatic static cost analysis infers information about the resources used by programs without actually running them with concrete data, and presents such information as functions of input data sizes. Most of the analysis tools for logic programs (and other languages) are based on setting up recurrence relations representing (bounds on) the computational cost of predicates, and solving them to find closed-form functions that are equivalent to (or a bound on) them. Such recurrence solving is a bottleneck in current tools: many of the recurrences that arise during the analysis cannot be solved with current solvers, such as Computer Algebra Systems (CASs), so that specific methods for different classes of recurrences need to be developed. We address such a challenge by developing a novel, general approach for solving arbitrary, constrained recurrence relations, that uses machine-learning sparse regression techniques to guess a candidate closed-form function, and a combination of an SMT-so
    
[^69]: 你所需要的只是旋转：基于自旋网络的SU(2)等价变分量子电路

    All you need is spin: SU(2) equivariant variational quantum circuits based on spin networks. (arXiv:2309.07250v1 [quant-ph])

    [http://arxiv.org/abs/2309.07250](http://arxiv.org/abs/2309.07250)

    本文提出使用自旋网络构建SU(2)等价量子电路，通过编码群结构来限制优化空间， 具有旋转对称性，比其他已知的构造更直接实现在量子硬件上。

    

    变分算法要求将优化空间自然地限制在一个有效的范围内。在几何量子机器学习中，将群结构编码到参数化的量子电路中，将问题的对称性作为归纳偏置来考虑，可以实现这一点。然而，构建这样的电路是有挑战性的，因为尚未出现明确的指导原则。在本文中，我们提出使用自旋网络，一种在群变换下保持不变的有向张量网络形式，来设计SU(2)等价量子电路ansatz - 具有旋转对称性的电路。通过改变使SU(2)群作用块对角化的基础，这些网络为构建参数化等价量子电路提供了一个自然的构建模块。我们证明了我们的构造在数学上等效于其他已知的构造，例如基于交错和广义排列的构造，但在量子硬件上实现更直接。

    Variational algorithms require architectures that naturally constrain the optimisation space to run efficiently. In geometric quantum machine learning, one achieves this by encoding group structure into parameterised quantum circuits to include the symmetries of a problem as an inductive bias. However, constructing such circuits is challenging as a concrete guiding principle has yet to emerge. In this paper, we propose the use of spin networks, a form of directed tensor network invariant under a group transformation, to devise SU(2) equivariant quantum circuit ans\"atze -- circuits possessing spin rotation symmetry. By changing to the basis that block diagonalises SU(2) group action, these networks provide a natural building block for constructing parameterised equivariant quantum circuits. We prove that our construction is mathematically equivalent to other known constructions, such as those based on twirling and generalised permutations, but more direct to implement on quantum hardwa
    
[^70]: 使用贝叶斯优化自动调优基于Apache TVM的科学应用

    Autotuning Apache TVM-based Scientific Applications Using Bayesian Optimization. (arXiv:2309.07235v1 [cs.LG])

    [http://arxiv.org/abs/2309.07235](http://arxiv.org/abs/2309.07235)

    本文提出了一种基于贝叶斯优化的TVM自动调优框架，使用TVM张量表达语言实现了LU分解、Cholesky分解和3mm等线性代数核。在GPU集群上的实验结果表明，该框架在大多数情况下优于传统的AutoTVM框架。

    

    Apache TVM是一个开源的机器学习编译器框架，旨在优化各种硬件平台上的计算。本文提出了一种基于贝叶斯优化的新型TVM自动调优框架，并使用TVM张量表达语言实现了LU分解、Cholesky分解和3mm等线性代数核。我们使用这些科学计算核来评估我们的方法在阿贡国家实验室的GPU集群"Swing"上的有效性。我们将提出的自动调优框架与TVM自动调优框架AutoTVM的四个调优器进行比较，并发现我们的框架在大多数情况下表现优于AutoTVM。

    Apache TVM (Tensor Virtual Machine), an open source machine learning compiler framework designed to optimize computations across various hardware platforms, provides an opportunity to improve the performance of dense matrix factorizations such as LU (Lower Upper) decomposition and Cholesky decomposition on GPUs and AI (Artificial Intelligence) accelerators. In this paper, we propose a new TVM autotuning framework using Bayesian Optimization and use the TVM tensor expression language to implement linear algebra kernels such as LU, Cholesky, and 3mm. We use these scientific computation kernels to evaluate the effectiveness of our methods on a GPU cluster, called Swing, at Argonne National Laboratory. We compare the proposed autotuning framework with the TVM autotuning framework AutoTVM with four tuners and find that our framework outperforms AutoTVM in most cases.
    
[^71]: EarthPT：地球观测的基础模型

    EarthPT: a foundation model for Earth Observation. (arXiv:2309.07207v1 [cs.LG])

    [http://arxiv.org/abs/2309.07207](http://arxiv.org/abs/2309.07207)

    EarthPT是一个地球观测的预训练transformer模型，能够准确预测未来地表反射值，并提供有意义的嵌入信息用于下游任务。

    

    我们介绍了EarthPT - 一种地球观测(EO)预训练的transformer模型。EarthPT是一个7亿参数的解码transformer基础模型，以自监督的方式进行训练，并专门针对EO应用进行开发。我们证明EarthPT是一个有效的预测模型，可以准确地预测未来400-2300 nm范围内的像素级地表反射值。例如，在一个为期五个月的测试数据集上，地表植被指数（NDVI）的演变预测的典型误差约为0.05（在-1 -> 1的自然范围内），性能超过基于历史平均的简单相位折叠模型。我们还证明了EarthPT学到的嵌入具有语义上有意义的信息，并且可以用于下游任务，如高精度、动态的土地利用分类。令人兴奋的是，我们注意到EO数据的丰富性提供了理论上的...

    We introduce EarthPT -- an Earth Observation (EO) pretrained transformer. EarthPT is a 700 million parameter decoding transformer foundation model trained in an autoregressive self-supervised manner and developed specifically with EO use-cases in mind. We demonstrate that EarthPT is an effective forecaster that can accurately predict future pixel-level surface reflectances across the 400-2300 nm range well into the future. For example, forecasts of the evolution of the Normalised Difference Vegetation Index (NDVI) have a typical error of approximately 0.05 (over a natural range of -1 -> 1) at the pixel level over a five month test set horizon, out-performing simple phase-folded models based on historical averaging. We also demonstrate that embeddings learnt by EarthPT hold semantically meaningful information and could be exploited for downstream tasks such as highly granular, dynamic land use classification. Excitingly, we note that the abundance of EO data provides us with -- in theor
    
[^72]: 通过时间滞后信息瓶颈的潜在表示和马尔可夫过程模拟

    Latent Representation and Simulation of Markov Processes via Time-Lagged Information Bottleneck. (arXiv:2309.07200v1 [cs.LG])

    [http://arxiv.org/abs/2309.07200](http://arxiv.org/abs/2309.07200)

    本文介绍了一种通过时间滞后信息瓶颈的方法，将复杂系统映射到简化表示空间并模拟时间上的大跳跃。实验证明该方法能够准确模拟原始过程的统计特性和动力学，优于现有的时间滞后降维方法。

    

    马尔可夫过程是描述各个领域中动态系统的广泛使用的数学模型。然而，由于需要准确积分的短时间步长，精确模拟长时间尺度上的大规模系统计算量很大。在本文中，我们引入了一种将复杂系统映射到简化表示空间并模拟时间上的大跳跃的推理过程。为了实现这一点，我们提出了基于信息理论的原则目标-时间滞后信息瓶颈（T-IB），它旨在捕捉相关的时间特征，同时丢弃高频信息以简化模拟任务并最小化推理误差。我们的实验证明，T-IB学习了信息最优的表示，能够准确地模拟原始过程在选择的时间滞后下的统计特性和动力学，并且优于现有的时间滞后降维方法。

    Markov processes are widely used mathematical models for describing dynamic systems in various fields. However, accurately simulating large-scale systems at long time scales is computationally expensive due to the short time steps required for accurate integration. In this paper, we introduce an inference process that maps complex systems into a simplified representational space and models large jumps in time. To achieve this, we propose Time-lagged Information Bottleneck (T-IB), a principled objective rooted in information theory, which aims to capture relevant temporal features while discarding high-frequency information to simplify the simulation task and minimize the inference error. Our experiments demonstrate that T-IB learns information-optimal representations for accurately modeling the statistical properties and dynamics of the original process at a selected time lag, outperforming existing time-lagged dimensionality reduction methods.
    
[^73]: 在受信任的执行环境中减轻联邦学习中的对抗性攻击

    Mitigating Adversarial Attacks in Federated Learning with Trusted Execution Environments. (arXiv:2309.07197v1 [cs.LG])

    [http://arxiv.org/abs/2309.07197](http://arxiv.org/abs/2309.07197)

    在联邦学习中，通过使用受信任的执行环境，可以减轻被入侵节点利用对抗性攻击来探测模型的风险。这可以避免危险的现实世界场景，如被篡改的交通标志导致自动驾驶车辆错误识别，或者被污染的本地数据集。

    

    联邦学习的主要前提是本地计算机学习模型更新，以保护用户数据隐私。这种方法设计上避免了用户数据离开其设备的情况。一旦更新汇总，模型将广播到联邦中的所有节点。然而，如果没有适当的防御措施，被入侵的节点可以在其本地内存中探测模型，寻找对抗性示例，这可能导致危险的现实世界场景。例如，在基于图像的应用程序中，对抗性示例包含微微扰动的图像，人眼无法察觉，但被本地模型错误分类。然后，这些对抗性图像稍后会被呈现给受害节点的对应模型，以重放攻击。典型的示例利用了传播策略，如被更改的交通标志（修补攻击），这些标志不再被自动驾驶车辆识别，或者看似未被更改的样本，这些样本会破坏联邦学习方案的鲁棒性，污染本地数据集。

    The main premise of federated learning (FL) is that machine learning model updates are computed locally to preserve user data privacy. This approach avoids by design user data to ever leave the perimeter of their device. Once the updates aggregated, the model is broadcast to all nodes in the federation. However, without proper defenses, compromised nodes can probe the model inside their local memory in search for adversarial examples, which can lead to dangerous real-world scenarios. For instance, in image-based applications, adversarial examples consist of images slightly perturbed to the human eye getting misclassified by the local model. These adversarial images are then later presented to a victim node's counterpart model to replay the attack. Typical examples harness dissemination strategies such as altered traffic signs (patch attacks) no longer recognized by autonomous vehicles or seemingly unaltered samples that poison the local dataset of the FL scheme to undermine its robustn
    
[^74]: 通过结合神经网络和积分形式的方法实现鲁棒的SINDy方法

    A Robust SINDy Approach by Combining Neural Networks and an Integral Form. (arXiv:2309.07193v1 [math.DS])

    [http://arxiv.org/abs/2309.07193](http://arxiv.org/abs/2309.07193)

    本文提出了一种通过结合神经网络和积分形式的方法实现鲁棒的SINDy方法，能够从嘈杂和稀缺的数据中发现非线性控制方程。

    

    从数据中发现控制方程是一个长期以来的研究领域。一种被广泛使用的用于这个目的的方法是稀疏回归非线性动力学，也称为SINDy。尽管进行了几次尝试，但嘈杂和稀缺的数据仍然对SINDy方法的成功构成了严重的挑战。本文中，我们讨论了一种从嘈杂和稀缺数据中发现非线性控制方程的鲁棒方法。为了做到这一点，我们利用神经网络根据测量数据学习一个隐式表示，不仅可以在测量区域内产生输出，而且输出的时间演变可以被描述为一个动力系统。此外，我们在SINDy框架的精神中学习这样一个动力系统。利用神经网络的隐式表示，我们使用自动微分工具获得了SINDy所需的导数信息。为了增强我们方法的鲁棒性，我们进一步纳入了

    The discovery of governing equations from data has been an active field of research for decades. One widely used methodology for this purpose is sparse regression for nonlinear dynamics, known as SINDy. Despite several attempts, noisy and scarce data still pose a severe challenge to the success of the SINDy approach. In this work, we discuss a robust method to discover nonlinear governing equations from noisy and scarce data. To do this, we make use of neural networks to learn an implicit representation based on measurement data so that not only it produces the output in the vicinity of the measurements but also the time-evolution of output can be described by a dynamical system. Additionally, we learn such a dynamic system in the spirit of the SINDy framework. Leveraging the implicit representation using neural networks, we obtain the derivative information -- required for SINDy -- using an automatic differentiation tool. To enhance the robustness of our methodology, we further incorp
    
[^75]: 数据增强和3D-CNN深度对阿尔茨海默病检测的影响

    The effect of data augmentation and 3D-CNN depth on Alzheimer's Disease detection. (arXiv:2309.07192v1 [eess.IV])

    [http://arxiv.org/abs/2309.07192](http://arxiv.org/abs/2309.07192)

    本研究通过严格遵守数据处理、实验设计和模型评估的最佳实践来确保可重复和可靠的机器学习。针对阿尔茨海默病检测问题，我们研究了不同的数据增强技术和模型复杂度对整体性能的影响，并采用了交叉验证和多次训练试验来补偿数据稀缺性和初始随机参数。

    

    机器学习已经成为医疗领域中一种有前景的方法，胜过传统统计技术。然而，为了将机器学习确立为临床实践中可靠的工具，遵守最佳实践，包括数据处理、实验设计和模型评估非常重要。本研究总结并严格遵守这些实践，以确保可重复和可靠的机器学习。具体来说，我们关注阿尔茨海默病检测，这作为医疗领域中一个具有挑战性的问题的典型示例。我们研究了不同的数据增强技术和模型复杂度对整体性能的影响。我们使用ADNI数据集中的MRI数据解决了一个分类问题，采用了三维卷积神经网络（CNN）。实验采用交叉验证和多次训练试验来补偿数据稀缺性和初始随机参数。在这个框架下，我们训练了15个预测模型，考虑了三种不同的模型复杂度和数据增强技术。

    Machine Learning (ML) has emerged as a promising approach in healthcare, outperforming traditional statistical techniques. However, to establish ML as a reliable tool in clinical practice, adherence to best practices regarding data handling, experimental design, and model evaluation is crucial. This work summarizes and strictly observes such practices to ensure reproducible and reliable ML. Specifically, we focus on Alzheimer's Disease (AD) detection, which serves as a paradigmatic example of challenging problem in healthcare. We investigate the impact of different data augmentation techniques and model complexity on the overall performance. We consider MRI data from ADNI dataset to address a classification problem employing 3D Convolutional Neural Network (CNN). The experiments are designed to compensate for data scarcity and initial random parameters by utilizing cross-validation and multiple training trials. Within this framework, we train 15 predictive models, considering three dif
    
[^76]: 学习漂移：通过漂移正则化在非独立同分布数据上进行联邦学习

    Learning From Drift: Federated Learning on Non-IID Data via Drift Regularization. (arXiv:2309.07189v1 [cs.LG])

    [http://arxiv.org/abs/2309.07189](http://arxiv.org/abs/2309.07189)

    通过漂移正则化，我们提出了学习漂移（LfD）方法，它能在非独立同分布数据上有效地训练模型，防止性能下降。

    

    联邦学习算法在独立同分布（IID）数据上表现良好，但在非独立同分布（Non-IID）数据上却存在较大困难。尽管已经有许多研究项目来解决这个问题，但最近的研究发现它们在与IID数据的训练相比仍然不够优化。在本研究中，我们仔细分析了在非IID环境中存在的方法。有趣的是，我们发现正则化分类器的输出在防止非IID数据性能下降方面非常有效。基于这一发现，我们提出了学习漂移（LfD）的新方法，用于在异构环境中有效训练模型。我们的方案包括两个关键组成部分：漂移估计和漂移正则化。具体而言，LfD首先估计本地模型与全局模型之间的差异（即漂移），然后对本地模型进行正则化以避免过度漂移。

    Federated learning algorithms perform reasonably well on independent and identically distributed (IID) data. They, on the other hand, suffer greatly from heterogeneous environments, i.e., Non-IID data. Despite the fact that many research projects have been done to address this issue, recent findings indicate that they are still sub-optimal when compared to training on IID data. In this work, we carefully analyze the existing methods in heterogeneous environments. Interestingly, we find that regularizing the classifier's outputs is quite effective in preventing performance degradation on Non-IID data. Motivated by this, we propose Learning from Drift (LfD), a novel method for effectively training the model in heterogeneous settings. Our scheme encapsulates two key components: drift estimation and drift regularization. Specifically, LfD first estimates how different the local model is from the global model (i.e., drift). The local model is then regularized such that it does not fall in t
    
[^77]: 在存在截尾数据的情况下预测滚珠轴承的寿命

    Predicting Survival Time of Ball Bearings in the Presence of Censoring. (arXiv:2309.07188v1 [eess.SP])

    [http://arxiv.org/abs/2309.07188](http://arxiv.org/abs/2309.07188)

    本文提出了一种新的方法，使用生存分析来预测滚珠轴承的失效时间，并且通过分析频域数据和训练多个生存模型来实现。模型可以以时间为基础进行风险的概率预测，并允许比较不同组轴承的生存函数。

    

    滚珠轴承在各种制造和机械领域中得到广泛应用，基于机器学习的方法已经被广泛采用来监测磨损并在故障发生之前发现缺陷。然而，很少有研究涉及到截尾数据的问题，即未观察到故障的情况。本文提出了一种新的方法，使用生存分析来预测滚珠轴承的失效时间。首先，我们在频域中分析轴承数据，并通过比较库尔巴赫-莱布勒散度和其破坏频率区间与其破坏频率区间之间的标准差来标注轴承故障时刻。其次，我们训练了多个生存模型，根据标注数据和从时域提取的协变量（如偏度、峰度和熵）来估计失效时间。这些模型可以以时间为基础进行风险的概率预测，并允许我们比较不同组的轴承的生存函数。我们进行了演示。

    Ball bearings find widespread use in various manufacturing and mechanical domains, and methods based on machine learning have been widely adopted in the field to monitor wear and spot defects before they lead to failures. Few studies, however, have addressed the problem of censored data, in which failure is not observed. In this paper, we propose a novel approach to predict the time to failure in ball bearings using survival analysis. First, we analyze bearing data in the frequency domain and annotate when a bearing fails by comparing the Kullback-Leibler divergence and the standard deviation between its break-in frequency bins and its break-out frequency bins. Second, we train several survival models to estimate the time to failure based on the annotated data and covariates extracted from the time domain, such as skewness, kurtosis and entropy. The models give a probabilistic prediction of risk over time and allow us to compare the survival function between groups of bearings. We demo
    
[^78]: 基于自适应图时空卷积网络和序列分解的多步叶绿素浓度预测

    Multi-step prediction of chlorophyll concentration based on Adaptive Graph-Temporal Convolutional Network with Series Decomposition. (arXiv:2309.07187v1 [cs.LG])

    [http://arxiv.org/abs/2309.07187](http://arxiv.org/abs/2309.07187)

    本文提出了一种基于自适应图时空卷积网络和序列分解的多步叶绿素浓度预测模型，可以有效挖掘数据中的非线性特征，对环境保护和水产养殖具有重要意义。

    

    叶绿素浓度能够很好地反映水体的营养状况和藻类水华，是评估水质的重要指标。叶绿素浓度变化趋势的预测对环境保护和水产养殖具有重要意义。然而，许多影响叶绿素浓度的因素之间存在复杂和不可辨识的非线性关系。为了有效挖掘数据中包含的非线性特征，本文提出了一种时间序列分解自适应图时空卷积网络（AGTCNSD）预测模型。首先，使用移动平均法将原始序列分解为趋势分量和周期分量。其次，基于图卷积神经网络建模水质参数数据，并定义参数嵌入矩阵。利用矩阵分解的思想为每个节点分配权重参数。自适应图卷积学习了最佳的节点权重参数和邻居节点的信息。

    Chlorophyll concentration can well reflect the nutritional status and algal blooms of water bodies, and is an important indicator for evaluating water quality. The prediction of chlorophyll concentration change trend is of great significance to environmental protection and aquaculture. However, there is a complex and indistinguishable nonlinear relationship between many factors affecting chlorophyll concentration. In order to effectively mine the nonlinear features contained in the data. This paper proposes a time-series decomposition adaptive graph-time convolutional network ( AGTCNSD ) prediction model. Firstly, the original sequence is decomposed into trend component and periodic component by moving average method. Secondly, based on the graph convolutional neural network, the water quality parameter data is modeled, and a parameter embedding matrix is defined. The idea of matrix decomposition is used to assign weight parameters to each node. The adaptive graph convolution learns th
    
[^79]: 基于高级信号处理和机器学习的声音相关呼吸疾病分类用于辅助诊断支持

    Audio-Based Classification of Respiratory Diseases using Advanced Signal Processing and Machine Learning for Assistive Diagnosis Support. (arXiv:2309.07183v1 [eess.SP])

    [http://arxiv.org/abs/2309.07183](http://arxiv.org/abs/2309.07183)

    本研究利用声音数据训练多个机器学习模型来对呼吸疾病进行分类。方法采用经验模态分解和频谱分析来提取与心血管和呼吸模式相关的生理信号，并通过特征提取和预测建模实现快速筛查和诊断支持。

    

    在全球医疗保健中，呼吸系统疾病是导致死亡的主要原因，这突显了迅速准确的诊断的需求。为了推进听诊的快速筛查技术，我们的研究侧重于利用其中一个最大的公开的呼吸音医学数据库来训练多种机器学习模型，能够对不同的健康状况进行分类。我们的方法结合了经验模态分解（EMD）和频谱分析，从声学数据中提取与心血管和呼吸模式密切相关的生理相关生物信号，使我们的方法与传统声学特征提取方法有所不同。我们使用功率谱密度分析和滤波技术来选择与潜在生理现象强相关的固有模态函数（IMFs）。这些生物信号经过全面的特征提取过程用于预测建模。首先，我们部署一个二分类模型

    In global healthcare, respiratory diseases are a leading cause of mortality, underscoring the need for rapid and accurate diagnostics. To advance rapid screening techniques via auscultation, our research focuses on employing one of the largest publicly available medical database of respiratory sounds to train multiple machine learning models able to classify different health conditions. Our method combines Empirical Mode Decomposition (EMD) and spectral analysis to extract physiologically relevant biosignals from acoustic data, closely tied to cardiovascular and respiratory patterns, making our approach apart in its departure from conventional audio feature extraction practices. We use Power Spectral Density analysis and filtering techniques to select Intrinsic Mode Functions (IMFs) strongly correlated with underlying physiological phenomena. These biosignals undergo a comprehensive feature extraction process for predictive modeling. Initially, we deploy a binary classification model t
    
[^80]: 使用预训练深度学习模型的睡眠阶段分类

    Sleep Stage Classification Using a Pre-trained Deep Learning Model. (arXiv:2309.07182v1 [eess.SP])

    [http://arxiv.org/abs/2309.07182](http://arxiv.org/abs/2309.07182)

    本研究提出了一种名为"EEGMobile"的机器学习模型，在睡眠阶段分类中取得了优于其他模型的准确率，特别在N1阶段表现更佳。

    

    睡眠障碍是常见的人类疾病之一。睡眠阶段的分类在诊断睡眠障碍、监测治疗效果和理解睡眠阶段与各种健康状况之间的关系方面起着基础性作用。精确而有效地分类这些阶段可以显着提升我们对与睡眠相关现象的理解，从而最终改善健康结果和疾病治疗效果。其他提出的模型往往耗时且缺乏足够的准确性，特别是在N1阶段。本研究的主要目标是提出一种名为"EEGMobile"的机器学习模型。该模型利用预训练模型，并从脑信号的脑电图谱图学习。在名为"Sleep-EDF20"的公开数据集上，该模型实现了86.97%的准确率，表现优于其他研究者提出的模型。此外，它在N1阶段记录了56.4%的准确率，这是更好的。

    One of the common human diseases is sleep disorders. The classification of sleep stages plays a fundamental role in diagnosing sleep disorders, monitoring treatment effectiveness, and understanding the relationship between sleep stages and various health conditions. A precise and efficient classification of these stages can significantly enhance our understanding of sleep-related phenomena and ultimately lead to improved health outcomes and disease treatment.  Models others propose are often time-consuming and lack sufficient accuracy, especially in stage N1. The main objective of this research is to present a machine-learning model called "EEGMobile". This model utilizes pre-trained models and learns from electroencephalogram (EEG) spectrograms of brain signals. The model achieved an accuracy of 86.97% on a publicly available dataset named "Sleep-EDF20", outperforming other models proposed by different researchers. Moreover, it recorded an accuracy of 56.4% in stage N1, which is bette
    
[^81]: 大幻觉：软件可移植性的神话及其对机器学习进展的影响

    The Grand Illusion: The Myth of Software Portability and Implications for ML Progress. (arXiv:2309.07181v1 [cs.SE])

    [http://arxiv.org/abs/2309.07181](http://arxiv.org/abs/2309.07181)

    这项研究通过大规模研究了流行的机器学习框架在不同硬件上的可移植性，发现它们在转移到其他硬件上时可能会丢失超过40%的关键功能。这种不可移植性对机器学习创新的速度产生了负面影响。

    

    推动机器学习的边界往往需要探索不同的硬件和软件组合。然而，自由地在不同的工具堆栈之间进行实验可能与效率的追求相冲突，这导致了越来越专门的人工智能硬件的出现，并推动了围绕一小部分机器学习框架的整合。如果软件和硬件同时发展，那么探索性研究可能受到限制，使得远离流行工具堆栈的主流思想更加困难。虽然这种摩擦越来越多地影响到机器学习中的创新速度，但据我们所知，工具的不可移植性尚未得到量化。在这项工作中，我们提出了一个问题：流行的机器学习软件框架有多可移植？我们对主要的机器学习框架在不同硬件类型上的可移植性进行了大规模的研究。我们的研究结果描绘出了一个令人不安的画面--当这些框架转移到其他硬件上时，它们可能会丢失超过40%的关键功能。

    Pushing the boundaries of machine learning often requires exploring different hardware and software combinations. However, the freedom to experiment across different tooling stacks can be at odds with the drive for efficiency, which has produced increasingly specialized AI hardware and incentivized consolidation around a narrow set of ML frameworks. Exploratory research can be restricted if software and hardware are co-evolving, making it even harder to stray away from mainstream ideas that work well with popular tooling stacks. While this friction increasingly impacts the rate of innovation in machine learning, to our knowledge the lack of portability in tooling has not been quantified. In this work, we ask: How portable are popular ML software frameworks? We conduct a large-scale study of the portability of mainstream ML frameworks across different hardware types. Our findings paint an uncomfortable picture -- frameworks can lose more than 40% of their key functions when ported to ot
    
[^82]: CloudBrain-NMR: 一种智能云计算平台，用于NMR光谱处理、重建和分析

    CloudBrain-NMR: An Intelligent Cloud Computing Platform for NMR Spectroscopy Processing, Reconstruction and Analysis. (arXiv:2309.07178v1 [q-bio.QM])

    [http://arxiv.org/abs/2309.07178](http://arxiv.org/abs/2309.07178)

    CloudBrain-NMR是一个智能云计算平台，用于NMR光谱处理和分析，通过在线访问，无需用户端安装任何程序，使用并行计算来加快处理时间。

    

    核磁共振(NMR)光谱学在化学和生物学中作为一种强大的分析工具，已经被广泛应用于研究分子结构和动力学。然而，对从NMR光谱仪获取的原始数据进行处理和后续定量分析涉及各种专门工具，这就需要全面掌握程序和NMR方面的知识。特别是，由于计算设置复杂，新兴的深度学习工具在NMR中的应用并不容易。因此，NMR处理对于化学家和生物学家来说并不是一项容易的任务。在这项工作中，我们提出了CloudBrain-NMR，一种智能在线云计算平台，用于NMR数据的读取，处理，重建和定量分析。该平台通过Web浏览器方便地访问，无需用户端安装任何程序。CloudBrain-NMR使用图形处理单元和中央处理单元进行并行计算，从而大大缩短了处理时间。

    Nuclear Magnetic Resonance (NMR) spectroscopy has served as a powerful analytical tool for studying molecular structure and dynamics in chemistry and biology. However, the processing of raw data acquired from NMR spectrometers and subsequent quantitative analysis involves various specialized tools, which necessitates comprehensive knowledge in programming and NMR. Particularly, the emerging deep learning tools is hard to be widely used in NMR due to the sophisticated setup of computation. Thus, NMR processing is not an easy task for chemist and biologists. In this work, we present CloudBrain-NMR, an intelligent online cloud computing platform designed for NMR data reading, processing, reconstruction, and quantitative analysis. The platform is conveniently accessed through a web browser, eliminating the need for any program installation on the user side. CloudBrain-NMR uses parallel computing with graphics processing units and central processing units, resulting in significantly shorten
    
[^83]: 最优和公平的鼓励政策评估与学习

    Optimal and Fair Encouragement Policy Evaluation and Learning. (arXiv:2309.07176v1 [cs.LG])

    [http://arxiv.org/abs/2309.07176](http://arxiv.org/abs/2309.07176)

    本研究探讨了在关键领域中针对鼓励政策的最优和公平评估以及学习的问题，研究发现在人类不遵循治疗建议的情况下，最优策略规则只是建议。同时，针对治疗的异质性和公平考虑因素，决策者的权衡和决策规则也会发生变化。在社会服务领域，研究显示存在一个使用差距问题，那些最有可能受益的人却无法获得这些益服务。

    

    在关键领域中，强制个体接受治疗通常是不可能的，因此在人类不遵循治疗建议的情况下，最优策略规则只是建议。在这些领域中，接受治疗的个体可能存在异质性，治疗效果也可能存在异质性。虽然最优治疗规则可以最大化整个人群的因果结果，但在鼓励的情况下，对于访问平等限制或其他公平考虑因素可能是相关的。例如，在社会服务领域，一个持久的难题是那些最有可能从中受益的人中那些获益服务的使用差距。当决策者对访问和平均结果都有分配偏好时，最优决策规则会发生变化。我们研究了因果识别、统计方差减少估计和稳健估计的最优治疗规则，包括在违反阳性条件的情况下。

    In consequential domains, it is often impossible to compel individuals to take treatment, so that optimal policy rules are merely suggestions in the presence of human non-adherence to treatment recommendations. In these same domains, there may be heterogeneity both in who responds in taking-up treatment, and heterogeneity in treatment efficacy. While optimal treatment rules can maximize causal outcomes across the population, access parity constraints or other fairness considerations can be relevant in the case of encouragement. For example, in social services, a persistent puzzle is the gap in take-up of beneficial services among those who may benefit from them the most. When in addition the decision-maker has distributional preferences over both access and average outcomes, the optimal decision rule changes. We study causal identification, statistical variance-reduced estimation, and robust estimation of optimal treatment rules, including under potential violations of positivity. We c
    
[^84]: MELAGE: 一种纯Python基于新生儿神经影像软件

    MELAGE: A purely python based Neuroimaging software (Neonatal). (arXiv:2309.07175v1 [eess.IV])

    [http://arxiv.org/abs/2309.07175](http://arxiv.org/abs/2309.07175)

    MELAGE是一款纯Python基于新生儿神经影像软件，具有卓越的适应性和丰富的功能。其核心特点是利用深度学习模块的半自动脑部提取工具，可精确高效地提取MRI和3D超声数据中的脑结构。该软件在图像分析、深度学习算法集成以及医学影像领域中具有广泛的应用前景。

    

    MELAGE是一种开创性的基于Python的神经影像软件，成为可视化、处理和分析医学图像的多功能工具。最初为了解决新生儿期处理3D超声和MRI脑图像的独特挑战而设计，MELAGE展示了出色的适应性，扩展了其在成人人脑影像领域的应用。MELAGE的核心是一个由深度学习模块增强的半自动脑部提取工具，确保从MRI和3D超声数据中精确高效地提取脑结构。此外，MELAGE还提供全面的功能套件，包括动态3D可视化、准确的测量和交互式图像分割。这个创新软件为研究人员和临床医生带来了巨大的希望，提供了简化的图像分析、与深度学习算法的无缝集成以及在医学影像领域的广泛适用性。

    MELAGE, a pioneering Python-based neuroimaging software, emerges as a versatile tool for the visualization, processing, and analysis of medical images. Initially conceived to address the unique challenges of processing 3D ultrasound and MRI brain images during the neonatal period, MELAGE exhibits remarkable adaptability, extending its utility to the domain of adult human brain imaging. At its core, MELAGE features a semi-automatic brain extraction tool empowered by a deep learning module, ensuring precise and efficient brain structure extraction from MRI and 3D Ultrasound data. Moreover, MELAGE offers a comprehensive suite of features, encompassing dynamic 3D visualization, accurate measurements, and interactive image segmentation. This transformative software holds immense promise for researchers and clinicians, offering streamlined image analysis, seamless integration with deep learning algorithms, and broad applicability in the realm of medical imaging.
    
[^85]: HurriCast：使用机器学习和统计建模的自动化框架用于飓风预测

    HurriCast: An Automatic Framework Using Machine Learning and Statistical Modeling for Hurricane Forecasting. (arXiv:2309.07174v1 [cs.LG])

    [http://arxiv.org/abs/2309.07174](http://arxiv.org/abs/2309.07174)

    本研究提出了HurriCast，一种使用机器学习和统计建模的自动化框架，通过组合ARIMA模型和K-MEANS算法以更好地捕捉飓风趋势，并结合Autoencoder进行改进的飓风模拟，从而有效模拟历史飓风行为并提供详细的未来预测。这项研究通过利用全面且有选择性的数据集，丰富了对飓风模式的理解，并为风险管理策略提供了可操作的见解。

    

    飓风由于其灾害性影响而在美国面临重大挑战。减轻这些风险很重要，保险业在这方面起着重要作用，使用复杂的统计模型进行风险评估。然而，这些模型常常忽视关键的时间和空间飓风模式，并受到数据稀缺的限制。本研究引入了一种改进的方法，将ARIMA模型和K-MEANS相结合，以更好地捕捉飓风趋势，并使用Autoencoder进行改进的飓风模拟。我们的实验证明，这种混合方法有效地模拟了历史飓风行为，同时提供了潜在未来路径和强度的详细预测。此外，通过利用全面而有选择性的数据集，我们的模拟丰富了对飓风模式的当前理解，并为风险管理策略提供了可操作的见解。

    Hurricanes present major challenges in the U.S. due to their devastating impacts. Mitigating these risks is important, and the insurance industry is central in this effort, using intricate statistical models for risk assessment. However, these models often neglect key temporal and spatial hurricane patterns and are limited by data scarcity. This study introduces a refined approach combining the ARIMA model and K-MEANS to better capture hurricane trends, and an Autoencoder for enhanced hurricane simulations. Our experiments show that this hybrid methodology effectively simulate historical hurricane behaviors while providing detailed projections of potential future trajectories and intensities. Moreover, by leveraging a comprehensive yet selective dataset, our simulations enrich the current understanding of hurricane patterns and offer actionable insights for risk management strategies.
    
[^86]: 使用无监督学习、有监督学习和数字孪生技术进行深对流冰风暴分类

    Using Unsupervised and Supervised Learning and Digital Twin for Deep Convective Ice Storm Classification. (arXiv:2309.07173v1 [cs.LG])

    [http://arxiv.org/abs/2309.07173](http://arxiv.org/abs/2309.07173)

    在这篇论文中，我们使用无监督学习、有监督学习和数字孪生技术来对深对流冰风暴进行分类。我们通过生成模拟的前瞻辐射计数据和“科学”隐藏变量的地球大气数字孪生技术来训练一个分类器，并且通过对科学隐藏变量进行K均值聚类来生成自动标记的数据集。

    

    智能冰云感知（SMICES）是一个小卫星概念，其中主雷达根据前瞻辐射计收集的信息智能地针对冰风暴目标。准确识别雷达计收集的八个波段的云暴类型对于智能目标选择非常关键。感兴趣的云暴类型包括：晴空、薄雨层云、层云、多雨顶和对流核心。我们描述了利用机器学习和地球大气数字孪生技术进行这种分类器推导的多步骤过程。首先，使用名为天气研究预报（WRF）的地球大气数字孪生生成模拟的前瞻辐射计数据以及更深层的“科学”隐藏变量。数据集模拟了加勒比地区的热带地区和美国大西洋沿岸的非热带地区。人类专家利用科学隐藏变量上的K均值聚类生成数据的自动标记-将每个数据映射到相应的云暴类型。

    Smart Ice Cloud Sensing (SMICES) is a small-sat concept in which a primary radar intelligently targets ice storms based on information collected by a lookahead radiometer. Critical to the intelligent targeting is accurate identification of storm/cloud types from eight bands of radiance collected by the radiometer. The cloud types of interest are: clear sky, thin cirrus, cirrus, rainy anvil, and convection core.  We describe multi-step use of Machine Learning and Digital Twin of the Earth's atmosphere to derive such a classifier. First, a digital twin of Earth's atmosphere called a Weather Research Forecast (WRF) is used generate simulated lookahead radiometer data as well as deeper "science" hidden variables. The datasets simulate a tropical region over the Caribbean and a non-tropical region over the Atlantic coast of the United States. A K-means clustering over the scientific hidden variables was utilized by human experts to generate an automatic labelling of the data - mapping each 
    
[^87]: 探索大型语言模型在本体对齐中的应用

    Exploring Large Language Models for Ontology Alignment. (arXiv:2309.07172v1 [cs.AI])

    [http://arxiv.org/abs/2309.07172](http://arxiv.org/abs/2309.07172)

    本文研究了大型语言模型在本体对齐中的应用，并发现它们有潜力在谨慎的框架和提示设计下超越现有的本体对齐系统。

    

    本文研究了最近的生成式大型语言模型（LLMs）（如GPT系列和Flan-T5）在本体对齐中的适用性，用于识别本体之间的概念等价映射。为了测试Flan-T5-XXL和GPT-3.5-turbo的零样本性能，我们利用了OAEI Bio-ML track的两个等价匹配数据集中具有挑战性的子集，并考虑到概念标签和结构上下文。初步结果表明，LLMs有可能在谨慎的框架和提示设计的情况下，胜过现有的本体对齐系统如BERTMap。

    This work investigates the applicability of recent generative Large Language Models (LLMs), such as the GPT series and Flan-T5, to ontology alignment for identifying concept equivalence mappings across ontologies. To test the zero-shot performance of Flan-T5-XXL and GPT-3.5-turbo, we leverage challenging subsets from two equivalence matching datasets of the OAEI Bio-ML track, taking into account concept labels and structural contexts. Preliminary findings suggest that LLMs have the potential to outperform existing ontology alignment systems like BERTMap, given careful framework and prompt design.
    
[^88]: 传感器数据在人体活动识别中的综述

    Overview of Human Activity Recognition Using Sensor Data. (arXiv:2309.07170v1 [eess.SP])

    [http://arxiv.org/abs/2309.07170](http://arxiv.org/abs/2309.07170)

    本研究对传感器数据在人体活动识别中的应用进行了综述，总结了传感器数据的使用和HAR技术的应用，提出了几种常见的机器学习方法，并探讨了HAR面临的挑战。

    

    人体活动识别（HAR）是一个重要的研究领域，在家庭自动化、工作场所自动化、安全监控以及医疗等多个应用中得到应用。在过去的十年中，从传统机器学习方法到最新发展的深度学习技术和物联网，HAR领域取得了显著的贡献。尽管已经发表了几篇综述和调研研究，但缺乏基于传感器的HAR综述研究，重点总结了可穿戴传感器和智能家居传感器数据的使用以及HAR和深度学习技术的应用。因此，我们总览了基于传感器的HAR，讨论了几个依赖于HAR的重要应用，并强调了已经用于HAR的常见机器学习方法。最后，我们探讨了几个HAR面临的挑战，这些挑战应该被解决以进一步提高HAR的鲁棒性。

    Human activity recognition (HAR) is an essential research field that has been used in different applications including home and workplace automation, security and surveillance as well as healthcare. Starting from conventional machine learning methods to the recently developing deep learning techniques and the Internet of things, significant contributions have been shown in the HAR area in the last decade. Even though several review and survey studies have been published, there is a lack of sensor-based HAR overview studies focusing on summarising the usage of wearable sensors and smart home sensors data as well as applications of HAR and deep learning techniques. Hence, we overview sensor-based HAR, discuss several important applications that rely on HAR, and highlight the most common machine learning methods that have been used for HAR. Finally, several challenges of HAR are explored that should be addressed to further improve the robustness of HAR.
    
[^89]: 复合移位算子的频率收敛问题

    Frequency Convergence of Complexon Shift Operators. (arXiv:2309.07169v1 [eess.SP])

    [http://arxiv.org/abs/2309.07169](http://arxiv.org/abs/2309.07169)

    本文研究了拓扑信号处理中复合子的可转移性，通过构造边际复合子和复合移位算子，研究其特征值和特征向量，并证明了复合子收敛时对应的复合移位算子的特征值会收敛到极限复合子的特征值。这些结果拓展了图信号处理框架。

    

    拓扑信号处理(TSP)利用单纯形复合来建模比顶点和边更高阶的结构。本文研究了TSP的可转移性，通过一种称为复合子的广义高阶图的版本。我们回顾了复合子的概念，即单纯形复合序列的极限[1]。受图移位算子的积分算子形式的启发，我们根据复合子的所有可能尺寸的组件构造了边际复合子和复合移位算子(CSO)。我们研究了CSO的特征值和特征向量，并将它们与一类新的加权邻接矩阵相关联。我们证明，当一个单纯形复合序列收敛到一个复合子时，相应的CSO的特征值收敛到极限复合子的特征值。这些结果暗示了在大型单纯形复合或单纯形复合序列上的学习可转移性，从而推广了图信号处理框架。

    Topological signal processing (TSP) utilizes simplicial complexes to model structures with higher order than vertices and edges. In this paper, we study the transferability of TSP via a generalized higher-order version of graphon, known as complexon. We recall the notion of a complexon as the limit of a simplicial complex sequence [1]. Inspired by the integral operator form of graphon shift operators, we construct a marginal complexon and complexon shift operator (CSO) according to components of all possible dimensions from the complexon. We investigate the CSO's eigenvalues and eigenvectors, and relate them to a new family of weighted adjacency matrices. We prove that when a simplicial complex sequence converges to a complexon, the eigenvalues of the corresponding CSOs converge to that of the limit complexon. These results hint at learning transferability on large simplicial complexes or simplicial complex sequences, which generalize the graphon signal processing framework.
    
[^90]: 通过可达性分析在分层强化学习中对目标空间进行抽象

    Goal Space Abstraction in Hierarchical Reinforcement Learning via Reachability Analysis. (arXiv:2309.07168v1 [cs.LG])

    [http://arxiv.org/abs/2309.07168](http://arxiv.org/abs/2309.07168)

    本文介绍了一种通过可达性分析在分层强化学习中对目标空间进行抽象的方法，以自动发现符号目标表示。该方法通过将具有相似任务角色的环境状态集合在一起的紧密关联表示来发现子目标，在导航任务中表现出可解释性和数据效率。

    

    开放式学习极大地受益于使用符号方法进行目标表示，因为它们提供了一种将知识结构化为高效且易于迁移的学习方式。然而，依赖于符号推理的现有分层强化学习方法通常受限于手动目标表示。自主发现符号目标表示的挑战在于它必须保留关键信息，如环境动力学。在这项工作中，我们提出了一种通过紧密关联（即将具有类似任务角色的环境状态集合在一起）的新兴表示发现子目标的发展机制。我们创建了一个逐渐学习此表示以及策略的分层强化学习算法，并使用导航任务进行评估，以展示学到的表示具有可解释性并实现数据效率。

    Open-ended learning benefits immensely from the use of symbolic methods for goal representation as they offer ways to structure knowledge for efficient and transferable learning. However, the existing Hierarchical Reinforcement Learning (HRL) approaches relying on symbolic reasoning are often limited as they require a manual goal representation. The challenge in autonomously discovering a symbolic goal representation is that it must preserve critical information, such as the environment dynamics. In this work, we propose a developmental mechanism for subgoal discovery via an emergent representation that abstracts (i.e., groups together) sets of environment states that have similar roles in the task. We create a HRL algorithm that gradually learns this representation along with the policies and evaluate it on navigation tasks to show the learned representation is interpretable and results in data efficiency.
    
[^91]: EEG基于认知负荷检测的实验范式和深度神经网络的系统综述

    Systematic Review of Experimental Paradigms and Deep Neural Networks for Electroencephalography-Based Cognitive Workload Detection. (arXiv:2309.07163v1 [eess.SP])

    [http://arxiv.org/abs/2309.07163](http://arxiv.org/abs/2309.07163)

    本文对基于EEG的认知负荷估计进行了系统综述，发现DNN在离线和实时分类方面都表现出色，但研究中较少采用可解释的神经网络模型。

    

    本文对基于脑电图(EEG)的认知负荷(CWL)估计进行了系统综述。文章的重点有两个方面：识别用于可靠地引发离散和可量化的认知负荷水平的不同实验范式，以及用于信号分类的深度神经网络(DNN)中常用输入形式的特定性质和表征结构。分析揭示了许多研究使用原生表示的EEG信号的二维矩阵进行离线CW检测。然而，只有少数研究采用了用于实时CWL估计的在线或伪在线分类策略。此外，在这份综述中，只有几个可解释的DNN和一个生成模型被用于认知负荷检测。总的来说，DNN证明是用于分类EEG信号的有价值的工具。

    This article summarizes a systematic review of the electroencephalography (EEG)-based cognitive workload (CWL) estimation. The focus of the article is twofold: identify the disparate experimental paradigms used for reliably eliciting discreet and quantifiable levels of cognitive load and the specific nature and representational structure of the commonly used input formulations in deep neural networks (DNNs) used for signal classification. The analysis revealed a number of studies using EEG signals in its native representation of a two-dimensional matrix for offline classification of CWL. However, only a few studies adopted an online or pseudo-online classification strategy for real-time CWL estimation. Further, only a couple of interpretable DNNs and a single generative model were employed for cognitive load detection till date during this review. More often than not, researchers were using DNNs as black-box type models. In conclusion, DNNs prove to be valuable tools for classifying EE
    
[^92]: 一种强大且简单的BCI MI解码的深度学习基线

    A Strong and Simple Deep Learning Baseline for BCI MI Decoding. (arXiv:2309.07159v1 [eess.SP])

    [http://arxiv.org/abs/2309.07159](http://arxiv.org/abs/2309.07159)

    本论文提出了一种强大且简单的深度学习基线EEG-SimpleConv，用于BCI中的运动想象解码。与其他方法相比，EEG-SimpleConv表现至少同样好或更高效，具有强大的知识传递能力，推理时间较低。

    

    我们提出了EEG-SimpleConv，这是一个用于BCI中运动想象解码的直观的一维卷积神经网络。我们的主要动机是提出一个非常简单的基线来进行比较，仅使用文献中非常标准的元素。我们在四个EEG运动想象数据集上评估其性能，包括模拟在线设置，并将其与最近的深度学习和机器学习方法进行比较。EEG-SimpleConv至少与其他方法一样好甚至更高效，在主体间展示了强大的知识传递能力，同时具有低推理时间的成本。我们主张使用现成的元素而不是提出特定的解决方案，可以显著帮助BCI中深度学习方法的采用。我们提供了模型和实验的代码。

    We propose EEG-SimpleConv, a straightforward 1D convolutional neural network for Motor Imagery decoding in BCI. Our main motivation is to propose a very simple baseline to compare to, using only very standard ingredients from the literature. We evaluate its performance on four EEG Motor Imagery datasets, including simulated online setups, and compare it to recent Deep Learning and Machine Learning approaches. EEG-SimpleConv is at least as good or far more efficient than other approaches, showing strong knowledge-transfer capabilities across subjects, at the cost of a low inference time. We advocate that using off-the-shelf ingredients rather than coming with ad-hoc solutions can significantly help the adoption of Deep Learning approaches for BCI. We make the code of the models and the experiments accessible.
    
[^93]: AI中的压缩实数：使用RISC-V CPU的案例研究

    Compressed Real Numbers for AI: a case-study using a RISC-V CPU. (arXiv:2309.07158v1 [cs.LG])

    [http://arxiv.org/abs/2309.07158](http://arxiv.org/abs/2309.07158)

    本文针对AI中的深度神经网络（DNN）使用较低精度进行训练，探讨了16位和8位压缩格式的应用。通过在计算之前解压压缩操作数，可以提高带宽使用和缓存效率。

    

    正如最近的研究所示，通常使用单精度IEEE 754浮点数（二进制32位）训练的深度神经网络（DNN）也可以使用较低精度。因此，16位和8位的压缩格式引起了相当大的关注。本文侧重于两个已经在机器学习应用中压缩二进制32位数取得有趣结果的格式族：bfloat和posit。尽管16位和8位的bfloat/posit常用于减少已经训练好的DNN的权重/偏差的存储，但推理仍然经常在CPU的32位FPU上进行（尤其是如果没有GPU可用）。本文提出了一种在计算之前解压bfloat/posit张量的方法，即在压缩操作数加载到支持向量的向量寄存器之后，以节省带宽使用和提高缓存效率。最后，我们展示了

    As recently demonstrated, Deep Neural Networks (DNN), usually trained using single precision IEEE 754 floating point numbers (binary32), can also work using lower precision. Therefore, 16-bit and 8-bit compressed format have attracted considerable attention. In this paper, we focused on two families of formats that have already achieved interesting results in compressing binary32 numbers in machine learning applications, without sensible degradation of the accuracy: bfloat and posit. Even if 16-bit and 8-bit bfloat/posit are routinely used for reducing the storage of the weights/biases of trained DNNs, the inference still often happens on the 32-bit FPU of the CPU (especially if GPUs are not available). In this paper we propose a way to decompress a tensor of bfloat/posits just before computations, i.e., after the compressed operands have been loaded within the vector registers of a vector capable CPU, in order to save bandwidth usage and increase cache efficiency. Finally, we show the
    
[^94]: 未知模式下的配电网线路中断识别与性能保证

    Distribution Grid Line Outage Identification with Unknown Pattern and Performance Guarantee. (arXiv:2309.07157v1 [cs.LG])

    [http://arxiv.org/abs/2309.07157](http://arxiv.org/abs/2309.07157)

    本研究提出了一种实用而稳健的检测方法，只利用电压幅值，无需相角或功率流数据，通过数据驱动的方法学习中断后分布的参数，并通过添加Bregman散度约束来解决可行性问题，证明可以学习到最优参数。

    

    配电网线路中断识别对于可持续的电网运行至关重要。本研究提出了一种实用而稳健的检测方法，只利用可用的电压幅值，消除了昂贵的相角或功率流数据的需求。针对传感器数据，许多现有的基于变点检测的检测方法需要对中断模式进行先验知识，而真实世界中的中断场景却是未知的。为了消除这种不切实际的要求，我们提出了一种数据驱动的方法，通过梯度下降来学习中断后分布的参数。然而，直接使用梯度下降存在可行性问题。为了解决这个问题，我们通过添加Bregman散度约束来修改我们的方法，以控制参数更新的轨迹，从而消除了可行性问题。如今，及时操作是关键，我们证明通过利用收敛保证可以学习到最优参数。

    Line outage identification in distribution grids is essential for sustainable grid operation. In this work, we propose a practical yet robust detection approach that utilizes only readily available voltage magnitudes, eliminating the need for costly phase angles or power flow data. Given the sensor data, many existing detection methods based on change-point detection require prior knowledge of outage patterns, which are unknown for real-world outage scenarios. To remove this impractical requirement, we propose a data-driven method to learn the parameters of the post-outage distribution through gradient descent. However, directly using gradient descent presents feasibility issues. To address this, we modify our approach by adding a Bregman divergence constraint to control the trajectory of the parameter updates, which eliminates the feasibility problems. As timely operation is the key nowadays, we prove that the optimal parameters can be learned with convergence guarantees via leveragin
    
[^95]: 深入研究睡眠：基于单通道脑电图的睡眠阶段分类与模型解释性

    A Deep Dive into Sleep: Single-Channel EEG-Based Sleep Stage Classification with Model Interpretability. (arXiv:2309.07156v1 [eess.SP])

    [http://arxiv.org/abs/2309.07156](http://arxiv.org/abs/2309.07156)

    本文提出了一种基于单通道脑电图的睡眠阶段分类方法，使用SE-Resnet-Bi-LSTM架构进行睡眠阶段的分类，并进行了模型解释性分析。在三个不同的数据集上进行了全面评估，取得了显著的准确率和宏F1得分。

    

    睡眠是一种基本的生理过程，在我们的生活中占据着重要的部分。准确分类睡眠阶段是评估睡眠质量和识别可能的睡眠障碍的关键工具。本研究引入了一种新颖的方法，利用SE-Resnet-Bi-LSTM架构将睡眠分类为五个不同的阶段。分类过程基于对单通道脑电图（EEG）的分析。建议的框架由两个基本元素组成：利用SE-ResNet的特征提取器和利用Bi-LSTM单元堆栈的时间上下文编码器。我们的方法的有效性通过在三个不同的数据集上进行的全面评估得到证实，分别是SLeepEDF-20、SleepEDF-78和SHHS。值得注意的是，我们的方法在相应的数据集上分别达到了显著的准确率，分别为87.5％、83.9％和87.8％，并且在宏F1得分方面分别为82.5、78.9和81.9。

    Sleep, a fundamental physiological process, occupies a significant portion of our lives. Accurate classification of sleep stages serves as a crucial tool for evaluating sleep quality and identifying probable sleep disorders. This work introduces a novel methodology that utilises a SE-Resnet-Bi-LSTM architecture to classify sleep into five separate stages. The classification process is based on the analysis of single-channel electroencephalograms (EEGs). The framework that has been suggested consists of two fundamental elements: a feature extractor that utilises SE-ResNet, and a temporal context encoder that use stacks of Bi-LSTM units.The effectiveness of our approach is substantiated by thorough assessments conducted on three different datasets, namely SLeepEDF-20, SleepEDF-78, and SHHS. Significantly, our methodology attains notable levels of accuracy, specifically 87.5\%, 83.9\%, and 87.8\%, along with macro-F1 scores of 82.5, 78.9, and 81.9 for the corresponding datasets. Notably, 
    
[^96]: 在复杂网络中寻找影响者：一种有效的深度强化学习方法

    Finding Influencers in Complex Networks: An Effective Deep Reinforcement Learning Approach. (arXiv:2309.07153v1 [cs.SI])

    [http://arxiv.org/abs/2309.07153](http://arxiv.org/abs/2309.07153)

    本文提出了一种有效的深度强化学习模型，通过将图神经网络作为编码器、强化学习作为解码器，实现了在复杂网络中寻找影响者的任务上的优越性能，超过了传统的最佳影响力最大化算法。

    

    在社交网络分析中，最大化复杂网络中的影响力是一个实际重要但计算上具有挑战性的任务，因为它是一个NP难问题。目前大多数近似或启发式方法要么需要巨大的人工设计工作，要么在效果和效率之间无法达到令人满意的平衡。最近的机器学习尝试只注重速度，而缺乏性能提升。本文中，与以前的尝试不同，我们提出了一种有效的深度强化学习模型，它在传统的最佳影响力最大化算法上表现出优越的性能。具体而言，我们设计了一个端到端的学习框架，将图神经网络作为编码器，强化学习作为解码器，命名为DREIM。通过在小规模合成图上进行大量训练，DREIM在非常大的合成网络和真实世界网络上在解决质量方面超过了最先进的基准方法，我们还通过实验证明了其线性可扩展性。

    Maximizing influences in complex networks is a practically important but computationally challenging task for social network analysis, due to its NPhard nature. Most current approximation or heuristic methods either require tremendous human design efforts or achieve unsatisfying balances between effectiveness and efficiency. Recent machine learning attempts only focus on speed but lack performance enhancement. In this paper, different from previous attempts, we propose an effective deep reinforcement learning model that achieves superior performances over traditional best influence maximization algorithms. Specifically, we design an end-to-end learning framework that combines graph neural network as the encoder and reinforcement learning as the decoder, named DREIM. Trough extensive training on small synthetic graphs, DREIM outperforms the state-of-the-art baseline methods on very large synthetic and real-world networks on solution quality, and we also empirically show its linear sca
    
[^97]: 通过知识蒸馏和潜在扩散模型从脑电图中解码视觉脑表示

    Decoding visual brain representations from electroencephalography through Knowledge Distillation and latent diffusion models. (arXiv:2309.07149v1 [eess.SP])

    [http://arxiv.org/abs/2309.07149](http://arxiv.org/abs/2309.07149)

    本研究提出了一种创新方法，利用脑电图数据解码人脑中的视觉表示。通过将EEG数据转换为频谱图并使用卷积神经网络进行训练，结合基于知识蒸馏的图像分类教师网络，我们的模型在图像分类和重建任务上表现出色。

    

    从人脑活动中解码视觉表示已成为一个蓬勃发展的研究领域，特别是在脑机接口的背景下。我们的研究提出了一种创新方法，利用来自图像Net数据集的脑电图（EEG）数据来分类和重构图像（即“脑解码”）。我们分析了来自6名参与者的EEG记录，每名参与者观看了覆盖40个独特语义类别的50个图像。这些EEG读数被转换为频谱图，然后用于训练一个卷积神经网络（CNN），集成了基于预训练的对比语言-图像预训练（CLIP）图像分类教师网络的知识蒸馏过程。这种策略使我们的模型达到了80%的前五位准确率，显著优于标准CNN和各种基于RNN的基准。此外，我们还引入了一种图像重构机制。

    Decoding visual representations from human brain activity has emerged as a thriving research domain, particularly in the context of brain-computer interfaces. Our study presents an innovative method that employs to classify and reconstruct images from the ImageNet dataset using electroencephalography (EEG) data from subjects that had viewed the images themselves (i.e. "brain decoding"). We analyzed EEG recordings from 6 participants, each exposed to 50 images spanning 40 unique semantic categories. These EEG readings were converted into spectrograms, which were then used to train a convolutional neural network (CNN), integrated with a knowledge distillation procedure based on a pre-trained Contrastive Language-Image Pre-Training (CLIP)-based image classification teacher network. This strategy allowed our model to attain a top-5 accuracy of 80%, significantly outperforming a standard CNN and various RNN-based benchmarks. Additionally, we incorporated an image reconstruction mechanism ba
    
[^98]: DGSD: 基于动态图自蒸馏的基于EEG的听觉空间注意力检测

    DGSD: Dynamical Graph Self-Distillation for EEG-Based Auditory Spatial Attention Detection. (arXiv:2309.07147v1 [eess.SP])

    [http://arxiv.org/abs/2309.07147](http://arxiv.org/abs/2309.07147)

    本文提出了一种基于动态图自蒸馏的方法，用于处理具有非欧几里得特征的 EEG 信号，并提高听觉空间注意力检测的性能。

    

    听觉注意力检测 (AAD) 旨在从多说话者环境中的脑信号中检测目标说话者。尽管基于 EEG 的 AAD 方法近年来取得了令人期待的结果，但目前的方法主要依赖于为处理像图像这样的欧几里得数据设计的传统卷积神经网络。这使得处理具有非欧几里得特征的 EEG 信号变得具有挑战性。为了解决这个问题，本文提出了一种用于 AAD 的动态图自蒸馏 (DGSD) 方法，其不需要语音刺激作为输入。具体来说，为了有效表示 EEG 信号的非欧几里得特性，应用动态图卷积网络来表示 EEG 信号的图结构，该网络还可以提取与听觉空间注意力相关的关键特征。此外，为了进一步提高 AAD 检测性能，进行了自蒸馏，包括特征蒸馏和分层蒸馏。

    Auditory Attention Detection (AAD) aims to detect target speaker from brain signals in a multi-speaker environment. Although EEG-based AAD methods have shown promising results in recent years, current approaches primarily rely on traditional convolutional neural network designed for processing Euclidean data like images. This makes it challenging to handle EEG signals, which possess non-Euclidean characteristics. In order to address this problem, this paper proposes a dynamical graph self-distillation (DGSD) approach for AAD, which does not require speech stimuli as input. Specifically, to effectively represent the non-Euclidean properties of EEG signals, dynamical graph convolutional networks are applied to represent the graph structure of EEG signals, which can also extract crucial features related to auditory spatial attention in EEG signals. In addition, to further improve AAD detection performance, self-distillation, consisting of feature distillation and hierarchical distillation
    
[^99]: ETP: 通过ECG-Text预训练学习可迁移的ECG表示

    ETP: Learning Transferable ECG Representations via ECG-Text Pre-training. (arXiv:2309.07145v1 [eess.SP])

    [http://arxiv.org/abs/2309.07145](http://arxiv.org/abs/2309.07145)

    本论文介绍了ECG-Text预训练（ETP）框架，它通过将ECG信号与文本报告对齐，实现了跨模态ECG特征学习。ETP在线性评估和零样本分类任务中表现出色，并展示了其在跨模态ECG特征学习方面的鲁棒性和可迁移性。

    

    在心血管保健领域中，心电图（ECG）作为一种重要的非侵入性诊断工具。尽管近年来自我监督学习（SSL）在ECG表示学习方面取得了进展，但这些技术通常需要注释样本，并且在微调阶段难以处理不存在的类别。为了解决这些限制，我们引入了ECG-Text预训练（ETP），这是一种创新的框架，旨在学习将ECG信号与文本报告联系起来的跨模态表示。该框架首次在ECG领域中利用了零样本分类任务。ETP使用ECG编码器和预训练语言模型来将ECG信号与其相应的文本报告对齐。所提出的框架在线性评估和零样本分类任务中表现出色，在PTB-XL和CPSC2018数据集上的实验结果表明其具有鲁棒且可迁移的跨模态ECG特征学习能力。

    In the domain of cardiovascular healthcare, the Electrocardiogram (ECG) serves as a critical, non-invasive diagnostic tool. Although recent strides in self-supervised learning (SSL) have been promising for ECG representation learning, these techniques often require annotated samples and struggle with classes not present in the fine-tuning stages. To address these limitations, we introduce ECG-Text Pre-training (ETP), an innovative framework designed to learn cross-modal representations that link ECG signals with textual reports. For the first time, this framework leverages the zero-shot classification task in the ECG domain. ETP employs an ECG encoder along with a pre-trained language model to align ECG signals with their corresponding textual reports. The proposed framework excels in both linear evaluation and zero-shot classification tasks, as demonstrated on the PTB-XL and CPSC2018 datasets, showcasing its ability for robust and generalizable cross-modal ECG feature learning.
    
[^100]: 基于人工智能的乒乓球运动员动作技能识别与评估系统设计

    Design of Recognition and Evaluation System for Table Tennis Players' Motor Skills Based on Artificial Intelligence. (arXiv:2309.07141v1 [eess.SP])

    [http://arxiv.org/abs/2309.07141](http://arxiv.org/abs/2309.07141)

    本研究基于人工智能，设计了一种用于识别和评估乒乓球运动员动作技能的系统，通过改进可穿戴设备，并利用特征工程、降维和不同评估指标的损失函数实现了动作的模式识别和层次化评估。

    

    随着电子科学技术的飞速发展，对可穿戴设备的研究不断更新，但目前来看，对于可穿戴设备识别和分析特定运动的能力还不够全面。基于此，本文改进了乒乓球运动的可穿戴设备，并通过人工智能实现了乒乓球运动员动作技能的模式识别和评估。首先设计了一个设备来收集乒乓球运动员的动作信息，并对实际运动数据进行处理。其次，制作了一个滑动窗口来将收集到的动作数据分割成六个乒乓球基准动作的特征数据库。然后，基于特征工程构建了动作特征，并通过降维后的不同模型来识别不同的动作技能。最后，利用不同评估指标的损失函数建立了动作技能的层次化评估系统。

    With the rapid development of electronic science and technology, the research on wearable devices is constantly updated, but for now, it is not comprehensive for wearable devices to recognize and analyze the movement of specific sports. Based on this, this paper improves wearable devices of table tennis sport, and realizes the pattern recognition and evaluation of table tennis players' motor skills through artificial intelligence. Firstly, a device is designed to collect the movement information of table tennis players and the actual movement data is processed. Secondly, a sliding window is made to divide the collected motion data into a characteristic database of six table tennis benchmark movements. Thirdly, motion features were constructed based on feature engineering, and motor skills were identified for different models after dimensionality reduction. Finally, the hierarchical evaluation system of motor skills is established with the loss functions of different evaluation indexes.
    
[^101]: 基于CNN-SAEDN-Res的短期功率负荷预测方法

    Short-term power load forecasting method based on CNN-SAEDN-Res. (arXiv:2309.07140v1 [eess.SP])

    [http://arxiv.org/abs/2309.07140](http://arxiv.org/abs/2309.07140)

    提出了一种基于CNN-SAEDN-Res的短期功率负荷预测方法，通过结合卷积神经网络、自注意力编码器-解码器网络和残差修正技术，能够有效处理带有非时序因素的负荷数据并提高预测精度。

    

    在深度学习中，带有非时序因素的负荷数据难以通过序列模型进行处理。这个问题导致了预测的精度不足。因此，提出了一种基于卷积神经网络（CNN）、自注意力编码器-解码器网络（SAEDN）和残差修正（Res）的短期负荷预测方法。在该方法中，特征提取模块由一个二维卷积神经网络组成，用于挖掘数据之间的局部相关性并获取高维数据特征。初始负荷预测模块由一个自注意力编码器-解码器网络和一个前馈神经网络（FFN）组成。该模块利用自注意机制对高维特征进行编码。这个操作可以获取数据之间的全局相关性。因此，该模型能够基于混合了非时序因素的数据中的耦合关系保留重要的信息。

    In deep learning, the load data with non-temporal factors are difficult to process by sequence models. This problem results in insufficient precision of the prediction. Therefore, a short-term load forecasting method based on convolutional neural network (CNN), self-attention encoder-decoder network (SAEDN) and residual-refinement (Res) is proposed. In this method, feature extraction module is composed of a two-dimensional convolutional neural network, which is used to mine the local correlation between data and obtain high-dimensional data features. The initial load fore-casting module consists of a self-attention encoder-decoder network and a feedforward neural network (FFN). The module utilizes self-attention mechanisms to encode high-dimensional features. This operation can obtain the global correlation between data. Therefore, the model is able to retain important information based on the coupling relationship between the data in data mixed with non-time series factors. Then, self
    
[^102]: 基于多编码器自编码器的自监督盲源分离

    Self-Supervised Blind Source Separation via Multi-Encoder Autoencoders. (arXiv:2309.07138v1 [eess.SP])

    [http://arxiv.org/abs/2309.07138](http://arxiv.org/abs/2309.07138)

    本论文提出了一种基于多编码器自编码器和自监督学习的方法，用于解决盲源分离问题。通过训练网络进行输入解码和重构，然后利用编码掩蔽技术进行源推断，同时引入路径分离损失以促进稀疏性。

    

    盲源分离（BSS）的任务是在没有先验知识的情况下从混合信号中分离出源信号和混合系统。这是一个具有挑战性的问题，通常需要对混合系统和源信号做出限制性的假设。本文提出了一种新颖的方法来解决非线性混合的BSS问题，该方法利用多编码器自编码器的自然特征子空间专门化能力，并通过完全自监督学习进行训练，而不需要强先验知识。在训练阶段，我们的方法将输入解码成多编码器网络的单独编码空间，然后在解码器内重新混合这些表示以重构输入。然后，为了进行源推断，我们引入了一种新颖的编码掩蔽技术，即屏蔽除一个编码外的所有编码，使得解码器能够估计源信号。为此，我们还引入了一种称为路径分离损失的方法，以促进编码之间的稀疏性。

    The task of blind source separation (BSS) involves separating sources from a mixture without prior knowledge of the sources or the mixing system. This is a challenging problem that often requires making restrictive assumptions about both the mixing system and the sources. In this paper, we propose a novel method for addressing BSS of non-linear mixtures by leveraging the natural feature subspace specialization ability of multi-encoder autoencoders with fully self-supervised learning without strong priors. During the training phase, our method unmixes the input into the separate encoding spaces of the multi-encoder network and then remixes these representations within the decoder for a reconstruction of the input. Then to perform source inference, we introduce a novel encoding masking technique whereby masking out all but one of the encodings enables the decoder to estimate a source signal. To this end, we also introduce a so-called pathway separation loss that encourages sparsity betwe
    
[^103]: 使用正向和逆向模式的自动微分将偏微分方程引入JAX

    Bringing PDEs to JAX with forward and reverse modes automatic differentiation. (arXiv:2309.07137v1 [cs.MS])

    [http://arxiv.org/abs/2309.07137](http://arxiv.org/abs/2309.07137)

    这篇论文介绍了如何将偏微分方程引入到JAX库中，通过使用Firedrake有限元库的接口来实现。而使用正向和逆向模式的自动微分计算方法，可以高效地组合有限元求解器和可微程序。

    

    偏微分方程（PDEs）用于描述各种物理现象。通常这些方程没有解析解，而是使用数值逼近方法。其中一种常见的求解PDEs的方法是有限元方法。在科学计算中，计算解对输入参数的导数信息非常重要。我们将JAX自动微分库扩展到与Firedrake有限元库的接口。PDE的高级符号表示允许通过绕过基础非线性求解器的低级多次迭代来进行微分。通过Firedrake求解器的微分计算使用切线线性和伴随方程来完成。这使得有限元求解器可以与任意可微程序高效组合。代码可在github.com/IvanYashchuk/jax-firedrake上找到。

    Partial differential equations (PDEs) are used to describe a variety of physical phenomena. Often these equations do not have analytical solutions and numerical approximations are used instead. One of the common methods to solve PDEs is the finite element method. Computing derivative information of the solution with respect to the input parameters is important in many tasks in scientific computing. We extend JAX automatic differentiation library with an interface to Firedrake finite element library. High-level symbolic representation of PDEs allows bypassing differentiating through low-level possibly many iterations of the underlying nonlinear solvers. Differentiating through Firedrake solvers is done using tangent-linear and adjoint equations. This enables the efficient composition of finite element solvers with arbitrary differentiable programs. The code is available at github.com/IvanYashchuk/jax-firedrake.
    
[^104]: 基于掩码Transformer的心电图分类研究

    Masked Transformer for Electrocardiogram Classification. (arXiv:2309.07136v1 [eess.SP])

    [http://arxiv.org/abs/2309.07136](http://arxiv.org/abs/2309.07136)

    提出了一种基于掩码Transformer的ECG分类方法，命名为MTECG，扩展了掩码自动编码器在ECG时间序列上的应用，该方法在广泛的掩码比例下表现稳定良好，并进行了消融实验验证了重构目标的波动性、训练计划长度、逐层学习率衰减和DropPath率的重要性。

    

    心电图（ECG）是临床应用中最重要的诊断工具之一。随着先进算法的出现，各种深度学习模型已被应用于ECG任务。然而，尽管Transformer在计算机视觉和自然语言处理领域取得了广泛成功，但其在ECG数据上的潜力尚未得到实现。在本研究中，我们提出了一种有用的基于掩码Transformer的ECG分类方法，称为MTECG，它将掩码自动编码器的应用扩展到了ECG时间序列上。我们构建了一个包含220,251个ECG记录的数据集，这些记录由医学专家进行了广泛的诊断注释，以探索MTECG的特性。在提出的训练策略下，一个只有5.7M参数的轻量级模型在广泛的掩码比例（5%-75%）下表现稳定良好。消融研究突出了波动重构目标、训练计划长度、逐层学习率衰减和DropPath率的重要性。实验发现MTECG耗时较少且能够有效分类各种心电图。

    Electrocardiogram (ECG) is one of the most important diagnostic tools in clinical applications. With the advent of advanced algorithms, various deep learning models have been adopted for ECG tasks. However, the potential of Transformers for ECG data is not yet realized, despite their widespread success in computer vision and natural language processing. In this work, we present a useful masked Transformer method for ECG classification referred to as MTECG, which expands the application of masked autoencoders to ECG time series. We construct a dataset comprising 220,251 ECG recordings with a broad range of diagnoses annoated by medical experts to explore the properties of MTECG. Under the proposed training strategies, a lightweight model with 5.7M parameters performs stably well on a broad range of masking ratios (5%-75%). The ablation studies highlight the importance of fluctuated reconstruction targets, training schedule length, layer-wise LR decay and DropPath rate. The experiments o
    
[^105]: EpiDeNet：用于嵌入式系统的节能性癫痫检测方法

    EpiDeNet: An Energy-Efficient Approach to Seizure Detection for Embedded Systems. (arXiv:2309.07135v1 [eess.SP])

    [http://arxiv.org/abs/2309.07135](http://arxiv.org/abs/2309.07135)

    EpiDeNet是一种节能性癫痫检测方法，通过引入新的轻量级癫痫检测网络和结合灵敏度和特异度的损失函数(SSWCE)，成功应对了严重不平衡的数据集挑战，并在两个不同的数据集上实现了高达91.16％和92.00％的癫痫事件检测准确率。

    

    癫痫是一种广泛存在的神经系统疾病，影响着数百万患者的生活，连续监测与自动化癫痫检测成为有效治疗的必要条件。为了在日常生活条件下实现长期护理，需要舒适和智能的可穿戴设备，并且要具备长电池寿命，这反过来对资源受限和节能计算解决方案提出了要求。在这种情况下，癫痫检测的机器学习算法的发展面临着严重不平衡的数据集挑战。本文介绍了一种新的轻量级癫痫检测网络EpiDeNet和一种新的损失函数Sensitivity-Specificity Weighted Cross-Entropy (SSWCE)，它结合了灵敏度和特异度，以应对严重不平衡的数据集挑战。提出的EpiDeNet-SSWCE方法在两个不同的数据集（CHB-MIT和PEDESITE）上成功检测到了91.16％和92.00％的癫痫事件。

    Epilepsy is a prevalent neurological disorder that affects millions of individuals globally, and continuous monitoring coupled with automated seizure detection appears as a necessity for effective patient treatment. To enable long-term care in daily-life conditions, comfortable and smart wearable devices with long battery life are required, which in turn set the demand for resource-constrained and energy-efficient computing solutions. In this context, the development of machine learning algorithms for seizure detection faces the challenge of heavily imbalanced datasets. This paper introduces EpiDeNet, a new lightweight seizure detection network, and Sensitivity-Specificity Weighted Cross-Entropy (SSWCE), a new loss function that incorporates sensitivity and specificity, to address the challenge of heavily unbalanced datasets. The proposed EpiDeNet-SSWCE approach demonstrates the successful detection of 91.16% and 92.00% seizure events on two different datasets (CHB-MIT and PEDESITE, re
    
[^106]: 基于熵的机器学习模型用于智能物联网环境中帕金森病的诊断和监测

    Entropy-based machine learning model for diagnosis and monitoring of Parkinson's Disease in smart IoT environment. (arXiv:2309.07134v1 [eess.SP])

    [http://arxiv.org/abs/2309.07134](http://arxiv.org/abs/2309.07134)

    基于熵的机器学习模型在智能物联网环境中利用静息状态脑电信号进行帕金森病的诊断和监测。通过计算不同类型的熵，我们发现模糊熵在诊断和监测PD方面表现最佳。通过使用特定的特征组合，我们实现了高准确度的PD诊断。最重要的脑电信号频率范围和通道位置也被确定。

    

    本研究提出了一个计算效率高的机器学习模型，用于在物联网环境中利用静息状态脑电信号（rs-EEG）对帕金森病（PD）进行诊断和监测。我们计算了不同类型的熵，并发现模糊熵在使用rs-EEG进行PD诊断和监测方面表现最好。我们还研究了不同信号频率范围和脑电通道的组合，以准确诊断PD。最后，在较少的特征（11个特征）下，我们实现了约99.9%的最大分类准确度（ARKF）。我们确定了最明显的脑电信号频率范围，并发现高分类准确度取决于低频信号成分（0-4 Hz）。此外，最具信息量的信号主要来自头部的右半球（F8、P8、T8、FC6）。此外，我们还评估了使用三种不同长度的E

    The study presents the concept of a computationally efficient machine learning (ML) model for diagnosing and monitoring Parkinson's disease (PD) in an Internet of Things (IoT) environment using rest-state EEG signals (rs-EEG). We computed different types of entropy from EEG signals and found that Fuzzy Entropy performed the best in diagnosing and monitoring PD using rs-EEG. We also investigated different combinations of signal frequency ranges and EEG channels to accurately diagnose PD. Finally, with a fewer number of features (11 features), we achieved a maximum classification accuracy (ARKF) of ~99.9%. The most prominent frequency range of EEG signals has been identified, and we have found that high classification accuracy depends on low-frequency signal components (0-4 Hz). Moreover, the most informative signals were mainly received from the right hemisphere of the head (F8, P8, T8, FC6). Furthermore, we assessed the accuracy of the diagnosis of PD using three different lengths of E
    
[^107]: 使用可穿戴设备的机器学习模型自动识别认知能力较差的老年人

    Using wearable device-based machine learning models to autonomously identify older adults with poor cognition. (arXiv:2309.07133v1 [eess.SP])

    [http://arxiv.org/abs/2309.07133](http://arxiv.org/abs/2309.07133)

    通过使用可穿戴设备的机器学习模型，可以在老年人正常生活条件下连续监测其认知水平，并能够预测认知能力较差的情况，为早期干预提供了替代方法。

    

    进行认知测试对患者和临床医生来说非常耗时。基于可穿戴设备的预测模型可以在正常生活条件下进行持续的健康监测，并可以作为早期干预认知障碍老年人的一种替代方法。在本研究中，我们首先提取了与生物钟节律、环境光照暴露、身体活动水平、睡眠和信号处理相关的新颖可穿戴特征。然后，我们评估了基于可穿戴设备的机器学习模型根据数字符号替代测试（DSST）、建立阿尔茨海默病登记簿学习子测验（CERAD-WL）和动物流利性测试（AFT）的结果来预测认知能力较差的能力。我们发现，与包含年龄、性别、教育水平、婚姻状况、家庭收入、糖尿病状态和抑郁状况等基准模型相比，基于可穿戴设备的模型在预测三个认知结果时具有显着更高的AUC。

    Conducting cognitive tests is time-consuming for patients and clinicians. Wearable device-based prediction models allow for continuous health monitoring under normal living conditions and could offer an alternative to identifying older adults with cognitive impairments for early interventions. In this study, we first derived novel wearable-based features related to circadian rhythms, ambient light exposure, physical activity levels, sleep, and signal processing. Then, we quantified the ability of wearable-based machine-learning models to predict poor cognition based on outcomes from the Digit Symbol Substitution Test (DSST), the Consortium to Establish a Registry for Alzheimers Disease Word-Learning subtest (CERAD-WL), and the Animal Fluency Test (AFT). We found that the wearable-based models had significantly higher AUCs when predicting all three cognitive outcomes compared to benchmark models containing age, sex, education, marital status, household income, diabetic status, depressio
    
[^108]: 有向加权图的最优输运距离：以细胞间通讯网络为案例研究

    Optimal transport distances for directed, weighted graphs: a case study with cell-cell communication networks. (arXiv:2309.07030v1 [cs.LG])

    [http://arxiv.org/abs/2309.07030](http://arxiv.org/abs/2309.07030)

    本文提出了两种基于最优输运的距离度量，用于比较有向图，并通过仿真图数据和单细胞RNA-seq数据推断的实际细胞间通讯图对其相对表现进行了评估。

    

    近年来，比较最优输运图引起了相当大的关注，因为最优输运引起的距离既提供了图之间的合理度量，又通过输运计划的可解释描述了图之间相关变化。然而，由于缺乏对称性，通常考虑的最优输运距离主要用于无向图。本文提出了两种基于最优输运变体的距离度量来比较有向图：（i）地球移动距离（Wasserstein）和（ii）Gromov-Wasserstein（GW）距离。我们评估了这两种距离，并讨论了它们在仿真图数据和基于单细胞RNA-seq数据推断的实际有向细胞间通讯图上的相对表现。

    Comparing graphs of optimal transport has recently gained significant attention, as the distances induced by optimal transport provide both a principled metric between graphs as well as an interpretable description of the associated changes between graphs in terms of a transport plan. As the lack of symmetry introduces challenges in the typically considered formulations, optimal transport distances for graphs have mostly been developed for undirected graphs. Here, we propose two distance measures to compare directed graphs based on variants of optimal transport: (i) an earth movers distance (Wasserstein) and (ii) a Gromov-Wasserstein (GW) distance. We evaluate these two distances and discuss their relative performance for both simulated graph data and real-world directed cell-cell communication graphs, inferred from single-cell RNA-seq data.
    
[^109]: 缺失数据下的不确定性交通预测

    Uncertainty-aware Traffic Prediction under Missing Data. (arXiv:2309.06800v1 [cs.LG])

    [http://arxiv.org/abs/2309.06800](http://arxiv.org/abs/2309.06800)

    本研究提出了一种考虑不确定性的交通预测方法，可以处理缺失数据和测量不确定性，并适用于风险敏感任务和决策导向问题。

    

    交通预测是一个重要的课题，因为它在交通领域有广泛的应用。近期，许多研究取得了很好的结果。然而，大多数研究假设预测位置有完整或至少部分的历史记录，不能扩展到无历史记录的位置。在现实场景中，由于预算限制和安装可行性问题，传感器的部署可能受限，这使得大多数当前模型不适用。虽然少数文献尝试在缺失位置上插补交通状态，但这些方法需要与传感器位置同时观测的数据，使它们不适用于预测任务。另一个缺点是缺乏对预测不确定性的测量，使得之前的工作不适用于风险敏感的任务或涉及决策的情况。为了填补这一空白，受到先前的归纳图神经网络的启发，本文提出了一种考虑不确定性的方法。

    Traffic prediction is a crucial topic because of its broad scope of applications in the transportation domain. Recently, various studies have achieved promising results. However, most studies assume the prediction locations have complete or at least partial historical records and cannot be extended to non-historical recorded locations. In real-life scenarios, the deployment of sensors could be limited due to budget limitations and installation availability, which makes most current models not applicable. Though few pieces of literature tried to impute traffic states at the missing locations, these methods need the data simultaneously observed at the locations with sensors, making them not applicable to prediction tasks. Another drawback is the lack of measurement of uncertainty in prediction, making prior works unsuitable for risk-sensitive tasks or involving decision-making. To fill the gap, inspired by the previous inductive graph neural network, this work proposed an uncertainty-awa
    
[^110]: 深度非参数凸化滤波在计算摄影，图像合成和对抗性防御中的应用

    Deep Nonparametric Convexified Filtering for Computational Photography, Image Synthesis and Adversarial Defense. (arXiv:2309.06724v1 [cs.CV])

    [http://arxiv.org/abs/2309.06724](http://arxiv.org/abs/2309.06724)

    该论文提出了一种使用深度非参数凸化滤波（DNCF）的通用框架，用于计算摄影领域中的图像恢复。DNCF具有强大的泛化性和对抗性图像处理的鲁棒性，同时能够实现实时的对抗性图像分类网络防御。

    

    我们旨在提供一个通用框架，用于从不完美的图像中恢复真实场景的计算摄影，通过深度非参数凸化滤波（DNCF）。它由一个非参数深度网络组成，以模拟图像形成背后的物理方程，如降噪、超分辨率、修复和闪光。DNCF没有依赖于训练数据的参数化，因此具有强大的泛化性和对抗性图像处理的鲁棒性。在推理过程中，我们还鼓励网络参数为非负，并在输入和参数上创建一个双凸函数，这适应于运行时间不足的二阶优化算法，相对于Deep Image Prior有10倍的加速。通过这些工具，我们在实时中实验证明了其对抗图像分类深度网络攻击算法的能力。

    We aim to provide a general framework of for computational photography that recovers the real scene from imperfect images, via the Deep Nonparametric Convexified Filtering (DNCF). It is consists of a nonparametric deep network to resemble the physical equations behind the image formation, such as denoising, super-resolution, inpainting, and flash. DNCF has no parameterization dependent on training data, therefore has a strong generalization and robustness to adversarial image manipulation. During inference, we also encourage the network parameters to be nonnegative and create a bi-convex function on the input and parameters, and this adapts to second-order optimization algorithms with insufficient running time, having 10X acceleration over Deep Image Prior. With these tools, we empirically verify its capability to defend image classification deep networks against adversary attack algorithms in real-time.
    
[^111]: 分布式机器学习资源上的混合算法选择和超参数调整: 一种基于层级代理的方法

    Hybrid Algorithm Selection and Hyperparameter Tuning on Distributed Machine Learning Resources: A Hierarchical Agent-based Approach. (arXiv:2309.06604v1 [cs.LG])

    [http://arxiv.org/abs/2309.06604](http://arxiv.org/abs/2309.06604)

    本文提出了一种基于代理的层级机器学习平台，用于选择分布式组织的机器学习算法并同时调整其超参数。该方法具有可伸缩性、灵活性和鲁棒性，并支持自动化和协同的功能。

    

    算法选择和超参数调整是学术界和应用机器学习中关键的步骤。然而，由于机器学习资源数量的大幅增加、多样性和分布性，这些步骤变得越来越复杂。当将多智能体系统应用于机器学习平台的设计时，会带来可伸缩性、灵活性和鲁棒性等多个独特特性。本文提出了一种完全自动和协同的基于代理的机制，用于选择分布式组织的机器学习算法，并同时调整其超参数。我们的方法基于现有的基于代理的层级机器学习平台，并通过增强其查询结构来支持上述功能，而不限于特定的学习、选择和调整机制。我们进行了理论评估、形式验证和分析。

    Algorithm selection and hyperparameter tuning are critical steps in both academic and applied machine learning. On the other hand, these steps are becoming ever increasingly delicate due to the extensive rise in the number, diversity, and distributedness of machine learning resources. Multi-agent systems, when applied to the design of machine learning platforms, bring about several distinctive characteristics such as scalability, flexibility, and robustness, just to name a few. This paper proposes a fully automatic and collaborative agent-based mechanism for selecting distributedly organized machine learning algorithms and simultaneously tuning their hyperparameters. Our method builds upon an existing agent-based hierarchical machine-learning platform and augments its query structure to support the aforementioned functionalities without being limited to specific learning, selection, and tuning mechanisms. We have conducted theoretical assessments, formal verification, and analytical st
    
[^112]: 通过重要性重新加权纠正采样偏差的空间建模方法

    Correcting sampling biases via importancereweighting for spatial modeling. (arXiv:2309.04824v1 [cs.LG])

    [http://arxiv.org/abs/2309.04824](http://arxiv.org/abs/2309.04824)

    通过重要性重新加权修正采样偏差的空间建模方法，可以有效解决分布偏移问题，实现目标错误的无偏估计。该方法在人工数据实验中表现出优势，使预测误差从7%降至2%，且样本量越大效果越好。

    

    在机器学习模型中，由于分布偏差，特别是在环境研究中的空间数据，对错误的估计通常是复杂的。我们引入了一种基于重要性采样思想的方法，以获得目标错误的无偏估计。通过考虑期望错误和可用数据之间的差异，我们的方法在每个样本点上重新加权错误并抵消偏移。我们使用重要性采样技术和核密度估计来进行重加权。我们使用类似真实世界空间数据集的人工数据验证了我们方法的有效性。我们的研究结果证明了我们提出的方法在目标错误估计方面的优势，并为分布偏移问题提供了解决方案。总体预测误差从7%降低到仅为2%，且随着样本量的增加而进一步降低。

    In machine learning models, the estimation of errors is often complex due to distribution bias, particularly in spatial data such as those found in environmental studies. We introduce an approach based on the ideas of importance sampling to obtain an unbiased estimate of the target error. By taking into account difference between desirable error and available data, our method reweights errors at each sample point and neutralizes the shift. Importance sampling technique and kernel density estimation were used for reweighteing. We validate the effectiveness of our approach using artificial data that resemble real-world spatial datasets. Our findings demonstrate advantages of the proposed approach for the estimation of the target error, offering a solution to a distribution shift problem. Overall error of predictions dropped from 7% to just 2% and it gets smaller for larger samples.
    
[^113]: 通过分类哈希表示和分层强化交叉进行自优化特征生成

    Self-optimizing Feature Generation via Categorical Hashing Representation and Hierarchical Reinforcement Crossing. (arXiv:2309.04612v1 [cs.LG])

    [http://arxiv.org/abs/2309.04612](http://arxiv.org/abs/2309.04612)

    通过分类哈希表示和分层强化交叉，提出了一种自优化特征生成的通用框架，解决了现有系统中的一些挑战，包括有意义、稳健和高效的生成。

    

    特征生成旨在生成新的有意义的特征，以创建一个具有区分度的表示空间。当生成的特征来自具有固有特征交互的特征对时，生成的特征是有意义的。在现实世界中，经验丰富的数据科学家可以识别出潜在有用的特征-特征交互，并从指数级的搜索空间中生成有意义的维度，在最优的生成路径上以最优的交叉形式。但是，机器的人类化能力有限。我们将这类学习任务概括为自优化特征生成。自优化特征生成对现有系统提出了几个未解决的挑战：有意义、稳健和高效的生成。为了解决这些挑战，我们提出了一种基于原则和通用的表示交叉框架来解决自优化特征生成。为了实现哈希表示，我们提出了一个三步法：特征离散化、特征哈希和...

    Feature generation aims to generate new and meaningful features to create a discriminative representation space.A generated feature is meaningful when the generated feature is from a feature pair with inherent feature interaction. In the real world, experienced data scientists can identify potentially useful feature-feature interactions, and generate meaningful dimensions from an exponentially large search space, in an optimal crossing form over an optimal generation path. But, machines have limited human-like abilities.We generalize such learning tasks as self-optimizing feature generation. Self-optimizing feature generation imposes several under-addressed challenges on existing systems: meaningful, robust, and efficient generation. To tackle these challenges, we propose a principled and generic representation-crossing framework to solve self-optimizing feature generation.To achieve hashing representation, we propose a three-step approach: feature discretization, feature hashing, and 
    
[^114]: DMI (Deuterium Metabolic Imaging) 的敏感性增强的深度学习方法

    A Deep Learning Method for Sensitivity Enhancement of Deuterium Metabolic Imaging (DMI). (arXiv:2309.04100v1 [eess.IV])

    [http://arxiv.org/abs/2309.04100](http://arxiv.org/abs/2309.04100)

    本研究提出了一种用于增强DMI敏感性的深度学习方法，通过训练卷积神经网络来估计低信噪比和失真的DMI FID的代谢物浓度，并通过MRI的边缘保护正则化进一步提高估计精度。实验结果显示，该方法在提高参数质量方面具有潜在的改进效果。

    

    目的：DMI（Deuterium Metabolic Imaging）的空间分辨率和最小扫描时间受到可达到的信噪比的限制。本研究提出了一种用于增强DMI敏感性的深度学习方法。方法：设计了一个卷积神经网络（CNN）来估计低信噪比和失真的DMI FID的2H标记代谢物浓度。CNN使用合成数据进行训练，这些数据代表通常在体内遇到的一系列信噪比水平。通过对每个DMI数据集使用基于MRI的边缘保护正则化进行CNN的微调，进一步提高了估计精度。提出的处理方法，称为PRECISE-DMI（PReserved Edge ConvolutIonal neural network for Sensitivity Enhanced DMI），应用于模拟研究和体内实验，评估了在SNR上预期的改进，并研究了可能的不准确性。结果：PRECISE-DMI在低信噪比数据集的代谢图像上有着明显的改善，并提高了参数的质量。

    Purpose: Common to most MRSI techniques, the spatial resolution and the minimal scan duration of Deuterium Metabolic Imaging (DMI) are limited by the achievable SNR. This work presents a deep learning method for sensitivity enhancement of DMI.  Methods: A convolutional neural network (CNN) was designed to estimate the 2H-labeled metabolite concentrations from low SNR and distorted DMI FIDs. The CNN was trained with synthetic data that represent a range of SNR levels typically encountered in vivo. The estimation precision was further improved by fine-tuning the CNN with MRI-based edge-preserving regularization for each DMI dataset. The proposed processing method, PReserved Edge ConvolutIonal neural network for Sensitivity Enhanced DMI (PRECISE-DMI), was applied to simulation studies and in vivo experiments to evaluate the anticipated improvements in SNR and investigate the potential for inaccuracies.  Results: PRECISE-DMI visually improved the metabolic maps of low SNR datasets, and qua
    
[^115]: eDKM:一种用于大型语言模型的高效准确的训练时权重聚类方法

    eDKM: An Efficient and Accurate Train-time Weight Clustering for Large Language Models. (arXiv:2309.00964v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.00964](http://arxiv.org/abs/2309.00964)

    本文提出了一种内存高效的DKM实现，即eDKM，用于大型语言模型的高效准确的训练时权重聚类方法。它通过减小DKM的内存占用，解决了LLM压缩中的训练开销问题。

    

    由于大型语言模型（LLMs）在许多复杂语言任务上表现出高质量性能，因此将这些LLMs引入移动设备以实现更快的响应和更好的隐私保护引起了极大兴趣。然而，LLMs的规模（数十亿个参数）需要高效的压缩才能适应存储有限的设备。在众多压缩技术中，权重聚类是LLM压缩的领先候选方法之一，并得到了现代智能手机的支持。然而，其训练开销对LLM的微调来说是难以承受的。特别是，可微分K均值聚类（DKM）已经显示出在压缩比和准确性回归之间的最先进折衷方案，但其较大的内存复杂性使其几乎不可能应用于训练时的LLM压缩。因此，在本文中，我们提出了一种内存高效的DKM实现，即eDKM，通过创新的技术减小了DKM的内存占用。

    Since Large Language Models or LLMs have demonstrated high-quality performance on many complex language tasks, there is a great interest in bringing these LLMs to mobile devices for faster responses and better privacy protection. However, the size of LLMs (i.e., billions of parameters) requires highly effective compression to fit into storage-limited devices. Among many compression techniques, weight-clustering, a form of non-linear quantization, is one of the leading candidates for LLM compression, and supported by modern smartphones. Yet, its training overhead is prohibitively significant for LLM fine-tuning. Especially, Differentiable KMeans Clustering, or DKM, has shown the state-of-the-art trade-off between compression ratio and accuracy regression, but its large memory complexity makes it nearly impossible to apply to train-time LLM compression. In this paper, we propose a memory-efficient DKM implementation, eDKM powered by novel techniques to reduce the memory footprint of DKM 
    
[^116]: GBE-MLZSL: 一种用于多标签零样本学习的群体双增强框架

    GBE-MLZSL: A Group Bi-Enhancement Framework for Multi-Label Zero-Shot Learning. (arXiv:2309.00923v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.00923](http://arxiv.org/abs/2309.00923)

    GBE-MLZSL是一种用于多标签零样本学习的群体双增强框架，通过整合局部和全局特征，解决了在推理未见类时模型失去主要组成部分把握的问题。

    

    本文研究了多标签场景下的零样本学习（MLZSL）的一个具有挑战性的问题，即模型通过使用已见类和辅助知识（例如语义信息）在一个样本（例如一张图片）中识别多个未见过的类别。现有方法通常通过分析样本中各个已见类的关系（从空间或语义特征的角度），并将学习到的模型迁移到未见类上。但它们忽略了局部和全局特征的有效整合。也就是说，在推理未见类时，全局特征表示图像在特征空间中的主要方向，而局部特征应该在一定范围内保持唯一性。这种整合的忽视会使模型失去对图像主要组成部分的把握。仅在推理阶段依赖于已见类的局部存在会引入不可避免的偏差。在本文中，我们提出一种群体双增强框架，用于解决这个问题。

    This paper investigates a challenging problem of zero-shot learning in the multi-label scenario (MLZSL), wherein, the model is trained to recognize multiple unseen classes within a sample (e.g., an image) based on seen classes and auxiliary knowledge, e.g., semantic information. Existing methods usually resort to analyzing the relationship of various seen classes residing in a sample from the dimension of spatial or semantic characteristics, and transfer the learned model to unseen ones. But they ignore the effective integration of local and global features. That is, in the process of inferring unseen classes, global features represent the principal direction of the image in the feature space, while local features should maintain uniqueness within a certain range. This integrated neglect will make the model lose its grasp of the main components of the image. Relying only on the local existence of seen classes during the inference stage introduces unavoidable bias. In this paper, we pro
    
[^117]: DoRA: 基于领域的低资源房地产评估自监督学习框架

    DoRA: Domain-Based Self-Supervised Learning Framework for Low-Resource Real Estate Appraisal. (arXiv:2309.00855v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.00855](http://arxiv.org/abs/2309.00855)

    DoRA是一种基于领域的低资源房地产评估自监督学习框架，通过学习未标记的房地产数据集合来减少主观性，并且融入领域知识。

    

    连接需求和供应的市场系统已被探索用于开发对房地产进行公正的决策。房地产评估是金融机构中一项高成本的资产估值任务，因为它需要领域专家基于相应的知识和市场的判断来评估估价。现有的自动化估值模型减少了领域专家的主观性，但需要大量的交易用于有效评估，这主要受限于交易的标记工作以及对新开发和农村地区的泛化能力。现有的自监督学习（SSL）用于表格数据学习未标记的房地产集合时，忽视了各种重要特征，并且无法融入领域知识。本文提出了DoRA，一种基于领域的低资源房地产评估自监督学习框架。

    The marketplace system connecting demands and supplies has been explored to develop unbiased decision-making in valuing properties. Real estate appraisal serves as one of the high-cost property valuation tasks for financial institutions since it requires domain experts to appraise the estimation based on the corresponding knowledge and the judgment of the market. Existing automated valuation models reducing the subjectivity of domain experts require a large number of transactions for effective evaluation, which is predominantly limited to not only the labeling efforts of transactions but also the generalizability of new developing and rural areas. To learn representations from unlabeled real estate sets, existing self-supervised learning (SSL) for tabular data neglects various important features, and fails to incorporate domain knowledge. In this paper, we propose DoRA, a Domain-based self-supervised learning framework for low-resource Real estate Appraisal. DoRA is pre-trained with an
    
[^118]: 两个列表什么时候比一个列表更好？合作决策中的益处和伤害

    When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-making. (arXiv:2308.11721v1 [cs.LG])

    [http://arxiv.org/abs/2308.11721](http://arxiv.org/abs/2308.11721)

    这项研究分析了一种特定类型的人工和算法合作，对于多个噪音模型来说，将选择项目的子集大小$k$设置在$[2, n-1]$范围内能够最大化最终选择最佳项目的概率。

    

    在过去的机器学习研究中，很大一部分关注的是算法的性能，但最近更多地关注于优化人工和算法的联合性能。在这里，我们分析了一种特定类型的人工和算法合作，在这种合作中，算法可以访问一组n个项目，并将大小为k的一个子集呈现给人类，然后人类从这些k个项目中选择一个最终项目。这种情况可以模拟内容推荐、路径规划或任何类型的标注任务。由于人类和算法都对项目的真实排序有着不完美、有噪音的信息，关键问题是：哪个$k$值能最大化最终选择最佳项目的概率？对于$k=1$，算法单独行动时性能最优，而对于$k=n$，人类单独行动时性能最优。令人惊讶的是，我们发现对于多个噪音模型，将$k$设置在$[2, n-1]$范围内是最优的，也就是说，合作有明显的益处。

    Historically, much of machine learning research has focused on the performance of the algorithm alone, but recently more attention has been focused on optimizing joint human-algorithm performance. Here, we analyze a specific type of human-algorithm collaboration where the algorithm has access to a set of $n$ items, and presents a subset of size $k$ to the human, who selects a final item from among those $k$. This scenario could model content recommendation, route planning, or any type of labeling task. Because both the human and algorithm have imperfect, noisy information about the true ordering of items, the key question is: which value of $k$ maximizes the probability that the best item will be ultimately selected? For $k=1$, performance is optimized by the algorithm acting alone, and for $k=n$ it is optimized by the human acting alone. Surprisingly, we show that for multiple of noise models, it is optimal to set $k \in [2, n-1]$ - that is, there are strict benefits to collaborating,
    
[^119]: 通过CFD耦合多精度贝叶斯优化的机器学习辅助下的新反应器设计的发现

    Machine Learning-Assisted Discovery of Novel Reactor Designs via CFD-Coupled Multi-fidelity Bayesian Optimisation. (arXiv:2308.08841v1 [cs.CE])

    [http://arxiv.org/abs/2308.08841](http://arxiv.org/abs/2308.08841)

    本研究通过应用多精度贝叶斯优化方法，结合参数化网格化和模拟，提出了两种新的螺旋管参数化方法，用于发现新的反应器设计。这种方法能够处理高维度和复杂的优化问题，并克服了没有梯度的非局部优化困难。

    

    添加制造技术使得更先进的反应器几何结构成为可能，为更大和更复杂的设计空间提供了潜在机会。在更广泛的设计空间中确定和优化有希望的配置对于现有的人为中心的设计方法来说是一个重大挑战。鉴于算法改进和添加制造的出现，我们提出了两种新的螺旋管参数化方法，可以改变截面和螺旋路径，从而产生了一系列高维的复杂优化问题。为了确保可行的非局部优化，在没有梯度的情况下，我们应用多精度贝叶斯优化。我们的方法描述了多个连续的精度，并与参数化网格化和模拟相结合，实现了低质量、

    Additive manufacturing has enabled the production of more advanced reactor geometries, resulting in the potential for significantly larger and more complex design spaces. Identifying and optimising promising configurations within broader design spaces presents a significant challenge for existing human-centric design approaches. As such, existing parameterisations of coiled-tube reactor geometries are low-dimensional with expensive optimisation limiting more complex solutions. Given algorithmic improvements and the onset of additive manufacturing, we propose two novel coiled-tube parameterisations enabling the variation of cross-section and coil path, resulting in a series of high dimensional, complex optimisation problems. To ensure tractable, non-local optimisation where gradients are not available, we apply multi-fidelity Bayesian optimisation. Our approach characterises multiple continuous fidelities and is coupled with parameterised meshing and simulation, enabling lower quality, 
    
[^120]: 基于神经分类先验的基于物理的角色控制研究

    Neural Categorical Priors for Physics-Based Character Control. (arXiv:2308.07200v2 [cs.GR] UPDATED)

    [http://arxiv.org/abs/2308.07200](http://arxiv.org/abs/2308.07200)

    本文提出了一种新的基于神经分类先验的学习框架，用于控制基于物理的角色，利用强化学习和离散信息瓶颈，生成高质量逼真的行为。

    

    最近在学习可重用运动先验方面取得了一些进展，证明了它们在生成自然行为方面的有效性。在本文中，我们提出了一种新的学习框架，用于控制基于物理的角色，相比现有最先进的方法，显著改进了运动质量和多样性。所提出的方法利用强化学习（RL）来追踪和模仿来自非结构化运动剪辑的逼真动作，使用离散信息瓶颈，如矢量量化变分自动编码器（VQ-VAE）中所采用的那样。该结构将来自运动剪辑的最相关信息压缩成一个紧凑而且信息丰富的潜在空间，即一个离散的向量量化码空间。通过从经过训练的分类先验分布中采样空间中的码，可以生成高质量逼真的行为，类似于在计算机视觉中使用VQ-VAE。虽然这个先验分布可以通过监督方法进行训练，

    Recent advances in learning reusable motion priors have demonstrated their effectiveness in generating naturalistic behaviors. In this paper, we propose a new learning framework in this paradigm for controlling physics-based characters with significantly improved motion quality and diversity over existing state-of-the-art methods. The proposed method uses reinforcement learning (RL) to initially track and imitate life-like movements from unstructured motion clips using the discrete information bottleneck, as adopted in the Vector Quantized Variational AutoEncoder (VQ-VAE). This structure compresses the most relevant information from the motion clips into a compact yet informative latent space, i.e., a discrete space over vector quantized codes. By sampling codes in the space from a trained categorical prior distribution, high-quality life-like behaviors can be generated, similar to the usage of VQ-VAE in computer vision. Although this prior distribution can be trained with the supervis
    
[^121]: 通过高阶HSIC学习具有增量信息的非参数DAGs

    Learning nonparametric DAGs with incremental information via high-order HSIC. (arXiv:2308.05969v1 [cs.LG])

    [http://arxiv.org/abs/2308.05969](http://arxiv.org/abs/2308.05969)

    本文提出了一个基于高阶HSIC的方法，在学习Bayesian网络中解决了局部变量同时具有直接和间接依赖关系的问题，通过确定子集和两阶段算法来进行局部修正，取得了良好的效果。

    

    针对学习贝叶斯网络（BN）的基于评分的方法，目标是最大化全局评分函数。然而，如果局部变量同时具有直接和间接依赖关系，那么基于评分函数的全局优化将忽略具有间接依赖关系的变量之间的边缘，其得分小于具有直接依赖关系的边缘。本文提出了一个基于确定子集的可辨识性条件，以识别潜在的DAG。通过可辨识性条件，我们开发了一个两阶段算法，即最优调整（OT）算法，以在全局优化的基础上进行局部修正。在最优阶段，基于一阶Hilbert-Schmidt独立性准则（HSIC）的优化问题给出了一个估计的骨架作为初始确定的父节点子集。在调整阶段，根据高阶HSIC的理论证明增量特性，对骨架进行局部调整，包括删除、添加和DAG格式化策略。

    Score-based methods for learning Bayesain networks(BN) aim to maximizing the global score functions. However, if local variables have direct and indirect dependence simultaneously, the global optimization on score functions misses edges between variables with indirect dependent relationship, of which scores are smaller than those with direct dependent relationship. In this paper, we present an identifiability condition based on a determined subset of parents to identify the underlying DAG. By the identifiability condition, we develop a two-phase algorithm namely optimal-tuning (OT) algorithm to locally amend the global optimization. In the optimal phase, an optimization problem based on first-order Hilbert-Schmidt independence criterion (HSIC) gives an estimated skeleton as the initial determined parents subset. In the tuning phase, the skeleton is locally tuned by deletion, addition and DAG-formalization strategies using the theoretically proved incremental properties of high-order HS
    
[^122]: 可转移的图神经指纹模型快速应对未来生物威胁

    Transferable Graph Neural Fingerprint Models for Quick Response to Future Bio-Threats. (arXiv:2308.01921v1 [q-bio.BM])

    [http://arxiv.org/abs/2308.01921](http://arxiv.org/abs/2308.01921)

    该论文提出了一种可转移的图神经指纹模型，用于快速应对未来的生物威胁。通过利用包含30万种候选药物和23个冠状病毒蛋白靶的COVID-19药物对接数据集，训练了高通量虚拟COVID-19药物筛选的图神经指纹模型。与传统指纹方法相比，该模型在对接得分上具有较高的预测准确性，并且提出了可转移的图神经指纹方法，能够适用于未知的靶点。

    

    基于配体结合亲和力的药物分子快速筛选是药物发现管线中的重要步骤。图神经指纹是一种用于开发高通量和高准确性分子对接代理的有希望方法。在这项研究中，我们建立了一个包含约30万种药物候选物和23个冠状病毒蛋白靶的COVID-19药物对接数据集。利用这个数据集，我们训练了图神经指纹对接模型，用于高通量虚拟COVID-19药物筛选。图神经指纹模型在对接得分上具有很高的预测准确性，对大多数对接靶点的均方误差低于0.21 kcal/mol，相比传统圆形指纹方法有显著改进。为了使神经指纹适用于未知的靶点，我们还提出了一种在多个靶点上训练的可转移的图神经指纹方法。

    Fast screening of drug molecules based on the ligand binding affinity is an important step in the drug discovery pipeline. Graph neural fingerprint is a promising method for developing molecular docking surrogates with high throughput and great fidelity. In this study, we built a COVID-19 drug docking dataset of about 300,000 drug candidates on 23 coronavirus protein targets. With this dataset, we trained graph neural fingerprint docking models for high-throughput virtual COVID-19 drug screening. The graph neural fingerprint models yield high prediction accuracy on docking scores with the mean squared error lower than $0.21$ kcal/mol for most of the docking targets, showing significant improvement over conventional circular fingerprint methods. To make the neural fingerprints transferable for unknown targets, we also propose a transferable graph neural fingerprint method trained on multiple targets. With comparable accuracy to target-specific graph neural fingerprint models, the transf
    
[^123]: 使用端到端视频异常检测系统来基准测试Jetson边缘设备

    Benchmarking Jetson Edge Devices with an End-to-end Video-based Anomaly Detection System. (arXiv:2307.16834v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.16834](http://arxiv.org/abs/2307.16834)

    本论文实现了一个端到端的视频异常检测系统，通过从监控视频输入进行犯罪现场异常检测，并在多个Jetson边缘设备上部署和运行。这是对Jetson平台在深度学习算法执行方面性能的基准测试分析的创新。

    

    创新的嵌入式系统平台，特别是硬件加速，显着影响了深度学习在现实场景中的应用。这些创新将人类劳动转化为自动化的智能系统，应用于自动驾驶、机器人技术、物联网和许多其他有重大影响的应用领域。NVIDIA的Jetson平台是在执行深度学习算法方面能够提供能效和吞吐率最佳性能的先驱之一。先前的大部分基准测试分析都是基于2D图像，并使用单个深度学习模型进行每个比较结果。在本文中，我们实现了一种从监控视频输入的端到端基于视频的犯罪现场异常检测系统，并将该系统部署在多个Jetson边缘设备上（Nano、AGX Xavier、Orin Nano）。比较分析包括将Torch-TensorRT集成为软件。

    Innovative enhancement in embedded system platforms, specifically hardware accelerations, significantly influence the application of deep learning in real-world scenarios. These innovations translate human labor efforts into automated intelligent systems employed in various areas such as autonomous driving, robotics, Internet-of-Things (IoT), and numerous other impactful applications. NVIDIA's Jetson platform is one of the pioneers in offering optimal performance regarding energy efficiency and throughput in the execution of deep learning algorithms. Previously, most benchmarking analysis was based on 2D images with a single deep learning model for each comparison result. In this paper, we implement an end-to-end video-based crime-scene anomaly detection system inputting from surveillance videos and the system is deployed and completely operates on multiple Jetson edge devices (Nano, AGX Xavier, Orin Nano). The comparison analysis includes the integration of Torch-TensorRT as a softwar
    
[^124]: GOKU-UI：通过关注力和多射击实现连续时间生成模型的普适推理

    GOKU-UI: Ubiquitous Inference through Attention and Multiple Shooting for Continuous-time Generative Models. (arXiv:2307.05735v1 [cs.LG])

    [http://arxiv.org/abs/2307.05735](http://arxiv.org/abs/2307.05735)

    GOKU-UI是一种普适推理的SciML生成模型，通过关注机制和多射击训练策略，在包括随机微分方程在内的各种微分方程中展现出了优异的性能表现。它具有显著的数据效率，并在合成和实证数据集上优于所有基线模型。

    

    科学机器学习（SciML）是一个蓬勃发展的领域，它将领域感知和可解释的模型与通用的机器学习技术相结合。在这项工作中，我们介绍了GOKU-UI，这是SciML生成模型GOKU-nets的一种演进。GOKU-UI扩展了原始模型的范围，将其他类别的微分方程，如随机微分方程（SDEs），融入其中，并通过关注机制和在潜在空间中的新型多射击训练策略实现了分布式的、即无处不在的推理。这些改进使其在重建和预测任务中的性能显著提高，我们通过对模拟和实证数据的评估进行了验证。具体而言，即使训练集的大小缩小了32倍，GOKU-UI在合成数据集上表现出色，超过了所有基线模型，凸显其出色的数据效率。此外，当应用于实证的人脑数据时，同时融合了随机微分方程和注意力机制的GOKU-UI也展现出了出色的性能。

    Scientific Machine Learning (SciML) is a burgeoning field that synergistically combines domain-aware and interpretable models with agnostic machine learning techniques. In this work, we introduce GOKU-UI, an evolution of the SciML generative model GOKU-nets. The GOKU-UI broadens the original model's spectrum to incorporate other classes of differential equations, such as Stochastic Differential Equations (SDEs), and integrates a distributed, i.e. ubiquitous, inference through attention mechanisms and a novel multiple shooting training strategy in the latent space. These enhancements have led to a significant increase in its performance in both reconstruction and forecast tasks, as demonstrated by our evaluation of simulated and empirical data. Specifically, GOKU-UI outperformed all baseline models on synthetic datasets even with a training set 32-fold smaller, underscoring its remarkable data efficiency. Furthermore, when applied to empirical human brain data, while incorporating stoch
    
[^125]: 从自然语言描述生成参数化BRDFs

    Generating Parametric BRDFs from Natural Language Descriptions. (arXiv:2306.15679v1 [cs.GR])

    [http://arxiv.org/abs/2306.15679](http://arxiv.org/abs/2306.15679)

    这项研究开发了一个模型，可以根据描述性的文本提示生成参数化的BRDFs，为艺术性地创作3D环境提供了一种新的方法。

    

    艺术性地创作3D环境是一项艰巨的工作，同时也需要熟练的内容创作者。使用机器学习来解决生成3D内容的不同方面，如生成网格、排列几何、合成纹理等方面已取得了令人印象深刻的改进。在本文中，我们开发了一个模型，可以根据描述性的文本提示生成双向反射分布函数（BRDFs）。 BRDFs是表征光与表面材料相互作用的四维概率分布。它们可以被表示为参数化的形式，也可以通过列举每对入射和出射角度的概率密度来表示。前者适用于艺术编辑，而后者则用于测量实际材料的外观。许多工作都集中在从材料图像假设BRDF模型。我们学习了从材料的文本描述到参数化BRDF的映射。我们的模型是一个...

    Artistic authoring of 3D environments is a laborious enterprise that also requires skilled content creators. There have been impressive improvements in using machine learning to address different aspects of generating 3D content, such as generating meshes, arranging geometry, synthesizing textures, etc. In this paper we develop a model to generate Bidirectional Reflectance Distribution Functions (BRDFs) from descriptive textual prompts. BRDFs are four dimensional probability distributions that characterize the interaction of light with surface materials. They are either represented parametrically, or by tabulating the probability density associated with every pair of incident and outgoing angles. The former lends itself to artistic editing while the latter is used when measuring the appearance of real materials. Numerous works have focused on hypothesizing BRDF models from images of materials. We learn a mapping from textual descriptions of materials to parametric BRDFs. Our model is f
    
[^126]: EmbodiedGPT: 通过思维链预训练的视觉语言模型

    EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought. (arXiv:2305.15021v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2305.15021](http://arxiv.org/abs/2305.15021)

    EmbodiedGPT是一种端到端的多模态基础模型，通过思维链预训练的方式，赋予具有多模态理解和执行能力的实体代理人。

    

    赋予具有多模态理解和执行能力的实体代理人，我们介绍了一种端到端的多模态基础模型EmbodiedGPT，用于实体智能。为了实现这一目标，我们做出了以下努力：（i）我们创建了一个大规模的嵌入式规划数据集，命名为EgoCOT，该数据集由Ego4D数据集中精选的视频和相应的高质量语言指令组成。具体而言，我们使用“思维链”模式生成一系列子目标，以实现有效的嵌入式规划。（ii）我们推出了一种高质量计划生成的高效训练方法，通过前缀调优将7B大型语言模型（LLM）调整到EgoCOT数据集上。（iii）我们介绍了一种从LLM生成的计划中提取与任务相关特征的范例方法。

    Embodied AI is a crucial frontier in robotics, capable of planning and executing action sequences for robots to accomplish long-horizon tasks in physical environments. In this work, we introduce EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI, empowering embodied agents with multi-modal understanding and execution capabilities. To achieve this, we have made the following efforts: (i) We craft a large-scale embodied planning dataset, termed EgoCOT. The dataset consists of carefully selected videos from the Ego4D dataset, along with corresponding high-quality language instructions. Specifically, we generate a sequence of sub-goals with the "Chain of Thoughts" mode for effective embodied planning. (ii) We introduce an efficient training approach to EmbodiedGPT for high-quality plan generation, by adapting a 7B large language model (LLM) to the EgoCOT dataset via prefix tuning. (iii) We introduce a paradigm for extracting task-related features from LLM-generated pla
    
[^127]: SpikeCP: 通过极限预测实现延迟自适应可靠脉冲神经网络

    SpikeCP: Delay-Adaptive Reliable Spiking Neural Networks via Conformal Prediction. (arXiv:2305.11322v1 [cs.NE])

    [http://arxiv.org/abs/2305.11322](http://arxiv.org/abs/2305.11322)

    这篇论文提出了一种新的脉冲神经网络模型，能够通过极限预测实现自适应的推断延迟，从而节约能源与提高可靠性。

    

    脉冲神经网络（SNN）通过内部事件驱动的神经动态处理时间序列数据，其能量消耗取决于输入演示期间神经元之间交换的脉冲数量。在典型的SNN分类器实现中，决策是在整个输入序列被处理后产生的，导致延迟和能量消耗水平在输入之间是相对均匀的。最近引入的延迟自适应SNN可根据每个示例的难度来定制推断延迟 - 以及随之而来的能耗 - 通过在SNN模型足够“自信”时产生早期决策来实现。

    Spiking neural networks (SNNs) process time-series data via internal event-driven neural dynamics whose energy consumption depends on the number of spikes exchanged between neurons over the course of the input presentation. In typical implementations of an SNN classifier, decisions are produced after the entire input sequence has been processed, resulting in latency and energy consumption levels that are fairly uniform across inputs. Recently introduced delay-adaptive SNNs tailor the inference latency -- and, with it, the energy consumption -- to the difficulty of each example, by producing an early decision when the SNN model is sufficiently ``confident''. In this paper, we start by observing that, as an SNN processes input samples, its classification decisions tend to be first under-confident and then over-confident with respect to the decision's ground-truth, unknown, test accuracy. This makes it difficult to determine a stopping time that ensures a desired level of accuracy. To add
    
[^128]: 深度时空聚类：多维气候数据的临时聚类方法

    Deep Spatiotemporal Clustering: A Temporal Clustering Approach for Multi-dimensional Climate Data. (arXiv:2304.14541v1 [cs.LG])

    [http://arxiv.org/abs/2304.14541](http://arxiv.org/abs/2304.14541)

    该论文提出了一种新颖的算法Deep Spatiotemporal Clustering (DSC) ，用于使用无监督深度学习方法进行高维时空数据的时间聚类，DSC利用自动编码器集成CNN-RNN层学习时空数据的潜在表示，并优化聚类损失和数据重建损失以改善聚类分配和非线性重建能力。

    

    使用无监督方法对高维时空数据进行聚类是许多数据驱动应用程序中的一个具有挑战性的问题。现有的无监督聚类的最先进方法使用不同的相似性和距离函数，但集中于数据的空间或时间特征。我们集中于使用空间和时间特征的联合深度表示学习，提出了Deep Spatiotemporal Clustering (DSC)，这是一种新颖的算法，用于使用无监督深度学习方法进行高维时空数据的时间聚类。受U-net架构启发，DSC利用自动编码器集成CNN-RNN层学习时空数据的潜在表示。 DSC还包括一个独特的潜在表示层，用于使用学生t分布进行聚类分配。通过同时优化聚类损失和数据重建损失，该算法逐渐改善聚类分配和非线性重建能力。

    Clustering high-dimensional spatiotemporal data using an unsupervised approach is a challenging problem for many data-driven applications. Existing state-of-the-art methods for unsupervised clustering use different similarity and distance functions but focus on either spatial or temporal features of the data. Concentrating on joint deep representation learning of spatial and temporal features, we propose Deep Spatiotemporal Clustering (DSC), a novel algorithm for the temporal clustering of high-dimensional spatiotemporal data using an unsupervised deep learning method. Inspired by the U-net architecture, DSC utilizes an autoencoder integrating CNN-RNN layers to learn latent representations of the spatiotemporal data. DSC also includes a unique layer for cluster assignment on latent representations that uses the Student's t-distribution. By optimizing the clustering loss and data reconstruction loss simultaneously, the algorithm gradually improves clustering assignments and the nonlinea
    
[^129]: TempEE：基于时空平行Transformer实现雷达回波外推

    TempEE: Temporal-Spatial Parallel Transformer for Radar Echo Extrapolation Beyond Auto-Regression. (arXiv:2304.14131v1 [eess.SP])

    [http://arxiv.org/abs/2304.14131](http://arxiv.org/abs/2304.14131)

    本文提出了一个新型的雷达回波外推算法TempEE，该算法利用时空相关特征和Transformer技术，通过从多帧回波图像中提取特征，准确地表示了降水的非平稳运动过程，克服了传统算法的局限性，在三个真实数据集上表现出色。

    

    气象雷达反射率数据（也称为回波）在预测降水和进行短期强降雨的精确快速预测方面发挥着至关重要的作用。与传统模型相比，基于深度学习的雷达回波外推算法更加有效和高效。然而，高度可靠且具有广泛适用性的算法的发展受到三个主要瓶颈的制约：累积误差扩散、稀疏回波分布的不精确表示以及非平稳运动过程的不准确描述。为了解决这些问题，本文提出了一种利用时空相关特征和Transformer技术的新型雷达回波外推算法。该算法从多帧回波图像中提取特征，准确地表示了降水的非平稳运动过程。所提出的算法使用一种新的并行Temporal-Spatial Parallel Transformer（TempEE），确保了在雷达回波外推中的高准确性和高效率。此外，该算法克服了传统NWP模型和自回归算法的局限性，能够准确地在回波持续时间之外进行外推。在三个真实数据集上，所提出的算法优于现有最先进的方法，展示了其实际应用的潜力。

    The meteorological radar reflectivity data, also known as echo, plays a crucial role in predicting precipitation and enabling accurate and fast forecasting of short-term heavy rainfall without the need for complex Numerical Weather Prediction (NWP) model. Compared to conventional model, Deep Learning (DL)-based radar echo extrapolation algorithms are more effective and efficient. However, the development of highly reliable and generalized algorithms is hindered by three main bottlenecks: cumulative error spreading, imprecise representation of sparse echo distribution, and inaccurate description of non-stationary motion process. To address these issues, this paper presents a novel radar echo extrapolation algorithm that utilizes temporal-spatial correlation features and the Transformer technology. The algorithm extracts features from multi-frame echo images that accurately represent non-stationary motion processes for precipitation prediction. The proposed algorithm uses a novel paralle
    
[^130]: 基于激光注入的参数攻击对嵌入式神经网络的评估

    Evaluation of Parameter-based Attacks against Embedded Neural Networks with Laser Injection. (arXiv:2304.12876v1 [cs.CR])

    [http://arxiv.org/abs/2304.12876](http://arxiv.org/abs/2304.12876)

    本研究首次成功地在32位Cortex-M微控制器上使用激光注入进行了比特翻转攻击，强调了典型深度神经网络的缺乏强健性。

    

    机器学习系统安全认证行动的到来，加上模型在多个硬件平台上的大规模部署，引起了极大的评估挑战。最近，大多数研究工作集中在基于API的攻击上，将机器学习模型视为纯算法抽象。然而，新的基于实现的威胁已被揭示，强调提出实践和基于仿真的方法，以适当评估模型的强健性。一个主要关注点是基于参数的攻击（例如比特翻转攻击），当内部存储器中的参数被精确和最优地更改时，强调了典型深度神经网络模型的缺乏强健性。针对安全测试目的，本研究首次在32位Cortex-M微控制器上使用激光故障注入，实际报告了成功的BFA变体。

    Upcoming certification actions related to the security of machine learning (ML) based systems raise major evaluation challenges that are amplified by the large-scale deployment of models in many hardware platforms. Until recently, most of research works focused on API-based attacks that consider a ML model as a pure algorithmic abstraction. However, new implementation-based threats have been revealed, emphasizing the urgency to propose both practical and simulation-based methods to properly evaluate the robustness of models. A major concern is parameter-based attacks (such as the Bit-Flip Attack, BFA) that highlight the lack of robustness of typical deep neural network models when confronted by accurate and optimal alterations of their internal parameters stored in memory. Setting in a security testing purpose, this work practically reports, for the first time, a successful variant of the BFA on a 32-bit Cortex-M microcontroller using laser fault injection. It is a standard fault injec
    
[^131]: 对比调节: 帮助遗忘掩码自编码器的一点小帮助

    Contrastive Tuning: A Little Help to Make Masked Autoencoders Forget. (arXiv:2304.10520v1 [cs.CV])

    [http://arxiv.org/abs/2304.10520](http://arxiv.org/abs/2304.10520)

    本文提出了一种新方法：掩码自编码器对比调整(MAE-CT)，利用最近邻对比学习（NNCLR）在标记数据较少的情况下实现下游分类，该方法可以将图像的丰富特征聚类成对象的语义聚类。

    

    掩码图像建模方法，如掩码自编码器（MAE），可以有效地学习输入的丰富表示。但是，为了适应下游任务，由于其丰富的特征不仅捕获了对象而且还包括不相关的图像背景，因此它们需要足够数量的标记数据。相比之下，实例辨别方法侧重于对象。在这项工作中，我们研究如何将MIM的效率和可伸缩性与ID的能力相结合，以在缺少大量标记数据的情况下执行下游分类。为此，我们引入了掩码自编码器对比调整（MAE-CT），这是一种顺序方法，可以将最近邻对比学习（NNCLR）应用于预先训练的MAE。MAE-CT调整了丰富的特征，使它们形成对象的语义聚类，而不使用任何标签。应用于大型和巨型Vision Transformer（ViT）模型时，MAE-CT在线性探测，k-均值聚类和半监督少量样本学习方面匹配或超越了在ImageNet上训练的先前的自我监督方法。

    Masked Image Modeling (MIM) methods, like Masked Autoencoders (MAE), efficiently learn a rich representation of the input. However, for adapting to downstream tasks, they require a sufficient amount of labeled data since their rich features capture not only objects but also less relevant image background. In contrast, Instance Discrimination (ID) methods focus on objects. In this work, we study how to combine the efficiency and scalability of MIM with the ability of ID to perform downstream classification in the absence of large amounts of labeled data. To this end, we introduce Masked Autoencoder Contrastive Tuning (MAE-CT), a sequential approach that applies Nearest Neighbor Contrastive Learning (NNCLR) to a pre-trained MAE. MAE-CT tunes the rich features such that they form semantic clusters of objects without using any labels. Applied to large and huge Vision Transformer (ViT) models, MAE-CT matches or excels previous self-supervised methods trained on ImageNet in linear probing, k
    
[^132]: 大规模语言模型中潜在空间理论对应新兴能力

    A Latent Space Theory for Emergent Abilities in Large Language Models. (arXiv:2304.09960v1 [cs.CL])

    [http://arxiv.org/abs/2304.09960](http://arxiv.org/abs/2304.09960)

    本文探讨了大规模语言模型中的贝叶斯推断和稀疏联合分布，证明了LLMs能够完成语言理解、上下文学习、思路启发以及有效指令微调的新兴能力。

    

    语言并不是随机生成，而是为了传递信息。语言与其底层含义之间存在强烈的关联，在其相关性方面有着严重偏差的稀疏联合分布。此外，由于稀疏性，这些高峰值恰好与语言的边缘分布匹配。随着大数据和大模型上训练的LLMs的出现，我们现在可以精确评估语言的边缘分布，这提供了一种方便的探索联合分布稀疏结构实现有效推理的方式。在本文中，我们将语言分类为明确与{\epsilon}-模糊，并提出定量结果，以表明LLMs的新兴能力（例如语言理解、上下文学习、思路启发以及有效指令微调）都可以归因于对稀疏联合分布进行贝叶斯推断。

    Languages are not created randomly but rather to communicate information. There is a strong association between languages and their underlying meanings, resulting in a sparse joint distribution that is heavily peaked according to their correlations. Moreover, these peak values happen to match with the marginal distribution of languages due to the sparsity. With the advent of LLMs trained on big data and large models, we can now precisely assess the marginal distribution of languages, providing a convenient means of exploring the sparse structures in the joint distribution for effective inferences. In this paper, we categorize languages as either unambiguous or {\epsilon}-ambiguous and present quantitative results to demonstrate that the emergent abilities of LLMs, such as language understanding, in-context learning, chain-of-thought prompting, and effective instruction fine-tuning, can all be attributed to Bayesian inference on the sparse joint distribution of languages.
    
[^133]: 权重连体网络用于从MRI图像预测阿尔茨海默病发病时间

    Weighted Siamese Network to Predict the Time to Onset of Alzheimer's Disease from MRI Images. (arXiv:2304.07097v1 [eess.IV])

    [http://arxiv.org/abs/2304.07097](http://arxiv.org/abs/2304.07097)

    本文提出使用加权连体网络对进展性MCI患者进行序数分类，以预测他们距离严重AD阶段的距离，从而实现更准确的早期检测。

    

    阿尔茨海默病（AD）是痴呆症最常见的原因，是一种进行性疾病，会先出现轻度认知障碍（MCI）。早期检测对于做出治疗决策至关重要。然而，大多数关于计算机辅助检测AD的文献聚焦于将脑图像分类为三个主要类别之一：健康、MCI和AD；或将MCI患者分类为(1)进展：在给定研究期内的未来检查时间进展到AD的患者，以及(2)稳定：持续作为MCI患者而从未进展为AD。这种方法错过了准确识别进展性MCI患者轨迹的机会。本文重新审视了AD识别的脑图像分类任务，将其重新构建为一个序数分类任务，以预测患者距离严重AD阶段有多近。为此，我们从阿尔茨海默病神经影像计划（ADNI）数据集中选择进展性MCI患者，并使用加权连体网络进行实验。

    Alzheimer's Disease (AD), which is the most common cause of dementia, is a progressive disease preceded by Mild Cognitive Impairment (MCI). Early detection of the disease is crucial for making treatment decisions. However, most of the literature on computer-assisted detection of AD focuses on classifying brain images into one of three major categories: healthy, MCI, and AD; or categorising MCI patients into one of (1) progressive: those who progress from MCI to AD at a future examination time during a given study period, and (2) stable: those who stay as MCI and never progress to AD. This misses the opportunity to accurately identify the trajectory of progressive MCI patients. In this paper, we revisit the brain image classification task for AD identification and re-frame it as an ordinal classification task to predict how close a patient is to the severe AD stage. To this end, we select progressive MCI patients from the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset and co
    
[^134]: 针对Jumbo-Visma队卡路里预测的符合性回归

    Conformal Regression in Calorie Prediction for Team Jumbo-Visma. (arXiv:2304.03778v1 [cs.LG])

    [http://arxiv.org/abs/2304.03778](http://arxiv.org/abs/2304.03778)

    本文提出一种新的符合性回归方法，通过预测动力和速度来为自行车比赛中的每个骑手提供卡路里需求的估计。

    

    UCI WorldTour赛事是男子精英公路自行车比赛的顶级赛事，对骑行者的体能和耐力进行考验。Jumbo-Visma队的教练们长期以来一直负责预测每个比赛日历中荷兰队的每位骑手的能量需求，以确保骑手在比赛过程中有足够的能量和资源来保持高水平表现。本文提出了一种新的更有效的预测骑行比赛能量需求的方法。通过回归模型预测速度和动力，为每个阶段的每个个体骑手提供卡路里需求估计。

    UCI WorldTour races, the premier men's elite road cycling tour, are grueling events that put riders' physical fitness and endurance to the test. The coaches of Team Jumbo-Visma have long been responsible for predicting the energy needs of each rider of the Dutch team for every race on the calendar. Those must be estimated to ensure riders have the energy and resources necessary to maintain a high level of performance throughout a race. This task, however, is both time-consuming and challenging, as it requires precise estimates of race speed and power output. Traditionally, the approach to predicting energy needs has relied on coaches' judgement and experience, but this method has its limitations and often leads to inaccurate predictions. In this paper, we propose a new, more effective approach to predicting energy needs for cycling races. By predicting the speed and power with regression models, we provide the coaches with calorie needs estimate for each individual rider per stage inst
    
[^135]: 使用知识蒸馏增强作物分割的域泛化

    Domain Generalization for Crop Segmentation with Knowledge Distillation. (arXiv:2304.01029v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2304.01029](http://arxiv.org/abs/2304.01029)

    本文针对作物分割问题提出了一种使用知识蒸馏的方法来增强域泛化能力，通过将来自源域的模型集合的知识传递给学生模型，实现了对新的作物和环境条件的泛化处理。

    

    近年来，精确农业逐渐朝着自动化的方向发展，以支持与田间管理相关的所有活动。服务机器人在这一演变中发挥重要作用，通过部署能够在田间导航并在无需人为干预的情况下执行任务的自主代理，例如监测、喷洒和收获。为了执行这些精确的动作，移动机器人需要一个实时感知系统，能够理解周围环境并在野外识别目标。对于实际应用来说，对新的作物和环境条件进行泛化是至关重要的，因为很少有标记样本可用。在本文中，我们研究了作物分割问题，并提出了一种使用知识蒸馏增强域泛化的新方法。在所提出的框架中，我们将来自源域上单独训练的模型集合的知识传递给一个可以适应未见目标域的学生模型。

    In recent years, precision agriculture has gradually oriented farming closer to automation processes to support all the activities related to field management. Service robotics plays a predominant role in this evolution by deploying autonomous agents that can navigate fields while performing tasks without human intervention, such as monitoring, spraying, and harvesting. To execute these precise actions, mobile robots need a real-time perception system that understands their surroundings and identifies their targets in the wild. Generalizing to new crops and environmental conditions is critical for practical applications, as labeled samples are rarely available. In this paper, we investigate the problem of crop segmentation and propose a novel approach to enhance domain generalization using knowledge distillation. In the proposed framework, we transfer knowledge from an ensemble of models individually trained on source domains to a student model that can adapt to unseen target domains. 
    
[^136]: 任意深度的一维神经网络的不动点

    Fixed points of arbitrarily deep 1-dimensional neural networks. (arXiv:2303.12814v1 [stat.ML])

    [http://arxiv.org/abs/2303.12814](http://arxiv.org/abs/2303.12814)

    本研究发现，具有对数S型激活函数的任意深度的一维神经网络最多只有三个不动点，为深度神经网络的应用和理论之间构建了一个必要的桥梁。

    

    本文介绍了一个在$\mathbb{R}$上具有合成性且包含对数S型函数的新函数类。我们使用这个类来证明具有对数S型激活函数的任意深度的一维神经网络最多只有三个不动点。虽然这样的神经网络远离实际应用，但我们能够完全理解它们的不动点，并为深度神经网络的应用和理论之间构建了一个必要的桥梁。

    In this paper, we introduce a new class of functions on $\mathbb{R}$ that is closed under composition, and contains the logistic sigmoid function. We use this class to show that any 1-dimensional neural network of arbitrary depth with logistic sigmoid activation functions has at most three fixed points. While such neural networks are far from real world applications, we are able to completely understand their fixed points, providing a foundation to the much needed connection between application and theory of deep neural networks.
    
[^137]: MAHTM：分层可交易微电网的多智能体框架

    MAHTM: A Multi-Agent Framework for Hierarchical Transactive Microgrids. (arXiv:2303.08447v1 [cs.LG])

    [http://arxiv.org/abs/2303.08447](http://arxiv.org/abs/2303.08447)

    该论文提出了一个多智能体强化学习框架，用于管理微电网中的能源交易。该框架通过最小化碳足迹，同时平衡可再生能源和传统能源的消费和生产，并考虑能源变化。

    

    将可变的可再生能源并入电网给系统运营商带来挑战，使得在能源可用性、成本承受能力和污染可控性之间取得最优折衷成为一项难题。本文提出了一种管理微电网能源交易的多智能体强化学习框架。该框架解决了上述挑战：旨在通过最小化碳足迹的方式优化可用资源的利用，并使所有利益相关方受益。所提出的架构由三层代理组成，每层代理追求不同的目标。第一层由生产者和消费者组成，旨在最小化总能源成本。其他两层控制能源价格，以减少碳足迹，同时平衡可再生能源和传统能源的消费和生产。该框架还考虑了能源需求和供应的波动。

    Integrating variable renewable energy into the grid has posed challenges to system operators in achieving optimal trade-offs among energy availability, cost affordability, and pollution controllability. This paper proposes a multi-agent reinforcement learning framework for managing energy transactions in microgrids. The framework addresses the challenges above: it seeks to optimize the usage of available resources by minimizing the carbon footprint while benefiting all stakeholders. The proposed architecture consists of three layers of agents, each pursuing different objectives. The first layer, comprised of prosumers and consumers, minimizes the total energy cost. The other two layers control the energy price to decrease the carbon impact while balancing the consumption and production of both renewable and conventional energy. This framework also takes into account fluctuations in energy demand and supply.
    
[^138]: Kernel Conditional Moment Constraints for Confounding Robust Inference. (arXiv:2302.13348v2 [stat.ML] UPDATED)

    Kernel Conditional Moment Constraints for Confounding Robust Inference. (arXiv:2302.13348v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.13348](http://arxiv.org/abs/2302.13348)

    本文提出了一种应对混淆因素的核条件矩约束方法，通过利用核方法得到的条件矩约束的近似估计器，实现了对政策评估的尖锐下界估计，并能对经典边际敏感度模型进行新颖扩展。

    

    我们研究离线情境推断中未观察到的混淆因素对政策评估的影响。敏感性分析方法通常用于在给定不确定性集上估计最坏混淆情况下的政策价值。然而，现有的工作往往为了可行性而采用一些粗略的不确定性集的放松，导致对政策价值的过度保守估计。在本文中，我们提出了一种通用的估计器，它提供了政策价值的尖锐下界。可以证明我们的估计器包含了Dorn和Guo（2022年）最近提出的尖锐估计器作为一个特例，而我们的方法则利用f-散度来实现对经典边际敏感度模型的新颖扩展。为了构建我们的估计器，我们利用核方法得到了条件矩约束的可行近似，而传统的非尖锐估计器则未能考虑到这一点。在理论分析中，我们提供了一个条件，用于降低传统非尖锐估计器受混淆因素影响的程度。

    We study policy evaluation of offline contextual bandits subject to unobserved confounders. Sensitivity analysis methods are commonly used to estimate the policy value under the worst-case confounding over a given uncertainty set. However, existing work often resorts to some coarse relaxation of the uncertainty set for the sake of tractability, leading to overly conservative estimation of the policy value. In this paper, we propose a general estimator that provides a sharp lower bound of the policy value. It can be shown that our estimator contains the recently proposed sharp estimator by Dorn and Guo (2022) as a special case, and our method enables a novel extension of the classical marginal sensitivity model using f-divergence. To construct our estimator, we leverage the kernel method to obtain a tractable approximation to the conditional moment constraints, which traditional non-sharp estimators failed to take into account. In the theoretical analysis, we provide a condition for the
    
[^139]: 基于深度强化学习的启发式方法在自旋玻璃基态问题上的测试：更广泛的视角

    Deep reinforced learning heuristic tested on spin-glass ground states: The larger picture. (arXiv:2302.10848v2 [cond-mat.dis-nn] UPDATED)

    [http://arxiv.org/abs/2302.10848](http://arxiv.org/abs/2302.10848)

    该研究提出了一种基于深度强化学习的启发式方法，用于增强组合优化过程，并在自旋玻璃基态问题上取得了改进结果。研究结果表明，相对于传统方法如模拟退火或并行退火，强化学习方法在提供相当质量的结果之前减少了运行时间。

    

    在Changjun Fan等人的研究中，作者们提出了一种基于深度强化学习的方法来增强组合优化启发式方法。具体而言，他们针对几个自旋玻璃基态问题展示了结果，其中非平面网络上的实例通常是NP困难的，与几种基于蒙特卡洛的方法进行比较，如模拟退火（SA）或并行退火（PT）。事实上，这些结果表明，强化学习相对于SA或PT改进了结果，或者至少在获得与其他方法相当质量的结果之前，减少了启发式方法的运行时间。为了证明他们的方法“优越”，作者采取了两种基本策略：（1）调用商业GUROBI求解器获取一些精确基态样本作为测试基准进行比较，以及（2）将他们的方法与其他策略进行了直接对比。

    In Changjun Fan et al. [Nature Communications https://doi.org/10.1038/s41467-023-36363-w (2023)], the authors present a deep reinforced learning approach to augment combinatorial optimization heuristics. In particular, they present results for several spin glass ground state problems, for which instances on non-planar networks are generally NP-hard, in comparison with several Monte Carlo based methods, such as simulated annealing (SA) or parallel tempering (PT). Indeed, those results demonstrate that the reinforced learning improves the results over those obtained with SA or PT, or at least allows for reduced runtimes for the heuristics before results of comparable quality have been obtained relative to those other methods. To facilitate the conclusion that their method is ''superior'', the authors pursue two basic strategies: (1) A commercial GUROBI solver is called on to procure a sample of exact ground states as a testbed to compare with, and (2) a head-to-head comparison between th
    
[^140]: 基于随机先验网络的高维输出可扩展贝叶斯优化

    Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks. (arXiv:2302.07260v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07260](http://arxiv.org/abs/2302.07260)

    本文提出了一个基于带随机先验的神经网络的深度学习框架用于高维输出的贝叶斯优化，可有效地处理全局优化问题，即使在高维度向量空间或无限维函数空间中也能近似功能关系。

    

    科学和工程中的一些基本问题涉及到未知的高维度映射一组可控变量到昂贵实验结果的黑盒函数的全局优化任务。贝叶斯优化（BO）技术已被证明在使用相对较少的目标函数评估时处理全局优化问题时非常有效，但当处理高维输出时，其性能受到影响。为克服维度主要挑战，本文提出了一个基于带随机先验的神经网络的自举集成的BO和序贯决策制定的深度学习框架。使用适当的体系结构选择，我们证明了所提出的框架可以近似设计变量和感兴趣量之间的功能关系，即使在后者取值于高维向量空间或甚至无限维函数空间的情况下。在贝叶斯优化的背景下，该方法允许高效和可扩展的处理高维度黑盒函数的全局优化。

    Several fundamental problems in science and engineering consist of global optimization tasks involving unknown high-dimensional (black-box) functions that map a set of controllable variables to the outcomes of an expensive experiment. Bayesian Optimization (BO) techniques are known to be effective in tackling global optimization problems using a relatively small number objective function evaluations, but their performance suffers when dealing with high-dimensional outputs. To overcome the major challenge of dimensionality, here we propose a deep learning framework for BO and sequential decision making based on bootstrapped ensembles of neural architectures with randomized priors. Using appropriate architecture choices, we show that the proposed framework can approximate functional relationships between design variables and quantities of interest, even in cases where the latter take values in high-dimensional vector spaces or even infinite-dimensional function spaces. In the context of 
    
[^141]: 关于梯度下降动态和深度学习不稳定性的连续时间模型研究

    On a continuous time model of gradient descent dynamics and instability in deep learning. (arXiv:2302.01952v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.01952](http://arxiv.org/abs/2302.01952)

    本文提出了一个连续时间流模型——主要流（PF），通过对Hessian矩阵特征分解的依赖，捕捉到了梯度下降中的发散和振荡行为、逃逸局部极小值和鞍点的连续性流，并解释了深度学习中的稳定边缘现象。通过对不稳定性的新理解，提出了一种学习率适应方法，可以控制训练稳定性和测试集评估性能之间的权衡。

    

    深度学习成功的秘诀在于神经网络和基于梯度的优化的结合。然而，理解梯度下降的行为，特别是其不稳定性，落后于其经验成功。为了增加研究梯度下降的理论工具，我们提出了主要流（PF），一种近似梯度下降动态的连续时间流。据我们所知，PF是唯一捕捉到梯度下降的发散和振荡行为，包括逃逸局部极小值和鞍点的连续性流。通过其对于Hessian特征分解的依赖，PF解释了深度学习中最近观察到的稳定边缘现象。通过对不稳定性的新理解，我们提出了一种学习率适应方法，使我们能够控制训练稳定性和测试集评估性能之间的权衡。

    The recipe behind the success of deep learning has been the combination of neural networks and gradient-based optimization. Understanding the behavior of gradient descent however, and particularly its instability, has lagged behind its empirical success. To add to the theoretical tools available to study gradient descent we propose the principal flow (PF), a continuous time flow that approximates gradient descent dynamics. To our knowledge, the PF is the only continuous flow that captures the divergent and oscillatory behaviors of gradient descent, including escaping local minima and saddle points. Through its dependence on the eigendecomposition of the Hessian the PF sheds light on the recently observed edge of stability phenomena in deep learning. Using our new understanding of instability we propose a learning rate adaptation method which enables us to control the trade-off between training stability and test set evaluation performance.
    
[^142]: 使用语言模型提示进行推理：一项调查

    Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09597](http://arxiv.org/abs/2212.09597)

    本文提供了使用语言模型提示进行推理的前沿研究综合调查。讨论了新兴推理能力出现的潜在原因，并提供系统资源帮助初学者。

    

    推理作为复杂问题解决的重要能力，可以为医疗诊断、谈判等各种实际应用提供后端支持。本文对使用语言模型提示进行推理的前沿研究进行了综合调查。我们介绍了研究成果的比较和总结，并提供了系统资源以帮助初学者。我们还讨论了新兴推理能力出现的潜在原因，并突出了未来的研究方向。资源可在 https://github.com/zjunlp/Prompt4ReasoningPapers 上获取（定期更新）。

    Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
    
[^143]: 通过预基调过滤消除basecalling中的浪费计算的TargetCall

    TargetCall: Eliminating the Wasted Computation in Basecalling via Pre-Basecalling Filtering. (arXiv:2212.04953v2 [q-bio.GN] UPDATED)

    [http://arxiv.org/abs/2212.04953](http://arxiv.org/abs/2212.04953)

    TargetCall通过预基调过滤，消除了basecalling中的浪费计算，提高了基因组分析流程的效率。

    

    Basecalling是纳米孔测序分析中的重要步骤，它将纳米孔测序仪的原始信号转换为核酸序列，即reads。最先进的basecallers使用复杂的深度学习模型实现高度的basecalling准确性。这使得basecalling在计算上效率低下且内存消耗大，成为整个基因组分析流程的瓶颈。然而，对于许多应用来说，大多数reads与感兴趣的参考基因组不匹配（即目标参考基因组），因此会在后续的基因组流程步骤中被丢弃，浪费了basecalling的计算。为了解决这个问题，我们提出了TargetCall，这是第一个用于消除basecalling中浪费计算的预基调过滤器。TargetCall的关键思想是在basecalling之前丢弃不会与目标参考基因组匹配的reads（即非目标reads）。TargetCall由两个主要组件组成：（1）LightCall，一个轻量级的神经网络basecaller，产生噪声reads；

    Basecalling is an essential step in nanopore sequencing analysis where the raw signals of nanopore sequencers are converted into nucleotide sequences, i.e., reads. State-of-the-art basecallers employ complex deep learning models to achieve high basecalling accuracy. This makes basecalling computationally-inefficient and memory-hungry; bottlenecking the entire genome analysis pipeline. However, for many applications, the majority of reads do no match the reference genome of interest (i.e., target reference) and thus are discarded in later steps in the genomics pipeline, wasting the basecalling computation. To overcome this issue, we propose TargetCall, the first pre-basecalling filter to eliminate the wasted computation in basecalling. TargetCall's key idea is to discard reads that will not match the target reference (i.e., off-target reads) prior to basecalling. TargetCall consists of two main components: (1) LightCall, a lightweight neural network basecaller that produces noisy reads;
    
[^144]: 奖励并非必要：如何为终身学习创建一个组合性自我保护智能体

    Reward is not Necessary: How to Create a Compositional Self-Preserving Agent for Life-Long Learning. (arXiv:2211.10851v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.10851](http://arxiv.org/abs/2211.10851)

    这项研究表明，我们可以使用内在动机衡量标准而不依赖于奖励来创建一个具有自我保护能力的智能体。

    

    强化学习认为最大化奖励和避免惩罚是解释目标导向行为的核心。然而，在一生中，生物需要学习关于世界结构的许多不同方面：世界状态和状态转移动力学。随着智能体融入新知识，状态组合的数量以指数级增长，并且对于给定的状态组合，没有明显定义的预设奖励或成本的加权组合，因为这样的加权需要在智能体在世界中的经验之前对好的和坏的组合进行编码。因此，我们必须在大状态空间中开发更自然的行为和动机模型。我们展示了仅使用内在动机衡量标准（即赋予能力）是可能的，该标准衡量智能体在转移操作者下实现许多可能未来的能力。我们建议将赋予能力扩展到分层状态空间中。

    Reinforcement Learning views the maximization of rewards and avoidance of punishments as central to explaining goal-directed behavior. However, over a life, organisms will need to learn about many different aspects of the world's structure: the states of the world and state-vector transition dynamics. The number of combinations of states grows exponentially as an agent incorporates new knowledge, and there is no obvious weighted combination of pre-existing rewards or costs defined for a given combination of states, as such a weighting would need to encode information about good and bad combinations prior to an agent's experience in the world. Therefore, we must develop more naturalistic accounts of behavior and motivation in large state-spaces. We show that it is possible to use only the intrinsic motivation metric of empowerment, which measures the agent's capacity to realize many possible futures under a transition operator. We propose to scale empowerment to hierarchical state-space
    
[^145]: 有效稀疏推理用于条件GAN和扩散模型

    Efficient Spatially Sparse Inference for Conditional GANs and Diffusion Models. (arXiv:2211.02048v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.02048](http://arxiv.org/abs/2211.02048)

    提出了空间稀疏推理（SSI）的通用技术，该技术选择性地为编辑区域执行计算并加速各种生成模型，包括条件GAN和扩散模型。通过缓存和重复使用原始图像的特征图，我们将卷积滤波器稀疏地应用于编辑区域，并在未编辑的区域中重复使用缓存特征，从而通过约$1\%$的区域编辑来减少计算资源的浪费。

    

    在图像编辑中，现有的深度生成模型往往会从头开始重新合成整个输出，包括未编辑的区域。这导致计算资源的浪费，尤其是对于较小的编辑操作。在这项工作中，我们提出了空间稀疏推理（SSI）的通用技术，该技术选择性地为编辑区域执行计算并加速各种生成模型，包括条件GAN和扩散模型。我们的关键观察是用户倾向于逐渐改变输入图像，这激发了我们缓存和重复使用原始图像的特征图的想法。给定编辑过的图像，我们稀疏地将卷积滤波器应用于编辑区域，同时重复使用未编辑区域的缓存特征。基于我们的算法，我们进一步提出了稀疏渐进式生成引擎（SIGE）来将计算减少转化为在现成硬件上的延迟减少。我们的方法通过约$1\%$的区域编辑，减少了计算资源的浪费。

    During image editing, existing deep generative models tend to re-synthesize the entire output from scratch, including the unedited regions. This leads to a significant waste of computation, especially for minor editing operations. In this work, we present Spatially Sparse Inference (SSI), a general-purpose technique that selectively performs computation for edited regions and accelerates various generative models, including both conditional GANs and diffusion models. Our key observation is that users tend to gradually change the input image. This motivates us to cache and reuse the feature maps of the original image. Given an edited image, we sparsely apply the convolutional filters to the edited regions while reusing the cached features for the unedited areas. Based on our algorithm, we further propose Sparse Incremental Generative Engine (SIGE) to convert the computation reduction to latency reduction on off-the-shelf hardware. With about $1\%$-area edits, our method reduces the comp
    
[^146]: ConSpec: 突出强化学习中的关键步骤，实现快速学习和泛化

    ConSpec: honing in on critical steps for rapid learning and generalization in RL. (arXiv:2210.05845v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.05845](http://arxiv.org/abs/2210.05845)

    ConSpec是一个新的强化学习算法，通过离线对比学习来确定任务中的关键步骤，实现快速学习和泛化。该算法通过学习关键步骤的原型，并在当前状态匹配时提供内在奖励，具有快速识别关键步骤和可解释的信用分配的优势。

    

    在现实生活中，成功往往取决于多个关键步骤，这些步骤在时间上相距较远，与最终奖励也相距甚远。传统的强化学习方法在信用分配方面依赖Bellman方程，很难识别这些关键步骤。本文提出了一种新的强化学习算法，使用离线对比学习来确定关键步骤。这个算法被称为对比内省（ConSpec），可以添加到任何现有的强化学习算法中。ConSpec通过一种新颖的对比损失学习任务中的关键步骤的原型，并在当前状态与这些原型之一匹配时提供内在奖励。ConSpec中的原型在信用分配方面具有两个关键优势：（1）它们使得能够迅速识别所有关键步骤；（2）它们以容易解释的方式实现这一点，使得在感觉特征改变时可以进行超出分布的泛化。与其他当代的强化学习方法不同，

    In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on critical steps. This algorithm, which we call contrastive introspection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of these prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (1) They enable rapid identification of all the critical steps. (2) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL
    
[^147]: BAFFLE: 离线增强学习中的后门攻击

    BAFFLE: Backdoor Attack in Offline Reinforcement Learning. (arXiv:2210.04688v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.04688](http://arxiv.org/abs/2210.04688)

    本文研究离线增强学习中的后门攻击，通过向数据中添加扰动，使得智能体在注入触发器的观测值上采取低奖励动作，从而提出了BAFFLE方法。

    

    越来越多的研究关注于强化学习（RL）方法，允许智能体通过与环境的交互中收集的试错经验进行学习。最近，离线RL成为一种流行的RL范例，因为它节省了与环境的交互。在离线RL中，数据提供者共享大规模的预先收集的数据集，其他人可以在不与环境交互的情况下训练高质量的智能体。这种范例在机器人控制、自动驾驶等关键任务中表现出有效性。然而，较少关注研究离线RL系统的安全威胁。本文关注后门攻击，其中一些扰动被添加到数据（观测值）中，使得在给定正常观测值的情况下，智能体采取高奖励的动作，在注入触发器的观测值上采取低奖励的动作。在本文中，我们提出了BAFFLE（离线增强学习中的后门攻击），这是一种方法。

    A growing body of research has focused on the Reinforcement Learning (RL) methods which allow the agent to learn from trial-and-error experiences gathered during the interaction with the environment. Recently, offline RL becomes a popular RL paradigm because it saves the interactions with environments. In offline RL, data providers share large pre-collected datasets, and others can train high-quality agents without interacting with the environments. This paradigm has demonstrated effectiveness in critical tasks like robot control, autonomous driving, etc. However, less attention is paid to investigating the security threats to the offline RL system. This paper focuses on backdoor attacks, where some perturbations are added to the data (observations) such that given normal observations, the agent takes high-rewards actions, and low-reward actions on observations injected with triggers. In this paper, we propose Baffle (Backdoor Attack for Offline Reinforcement Learning), an approach tha
    
[^148]: LambdaKG:基于预训练语言模型的知识图谱嵌入库

    LambdaKG: A Library for Pre-trained Language Model-Based Knowledge Graph Embeddings. (arXiv:2210.00305v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.00305](http://arxiv.org/abs/2210.00305)

    LambdaKG是一个基于预训练语言模型的知识图谱嵌入库，提供了多个预训练语言模型和支持多种任务，如知识图谱补全、问答、推荐和知识探索。

    

    知识图谱（KG）通常具有异构的图结构和文本丰富的实体/关系信息。基于文本的KG嵌入可以通过使用预训练语言模型对描述进行编码来表示实体，但目前尚无专门为PLM与KG设计的开源库。本文介绍了LambdaKG，一个带有多个预训练语言模型（如BERT，BART，T5，GPT-3）并支持各种任务（如知识图谱补全，问答，推荐和知识探索）的KGE库。LambdaKG在https://github.com/zjunlp/PromptKG/tree/main/lambdaKG上公开开源，并提供了演示视频和长期维护。

    Knowledge Graphs (KGs) often have two characteristics: heterogeneous graph structure and text-rich entity/relation information. Text-based KG embeddings can represent entities by encoding descriptions with pre-trained language models, but no open-sourced library is specifically designed for KGs with PLMs at present. In this paper, we present LambdaKG, a library for KGE that equips with many pre-trained language models (e.g., BERT, BART, T5, GPT-3), and supports various tasks (e.g., knowledge graph completion, question answering, recommendation, and knowledge probing). LambdaKG is publicly open-sourced at https://github.com/zjunlp/PromptKG/tree/main/lambdaKG, with a demo video at this http URL and long-term maintenance.
    
[^149]: TrojViT: 视觉Transformer中的后门插入

    TrojViT: Trojan Insertion in Vision Transformers. (arXiv:2208.13049v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.13049](http://arxiv.org/abs/2208.13049)

    本文提出了一种针对视觉Transformer的隐秘实用的后门攻击方法TrojViT，通过补丁级触发器将一些易受攻击位组成的参数建立为特洛伊木马。

    

    视觉Transformer（ViT）在各种与视觉相关的任务中展示了最先进的性能。ViTs的成功激励了攻击者对其进行后门攻击。虽然传统的卷积神经网络（CNNs）对后门攻击的脆弱性是众所周知的，但ViTs的后门攻击很少被研究。与通过卷积捕捉像素级局部特征的CNNs相比，ViTs通过补丁和关注机制提取全局上下文信息。将CNN特定的后门攻击天真地移植到ViTs只能产生低干净数据准确性和低攻击成功率。在本文中，我们提出了一个隐秘而实用的ViT特定后门攻击TrojViT。TrojViT生成一个补丁级的触发器，旨在通过补丁显著性排序和关注目标损失，在DRAM存储器中的一些易受攻击位组成的ViT的参数上建立特洛伊木马。 TrojViT还使用最小化调整参数的方法。

    Vision Transformers (ViTs) have demonstrated the state-of-the-art performance in various vision-related tasks. The success of ViTs motivates adversaries to perform backdoor attacks on ViTs. Although the vulnerability of traditional CNNs to backdoor attacks is well-known, backdoor attacks on ViTs are seldom-studied. Compared to CNNs capturing pixel-wise local features by convolutions, ViTs extract global context information through patches and attentions. Na\"ively transplanting CNN-specific backdoor attacks to ViTs yields only a low clean data accuracy and a low attack success rate. In this paper, we propose a stealth and practical ViT-specific backdoor attack $TrojViT$. Rather than an area-wise trigger used by CNN-specific backdoor attacks, TrojViT generates a patch-wise trigger designed to build a Trojan composed of some vulnerable bits on the parameters of a ViT stored in DRAM memory through patch salience ranking and attention-target loss. TrojViT further uses minimum-tuned paramet
    
[^150]: 高斯过程代理模型用于神经网络

    Gaussian Process Surrogate Models for Neural Networks. (arXiv:2208.06028v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.06028](http://arxiv.org/abs/2208.06028)

    通过利用高斯过程构建神经网络的代理模型，我们解决了深度学习系统行为预测和架构算法选择的问题，并展示了模型在捕捉现象、确定关键数据点和预测泛化能力方面的实际应用价值。

    

    无法理解和预测深度学习系统的行为使得很难决定针对给定问题使用什么架构和算法。在科学和工程中，建模是一种用于理解复杂系统的方法论，其内部过程是不透明的。建模用简化的、可解释的替代简化系统。受此启发，我们使用高斯过程构建了一类神经网络的代理模型。我们不是从无限神经网络推导内核，而是从有限神经网络的自然行为中经验性地学习内核。我们展示了我们的方法捕捉到了与神经网络频谱偏差相关的现象，并且展示了我们的代理模型可以用于解决实际问题，如确定哪些点对特定神经网络的行为影响最大，并预测哪些架构和算法对特定数据集具有良好的泛化能力。

    Not being able to understand and predict the behavior of deep learning systems makes it hard to decide what architecture and algorithm to use for a given problem. In science and engineering, modeling is a methodology used to understand complex systems whose internal processes are opaque. Modeling replaces a complex system with a simpler, more interpretable surrogate. Drawing inspiration from this, we construct a class of surrogate models for neural networks using Gaussian processes. Rather than deriving kernels for infinite neural networks, we learn kernels empirically from the naturalistic behavior of finite neural networks. We demonstrate our approach captures existing phenomena related to the spectral bias of neural networks, and then show that our surrogate models can be used to solve practical problems such as identifying which points most influence the behavior of specific neural networks and predicting which architectures and algorithms will generalize well for specific datasets
    
[^151]: 机器学习和计算机视觉技术在蜜蜂监测应用中的应用

    Machine Learning and Computer Vision Techniques in Bee Monitoring Applications. (arXiv:2208.00085v1 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2208.00085](http://arxiv.org/abs/2208.00085)

    本文介绍了机器学习和计算机视觉在蜜蜂监测中的最新应用，展示了自动化蜜蜂计数算法的潜力，并希望能够激发其他科学家的灵感和兴趣。

    

    机器学习和计算机视觉是快速发展的领域，已经证明能够解决非常复杂的任务。它们可以用于监测蜜蜂群体并检查其健康状况，从而在情况变得严重之前，识别出潜在危险状态，或者更好地计划定期蜜蜂群体检查，从而节省重要的成本。本文概述了用于蜜蜂监测的最先进的计算机视觉和机器学习应用，并以自动化蜜蜂计数算法为例展示了这些方法的潜力。本文面向兽医学和蜜蜂学专业人员和专家，旨在向他们介绍机器学习的可能性，因此每个应用类别都以简要的理论介绍和与其基本方法相关的动机开篇。我们希望这篇论文能激发其他科学家的灵感...

    Machine learning and computer vision are dynamically growing fields, which have proven to be able to solve very complex tasks. They could also be used for the monitoring of the honeybee colonies and for the inspection of their health state, which could identify potentially dangerous states before the situation is critical, or to better plan periodic bee colony inspections and therefore save significant costs. In this paper, we present an overview of the state-of-the-art computer vision and machine learning applications used for bee monitoring. We also demonstrate the potential of those methods as an example of an automated bee counter algorithm. The paper is aimed at veterinary and apidology professionals and experts, who might not be familiar with machine learning to introduce to them its possibilities, therefore each family of applications is opened by a brief theoretical introduction and motivation related to its base method. We hope that this paper will inspire other scientists to 
    
[^152]: 适应性联邦相关框架用于时空图学习

    An Adaptive Federated Relevance Framework for Spatial Temporal Graph Learning. (arXiv:2206.03420v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.03420](http://arxiv.org/abs/2206.03420)

    该论文提出了一种适应性联邦相关框架，用于处理时空数据中的复杂预测任务，并解决了同时融合空间信息的相互依赖性和动态的时间变化的挑战。

    

    由于相关应用在许多领域的快速发展，时空数据包含丰富的信息并近年来受到广泛研究。例如，医疗机构经常使用附着在患者不同部位的电极，分析带有空间和时间特征的电生理数据进行健康评估和疾病诊断。现有研究主要使用深度学习技术，如卷积神经网络（CNN）或循环神经网络（RNN），提取隐藏的时空特征。然而，同时融合空间信息的相互依赖性和动态的时间变化是具有挑战性的。实际上，对于一个利用这些时空特征来完成复杂预测任务的模型，通常需要大量的训练数据才能获得令人满意的模型性能。考虑到上述挑战，我们提出了一种适应性联邦相关框架，名为.....

    Spatial-temporal data contains rich information and has been widely studied in recent years due to the rapid development of relevant applications in many fields. For instance, medical institutions often use electrodes attached to different parts of a patient to analyse the electorencephal data rich with spatial and temporal features for health assessment and disease diagnosis. Existing research has mainly used deep learning techniques such as convolutional neural network (CNN) or recurrent neural network (RNN) to extract hidden spatial-temporal features. Yet, it is challenging to incorporate both inter-dependencies spatial information and dynamic temporal changes simultaneously. In reality, for a model that leverages these spatial-temporal features to fulfil complex prediction tasks, it often requires a colossal amount of training data in order to obtain satisfactory model performance. Considering the above-mentioned challenges, we propose an adaptive federated relevance framework, nam
    
[^153]: 物理-不可知对象的元学习再抓取策略

    Meta-Learning Regrasping Strategies for Physical-Agnostic Objects. (arXiv:2205.11110v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2205.11110](http://arxiv.org/abs/2205.11110)

    本研究提出了一种元学习算法，ConDex，用于自主识别具有未知物理属性的非均匀对象，并实现精确的抓取点估计。与现有方法相比，ConDex在性能上表现出优势，并且生成了两个新的对象数据集用于进一步研究。

    

    在真实世界的应用中，抓取非均匀对象仍然是一项具有挑战性的任务，原因是存在未知的物理属性，如质量分布和摩擦系数。在本研究中，我们提出了一种称为ConDex的元学习算法，它将条件神经过程（CNP）与DexNet-2.0相结合，使用深度图像自主识别对象的潜在物理属性。ConDex能够有效地从有限试验中获取物理嵌入，实现精确的抓取点估计。此外，ConDex能够以在线方式迭代地更新预测的抓取质量。据我们所知，我们是第一个生成两个面向非均匀物理属性的对象数据集，包括不同的质量分布和摩擦系数。在仿真环境中进行的广泛评估表明，ConDex在性能上优于DexNet-2.0和现有的基于元学习的抓取流水线。此外，ConDex展示了

    Grasping inhomogeneous objects in real-world applications remains a challenging task due to the unknown physical properties such as mass distribution and coefficient of friction. In this study, we propose a meta-learning algorithm called ConDex, which incorporates Conditional Neural Processes (CNP) with DexNet-2.0 to autonomously discern the underlying physical properties of objects using depth images. ConDex efficiently acquires physical embeddings from limited trials, enabling precise grasping point estimation. Furthermore, ConDex is capable of updating the predicted grasping quality iteratively from new trials in an online fashion. To the best of our knowledge, we are the first who generate two object datasets focusing on inhomogeneous physical properties with varying mass distributions and friction coefficients. Extensive evaluations in simulation demonstrate ConDex's superior performance over DexNet-2.0 and existing meta-learning-based grasping pipelines. Furthermore, ConDex shows
    
[^154]: 无模型学习通过循环集的吸引域

    Model-free Learning of Regions of Attraction via Recurrent Sets. (arXiv:2204.10372v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.10372](http://arxiv.org/abs/2204.10372)

    提出了一种无模型学习渐近稳定平衡点吸引域的方法，通过学习满足循环包含性概念的集合，并利用循环性质计算吸引域的内部逼近。

    

    我们考虑在没有明确的动力学模型的情况下学习渐近稳定平衡点的吸引域的内部逼近问题。我们提出使用更宽松的包含性概念——循环，而不是利用带有有限不确定性的近似模型来找到包含在吸引域中的（稳定的）不变集。我们定义一个集合为$\tau$-循环（相应地为$k$-循环），如果每个从该集合开始的轨迹在最多$\tau$秒（相应地$k$步）后返回到该集合。我们证明，在温和的假设下，包含一个稳定平衡点的$\tau$-循环集必须是其吸引域的子集。然后，我们利用这个特性开发了使用通过采样有限长度轨迹获得的循环反例来计算吸引域内部逼近的算法。我们的算法依次处理样本，这使得它们能够在初始离线运行之后继续执行。

    We consider the problem of learning an inner approximation of the region of attraction (ROA) of an asymptotically stable equilibrium point without an explicit model of the dynamics. Rather than leveraging approximate models with bounded uncertainty to find a (robust) invariant set contained in the ROA, we propose to learn sets that satisfy a more relaxed notion of containment known as recurrence. We define a set to be $\tau$-recurrent (resp. $k$-recurrent) if every trajectory that starts within the set, returns to it after at most $\tau$ seconds (resp. $k$ steps). We show that under mild assumptions a $\tau$-recurrent set containing a stable equilibrium must be a subset of its ROA. We then leverage this property to develop algorithms that compute inner approximations of the ROA using counter-examples of recurrence that are obtained by sampling finite-length trajectories. Our algorithms process samples sequentially, which allow them to continue being executed even after an initial offli
    
[^155]: 用于计算具有跳跃性的随机动力学系统最可能转换路径的最优控制方法

    An Optimal Control Method to Compute the Most Likely Transition Path for Stochastic Dynamical Systems with Jumps. (arXiv:2203.16874v2 [math.NA] UPDATED)

    [http://arxiv.org/abs/2203.16874](http://arxiv.org/abs/2203.16874)

    本论文提出了一种用于计算具有跳跃性的随机动力学系统最可能转换路径的最优控制方法，通过解决一个最优控制问题和运用神经网络方法来克服相关速率函数无法明确表示的挑战。

    

    许多复杂的现实世界现象表现出突然、间歇或跳跃行为，更适合用非高斯L\'evy噪声下的随机微分方程来描述。在这些复杂现象中，转变状态之间的最可能转换路径非常重要，因为这些稀有事件在某些情景下可能会产生很高的影响。基于大偏差原理，最可能的转换路径可以被视为连接两个点的路径上的速率函数的极小化。计算非高斯L\'evy噪声下的随机动力学系统的最可能转换路径的挑战之一是相关的速率函数无法通过路径明确表示。出于这个原因，我们制定了一个最优控制问题，以获得最可能转换路径的最优状态。然后，我们开发了一种神经网络方法来解决这个问题。对高斯和非高斯情况进行了几个实验。

    Many complex real world phenomena exhibit abrupt, intermittent or jumping behaviors, which are more suitable to be described by stochastic differential equations under non-Gaussian L\'evy noise. Among these complex phenomena, the most likely transition paths between metastable states are important since these rare events may have a high impact in certain scenarios. Based on the large deviation principle, the most likely transition path could be treated as the minimizer of the rate function upon paths that connect two points. One of the challenges to calculate the most likely transition path for stochastic dynamical systems under non-Gaussian L\'evy noise is that the associated rate function can not be explicitly expressed by paths. For this reason, we formulate an optimal control problem to obtain the optimal state as the most likely transition path. We then develop a neural network method to solve this issue. Several experiments are investigated for both Gaussian and non-Gaussian case
    
[^156]: 模型重新编程：资源高效的跨领域机器学习

    Model Reprogramming: Resource-Efficient Cross-Domain Machine Learning. (arXiv:2202.10629v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.10629](http://arxiv.org/abs/2202.10629)

    模型重新编程是一种资源高效的跨领域机器学习方法，通过重新利用和重用预训练模型，无需模型细调即可在目标领域解决任务。这种方法在许多应用中优于迁移学习和从头训练。

    

    在视觉、语言和语音等数据丰富的领域中，深度学习在提供高性能的特定任务模型方面占据主导地位，甚至可以学习通用的任务无关表示以便有效地进行下游任务的细调。然而，在资源有限的领域中，深度学习仍面临多个挑战，包括：（i）数据有限；（ii）模型开发成本受限；（iii）缺乏足够的预训练模型以便有效进行细调。本文概述了模型重新编程的概念来弥合这一差距。模型重新编程通过从源领域重新利用和重用一个精心开发的预训练模型，在目标领域解决任务而无需进行模型细调，从而实现了资源高效的跨领域机器学习，源领域和目标领域可以差异巨大。在许多应用中，模型重新编程优于迁移学习和从头训练。本文阐述了模型重新编程的方法论，并总结了现有的应用情况。

    In data-rich domains such as vision, language, and speech, deep learning prevails to deliver high-performance task-specific models and can even learn general task-agnostic representations for efficient finetuning to downstream tasks. However, deep learning in resource-limited domains still faces multiple challenges including (i) limited data, (ii) constrained model development cost, and (iii) lack of adequate pre-trained models for effective finetuning. This paper provides an overview of model reprogramming to bridge this gap. Model reprogramming enables resource-efficient cross-domain machine learning by repurposing and reusing a well-developed pre-trained model from a source domain to solve tasks in a target domain without model finetuning, where the source and target domains can be vastly different. In many applications, model reprogramming outperforms transfer learning and training from scratch. This paper elucidates the methodology of model reprogramming, summarizes existing use c
    
[^157]: 随机特征放大：神经网络中的特征学习和泛化

    Random Feature Amplification: Feature Learning and Generalization in Neural Networks. (arXiv:2202.07626v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.07626](http://arxiv.org/abs/2202.07626)

    本文通过分析在随机初始化后通过逻辑损失函数进行梯度下降训练的两层ReLU网络在特征学习过程中的表征，证明了对于具有二元标签的由XOR-like函数生成的数据，尽管线性分类器在该分布上无法更好地工作，但该网络的泛化误差接近于标签噪声率。通过一种新颖的证明技巧，揭示了初始化时大多数神经元作为随机特征，随后通过梯度下降动力学将这些弱的随机特征放大为有用的特征。

    

    在这项工作中，我们提供了对由梯度下降训练的两层ReLU网络在随机初始化后，通过逻辑损失函数对特征学习过程的表征。我们考虑由输入特征的XOR-like函数生成的具有二元标签的数据。我们允许一个固定比例的训练标签被对手损坏。我们证明，尽管线性分类器对于我们考虑的分布而言不比随机猜测更好，但通过梯度下降训练的两层ReLU网络的泛化误差接近于标签噪声率。我们开发了一种新颖的证明技巧，表明在初始化时，绝大多数神经元作为仅与有用特征弱相关的随机特征，而梯度下降动力学将这些弱的随机特征放大为强有用的特征。

    In this work, we provide a characterization of the feature-learning process in two-layer ReLU networks trained by gradient descent on the logistic loss following random initialization. We consider data with binary labels that are generated by an XOR-like function of the input features. We permit a constant fraction of the training labels to be corrupted by an adversary. We show that, although linear classifiers are no better than random guessing for the distribution we consider, two-layer ReLU networks trained by gradient descent achieve generalization error close to the label noise rate. We develop a novel proof technique that shows that at initialization, the vast majority of neurons function as random features that are only weakly correlated with useful features, and the gradient descent dynamics 'amplify' these weak, random features to strong, useful features.
    
[^158]: 不需要线性关系的良性过拟合：通过梯度下降训练的神经网络分类器用于噪声线性数据

    Benign Overfitting without Linearity: Neural Network Classifiers Trained by Gradient Descent for Noisy Linear Data. (arXiv:2202.05928v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.05928](http://arxiv.org/abs/2202.05928)

    本文研究了使用梯度下降训练的神经网络在泛化时能够很好应对噪声数据的良性过拟合现象。研究表明，在特定条件下，神经网络能够将训练误差降至零并完美地适应带有噪声标签的数据，并同时达到最优的测试误差。

    

    良性过拟合是指插值模型在存在噪声数据的情况下能够很好地泛化的现象，最早出现在使用梯度下降训练的神经网络模型中。为了更好地理解这一实证观察，我们考虑了两层神经网络在随机初始化后通过梯度下降在逻辑损失函数上进行插值训练的泛化误差。我们假设数据来自于明显分离的类条件对数凹分布，并允许训练标签中的一定比例被对手篡改。我们证明在这种情况下，神经网络表现出良性过拟合的特点：它们可以被驱动到零训练误差，完美地拟合任何有噪声的训练标签，并同时达到极小化最大化最优测试误差。与之前关于良性过拟合需要线性或基于核的预测器的工作相比，我们的分析在模型和学习动态都是基本非线性的情况下成立。

    Benign overfitting, the phenomenon where interpolating models generalize well in the presence of noisy data, was first observed in neural network models trained with gradient descent. To better understand this empirical observation, we consider the generalization error of two-layer neural networks trained to interpolation by gradient descent on the logistic loss following random initialization. We assume the data comes from well-separated class-conditional log-concave distributions and allow for a constant fraction of the training labels to be corrupted by an adversary. We show that in this setting, neural networks exhibit benign overfitting: they can be driven to zero training error, perfectly fitting any noisy training labels, and simultaneously achieve minimax optimal test error. In contrast to previous work on benign overfitting that require linear or kernel-based predictors, our analysis holds in a setting where both the model and learning dynamics are fundamentally nonlinear.
    
[^159]: PolicyCleanse：强化学习中的后门检测与缓解

    PolicyCleanse: Backdoor Detection and Mitigation in Reinforcement Learning. (arXiv:2202.03609v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.03609](http://arxiv.org/abs/2202.03609)

    本文提出了PolicyCleanse方法，用于检测和缓解多智能体强化学习系统中的后门攻击。该方法基于激活的特洛伊智能体累积奖励的下降特性进行检测，并尝试缓解其特洛伊行为。

    

    尽管强化学习在现实世界中的应用越来越受欢迎，但RL系统的安全性和稳健性仍值得更多关注和探索。最近的研究揭示了在多智能体强化学习环境中，可以向受害者智能体（即特洛伊智能体）注入后门触发动作，这可能导致灾难性失败。为确保RL智能体对恶意后门的安全性，本文提出了在多智能体竞争性强化学习系统中的后门检测问题，其目标是检测特洛伊智能体以及相应的潜在触发动作，并进一步尝试缓解其特洛伊行为。为解决这个问题，我们提出了基于激活的特洛伊智能体累积奖励在几个时间步之后明显下降的PolicyCleanse。除了PolicyCleanse，我们还设计了一个机器未完成的部分...

    While real-world applications of reinforcement learning are becoming popular, the security and robustness of RL systems are worthy of more attention and exploration. In particular, recent works have revealed that, in a multi-agent RL environment, backdoor trigger actions can be injected into a victim agent (a.k.a. Trojan agent), which can result in a catastrophic failure as soon as it sees the backdoor trigger action. To ensure the security of RL agents against malicious backdoors, in this work, we propose the problem of Backdoor Detection in a multi-agent competitive reinforcement learning system, with the objective of detecting Trojan agents as well as the corresponding potential trigger actions, and further trying to mitigate their Trojan behavior. In order to solve this problem, we propose PolicyCleanse that is based on the property that the activated Trojan agents accumulated rewards degrade noticeably after several timesteps. Along with PolicyCleanse, we also design a machine unl
    
[^160]: 通过群等变卷积量子Ansatze加速学习量子态

    Speeding up Learning Quantum States through Group Equivariant Convolutional Quantum Ans\"atze. (arXiv:2112.07611v3 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2112.07611](http://arxiv.org/abs/2112.07611)

    通过建立$S_n$-等变卷积量子Ansatze，我们证明了其能够在具有SU($d$)对称性的广泛量子机器学习问题中生成任意幺正矩阵，同时验证了4-local SU($d$)对称幺正矩阵的可实现性。

    

    我们基于Schur-Weyl对偶，建立了一个理论框架，用于$S_n$-等变卷积量子电路和SU$(d)$对称性，将Jordan的置换量子计算(PQC)形式主义扩展和泛化。我们利用Okounkov-Vershik方法来证明Harrow在奇异边表示基之间的等价性，并利用Young-Jucys-Murphy(YJM)元素建立了$S_n$-等变卷积量子交替Ansatze($S_n$-CQA)。我们证明了$S_n$-CQA能够在任何给定的$S_n$ irrep扇区中生成任意幺正矩阵，这可以作为具有SU($d$)对称性的广泛量子机器学习问题的通用模型。我们的方法提供了另一种证明量子近似优化算法(QAOA)的普适性的方式，并验证了4-local SU($d$)对称幺正矩阵是可实现的。

    We develop a theoretical framework for $S_n$-equivariant convolutional quantum circuits with SU$(d)$-symmetry, building on and significantly generalizing Jordan's Permutational Quantum Computing (PQC) formalism based on Schur-Weyl duality connecting both SU$(d)$ and $S_n$ actions on qudits. In particular, we utilize the Okounkov-Vershik approach to prove Harrow's statement (Ph.D. Thesis 2005 p.160) on the equivalence between $\operatorname{SU}(d)$ and $S_n$ irrep bases and to establish the $S_n$-equivariant Convolutional Quantum Alternating Ans\"atze ($S_n$-CQA) using Young-Jucys-Murphy (YJM) elements. We prove that $S_n$-CQA is able to generate any unitary in any given $S_n$ irrep sector, which may serve as a universal model for a wide array of quantum machine learning problems with the presence of SU($d$) symmetry. Our method provides another way to prove the universality of Quantum Approximate Optimization Algorithm (QAOA) and verifies that 4-local SU($d$) symmetric unitaries are su
    
[^161]: Pareto对抗鲁棒性：平衡空间鲁棒性和基于敏感性的鲁棒性

    Pareto Adversarial Robustness: Balancing Spatial Robustness and Sensitivity-based Robustness. (arXiv:2111.01996v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.01996](http://arxiv.org/abs/2111.01996)

    本文提出了Pareto Adversarial Robustness策略，通过整合空间鲁棒性方法和基于敏感性的鲁棒性，实现了通用对抗鲁棒性。同时，从鲁棒表示的角度提供了自然准确性、敏感性鲁棒性和空间鲁棒性之间的关系。

    

    对抗鲁棒性主要包括基于敏感性的鲁棒性和空间鲁棒性，在实现鲁棒泛化方面起着重要作用。本文旨在设计实现通用对抗鲁棒性的策略。为了实现这一目标，我们首先研究了相对较少探索的空间鲁棒性领域。然后，我们将现有的空间鲁棒性方法整合起来，将局部和全局空间的脆弱性结合到一个统一的空间攻击和对抗训练方法中。此外，我们从鲁棒表示的角度提供了自然准确性、基于敏感性的鲁棒性和空间鲁棒性之间的全面关系，并且提供了强有力的证据支持。关键是，为了将各种鲁棒性组成部分之间的相互影响纳入一个统一的框架中，我们将帕累托准则引入到对抗鲁棒性分析中，提出了一种名为Pareto Adversarial Robustness的新策略。

    Adversarial robustness, which primarily comprises sensitivity-based robustness and spatial robustness, plays an integral part in achieving robust generalization. In this paper, we endeavor to design strategies to achieve universal adversarial robustness. To achieve this, we first investigate the relatively less-explored realm of spatial robustness. Then, we integrate the existing spatial robustness methods by incorporating both local and global spatial vulnerability into a unified spatial attack and adversarial training approach. Furthermore, we present a comprehensive relationship between natural accuracy, sensitivity-based robustness, and spatial robustness, supported by strong evidence from the perspective of robust representation. Crucially, to reconcile the interplay between the mutual impacts of various robustness components into one unified framework, we incorporate the \textit{Pareto criterion} into the adversarial robustness analysis, yielding a novel strategy called Pareto Ad
    
[^162]: 用于神经文本到语音的高效采样的离散声学空间

    Discrete Acoustic Space for an Efficient Sampling in Neural Text-To-Speech. (arXiv:2110.12539v3 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2110.12539](http://arxiv.org/abs/2110.12539)

    我们提出了一种使用拆分向量量化变分自编码器(SVQ-VAE)架构的离散声学空间，相比于之前的架构，该模型既保留了使用话语级限制的好处，又具有足够小的离散潜在空间以从文本进行高效预测。在实验中，我们证明了SVQ-VAE在自然度方面明显优于其他模型，并且可从文本进行预测，减少了合成和录音之间的差距。

    

    我们提出了一种使用拆分向量量化变分自编码器(SVQ-VAE)架构的离散声学空间，作为对知名的变分自编码器(VAE)和向量量化变分自编码器(VQ-VAE)架构的增强。与之前的架构相比，我们的模型保留了使用话语级限制的好处，同时保持了显著的表示能力，并且离散潜在空间小到足够从文本进行高效预测。我们在表达性任务对话领域的录音上训练模型，并显示SVQ-VAE在自然度方面明显优于VAE和VQ-VAE模型。此外，我们证明了SVQ-VAE潜在声学空间可从文本进行预测，将标准的恒定向量合成和声码器录音之间的差距减少了32%。

    We present a Split Vector Quantized Variational Autoencoder (SVQ-VAE) architecture using a split vector quantizer for NTTS, as an enhancement to the well-known Variational Autoencoder (VAE) and Vector Quantized Variational Autoencoder (VQ-VAE) architectures. Compared to these previous architectures, our proposed model retains the benefits of using an utterance-level bottleneck, while keeping significant representation power and a discretized latent space small enough for efficient prediction from text. We train the model on recordings in the expressive task-oriented dialogues domain and show that SVQ-VAE achieves a statistically significant improvement in naturalness over the VAE and VQ-VAE models. Furthermore, we demonstrate that the SVQ-VAE latent acoustic space is predictable from text, reducing the gap between the standard constant vector synthesis and vocoded recordings by 32%.
    
[^163]: 通过半定规划，联合社区检测和旋转同步

    Joint Community Detection and Rotational Synchronization via Semidefinite Programming. (arXiv:2105.06031v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2105.06031](http://arxiv.org/abs/2105.06031)

    本文提出了一种通过半定规划来联合社区检测和旋转同步的方法，在存在异构数据的情况下，能够准确恢复出旋转和聚类身份，并通过数值实验证明了其有效性和准确恢复性的相变现象。

    

    在存在异构数据的情况下，随机旋转的对象落入多个潜在类别中，同时对它们进行聚类和基于两两关系进行同步是具有挑战性的。这导致了社区检测和同步的联合问题。我们提出了一系列半定松弛，并证明了它们在将著名的随机块模型扩展到旋转和聚类身份都需要确定的新情况时的精确恢复性。数值实验证明了我们提出的算法的有效性，并证实了我们的理论结果，表明了准确恢复性的明显相变。

    In the presence of heterogeneous data, where randomly rotated objects fall into multiple underlying categories, it is challenging to simultaneously classify them into clusters and synchronize them based on pairwise relations. This gives rise to the joint problem of community detection and synchronization. We propose a series of semidefinite relaxations, and prove their exact recovery when extending the celebrated stochastic block model to this new setting where both rotations and cluster identities are to be determined. Numerical experiments demonstrate the efficacy of our proposed algorithms and confirm our theoretical result which indicates a sharp phase transition for exact recovery.
    
[^164]: BAARD：通过检测适用性、可靠性和可决策性来阻挡对抗性样本

    BAARD: Blocking Adversarial Examples by Testing for Applicability, Reliability and Decidability. (arXiv:2105.00495v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.00495](http://arxiv.org/abs/2105.00495)

    该论文提出了一种新的对抗性样本检测方法，借鉴了化学信息学中的适用性域概念，通过判断样本是否在分类器的域内来阻挡对抗性样本。

    

    对抗性防御保护机器学习模型免受对抗性攻击，但往往针对特定类型的模型或攻击进行定制。缺乏关于未知潜在攻击的信息使得检测对抗性样本具有挑战性。此外，攻击者不需要遵守防御者制定的规则。为了解决这个问题，我们借鉴了化学信息学中的适用性域的概念。化学信息学模型之所以难以进行准确预测，是因为只有有限数量的化合物被知晓并可用于训练。适用性域根据已知化合物定义一个域，并拒绝任何落在域外的未知化合物。类似地，对抗性样本最初是无害的输入，但可以通过移动到分类器域外来操纵以逃避可靠的分类。我们是第一个识别出适用性域与对抗性检测之间相似性的研究。我们不再关注未知攻击，而是专注于检测样本是否在分类器的域内。

    Adversarial defenses protect machine learning models from adversarial attacks, but are often tailored to one type of model or attack. The lack of information on unknown potential attacks makes detecting adversarial examples challenging. Additionally, attackers do not need to follow the rules made by the defender. To address this problem, we take inspiration from the concept of Applicability Domain in cheminformatics. Cheminformatics models struggle to make accurate predictions because only a limited number of compounds are known and available for training. Applicability Domain defines a domain based on the known compounds and rejects any unknown compound that falls outside the domain. Similarly, adversarial examples start as harmless inputs, but can be manipulated to evade reliable classification by moving outside the domain of the classifier. We are the first to identify the similarity between Applicability Domain and adversarial detection. Instead of focusing on unknown attacks, we f
    
[^165]: 基于Copula模型的缺失非随机截尾指标的生存估计

    Survival Estimation for Missing not at Random Censoring Indicators based on Copula Models. (arXiv:2009.01726v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2009.01726](http://arxiv.org/abs/2009.01726)

    本论文提出了一种基于Copula模型的新的估计器，用于处理缺失非随机截尾指标下的条件生存函数估计问题。通过模拟实验和真实数据分析，我们验证了该估计器的实用性。

    

    在具有协变量的右截尾数据中，条件Kaplan-Meier估计器（也称为Beran估计器）一致估计感兴趣事件的随机追踪的条件生存函数。然而，必要条件是对每个个体是否被截尾有明确的了解，而在实践中可能是不完整的。因此，我们提出了当截尾指标是一般随机变量时对Beran估计器进行研究，并讨论了Beran估计器效率的必要条件。基于缺失非随机（MNAR）截尾指标的条件Copula模型，我们提供了一种新的条件生存函数估计器。除了理论结果，我们通过模拟研究对小样本中的估计器进行了演示，并通过分析合成和真实数据展示了它们的实际适用性。

    In the presence of right-censored data with covariates, the conditional Kaplan-Meier estimator (also known as the Beran estimator) consistently estimates the conditional survival function of the random follow-up for the event of interest. However, a necessary condition is the unambiguous knowledge of whether each individual is censored or not, which may be incomplete in practice. We therefore propose a study of the Beran estimator when the censoring indicators are generic random variables and discuss necessary conditions for the efficiency of the Beran estimator. From this, we provide a new estimator for the conditional survival function with missing not at random (MNAR) censoring indicators based on a conditional copula model for the missingness mechanism. In addition to the theoretical results, we illustrate how the estimators work for small samples through a simulation study and show their practical applicability by analyzing synthetic and real data.
    
[^166]: 在多面体上寻找二次函数的局部最小化问题的复杂性研究

    On the complexity of finding a local minimizer of a quadratic function over a polytope. (arXiv:2008.05558v5 [math.OC] UPDATED)

    [http://arxiv.org/abs/2008.05558](http://arxiv.org/abs/2008.05558)

    除非P=NP，否则不存在一个多项式时间算法，能够找到一个在欧几里得距离$c^n$（对于任意常数$c \ge 0$）内的二次函数在多面体上的局部最小化点。

    

    我们证明，除非P=NP，否则不存在一个多项式时间算法，能够找到一个在欧几里得距离$c^n$（对于任意常数$c \ge 0$）内的二次函数在多面体上的局部最小化点。这个结果（即使在$c=0$的情况下）回答了Pardalos和Vavasis在1992年关于数值优化复杂性理论中提出的一个由七个开放问题组成的列表中的一个问题。我们的证明技巧还意味着判断一个二次函数是否在（无界）多面体上有局部最小化点的问题，以及判断一个四次多项式是否有局部最小化点的问题都是NP难问题。

    We show that unless P=NP, there cannot be a polynomial-time algorithm that finds a point within Euclidean distance $c^n$ (for any constant $c \ge 0$) of a local minimizer of an $n$-variate quadratic function over a polytope. This result (even with $c=0$) answers a question of Pardalos and Vavasis that appeared in 1992 on a list of seven open problems in complexity theory for numerical optimization. Our proof technique also implies that the problem of deciding whether a quadratic function has a local minimizer over an (unbounded) polyhedron, and that of deciding if a quartic polynomial has a local minimizer are NP-hard.
    
[^167]: 关于非凸-凹极小极大问题的梯度下降算法

    On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems. (arXiv:1906.00331v9 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1906.00331](http://arxiv.org/abs/1906.00331)

    该论文提出了一种用于解决非凸-凹极小极大问题的梯度下降算法，并进行了复杂性分析，证明了该算法可以高效地找到函数的稳定点。该研究对于非凸-凹极小极大问题的非渐进分析是首次进行的。

    

    我们考虑非凸-凹极小极大问题，即$\min_{\mathbf{x}} \max_{\mathbf{y} \in \mathcal{Y}} f(\mathbf{x}, \mathbf{y})$，其中$f$在$\mathbf{x}$上是非凸的但在$\mathbf{y}$上是凹的，$\mathcal{Y}$是一个凸且有界的集合。解决这个问题最常用的算法之一是著名的梯度下降上升（GDA）算法，在机器学习、控制理论和经济学中得到了广泛应用。尽管凸-凹设置下有着广泛的收敛结果，但在一般情况下，相同步长的GDA可能会收敛到极限环，甚至发散。本文介绍了解决非凸-凹极小极大问题的两时间尺度GDA的复杂度结果，并证明了该算法可以高效地找到函数$\Phi(\cdot) := \max_{\mathbf{y} \in \mathcal{Y}} f(\cdot, \mathbf{y})$的一个稳定点。据我们所知，这是第一个在这个设置中的非渐进分析结果，揭示了有关两时间尺度GDA的性质。

    We consider nonconvex-concave minimax problems, $\min_{\mathbf{x}} \max_{\mathbf{y} \in \mathcal{Y}} f(\mathbf{x}, \mathbf{y})$, where $f$ is nonconvex in $\mathbf{x}$ but concave in $\mathbf{y}$ and $\mathcal{Y}$ is a convex and bounded set. One of the most popular algorithms for solving this problem is the celebrated gradient descent ascent (GDA) algorithm, which has been widely used in machine learning, control theory and economics. Despite the extensive convergence results for the convex-concave setting, GDA with equal stepsize can converge to limit cycles or even diverge in a general setting. In this paper, we present the complexity results on two-time-scale GDA for solving nonconvex-concave minimax problems, showing that the algorithm can find a stationary point of the function $\Phi(\cdot) := \max_{\mathbf{y} \in \mathcal{Y}} f(\cdot, \mathbf{y})$ efficiently. To the best our knowledge, this is the first nonasymptotic analysis for two-time-scale GDA in this setting, shedding lig
    

