# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [On the Effect of Contextual Information on Human Delegation Behavior in Human-AI collaboration.](http://arxiv.org/abs/2401.04729) | 本研究探讨了在人工智能协作中提供上下文信息对人类委托行为的影响，发现提供上下文信息显著提高了人工智能与人类团队的表现，并且委托行为在不同上下文信息下发生显著变化。这项研究推进了对人工智能委托中人工智能与人类互动的理解，并为设计更有效的协作系统提供了见解。 |
| [^2] | [U-Mamba: Enhancing Long-range Dependency for Biomedical Image Segmentation.](http://arxiv.org/abs/2401.04722) | U-Mamba是一个通用网络，通过设计混合的CNN-SSM块，将卷积层的局部特征提取能力与SSM的捕捉长程依赖性的能力结合，从而增强了生物医学图像分割的长程依赖性。 |
| [^3] | [AI-based Mapping of the Conservation Status of Orchid Assemblages at Global Scale.](http://arxiv.org/abs/2401.04691) | 本研究通过使用一个新的深度物种分布模型，对全球范围内兰科群落的保护状况进行了评估和映射，提出了衡量保护状况的两个指标，并展示了这些指标在世界范围内的变化和与受保护地区的关系。 |
| [^4] | [Mixture of multilayer stochastic block models for multiview clustering.](http://arxiv.org/abs/2401.04682) | 本文提出了一种混合多层次随机块模型的方法，用于聚合来自不同信息源的多个聚类，并将观测分区到不同的聚类中，考虑到它们在组件内的特定性。该方法在合成数据和全球食品贸易网络中得到了有趣的结果。 |
| [^5] | [RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation.](http://arxiv.org/abs/2401.04679) | RoSA是一种新的PEFT方法，通过在预训练权重上训练低秩和高度稀疏的组件，以高效近似完全微调的性能，来实现准确的参数高效微调。在多个生成任务中，RoSA表现出优于其他方法的性能。 |
| [^6] | [Transfer-Learning-Based Autotuning Using Gaussian Copula.](http://arxiv.org/abs/2401.04669) | 该论文提出了一种基于高斯Copula的迁移学习自动调优方法，通过利用先前调优数据中搜索空间的高性能区域建模，为新任务生成高性能配置。 |
| [^7] | [Benchmark Analysis of Various Pre-trained Deep Learning Models on ASSIRA Cats and Dogs Dataset.](http://arxiv.org/abs/2401.04666) | 本研究基于ASSIRA猫狗数据集，比较了使用不同优化器和损失函数的各种预训练模型，并通过改变超参数来提高模型准确性。 |
| [^8] | [A novel framework for generalization of deep hidden physics models.](http://arxiv.org/abs/2401.04648) | 这项工作提出了一种改进的隐含物理模型框架，可以泛化适应系统输入、参数和域的变化，并在系统发现中展现了潜力。 |
| [^9] | [Advancing Ante-Hoc Explainable Models through Generative Adversarial Networks.](http://arxiv.org/abs/2401.04647) | 本文提出了一种先验可解释模型，通过在主分类器网络中添加无监督的解释生成器和对抗训练的方式，实现了模型的可解释性和性能的提升。该方法通过训练解释模块提取视觉概念，同时使用生成对抗网络模块来区分生成的图像和真实图像。实验证明了该方法的鲁棒性，并展示了学到的概念与对象部分和视觉属性的语义一致性。 |
| [^10] | [Applying Large Language Models API to Issue Classification Problem.](http://arxiv.org/abs/2401.04637) | 本研究通过应用生成式预训练转换器（GPT）模型，提出一种可靠的自动化方法来解决问题报告的优先级排序问题，即使在较小的训练数据集下仍能保持可靠性，减少了对大量训练数据的依赖性。 |
| [^11] | [Hypercomplex neural network in time series forecasting of stock data.](http://arxiv.org/abs/2401.04632) | 本文测试了三种不同的神经网络架构，将其用于股票数据的时间序列预测。结果显示，具有超复数密集层的架构在准确性方面表现类似于其他架构，但可训练参数更少。此外，输入时间序列的顺序对有效性有影响。 |
| [^12] | [Deep Reinforcement Multi-agent Learning framework for Information Gathering with Local Gaussian Processes for Water Monitoring.](http://arxiv.org/abs/2401.04631) | 本文提出了一个深度强化多智能体学习框架，结合局部高斯过程，用于水污染监测。使用局部高斯过程准确建模不同空间相关性中的信息，并采用深度卷积策略决策方法来获得有效的监测策略。通过双重深度 Q 学习算法训练智能体以减小估计误差。 |
| [^13] | [Distribution-Free Conformal Joint Prediction Regions for Neural Marked Temporal Point Processes.](http://arxiv.org/abs/2401.04612) | 本文为神经标记时序点过程模型开发了可靠的不确定性量化方法，通过相容预测框架生成无分布假设的联合预测区间，并提供较好的边际覆盖保证。 |
| [^14] | [Enhanced Distribution Alignment for Post-Training Quantization of Diffusion Models.](http://arxiv.org/abs/2401.04585) | 本文提出了一种扩展分布对齐方法以解决后训练量化对于弥散模型的分布不匹配问题，该方法在低延迟应用中具有较高的潜力，并且能有效提升性能。 |
| [^15] | [Masked Audio Generation using a Single Non-Autoregressive Transformer.](http://arxiv.org/abs/2401.04577) | MAGNeT是一种遮蔽生成序列建模方法，使用单一非自回归Transformer生成具有高质量的音频，并引入了一种新颖的重新评分方法来提高生成音频的质量。同时，MAGNeT还探索了混合版本，可在自回归模式和非自回归模式下生成序列。在实验中证明MAGNeT在文本到音乐和文本到音频生成任务中具有高效性。 |
| [^16] | [Robust Imitation Learning for Automated Game Testing.](http://arxiv.org/abs/2401.04572) | 提出了一个新的基于模仿学习的架构EVOLUTE，结合了行为克隆和能量模型，用于自动化游戏测试。该架构将自主代理的动作空间拆分为连续和离散任务，以优化控制和训练的效果。实验证明，在射击和驾驶游戏中，EVOLUTE表现出良好的性能。 |
| [^17] | [HyperGANStrument: Instrument Sound Synthesis and Editing with Pitch-Invariant Hypernetworks.](http://arxiv.org/abs/2401.04558) | 本文介绍了HyperGANStrument方法，使用无关音高的超网络来调制预训练的GANStrument生成器，进一步提高了重构能力和音高准确性，增强了合成声音的可编辑性。 |
| [^18] | [Linear Recursive Feature Machines provably recover low-rank matrices.](http://arxiv.org/abs/2401.04553) | 该论文介绍了递归特征机器（RFMs）算法，它通过交替重新加权特征向量和在转换空间中学习预测函数来执行显式的特征学习。研究分析了RFM在稀疏线性回归和低秩矩阵恢复问题中的维数减少性能。 |
| [^19] | [Evaluating Language Model Agency through Negotiations.](http://arxiv.org/abs/2401.04536) | 本研究通过谈判游戏的视角，提出共同评估语言模型（LM）的性能和对齐，以更好地反映真实世界的部署条件，并避免数据泄漏。通过评估多轮次和跨模型交互，我们发现了LM的自我对弈和交叉对弈性能。 |
| [^20] | [Semi-Supervised Deep Sobolev Regression: Estimation, Variable Selection and Beyond.](http://arxiv.org/abs/2401.04535) | 我们提出了一种半监督深度Sobolev回归器，利用深度神经网络进行梯度范数正则化，可以同时估计回归函数和其梯度，即使存在显著领域变化。这在半监督学习中利用无标签数据方面具有可证优势。 |
| [^21] | [Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search.](http://arxiv.org/abs/2401.04514) | 本论文提出一种扩展的生成增强检索（GAR）框架，通过对代码进行重写来解决代码搜索中存在的风格不匹配问题，实验结果表明该方法显著提高了检索准确性。 |
| [^22] | [Zero Shot Audio to Audio Emotion Transfer With Speaker Disentanglement.](http://arxiv.org/abs/2401.04511) | 本文提出了一种名为ZEST的零样本情感风格转移方法，允许将给定源音频中的情感内容与目标音频中的情感内容进行转移，同时保留源音频的说话人和语音内容。通过分解语音为语义标记、说话人表示和情感嵌入，并利用重构模型进行训练，实现了高效的转换过程。 |
| [^23] | [Data-driven Nonlinear Model Reduction using Koopman Theory: Integrated Control Form and NMPC Case Study.](http://arxiv.org/abs/2401.04508) | 本研究基于Koopman理论，提出了一种数据驱动非线性模型简化方法。通过结合延迟坐标编码和全状态解码的模型结构，我们成功地实现了高纯度低温精馏塔的实时非线性模型预测控制。 |
| [^24] | [SpiNNaker2: A Large-Scale Neuromorphic System for Event-Based and Asynchronous Machine Learning.](http://arxiv.org/abs/2401.04491) | SpiNNaker2是一种用于大规模基于事件和异步机器学习的神经形态系统芯片，能够通过整合计算原理来减少计算成本和能源消耗。 |
| [^25] | [Optimal Survival Trees: A Dynamic Programming Approach.](http://arxiv.org/abs/2401.04489) | 本文提出了一种利用动态规划的生存树方法，通过递归分割人口和预测不同的生存分布来发现复杂的非线性关系。该方法具有优化保证，并通过特殊算法提高了可扩展性，运行时间优于某些启发式算法。 |
| [^26] | [Continuously Learning New Words in Automatic Speech Recognition.](http://arxiv.org/abs/2401.04482) | 该论文提出了一种自我监督的持续学习方法，用于解决自动语音识别中识别新词的问题。通过对讲座录音进行推理和收集包含新词的话语，然后在自适应数据集上进行持续学习，可以在新词出现频率较高时提高性能，同时保持整体性能。 |
| [^27] | [TwinBooster: Synergising Large Language Models with Barlow Twins and Gradient Boosting for Enhanced Molecular Property Prediction.](http://arxiv.org/abs/2401.04478) | TwinBooster结合了大语言模型、Barlow Twins和梯度提升，通过整合生物检测方法和分子指纹，实现了对未见过的生物检测方法和分子属性的精确预测，该方法在数据稀缺的情况下展现出了优秀的性能。 |
| [^28] | [A Survey on Efficient Federated Learning Methods for Foundation Model Training.](http://arxiv.org/abs/2401.04472) | 这项调查研究了高效联邦学习方法在基础模型训练中的应用，提出了一个新的分类方法以优化计算和通信效率。该研究还讨论了当前广泛使用的FL框架，并展望了未来的研究潜力。 |
| [^29] | [PhilEO Bench: Evaluating Geo-Spatial Foundation Models.](http://arxiv.org/abs/2401.04464) | 本文引入了PhilEO Bench，一个新颖的EO基础模型评估框架，旨在解决EO领域中缺乏标记数据的问题。框架包括测试平台和一个400 GB Sentinel-2数据集，用于建筑密度估计、道路分割和土地覆盖分类的标签，通过实验评估了不同的基础模型。 |
| [^30] | [AI Competitions and Benchmarks, Practical issues: Proposals, grant money, sponsors, prizes, dissemination, publicity.](http://arxiv.org/abs/2401.04452) | 本章提供了组织人工智能竞赛所需考虑的实践问题的综述，包括激励参与的策略、社区参与的核心内容以及后勤问题的管理和执行。 |
| [^31] | [Sea wave data reconstruction using micro-seismic measurements and machine learning methods.](http://arxiv.org/abs/2401.04431) | 本研究提出了使用微地震测量和机器学习方法重建海浪数据的系统，通过测量海浪产生的微地震信号并训练机器学习算法，实现对缺失浮标数据的准确重建。 |
| [^32] | [Meta-forests: Domain generalization on random forests with meta-learning.](http://arxiv.org/abs/2401.04425) | 本文提出了一种新的领域泛化算法"元森林"，通过整合元学习策略和最大均值差异度量，提高分类器的泛化能力。该算法在每个元任务中进行元学习优化，并利用最大均值差异作为正则化项，来惩罚泛化性能差的情况。 |
| [^33] | [Fine-Grained Embedding Dimension Optimization During Training for Recommender Systems.](http://arxiv.org/abs/2401.04408) | 本文提出了一种细粒度嵌入维度优化方法（FIITED），能够在推荐系统的训练过程中根据嵌入向量的重要性不断调整其维度，并设计了一种虚拟哈希索引哈希表的嵌入存储系统以有效节省内存。 |
| [^34] | [IGNITE: Individualized GeNeration of Imputations in Time-series Electronic health records.](http://arxiv.org/abs/2401.04402) | 个体化时间序列电子健康记录的生成模型IGNITE通过学习个体的动态特征，结合人口特征和治疗信息，生成个性化的真实值，为个体化医疗提供了有价值的方式。 |
| [^35] | [The Role of Higher-Order Cognitive Models in Active Learning.](http://arxiv.org/abs/2401.04397) | 本论文讨论了在人工智能中构建能与人类高效协作的机器的目标，介绍了使用高阶认知模型的主动学习的实际例子，并提出了一种新的主动学习范式，利用人类作为主动数据源，并考虑他们更高级别的代理作用。 |
| [^36] | [Machine unlearning through fine-grained model parameters perturbation.](http://arxiv.org/abs/2401.04385) | 本文提出了一种精细的机器去学习策略，通过细粒度模型参数的扰动来实现用户隐私保护，同时保持可控的计算成本。采用遗忘率和记忆保留率等新的指标来评估去学习效果和模型泛化能力。 |
| [^37] | [Towards Explainable Artificial Intelligence (XAI): A Data Mining Perspective.](http://arxiv.org/abs/2401.04374) | 本研究提出了一种“数据为中心”的视角，探讨了数据收集、处理和分析在可解释的人工智能中的作用。研究将现有工作分为三个类别：深度模型的解释、训练数据的影响和领域知识的见解。通过数据挖掘操作，我们总结了这些XAI方法。 |
| [^38] | [Stable generative modeling using diffusion maps.](http://arxiv.org/abs/2401.04372) | 本文提出了一种稳定的生成建模方法，通过将扩散映射与朗之万动力学相结合，在仅有有限数量的训练样本的情况下生成新样本，并解决了时间步长僵硬随机微分方程中的稳定性问题。 |
| [^39] | [Air Quality Forecasting Using Machine Learning: A Global perspective with Relevance to Low-Resource Settings.](http://arxiv.org/abs/2401.04369) | 本研究提出了一种新的机器学习方法，利用少量的空气质量数据来准确预测空气质量。通过使用世界天气存储库的数据，考虑了来自197个首都城市的气象、空气污染物和空气质量指数特征，实现了可靠的预测。随机森林算法在分类方面表现最好，提高了模型的泛化能力。极大地提升了低资源环境下空气质量预测的适用性。 |
| [^40] | [Enhancing Acute Kidney Injury Prediction through Integration of Drug Features in Intensive Care Units.](http://arxiv.org/abs/2401.04368) | 本研究通过整合药物特征，提出了一种新方法来增强重症监护病房中急性肾损伤预测模型。通过利用患者处方数据和扩展连接指纹（ECFP）进行模态转换，填补了重症监护设置中的研究空白。 |
| [^41] | [SoK: Facial Deepfake Detectors.](http://arxiv.org/abs/2401.04364) | 本文对最新的面部深度伪造检测器进行了全面回顾和分析，提供了对其有效性影响因素的深入见解，并在各种攻击场景中进行了评估。 |
| [^42] | [A Change Point Detection Integrated Remaining Useful Life Estimation Model under Variable Operating Conditions.](http://arxiv.org/abs/2401.04351) | 本研究提出了一种新的时间动态学习模型，用于在可变工况下检测设备的变点，并利用这些变点来提高剩余寿命估计的准确性。 |
| [^43] | [Private Fine-tuning of Large Language Models with Zeroth-order Optimization.](http://arxiv.org/abs/2401.04343) | 引入了DP-ZO，一种通过私有化零阶优化来保护大型语言模型训练数据隐私的方法。 |
| [^44] | [G-Meta: Distributed Meta Learning in GPU Clusters for Large-Scale Recommender Systems.](http://arxiv.org/abs/2401.04338) | 本文提出了一个用于大规模推荐系统中的GPU集群分布式元学习的高性能框架G-Meta，通过利用数据并行性和模型并行性以及设计高效的元-IO流水线，实现了高速分布式训练。 |
| [^45] | [Deep Efficient Private Neighbor Generation for Subgraph Federated Learning.](http://arxiv.org/abs/2401.04336) | 本文提出了FedDEP，用于解决子图联邦学习中的信息传播不完整的问题，并提出了一系列新颖的技术设计，包括深度邻居生成和高效的私密领域生成。 |
| [^46] | [Coupling Graph Neural Networks with Fractional Order Continuous Dynamics: A Robustness Study.](http://arxiv.org/abs/2401.04331) | 本文详细研究了图神经分数阶微分方程模型的鲁棒性，通过实施分数阶微积分，模型在特征更新过程中考虑了长期记忆，对抗性条件下的性能仍未得到广泛探究。 |
| [^47] | [Private Truly-Everlasting Robust-Prediction.](http://arxiv.org/abs/2401.04311) | 文章介绍了私人真正永恒的强大预测（PEP）模型，它通过提供对预测oracle的黑盒访问来实现不同ially private learning，保护了初始训练集和无尽分类查询的隐私。文章提出了对PEP定义的两个概念性改进，并展示了比以前的工作更好的新构建。具体而言，通过加入鲁棒性和私人雅克比机制，提供了可靠准确的预测。 |
| [^48] | [Advancing Deep Active Learning & Data Subset Selection: Unifying Principles with Information-Theory Intuitions.](http://arxiv.org/abs/2401.04305) | 本论文探索了基于信息论原则的主动学习和主动采样方面的数据子集选择技术，以提高深度学习模型的标签和训练效率。 |
| [^49] | [Setting the Record Straight on Transformer Oversmoothing.](http://arxiv.org/abs/2401.04301) | Transformers are not inherently low-pass filters as previously thought and whether they oversmooth or not depends on the eigenspectrum of their update equations. Based on the analysis, a simple way to parameterize the weights of the Transformer update equations is derived, allowing for control over its spectrum and preventing oversmoothing. |
| [^50] | [Universal Consistency of Wide and Deep ReLU Neural Networks and Minimax Optimal Convergence Rates for Kolmogorov-Donoho Optimal Function Classes.](http://arxiv.org/abs/2401.04286) | 本文扩展了之前的结果，证明了基于宽而深的ReLU神经网络和逻辑损失训练的分类规则具有普适一致性，并给出了一类概率测度条件下基于神经网络的分类器实现极小极限收敛速率的充分条件。 |
| [^51] | [A Fast Graph Search Algorithm with Dynamic Optimization and Reduced Histogram for Discrimination of Binary Classification Problem.](http://arxiv.org/abs/2401.04282) | 本研究提出了一种用于二分类问题的快速图搜索算法，通过动态优化和减少直方图的方法来提高区分结果。该算法在支持向量机模型的基础上应用，显著提高了真正例并减少了假正例。 |
| [^52] | [Predicting the structure of dynamic graphs.](http://arxiv.org/abs/2401.04280) | 本文提出了一种预测动态图结构的方法，利用时间序列方法预测未来时间点的节点度，并结合通量平衡分析方法获得未来图的结构，评估了该方法在合成和真实数据集上的实用性和适用性。 |
| [^53] | [Attention versus Contrastive Learning of Tabular Data -- A Data-centric Benchmarking.](http://arxiv.org/abs/2401.04266) | 本文通过广泛评估28个表格数据集，对比了注意力和对比学习方法与传统深度学习和机器学习方法在表格数据上的性能，结果表明需要以数据为中心进行评测，以解决性能差距问题。 |
| [^54] | [Explaining the Power of Topological Data Analysis in Graph Machine Learning.](http://arxiv.org/abs/2401.04250) | 在图机器学习中，拓扑数据分析（TDA）被赞赏其捕捉数据中复杂形状和结构的能力，其鲁棒性和可解释性得到验证，但在特定实验中未能显著提高现有方法的预测能力。 |
| [^55] | [Scalable Normalizing Flows Enable Boltzmann Generators for Macromolecules.](http://arxiv.org/abs/2401.04246) | 提出了一种新颖的流体架构来高效地学习蛋白质的构象分布，并通过利用2-Wasserstein损失平滑地实现了大分子的玻尔兹曼发生器的训练。 |
| [^56] | [A learning-based mathematical programming formulation for the automatic configuration of optimization solvers.](http://arxiv.org/abs/2401.04237) | 该论文提出了一种基于机器学习和优化的方法，用于自动配置优化求解器。通过学习已解决的实例和配置，构建了性能函数，并使用数学规划来寻找给定实例的最佳求解器配置。 |
| [^57] | [Towards a Machine Learning-Based Approach to Predict Space Object Density Distributions.](http://arxiv.org/abs/2401.04212) | 我们提出了一种基于机器学习的模型来预测空间物体的密度分布，以应对低地球轨道的拥塞并提升对空间环境的可持续性。 |
| [^58] | [Curiosity & Entropy Driven Unsupervised RL in Multiple Environments.](http://arxiv.org/abs/2401.04198) | 本论文提出了一种无指导的强化学习方法，在多个环境下通过好奇心和熵驱动实现了性能的改进。 |
| [^59] | [Dense Hopfield Networks in the Teacher-Student Setting.](http://arxiv.org/abs/2401.04191) | 密集化霍普菲尔德网络在师生模式下的研究揭示了铁磁相学习和原型学习的特点，同时发现在特定条件下的关键训练集大小。此外，研究还表明学生比教师具有更广泛的容忍度。 |
| [^60] | [FlopPITy: Enabling self-consistent exoplanet atmospheric retrievals with machine learning.](http://arxiv.org/abs/2401.04168) | 本研究利用机器学习算法实现了自洽的外行星大气召回，加快了召回速度并允许使用更复杂的大气模型进行召回。 |
| [^61] | [Efficient Selective Audio Masked Multimodal Bottleneck Transformer for Audio-Video Classification.](http://arxiv.org/abs/2401.04154) | 这项研究提出了一种高效选择性音频掩蔽多模态瓶颈Transformer用于音视频分类，通过音视频Transformer提取时空表示并结合自监督目标进行训练，实现了有效的多模态学习和语义音频活动的学习。 |
| [^62] | [Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning.](http://arxiv.org/abs/2401.04151) | 本论文提出了一种名为LoRA链（COLA）的迭代优化框架，通过残差学习过程将LoRA模块与预训练的语言模型参数合并，并重新初始化新的LoRA模块的优化过程，从而实现LoRA和全参数微调之间的平衡。 |
| [^63] | [Online Test-Time Adaptation of Spatial-Temporal Traffic Flow Forecasting.](http://arxiv.org/abs/2401.04148) | 本研究首次研究了在线测试时间适应技术在时空交通流量预测中的应用，提出了一种自适应的双重校正方法，通过对模型输出的分解和校正来提高预测的准确性。 |
| [^64] | [Learn Once Plan Arbitrarily (LOPA): Attention-Enhanced Deep Reinforcement Learning Method for Global Path Planning.](http://arxiv.org/abs/2401.04145) | LOPA是一种增强注意力的深度强化学习方法，用于解决全局路径规划中的收敛性和泛化性等挑战。 |
| [^65] | [Robust Calibration For Improved Weather Prediction Under Distributional Shift.](http://arxiv.org/abs/2401.04144) | 本研究通过混合专家模型、数据增强和鲁棒后校准方法，相比提升树模型，使用深度神经网络在天气预测中实现更准确和更好校准的结果。 |
| [^66] | [On The Potential of The Fractal Geometry and The CNNs Ability to Encode it.](http://arxiv.org/abs/2401.04141) | 本研究探讨分形几何的潜力和深度学习网络对其编码能力的限制。通过相关性分析实验和人类评估，我们发现深度网络不能提取复杂且高层次的分形特征。然而，在需要对象结构对分类任务至关重要的应用中，分形特征表现出很好的有效性。 |
| [^67] | [CCNETS: A Novel Brain-Inspired Approach for Enhanced Pattern Recognition in Imbalanced Datasets.](http://arxiv.org/abs/2401.04139) | CCNETS是一种新颖的脑启发方法，通过模拟大脑的信息处理，通过生成高质量的数据集来增强不平衡数据集中的模式识别，特别关注处理机器学习中的不平衡数据集的挑战。 |
| [^68] | [Global-Aware Enhanced Spatial-Temporal Graph Recurrent Networks: A New Framework For Traffic Flow Prediction.](http://arxiv.org/abs/2401.04135) | 该论文提出了一个名为GA-STGRN的新的交通流量预测框架，通过结合图卷积网络和循环神经网络来进行时空建模。该框架中的一个关键创新是引入了全局感知层来帮助捕捉全局信息。另外，为了建模非固定的图结构和捕捉局部特征，还提出了一个序列感知的图神经网络。 |
| [^69] | [SynHIN: Generating Synthetic Heterogeneous Information Network for Explainable AI.](http://arxiv.org/abs/2401.04133) | 该论文提出了一种生成合成异构信息网络的方法，用于可解释人工智能。该方法通过识别现实世界数据集中的模式，构建合成网络，并确保生成的合成图数据与真实数据接近。这提供了用于节点分类任务的合成异构图数据集。 |
| [^70] | [Unsupervised Test-Time Adaptation via Plug-and-Play Transformer Modules.](http://arxiv.org/abs/2401.04130) | 这项工作介绍了PLUTO:一种插拔式模块化的测试时领域适应策略，通过预先训练一系列针对不同源领域的模块，有效地创建了一个"模块存储库"。采用无监督的测试时自适应方法，从存储库中选择稀疏的相关模块的子集，并创建选中模块的加权组合，实现了对新领域的自适应。 |
| [^71] | [DeepPhysiNet: Bridging Deep Learning and Atmospheric Physics for Accurate and Continuous Weather Modeling.](http://arxiv.org/abs/2401.04125) | DeepPhysiNet框架将物理定律纳入深度学习模型中，实现了准确和连续的天气系统模拟。 |
| [^72] | [Why is the User Interface a Dark Pattern? : Explainable Auto-Detection and its Analysis.](http://arxiv.org/abs/2401.04119) | 本研究通过使用BERT模型进行自动检测和LIME、SHAP等解释技术进行解释，揭示了黑暗模式中影响预测的关键术语，为用户提供防范黑暗模式的见解。 |
| [^73] | [Timeline-based Process Discovery.](http://arxiv.org/abs/2401.04114) | 本文提出了一种基于时间线的流程发现方法，能够明确表示时间轴，该方法在两个BPIC数据集和一个专有数据集上的评估中展现出与标准布局技术相比的优势。 |
| [^74] | [A Primer on Temporal Graph Learning.](http://arxiv.org/abs/2401.03988) | 本文介绍了时间图学习的基本知识，包括TGL框架中的重要概念、相关的学习架构以及经典的时间序列预测方法，为TGL的可解释学习解决方案提供了灵感。 |
| [^75] | [Lessons Learned: Reproducibility, Replicability, and When to Stop.](http://arxiv.org/abs/2401.03736) | 本论文从重现产品预测热带气旋生成的经验中提炼出的教训，提出了一个二维框架来指导外部研究的再现和复制。该框架结合了数据集、度量指标和模型三个关键方面，并为研究人员在自己的研究中应用前期工作和指导研究方向提供了工具。 |
| [^76] | [Generalized Lagrangian Neural Networks.](http://arxiv.org/abs/2401.03728) | 本文介绍了对拉格朗日神经网络进行了开创性扩展的广义拉格朗日神经网络，通过将广义拉格朗日方程运用于模型构建，提高了预测准确性并保证了拉格朗日方程的成立。 |
| [^77] | [Token-free LLMs Can Generate Chinese Classical Poetry with More Accurate Format.](http://arxiv.org/abs/2401.03512) | 本文提出了一种无需分词的大型语言模型（LLMs）来生成中国古典诗词，并解决了格式不准确性的问题。验证了现有基于分词的模型在字符和分词之间的关系方面的知识有限，并展示了如何通过定制模型解决这一问题。 |
| [^78] | [Multi-Modal Representation Learning for Molecular Property Prediction: Sequence, Graph, Geometry.](http://arxiv.org/abs/2401.03369) | 这篇论文提出了一种新颖的多模态表示学习模型SGGRL，将序列、图形和几何特征整合在一起，用于分子性质预测。 |
| [^79] | [On Sample-Efficient Offline Reinforcement Learning: Data Diversity, Posterior Sampling, and Beyond.](http://arxiv.org/abs/2401.03301) | 本文提出了通过数据多样性概念来统一离线强化学习算法的方法，并证明了基于版本空间、正则化优化和后验采样的算法在标准假设下达到了可比的样本效率。 |
| [^80] | [Fair Sampling in Diffusion Models through Switching Mechanism.](http://arxiv.org/abs/2401.03140) | 本论文提出了一种称为“属性切换”的公平抽样机制，用于解决扩散模型中公平性的问题。通过在生成的数据中混淆敏感属性，该方法能够实现生成公平数据和保持数据效用的目标。 |
| [^81] | [The Tactician's Web of Large-Scale Formal Knowledge.](http://arxiv.org/abs/2401.02950) | The Tactician's Web是一个大规模形式化数学知识网络，通过Coq证明助手和丰富的数据表示与证明工程师进行交互，并提供了机器学习、分析和证明工程的实用工具。 |
| [^82] | [Graph2Tac: Learning Hierarchical Representations of Math Concepts in Theorem proving.](http://arxiv.org/abs/2401.02949) | 本文提出了一种名为Graph2Tac的图神经网络模型，用于在定理证明中学习数学概念的分层表示。该模型能够动态地将新的数学概念纳入到知识库中，并在Coq证明助手中进行训练和应用。 |
| [^83] | [Weakly Semi-supervised Tool Detection in Minimally Invasive Surgery Videos.](http://arxiv.org/abs/2401.02791) | 本研究提出了一种在微创手术视频中弱半监督下检测手术工具的方法。通过使用共现损失来利用图像级标签中工具对之间的共现关系，平衡了注释负担和检测性能，克服了分类困难。 |
| [^84] | [View-based Explanations for Graph Neural Networks.](http://arxiv.org/abs/2401.02086) | 这篇论文提出了一种基于视图的解释方法来解释图神经网络(GNNs)的行为，通过生成解释视图和图模式来解释特定类别的结果。 |
| [^85] | [GPS-SSL: Guided Positive Sampling to Inject Prior Into Self-Supervised Learning.](http://arxiv.org/abs/2401.01990) | GPS-SSL是一种将先验知识注入到自监督学习中的通用方法，通过设计度量空间并利用最近邻采样生成正样本。它可以减少对强数据增强的依赖，因此在Cifar10上达到了更好的效果。 |
| [^86] | [Multilingual Instruction Tuning With Just a Pinch of Multilinguality.](http://arxiv.org/abs/2401.01854) | 本研究研究了多语言指令调优中的多语言性对跨语言指令遵循的影响。研究发现，即使在单语调优过程中，许多语言也可以将一些指令遵循能力转移到其他语言上。此外，只有40个多语言示例能够显著提高多语言指令遵循。总体来说，多语言混合调优的模型在多种语言上的表现相比单语调优的模型要好或者不相上下，尽管使用的这些语言的训练示例数量只有10倍少。 |
| [^87] | [A Comprehensive Study of Knowledge Editing for Large Language Models.](http://arxiv.org/abs/2401.01286) | 本研究全面研究了大型语言模型的知识编辑，旨在有效修改模型的行为，同时保持整体性能。 |
| [^88] | [Federated Class-Incremental Learning with New-Class Augmented Self-Distillation.](http://arxiv.org/abs/2401.00622) | 该论文提出了一种具有增量学习和自蒸馏的联邦化分类学习方法，通过丰富历史模型的类别分数并利用结合的知识进行自蒸馏，实现了更充分精确的知识传递。 |
| [^89] | [Isolated pulsar population synthesis with simulation-based inference.](http://arxiv.org/abs/2312.14848) | 本论文使用模拟推断方法结合脉冲星种群合成，来限制孤立银河射电脉冲星的磁旋转特性。 |
| [^90] | [Time-Transformer: Integrating Local and Global Features for Better Time Series Generation.](http://arxiv.org/abs/2312.11714) | 本文提出了一种新的时间序列生成模型，通过时间变换器同时学习本地和全局特征，实现了对时间序列数据的更好生成能力。 |
| [^91] | [STEAM & MoSAFE: SOTIF Error-and-Failure Model & Analysis for AI-Enabled Driving Automation.](http://arxiv.org/abs/2312.09559) | 本文通过定义SOTIF时间误差与故障模型(STEAM)，填补了现有标准中在识别和评估AI引起的危险错误方面的不足，以更好地评估AI驱动的驾驶自动化系统中的安全风险。 |
| [^92] | [Convergence Rates for Stochastic Approximation: Biased Noise with Unbounded Variance, and Applications.](http://arxiv.org/abs/2312.02828) | 本论文研究了带有无界方差的有偏噪声对随机逼近算法的收敛速度的影响，并介绍了该算法在各个领域的应用。 |
| [^93] | [Advanced Large Language Model (LLM)-Driven Verilog Development: Enhancing Power, Performance, and Area Optimization in Code Synthesis.](http://arxiv.org/abs/2312.01022) | 本研究介绍了先进语言模型（ALMs）在Verilog编程中的应用，通过采用创新的框架和双阶段细化协议，能够提高代码的精确性和与功耗-性能-面积（PPA）基准的对齐性。 |
| [^94] | [Deep Interactive Segmentation of Medical Images: A Systematic Review and Taxonomy.](http://arxiv.org/abs/2311.13964) | 本综述系统地总结了医学图像交互式分割的最新研究进展，通过深度学习的方法推动了领域发展，但目前存在方法之间的比较缺乏的问题。 |
| [^95] | [LLMs cannot find reasoning errors, but can correct them!.](http://arxiv.org/abs/2311.08516) | 本文研究了LLMs在自我纠正过程中的错误发现和输出纠正两个核心组成部分。研究发现LLMs通常难以发现逻辑错误，但通过使用回溯方法可以在提供错误位置信息时获得大幅改进。 |
| [^96] | [Understanding Deep Representation Learning via Layerwise Feature Compression and Discrimination.](http://arxiv.org/abs/2311.02960) | 本文通过研究中间特征的结构，揭示了深度网络在层级特征学习过程中的演化模式。研究发现线性层在特征学习中起到了与深层非线性网络类似的作用。 |
| [^97] | [PolyThrottle: Energy-efficient Neural Network Inference on Edge Devices.](http://arxiv.org/abs/2310.19991) | PolyThrottle是一种在边缘设备上进行节能神经网络推理的解决方案，通过优化设备上的硬件元素配置，可以达到高达36%的能量节省，并且能够快速收敛到近乎最优的设置。 |
| [^98] | [Causal-structure Driven Augmentations for Text OOD Generalization.](http://arxiv.org/abs/2310.12803) | 本文提出了一种基于因果结构的反事实数据增强方法，用于改善文本分类器在应用中的泛化效果，特别适用于存在虚假相关性的标签与属性预测问题。 |
| [^99] | [CORN: Co-Trained Full-Reference And No-Reference Audio Metrics.](http://arxiv.org/abs/2310.09388) | CORN是一个新颖的框架，将全参考和非参考音频度量结合起来，并尝试在训练时同时训练这两种模型。CORN FR模式同时具备全参考和非参考度量的性能。 |
| [^100] | [Risk Assessment and Statistical Significance in the Age of Foundation Models.](http://arxiv.org/abs/2310.07132) | 本论文提出了一个分布框架，用于评估具有统计显著性的基础模型的风险。通过一种新的统计相对测试方法，该框架结合了一阶和二阶随机优势，并借鉴了计量经济学和数学金融中常用的平均风险模型。在给定指定度量量化的防护栏的情况下，我们还开发了一种基于风险意识的基础模型选择方法。受数学金融中的投资组合优化和选择理论的启发，我们为每个模型定义了一个"度量组合"，并根据这些组合的随机优势进行模型选择。 |
| [^101] | [FABind: Fast and Accurate Protein-Ligand Binding.](http://arxiv.org/abs/2310.06763) | FABind是一个结合了口袋预测和对接的端到端模型，旨在实现快速准确的蛋白-配体结合预测。 |
| [^102] | [Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design.](http://arxiv.org/abs/2310.04343) | 本研究提出了NAEPro模型，通过函数几何引导的方法共同设计蛋白质序列和结构。实验结果表明，该模型在氨基酸恢复率、TM分数和RMSD等指标上表现出色。 |
| [^103] | [Homotopy Relaxation Training Algorithms for Infinite-Width Two-Layer ReLU Neural Networks.](http://arxiv.org/abs/2309.15244) | 本文提出了一种名为同伦松弛训练算法（HRTA）的新的训练方法，它通过构建无缝连接线性激活函数和ReLU激活函数的同伦激活函数，并松弛同伦参数以增强训练精细化过程，加速了训练过程，在神经切线核（NTK）的背景下，实现了显著改进的收敛速度，并展示了对其他激活函数和深度神经网络的潜力。 |
| [^104] | [Auto-grading C programming assignments with CodeBERT and Random Forest Regressor.](http://arxiv.org/abs/2309.15216) | 本研究提供了使用机器学习和深度学习方法对C编程作业进行自动评分的分析，并引入了基于代码的转换器词嵌入模型CodeBERT。测试结果表明该策略的有效性，并讨论了统计方法和深度学习技术之间的对比。 |
| [^105] | [BiSinger: Bilingual Singing Voice Synthesis.](http://arxiv.org/abs/2309.14089) | BiSinger是一个双语合成歌声系统，通过设计共享表示、融合数据集和使用开源技术，实现了一种可以进行英语和汉语普通话混合编码歌声合成的单一模型，并保持了汉语歌曲的表现。 |
| [^106] | [Understanding Deep Gradient Leakage via Inversion Influence Functions.](http://arxiv.org/abs/2309.13016) | 本文提出了一种新的方法I²F，可以有效近似深度梯度泄露攻击，并建立了恢复图像和私有梯度之间的连接。通过这个方法，我们能够更好地理解和应对深度梯度泄露攻击。 |
| [^107] | [FedDCSR: Federated Cross-domain Sequential Recommendation via Disentangled Representation Learning.](http://arxiv.org/abs/2309.08420) | 提出了一种名为FedDCSR的联邦跨领域顺序推荐框架，通过解缠表示学习来处理不同领域之间的序列特征异质性，并保护数据隐私。 |
| [^108] | [Long-term drought prediction using deep neural networks based on geospatial weather data.](http://arxiv.org/abs/2309.06212) | 基于地理气象数据的深度神经网络用于长期干旱预测，提出了一种端到端解决方案来预测特定地区干旱概率。采用卷积LSTM和Transformer模型相比其他基线模型能够获得更高的准确性。 |
| [^109] | [s-ID: Causal Effect Identification in a Sub-Population.](http://arxiv.org/abs/2309.02281) | 该论文介绍了在子群体中进行因果效应识别的问题，提出并倡导了s-ID问题。论文提供了必要和充分条件，以便从子群体的观测分布中识别出因果效应。 |
| [^110] | [Price-Discrimination Game for Distributed Resource Management in Federated Learning.](http://arxiv.org/abs/2308.13838) | 本论文提出了一种价格差异化游戏（PDG），通过对不同客户提供的服务进行定价差异化，改善了联合学习的性能并降低了激励客户参与联合学习的成本。 |
| [^111] | [Dynamic algorithms for k-center on graphs.](http://arxiv.org/abs/2307.15557) | 本文提出了针对动态图中的k中心问题的高效算法，包括确定性递减的（2+ε）近似算法和随机增量的（4+ε）近似算法，同时给出了针对加权图的摊销更新时间为kn^{o(1)}的算法。此外，通过简化方法得到了对于k中心问题的全动态（2+ε）近似算法。 |
| [^112] | [Attention to Entropic Communication.](http://arxiv.org/abs/2307.11423) | 该论文研究了在通信理论中结合注意力和相对熵的概念。研究发现，在通信中使用注意力导向的加权相对熵是不适当的，而适当的注意力通信可通过发送者仅需要了解接收者的效用函数来实现最佳通知。 |
| [^113] | [Reinforcement Learning for Photonic Component Design.](http://arxiv.org/abs/2307.11075) | 本文提出了一种新的基于强化学习的fab-in-the-loop算法，用于光子器件设计，并成功将插入损耗降低至3.24 dB，并实现了在150纳米带宽下不到10.2 dB的损耗。 |
| [^114] | [Online Laplace Model Selection Revisited.](http://arxiv.org/abs/2307.06093) | 本研究重新推导了在线 Laplace 方法，并将其目标定位为模态修正的变分上界，避免了对平稳性的假设。通过使用全批量梯度的在线算法，我们演示了在实践中实现了这些最优点，并验证了其适用性。 |
| [^115] | [Multigrid-Augmented Deep Learning for the Helmholtz Equation: Better Scalability with Compact Implicit Layers.](http://arxiv.org/abs/2306.17486) | 通过结合多网格求解器和卷积神经网络，该论文提出了一种用于解决离散异质Helmholtz方程的迭代深度学习方法，在可伸缩性和求解速度上优于传统方法。其中的三个主要创新包括引入隐式层来解决CNN中的视野问题、改进CNN预条件技术以提高性能，并提出了一种多尺度训练方法使网络能够处理不同尺寸的问题。 |
| [^116] | [Wind Noise Reduction with a Diffusion-based Stochastic Regeneration Model.](http://arxiv.org/abs/2306.12867) | 本文介绍了一种基于扩散的随机再生模型的风噪声降噪方法，该方法使得风噪声降噪效果优于其他基于神经网络的方法和纯预测和生成模型，在使用模拟和真实记录的风噪声数据集上进行了测试，并在真实记录的风噪声数据集上具有很好的泛化性能。 |
| [^117] | [Clarify Confused Nodes Through Separated Learning.](http://arxiv.org/abs/2306.02285) | 本文提出了使用邻域混淆度量来分离学习解决图神经网络中混淆节点的问题。这种方法可以更可靠地区分异质节点和同质节点，并改善性能。 |
| [^118] | [Union Subgraph Neural Networks.](http://arxiv.org/abs/2305.15747) | 本文提出了一种新型图神经网络UnionSNN，注入了邻居连接信息，通过联合子图来编码高阶连接性，实验证明其在节点分类任务中优于1-WL和当前最先进的GNN模型。 |
| [^119] | [Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design.](http://arxiv.org/abs/2305.13035) | 本研究通过改进缩放定律方法推测出计算-优化模型形状，成功实现了形状优化视觉变换器SoViT，该模型在相同计算量下，取得了与超过其两倍大小的模型相竞争的结果。 |
| [^120] | [Variational Classification.](http://arxiv.org/abs/2305.10406) | 提出一种新的变分分类方法，通过引入潜变量建模来优化训练，允许灵活的设计选择以改善校准和对抗鲁棒性，实验结果表明其对于域外数据的分类准确性得到了保持。 |
| [^121] | [FedNC: A Secure and Efficient Federated Learning Method Inspired by Network Coding.](http://arxiv.org/abs/2305.03292) | 本文提出FedNC，一个联合学习的通信框架，结合了网络编码技术，能够提高系统的隐私、吞吐量和鲁棒性。 |
| [^122] | [Multi-Source to Multi-Target Decentralized Federated Domain Adaptation.](http://arxiv.org/abs/2304.12422) | 本文提出了一种分布式联邦学习方法，可将机器学习模型从标记数据丰富的设备转移到未标记数据设备以提高数据利用率。该方法考虑了设备分类和源-目标链接形成的权衡。 |
| [^123] | [Auditing and Generating Synthetic Data with Controllable Trust Trade-offs.](http://arxiv.org/abs/2304.10819) | 本论文提出了一个审计框架，能够以全面的方式评估合成数据和AI模型的具体效果，包括偏见和歧视预防、对真实数据的忠实程度、效用、鲁棒性和隐私保护。在多个用例中，审计框架平衡了信任和效用之间的权衡。 |
| [^124] | [Optimal rates of approximation by shallow ReLU$^k$ neural networks and applications to nonparametric regression.](http://arxiv.org/abs/2304.01561) | 本文研究了Shallow ReLU$^k$神经网络的逼近容量，证明了其最优逼近速率。应用到非参数回归上，证明了浅层神经网络可以实现学习H\"older函数的最优渐进速率，补充了深层神经网络的最近结果。 |
| [^125] | [Conformal Prediction Regions for Time Series using Linear Complementarity Programming.](http://arxiv.org/abs/2304.01075) | 本文提出了一种基于优化的方法，通过将预测误差参数化为多个时间步长，以找到不保守的预测区间，实现在使用学习启用的时间序列预测器进行长期规划和验证。 |
| [^126] | [Handling Long and Richly Constrained Tasks through Constrained Hierarchical Reinforcement Learning.](http://arxiv.org/abs/2302.10639) | 本文通过约束层次强化学习的机制解决了长期和丰富约束的任务，在机器人清洁房屋的场景中展示了良好的性能。 |
| [^127] | [Non-separable Covariance Kernels for Spatiotemporal Gaussian Processes based on a Hybrid Spectral Method and the Harmonic Oscillator.](http://arxiv.org/abs/2302.09580) | 该论文提出了一种基于混合谱方法和谐振子的非可分离协方差核的时空高斯过程研究，通过物理论证推导出一类新型的非可分离协方差核，能更好地捕捉观测到的时空相关性。 |
| [^128] | [Two-Stage Constrained Actor-Critic for Short Video Recommendation.](http://arxiv.org/abs/2302.01680) | 本论文提出了一种两阶段有约束的演员-评论家算法，用于解决短视频推荐问题。通过将短视频推荐问题建模为约束马尔可夫决策过程，我们解决了在用户交互和多样的响应中优化累计观看时间的问题。进行了两阶段的策略学习，并且能够同时满足主要目标和辅助目标的约束。 |
| [^129] | [A Data-Driven Gaussian Process Filter for Electrocardiogram Denoising.](http://arxiv.org/abs/2301.02607) | 此论文提出了一种基于数据驱动的高斯过程滤波器，用于心电图去噪。通过使用心电图相位域和高斯分布假设，简化了计算过程，实现了无主观超参数的高效滤波器。 |
| [^130] | [General-Purpose In-Context Learning by Meta-Learning Transformers.](http://arxiv.org/abs/2212.04458) | 本文展示了Transformer和其他黑盒模型可以通过元学习训练成为通用的上下文学习器，该模型可以在各种问题上进行测试集预测，无需定义推理模型、训练损失或优化算法。 |
| [^131] | [Distribution Free Prediction Sets for Node Classification.](http://arxiv.org/abs/2211.14555) | 本文利用近期在设定预测下的进展，构建了预测集以适应归纳学习场景下的节点分类，证明了提供了比简单的符合预测应用更加紧致和良好校准的预测集。 |
| [^132] | [DyG2Vec: Representation Learning for Dynamic Graphs with Self-Supervision.](http://arxiv.org/abs/2210.16906) | DyG2Vec是一个具有自监督学习能力的动态图表征学习模型，采用了高效而有效的注意力编码器和非对比自监督学习方法，能够提取丰富的时间嵌入表示。在基准数据集上的实验结果表明，该模型在未来链接预测任务中表现出色。 |
| [^133] | [Learning image representations for anomaly detection: application to discovery of histological alterations in drug development.](http://arxiv.org/abs/2210.07675) | 该论文提出了一种基于CNN的异常检测系统，通过对健康组织进行辅助任务训练，使表示适应组织中的相关细节，实现对组织学图像中的异常情况检测。 |
| [^134] | [LL-GNN: Low Latency Graph Neural Networks on FPGAs for High Energy Physics.](http://arxiv.org/abs/2209.14065) | 本文提出了一种新型的基于FPGA的低延迟图神经网络(LL-GNN)架构，针对粒子探测器领域的特殊需求，通过外积矩阵乘法方法、结构化邻接矩阵和列主数据布局等优化措施，实现了亚微秒级别的网络部署，并提供了一种GNN特定的算法-硬件协同设计方法。 |
| [^135] | [Exploiting Cultural Biases via Homoglyphs in Text-to-Image Synthesis.](http://arxiv.org/abs/2209.08891) | 研究表明，在文本到图像合成过程中，通过插入异形字，模型会反映生成图片中的文化刻板印象和偏见。这一现象的根本原因是模型的文本编码器。而恶意用户或服务提供商还可能利用类似外形的非拉丁字符，故意引入偏见，创造种族主义刻板印象。 |
| [^136] | [On the Evolution of A.I. and Machine Learning: Towards a Meta-level Measuring and Understanding Impact, Influence, and Leadership at Premier A.I. Conferences.](http://arxiv.org/abs/2205.13131) | 本研究旨在理解人工智能和机器学习的演变，通过衡量研究者在该领域的影响、影响力和领导力，并通过分析在主要人工智能会议上发表的论文，揭示了人工智能领域的发展和演变。人工智能的发展导致了学术论文数量的增加，本研究构建了全面的引用和合作数据集，对相关关系进行了计算。 |
| [^137] | [Diverse super-resolution with pretrained deep hiererarchical VAEs.](http://arxiv.org/abs/2205.10347) | 本文提出了使用预训练的深度分层VAE作为先验，通过结合低分辨率编码器和预训练的生成模型进行图像超分辨率处理。在人脸超分辨率任务中，我们的方法在计算效率和样本质量之间取得了有利的权衡。 |
| [^138] | [Lifelong Ensemble Learning based on Multiple Representations for Few-Shot Object Recognition.](http://arxiv.org/abs/2205.01982) | 本文提出了一种基于多重表示的终身集成学习方法，用于开放式场景下的少样本目标识别问题，适用于三维物体类别数量不固定、随时间增长的场景。模型针对各种类型物体进行处理，并进行了广泛实验。 |
| [^139] | [Molecule Generation for Drug Design: a Graph Learning Perspective.](http://arxiv.org/abs/2202.09212) | 本论文调研了药物设计领域中基于图学习的分子生成方法，分为一步到位法、基于片段法和节点逐个法三类，介绍了公共数据集和评估指标，并讨论了未来研究的挑战和方向。 |
| [^140] | [Weighted Isolation and Random Cut Forest Algorithms for Anomaly Detection.](http://arxiv.org/abs/2202.01891) | 该论文提出了加权孤立森林（WIF）和加权随机切割森林（WRCF）算法，用于改进异常检测，特别是时间序列数据的异常检测。这些算法利用考虑整体数据结构的权重策略确定分割值，相比原有算法表现出更好的性能。 |
| [^141] | [Privacy-Preserving Logistic Regression Training with A Faster Gradient Variant.](http://arxiv.org/abs/2201.10838) | 本文提出了一种更快的梯度变种——二次梯度，用于在同态加密领域实现隐私保护的逻辑回归训练，并成功提升了收敛速度，实现了同态逻辑回归训练仅需较少的迭代次数。 |
| [^142] | [Convergence of stochastic gradient descent schemes for Lojasiewicz-landscapes.](http://arxiv.org/abs/2102.09385) | 本文研究了Lojasiewicz-landscape下的随机梯度下降方案的收敛性，证明了当SGD保持有界且具有可数个临界点，或目标函数满足Lojasiewicz不等式时，SGD将收敛。此外，在神经网络中使用一些特定的解析激活函数时，如果训练中的信号和响应是紧支撑的，SGD也将收敛。 |

# 详细

[^1]: 关于上下文信息对人类在人工智能协作中的委托行为的影响

    On the Effect of Contextual Information on Human Delegation Behavior in Human-AI collaboration. (arXiv:2401.04729v1 [cs.HC])

    [http://arxiv.org/abs/2401.04729](http://arxiv.org/abs/2401.04729)

    本研究探讨了在人工智能协作中提供上下文信息对人类委托行为的影响，发现提供上下文信息显著提高了人工智能与人类团队的表现，并且委托行为在不同上下文信息下发生显著变化。这项研究推进了对人工智能委托中人工智能与人类互动的理解，并为设计更有效的协作系统提供了见解。

    

    人工智能的不断增强能力为人工智能与人类的协作带来了新的可能性。利用现有的互补能力，让人们将个别实例委托给人工智能是一种有前景的方法。然而，使人们有效地委托实例需要他们评估自己和人工智能在给定任务的背景下的能力。在这项工作中，我们探讨了在人类决定将实例委托给人工智能时提供上下文信息的效果。我们发现，提供上下文信息显著提高了人工智能与人类团队的表现。此外，我们还表明，当参与者接收到不同类型的上下文信息时，委托行为会发生显著变化。总体而言，这项研究推进了人工智能委托中人工智能与人类互动的理解，并为设计更有效的协作系统提供了可行的见解。

    The constantly increasing capabilities of artificial intelligence (AI) open new possibilities for human-AI collaboration. One promising approach to leverage existing complementary capabilities is allowing humans to delegate individual instances to the AI. However, enabling humans to delegate instances effectively requires them to assess both their own and the AI's capabilities in the context of the given task. In this work, we explore the effects of providing contextual information on human decisions to delegate instances to an AI. We find that providing participants with contextual information significantly improves the human-AI team performance. Additionally, we show that the delegation behavior changes significantly when participants receive varying types of contextual information. Overall, this research advances the understanding of human-AI interaction in human delegation and provides actionable insights for designing more effective collaborative systems.
    
[^2]: U-Mamba:增强生物医学图像分割的长程依赖性

    U-Mamba: Enhancing Long-range Dependency for Biomedical Image Segmentation. (arXiv:2401.04722v1 [eess.IV])

    [http://arxiv.org/abs/2401.04722](http://arxiv.org/abs/2401.04722)

    U-Mamba是一个通用网络，通过设计混合的CNN-SSM块，将卷积层的局部特征提取能力与SSM的捕捉长程依赖性的能力结合，从而增强了生物医学图像分割的长程依赖性。

    

    卷积神经网络（CNN）和Transformer是生物医学图像分割中最流行的架构，但由于固有的局部性或计算复杂性，它们的处理长程依赖性的能力有限。为了解决这个挑战，我们介绍了U-Mamba，一个用于生物医学图像分割的通用网络。受到状态空间序列模型（SSM）的启发，SSM是一类以处理长序列能力强大而闻名的深度序列模型，我们设计了一个混合的CNN-SSM块，将卷积层的局部特征提取能力与SSM的捕捉长程依赖性的能力结合起来。此外，U-Mamba具有自配置机制，可以自动适应各种数据集，无需手动干预。我们在包括CT和MR图像中的3D腹部器官分割、内窥镜图像中的器械分割等四个不同任务上进行了大量实验。

    Convolutional Neural Networks (CNNs) and Transformers have been the most popular architectures for biomedical image segmentation, but both of them have limited ability to handle long-range dependencies because of inherent locality or computational complexity. To address this challenge, we introduce U-Mamba, a general-purpose network for biomedical image segmentation. Inspired by the State Space Sequence Models (SSMs), a new family of deep sequence models known for their strong capability in handling long sequences, we design a hybrid CNN-SSM block that integrates the local feature extraction power of convolutional layers with the abilities of SSMs for capturing the long-range dependency. Moreover, U-Mamba enjoys a self-configuring mechanism, allowing it to automatically adapt to various datasets without manual intervention. We conduct extensive experiments on four diverse tasks, including the 3D abdominal organ segmentation in CT and MR images, instrument segmentation in endoscopy imag
    
[^3]: 基于人工智能的全球范围内兰科群落保护状况的映射

    AI-based Mapping of the Conservation Status of Orchid Assemblages at Global Scale. (arXiv:2401.04691v1 [cs.LG])

    [http://arxiv.org/abs/2401.04691](http://arxiv.org/abs/2401.04691)

    本研究通过使用一个新的深度物种分布模型，对全球范围内兰科群落的保护状况进行了评估和映射，提出了衡量保护状况的两个指标，并展示了这些指标在世界范围内的变化和与受保护地区的关系。

    

    尽管生物多样性面临越来越多的威胁，但目前还没有准确的全球地图显示物种群落是否处于危险之中。本研究通过使用一个新的基于深度学习的物种分布模型，对标志性的兰科植物家族进行了全球范围内和以公里为单位的保护状况评估和映射，并对不同尺度的洞察力进行了讨论。我们训练了一个基于1百万个兰科物种14千个观察记录的深度物种分布模型，用于预测全球范围内和以公里为单位的兰科群落。我们提出了两个主要的群落保护状况指标：（一）受威胁物种的比例，和（二）群落中最受威胁物种的状态。我们展示并分析了这些指标在世界范围内的变化，以及它们与苏门答腊岛上当前受保护地区的关系。网上提供的全球和互动地图显示了兰科群落的保护状况指标，在所有尺度上存在明显的空间变化。

    Although increasing threats on biodiversity are now widely recognised, there are no accurate global maps showing whether and where species assemblages are at risk. We hereby assess and map at kilometre resolution the conservation status of the iconic orchid family, and discuss the insights conveyed at multiple scales. We introduce a new Deep Species Distribution Model trained on 1M occurrences of 14K orchid species to predict their assemblages at global scale and at kilometre resolution. We propose two main indicators of the conservation status of the assemblages: (i) the proportion of threatened species, and (ii) the status of the most threatened species in the assemblage. We show and analyze the variation of these indicators at World scale and in relation to currently protected areas in Sumatra island. Global and interactive maps available online show the indicators of conservation status of orchid assemblages, with sharp spatial variations at all scales. The highest level of threat 
    
[^4]: 混合多层次随机块模型用于多视角聚类

    Mixture of multilayer stochastic block models for multiview clustering. (arXiv:2401.04682v1 [cs.LG])

    [http://arxiv.org/abs/2401.04682](http://arxiv.org/abs/2401.04682)

    本文提出了一种混合多层次随机块模型的方法，用于聚合来自不同信息源的多个聚类，并将观测分区到不同的聚类中，考虑到它们在组件内的特定性。该方法在合成数据和全球食品贸易网络中得到了有趣的结果。

    

    在这项工作中，我们提出了一种用于聚合来自不同信息源的多个聚类的原始方法。每个分区由观测之间的共属性矩阵进行编码。我们的方法使用了混合多层次随机块模型（SBM）将具有相似信息的共属性矩阵分组为组件，并将观测分区到不同的聚类中，考虑到它们在组件内的特定性。模型参数的可识别性被建立，并提出了一个变分贝叶斯EM算法来估计这些参数。贝叶斯框架允许选择最优的聚类和组件数量。利用合成数据将提出的方法与共识聚类和基于张量的算法进行了比较，用于大规模复杂网络中的社区检测。最后，该方法被用于分析全球食品贸易网络，得到了有趣的结构。

    In this work, we propose an original method for aggregating multiple clustering coming from different sources of information. Each partition is encoded by a co-membership matrix between observations. Our approach uses a mixture of multilayer Stochastic Block Models (SBM) to group co-membership matrices with similar information into components and to partition observations into different clusters, taking into account their specificities within the components. The identifiability of the model parameters is established and a variational Bayesian EM algorithm is proposed for the estimation of these parameters. The Bayesian framework allows for selecting an optimal number of clusters and components. The proposed approach is compared using synthetic data with consensus clustering and tensor-based algorithms for community detection in large-scale complex networks. Finally, the method is utilized to analyze global food trading networks, leading to structures of interest.
    
[^5]: RoSA: 通过鲁棒适应实现准确的参数高效微调

    RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v1 [cs.CL])

    [http://arxiv.org/abs/2401.04679](http://arxiv.org/abs/2401.04679)

    RoSA是一种新的PEFT方法，通过在预训练权重上训练低秩和高度稀疏的组件，以高效近似完全微调的性能，来实现准确的参数高效微调。在多个生成任务中，RoSA表现出优于其他方法的性能。

    

    我们研究了在大语言模型 (LLMs) 的背景下，能够在有限的计算和内存预算下提供良好准确性的参数高效微调 (PEFT) 方法。我们提出了一种新的PEFT方法，称为RoSA，受鲁棒主成分分析 (PCA) 的启发，它在一组固定的预训练权重上共同训练$\textit{低秩}$和$\textit{高度稀疏}$的组件，以高效近似完全微调（FFT）解决方案的性能。我们展示了RoSA在一系列具有挑战性的生成任务上的性能，例如小学数学和SQL查询生成，这些任务需要进行微调以获得良好性能，我们证明了在相同的参数预算下，RoSA优于LoRA和纯粹的稀疏微调。我们通过稀疏GPU内核为RoSA提供系统支持，以补充训练算法，从而实现内存和计算效率的训练。我们的代码将在https://github.com/IST-DASLab上提供。

    We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memoryand computationally-efficient training. Our code will be made available at https://github.com/IST-DASLab
    
[^6]: 基于高斯Copula的迁移学习自动调优

    Transfer-Learning-Based Autotuning Using Gaussian Copula. (arXiv:2401.04669v1 [cs.LG])

    [http://arxiv.org/abs/2401.04669](http://arxiv.org/abs/2401.04669)

    该论文提出了一种基于高斯Copula的迁移学习自动调优方法，通过利用先前调优数据中搜索空间的高性能区域建模，为新任务生成高性能配置。

    

    随着多样化的高性能计算（HPC）系统的发展，应用程序有机会解决比以往更大的问题。鉴于这些HPC系统和应用程序调优的显著增加的复杂性，经验性能调优（如自动调优）近年来成为一种有前途的方法。尽管其有效性，自动调优往往是一种计算复杂度高的方法。基于传输学习（TL）的自动调优试图通过利用先前调优的数据来解决这个问题。目前TL方法为自动调优在建模参数配置和性能之间的关系上花费了大量时间，对于新任务的少样本（即，少数经验评估）调优是无效的。我们首次介绍了基于高斯Copula（GC）的生成式TL自动调优方法，通过对先前数据中搜索空间的高性能区域建模，为新任务生成高性能配置。

    As diverse high-performance computing (HPC) systems are built, many opportunities arise for applications to solve larger problems than ever before. Given the significantly increased complexity of these HPC systems and application tuning, empirical performance tuning, such as autotuning, has emerged as a promising approach in recent years. Despite its effectiveness, autotuning is often a computationally expensive approach. Transfer learning (TL)-based autotuning seeks to address this issue by leveraging the data from prior tuning. Current TL methods for autotuning spend significant time modeling the relationship between parameter configurations and performance, which is ineffective for few-shot (that is, few empirical evaluations) tuning on new tasks. We introduce the first generative TL-based autotuning approach based on the Gaussian copula (GC) to model the high-performing regions of the search space from prior data and then generate high-performing configurations for new tasks. This 
    
[^7]: ASSIRA猫狗数据集上各种预训练深度学习模型的基准分析

    Benchmark Analysis of Various Pre-trained Deep Learning Models on ASSIRA Cats and Dogs Dataset. (arXiv:2401.04666v1 [cs.CV])

    [http://arxiv.org/abs/2401.04666](http://arxiv.org/abs/2401.04666)

    本研究基于ASSIRA猫狗数据集，比较了使用不同优化器和损失函数的各种预训练模型，并通过改变超参数来提高模型准确性。

    

    作为深度学习的最基本应用和实现，图像分类已经越来越受欢迎。知名数据科学社区提供了各种数据集来对机器学习算法和预训练模型进行基准测试。ASSIRA猫狗数据集是其中之一，并且在这项研究中被使用，因为它的整体接受度和基准标准。通过使用不同类型的优化器和损失函数，对各种预训练模型进行了比较。改变超参数以获得模型的最佳结果。通过应用这种方法，我们在不对训练模型进行重大更改的情况下获得了更高的准确性。为了运行实验证明了该数据集以准确率超过先前的实验。

    As the most basic application and implementation of deep learning, image classification has grown in popularity. Various datasets are provided by renowned data science communities for benchmarking machine learning algorithms and pre-trained models. The ASSIRA Cats & Dogs dataset is one of them and is being used in this research for its overall acceptance and benchmark standards. A comparison of various pre-trained models is demonstrated by using different types of optimizers and loss functions. Hyper-parameters are changed to gain the best result from a model. By applying this approach, we have got higher accuracy without major changes in the training model. To run the experiment, we used three different computer architectures: a laptop equipped with NVIDIA GeForce GTX 1070, a laptop equipped with NVIDIA GeForce RTX 3080Ti, and a desktop equipped with NVIDIA GeForce RTX 3090. The acquired results demonstrate supremacy in terms of accuracy over the previously done experiments on this da
    
[^8]: 一种用于深度隐含物理模型泛化的新框架

    A novel framework for generalization of deep hidden physics models. (arXiv:2401.04648v1 [cs.LG])

    [http://arxiv.org/abs/2401.04648](http://arxiv.org/abs/2401.04648)

    这项工作提出了一种改进的隐含物理模型框架，可以泛化适应系统输入、参数和域的变化，并在系统发现中展现了潜力。

    

    在各种工程和工业应用中，对于系统的建模是一个常见的问题，其中完整的系统信息是未知的，要么是因为考虑到所有涉及的复杂物理是不可能的，要么是为了在可用资源的限制内考虑更简单的模型。最近在灰盒建模领域的进展，如深度隐含物理模型，通过结合数据和物理来解决这个问题。然而，对于大多数实际应用，模型的泛化能力是一个关键问题，因为为每个系统输入和参数的微小变化或域配置的修改重新训练模型可能导致模型在经济上不可行。在这项工作中，我们提出了一个对隐含物理模型思想的新改进，可以适应系统输入、参数和域的变化。我们还展示了这种方法在系统发现中的潜力，并帮助学习到了变化后系统输入、参数和域的隐藏物理规律。

    Modelling of systems where the full system information is unknown is an oft encountered problem for various engineering and industrial applications, as it's either impossible to consider all the complex physics involved or simpler models are considered to keep within the limits of the available resources. Recent advances in greybox modelling like the deep hidden physics models address this space by combining data and physics. However, for most real-life applications, model generalizability is a key issue, as retraining a model for every small change in system inputs and parameters or modification in domain configuration can render the model economically unviable. In this work we present a novel enhancement to the idea of hidden physics models which can generalize for changes in system inputs, parameters and domains. We also show that this approach holds promise in system discovery as well and helps learn the hidden physics for the changed system inputs, parameters and domain configurat
    
[^9]: 通过生成对抗网络推进先验可解释模型

    Advancing Ante-Hoc Explainable Models through Generative Adversarial Networks. (arXiv:2401.04647v1 [cs.CV])

    [http://arxiv.org/abs/2401.04647](http://arxiv.org/abs/2401.04647)

    本文提出了一种先验可解释模型，通过在主分类器网络中添加无监督的解释生成器和对抗训练的方式，实现了模型的可解释性和性能的提升。该方法通过训练解释模块提取视觉概念，同时使用生成对抗网络模块来区分生成的图像和真实图像。实验证明了该方法的鲁棒性，并展示了学到的概念与对象部分和视觉属性的语义一致性。

    

    本文提出了一种新的概念学习框架，用于增强视觉分类任务中模型的可解释性和性能。我们的方法将一个无监督的解释生成器添加到主分类器网络中，并利用对抗训练。在训练过程中，解释模块被优化以从分类器的潜在表示中提取视觉概念，而基于生成对抗网络的模块则旨在区分从概念中生成的图像和真实图像。这种联合训练方案使得模型能够将其内部学习到的概念与人可解释的视觉属性隐式地对齐。全面的实验证明了我们方法的鲁棒性，同时产生了连贯的概念激活。我们分析了学到的概念，展示了它们与对象部分和视觉属性之间的语义一致性。我们还研究了对抗训练协议中的扰动对分类和概念获取的影响。总之，本文通过生成对抗网络推进了先验可解释模型。

    This paper presents a novel concept learning framework for enhancing model interpretability and performance in visual classification tasks. Our approach appends an unsupervised explanation generator to the primary classifier network and makes use of adversarial training. During training, the explanation module is optimized to extract visual concepts from the classifier's latent representations, while the GAN-based module aims to discriminate images generated from concepts, from true images. This joint training scheme enables the model to implicitly align its internally learned concepts with human-interpretable visual properties. Comprehensive experiments demonstrate the robustness of our approach, while producing coherent concept activations. We analyse the learned concepts, showing their semantic concordance with object parts and visual attributes. We also study how perturbations in the adversarial training protocol impact both classification and concept acquisition. In summary, this 
    
[^10]: 将大型语言模型API应用于问题分类问题

    Applying Large Language Models API to Issue Classification Problem. (arXiv:2401.04637v1 [cs.SE])

    [http://arxiv.org/abs/2401.04637](http://arxiv.org/abs/2401.04637)

    本研究通过应用生成式预训练转换器（GPT）模型，提出一种可靠的自动化方法来解决问题报告的优先级排序问题，即使在较小的训练数据集下仍能保持可靠性，减少了对大量训练数据的依赖性。

    

    在软件工程中，问题报告的有效排序对于优化资源分配和及时解决关键问题至关重要。然而，手动对问题报告进行分类以进行排序是费力且缺乏可伸缩性的。相反，许多开源软件项目使用自动化流程解决此问题，尽管需要大量的数据集进行充分的训练。这项研究旨在设计一种自动化方法，在使用较小的数据集进行训练时仍能确保问题排序的可靠性。我们提出的方法利用生成式预训练转换器（GPT）的能力，认识到它们在处理这个任务时的高效性。通过利用这样的模型的能力，我们的目标是开发一个准确优先级问题报告的可靠系统，降低对大量训练数据的需求，同时保持可靠性。在我们的研究中，我们已经开发了一个可靠的基于GPT的方法来准确标记问题报告。

    Effective prioritization of issue reports is crucial in software engineering to optimize resource allocation and address critical problems promptly. However, the manual classification of issue reports for prioritization is laborious and lacks scalability. Alternatively, many open source software (OSS) projects employ automated processes for this task, albeit relying on substantial datasets for adequate training. This research seeks to devise an automated approach that ensures reliability in issue prioritization, even when trained on smaller datasets. Our proposed methodology harnesses the power of Generative Pre-trained Transformers (GPT), recognizing their potential to efficiently handle this task. By leveraging the capabilities of such models, we aim to develop a robust system for prioritizing issue reports accurately, mitigating the necessity for extensive training data while maintaining reliability. In our research, we have developed a reliable GPT-based approach to accurately labe
    
[^11]: 股票数据时间序列预测中的四元数神经网络

    Hypercomplex neural network in time series forecasting of stock data. (arXiv:2401.04632v1 [cs.NE])

    [http://arxiv.org/abs/2401.04632](http://arxiv.org/abs/2401.04632)

    本文测试了三种不同的神经网络架构，将其用于股票数据的时间序列预测。结果显示，具有超复数密集层的架构在准确性方面表现类似于其他架构，但可训练参数更少。此外，输入时间序列的顺序对有效性有影响。

    

    本文测试了三种用于时间序列预测的架构。它们的区别在于输入层包含卷积层、LSTM层或四元数4D代数的超复数层。输入是四个相关的股票市场时间序列，预测其中一个的结果。通过优化与架构类别相关的超参数，比较了最佳神经网络在类别内的表现。结果显示，在大多数情况下，具有超复数密集层的架构提供了与其他架构相似的MAE准确性，但可训练参数要少得多。由于这一点，超复数神经网络可以比其他测试的架构更快地学习和处理数据。此外，输入时间序列的顺序对有效性有影响。

    The three classes of architectures for time series prediction were tested. They differ by input layers which contain either convolutional, LSTM, or dense hypercomplex layers for 4D algebras. The input was four related Stock Market time series, and the prediction of one of them is expected. The optimization of hyperparameters related to the classes of architectures was performed in order to compare the best neural networks within the class. The results show that in most cases, the architecture with a hypercomplex dense layer provides similar MAE accuracy to other architectures, however, with considerably less trainable parameters. Thanks to it, hypercomplex neural networks can be learned and process data faster than the other tested architectures. Moreover, the order of the input time series has an impact on effectively.
    
[^12]: 用于水污染监测的深度强化多智能体学习框架和局部高斯过程

    Deep Reinforcement Multi-agent Learning framework for Information Gathering with Local Gaussian Processes for Water Monitoring. (arXiv:2401.04631v1 [cs.AI])

    [http://arxiv.org/abs/2401.04631](http://arxiv.org/abs/2401.04631)

    本文提出了一个深度强化多智能体学习框架，结合局部高斯过程，用于水污染监测。使用局部高斯过程准确建模不同空间相关性中的信息，并采用深度卷积策略决策方法来获得有效的监测策略。通过双重深度 Q 学习算法训练智能体以减小估计误差。

    

    为了有效监测水质，本文提出了一个由自主水面车组成的多智能体系统。为了实现对船队的安全控制，船队策略应该能够基于测量结果和船队状态进行行动。本文提出使用局部高斯过程和深度强化学习来联合获得有效的监测策略。局部高斯过程可以准确地建模不同空间相关性中的信息，从而更准确地捕捉水质信息。文中还提出了一种基于深度卷积策略的决策方法，通过观察这个模型的均值和方差，使用信息增益奖励进行决策。通过双重深度 Q 学习算法，智能体被训练以在安全的情况下尽量减小估计误差。

    The conservation of hydrological resources involves continuously monitoring their contamination. A multi-agent system composed of autonomous surface vehicles is proposed in this paper to efficiently monitor the water quality. To achieve a safe control of the fleet, the fleet policy should be able to act based on measurements and to the the fleet state. It is proposed to use Local Gaussian Processes and Deep Reinforcement Learning to jointly obtain effective monitoring policies. Local Gaussian processes, unlike classical global Gaussian processes, can accurately model the information in a dissimilar spatial correlation which captures more accurately the water quality information. A Deep convolutional policy is proposed, that bases the decisions on the observation on the mean and variance of this model, by means of an information gain reward. Using a Double Deep Q-Learning algorithm, agents are trained to minimize the estimation error in a safe manner thanks to a Consensus-based heuristi
    
[^13]: 无分布假设的神经标记时序点过程的无偏预测区间

    Distribution-Free Conformal Joint Prediction Regions for Neural Marked Temporal Point Processes. (arXiv:2401.04612v1 [cs.LG])

    [http://arxiv.org/abs/2401.04612](http://arxiv.org/abs/2401.04612)

    本文为神经标记时序点过程模型开发了可靠的不确定性量化方法，通过相容预测框架生成无分布假设的联合预测区间，并提供较好的边际覆盖保证。

    

    在连续时间的不规则间隔观测到的标记事件序列在各个领域中普遍存在。时序点过程（Temporal Point Processes，TPPs）提供了建模这些序列的数学框架，可以进行诸如预测未来事件的到达时间和相关标记等推断。然而，由于模型规范错误或缺乏训练数据，这些概率模型可能对未知的真实基础过程提供较差的近似，从中提取的预测区域可能是对基础不确定性的不可靠估计。本文基于相容预测框架，为神经TPP模型提供更可靠的不确定性量化方法。其中的一个主要目标是生成到达时间和标记的无分布假设的联合预测区间，并提供有限样本边际覆盖保证。一个关键挑战是同时处理严格正的连续响应和分类响应，而不需要分布假设。

    Sequences of labeled events observed at irregular intervals in continuous time are ubiquitous across various fields. Temporal Point Processes (TPPs) provide a mathematical framework for modeling these sequences, enabling inferences such as predicting the arrival time of future events and their associated label, called mark. However, due to model misspecification or lack of training data, these probabilistic models may provide a poor approximation of the true, unknown underlying process, with prediction regions extracted from them being unreliable estimates of the underlying uncertainty. This paper develops more reliable methods for uncertainty quantification in neural TPP models via the framework of conformal prediction. A primary objective is to generate a distribution-free joint prediction region for the arrival time and mark, with a finite-sample marginal coverage guarantee. A key challenge is to handle both a strictly positive, continuous response and a categorical response, withou
    
[^14]: 扩展分布对齐来实现弥散模型的后训练量化

    Enhanced Distribution Alignment for Post-Training Quantization of Diffusion Models. (arXiv:2401.04585v1 [cs.CV])

    [http://arxiv.org/abs/2401.04585](http://arxiv.org/abs/2401.04585)

    本文提出了一种扩展分布对齐方法以解决后训练量化对于弥散模型的分布不匹配问题，该方法在低延迟应用中具有较高的潜力，并且能有效提升性能。

    

    通过迭代噪声估计，扩散模型在图像生成任务中取得了巨大成功。然而，繁重的去噪过程和复杂的神经网络阻碍了它们在实际场景中的低延迟应用。量化可以有效降低模型复杂度，而后训练量化(PTQ)在加速去噪过程方面具有很高的潜力，并且不需要微调。不幸的是，我们发现由于不同去噪步骤中激活的高度动态分布，现有的扩散模型的PTQ方法在校准样本和重构输出两个层面上都存在分布不匹配的问题，导致性能远低于令人满意的水平，特别是在低位情况下。在本文中，我们提出了增强的分布对齐用于弥散模型的后训练量化(EDA-DM)来解决上述问题。具体来说，在校准样本层面，我们基于...[缺省]

    Diffusion models have achieved great success in image generation tasks through iterative noise estimation. However, the heavy denoising process and complex neural networks hinder their low-latency applications in real-world scenarios. Quantization can effectively reduce model complexity, and post-training quantization (PTQ), which does not require fine-tuning, is highly promising in accelerating the denoising process. Unfortunately, we find that due to the highly dynamic distribution of activations in different denoising steps, existing PTQ methods for diffusion models suffer from distribution mismatch issues at both calibration sample level and reconstruction output level, which makes the performance far from satisfactory, especially in low-bit cases. In this paper, we propose Enhanced Distribution Alignment for Post-Training Quantization of Diffusion Models (EDA-DM) to address the above issues. Specifically, at the calibration sample level, we select calibration samples based on the 
    
[^15]: 使用单一非自回归Transformer生成遮蔽音频

    Masked Audio Generation using a Single Non-Autoregressive Transformer. (arXiv:2401.04577v1 [cs.SD])

    [http://arxiv.org/abs/2401.04577](http://arxiv.org/abs/2401.04577)

    MAGNeT是一种遮蔽生成序列建模方法，使用单一非自回归Transformer生成具有高质量的音频，并引入了一种新颖的重新评分方法来提高生成音频的质量。同时，MAGNeT还探索了混合版本，可在自回归模式和非自回归模式下生成序列。在实验中证明MAGNeT在文本到音乐和文本到音频生成任务中具有高效性。

    

    我们介绍了一种名为MAGNeT的遮蔽生成序列建模方法，它直接操作多个音频令牌流。与以往的方法不同，MAGNeT由单阶段非自回归Transformer组成。在训练过程中，我们根据遮蔽计划器预测遮蔽令牌的范围，而在推断过程中，我们逐步构建输出序列使用多个解码步骤。为了进一步提高生成音频的质量，我们引入了一种新颖的重新评分方法，其中我们利用外部预训练模型来重新评分和排名MAGNeT的预测结果，这些结果将被用于后续的解码步骤。最后，我们探索了MAGNeT的混合版本，其中我们在自回归模式下生成前几秒钟，而其余的序列则以并行方式进行解码。我们展示了MAGNeT在文本到音乐和文本到音频生成任务中的效率，并进行了广泛的实验验证。

    We introduce MAGNeT, a masked generative sequence modeling method that operates directly over several streams of audio tokens. Unlike prior work, MAGNeT is comprised of a single-stage, non-autoregressive transformer. During training, we predict spans of masked tokens obtained from a masking scheduler, while during inference we gradually construct the output sequence using several decoding steps. To further enhance the quality of the generated audio, we introduce a novel rescoring method in which, we leverage an external pre-trained model to rescore and rank predictions from MAGNeT, which will be then used for later decoding steps. Lastly, we explore a hybrid version of MAGNeT, in which we fuse between autoregressive and non-autoregressive models to generate the first few seconds in an autoregressive manner while the rest of the sequence is being decoded in parallel. We demonstrate the efficiency of MAGNeT for the task of text-to-music and text-to-audio generation and conduct an extensi
    
[^16]: 自动游戏测试的稳健模仿学习

    Robust Imitation Learning for Automated Game Testing. (arXiv:2401.04572v1 [cs.LG])

    [http://arxiv.org/abs/2401.04572](http://arxiv.org/abs/2401.04572)

    提出了一个新的基于模仿学习的架构EVOLUTE，结合了行为克隆和能量模型，用于自动化游戏测试。该架构将自主代理的动作空间拆分为连续和离散任务，以优化控制和训练的效果。实验证明，在射击和驾驶游戏中，EVOLUTE表现出良好的性能。

    

    游戏开发是一个漫长的过程，在产品上市之前需要经过许多阶段。人工游戏测试是其中最耗时的，因为测试人员需要反复执行任务以寻找代码中的错误。因此，自动化测试被视为游戏行业的关键技术，它将显著提高开发成本和效率。为此，我们提出了EVOLUTE，一种新颖的基于模仿学习的架构，它将行为克隆（BC）与能量模型（EBM）相结合。EVOLUTE是一个两流模型，将自主代理的动作空间分为连续和离散任务。EBM流处理连续任务，以获得更精细和自适应的控制，而BC流处理离散动作，以方便训练。我们在一个射击和驾驶游戏中评估了EVOLUTE的性能，其中代理需要导航并持续识别目标进行攻击。

    Game development is a long process that involves many stages before a product is ready for the market. Human play testing is among the most time consuming, as testers are required to repeatedly perform tasks in the search for errors in the code. Therefore, automated testing is seen as a key technology for the gaming industry, as it would dramatically improve development costs and efficiency. Toward this end, we propose EVOLUTE, a novel imitation learning-based architecture that combines behavioural cloning (BC) with energy based models (EBMs). EVOLUTE is a two-stream ensemble model that splits the action space of autonomous agents into continuous and discrete tasks. The EBM stream handles the continuous tasks, to have a more refined and adaptive control, while the BC stream handles discrete actions, to ease training. We evaluate the performance of EVOLUTE in a shooting-and-driving game, where the agent is required to navigate and continuously identify targets to attack. The proposed mo
    
[^17]: HyperGANStrument: 使用无关音高超网络的乐器音声合成和编辑

    HyperGANStrument: Instrument Sound Synthesis and Editing with Pitch-Invariant Hypernetworks. (arXiv:2401.04558v1 [cs.SD])

    [http://arxiv.org/abs/2401.04558](http://arxiv.org/abs/2401.04558)

    本文介绍了HyperGANStrument方法，使用无关音高的超网络来调制预训练的GANStrument生成器，进一步提高了重构能力和音高准确性，增强了合成声音的可编辑性。

    

    GANStrument利用具有无关音高特征提取器和实例条件技术的GAN，在合成逼真的乐器声音方面显示出了显着的能力。为了进一步提高重构能力和音高准确性以增强用户提供声音的可编辑性，我们提出了HyperGANStrument，它引入了一个无关音高的超网络来调制预训练的GANStrument生成器的权重，给定一次性声音作为输入。超网络调制为生成器在重构输入声音方面提供了反馈。此外，我们利用对抗微调方案对超网络进行优化，以提高生成器的重构保真度和生成多样性。实验结果表明，所提出的模型不仅增强了GANStrument的生成能力，还显著提高了合成声音的可编辑性。在线演示页面提供音频示例。

    GANStrument, exploiting GANs with a pitch-invariant feature extractor and instance conditioning technique, has shown remarkable capabilities in synthesizing realistic instrument sounds. To further improve the reconstruction ability and pitch accuracy to enhance the editability of user-provided sound, we propose HyperGANStrument, which introduces a pitch-invariant hypernetwork to modulate the weights of a pre-trained GANStrument generator, given a one-shot sound as input. The hypernetwork modulation provides feedback for the generator in the reconstruction of the input sound. In addition, we take advantage of an adversarial fine-tuning scheme for the hypernetwork to improve the reconstruction fidelity and generation diversity of the generator. Experimental results show that the proposed model not only enhances the generation capability of GANStrument but also significantly improves the editability of synthesized sounds. Audio examples are available at the online demo page.
    
[^18]: 线性递归特征机器可以可靠地恢复低秩矩阵

    Linear Recursive Feature Machines provably recover low-rank matrices. (arXiv:2401.04553v1 [stat.ML])

    [http://arxiv.org/abs/2401.04553](http://arxiv.org/abs/2401.04553)

    该论文介绍了递归特征机器（RFMs）算法，它通过交替重新加权特征向量和在转换空间中学习预测函数来执行显式的特征学习。研究分析了RFM在稀疏线性回归和低秩矩阵恢复问题中的维数减少性能。

    

    机器学习中一个基本的问题是要理解神经网络如何准确预测，同时似乎避免了维数诅咒。一个可能的解释是神经网络的常见训练算法隐含地进行维数减少 - 一个被称为特征学习的过程。最近的工作假设特征学习的效果可以从一个称为平均梯度外积（AGOP）的经典统计估计器中推测出来。作者提出了递归特征机器（RFMs）作为一种算法，通过在转换空间中交替进行（1）通过AGOP对特征向量重新加权和（2）学习预测函数，显式地执行特征学习。在这项工作中，我们通过关注稀疏线性回归和低秩矩阵恢复中出现的过参数化问题的类别，开发了关于RFM如何进行维数减少的第一个理论保证。具体地，我们展示了RFM在限制条件下的性能。

    A fundamental problem in machine learning is to understand how neural networks make accurate predictions, while seemingly bypassing the curse of dimensionality. A possible explanation is that common training algorithms for neural networks implicitly perform dimensionality reduction - a process called feature learning. Recent work posited that the effects of feature learning can be elicited from a classical statistical estimator called the average gradient outer product (AGOP). The authors proposed Recursive Feature Machines (RFMs) as an algorithm that explicitly performs feature learning by alternating between (1) reweighting the feature vectors by the AGOP and (2) learning the prediction function in the transformed space. In this work, we develop the first theoretical guarantees for how RFM performs dimensionality reduction by focusing on the class of overparametrized problems arising in sparse linear regression and low-rank matrix recovery. Specifically, we show that RFM restricted t
    
[^19]: 通过谈判评估语言模型的代理能力

    Evaluating Language Model Agency through Negotiations. (arXiv:2401.04536v1 [cs.CL])

    [http://arxiv.org/abs/2401.04536](http://arxiv.org/abs/2401.04536)

    本研究通过谈判游戏的视角，提出共同评估语言模型（LM）的性能和对齐，以更好地反映真实世界的部署条件，并避免数据泄漏。通过评估多轮次和跨模型交互，我们发现了LM的自我对弈和交叉对弈性能。

    

    公司、组织和政府越来越多地利用语言模型（LM）展示类似代理行为的出色能力。随着LM被采用来执行越来越具有自主性的任务，迫切需要可靠且可扩展的评估基准。当前主要是静态的LM基准无法很好地评估此类动态应用。因此，我们提议通过谈判游戏的视角来共同评估LM的性能和对齐。我们认为这个共同任务更好地反映了真实世界的部署条件，并提供了关于LM决策过程的见解。至关重要的是，谈判游戏使我们能够研究多轮次和跨模型交互，调整复杂性，并避免评估中的意外数据泄漏。我们报告了来自几个主要供应商的六个公开可访问的LM在各种谈判游戏上的结果，评估了自我对弈和交叉对弈性能。值得注意的发现包括：（i）开源模式

    Companies, organizations, and governments increasingly exploit Language Models' (LM) remarkable capability to display agent-like behavior. As LMs are adopted to perform tasks with growing autonomy, there exists an urgent need for reliable and scalable evaluation benchmarks. Current, predominantly static LM benchmarks are ill-suited to evaluate such dynamic applications. Thus, we propose jointly evaluating LM performance and alignment through the lenses of negotiation games. We argue that this common task better reflects real-world deployment conditions while offering insights into LMs' decision-making processes. Crucially, negotiation games allow us to study multi-turn, and cross-model interactions, modulate complexity, and side-step accidental data leakage in evaluation. We report results for six publicly accessible LMs from several major providers on a variety of negotiation games, evaluating both self-play and cross-play performance. Noteworthy findings include: (i) open-source mode
    
[^20]: 半监督深度Sobolev回归: 估计、变量选择及其他

    Semi-Supervised Deep Sobolev Regression: Estimation, Variable Selection and Beyond. (arXiv:2401.04535v1 [stat.ML])

    [http://arxiv.org/abs/2401.04535](http://arxiv.org/abs/2401.04535)

    我们提出了一种半监督深度Sobolev回归器，利用深度神经网络进行梯度范数正则化，可以同时估计回归函数和其梯度，即使存在显著领域变化。这在半监督学习中利用无标签数据方面具有可证优势。

    

    我们提出了SDORE，一种半监督深度Sobolev回归器，用于非参数估计潜在的回归函数及其梯度。SDORE使用深度神经网络来最小化经验风险，并采用梯度范数正则化，允许对无标签数据计算梯度范数。我们对SDORE的收敛速度进行了全面分析，并建立了回归函数的最小化最优速率。重要的是，在存在显著领域变化的情况下，我们还推导出了关联的插值梯度估计器的收敛速度。这些理论结果为选择正则化参数和确定神经网络的大小提供了有价值的先验指导，并展示了在半监督学习中利用无标签数据的可证优势。据我们所知，SDORE是第一个同时估计回归函数及其梯度的可证神经网络方法，具有多样化的应用。

    We propose SDORE, a semi-supervised deep Sobolev regressor, for the nonparametric estimation of the underlying regression function and its gradient. SDORE employs deep neural networks to minimize empirical risk with gradient norm regularization, allowing computation of the gradient norm on unlabeled data. We conduct a comprehensive analysis of the convergence rates of SDORE and establish a minimax optimal rate for the regression function. Crucially, we also derive a convergence rate for the associated plug-in gradient estimator, even in the presence of significant domain shift. These theoretical findings offer valuable prior guidance for selecting regularization parameters and determining the size of the neural network, while showcasing the provable advantage of leveraging unlabeled data in semi-supervised learning. To the best of our knowledge, SDORE is the first provable neural network-based approach that simultaneously estimates the regression function and its gradient, with diverse
    
[^21]: 重写代码：一种用于大型语言模型增强代码搜索的简单方法

    Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search. (arXiv:2401.04514v1 [cs.SE])

    [http://arxiv.org/abs/2401.04514](http://arxiv.org/abs/2401.04514)

    本论文提出一种扩展的生成增强检索（GAR）框架，通过对代码进行重写来解决代码搜索中存在的风格不匹配问题，实验结果表明该方法显著提高了检索准确性。

    

    在代码搜索中，生成增强检索（GAR）框架是一种有前景的策略，通过生成示例代码片段来增强查询，以解决代码片段和自然语言查询之间的主要模态不匹配问题，尤其是在大型语言模型（LLM）展示了代码生成能力的情况下。然而，我们的初步调查发现，LLM增强框架所提供的改进有一定的限制。这种限制可能是因为生成的代码，尽管在功能上准确，但在代码库中与基准代码之间经常显示出明显的风格偏差。在本文中，我们扩展了基础GAR框架，并提出了一种简单而有效的方法，通过对代码库中的代码进行重写（ReCo）来进行风格规范化。实验结果表明，ReCo显著提高了检索准确性。

    In code search, the Generation-Augmented Retrieval (GAR) framework, which generates exemplar code snippets to augment queries, has emerged as a promising strategy to address the principal challenge of modality misalignment between code snippets and natural language queries, particularly with the demonstrated code generation capabilities of Large Language Models (LLMs). Nevertheless, our preliminary investigations indicate that the improvements conferred by such an LLM-augmented framework are somewhat constrained. This limitation could potentially be ascribed to the fact that the generated codes, albeit functionally accurate, frequently display a pronounced stylistic deviation from the ground truth code in the codebase. In this paper, we extend the foundational GAR framework and propose a simple yet effective method that additionally Rewrites the Code (ReCo) within the codebase for style normalization. Experimental results demonstrate that ReCo significantly boosts retrieval accuracy ac
    
[^22]: 零样本音频到音频情感转移与说话人分离

    Zero Shot Audio to Audio Emotion Transfer With Speaker Disentanglement. (arXiv:2401.04511v1 [eess.AS])

    [http://arxiv.org/abs/2401.04511](http://arxiv.org/abs/2401.04511)

    本文提出了一种名为ZEST的零样本情感风格转移方法，允许将给定源音频中的情感内容与目标音频中的情感内容进行转移，同时保留源音频的说话人和语音内容。通过分解语音为语义标记、说话人表示和情感嵌入，并利用重构模型进行训练，实现了高效的转换过程。

    

    音频到音频的风格转移问题涉及将源音频的风格特征替换为目标音频的风格特征，同时保留源音频的与内容相关的属性。本文提出了一种高效的方法，称为零样本情感风格转移（ZEST），它允许将给定源音频中的情感内容与嵌入在目标音频中的情感内容进行转移，同时保留源音频的说话人和语音内容。所提出的系统基于将语音分解为语义标记、说话人表示和情感嵌入。利用这些因素，我们提出了一个框架来重建给定语音信号的音高轮廓，并训练一个解码器来重建语音信号。该模型使用基于自监督的重构损失进行训练。在转换过程中，情感嵌入仅从目标音频中得出，而其他因素均来自源音频。

    The problem of audio-to-audio (A2A) style transfer involves replacing the style features of the source audio with those from the target audio while preserving the content related attributes of the source audio. In this paper, we propose an efficient approach, termed as Zero-shot Emotion Style Transfer (ZEST), that allows the transfer of emotional content present in the given source audio with the one embedded in the target audio while retaining the speaker and speech content from the source. The proposed system builds upon decomposing speech into semantic tokens, speaker representations and emotion embeddings. Using these factors, we propose a framework to reconstruct the pitch contour of the given speech signal and train a decoder that reconstructs the speech signal. The model is trained using a self-supervision based reconstruction loss. During conversion, the emotion embedding is alone derived from the target audio, while rest of the factors are derived from the source audio. In our
    
[^23]: 基于Koopman理论的数据驱动非线性模型简化：综合控制形式和NMPC案例研究

    Data-driven Nonlinear Model Reduction using Koopman Theory: Integrated Control Form and NMPC Case Study. (arXiv:2401.04508v1 [eess.SY])

    [http://arxiv.org/abs/2401.04508](http://arxiv.org/abs/2401.04508)

    本研究基于Koopman理论，提出了一种数据驱动非线性模型简化方法。通过结合延迟坐标编码和全状态解码的模型结构，我们成功地实现了高纯度低温精馏塔的实时非线性模型预测控制。

    

    我们使用Koopman理论对带有控制的非线性动态系统进行数据驱动模型简化。我们提出了将延迟坐标编码测量和全状态解码相结合的通用模型结构，以集成简化的Koopman建模和状态估计。我们提出了一种深度学习方法来训练所提出的模型。案例研究表明，我们的方法提供了精确的控制模型，并实现了高纯度低温精馏塔的实时非线性模型预测控制。

    We use Koopman theory for data-driven model reduction of nonlinear dynamical systems with controls. We propose generic model structures combining delay-coordinate encoding of measurements and full-state decoding to integrate reduced Koopman modeling and state estimation. We present a deep-learning approach to train the proposed models. A case study demonstrates that our approach provides accurate control models and enables real-time capable nonlinear model predictive control of a high-purity cryogenic distillation column.
    
[^24]: SpiNNaker2: 一种用于基于事件和异步机器学习的大规模神经形态系统

    SpiNNaker2: A Large-Scale Neuromorphic System for Event-Based and Asynchronous Machine Learning. (arXiv:2401.04491v1 [cs.ET])

    [http://arxiv.org/abs/2401.04491](http://arxiv.org/abs/2401.04491)

    SpiNNaker2是一种用于大规模基于事件和异步机器学习的神经形态系统芯片，能够通过整合计算原理来减少计算成本和能源消耗。

    

    人工神经网络和特定领域硬件加速器（如GPU和TPU）的共同进步已经在机器学习研究的许多领域取得了成功。然而，随着更大模型和更多数据的需求不断增长，这种发展伴随着对计算资源的快速增长。与此同时，基础模型的新特性（如上下文学习）为机器学习应用带来了新的机遇。然而，这些应用的计算成本限制了数据中心以及移动设备和边缘系统的技术应用。为了减少能源消耗和显著的延迟，神经形态计算系统通过利用低功耗模拟和数字技术深度整合神经生物学系统的计算原理。SpiNNaker2是一种用于可扩展机器学习的数字神经形态芯片。SpiNNaker2的基于事件和异步的设计允许灵活的计算组合。

    The joint progress of artificial neural networks (ANNs) and domain specific hardware accelerators such as GPUs and TPUs took over many domains of machine learning research. This development is accompanied by a rapid growth of the required computational demands for larger models and more data. Concurrently, emerging properties of foundation models such as in-context learning drive new opportunities for machine learning applications. However, the computational cost of such applications is a limiting factor of the technology in data centers, and more importantly in mobile devices and edge systems. To mediate the energy footprint and non-trivial latency of contemporary systems, neuromorphic computing systems deeply integrate computational principles of neurobiological systems by leveraging low-power analog and digital technologies. SpiNNaker2 is a digital neuromorphic chip developed for scalable machine learning. The event-based and asynchronous design of SpiNNaker2 allows the composition 
    
[^25]: 最优生存树: 一种动态规划方法

    Optimal Survival Trees: A Dynamic Programming Approach. (arXiv:2401.04489v1 [cs.LG])

    [http://arxiv.org/abs/2401.04489](http://arxiv.org/abs/2401.04489)

    本文提出了一种利用动态规划的生存树方法，通过递归分割人口和预测不同的生存分布来发现复杂的非线性关系。该方法具有优化保证，并通过特殊算法提高了可扩展性，运行时间优于某些启发式算法。

    

    生存分析基于历史数据来研究和预测死亡时间或其他不可重复事件的时间，而某些实例的真实死亡时间是未知的。生存树通过递归地分割人口并在每个叶节点预测不同的生存分布，能够发现紧凑且易于理解的模型中的复杂非线性关系。我们使用动态规划提供了第一个具有优化保证的生存树方法，能够评估启发式算法的优化间隙。我们通过一种特殊的算法来计算深度为2的树，提高了我们方法的可扩展性。实验证明，在获取与最先进技术相当的样本外性能的同时，我们方法的运行时间甚至优于某些启发式算法。

    Survival analysis studies and predicts the time of death, or other singular unrepeated events, based on historical data, while the true time of death for some instances is unknown. Survival trees enable the discovery of complex nonlinear relations in a compact human comprehensible model, by recursively splitting the population and predicting a distinct survival distribution in each leaf node. We use dynamic programming to provide the first survival tree method with optimality guarantees, enabling the assessment of the optimality gap of heuristics. We improve the scalability of our method through a special algorithm for computing trees up to depth two. The experiments show that our method's run time even outperforms some heuristics for realistic cases while obtaining similar out-of-sample performance with the state-of-the-art.
    
[^26]: 在自动语音识别中持续学习新词

    Continuously Learning New Words in Automatic Speech Recognition. (arXiv:2401.04482v1 [cs.CL])

    [http://arxiv.org/abs/2401.04482](http://arxiv.org/abs/2401.04482)

    该论文提出了一种自我监督的持续学习方法，用于解决自动语音识别中识别新词的问题。通过对讲座录音进行推理和收集包含新词的话语，然后在自适应数据集上进行持续学习，可以在新词出现频率较高时提高性能，同时保持整体性能。

    

    尽管最近取得了进展，但自动语音识别（ASR）系统仍然远未完美。典型的错误包括缩写词、命名实体和领域特定的专用词，这些词几乎没有或没有数据可用来训练。为了解决识别这些词的问题，我们提出了一种自我监督的持续学习方法。给定带有对应幻灯片的讲座录音，我们通过使用先前工作中的记忆增强型ASR模型来将模型偏向于从幻灯片中解码新词。然后，我们对讲座进行推理，将包含检测到的新词的话语收集到自适应数据集中。接着，对这个集合进行持续学习，通过调整添加到模型的每个权重矩阵的低秩矩阵权重。整个过程对多个讲座进行迭代。我们展示了通过这种方法，我们在新词出现频率较高时获得了性能的提升（超过80%的召回率），同时保持了模型的整体性能。

    Despite recent advances, Automatic Speech Recognition (ASR) systems are still far from perfect. Typical errors include acronyms, named entities and domain-specific special words for which little or no data is available. To address the problem of recognizing these words, we propose an self-supervised continual learning approach. Given the audio of a lecture talk with corresponding slides, we bias the model towards decoding new words from the slides by using a memory-enhanced ASR model from previous work. Then, we perform inference on the talk, collecting utterances that contain detected new words into an adaptation dataset. Continual learning is then performed on this set by adapting low-rank matrix weights added to each weight matrix of the model. The whole procedure is iterated for many talks. We show that with this approach, we obtain increasing performance on the new words when they occur more frequently (more than 80% recall) while preserving the general performance of the model.
    
[^27]: TwinBooster: 结合Barlow Twins和梯度提升的大语言模型协同增强分子属性预测

    TwinBooster: Synergising Large Language Models with Barlow Twins and Gradient Boosting for Enhanced Molecular Property Prediction. (arXiv:2401.04478v1 [q-bio.BM])

    [http://arxiv.org/abs/2401.04478](http://arxiv.org/abs/2401.04478)

    TwinBooster结合了大语言模型、Barlow Twins和梯度提升，通过整合生物检测方法和分子指纹，实现了对未见过的生物检测方法和分子属性的精确预测，该方法在数据稀缺的情况下展现出了优秀的性能。

    

    药物发现和开发的成功依赖于对分子活性和属性的精确预测。虽然基于计算的分子属性预测显示出了显著的潜力，但其使用迄今为止仅限于大量数据可用的检测方法。在本研究中，我们使用经过微调的大语言模型，结合了基于文本信息的生物检测方法，并使用了一种新颖的自监督学习方法的Siamese神经网络Barlow Twins。该架构利用检测方法信息和分子指纹提取真实的分子信息。TwinBooster通过提供最先进的零样本学习任务，实现了对未见过的生物检测方法和分子的属性预测。值得注意的是，我们的人工智能流水线在FS-Mol基准测试上表现出优秀的性能。这一突破展示了深度学习在通常数据稀缺的关键属性预测任务中的应用。

    The success of drug discovery and development relies on the precise prediction of molecular activities and properties. While in silico molecular property prediction has shown remarkable potential, its use has been limited so far to assays for which large amounts of data are available. In this study, we use a fine-tuned large language model to integrate biological assays based on their textual information, coupled with Barlow Twins, a Siamese neural network using a novel self-supervised learning approach. This architecture uses both assay information and molecular fingerprints to extract the true molecular information. TwinBooster enables the prediction of properties of unseen bioassays and molecules by providing state-of-the-art zero-shot learning tasks. Remarkably, our artificial intelligence pipeline shows excellent performance on the FS-Mol benchmark. This breakthrough demonstrates the application of deep learning to critical property prediction tasks where data is typically scarce.
    
[^28]: 关于高效联邦学习方法在基础模型训练中的调查

    A Survey on Efficient Federated Learning Methods for Foundation Model Training. (arXiv:2401.04472v1 [cs.LG])

    [http://arxiv.org/abs/2401.04472](http://arxiv.org/abs/2401.04472)

    这项调查研究了高效联邦学习方法在基础模型训练中的应用，提出了一个新的分类方法以优化计算和通信效率。该研究还讨论了当前广泛使用的FL框架，并展望了未来的研究潜力。

    

    联邦学习（FL）已成为一种促进隐私保护协作训练的成熟技术。然而，新的FL方法通常只涉及小型深度学习模型的贡献。随着Transformer模型的巨大成功，一个问题出现了：如何使基础模型在FL应用中实施起来？鉴于在FL中计算和通信的时间消耗通常相似，我们引入了一个关于在FL应用中的计算和通信效率方法的新的分类方法。这些方法旨在优化训练时间并减少客户端与服务器之间的通信。我们还研究了目前广泛使用的FL框架，并根据FL研究及其延伸的现有方法讨论了未来的研究潜力。

    Federated Learning (FL) has become an established technique to facilitate privacy-preserving collaborative training. However, new approaches to FL often discuss their contributions involving small deep-learning models only. With the tremendous success of transformer models, the following question arises: What is necessary to operationalize foundation models in an FL application? Knowing that computation and communication often take up similar amounts of time in FL, we introduce a novel taxonomy focused on computational and communication efficiency methods in FL applications. This said, these methods aim to optimize the training time and reduce communication between clients and the server. We also look at the current state of widely used FL frameworks and discuss future research potentials based on existing approaches in FL research and beyond.
    
[^29]: PhilEO Bench: 评估地理空间基础模型

    PhilEO Bench: Evaluating Geo-Spatial Foundation Models. (arXiv:2401.04464v1 [cs.CV])

    [http://arxiv.org/abs/2401.04464](http://arxiv.org/abs/2401.04464)

    本文引入了PhilEO Bench，一个新颖的EO基础模型评估框架，旨在解决EO领域中缺乏标记数据的问题。框架包括测试平台和一个400 GB Sentinel-2数据集，用于建筑密度估计、道路分割和土地覆盖分类的标签，通过实验评估了不同的基础模型。

    

    地球观测（EO）卫星捕捉到大量未标记的数据，其中Sentinel-2星座每天产生1.6 TB的数据。这使得遥感成为一个数据丰富的领域，非常适合机器学习（ML）解决方案。然而，应用ML模型到EO领域的瓶颈在于缺乏经过注释的数据，因为注释是一项费时费力的过程。因此，该领域的研究聚焦于自我监督学习和基础模型方法。本文通过引入PhilEO Bench，一个新颖的EO基础模型评估框架，以评估不同的基础模型的公平性和一致性基准。该框架包括一个测试平台和一个新颖的400 GB Sentinel-2数据集，其中包含了三个下游任务的标签，即建筑密度估计、道路分割和土地覆盖分类。我们使用我们的框架进行实验，评估了不同的基础模型，包括Prithvi和SatMAE。

    Massive amounts of unlabelled data are captured by Earth Observation (EO) satellites, with the Sentinel-2 constellation generating 1.6 TB of data daily. This makes Remote Sensing a data-rich domain well suited to Machine Learning (ML) solutions. However, a bottleneck in applying ML models to EO is the lack of annotated data as annotation is a labour-intensive and costly process. As a result, research in this domain has focused on Self-Supervised Learning and Foundation Model approaches. This paper addresses the need to evaluate different Foundation Models on a fair and uniform benchmark by introducing the PhilEO Bench, a novel evaluation framework for EO Foundation Models. The framework comprises of a testbed and a novel 400 GB Sentinel-2 dataset containing labels for three downstream tasks, building density estimation, road segmentation, and land cover classification. We present experiments using our framework evaluating different Foundation Models, including Prithvi and SatMAE, at mu
    
[^30]: AI竞赛和基准，实践问题：提案、拨款、赞助、奖品、传播、宣传

    AI Competitions and Benchmarks, Practical issues: Proposals, grant money, sponsors, prizes, dissemination, publicity. (arXiv:2401.04452v1 [cs.LG])

    [http://arxiv.org/abs/2401.04452](http://arxiv.org/abs/2401.04452)

    本章提供了组织人工智能竞赛所需考虑的实践问题的综述，包括激励参与的策略、社区参与的核心内容以及后勤问题的管理和执行。

    

    本章综述了组织人工智能竞赛所涉及的实践问题。我们首先讨论激励参与的策略，包括有效的沟通技巧、与该领域的热门话题对齐、奖项结构、潜在的招聘机会等等。然后，我们转向社区参与的核心，并探讨组织的最佳实践和有效的输出传播方式。最后，本章介绍了组织人员和资源分配等后勤问题，以有效管理和执行竞赛。通过研究这些实际问题，读者将获得有关从创立到完成的AI竞赛组织的可行见解。

    This chapter provides a comprehensive overview of the pragmatic aspects involved in organizing AI competitions. We begin by discussing strategies to incentivize participation, touching upon effective communication techniques, aligning with trending topics in the field, structuring awards, potential recruitment opportunities, and more. We then shift to the essence of community engagement, and into organizational best practices and effective means of disseminating challenge outputs. Lastly, the chapter addresses the logistics, exposing on costs, required manpower, and resource allocation for effectively managing and executing a challenge. By examining these practical problems, readers will gain actionable insights to navigate the multifaceted landscape of AI competition organization, from inception to completion.
    
[^31]: 使用微地震测量和机器学习方法重建海浪数据

    Sea wave data reconstruction using micro-seismic measurements and machine learning methods. (arXiv:2401.04431v1 [physics.ins-det])

    [http://arxiv.org/abs/2401.04431](http://arxiv.org/abs/2401.04431)

    本研究提出了使用微地震测量和机器学习方法重建海浪数据的系统，通过测量海浪产生的微地震信号并训练机器学习算法，实现对缺失浮标数据的准确重建。

    

    海浪监测在海洋学的许多应用中至关重要，如天气和波浪模型的验证。传统的现场解决方案基于锚定浮标，其测量通常被视为标准。然而，由于暴露在恶劣环境中，它们不可靠，需要频繁维护，并且数据集存在许多间断。为了克服以前的限制，我们提出了一个系统，其中包括一个浮标，一个微地震测量站和一个机器学习算法。工作原理基于测量海浪产生的微地震信号。因此，机器学习算法将被训练以从微地震数据重建缺失的浮标数据。由于微地震站可以安装在室内，它保证了高可靠性，而机器学习算法提供了准确重建缺失的浮标数据。在这项工作中，我们介绍了处理数据的方法，开发和训练机器学习算法的方法。

    Sea wave monitoring is key in many applications in oceanography such as the validation of weather and wave models. Conventional in situ solutions are based on moored buoys whose measurements are often recognized as a standard. However, being exposed to a harsh environment, they are not reliable, need frequent maintenance, and the datasets feature many gaps. To overcome the previous limitations, we propose a system including a buoy, a micro-seismic measuring station, and a machine learning algorithm. The working principle is based on measuring the micro-seismic signals generated by the sea waves. Thus, the machine learning algorithm will be trained to reconstruct the missing buoy data from the micro-seismic data. As the micro-seismic station can be installed indoor, it assures high reliability while the machine learning algorithm provides accurate reconstruction of the missing buoy data. In this work, we present the methods to process the data, develop and train the machine learning alg
    
[^32]: 元森林：基于元学习的随机森林的领域泛化

    Meta-forests: Domain generalization on random forests with meta-learning. (arXiv:2401.04425v1 [cs.CV])

    [http://arxiv.org/abs/2401.04425](http://arxiv.org/abs/2401.04425)

    本文提出了一种新的领域泛化算法"元森林"，通过整合元学习策略和最大均值差异度量，提高分类器的泛化能力。该算法在每个元任务中进行元学习优化，并利用最大均值差异作为正则化项，来惩罚泛化性能差的情况。

    

    领域泛化是一种流行的机器学习技术，通过从多个源领域学习，使模型在未见过的目标领域上表现良好。领域泛化在数据有限、困难或昂贵收集的情况下非常有用，比如目标识别和生物医学领域。本文提出了一种新颖的领域泛化算法，名为"元森林"，它在基本的随机森林模型基础上，结合了元学习策略和最大均值差异度量。元森林的目标是通过减少树之间的相关性并增强其强度，提高分类器的泛化能力。具体而言，元森林在每个元任务中进行元学习优化，同时利用最大均值差异作为正则化项，在元测试过程中惩罚泛化性能差的情况。为了评估我们算法的有效性，我们将其应用于两个公开的领域泛化数据集上。

    Domain generalization is a popular machine learning technique that enables models to perform well on the unseen target domain, by learning from multiple source domains. Domain generalization is useful in cases where data is limited, difficult, or expensive to collect, such as in object recognition and biomedicine. In this paper, we propose a novel domain generalization algorithm called "meta-forests", which builds upon the basic random forests model by incorporating the meta-learning strategy and maximum mean discrepancy measure. The aim of meta-forests is to enhance the generalization ability of classifiers by reducing the correlation among trees and increasing their strength. More specifically, meta-forests conducts meta-learning optimization during each meta-task, while also utilizing the maximum mean discrepancy as a regularization term to penalize poor generalization performance in the meta-test process. To evaluate the effectiveness of our algorithm, we test it on two publicly ob
    
[^33]: 在推荐系统的训练过程中优化细粒度嵌入维度

    Fine-Grained Embedding Dimension Optimization During Training for Recommender Systems. (arXiv:2401.04408v1 [cs.IR])

    [http://arxiv.org/abs/2401.04408](http://arxiv.org/abs/2401.04408)

    本文提出了一种细粒度嵌入维度优化方法（FIITED），能够在推荐系统的训练过程中根据嵌入向量的重要性不断调整其维度，并设计了一种虚拟哈希索引哈希表的嵌入存储系统以有效节省内存。

    

    现代深度学习推荐模型中的大型嵌入表在训练和推断过程中需要过大的内存。为了减小训练时的内存占用，本文提出了一种细粒度嵌入维度优化方法 (FIITED)。根据嵌入向量的重要性不同，FIITED在训练过程中连续调整每个嵌入向量的维度，将更重要的嵌入向量分配更长的维度，并能够适应数据的动态变化。同时，本文设计了一种基于虚拟哈希的物理索引哈希表的嵌入存储系统，以实现嵌入维度的调整并有效地节省内存。对两个行业模型的实验表明，FIITED能够将嵌入的大小减小超过65%，同时保持训练模型的质量，比现有的一种在训练过程中进行嵌入修剪的方法节省更多内存。

    Huge embedding tables in modern Deep Learning Recommender Models (DLRM) require prohibitively large memory during training and inference. Aiming to reduce the memory footprint of training, this paper proposes FIne-grained In-Training Embedding Dimension optimization (FIITED). Given the observation that embedding vectors are not equally important, FIITED adjusts the dimension of each individual embedding vector continuously during training, assigning longer dimensions to more important embeddings while adapting to dynamic changes in data. A novel embedding storage system based on virtually-hashed physically-indexed hash tables is designed to efficiently implement the embedding dimension adjustment and effectively enable memory saving. Experiments on two industry models show that FIITED is able to reduce the size of embeddings by more than 65% while maintaining the trained model's quality, saving significantly more memory than a state-of-the-art in-training embedding pruning method. On p
    
[^34]: IGNITE: 个体化时间序列电子健康记录的生成模型

    IGNITE: Individualized GeNeration of Imputations in Time-series Electronic health records. (arXiv:2401.04402v1 [cs.LG])

    [http://arxiv.org/abs/2401.04402](http://arxiv.org/abs/2401.04402)

    个体化时间序列电子健康记录的生成模型IGNITE通过学习个体的动态特征，结合人口特征和治疗信息，生成个性化的真实值，为个体化医疗提供了有价值的方式。

    

    电子健康记录为推动个体化医疗提供了有价值的方式，可以根据个体差异量身定制治疗方案。为了实现这一目标，许多数据驱动的机器学习和统计模型借助丰富的纵向电子健康记录来研究患者的生理和治疗效果。然而，纵向电子健康记录往往稀疏且存在大量缺失，其中缺失的信息也可能反映患者的健康状况。因此，数据驱动模型在个体化医疗中的成功严重依赖于如何从生理数据、治疗以及数据中的缺失值来表示电子健康记录。为此，我们提出了一种新颖的深度学习模型，该模型可以在个体的人口特征和治疗的条件下，学习多变量数据的患者动态，并生成个性化的真实值。

    Electronic Health Records present a valuable modality for driving personalized medicine, where treatment is tailored to fit individual-level differences. For this purpose, many data-driven machine learning and statistical models rely on the wealth of longitudinal EHRs to study patients' physiological and treatment effects. However, longitudinal EHRs tend to be sparse and highly missing, where missingness could also be informative and reflect the underlying patient's health status. Therefore, the success of data-driven models for personalized medicine highly depends on how the EHR data is represented from physiological data, treatments, and the missing values in the data. To this end, we propose a novel deep-learning model that learns the underlying patient dynamics over time across multivariate data to generate personalized realistic values conditioning on an individual's demographic characteristics and treatments. Our proposed model, IGNITE (Individualized GeNeration of Imputations in
    
[^35]: 高阶认知模型在主动学习中的作用

    The Role of Higher-Order Cognitive Models in Active Learning. (arXiv:2401.04397v1 [cs.LG])

    [http://arxiv.org/abs/2401.04397](http://arxiv.org/abs/2401.04397)

    本论文讨论了在人工智能中构建能与人类高效协作的机器的目标，介绍了使用高阶认知模型的主动学习的实际例子，并提出了一种新的主动学习范式，利用人类作为主动数据源，并考虑他们更高级别的代理作用。

    

    构建能够与人类高效协作的机器一直是人工智能领域长期以来的目标。特别是在存在不确定性的情况下，最优的合作通常需要人类和人工智能代理之间相互建模，并利用这些模型推断潜在的目标、信念或意图，可能涉及多级递归。在以前的认知科学、语言学和机器人学的研究中，也提供了这种高阶认知在人类行为中的经验证据。我们提倡一种新的主动学习范式，利用人类作为主动数据源，并考虑他们更高级别的代理作用。具体来说，我们讨论了提升代理级别如何导致主动学习系统和教师之间产生不同形式的合理沟通。此外，我们提供了一个使用高阶认知模型的主动学习的实际例子。

    Building machines capable of efficiently collaborating with humans has been a longstanding goal in artificial intelligence. Especially in the presence of uncertainties, optimal cooperation often requires that humans and artificial agents model each other's behavior and use these models to infer underlying goals, beliefs or intentions, potentially involving multiple levels of recursion. Empirical evidence for such higher-order cognition in human behavior is also provided by previous works in cognitive science, linguistics, and robotics. We advocate for a new paradigm for active learning for human feedback that utilises humans as active data sources while accounting for their higher levels of agency. In particular, we discuss how increasing level of agency results in qualitatively different forms of rational communication between an active learning system and a teacher. Additionally, we provide a practical example of active learning using a higher-order cognitive model. This is accompani
    
[^36]: 通过细粒度模型参数扰动实现机器去学习

    Machine unlearning through fine-grained model parameters perturbation. (arXiv:2401.04385v1 [cs.LG])

    [http://arxiv.org/abs/2401.04385](http://arxiv.org/abs/2401.04385)

    本文提出了一种精细的机器去学习策略，通过细粒度模型参数的扰动来实现用户隐私保护，同时保持可控的计算成本。采用遗忘率和记忆保留率等新的指标来评估去学习效果和模型泛化能力。

    

    机器去学习技术涉及到撤销数据记录和减小该数据对训练模型的影响，从而帮助实现用户隐私保护目标，但会带来显著的计算成本。基于参数扰动的权重去学习是一种通用方法，但通常涉及到全局修改参数。我们提出了精细的Top-K和Random-k参数扰动不精确机器去学习策略，以满足隐私需求同时保持计算成本可控。为了展示我们策略的有效性，我们还解决了评估机器去学习效果的挑战，考虑了模型在去学习和剩余数据上的广义性能。为了更好地评估去学习效果和模型泛化能力，我们提出了新的指标，即遗忘率和记忆保留率。然而，对于不精确的机器去学习，现有的指标无法对去学习程度进行准确量化。

    Machine unlearning techniques, which involve retracting data records and reducing influence of said data on trained models, help with the user privacy protection objective but incur significant computational costs. Weight perturbation-based unlearning is a general approach, but it typically involves globally modifying the parameters. We propose fine-grained Top-K and Random-k parameters perturbed inexact machine unlearning strategies that address the privacy needs while keeping the computational costs tractable.  In order to demonstrate the efficacy of our strategies we also tackle the challenge of evaluating the effectiveness of machine unlearning by considering the model's generalization performance across both unlearning and remaining data. To better assess the unlearning effect and model generalization, we propose novel metrics, namely, the forgetting rate and memory retention rate. However, for inexact machine unlearning, current metrics are inadequate in quantifying the degree of
    
[^37]: 迈向可解释的人工智能（XAI）：一个数据挖掘的视角

    Towards Explainable Artificial Intelligence (XAI): A Data Mining Perspective. (arXiv:2401.04374v1 [cs.AI])

    [http://arxiv.org/abs/2401.04374](http://arxiv.org/abs/2401.04374)

    本研究提出了一种“数据为中心”的视角，探讨了数据收集、处理和分析在可解释的人工智能中的作用。研究将现有工作分为三个类别：深度模型的解释、训练数据的影响和领域知识的见解。通过数据挖掘操作，我们总结了这些XAI方法。

    

    鉴于深度神经网络（DNN）的复杂性和透明度不足，人们进行了大量努力，以使这些系统更具解释性或在可访问的术语中解释其行为。与大多数评论不同，该工作采用“数据为中心”的观点，研究数据收集，处理和分析如何促成可解释的人工智能（XAI）。我们将现有工作分为三类，根据其目的进行分类：深度模型的解释，涉及将数据点与模型输出相关联的特征归因和推理过程；训练数据的影响，研究训练数据细微差异（如数据评估和样本异常）对决策过程的影响；以及领域知识的见解，从数据和模型中发现潜在模式，并促进社会价值和科学发现的新知识。具体而言，我们将XAI方法论提炼为数据挖掘操作。

    Given the complexity and lack of transparency in deep neural networks (DNNs), extensive efforts have been made to make these systems more interpretable or explain their behaviors in accessible terms. Unlike most reviews, which focus on algorithmic and model-centric perspectives, this work takes a "data-centric" view, examining how data collection, processing, and analysis contribute to explainable AI (XAI). We categorize existing work into three categories subject to their purposes: interpretations of deep models, referring to feature attributions and reasoning processes that correlate data points with model outputs; influences of training data, examining the impact of training data nuances, such as data valuation and sample anomalies, on decision-making processes; and insights of domain knowledge, discovering latent patterns and fostering new knowledge from data and models to advance social values and scientific discovery. Specifically, we distill XAI methodologies into data mining op
    
[^38]: 使用扩散映射进行稳定的生成建模

    Stable generative modeling using diffusion maps. (arXiv:2401.04372v1 [stat.ML])

    [http://arxiv.org/abs/2401.04372](http://arxiv.org/abs/2401.04372)

    本文提出了一种稳定的生成建模方法，通过将扩散映射与朗之万动力学相结合，在仅有有限数量的训练样本的情况下生成新样本，并解决了时间步长僵硬随机微分方程中的稳定性问题。

    

    我们考虑从仅有足够数量的训练样本可得到的未知分布中抽样的问题。在生成建模的背景下，这样的设置最近引起了相当大的关注。本文中，我们提出了一种将扩散映射和朗之万动力学相结合的生成模型。扩散映射用于从可用的训练样本中近似得到漂移项，然后在离散时间的朗之万采样器中实现生成新样本。通过将核带宽设置为与未调整的朗之万算法中使用的时间步长匹配，我们的方法可以有效地避免通常与时间步长僵硬随机微分方程相关的稳定性问题。更准确地说，我们引入了一种新颖的分裂步骤方案，确保生成的样本保持在训练样本的凸包内。我们的框架可以自然地扩展为生成条件样本。我们展示了性能。

    We consider the problem of sampling from an unknown distribution for which only a sufficiently large number of training samples are available. Such settings have recently drawn considerable interest in the context of generative modelling. In this paper, we propose a generative model combining diffusion maps and Langevin dynamics. Diffusion maps are used to approximate the drift term from the available training samples, which is then implemented in a discrete-time Langevin sampler to generate new samples. By setting the kernel bandwidth to match the time step size used in the unadjusted Langevin algorithm, our method effectively circumvents any stability issues typically associated with time-stepping stiff stochastic differential equations. More precisely, we introduce a novel split-step scheme, ensuring that the generated samples remain within the convex hull of the training samples. Our framework can be naturally extended to generate conditional samples. We demonstrate the performance
    
[^39]: 使用机器学习的空气质量预报：具有针对低资源环境的全球视角

    Air Quality Forecasting Using Machine Learning: A Global perspective with Relevance to Low-Resource Settings. (arXiv:2401.04369v1 [cs.LG])

    [http://arxiv.org/abs/2401.04369](http://arxiv.org/abs/2401.04369)

    本研究提出了一种新的机器学习方法，利用少量的空气质量数据来准确预测空气质量。通过使用世界天气存储库的数据，考虑了来自197个首都城市的气象、空气污染物和空气质量指数特征，实现了可靠的预测。随机森林算法在分类方面表现最好，提高了模型的泛化能力。极大地提升了低资源环境下空气质量预测的适用性。

    

    空气污染是全球第四大死因。虽然在这个领域已经进行了大量的研究，但大多数方法在预测时依赖于大型数据集。这限制了它们在低资源环境中的适用性，而这样的环境更加脆弱。本研究通过提出一种新的机器学习方法来准确预测空气质量，解决了这个问题。通过利用世界天气存储库，考虑了来自197个首都城市的气象、空气污染物和空气质量指数数据，预测了未来一天的空气质量。多个机器学习模型的评估证明了随机森林算法在生成可靠预测方面的有效性，特别是在应用于分类而不是回归时，该方法提高了模型的泛化能力42%，实现了回归0.38和分类0.89的交叉验证分数。

    Air pollution stands as the fourth leading cause of death globally. While extensive research has been conducted in this domain, most approaches rely on large datasets when it comes to prediction. This limits their applicability in low-resource settings though more vulnerable. This study addresses this gap by proposing a novel machine learning approach for accurate air quality prediction using two months of air quality data. By leveraging the World Weather Repository, the meteorological, air pollutant, and Air Quality Index features from 197 capital cities were considered to predict air quality for the next day. The evaluation of several machine learning models demonstrates the effectiveness of the Random Forest algorithm in generating reliable predictions, particularly when applied to classification rather than regression, approach which enhances the model's generalizability by 42%, achieving a cross-validation score of 0.38 for regression and 0.89 for classification. To instill confid
    
[^40]: 通过整合药物特征增强重症监护病房中急性肾损伤预测

    Enhancing Acute Kidney Injury Prediction through Integration of Drug Features in Intensive Care Units. (arXiv:2401.04368v1 [cs.LG])

    [http://arxiv.org/abs/2401.04368](http://arxiv.org/abs/2401.04368)

    本研究通过整合药物特征，提出了一种新方法来增强重症监护病房中急性肾损伤预测模型。通过利用患者处方数据和扩展连接指纹（ECFP）进行模态转换，填补了重症监护设置中的研究空白。

    

    急性肾损伤（AKI）预测与对肾功能产生不良影响的肾毒性药物之间的关系尚未在重症监护设置中得到探索。导致该研究空白的一个因素是在重症监护室（ICU）背景下对药物模态的有限研究，这是由于将处方数据处理为相应的药物表示并对这些药物表示的全面理解的挑战。本研究通过提出一种新方法来填补这一空白，该方法利用患者处方数据作为一种模态来改进现有的AKI预测模型。我们的研究基于电子健康记录（EHR）数据，提取相关的患者处方信息并将其转换为我们的研究中选择的药物表示，即扩展连接指纹（ECFP）。此外，我们采用独特的多模态方法，开发机器学习模型...

    The relationship between acute kidney injury (AKI) prediction and nephrotoxic drugs, or drugs that adversely affect kidney function, is one that has yet to be explored in the critical care setting. One contributing factor to this gap in research is the limited investigation of drug modalities in the intensive care unit (ICU) context, due to the challenges of processing prescription data into the corresponding drug representations and a lack in the comprehensive understanding of these drug representations. This study addresses this gap by proposing a novel approach that leverages patient prescription data as a modality to improve existing models for AKI prediction. We base our research on Electronic Health Record (EHR) data, extracting the relevant patient prescription information and converting it into the selected drug representation for our research, the extended-connectivity fingerprint (ECFP). Furthermore, we adopt a unique multimodal approach, developing machine learning models an
    
[^41]: SoK：面部深度伪造检测器

    SoK: Facial Deepfake Detectors. (arXiv:2401.04364v1 [cs.CV])

    [http://arxiv.org/abs/2401.04364](http://arxiv.org/abs/2401.04364)

    本文对最新的面部深度伪造检测器进行了全面回顾和分析，提供了对其有效性影响因素的深入见解，并在各种攻击场景中进行了评估。

    

    深度伪造技术迅速成为对社会构成深远和严重威胁的原因之一，主要由于其易于制作和传播。这种情况加速了深度伪造检测技术的发展。然而，许多现有的检测器在验证时 heavily 依赖实验室生成的数据集，这可能无法有效地让它们应对新颖、新兴和实际的深度伪造技术。本文对最新的深度伪造检测器进行广泛全面的回顾和分析，根据几个关键标准对它们进行评估。这些标准将这些检测器分为 4 个高级组别和 13 个细粒度子组别，都遵循一个统一的标准概念框架。这种分类和框架提供了对影响检测器功效的因素的深入和实用的见解。我们对 16 个主要的检测器在各种标准的攻击场景中的普适性进行评估，包括黑盒攻击场景。

    Deepfakes have rapidly emerged as a profound and serious threat to society, primarily due to their ease of creation and dissemination. This situation has triggered an accelerated development of deepfake detection technologies. However, many existing detectors rely heavily on lab-generated datasets for validation, which may not effectively prepare them for novel, emerging, and real-world deepfake techniques. In this paper, we conduct an extensive and comprehensive review and analysis of the latest state-of-the-art deepfake detectors, evaluating them against several critical criteria. These criteria facilitate the categorization of these detectors into 4 high-level groups and 13 fine-grained sub-groups, all aligned with a unified standard conceptual framework. This classification and framework offer deep and practical insights into the factors that affect detector efficacy. We assess the generalizability of 16 leading detectors across various standard attack scenarios, including black-bo
    
[^42]: 在可变工况下的变点检测综合剩余寿命估计模型

    A Change Point Detection Integrated Remaining Useful Life Estimation Model under Variable Operating Conditions. (arXiv:2401.04351v1 [cs.LG])

    [http://arxiv.org/abs/2401.04351](http://arxiv.org/abs/2401.04351)

    本研究提出了一种新的时间动态学习模型，用于在可变工况下检测设备的变点，并利用这些变点来提高剩余寿命估计的准确性。

    

    通过提供退化过程的开始信息，健康状态评估成为可靠的复杂设备剩余寿命（RUL）估计的重要前提。本文提出了一种新颖的基于时间动态学习的模型，用于检测个体设备的变点，即使在可变的工况下，并利用所学到的变点来提高RUL估计的准确性。在离线模型开发过程中，多变量传感器数据被分解，以学习可推广和代表多个工况下正常运行动态的融合时间相关特征。基于这些学到的时间特征，构建监控统计值和控制限制阈值以动态地检测设备级别的变点。然后，检测到的变点为训练基于长短期记忆（LSTM）的RUL估计模型提供退化数据标记。

    By informing the onset of the degradation process, health status evaluation serves as a significant preliminary step for reliable remaining useful life (RUL) estimation of complex equipment. This paper proposes a novel temporal dynamics learning-based model for detecting change points of individual devices, even under variable operating conditions, and utilises the learnt change points to improve the RUL estimation accuracy. During offline model development, the multivariate sensor data are decomposed to learn fused temporal correlation features that are generalisable and representative of normal operation dynamics across multiple operating conditions. Monitoring statistics and control limit thresholds for normal behaviour are dynamically constructed from these learnt temporal features for the unsupervised detection of device-level change points. The detected change points then inform the degradation data labelling for training a long short-term memory (LSTM)-based RUL estimation model
    
[^43]: 私有零阶优化的大型语言模型的私有微调

    Private Fine-tuning of Large Language Models with Zeroth-order Optimization. (arXiv:2401.04343v1 [cs.LG])

    [http://arxiv.org/abs/2401.04343](http://arxiv.org/abs/2401.04343)

    引入了DP-ZO，一种通过私有化零阶优化来保护大型语言模型训练数据隐私的方法。

    

    在私有数据集上对大型预训练模型进行微调可能会存在违反隐私的风险。差分隐私是一种通过强制算法稳定性来减轻隐私风险的框架。DP-SGD可以以保护隐私的方式训练具有私有数据的模型，但会带来性能损失和重大工程挑战。我们引入了DP-ZO，一种通过私有化零阶优化来保护训练数据隐私的大型语言模型微调方法。我们的方法设计的一个关键见解是，我们使用的零阶算法SPSA中的梯度方向始终是随机的，而仅依赖于私有数据的信息是步长，即一个标量。因此，我们只需要对标量步长进行隐私处理，这是存储效率高的方法。DP-ZO可以使用拉普拉斯噪声或高斯噪声来实现，在不同任务之间提供了隐私和效用之间的强大权衡。

    Fine-tuning large pretrained models on private datasets may run the risk of violating privacy. Differential privacy is a framework for mitigating privacy risks by enforcing algorithmic stability. DP-SGD enables training models with private data in a privacy-preserving manner, but raises new obstacles in the form of performance loss and significant engineering challenges. We introduce DP-ZO, a new method for fine-tuning large language models that preserves the privacy of training data by privatizing zeroth-order optimization. A key insight into the design of our method is that the direction of the gradient in SPSA, the zeroth-order algorithm we use, is always random and the only information that depends on private data is the step size, i.e., a scalar. Therefore, we only need to privatize the scalar step size, which is memory-efficient. DP-ZO, which can be instantiated with either Laplace or Gaussian noise, provides a strong privacy-utility trade-off across different tasks, and model si
    
[^44]: G-Meta: 大规模推荐系统中的GPU集群分布式元学习

    G-Meta: Distributed Meta Learning in GPU Clusters for Large-Scale Recommender Systems. (arXiv:2401.04338v1 [cs.LG])

    [http://arxiv.org/abs/2401.04338](http://arxiv.org/abs/2401.04338)

    本文提出了一个用于大规模推荐系统中的GPU集群分布式元学习的高性能框架G-Meta，通过利用数据并行性和模型并行性以及设计高效的元-IO流水线，实现了高速分布式训练。

    

    最近，一个名为元学习的新范式被广泛应用于深度学习推荐模型(DLRM)，并在统计性能方面取得了显著的改进，特别是在冷启动场景中。然而，现有的系统并没有为基于元学习的DLRM模型量身定制，并且在GPU集群的分布式训练中存在关于效率的重要问题。这是因为传统的深度学习流水线对于元学习中的两个任务特定数据集和两个更新循环并没有进行优化。本文提出了一个高性能框架，用于在GPU集群上进行基于优化的元DLRM模型的大规模训练，即G-Meta。首先，G-Meta利用数据并行性和模型并行性，并对计算和通信效率进行精心协调，实现高速分布式训练。其次，它提出了一个用于高效数据摄入的元-IO流水线，以缓解输入/输出瓶颈。进行了各种实验

    Recently, a new paradigm, meta learning, has been widely applied to Deep Learning Recommendation Models (DLRM) and significantly improves statistical performance, especially in cold-start scenarios. However, the existing systems are not tailored for meta learning based DLRM models and have critical problems regarding efficiency in distributed training in the GPU cluster. It is because the conventional deep learning pipeline is not optimized for two task-specific datasets and two update loops in meta learning. This paper provides a high-performance framework for large-scale training for Optimization-based Meta DLRM models over the \textbf{G}PU cluster, namely \textbf{G}-Meta. Firstly, G-Meta utilizes both data parallelism and model parallelism with careful orchestration regarding computation and communication efficiency, to enable high-speed distributed training. Secondly, it proposes a Meta-IO pipeline for efficient data ingestion to alleviate the I/O bottleneck. Various experimental r
    
[^45]: 深度高效的私密领域生成用于子图联邦学习

    Deep Efficient Private Neighbor Generation for Subgraph Federated Learning. (arXiv:2401.04336v1 [cs.LG])

    [http://arxiv.org/abs/2401.04336](http://arxiv.org/abs/2401.04336)

    本文提出了FedDEP，用于解决子图联邦学习中的信息传播不完整的问题，并提出了一系列新颖的技术设计，包括深度邻居生成和高效的私密领域生成。

    

    在现实应用中，巨大图通常以非中心化子图的形式由多个数据所有者分散存储。为了保护数据隐私，在不损害数据隐私的前提下，考虑到子图联邦学习（subgraph FL）场景是很自然的，其中每个本地客户端持有整个全局图的子图，以获取全局一般化的图挖掘模型。为了解决由于缺少跨子图邻居而导致的局部子图上的信息传播不完整的独特挑战，以前的工作通过缺失邻居生成器和GNN的联合FL来增加本地邻域。然而，它们在FL的效用性、效率性和隐私目标方面存在深层次的限制。在这项工作中，我们提出了FedDEP来全面解决子图FL中的这些挑战。FedDEP包括一系列新颖的技术设计：(1) 利用潜在缺失邻居的GNN嵌入进行深度邻居生成；(2) Effic...

    Behemoth graphs are often fragmented and separately stored by multiple data owners as distributed subgraphs in many realistic applications. Without harming data privacy, it is natural to consider the subgraph federated learning (subgraph FL) scenario, where each local client holds a subgraph of the entire global graph, to obtain globally generalized graph mining models. To overcome the unique challenge of incomplete information propagation on local subgraphs due to missing cross-subgraph neighbors, previous works resort to the augmentation of local neighborhoods through the joint FL of missing neighbor generators and GNNs. Yet their technical designs have profound limitations regarding the utility, efficiency, and privacy goals of FL. In this work, we propose FedDEP to comprehensively tackle these challenges in subgraph FL. FedDEP consists of a series of novel technical designs: (1) Deep neighbor generation through leveraging the GNN embeddings of potential missing neighbors; (2) Effic
    
[^46]: 将图神经网络与分数阶连续动力学相结合：鲁棒性研究

    Coupling Graph Neural Networks with Fractional Order Continuous Dynamics: A Robustness Study. (arXiv:2401.04331v1 [cs.LG])

    [http://arxiv.org/abs/2401.04331](http://arxiv.org/abs/2401.04331)

    本文详细研究了图神经分数阶微分方程模型的鲁棒性，通过实施分数阶微积分，模型在特征更新过程中考虑了长期记忆，对抗性条件下的性能仍未得到广泛探究。

    

    本文严格研究了图神经分数阶微分方程(FDE)模型的鲁棒性。该框架通过实施分数阶Caputo导数，超越了传统的图神经整数阶常微分方程(ODE)模型。利用分数阶微积分，我们的模型在特征更新过程中考虑了长期记忆，与传统图神经ODE模型中的无记忆马尔可夫更新不同。图神经FDE模型相对于图神经ODE模型在没有攻击或扰动的环境中已经被证明具有优势。尽管传统的图神经ODE模型在现有文献中已经被验证在存在对抗性攻击时具有一定的稳定性和弹性，但图神经FDE模型的鲁棒性，特别是在对抗性条件下的表现，仍未得到广泛探究。本文对图神经FDE模型的鲁棒性进行了详细评估。

    In this work, we rigorously investigate the robustness of graph neural fractional-order differential equation (FDE) models. This framework extends beyond traditional graph neural (integer-order) ordinary differential equation (ODE) models by implementing the time-fractional Caputo derivative. Utilizing fractional calculus allows our model to consider long-term memory during the feature updating process, diverging from the memoryless Markovian updates seen in traditional graph neural ODE models. The superiority of graph neural FDE models over graph neural ODE models has been established in environments free from attacks or perturbations. While traditional graph neural ODE models have been verified to possess a degree of stability and resilience in the presence of adversarial attacks in existing literature, the robustness of graph neural FDE models, especially under adversarial conditions, remains largely unexplored. This paper undertakes a detailed assessment of the robustness of graph 
    
[^47]: 私人真正永恒的强大预测。

    Private Truly-Everlasting Robust-Prediction. (arXiv:2401.04311v1 [cs.LG])

    [http://arxiv.org/abs/2401.04311](http://arxiv.org/abs/2401.04311)

    文章介绍了私人真正永恒的强大预测（PEP）模型，它通过提供对预测oracle的黑盒访问来实现不同ially private learning，保护了初始训练集和无尽分类查询的隐私。文章提出了对PEP定义的两个概念性改进，并展示了比以前的工作更好的新构建。具体而言，通过加入鲁棒性和私人雅克比机制，提供了可靠准确的预测。

    

    私人永恒预测（PEP）是最近由Naor等人[2023]提出的不同ially private learning模型，其中学习者从不公开发布假设。相反，它提供对“预测oracle”的黑盒访问权限，该oracle可以预测从基础分布中抽取的无尽未标记示例的标签。重要的是，PEP可同时保护初始训练集和无尽分类查询的隐私。我们对PEP的定义进行了两个概念性改进，并展示了与之前的工作相比的显着改进的新构建。具体而言，（1）鲁棒性：PEP只在所有分类查询都是从正确的基础分布中抽取得到时才保证准确性。一些超出分布的查询可能会破坏未来查询的预测oracle的有效性， 即使这些查询是从正确的分布中抽样得到的。我们将鲁棒性与私人雅克比机制相结合，提供有保障的准确预测。

    Private Everlasting Prediction (PEP), recently introduced by Naor et al. [2023], is a model for differentially private learning in which the learner never publicly releases a hypothesis. Instead, it provides black-box access to a "prediction oracle" that can predict the labels of an endless stream of unlabeled examples drawn from the underlying distribution. Importantly, PEP provides privacy both for the initial training set and for the endless stream of classification queries. We present two conceptual modifications to the definition of PEP, as well as new constructions exhibiting significant improvements over prior work. Specifically,  (1) Robustness: PEP only guarantees accuracy provided that all the classification queries are drawn from the correct underlying distribution. A few out-of-distribution queries might break the validity of the prediction oracle for future queries, even for future queries which are sampled from the correct distribution. We incorporate robustness against s
    
[^48]: 推进深度主动学习和数据子集选择：用信息论直觉统一原则

    Advancing Deep Active Learning & Data Subset Selection: Unifying Principles with Information-Theory Intuitions. (arXiv:2401.04305v1 [cs.LG])

    [http://arxiv.org/abs/2401.04305](http://arxiv.org/abs/2401.04305)

    本论文探索了基于信息论原则的主动学习和主动采样方面的数据子集选择技术，以提高深度学习模型的标签和训练效率。

    

    本论文的核心目标是通过改进深度学习模型的标签和训练效率来提高深度学习的实用性。为此，我们探讨了基于信息论原则的数据子集选择技术，特别是主动学习和主动采样。主动学习提高了标签效率，而主动采样则提高了训练效率。监督式深度学习模型通常需要大量带标签的数据进行训练。标签获取可能既昂贵又耗时，并且训练大模型需要大量资源，这限制了其在学术研究和“大型科技公司”以外的应用。现有的深度学习数据子集选择方法通常依赖于启发式方法或缺乏基于信息论的原则基础。相比之下，本论文对深度学习中的数据子集选择目标及其应用进行了研究，力求通过信息论的启发，提出更具原则性的方法。

    At its core, this thesis aims to enhance the practicality of deep learning by improving the label and training efficiency of deep learning models. To this end, we investigate data subset selection techniques, specifically active learning and active sampling, grounded in information-theoretic principles. Active learning improves label efficiency, while active sampling enhances training efficiency. Supervised deep learning models often require extensive training with labeled data. Label acquisition can be expensive and time-consuming, and training large models is resource-intensive, hindering the adoption outside academic research and ``big tech.'' Existing methods for data subset selection in deep learning often rely on heuristics or lack a principled information-theoretic foundation. In contrast, this thesis examines several objectives for data subset selection and their applications within deep learning, striving for a more principled approach inspired by information theory. We begin 
    
[^49]: 《关于Transformer过度平滑的真相》

    Setting the Record Straight on Transformer Oversmoothing. (arXiv:2401.04301v1 [cs.LG])

    [http://arxiv.org/abs/2401.04301](http://arxiv.org/abs/2401.04301)

    Transformers are not inherently low-pass filters as previously thought and whether they oversmooth or not depends on the eigenspectrum of their update equations. Based on the analysis, a simple way to parameterize the weights of the Transformer update equations is derived, allowing for control over its spectrum and preventing oversmoothing.

    

    最近，基于Transformer的模型在各个领域取得了巨大成功。与此同时，最新的研究表明，Transformer本质上是一种低通滤波器，会逐渐过度平滑输入数据，降低其表示能力。一个自然的问题是：在存在这个缺陷的情况下，Transformer是如何取得这些成功的？在这项研究中，我们展示了事实上Transformer并不本质上是一种低通滤波器。相反，Transformer是否过度平滑取决于其更新方程的特征谱。我们的分析扩展了之前关于过度平滑和相关现象的研究。我们表明，许多成功的Transformer模型具有满足避免过度平滑条件的注意力和权重。基于这个分析，我们提出了一种简单的方法，对Transformer更新方程的权重进行参数化，使其可以控制其谱特性，确保不会发生过度平滑。与传统的方法相比，我们的方法可以更好地控制过度平滑问题。

    Transformer-based models have recently become wildly successful across a diverse set of domains. At the same time, recent work has shown that Transformers are inherently low-pass filters that gradually oversmooth the inputs, reducing the expressivity of their representations. A natural question is: How can Transformers achieve these successes given this shortcoming? In this work we show that in fact Transformers are not inherently low-pass filters. Instead, whether Transformers oversmooth or not depends on the eigenspectrum of their update equations. Our analysis extends prior work in oversmoothing and in the closely-related phenomenon of rank collapse. We show that many successful Transformer models have attention and weights which satisfy conditions that avoid oversmoothing. Based on this analysis, we derive a simple way to parameterize the weights of the Transformer update equations that allows for control over its spectrum, ensuring that oversmoothing does not occur. Compared to a 
    
[^50]: 宽而深的ReLU神经网络的普适一致性以及Kolmogorov-Donoho最优函数类的极小极限收敛速率

    Universal Consistency of Wide and Deep ReLU Neural Networks and Minimax Optimal Convergence Rates for Kolmogorov-Donoho Optimal Function Classes. (arXiv:2401.04286v1 [stat.ML])

    [http://arxiv.org/abs/2401.04286](http://arxiv.org/abs/2401.04286)

    本文扩展了之前的结果，证明了基于宽而深的ReLU神经网络和逻辑损失训练的分类规则具有普适一致性，并给出了一类概率测度条件下基于神经网络的分类器实现极小极限收敛速率的充分条件。

    

    本文首先扩展了FL93的结果，并证明了基于宽而深的ReLU神经网络和逻辑损失训练的分类规则的普适一致性。与FL93中分解估计和经验误差的方法不同，我们根据一个广泛的神经网络能够插值任意数量的点的观察，直接分析分类风险。其次，我们给出了一类概率测度的充分条件，在这些条件下，基于神经网络的分类器实现了极小极限收敛速率。我们的结果源于实践者观察到神经网络通常被训练成达到0训练误差的事实，这也是我们提出的神经网络分类器的情况。我们的证明依赖于最近在经验风险最小化和深ReLU神经网络的逼近速率方面的发展，适用于不同的感兴趣函数类的应用。

    In this paper, we first extend the result of FL93 and prove universal consistency for a classification rule based on wide and deep ReLU neural networks trained on the logistic loss. Unlike the approach in FL93 that decomposes the estimation and empirical error, we directly analyze the classification risk based on the observation that a realization of a neural network that is wide enough is capable of interpolating an arbitrary number of points. Secondly, we give sufficient conditions for a class of probability measures under which classifiers based on neural networks achieve minimax optimal rates of convergence. Our result is motivated from the practitioner's observation that neural networks are often trained to achieve 0 training error, which is the case for our proposed neural network classifiers. Our proofs hinge on recent developments in empirical risk minimization and on approximation rates of deep ReLU neural networks for various function classes of interest. Applications to clas
    
[^51]: 快速图搜索算法与动态优化和减少直方图用于二分类问题的区分 (arXiv:2401.04282v1 [cs.LG])

    A Fast Graph Search Algorithm with Dynamic Optimization and Reduced Histogram for Discrimination of Binary Classification Problem. (arXiv:2401.04282v1 [cs.LG])

    [http://arxiv.org/abs/2401.04282](http://arxiv.org/abs/2401.04282)

    本研究提出了一种用于二分类问题的快速图搜索算法，通过动态优化和减少直方图的方法来提高区分结果。该算法在支持向量机模型的基础上应用，显著提高了真正例并减少了假正例。

    

    本研究开发了一种图搜索算法，用于找到二分类问题的最优区分路径。目标函数被定义为真正例（TP）和假正例（FP）之间变异性的差异。它使用深度优先搜索（DFS）算法来寻找自顶向下的区分路径。它提出了一种动态优化过程，以在上层优化TP，然后在下层减少FP。为了加速计算速度并提高准确性，它提出了一种带有可变箱大小的减小直方图算法，而不是循环遍历所有数据点，以找到区分的特征阈值。该算法应用于支持向量机（SVM）模型上，用于预测一个人是否健康。它显著提高了SVM结果的TP并减少了FP （例如，FP减少了90%，而TP仅损失了5%）。图搜索自动生成了39个排序的区分路径。

    This study develops a graph search algorithm to find the optimal discrimination path for the binary classification problem. The objective function is defined as the difference of variations between the true positive (TP) and false positive (FP). It uses the depth first search (DFS) algorithm to find the top-down paths for discrimination. It proposes a dynamic optimization procedure to optimize TP at the upper levels and then reduce FP at the lower levels. To accelerate computing speed with improving accuracy, it proposes a reduced histogram algorithm with variable bin size instead of looping over all data points, to find the feature threshold of discrimination. The algorithm is applied on top of a Support Vector Machine (SVM) model for a binary classification problem on whether a person is fit or unfit. It significantly improves TP and reduces FP of the SVM results (e.g., reduced FP by 90% with a loss of only\ 5% TP). The graph search auto-generates 39 ranked discrimination paths withi
    
[^52]: 预测动态图的结构

    Predicting the structure of dynamic graphs. (arXiv:2401.04280v1 [cs.LG])

    [http://arxiv.org/abs/2401.04280](http://arxiv.org/abs/2401.04280)

    本文提出了一种预测动态图结构的方法，利用时间序列方法预测未来时间点的节点度，并结合通量平衡分析方法获得未来图的结构，评估了该方法在合成和真实数据集上的实用性和适用性。

    

    动态图嵌入、归纳和增量学习有助于预测任务，如节点分类和链接预测。然而，从图的时间序列中预测未来时间步的图结构，允许有新节点，并没有受到太多关注。在本文中，我们提出了一种这样的方法。我们使用时间序列方法预测未来时间点的节点度，并将其与通量平衡分析（一种在生物化学中使用的线性规划方法）结合起来，以获得未来图的结构。此外，我们探索了不同参数值的预测图分布。我们使用合成和真实数据集评估了该方法，并展示了其实用性和适用性。

    Dynamic graph embeddings, inductive and incremental learning facilitate predictive tasks such as node classification and link prediction. However, predicting the structure of a graph at a future time step from a time series of graphs, allowing for new nodes has not gained much attention. In this paper, we present such an approach. We use time series methods to predict the node degree at future time points and combine it with flux balance analysis -- a linear programming method used in biochemistry -- to obtain the structure of future graphs. Furthermore, we explore the predictive graph distribution for different parameter values. We evaluate this method using synthetic and real datasets and demonstrate its utility and applicability.
    
[^53]: Attention versus Contrastive Learning of Tabular Data -- A Data-centric Benchmarking. (arXiv:2401.04266v1 [cs.LG])

    Attention versus Contrastive Learning of Tabular Data -- A Data-centric Benchmarking. (arXiv:2401.04266v1 [cs.LG])

    [http://arxiv.org/abs/2401.04266](http://arxiv.org/abs/2401.04266)

    本文通过广泛评估28个表格数据集，对比了注意力和对比学习方法与传统深度学习和机器学习方法在表格数据上的性能，结果表明需要以数据为中心进行评测，以解决性能差距问题。

    

    尽管在图像和文本学习方面取得了突破性的成功，深度学习在处理表格数据方面与传统机器学习相比并未取得显著改进。这种性能差距凸显了对学习算法进行以数据为中心的处理和评测的需求。最近，注意力和对比学习的突破已经改变了计算机视觉和自然语言处理的范式。然而，这些先进的深度模型在表格数据上的有效性研究非常有限，仅使用了少数几个具有非常大的样本数量的数据集，并在与有限数量的基线模型进行评测后报告了混合的结果。我们认为，文献中表格数据集的异质性和基线模型的选择性可能会对评测结果产生偏差。本文在28个表格数据集（14个易于分类和14个难以分类）上广泛评估了最先进的注意力和对比学习方法与传统深度学习和机器学习方法的对比表现。

    Despite groundbreaking success in image and text learning, deep learning has not achieved significant improvements against traditional machine learning (ML) when it comes to tabular data. This performance gap underscores the need for data-centric treatment and benchmarking of learning algorithms. Recently, attention and contrastive learning breakthroughs have shifted computer vision and natural language processing paradigms. However, the effectiveness of these advanced deep models on tabular data is sparsely studied using a few data sets with very large sample sizes, reporting mixed findings after benchmarking against a limited number of baselines. We argue that the heterogeneity of tabular data sets and selective baselines in the literature can bias the benchmarking outcomes. This article extensively evaluates state-of-the-art attention and contrastive learning methods on a wide selection of 28 tabular data sets (14 easy and 14 hard-to-classify) against traditional deep and machine le
    
[^54]: 解释在图机器学习中拓扑数据分析的作用

    Explaining the Power of Topological Data Analysis in Graph Machine Learning. (arXiv:2401.04250v1 [cs.LG])

    [http://arxiv.org/abs/2401.04250](http://arxiv.org/abs/2401.04250)

    在图机器学习中，拓扑数据分析（TDA）被赞赏其捕捉数据中复杂形状和结构的能力，其鲁棒性和可解释性得到验证，但在特定实验中未能显著提高现有方法的预测能力。

    

    拓扑数据分析（TDA）因其能够捕捉数据中复杂的形状和结构而受到研究人员的赞誉。TDA被认为可以处理噪声和高维数据集，并且其可解释性被认为有助于直观理解模型行为。然而，有关TDA的能力和使用价值的声明仅在将基于TDA的模型与其他图机器学习方法（如图神经网络）进行比较的应用领域中部分地得到测试。我们通过一系列细致的实验对TDA的声明进行了全面检验，并验证了其优点。我们的结果证实了TDA对离群点的鲁棒性和可解释性，与支持者的观点一致。然而，我们发现在我们的特定实验中，TDA并没有显著提高现有方法的预测能力，同时还带来了显著的计算成本。我们研究了与图特性相关的现象，例如小尺度世界的存在。

    Topological Data Analysis (TDA) has been praised by researchers for its ability to capture intricate shapes and structures within data. TDA is considered robust in handling noisy and high-dimensional datasets, and its interpretability is believed to promote an intuitive understanding of model behavior. However, claims regarding the power and usefulness of TDA have only been partially tested in application domains where TDA-based models are compared to other graph machine learning approaches, such as graph neural networks. We meticulously test claims on TDA through a comprehensive set of experiments and validate their merits. Our results affirm TDA's robustness against outliers and its interpretability, aligning with proponents' arguments. However, we find that TDA does not significantly enhance the predictive power of existing methods in our specific experiments, while incurring significant computational costs. We investigate phenomena related to graph characteristics, such as small di
    
[^55]: 可扩展的归一化流使得玻尔兹曼发生器能够模拟大分子物质

    Scalable Normalizing Flows Enable Boltzmann Generators for Macromolecules. (arXiv:2401.04246v1 [cs.LG])

    [http://arxiv.org/abs/2401.04246](http://arxiv.org/abs/2401.04246)

    提出了一种新颖的流体架构来高效地学习蛋白质的构象分布，并通过利用2-Wasserstein损失平滑地实现了大分子的玻尔兹曼发生器的训练。

    

    蛋白质的玻尔兹曼分布为其所有功能状态提供了一个路线图。归一化流是模拟这种分布的一种有前途的工具，但是当前的方法对于典型的药物靶标来说是不可解的；由于系统的大小、分子内部势能的异质性和远程相互作用的存在，它们在计算上是不可解的。为了解决这些问题，我们提出了一种新颖的流体架构，利用分裂通道和门控注意力来高效地学习由内部坐标定义的蛋白质的构象分布。我们展示了通过利用2-Wasserstein损失，可以平滑地从最大似然训练过渡到基于能量的训练，从而实现大分子的玻尔兹曼发生器的训练。我们在HP35(nle-nle)维林头部片段和蛋白质G中评估了我们的模型和训练策略。我们证明了标准的架构和训练策略无法处理这些问题。

    The Boltzmann distribution of a protein provides a roadmap to all of its functional states. Normalizing flows are a promising tool for modeling this distribution, but current methods are intractable for typical pharmacological targets; they become computationally intractable due to the size of the system, heterogeneity of intra-molecular potential energy, and long-range interactions. To remedy these issues, we present a novel flow architecture that utilizes split channels and gated attention to efficiently learn the conformational distribution of proteins defined by internal coordinates. We show that by utilizing a 2-Wasserstein loss, one can smooth the transition from maximum likelihood training to energy-based training, enabling the training of Boltzmann Generators for macromolecules. We evaluate our model and training strategy on villin headpiece HP35(nle-nle), a 35-residue subdomain, and protein G, a 56-residue protein. We demonstrate that standard architectures and training strate
    
[^56]: 基于学习的数学规划方法用于自动配置优化求解器

    A learning-based mathematical programming formulation for the automatic configuration of optimization solvers. (arXiv:2401.04237v1 [math.OC])

    [http://arxiv.org/abs/2401.04237](http://arxiv.org/abs/2401.04237)

    该论文提出了一种基于机器学习和优化的方法，用于自动配置优化求解器。通过学习已解决的实例和配置，构建了性能函数，并使用数学规划来寻找给定实例的最佳求解器配置。

    

    我们提出一种基于机器学习和优化的方法，用于选择给定实例的求解器配置。首先，我们利用一组已解决的实例和配置来学习求解器的性能函数。其次，我们制定了一个混合整数非线性规划，其中目标/约束明确地编码了学习到的信息，并且在未知实例到来时进行求解，以找到基于性能函数的最佳求解器配置。我们方法的主要创新在于将配置集查找问题形式化为数学程序，这使我们能够a)对配置强制执行依赖和兼容性约束，并b)利用即插即用的优化工具高效求解该问题。

    We propose a methodology, based on machine learning and optimization, for selecting a solver configuration for a given instance. First, we employ a set of solved instances and configurations in order to learn a performance function of the solver. Secondly, we formulate a mixed-integer nonlinear program where the objective/constraints explicitly encode the learnt information, and which we solve, upon the arrival of an unknown instance, to find the best solver configuration for that instance, based on the performance function. The main novelty of our approach lies in the fact that the configuration set search problem is formulated as a mathematical program, which allows us to a) enforce hard dependence and compatibility constraints on the configurations, and b) solve it efficiently with off-the-shelf optimization tools.
    
[^57]: 基于机器学习的预测空间物体密度分布的方法

    Towards a Machine Learning-Based Approach to Predict Space Object Density Distributions. (arXiv:2401.04212v1 [physics.space-ph])

    [http://arxiv.org/abs/2401.04212](http://arxiv.org/abs/2401.04212)

    我们提出了一种基于机器学习的模型来预测空间物体的密度分布，以应对低地球轨道的拥塞并提升对空间环境的可持续性。

    

    随着人造空间物体数量的快速增加，低地球轨道（LEO）面临着严重的拥塞问题，这给空间运营商带来了挑战，并威胁着空间环境的可持续性。目前用于研究这一演变的模型虽然详细，但计算量大。为了解决这些问题，我们提出了一种新颖的基于机器学习的模型，作为MIT轨道能力工具（MOCAT）的延伸。这个先进模型旨在加速ASO密度分布的传播，并且它的训练基于由一个建立在已有精确模型的空间环境演化模拟生成的数百个实例。我们研究了不同的基于深度学习的解决方案，可以作为ASO传播的良好候选，并管理数据的高维度特点。为了评估模型的能力，我们在长期预测场景（约100年）进行实验，分析性能下降的原因和方式。

    With the rapid increase in the number of Anthropogenic Space Objects (ASOs), Low Earth Orbit (LEO) is facing significant congestion, thereby posing challenges to space operators and risking the viability of the space environment for varied uses. Current models for examining this evolution, while detailed, are computationally demanding. To address these issues, we propose a novel machine learning-based model, as an extension of the MIT Orbital Capacity Tool (MOCAT). This advanced model is designed to accelerate the propagation of ASO density distributions, and it is trained on hundreds of simulations generated by an established and accurate model of the space environment evolution. We study how different deep learning-based solutions can potentially be good candidates for ASO propagation and manage the high-dimensionality of the data. To assess the model's capabilities, we conduct experiments in long term forecasting scenarios (around 100 years), analyze how and why the performance degr
    
[^58]: 无指导下多环境中的好奇心与熵驱动强化学习

    Curiosity & Entropy Driven Unsupervised RL in Multiple Environments. (arXiv:2401.04198v1 [cs.LG])

    [http://arxiv.org/abs/2401.04198](http://arxiv.org/abs/2401.04198)

    本论文提出了一种无指导的强化学习方法，在多个环境下通过好奇心和熵驱动实现了性能的改进。

    

    本论文的作者们提出了一种名为alpha-MEPOL的方法来解决多环境下的无指导强化学习问题。他们通过使用整个环境类别的交互来预训练一个与任务无关的探索策略，然后利用监督来对各种任务进行微调。我们在原始工作的基础上进行了扩展，旨在提高性能。我们主要提出并实验了五个新的修改方法：使用基于熵的概率分布来采样轨迹，动态alpha，更高的KL散度阈值，以好奇心驱动的探索，以及基于好奇心的alpha分位数采样。动态alpha和更高的KL散度阈值都相对于早期工作的基线方法有了显著的改进。当样本空间较小时，PDF采样没有提供任何改进，因为它与基线方法近似等价。在高维环境中，这些修改方法的添加提高了性能。

    The authors of 'Unsupervised Reinforcement Learning in Multiple environments' propose a method, alpha-MEPOL, to tackle unsupervised RL across multiple environments. They pre-train a task-agnostic exploration policy using interactions from an entire environment class and then fine-tune this policy for various tasks using supervision. We expanded upon this work, with the goal of improving performance. We primarily propose and experiment with five new modifications to the original work: sampling trajectories using an entropy-based probability distribution, dynamic alpha, higher KL Divergence threshold, curiosity-driven exploration, and alpha-percentile sampling on curiosity. Dynamic alpha and higher KL-Divergence threshold both provided a significant improvement over the baseline from the earlier work. PDF-sampling failed to provide any improvement due to it being approximately equivalent to the baseline method when the sample space is small. In high-dimensional environments, the addition
    
[^59]: 密集化霍普菲尔德网络在师生模式下的应用

    Dense Hopfield Networks in the Teacher-Student Setting. (arXiv:2401.04191v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2401.04191](http://arxiv.org/abs/2401.04191)

    密集化霍普菲尔德网络在师生模式下的研究揭示了铁磁相学习和原型学习的特点，同时发现在特定条件下的关键训练集大小。此外，研究还表明学生比教师具有更广泛的容忍度。

    

    密集化霍普菲尔德网络以其从特征到原型的转变和对抗性鲁棒性而闻名。然而，以往的理论研究主要关注其存储容量。我们通过研究师生模式下的p-体霍普菲尔德网络的相图，填补了这一空白，揭示了类似于原型和特征学习范围的铁磁相。在Nishimori线上，我们发现了高效模式检索所需的训练集的临界大小。有趣的是，我们发现师生模式的顺磁到铁磁转变与直接模型（即随机模式）的顺磁到自旋玻璃转变一致。在Nishimori线之外，我们研究了学习性能与推断温度和数据集噪声的关系。此外，我们还展示了在学生比教师使用较大的p值时，学生具有广泛的容忍度。

    Dense Hopfield networks are known for their feature to prototype transition and adversarial robustness. However, previous theoretical studies have been mostly concerned with their storage capacity. We bridge this gap by studying the phase diagram of p-body Hopfield networks in the teacher-student setting of an unsupervised learning problem, uncovering ferromagnetic phases reminiscent of the prototype and feature learning regimes. On the Nishimori line, we find the critical size of the training set necessary for efficient pattern retrieval. Interestingly, we find that that the paramagnetic to ferromagnetic transition of the teacher-student setting coincides with the paramagnetic to spin-glass transition of the direct model, i.e. with random patterns. Outside of the Nishimori line, we investigate the learning performance in relation to the inference temperature and dataset noise. Moreover, we show that using a larger p for the student than the teacher gives the student an extensive toler
    
[^60]: FlopPITy: 利用机器学习实现自洽的外行星大气召回

    FlopPITy: Enabling self-consistent exoplanet atmospheric retrievals with machine learning. (arXiv:2401.04168v1 [astro-ph.EP])

    [http://arxiv.org/abs/2401.04168](http://arxiv.org/abs/2401.04168)

    本研究利用机器学习算法实现了自洽的外行星大气召回，加快了召回速度并允许使用更复杂的大气模型进行召回。

    

    一般使用贝叶斯召回技术来解释外行星大气观测结果以约束物理和化学性质，但这些方法需要大量模型计算，因此需要在模型复杂性和运行时间之间做出妥协。为了加快召回速度，本研究实现并测试了顺序神经后验估计（SNPE），一种机器学习推断算法，用于外行星大气召回。通过生成100个合成观测结果，并利用这些结果进行召回测试，旨在使用计算温度结构的辐射传输模型等更复杂的大气模型来提升召回速度。

    Interpreting the observations of exoplanet atmospheres to constrain physical and chemical properties is typically done using Bayesian retrieval techniques. Because these methods require many model computations, a compromise is made between model complexity and run time. Reaching this compromise leads to the simplification of many physical and chemical processes (e.g. parameterised temperature structure). Here we implement and test sequential neural posterior estimation (SNPE), a machine learning inference algorithm, for exoplanet atmospheric retrievals. The goal is to speed up retrievals so they can be run with more computationally expensive atmospheric models, such as those computing the temperature structure using radiative transfer. We generate 100 synthetic observations using ARCiS (ARtful Modeling Code for exoplanet Science, an atmospheric modelling code with the flexibility to compute models in varying degrees of complexity) and perform retrievals on them to test the faithfulness
    
[^61]: 高效选择性音频掩蔽多模态瓶颈Transformer用于音视频分类

    Efficient Selective Audio Masked Multimodal Bottleneck Transformer for Audio-Video Classification. (arXiv:2401.04154v1 [cs.CV])

    [http://arxiv.org/abs/2401.04154](http://arxiv.org/abs/2401.04154)

    这项研究提出了一种高效选择性音频掩蔽多模态瓶颈Transformer用于音视频分类，通过音视频Transformer提取时空表示并结合自监督目标进行训练，实现了有效的多模态学习和语义音频活动的学习。

    

    音频和视频是主流媒体平台（如YouTube）中最常见的两种形式。为了有效地从多模态视频中学习，本文提出了一种新的音视频识别方法，称为音视频Transformer（AVT），利用视频Transformer的有效时空表示来提高动作识别的准确性。为了进行多模态融合，简单地通过跨模态Transformer连接多模态令牌会占用大量计算和内存资源，因此我们通过音视频瓶颈Transformer降低了跨模态复杂性。为了提高多模态Transformer的学习效率，我们将自监督目标（即音视频对比学习、音视频匹配和音视频遮蔽学习）整合到AVT训练中，将多样的音频和视频表示映射到一个通用的多模态表示空间中。我们还提出了一个音频片段遮蔽损失来学习语义音频活动。

    Audio and video are two most common modalities in the mainstream media platforms, e.g., YouTube. To learn from multimodal videos effectively, in this work, we propose a novel audio-video recognition approach termed audio video Transformer, AVT, leveraging the effective spatio-temporal representation by the video Transformer to improve action recognition accuracy. For multimodal fusion, simply concatenating multimodal tokens in a cross-modal Transformer requires large computational and memory resources, instead we reduce the cross-modality complexity through an audio-video bottleneck Transformer. To improve the learning efficiency of multimodal Transformer, we integrate self-supervised objectives, i.e., audio-video contrastive learning, audio-video matching, and masked audio and video learning, into AVT training, which maps diverse audio and video representations into a common multimodal representation space. We further propose a masked audio segment loss to learn semantic audio activit
    
[^62]: LoRA链：通过残差学习实现语言模型的高效微调

    Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning. (arXiv:2401.04151v1 [cs.LG])

    [http://arxiv.org/abs/2401.04151](http://arxiv.org/abs/2401.04151)

    本论文提出了一种名为LoRA链（COLA）的迭代优化框架，通过残差学习过程将LoRA模块与预训练的语言模型参数合并，并重新初始化新的LoRA模块的优化过程，从而实现LoRA和全参数微调之间的平衡。

    

    微调是将预训练的大型语言模型定制为特定任务的主要方法。随着模型规模和任务多样性的扩大，参数高效的微调方法变得至关重要。其中最常用的方法之一是低秩适应（LoRA）及其变体。LoRA将权重更新编码为两个低秩矩阵的乘积。尽管具有一定优势，但在某些任务的泛化错误方面，LoRA无法完全参数化微调。我们引入了一种称为LoRA链（COLA）的迭代优化框架，受Frank-Wolfe算法的启发，以弥合LoRA和全参数微调之间的差距，而不会增加额外的计算成本或内存开销。COLA采用残差学习过程，在预训练语言模型参数中合并学到的LoRA模块，并重新初始化新生的LoRA模块的优化过程。我们提供了理论上的收敛保证以及...

    Fine-tuning is the primary methodology for tailoring pre-trained large language models to specific tasks. As the model's scale and the diversity of tasks expand, parameter-efficient fine-tuning methods are of paramount importance. One of the most widely used family of methods is low-rank adaptation (LoRA) and its variants. LoRA encodes weight update as the product of two low-rank matrices. Despite its advantages, LoRA falls short of full-parameter fine-tuning in terms of generalization error for certain tasks.  We introduce Chain of LoRA (COLA), an iterative optimization framework inspired by the Frank-Wolfe algorithm, to bridge the gap between LoRA and full parameter fine-tuning, without incurring additional computational costs or memory overheads. COLA employs a residual learning procedure where it merges learned LoRA modules into the pre-trained language model parameters and re-initilize optimization for new born LoRA modules. We provide theoretical convergence guarantees as well as
    
[^63]: 在线测试时间适应性的时空交通流量预测

    Online Test-Time Adaptation of Spatial-Temporal Traffic Flow Forecasting. (arXiv:2401.04148v1 [cs.LG])

    [http://arxiv.org/abs/2401.04148](http://arxiv.org/abs/2401.04148)

    本研究首次研究了在线测试时间适应技术在时空交通流量预测中的应用，提出了一种自适应的双重校正方法，通过对模型输出的分解和校正来提高预测的准确性。

    

    准确的时空交通流量预测对于交通管理人员实施控制措施和驾驶员选择最佳行驶路线至关重要。传统基于深度学习的交通流量预测方法通常依赖于历史数据来训练模型，然后用于对未来数据进行预测。然而，由于历史数据和未来数据之间的时间漂移，训练模型的性能通常会下降。为了使基于历史数据训练的模型能够在完全在线的方式下更好地适应未来数据，本文首次研究了时空交通流量预测问题的在线测试时间适应性技术。为此，我们提出了一种自适应的双重校正方法，通过对训练模型的输出进行季节性和趋势周期部分的分解，并在测试阶段使用两个独立的模块对其进行校正。

    Accurate spatial-temporal traffic flow forecasting is crucial in aiding traffic managers in implementing control measures and assisting drivers in selecting optimal travel routes. Traditional deep-learning based methods for traffic flow forecasting typically rely on historical data to train their models, which are then used to make predictions on future data. However, the performance of the trained model usually degrades due to the temporal drift between the historical and future data. To make the model trained on historical data better adapt to future data in a fully online manner, this paper conducts the first study of the online test-time adaptation techniques for spatial-temporal traffic flow forecasting problems. To this end, we propose an Adaptive Double Correction by Series Decomposition (ADCSD) method, which first decomposes the output of the trained model into seasonal and trend-cyclical parts and then corrects them by two separate modules during the testing phase using the la
    
[^64]: 一种用于全局路径规划的增强注意力深度强化学习方法：Learn Once Plan Arbitrarily (LOPA)

    Learn Once Plan Arbitrarily (LOPA): Attention-Enhanced Deep Reinforcement Learning Method for Global Path Planning. (arXiv:2401.04145v1 [cs.LG])

    [http://arxiv.org/abs/2401.04145](http://arxiv.org/abs/2401.04145)

    LOPA是一种增强注意力的深度强化学习方法，用于解决全局路径规划中的收敛性和泛化性等挑战。

    

    深度强化学习(DRL)方法在路径规划任务中显示出了很大的潜力。然而，在处理全局规划任务时，这些方法面临着收敛性和泛化性等严重的挑战。为此，我们在本文中提出了一种名为LOPA（Learn Once Plan Arbitrarily）的增强注意力深度强化学习方法。首先，我们从DRL的观察角度分析了这些问题的原因，揭示了传统设计导致DRL受到无关地图信息的干扰。其次，我们开发了LOPA，利用一种新颖的增强注意力机制，以改善对观察中关键信息的注意力能力。这种机制通过两个步骤实现：(1)构建一个注意力模型，将DRL的观察转换为两个动态视图：局部和全局，显著指导LOPA关注给定地图上的关键信息；(2)构建一个双通道网络来处理这两个视图。

    Deep reinforcement learning (DRL) methods have recently shown promise in path planning tasks. However, when dealing with global planning tasks, these methods face serious challenges such as poor convergence and generalization. To this end, we propose an attention-enhanced DRL method called LOPA (Learn Once Plan Arbitrarily) in this paper. Firstly, we analyze the reasons of these problems from the perspective of DRL's observation, revealing that the traditional design causes DRL to be interfered by irrelevant map information. Secondly, we develop the LOPA which utilizes a novel attention-enhanced mechanism to attain an improved attention capability towards the key information of the observation. Such a mechanism is realized by two steps: (1) an attention model is built to transform the DRL's observation into two dynamic views: local and global, significantly guiding the LOPA to focus on the key information on the given maps; (2) a dual-channel network is constructed to process these two
    
[^65]: 改进天气预测的鲁棒性校准

    Robust Calibration For Improved Weather Prediction Under Distributional Shift. (arXiv:2401.04144v1 [cs.LG])

    [http://arxiv.org/abs/2401.04144](http://arxiv.org/abs/2401.04144)

    本研究通过混合专家模型、数据增强和鲁棒后校准方法，相比提升树模型，使用深度神经网络在天气预测中实现更准确和更好校准的结果。

    

    本文介绍了在“鲁棒性和不确定性在现实世界分布转移下的挑战——移位挑战”中改善跨域天气预测和不确定性估计的结果。我们发现，通过利用从计算机视觉领域借鉴的高级数据增强技术以及混合专家模型，并结合对预测不确定性进行鲁棒后校准，我们可以在用于表格数据的深度神经网络中实现比提升树模型更准确和更好校准的结果。我们使用多种指标对我们的预测进行量化，提出了几个未来的探究和实验方向来提升性能。

    In this paper, we present results on improving out-of-domain weather prediction and uncertainty estimation as part of the \texttt{Shifts Challenge on Robustness and Uncertainty under Real-World Distributional Shift} challenge. We find that by leveraging a mixture of experts in conjunction with an advanced data augmentation technique borrowed from the computer vision domain, in conjunction with robust \textit{post-hoc} calibration of predictive uncertainties, we can potentially achieve more accurate and better-calibrated results with deep neural networks than with boosted tree models for tabular data. We quantify our predictions using several metrics and propose several future lines of inquiry and experimentation to boost performance.
    
[^66]: 关于分形几何和卷积神经网络（CNN）对其编码能力的潜力研究

    On The Potential of The Fractal Geometry and The CNNs Ability to Encode it. (arXiv:2401.04141v1 [cs.LG])

    [http://arxiv.org/abs/2401.04141](http://arxiv.org/abs/2401.04141)

    本研究探讨分形几何的潜力和深度学习网络对其编码能力的限制。通过相关性分析实验和人类评估，我们发现深度网络不能提取复杂且高层次的分形特征。然而，在需要对象结构对分类任务至关重要的应用中，分形特征表现出很好的有效性。

    

    分形维数通过研究模式在测量尺度下的变化来提供对象复杂性的统计指标。虽然在几种分类任务中有用，但在深度学习应用中对分形维数的研究还不够深入。本研究探讨了深度模型学习到的特征，并研究了这些深度网络是否能够像分形维数一样编码复杂且高层次的特征。具体而言，我们进行了相关性分析实验，结果表明深度网络在任何层次都不能提取出这样的特征。我们将分析研究与人类评估相结合，研究了深度学习网络和仅操作分形特征的模型之间的差异。此外，我们还展示了分形特征在需要对象结构对分类任务至关重要的应用中的有效性。我们实证表明，在分形特征上训练浅层网络能够取得良好的性能。

    The fractal dimension provides a statistical index of object complexity by studying how the pattern changes with the measuring scale. Although useful in several classification tasks, the fractal dimension is under-explored in deep learning applications. In this work, we investigate the features that are learned by deep models and we study whether these deep networks are able to encode features as complex and high-level as the fractal dimensions. Specifically, we conduct a correlation analysis experiment to show that deep networks are not able to extract such a feature in none of their layers. We combine our analytical study with a human evaluation to investigate the differences between deep learning networks and models that operate on the fractal feature solely. Moreover, we show the effectiveness of fractal features in applications where the object structure is crucial for the classification task. We empirically show that training a shallow network on fractal features achieves perform
    
[^67]: CCNETS:一种新颖的脑启发方法用于增强不平衡数据集中的模式识别

    CCNETS: A Novel Brain-Inspired Approach for Enhanced Pattern Recognition in Imbalanced Datasets. (arXiv:2401.04139v1 [cs.LG])

    [http://arxiv.org/abs/2401.04139](http://arxiv.org/abs/2401.04139)

    CCNETS是一种新颖的脑启发方法，通过模拟大脑的信息处理，通过生成高质量的数据集来增强不平衡数据集中的模式识别，特别关注处理机器学习中的不平衡数据集的挑战。

    

    本研究介绍了CCNETS（具有因果合作网络的因果学习），这是一种新颖的基于生成模型的分类器，旨在解决模式识别中不平衡数据集生成的挑战。CCNETS独特地设计成模拟类似于大脑的信息处理，并包括三个主要组件：解释器、生成器和推理器。每个组件都被设计成模仿特定的大脑功能，有助于生成高质量的数据集并增强分类性能。该模型特别关注在机器学习中处理不平衡数据集的常见和重要挑战。通过将CCNETS应用于一个“欺诈数据集”，其中正常交易明显多于欺诈交易（99.83％ vs. 0.17％），证明了CCNETS的有效性。传统方法往往在处理这种不平衡时遇到困难，导致性能指标不均衡。然而，CCNETS展现出优越的分类能力，通过其性能指标的改善来体现。

    This study introduces CCNETS (Causal Learning with Causal Cooperative Nets), a novel generative model-based classifier designed to tackle the challenge of generating data for imbalanced datasets in pattern recognition. CCNETS is uniquely crafted to emulate brain-like information processing and comprises three main components: Explainer, Producer, and Reasoner. Each component is designed to mimic specific brain functions, which aids in generating high-quality datasets and enhancing classification performance.  The model is particularly focused on addressing the common and significant challenge of handling imbalanced datasets in machine learning. CCNETS's effectiveness is demonstrated through its application to a "fraud dataset," where normal transactions significantly outnumber fraudulent ones (99.83% vs. 0.17%). Traditional methods often struggle with such imbalances, leading to skewed performance metrics. However, CCNETS exhibits superior classification ability, as evidenced by its pe
    
[^68]: 具有全局感知增强的时空图递归网络：交通流量预测的新框架

    Global-Aware Enhanced Spatial-Temporal Graph Recurrent Networks: A New Framework For Traffic Flow Prediction. (arXiv:2401.04135v1 [cs.LG])

    [http://arxiv.org/abs/2401.04135](http://arxiv.org/abs/2401.04135)

    该论文提出了一个名为GA-STGRN的新的交通流量预测框架，通过结合图卷积网络和循环神经网络来进行时空建模。该框架中的一个关键创新是引入了全局感知层来帮助捕捉全局信息。另外，为了建模非固定的图结构和捕捉局部特征，还提出了一个序列感知的图神经网络。

    

    交通流量预测在减少交通拥堵和提高运输效率方面起着至关重要的作用。虽然将图卷积网络与循环神经网络相结合进行时空建模在这个领域是一种常见的策略，但是循环神经网络的受限结构限制了它们捕捉全局信息的能力。对于空间建模，许多之前的研究学习一个被假设为固定且在所有时间步骤上均匀的图结构，这可能并不正确。本文提出了一种新颖的交通预测框架，称为全局感知增强时空图递归网络（GA-STGRN），包括两个核心组件：时空图递归神经网络和全局感知层。在这个框架中，提出了三个创新的预测模型。提出了一种序列感知的图神经网络，并将其整合到门控循环单元（GRU）中，以学习不同时间步骤下非固定的图形并捕捉局部特征。

    Traffic flow prediction plays a crucial role in alleviating traffic congestion and enhancing transport efficiency. While combining graph convolution networks with recurrent neural networks for spatial-temporal modeling is a common strategy in this realm, the restricted structure of recurrent neural networks limits their ability to capture global information. For spatial modeling, many prior studies learn a graph structure that is assumed to be fixed and uniform at all time steps, which may not be true. This paper introduces a novel traffic prediction framework, Global-Aware Enhanced Spatial-Temporal Graph Recurrent Network (GA-STGRN), comprising two core components: a spatial-temporal graph recurrent neural network and a global awareness layer. Within this framework, three innovative prediction models are formulated. A sequence-aware graph neural network is proposed and integrated into the Gated Recurrent Unit (GRU) to learn non-fixed graphs at different time steps and capture local te
    
[^69]: SynHIN: 生成用于可解释人工智能的合成异构信息网络

    SynHIN: Generating Synthetic Heterogeneous Information Network for Explainable AI. (arXiv:2401.04133v1 [cs.LG])

    [http://arxiv.org/abs/2401.04133](http://arxiv.org/abs/2401.04133)

    该论文提出了一种生成合成异构信息网络的方法，用于可解释人工智能。该方法通过识别现实世界数据集中的模式，构建合成网络，并确保生成的合成图数据与真实数据接近。这提供了用于节点分类任务的合成异构图数据集。

    

    图神经网络在各个领域有着优秀的表现，从检测电子商务垃圾邮件到社交网络分类问题。然而，缺乏公共图数据集阻碍了研究进展，尤其是在异构信息网络（HIN）方面。由于图神经网络解释模型的进展，对于公平的HIN比较而言，需要数据集的需求越来越大。为此，我们提出了SynHIN，一种生成合成异构信息网络的独特方法。SynHIN识别现实世界数据集中的模式，总结图统计数据，并构建合成网络。我们的方法利用In-Cluster和Out-Cluster Merge模块从主要的模式集群构建合成HIN。在In/Out-Cluster合并和符合真实数据集约束的后修剪过程后，我们确保合成的图统计数据与参考数据接近。SynHIN生成用于节点分类任务的合成异构图数据集，使用主要的模式作为输入。

    Graph Neural Networks (GNNs) excel in various domains, from detecting e-commerce spam to social network classification problems. However, the lack of public graph datasets hampers research progress, particularly in heterogeneous information networks (HIN). The demand for datasets for fair HIN comparisons is growing due to advancements in GNN interpretation models. In response, we propose SynHIN, a unique method for generating synthetic heterogeneous information networks. SynHIN identifies motifs in real-world datasets, summarizes graph statistics, and constructs a synthetic network. Our approach utilizes In-Cluster and Out-Cluster Merge modules to build the synthetic HIN from primary motif clusters. After In/Our-Cluster mergers and a post-pruning process fitting the real dataset constraints, we ensure the synthetic graph statistics align closely with the reference one. SynHIN generates a synthetic heterogeneous graph dataset for node classification tasks, using the primary motif as the
    
[^70]: 无监督的测试时自适应：通过插入和播放变换器模块

    Unsupervised Test-Time Adaptation via Plug-and-Play Transformer Modules. (arXiv:2401.04130v1 [cs.LG])

    [http://arxiv.org/abs/2401.04130](http://arxiv.org/abs/2401.04130)

    这项工作介绍了PLUTO:一种插拔式模块化的测试时领域适应策略，通过预先训练一系列针对不同源领域的模块，有效地创建了一个"模块存储库"。采用无监督的测试时自适应方法，从存储库中选择稀疏的相关模块的子集，并创建选中模块的加权组合，实现了对新领域的自适应。

    

    参数高效调优(PET)方法，如LoRA、Adapter和Visual Prompt Tuning(VPT)，通过调整变换器模型中的小模块，在使适应新领域方面取得了成功。然而，在测试过程中遇到的领域数量可能非常大，数据通常是无标签的。因此，适应新领域是具有挑战性的，也不现实为每个这样的领域生成定制的调整模块。为了应对这些挑战，本文引入了PLUTO：一种插拔模块化的测试时领域适应策略。我们预训练了一系列模块，每个模块专为不同的源领域进行了专门设计，有效地创建了一个"模块存储库"。给定一个带有少样本无标签数据的目标域，我们引入了一种无监督的测试时自适应(TTA)方法，来(1)从库中选择出稀疏的相关模块的子集，并且(2)在不调整权重的情况下创建选中模块的加权组合。这种插拔式的特性使得它可===

    Parameter-efficient tuning (PET) methods such as LoRA, Adapter, and Visual Prompt Tuning (VPT) have found success in enabling adaptation to new domains by tuning small modules within a transformer model. However, the number of domains encountered during test time can be very large, and the data is usually unlabeled. Thus, adaptation to new domains is challenging; it is also impractical to generate customized tuned modules for each such domain. Toward addressing these challenges, this work introduces PLUTO: a Plug-and-pLay modUlar Test-time domain adaptatiOn strategy. We pre-train a large set of modules, each specialized for different source domains, effectively creating a ``module store''. Given a target domain with few-shot unlabeled data, we introduce an unsupervised test-time adaptation (TTA) method to (1) select a sparse subset of relevant modules from this store and (2) create a weighted combination of selected modules without tuning their weights. This plug-and-play nature enable
    
[^71]: DeepPhysiNet: 结合深度学习和大气物理学进行准确和连续的天气模拟

    DeepPhysiNet: Bridging Deep Learning and Atmospheric Physics for Accurate and Continuous Weather Modeling. (arXiv:2401.04125v1 [physics.ao-ph])

    [http://arxiv.org/abs/2401.04125](http://arxiv.org/abs/2401.04125)

    DeepPhysiNet框架将物理定律纳入深度学习模型中，实现了准确和连续的天气系统模拟。

    

    准确的天气预报对人类活动非常重要。目前，天气预报有两种范式：数值天气预报（NWP）和基于深度学习的预测（DLP）。NWP利用大气物理学进行天气模拟，但数据利用不足和计算成本高的问题使其受到限制，而DLP可以直接从大量数据中学习天气模式，但难以融入物理定律。这两种范式各有优势和劣势，并且不互相兼容，因为NWP中采用的物理定律描述了坐标和气象变量之间的关系，而DLP直接学习气象变量之间的关系而不考虑坐标。为了解决这些问题，我们引入了DeepPhysiNet框架，将物理定律纳入深度学习模型中，以实现准确和连续的天气系统模拟。首先，我们基于多层网络构建物理网络模型。

    Accurate weather forecasting holds significant importance to human activities. Currently, there are two paradigms for weather forecasting: Numerical Weather Prediction (NWP) and Deep Learning-based Prediction (DLP). NWP utilizes atmospheric physics for weather modeling but suffers from poor data utilization and high computational costs, while DLP can learn weather patterns from vast amounts of data directly but struggles to incorporate physical laws. Both paradigms possess their respective strengths and weaknesses, and are incompatible, because physical laws adopted in NWP describe the relationship between coordinates and meteorological variables, while DLP directly learns the relationships between meteorological variables without consideration of coordinates. To address these problems, we introduce the DeepPhysiNet framework, incorporating physical laws into deep learning models for accurate and continuous weather system modeling. First, we construct physics networks based on multilay
    
[^72]: 为什么用户界面是一种黑暗模式？：可解释的自动检测及其分析。

    Why is the User Interface a Dark Pattern? : Explainable Auto-Detection and its Analysis. (arXiv:2401.04119v1 [cs.HC])

    [http://arxiv.org/abs/2401.04119](http://arxiv.org/abs/2401.04119)

    本研究通过使用BERT模型进行自动检测和LIME、SHAP等解释技术进行解释，揭示了黑暗模式中影响预测的关键术语，为用户提供防范黑暗模式的见解。

    

    黑暗模式是在线服务中误导用户行为的欺骗性用户界面设计。隐私侵犯、财务损失和情绪困扰等黑暗模式可能会对用户造成伤害。这些问题近年来一直是广泛讨论的话题。本文研究了可解释的黑暗模式自动检测，即为什么会将特定的用户界面检测为具有黑暗模式。首先，我们使用基于Transformer的预训练语言模型BERT对电子商务中的黑暗模式进行了文本数据集的自动检测模型训练。然后，我们应用了LIME和SHAP等后置解释技术对训练模型进行了解释，揭示了影响每个预测作为黑暗模式的术语。此外，我们还提取和分析了影响黑暗模式的术语。我们的研究结果可以帮助用户免受黑暗模式的操纵。

    Dark patterns are deceptive user interface designs for online services that make users behave in unintended ways. Dark patterns, such as privacy invasion, financial loss, and emotional distress, can harm users. These issues have been the subject of considerable debate in recent years. In this paper, we study interpretable dark pattern auto-detection, that is, why a particular user interface is detected as having dark patterns. First, we trained a model using transformer-based pre-trained language models, BERT, on a text-based dataset for the automatic detection of dark patterns in e-commerce. Then, we applied post-hoc explanation techniques, including local interpretable model agnostic explanation (LIME) and Shapley additive explanations (SHAP), to the trained model, which revealed which terms influence each prediction as a dark pattern. In addition, we extracted and analyzed terms that affected the dark patterns. Our findings may prevent users from being manipulated by dark patterns, 
    
[^73]: 基于时间线的流程发现

    Timeline-based Process Discovery. (arXiv:2401.04114v1 [cs.HC])

    [http://arxiv.org/abs/2401.04114](http://arxiv.org/abs/2401.04114)

    本文提出了一种基于时间线的流程发现方法，能够明确表示时间轴，该方法在两个BPIC数据集和一个专有数据集上的评估中展现出与标准布局技术相比的优势。

    

    自动流程发现的一个关键问题是提供对业务流程性能方面的洞察。在这方面，等待时间尤为重要。出乎意料的是，目前的自动流程发现技术生成直接后续图和可比较的流程模型，但往往错失了明确表示时间轴的机会。本文提出了一种自动构建与时间轴明确对齐的流程模型的方法。我们以直接后续图为例说明了我们的方法。我们使用两个BPIC数据集和一个专有数据集进行评估，突显了与标准布局技术相比，这种表示法的优势。

    A key concern of automatic process discovery is to provide insights into performance aspects of business processes. Waiting times are of particular importance in this context. For that reason, it is surprising that current techniques for automatic process discovery generate directly-follows graphs and comparable process models, but often miss the opportunity to explicitly represent the time axis. In this paper, we present an approach for automatically constructing process models that explicitly align with a time axis. We exemplify our approach for directly-follows graphs. Our evaluation using two BPIC datasets and a proprietary dataset highlight the benefits of this representation in comparison to standard layout techniques.
    
[^74]: 时间图学习的基本知识

    A Primer on Temporal Graph Learning. (arXiv:2401.03988v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.03988](http://arxiv.org/abs/2401.03988)

    本文介绍了时间图学习的基本知识，包括TGL框架中的重要概念、相关的学习架构以及经典的时间序列预测方法，为TGL的可解释学习解决方案提供了灵感。

    

    本文旨在通过概念先行的方法，使读者熟悉时间图学习（TGL）。我们系统地介绍了理解TGL框架所必不可少的重要概念。除了定性解释，我们还在适用的情况下引入了数学公式，提升了文本的清晰度。由于TGL涉及时间和空间学习，我们介绍了相关的学习架构，从循环神经网络和卷积神经网络到转换器和图神经网络。我们还讨论了经典的时间序列预测方法，以激发对TGL的可解释学习解决方案的灵感。

    This document aims to familiarize readers with temporal graph learning (TGL) through a concept-first approach. We have systematically presented vital concepts essential for understanding the workings of a TGL framework. In addition to qualitative explanations, we have incorporated mathematical formulations where applicable, enhancing the clarity of the text. Since TGL involves temporal and spatial learning, we introduce relevant learning architectures ranging from recurrent and convolutional neural networks to transformers and graph neural networks. We also discuss classical time series forecasting methods to inspire interpretable learning solutions for TGL.
    
[^75]: 课程学习：可重复性、可复制性和停止时机

    Lessons Learned: Reproducibility, Replicability, and When to Stop. (arXiv:2401.03736v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.03736](http://arxiv.org/abs/2401.03736)

    本论文从重现产品预测热带气旋生成的经验中提炼出的教训，提出了一个二维框架来指导外部研究的再现和复制。该框架结合了数据集、度量指标和模型三个关键方面，并为研究人员在自己的研究中应用前期工作和指导研究方向提供了工具。

    

    在确保自己研究的可重复性方面已经有了广泛的指导，但关于如何在自己的研究中对外部研究进行再现和复制的讨论很少。为了引发这个讨论，我们从我们重现一个用于预测热带气旋生成的运营产品的经验中提取教训，提出了一个二维框架，以提供关于再现和复制的指导。我们的框架在一个轴上表示模型拟合，在另一个轴上表示推理中的使用，依赖于数据集、度量指标和模型这三个关键方面。通过在这个二维平面上评估我们研究的轨迹，我们可以更好地说明使用我们研究得出的结论。此外，我们还使用这个框架来评估在大气科学中基准数据集的实用性。我们的二维框架为研究人员，特别是初级研究人员，提供了一个工具，使他们能够将前期工作纳入到自己的研究中，并指导研究方向。

    While extensive guidance exists for ensuring the reproducibility of one's own study, there is little discussion regarding the reproduction and replication of external studies within one's own research. To initiate this discussion, drawing lessons from our experience reproducing an operational product for predicting tropical cyclogenesis, we present a two-dimensional framework to offer guidance on reproduction and replication. Our framework, representing model fitting on one axis and its use in inference on the other, builds upon three key aspects: the dataset, the metrics, and the model itself. By assessing the trajectories of our studies on this 2D plane, we can better inform the claims made using our research. Additionally, we use this framework to contextualize the utility of benchmark datasets in the atmospheric sciences. Our two-dimensional framework provides a tool for researchers, especially early career researchers, to incorporate prior work in their own research and to inform 
    
[^76]: 广义拉格朗日神经网络

    Generalized Lagrangian Neural Networks. (arXiv:2401.03728v2 [math.DS] UPDATED)

    [http://arxiv.org/abs/2401.03728](http://arxiv.org/abs/2401.03728)

    本文介绍了对拉格朗日神经网络进行了开创性扩展的广义拉格朗日神经网络，通过将广义拉格朗日方程运用于模型构建，提高了预测准确性并保证了拉格朗日方程的成立。

    

    将神经网络用于求解常微分方程（ODEs）的方法在计算数学中具有重要的研究方向。在神经网络结构中，将ODEs的内在结构整合进去可以提高预测能力，减少数据使用量。在这些结构化ODE形式中，拉格朗日表示法由于其重要的物理基础而脱颖而出。在这个框架上，Bhattoo引入了拉格朗日神经网络（LNNs）的概念。然后在本文中，我们介绍了对拉格朗日神经网络（LNNs）进行了开创性扩展的广义拉格朗日神经网络。通过利用拉格朗日方程中拉格朗日量的基本重要性，我们根据广义拉格朗日方程来构建模型。这种修改不仅提高了预测准确性，而且保证了拉格朗日方程的成立。

    Incorporating neural networks for the solution of Ordinary Differential Equations (ODEs) represents a pivotal research direction within computational mathematics. Within neural network architectures, the integration of the intrinsic structure of ODEs offers advantages such as enhanced predictive capabilities and reduced data utilization. Among these structural ODE forms, the Lagrangian representation stands out due to its significant physical underpinnings. Building upon this framework, Bhattoo introduced the concept of Lagrangian Neural Networks (LNNs). Then in this article, we introduce a groundbreaking extension (Genralized Lagrangian Neural Networks) to Lagrangian Neural Networks (LNNs), innovatively tailoring them for non-conservative systems. By leveraging the foundational importance of the Lagrangian within Lagrange's equations, we formulate the model based on the generalized Lagrange's equation. This modification not only enhances prediction accuracy but also guarantees Lagrang
    
[^77]: 无需分词的大型语言模型能够以更准确的格式生成中国古典诗词

    Token-free LLMs Can Generate Chinese Classical Poetry with More Accurate Format. (arXiv:2401.03512v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.03512](http://arxiv.org/abs/2401.03512)

    本文提出了一种无需分词的大型语言模型（LLMs）来生成中国古典诗词，并解决了格式不准确性的问题。验证了现有基于分词的模型在字符和分词之间的关系方面的知识有限，并展示了如何通过定制模型解决这一问题。

    

    经过微调的大型语言模型（如ChatGPT和Qwen-chat）能够根据人类的指令生成中国古典诗词。虽然语言模型在内容方面表现良好，但通常在格式上存在问题，每行字符的数量有时过多或不足。由于大多数最新的语言模型是基于分词的，我们认为格式不准确是由于"分词规划"任务的难度，即语言模型需要准确知道每个分词中包含多少个字符，并基于这个知识进行长度控制规划。本文首先通过展示现有的基于分词的大型语言模型在分词和字符之间的关系方面知识有限来验证我们的假设。我们使用了拼写比赛探测程序，并发现Qwen-chat在近15%的中文拼写测试中失败。然后，我们展示了一个基于分词的模型可以轻松定制成无需分词的模型（对于中文来说），从而能够很大程度上解决格式准确性问题。

    Finetuned large language models (such as ChatGPT and Qwen-chat) can generate Chinese classical poetry following human's instructions. LLMs perform well in content, but are usually lacking in format, with occasionally excess or insufficient number of characters in each line. Since most SOTA LLMs are token-based, we assume that the format inaccuracy is due to the difficulty of the "token planning" task, which means that the LLM need to know exactly how much characters are contained in each token and do length-control planning based on that knowledge. In this paper, we first confirm our assumption by showing that existing token-based large language models has limited knowledge on token-character relationship. We use a spelling bee probing procedure, and find that Qwen-chat failed in nearly 15% Chinese spelling test. We then show that a token-based model can be easily tailored into a token-free model (in terms of Chinese), which can largely solve the format accuracy problem. Our tailoring 
    
[^78]: 多模态表示学习在分子性质预测中的应用：序列、图形、几何

    Multi-Modal Representation Learning for Molecular Property Prediction: Sequence, Graph, Geometry. (arXiv:2401.03369v2 [q-bio.MN] UPDATED)

    [http://arxiv.org/abs/2401.03369](http://arxiv.org/abs/2401.03369)

    这篇论文提出了一种新颖的多模态表示学习模型SGGRL，将序列、图形和几何特征整合在一起，用于分子性质预测。

    

    分子性质预测是将分子与某些生化性质进行标记的任务，对药物发现和设计过程起到重要作用。近年来，随着机器学习的进步，基于深度学习的分子性质预测已成为传统方法资源密集型性质的解决方案，引起了广泛关注。在这些方法中，分子表示学习是决定分子性质预测性能的关键因素。已经提出了许多基于序列、图形和几何的方法。然而，现有研究大多只关注一种模态用于学习分子表示，未能全面捕捉分子的特征和信息。本文提出了一种新颖的多模态表示学习模型SGGRL，将序列、图形和几何特征整合在一起，用于分子性质预测。

    Molecular property prediction refers to the task of labeling molecules with some biochemical properties, playing a pivotal role in the drug discovery and design process. Recently, with the advancement of machine learning, deep learning-based molecular property prediction has emerged as a solution to the resource-intensive nature of traditional methods, garnering significant attention. Among them, molecular representation learning is the key factor for molecular property prediction performance. And there are lots of sequence-based, graph-based, and geometry-based methods that have been proposed. However, the majority of existing studies focus solely on one modality for learning molecular representations, failing to comprehensively capture molecular characteristics and information. In this paper, a novel multi-modal representation learning model, which integrates the sequence, graph, and geometry characteristics, is proposed for molecular property prediction, called SGGRL. Specifically, 
    
[^79]: 在样本高效的离线强化学习中：数据多样性、后验采样，以及更多

    On Sample-Efficient Offline Reinforcement Learning: Data Diversity, Posterior Sampling, and Beyond. (arXiv:2401.03301v1 [cs.LG])

    [http://arxiv.org/abs/2401.03301](http://arxiv.org/abs/2401.03301)

    本文提出了通过数据多样性概念来统一离线强化学习算法的方法，并证明了基于版本空间、正则化优化和后验采样的算法在标准假设下达到了可比的样本效率。

    

    我们试图理解什么促进了对于序贝叶斯决策的历史数据集进行样本高效学习，这个问题通常被称为离线强化学习（RL）。此外，我们对于在利用（值）函数逼近的同时享受样本效率的算法感兴趣。在本文中，我们通过提出一个包括离线RL中覆盖度量的先前概念的数据多样性概念来解决这些基本问题，并且利用这个概念将基于版本空间（VS）、正则化优化（RO）和后验采样（PS）的三个不同类别的离线RL算法进行统一。我们在标准假设下证明，基于VS、基于RO和基于PS的算法达到了\emph{可比}的样本效率，这恢复了在有限和线性模型类别下的最优性的标准假设的边界。这个结果令人惊讶，因为之前的研究表明这些算法不具有有利性的样本效率。

    We seek to understand what facilitates sample-efficient learning from historical datasets for sequential decision-making, a problem that is popularly known as offline reinforcement learning (RL). Further, we are interested in algorithms that enjoy sample efficiency while leveraging (value) function approximation. In this paper, we address these fundamental questions by (i) proposing a notion of data diversity that subsumes the previous notions of coverage measures in offline RL and (ii) using this notion to {unify} three distinct classes of offline RL algorithms based on version spaces (VS), regularized optimization (RO), and posterior sampling (PS). We establish that VS-based, RO-based, and PS-based algorithms, under standard assumptions, achieve \emph{comparable} sample efficiency, which recovers the state-of-the-art sub-optimality bounds for finite and linear model classes with the standard assumptions. This result is surprising, given that the prior work suggested an unfavorable sa
    
[^80]: 通过切换机制，在扩散模型中实现公平抽样

    Fair Sampling in Diffusion Models through Switching Mechanism. (arXiv:2401.03140v1 [cs.LG])

    [http://arxiv.org/abs/2401.03140](http://arxiv.org/abs/2401.03140)

    本论文提出了一种称为“属性切换”的公平抽样机制，用于解决扩散模型中公平性的问题。通过在生成的数据中混淆敏感属性，该方法能够实现生成公平数据和保持数据效用的目标。

    

    扩散模型通过良好逼近潜在概率分布，在生成任务中展现了高效性。然而，扩散模型在公平性方面受到训练数据的内在偏差的放大。尽管扩散模型的抽样过程可以通过条件引导来控制，但之前的研究试图找到实证引导来实现定量公平性。为了解决这个限制，我们提出了一种称为“属性切换”机制的具有公平意识的抽样方法，用于扩散模型。在不需要额外训练的情况下，所提出的抽样方法可以在生成的数据中混淆敏感属性，而不依赖分类器。我们在两个关键方面从数学上证明了和实验证明了所提方法的有效性：(i)生成公平数据和(ii)保持生成数据的效用。

    Diffusion models have shown their effectiveness in generation tasks by well-approximating the underlying probability distribution. However, diffusion models are known to suffer from an amplified inherent bias from the training data in terms of fairness. While the sampling process of diffusion models can be controlled by conditional guidance, previous works have attempted to find empirical guidance to achieve quantitative fairness. To address this limitation, we propose a fairness-aware sampling method called \textit{attribute switching} mechanism for diffusion models. Without additional training, the proposed sampling can obfuscate sensitive attributes in generated data without relying on classifiers. We mathematically prove and experimentally demonstrate the effectiveness of the proposed method on two key aspects: (i) the generation of fair data and (ii) the preservation of the utility of the generated data.
    
[^81]: The Tactician's Web的大规模形式知识网络

    The Tactician's Web of Large-Scale Formal Knowledge. (arXiv:2401.02950v1 [cs.LO])

    [http://arxiv.org/abs/2401.02950](http://arxiv.org/abs/2401.02950)

    The Tactician's Web是一个大规模形式化数学知识网络，通过Coq证明助手和丰富的数据表示与证明工程师进行交互，并提供了机器学习、分析和证明工程的实用工具。

    

    The Tactician's Web是一个平台，提供了一个大规模的强关联的、机器验证的形式化数学知识网络，方便机器学习、分析和证明工程。基于Coq证明助手构建，该平台导出一个包含各种形式理论的数据集，呈现为定义、定理、证明项、策略和证明状态的网络。理论既可以编码为语义图（如下所示），也可以作为可读的文本，各有优缺点。证明代理可以通过相同的丰富数据表示与Coq进行交互，并可以在一组定理上自动进行基准测试。与Coq的紧密集成提供了将代理作为实用工具提供给证明工程师的独特可能性。

    The Tactician's Web is a platform offering a large web of strongly interconnected, machine-checked, formal mathematical knowledge conveniently packaged for machine learning, analytics, and proof engineering. Built on top of the Coq proof assistant, the platform exports a dataset containing a wide variety of formal theories, presented as a web of definitions, theorems, proof terms, tactics, and proof states. Theories are encoded both as a semantic graph (rendered below) and as human-readable text, each with a unique set of advantages and disadvantages. Proving agents may interact with Coq through the same rich data representation and can be automatically benchmarked on a set of theorems. Tight integration with Coq provides the unique possibility to make agents available to proof engineers as practical tools.
    
[^82]: Graph2Tac: 在定理证明中学习数学概念的分层表示

    Graph2Tac: Learning Hierarchical Representations of Math Concepts in Theorem proving. (arXiv:2401.02949v1 [cs.LG])

    [http://arxiv.org/abs/2401.02949](http://arxiv.org/abs/2401.02949)

    本文提出了一种名为Graph2Tac的图神经网络模型，用于在定理证明中学习数学概念的分层表示。该模型能够动态地将新的数学概念纳入到知识库中，并在Coq证明助手中进行训练和应用。

    

    数学及其应用中存在大量的概念。它们在不同的学科领域中有很大的变化，并且每篇数学论文或应用中都会引入新的概念。形式化理论建立了一个层次结构，其中包括了定义、定理和相互引用的证明。当一个AI代理人证明一个新的定理时，大多数与该定理相关的数学概念和引理在训练过程中可能从未被见过。这在Coq证明助手中尤为明显，该助手拥有各种各样的Coq项目，每个项目都有自己的定义、引理，甚至用于证明这些引理的自定义策略过程。将这样的新信息即时地融入到代理人的知识库中对于代理人至关重要。我们通过利用一个新的、大规模的、基于图的数据集，在Coq中进行机器学习。我们利用Coq术语的忠实图表示，创建了一种新颖的图神经网络Graph2Tac，该网络通过定义之间的依赖关系创建了一个有向图。

    Concepts abound in mathematics and its applications. They vary greatly between subject areas, and new ones are introduced in each mathematical paper or application. A formal theory builds a hierarchy of definitions, theorems and proofs that reference each other. When an AI agent is proving a new theorem, most of the mathematical concepts and lemmas relevant to that theorem may have never been seen during training. This is especially true in the Coq proof assistant, which has a diverse library of Coq projects, each with its own definitions, lemmas, and even custom tactic procedures used to prove those lemmas. It is essential for agents to incorporate such new information into their knowledge base on the fly. We work towards this goal by utilizing a new, large-scale, graph-based dataset for machine learning in Coq. We leverage a faithful graph-representation of Coq terms that induces a directed graph of dependencies between definitions to create a novel graph neural network, Graph2Tac (G
    
[^83]: 弱半监督下的微创手术视频工具检测

    Weakly Semi-supervised Tool Detection in Minimally Invasive Surgery Videos. (arXiv:2401.02791v1 [cs.CV])

    [http://arxiv.org/abs/2401.02791](http://arxiv.org/abs/2401.02791)

    本研究提出了一种在微创手术视频中弱半监督下检测手术工具的方法。通过使用共现损失来利用图像级标签中工具对之间的共现关系，平衡了注释负担和检测性能，克服了分类困难。

    

    手术工具检测对于分析和评估微创手术视频至关重要。目前的方法主要基于需要大量完整的实例级标签（即边界框）的监督方法。然而，由于注释的负担，具有实例级标签的大型图像数据集通常很有限。因此，在提供图像级标签而不是实例级标签时，手术工具检测变得重要，因为图像级注释比实例级注释更具时间效率。在这项工作中，我们提出在极高的注释负担和检测性能之间寻求平衡。我们进一步提出了一种共现损失，该损失考虑了某些工具对在图像中经常共同出现的特性，以利用图像级标签。用共现损失对共现关系的知识进行封装有助于克服分类困难，因为一些手术工具的分类困难源于这样一个事实：它们经常以成对出现。

    Surgical tool detection is essential for analyzing and evaluating minimally invasive surgery videos. Current approaches are mostly based on supervised methods that require large, fully instance-level labels (i.e., bounding boxes). However, large image datasets with instance-level labels are often limited because of the burden of annotation. Thus, surgical tool detection is important when providing image-level labels instead of instance-level labels since image-level annotations are considerably more time-efficient than instance-level annotations. In this work, we propose to strike a balance between the extremely costly annotation burden and detection performance. We further propose a co-occurrence loss, which considers a characteristic that some tool pairs often co-occur together in an image to leverage image-level labels. Encapsulating the knowledge of co-occurrence using the co-occurrence loss helps to overcome the difficulty in classification that originates from the fact that some 
    
[^84]: 基于视图的图神经网络解释

    View-based Explanations for Graph Neural Networks. (arXiv:2401.02086v1 [cs.LG])

    [http://arxiv.org/abs/2401.02086](http://arxiv.org/abs/2401.02086)

    这篇论文提出了一种基于视图的解释方法来解释图神经网络(GNNs)的行为，通过生成解释视图和图模式来解释特定类别的结果。

    

    研究生成图神经网络(GNNs)的解释，以了解它们在图分类等分析任务中的行为。现有的方法旨在理解GNNs的整体结果，而不是针对特定类别的解释，并且可能返回难以访问或直接查询的解释结构。我们提出了一种新颖的范式GVEX，用于生成图解释的图视图。我们设计了一种两层的解释结构，称为解释视图。解释视图包括一组图模式和一组诱发的解释子图。给定一个包含多个图和由基于GNN的分类器M分配的特定类别标签l的数据库G，它简洁地描述了最好解释为什么l由M分配的G的分数。我们提出了质量度量方法，并制定了一个优化问题来计算GNN解释的最佳解释视图。我们证明了该问题是Σ^2_P难的。

    Generating explanations for graph neural networks (GNNs) has been studied to understand their behavior in analytical tasks such as graph classification. Existing approaches aim to understand the overall results of GNNs rather than providing explanations for specific class labels of interest, and may return explanation structures that are hard to access, nor directly queryable.  We propose GVEX, a novel paradigm that generates Graph Views for EXplanation. (1) We design a two-tier explanation structure called explanation views. An explanation view consists of a set of graph patterns and a set of induced explanation subgraphs. Given a database G of multiple graphs and a specific class label l assigned by a GNN-based classifier M, it concisely describes the fraction of G that best explains why l is assigned by M. (2) We propose quality measures and formulate an optimization problem to compute optimal explanation views for GNN explanation. We show that the problem is $\Sigma^2_P$-hard. (3) 
    
[^85]: GPS-SSL: 引导正样本采样将先验知识注入到自监督学习中

    GPS-SSL: Guided Positive Sampling to Inject Prior Into Self-Supervised Learning. (arXiv:2401.01990v1 [cs.CV])

    [http://arxiv.org/abs/2401.01990](http://arxiv.org/abs/2401.01990)

    GPS-SSL是一种将先验知识注入到自监督学习中的通用方法，通过设计度量空间并利用最近邻采样生成正样本。它可以减少对强数据增强的依赖，因此在Cifar10上达到了更好的效果。

    

    我们提出了引导正样本采样自监督学习（GPS-SSL），这是一种将先验知识注入到自监督学习（SSL）正样本选择的通用方法。当前的SSL方法利用数据增强（DA）生成正样本，并将先验知识结合进去，但是错误或者过弱的DA会严重降低所学到的表示的质量。GPS-SSL则提出设计一个度量空间，使得欧氏距离成为语义关系的有意义的替代。在这个空间中，可以通过最近邻采样生成正样本。任何先验知识都可以独立地嵌入到这个度量空间中，而不受所使用的DA影响。由于其简单性，GPS-SSL适用于任何SSL方法，如SimCLR或BYOL。GPS-SSL的一个关键好处是减少了定制强DA的压力。例如，GPS-SSL在Cifar10上使用弱DA达到了85.58％，而基准值只达到了37.51％。

    We propose Guided Positive Sampling Self-Supervised Learning (GPS-SSL), a general method to inject a priori knowledge into Self-Supervised Learning (SSL) positive samples selection. Current SSL methods leverage Data-Augmentations (DA) for generating positive samples and incorporate prior knowledge - an incorrect, or too weak DA will drastically reduce the quality of the learned representation. GPS-SSL proposes instead to design a metric space where Euclidean distances become a meaningful proxy for semantic relationship. In that space, it is now possible to generate positive samples from nearest neighbor sampling. Any prior knowledge can now be embedded into that metric space independently from the employed DA. From its simplicity, GPS-SSL is applicable to any SSL method, e.g. SimCLR or BYOL. A key benefit of GPS-SSL is in reducing the pressure in tailoring strong DAs. For example GPS-SSL reaches 85.58% on Cifar10 with weak DA while the baseline only reaches 37.51%. We therefore move a 
    
[^86]: 多语言指令调优中的多语言性

    Multilingual Instruction Tuning With Just a Pinch of Multilinguality. (arXiv:2401.01854v1 [cs.CL])

    [http://arxiv.org/abs/2401.01854](http://arxiv.org/abs/2401.01854)

    本研究研究了多语言指令调优中的多语言性对跨语言指令遵循的影响。研究发现，即使在单语调优过程中，许多语言也可以将一些指令遵循能力转移到其他语言上。此外，只有40个多语言示例能够显著提高多语言指令遵循。总体来说，多语言混合调优的模型在多种语言上的表现相比单语调优的模型要好或者不相上下，尽管使用的这些语言的训练示例数量只有10倍少。

    

    随着大型语言模型（LLMs）的全球采纳，它们在多语言指令遵循能力变得越来越重要。一种有前途的方法是跨语言转移，通过在另一种语言上微调，模型可以在某种语言上获得特定的功能。本文研究了多语言LLM在指令调优过程中的多语言性对跨语言指令遵循的影响。首先我们发现，即使在单语调优过程中，许多语言也可以将一些指令遵循能力转移到其他语言上。此外，我们发现在英语调优集合中，只有40个多语言示例能够显著提高多语言指令遵循，在调优过程中不论是已见语言还是未见语言。总的来说，我们观察到在多语言混合调优的模型在多种语言上的表现相比单语调优的模型要好或者不相上下，尽管使用的这些语言的训练示例数量只有10倍少。

    As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial. One promising approach is cross-lingual transfer, where a model acquires specific functionality on some language by finetuning on another language. In this work, we investigate how multilinguality during instruction tuning of a multilingual LLM affects instruction-following across languages. We first show that many languages transfer some instruction-following capabilities to other languages from even monolingual tuning. Furthermore, we find that only 40 multilingual examples in an English tuning set substantially improve multilingual instruction-following, both in seen and unseen languages during tuning. In general, we observe that models tuned on multilingual mixtures exhibit comparable or superior performance in several languages compared to monolingually tuned models, despite training on 10x fewer examples in those language
    
[^87]: 大型语言模型的知识编辑全面研究

    A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v1 [cs.CL])

    [http://arxiv.org/abs/2401.01286](http://arxiv.org/abs/2401.01286)

    本研究全面研究了大型语言模型的知识编辑，旨在有效修改模型的行为，同时保持整体性能。

    

    大型语言模型(LLM)在理解和生成与人类交流紧密相似的文本方面展现出了非凡的能力。然而，其主要限制在于训练过程中的显著计算需求，这是由于其广泛的参数化造成的。这一挑战在于世界的动态性，需要频繁更新LLM以修正过时的信息或集成新知识，从而确保其持续的相关性。许多应用需要在训练后进行持续的模型调整，以解决缺陷或不良行为。近年来，对于LLM的知识编辑技术的兴趣越来越高，在特定领域内有效地修改LLM的行为，同时保持整体性能在各种输入中的表现。本文首先定义了知识编辑的目标和挑战，然后综述了现有的知识编辑方法和技术，并讨论了其应用和未来发展的方向。

    Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication. However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization. This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance. Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors. There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications. To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs' behaviors within specific domains while preserving overall performance across various inputs. In this paper, we first define the kno
    
[^88]: 具有增量学习和自蒸馏的联邦化分类学习

    Federated Class-Incremental Learning with New-Class Augmented Self-Distillation. (arXiv:2401.00622v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.00622](http://arxiv.org/abs/2401.00622)

    该论文提出了一种具有增量学习和自蒸馏的联邦化分类学习方法，通过丰富历史模型的类别分数并利用结合的知识进行自蒸馏，实现了更充分精确的知识传递。

    

    联邦学习（FL）使参与者能够在保护原始数据隐私的同时进行协作模型训练。主流的FL方法忽视了现实世界数据的动态特性，尤其是数据在时间上的增长和类别上的多样化。这一忽视导致FL方法在吸收新数据时不可避免地遗忘之前学习到的信息。为了应对这一挑战，我们提出了一种新颖的联邦化增量学习方法，称为联邦化增量学习与增强自蒸馏（FedCLASS）。FedCLASS的核心是通过当前模型预测的新类别分数丰富历史模型的类别分数，并利用结合的知识进行自蒸馏，从而实现了更充分精确的知识传递，从历史模型到当前模型。

    Federated Learning (FL) enables collaborative model training among participants while guaranteeing the privacy of raw data. Mainstream FL methodologies overlook the dynamic nature of real-world data, particularly its tendency to grow in volume and diversify in classes over time. This oversight results in FL methods suffering from catastrophic forgetting, where the trained models inadvertently discard previously learned information upon assimilating new data. In response to this challenge, we propose a novel Federated Class-Incremental Learning (FCIL) method, named \underline{Fed}erated \underline{C}lass-Incremental \underline{L}earning with New-Class \underline{A}ugmented \underline{S}elf-Di\underline{S}tillation (FedCLASS). The core of FedCLASS is to enrich the class scores of historical models with new class scores predicted by current models and utilize the combined knowledge for self-distillation, enabling a more sufficient and precise knowledge transfer from historical models to c
    
[^89]: 用基于模拟推断的孤立脉冲星种群合成

    Isolated pulsar population synthesis with simulation-based inference. (arXiv:2312.14848v1 [astro-ph.HE] CROSS LISTED)

    [http://arxiv.org/abs/2312.14848](http://arxiv.org/abs/2312.14848)

    本论文使用模拟推断方法结合脉冲星种群合成，来限制孤立银河射电脉冲星的磁旋转特性。

    

    我们将脉冲星种群合成与基于模拟推断相结合，以限制孤立银河射电脉冲星的磁旋转特性。我们首先构建了一个灵活的框架来模拟中子星的诞生特性和演化，重点是它们的动力学、旋转和磁性特征。特别是，我们从对数正态分布中采样初始磁场强度B和自转周期P，并用幂律来捕捉后期磁场的衰减。每个对数正态分布由均值μlogB，μlogP和标准差σlogB，σlogP描述，而幂律由指数a_late描述，共计五个自由参数。然后我们模拟了星体的射电发射和观测偏差，以模拟三个射电调查中的探测，并通过改变输入参数产生了一个大型的合成P-Ṗ图数据库。接着我们采用基于模拟推断的方法进行推断

    We combine pulsar population synthesis with simulation-based inference to constrain the magneto-rotational properties of isolated Galactic radio pulsars. We first develop a flexible framework to model neutron-star birth properties and evolution, focusing on their dynamical, rotational and magnetic characteristics. In particular, we sample initial magnetic-field strengths, $B$, and spin periods, $P$, from log-normal distributions and capture the late-time magnetic-field decay with a power law. Each log-normal is described by a mean, $\mu_{\log B}, \mu_{\log P}$, and standard deviation, $\sigma_{\log B}, \sigma_{\log P}$, while the power law is characterized by the index, $a_{\rm late}$, resulting in five free parameters. We subsequently model the stars' radio emission and observational biases to mimic detections with three radio surveys, and produce a large database of synthetic $P$-$\dot{P}$ diagrams by varying our input parameters. We then follow a simulation-based inference approach 
    
[^90]: 时间变换器：融合本地和全局特征以实现更好的时间序列生成

    Time-Transformer: Integrating Local and Global Features for Better Time Series Generation. (arXiv:2312.11714v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.11714](http://arxiv.org/abs/2312.11714)

    本文提出了一种新的时间序列生成模型，通过时间变换器同时学习本地和全局特征，实现了对时间序列数据的更好生成能力。

    

    生成时间序列数据是解决数据不足问题的一种有前景的方法。然而，由于时间序列数据的复杂时间特性，包括本地相关性和全局依赖性，使其成为具有挑战性的任务。大多数现有的生成模型未能有效学习时间序列数据的本地和全局特性。为了解决这个问题，我们提出了一种新颖的时间序列生成模型，命名为'时间变换器AAE'，它由一个对抗性自动编码器（AAE）和一个名为'时间变换器'的新设计架构组成。时间变换器首先通过层次并行设计同时学习本地和全局特征，结合了时间卷积网络和Transformer的能力，分别提取本地特征和全局依赖性。其次，提出了一个双向交叉注意力来在两个分支之间提供互补的引导，并实现本地特征和全局特征的合适融合。

    Generating time series data is a promising approach to address data deficiency problems. However, it is also challenging due to the complex temporal properties of time series data, including local correlations as well as global dependencies. Most existing generative models have failed to effectively learn both the local and global properties of time series data. To address this open problem, we propose a novel time series generative model named 'Time-Transformer AAE', which consists of an adversarial autoencoder (AAE) and a newly designed architecture named 'Time-Transformer' within the decoder. The Time-Transformer first simultaneously learns local and global features in a layer-wise parallel design, combining the abilities of Temporal Convolutional Networks and Transformer in extracting local features and global dependencies respectively. Second, a bidirectional cross attention is proposed to provide complementary guidance across the two branches and achieve proper fusion between loc
    
[^91]: STEAM & MoSAFE: AI-Enabled Driving Automation的SOTIF错误与故障模型分析

    STEAM & MoSAFE: SOTIF Error-and-Failure Model & Analysis for AI-Enabled Driving Automation. (arXiv:2312.09559v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.09559](http://arxiv.org/abs/2312.09559)

    本文通过定义SOTIF时间误差与故障模型(STEAM)，填补了现有标准中在识别和评估AI引起的危险错误方面的不足，以更好地评估AI驱动的驾驶自动化系统中的安全风险。

    

    驾驶自动化系统(DAS)受复杂的道路环境和车辆行为的影响，越来越依赖于先进的传感器和人工智能(AI)。这些特性导致了源自规范不足和技术性能限制的特殊安全故障，其中传感器和AI引入的错误在大小和时间模式上变化，可能造成安全风险。安全的预期功能性(SOTIF)标准是应对这些问题的一种有希望的框架，专注于基于场景的分析以识别危险行为及其原因。尽管当前的标准提供了基本的因果模型和高级流程指导，但在AI的上下文中，它缺乏用于识别和评估危险错误的概念。本文提出了两个关键贡献来填补这一差距。首先，它将SOTIF时间误差与故障模型(STEAM)定义为一种细化的模型，包括用于描述AI引起的各种错误的要素。通过这一模型，可以更好地评估AI驱动的驾驶自动化系统中的安全风险。

    Driving Automation Systems (DAS) are subject to complex road environments and vehicle behaviors and increasingly rely on sophisticated sensors and Artificial Intelligence (AI). These properties give rise to unique safety faults stemming from specification insufficiencies and technological performance limitations, where sensors and AI introduce errors that vary in magnitude and temporal patterns, posing potential safety risks. The Safety of the Intended Functionality (SOTIF) standard emerges as a promising framework for addressing these concerns, focusing on scenario-based analysis to identify hazardous behaviors and their causes. Although the current standard provides a basic cause-and-effect model and high-level process guidance, it lacks concepts required to identify and evaluate hazardous errors, especially within the context of AI.  This paper introduces two key contributions to bridge this gap. First, it defines the SOTIF Temporal Error and Failure Model (STEAM) as a refinement of
    
[^92]: 随机逼近的收敛速度：带有无界方差的有偏噪声和应用

    Convergence Rates for Stochastic Approximation: Biased Noise with Unbounded Variance, and Applications. (arXiv:2312.02828v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2312.02828](http://arxiv.org/abs/2312.02828)

    本论文研究了带有无界方差的有偏噪声对随机逼近算法的收敛速度的影响，并介绍了该算法在各个领域的应用。

    

    1951年罗宾斯和莫洛引入的随机逼近（SA）算法已经成为解方程$\mathbf{f}({\boldsymbol{\theta}}) = \mathbf{0}$的标准方法，当只有$\mathbf{f}(\cdot)$的带噪声测量可用时。如果对于某个函数$J(\cdot)$，$\mathbf{f}({\boldsymbol{\theta}}) = \nabla J({\boldsymbol{\theta}})$，那么SA也可以用来寻找$J(\cdot)$的一个稳定点。在每个时间$t$，当前的猜测${\boldsymbol{\theta}}_t$通过形式为$\mathbf{f}({\boldsymbol{\theta}}_t) + {\boldsymbol{\xi}}_{t+1}$的带噪声测量更新为${\boldsymbol{\theta}}_{t+1}$。在许多文献中，假设误差项${\boldsymbol{\xi}}_{t+1}$的条件均值为零，和/或者它的条件方差随$t$（而不是${\boldsymbol{\theta}}_t$）被限制。多年来，SA已经应用于各种领域，本文重点研究其中一个领域。

    The Stochastic Approximation (SA) algorithm introduced by Robbins and Monro in 1951 has been a standard method for solving equations of the form $\mathbf{f}({\boldsymbol {\theta}}) = \mathbf{0}$, when only noisy measurements of $\mathbf{f}(\cdot)$ are available. If $\mathbf{f}({\boldsymbol {\theta}}) = \nabla J({\boldsymbol {\theta}})$ for some function $J(\cdot)$, then SA can also be used to find a stationary point of $J(\cdot)$. At each time $t$, the current guess ${\boldsymbol {\theta}}_t$ is updated to ${\boldsymbol {\theta}}_{t+1}$ using a noisy measurement of the form $\mathbf{f}({\boldsymbol {\theta}}_t) + {\boldsymbol {\xi}}_{t+1}$. In much of the literature, it is assumed that the error term ${\boldsymbol {\xi}}_{t+1}$ has zero conditional mean, and/or that its conditional variance is bounded as a function of $t$ (though not necessarily with respect to ${\boldsymbol {\theta}}_t$). Over the years, SA has been applied to a variety of areas, out of which the focus in this paper i
    
[^93]: 先进的大型语言模型（LLM）驱动的Verilog开发：在代码合成中增强功耗、性能和面积优化

    Advanced Large Language Model (LLM)-Driven Verilog Development: Enhancing Power, Performance, and Area Optimization in Code Synthesis. (arXiv:2312.01022v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.01022](http://arxiv.org/abs/2312.01022)

    本研究介绍了先进语言模型（ALMs）在Verilog编程中的应用，通过采用创新的框架和双阶段细化协议，能够提高代码的精确性和与功耗-性能-面积（PPA）基准的对齐性。

    

    本研究探讨了先进语言模型（ALMs）在电子硬件设计中的应用，特别关注Verilog编程的合成和增强。我们引入了一种创新的框架，旨在评估和提高ALMs在这一领域的生产力。该方法从通过ALMs进行Verilog编程的初始构建开始，然后采用独特的双阶段细化协议。首个阶段优先增强代码的操作和语言精确性，而后一个阶段则致力于将代码与功耗-性能-面积（PPA）基准对齐，这是有效硬件设计的关键组成部分。这种将错误修复与PPA增强相结合的分流策略，显著提升了ALM创建的代码的质量。

    The increasing use of Advanced Language Models (ALMs) in diverse sectors, particularly due to their impressive capability to generate top-tier content following linguistic instructions, forms the core of this investigation. This study probes into ALMs' deployment in electronic hardware design, with a specific emphasis on the synthesis and enhancement of Verilog programming. We introduce an innovative framework, crafted to assess and amplify ALMs' productivity in this niche. The methodology commences with the initial crafting of Verilog programming via ALMs, succeeded by a distinct dual-stage refinement protocol. The premier stage prioritizes augmenting the code's operational and linguistic precision, while the latter stage is dedicated to aligning the code with Power-Performance-Area (PPA) benchmarks, a pivotal component in proficient hardware design. This bifurcated strategy, merging error remediation with PPA enhancement, has yielded substantial upgrades in the caliber of ALM-created
    
[^94]: 医学图像的深度交互式分割：系统综述和分类

    Deep Interactive Segmentation of Medical Images: A Systematic Review and Taxonomy. (arXiv:2311.13964v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2311.13964](http://arxiv.org/abs/2311.13964)

    本综述系统地总结了医学图像交互式分割的最新研究进展，通过深度学习的方法推动了领域发展，但目前存在方法之间的比较缺乏的问题。

    

    交互式分割是医学图像分析中的关键研究领域，旨在通过结合人类反馈来提高昂贵注释的效率。这些反馈以点击、涂鸦或掩膜的形式出现，允许对模型输出进行迭代优化，以便有效引导系统实现所需的行为。近年来，基于深度学习的方法推动了结果达到一个新水平，导致该领域快速增长，仅医学影像领域就提出了121种方法。在本综述中，我们提供了这一新兴领域的结构化概述，包括全面的分类法、现有方法的系统综述以及对当前实践的深入分析。基于这些贡献，我们讨论了该领域的挑战和机遇。例如，我们发现方法之间的比较严重缺乏，需要通过标准化基准和基准数据进行解决。

    Interactive segmentation is a crucial research area in medical image analysis aiming to boost the efficiency of costly annotations by incorporating human feedback. This feedback takes the form of clicks, scribbles, or masks and allows for iterative refinement of the model output so as to efficiently guide the system towards the desired behavior. In recent years, deep learning-based approaches have propelled results to a new level causing a rapid growth in the field with 121 methods proposed in the medical imaging domain alone. In this review, we provide a structured overview of this emerging field featuring a comprehensive taxonomy, a systematic review of existing methods, and an in-depth analysis of current practices. Based on these contributions, we discuss the challenges and opportunities in the field. For instance, we find that there is a severe lack of comparison across methods which needs to be tackled by standardized baselines and benchmarks.
    
[^95]: LLMs无法找到推理错误，但可以纠正它们！（arXiv：2311.08516v2 [cs.AI] UPDATED）

    LLMs cannot find reasoning errors, but can correct them!. (arXiv:2311.08516v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2311.08516](http://arxiv.org/abs/2311.08516)

    本文研究了LLMs在自我纠正过程中的错误发现和输出纠正两个核心组成部分。研究发现LLMs通常难以发现逻辑错误，但通过使用回溯方法可以在提供错误位置信息时获得大幅改进。

    

    尽管自我纠正在改善LLM输出的风格和质量方面显示出了潜力（例如Chen等，2023；Madaan等，2023），最近对逻辑或推理错误进行自我纠正的尝试通常会导致正确答案变为错误，从而总体表现变差（Huang等，2023）。在本文中，我们将自我纠正过程分解为两个核心组成部分：错误发现和输出纠正。对于错误发现，我们发布了BIG-Bench Mistake，这是一个Chain-of-Thought推理轨迹中的逻辑错误数据集。我们为几种最先进的LLM提供基准数，并证明LLM通常难以发现逻辑错误。对于输出纠正，我们提出了一种回溯方法，在提供错误位置信息时可以大幅改进。我们将回溯解释为对强化学习方法的轻量级替代方案，并展示了在60-70％准确率下保持有效性的奖励模型。

    While self-correction has shown promise in improving LLM outputs in terms of style and quality (e.g. Chen et al., 2023; Madaan et al., 2023), recent attempts to self-correct logical or reasoning errors often cause correct answers to become incorrect, resulting in worse performances overall (Huang et al., 2023). In this paper, we break down the self-correction process into two core components: mistake finding and output correction. For mistake finding, we release BIG-Bench Mistake, a dataset of logical mistakes in Chain-of-Thought reasoning traces. We provide benchmark numbers for several state-of-the-art LLMs, and demonstrate that LLMs generally struggle with finding logical mistakes. For output correction, we propose a backtracking method which provides large improvements when given information on mistake location. We construe backtracking as a lightweight alternative to reinforcement learning methods, and show that it remains effective with a reward model at 60-70% accuracy.
    
[^96]: 通过层间特征压缩和差别性学习理解深度表示学习

    Understanding Deep Representation Learning via Layerwise Feature Compression and Discrimination. (arXiv:2311.02960v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.02960](http://arxiv.org/abs/2311.02960)

    本文通过研究中间特征的结构，揭示了深度网络在层级特征学习过程中的演化模式。研究发现线性层在特征学习中起到了与深层非线性网络类似的作用。

    

    在过去的十年中，深度学习已经证明是从原始数据中学习有意义特征的一种高效工具。然而，深度网络如何在不同层级上进行等级特征学习仍然是一个开放问题。在这项工作中，我们试图通过研究中间特征的结构揭示这个谜团。受到我们实证发现的线性层在特征学习中模仿非线性网络中深层的角色的启发，我们研究了深度线性网络如何将输入数据转化为输出，通过研究训练后的每个层的输出（即特征）在多类分类问题的背景下。为了实现这个目标，我们首先定义了衡量中间特征的类内压缩和类间差别性的度量标准。通过对这两个度量标准的理论分析，我们展示了特征从浅层到深层的演变遵循着一种简单而量化的模式，前提是输入数据是

    Over the past decade, deep learning has proven to be a highly effective tool for learning meaningful features from raw data. However, it remains an open question how deep networks perform hierarchical feature learning across layers. In this work, we attempt to unveil this mystery by investigating the structures of intermediate features. Motivated by our empirical findings that linear layers mimic the roles of deep layers in nonlinear networks for feature learning, we explore how deep linear networks transform input data into output by investigating the output (i.e., features) of each layer after training in the context of multi-class classification problems. Toward this goal, we first define metrics to measure within-class compression and between-class discrimination of intermediate features, respectively. Through theoretical analysis of these two metrics, we show that the evolution of features follows a simple and quantitative pattern from shallow to deep layers when the input data is
    
[^97]: PolyThrottle: 边缘设备上的节能神经网络推理

    PolyThrottle: Energy-efficient Neural Network Inference on Edge Devices. (arXiv:2310.19991v1 [cs.LG])

    [http://arxiv.org/abs/2310.19991](http://arxiv.org/abs/2310.19991)

    PolyThrottle是一种在边缘设备上进行节能神经网络推理的解决方案，通过优化设备上的硬件元素配置，可以达到高达36%的能量节省，并且能够快速收敛到近乎最优的设置。

    

    随着神经网络在各个领域的部署，其能量需求也相应增长。尽管一些先前的工作专注于在训练过程中减少能量消耗，但是ML驱动系统的连续运行在推理过程中会导致显著的能量消耗。本文调查了设备上的硬件元素配置（如GPU、内存和CPU频率）在常规微调的神经网络推理中如何影响能量消耗，这在先前的研究中经常被忽视。我们提出了一种名为PolyThrottle的解决方案，通过受约束的贝叶斯优化以节能的方式对各个硬件组件的配置进行优化。我们的实证评估揭示了能量性能平衡的新颖方面，表明我们可以为流行模型节省高达36%的能量。我们还验证了PolyThrottle可以在满足应用约束条件的同时快速收敛到接近最优的设置。

    As neural networks (NN) are deployed across diverse sectors, their energy demand correspondingly grows. While several prior works have focused on reducing energy consumption during training, the continuous operation of ML-powered systems leads to significant energy use during inference. This paper investigates how the configuration of on-device hardware-elements such as GPU, memory, and CPU frequency, often neglected in prior studies, affects energy consumption for NN inference with regular fine-tuning. We propose PolyThrottle, a solution that optimizes configurations across individual hardware components using Constrained Bayesian Optimization in an energy-conserving manner. Our empirical evaluation uncovers novel facets of the energy-performance equilibrium showing that we can save up to 36 percent of energy for popular models. We also validate that PolyThrottle can quickly converge towards near-optimal settings while satisfying application constraints.
    
[^98]: 基于因果结构的文本离群值泛化增强方法

    Causal-structure Driven Augmentations for Text OOD Generalization. (arXiv:2310.12803v1 [cs.LG])

    [http://arxiv.org/abs/2310.12803](http://arxiv.org/abs/2310.12803)

    本文提出了一种基于因果结构的反事实数据增强方法，用于改善文本分类器在应用中的泛化效果，特别适用于存在虚假相关性的标签与属性预测问题。

    

    文本分类器对虚假相关性的依赖可能导致在实际应用中的泛化效果不佳，这引发了对其在如医疗领域等安全关键行业中使用的担忧。在本研究中，我们提出使用因果结构知识指导的反事实数据增强方法，模拟对虚假特征进行干预，以学习更加鲁棒的文本分类器。我们证明了在标签与属性之间存在虚假相关性的预测问题中，这种策略是合适的。在这种问题的假设下，我们讨论了反事实数据增强相对于重要性重加权的有利样本复杂性。实际上，我们使用辅助数据通过差分在差分的方法来匹配样本，并使用大型语言模型（LLM）来表示文本的条件概率。通过对从医学叙述中学习与看护者无关的临床诊断预测器以及半合成数据上进行了广泛的实验。

    The reliance of text classifiers on spurious correlations can lead to poor generalization at deployment, raising concerns about their use in safety-critical domains such as healthcare. In this work, we propose to use counterfactual data augmentation, guided by knowledge of the causal structure of the data, to simulate interventions on spurious features and to learn more robust text classifiers. We show that this strategy is appropriate in prediction problems where the label is spuriously correlated with an attribute. Under the assumptions of such problems, we discuss the favorable sample complexity of counterfactual data augmentation, compared to importance re-weighting. Pragmatically, we match examples using auxiliary data, based on diff-in-diff methodology, and use a large language model (LLM) to represent a conditional probability of text. Through extensive experimentation on learning caregiver-invariant predictors of clinical diagnoses from medical narratives and on semi-synthetic 
    
[^99]: CORN: 全参考和非参考音频度量的共训练模型

    CORN: Co-Trained Full-Reference And No-Reference Audio Metrics. (arXiv:2310.09388v1 [eess.AS])

    [http://arxiv.org/abs/2310.09388](http://arxiv.org/abs/2310.09388)

    CORN是一个新颖的框架，将全参考和非参考音频度量结合起来，并尝试在训练时同时训练这两种模型。CORN FR模式同时具备全参考和非参考度量的性能。

    

    感知评估是各种音频处理任务中至关重要的方面。全参考（FR）或基于相似性的度量依赖于高质量的参考录音，将其与录音的低质量或损坏版本进行比较以进行评估。相反，非参考（NR）度量评估录音而不依赖参考。FR和NR两种方法相对于彼此都具有优势和缺点。本文中，我们提出了一种新颖的框架称为CORN，将这两种方法结合起来，同时训练FR和NR模型。训练完成后，可以独立应用这些模型。我们通过预测几个常见的客观度量指标以及在两种不同架构上进行评估CORN。使用CORN训练的NR模型在训练期间可以访问参考录音，因此可以预期，它始终优于独立训练的基线NR模型。更令人印象深刻的是CORN FR模式可以同时提供全参考和非参考度量的性能。

    Perceptual evaluation constitutes a crucial aspect of various audio-processing tasks. Full reference (FR) or similarity-based metrics rely on high-quality reference recordings, to which lower-quality or corrupted versions of the recording may be compared for evaluation. In contrast, no-reference (NR) metrics evaluate a recording without relying on a reference. Both the FR and NR approaches exhibit advantages and drawbacks relative to each other. In this paper, we present a novel framework called CORN that amalgamates these dual approaches, concurrently training both FR and NR models together. After training, the models can be applied independently. We evaluate CORN by predicting several common objective metrics and across two different architectures. The NR model trained using CORN has access to a reference recording during training, and thus, as one would expect, it consistently outperforms baseline NR models trained independently. Perhaps even more remarkable is that the CORN FR mode
    
[^100]: 在基础模型时代的风险评估和统计显著性

    Risk Assessment and Statistical Significance in the Age of Foundation Models. (arXiv:2310.07132v1 [cs.LG])

    [http://arxiv.org/abs/2310.07132](http://arxiv.org/abs/2310.07132)

    本论文提出了一个分布框架，用于评估具有统计显著性的基础模型的风险。通过一种新的统计相对测试方法，该框架结合了一阶和二阶随机优势，并借鉴了计量经济学和数学金融中常用的平均风险模型。在给定指定度量量化的防护栏的情况下，我们还开发了一种基于风险意识的基础模型选择方法。受数学金融中的投资组合优化和选择理论的启发，我们为每个模型定义了一个"度量组合"，并根据这些组合的随机优势进行模型选择。

    

    我们提出了一个分布框架，用于评估具有统计显著性的基础模型的社会技术风险。我们的方法依赖于一种基于实际随机变量的一阶和二阶随机优势的新的统计相对测试。我们表明，这个测试中的二阶统计与在计量经济学和数学金融中常用的平均风险模型相联系，用于在选择方案时平衡风险和效用。利用这个框架，我们正式开发了一种基于风险意识的基础模型选择方法，给定由指定度量量化的防护栏。受数学金融中的投资组合优化和选择理论的启发，我们为每个模型定义了一个"度量组合"，作为聚合一系列度量的手段，并根据这些组合的随机优势进行模型选择。我们的测试的统计显著性在理论上由通过中心极限的渐近分析支持。

    We propose a distributional framework for assessing socio-technical risks of foundation models with quantified statistical significance. Our approach hinges on a new statistical relative testing based on first and second order stochastic dominance of real random variables. We show that the second order statistics in this test are linked to mean-risk models commonly used in econometrics and mathematical finance to balance risk and utility when choosing between alternatives. Using this framework, we formally develop a risk-aware approach for foundation model selection given guardrails quantified by specified metrics. Inspired by portfolio optimization and selection theory in mathematical finance, we define a \emph{metrics portfolio} for each model as a means to aggregate a collection of metrics, and perform model selection based on the stochastic dominance of these portfolios. The statistical significance of our tests is backed theoretically by an asymptotic analysis via central limit th
    
[^101]: FABind: 快速准确的蛋白-配体结合

    FABind: Fast and Accurate Protein-Ligand Binding. (arXiv:2310.06763v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.06763](http://arxiv.org/abs/2310.06763)

    FABind是一个结合了口袋预测和对接的端到端模型，旨在实现快速准确的蛋白-配体结合预测。

    

    在药物发现中，对蛋白质和配体之间的相互作用进行建模并准确预测其结合结构是一项关键但具有挑战性的任务。深度学习的最新进展在应对这一挑战方面显示出了希望，采样法和回归法成为两种突出的方法。然而，这些方法都存在明显的局限性。采样法通常由于需要生成多个候选结构来进行选择而效率较低。而回归法提供了快速的预测，但可能会导致准确性降低。另外，蛋白质大小的变化通常需要外部模块来选择合适的结合口袋，进一步影响效率。在这项工作中，我们提出了FABind，一个将口袋预测和对接相结合的端到端模型，以实现准确和快速的蛋白-配体结合。

    Modeling the interaction between proteins and ligands and accurately predicting their binding structures is a critical yet challenging task in drug discovery. Recent advancements in deep learning have shown promise in addressing this challenge, with sampling-based and regression-based methods emerging as two prominent approaches. However, these methods have notable limitations. Sampling-based methods often suffer from low efficiency due to the need for generating multiple candidate structures for selection. On the other hand, regression-based methods offer fast predictions but may experience decreased accuracy. Additionally, the variation in protein sizes often requires external modules for selecting suitable binding pockets, further impacting efficiency. In this work, we propose $\mathbf{FABind}$, an end-to-end model that combines pocket prediction and docking to achieve accurate and fast protein-ligand binding. $\mathbf{FABind}$ incorporates a unique ligand-informed pocket prediction
    
[^102]: 函数几何引导的蛋白质序列和骨架结构共同设计

    Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design. (arXiv:2310.04343v1 [cs.LG])

    [http://arxiv.org/abs/2310.04343](http://arxiv.org/abs/2310.04343)

    本研究提出了NAEPro模型，通过函数几何引导的方法共同设计蛋白质序列和结构。实验结果表明，该模型在氨基酸恢复率、TM分数和RMSD等指标上表现出色。

    

    蛋白质是几乎所有生物体中负责基本功能的大分子。设计合理的具有期望功能的蛋白质至关重要。蛋白质的序列和结构密切相关，它们共同决定了其功能。在本文中，我们提出了NAEPro，一种基于自动检测到的功能位点共同设计蛋白质序列和结构的模型。NAEPro采用了注意力和等变层的交错网络，可以捕捉序列中的全局相关性以及三维空间中最近氨基酸的局部影响。这种架构在两个层面上促进了有效而经济的信息传递。我们在两个蛋白质数据集（β-内酰胺酶和肌红蛋白）上评估了我们的模型和几个强竞争基线。实验结果表明，我们的模型在所有竞争对手中始终实现了最高的氨基酸恢复率、TM分数和最低的RMSD。这些发现证明了该模型的能力。

    Proteins are macromolecules responsible for essential functions in almost all living organisms. Designing reasonable proteins with desired functions is crucial. A protein's sequence and structure are strongly correlated and they together determine its function. In this paper, we propose NAEPro, a model to jointly design Protein sequence and structure based on automatically detected functional sites. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from nearest amino acids in three dimensional (3D) space. Such an architecture facilitates effective yet economic message passing at two levels. We evaluate our model and several strong baselines on two protein datasets, $\beta$-lactamase and myoglobin. Experimental results show that our model consistently achieves the highest amino acid recovery rate, TM-score, and the lowest RMSD among all competitors. These findings prove the capab
    
[^103]: 无穷宽度两层ReLU神经网络的同伦松弛训练算法

    Homotopy Relaxation Training Algorithms for Infinite-Width Two-Layer ReLU Neural Networks. (arXiv:2309.15244v1 [cs.LG])

    [http://arxiv.org/abs/2309.15244](http://arxiv.org/abs/2309.15244)

    本文提出了一种名为同伦松弛训练算法（HRTA）的新的训练方法，它通过构建无缝连接线性激活函数和ReLU激活函数的同伦激活函数，并松弛同伦参数以增强训练精细化过程，加速了训练过程，在神经切线核（NTK）的背景下，实现了显著改进的收敛速度，并展示了对其他激活函数和深度神经网络的潜力。

    

    本文提出了一种新的训练方法，称为同伦松弛训练算法（HRTA），旨在加速训练过程，与传统方法相比。我们的算法结合了两个关键机制：一个是构建无缝连接线性激活函数和ReLU激活函数的同伦激活函数；另一个技术是松弛同伦参数以增强训练精细化过程。我们在神经切线核（NTK）的背景下对这种新方法进行了深入分析，揭示了显著改进的收敛速度。我们的实验结果，尤其是在考虑更大宽度的网络时，验证了理论结论。这种提议的HRTA展示了对其他激活函数和深度神经网络的潜力。

    In this paper, we present a novel training approach called the Homotopy Relaxation Training Algorithm (HRTA), aimed at accelerating the training process in contrast to traditional methods. Our algorithm incorporates two key mechanisms: one involves building a homotopy activation function that seamlessly connects the linear activation function with the ReLU activation function; the other technique entails relaxing the homotopy parameter to enhance the training refinement process. We have conducted an in-depth analysis of this novel method within the context of the neural tangent kernel (NTK), revealing significantly improved convergence rates. Our experimental results, especially when considering networks with larger widths, validate the theoretical conclusions. This proposed HRTA exhibits the potential for other activation functions and deep neural networks.
    
[^104]: 使用CodeBERT和Random Forest Regressor进行自动评分的C编程作业

    Auto-grading C programming assignments with CodeBERT and Random Forest Regressor. (arXiv:2309.15216v1 [cs.LG])

    [http://arxiv.org/abs/2309.15216](http://arxiv.org/abs/2309.15216)

    本研究提供了使用机器学习和深度学习方法对C编程作业进行自动评分的分析，并引入了基于代码的转换器词嵌入模型CodeBERT。测试结果表明该策略的有效性，并讨论了统计方法和深度学习技术之间的对比。

    

    手动评分编程作业因复杂性和主观性而具有挑战性。然而，使用深度学习进行自动评分简化了任务。它客观地评估代码质量，检测错误，并准确地分配分数，减轻了教师的负担，同时确保了高效和公平的评估。本研究使用回归、卷积神经网络（CNN）和长短期记忆（LSTM）等机器学习和深度学习方法对C编程作业进行自动评分的分析。使用一种基于代码的转换器词嵌入模型CodeBERT，将文本代码输入转换为向量，然后将向量输入到几个模型中。测试结果证明了建议策略的有效性，均方根误差（RMSE）为1.89。本研究还讨论了统计方法和深度学习技术之间的对比。

    Grading coding assignments manually is challenging due to complexity and subjectivity. However, auto-grading with deep learning simplifies the task. It objectively assesses code quality, detects errors, and assigns marks accurately, reducing the burden on instructors while ensuring efficient and fair assessment. This study provides an analysis of auto-grading of the C programming assignments using machine learning and deep learning approaches like regression, convolutional neural networks (CNN) and long short-term memory (LSTM). Using a code-based transformer word embedding model called CodeBERT, the textual code inputs were transformed into vectors, and the vectors were then fed into several models. The testing findings demonstrated the efficacy of the suggested strategy with a root mean squared error (RMSE) of 1.89. The contrast between statistical methods and deep learning techniques is discussed in the study.
    
[^105]: BiSinger: 双语合成歌声系统

    BiSinger: Bilingual Singing Voice Synthesis. (arXiv:2309.14089v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2309.14089](http://arxiv.org/abs/2309.14089)

    BiSinger是一个双语合成歌声系统，通过设计共享表示、融合数据集和使用开源技术，实现了一种可以进行英语和汉语普通话混合编码歌声合成的单一模型，并保持了汉语歌曲的表现。

    

    尽管文本到语音合成技术在歌声合成方面取得了很大的进展，但多语种合成歌声模型仍然相对较少探索。本文介绍了BiSinger，这是一个用于英语和汉语普通话的双语流行歌声合成系统。当前的系统需要分别针对每种语言进行建模，并且无法准确地表示汉语和英语，从而阻碍了混合编码歌声合成。为了解决这个问题，我们设计了一种汉语和英语歌声之间的共享表示，通过使用CMU字典和映射规则实现。我们使用开源歌声转换技术融合了单语歌声数据集，以生成双语歌声，同时还探索了双语言音数据的潜在用途。实验验证了我们的语言无关表示和相关数据集的整合使得单一模型在英语和混合编码歌声合成方面性能更好，同时保持了汉语歌曲的表现。音频样本可在某网址找到。

    Although Singing Voice Synthesis (SVS) has made great strides with Text-to-Speech (TTS) techniques, multilingual singing voice modeling remains relatively unexplored. This paper presents BiSinger, a bilingual pop SVS system for English and Chinese Mandarin. Current systems require separate models per language and cannot accurately represent both Chinese and English, hindering code-switch SVS. To address this gap, we design a shared representation between Chinese and English singing voices, achieved by using the CMU dictionary with mapping rules. We fuse monolingual singing datasets with open-source singing voice conversion techniques to generate bilingual singing voices while also exploring the potential use of bilingual speech data. Experiments affirm that our language-independent representation and incorporation of related datasets enable a single model with enhanced performance in English and code-switch SVS while maintaining Chinese song performance. Audio samples are available at 
    
[^106]: 通过反演影响函数理解深度梯度泄露

    Understanding Deep Gradient Leakage via Inversion Influence Functions. (arXiv:2309.13016v1 [cs.LG])

    [http://arxiv.org/abs/2309.13016](http://arxiv.org/abs/2309.13016)

    本文提出了一种新的方法I²F，可以有效近似深度梯度泄露攻击，并建立了恢复图像和私有梯度之间的连接。通过这个方法，我们能够更好地理解和应对深度梯度泄露攻击。

    

    深度梯度泄露（DGL）是一种非常有效的攻击方法，可以从梯度向量中恢复私有训练图像。这种攻击对于具有敏感数据的客户端分布式学习提出了重要的隐私挑战，其中客户端需要共享梯度。防御此类攻击需要但缺乏对隐私泄露发生的时间和方式的理解，主要是因为深度网络的黑盒特性。在本文中，我们提出了一种新颖的反演影响函数（I²F），通过隐式解决DGL问题，建立了恢复图像和私有梯度之间的闭式连接。与直接解决DGL相比，I²F在分析深度网络时具有可扩展性，仅需要梯度和雅可比向量乘积的预言访问。我们通过实验证明，I²F在不同的模型架构、数据集、攻击实现和基于噪声的防御中都能有效近似DGL。我们通过这种新颖的工具，能够更好地了解深度梯度泄露的机理和应对方法。

    Deep Gradient Leakage (DGL) is a highly effective attack that recovers private training images from gradient vectors. This attack casts significant privacy challenges on distributed learning from clients with sensitive data, where clients are required to share gradients. Defending against such attacks requires but lacks an understanding of when and how privacy leakage happens, mostly because of the black-box nature of deep networks. In this paper, we propose a novel Inversion Influence Function (I$^2$F) that establishes a closed-form connection between the recovered images and the private gradients by implicitly solving the DGL problem. Compared to directly solving DGL, I$^2$F is scalable for analyzing deep networks, requiring only oracle access to gradients and Jacobian-vector products. We empirically demonstrate that I$^2$F effectively approximated the DGL generally on different model architectures, datasets, attack implementations, and noise-based defenses. With this novel tool, we 
    
[^107]: FedDCSR: 通过解缠表示学习实现联邦跨领域顺序推荐

    FedDCSR: Federated Cross-domain Sequential Recommendation via Disentangled Representation Learning. (arXiv:2309.08420v1 [cs.LG])

    [http://arxiv.org/abs/2309.08420](http://arxiv.org/abs/2309.08420)

    提出了一种名为FedDCSR的联邦跨领域顺序推荐框架，通过解缠表示学习来处理不同领域之间的序列特征异质性，并保护数据隐私。

    

    近年来，利用来自多个领域的用户序列数据的跨领域顺序推荐(CSR)受到了广泛关注。然而，现有的CSR方法需要在领域之间共享原始用户数据，这违反了《通用数据保护条例》(GDPR)。因此，有必要将联邦学习(FL)和CSR相结合，充分利用不同领域的知识，同时保护数据隐私。然而，不同领域之间的序列特征异质性对FL的整体性能有显著影响。在本文中，我们提出了FedDCSR，这是一种通过解缠表示学习的新型联邦跨领域顺序推荐框架。具体而言，为了解决不同领域之间的序列特征异质性，我们引入了一种称为领域内-领域间序列表示解缠(SRD)的方法，将用户序列特征解缠成领域共享和领域专属特征。

    Cross-domain Sequential Recommendation (CSR) which leverages user sequence data from multiple domains has received extensive attention in recent years. However, the existing CSR methods require sharing origin user data across domains, which violates the General Data Protection Regulation (GDPR). Thus, it is necessary to combine federated learning (FL) and CSR to fully utilize knowledge from different domains while preserving data privacy. Nonetheless, the sequence feature heterogeneity across different domains significantly impacts the overall performance of FL. In this paper, we propose FedDCSR, a novel federated cross-domain sequential recommendation framework via disentangled representation learning. Specifically, to address the sequence feature heterogeneity across domains, we introduce an approach called inter-intra domain sequence representation disentanglement (SRD) to disentangle the user sequence features into domain-shared and domain-exclusive features. In addition, we design
    
[^108]: 基于地理气象数据的深度神经网络用于长期干旱预测

    Long-term drought prediction using deep neural networks based on geospatial weather data. (arXiv:2309.06212v1 [cs.LG])

    [http://arxiv.org/abs/2309.06212](http://arxiv.org/abs/2309.06212)

    基于地理气象数据的深度神经网络用于长期干旱预测，提出了一种端到端解决方案来预测特定地区干旱概率。采用卷积LSTM和Transformer模型相比其他基线模型能够获得更高的准确性。

    

    在农业实践中，准确预测特定地区干旱概率对于决策具有重要性。尤其对于长期决策，提前一年进行预测至关重要。然而，由于感兴趣区域及其相邻区域内各种因素的复杂相互作用，预测这一概率存在挑战。在本研究中，我们提出了一种基于各种时空神经网络的端到端解决方案来解决这个问题。所考虑的模型主要是根据Palmer干旱严重指数（PDSI）预测感兴趣亚区的干旱强度，利用气候模型的内在因素和见解来提高干旱预测的准确性。比较评估结果表明，与基准梯度提升和逻辑回归解决方案相比，卷积LSTM（ConvLSTM）和Transformer模型的准确性更高。前两种模型取得了令人印象深刻的ROC AUC分数，高达0.90

    The accurate prediction of drought probability in specific regions is crucial for informed decision-making in agricultural practices. It is important to make predictions one year in advance, particularly for long-term decisions. However, forecasting this probability presents challenges due to the complex interplay of various factors within the region of interest and neighboring areas. In this study, we propose an end-to-end solution to address this issue based on various spatiotemporal neural networks. The models considered focus on predicting the drought intensity based on the Palmer Drought Severity Index (PDSI) for subregions of interest, leveraging intrinsic factors and insights from climate models to enhance drought predictions.  Comparative evaluations demonstrate the superior accuracy of Convolutional LSTM (ConvLSTM) and transformer models compared to baseline gradient boosting and logistic regression solutions. The two former models achieved impressive ROC AUC scores from 0.90 
    
[^109]: s-ID：在子群体中的因果效应识别

    s-ID: Causal Effect Identification in a Sub-Population. (arXiv:2309.02281v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.02281](http://arxiv.org/abs/2309.02281)

    该论文介绍了在子群体中进行因果效应识别的问题，提出并倡导了s-ID问题。论文提供了必要和充分条件，以便从子群体的观测分布中识别出因果效应。

    

    在子群体中的因果推断涉及到识别干预对特定子组的因果效应，这些子组通过抽样过程中的系统偏差与整个群体有所区别。然而，忽略子群体引入的细微差别可能导致错误的推断，或者限制现有方法的适用性。我们引入并倡导子群体中的因果推断问题（以下简称s-ID），其中我们仅可以访问目标子群体的观测数据（而不是整个群体）。现有的子群体推断问题是基于给定的数据分布源于整个群体的前提，因此无法解决s-ID问题。为了填补这一差距，我们提供了在因果图中必须满足的必要和充分条件，以便从该子群体的观测分布中识别出子群体中的因果效应。

    Causal inference in a sub-population involves identifying the causal effect of an intervention on a specific subgroup, which is distinguished from the whole population through the influence of systematic biases in the sampling process. However, ignoring the subtleties introduced by sub-populations can either lead to erroneous inference or limit the applicability of existing methods. We introduce and advocate for a causal inference problem in sub-populations (henceforth called s-ID), in which we merely have access to observational data of the targeted sub-population (as opposed to the entire population). Existing inference problems in sub-populations operate on the premise that the given data distributions originate from the entire population, thus, cannot tackle the s-ID problem. To address this gap, we provide necessary and sufficient conditions that must hold in the causal graph for a causal effect in a sub-population to be identifiable from the observational distribution of that sub
    
[^110]: 分布式资源管理中的价格差异化游戏对联合学习的影响

    Price-Discrimination Game for Distributed Resource Management in Federated Learning. (arXiv:2308.13838v1 [cs.LG])

    [http://arxiv.org/abs/2308.13838](http://arxiv.org/abs/2308.13838)

    本论文提出了一种价格差异化游戏（PDG），通过对不同客户提供的服务进行定价差异化，改善了联合学习的性能并降低了激励客户参与联合学习的成本。

    

    在传统的联合学习中，参数服务器和多个分布式客户端可以形成典型的买方市场，其中PS/买家数量远远少于客户端/卖家数量。为了改善联合学习的性能并减少激励客户参与联合学习的成本，本文提出了对不同客户提供的服务进行定价差异化，而不是简单地为不同客户提供相同的服务定价。价格差异化基于对联合学习带来的性能改进和计算通信能力的异质性。为此，本文提出了一个价格差异化游戏（PDG），全面解决了联合学习中的分布式资源管理问题，包括多目标权衡、客户端选择和激励机制。由于PDG是一个混合整数非线性规划（MINLP）问题，本文提出了一个具有低计算成本的分布式半启发式算法来解决该问题。

    In vanilla federated learning (FL) such as FedAvg, the parameter server (PS) and multiple distributed clients can form a typical buyer's market, where the number of PS/buyers of FL services is far less than the number of clients/sellers. In order to improve the performance of FL and reduce the cost of motivating clients to participate in FL, this paper proposes to differentiate the pricing for services provided by different clients rather than simply providing the same service pricing for different clients. The price is differentiated based on the performance improvements brought to FL and their heterogeneity in computing and communication capabilities. To this end, a price-discrimination game (PDG) is formulated to comprehensively address the distributed resource management problems in FL, including multi-objective trade-off, client selection, and incentive mechanism. As the PDG is a mixed-integer nonlinear programming (MINLP) problem, a distributed semi-heuristic algorithm with low c
    
[^111]: 图上k中心问题的动态算法

    Dynamic algorithms for k-center on graphs. (arXiv:2307.15557v1 [cs.DS])

    [http://arxiv.org/abs/2307.15557](http://arxiv.org/abs/2307.15557)

    本文提出了针对动态图中的k中心问题的高效算法，包括确定性递减的（2+ε）近似算法和随机增量的（4+ε）近似算法，同时给出了针对加权图的摊销更新时间为kn^{o(1)}的算法。此外，通过简化方法得到了对于k中心问题的全动态（2+ε）近似算法。

    

    本文针对动态图中的k中心问题提出了第一种高效算法。在这个问题中，目标是通过选择k个中心将输入分为k个集合，使得任意数据点到最近中心的最大距离最小化。已知在该问题中，要获得优于2的近似解是NP难的。尽管在许多应用中，输入可以自然地建模为图，但在动态环境下的k中心问题的先前研究都是基于度量空间的。本文给出了一种确定性递减的（2+ε）近似算法和一种随机增量的（4+ε）近似算法，对于加权图，两种算法的摊销更新时间为kn^{o(1)}。此外，我们还展示了一种约简方法，从而得到了对于k中心问题的全动态（2+ε）近似算法，其最坏情况更新时间与维护（1+k近似解的最新上界相差不超过k。

    In this paper we give the first efficient algorithms for the $k$-center problem on dynamic graphs undergoing edge updates. In this problem, the goal is to partition the input into $k$ sets by choosing $k$ centers such that the maximum distance from any data point to the closest center is minimized. It is known that it is NP-hard to get a better than $2$ approximation for this problem.  While in many applications the input may naturally be modeled as a graph, all prior works on $k$-center problem in dynamic settings are on metrics. In this paper, we give a deterministic decremental $(2+\epsilon)$-approximation algorithm and a randomized incremental $(4+\epsilon)$-approximation algorithm, both with amortized update time $kn^{o(1)}$ for weighted graphs. Moreover, we show a reduction that leads to a fully dynamic $(2+\epsilon)$-approximation algorithm for the $k$-center problem, with worst-case update time that is within a factor $k$ of the state-of-the-art upper bound for maintaining $(1+
    
[^112]: 注意力对熵通信的影响

    Attention to Entropic Communication. (arXiv:2307.11423v1 [cs.IT])

    [http://arxiv.org/abs/2307.11423](http://arxiv.org/abs/2307.11423)

    该论文研究了在通信理论中结合注意力和相对熵的概念。研究发现，在通信中使用注意力导向的加权相对熵是不适当的，而适当的注意力通信可通过发送者仅需要了解接收者的效用函数来实现最佳通知。

    

    注意力的概念是指在人工智能中强调特定数据重要性的数值权重，在通信理论中相对熵（RE，也称为库尔巴克-勒布勒散度）发挥着核心作用。在这里，我们结合了这些概念，即注意力和RE。RE引导带宽有限通信中的最佳编码以及通过最大熵原理（MEP）进行最佳消息解码。在编码场景中，RE可以从四个要求中推导出来，即分析性、局部性、适当性和校准性。而用于通信中注意力导向的加权RE实际上是不适当的。为了看到适当的注意力通信是如何出现的，我们分析了一个场景，即消息发送者希望确保接收者能够执行知情的操作。如果接收者使用MEP解码消息，则发送者只需要知道接收者的效用函数来进行最佳通知，不需要知道接收者的策略。

    The concept of attention, numerical weights that emphasize the importance of particular data, has proven to be very relevant in artificial intelligence. Relative entropy (RE, aka Kullback-Leibler divergence) plays a central role in communication theory. Here we combine these concepts, attention and RE. RE guides optimal encoding of messages in bandwidth-limited communication as well as optimal message decoding via the maximum entropy principle (MEP). In the coding scenario, RE can be derived from four requirements, namely being analytical, local, proper, and calibrated. Weighted RE, used for attention steering in communications, turns out to be improper. To see how proper attention communication can emerge, we analyze a scenario of a message sender who wants to ensure that the receiver of the message can perform well-informed actions. If the receiver decodes the message using the MEP, the sender only needs to know the receiver's utility function to inform optimally, but not the receive
    
[^113]: 光子器件设计的强化学习

    Reinforcement Learning for Photonic Component Design. (arXiv:2307.11075v2 [physics.optics] UPDATED)

    [http://arxiv.org/abs/2307.11075](http://arxiv.org/abs/2307.11075)

    本文提出了一种新的基于强化学习的fab-in-the-loop算法，用于光子器件设计，并成功将插入损耗降低至3.24 dB，并实现了在150纳米带宽下不到10.2 dB的损耗。

    

    我们提出了一种新的基于强化学习的fab-in-the-loop算法，用于设计考虑到纳米制造过程中的缺陷的纳米光子器件。作为这种技术潜力的展示，我们将其应用于在气体包覆的220纳米硅层绝缘体单刻蚀平台上制造的光子晶体光栅耦合器的设计。这种fab-in-the-loop算法将插入损耗从8.8 dB降低到3.24 dB。使用我们的fab-in-the-loop算法，产生的最宽带宽设计可以在其最低点处以不到10.2 dB的损耗覆盖150纳米的带宽。

    We present a new fab-in-the-loop reinforcement learning algorithm for the design of nano-photonic components that accounts for the imperfections present in nanofabrication processes. As a demonstration of the potential of this technique, we apply it to the design of photonic crystal grating couplers fabricated on an air clad 220 nm silicon on insulator single etch platform. This fab-in-the-loop algorithm improves the insertion loss from 8.8 to 3.24 dB. The widest bandwidth designs produced using our fab-in-the-loop algorithm can cover a 150 nm bandwidth with less than 10.2 dB of loss at their lowest point.
    
[^114]: 在线 Laplace 模型选择的再探讨

    Online Laplace Model Selection Revisited. (arXiv:2307.06093v1 [cs.LG])

    [http://arxiv.org/abs/2307.06093](http://arxiv.org/abs/2307.06093)

    本研究重新推导了在线 Laplace 方法，并将其目标定位为模态修正的变分上界，避免了对平稳性的假设。通过使用全批量梯度的在线算法，我们演示了在实践中实现了这些最优点，并验证了其适用性。

    

    Laplace 近似为神经网络提供了一个封闭形式的模型选择目标。在贝叶斯深度学习领域，将神经网络参数与超参数（如权重衰减强度）一起进行优化的在线变体方法再次引起了人们的关注。然而，这些方法违反了 Laplace 方法的一个关键假设，即近似是围绕损失的模态进行的，这就对它们的合理性提出了质疑。本研究重新推导了在线 Laplace 方法，展示了它们针对 Laplace 证据的一个修正模态的变分上界，从而避免了对平稳性的假设。在线 Laplace 方法及其修正模态的对应点满足两个条件：1. 神经网络参数是一个最大后验概率，满足 Laplace 方法的假设；2. 超参数最大化 Laplace 证据，从而促使在线方法的应用。我们通过使用全批量梯度的在线算法演示了这些最优点在实践中的近似程度。

    The Laplace approximation provides a closed-form model selection objective for neural networks (NN). Online variants, which optimise NN parameters jointly with hyperparameters, like weight decay strength, have seen renewed interest in the Bayesian deep learning community. However, these methods violate Laplace's method's critical assumption that the approximation is performed around a mode of the loss, calling into question their soundness. This work re-derives online Laplace methods, showing them to target a variational bound on a mode-corrected variant of the Laplace evidence which does not make stationarity assumptions. Online Laplace and its mode-corrected counterpart share stationary points where 1. the NN parameters are a maximum a posteriori, satisfying the Laplace method's assumption, and 2. the hyperparameters maximise the Laplace evidence, motivating online methods. We demonstrate that these optima are roughly attained in practise by online algorithms using full-batch gradien
    
[^115]: 用于Helmholtz方程的多网格增强深度学习方法：通过紧致隐式层提高可伸缩性

    Multigrid-Augmented Deep Learning for the Helmholtz Equation: Better Scalability with Compact Implicit Layers. (arXiv:2306.17486v1 [cs.LG])

    [http://arxiv.org/abs/2306.17486](http://arxiv.org/abs/2306.17486)

    通过结合多网格求解器和卷积神经网络，该论文提出了一种用于解决离散异质Helmholtz方程的迭代深度学习方法，在可伸缩性和求解速度上优于传统方法。其中的三个主要创新包括引入隐式层来解决CNN中的视野问题、改进CNN预条件技术以提高性能，并提出了一种多尺度训练方法使网络能够处理不同尺寸的问题。

    

    我们提出了一种基于深度学习的迭代方法来解决离散异质Helmholtz方程在高波数下的问题。通过将经典的迭代多网格求解器和卷积神经网络（CNN）与预条件技术结合起来，我们得到了一个更快且可伸缩性更好的学习型神经求解器，相比标准的多网格求解器更优。我们的方法在先前这类神经方法的基础上提出了三个主要贡献。首先，我们构建了一个多层U-Net-like编码器-求解器CNN，其中在U-Net的最粗糙网格上包含一个隐式层，卷积核被反转。这种方法缓解了CNN中的视野问题，并允许更好的可伸缩性。其次，我们在参数数量、计算时间和收敛速度方面改进了先前的CNN预条件器。第三，我们提出了一种多尺度训练方法，使网络能够扩展到之前未见过的尺寸问题，同时仍保持合理的训练过程。我们的编码器

    We present a deep learning-based iterative approach to solve the discrete heterogeneous Helmholtz equation for high wavenumbers. Combining classical iterative multigrid solvers and convolutional neural networks (CNNs) via preconditioning, we obtain a learned neural solver that is faster and scales better than a standard multigrid solver. Our approach offers three main contributions over previous neural methods of this kind. First, we construct a multilevel U-Net-like encoder-solver CNN with an implicit layer on the coarsest grid of the U-Net, where convolution kernels are inverted. This alleviates the field of view problem in CNNs and allows better scalability. Second, we improve upon the previous CNN preconditioner in terms of the number of parameters, computation time, and convergence rates. Third, we propose a multiscale training approach that enables the network to scale to problems of previously unseen dimensions while still maintaining a reasonable training procedure. Our encoder
    
[^116]: 基于扩散的随机再生模型的风噪声降噪方法

    Wind Noise Reduction with a Diffusion-based Stochastic Regeneration Model. (arXiv:2306.12867v1 [eess.AS])

    [http://arxiv.org/abs/2306.12867](http://arxiv.org/abs/2306.12867)

    本文介绍了一种基于扩散的随机再生模型的风噪声降噪方法，该方法使得风噪声降噪效果优于其他基于神经网络的方法和纯预测和生成模型，在使用模拟和真实记录的风噪声数据集上进行了测试，并在真实记录的风噪声数据集上具有很好的泛化性能。

    

    本文介绍了一种单通道风噪音降低方法，使用了我们先前提出的结合预测和生成建模的基于扩散的随机再生模型。我们引入了一个非加性的语音噪声模型来解释膜的非线性变形，这种变形是由风流和可能的剪裁引起的。我们证明了我们的随机再生模型在使用模拟和真实记录的风噪声数据集上表现优于其他基于神经网络的风噪声降低方法以及纯预测和生成模型。我们进一步展示了所提出的方法在未见过的真实记录的风噪声数据集上具有很好的泛化性能。所提出方法的音频样本，数据生成脚本和代码可以在线上找到(https://uhh.de/inf-sp-storm-wind)。

    In this paper we present a method for single-channel wind noise reduction using our previously proposed diffusion-based stochastic regeneration model combining predictive and generative modelling. We introduce a non-additive speech in noise model to account for the non-linear deformation of the membrane caused by the wind flow and possible clipping. We show that our stochastic regeneration model outperforms other neural-network-based wind noise reduction methods as well as purely predictive and generative models, on a dataset using simulated and real-recorded wind noise. We further show that the proposed method generalizes well by testing on an unseen dataset with real-recorded wind noise. Audio samples, data generation scripts and code for the proposed methods can be found online (https://uhh.de/inf-sp-storm-wind).
    
[^117]: 通过分离学习解决混淆节点问题

    Clarify Confused Nodes Through Separated Learning. (arXiv:2306.02285v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02285](http://arxiv.org/abs/2306.02285)

    本文提出了使用邻域混淆度量来分离学习解决图神经网络中混淆节点的问题。这种方法可以更可靠地区分异质节点和同质节点，并改善性能。

    

    图神经网络（GNN）在图导向任务中取得了显著的进展。然而，现实世界的图中不可避免地包含一定比例的异质节点，这挑战了经典GNN的同质性假设，并阻碍了其性能。现有研究大多数仍设计了具有异质节点和同质节点间共享权重的通用模型。尽管这些努力中包含了高阶信息和多通道架构，但往往效果不佳。少数研究尝试训练不同节点组的分离学习，但受到了不合适的分离度量和低效率的影响。本文首先提出了一种新的度量指标，称为邻域混淆（NC），以便更可靠地分离节点。我们观察到具有不同NC值的节点组在组内准确度和可视化嵌入上存在一定差异。这为基于邻域混淆的图卷积网络（NC-GCN）铺平了道路。

    Graph neural networks (GNNs) have achieved remarkable advances in graph-oriented tasks. However, real-world graphs invariably contain a certain proportion of heterophilous nodes, challenging the homophily assumption of classical GNNs and hindering their performance. Most existing studies continue to design generic models with shared weights between heterophilous and homophilous nodes. Despite the incorporation of high-order messages or multi-channel architectures, these efforts often fall short. A minority of studies attempt to train different node groups separately but suffer from inappropriate separation metrics and low efficiency. In this paper, we first propose a new metric, termed Neighborhood Confusion (NC), to facilitate a more reliable separation of nodes. We observe that node groups with different levels of NC values exhibit certain differences in intra-group accuracy and visualized embeddings. These pave the way for Neighborhood Confusion-guided Graph Convolutional Network (N
    
[^118]: Union Subgraph神经网络

    Union Subgraph Neural Networks. (arXiv:2305.15747v1 [cs.LG])

    [http://arxiv.org/abs/2305.15747](http://arxiv.org/abs/2305.15747)

    本文提出了一种新型图神经网络UnionSNN，注入了邻居连接信息，通过联合子图来编码高阶连接性，实验证明其在节点分类任务中优于1-WL和当前最先进的GNN模型。

    

    图神经网络(GNNs)被广泛用于许多应用领域的图表示学习。由于它们通过迭代传递消息来处理有根子树，因此普通的GNNs的表达能力上限为1维Weisfeiler-Leman(1-WL)测试。在本文中，我们通过注入从新类型的子结构中提取的邻居连接信息来增强GNNs。我们首先研究了局部邻域中存在的不同连接性，并确定了一个称为联合子图的子结构，它能够捕捉到一条边的1-跳邻居的完整图像。然后，我们设计了一种基于最短路径的子结构描述符，具有三个良好的性质，并且可以有效地编码联合子图中的高阶连接性。通过注入编码邻居连接性，我们提出了一种新的模型，即Union Subgraph神经网络(UnionSNN)，它被证明在区分非同构图方面比1-WL严格更强。在基准数据集上的实验表明，UnionSNN始终优于最先进的GNN模型，并在多个节点分类任务上取得了显著的改进。

    Graph Neural Networks (GNNs) are widely used for graph representation learning in many application domains. The expressiveness of vanilla GNNs is upper-bounded by 1-dimensional Weisfeiler-Leman (1-WL) test as they operate on rooted subtrees through iterative message passing. In this paper, we empower GNNs by injecting neighbor-connectivity information extracted from a new type of substructure. We first investigate different kinds of connectivities existing in a local neighborhood and identify a substructure called union subgraph, which is able to capture the complete picture of the 1-hop neighborhood of an edge. We then design a shortest-path-based substructure descriptor that possesses three nice properties and can effectively encode the high-order connectivities in union subgraphs. By infusing the encoded neighbor connectivities, we propose a novel model, namely Union Subgraph Neural Network (UnionSNN), which is proven to be strictly more powerful than 1-WL in distinguishing non-isom
    
[^119]: 论文标题：使ViT成形：计算-优化模型设计的缩放定律。

    Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design. (arXiv:2305.13035v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.13035](http://arxiv.org/abs/2305.13035)

    本研究通过改进缩放定律方法推测出计算-优化模型形状，成功实现了形状优化视觉变换器SoViT，该模型在相同计算量下，取得了与超过其两倍大小的模型相竞争的结果。

    

    近期，缩放定律被用来推导在给定计算时间范围内的计算-优化模型大小（参数数量）。我们发展并改进了这些方法，以推测如宽度和深度等计算-优化模型形状，并在视觉变换器中成功实现了这一点。我们经过形状优化的视觉变换器SoViT，在仅使用相同数量的计算量进行预训练的情况下，取得了与超过其两倍大小的模型相竞争的结果。例如，SoViT-400m/14在ILSRCV2012上取得了90.3%的微调准确度，超过了更大的ViT-g/14，在相同设置下接近ViT-G/14，同时推断成本也不到一半。我们进行了多个任务的彻底评估，例如图像分类、字幕、VQA和零-shot转移，在广泛领域中展示了我们模型的有效性并确定了其限制。总体而言，我们的研究发现挑战了盲目扩大视觉模型的现有方法。

    Scaling laws have been recently employed to derive compute-optimal model size (number of parameters) for a given compute duration. We advance and refine such methods to infer compute-optimal model shapes, such as width and depth, and successfully implement this in vision transformers. Our shape-optimized vision transformer, SoViT, achieves results competitive with models that exceed twice its size, despite being pre-trained with an equivalent amount of compute. For example, SoViT-400m/14 achieves 90.3% fine-tuning accuracy on ILSRCV2012, surpassing the much larger ViT-g/14 and approaching ViT-G/14 under identical settings, with also less than half the inference cost. We conduct a thorough evaluation across multiple tasks, such as image classification, captioning, VQA and zero-shot transfer, demonstrating the effectiveness of our model across a broad range of domains and identifying limitations. Overall, our findings challenge the prevailing approach of blindly scaling up vision models 
    
[^120]: 变分分类

    Variational Classification. (arXiv:2305.10406v1 [cs.LG])

    [http://arxiv.org/abs/2305.10406](http://arxiv.org/abs/2305.10406)

    提出一种新的变分分类方法，通过引入潜变量建模来优化训练，允许灵活的设计选择以改善校准和对抗鲁棒性，实验结果表明其对于域外数据的分类准确性得到了保持。

    

    我们提出了一种传统神经网络方法的新型扩展，称为变分分类 (VC)。通过引入潜变量建模，类似于变分自编码器和传统自编码器之间的关系，我们得到了一个基于证据下界 (ELBO) 的训练目标，采用对抗性方法优化。我们的VC模型允许在设计选择方面更加灵活，特别是类条件潜先验，而不是在现成的softmax分类器中做出的隐式假设。在图像和文本分类数据集上的实证评估表明，我们的方法在保持预测准确性的同时，改善了其他良好特性，如校准和对抗鲁棒性，即使应用于域外数据。

    We present a novel extension of the traditional neural network approach to classification tasks, referred to as variational classification (VC). By incorporating latent variable modeling, akin to the relationship between variational autoencoders and traditional autoencoders, we derive a training objective based on the evidence lower bound (ELBO), optimized using an adversarial approach. Our VC model allows for more flexibility in design choices, in particular class-conditional latent priors, in place of the implicit assumptions made in off-the-shelf softmax classifiers. Empirical evaluation on image and text classification datasets demonstrates the effectiveness of our approach in terms of maintaining prediction accuracy while improving other desirable properties such as calibration and adversarial robustness, even when applied to out-of-domain data.
    
[^121]: FedNC：基于网络编码启发的安全高效联合学习方法

    FedNC: A Secure and Efficient Federated Learning Method Inspired by Network Coding. (arXiv:2305.03292v1 [cs.LG])

    [http://arxiv.org/abs/2305.03292](http://arxiv.org/abs/2305.03292)

    本文提出FedNC，一个联合学习的通信框架，结合了网络编码技术，能够提高系统的隐私、吞吐量和鲁棒性。

    

    联合学习是一种有前途的分布式学习机制，但仍然面临两个主要挑战，即隐私泄漏和系统效率。在本文中，我们从网络信息理论的角度重新构思了联合学习系统，并制定了一个原创的联合学习通信框架FedNC，该框架受到网络编码的启发。 FedNC的主要思想是通过对原始数据包进行随机线性组合，将本地模型的信息混合在一起，然后再上传进行进一步的聚合，从而使FL系统更加安全，吞吐量更高，鲁棒性更好。据我们所知，这是第一个将NC引入FL的框架。随着FL在实际网络框架中的不断演变，可以基于FedNC进一步设计更多的应用和变体。

    Federated Learning (FL) is a promising distributed learning mechanism which still faces two major challenges, namely privacy breaches and system efficiency. In this work, we reconceptualize the FL system from the perspective of network information theory, and formulate an original FL communication framework, FedNC, which is inspired by Network Coding (NC). The main idea of FedNC is mixing the information of the local models by making random linear combinations of the original packets, before uploading for further aggregation. Due to the benefits of the coding scheme, both theoretical and experimental analysis indicate that FedNC improves the performance of traditional FL in several important ways, including security, throughput, and robustness. To the best of our knowledge, this is the first framework where NC is introduced in FL. As FL continues to evolve within practical network frameworks, more applications and variants can be further designed based on FedNC.
    
[^122]: 多源到多目标的分布式联邦领域自适应

    Multi-Source to Multi-Target Decentralized Federated Domain Adaptation. (arXiv:2304.12422v1 [cs.DC])

    [http://arxiv.org/abs/2304.12422](http://arxiv.org/abs/2304.12422)

    本文提出了一种分布式联邦学习方法，可将机器学习模型从标记数据丰富的设备转移到未标记数据设备以提高数据利用率。该方法考虑了设备分类和源-目标链接形成的权衡。

    

    联邦学习中设备间的异质性通常指统计（例如，非独立同分布的数据分布）和资源（例如，通信带宽）维度。本文聚焦另一个重要维度：各设备所拥有的标记和未标记数据数量/分布。为了利用所有数据，我们开发了一种分布式联邦领域适应方法，将机器学习模型从标记数据高质量设备（称为源）转移到低质量或未标记数据设备（称为目标）。我们的方法，“源-目标确定和链接形成”（ST-LF），在考虑模型精度和通信能量效率之间的权衡的同时，优化设备分类和源-目标链接形成。

    Heterogeneity across devices in federated learning (FL) typically refers to statistical (e.g., non-i.i.d. data distributions) and resource (e.g., communication bandwidth) dimensions. In this paper, we focus on another important dimension that has received less attention: varying quantities/distributions of labeled and unlabeled data across devices. In order to leverage all data, we develop a decentralized federated domain adaptation methodology which considers the transfer of ML models from devices with high quality labeled data (called sources) to devices with low quality or unlabeled data (called targets). Our methodology, Source-Target Determination and Link Formation (ST-LF), optimizes both (i) classification of devices into sources and targets and (ii) source-target link formation, in a manner that considers the trade-off between ML model accuracy and communication energy efficiency. To obtain a concrete objective function, we derive a measurable generalization error bound that ac
    
[^123]: 可控的信任权衡下的合成数据审计与生成

    Auditing and Generating Synthetic Data with Controllable Trust Trade-offs. (arXiv:2304.10819v1 [cs.LG])

    [http://arxiv.org/abs/2304.10819](http://arxiv.org/abs/2304.10819)

    本论文提出了一个审计框架，能够以全面的方式评估合成数据和AI模型的具体效果，包括偏见和歧视预防、对真实数据的忠实程度、效用、鲁棒性和隐私保护。在多个用例中，审计框架平衡了信任和效用之间的权衡。

    

    现实中收集的数据往往存在偏差、不平衡，并且有泄露敏感和隐私信息的风险。这一事实引发了创建合成数据集的想法，以减轻真实数据中固有的风险、偏见、伤害和隐私问题。这个概念依赖于生成AI模型，以产生不偏执、保护隐私的合成数据，同时忠实于真实数据。在这种新范式中，我们如何知道这种方法是否兑现了其承诺？我们提出了一个审计框架，提供了对合成数据集和基于它们训练的AI模型的全面评估，围绕偏见和歧视的预防、对真实数据的忠实程度、效用、鲁棒性和隐私保护。我们通过审计多个生成模型在不同用例中展示了我们的框架，包括教育、医疗保健、银行、人力资源，以及从表格，时间序列到自然语言的不同模态。我们的用例展示了在合成数据生成中平衡信任和效用的权衡的重要性。

    Data collected from the real world tends to be biased, unbalanced, and at risk of exposing sensitive and private information. This reality has given rise to the idea of creating synthetic datasets to alleviate risk, bias, harm, and privacy concerns inherent in the real data. This concept relies on Generative AI models to produce unbiased, privacy-preserving synthetic data while being true to the real data. In this new paradigm, how can we tell if this approach delivers on its promises? We present an auditing framework that offers a holistic assessment of synthetic datasets and AI models trained on them, centered around bias and discrimination prevention, fidelity to the real data, utility, robustness, and privacy preservation. We showcase our framework by auditing multiple generative models on diverse use cases, including education, healthcare, banking, human resources, and across different modalities, from tabular, to time-series, to natural language. Our use cases demonstrate the imp
    
[^124]: Shallow ReLU$^k$神经网络的逼近速率及其在非参数回归中的应用

    Optimal rates of approximation by shallow ReLU$^k$ neural networks and applications to nonparametric regression. (arXiv:2304.01561v1 [stat.ML])

    [http://arxiv.org/abs/2304.01561](http://arxiv.org/abs/2304.01561)

    本文研究了Shallow ReLU$^k$神经网络的逼近容量，证明了其最优逼近速率。应用到非参数回归上，证明了浅层神经网络可以实现学习H\"older函数的最优渐进速率，补充了深层神经网络的最近结果。

    

    本研究探讨与Shallow ReLU$^k$神经网络相关的变异空间的逼近容量。结果表明，在有限变异范数下，容纳了足够平滑的函数。对于较少平滑的函数，根据变异范数建立了逼近速率。运用这些结果，我们可以证明Shallow ReLU$^k$神经网络的最优逼近速率。同时阐明了这些结果如何用于推导深层神经网络和卷积神经网络(CNNs)的逼近界限。为应用研究，我们使用了三种ReLU神经网络模型：浅层神经网络，超参数神经网络和CNN进行非参数回归收敛速率研究。特别地，我们展示了浅层神经网络可以实现学习H\"older函数的最优渐进速率，这补充了深层神经网络的最近结果。

    We study the approximation capacity of some variation spaces corresponding to shallow ReLU$^k$ neural networks. It is shown that sufficiently smooth functions are contained in these spaces with finite variation norms. For functions with less smoothness, the approximation rates in terms of the variation norm are established. Using these results, we are able to prove the optimal approximation rates in terms of the number of neurons for shallow ReLU$^k$ neural networks. It is also shown how these results can be used to derive approximation bounds for deep neural networks and convolutional neural networks (CNNs). As applications, we study convergence rates for nonparametric regression using three ReLU neural network models: shallow neural network, over-parameterized neural network, and CNN. In particular, we show that shallow neural networks can achieve the minimax optimal rates for learning H\"older functions, which complements recent results for deep neural networks. It is also proven th
    
[^125]: 使用线性互补编程的时序符合预测区间

    Conformal Prediction Regions for Time Series using Linear Complementarity Programming. (arXiv:2304.01075v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2304.01075](http://arxiv.org/abs/2304.01075)

    本文提出了一种基于优化的方法，通过将预测误差参数化为多个时间步长，以找到不保守的预测区间，实现在使用学习启用的时间序列预测器进行长期规划和验证。

    

    符合预测是一种用于生成机器学习模型预测区间的统计工具，其具有高概率有效性。然而，将符合预测应用于时间序列数据会导致保守的预测区间。本文提出了一种基于优化的方法来减少这种保守性，以便在使用学习启用的时间序列预测器进行长期规划和验证。我们将预测误差参数化为多个时间步长，通过对额外数据集上的参数进行优化，找到了不保守的预测区间。我们表明，该问题可以被看作是一个混合整数线性互补规划（MILCP），我们将其放宽为一个线性互补规划（LCP）。

    Conformal prediction is a statistical tool for producing prediction regions of machine learning models that are valid with high probability. However, applying conformal prediction to time series data leads to conservative prediction regions. In fact, to obtain prediction regions over $T$ time steps with confidence $1-\delta$, {previous works require that each individual prediction region is valid} with confidence $1-\delta/T$. We propose an optimization-based method for reducing this conservatism to enable long horizon planning and verification when using learning-enabled time series predictors. Instead of considering prediction errors individually at each time step, we consider a parameterized prediction error over multiple time steps. By optimizing the parameters over an additional dataset, we find prediction regions that are not conservative. We show that this problem can be cast as a mixed integer linear complementarity program (MILCP), which we then relax into a linear complementa
    
[^126]: 通过约束层次强化学习处理长期和丰富约束的任务

    Handling Long and Richly Constrained Tasks through Constrained Hierarchical Reinforcement Learning. (arXiv:2302.10639v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.10639](http://arxiv.org/abs/2302.10639)

    本文通过约束层次强化学习的机制解决了长期和丰富约束的任务，在机器人清洁房屋的场景中展示了良好的性能。

    

    过去在目标导向的强化学习（RL）设置中，通常通过对轨迹施加约束来处理安全问题，对于短期任务表现良好。本文特别关注解决时间上延续的决策问题，例如机器人在清洁房屋的不同区域时，需要避开湿滑和不安全的区域（例如楼梯），同时保持足够的电量移动到充电站；而且面临复杂的安全约束。我们的主要创新是将上层的约束搜索代理（从给定的起始状态到远处目标状态计算最大化回报策略，同时满足成本约束）与底层的目标条件强化学习代理（估计在附近状态之间移动的成本和回报值）结合使用的安全约束搜索与层次强化学习（CoSHRL）机制。CoSHRL的一个主要优势在于它可以处理对轨迹上的约束。

    Safety in goal directed Reinforcement Learning (RL) settings has typically been handled through constraints over trajectories and have demonstrated good performance in primarily short horizon tasks. In this paper, we are specifically interested in the problem of solving temporally extended decision making problems such as robots cleaning different areas in a house while avoiding slippery and unsafe areas (e.g., stairs) and retaining enough charge to move to a charging dock; in the presence of complex safety constraints. Our key contribution is a (safety) Constrained Search with Hierarchical Reinforcement Learning (CoSHRL) mechanism that combines an upper level constrained search agent (which computes a reward maximizing policy from a given start to a far away goal state while satisfying cost constraints) with a low-level goal conditioned RL agent (which estimates cost and reward values to move between nearby states). A major advantage of CoSHRL is that it can handle constraints on the 
    
[^127]: 基于混合谱方法和谐振子的非可分离协方差核的时空高斯过程研究

    Non-separable Covariance Kernels for Spatiotemporal Gaussian Processes based on a Hybrid Spectral Method and the Harmonic Oscillator. (arXiv:2302.09580v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.09580](http://arxiv.org/abs/2302.09580)

    该论文提出了一种基于混合谱方法和谐振子的非可分离协方差核的时空高斯过程研究，通过物理论证推导出一类新型的非可分离协方差核，能更好地捕捉观测到的时空相关性。

    

    高斯过程提供了一个灵活的非参数框架，用于近似高维空间中的函数。协方差核是高斯过程的主要引擎，包含了预测分布的相关性。对于具有时空数据集的应用，合适的核应该建模联合的时空依赖关系。可分离的时空协方差核提供了简单和计算效率较高的方案。然而，非可分离核包含了更好地捕捉观测到的相关性的时空交互作用。大多数具有显式表达式的非可分离核是基于数学考虑（可允许条件）而非基于第一原理导出的。我们提出了一种基于物理论证的混合谱方法来生成协方差核。我们使用这种方法推导了一类新型的物理动机的非可分离协方差核，它们的根源来自随机线性...

    Gaussian processes provide a flexible, non-parametric framework for the approximation of functions in high-dimensional spaces. The covariance kernel is the main engine of Gaussian processes, incorporating correlations that underpin the predictive distribution. For applications with spatiotemporal datasets, suitable kernels should model joint spatial and temporal dependence. Separable space-time covariance kernels offer simplicity and computational efficiency. However, non-separable kernels include space-time interactions that better capture observed correlations. Most non-separable kernels that admit explicit expressions are based on mathematical considerations (admissibility conditions) rather than first-principles derivations. We present a hybrid spectral approach for generating covariance kernels which is based on physical arguments. We use this approach to derive a new class of physically motivated, non-separable covariance kernels which have their roots in the stochastic, linear, 
    
[^128]: 两阶段有约束的演员-评论家算法用于短视频推荐

    Two-Stage Constrained Actor-Critic for Short Video Recommendation. (arXiv:2302.01680v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01680](http://arxiv.org/abs/2302.01680)

    本论文提出了一种两阶段有约束的演员-评论家算法，用于解决短视频推荐问题。通过将短视频推荐问题建模为约束马尔可夫决策过程，我们解决了在用户交互和多样的响应中优化累计观看时间的问题。进行了两阶段的策略学习，并且能够同时满足主要目标和辅助目标的约束。

    

    社交媒体上短视频的广泛流行为视频分享平台上的推荐系统优化提供了新的机遇和挑战。用户与系统依次交互，并提供包括观看时间和对多个视频的各种类型交互在内的复杂多面 responses。一方面，平台旨在长期优化用户的累计观看时间（主要目标），这可以通过强化学习有效优化。另一方面，平台还需要满足适应多个用户交互 responses（辅助目标）的约束，如 follow、share 等。在本文中，我们将短视频推荐问题作为约束马尔可夫决策过程（CMDP）进行了建模。我们发现传统的约束强化学习算法在这种情况下效果不好。我们提出了一种新颖的两阶段有约束的演员-评论家方法：第一阶段，我们学习个体的策略，以优化主要目标。第二阶段，我们进一步学习共享的策略，以满足辅助目标的约束。

    The wide popularity of short videos on social media poses new opportunities and challenges to optimize recommender systems on the video-sharing platforms. Users sequentially interact with the system and provide complex and multi-faceted responses, including watch time and various types of interactions with multiple videos. One the one hand, the platforms aims at optimizing the users' cumulative watch time (main goal) in long term, which can be effectively optimized by Reinforcement Learning. On the other hand, the platforms also needs to satisfy the constraint of accommodating the responses of multiple user interactions (auxiliary goals) such like, follow, share etc. In this paper, we formulate the problem of short video recommendation as a Constrained Markov Decision Process (CMDP). We find that traditional constrained reinforcement learning algorithms can not work well in this setting. We propose a novel two-stage constrained actor-critic method: At stage one, we learn individual pol
    
[^129]: 基于数据驱动的高斯过程滤波器用于心电图去噪

    A Data-Driven Gaussian Process Filter for Electrocardiogram Denoising. (arXiv:2301.02607v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2301.02607](http://arxiv.org/abs/2301.02607)

    此论文提出了一种基于数据驱动的高斯过程滤波器，用于心电图去噪。通过使用心电图相位域和高斯分布假设，简化了计算过程，实现了无主观超参数的高效滤波器。

    

    目标: 基于高斯过程（GP）的滤波器在各种应用中已经有效地用于心电图（ECG）滤波，但是计算复杂度较高，而且其超参数的选择通常是主观的。方法: 我们开发了一种基于数据驱动的GP滤波器来解决这两个问题，使用ECG相位域的概念，该域是将ECG节拍变形到固定数量的样本和对齐的R峰上，假设其符合高斯分布。在这个假设下，样本均值和协方差矩阵的计算变得简化，可以在数据驱动的方式下高效地实现GP滤波器，而不需要主观超参数。我们在PhysioNet QT数据库上评估了所提出的滤波器，并将其与最先进的小波滤波器进行了比较。通过测量滤波器在信噪比（SNR）为-5至30 dB范围内（每隔5 dB）的提升来评估其性能。

    Objective: Gaussian Processes (GP)-based filters, which have been effectively used for various applications including electrocardiogram (ECG) filtering can be computationally demanding and the choice of their hyperparameters is typically ad hoc. Methods: We develop a data-driven GP filter to address both issues, using the notion of the ECG phase domain -- a time-warped representation of the ECG beats onto a fixed number of samples and aligned R-peaks, which is assumed to follow a Gaussian distribution. Under this assumption, the computation of the sample mean and covariance matrix is simplified, enabling an efficient implementation of the GP filter in a data-driven manner, with no ad hoc hyperparameters. The proposed filter is evaluated and compared with a state-of-the-art wavelet-based filter, on the PhysioNet QT Database. The performance is evaluated by measuring the signal-to-noise ratio (SNR) improvement of the filter at SNR levels ranging from -5 to 30dB, in 5dB steps, using addit
    
[^130]: 通过元学习Transformer实现通用上下文学习

    General-Purpose In-Context Learning by Meta-Learning Transformers. (arXiv:2212.04458v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.04458](http://arxiv.org/abs/2212.04458)

    本文展示了Transformer和其他黑盒模型可以通过元学习训练成为通用的上下文学习器，该模型可以在各种问题上进行测试集预测，无需定义推理模型、训练损失或优化算法。

    

    现代机器学习要求系统设计者指定学习流程的方方面面，例如损失函数、架构和优化器。而元学习，或者学会学习，目标是学习这些方面，并承诺以更少的手动工作开启更大的能力。元学习的一个特别雄心勃勃的目标是从头开始训练通用的上下文学习算法，仅使用带有最小归纳偏见的黑盒模型。这样的模型接收训练数据，并在各种问题上产生测试集预测，而无需定义推理模型、训练损失或优化算法。在本文中，我们展示了Transformer和其他黑盒模型可以被元训练成为通用的上下文学习器。我们通过模型大小、任务数量和元优化引起的算法之间的转换进行了表征，这些算法可以实现泛化，也可以实现记忆，还有一些算法根本无法进行元训练。

    Modern machine learning requires system designers to specify aspects of the learning pipeline, such as losses, architectures, and optimizers. Meta-learning, or learning-to-learn, instead aims to learn those aspects, and promises to unlock greater capabilities with less manual effort. One particularly ambitious goal of meta-learning is to train general-purpose in-context learning algorithms from scratch, using only black-box models with minimal inductive bias. Such a model takes in training data, and produces test-set predictions across a wide range of problems, without any explicit definition of an inference model, training loss, or optimization algorithm. In this paper we show that Transformers and other black-box models can be meta-trained to act as general-purpose in-context learners. We characterize transitions between algorithms that generalize, algorithms that memorize, and algorithms that fail to meta-train at all, induced by changes in model size, number of tasks, and meta-opti
    
[^131]: 无分布假设的节点分类预测集

    Distribution Free Prediction Sets for Node Classification. (arXiv:2211.14555v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.14555](http://arxiv.org/abs/2211.14555)

    本文利用近期在设定预测下的进展，构建了预测集以适应归纳学习场景下的节点分类，证明了提供了比简单的符合预测应用更加紧致和良好校准的预测集。

    

    图神经网络通常可以在许多重要的真实数据集上达到高精度分类的效果，但其无法提供严格的预测不确定性定义。由于图结构引起的数据点依赖性，量化GNN模型的置信度很困难。本文利用近期在设定预测下的进展，构建了预测集以适应归纳学习场景下的节点分类。我们对现有的换位分类方法进行了改进，通过适当加权符合分数来反映网络结构。通过在常用标准基准数据集上使用流行的GNN模型进行实验，我们证明了我们的方法提供了比简单的符合预测应用更加紧致和良好校准的预测集。

    Graph Neural Networks (GNNs) are able to achieve high classification accuracy on many important real world datasets, but provide no rigorous notion of predictive uncertainty. Quantifying the confidence of GNN models is difficult due to the dependence between datapoints induced by the graph structure.  We leverage recent advances in conformal prediction to construct prediction sets for node classification in inductive learning scenarios. We do this by taking an existing approach for conformal classification that relies on \textit{exchangeable} data and modifying it by appropriately weighting the conformal scores to reflect the network structure. We show through experiments on standard benchmark datasets using popular GNN models that our approach provides tighter and better calibrated prediction sets than a naive application of conformal prediction.
    
[^132]: DyG2Vec: 带有自监督的动态图表征学习

    DyG2Vec: Representation Learning for Dynamic Graphs with Self-Supervision. (arXiv:2210.16906v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.16906](http://arxiv.org/abs/2210.16906)

    DyG2Vec是一个具有自监督学习能力的动态图表征学习模型，采用了高效而有效的注意力编码器和非对比自监督学习方法，能够提取丰富的时间嵌入表示。在基准数据集上的实验结果表明，该模型在未来链接预测任务中表现出色。

    

    时间图神经网络已经展示出在通过自动提取时间模式来学习归纳表示方面的有希望结果。然而，以往的工作常常依赖于复杂的记忆模块或低效的随机游走方法来构建时间表示。此外，现有的动态图编码器不容易适应自监督范式，这阻碍了它们利用无标签数据。为了解决这些限制，我们提出了一种高效而有效的基于注意力的编码器，利用时间边编码和基于窗口的子图采样来生成任务无关的嵌入。此外，我们提出了一种使用非对比SSL的联合嵌入架构，以学习丰富的时间嵌入而不需要标签。在7个基准数据集上的实验结果表明，我们的模型在传导设置和归纳设置的未来链接预测任务中，平均优于现有的SoTA基线4.23％和3.30％。

    Temporal graph neural networks have shown promising results in learning inductive representations by automatically extracting temporal patterns. However, previous works often rely on complex memory modules or inefficient random walk methods to construct temporal representations. In addition, the existing dynamic graph encoders are non-trivial to adapt to self-supervised paradigms, which prevents them from utilizing unlabeled data. To address these limitations, we present an efficient yet effective attention-based encoder that leverages temporal edge encodings and window-based subgraph sampling to generate task-agnostic embeddings. Moreover, we propose a joint-embedding architecture using non-contrastive SSL to learn rich temporal embeddings without labels. Experimental results on 7 benchmark datasets indicate that on average, our model outperforms SoTA baselines on the future link prediction task by 4.23% for the transductive setting and 3.30% for the inductive setting while only requi
    
[^133]: 学习图像表示以进行异常检测：在药物开发中发现组织学改变的应用

    Learning image representations for anomaly detection: application to discovery of histological alterations in drug development. (arXiv:2210.07675v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.07675](http://arxiv.org/abs/2210.07675)

    该论文提出了一种基于CNN的异常检测系统，通过对健康组织进行辅助任务训练，使表示适应组织中的相关细节，实现对组织学图像中的异常情况检测。

    

    我们提出了一种用于组织病理学图像异常检测的系统。在组织学中，正常样本通常是大量存在的，而异常（病理）情况通常很少或不可用。在这种情况下，使用在健康数据上训练的单类分类器可以检测到分布外的异常样本。这样的方法与预训练的卷积神经网络（CNN）图像表示相结合，以前已经用于异常检测（AD）。但是，预训练的现成CNN表示可能对组织中的异常情况不敏感，而健康组织的自然变异可能导致远离的表示。为了使表示适应健康组织中的相关细节，我们建议在辅助任务上训练CNN，该任务区分不同物种、器官和染色试剂的健康组织。几乎不需要额外的标注工作量，因为健康样本可以自动获得上述标签。在训练中，我们强制执行

    We present a system for anomaly detection in histopathological images. In histology, normal samples are usually abundant, whereas anomalous (pathological) cases are scarce or not available. Under such settings, one-class classifiers trained on healthy data can detect out-of-distribution anomalous samples. Such approaches combined with pre-trained Convolutional Neural Network (CNN) representations of images were previously employed for anomaly detection (AD). However, pre-trained off-the-shelf CNN representations may not be sensitive to abnormal conditions in tissues, while natural variations of healthy tissue may result in distant representations. To adapt representations to relevant details in healthy tissue we propose training a CNN on an auxiliary task that discriminates healthy tissue of different species, organs, and staining reagents. Almost no additional labeling workload is required, since healthy samples come automatically with aforementioned labels. During training we enforce
    
[^134]: LL-GNN: 基于FPGA的低延迟图神经网络在高能物理领域的应用

    LL-GNN: Low Latency Graph Neural Networks on FPGAs for High Energy Physics. (arXiv:2209.14065v4 [cs.AR] UPDATED)

    [http://arxiv.org/abs/2209.14065](http://arxiv.org/abs/2209.14065)

    本文提出了一种新型的基于FPGA的低延迟图神经网络(LL-GNN)架构，针对粒子探测器领域的特殊需求，通过外积矩阵乘法方法、结构化邻接矩阵和列主数据布局等优化措施，实现了亚微秒级别的网络部署，并提供了一种GNN特定的算法-硬件协同设计方法。

    

    本文提出了一种新型的可重构架构，用于实现基于FPGA的低延迟图神经网络(LL-GNN)，以支持具有前所未有的低延迟性能的粒子探测器。由于在欧洲核子研究中心大型强子对撞机实验的一级触发器中需要以每秒数百太字节的数据速率进行在线事件选择，因此将基于FPGA的GNNs应用于粒子探测器面临着独特的挑战，需要在亚微秒级别内部署网络。本文提出了一种基于外积的矩阵乘法方法，并利用结构化邻接矩阵和列主数据布局进行了优化。此外，还引入了一种融合步骤，通过消除不必要的边界进一步降低了端到端设计的延迟。此外，本文还提出了一种GNN特定的算法-硬件协同设计方法，在给定延迟限制下寻找更好延迟和更高精度的设计。

    This work presents a novel reconfigurable architecture for Low Latency Graph Neural Network (LL-GNN) designs for particle detectors, delivering unprecedented low latency performance. Incorporating FPGA-based GNNs into particle detectors presents a unique challenge since it requires sub-microsecond latency to deploy the networks for online event selection with a data rate of hundreds of terabytes per second in the Level-1 triggers at the CERN Large Hadron Collider experiments. This paper proposes a novel outer-product based matrix multiplication approach, which is enhanced by exploiting the structured adjacency matrix and a column-major data layout. Moreover, a fusion step is introduced to further reduce the end-to-end design latency by eliminating unnecessary boundaries. Furthermore, a GNN-specific algorithm-hardware co-design approach is presented which not only finds a design with a much better latency but also finds a high accuracy design under given latency constraints. To facilita
    
[^135]: 利用同形异义字在文本到图像合成中挖掘文化偏见

    Exploiting Cultural Biases via Homoglyphs in Text-to-Image Synthesis. (arXiv:2209.08891v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.08891](http://arxiv.org/abs/2209.08891)

    研究表明，在文本到图像合成过程中，通过插入异形字，模型会反映生成图片中的文化刻板印象和偏见。这一现象的根本原因是模型的文本编码器。而恶意用户或服务提供商还可能利用类似外形的非拉丁字符，故意引入偏见，创造种族主义刻板印象。

    

    文本到图像合成模型，例如DALL-E 2和Stable Diffusion，近年来吸引了学术界和广大公众的广泛关注。这些模型可以在文本描述的条件下生成描绘各种概念和风格的高质量图像。然而，这些模型从大量的训练数据中采用了与特定Unicode脚本相关的文化特征，这可能不会立即显现。我们展示了通过在文本描述中简单插入单个非拉丁字符，常见模型呈现出生成图像中的文化刻板印象和偏见。我们定性和定量分析了这种行为，并确定了模型的文本编码器是这一现象的根本原因。此外，恶意用户或服务提供商可能试图意图性地通过将拉丁字符替换为非拉丁脚本中外形相似的字符，来引入偏见，创造种族主义刻板印象。

    Models for text-to-image synthesis, such as DALL-E~2 and Stable Diffusion, have recently drawn a lot of interest from academia and the general public. These models are capable of producing high-quality images that depict a variety of concepts and styles when conditioned on textual descriptions. However, these models adopt cultural characteristics associated with specific Unicode scripts from their vast amount of training data, which may not be immediately apparent. We show that by simply inserting single non-Latin characters in a textual description, common models reflect cultural stereotypes and biases in their generated images. We analyze this behavior both qualitatively and quantitatively, and identify a model's text encoder as the root cause of the phenomenon. Additionally, malicious users or service providers may try to intentionally bias the image generation to create racist stereotypes by replacing Latin characters with similarly-looking characters from non-Latin scripts, so-cal
    
[^136]: 论人工智能和机器学习的演变：朝着在主要人工智能会议上衡量和理解影响、影响力和领导力的元级的方向发展

    On the Evolution of A.I. and Machine Learning: Towards a Meta-level Measuring and Understanding Impact, Influence, and Leadership at Premier A.I. Conferences. (arXiv:2205.13131v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2205.13131](http://arxiv.org/abs/2205.13131)

    本研究旨在理解人工智能和机器学习的演变，通过衡量研究者在该领域的影响、影响力和领导力，并通过分析在主要人工智能会议上发表的论文，揭示了人工智能领域的发展和演变。人工智能的发展导致了学术论文数量的增加，本研究构建了全面的引用和合作数据集，对相关关系进行了计算。

    

    人工智能现在被认为是一项对人类生活产生广泛影响的通用技术。本研究旨在从研究者对该领域的贡献角度来理解人工智能以及机器学习的演变。为此，我们提出了几种衡量人工智能和机器学习研究者影响、影响力和领导力的指标，并通过研究自1969年首次举办国际人工智能联合会议 (IJCAI) 以来在主要人工智能和机器学习会议上发表的论文，对人工智能领域的发展和演变进行探索，从而在一定程度上对人工智能的历史和演变有了新的认识。人工智能的发展和演变导致了学术论文的增加，在过去的六十年来发表的文章数量也有所反映。我们构建了全面的引用合作与论文-作者数据集，并计算了合作引用关系。

    Artificial Intelligence is now recognized as a general-purpose technology with ample impact on human life. This work aims at understanding the evolution of AI and, in particular Machine learning, from the perspective of researchers' contributions to the field. In order to do so, we present several measures allowing the analyses of AI and machine learning researchers' impact, influence, and leadership over the last decades. This work also contributes, to a certain extent, to shed new light on the history and evolution of AI by exploring the dynamics involved in the field's evolution by looking at papers published at the flagship AI and machine learning conferences since the first International Joint Conference on Artificial Intelligence (IJCAI) held in 1969. AI development and evolution have led to increasing research output, reflected in the number of articles published over the last sixty years. We construct comprehensive citation collaboration and paper-author datasets and compute co
    
[^137]: 使用预训练的深度分层VAE实现多样的超分辨率图像

    Diverse super-resolution with pretrained deep hiererarchical VAEs. (arXiv:2205.10347v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2205.10347](http://arxiv.org/abs/2205.10347)

    本文提出了使用预训练的深度分层VAE作为先验，通过结合低分辨率编码器和预训练的生成模型进行图像超分辨率处理。在人脸超分辨率任务中，我们的方法在计算效率和样本质量之间取得了有利的权衡。

    

    我们研究了在图像超分辨率问题中生成多样解决方案的问题。从概率的角度来看，可以通过从逆问题的后验分布中进行采样来实现这一点，这需要在高分辨率图像上定义一个先验分布。在这项工作中，我们提出使用一个预训练的分层变分自编码器（HVAE）作为先验。我们训练一个轻量级的随机编码器，将低分辨率图像编码到预训练HVAE的潜在空间中。在推理过程中，我们将低分辨率编码器和预训练的生成模型结合起来对图像进行超分辨率处理。我们在人脸超分辨率任务上证明了我们的方法在条件正则化流技术的计算效率和扩散方法的样本质量之间提供了有利的权衡。

    We investigate the problem of producing diverse solutions to an image super-resolution problem. From a probabilistic perspective, this can be done by sampling from the posterior distribution of an inverse problem, which requires the definition of a prior distribution on the high-resolution images. In this work, we propose to use a pretrained hierarchical variational autoencoder (HVAE) as a prior. We train a lightweight stochastic encoder to encode low-resolution images in the latent space of a pretrained HVAE. At inference, we combine the low-resolution encoder and the pretrained generative model to super-resolve an image. We demonstrate on the task of face super-resolution that our method provides an advantageous trade-off between the computational efficiency of conditional normalizing flows techniques and the sample quality of diffusion based methods.
    
[^138]: 基于多重表示的终身集成学习在少样本目标识别中的应用

    Lifelong Ensemble Learning based on Multiple Representations for Few-Shot Object Recognition. (arXiv:2205.01982v4 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2205.01982](http://arxiv.org/abs/2205.01982)

    本文提出了一种基于多重表示的终身集成学习方法，用于开放式场景下的少样本目标识别问题，适用于三维物体类别数量不固定、随时间增长的场景。模型针对各种类型物体进行处理，并进行了广泛实验。

    

    服务机器人越来越多地融入我们的日常生活，帮助我们处理各种任务。在这样的环境中，机器人经常面对新的物体，并需要以开放式方式学习它们。此外，这样的机器人必须能够识别各种物体类别。本文提出了一种基于多重表示的终身集成学习方法，以解决少样本目标识别问题。具体地，我们基于深度表示和手工制作的3D形状描述符形成集成方法。为了促进终身学习，每种方法都配备了一个存储和瞬间检索物体信息的记忆单元。所提出的模型适用于三维物体类别数量不固定、随时间增长的开放式学习场景。我们进行了广泛的实验，评估了所提出方法在离线及开放式场景下的性能。

    Service robots are integrating more and more into our daily lives to help us with various tasks. In such environments, robots frequently face new objects while working in the environment and need to learn them in an open-ended fashion. Furthermore, such robots must be able to recognize a wide range of object categories. In this paper, we present a lifelong ensemble learning approach based on multiple representations to address the few-shot object recognition problem. In particular, we form ensemble methods based on deep representations and handcrafted 3D shape descriptors. To facilitate lifelong learning, each approach is equipped with a memory unit for storing and retrieving object information instantly. The proposed model is suitable for open-ended learning scenarios where the number of 3D object categories is not fixed and can grow over time. We have performed extensive sets of experiments to assess the performance of the proposed approach in offline, and open-ended scenarios. For t
    
[^139]: 药物设计中的分子生成：图学习的视角

    Molecule Generation for Drug Design: a Graph Learning Perspective. (arXiv:2202.09212v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.09212](http://arxiv.org/abs/2202.09212)

    本论文调研了药物设计领域中基于图学习的分子生成方法，分为一步到位法、基于片段法和节点逐个法三类，介绍了公共数据集和评估指标，并讨论了未来研究的挑战和方向。

    

    机器学习，特别是图学习，在各个领域都获得了越来越多的认可，具有变革性的影响。其中一个有前景的应用领域是药物设计和发现，特别是在制药行业中。我们的调研提供了药物设计领域现有方法的综合概述，特别关注将（深度）图学习技术纳入其中的全新药物设计方法。我们将这些方法分为三个不同的组：一）一步到位法，二）基于片段法，三）节点逐个法。此外，我们介绍了一些关键的公共数据集，并概述了分子生成和优化常用的评估指标。最后，我们讨论了该领域现有的挑战并提出了未来研究的潜在方向。

    Machine learning, particularly graph learning, is gaining increasing recognition for its transformative impact across various fields. One such promising application is in the realm of molecule design and discovery, notably within the pharmaceutical industry. Our survey offers a comprehensive overview of state-of-the-art methods in molecule design, particularly focusing on \emph{de novo} drug design, which incorporates (deep) graph learning techniques. We categorize these methods into three distinct groups: \emph{i)} \emph{all-at-once}, \emph{ii)} \emph{fragment-based}, and \emph{iii)} \emph{node-by-node}. Additionally, we introduce some key public datasets and outline the commonly used evaluation metrics for both the generation and optimization of molecules. In the end, we discuss the existing challenges in this field and suggest potential directions for future research.
    
[^140]: 异常检测中的加权孤立森林和随机切割森林算法

    Weighted Isolation and Random Cut Forest Algorithms for Anomaly Detection. (arXiv:2202.01891v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.01891](http://arxiv.org/abs/2202.01891)

    该论文提出了加权孤立森林（WIF）和加权随机切割森林（WRCF）算法，用于改进异常检测，特别是时间序列数据的异常检测。这些算法利用考虑整体数据结构的权重策略确定分割值，相比原有算法表现出更好的性能。

    

    针对异常检测，特别是时间序列数据的异常检测，已经开发出了随机切割森林（RCF）算法。RCF算法是孤立森林（IF）算法的改进版本。与IF算法不同，RCF算法可以通过将输入插入构建的树网络来确定实时输入是否包含异常。已经开发出了多种RCF算法，包括鲁棒RCF（RRCF），其中切割过程是自适应地概率选择的。RRCF算法表现出比IF算法更好的性能，因为它基于数据的几何范围决定维度的切割，而IF算法随机选择维度的切割。然而，IF和RRCF都没有考虑整体数据结构，因为分割值是随机选择的。在本文中，我们提出了新的IF和RCF算法，分别称为加权IF（WIF）和加权RCF（WRCF）算法。它们的分割值由一种考虑整体数据结构的权重策略确定。

    Random cut forest (RCF) algorithms have been developed for anomaly detection, particularly in time series data. The RCF algorithm is an improved version of the isolation forest (IF) algorithm. Unlike the IF algorithm, the RCF algorithm can determine whether real-time input contains an anomaly by inserting the input into the constructed tree network. Various RCF algorithms, including Robust RCF (RRCF), have been developed, where the cutting procedure is adaptively chosen probabilistically. The RRCF algorithm demonstrates better performance than the IF algorithm, as dimension cuts are decided based on the geometric range of the data, whereas the IF algorithm randomly chooses dimension cuts. However, the overall data structure is not considered in both IF and RRCF, given that split values are chosen randomly. In this paper, we propose new IF and RCF algorithms, referred to as the weighted IF (WIF) and weighted RCF (WRCF) algorithms, respectively. Their split values are determined by consi
    
[^141]: 隐私保护的逻辑回归训练及其更快的梯度变种

    Privacy-Preserving Logistic Regression Training with A Faster Gradient Variant. (arXiv:2201.10838v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2201.10838](http://arxiv.org/abs/2201.10838)

    本文提出了一种更快的梯度变种——二次梯度，用于在同态加密领域实现隐私保护的逻辑回归训练，并成功提升了收敛速度，实现了同态逻辑回归训练仅需较少的迭代次数。

    

    多年来，加密数据上的逻辑回归训练一直是一个有吸引力的安全解决方案。本文提出了一种更快的梯度变种——二次梯度，用于在同态加密领域实现逻辑回归训练，其核心可以看作是简化的固定Hessian的扩展。我们分别向Nesterov的加速梯度方法（NAG）和自适应梯度算法（Adagrad）增强了该梯度变种，并在多个数据集上评估了增强算法。实验结果表明，增强方法在收敛速度上比朴素的一阶梯度方法具有最先进的性能。然后，我们采用增强的NAG方法来实现同态逻辑回归训练，并在仅3次迭代中获得了可比较的结果。

    Logistic regression training over encrypted data has been an attractive idea to security concerns for years. In this paper, we propose a faster gradient variant called $\texttt{quadratic gradient}$ to implement logistic regression training in a homomorphic encryption domain, the core of which can be seen as an extension of the simplified fixed Hessian.  We enhance Nesterov's accelerated gradient (NAG) and Adaptive Gradient Algorithm (Adagrad) respectively with this gradient variant and evaluate the enhanced algorithms on several datasets. Experimental results show that the enhanced methods have a state-of-the-art performance in convergence speed compared to the naive first-order gradient methods. We then adopt the enhanced NAG method to implement homomorphic logistic regression training and obtain a comparable result by only $3$ iterations.
    
[^142]: Lojasiewicz-landscape的随机梯度下降方案的收敛性

    Convergence of stochastic gradient descent schemes for Lojasiewicz-landscapes. (arXiv:2102.09385v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2102.09385](http://arxiv.org/abs/2102.09385)

    本文研究了Lojasiewicz-landscape下的随机梯度下降方案的收敛性，证明了当SGD保持有界且具有可数个临界点，或目标函数满足Lojasiewicz不等式时，SGD将收敛。此外，在神经网络中使用一些特定的解析激活函数时，如果训练中的信号和响应是紧支撑的，SGD也将收敛。

    

    在本文中，我们考虑了对弱假设下的随机梯度下降方案（SGD）的收敛性，包括动量随机梯度下降（MSGD）。具体而言，我们证明了在SGD保持有界的情况下，如果存在可数个临界点或者目标函数在所有临界水平附近满足Lojasiewicz不等式，如同所有解析函数一样，SGD将会收敛。特别地，我们证明了对于具有解析激活函数（如softplus、sigmoid和双曲正切）的神经网络，在随机变量对训练中的信号和响应进行紧支撑的情况下，SGD将在保持有界的情况下收敛。

    In this article, we consider convergence of stochastic gradient descent schemes (SGD), including momentum stochastic gradient descent (MSGD), under weak assumptions on the underlying landscape. More explicitly, we show that on the event that the SGD stays bounded we have convergence of the SGD if there is only a countable number of critical points or if the objective function satisfies Lojasiewicz-inequalities around all critical levels as all analytic functions do. In particular, we show that for neural networks with analytic activation function such as softplus, sigmoid and the hyperbolic tangent, SGD converges on the event of staying bounded, if the random variables modelling the signal and response in the training are compactly supported.
    

