# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities.](http://arxiv.org/abs/2401.14405) | 本文提出了一种名为多模态路径的方法，通过利用其他模态的无关数据来改进特定模态的Transformer，实现了两个模型之间的组件连接，从而提高了模型的序列建模能力。 |
| [^2] | [Deconstructing Denoising Diffusion Models for Self-Supervised Learning.](http://arxiv.org/abs/2401.14404) | 本研究对最初用于图像生成的去噪扩散模型（DDM）进行了解构，发现只有很少的现代组件对于学习良好的表示是至关重要的。通过将DDM逐步转化为经典的去噪自编码器（DAE），我们提出了一种高度简化且类似于经典DAE的方法。该研究希望重新引起人们对现代自监督学习领域中经典方法的兴趣。 |
| [^3] | [Adaptive Mobile Manipulation for Articulated Objects In the Open World.](http://arxiv.org/abs/2401.14403) | 本文介绍了一种针对开放环境中关节物体操作的全栈方法，机器人通过自适应学习框架从少量数据中学习，并通过在线实践学习适应训练分布之外的新对象。同时，还开发了低成本的移动操作硬件平台。 |
| [^4] | [pix2gestalt: Amodal Segmentation by Synthesizing Wholes.](http://arxiv.org/abs/2401.14398) | pix2gestalt是一个用于零样本非模态分割的框架，能够学习估计被遮挡的整个对象的形状和外观，通过利用大规模扩散模型，学习条件扩散模型来重建整个对象，在多个基准测试中优于有监督的基线方法，同时可以显著提高在存在遮挡情况下的对象识别和3D重建方法的性能。 |
| [^5] | [Smooth Ranking SVM via Cutting-Plane Method.](http://arxiv.org/abs/2401.14388) | 本论文提出了一种利用割平面方法的平滑排名SVM算法来最大化分类性能中的AUC指标。通过迭代引入割平面以防止过拟合，并惩罚模型的变化。 |
| [^6] | [An Orthogonal Polynomial Kernel-Based Machine Learning Model for Differential-Algebraic Equations.](http://arxiv.org/abs/2401.14382) | 这项研究通过使用正交多项式核和LS-SVR算法，提出了一种用于解决差分代数方程系统的新的机器学习模型，并通过模拟验证了其有效性。 |
| [^7] | [Manifold GCN: Diffusion-based Convolutional Neural Network for Manifold-valued Graphs.](http://arxiv.org/abs/2401.14381) | 本研究提出了两个用于具有流形值特征的图的神经网络层。这些层具有对节点排列和特征流形的等变性，并在深度学习任务中显示出有益的归纳偏差。 |
| [^8] | [UrbanGenAI: Reconstructing Urban Landscapes using Panoptic Segmentation and Diffusion Models.](http://arxiv.org/abs/2401.14379) | 本文介绍了一种使用全景分割和扩散模型重构城市景观的新方法，该方法通过整合先进的图像分割和扩散模型，实现了对城市设计的全面处理，并在物体检测和文本到图像生成方面取得了显著准确性。 |
| [^9] | [TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation.](http://arxiv.org/abs/2401.14373) | TURNA是一种用于土耳其语的低资源语言模型，具备自然语言理解和生成任务能力。TURNA通过预训练的编码-解码架构在理解和生成任务中表现优于多语言模型，并能与土耳其语单语模型竞争。 |
| [^10] | [Genie: Achieving Human Parity in Content-Grounded Datasets Generation.](http://arxiv.org/abs/2401.14367) | Genie是一个用于自动生成高质量内容导向数据的方法，通过三个阶段实现：内容准备、生成和过滤。在人类评估中，生成的数据被发现是自然且高质量的，并且通过与使用人工数据训练的模型比较，我们的模型表现相当或更好。 |
| [^11] | [MoE-Infinity: Activation-Aware Expert Offloading for Efficient MoE Serving.](http://arxiv.org/abs/2401.14361) | MoE-Infinity是一种成本高效的MoE服务系统，通过激活感知的专家卸载和缓存技术，显著降低了延迟，并提高了成本性能。 |
| [^12] | [ServerlessLLM: Locality-Enhanced Serverless Inference for Large Language Models.](http://arxiv.org/abs/2401.14351) | ServerlessLLM是一种用于大型语言模型的增强本地化无服务器推理系统，通过优化检查点加载、本地化推理和服务器分配来实现高效且低延迟的推理过程。 |
| [^13] | [Class-attribute Priors: Adapting Optimization to Heterogeneity and Fairness Objective.](http://arxiv.org/abs/2401.14343) | 本研究提出了一种名称为CAP的方法，通过生成类别特定的学习策略来更好地适应异质性数据，并在损失函数设计和标签不平衡问题方面取得了显著改进。 |
| [^14] | [Estimation of partially known Gaussian graphical models with score-based structural priors.](http://arxiv.org/abs/2401.14340) | 本论文提出了一种基于得分结构先验的算法，用于估计部分已知高斯图模型。通过使用图神经网络来估计图的得分函数，我们可以在生成样本时利用退火朗格维能扩散，从而更准确地估计后验分布。数值实验表明，我们的方法具有明显的优势。 |
| [^15] | [SunBlock: Cloudless Protection for IoT Systems.](http://arxiv.org/abs/2401.14332) | 本文研究了在家庭路由器上使用AI工具和经典流量过滤算法进行本地有效的物联网威胁检测，其结果表明装备我们的解决方案的典型家庭路由器能够在保护物联网网络方面与现有的流行解决方案相当或优于其性能。 |
| [^16] | ["All of Me": Mining Users' Attributes from their Public Spotify Playlists.](http://arxiv.org/abs/2401.14296) | 本研究调查了Spotify用户属性与他们公开播放列表之间的关系，特别关注识别与用户个人属性相关的音乐特征。 |
| [^17] | [Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts.](http://arxiv.org/abs/2401.14295) | 这篇论文探讨了结合结构的提示工程在提高大型语言模型推理性能方面的前景，通过思维链、思维树或思维图的设计来引导整体推理过程。通过大量实例，这种范式显著增强了模型在多个任务中的能力。总的来说，论文提供了一个通用蓝图，为未来的发展铺平道路。 |
| [^18] | [Speech foundation models on intelligibility prediction for hearing-impaired listeners.](http://arxiv.org/abs/2401.14289) | 对于听力受损听众智能预测应用，对语音基础模型（SFM）进行了系统评估，并提出了一种简单的方法，在冻结的SFM之上学习一个轻量级的专门的预测头，这一方法在Clarity Prediction Challenge 2（CPC2）中取得了胜利，展示了其在语音知觉应用中的潜力。 |
| [^19] | [Information Leakage Detection through Approximate Bayes-optimal Prediction.](http://arxiv.org/abs/2401.14283) | 本论文通过建立一个理论框架，利用统计学习理论和信息论来准确量化和检测信息泄漏，通过近似贝叶斯预测的对数损失和准确性来准确估计互信息。 |
| [^20] | [Producing Plankton Classifiers that are Robust to Dataset Shift.](http://arxiv.org/abs/2401.14256) | 本研究针对浮游生物分类器在数据集变化下性能下降的挑战，通过整合不同部署日的数据集并评估超出数据集的性能，揭示了分类器在实际情景中遇到的失败情况。基于对导致性能下降的条件的研究，我们提出了一种预防性评估方法，用于识别新数据分类中可能出现的问题，并确定影响分类的特征。 |
| [^21] | [Interpretable Solutions for Breast Cancer Diagnosis with Grammatical Evolution and Data Augmentation.](http://arxiv.org/abs/2401.14255) | 本文通过结合语法演化和数据增强方案，利用合成数据生成技术STEM来训练可解释的乳腺癌诊断模型，解决了数据不平衡和模型可解释性的问题。 |
| [^22] | [Enhanced Labeling Technique for Reddit Text and Fine-Tuned Longformer Models for Classifying Depression Severity in English and Luganda.](http://arxiv.org/abs/2401.14240) | 本研究通过提出的标注方法对Reddit文本进行分类，并细调了Longformer模型，研究结果表明该模型在英语和卢干达语的抑郁症严重程度分类中表现出色，优于基准模型。 |
| [^23] | [AR-GAN: Generative Adversarial Network-Based Defense Method Against Adversarial Attacks on the Traffic Sign Classification System of Autonomous Vehicles.](http://arxiv.org/abs/2401.14232) | 这篇论文提出了一种基于生成对抗网络的防御方法AR-GAN，用于自动驾驶车辆中的交通标志分类系统，该方法具有在各种对抗攻击下保持高性能的能力。 |
| [^24] | [Assessing the Portability of Parameter Matrices Trained by Parameter-Efficient Finetuning Methods.](http://arxiv.org/abs/2401.14228) | 本文研究了通过参数高效微调方法训练的模块的可移植性，发现这些移植的模块在各种情景下表现出优异的性能，可以有效地重复利用任务特定知识。 |
| [^25] | [Sample Efficient Reinforcement Learning by Automatically Learning to Compose Subtasks.](http://arxiv.org/abs/2401.14226) | 本工作提出了一种用于提高样本效率的强化学习算法，通过自动构建奖励函数来选择最优子任务，在复杂任务中取得了较好的性能。 |
| [^26] | [Communication-Efficient Federated Learning through Adaptive Weight Clustering and Server-Side Distillation.](http://arxiv.org/abs/2401.14211) | 本论文提出了一种名为FedCompress的新方法，通过动态权重聚类和服务器端知识蒸馏的结合，实现了高效通信的联邦学习。该方法在降低通信成本的同时，能够学习到高度可泛化的模型。 |
| [^27] | [At the junction between deep learning and statistics of extremes: formalizing the landslide hazard definition.](http://arxiv.org/abs/2401.14210) | 该论文通过使用深度学习和极值理论驱动的模型，开发了一个统一的方法来估计滑坡灾害。这种方法将滑坡位置的空间信息、威胁程度和频率结合起来，填补了目前在处理大范围地区时仅考虑两个元素的不足之处。 |
| [^28] | [MTRGL:Effective Temporal Correlation Discerning through Multi-modal Temporal Relational Graph Learning.](http://arxiv.org/abs/2401.14199) | 该论文引入了一种新的框架——多模态时间关系图学习（MTRGL），通过将时间序列数据和离散特征结合成一个时间图，并采用记忆机制的图神经网络来辨识实体间的时间相关性，取得了在实证实验中的优异表现。这一研究对于提升自动化配对交易策略具有重要意义。 |
| [^29] | [DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence.](http://arxiv.org/abs/2401.14196) | DeepSeek-Coder是一系列开源代码模型，通过在高质量项目级代码语料库上进行预训练和采用填空任务和16K窗口来增强代码生成和填充，不仅在多个基准测试中取得了与开源代码模型同样的最新表现，而且超过了现有的闭源模型。 |
| [^30] | [How Can Large Language Models Understand Spatial-Temporal Data?.](http://arxiv.org/abs/2401.14192) | 本文提出了一种名为STG-LLM的创新方法，用于使大型语言模型能够理解时空数据并进行预测。该方法利用STG-Tokenizer将复杂的图形数据转化为简洁的标记，再通过STG-Adapter将标记化数据与LLM的理解能力进行连接。通过微调参数，STG-LLM能够有效地把握标记的语义，同时保留LLM的自然语言理解能力。通过广泛的实验验证了STG-LLM的优越性能。 |
| [^31] | [Friendly Attacks to Improve Channel Coding Reliability.](http://arxiv.org/abs/2401.14184) | 本文引入了一种名为“友好攻击”的新方法，通过在传输前对码字进行轻微扰动，有效提高了纠错信道编码的性能，提高了可靠性。 |
| [^32] | [Alleviating Structural Distribution Shift in Graph Anomaly Detection.](http://arxiv.org/abs/2401.14155) | 这项研究从特征视角解决了图形异常检测中的结构分布偏移问题，通过抵抗异常节点的高异质性，同时从同质邻居中受益于正常节点的学习。 |
| [^33] | [True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning.](http://arxiv.org/abs/2401.14151) | 本研究通过使用大型语言模型（LLMs）作为决策智能体，通过强化学习与具身环境高效互动来解决LLMs与环境之间知识不对齐的问题。通过查询LLMs的联合概率，形成行为策略，并通过两种归一化方法和四个提示设计原则提高策略的稳定性和鲁棒性。最后，通过设计参数高效的训练架构提高学习效率。 |
| [^34] | [Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Conditional Interpretations.](http://arxiv.org/abs/2401.14142) | 基于能量的概念瓶颈模型统一了预测、概念干预和条件解释的功能，解决了现有方法在高阶非线性相互作用和复杂条件依赖关系上的限制。 |
| [^35] | [Convolutional Neural Networks can achieve binary bail judgement classification.](http://arxiv.org/abs/2401.14135) | 本文研究使用卷积神经网络在印度印地文法律文件上进行二元保释判断分类，取得了93％的准确率，改进了之前的准确率基准。 |
| [^36] | [Equivariant Manifold Neural ODEs and Differential Invariants.](http://arxiv.org/abs/2401.14131) | 本文提出了一种等变流形神经常微分方程的几何框架，利用微分不变量和嵌入等变流形中的增广形式进行建模和逼近。 |
| [^37] | [Attention-based Efficient Classification for 3D MRI Image of Alzheimer's Disease.](http://arxiv.org/abs/2401.14130) | 本研究提出了一种基于注意力机制的高效阿尔茨海默病分类算法，通过使用预训练的ResNet网络作为骨干，并结合后融合算法和注意机制，准确捕捉了 MRI 图像中的重要特征，使诊断准确性得到提高。 |
| [^38] | [FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design.](http://arxiv.org/abs/2401.14112) | FP6-LLM提出了一种支持六位量化的GPU算法-系统协同设计方案，实现了在大型语言模型中推断成本和模型质量之间的平衡。 |
| [^39] | [Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators.](http://arxiv.org/abs/2401.14110) | 本论文提出了一种简单的方法，可以训练和微调高端DNN，实现使用更便宜的12位累加器，而不会导致精度降低，并展示了使用细粒度梯度近似可以提高DNN的精度。 |
| [^40] | [CompactifAI: Extreme Compression of Large Language Models using Quantum-Inspired Tensor Networks.](http://arxiv.org/abs/2401.14109) | CompactifAI是一种使用量子启发的张量网络对大型语言模型进行极压缩的创新方法，相比于传统的压缩方法，它更注重模型的相关空间，实现更加可控和精细的压缩。 |
| [^41] | [Learning under Label Noise through Few-Shot Human-in-the-Loop Refinement.](http://arxiv.org/abs/2401.14107) | 本文提出了一种名为少样本人机协作改进（FHLR）的方法，用于解决可穿戴数据中的噪声标签学习问题。该方法通过使用弱标签学习种子模型，使用少量的专家修正对种子模型进行微调，并通过加权参数平均将种子模型和微调模型合并，以提高泛化能力和鲁棒性。 |
| [^42] | [McUDI: Model-Centric Unsupervised Degradation Indicator for Failure Prediction AIOps Solutions.](http://arxiv.org/abs/2401.14093) | 本文提出了一种模型中心的无监督退化指示器McUDI，用于检测AIOps模型由于数据变化而需要重新训练的时机，并展示了在AIOps解决方案中应用McUDI可以减少标记样本数量的方法。 |
| [^43] | [A Modular Approach to Automatic Cyber Threat Attribution using Opinion Pools.](http://arxiv.org/abs/2401.14090) | 本论文提出了一种模块化架构，利用舆情池来自动化网络威胁归因过程。同时，还提出了一种匹配聚合器作为聚合方法，以增加可追踪性和解释性。 |
| [^44] | [Generating Likely Counterfactuals Using Sum-Product Networks.](http://arxiv.org/abs/2401.14086) | 由于用户需求和最近的法规要求，需要对AI系统所做出的决策进行解释。本论文提出了一种使用Sum-Product Networks模拟寻找高可能性反事实推理的系统，该系统能够提供满足多个常见要求的最佳解释。 |
| [^45] | [Accelerating Fractional PINNs using Operational Matrices of Derivative.](http://arxiv.org/abs/2401.14081) | 本文提出了一种操作矩阵方法来加速分数PINNs的训练，通过非均匀离散化分数Caputo算子，实现了对分数导数的快速计算。使用Legendre神经块（LNB）架构提高了PINNs的准确性。 |
| [^46] | [ProCNS: Progressive Prototype Calibration and Noise Suppression for Weakly-Supervised Medical Image Segmentation.](http://arxiv.org/abs/2401.14074) | ProCNS是一种用于弱监督医学图像分割的新方法，采用渐进式原型校准和噪声抑制的原则来解决现有方法中的问题。 |
| [^47] | [Neural Sinkhorn Gradient Flow.](http://arxiv.org/abs/2401.14069) | 神经Sinkhorn梯度流模型通过参数化Wasserstein梯度流中的速度场，并利用速度场匹配训练方案实现高效的推理过程。其理论分析表明，随着样本大小趋近于无穷大，经验近似值的均场极限会收敛到真实的底层速度场。 |
| [^48] | [Novel application of Relief Algorithm in cascaded artificial neural network to predict wind speed for wind power resource assessment in India.](http://arxiv.org/abs/2401.14065) | 该研究采用了新颖的Relief算法在级联人工神经网络中进行风速预测，相较于传统模型，该方法具有更高的准确性，该方法的关键在于选择最相关的输入参数。 |
| [^49] | [Left/Right Brain, human motor control and the implications for robotics.](http://arxiv.org/abs/2401.14057) | 本研究通过训练不同的损失函数，实现了类似于人类的左右半球专门化控制系统，该系统在不同的运动任务中展现出协调性、运动效率和位置稳定性的优势。 |
| [^50] | [Novel Quadratic Constraints for Extending LipSDP beyond Slope-Restricted Activations.](http://arxiv.org/abs/2401.14033) | 本文提出了新的二次约束，将LipSDP方法扩展到斜率限制之外的激活函数，从而解决了该方法的主要限制问题。 |
| [^51] | [Sparse and Transferable Universal Singular Vectors Attack.](http://arxiv.org/abs/2401.14031) | 本文提出了一种稀疏可传递的通用奇异向量攻击方法，通过在隐藏层中利用截断幂迭代来获得稀疏的$(p,q)$-奇异向量。在ImageNet基准验证子集上的实验证明，该方法可以在仅破坏5%的像素和利用256个样本的情况下，达到与密集基线相当的愚弄率超过50%的结果。同时，该攻击方法不影响人类解决任务的能力，并且构造的扰动是高度可传递的。 |
| [^52] | [Towards a Systems Theory of Algorithms.](http://arxiv.org/abs/2401.14029) | 向算法的系统理论迈进，将算法视为开放的动态系统与其他算法、物理系统、人类或数据库交互，提供宝贵的洞见。 |
| [^53] | [The Risk of Federated Learning to Skew Fine-Tuning Features and Underperform Out-of-Distribution Robustness.](http://arxiv.org/abs/2401.14027) | 联邦学习存在扭曲微调特征和损害模型抗分布鲁棒性的风险，为此我们引入了GNP算法来保证模型在目标分布上的准确性不会下降。 |
| [^54] | [DNA Sequence Classification with Compressors.](http://arxiv.org/abs/2401.14025) | 该论文介绍了一种专门用于DNA序列分析的新型分类方法，该方法利用压缩算法高效处理和分类基因组序列。这种方法不仅准确性高，而且比传统方法更节省计算资源。 |
| [^55] | [Accelerating Retrieval-Augmented Language Model Serving with Speculation.](http://arxiv.org/abs/2401.14021) | 提出了RaLMSpec，这是一个使用推测加速检索增强型语言模型服务的框架，通过推测式检索和批量验证提供了通用的加速效果，并通过进一步优化和并发处理，提高了性能。 |
| [^56] | [Cross-Domain Few-Shot Learning via Adaptive Transformer Networks.](http://arxiv.org/abs/2401.13987) | 本文提出了一种名为ADAPTER的自适应变压器网络，用于跨领域的少样本学习。借助双向交叉注意力和标签平滑方法，ADAPTER能够在存在领域转移的情况下学习可转移的特征，并避免监督崩溃问题。该方法在BSCD-FSL基准测试中表现优秀，超过了之前的方法。 |
| [^57] | [Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning.](http://arxiv.org/abs/2401.13986) | 本文提出了一种通过解释一致性微调方法，使得大型语言模型（LLMs）在相关示例上生成更一致的自然语言解释。实验证明，该方法在不同领域的问答数据集上相对提高了10.0%的解释一致性，并且能够泛化到其他数据集。 |
| [^58] | [Leeroo Orchestrator: Elevating LLMs Performance Through Model Integration.](http://arxiv.org/abs/2401.13979) | 本研究提出了Leeroo编排器的架构，通过集成多个训练过的LLMs模型，实现了一个新的最先进模型。该编排器在性能上与Mixtral模型相当，并且成本只有其三分之二。当允许更高的成本时，Leeroo编排器的准确性超过了Mixtral模型，并且当集成GPT4时进一步提升。 |
| [^59] | [Evaluating the Determinants of Mode Choice Using Statistical and Machine Learning Techniques in the Indian Megacity of Bengaluru.](http://arxiv.org/abs/2401.13977) | 该研究运用统计学和机器学习技术，调查印度城市班加罗尔的低收入和低中产阶级家庭在交通方式选择方面的决策行为，并发现随机森林模型在预测准确性方面表现最佳。 |
| [^60] | [Stochastic Weakly Convex Optimization Beyond Lipschitz Continuity.](http://arxiv.org/abs/2401.13971) | 本文研究了没有标准Lipschitz连续性假设的随机弱凸优化问题，并提出了新的自适应正则化策略。结果表明，一类广泛的随机算法具有$\mathcal{O} ( 1 / \sqrt{K})$的收敛速度和常数的失败率。 |
| [^61] | [Dynamic Long-Term Time-Series Forecasting via Meta Transformer Networks.](http://arxiv.org/abs/2401.13968) | 本文提出了一种名为Meta-Transformer Networks（MANTRA）的模型，用于动态长期时间序列预测。该模型通过快速和慢速学习器的结合，以及使用通用表示转换层，实现对动态环境的快速适应，并在实验中表现出优于基准算法的性能提升。 |
| [^62] | [Networked Multiagent Reinforcement Learning for Peer-to-Peer Energy Trading.](http://arxiv.org/abs/2401.13947) | 本文提出了一个利用多智能体强化学习框架来实现点对点能源交易的方法，该方法帮助自动化消费者的竞标和管理，并解决了可再生能源零边际成本和物理约束的问题。 |
| [^63] | [Reinforcement Learning with Hidden Markov Models for Discovering Decision-Making Dynamics.](http://arxiv.org/abs/2401.13929) | 本论文针对重性抑郁障碍(MDD)中的奖励处理异常，使用强化学习模型和隐马尔可夫模型结合，探索决策制定过程中的学习策略动态对个体奖励学习能力的影响。 |
| [^64] | [Towards 3D Molecule-Text Interpretation in Language Models.](http://arxiv.org/abs/2401.13923) | 提出了一个名为3D-MoLM的模型，通过给语言模型配备一个3D分子编码器，实现了对3D分子-文本的解释和分析，此模型在下游任务上显著优于现有基线。 |
| [^65] | [LocMoE: A Low-overhead MoE for Large Language Model Training.](http://arxiv.org/abs/2401.13920) | LocMoE提出了一种新的路由策略，通过将部分节点间通信转换为节点内通信，结合负载平衡和局部性，以提高大型语言模型训练的性能。 |
| [^66] | [Spectral Clustering for Discrete Distributions.](http://arxiv.org/abs/2401.13913) | 本文提出了一种基于谱聚类和分布相似度度量的框架来解决离散分布聚类问题。通过使用线性最优传输构建相似度矩阵，我们在聚类准确性和计算效率方面取得了显著的改进。 |
| [^67] | [A Survey of Deep Learning and Foundation Models for Time Series Forecasting.](http://arxiv.org/abs/2401.13912) | 深度学习在时间序列预测中显示出显著优势，但在疫情预测方面仍面临挑战，基础模型的开发可以帮助解决这些问题。 |
| [^68] | [Empowering Machines to Think Like Chemists: Unveiling Molecular Structure-Polarity Relationships with Hierarchical Symbolic Regression.](http://arxiv.org/abs/2401.13904) | 本论文介绍了一种无监督的层次符号回归方法(UHiSR)，通过结合层次神经网络和符号回归，实现了自动提取化学直观的极性指数，并发现了将分子结构与色谱行为联系起来的可解释方程。 |
| [^69] | [Cross-Modal Prototype based Multimodal Federated Learning under Severely Missing Modality.](http://arxiv.org/abs/2401.13898) | 提出了一种适用于严重缺失模态的多模态联邦学习方法MFCPL，通过完整的原型提供多样的模态知识，解决了数据异质性和缺失模态带来的稳健性问题。 |
| [^70] | [A comparative study of zero-shot inference with large language models and supervised modeling in breast cancer pathology classification.](http://arxiv.org/abs/2401.13887) | 本研究比较了大型语言模型与监督建模在乳腺癌病理分类上的零样本推断能力，发现GPT-4模型在所有任务中要么明显优于，要么与最佳的监督模型LSTM-Att模型相当。 |
| [^71] | [Constant Stepsize Q-learning: Distributional Convergence, Bias and Extrapolation.](http://arxiv.org/abs/2401.13884) | 本文研究了常数步长异步Q-learning算法，并通过分析其与马尔可夫链的关联，展示了其迭代在分布上的收敛性和指数收敛速度。同时，研究者还建立了该算法迭代的中心极限定理，并对其渐近偏差进行了精确的展开。通过这些分析，我们可以深入理解这种算法的优化效果和偏差特性。 |
| [^72] | [Is Temperature Sample Efficient for Softmax Gaussian Mixture of Experts?.](http://arxiv.org/abs/2401.13875) | 本文研究了温度对Softmax高斯混合专家的采样效率的影响，证明了由于温度和其他模型参数之间的相互作用，参数估计的收敛速度较慢，并且可能很慢。 |
| [^73] | [Edge Conditional Node Update Graph Neural Network for Multi-variate Time Series Anomaly Detection.](http://arxiv.org/abs/2401.13872) | 本文介绍了一种边缘条件节点更新的图神经网络（ECNU-GNN），用于多变量时间序列异常检测。该模型根据连接的边动态转换源节点表示以恰当地表示目标节点，在实际数据集上表现出了显著的性能提升。 |
| [^74] | [Inverse Molecular Design with Multi-Conditional Diffusion Guidance.](http://arxiv.org/abs/2401.13858) | 借助多条件扩散引导的逆分子设计模型在材料和药物发现方面具有巨大潜力。通过引入Transformer-based去噪模型和图依赖的扩散过程，该模型能够在多个条件约束下准确地生成聚合物和小分子。 |
| [^75] | [Embedding Attack Project (Work Report).](http://arxiv.org/abs/2401.13854) | Embedding Attack Project的工作报告总结了嵌入式攻击项目的MIA实验，发现了以下六个结论：（1）过拟合程度与模型的易受攻击性成正比；（2）模型中的早期嵌入层更不容易受到隐私泄漏的影响；（3）更深层的模型层包含更多的成员信息；（4）如果嵌入和对应的训练标签都被泄露，模型对MIA更脆弱；（5）可以使用伪标签来增加MIA的成功率；（6）尽管MIA和PIA的成功率存在差异 |
| [^76] | [Scaling NVIDIA's multi-speaker multi-lingual TTS systems with voice cloning to Indic Languages.](http://arxiv.org/abs/2401.13851) | 本文介绍了NVIDIA开发的TTS模型，利用RAD-MMM和P-Flow实现了多语言TTS的训练，其中P-Flow在零样本TTS方面表现出色，在2024挑战中获得了第一名。 |
| [^77] | [A V2X-based Privacy Preserving Federated Measuring and Learning System.](http://arxiv.org/abs/2401.13848) | 本文介绍了一种基于V2X的隐私保护联合测量和学习系统，通过V2V通信向其他车辆提供实时数据，同时通过V2N链路上的FL方案创建交通网络的预测模型。 |
| [^78] | [Enumerating the k-fold configurations in multi-class classification problems.](http://arxiv.org/abs/2401.13843) | 本研究介绍了一种在多类分类问题中枚举k折配置的算法，用来解决基于k折交叉验证的性能得分不可复现的问题。 |
| [^79] | [Machine learning for industrial sensing and control: A survey and practical perspective.](http://arxiv.org/abs/2401.13836) | 本文调查了机器学习在工业传感和控制中的应用，确定了在过程工业中实际成功的关键技术，并讨论了软传感、过程优化和控制等核心应用领域的需求和挑战。 |
| [^80] | [The Calibration Gap between Model and Human Confidence in Large Language Models.](http://arxiv.org/abs/2401.13835) | 该论文研究了大型语言模型在传达置信度方面模型和人类之间存在的差距，并发现默认解释会导致用户过高估计模型置信度和准确性。 |
| [^81] | [Traffic Learning and Proactive UAV Trajectory Planning for Data Uplink in Markovian IoT Models.](http://arxiv.org/abs/2401.13827) | 本文提出了一个新的学习框架，用于估计基于马尔可夫事件的物联网设备的流量情况，并优化多个无人机的轨迹和调度策略，以降低信息时代、节省能量和提高吞吐量。 |
| [^82] | [Navigating Dataset Documentations in AI: A Large-Scale Analysis of Dataset Cards on Hugging Face.](http://arxiv.org/abs/2401.13822) | 本研究通过分析Hugging Face上的数据集文档，提供了Hugging Face数据集生态系统的概览并洞察到数据集文档实践的关键问题。研究发现，数据集卡片完成率与数据集受欢迎程度存在显著异质性，从业者在数据集描述和数据集结构部分更为关注，而对于使用数据的考虑部分较为忽视。 |
| [^83] | [Investigating the Efficacy of Large Language Models for Code Clone Detection.](http://arxiv.org/abs/2401.13802) | 这项研究探索了大型语言模型在代码克隆检测任务中的应用。 |
| [^84] | [Don't Push the Button! Exploring Data Leakage Risks in Machine Learning and Transfer Learning.](http://arxiv.org/abs/2401.13796) | 本文讨论了机器学习中的数据泄露问题，即未预期的信息污染训练数据，影响模型性能评估，用户可能由于缺乏理解而忽视关键步骤，导致乐观的性能估计在实际场景中不成立。 |
| [^85] | [Traffic Pattern Classification in Smart Cities Using Deep Recurrent Neural Network.](http://arxiv.org/abs/2401.13794) | 本论文使用深度递归神经网络对智能城市中的交通模式进行了分类，提出了一种能够有效捕捉交通模式动态和序列特征的新方法，并在实验中表现出超过现有方法的准确率、精确率、召回率和F1分数。研究结果对智能城市具有重要意义。 |
| [^86] | [Tweets to Citations: Unveiling the Impact of Social Media Influencers on AI Research Visibility.](http://arxiv.org/abs/2401.13782) | 本文研究了社交媒体影响者在提高机器学习研究的可见性方面的作用，发现被这些影响者认可的论文引用次数显著增加，中位数引用次数比对照组高2-3倍。此外，该研究还探讨了被展示作者的地理、性别和机构多样性。 |
| [^87] | [Faster Convergence with Less Communication: Broadcast-Based Subgraph Sampling for Decentralized Learning over Wireless Networks.](http://arxiv.org/abs/2401.13779) | 本文提出了一种名为BASS的基于广播的子图采样方法，用于加速去中心化学习算法的收敛速度，并减少通信成本。 |
| [^88] | [Multiview Graph Learning with Consensus Graph.](http://arxiv.org/abs/2401.13769) | 这篇论文提出了一种用于多视图图学习的方法，通过一致图来确保视图之间的相似性和共享结构，解决了现有方法无法推断出视图共享结构的问题。 |
| [^89] | [NLICE: Synthetic Medical Record Generation for Effective Primary Healthcare Differential Diagnosis.](http://arxiv.org/abs/2401.13756) | 本文提出了一种针对初级医疗护理鉴别诊断的合成病历生成方法，通过使用医学知识和机器学习模型，构建了具有上下文信息的患者记录，并展示了使用这些数据训练疾病模型的有效性结果。 |
| [^90] | [A Systematic Approach to Robustness Modelling for Deep Convolutional Neural Networks.](http://arxiv.org/abs/2401.13751) | 本论文提出一种系统化方法，用于针对深度卷积神经网络进行鲁棒性建模。研究发现隐藏层数量对模型的推广性能有影响，同时还测试了模型大小、浮点精度、训练数据和模型输出的噪声水平等参数。为了改进模型的预测能力和计算成本，提出了一种使用诱发故障来建模故障概率的方法。 |
| [^91] | [Conformal Prediction Sets Improve Human Decision Making.](http://arxiv.org/abs/2401.13744) | 该研究表明，通过规范预测量化模型的不确定性，可以提高人类决策的准确性和效果，对人机协同决策具有实用价值。 |
| [^92] | [Supporting Sensemaking of Large Language Model Outputs at Scale.](http://arxiv.org/abs/2401.13726) | 该论文研究了如何同时展示大规模语言模型的多个回应，并设计了五个功能来支持用户在不同任务中进行理解。通过用户研究和案例研究，该研究发现这些功能能够支持各种理解任务，甚至解决了以前被认为过于困难的任务。 |
| [^93] | [Uncertainty-Guided Alignment for Unsupervised Domain Adaptation in Regression.](http://arxiv.org/abs/2401.13721) | 该论文提出了一种利用不确定性引导的无监督领域自适应回归方法，通过将不确定性作为置信度估计和嵌入空间的正则项来实现对齐。 |
| [^94] | [Inference Attacks Against Face Recognition Model without Classification Layers.](http://arxiv.org/abs/2401.13719) | 提出了一种针对没有分类层的实际人脸识别模型的新型推理攻击，通过分析中间特征与批归一化参数之间的距离，设计了一个简单但有效的攻击模型，可以确定一个人脸图像是否属于模型的成员。 |
| [^95] | [Can I trust my fake data -- A comprehensive quality assessment framework for synthetic tabular data in healthcare.](http://arxiv.org/abs/2401.13716) | 这项研究开发了一个综合质量评估框架，用于评估医疗领域中合成表格数据的质量，并解决了现有框架中存在的一些问题和限制。 |
| [^96] | [Value-Driven Mixed-Precision Quantization for Patch-Based Inference on Microcontrollers.](http://arxiv.org/abs/2401.13714) | 本文提出了一种名为QuantMCU的基于价值驱动的混合精度量化方法，用于在微控制器上进行基于块的推理。通过利用价值驱动块分类(VDPC)将块分为两类，并对包含异常值的块进行8位量化，QuantMCU能够减少冗余计算，从而提高模型执行效率。 |
| [^97] | [EMP: Effective Multidimensional Persistence for Graph Representation Learning.](http://arxiv.org/abs/2401.13713) | 本论文提出了EMP框架，通过同时变化多个尺度参数来探索数据，并将描述符函数整合到分析过程中，从而提供一个高度表达的数据摘要。 |
| [^98] | [Accelerating hyperbolic t-SNE.](http://arxiv.org/abs/2401.13708) | 本文提出了加速双曲线t-SNE算法，通过引入极坐标四叉树的加速结构，解决了现有方法在处理大规模输入数据时的效率问题。 |
| [^99] | [Generative AI-Driven Human Digital Twin in IoT-Healthcare: A Comprehensive Survey.](http://arxiv.org/abs/2401.13699) | 该论文调查了在物联网健康护理中利用生成式人工智能驱动的人类数字孪生的应用。人类数字孪生作为多功能、生动的人类数字测试平台，可以模拟结果并指导实际治疗，从而提高物联网健康护理的能力。 |
| [^100] | [Inverse analysis of granular flows using differentiable graph neural network simulator.](http://arxiv.org/abs/2401.13695) | 通过使用可微分图神经网络模拟器进行反演分析，解决了传统模拟器计算开销大、不可微等问题，提高了计算效率和准确性。 |
| [^101] | [Process Mining for Unstructured Data: Challenges and Research Directions.](http://arxiv.org/abs/2401.13677) | 这篇论文讨论了将过程挖掘应用于非结构化数据所面临的挑战，并提出了初步解决方案和未来的研究方向。 |
| [^102] | [Determinants of renewable energy consumption in Madagascar: Evidence from feature selection algorithms.](http://arxiv.org/abs/2401.13671) | 这项研究使用特征选择算法识别了马达加斯加可再生能源消费的影响因素，包括经济、金融、社会和环境方面的因素，并通过多种机器学习算法进行了评估。 |
| [^103] | [Inadequacy of common stochastic neural networks for reliable clinical decision support.](http://arxiv.org/abs/2401.13657) | 本研究调查了随机神经网络在临床应用中的可靠性，并发现常见的深度学习方法在数据转移情况下过于自信。这强调了对本地不确定性可靠估计及其向最终用户传达的重要性。 |
| [^104] | [Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity Detectors.](http://arxiv.org/abs/2401.13652) | 本文提出了一种利用图信息神经网络和稀疏网格来检测不连续函数不连续界面的新方法，该方法在维度大于3的情况下表现出高效且准确的不连续性检测能力，在维度n = 2和n = 4的函数上进行的实验验证了其高效性和泛化能力，并具有可移植性和多功能性。 |
| [^105] | [Masked Particle Modeling on Sets: Towards Self-Supervised High Energy Physics Foundation Models.](http://arxiv.org/abs/2401.13537) | 本文提出了一种称为遮蔽粒子建模（MPM）的自监督方法，用于学习高能物理科学数据中无序输入的通用表示。该方法通过预训练学习置换不变的函数，在构建适用于多种任务的高能物理基础模型方面具有潜力。 |
| [^106] | [Finetuning Foundation Models for Joint Analysis Optimization.](http://arxiv.org/abs/2401.13536) | 本论文中展示了在高能物理学中，通过超越顺序优化或重建和分析组件的标准范 paradigm，可以实现性能和数据效率的显著提升。通过搜索通过中间 di-Higgs 系统衰变的重共振体为四个 $b$-喷注的示例用例，我们将高能物理学重建和分析与现代机器学习工作流进行了连接，并量化了其收益。 |
| [^107] | [Towards Understanding the Riemannian SGD and SVRG Flows on Wasserstein Probabilistic Space.](http://arxiv.org/abs/2401.13530) | 本文研究了在Wasserstein概率空间上的Riemannian SGD和SVRG流的优化方法，通过构建随机微分方程来丰富Wasserstein空间中的连续优化方法。 |
| [^108] | [Debiased Sample Selection for Combating Noisy Labels.](http://arxiv.org/abs/2401.13360) | 本文提出了一个无噪声专家模型（ITEM）来解决样本选择中的训练偏差和数据偏差问题。通过设计一个鲁棒的网络架构来集成多个专家，可以减少选择集不平衡和累积错误，并在使用更少参数的情况下实现更好的选择和预测性能。 |
| [^109] | [DittoGym: Learning to Control Soft Shape-Shifting Robots.](http://arxiv.org/abs/2401.13231) | 这篇论文介绍了一种学习控制软形变机器人的方法，并且提出了一个全面的强化学习基准系统DittoGym，该系统需要对机器人的形态进行细粒度变化来完成任务。 |
| [^110] | [Binary structured physics-informed neural networks for solving equations with rapidly changing solutions.](http://arxiv.org/abs/2401.12806) | 本论文提出了一种二进制结构的物理信息神经网络框架，通过利用二进制结构来捕捉局部特征，并解决了传统物理信息神经网络在处理具有快速变化解的方程时的困难。 |
| [^111] | [Energy-based Automated Model Evaluation.](http://arxiv.org/abs/2401.12689) | 提出了一种基于能量的自动化模型评估方法，通过建立关于个体样本相关信息的元分布统计量，能够更高效和有效地评估机器学习模型的性能，解决了AutoEval框架中的过度自信、存储和计算成本高等问题。 |
| [^112] | [BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language Models.](http://arxiv.org/abs/2401.12522) | BiTA是一种用于大语言模型的创新方法，通过双向调整实现了无损加速。它采用简化的半自回归生成和草稿验证，通过高效的基于树的解码同时进行候选生成和验证，提高了推理效率。这种方法不需要额外的辅助模型或显著的额外内存开销。 |
| [^113] | [Instructional Fingerprinting of Large Language Models.](http://arxiv.org/abs/2401.12255) | 这项研究提出了一种指纹识别大型语言模型的方法，通过轻量级的指令调整，保护知识产权并确保遵守许可条款。实验证明这种方法不影响模型的正常行为，并且具有鲁棒性和高效训练的特点。 |
| [^114] | [The Surprising Harmfulness of Benign Overfitting for Adversarial Robustness.](http://arxiv.org/abs/2401.12236) | 这项研究证明了即使机器学习模型在训练过程中对噪声数据拟合得很好，对敌对示例具有鲁棒性，但当面临敌对操纵的数据时，过度拟合的模型可能会给系统带来意外的危害。 |
| [^115] | [TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for Lazy Clients.](http://arxiv.org/abs/2401.12012) | TurboSVM-FL是一种新颖的联邦聚合策略，通过SVM聚合为懒惰客户端增强联邦学习。这种策略在不增加客户端计算负担的情况下解决了联邦学习中的收敛速度慢的问题。 |
| [^116] | [The Synergy Between Optimal Transport Theory and Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2401.10949) | 本文研究了最优输运理论与多智能体强化学习之间的协同作用。通过利用最优输运来处理分布和运输问题，增强了多智能体强化学习的效率、协调性和适应性。通过在政策对齐、分布式资源管理、应对非平稳性、可扩展的多智能体学习和提高能源效率五个方面应用最优输运理论，为解决可扩展性问题、优化资源分配和在合作环境中对齐智能体策略提供了新的方法。 |
| [^117] | [AI in Supply Chain Risk Assessment: A Systematic Literature Review and Bibliometric Analysis.](http://arxiv.org/abs/2401.10895) | 本文通过系统文献综述和文献计量分析，填补了供应链风险评估中新兴人工智能/机器学习技术的研究空白，为了解这些技术在实践中的实际影响提供了关键见解。 |
| [^118] | [Neglected Hessian component explains mysteries in Sharpness regularization.](http://arxiv.org/abs/2401.10809) | 这篇论文研究了在深度学习中，明确或隐含地惩罚二阶信息可以提高泛化性能，而权重噪声和梯度惩罚则很少能带来这样的好处。作者通过对损失的黑塞矩阵的结构进行解释，提出了特征的开发和特征的探索之间的量化分离。同时，作者发现忽视的非线性建模误差矩阵 (NME) 实际上很重要，可以解释为什么梯度惩罚对激活函数的选择非常敏感。此外，作者通过设计干预措施来改进性能，并提供了证据挑战了以往的观点，认为权重噪声和梯度惩罚是等效的。 |
| [^119] | [Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences.](http://arxiv.org/abs/2401.10529) | Mementos是一个新的基准测试，旨在评估多模态大型语言模型在图像序列推理中的能力。研究发现，现有的MLLM在准确描述图像序列的动态信息方面存在困难，容易导致物体及其行为的错误描述或错觉。 |
| [^120] | [Exploration and Anti-Exploration with Distributional Random Network Distillation.](http://arxiv.org/abs/2401.09750) | 该论文提出了一种新的深度强化学习探索算法，称为分布式随机网络蒸馏（DRND）。该算法通过蒸馏随机网络的分布和隐式融入伪计数来改进奖励分配的精度，从而增强了探索过程。理论分析和实验结果均表明该方法的优越性。 |
| [^121] | [SymTC: A Symbiotic Transformer-CNN Net for Instance Segmentation of Lumbar Spine MRI.](http://arxiv.org/abs/2401.09627) | SymTC是一种新颖的腰椎MR图像分割模型，通过将Transformer和CNN相结合，并利用位置嵌入和自注意力模块，实现了更准确的实例分割。 |
| [^122] | [cedar: Composable and Optimized Machine Learning Input Data Pipelines.](http://arxiv.org/abs/2401.08895) | cedar是一个编程模型和框架，可以轻松构建、优化和执行机器学习输入数据管道。它提供了易于使用的编程接口和可组合运算符，支持任意ML框架和库。通过解决当前输入数据系统无法充分利用性能优化的问题，cedar提高了资源利用效率，满足了庞大数据量和高训练吞吐量的需求。 |
| [^123] | [Shabari: Delayed Decision-Making for Faster and Efficient Serverless Function.](http://arxiv.org/abs/2401.08859) | Shabari是一个延迟决策的无服务器资源管理框架，通过对函数输入的延迟来减轻无服务器系统中的性能变异性和资源低利用率问题。 |
| [^124] | [Decoupled Prototype Learning for Reliable Test-Time Adaptation.](http://arxiv.org/abs/2401.08703) | 提出了一种名为Decoupled Prototype Learning (DPL)的解耦样本学习方法，通过使用样本原型为中心的损失计算，来解决测试时自适应中噪声伪标签的问题。 |
| [^125] | [SAiD: Speech-driven Blendshape Facial Animation with Diffusion.](http://arxiv.org/abs/2401.08655) | 提出了一种使用扩散模型（SAiD）驱动的语音驱动的三维面部动画方法，通过轻量级的Transformer-based U-Net模型和音频与视觉的交叉模态对齐偏差，实现了较好的唇部同步和更多样化的唇部运动。 |
| [^126] | [DiConStruct: Causal Concept-based Explanations through Black-Box Distillation.](http://arxiv.org/abs/2401.08534) | DiConStruct是一种基于黑盒模型的因果概念解释方法，通过创建结构性因果模型和概念归因方式提供更具可解释性的局部解释。 |
| [^127] | [Contrastive Perplexity for Controlled Generation: An Application in Detoxifying Large Language Models.](http://arxiv.org/abs/2401.08491) | 这项研究研究了对比学习目标的集成到微调大型语言模型中，以解决其产生不可取内容的问题，并展示了在清洁领域中有效减少有害内容生成的方法。 |
| [^128] | [Can Probabilistic Feedback Drive User Impacts in Online Platforms?.](http://arxiv.org/abs/2401.05304) | 这项工作探讨了内容推荐系统可能对用户产生的负面影响，并指出这种影响不仅可能由平台目标与用户福利不一致引起，还可能由学习算法对不同内容的反馈率差异造成，提出了使用多臂赌博机框架和概率反馈的解决方法。 |
| [^129] | [Convergence Rate Maximization for Split Learning-based Control of EMG Prosthetic Devices.](http://arxiv.org/abs/2401.03233) | 本文介绍了一种用于最大化模型收敛速率的Split Learning肌电假肢控制中的切层选择算法，通过加速收敛过程提高了假肢控制的性能。 |
| [^130] | [Harmonizing Covariance and Expressiveness for Deep Hamiltonian Regression in Crystalline Material Research: a Hybrid Cascaded Regression Framework.](http://arxiv.org/abs/2401.00744) | 在深度哈密顿回归中，实现协方差和网络表达能力之间的平衡一直是一个挑战。本文提出了一个混合级联回归框架，在第一阶段通过协变神经网络建模并产生协变特征和基线预测，辅助第二阶段学习协方差。同时，第二阶段使用非线性图形Transformer网络进行结构建模，提高了哈密顿预测的表达能力。 |
| [^131] | [Robust Neural Pruning with Gradient Sampling Optimization for Residual Neural Networks.](http://arxiv.org/abs/2312.16020) | 本研究通过在剪枝过程中应用梯度采样技术，实现了在资源有限的场景中保持高准确性水平的鲁棒剪枝。相关实验证明，采用梯度采样技术进行优化的模型比传统优化方法更能有效地保持准确性。这一创新方法在各种数据集和神经架构上得到了验证，并且理论上解释了梯度采样技术对模型鲁棒性的贡献。 |
| [^132] | [An Adaptive Placement and Parallelism Framework for Accelerating RLHF Training.](http://arxiv.org/abs/2312.11819) | 提出了一种自适应模型部署和并行框架，用于加速RLHF训练。该框架提供了两种灵活的模型部署策略，其中交替策略有助于减少内存冗余和通信成本。 |
| [^133] | [A Survey of Reasoning with Foundation Models.](http://arxiv.org/abs/2312.11562) | 本文调查了使用基础模型进行推理的研究，介绍了最新的推理任务、方法和基准，并讨论了基础模型中推理能力的未来发展方向。 |
| [^134] | [TrojFST: Embedding Trojans in Few-shot Prompt Tuning.](http://arxiv.org/abs/2312.10467) | TrojFST是一种在少样本提示调优框架中进行后门攻击的方法，通过引入平衡的污染学习、选择性令牌污染和...等模块来解决构建基于提示的后门的困难。 |
| [^135] | [Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs.](http://arxiv.org/abs/2312.05934) | 该研究比较了无监督的微调和检索增强生成（RAG）这两种常见方法在LLMs中的应用。结果发现，RAG在现有知识和新知识上表现出更好的性能，而LLMs通过无监督的微调学习新的事实信息较困难。 |
| [^136] | [Semi-Supervised Active Learning for Semantic Segmentation in Unknown Environments Using Informative Path Planning.](http://arxiv.org/abs/2312.04402) | 提出了一种基于信息路径规划的半监督主动学习方法，用于解决在未知环境中进行语义分割的问题，通过减少人工标注量，提高了机器人的感知能力。 |
| [^137] | [On the Nystrom Approximation for Preconditioning in Kernel Machines.](http://arxiv.org/abs/2312.03311) | 本文分析了核机器预处理中使用Nystrom逼近的权衡。研究表明，使用对数大小的样本能够让Nystrom逼近的预处理器几乎与梯度下降同样有效地加速。 |
| [^138] | [Can LLMs Patch Security Issues?.](http://arxiv.org/abs/2312.00024) | 本文提出了一种新的方法, Feedback-Driven Solution Synthesis (FDSS), 旨在通过将LLMs与静态代码分析工具Bandit结合，解决代码中的安全漏洞问题。该方法在现有方法的基础上有显著改进，并引入了一个新的数据集PythonSecurityEval。 |
| [^139] | [Short vs. Long-term Coordination of Drones: When Distributed Optimization Meets Deep Reinforcement Learning.](http://arxiv.org/abs/2311.09852) | 这项研究介绍了一种将短期计划生成和选择与分布式优化以及深度强化学习相结合的渐进方法，用于无人机的协调和规划。实验结果表明，与最先进的方法相比，该方法在动态环境中具有出色的性能。 |
| [^140] | [2D-RC: Two-Dimensional Neural Network Approach for OTFS Symbol Detection.](http://arxiv.org/abs/2311.08543) | 这篇论文介绍了一种新颖的二维神经网络方法，用于高运动性场景下无线通信中的OTFS符号检测。该方法将OTFS系统的领域知识与在线子帧符号检测的设计结合起来，并通过引入二维循环填充和滤波结构来实现。 |
| [^141] | [Massive Editing for Large Language Models via Meta Learning.](http://arxiv.org/abs/2311.04661) | 本论文提出了一种通过元学习实现大规模语言模型的大规模编辑的方法。该方法利用超网络来生成参数变化，通过解决最小二乘问题来更新语言模型的参数。通过将计算分离在超网络和语言模型之间，使得可以同时编辑多个事实。该方法在不同架构的语言模型上进行了评估。 |
| [^142] | [Leveraging sinusoidal representation networks to predict fMRI signals from EEG.](http://arxiv.org/abs/2311.04234) | 本论文提出了一种用于预测fMRI信号的新方法，通过利用正弦表示网络从EEG中获取fMRI信息，弥补了两者之间空间分辨率和成本的限制。 |
| [^143] | [Trustworthy Edge Machine Learning: A Survey.](http://arxiv.org/abs/2310.17944) | 可信任的边缘机器学习是边缘计算和机器学习的融合，面临各种挑战，本调查总结了对其的定义、属性、框架、技术和解决方案，并强调了在6G网络中的重要性。 |
| [^144] | [Benchmarking the Sim-to-Real Gap in Cloth Manipulation.](http://arxiv.org/abs/2310.09543) | 该论文提出了一个基准数据集，用于评估布料操控中的模拟与现实差距。通过进行动态和准静态的布料操控任务以及与刚性桌子的接触来收集数据。结果评估了四个流行的可变形物体模拟器的现实差距、计算时间和模拟稳定性，并讨论了每个模拟器的优缺点。 |
| [^145] | [A Transfer-Learning-Based Prognosis Prediction Paradigm that Bridges Data Distribution Shift across EMR Datasets.](http://arxiv.org/abs/2310.07799) | 本研究提出了一种基于迁移学习的EMR数据集之间数据分布变化的预后预测模型，通过构建过渡模型解决了不同数据集的特征不匹配问题，提高了深度学习模型的效率。 |
| [^146] | [Boosting Facial Action Unit Detection Through Jointly Learning Facial Landmark Detection and Domain Separation and Reconstruction.](http://arxiv.org/abs/2310.05207) | 本文提出了一种新的面部动作单位（AU）检测框架，通过共享参数和引入多任务学习，在面部标志检测和AU域分离与重建之间实现了更好的性能。实验证明我们方法在野外AU检测方面优于现有方法。 |
| [^147] | [Secure and Effective Data Appraisal for Machine Learning.](http://arxiv.org/abs/2310.02373) | 本文介绍了一种机密的数据选择和评估方法，通过创新的流程和简化的低维度操作来实现，以保护数据和模型的隐私，并在多个Transformer模型和NLP/CV基准测试中进行了评估。 |
| [^148] | [MIML: Multiplex Image Machine Learning for High Precision Cell Classification via Mechanical Traits within Microfluidic Systems.](http://arxiv.org/abs/2309.08421) | 本研究开发了一种新颖的机器学习框架MIML，该框架通过将无标记细胞图像与生物力学属性数据相结合，实现了高精度细胞分类。该方法利用了形态信息，将细胞属性理解得更全面，相较于仅考虑单一数据类型的模型，实现了98.3％的分类精度。该方法已在白细胞和肿瘤细胞分类中得到证明，并具有更广泛的应用潜力。 |
| [^149] | [A Strong and Simple Deep Learning Baseline for BCI MI Decoding.](http://arxiv.org/abs/2309.07159) | 本论文提出了一种强大且简单的深度学习基线EEG-SimpleConv，用于BCI中的运动想象解码。与其他方法相比，EEG-SimpleConv表现至少同样好或更高效，具有强大的知识传递能力，推理时间较低。 |
| [^150] | [Online Infinite-Dimensional Regression: Learning Linear Operators.](http://arxiv.org/abs/2309.06548) | 在这篇论文中，我们研究了在线设置下学习无限维线性算子的问题。我们证明了在一定的条件下，线性算子是可以在线学习的，而在另一些条件下则不可以。我们还证明了在线均一收敛和学习能力之间的分离，并在PAC设置下得到了相同的结果。 |
| [^151] | [Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity.](http://arxiv.org/abs/2309.04160) | 本研究提出了一种利用原型患者表示和特征缺失感知校准的间接插补方法，以缓解电子健康记录数据稀疏性问题，通过获取更密集的嵌入来提高预测模型的有效性。 |
| [^152] | [Temporal Inductive Path Neural Network for Temporal Knowledge Graph Reasoning.](http://arxiv.org/abs/2309.03251) | 本论文提出了一种临时归纳路径神经网络（TiPNN）用于时间知识图的推理，采用实体独立的角度建模历史信息，并通过临时归纳路径提取结构和时间信息。 |
| [^153] | [Towards Generalizable Neural Solvers for Vehicle Routing Problems via Ensemble with Transferrable Local Policy.](http://arxiv.org/abs/2308.14104) | 本文介绍了一种通过结合全局信息和本地特征，实现对车辆路径问题的通用神经求解器的方法，并将其应用于解决现实世界的复杂问题。 |
| [^154] | [Heterogeneous Federated Learning via Personalized Generative Networks.](http://arxiv.org/abs/2308.13265) | 本文通过个性化生成网络实现了异构联邦学习，解决了数据统计异质性的问题，并通过对客户端之间知识传递的方法，提高了全局模型的收敛效果。 |
| [^155] | [Multi-Objective Optimization for Sparse Deep Neural Network Training.](http://arxiv.org/abs/2308.12243) | 这项工作提出了一种用于稀疏深度神经网络训练的多目标优化算法，通过加权Chebyshev标量化和增广Lagrangian方法，解决了同时优化多个任务时的挑战。 |
| [^156] | [Variational Autoencoding of Dental Point Clouds.](http://arxiv.org/abs/2307.10895) | 本论文介绍了一种新颖的点云变分自编码器（VF-Net）用于牙科点云数据的处理，该模型在各种任务中具有显著的性能，包括网格生成、形状完整和表示学习。 |
| [^157] | [Unsupervised Conditional Slot Attention for Object Centric Learning.](http://arxiv.org/abs/2307.09437) | 本文提出了一种无监督的条件槽注意力方法，通过使用概率性槽字典（PSD）实现了专门的槽位绑定和物体层次调节分布，在多个后续任务中展示了其优势。 |
| [^158] | [Dynamic Graph Attention for Anomaly Detection in Heterogeneous Sensor Networks.](http://arxiv.org/abs/2307.03761) | 该论文提出了一种基于图的方法，命名为DyGATAD，用于在异构传感器网络中进行异常检测。该方法利用动态图注意力机制来识别集体异常行为，其中异常行为可能由于系统内部相互关系的变化引起。这在工业物联网监控系统中具有重要的实际应用价值。 |
| [^159] | [Variational quantum regression algorithm with encoded data structure.](http://arxiv.org/abs/2307.03334) | 本文介绍了一个具有编码数据结构的变分量子回归算法，在量子机器学习中具有模型解释性，并能有效地处理互连度较高的量子比特。算法通过压缩编码和数字-模拟门操作，大大提高了在噪声中尺度量子计算机上的运行时间复杂度。 |
| [^160] | [What do self-supervised speech models know about words?.](http://arxiv.org/abs/2307.00162) | 通过对自我监督的语音模型进行分析，发现这些模型在不同层中编码了不同的语言信息，也学习了类似音素的子词单元。与单词相关的信息主要在中间的模型层中，同时一些低级信息在更高的层中也得以保留。 |
| [^161] | [Realistic Synthetic Financial Transactions for Anti-Money Laundering Models.](http://arxiv.org/abs/2306.16424) | 本文提供了一个逼真的合成金融交易数据集生成器和一组合成的反洗钱数据集，以满足训练模型和推进领域发展的需求。 |
| [^162] | [Adversarial Resilience in Sequential Prediction via Abstention.](http://arxiv.org/abs/2306.13119) | 本文提出了一种处理顺序预测的模型，允许在不对对抗性样例进行预测的情况下提高算法抗对抗攻击的能力。 |
| [^163] | [Pure Exploration in Bandits with Linear Constraints.](http://arxiv.org/abs/2306.12774) | 本文提出了两种相对于线性约束下的多臂赌博问题都是渐进最优的算法。这两种算法都试图跟踪基于下界计算和通过对正常锥体边界进行加权投影所获得的最优分配。 |
| [^164] | [GateON: an unsupervised method for large scale continual learning.](http://arxiv.org/abs/2306.01690) | GateON是一种用于大规模连续学习的无监督方法，通过可学习的活动门控和参数相关性的在线估计来防止重要知识被覆盖，同时通过定点神经元的重新激活机制解决了网络饱和的问题。 |
| [^165] | [Successor-Predecessor Intrinsic Exploration.](http://arxiv.org/abs/2305.15277) | 后续前导内在探索是一种基于新颖内在奖励的探索算法，它利用回顾信息并结合前瞻信息，以在强化学习中实现高效且生态学合理的探索行为。 |
| [^166] | [Point2SSM: Learning Morphological Variations of Anatomies from Point Cloud.](http://arxiv.org/abs/2305.14486) | Point2SSM可以直接从点云中构建出解剖统计形态模型，克服了传统方法需要无噪声表面网格或二进制体积，依赖于假设或预定义模板，以及同时优化导致长时间推断新数据的局限性。 |
| [^167] | [Mitigating Label Noise through Data Ambiguation.](http://arxiv.org/abs/2305.13764) | 本文提出了一种通过数据模糊化来缓解标签噪声的方法，即添加额外的、互补的候选标签，利用所谓的超集学习框架构建基于置信阈值的集合值目标。 |
| [^168] | [Democratized Diffusion Language Model.](http://arxiv.org/abs/2305.10818) | 本文提出了一个基于CDCD框架的民主扩散语言模型（DDLM），并通过GLUE基准测试了其知识转移能力，为研究人员提供了DDLM训练和评估流程以及已训练的DDLM模型。 |
| [^169] | [Algorithms for Social Justice: Affirmative Action in Social Networks.](http://arxiv.org/abs/2305.03223) | 本文介绍了一个新的基于谱图理论的链接推荐算法ERA-Link，旨在缓解现有推荐算法带来的信息孤岛和社会成见，实现社交网络平台的社会正义目标。 |
| [^170] | [The effectiveness of MAE pre-pretraining for billion-scale pretraining.](http://arxiv.org/abs/2303.13496) | 本文在计算机视觉领域提出了一种自我监督的MAE技术预前置训练方法，该方法适用于亿级预训练规模，并可显著提高模型收敛性和下游转移性能。 |
| [^171] | [Lipschitz-bounded 1D convolutional neural networks using the Cayley transform and the controllability Gramian.](http://arxiv.org/abs/2303.11835) | 本文提出了一个逐层参数化方法，用于实现内置鲁棒性保证的1D卷积神经网络。该方法基于CNN特征的Lipschitz常数作为鲁棒性度量，并使用Cayley变换和可控性Gram矩来实现CNN的Lipschitz连续性和无约束训练，最后在心律失常数据分类任务中取得了改进的鲁棒性。 |
| [^172] | [Domain Randomization for Robust, Affordable and Effective Closed-loop Control of Soft Robots.](http://arxiv.org/abs/2303.04136) | 本研究首次展示了领域随机化方法如何通过增强软机器人的强化学习策略来解决模型与真实平台之间的领域差距，实现鲁棒、经济和有效的闭环控制。 |
| [^173] | [Rotation Invariant Quantization for Model Compression.](http://arxiv.org/abs/2303.03106) | 本研究提出了一种旋转不变量量化（RIQ）技术，可以在不同层次上实现混合精度量化，用于后训练神经网络模型压缩，并证明了其在压缩方面的优势。在多种模型和任务上进行了严格评估，取得了令人满意的结果。 |
| [^174] | [Correlation Clustering with Active Learning of Pairwise Similarities.](http://arxiv.org/abs/2302.10295) | 本文研究了相关聚类中成对相似性不事先给出的情况，并开发了一个通用的主动学习框架，适应各种相关聚类算法和查询策略，同时具有适应性灵活、噪声鲁棒性等优势。 |
| [^175] | [When Can We Track Significant Preference Shifts in Dueling Bandits?.](http://arxiv.org/abs/2302.06595) | 这个论文研究了具有分布转变的决斗对抗问题，并探讨了设计自适应算法以解决动态遗憾的问题，结果发现取决于底层偏好分布的属性。达到$O(\sqrt{K\tilde{L}T})$的动态遗憾是不可能的；对于$\text{SST} \cap \text{STI}$情况，存在一种算法实现动态遗憾为$O(\sqrt{K\tilde{L}T})$。 |
| [^176] | [A Generalized Surface Loss for Reducing the Hausdorff Distance in Medical Imaging Segmentation.](http://arxiv.org/abs/2302.03868) | 该论文提出了一种名为广义表面损失函数的新方法，用于在医学图像分割中降低Hausdorff距离。该方法考虑了类别不平衡，并具有比现有方法更好的数值特性。该方法对于肿瘤分割等应用非常重要。 |
| [^177] | [RS-Del: Edit Distance Robustness Certificates for Sequence Classifiers via Randomized Deletion.](http://arxiv.org/abs/2302.01757) | 本文提出了一种适应于离散序列分类器的随机删除（RS-Del）平滑机制，提供针对编辑距离受限对抗性的鲁棒性证明。 |
| [^178] | [Private, fair and accurate: Training large-scale, privacy-preserving AI models in medical imaging.](http://arxiv.org/abs/2302.01622) | 本研究评估了隐私保护训练医学影像人工智能模型的准确性和公平性，并与非隐私训练进行了比较。研究结果可为隐私保护技术的广泛应用提供重要参考。 |
| [^179] | [GNN-based Passenger Request Prediction.](http://arxiv.org/abs/2301.02515) | 本文开发了一个基于图神经网络和注意力机制的模型来预测乘客的起点-终点（OD）流。该模型利用各种线性和非线性依赖关系，并捕捉重复模式和上下文数据。通过广泛的仿真实验，结果显示我们的模型性能优于现有基线模型。 |
| [^180] | [Machine Learning Systems are Bloated and Vulnerable.](http://arxiv.org/abs/2212.09437) | 本文研究了机器学习容器中存在的臃肿问题，并开发了MMLB框架进行分析和量化，结果表明在某些情况下，臃肿会占据容器总大小的80％， 显著增加了容器的供应时间，最多增加了370％，且导致漏洞恶化最高达99％。 |
| [^181] | [Transfer Learning for Contextual Multi-armed Bandits.](http://arxiv.org/abs/2211.12612) | 本文研究了上下文多臂赌博机问题中的转移学习，提出了一种新的算法来最小化遗憾，并量化了源领域数据对目标领域学习的贡献。 |
| [^182] | [HyperSound: Generating Implicit Neural Representations of Audio Signals with Hypernetworks.](http://arxiv.org/abs/2211.01839) | 该论文介绍了一种名为HyperSound的方法，利用超网络生成音频信号的隐式神经表示。与其他模型相比，该方法在重构声波方面具有可比较的质量。 |
| [^183] | [Learning Individual Treatment Effects under Heterogeneous Interference in Networks.](http://arxiv.org/abs/2210.14080) | 本文针对网络观测数据中个体治疗效应的估计问题，提出了一种新颖的双加权回归（DWR）算法，通过同时学习注意权重来解决网络干扰和异质干预的挑战。 |
| [^184] | [Bridging Distributional and Risk-sensitive Reinforcement Learning with Provable Regret Bounds.](http://arxiv.org/abs/2210.14051) | 本论文证明了使用分布式强化学习方法可以实现风险敏感强化学习的遗憾保证问题，并提出了两种新颖算法，其遗憾上界与先前方法相匹配。 |
| [^185] | [Convolutional Persistence Transforms.](http://arxiv.org/abs/2208.02107) | 本文介绍了一种称为卷积坚持转换的方法，通过将数据与滤波器进行卷积计算来获取拓扑特征化。通过证明其持久图是一个注入不变量，证明了卷积坚持转换的独特性。该方法具有改善的稳定性和更大的灵活性。 |
| [^186] | [Self-Supervised Training with Autoencoders for Visual Anomaly Detection.](http://arxiv.org/abs/2206.11723) | 本文提出了一种自监督学习方法，通过修改重构误差的方式集中在数据流形上，从而解决深度卷积自编码器在视觉异常检测中容易出现的重构异常信号而导致检测效果不佳的问题。 |
| [^187] | [Risk Measures and Upper Probabilities: Coherence and Stratification.](http://arxiv.org/abs/2206.03183) | 该论文研究了富有决策实际意义的聚合函数类，以构建不确定性处理的数学基础，通过分析谱风险度量、Choquet积分和Lorentz范数的特性，提出了一种对风险度量进行分层的方法，并在机器学习实践中得到了验证。 |
| [^188] | [MCCE: Monte Carlo sampling of realistic counterfactual explanations.](http://arxiv.org/abs/2111.09790) | MCCE是一个新颖的反事实解释方法，通过模拟可变特征和决策的联合分布，生成处于流形上、可行并且有效的反事实。与其他方法相比，MCCE可以处理任何类型的预测模型和具有多个级别的分类特征。 |
| [^189] | [EvadeDroid: A Practical Evasion Attack on Machine Learning for Black-box Android Malware Detection.](http://arxiv.org/abs/2110.03301) | 本论文提出了一种可实际使用并可以有效逃避黑盒Android恶意软件检测器的对抗攻击——EvadeDroid，并且可以保留原始恶意软件应用程序的功能。 |
| [^190] | [Derivative-free Alternating Projection Algorithms for General Nonconvex-Concave Minimax Problems.](http://arxiv.org/abs/2108.00473) | 本文提出针对非凸-凹极小极大问题的无导数交替投影算法，包括光滑问题的交替随机梯度投影算法（ZO-AGP），以及块状非光滑问题的分块交替随机近端梯度算法（ZO-BAPG）。这些算法具有较少的函数值估计和较高的迭代复杂度。 |
| [^191] | [A Link between Coding Theory and Cross-Validation with Applications.](http://arxiv.org/abs/2103.11856) | 本研究研究了编码理论与交叉验证之间的联系，并发现了学习算法在固定数据上能解决不同二进制分类问题的数量与误差检测码理论密切相关。我们还对一种特定的交叉验证方法下的最大分类问题数量进行了研究，这取决于常权码的码字数量。同时，我们推广了常权码的概念，并证明了类似的结果适用于其他交叉验证错误和轻量级常权码。 |
| [^192] | [Graph Representation Learning via Diversity-preserving Graph Refinement.](http://arxiv.org/abs/2103.07295) | 该论文提出了一种基于多样性保持的图结构细化的图表示学习方法，它利用已学习的节点表示来逐步改善图形结构质量。在多个下游任务上，包括节点分类、链接预测和图聚类，实验表明该方法优于现有的最先进方法。 |
| [^193] | [Gradient Flows for Regularized Stochastic Control Problems.](http://arxiv.org/abs/2006.05956) | 本文研究了正则化随机控制问题中渐变流的应用，找到了适合的度量空间，在可行控制集中构建了渐变流，使得成本函数递减，证明了渐变流的不变测度满足最优性原则，且指数级收敛。此外，最优测度值控制过程具有贝叶斯解释，可以融入先验知识。本研究旨在扩展增强学习中随机梯度算法用于解决控制问题的理论基础。 |

# 详细

[^1]: 多模态路径：通过其他模态的无关数据来改进Transformer

    Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities. (arXiv:2401.14405v1 [cs.CV])

    [http://arxiv.org/abs/2401.14405](http://arxiv.org/abs/2401.14405)

    本文提出了一种名为多模态路径的方法，通过利用其他模态的无关数据来改进特定模态的Transformer，实现了两个模型之间的组件连接，从而提高了模型的序列建模能力。

    

    我们提出使用来自其他模态的无关数据来改进特定模态的Transformer，例如，使用音频或点云数据集来改进ImageNet模型。我们强调目标模态的数据样本与其他模态无关，这与利用不同模态的配对数据（如CLIP）或交错数据的其他方法不同。我们提出了一种名为多模态路径的方法-给定目标模态和设计用于该模态的Transformer，我们使用使用另一个模态的数据训练的辅助Transformer，并构建路径来连接两个模型的组件，以便目标模态的数据可以被两个模型处理。通过这种方式，我们利用了从两个模态获得的Transformer的通用序列建模能力。作为具体实现，我们通常使用特定模态的tokenizer和任务特定的head，但是利用辅助模型的Transformer block。

    We propose to improve transformers of a specific modality with irrelevant data from other modalities, e.g., improve an ImageNet model with audio or point cloud datasets. We would like to highlight that the data samples of the target modality are irrelevant to the other modalities, which distinguishes our method from other works utilizing paired (e.g., CLIP) or interleaved data of different modalities. We propose a methodology named Multimodal Pathway - given a target modality and a transformer designed for it, we use an auxiliary transformer trained with data of another modality and construct pathways to connect components of the two models so that data of the target modality can be processed by both models. In this way, we utilize the universal sequence-to-sequence modeling abilities of transformers obtained from two modalities. As a concrete implementation, we use a modality-specific tokenizer and task-specific head as usual but utilize the transformer blocks of the auxiliary model v
    
[^2]: 解构去噪扩散模型用于自监督学习

    Deconstructing Denoising Diffusion Models for Self-Supervised Learning. (arXiv:2401.14404v1 [cs.CV])

    [http://arxiv.org/abs/2401.14404](http://arxiv.org/abs/2401.14404)

    本研究对最初用于图像生成的去噪扩散模型（DDM）进行了解构，发现只有很少的现代组件对于学习良好的表示是至关重要的。通过将DDM逐步转化为经典的去噪自编码器（DAE），我们提出了一种高度简化且类似于经典DAE的方法。该研究希望重新引起人们对现代自监督学习领域中经典方法的兴趣。

    

    在这项研究中，我们研究了最初用于图像生成的去噪扩散模型（DDM）的表示学习能力。我们的理念是逐步地解构DDM，将其转化为经典的去噪自编码器（DAE）。这种解构过程允许我们探索现代DDM的各个组件如何影响自监督表示学习。我们观察到，只有很少的现代组件对于学习良好的表示是至关重要的，而其他许多组件则是非必要的。我们的研究最终提出了一种高度简化且在很大程度上类似于经典DAE的方法。我们希望我们的研究能重新引起人们对现代自监督学习领域中一类经典方法的兴趣。

    In this study, we examine the representation learning abilities of Denoising Diffusion Models (DDM) that were originally purposed for image generation. Our philosophy is to deconstruct a DDM, gradually transforming it into a classical Denoising Autoencoder (DAE). This deconstructive procedure allows us to explore how various components of modern DDMs influence self-supervised representation learning. We observe that only a very few modern components are critical for learning good representations, while many others are nonessential. Our study ultimately arrives at an approach that is highly simplified and to a large extent resembles a classical DAE. We hope our study will rekindle interest in a family of classical methods within the realm of modern self-supervised learning.
    
[^3]: 自适应移动操作在开放环境中的可关节物体研究

    Adaptive Mobile Manipulation for Articulated Objects In the Open World. (arXiv:2401.14403v1 [cs.RO])

    [http://arxiv.org/abs/2401.14403](http://arxiv.org/abs/2401.14403)

    本文介绍了一种针对开放环境中关节物体操作的全栈方法，机器人通过自适应学习框架从少量数据中学习，并通过在线实践学习适应训练分布之外的新对象。同时，还开发了低成本的移动操作硬件平台。

    

    在开放的无结构环境中部署机器人一直是一个长期存在的问题。然而，机器人通常只在封闭的实验室环境中进行研究，之前的移动操作工作也仅限于拾取、移动、放置，这在这一领域中只是冰山一角。在本文中，我们引入了开放世界移动操作系统，采用全栈方法来解决现实世界中可关节物体的操作，例如真实世界中的门、柜子、抽屉和冰箱。机器人利用自适应学习框架，通过行为克隆先从一小组数据中学习，然后通过在线实践学习来处理训练分布之外的新对象。我们还开发了一个低成本的移动操作硬件平台，能够在无结构环境中进行安全和自主的在线适应，成本约为20,000美元。在我们的实验中，我们使用了20个可关节的物体。

    Deploying robots in open-ended unstructured environments such as homes has been a long-standing research problem. However, robots are often studied only in closed-off lab settings, and prior mobile manipulation work is restricted to pick-move-place, which is arguably just the tip of the iceberg in this area. In this paper, we introduce Open-World Mobile Manipulation System, a full-stack approach to tackle realistic articulated object operation, e.g. real-world doors, cabinets, drawers, and refrigerators in open-ended unstructured environments. The robot utilizes an adaptive learning framework to initially learns from a small set of data through behavior cloning, followed by learning from online practice on novel objects that fall outside the training distribution. We also develop a low-cost mobile manipulation hardware platform capable of safe and autonomous online adaptation in unstructured environments with a cost of around 20,000 USD. In our experiments we utilize 20 articulate obje
    
[^4]: pix2gestalt：通过合成整体进行非模态分割

    pix2gestalt: Amodal Segmentation by Synthesizing Wholes. (arXiv:2401.14398v1 [cs.CV])

    [http://arxiv.org/abs/2401.14398](http://arxiv.org/abs/2401.14398)

    pix2gestalt是一个用于零样本非模态分割的框架，能够学习估计被遮挡的整个对象的形状和外观，通过利用大规模扩散模型，学习条件扩散模型来重建整个对象，在多个基准测试中优于有监督的基线方法，同时可以显著提高在存在遮挡情况下的对象识别和3D重建方法的性能。

    

    我们介绍了pix2gestalt，一个用于零样本非模态分割的框架，它学习估计部分被遮挡的整个对象的形状和外观。通过利用大规模扩散模型并将它们的表示迁移到这个任务上，我们学习了一个条件扩散模型，用于在具有挑战性的零样本情况下重建整个对象，包括违反自然和物理先验的艺术品等示例。作为训练数据，我们使用一个经过合成筛选的数据集，其中包含带有遮挡对象及其整个对象对应物的配对。实验证明，我们的方法在已建立的基准测试中优于有监督的基线方法。此外，在存在遮挡的情况下，我们的模型还可以显著提高现有对象识别和3D重建方法的性能。

    We introduce pix2gestalt, a framework for zero-shot amodal segmentation, which learns to estimate the shape and appearance of whole objects that are only partially visible behind occlusions. By capitalizing on large-scale diffusion models and transferring their representations to this task, we learn a conditional diffusion model for reconstructing whole objects in challenging zero-shot cases, including examples that break natural and physical priors, such as art. As training data, we use a synthetically curated dataset containing occluded objects paired with their whole counterparts. Experiments show that our approach outperforms supervised baselines on established benchmarks. Our model can furthermore be used to significantly improve the performance of existing object recognition and 3D reconstruction methods in the presence of occlusions.
    
[^5]: 利用割平面方法的平滑排名SVM

    Smooth Ranking SVM via Cutting-Plane Method. (arXiv:2401.14388v1 [cs.LG])

    [http://arxiv.org/abs/2401.14388](http://arxiv.org/abs/2401.14388)

    本论文提出了一种利用割平面方法的平滑排名SVM算法来最大化分类性能中的AUC指标。通过迭代引入割平面以防止过拟合，并惩罚模型的变化。

    

    最受欢迎的分类算法旨在在训练过程中最大化分类准确性。然而，在存在类别不平衡的情况下，这种策略可能会失败，因为可以通过对大多数类别过度拟合来训练高准确性的模型。另一方面，在存在类别不平衡的情况下，曲线下面积（AUC）是一种常用的度量方法，用来比较不同算法的分类性能，并且已经提出了许多专注于在训练过程中直接优化该度量的方法。其中，基于SVM的公式特别受欢迎，因为该公式可以轻松地结合不同的正则化策略。在这项工作中，我们开发了一种基于割平面方法的原型学习方法，类似于排名SVM，以最大化AUC。我们的算法通过迭代地引入割平面来学习更简单的模型，从而以非常规的方式防止过拟合。此外，它惩罚了模型的变化。

    The most popular classification algorithms are designed to maximize classification accuracy during training. However, this strategy may fail in the presence of class imbalance since it is possible to train models with high accuracy by overfitting to the majority class. On the other hand, the Area Under the Curve (AUC) is a widely used metric to compare classification performance of different algorithms when there is a class imbalance, and various approaches focusing on the direct optimization of this metric during training have been proposed. Among them, SVM-based formulations are especially popular as this formulation allows incorporating different regularization strategies easily. In this work, we develop a prototype learning approach that relies on cutting-plane method, similar to Ranking SVM, to maximize AUC. Our algorithm learns simpler models by iteratively introducing cutting planes, thus overfitting is prevented in an unconventional way. Furthermore, it penalizes the changes in
    
[^6]: 一种基于正交多项式核的差分代数方程机器学习模型

    An Orthogonal Polynomial Kernel-Based Machine Learning Model for Differential-Algebraic Equations. (arXiv:2401.14382v1 [math.NA])

    [http://arxiv.org/abs/2401.14382](http://arxiv.org/abs/2401.14382)

    这项研究通过使用正交多项式核和LS-SVR算法，提出了一种用于解决差分代数方程系统的新的机器学习模型，并通过模拟验证了其有效性。

    

    最近引入的最小二乘支持向量回归（LS-SVR）算法用于解决微分和积分方程引起了兴趣。在本研究中，我们扩展了该算法的应用，以解决差分代数方程系统（DAEs）。我们的工作通过建立LS-SVR机器学习模型、加权残差方法和勒让德正交多项式之间的联系，提出了一种用运算符格式解决一般DAEs的新方法。为了评估我们提出的方法的有效性，我们进行了各种DAE场景的模拟，如非线性系统、分数阶导数、积分-微分和偏微分DAEs。最后，我们对比了我们提出的方法与目前已建立的现有技术方法，证明了其可靠性和有效性。

    The recent introduction of the Least-Squares Support Vector Regression (LS-SVR) algorithm for solving differential and integral equations has sparked interest. In this study, we expand the application of this algorithm to address systems of differential-algebraic equations (DAEs). Our work presents a novel approach to solving general DAEs in an operator format by establishing connections between the LS-SVR machine learning model, weighted residual methods, and Legendre orthogonal polynomials. To assess the effectiveness of our proposed method, we conduct simulations involving various DAE scenarios, such as nonlinear systems, fractional-order derivatives, integro-differential, and partial DAEs. Finally, we carry out comparisons between our proposed method and currently established state-of-the-art approaches, demonstrating its reliability and effectiveness.
    
[^7]: 面向流形值图的扩散卷积神经网络：多重难题图神经网络层

    Manifold GCN: Diffusion-based Convolutional Neural Network for Manifold-valued Graphs. (arXiv:2401.14381v1 [cs.LG])

    [http://arxiv.org/abs/2401.14381](http://arxiv.org/abs/2401.14381)

    本研究提出了两个用于具有流形值特征的图的神经网络层。这些层具有对节点排列和特征流形的等变性，并在深度学习任务中显示出有益的归纳偏差。

    

    我们提出了两种用于具有Riemannian流形特征的图上的图神经网络层。第一，基于流形值图的扩散方程，我们构建了一个扩散层，可以应用于任意数量的节点和图连接模式。第二，我们通过将向量神经元框架的思想转化到我们的一般设置中，建立了一个切线多层感知器。这两个层对节点排列和特征流形的等变具有响应，这些特性在许多深度学习任务中已被证明具有有益的归纳偏差。我们在合成数据上以及在右侧海马三角网格上分类阿尔茨海默病的数值实例表明我们建立的层具有非常好的性能。

    We propose two graph neural network layers for graphs with features in a Riemannian manifold. First, based on a manifold-valued graph diffusion equation, we construct a diffusion layer that can be applied to an arbitrary number of nodes and graph connectivity patterns. Second, we model a tangent multilayer perceptron by transferring ideas from the vector neuron framework to our general setting. Both layers are equivariant with respect to node permutations and isometries of the feature manifold. These properties have been shown to lead to a beneficial inductive bias in many deep learning tasks. Numerical examples on synthetic data as well as on triangle meshes of the right hippocampus to classify Alzheimer's disease demonstrate the very good performance of our layers.
    
[^8]: UrbanGenAI: 使用全景分割和扩散模型重构城市景观

    UrbanGenAI: Reconstructing Urban Landscapes using Panoptic Segmentation and Diffusion Models. (arXiv:2401.14379v1 [cs.CV])

    [http://arxiv.org/abs/2401.14379](http://arxiv.org/abs/2401.14379)

    本文介绍了一种使用全景分割和扩散模型重构城市景观的新方法，该方法通过整合先进的图像分割和扩散模型，实现了对城市设计的全面处理，并在物体检测和文本到图像生成方面取得了显著准确性。

    

    在当代设计实践中，计算机视觉和生成式人工智能(genAI)的整合代表了一种转变，更加交互和包容的过程。这些技术提供了图像分析和生成的新维度，在城市景观重建的背景下尤其重要。本文提出了一种新的工作流程，以原型应用程序的形式展示，旨在利用先进的图像分割和扩散模型之间的协同作用，实现对城市设计的全面方法。我们的方法包括详细的图像分割模型OneFormer和通过ControlNet实现的稳定扩散XL（SDXL）扩散模型，用于从文本描述中生成图像。验证结果表明，原型应用程序具有很高的性能，展示了在物体检测和文本到图像生成方面的显著准确性。

    In contemporary design practices, the integration of computer vision and generative artificial intelligence (genAI) represents a transformative shift towards more interactive and inclusive processes. These technologies offer new dimensions of image analysis and generation, which are particularly relevant in the context of urban landscape reconstruction. This paper presents a novel workflow encapsulated within a prototype application, designed to leverage the synergies between advanced image segmentation and diffusion models for a comprehensive approach to urban design. Our methodology encompasses the OneFormer model for detailed image segmentation and the Stable Diffusion XL (SDXL) diffusion model, implemented through ControlNet, for generating images from textual descriptions. Validation results indicated a high degree of performance by the prototype application, showcasing significant accuracy in both object detection and text-to-image generation. This was evidenced by superior Inter
    
[^9]: TURNA: 一种用于增强理解和生成的土耳其编码-解码语言模型

    TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation. (arXiv:2401.14373v1 [cs.CL])

    [http://arxiv.org/abs/2401.14373](http://arxiv.org/abs/2401.14373)

    TURNA是一种用于土耳其语的低资源语言模型，具备自然语言理解和生成任务能力。TURNA通过预训练的编码-解码架构在理解和生成任务中表现优于多语言模型，并能与土耳其语单语模型竞争。

    

    自然语言处理的最新进展主要偏向于资源丰富且以英语为中心的模型，这导致了与资源稀缺的语言之间存在显著差距。在这项工作中，我们介绍了TURNA语言模型，该模型针对资源稀缺的土耳其语开发，能够进行自然语言理解和生成任务。TURNA使用基于统一框架UL2的编码-解码架构进行预训练，并且我们专门为此目的筛选了一个多样的语料库。我们对TURNA在土耳其语的三个生成任务和五个理解任务上进行了评估。结果表明，TURNA在理解和生成任务上优于多语言模型，并与土耳其语单语模型在理解任务上竞争。TURNA已在https://huggingface.co/boun-tabi-LMG/TURNA 上提供。

    The recent advances in natural language processing have predominantly favored well-resourced English-centric models, resulting in a significant gap with low-resource languages. In this work, we introduce the language model TURNA, which is developed for the low-resource language Turkish and is capable of both natural language understanding and generation tasks. TURNA is pretrained with an encoder-decoder architecture based on the unified framework UL2 with a diverse corpus that we specifically curated for this purpose. We evaluated TURNA with three generation tasks and five understanding tasks for Turkish. The results show that TURNA outperforms several multilingual models in both understanding and generation tasks, and competes with monolingual Turkish models in understanding tasks. TURNA is made available at https://huggingface.co/boun-tabi-LMG/TURNA .
    
[^10]: Genie：实现内容导向数据集生成的人类水平

    Genie: Achieving Human Parity in Content-Grounded Datasets Generation. (arXiv:2401.14367v1 [cs.CL])

    [http://arxiv.org/abs/2401.14367](http://arxiv.org/abs/2401.14367)

    Genie是一个用于自动生成高质量内容导向数据的方法，通过三个阶段实现：内容准备、生成和过滤。在人类评估中，生成的数据被发现是自然且高质量的，并且通过与使用人工数据训练的模型比较，我们的模型表现相当或更好。

    

    对于内容导向生成任务，缺乏高质量的数据被认为是推动这些任务发展的主要障碍。为了解决这个问题，我们提出了Genie，一种用于自动生成高质量内容导向数据的新方法。它包括三个阶段：（a）内容准备，（b）生成：从内容中创建特定任务的示例（例如问题-答案对或摘要），（c）过滤机制，旨在确保生成数据的质量和可信度。我们通过生成三个大规模的合成数据来展示这种方法：长型问题回答（LFQA）、摘要和信息提取。在人类评估中，我们生成的数据被发现是自然且高质量的。此外，我们将使用我们的数据训练的模型与使用人工编写的数据训练的模型进行比较 - 对于LFQA，我们与ELI5和ASQA进行比较，对于摘要，我们与CNN-DailyMail进行比较。我们表明，我们的模型与或超过使用人工数据训练的模型。

    The lack of high-quality data for content-grounded generation tasks has been identified as a major obstacle to advancing these tasks. To address this gap, we propose Genie, a novel method for automatically generating high-quality content-grounded data. It consists of three stages: (a) Content Preparation, (b) Generation: creating task-specific examples from the content (e.g., question-answer pairs or summaries). (c) Filtering mechanism aiming to ensure the quality and faithfulness of the generated data. We showcase this methodology by generating three large-scale synthetic data, making wishes, for Long-Form Question-Answering (LFQA), summarization, and information extraction. In a human evaluation, our generated data was found to be natural and of high quality. Furthermore, we compare models trained on our data with models trained on human-written data -- ELI5 and ASQA for LFQA and CNN-DailyMail for Summarization. We show that our models are on par with or outperforming models trained 
    
[^11]: MoE-Infinity：用于高效MoE服务的激活感知专家卸载系统

    MoE-Infinity: Activation-Aware Expert Offloading for Efficient MoE Serving. (arXiv:2401.14361v1 [cs.LG])

    [http://arxiv.org/abs/2401.14361](http://arxiv.org/abs/2401.14361)

    MoE-Infinity是一种成本高效的MoE服务系统，通过激活感知的专家卸载和缓存技术，显著降低了延迟，并提高了成本性能。

    

    本文介绍了MoE-Infinity，一种成本高效的专家混合(MoE)服务系统，实现了激活感知的专家卸载。MoE-Infinity具有序列级专家激活追踪的特点，这是一种擅长识别稀疏激活并捕捉MoE推理的时间局部性的新方法。通过分析这些追踪，MoE-Infinity执行了新颖的激活感知专家预取和缓存，大大降低了通常与卸载专家相关的延迟开销，提高了成本性能。在一个集群中进行的大量实验表明，MoE-Infinity优于许多现有的系统和方法，对于各种MoEs，将延迟降低了420倍，将部署成本降低了8倍以上。MoE-Infinity的源代码可在https://github.com/TorchMoE/MoE-Infinity公开获取。

    This paper presents MoE-Infinity, a cost-efficient mixture-of-expert (MoE) serving system that realizes activation-aware expert offloading. MoE-Infinity features sequence-level expert activation tracing, a new approach adept at identifying sparse activations and capturing the temporal locality of MoE inference. By analyzing these traces, MoE-Infinity performs novel activation-aware expert prefetching and caching, substantially reducing the latency overheads usually associated with offloading experts for improved cost performance. Extensive experiments in a cluster show that MoE-Infinity outperforms numerous existing systems and approaches, reducing latency by 4 20X and decreasing deployment costs by over 8X for various MoEs. MoE-Infinity's source code is publicly available at https://github.com/TorchMoE/MoE-Infinity
    
[^12]: ServerlessLLM：增强本地化的用于大型语言模型的无服务器推理系统

    ServerlessLLM: Locality-Enhanced Serverless Inference for Large Language Models. (arXiv:2401.14351v1 [cs.LG])

    [http://arxiv.org/abs/2401.14351](http://arxiv.org/abs/2401.14351)

    ServerlessLLM是一种用于大型语言模型的增强本地化无服务器推理系统，通过优化检查点加载、本地化推理和服务器分配来实现高效且低延迟的推理过程。

    

    本文介绍了ServerlessLLM，一种增强本地化的用于大型语言模型(LLM)的无服务器推理系统。ServerlessLLM利用GPU服务器上可用的存储和内存设备的大容量和带宽，从而减少昂贵的远程检查点下载并实现高效的检查点加载。ServerlessLLM通过三个主要贡献实现了这一目标：(i)通过一种新颖的加载优化检查点格式设计和高效的多级检查点加载系统实现快速LLM检查点加载；(ii)利用本地化推理和实时迁移，使ServerlessLLM能够在保持低延迟的同时有效地实现本地化驱动的服务器分配；(iii)本地化感知的服务器分配，使ServerlessLLM能够评估集群中每个服务器的状态，并有效地安排模型启动时间以充分利用本地检查点位置。我们进行了全面的实验，包括微基准测试和大规模语言模型评估，验证了ServerlessLLM的有效性和性能优势。

    This paper presents ServerlessLLM, a locality-enhanced serverless inference system for Large Language Models (LLMs). ServerlessLLM exploits the substantial capacity and bandwidth of storage and memory devices available on GPU servers, thereby reducing costly remote checkpoint downloads and achieving efficient checkpoint loading. ServerlessLLM achieves this through three main contributions: (i) fast LLM checkpoint loading via a novel loading-optimized checkpoint format design, coupled with an efficient multi-tier checkpoint loading system; (ii) locality-driven LLM inference with live migration, which allows ServerlessLLM to effectively achieve locality-driven server allocation while preserving the low latency of ongoing LLM inference; and (iii) locality-aware server allocation, enabling ServerlessLLM to evaluate the status of each server in a cluster and effectively schedule model startup time to capitalize on local checkpoint placement. Our comprehensive experiments, which include micr
    
[^13]: 类属性先验：将优化方法应用于异质性和公平目标

    Class-attribute Priors: Adapting Optimization to Heterogeneity and Fairness Objective. (arXiv:2401.14343v1 [cs.LG])

    [http://arxiv.org/abs/2401.14343](http://arxiv.org/abs/2401.14343)

    本研究提出了一种名称为CAP的方法，通过生成类别特定的学习策略来更好地适应异质性数据，并在损失函数设计和标签不平衡问题方面取得了显著改进。

    

    现代分类问题在各个类别之间存在异质性：每个类别可能具有独特的属性，例如样本大小，标签质量或可预测性（易 vs 难），以及在测试时的变量重要性。如果不注意处理，这些异质性会阻碍学习过程，尤其是在优化公平目标时。在高斯混合模型设定下，我们证明了为了达到平衡准确度，最优的支持向量机分类器需要适应类别属性。这激发了我们提出了CAP：一种基于类别属性生成类别特定学习策略（例如超参数）的有效和通用方法。通过这种方式，优化过程更好地适应异质性。CAP相比于将不同的超参数分配给每个类别的朴素方法有显著改进。我们将CAP实例化为损失函数设计和事后对数调整，重点关注标签不平衡问题。

    Modern classification problems exhibit heterogeneities across individual classes: Each class may have unique attributes, such as sample size, label quality, or predictability (easy vs difficult), and variable importance at test-time. Without care, these heterogeneities impede the learning process, most notably, when optimizing fairness objectives. Confirming this, under a gaussian mixture setting, we show that the optimal SVM classifier for balanced accuracy needs to be adaptive to the class attributes. This motivates us to propose CAP: An effective and general method that generates a class-specific learning strategy (e.g. hyperparameter) based on the attributes of that class. This way, optimization process better adapts to heterogeneities. CAP leads to substantial improvements over the naive approach of assigning separate hyperparameters to each class. We instantiate CAP for loss function design and post-hoc logit adjustment, with emphasis on label-imbalanced problems. We show that CA
    
[^14]: 基于得分结构先验的部分已知高斯图模型估计

    Estimation of partially known Gaussian graphical models with score-based structural priors. (arXiv:2401.14340v1 [stat.ML])

    [http://arxiv.org/abs/2401.14340](http://arxiv.org/abs/2401.14340)

    本论文提出了一种基于得分结构先验的算法，用于估计部分已知高斯图模型。通过使用图神经网络来估计图的得分函数，我们可以在生成样本时利用退火朗格维能扩散，从而更准确地估计后验分布。数值实验表明，我们的方法具有明显的优势。

    

    我们提出了一种新的算法，用于支持估计部分已知的高斯图模型，并且结合了关于底层图的先验信息。与传统方法相比，传统方法使用点估计方法基于最大似然或最大后验准则，并使用（简单的）精度矩阵先验来提供点估计。我们考虑对图进行先验，并依赖退火朗格维能扩散从后验分布中生成样本。由于朗格维能采样器需要访问底层图先验的得分函数，因此我们使用图神经网络来有效地从图数据集（事先可用或从已知分布生成）估计得分。数值实验证明了我们方法的优势。

    We propose a novel algorithm for the support estimation of partially known Gaussian graphical models that incorporates prior information about the underlying graph. In contrast to classical approaches that provide a point estimate based on a maximum likelihood or a maximum a posteriori criterion using (simple) priors on the precision matrix, we consider a prior on the graph and rely on annealed Langevin diffusion to generate samples from the posterior distribution. Since the Langevin sampler requires access to the score function of the underlying graph prior, we use graph neural networks to effectively estimate the score from a graph dataset (either available beforehand or generated from a known distribution). Numerical experiments demonstrate the benefits of our approach.
    
[^15]: SunBlock：无云保护物联网系统

    SunBlock: Cloudless Protection for IoT Systems. (arXiv:2401.14332v1 [cs.CR])

    [http://arxiv.org/abs/2401.14332](http://arxiv.org/abs/2401.14332)

    本文研究了在家庭路由器上使用AI工具和经典流量过滤算法进行本地有效的物联网威胁检测，其结果表明装备我们的解决方案的典型家庭路由器能够在保护物联网网络方面与现有的流行解决方案相当或优于其性能。

    

    随着越来越多的物联网设备进入家庭，潜在的信息泄漏渠道以及相关的安全威胁和隐私风险也在增加。尽管在无保护的家庭网络中存在对物联网设备的长期攻击历史，但准确、快速地检测和预防此类攻击的问题仍然存在。许多现有的物联网保护解决方案是基于云的，有时效果不佳，并且可能与未知第三方共享消费者数据。本文研究了在家庭路由器上使用人工智能工具结合经典基于规则的流量过滤算法来有效检测物联网威胁的潜力。我们的结果表明，通过机器学习和流量过滤逻辑导致路由器硬件资源略微增加，装备我们的解决方案的典型家庭路由器能够有效检测风险并保护典型的家庭物联网网络，与现有的流行解决方案相当或优于其性能。

    With an increasing number of Internet of Things (IoT) devices present in homes, there is a rise in the number of potential information leakage channels and their associated security threats and privacy risks. Despite a long history of attacks on IoT devices in unprotected home networks, the problem of accurate, rapid detection and prevention of such attacks remains open. Many existing IoT protection solutions are cloud-based, sometimes ineffective, and might share consumer data with unknown third parties. This paper investigates the potential for effective IoT threat detection locally, on a home router, using AI tools combined with classic rule-based traffic-filtering algorithms. Our results show that with a slight rise of router hardware resources caused by machine learning and traffic filtering logic, a typical home router instrumented with our solution is able to effectively detect risks and protect a typical home IoT network, equaling or outperforming existing popular solutions, wi
    
[^16]: "All of Me": 从公开的Spotify播放列表中挖掘用户属性

    "All of Me": Mining Users' Attributes from their Public Spotify Playlists. (arXiv:2401.14296v1 [cs.CR])

    [http://arxiv.org/abs/2401.14296](http://arxiv.org/abs/2401.14296)

    本研究调查了Spotify用户属性与他们公开播放列表之间的关系，特别关注识别与用户个人属性相关的音乐特征。

    

    在数字音乐流媒体时代，像Spotify这样的平台上的播放列表已经成为个人音乐体验的重要组成部分。人们创建并公开分享自己的播放列表，以表达他们的音乐品味，推广他们最喜爱的艺术家的发现，并促进社交联系。这些可以公开访问的播放列表超越了仅仅音乐偏好的界限：它们是丰富洞察用户属性和身份的来源。例如，老年人的音乐偏好可能更偏向于弗兰克·辛纳屈，而比莉·艾利什仍然是十几岁青少年的首选。因此，这些播放列表成为了一扇了解音乐身份多样而不断演变的窗口。在这项工作中，我们研究了Spotify用户属性和他们的公开播放列表之间的关系。我们特别关注识别与用户个人属性相关的经常出现的音乐特征，例如人口统计信息，习惯或个性等。

    In the age of digital music streaming, playlists on platforms like Spotify have become an integral part of individuals' musical experiences. People create and publicly share their own playlists to express their musical tastes, promote the discovery of their favorite artists, and foster social connections. These publicly accessible playlists transcend the boundaries of mere musical preferences: they serve as sources of rich insights into users' attributes and identities. For example, the musical preferences of elderly individuals may lean more towards Frank Sinatra, while Billie Eilish remains a favored choice among teenagers. These playlists thus become windows into the diverse and evolving facets of one's musical identity.  In this work, we investigate the relationship between Spotify users' attributes and their public playlists. In particular, we focus on identifying recurring musical characteristics associated with users' individual attributes, such as demographics, habits, or perso
    
[^17]: 推理的拓扑学：揭秘思维链、树和图

    Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts. (arXiv:2401.14295v1 [cs.CL])

    [http://arxiv.org/abs/2401.14295](http://arxiv.org/abs/2401.14295)

    这篇论文探讨了结合结构的提示工程在提高大型语言模型推理性能方面的前景，通过思维链、思维树或思维图的设计来引导整体推理过程。通过大量实例，这种范式显著增强了模型在多个任务中的能力。总的来说，论文提供了一个通用蓝图，为未来的发展铺平道路。

    

    自然语言处理（NLP）领域近年来取得了显著进展，特别是在通过创新的提示技术提高大型语言模型（LLM）性能方面。其中，与结构相结合的提示工程被视为一种有前途的范式，其设计如思维链、思维树或思维图等，通过结构指导整体LLM推理过程。通过大量实例的说明，这种范式显著增强了LLM在逻辑或数学推理、规划或创造性写作等各种任务中的能力。为了方便理解这个不断发展的领域并为未来的发展铺平道路，我们设计了一个有效和高效的LLM推理方案的通用蓝图。为此，我们对提示执行流程进行了深入分析，澄清并明确定义了不同的概念。然后我们建立第一个分类系统

    The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxon
    
[^18]: 对听力受损听众智能预测的语音基础模型的研究

    Speech foundation models on intelligibility prediction for hearing-impaired listeners. (arXiv:2401.14289v1 [cs.SD])

    [http://arxiv.org/abs/2401.14289](http://arxiv.org/abs/2401.14289)

    对于听力受损听众智能预测应用，对语音基础模型（SFM）进行了系统评估，并提出了一种简单的方法，在冻结的SFM之上学习一个轻量级的专门的预测头，这一方法在Clarity Prediction Challenge 2（CPC2）中取得了胜利，展示了其在语音知觉应用中的潜力。

    

    语音基础模型（SFM）已经在许多语音处理任务中进行了基准测试，往往以最小的调整实现了最先进的性能。然而，对于与语音知觉社区感兴趣的应用，SFM范式的研究明显较少。本文对10种SFM进行了系统评估，以在一个这样的应用中进行研究：语音清晰度预测。我们专注于Clarity Prediction Challenge 2（CPC2）的非侵入性设置，其中任务是预测听力受损听众从噪声录音中正确理解的单词百分比。我们提出了一种简单的方法，该方法在冻结的SFM之上学习一个轻量级的专门的预测头来解决这个问题。我们的结果显示出SFM之间性能差异有统计学上的显著差异。我们的方法在CPC2中获得了获胜的提交，展示了它在语音知觉应用中的潜力。

    Speech foundation models (SFMs) have been benchmarked on many speech processing tasks, often achieving state-of-the-art performance with minimal adaptation. However, the SFM paradigm has been significantly less explored for applications of interest to the speech perception community. In this paper we present a systematic evaluation of 10 SFMs on one such application: Speech intelligibility prediction. We focus on the non-intrusive setup of the Clarity Prediction Challenge 2 (CPC2), where the task is to predict the percentage of words correctly perceived by hearing-impaired listeners from speech-in-noise recordings. We propose a simple method that learns a lightweight specialized prediction head on top of frozen SFMs to approach the problem. Our results reveal statistically significant differences in performance across SFMs. Our method resulted in the winning submission in the CPC2, demonstrating its promise for speech perception applications.
    
[^19]: 通过近似贝叶斯最优预测检测信息泄漏

    Information Leakage Detection through Approximate Bayes-optimal Prediction. (arXiv:2401.14283v1 [stat.ML])

    [http://arxiv.org/abs/2401.14283](http://arxiv.org/abs/2401.14283)

    本论文通过建立一个理论框架，利用统计学习理论和信息论来准确量化和检测信息泄漏，通过近似贝叶斯预测的对数损失和准确性来准确估计互信息。

    

    在今天的以数据驱动的世界中，公开可获得的信息的增加加剧了信息泄漏（IL）的挑战，引发了安全问题。IL涉及通过系统的可观察信息无意地将秘密（敏感）信息暴露给未经授权的方，传统的统计方法通过估计可观察信息和秘密信息之间的互信息（MI）来检测IL，面临维度灾难、收敛、计算复杂度和MI估计错误等挑战。此外，虽然新兴的监督机器学习（ML）方法在二进制系统敏感信息的检测上有效，但缺乏一个全面的理论框架。为了解决这些限制，我们使用统计学习理论和信息论建立了一个理论框架来准确量化和检测IL。我们证明了可以通过近似贝叶斯预测的对数损失和准确性来准确估计MI。

    In today's data-driven world, the proliferation of publicly available information intensifies the challenge of information leakage (IL), raising security concerns. IL involves unintentionally exposing secret (sensitive) information to unauthorized parties via systems' observable information. Conventional statistical approaches, which estimate mutual information (MI) between observable and secret information for detecting IL, face challenges such as the curse of dimensionality, convergence, computational complexity, and MI misestimation. Furthermore, emerging supervised machine learning (ML) methods, though effective, are limited to binary system-sensitive information and lack a comprehensive theoretical framework. To address these limitations, we establish a theoretical framework using statistical learning theory and information theory to accurately quantify and detect IL. We demonstrate that MI can be accurately estimated by approximating the log-loss and accuracy of the Bayes predict
    
[^20]: 产生对数据集变化具有鲁棒性的浮游生物分类器

    Producing Plankton Classifiers that are Robust to Dataset Shift. (arXiv:2401.14256v1 [cs.CV])

    [http://arxiv.org/abs/2401.14256](http://arxiv.org/abs/2401.14256)

    本研究针对浮游生物分类器在数据集变化下性能下降的挑战，通过整合不同部署日的数据集并评估超出数据集的性能，揭示了分类器在实际情景中遇到的失败情况。基于对导致性能下降的条件的研究，我们提出了一种预防性评估方法，用于识别新数据分类中可能出现的问题，并确定影响分类的特征。

    

    现代浮游生物高通量监测依赖于深度学习分类器对水生态系统中的物种进行识别。尽管在理论性能方面令人满意，但数据集变化给部署过程中的性能带来了显著挑战。在我们的研究中，我们将ZooLake数据集与10个独立部署日的手动注释图像集成，作为测试单元，以评估超出数据集的性能。我们的分析揭示了分类器在理想情况下表现良好的情况下，在实际情景中遇到显著失败的实例。例如，一个具有92％标称测试准确度的MobileNet模型显示了77％的超出数据集准确度。我们系统地研究了导致超出数据集性能下降的条件，并提出了一种预防性评估方法，以识别对新数据进行分类时可能出现的问题，并确定影响分类的超出数据集图像中的特征。我们提出了一个三步流程：(i)识别超出数据集的实例，(ii)评估超出数据集性能的影响因素，(iii)预测新数据的分类准确性。

    Modern plankton high-throughput monitoring relies on deep learning classifiers for species recognition in water ecosystems. Despite satisfactory nominal performances, a significant challenge arises from Dataset Shift, which causes performances to drop during deployment. In our study, we integrate the ZooLake dataset with manually-annotated images from 10 independent days of deployment, serving as test cells to benchmark Out-Of-Dataset (OOD) performances. Our analysis reveals instances where classifiers, initially performing well in In-Dataset conditions, encounter notable failures in practical scenarios. For example, a MobileNet with a 92% nominal test accuracy shows a 77% OOD accuracy. We systematically investigate conditions leading to OOD performance drops and propose a preemptive assessment method to identify potential pitfalls when classifying new data, and pinpoint features in OOD images that adversely impact classification. We present a three-step pipeline: (i) identifying OOD d
    
[^21]: 使用语法演化和数据增强的可解释乳腺癌诊断方案

    Interpretable Solutions for Breast Cancer Diagnosis with Grammatical Evolution and Data Augmentation. (arXiv:2401.14255v1 [cs.LG])

    [http://arxiv.org/abs/2401.14255](http://arxiv.org/abs/2401.14255)

    本文通过结合语法演化和数据增强方案，利用合成数据生成技术STEM来训练可解释的乳腺癌诊断模型，解决了数据不平衡和模型可解释性的问题。

    

    医学影像诊断越来越依赖机器学习模型。然而，这个任务常常受到严重不平衡的数据集的困扰，其中阳性病例可能非常罕见。而它们的可解释性又非常有限，这一点越来越重要。尽管事后解释技术（如SHAP和LIME）在所谓的黑盒模型上已经有一定的成功，但使用内在可理解的模型会使这些努力更加有成效。本文通过展示如何利用一种相对较新的合成数据生成技术STEM，用于产生用语法演化产生的可解释模型所需的训练数据，来解决这些问题。STEM是一种最近引入的合成少数类过采样技术（SMOTE）、编辑最近邻（ENN）和混合的组合；它先前已成功用于解决类别间和类别内不平衡问题。

    Medical imaging diagnosis increasingly relies on Machine Learning (ML) models. This is a task that is often hampered by severely imbalanced datasets, where positive cases can be quite rare. Their use is further compromised by their limited interpretability, which is becoming increasingly important. While post-hoc interpretability techniques such as SHAP and LIME have been used with some success on so-called black box models, the use of inherently understandable models makes such endeavors more fruitful. This paper addresses these issues by demonstrating how a relatively new synthetic data generation technique, STEM, can be used to produce data to train models produced by Grammatical Evolution (GE) that are inherently understandable. STEM is a recently introduced combination of the Synthetic Minority Oversampling Technique (SMOTE), Edited Nearest Neighbour (ENN), and Mixup; it has previously been successfully used to tackle both between class and within class imbalance issues. We test o
    
[^22]: 基于Reddit文本增强标注技术和细调Longformer模型的英语和卢干达抑郁症严重程度分类研究

    Enhanced Labeling Technique for Reddit Text and Fine-Tuned Longformer Models for Classifying Depression Severity in English and Luganda. (arXiv:2401.14240v1 [cs.CL])

    [http://arxiv.org/abs/2401.14240](http://arxiv.org/abs/2401.14240)

    本研究通过提出的标注方法对Reddit文本进行分类，并细调了Longformer模型，研究结果表明该模型在英语和卢干达语的抑郁症严重程度分类中表现出色，优于基准模型。

    

    抑郁症是全球负担重的、难以控制的心理健康问题之一。专家们可以使用贝克抑郁量表（BDI）问卷早期检测其严重程度，给患者施用适当药物，阻止其进展。由于担心可能的污名化，许多患者转向Reddit等社交媒体平台寻求建议和帮助。本研究从Reddit提取文本以促进诊断过程。它采用了一种提出的标注方法对文本进行分类，并随后对Longformer模型进行了细调。将该模型与基准模型进行了比较，包括朴素贝叶斯、随机森林、支持向量机和梯度提升。我们的研究结果显示，Longformer模型在英语（48%）和卢干达语（45%）的自定义数据集上优于基准模型。

    Depression is a global burden and one of the most challenging mental health conditions to control. Experts can detect its severity early using the Beck Depression Inventory (BDI) questionnaire, administer appropriate medication to patients, and impede its progression. Due to the fear of potential stigmatization, many patients turn to social media platforms like Reddit for advice and assistance at various stages of their journey. This research extracts text from Reddit to facilitate the diagnostic process. It employs a proposed labeling approach to categorize the text and subsequently fine-tunes the Longformer model. The model's performance is compared against baseline models, including Naive Bayes, Random Forest, Support Vector Machines, and Gradient Boosting. Our findings reveal that the Longformer model outperforms the baseline models in both English (48%) and Luganda (45%) languages on a custom-made dataset.
    
[^23]: AR-GAN: 基于生成对抗网络的自动驾驶车辆交通标志分类系统的防御方法

    AR-GAN: Generative Adversarial Network-Based Defense Method Against Adversarial Attacks on the Traffic Sign Classification System of Autonomous Vehicles. (arXiv:2401.14232v1 [cs.CV])

    [http://arxiv.org/abs/2401.14232](http://arxiv.org/abs/2401.14232)

    这篇论文提出了一种基于生成对抗网络的防御方法AR-GAN，用于自动驾驶车辆中的交通标志分类系统，该方法具有在各种对抗攻击下保持高性能的能力。

    

    本研究开发了一种基于生成对抗网络（GAN）的防御方法，用于自动驾驶车辆中的交通标志分类，称为攻击鲁棒的GAN（AR-GAN）。AR-GAN的创新之处在于（i）假设对抗攻击模型和样本一无所知，（ii）在各种对抗攻击类型下始终提供高水平的交通标志分类性能。AR-GAN分类系统由一个通过重建去噪图像的生成器和一个对重建图像进行分类的分类器组成。作者在没有攻击和各种对抗攻击（如快速梯度符号法（FGSM），DeepFool，Carlini和Wagner（C&W），以及投影梯度下降（PGD））的情况下测试了AR-GAN。作者考虑了这些攻击的两种形式，即（i）黑盒攻击（假设攻击者对分类器没有任何先验知识），以及（ii）白盒攻击（假设攻击者拥有完全知识）。

    This study developed a generative adversarial network (GAN)-based defense method for traffic sign classification in an autonomous vehicle (AV), referred to as the attack-resilient GAN (AR-GAN). The novelty of the AR-GAN lies in (i) assuming zero knowledge of adversarial attack models and samples and (ii) providing consistently high traffic sign classification performance under various adversarial attack types. The AR-GAN classification system consists of a generator that denoises an image by reconstruction, and a classifier that classifies the reconstructed image. The authors have tested the AR-GAN under no-attack and under various adversarial attacks, such as Fast Gradient Sign Method (FGSM), DeepFool, Carlini and Wagner (C&W), and Projected Gradient Descent (PGD). The authors considered two forms of these attacks, i.e., (i) black-box attacks (assuming the attackers possess no prior knowledge of the classifier), and (ii) white-box attacks (assuming the attackers possess full knowledge
    
[^24]: 评估通过参数高效微调方法训练的参数矩阵的可移植性

    Assessing the Portability of Parameter Matrices Trained by Parameter-Efficient Finetuning Methods. (arXiv:2401.14228v1 [cs.CL])

    [http://arxiv.org/abs/2401.14228](http://arxiv.org/abs/2401.14228)

    本文研究了通过参数高效微调方法训练的模块的可移植性，发现这些移植的模块在各种情景下表现出优异的性能，可以有效地重复利用任务特定知识。

    

    随着训练规模越来越大的语言模型的成本增加，对重复利用先前学到的知识的兴趣也在增加。迁移学习方法表明，重复利用非任务特定知识可以帮助后续特定任务学习。本文研究了相反的情况：将从一个模型移植编码任务特定知识的完整功能模块到另一个模型。我们设计了一项包括1,440个训练/测试运行的研究，以测试通过参数高效微调(PEFT)技术训练的模块的可移植性，以情感分析为示例任务。我们在各种场景中测试了可移植性，涉及不同的PEFT技术和不同的预训练主机模型，等等。我们将移植的模块的性能与(i)从头开始训练的相等模块的性能和(ii)从与移植的模块相同分布的参数中采样训练的模块进行比较。我们发现，移植的模块的性能远远超过所测试的两种替代方案的性能，但需注意一些局限性。

    As the cost of training ever larger language models has grown, so has the interest in reusing previously learnt knowledge. Transfer learning methods have shown how reusing non-task-specific knowledge can help in subsequent task-specific learning. In this paper, we investigate the inverse: porting whole functional modules that encode task-specific knowledge from one model to another. We designed a study comprising 1,440 training/testing runs to test the portability of modules trained by parameter-efficient finetuning (PEFT) techniques, using sentiment analysis as an example task. We test portability in a wide range of scenarios, involving different PEFT techniques and different pretrained host models, among other dimensions. We compare the performance of ported modules with that of equivalent modules trained (i) from scratch, and (ii) from parameters sampled from the same distribution as the ported module. We find that the ported modules far outperform the two alternatives tested, but t
    
[^25]: 自动学习组合子任务的高样本效率强化学习

    Sample Efficient Reinforcement Learning by Automatically Learning to Compose Subtasks. (arXiv:2401.14226v1 [cs.LG])

    [http://arxiv.org/abs/2401.14226](http://arxiv.org/abs/2401.14226)

    本工作提出了一种用于提高样本效率的强化学习算法，通过自动构建奖励函数来选择最优子任务，在复杂任务中取得了较好的性能。

    

    提高样本效率在强化学习中是核心问题，尤其是在奖励稀疏的环境中。最近的一些方法提出将奖励函数作为手动设计或学习得到的奖励结构来改进学习效率。手动设计的奖励结构可能存在不准确性，而现有的自动学习方法对于复杂任务而言常常计算难以处理。不准确或局部的奖励结构的整合在强化学习算法中无法学到最优策略。在本文中，我们提出了一种能够自动构建奖励函数提高样本效率的强化学习算法，给定一组表示子任务的标签。在对任务的最小了解下，我们训练了一个高层策略，在每个状态下选择最优的子任务，同时还有一个低层策略高效学习完成每个子任务。我们评估了我们的算法在不同环境下的表现，并与其他方法进行了对比。

    Improving sample efficiency is central to Reinforcement Learning (RL), especially in environments where the rewards are sparse. Some recent approaches have proposed to specify reward functions as manually designed or learned reward structures whose integrations in the RL algorithms are claimed to significantly improve the learning efficiency. Manually designed reward structures can suffer from inaccuracy and existing automatically learning methods are often computationally intractable for complex tasks. The integration of inaccurate or partial reward structures in RL algorithms fail to learn optimal policies. In this work, we propose an RL algorithm that can automatically structure the reward function for sample efficiency, given a set of labels that signify subtasks. Given such minimal knowledge about the task, we train a high-level policy that selects optimal sub-tasks in each state together with a low-level policy that efficiently learns to complete each sub-task. We evaluate our al
    
[^26]: 通过自适应权重聚类和服务器端蒸馏实现高效通信的联邦学习

    Communication-Efficient Federated Learning through Adaptive Weight Clustering and Server-Side Distillation. (arXiv:2401.14211v1 [cs.LG])

    [http://arxiv.org/abs/2401.14211](http://arxiv.org/abs/2401.14211)

    本论文提出了一种名为FedCompress的新方法，通过动态权重聚类和服务器端知识蒸馏的结合，实现了高效通信的联邦学习。该方法在降低通信成本的同时，能够学习到高度可泛化的模型。

    

    联邦学习是一种有望在保护数据隐私的同时，通过多个设备共同训练深度神经网络的技术。然而，由于训练过程中重复的服务器-客户端通信导致了过多的通信成本，这给联邦学习带来了困难。为了解决这个挑战，我们应用了模型压缩技术，例如稀疏化和权重聚类，然而这些技术通常需要修改底层的模型聚合方案或者涉及繁琐的超参数调整，后者不仅调整了模型的压缩率，还限制了模型在不断增长的数据上的持续改进潜力。在本文中，我们提出了一种新颖的方法FedCompress，它结合了动态权重聚类和服务器端知识蒸馏，以降低通信成本同时学习高度可泛化的模型。通过对多个公共数据集进行全面评估，我们证明了我们的方法相比于其他方法的有效性。

    Federated Learning (FL) is a promising technique for the collaborative training of deep neural networks across multiple devices while preserving data privacy. Despite its potential benefits, FL is hindered by excessive communication costs due to repeated server-client communication during training. To address this challenge, model compression techniques, such as sparsification and weight clustering are applied, which often require modifying the underlying model aggregation schemes or involve cumbersome hyperparameter tuning, with the latter not only adjusts the model's compression rate but also limits model's potential for continuous improvement over growing data. In this paper, we propose FedCompress, a novel approach that combines dynamic weight clustering and server-side knowledge distillation to reduce communication costs while learning highly generalizable models. Through a comprehensive evaluation on diverse public datasets, we demonstrate the efficacy of our approach compared to
    
[^27]: 在深度学习与极值统计之间：对滑坡灾害定义的形式化

    At the junction between deep learning and statistics of extremes: formalizing the landslide hazard definition. (arXiv:2401.14210v1 [cs.LG])

    [http://arxiv.org/abs/2401.14210](http://arxiv.org/abs/2401.14210)

    该论文通过使用深度学习和极值理论驱动的模型，开发了一个统一的方法来估计滑坡灾害。这种方法将滑坡位置的空间信息、威胁程度和频率结合起来，填补了目前在处理大范围地区时仅考虑两个元素的不足之处。

    

    滑坡灾害的最常用定义结合了滑坡位置的空间信息（易发性）、威胁程度（强度）和频率（重现期）。在处理大范围地区时，通常只考虑和估计前两个元素。即便如此，分离的模型仍是标准，对频率的研究很少。频率和强度相互交织并相互依赖，因为更大规模的事件发生的频率较低，反之亦然。然而，由于缺乏多时期清单和联合统计模型，通过统一的灾害模型对这些属性进行建模一直是具有挑战性的，尚未尝试过。在这里，我们开发了一个统一模型，以单元坡度水平估计滑坡灾害，以解决这些差距。我们采用深度学习结合极值理论驱动的模型，分析了尼泊尔30年土壤降雨引发滑坡的清单，并评估了多个地区的滑坡灾害。

    The most adopted definition of landslide hazard combines spatial information about landslide location (susceptibility), threat (intensity), and frequency (return period). Only the first two elements are usually considered and estimated when working over vast areas. Even then, separate models constitute the standard, with frequency being rarely investigated. Frequency and intensity are intertwined and depend on each other because larger events occur less frequently and vice versa. However, due to the lack of multi-temporal inventories and joint statistical models, modelling such properties via a unified hazard model has always been challenging and has yet to be attempted. Here, we develop a unified model to estimate landslide hazard at the slope unit level to address such gaps. We employed deep learning, combined with a model motivated by extreme-value theory to analyse an inventory of 30 years of observed rainfall-triggered landslides in Nepal and assess landslide hazard for multiple r
    
[^28]: MTRGL：通过多模态时间关系图学习有效辨识时间相关性

    MTRGL:Effective Temporal Correlation Discerning through Multi-modal Temporal Relational Graph Learning. (arXiv:2401.14199v1 [cs.LG])

    [http://arxiv.org/abs/2401.14199](http://arxiv.org/abs/2401.14199)

    该论文引入了一种新的框架——多模态时间关系图学习（MTRGL），通过将时间序列数据和离散特征结合成一个时间图，并采用记忆机制的图神经网络来辨识实体间的时间相关性，取得了在实证实验中的优异表现。这一研究对于提升自动化配对交易策略具有重要意义。

    

    本研究探索了深度学习与金融市场应用之间的协同作用，重点关注配对交易。这种市场中性策略对量化金融至关重要，并且适用于先进的深度学习技术。配对交易中的一个关键挑战是辨识实体之间的时间相关性，这要求整合多样化的数据模态。为了应对这一挑战，我们介绍了一种新的框架，即多模态时间关系图学习（MTRGL）。MTRGL将时间序列数据和离散特征结合到一个时间图中，并采用了基于记忆的时间图神经网络。这种方法将时间相关性识别重新定义为一个时间图链路预测任务，取得了实证成功。我们在真实数据集上的实验验证了MTRGL的卓越性能，强调其在优化自动化配对交易策略方面的潜力。

    In this study, we explore the synergy of deep learning and financial market applications, focusing on pair trading. This market-neutral strategy is integral to quantitative finance and is apt for advanced deep-learning techniques. A pivotal challenge in pair trading is discerning temporal correlations among entities, necessitating the integration of diverse data modalities. Addressing this, we introduce a novel framework, Multi-modal Temporal Relation Graph Learning (MTRGL). MTRGL combines time series data and discrete features into a temporal graph and employs a memory-based temporal graph neural network. This approach reframes temporal correlation identification as a temporal graph link prediction task, which has shown empirical success. Our experiments on real-world datasets confirm the superior performance of MTRGL, emphasizing its promise in refining automated pair trading strategies.
    
[^29]: DeepSeek-Coder: 在大型语言模型与编程相遇的时候--代码智能的崛起

    DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence. (arXiv:2401.14196v1 [cs.SE])

    [http://arxiv.org/abs/2401.14196](http://arxiv.org/abs/2401.14196)

    DeepSeek-Coder是一系列开源代码模型，通过在高质量项目级代码语料库上进行预训练和采用填空任务和16K窗口来增强代码生成和填充，不仅在多个基准测试中取得了与开源代码模型同样的最新表现，而且超过了现有的闭源模型。

    

    大型语言模型的快速发展为软件开发中的代码智能带来了革命。然而，闭源模型的主导地位限制了广泛的研究和开发。为了解决这个问题，我们介绍了DeepSeek-Coder系列，这是一系列开源代码模型，大小从1.3B到33B，从头开始在2万亿个标记上进行训练。这些模型在高质量项目级代码语料库上进行了预训练，并采用填空任务和16K窗口来增强代码生成和填充。我们广泛的评估表明，DeepSeek-Coder不仅在多个基准测试中取得了与开源代码模型同样的最新表现，而且超过了现有的Codex和GPT-3.5等闭源模型。此外，DeepSeek-Coder模型采用了宽松的许可证，既允许研究，也允许无限制的商业使用。

    The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.
    
[^30]: 如何让大型语言模型理解时空数据？

    How Can Large Language Models Understand Spatial-Temporal Data?. (arXiv:2401.14192v1 [cs.LG])

    [http://arxiv.org/abs/2401.14192](http://arxiv.org/abs/2401.14192)

    本文提出了一种名为STG-LLM的创新方法，用于使大型语言模型能够理解时空数据并进行预测。该方法利用STG-Tokenizer将复杂的图形数据转化为简洁的标记，再通过STG-Adapter将标记化数据与LLM的理解能力进行连接。通过微调参数，STG-LLM能够有效地把握标记的语义，同时保留LLM的自然语言理解能力。通过广泛的实验验证了STG-LLM的优越性能。

    

    尽管大型语言模型（LLM）在自然语言处理和计算机视觉等任务中占据主导地位，但利用它们的能力进行时空预测仍然具有挑战性。时序文本与复杂的时空数据之间的差异阻碍了该应用的实现。为了解决这个问题，本文提出了STG-LLM，一种创新的方法，为LLM赋予了时空预测的能力。我们通过以下方式解决数据不匹配的问题：1）STG-Tokenizer：这个时空图形标记器将复杂的图形数据转化为简洁的标记，捕捉了空间和时间关系；2）STG-Adapter：这个精简的适配器由线性编码和解码层组成，填补了标记化数据和LLM理解之间的差距。通过仅微调一小部分参数，它可以有效地把握STG-Tokenizer生成的标记的语义，同时保留LLM的原始自然语言理解能力。通过在多种数据集上进行广泛实验，我们验证了STG-LLM的优越性能。

    While Large Language Models (LLMs) dominate tasks like natural language processing and computer vision, harnessing their power for spatial-temporal forecasting remains challenging. The disparity between sequential text and complex spatial-temporal data hinders this application. To address this issue, this paper introduces STG-LLM, an innovative approach empowering LLMs for spatial-temporal forecasting. We tackle the data mismatch by proposing: 1) STG-Tokenizer: This spatial-temporal graph tokenizer transforms intricate graph data into concise tokens capturing both spatial and temporal relationships; 2) STG-Adapter: This minimalistic adapter, consisting of linear encoding and decoding layers, bridges the gap between tokenized data and LLM comprehension. By fine-tuning only a small set of parameters, it can effectively grasp the semantics of tokens generated by STG-Tokenizer, while preserving the original natural language understanding capabilities of LLMs. Extensive experiments on diver
    
[^31]: 通过友好攻击来提高信道编码的可靠性

    Friendly Attacks to Improve Channel Coding Reliability. (arXiv:2401.14184v1 [cs.IT])

    [http://arxiv.org/abs/2401.14184](http://arxiv.org/abs/2401.14184)

    本文引入了一种名为“友好攻击”的新方法，通过在传输前对码字进行轻微扰动，有效提高了纠错信道编码的性能，提高了可靠性。

    

    本论文引入了一种名为“友好攻击”的新方法，旨在提高纠错信道编码的性能。受到对抗性攻击概念的启发，我们的方法利用对神经网络输入进行轻微扰动的想法，对网络性能产生了重大影响。通过在传输前向固定点调制的码字引入小的扰动，我们有效地提高了解码器的性能，而不违反输入功率约束。扰动设计由修改过的迭代快速梯度方法完成。本研究考察了适用于计算梯度以获得所需扰动的各种解码器架构。具体而言，我们考虑了在LDPC码中的置信传播（BP），在极化码中的误差纠正码变换器、BP和神经BP（NBP），以及在卷积码中的神经BCJR。我们证明了所提出的友好攻击方法可以提高可靠性。

    This paper introduces a novel approach called "friendly attack" aimed at enhancing the performance of error correction channel codes. Inspired by the concept of adversarial attacks, our method leverages the idea of introducing slight perturbations to the neural network input, resulting in a substantial impact on the network's performance. By introducing small perturbations to fixed-point modulated codewords before transmission, we effectively improve the decoder's performance without violating the input power constraint. The perturbation design is accomplished by a modified iterative fast gradient method. This study investigates various decoder architectures suitable for computing gradients to obtain the desired perturbations. Specifically, we consider belief propagation (BP) for LDPC codes; the error correcting code transformer, BP and neural BP (NBP) for polar codes, and neural BCJR for convolutional codes. We demonstrate that the proposed friendly attack method can improve the relia
    
[^32]: 缓解图形异常检测中的结构分布偏移问题

    Alleviating Structural Distribution Shift in Graph Anomaly Detection. (arXiv:2401.14155v1 [cs.LG])

    [http://arxiv.org/abs/2401.14155](http://arxiv.org/abs/2401.14155)

    这项研究从特征视角解决了图形异常检测中的结构分布偏移问题，通过抵抗异常节点的高异质性，同时从同质邻居中受益于正常节点的学习。

    

    图形异常检测是一个具有挑战性的二分类问题，由于异常节点和正常节点之间的结构分布不同，使得该问题变得困难。异常节点是少数，因此与正常节点相比，具有较高的异质性和较低的同质性。此外，由于各种时间因素和人类专家的注释偏好，异质性和同质性在训练和测试数据之间可能会发生变化，这在本文中被称为结构分布偏移（SDS）。主流方法是基于图神经网络（GNN）构建的，通过聚合同质邻居有利于正常节点的分类，但忽视了异常节点的SDS问题，导致泛化能力差。本研究从特征视角解决了这个问题。我们观察到SDS的程度在异常节点和正常节点之间有所不同。因此，解决这个问题的关键在于对异常节点抵抗高异质性的同时，从同质邻居中受益于正常节点的学习。

    Graph anomaly detection (GAD) is a challenging binary classification problem due to its different structural distribution between anomalies and normal nodes -- abnormal nodes are a minority, therefore holding high heterophily and low homophily compared to normal nodes. Furthermore, due to various time factors and the annotation preferences of human experts, the heterophily and homophily can change across training and testing data, which is called structural distribution shift (SDS) in this paper. The mainstream methods are built on graph neural networks (GNNs), benefiting the classification of normals from aggregating homophilous neighbors, yet ignoring the SDS issue for anomalies and suffering from poor generalization.  This work solves the problem from a feature view. We observe that the degree of SDS varies between anomalies and normal nodes. Hence to address the issue, the key lies in resisting high heterophily for anomalies meanwhile benefiting the learning of normals from homophi
    
[^33]: 真知来源于实践：通过强化学习使LLMs与具身环境对齐的方法研究

    True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning. (arXiv:2401.14151v1 [cs.LG])

    [http://arxiv.org/abs/2401.14151](http://arxiv.org/abs/2401.14151)

    本研究通过使用大型语言模型（LLMs）作为决策智能体，通过强化学习与具身环境高效互动来解决LLMs与环境之间知识不对齐的问题。通过查询LLMs的联合概率，形成行为策略，并通过两种归一化方法和四个提示设计原则提高策略的稳定性和鲁棒性。最后，通过设计参数高效的训练架构提高学习效率。

    

    尽管在众多任务中取得了令人印象深刻的表现，但大型语言模型（LLMs）在解决简单的决策任务上经常失败，原因是LLMs中的知识与环境不对齐。相反，强化学习（RL）智能体从零开始学习策略，这使得它们始终与环境保持一致，但难以将先前的知识整合到其中以进行有效的探索。为了缩小这一差距，我们提出了TWOSOME，一种新颖的在线框架，利用LLMs作为决策智能体，通过RL与具身环境高效互动并实现对齐，而无需任何准备好的数据集或环境的先前知识。首先，我们使用LLMs查询每个有效动作的联合概率以形成行为策略。然后，为了增强策略的稳定性和鲁棒性，我们提出了两种归一化方法，并总结了四个提示设计原则。最后，我们设计了一种新颖的参数高效的训练架构，其中包括一个行为评估和选择算法来提高学习效率。

    Despite the impressive performance across numerous tasks, large language models (LLMs) often fail in solving simple decision-making tasks due to the misalignment of the knowledge in LLMs with environments. On the contrary, reinforcement learning (RL) agents learn policies from scratch, which makes them always align with environments but difficult to incorporate prior knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a novel general online framework that deploys LLMs as decision-making agents to efficiently interact and align with embodied environments via RL without requiring any prepared datasets or prior knowledge of the environments. Firstly, we query the joint probabilities of each valid action with LLMs to form behavior policies. Then, to enhance the stability and robustness of the policies, we propose two normalization methods and summarize four prompt design principles. Finally, we design a novel parameter-efficient training architecture where the acto
    
[^34]: 基于能量的概念瓶颈模型：统一预测、概念干预和条件解释

    Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Conditional Interpretations. (arXiv:2401.14142v1 [cs.CV])

    [http://arxiv.org/abs/2401.14142](http://arxiv.org/abs/2401.14142)

    基于能量的概念瓶颈模型统一了预测、概念干预和条件解释的功能，解决了现有方法在高阶非线性相互作用和复杂条件依赖关系上的限制。

    

    现有方法，如概念瓶颈模型 (CBM)，在为黑盒深度学习模型提供基于概念的解释方面取得了成功。它们通常通过在给定输入的情况下预测概念，然后在给定预测的概念的情况下预测最终的类别标签。然而，它们经常无法捕捉到概念之间的高阶非线性相互作用，例如纠正一个预测的概念（例如“黄色胸部”）无法帮助纠正高度相关的概念（例如“黄色腹部”），导致最终准确率不理想；它们无法自然地量化不同概念和类别标签之间的复杂条件依赖关系（例如对于一个带有类别标签“Kentucky Warbler”和概念“黑色嘴巴”的图像，模型能够正确预测另一个概念“黑色冠”的概率是多少），因此无法提供关于黑盒模型工作原理更深层次的洞察。针对这些限制，我们提出了基于能量的概念瓶颈模型（Energy-based Concept Bottleneck Models）。

    Existing methods, such as concept bottleneck models (CBMs), have been successful in providing concept-based interpretations for black-box deep learning models. They typically work by predicting concepts given the input and then predicting the final class label given the predicted concepts. However, (1) they often fail to capture the high-order, nonlinear interaction between concepts, e.g., correcting a predicted concept (e.g., "yellow breast") does not help correct highly correlated concepts (e.g., "yellow belly"), leading to suboptimal final accuracy; (2) they cannot naturally quantify the complex conditional dependencies between different concepts and class labels (e.g., for an image with the class label "Kentucky Warbler" and a concept "black bill", what is the probability that the model correctly predicts another concept "black crown"), therefore failing to provide deeper insight into how a black-box model works. In response to these limitations, we propose Energy-based Concept Bot
    
[^35]: 卷积神经网络可以实现二元保释判断分类

    Convolutional Neural Networks can achieve binary bail judgement classification. (arXiv:2401.14135v1 [cs.CL])

    [http://arxiv.org/abs/2401.14135](http://arxiv.org/abs/2401.14135)

    本文研究使用卷积神经网络在印度印地文法律文件上进行二元保释判断分类，取得了93％的准确率，改进了之前的准确率基准。

    

    印度法律领域中缺乏机器学习（ML）的实施，该领域的任何研究通常是基于英语数据和高等法院的数据。忽略了印度低级法院和不同地区语言的数据。本文在一组印地文法律文件上部署了卷积神经网络（CNN）架构。我们使用CNN模型进行保释预测任务，取得了93％的总体准确率，这是对印度北方邦20个地区数据的Kapoor等人（2022）设定的准确率基准的改进。

    There is an evident lack of implementation of Machine Learning (ML) in the legal domain in India, and any research that does take place in this domain is usually based on data from the higher courts of law and works with English data. The lower courts and data from the different regional languages of India are often overlooked. In this paper, we deploy a Convolutional Neural Network (CNN) architecture on a corpus of Hindi legal documents. We perform a bail Prediction task with the help of a CNN model and achieve an overall accuracy of 93\% which is an improvement on the benchmark accuracy, set by Kapoor et al. (2022), albeit in data from 20 districts of the Indian state of Uttar Pradesh.
    
[^36]: 等变流形神经常微分方程与微分不变量

    Equivariant Manifold Neural ODEs and Differential Invariants. (arXiv:2401.14131v1 [cs.LG])

    [http://arxiv.org/abs/2401.14131](http://arxiv.org/abs/2401.14131)

    本文提出了一种等变流形神经常微分方程的几何框架，利用微分不变量和嵌入等变流形中的增广形式进行建模和逼近。

    

    在本文中，我们为等变流形神经常微分方程（NODE）开发了一种明显几何的框架，并使用它来分析它们在对称数据建模方面的能力。首先，我们考虑Lie群G在光滑流形M上的作用，并建立了向量场的等变性、相应柯西问题的对称性以及关联的NODE的等变性之间的等价关系。我们还提出了一种关于G在M上作用的微分不变量的等变NODE新形式，基于Lie理论来描述微分方程对称性的参数化方法，既对M也对G都是不可知的。其次，我们通过嵌入等变流中构造了增广流形NODE，并证明它们是任何路径连通M上等变微分同胚的通用逼近器。

    In this paper we develop a manifestly geometric framework for equivariant manifold neural ordinary differential equations (NODEs), and use it to analyse their modelling capabilities for symmetric data. First, we consider the action of a Lie group $G$ on a smooth manifold $M$ and establish the equivalence between equivariance of vector fields, symmetries of the corresponding Cauchy problems, and equivariance of the associated NODEs. We also propose a novel formulation of the equivariant NODEs in terms of the differential invariants of the action of $G$ on $M$, based on Lie theory for symmetries of differential equations, which provides an efficient parameterisation of the space of equivariant vector fields in a way that is agnostic to both the manifold $M$ and the symmetry group $G$. Second, we construct augmented manifold NODEs, through embeddings into equivariant flows, and show that they are universal approximators of equivariant diffeomorphisms on any path-connected $M$. Furthermore
    
[^37]: 基于注意力的高效分类算法用于阿尔茨海默病的3D MRI图像

    Attention-based Efficient Classification for 3D MRI Image of Alzheimer's Disease. (arXiv:2401.14130v1 [eess.IV])

    [http://arxiv.org/abs/2401.14130](http://arxiv.org/abs/2401.14130)

    本研究提出了一种基于注意力机制的高效阿尔茨海默病分类算法，通过使用预训练的ResNet网络作为骨干，并结合后融合算法和注意机制，准确捕捉了 MRI 图像中的重要特征，使诊断准确性得到提高。

    

    早期诊断阿尔茨海默病是一个具有挑战性的任务，因为其细微复杂的临床症状。在这个领域中，使用深度学习辅助的医学诊断和图像识别技术已经成为一个重要的研究课题。特征必须准确捕捉解剖结构的主要变化，然而，通过深度学习训练进行特征提取是昂贵且耗时的。本研究提出了一种基于卷积神经网络的新型阿尔茨海默病检测模型。该模型利用预训练的ResNet网络作为骨干，结合后融合算法和注意机制来处理3D医学图像。实验结果表明，采用的2D融合算法有效地提高了模型的训练效率。引入的注意机制准确地对图像中的重要区域进行加权，进一步提高了模型的诊断准确性。

    Early diagnosis of Alzheimer Diagnostics (AD) is a challenging task due to its subtle and complex clinical symptoms. Deep learning-assisted medical diagnosis using image recognition techniques has become an important research topic in this field. The features have to accurately capture main variations of anatomical brain structures. However, time-consuming is expensive for feature extraction by deep learning training. This study proposes a novel Alzheimer's disease detection model based on Convolutional Neural Networks. The model utilizes a pre-trained ResNet network as the backbone, incorporating post-fusion algorithm for 3D medical images and attention mechanisms. The experimental results indicate that the employed 2D fusion algorithm effectively improves the model's training expense. And the introduced attention mechanism accurately weights important regions in images, further enhancing the model's diagnostic accuracy.
    
[^38]: FP6-LLM: 通过FP6中心算法-系统协同设计高效提供大型语言模型

    FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design. (arXiv:2401.14112v1 [cs.LG])

    [http://arxiv.org/abs/2401.14112](http://arxiv.org/abs/2401.14112)

    FP6-LLM提出了一种支持六位量化的GPU算法-系统协同设计方案，实现了在大型语言模型中推断成本和模型质量之间的平衡。

    

    六位量化（FP6）可以有效地减小大型语言模型（LLM）的大小，并在不同应用中保持模型质量的一致性。然而，现有系统不提供FP6量化的张量核心支持，并且在LLM推断过程中很难实现实际性能改进。由于（1）模型权重具有不规则位宽的不友好内存访问和（2）权重去量化的高运行时开销，支持在GPU上进行FP6量化是具有挑战性的。为了解决这些问题，我们提出了TC-FPx，这是第一个具有统一张量核心支持的浮点权重的完整GPU内核设计方案，适用于各种量化位宽。我们将TC-FPx内核集成到现有推断系统中，提供了新的端到端支持（称为FP6-LLM）用于量化LLM推断，从而实现了推断成本和模型质量之间更好的平衡。实验证明，FP6-LLM仅使用一部分存储空间就可以进行LLaMA-70b的推断。

    Six-bit quantization (FP6) can effectively reduce the size of large language models (LLMs) and preserve the model quality consistently across varied applications. However, existing systems do not provide Tensor Core support for FP6 quantization and struggle to achieve practical performance improvements during LLM inference. It is challenging to support FP6 quantization on GPUs due to (1) unfriendly memory access of model weights with irregular bit-width and (2) high runtime overhead of weight de-quantization. To address these problems, we propose TC-FPx, the first full-stack GPU kernel design scheme with unified Tensor Core support of float-point weights for various quantization bit-width. We integrate TC-FPx kernel into an existing inference system, providing new end-to-end support (called FP6-LLM) for quantized LLM inference, where better trade-offs between inference cost and model quality are achieved. Experiments show that FP6-LLM enables the inference of LLaMA-70b using only a sin
    
[^39]: 以较低比特宽度的累加器降低深度网络推理成本

    Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators. (arXiv:2401.14110v1 [cs.LG])

    [http://arxiv.org/abs/2401.14110](http://arxiv.org/abs/2401.14110)

    本论文提出了一种简单的方法，可以训练和微调高端DNN，实现使用更便宜的12位累加器，而不会导致精度降低，并展示了使用细粒度梯度近似可以提高DNN的精度。

    

    目前大多数关于深度神经网络（DNN）量化的研究都着重于降低高级框架可见的张量精度（例如权重、激活和梯度）。然而，当前的硬件仍然依赖于高精度的核心操作，最重要的是累加乘积的运算。这种高精度累加运算逐渐成为主要的计算瓶颈。这是因为到目前为止，低精度累加器的使用导致了性能的显著降低。在这项工作中，我们提出了一种简单的方法来训练和微调高端DNN，首次实现使用更便宜的12位累加器，而不会导致显著的精度降低。最后，我们还展示出随着累加精度进一步降低，使用细粒度梯度近似可以提高DNN的精度。

    The majority of the research on the quantization of Deep Neural Networks (DNNs) is focused on reducing the precision of tensors visible by high-level frameworks (e.g., weights, activations, and gradients). However, current hardware still relies on high-accuracy core operations. Most significant is the operation of accumulating products. This high-precision accumulation operation is gradually becoming the main computational bottleneck. This is because, so far, the usage of low-precision accumulators led to a significant degradation in performance. In this work, we present a simple method to train and fine-tune high-end DNNs, to allow, for the first time, utilization of cheaper, $12$-bits accumulators, with no significant degradation in accuracy. Lastly, we show that as we decrease the accumulation precision further, using fine-grained gradient approximations can improve the DNN accuracy.
    
[^40]: CompactifAI: 使用量子启发的张量网络对大型语言模型进行极压缩

    CompactifAI: Extreme Compression of Large Language Models using Quantum-Inspired Tensor Networks. (arXiv:2401.14109v1 [cs.CL])

    [http://arxiv.org/abs/2401.14109](http://arxiv.org/abs/2401.14109)

    CompactifAI是一种使用量子启发的张量网络对大型语言模型进行极压缩的创新方法，相比于传统的压缩方法，它更注重模型的相关空间，实现更加可控和精细的压缩。

    

    大型语言模型（LLM）如ChatGPT和LlaMA在生成人工智能（AI）方面取得了快速进展，但其庞大的规模带来了重要挑战，如巨大的训练和推断成本、较大的能源需求以及现场部署的限制。传统的压缩方法如剪枝、蒸馏和低秩逼近主要关注减少网络中神经元的有效数量，而量化方法则侧重于降低单个权重的数值精度，以减小模型大小同时保持神经元数目不变。虽然这些压缩方法在实践中取得了相对成功，但没有令人信服的理由认为截断神经元的数量是一种最优策略。本文介绍了一种创新的LLM压缩方法CompactifAI，它使用量子启发的张量网络，而不是传统的压缩方法，更注重模型的相关空间，实现更加可控和精细的压缩。

    Large Language Models (LLMs) such as ChatGPT and LlaMA are advancing rapidly in generative Artificial Intelligence (AI), but their immense size poses significant challenges, such as huge training and inference costs, substantial energy demands, and limitations for on-site deployment. Traditional compression methods such as pruning, distillation, and low-rank approximation focus on reducing the effective number of neurons in the network, while quantization focuses on reducing the numerical precision of individual weights to reduce the model size while keeping the number of neurons fixed. While these compression methods have been relatively successful in practice, there's no compelling reason to believe that truncating the number of neurons is an optimal strategy. In this context, this paper introduces CompactifAI, an innovative LLM compression approach using quantum-inspired Tensor Networks that focuses on the model's correlation space instead, allowing for a more controlled, refined an
    
[^41]: 通过少样本人机协作改进实现噪声标签下的学习

    Learning under Label Noise through Few-Shot Human-in-the-Loop Refinement. (arXiv:2401.14107v1 [cs.LG])

    [http://arxiv.org/abs/2401.14107](http://arxiv.org/abs/2401.14107)

    本文提出了一种名为少样本人机协作改进（FHLR）的方法，用于解决可穿戴数据中的噪声标签学习问题。该方法通过使用弱标签学习种子模型，使用少量的专家修正对种子模型进行微调，并通过加权参数平均将种子模型和微调模型合并，以提高泛化能力和鲁棒性。

    

    可穿戴技术能够连续监测各种健康指标，如身体活动、心率、睡眠和压力水平。可穿戴数据的一个关键挑战是获取高质量的标签。与视频等模态不同，可穿戴数据不包含关于用户的物理表现的明显线索，通常需要丰富的元数据。因此，在为这样的数据标注时，标签噪声可能成为一个越来越棘手的问题。在本文中，我们提出了一种解决噪声标签学习的新方法，称为少样本人机协作改进（FHLR）。我们的方法首先使用弱标签学习一个种子模型。接下来，它使用少量的专家修正对种子模型进行微调。最后，通过加权参数平均将种子模型和微调模型合并，以实现更好的泛化能力和鲁棒性。我们在四个具有挑战性的任务和数据集上评估了我们的方法

    Wearable technologies enable continuous monitoring of various health metrics, such as physical activity, heart rate, sleep, and stress levels. A key challenge with wearable data is obtaining quality labels. Unlike modalities like video where the videos themselves can be effectively used to label objects or events, wearable data do not contain obvious cues about the physical manifestation of the users and usually require rich metadata. As a result, label noise can become an increasingly thorny issue when labeling such data. In this paper, we propose a novel solution to address noisy label learning, entitled Few-Shot Human-in-the-Loop Refinement (FHLR). Our method initially learns a seed model using weak labels. Next, it fine-tunes the seed model using a handful of expert corrections. Finally, it achieves better generalizability and robustness by merging the seed and fine-tuned models via weighted parameter averaging. We evaluate our approach on four challenging tasks and datasets, and c
    
[^42]: McUDI: 模型中心的无监督退化指示器，用于故障预测AIOps解决方案

    McUDI: Model-Centric Unsupervised Degradation Indicator for Failure Prediction AIOps Solutions. (arXiv:2401.14093v1 [cs.SE])

    [http://arxiv.org/abs/2401.14093](http://arxiv.org/abs/2401.14093)

    本文提出了一种模型中心的无监督退化指示器McUDI，用于检测AIOps模型由于数据变化而需要重新训练的时机，并展示了在AIOps解决方案中应用McUDI可以减少标记样本数量的方法。

    

    由于运行数据不断变化，AIOps解决方案的性能会随时间下降。虽然周期性重训练是保持故障预测AIOps模型性能的最新技术，但该技术需要大量标记数据进行重新训练。在AIOps中获取标记数据是昂贵的，因为它需要领域专家的支持进行密集的注释。本文提出了一种模型中心的无监督退化指示器McUDI，它能够检测到AIOps模型由于数据变化而需要重新训练的确切时刻。我们进一步展示了如何在AIOps解决方案的维护流程中应用McUDI，可以在保持周期性重训练的类似性能的情况下，减少需要进行注释的样本数：对于作业故障预测需要30k个样本，对于磁盘故障预测需要260k个样本。

    Due to the continuous change in operational data, AIOps solutions suffer from performance degradation over time. Although periodic retraining is the state-of-the-art technique to preserve the failure prediction AIOps models' performance over time, this technique requires a considerable amount of labeled data to retrain. In AIOps obtaining label data is expensive since it requires the availability of domain experts to intensively annotate it. In this paper, we present McUDI, a model-centric unsupervised degradation indicator that is capable of detecting the exact moment the AIOps model requires retraining as a result of changes in data. We further show how employing McUDI in the maintenance pipeline of AIOps solutions can reduce the number of samples that require annotations with 30k for job failure prediction and 260k for disk failure prediction while achieving similar performance with periodic retraining.
    
[^43]: 自动化网络威胁归因的模块化方法：利用舆情池

    A Modular Approach to Automatic Cyber Threat Attribution using Opinion Pools. (arXiv:2401.14090v1 [cs.CR])

    [http://arxiv.org/abs/2401.14090](http://arxiv.org/abs/2401.14090)

    本论文提出了一种模块化架构，利用舆情池来自动化网络威胁归因过程。同时，还提出了一种匹配聚合器作为聚合方法，以增加可追踪性和解释性。

    

    网络威胁归因在增加对数字威胁的抵御能力方面起着重要作用。最近的研究侧重于自动化威胁归因过程并将其与其他工作集成在一起，如威胁侦测。为了支持网络威胁归因过程的自动化，本文提出了一种模块化架构作为当前整体式自动化方法的替代方案。模块化架构可以利用舆情池来结合具体归因器的输出。所提出的解决方案增加了威胁归因问题的可追踪性，并提供了更高的可用性和可解释性，与整体式方法相比。此外，本文还提出了一种匹配聚合器作为一种聚合方法，根据不同的特征对归因器进行配对，生成中间结果，然后最终生成单个概率质量函数 (Probability Mass Function, PMF) 作为输出。匹配聚合器依次应用了对数舆情和累加运算来生成最终的PMF。

    Cyber threat attribution can play an important role in increasing resilience against digital threats. Recent research focuses on automating the threat attribution process and on integrating it with other efforts, such as threat hunting. To support increasing automation of the cyber threat attribution process, this paper proposes a modular architecture as an alternative to current monolithic automated approaches. The modular architecture can utilize opinion pools to combine the output of concrete attributors. The proposed solution increases the tractability of the threat attribution problem and offers increased usability and interpretability, as opposed to monolithic alternatives. In addition, a Pairing Aggregator is proposed as an aggregation method that forms pairs of attributors based on distinct features to produce intermediary results before finally producing a single Probability Mass Function (PMF) as output. The Pairing Aggregator sequentially applies both the logarithmic opinion
    
[^44]: 使用Sum-Product Networks生成可能的反事实推理

    Generating Likely Counterfactuals Using Sum-Product Networks. (arXiv:2401.14086v1 [cs.AI])

    [http://arxiv.org/abs/2401.14086](http://arxiv.org/abs/2401.14086)

    由于用户需求和最近的法规要求，需要对AI系统所做出的决策进行解释。本论文提出了一种使用Sum-Product Networks模拟寻找高可能性反事实推理的系统，该系统能够提供满足多个常见要求的最佳解释。

    

    由于用户需求和最近的法规（GDPR、AI法案），需要解释AI系统所做出的决策。这些决策往往只能在事后解释，反事实推理成为常见的解释方式。什么构成了最佳的反事实解释必须考虑多个方面，其中“样本距离”是最常见的。我们认为，这一要求经常会导致不太可能且因此价值有限的解释。在这里，我们提出了一个能够提供高可能性解释的系统。我们展示了使用混合整数优化（MIO）模拟寻找满足反事实推理的许多常见要求的最有可能解释。在此过程中，我们提出了Sum-Product Network（SPN）的MIO表达，并使用SPN估计反事实的可能性，这对独立的兴趣也有用。与生成反事实解释的几种方法进行数值比较。

    Due to user demand and recent regulation (GDPR, AI Act), decisions made by AI systems need to be explained. These decisions are often explainable only post hoc, where counterfactual explanations are popular. The question of what constitutes the best counterfactual explanation must consider multiple aspects, where "distance from the sample" is the most common. We argue that this requirement frequently leads to explanations that are unlikely and, therefore, of limited value. Here, we present a system that provides high-likelihood explanations. We show that the search for the most likely explanations satisfying many common desiderata for counterfactual explanations can be modeled using mixed-integer optimization (MIO). In the process, we propose an MIO formulation of a Sum-Product Network (SPN) and use the SPN to estimate the likelihood of a counterfactual, which can be of independent interest. A numerical comparison against several methods for generating counterfactual explanations is pr
    
[^45]: 使用导数的操作矩阵加速分数PINNs

    Accelerating Fractional PINNs using Operational Matrices of Derivative. (arXiv:2401.14081v1 [cs.LG])

    [http://arxiv.org/abs/2401.14081](http://arxiv.org/abs/2401.14081)

    本文提出了一种操作矩阵方法来加速分数PINNs的训练，通过非均匀离散化分数Caputo算子，实现了对分数导数的快速计算。使用Legendre神经块（LNB）架构提高了PINNs的准确性。

    

    本文提出了一种新颖的操作矩阵方法，用于加速分数物理信息神经网络（fPINNs）的训练。我们的方法涉及对分数Caputo算子进行非均匀离散化，便于在Caputo型分数微分问题中计算分数导数。在这种方法中，操作矩阵是预先计算的，在训练阶段，自动微分被一个矩阵-向量乘积替代。虽然我们的方法适用于任何网络，但我们特别强调其在PINNs中成功实现的情况，强调在使用Legendre神经块（LNB）架构时所实现的增强准确性。LNB将Legendre多项式融入到PINN结构中，极大地提高了准确性。我们提出的方法的有效性在包括时滞微分方程和微分方程系统在内的多种微分方程中得到验证。

    This paper presents a novel operational matrix method to accelerate the training of fractional Physics-Informed Neural Networks (fPINNs). Our approach involves a non-uniform discretization of the fractional Caputo operator, facilitating swift computation of fractional derivatives within Caputo-type fractional differential problems with $0<\alpha<1$. In this methodology, the operational matrix is precomputed, and during the training phase, automatic differentiation is replaced with a matrix-vector product. While our methodology is compatible with any network, we particularly highlight its successful implementation in PINNs, emphasizing the enhanced accuracy achieved when utilizing the Legendre Neural Block (LNB) architecture. LNB incorporates Legendre polynomials into the PINN structure, providing a significant boost in accuracy. The effectiveness of our proposed method is validated across diverse differential equations, including Delay Differential Equations (DDEs) and Systems of Diffe
    
[^46]: ProCNS: 用于弱监督医学图像分割的渐进式原型校准和噪声抑制

    ProCNS: Progressive Prototype Calibration and Noise Suppression for Weakly-Supervised Medical Image Segmentation. (arXiv:2401.14074v1 [cs.CV])

    [http://arxiv.org/abs/2401.14074](http://arxiv.org/abs/2401.14074)

    ProCNS是一种用于弱监督医学图像分割的新方法，采用渐进式原型校准和噪声抑制的原则来解决现有方法中的问题。

    

    弱监督分割（WSS）作为缓解注释成本和模型性能之间冲突的解决方案而出现，采用稀疏的注释格式（例如点、涂鸦、块等）。典型的方法试图利用解剖和拓扑先验将稀疏注释直接扩展为伪标签。然而，由于对医学图像中模糊边缘的关注不足和对稀疏监督的不充分探索，现有方法往往会在噪声区域生成错误且过于自信的伪建议，导致模型误差累积和性能下降。在这项工作中，我们提出了一种新颖的WSS方法，名为ProCNS，它包含两个协同模块，设计原则是渐进式原型校准和噪声抑制。具体而言，我们设计了一种基于原型的区域空间相似性（PRSA）损失函数，最大化空间和语义元素之间的成对相似度，为我们感兴趣的模型提供了

    Weakly-supervised segmentation (WSS) has emerged as a solution to mitigate the conflict between annotation cost and model performance by adopting sparse annotation formats (e.g., point, scribble, block, etc.). Typical approaches attempt to exploit anatomy and topology priors to directly expand sparse annotations into pseudo-labels. However, due to a lack of attention to the ambiguous edges in medical images and insufficient exploration of sparse supervision, existing approaches tend to generate erroneous and overconfident pseudo proposals in noisy regions, leading to cumulative model error and performance degradation. In this work, we propose a novel WSS approach, named ProCNS, encompassing two synergistic modules devised with the principles of progressive prototype calibration and noise suppression. Specifically, we design a Prototype-based Regional Spatial Affinity (PRSA) loss to maximize the pair-wise affinities between spatial and semantic elements, providing our model of interest 
    
[^47]: 神经Sinkhorn梯度流

    Neural Sinkhorn Gradient Flow. (arXiv:2401.14069v1 [cs.LG])

    [http://arxiv.org/abs/2401.14069](http://arxiv.org/abs/2401.14069)

    神经Sinkhorn梯度流模型通过参数化Wasserstein梯度流中的速度场，并利用速度场匹配训练方案实现高效的推理过程。其理论分析表明，随着样本大小趋近于无穷大，经验近似值的均场极限会收敛到真实的底层速度场。

    

    针对特定泛函的Wasserstein梯度流在机器学习领域得到了广泛应用。最近，人们采用神经网络来近似处理Wasserstein梯度流中的一些难以计算的部分，并得到了高效的推理过程。本文介绍了神经Sinkhorn梯度流（NSGF）模型，该模型将Wasserstein梯度流的时间变化速度场参数化为Sinkhorn距离到目标分布与给定源分布的关系。我们在NSGF中利用速度场匹配训练方案，该方案只需要来自源分布和目标分布的样本来计算经验速度场近似值。我们的理论分析表明，随着样本大小趋近于无穷大，经验近似值的均场极限会收敛到真实的底层速度场。为了进一步提高模型在高维任务上的效率，引入了一个两阶段的NSGF

    Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. Recently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model, which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution. We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. To further enhance model efficiency on high-dimensional tasks, a two-phase NS
    
[^48]: 在印度风电资源评估中，采用级联人工神经网络中的Relief算法的新应用来预测风速

    Novel application of Relief Algorithm in cascaded artificial neural network to predict wind speed for wind power resource assessment in India. (arXiv:2401.14065v1 [cs.LG])

    [http://arxiv.org/abs/2401.14065](http://arxiv.org/abs/2401.14065)

    该研究采用了新颖的Relief算法在级联人工神经网络中进行风速预测，相较于传统模型，该方法具有更高的准确性，该方法的关键在于选择最相关的输入参数。

    

    风能发电由于气象变量的随机性而具有不可预测的特性。因此，能源业务和风能发电的控制需要从几秒到不同时间步长提前预测风速。为了解决预测的不足，已经使用了各种风速预测方法。预测性数据挖掘提供了多种风速预测方法，其中人工神经网络（ANN）是一种可靠和准确的方法。研究结果表明，与传统模型相比，ANN具有更好的准确性。风速预测模型的准确性发现取决于输入参数和所使用的算法的架构类型。因此，选择最相关的输入参数是风速预测领域的重要研究领域。本文的目标有两个：首先对风能和风速预测的人工神经网络进行广泛的回顾。讨论和分析使用Relief算法进行特征选择的特征。

    Wind power generated by wind has non-schedule nature due to stochastic nature of meteorological variable. Hence energy business and control of wind power generation requires prediction of wind speed (WS) from few seconds to different time steps in advance. To deal with prediction shortcomings, various WS prediction methods have been used. Predictive data mining offers variety of methods for WS predictions where artificial neural network (ANN) is one of the reliable and accurate methods. It is observed from the result of this study that ANN gives better accuracy in comparison conventional model. The accuracy of WS prediction models is found to be dependent on input parameters and architecture type algorithms utilized. So the selection of most relevant input parameters is important research area in WS predicton field. The objective of the paper is twofold: first extensive review of ANN for wind power and WS prediction is carried out. Discussion and analysis of feature selection using Rel
    
[^49]: 左/右脑、人类运动控制及对机器人的影响

    Left/Right Brain, human motor control and the implications for robotics. (arXiv:2401.14057v1 [cs.RO])

    [http://arxiv.org/abs/2401.14057](http://arxiv.org/abs/2401.14057)

    本研究通过训练不同的损失函数，实现了类似于人类的左右半球专门化控制系统，该系统在不同的运动任务中展现出协调性、运动效率和位置稳定性的优势。

    

    神经网络运动控制器相对传统控制方法具有各种优点，然而由于其无法产生可靠的精确运动，因此尚未得到广泛采用。本研究探讨了一种双侧神经网络架构作为运动任务的控制系统。我们旨在实现类似于人类在不同任务中观察到的半球专门化：优势系统（通常是右手、左半球）擅长协调和运动效率的任务，而非优势系统在需要位置稳定性的任务上表现更好。通过使用不同的损失函数对半球进行训练，实现了专门化。我们比较了具有专门化半球和无专门化半球、具有半球间连接（代表生物学脑桥）和无半球间连接、具有专门化和无专门化的单侧模型。

    Neural Network movement controllers promise a variety of advantages over conventional control methods however they are not widely adopted due to their inability to produce reliably precise movements. This research explores a bilateral neural network architecture as a control system for motor tasks. We aimed to achieve hemispheric specialisation similar to what is observed in humans across different tasks; the dominant system (usually the right hand, left hemisphere) excels at tasks involving coordination and efficiency of movement, and the non-dominant system performs better at tasks requiring positional stability. Specialisation was achieved by training the hemispheres with different loss functions tailored toward the expected behaviour of the respective hemispheres. We compared bilateral models with and without specialised hemispheres, with and without inter-hemispheric connectivity (representing the biological Corpus Callosum), and unilateral models with and without specialisation. 
    
[^50]: 扩展LipSDP以超越限制斜率激活函数的新二次约束

    Novel Quadratic Constraints for Extending LipSDP beyond Slope-Restricted Activations. (arXiv:2401.14033v1 [cs.LG])

    [http://arxiv.org/abs/2401.14033](http://arxiv.org/abs/2401.14033)

    本文提出了新的二次约束，将LipSDP方法扩展到斜率限制之外的激活函数，从而解决了该方法的主要限制问题。

    

    最近，半定规划（SDP）技术在为神经网络提供准确的Lipschitz界限方面显示出巨大的潜力。具体而言，LipSDP方法（Fazlyab等，2019）受到了广泛关注，并提供了可以在多项式时间保证下计算的最不保守的Lipschitz上界。然而，LipSDP的一个主要限制是其公式要求激活函数在[0,1]上具有限制斜率，从而阻止其进一步用于更一般的激活函数，如GroupSort、MaxMin和Householder。例如，可以将MaxMin激活重新写成残差ReLU网络。但是，将LipSDP直接应用于结果残差ReLU网络是保守的，甚至无法恢复MaxMin激活是1-Lipschitz的已知事实。本文弥合了这一差距，将LipSDP扩展到斜率限制之外的激活函数上。为此，我们提供了GroupSort、Ma的新二次约束。

    Recently, semidefinite programming (SDP) techniques have shown great promise in providing accurate Lipschitz bounds for neural networks. Specifically, the LipSDP approach (Fazlyab et al., 2019) has received much attention and provides the least conservative Lipschitz upper bounds that can be computed with polynomial time guarantees. However, one main restriction of LipSDP is that its formulation requires the activation functions to be slope-restricted on $[0,1]$, preventing its further use for more general activation functions such as GroupSort, MaxMin, and Householder. One can rewrite MaxMin activations for example as residual ReLU networks. However, a direct application of LipSDP to the resultant residual ReLU networks is conservative and even fails in recovering the well-known fact that the MaxMin activation is 1-Lipschitz. Our paper bridges this gap and extends LipSDP beyond slope-restricted activation functions. To this end, we provide novel quadratic constraints for GroupSort, Ma
    
[^51]: 稀疏可传递的通用奇异向量攻击

    Sparse and Transferable Universal Singular Vectors Attack. (arXiv:2401.14031v1 [cs.LG])

    [http://arxiv.org/abs/2401.14031](http://arxiv.org/abs/2401.14031)

    本文提出了一种稀疏可传递的通用奇异向量攻击方法，通过在隐藏层中利用截断幂迭代来获得稀疏的$(p,q)$-奇异向量。在ImageNet基准验证子集上的实验证明，该方法可以在仅破坏5%的像素和利用256个样本的情况下，达到与密集基线相当的愚弄率超过50%的结果。同时，该攻击方法不影响人类解决任务的能力，并且构造的扰动是高度可传递的。

    

    对抗攻击和模型的脆弱性研究是现代机器学习中的基本方向之一。最近的研究揭示了模型的脆弱性现象，了解其背后的机制对于改善神经网络的特性和可解释性至关重要。本文提出了一种新颖的稀疏通用白盒对抗攻击方法。我们的方法基于截断幂迭代，为雅可比矩阵的隐藏层的$(p,q)$-奇异向量提供稀疏性。在ImageNet基准验证子集上，我们在各种设置下分析了所提出的方法，实现了与密集基线相当的结果，愚弄率超过50%，但只破坏了5%的像素，并利用256个样本进行扰动拟合。我们还展示了我们的算法接受更高的攻击幅度，而不影响人类解决任务的能力。此外，我们还研究了构造的扰动是高度可传递的。

    The research in the field of adversarial attacks and models' vulnerability is one of the fundamental directions in modern machine learning. Recent studies reveal the vulnerability phenomenon, and understanding the mechanisms behind this is essential for improving neural network characteristics and interpretability. In this paper, we propose a novel sparse universal white-box adversarial attack. Our approach is based on truncated power iteration providing sparsity to $(p,q)$-singular vectors of the hidden layers of Jacobian matrices. Using the ImageNet benchmark validation subset, we analyze the proposed method in various settings, achieving results comparable to dense baselines with more than a 50% fooling rate while damaging only 5% of pixels and utilizing 256 samples for perturbation fitting. We also show that our algorithm admits higher attack magnitude without affecting the human ability to solve the task. Furthermore, we investigate that the constructed perturbations are highly tr
    
[^52]: 向算法的系统理论迈进

    Towards a Systems Theory of Algorithms. (arXiv:2401.14029v1 [math.OC])

    [http://arxiv.org/abs/2401.14029](http://arxiv.org/abs/2401.14029)

    向算法的系统理论迈进，将算法视为开放的动态系统与其他算法、物理系统、人类或数据库交互，提供宝贵的洞见。

    

    传统上，数值算法被视为独立的代码片段，局限于``in silico''存在。然而，这种观点不适用于许多现代计算方法，如控制、学习或优化中，其中``in vivo''算法与其环境进行交互。这些``开放''的例子包括各种实时基于优化的控制策略、强化学习、决策架构、在线优化等等。此外，即使在学习或优化中，``闭合''的算法也越来越多地被抽象为具有相互作用的动态模块和管道的块图。在这篇观点文章中，我们阐述了我们对即将发展的``算法的系统理论''的愿景，并主张将算法视为与其他算法、物理系统、人类或数据库交互的开放动态系统。值得注意的是，在系统理论的伞下开发的多种工具也提供了宝贵的洞见。

    Traditionally, numerical algorithms are seen as isolated pieces of code confined to an {\em in silico} existence. However, this perspective is not appropriate for many modern computational approaches in control, learning, or optimization, wherein {\em in vivo} algorithms interact with their environment. Examples of such {\em open} include various real-time optimization-based control strategies, reinforcement learning, decision-making architectures, online optimization, and many more. Further, even {\em closed} algorithms in learning or optimization are increasingly abstracted in block diagrams with interacting dynamic modules and pipelines. In this opinion paper, we state our vision on a to-be-cultivated {\em systems theory of algorithms} and argue in favour of viewing algorithms as open dynamical systems interacting with other algorithms, physical systems, humans, or databases. Remarkably, the manifold tools developed under the umbrella of systems theory also provide valuable insights
    
[^53]: 面向领域特定数据稀缺性和隐私问题的联邦学习的风险：扭曲微调特征并降低抗分布鲁棒性的影响

    The Risk of Federated Learning to Skew Fine-Tuning Features and Underperform Out-of-Distribution Robustness. (arXiv:2401.14027v1 [cs.LG])

    [http://arxiv.org/abs/2401.14027](http://arxiv.org/abs/2401.14027)

    联邦学习存在扭曲微调特征和损害模型抗分布鲁棒性的风险，为此我们引入了GNP算法来保证模型在目标分布上的准确性不会下降。

    

    为了解决与领域特定数据集相关的稀缺性和隐私问题，联邦学习与微调的整合已经成为一种实际解决方案。然而，我们发现联邦学习存在扭曲微调特征和损害模型的抗分布鲁棒性的风险。通过引入三个鲁棒性指标并在不同的鲁棒数据集上进行实验证明，我们通过分析模型特征空间中的多样性、可转移性和偏离程度来阐明这些现象。为了减轻联邦学习对模型鲁棒性的负面影响，我们引入了GNP，一种基于\textbf{G}eneral \textbf{N}oisy \textbf{P}rojection的鲁棒算法，确保在目标分布上准确性不会下降。具体来说，增强模型鲁棒性的关键策略是将鲁棒性从预训练模型转移到微调模型中，并结合添加一个小的...

    To tackle the scarcity and privacy issues associated with domain-specific datasets, the integration of federated learning in conjunction with fine-tuning has emerged as a practical solution. However, our findings reveal that federated learning has the risk of skewing fine-tuning features and compromising the out-of-distribution robustness of the model. By introducing three robustness indicators and conducting experiments across diverse robust datasets, we elucidate these phenomena by scrutinizing the diversity, transferability, and deviation within the model feature space. To mitigate the negative impact of federated learning on model robustness, we introduce GNP, a \underline{G}eneral \underline{N}oisy \underline{P}rojection-based robust algorithm, ensuring no deterioration of accuracy on the target distribution. Specifically, the key strategy for enhancing model robustness entails the transfer of robustness from the pre-trained model to the fine-tuned model, coupled with adding a sma
    
[^54]: DNA序列压缩器分类

    DNA Sequence Classification with Compressors. (arXiv:2401.14025v1 [q-bio.GN])

    [http://arxiv.org/abs/2401.14025](http://arxiv.org/abs/2401.14025)

    该论文介绍了一种专门用于DNA序列分析的新型分类方法，该方法利用压缩算法高效处理和分类基因组序列。这种方法不仅准确性高，而且比传统方法更节省计算资源。

    

    最近的DNA序列分类研究借鉴了复杂的机器学习技术，在对复杂的基因组数据进行分类时取得了显著的准确性。其中，像k-mer计数这样的方法已被证明在区分类似黑猩猩、狗和人类等不同物种的序列方面非常有效，在当代基因组研究中已经成为常用方法。然而，这些方法通常需要大量的计算资源，在可扩展性和效率方面面临挑战。为了解决这个问题，我们的研究引入了江等人基于压缩器的无参数分类方法的新型改进，专门用于DNA序列分析。这种创新方法利用多种压缩算法（如Gzip、Brotli和LZMA）来高效处理和分类基因组序列。这种方法不仅在准确性方面与现有技术保持一致，而且还提供了一种更加资源高效的替代方案。

    Recent studies in DNA sequence classification have leveraged sophisticated machine learning techniques, achieving notable accuracy in categorizing complex genomic data. Among these, methods such as k-mer counting have proven effective in distinguishing sequences from varied species like chimpanzees, dogs, and humans, becoming a staple in contemporary genomic research. However, these approaches often demand extensive computational resources, posing a challenge in terms of scalability and efficiency. Addressing this issue, our study introduces a novel adaptation of Jiang et al.'s compressor-based, parameter-free classification method, specifically tailored for DNA sequence analysis. This innovative approach utilizes a variety of compression algorithms, such as Gzip, Brotli, and LZMA, to efficiently process and classify genomic sequences. Not only does this method align with the current state-of-the-art in terms of accuracy, but it also offers a more resource-efficient alternative to trad
    
[^55]: 使用推测加速检索增强型语言模型服务

    Accelerating Retrieval-Augmented Language Model Serving with Speculation. (arXiv:2401.14021v1 [cs.LG])

    [http://arxiv.org/abs/2401.14021](http://arxiv.org/abs/2401.14021)

    提出了RaLMSpec，这是一个使用推测加速检索增强型语言模型服务的框架，通过推测式检索和批量验证提供了通用的加速效果，并通过进一步优化和并发处理，提高了性能。

    

    检索增强型语言模型（RaLM）通过将非参数的知识库与参数化的语言模型相结合，已经展示出解决知识密集型自然语言处理（NLP）任务的潜力。与对完全参数化模型进行微调不同，RaLM在适应最新数据和更好的来源归属机制方面具有低成本的优势。在众多的RaLM方法中，迭代式RaLM由于检索器与语言模型之间更频繁的互动而具有更好的生成质量。尽管有这些好处，迭代式RaLM通常会因为频繁的检索步骤而遇到高开销。为此，我们提出了RaLMSpec，这是一个基于推测的框架，通过推测式检索和批量验证，能够在保持相同模型输出的同时，提供通用加速的效果。通过进一步结合预取、最佳推测步幅调度器和异步验证，RaLMSpec能够自动利用并发性和并行性来最大程度地提高性能。

    Retrieval-augmented language models (RaLM) have demonstrated the potential to solve knowledge-intensive natural language processing (NLP) tasks by combining a non-parametric knowledge base with a parametric language model. Instead of fine-tuning a fully parametric model, RaLM excels at its low-cost adaptation to the latest data and better source attribution mechanisms. Among various RaLM approaches, iterative RaLM delivers a better generation quality due to a more frequent interaction between the retriever and the language model. Despite the benefits, iterative RaLM usually encounters high overheads due to the frequent retrieval step. To this end, we propose RaLMSpec, a speculation-inspired framework that provides generic speed-up over iterative RaLM while preserving the same model outputs through speculative retrieval and batched verification. By further incorporating prefetching, optimal speculation stride scheduler, and asynchronous verification, RaLMSpec can automatically exploit t
    
[^56]: 通过自适应变压器网络进行跨领域的少样本学习

    Cross-Domain Few-Shot Learning via Adaptive Transformer Networks. (arXiv:2401.13987v1 [cs.LG])

    [http://arxiv.org/abs/2401.13987](http://arxiv.org/abs/2401.13987)

    本文提出了一种名为ADAPTER的自适应变压器网络，用于跨领域的少样本学习。借助双向交叉注意力和标签平滑方法，ADAPTER能够在存在领域转移的情况下学习可转移的特征，并避免监督崩溃问题。该方法在BSCD-FSL基准测试中表现优秀，超过了之前的方法。

    

    大多数少样本学习方法依赖于基任务和目标任务之间的相同领域的假设，限制了它们的实际应用。本文提出了自适应变压器网络（ADAPTER），这是一种简单但有效的解决方案，用于处理基任务和目标任务之间存在大领域转移的跨领域少样本学习。ADAPTER基于双向交叉注意力的思想构建，以学习两个领域之间的可转移特征。所提出的架构使用DINO进行训练，以产生多样的、较少偏见的特征，以避免监督崩溃问题。此外，还提出了标签平滑方法，通过在嵌入空间中同时考虑接近样本的预测标签，提高预测的一致性和可靠性。ADAPTER的性能在BSCD-FSL基准测试中进行了严格评估，它以显著的优势超越了之前的方法。

    Most few-shot learning works rely on the same domain assumption between the base and the target tasks, hindering their practical applications. This paper proposes an adaptive transformer network (ADAPTER), a simple but effective solution for cross-domain few-shot learning where there exist large domain shifts between the base task and the target task. ADAPTER is built upon the idea of bidirectional cross-attention to learn transferable features between the two domains. The proposed architecture is trained with DINO to produce diverse, and less biased features to avoid the supervision collapse problem. Furthermore, the label smoothing approach is proposed to improve the consistency and reliability of the predictions by also considering the predicted labels of the close samples in the embedding space. The performance of ADAPTER is rigorously evaluated in the BSCD-FSL benchmarks in which it outperforms prior arts with significant margins.
    
[^57]: 通过解释一致性微调实现一致的自然语言解释

    Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning. (arXiv:2401.13986v1 [cs.CL])

    [http://arxiv.org/abs/2401.13986](http://arxiv.org/abs/2401.13986)

    本文提出了一种通过解释一致性微调方法，使得大型语言模型（LLMs）在相关示例上生成更一致的自然语言解释。实验证明，该方法在不同领域的问答数据集上相对提高了10.0%的解释一致性，并且能够泛化到其他数据集。

    

    大型语言模型（LLMs）通常能够生成令人信服、流畅的解释。然而，与人类不同，它们在不同输入上生成的解释常常不一致。例如，LLM在回答问题“麻雀能飞吗？”时可能生成解释“所有鸟都能飞”，但同时在回答与之相关的问题“企鹅能飞吗？”时回答“不行”。解释应该在相关示例中保持一致，以便让人类能够模拟LLM在多个示例上的决策过程。我们提出了解释一致性微调（EC-finetuning）方法，该方法通过适应LLM在相关示例上生成更一致的自然语言解释。EC-finetuning包括在经过精心构建的包含一致解释的合成数据上微调LLM。在各种不同领域的问答数据集上，EC-finetuning在四个微调数据集上相对提高了10.0%的解释一致性，并且在七个外部数据集上具有泛化性能。

    Large language models (LLMs) often generate convincing, fluent explanations. However, different from humans, they often generate inconsistent explanations on different inputs. For example, an LLM may generate the explanation "all birds can fly" when answering the question "Can sparrows fly?" but meanwhile answer "no" to the related question "Can penguins fly?". Explanations should be consistent across related examples so that they allow a human to simulate the LLM's decision process on multiple examples. We propose explanation-consistency finetuning (EC-finetuning), a method that adapts LLMs to generate more consistent natural-language explanations on related examples. EC-finetuning involves finetuning LLMs on synthetic data that is carefully constructed to contain consistent explanations. Across a variety of question-answering datasets in various domains, EC-finetuning yields a 10.0% relative explanation consistency improvement on four finetuning datasets, and generalizes to seven out
    
[^58]: Leeroo Orchestrator: 通过模型集成提高LLMs的性能

    Leeroo Orchestrator: Elevating LLMs Performance Through Model Integration. (arXiv:2401.13979v1 [cs.CL])

    [http://arxiv.org/abs/2401.13979](http://arxiv.org/abs/2401.13979)

    本研究提出了Leeroo编排器的架构，通过集成多个训练过的LLMs模型，实现了一个新的最先进模型。该编排器在性能上与Mixtral模型相当，并且成本只有其三分之二。当允许更高的成本时，Leeroo编排器的准确性超过了Mixtral模型，并且当集成GPT4时进一步提升。

    

    本文提出了一种架构，利用多个训练过的LLMs的集体知识，创建一个新的最先进模型。该框架的核心是一个基于LLM的编排器，能够选择最佳的底层LLM专家进行任务执行。受到强化学习中的自我对弈的启发，我们创建了一个查询生成、编排和评估的循环，为编排器生成训练数据。我们的评估主要针对MMLU基准，在Hugging Face上使用了具有7B、13B和34B参数的模型。结果显示我们的Leeroo编排器实现了与Mixtral模型相当的性能，但只产生了其成本的三分之二。此外，增加允许的成本超过了Mixtral的准确性，达到了75.9%的准确性。当将GPT4集成到底层模型池中时，进一步提升也得到了观察。

    In this paper, we propose an architecture to harness the collective knowledge of multiple trained LLMs to create a new state-of-the-art. At the core of this framework is a LLM-based orchestrator that is adept at picking the right underlying LLM experts for optimal task execution. Inspired by self-play in reinforcement learning, we created a loop of query generation, orchestration, and evaluation to generate training data for the orchestrator. Our evaluation focused on the MMLU benchmark, employing models with 7B, 13B, and 34B parameters available on Hugging Face. The results demonstrate new state-of-the-art open-source models: Our Leeroo orchestrator achieves performance on par with the Mixtral model while incurring only two-thirds of its cost. Moreover, increasing the allowed cost surpasses Mixtral's accuracy by over 5% at the same cost level, reaching an accuracy of 75.9%. Further enhancements were observed when integrating GPT4 into the underlying model pool. The Leeroo orchestrator
    
[^59]: 在印度城市班加罗尔中使用统计学和机器学习技术评估交通方式选择的决定因素

    Evaluating the Determinants of Mode Choice Using Statistical and Machine Learning Techniques in the Indian Megacity of Bengaluru. (arXiv:2401.13977v1 [cs.LG])

    [http://arxiv.org/abs/2401.13977](http://arxiv.org/abs/2401.13977)

    该研究运用统计学和机器学习技术，调查印度城市班加罗尔的低收入和低中产阶级家庭在交通方式选择方面的决策行为，并发现随机森林模型在预测准确性方面表现最佳。

    

    交通方式选择的决策对交通规划至关重要。虽然传统上使用离散选择模型等统计学习技术，但由于机器学习（ML）模型具有更高的预测性能，最近在交通规划者中引起了关注。然而，ML模型的黑盒特性在解释性方面存在显著挑战，限制了它们在决策和政策制定中的实际应用。本研究利用班加罗尔市$1350$个低收入和低中产阶级家庭的数据集，使用多项式逻辑模型和决策树、随机森林、极端梯度提升和支持向量机等机器学习分类器来调查交通方式选择行为的决策。在准确性方面，随机森林模型表现最佳（训练数据为$0.788$，测试数据为$0.605$），优于所有其他模型。这项研究采用了现代解释性工具，为决策制定者提供了有关交通方式选择行为的深入洞察。

    The decision making involved behind the mode choice is critical for transportation planning. While statistical learning techniques like discrete choice models have been used traditionally, machine learning (ML) models have gained traction recently among the transportation planners due to their higher predictive performance. However, the black box nature of ML models pose significant interpretability challenges, limiting their practical application in decision and policy making. This study utilised a dataset of $1350$ households belonging to low and low-middle income bracket in the city of Bengaluru to investigate mode choice decision making behaviour using Multinomial logit model and ML classifiers like decision trees, random forests, extreme gradient boosting and support vector machines. In terms of accuracy, random forest model performed the best ($0.788$ on training data and $0.605$ on testing data) compared to all the other models. This research has adopted modern interpretability 
    
[^60]: 超过Lipschitz连续性的随机弱凸优化

    Stochastic Weakly Convex Optimization Beyond Lipschitz Continuity. (arXiv:2401.13971v1 [math.OC])

    [http://arxiv.org/abs/2401.13971](http://arxiv.org/abs/2401.13971)

    本文研究了没有标准Lipschitz连续性假设的随机弱凸优化问题，并提出了新的自适应正则化策略。结果表明，一类广泛的随机算法具有$\mathcal{O} ( 1 / \sqrt{K})$的收敛速度和常数的失败率。

    

    本文研究了在没有标准Lipschitz连续性假设的情况下的随机弱凸优化。基于新的自适应正则化（步长）策略，我们证明了一类广泛的随机算法，包括随机次梯度法，具有$\mathcal{O} ( 1 / \sqrt{K})$的收敛速度，失败率为常数。我们的分析基于相当弱的假设：Lipschitz参数可以通过$\|x\|$的一般增长函数来限制，或者通过独立随机样本进行局部估计。

    This paper considers stochastic weakly convex optimization without the standard Lipschitz continuity assumption. Based on new adaptive regularization (stepsize) strategies, we show that a wide class of stochastic algorithms, including the stochastic subgradient method, preserve the $\mathcal{O} ( 1 / \sqrt{K})$ convergence rate with constant failure rate. Our analyses rest on rather weak assumptions: the Lipschitz parameter can be either bounded by a general growth function of $\|x\|$ or locally estimated through independent random samples.
    
[^61]: 动态长期时间序列预测：基于元转换网络的研究

    Dynamic Long-Term Time-Series Forecasting via Meta Transformer Networks. (arXiv:2401.13968v1 [cs.LG])

    [http://arxiv.org/abs/2401.13968](http://arxiv.org/abs/2401.13968)

    本文提出了一种名为Meta-Transformer Networks（MANTRA）的模型，用于动态长期时间序列预测。该模型通过快速和慢速学习器的结合，以及使用通用表示转换层，实现对动态环境的快速适应，并在实验中表现出优于基准算法的性能提升。

    

    在实践中，可靠的长期时间序列预测模型面临诸多挑战，如低计算和内存占用以及对动态学习环境的鲁棒性。本文提出了一种名为Meta-Transformer Networks（MANTRA）的模型，用于处理动态的长期时间序列预测任务。MANTRA依赖于快速和慢速学习器的概念，其中一组快速学习器学习数据分布的不同方面，同时快速适应变化。慢速学习器为快速学习器定制适当的表示。通过使用通用表示转换层产生任务适应性表示，并具有少量参数，实现对动态环境的快速适应。我们在四个具有不同预测长度的数据集上进行实验，结果表明我们的方法在多变量和单变量设置下至少比基准算法改进了3％。MANTRA的源代码可在链接中找到。

    A reliable long-term time-series forecaster is highly demanded in practice but comes across many challenges such as low computational and memory footprints as well as robustness against dynamic learning environments. This paper proposes Meta-Transformer Networks (MANTRA) to deal with the dynamic long-term time-series forecasting tasks. MANTRA relies on the concept of fast and slow learners where a collection of fast learners learns different aspects of data distributions while adapting quickly to changes. A slow learner tailors suitable representations to fast learners. Fast adaptations to dynamic environments are achieved using the universal representation transformer layers producing task-adapted representations with a small number of parameters. Our experiments using four datasets with different prediction lengths demonstrate the advantage of our approach with at least $3\%$ improvements over the baseline algorithms for both multivariate and univariate settings. Source codes of MANT
    
[^62]: 网络化多智能体强化学习用于点对点能源交易

    Networked Multiagent Reinforcement Learning for Peer-to-Peer Energy Trading. (arXiv:2401.13947v1 [eess.SY])

    [http://arxiv.org/abs/2401.13947](http://arxiv.org/abs/2401.13947)

    本文提出了一个利用多智能体强化学习框架来实现点对点能源交易的方法，该方法帮助自动化消费者的竞标和管理，并解决了可再生能源零边际成本和物理约束的问题。

    

    利用分布式可再生能源和能量储存资源进行点对点能源交易被长期认为是提高能源系统弹性和可持续性的解决方案。然而，消费者和自给自足者（具有能源发电资源的人）缺乏进行重复点对点交易的专业知识，并且可再生能源的零边际成本在确定公平市场价格方面存在挑战。为了解决这些问题，我们提出了多智能体强化学习（MARL）框架，以帮助自动化消费者对太阳能光伏和能量储存资源的竞标和管理，在一种利用供需比的点对点清算机制下。此外，我们展示了MARL框架如何整合物理网络约束以实现电压控制，从而确保点对点能源交易的物理可行性，并为真实世界的实施铺平了道路。

    Utilizing distributed renewable and energy storage resources in local distribution networks via peer-to-peer (P2P) energy trading has long been touted as a solution to improve energy systems' resilience and sustainability. Consumers and prosumers (those who have energy generation resources), however, do not have the expertise to engage in repeated P2P trading, and the zero-marginal costs of renewables present challenges in determining fair market prices. To address these issues, we propose multi-agent reinforcement learning (MARL) frameworks to help automate consumers' bidding and management of their solar PV and energy storage resources, under a specific P2P clearing mechanism that utilizes the so-called supply-demand ratio. In addition, we show how the MARL frameworks can integrate physical network constraints to realize voltage control, hence ensuring physical feasibility of the P2P energy trading and paving way for real-world implementations.
    
[^63]: 使用隐马尔可夫模型的强化学习来发现决策动态

    Reinforcement Learning with Hidden Markov Models for Discovering Decision-Making Dynamics. (arXiv:2401.13929v1 [cs.LG])

    [http://arxiv.org/abs/2401.13929](http://arxiv.org/abs/2401.13929)

    本论文针对重性抑郁障碍(MDD)中的奖励处理异常，使用强化学习模型和隐马尔可夫模型结合，探索决策制定过程中的学习策略动态对个体奖励学习能力的影响。

    

    由于其复杂和异质性，重性抑郁障碍(MDD)在诊断和治疗方面存在挑战。新的证据表明奖励处理异常可能作为MDD的行为标记。为了衡量奖励处理，患者执行涉及做出选择或对与不同结果相关联的刺激作出反应的基于计算机的行为任务。强化学习(RL)模型被拟合以提取衡量奖励处理各个方面的参数，以表征患者在行为任务中的决策方式。最近的研究发现，仅基于单个RL模型的奖励学习表征不足; 相反，决策过程中可能存在多种策略之间的切换。一个重要的科学问题是决策制定中学习策略的动态如何影响MDD患者的奖励学习能力。由概率奖励任务(PRT)所启发

    Major depressive disorder (MDD) presents challenges in diagnosis and treatment due to its complex and heterogeneous nature. Emerging evidence indicates that reward processing abnormalities may serve as a behavioral marker for MDD. To measure reward processing, patients perform computer-based behavioral tasks that involve making choices or responding to stimulants that are associated with different outcomes. Reinforcement learning (RL) models are fitted to extract parameters that measure various aspects of reward processing to characterize how patients make decisions in behavioral tasks. Recent findings suggest the inadequacy of characterizing reward learning solely based on a single RL model; instead, there may be a switching of decision-making processes between multiple strategies. An important scientific question is how the dynamics of learning strategies in decision-making affect the reward learning ability of individuals with MDD. Motivated by the probabilistic reward task (PRT) wi
    
[^64]: 在语言模型中实现对3D分子-文本的解释

    Towards 3D Molecule-Text Interpretation in Language Models. (arXiv:2401.13923v1 [cs.LG])

    [http://arxiv.org/abs/2401.13923](http://arxiv.org/abs/2401.13923)

    提出了一个名为3D-MoLM的模型，通过给语言模型配备一个3D分子编码器，实现了对3D分子-文本的解释和分析，此模型在下游任务上显著优于现有基线。

    

    语言模型（LMs）在各个领域有着很大的影响。然而，它们对于理解3D分子结构的固有限制极大地限制了它们在生物分子领域的潜力。为了弥补这一差距，我们关注于3D分子-文本解释，并提出3D-MoLM：3D分子语言模型。具体而言，3D-MoLM通过为LM配备一个3D分子编码器，使得LM能够解释和分析3D分子。这种集成是通过一个3D分子-文本投影器实现的，它连接了3D分子编码器的表示空间和LM的输入空间。此外，为了增强3D-MoLM在跨模态分子理解和指令跟随方面的能力，我们精心策划了一个以3D分子为中心的指引调整数据集--3D-MoIT。通过3D分子-文本对齐和3D分子中心的指引调整，3D-MoLM建立了3D分子编码器和LM的集成。它在下游任务上显著超过了现有的基线。

    Language Models (LMs) have greatly influenced diverse domains. However, their inherent limitation in comprehending 3D molecular structures has considerably constrained their potential in the biomolecular domain. To bridge this gap, we focus on 3D molecule-text interpretation, and propose 3D-MoLM: 3D-Molecular Language Modeling. Specifically, 3D-MoLM enables an LM to interpret and analyze 3D molecules by equipping the LM with a 3D molecular encoder. This integration is achieved by a 3D molecule-text projector, bridging the 3D molecular encoder's representation space and the LM's input space. Moreover, to enhance 3D-MoLM's ability of cross-modal molecular understanding and instruction following, we meticulously curated a 3D molecule-centric instruction tuning dataset -- 3D-MoIT. Through 3D molecule-text alignment and 3D molecule-centric instruction tuning, 3D-MoLM establishes an integration of 3D molecular encoder and LM. It significantly surpasses existing baselines on downstream tasks,
    
[^65]: LocMoE: 一种用于大型语言模型训练的低开销MoE

    LocMoE: A Low-overhead MoE for Large Language Model Training. (arXiv:2401.13920v1 [cs.LG])

    [http://arxiv.org/abs/2401.13920](http://arxiv.org/abs/2401.13920)

    LocMoE提出了一种新的路由策略，通过将部分节点间通信转换为节点内通信，结合负载平衡和局部性，以提高大型语言模型训练的性能。

    

    混合专家模型（MoE）是一种广泛采用的分布式和集成学习方法，用于大型语言模型（LLM），由于其能够有效稀疏和扩展模型，因此备受青睐。然而，MoE的性能受到负载不平衡和全对全通信的高延迟的限制，同时由于大量的专家容量导致相对冗余的计算。负载不平衡可能是由于现有路由策略始终倾向于选择特定的专家导致的。全对全过程中频繁的节点间通信也显著延长了训练时间。为了缓解上述性能问题，我们提出了一种新的路由策略，通过将部分节点间通信转换为节点内通信，结合负载平衡和局部性。值得注意的是，我们阐明了专家容量的最小阈值，通过将专家的门控权重与分配的标记之间的最大角偏差计算出来。

    The Mixtures-of-Experts (MoE) model is a widespread distributed and integrated learning method for large language models (LLM), which is favored due to its ability to sparsify and expand models efficiently. However, the performance of MoE is limited by load imbalance and high latency of All-To-All communication, along with relatively redundant computation owing to large expert capacity. Load imbalance may result from existing routing policies that consistently tend to select certain experts. The frequent inter-node communication in the All-To-All procedure also significantly prolongs the training time. To alleviate the above performance problems, we propose a novel routing strategy that combines load balance and locality by converting partial inter-node communication to that of intra-node. Notably, we elucidate that there is a minimum threshold for expert capacity, calculated through the maximal angular deviation between the gating weights of the experts and the assigned tokens. We por
    
[^66]: 离散分布的谱聚类

    Spectral Clustering for Discrete Distributions. (arXiv:2401.13913v1 [cs.LG])

    [http://arxiv.org/abs/2401.13913](http://arxiv.org/abs/2401.13913)

    本文提出了一种基于谱聚类和分布相似度度量的框架来解决离散分布聚类问题。通过使用线性最优传输构建相似度矩阵，我们在聚类准确性和计算效率方面取得了显著的改进。

    

    离散分布聚类（D2C）通常通过Wasserstein质心方法来解决。这些方法在一个共同的假设下工作，即聚类可以通过质心很好地表示，但在许多实际应用中可能不成立。在本文中，我们提出了一个简单而有效的基于谱聚类和分布相似度度量（例如最大均值差异和Wasserstein距离）的框架来解决D2C问题。为了提高可扩展性，我们提出使用线性最优传输在大型数据集上高效地构建相似度矩阵。我们提供了理论保证，保证了所提方法在聚类分布方面的成功。在合成数据和真实数据上的实验证明，我们的方法在聚类准确性和计算效率方面大大优于基准方法。

    Discrete distribution clustering (D2C) was often solved by Wasserstein barycenter methods. These methods are under a common assumption that clusters can be well represented by barycenters, which may not hold in many real applications. In this work, we propose a simple yet effective framework based on spectral clustering and distribution affinity measures (e.g., maximum mean discrepancy and Wasserstein distance) for D2C. To improve the scalability, we propose to use linear optimal transport to construct affinity matrices efficiently on large datasets. We provide theoretical guarantees for the success of the proposed methods in clustering distributions. Experiments on synthetic and real data show that our methods outperform the baselines largely in terms of both clustering accuracy and computational efficiency.
    
[^67]: 深度学习和基础模型在时间序列预测中的调查

    A Survey of Deep Learning and Foundation Models for Time Series Forecasting. (arXiv:2401.13912v1 [cs.LG])

    [http://arxiv.org/abs/2401.13912](http://arxiv.org/abs/2401.13912)

    深度学习在时间序列预测中显示出显著优势，但在疫情预测方面仍面临挑战，基础模型的开发可以帮助解决这些问题。

    

    深度学习已成功应用于许多领域，然而其在时间序列预测方面的优势出现得比较慢。最近，在著名的Makridakis (M)竞赛中，传统统计或机器学习技术的混合模型仅最近成为表现最佳的模型。随着深度学习在时间序列预测中的架构进步（例如，具有注意力的编码器-解码器、转换器和图神经网络），深度学习开始展示出显著优势。然而，在疫情预测领域，深度学习模型仍面临挑战：训练数据的时间序列长度不够，对累积的科学知识缺乏认识，以及模型的可解释性。为此，基础模型（具有广泛预训练的大型深度学习模型）的开发使得模型能够理解模式并获取可以应用于新相关问题的知识。

    Deep Learning has been successfully applied to many application domains, yet its advantages have been slow to emerge for time series forecasting. For example, in the well-known Makridakis (M) Competitions, hybrids of traditional statistical or machine learning techniques have only recently become the top performers. With the recent architectural advances in deep learning being applied to time series forecasting (e.g., encoder-decoders with attention, transformers, and graph neural networks), deep learning has begun to show significant advantages. Still, in the area of pandemic prediction, there remain challenges for deep learning models: the time series is not long enough for effective training, unawareness of accumulated scientific knowledge, and interpretability of the model. To this end, the development of foundation models (large deep learning models with extensive pre-training) allows models to understand patterns and acquire knowledge that can be applied to new related problems b
    
[^68]: 赋予机器像化学家一样思考的能力：用层次符号回归揭示分子结构极性关系

    Empowering Machines to Think Like Chemists: Unveiling Molecular Structure-Polarity Relationships with Hierarchical Symbolic Regression. (arXiv:2401.13904v1 [cs.LG])

    [http://arxiv.org/abs/2401.13904](http://arxiv.org/abs/2401.13904)

    本论文介绍了一种无监督的层次符号回归方法(UHiSR)，通过结合层次神经网络和符号回归，实现了自动提取化学直观的极性指数，并发现了将分子结构与色谱行为联系起来的可解释方程。

    

    薄层色谱法(TLC)是分子极性分析中至关重要的技术。尽管其重要性，对于TLC的预测模型，尤其是那些由人工智能驱动的模型的可解释性仍然面临挑战。当前的方法要么利用高维分子指纹，要么利用基于领域知识的特征工程，经常面临表达能力和可解释性之间的两难选择。为了弥合这一差距，我们介绍了无监督的层次符号回归(UHiSR)方法，结合了层次神经网络和符号回归。UHiSR自动提取化学直观的极性指数，并发现将分子结构与色谱行为联系起来的可解释方程。

    Thin-layer chromatography (TLC) is a crucial technique in molecular polarity analysis. Despite its importance, the interpretability of predictive models for TLC, especially those driven by artificial intelligence, remains a challenge. Current approaches, utilizing either high-dimensional molecular fingerprints or domain-knowledge-driven feature engineering, often face a dilemma between expressiveness and interpretability. To bridge this gap, we introduce Unsupervised Hierarchical Symbolic Regression (UHiSR), combining hierarchical neural networks and symbolic regression. UHiSR automatically distills chemical-intuitive polarity indices, and discovers interpretable equations that link molecular structure to chromatographic behavior.
    
[^69]: 跨模态原型基础的多模态联邦学习在严重缺失模态下的应用

    Cross-Modal Prototype based Multimodal Federated Learning under Severely Missing Modality. (arXiv:2401.13898v1 [cs.LG])

    [http://arxiv.org/abs/2401.13898](http://arxiv.org/abs/2401.13898)

    提出了一种适用于严重缺失模态的多模态联邦学习方法MFCPL，通过完整的原型提供多样的模态知识，解决了数据异质性和缺失模态带来的稳健性问题。

    

    多模态联邦学习（MFL）作为一种去中心化的机器学习范例已经出现，它允许具有不同模态的多个客户端在不共享私人数据的情况下合作训练机器学习模型，跨多样的数据源。然而，数据异质性和严重缺失模态等挑战给MFL的稳健性带来重要阻碍，严重影响全局模型的性能。在本文中，我们提出了一种适用于严重缺失模态下的多模态联邦学习的新方法，即多模态联邦交叉原型学习（MFCPL），通过对模态共享级别进行完整的原型来提供多样的模态知识。

    Multimodal federated learning (MFL) has emerged as a decentralized machine learning paradigm, allowing multiple clients with different modalities to collaborate on training a machine learning model across diverse data sources without sharing their private data. However, challenges, such as data heterogeneity and severely missing modalities, pose crucial hindrances to the robustness of MFL, significantly impacting the performance of global model. The absence of a modality introduces misalignment during the local training phase, stemming from zero-filling in the case of clients with missing modalities. Consequently, achieving robust generalization in global model becomes imperative, especially when dealing with clients that have incomplete data. In this paper, we propose Multimodal Federated Cross Prototype Learning (MFCPL), a novel approach for MFL under severely missing modalities by conducting the complete prototypes to provide diverse modality knowledge in modality-shared level with 
    
[^70]: 通过大型语言模型和监督建模的零样本推断进行乳腺癌病理分类的比较研究

    A comparative study of zero-shot inference with large language models and supervised modeling in breast cancer pathology classification. (arXiv:2401.13887v1 [cs.CL])

    [http://arxiv.org/abs/2401.13887](http://arxiv.org/abs/2401.13887)

    本研究比较了大型语言模型与监督建模在乳腺癌病理分类上的零样本推断能力，发现GPT-4模型在所有任务中要么明显优于，要么与最佳的监督模型LSTM-Att模型相当。

    

    尽管监督机器学习在从临床记录中提取信息方面十分流行，但创建大型注释数据集需要广泛的领域专业知识和耗费时间。与此同时，大型语言模型（LLMs）展示了很强的迁移学习能力。在本研究中，我们探讨了最近的LLMs是否可以减少对大规模数据注释的需求。我们整理了一个手动标注的769个乳腺癌病理报告的数据集（标注了13个类别），来比较GPT-4模型和GPT-3.5模型的零样本分类能力与三种模型架构的监督分类性能：随机森林分类器，具有注意力的长短期记忆网络（LSTM-Att）和UCSF-BERT模型。在所有13个任务中，GPT-4模型的性能要么明显优于最佳的监督模型LSTM-Att模型（平均宏F1得分为0.83 vs. 0.75），要么与其相当。在存在标签之间高度不平衡的任务中，di

    Although supervised machine learning is popular for information extraction from clinical notes, creating large annotated datasets requires extensive domain expertise and is time-consuming. Meanwhile, large language models (LLMs) have demonstrated promising transfer learning capability. In this study, we explored whether recent LLMs can reduce the need for large-scale data annotations. We curated a manually-labeled dataset of 769 breast cancer pathology reports, labeled with 13 categories, to compare zero-shot classification capability of the GPT-4 model and the GPT-3.5 model with supervised classification performance of three model architectures: random forests classifier, long short-term memory networks with attention (LSTM-Att), and the UCSF-BERT model. Across all 13 tasks, the GPT-4 model performed either significantly better than or as well as the best supervised model, the LSTM-Att model (average macro F1 score of 0.83 vs. 0.75). On tasks with high imbalance between labels, the di
    
[^71]: Constant Stepsize Q-learning: 分布收敛性、偏差和外推

    Constant Stepsize Q-learning: Distributional Convergence, Bias and Extrapolation. (arXiv:2401.13884v1 [stat.ML])

    [http://arxiv.org/abs/2401.13884](http://arxiv.org/abs/2401.13884)

    本文研究了常数步长异步Q-learning算法，并通过分析其与马尔可夫链的关联，展示了其迭代在分布上的收敛性和指数收敛速度。同时，研究者还建立了该算法迭代的中心极限定理，并对其渐近偏差进行了精确的展开。通过这些分析，我们可以深入理解这种算法的优化效果和偏差特性。

    

    随机逼近（SA）是一种广泛应用于各个领域的算法方法，包括优化和强化学习（RL）。在RL算法中，由于其经验成功，Q-learning特别受欢迎。在本文中，我们研究了常数步长异步Q-learning，这在实践中通常用于快速收敛。通过将常数步长Q-learning与时间均匀的马尔可夫链相关联，我们展示了迭代在Wasserstein距离下的分布收敛性，并建立了其指数收敛速度。我们还为Q-learning迭代建立了中心极限定理，证明了平均迭代的渐近正态性。此外，我们提供了平均迭代步骤的渐近偏差的明确展开。具体而言，偏差与步长成正比，直到高阶项，并为线性系数提供了明确的表达式。这种对偏差的精确描述可以使得我们可以探索出对于这种偏差的更深入理解。

    Stochastic Approximation (SA) is a widely used algorithmic approach in various fields, including optimization and reinforcement learning (RL). Among RL algorithms, Q-learning is particularly popular due to its empirical success. In this paper, we study asynchronous Q-learning with constant stepsize, which is commonly used in practice for its fast convergence. By connecting the constant stepsize Q-learning to a time-homogeneous Markov chain, we show the distributional convergence of the iterates in Wasserstein distance and establish its exponential convergence rate. We also establish a Central Limit Theory for Q-learning iterates, demonstrating the asymptotic normality of the averaged iterates. Moreover, we provide an explicit expansion of the asymptotic bias of the averaged iterate in stepsize. Specifically, the bias is proportional to the stepsize up to higher-order terms and we provide an explicit expression for the linear coefficient. This precise characterization of the bias allows
    
[^72]: 温度对Softmax高斯混合专家是否具有采样效率？

    Is Temperature Sample Efficient for Softmax Gaussian Mixture of Experts?. (arXiv:2401.13875v1 [stat.ML])

    [http://arxiv.org/abs/2401.13875](http://arxiv.org/abs/2401.13875)

    本文研究了温度对Softmax高斯混合专家的采样效率的影响，证明了由于温度和其他模型参数之间的相互作用，参数估计的收敛速度较慢，并且可能很慢。

    

    最近，密集-稀疏门控专家混合模型（MoE）已成为广为使用的稀疏MoE的有效替代方案。与后者模型中固定激活的专家数量不同，前者模型利用温度来控制softmax权重分布和MoE的稀疏性，在训练过程中稳定专家的专业化。然而，尽管以前有尝试从理论上理解稀疏MoE的方法，对于密集到稀疏门控MoE的全面分析仍然困难重重。因此，本文旨在探讨密集到稀疏门控对Gaussian MoE下的最大似然估计的影响。我们证明由于温度和其他模型参数之间通过一些偏微分方程的相互作用，参数估计的收敛速度比任何多项式速率都要慢，并且可能慢到$\mathcal{

    Dense-to-sparse gating mixture of experts (MoE) has recently become an effective alternative to a well-known sparse MoE. Rather than fixing the number of activated experts as in the latter model, which could limit the investigation of potential experts, the former model utilizes the temperature to control the softmax weight distribution and the sparsity of the MoE during training in order to stabilize the expert specialization. Nevertheless, while there are previous attempts to theoretically comprehend the sparse MoE, a comprehensive analysis of the dense-to-sparse gating MoE has remained elusive. Therefore, we aim to explore the impacts of the dense-to-sparse gate on the maximum likelihood estimation under the Gaussian MoE in this paper. We demonstrate that due to interactions between the temperature and other model parameters via some partial differential equations, the convergence rates of parameter estimations are slower than any polynomial rates, and could be as slow as $\mathcal{
    
[^73]: 边缘条件节点更新图神经网络用于多变量时间序列异常检测

    Edge Conditional Node Update Graph Neural Network for Multi-variate Time Series Anomaly Detection. (arXiv:2401.13872v1 [cs.LG])

    [http://arxiv.org/abs/2401.13872](http://arxiv.org/abs/2401.13872)

    本文介绍了一种边缘条件节点更新的图神经网络（ECNU-GNN），用于多变量时间序列异常检测。该模型根据连接的边动态转换源节点表示以恰当地表示目标节点，在实际数据集上表现出了显著的性能提升。

    

    随着智能化系统的迅速发展，传感器的增加显著复杂化了对系统状态的手动监测。因此，基于图的时间序列异常检测方法引起了人们的关注，因为它们能够明确表示传感器之间的关系。然而，这些方法通常在更新不同目标节点表示时，对所有连接的目标节点应用统一的源节点表示。此外，常用于推断未知图结构的图注意力机制可能限制源节点表示的多样性。本文介绍了边缘条件节点更新图神经网络（ECNU-GNN）。我们的模型配备了一个边缘条件节点更新模块，根据连接的边动态转换源节点表示以恰当地表示目标节点。我们在三个实际数据集上验证了性能：SWaT、WADI和PSM。我们的模型表现出5的性能提升。

    With the rapid advancement in cyber-physical systems, the increasing number of sensors has significantly complicated manual monitoring of system states. Consequently, graph-based time-series anomaly detection methods have gained attention due to their ability to explicitly represent relationships between sensors. However, these methods often apply a uniform source node representation across all connected target nodes, even when updating different target node representations. Moreover, the graph attention mechanism, commonly used to infer unknown graph structures, could constrain the diversity of source node representations. In this paper, we introduce the Edge Conditional Node-update Graph Neural Network (ECNU-GNN). Our model, equipped with an edge conditional node update module, dynamically transforms source node representations based on connected edges to represent target nodes aptly. We validate performance on three real-world datasets: SWaT, WADI, and PSM. Our model demonstrates 5.
    
[^74]: 借助多条件扩散引导的逆分子设计

    Inverse Molecular Design with Multi-Conditional Diffusion Guidance. (arXiv:2401.13858v1 [cs.LG])

    [http://arxiv.org/abs/2401.13858](http://arxiv.org/abs/2401.13858)

    借助多条件扩散引导的逆分子设计模型在材料和药物发现方面具有巨大潜力。通过引入Transformer-based去噪模型和图依赖的扩散过程，该模型能够在多个条件约束下准确地生成聚合物和小分子。

    

    借助扩散模型进行逆分子设计在材料和药物发现方面具有巨大潜力。虽然在无条件分子生成方面取得了成功，但将合成评分和气体渗透性等多个属性作为条件约束集成到扩散模型中仍未被探索。我们引入了多条件扩散引导。所提出的基于Transformer的去噪模型具有一个条件编码器，该编码器学习了数值和分类条件的表示。组成结构编码器-解码器的去噪模型在条件表示下进行去噪训练。扩散过程变得依赖于图来准确估计分子中与图相关的噪声，而不像以前的模型仅关注原子或键的边缘分布。我们广泛验证了我们的模型在多条件聚合物和小分子生成方面的优越性。结果显示我们在分布度量方面的优势。

    Inverse molecular design with diffusion models holds great potential for advancements in material and drug discovery. Despite success in unconditional molecule generation, integrating multiple properties such as synthetic score and gas permeability as condition constraints into diffusion models remains unexplored. We introduce multi-conditional diffusion guidance. The proposed Transformer-based denoising model has a condition encoder that learns the representations of numerical and categorical conditions. The denoising model, consisting of a structure encoder-decoder, is trained for denoising under the representation of conditions. The diffusion process becomes graph-dependent to accurately estimate graph-related noise in molecules, unlike the previous models that focus solely on the marginal distributions of atoms or bonds. We extensively validate our model for multi-conditional polymer and small molecule generation. Results demonstrate our superiority across metrics from distribution
    
[^75]: 嵌入式攻击项目（工作报告）

    Embedding Attack Project (Work Report). (arXiv:2401.13854v1 [cs.LG])

    [http://arxiv.org/abs/2401.13854](http://arxiv.org/abs/2401.13854)

    Embedding Attack Project的工作报告总结了嵌入式攻击项目的MIA实验，发现了以下六个结论：（1）过拟合程度与模型的易受攻击性成正比；（2）模型中的早期嵌入层更不容易受到隐私泄漏的影响；（3）更深层的模型层包含更多的成员信息；（4）如果嵌入和对应的训练标签都被泄露，模型对MIA更脆弱；（5）可以使用伪标签来增加MIA的成功率；（6）尽管MIA和PIA的成功率存在差异

    

    该报告总结了嵌入式攻击项目的全部MIA实验（成员推断攻击），包括威胁模型、实验设置、实验结果、发现和讨论。目前的结果涵盖了对从计算机视觉到语言建模的6个人工智能模型上评估两种主要MIA策略（基于损失和嵌入的MIA）。关于MIA防御和邻近比较嵌入攻击还有两个正在进行的实验。这些都是正在进行中的项目。目前关于MIA和PIA的工作可以总结为六个结论：（1）过拟合程度与模型的易受攻击性成正比；（2）模型中的早期嵌入层更不容易受到隐私泄漏的影响；（3）更深层的模型层包含更多的成员信息；（4）如果嵌入和对应的训练标签都被泄露，模型对MIA更脆弱；（5）可以使用伪标签来增加MIA的成功率；（6）尽管MIA和PIA的成功率存在差异

    This report summarizes all the MIA experiments (Membership Inference Attacks) of the Embedding Attack Project, including threat models, experimental setup, experimental results, findings and discussion. Current results cover the evaluation of two main MIA strategies (loss-based and embedding-based MIAs) on 6 AI models ranging from Computer Vision to Language Modelling. There are two ongoing experiments on MIA defense and neighborhood-comparison embedding attacks. These are ongoing projects.  The current work on MIA and PIA can be summarized into six conclusions: (1) Amount of overfitting is directly proportional to model's vulnerability; (2) early embedding layers in the model are less susceptible to privacy leaks; (3) Deeper model layers contain more membership information; (4) Models are more vulnerable to MIA if both embeddings and corresponding training labels are compromised; (5) it is possible to use pseudo-labels to increase the MIA success; and (6) although MIA and PIA success 
    
[^76]: 利用声音克隆将NVIDIA的多语言TTS系统扩展到印度语言

    Scaling NVIDIA's multi-speaker multi-lingual TTS systems with voice cloning to Indic Languages. (arXiv:2401.13851v1 [cs.SD])

    [http://arxiv.org/abs/2401.13851](http://arxiv.org/abs/2401.13851)

    本文介绍了NVIDIA开发的TTS模型，利用RAD-MMM和P-Flow实现了多语言TTS的训练，其中P-Flow在零样本TTS方面表现出色，在2024挑战中获得了第一名。

    

    在本文中，我们描述了NVIDIA为MMITS-VC（多语言、多语种印度TTS与声音克隆）2024挑战开发的TTS模型。在1和2轨道中，我们利用RAD-MMM在目标说话人数据上进行了少样本TTS的训练。在第3轨道中，我们利用P-Flow进行了零样本TTS的训练，同时使用了挑战数据集和外部数据集。我们对所有提交使用HiFi-GAN vocoders。RAD-MMM在1和2轨道上表现出竞争力，而P-Flow在第3轨道上排名第一，平均意见分(MOS)为4.4，说话人相似度分数(SMOS)为3.62。

    In this paper, we describe the TTS models developed by NVIDIA for the MMITS-VC (Multi-speaker, Multi-lingual Indic TTS with Voice Cloning) 2024 Challenge. In Tracks 1 and 2, we utilize RAD-MMM to perform few-shot TTS by training additionally on 5 minutes of target speaker data. In Track 3, we utilize P-Flow to perform zero-shot TTS by training on the challenge dataset as well as external datasets. We use HiFi-GAN vocoders for all submissions. RAD-MMM performs competitively on Tracks 1 and 2, while P-Flow ranks first on Track 3, with mean opinion score (MOS) 4.4 and speaker similarity score (SMOS) of 3.62.
    
[^77]: 一种基于V2X的隐私保护联合测量和学习系统

    A V2X-based Privacy Preserving Federated Measuring and Learning System. (arXiv:2401.13848v1 [cs.LG])

    [http://arxiv.org/abs/2401.13848](http://arxiv.org/abs/2401.13848)

    本文介绍了一种基于V2X的隐私保护联合测量和学习系统，通过V2V通信向其他车辆提供实时数据，同时通过V2N链路上的FL方案创建交通网络的预测模型。

    

    未来的自动驾驶车辆将使用各种传感器生成大量数据。这些数据不仅可以用于自动驾驶算法，还可以帮助其他车辆或基础设施进行实时决策。因此，车辆需要通过车辆到一切(V2X)技术来交换测量数据。此外，预测道路网络的状态可能也会有益处。通过这种预测，我们可以减轻道路拥堵、平衡停车场使用情况或优化交通流动。这将降低运输成本，减少环境影响。在本文中，我们提出了一种联合测量和学习系统，通过车辆到车辆(V2V)通信向其他车辆提供实时数据，同时通过车辆到网络(V2N)链路上的联合学习(FL)方案创建交通网络的预测模型。由于尚无真实世界的自动驾驶数据，我们模拟了数据，

    Future autonomous vehicles (AVs) will use a variety of sensors that generate a vast amount of data. Naturally, this data not only serves self-driving algorithms; but can also assist other vehicles or the infrastructure in real-time decision-making. Consequently, vehicles shall exchange their measurement data over Vehicle-to-Everything (V2X) technologies. Moreover, predicting the state of the road network might be beneficial too. With such a prediction, we might mitigate road congestion, balance parking lot usage, or optimize the traffic flow. That would decrease transportation costs as well as reduce its environmental impact.  In this paper, we propose a federated measurement and learning system that provides real-time data to fellow vehicles over Vehicle-to-Vehicle (V2V) communication while also operating a federated learning (FL) scheme over the Vehicle-to-Network (V2N) link to create a predictive model of the transportation network. As we are yet to have real-world AV data, we model
    
[^78]: 在多类分类问题中枚举k折配置

    Enumerating the k-fold configurations in multi-class classification problems. (arXiv:2401.13843v1 [cs.LG])

    [http://arxiv.org/abs/2401.13843](http://arxiv.org/abs/2401.13843)

    本研究介绍了一种在多类分类问题中枚举k折配置的算法，用来解决基于k折交叉验证的性能得分不可复现的问题。

    

    K折交叉验证是评估分类器性能的常用工具。人工智能面临的可重复性危机部分源于报告的基于k折交叉验证的性能得分不可复现。最近，我们引入了数值技术来测试所声称的性能得分和实验设置的一致性。在一个关键的使用案例中，该方法依赖于对所有k折配置的组合枚举，对于这种情况，我们在二分类问题中提出了一个算法。

    K-fold cross-validation is a widely used tool for assessing classifier performance. The reproducibility crisis faced by artificial intelligence partly results from the irreproducibility of reported k-fold cross-validation-based performance scores. Recently, we introduced numerical techniques to test the consistency of claimed performance scores and experimental setups. In a crucial use case, the method relies on the combinatorial enumeration of all k-fold configurations, for which we proposed an algorithm in the binary classification case.
    
[^79]: 机器学习在工业传感和控制中的应用：一项调查和实践视角

    Machine learning for industrial sensing and control: A survey and practical perspective. (arXiv:2401.13836v1 [eess.SY])

    [http://arxiv.org/abs/2401.13836](http://arxiv.org/abs/2401.13836)

    本文调查了机器学习在工业传感和控制中的应用，确定了在过程工业中实际成功的关键技术，并讨论了软传感、过程优化和控制等核心应用领域的需求和挑战。

    

    随着深度学习的兴起，过程工业领域对于利用大规模非线性传感和控制问题的数据产生了新的兴趣。我们确定了在过程工业中取得实际成功的关键统计和机器学习技术。为此，我们以混合建模为起点，提供了支撑核心应用领域的方法论框架：软传感、过程优化和控制。软传感包含了统计和机器学习方法在工业应用的丰富案例。我们定量地识别了研究趋势，从而揭示了实践中最成功的技术。我们考虑了两种不同的数据驱动优化和控制方法：混合建模结合数学规划技术以及强化学习。在这些应用领域中，我们讨论了它们各自的工业需求和挑战。一个共同的挑战是解释性和效率问题。

    With the rise of deep learning, there has been renewed interest within the process industries to utilize data on large-scale nonlinear sensing and control problems. We identify key statistical and machine learning techniques that have seen practical success in the process industries. To do so, we start with hybrid modeling to provide a methodological framework underlying core application areas: soft sensing, process optimization, and control. Soft sensing contains a wealth of industrial applications of statistical and machine learning methods. We quantitatively identify research trends, allowing insight into the most successful techniques in practice.  We consider two distinct flavors for data-driven optimization and control: hybrid modeling in conjunction with mathematical programming techniques and reinforcement learning. Throughout these application areas, we discuss their respective industrial requirements and challenges.  A common challenge is the interpretability and efficiency o
    
[^80]: 语言模型中模型和人类置信度之间的校准差距

    The Calibration Gap between Model and Human Confidence in Large Language Models. (arXiv:2401.13835v1 [cs.LG])

    [http://arxiv.org/abs/2401.13835](http://arxiv.org/abs/2401.13835)

    该论文研究了大型语言模型在传达置信度方面模型和人类之间存在的差距，并发现默认解释会导致用户过高估计模型置信度和准确性。

    

    为了使大型语言模型（LLM）能够获得人类的信任，它们需要在某种意义上实现良好的校准，即能够准确评估和传达它们的预测正确的可能性。最近的研究关注了LLM内部置信度评估的质量，但问题仍然是LLM能够如何将这种内部模型置信度传达给人类用户。本文探讨了人类对LLM响应的外部置信度与模型内部置信度之间的差距。通过涉及多项选择题的实验，我们系统地检查了人类用户识别LLM输出可信度的能力。我们的研究重点分为两个方面：（1）评估用户对真实LLM置信度的感知和（2）调查个性化解释对该感知的影响。研究结果显示，LLM的默认解释往往会导致用户过高估计模型的置信度和准确性。通过修改解释的方式可以减小这种误差。

    For large language models (LLMs) to be trusted by humans they need to be well-calibrated in the sense that they can accurately assess and communicate how likely it is that their predictions are correct. Recent work has focused on the quality of internal LLM confidence assessments, but the question remains of how well LLMs can communicate this internal model confidence to human users. This paper explores the disparity between external human confidence in an LLM's responses and the internal confidence of the model. Through experiments involving multiple-choice questions, we systematically examine human users' ability to discern the reliability of LLM outputs. Our study focuses on two key areas: (1) assessing users' perception of true LLM confidence and (2) investigating the impact of tailored explanations on this perception. The research highlights that default explanations from LLMs often lead to user overestimation of both the model's confidence and its' accuracy. By modifying the expl
    
[^81]: 用于马尔可夫物联网模型中数据上行的流量学习和主动无人机轨迹规划。

    Traffic Learning and Proactive UAV Trajectory Planning for Data Uplink in Markovian IoT Models. (arXiv:2401.13827v1 [cs.LG])

    [http://arxiv.org/abs/2401.13827](http://arxiv.org/abs/2401.13827)

    本文提出了一个新的学习框架，用于估计基于马尔可夫事件的物联网设备的流量情况，并优化多个无人机的轨迹和调度策略，以降低信息时代、节省能量和提高吞吐量。

    

    信息时代(AoI)用于衡量数据的新鲜度。在物联网网络中，传统的资源管理方案依赖于设备和基站(BS)之间的消息交换，导致AoI高、能量消耗高且可靠性低。作为飞行基站的无人机(UAV)在减小AoI、节省能量和提高吞吐量方面具有许多优势。本文提出了一个基于学习的新框架，根据马尔科维事件估计物联网设备的流量到达情况。学习过程用于优化多个无人机的轨迹和调度策略。首先，BS预测设备未来的流量。我们比较了两种流量预测器：前向算法(FA)和长短期记忆(LSTM)。然后，我们提出了一种用于优化每个无人机最优策略的深度强化学习(DRL)方法。最后，我们针对提出的优化奖励函数进行了操作。

    The age of information (AoI) is used to measure the freshness of the data. In IoT networks, the traditional resource management schemes rely on a message exchange between the devices and the base station (BS) before communication which causes high AoI, high energy consumption, and low reliability. Unmanned aerial vehicles (UAVs) as flying BSs have many advantages in minimizing the AoI, energy-saving, and throughput improvement. In this paper, we present a novel learning-based framework that estimates the traffic arrival of IoT devices based on Markovian events. The learning proceeds to optimize the trajectory of multiple UAVs and their scheduling policy. First, the BS predicts the future traffic of the devices. We compare two traffic predictors: the forward algorithm (FA) and the long short-term memory (LSTM). Afterward, we propose a deep reinforcement learning (DRL) approach to optimize the optimal policy of each UAV. Finally, we manipulate the optimum reward function for the proposed
    
[^82]: 在AI中浏览数据集文档：Hugging Face数据集卡片的大规模分析

    Navigating Dataset Documentations in AI: A Large-Scale Analysis of Dataset Cards on Hugging Face. (arXiv:2401.13822v1 [cs.LG])

    [http://arxiv.org/abs/2401.13822](http://arxiv.org/abs/2401.13822)

    本研究通过分析Hugging Face上的数据集文档，提供了Hugging Face数据集生态系统的概览并洞察到数据集文档实践的关键问题。研究发现，数据集卡片完成率与数据集受欢迎程度存在显著异质性，从业者在数据集描述和数据集结构部分更为关注，而对于使用数据的考虑部分较为忽视。

    

    机器学习的进展与数据集的创建密切相关。虽然数据文档被广泛认为对于ML的可靠性、可重复性和透明性至关重要，但我们缺乏对当前数据集文档实践的系统实证理解。为了揭示这个问题，我们以Hugging Face为例，这是一个用于共享和协作ML模型和数据集的最大平台。通过分析Hugging Face上的7433个数据集文档，我们的调查提供了Hugging Face数据集生态系统的概览和对数据集文档实践的洞察，得出了5个主要发现：（1）数据集卡片完成率显示出与数据集的受欢迎程度相关的显著异质性。（2）对数据集卡片中的每个部分进行细致的考察表明，从业者似乎更重视数据集描述和数据集结构部分，而对于使用数据的考虑部分更加忽视。

    Advances in machine learning are closely tied to the creation of datasets. While data documentation is widely recognized as essential to the reliability, reproducibility, and transparency of ML, we lack a systematic empirical understanding of current dataset documentation practices. To shed light on this question, here we take Hugging Face -- one of the largest platforms for sharing and collaborating on ML models and datasets -- as a prominent case study. By analyzing all 7,433 dataset documentation on Hugging Face, our investigation provides an overview of the Hugging Face dataset ecosystem and insights into dataset documentation practices, yielding 5 main findings: (1) The dataset card completion rate shows marked heterogeneity correlated with dataset popularity. (2) A granular examination of each section within the dataset card reveals that the practitioners seem to prioritize Dataset Description and Dataset Structure sections, while the Considerations for Using the Data section rec
    
[^83]: 研究大型语言模型在代码克隆检测方面的功效

    Investigating the Efficacy of Large Language Models for Code Clone Detection. (arXiv:2401.13802v1 [cs.SE])

    [http://arxiv.org/abs/2401.13802](http://arxiv.org/abs/2401.13802)

    这项研究探索了大型语言模型在代码克隆检测任务中的应用。

    

    大型语言模型（LLMs）在各种自然语言处理和软件工程任务中表现出了显著的成功，例如代码生成。LLMs主要在基于提示的零/少样本范式中被用于指导模型完成任务。本研究探索了LLMs在代码克隆检测（CCD）这一非生成任务中的适用性。

    Large Language Models (LLMs) have demonstrated remarkable success in various natural language processing and software engineering tasks, such as code generation. The LLMs are mainly utilized in the prompt-based zero/few-shot paradigm to guide the model in accomplishing the task. %\textbf{Goal:} GPT-based models are one of the popular ones studied for tasks such as code comment generation or test generation. These tasks are `generative' tasks. However, there is limited research on the usage of LLMs for `non-generative' tasks such as classification using the prompt-based paradigm. In this preliminary exploratory study, we investigated the applicability of LLMs for Code Clone Detection (CCD), a non-generative task. %\textbf{Method:} By building a mono-lingual and cross-lingual CCD dataset derived from CodeNet, we first investigated two different prompts using ChatGPT to detect \textcolor{black}{Type-4} code clones in Java-Java and Java-Ruby pairs in a zero-shot setting. We \textcolor{blac
    
[^84]: 不要按按钮！探索机器学习和迁移学习中的数据泄露风险

    Don't Push the Button! Exploring Data Leakage Risks in Machine Learning and Transfer Learning. (arXiv:2401.13796v1 [cs.LG])

    [http://arxiv.org/abs/2401.13796](http://arxiv.org/abs/2401.13796)

    本文讨论了机器学习中的数据泄露问题，即未预期的信息污染训练数据，影响模型性能评估，用户可能由于缺乏理解而忽视关键步骤，导致乐观的性能估计在实际场景中不成立。

    

    机器学习（ML）在各个领域取得了革命性的进展，为多个领域提供了预测能力。然而，随着ML工具的日益可获得性，许多从业者缺乏深入的ML专业知识，采用了“按按钮”方法，利用用户友好的界面而忽视了底层算法的深入理解。虽然这种方法提供了便利，但它引发了对结果可靠性的担忧，导致了错误的性能评估等挑战。本文解决了ML中的一个关键问题，即数据泄露，其中未预期的信息污染了训练数据，影响了模型的性能评估。由于缺乏理解，用户可能会无意中忽视关键步骤，从而导致在现实场景中可能不成立的乐观性能估计。评估性能与实际在新数据上的性能的差异是一个重要的关注点。本文特别将ML中的数据泄露分为不同类别，并讨论了相关解决方法。

    Machine Learning (ML) has revolutionized various domains, offering predictive capabilities in several areas. However, with the increasing accessibility of ML tools, many practitioners, lacking deep ML expertise, adopt a "push the button" approach, utilizing user-friendly interfaces without a thorough understanding of underlying algorithms. While this approach provides convenience, it raises concerns about the reliability of outcomes, leading to challenges such as incorrect performance evaluation. This paper addresses a critical issue in ML, known as data leakage, where unintended information contaminates the training data, impacting model performance evaluation. Users, due to a lack of understanding, may inadvertently overlook crucial steps, leading to optimistic performance estimates that may not hold in real-world scenarios. The discrepancy between evaluated and actual performance on new data is a significant concern. In particular, this paper categorizes data leakage in ML, discussi
    
[^85]: 使用深度递归神经网络对智能城市中的交通模式进行分类

    Traffic Pattern Classification in Smart Cities Using Deep Recurrent Neural Network. (arXiv:2401.13794v1 [cs.LG])

    [http://arxiv.org/abs/2401.13794](http://arxiv.org/abs/2401.13794)

    本论文使用深度递归神经网络对智能城市中的交通模式进行了分类，提出了一种能够有效捕捉交通模式动态和序列特征的新方法，并在实验中表现出超过现有方法的准确率、精确率、召回率和F1分数。研究结果对智能城市具有重要意义。

    

    本文研究了使用深度递归神经网络对智能城市中的交通模式进行分类的方法。我们提出了一种基于深度递归神经网络的交通模式分类新方法，能够有效捕捉交通模式的动态和序列特征。所提出的模型结合了卷积和递归层，从交通模式数据中提取特征，并使用SoftMax层对交通模式进行分类。实验结果表明，所提出的模型在准确率、精确率、召回率和F1分数方面优于现有方法。此外，我们对结果进行了深入分析，并讨论了所提出模型对智能城市的影响。结果表明，所提出的模型能够以高达95%的精确率准确分类智能城市中的交通模式。该模型在实际交通模式数据集上进行了评估，并与现有分类方法进行了比较。

    This paper examines the use of deep recurrent neural networks to classify traffic patterns in smart cities. We propose a novel approach to traffic pattern classification based on deep recurrent neural networks, which can effectively capture traffic patterns' dynamic and sequential features. The proposed model combines convolutional and recurrent layers to extract features from traffic pattern data and a SoftMax layer to classify traffic patterns. Experimental results show that the proposed model outperforms existing methods regarding accuracy, precision, recall, and F1 score. Furthermore, we provide an in depth analysis of the results and discuss the implications of the proposed model for smart cities. The results show that the proposed model can accurately classify traffic patterns in smart cities with a precision of as high as 95%. The proposed model is evaluated on a real world traffic pattern dataset and compared with existing classification methods.
    
[^86]: 从推特到引用：揭示社交媒体影响者对人工智能研究可见性的影响

    Tweets to Citations: Unveiling the Impact of Social Media Influencers on AI Research Visibility. (arXiv:2401.13782v1 [cs.DL])

    [http://arxiv.org/abs/2401.13782](http://arxiv.org/abs/2401.13782)

    本文研究了社交媒体影响者在提高机器学习研究的可见性方面的作用，发现被这些影响者认可的论文引用次数显著增加，中位数引用次数比对照组高2-3倍。此外，该研究还探讨了被展示作者的地理、性别和机构多样性。

    

    随着人工智能和机器学习会议上被接受的论文数量达到数千篇，研究人员如何获取和阅读研究论文变得不清楚。本文研究了社交媒体影响者在增强机器学习研究可见性中的作用，特别是他们分享的论文引用次数。我们编制了一个包括8000多篇论文的全面数据集，涵盖了2018年12月至2023年10月的推特，以及基于出版年份、会议地点和摘要主题进行1：1匹配的对照组。我们的分析揭示了这些影响者认可的论文引用次数显著增加，中位数引用次数比对照组高2-3倍。此外，该研究还深入研究了被展示作者的地理、性别和机构多样性。这些发现突显了社交媒体在学术交流中的不断扩大的影响力，并强调了当今数字化时代不断发展的生态系统的重要性。

    As the number of accepted papers at AI and ML conferences reaches into the thousands, it has become unclear how researchers access and read research publications. In this paper, we investigate the role of social media influencers in enhancing the visibility of machine learning research, particularly the citation counts of papers they share. We have compiled a comprehensive dataset of over 8,000 papers, spanning tweets from December 2018 to October 2023, alongside 1:1 matched controls based on publication year, venue, and abstract topics. Our analysis reveals a significant increase in citations for papers endorsed by these influencers, with median citation counts 2-3 times higher than those of the control group. Additionally, the study delves into the geographic, gender, and institutional diversity of highlighted authors. These findings highlight the expanding influence of social media in scholarly communication and underscore the importance of an evolving ecosystem in today's digital a
    
[^87]: 更快的收敛速度和更少的通信成本：用于无线网络的基于广播的子图采样的去中心化学习

    Faster Convergence with Less Communication: Broadcast-Based Subgraph Sampling for Decentralized Learning over Wireless Networks. (arXiv:2401.13779v1 [cs.IT])

    [http://arxiv.org/abs/2401.13779](http://arxiv.org/abs/2401.13779)

    本文提出了一种名为BASS的基于广播的子图采样方法，用于加速去中心化学习算法的收敛速度，并减少通信成本。

    

    基于共识的去中心化随机梯度下降(D-SGD)是一种广泛采用的算法，用于网络代理之间的去中心化机器学习模型训练。D-SGD的一个关键部分是基于共识的模型平均，它严重依赖于节点之间的信息交换和融合。特别地，对于在无线网络上的共识平均，通信协调是必要的，以确定节点何时以及如何访问信道，并将信息传输（或接收）给（或从）邻居节点。在这项工作中，我们提出了一种名为BASS的基于广播的子图采样方法，旨在加快D-SGD的收敛速度，并考虑每轮迭代的实际通信成本。BASS创建一组混合矩阵候选项，表示基础拓扑的稀疏子图。在每个共识迭代中，将采样一个混合矩阵，从而产生一个特定的调度决策，激活多个无碰撞的节点子集。

    Consensus-based decentralized stochastic gradient descent (D-SGD) is a widely adopted algorithm for decentralized training of machine learning models across networked agents. A crucial part of D-SGD is the consensus-based model averaging, which heavily relies on information exchange and fusion among the nodes. Specifically, for consensus averaging over wireless networks, communication coordination is necessary to determine when and how a node can access the channel and transmit (or receive) information to (or from) its neighbors. In this work, we propose $\texttt{BASS}$, a broadcast-based subgraph sampling method designed to accelerate the convergence of D-SGD while considering the actual communication cost per iteration. $\texttt{BASS}$ creates a set of mixing matrix candidates that represent sparser subgraphs of the base topology. In each consensus iteration, one mixing matrix is sampled, leading to a specific scheduling decision that activates multiple collision-free subsets of node
    
[^88]: 多视图图学习与一致图

    Multiview Graph Learning with Consensus Graph. (arXiv:2401.13769v1 [eess.SP])

    [http://arxiv.org/abs/2401.13769](http://arxiv.org/abs/2401.13769)

    这篇论文提出了一种用于多视图图学习的方法，通过一致图来确保视图之间的相似性和共享结构，解决了现有方法无法推断出视图共享结构的问题。

    

    图拓扑推断，即从给定的节点观察数据中学习图，是许多应用领域中的重要任务。现有的方法大多局限于学习单个图，假设观察数据是均匀的。这是有问题的，因为许多现代数据集是异质的或混合的，并涉及多个相关的图，即多视图图。最近的工作提出了学习多视图图，通过成对正则化保证了学习的视图图的相似性，即鼓励每对视图具有相似的结构。然而，这种方法无法推断出视图之间的共享结构。在这项工作中，我们提出了一种基于一致正则化的替代方法，通过学习表示视图的共同结构的一致图，确保视图之间的相似性。具体而言，我们提出了一个优化问题，假设图数据在多视图图上是平滑的，并且图的拓扑结构在视图之间是一致的。

    Graph topology inference, i.e., learning graphs from a given set of nodal observations, is a significant task in many application domains. Existing approaches are mostly limited to learning a single graph assuming that the observed data is homogeneous. This is problematic because many modern datasets are heterogeneous or mixed and involve multiple related graphs, i.e., multiview graphs. Recent work proposing to learn multiview graphs ensures the similarity of learned view graphs through pairwise regularization, where each pair of views is encouraged to have similar structures. However, this approach cannot infer the shared structure across views. In this work, we propose an alternative method based on consensus regularization, where views are ensured to be similar through a learned consensus graph representing the common structure of the views. In particular, we propose an optimization problem, where graph data is assumed to be smooth over the multiview graph and the topology of the in
    
[^89]: NLICE: 针对有效的初级医疗护理鉴别诊断的合成病历生成

    NLICE: Synthetic Medical Record Generation for Effective Primary Healthcare Differential Diagnosis. (arXiv:2401.13756v1 [cs.LG])

    [http://arxiv.org/abs/2401.13756](http://arxiv.org/abs/2401.13756)

    本文提出了一种针对初级医疗护理鉴别诊断的合成病历生成方法，通过使用医学知识和机器学习模型，构建了具有上下文信息的患者记录，并展示了使用这些数据训练疾病模型的有效性结果。

    

    本文提供了一种系统方法，用于创建基于医学知识的患者记录，用于鉴别诊断相关活动。此外，还提供了评估基于给定症状能够区分各种状况的机器学习模型。我们使用名为SymCat的公共疾病-症状数据源结合Synthea来构建患者记录。为了增加合成数据的表达性，我们使用了一种医学标准化的症状建模方法NLICE，为每种状况的合成数据增加额外的上下文信息。此外，还对朴素贝叶斯和随机森林模型在合成数据上进行了评估和比较。本文展示了如何成功构建基于SymCat和NLICE的数据集，并展示了使用这些数据集进行训练预测疾病模型的有效性结果。

    This paper offers a systematic method for creating medical knowledge-grounded patient records for use in activities involving differential diagnosis. Additionally, an assessment of machine learning models that can differentiate between various conditions based on given symptoms is also provided. We use a public disease-symptom data source called SymCat in combination with Synthea to construct the patients records. In order to increase the expressive nature of the synthetic data, we use a medically-standardized symptom modeling method called NLICE to augment the synthetic data with additional contextual information for each condition. In addition, Naive Bayes and Random Forest models are evaluated and compared on the synthetic data. The paper shows how to successfully construct SymCat-based and NLICE-based datasets. We also show results for the effectiveness of using the datasets to train predictive disease models. The SymCat-based dataset is able to train a Naive Bayes and Random Fores
    
[^90]: 一种针对深度卷积神经网络的鲁棒性建模的系统化方法

    A Systematic Approach to Robustness Modelling for Deep Convolutional Neural Networks. (arXiv:2401.13751v1 [cs.LG])

    [http://arxiv.org/abs/2401.13751](http://arxiv.org/abs/2401.13751)

    本论文提出一种系统化方法，用于针对深度卷积神经网络进行鲁棒性建模。研究发现隐藏层数量对模型的推广性能有影响，同时还测试了模型大小、浮点精度、训练数据和模型输出的噪声水平等参数。为了改进模型的预测能力和计算成本，提出了一种使用诱发故障来建模故障概率的方法。

    

    当有大量标记数据可用时，卷积神经网络已经被证明在许多领域都可以广泛应用。最近的趋势是使用具有越来越多可调参数的模型，以提高模型准确性，降低模型损失或创建更具对抗鲁棒性的模型，而这些目标通常相互矛盾。特别是，最近的理论研究提出了对更大模型能否推广到受控的训练和测试集之外的数据的疑问。因此，我们研究了ResNet模型中隐藏层的数量在MNIST、CIFAR10和CIFAR100数据集上的作用。我们测试了各种参数，包括模型的大小、浮点精度，以及训练数据和模型输出的噪声水平。为了改进模型的预测能力和计算成本，我们提供了一种使用诱发故障来建模故障概率的方法。

    Convolutional neural networks have shown to be widely applicable to a large number of fields when large amounts of labelled data are available. The recent trend has been to use models with increasingly larger sets of tunable parameters to increase model accuracy, reduce model loss, or create more adversarially robust models -- goals that are often at odds with one another. In particular, recent theoretical work raises questions about the ability for even larger models to generalize to data outside of the controlled train and test sets. As such, we examine the role of the number of hidden layers in the ResNet model, demonstrated on the MNIST, CIFAR10, CIFAR100 datasets. We test a variety of parameters including the size of the model, the floating point precision, and the noise level of both the training data and the model output. To encapsulate the model's predictive power and computational cost, we provide a method that uses induced failures to model the probability of failure as a fun
    
[^91]: 《规范预测集提升人类决策能力》

    Conformal Prediction Sets Improve Human Decision Making. (arXiv:2401.13744v1 [cs.LG])

    [http://arxiv.org/abs/2401.13744](http://arxiv.org/abs/2401.13744)

    该研究表明，通过规范预测量化模型的不确定性，可以提高人类决策的准确性和效果，对人机协同决策具有实用价值。

    

    作为对日常查询的回应，人类明确地表达不确定性，并在不确定的情况下提供替代答案。通过规范预测输出校准的预测集，模仿了人类的这种行为；更大的预测集表示更大的不确定性，同时提供了替代方案。在这项工作中，我们通过实施预注册的随机对照试验，并给人类受试者提供规范预测集，研究了规范预测集对人类决策的实用性。通过统计学显著性，我们发现当人类获得规范预测集时，他们在任务上的准确性比使用相同覆盖保证的固定尺寸预测集时有所提高。结果表明，用规范预测量化模型的不确定性有助于人机协同决策和人工智能团队的决策。

    In response to everyday queries, humans explicitly signal uncertainty and offer alternative answers when they are unsure. Machine learning models that output calibrated prediction sets through conformal prediction mimic this human behaviour; larger sets signal greater uncertainty while providing alternatives. In this work, we study the usefulness of conformal prediction sets as an aid for human decision making by conducting a pre-registered randomized controlled trial with conformal prediction sets provided to human subjects. With statistical significance, we find that when humans are given conformal prediction sets their accuracy on tasks improves compared to fixed-size prediction sets with the same coverage guarantee. The results show that quantifying model uncertainty with conformal prediction is helpful for human-in-the-loop decision making and human-AI teams.
    
[^92]: 在大规模语言模型输出中支持理解的工具

    Supporting Sensemaking of Large Language Model Outputs at Scale. (arXiv:2401.13726v1 [cs.HC])

    [http://arxiv.org/abs/2401.13726](http://arxiv.org/abs/2401.13726)

    该论文研究了如何同时展示大规模语言模型的多个回应，并设计了五个功能来支持用户在不同任务中进行理解。通过用户研究和案例研究，该研究发现这些功能能够支持各种理解任务，甚至解决了以前被认为过于困难的任务。

    

    大规模语言模型能够对单个提示生成多种回应，然而很少有研究工作花费在帮助最终用户或系统设计者利用这种能力上。本文探讨了如何同时呈现多个大规模语言模型的回应。我们设计了五个功能，其中包括计算文本文档之间相似性和差异性的现有和新颖方法，以及如何呈现它们的输出。我们拥有一项受控用户研究（n=24）和八个案例研究，评估了这些功能以及它们如何支持用户进行不同任务。我们发现这些功能支持各种理解任务，甚至使以前被参与者认为过于困难的任务变得可行。最后，我们提出了设计指南，以指导未来对新的大规模语言模型接口的探索。

    Large language models (LLMs) are capable of generating multiple responses to a single prompt, yet little effort has been expended to help end-users or system designers make use of this capability. In this paper, we explore how to present many LLM responses at once. We design five features, which include both pre-existing and novel methods for computing similarities and differences across textual documents, as well as how to render their outputs. We report on a controlled user study (n=24) and eight case studies evaluating these features and how they support users in different tasks. We find that the features support a wide variety of sensemaking tasks and even make tasks previously considered to be too difficult by our participants now tractable. Finally, we present design guidelines to inform future explorations of new LLM interfaces.
    
[^93]: 无监督领域自适应回归中的不确定性引导对齐

    Uncertainty-Guided Alignment for Unsupervised Domain Adaptation in Regression. (arXiv:2401.13721v1 [cs.CV])

    [http://arxiv.org/abs/2401.13721](http://arxiv.org/abs/2401.13721)

    该论文提出了一种利用不确定性引导的无监督领域自适应回归方法，通过将不确定性作为置信度估计和嵌入空间的正则项来实现对齐。

    

    无监督领域自适应回归（UDAR）旨在将来自有标签源领域的模型调整到无标签目标领域，以完成回归任务。最近在UDAR领域取得的成功主要集中在子空间对齐上，涉及整个特征空间中所选择子空间的对齐。这与用于分类的特征对齐方法形成对比，后者旨在对齐整个特征空间，在分类任务中已被证明是有效的，但在回归任务中效果较差。具体而言，分类任务旨在在整个嵌入空间的维度上识别独立的簇，而回归任务对数据表示的结构性要求较低，需要额外的指导以实现有效的对齐。在本文中，我们提出了一种通过不确定性引导的有效UDAR方法。我们的方法具有双重作用：提供了对预测结果的置信度衡量，并作为嵌入空间的正则化。具体而言，我们利用深度证据模型来提供对预测的置信度估计，并将其作为嵌入空间的正则项进行优化。

    Unsupervised Domain Adaptation for Regression (UDAR) aims to adapt a model from a labeled source domain to an unlabeled target domain for regression tasks. Recent successful works in UDAR mostly focus on subspace alignment, involving the alignment of a selected subspace within the entire feature space. This contrasts with the feature alignment methods used for classification, which aim at aligning the entire feature space and have proven effective but are less so in regression settings. Specifically, while classification aims to identify separate clusters across the entire embedding dimension, regression induces less structure in the data representation, necessitating additional guidance for efficient alignment. In this paper, we propose an effective method for UDAR by incorporating guidance from uncertainty. Our approach serves a dual purpose: providing a measure of confidence in predictions and acting as a regularization of the embedding space. Specifically, we leverage the Deep Evid
    
[^94]: 不需要分类层的人脸识别模型的推理攻击

    Inference Attacks Against Face Recognition Model without Classification Layers. (arXiv:2401.13719v1 [cs.CV])

    [http://arxiv.org/abs/2401.13719](http://arxiv.org/abs/2401.13719)

    提出了一种针对没有分类层的实际人脸识别模型的新型推理攻击，通过分析中间特征与批归一化参数之间的距离，设计了一个简单但有效的攻击模型，可以确定一个人脸图像是否属于模型的成员。

    

    人脸识别(FR)已经应用于日常生活的几乎所有方面，但它始终伴随着泄漏私人信息的潜在风险。目前，几乎所有对FR的攻击模型都严重依赖于分类层的存在。然而，在实践中，FR模型可以通过模型骨干获得输入的复杂特征，然后与目标进行推理比较，这与明确采用对数或其他损失的分类层的输出无关。在这项工作中，我们提出了一种针对没有分类层的实际FR模型的新型推理攻击，由两个阶段组成。第一阶段是成员推理攻击。具体而言，我们分析了中间特征和批归一化(BN)参数之间的距离。结果表明，这个距离是成员推理的关键指标。因此，我们设计了一个简单但有效的攻击模型，可以确定一个人脸图像是否属于模型的成员。

    Face recognition (FR) has been applied to nearly every aspect of daily life, but it is always accompanied by the underlying risk of leaking private information. At present, almost all attack models against FR rely heavily on the presence of a classification layer. However, in practice, the FR model can obtain complex features of the input via the model backbone, and then compare it with the target for inference, which does not explicitly involve the outputs of the classification layer adopting logit or other losses. In this work, we advocate a novel inference attack composed of two stages for practical FR models without a classification layer. The first stage is the membership inference attack. Specifically, We analyze the distances between the intermediate features and batch normalization (BN) parameters. The results indicate that this distance is a critical metric for membership inference. We thus design a simple but effective attack model that can determine whether a face image is f
    
[^95]: 我能相信我的假数据吗--一种综合质量评估框架用于医疗领域的合成表格数据。

    Can I trust my fake data -- A comprehensive quality assessment framework for synthetic tabular data in healthcare. (arXiv:2401.13716v1 [cs.LG])

    [http://arxiv.org/abs/2401.13716](http://arxiv.org/abs/2401.13716)

    这项研究开发了一个综合质量评估框架，用于评估医疗领域中合成表格数据的质量，并解决了现有框架中存在的一些问题和限制。

    

    在医疗领域，确保人工智能工具的安全采用取决于获得足够的训练、测试和验证数据。为了解决隐私问题和监管要求，使用合成数据被提出。合成数据是通过在真实数据上训练生成器来创建具有类似统计属性的数据集。不同分类标准的竞争性指标已被提出，导致了一个复杂的情况。优化质量需要平衡使数据适用于使用的考虑因素，但现有框架中留下了一些相关的维度。我们对在表格医疗数据范围内评估质量指标和使用深度生成方法生成的合成数据进行了全面的文献综述。基于此和团队的集体经验，我们开发了一个概念性的质量保证框架。该框架的适用性与荷兰国家癌症登记中心的实际案例进行了基准测试。

    Ensuring safe adoption of AI tools in healthcare hinges on access to sufficient data for training, testing and validation. In response to privacy concerns and regulatory requirements, using synthetic data has been suggested. Synthetic data is created by training a generator on real data to produce a dataset with similar statistical properties. Competing metrics with differing taxonomies for quality evaluation have been suggested, resulting in a complex landscape. Optimising quality entails balancing considerations that make the data fit for use, yet relevant dimensions are left out of existing frameworks. We performed a comprehensive literature review on the use of quality evaluation metrics on SD within the scope of tabular healthcare data and SD made using deep generative methods. Based on this and the collective team experiences, we developed a conceptual framework for quality assurance. The applicability was benchmarked against a practical case from the Dutch National Cancer Regist
    
[^96]: 基于价值驱动的混合精度量化在微控制器上进行基于块的推理

    Value-Driven Mixed-Precision Quantization for Patch-Based Inference on Microcontrollers. (arXiv:2401.13714v1 [cs.CV])

    [http://arxiv.org/abs/2401.13714](http://arxiv.org/abs/2401.13714)

    本文提出了一种名为QuantMCU的基于价值驱动的混合精度量化方法，用于在微控制器上进行基于块的推理。通过利用价值驱动块分类(VDPC)将块分为两类，并对包含异常值的块进行8位量化，QuantMCU能够减少冗余计算，从而提高模型执行效率。

    

    在微控制器单元（MCU）上部署神经网络面临着计算和存储资源有限的严重挑战。之前的研究探索了基于块的推理作为一种在不损失模型精度的情况下节省存储器的策略。然而，这种技术存在严重的冗余计算开销，导致执行延迟显著增加。解决这个问题的可行解决方案是混合精度量化，但它面临着精度降级和耗时搜索的挑战。在本文中，我们提出了QuantMCU，一种利用基于价值驱动的混合精度量化来减少冗余计算的新型基于块的推理方法。我们首先利用基于价值驱动的块分类（VDPC）来保持模型的精度。VDPC将块分为两类，根据它们是否包含异常值。对于包含异常值的块，我们对特征图进行8位量化。

    Deploying neural networks on microcontroller units (MCUs) presents substantial challenges due to their constrained computation and memory resources. Previous researches have explored patch-based inference as a strategy to conserve memory without sacrificing model accuracy. However, this technique suffers from severe redundant computation overhead, leading to a substantial increase in execution latency. A feasible solution to address this issue is mixed-precision quantization, but it faces the challenges of accuracy degradation and a time-consuming search time. In this paper, we propose QuantMCU, a novel patch-based inference method that utilizes value-driven mixed-precision quantization to reduce redundant computation. We first utilize value-driven patch classification (VDPC) to maintain the model accuracy. VDPC classifies patches into two classes based on whether they contain outlier values. For patches containing outlier values, we apply 8-bit quantization to the feature maps on the 
    
[^97]: EMP: 有效的多维持久化用于图表示学习

    EMP: Effective Multidimensional Persistence for Graph Representation Learning. (arXiv:2401.13713v1 [cs.LG])

    [http://arxiv.org/abs/2401.13713](http://arxiv.org/abs/2401.13713)

    本论文提出了EMP框架，通过同时变化多个尺度参数来探索数据，并将描述符函数整合到分析过程中，从而提供一个高度表达的数据摘要。

    

    拓扑数据分析(TDA)在从流形学习到图分类的各种机器学习任务中越来越受到关注。TDA中的关键技术是持久化同调(PH)，通过跟踪潜在结构随尺度参数变化的演化，为数据提供独特的拓扑特征。现有的PH工具仅能通过单个过滤参数对数据进行分析。然而，许多情况下需要考虑多个相关参数以获取更详细的数据洞察。我们通过引入有效的多维持久化(EMP)框架来解决这个问题。该框架通过同时改变多个尺度参数来探索数据。该框架将描述符函数整合到分析过程中，得到一个高度表达的数据摘要。它无缝地将建立的单一PH摘要集成到多维对应物中，如EMP景观、轮廓线等。

    Topological data analysis (TDA) is gaining prominence across a wide spectrum of machine learning tasks that spans from manifold learning to graph classification. A pivotal technique within TDA is persistent homology (PH), which furnishes an exclusive topological imprint of data by tracing the evolution of latent structures as a scale parameter changes. Present PH tools are confined to analyzing data through a single filter parameter. However, many scenarios necessitate the consideration of multiple relevant parameters to attain finer insights into the data. We address this issue by introducing the Effective Multidimensional Persistence (EMP) framework. This framework empowers the exploration of data by simultaneously varying multiple scale parameters. The framework integrates descriptor functions into the analysis process, yielding a highly expressive data summary. It seamlessly integrates established single PH summaries into multidimensional counterparts like EMP Landscapes, Silhouett
    
[^98]: 加速双曲线t-SNE算法

    Accelerating hyperbolic t-SNE. (arXiv:2401.13708v1 [cs.HC])

    [http://arxiv.org/abs/2401.13708](http://arxiv.org/abs/2401.13708)

    本文提出了加速双曲线t-SNE算法，通过引入极坐标四叉树的加速结构，解决了现有方法在处理大规模输入数据时的效率问题。

    

    在各个领域中，理解层次化或高维数据的结构是必要的。双曲空间已经被证明是一种重要的嵌入计算和分析工具，因为它们的非线性特性很适合处理树状或图形数据。因此，它们也被用于高维数据的可视化，其中它们表现出更好的嵌入性能。然而，现有的嵌入到双曲空间的降维方法都不能很好地处理输入数据的规模扩展问题。这是因为嵌入是通过迭代优化方案计算的，而每次迭代的计算成本与输入规模的平方成正比。此外，由于双曲空间的非线性特性，欧几里德加速结构不能直接转化为双曲设置。本文介绍了第一个用于双曲嵌入加速的结构，基于极坐标四叉树。我们将其应用于t-SNE算法，提出了加速双曲线t-SNE算法。

    The need to understand the structure of hierarchical or high-dimensional data is present in a variety of fields. Hyperbolic spaces have proven to be an important tool for embedding computations and analysis tasks as their non-linear nature lends itself well to tree or graph data. Subsequently, they have also been used in the visualization of high-dimensional data, where they exhibit increased embedding performance. However, none of the existing dimensionality reduction methods for embedding into hyperbolic spaces scale well with the size of the input data. That is because the embeddings are computed via iterative optimization schemes and the computation cost of every iteration is quadratic in the size of the input. Furthermore, due to the non-linear nature of hyperbolic spaces, Euclidean acceleration structures cannot directly be translated to the hyperbolic setting. This paper introduces the first acceleration structure for hyperbolic embeddings, building upon a polar quadtree. We com
    
[^99]: 生成式人工智能驱动的物联网健康护理中的人类数字孪生：一项综合调查

    Generative AI-Driven Human Digital Twin in IoT-Healthcare: A Comprehensive Survey. (arXiv:2401.13699v1 [cs.HC])

    [http://arxiv.org/abs/2401.13699](http://arxiv.org/abs/2401.13699)

    该论文调查了在物联网健康护理中利用生成式人工智能驱动的人类数字孪生的应用。人类数字孪生作为多功能、生动的人类数字测试平台，可以模拟结果并指导实际治疗，从而提高物联网健康护理的能力。

    

    物联网可以显著提高人类生活的质量，特别是在健康护理方面吸引了广泛的关注。同时，人类数字孪生被提出作为一种创新的范式，可以全面地描述个体人体在数字世界中的复制，并实时反映其物理状况。自然地，人类数字孪生被设想为通过充当多功能、生动的人类数字测试平台来增强物联网健康护理的能力，模拟结果并指导实际治疗。然而，成功建立人类数字孪生需要高保真度的虚拟建模和强大的信息交互，但可能存在稀缺、偏倚和噪声数据。幸运的是，最近流行的一种名为生成式人工智能（GAI）的技术可能是一个有前途的解决方案，因为它可以利用先进的人工智能算法自动生成、操作和修改渐变视图。

    The Internet of things (IoT) can significantly enhance the quality of human life, specifically in healthcare, attracting extensive attentions to IoT-healthcare services. Meanwhile, the human digital twin (HDT) is proposed as an innovative paradigm that can comprehensively characterize the replication of the individual human body in the digital world and reflect its physical status in real time. Naturally, HDT is envisioned to empower IoT-healthcare beyond the application of healthcare monitoring by acting as a versatile and vivid human digital testbed, simulating the outcomes and guiding the practical treatments. However, successfully establishing HDT requires high-fidelity virtual modeling and strong information interactions but possibly with scarce, biased and noisy data. Fortunately, a recent popular technology called generative artificial intelligence (GAI) may be a promising solution because it can leverage advanced AI algorithms to automatically create, manipulate, and modify val
    
[^100]: 通过可微分图神经网络模拟器进行颗粒流的反演分析

    Inverse analysis of granular flows using differentiable graph neural network simulator. (arXiv:2401.13695v1 [physics.geo-ph])

    [http://arxiv.org/abs/2401.13695](http://arxiv.org/abs/2401.13695)

    通过使用可微分图神经网络模拟器进行反演分析，解决了传统模拟器计算开销大、不可微等问题，提高了计算效率和准确性。

    

    颗粒流中的反演问题，如山体滑坡和碎屑流，涉及基于目标波动剖面估计材料参数或边界条件。传统的高保真度模拟器对这些反演问题是计算密集型的，限制了可能的模拟次数。此外，它们的不可微性使得梯度优化方法无法应用，而这些方法在高维问题中以其效率而闻名。虽然基于机器学习的代理模型提供了计算效率和可微性，但由于其依赖于低维输入-输出映射，无法捕捉到颗粒流的完整物理过程，因此往往难以推广到训练数据之外。我们提出了一种新颖的可微分图神经网络模拟器(GNS)，通过将图神经网络的反向模式自动微分与基于梯度的优化相结合，用于解决反演问题。GNS学习了颗粒流的动力学特性。

    Inverse problems in granular flows, such as landslides and debris flows, involve estimating material parameters or boundary conditions based on target runout profile. Traditional high-fidelity simulators for these inverse problems are computationally demanding, restricting the number of simulations possible. Additionally, their non-differentiable nature makes gradient-based optimization methods, known for their efficiency in high-dimensional problems, inapplicable. While machine learning-based surrogate models offer computational efficiency and differentiability, they often struggle to generalize beyond their training data due to their reliance on low-dimensional input-output mappings that fail to capture the complete physics of granular flows. We propose a novel differentiable graph neural network simulator (GNS) by combining reverse mode automatic differentiation of graph neural networks with gradient-based optimization for solving inverse problems. GNS learns the dynamics of granula
    
[^101]: 用于非结构化数据的过程挖掘：挑战与研究方向

    Process Mining for Unstructured Data: Challenges and Research Directions. (arXiv:2401.13677v1 [cs.DB])

    [http://arxiv.org/abs/2401.13677](http://arxiv.org/abs/2401.13677)

    这篇论文讨论了将过程挖掘应用于非结构化数据所面临的挑战，并提出了初步解决方案和未来的研究方向。

    

    将过程挖掘应用于非结构化数据可能显著提升对非结构化数据常见数据格式领域的新见解。要有效分析非结构化数据，并对分析结果传达信心，需要克服多个挑战。本文旨在讨论这些挑战，提出初步解决方案并描述未来的研究方向。我们希望这篇文章为未来在这个主题上的合作奠定基础。

    The application of process mining for unstructured data might significantly elevate novel insights into disciplines where unstructured data is a common data format. To efficiently analyze unstructured data by process mining and to convey confidence into the analysis result, requires bridging multiple challenges. The purpose of this paper is to discuss these challenges, present initial solutions and describe future research directions. We hope that this article lays the foundations for future collaboration on this topic.
    
[^102]: 马达加斯加可再生能源消费的决定因素：基于特征选择算法的证据。

    Determinants of renewable energy consumption in Madagascar: Evidence from feature selection algorithms. (arXiv:2401.13671v1 [econ.GN])

    [http://arxiv.org/abs/2401.13671](http://arxiv.org/abs/2401.13671)

    这项研究使用特征选择算法识别了马达加斯加可再生能源消费的影响因素，包括经济、金融、社会和环境方面的因素，并通过多种机器学习算法进行了评估。

    

    这篇论文的目的是识别马达加斯加可再生能源消费的影响因素。我们测试了宏观经济、金融、社会和环境等方面的12个特征，包括经济增长、国内投资、外国直接投资、金融发展、工业发展、通货膨胀、收入分配、贸易开放度、汇率、旅游业发展、环境质量和城市化。为了评估它们的重要性，我们假定再生能源消费与这些特征之间存在线性关系，时间跨度为1990年至2021年。接下来，我们应用了不同的机器学习特征选择算法，包括基于过滤器的（线性回归的相对重要性、相关方法），嵌入式的（LASSO），以及包装式的（最佳子集回归、逐步回归、递归特征消除、迭代预测子加权偏最小二乘法、Boruta、模拟退火和遗传算法）方法。我们的分析揭示了...

    The aim of this note is to identify the factors influencing renewable energy consumption in Madagascar. We tested 12 features covering macroeconomic, financial, social, and environmental aspects, including economic growth, domestic investment, foreign direct investment, financial development, industrial development, inflation, income distribution, trade openness, exchange rate, tourism development, environmental quality, and urbanization. To assess their significance, we assumed a linear relationship between renewable energy consumption and these features over the 1990-2021 period. Next, we applied different machine learning feature selection algorithms classified as filter-based (relative importance for linear regression, correlation method), embedded (LASSO), and wrapper-based (best subset regression, stepwise regression, recursive feature elimination, iterative predictor weighting partial least squares, Boruta, simulated annealing, and genetic algorithms) methods. Our analysis revea
    
[^103]: 常见的随机神经网络在可靠性临床决策支持方面的不足

    Inadequacy of common stochastic neural networks for reliable clinical decision support. (arXiv:2401.13657v1 [cs.LG])

    [http://arxiv.org/abs/2401.13657](http://arxiv.org/abs/2401.13657)

    本研究调查了随机神经网络在临床应用中的可靠性，并发现常见的深度学习方法在数据转移情况下过于自信。这强调了对本地不确定性可靠估计及其向最终用户传达的重要性。

    

    由于伦理和安全相关的关切，人工智能的广泛应用于医学决策仍然受阻。对于基于人工智能的医疗决策支持系统来说，可靠性和可信度至关重要。然而，常见的深度学习方法在数据转移下往往过于自信。这种在基于证据的场景之外不恰当的推理可能会产生严重后果。这凸显了对本地不确定性可靠估计及其向最终用户传达的重要性。尽管随机神经网络被誉为这些问题的潜在解决方案，但本研究调查了其在临床应用中的实际可靠性。我们以从MIMIC3研究中使用的EHR的ICU住院病死率预测为例来进行分析。对于EHR时间序列的预测，我们采用了仅编码器的Transformer模型。通过纳入常见方法来实现模型函数的随机性。

    Widespread adoption of AI for medical decision making is still hindered due to ethical and safety-related concerns. For AI-based decision support systems in healthcare settings it is paramount to be reliable and trustworthy. Common deep learning approaches, however, have the tendency towards overconfidence under data shift. Such inappropriate extrapolation beyond evidence-based scenarios may have dire consequences. This highlights the importance of reliable estimation of local uncertainty and its communication to the end user. While stochastic neural networks have been heralded as a potential solution to these issues, this study investigates their actual reliability in clinical applications. We centered our analysis on the exemplary use case of mortality prediction for ICU hospitalizations using EHR from MIMIC3 study. For predictions on the EHR time series, Encoder-Only Transformer models were employed. Stochasticity of model functions was achieved by incorporating common methods such 
    
[^104]: 基于稀疏网格的不连续性检测的图信息神经网络

    Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity Detectors. (arXiv:2401.13652v1 [cs.LG])

    [http://arxiv.org/abs/2401.13652](http://arxiv.org/abs/2401.13652)

    本文提出了一种利用图信息神经网络和稀疏网格来检测不连续函数不连续界面的新方法，该方法在维度大于3的情况下表现出高效且准确的不连续性检测能力，在维度n = 2和n = 4的函数上进行的实验验证了其高效性和泛化能力，并具有可移植性和多功能性。

    

    本文提出了一种新颖的方法来检测不连续函数的不连续界面。该方法利用了基于图的神经网络（GINNs）和稀疏网格来解决维度大于3的情况下的不连续性检测。训练过的GINNs在稀疏网格上识别有问题的点，并利用构建在网格上的图结构实现高效准确的不连续性检测性能。我们还引入了一种递归算法用于一般的基于稀疏网格的检测器，具有收敛性和易于应用性。在维度n=2和n=4的函数上进行的数值实验证明了GINNs在检测不连续界面方面的高效性和鲁棒泛化能力。值得注意的是，经过训练的GINNs具有可移植性和多功能性，可以集成到各种算法中并共享给用户。

    In this paper, we present a novel approach for detecting the discontinuity interfaces of a discontinuous function. This approach leverages Graph-Informed Neural Networks (GINNs) and sparse grids to address discontinuity detection also in domains of dimension larger than 3. GINNs, trained to identify troubled points on sparse grids, exploit graph structures built on the grids to achieve efficient and accurate discontinuity detection performances. We also introduce a recursive algorithm for general sparse grid-based detectors, characterized by convergence properties and easy applicability. Numerical experiments on functions with dimensions n = 2 and n = 4 demonstrate the efficiency and robust generalization of GINNs in detecting discontinuity interfaces. Notably, the trained GINNs offer portability and versatility, allowing integration into various algorithms and sharing among users.
    
[^105]: 基于集合的遮蔽粒子建模：走向自监督高能物理基础模型

    Masked Particle Modeling on Sets: Towards Self-Supervised High Energy Physics Foundation Models. (arXiv:2401.13537v1 [hep-ph])

    [http://arxiv.org/abs/2401.13537](http://arxiv.org/abs/2401.13537)

    本文提出了一种称为遮蔽粒子建模（MPM）的自监督方法，用于学习高能物理科学数据中无序输入的通用表示。该方法通过预训练学习置换不变的函数，在构建适用于多种任务的高能物理基础模型方面具有潜力。

    

    本文提出了一种称为"遮蔽粒子建模"（MPM）的自监督方法，用于学习高能物理（HEP）科学数据中无序输入的通用、可转移和可重用表示。这项工作提供了一种新颖的方案，通过基于遮蔽建模的预训练来学习集合上的置换不变函数。更一般地，这项工作在构建可以通过自监督学习进行通用预训练并稍后精调用于各种下游任务的HEP大型基础模型方面迈出了一步。在MPM中，集合中的粒子被遮蔽，训练的目标是恢复它们的身份，身份由预训练的向量量化变分自动编码器的离散化标记表示定义。我们研究了该方法在对撞机物理实验中高能喷注样本上的有效性，包括离散化、置换不变性和排序的影响。

    We propose \textit{masked particle modeling} (MPM) as a self-supervised method for learning generic, transferable, and reusable representations on unordered sets of inputs for use in high energy physics (HEP) scientific data. This work provides a novel scheme to perform masked modeling based pre-training to learn permutation invariant functions on sets. More generally, this work provides a step towards building large foundation models for HEP that can be generically pre-trained with self-supervised learning and later fine-tuned for a variety of down-stream tasks. In MPM, particles in a set are masked and the training objective is to recover their identity, as defined by a discretized token representation of a pre-trained vector quantized variational autoencoder. We study the efficacy of the method in samples of high energy jets at collider physics experiments, including studies on the impact of discretization, permutation invariance, and ordering. We also study the fine-tuning capabili
    
[^106]: 为联合分析优化调整基础模型

    Finetuning Foundation Models for Joint Analysis Optimization. (arXiv:2401.13536v1 [hep-ex])

    [http://arxiv.org/abs/2401.13536](http://arxiv.org/abs/2401.13536)

    本论文中展示了在高能物理学中，通过超越顺序优化或重建和分析组件的标准范 paradigm，可以实现性能和数据效率的显著提升。通过搜索通过中间 di-Higgs 系统衰变的重共振体为四个 $b$-喷注的示例用例，我们将高能物理学重建和分析与现代机器学习工作流进行了连接，并量化了其收益。

    

    在这项工作中，我们展示了在高能物理学中可以通过超越顺序优化或重建和分析组件的标准范 paradigm，实现性能和数据效率的显著提升。我们将高能物理学重建和分析与现代机器学习工作流（如预训练、微调、领域适应和高维嵌入空间）进行了概念上的连接，并量化了通过搜索通过中间 di-Higgs 系统衰变的重共振体为四个 $b$-喷注的示例用例中的收益。

    In this work we demonstrate that significant gains in performance and data efficiency can be achieved in High Energy Physics (HEP) by moving beyond the standard paradigm of sequential optimization or reconstruction and analysis components. We conceptually connect HEP reconstruction and analysis to modern machine learning workflows such as pretraining, finetuning, domain adaptation and high-dimensional embedding spaces and quantify the gains in the example usecase of searches of heavy resonances decaying via an intermediate di-Higgs system to four $b$-jets.
    
[^107]: 在Wasserstein概率空间上理解Riemannian SGD和SVRG流的研究

    Towards Understanding the Riemannian SGD and SVRG Flows on Wasserstein Probabilistic Space. (arXiv:2401.13530v1 [cs.LG])

    [http://arxiv.org/abs/2401.13530](http://arxiv.org/abs/2401.13530)

    本文研究了在Wasserstein概率空间上的Riemannian SGD和SVRG流的优化方法，通过构建随机微分方程来丰富Wasserstein空间中的连续优化方法。

    

    最近，对于Riemannian流形上的优化研究为优化领域提供了新的见解。在这方面，概率测度度量空间作为流形，配备第二阶Wasserstein距离，尤其引人关注，因为在其上的优化可以与实际的采样过程相关联。一般来说，Wasserstein空间上的最优化方法是Riemannian梯度流（即，在最小化KL散度时的Langevin动力学）。在本文中，我们旨在通过将梯度流延展到随机梯度下降（SGD）流和随机方差减少梯度（SVRG）流，丰富Wasserstein空间中的连续优化方法。Euclidean空间上的这两种流是标准的随机优化方法，而它们在Riemannian空间中的对应方法尚未被探索。通过利用Wasserstein空间中的结构，我们构建了一个随机微分方程（SDE）来近似离散动态。

    Recently, optimization on the Riemannian manifold has provided new insights to the optimization community. In this regard, the manifold taken as the probability measure metric space equipped with the second-order Wasserstein distance is of particular interest, since optimization on it can be linked to practical sampling processes. In general, the oracle (continuous) optimization method on Wasserstein space is Riemannian gradient flow (i.e., Langevin dynamics when minimizing KL divergence). In this paper, we aim to enrich the continuous optimization methods in the Wasserstein space by extending the gradient flow into the stochastic gradient descent (SGD) flow and stochastic variance reduction gradient (SVRG) flow. The two flows on Euclidean space are standard stochastic optimization methods, while their Riemannian counterparts are not explored yet. By leveraging the structures in Wasserstein space, we construct a stochastic differential equation (SDE) to approximate the discrete dynamic
    
[^108]: 对抗噪声标签的无偏样本选择

    Debiased Sample Selection for Combating Noisy Labels. (arXiv:2401.13360v1 [cs.LG])

    [http://arxiv.org/abs/2401.13360](http://arxiv.org/abs/2401.13360)

    本文提出了一个无噪声专家模型（ITEM）来解决样本选择中的训练偏差和数据偏差问题。通过设计一个鲁棒的网络架构来集成多个专家，可以减少选择集不平衡和累积错误，并在使用更少参数的情况下实现更好的选择和预测性能。

    

    学习使用噪声标签旨在确保模型在标签错误的训练集上具有泛化能力。样本选择策略通过选择可靠的标签子集来实现有希望的性能。本文实证表明，现有的样本选择方法在实践中存在数据和训练偏差，分别表示为选择集不平衡和累积错误。然而，先前的研究只处理了训练偏差。为了解决这个局限性，我们提出了一个适用于样本选择的无噪声专家模型（ITEM）。具体来说，为了减轻训练偏差，我们设计了一个鲁棒的网络架构，与多个专家集成。与目前的双分支网络相比，我们的网络在训练更少参数的情况下，通过集成这些专家来实现更好的选择和预测性能。同时，为了减轻数据偏差，我们提出了一种混合采样策略。

    Learning with noisy labels aims to ensure model generalization given a label-corrupted training set. The sample selection strategy achieves promising performance by selecting a label-reliable subset for model training. In this paper, we empirically reveal that existing sample selection methods suffer from both data and training bias that are represented as imbalanced selected sets and accumulation errors in practice, respectively. However, only the training bias was handled in previous studies. To address this limitation, we propose a noIse-Tolerant Expert Model (ITEM) for debiased learning in sample selection. Specifically, to mitigate the training bias, we design a robust network architecture that integrates with multiple experts. Compared with the prevailing double-branch network, our network exhibits better performance of selection and prediction by ensembling these experts while training with fewer parameters. Meanwhile, to mitigate the data bias, we propose a mixed sampling strat
    
[^109]: DittoGym:学习控制软形变机器人

    DittoGym: Learning to Control Soft Shape-Shifting Robots. (arXiv:2401.13231v1 [cs.RO] CROSS LISTED)

    [http://arxiv.org/abs/2401.13231](http://arxiv.org/abs/2401.13231)

    这篇论文介绍了一种学习控制软形变机器人的方法，并且提出了一个全面的强化学习基准系统DittoGym，该系统需要对机器人的形态进行细粒度变化来完成任务。

    

    机器人共同设计，其中机器人的形态优化与学习的策略共同解决特定任务，是一个新兴的研究领域。对于软机器人来说，这一领域具有特别的潜力，因为软机器人可以通过新颖的制造技术实现学习到的形态和执行器。受自然界和最近的新型机器人设计的启发，我们提出更进一步探索新型可重构机器人，即在其寿命内可以改变形态的机器人。我们将可重构软机器人的控制形式化为高维强化学习问题。我们在同一action空间中统一形态变化、运动和与环境的互动，并引入合适的粗到细的课程表，使我们能够发现实现对最终机器人进行细粒度控制的策略。我们还介绍了DittoGym，这是一个针对可重构软机器人的全面强化学习基准，需要对形态进行细粒度变化来完成任务。

    Robot co-design, where the morphology of a robot is optimized jointly with a learned policy to solve a specific task, is an emerging area of research. It holds particular promise for soft robots, which are amenable to novel manufacturing techniques that can realize learned morphologies and actuators. Inspired by nature and recent novel robot designs, we propose to go a step further and explore the novel reconfigurable robots, defined as robots that can change their morphology within their lifetime. We formalize control of reconfigurable soft robots as a high-dimensional reinforcement learning (RL) problem. We unify morphology change, locomotion, and environment interaction in the same action space, and introduce an appropriate, coarse-to-fine curriculum that enables us to discover policies that accomplish fine-grained control of the resulting robots. We also introduce DittoGym, a comprehensive RL benchmark for reconfigurable soft robots that require fine-grained morphology changes to a
    
[^110]: 二进制结构的物理信息神经网络用于解决具有快速变化解的方程

    Binary structured physics-informed neural networks for solving equations with rapidly changing solutions. (arXiv:2401.12806v1 [cs.LG])

    [http://arxiv.org/abs/2401.12806](http://arxiv.org/abs/2401.12806)

    本论文提出了一种二进制结构的物理信息神经网络框架，通过利用二进制结构来捕捉局部特征，并解决了传统物理信息神经网络在处理具有快速变化解的方程时的困难。

    

    物理信息神经网络(PINNs)，基于深度学习，已成为解决偏微分方程(PDEs)的一种有前途的方法。通过将PDEs描述的物理信息嵌入前馈神经网络中，PINNs被训练为替代模型，以近似解决方案而无需标签数据。然而，尽管PINNs表现出了卓越的性能，但它们在处理具有快速变化解的方程时可能会遇到困难。这些困难包括收敛速度慢、易陷入局部最小值和解决精度降低。为了解决这些问题，我们提出了一种二进制结构的物理信息神经网络(BsPINN)框架，该框架使用二进制结构的神经网络(BsNN)作为神经网络组件。通过利用二进制结构，BsPINNs在捕捉局部特征方面表现出色

    Physics-informed neural networks (PINNs), rooted in deep learning, have emerged as a promising approach for solving partial differential equations (PDEs). By embedding the physical information described by PDEs into feedforward neural networks, PINNs are trained as surrogate models to approximate solutions without the need for label data. Nevertheless, even though PINNs have shown remarkable performance, they can face difficulties, especially when dealing with equations featuring rapidly changing solutions. These difficulties encompass slow convergence, susceptibility to becoming trapped in local minima, and reduced solution accuracy. To address these issues, we propose a binary structured physics-informed neural network (BsPINN) framework, which employs binary structured neural network (BsNN) as the neural network component. By leveraging a binary structure that reduces inter-neuron connections compared to fully connected neural networks, BsPINNs excel in capturing the local features 
    
[^111]: 基于能量的自动化模型评估

    Energy-based Automated Model Evaluation. (arXiv:2401.12689v1 [cs.LG])

    [http://arxiv.org/abs/2401.12689](http://arxiv.org/abs/2401.12689)

    提出了一种基于能量的自动化模型评估方法，通过建立关于个体样本相关信息的元分布统计量，能够更高效和有效地评估机器学习模型的性能，解决了AutoEval框架中的过度自信、存储和计算成本高等问题。

    

    传统的机器学习模型评估协议依赖于标记的、假设独立同分布的测试数据集，而这在实际应用中往往并不常见。自动模型评估（AutoEval）提出了一种替代传统工作流程的方法，通过形成一个接近预测性能的测试管线，而无需真实标签的存在。尽管AutoEval框架近年来取得了一些成功，但仍存在过度自信、存储和计算成本高的问题。因此，我们提出了一种新颖的度量方式——元分布能量（MDE），它可以使AutoEval框架更加高效和有效。MDE的核心是建立一个关于个体样本相关信息（能量）的元分布统计量，然后通过基于能量的学习提供更平滑的表示能力。我们通过将MDE与分类损失相连接，进一步提供了理论洞见。我们还提供了大量实验证据来验证我们的方法。

    The conventional evaluation protocols on machine learning models rely heavily on a labeled, i.i.d-assumed testing dataset, which is not often present in real world applications. The Automated Model Evaluation (AutoEval) shows an alternative to this traditional workflow, by forming a proximal prediction pipeline of the testing performance without the presence of ground-truth labels. Despite its recent successes, the AutoEval frameworks still suffer from an overconfidence issue, substantial storage and computational cost. In that regard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that allows the AutoEval framework to be both more efficient and effective. The core of the MDE is to establish a meta-distribution statistic, on the information (energy) associated with individual samples, then offer a smoother representation enabled by energy-based learning. We further provide our theoretical insights by connecting the MDE with the classification loss. We provide extensive
    
[^112]: BiTA: 大语言模型中无损加速的双向调整

    BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language Models. (arXiv:2401.12522v1 [cs.CL])

    [http://arxiv.org/abs/2401.12522](http://arxiv.org/abs/2401.12522)

    BiTA是一种用于大语言模型的创新方法，通过双向调整实现了无损加速。它采用简化的半自回归生成和草稿验证，通过高效的基于树的解码同时进行候选生成和验证，提高了推理效率。这种方法不需要额外的辅助模型或显著的额外内存开销。

    

    大型语言模型（LLMs）通常在推理过程中使用自回归生成，导致高内存带宽需求和延迟延长。为了减轻这种效率低下的问题，我们提出了一种创新方法——双向调整以实现无损加速（BiTA），通过简化的半自回归生成和草稿验证来加速LLMs。受启发于提示调整的概念，我们使用一种参数高效的设计，称为双向调整，来增强LLMs在半自回归生成方面的能力。采用高效的基于树的解码，模型可以同时进行草稿候选生成和验证，确保输出结果与它们的自回归对应物在贪婪抽样下完全相同。BiTA作为一个轻量级的插件模块，可以无缝增强现有LLMs的推理效率，而无需额外的辅助模型或承担显著的额外内存开销。通过应用提出的BiTA，LLaMA-2-70B-Chat实现了

    Large language models (LLMs) commonly employ autoregressive generation during inference, leading to high memory bandwidth demand and consequently extended latency. To mitigate this inefficiency, we present Bi-directional Tuning for lossless Acceleration (BiTA), an innovative method expediting LLMs via streamlined semi-autoregressive generation and draft verification. Inspired by the concept of prompt tuning, we enhance LLMs with a parameter-efficient design called bi-directional tuning for the capability in semi-autoregressive generation. Employing efficient tree-based decoding, the models perform draft candidate generation and verification in parallel, ensuring outputs identical to their autoregressive counterparts under greedy sampling. BiTA serves as a lightweight plug-in module, seamlessly boosting the inference efficiency of existing LLMs without requiring additional assistance models or incurring significant extra memory costs. Applying the proposed BiTA, LLaMA-2-70B-Chat achieve
    
[^113]: 大型语言模型的指令指纹识别

    Instructional Fingerprinting of Large Language Models. (arXiv:2401.12255v1 [cs.CR])

    [http://arxiv.org/abs/2401.12255](http://arxiv.org/abs/2401.12255)

    这项研究提出了一种指纹识别大型语言模型的方法，通过轻量级的指令调整，保护知识产权并确保遵守许可条款。实验证明这种方法不影响模型的正常行为，并且具有鲁棒性和高效训练的特点。

    

    从零开始训练大型语言模型（LLM）的巨大成本使得对模型进行指纹识别以保护知识产权成为必要，通过所有权认证并确保下游用户和开发者遵守许可条款（如限制商业使用）。在这项研究中，我们提出了LLM指纹识别的试点研究，作为一种非常轻量级的指令调整形式。模型发布者指定一个机密的私钥，并将其植入为一个指令后门，当密钥存在时，导致LLM生成特定的文本。对11个常用LLMs的结果表明，这种方法轻量级且不影响模型的正常行为。它还可以防止发布者过度宣称，对指纹猜测和参数高效训练保持鲁棒性，并支持类似于MIT许可证的多阶段指纹识别。代码可在https://cnut1648.github.io/Model-Fingerprint/中获得。

    The exorbitant cost of training Large language models (LLMs) from scratch makes it essential to fingerprint the models to protect intellectual property via ownership authentication and to ensure downstream users and developers comply with their license terms (e.g. restricting commercial use). In this study, we present a pilot study on LLM fingerprinting as a form of very lightweight instruction tuning. Model publisher specifies a confidential private key and implants it as an instruction backdoor that causes the LLM to generate specific text when the key is present. Results on 11 popularly-used LLMs showed that this approach is lightweight and does not affect the normal behavior of the model. It also prevents publisher overclaim, maintains robustness against fingerprint guessing and parameter-efficient training, and supports multi-stage fingerprinting akin to MIT License. Code is available in https://cnut1648.github.io/Model-Fingerprint/.
    
[^114]: 无害过度拟合对敌对鲁棒性的意外危害

    The Surprising Harmfulness of Benign Overfitting for Adversarial Robustness. (arXiv:2401.12236v1 [cs.LG])

    [http://arxiv.org/abs/2401.12236](http://arxiv.org/abs/2401.12236)

    这项研究证明了即使机器学习模型在训练过程中对噪声数据拟合得很好，对敌对示例具有鲁棒性，但当面临敌对操纵的数据时，过度拟合的模型可能会给系统带来意外的危害。

    

    最近的实证和理论研究已经证明了大规模机器学习模型对训练噪声数据的泛化能力。在本文中，我们证明了一个令人惊讶的结果：即使真正的数据本身对敌对示例具有鲁棒性，而且过度拟合的模型在“标准”的样本外风险目标上是无害的，但在样本外数据受到敌对操纵时，这种无害的过度拟合过程可能是有害的。具体而言，我们的主要结果包含两个部分：（i）在过度参数化线性模型中，最小范数估计总是在“无害过度拟合”设置中导致敌对易受攻击；（ii）我们验证了每个岭回归估计器的标准风险和“敌对”风险之间的渐进权衡结果，这意味着在适当的条件下，这两个项目不能同时通过任何单个岭正则化参数的选择来保持很小。

    Recent empirical and theoretical studies have established the generalization capabilities of large machine learning models that are trained to (approximately or exactly) fit noisy data. In this work, we prove a surprising result that even if the ground truth itself is robust to adversarial examples, and the benignly overfitted model is benign in terms of the ``standard'' out-of-sample risk objective, this benign overfitting process can be harmful when out-of-sample data are subject to adversarial manipulation. More specifically, our main results contain two parts: (i) the min-norm estimator in overparameterized linear model always leads to adversarial vulnerability in the ``benign overfitting'' setting; (ii) we verify an asymptotic trade-off result between the standard risk and the ``adversarial'' risk of every ridge regression estimator, implying that under suitable conditions these two items cannot both be small at the same time by any single choice of the ridge regularization parame
    
[^115]: TurboSVM-FL: 通过SVM聚合为懒惰客户端增强联邦学习

    TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for Lazy Clients. (arXiv:2401.12012v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.12012](http://arxiv.org/abs/2401.12012)

    TurboSVM-FL是一种新颖的联邦聚合策略，通过SVM聚合为懒惰客户端增强联邦学习。这种策略在不增加客户端计算负担的情况下解决了联邦学习中的收敛速度慢的问题。

    

    联邦学习是一种分布式协作机器学习范例，在近年来获得了强烈的推动力。在联邦学习中，中央服务器定期通过客户端协调模型，并聚合由客户端在本地训练的模型，而无需访问本地数据。尽管具有潜力，但联邦学习的实施仍然面临一些挑战，主要是由于数据异质性导致的收敛速度慢。收敛速度慢在跨设备联邦学习场景中尤为问题，其中客户端可能受到计算能力和存储空间的严重限制，因此对客户端产生额外计算或内存负担的方法，如辅助目标项和更大的训练迭代次数，可能不实际。在本文中，我们提出了一种新颖的联邦聚合策略TurboSVM-FL，它不会给客户端增加额外的计算负担

    Federated learning is a distributed collaborative machine learning paradigm that has gained strong momentum in recent years. In federated learning, a central server periodically coordinates models with clients and aggregates the models trained locally by clients without necessitating access to local data. Despite its potential, the implementation of federated learning continues to encounter several challenges, predominantly the slow convergence that is largely due to data heterogeneity. The slow convergence becomes particularly problematic in cross-device federated learning scenarios where clients may be strongly limited by computing power and storage space, and hence counteracting methods that induce additional computation or memory cost on the client side such as auxiliary objective terms and larger training iterations can be impractical. In this paper, we propose a novel federated aggregation strategy, TurboSVM-FL, that poses no additional computation burden on the client side and c
    
[^116]: 《最优输运理论与多智能体强化学习之间的协同作用》

    The Synergy Between Optimal Transport Theory and Multi-Agent Reinforcement Learning. (arXiv:2401.10949v1 [cs.MA])

    [http://arxiv.org/abs/2401.10949](http://arxiv.org/abs/2401.10949)

    本文研究了最优输运理论与多智能体强化学习之间的协同作用。通过利用最优输运来处理分布和运输问题，增强了多智能体强化学习的效率、协调性和适应性。通过在政策对齐、分布式资源管理、应对非平稳性、可扩展的多智能体学习和提高能源效率五个方面应用最优输运理论，为解决可扩展性问题、优化资源分配和在合作环境中对齐智能体策略提供了新的方法。

    

    本文探讨了最优输运（OT）理论与多智能体强化学习（MARL）的整合。该整合使用OT处理分布和运输问题，以提高MARL的效率、协调性和适应性。OT在以下五个关键领域可以影响MARL：（1）政策对齐，利用OT的Wasserstein度量来将不同的智能体策略对齐到统一的目标上；（2）分布式资源管理，利用OT来优化智能体之间的资源分配；（3）应对非平稳性，利用OT适应动态环境变化；（4）可扩展的多智能体学习，利用OT将大规模学习目标分解为可管理的任务；（5）提高能源效率，应用OT原则来开发可持续的MARL系统。本文阐述了OT与MARL之间的协同作用如何解决可扩展性问题、优化资源分配、在合作环境中对齐智能体策略。

    This paper explores the integration of optimal transport (OT) theory with multi-agent reinforcement learning (MARL). This integration uses OT to handle distributions and transportation problems to enhance the efficiency, coordination, and adaptability of MARL. There are five key areas where OT can impact MARL: (1) policy alignment, where OT's Wasserstein metric is used to align divergent agent strategies towards unified goals; (2) distributed resource management, employing OT to optimize resource allocation among agents; (3) addressing non-stationarity, using OT to adapt to dynamic environmental shifts; (4) scalable multi-agent learning, harnessing OT for decomposing large-scale learning objectives into manageable tasks; and (5) enhancing energy efficiency, applying OT principles to develop sustainable MARL systems. This paper articulates how the synergy between OT and MARL can address scalability issues, optimize resource distribution, align agent policies in cooperative environments,
    
[^117]: 供应链风险评估中的人工智能：一项系统文献综述和文献计量分析

    AI in Supply Chain Risk Assessment: A Systematic Literature Review and Bibliometric Analysis. (arXiv:2401.10895v1 [cs.LG])

    [http://arxiv.org/abs/2401.10895](http://arxiv.org/abs/2401.10895)

    本文通过系统文献综述和文献计量分析，填补了供应链风险评估中新兴人工智能/机器学习技术的研究空白，为了解这些技术在实践中的实际影响提供了关键见解。

    

    通过整合人工智能和机器学习技术，供应链风险评估(SCRA)经历了深刻的演变，革新了预测能力和风险缓解策略。这种演变的重要性在于在现代供应链中确保运营的韧性和连续性，需要稳健的风险管理策略。以往的综述已经概述了已建立的方法，但忽视了新兴的人工智能/机器学习技术，在理解其在SCRA中的实际影响方面存在明显的研究空白。本文进行了系统的文献综述，并结合了全面的文献计量分析。我们仔细研究了1717篇论文，并从2014年至2023年之间发表的48篇文章中获得了关键见解。该综述填补了这一研究空白，通过回答关键研究问题，探究了现有的人工智能/机器学习技术、方法论、研究结果和未来发展方向。

    Supply chain risk assessment (SCRA) has witnessed a profound evolution through the integration of artificial intelligence (AI) and machine learning (ML) techniques, revolutionizing predictive capabilities and risk mitigation strategies. The significance of this evolution stems from the critical role of robust risk management strategies in ensuring operational resilience and continuity within modern supply chains. Previous reviews have outlined established methodologies but have overlooked emerging AI/ML techniques, leaving a notable research gap in understanding their practical implications within SCRA. This paper conducts a systematic literature review combined with a comprehensive bibliometric analysis. We meticulously examined 1,717 papers and derived key insights from a select group of 48 articles published between 2014 and 2023. The review fills this research gap by addressing pivotal research questions, and exploring existing AI/ML techniques, methodologies, findings, and future 
    
[^118]: 被忽视的黑塞 (Hessian) 组件解释了锐化正则化中的谜团

    Neglected Hessian component explains mysteries in Sharpness regularization. (arXiv:2401.10809v1 [cs.LG])

    [http://arxiv.org/abs/2401.10809](http://arxiv.org/abs/2401.10809)

    这篇论文研究了在深度学习中，明确或隐含地惩罚二阶信息可以提高泛化性能，而权重噪声和梯度惩罚则很少能带来这样的好处。作者通过对损失的黑塞矩阵的结构进行解释，提出了特征的开发和特征的探索之间的量化分离。同时，作者发现忽视的非线性建模误差矩阵 (NME) 实际上很重要，可以解释为什么梯度惩罚对激活函数的选择非常敏感。此外，作者通过设计干预措施来改进性能，并提供了证据挑战了以往的观点，认为权重噪声和梯度惩罚是等效的。

    

    最近的研究表明，像SAM这样明确或隐含地惩罚二阶信息的方法可以提高深度学习中的泛化性能。类似的方法，如权重噪声和梯度惩罚，经常无法提供这样的好处。我们展示了这些差异可以通过损失的黑塞矩阵的结构来解释。首先，我们展示了常用的黑塞矩阵的分解可以定量解释为将特征的开发和特征的探索分开。特征的探索可以通过非线性建模误差矩阵(NME)来描述，这在文献中通常被忽视，因为它在插值中消失。我们的工作表明，NME事实上很重要，因为它可以解释为什么梯度惩罚对激活函数的选择敏感。利用这一见解，我们设计了改进性能的干预措施。我们也提供了证据来挑战了长期以来权重噪声和梯度惩罚的等效性。

    Recent work has shown that methods like SAM which either explicitly or implicitly penalize second order information can improve generalization in deep learning. Seemingly similar methods like weight noise and gradient penalties often fail to provide such benefits. We show that these differences can be explained by the structure of the Hessian of the loss. First, we show that a common decomposition of the Hessian can be quantitatively interpreted as separating the feature exploitation from feature exploration. The feature exploration, which can be described by the Nonlinear Modeling Error matrix (NME), is commonly neglected in the literature since it vanishes at interpolation. Our work shows that the NME is in fact important as it can explain why gradient penalties are sensitive to the choice of activation function. Using this insight we design interventions to improve performance. We also provide evidence that challenges the long held equivalence of weight noise and gradient penalties.
    
[^119]: Mementos: 一种针对图像序列的多模态大型语言模型推理的综合基准测试

    Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences. (arXiv:2401.10529v1 [cs.CV])

    [http://arxiv.org/abs/2401.10529](http://arxiv.org/abs/2401.10529)

    Mementos是一个新的基准测试，旨在评估多模态大型语言模型在图像序列推理中的能力。研究发现，现有的MLLM在准确描述图像序列的动态信息方面存在困难，容易导致物体及其行为的错误描述或错觉。

    

    多模态大型语言模型（MLLMs）在处理各种视觉语言任务方面展示了高超的能力。然而，目前的MLLM基准测试主要用于评估基于单个图像的静态信息的推理能力，而现代MLLM在从图像序列中进行推断的能力，在理解不断变化的世界方面的重要性却被较少研究。为了解决这一挑战，本文引入了一个新的基准测试Mementos，用于评估MLLM的序列图像推理能力。Mementos包括4761个具有不同长度的多样的图像序列。我们还采用了GPT-4辅助方法来评估MLLM的推理性能。通过对Mementos中包括GPT-4V和Gemini在内的九个最新MLLM进行仔细评估，我们发现它们在准确描述所给图像序列的动态信息方面存在困难，往往导致对象及其对应行为的错误描述或错觉。

    Multimodal Large Language Models (MLLMs) have demonstrated proficiency in handling a variety of visual-language tasks. However, current MLLM benchmarks are predominantly designed to evaluate reasoning based on static information about a single image, and the ability of modern MLLMs to extrapolate from image sequences, which is essential for understanding our ever-changing world, has been less investigated. To address this challenge, this paper introduces Mementos, a new benchmark designed to assess MLLMs' sequential image reasoning abilities. Mementos features 4,761 diverse image sequences with varying lengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning performance. Through a careful evaluation of nine recent MLLMs on Mementos, including GPT-4V and Gemini, we find that they struggle to accurately describe dynamic information about given image sequences, often leading to hallucinations/misrepresentations of objects and their corresponding behaviors. Our quantitati
    
[^120]: 使用分布式随机网络蒸馏的探索和反探索

    Exploration and Anti-Exploration with Distributional Random Network Distillation. (arXiv:2401.09750v1 [cs.LG])

    [http://arxiv.org/abs/2401.09750](http://arxiv.org/abs/2401.09750)

    该论文提出了一种新的深度强化学习探索算法，称为分布式随机网络蒸馏（DRND）。该算法通过蒸馏随机网络的分布和隐式融入伪计数来改进奖励分配的精度，从而增强了探索过程。理论分析和实验结果均表明该方法的优越性。

    

    在深度强化学习中，探索仍然是一个重要问题，对于一个智能体在未知环境中取得高回报至关重要。虽然目前的探索随机网络蒸馏（Random Network Distillation，RND）算法已在许多环境中证明有效，但它在奖励分配上往往需要更高的区分能力。本文突出了RND中的“奖励不一致”问题，并指出了其主要限制。为了解决这个问题，我们引入了分布式RND（DRND），它是RND的一个变体。DRND通过蒸馏随机网络的分布并隐式地融入伪计数来改进奖励分配的精度，从而增强了探索过程。我们的方法有效地缓解了不一致问题，而不会引入显著的计算开销。理论分析和实验结果均证明了我们方法的优越性。

    Exploration remains a critical issue in deep reinforcement learning for an agent to attain high returns in unknown environments. Although the prevailing exploration Random Network Distillation (RND) algorithm has been demonstrated to be effective in numerous environments, it often needs more discriminative power in bonus allocation. This paper highlights the ``bonus inconsistency'' issue within RND, pinpointing its primary limitation. To address this issue, we introduce the Distributional RND (DRND), a derivative of the RND. DRND enhances the exploration process by distilling a distribution of random networks and implicitly incorporating pseudo counts to improve the precision of bonus allocation. This refinement encourages agents to engage in more extensive exploration. Our method effectively mitigates the inconsistency issue without introducing significant computational overhead. Both theoretical analysis and experimental results demonstrate the superiority of our approach over the or
    
[^121]: SymTC:一种用于腰椎MRI实例分割的共生Transformer-CNN网络

    SymTC: A Symbiotic Transformer-CNN Net for Instance Segmentation of Lumbar Spine MRI. (arXiv:2401.09627v1 [eess.IV])

    [http://arxiv.org/abs/2401.09627](http://arxiv.org/abs/2401.09627)

    SymTC是一种新颖的腰椎MR图像分割模型，通过将Transformer和CNN相结合，并利用位置嵌入和自注意力模块，实现了更准确的实例分割。

    

    椎间盘疾病是一种常见病症，经常导致间歇性或持续性的腰背疼痛，对该疾病的诊断和评估依赖于腰椎MR图像中椎骨和椎间盘几何形状的准确测量。深度神经网络（DNN）模型可以帮助临床医生以更高效的方式自动化地对腰椎的个体实例（椎骨和椎间盘）进行图像分割，这被称为实例图像分割。在这项工作中，我们提出了SymTC，一种创新的腰椎MR图像分割模型，它结合了Transformer和卷积神经网络（CNN）的优势。具体而言，我们设计了一个并行的双路径架构来融合CNN层和Transformer层，并在Transformer的自注意力模块中集成了一种新颖的位置嵌入，增强了位置信息的利用以实现更准确的分割。为了进一步提高模型的性能，我们引入了一个新的定位系统进行模型优化。

    Intervertebral disc disease, a prevalent ailment, frequently leads to intermittent or persistent low back pain, and diagnosing and assessing of this disease rely on accurate measurement of vertebral bone and intervertebral disc geometries from lumbar MR images. Deep neural network (DNN) models may assist clinicians with more efficient image segmentation of individual instances (disks and vertebrae) of the lumbar spine in an automated way, which is termed as instance image segmentation. In this work, we proposed SymTC, an innovative lumbar spine MR image segmentation model that combines the strengths of Transformer and Convolutional Neural Network (CNN). Specifically, we designed a parallel dual-path architecture to merge CNN layers and Transformer layers, and we integrated a novel position embedding into the self-attention module of Transformer, enhancing the utilization of positional information for more accurate segmentation. To further improves model performance, we introduced a new
    
[^122]: cedar：可组合和优化的机器学习输入数据管道

    cedar: Composable and Optimized Machine Learning Input Data Pipelines. (arXiv:2401.08895v1 [cs.LG])

    [http://arxiv.org/abs/2401.08895](http://arxiv.org/abs/2401.08895)

    cedar是一个编程模型和框架，可以轻松构建、优化和执行机器学习输入数据管道。它提供了易于使用的编程接口和可组合运算符，支持任意ML框架和库。通过解决当前输入数据系统无法充分利用性能优化的问题，cedar提高了资源利用效率，满足了庞大数据量和高训练吞吐量的需求。

    

    输入数据管道是每个机器学习（ML）训练任务的重要组成部分。它负责读取大量的训练数据，使用复杂的变换处理样本批次，并以低延迟和高吞吐量将其加载到训练节点上。高性能的输入数据系统变得越来越关键，原因是数据量急剧增加和训练吞吐量的要求。然而，当前的输入数据系统无法充分利用关键的性能优化，导致资源利用效率极低的基础设施，或者更糟糕地，浪费昂贵的加速器。为了满足这些需求，我们提出了cedar，一个编程模型和框架，允许用户轻松构建、优化和执行输入数据管道。cedar提供了易于使用的编程接口，允许用户使用可组合运算符来定义支持任意ML框架和库的输入数据管道。

    The input data pipeline is an essential component of each machine learning (ML) training job. It is responsible for reading massive amounts of training data, processing batches of samples using complex of transformations, and loading them onto training nodes at low latency and high throughput. Performant input data systems are becoming increasingly critical, driven by skyrocketing data volumes and training throughput demands. Unfortunately, current input data systems cannot fully leverage key performance optimizations, resulting in hugely inefficient infrastructures that require significant resources -- or worse -- underutilize expensive accelerators.  To address these demands, we present cedar, a programming model and framework that allows users to easily build, optimize, and execute input data pipelines. cedar presents an easy-to-use programming interface, allowing users to define input data pipelines using composable operators that support arbitrary ML frameworks and libraries. Mean
    
[^123]: Shabari: 延迟决策以实现更快、更高效的无服务器函数

    Shabari: Delayed Decision-Making for Faster and Efficient Serverless Function. (arXiv:2401.08859v1 [cs.DC])

    [http://arxiv.org/abs/2401.08859](http://arxiv.org/abs/2401.08859)

    Shabari是一个延迟决策的无服务器资源管理框架，通过对函数输入的延迟来减轻无服务器系统中的性能变异性和资源低利用率问题。

    

    无服务器计算减轻了开发人员对资源管理的负担，为用户提供了易用性，并为提供者优化资源利用率提供了机会。然而，今天的无服务器系统在函数调用方面缺乏性能保证，从而限制了对性能关键应用程序的支持：我们观察到严重的性能变异性（高达6倍）。提供者缺乏对用户函数的可见性，因此很难对其进行合适的资源规模化：我们观察到严重的资源低利用率（高达80%）。为了理解性能变异性和低利用率背后的原因，我们对常见部署的无服务器函数进行了测量研究，并了解到函数性能和资源利用率关键取决于函数语义和输入。我们的主要认识是在函数输入可用之后延迟资源分配决策。我们引入了Shabari，一个用于无服务器的资源管理框架。

    Serverless computing relieves developers from the burden of resource management, thus providing ease-of-use to the users and the opportunity to optimize resource utilization for the providers. However, today's serverless systems lack performance guarantees for function invocations, thus limiting support for performance-critical applications: we observed severe performance variability (up to 6x). Providers lack visibility into user functions and hence find it challenging to right-size them: we observed heavy resource underutilization (up to 80%). To understand the causes behind the performance variability and underutilization, we conducted a measurement study of commonly deployed serverless functions and learned that the function performance and resource utilization depend crucially on function semantics and inputs. Our key insight is to delay making resource allocation decisions until after the function inputs are available. We introduce Shabari, a resource management framework for ser
    
[^124]: 可靠的测试时自适应的解耦样本学习

    Decoupled Prototype Learning for Reliable Test-Time Adaptation. (arXiv:2401.08703v1 [cs.LG])

    [http://arxiv.org/abs/2401.08703](http://arxiv.org/abs/2401.08703)

    提出了一种名为Decoupled Prototype Learning (DPL)的解耦样本学习方法，通过使用样本原型为中心的损失计算，来解决测试时自适应中噪声伪标签的问题。

    

    测试时自适应（TTA）是在推理过程中持续将预训练的源模型适应到目标域的任务。一种常见的方法是使用估计的伪标签使用交叉熵损失微调模型。然而，该方法的性能很容易受到噪声伪标签的影响。本研究发现，将每个样本的分类错误最小化会使交叉熵损失对标签噪声非常敏感。为解决这个问题，我们提出了一种新颖的解耦样本学习（DPL）方法，采用样本原型为中心的损失计算。首先，我们解耦了类原型的优化。对于每个类原型，我们采用对比方式减小其与正样本的距离，并增加其与负样本的距离。这种策略可以防止模型对噪声伪标签过拟合。其次，我们提出了一种基于记忆的策略，以增强DPL在TTA中经常遇到的小批量情况下的鲁棒性。我们更新每个类别的原型时使用记忆策略。

    Test-time adaptation (TTA) is a task that continually adapts a pre-trained source model to the target domain during inference. One popular approach involves fine-tuning model with cross-entropy loss according to estimated pseudo-labels. However, its performance is significantly affected by noisy pseudo-labels. This study reveals that minimizing the classification error of each sample causes the cross-entropy loss's vulnerability to label noise. To address this issue, we propose a novel Decoupled Prototype Learning (DPL) method that features prototype-centric loss computation. First, we decouple the optimization of class prototypes. For each class prototype, we reduce its distance with positive samples and enlarge its distance with negative samples in a contrastive manner. This strategy prevents the model from overfitting to noisy pseudo-labels. Second, we propose a memory-based strategy to enhance DPL's robustness for the small batch sizes often encountered in TTA. We update each class
    
[^125]: SAiD: 使用扩散方法驱动的语音驱动表情动画

    SAiD: Speech-driven Blendshape Facial Animation with Diffusion. (arXiv:2401.08655v1 [cs.CV])

    [http://arxiv.org/abs/2401.08655](http://arxiv.org/abs/2401.08655)

    提出了一种使用扩散模型（SAiD）驱动的语音驱动的三维面部动画方法，通过轻量级的Transformer-based U-Net模型和音频与视觉的交叉模态对齐偏差，实现了较好的唇部同步和更多样化的唇部运动。

    

    尽管进行了大量研究，但由于缺乏大规模的视听数据集，语音驱动的三维面部动画仍然具有挑战性。大多数过去的工作通常采用最小二乘法在小数据集上学习回归模型，但在从语音生成各种唇部动作方面遇到困难，并且需要大量精细调整生成的输出结果。为了解决这些问题，我们提出了一种使用扩散模型（SAiD）驱动的语音驱动的三维面部动画，这是一种轻量级的基于Transformer的U-Net模型，具有音频和视觉之间的交叉模态对齐偏差，以增强唇部同步。此外，我们还介绍了BlendVOCA，这是一种语音音频和混合形状面部模型参数对的基准数据集，以解决公共资源的缺乏问题。我们的实验结果表明，所提出的方法在唇部同步方面达到了与基线相当或更好的性能，确保了更多样化的唇部运动，并简化了动画流程。

    Speech-driven 3D facial animation is challenging due to the scarcity of large-scale visual-audio datasets despite extensive research. Most prior works, typically focused on learning regression models on a small dataset using the method of least squares, encounter difficulties generating diverse lip movements from speech and require substantial effort in refining the generated outputs. To address these issues, we propose a speech-driven 3D facial animation with a diffusion model (SAiD), a lightweight Transformer-based U-Net with a cross-modality alignment bias between audio and visual to enhance lip synchronization. Moreover, we introduce BlendVOCA, a benchmark dataset of pairs of speech audio and parameters of a blendshape facial model, to address the scarcity of public resources. Our experimental results demonstrate that the proposed approach achieves comparable or superior performance in lip synchronization to baselines, ensures more diverse lip movements, and streamlines the animati
    
[^126]: DiConStruct: 基于黑盒精华的因果概念解释

    DiConStruct: Causal Concept-based Explanations through Black-Box Distillation. (arXiv:2401.08534v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.08534](http://arxiv.org/abs/2401.08534)

    DiConStruct是一种基于黑盒模型的因果概念解释方法，通过创建结构性因果模型和概念归因方式提供更具可解释性的局部解释。

    

    模型可解释性在人工智能决策系统中起着核心作用。理想情况下，解释应该使用人可解释的语义概念来表达。此外，解释器应该捕捉这些概念之间的因果关系，以便对解释进行推理。最后，解释方法应该高效，并不损害预测任务的性能。尽管近年来AI解释性取得了快速进展，但据我们所知，至今没有一种方法满足这三个条件。事实上，主流的局部概念可解释性方法不产生因果解释，并在解释性和预测性能之间存在权衡。我们提出了DiConStruct，一种既基于概念又具有因果性的解释方法，旨在通过结构性因果模型和概念归因方式创建更具可解释性的局部解释。我们的解释器作为一个精华模型适用于任何黑盒机器学习模型。

    Model interpretability plays a central role in human-AI decision-making systems. Ideally, explanations should be expressed using human-interpretable semantic concepts. Moreover, the causal relations between these concepts should be captured by the explainer to allow for reasoning about the explanations. Lastly, explanation methods should be efficient and not compromise the performance of the predictive task. Despite the rapid advances in AI explainability in recent years, as far as we know to date, no method fulfills these three properties. Indeed, mainstream methods for local concept explainability do not produce causal explanations and incur a trade-off between explainability and prediction performance. We present DiConStruct, an explanation method that is both concept-based and causal, with the goal of creating more interpretable local explanations in the form of structural causal models and concept attributions. Our explainer works as a distillation model to any black-box machine l
    
[^127]: 对比困惑度在受控生成中的应用：清洁大型语言模型

    Contrastive Perplexity for Controlled Generation: An Application in Detoxifying Large Language Models. (arXiv:2401.08491v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.08491](http://arxiv.org/abs/2401.08491)

    这项研究研究了对比学习目标的集成到微调大型语言模型中，以解决其产生不可取内容的问题，并展示了在清洁领域中有效减少有害内容生成的方法。

    

    大型语言模型产生不可取和事实不正确的内容在很大程度上是一个挑战和未解决的问题。本文研究了对比学习目标的集成，用于微调语言模型以进行隐式知识编辑和受控文本生成。通过对比方式优化训练目标，即对齐文本的困惑度。为了以自监督的方式训练模型，我们利用现成的语言模型来生成训练数据。我们展示了在清洁领域的适用性。在此过程中，所提出的方法显著减少了生成有害内容的数量，同时保留了对于常识推理和阅读理解等下游任务的实用性。所提出的方法在概念上简单但经验上强大。

    The generation of undesirable and factually incorrect content of large language models poses a significant challenge and remains largely an unsolved issue. This paper studies the integration of a contrastive learning objective for fine-tuning LLMs for implicit knowledge editing and controlled text generation. Optimizing the training objective entails aligning text perplexities in a contrastive fashion. To facilitate training the model in a self-supervised fashion, we leverage an off-the-shelf LLM for training data generation. We showcase applicability in the domain of detoxification. Herein, the proposed approach leads to a significant decrease in the generation of toxic content while preserving general utility for downstream tasks such as commonsense reasoning and reading comprehension. The proposed approach is conceptually simple but empirically powerful.
    
[^128]: 能否用概率反馈推动在线平台对用户产生影响？

    Can Probabilistic Feedback Drive User Impacts in Online Platforms?. (arXiv:2401.05304v1 [cs.LG])

    [http://arxiv.org/abs/2401.05304](http://arxiv.org/abs/2401.05304)

    这项工作探讨了内容推荐系统可能对用户产生的负面影响，并指出这种影响不仅可能由平台目标与用户福利不一致引起，还可能由学习算法对不同内容的反馈率差异造成，提出了使用多臂赌博机框架和概率反馈的解决方法。

    

    常见的解释是内容推荐系统对用户产生负面影响是由于平台目标与用户福利之间的不对齐。在这项工作中，我们展示了平台目标不一致并不是对用户产生意外影响的唯一潜在原因：即使平台目标完全与用户福利一致，学习算法也可能对用户产生负面影响。这些用户影响的来源是不同内容可能以不同的速率产生可观察的用户反应（反馈信息）；这些反馈速率可能与影响用户体验的内容属性（如争议性或创作者的人口相似度）相关。由于反馈速率的差异可能会影响学习算法与不同内容的交互频率，学习算法可能会无意中推广具有某些特定属性的内容。使用多臂赌博机框架与概率反馈，我们提出了一种新的方法来解决这一问题。

    A common explanation for negative user impacts of content recommender systems is misalignment between the platform's objective and user welfare. In this work, we show that misalignment in the platform's objective is not the only potential cause of unintended impacts on users: even when the platform's objective is fully aligned with user welfare, the platform's learning algorithm can induce negative downstream impacts on users. The source of these user impacts is that different pieces of content may generate observable user reactions (feedback information) at different rates; these feedback rates may correlate with content properties, such as controversiality or demographic similarity of the creator, that affect the user experience. Since differences in feedback rates can impact how often the learning algorithm engages with different content, the learning algorithm may inadvertently promote content with certain such properties. Using the multi-armed bandit framework with probabilistic f
    
[^129]: 基于Split Learning的肌电假肢控制中的收敛速率最大化

    Convergence Rate Maximization for Split Learning-based Control of EMG Prosthetic Devices. (arXiv:2401.03233v1 [cs.LG])

    [http://arxiv.org/abs/2401.03233](http://arxiv.org/abs/2401.03233)

    本文介绍了一种用于最大化模型收敛速率的Split Learning肌电假肢控制中的切层选择算法，通过加速收敛过程提高了假肢控制的性能。

    

    Split Learning (SL)是一种有前途的分布式学习方法，可以在资源有限的环境中应用于基于肌电的假肢控制。与深度学习和联邦学习等其他学习方法相比，SL能够提供更优的解决方案，因为假肢设备在处理能力和电池寿命方面非常有限。在这些情况下，实现SL的可行性源于其固有的模型分割，其中客户端执行较小的模型部分。然而，选择不恰当的切层会阻碍SL系统的训练过程。本文提出了一种用于最大化模型收敛速率的切层选择算法。性能评估表明，所提出的算法在改善假肢控制的肌电模式识别任务中显著加速了收敛过程。

    Split Learning (SL) is a promising Distributed Learning approach in electromyography (EMG) based prosthetic control, due to its applicability within resource-constrained environments. Other learning approaches, such as Deep Learning and Federated Learning (FL), provide suboptimal solutions, since prosthetic devices are extremely limited in terms of processing power and battery life. The viability of implementing SL in such scenarios is caused by its inherent model partitioning, with clients executing the smaller model segment. However, selecting an inadequate cut layer hinders the training process in SL systems. This paper presents an algorithm for optimal cut layer selection in terms of maximizing the convergence rate of the model. The performance evaluation demonstrates that the proposed algorithm substantially accelerates the convergence in an EMG pattern recognition task for improving prosthetic device control.
    
[^130]: 在晶体材料研究中，将协方差和表达能力融合为深度哈密顿回归：一种混合级联回归框架

    Harmonizing Covariance and Expressiveness for Deep Hamiltonian Regression in Crystalline Material Research: a Hybrid Cascaded Regression Framework. (arXiv:2401.00744v2 [physics.comp-ph] UPDATED)

    [http://arxiv.org/abs/2401.00744](http://arxiv.org/abs/2401.00744)

    在深度哈密顿回归中，实现协方差和网络表达能力之间的平衡一直是一个挑战。本文提出了一个混合级联回归框架，在第一阶段通过协变神经网络建模并产生协变特征和基线预测，辅助第二阶段学习协方差。同时，第二阶段使用非线性图形Transformer网络进行结构建模，提高了哈密顿预测的表达能力。

    

    在材料研究中，深度学习用于哈密顿回归量子系统需要满足协方差定律，其中实现SO(3)等变性而不损失网络的表达能力是一个难以解决的挑战，因为非线性映射的理论等变性保证受限。为了解决协方差-表达能力困境，我们提出了一种混合框架，分为两个级联回归阶段。第一阶段使用一个理论上保证的协变神经网络来建模三维原子系统的对称性，产生理论上的协变特征和基线哈密顿预测，帮助第二阶段学习协变性。同时，第二阶段使用我们提出的非线性三维图形Transformer网络来进行三维原子系统的结构建模，将第一阶段的输出精细化为具有更好表达能力的哈密顿预测。通过理论上的协变性和更好的表达能力，实现了对哈密顿回归的改进。

    Deep learning for Hamiltonian regression of quantum systems in material research necessitates satisfying the covariance laws, among which achieving SO(3)-equivariance without sacrificing the expressiveness of networks remains an elusive challenge due to the restriction to non-linear mappings on guaranteeing theoretical equivariance. To alleviate the covariance-expressiveness dilemma, we propose a hybrid framework with two cascaded regression stages. The first stage, with a theoretically-guaranteed covariant neural network modeling symmetry properties of 3D atom systems, yields theoretically covariant features and baseline Hamiltonian predictions, assisting the second stage in learning covariance. Meanwhile, the second stage, powered by a non-linear 3D graph Transformer network we propose for structural modeling of 3D atomic systems, refines the first stage's output as a fine-grained prediction of Hamiltonians with better expressiveness capability. The combination of a theoretically cov
    
[^131]: 基于梯度采样优化的残差神经网络的鲁棒剪枝

    Robust Neural Pruning with Gradient Sampling Optimization for Residual Neural Networks. (arXiv:2312.16020v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.16020](http://arxiv.org/abs/2312.16020)

    本研究通过在剪枝过程中应用梯度采样技术，实现了在资源有限的场景中保持高准确性水平的鲁棒剪枝。相关实验证明，采用梯度采样技术进行优化的模型比传统优化方法更能有效地保持准确性。这一创新方法在各种数据集和神经架构上得到了验证，并且理论上解释了梯度采样技术对模型鲁棒性的贡献。

    

    在本研究中，我们探讨了一种创新的神经网络优化方法，重点是在剪枝过程中应用梯度采样技术，类似于StochGradAdam中的技术。我们的主要目标是在资源有限的场景中保持剪枝模型的高准确性水平，这是一个关键性挑战。我们的广泛实验表明，采用梯度采样技术优化的模型在剪枝过程中比使用传统优化方法更有效地保持准确性。这一发现强调了梯度采样在促进鲁棒学习和使网络在复杂度大大降低后仍能保留关键信息方面的重要性。我们通过各种数据集和神经架构验证了我们的方法，展示了它的广泛适用性和效果。该论文还深入探讨了理论方面，解释了梯度采样技术如何增强模型的鲁棒性。

    In this study, we explore an innovative approach for neural network optimization, focusing on the application of gradient sampling techniques, similar to those in StochGradAdam, during the pruning process. Our primary objective is to maintain high accuracy levels in pruned models, a critical challenge in resource-limited scenarios. Our extensive experiments reveal that models optimized with gradient sampling techniques are more effective at preserving accuracy during pruning compared to those using traditional optimization methods. This finding underscores the significance of gradient sampling in facilitating robust learning and enabling networks to retain crucial information even after substantial reduction in their complexity. We validate our approach across various datasets and neural architectures, demonstrating its broad applicability and effectiveness. The paper also delves into the theoretical aspects, explaining how gradient sampling techniques contribute to the robustness of m
    
[^132]: 一种用于加速RLHF训练的自适应部署和并行框架

    An Adaptive Placement and Parallelism Framework for Accelerating RLHF Training. (arXiv:2312.11819v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.11819](http://arxiv.org/abs/2312.11819)

    提出了一种自适应模型部署和并行框架，用于加速RLHF训练。该框架提供了两种灵活的模型部署策略，其中交替策略有助于减少内存冗余和通信成本。

    

    最近，像ChatGPT或InstructGPT这样的大型语言模型（LLM）在人工智能领域产生了重大影响。许多研究尝试复现复杂的InstructGPT的训练流程，即基于人类反馈的强化学习（RLHF）。然而，主流的分布式RLHF训练方法通常采用固定的模型部署策略，称为Flattening策略。该策略将RLHF中涉及的四个相互依赖的模型视为单个实体，将它们分配到所有设备上，并应用于单个模型设计的并行技术，而不考虑每个模型固有的不同工作负载。结果，该策略加剧了RLHF训练中的生成瓶颈，并降低了整体训练效率。为了解决这些问题，我们提出了一种自适应模型部署框架，提供了两种灵活的模型部署策略。交替策略有助于减少内存冗余和通信成本。

    Recently, ChatGPT or InstructGPT like large language models (LLM) has made a significant impact in the AI world. Many works have attempted to reproduce the complex InstructGPT's training pipeline, namely Reinforcement Learning with Human Feedback (RLHF). However, the mainstream distributed RLHF training methods typically adopt a fixed model placement strategy, referred to as the Flattening strategy. This strategy treats all four interdependent models involved in RLHF as a single entity, distributing them across all devices and applying parallelism techniques designed for a single model, regardless of the different workloads inherent to each model. As a result, this strategy exacerbates the generation bottlenecks in the RLHF training and degrades the overall training efficiency. To address these issues, we propose an adaptive model placement framework that offers two flexible model placement strategies. The Interleaving strategy helps reduce memory redundancy and communication costs of 
    
[^133]: 使用基础模型进行推理的调查

    A Survey of Reasoning with Foundation Models. (arXiv:2312.11562v5 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.11562](http://arxiv.org/abs/2312.11562)

    本文调查了使用基础模型进行推理的研究，介绍了最新的推理任务、方法和基准，并讨论了基础模型中推理能力的未来发展方向。

    

    推理是复杂问题解决的关键能力，在谈判、医学诊断和刑事调查等各种现实世界环境中起着重要作用。它在人工通用智能（AGI）领域中作为一种基本方法学。随着基础模型（如大型语言模型）的不断发展，越来越多的研究者对它们在推理任务中的能力产生了兴趣。在本文中，我们介绍了用于推理的开创性基础模型，并突出了各种推理任务、方法和基准的最新进展。我们还深入探讨了基础模型中推理能力的潜在未来发展方向，并讨论了多模式学习、自主代理和超级对齐在推理背景下的相关性。通过讨论这些未来研究方向，我们希望激发研究者们在这一领域的探索，推动进一步的发展。

    Reasoning, a crucial ability for complex problem-solving, plays a pivotal role in various real-world settings such as negotiation, medical diagnosis, and criminal investigation. It serves as a fundamental methodology in the field of Artificial General Intelligence (AGI). With the ongoing development of foundation models, e.g., Large Language Models (LLMs), there is a growing interest in exploring their abilities in reasoning tasks. In this paper, we introduce seminal foundation models proposed or adaptable for reasoning, highlighting the latest advancements in various reasoning tasks, methods, and benchmarks. We then delve into the potential future directions behind the emergence of reasoning abilities within foundation models. We also discuss the relevance of multimodal learning, autonomous agents, and super alignment in the context of reasoning. By discussing these future research directions, we hope to inspire researchers in their exploration of this field, stimulate further advance
    
[^134]: TrojFST: 将特洛伊木马嵌入到少样本提示调优中

    TrojFST: Embedding Trojans in Few-shot Prompt Tuning. (arXiv:2312.10467v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.10467](http://arxiv.org/abs/2312.10467)

    TrojFST是一种在少样本提示调优框架中进行后门攻击的方法，通过引入平衡的污染学习、选择性令牌污染和...等模块来解决构建基于提示的后门的困难。

    

    提示调优已经成为一种非常有效的方法，用于适应使用有限输入样本的预训练语言模型（PLM）来处理新的自然语言处理任务。然而，提示调优的成功导致对手试图针对该技术进行后门攻击。之前基于提示的后门攻击在少样本提示调优方面面临挑战，需要全模型微调或大规模训练数据集。我们观察到使用少样本提示调优构建基于提示的后门的困难，这涉及冻结PLM并在一组受限制的输入样本上调优软提示。这种方法引入了一个不平衡的污染数据集，容易过拟合并且缺乏注意力感知。为了解决这些挑战，我们在少样本提示调优框架中引入了TrojFST用于后门攻击。TrojFST包括三个模块：平衡的污染学习、选择性令牌污染和...

    Prompt-tuning has emerged as a highly effective approach for adapting a pre-trained language model (PLM) to handle new natural language processing tasks with limited input samples. However, the success of prompt-tuning has led to adversaries attempting backdoor attacks against this technique. Previous prompt-based backdoor attacks faced challenges when implemented through few-shot prompt-tuning, requiring either full-model fine-tuning or a large training dataset. We observe the difficulty in constructing a prompt-based backdoor using few-shot prompt-tuning, which involves freezing the PLM and tuning a soft prompt with a restricted set of input samples. This approach introduces an imbalanced poisoned dataset, making it susceptible to overfitting and lacking attention awareness. To address these challenges, we introduce TrojFST for backdoor attacks within the framework of few-shot prompt-tuning. TrojFST comprises three modules: balanced poison learning, selective token poisoning, and tro
    
[^135]: Fine-Tuning还是检索？比较在LLMs中的知识注入

    Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs. (arXiv:2312.05934v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.05934](http://arxiv.org/abs/2312.05934)

    该研究比较了无监督的微调和检索增强生成（RAG）这两种常见方法在LLMs中的应用。结果发现，RAG在现有知识和新知识上表现出更好的性能，而LLMs通过无监督的微调学习新的事实信息较困难。

    

    大型语言模型（LLMs）在其预训练的权重中封装了大量的事实信息，正如它们能够在不同领域回答各种问题所证明的那样。然而，这种知识本质上是有限的，很大程度上依赖于训练数据的特性。因此，使用外部数据集来整合新的信息或改进LLMs在已见信息上的能力面临着重大挑战。在这个研究中，我们比较了两种常见的方法：无监督的微调和检索增强生成（RAG）。我们在不同主题的各种知识密集型任务上评估了这两种方法。我们的发现表明，虽然无监督的微调能够提供一定的改进，但RAG在现有知识和完全新知识上始终表现出更好的性能。此外，我们发现LLMs很难通过无监督的微调来学习新的事实信息，并且暴露

    Large language models (LLMs) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of LLMs on previously seen information poses a significant challenge. In this study, we compare two common approaches: unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while unsupervised fine-tuning offers some improvement, RAG consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing
    
[^136]: 在未知环境中使用信息路径规划的半监督主动学习进行语义分割

    Semi-Supervised Active Learning for Semantic Segmentation in Unknown Environments Using Informative Path Planning. (arXiv:2312.04402v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2312.04402](http://arxiv.org/abs/2312.04402)

    提出了一种基于信息路径规划的半监督主动学习方法，用于解决在未知环境中进行语义分割的问题，通过减少人工标注量，提高了机器人的感知能力。

    

    语义分割使得机器人能够对其环境进行感知和推理，超出了几何学的范畴。大多数这类系统建立在深度学习方法的基础上。由于自主机器人通常部署在初始未知环境中，对静态数据集进行预训练不能总是捕捉到多样的领域，限制了机器人在任务中的感知性能。最近出现了一些自监督和完全监督的主动学习方法来改进机器人的视觉。这些方法依赖于大规模的领域内预训练数据集，或者需要大量人工标注的工作。我们提出了一种基于地图的自适应路径规划方法，用于半监督的语义分割主动学习，相比完全监督方法，极大地减少了人工标注的需求。我们利用自适应路径规划器来引导机器人探索未知空间的边界，并收集具有高模型不确定性的训练数据进行人工标注。我们方法的一个关键方面是结合了稀疏的

    Semantic segmentation enables robots to perceive and reason about their environments beyond geometry. Most of such systems build upon deep learning approaches. As autonomous robots are commonly deployed in initially unknown environments, pre-training on static datasets cannot always capture the variety of domains and limits the robot's perception performance during missions. Recently, self-supervised and fully supervised active learning methods emerged to improve a robot's vision. These approaches rely on large in-domain pre-training datasets or require substantial human labelling effort. We propose a planning method for semi-supervised active learning of semantic segmentation that substantially reduces human labelling requirements compared to fully supervised approaches. We leverage an adaptive map-based planner guided towards the frontiers of unexplored space with high model uncertainty collecting training data for human labelling. A key aspect of our approach is to combine the spars
    
[^137]: 对于核机器在预处理中的Nystrom逼近

    On the Nystrom Approximation for Preconditioning in Kernel Machines. (arXiv:2312.03311v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2312.03311](http://arxiv.org/abs/2312.03311)

    本文分析了核机器预处理中使用Nystrom逼近的权衡。研究表明，使用对数大小的样本能够让Nystrom逼近的预处理器几乎与梯度下降同样有效地加速。

    

    核方法是机器学习中一类流行的非线性预测模型。学习核模型的可扩展算法需要具有迭代性质，但由于糟糕的条件，收敛可能很慢。谱预处理是加快训练核模型迭代算法收敛速度的重要工具。然而，计算和存储谱预处理器可能代价高昂，会导致大量的计算和存储开销，限制了核方法在大型数据集问题上的应用。Nystrom逼近的谱预处理器通常更便宜和更容易计算和存储，并在实际应用中取得了成功。本文分析了使用这种逼近预处理器的权衡。具体来说，我们表明与数据集大小相关的对数样本数量能够让基于Nystrom逼近的预处理器几乎与梯度下降同样有效地加速。

    Kernel methods are a popular class of nonlinear predictive models in machine learning. Scalable algorithms for learning kernel models need to be iterative in nature, but convergence can be slow due to poor conditioning. Spectral preconditioning is an important tool to speed-up the convergence of such iterative algorithms for training kernel models. However computing and storing a spectral preconditioner can be expensive which can lead to large computational and storage overheads, precluding the application of kernel methods to problems with large datasets. A Nystrom approximation of the spectral preconditioner is often cheaper to compute and store, and has demonstrated success in practical applications. In this paper we analyze the trade-offs of using such an approximated preconditioner. Specifically, we show that a sample of logarithmic size (as a function of the size of the dataset) enables the Nystrom-based approximated preconditioner to accelerate gradient descent nearly as well as
    
[^138]: LLMs能够修复安全问题吗？

    Can LLMs Patch Security Issues?. (arXiv:2312.00024v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2312.00024](http://arxiv.org/abs/2312.00024)

    本文提出了一种新的方法, Feedback-Driven Solution Synthesis (FDSS), 旨在通过将LLMs与静态代码分析工具Bandit结合，解决代码中的安全漏洞问题。该方法在现有方法的基础上有显著改进，并引入了一个新的数据集PythonSecurityEval。

    

    大型语言模型(LLMs)在代码生成方面显示出了令人印象深刻的能力。然而，类似于人类开发者，这些模型可能会生成包含安全漏洞和缺陷的代码。编写安全代码仍然是一个重大挑战，因为漏洞通常在程序与外部系统或服务（如数据库和操作系统）之间的交互过程中出现。在本文中，我们提出了一种新颖的方法，即基于反馈的解决方案合成（FDSS），旨在探索使用LLMs接收来自静态代码分析工具Bandit的反馈，然后LLMs生成潜在解决方案来解决安全漏洞。每个解决方案以及易受攻击的代码随后被送回LLMs进行代码完善。我们的方法在基线上表现出显著改进，并优于现有方法。此外，我们引入了一个新的数据集PythonSecurityEval，该数据集收集了来自Stack Overflow的真实场景数据。

    Large Language Models (LLMs) have shown impressive proficiency in code generation. Nonetheless, similar to human developers, these models might generate code that contains security vulnerabilities and flaws. Writing secure code remains a substantial challenge, as vulnerabilities often arise during interactions between programs and external systems or services, such as databases and operating systems. In this paper, we propose a novel approach, Feedback-Driven Solution Synthesis (FDSS), designed to explore the use of LLMs in receiving feedback from Bandit, which is a static code analysis tool, and then the LLMs generate potential solutions to resolve security vulnerabilities. Each solution, along with the vulnerable code, is then sent back to the LLM for code refinement. Our approach shows a significant improvement over the baseline and outperforms existing approaches. Furthermore, we introduce a new dataset, PythonSecurityEval, collected from real-world scenarios on Stack Overflow to e
    
[^139]: 短期与长期无人机协调：分布式优化与深度强化学习的交汇

    Short vs. Long-term Coordination of Drones: When Distributed Optimization Meets Deep Reinforcement Learning. (arXiv:2311.09852v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2311.09852](http://arxiv.org/abs/2311.09852)

    这项研究介绍了一种将短期计划生成和选择与分布式优化以及深度强化学习相结合的渐进方法，用于无人机的协调和规划。实验结果表明，与最先进的方法相比，该方法在动态环境中具有出色的性能。

    

    在智能城市中，支持充电技术的自主交互式无人机群可以提供引人注目的感知能力，例如交通监测和灾难响应。现有方法，包括分布式优化和深度强化学习(DRL)，旨在协调无人机以实现成本效益高、高质量的导航、感知和充电。然而，它们面临着重大挑战：短期优化在动态环境中的意外变化下并不有效，而长期学习缺乏可扩展性、韧性和灵活性。为了弥合这一差距，本文提出了一种新的渐进方法，将基于分布式优化的短期计划生成和选择与基于DRL的长期飞行方向的战略调度相结合。通过对从现实城市移动中生成的数据集进行广泛实验，表明所提出的解决方案与最先进的方法相比具有卓越的性能。

    Swarms of autonomous interactive drones, with the support of recharging technology, can provide compelling sensing capabilities in Smart Cities, such as traffic monitoring and disaster response. Existing approaches, including distributed optimization and deep reinforcement learning (DRL), aim to coordinate drones to achieve cost-effective, high-quality navigation, sensing, and charging. However, they face grand challenges: short-term optimization is not effective in dynamic environments with unanticipated changes, while long-term learning lacks scalability, resilience, and flexibility. To bridge this gap, this paper introduces a new progressive approach that combines short-term plan generation and selection based on distributed optimization with a DRL-based long-term strategic scheduling of flying direction. Extensive experimentation with datasets generated from realistic urban mobility underscores an outstanding performance of the proposed solution compared to state-of-the-art. We als
    
[^140]: 2D-RC: 二维神经网络方法用于OTFS符号检测

    2D-RC: Two-Dimensional Neural Network Approach for OTFS Symbol Detection. (arXiv:2311.08543v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2311.08543](http://arxiv.org/abs/2311.08543)

    这篇论文介绍了一种新颖的二维神经网络方法，用于高运动性场景下无线通信中的OTFS符号检测。该方法将OTFS系统的领域知识与在线子帧符号检测的设计结合起来，并通过引入二维循环填充和滤波结构来实现。

    

    正交时频空间（OTFS）是高运动性场景下无线通信的一种有前途的调制方案。最近，一种基于储层计算（RC）的方法已经引入到OTFS系统中的在线子帧符号检测中，该方法只利用了有限数量的空中（OTA）导频符号用于训练。然而，这种方法没有充分利用OTFS系统特定的领域知识，无法充分发挥RC的潜力。本文介绍一种新颖的二维RC（2D-RC）方法，将OTFS系统的领域知识结合到在线子帧符号检测的设计中。具体而言，由于延迟-多普勒（DD）域中的信道相互作用是一个二维（2D）的循环操作，2D-RC被设计为具有2D循环填充过程和2D滤波结构以嵌入此知识。通过引入这种架构，2D-RC可以在DD域中仅使用一个单一的导频符号操作。

    Orthogonal time frequency space (OTFS) is a promising modulation scheme for wireless communication in high-mobility scenarios. Recently, a reservoir computing (RC) based approach has been introduced for online subframe-based symbol detection in the OTFS system, where only a limited number of over-the-air (OTA) pilot symbols are utilized for training. However, this approach does not leverage the domain knowledge specific to the OTFS system to fully unlock the potential of RC. This paper introduces a novel two-dimensional RC (2D-RC) method that incorporates the domain knowledge of the OTFS system into the design for symbol detection in an online subframe-based manner. Specifically, as the channel interaction in the delay-Doppler (DD) domain is a two-dimensional (2D) circular operation, the 2D-RC is designed to have the 2D circular padding procedure and the 2D filtering structure to embed this knowledge. With the introduced architecture, 2D-RC can operate in the DD domain with only a sing
    
[^141]: 通过元学习实现大规模语言模型的大规模编辑

    Massive Editing for Large Language Models via Meta Learning. (arXiv:2311.04661v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.04661](http://arxiv.org/abs/2311.04661)

    本论文提出了一种通过元学习实现大规模语言模型的大规模编辑的方法。该方法利用超网络来生成参数变化，通过解决最小二乘问题来更新语言模型的参数。通过将计算分离在超网络和语言模型之间，使得可以同时编辑多个事实。该方法在不同架构的语言模型上进行了评估。

    

    虽然大规模语言模型（LLM）通过对预训练语料库学习知识成为可能，但所获得的知识随着时间的推移可能是基本不正确或过时的，这需要在训练后纠正语言模型（LM）的知识。一种有前景的方法是利用超网络生成参数偏移，然而现有的超网络在同步编辑操作数量方面存在扩展性不足的问题。为解决这个问题，我们提出了大规模语言模型编辑网络（MALMEN），它将参数偏移聚合形式化为最小二乘问题，并使用正规方程更新LM参数。为适应在有限内存预算下同时编辑多个事实，我们将超网络和LM上的计算分离，使得两个神经网络都可以具有任意批量大小。我们的方法通过对具有不同架构的LM（例如BERT-base）进行高达数千个事实的编辑进行了评估。

    While large language models (LLMs) have enabled learning knowledge from the pre-training corpora, the acquired knowledge may be fundamentally incorrect or outdated over time, which necessitates rectifying the knowledge of the language model (LM) after the training. A promising approach involves employing a hyper-network to generate parameter shift, whereas existing hyper-networks suffer from inferior scalability in synchronous editing operation amount. To mitigate the problem, we propose the MAssive Language Model Editing Network (MALMEN), which formulates the parameter shift aggregation as the least square problem, subsequently updating the LM parameters using the normal equation. To accommodate editing multiple facts simultaneously with limited memory budgets, we separate the computation on the hyper-network and LM, enabling arbitrary batch size on both neural networks. Our method is evaluated by editing up to thousands of facts on LMs with different architectures, i.e., BERT-base, G
    
[^142]: 利用正弦表示网络从脑电图预测功能磁共振成像信号

    Leveraging sinusoidal representation networks to predict fMRI signals from EEG. (arXiv:2311.04234v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2311.04234](http://arxiv.org/abs/2311.04234)

    本论文提出了一种用于预测fMRI信号的新方法，通过利用正弦表示网络从EEG中获取fMRI信息，弥补了两者之间空间分辨率和成本的限制。

    

    在现代神经科学中，功能磁共振成像（fMRI）是一种至关重要且不可替代的工具，可提供对整个大脑活动动态的非侵入性窗口。然而，fMRI受到血管动力学模糊、高成本、固定不动以及与金属植入物不兼容等限制。脑电图（EEG）与fMRI互补，在高时序分辨率下可以直接记录皮层电活动，但空间分辨率较低，无法恢复有关深层皮层以下结构的信息。从EEG中获取fMRI信息将实现成本效益高，对更广泛的脑区进行成像。此外，跨模态模型还将有助于解释fMRI信号。然而，由于EEG和fMRI都是高维且容易产生伪影的，目前很难从EEG建模fMRI。为了应对这个挑战，我们提出了一种新颖的方法

    In modern neuroscience, functional magnetic resonance imaging (fMRI) has been a crucial and irreplaceable tool that provides a non-invasive window into the dynamics of whole-brain activity. Nevertheless, fMRI is limited by hemodynamic blurring as well as high cost, immobility, and incompatibility with metal implants. Electroencephalography (EEG) is complementary to fMRI and can directly record the cortical electrical activity at high temporal resolution, but has more limited spatial resolution and is unable to recover information about deep subcortical brain structures. The ability to obtain fMRI information from EEG would enable cost-effective, imaging across a wider set of brain regions. Further, beyond augmenting the capabilities of EEG, cross-modality models would facilitate the interpretation of fMRI signals. However, as both EEG and fMRI are high-dimensional and prone to artifacts, it is currently challenging to model fMRI from EEG. To address this challenge, we propose a novel a
    
[^143]: 可信任的边缘机器学习：一项调查研究

    Trustworthy Edge Machine Learning: A Survey. (arXiv:2310.17944v1 [cs.LG])

    [http://arxiv.org/abs/2310.17944](http://arxiv.org/abs/2310.17944)

    可信任的边缘机器学习是边缘计算和机器学习的融合，面临各种挑战，本调查总结了对其的定义、属性、框架、技术和解决方案，并强调了在6G网络中的重要性。

    

    边缘计算（EC）和机器学习（ML）的融合，即边缘机器学习（EML），通过利用分布式网络资源以合作方式进行联合训练和推断，已成为一个备受关注的研究领域。然而，EML面临资源限制、异构网络环境以及不同应用的多样化服务需求等各种挑战，这些因素共同影响着EML在利益相关者眼中的可信度。本调查提供了对可信任的EML定义、属性、框架、技术和解决方案的全面总结。具体而言，我们首先强调了在第六代（6G）网络中可信任的EML的重要性。然后，我们从部署和实际应用场景的挑战的角度讨论了可信任性的必要性。随后，我们提供了可信任的EML的初步定义，并探讨了其关键属性。

    The convergence of Edge Computing (EC) and Machine Learning (ML), known as Edge Machine Learning (EML), has become a highly regarded research area by utilizing distributed network resources to perform joint training and inference in a cooperative manner. However, EML faces various challenges due to resource constraints, heterogeneous network environments, and diverse service requirements of different applications, which together affect the trustworthiness of EML in the eyes of its stakeholders. This survey provides a comprehensive summary of definitions, attributes, frameworks, techniques, and solutions for trustworthy EML. Specifically, we first emphasize the importance of trustworthy EML within the context of Sixth-Generation (6G) networks. We then discuss the necessity of trustworthiness from the perspective of challenges encountered during deployment and real-world application scenarios. Subsequently, we provide a preliminary definition of trustworthy EML and explore its key attrib
    
[^144]: 在布料操控中对模拟与现实差距的基准测试

    Benchmarking the Sim-to-Real Gap in Cloth Manipulation. (arXiv:2310.09543v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2310.09543](http://arxiv.org/abs/2310.09543)

    该论文提出了一个基准数据集，用于评估布料操控中的模拟与现实差距。通过进行动态和准静态的布料操控任务以及与刚性桌子的接触来收集数据。结果评估了四个流行的可变形物体模拟器的现实差距、计算时间和模拟稳定性，并讨论了每个模拟器的优缺点。

    

    真实的物理引擎对于学习如何在模拟中操控可变形物体（如服装）起着至关重要的作用。通过这种方式，研究人员可以避免在真实世界中感知物体的变形等挑战。尽管大量使用模拟来完成这项任务，但很少有工作评估可变形物体模拟器和现实世界数据之间的真实差距。我们提供了一个基准数据集来评估布料操控中的模拟与现实差距。该数据集通过进行动态和准静态的布料操控任务以及与刚性桌子的接触来收集。我们使用该数据集评估了四个流行的可变形物体模拟器——MuJoCo、Bullet、Flex和SOFA的现实差距、计算时间和模拟稳定性。此外，我们还讨论了每个模拟器的优点和缺点。该基准数据集是开源的。附加材料、视频和代码可以在 https://sites.google.com/view/cloth-sim2r 找到。

    Realistic physics engines play a crucial role for learning to manipulate deformable objects such as garments in simulation. By doing so, researchers can circumvent challenges such as sensing the deformation of the object in the realworld. In spite of the extensive use of simulations for this task, few works have evaluated the reality gap between deformable object simulators and real-world data. We present a benchmark dataset to evaluate the sim-to-real gap in cloth manipulation. The dataset is collected by performing a dynamic as well as a quasi-static cloth manipulation task involving contact with a rigid table. We use the dataset to evaluate the reality gap, computational time, and simulation stability of four popular deformable object simulators: MuJoCo, Bullet, Flex, and SOFA. Additionally, we discuss the benefits and drawbacks of each simulator. The benchmark dataset is open-source. Supplementary material, videos, and code, can be found at https://sites.google.com/view/cloth-sim2r
    
[^145]: 基于迁移学习的EMR数据集之间数据分布变化的预后预测模型

    A Transfer-Learning-Based Prognosis Prediction Paradigm that Bridges Data Distribution Shift across EMR Datasets. (arXiv:2310.07799v1 [cs.LG])

    [http://arxiv.org/abs/2310.07799](http://arxiv.org/abs/2310.07799)

    本研究提出了一种基于迁移学习的EMR数据集之间数据分布变化的预后预测模型，通过构建过渡模型解决了不同数据集的特征不匹配问题，提高了深度学习模型的效率。

    

    由于对新兴疾病的信息有限，症状很难被察觉和认识到，因此可能忽视临床干预的窗口。期望能够建立一个有效的预后模型，辅助医生进行正确诊断和制定个性化治疗方案，从而及时预防不利结果。然而，在疾病早期阶段，由于数据收集和临床经验有限，再加上对隐私和伦理的考虑，导致可供参考的数据受限，甚至难以正确标记数据标签。此外，不同疾病或同一疾病不同来源的电子医疗记录（EMR）数据可能存在严重的跨数据集特征不匹配问题，严重影响深度学习模型的效率。本文介绍了一种迁移学习方法，建立一个从源数据集到目标数据集的过渡模型，通过对特征进行约束，来解决数据分布变化的问题。

    Due to the limited information about emerging diseases, symptoms are hard to be noticed and recognized, so that the window for clinical intervention could be ignored. An effective prognostic model is expected to assist doctors in making right diagnosis and designing personalized treatment plan, so to promptly prevent unfavorable outcomes. However, in the early stage of a disease, limited data collection and clinical experiences, plus the concern out of privacy and ethics, may result in restricted data availability for reference, to the extent that even data labels are difficult to mark correctly. In addition, Electronic Medical Record (EMR) data of different diseases or of different sources of the same disease can prove to be having serious cross-dataset feature misalignment problems, greatly mutilating the efficiency of deep learning models. This article introduces a transfer learning method to build a transition model from source dataset to target dataset. By way of constraining the 
    
[^146]: 通过同时学习面部标志检测、域分离和重建来提高面部动作单位检测的精度

    Boosting Facial Action Unit Detection Through Jointly Learning Facial Landmark Detection and Domain Separation and Reconstruction. (arXiv:2310.05207v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.05207](http://arxiv.org/abs/2310.05207)

    本文提出了一种新的面部动作单位（AU）检测框架，通过共享参数和引入多任务学习，在面部标志检测和AU域分离与重建之间实现了更好的性能。实验证明我们方法在野外AU检测方面优于现有方法。

    

    最近，如何将大量的在野非标记面部图像引入监督式面部动作单位（AU）检测框架中成为一个具有挑战性的问题。本文提出了一种新的AU检测框架，通过共享同构面部提取模块的参数，引入多任务学习，同时学习AU域分离和重建以及面部标志检测。另外，我们提出了一种基于对比学习的新特征对齐方案，通过简单的投影器和改进的对比损失添加了四个额外的中间监督器来促进特征重建的过程。在两个基准测试上的实验结果表明，我们在野外AU检测方面优于现有的方法。

    Recently how to introduce large amounts of unlabeled facial images in the wild into supervised Facial Action Unit (AU) detection frameworks has become a challenging problem. In this paper, we propose a new AU detection framework where multi-task learning is introduced to jointly learn AU domain separation and reconstruction and facial landmark detection by sharing the parameters of homostructural facial extraction modules. In addition, we propose a new feature alignment scheme based on contrastive learning by simple projectors and an improved contrastive loss, which adds four additional intermediate supervisors to promote the feature reconstruction process. Experimental results on two benchmarks demonstrate our superiority against the state-of-the-art methods for AU detection in the wild.
    
[^147]: 机器学习的安全有效数据评估

    Secure and Effective Data Appraisal for Machine Learning. (arXiv:2310.02373v1 [cs.LG])

    [http://arxiv.org/abs/2310.02373](http://arxiv.org/abs/2310.02373)

    本文介绍了一种机密的数据选择和评估方法，通过创新的流程和简化的低维度操作来实现，以保护数据和模型的隐私，并在多个Transformer模型和NLP/CV基准测试中进行了评估。

    

    一个无拘无束的数据市场需要在数据所有者和模型所有者最终交易前能够对训练数据进行私密选择和评估。为了保护数据和模型的隐私，这个过程涉及使用多方计算(MPC)来审查目标模型。尽管之前的研究认为基于MPC的Transformer模型评估过于耗费资源，本文介绍了一种创新的方法，使数据选择成为可行的。本研究的贡献包括三个关键要素：(1)使用MPC进行机密数据选择的开创性流程；(2)通过在有限的相关数据子集上训练简化的低维度MLP来复制复杂的高维度操作；(3)并发、多阶段地实现MPC。所提出的方法在一系列Transformer模型和NLP/CV基准测试中进行了评估。与直接基于MPC的评估相比

    Essential for an unfettered data market is the ability to discreetly select and evaluate training data before finalizing a transaction between the data owner and model owner. To safeguard the privacy of both data and model, this process involves scrutinizing the target model through Multi-Party Computation (MPC). While prior research has posited that the MPC-based evaluation of Transformer models is excessively resource-intensive, this paper introduces an innovative approach that renders data selection practical. The contributions of this study encompass three pivotal elements: (1) a groundbreaking pipeline for confidential data selection using MPC, (2) replicating intricate high-dimensional operations with simplified low-dimensional MLPs trained on a limited subset of pertinent data, and (3) implementing MPC in a concurrent, multi-phase manner. The proposed method is assessed across an array of Transformer models and NLP/CV benchmarks. In comparison to the direct MPC-based evaluation 
    
[^148]: MIML: 通过微流控系统内的机械特性对高精度细胞分类进行多重图像机器学习

    MIML: Multiplex Image Machine Learning for High Precision Cell Classification via Mechanical Traits within Microfluidic Systems. (arXiv:2309.08421v1 [eess.IV])

    [http://arxiv.org/abs/2309.08421](http://arxiv.org/abs/2309.08421)

    本研究开发了一种新颖的机器学习框架MIML，该框架通过将无标记细胞图像与生物力学属性数据相结合，实现了高精度细胞分类。该方法利用了形态信息，将细胞属性理解得更全面，相较于仅考虑单一数据类型的模型，实现了98.3％的分类精度。该方法已在白细胞和肿瘤细胞分类中得到证明，并具有更广泛的应用潜力。

    

    无标记细胞分类有助于为进一步使用或检查提供原始细胞，然而现有技术在特异性和速度方面往往不足。在本研究中，我们通过开发一种新颖的机器学习框架MIML来解决这些局限性。该架构将无标记细胞图像与生物力学属性数据相结合，利用每个细胞固有的广阔且常常被低估的形态信息。通过整合这两种类型的数据，我们的模型提供了对细胞属性更全面的理解，利用了传统机器学习模型中通常被丢弃的形态信息。这种方法使细胞分类精度达到了惊人的98.3％，大大优于仅考虑单一数据类型的模型。MIML已被证明在白细胞和肿瘤细胞分类中有效，并具有更广泛的应用潜力。

    Label-free cell classification is advantageous for supplying pristine cells for further use or examination, yet existing techniques frequently fall short in terms of specificity and speed. In this study, we address these limitations through the development of a novel machine learning framework, Multiplex Image Machine Learning (MIML). This architecture uniquely combines label-free cell images with biomechanical property data, harnessing the vast, often underutilized morphological information intrinsic to each cell. By integrating both types of data, our model offers a more holistic understanding of the cellular properties, utilizing morphological information typically discarded in traditional machine learning models. This approach has led to a remarkable 98.3\% accuracy in cell classification, a substantial improvement over models that only consider a single data type. MIML has been proven effective in classifying white blood cells and tumor cells, with potential for broader applicatio
    
[^149]: 一种强大且简单的BCI MI解码的深度学习基线

    A Strong and Simple Deep Learning Baseline for BCI MI Decoding. (arXiv:2309.07159v1 [eess.SP])

    [http://arxiv.org/abs/2309.07159](http://arxiv.org/abs/2309.07159)

    本论文提出了一种强大且简单的深度学习基线EEG-SimpleConv，用于BCI中的运动想象解码。与其他方法相比，EEG-SimpleConv表现至少同样好或更高效，具有强大的知识传递能力，推理时间较低。

    

    我们提出了EEG-SimpleConv，这是一个用于BCI中运动想象解码的直观的一维卷积神经网络。我们的主要动机是提出一个非常简单的基线来进行比较，仅使用文献中非常标准的元素。我们在四个EEG运动想象数据集上评估其性能，包括模拟在线设置，并将其与最近的深度学习和机器学习方法进行比较。EEG-SimpleConv至少与其他方法一样好甚至更高效，在主体间展示了强大的知识传递能力，同时具有低推理时间的成本。我们主张使用现成的元素而不是提出特定的解决方案，可以显著帮助BCI中深度学习方法的采用。我们提供了模型和实验的代码。

    We propose EEG-SimpleConv, a straightforward 1D convolutional neural network for Motor Imagery decoding in BCI. Our main motivation is to propose a very simple baseline to compare to, using only very standard ingredients from the literature. We evaluate its performance on four EEG Motor Imagery datasets, including simulated online setups, and compare it to recent Deep Learning and Machine Learning approaches. EEG-SimpleConv is at least as good or far more efficient than other approaches, showing strong knowledge-transfer capabilities across subjects, at the cost of a low inference time. We advocate that using off-the-shelf ingredients rather than coming with ad-hoc solutions can significantly help the adoption of Deep Learning approaches for BCI. We make the code of the models and the experiments accessible.
    
[^150]: 在在线设置下学习线性算子的无限维回归

    Online Infinite-Dimensional Regression: Learning Linear Operators. (arXiv:2309.06548v1 [stat.ML])

    [http://arxiv.org/abs/2309.06548](http://arxiv.org/abs/2309.06548)

    在这篇论文中，我们研究了在线设置下学习无限维线性算子的问题。我们证明了在一定的条件下，线性算子是可以在线学习的，而在另一些条件下则不可以。我们还证明了在线均一收敛和学习能力之间的分离，并在PAC设置下得到了相同的结果。

    

    我们考虑在线设置下学习两个无限维希尔伯特空间之间的线性算子问题，通过最小二乘损失函数进行学习。我们证明了在$p \in [1, \infty)$范围内，具有均匀有界$p$-Schatten范数的线性算子类是可以在线学习的。另一方面，我们证明了具有均匀有界算子范数的线性算子类\textit{不}是可以在线学习的。此外，我们通过找到一类有界线性算子，证明了在线均一收敛和学习能力之间的分离。最后，我们证明了不可能性结果和均一收敛与学习能力之间的分离在PAC设置下同样成立。

    We consider the problem of learning linear operators under squared loss between two infinite-dimensional Hilbert spaces in the online setting. We show that the class of linear operators with uniformly bounded $p$-Schatten norm is online learnable for any $p \in [1, \infty)$. On the other hand, we prove an impossibility result by showing that the class of uniformly bounded linear operators with respect to the operator norm is \textit{not} online learnable. Moreover, we show a separation between online uniform convergence and online learnability by identifying a class of bounded linear operators that is online learnable but uniform convergence does not hold. Finally, we prove that the impossibility result and the separation between uniform convergence and learnability also hold in the agnostic PAC setting.
    
[^151]: 利用特征缺失感知校准的原型患者表示来缓解电子健康记录数据稀疏性问题

    Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity. (arXiv:2309.04160v1 [cs.LG])

    [http://arxiv.org/abs/2309.04160](http://arxiv.org/abs/2309.04160)

    本研究提出了一种利用原型患者表示和特征缺失感知校准的间接插补方法，以缓解电子健康记录数据稀疏性问题，通过获取更密集的嵌入来提高预测模型的有效性。

    

    电子健康记录（EHR）数据经常呈现稀疏特征，给预测建模带来挑战。当前的直接插补方法（如矩阵插补方法）依赖于参考类似行或列来完成原始缺失数据，不区分插补和实际值。因此，模型可能会无意中将与预测目标无关的或具有欺骗性的信息纳入其中，从而损害下游性能的效果。虽然一些方法尝试在直接插补后重新校准或增强EHR嵌入，但它们经常错误地优先考虑插补特征。这种优先错误可能会给模型引入偏见或不准确性。为了解决这些问题，我们的工作采用间接插补，利用类似患者的原型表示获取更密集的嵌入。认识到在衡量时通常将缺失特征与存在特征相同的限制时

    Electronic Health Record (EHR) data frequently exhibits sparse characteristics, posing challenges for predictive modeling. Current direct imputation such as matrix imputation approaches hinge on referencing analogous rows or columns to complete raw missing data and do not differentiate between imputed and actual values. As a result, models may inadvertently incorporate irrelevant or deceptive information with respect to the prediction objective, thereby compromising the efficacy of downstream performance. While some methods strive to recalibrate or augment EHR embeddings after direct imputation, they often mistakenly prioritize imputed features. This misprioritization can introduce biases or inaccuracies into the model. To tackle these issues, our work resorts to indirect imputation, where we leverage prototype representations from similar patients to obtain a denser embedding. Recognizing the limitation that missing features are typically treated the same as present ones when measurin
    
[^152]: 临时归纳路径神经网络用于时间知识图推理

    Temporal Inductive Path Neural Network for Temporal Knowledge Graph Reasoning. (arXiv:2309.03251v1 [cs.AI])

    [http://arxiv.org/abs/2309.03251](http://arxiv.org/abs/2309.03251)

    本论文提出了一种临时归纳路径神经网络（TiPNN）用于时间知识图的推理，采用实体独立的角度建模历史信息，并通过临时归纳路径提取结构和时间信息。

    

    时间知识图（TKG）是传统知识图（KG）的扩展，融入了时间维度。在TKGs上进行推理是一个关键任务，旨在基于历史事件预测未来事实。关键挑战在于揭示历史子图和时间模式中的结构依赖关系。大多数现有方法依靠实体建模来模拟TKGs，因为图中的节点在知识表示中起着至关重要的作用。然而，现实场景通常涉及大量实体，并且随着时间的推移会出现新实体。这使得依赖于实体的方法很难应对大量实体，并且有效处理新出现的实体也成为一个重要的挑战。因此，我们提出了一种临时归纳路径神经网络（TiPNN），它以实体独立的角度对历史信息进行建模。具体而言，TiPNN采用了一个统一的图，名为历史时间图，来建模历史信息，并通过临时归纳路径提取结构和时间信息。

    Temporal Knowledge Graph (TKG) is an extension of traditional Knowledge Graph (KG) that incorporates the dimension of time. Reasoning on TKGs is a crucial task that aims to predict future facts based on historical occurrences. The key challenge lies in uncovering structural dependencies within historical subgraphs and temporal patterns. Most existing approaches model TKGs relying on entity modeling, as nodes in the graph play a crucial role in knowledge representation. However, the real-world scenario often involves an extensive number of entities, with new entities emerging over time. This makes it challenging for entity-dependent methods to cope with extensive volumes of entities, and effectively handling newly emerging entities also becomes a significant challenge. Therefore, we propose Temporal Inductive Path Neural Network (TiPNN), which models historical information in an entity-independent perspective. Specifically, TiPNN adopts a unified graph, namely history temporal graph, to
    
[^153]: 通过具有可迁移本地策略的集成，实现对车辆路径问题的通用神经求解器

    Towards Generalizable Neural Solvers for Vehicle Routing Problems via Ensemble with Transferrable Local Policy. (arXiv:2308.14104v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.14104](http://arxiv.org/abs/2308.14104)

    本文介绍了一种通过结合全局信息和本地特征，实现对车辆路径问题的通用神经求解器的方法，并将其应用于解决现实世界的复杂问题。

    

    机器学习已被应用于帮助解决NP困难的组合优化问题。目前一种常见的方式是通过深度神经网络学习构建解决方案，由于其高效性和对专业知识要求较少，因此越来越受到关注。然而，许多用于车辆路径问题（VRPs）的神经构建方法都集中在具有指定节点分布和有限规模的合成问题实例上，导致在通常涉及复杂且未知节点分布以及大规模问题的现实世界问题上性能较差。为了使神经VRP求解器更实用，我们设计了一个从本地可迁移拓扑特征中学习的辅助策略，称为本地策略，并与典型的构建策略（从VRP实例的全局信息中学习）相结合形成一个集成策略。通过联合训练，聚合的策略相互协作和互补，以提升泛化性能。

    Machine learning has been adapted to help solve NP-hard combinatorial optimization problems. One prevalent way is learning to construct solutions by deep neural networks, which has been receiving more and more attention due to the high efficiency and less requirement for expert knowledge. However, many neural construction methods for Vehicle Routing Problems (VRPs) focus on synthetic problem instances with specified node distributions and limited scales, leading to poor performance on real-world problems which usually involve complex and unknown node distributions together with large scales. To make neural VRP solvers more practical, we design an auxiliary policy that learns from the local transferable topological features, named local policy, and integrate it with a typical construction policy (which learns from the global information of VRP instances) to form an ensemble policy. With joint training, the aggregated policies perform cooperatively and complementarily to boost generaliza
    
[^154]: 个性化生成网络实现异构联邦学习

    Heterogeneous Federated Learning via Personalized Generative Networks. (arXiv:2308.13265v1 [cs.LG])

    [http://arxiv.org/abs/2308.13265](http://arxiv.org/abs/2308.13265)

    本文通过个性化生成网络实现了异构联邦学习，解决了数据统计异质性的问题，并通过对客户端之间知识传递的方法，提高了全局模型的收敛效果。

    

    联邦学习允许多个客户端构建一个共同的全局机器学习模型，而无需共享数据。然而，联邦学习面临客户端数据的统计异质性的挑战，这降低了性能并减慢了向全局模型的收敛速度。本文提供了理论证明，最小化客户端之间的异质性有助于每个单独客户端的全局模型的收敛。这在客户端之间出现经验概念转变非常重要，而不仅仅是考虑到已被研究过的不平衡类别。因此，我们提出了一种知识传递方法，其中服务器训练客户端特定的生成器。每个生成器为相应的客户端生成样本，以消除与其他客户端模型的冲突。在合成和真实数据上进行的实验证明以及理论研究支持了我们方法的有效性。

    Federated Learning (FL) allows several clients to construct a common global machine-learning model without having to share their data. FL, however, faces the challenge of statistical heterogeneity between the client's data, which degrades performance and slows down the convergence toward the global model. In this paper, we provide theoretical proof that minimizing heterogeneity between clients facilitates the convergence of a global model for every single client. This becomes particularly important under empirical concept shifts among clients, rather than merely considering imbalanced classes, which have been studied until now. Therefore, we propose a method for knowledge transfer between clients where the server trains client-specific generators. Each generator generates samples for the corresponding client to remove the conflict with other clients' models. Experiments conducted on synthetic and real data, along with a theoretical study, support the effectiveness of our method in cons
    
[^155]: 用于稀疏深度神经网络训练的多目标优化

    Multi-Objective Optimization for Sparse Deep Neural Network Training. (arXiv:2308.12243v1 [cs.LG])

    [http://arxiv.org/abs/2308.12243](http://arxiv.org/abs/2308.12243)

    这项工作提出了一种用于稀疏深度神经网络训练的多目标优化算法，通过加权Chebyshev标量化和增广Lagrangian方法，解决了同时优化多个任务时的挑战。

    

    在深度学习的各种场景中，会自然地出现不同的冲突优化准则。这些准则可以解决不同的主任务（如多任务学习设置），也可以解决主要任务和次要任务，例如损失最小化与稀疏性。通常的方法是简单地加权准则，但在凸设置中才有效。本文提出了一种多目标优化算法，对多任务深度神经网络（DNNs）进行训练，使用改进的加权Chebyshev标量化方法。通过使用这种标量化技术，算法可以识别原始问题的所有最优解，同时将其复杂性降低为一系列单目标问题。然后，使用增广Lagrangian方法来解决简化后的问题，从而可以使用常见的优化技术，如Adam和随机梯度下降，同时有效地处理约束条件。我们的工作旨在解决经济化问题。

    Different conflicting optimization criteria arise naturally in various Deep Learning scenarios. These can address different main tasks (i.e., in the setting of Multi-Task Learning), but also main and secondary tasks such as loss minimization versus sparsity. The usual approach is a simple weighting of the criteria, which formally only works in the convex setting. In this paper, we present a Multi-Objective Optimization algorithm using a modified Weighted Chebyshev scalarization for training Deep Neural Networks (DNNs) with respect to several tasks. By employing this scalarization technique, the algorithm can identify all optimal solutions of the original problem while reducing its complexity to a sequence of single-objective problems. The simplified problems are then solved using an Augmented Lagrangian method, enabling the use of popular optimization techniques such as Adam and Stochastic Gradient Descent, while efficaciously handling constraints. Our work aims to address the (economi
    
[^156]: 牙科点云的变分自编码

    Variational Autoencoding of Dental Point Clouds. (arXiv:2307.10895v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.10895](http://arxiv.org/abs/2307.10895)

    本论文介绍了一种新颖的点云变分自编码器（VF-Net）用于牙科点云数据的处理，该模型在各种任务中具有显著的性能，包括网格生成、形状完整和表示学习。

    

    数字牙科学取得了重大进展，但仍面临许多挑战。本文介绍了FDI 16数据集，这是一个包含了大量牙齿网格和点云的数据集。此外，我们还提出了一种新颖的方法：变分FoldingNet（VF-Net），这是一种专为点云设计的完全概率变分自编码器。值得注意的是，先前的点云潜变量模型缺乏输入和输出点之间的一一对应关系。相反，它们依赖于优化Chamfer距离，这是一种缺乏归一化分布对应的度量，因此不适合概率建模。我们用合适的编码器取代了明确的最小化Chamfer距离，提高了计算效率，同时简化了概率扩展。这使得在各种任务中都可以直接应用，包括网格生成、形状完整和表示学习。在实证方面，我们提供了牙齿重建中较低的重建误差的证据。

    Digital dentistry has made significant advancements, yet numerous challenges remain. This paper introduces the FDI 16 dataset, an extensive collection of tooth meshes and point clouds. Additionally, we present a novel approach: Variational FoldingNet (VF-Net), a fully probabilistic variational autoencoder designed for point clouds. Notably, prior latent variable models for point clouds lack a one-to-one correspondence between input and output points. Instead, they rely on optimizing Chamfer distances, a metric that lacks a normalized distributional counterpart, rendering it unsuitable for probabilistic modeling. We replace the explicit minimization of Chamfer distances with a suitable encoder, increasing computational efficiency while simplifying the probabilistic extension. This allows for straightforward application in various tasks, including mesh generation, shape completion, and representation learning. Empirically, we provide evidence of lower reconstruction error in dental recon
    
[^157]: 无监督条件槽注意力用于物体中心学习

    Unsupervised Conditional Slot Attention for Object Centric Learning. (arXiv:2307.09437v1 [cs.LG])

    [http://arxiv.org/abs/2307.09437](http://arxiv.org/abs/2307.09437)

    本文提出了一种无监督的条件槽注意力方法，通过使用概率性槽字典（PSD）实现了专门的槽位绑定和物体层次调节分布，在多个后续任务中展示了其优势。

    

    提取物体层次的表示以进行后续的推理任务是人工智能中涌现的一个领域。在无监督的设置中学习物体中心表示面临多个挑战，其中一个关键挑战是将任意数量的物体实例绑定到专门的物体槽位。最近的物体中心表示方法如槽位注意力利用迭代式注意力学习具有动态推理层级绑定的可组合表示，但未能达到专门的槽位绑定。为了解决这个问题，本文提出了一种使用新颖的概率性槽字典（PSD）的无监督条件槽注意力。我们将PSD定义为（i）抽象的物体层次属性向量作为键，（ii）参数化高斯分布作为相应的值。我们在多个后续任务中展示了学习到的具体物体层次调节分布的好处，包括物体发现、组合式场景生成和组合式视觉推理。

    Extracting object-level representations for downstream reasoning tasks is an emerging area in AI. Learning object-centric representations in an unsupervised setting presents multiple challenges, a key one being binding an arbitrary number of object instances to a specialized object slot. Recent object-centric representation methods like Slot Attention utilize iterative attention to learn composable representations with dynamic inference level binding but fail to achieve specialized slot level binding. To address this, in this paper we propose Unsupervised Conditional Slot Attention using a novel Probabilistic Slot Dictionary (PSD). We define PSD with (i) abstract object-level property vectors as key and (ii) parametric Gaussian distribution as its corresponding value. We demonstrate the benefits of the learnt specific object-level conditioning distributions in multiple downstream tasks, namely object discovery, compositional scene generation, and compositional visual reasoning. We show
    
[^158]: 异构传感器网络中的异常检测的动态图注意力

    Dynamic Graph Attention for Anomaly Detection in Heterogeneous Sensor Networks. (arXiv:2307.03761v1 [cs.LG])

    [http://arxiv.org/abs/2307.03761](http://arxiv.org/abs/2307.03761)

    该论文提出了一种基于图的方法，命名为DyGATAD，用于在异构传感器网络中进行异常检测。该方法利用动态图注意力机制来识别集体异常行为，其中异常行为可能由于系统内部相互关系的变化引起。这在工业物联网监控系统中具有重要的实际应用价值。

    

    在数字化转型的时代，由工业物联网 (IIoT) 监控的系统通过异构传感器网络生成大量的多元时间序列 (MTS) 数据。虽然这些数据有助于条件监控和异常检测，但是传感器网络中日益复杂和相互依赖的关系也给异常检测带来了重大挑战。尽管在这个领域取得了一些进展，但主要集中在点异常和背景异常，对集体异常的关注较少。集体异常的一种常见变种是异常集体行为由系统内部的相互关系变化引起。这可能是由于异常环境条件（如过热）、由于网络攻击造成的不正确操作设置或系统级故障引起的。为了解决这些挑战，本文提出了 DyGATAD（一种动态图注意力的异常检测方法），采用基于图的方法来识别传感器网络中的异常行为。

    In the era of digital transformation, systems monitored by the Industrial Internet of Things (IIoTs) generate large amounts of Multivariate Time Series (MTS) data through heterogeneous sensor networks. While this data facilitates condition monitoring and anomaly detection, the increasing complexity and interdependencies within the sensor network pose significant challenges for anomaly detection. Despite progress in this field, much of the focus has been on point anomalies and contextual anomalies, with lesser attention paid to collective anomalies. A less addressed but common variant of collective anomalies is when the abnormal collective behavior is caused by shifts in interrelationships within the system. This can be due to abnormal environmental conditions like overheating, improper operational settings resulting from cyber-physical attacks, or system-level faults. To address these challenges, this paper proposes DyGATAD (Dynamic Graph Attention for Anomaly Detection), a graph-based
    
[^159]: 具有编码数据结构的变分量子回归算法

    Variational quantum regression algorithm with encoded data structure. (arXiv:2307.03334v1 [quant-ph])

    [http://arxiv.org/abs/2307.03334](http://arxiv.org/abs/2307.03334)

    本文介绍了一个具有编码数据结构的变分量子回归算法，在量子机器学习中具有模型解释性，并能有效地处理互连度较高的量子比特。算法通过压缩编码和数字-模拟门操作，大大提高了在噪声中尺度量子计算机上的运行时间复杂度。

    

    变分量子算法(VQAs)被广泛应用于解决实际问题，如组合优化、量子化学模拟、量子机器学习和噪声量子计算机上的量子错误纠正。对于变分量子机器学习，尚未开发出将模型解释性内嵌到算法中的变分算法。本文构建了一个量子回归算法，并确定了变分参数与学习回归系数之间的直接关系，同时采用了将数据直接编码为反映经典数据表结构的量子幅度的电路。该算法特别适用于互连度较高的量子比特。通过压缩编码和数字-模拟门操作，运行时间复杂度在数据输入量编码的情况下对数级更有优势，显著提升了噪声中尺度量子计算机的性能。

    Variational quantum algorithms (VQAs) prevail to solve practical problems such as combinatorial optimization, quantum chemistry simulation, quantum machine learning, and quantum error correction on noisy quantum computers. For variational quantum machine learning, a variational algorithm with model interpretability built into the algorithm is yet to be exploited. In this paper, we construct a quantum regression algorithm and identify the direct relation of variational parameters to learned regression coefficients, while employing a circuit that directly encodes the data in quantum amplitudes reflecting the structure of the classical data table. The algorithm is particularly suitable for well-connected qubits. With compressed encoding and digital-analog gate operation, the run time complexity is logarithmically more advantageous than that for digital 2-local gate native hardware with the number of data entries encoded, a decent improvement in noisy intermediate-scale quantum computers a
    
[^160]: 自我监督的语音模型对单词的了解程度是什么？

    What do self-supervised speech models know about words?. (arXiv:2307.00162v1 [cs.CL])

    [http://arxiv.org/abs/2307.00162](http://arxiv.org/abs/2307.00162)

    通过对自我监督的语音模型进行分析，发现这些模型在不同层中编码了不同的语言信息，也学习了类似音素的子词单元。与单词相关的信息主要在中间的模型层中，同时一些低级信息在更高的层中也得以保留。

    

    在过去几年中，许多自我监督的语音模型（S3Ms）被引入，为各种语音任务提供了性能和数据效率的改进。有证据表明，不同的S3Ms在不同的层中编码语言信息，而且一些S3Ms似乎学习了类似于音素的子词单元。然而，这些模型捕捉更大的语言单元（如单词）的程度以及单词相关信息的编码位置仍然不清楚。在这项研究中，我们对来自三个S3Ms的不同层的单词片段表示进行了多种分析：wav2vec2、HuBERT和WavLM。我们利用规范相关分析（CCA），一种轻量级的分析工具，来衡量这些表示与单词级语言属性之间的相似性。我们发现最大的单词级语言内容往往出现在中间的模型层，而一些低级信息（如发音）也在更高的层中保留。

    Many self-supervised speech models (S3Ms) have been introduced over the last few years, producing performance and data efficiency improvements for a variety of speech tasks. Evidence is emerging that different S3Ms encode linguistic information in different layers, and also that some S3Ms appear to learn phone-like sub-word units. However, the extent to which these models capture larger linguistic units, such as words, and where word-related information is encoded, remains unclear. In this study, we conduct several analyses of word segment representations extracted from different layers of three S3Ms: wav2vec2, HuBERT, and WavLM. We employ canonical correlation analysis (CCA), a lightweight analysis tool, to measure the similarity between these representations and word-level linguistic properties. We find that the maximal word-level linguistic content tends to be found in intermediate model layers, while some lower-level information like pronunciation is also retained in higher layers 
    
[^161]: 逼真的合成金融交易用于反洗钱模型

    Realistic Synthetic Financial Transactions for Anti-Money Laundering Models. (arXiv:2306.16424v1 [cs.AI])

    [http://arxiv.org/abs/2306.16424](http://arxiv.org/abs/2306.16424)

    本文提供了一个逼真的合成金融交易数据集生成器和一组合成的反洗钱数据集，以满足训练模型和推进领域发展的需求。

    

    随着金融的广泛数字化和加密货币的日益流行，网络犯罪分子设计的欺诈方案越来越复杂。洗钱——将非法资金移动以掩盖其来源——可以跨越银行和国界，产生复杂的交易模式。联合国估计每年全球洗钱金额占全球GDP的2-5%，约为0.8-2.0万亿美元。不幸的是，通常无法获得用于训练机器学习模型来检测洗钱的真实数据，且之前的合成数据生成器存在显著缺陷。为了比较模型并推进该领域的发展，需要一个逼真、标准化、公开可用的基准数据集。为此，本文提出了一种合成金融交易数据集生成器和一组合成的反洗钱数据集。我们根据实际交易尽可能地校准了这个基于代理的生成器。

    With the widespread digitization of finance and the increasing popularity of cryptocurrencies, the sophistication of fraud schemes devised by cybercriminals is growing. Money laundering -- the movement of illicit funds to conceal their origins -- can cross bank and national boundaries, producing complex transaction patterns. The UN estimates 2-5\% of global GDP or \$0.8 - \$2.0 trillion dollars are laundered globally each year. Unfortunately, real data to train machine learning models to detect laundering is generally not available, and previous synthetic data generators have had significant shortcomings. A realistic, standardized, publicly-available benchmark is needed for comparing models and for the advancement of the area.  To this end, this paper contributes a synthetic financial transaction dataset generator and a set of synthetically generated AML (Anti-Money Laundering) datasets. We have calibrated this agent-based generator to match real transactions as closely as possible and
    
[^162]: 通过弃权实现顺序预测中的对抗鲁棒性

    Adversarial Resilience in Sequential Prediction via Abstention. (arXiv:2306.13119v1 [cs.LG])

    [http://arxiv.org/abs/2306.13119](http://arxiv.org/abs/2306.13119)

    本文提出了一种处理顺序预测的模型，允许在不对对抗性样例进行预测的情况下提高算法抗对抗攻击的能力。

    

    本文研究了在带有允许注入干净标签对抗性（或超出分布）示例的对抗者的情况下，在随机设置下的顺序预测问题。针对纯随机数据的算法在存在此类对抗性示例的情况下往往失败，从而导致错误的预测。这在许多高风险应用中是不可取的，例如医学建议，这里弃权不进行对抗性示例的预测优于误分类。另一方面，假设完全对抗性数据导致非常悲观的界限，在实践中往往是空洞的。为了实现这一目标，我们提出了一种新的顺序预测模型，它位于纯随机和完全对抗性设置之间，通过允许学习器在对抗样例上无代价地放弃进行预测来实现。假设访问非对抗样例的边际分布，我们设计了一个学习器，其误差随着VC维的变化而变化。

    We study the problem of sequential prediction in the stochastic setting with an adversary that is allowed to inject clean-label adversarial (or out-of-distribution) examples. Algorithms designed to handle purely stochastic data tend to fail in the presence of such adversarial examples, often leading to erroneous predictions. This is undesirable in many high-stakes applications such as medical recommendations, where abstaining from predictions on adversarial examples is preferable to misclassification. On the other hand, assuming fully adversarial data leads to very pessimistic bounds that are often vacuous in practice.  To capture this motivation, we propose a new model of sequential prediction that sits between the purely stochastic and fully adversarial settings by allowing the learner to abstain from making a prediction at no cost on adversarial examples. Assuming access to the marginal distribution on the non-adversarial examples, we design a learner whose error scales with the VC 
    
[^163]: 线性约束下的多臂赌博纯探索算法

    Pure Exploration in Bandits with Linear Constraints. (arXiv:2306.12774v1 [cs.LG])

    [http://arxiv.org/abs/2306.12774](http://arxiv.org/abs/2306.12774)

    本文提出了两种相对于线性约束下的多臂赌博问题都是渐进最优的算法。这两种算法都试图跟踪基于下界计算和通过对正常锥体边界进行加权投影所获得的最优分配。

    

    本文解决多臂赌博问题中存在线性约束的情况下，如何在一定置信度下确定最优策略的问题。与标准的最优臂识别问题不同，这种情况下的最优策略可能不是确定性的，而是可能在多个臂之间进行混合。这种情况改变了问题的几何形状，我们通过信息论下界进行了描述。我们提出了两种相对于此设置都是渐进最优的算法，其中一个基于“跟踪停止”方法，另一个基于博弈理论的方法。这两种算法都试图跟踪基于下界计算和通过对正常锥体边界进行加权投影所获得的最优分配。最后，我们提供了实证结果，验证了我们的界限，并展示了约束如何改变问题的难度。

    We address the problem of identifying the optimal policy with a fixed confidence level in a multi-armed bandit setup, when \emph{the arms are subject to linear constraints}. Unlike the standard best-arm identification problem which is well studied, the optimal policy in this case may not be deterministic and could mix between several arms. This changes the geometry of the problem which we characterize via an information-theoretic lower bound. We introduce two asymptotically optimal algorithms for this setting, one based on the Track-and-Stop method and the other based on a game-theoretic approach. Both these algorithms try to track an optimal allocation based on the lower bound and computed by a weighted projection onto the boundary of a normal cone. Finally, we provide empirical results that validate our bounds and visualize how constraints change the hardness of the problem.
    
[^164]: GateON: 一种用于大规模连续学习的无监督方法

    GateON: an unsupervised method for large scale continual learning. (arXiv:2306.01690v1 [cs.LG])

    [http://arxiv.org/abs/2306.01690](http://arxiv.org/abs/2306.01690)

    GateON是一种用于大规模连续学习的无监督方法，通过可学习的活动门控和参数相关性的在线估计来防止重要知识被覆盖，同时通过定点神经元的重新激活机制解决了网络饱和的问题。

    

    连续学习（CL）的目标是在不对早期任务进行重新训练的情况下按顺序学习任务。然而，传统的神经网络在经过CL训练后会出现灾难性遗忘和有限的泛化能力。为了克服这些问题，我们引入了一种新的方法，称为'Gate and Obstruct Network'（GateON）。GateON将可学习的活动门控与参数相关性的在线估计相结合，以防止重要知识被覆盖。我们的方法在任务之间生成部分重叠的路径，允许在顺序学习过程中进行正向和反向转移。GateON通过定点神经元的重新激活机制来解决参数固定后网络饱和的问题，实现了大规模连续学习。GateON适用于各种网络（全连接、CNN、Transformers），计算复杂度低，有效地学习了高达100个MNIST学习任务，并在预训练BERT中取得了顶尖结果。

    The objective of continual learning (CL) is to learn tasks sequentially without retraining on earlier tasks. However, when subjected to CL, traditional neural networks exhibit catastrophic forgetting and limited generalization. To overcome these problems, we introduce a novel method called 'Gate and Obstruct Network' (GateON). GateON combines learnable gating of activity and online estimation of parameter relevance to safeguard crucial knowledge from being overwritten. Our method generates partially overlapping pathways between tasks which permits forward and backward transfer during sequential learning. GateON addresses the issue of network saturation after parameter fixation by a re-activation mechanism of fixed neurons, enabling large-scale continual learning. GateON is implemented on a wide range of networks (fully-connected, CNN, Transformers), has low computational complexity, effectively learns up to 100 MNIST learning tasks, and achieves top-tier results for pre-trained BERT in
    
[^165]: 后续前导内在探索

    Successor-Predecessor Intrinsic Exploration. (arXiv:2305.15277v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15277](http://arxiv.org/abs/2305.15277)

    后续前导内在探索是一种基于新颖内在奖励的探索算法，它利用回顾信息并结合前瞻信息，以在强化学习中实现高效且生态学合理的探索行为。

    

    在强化学习中，探索对于那些外部奖励稀缺的环境非常重要。本文侧重于利用内在奖励进行探索，即代理器使用自我生成的内在奖励临时增加外部奖励。尽管内在奖励的研究有着悠久的历史，但现有的方法都集中于根据状态的未来前景度量来构成内在奖励，忽视了转移序列的回顾结构中所蕴含的信息。在本文中，我们认为代理器可以利用回顾信息来生成具有结构感知能力的探索行为，以基于整体而非局部信息的方式实现高效的探索。我们提出了一种基于新颖内在奖励的模型——后续前导内在探索 (SPIE) 算法。实验结果表明，SPIE 能够产生更加高效和生态学合理的探索行为。

    Exploration is essential in reinforcement learning, particularly in environments where external rewards are sparse. Here we focus on exploration with intrinsic rewards, where the agent transiently augments the external rewards with self-generated intrinsic rewards. Although the study of intrinsic rewards has a long history, existing methods focus on composing the intrinsic reward based on measures of future prospects of states, ignoring the information contained in the retrospective structure of transition sequences. Here we argue that the agent can utilise retrospective information to generate explorative behaviour with structure-awareness, facilitating efficient exploration based on global instead of local information. We propose Successor-Predecessor Intrinsic Exploration (SPIE), an exploration algorithm based on a novel intrinsic reward combining prospective and retrospective information. We show that SPIE yields more efficient and ethologically plausible exploratory behaviour in e
    
[^166]: Point2SSM：从点云学习解剖结构的形态变异

    Point2SSM: Learning Morphological Variations of Anatomies from Point Cloud. (arXiv:2305.14486v1 [cs.CV])

    [http://arxiv.org/abs/2305.14486](http://arxiv.org/abs/2305.14486)

    Point2SSM可以直接从点云中构建出解剖统计形态模型，克服了传统方法需要无噪声表面网格或二进制体积，依赖于假设或预定义模板，以及同时优化导致长时间推断新数据的局限性。

    

    我们介绍了Point2SSM，一种新的无监督学习方法，可以直接从点云精确地构建基于对应关系的解剖统计形态模型（SSM）。 SSM在临床研究中非常关键，用于分析骨骼和器官中的群体水平形态变异。然而，传统的SSM创建方法具有一定局限性，如需要无噪声表面网格或二进制体积，依赖于假设或预定义模板，以及针对整个队列的同时优化导致长时间推断新数据。Point2SSM通过提供数据驱动的解决方案来克服这些障碍，从原始点云中直接推断SSM，减少了推断负担，并提高了适用性，因为点云更容易获取。

    We introduce Point2SSM, a novel unsupervised learning approach that can accurately construct correspondence-based statistical shape models (SSMs) of anatomy directly from point clouds. SSMs are crucial in clinical research for analyzing the population-level morphological variation in bones and organs. However, traditional methods for creating SSMs have limitations that hinder their widespread adoption, such as the need for noise-free surface meshes or binary volumes, reliance on assumptions or predefined templates, and simultaneous optimization of the entire cohort leading to lengthy inference times given new data. Point2SSM overcomes these barriers by providing a data-driven solution that infers SSMs directly from raw point clouds, reducing inference burdens and increasing applicability as point clouds are more easily acquired. Deep learning on 3D point clouds has seen recent success in unsupervised representation learning, point-to-point matching, and shape correspondence; however, t
    
[^167]: 通过数据模糊化缓解标签噪声

    Mitigating Label Noise through Data Ambiguation. (arXiv:2305.13764v1 [cs.LG])

    [http://arxiv.org/abs/2305.13764](http://arxiv.org/abs/2305.13764)

    本文提出了一种通过数据模糊化来缓解标签噪声的方法，即添加额外的、互补的候选标签，利用所谓的超集学习框架构建基于置信阈值的集合值目标。

    

    标签噪声是机器学习中的一个重要挑战，特别是在深度学习中，具有高表现能力的大型模型主导了该领域。这种模型容易记忆错误的标签，从而损害泛化性能。已经提出了许多方法来解决这个问题，包括强健的损失函数和更复杂的标签校正方法。鲁棒性损失函数由于简单而具有吸引力，但通常缺乏灵活性，而标签校正通常会增加训练设置的复杂性。在本文中，我们建议通过“模糊化”目标信息来解决两种方法的缺点，在观察到的训练标签不足够可信时，添加附加的、互补的候选标签。更确切地说，我们利用所谓的超集学习框架来构建基于置信阈值的集合值目标，这些目标提供不精确但更可靠的信息。

    Label noise poses an important challenge in machine learning, especially in deep learning, in which large models with high expressive power dominate the field. Models of that kind are prone to memorizing incorrect labels, thereby harming generalization performance. Many methods have been proposed to address this problem, including robust loss functions and more complex label correction approaches. Robust loss functions are appealing due to their simplicity, but typically lack flexibility, while label correction usually adds substantial complexity to the training setup. In this paper, we suggest to address the shortcomings of both methodologies by "ambiguating" the target information, adding additional, complementary candidate labels in case the learner is not sufficiently convinced of the observed training label. More precisely, we leverage the framework of so-called superset learning to construct set-valued targets based on a confidence threshold, which deliver imprecise yet more reli
    
[^168]: 民主扩散语言模型

    Democratized Diffusion Language Model. (arXiv:2305.10818v1 [cs.LG])

    [http://arxiv.org/abs/2305.10818](http://arxiv.org/abs/2305.10818)

    本文提出了一个基于CDCD框架的民主扩散语言模型（DDLM），并通过GLUE基准测试了其知识转移能力，为研究人员提供了DDLM训练和评估流程以及已训练的DDLM模型。

    

    尽管扩散模型在自然语言处理中有潜在好处，但目前公开的实现、训练模型或可重现的训练程序并不存在。为解决这些挑战，我们提出了基于CDCD框架的民主扩散语言模型（DDLM）。我们提出了一种用C4数据集简化的DDLM训练流程，并对训练模型的行为进行了深入分析。此外，我们引入了一种用于速度更快的采样的新型早期退出策略，该策略针对使用得分插值训练的模型。由于此前没有研究旨在使用预训练扩散LM解决下游任务（例如分类任务），我们在GLUE基准上进行了实验，以研究DDLM的知识转移能力。通过本文，我们提出了可供其他研究人员使用的DDLM训练和评估流程以及预先训练的DDLM模型，这些模型可在未来的D相关的研究中使用。

    Despite the potential benefits of Diffusion Models for NLP applications, publicly available implementations, trained models, or reproducible training procedures currently need to be publicly available. We present the Democratized Diffusion Language Model (DDLM), based on the Continuous Diffusion for Categorical Data (CDCD) framework, to address these challenges. We propose a simplified training procedure for DDLM using the C4 dataset and perform an in-depth analysis of the trained model's behavior. Furthermore, we introduce a novel early-exiting strategy for faster sampling with models trained with score interpolation. Since no previous works aimed at solving downstream tasks with pre-trained Diffusion LM (e.g., classification tasks), we experimented with GLUE Benchmark to study the ability of DDLM to transfer knowledge. With this paper, we propose available training and evaluation pipelines to other researchers and pre-trained DDLM models, which could be used in future research with D
    
[^169]: 社会正义算法：社交网络中的平权行动

    Algorithms for Social Justice: Affirmative Action in Social Networks. (arXiv:2305.03223v1 [cs.SI])

    [http://arxiv.org/abs/2305.03223](http://arxiv.org/abs/2305.03223)

    本文介绍了一个新的基于谱图理论的链接推荐算法ERA-Link，旨在缓解现有推荐算法带来的信息孤岛和社会成见，实现社交网络平台的社会正义目标。

    

    链接推荐算法对于世界各地数十亿用户的人际关系产生了影响。为了最大化相关性，它们通常建议连接相互相似的用户。然而，这被发现会产生信息孤岛，加剧弱势突出群体所遭受的孤立，并延续社会成见。为了缓解这些限制，大量研究致力于实现公平的链接推荐方法。然而，大多数方法并不质疑链接推荐算法的最终目标，即数据交易的复杂商业模型中用户参与的货币化。本文主张实现社交网络平台玩家和目的的多样化，以实现社会正义。为了说明这一概念目标，我们提出了ERA-Link，这是一种基于谱图理论的新型链接推荐算法，可以抵消系统性的社会歧视。

    Link recommendation algorithms contribute to shaping human relations of billions of users worldwide in social networks. To maximize relevance, they typically propose connecting users that are similar to each other. This has been found to create information silos, exacerbating the isolation suffered by vulnerable salient groups and perpetuating societal stereotypes. To mitigate these limitations, a significant body of work has been devoted to the implementation of fair link recommendation methods. However, most approaches do not question the ultimate goal of link recommendation algorithms, namely the monetization of users' engagement in intricate business models of data trade. This paper advocates for a diversification of players and purposes of social network platforms, aligned with the pursue of social justice. To illustrate this conceptual goal, we present ERA-Link, a novel link recommendation algorithm based on spectral graph theory that counteracts the systemic societal discriminat
    
[^170]: MAE预前置训练对于亿级预训练的有效性

    The effectiveness of MAE pre-pretraining for billion-scale pretraining. (arXiv:2303.13496v1 [cs.CV])

    [http://arxiv.org/abs/2303.13496](http://arxiv.org/abs/2303.13496)

    本文在计算机视觉领域提出了一种自我监督的MAE技术预前置训练方法，该方法适用于亿级预训练规模，并可显著提高模型收敛性和下游转移性能。

    

    本文重新审视了计算机视觉中用于视觉识别任务的标准预训练-微调范式。通常情况下，最先进的基础模型使用数十亿张图像的大规模（弱）监督数据集进行预训练。我们引入了一个额外的预前置训练阶段，它使用了自我监督的MAE技术来初始化模型。虽然MAE技术仅被证明能够与模型大小相缩放，但我们发现它也可以随数据集大小缩放。因此，我们基于MAE的预前置训练可同时适用于训练基础模型的模型和数据规模。预前置训练在一系列模型规模（参数数百万到数十亿）和数据集大小（图像数百万到数十亿）上一致提高了模型收敛性和下游转移性能，且我们还测量了其在10个不同的视觉识别任务上的有效性，包括图像分类、视频识别和目标检测。

    This paper revisits the standard pretrain-then-finetune paradigm used in computer vision for visual recognition tasks. Typically, state-of-the-art foundation models are pretrained using large scale (weakly) supervised datasets with billions of images. We introduce an additional pre-pretraining stage that is simple and uses the self-supervised MAE technique to initialize the model. While MAE has only been shown to scale with the size of models, we find that it scales with the size of the training dataset as well. Thus, our MAE-based pre-pretraining scales with both model and data size making it applicable for training foundation models. Pre-pretraining consistently improves both the model convergence and the downstream transfer performance across a range of model scales (millions to billions of parameters), and dataset sizes (millions to billions of images). We measure the effectiveness of pre-pretraining on 10 different visual recognition tasks spanning image classification, video reco
    
[^171]: 利用Cayley变换和可控性Gram矩的Lipschitz-bounded 1D卷积神经网络(arXiv:2303.11835v1 [cs.LG])

    Lipschitz-bounded 1D convolutional neural networks using the Cayley transform and the controllability Gramian. (arXiv:2303.11835v1 [cs.LG])

    [http://arxiv.org/abs/2303.11835](http://arxiv.org/abs/2303.11835)

    本文提出了一个逐层参数化方法，用于实现内置鲁棒性保证的1D卷积神经网络。该方法基于CNN特征的Lipschitz常数作为鲁棒性度量，并使用Cayley变换和可控性Gram矩来实现CNN的Lipschitz连续性和无约束训练，最后在心律失常数据分类任务中取得了改进的鲁棒性。

    

    我们建立了一种用于1D卷积神经网络（CNN）的逐层参数化，具有内置的端到端鲁棒性保证。我们使用CNN特征的Lipschitz常数作为鲁棒性度量。我们基于Cayley变换对正交矩阵进行参数化以及对卷积层的状态空间表征的可控性Gram矩进行参数化。所提出的参数化设计满足线性矩阵不等式，从而实现CNN的Lipschitz连续性，进一步实现Lipschitz-bounded 1D CNNs的无约束训练。最后，我们对心律失常数据进行Lipschitz-bounded 1D CNNs的分类训练，并展示了其改进的鲁棒性。

    We establish a layer-wise parameterization for 1D convolutional neural networks (CNNs) with built-in end-to-end robustness guarantees. Herein, we use the Lipschitz constant of the input-output mapping characterized by a CNN as a robustness measure. We base our parameterization on the Cayley transform that parameterizes orthogonal matrices and the controllability Gramian for the state space representation of the convolutional layers. The proposed parameterization by design fulfills linear matrix inequalities that are sufficient for Lipschitz continuity of the CNN, which further enables unconstrained training of Lipschitz-bounded 1D CNNs. Finally, we train Lipschitz-bounded 1D CNNs for the classification of heart arrythmia data and show their improved robustness.
    
[^172]: 面向软机器人的领域随机化实现鲁棒、经济和有效的闭环控制

    Domain Randomization for Robust, Affordable and Effective Closed-loop Control of Soft Robots. (arXiv:2303.04136v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.04136](http://arxiv.org/abs/2303.04136)

    本研究首次展示了领域随机化方法如何通过增强软机器人的强化学习策略来解决模型与真实平台之间的领域差距，实现鲁棒、经济和有效的闭环控制。

    

    软机器人由于其固有的安全性和可适应性而越来越受欢迎。然而，无限的自由度使得它们的建模变得困难，而且在许多情况下只能得到一个近似的描述。这个挑战使得基于强化学习（RL）的方法在实际场景中变得低效，因为模型与真实平台之间存在巨大的领域差距。在这项工作中，我们首次展示了领域随机化（DR）如何通过增强软机器人的RL策略来解决这个问题：i）对未知动力学参数的鲁棒性；ii）通过利用更简化的动力学模型进行学习，减少训练时间；iii）更好的环境探索，可以利用环境约束以实现最佳性能。此外，我们还引入了一个新的算法扩展来自先前适应性领域随机化方法，用于自动推断动态特征。

    Soft robots are gaining popularity thanks to their intrinsic safety to contacts and adaptability. However, the potentially infinite number of Degrees of Freedom makes their modeling a daunting task, and in many cases only an approximated description is available. This challenge makes reinforcement learning (RL) based approaches inefficient when deployed on a realistic scenario, due to the large domain gap between models and the real platform. In this work, we demonstrate, for the first time, how Domain Randomization (DR) can solve this problem by enhancing RL policies for soft robots with: i) robustness w.r.t. unknown dynamics parameters; ii) reduced training times by exploiting drastically simpler dynamic models for learning; iii) better environment exploration, which can lead to exploitation of environmental constraints for optimal performance. Moreover, we introduce a novel algorithmic extension to previous adaptive domain randomization methods for the automatic inference of dynamic
    
[^173]: 旋转不变量量化用于模型压缩

    Rotation Invariant Quantization for Model Compression. (arXiv:2303.03106v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03106](http://arxiv.org/abs/2303.03106)

    本研究提出了一种旋转不变量量化（RIQ）技术，可以在不同层次上实现混合精度量化，用于后训练神经网络模型压缩，并证明了其在压缩方面的优势。在多种模型和任务上进行了严格评估，取得了令人满意的结果。

    

    后训练神经网络（NN）模型压缩是一种将大型、消耗内存的模型部署到内存资源有限设备上的吸引人的方法。本研究探讨了NN模型压缩的速率-失真权衡。首先，我们提出了一种旋转不变量量化（RIQ）技术，它利用一个单一参数量化整个NN模型，在每个层次上得到不同的速率，即混合精度量化。然后，我们证明了我们的旋转不变量方法在压缩方面的优势。我们对RIQ进行了严格评估，并展示了它在各种模型和任务上的能力。例如，RIQ在预训练的VGG稠密和修剪模型上分别实现了19.4倍和52.9倍的压缩比，精度降低小于0.4%。代码可以在\url{https://github.com/ehaleva/RIQ}上找到。

    Post-training Neural Network (NN) model compression is an attractive approach for deploying large, memory-consuming models on devices with limited memory resources. In this study, we investigate the rate-distortion tradeoff for NN model compression. First, we suggest a Rotation-Invariant Quantization (RIQ) technique that utilizes a single parameter to quantize the entire NN model, yielding a different rate at each layer, i.e., mixed-precision quantization. Then, we prove that our rotation-invariant approach is optimal in terms of compression. We rigorously evaluate RIQ and demonstrate its capabilities on various models and tasks. For example, RIQ facilitates $\times 19.4$ and $\times 52.9$ compression ratios on pre-trained VGG dense and pruned models, respectively, with $<0.4\%$ accuracy degradation. Code is available in \url{https://github.com/ehaleva/RIQ}.
    
[^174]: 使用主动学习方法的相关聚类

    Correlation Clustering with Active Learning of Pairwise Similarities. (arXiv:2302.10295v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10295](http://arxiv.org/abs/2302.10295)

    本文研究了相关聚类中成对相似性不事先给出的情况，并开发了一个通用的主动学习框架，适应各种相关聚类算法和查询策略，同时具有适应性灵活、噪声鲁棒性等优势。

    

    相关聚类是一个众所周知的无监督学习设置，处理正负相似性对。在本文中，我们研究了一种情况，即成对相似性不事先给出，必须以高效的方式查询。为此，我们开发了一个通用的主动学习框架，针对这个任务具有多种优势，例如，用户/注释者可以提供各种反馈类型、适应任何相关聚类算法和查询策略以及对噪声具有鲁棒性。此外，我们还提出和分析了一些适合这种设置的新的查询策略。通过几个实验研究，我们展示了我们框架和所提出的查询策略的有效性。

    Correlation clustering is a well-known unsupervised learning setting that deals with positive and negative pairwise similarities. In this paper, we study the case where the pairwise similarities are not given in advance and must be queried in a cost-efficient way. Thereby, we develop a generic active learning framework for this task that benefits from several advantages, e.g., flexibility in the type of feedback that a user/annotator can provide, adaptation to any correlation clustering algorithm and query strategy, and robustness to noise. In addition, we propose and analyze a number of novel query strategies suited to this setting. We demonstrate the effectiveness of our framework and the proposed query strategies via several experimental studies.
    
[^175]: 何时可以追踪到决斗对抗中的显著偏好转变？

    When Can We Track Significant Preference Shifts in Dueling Bandits?. (arXiv:2302.06595v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06595](http://arxiv.org/abs/2302.06595)

    这个论文研究了具有分布转变的决斗对抗问题，并探讨了设计自适应算法以解决动态遗憾的问题，结果发现取决于底层偏好分布的属性。达到$O(\sqrt{K\tilde{L}T})$的动态遗憾是不可能的；对于$\text{SST} \cap \text{STI}$情况，存在一种算法实现动态遗憾为$O(\sqrt{K\tilde{L}T})$。

    

    在信息检索、推荐系统等领域应用广泛的$K$臂决斗对抗问题中，反馈以有噪声的成对偏好形式给出，因此得到了广泛研究。考虑到用户的偏好/口味可能随时间演变，我们研究了具有分布转变的决斗对抗问题。具体来说，我们研究了最近提出的显著转变概念（Suk和Kpotufe，2022），并提出是否可以设计一种自适应算法来解决具有$O(\sqrt{K\tilde{L}T})$动态遗憾（regret）的决斗问题，其中$\tilde{L}$是偏好中显著转变的（未知）数量。我们表明，这个问题的答案取决于底层偏好分布的属性。首先，我们给出了一个不可能的结果，排除了在广受研究的Condorcet和SST偏好分布类下具有$O(\sqrt{K\tilde{L}T})$动态遗憾的任何算法。其次，我们表明$\text{SST} \cap \text{STI}$是大规模的情况。

    The $K$-armed dueling bandits problem, where the feedback is in the form of noisy pairwise preferences, has been widely studied due its applications in information retrieval, recommendation systems, etc. Motivated by concerns that user preferences/tastes can evolve over time, we consider the problem of dueling bandits with distribution shifts. Specifically, we study the recent notion of significant shifts (Suk and Kpotufe, 2022), and ask whether one can design an adaptive algorithm for the dueling problem with $O(\sqrt{K\tilde{L}T})$ dynamic regret, where $\tilde{L}$ is the (unknown) number of significant shifts in preferences. We show that the answer to this question depends on the properties of underlying preference distributions.  Firstly, we give an impossibility result that rules out any algorithm with $O(\sqrt{K\tilde{L}T})$ dynamic regret under the well-studied Condorcet and SST classes of preference distributions. Secondly, we show that $\text{SST} \cap \text{STI}$ is the large
    
[^176]: 降低医学成像分割中Hausdorff距离的广义表面损失函数

    A Generalized Surface Loss for Reducing the Hausdorff Distance in Medical Imaging Segmentation. (arXiv:2302.03868v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2302.03868](http://arxiv.org/abs/2302.03868)

    该论文提出了一种名为广义表面损失函数的新方法，用于在医学图像分割中降低Hausdorff距离。该方法考虑了类别不平衡，并具有比现有方法更好的数值特性。该方法对于肿瘤分割等应用非常重要。

    

    在医学成像分割中，Dice系数和基于Hausdorff的度量是评估深度学习模型成功的标准。然而，现代医学图像分割的损失函数通常只考虑Dice系数或类似的基于区域的度量。因此，使用这些损失函数训练的分割结构在Dice系数上可能具有高准确度，但在基于Hausdorff的度量上准确度低。基于Hausdorff的度量准确度低可能会对肿瘤分割等应用造成问题，因为这些度量值至关重要。例如，高Dice评分伴随着显著的Hausdorff错误可能意味着预测未能检测到小的肿瘤。我们提出了广义表面损失函数，这是一种新颖的损失函数，用于最小化基于Hausdorff的度量，并具有比当前方法更理想的数值特性和用于处理类别不平衡的加权项。

    Within medical imaging segmentation, the Dice coefficient and Hausdorff-based metrics are standard measures of success for deep learning models. However, modern loss functions for medical image segmentation often only consider the Dice coefficient or similar region-based metrics during training. As a result, segmentation architectures trained over such loss functions run the risk of achieving high accuracy for the Dice coefficient but low accuracy for Hausdorff-based metrics. Low accuracy on Hausdorff-based metrics can be problematic for applications such as tumor segmentation, where such benchmarks are crucial. For example, high Dice scores accompanied by significant Hausdorff errors could indicate that the predictions fail to detect small tumors. We propose the Generalized Surface Loss function, a novel loss function to minimize Hausdorff-based metrics with more desirable numerical properties than current methods and with weighting terms for class imbalance. Our loss function outperf
    
[^177]: RS-Del: 随机删除对序列分类器的编辑距离鲁棒性证明

    RS-Del: Edit Distance Robustness Certificates for Sequence Classifiers via Randomized Deletion. (arXiv:2302.01757v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2302.01757](http://arxiv.org/abs/2302.01757)

    本文提出了一种适应于离散序列分类器的随机删除（RS-Del）平滑机制，提供针对编辑距离受限对抗性的鲁棒性证明。

    

    随机平滑是构建具有认证鲁棒性的分类器的主要方法。现有的随机平滑方法主要针对具有连续输入（如图像）的分类器，其中常常研究$\ell_p$范数受限的对抗性示例。然而，对于具有离散或可变大小输入（例如源代码）的分类器的研究较少，这些分类器需要不同的威胁模型和平滑机制。在本研究中，我们修改了随机平滑方法，以适用于离散序列分类器，以提供针对编辑距离受限的对抗性的可证明的鲁棒性。我们提出的平滑机制随机删除（RS-Del）应用了随机删除编辑，这种方式（也许令人惊讶地）足以提供针对对抗性删除、插入和替换编辑的鲁棒性。我们的认证证明不同于传统的Neyman-Pearson方法，在我们的情况下无法计算，而是围绕着另一种方式进行组织。

    Randomized smoothing is a leading approach for constructing classifiers that are certifiably robust against adversarial examples. Existing work on randomized smoothing has focused on classifiers with continuous inputs, such as images, where $\ell_p$-norm bounded adversaries are commonly studied. However, there has been limited work for classifiers with discrete or variable-size inputs, such as for source code, which require different threat models and smoothing mechanisms. In this work, we adapt randomized smoothing for discrete sequence classifiers to provide certified robustness against edit distance-bounded adversaries. Our proposed smoothing mechanism randomized deletion (RS-Del) applies random deletion edits, which are (perhaps surprisingly) sufficient to confer robustness against adversarial deletion, insertion and substitution edits. Our proof of certification deviates from the established Neyman-Pearson approach, which is intractable in our setting, and is instead organized aro
    
[^178]: 私密、公平且精确：在医学影像中训练大规模隐私保护的人工智能模型

    Private, fair and accurate: Training large-scale, privacy-preserving AI models in medical imaging. (arXiv:2302.01622v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2302.01622](http://arxiv.org/abs/2302.01622)

    本研究评估了隐私保护训练医学影像人工智能模型的准确性和公平性，并与非隐私训练进行了比较。研究结果可为隐私保护技术的广泛应用提供重要参考。

    

    人工智能模型在医学领域的应用越来越多。然而，由于医学数据的高度敏感性，需要采取特殊措施确保其保护。保护隐私的黄金标准是引入差分隐私（DP）来进行模型训练。先前的研究表明，DP对模型的准确性和公平性有负面影响，这在医学中是不可接受的，并且是隐私保护技术广泛应用的主要障碍。在这项工作中，我们评估了隐私保护训练人工智能模型对准确性和公平性的影响，与非隐私训练进行了比较。为此，我们使用了两个数据集：（1）一个大规模数据集（N=193,311）的高质量临床胸部X射线图像，和（2）一个数据集（N=1,625）的3D腹部计算机断层扫描（CT）图像，用于分类胰腺导管腺癌（PDAC）的存在。两个数据集均为回顾性采集，并由经验丰富的医学影像专家进行手动标注。

    Artificial intelligence (AI) models are increasingly used in the medical domain. However, as medical data is highly sensitive, special precautions to ensure its protection are required. The gold standard for privacy preservation is the introduction of differential privacy (DP) to model training. Prior work indicates that DP has negative implications on model accuracy and fairness, which are unacceptable in medicine and represent a main barrier to the widespread use of privacy-preserving techniques. In this work, we evaluated the effect of privacy-preserving training of AI models regarding accuracy and fairness compared to non-private training. For this, we used two datasets: (1) A large dataset (N=193,311) of high quality clinical chest radiographs, and (2) a dataset (N=1,625) of 3D abdominal computed tomography (CT) images, with the task of classifying the presence of pancreatic ductal adenocarcinoma (PDAC). Both were retrospectively collected and manually labeled by experienced radio
    
[^179]: 基于GNN的乘客请求预测

    GNN-based Passenger Request Prediction. (arXiv:2301.02515v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.02515](http://arxiv.org/abs/2301.02515)

    本文开发了一个基于图神经网络和注意力机制的模型来预测乘客的起点-终点（OD）流。该模型利用各种线性和非线性依赖关系，并捕捉重复模式和上下文数据。通过广泛的仿真实验，结果显示我们的模型性能优于现有基线模型。

    

    乘客请求预测对于乘车共享平台的运营规划、控制和管理至关重要。虽然需求预测问题已得到广泛研究，但乘客的起点-终点（OD）流预测却在研究界中受到较少关注。本文开发了一个基于图神经网络框架和注意力机制的模型来预测乘客的OD流。该模型利用不同位置起始的请求之间产生的各种线性和非线性依赖关系，并捕捉了该地点的重复模式和上下文数据。此外，确定了覆盖道路网络并保持模型复杂性和准确性的网格单元的最佳大小。通过广泛的仿真来检查我们提出的方法及其各个组件的特征。结果显示我们提出的模型相比现有基线具有更优越的性能。

    Passenger request prediction is essential for operations planning, control, and management in ride-sharing platforms. While the demand prediction problem has been studied extensively, the Origin-Destination (OD) flow prediction of passengers has received less attention from the research community. This paper develops a Graph Neural Network framework along with the Attention Mechanism to predict the OD flow of passengers. The proposed framework exploits various linear and non-linear dependencies that arise among requests originating from different locations and captures the repetition pattern and the contextual data of that place. Moreover, the optimal size of the grid cell that covers the road network and preserves the complexity and accuracy of the model is determined. Extensive simulations are conducted to examine the characteristics of our proposed approach and its various components. The results show the superior performance of our proposed model compared to the existing baselines.
    
[^180]: 机器学习系统臃肿且存在漏洞

    Machine Learning Systems are Bloated and Vulnerable. (arXiv:2212.09437v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2212.09437](http://arxiv.org/abs/2212.09437)

    本文研究了机器学习容器中存在的臃肿问题，并开发了MMLB框架进行分析和量化，结果表明在某些情况下，臃肿会占据容器总大小的80％， 显著增加了容器的供应时间，最多增加了370％，且导致漏洞恶化最高达99％。

    

    当今的软件代码和功能冗余，并大多数用户并不使用。这种臃肿占据整个软件栈，从操作系统到后端、前端和网页都普遍存在。本文重点分析和量化机器学习容器中的臃肿问题。我们开发了MMLB框架来分析机器学习容器的臃肿，测量容器和软件包的臃肿程度。该工具量化了臃肿的来源，并与漏洞分析工具集成，以评估臃肿对容器漏洞的影响。通过对Tensorflow、Pytorch和NVIDIA的15个机器学习容器进行实验，我们发现臃肿是一个重要问题，在某些情况下占容器总大小的80%。我们的结果表明，臃肿显著增加了容器的供应时间，最多增加了370％，并导致漏洞恶化，最高达99％。

    Today's software is bloated with both code and features that are not used by most users. This bloat is prevalent across the entire software stack, from the operating system, all the way to software backends, frontends, and web-pages. In this paper, we focus on analyzing and quantifying bloat in machine learning containers. We develop MMLB, a framework to analyze bloat in machine learning containers, measuring the amount of bloat that exists on the container and package levels. Our tool quantifies the sources of bloat and integrates with vulnerability analysis tools to evaluate the impact of bloat on container vulnerabilities. Through experimentation with 15 machine learning containers from Tensorflow, Pytorch, and NVIDIA, we show that bloat is a significant issue, accounting for up to 80% of the container size in some cases. Our results demonstrate that bloat significantly increases the container provisioning time by up to 370% and exacerbates vulnerabilities by up to 99%.
    
[^181]: 转移学习用于上下文多臂赌博机问题

    Transfer Learning for Contextual Multi-armed Bandits. (arXiv:2211.12612v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.12612](http://arxiv.org/abs/2211.12612)

    本文研究了上下文多臂赌博机问题中的转移学习，提出了一种新的算法来最小化遗憾，并量化了源领域数据对目标领域学习的贡献。

    

    在面对一系列应用的驱动下，本文研究了在协变量转移模型下非参数上下文多臂赌博机的转移学习问题，其中在目标赌博机开始学习之前，我们已经收集到了源赌博机的数据。本文建立了累积遗憾的最小化速率收敛性，并提出了一种新颖的转移学习算法，它达到了最小化遗憾的极限。结果量化了源领域数据在上下文非参数多臂赌博机学习中对目标领域学习的贡献。鉴于对未知平滑性的一般不可能性，我们开发了一种数据驱动算法，它在额外的自相似性假设下在大量参数空间中自动适应未知参数，并实现了接近最优的统计保证（除以对数因子）。通过模拟实验说明了转移学习在上下文多臂赌博机问题中的好处。

    Motivated by a range of applications, we study in this paper the problem of transfer learning for nonparametric contextual multi-armed bandits under the covariate shift model, where we have data collected on source bandits before the start of the target bandit learning. The minimax rate of convergence for the cumulative regret is established and a novel transfer learning algorithm that attains the minimax regret is proposed. The results quantify the contribution of the data from the source domains for learning in the target domain in the context of nonparametric contextual multi-armed bandits.  In view of the general impossibility of adaptation to unknown smoothness, we develop a data-driven algorithm that achieves near-optimal statistical guarantees (up to a logarithmic factor) while automatically adapting to the unknown parameters over a large collection of parameter spaces under an additional self-similarity assumption. A simulation study is carried out to illustrate the benefits of
    
[^182]: HyperSound：利用超网络生成音频信号的隐式神经表示

    HyperSound: Generating Implicit Neural Representations of Audio Signals with Hypernetworks. (arXiv:2211.01839v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2211.01839](http://arxiv.org/abs/2211.01839)

    该论文介绍了一种名为HyperSound的方法，利用超网络生成音频信号的隐式神经表示。与其他模型相比，该方法在重构声波方面具有可比较的质量。

    

    隐式神经表示（INRs）是一个快速发展的研究领域，提供了多媒体信号的替代表示方式。最近，INRs的应用包括图像超分辨率、高维信号压缩或3D渲染等。然而，这些解决方案通常集中在视觉数据上，将它们适应到音频领域并不容易。此外，这需要为每个数据样本单独训练模型。为了解决这个限制，我们提出了HyperSound，一种利用超网络实现在训练时未见过的音频信号的INRs的元学习方法。我们展示了我们的方法可以重构与其他最先进模型相媲美的声波。

    Implicit neural representations (INRs) are a rapidly growing research field, which provides alternative ways to represent multimedia signals. Recent applications of INRs include image super-resolution, compression of high-dimensional signals, or 3D rendering. However, these solutions usually focus on visual data, and adapting them to the audio domain is not trivial. Moreover, it requires a separately trained model for every data sample. To address this limitation, we propose HyperSound, a meta-learning method leveraging hypernetworks to produce INRs for audio signals unseen at training time. We show that our approach can reconstruct sound waves with quality comparable to other state-of-the-art models.
    
[^183]: 在网络中学习异质干预下的个体治疗效应

    Learning Individual Treatment Effects under Heterogeneous Interference in Networks. (arXiv:2210.14080v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14080](http://arxiv.org/abs/2210.14080)

    本文针对网络观测数据中个体治疗效应的估计问题，提出了一种新颖的双加权回归（DWR）算法，通过同时学习注意权重来解决网络干扰和异质干预的挑战。

    

    近年来，从网络观测数据中估计个体治疗效应引起了人们越来越多的关注。在网络场景中，一个主要挑战是稳定单元治疗价值假设（SUTVA）的违反，该假设认为一个单位的治疗分配不会影响其他人的结果。在网络数据中，由于干扰，一个单位的结果不仅受到其治疗的影响（即直接效应），还受到其他人的治疗影响（即溢出效应）。此外，其他单位的影响始终是异质的（例如，与兴趣相似的朋友对一个人的影响与兴趣不同的朋友不同）。本文针对在异质干扰下估计个体治疗效应（包括直接效应和溢出效应）的问题，我们提出了一种新颖的双加权回归（DWR）算法，通过同时学习捕捉异质干预的注意权重来解决这个问题。

    Estimates of individual treatment effects from networked observational data are attracting increasing attention these days. One major challenge in network scenarios is the violation of the stable unit treatment value assumption (SUTVA), which assumes that the treatment assignment of a unit does not influence others' outcomes. In network data, due to interference, the outcome of a unit is influenced not only by its treatment (i.e., direct effects) but also by others' treatments (i.e., spillover effects). Furthermore, the influences from other units are always heterogeneous (e.g., friends with similar interests affect a person differently than friends with different interests). In this paper, we focus on the problem of estimating individual treatment effects (both direct and spillover effects) under heterogeneous interference. To address this issue, we propose a novel Dual Weighting Regression (DWR) algorithm by simultaneously learning attention weights that capture the heterogeneous int
    
[^184]: 证明了分布式风险敏感强化学习与风险敏感强化学习之间的可证明遗憾上界

    Bridging Distributional and Risk-sensitive Reinforcement Learning with Provable Regret Bounds. (arXiv:2210.14051v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14051](http://arxiv.org/abs/2210.14051)

    本论文证明了使用分布式强化学习方法可以实现风险敏感强化学习的遗憾保证问题，并提出了两种新颖算法，其遗憾上界与先前方法相匹配。

    

    本论文研究了使用分布式强化学习方法对风险敏感强化学习（RSRL）的遗憾保证问题。具体而言，我们考虑了目标为回报的熵风险测度（EntRM）的有限情节马尔可夫决策过程。通过利用EntRM的一个关键属性，独立性属性，我们建立了风险敏感分布式动态规划框架。然后，我们提出了两种新颖的分布式强化学习算法，通过两种不同的方案实现了乐观性，包括基于模型和无模型的方法。我们证明了这两种算法都达到了$\tilde{\mathcal{O}}(\frac{\exp(|\beta| H)-1}{|\beta|}H\sqrt{S^2AK})$的遗憾上界，其中$S$，$A$，$K$和$H$分别表示状态的数量，动作的数量，情节的数量和时间的长度。这与\cite{fei2021exponential}中提出的RSVI2相一致，并进行了新颖的分布式分析。据我们所知，这是第一个以样本复杂度方向将分布式强化学习和风险敏感强化学习联系起来的遗憾分析。

    We study the regret guarantee for risk-sensitive reinforcement learning (RSRL) via distributional reinforcement learning (DRL) methods. In particular, we consider finite episodic Markov decision processes whose objective is the entropic risk measure (EntRM) of return. By leveraging a key property of the EntRM, the independence property, we establish the risk-sensitive distributional dynamic programming framework. We then propose two novel DRL algorithms that implement optimism through two different schemes, including a model-free one and a model-based one.  We prove that they both attain $\tilde{\mathcal{O}}(\frac{\exp(|\beta| H)-1}{|\beta|}H\sqrt{S^2AK})$ regret upper bound, where $S$, $A$, $K$, and $H$ represent the number of states, actions, episodes, and the time horizon, respectively. It matches RSVI2 proposed in \cite{fei2021exponential}, with novel distributional analysis. To the best of our knowledge, this is the first regret analysis that bridges DRL and RSRL in terms of sampl
    
[^185]: 卷积坚持转换

    Convolutional Persistence Transforms. (arXiv:2208.02107v2 [math.AT] UPDATED)

    [http://arxiv.org/abs/2208.02107](http://arxiv.org/abs/2208.02107)

    本文介绍了一种称为卷积坚持转换的方法，通过将数据与滤波器进行卷积计算来获取拓扑特征化。通过证明其持久图是一个注入不变量，证明了卷积坚持转换的独特性。该方法具有改善的稳定性和更大的灵活性。

    

    本文考虑了定义在简单复合体上的数据（如图像和标记图）的拓扑特征化，通过将这些数据与不同的滤波器进行卷积计算持续性。将卷积滤波器视为局部模式，所得卷积的持久图描述了模式在简单复合体上的分布方式。这种流程，我们称之为卷积坚持转换，扩展了拓扑学在此类数据中观察模式的能力。此外，我们证明（一般情况下），对于任意两个标记复合体，都可以找到某个滤波器，使它们产生不同的持久图，使得所有可能的卷积持久图的集合是一个注入不变量。这通过显示卷积坚持转换是另一个拓扑不变量——持续同调变换的特殊情况来证明。卷积坚持转换的其他优点是改善的稳定性，更大的灵活性。

    In this paper, we consider topological featurizations of data defined over simplicial complexes, like images and labeled graphs, obtained by convolving this data with various filters before computing persistence. Viewing a convolution filter as a local motif, the persistence diagram of the resulting convolution describes the way the motif is distributed across the simplicial complex. This pipeline, which we call convolutional persistence, extends the capacity of topology to observe patterns in such data. Moreover, we prove that (generically speaking) for any two labeled complexes one can find some filter for which they produce different persistence diagrams, so that the collection of all possible convolutional persistence diagrams is an injective invariant. This is proven by showing convolutional persistence to be a special case of another topological invariant, the Persistent Homology Transform. Other advantages of convolutional persistence are improved stability, greater flexibility 
    
[^186]: 自编码器的自监督训练用于视觉异常检测

    Self-Supervised Training with Autoencoders for Visual Anomaly Detection. (arXiv:2206.11723v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.11723](http://arxiv.org/abs/2206.11723)

    本文提出了一种自监督学习方法，通过修改重构误差的方式集中在数据流形上，从而解决深度卷积自编码器在视觉异常检测中容易出现的重构异常信号而导致检测效果不佳的问题。

    

    深度卷积自编码器提供了一种有效的工具，可以以无监督的方式学习非线性降维。最近，它们已被用于视觉领域的异常检测任务。通过使用无异常的样本优化重构误差，普遍认为相应的网络应在应用阶段未能准确重构异常区域。这个目标通常通过控制网络的容量来解决，要么通过减少瓶颈层的大小，要么通过对其激活施加稀疏约束。然而，这两种技术都没有明确惩罚异常信号的重构，通常导致检测效果不佳。我们通过自适应自监督学习方法来解决这个问题，这种方法允许在训练过程中使用判别信息，通过修改的重构误差集中在数据流形上。这使得模型能够产生局部一致的重构结果。

    Deep convolutional autoencoders provide an effective tool for learning non-linear dimensionality reduction in an unsupervised way. Recently, they have been used for the task of anomaly detection in the visual domain. By optimising for the reconstruction error using anomaly-free examples, the common belief is that a corresponding network should fail to accurately reconstruct anomalous regions in the application phase. This goal is typically addressed by controlling the capacity of the network by either reducing the size of the bottleneck layer or enforcing sparsity constraints on its activations. However, neither of these techniques does explicitly penalize reconstruction of anomalous signals often resulting in poor detection. We tackle this problem by adapting a self-supervised learning regime, which allows to use discriminative information during training focusing on the data manifold by means of a modified reconstruction error. This regularizes the model to produce locally consistent
    
[^187]: 风险度量和上概率：连贯性和分层

    Risk Measures and Upper Probabilities: Coherence and Stratification. (arXiv:2206.03183v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.03183](http://arxiv.org/abs/2206.03183)

    该论文研究了富有决策实际意义的聚合函数类，以构建不确定性处理的数学基础，通过分析谱风险度量、Choquet积分和Lorentz范数的特性，提出了一种对风险度量进行分层的方法，并在机器学习实践中得到了验证。

    

    机器学习通常假设经典概率论，这意味着聚合建立在期望之上。现在有多个理由来寻找比经典概率论更丰富的替代数学基础，用于机器学习。我们系统地研究了一种强大而丰富的替代聚合函数类，被称为谱风险度量、Choquet积分或Lorentz范数。我们提出了一系列表征结果，并展示了这个谱家族的特殊之处。通过利用可重排不变Banach空间理论的结果，我们得到了一种自然的对所有连贯风险度量进行分层的方法，用于表示它们所引导的上概率。我们从实证角度演示了这种新的不确定性处理方法如何帮助解决实际的机器学习问题。

    Machine learning typically presupposes classical probability theory which implies that aggregation is built upon expectation. There are now multiple reasons to motivate looking at richer alternatives to classical probability theory as a mathematical foundation for machine learning. We systematically examine a powerful and rich class of alternative aggregation functionals, known variously as spectral risk measures, Choquet integrals or Lorentz norms. We present a range of characterization results, and demonstrate what makes this spectral family so special. In doing so we arrive at a natural stratification of all coherent risk measures in terms of the upper probabilities that they induce by exploiting results from the theory of rearrangement invariant Banach spaces. We empirically demonstrate how this new approach to uncertainty helps tackling practical machine learning problems.
    
[^188]: MCCE：蒙特卡洛采样的现实反事实解释

    MCCE: Monte Carlo sampling of realistic counterfactual explanations. (arXiv:2111.09790v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2111.09790](http://arxiv.org/abs/2111.09790)

    MCCE是一个新颖的反事实解释方法，通过模拟可变特征和决策的联合分布，生成处于流形上、可行并且有效的反事实。与其他方法相比，MCCE可以处理任何类型的预测模型和具有多个级别的分类特征。

    

    我们引入了MCCE: Monte Carlo采样的有效和现实的表格数据反事实解释，这是一种新颖的反事实解释方法，通过对给定不可变特征和决策的可变特征的联合分布进行建模，生成处于流形上、可行并且有效的反事实。与其他依赖变分自动编码器和具有严格预测模型和数据要求的流形方法不同，MCCE可以处理任何类型的预测模型和具有两个以上级别的分类特征。MCCE首先使用自回归生成模型对特征和决策的联合分布进行建模，其中条件概率使用决策树进行估计。然后，它从该模型中采样一大组观测值，最后删除不符合特定条件的样本。我们使用四个知名数据集将MCCE与一系列最先进的流形反事实方法进行比较，并展示了MCCE的优越性。

    We introduce MCCE: Monte Carlo sampling of valid and realistic Counterfactual Explanations for tabular data, a novel counterfactual explanation method that generates on-manifold, actionable and valid counterfactuals by modeling the joint distribution of the mutable features given the immutable features and the decision. Unlike other on-manifold methods that tend to rely on variational autoencoders and have strict prediction model and data requirements, MCCE handles any type of prediction model and categorical features with more than two levels. MCCE first models the joint distribution of the features and the decision with an autoregressive generative model where the conditionals are estimated using decision trees. Then, it samples a large set of observations from this model, and finally, it removes the samples that do not obey certain criteria. We compare MCCE with a range of state-of-the-art on-manifold counterfactual methods using four well-known data sets and show that MCCE outperfo
    
[^189]: EvadeDroid：一种对黑盒Android恶意软件检测的实用逃逸攻击

    EvadeDroid: A Practical Evasion Attack on Machine Learning for Black-box Android Malware Detection. (arXiv:2110.03301v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.03301](http://arxiv.org/abs/2110.03301)

    本论文提出了一种可实际使用并可以有效逃避黑盒Android恶意软件检测器的对抗攻击——EvadeDroid，并且可以保留原始恶意软件应用程序的功能。

    

    在过去的十年里，研究人员通过开发逃避攻击来广泛探索Android恶意软件检测器对对抗性样本的漏洞；然而，在现实世界中，这些攻击的实用性仍然有争议。大多数研究假定攻击者知道用于恶意软件检测的目标分类器的详细信息，而在现实中，恶意行为者只能获得有限的目标分类器访问权限。本文介绍了EvadeDroid，一种实用的基于决策的对抗攻击，旨在有效逃避黑盒Android恶意软件检测器的检测。除了生成实际的对抗性恶意软件外，所提出的逃避攻击还可以保留原始恶意软件应用程序的功能。通过利用基于n-gram的方法构建来自良性样本的功能保持转换，这些转换与恶意软件应用程序具有操作码级别的相似性。

    Over the last decade, researchers have extensively explored the vulnerabilities of Android malware detectors to adversarial examples through the development of evasion attacks; however, the practicality of these attacks in real-world scenarios remains arguable. The majority of studies have assumed attackers know the details of the target classifiers used for malware detection, while in reality, malicious actors have limited access to the target classifiers. This paper introduces EvadeDroid, a practical decision-based adversarial attack designed to effectively evade black-box Android malware detectors in real-world scenarios. In addition to generating real-world adversarial malware, the proposed evasion attack can also preserve the functionality of the original malware applications (apps). EvadeDroid constructs a collection of functionality-preserving transformations derived from benign donors that share opcode-level similarity with malware apps by leveraging an n-gram-based approach. T
    
[^190]: 针对非凸-凹极小极大问题的无导数交替投影算法

    Derivative-free Alternating Projection Algorithms for General Nonconvex-Concave Minimax Problems. (arXiv:2108.00473v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2108.00473](http://arxiv.org/abs/2108.00473)

    本文提出针对非凸-凹极小极大问题的无导数交替投影算法，包括光滑问题的交替随机梯度投影算法（ZO-AGP），以及块状非光滑问题的分块交替随机近端梯度算法（ZO-BAPG）。这些算法具有较少的函数值估计和较高的迭代复杂度。

    

    本文研究了非凸-凹极小极大问题的零阶算法，这类问题近年在机器学习、信号处理等领域引起了广泛关注。我们提出了一种零阶交替随机梯度投影（ZO-AGP）算法来解决光滑的非凸-凹极小极大问题，其迭代复杂度为 $\mathcal{O}(\varepsilon^{-4})$，每次迭代的函数值估计次数为 $\mathcal{O}(d_{x}+d_{y})$。此外，我们还提出了一种零阶分块交替随机近端梯度算法（ZO-BAPG）来解决块状非光滑的非凸-凹极小极大优化问题，其迭代复杂度为 $\mathcal{O}(\varepsilon^{-4})$，每次迭代的函数值估计次数为 $\mathcal{O}(K d_{x}+d_{y})$。据我们所知，这是首次提出这些算法。

    In this paper, we study zeroth-order algorithms for nonconvex-concave minimax problems, which have attracted widely attention in machine learning, signal processing and many other fields in recent years. We propose a zeroth-order alternating randomized gradient projection (ZO-AGP) algorithm for smooth nonconvex-concave minimax problems, and its iteration complexity to obtain an $\varepsilon$-stationary point is bounded by $\mathcal{O}(\varepsilon^{-4})$, and the number of function value estimation is bounded by $\mathcal{O}(d_{x}+d_{y})$ per iteration. Moreover, we propose a zeroth-order block alternating randomized proximal gradient algorithm (ZO-BAPG) for solving block-wise nonsmooth nonconvex-concave minimax optimization problems, and the iteration complexity to obtain an $\varepsilon$-stationary point is bounded by $\mathcal{O}(\varepsilon^{-4})$ and the number of function value estimation per iteration is bounded by $\mathcal{O}(K d_{x}+d_{y})$. To the best of our knowledge, this 
    
[^191]: 编码理论与交叉验证的联系及其应用

    A Link between Coding Theory and Cross-Validation with Applications. (arXiv:2103.11856v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.11856](http://arxiv.org/abs/2103.11856)

    本研究研究了编码理论与交叉验证之间的联系，并发现了学习算法在固定数据上能解决不同二进制分类问题的数量与误差检测码理论密切相关。我们还对一种特定的交叉验证方法下的最大分类问题数量进行了研究，这取决于常权码的码字数量。同时，我们推广了常权码的概念，并证明了类似的结果适用于其他交叉验证错误和轻量级常权码。

    

    单个学习算法在给定数据上能解决多少个不同的二进制分类问题，其中要求交叉验证错误为零或最多给定数量？在前一种情况下，根据无免费午餐定理，这个数量是有限的，我们表明精确答案由误差检测码理论给出。作为案例研究，我们关注AUC性能度量和留一对交叉验证(LPOCV)，其中每个可能具有不同类标签的数据对都会被暂时保留。我们发现，对于固定类比例的分类问题，学习算法能够实现零LPOCV错误的最大数量等于常权码(CWC)中的最大码字数量，具有一定的技术性质。然后，我们通过引入轻量级常权码(light CWC)来推广CWC，并证明了对于非零LPOCV错误和轻量级常权码的类似结果。此外，我们对码字数量的最大上界和最大下界也进行了证明。

    How many different binary classification problems a single learning algorithm can solve on a fixed data with exactly zero or at most a given number of cross-validation errors? While the number in the former case is known to be limited by the no-free-lunch theorem, we show that the exact answers are given by the theory of error detecting codes. As a case study, we focus on the AUC performance measure and leave-pair-out cross-validation (LPOCV), in which every possible pair of data with different class labels is held out at a time. We shown that the maximal number of classification problems with fixed class proportion, for which a learning algorithm can achieve zero LPOCV error, equals the maximal number of code words in a constant weight code (CWC), with certain technical properties. We then generalize CWCs by introducing light CWCs and prove an analogous result for nonzero LPOCV errors and light CWCs. Moreover, we prove both upper and lower bounds on the maximal numbers of code words i
    
[^192]: 基于多样性保持的图结构细化的图表示学习

    Graph Representation Learning via Diversity-preserving Graph Refinement. (arXiv:2103.07295v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.07295](http://arxiv.org/abs/2103.07295)

    该论文提出了一种基于多样性保持的图结构细化的图表示学习方法，它利用已学习的节点表示来逐步改善图形结构质量。在多个下游任务上，包括节点分类、链接预测和图聚类，实验表明该方法优于现有的最先进方法。

    

    对于真实的图数据，节点之间的复杂关系通常被表示为硬性二进制链接。显然，这是一种离散和简化的连续关系形式，严重限制了学习到的节点表示的可表达性。另一方面，嵌入空间中获得的节点表示可以反过来揭示节点之间的内在关系。为了更好地特征化节点关系并进一步促进节点表示的学习，一种直观的方法是使用嵌入的节点表示来细化最初给定的图结构。但是，全局细化所有节点之间的关系无法区分将不可避免地导致一些噪声边缘，这可能进一步混淆节点表示学习模型的训练。此外，大型图形上也存在可扩展性问题。为了解决这些问题，我们提出了一种局部结构感知的图形细化方法，利用已经学到的节点表示逐步改善图形结构质量。具体而言，我们首先通过对图中的随机游走模拟生成一个多样化的邻域结构集。然后，对于每个模拟邻域，我们在整个细化过程中保持邻域结构的多样性，同时细化邻域内的节点关系。在各种基准数据集上进行的评估实验结果表明，所提出的方法在多个下游任务上，包括节点分类、链接预测和图聚类，均优于现有最先进的方法。

    For real-world graph data, the complex relationship between nodes is often represented as a hard binary link. Obviously, it is a discrete and simplified form of continuous relationship between nodes, which seriously limits the expressibility of the learned node representation. On the other hand, the node representation obtained in the embedding space can in turn be used to reveal the intrinsic relationship between nodes. To better characterize the node relationships and further facilitate the learning of node representation, an intuitive way is to refine the originally given graph structure with the embedded node representations. However, such global refinement of the relationships among all nodes without distinction will inevitably lead to some noisy edges, which may further confuse the training of the node representation learning model. In addition, it also has scalability problems on large graphs. To address these issues, we propose a local structure aware graph refinement to progre
    
[^193]: 渐变流用于正则化随机控制问题

    Gradient Flows for Regularized Stochastic Control Problems. (arXiv:2006.05956v5 [math.OC] UPDATED)

    [http://arxiv.org/abs/2006.05956](http://arxiv.org/abs/2006.05956)

    本文研究了正则化随机控制问题中渐变流的应用，找到了适合的度量空间，在可行控制集中构建了渐变流，使得成本函数递减，证明了渐变流的不变测度满足最优性原则，且指数级收敛。此外，最优测度值控制过程具有贝叶斯解释，可以融入先验知识。本研究旨在扩展增强学习中随机梯度算法用于解决控制问题的理论基础。

    

    本文研究的是将行动空间设定为概率测度、目标函数通过相对熵惩罚的随机控制问题。我们确定了适合的度量空间，构建了对测度值控制过程的渐变流，它在可行控制集中保证了成本函数的递减。我们证明了该渐变流的任何不变测度都满足庞特里亚金最优性原理。如果所处理的问题是足够凸的，渐变流将指数级收敛。此外，最优测度值控制过程具有贝叶斯解释，这意味着在解决此类随机控制问题时可以融入先验知识。本工作的动机是为了扩展用于解决控制问题的增强学习社区广泛采用的随机梯度类型算法的收敛的理论基础。

    This paper studies stochastic control problems with the action space taken to be probability measures, with the objective penalised by the relative entropy. We identify suitable metric space on which we construct a gradient flow for the measure-valued control process, in the set of admissible controls, along which the cost functional is guaranteed to decrease. It is shown that any invariant measure of this gradient flow satisfies the Pontryagin optimality principle. If the problem we work with is sufficiently convex, the gradient flow converges exponentially fast. Furthermore, the optimal measure-valued control process admits a Bayesian interpretation which means that one can incorporate prior knowledge when solving such stochastic control problems. This work is motivated by a desire to extend the theoretical underpinning for the convergence of stochastic gradient type algorithms widely employed in the reinforcement learning community to solve control problems.
    

