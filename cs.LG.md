# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [A Survey on Automated Design of Metaheuristic Algorithms.](http://arxiv.org/abs/2303.06532) | 本文综述了自动设计元启发式算法的形式化、方法论、挑战和研究趋势，讨论了自动设计的潜在未来方向和开放问题。 |
| [^2] | [Making Batch Normalization Great in Federated Deep Learning.](http://arxiv.org/abs/2303.06530) | 本文研究了在联邦学习中使用批标准化和群组归一化的效果，发现在适当的处理下，批标准化可以在广泛的联邦学习设置中具有很高的竞争力，而且这不需要额外的训练或通信成本。 |
| [^3] | [Data Dependent Regret Guarantees Against General Comparators for Full or Bandit Feedback.](http://arxiv.org/abs/2303.06526) | 该论文提出了一个数据相关的在线学习算法框架，可以在全专家反馈和Bandit反馈设置中具有数据相关的遗憾保证，适用于各种问题场景。 |
| [^4] | [Lossless Point Cloud Geometry and Attribute Compression Using a Learned Conditional Probability Model.](http://arxiv.org/abs/2303.06519) | 本文提出了一种使用稀疏张量深度神经网络学习点云几何和颜色概率分布的高效无损点云压缩方法，具有更高的压缩比和更快的压缩速度。 |
| [^5] | [Deep probabilistic model for lossless scalable point cloud attribute compression.](http://arxiv.org/abs/2303.06517) | 本文提出了一种利用深度概率模型进行无损可扩展点云属性压缩的方法，通过多尺度架构提供准确的上下文，从而最小化编码比特率，同时允许从无损压缩的比特流中轻松提取较低质量的版本。该方法在实验中表现优于最近提出的方法，并与最新的G-PCC版本14相当，且编码时间更快。 |
| [^6] | [Opening Up the Neural Network Classifier for Shap Score Computation.](http://arxiv.org/abs/2303.06516) | 本文提出了一种高效计算机器学习模型分类中Shap解释分数的方法，通过将二进制神经网络转换为布尔电路，并使用知识编译技术，将电路视为开放式模型，通过最近的高效算法计算Shap分数，相比于将BNN视为黑盒模型直接计算Shap，性能有了显著的提高。 |
| [^7] | [Multistage Stochastic Optimization via Kernels.](http://arxiv.org/abs/2303.06515) | 我们提出了一种基于核的多阶段随机优化方法，能够在多维设置中表现良好，并且在数据规模较大时仍然可行。 |
| [^8] | [Detection of DDoS Attacks in Software Defined Networking Using Machine Learning Models.](http://arxiv.org/abs/2303.06513) | 本文研究了使用机器学习算法在软件定义网络（SDN）环境中检测分布式拒绝服务（DDoS）攻击的有效性，通过测试四种算法，其中随机森林算法表现最佳。 |
| [^9] | [Generalizing and Decoupling Neural Collapse via Hyperspherical Uniformity Gap.](http://arxiv.org/abs/2303.06484) | 本文提出了一个广义神经坍塌假设，有效地包含了原始神经坍塌，并将其分解为两个目标：最小化类内变异性和最大化类间可分性。使用超球统一性作为量化这两个目标的统一框架，并提出了一个通用目标——超球统一性差（HUG），它由类间和类内超球统一性之间的差异定义。 |
| [^10] | [Knowledge Distillation for Efficient Sequences of Training Runs.](http://arxiv.org/abs/2303.06480) | 本文研究了如何利用先前运行中的计算来减少未来运行成本的问题，使用知识蒸馏（KD），通过将未来运行与来自先前运行的KD相结合，可以显著减少训练这些模型所需的时间，KD的开销降低了80-90％，对准确性影响很小，并在整体成本方面实现了巨大的帕累托改进。 |
| [^11] | [Multimodal Data Integration for Oncology in the Era of Deep Neural Networks: A Review.](http://arxiv.org/abs/2303.06471) | 本文综述了深度神经网络在多模态数据整合方面的应用，以提高癌症诊断和治疗的准确性和可靠性。 |
| [^12] | [Prefix-tree Decoding for Predicting Mass Spectra from Molecules.](http://arxiv.org/abs/2303.06470) | 本文提出了一种基于前缀树的中间策略，通过将质谱视为化学公式的集合来预测分子的质谱，克服了化学子公式的组合可能性。 |
| [^13] | [Graph Neural Network contextual embedding for Deep Learning on Tabular Data.](http://arxiv.org/abs/2303.06455) | 本文介绍了一种基于图神经网络的深度学习模型，使用交互网络进行上下文嵌入，用于表格数据的处理。该模型在公共数据集上的表现优于最近的深度学习基准调查，并且与提升树解决方案相比也取得了竞争性的结果。 |
| [^14] | [DECOMPL: Decompositional Learning with Attention Pooling for Group Activity Recognition from a Single Volleyball Image.](http://arxiv.org/abs/2303.06439) | 本文提出了一种新的排球视频团体活动识别技术DECOMPL，它由两个互补的分支组成，使用选择性的注意力池化提取特征，考虑参与者的当前配置，并从框坐标中提取空间信息。同时，本文发现排球数据集的标签方案降低了活动中的团体概念。 |
| [^15] | [On Neural Architectures for Deep Learning-based Source Separation of Co-Channel OFDM Signals.](http://arxiv.org/abs/2303.06438) | 本文研究了涉及OFDM信号的单通道源分离问题，通过原型问题评估了使用面向音频的神经网络架构在分离共信道OFDM波形方面的有效性，并提出了关键的领域知识修改网络参数化的解决方案。 |
| [^16] | [Reinforcement Learning-based Counter-Misinformation Response Generation: A Case Study of COVID-19 Vaccine Misinformation.](http://arxiv.org/abs/2303.06433) | 本研究使用强化学习算法创建了反虚假信息响应生成模型，以帮助普通用户有效纠正虚假信息。 |
| [^17] | [Anomaly Detection with Ensemble of Encoder and Decoder.](http://arxiv.org/abs/2303.06431) | 本文提出了一种新颖的异常检测方法，通过多个编码器和解码器对正常样本的数据分布进行建模，将输入样本映射到潜在空间，然后从潜在向量中重构输出样本，最终将重构的样本映射到潜在表示。在训练阶段，通过最小化重构损失和编码来优化参数。 |
| [^18] | [Learning interpretable causal networks from very large datasets, application to 400,000 medical records of breast cancer patients.](http://arxiv.org/abs/2303.06423) | 本文提出了一种更可靠和可扩展的因果发现方法（iMIIC），并在来自美国监测、流行病学和终末结果计划的396,179名乳腺癌患者的医疗保健数据上展示了其独特能力。超过90％的预测因果效应是正确的，而其余的意外直接和间接因果效应可以解释为诊断程序、治疗时间、患者偏好或社会经济差距。 |
| [^19] | [Robust Learning from Explanations.](http://arxiv.org/abs/2303.06419) | 本文提出了一种新的机器学习方法，将机器学习从解释（MLX）重新构建为对抗鲁棒性问题，通过人类提供的解释来指定一个低维流形，从而减轻了对强参数正则化的需求，并在合成和真实世界基准测试中取得了最新结果。 |
| [^20] | [Brain Diffuser: An End-to-End Brain Image to Brain Network Pipeline.](http://arxiv.org/abs/2303.06410) | 本文提出了一种基于扩散的端到端脑网络生成模型Brain Diffuser，直接从DTI中形成结构性脑网络。对于阿尔茨海默病的情况，所提出的模型在ADNI数据库上的表现优于现有工具包的结果。 |
| [^21] | [Automatic Detection of Signalling Behaviour from Assistance Dogs as they Forecast the Onset of Epileptic Seizures in Humans.](http://arxiv.org/abs/2303.06407) | 本研究探讨了如何自动检测辅助犬预测人类癫痫发作时的信号行为，以提高癫痫患者的生活质量。 |
| [^22] | [No-regret Algorithms for Fair Resource Allocation.](http://arxiv.org/abs/2303.06396) | 本文提出了一种无遗憾算法，用于公平资源分配问题，该算法可以实现$c_\alpha$-近似次线性遗憾，其中近似因子$c_\alpha=(1-\alpha)^{-(1-\alpha)}\leq 1.445$，对于$0\leq \alpha < 1$。 |
| [^23] | [A Novel Method Combines Moving Fronts, Data Decomposition and Deep Learning to Forecast Intricate Time Series.](http://arxiv.org/abs/2303.06394) | 本文提出了一种新的方法，结合移动前沿、数据分解和深度学习，用于预测复杂时间序列。该方法通过经验小波变换将时间序列分解成更简单的组成序列，并使用移动前沿方法防止数据泄漏。 |
| [^24] | [Uncertainty-Aware Off-Policy Learning.](http://arxiv.org/abs/2303.06389) | 本文提出了一种不确定性感知的倒数概率分数估计器（UIPS），用于改进离线学习，通过明确模拟估计的记录策略中的不确定性，相对于广泛的最先进基线具有优越的样本效率。 |
| [^25] | [Scope and Arbitration in Machine Learning Clinical EEG Classification.](http://arxiv.org/abs/2303.06386) | 本文提出了两种方法来解决机器学习临床脑电图分类中窗口标签可能误导的问题：增加窗口长度和引入第二阶段模型来仲裁记录内的窗口特定预测。在Temple大学医院异常脑电图语料库上评估这些方法，最先进的平均准确度从89.8％显着提高到93.3％。 |
| [^26] | [Learning to Precode for Integrated Sensing and Communications Systems.](http://arxiv.org/abs/2303.06381) | 本文提出了一种无监督学习神经模型，用于设计集成感知和通信（ISAC）系统的传输预编码器，以最大化最坏情况下的目标照明功率，同时确保所有用户的最小信干噪比（SINR）。通过数值模拟，证明了该方法在存在信道估计误差的情况下优于传统的基于优化的方法，同时产生较小的计算复杂度，并且在不同的信道条件下具有良好的泛化能力。 |
| [^27] | [Assessing gender fairness in EEG-based machine learning detection of Parkinson's disease: A multi-center study.](http://arxiv.org/abs/2303.06376) | 本研究在多中心环境中对基于EEG的机器学习算法进行了性别子组群的检测能力分析，发现男性和女性的PD检测能力存在显着差异。 |
| [^28] | [Explainable AI for Time Series via Virtual Inspection Layers.](http://arxiv.org/abs/2303.06365) | 本文提出了一种虚拟检查层，将时间序列转换为可解释的表示，并允许通过本地XAI方法将相关性归因传播到该表示。我们将一系列XAI方法的适用性扩展到需要转换后才能解释输入的领域。我们展示了DFT-LRP在各种时间序列分类设置中的有用性，如音频和电子健康记录。 |
| [^29] | [Privacy-Preserving Cooperative Visible Light Positioning for Nonstationary Environment: A Federated Learning Perspective.](http://arxiv.org/abs/2303.06361) | 本文提出了一种基于联邦学习的合作可见光定位方案，通过共同训练适应环境变化的全局模型，提高了在非静态环境下的定位精度和泛化能力。 |
| [^30] | [FedLP: Layer-wise Pruning Mechanism for Communication-Computation Efficient Federated Learning.](http://arxiv.org/abs/2303.06360) | 本文提出了一种显式的FL剪枝框架FedLP，采用局部训练和联邦更新中的层次剪枝，对不同类型的深度学习模型具有普适性，可以缓解通信和计算的系统瓶颈，并且性能下降较小。 |
| [^31] | [Resurrecting Recurrent Neural Networks for Long Sequences.](http://arxiv.org/abs/2303.06349) | 本文研究了如何通过对标准RNN进行精心设计，包括线性化和对角化循环、使用更好的参数化和初始化以及确保正常化前向传递等一系列改变，来恢复深度SSM在长距离推理任务上的卓越性能，同时匹配它们的训练速度。 |
| [^32] | [Contrastive Learning under Heterophily.](http://arxiv.org/abs/2303.06344) | 本文提出了第一个图形对比学习方法，以解决现有图形对比学习方法在异质性下无法学习高质量表示的问题。 |
| [^33] | [Intelligent diagnostic scheme for lung cancer screening with Raman spectra data by tensor network machine learning.](http://arxiv.org/abs/2303.06340) | 本文提出了一种基于张量网络机器学习的方案，通过筛查呼出气中挥发性有机化合物（VOC）的Raman光谱数据，可可靠地预测肺癌患者及其阶段。 |
| [^34] | [AutoMLP: Automated MLP for Sequential Recommendations.](http://arxiv.org/abs/2303.06337) | AutoMLP是一种新颖的序列推荐系统，通过自动化和自适应搜索算法，更好地模拟用户的长期/短期兴趣，实现更好的推荐效果。 |
| [^35] | [MetaViewer: Towards A Unified Multi-View Representation.](http://arxiv.org/abs/2303.06329) | 该论文提出了一种新颖的基于双层优化的多视图学习框架MetaViewer，通过统一到特定的方式学习表示，避免了手动预先指定的融合函数和混合在特征中的视图专用冗余信息可能会降低所得表示的质量的问题。 |
| [^36] | [A Novel Tensor-Expert Hybrid Parallelism Approach to Scale Mixture-of-Experts Training.](http://arxiv.org/abs/2303.06318) | 本文提出了一种新的混合并行算法，结合了张量、专家和数据并行，以实现MoE模型的训练，其基本模型比当前最先进的DeepSpeed-MoE大4-8倍。 |
| [^37] | [One Neuron Saved Is One Neuron Earned: On Parametric Efficiency of Quadratic Networks.](http://arxiv.org/abs/2303.06316) | 本文研究了二次神经元的参数效率，证明了其卓越性能是由于内在表达能力而非参数增加。 |
| [^38] | [Stabilizing and Improving Federated Learning with Non-IID Data and Client Dropout in IoT Systems.](http://arxiv.org/abs/2303.06314) | 本文提出了一个简单而有效的框架，通过引入一个先验校准的softmax函数来计算交叉熵损失和基于原型的特征提取来维护一个平衡的分类器头，以稳定和改进联邦学习。 |
| [^39] | [Generative Adversarial Networks for Scintillation Signal Simulation in EXO-200.](http://arxiv.org/abs/2303.06311) | 本文介绍了一种基于生成对抗网络的新方法，用于从EXO-200实验的时间投影室中模拟光电探测器信号。该方法能够比传统的模拟方法快一个数量级地产生高质量的模拟波形，并且能够从训练样本中推广并识别数据的显著高级特征。 |
| [^40] | [Driver Drowsiness Detection System: An Approach By Machine Learning Application.](http://arxiv.org/abs/2303.06310) | 驾驶员疲劳是导致交通事故的主要原因之一，本研究旨在通过机器学习算法开发一种实时检测驾驶员疲劳的系统。 |
| [^41] | [Virtual Mouse And Assistant: A Technological Revolution Of Artificial Intelligence.](http://arxiv.org/abs/2303.06309) | 本文介绍了虚拟助手的性能提升，虚拟助手是一种能够理解自然语言语音命令并能代表您执行任务的软件，可以完成几乎任何您自己可以完成的特定智能手机或PC活动，而且列表不断扩大。 |
| [^42] | [Blockchain-based decentralized voting system security Perspective: Safe and secure for digital voting system.](http://arxiv.org/abs/2303.06306) | 本文研究了基于区块链的去中心化投票系统，提出了一种独特的身份识别方式，使得每个人都能追踪投票欺诈，系统非常安全。 |
| [^43] | [Adversarial Attacks and Defenses in Machine Learning-Powered Networks: A Contemporary Survey.](http://arxiv.org/abs/2303.06302) | 本文综述了机器学习网络中的对抗攻击和防御技术，重点关注基于深度神经网络的分类模型。对最近的对抗攻击方法和最先进的对抗防御技术进行了全面分类，并以视觉上吸引人的表格和树状图的形式呈现。方法分类为反攻击检测和鲁棒性增强，特别关注于基于正则化的增强鲁棒性的方法。还探讨了新的攻击途径，包括基于搜索的攻击。 |
| [^44] | [MLP-SRGAN: A Single-Dimension Super Resolution GAN using MLP-Mixer.](http://arxiv.org/abs/2303.06298) | MLP-SRGAN是一种单维超分辨率GAN，使用MLP-Mixer和卷积层进行上采样，可用于FLAIR MRI图像的超分辨率重建，提出了新的图像质量度量方法。 |
| [^45] | [Stabilizing Transformer Training by Preventing Attention Entropy Collapse.](http://arxiv.org/abs/2303.06296) | 本文研究了Transformer的训练动态，发现低注意力熵伴随着高训练不稳定性，提出了一种简单而有效的解决方案$\sigma$Reparam，成功地防止了注意力层中的熵崩溃，促进了更稳定的训练。 |
| [^46] | [Space-Invariant Projection in Streaming Network Embedding.](http://arxiv.org/abs/2303.06293) | 本文提供了一个最大新节点数量的阈值，该阈值使节点嵌入空间保持近似等效，并提出了一种生成框架，称为空间不变投影（SIP），使任意静态MF嵌入方案能够快速嵌入动态网络中的新节点。 |
| [^47] | [Machine Learning Enhanced Hankel Dynamic-Mode Decomposition.](http://arxiv.org/abs/2303.06289) | 本文提出了一种基于深度学习DMD的方法，称为DLHDMD，利用Takens嵌入定理的基本见解开发了一种自适应学习方案，更好地捕捉了高维和混沌动力学，能够为混沌时间序列生成准确的动态。 |
| [^48] | [Investigating Stateful Defenses Against Black-Box Adversarial Examples.](http://arxiv.org/abs/2303.06280) | 本文探究了有状态防御黑盒对抗样本的方法，提出了一种新的有状态防御模型，可以在CIFAR10数据集上达到82.2％的准确性，在ImageNet数据集上达到76.5％的准确性。 |
| [^49] | [Enhancing Protein Language Models with Structure-based Encoder and Pre-training.](http://arxiv.org/abs/2303.06275) | 本文提出了一种结合基于结构的编码器和预训练的蛋白质语言模型，以明确地编码蛋白质结构，获得更好的结构感知蛋白质表示，并在实验中验证了其有效性。 |
| [^50] | [CoNIC Challenge: Pushing the Frontiers of Nuclear Detection, Segmentation, Classification and Counting.](http://arxiv.org/abs/2303.06274) | CoNIC挑战使用最大的数据集评估核分割和细胞组成，刺激了可重复的细胞识别算法的开发，发现嗜酸性粒细胞和中性粒细胞在肿瘤中发挥重要作用。 |
| [^51] | [DEPLOYR: A technical framework for deploying custom real-time machine learning models into the electronic medical record.](http://arxiv.org/abs/2303.06269) | DEPLOYR是一个技术框架，可以将研究人员创建的临床机器学习模型实时部署到广泛使用的电子病历系统中，并提供监控和反馈机制。 |
| [^52] | [Quantum Machine Learning Implementations: Proposals and Experiments.](http://arxiv.org/abs/2303.06263) | 本文概述了量子机器学习领域中最近的理论提议及其实验实现，重点回顾了量子强化学习、量子自编码器和量子记忆电阻器等高影响主题，并强调了推动这项技术的初步量子实现的必要性。 |
| [^53] | [Interpretable Outlier Summarization.](http://arxiv.org/abs/2303.06261) | STAIR提出了一种可解释的异常值汇总方法，通过学习一组紧凑的人类可理解规则，以汇总和解释异常检测结果，具有强大的可解释性，以准确地总结检测结果。 |
| [^54] | [Predicting risk of delirium from ambient noise and light information in the ICU.](http://arxiv.org/abs/2303.06253) | 本研究利用环境噪声和光线信息，开发了第一个基于深度学习的ICU患者谵妄预测模型，为谵妄的预防和治疗提供了新思路。 |
| [^55] | [Zone-based Federated Learning for Mobile Sensing Data.](http://arxiv.org/abs/2303.06246) | 本文提出了一种基于区域的联邦学习方法，用于训练移动感知数据的深度学习模型。该方法将物理空间划分为地理区域，并映射到移动边缘云系统架构，以实现良好的模型准确性和可扩展性。每个区域都有一个联合训练模型，能够很好地适应该区域用户的数据和行为，并保护用户数据隐私。 |
| [^56] | [HYperbolic Self-Paced Learning for Self-Supervised Skeleton-based Action Representations.](http://arxiv.org/abs/2303.06242) | 本文提出了一种新的超似曲自适应模型（HYSP）用于学习基于骨架的动作表示，采用自监督学习，使用数据增强来生成同一样本的两个视图，并通过将一个视图与另一个视图匹配来学习，使用超似曲不确定性来确定算法学习速度，假设不确定性较小的样本应更强烈地推动训练，具有更大的权重和速度。 |
| [^57] | [Do we need entire training data for adversarial training?.](http://arxiv.org/abs/2303.06241) | 本文提出了一种新的对抗训练方法，通过对训练数据集进行筛选，仅使用易受对抗攻击的样本进行训练，从而减少训练时间。 |
| [^58] | [Complement Sparsification: Low-Overhead Model Pruning for Federated Learning.](http://arxiv.org/abs/2303.06237) | 本文提出了一种名为补充稀疏化的模型剪枝机制，通过在服务器和客户端之间进行互补和协作的剪枝来满足联邦学习中低双向通信开销、客户端低计算开销和良好模型准确性的要求。 |
| [^59] | [Compressive Sensing with Tensorized Autoencoder.](http://arxiv.org/abs/2303.06235) | 本文提出了一种利用张量环因子分解的自动编码器来恢复图像的方法，该方法在修复和去噪应用中表现出更好的重建质量。 |
| [^60] | [Optimal and Private Learning from Human Response Data.](http://arxiv.org/abs/2303.06234) | 本文提出了一种新的谱估计算法，具有高效和准确的特点，可以在温和的采样条件下实现极小化最优误差界限，同时在top-$K$恢复的样本复杂度方面具有最优性。 |
| [^61] | [MCROOD: Multi-Class Radar Out-Of-Distribution Detection.](http://arxiv.org/abs/2303.06232) | 本文提出了一种基于重建的多类OOD检测器，该检测器在雷达距离多普勒图像（RDIs）上运行。检测器旨在将除坐、站或走的人以外的任何移动物体分类为OOD。作者还提供了一种简单而有效的预处理技术，以检测呼吸等微小的人体运动。在实验中，该方法表现优于最先进的OOD检测方法。 |
| [^62] | [CHGNN: A Semi-Supervised Contrastive Hypergraph Learning Network.](http://arxiv.org/abs/2303.06213) | CHGNN是一种半监督对比超图学习网络，利用自监督对比学习技术从标记和未标记的数据中学习，包括自适应超图视图生成器、改进的超图编码器和联合损失函数。 |
| [^63] | [A Theoretical Analysis Of Nearest Neighbor Search On Approximate Near Neighbor Graph.](http://arxiv.org/abs/2303.06210) | 本文提出了解决NN-Search的理论保证，通过贪心搜索ANN-Graph来解决低维密集向量的问题。这些结果提供了与构建近邻图时的近似相关的权衡的量化，为更多可证明的高效基于图的NN-Search算法打开了大门。 |
| [^64] | [Fast computation of permutation equivariant layers with the partition algebra.](http://arxiv.org/abs/2303.06208) | 本文提出了一种利用分区代数计算置换等变层输出和梯度的新算法，可以在输入大小的线性和二次时间内计算，有效性得到了在几个基准数据集上的证明。 |
| [^65] | [Interpretable Joint Event-Particle Reconstruction for Neutrino Physics at NOvA with Sparse CNNs and Transformers.](http://arxiv.org/abs/2303.06201) | 本研究提出了一种新颖的神经网络架构TransformerCVN，将卷积和注意力相结合，实现了对NOvA实验中复杂事件中多个粒子的联合分类和重建，为准确测量标准模型的关键参数提供了重要支持。 |
| [^66] | [Deflated HeteroPCA: Overcoming the curse of ill-conditioning in heteroskedastic PCA.](http://arxiv.org/abs/2303.06198) | 本文提出了一种新的算法，称为缩减异方差PCA，它在克服病态问题的同时实现了近乎最优和无条件数的理论保证。 |
| [^67] | [Papaya: Federated Learning, but Fully Decentralized.](http://arxiv.org/abs/2303.06189) | Papaya是一种点对点学习系统，节点在自己的数据上进行训练，并定期根据学习的信任矩阵将其参数与同伴的参数进行加权平均，从而实现联邦学习的去中心化，避免了集中式服务器的带宽和资源密集型限制和隐私问题。 |
| [^68] | [Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert (MoE) Inference.](http://arxiv.org/abs/2303.06182) | 本文提出了三种优化技术来缓解混合专家（MoE）模型在推理时的低效率，包括动态门控、专家缓冲和专家负载平衡。这些技术可以显著提高执行时间和减少内存使用。 |
| [^69] | [Optimizing Federated Learning for Medical Image Classification on Distributed Non-iid Datasets with Partial Labels.](http://arxiv.org/abs/2303.06180) | 本文提出了FedFBN，一个联邦学习框架，使用预训练的网络作为模型后端，并在整个训练过程中冻结批量归一化层，以优化分布式非独立同分布数据和部分标签的医学图像分类。 |
| [^70] | [Software Vulnerability Prediction Knowledge Transferring Between Programming Languages.](http://arxiv.org/abs/2303.06177) | 本研究提出了一种转移学习技术，利用可用数据集生成一个模型，以检测不同编程语言中的常见漏洞。结果表明，所提出的模型以平均召回率为72％检测C和Java代码中的漏洞。 |
| [^71] | [Unifying Grokking and Double Descent.](http://arxiv.org/abs/2303.06173) | 本文提出了一种模式学习速度框架，用于统一理解Grokking和双重下降，同时提供了模型智能Grokking的第一个演示。 |
| [^72] | [DP-Fast MH: Private, Fast, and Accurate Metropolis-Hastings for Large-Scale Bayesian Inference.](http://arxiv.org/abs/2303.06171) | 本文提出了一种新的DP-Fast MH算法，用于大规模贝叶斯推断，具有精确、快速和隐私保护的特点。 |
| [^73] | [MOELA: A Multi-Objective Evolutionary/Learning Design Space Exploration Framework for 3D Heterogeneous Manycore Platforms.](http://arxiv.org/abs/2303.06169) | MOELA是一个多目标设计空间探索框架，结合了进化搜索和学习搜索，用于优化3D NoC启用的异构多核系统中的多个目标，相比现有技术，MOELA可以提高解决方案的查找速度，提高Pareto Hypervolume（PHV）和能量延迟乘积（EDP）。 |
| [^74] | [Overcoming Bias in Pretrained Models by Manipulating the Finetuning Dataset.](http://arxiv.org/abs/2303.06167) | 本文研究了预训练模型中的偏见问题，发现微调模型可以继承预训练模型的偏见，但通过对微调数据集进行干预可以纠正这种偏见，而且对性能的影响很小。这表明仔细策划微调数据集对于减少下游任务中的偏见非常重要，这样做甚至可以弥补预训练模型中的偏见。 |
| [^75] | [Understanding the Synergies between Quality-Diversity and Deep Reinforcement Learning.](http://arxiv.org/abs/2303.06164) | 本文提出了广义演员-评论家QD-RL框架，用于QD-RL设置中的演员-评论家深度RL方法。该框架引入了两种新算法，PGA-ME（SAC）和PGA-ME（DroQ），将深度RL的最新进展应用于QD-RL设置，并解决了现有QD-RL算法无法解决的人形环境问题。 |
| [^76] | [Digital Twin-Assisted Knowledge Distillation Framework for Heterogeneous Federated Learning.](http://arxiv.org/abs/2303.06155) | 本文提出了一种数字孪生辅助的知识蒸馏框架，用于解决联邦学习系统中的异构性问题，用户可以选择自己的神经网络模型并从大型教师模型中蒸馏知识，同时利用数字孪生在服务器上训练大型教师模型，最终通过混合整数规划和Q-learning算法实现模型选择和资源分配。 |
| [^77] | [Resource saving taxonomy classification with k-mer distributions and machine learning.](http://arxiv.org/abs/2303.06154) | 本文提出了一种基于k-mer分布和机器学习的分类法，可以节约资源并提高分类器的性能。 |
| [^78] | [NoiseCAM: Explainable AI for the Boundary Between Noise and Adversarial Attacks.](http://arxiv.org/abs/2303.06151) | 本文提出了一种名为NoiseCAM的算法，该算法可以定位易受攻击层并检测对抗性示例，同时不会对混入输入的高斯随机噪声做出反应。 |
| [^79] | [Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings.](http://arxiv.org/abs/2303.05737) | 本文提出了一种临床BERTScore（CBERTScore）度量，它比其他度量更严厉地惩罚临床相关的错误，更接近于临床医生对医学句子的偏好。作者还收集了13个临床医生对149个现实医学句子的偏好基准，称为临床转录偏好基准（CTP），证明CBERTScore更接近于临床医生的偏好，并将基准发布给社区以进一步开发具有临床意识的ASR度量。 |
| [^80] | [Bounding the Probabilities of Benefit and Harm Through Sensitivity Parameters and Proxies.](http://arxiv.org/abs/2303.05396) | 本文提出了两种方法来限制未测量混杂下的受益和伤害概率，一种是通过敏感性参数计算概率的上限或下限，另一种是利用代理变量得到更紧的界限。 |
| [^81] | [Real-time scheduling of renewable power systems through planning-based reinforcement learning.](http://arxiv.org/abs/2303.05205) | 本文提出了一种基于规划强化学习算法和真实电力网环境的系统解决方案，可以实现发电机的规划和更细的时间分辨率调整，从而增加了电网的能力。 |
| [^82] | [A Study of Variable-Role-based Feature Enrichment in Neural Models of Code.](http://arxiv.org/abs/2303.04942) | 本文研究了一种基于变量角色的无监督特征增强方法对代码神经模型性能的影响，通过在数据集程序中添加单个变量的角色来丰富源代码数据集，并因此对变量角色增强在训练Code2Seq模型中的影响进行了研究。 |
| [^83] | [Enabling Non-Linear Quantum Operations through Variational Quantum Splines.](http://arxiv.org/abs/2303.04788) | 本文提出了一种新方法——广义QSplines，使用混合量子-经典计算来近似非线性量子激活函数，克服了原始QSplines在量子硬件方面的高要求，并适合嵌入现有的量子神经网络架构中。 |
| [^84] | [Enhanced Adaptive Gradient Algorithms for Nonconvex-PL Minimax Optimization.](http://arxiv.org/abs/2303.03984) | 本文提出了一类增强的基于动量的梯度下降上升方法（即MSGDA和AdaMSGDA）来解决非凸-PL极小极大问题，其中AdaMSGDA算法可以使用各种自适应学习率来更新变量$x$和$y$，而不依赖于任何全局和坐标自适应学习率。理论上，我们证明了我们的MSGDA和AdaMSGDA方法在找到$\epsilon$-稳定解时，只需要在每个循环中进行一次采样，就可以获得已知的最佳样本（梯度）复杂度$O(\epsilon^{-3})$。 |
| [^85] | [PreFallKD: Pre-Impact Fall Detection via CNN-ViT Knowledge Distillation.](http://arxiv.org/abs/2303.03634) | 本文提出了一种基于CNN-ViT知识蒸馏的预防跌倒系统PreFallKD，通过将预训练的教师模型的检测知识转移到轻量级卷积神经网络的学生模型上，实现了检测性能和计算复杂性的平衡。 |
| [^86] | [Heterogeneous Graph Learning for Acoustic Event Classification.](http://arxiv.org/abs/2303.02665) | 本文提出了一种新模型，异构图跨模态网络（HGCN），它学习跨模态边缘，可以适应各种空间和时间尺度，有效地连接了跨模态的相关节点，在声音事件分类中表现出最先进的性能。 |
| [^87] | [Prismer: A Vision-Language Model with An Ensemble of Experts.](http://arxiv.org/abs/2303.02506) | Prismer是一种数据和参数高效的视觉语言模型，它利用了一组领域专家的集合，通过汇集这些专家知识并将其适应于各种视觉语言推理任务，实现了与当前最先进模型竞争的微调和少样本学习性能，同时需要少至两个数量级的训练数据。 |
| [^88] | [Learning to Influence Human Behavior with Offline Reinforcement Learning.](http://arxiv.org/abs/2303.02265) | 本文提出了一种通过离线强化学习学习影响人类行为的方法，可以提高人类在协作任务中的表现。 |
| [^89] | [Uncertainty Estimation by Fisher Information-based Evidential Deep Learning.](http://arxiv.org/abs/2303.02045) | 本文提出了一种基于Fisher信息的证据深度学习方法，用于解决高数据不确定性样本但注释为one-hot标签的情况下证据学习过程被过度惩罚并受到阻碍的问题。 |
| [^90] | [An Efficient Tester-Learner for Halfspaces.](http://arxiv.org/abs/2302.14853) | 我们提出了第一个在可测试学习模型中学习半空间的高效算法，该算法在多项式时间内运行，并输出一个在任何强对数凹目标分布下具有（信息理论上最优的）误差的假设。 |
| [^91] | [A Dataset for Learning Graph Representations to Predict Customer Returns in Fashion Retail.](http://arxiv.org/abs/2302.14096) | 该论文介绍了一个由ASOS收集的新型数据集，用于解决时尚零售生态系统中预测客户退货的挑战。研究者使用图表示学习方法，提高了退货预测分类任务的F1分数至0.792，这比其他模型有所改进。 |
| [^92] | [Revisiting Self-Training with Regularized Pseudo-Labeling for Tabular Data.](http://arxiv.org/abs/2302.14013) | 本文重新审视了自我训练，并引入了课程伪标签用于表格领域。提出了一种新的伪标签方法，它规范化了置信度分数。 |
| [^93] | [Learning Topology-Specific Experts for Molecular Property Prediction.](http://arxiv.org/abs/2302.13693) | 本文提出了TopExpert，利用拓扑特定的预测模型（称为专家），每个专家负责每个共享类似拓扑语义的分子组，以提高分子属性预测的性能。 |
| [^94] | [UnbiasedNets: A Dataset Diversification Framework for Robustness Bias Alleviation in Neural Networks.](http://arxiv.org/abs/2302.12538) | 本文提出了UnbiasedNets框架，通过利用K-means聚类和NN的噪声容忍度来使给定的训练数据集多样化，生成平衡的数据集并减少数据集内的偏差。 |
| [^95] | [Multimodal Federated Learning via Contrastive Representation Ensemble.](http://arxiv.org/abs/2302.08888) | 本文提出了一种名为CreamFL的多模态联邦学习框架，可以从具有异构模型架构和数据模态的客户端中训练更大的服务器模型，同时只在公共数据集上传递知识。 |
| [^96] | [Deep Convolutional Neural Network for Plume Rise Measurements in Industrial Environments.](http://arxiv.org/abs/2302.07416) | 本文提出了一种低成本的测量技术，用于监测烟囱烟羽并进行长期实时测量。基于深度卷积神经网络（DCNNs）开发了一种两阶段方法，其中第一阶段使用改进的Mask R-CNN来识别烟羽，第二阶段使用最小二乘法将渐近模型拟合到边界中心线中，以估计烟囱烟羽上升。 |
| [^97] | [Learning a model is paramount for sample efficiency in reinforcement learning control of PDEs.](http://arxiv.org/abs/2302.07160) | 本文强调在使用强化学习进行偏微分方程控制时使用动态模型的重要性。使用卷积LSTM作为带有激励的数据驱动代理模型可以显著减少从真实系统中采样所需的总数据量。迭代更新模型对于避免RL训练中的偏差非常重要。 |
| [^98] | [Lightsolver challenges a leading deep learning solver for Max-2-SAT problems.](http://arxiv.org/abs/2302.06926) | 本文比较了LightSolver的量子启发式算法和领先的深度学习求解器在MAX-2-SAT问题上的表现，实验结果表明LightSolver实现了显著更小的最优解时间。 |
| [^99] | [On Penalty-based Bilevel Gradient Descent Method.](http://arxiv.org/abs/2302.05185) | 本文提出了基于惩罚的双层梯度下降算法，解决了下层非强凸约束双层问题，实验表明该算法有效。 |
| [^100] | [Exploiting Sparsity in Pruned Neural Networks to Optimize Large Model Training.](http://arxiv.org/abs/2302.05045) | 本文提出了一种新的方法，利用稀疏子网络来优化两种流行的深度学习并行算法 - 数据并行和层间并行的内存利用和通信。在512个NVIDIA V100 GPU上，我们的优化将27亿参数模型的内存消耗减少了74％，总通信时间减少了40％。 |
| [^101] | [The Unfairness of Fair Machine Learning: Levelling down and strict egalitarianism by default.](http://arxiv.org/abs/2302.02404) | 公平机器学习的定义过于简单，许多公平性措施会导致性能降级和水平下降，使每个人都变得更糟，这种做法不符合实质平等的机会。 |
| [^102] | [Interpretations of Domain Adaptations via Layer Variational Analysis.](http://arxiv.org/abs/2302.01798) | 本研究通过层变分分析证明了转移学习的成功可以通过相应的数据条件得到保证，并提出了一种基于网络的转移学习的替代方法，该方法在领域适应方面显示出了效率和准确性的提高。 |
| [^103] | [Rethinking Semi-Supervised Medical Image Segmentation: A Variance-Reduction Perspective.](http://arxiv.org/abs/2302.01735) | 本文提出了ARCO，一种半监督对比学习（CL）框架，其中包括医学图像分割中的分层组采样理论。通过方差缩减估计的概念来构建ARCO，并表明某些方差缩减技术在医学图像分割中特别有益。 |
| [^104] | [Probabilistic Point Cloud Modeling via Self-Organizing Gaussian Mixture Models.](http://arxiv.org/abs/2302.00047) | 本文提出了一种基于自组织高斯混合模型的概率点云建模方法，可以根据场景复杂度自动调整模型复杂度，相比现有技术具有更好的泛化性能。 |
| [^105] | [LDMIC: Learning-based Distributed Multi-view Image Coding.](http://arxiv.org/abs/2301.09799) | LDMIC是一种基于学习的分布式多视图图像编码框架，通过独立编码器和联合上下文传输模块实现了全局视图间的相关性捕捉，对几何关系不敏感。 |
| [^106] | [Towards NeuroAI: Introducing Neuronal Diversity into Artificial Neural Networks.](http://arxiv.org/abs/2301.09245) | 引入神经元多样性可以解决人工神经网络的基本问题，走向神经人工智能。 |
| [^107] | [Trustworthiness Score to Evaluate CNNs Predictions.](http://arxiv.org/abs/2301.08839) | 本文提出了一种用于评估CNN预测可信度的可信度分数（TS）度量标准，通过检查CNN所做预测中的某些特征的存在来量化预测的可信度。 |
| [^108] | [SegViz: A federated-learning based framework for multi-organ segmentation on heterogeneous data sets with partial annotations.](http://arxiv.org/abs/2301.07074) | SegViz是一种基于联邦学习的框架，用于从分布式的非i.i.d数据集中训练具有部分注释的分割模型。使用FedBN作为聚合策略的SegViz框架在外部BTCV集上表现出优异的性能，分割的dice分数分别为0.93、0.83、0.55和0.75。 |
| [^109] | [Generalized Invariant Matching Property via LASSO.](http://arxiv.org/abs/2301.05975) | 本文提出了一种基于LASSO的广义不变匹配性质，通过制定具有内在稀疏性的高维问题，将不变匹配性质推广到仅目标被干预的重要情况，并提出了一种更加稳健和计算效率高的算法，改进了现有算法。 |
| [^110] | [Kinematic Evidence of an Embedded Protoplanet in HD 142666 Identified by Machine Learning.](http://arxiv.org/abs/2301.05075) | 该论文使用机器学习模型在HD 142666的盘中识别出强烈的、局部的非开普勒运动，进而得出该盘中存在一个行星的结论，这是使用机器学习识别原行星盘中先前被忽视的非开普勒特征的第一步。 |
| [^111] | [Perceptual-Neural-Physical Sound Matching.](http://arxiv.org/abs/2301.02886) | 本文提出了一种新的声音匹配算法，称为感知-神经-物理损失（PNP），它是频谱损失的最优二次近似，能够更好地适应不同参数的感知重要性，同时具有快速收敛的特点。 |
| [^112] | [Image Data Augmentation Approaches: A Comprehensive Survey and Future directions.](http://arxiv.org/abs/2301.02830) | 本文综述了图像数据增强方法，提供了全面的分类法和每种技术的优缺点，并给出了数据增强对图像分类、目标检测和语义分割等计算机视觉任务的全面结果。 |
| [^113] | [Unlearnable Clusters: Towards Label-agnostic Unlearnable Examples.](http://arxiv.org/abs/2301.01217) | 本文提出了一种更实用的标签不可知设置，以生成不可学习的样本，防止未经授权的机器学习模型训练。 |
| [^114] | [A attention way in Explainable methods for infant brain.](http://arxiv.org/abs/2301.00815) | 本文提出了一种可解释的几何深度网络，通过端到端学习解释因素以增强区分性表示提取，以反向保证细粒度的可解释性，适用于神经影像和神经科学研究中的高维数据。 |
| [^115] | [One-shot domain adaptation in video-based assessment of surgical skills.](http://arxiv.org/abs/2301.00812) | 本文提出了一种元学习模型A-VBANet，可以通过一次性学习提供领域不可知的手术技能分类，成功地适应了模拟任务和腹腔镜胆囊切除术，为基于视频的手术技能评估提供了领域不可知程序。 |
| [^116] | [UniDA3D: Unified Domain Adaptive 3D Semantic Segmentation Pipeline.](http://arxiv.org/abs/2212.10390) | 本文提出了UniDA3D，一种统一的域自适应三维语义分割管道，通过设计统一的源和目标主动采样策略，可以解决三维分割领域中的多个自适应任务，并探索了实现多模态采样策略的可能性。 |
| [^117] | [A large-scale and PCR-referenced vocal audio dataset for COVID-19.](http://arxiv.org/abs/2212.07738) | 英国COVID-19 Vocal Audio Dataset是迄今为止最大的SARS-CoV-2 PCR参考音频记录集合，旨在为训练和评估使用声音数据分类SARS-CoV-2感染状态或相关呼吸症状的机器学习模型而设计。 |
| [^118] | [Client Selection for Federated Bayesian Learning.](http://arxiv.org/abs/2212.05492) | 本文提出了两种基于核化Stein差异（KSD）和希尔伯特内积（HIP）的DSVGD选择方案，以提高联邦贝叶斯学习中的模型收敛和通信效率。 |
| [^119] | [P{\O}DA: Prompt-driven Zero-shot Domain Adaptation.](http://arxiv.org/abs/2212.03241) | 本文提出了一种基于提示的零样本领域自适应方法，通过利用预训练的对比视觉语言模型（CLIP）来优化源特征的仿射变换，将其引导到目标文本嵌入中，同时保留其内容和语义。实验表明，该方法在几个数据集上显著优于基于CLIP的风格转移基线，用于下游任务。 |
| [^120] | [Unifying Vision, Text, and Layout for Universal Document Processing.](http://arxiv.org/abs/2212.02623) | 本文提出了通用文档处理（UDOP）模型，将文本、图像和布局模态以及各种任务格式统一起来，通过一种新颖的Transformer模型实现预训练和多域下游任务的统一，同时实现了高质量的神经文档编辑和内容定制。 |
| [^121] | [Deep Neural Mel-Subband Beamformer for In-car Speech Separation.](http://arxiv.org/abs/2211.12590) | 本文提出了一种基于DL的Mel-Subband时空波束成形器，用于在车载环境中进行语音分离，通过基于Mel尺度的子带选择策略，实现对低频的细粒度处理和对高频的粗粒度处理，降低了计算成本和推理时间。 |
| [^122] | [SinFusion: Training Diffusion Models on a Single Image or Video.](http://arxiv.org/abs/2211.11743) | 本文提出了一种在单张图像或视频上训练扩散模型的方法，称为SinFusion。该模型可以解决各种图像/视频特定的操作任务，包括从少量帧中学习单个输入视频的运动和动态，生成相同动态场景的多样化新视频样本，将短视频推广为长视频（向前和向后）并执行视频上采样。 |
| [^123] | [LA-VocE: Low-SNR Audio-visual Speech Enhancement using Neural Vocoders.](http://arxiv.org/abs/2211.10999) | LA-VocE是一种新的音频视觉语音增强方法，使用神经声码器将从嘈杂的音频视觉语音预测的mel频谱图转换为波形音频，适用于多种语言和不同水平的背景噪声和语音干扰。 |
| [^124] | [Differentiable Uncalibrated Imaging.](http://arxiv.org/abs/2211.10525) | 本文提出了一种可微的成像框架，以解决测量坐标的不确定性，通过隐式神经网络和可微分样条插值器实现。该方法应用于2D和3D计算机断层扫描，产生了改进的重建结果。 |
| [^125] | [HMOE: Hypernetwork-based Mixture of Experts for Domain Generalization.](http://arxiv.org/abs/2211.08253) | 本文提出了一种新的领域泛化方法HMOE，它不需要领域标签，更具可解释性，使用超网络生成专家权重，能够在低维向量空间中探索专家的相似性，实验结果表明HMOE可以划分混合数据并取得更好的效果。 |
| [^126] | [Learning Neuro-symbolic Programs for Language Guided Robot Manipulation.](http://arxiv.org/abs/2211.06652) | 该论文提出了一种学习神经符号程序以进行语言引导的机器人操作的方法，可以处理语言和感知变化，端到端可训练，不需要中间监督。该方法使用符号推理构造，在潜在的神经物体为中心的表示上操作，允许对输入场景进行更深入的推理。 |
| [^127] | [Efficient brain age prediction from 3D MRI volumes using 2D projections.](http://arxiv.org/abs/2211.05762) | 本文提出了一种使用2D投影从3D MRI体积中高效预测脑龄的方法，相比于使用3D CNN，该方法在计算速度上有两个数量级的提升，对于没有3D CNN昂贵GPU硬件的研究人员非常有用。 |
| [^128] | [Duality for Neural Networks through Reproducing Kernel Banach Spaces.](http://arxiv.org/abs/2211.05020) | 本文提出了一种新的再生核Banach空间（RKBS）方法，用于解决神经网络中的对偶性问题，构建了神经网络的鞍点问题，可用于原始-对偶优化的整个领域。 |
| [^129] | [Privacy-Aware Compression for Federated Learning Through Numerical Mechanism Design.](http://arxiv.org/abs/2211.03942) | 本文提出了一种新的插值MVU机制，通过数值机制设计实现面向隐私的联邦学习压缩，具有更好的隐私效用权衡和更高的可扩展性，并在各种数据集上提供了通信高效的私有FL的SOTA结果。 |
| [^130] | [Efficient ECG-based Atrial Fibrillation Detection via Parameterised Hypercomplex Neural Networks.](http://arxiv.org/abs/2211.02678) | 本文提出了一种基于参数化超复数神经网络的轻量级卷积神经网络方法，用于心房颤动检测。该方法在可穿戴设备上训练小规模CNN，克服了有限的计算资源。在两个公开可用的ECG数据集上，该方法表现出与实值CNN相当的性能，但使用了显着较少的模型参数。 |
| [^131] | [Graph Neural Networks on SPD Manifolds for Motor Imagery Classification: A Perspective from the Time-Frequency Analysis.](http://arxiv.org/abs/2211.02641) | 本文介绍了一种基于SPD流形的图神经网络用于运动想象分类，利用EEG的二阶统计量，相比传统方法具有更好的性能。 |
| [^132] | [Cutting Through the Noise: An Empirical Comparison of Psychoacoustic and Envelope-based Features for Machinery Fault Detection.](http://arxiv.org/abs/2211.01704) | 本文提出了一个自动化和噪声鲁棒的听觉检查系统，用于检测机械部件的健康状况。我们提供了一个基准来比较不同类型的包络特征与心理声学特征。我们是第一个应用时变心理声学特征进行故障检测的人。 |
| [^133] | [Using Emotion Embeddings to Transfer Knowledge Between Emotions, Languages, and Annotation Formats.](http://arxiv.org/abs/2211.00171) | 本文研究了如何通过利用多语言模型和Demux来构建一个可以在不同情感、语言和注释格式之间转换的单一模型，以实现知识共享和降低训练成本。 |
| [^134] | [A Machine Learning Tutorial for Operational Meteorology, Part II: Neural Networks and Deep Learning.](http://arxiv.org/abs/2211.00147) | 本文讨论了机器学习在气象学中的应用，特别是神经网络和深度学习。涵盖了感知器、人工神经网络、卷积神经网络和U型网络等方法。 |
| [^135] | [Diffusion models for missing value imputation in tabular data.](http://arxiv.org/abs/2210.17128) | 本文提出了一种名为“表格数据条件分数扩散模型”（TabCSDI）的扩散模型方法，用于处理表格数据中的缺失值插补，该方法同时处理分类变量和数值变量，实验结果表明其在各种数据集上都取得了优异的性能。 |
| [^136] | [Learning Audio Features with Metadata and Contrastive Learning.](http://arxiv.org/abs/2210.16192) | 本研究使用监督对比学习结合可用元数据解决多个前置任务，学习数据的良好表示。在呼吸音分类数据集上，仅使用元数据学习表示可以获得与仅使用类标签的交叉熵相似的性能。在使用多个监督对比学习将类标签与元数据相结合时，获得了最先进的得分。 |
| [^137] | [Leveraging Label Correlations in a Multi-label Setting: A Case Study in Emotion.](http://arxiv.org/abs/2210.15842) | 本文研究了利用标签相关性来改善情感检测的方法，开发了两种建模方法来捕捉情感词本身的词汇关联性，并将情感表示的成对约束作为正则化项与模型的分类损失一起集成，展示了在SemEval 2018任务1 E-c中使用单语BERT模型展示了西班牙语、英语和阿拉伯语的最新性能。 |
| [^138] | [Broken Neural Scaling Laws.](http://arxiv.org/abs/2210.14891) | 本文提出了一个平滑破碎的幂律函数形式，可以准确地模拟和外推深度神经网络的缩放行为，适用于各种架构和大量不同任务，包括视觉、语言、音频、视频、生成建模、对比学习、机器人、不确定性估计/校准、对抗鲁棒性、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。 |
| [^139] | [D-Shape: Demonstration-Shaped Reinforcement Learning via Goal Conditioning.](http://arxiv.org/abs/2210.14428) | D-Shape是一种新的结合IL和RL的方法，它使用奖励塑形和目标条件化RL的思想来解决次优演示与回报最大化目标之间的冲突，能够在稀疏奖励网格世界领域中提高样本效率并一致地收敛到最优策略。 |
| [^140] | [A Dynamical System View of Langevin-Based Non-Convex Sampling.](http://arxiv.org/abs/2210.13867) | 本文提出了一种新的框架，通过利用动力系统理论中的几个工具来解决非凸采样中的重要挑战。对于一大类最先进的采样方案，它们在Wasserstein距离下的最后迭代收敛可以归结为对它们的连续时间对应物的研究，这是更好理解的。 |
| [^141] | [Removing Radio Frequency Interference from Auroral Kilometric Radiation with Stacked Autoencoders.](http://arxiv.org/abs/2210.12931) | 本研究利用深度学习算法中的自编码器，提出了一种名为DAARE的去噪自编码器，用于消除极光千米辐射中的射频干扰。DAARE在合成AKR观测中实现了42.2的峰值信噪比(PSNR)和0.981的结构相似度(SSIM)，相比于最先进的滤波和去噪网络，PSNR提高了3.9，SSIM提高了0.064。定性比较表明，DAARE能够有效地消除RFI。 |
| [^142] | [Play It Back: Iterative Attention for Audio Recognition.](http://arxiv.org/abs/2210.11328) | 该论文提出了一种基于注意力的架构，通过选择性重复跨越音频序列的最具区分性的声音来进行关注，最终实现了在三个音频分类基准测试中始终实现最先进的性能。 |
| [^143] | [Discovering Many Diverse Solutions with Bayesian Optimization.](http://arxiv.org/abs/2210.10953) | ROBOT是一种新的贝叶斯优化方法，可以找到一组高性能、多样化的解决方案，解决了传统单目标贝叶斯优化方法只能找到一个最佳解决方案的局限性。 |
| [^144] | [Attribution and Obfuscation of Neural Text Authorship: A Data Mining Perspective.](http://arxiv.org/abs/2210.10488) | 本文调查了神经文本生成技术在作者归属和混淆中的应用，提出了需要开发新型AA / AO解决方案来处理神经文本的问题。 |
| [^145] | [Communication-Efficient Topologies for Decentralized Learning with $O(1)$ Consensus Rate.](http://arxiv.org/abs/2210.07881) | 本文提出了一种新的拓扑结构家族EquiTopo，它具有（几乎）恒定的度数和与网络大小无关的共识速率，用于衡量混合效率。 |
| [^146] | [Self-Supervised Geometric Correspondence for Category-Level 6D Object Pose Estimation in the Wild.](http://arxiv.org/abs/2210.07199) | 本文提出了一种自监督学习方法，直接在大规模真实世界物体视频上进行类别级6D姿态估计。通过表面嵌入学习了输入图像和规范形状之间的密集对应关系，并提出了新颖的几何循环一致性损失。学习到的对应关系可以应用于6D姿态估计和其他任务。 |
| [^147] | [PDEBENCH: An Extensive Benchmark for Scientific Machine Learning.](http://arxiv.org/abs/2210.07182) | PDEBench是一个基于偏微分方程的时间依赖性模拟任务基准套件，包括代码和数据，可用于对新型机器学习模型的性能进行基准测试，同时还可以与经典数值模拟和机器学习基线进行比较。 |
| [^148] | [Hybrid RL: Using Both Offline and Online Data Can Make RL Efficient.](http://arxiv.org/abs/2210.06718) | 本文提出了一种混合强化学习设置，通过同时使用离线和在线数据，可以设计出简单且高效的算法，我们证明了该算法在离线数据集支持高质量策略且环境具有有界双线性秩的情况下既具有计算效率又具有统计效率。在实验中，我们展示了使用神经网络函数逼近的Hy-Q的表现优于其他算法。 |
| [^149] | [Relational Attention: Generalizing Transformers for Graph-Structured Tasks.](http://arxiv.org/abs/2210.05062) | 本文提出了一种关系注意力机制，将Transformer推广到图结构任务中，通过考虑和更新边向量，实现了对图结构数据的推理，相比于专门设计用于推理图结构数据的最先进的图神经网络，在各种图结构任务上都有明显的优势。 |
| [^150] | [Symmetry Defense Against CNN Adversarial Perturbation Attacks.](http://arxiv.org/abs/2210.04087) | 本文提出了一种对称防御方法，通过翻转或水平翻转对称对抗样本来提高对抗性鲁棒性，同时使用子群对称性进行分类。 |
| [^151] | [Label Propagation with Weak Supervision.](http://arxiv.org/abs/2210.03594) | 本文提出了一种利用弱监督信息的标签传播算法，通过利用未标记数据上的概率假设标签，结合局部几何特性和先验信息的质量，提供了一个误差界，并提出了一个框架，用于合并多个噪声信息源。在多个基准弱监督分类任务上展示了方法的能力，显示出对现有半监督和弱监督方法的改进。 |
| [^152] | [PSVRF: Learning to restore Pitch-Shifted Voice without reference.](http://arxiv.org/abs/2210.02731) | 本文提出了一种无参考方法PSVRF，用于高质量还原变调语音，可以增强ASV系统对音高缩放攻击的鲁棒性，性能甚至超过了最先进的基于参考的方法。 |
| [^153] | [Sparsity by Redundancy: Solving $L_1$ with SGD.](http://arxiv.org/abs/2210.01212) | 该论文提出了一种通过冗余重参数化和简单的随机梯度下降来最小化带有$L_1$惩罚的通用可微损失函数的方法，称为\textit{spred}，是$L_1$的精确求解器，可用于训练稀疏神经网络以执行基因选择任务和神经网络压缩任务，弥合了深度学习中的稀疏性和传统统计学习之间的差距。 |
| [^154] | [Unsupervised Model Selection for Time-series Anomaly Detection.](http://arxiv.org/abs/2210.01078) | 本文提出了一种无监督模型选择方法，用于时间序列异常检测，通过三类替代度量，即预测误差、模型中心性和注入合成异常的性能，选择最准确的模型。 |
| [^155] | [What shapes the loss landscape of self-supervised learning?.](http://arxiv.org/abs/2210.00638) | 本文通过分析自监督学习的损失函数空间，回答了维度崩溃的原因和影响，以及维度崩溃如何有益，并影响SSL对数据不平衡的鲁棒性。 |
| [^156] | [Differentiable Parsing and Visual Grounding of Natural Language Instructions for Object Placement.](http://arxiv.org/abs/2210.00215) | ParaGon是一种可微分的自然语言指令解析和视觉定位方法，通过将语言指令解析为以对象为中心的图形表示，以单独定位对象，并使用一种新颖的基于粒子的图神经网络来推理关于带有不确定性的物体放置。 |
| [^157] | [Recipro-CAM: Fast gradient-free visual explanations for convolutional neural networks.](http://arxiv.org/abs/2209.14074) | Recipro-CAM是一种快速无梯度的可解释性卷积神经网络的视觉解释方法，通过对提取的特征图进行空间掩蔽，利用激活图和目标类别的网络预测之间的相关性，解决了CAM和Grad-CAM方法的架构限制和梯度计算负担问题，具有更短的执行时间，适用于实际解决方案。 |
| [^158] | [On the Stability Analysis of Open Federated Learning Systems.](http://arxiv.org/abs/2209.12307) | 本文研究了开放式联邦学习系统的稳定性问题，提出了一种新的性能度量，即开放式FL系统的稳定性，并在假设本地客户端函数是强凸和平滑的情况下，理论上量化了两种FL算法的稳定半径。 |
| [^159] | [U-Sleep's resilience to AASM guidelines.](http://arxiv.org/abs/2209.11173) | 本研究表明，基于深度学习的U-Sleep睡眠评分算法可以弹性地使用非推荐或非传统的导联，而不需要严格遵守AASM指南。 |
| [^160] | [Learning Sparse Graphon Mean Field Games.](http://arxiv.org/abs/2209.03880) | 本文提出了一种新型的GMFG公式，称为LPGMFG，它利用$L^p$图形的图形理论概念，提供了一种机器学习工具，以有效且准确地近似解决稀疏网络问题，特别是幂律网络。我们推导出理论存在和收敛保证，并给出了实证例子，证明了我们的学习方法的准确性。 |
| [^161] | [Estimation of Correlation Matrices from Limited time series Data using Machine Learning.](http://arxiv.org/abs/2209.01198) | 本文提出了一种利用机器学习从少数节点的有限时间序列信息预测整个系统相关矩阵的方法，验证了只有基础系统的子集的有限时间序列足以进行良好的相关矩阵预测。 |
| [^162] | [Bayesian Optimization-based Combinatorial Assignment.](http://arxiv.org/abs/2208.14698) | 本文提出了一种基于贝叶斯优化的组合分配（BOCA）机制，通过将捕获模型不确定性的方法集成到迭代组合拍卖机制中，解决了组合分配领域中先前工作的主要缺点，能够更好地引导代理提供信息。 |
| [^163] | [EGFR Mutation Prediction of Lung Biopsy Images using Deep Learning.](http://arxiv.org/abs/2208.12506) | 本文使用深度学习技术，通过对肺活检图像进行分析，实现了对EGFR突变的预测，为肺癌治疗提供了更经济、更快捷的诊断方法。 |
| [^164] | [Estimating a potential without the agony of the partition function.](http://arxiv.org/abs/2208.09433) | 本文提出了一种不需要计算配分函数的潜力估计方法，基于最大后验估计（MAP）估计器，将问题重新表述为优化问题，并提出了一种最小作用量类型的势函数，使我们能够快速将优化问题解决为前馈双曲神经网络。 |
| [^165] | [Evaluating Continual Test-Time Adaptation for Contextual and Semantic Domain Shifts.](http://arxiv.org/abs/2208.08767) | 本文评估了针对上下文和语义领域漂移的连续测试时间适应性，无需标签。研究发现，连续测试时间适应性（CoTTA）是一种有效的方法。 |
| [^166] | [Uconv-Conformer: High Reduction of Input Sequence Length for End-to-End Speech Recognition.](http://arxiv.org/abs/2208.07657) | 本文提出了一种新型Uconv-Conformer架构，可以将输入序列长度缩短16倍，加速中间层的工作，同时通过使用上采样块解决了收敛问题，表现出更好的WER和更快的训练和推理速度。 |
| [^167] | [Physics-Constrained Deep Learning for Climate Downscaling.](http://arxiv.org/abs/2208.05424) | 本文提出了一种物理约束深度学习降尺度模型的方法，以保证模型在预测物理变量时满足守恒定律，并提高其性能。 |
| [^168] | [Intrinsic dimension estimation for discrete metrics.](http://arxiv.org/abs/2207.09688) | 本文介绍了一种算法，用于推断嵌入离散空间的数据集的内在维度（ID），并在物种指纹的代谢组学数据集上展示了其准确性，发现一个令人惊讶的小ID，约为2的数量级。 |
| [^169] | [XG-BoT: An Explainable Deep Graph Neural Network for Botnet Detection and Forensics.](http://arxiv.org/abs/2207.09088) | 本文提出了一种名为XG-BoT的可解释的深度图神经网络模型，用于检测大规模网络中的恶意僵尸网络节点，并通过突出显示可疑的网络流和相关的僵尸网络节点来执行自动网络取证。该模型在关键评估指标方面优于现有的最先进方法。 |
| [^170] | [Classification and Generation of real-world data with an Associative Memory Model.](http://arxiv.org/abs/2207.04827) | 本文提出了一种基于联想记忆模型的多模态框架，可以以容错的方式存储和检索大量真实世界数据，并且可以用于推断缺失的模态。 |
| [^171] | [Multi-Frequency Joint Community Detection and Phase Synchronization.](http://arxiv.org/abs/2206.12276) | 本文提出了两种简单而高效的算法，利用MLE公式并从多个频率的信息中受益，用于解决具有相对相位的随机块模型上的联合社区检测和相位同步问题。 |
| [^172] | [Max-Margin Works while Large Margin Fails: Generalization without Uniform Convergence.](http://arxiv.org/abs/2206.07892) | 本文证明了在UC失败的情况下，最大间隔分类器可以实现几乎没有测试损失的泛化，提供了新的泛化界限。 |
| [^173] | [Gradient Boosting Performs Gaussian Process Inference.](http://arxiv.org/abs/2206.05608) | 本文表明，基于对称决策树的梯度提升可以等价地重构为一种核方法，该方法收敛于某个核岭回归问题的解，从而使我们能够轻松地将梯度提升转换为从后验中提供更好的知识不确定性估计的采样器，通过后验方差的蒙特卡罗估计，从而允许更好的知识不确定性估计，导致改进的域外检测。 |
| [^174] | [Bootstrapping Semi-supervised Medical Image Segmentation with Anatomical-aware Contrastive Distillation.](http://arxiv.org/abs/2206.02307) | 本文提出了一种基于解剖感知对比蒸馏的半监督医学图像分割引导启动方法，通过软标记负样本和捕获更多语义上相似的特征来解决医学图像数据不平衡的问题。 |
| [^175] | [Optimization with access to auxiliary information.](http://arxiv.org/abs/2206.00395) | 本文研究了在计算目标函数梯度很昂贵或有限的情况下，给定一些辅助函数的情况下，如何最小化目标函数。作者提出了两种通用的新算法，并证明了这个框架可以受益于目标和辅助信息之间的Hessian相似性假设。 |
| [^176] | [DM$^2$: Decentralized Multi-Agent Reinforcement Learning for Distribution Matching.](http://arxiv.org/abs/2206.00233) | 本文提出了一种基于分布匹配的去中心化多智能体强化学习方法，每个智能体独立地最小化与目标访问分布的相应分量的分布不匹配，可以实现收敛到生成目标分布的联合策略。 |
| [^177] | [Maximum Mean Discrepancy on Exponential Windows for Online Change Detection.](http://arxiv.org/abs/2205.12706) | 本文提出了一种基于指数窗口的最大均值差异在线变化检测算法，能够有效地检测数据流中的变化。 |
| [^178] | [Adversarial random forests for density estimation and generative modeling.](http://arxiv.org/abs/2205.09435) | 本文提出了一种使用对抗随机森林进行密度估计和数据合成的方法，该方法可以提供平滑的（非）条件密度，并允许完全合成数据生成，同时在各种表格数据基准测试中实现了与最先进的概率电路和深度学习模型相当或更好的性能，平均执行速度快了两个数量级。 |
| [^179] | [Visuomotor Control in Multi-Object Scenes Using Object-Aware Representations.](http://arxiv.org/abs/2205.06333) | 本文探讨了使用物体感知表示学习技术进行机器人任务的有效性，以解决当前方法学习任务特定表示不能很好地转移到其他任务的问题，以及由监督方法学习的表示需要大量标记数据集的问题。 |
| [^180] | [Matrix and graph representations of vine copula structures.](http://arxiv.org/abs/2205.04783) | 本文研究了Vine copula结构的矩阵和图形表示，证明了它们之间的等价性，并提出了一种新的方法来构建矩阵。这些算法的运行时间也被计算了。 |
| [^181] | [Non-Stationary Bandit Learning via Predictive Sampling.](http://arxiv.org/abs/2205.01970) | 本文提出了一种预测抽样算法，用于解决非平稳赌博机学习问题。该算法通过降低获取信息的优先级，解决了Thompson抽样在非平稳环境下表现不佳的问题，并在所有非平稳环境中优于Thompson抽样。 |
| [^182] | [Short-Term Density Forecasting of Low-Voltage Load using Bernstein-Polynomial Normalizing Flows.](http://arxiv.org/abs/2204.13939) | 本文提出了一种基于伯恩斯坦多项式归一化流的灵活条件密度预测方法，用于短期低压负荷预测，相比传统方法表现更好，可用于规划和运营低碳能源系统。 |
| [^183] | [Representation Learning by Detecting Incorrect Location Embeddings.](http://arxiv.org/abs/2204.04788) | 本文提出了一种新的自监督学习（SSL）损失，用于图像表示学习。通过检测错误的位置嵌入，我们可以提高深度神经网络的泛化能力，使其更加鲁棒。我们称这种方法为DILEMMA，将其应用于MoCoV3、DINO和SimCLR，分别显示它们的性能提高了4.41%、3.97%和0.5%。 |
| [^184] | [DynLight: Realize dynamic phase duration with multi-level traffic signal control.](http://arxiv.org/abs/2204.03471) | 本文已被撤回，原因是语言和理论描述不够令人满意，作者已经进行了修订和更新。 |
| [^185] | [Deep Feature Screening: Feature Selection for Ultra High-Dimensional Data via Deep Neural Networks.](http://arxiv.org/abs/2204.01682) | 本文提出了一种新的两步非参数方法，称为深度特征筛选（DeepFS），可以克服高维、低样本数据上的困难和挑战，并对超高维、低样本数据进行高精度的特征筛选。 |
| [^186] | [Separate and conquer heuristic allows robust mining of contrast sets in classification, regression, and survival data.](http://arxiv.org/abs/2204.00497) | 本文提出了一种基于分离征服的对比集挖掘算法RuleKit-CS，该算法通过多次通过伴随属性惩罚方案提供描述具有不同属性的相同示例的对比集，区别于标准的分离征服。该算法还被推广到回归和生存数据，允许识别标签属性/生存预测与预定义对比组的标签/预测一致的对比集。 |
| [^187] | [Generative Modeling Helps Weak Supervision (and Vice Versa).](http://arxiv.org/abs/2203.12023) | 本文提出了一种融合程序弱监督和生成对抗网络的模型，通过对齐离散潜在变量和弱监督派生的标签估计，改善了未观察到的标签的估计，实现了数据增强。 |
| [^188] | [Effects of Epileptiform Activity on Discharge Outcome in Critically Ill Patients.](http://arxiv.org/abs/2203.04920) | 本研究旨在探讨癫痫样活动对危重病患者出院结局的影响，通过回顾性横断面研究发现，如果每个人都经历了某种EA负荷并且未接受治疗，出院mRS会发生变化。 |
| [^189] | [Towards Targeted Change Detection with Heterogeneous Remote Sensing Images for Forest Mortality Mapping.](http://arxiv.org/abs/2203.00049) | 本文提出了一种基于图像到图像转换和单类分类的新方法，用于检测生态系统某些干扰的弱信号，以绘制由几何蛾爆发引起的森林死亡率在稀疏森林-苔原生态过渡带中的地图。 |
| [^190] | [Random Laplacian Features for Learning with Hyperbolic Space.](http://arxiv.org/abs/2202.06854) | 本文提出了一种简单的方法，通过学习输入的超几何嵌入，使用一种编码几何先验的映射将其映射到欧几里得空间，并最终使用标准欧几里得网络。关键的洞见是使用拉普拉斯算子的特征函数的随机特征映射，我们展示了它可以近似超几何空间上的任何等度量不变核。 |
| [^191] | [PGMax: Factor Graphs for Discrete Probabilistic Graphical Models and Loopy Belief Propagation in JAX.](http://arxiv.org/abs/2202.04110) | PGMax是一个用于离散概率图模型的因子图工具，可以在JAX中自动运行高效且可扩展的循环置信传播，与现有替代方案相比，PGMax获得了更高质量的推理结果，推理时间加速高达三个数量级。 |
| [^192] | [Learning Similarity Metrics for Volumetric Simulations with Multiscale CNNs.](http://arxiv.org/abs/2202.04109) | 本文提出了一种基于熵的相似度模型，用于评估基于运输和运动的模拟产生的标量和矢量数据的相似度，并提出了一种多尺度CNN架构，用于计算体积相似度度量（VolSiM）。 |
| [^193] | [Is the Performance of My Deep Network Too Good to Be True? A Direct Approach to Estimating the Bayes Error in Binary Classification.](http://arxiv.org/abs/2202.00395) | 本文提出了一种简单直接的贝叶斯误差估计器，可以用于评估具有最先进性能的分类器的标准，并可用于检测测试集过拟合。与其他方法相比，我们的方法是无模型的，甚至是无实例的。此外，它没有超参数，并且在实证上比几个基线给出了更准确的贝叶斯误差估计。 |
| [^194] | [State-Conditioned Adversarial Subgoal Generation.](http://arxiv.org/abs/2201.09635) | 本文提出了一种新的分层强化学习方法，通过对抗性学习来减轻高层策略不稳定的问题，提高了学习效率和性能。 |
| [^195] | [Landslide Susceptibility Modeling by Interpretable Neural Network.](http://arxiv.org/abs/2201.06837) | 本文介绍了一种可解释神经网络（SNN）优化框架，用于评估滑坡易发性。SNN模型发现坡度和降水的乘积以及坡向是高滑坡易发性的重要主要贡献因素。 |
| [^196] | [Egeria: Efficient DNN Training with Knowledge-Guided Layer Freezing.](http://arxiv.org/abs/2201.06227) | Egeria是一种基于知识引导的DNN训练系统，通过跳过DNN层的计算和通信来实现高效训练，利用参考模型中的语义知识准确评估单个层的训练可塑性，并安全地冻结已收敛的层，从而节省相应的反向计算和通信。 |
| [^197] | [PRONTO: Preamble Overhead Reduction with Neural Networks for Coarse Synchronization.](http://arxiv.org/abs/2112.10885) | 本文提出了一种名为PRONTO的基于神经网络的方案，用于在WiFi基础波形中执行粗略的时间和频率同步，以减少前导开销。该方案通过消除传统短训练场（L-STF）来缩短前导长度，并使用其他前导字段（特别是传统的长训练场（L-LTF））执行估计。 |
| [^198] | [Two-view Graph Neural Networks for Knowledge Graph Completion.](http://arxiv.org/abs/2112.09231) | 本文提出了一种名为WGE的图神经网络模型，通过两个单一的实体和关系为中心的图来学习实体和关系的向量表示，并在知识图谱补全任务上取得了优异的性能。 |
| [^199] | [Location Leakage in Federated Signal Maps.](http://arxiv.org/abs/2112.03452) | 本文研究了在联邦学习框架下，通过梯度泄漏攻击推断用户位置的问题，并提出了一种保护位置隐私的方法。 |
| [^200] | [FLSys: Toward an Open Ecosystem for Federated Learning Mobile Apps.](http://arxiv.org/abs/2111.09445) | 本文介绍了FLSys，一个移动-云联邦学习（FL）系统，可以成为FL模型和应用程序开放生态系统的关键组成部分。FLSys旨在在智能手机上使用移动感测数据。它平衡了模型性能和资源消耗，容忍通信故障，并实现了可扩展性。FLSys提供了先进的隐私保护机制和一个通用的API，供第三方应用程序开发人员访问FL模型。 |
| [^201] | [A Unified and Fast Interpretable Model for Predictive Analytics.](http://arxiv.org/abs/2111.08255) | 本文提出了一种名为FXAM的统一且快速可解释的预测分析模型，通过统一的可加模型扩展了GAM的建模能力，用于数值、分类和时间特征。FXAM进行了一种新颖的训练过程，称为三阶段迭代（TSI），分别对数值、分类和时间特征进行学习，具有较高的准确性和训练效率。 |
| [^202] | [Exploiting Action Impact Regularity and Exogenous State Variables for Offline Reinforcement Learning.](http://arxiv.org/abs/2111.08066) | 本文提出了一种利用行动影响规律和外生状态变量进行离线强化学习的算法，该算法在现实世界的许多领域中成立，包括金融市场，并在模拟和真实世界中的不同数据收集策略中优于现有的离线强化学习算法。 |
| [^203] | [How I Learned to Stop Worrying and Love Retraining.](http://arxiv.org/abs/2111.00843) | 本文提出了一种简单的线性学习率时间表，可以大大缩短重新训练阶段，同时提出了一种方法来自适应地选择线性时间表的初始值，并对初始密集训练阶段施加预算，从而改进了现有的重新训练方法。 |
| [^204] | [Holistic Deep Learning.](http://arxiv.org/abs/2110.15829) | 本文提出了一种全面深度学习框架，通过解决输入扰动的脆弱性、过度参数化和性能不稳定性等挑战，全面提高了准确性、鲁棒性、稀疏性和稳定性，适用于表格和图像数据集。提供了选择适当的训练损失函数的建议。 |
| [^205] | [ProxyBO: Accelerating Neural Architecture Search via Bayesian Optimization with Zero-cost Proxies.](http://arxiv.org/abs/2110.10423) | 本文提出了一种名为ProxyBO的高效贝叶斯优化框架，利用零成本代理加速神经架构搜索，通过广义能力测量估计代理在任务上的适应性，并设计了一种新的收购函数，基于它们的动态影响将BO与零成本代理相结合，实验证明ProxyBO在五个任务上始终优于竞争基线。 |
| [^206] | [Why Adversarial Reprogramming Works, When It Fails, and How to Tell the Difference.](http://arxiv.org/abs/2108.11673) | 对抗性重编程可以重新利用机器学习模型执行不同的任务，成功取决于平均输入梯度的大小，当输入梯度更加对齐且输入具有更高的维度时，平均输入梯度会增长。 |
| [^207] | [Adversarially Regularized Graph Attention Networks for Inductive Learning on Partially Labeled Graphs.](http://arxiv.org/abs/2106.03393) | 本文提出了一种对抗正则化图注意力模型，用于在部分标记的图中分类新添加的节点，通过聚合其相邻节点的信息生成节点的表示，从而自然地推广到以前未见过的节点。 |
| [^208] | [Data-Driven Reachability Analysis from Noisy Data.](http://arxiv.org/abs/2105.07229) | 本文提出了一种从嘈杂的数据中计算可达集的算法，适用于不同类型的系统，包括线性、多项式和非线性系统。算法基于矩阵zonotope，可以提供较少保守的可达集，并且可以将关于未知系统模型的先前知识纳入计算。算法具有理论保证，并在多个数值示例和实际实验中得到了验证。 |
| [^209] | [NOMU: Neural Optimization-based Model Uncertainty.](http://arxiv.org/abs/2102.13640) | NOMU是一种新的神经网络模型，可以在无噪声设置下，通过设计一个由两个连接的子NN组成的网络架构，并使用精心设计的损失函数进行训练，来捕捉NN的模型不确定性。该模型满足五个关于模型不确定性的重要愿望。 |
| [^210] | [The Disparate Impact of Uncertainty: Affirmative Action vs. Affirmative Information.](http://arxiv.org/abs/2102.10019) | 本文证明了不确定性对不同人群的影响是不平等的，虽然它会在所有人口群体中产生误差，但误差的类型会有系统性的变化。我们提出了一种名为平权信息的策略，可以消除这种差异并扩大机会的获取，这可以作为平权行动的替代方案。 |
| [^211] | [SoK: Training Machine Learning Models over Multiple Sources with Privacy Preservation.](http://arxiv.org/abs/2012.03386) | 本文综述了隐私保护下多源训练机器学习模型的两种主流解决方案：安全多方学习和联邦学习，并对它们的安全性、效率、数据分布、训练模型的准确性和应用场景进行了比较和讨论，同时探讨了未来的研究方向。 |
| [^212] | [LOCUS: A Novel Decomposition Method for Brain Network Connectivity Matrices using Low-rank Structure with Uniform Sparsity.](http://arxiv.org/abs/2008.08915) | LOCUS是一种新的大脑网络连接矩阵分解方法，使用低秩结构和均匀稀疏性，能够更有效和准确地分离连接矩阵源，有望成为理解大脑组织的关键。 |
| [^213] | [FRMDN: Flow-based Recurrent Mixture Density Network.](http://arxiv.org/abs/2008.02144) | 本文提出了一种基于流的循环混合密度网络，通过在每个时间步上定义一个高斯混合模型，将循环混合密度网络推广到非线性变换的目标序列上，该模型在图像序列的拟合度上表现显著，具有显著的建模能力，在对数似然度量方面优于其他最先进的方法。 |
| [^214] | [Anomaly Awareness.](http://arxiv.org/abs/2007.14462) | 该论文提出了一种新的异常检测算法，称为异常感知。该算法通过修改成本函数来学习正常事件，并了解异常事件。该方法在不同的粒子物理情况和标准计算机视觉任务中的应用，能够有效地识别以前未见过的异常，并在了解足够多的异常时变得更加稳健。 |
| [^215] | [COMET: Convolutional Dimension Interaction for Collaborative Filtering.](http://arxiv.org/abs/2007.14129) | COMET是一种新的基于表示学习的模型，它可以同时模拟历史交互和嵌入维度之间的高阶交互模式。 |
| [^216] | [An FPGA-Based On-Device Reinforcement Learning Approach using Online Sequential Learning.](http://arxiv.org/abs/2005.04646) | 本文提出了一种基于FPGA的轻量级强化学习方法，利用OS-ELM算法进行训练，避免了DQN需要大量缓冲区和批处理的问题，并通过L2正则化和谱归一化的组合使得强化学习更加稳定。 |
| [^217] | [Feature-Based Interpolation and Geodesics in the Latent Spaces of Generative Models.](http://arxiv.org/abs/1904.03445) | 本文提出了一种通用和统一的插值方法，它同时允许我们在任意密度的情况下搜索测地线和插值曲线。最大化曲线的质量度量可以等价地理解为在空间上某种重新定义的黎曼度量下搜索测地线。 |
| [^218] | [Improving SGD convergence by online linear regression of gradients in multiple statistically relevant directions.](http://arxiv.org/abs/1901.11457) | 本文提出了一种在线线性回归的方法，通过在多个统计相关方向上进行梯度的回归来提高SGD的收敛性，解决了标准方法只考虑单个方向的问题，同时避免了二阶方法的成本和数值不稳定性。 |

# 详细

[^1]: 自动设计元启发式算法的综述

    A Survey on Automated Design of Metaheuristic Algorithms. (arXiv:2303.06532v1 [cs.NE])

    [http://arxiv.org/abs/2303.06532](http://arxiv.org/abs/2303.06532)

    本文综述了自动设计元启发式算法的形式化、方法论、挑战和研究趋势，讨论了自动设计的潜在未来方向和开放问题。

    This paper presents a broad picture of the formalization, methodologies, challenges, and research trends of automated design of metaheuristic algorithms, and discusses the potential future directions and open issues in this field.

    元启发式算法由于其能够独立于问题结构和问题领域进行搜索的能力，已经引起了学术界和工业界的广泛关注。通常，需要人类专家手动调整算法以适应解决目标问题。手动调整过程可能是费力的、容易出错的，并且需要大量的专业知识。这引起了对自动设计元启发式算法的越来越多的兴趣和需求，以减少人类干预。自动设计可以使高性能算法对更广泛的研究人员和实践者可用；通过利用计算能力来充分探索潜在的设计选择，自动设计可以达到甚至超过人类水平的设计。本文通过对现有工作的共同点和差异进行调查，提出了自动设计元启发式算法的形式化、方法论、挑战和研究趋势的广泛概述。我们还讨论了这一领域的潜在未来方向和开放问题。

    Metaheuristic algorithms have attracted wide attention from academia and industry due to their capability of conducting search independent of problem structures and problem domains. Often, human experts are requested to manually tailor algorithms to fit for solving a targeted problem. The manual tailoring process may be laborious, error-prone, and require intensive specialized knowledge. This gives rise to increasing interests and demands for automated design of metaheuristic algorithms with less human intervention. The automated design could make high-performance algorithms accessible to a much broader range of researchers and practitioners; and by leveraging computing power to fully explore the potential design choices, automated design could reach or even surpass human-level design. This paper presents a broad picture of the formalization, methodologies, challenges, and research trends of automated design of metaheuristic algorithms, by conducting a survey on the common grounds and 
    
[^2]: 在联邦深度学习中优化批标准化

    Making Batch Normalization Great in Federated Deep Learning. (arXiv:2303.06530v1 [cs.LG])

    [http://arxiv.org/abs/2303.06530](http://arxiv.org/abs/2303.06530)

    本文研究了在联邦学习中使用批标准化和群组归一化的效果，发现在适当的处理下，批标准化可以在广泛的联邦学习设置中具有很高的竞争力，而且这不需要额外的训练或通信成本。

    This paper studies the use of batch normalization and group normalization in federated learning, and finds that with proper treatments, batch normalization can be highly competitive across a wide range of federated learning settings, and this requires no additional training or communication costs.

    批标准化（BN）通常用于现代深度神经网络（DNN）中，以提高稳定性并加速集中式训练的收敛速度。在具有非IID分散数据的联邦学习（FL）中，先前的研究观察到使用BN进行训练可能会由于训练和测试之间的BN统计不匹配而阻碍性能。因此，群组归一化（GN）更常用于FL作为BN的替代方法。然而，通过我们在各种FL设置下的实证研究，我们发现BN和GN之间没有一致的优胜者。这促使我们重新审视FL中归一化层的使用。我们发现，在适当的处理下，BN可以在广泛的FL设置中具有很高的竞争力，而且这不需要额外的训练或通信成本。我们希望我们的研究可以成为FL未来实际使用和理论分析的有价值参考。

    Batch Normalization (BN) is commonly used in modern deep neural networks (DNNs) to improve stability and speed up convergence during centralized training. In federated learning (FL) with non-IID decentralized data, previous works observed that training with BN could hinder performance due to the mismatch of the BN statistics between training and testing. Group Normalization (GN) is thus more often used in FL as an alternative to BN. However, from our empirical study across various FL settings, we see no consistent winner between BN and GN. This leads us to revisit the use of normalization layers in FL. We find that with proper treatments, BN can be highly competitive across a wide range of FL settings, and this requires no additional training or communication costs. We hope that our study could serve as a valuable reference for future practical usage and theoretical analysis in FL.
    
[^3]: 数据相关的在线学习算法框架

    Data Dependent Regret Guarantees Against General Comparators for Full or Bandit Feedback. (arXiv:2303.06526v1 [cs.LG])

    [http://arxiv.org/abs/2303.06526](http://arxiv.org/abs/2303.06526)

    该论文提出了一个数据相关的在线学习算法框架，可以在全专家反馈和Bandit反馈设置中具有数据相关的遗憾保证，适用于各种问题场景。

    This paper proposes a data-dependent online learning algorithm framework that has data-dependent regret guarantees in both full expert feedback and bandit feedback settings, applicable for a wide variety of problem scenarios.

    我们研究了对抗性在线学习问题，并创建了一个完全在线的算法框架，具有在全专家反馈和Bandit反馈设置中具有数据相关的遗憾保证。我们研究了我们的算法对一般比较器的预期性能，使其适用于各种问题场景。我们的算法从通用预测角度工作，使用的性能度量是对任意比较器序列的预期遗憾，即我们的损失与竞争损失序列之间的差异。竞争类可以设计为包括固定臂选择、切换Bandit、上下文Bandit、周期Bandit或任何其他感兴趣的竞争。竞争类中的序列通常由具体应用程序确定，并应相应地设计。我们的算法既不使用也不需要任何有关损失序列的初步信息，完全在线。其

    We study the adversarial online learning problem and create a completely online algorithmic framework that has data dependent regret guarantees in both full expert feedback and bandit feedback settings. We study the expected performance of our algorithm against general comparators, which makes it applicable for a wide variety of problem scenarios. Our algorithm works from a universal prediction perspective and the performance measure used is the expected regret against arbitrary comparator sequences, which is the difference between our losses and a competing loss sequence. The competition class can be designed to include fixed arm selections, switching bandits, contextual bandits, periodic bandits or any other competition of interest. The sequences in the competition class are generally determined by the specific application at hand and should be designed accordingly. Our algorithm neither uses nor needs any preliminary information about the loss sequences and is completely online. Its
    
[^4]: 使用学习的条件概率模型进行无损点云几何和属性压缩

    Lossless Point Cloud Geometry and Attribute Compression Using a Learned Conditional Probability Model. (arXiv:2303.06519v1 [eess.IV])

    [http://arxiv.org/abs/2303.06519](http://arxiv.org/abs/2303.06519)

    本文提出了一种使用稀疏张量深度神经网络学习点云几何和颜色概率分布的高效无损点云压缩方法，具有更高的压缩比和更快的压缩速度。

    This paper proposes an efficient lossless point cloud compression method that uses sparse tensor-based deep neural networks to learn point cloud geometry and color probability distributions, achieving higher compression ratio and faster compression speed compared to the state-of-the-art method from Moving Pict.

    近年来，我们在生活的许多方面都见证了点云数据的存在，从沉浸式媒体、自动驾驶到医疗保健，但代价是巨大的数据量。本文提出了一种高效的无损点云压缩方法，使用稀疏张量深度神经网络学习点云几何和颜色概率分布。我们的方法使用统一的稀疏表示将点云表示为具有不同位深度的占用特征和三个属性特征。这使我们能够使用稀疏张量神经网络有效地利用点云内的特征和点内依赖关系，从而为算术编码器构建准确的自回归上下文模型。据我们所知，这是第一个基于学习的无损点云几何和属性压缩方法。与Moving Pict的最新无损点云压缩方法相比，我们的方法在保持无损压缩的同时，具有更高的压缩比和更快的压缩速度。

    In recent years, we have witnessed the presence of point cloud data in many aspects of our life, from immersive media, autonomous driving to healthcare, although at the cost of a tremendous amount of data. In this paper, we present an efficient lossless point cloud compression method that uses sparse tensor-based deep neural networks to learn point cloud geometry and color probability distributions. Our method represents a point cloud with both occupancy feature and three attribute features at different bit depths in a unified sparse representation. This allows us to efficiently exploit feature-wise and point-wise dependencies within point clouds using a sparse tensor-based neural network and thus build an accurate auto-regressive context model for an arithmetic coder. To the best of our knowledge, this is the first learning-based lossless point cloud geometry and attribute compression approach. Compared with the-state-of-the-art lossless point cloud compression method from Moving Pict
    
[^5]: 深度概率模型用于无损可扩展点云属性压缩

    Deep probabilistic model for lossless scalable point cloud attribute compression. (arXiv:2303.06517v1 [eess.IV])

    [http://arxiv.org/abs/2303.06517](http://arxiv.org/abs/2303.06517)

    本文提出了一种利用深度概率模型进行无损可扩展点云属性压缩的方法，通过多尺度架构提供准确的上下文，从而最小化编码比特率，同时允许从无损压缩的比特流中轻松提取较低质量的版本。该方法在实验中表现优于最近提出的方法，并与最新的G-PCC版本14相当，且编码时间更快。

    This paper proposes a deep probabilistic model for lossless scalable point cloud attribute compression, which utilizes a multiscale architecture to provide accurate context for attribute probability modeling and allows for easily extracting lower quality versions from the losslessly compressed bitstream. The method outperforms recently proposed methods and is on par with the latest G-PCC version 14, with substantially faster coding time.

    近年来，已经提出了几种利用先进的深度学习技术的点云几何压缩方法，但是关于属性压缩，特别是无损压缩的工作还很有限。在这项工作中，我们构建了一种端到端的多尺度点云属性编码方法（MNeT），该方法逐步将属性投影到多尺度潜在空间上。多尺度架构为属性概率建模提供了准确的上下文，从而通过单个网络预测最小化编码比特率。此外，我们的方法允许可扩展编码，可以从无损压缩的比特流中轻松提取较低质量的版本。我们在来自MVUB和MPEG的一组点云上验证了我们的方法，并表明我们的方法优于最近提出的方法，并与最新的G-PCC版本14相当。此外，我们的编码时间比G-PCC快得多。

    In recent years, several point cloud geometry compression methods that utilize advanced deep learning techniques have been proposed, but there are limited works on attribute compression, especially lossless compression. In this work, we build an end-to-end multiscale point cloud attribute coding method (MNeT) that progressively projects the attributes onto multiscale latent spaces. The multiscale architecture provides an accurate context for the attribute probability modeling and thus minimizes the coding bitrate with a single network prediction. Besides, our method allows scalable coding that lower quality versions can be easily extracted from the losslessly compressed bitstream. We validate our method on a set of point clouds from MVUB and MPEG and show that our method outperforms recently proposed methods and on par with the latest G-PCC version 14. Besides, our coding time is substantially faster than G-PCC.
    
[^6]: 打开神经网络分类器以计算Shap分数

    Opening Up the Neural Network Classifier for Shap Score Computation. (arXiv:2303.06516v1 [cs.AI])

    [http://arxiv.org/abs/2303.06516](http://arxiv.org/abs/2303.06516)

    本文提出了一种高效计算机器学习模型分类中Shap解释分数的方法，通过将二进制神经网络转换为布尔电路，并使用知识编译技术，将电路视为开放式模型，通过最近的高效算法计算Shap分数，相比于将BNN视为黑盒模型直接计算Shap，性能有了显著的提高。

    This paper proposes an efficient method for computing Shap explanation scores in machine learning model classification by transforming binary neural networks into Boolean circuits and treating the resulting circuit as an open-box model, which leads to a significant improvement in performance compared to computing Shap directly on the BNN treated as a black-box model.

    我们解决了使用机器学习模型进行分类的Shap解释分数的高效计算问题。为此，我们展示了将二进制神经网络（BNN）转换为确定性和可分解的布尔电路，使用知识编译技术。所得到的电路被视为开放式模型，通过最近的高效算法计算Shap分数。详细的实验表明，与将BNN视为黑盒模型直接计算Shap相比，性能有了显著的提高。

    We address the problem of efficiently computing Shap explanation scores for classifications with machine learning models. With this goal, we show the transformation of binary neural networks (BNNs) for classification into deterministic and decomposable Boolean circuits, for which knowledge compilation techniques are used. The resulting circuit is treated as an open-box model, to compute Shap scores by means of a recent efficient algorithm for this class of circuits. Detailed experiments show a considerable gain in performance in comparison with computing Shap directly on the BNN treated as a black-box model.
    
[^7]: 基于核的多阶段随机优化

    Multistage Stochastic Optimization via Kernels. (arXiv:2303.06515v1 [math.OC])

    [http://arxiv.org/abs/2303.06515](http://arxiv.org/abs/2303.06515)

    我们提出了一种基于核的多阶段随机优化方法，能够在多维设置中表现良好，并且在数据规模较大时仍然可行。

    

    我们提出了一种非参数、数据驱动、可行的方法来解决多阶段随机优化问题，其中决策不影响不确定性。所提出的框架将决策变量表示为再生核希尔伯特空间的元素，并执行函数随机梯度下降来最小化经验正则化损失。通过结合基于函数子空间投影的稀疏化技术，我们能够克服标准核方法引入的计算复杂度随着数据大小的增加而增加的问题。我们证明了所提出的方法在具有辅助信息的多阶段随机优化中是渐近最优的。在各种随机库存管理问题的计算实验中，我们的方法在多维设置中表现良好，并且在数据规模较大时仍然可行。最后，通过计算库存控制问题的最优损失的下界，我们展示了所提出的方法的有效性。

    We develop a non-parametric, data-driven, tractable approach for solving multistage stochastic optimization problems in which decisions do not affect the uncertainty. The proposed framework represents the decision variables as elements of a reproducing kernel Hilbert space and performs functional stochastic gradient descent to minimize the empirical regularized loss. By incorporating sparsification techniques based on function subspace projections we are able to overcome the computational complexity that standard kernel methods introduce as the data size increases. We prove that the proposed approach is asymptotically optimal for multistage stochastic optimization with side information. Across various computational experiments on stochastic inventory management problems, {our method performs well in multidimensional settings} and remains tractable when the data size is large. Lastly, by computing lower bounds for the optimal loss of the inventory control problem, we show that the propo
    
[^8]: 使用机器学习模型检测软件定义网络中的DDoS攻击

    Detection of DDoS Attacks in Software Defined Networking Using Machine Learning Models. (arXiv:2303.06513v1 [cs.LG])

    [http://arxiv.org/abs/2303.06513](http://arxiv.org/abs/2303.06513)

    本文研究了使用机器学习算法在软件定义网络（SDN）环境中检测分布式拒绝服务（DDoS）攻击的有效性，通过测试四种算法，其中随机森林算法表现最佳。

    This paper investigates the effectiveness of using machine learning algorithms to detect distributed denial-of-service (DDoS) attacks in software-defined networking (SDN) environments, and tests four algorithms on the CICDDoS2019 dataset, with Random Forest performing the best.

    软件定义网络（SDN）的概念代表了一种现代的网络方法，通过网络抽象将控制平面与数据平面分离，从而实现与传统网络相比更灵活、可编程和动态的架构。控制平面和数据平面的分离导致了高度的网络弹性，但也带来了新的安全风险，包括分布式拒绝服务（DDoS）攻击的威胁，这在SDN环境中构成了新的挑战。本文研究了使用机器学习算法在软件定义网络（SDN）环境中检测分布式拒绝服务（DDoS）攻击的有效性。在CICDDoS2019数据集上测试了四种算法，包括随机森林、决策树、支持向量机和XGBoost，其中时间戳特征被删除等。通过准确率、召回率、准确率和F1分数等指标评估了性能，其中随机森林算法表现最佳。

    The concept of Software Defined Networking (SDN) represents a modern approach to networking that separates the control plane from the data plane through network abstraction, resulting in a flexible, programmable and dynamic architecture compared to traditional networks. The separation of control and data planes has led to a high degree of network resilience, but has also given rise to new security risks, including the threat of distributed denial-of-service (DDoS) attacks, which pose a new challenge in the SDN environment. In this paper, the effectiveness of using machine learning algorithms to detect distributed denial-of-service (DDoS) attacks in software-defined networking (SDN) environments is investigated. Four algorithms, including Random Forest, Decision Tree, Support Vector Machine, and XGBoost, were tested on the CICDDoS2019 dataset, with the timestamp feature dropped among others. Performance was assessed by measures of accuracy, recall, accuracy, and F1 score, with the Rando
    
[^9]: 通过超球统一性差填补神经坍塌的泛化和解耦

    Generalizing and Decoupling Neural Collapse via Hyperspherical Uniformity Gap. (arXiv:2303.06484v1 [cs.LG])

    [http://arxiv.org/abs/2303.06484](http://arxiv.org/abs/2303.06484)

    本文提出了一个广义神经坍塌假设，有效地包含了原始神经坍塌，并将其分解为两个目标：最小化类内变异性和最大化类间可分性。使用超球统一性作为量化这两个目标的统一框架，并提出了一个通用目标——超球统一性差（HUG），它由类间和类内超球统一性之间的差异定义。

    This paper proposes a generalized neural collapse hypothesis that effectively subsumes the original neural collapse and decomposes it into two objectives: minimizing intra-class variability and maximizing inter-class separability. The authors use hyperspherical uniformity as a unified framework to quantify these objectives and propose a general objective, hyperspherical uniformity gap (HUG), which is defined by the difference between inter-class and intra-class hyperspherical uniformity.

    神经坍塌现象描述了深度神经网络的底层几何对称性，其中深度学习的特征和分类器都收敛于一个等角紧框架。已经证明，交叉熵损失和均方误差都可以导致神经坍塌。我们消除了神经坍塌对特征维度和类别数量的关键假设，然后提出了一个广义神经坍塌假设，有效地包含了原始神经坍塌。受神经坍塌描述神经网络训练目标的启发，我们将广义神经坍塌分解为两个目标：最小化类内变异性和最大化类间可分性。然后，我们使用超球统一性（它描述了单位超球上均匀性的程度）作为量化这两个目标的统一框架。最后，我们提出了一个通用目标——超球统一性差（HUG），它由类间和类内超球统一性之间的差异定义。

    The neural collapse (NC) phenomenon describes an underlying geometric symmetry for deep neural networks, where both deeply learned features and classifiers converge to a simplex equiangular tight frame. It has been shown that both cross-entropy loss and mean square error can provably lead to NC. We remove NC's key assumption on the feature dimension and the number of classes, and then present a generalized neural collapse (GNC) hypothesis that effectively subsumes the original NC. Inspired by how NC characterizes the training target of neural networks, we decouple GNC into two objectives: minimal intra-class variability and maximal inter-class separability. We then use hyperspherical uniformity (which characterizes the degree of uniformity on the unit hypersphere) as a unified framework to quantify these two objectives. Finally, we propose a general objective -- hyperspherical uniformity gap (HUG), which is defined by the difference between inter-class and intra-class hyperspherical un
    
[^10]: 高效训练序列的知识蒸馏

    Knowledge Distillation for Efficient Sequences of Training Runs. (arXiv:2303.06480v1 [cs.LG])

    [http://arxiv.org/abs/2303.06480](http://arxiv.org/abs/2303.06480)

    本文研究了如何利用先前运行中的计算来减少未来运行成本的问题，使用知识蒸馏（KD），通过将未来运行与来自先前运行的KD相结合，可以显著减少训练这些模型所需的时间，KD的开销降低了80-90％，对准确性影响很小，并在整体成本方面实现了巨大的帕累托改进。

    This paper studies how to reduce the cost of future runs by utilizing the computation invested in previous runs using knowledge distillation (KD). Augmenting future runs with KD from previous runs dramatically reduces the time necessary to train these models, and the overhead of KD can be reduced by 80-90% with minimal effect on accuracy, resulting in vast pareto-improvements in overall cost.

    在许多实际场景中，如超参数搜索或使用新数据进行持续重新训练，相关的训练运行会按顺序执行多次。目前的做法是从头开始独立训练每个模型。我们研究了利用先前运行中的计算来减少未来运行成本的问题，使用知识蒸馏（KD）。我们发现，将未来运行与来自先前运行的KD相结合，可以显著减少训练这些模型所需的时间，即使考虑到KD的开销。我们通过两种策略改进了这些结果，将KD的开销降低了80-90％，对准确性影响很小，并在整体成本方面实现了巨大的帕累托改进。我们得出结论，KD是减少实践中训练最终模型之前昂贵的准备工作成本的有前途的途径。

    In many practical scenarios -- like hyperparameter search or continual retraining with new data -- related training runs are performed many times in sequence. Current practice is to train each of these models independently from scratch. We study the problem of exploiting the computation invested in previous runs to reduce the cost of future runs using knowledge distillation (KD). We find that augmenting future runs with KD from previous runs dramatically reduces the time necessary to train these models, even taking into account the overhead of KD. We improve on these results with two strategies that reduce the overhead of KD by 80-90% with minimal effect on accuracy and vast pareto-improvements in overall cost. We conclude that KD is a promising avenue for reducing the cost of the expensive preparatory work that precedes training final models in practice.
    
[^11]: 深度神经网络时代的肿瘤多模态数据整合：一篇综述

    Multimodal Data Integration for Oncology in the Era of Deep Neural Networks: A Review. (arXiv:2303.06471v1 [cs.LG])

    [http://arxiv.org/abs/2303.06471](http://arxiv.org/abs/2303.06471)

    本文综述了深度神经网络在多模态数据整合方面的应用，以提高癌症诊断和治疗的准确性和可靠性。

    This review article analyzes the application of deep neural networks in multimodal data integration to improve the accuracy and reliability of cancer diagnosis and treatment.

    癌症在不同尺度、模态和分辨率的获取数据中具有关系信息，例如放射学、病理学、基因组学、蛋白质组学和临床记录。整合多种数据类型可以提高癌症诊断和治疗的准确性和可靠性。可能存在人类或现有技术工具无法视觉上区分的与疾病相关的信息。传统方法通常关注单个尺度的生物系统的部分或单一模态信息，并未涵盖数据异质性的完整光谱。深度神经网络促进了复杂的多模态数据融合方法的发展，可以从多个来源提取和整合相关信息。最近的深度学习框架，如图形神经网络（GNN）和变压器，在多模态学习方面取得了显着的成功。本综述文章提供了对当前多模态数据整合方法的深入分析。

    Cancer has relational information residing at varying scales, modalities, and resolutions of the acquired data, such as radiology, pathology, genomics, proteomics, and clinical records. Integrating diverse data types can improve the accuracy and reliability of cancer diagnosis and treatment. There can be disease-related information that is too subtle for humans or existing technological tools to discern visually. Traditional methods typically focus on partial or unimodal information about biological systems at individual scales and fail to encapsulate the complete spectrum of the heterogeneous nature of data. Deep neural networks have facilitated the development of sophisticated multimodal data fusion approaches that can extract and integrate relevant information from multiple sources. Recent deep learning frameworks such as Graph Neural Networks (GNNs) and Transformers have shown remarkable success in multimodal learning. This review article provides an in-depth analysis of the state-
    
[^12]: 基于前缀树的分子质谱预测

    Prefix-tree Decoding for Predicting Mass Spectra from Molecules. (arXiv:2303.06470v1 [q-bio.QM])

    [http://arxiv.org/abs/2303.06470](http://arxiv.org/abs/2303.06470)

    本文提出了一种基于前缀树的中间策略，通过将质谱视为化学公式的集合来预测分子的质谱，克服了化学子公式的组合可能性。

    This paper proposes an intermediate strategy for predicting mass spectra from molecules by treating mass spectra as sets of chemical formulae, which are themselves multisets of atoms, and decoding the formula set using a prefix tree structure, atom-type by atom-type, overcoming the combinatorial possibilities for chemical subformulae.

    计算预测分子的质谱已经实现了临床相关代谢物的发现。然而，这样的预测工具仍然存在局限性，因为它们占据了两个极端，要么通过过度刚性的约束和较差的时间复杂度组合分子来进行操作，要么通过解码有损和非物理离散化的光谱向量来进行操作。在这项工作中，我们介绍了一种新的中间策略，通过将质谱视为化学公式的集合来预测分子的质谱，这些化学公式本身是原子的多重集合。在首先对输入分子图进行编码后，我们解码一组化学子公式，每个化学子公式指定质谱中的一个预测峰，其强度由第二个模型预测。我们的关键洞察力是通过使用前缀树结构，逐个原子类型地解码公式集，克服了化学子公式的组合可能性。

    Computational predictions of mass spectra from molecules have enabled the discovery of clinically relevant metabolites. However, such predictive tools are still limited as they occupy one of two extremes, either operating (a) by fragmenting molecules combinatorially with overly rigid constraints on potential rearrangements and poor time complexity or (b) by decoding lossy and nonphysical discretized spectra vectors. In this work, we introduce a new intermediate strategy for predicting mass spectra from molecules by treating mass spectra as sets of chemical formulae, which are themselves multisets of atoms. After first encoding an input molecular graph, we decode a set of chemical subformulae, each of which specify a predicted peak in the mass spectra, the intensities of which are predicted by a second model. Our key insight is to overcome the combinatorial possibilities for chemical subformulae by decoding the formula set using a prefix tree structure, atom-type by atom-type, represent
    
[^13]: 基于图神经网络的上下文嵌入在表格数据深度学习中的应用

    Graph Neural Network contextual embedding for Deep Learning on Tabular Data. (arXiv:2303.06455v1 [cs.LG])

    [http://arxiv.org/abs/2303.06455](http://arxiv.org/abs/2303.06455)

    本文介绍了一种基于图神经网络的深度学习模型，使用交互网络进行上下文嵌入，用于表格数据的处理。该模型在公共数据集上的表现优于最近的深度学习基准调查，并且与提升树解决方案相比也取得了竞争性的结果。

    This paper introduces a novel deep learning model based on Graph Neural Network (GNN) with Interaction Network (IN) for contextual embedding, which outperforms the recent DL benchmark on five public datasets and achieves competitive results compared to boosted-tree solutions in tabular data processing.

    所有行业都试图利用现有的大数据进行基于人工智能的应用，这些数据通常以所谓的表格形式存在，其中每个记录由许多异构的连续和分类列组成，也称为特征。深度学习在自然语言处理等与人类技能相关的领域中已经取得了重大突破，但其在表格数据上的应用更具挑战性。更经典的机器学习模型，如基于树的集成模型通常表现更好。本文介绍了一种新颖的深度学习模型，它使用图神经网络（GNN），更具体地说是交互网络（IN），进行上下文嵌入。其结果优于最近发布的基于五个公共数据集的深度学习基准调查，与提升树解决方案相比也取得了竞争性的结果。

    All industries are trying to leverage Artificial Intelligence (AI) based on their existing big data which is available in so called tabular form, where each record is composed of a number of heterogeneous continuous and categorical columns also known as features. Deep Learning (DL) has consituted a major breathrough for AI in fields related to human skills like natural language processing, but its applicability to tabular data has been more challenging. More classical Machine Learning (ML) models like tree-based ensemble ones usually perform better. In this manuscript a novel DL model that uses Graph Neural Network (GNN), more specifically Interaction Network (IN), for contextual embedding is introduced. Its results outperform those of the recently published survey with DL benchmark based on five public datasets, achieving also competitive results when compared to boosted-tree solutions.
    
[^14]: DECOMPL: 一种基于注意力池化的分解学习技术，用于从单个排球图像中识别团体活动

    DECOMPL: Decompositional Learning with Attention Pooling for Group Activity Recognition from a Single Volleyball Image. (arXiv:2303.06439v1 [cs.CV])

    [http://arxiv.org/abs/2303.06439](http://arxiv.org/abs/2303.06439)

    本文提出了一种新的排球视频团体活动识别技术DECOMPL，它由两个互补的分支组成，使用选择性的注意力池化提取特征，考虑参与者的当前配置，并从框坐标中提取空间信息。同时，本文发现排球数据集的标签方案降低了活动中的团体概念。

    This paper proposes a novel GAR technique for volleyball videos, DECOMPL, which consists of two complementary branches, using selective attention pooling to extract features, considering the current configuration of actors and extracting spatial information from box coordinates. The paper also reveals that the labeling scheme of the Volleyball dataset degrades the group concept in activities.

    团体活动识别旨在检测场景中多个参与者执行的活动。先前的工作基于RGB、光流或关键点数据类型对时空特征进行建模。然而，同时使用时间性和这些数据类型会显著增加计算复杂度。我们的假设是，仅使用RGB数据而不考虑时间性，可以在几乎不损失准确性的情况下保持性能。为此，我们提出了一种新的排球视频团体活动识别技术DECOMPL，它由两个互补的分支组成。在视觉分支中，它使用选择性的注意力池化提取特征。在坐标分支中，它考虑参与者的当前配置，并从框坐标中提取空间信息。此外，我们分析了排球数据集，发现其标签方案降低了活动中的团体概念。

    Group Activity Recognition (GAR) aims to detect the activity performed by multiple actors in a scene. Prior works model the spatio-temporal features based on the RGB, optical flow or keypoint data types. However, using both the temporality and these data types altogether increase the computational complexity significantly. Our hypothesis is that by only using the RGB data without temporality, the performance can be maintained with a negligible loss in accuracy. To that end, we propose a novel GAR technique for volleyball videos, DECOMPL, which consists of two complementary branches. In the visual branch, it extracts the features using attention pooling in a selective way. In the coordinate branch, it considers the current configuration of the actors and extracts the spatial information from the box coordinates. Moreover, we analyzed the Volleyball dataset that the recent literature is mostly based on, and realized that its labeling scheme degrades the group concept in the activities to
    
[^15]: 关于深度学习源分离中的神经网络架构：共信道OFDM信号的分离

    On Neural Architectures for Deep Learning-based Source Separation of Co-Channel OFDM Signals. (arXiv:2303.06438v1 [eess.SP])

    [http://arxiv.org/abs/2303.06438](http://arxiv.org/abs/2303.06438)

    本文研究了涉及OFDM信号的单通道源分离问题，通过原型问题评估了使用面向音频的神经网络架构在分离共信道OFDM波形方面的有效性，并提出了关键的领域知识修改网络参数化的解决方案。

    This paper studies the single-channel source separation problem involving OFDM signals and evaluates the efficacy of using audio-oriented neural architectures in separating co-channel OFDM waveforms. Critical domain-informed modifications to the network parameterization are proposed based on insights from OFDM structures.

    本文研究了涉及正交频分复用（OFDM）信号的单通道源分离问题，这种信号在许多现代数字通信系统中普遍存在。在单声道源分离方面已经进行了相关的努力，其中采用了最先进的神经网络架构来训练端到端的音频信号分离器（作为一维时间序列）。通过基于OFDM源模型的原型问题，我们评估并质疑了使用面向音频的神经网络架构在基于通信波形相关特征分离信号方面的有效性。也许令人惊讶的是，我们证明在某些配置中，即使在理论上可以实现完美分离的情况下，这些面向音频的神经网络架构在分离共信道OFDM波形方面表现不佳。然而，我们提出了关键的领域知识修改网络参数化，基于OFDM结构的洞察，可以共同解决这个问题。

    We study the single-channel source separation problem involving orthogonal frequency-division multiplexing (OFDM) signals, which are ubiquitous in many modern-day digital communication systems. Related efforts have been pursued in monaural source separation, where state-of-the-art neural architectures have been adopted to train an end-to-end separator for audio signals (as 1-dimensional time series). In this work, through a prototype problem based on the OFDM source model, we assess -- and question -- the efficacy of using audio-oriented neural architectures in separating signals based on features pertinent to communication waveforms. Perhaps surprisingly, we demonstrate that in some configurations, where perfect separation is theoretically attainable, these audio-oriented neural architectures perform poorly in separating co-channel OFDM waveforms. Yet, we propose critical domain-informed modifications to the network parameterization, based on insights from OFDM structures, that can co
    
[^16]: 基于强化学习的反虚假信息响应生成：以COVID-19疫苗虚假信息为例

    Reinforcement Learning-based Counter-Misinformation Response Generation: A Case Study of COVID-19 Vaccine Misinformation. (arXiv:2303.06433v1 [cs.SI])

    [http://arxiv.org/abs/2303.06433](http://arxiv.org/abs/2303.06433)

    本研究使用强化学习算法创建了反虚假信息响应生成模型，以帮助普通用户有效纠正虚假信息。

    This study creates a counter-misinformation response generation model using reinforcement learning algorithm to empower ordinary users to effectively correct misinformation.

    在线虚假信息的传播威胁着公共卫生、民主和更广泛的社会。本文旨在创建一个反虚假信息响应生成模型，以赋予用户有效纠正虚假信息的能力。本研究创建了两个虚假信息和反虚假信息数据集，使用强化学习算法训练模型，使其能够生成有效的反虚假信息响应。

    The spread of online misinformation threatens public health, democracy, and the broader society. While professional fact-checkers form the first line of defense by fact-checking popular false claims, they do not engage directly in conversations with misinformation spreaders. On the other hand, non-expert ordinary users act as eyes-on-the-ground who proactively counter misinformation -- recent research has shown that 96% counter-misinformation responses are made by ordinary users. However, research also found that 2/3 times, these responses are rude and lack evidence. This work seeks to create a counter-misinformation response generation model to empower users to effectively correct misinformation. This objective is challenging due to the absence of datasets containing ground-truth of ideal counter-misinformation responses, and the lack of models that can generate responses backed by communication theories. In this work, we create two novel datasets of misinformation and counter-misinfo
    
[^17]: 带有编码器和解码器集成的异常检测

    Anomaly Detection with Ensemble of Encoder and Decoder. (arXiv:2303.06431v1 [cs.LG])

    [http://arxiv.org/abs/2303.06431](http://arxiv.org/abs/2303.06431)

    本文提出了一种新颖的异常检测方法，通过多个编码器和解码器对正常样本的数据分布进行建模，将输入样本映射到潜在空间，然后从潜在向量中重构输出样本，最终将重构的样本映射到潜在表示。在训练阶段，通过最小化重构损失和编码来优化参数。

    This paper proposes a novel anomaly detection method by modeling the data distribution of normal samples via multiple encoders and decoders, mapping input samples into a latent space, reconstructing output samples from latent vectors, and mapping reconstructed samples to latent representations. Parameters are optimized during the training phase by minimizing the reconstruction loss and encoding.

    黑客和虚假数据注入可能会威胁电网的日常运营并造成重大经济损失。电网中的异常检测旨在检测和区分由针对电力系统的网络攻击引起的异常，这对于保持电网的正常运行和高效至关重要。已经应用了不同的方法进行异常检测，例如统计方法和基于机器学习的方法。通常，基于机器学习的方法需要对正常数据分布进行建模。在这项工作中，我们提出了一种新颖的异常检测方法，通过多个编码器和解码器对正常样本的数据分布进行建模。具体而言，所提出的方法将输入样本映射到潜在空间，然后从潜在向量中重构输出样本。额外的编码器最终将重构的样本映射到潜在表示。在训练阶段，我们通过最小化重构损失和编码来优化参数。

    Hacking and false data injection from adversaries can threaten power grids' everyday operations and cause significant economic loss. Anomaly detection in power grids aims to detect and discriminate anomalies caused by cyber attacks against the power system, which is essential for keeping power grids working correctly and efficiently. Different methods have been applied for anomaly detection, such as statistical methods and machine learning-based methods. Usually, machine learning-based methods need to model the normal data distribution. In this work, we propose a novel anomaly detection method by modeling the data distribution of normal samples via multiple encoders and decoders. Specifically, the proposed method maps input samples into a latent space and then reconstructs output samples from latent vectors. The extra encoder finally maps reconstructed samples to latent representations. During the training phase, we optimize parameters by minimizing the reconstruction loss and encoding
    
[^18]: 从大型数据集中学习可解释的因果网络，以乳腺癌患者的40万份医疗记录为例

    Learning interpretable causal networks from very large datasets, application to 400,000 medical records of breast cancer patients. (arXiv:2303.06423v1 [q-bio.QM])

    [http://arxiv.org/abs/2303.06423](http://arxiv.org/abs/2303.06423)

    本文提出了一种更可靠和可扩展的因果发现方法（iMIIC），并在来自美国监测、流行病学和终末结果计划的396,179名乳腺癌患者的医疗保健数据上展示了其独特能力。超过90％的预测因果效应是正确的，而其余的意外直接和间接因果效应可以解释为诊断程序、治疗时间、患者偏好或社会经济差距。

    This paper proposes a more reliable and scalable causal discovery method (iMIIC) and showcases its unique capabilities on healthcare data from 396,179 breast cancer patients from the US Surveillance, Epidemiology, and End Results program. Over 90% of predicted causal effects appear correct, while the remaining unexpected direct and indirect causal effects can be interpreted in terms of diagnostic procedures, therapeutic timing, patient preference or socio-economic disparity.

    发现因果效应是科学研究的核心，但当只有观察数据可用时，这仍然具有挑战性。在实践中，因果网络难以学习和解释，并且仅限于相对较小的数据集。我们报告了一种更可靠和可扩展的因果发现方法（iMIIC），基于一般的互信息最大原则，它极大地提高了推断的因果关系的精度，同时区分了真正的原因和假定的和潜在的因果效应。我们展示了iMIIC在来自美国监测、流行病学和终末结果计划的396,179名乳腺癌患者的合成和现实医疗保健数据上的独特能力。超过90％的预测因果效应是正确的，而其余的意外直接和间接因果效应可以解释为诊断程序、治疗时间、患者偏好或社会经济差距。iMIIC的独特能力开辟了发现可靠和可解释的因果网络的新途径。

    Discovering causal effects is at the core of scientific investigation but remains challenging when only observational data is available. In practice, causal networks are difficult to learn and interpret, and limited to relatively small datasets. We report a more reliable and scalable causal discovery method (iMIIC), based on a general mutual information supremum principle, which greatly improves the precision of inferred causal relations while distinguishing genuine causes from putative and latent causal effects. We showcase iMIIC on synthetic and real-life healthcare data from 396,179 breast cancer patients from the US Surveillance, Epidemiology, and End Results program. More than 90\% of predicted causal effects appear correct, while the remaining unexpected direct and indirect causal effects can be interpreted in terms of diagnostic procedures, therapeutic timing, patient preference or socio-economic disparity. iMIIC's unique capabilities open up new avenues to discover reliable and
    
[^19]: 从解释中进行鲁棒学习

    Robust Learning from Explanations. (arXiv:2303.06419v1 [cs.LG])

    [http://arxiv.org/abs/2303.06419](http://arxiv.org/abs/2303.06419)

    本文提出了一种新的机器学习方法，将机器学习从解释（MLX）重新构建为对抗鲁棒性问题，通过人类提供的解释来指定一个低维流形，从而减轻了对强参数正则化的需求，并在合成和真实世界基准测试中取得了最新结果。

    This paper proposes a new machine learning approach, recasting machine learning from explanations (MLX) as an adversarial robustness problem, which specifies a lower dimensional manifold from which perturbations can be drawn based on human-provided annotations, and shows improved performance over prior MLX methods on both synthetic and real-world benchmarks.

    机器学习从解释（MLX）是一种学习方法，它使用人类提供的有关每个输入的相关特征的注释，以确保模型预测的原因正确。现有的MLX方法严重依赖于特定的模型解释方法，并需要强大的参数正则化来对齐模型和人类解释，导致次优性能。我们将MLX重新构建为对抗鲁棒性问题，其中人类解释指定了一个低维流形，可以从中绘制扰动，并理论上和实验上展示了这种方法如何减轻对强参数正则化的需求。我们考虑了实现鲁棒性的各种方法，从而提高了先前MLX方法的性能。最后，我们将鲁棒性与早期的MLX方法相结合，产生了在合成和真实世界基准测试中的最新结果。

    Machine learning from explanations (MLX) is an approach to learning that uses human-provided annotations of relevant features for each input to ensure that model predictions are right for the right reasons. Existing MLX approaches rely heavily on a specific model interpretation approach and require strong parameter regularization to align model and human explanations, leading to sub-optimal performance. We recast MLX as an adversarial robustness problem, where human explanations specify a lower dimensional manifold from which perturbations can be drawn, and show both theoretically and empirically how this approach alleviates the need for strong parameter regularization. We consider various approaches to achieving robustness, leading to improved performance over prior MLX methods. Finally, we combine robustness with an earlier MLX method, yielding state-of-the-art results on both synthetic and real-world benchmarks.
    
[^20]: Brain Diffuser：一种端到端的脑图像到脑网络管道

    Brain Diffuser: An End-to-End Brain Image to Brain Network Pipeline. (arXiv:2303.06410v1 [cs.AI])

    [http://arxiv.org/abs/2303.06410](http://arxiv.org/abs/2303.06410)

    本文提出了一种基于扩散的端到端脑网络生成模型Brain Diffuser，直接从DTI中形成结构性脑网络。对于阿尔茨海默病的情况，所提出的模型在ADNI数据库上的表现优于现有工具包的结果。

    This paper proposes a diffusion based end-to-end brain network generative model Brain Diffuser that directly shapes the structural brain networks from DTI. For the case of Alzheimer's disease, the proposed model performs better than the results from existing toolkits on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database.

    脑网络分析对于诊断和干预阿尔茨海默病（AD）至关重要。然而，以往的研究主要依赖于特定的耗时和主观的工具包。只有少数工具可以从脑扩散张量图像（DTI）中获取结构性脑网络。在本文中，我们提出了一种基于扩散的端到端脑网络生成模型Brain Diffuser，直接从DTI中形成结构性脑网络。与现有工具包相比，Brain Diffuser通过分析受试者之间结构性脑网络的差异，利用更多的结构连接特征和与疾病相关的信息。对于阿尔茨海默病的情况，所提出的模型在阿尔茨海默病神经影像学倡议（ADNI）数据库上的表现优于现有工具包的结果。

    Brain network analysis is essential for diagnosing and intervention for Alzheimer's disease (AD). However, previous research relied primarily on specific time-consuming and subjective toolkits. Only few tools can obtain the structural brain networks from brain diffusion tensor images (DTI). In this paper, we propose a diffusion based end-to-end brain network generative model Brain Diffuser that directly shapes the structural brain networks from DTI. Compared to existing toolkits, Brain Diffuser exploits more structural connectivity features and disease-related information by analyzing disparities in structural brain networks across subjects. For the case of Alzheimer's disease, the proposed model performs better than the results from existing toolkits on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database.
    
[^21]: 自动检测辅助犬预测人类癫痫发作时的信号行为

    Automatic Detection of Signalling Behaviour from Assistance Dogs as they Forecast the Onset of Epileptic Seizures in Humans. (arXiv:2303.06407v1 [cs.LG])

    [http://arxiv.org/abs/2303.06407](http://arxiv.org/abs/2303.06407)

    本研究探讨了如何自动检测辅助犬预测人类癫痫发作时的信号行为，以提高癫痫患者的生活质量。

    This study explores how to automatically detect signalling behaviour from assistance dogs as they forecast the onset of epileptic seizures in humans, to improve the quality of life for epilepsy patients.

    癫痫是世界上最常见的神经系统疾病之一，影响着数百万人。癫痫发作通常是由于人脑中的非协调电放电引起的，可能会造成伤害，包括倒地和失去意识。如果能够预测癫痫发作的开始，那么可以将受试者置于安全的环境或位置，以最小化由于倒地而导致的自我伤害。然而，在日常的不受控制的环境中，没有明确的方法来预测癫痫发作。先前的研究表明，宠物狗有能力通过嗅探受试者在癫痫发作前皮肤散发的特征挥发性有机化合物来检测癫痫发作的开始，有些辅助犬经过训练，可以向其主人/训练员发出信号。在这项工作中，我们确定了如何自动检测信号行为。

    Epilepsy or the occurrence of epileptic seizures, is one of the world's most well-known neurological disorders affecting millions of people. Seizures mostly occur due to non-coordinated electrical discharges in the human brain and may cause damage, including collapse and loss of consciousness. If the onset of a seizure can be forecast then the subject can be placed into a safe environment or position so that self-injury as a result of a collapse can be minimised. However there are no definitive methods to predict seizures in an everyday, uncontrolled environment. Previous studies have shown that pet dogs have the ability to detect the onset of an epileptic seizure by scenting the characteristic volatile organic compounds exuded through the skin by a subject prior a seizure occurring and there are cases where assistance dogs, trained to scent the onset of a seizure, can signal this to their owner/trainer. In this work we identify how we can automatically detect the signalling behaviours
    
[^22]: 无遗憾算法用于公平资源分配

    No-regret Algorithms for Fair Resource Allocation. (arXiv:2303.06396v1 [cs.LG])

    [http://arxiv.org/abs/2303.06396](http://arxiv.org/abs/2303.06396)

    本文提出了一种无遗憾算法，用于公平资源分配问题，该算法可以实现$c_\alpha$-近似次线性遗憾，其中近似因子$c_\alpha=(1-\alpha)^{-(1-\alpha)}\leq 1.445$，对于$0\leq \alpha < 1$。

    This paper proposes a no-regret algorithm for fair resource allocation, which achieves $c_\alpha$-approximate sublinear regret with the approximation factor $c_\alpha=(1-\alpha)^{-(1-\alpha)}\leq 1.445,$ for $0\leq \alpha < 1$.

    本文考虑了一个公平资源分配问题，该问题在无遗憾设置下针对无限制的对手。目标是以在线方式公平地分配多个代理的资源，使得最优静态预知分配和在线策略的代理的聚合α-公平效用之差随时间增长的速度为次线性。由于α-公平性函数的非加性特性，该问题具有挑战性。先前的研究表明，该问题不存在具有次线性标准遗憾的在线策略。本文提出了一种高效的在线资源分配策略，称为在线比例公平（OPF），该策略实现了$c_\alpha$-近似次线性遗憾，其中近似因子$c_\alpha=(1-\alpha)^{-(1-\alpha)}\leq 1.445$，对于$0\leq \alpha < 1$。该问题的$c_\alpha$-遗憾上界展现出了一个令人惊讶的相变现象。遗憾上界从一个幂函数变为一个对数函数。

    We consider a fair resource allocation problem in the no-regret setting against an unrestricted adversary. The objective is to allocate resources equitably among several agents in an online fashion so that the difference of the aggregate $\alpha$-fair utilities of the agents between an optimal static clairvoyant allocation and that of the online policy grows sub-linearly with time. The problem is challenging due to the non-additive nature of the $\alpha$-fairness function. Previously, it was shown that no online policy can exist for this problem with a sublinear standard regret. In this paper, we propose an efficient online resource allocation policy, called Online Proportional Fair (OPF), that achieves $c_\alpha$-approximate sublinear regret with the approximation factor $c_\alpha=(1-\alpha)^{-(1-\alpha)}\leq 1.445,$ for $0\leq \alpha < 1$. The upper bound to the $c_\alpha$-regret for this problem exhibits a surprising phase transition phenomenon. The regret bound changes from a power
    
[^23]: 一种结合移动前沿、数据分解和深度学习的新方法用于预测复杂时间序列

    A Novel Method Combines Moving Fronts, Data Decomposition and Deep Learning to Forecast Intricate Time Series. (arXiv:2303.06394v1 [cs.LG])

    [http://arxiv.org/abs/2303.06394](http://arxiv.org/abs/2303.06394)

    本文提出了一种新的方法，结合移动前沿、数据分解和深度学习，用于预测复杂时间序列。该方法通过经验小波变换将时间序列分解成更简单的组成序列，并使用移动前沿方法防止数据泄漏。

    This paper proposes a novel method that combines moving fronts, data decomposition, and deep learning to forecast intricate time series. The method decomposes the time series into simpler constituent series using empirical wavelet transform and prevents data leakage using the moving front method.

    高变异性的单变量时间序列甚至对深度神经网络（DNN）也构成挑战。为了克服这一问题，单变量时间序列被分解成更简单的组成序列，它们的总和等于原始序列。本文演示了传统的一次分解技术存在数据泄漏的问题。因此，提出了一种新的移动前沿（MF）方法来防止数据泄漏，使分解后的序列可以像其他时间序列一样处理。印度夏季季风降雨（ISMR）是一个非常复杂的时间序列，对DNN构成挑战，因此被选为示例。从众多可用的信号处理工具中，经验小波变换（EWT）被选择用于将ISMR分解成更简单的组成序列，因为它被发现比其他流行的算法，如自适应噪声完全集合经验模态分解（CEEMDAN）更有效。

    A univariate time series with high variability can pose a challenge even to Deep Neural Network (DNN). To overcome this, a univariate time series is decomposed into simpler constituent series, whose sum equals the original series. As demonstrated in this article, the conventional one-time decomposition technique suffers from a leak of information from the future, referred to as a data leak. In this work, a novel Moving Front (MF) method is proposed to prevent data leakage, so that the decomposed series can be treated like other time series. Indian Summer Monsoon Rainfall (ISMR) is a very complex time series, which poses a challenge to DNN and is therefore selected as an example. From the many signal processing tools available, Empirical Wavelet Transform (EWT) was chosen for decomposing the ISMR into simpler constituent series, as it was found to be more effective than the other popular algorithm, Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN). The propose
    
[^24]: 不确定性感知的离线学习

    Uncertainty-Aware Off-Policy Learning. (arXiv:2303.06389v1 [cs.LG])

    [http://arxiv.org/abs/2303.06389](http://arxiv.org/abs/2303.06389)

    本文提出了一种不确定性感知的倒数概率分数估计器（UIPS），用于改进离线学习，通过明确模拟估计的记录策略中的不确定性，相对于广泛的最先进基线具有优越的样本效率。

    This paper proposes an Uncertainty-aware Inverse Propensity Score estimator (UIPS) for improved off-policy learning, which explicitly models the uncertainty in the estimated logging policy and demonstrates advantageous sample efficiency against an extensive list of state-of-the-art baselines on synthetic and three real-world recommendation datasets.

    离线学习是指仅通过记录的反馈数据进行策略优化的过程，在各种实际应用中显示出重要性，例如搜索引擎、推荐系统等。虽然生成记录数据的真实记录策略通常是未知的，但以前的工作仅在离线学习中采用其估计值，忽略了由于这种估计器导致的高偏差和高方差，特别是在具有小且估计不准确的记录概率的样本上。在这项工作中，我们明确地模拟了估计的记录策略中的不确定性，并提出了一种不确定性感知的倒数概率分数估计器（UIPS）来改进离线学习。在合成和三个真实的推荐数据集上的实验结果表明，所提出的UIPS估计器相对于广泛的最先进基线具有优越的样本效率。

    Off-policy learning, referring to the procedure of policy optimization with access only to logged feedback data, has shown importance in various real-world applications, such as search engines, recommender systems, and etc. While the ground-truth logging policy, which generates the logged data, is usually unknown, previous work simply takes its estimated value in off-policy learning, ignoring both high bias and high variance resulted from such an estimator, especially on samples with small and inaccurately estimated logging probabilities. In this work, we explicitly model the uncertainty in the estimated logging policy and propose a Uncertainty-aware Inverse Propensity Score estimator (UIPS) for improved off-policy learning. Experiment results on synthetic and three real-world recommendation datasets demonstrate the advantageous sample efficiency of the proposed UIPS estimator against an extensive list of state-of-the-art baselines.
    
[^25]: 机器学习临床脑电图分类中的范围和仲裁

    Scope and Arbitration in Machine Learning Clinical EEG Classification. (arXiv:2303.06386v1 [cs.LG])

    [http://arxiv.org/abs/2303.06386](http://arxiv.org/abs/2303.06386)

    本文提出了两种方法来解决机器学习临床脑电图分类中窗口标签可能误导的问题：增加窗口长度和引入第二阶段模型来仲裁记录内的窗口特定预测。在Temple大学医院异常脑电图语料库上评估这些方法，最先进的平均准确度从89.8％显着提高到93.3％。

    This paper proposes two methods to address the problem of potentially misleading window labels in machine learning clinical EEG classification: increasing window length and introducing a second-stage model to arbitrate between window-specific predictions within a recording. Evaluating these methods on the Temple University Hospital Abnormal EEG Corpus, the state-of-the-art average accuracy was significantly improved from 89.8% to 93.3%.

    临床脑电图解读的一个关键任务是将记录或会话分类为正常或异常。在机器学习方法中，为了实际操作的原因，通常将记录分成较短的窗口，并且这些窗口继承其父记录的标签。我们假设以这种方式派生的窗口标签可能会误导，例如，没有明显异常的窗口可能会被标记为“异常”，从而破坏学习过程并降低性能。我们探索了两种可分离的方法来缓解这个问题：增加窗口长度和引入第二阶段模型来仲裁记录内的窗口特定预测。在Temple大学医院异常脑电图语料库上评估这些方法，我们将最先进的平均准确度从89.8％显着提高到93.3％。这个结果挑战了先前对该数据集性能上限的估计，代表了迈向更好性能的重要一步。

    A key task in clinical EEG interpretation is to classify a recording or session as normal or abnormal. In machine learning approaches to this task, recordings are typically divided into shorter windows for practical reasons, and these windows inherit the label of their parent recording. We hypothesised that window labels derived in this manner can be misleading for example, windows without evident abnormalities can be labelled `abnormal' disrupting the learning process and degrading performance. We explored two separable approaches to mitigate this problem: increasing the window length and introducing a second-stage model to arbitrate between the window-specific predictions within a recording. Evaluating these methods on the Temple University Hospital Abnormal EEG Corpus, we significantly improved state-of-the-art average accuracy from 89.8 percent to 93.3 percent. This result defies previous estimates of the upper limit for performance on this dataset and represents a major step towar
    
[^26]: 学习预编码用于集成感知和通信系统

    Learning to Precode for Integrated Sensing and Communications Systems. (arXiv:2303.06381v1 [eess.SP])

    [http://arxiv.org/abs/2303.06381](http://arxiv.org/abs/2303.06381)

    本文提出了一种无监督学习神经模型，用于设计集成感知和通信（ISAC）系统的传输预编码器，以最大化最坏情况下的目标照明功率，同时确保所有用户的最小信干噪比（SINR）。通过数值模拟，证明了该方法在存在信道估计误差的情况下优于传统的基于优化的方法，同时产生较小的计算复杂度，并且在不同的信道条件下具有良好的泛化能力。

    This paper proposes an unsupervised learning neural model to design transmit precoders for integrated sensing and communication (ISAC) systems to maximize the worst-case target illumination power while ensuring a minimum signal-to-interference-plus-noise ratio (SINR) for all the users. The proposed method outperforms traditional optimization-based methods in presence of channel estimation errors while incurring lesser computational complexity and generalizing well across different channel conditions that were not shown during training.

    本文提出了一种无监督学习神经模型，用于设计集成感知和通信（ISAC）系统的传输预编码器，以最大化最坏情况下的目标照明功率，同时确保所有用户的最小信干噪比（SINR）。从上行导频和回波中学习传输预编码器的问题可以看作是一个参数化函数估计问题，我们提出使用神经网络模型来学习这个函数。为了学习神经网络参数，我们开发了一种基于一阶最优性条件的损失函数，以纳入SINR和功率约束。通过数值模拟，我们证明了所提出的方法在存在信道估计误差的情况下优于传统的基于优化的方法，同时产生较小的计算复杂度，并且在不同的信道条件下具有良好的泛化能力，这些条件在训练期间没有显示出来。

    In this paper, we present an unsupervised learning neural model to design transmit precoders for integrated sensing and communication (ISAC) systems to maximize the worst-case target illumination power while ensuring a minimum signal-to-interference-plus-noise ratio (SINR) for all the users. The problem of learning transmit precoders from uplink pilots and echoes can be viewed as a parameterized function estimation problem and we propose to learn this function using a neural network model. To learn the neural network parameters, we develop a novel loss function based on the first-order optimality conditions to incorporate the SINR and power constraints. Through numerical simulations, we demonstrate that the proposed method outperforms traditional optimization-based methods in presence of channel estimation errors while incurring lesser computational complexity and generalizing well across different channel conditions that were not shown during training.
    
[^27]: 评估基于EEG的机器学习在帕金森病检测中的性别公平性：一项多中心研究

    Assessing gender fairness in EEG-based machine learning detection of Parkinson's disease: A multi-center study. (arXiv:2303.06376v1 [eess.SP])

    [http://arxiv.org/abs/2303.06376](http://arxiv.org/abs/2303.06376)

    本研究在多中心环境中对基于EEG的机器学习算法进行了性别子组群的检测能力分析，发现男性和女性的PD检测能力存在显着差异。

    This study analyzed the detection ability of gender sub-groups in a multi-center setting of a previously developed machine learning algorithm based on EEG, finding significant differences in Parkinson's disease detection ability between males and females.

    随着基于机器学习（ML）和静息态脑电图（rs-EEG）的自动工具在帕金森病（PD）检测中的数量不断增长，通过公平性和偏差分析评估可能加剧健康差异的问题变得更加重要。受保护的属性，如性别，在PD诊断开发中发挥重要作用。然而，来自不同性别的子组群体的分析很少在ML模型的开发或PD检测的性能评估中考虑。在这项工作中，我们对基于静息态脑电图功率谱密度（PSD）特征的先前开发的ML算法在多中心环境中的性别子组群的检测能力进行了系统分析。我们发现在测试时间（80.5％对63.7％的准确性）男性和女性的PD检测能力存在显着差异，并且一组顶部和前额脑电图通道和频率存在显着更高的活动。

    As the number of automatic tools based on machine learning (ML) and resting-state electroencephalography (rs-EEG) for Parkinson's disease (PD) detection keeps growing, the assessment of possible exacerbation of health disparities by means of fairness and bias analysis becomes more relevant. Protected attributes, such as gender, play an important role in PD diagnosis development. However, analysis of sub-group populations stemming from different genders is seldom taken into consideration in ML models' development or the performance assessment for PD detection. In this work, we perform a systematic analysis of the detection ability for gender sub-groups in a multi-center setting of a previously developed ML algorithm based on power spectral density (PSD) features of rs-EEG. We find significant differences in the PD detection ability for males and females at testing time (80.5% vs. 63.7% accuracy) and significantly higher activity for a set of parietal and frontal EEG channels and frequen
    
[^28]: 通过虚拟检查层实现时间序列的可解释性人工智能

    Explainable AI for Time Series via Virtual Inspection Layers. (arXiv:2303.06365v1 [cs.LG])

    [http://arxiv.org/abs/2303.06365](http://arxiv.org/abs/2303.06365)

    本文提出了一种虚拟检查层，将时间序列转换为可解释的表示，并允许通过本地XAI方法将相关性归因传播到该表示。我们将一系列XAI方法的适用性扩展到需要转换后才能解释输入的领域。我们展示了DFT-LRP在各种时间序列分类设置中的有用性，如音频和电子健康记录。

    This paper proposes a virtual inspection layer that transforms time series into an interpretable representation and allows for relevance attributions to be propagated to this representation via local XAI methods. The applicability of a family of XAI methods is extended to domains where the input is only interpretable after a transformation. The usefulness of DFT-LRP is demonstrated in various time series classification settings, such as audio and electronic health records.

    最近几年，可解释人工智能（XAI）领域取得了很大进展，但主要是在计算机视觉和自然语言处理方面。对于时间序列，由于输入通常不可解释，因此只有有限的XAI研究可用。在这项工作中，我们提出了一种虚拟检查层，将时间序列转换为可解释的表示，并允许通过本地XAI方法（如逐层相关传播（LRP））将相关性归因传播到该表示。通过这种方式，我们将一系列XAI方法的适用性扩展到需要转换后才能解释输入的领域（例如语音）。在这里，我们专注于傅里叶变换，这在时间序列解释和LRP中被广泛应用，并将我们的方法称为DFT-LRP。我们展示了DFT-LRP在各种时间序列分类设置中的有用性，如音频和电子健康记录。我们展示了如何使用DFT-LRP来可视化和解释模型的决策。

    The field of eXplainable Artificial Intelligence (XAI) has greatly advanced in recent years, but progress has mainly been made in computer vision and natural language processing. For time series, where the input is often not interpretable, only limited research on XAI is available. In this work, we put forward a virtual inspection layer, that transforms the time series to an interpretable representation and allows to propagate relevance attributions to this representation via local XAI methods like layer-wise relevance propagation (LRP). In this way, we extend the applicability of a family of XAI methods to domains (e.g. speech) where the input is only interpretable after a transformation. Here, we focus on the Fourier transformation which is prominently applied in the interpretation of time series and LRP and refer to our method as DFT-LRP. We demonstrate the usefulness of DFT-LRP in various time series classification settings like audio and electronic health records. We showcase how 
    
[^29]: 面向非静态环境的隐私保护合作可见光定位：联邦学习视角

    Privacy-Preserving Cooperative Visible Light Positioning for Nonstationary Environment: A Federated Learning Perspective. (arXiv:2303.06361v1 [eess.SP])

    [http://arxiv.org/abs/2303.06361](http://arxiv.org/abs/2303.06361)

    本文提出了一种基于联邦学习的合作可见光定位方案，通过共同训练适应环境变化的全局模型，提高了在非静态环境下的定位精度和泛化能力。

    This paper proposes a cooperative visible light positioning scheme based on federated learning, which improves the positioning accuracy and generalization capability in nonstationary environments by jointly training a global model adaptive to environmental changes without sharing private data of users.

    可见光定位（VLP）作为一种有前途的室内定位技术，已经引起了足够的关注。然而，在非静态环境下，由于高度时变的信道，VLP的性能受到限制。为了提高非静态环境下的定位精度和泛化能力，本文提出了一种基于联邦学习（FL）的合作VLP方案。利用FL框架，用户可以共同训练适应环境变化的全局模型，而不共享用户的私有数据。此外，提出了一种合作可见光定位网络（CVPosNet），以加速收敛速度和提高定位精度。仿真结果表明，所提出的方案在非静态环境下优于基准方案。

    Visible light positioning (VLP) has drawn plenty of attention as a promising indoor positioning technique. However, in nonstationary environments, the performance of VLP is limited because of the highly time-varying channels. To improve the positioning accuracy and generalization capability in nonstationary environments, a cooperative VLP scheme based on federated learning (FL) is proposed in this paper. Exploiting the FL framework, a global model adaptive to environmental changes can be jointly trained by users without sharing private data of users. Moreover, a Cooperative Visible-light Positioning Network (CVPosNet) is proposed to accelerate the convergence rate and improve the positioning accuracy. Simulation results show that the proposed scheme outperforms the benchmark schemes, especially in nonstationary environments.
    
[^30]: FedLP: 一种用于通信计算高效的联邦学习的层次剪枝机制

    FedLP: Layer-wise Pruning Mechanism for Communication-Computation Efficient Federated Learning. (arXiv:2303.06360v1 [cs.LG])

    [http://arxiv.org/abs/2303.06360](http://arxiv.org/abs/2303.06360)

    本文提出了一种显式的FL剪枝框架FedLP，采用局部训练和联邦更新中的层次剪枝，对不同类型的深度学习模型具有普适性，可以缓解通信和计算的系统瓶颈，并且性能下降较小。

    This paper proposes an explicit FL pruning framework, FedLP, which adopts layer-wise pruning in local training and federated updating, and is model-agnostic and universal for different types of deep learning models. FedLP can relieve the system bottlenecks of communication and computation with marginal performance decay.

    联邦学习（FL）已经成为一种高效且隐私保护的分布式学习方案。本文主要关注FL中计算和通信的优化，采用局部训练和联邦更新中的层次剪枝，提出了一个显式的FL剪枝框架FedLP（Federated Layer-wise Pruning），该框架对不同类型的深度学习模型具有普适性。为具有同质本地模型和异质本地模型的场景设计了两种特定的FedLP方案。通过理论和实验评估，证明了FedLP可以缓解通信和计算的系统瓶颈，并且性能下降较小。据我们所知，FedLP是第一个正式将层次剪枝引入FL的框架。在联邦学习范围内，可以基于FedLP进一步设计更多的变体和组合。

    Federated learning (FL) has prevailed as an efficient and privacy-preserved scheme for distributed learning. In this work, we mainly focus on the optimization of computation and communication in FL from a view of pruning. By adopting layer-wise pruning in local training and federated updating, we formulate an explicit FL pruning framework, FedLP (Federated Layer-wise Pruning), which is model-agnostic and universal for different types of deep learning models. Two specific schemes of FedLP are designed for scenarios with homogeneous local models and heterogeneous ones. Both theoretical and experimental evaluations are developed to verify that FedLP relieves the system bottlenecks of communication and computation with marginal performance decay. To the best of our knowledge, FedLP is the first framework that formally introduces the layer-wise pruning into FL. Within the scope of federated learning, more variants and combinations can be further designed based on FedLP.
    
[^31]: 复活长序列的循环神经网络

    Resurrecting Recurrent Neural Networks for Long Sequences. (arXiv:2303.06349v1 [cs.LG])

    [http://arxiv.org/abs/2303.06349](http://arxiv.org/abs/2303.06349)

    本文研究了如何通过对标准RNN进行精心设计，包括线性化和对角化循环、使用更好的参数化和初始化以及确保正常化前向传递等一系列改变，来恢复深度SSM在长距离推理任务上的卓越性能，同时匹配它们的训练速度。

    This paper explores how to recover the impressive performance of deep state-space models (SSMs) on long-range reasoning tasks by carefully designing deep RNNs using standard signal propagation arguments, including linearizing and diagonalizing the recurrence, using better parameterizations and initializations, and ensuring proper normalization of the forward pass.

    循环神经网络（RNN）在长序列上提供快速推理，但难以优化且训练速度慢。最近，深度状态空间模型（SSM）在长序列建模任务上表现出色，并具有快速可并行化的训练和类似RNN的快速推理的额外优势。然而，虽然SSM与RNN在表面上相似，但存在重要差异，使得不清楚它们在RNN上的性能提升来自何处。在本文中，我们展示了使用标准信号传播论据精心设计的深度RNN可以恢复深度SSM在长距离推理任务上的卓越性能，同时匹配它们的训练速度。为了实现这一点，我们分析和消融了一系列对标准RNN的更改，包括线性化和对角化循环，使用更好的参数化和初始化，并确保正常化前向传递。我们的结果提供了关于深度RNN和深度SSM性能差异来源的新见解。

    Recurrent Neural Networks (RNNs) offer fast inference on long sequences but are hard to optimize and slow to train. Deep state-space models (SSMs) have recently been shown to perform remarkably well on long sequence modeling tasks, and have the added benefits of fast parallelizable training and RNN-like fast inference. However, while SSMs are superficially similar to RNNs, there are important differences that make it unclear where their performance boost over RNNs comes from. In this paper, we show that careful design of deep RNNs using standard signal propagation arguments can recover the impressive performance of deep SSMs on long-range reasoning tasks, while also matching their training speed. To achieve this, we analyze and ablate a series of changes to standard RNNs including linearizing and diagonalizing the recurrence, using better parameterizations and initializations, and ensuring proper normalization of the forward pass. Our results provide new insights on the origins of the 
    
[^32]: 异质性下的对比学习

    Contrastive Learning under Heterophily. (arXiv:2303.06344v1 [cs.LG])

    [http://arxiv.org/abs/2303.06344](http://arxiv.org/abs/2303.06344)

    本文提出了第一个图形对比学习方法，以解决现有图形对比学习方法在异质性下无法学习高质量表示的问题。

    This paper proposes the first graph contrastive learning method to address the problem that existing graph contrastive learning methods cannot learn high-quality representations under heterophily.

    图神经网络是一种强大的工具，可以在具有特定任务节点标签的情况下学习节点表示。然而，在许多应用中，为图形获取标签是昂贵的。这在大型图形的情况下尤其如此。为了解决这个问题，已经有一些工作在没有标签的情况下以自监督的方式学习节点表示。对比学习（CL）在以自监督的方式学习表示方面特别受欢迎。一般来说，CL方法通过最大化相同示例的增强视图的表示之间的相似性，并最小化不同示例的增强视图之间的相似性来工作。然而，现有的图形CL方法不能在异质性下学习高质量的表示，其中连接的节点倾向于属于不同的类。这是因为在异质性下，同一示例的增强可能彼此不相似。在这项工作中，我们通过提出第一个图形对比学习方法来解决上述问题。

    Graph Neural Networks are powerful tools for learning node representations when task-specific node labels are available. However, obtaining labels for graphs is expensive in many applications. This is particularly the case for large graphs. To address this, there has been a body of work to learn node representations in a self-supervised manner without labels. Contrastive learning (CL), has been particularly popular to learn representations in a self-supervised manner. In general, CL methods work by maximizing the similarity between representations of augmented views of the same example, and minimizing the similarity between augmented views of different examples. However, existing graph CL methods cannot learn high-quality representations under heterophily, where connected nodes tend to belong to different classes. This is because under heterophily, augmentations of the same example may not be similar to each other. In this work, we address the above problem by proposing the first graph
    
[^33]: 基于张量网络机器学习的Raman光谱数据肺癌智能诊断方案

    Intelligent diagnostic scheme for lung cancer screening with Raman spectra data by tensor network machine learning. (arXiv:2303.06340v1 [q-bio.QM])

    [http://arxiv.org/abs/2303.06340](http://arxiv.org/abs/2303.06340)

    本文提出了一种基于张量网络机器学习的方案，通过筛查呼出气中挥发性有机化合物（VOC）的Raman光谱数据，可可靠地预测肺癌患者及其阶段。

    This paper proposes a tensor-network machine learning method to reliably predict lung cancer patients and their stages via screening Raman spectra data of Volatile organic compounds (VOCs) in exhaled breath.

    人工智能（AI）已经在生物医学科学中带来了巨大的影响，从学术研究到临床应用，例如生物标志物的检测和诊断、治疗优化以及药物发现中新的治疗靶点的识别。然而，当代AI技术，特别是深度机器学习（ML），严重受到非可解释性的影响，这可能会不可控地导致错误的预测。对于ML的可解释性尤其重要，因为消费者必须从坚实的基础或令人信服的解释中获得必要的安全感和信任感。在这项工作中，我们提出了一种基于张量网络（TN）-ML方法的方案，通过筛查呼出气中挥发性有机化合物（VOC）的Raman光谱数据，可可靠地预测肺癌患者及其阶段，这些数据通常适用于生物标志物，并被认为是非侵入性肺癌筛查的理想方式。TN-ML的预测基于

    Artificial intelligence (AI) has brought tremendous impacts on biomedical sciences from academic researches to clinical applications, such as in biomarkers' detection and diagnosis, optimization of treatment, and identification of new therapeutic targets in drug discovery. However, the contemporary AI technologies, particularly deep machine learning (ML), severely suffer from non-interpretability, which might uncontrollably lead to incorrect predictions. Interpretability is particularly crucial to ML for clinical diagnosis as the consumers must gain necessary sense of security and trust from firm grounds or convincing interpretations. In this work, we propose a tensor-network (TN)-ML method to reliably predict lung cancer patients and their stages via screening Raman spectra data of Volatile organic compounds (VOCs) in exhaled breath, which are generally suitable as biomarkers and are considered to be an ideal way for non-invasive lung cancer screening. The prediction of TN-ML is based
    
[^34]: AutoMLP: 自动化MLP用于序列推荐

    AutoMLP: Automated MLP for Sequential Recommendations. (arXiv:2303.06337v1 [cs.IR])

    [http://arxiv.org/abs/2303.06337](http://arxiv.org/abs/2303.06337)

    AutoMLP是一种新颖的序列推荐系统，通过自动化和自适应搜索算法，更好地模拟用户的长期/短期兴趣，实现更好的推荐效果。

    AutoMLP is a novel sequential recommender system that models users' long/short-term interests through an automated and adaptive search algorithm, achieving better recommendation performance.

    序列推荐系统旨在根据用户的历史交互来预测他们下一个感兴趣的项目。然而，长期存在的问题是如何区分用户的长期/短期兴趣，这可能是异质的并对下一个推荐产生不同的贡献。现有方法通常通过穷举搜索或经验经验设置预定义的短期兴趣长度，这既高度低效又产生次优结果。最近的先进基于变压器的模型可以实现最先进的性能，尽管存在上述问题，但它们对输入序列的长度具有二次计算复杂度。为此，本文提出了一种新颖的序列推荐系统AutoMLP，旨在更好地模拟用户的长期/短期兴趣。此外，我们设计了一种自动化和自适应搜索算法，以通过端到端优化获得更好的短期兴趣长度。通过实验，我们证明了AutoMLP的有效性和效率。

    Sequential recommender systems aim to predict users' next interested item given their historical interactions. However, a long-standing issue is how to distinguish between users' long/short-term interests, which may be heterogeneous and contribute differently to the next recommendation. Existing approaches usually set pre-defined short-term interest length by exhaustive search or empirical experience, which is either highly inefficient or yields subpar results. The recent advanced transformer-based models can achieve state-of-the-art performances despite the aforementioned issue, but they have a quadratic computational complexity to the length of the input sequence. To this end, this paper proposes a novel sequential recommender system, AutoMLP, aiming for better modeling users' long/short-term interests from their historical interactions. In addition, we design an automated and adaptive search algorithm for preferable short-term interest length via end-to-end optimization. Through ext
    
[^35]: MetaViewer: 朝着统一的多视图表示迈进

    MetaViewer: Towards A Unified Multi-View Representation. (arXiv:2303.06329v1 [cs.CV])

    [http://arxiv.org/abs/2303.06329](http://arxiv.org/abs/2303.06329)

    该论文提出了一种新颖的基于双层优化的多视图学习框架MetaViewer，通过统一到特定的方式学习表示，避免了手动预先指定的融合函数和混合在特征中的视图专用冗余信息可能会降低所得表示的质量的问题。

    This paper proposes a novel bi-level-optimization-based multi-view learning framework, MetaViewer, which learns the representation in a uniform-to-specific manner, avoiding the problem of manually pre-specify fusion functions and view-private redundant information mixed in features that potentially degrade the quality of the derived representation.

    现有的多视图表示学习方法通常遵循特定到统一的流程，从每个视图中提取潜在特征，然后融合或对齐它们以获得统一的对象表示。然而，手动预先指定的融合函数和混合在特征中的视图专用冗余信息可能会降低所得表示的质量。为了克服这些问题，我们提出了一种新颖的基于双层优化的多视图学习框架，其中表示是以统一到特定的方式学习的。具体而言，我们训练一个元学习器，即MetaViewer，在外层优化中学习融合和建模视图共享的元表示。从这个元表示开始，需要在内层训练视图特定的基学习器，以快速重构相应的视图。MetaViewer最终通过观察所有视图上从统一到特定的重构过程来更新，并学习最佳融合方案。

    Existing multi-view representation learning methods typically follow a specific-to-uniform pipeline, extracting latent features from each view and then fusing or aligning them to obtain the unified object representation. However, the manually pre-specify fusion functions and view-private redundant information mixed in features potentially degrade the quality of the derived representation. To overcome them, we propose a novel bi-level-optimization-based multi-view learning framework, where the representation is learned in a uniform-to-specific manner. Specifically, we train a meta-learner, namely MetaViewer, to learn fusion and model the view-shared meta representation in outer-level optimization. Start with this meta representation, view-specific base-learners are then required to rapidly reconstruct the corresponding view in inner-level. MetaViewer eventually updates by observing reconstruction processes from uniform to specific over all views, and learns an optimal fusion scheme that
    
[^36]: 一种新的张量专家混合并行方法来扩展混合专家训练

    A Novel Tensor-Expert Hybrid Parallelism Approach to Scale Mixture-of-Experts Training. (arXiv:2303.06318v1 [cs.LG])

    [http://arxiv.org/abs/2303.06318](http://arxiv.org/abs/2303.06318)

    本文提出了一种新的混合并行算法，结合了张量、专家和数据并行，以实现MoE模型的训练，其基本模型比当前最先进的DeepSpeed-MoE大4-8倍。

    This paper proposes a novel hybrid parallel algorithm that combines tensor, expert, and data parallelism to enable the training of MoE models with 4-8x larger base models than the current state-of-the-art -- DeepSpeed-MoE.

    最近提出了一种名为Mixture-of-Experts（MoE）的新型神经网络架构，通过添加稀疏激活的专家块来增加神经网络（基本模型）的参数，而不改变训练或推理的总浮点操作数。理论上，这种架构允许我们训练任意大的模型，同时保持计算成本与基本模型相同。然而，在64到128个专家块之外，先前的工作观察到这些MoE模型的测试准确性递减。因此，训练高质量的MoE模型需要我们扩展基本模型的大小以及专家块的数量。在这项工作中，我们提出了一种新颖的三维混合并行算法，结合了张量、专家和数据并行，以实现MoE模型的训练，其基本模型比当前最先进的DeepSpeed-MoE大4-8倍。我们在优化器步骤中提出了内存优化。

    A new neural network architecture called Mixture-of-Experts (MoE) has been proposed recently that increases the parameters of a neural network (the base model) by adding sparsely activated expert blocks, without changing the total number of floating point operations for training or inference. In theory, this architecture allows us to train arbitrarily large models while keeping the computational costs same as that of the base model. However, beyond 64 to 128 experts blocks, prior work has observed diminishing returns in the test accuracies of these MoE models. Thus, training high quality MoE models requires us to scale the size of the base models, along with the number of expert blocks. In this work, we propose a novel, three-dimensional, hybrid parallel algorithm that combines tensor, expert, and data parallelism to enable the training of MoE models with 4-8x larger base models than the current state-of-the-art -- DeepSpeed-MoE. We propose memory optimizations in the optimizer step, a
    
[^37]: 一个神经元的节省就是一个神经元的收益：关于二次网络参数效率的研究

    One Neuron Saved Is One Neuron Earned: On Parametric Efficiency of Quadratic Networks. (arXiv:2303.06316v1 [cs.LG])

    [http://arxiv.org/abs/2303.06316](http://arxiv.org/abs/2303.06316)

    本文研究了二次神经元的参数效率，证明了其卓越性能是由于内在表达能力而非参数增加。

    This paper studies the parametric efficiency of quadratic neurons and confirms that their superior performance is due to intrinsic expressive capability rather than increased parameters.

    受生物神经系统中神经元多样性的启发，大量研究提出了设计新型人工神经元并将神经元多样性引入人工神经网络的方法。最近提出的二次神经元，将传统神经元中的内积操作替换为二次操作，在许多重要任务中取得了巨大成功。尽管二次神经元的结果很有前途，但仍存在一个未解决的问题：二次网络的卓越性能仅仅是由于参数增加还是由于内在表达能力？在未澄清这个问题的情况下，二次网络的性能总是令人怀疑。此外，解决这个问题就是找到二次网络的杀手应用。在本文中，通过理论和实证研究，我们展示了二次网络具有参数效率，从而确认了二次网络的卓越性能是由于其内在表达能力而非参数增加。

    Inspired by neuronal diversity in the biological neural system, a plethora of studies proposed to design novel types of artificial neurons and introduce neuronal diversity into artificial neural networks. Recently proposed quadratic neuron, which replaces the inner-product operation in conventional neurons with a quadratic one, have achieved great success in many essential tasks. Despite the promising results of quadratic neurons, there is still an unresolved issue: \textit{Is the superior performance of quadratic networks simply due to the increased parameters or due to the intrinsic expressive capability?} Without clarifying this issue, the performance of quadratic networks is always suspicious. Additionally, resolving this issue is reduced to finding killer applications of quadratic networks. In this paper, with theoretical and empirical studies, we show that quadratic networks enjoy parametric efficiency, thereby confirming that the superior performance of quadratic networks is due
    
[^38]: 在物联网系统中通过非独立同分布数据和客户端dropout来稳定和改进联邦学习

    Stabilizing and Improving Federated Learning with Non-IID Data and Client Dropout in IoT Systems. (arXiv:2303.06314v1 [cs.LG])

    [http://arxiv.org/abs/2303.06314](http://arxiv.org/abs/2303.06314)

    本文提出了一个简单而有效的框架，通过引入一个先验校准的softmax函数来计算交叉熵损失和基于原型的特征提取来维护一个平衡的分类器头，以稳定和改进联邦学习。

    This paper proposes a simple yet effective framework to stabilize and improve federated learning by introducing a prior-calibrated softmax function for computing the cross-entropy loss and a prototype-based feature extraction to maintain a balanced classifier head.

    联邦学习是一种新兴的技术，用于在不暴露私有数据的情况下在分散的客户端上训练深度模型，然而它受到标签分布偏斜的影响，通常导致收敛缓慢和模型性能下降。当参与的客户端处于不稳定的环境并经常掉线时，这个挑战可能更加严重。为了解决这个问题，我们提出了一个简单而有效的框架，通过引入一个先验校准的softmax函数来计算交叉熵损失和基于原型的特征提取来维护一个平衡的分类器头。

    Federated learning is an emerging technique for training deep models over decentralized clients without exposing private data, which however suffers from label distribution skew and usually results in slow convergence and degraded model performance. This challenge could be more serious when the participating clients are in unstable circumstances and dropout frequently. Previous work and our empirical observations demonstrate that the classifier head for classification task is more sensitive to label skew and the unstable performance of FedAvg mainly lies in the imbalanced training samples across different classes. The biased classifier head will also impact the learning of feature representations. Therefore, maintaining a balanced classifier head is of significant importance for building a better global model. To tackle this issue, we propose a simple yet effective framework by introducing a prior-calibrated softmax function for computing the cross-entropy loss and a prototype-based fe
    
[^39]: 生成对抗网络在EXO-200闪烁信号模拟中的应用

    Generative Adversarial Networks for Scintillation Signal Simulation in EXO-200. (arXiv:2303.06311v1 [hep-ex])

    [http://arxiv.org/abs/2303.06311](http://arxiv.org/abs/2303.06311)

    本文介绍了一种基于生成对抗网络的新方法，用于从EXO-200实验的时间投影室中模拟光电探测器信号。该方法能够比传统的模拟方法快一个数量级地产生高质量的模拟波形，并且能够从训练样本中推广并识别数据的显著高级特征。

    This paper introduces a novel approach using Generative Adversarial Networks to simulate photodetector signals from the time projection chamber of the EXO-200 experiment. The method is able to produce high-quality simulated waveforms an order of magnitude faster than traditional simulation methods and can generalize from the training sample and discern salient high-level features of the data.

    基于模拟或实际事件样本训练的生成对抗网络被提出作为一种以降低计算成本为代价生成大规模模拟数据集的方法。本文展示了一种新的方法，用于从EXO-200实验的时间投影室中模拟光电探测器信号。该方法基于Wasserstein生成对抗网络，这是一种深度学习技术，允许对给定对象集的总体分布进行隐式非参数估计。我们的网络使用原始闪烁波形作为输入，通过对真实校准数据进行训练。我们发现，它能够比传统的模拟方法快一个数量级地产生高质量的模拟波形，并且重要的是，能够从训练样本中推广并识别数据的显著高级特征。特别是，网络正确推断出探测器中闪烁光响应的位置依赖性和相关性。

    Generative Adversarial Networks trained on samples of simulated or actual events have been proposed as a way of generating large simulated datasets at a reduced computational cost. In this work, a novel approach to perform the simulation of photodetector signals from the time projection chamber of the EXO-200 experiment is demonstrated. The method is based on a Wasserstein Generative Adversarial Network - a deep learning technique allowing for implicit non-parametric estimation of the population distribution for a given set of objects. Our network is trained on real calibration data using raw scintillation waveforms as input. We find that it is able to produce high-quality simulated waveforms an order of magnitude faster than the traditional simulation approach and, importantly, generalize from the training sample and discern salient high-level features of the data. In particular, the network correctly deduces position dependency of scintillation light response in the detector and corr
    
[^40]: 驾驶员疲劳检测系统：一种机器学习应用方法

    Driver Drowsiness Detection System: An Approach By Machine Learning Application. (arXiv:2303.06310v1 [cs.LG])

    [http://arxiv.org/abs/2303.06310](http://arxiv.org/abs/2303.06310)

    驾驶员疲劳是导致交通事故的主要原因之一，本研究旨在通过机器学习算法开发一种实时检测驾驶员疲劳的系统。

    Driver drowsiness is one of the main causes of traffic accidents. This study aims to develop a real-time detection system for driver drowsiness using machine learning algorithms.

    交通事故导致大多数人的死亡和伤害。每年有数百万人因交通事故受伤或死亡，这与世界卫生组织的数据一致。没有得到足够睡眠、休息或感到疲倦的驾驶员可能会在驾驶过程中睡着，危及自己和其他道路使用者的安全。研究表明，由于疲劳驾驶而导致的重大道路事故。现在，疲劳驾驶成为发生疲劳的主要原因。现在，疲劳成为增加道路事故数量的主要原因。这成为一个非常重要的问题，需要尽快解决。所有设备的主要目标是提高实时检测疲劳的性能。许多设备已经开发出来，用于检测疲劳，这些设备依赖于不同的人工智能算法。因此，我们的研究也与驾驶员疲劳检测有关。

    The majority of human deaths and injuries are caused by traffic accidents. A million people worldwide die each year due to traffic accident injuries, consistent with the World Health Organization. Drivers who do not receive enough sleep, rest, or who feel weary may fall asleep behind the wheel, endangering both themselves and other road users. The research on road accidents specified that major road accidents occur due to drowsiness while driving. These days, it is observed that tired driving is the main reason to occur drowsiness. Now, drowsiness becomes the main principle for to increase in the number of road accidents. This becomes a major issue in a world which is very important to resolve as soon as possible. The predominant goal of all devices is to improve the performance to detect drowsiness in real time. Many devices were developed to detect drowsiness, which depend on different artificial intelligence algorithms. So, our research is also related to driver drowsiness detection
    
[^41]: 虚拟鼠标和助手：人工智能的技术革命

    Virtual Mouse And Assistant: A Technological Revolution Of Artificial Intelligence. (arXiv:2303.06309v1 [cs.HC])

    [http://arxiv.org/abs/2303.06309](http://arxiv.org/abs/2303.06309)

    本文介绍了虚拟助手的性能提升，虚拟助手是一种能够理解自然语言语音命令并能代表您执行任务的软件，可以完成几乎任何您自己可以完成的特定智能手机或PC活动，而且列表不断扩大。

    This paper introduces the performance improvement of virtual assistants, which are software that understands natural language voice commands and can perform tasks on your behalf. They can complete practically any specific smartphone or PC activity that you can complete on your own, and the list is continually expanding.

    本文旨在提高虚拟助手的性能。那么什么是虚拟助手？应用软件，通常称为虚拟助手，也称为AI助手或数字助手，是一种能够理解自然语言语音命令并能代表您执行任务的软件。虚拟助手可以完成几乎任何您自己可以完成的特定智能手机或PC活动，而且列表不断扩大。虚拟助手通常可以完成各种各样的任务，包括安排会议、发送消息和监控天气。以前的虚拟助手，如Google助手和Cortana，在某些方面有限制，因为它们只能执行搜索，而不是完全自动化。例如，这些引擎没有能力前进和倒带歌曲，以保持歌曲的控制功能；它们只能具有搜索歌曲的模块。

    The purpose of this paper is to enhance the performance of the virtual assistant. So, what exactly is a virtual assistant. Application software, often called virtual assistants, also known as AI assistants or digital assistants, is software that understands natural language voice commands and can perform tasks on your behalf. What does a virtual assistant do. Virtual assistants can complete practically any specific smartphone or PC activity that you can complete on your own, and the list is continually expanding. Virtual assistants typically do an impressive variety of tasks, including scheduling meetings, delivering messages, and monitoring the weather. Previous virtual assistants, like Google Assistant and Cortana, had limits in that they could only perform searches and were not entirely automated. For instance, these engines do not have the ability to forward and rewind the song in order to maintain the control function of the song; they can only have the module to search for songs 
    
[^42]: 基于区块链的去中心化投票系统安全视角：数字投票系统的安全性和保障

    Blockchain-based decentralized voting system security Perspective: Safe and secure for digital voting system. (arXiv:2303.06306v1 [cs.LG])

    [http://arxiv.org/abs/2303.06306](http://arxiv.org/abs/2303.06306)

    本文研究了基于区块链的去中心化投票系统，提出了一种独特的身份识别方式，使得每个人都能追踪投票欺诈，系统非常安全。

    This paper studies the blockchain-based decentralized voting system and proposes a unique identification method that enables everyone to trace vote fraud, making the system incredibly safe.

    本研究主要关注基于区块链的投票系统，为选民、候选人和官员参与和管理投票提供便利。由于我们在后端使用了区块链，使得每个人都能追踪投票欺诈，因此我们的系统非常安全。本文提出了一种独特的身份识别方式，即使用Aadhar卡号或OTP生成，然后用户可以利用投票系统投票。提出了比特币的建议，比特币是一种虚拟货币系统，由中央机构决定生产货币、转移所有权和验证交易，包括点对点网络在区块链系统中，账本在多个相同的数据库中复制，由不同的进程托管和更新，如果对一个节点进行更改并发生交易，则所有其他节点会同时更新，价值和资产的记录将永久交换，只有用户和系统需要进行验证。

    This research study focuses primarily on Block-Chain-based voting systems, which facilitate participation in and administration of voting for voters, candidates, and officials. Because we used Block-Chain in the backend, which enables everyone to trace vote fraud, our system is incredibly safe. This paper approach any unique identification the Aadhar Card number or an OTP will be generated then user can utilise the voting system to cast his/her vote. A proposal for Bit-coin, a virtual currency system that is decided by a central authority for producing money, transferring ownership, and validating transactions, included the peer-to-peer network in a Block-Chain system, the ledger is duplicated across several, identical databases which is hosted and updated by a different process and all other nodes are updated concurrently if changes made to one node and a transaction occurs, the records of the values and assets are permanently exchanged, Only the user and the system need to be verifie
    
[^43]: 机器学习网络中的对抗攻击与防御：现代综述

    Adversarial Attacks and Defenses in Machine Learning-Powered Networks: A Contemporary Survey. (arXiv:2303.06302v1 [cs.LG])

    [http://arxiv.org/abs/2303.06302](http://arxiv.org/abs/2303.06302)

    本文综述了机器学习网络中的对抗攻击和防御技术，重点关注基于深度神经网络的分类模型。对最近的对抗攻击方法和最先进的对抗防御技术进行了全面分类，并以视觉上吸引人的表格和树状图的形式呈现。方法分类为反攻击检测和鲁棒性增强，特别关注于基于正则化的增强鲁棒性的方法。还探讨了新的攻击途径，包括基于搜索的攻击。

    This survey provides a comprehensive overview of recent advancements in adversarial attack and defense techniques in machine learning and deep neural networks, with a focus on deep neural network-based classification models. The methods are classified into counter-attack detection and robustness enhancement, with a specific focus on regularization-based methods for enhancing robustness. New avenues of attack are also explored, including search-based attacks.

    由于深度学习在互联网和相关场景中的快速增长应用，机器学习和深度神经网络中的对抗攻击和防御已经引起了极大的关注。本综述全面概述了对抗攻击和防御技术领域的最新进展，重点关注基于深度神经网络的分类模型。具体而言，我们根据攻击原理对最近的对抗攻击方法和最先进的对抗防御技术进行了全面分类，并以视觉上吸引人的表格和树状图的形式呈现。这是基于对现有工作的严格评估，包括对其优点和局限性的分析。我们还将方法分类为反攻击检测和鲁棒性增强，特别关注于基于正则化的增强鲁棒性的方法。还探讨了新的攻击途径，包括基于搜索的攻击。

    Adversarial attacks and defenses in machine learning and deep neural network have been gaining significant attention due to the rapidly growing applications of deep learning in the Internet and relevant scenarios. This survey provides a comprehensive overview of the recent advancements in the field of adversarial attack and defense techniques, with a focus on deep neural network-based classification models. Specifically, we conduct a comprehensive classification of recent adversarial attack methods and state-of-the-art adversarial defense techniques based on attack principles, and present them in visually appealing tables and tree diagrams. This is based on a rigorous evaluation of the existing works, including an analysis of their strengths and limitations. We also categorize the methods into counter-attack detection and robustness enhancement, with a specific focus on regularization-based methods for enhancing robustness. New avenues of attack are also explored, including search-base
    
[^44]: MLP-SRGAN: 使用MLP-Mixer的单维超分辨率GAN

    MLP-SRGAN: A Single-Dimension Super Resolution GAN using MLP-Mixer. (arXiv:2303.06298v1 [cs.CV])

    [http://arxiv.org/abs/2303.06298](http://arxiv.org/abs/2303.06298)

    MLP-SRGAN是一种单维超分辨率GAN，使用MLP-Mixer和卷积层进行上采样，可用于FLAIR MRI图像的超分辨率重建，提出了新的图像质量度量方法。

    MLP-SRGAN is a single-dimension Super Resolution GAN that utilizes MLP-Mixers and convolutional layers for upsampling, and can be used for super-resolution reconstruction of FLAIR MRI images. New image quality metrics were proposed.

    我们提出了一种新的架构，称为MLP-SRGAN，它是一种单维超分辨率生成对抗网络（SRGAN），利用多层感知器混合器（MLP-Mixer）以及卷积层在切片方向上进行上采样。 MLP-SRGAN使用MSSEG2挑战数据集中的高分辨率（HR）FLAIR MRI进行训练和验证。该方法应用于三个低空间分辨率的多中心FLAIR数据集（CAIN，ADNI，CCNA）的图像，以检查在保留（未见）临床数据上的性能。将上采样结果与几种最先进的SR网络进行比较。对于具有高分辨率（HR）基本事实的图像，使用峰值信噪比（PSNR）和结构相似性指数（SSIM）来衡量上采样性能。提出了几种新的结构，无参考图像质量度量，以在缺乏基础事实的情况下量化锐度（边缘强度），噪声（熵）和模糊度（低频信息）。

    We propose a novel architecture called MLP-SRGAN, which is a single-dimension Super Resolution Generative Adversarial Network (SRGAN) that utilizes Multi-Layer Perceptron Mixers (MLP-Mixers) along with convolutional layers to upsample in the slice direction. MLP-SRGAN is trained and validated using high resolution (HR) FLAIR MRI from the MSSEG2 challenge dataset. The method was applied to three multicentre FLAIR datasets (CAIN, ADNI, CCNA) of images with low spatial resolution in the slice dimension to examine performance on held-out (unseen) clinical data. Upsampled results are compared to several state-of-the-art SR networks. For images with high resolution (HR) ground truths, peak-signal-to-noise-ratio (PSNR) and structural similarity index (SSIM) are used to measure upsampling performance. Several new structural, no-reference image quality metrics were proposed to quantify sharpness (edge strength), noise (entropy), and blurriness (low frequency information) in the absence of groun
    
[^45]: 防止注意力熵崩溃的Transformer训练稳定性研究

    Stabilizing Transformer Training by Preventing Attention Entropy Collapse. (arXiv:2303.06296v1 [cs.LG])

    [http://arxiv.org/abs/2303.06296](http://arxiv.org/abs/2303.06296)

    本文研究了Transformer的训练动态，发现低注意力熵伴随着高训练不稳定性，提出了一种简单而有效的解决方案$\sigma$Reparam，成功地防止了注意力层中的熵崩溃，促进了更稳定的训练。

    This paper investigates the training dynamics of Transformers and proposes a simple and efficient solution, $\sigma$Reparam, to prevent entropy collapse in the attention layers, promoting more stable training.

    训练稳定性对于Transformer至关重要。本文通过研究注意力层的演变来探究Transformer的训练动态。特别地，我们在训练过程中跟踪每个注意力头的注意力熵，这是模型锐度的代理。我们发现，在不同的架构和任务中存在一种常见模式，即低注意力熵伴随着高训练不稳定性，这可能采取振荡损失或发散的形式。我们将病态低注意力熵，对应高度集中的注意力分数，称为$\textit{熵崩溃}$。作为一种解决方案，我们提出了$\sigma$Reparam，一种简单而有效的解决方案，其中我们使用谱归一化和额外的学习标量重新参数化所有线性层。我们证明了所提出的重新参数化成功地防止了注意力层中的熵崩溃，促进了更稳定的训练。此外，我们

    Training stability is of great importance to Transformers. In this work, we investigate the training dynamics of Transformers by examining the evolution of the attention layers. In particular, we track the attention entropy for each attention head during the course of training, which is a proxy for model sharpness. We identify a common pattern across different architectures and tasks, where low attention entropy is accompanied by high training instability, which can take the form of oscillating loss or divergence. We denote the pathologically low attention entropy, corresponding to highly concentrated attention scores, as $\textit{entropy collapse}$. As a remedy, we propose $\sigma$Reparam, a simple and efficient solution where we reparametrize all linear layers with spectral normalization and an additional learned scalar. We demonstrate that the proposed reparameterization successfully prevents entropy collapse in the attention layers, promoting more stable training. Additionally, we 
    
[^46]: 流式网络嵌入中的空间不变投影

    Space-Invariant Projection in Streaming Network Embedding. (arXiv:2303.06293v1 [cs.SI])

    [http://arxiv.org/abs/2303.06293](http://arxiv.org/abs/2303.06293)

    本文提供了一个最大新节点数量的阈值，该阈值使节点嵌入空间保持近似等效，并提出了一种生成框架，称为空间不变投影（SIP），使任意静态MF嵌入方案能够快速嵌入动态网络中的新节点。

    This paper provides a threshold for the maximum number of new nodes that keep the node embedding space approximately equivalent, and proposes a generation framework called Space-Invariant Projection (SIP) to enable fast embedding of new nodes in dynamic networks using any static MF-based embedding scheme.

    动态网络中新到达的节点会逐渐使节点嵌入空间漂移，因此需要重新训练节点嵌入和下游模型。然而，很少有人在理论或实验中考虑这些新节点的确切阈值大小，即使这些新节点的大小低于某个阈值，节点嵌入空间也很难被维护。本文从矩阵扰动理论的角度提供了一个最大新节点数量的阈值，该阈值使节点嵌入空间保持近似等效，并经过了实证验证。因此，理论上保证了当新到达节点的数量低于此阈值时，这些新节点的嵌入可以快速从原始节点的嵌入中导出。因此，提出了一种生成框架，称为空间不变投影（SIP），使任意静态MF嵌入方案能够快速嵌入动态网络中的新节点。SIP的时间复杂度与网络大小成线性关系。

    Newly arriving nodes in dynamics networks would gradually make the node embedding space drifted and the retraining of node embedding and downstream models indispensable. An exact threshold size of these new nodes, below which the node embedding space will be predicatively maintained, however, is rarely considered in either theory or experiment. From the view of matrix perturbation theory, a threshold of the maximum number of new nodes that keep the node embedding space approximately equivalent is analytically provided and empirically validated. It is therefore theoretically guaranteed that as the size of newly arriving nodes is below this threshold, embeddings of these new nodes can be quickly derived from embeddings of original nodes. A generation framework, Space-Invariant Projection (SIP), is accordingly proposed to enables arbitrary static MF-based embedding schemes to embed new nodes in dynamics networks fast. The time complexity of SIP is linear with the network size. By combinin
    
[^47]: 机器学习增强的Hankel动态模态分解

    Machine Learning Enhanced Hankel Dynamic-Mode Decomposition. (arXiv:2303.06289v1 [cs.LG])

    [http://arxiv.org/abs/2303.06289](http://arxiv.org/abs/2303.06289)

    本文提出了一种基于深度学习DMD的方法，称为DLHDMD，利用Takens嵌入定理的基本见解开发了一种自适应学习方案，更好地捕捉了高维和混沌动力学，能够为混沌时间序列生成准确的动态。

    This paper proposes a deep learning DMD based method, called DLHDMD, which uses the fundamental insight of Takens' Embedding Theorem to develop an adaptive learning scheme that better captures higher dimensional and chaotic dynamics, and is able to generate accurate dynamics for chaotic time series.

    尽管时间序列的获取变得越来越简单和复杂，但从时间序列中开发动态模型仍然是一个具有挑战性和不断发展的问题领域。在过去几年中，为了解决这个问题，机器学习工具已经与所谓的动态模态分解（DMD）相结合。这种通用方法已被证明是一个特别有前途的精密和准确的模型开发途径。在此基础上，我们开发了一种基于深度学习DMD的方法，利用Takens嵌入定理的基本见解开发了一种自适应学习方案，更好地捕捉了高维和混沌动力学。我们称这种方法为深度学习Hankel DMD（DLHDMD）。我们展示了DLHDMD能够为混沌时间序列生成准确的动态，并探讨了我们的方法如何学习映射，这些映射在成功训练后往往趋向于显著的特征。

    While the acquisition of time series has become increasingly more straightforward and sophisticated, developing dynamical models from time series is still a challenging and ever evolving problem domain. Within the last several years, to address this problem, there has been a merging of machine learning tools with what is called the dynamic mode decomposition (DMD). This general approach has been shown to be an especially promising avenue for sophisticated and accurate model development. Building on this prior body of work, we develop a deep learning DMD based method which makes use of the fundamental insight of Takens' Embedding Theorem to develop an adaptive learning scheme that better captures higher dimensional and chaotic dynamics. We call this method the Deep Learning Hankel DMD (DLHDMD). We show that the DLHDMD is able to generate accurate dynamics for chaotic time series, and we likewise explore how our method learns mappings which tend, after successful training, to significant
    
[^48]: 探究有状态防御黑盒对抗样本

    Investigating Stateful Defenses Against Black-Box Adversarial Examples. (arXiv:2303.06280v1 [cs.CR])

    [http://arxiv.org/abs/2303.06280](http://arxiv.org/abs/2303.06280)

    本文探究了有状态防御黑盒对抗样本的方法，提出了一种新的有状态防御模型，可以在CIFAR10数据集上达到82.2％的准确性，在ImageNet数据集上达到76.5％的准确性。

    This paper investigates stateful defenses against black-box adversarial examples and proposes a new stateful defense model that achieves 82.2% accuracy on the CIFAR10 dataset and 76.5% accuracy on the ImageNet dataset.

    防御机器学习（ML）模型免受白盒对抗攻击已被证明极为困难。相反，最近的工作提出了有状态防御，试图防御更受限制的黑盒攻击者。这些防御通过跟踪传入模型查询的历史记录，并拒绝那些可疑地相似的查询来操作。目前最先进的有状态防御Blacklight是在USENIX Security '22上提出的，声称可以防止几乎100％的CIFAR10和ImageNet数据集上的攻击。在本文中，我们观察到攻击者可以通过简单调整现有黑盒攻击的参数，显著降低受Blacklight保护的分类器的准确性（例如，在CIFAR10上从82.2％降至6.4％）。受到这一惊人观察的启发，我们提供了有状态防御的系统化，以了解为什么现有的有状态防御模型会失败。最后，我们提出了一种新的有状态防御模型，该模型在CIFAR10数据集上的准确性为82.2％，在ImageNet数据集上的准确性为76.5％。

    Defending machine-learning (ML) models against white-box adversarial attacks has proven to be extremely difficult. Instead, recent work has proposed stateful defenses in an attempt to defend against a more restricted black-box attacker. These defenses operate by tracking a history of incoming model queries, and rejecting those that are suspiciously similar. The current state-of-the-art stateful defense Blacklight was proposed at USENIX Security '22 and claims to prevent nearly 100% of attacks on both the CIFAR10 and ImageNet datasets. In this paper, we observe that an attacker can significantly reduce the accuracy of a Blacklight-protected classifier (e.g., from 82.2% to 6.4% on CIFAR10) by simply adjusting the parameters of an existing black-box attack. Motivated by this surprising observation, since existing attacks were evaluated by the Blacklight authors, we provide a systematization of stateful defenses to understand why existing stateful defense models fail. Finally, we propose a
    
[^49]: 结合基于结构的编码器和预训练的蛋白质语言模型的增强

    Enhancing Protein Language Models with Structure-based Encoder and Pre-training. (arXiv:2303.06275v1 [q-bio.QM])

    [http://arxiv.org/abs/2303.06275](http://arxiv.org/abs/2303.06275)

    本文提出了一种结合基于结构的编码器和预训练的蛋白质语言模型，以明确地编码蛋白质结构，获得更好的结构感知蛋白质表示，并在实验中验证了其有效性。

    This paper proposes enhancing protein language models with structure-based encoder and pre-training to explicitly encode protein structures for better structure-aware protein representations, and empirically verifies its effectiveness.

    在大规模蛋白质序列语料库上预训练的蛋白质语言模型（PLMs）在各种下游蛋白质理解任务中取得了令人印象深刻的表现。尽管能够隐式地捕获残基间的接触信息，但基于变压器的PLMs不能明确地编码蛋白质结构，以获得更好的结构感知蛋白质表示。此外，尽管结构对于确定功能很重要，但尚未探索在可用蛋白质结构上进行预训练以改进这些PLMs的能力。为了解决这些限制，我们在本文中使用基于结构的编码器和预训练来增强PLMs。

    Protein language models (PLMs) pre-trained on large-scale protein sequence corpora have achieved impressive performance on various downstream protein understanding tasks. Despite the ability to implicitly capture inter-residue contact information, transformer-based PLMs cannot encode protein structures explicitly for better structure-aware protein representations. Besides, the power of pre-training on available protein structures has not been explored for improving these PLMs, though structures are important to determine functions. To tackle these limitations, in this work, we enhance the PLMs with structure-based encoder and pre-training. We first explore feasible model architectures to combine the advantages of a state-of-the-art PLM (i.e., ESM-1b1) and a state-of-the-art protein structure encoder (i.e., GearNet). We empirically verify the ESM-GearNet that connects two encoders in a series way as the most effective combination model. To further improve the effectiveness of ESM-GearNe
    
[^50]: CoNIC挑战：推动核检测、分割、分类和计数的前沿（arXiv:2303.06274v1 [cs.CV]）

    CoNIC Challenge: Pushing the Frontiers of Nuclear Detection, Segmentation, Classification and Counting. (arXiv:2303.06274v1 [cs.CV])

    [http://arxiv.org/abs/2303.06274](http://arxiv.org/abs/2303.06274)

    CoNIC挑战使用最大的数据集评估核分割和细胞组成，刺激了可重复的细胞识别算法的开发，发现嗜酸性粒细胞和中性粒细胞在肿瘤中发挥重要作用。

    The CoNIC challenge used the largest dataset to evaluate nuclear segmentation and cellular composition, stimulated the development of reproducible algorithms for cellular recognition, and found that eosinophils and neutrophils play an important role in tumors.

    核检测、分割和形态测量是帮助我们进一步了解组织学和患者预后关系的关键。为了推动这一领域的创新，我们使用目前最大的数据集设置了一个社区广泛的挑战，以评估核分割和细胞组成。我们的挑战名为CoNIC，刺激了可重复的细胞识别算法的开发，并在公共排行榜上进行实时结果检查。我们基于1,658个结肠组织的全切片图像对表现最佳的模型进行了广泛的后挑战分析。每个模型检测到约7亿个细胞核，相关特征用于不良增生分级和生存分析，我们证明了挑战对先前最先进技术的改进导致了下游性能的显著提升。我们的发现还表明，嗜酸性粒细胞和中性粒细胞在肿瘤中发挥重要作用。

    Nuclear detection, segmentation and morphometric profiling are essential in helping us further understand the relationship between histology and patient outcome. To drive innovation in this area, we setup a community-wide challenge using the largest available dataset of its kind to assess nuclear segmentation and cellular composition. Our challenge, named CoNIC, stimulated the development of reproducible algorithms for cellular recognition with real-time result inspection on public leaderboards. We conducted an extensive post-challenge analysis based on the top-performing models using 1,658 whole-slide images of colon tissue. With around 700 million detected nuclei per model, associated features were used for dysplasia grading and survival analysis, where we demonstrated that the challenge's improvement over the previous state-of-the-art led to significant boosts in downstream performance. Our findings also suggest that eosinophils and neutrophils play an important role in the tumour m
    
[^51]: DEPLOYR：将自定义实时机器学习模型部署到电子病历的技术框架

    DEPLOYR: A technical framework for deploying custom real-time machine learning models into the electronic medical record. (arXiv:2303.06269v1 [cs.LG])

    [http://arxiv.org/abs/2303.06269](http://arxiv.org/abs/2303.06269)

    DEPLOYR是一个技术框架，可以将研究人员创建的临床机器学习模型实时部署到广泛使用的电子病历系统中，并提供监控和反馈机制。

    DEPLOYR is a technical framework that enables real-time deployment and monitoring of researcher-created clinical machine learning models into widely used electronic medical record systems, with mechanisms for triggering inference, collecting real-time data, displaying inferences back to end-users, monitoring performance, and silent deployment.

    在医疗保健领域，机器学习（ML）应用得到了广泛研究，但成功的转化却很少。医疗机构正在建立框架来管理和促进准确、可操作和可靠的模型的实施，这些模型与临床工作流程相结合。这样的治理框架需要一个相应的技术框架，以资源有效的方式部署模型。在这里，我们介绍了DEPLOYR，一个技术框架，用于实时部署和监控研究人员创建的临床ML模型到广泛使用的电子病历（EMR）系统中。我们讨论了核心功能和设计决策，包括基于EMR软件内部操作触发推理的机制、收集实时数据以进行推理的模块、通过显示推理结果回馈给最终用户的机制、跟踪部署模型性能的监控模块、静默部署等机制。

    Machine learning (ML) applications in healthcare are extensively researched, but successful translations to the bedside are scant. Healthcare institutions are establishing frameworks to govern and promote the implementation of accurate, actionable and reliable models that integrate with clinical workflow. Such governance frameworks require an accompanying technical framework to deploy models in a resource efficient manner. Here we present DEPLOYR, a technical framework for enabling real-time deployment and monitoring of researcher created clinical ML models into a widely used electronic medical record (EMR) system. We discuss core functionality and design decisions, including mechanisms to trigger inference based on actions within EMR software, modules that collect real-time data to make inferences, mechanisms that close-the-loop by displaying inferences back to end-users within their workflow, monitoring modules that track performance of deployed models over time, silent deployment ca
    
[^52]: 量子机器学习实现：提议和实验

    Quantum Machine Learning Implementations: Proposals and Experiments. (arXiv:2303.06263v1 [quant-ph])

    [http://arxiv.org/abs/2303.06263](http://arxiv.org/abs/2303.06263)

    本文概述了量子机器学习领域中最近的理论提议及其实验实现，重点回顾了量子强化学习、量子自编码器和量子记忆电阻器等高影响主题，并强调了推动这项技术的初步量子实现的必要性。

    This article provides an overview of recent theoretical proposals and experimental implementations in the field of quantum machine learning, with a focus on high-impact topics such as quantum reinforcement learning, quantum autoencoders, and quantum memristors. The article emphasizes the necessity of pushing forward initial quantum implementations of this technology to achieve better machine learning calculations than any current or future computing paradigm.

    本文概述了量子机器学习领域中最近的理论提议及其实验实现，并回顾了特定的高影响主题，如量子强化学习、量子自编码器和量子记忆电阻器，以及它们在量子光子学和超导电路平台上的实验实现。量子机器学习领域可能是首批为工业和社会带来益处的量子技术之一。因此，有必要推动这项技术的初步量子实现，以在嘈杂的中间规模量子计算机上实现比任何当前或未来计算范式更好的机器学习计算。

    This article gives an overview and a perspective of recent theoretical proposals and their experimental implementations in the field of quantum machine learning. Without an aim to being exhaustive, the article reviews specific high-impact topics such as quantum reinforcement learning, quantum autoencoders, and quantum memristors, and their experimental realizations in the platforms of quantum photonics and superconducting circuits. The field of quantum machine learning could be among the first quantum technologies producing results that are beneficial for industry and, in turn, to society. Therefore, it is necessary to push forward initial quantum implementations of this technology, in Noisy Intermediate-Scale Quantum Computers, aiming for achieving fruitful calculations in machine learning that are better than with any other current or future computing paradigm.
    
[^53]: 可解释的异常值汇总

    Interpretable Outlier Summarization. (arXiv:2303.06261v1 [cs.LG])

    [http://arxiv.org/abs/2303.06261](http://arxiv.org/abs/2303.06261)

    STAIR提出了一种可解释的异常值汇总方法，通过学习一组紧凑的人类可理解规则，以汇总和解释异常检测结果，具有强大的可解释性，以准确地总结检测结果。

    STAIR proposes an interpretable outlier summarization method by learning a compact set of human understandable rules to summarize and explain the anomaly detection results, which has strong interpretability to accurately summarize the detection results.

    异常值检测在实际应用中是至关重要的，以防止金融欺诈、防御网络入侵或检测即将发生的设备故障。为了减少人力评估异常值检测结果的工作量，并有效地将异常值转化为可操作的见解，用户通常希望系统自动产生可解释的异常值检测结果的子组的汇总。然而，到目前为止，没有这样的系统存在。为了填补这一空白，我们提出了STAIR，它学习了一组紧凑的人类可理解规则，以汇总和解释异常检测结果。STAIR不使用经典的决策树算法来产生这些规则，而是提出了一个新的优化目标，以产生少量规则，具有最小的复杂性，因此具有强大的可解释性，以准确地总结检测结果。STAIR的学习算法通过迭代分割大规则来产生规则集，并在每个i中最大化这个目标，是最优的。

    Outlier detection is critical in real applications to prevent financial fraud, defend network intrusions, or detecting imminent device failures. To reduce the human effort in evaluating outlier detection results and effectively turn the outliers into actionable insights, the users often expect a system to automatically produce interpretable summarizations of subgroups of outlier detection results. Unfortunately, to date no such systems exist. To fill this gap, we propose STAIR which learns a compact set of human understandable rules to summarize and explain the anomaly detection results. Rather than use the classical decision tree algorithms to produce these rules, STAIR proposes a new optimization objective to produce a small number of rules with least complexity, hence strong interpretability, to accurately summarize the detection results. The learning algorithm of STAIR produces a rule set by iteratively splitting the large rules and is optimal in maximizing this objective in each i
    
[^54]: ICU环境噪声和光线信息预测谵妄风险

    Predicting risk of delirium from ambient noise and light information in the ICU. (arXiv:2303.06253v1 [cs.LG])

    [http://arxiv.org/abs/2303.06253](http://arxiv.org/abs/2303.06253)

    本研究利用环境噪声和光线信息，开发了第一个基于深度学习的ICU患者谵妄预测模型，为谵妄的预防和治疗提供了新思路。

    This study developed the first deep-learning based delirium prediction model for ICU patients using only ambient noise and light information, providing new insights for the prevention and treatment of delirium.

    现有的重症监护室（ICU）谵妄预测模型没有考虑环境因素，尽管有强有力的证据表明它们对谵妄有影响。本研究利用Thunderboard、ActiGraph传感器和iPod with AudioTools应用程序，仅使用环境噪声和光线信息，报道了第一个基于深度学习的ICU患者谵妄预测模型。这些测量数据从2021年5月至2022年9月收集自102名患者的ICU病房，分为白天（0700至1859）和夜间（1900至0659）。使用这些数据训练深度学习模型，预测ICU住院期间或出院后4天内的谵妄发生率。最后，分析结果得分以评估每个特征的重要性和方向性。白天的噪声水平显著高于夜间的噪声水平。当仅使用噪声特征或噪声和光特征的组合时，1-D卷积神经网络

    Existing Intensive Care Unit (ICU) delirium prediction models do not consider environmental factors despite strong evidence of their influence on delirium. This study reports the first deep-learning based delirium prediction model for ICU patients using only ambient noise and light information. Ambient light and noise intensities were measured from ICU rooms of 102 patients from May 2021 to September 2022 using Thunderboard, ActiGraph sensors and an iPod with AudioTools application. These measurements were divided into daytime (0700 to 1859) and nighttime (1900 to 0659). Deep learning models were trained using this data to predict the incidence of delirium during ICU stay or within 4 days of discharge. Finally, outcome scores were analyzed to evaluate the importance and directionality of every feature. Daytime noise levels were significantly higher than nighttime noise levels. When using only noise features or a combination of noise and light features 1-D convolutional neural networks 
    
[^55]: 基于区域的联邦学习用于移动感知数据

    Zone-based Federated Learning for Mobile Sensing Data. (arXiv:2303.06246v1 [cs.LG])

    [http://arxiv.org/abs/2303.06246](http://arxiv.org/abs/2303.06246)

    本文提出了一种基于区域的联邦学习方法，用于训练移动感知数据的深度学习模型。该方法将物理空间划分为地理区域，并映射到移动边缘云系统架构，以实现良好的模型准确性和可扩展性。每个区域都有一个联合训练模型，能够很好地适应该区域用户的数据和行为，并保护用户数据隐私。

    This paper proposes a zone-based federated learning method for training deep learning models with mobile sensing data. The method divides the physical space into geographical zones and maps them to a mobile-edge-cloud system architecture for good model accuracy and scalability. Each zone has a federated training model that adapts well to the data and behaviors of users in that zone, while protecting user data privacy.

    移动应用程序，如mHealth和健康应用程序，可以从使用智能手机或可穿戴设备收集的移动感知数据训练的深度学习（DL）模型中受益。然而，目前没有移动感知DL系统能够同时实现良好的模型准确性，适应用户的移动行为，随着用户数量的增加而扩展，并保护用户数据隐私。我们提出了基于区域的联邦学习（ZoneFL）来解决这些要求。ZoneFL将物理空间划分为地理区域，映射到移动边缘云系统架构，以实现良好的模型准确性和可扩展性。每个区域都有一个联合训练模型，称为区域模型，它能够很好地适应该区域用户的数据和行为。受益于FL设计，ZoneFL培训期间保护用户数据隐私。我们提出了两种新颖的基于区域的联合训练算法来优化区域模型以适应用户的移动行为：区域合并和分裂（ZMS）和Zo

    Mobile apps, such as mHealth and wellness applications, can benefit from deep learning (DL) models trained with mobile sensing data collected by smart phones or wearable devices. However, currently there is no mobile sensing DL system that simultaneously achieves good model accuracy while adapting to user mobility behavior, scales well as the number of users increases, and protects user data privacy. We propose Zone-based Federated Learning (ZoneFL) to address these requirements. ZoneFL divides the physical space into geographical zones mapped to a mobile-edge-cloud system architecture for good model accuracy and scalability. Each zone has a federated training model, called a zone model, which adapts well to data and behaviors of users in that zone. Benefiting from the FL design, the user data privacy is protected during the ZoneFL training. We propose two novel zone-based federated training algorithms to optimize zone models to user mobility behavior: Zone Merge and Split (ZMS) and Zo
    
[^56]: 基于双视角的超似曲自适应学习用于自监督骨架动作表示

    HYperbolic Self-Paced Learning for Self-Supervised Skeleton-based Action Representations. (arXiv:2303.06242v1 [cs.CV])

    [http://arxiv.org/abs/2303.06242](http://arxiv.org/abs/2303.06242)

    本文提出了一种新的超似曲自适应模型（HYSP）用于学习基于骨架的动作表示，采用自监督学习，使用数据增强来生成同一样本的两个视图，并通过将一个视图与另一个视图匹配来学习，使用超似曲不确定性来确定算法学习速度，假设不确定性较小的样本应更强烈地推动训练，具有更大的权重和速度。

    This paper proposes a novel HYperbolic Self-Paced model (HYSP) for learning skeleton-based action representations, which adopts self-supervision and uses data augmentations to generate two views of the same sample, and learns by matching one to the other. It uses hyperbolic uncertainty to determine the algorithmic learning pace, assuming that less uncertain samples should be more strongly driving the training, with a larger weight and pace.

    自适应学习在一些任务中有益，例如弱监督学习和领域自适应，可以选择和排序训练样本序列，从易到难。然而，它在无监督学习中的适用性仍未被探索，其中任务的知识在训练期间成熟。我们提出了一种新的超似曲自适应模型（HYSP）用于学习基于骨架的动作表示。HYSP采用自监督：它使用数据增强来生成同一样本的两个视图，并通过将一个视图（称为在线）与另一个视图（目标）匹配来学习。我们建议使用超似曲不确定性来确定算法学习速度，假设不确定性较小的样本应更强烈地推动训练，具有更大的权重和速度。超似曲不确定性是采用的超似曲神经网络的副产品，它在训练期间成熟，与额外成本相比，没有额外成本。

    Self-paced learning has been beneficial for tasks where some initial knowledge is available, such as weakly supervised learning and domain adaptation, to select and order the training sample sequence, from easy to complex. However its applicability remains unexplored in unsupervised learning, whereby the knowledge of the task matures during training. We propose a novel HYperbolic Self-Paced model (HYSP) for learning skeleton-based action representations. HYSP adopts self-supervision: it uses data augmentations to generate two views of the same sample, and it learns by matching one (named online) to the other (the target). We propose to use hyperbolic uncertainty to determine the algorithmic learning pace, under the assumption that less uncertain samples should be more strongly driving the training, with a larger weight and pace. Hyperbolic uncertainty is a by-product of the adopted hyperbolic neural networks, it matures during training and it comes with no extra cost, compared to the e
    
[^57]: 对抗训练是否需要使用整个训练数据集？

    Do we need entire training data for adversarial training?. (arXiv:2303.06241v1 [cs.CV])

    [http://arxiv.org/abs/2303.06241](http://arxiv.org/abs/2303.06241)

    本文提出了一种新的对抗训练方法，通过对训练数据集进行筛选，仅使用易受对抗攻击的样本进行训练，从而减少训练时间。

    This paper proposes a new adversarial training method that reduces training time by selecting only the adversarially-prone samples from the training dataset.

    深度神经网络（DNN）被用于解决许多领域的问题，包括自动驾驶汽车和医学图像等安全关键领域。DNN对抗攻击的脆弱性已经被广泛关注。近年来，已经提出了许多方法来通过对抗训练来解决这个问题。几乎所有的方法都会为整个训练数据集生成对抗性示例，从而大大增加了训练时间。我们展示了通过仅使用训练数据的子集进行对抗训练，可以减少任何对抗训练算法的训练时间。为了选择子集，我们从训练数据中过滤出易受对抗攻击的样本。我们对所有训练样本执行简单的对抗攻击，以过滤出这个子集。在这个攻击中，我们向每个像素添加一个小扰动和几条网格线到输入图像中。我们对易受对抗攻击的子集进行对抗训练，并且...

    Deep Neural Networks (DNNs) are being used to solve a wide range of problems in many domains including safety-critical domains like self-driving cars and medical imagery. DNNs suffer from vulnerability against adversarial attacks. In the past few years, numerous approaches have been proposed to tackle this problem by training networks using adversarial training. Almost all the approaches generate adversarial examples for the entire training dataset, thus increasing the training time drastically. We show that we can decrease the training time for any adversarial training algorithm by using only a subset of training data for adversarial training. To select the subset, we filter the adversarially-prone samples from the training data. We perform a simple adversarial attack on all training examples to filter this subset. In this attack, we add a small perturbation to each pixel and a few grid lines to the input image.  We perform adversarial training on the adversarially-prone subset and mi
    
[^58]: 低开销模型剪枝：面向联邦学习的补充稀疏化

    Complement Sparsification: Low-Overhead Model Pruning for Federated Learning. (arXiv:2303.06237v1 [cs.LG])

    [http://arxiv.org/abs/2303.06237](http://arxiv.org/abs/2303.06237)

    本文提出了一种名为补充稀疏化的模型剪枝机制，通过在服务器和客户端之间进行互补和协作的剪枝来满足联邦学习中低双向通信开销、客户端低计算开销和良好模型准确性的要求。

    This paper proposes a model pruning mechanism called Complement Sparsification (CS), which satisfies the requirements of low bidirectional communication overhead between the server and the clients, low computation overhead at the clients, and good model accuracy in federated learning through complementary and collaborative pruning done at the server and the clients.

    联邦学习（FL）是一种隐私保护的分布式深度学习范例，涉及大量通信和计算工作，这对于资源受限的移动和物联网设备是一个问题。模型剪枝/稀疏化开发了可以解决此问题的稀疏模型，但现有的稀疏化解决方案不能同时满足服务器和客户端之间低双向通信开销、客户端低计算开销和良好模型准确性的要求，在FL假设下，服务器无法访问原始数据以微调修剪的模型。我们提出了补充稀疏化（CS），这是一种剪枝机制，通过在服务器和客户端之间进行互补和协作的剪枝来满足所有这些要求。在每一轮中，CS创建一个全局稀疏模型，其中包含捕获所有客户端的一般数据分布的权重，而客户端则创建本地稀疏模型。

    Federated Learning (FL) is a privacy-preserving distributed deep learning paradigm that involves substantial communication and computation effort, which is a problem for resource-constrained mobile and IoT devices. Model pruning/sparsification develops sparse models that could solve this problem, but existing sparsification solutions cannot satisfy at the same time the requirements for low bidirectional communication overhead between the server and the clients, low computation overhead at the clients, and good model accuracy, under the FL assumption that the server does not have access to raw data to fine-tune the pruned models. We propose Complement Sparsification (CS), a pruning mechanism that satisfies all these requirements through a complementary and collaborative pruning done at the server and the clients. At each round, CS creates a global sparse model that contains the weights that capture the general data distribution of all clients, while the clients create local sparse model
    
[^59]: 带张量自编码器的压缩感知

    Compressive Sensing with Tensorized Autoencoder. (arXiv:2303.06235v1 [cs.CV])

    [http://arxiv.org/abs/2303.06235](http://arxiv.org/abs/2303.06235)

    本文提出了一种利用张量环因子分解的自动编码器来恢复图像的方法，该方法在修复和去噪应用中表现出更好的重建质量。

    This paper proposes a method for image recovery using an autoencoder with tensor ring factorization, which achieves better reconstruction quality in inpainting and denoising applications.

    深度网络可以训练成将图像映射到低维潜在空间的工具。在许多情况下，集合中的不同图像是彼此的关节版本；例如，同一物体具有不同的照明、背景或姿势。此外，在许多情况下，图像的某些部分可能会受到噪声或缺失条目的影响。在本文中，我们的目标是利用数据的结构先验来恢复图像，而没有访问地面真实（干净）图像。这样的恢复问题属于压缩感知领域。我们建议在嵌入空间上学习带有张量环因子分解的自动编码器，以对数据施加结构约束。特别地，我们在自动编码器的瓶颈层中使用张量环结构，利用结构化数据集的软标签。我们通过实验证明了所提出的方法在修复和去噪应用中的有效性。所得到的方法实现了更好的重建质量。

    Deep networks can be trained to map images into a low-dimensional latent space. In many cases, different images in a collection are articulated versions of one another; for example, same object with different lighting, background, or pose. Furthermore, in many cases, parts of images can be corrupted by noise or missing entries. In this paper, our goal is to recover images without access to the ground-truth (clean) images using the articulations as structural prior of the data. Such recovery problems fall under the domain of compressive sensing. We propose to learn autoencoder with tensor ring factorization on the the embedding space to impose structural constraints on the data. In particular, we use a tensor ring structure in the bottleneck layer of the autoencoder that utilizes the soft labels of the structured dataset. We empirically demonstrate the effectiveness of the proposed approach for inpainting and denoising applications. The resulting method achieves better reconstruction qu
    
[^60]: 从人类反应数据中进行最优和私密学习

    Optimal and Private Learning from Human Response Data. (arXiv:2303.06234v1 [cs.LG])

    [http://arxiv.org/abs/2303.06234](http://arxiv.org/abs/2303.06234)

    本文提出了一种新的谱估计算法，具有高效和准确的特点，可以在温和的采样条件下实现极小化最优误差界限，同时在top-$K$恢复的样本复杂度方面具有最优性。

    This paper proposes a new spectral estimation algorithm that is efficient and accurate, achieving the minimax optimal error bound (modulo a log factor) under mild sampling conditions, and enjoys optimal sample complexity for top-$K$ recovery.

    项目反应理论（IRT）是研究人们如何做出概率决策的学科，具有教育测试、推荐系统等多种应用。二元反应数据的Rasch模型是IRT中最基本的模型之一，仍然是一个具有重要实际意义的研究领域。最近，Nguyen和Zhang（2022）提出了一种新的谱估计算法，具有高效和准确的特点。在本文中，我们以两种重要的方式扩展了他们的结果。首先，我们获得了谱算法的精细逐项误差界限，补充了他们工作中的“平均误差”$\ell_2$界限。值得注意的是，在温和的采样条件下，谱算法实现了极小化最优误差界限（模除对数因子）。在精细分析的基础上，我们还展示了谱算法在top-$K$恢复的样本复杂度方面具有最优性（例如，从批准/不批准反应数据中识别最佳的$K$个项目），解释了实验结果。

    Item response theory (IRT) is the study of how people make probabilistic decisions, with diverse applications in education testing, recommendation systems, among others. The Rasch model of binary response data, one of the most fundamental models in IRT, remains an active area of research with important practical significance. Recently, Nguyen and Zhang (2022) proposed a new spectral estimation algorithm that is efficient and accurate. In this work, we extend their results in two important ways. Firstly, we obtain a refined entrywise error bound for the spectral algorithm, complementing the `average error' $\ell_2$ bound in their work. Notably, under mild sampling conditions, the spectral algorithm achieves the minimax optimal error bound (modulo a log factor). Building on the refined analysis, we also show that the spectral algorithm enjoys optimal sample complexity for top-$K$ recovery (e.g., identifying the best $K$ items from approval/disapproval response data), explaining the empir
    
[^61]: MCROOD: 多类雷达超出分布检测

    MCROOD: Multi-Class Radar Out-Of-Distribution Detection. (arXiv:2303.06232v1 [cs.CV])

    [http://arxiv.org/abs/2303.06232](http://arxiv.org/abs/2303.06232)

    本文提出了一种基于重建的多类OOD检测器，该检测器在雷达距离多普勒图像（RDIs）上运行。检测器旨在将除坐、站或走的人以外的任何移动物体分类为OOD。作者还提供了一种简单而有效的预处理技术，以检测呼吸等微小的人体运动。在实验中，该方法表现优于最先进的OOD检测方法。

    This paper proposes a reconstruction-based multi-class OOD detector that operates on radar range doppler images (RDIs). The detector aims to classify any moving object other than a person sitting, standing, or walking as OOD. The authors also provide a simple yet effective pre-processing technique to detect minor human body movements like breathing. The method outperforms state-of-the-art OOD detection methods in experiments.

    最近，由于其在安全部署现代深度学习（DL）架构中的关键作用，超出分布（OOD）检测受到特别关注。本文提出了一种基于重建的多类OOD检测器，该检测器在雷达距离多普勒图像（RDIs）上运行。检测器旨在将除坐、站或走的人以外的任何移动物体分类为OOD。我们还提供了一种简单而有效的预处理技术，以检测呼吸等微小的人体运动。这个简单的想法被称为呼吸检测器（RESPD），可以减轻OOD检测的负担，特别是对于人坐和人站的类别。在我们收集的60GHz短距离FMCW雷达数据集上，我们分别为坐、站和走三个类别实现了97.45％、92.13％和96.58％的AUROC。我们进行了大量实验，并表明我们的方法优于最先进的OOD检测方法。此外，我们的流程比第二好的方法快24倍，并且是v

    Out-of-distribution (OOD) detection has recently received special attention due to its critical role in safely deploying modern deep learning (DL) architectures. This work proposes a reconstruction-based multi-class OOD detector that operates on radar range doppler images (RDIs). The detector aims to classify any moving object other than a person sitting, standing, or walking as OOD. We also provide a simple yet effective pre-processing technique to detect minor human body movements like breathing. The simple idea is called respiration detector (RESPD) and eases the OOD detection, especially for human sitting and standing classes. On our dataset collected by 60GHz short-range FMCW Radar, we achieve AUROCs of 97.45%, 92.13%, and 96.58% for sitting, standing, and walking classes, respectively. We perform extensive experiments and show that our method outperforms state-of-the-art (SOTA) OOD detection methods. Also, our pipeline performs 24 times faster than the second-best method and is v
    
[^62]: CHGNN: 一种半监督对比超图学习网络

    CHGNN: A Semi-Supervised Contrastive Hypergraph Learning Network. (arXiv:2303.06213v1 [cs.LG])

    [http://arxiv.org/abs/2303.06213](http://arxiv.org/abs/2303.06213)

    CHGNN是一种半监督对比超图学习网络，利用自监督对比学习技术从标记和未标记的数据中学习，包括自适应超图视图生成器、改进的超图编码器和联合损失函数。

    CHGNN is a semi-supervised contrastive hypergraph learning network that exploits self-supervised contrastive learning techniques to learn from labeled and unlabeled data. It includes an adaptive hypergraph view generator, an improved hypergraph encoder, and a joint loss function.

    超图可以模拟应用程序中发现的数据对象之间的高阶关系，例如社交网络和生物信息学。然而，最近的超图学习研究将图卷积网络扩展到超图，但无法有效地从未标记数据的特征中学习。为了进行这样的学习，我们提出了一种对比超图神经网络CHGNN，它利用自监督对比学习技术从标记和未标记的数据中学习。首先，CHGNN包括一个自适应超图视图生成器，采用自动增强策略，并学习最小充分视图的扰动概率分布。其次，CHGNN包含一个改进的超图编码器，考虑到超边的同质性，以有效地融合信息。第三，CHGNN配备了一个联合损失函数，结合了视图生成器的相似性损失、节点分类损失和超边同质性损失，注入监督信号。

    Hypergraphs can model higher-order relationships among data objects that are found in applications such as social networks and bioinformatics. However, recent studies on hypergraph learning that extend graph convolutional networks to hypergraphs cannot learn effectively from features of unlabeled data. To such learning, we propose a contrastive hypergraph neural network, CHGNN, that exploits self-supervised contrastive learning techniques to learn from labeled and unlabeled data. First, CHGNN includes an adaptive hypergraph view generator that adopts an auto-augmentation strategy and learns a perturbed probability distribution of minimal sufficient views. Second, CHGNN encompasses an improved hypergraph encoder that considers hyperedge homogeneity to fuse information effectively. Third, CHGNN is equipped with a joint loss function that combines a similarity loss for the view generator, a node classification loss, and a hyperedge homogeneity loss to inject supervision signals. It also i
    
[^63]: 近似最近邻图上最近邻搜索的理论分析

    A Theoretical Analysis Of Nearest Neighbor Search On Approximate Near Neighbor Graph. (arXiv:2303.06210v1 [cs.LG])

    [http://arxiv.org/abs/2303.06210](http://arxiv.org/abs/2303.06210)

    本文提出了解决NN-Search的理论保证，通过贪心搜索ANN-Graph来解决低维密集向量的问题。这些结果提供了与构建近邻图时的近似相关的权衡的量化，为更多可证明的高效基于图的NN-Search算法打开了大门。

    This paper presents theoretical guarantees for solving NN-Search by greedy search on ANN-Graph for low dimensional and dense vectors. The results provide quantification of the trade-offs associated with the approximation while building a near neighbor graph, and open the door for more provable efficient graph-based NN-Search algorithms.

    基于图的算法在最近邻搜索问题中展示了最先进的性能。这些经验成功促使需要理论结果来保证这些算法的搜索质量和效率。然而，在基于图的最近邻搜索算法中存在实践与理论之间的差距。当前的理论文献集中于精确最近邻图上的贪心搜索，而从业者使用近似最近邻图（ANN-Graph）来减少预处理时间。本文通过提供解决低维密集向量的贪心搜索ANN-Graph的NN-Search的理论保证来弥合这一差距。为了建立这座桥梁，我们利用了计算几何学中的几个新颖工具。我们的结果提供了与构建近邻图时的近似相关的权衡的量化。我们希望我们的结果将为更多可证明的高效基于图的NN-Search算法打开大门。

    Graph-based algorithms have demonstrated state-of-the-art performance in the nearest neighbor search (NN-Search) problem. These empirical successes urge the need for theoretical results that guarantee the search quality and efficiency of these algorithms. However, there exists a practice-to-theory gap in the graph-based NN-Search algorithms. Current theoretical literature focuses on greedy search on exact near neighbor graph while practitioners use approximate near neighbor graph (ANN-Graph) to reduce the preprocessing time. This work bridges this gap by presenting the theoretical guarantees of solving NN-Search via greedy search on ANN-Graph for low dimensional and dense vectors. To build this bridge, we leverage several novel tools from computational geometry. Our results provide quantification of the trade-offs associated with the approximation while building a near neighbor graph. We hope our results will open the door for more provable efficient graph-based NN-Search algorithms.
    
[^64]: 利用分区代数快速计算置换等变层

    Fast computation of permutation equivariant layers with the partition algebra. (arXiv:2303.06208v1 [cs.LG])

    [http://arxiv.org/abs/2303.06208](http://arxiv.org/abs/2303.06208)

    本文提出了一种利用分区代数计算置换等变层输出和梯度的新算法，可以在输入大小的线性和二次时间内计算，有效性得到了在几个基准数据集上的证明。

    This paper proposes a new algorithm for computing the output and gradient of permutation equivariant linear layers using the partition algebra, which can be computed in time linear and quadratic in the input size, respectively. The effectiveness of the approach is demonstrated on several benchmark datasets.

    线性神经网络层，无论是等变还是不变于其输入的排列，都是现代深度学习架构的核心构建块。例如DeepSets的层，以及出现在transformers和一些图神经网络的注意力块中的线性层。置换等变线性层的空间可以被识别为某个对称群表示的不变子空间，并且最近的工作通过展示一组基础，其向量是标准基础元素在对称群作用下轨道的总和，来参数化这个空间。参数化打开了通过梯度下降学习置换等变线性层权重的可能性。置换等变线性层的空间是分区代数的一般化，这是一种在统计物理学中首次发现的对象，与对称群的表示论有着深刻的联系，而上述基础与分区代数的基础密切相关。在本文中，我们展示了如何利用这种联系，在输入大小的线性时间内计算置换等变线性层的输出，并在输入大小的二次时间内计算损失相对于权重的梯度。我们的方法基于一种计算分区代数在向量上作用的新算法，我们称之为“分区卷积”。我们展示了分区卷积可以在输入向量大小的线性时间内计算，并且可以用于在输入大小的线性和二次时间内计算置换等变线性层的输出和梯度，分别。我们还展示了如何使用分区卷积来计算某些非线性置换等变层的输出和梯度，并在几个基准数据集上展示了我们方法的有效性。

    Linear neural network layers that are either equivariant or invariant to permutations of their inputs form core building blocks of modern deep learning architectures. Examples include the layers of DeepSets, as well as linear layers occurring in attention blocks of transformers and some graph neural networks. The space of permutation equivariant linear layers can be identified as the invariant subspace of a certain symmetric group representation, and recent work parameterized this space by exhibiting a basis whose vectors are sums over orbits of standard basis elements with respect to the symmetric group action. A parameterization opens up the possibility of learning the weights of permutation equivariant linear layers via gradient descent. The space of permutation equivariant linear layers is a generalization of the partition algebra, an object first discovered in statistical physics with deep connections to the representation theory of the symmetric group, and the basis described abo
    
[^65]: NOvA中基于稀疏CNN和Transformer的可解释联合事件-粒子重建的研究

    Interpretable Joint Event-Particle Reconstruction for Neutrino Physics at NOvA with Sparse CNNs and Transformers. (arXiv:2303.06201v1 [cs.LG])

    [http://arxiv.org/abs/2303.06201](http://arxiv.org/abs/2303.06201)

    本研究提出了一种新颖的神经网络架构TransformerCVN，将卷积和注意力相结合，实现了对NOvA实验中复杂事件中多个粒子的联合分类和重建，为准确测量标准模型的关键参数提供了重要支持。

    This study proposes a novel neural network architecture, TransformerCVN, which combines convolution and attention to achieve joint classification and reconstruction of multiple particles in complex events in the NOvA experiment, providing important support for accurately measuring key parameters of the standard model.

    NOvA长基线中微子振荡实验观测到的复杂事件包含了理解标准模型中最难以捉摸的粒子的重要信息。NOvA探测器观测到来自Fermilab NuMI束流的中微子相互作用。将这些相互作用事件中产生的粒子与它们的源粒子关联起来，即重建，对于准确测量标准模型的关键参数至关重要。事件可能包含多个粒子，每个粒子产生稀疏的高维空间观测数据，当前的方法仅限于评估单个粒子。为了准确标记这些众多的高维观测数据，我们提出了一种新颖的神经网络架构，它将卷积所实现的空间学习与注意力所实现的上下文学习相结合。这种联合方法，TransformerCVN，同时对每个事件进行分类和重建每个单独粒子的身份。

    The complex events observed at the NOvA long-baseline neutrino oscillation experiment contain vital information for understanding the most elusive particles in the standard model. The NOvA detectors observe interactions of neutrinos from the NuMI beam at Fermilab. Associating the particles produced in these interaction events to their source particles, a process known as reconstruction, is critical for accurately measuring key parameters of the standard model. Events may contain several particles, each producing sparse high-dimensional spatial observations, and current methods are limited to evaluating individual particles. To accurately label these numerous, high-dimensional observations, we present a novel neural network architecture that combines the spatial learning enabled by convolutions with the contextual learning enabled by attention. This joint approach, TransformerCVN, simultaneously classifies each event and reconstructs every individual particle's identity. TransformerCVN 
    
[^66]: 克服异方差PCA中病态问题的缩减算法

    Deflated HeteroPCA: Overcoming the curse of ill-conditioning in heteroskedastic PCA. (arXiv:2303.06198v1 [math.ST])

    [http://arxiv.org/abs/2303.06198](http://arxiv.org/abs/2303.06198)

    本文提出了一种新的算法，称为缩减异方差PCA，它在克服病态问题的同时实现了近乎最优和无条件数的理论保证。

    This paper proposes a novel algorithm, called Deflated-HeteroPCA, that overcomes the curse of ill-conditioning in heteroskedastic PCA while achieving near-optimal and condition-number-free theoretical guarantees.

    本文关注于从受污染的数据中估计低秩矩阵X*的列子空间。当存在异方差噪声和不平衡的维度（即n2 >> n1）时，如何在容纳最广泛的信噪比范围的同时获得最佳的统计精度变得特别具有挑战性。虽然最先进的算法HeteroPCA成为解决这个问题的强有力的解决方案，但它遭受了“病态问题的诅咒”，即随着X*的条件数增长，其性能会下降。为了克服这个关键问题而不影响允许的信噪比范围，我们提出了一种新的算法，称为缩减异方差PCA，它在$\ell_2$和$\ell_{2,\infty}$统计精度方面实现了近乎最优和无条件数的理论保证。所提出的算法将谱分成两部分

    This paper is concerned with estimating the column subspace of a low-rank matrix $\boldsymbol{X}^\star \in \mathbb{R}^{n_1\times n_2}$ from contaminated data. How to obtain optimal statistical accuracy while accommodating the widest range of signal-to-noise ratios (SNRs) becomes particularly challenging in the presence of heteroskedastic noise and unbalanced dimensionality (i.e., $n_2\gg n_1$). While the state-of-the-art algorithm $\textsf{HeteroPCA}$ emerges as a powerful solution for solving this problem, it suffers from "the curse of ill-conditioning," namely, its performance degrades as the condition number of $\boldsymbol{X}^\star$ grows. In order to overcome this critical issue without compromising the range of allowable SNRs, we propose a novel algorithm, called $\textsf{Deflated-HeteroPCA}$, that achieves near-optimal and condition-number-free theoretical guarantees in terms of both $\ell_2$ and $\ell_{2,\infty}$ statistical accuracy. The proposed algorithm divides the spectrum
    
[^67]: Papaya：联邦学习，但完全去中心化

    Papaya: Federated Learning, but Fully Decentralized. (arXiv:2303.06189v1 [cs.LG])

    [http://arxiv.org/abs/2303.06189](http://arxiv.org/abs/2303.06189)

    Papaya是一种点对点学习系统，节点在自己的数据上进行训练，并定期根据学习的信任矩阵将其参数与同伴的参数进行加权平均，从而实现联邦学习的去中心化，避免了集中式服务器的带宽和资源密集型限制和隐私问题。

    Papaya is a peer-to-peer learning system that allows nodes to train on their own data and periodically perform a weighted average of their parameters with that of their peers according to a learned trust matrix, achieving decentralized federated learning and avoiding the bandwidth and resource-heavy constraint and privacy concerns of a centralized server.

    联邦学习系统使用集中式服务器来聚合模型更新。这是一种带宽和资源密集型的限制，并暴露系统的隐私问题。相反，我们实现了一种点对点学习系统，其中节点在自己的数据上进行训练，并定期根据学习的信任矩阵将其参数与同伴的参数进行加权平均。到目前为止，我们已经创建了一个模型客户端框架，并使用多个虚拟节点在同一台计算机上运行实验来验证所提出的系统。我们使用了我们提案的第一轮中所述的策略来证明共享参数的点对点学习概念。我们现在希望运行更多实验，并构建一个更可部署的真实世界系统。

    Federated Learning systems use a centralized server to aggregate model updates. This is a bandwidth and resource-heavy constraint and exposes the system to privacy concerns. We instead implement a peer to peer learning system in which nodes train on their own data and periodically perform a weighted average of their parameters with that of their peers according to a learned trust matrix. So far, we have created a model client framework and have been using this to run experiments on the proposed system using multiple virtual nodes which in reality exist on the same computer. We used this strategy as stated in Iteration 1 of our proposal to prove the concept of peer to peer learning with shared parameters. We now hope to run more experiments and build a more deployable real world system for the same.
    
[^68]: 迈向MoE部署：缓解混合专家（MoE）推理中的低效率

    Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert (MoE) Inference. (arXiv:2303.06182v1 [cs.DC])

    [http://arxiv.org/abs/2303.06182](http://arxiv.org/abs/2303.06182)

    本文提出了三种优化技术来缓解混合专家（MoE）模型在推理时的低效率，包括动态门控、专家缓冲和专家负载平衡。这些技术可以显著提高执行时间和减少内存使用。

    This paper proposes three optimization techniques to mitigate inefficiencies in Mixture-of-Experts (MoE) models during inference, including dynamic gating, expert buffering, and expert load balancing. These techniques can significantly improve execution time and reduce memory usage.

    混合专家（MoE）模型最近在计算机视觉和自然语言处理的广泛任务中取得了最先进的性能。它们在训练期间有效地扩展了模型容量，同时增加的计算成本很小。然而，由于其庞大的模型大小和复杂的通信模式，部署这样的模型进行推理是困难的。在这项工作中，我们提供了两个MoE工作负载的特征化，即语言建模（LM）和机器翻译（MT），并确定了它们在部署时的低效率来源。我们提出了三种优化技术来缓解低效率的来源，即（1）动态门控，（2）专家缓冲和（3）专家负载平衡。我们展示了动态门控可以使LM的执行时间提高1.25-4倍，MT编码器提高2-5倍，MT解码器提高1.09-1.5倍。它还可以将LM的内存使用减少高达1.36倍，MT的内存使用减少高达1.1倍。

    Mixture-of-Experts (MoE) models have recently gained steam in achieving the state-of-the-art performance in a wide range of tasks in computer vision and natural language processing. They effectively expand the model capacity while incurring a minimal increase in computation cost during training. However, deploying such models for inference is difficult due to their large model size and complex communication pattern. In this work, we provide a characterization of two MoE workloads, namely Language Modeling (LM) and Machine Translation (MT) and identify their sources of inefficiencies at deployment.  We propose three optimization techniques to mitigate sources of inefficiencies, namely (1) Dynamic gating, (2) Expert Buffering, and (3) Expert load balancing. We show that dynamic gating improves execution time by 1.25-4$\times$ for LM, 2-5$\times$ for MT Encoder and 1.09-1.5$\times$ for MT Decoder. It also reduces memory usage by up to 1.36$\times$ for LM and up to 1.1$\times$ for MT. We f
    
[^69]: 针对分布式非独立同分布数据和部分标签的医学图像分类，优化联邦学习

    Optimizing Federated Learning for Medical Image Classification on Distributed Non-iid Datasets with Partial Labels. (arXiv:2303.06180v1 [cs.LG])

    [http://arxiv.org/abs/2303.06180](http://arxiv.org/abs/2303.06180)

    本文提出了FedFBN，一个联邦学习框架，使用预训练的网络作为模型后端，并在整个训练过程中冻结批量归一化层，以优化分布式非独立同分布数据和部分标签的医学图像分类。

    This paper proposes FedFBN, a federated learning framework that uses pretrained networks as the model backend and freezes the batch normalization layers throughout the training process to optimize medical image classification on distributed non-iid datasets with partial labels.

    大量的胸部X光数据集已经带头使用深度学习进行异常检测。然而，这些数据集专注于检测可能存在的一部分疾病标签，因此使它们成为分布式和非独立同分布的部分标签数据集。最近的文献指出，批量归一化层对于联邦学习的收敛具有影响，因为它们与具有部分标签的非独立同分布数据相关的域漂移。为此，我们提出了FedFBN，这是一个联邦学习框架，它从迁移学习中汲取灵感，使用预训练的网络作为模型后端，并在整个训练过程中冻结批量归一化层。我们使用合成iid玩具数据集和大规模非iid数据集评估FedFBN与当前FL策略。我们的结果表明，FedFBN优于使用分布式和非独立同分布数据训练全局模型的当前聚合策略。

    Numerous large-scale chest x-ray datasets have spearheaded expert-level detection of abnormalities using deep learning. However, these datasets focus on detecting a subset of disease labels that could be present, thus making them distributed and non-iid with partial labels. Recent literature has indicated the impact of batch normalization layers on the convergence of federated learning due to domain shift associated with non-iid data with partial labels. To that end, we propose FedFBN, a federated learning framework that draws inspiration from transfer learning by using pretrained networks as the model backend and freezing the batch normalization layers throughout the training process. We evaluate FedFBN with current FL strategies using synthetic iid toy datasets and large-scale non-iid datasets across scenarios with partial and complete labels. Our results demonstrate that FedFBN outperforms current aggregation strategies for training global models using distributed and non-iid data w
    
[^70]: 编程语言之间的软件漏洞预测知识转移

    Software Vulnerability Prediction Knowledge Transferring Between Programming Languages. (arXiv:2303.06177v1 [cs.SE])

    [http://arxiv.org/abs/2303.06177](http://arxiv.org/abs/2303.06177)

    本研究提出了一种转移学习技术，利用可用数据集生成一个模型，以检测不同编程语言中的常见漏洞。结果表明，所提出的模型以平均召回率为72％检测C和Java代码中的漏洞。

    This study proposes a transfer learning technique to detect common vulnerabilities in different programming languages by leveraging available datasets. The results show that the proposed model detects vulnerabilities in both C and Java codes with an average recall of 72%.

    开发自动化和智能的软件漏洞检测模型一直受到研究和开发社区的关注。这个领域最大的挑战之一是缺乏所有不同编程语言的代码样本。在本研究中，我们通过提出一种转移学习技术来解决这个问题，利用可用数据集生成一个模型，以检测不同编程语言中的常见漏洞。我们使用C源代码样本训练卷积神经网络（CNN）模型，然后使用Java源代码样本来采用和评估学习的模型。我们使用两个基准数据集的代码样本：NIST软件保障参考数据集（SARD）和Draper VDISC数据集。结果表明，所提出的模型以平均召回率为72％检测C和Java代码中的漏洞。此外，我们采用可解释的AI来调查每个特征对知识转移机制的贡献程度。

    Developing automated and smart software vulnerability detection models has been receiving great attention from both research and development communities. One of the biggest challenges in this area is the lack of code samples for all different programming languages. In this study, we address this issue by proposing a transfer learning technique to leverage available datasets and generate a model to detect common vulnerabilities in different programming languages. We use C source code samples to train a Convolutional Neural Network (CNN) model, then, we use Java source code samples to adopt and evaluate the learned model. We use code samples from two benchmark datasets: NIST Software Assurance Reference Dataset (SARD) and Draper VDISC dataset. The results show that proposed model detects vulnerabilities in both C and Java codes with average recall of 72\%. Additionally, we employ explainable AI to investigate how much each feature contributes to the knowledge transfer mechanisms between 
    
[^71]: 统一理解Grokking和双重下降

    Unifying Grokking and Double Descent. (arXiv:2303.06173v1 [cs.LG])

    [http://arxiv.org/abs/2303.06173](http://arxiv.org/abs/2303.06173)

    本文提出了一种模式学习速度框架，用于统一理解Grokking和双重下降，同时提供了模型智能Grokking的第一个演示。

    This paper proposes a framework of pattern learning speeds to unify the understanding of Grokking and double descent, and provides the first demonstration of model-wise Grokking.

    在深度学习中，对泛化的原则性理解可能需要将不同的观察结果统一到一个概念框架下。以前的工作研究了“Grokking”，这是一种训练动态，其中持续的近乎完美的训练表现和近乎偶然的测试表现最终会导致泛化，以及表面上类似的“双重下降”。到目前为止，这些主题已经被孤立地研究。我们假设Grokking和双重下降可以被理解为模式学习速度框架内相同学习动态的实例。我们提出，当改变模型容量而不是优化步骤时，该框架也适用，并提供了模型智能Grokking的第一个演示。

    A principled understanding of generalization in deep learning may require unifying disparate observations under a single conceptual framework. Previous work has studied \emph{grokking}, a training dynamic in which a sustained period of near-perfect training performance and near-chance test performance is eventually followed by generalization, as well as the superficially similar \emph{double descent}. These topics have so far been studied in isolation. We hypothesize that grokking and double descent can be understood as instances of the same learning dynamics within a framework of pattern learning speeds. We propose that this framework also applies when varying model capacity instead of optimization steps, and provide the first demonstration of model-wise grokking.
    
[^72]: DP-Fast MH: 大规模贝叶斯推断的私有、快速、准确的Metropolis-Hastings算法

    DP-Fast MH: Private, Fast, and Accurate Metropolis-Hastings for Large-Scale Bayesian Inference. (arXiv:2303.06171v1 [cs.LG])

    [http://arxiv.org/abs/2303.06171](http://arxiv.org/abs/2303.06171)

    本文提出了一种新的DP-Fast MH算法，用于大规模贝叶斯推断，具有精确、快速和隐私保护的特点。

    This paper proposes a new DP-Fast MH algorithm for large-scale Bayesian inference, which is accurate, fast, and privacy-preserving.

    贝叶斯推断提供了一个从复杂数据中学习和在不确定性下推理的原则性框架。它已经广泛应用于机器学习任务，如医学诊断、药物设计和政策制定。在这些常见应用中，数据可能非常敏感。差分隐私（DP）提供了具有强大最坏情况隐私保证的数据分析工具，并已发展成为隐私保护数据分析的主要方法。在本文中，我们研究了Metropolis-Hastings（MH）算法，这是最基本的MCMC方法之一，用于差分隐私下的大规模贝叶斯推断。虽然大多数现有的私有MCMC算法为了获得隐私而牺牲了准确性和效率，但我们提供了第一个精确且快速的DP MH算法，大多数迭代中仅使用一个小批量的数据。我们进一步揭示了隐私、可扩展性（即批量大小）和效率（即收敛速度）之间的三重权衡，从理论上说明了这一点。

    Bayesian inference provides a principled framework for learning from complex data and reasoning under uncertainty. It has been widely applied in machine learning tasks such as medical diagnosis, drug design, and policymaking. In these common applications, the data can be highly sensitive. Differential privacy (DP) offers data analysis tools with powerful worst-case privacy guarantees and has been developed as the leading approach in privacy-preserving data analysis. In this paper, we study Metropolis-Hastings (MH), one of the most fundamental MCMC methods, for large-scale Bayesian inference under differential privacy. While most existing private MCMC algorithms sacrifice accuracy and efficiency to obtain privacy, we provide the first exact and fast DP MH algorithm, using only a minibatch of data in most iterations. We further reveal, for the first time, a three-way trade-off among privacy, scalability (i.e. the batch size), and efficiency (i.e. the convergence rate), theoretically char
    
[^73]: MOELA：用于3D异构多核平台的多目标进化/学习设计空间探索框架

    MOELA: A Multi-Objective Evolutionary/Learning Design Space Exploration Framework for 3D Heterogeneous Manycore Platforms. (arXiv:2303.06169v1 [cs.LG])

    [http://arxiv.org/abs/2303.06169](http://arxiv.org/abs/2303.06169)

    MOELA是一个多目标设计空间探索框架，结合了进化搜索和学习搜索，用于优化3D NoC启用的异构多核系统中的多个目标，相比现有技术，MOELA可以提高解决方案的查找速度，提高Pareto Hypervolume（PHV）和能量延迟乘积（EDP）。

    MOELA is a multi-objective design space exploration framework that combines evolutionary-based search with learning-based local search to optimize multiple objectives in 3D NoC enabled heterogeneous manycore systems. Compared to state-of-the-art approaches, MOELA increases the speed of finding solutions, improves Pareto Hypervolume (PHV) and energy-delay-product (EDP).

    为了支持深度机器学习和图处理等新兴应用，需要能够集成多个处理元件（PE）的3D网络芯片（NoC）启用的异构多核平台。然而，由于巨大的设计空间和长时间的评估时间，设计具有多个目标的这种复杂系统可能具有挑战性。为了优化这样的系统，我们提出了一种新的多目标设计空间探索框架MOELA，它将基于进化的搜索的优点与基于学习的局部搜索相结合，以快速确定PE和通信链路的放置位置，以优化3D NoC启用的异构多核系统中的多个目标（例如延迟，吞吐量和能量）。与现有技术相比，MOELA可以将解决方案的查找速度提高多达128倍，在5个目标的情况下，可以将Pareto Hypervolume（PHV）提高多达12.14倍，并将能量延迟乘积（EDP）提高多达7.7％。

    To enable emerging applications such as deep machine learning and graph processing, 3D network-on-chip (NoC) enabled heterogeneous manycore platforms that can integrate many processing elements (PEs) are needed. However, designing such complex systems with multiple objectives can be challenging due to the huge associated design space and long evaluation times. To optimize such systems, we propose a new multi-objective design space exploration framework called MOELA that combines the benefits of evolutionary-based search with a learning-based local search to quickly determine PE and communication link placement to optimize multiple objectives (e.g., latency, throughput, and energy) in 3D NoC enabled heterogeneous manycore systems. Compared to state-of-the-art approaches, MOELA increases the speed of finding solutions by up to 128x, leads to a better Pareto Hypervolume (PHV) by up to 12.14x and improves energy-delay-product (EDP) by up to 7.7% in a 5-objective scenario.
    
[^74]: 通过操作微调数据集来克服预训练模型中的偏见

    Overcoming Bias in Pretrained Models by Manipulating the Finetuning Dataset. (arXiv:2303.06167v1 [cs.CV])

    [http://arxiv.org/abs/2303.06167](http://arxiv.org/abs/2303.06167)

    本文研究了预训练模型中的偏见问题，发现微调模型可以继承预训练模型的偏见，但通过对微调数据集进行干预可以纠正这种偏见，而且对性能的影响很小。这表明仔细策划微调数据集对于减少下游任务中的偏见非常重要，这样做甚至可以弥补预训练模型中的偏见。

    This paper investigates the bias problem in pretrained models and finds that finetuned models can inherit the biases of pretrained models, but these biases can be corrected by manipulating the finetuning dataset with little impact on performance. This implies that careful curation of the finetuning dataset is important for reducing biases on a downstream task, and doing so can even compensate for bias in the pretrained model.

    转移学习通过允许在大规模数据集上预训练的模型的表达特征被微调到更小、更具领域特定性的数据集的目标任务中而受益。然而，有人担心这些预训练模型可能带有自己的偏见，这些偏见会传播到微调模型中。在这项工作中，我们研究了偏见，当偏见被概念化为目标任务和敏感属性之间的虚假相关性以及数据集中特定群体的代表性不足时。在偏见的两种概念下，我们发现(1)在预训练模型的基础上微调的模型确实可以继承它们的偏见，但(2)通过对微调数据集进行相对较小的干预，这种偏见可以得到纠正，而且对性能的影响往往可以忽略不计。我们的发现意味着，仔细策划微调数据集对于减少下游任务中的偏见非常重要，这样做甚至可以弥补预训练模型中的偏见。

    Transfer learning is beneficial by allowing the expressive features of models pretrained on large-scale datasets to be finetuned for the target task of smaller, more domain-specific datasets. However, there is a concern that these pretrained models may come with their own biases which would propagate into the finetuned model. In this work, we investigate bias when conceptualized as both spurious correlations between the target task and a sensitive attribute as well as underrepresentation of a particular group in the dataset. Under both notions of bias, we find that (1) models finetuned on top of pretrained models can indeed inherit their biases, but (2) this bias can be corrected for through relatively minor interventions to the finetuning dataset, and often with a negligible impact to performance. Our findings imply that careful curation of the finetuning dataset is important for reducing biases on a downstream task, and doing so can even compensate for bias in the pretrained model.
    
[^75]: 理解质量多样性和深度强化学习之间的协同作用

    Understanding the Synergies between Quality-Diversity and Deep Reinforcement Learning. (arXiv:2303.06164v1 [cs.LG])

    [http://arxiv.org/abs/2303.06164](http://arxiv.org/abs/2303.06164)

    本文提出了广义演员-评论家QD-RL框架，用于QD-RL设置中的演员-评论家深度RL方法。该框架引入了两种新算法，PGA-ME（SAC）和PGA-ME（DroQ），将深度RL的最新进展应用于QD-RL设置，并解决了现有QD-RL算法无法解决的人形环境问题。

    This paper proposes a Generalized Actor-Critic QD-RL framework for actor-critic deep RL methods in the QD-RL setting. The framework introduces two new algorithms, PGA-ME (SAC) and PGA-ME (DroQ), which apply recent advancements in Deep RL to the QD-RL setting and solve the humanoid environment problem that existing QD-RL algorithms cannot solve.

    质量多样性（QD）和深度强化学习（RL）之间的协同作用已经导致了强大的混合QD-RL算法，展示了巨大的潜力，并带来了两个领域的最佳实践。然而，尽管其他RL算法取得了显著进展，但在先前的混合方法中仅使用了单个深度RL算法（TD3）。此外，QD和RL之间的优化过程存在根本差异，需要更加原则性的方法。我们提出了广义演员-评论家QD-RL，这是一个统一的模块化框架，用于QD-RL设置中的演员-评论家深度RL方法。该框架提供了一条研究深度RL在QD-RL设置中的见解的路径，这是在QD-RL中取得进展的重要且有效的方法。我们引入了两种新算法，PGA-ME（SAC）和PGA-ME（DroQ），将深度RL的最新进展应用于QD-RL设置，并解决了现有QD-RL算法无法解决的人形环境问题。

    The synergies between Quality-Diversity (QD) and Deep Reinforcement Learning (RL) have led to powerful hybrid QD-RL algorithms that have shown tremendous potential, and brings the best of both fields. However, only a single deep RL algorithm (TD3) has been used in prior hybrid methods despite notable progress made by other RL algorithms. Additionally, there are fundamental differences in the optimization procedures between QD and RL which would benefit from a more principled approach. We propose Generalized Actor-Critic QD-RL, a unified modular framework for actor-critic deep RL methods in the QD-RL setting. This framework provides a path to study insights from Deep RL in the QD-RL setting, which is an important and efficient way to make progress in QD-RL. We introduce two new algorithms, PGA-ME (SAC) and PGA-ME (DroQ) which apply recent advancements in Deep RL to the QD-RL setting, and solves the humanoid environment which was not possible using existing QD-RL algorithms. However, we 
    
[^76]: 数字孪生辅助异构联邦学习的知识蒸馏框架

    Digital Twin-Assisted Knowledge Distillation Framework for Heterogeneous Federated Learning. (arXiv:2303.06155v1 [cs.LG])

    [http://arxiv.org/abs/2303.06155](http://arxiv.org/abs/2303.06155)

    本文提出了一种数字孪生辅助的知识蒸馏框架，用于解决联邦学习系统中的异构性问题，用户可以选择自己的神经网络模型并从大型教师模型中蒸馏知识，同时利用数字孪生在服务器上训练大型教师模型，最终通过混合整数规划和Q-learning算法实现模型选择和资源分配。

    This paper proposes a digital twin-assisted knowledge distillation framework for heterogeneous federated learning, where users can select their own neural network models and distill knowledge from a big teacher model, and the teacher model can be trained on a digital twin located in the server. The joint problem of model selection and training offloading and resource allocation for users is formulated as a mixed integer programming problem and solved using Q-learning and optimization algorithms.

    本文提出了一种知识蒸馏驱动的联邦学习框架，以应对联邦学习系统中的异构性，其中每个用户可以根据需要选择其神经网络模型，并使用自己的私有数据集从大型教师模型中蒸馏知识。为了克服在资源有限的用户设备上训练大型教师模型的挑战，利用数字孪生的方式，教师模型可以在具有足够计算资源的服务器上的数字孪生中进行训练。然后，在模型蒸馏期间，每个用户可以在物理实体或数字代理处更新其模型的参数。为用户选择模型和训练卸载和资源分配制定了混合整数规划（MIP）问题。为了解决这个问题，联合使用Q-learning和优化，其中Q-learning为用户选择模型并确定是在本地还是在服务器上进行训练，而优化则用于资源分配。

    In this paper, to deal with the heterogeneity in federated learning (FL) systems, a knowledge distillation (KD) driven training framework for FL is proposed, where each user can select its neural network model on demand and distill knowledge from a big teacher model using its own private dataset. To overcome the challenge of train the big teacher model in resource limited user devices, the digital twin (DT) is exploit in the way that the teacher model can be trained at DT located in the server with enough computing resources. Then, during model distillation, each user can update the parameters of its model at either the physical entity or the digital agent. The joint problem of model selection and training offloading and resource allocation for users is formulated as a mixed integer programming (MIP) problem. To solve the problem, Q-learning and optimization are jointly used, where Q-learning selects models for users and determines whether to train locally or on the server, and optimiz
    
[^77]: 基于k-mer分布和机器学习的资源节约分类法

    Resource saving taxonomy classification with k-mer distributions and machine learning. (arXiv:2303.06154v1 [q-bio.GN])

    [http://arxiv.org/abs/2303.06154](http://arxiv.org/abs/2303.06154)

    本文提出了一种基于k-mer分布和机器学习的分类法，可以节约资源并提高分类器的性能。

    This paper proposes a resource-saving classification method based on k-mer distributions and machine learning, which can improve the performance of classifiers and reduce the consumption of energy.

    现代高通量测序技术（如宏基因组测序）生成数百万个序列，需要根据它们的分类级别进行分类。现代方法要么应用本地比对和与现有数据集（如MMseqs2）的比较，要么使用深度神经网络（如DeepMicrobes和BERTax）。基于比对的方法在运行时间方面成本高，特别是由于数据库变得越来越大。对于基于深度学习的方法，需要专门的硬件进行计算，这消耗大量能源。在本文中，我们建议使用从DNA中获得的k-mer分布作为特征，使用机器学习方法（如子空间k最近邻算法、神经网络或袋装决策树）来分类其分类起源。此外，我们提出了一种特征空间数据集平衡方法，允许减少训练数据集并提高分类器的性能。通过比较性能，我们证明了我们的方法比现有方法更有效。

    Modern high throughput sequencing technologies like metagenomic sequencing generate millions of sequences which have to be classified based on their taxonomic rank. Modern approaches either apply local alignment and comparison to existing data sets like MMseqs2 or use deep neural networks as it is done in DeepMicrobes and BERTax. Alignment-based approaches are costly in terms of runtime, especially since databases get larger and larger. For the deep learning-based approaches, specialized hardware is necessary for a computation, which consumes large amounts of energy. In this paper, we propose to use $k$-mer distributions obtained from DNA as features to classify its taxonomic origin using machine learning approaches like the subspace $k$-nearest neighbors algorithm, neural networks or bagged decision trees. In addition, we propose a feature space data set balancing approach, which allows reducing the data set for training and improves the performance of the classifiers. By comparing pe
    
[^78]: NoiseCAM: 用于噪声和对抗攻击边界的可解释人工智能

    NoiseCAM: Explainable AI for the Boundary Between Noise and Adversarial Attacks. (arXiv:2303.06151v1 [cs.LG])

    [http://arxiv.org/abs/2303.06151](http://arxiv.org/abs/2303.06151)

    本文提出了一种名为NoiseCAM的算法，该算法可以定位易受攻击层并检测对抗性示例，同时不会对混入输入的高斯随机噪声做出反应。

    This paper proposes a NoiseCAM algorithm that can locate vulnerable layers and detect adversarial examples, while not responding to Gaussian random noise mixed in the inputs.

    深度学习和深度神经网络在各个领域得到了广泛应用。然而，对抗攻击很容易误导神经网络并导致错误决策。在安全关键应用中高度需要防御机制。本文首先使用梯度类激活映射（GradCAM）分析VGG-16网络在其输入与对抗扰动或高斯噪声混合时的行为偏差。特别地，我们的方法可以定位对抗扰动和高斯噪声敏感的易受攻击层。我们还展示易受攻击层的行为偏差可以用于检测对抗性示例。其次，我们提出了一种新的NoiseCAM算法，该算法集成了全局和像素级加权类激活映射的信息。我们的算法容易受到对抗扰动的影响，不会对混入输入的高斯随机噪声做出反应。第三，我们比较了检测对抗性示例的方法。

    Deep Learning (DL) and Deep Neural Networks (DNNs) are widely used in various domains. However, adversarial attacks can easily mislead a neural network and lead to wrong decisions. Defense mechanisms are highly preferred in safety-critical applications. In this paper, firstly, we use the gradient class activation map (GradCAM) to analyze the behavior deviation of the VGG-16 network when its inputs are mixed with adversarial perturbation or Gaussian noise. In particular, our method can locate vulnerable layers that are sensitive to adversarial perturbation and Gaussian noise. We also show that the behavior deviation of vulnerable layers can be used to detect adversarial examples. Secondly, we propose a novel NoiseCAM algorithm that integrates information from globally and pixel-level weighted class activation maps. Our algorithm is susceptible to adversarial perturbations and will not respond to Gaussian random noise mixed in the inputs. Third, we compare detecting adversarial examples 
    
[^79]: 临床BERTScore：临床环境下自动语音识别性能的改进度量

    Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings. (arXiv:2303.05737v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2303.05737](http://arxiv.org/abs/2303.05737)

    本文提出了一种临床BERTScore（CBERTScore）度量，它比其他度量更严厉地惩罚临床相关的错误，更接近于临床医生对医学句子的偏好。作者还收集了13个临床医生对149个现实医学句子的偏好基准，称为临床转录偏好基准（CTP），证明CBERTScore更接近于临床医生的偏好，并将基准发布给社区以进一步开发具有临床意识的ASR度量。

    The paper proposes a Clinical BERTScore (CBERTScore) metric for ASR in medical contexts, which penalizes clinically-relevant mistakes more than other metrics and aligns more closely with clinician preferences. The authors also collect a benchmark of clinician preferences on medical sentences and release it for the community to further develop clinically-aware ASR metrics.

    医学环境中的自动语音识别（ASR）有潜力节省时间，降低成本，提高报告准确性并减少医生的疲劳。然而，由于避免医学相关的转录错误的重要性，医疗行业采用这种技术的速度较慢。在这项工作中，我们提出了临床BERTScore（CBERTScore），这是一种ASR度量，它比其他度量（WER、BLUE、METEOR等）更严厉地惩罚临床相关的错误。我们证明了这个度量更接近于临床医生对医学句子的偏好，有时差距很大。我们收集了13个临床医生对149个现实医学句子的偏好基准，称为临床转录偏好基准（CTP），证明CBERTScore更接近于临床医生的偏好，并将基准发布给社区以进一步开发具有临床意识的ASR度量。

    Automatic Speech Recognition (ASR) in medical contexts has the potential to save time, cut costs, increase report accuracy, and reduce physician burnout. However, the healthcare industry has been slower to adopt this technology, in part due to the importance of avoiding medically-relevant transcription mistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR metric that penalizes clinically-relevant mistakes more than others. We demonstrate that this metric more closely aligns with clinician preferences on medical sentences as compared to other metrics (WER, BLUE, METEOR, etc), sometimes by wide margins. We collect a benchmark of 13 clinician preferences on 149 realistic medical sentences called the Clinician Transcript Preference benchmark (CTP), demonstrate that CBERTScore more closely matches what clinicians prefer, and release the benchmark for the community to further develop clinically-aware ASR metrics.
    
[^80]: 通过敏感性参数和代理变量限制受益和伤害的概率

    Bounding the Probabilities of Benefit and Harm Through Sensitivity Parameters and Proxies. (arXiv:2303.05396v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2303.05396](http://arxiv.org/abs/2303.05396)

    本文提出了两种方法来限制未测量混杂下的受益和伤害概率，一种是通过敏感性参数计算概率的上限或下限，另一种是利用代理变量得到更紧的界限。

    This paper presents two methods for bounding the probabilities of benefit and harm under unmeasured confounding, one is to compute the upper or lower bound of the probability through sensitivity parameters, and the other is to derive tighter bounds using a measured nondifferential proxy of the unmeasured confounder.

    我们提出了两种方法来限制未测量混杂下的受益和伤害概率。第一种方法计算任一概率的（上限或下限），作为观察数据分布和两个直观敏感性参数的函数，然后可以将其呈现给分析师作为2-D图以协助其决策。第二种方法假设存在未测量混杂因素的测量非差异代理变量（即直接效应）。使用此代理变量，可以从仅观察到的数据分布中导出比现有界限更紧的界限。

    We present two methods for bounding the probabilities of benefit and harm under unmeasured confounding. The first method computes the (upper or lower) bound of either probability as a function of the observed data distribution and two intuitive sensitivity parameters which, then, can be presented to the analyst as a 2-D plot to assist her in decision making. The second method assumes the existence of a measured nondifferential proxy (i.e., direct effect) of the unmeasured confounder. Using this proxy, tighter bounds than the existing ones can be derived from just the observed data distribution.
    
[^81]: 基于规划强化学习的可再生能源电力系统实时调度

    Real-time scheduling of renewable power systems through planning-based reinforcement learning. (arXiv:2303.05205v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.05205](http://arxiv.org/abs/2303.05205)

    本文提出了一种基于规划强化学习算法和真实电力网环境的系统解决方案，可以实现发电机的规划和更细的时间分辨率调整，从而增加了电网的能力。

    This paper proposes a systematic solution based on the state-of-the-art reinforcement learning algorithm and the real power grid environment, which enables planning and finer time resolution adjustments of power generators, including unit commitment and economic dispatch, thus increasing the grid's ability.

    不断增长的可再生能源来源对传统电力调度提出了重大挑战。运营商难以获得准确的可再生能源发电日前预测，因此需要未来调度系统根据超短期预测进行实时调度决策。受计算速度限制，传统的基于优化的方法无法解决这个问题。最近强化学习（RL）的发展已经展示了解决这个挑战的潜力。然而，现有的RL方法在约束复杂性、算法性能和环境保真度方面不足。我们是第一个提出基于最先进的强化学习算法和真实电力网环境的系统解决方案。所提出的方法使发电机的规划和更细的时间分辨率调整成为可能，包括机组组合和经济调度，从而增加了电网的能力。

    The growing renewable energy sources have posed significant challenges to traditional power scheduling. It is difficult for operators to obtain accurate day-ahead forecasts of renewable generation, thereby requiring the future scheduling system to make real-time scheduling decisions aligning with ultra-short-term forecasts. Restricted by the computation speed, traditional optimization-based methods can not solve this problem. Recent developments in reinforcement learning (RL) have demonstrated the potential to solve this challenge. However, the existing RL methods are inadequate in terms of constraint complexity, algorithm performance, and environment fidelity. We are the first to propose a systematic solution based on the state-of-the-art reinforcement learning algorithm and the real power grid environment. The proposed approach enables planning and finer time resolution adjustments of power generators, including unit commitment and economic dispatch, thus increasing the grid's abilit
    
[^82]: 代码神经模型中基于变量角色的特征增强研究

    A Study of Variable-Role-based Feature Enrichment in Neural Models of Code. (arXiv:2303.04942v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04942](http://arxiv.org/abs/2303.04942)

    本文研究了一种基于变量角色的无监督特征增强方法对代码神经模型性能的影响，通过在数据集程序中添加单个变量的角色来丰富源代码数据集，并因此对变量角色增强在训练Code2Seq模型中的影响进行了研究。

    This paper investigates the impact of an unsupervised feature enrichment approach based on variable roles on the performance of neural models of code, and enriches a source code dataset by adding the role of individual variables in the dataset programs, thereby conducting a study on the impact of variable role enrichment in training the Code2Seq model.

    尽管深度神经模型大大减少了特征工程的开销，但输入中可用的特征可能会显著影响模型的训练成本和性能。本文探讨了一种基于变量角色的无监督特征增强方法对代码神经模型性能的影响。变量角色的概念（如Sajaniemi等人的作品中所介绍的）已被发现有助于学生的编程能力。本文研究了这个概念是否会提高代码神经模型的性能。据我们所知，这是第一篇研究Sajaniemi等人的变量角色概念如何影响代码神经模型的工作。具体而言，我们通过在数据集程序中添加单个变量的角色来丰富源代码数据集，并因此对变量角色增强在训练Code2Seq模型中的影响进行了研究。

    Although deep neural models substantially reduce the overhead of feature engineering, the features readily available in the inputs might significantly impact training cost and the performance of the models. In this paper, we explore the impact of an unsuperivsed feature enrichment approach based on variable roles on the performance of neural models of code. The notion of variable roles (as introduced in the works of Sajaniemi et al. [Refs. 1,2]) has been found to help students' abilities in programming. In this paper, we investigate if this notion would improve the performance of neural models of code. To the best of our knowledge, this is the first work to investigate how Sajaniemi et al.'s concept of variable roles can affect neural models of code. In particular, we enrich a source code dataset by adding the role of individual variables in the dataset programs, and thereby conduct a study on the impact of variable role enrichment in training the Code2Seq model. In addition, we shed l
    
[^83]: 通过变分量子样条使非线性量子操作成为可能

    Enabling Non-Linear Quantum Operations through Variational Quantum Splines. (arXiv:2303.04788v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2303.04788](http://arxiv.org/abs/2303.04788)

    本文提出了一种新方法——广义QSplines，使用混合量子-经典计算来近似非线性量子激活函数，克服了原始QSplines在量子硬件方面的高要求，并适合嵌入现有的量子神经网络架构中。

    This paper proposes a novel method, Generalised QSplines (GQSplines), for approximating non-linear quantum activation functions using hybrid quantum-classical computation, which overcomes the highly demanding requirements of the original QSplines in terms of quantum hardware and is suitable to be embedded in existing quantum neural network architectures.

    量子力学的假设仅对量子状态施加幺正变换，这对于量子机器学习算法来说是一个严重的限制。最近提出了量子样条（QSplines）来近似量子激活函数，以在量子算法中引入非线性。然而，QSplines使用HHL作为子程序，并需要一个容错的量子计算机才能正确实现。本文提出了广义QSplines（GQSplines），一种使用混合量子-经典计算来近似非线性量子激活函数的新方法。GQSplines克服了原始QSplines在量子硬件方面的高要求，并可以使用近期的量子计算机来实现。此外，所提出的方法依赖于灵活的问题表示，适合嵌入现有的量子神经网络架构中。

    The postulates of quantum mechanics impose only unitary transformations on quantum states, which is a severe limitation for quantum machine learning algorithms. Quantum Splines (QSplines) have recently been proposed to approximate quantum activation functions to introduce non-linearity in quantum algorithms. However, QSplines make use of the HHL as a subroutine and require a fault-tolerant quantum computer to be correctly implemented. This work proposes the Generalised QSplines (GQSplines), a novel method for approximating non-linear quantum activation functions using hybrid quantum-classical computation. The GQSplines overcome the highly demanding requirements of the original QSplines in terms of quantum hardware and can be implemented using near-term quantum computers. Furthermore, the proposed method relies on a flexible problem representation for non-linear approximation and it is suitable to be embedded in existing quantum neural network architectures. In addition, we provide a pr
    
[^84]: 非凸-PL极小极大优化的增强自适应梯度算法

    Enhanced Adaptive Gradient Algorithms for Nonconvex-PL Minimax Optimization. (arXiv:2303.03984v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2303.03984](http://arxiv.org/abs/2303.03984)

    本文提出了一类增强的基于动量的梯度下降上升方法（即MSGDA和AdaMSGDA）来解决非凸-PL极小极大问题，其中AdaMSGDA算法可以使用各种自适应学习率来更新变量$x$和$y$，而不依赖于任何全局和坐标自适应学习率。理论上，我们证明了我们的MSGDA和AdaMSGDA方法在找到$\epsilon$-稳定解时，只需要在每个循环中进行一次采样，就可以获得已知的最佳样本（梯度）复杂度$O(\epsilon^{-3})$。

    This paper proposes a class of enhanced momentum-based gradient descent ascent methods (MSGDA and AdaMSGDA) to solve nonconvex-PL minimax problems, where the AdaMSGDA algorithm can use various adaptive learning rates to update variables x and y without relying on any global and coordinate-wise adaptive learning rates. Theoretical analysis shows that MSGDA and AdaMSGDA methods have the best known sample (gradient) complexity of O(ε−3) in finding an ε-stationary solution.

    本文研究了一类非凸非凹的极小极大优化问题（即$\min_x\max_y f(x,y)$），其中$f(x,y)$在$x$上可能是非凸的，在$y$上是非凹的，并满足Polyak-Lojasiewicz（PL）条件。此外，我们提出了一类增强的基于动量的梯度下降上升方法（即MSGDA和AdaMSGDA）来解决这些随机非凸-PL极小极大问题。特别地，我们的AdaMSGDA算法可以使用各种自适应学习率来更新变量$x$和$y$，而不依赖于任何全局和坐标自适应学习率。理论上，我们提出了一种有效的收敛分析框架来解决我们的方法。具体而言，我们证明了我们的MSGDA和AdaMSGDA方法在找到$\epsilon$-稳定解（即$\mathbb{E}\|\nabla F(x)\|\leq \epsilon$，其中$F(x)=\max_y f(x,y)$）时，只需要在每个循环中进行一次采样，就可以获得已知的最佳样本（梯度）复杂度$O(\epsilon^{-3})$。

    In the paper, we study a class of nonconvex nonconcave minimax optimization problems (i.e., $\min_x\max_y f(x,y)$), where $f(x,y)$ is possible nonconvex in $x$, and it is nonconcave and satisfies the Polyak-Lojasiewicz (PL) condition in $y$. Moreover, we propose a class of enhanced momentum-based gradient descent ascent methods (i.e., MSGDA and AdaMSGDA) to solve these stochastic Nonconvex-PL minimax problems. In particular, our AdaMSGDA algorithm can use various adaptive learning rates in updating the variables $x$ and $y$ without relying on any global and coordinate-wise adaptive learning rates. Theoretically, we present an effective convergence analysis framework for our methods. Specifically, we prove that our MSGDA and AdaMSGDA methods have the best known sample (gradient) complexity of $O(\epsilon^{-3})$ only requiring one sample at each loop in finding an $\epsilon$-stationary solution (i.e., $\mathbb{E}\|\nabla F(x)\|\leq \epsilon$, where $F(x)=\max_y f(x,y)$). This manuscript 
    
[^85]: PreFallKD: 基于CNN-ViT知识蒸馏的预防跌倒系统

    PreFallKD: Pre-Impact Fall Detection via CNN-ViT Knowledge Distillation. (arXiv:2303.03634v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2303.03634](http://arxiv.org/abs/2303.03634)

    本文提出了一种基于CNN-ViT知识蒸馏的预防跌倒系统PreFallKD，通过将预训练的教师模型的检测知识转移到轻量级卷积神经网络的学生模型上，实现了检测性能和计算复杂性的平衡。

    This paper proposes a pre-impact fall detection system called PreFallKD, which uses CNN-ViT knowledge distillation to transfer detection knowledge from a pre-trained teacher model to a lightweight convolutional neural network student model. The system achieves a balance between detection performance and computational complexity.

    跌倒事故是老龄化社会中的重要问题。近年来，许多研究人员使用深度学习开发了预防跌倒系统，以支持基于可穿戴设备的跌倒保护系统，以预防严重的伤害。然而，大多数工作只使用简单的神经网络模型，而不是考虑到资源受限的移动设备和严格的延迟要求的复杂模型的可用性。在这项工作中，我们提出了一种新颖的基于CNN-ViT知识蒸馏的预防跌倒系统，即PreFallKD，以在检测性能和计算复杂性之间取得平衡。所提出的PreFallKD将检测知识从预训练的教师模型（视觉变换器）转移到学生模型（轻量级卷积神经网络）。此外，我们应用数据增强技术来解决数据不平衡的问题。我们在KFall公共数据集上进行实验，并将PreFallKD与其他最先进的模型进行比较。

    Fall accidents are critical issues in an aging and aged society. Recently, many researchers developed pre-impact fall detection systems using deep learning to support wearable-based fall protection systems for preventing severe injuries. However, most works only employed simple neural network models instead of complex models considering the usability in resource-constrained mobile devices and strict latency requirements. In this work, we propose a novel pre-impact fall detection via CNN-ViT knowledge distillation, namely PreFallKD, to strike a balance between detection performance and computational complexity. The proposed PreFallKD transfers the detection knowledge from the pre-trained teacher model (vision transformer) to the student model (lightweight convolutional neural networks). Additionally, we apply data augmentation techniques to tackle issues of data imbalance. We conduct the experiment on the KFall public dataset and compare PreFallKD with other state-of-the-art models. The
    
[^86]: 异构图学习在声音事件分类中的应用

    Heterogeneous Graph Learning for Acoustic Event Classification. (arXiv:2303.02665v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2303.02665](http://arxiv.org/abs/2303.02665)

    本文提出了一种新模型，异构图跨模态网络（HGCN），它学习跨模态边缘，可以适应各种空间和时间尺度，有效地连接了跨模态的相关节点，在声音事件分类中表现出最先进的性能。

    This paper proposes a new model, Heterogeneous Graph Crossmodal Network (HGCN), which learns crossmodal edges and can adapt to various spatial and temporal scales, effectively connecting relevant nodes across modalities. It achieves state-of-the-art performance in acoustic event classification.

    异构图提供了一种紧凑、高效、可扩展的方式来建模涉及多个不同模态的数据。这使得使用异构图来建模音频视觉数据成为一种有吸引力的选择。然而，图结构在音频视觉数据中并不自然。音频视觉数据的图是手动构建的，这既困难又次优。在这项工作中，我们通过（i）提出一种参数化图构建策略来解决这个问题，以及（ii）学习跨模态边缘。为此，我们开发了一种新模型，异构图跨模态网络（HGCN），它学习跨模态边缘。我们提出的模型可以适应各种空间和时间尺度，因为它是参数化构建的，而可学习的跨模态边缘有效地连接了跨模态的相关节点。在一个大型基准数据集（AudioSet）上的实验表明，我们的模型是最先进的（0.53平均精度），优于transfo。

    Heterogeneous graphs provide a compact, efficient, and scalable way to model data involving multiple disparate modalities. This makes modeling audiovisual data using heterogeneous graphs an attractive option. However, graph structure does not appear naturally in audiovisual data. Graphs for audiovisual data are constructed manually which is both difficult and sub-optimal. In this work, we address this problem by (i) proposing a parametric graph construction strategy for the intra-modal edges, and (ii) learning the crossmodal edges. To this end, we develop a new model, heterogeneous graph crossmodal network (HGCN) that learns the crossmodal edges. Our proposed model can adapt to various spatial and temporal scales owing to its parametric construction, while the learnable crossmodal edges effectively connect the relevant nodes across modalities. Experiments on a large benchmark dataset (AudioSet) show that our model is state-of-the-art (0.53 mean average precision), outperforming transfo
    
[^87]: Prismer: 一种具有专家集合的视觉语言模型

    Prismer: A Vision-Language Model with An Ensemble of Experts. (arXiv:2303.02506v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02506](http://arxiv.org/abs/2303.02506)

    Prismer是一种数据和参数高效的视觉语言模型，它利用了一组领域专家的集合，通过汇集这些专家知识并将其适应于各种视觉语言推理任务，实现了与当前最先进模型竞争的微调和少样本学习性能，同时需要少至两个数量级的训练数据。

    Prismer is a data- and parameter-efficient vision-language model that leverages an ensemble of domain experts, achieving fine-tuned and few-shot learning performance competitive with current state-of-the-art models, whilst requiring up to two orders of magnitude less training data.

    最近的视觉语言模型展示了令人印象深刻的多模态生成能力。然而，通常它们需要在大规模数据集上训练庞大的模型。作为一种更可扩展的替代方案，我们介绍了Prismer，一种数据和参数高效的视觉语言模型，它利用了一组领域专家的集合。Prismer只需要训练少量组件，大部分网络权重从现成的预训练领域专家中继承，并在训练期间保持冻结状态。通过利用来自各种领域的专家，我们展示了Prismer可以有效地汇集这些专家知识并将其适应于各种视觉语言推理任务。在我们的实验中，我们展示了Prismer实现了与当前最先进模型竞争的微调和少样本学习性能，同时需要少至两个数量级的训练数据。代码可在https://github.com/NVlabs/prismer获得。

    Recent vision-language models have shown impressive multi-modal generation capabilities. However, typically they require training huge models on massive datasets. As a more scalable alternative, we introduce Prismer, a data- and parameter-efficient vision-language model that leverages an ensemble of domain experts. Prismer only requires training of a small number of components, with the majority of network weights inherited from readily-available, pre-trained domain experts, and kept frozen during training. By leveraging experts from a wide range of domains, we show that Prismer can efficiently pool this expert knowledge and adapt it to various vision-language reasoning tasks. In our experiments, we show that Prismer achieves fine-tuned and few-shot learning performance which is competitive with current state-of-the-art models, whilst requiring up to two orders of magnitude less training data. Code is available at https://github.com/NVlabs/prismer.
    
[^88]: 通过离线强化学习学习影响人类行为

    Learning to Influence Human Behavior with Offline Reinforcement Learning. (arXiv:2303.02265v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.02265](http://arxiv.org/abs/2303.02265)

    本文提出了一种通过离线强化学习学习影响人类行为的方法，可以提高人类在协作任务中的表现。

    This paper proposes a method of learning to influence human behavior through offline reinforcement learning, which can improve human performance in collaborative tasks.

    在现实世界中，学习代理与人类互动是最复杂的设置之一，因为人类往往由于复杂的偏见而表现出次优的、不可预测的行为。在这种情况下与人类互动的代理最终会影响这些人所采取的行动。我们的目标是使代理能够利用这种影响来提高人类在协作任务中的表现，随着任务的展开。与以前的工作不同，我们不假设与人员进行在线培训（这往往太昂贵和不安全），也不假设有高保真度环境模拟器的访问权限。我们的想法是，通过采用各种先前观察到的人类-人类交互数据并将其标记为任务奖励，离线强化学习（RL）可以学习组合行为的组件，并发现导致更理想的人类行为的行动。首先，我们展示了离线RL可以学习策略来影响和改善人类行为，尽管这些策略可能与人类的期望不同。

    In the real world, some of the most complex settings for learned agents involve interaction with humans, who often exhibit suboptimal, unpredictable behavior due to sophisticated biases. Agents that interact with people in such settings end up influencing the actions that these people take. Our goal in this work is to enable agents to leverage that influence to improve the human's performance in collaborative tasks, as the task unfolds. Unlike prior work, we do not assume online training with people (which tends to be too expensive and unsafe), nor access to a high fidelity simulator of the environment. Our idea is that by taking a variety of previously observed human-human interaction data and labeling it with the task reward, offline reinforcement learning (RL) can learn to combine components of behavior, and uncover actions that lead to more desirable human actions. First, we show that offline RL can learn strategies to influence and improve human behavior, despite those strategies 
    
[^89]: 基于Fisher信息的证据深度学习方法用于不确定性估计

    Uncertainty Estimation by Fisher Information-based Evidential Deep Learning. (arXiv:2303.02045v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02045](http://arxiv.org/abs/2303.02045)

    本文提出了一种基于Fisher信息的证据深度学习方法，用于解决高数据不确定性样本但注释为one-hot标签的情况下证据学习过程被过度惩罚并受到阻碍的问题。

    This paper proposes a Fisher Information-based Evidential Deep Learning method to address the problem of over-penalization and hindrance in evidence learning for high data uncertainty samples annotated with one-hot labels.

    不确定性估计是使深度学习在实际应用中可靠的关键因素。最近提出的证据神经网络通过将网络输出视为证据来参数化狄利克雷分布，明确考虑不同的不确定性，并在不确定性估计方面取得了令人印象深刻的性能。然而，对于高数据不确定性样本但注释为one-hot标签的情况，这些错误标记的类别的证据学习过程会被过度惩罚并受到阻碍。为了解决这个问题，我们提出了一种新的方法，基于Fisher信息的证据深度学习（$\mathcal{I}$-EDL）。特别地，我们引入Fisher信息矩阵（FIM）来衡量每个样本所携带的证据的信息量，根据这个信息量，我们可以动态地重新加权目标损失项，使网络更加专注于不确定类别的表示学习。我们的网络的泛化能力通过优化进一步提高。

    Uncertainty estimation is a key factor that makes deep learning reliable in practical applications. Recently proposed evidential neural networks explicitly account for different uncertainties by treating the network's outputs as evidence to parameterize the Dirichlet distribution, and achieve impressive performance in uncertainty estimation. However, for high data uncertainty samples but annotated with the one-hot label, the evidence-learning process for those mislabeled classes is over-penalized and remains hindered. To address this problem, we propose a novel method, Fisher Information-based Evidential Deep Learning ($\mathcal{I}$-EDL). In particular, we introduce Fisher Information Matrix (FIM) to measure the informativeness of evidence carried by each sample, according to which we can dynamically reweight the objective loss terms to make the network more focused on the representation learning of uncertain classes. The generalization ability of our network is further improved by opt
    
[^90]: 半空间的高效测试学习算法

    An Efficient Tester-Learner for Halfspaces. (arXiv:2302.14853v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14853](http://arxiv.org/abs/2302.14853)

    我们提出了第一个在可测试学习模型中学习半空间的高效算法，该算法在多项式时间内运行，并输出一个在任何强对数凹目标分布下具有（信息理论上最优的）误差的假设。

    We propose the first efficient algorithm for learning halfspaces in the testable learning model, which runs in polynomial time and outputs a hypothesis with (information-theoretically optimal) error for any strongly log-concave target distribution.

    我们提出了第一个在Rubinfeld和Vasilyan（2023）最近定义的可测试学习模型中学习半空间的高效算法。在这个模型中，当训练集通过相关测试时，学习者证明其输出假设的准确性接近最优，并且从某些目标分布（例如高斯分布）中抽取的训练集必须通过测试。这个模型比分布特定的不可知或Massart噪声模型更具挑战性，因为如果分布假设不成立，学习者可以任意失败。我们考虑目标分布为高斯分布（或更一般的任何强对数凹分布）的$d$维情况，噪声模型为Massart或对抗性（不可知）。对于Massart噪声，我们的测试学习算法在多项式时间内运行，并输出一个在任何强对数凹目标分布下具有（信息理论上最优的）误差$\mathsf{opt}+\epsilon$的假设。

    We give the first efficient algorithm for learning halfspaces in the testable learning model recently defined by Rubinfeld and Vasilyan (2023). In this model, a learner certifies that the accuracy of its output hypothesis is near optimal whenever the training set passes an associated test, and training sets drawn from some target distribution -- e.g., the Gaussian -- must pass the test. This model is more challenging than distribution-specific agnostic or Massart noise models where the learner is allowed to fail arbitrarily if the distributional assumption does not hold.  We consider the setting where the target distribution is Gaussian (or more generally any strongly log-concave distribution) in $d$ dimensions and the noise model is either Massart or adversarial (agnostic). For Massart noise, our tester-learner runs in polynomial time and outputs a hypothesis with (information-theoretically optimal) error $\mathsf{opt} + \epsilon$ for any strongly log-concave target distribution. For 
    
[^91]: 一份用于学习图表示以预测时尚零售客户退货的数据集

    A Dataset for Learning Graph Representations to Predict Customer Returns in Fashion Retail. (arXiv:2302.14096v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14096](http://arxiv.org/abs/2302.14096)

    该论文介绍了一个由ASOS收集的新型数据集，用于解决时尚零售生态系统中预测客户退货的挑战。研究者使用图表示学习方法，提高了退货预测分类任务的F1分数至0.792，这比其他模型有所改进。

    This paper introduces a novel dataset collected by ASOS for predicting customer returns in a fashion retail ecosystem. The researchers use Graph Representation Learning to improve the F1-score of the return prediction classification task to 0.792, outperforming other models.

    我们提出了一个由ASOS（一家主要的在线时尚零售商）收集的新型数据集，以解决在时尚零售生态系统中预测客户退货的挑战。通过发布这个庞大的数据集，我们希望激发研究社区和时尚行业之间的进一步合作。我们首先探讨了这个数据集的结构，重点关注图表示学习的应用，以利用自然数据结构并提供对数据中特定特征的统计洞察。除此之外，我们展示了一个退货预测分类任务的示例，其中包括一些基线模型（即没有中间表示学习步骤）和基于图表示的模型。我们展示了在下游退货预测分类任务中，使用图神经网络（GNN）可以找到F1分数为0.792，这比本文讨论的其他模型有所改进。除了这个增加的F1分数，我们还提出了一个l

    We present a novel dataset collected by ASOS (a major online fashion retailer) to address the challenge of predicting customer returns in a fashion retail ecosystem. With the release of this substantial dataset we hope to motivate further collaboration between research communities and the fashion industry. We first explore the structure of this dataset with a focus on the application of Graph Representation Learning in order to exploit the natural data structure and provide statistical insights into particular features within the data. In addition to this, we show examples of a return prediction classification task with a selection of baseline models (i.e. with no intermediate representation learning step) and a graph representation based model. We show that in a downstream return prediction classification task, an F1-score of 0.792 can be found using a Graph Neural Network (GNN), improving upon other models discussed in this work. Alongside this increased F1-score, we also present a l
    
[^92]: 重新审视带有正则化伪标签的表格数据自训练

    Revisiting Self-Training with Regularized Pseudo-Labeling for Tabular Data. (arXiv:2302.14013v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14013](http://arxiv.org/abs/2302.14013)

    本文重新审视了自我训练，并引入了课程伪标签用于表格领域。提出了一种新的伪标签方法，它规范化了置信度分数。

    This paper revisits self-training and introduces curriculum pseudo-labeling for tabular data. A novel pseudo-labeling approach is proposed to regularize the confidence scores of pseudo-labels generated from unlabeled data.

    半监督和自监督学习的最新进展已经打破了长期以来对于机器学习需要大量标记数据和未标记数据的无关性的信仰。虽然在各种数据上都取得了成功，但目前没有一种主导的半监督和自监督学习方法可以推广到表格数据（即大多数现有方法需要适当的表格数据集和架构）。在本文中，我们重新审视了自我训练，它可以应用于任何类型的算法，包括最广泛使用的架构——梯度提升决策树，并引入了课程伪标签（一种图像领域的最先进的伪标签技术）用于表格领域。此外，现有的伪标签技术在计算从未标记数据生成的伪标签的置信度分数时不能保证聚类假设。为了克服这个问题，我们提出了一种新的伪标签方法，它规范化了置信度分数。

    Recent progress in semi- and self-supervised learning has caused a rift in the long-held belief about the need for an enormous amount of labeled data for machine learning and the irrelevancy of unlabeled data. Although it has been successful in various data, there is no dominant semi- and self-supervised learning method that can be generalized for tabular data (i.e. most of the existing methods require appropriate tabular datasets and architectures). In this paper, we revisit self-training which can be applied to any kind of algorithm including the most widely used architecture, gradient boosting decision tree, and introduce curriculum pseudo-labeling (a state-of-the-art pseudo-labeling technique in image) for a tabular domain. Furthermore, existing pseudo-labeling techniques do not assure the cluster assumption when computing confidence scores of pseudo-labels generated from unlabeled data. To overcome this issue, we propose a novel pseudo-labeling approach that regularizes the confid
    
[^93]: 学习分子属性预测的拓扑专家

    Learning Topology-Specific Experts for Molecular Property Prediction. (arXiv:2302.13693v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13693](http://arxiv.org/abs/2302.13693)

    本文提出了TopExpert，利用拓扑特定的预测模型（称为专家），每个专家负责每个共享类似拓扑语义的分子组，以提高分子属性预测的性能。

    This paper proposes TopExpert, which leverages topology-specific prediction models (referred to as experts) to improve the performance of molecular property prediction by assigning each expert to a molecular group sharing similar topological semantics.

    最近，图神经网络（GNN）已成功应用于预测分子属性，这是一种具有各种应用的最经典的化学信息学任务。尽管它们很有效，但我们经验性地观察到，为具有不同结构模式的多种分子训练单个GNN模型会限制其预测性能。因此，我们提出了TopExpert，利用拓扑特定的预测模型（称为专家），每个专家负责每个共享类似拓扑语义的分子组。也就是说，每个专家在与其相应的拓扑组一起训练时学习特定于拓扑的判别特征。为了解决按其拓扑模式对分子进行分组的关键挑战，我们引入了基于聚类的门控模块，将输入分子分配到其中一个聚类中，并使用两种不同类型的自监督进一步优化门控模块。

    Recently, graph neural networks (GNNs) have been successfully applied to predicting molecular properties, which is one of the most classical cheminformatics tasks with various applications. Despite their effectiveness, we empirically observe that training a single GNN model for diverse molecules with distinct structural patterns limits its prediction performance. In this paper, motivated by this observation, we propose TopExpert to leverage topology-specific prediction models (referred to as experts), each of which is responsible for each molecular group sharing similar topological semantics. That is, each expert learns topology-specific discriminative features while being trained with its corresponding topological group. To tackle the key challenge of grouping molecules by their topological patterns, we introduce a clustering-based gating module that assigns an input molecule into one of the clusters and further optimizes the gating module with two different types of self-supervision:
    
[^94]: UnbiasedNets：神经网络中的鲁棒性偏差缓解的数据集多样化框架

    UnbiasedNets: A Dataset Diversification Framework for Robustness Bias Alleviation in Neural Networks. (arXiv:2302.12538v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12538](http://arxiv.org/abs/2302.12538)

    本文提出了UnbiasedNets框架，通过利用K-means聚类和NN的噪声容忍度来使给定的训练数据集多样化，生成平衡的数据集并减少数据集内的偏差。

    This paper proposes the UnbiasedNets framework, which leverages K-means clustering and the NN's noise tolerance to diversify the given training dataset, generating balanced datasets and reducing bias within them.

    过去几年中，训练神经网络（NN）模型的性能（以测试准确性为指标）已经显著提高，特别是随着深度学习的出现。然而，即使是最准确的NN也可能对特定输出分类存在偏差，这是由于可用训练数据集中的固有偏差所致，这可能会传播到实际实现中。本文涉及鲁棒性偏差，即通过对某个输出类别的噪声具有显着的大鲁棒性，与其余输出类别相比，训练的NN所表现出的偏差。结果表明，偏差来自不平衡的数据集，即所有输出类别未被平等地表示的数据集。为此，我们提出了UnbiasedNets框架，它利用K-means聚类和NN的噪声容忍度来使给定的训练数据集多样化，即使是相对较小的数据集。这将生成平衡的数据集并减少数据集内的偏差。

    Performance of trained neural network (NN) models, in terms of testing accuracy, has improved remarkably over the past several years, especially with the advent of deep learning. However, even the most accurate NNs can be biased toward a specific output classification due to the inherent bias in the available training datasets, which may propagate to the real-world implementations. This paper deals with the robustness bias, i.e., the bias exhibited by the trained NN by having a significantly large robustness to noise for a certain output class, as compared to the remaining output classes. The bias is shown to result from imbalanced datasets, i.e., the datasets where all output classes are not equally represented. Towards this, we propose the UnbiasedNets framework, which leverages K-means clustering and the NN's noise tolerance to diversify the given training dataset, even from relatively smaller datasets. This generates balanced datasets and reduces the bias within the datasets themse
    
[^95]: 多模态联邦学习：对比表示集成

    Multimodal Federated Learning via Contrastive Representation Ensemble. (arXiv:2302.08888v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08888](http://arxiv.org/abs/2302.08888)

    本文提出了一种名为CreamFL的多模态联邦学习框架，可以从具有异构模型架构和数据模态的客户端中训练更大的服务器模型，同时只在公共数据集上传递知识。

    This paper proposes a multimodal federated learning framework called CreamFL, which enables training larger server models from clients with heterogeneous model architectures and data modalities, while only communicating knowledge on public dataset.

    随着现代移动系统和物联网基础设施上的多媒体数据量的增加，利用这些丰富的多模态数据而不违反用户隐私成为一个关键问题。联邦学习（FL）作为集中式机器学习的隐私意识替代方案。然而，现有的扩展到多模态数据的FL方法都依赖于单模态级别的模型聚合，这限制了服务器和客户端在每个模态上具有相同的模型架构。这限制了全局模型的模型复杂度和数据容量，更不用说任务多样性了。在这项工作中，我们提出了对比表示集成和多模态FL聚合（CreamFL），这是一个多模态联邦学习框架，它可以从具有异构模型架构和数据模态的客户端中训练更大的服务器模型，同时只在公共数据集上传递知识。为了实现更好的多模态表示融合，我们设计了一个全局-

    With the increasing amount of multimedia data on modern mobile systems and IoT infrastructures, harnessing these rich multimodal data without breaching user privacy becomes a critical issue. Federated learning (FL) serves as a privacy-conscious alternative to centralized machine learning. However, existing FL methods extended to multimodal data all rely on model aggregation on single modality level, which restrains the server and clients to have identical model architecture for each modality. This limits the global model in terms of both model complexity and data capacity, not to mention task diversity. In this work, we propose Contrastive Representation Ensemble and Aggregation for Multimodal FL (CreamFL), a multimodal federated learning framework that enables training larger server models from clients with heterogeneous model architectures and data modalities, while only communicating knowledge on public dataset. To achieve better multimodal representation fusion, we design a global-
    
[^96]: 工业环境中烟羽上升测量的深度卷积神经网络

    Deep Convolutional Neural Network for Plume Rise Measurements in Industrial Environments. (arXiv:2302.07416v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07416](http://arxiv.org/abs/2302.07416)

    本文提出了一种低成本的测量技术，用于监测烟囱烟羽并进行长期实时测量。基于深度卷积神经网络（DCNNs）开发了一种两阶段方法，其中第一阶段使用改进的Mask R-CNN来识别烟羽，第二阶段使用最小二乘法将渐近模型拟合到边界中心线中，以估计烟囱烟羽上升。

    This paper proposes a low-cost measurement technology to monitor smokestack plume rise and make long-term, real-time measurements. A two-stage method based on Deep Convolutional Neural Networks (DCNNs) is developed, where the first stage uses an improved Mask R-CNN to recognize the plume and the second stage uses least squares to fit an asymptotic model to the plume boundaries centerline to estimate the smokestack plume rise.

    估计烟羽云高度对于各种应用非常重要，例如全球气候模型。烟囱烟羽上升是烟羽在其动量耗散并且烟羽和环境温度相等时向下飘移的恒定高度。尽管大多数空气质量模型使用不同的参数化来预测烟囱烟羽上升，但它们尚未得到彻底验证。本文提出了一种低成本的测量技术，用于监测烟囱烟羽并进行长期实时测量。为此，基于深度卷积神经网络（DCNNs）开发了一种两阶段方法。在第一阶段中，应用改进的Mask R-CNN，称为Deep Plume Rise Network（DPRNet），来识别烟羽。这里，分别使用图像处理分析和最小二乘法来检测烟羽边界并将渐近模型拟合到边界中心线中。该模型的关键点的y分量坐标被认为是烟囱烟羽上升。在第二阶段中

    Estimating Plume Cloud (PC) height is essential for various applications, such as global climate models. Smokestack Plume Rise (PR) is the constant height at which the PC is carried downwind as its momentum dissipates and the PC and the ambient temperatures equalize. Although different parameterizations are used in most air-quality models to predict PR, they have yet to be verified thoroughly. This paper proposes a low-cost measurement technology to monitor smokestack PCs and make long-term, real-time measurements of PR. For this purpose, a two-stage method is developed based on Deep Convolutional Neural Networks (DCNNs). In the first stage, an improved Mask R-CNN, called Deep Plume Rise Network (DPRNet), is applied to recognize the PC. Here, image processing analyses and least squares, respectively, are used to detect PC boundaries and fit an asymptotic model into the boundaries centerline. The y-component coordinate of this model's critical point is considered PR. In the second stage
    
[^97]: 学习模型对于强化学习控制偏微分方程的样本效率至关重要

    Learning a model is paramount for sample efficiency in reinforcement learning control of PDEs. (arXiv:2302.07160v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07160](http://arxiv.org/abs/2302.07160)

    本文强调在使用强化学习进行偏微分方程控制时使用动态模型的重要性。使用卷积LSTM作为带有激励的数据驱动代理模型可以显著减少从真实系统中采样所需的总数据量。迭代更新模型对于避免RL训练中的偏差非常重要。

    This paper emphasizes the importance of using dynamic models when using reinforcement learning for feedback control of partial differential equations. Using a convolutional LSTM as a data-driven surrogate model with actuation significantly reduces the total amount of required data sampled from the real system. Iteratively updating the model is crucial to avoid biases in the RL training.

    本文旨在强调在使用强化学习（RL）进行偏微分方程（PDEs）控制的反馈控制时使用动态模型的重要性。为了弥合我们在RL中看到的巨大承诺与复杂工程系统中的适用性之间的差距，主要挑战是在训练数据方面的巨大需求以及缺乏性能保证。我们提出了一种解决方案，使用卷积LSTM作为带有激励的数据驱动代理模型来解决第一个问题。我们证明了在训练RL代理的同时并行学习一个带有激励的模型可以显著减少从真实系统中采样所需的总数据量。此外，我们还展示了迭代更新模型对于避免RL训练中的偏差非常重要。详细的消融研究揭示了建模过程中最重要的因素。我们使用混沌Kuramoto-Sivashinsky方程进行了实验验证。

    The goal of this paper is to make a strong point for the usage of dynamical models when using reinforcement learning (RL) for feedback control of dynamical systems governed by partial differential equations (PDEs). To breach the gap between the immense promises we see in RL and the applicability in complex engineering systems, the main challenges are the massive requirements in terms of the training data, as well as the lack of performance guarantees. We present a solution for the first issue using a data-driven surrogate model in the form of a convolutional LSTM with actuation. We demonstrate that learning an actuated model in parallel to training the RL agent significantly reduces the total amount of required data sampled from the real system. Furthermore, we show that iteratively updating the model is of major importance to avoid biases in the RL training. Detailed ablation studies reveal the most important ingredients of the modeling process. We use the chaotic Kuramoto-Sivashinsky
    
[^98]: Lightsolver挑战领先的深度学习求解器解决Max-2-SAT问题

    Lightsolver challenges a leading deep learning solver for Max-2-SAT problems. (arXiv:2302.06926v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2302.06926](http://arxiv.org/abs/2302.06926)

    本文比较了LightSolver的量子启发式算法和领先的深度学习求解器在MAX-2-SAT问题上的表现，实验结果表明LightSolver实现了显著更小的最优解时间。

    This paper compares LightSolver's quantum-inspired algorithm to a leading deep-learning solver for the MAX-2-SAT problem, and shows that LightSolver achieves significantly smaller time-to-optimal-solution compared to a state-of-the-art deep-learning algorithm.

    最大2-SAT问题（MAX-2-SAT）是一种已知为NP难的组合决策问题。本文比较了LightSolver的量子启发式算法和领先的深度学习求解器在MAX-2-SAT问题上的表现。基准数据集上的实验表明，与最先进的深度学习算法相比，LightSolver实现了显著更小的最优解时间，其中性能提升的增益往往随着问题规模的增加而增加。

    Maximum 2-satisfiability (MAX-2-SAT) is a type of combinatorial decision problem that is known to be NP-hard. In this paper, we compare LightSolver's quantum-inspired algorithm to a leading deep-learning solver for the MAX-2-SAT problem. Experiments on benchmark data sets show that LightSolver achieves significantly smaller time-to-optimal-solution compared to a state-of-the-art deep-learning algorithm, where the gain in performance tends to increase with the problem size.
    
[^99]: 基于惩罚的双层梯度下降方法研究

    On Penalty-based Bilevel Gradient Descent Method. (arXiv:2302.05185v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05185](http://arxiv.org/abs/2302.05185)

    本文提出了基于惩罚的双层梯度下降算法，解决了下层非强凸约束双层问题，实验表明该算法有效。

    This paper proposes a penalty-based bilevel gradient descent algorithm to solve the constrained bilevel problem without lower-level strong convexity, and experiments show its efficiency.

    双层优化在超参数优化、元学习和强化学习等领域有广泛应用，但是双层优化问题难以解决。最近的可扩展双层算法主要集中在下层目标函数是强凸或无约束的双层优化问题上。在本文中，我们通过惩罚方法来解决双层问题。我们证明，在一定条件下，惩罚重构可以恢复原始双层问题的解。此外，我们提出了基于惩罚的双层梯度下降（PBGD）算法，并证明了其在下层非强凸约束双层问题上的有限时间收敛性。实验展示了所提出的PBGD算法的效率。

    Bilevel optimization enjoys a wide range of applications in hyper-parameter optimization, meta-learning and reinforcement learning. However, bilevel optimization problems are difficult to solve. Recent progress on scalable bilevel algorithms mainly focuses on bilevel optimization problems where the lower-level objective is either strongly convex or unconstrained. In this work, we tackle the bilevel problem through the lens of the penalty method. We show that under certain conditions, the penalty reformulation recovers the solutions of the original bilevel problem. Further, we propose the penalty-based bilevel gradient descent (PBGD) algorithm and establish its finite-time convergence for the constrained bilevel problem without lower-level strong convexity. Experiments showcase the efficiency of the proposed PBGD algorithm.
    
[^100]: 利用剪枝神经网络中的稀疏性来优化大型模型训练

    Exploiting Sparsity in Pruned Neural Networks to Optimize Large Model Training. (arXiv:2302.05045v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05045](http://arxiv.org/abs/2302.05045)

    本文提出了一种新的方法，利用稀疏子网络来优化两种流行的深度学习并行算法 - 数据并行和层间并行的内存利用和通信。在512个NVIDIA V100 GPU上，我们的优化将27亿参数模型的内存消耗减少了74％，总通信时间减少了40％。

    This paper proposes a novel approach that exploits sparse subnetworks to optimize memory utilization and communication in two popular algorithms for parallel deep learning, and demonstrates significant reductions in memory consumption and communication time on a 2.7 billion parameter model using 512 NVIDIA V100 GPUs.

    由于通信开销的显著增加，规模化神经网络的并行训练具有挑战性。最近，深度学习研究人员开发了各种剪枝算法，能够剪枝（即将神经网络中的参数设置为零）80-90％的参数，以产生与未剪枝父网络相等的稀疏子网络。在本文中，我们提出了一种新的方法，利用这些稀疏子网络来优化两种流行的深度学习并行算法 - 数据并行和层间并行的内存利用和通信。我们将我们的方法集成到AxoNN中，这是一个高度可扩展的并行深度学习框架，依赖于数据和层间并行，并展示了通信时间和内存利用的减少。在512个NVIDIA V100 GPU上，我们的优化将27亿参数模型的内存消耗减少了74％，总通信时间减少了40％，从而提供了

    Parallel training of neural networks at scale is challenging due to significant overheads arising from communication. Recently, deep learning researchers have developed a variety of pruning algorithms that are capable of pruning (i.e. setting to zero) 80-90% of the parameters in a neural network to yield sparse subnetworks that equal the accuracy of the unpruned parent network. In this work, we propose a novel approach that exploits these sparse subnetworks to optimize the memory utilization and communication in two popular algorithms for parallel deep learning namely -- data and inter-layer parallelism. We integrate our approach into AxoNN, a highly scalable framework for parallel deep learning that relies on data and inter-layer parallelism, and demonstrate the reduction in communication time and memory utilization. On 512 NVIDIA V100 GPUs, our optimizations reduce the memory consumption of a 2.7 billion parameter model by 74%, and the total communication time by 40%, thus providing 
    
[^101]: 公平机器学习的不公平性：默认的水平下降和严格平等主义

    The Unfairness of Fair Machine Learning: Levelling down and strict egalitarianism by default. (arXiv:2302.02404v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.02404](http://arxiv.org/abs/2302.02404)

    公平机器学习的定义过于简单，许多公平性措施会导致性能降级和水平下降，使每个人都变得更糟，这种做法不符合实质平等的机会。

    The oversimplification of fairness in machine learning leads to performance degradation and leveling down, which does not achieve substantive equality.

    近年来，机器学习中的公平性已成为一个高度活跃的研究和开发领域。大多数人简单地定义公平性，即公平意味着减少不同人群之间的表现或结果差距，同时尽可能保留原始系统的准确性。这种通过公平性措施简化平等的做法令人不安。许多当前的公平性措施既存在公平性和性能降级，或者“水平下降”，即通过使每个群体变得更糟或将表现更好的群体降到最差的水平来实现公平性。当公平只能通过在物质或关系方面使每个人变得更糟而实现，通过污名化、团结的损失、不平等的关注和错过实质平等的机会，似乎在将模糊的“公平”概念转化为实践时出现了问题。本文探讨了造成这种现象的原因和普遍性。

    In recent years fairness in machine learning (ML) has emerged as a highly active area of research and development. Most define fairness in simple terms, where fairness means reducing gaps in performance or outcomes between demographic groups while preserving as much of the accuracy of the original system as possible. This oversimplification of equality through fairness measures is troubling. Many current fairness measures suffer from both fairness and performance degradation, or "levelling down," where fairness is achieved by making every group worse off, or by bringing better performing groups down to the level of the worst off. When fairness can only be achieved by making everyone worse off in material or relational terms through injuries of stigma, loss of solidarity, unequal concern, and missed opportunities for substantive equality, something would appear to have gone wrong in translating the vague concept of 'fairness' into practice. This paper examines the causes and prevalence 
    
[^102]: 通过层变分分析解释领域适应

    Interpretations of Domain Adaptations via Layer Variational Analysis. (arXiv:2302.01798v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01798](http://arxiv.org/abs/2302.01798)

    本研究通过层变分分析证明了转移学习的成功可以通过相应的数据条件得到保证，并提出了一种基于网络的转移学习的替代方法，该方法在领域适应方面显示出了效率和准确性的提高。

    This study establishes the theory of transfer learning in deep learning through formal derivations and heuristic analysis, proving that the success of transfer learning can be guaranteed with corresponding data conditions. An alternative method for network-based transfer learning is proposed, which shows an increase in efficiency and accuracy for domain adaptation.

    转移学习在许多应用中表现出高效的性能，但有限的文献报道了其背后的机制。本研究建立了正式的推导和启发式分析，以制定深度学习中转移学习的理论。我们的框架利用层变分分析证明了转移学习的成功可以通过相应的数据条件得到保证。此外，我们的理论计算产生了对知识转移过程的直观解释。随后，我们推导出了一种基于网络的转移学习的替代方法。该方法在领域适应方面显示出了效率和准确性的提高。当适应期间的新领域数据足够稀疏时，它特别有优势。对各种任务的数值实验验证了我们的理论，并验证了我们的分析表达式在领域适应方面比梯度下降方法表现更好。

    Transfer learning is known to perform efficiently in many applications empirically, yet limited literature reports the mechanism behind the scene. This study establishes both formal derivations and heuristic analysis to formulate the theory of transfer learning in deep learning. Our framework utilizing layer variational analysis proves that the success of transfer learning can be guaranteed with corresponding data conditions. Moreover, our theoretical calculation yields intuitive interpretations towards the knowledge transfer process. Subsequently, an alternative method for network-based transfer learning is derived. The method shows an increase in efficiency and accuracy for domain adaptation. It is particularly advantageous when new domain data is sufficiently sparse during adaptation. Numerical experiments over diverse tasks validated our theory and verified that our analytic expression achieved better performance in domain adaptation than the gradient descent method.
    
[^103]: 重新思考半监督医学图像分割：方差缩减的视角

    Rethinking Semi-Supervised Medical Image Segmentation: A Variance-Reduction Perspective. (arXiv:2302.01735v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.01735](http://arxiv.org/abs/2302.01735)

    本文提出了ARCO，一种半监督对比学习（CL）框架，其中包括医学图像分割中的分层组采样理论。通过方差缩减估计的概念来构建ARCO，并表明某些方差缩减技术在医学图像分割中特别有益。

    This paper proposes ARCO, a semi-supervised contrastive learning (CL) framework with stratified group sampling theory in medical image segmentation. The concept of variance-reduced estimation is used to build ARCO, and certain variance-reduction techniques are shown to be particularly beneficial in medical image segmentation.

    对于医学图像分割，对比学习是提高视觉表示质量的主要方法，通过对比语义相似和不相似的样本对来实现。这是通过观察到，在没有访问地面真实标签的情况下，如果采样具有真正不同解剖特征的负样本，则可以显着提高性能。然而，在现实中，这些样本可能来自相似的解剖特征，模型可能难以区分少数尾类样本，使得尾类更容易被错误分类，这通常导致模型崩溃。在本文中，我们提出了ARCO，一种半监督对比学习（CL）框架，其中包括医学图像分割中的分层组采样理论。特别是，我们首先提出通过方差缩减估计的概念来构建ARCO，并表明某些方差缩减技术在医学图像分割中特别有益。

    For medical image segmentation, contrastive learning is the dominant practice to improve the quality of visual representations by contrasting semantically similar and dissimilar pairs of samples. This is enabled by the observation that without accessing ground truth label, negative examples with truly dissimilar anatomical features, if sampled, can significantly improve the performance. In reality, however, these samples may come from similar anatomical features and the models may struggle to distinguish the minority tail-class samples, making the tail classes more prone to misclassification, both of which typically lead to model collapse. In this paper, we propose ARCO, a semi-supervised contrastive learning (CL) framework with stratified group sampling theory in medical image segmentation. In particular, we first propose building ARCO through the concept of variance-reduced estimation, and show that certain variance-reduction techniques are particularly beneficial in medical image se
    
[^104]: 基于自组织高斯混合模型的概率点云建模

    Probabilistic Point Cloud Modeling via Self-Organizing Gaussian Mixture Models. (arXiv:2302.00047v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00047](http://arxiv.org/abs/2302.00047)

    本文提出了一种基于自组织高斯混合模型的概率点云建模方法，可以根据场景复杂度自动调整模型复杂度，相比现有技术具有更好的泛化性能。

    This paper proposes a probabilistic point cloud modeling method based on self-organizing Gaussian mixture models, which can automatically adjust the model complexity according to the scene complexity, and has better generalization performance compared to existing techniques.

    本文提出了一种连续的概率建模方法，用于使用有限高斯混合模型（GMM）对空间点云数据进行建模，其中组件的数量基于场景复杂性进行调整。我们利用信息论学习中的自组织原理，根据传感器数据中的相关信息自动调整GMM模型的复杂度。该方法在具有不同场景复杂度的实际数据上与现有的点云建模技术进行了评估。

    This letter presents a continuous probabilistic modeling methodology for spatial point cloud data using finite Gaussian Mixture Models (GMMs) where the number of components are adapted based on the scene complexity. Few hierarchical and adaptive methods have been proposed to address the challenge of balancing model fidelity with size. Instead, state-of-the-art mapping approaches require tuning parameters for specific use cases, but do not generalize across diverse environments. To address this gap, we utilize a self-organizing principle from information-theoretic learning to automatically adapt the complexity of the GMM model based on the relevant information in the sensor data. The approach is evaluated against existing point cloud modeling techniques on real-world data with varying degrees of scene complexity.
    
[^105]: LDMIC：基于学习的分布式多视图图像编码

    LDMIC: Learning-based Distributed Multi-view Image Coding. (arXiv:2301.09799v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2301.09799](http://arxiv.org/abs/2301.09799)

    LDMIC是一种基于学习的分布式多视图图像编码框架，通过独立编码器和联合上下文传输模块实现了全局视图间的相关性捕捉，对几何关系不敏感。

    LDMIC is a learning-based distributed multi-view image coding framework that captures global inter-view correlations through independent encoders and a joint context transfer module based on the cross-attention mechanism, which is insensitive to geometric relations.

    多视图图像压缩在3D相关应用中起着至关重要的作用。现有方法采用预测编码架构，需要联合编码压缩相应的视差和残差信息。这要求相机之间进行协作，并强制执行不同视图之间的极线几何约束，这使得在具有随机重叠视野的分布式相机系统中部署这些方法具有挑战性。同时，分布式源编码理论表明，可以通过独立编码和联合解码实现相关源的高效数据压缩，这激发了我们设计基于学习的分布式多视图图像编码（LDMIC）框架的动机。通过独立编码器，LDMIC引入了一个简单而有效的基于交叉注意机制的联合上下文传输模块，以有效捕捉全局视图间的相关性，对几何关系不敏感。

    Multi-view image compression plays a critical role in 3D-related applications. Existing methods adopt a predictive coding architecture, which requires joint encoding to compress the corresponding disparity as well as residual information. This demands collaboration among cameras and enforces the epipolar geometric constraint between different views, which makes it challenging to deploy these methods in distributed camera systems with randomly overlapping fields of view. Meanwhile, distributed source coding theory indicates that efficient data compression of correlated sources can be achieved by independent encoding and joint decoding, which motivates us to design a learning-based distributed multi-view image coding (LDMIC) framework. With independent encoders, LDMIC introduces a simple yet effective joint context transfer module based on the cross-attention mechanism at the decoder to effectively capture the global inter-view correlations, which is insensitive to the geometric relation
    
[^106]: 走向神经人工智能：将神经元多样性引入人工神经网络

    Towards NeuroAI: Introducing Neuronal Diversity into Artificial Neural Networks. (arXiv:2301.09245v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2301.09245](http://arxiv.org/abs/2301.09245)

    引入神经元多样性可以解决人工神经网络的基本问题，走向神经人工智能。

    Introducing neuronal diversity can solve the fundamental problems of artificial neural networks and lead to NeuroAI.

    在整个历史上，人工智能的发展，特别是人工神经网络，一直对越来越深入的大脑理解持开放态度并不断受到启发，例如卷积神经网络的开创性工作neocognitron的启发。根据新兴领域神经人工智能的动机，大量的神经科学知识可以通过赋予网络更强大的能力来催化下一代人工智能的发展。我们知道，人类大脑有许多形态和功能不同的神经元，而人工神经网络几乎完全建立在单一神经元类型上。在人类大脑中，神经元多样性是各种生物智能行为的一个启动因素。由于人工网络是人类大脑的缩影，引入神经元多样性应该有助于解决人工网络的诸如效率、解释性等基本问题。

    Throughout history, the development of artificial intelligence, particularly artificial neural networks, has been open to and constantly inspired by the increasingly deepened understanding of the brain, such as the inspiration of neocognitron, which is the pioneering work of convolutional neural networks. Per the motives of the emerging field: NeuroAI, a great amount of neuroscience knowledge can help catalyze the next generation of AI by endowing a network with more powerful capabilities. As we know, the human brain has numerous morphologically and functionally different neurons, while artificial neural networks are almost exclusively built on a single neuron type. In the human brain, neuronal diversity is an enabling factor for all kinds of biological intelligent behaviors. Since an artificial network is a miniature of the human brain, introducing neuronal diversity should be valuable in terms of addressing those essential problems of artificial networks such as efficiency, interpret
    
[^107]: 用于评估CNN预测的可信度分数

    Trustworthiness Score to Evaluate CNNs Predictions. (arXiv:2301.08839v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08839](http://arxiv.org/abs/2301.08839)

    本文提出了一种用于评估CNN预测可信度的可信度分数（TS）度量标准，通过检查CNN所做预测中的某些特征的存在来量化预测的可信度。

    This paper proposes a trustworthiness score (TS) metric to evaluate the confidence of CNN predictions, which quantifies the trustworthiness by checking for the existence of certain features in the predictions made by the CNN.

    由于卷积神经网络（CNN）的黑盒特性，无法在操作期间持续验证CNN，这使得开发人员和监管机构难以对使用CNN的自主系统的部署获得信心。在操作期间，了解CNN的预测何时可信或可疑对安全至关重要。基本方法是使用模型的输出置信度分数来评估预测是否可信或可疑。然而，模型的置信度分数是来自黑盒计算的结果，因此缺乏透明度，使得很难将可信度归因于预测。我们引入了可信度分数（TS），这是一种简单的度量标准，提供了一种更透明和有效的方式来提供CNN预测的信心。该度量标准通过检查CNN所做预测中的某些特征的存在来量化预测的可信度。

    Due to the black box nature of Convolutional neural networks (CNNs), the continuous validation of CNNs during operation is infeasible. As a result this makes it difficult for developers and regulators to gain confidence in the deployment of autonomous systems employing CNNs. It is critical for safety during operation to know when a CNN's predictions are trustworthy or suspicious. The basic approach is to use the model's output confidence score to assess if predictions are trustworthy or suspicious. However, the model's confidence score is a result of computations coming from a black box, therefore lacks transparency and makes it challenging to credit trustworthiness to predictions. We introduce the trustworthiness score (TS), a simple metric that provides a more transparent and effective way of providing confidence in CNNs predictions. The metric quantifies the trustworthiness in a prediction by checking for the existence of certain features in the predictions made by the CNN. The TS m
    
[^108]: SegViz：基于联邦学习的多器官分割框架，适用于具有部分注释的异构数据集

    SegViz: A federated-learning based framework for multi-organ segmentation on heterogeneous data sets with partial annotations. (arXiv:2301.07074v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.07074](http://arxiv.org/abs/2301.07074)

    SegViz是一种基于联邦学习的框架，用于从分布式的非i.i.d数据集中训练具有部分注释的分割模型。使用FedBN作为聚合策略的SegViz框架在外部BTCV集上表现出优异的性能，分割的dice分数分别为0.93、0.83、0.55和0.75。

    SegViz is a federated learning-based framework for training segmentation models from distributed non-i.i.d datasets with partial annotations. The SegViz framework using FedBN as the aggregation strategy demonstrated excellent performance on the external BTCV set with dice scores of 0.93, 0.83, 0.55, and 0.75 for segmentation.

    分割是医学图像深度学习中最基本的任务之一，由于其多个下游临床应用而备受关注。然而，为医学图像生成手动注释是耗时的、需要高技能的、昂贵的工作，特别是对于3D图像。一个潜在的解决方案是从多个组的部分注释数据集中聚合知识，使用联邦学习协作训练全局模型。为此，我们提出了SegViz，一种基于联邦学习的框架，用于从分布式的非i.i.d数据集中训练具有部分注释的分割模型。将SegViz的性能与分别在每个数据集上单独训练模型以及集中聚合所有数据集并训练单个模型进行比较。使用FedBN作为聚合策略的SegViz框架在外部BTCV集上表现出优异的性能，分割的dice分数分别为0.93、0.83、0.55和0.75。

    Segmentation is one of the most primary tasks in deep learning for medical imaging, owing to its multiple downstream clinical applications. However, generating manual annotations for medical images is time-consuming, requires high skill, and is an expensive effort, especially for 3D images. One potential solution is to aggregate knowledge from partially annotated datasets from multiple groups to collaboratively train global models using Federated Learning. To this end, we propose SegViz, a federated learning-based framework to train a segmentation model from distributed non-i.i.d datasets with partial annotations. The performance of SegViz was compared against training individual models separately on each dataset as well as centrally aggregating all the datasets in one place and training a single model. The SegViz framework using FedBN as the aggregation strategy demonstrated excellent performance on the external BTCV set with dice scores of 0.93, 0.83, 0.55, and 0.75 for segmentation 
    
[^109]: 基于LASSO的广义不变匹配性质

    Generalized Invariant Matching Property via LASSO. (arXiv:2301.05975v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2301.05975](http://arxiv.org/abs/2301.05975)

    本文提出了一种基于LASSO的广义不变匹配性质，通过制定具有内在稀疏性的高维问题，将不变匹配性质推广到仅目标被干预的重要情况，并提出了一种更加稳健和计算效率高的算法，改进了现有算法。

    This paper proposes a generalized invariant matching property via LASSO, which formulates a high-dimensional problem with intrinsic sparsity and extends the invariant matching property to the important setting when only the target is intervened. The paper also presents a more robust and computation-efficient algorithm by leveraging a variant of Lasso, improving upon the existing algorithms.

    在分布变化的情况下进行学习是一项具有挑战性的任务。一种基本的方法是通过结构因果模型利用不变性原则。然而，当响应被干预时，不变性原则被违反，使得这种情况变得困难。最近的研究开发了不变匹配性质来研究这种情况，并显示出良好的性能。在本文中，通过制定具有内在稀疏性的高维问题，我们将不变匹配性质推广到仅目标被干预的重要情况。我们提出了一种更加稳健和计算效率高的算法，利用Lasso的变体，改进了现有算法。

    Learning under distribution shifts is a challenging task. One principled approach is to exploit the invariance principle via the structural causal models. However, the invariance principle is violated when the response is intervened, making it a difficult setting. In a recent work, the invariant matching property has been developed to shed light on this scenario and shows promising performance. In this work, by formulating a high-dimensional problem with intrinsic sparsity, we generalize the invariant matching property for an important setting when only the target is intervened. We propose a more robust and computation-efficient algorithm by leveraging a variant of Lasso, improving upon the existing algorithms.
    
[^110]: 机器学习识别出HD 142666中嵌入的原行星的运动学证据

    Kinematic Evidence of an Embedded Protoplanet in HD 142666 Identified by Machine Learning. (arXiv:2301.05075v2 [astro-ph.EP] UPDATED)

    [http://arxiv.org/abs/2301.05075](http://arxiv.org/abs/2301.05075)

    该论文使用机器学习模型在HD 142666的盘中识别出强烈的、局部的非开普勒运动，进而得出该盘中存在一个行星的结论，这是使用机器学习识别原行星盘中先前被忽视的非开普勒特征的第一步。

    This paper uses machine learning models to identify strong, localized non-Keplerian motion in the disk HD 142666, and concludes that there is a planet in the disk, which represents a first step towards using machine learning to identify previously overlooked non-Keplerian features in protoplanetary disks.

    原行星盘的观测表明，形成系外行星会在盘中的气体和尘埃上留下特征印记。在气体中，这些形成中的系外行星会引起开普勒运动的偏差，可以通过分子线观测来检测。我们之前的工作表明，机器学习可以正确确定这些盘中是否存在行星。使用我们的机器学习模型，我们在HD 142666的盘中识别出强烈的、局部的非开普勒运动。随后进行的一个系统中有一个5个木星质量的行星在75天文单位处的流体动力学模拟再现了这种运动学结构。根据该领域目前已经建立的标准，我们得出结论：HD 142666中存在一个行星。这项工作代表了使用机器学习识别原行星盘中先前被忽视的非开普勒特征的第一步。

    Observations of protoplanetary disks have shown that forming exoplanets leave characteristic imprints on the gas and dust of the disk. In the gas, these forming exoplanets cause deviations from Keplerian motion, which can be detected through molecular line observations. Our previous work has shown that machine learning can correctly determine if a planet is present in these disks. Using our machine learning models, we identify strong, localized non-Keplerian motion within the disk HD 142666. Subsequent hydrodynamics simulations of a system with a 5 Jupiter-mass planet at 75 au recreates the kinematic structure. By currently established standards in the field, we conclude that HD 142666 hosts a planet. This work represents a first step towards using machine learning to identify previously overlooked non-Keplerian features in protoplanetary disks.
    
[^111]: 感知-神经-物理声音匹配

    Perceptual-Neural-Physical Sound Matching. (arXiv:2301.02886v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2301.02886](http://arxiv.org/abs/2301.02886)

    本文提出了一种新的声音匹配算法，称为感知-神经-物理损失（PNP），它是频谱损失的最优二次近似，能够更好地适应不同参数的感知重要性，同时具有快速收敛的特点。

    This paper proposes a new sound matching algorithm called Perceptual-Neural-Physical loss (PNP), which is the optimal quadratic approximation of spectral loss and can better accommodate the differing perceptual significance of each parameter while having fast convergence.

    声音匹配算法旨在通过参数化音频合成来近似目标波形。深度神经网络在匹配持续谐波音调方面取得了有希望的结果。然而，当目标是非平稳和非谐波的时候，例如打击乐器，任务就更具挑战性。我们将这个问题归因于损失函数的不足。一方面，参数域中的均方误差，称为“P-loss”，简单快速，但未能适应每个参数的不同感知重要性。另一方面，频谱时间域中的均方误差，称为“频谱损失”，在感知上是有动机的，并在可微分数字信号处理（DDSP）中发挥作用。然而，频谱损失是音高间隔的不良预测因素，其梯度可能计算成本高，因此收敛速度较慢。在这个困境中，我们提出了感知-神经-物理损失（PNP）。PNP是频谱损失的最优二次近似，同时具有快速收敛的特点。

    Sound matching algorithms seek to approximate a target waveform by parametric audio synthesis. Deep neural networks have achieved promising results in matching sustained harmonic tones. However, the task is more challenging when targets are nonstationary and inharmonic, e.g., percussion. We attribute this problem to the inadequacy of loss function. On one hand, mean square error in the parametric domain, known as "P-loss", is simple and fast but fails to accommodate the differing perceptual significance of each parameter. On the other hand, mean square error in the spectrotemporal domain, known as "spectral loss", is perceptually motivated and serves in differentiable digital signal processing (DDSP). Yet, spectral loss is a poor predictor of pitch intervals and its gradient may be computationally expensive; hence a slow convergence. Against this conundrum, we present Perceptual-Neural-Physical loss (PNP). PNP is the optimal quadratic approximation of spectral loss while being as fast 
    
[^112]: 图像数据增强方法：综述与未来方向

    Image Data Augmentation Approaches: A Comprehensive Survey and Future directions. (arXiv:2301.02830v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.02830](http://arxiv.org/abs/2301.02830)

    本文综述了图像数据增强方法，提供了全面的分类法和每种技术的优缺点，并给出了数据增强对图像分类、目标检测和语义分割等计算机视觉任务的全面结果。

    This article provides a comprehensive survey of advanced data augmentation techniques for computer vision tasks, including a novel taxonomy and evaluation of each technique's strengths and weaknesses. The article also presents comprehensive results of the data augmentation effect on popular computer vision tasks such as image classification, object detection, and semantic segmentation.

    深度学习算法在各种计算机视觉任务中表现出了显著的性能。然而，由于标记数据有限，导致网络过拟合问题，即网络在未见过的数据上的性能比训练数据差。因此，它限制了性能的提高。为了应对这个问题，提出了各种技术，如dropout、归一化和高级数据增强。其中，数据增强旨在通过包括样本多样性来扩大数据集大小，近来成为热门话题。在本文中，我们重点关注高级数据增强技术。我们提供了数据增强的背景、一个新颖而全面的分类法、以及每种技术的优点和缺点（在可能的情况下）。我们还提供了数据增强对三个流行的计算机视觉任务（如图像分类、目标检测和语义分割）的全面结果。

    Deep learning (DL) algorithms have shown significant performance in various computer vision tasks. However, having limited labelled data lead to a network overfitting problem, where network performance is bad on unseen data as compared to training data. Consequently, it limits performance improvement. To cope with this problem, various techniques have been proposed such as dropout, normalization and advanced data augmentation. Among these, data augmentation, which aims to enlarge the dataset size by including sample diversity, has been a hot topic in recent times. In this article, we focus on advanced data augmentation techniques. we provide a background of data augmentation, a novel and comprehensive taxonomy of reviewed data augmentation techniques, and the strengths and weaknesses (wherever possible) of each technique. We also provide comprehensive results of the data augmentation effect on three popular computer vision tasks, such as image classification, object detection and seman
    
[^113]: 不可学习的聚类：面向标签不可知的不可学习样本

    Unlearnable Clusters: Towards Label-agnostic Unlearnable Examples. (arXiv:2301.01217v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2301.01217](http://arxiv.org/abs/2301.01217)

    本文提出了一种更实用的标签不可知设置，以生成不可学习的样本，防止未经授权的机器学习模型训练。

    This paper proposes a more practical label-agnostic setting to generate unlearnable examples, which can prevent unauthorized training of machine learning models.

    在互联网上，越来越多的人对开发不可学习的示例（UEs）来防止视觉隐私泄露感兴趣。UEs是添加了不可见但不可学习噪声的训练样本，已经发现可以防止未经授权的机器学习模型训练。UEs通常是通过一个双层优化框架和一个替代模型生成的，以从原始样本中去除（最小化）错误，然后应用于保护数据免受未知目标模型的攻击。然而，现有的UE生成方法都依赖于一个理想的假设，称为标签一致性，即假定黑客和保护者对于给定的样本持有相同的标签。在这项工作中，我们提出并推广了一个更实用的标签不可知设置，其中黑客可能会以与保护者完全不同的方式利用受保护的数据。例如，由保护者持有的m类不可学习数据集可能被黑客作为n类数据集利用。现有的UE生成方法在这种情况下失效。

    There is a growing interest in developing unlearnable examples (UEs) against visual privacy leaks on the Internet. UEs are training samples added with invisible but unlearnable noise, which have been found can prevent unauthorized training of machine learning models. UEs typically are generated via a bilevel optimization framework with a surrogate model to remove (minimize) errors from the original samples, and then applied to protect the data against unknown target models. However, existing UE generation methods all rely on an ideal assumption called label-consistency, where the hackers and protectors are assumed to hold the same label for a given sample. In this work, we propose and promote a more practical label-agnostic setting, where the hackers may exploit the protected data quite differently from the protectors. E.g., a m-class unlearnable dataset held by the protector may be exploited by the hacker as a n-class dataset. Existing UE generation methods are rendered ineffective in
    
[^114]: 一种用于婴儿脑可解释方法的注意力机制

    A attention way in Explainable methods for infant brain. (arXiv:2301.00815v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.00815](http://arxiv.org/abs/2301.00815)

    本文提出了一种可解释的几何深度网络，通过端到端学习解释因素以增强区分性表示提取，以反向保证细粒度的可解释性，适用于神经影像和神经科学研究中的高维数据。

    This paper proposes an explainable geometric deep network that enhances discriminative representation extraction by end-to-end learning of explanation factors, which is a more intuitive strategy to inversely assure fine-grained explainability, suitable for high-dimensional data in neuroimaging and neuroscience studies containing noisy, redundant, and task-irrelevant information.

    在跨学科应用中部署可靠的深度学习技术需要学习模型输出准确且（更重要的是）可解释的预测。现有方法通常以事后方式解释网络输出，隐含地假设忠实的解释来自准确的预测/分类。我们提出相反的观点，即解释提升（甚至决定）分类。也就是说，端到端学习解释因素以增强区分性表示提取可能是一种更直观的策略，以反向保证细粒度的可解释性，例如在那些包含噪声，冗余和任务无关信息的高维数据的神经影像和神经科学研究中。在本文中，我们提出了一种可解释的几何深度网络。

    Deploying reliable deep learning techniques in interdisciplinary applications needs learned models to output accurate and ({even more importantly}) explainable predictions. Existing approaches typically explicate network outputs in a post-hoc fashion, under an implicit assumption that faithful explanations come from accurate predictions/classifications. We have an opposite claim that explanations boost (or even determine) classification. That is, end-to-end learning of explanation factors to augment discriminative representation extraction could be a more intuitive strategy to inversely assure fine-grained explainability, e.g., in those neuroimaging and neuroscience studies with high-dimensional data containing noisy, redundant, and task-irrelevant information. In this paper, we propose such an explainable geometric deep network dubbed.
    
[^115]: 一次性领域自适应在基于视频的手术技能评估中的应用

    One-shot domain adaptation in video-based assessment of surgical skills. (arXiv:2301.00812v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.00812](http://arxiv.org/abs/2301.00812)

    本文提出了一种元学习模型A-VBANet，可以通过一次性学习提供领域不可知的手术技能分类，成功地适应了模拟任务和腹腔镜胆囊切除术，为基于视频的手术技能评估提供了领域不可知程序。

    This paper proposes a meta-learning model, A-VBANet, that can deliver domain-agnostic surgical skill classification via one-shot learning. The model successfully adapts to simulated tasks and laparoscopic cholecystectomy, providing a domain-agnostic procedure for video-based assessment of surgical skills.

    深度学习已经实现了手术技能的自动和客观评估。然而，深度学习模型需要大量数据，并且受限于其训练领域。这阻止了它们过渡到数据有限的新任务。因此，领域自适应对于在现实生活中实现深度学习至关重要。在这里，我们提出了一种元学习模型A-VBANet，它可以通过一次性学习提供领域不可知的手术技能分类。我们在五个腹腔镜和机器人手术模拟器上开发了A-VBANet。此外，我们在腹腔镜胆囊切除术的手术室视频上进行了测试。我们的模型成功地适应了模拟任务，准确率高达99.5%（一次性）和99.9%（少量样本），在腹腔镜胆囊切除术中的准确率为89.7%。我们首次提供了基于视频的手术技能评估的领域不可知程序。这种方法的一个重要影响是它允许使用来自手术模拟器的数据来评估手术表现。

    Deep Learning (DL) has achieved automatic and objective assessment of surgical skills. However, DL models are data-hungry and restricted to their training domain. This prevents them from transitioning to new tasks where data is limited. Hence, domain adaptation is crucial to implement DL in real life. Here, we propose a meta-learning model, A-VBANet, that can deliver domain-agnostic surgical skill classification via one-shot learning. We develop the A-VBANet on five laparoscopic and robotic surgical simulators. Additionally, we test it on operating room (OR) videos of laparoscopic cholecystectomy. Our model successfully adapts with accuracies up to 99.5% in one-shot and 99.9% in few-shot settings for simulated tasks and 89.7% for laparoscopic cholecystectomy. For the first time, we provide a domain-agnostic procedure for video-based assessment of surgical skills. A significant implication of this approach is that it allows the use of data from surgical simulators to assess performance 
    
[^116]: UniDA3D: 统一的域自适应三维语义分割管道

    UniDA3D: Unified Domain Adaptive 3D Semantic Segmentation Pipeline. (arXiv:2212.10390v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.10390](http://arxiv.org/abs/2212.10390)

    本文提出了UniDA3D，一种统一的域自适应三维语义分割管道，通过设计统一的源和目标主动采样策略，可以解决三维分割领域中的多个自适应任务，并探索了实现多模态采样策略的可能性。

    This paper proposes UniDA3D, a unified domain adaptive 3D semantic segmentation pipeline, which can tackle several adaptation tasks in 3D segmentation field by designing a unified source-and-target active sampling strategy, and investigates the possibility of achieving a multi-modal sampling strategy.

    目前的三维语义分割模型是在现成的公共基准上训练的，但当这些训练良好的模型部署到新领域时，它们将不可避免地面临识别精度下降的挑战。本文介绍了一种统一的域自适应三维语义分割管道（UniDA3D），以增强弱泛化能力，并弥合域之间的点分布差距。与之前只关注单一自适应任务的研究不同，UniDA3D可以通过设计统一的源和目标主动采样策略来解决三维分割领域中的多个自适应任务，该策略从源域和目标域中选择最具信息量的子集以实现有效的模型自适应。此外，受到多模态二维-三维数据集的崛起的影响，UniDA3D探索了实现多模态采样策略的可能性，通过开发跨模态特征交互模块，可以提取代表性对。

    State-of-the-art 3D semantic segmentation models are trained on off-the-shelf public benchmarks, but they will inevitably face the challenge of recognition accuracy drop when these well-trained models are deployed to a new domain. In this paper, we introduce a Unified Domain Adaptive 3D semantic segmentation pipeline (UniDA3D) to enhance the weak generalization ability, and bridge the point distribution gap between domains. Different from previous studies that only focus on a single adaptation task, UniDA3D can tackle several adaptation tasks in 3D segmentation field, by designing a unified source-and-target active sampling strategy, which selects a maximally-informative subset from both source and target domains for effective model adaptation. Besides, benefiting from the rise of multi-modal 2D-3D datasets, UniDA3D investigates the possibility of achieving a multi-modal sampling strategy, by developing a cross-modality feature interaction module that can extract a representative pair 
    
[^117]: 一份大规模的、基于PCR的COVID-19声音数据集

    A large-scale and PCR-referenced vocal audio dataset for COVID-19. (arXiv:2212.07738v3 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2212.07738](http://arxiv.org/abs/2212.07738)

    英国COVID-19 Vocal Audio Dataset是迄今为止最大的SARS-CoV-2 PCR参考音频记录集合，旨在为训练和评估使用声音数据分类SARS-CoV-2感染状态或相关呼吸症状的机器学习模型而设计。

    The UK COVID-19 Vocal Audio Dataset is the largest collection of SARS-CoV-2 PCR-referenced audio recordings to date, designed for the training and evaluation of machine learning models that classify SARS-CoV-2 infection status or associated respiratory symptoms using vocal audio.

    英国COVID-19 Vocal Audio Dataset旨在为训练和评估使用声音数据分类SARS-CoV-2感染状态或相关呼吸症状的机器学习模型而设计。英国卫生安全局通过国家测试和追踪计划和REACT-1调查在2021年3月至2022年3月期间招募了自愿参与者，收集了自愿咳嗽、呼气和语音的音频记录，并将其与SARS-CoV-2检测结果相关联。该数据集是迄今为止最大的SARS-CoV-2 PCR参考音频记录集合。

    The UK COVID-19 Vocal Audio Dataset is designed for the training and evaluation of machine learning models that classify SARS-CoV-2 infection status or associated respiratory symptoms using vocal audio. The UK Health Security Agency recruited voluntary participants through the national Test and Trace programme and the REACT-1 survey in England from March 2021 to March 2022, during dominant transmission of the Alpha and Delta SARS-CoV-2 variants and some Omicron variant sublineages. Audio recordings of volitional coughs, exhalations, and speech were collected in the 'Speak up to help beat coronavirus' digital survey alongside demographic, self-reported symptom and respiratory condition data, and linked to SARS-CoV-2 test results. The UK COVID-19 Vocal Audio Dataset represents the largest collection of SARS-CoV-2 PCR-referenced audio recordings to date. PCR results were linked to 70,794 of 72,999 participants and 24,155 of 25,776 positive cases. Respiratory symptoms were reported by 45.6
    
[^118]: 联邦贝叶斯学习中的客户端选择

    Client Selection for Federated Bayesian Learning. (arXiv:2212.05492v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.05492](http://arxiv.org/abs/2212.05492)

    本文提出了两种基于核化Stein差异（KSD）和希尔伯特内积（HIP）的DSVGD选择方案，以提高联邦贝叶斯学习中的模型收敛和通信效率。

    This paper proposes two selection schemes for Distributed Stein Variational Gradient Descent (DSVGD) based on Kernelized Stein Discrepancy (KSD) and Hilbert Inner Product (HIP) to improve the model convergence and communication efficiency in federated Bayesian learning.

    分布式Stein变分梯度下降（DSVGD）是一种非参数分布式学习框架，用于联邦贝叶斯学习，多个客户端通过与服务器通信一定数量的非随机和交互粒子来共同训练机器学习模型。由于通信资源有限，选择具有最具信息性的本地学习更新的客户端可以提高模型收敛和通信效率。本文提出了两种基于核化Stein差异（KSD）和希尔伯特内积（HIP）的DSVGD选择方案。我们推导了两种方案每次迭代全局自由能下降的上界，然后将其最小化以加速模型收敛。我们使用各种学习任务和数据集评估和比较了我们的方案与传统方案在模型准确性、收敛速度和稳定性方面的表现。

    Distributed Stein Variational Gradient Descent (DSVGD) is a non-parametric distributed learning framework for federated Bayesian learning, where multiple clients jointly train a machine learning model by communicating a number of non-random and interacting particles with the server. Since communication resources are limited, selecting the clients with most informative local learning updates can improve the model convergence and communication efficiency. In this paper, we propose two selection schemes for DSVGD based on Kernelized Stein Discrepancy (KSD) and Hilbert Inner Product (HIP). We derive the upper bound on the decrease of the global free energy per iteration for both schemes, which is then minimized to speed up the model convergence. We evaluate and compare our schemes with conventional schemes in terms of model accuracy, convergence speed, and stability using various learning tasks and datasets.
    
[^119]: P{\O}DA: 基于提示的零样本领域自适应

    P{\O}DA: Prompt-driven Zero-shot Domain Adaptation. (arXiv:2212.03241v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.03241](http://arxiv.org/abs/2212.03241)

    本文提出了一种基于提示的零样本领域自适应方法，通过利用预训练的对比视觉语言模型（CLIP）来优化源特征的仿射变换，将其引导到目标文本嵌入中，同时保留其内容和语义。实验表明，该方法在几个数据集上显著优于基于CLIP的风格转移基线，用于下游任务。

    This paper proposes a prompt-driven zero-shot domain adaptation method, which leverages a pretrained contrastive vision-language model (CLIP) to optimize affine transformations of source features, steering them towards target text embeddings, while preserving their content and semantics. Experiments demonstrate that the method significantly outperforms CLIP-based style transfer baselines on several datasets for the downstream task at hand.

    领域自适应在计算机视觉领域得到了广泛的研究，但仍需要在训练时访问目标图像，这在某些不常见的情况下可能是不可行的。本文提出了“基于提示的零样本领域自适应”任务，其中我们仅使用目标域的单个通用文本描述（即提示）来调整在源域上训练的模型。首先，我们利用预训练的对比视觉语言模型（CLIP）来优化源特征的仿射变换，将其引导到目标文本嵌入中，同时保留其内容和语义。其次，我们展示了增强的特征可以用于执行语义分割的零样本领域自适应。实验表明，我们的方法在几个数据集上显著优于基于CLIP的风格转移基线，用于下游任务。我们的基于提示的方法甚至在某些数据集上优于一次性无监督领域自适应，并且gi

    Domain adaptation has been vastly investigated in computer vision but still requires access to target images at train time, which might be intractable in some uncommon conditions. In this paper, we propose the task of `Prompt-driven Zero-shot Domain Adaptation', where we adapt a model trained on a source domain using only a single general textual description of the target domain, i.e., a prompt. First, we leverage a pretrained contrastive vision-language model (CLIP) to optimize affine transformations of source features, steering them towards target text embeddings, while preserving their content and semantics. Second, we show that augmented features can be used to perform zero-shot domain adaptation for semantic segmentation. Experiments demonstrate that our method significantly outperforms CLIP-based style transfer baselines on several datasets for the downstream task at hand. Our prompt-driven approach even outperforms one-shot unsupervised domain adaptation on some datasets, and gi
    
[^120]: 统一视觉、文本和布局的通用文档处理

    Unifying Vision, Text, and Layout for Universal Document Processing. (arXiv:2212.02623v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.02623](http://arxiv.org/abs/2212.02623)

    本文提出了通用文档处理（UDOP）模型，将文本、图像和布局模态以及各种任务格式统一起来，通过一种新颖的Transformer模型实现预训练和多域下游任务的统一，同时实现了高质量的神经文档编辑和内容定制。

    This paper proposes the Universal Document Processing (UDOP) model, which unifies text, image, and layout modalities together with varied task formats, and achieves pretraining and multi-domain downstream tasks unification through a novel Transformer model. It also achieves high-quality neural document editing and content customization.

    我们提出了通用文档处理（UDOP），这是一个基础的文档AI模型，它将文本、图像和布局模态以及各种任务格式（包括文档理解和生成）统一起来。UDOP利用文本内容和文档图像之间的空间相关性，用一个统一的表示来建模图像、文本和布局模态。通过一种新颖的Vision-Text-Layout Transformer，UDOP将预训练和多域下游任务统一到基于提示的序列生成方案中。UDOP在大规模无标签文档语料库和多样化标记数据上使用创新的自监督目标进行预训练。UDOP还通过遮蔽图像重建学习从文本和布局模态生成文档图像。据我们所知，这是文档AI领域中第一次使用一个模型同时实现高质量的神经文档编辑和内容定制。我们的方法在8个文档处理基准数据集上取得了最先进的结果。

    We propose Universal Document Processing (UDOP), a foundation Document AI model which unifies text, image, and layout modalities together with varied task formats, including document understanding and generation. UDOP leverages the spatial correlation between textual content and document image to model image, text, and layout modalities with one uniform representation. With a novel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain downstream tasks into a prompt-based sequence generation scheme. UDOP is pretrained on both large-scale unlabeled document corpora using innovative self-supervised objectives and diverse labeled data. UDOP also learns to generate document images from text and layout modalities via masked image reconstruction. To the best of our knowledge, this is the first time in the field of document AI that one model simultaneously achieves high-quality neural document editing and content customization. Our method sets the state-of-the-art on 8 Docu
    
[^121]: 深度神经Mel-Subband波束成形器用于车载语音分离

    Deep Neural Mel-Subband Beamformer for In-car Speech Separation. (arXiv:2211.12590v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2211.12590](http://arxiv.org/abs/2211.12590)

    本文提出了一种基于DL的Mel-Subband时空波束成形器，用于在车载环境中进行语音分离，通过基于Mel尺度的子带选择策略，实现对低频的细粒度处理和对高频的粗粒度处理，降低了计算成本和推理时间。

    This paper proposes a DL-based Mel-Subband spatio-temporal beamformer for speech separation in a car environment, which reduces computational costs and inference time by using a Mel-scale based subband selection strategy for fine-grained processing of lower frequencies and coarse-grained processing of higher frequencies.

    当前的深度学习（DL）基于波束成形技术已被证明在语音分离中有效，但它们通常被设计为独立处理窄带（NB）频率，这导致更高的计算成本和推理时间，使它们不适合实际应用。在本文中，我们提出了基于DL的Mel-Subband时空波束成形器，以在车载环境中进行语音分离，从而降低计算成本和推理时间。与传统的子带（SB）方法相反，我们的框架使用基于Mel尺度的子带选择策略，确保对大多数语音共振结构存在的低频进行细粒度处理，对高频进行粗粒度处理。以递归方式，从估计的子带语音和噪声协方差矩阵中确定每个扬声器位置/区域的鲁棒帧级波束成形权重。此外，所提出的框架还估计并抑制任何回声。

    While current deep learning (DL)-based beamforming techniques have been proved effective in speech separation, they are often designed to process narrow-band (NB) frequencies independently which results in higher computational costs and inference times, making them unsuitable for real-world use. In this paper, we propose DL-based mel-subband spatio-temporal beamformer to perform speech separation in a car environment with reduced computation cost and inference time. As opposed to conventional subband (SB) approaches, our framework uses a mel-scale based subband selection strategy which ensures a fine-grained processing for lower frequencies where most speech formant structure is present, and coarse-grained processing for higher frequencies. In a recursive way, robust frame-level beamforming weights are determined for each speaker location/zone in a car from the estimated subband speech and noise covariance matrices. Furthermore, proposed framework also estimates and suppresses any echo
    
[^122]: SinFusion：在单张图像或视频上训练扩散模型

    SinFusion: Training Diffusion Models on a Single Image or Video. (arXiv:2211.11743v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.11743](http://arxiv.org/abs/2211.11743)

    本文提出了一种在单张图像或视频上训练扩散模型的方法，称为SinFusion。该模型可以解决各种图像/视频特定的操作任务，包括从少量帧中学习单个输入视频的运动和动态，生成相同动态场景的多样化新视频样本，将短视频推广为长视频（向前和向后）并执行视频上采样。

    

    扩散模型在图像和视频生成方面取得了巨大的进展，超过了GAN在质量和多样性方面。然而，它们通常是在非常大的数据集上训练的，并且不自然地适应于操作给定的输入图像或视频。在本文中，我们展示了如何通过在单个输入图像或视频上训练扩散模型来解决这个问题。我们的图像/视频特定扩散模型（SinFusion）学习单个图像或视频的外观和动态，同时利用扩散模型的条件能力。它可以解决各种图像/视频特定的操作任务。特别地，我们的模型可以从少量帧中学习单个输入视频的运动和动态。然后，它可以生成相同动态场景的多样化新视频样本，将短视频推广为长视频（向前和向后）并执行视频上采样。这些任务中的大多数都无法通过当前的视频特定生成方法实现。

    Diffusion models exhibited tremendous progress in image and video generation, exceeding GANs in quality and diversity. However, they are usually trained on very large datasets and are not naturally adapted to manipulate a given input image or video. In this paper we show how this can be resolved by training a diffusion model on a single input image or video. Our image/video-specific diffusion model (SinFusion) learns the appearance and dynamics of the single image or video, while utilizing the conditioning capabilities of diffusion models. It can solve a wide array of image/video-specific manipulation tasks. In particular, our model can learn from few frames the motion and dynamics of a single input video. It can then generate diverse new video samples of the same dynamic scene, extrapolate short videos into long ones (both forward and backward in time) and perform video upsampling. Most of these tasks are not realizable by current video-specific generation methods.
    
[^123]: LA-VocE: 使用神经声码器的低信噪比音频视觉语音增强

    LA-VocE: Low-SNR Audio-visual Speech Enhancement using Neural Vocoders. (arXiv:2211.10999v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2211.10999](http://arxiv.org/abs/2211.10999)

    LA-VocE是一种新的音频视觉语音增强方法，使用神经声码器将从嘈杂的音频视觉语音预测的mel频谱图转换为波形音频，适用于多种语言和不同水平的背景噪声和语音干扰。

    LA-VocE is a new audio-visual speech enhancement method that uses a neural vocoder to convert mel-spectrograms predicted from noisy audio-visual speech via a transformer-based architecture into waveform audio, and is applicable to multiple languages and different levels of background noise and speech interference.

    音频视觉语音增强旨在通过利用音频本身以及目标说话者的唇部运动从嘈杂的环境中提取干净的语音。这种方法已经被证明比仅使用音频的语音增强方法更有效，特别是对于消除干扰语音。尽管语音合成方面取得了最近的进展，但大多数音频视觉方法仍然使用频谱映射/掩蔽来重现干净的音频，通常会在现有的语音增强架构中添加视觉骨干。在这项工作中，我们提出了LA-VocE，一种新的两阶段方法，通过基于Transformer的架构从嘈杂的音频视觉语音预测mel频谱图，然后使用神经声码器（HiFi-GAN）将它们转换为波形音频。我们在数千个说话者和11种以上不同的语言上训练和评估我们的框架，并研究我们的模型适应不同水平的背景噪声和语音干扰的能力。我们的实验表明

    Audio-visual speech enhancement aims to extract clean speech from a noisy environment by leveraging not only the audio itself but also the target speaker's lip movements. This approach has been shown to yield improvements over audio-only speech enhancement, particularly for the removal of interfering speech. Despite recent advances in speech synthesis, most audio-visual approaches continue to use spectral mapping/masking to reproduce the clean audio, often resulting in visual backbones added to existing speech enhancement architectures. In this work, we propose LA-VocE, a new two-stage approach that predicts mel-spectrograms from noisy audio-visual speech via a transformer-based architecture, and then converts them into waveform audio using a neural vocoder (HiFi-GAN). We train and evaluate our framework on thousands of speakers and 11+ different languages, and study our model's ability to adapt to different levels of background noise and speech interference. Our experiments show that 
    
[^124]: 可微非标定成像

    Differentiable Uncalibrated Imaging. (arXiv:2211.10525v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2211.10525](http://arxiv.org/abs/2211.10525)

    本文提出了一种可微的成像框架，以解决测量坐标的不确定性，通过隐式神经网络和可微分样条插值器实现。该方法应用于2D和3D计算机断层扫描，产生了改进的重建结果。

    This paper proposes a differentiable imaging framework to address uncertainty in measurement coordinates, using implicit neural networks and differentiable spline interpolators. The method is applied to 2D and 3D computed tomography and produces improved reconstructions.

    我们提出了一种可微成像框架，以解决测量坐标（如传感器位置和投影角度）的不确定性。我们将问题公式化为在未知节点处的测量插值，通过正向算子进行监督。为了解决这个问题，我们应用了隐式神经网络，也称为神经场，它们在输入坐标方面自然可微分。我们还开发了可微分样条插值器，其性能与神经网络一样好，需要更少的优化时间，并且具有良好的性质。可微性是关键，因为它允许我们共同拟合测量表示，优化不确定的测量坐标，并执行图像重建，从而确保一致的标定。我们将我们的方法应用于2D和3D计算机断层扫描，并展示了与不考虑缺乏标定的基线相比，它产生了改进的重建结果。所提出的框架的灵活性

    We propose a differentiable imaging framework to address uncertainty in measurement coordinates such as sensor locations and projection angles. We formulate the problem as measurement interpolation at unknown nodes supervised through the forward operator. To solve it we apply implicit neural networks, also known as neural fields, which are naturally differentiable with respect to the input coordinates. We also develop differentiable spline interpolators which perform as well as neural networks, require less time to optimize and have well-understood properties. Differentiability is key as it allows us to jointly fit a measurement representation, optimize over the uncertain measurement coordinates, and perform image reconstruction which in turn ensures consistent calibration. We apply our approach to 2D and 3D computed tomography and show that it produces improved reconstructions compared to baselines that do not account for the lack of calibration. The flexibility of the proposed framew
    
[^125]: HMOE: 基于超网络的专家混合模型用于领域泛化

    HMOE: Hypernetwork-based Mixture of Experts for Domain Generalization. (arXiv:2211.08253v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08253](http://arxiv.org/abs/2211.08253)

    本文提出了一种新的领域泛化方法HMOE，它不需要领域标签，更具可解释性，使用超网络生成专家权重，能够在低维向量空间中探索专家的相似性，实验结果表明HMOE可以划分混合数据并取得更好的效果。

    This paper proposes a novel domain generalization method called HMOE, which does not rely on domain labels and is more interpretable. HMOE uses hypernetworks to generate experts' weights, which allows experts to share useful meta-knowledge and enables exploring experts' similarities in a low-dimensional vector space. Experimental results show that HMOE can divide mixed data and achieve better performance.

    由于领域转移，机器学习系统通常无法很好地推广到与训练数据不同的领域，这就是领域泛化（DG）的目的。尽管已经开发了各种各样的DG方法，但大多数缺乏可解释性，并且需要在许多实际场景中不可用的领域标签。本文提出了一种新的DG方法，称为HMOE：基于超网络的专家混合模型（MoE），它不依赖于领域标签，并且更具可解释性。MoE在识别数据中的异质模式方面证明了其有效性。对于DG问题，异质性正是由于领域转移而产生的。HMOE使用超网络将向量作为输入来生成专家权重，这使得专家可以共享有用的元知识，并能够在低维向量空间中探索专家的相似性。我们在公平和统一的基准测试-DomainBed下将HMOE与其他DG算法进行比较。我们的广泛实验表明，HMOE可以划分混合数据并取得更好的效果。

    Due to domain shift, machine learning systems typically fail to generalize well to domains different from those of training data, which is what domain generalization (DG) aims to address. Although various DG methods have been developed, most of them lack interpretability and require domain labels that are not available in many real-world scenarios. This paper presents a novel DG method, called HMOE: Hypernetwork-based Mixture of Experts (MoE), which does not rely on domain labels and is more interpretable. MoE proves effective in identifying heterogeneous patterns in data. For the DG problem, heterogeneity arises exactly from domain shift. HMOE uses hypernetworks taking vectors as input to generate experts' weights, which allows experts to share useful meta-knowledge and enables exploring experts' similarities in a low-dimensional vector space. We compare HMOE with other DG algorithms under a fair and unified benchmark-DomainBed. Our extensive experiments show that HMOE can divide mixe
    
[^126]: 学习神经符号程序以进行语言引导的机器人操作

    Learning Neuro-symbolic Programs for Language Guided Robot Manipulation. (arXiv:2211.06652v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2211.06652](http://arxiv.org/abs/2211.06652)

    该论文提出了一种学习神经符号程序以进行语言引导的机器人操作的方法，可以处理语言和感知变化，端到端可训练，不需要中间监督。该方法使用符号推理构造，在潜在的神经物体为中心的表示上操作，允许对输入场景进行更深入的推理。

    This paper proposes a method for learning neuro-symbolic programs for language guided robot manipulation, which can handle linguistic and perceptual variations, is end-to-end trainable, and requires no intermediate supervision. The method uses symbolic reasoning constructs that operate on a latent neural object-centric representation, allowing for deeper reasoning over the input scene.

    给定自然语言指令和输入场景，我们的目标是训练一个模型，输出一个可以由机器人执行的操作程序。先前的方法存在以下限制之一：（i）依赖手工编码的概念符号，限制了超出训练期间所见的一般化能力[1]（ii）从指令中推断出动作序列，但需要密集的子目标监督[2]或（iii）缺乏解释复杂指令所需的语义，这种语义需要更深入的以物体为中心的推理[3]。相比之下，我们的方法可以处理语言和感知变化，端到端可训练，不需要中间监督。所提出的模型使用符号推理构造，这些构造在潜在的神经物体为中心的表示上操作，允许对输入场景进行更深入的推理。我们方法的核心是一个模块化结构，包括分层指令解析器和动作模拟器，以学习解耦的行动序列。

    Given a natural language instruction and an input scene, our goal is to train a model to output a manipulation program that can be executed by the robot. Prior approaches for this task possess one of the following limitations: (i) rely on hand-coded symbols for concepts limiting generalization beyond those seen during training [1] (ii) infer action sequences from instructions but require dense sub-goal supervision [2] or (iii) lack semantics required for deeper object-centric reasoning inherent in interpreting complex instructions [3]. In contrast, our approach can handle linguistic as well as perceptual variations, end-to-end trainable and requires no intermediate supervision. The proposed model uses symbolic reasoning constructs that operate on a latent neural object-centric representation, allowing for deeper reasoning over the input scene. Central to our approach is a modular structure consisting of a hierarchical instruction parser and an action simulator to learn disentangled act
    
[^127]: 使用2D投影从3D MRI体积中高效预测脑龄

    Efficient brain age prediction from 3D MRI volumes using 2D projections. (arXiv:2211.05762v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2211.05762](http://arxiv.org/abs/2211.05762)

    本文提出了一种使用2D投影从3D MRI体积中高效预测脑龄的方法，相比于使用3D CNN，该方法在计算速度上有两个数量级的提升，对于没有3D CNN昂贵GPU硬件的研究人员非常有用。

    This paper proposes an efficient method for predicting brain age from 3D MRI volumes using 2D projections, which is two orders of magnitude faster than using 3D CNNs and is important for researchers without access to expensive GPU hardware.

    在高分辨率医学体积上使用3D CNN非常计算密集，特别是对于像英国生物库这样的大型数据集，该库旨在扫描10万个受试者。在这里，我们证明了使用2D CNN在3D体积的几个2D投影（代表轴向，矢状面和冠状面切片的平均值和标准差）上进行预测脑龄时，可以获得合理的测试准确性。使用我们的方法，使用单个GPU进行的一次训练时，20324个受试者需要20-50秒，比小型3D CNN快两个数量级。这些结果对于没有3D CNN昂贵GPU硬件的研究人员非常重要。

    Using 3D CNNs on high resolution medical volumes is very computationally demanding, especially for large datasets like the UK Biobank which aims to scan 100,000 subjects. Here we demonstrate that using 2D CNNs on a few 2D projections (representing mean and standard deviation across axial, sagittal and coronal slices) of the 3D volumes leads to reasonable test accuracy when predicting the age from brain volumes. Using our approach, one training epoch with 20,324 subjects takes 20 - 50 seconds using a single GPU, which two orders of magnitude faster compared to a small 3D CNN. These results are important for researchers who do not have access to expensive GPU hardware for 3D CNNs.
    
[^128]: 通过再生核Banach空间的神经网络对偶性

    Duality for Neural Networks through Reproducing Kernel Banach Spaces. (arXiv:2211.05020v3 [math.FA] UPDATED)

    [http://arxiv.org/abs/2211.05020](http://arxiv.org/abs/2211.05020)

    本文提出了一种新的再生核Banach空间（RKBS）方法，用于解决神经网络中的对偶性问题，构建了神经网络的鞍点问题，可用于原始-对偶优化的整个领域。

    This paper proposes a new method using Reproducing Kernel Banach spaces (RKBS) to solve the duality problem in neural networks, constructing the saddle point problem for neural networks, which can be used in the whole field of primal-dual optimization.

    再生核希尔伯特空间（RKHS）已经成为机器学习各个领域中非常成功的工具。最近，Barron空间已被用于证明神经网络的泛化误差界限。不幸的是，由于权重的强非线性耦合，Barron空间无法用RKHS的术语理解。这可以通过使用更一般的再生核Banach空间（RKBS）来解决。我们展示了这些Barron空间属于一类积分RKBS。这个类也可以理解为RKHS空间的无限并集。此外，我们展示了这种RKBS的对偶空间，再次是一个RKBS，其中数据和参数的角色互换，形成一个包括再生核的伴随RKBS对。这使我们能够构建神经网络的鞍点问题，可用于原始-对偶优化的整个领域。

    Reproducing Kernel Hilbert spaces (RKHS) have been a very successful tool in various areas of machine learning. Recently, Barron spaces have been used to prove bounds on the generalisation error for neural networks. Unfortunately, Barron spaces cannot be understood in terms of RKHS due to the strong nonlinear coupling of the weights. This can be solved by using the more general Reproducing Kernel Banach spaces (RKBS). We show that these Barron spaces belong to a class of integral RKBS. This class can also be understood as an infinite union of RKHS spaces. Furthermore, we show that the dual space of such RKBSs, is again an RKBS where the roles of the data and parameters are interchanged, forming an adjoint pair of RKBSs including a reproducing kernel. This allows us to construct the saddle point problem for neural networks, which can be used in the whole field of primal-dual optimisation.
    
[^129]: 面向隐私的联邦学习压缩：通过数值机制设计

    Privacy-Aware Compression for Federated Learning Through Numerical Mechanism Design. (arXiv:2211.03942v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.03942](http://arxiv.org/abs/2211.03942)

    本文提出了一种新的插值MVU机制，通过数值机制设计实现面向隐私的联邦学习压缩，具有更好的隐私效用权衡和更高的可扩展性，并在各种数据集上提供了通信高效的私有FL的SOTA结果。

    This paper proposes a new Interpolated MVU mechanism for privacy-aware compression in federated learning, which achieves a better privacy-utility trade-off and scalability through numerical mechanism design, and provides SOTA results on communication-efficient private FL on a variety of datasets.

    在私有联邦学习（FL）中，服务器聚合来自大量客户端的差分隐私更新，以训练机器学习模型。这种情况下的主要挑战是在隐私和学习模型的分类准确性以及客户端和服务器之间通信的位数之间平衡隐私。先前的工作通过设计一种隐私感知压缩机制（称为最小方差无偏（MVU）机制）来实现良好的权衡，该机制通过数值求解优化问题来确定机制的参数。本文在此基础上引入了一种新的插值过程，用于数值设计过程，从而实现更高效的隐私分析。结果是新的插值MVU机制，它更具可扩展性，具有更好的隐私效用权衡，并在各种数据集上提供了通信高效的私有FL的SOTA结果。

    In private federated learning (FL), a server aggregates differentially private updates from a large number of clients in order to train a machine learning model. The main challenge in this setting is balancing privacy with both classification accuracy of the learnt model as well as the number of bits communicated between the clients and server. Prior work has achieved a good trade-off by designing a privacy-aware compression mechanism, called the minimum variance unbiased (MVU) mechanism, that numerically solves an optimization problem to determine the parameters of the mechanism. This paper builds upon it by introducing a new interpolation procedure in the numerical design process that allows for a far more efficient privacy analysis. The result is the new Interpolated MVU mechanism that is more scalable, has a better privacy-utility trade-off, and provides SOTA results on communication-efficient private FL on a variety of datasets.
    
[^130]: 基于参数化超复数神经网络的心房颤动检测的高效ECG方法

    Efficient ECG-based Atrial Fibrillation Detection via Parameterised Hypercomplex Neural Networks. (arXiv:2211.02678v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2211.02678](http://arxiv.org/abs/2211.02678)

    本文提出了一种基于参数化超复数神经网络的轻量级卷积神经网络方法，用于心房颤动检测。该方法在可穿戴设备上训练小规模CNN，克服了有限的计算资源。在两个公开可用的ECG数据集上，该方法表现出与实值CNN相当的性能，但使用了显着较少的模型参数。

    This paper proposes a lightweight convolutional neural network method based on parameterized hypercomplex neural networks for atrial fibrillation detection. The method trains small-scale CNNs on wearable devices, overcoming limited computing resources. The approach shows comparable performance to real-valued CNNs on two publicly available ECG datasets using significantly fewer model parameters.

    心房颤动（AF）是最常见的心律失常，与中风等严重疾病的高风险相关。嵌入自动和及时的AF评估的可穿戴设备使用心电图（ECG）已被证明在预防危及生命的情况方面具有前景。虽然深度神经网络在模型性能方面表现出优越性，但它们在可穿戴设备上的使用受到模型性能和复杂性之间的权衡的限制。在这项工作中，我们提出使用带有参数化超复数（PH）层的轻量级卷积神经网络（CNN）来基于ECG进行AF检测。所提出的方法训练小规模CNN，从而克服了可穿戴设备上的有限计算资源。我们使用显着较少的模型参数在两个公开可用的ECG数据集上展示了与相应的实值CNN相当的性能。PH模型比其他超复数神经网络更灵活，可以在...

    Atrial fibrillation (AF) is the most common cardiac arrhythmia and associated with a high risk for serious conditions like stroke. The use of wearable devices embedded with automatic and timely AF assessment from electrocardiograms (ECGs) has shown to be promising in preventing life-threatening situations. Although deep neural networks have demonstrated superiority in model performance, their use on wearable devices is limited by the trade-off between model performance and complexity. In this work, we propose to use lightweight convolutional neural networks (CNNs) with parameterised hypercomplex (PH) layers for AF detection based on ECGs. The proposed approach trains small-scale CNNs, thus overcoming the limited computing resources on wearable devices. We show comparable performance to corresponding real-valued CNNs on two publicly available ECG datasets using significantly fewer model parameters. PH models are more flexible than other hypercomplex neural networks and can operate on an
    
[^131]: 基于SPD流形的图神经网络用于运动想象分类：来自时频分析的视角

    Graph Neural Networks on SPD Manifolds for Motor Imagery Classification: A Perspective from the Time-Frequency Analysis. (arXiv:2211.02641v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2211.02641](http://arxiv.org/abs/2211.02641)

    本文介绍了一种基于SPD流形的图神经网络用于运动想象分类，利用EEG的二阶统计量，相比传统方法具有更好的性能。

    This paper introduces a graph neural network based on SPD manifolds for motor imagery classification, which utilizes second-order statistics of EEG signals and outperforms traditional methods.

    运动想象（MI）的分类是脑电图（EEG）基础脑机接口（BCI）领域中备受追捧的研究课题，具有巨大的商业价值。过去二十年，MI-EEG分类器的趋势发生了根本性的转变，其性能逐渐提高。 Tensor-CSPNet的出现是BCI研究中第一个几何深度学习（GDL）框架的必要性，其归因于信号的非欧几里德性质的特征化。从根本上讲，Tensor-CSPNet是一种基于深度学习的分类器，利用EEG的二阶统计量。与利用EEG信号的一阶统计量的传统方法相比，利用这些二阶统计量代表了经典的处理方法。这些统计量提供了足够的区分信息，使它们适用于MI-EEG分类。在本研究中，我们介绍了另一种GDL分类器，

    The classification of motor imagery (MI) is a highly sought-after research topic in the field of Electroencephalography (EEG)-based brain-computer interfaces (BCIs), with immense commercial value. Over the past two decades, there has been a fundamental shift in the trend of MI-EEG classifiers, resulting in a gradual increase in their performance. The emergence of Tensor-CSPNet, the first geometric deep learning (GDL) framework in BCI research, is attributed to the imperative of characterizing the non-Euclidean nature of signals. Fundamentally, Tensor-CSPNet is a deep learning-based classifier that capitalizes on the second-order statistics of EEGs. In contrast to the conventional approach of utilizing first-order statistics for EEG signals, the utilization of these second-order statistics represents the classical treatment. These statistics provide adequate discriminative information, rendering them suitable for MI-EEG classification. In this study, we introduce another GDL classifier,
    
[^132]: 去除噪音：心理声学和基于包络的特征在机械故障检测中的实证比较

    Cutting Through the Noise: An Empirical Comparison of Psychoacoustic and Envelope-based Features for Machinery Fault Detection. (arXiv:2211.01704v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2211.01704](http://arxiv.org/abs/2211.01704)

    本文提出了一个自动化和噪声鲁棒的听觉检查系统，用于检测机械部件的健康状况。我们提供了一个基准来比较不同类型的包络特征与心理声学特征。我们是第一个应用时变心理声学特征进行故障检测的人。

    This paper presents an automated and noise-robust auditory inspection system for detecting the health condition of mechanical parts. A benchmark is provided to compare different types of envelope features with psychoacoustic features. The authors are the first to apply time-varying psychoacoustic features for fault detection.

    基于声学的故障检测具有监测机械部件健康状况的高潜力。然而，工业环境的背景噪音可能会对故障检测的性能产生负面影响。目前对于提高故障检测对工业环境噪声的鲁棒性的关注有限。因此，我们提出了Lenze生产背景噪声（LPBN）真实世界数据集和用于齿轮电机末端检查的自动化和噪声鲁棒的听觉检查（ARAI）系统。采用声学阵列从具有轻微故障、重大故障或健康的电机中获取数据。提供了一个基准来比较基于专家对齿轮箱的知识的不同类型的包络特征与心理声学特征。据我们所知，我们是第一个应用时变心理声学特征进行故障检测的人。我们训练了一种最先进的单类分类器，使用来自健康电机的样本进行训练。

    Acoustic-based fault detection has a high potential to monitor the health condition of mechanical parts. However, the background noise of an industrial environment may negatively influence the performance of fault detection. Limited attention has been paid to improving the robustness of fault detection against industrial environmental noise. Therefore, we present the Lenze production background-noise (LPBN) real-world dataset and an automated and noise-robust auditory inspection (ARAI) system for the end-of-line inspection of geared motors. An acoustic array is used to acquire data from motors with a minor fault, major fault, or which are healthy. A benchmark is provided to compare the psychoacoustic features with different types of envelope features based on expert knowledge of the gearbox. To the best of our knowledge, we are the first to apply time-varying psychoacoustic features for fault detection. We train a state-of-the-art one-class-classifier, on samples from healthy motors an
    
[^133]: 使用情感嵌入在情感、语言和注释格式之间传递知识

    Using Emotion Embeddings to Transfer Knowledge Between Emotions, Languages, and Annotation Formats. (arXiv:2211.00171v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.00171](http://arxiv.org/abs/2211.00171)

    本文研究了如何通过利用多语言模型和Demux来构建一个可以在不同情感、语言和注释格式之间转换的单一模型，以实现知识共享和降低训练成本。

    This paper studies how to build a single model that can transition between different emotions, languages, and annotation formats by leveraging multilingual models and Demux, to achieve knowledge sharing and reduce training costs.

    随着越来越多的学科将情感融入其理论和应用中，从文本中推断情感的需求不断多样化。这些需求包括推断不同类型的情感、处理多种语言和不同的注释格式。不同配置之间的共享模型将使知识共享和训练成本降低，并简化在新环境中部署情感识别模型的过程。在这项工作中，我们研究了如何通过利用多语言模型和Demux来构建一个可以在这些不同配置之间转换的单一模型，Demux是一个基于transformer的模型，其输入包括感兴趣的情感，使我们能够动态地改变模型预测的情感。Demux还产生情感嵌入，对它们执行操作可以通过汇集每个簇的嵌入来过渡到情感簇。我们展示了Demux可以同时传输k

    The need for emotional inference from text continues to diversify as more and more disciplines integrate emotions into their theories and applications. These needs include inferring different emotion types, handling multiple languages, and different annotation formats. A shared model between different configurations would enable the sharing of knowledge and a decrease in training costs, and would simplify the process of deploying emotion recognition models in novel environments. In this work, we study how we can build a single model that can transition between these different configurations by leveraging multilingual models and Demux, a transformer-based model whose input includes the emotions of interest, enabling us to dynamically change the emotions predicted by the model. Demux also produces emotion embeddings, and performing operations on them allows us to transition to clusters of emotions by pooling the embeddings of each cluster. We show that Demux can simultaneously transfer k
    
[^134]: 操作气象学的机器学习教程，第二部分：神经网络和深度学习

    A Machine Learning Tutorial for Operational Meteorology, Part II: Neural Networks and Deep Learning. (arXiv:2211.00147v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.00147](http://arxiv.org/abs/2211.00147)

    本文讨论了机器学习在气象学中的应用，特别是神经网络和深度学习。涵盖了感知器、人工神经网络、卷积神经网络和U型网络等方法。

    This paper discusses the application of machine learning in meteorology, specifically neural networks and deep learning. It covers methods such as perceptrons, artificial neural networks, convolutional neural networks, and U-networks.

    在过去的十年中，机器学习在气象学中的应用迅速增长。特别是神经网络和深度学习的使用率前所未有。为了填补缺乏以气象学视角涵盖神经网络的资源，本文以平易近人的语言格式讨论了机器学习方法，针对操作气象学界。这是一对旨在为气象学家提供机器学习资源的论文中的第二篇。第一篇论文侧重于传统的机器学习方法（例如随机森林），而本文则涵盖了广泛的神经网络和深度学习方法。具体而言，本文涵盖了感知器、人工神经网络、卷积神经网络和U型网络。与第一篇论文一样，本文讨论了与神经网络及其训练相关的术语。然后，本文提供了每种方法背后的一些直觉，并以展示每种方法的实例来结束。

    Over the past decade the use of machine learning in meteorology has grown rapidly. Specifically neural networks and deep learning have been used at an unprecedented rate. In order to fill the dearth of resources covering neural networks with a meteorological lens, this paper discusses machine learning methods in a plain language format that is targeted for the operational meteorological community. This is the second paper in a pair that aim to serve as a machine learning resource for meteorologists. While the first paper focused on traditional machine learning methods (e.g., random forest), here a broad spectrum of neural networks and deep learning methods are discussed. Specifically this paper covers perceptrons, artificial neural networks, convolutional neural networks and U-networks. Like the part 1 paper, this manuscript discusses the terms associated with neural networks and their training. Then the manuscript provides some intuition behind every method and concludes by showing ea
    
[^135]: 表格数据中的缺失值插补扩散模型

    Diffusion models for missing value imputation in tabular data. (arXiv:2210.17128v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17128](http://arxiv.org/abs/2210.17128)

    本文提出了一种名为“表格数据条件分数扩散模型”（TabCSDI）的扩散模型方法，用于处理表格数据中的缺失值插补，该方法同时处理分类变量和数值变量，实验结果表明其在各种数据集上都取得了优异的性能。

    This paper proposes a diffusion model approach called "Conditional Score-based Diffusion Models for Tabular data" (TabCSDI) for missing value imputation in tabular data, which effectively handles categorical variables and numerical variables simultaneously, and achieves excellent performance on various datasets.

    机器学习中的缺失值插补是使用可用信息准确估计数据集中缺失值的任务。在这个任务中，已经提出了几种深度生成建模方法，并证明了它们的有用性，例如生成对抗插补网络。最近，扩散模型因其在图像、文本、音频等生成建模任务中的有效性而受到关注。据我们所知，对于表格数据中缺失值插补的扩散模型的有效性研究还不够。基于最近对于时间序列数据插补的扩散模型的发展，我们提出了一种名为“表格数据条件分数扩散模型”（TabCSDI）的扩散模型方法。为了有效地同时处理分类变量和数值变量，我们研究了三种技术：独热编码、模拟位编码和特征标记化。实验结果表明，我们的方法在各种数据集上都取得了优异的性能。

    Missing value imputation in machine learning is the task of estimating the missing values in the dataset accurately using available information. In this task, several deep generative modeling methods have been proposed and demonstrated their usefulness, e.g., generative adversarial imputation networks. Recently, diffusion models have gained popularity because of their effectiveness in the generative modeling task in images, texts, audio, etc. To our knowledge, less attention has been paid to the investigation of the effectiveness of diffusion models for missing value imputation in tabular data. Based on recent development of diffusion models for time-series data imputation, we propose a diffusion model approach called "Conditional Score-based Diffusion Models for Tabular data" (TabCSDI). To effectively handle categorical variables and numerical variables simultaneously, we investigate three techniques: one-hot encoding, analog bits encoding, and feature tokenization. Experimental resul
    
[^136]: 利用元数据和对比学习学习音频特征

    Learning Audio Features with Metadata and Contrastive Learning. (arXiv:2210.16192v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2210.16192](http://arxiv.org/abs/2210.16192)

    本研究使用监督对比学习结合可用元数据解决多个前置任务，学习数据的良好表示。在呼吸音分类数据集上，仅使用元数据学习表示可以获得与仅使用类标签的交叉熵相似的性能。在使用多个监督对比学习将类标签与元数据相结合时，获得了最先进的得分。

    This study uses supervised contrastive learning combined with available metadata to solve multiple pretext tasks that learn a good representation of data. Learning representations using only metadata obtains similar performance as using cross entropy with class labels only. State-of-the-art score is obtained when combining class labels with metadata using multiple supervised contrastive learning.

    基于注释的监督学习方法一直是分类问题的最先进技术，但是在低数据情况下，它们的泛化能力可能受到限制。本研究使用监督对比学习结合可用元数据解决多个前置任务，学习数据的良好表示。我们将我们的方法应用于ICBHI，这是一个适合这种情况的呼吸音分类数据集。我们表明，仅使用元数据学习表示，而不使用类标签，可以获得与仅使用这些标签的交叉熵相似的性能。此外，我们使用多个监督对比学习将类标签与元数据相结合时，获得了最先进的得分。这项工作表明，在监督对比设置中使用多个元数据源的潜力，特别是在类不平衡和少量数据的情况下。我们的代码已发布。

    Methods based on supervised learning using annotations in an end-to-end fashion have been the state-of-the-art for classification problems. However, they may be limited in their generalization capability, especially in the low data regime. In this study, we address this issue using supervised contrastive learning combined with available metadata to solve multiple pretext tasks that learn a good representation of data. We apply our approach on ICBHI, a respiratory sound classification dataset suited for this setting. We show that learning representations using only metadata, without class labels, obtains similar performance as using cross entropy with those labels only. In addition, we obtain state-of-the-art score when combining class labels with metadata using multiple supervised contrastive learning. This work suggests the potential of using multiple metadata sources in supervised contrastive settings, in particular in settings with class imbalance and few data. Our code is released 
    
[^137]: 在多标签情感识别中利用标签相关性的研究：以情感为例

    Leveraging Label Correlations in a Multi-label Setting: A Case Study in Emotion. (arXiv:2210.15842v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.15842](http://arxiv.org/abs/2210.15842)

    本文研究了利用标签相关性来改善情感检测的方法，开发了两种建模方法来捕捉情感词本身的词汇关联性，并将情感表示的成对约束作为正则化项与模型的分类损失一起集成，展示了在SemEval 2018任务1 E-c中使用单语BERT模型展示了西班牙语、英语和阿拉伯语的最新性能。

    This paper investigates ways to exploit label correlations in multi-label emotion recognition models to improve emotion detection, develops two modeling approaches to capture word associations of the emotion words themselves, and integrates pairwise constraints of emotion representations as regularization terms alongside the classification loss of the models, demonstrating state-of-the-art performance across Spanish, English, and Arabic in SemEval 2018 Task 1 E-c using monolingual BERT-based models.

    在文本中检测表达的情感已经成为许多领域的关键。本文研究了利用多标签情感识别模型中的标签相关性来改善情感检测的方法。首先，我们开发了两种建模方法来捕捉情感词本身的词汇关联性，一种是将情感包含在输入中，另一种是利用遮蔽语言建模（MLM）。其次，我们将情感表示的成对约束作为正则化项与模型的分类损失一起集成。我们将这些项分为两类，局部和全局。前者根据金标签动态变化，而后者在训练期间保持不变。我们在SemEval 2018任务1 E-c中使用单语BERT模型展示了西班牙语、英语和阿拉伯语的最新性能。除了更好的性能外，我们还展示了改进的鲁棒性。

    Detecting emotions expressed in text has become critical to a range of fields. In this work, we investigate ways to exploit label correlations in multi-label emotion recognition models to improve emotion detection. First, we develop two modeling approaches to the problem in order to capture word associations of the emotion words themselves, by either including the emotions in the input, or by leveraging Masked Language Modeling (MLM). Second, we integrate pairwise constraints of emotion representations as regularization terms alongside the classification loss of the models. We split these terms into two categories, local and global. The former dynamically change based on the gold labels, while the latter remain static during training. We demonstrate state-of-the-art performance across Spanish, English, and Arabic in SemEval 2018 Task 1 E-c using monolingual BERT-based models. On top of better performance, we also demonstrate improved robustness. Code is available at https://github.com/
    
[^138]: 破碎的神经缩放定律

    Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14891](http://arxiv.org/abs/2210.14891)

    本文提出了一个平滑破碎的幂律函数形式，可以准确地模拟和外推深度神经网络的缩放行为，适用于各种架构和大量不同任务，包括视觉、语言、音频、视频、生成建模、对比学习、机器人、不确定性估计/校准、对抗鲁棒性、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。

    This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.

    我们提出了一个平滑破碎的幂律函数形式（我们称之为破碎的神经缩放定律（BNSL）），它准确地模拟和外推了深度神经网络的缩放行为（即感兴趣的评估指标随用于训练的计算量、模型参数数量、训练数据集大小或上游性能变化而变化）对于各种架构和大量不同任务中的每个任务，包括大规模视觉、语言、音频、视频、扩散、生成建模、多模态学习、对比学习、AI对齐、机器人、超出分布（OOD）泛化、持续学习、不确定性估计/校准、超出分布检测、对抗鲁棒性、蒸馏、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。

    We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
    
[^139]: D-Shape: 通过目标条件化实现演示形状的强化学习

    D-Shape: Demonstration-Shaped Reinforcement Learning via Goal Conditioning. (arXiv:2210.14428v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14428](http://arxiv.org/abs/2210.14428)

    D-Shape是一种新的结合IL和RL的方法，它使用奖励塑形和目标条件化RL的思想来解决次优演示与回报最大化目标之间的冲突，能够在稀疏奖励网格世界领域中提高样本效率并一致地收敛到最优策略。

    D-Shape is a new method that combines imitation learning (IL) and reinforcement learning (RL) using reward shaping and goal-conditioned RL to resolve the conflict between suboptimal demonstrations and return-maximization objective of RL. It improves sample efficiency and consistently converges to the optimal policy in sparse-reward gridworld domains.

    将模仿学习（IL）和强化学习（RL）相结合是解决自主行为获取中样本效率低下的一种有前途的方法，但这样做的方法通常假定所需的行为演示由专家提供，该专家相对于任务奖励表现最佳。然而，如果提供的演示是次优的，则面临一个基本挑战，即IL的演示匹配目标与RL的回报最大化目标冲突。本文介绍了D-Shape，一种新的结合IL和RL的方法，它使用奖励塑形和目标条件化RL的思想来解决上述冲突。D-Shape允许从次优演示中学习，同时保留了找到相对于任务奖励的最优策略的能力。我们在稀疏奖励网格世界领域实验验证了D-Shape，结果表明它在样本效率方面优于RL，并且能够一致地收敛到最优策略。

    While combining imitation learning (IL) and reinforcement learning (RL) is a promising way to address poor sample efficiency in autonomous behavior acquisition, methods that do so typically assume that the requisite behavior demonstrations are provided by an expert that behaves optimally with respect to a task reward. If, however, suboptimal demonstrations are provided, a fundamental challenge appears in that the demonstration-matching objective of IL conflicts with the return-maximization objective of RL. This paper introduces D-Shape, a new method for combining IL and RL that uses ideas from reward shaping and goal-conditioned RL to resolve the above conflict. D-Shape allows learning from suboptimal demonstrations while retaining the ability to find the optimal policy with respect to the task reward. We experimentally validate D-Shape in sparse-reward gridworld domains, showing that it both improves over RL in terms of sample efficiency and converges consistently to the optimal polic
    
[^140]: Langevin-Based Non-Convex Sampling的动力学系统视角

    A Dynamical System View of Langevin-Based Non-Convex Sampling. (arXiv:2210.13867v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13867](http://arxiv.org/abs/2210.13867)

    本文提出了一种新的框架，通过利用动力系统理论中的几个工具来解决非凸采样中的重要挑战。对于一大类最先进的采样方案，它们在Wasserstein距离下的最后迭代收敛可以归结为对它们的连续时间对应物的研究，这是更好理解的。

    This paper proposes a new framework that uses tools from the theory of dynamical systems to address important challenges in non-convex sampling. For a large class of state-of-the-art sampling schemes, their last-iterate convergence in Wasserstein distances can be reduced to the study of their continuous-time counterparts, which is much better understood.

    非凸采样是机器学习中的一个关键挑战，对于深度学习中的非凸优化以及近似概率推断都至关重要。尽管其重要性，理论上仍存在许多重要挑战：现有的保证通常仅适用于平均迭代而不是更理想的最后迭代，缺乏捕捉变量尺度（如Wasserstein距离）的收敛度量，主要适用于随机梯度Langevin动力学等基本方案。在本文中，我们开发了一个新的框架，通过利用动力系统理论中的几个工具来解决上述问题。我们的关键结果是，对于一大类最先进的采样方案，它们在Wasserstein距离下的最后迭代收敛可以归结为对它们的连续时间对应物的研究，这是更好理解的。结合MCMC采样的标准假设，我们的理论立即产生了

    Non-convex sampling is a key challenge in machine learning, central to non-convex optimization in deep learning as well as to approximate probabilistic inference. Despite its significance, theoretically there remain many important challenges: Existing guarantees (1) typically only hold for the averaged iterates rather than the more desirable last iterates, (2) lack convergence metrics that capture the scales of the variables such as Wasserstein distances, and (3) mainly apply to elementary schemes such as stochastic gradient Langevin dynamics. In this paper, we develop a new framework that lifts the above issues by harnessing several tools from the theory of dynamical systems. Our key result is that, for a large class of state-of-the-art sampling schemes, their last-iterate convergence in Wasserstein distances can be reduced to the study of their continuous-time counterparts, which is much better understood. Coupled with standard assumptions of MCMC sampling, our theory immediately yie
    
[^141]: 利用堆叠自编码器消除极光千米辐射中的射频干扰

    Removing Radio Frequency Interference from Auroral Kilometric Radiation with Stacked Autoencoders. (arXiv:2210.12931v3 [astro-ph.IM] UPDATED)

    [http://arxiv.org/abs/2210.12931](http://arxiv.org/abs/2210.12931)

    本研究利用深度学习算法中的自编码器，提出了一种名为DAARE的去噪自编码器，用于消除极光千米辐射中的射频干扰。DAARE在合成AKR观测中实现了42.2的峰值信噪比(PSNR)和0.981的结构相似度(SSIM)，相比于最先进的滤波和去噪网络，PSNR提高了3.9，SSIM提高了0.064。定性比较表明，DAARE能够有效地消除RFI。

    This study proposes a denoising autoencoder named DAARE to remove radio frequency interference (RFI) from auroral kilometric radiation (AKR) signals collected at the South Pole Station. DAARE achieves 42.2 peak signal-to-noise ratio (PSNR) and 0.981 structural similarity (SSIM) on synthesized AKR observations, improving PSNR by 3.9 and SSIM by 0.064 compared to state-of-the-art filtering and denoising networks.

    天文射电频率数据可以帮助科学家分析天体物理现象。然而，这些数据可能会受到射频干扰(RFI)的影响，从而限制了对基础自然过程的观测。本研究将深度学习算法的最新进展扩展到天文数据中。我们从南极站收集的极光千米辐射(AKR)信号中，利用合成光谱图训练了一种名为DAARE的去噪自编码器，以消除RFI。DAARE在合成AKR观测中实现了42.2的峰值信噪比(PSNR)和0.981的结构相似度(SSIM)，相比于最先进的滤波和去噪网络，PSNR提高了3.9，SSIM提高了0.064。定性比较表明，DAARE能够有效地消除RFI。

    Radio frequency data in astronomy enable scientists to analyze astrophysical phenomena. However, these data can be corrupted by radio frequency interference (RFI) that limits the observation of underlying natural processes. In this study, we extend recent developments in deep learning algorithms to astronomy data. We remove RFI from time-frequency spectrograms containing auroral kilometric radiation (AKR), a coherent radio emission originating from the Earth's auroral zones that is used to study astrophysical plasmas. We propose a Denoising Autoencoder for Auroral Radio Emissions (DAARE) trained with synthetic spectrograms to denoise AKR signals collected at the South Pole Station. DAARE achieves 42.2 peak signal-to-noise ratio (PSNR) and 0.981 structural similarity (SSIM) on synthesized AKR observations, improving PSNR by 3.9 and SSIM by 0.064 compared to state-of-the-art filtering and denoising networks. Qualitative comparisons demonstrate DAARE's capability to effectively remove RFI
    
[^142]: 回放：迭代注意力用于音频识别

    Play It Back: Iterative Attention for Audio Recognition. (arXiv:2210.11328v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2210.11328](http://arxiv.org/abs/2210.11328)

    该论文提出了一种基于注意力的架构，通过选择性重复跨越音频序列的最具区分性的声音来进行关注，最终实现了在三个音频分类基准测试中始终实现最先进的性能。

    The paper proposes an end-to-end attention-based architecture that attends over the most discriminative sounds across the audio sequence through selective repetition, achieving consistently state-of-the-art performance across three audio-classification benchmarks.

    听觉认知的一个关键功能是随着时间的推移将特征声音与其相应的语义关联起来。人类试图区分细粒度音频类别时，通常会重播相同的区分性声音以增加其预测置信度。我们提出了一种端到端的基于注意力的架构，通过选择性重复跨越音频序列的最具区分性的声音来进行关注。我们的模型最初使用完整的音频序列，并通过插槽注意力迭代地细化重播的时间段。在每次播放时，所选段使用较小的跳跃长度重播，这代表了这些段内更高分辨率的特征。我们展示了我们的方法可以在三个音频分类基准测试中始终实现最先进的性能：AudioSet、VGG-Sound和EPIC-KITCHENS-100。

    A key function of auditory cognition is the association of characteristic sounds with their corresponding semantics over time. Humans attempting to discriminate between fine-grained audio categories, often replay the same discriminative sounds to increase their prediction confidence. We propose an end-to-end attention-based architecture that through selective repetition attends over the most discriminative sounds across the audio sequence. Our model initially uses the full audio sequence and iteratively refines the temporal segments replayed based on slot attention. At each playback, the selected segments are replayed using a smaller hop length which represents higher resolution features within these segments. We show that our method can consistently achieve state-of-the-art performance across three audio-classification benchmarks: AudioSet, VGG-Sound, and EPIC-KITCHENS-100.
    
[^143]: 用贝叶斯优化发现多样的解决方案

    Discovering Many Diverse Solutions with Bayesian Optimization. (arXiv:2210.10953v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10953](http://arxiv.org/abs/2210.10953)

    ROBOT是一种新的贝叶斯优化方法，可以找到一组高性能、多样化的解决方案，解决了传统单目标贝叶斯优化方法只能找到一个最佳解决方案的局限性。

    ROBOT is a new Bayesian optimization method that can find a portfolio of high-performing diverse solutions, addressing the limitation of traditional single-objective Bayesian optimization methods that only seek to find a single best solution.

    贝叶斯优化是一种用于黑盒目标函数的高效优化的流行方法。传统的单目标贝叶斯优化方法只寻求找到一个最佳解决方案，这在解决方案后期可能变得棘手的情况下会有很大的局限性。为了解决这个问题，我们提出了一种名为ROBOT的排序贝叶斯优化方法，旨在找到一组高性能、多样化的解决方案，这些解决方案根据用户指定的多样性度量进行排序。我们在几个真实世界的应用中评估了ROBOT，并展示了它可以发现大量高性能的多样化解决方案，同时与寻找单个最佳解决方案相比，需要很少的额外函数评估。

    Bayesian optimization (BO) is a popular approach for sample-efficient optimization of black-box objective functions. While BO has been successfully applied to a wide range of scientific applications, traditional approaches to single-objective BO only seek to find a single best solution. This can be a significant limitation in situations where solutions may later turn out to be intractable. For example, a designed molecule may turn out to violate constraints that can only be reasonably evaluated after the optimization process has concluded. To address this issue, we propose Rank-Ordered Bayesian Optimization with Trust-regions (ROBOT) which aims to find a portfolio of high-performing solutions that are diverse according to a user-specified diversity metric. We evaluate ROBOT on several real-world applications and show that it can discover large sets of high-performing diverse solutions while requiring few additional function evaluations compared to finding a single best solution.
    
[^144]: 神经文本作者归属和混淆：数据挖掘视角

    Attribution and Obfuscation of Neural Text Authorship: A Data Mining Perspective. (arXiv:2210.10488v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.10488](http://arxiv.org/abs/2210.10488)

    本文调查了神经文本生成技术在作者归属和混淆中的应用，提出了需要开发新型AA / AO解决方案来处理神经文本的问题。

    This survey investigates the application of neural text generation techniques in authorship attribution and obfuscation, and highlights the need for developing novel AA/AO solutions to deal with neural texts.

    作者归属（AA）和作者混淆（AO）是隐私研究中越来越受关注和重要的两个交织的研究问题。传统上，作者的概念及其随之而来的隐私关注仅针对人类作者。然而，由于自然语言处理中神经文本生成（NTG）技术的爆炸性进展，现在必须考虑人类、机器或它们的组合的作者身份。由于神经文本在恶意使用时的潜在威胁，了解传统AA / AO解决方案的局限性并开发处理神经文本的新型AA / AO解决方案变得至关重要。

    Two interlocking research questions of growing interest and importance in privacy research are Authorship Attribution (AA) and Authorship Obfuscation (AO). Given an artifact, especially a text t in question, an AA solution aims to accurately attribute t to its true author out of many candidate authors while an AO solution aims to modify t to hide its true authorship. Traditionally, the notion of authorship and its accompanying privacy concern is only toward human authors. However, in recent years, due to the explosive advancements in Neural Text Generation (NTG) techniques in NLP, capable of synthesizing human-quality open-ended texts (so-called "neural texts"), one has to now consider authorships by humans, machines, or their combination. Due to the implications and potential threats of neural texts when used maliciously, it has become critical to understand the limitations of traditional AA/AO solutions and develop novel AA/AO solutions in dealing with neural texts. In this survey, t
    
[^145]: 带有$O(1)$共识速率的分散式学习的通信高效拓扑结构

    Communication-Efficient Topologies for Decentralized Learning with $O(1)$ Consensus Rate. (arXiv:2210.07881v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2210.07881](http://arxiv.org/abs/2210.07881)

    本文提出了一种新的拓扑结构家族EquiTopo，它具有（几乎）恒定的度数和与网络大小无关的共识速率，用于衡量混合效率。

    This paper proposes a new family of topologies, EquiTopo, which has an (almost) constant degree and a network-size-independent consensus rate that is used to measure the mixing efficiency.

    分散式优化是分布式学习中的新兴范例，其中代理通过点对点通信实现网络范围内的解决方案，而无需中央服务器。由于通信往往比计算慢，因此当每个代理每次迭代仅与少数相邻代理通信时，它们可以比使用更多代理或中央服务器更快地完成迭代。然而，到达网络范围内的解决方案所需的总迭代次数受到代理信息通过通信“混合”的速度的影响。我们发现，流行的通信拓扑结构要么具有较大的最大度数（例如星形和完全图），要么在混合信息方面效果不佳（例如环和网格）。为了解决这个问题，我们提出了一种新的拓扑结构家族EquiTopo，它具有（几乎）恒定的度数和与网络大小无关的共识速率，用于衡量混合效率。在所提出的家族中，EquiStatic的度数为$

    Decentralized optimization is an emerging paradigm in distributed learning in which agents achieve network-wide solutions by peer-to-peer communication without the central server. Since communication tends to be slower than computation, when each agent communicates with only a few neighboring agents per iteration, they can complete iterations faster than with more agents or a central server. However, the total number of iterations to reach a network-wide solution is affected by the speed at which the agents' information is ``mixed'' by communication. We found that popular communication topologies either have large maximum degrees (such as stars and complete graphs) or are ineffective at mixing information (such as rings and grids). To address this problem, we propose a new family of topologies, EquiTopo, which has an (almost) constant degree and a network-size-independent consensus rate that is used to measure the mixing efficiency.  In the proposed family, EquiStatic has a degree of $
    
[^146]: 自监督几何对应用于野外类别级6D物体姿态估计

    Self-Supervised Geometric Correspondence for Category-Level 6D Object Pose Estimation in the Wild. (arXiv:2210.07199v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.07199](http://arxiv.org/abs/2210.07199)

    本文提出了一种自监督学习方法，直接在大规模真实世界物体视频上进行类别级6D姿态估计。通过表面嵌入学习了输入图像和规范形状之间的密集对应关系，并提出了新颖的几何循环一致性损失。学习到的对应关系可以应用于6D姿态估计和其他任务。

    This paper proposes a self-supervised learning approach for category-level 6D object pose estimation in the wild, which reconstructs the canonical 3D shape of an object category and learns dense correspondences between input images and the canonical shape via surface embedding. The proposed novel geometrical cycle-consistency losses construct cycles across 2D-3D spaces, across different instances and different time steps. The learned correspondence can be applied for 6D pose estimation and other tasks.

    尽管6D物体姿态估计在计算机视觉和机器人领域有广泛的应用，但由于缺乏注释，它仍然远未解决。当转向类别级6D姿态时，问题变得更加具有挑战性，因为需要对未见实例进行泛化。目前的方法受到从模拟或从人类收集的注释的限制。在本文中，我们通过引入一种自监督学习方法，直接在大规模真实世界物体视频上进行类别级6D姿态估计，克服了这一障碍。我们的框架重构了物体类别的规范3D形状，并通过表面嵌入学习了输入图像和规范形状之间的密集对应关系。对于训练，我们提出了新颖的几何循环一致性损失，它们在2D-3D空间、不同实例和不同时间步之间构建循环。学习到的对应关系可以应用于6D姿态估计和其他任务。

    While 6D object pose estimation has wide applications across computer vision and robotics, it remains far from being solved due to the lack of annotations. The problem becomes even more challenging when moving to category-level 6D pose, which requires generalization to unseen instances. Current approaches are restricted by leveraging annotations from simulation or collected from humans. In this paper, we overcome this barrier by introducing a self-supervised learning approach trained directly on large-scale real-world object videos for category-level 6D pose estimation in the wild. Our framework reconstructs the canonical 3D shape of an object category and learns dense correspondences between input images and the canonical shape via surface embedding. For training, we propose novel geometrical cycle-consistency losses which construct cycles across 2D-3D spaces, across different instances and different time steps. The learned correspondence can be applied for 6D pose estimation and othe
    
[^147]: PDEBENCH：科学机器学习的广泛基准测试

    PDEBENCH: An Extensive Benchmark for Scientific Machine Learning. (arXiv:2210.07182v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.07182](http://arxiv.org/abs/2210.07182)

    PDEBench是一个基于偏微分方程的时间依赖性模拟任务基准套件，包括代码和数据，可用于对新型机器学习模型的性能进行基准测试，同时还可以与经典数值模拟和机器学习基线进行比较。

    PDEBench is a benchmark suite of time-dependent simulation tasks based on Partial Differential Equations (PDEs), which includes code and data to benchmark the performance of novel machine learning models against both classical numerical simulations and machine learning baselines.

    近年来，基于机器学习的物理系统建模受到了越来越多的关注。尽管取得了一些令人印象深刻的进展，但仍缺乏易于使用但具有挑战性和代表性的科学ML基准测试。我们介绍了PDEBench，这是一个基于偏微分方程（PDE）的时间依赖性模拟任务基准套件。PDEBench包括代码和数据，以对新型机器学习模型的性能进行基准测试，同时还可以与经典数值模拟和机器学习基线进行比较。我们提出的基准问题集具有以下独特特征：（1）与现有基准测试相比，PDE的范围更广，从相对常见的示例到更现实和困难的问题；（2）与先前的工作相比，准备好使用的数据集更大，包括跨更多初始和边界条件以及PDE参数的多个模拟运行；（3）更广泛的基准测试，包括更多的性能指标和评估方法。

    Machine learning-based modeling of physical systems has experienced increased interest in recent years. Despite some impressive progress, there is still a lack of benchmarks for Scientific ML that are easy to use but still challenging and representative of a wide range of problems. We introduce PDEBench, a benchmark suite of time-dependent simulation tasks based on Partial Differential Equations (PDEs). PDEBench comprises both code and data to benchmark the performance of novel machine learning models against both classical numerical simulations and machine learning baselines. Our proposed set of benchmark problems contribute the following unique features: (1) A much wider range of PDEs compared to existing benchmarks, ranging from relatively common examples to more realistic and difficult problems; (2) much larger ready-to-use datasets compared to prior work, comprising multiple simulation runs across a larger number of initial and boundary conditions and PDE parameters; (3) more exte
    
[^148]: 混合强化学习：同时使用离线和在线数据可以使强化学习更加高效

    Hybrid RL: Using Both Offline and Online Data Can Make RL Efficient. (arXiv:2210.06718v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.06718](http://arxiv.org/abs/2210.06718)

    本文提出了一种混合强化学习设置，通过同时使用离线和在线数据，可以设计出简单且高效的算法，我们证明了该算法在离线数据集支持高质量策略且环境具有有界双线性秩的情况下既具有计算效率又具有统计效率。在实验中，我们展示了使用神经网络函数逼近的Hy-Q的表现优于其他算法。

    This paper proposes a hybrid reinforcement learning setting that uses both offline and online data to design simple and efficient algorithms. The authors prove that the algorithm is both computationally and statistically efficient whenever the offline dataset supports a high-quality policy and the environment has bounded bilinear rank. In experiments, they show that the Hy-Q algorithm with neural network function approximation outperforms other algorithms.

    我们考虑了一种混合强化学习设置（混合RL），其中代理可以通过离线数据集和实时在线交互收集经验。该框架缓解了纯离线和在线RL设置中出现的挑战，允许设计简单且高效的算法，无论是在理论还是实践中。我们通过将经典的Q学习/迭代算法适应于混合设置来展示这些优势，我们称之为混合Q学习或Hy-Q。在我们的理论结果中，我们证明了该算法在离线数据集支持高质量策略且环境具有有界双线性秩的情况下既具有计算效率又具有统计效率。值得注意的是，与策略梯度/迭代方法的保证相反，我们不需要对初始分布提供的覆盖范围做任何假设。在我们的实验结果中，我们展示了使用神经网络函数逼近的Hy-Q的表现优于其他算法。

    We consider a hybrid reinforcement learning setting (Hybrid RL), in which an agent has access to an offline dataset and the ability to collect experience via real-world online interaction. The framework mitigates the challenges that arise in both pure offline and online RL settings, allowing for the design of simple and highly effective algorithms, in both theory and practice. We demonstrate these advantages by adapting the classical Q learning/iteration algorithm to the hybrid setting, which we call Hybrid Q-Learning or Hy-Q. In our theoretical results, we prove that the algorithm is both computationally and statistically efficient whenever the offline dataset supports a high-quality policy and the environment has bounded bilinear rank. Notably, we require no assumptions on the coverage provided by the initial distribution, in contrast with guarantees for policy gradient/iteration methods. In our experimental results, we show that Hy-Q with neural network function approximation outper
    
[^149]: 关系注意力：将Transformer推广到图结构任务

    Relational Attention: Generalizing Transformers for Graph-Structured Tasks. (arXiv:2210.05062v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.05062](http://arxiv.org/abs/2210.05062)

    本文提出了一种关系注意力机制，将Transformer推广到图结构任务中，通过考虑和更新边向量，实现了对图结构数据的推理，相比于专门设计用于推理图结构数据的最先进的图神经网络，在各种图结构任务上都有明显的优势。

    This paper proposes a relational attention mechanism that generalizes Transformers for graph-structured tasks by considering and updating edge vectors in each Transformer layer, achieving reasoning over graph-structured data. Compared to state-of-the-art graph neural networks designed for reasoning over graph-structured data, it has significant advantages in various graph-structured tasks.

    Transformer可以灵活地操作表示任务特定实体及其属性的实值向量集，其中每个向量可能编码一个单词片段令牌及其在序列中的位置，或者一些不带位置的信息。但作为集合处理器，Transformer在推理更一般的图结构数据（其中节点表示实体，边表示实体之间的关系）方面处于劣势。为了解决这个缺点，我们将Transformer注意力推广到每个Transformer层中考虑和更新边向量。我们在各种图结构任务上评估了这个关系Transformer，包括大型且具有挑战性的CLRS算法推理基准测试。在那里，它明显优于专门设计用于推理图结构数据的最先进的图神经网络。我们的分析表明，这些收益归因于关系注意力固有的能力，即利用大量的边信息来推理。

    Transformers flexibly operate over sets of real-valued vectors representing task-specific entities and their attributes, where each vector might encode one word-piece token and its position in a sequence, or some piece of information that carries no position at all. But as set processors, transformers are at a disadvantage in reasoning over more general graph-structured data where nodes represent entities and edges represent relations between entities. To address this shortcoming, we generalize transformer attention to consider and update edge vectors in each transformer layer. We evaluate this relational transformer on a diverse array of graph-structured tasks, including the large and challenging CLRS Algorithmic Reasoning Benchmark. There, it dramatically outperforms state-of-the-art graph neural networks expressly designed to reason over graph-structured data. Our analysis demonstrates that these gains are attributable to relational attention's inherent ability to leverage the great
    
[^150]: 对抗性CNN扰动攻击的对称防御

    Symmetry Defense Against CNN Adversarial Perturbation Attacks. (arXiv:2210.04087v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.04087](http://arxiv.org/abs/2210.04087)

    本文提出了一种对称防御方法，通过翻转或水平翻转对称对抗样本来提高对抗性鲁棒性，同时使用子群对称性进行分类。

    This paper proposes a symmetry defense method to improve adversarial robustness by flipping or horizontally flipping symmetric adversarial samples, and uses subgroup symmetries for classification.

    卷积神经网络分类器（CNN）容易受到对抗性攻击，这些攻击会扰动原始样本以欺骗分类器，例如自动驾驶汽车的道路标志图像分类器。CNN在对称样本的分类中也缺乏不变性，因为CNN可以以不同的方式对称样本进行分类。考虑到CNN缺乏对抗性鲁棒性和CNN缺乏不变性，对称对抗样本的分类可能与其错误分类不同。本文通过设计一种对称防御来回答这个问题，在对抗者不知道防御的情况下，将对称对抗样本翻转或水平翻转后再进行分类。对于知道防御的对手，防御设计了一个Klein四个对称子群，其中包括水平翻转和像素反转对称性。对称防御使用子群对称性进行分类，以提高对抗性鲁棒性。

    Convolutional neural network classifiers (CNNs) are susceptible to adversarial attacks that perturb original samples to fool classifiers such as an autonomous vehicle's road sign image classifier. CNNs also lack invariance in the classification of symmetric samples because CNNs can classify symmetric samples differently. Considered together, the CNN lack of adversarial robustness and the CNN lack of invariance mean that the classification of symmetric adversarial samples can differ from their incorrect classification. Could symmetric adversarial samples revert to their correct classification? This paper answers this question by designing a symmetry defense that inverts or horizontally flips adversarial samples before classification against adversaries unaware of the defense. Against adversaries aware of the defense, the defense devises a Klein four symmetry subgroup that includes the horizontal flip and pixel inversion symmetries. The symmetry defense uses the subgroup symmetries in ac
    
[^151]: 弱监督下的标签传播算法

    Label Propagation with Weak Supervision. (arXiv:2210.03594v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.03594](http://arxiv.org/abs/2210.03594)

    本文提出了一种利用弱监督信息的标签传播算法，通过利用未标记数据上的概率假设标签，结合局部几何特性和先验信息的质量，提供了一个误差界，并提出了一个框架，用于合并多个噪声信息源。在多个基准弱监督分类任务上展示了方法的能力，显示出对现有半监督和弱监督方法的改进。

    This paper proposes a label propagation algorithm that utilizes weak supervision information, specifically probabilistic hypothesized labels on the unlabeled data, and provides an error bound that exploits both the local geometric properties of the underlying graph and the quality of the prior information. The approach is demonstrated on multiple benchmark weakly supervised classification tasks, showing improvements upon existing semi-supervised and weakly supervised methods.

    半监督学习和弱监督学习是当前机器学习应用中旨在减少标记数据需求的重要范式。本文介绍了一种新的对经典标签传播算法（LPA）（Zhu＆Ghahramani，2002）的分析，该算法利用了有用的先验信息，特别是未标记数据上的概率假设标签。我们提供了一个误差界，利用了底层图形的局部几何特性和先验信息的质量。我们还提出了一个框架，用于合并多个噪声信息源。特别是，我们考虑了弱监督的设置，其中我们的信息来源是弱标记者。我们在多个基准弱监督分类任务上展示了我们方法的能力，显示出对现有半监督和弱监督方法的改进。

    Semi-supervised learning and weakly supervised learning are important paradigms that aim to reduce the growing demand for labeled data in current machine learning applications. In this paper, we introduce a novel analysis of the classical label propagation algorithm (LPA) (Zhu & Ghahramani, 2002) that moreover takes advantage of useful prior information, specifically probabilistic hypothesized labels on the unlabeled data. We provide an error bound that exploits both the local geometric properties of the underlying graph and the quality of the prior information. We also propose a framework to incorporate multiple sources of noisy information. In particular, we consider the setting of weak supervision, where our sources of information are weak labelers. We demonstrate the ability of our approach on multiple benchmark weakly supervised classification tasks, showing improvements upon existing semi-supervised and weakly supervised methods.
    
[^152]: PSVRF: 无参考学习还原变调语音

    PSVRF: Learning to restore Pitch-Shifted Voice without reference. (arXiv:2210.02731v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2210.02731](http://arxiv.org/abs/2210.02731)

    本文提出了一种无参考方法PSVRF，用于高质量还原变调语音，可以增强ASV系统对音高缩放攻击的鲁棒性，性能甚至超过了最先进的基于参考的方法。

    This paper proposes a no-reference approach called PSVRF for high-quality restoration of pitch-shifted voice, which enhances the robustness of ASV systems to pitch-scaling attacks and even outperforms the state-of-the-art reference-based approach.

    音高缩放算法对自动说话人验证（ASV）系统的安全性有重要影响。虽然已经提出了许多反欺骗算法来识别变调语音并将其恢复到原始版本，但它们要么性能较差，要么需要原始语音作为参考，限制了应用前景。本文提出了一种无参考方法PSVRF，用于高质量还原变调语音。在AISHELL-1和AISHELL-3上的实验表明，PSVRF可以恢复被各种音高缩放技术伪装的语音，显然增强了ASV系统对音高缩放攻击的鲁棒性。此外，PSVRF的性能甚至超过了最先进的基于参考的方法。

    Pitch scaling algorithms have a significant impact on the security of Automatic Speaker Verification (ASV) systems. Although numerous anti-spoofing algorithms have been proposed to identify the pitch-shifted voice and even restore it to the original version, they either have poor performance or require the original voice as a reference, limiting the prospects of applications. In this paper, we propose a no-reference approach termed PSVRF$^1$ for high-quality restoration of pitch-shifted voice. Experiments on AISHELL-1 and AISHELL-3 demonstrate that PSVRF can restore the voice disguised by various pitch-scaling techniques, which obviously enhances the robustness of ASV systems to pitch-scaling attacks. Furthermore, the performance of PSVRF even surpasses that of the state-of-the-art reference-based approach.
    
[^153]: 通过冗余性实现稀疏性：用SGD求解$L_1$

    Sparsity by Redundancy: Solving $L_1$ with SGD. (arXiv:2210.01212v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01212](http://arxiv.org/abs/2210.01212)

    该论文提出了一种通过冗余重参数化和简单的随机梯度下降来最小化带有$L_1$惩罚的通用可微损失函数的方法，称为\textit{spred}，是$L_1$的精确求解器，可用于训练稀疏神经网络以执行基因选择任务和神经网络压缩任务，弥合了深度学习中的稀疏性和传统统计学习之间的差距。

    This paper proposes a method called "spred" to minimize a generic differentiable loss function with $L_1$ penalty using redundant reparametrization and straightforward stochastic gradient descent. It is an exact solver of $L_1$ and can be used to train sparse neural networks for gene selection tasks and neural network compression tasks, bridging the gap between sparsity in deep learning and conventional statistical learning.

    我们提出了一种通过冗余重参数化和简单的随机梯度下降来最小化带有$L_1$惩罚的通用可微损失函数的方法。我们的提议是$L_1$惩罚等价于带有权重衰减的可微重参数化的直接推广。我们证明了所提出的方法，即\textit{spred}，是$L_1$的精确求解器，并且对于通用的非凸函数，重参数化技巧是完全“良性”的。在实践中，我们展示了该方法的实用性，包括(1)训练稀疏神经网络以执行基因选择任务，其中涉及在非常高维空间中找到相关特征，以及(2)神经网络压缩任务，先前尝试应用$L_1$惩罚的方法均未成功。从概念上讲，我们的结果弥合了深度学习中的稀疏性和传统统计学习之间的差距。

    We propose to minimize a generic differentiable loss function with $L_1$ penalty with a redundant reparametrization and straightforward stochastic gradient descent. Our proposal is the direct generalization of a series of previous ideas that the $L_1$ penalty may be equivalent to a differentiable reparametrization with weight decay. We prove that the proposed method, \textit{spred}, is an exact solver of $L_1$ and that the reparametrization trick is completely ``benign" for a generic nonconvex function. Practically, we demonstrate the usefulness of the method in (1) training sparse neural networks to perform gene selection tasks, which involves finding relevant features in a very high dimensional space, and (2) neural network compression task, to which previous attempts at applying the $L_1$-penalty have been unsuccessful. Conceptually, our result bridges the gap between the sparsity in deep learning and conventional statistical learning.
    
[^154]: 无监督模型选择用于时间序列异常检测

    Unsupervised Model Selection for Time-series Anomaly Detection. (arXiv:2210.01078v3 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2210.01078](http://arxiv.org/abs/2210.01078)

    本文提出了一种无监督模型选择方法，用于时间序列异常检测，通过三类替代度量，即预测误差、模型中心性和注入合成异常的性能，选择最准确的模型。

    This paper proposes an unsupervised model selection method for time-series anomaly detection, which selects the most accurate model through three classes of surrogate metrics, namely, prediction error, model centrality, and performance on injected synthetic anomalies.

    时间序列中的异常检测具有广泛的实际应用。虽然文献中提出了许多异常检测方法，但最近的一项调查得出结论，没有一种方法在各种数据集上都最准确。更糟糕的是，异常标签在实践中很少可用。在文献中，如何在没有标签的情况下选择给定数据集的最准确模型的实际问题得到了很少的关注。本文回答了这个问题，即给定一个未标记的数据集和一组候选异常检测器，如何选择最准确的模型？为此，我们确定了三类替代（无监督）度量，即预测误差、模型中心性和注入合成异常的性能，并表明一些度量与标准监督异常检测性能度量（如$F_1$分数）高度相关，但程度不同。我们制定了多个度量组合的度量组合方法。

    Anomaly detection in time-series has a wide range of practical applications. While numerous anomaly detection methods have been proposed in the literature, a recent survey concluded that no single method is the most accurate across various datasets. To make matters worse, anomaly labels are scarce and rarely available in practice. The practical problem of selecting the most accurate model for a given dataset without labels has received little attention in the literature. This paper answers this question i.e. Given an unlabeled dataset and a set of candidate anomaly detectors, how can we select the most accurate model? To this end, we identify three classes of surrogate (unsupervised) metrics, namely, prediction error, model centrality, and performance on injected synthetic anomalies, and show that some metrics are highly correlated with standard supervised anomaly detection performance metrics such as the $F_1$ score, but to varying degrees. We formulate metric combination with multipl
    
[^155]: 自监督学习的损失函数空间是如何形成的？

    What shapes the loss landscape of self-supervised learning?. (arXiv:2210.00638v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00638](http://arxiv.org/abs/2210.00638)

    本文通过分析自监督学习的损失函数空间，回答了维度崩溃的原因和影响，以及维度崩溃如何有益，并影响SSL对数据不平衡的鲁棒性。

    This paper answers questions about the causes and effects of dimensional collapse in self-supervised learning (SSL) by analyzing the SSL loss landscape, and explores how dimensional collapse can be beneficial and affect the robustness of SSL against data imbalance.

    最近，防止表示完全和维度崩溃已成为自监督学习（SSL）的设计原则。然而，我们对理论的理解仍有疑问：这些崩溃何时发生？机制和原因是什么？我们通过推导和彻底分析SSL损失函数空间的可分析理论来回答这些问题。在这个理论中，我们确定了维度崩溃的原因，并研究了归一化和偏差的影响。最后，我们利用分析理论所提供的可解释性来理解维度崩溃如何有益，并影响SSL对数据不平衡的鲁棒性。

    Prevention of complete and dimensional collapse of representations has recently become a design principle for self-supervised learning (SSL). However, questions remain in our theoretical understanding: When do those collapses occur? What are the mechanisms and causes? We answer these questions by deriving and thoroughly analyzing an analytically tractable theory of SSL loss landscapes. In this theory, we identify the causes of the dimensional collapse and study the effect of normalization and bias. Finally, we leverage the interpretability afforded by the analytical theory to understand how dimensional collapse can be beneficial and what affects the robustness of SSL against data imbalance.
    
[^156]: 可微分的自然语言指令解析和视觉定位在物体放置任务中的应用

    Differentiable Parsing and Visual Grounding of Natural Language Instructions for Object Placement. (arXiv:2210.00215v4 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.00215](http://arxiv.org/abs/2210.00215)

    ParaGon是一种可微分的自然语言指令解析和视觉定位方法，通过将语言指令解析为以对象为中心的图形表示，以单独定位对象，并使用一种新颖的基于粒子的图神经网络来推理关于带有不确定性的物体放置。

    ParaGon is a differentiable method for natural language instruction parsing and visual grounding in object placement tasks. It parses language instructions into an object-centric graph representation to ground objects individually and uses a novel particle-based graph neural network to reason about object placements with uncertainty.

    我们提出了一种新的方法，PARsing And visual GrOuNding (ParaGon)，用于在物体放置任务中对自然语言进行定位。自然语言通常用组合性和歧义性描述对象和空间关系，这是有效语言定位的两个主要障碍。对于组合性，ParaGon将语言指令解析为以对象为中心的图形表示，以单独定位对象。对于歧义性，ParaGon使用一种新颖的基于粒子的图神经网络来推理关于带有不确定性的物体放置。本质上，ParaGon将解析算法集成到概率的数据驱动学习框架中。它是完全可微分的，并从数据中端到端地训练，以对抗复杂的，模糊的语言输入。

    We present a new method, PARsing And visual GrOuNding (ParaGon), for grounding natural language in object placement tasks. Natural language generally describes objects and spatial relations with compositionality and ambiguity, two major obstacles to effective language grounding. For compositionality, ParaGon parses a language instruction into an object-centric graph representation to ground objects individually. For ambiguity, ParaGon uses a novel particle-based graph neural network to reason about object placements with uncertainty. Essentially, ParaGon integrates a parsing algorithm into a probabilistic, data-driven learning framework. It is fully differentiable and trained end-to-end from data for robustness against complex, ambiguous language input.
    
[^157]: Recipro-CAM: 基于快速无梯度的可解释性卷积神经网络的视觉解释方法

    Recipro-CAM: Fast gradient-free visual explanations for convolutional neural networks. (arXiv:2209.14074v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.14074](http://arxiv.org/abs/2209.14074)

    Recipro-CAM是一种快速无梯度的可解释性卷积神经网络的视觉解释方法，通过对提取的特征图进行空间掩蔽，利用激活图和目标类别的网络预测之间的相关性，解决了CAM和Grad-CAM方法的架构限制和梯度计算负担问题，具有更短的执行时间，适用于实际解决方案。

    Recipro-CAM is a fast gradient-free visual explanation method for interpretable convolutional neural networks. It solves the architectural constraints and gradient computing burden issues of CAM and Grad-CAM methods by spatially masking the extracted feature maps to exploit the correlation between activation maps and network predictions for target classes, with shorter execution time and practical applicability.

    卷积神经网络（CNN）是计算机视觉中广泛使用的深度学习架构。然而，它的黑盒本质使得难以解释模型的行为。为了缓解这个问题，AI从业者探索了可解释性AI方法，如Class Activation Map（CAM）和Grad-CAM。虽然这些方法表现出了很好的前景，但它们受到架构限制或梯度计算负担的限制。为了解决这个问题，提出了Score-CAM和Ablation-CAM作为无梯度方法，但与基于CAM或Grad-CAM的方法相比，它们具有更长的执行时间，使它们不适合实际解决方案，尽管它们解决了梯度相关问题并启用了推理模式XAI。为了解决这个挑战，我们提出了一种快速无梯度的Recipro-CAM方法。我们的方法涉及对提取的特征图进行空间掩蔽，以利用激活图和目标类别的网络预测之间的相关性。

    The Convolutional Neural Network (CNN) is a widely used deep learning architecture for computer vision. However, its black box nature makes it difficult to interpret the behavior of the model. To mitigate this issue, AI practitioners have explored explainable AI methods like Class Activation Map (CAM) and Grad-CAM. Although these methods have shown promise, they are limited by architectural constraints or the burden of gradient computing. To overcome this issue, Score-CAM and Ablation-CAM have been proposed as gradient-free methods, but they have longer execution times compared to CAM or Grad-CAM based methods, making them unsuitable for real-world solution though they resolved gradient related issues and enabled inference mode XAI. To address this challenge, we propose a fast gradient-free Reciprocal CAM (Recipro-CAM) method. Our approach involves spatially masking the extracted feature maps to exploit the correlation between activation maps and network predictions for target classes.
    
[^158]: 开放式联邦学习系统的稳定性分析

    On the Stability Analysis of Open Federated Learning Systems. (arXiv:2209.12307v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.12307](http://arxiv.org/abs/2209.12307)

    本文研究了开放式联邦学习系统的稳定性问题，提出了一种新的性能度量，即开放式FL系统的稳定性，并在假设本地客户端函数是强凸和平滑的情况下，理论上量化了两种FL算法的稳定半径。

    This paper studies the stability issue of open federated learning systems, proposes a new performance metric, namely the stability of open FL systems, and theoretically quantifies the stability radius of two FL algorithms under the assumption that local clients' functions are strongly convex and smooth.

    我们考虑开放式联邦学习系统，其中客户端可能在联邦学习过程中加入和/或离开系统。由于存在客户端数量的变化，无法保证在开放系统中收敛到固定模型。因此，我们采用一种新的性能度量，称为开放式FL系统的稳定性，它量化了在开放系统中学习模型的大小。在假设本地客户端函数是强凸和平滑的情况下，我们理论上量化了两种FL算法（即本地SGD和本地Adam）的稳定半径。我们观察到，这个半径依赖于几个关键参数，包括函数条件数以及随机梯度的方差。我们的理论结果在合成和真实世界基准数据集上通过数值模拟进一步验证。

    We consider the open federated learning (FL) systems, where clients may join and/or leave the system during the FL process. Given the variability of the number of present clients, convergence to a fixed model cannot be guaranteed in open systems. Instead, we resort to a new performance metric that we term the stability of open FL systems, which quantifies the magnitude of the learned model in open systems. Under the assumption that local clients' functions are strongly convex and smooth, we theoretically quantify the radius of stability for two FL algorithms, namely local SGD and local Adam. We observe that this radius relies on several key parameters, including the function condition number as well as the variance of the stochastic gradient. Our theoretical results are further verified by numerical simulations on both synthetic and real-world benchmark data-sets.
    
[^159]: U-Sleep对AASM指南的弹性

    U-Sleep's resilience to AASM guidelines. (arXiv:2209.11173v3 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2209.11173](http://arxiv.org/abs/2209.11173)

    本研究表明，基于深度学习的U-Sleep睡眠评分算法可以弹性地使用非推荐或非传统的导联，而不需要严格遵守AASM指南。

    This study shows that the deep learning-based U-Sleep sleep scoring algorithm can flexibly use non-recommended or non-traditional derivations without strictly adhering to AASM guidelines.

    AASM指南是几十年努力的结果，旨在标准化睡眠评分程序，最终目标是共享全球通用的方法。该指南涵盖了从技术/数字规范（例如，推荐的EEG导联）到根据年龄详细的睡眠评分规则的几个方面。自动睡眠评分系统始终将标准作为基本指南。在这种情况下，深度学习表现出比传统机器学习更好的性能。我们的研究表明，基于深度学习的睡眠评分算法可能不需要充分利用临床知识或严格遵守AASM指南。具体而言，我们证明了U-Sleep，一种最先进的睡眠评分算法，即使使用临床非推荐或非传统的导联，也可以足够强大地解决评分任务，而且不需要利用有关受试者年龄的信息。

    AASM guidelines are the result of decades of efforts aiming at standardizing sleep scoring procedure, with the final goal of sharing a worldwide common methodology. The guidelines cover several aspects from the technical/digital specifications,e.g., recommended EEG derivations, to detailed sleep scoring rules accordingly to age. Automated sleep scoring systems have always largely exploited the standards as fundamental guidelines. In this context, deep learning has demonstrated better performance compared to classical machine learning. Our present work shows that a deep learning based sleep scoring algorithm may not need to fully exploit the clinical knowledge or to strictly adhere to the AASM guidelines. Specifically, we demonstrate that U-Sleep, a state-of-the-art sleep scoring algorithm, can be strong enough to solve the scoring task even using clinically non-recommended or non-conventional derivations, and with no need to exploit information about the chronological age of the subjec
    
[^160]: 学习稀疏图形均场博弈

    Learning Sparse Graphon Mean Field Games. (arXiv:2209.03880v3 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2209.03880](http://arxiv.org/abs/2209.03880)

    本文提出了一种新型的GMFG公式，称为LPGMFG，它利用$L^p$图形的图形理论概念，提供了一种机器学习工具，以有效且准确地近似解决稀疏网络问题，特别是幂律网络。我们推导出理论存在和收敛保证，并给出了实证例子，证明了我们的学习方法的准确性。

    This paper proposes a novel formulation of GMFGs, called LPGMFG, which leverages the graph theoretical concept of $L^p$ graphons and provides a machine learning tool to efficiently and accurately approximate solutions for sparse network problems, especially power law networks. The paper derives theoretical existence and convergence guarantees and gives empirical examples that demonstrate the accuracy of the learning method.

    尽管多智能体强化学习（MARL）领域在过去几年中取得了相当大的进展，但解决具有大量代理的系统仍然是一个难题。图形均场博弈（GMFG）使得可以对否则难以处理的MARL问题进行可扩展的分析。由于图形的数学结构，这种方法仅限于描述许多现实世界网络（如幂律图）的稠密图形，这是不足的。我们的论文介绍了GMFG的新型公式，称为LPGMFG，它利用$L^p$图形的图形理论概念，并提供了一种机器学习工具，以有效且准确地近似解决稀疏网络问题。这尤其包括在各种应用领域中经验观察到的幂律网络，这些网络无法被标准图形所捕捉。我们推导出理论存在和收敛保证，并给出了实证例子，证明了我们的学习方法的准确性。

    Although the field of multi-agent reinforcement learning (MARL) has made considerable progress in the last years, solving systems with a large number of agents remains a hard challenge. Graphon mean field games (GMFGs) enable the scalable analysis of MARL problems that are otherwise intractable. By the mathematical structure of graphons, this approach is limited to dense graphs which are insufficient to describe many real-world networks such as power law graphs. Our paper introduces a novel formulation of GMFGs, called LPGMFGs, which leverages the graph theoretical concept of $L^p$ graphons and provides a machine learning tool to efficiently and accurately approximate solutions for sparse network problems. This especially includes power law networks which are empirically observed in various application areas and cannot be captured by standard graphons. We derive theoretical existence and convergence guarantees and give empirical examples that demonstrate the accuracy of our learning ap
    
[^161]: 利用机器学习从有限时间序列数据中估计相关矩阵

    Estimation of Correlation Matrices from Limited time series Data using Machine Learning. (arXiv:2209.01198v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.01198](http://arxiv.org/abs/2209.01198)

    本文提出了一种利用机器学习从少数节点的有限时间序列信息预测整个系统相关矩阵的方法，验证了只有基础系统的子集的有限时间序列足以进行良好的相关矩阵预测。

    This paper proposes a method to predict the correlation matrix of entire systems from finite time series information of a few randomly selected nodes using supervised machine learning, and validates that only a limited time series of a subset of the entire system is enough to make good correlation matrix predictions.

    相关矩阵包含关于动态系统的各种时空信息。从少数节点的部分时间序列信息预测相关矩阵表征了整个基础系统的时空动态。这些信息有助于预测基础网络结构，例如从尖峰数据推断神经元连接，从表达数据推断基因之间的因果依赖关系，以及发现气候变化中的长距离影响。传统的预测相关矩阵的方法利用基础网络的所有节点的时间序列数据。在这里，我们使用监督机器学习技术从少数随机选择的节点的有限时间序列信息预测整个系统的相关矩阵。预测的准确性验证了只有基础系统的子集的有限时间序列足以进行良好的相关矩阵预测。此外，使用我们的方法可以更好地理解基础系统的时空动态。

    Correlation matrices contain a wide variety of spatio-temporal information about a dynamical system. Predicting correlation matrices from partial time series information of a few nodes characterizes the spatio-temporal dynamics of the entire underlying system. This information can help to predict the underlying network structure, e.g., inferring neuronal connections from spiking data, deducing causal dependencies between genes from expression data, and discovering long spatial range influences in climate variations. Traditional methods of predicting correlation matrices utilize time series data of all the nodes of the underlying networks. Here, we use a supervised machine learning technique to predict the correlation matrix of entire systems from finite time series information of a few randomly selected nodes. The accuracy of the prediction validates that only a limited time series of a subset of the entire system is enough to make good correlation matrix predictions. Furthermore, usin
    
[^162]: 基于贝叶斯优化的组合分配

    Bayesian Optimization-based Combinatorial Assignment. (arXiv:2208.14698v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.14698](http://arxiv.org/abs/2208.14698)

    本文提出了一种基于贝叶斯优化的组合分配（BOCA）机制，通过将捕获模型不确定性的方法集成到迭代组合拍卖机制中，解决了组合分配领域中先前工作的主要缺点，能够更好地引导代理提供信息。

    This paper proposes a Bayesian optimization-based combinatorial assignment (BOCA) mechanism, which addresses the main shortcoming of prior work in the combinatorial assignment domain by integrating a method for capturing model uncertainty into an iterative combinatorial auction mechanism, and can better elicit information from agents.

    本文研究组合分配领域，包括组合拍卖和课程分配。该领域的主要挑战是随着物品数量的增加，捆绑空间呈指数增长。为了解决这个问题，最近有几篇论文提出了基于机器学习的偏好引导算法，旨在从代理中仅引导出最重要的信息。然而，这些先前工作的主要缺点是它们没有对尚未引导出的捆绑值的机制不确定性进行建模。本文通过提出一种基于贝叶斯优化的组合分配（BOCA）机制来解决这个缺点。我们的关键技术贡献是将捕获模型不确定性的方法集成到迭代组合拍卖机制中。具体而言，我们设计了一种新的方法来估计可用于定义获取函数以确定下一个查询的上限不确定性界限。这使得机制能够

    We study the combinatorial assignment domain, which includes combinatorial auctions and course allocation. The main challenge in this domain is that the bundle space grows exponentially in the number of items. To address this, several papers have recently proposed machine learning-based preference elicitation algorithms that aim to elicit only the most important information from agents. However, the main shortcoming of this prior work is that it does not model a mechanism's uncertainty over values for not yet elicited bundles. In this paper, we address this shortcoming by presenting a Bayesian optimization-based combinatorial assignment (BOCA) mechanism. Our key technical contribution is to integrate a method for capturing model uncertainty into an iterative combinatorial auction mechanism. Concretely, we design a new method for estimating an upper uncertainty bound that can be used to define an acquisition function to determine the next query to the agents. This enables the mechanism 
    
[^163]: 使用深度学习预测肺活检图像中的EGFR突变

    EGFR Mutation Prediction of Lung Biopsy Images using Deep Learning. (arXiv:2208.12506v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.12506](http://arxiv.org/abs/2208.12506)

    本文使用深度学习技术，通过对肺活检图像进行分析，实现了对EGFR突变的预测，为肺癌治疗提供了更经济、更快捷的诊断方法。

    This paper uses deep learning technology to predict EGFR mutations through analysis of lung biopsy images, providing a more economical and faster diagnostic method for lung cancer treatment.

    肺癌治疗的标准诊断程序涉及组织学亚型分型和随后的关键驱动基因突变检测，如EGFR。尽管分子分析可以揭示驱动基因突变，但该过程通常昂贵且耗时。基于深度学习的图像分析提供了一种更经济的选择，可以直接从全幻灯片图像（WSIs）中发现驱动基因突变。在这项工作中，我们使用定制的深度学习管道进行弱监督，以识别hematoxylin和eosin染色的WSIs中EGFR突变的形态学相关性，以及检测肿瘤和组织学亚型。我们通过在两个肺癌数据集（TCGA和来自印度的私人数据集）上进行严格的实验和消融研究来证明我们管道的有效性。使用我们的管道，我们实现了肿瘤检测的平均曲线下面积（AUC）为0.964，组织学亚型为0.942。

    The standard diagnostic procedures for targeted therapies in lung cancer treatment involve histological subtyping and subsequent detection of key driver mutations, such as EGFR. Even though molecular profiling can uncover the driver mutation, the process is often expensive and time-consuming. Deep learning-oriented image analysis offers a more economical alternative for discovering driver mutations directly from whole slide images (WSIs). In this work, we used customized deep learning pipelines with weak supervision to identify the morphological correlates of EGFR mutation from hematoxylin and eosin-stained WSIs, in addition to detecting tumor and histologically subtyping it. We demonstrate the effectiveness of our pipeline by conducting rigorous experiments and ablation studies on two lung cancer datasets - TCGA and a private dataset from India. With our pipeline, we achieved an average area under the curve (AUC) of 0.964 for tumor detection, and 0.942 for histological subtyping betwe
    
[^164]: 不需要计算配分函数的潜力估计方法

    Estimating a potential without the agony of the partition function. (arXiv:2208.09433v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.09433](http://arxiv.org/abs/2208.09433)

    本文提出了一种不需要计算配分函数的潜力估计方法，基于最大后验估计（MAP）估计器，将问题重新表述为优化问题，并提出了一种最小作用量类型的势函数，使我们能够快速将优化问题解决为前馈双曲神经网络。

    This paper proposes a potential estimation method that does not require the computation of the partition function, based on Maximum A-Posteriori (MAP) estimators, reformulating the problem as an optimization problem, and proposing a least-action type potential that allows for quick solution as a feed-forward hyperbolic neural network.

    在计算统计和统计学习中，给定样本估计Gibbs密度函数是一个重要的问题。虽然最大似然方法被广泛使用，但它需要计算配分函数（即密度的归一化）。对于简单的低维问题，可以轻松计算该函数，但对于一般密度和高维问题，其计算是困难甚至是不可行的。在本文中，我们提出了一种基于最大后验估计（MAP）估计器的替代方法，我们将其命名为最大恢复MAP（MR-MAP），以导出不需要计算配分函数的估计器，并将问题重新表述为优化问题。我们进一步提出了一种最小作用量类型的势函数，使我们能够快速将优化问题解决为前馈双曲神经网络。我们在一些标准数据集上展示了我们方法的有效性。

    Estimating a Gibbs density function given a sample is an important problem in computational statistics and statistical learning. Although the well established maximum likelihood method is commonly used, it requires the computation of the partition function (i.e., the normalization of the density).  This function can be easily calculated for simple low-dimensional problems but its computation is difficult or even intractable for general densities and high-dimensional problems. In this paper we propose an alternative approach based on Maximum A-Posteriori (MAP) estimators, we name Maximum Recovery MAP (MR-MAP), to derive estimators that do not require the computation of the partition function, and reformulate the problem as an optimization problem. We further propose a least-action type potential that allows us to quickly solve the optimization problem as a feed-forward hyperbolic neural network. We demonstrate the effectiveness of our methods on some standard data sets.
    
[^165]: 评估针对上下文和语义领域漂移的连续测试时间适应性

    Evaluating Continual Test-Time Adaptation for Contextual and Semantic Domain Shifts. (arXiv:2208.08767v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.08767](http://arxiv.org/abs/2208.08767)

    本文评估了针对上下文和语义领域漂移的连续测试时间适应性，无需标签。研究发现，连续测试时间适应性（CoTTA）是一种有效的方法。

    This paper evaluates continual test-time adaptation for contextual and semantic domain shifts without labels. The study finds that Continual Test-Time Adaptation (CoTTA) is an effective method.

    本文旨在将预训练的卷积神经网络在测试时间内连续适应领域漂移，无需标签。我们评估了最新技术，如预测时间批量归一化（BN）、测试熵最小化（TENT）和连续测试时间适应性（CoTTA），并在两个现实和具有挑战性的领域漂移源上进行了测试。

    In this paper, our goal is to adapt a pre-trained convolutional neural network to domain shifts at test time. We do so continually with the incoming stream of test batches, without labels. The existing literature mostly operates on artificial shifts obtained via adversarial perturbations of a test image. Motivated by this, we evaluate the state of the art on two realistic and challenging sources of domain shifts, namely contextual and semantic shifts. Contextual shifts correspond to the environment types, for example, a model pre-trained on indoor context has to adapt to the outdoor context on CORe-50. Semantic shifts correspond to the capture types, for example a model pre-trained on natural images has to adapt to cliparts, sketches, and paintings on DomainNet. We include in our analysis recent techniques such as Prediction-Time Batch Normalization (BN), Test Entropy Minimization (TENT) and Continual Test-Time Adaptation (CoTTA). Our findings are three-fold: i) Test-time adaptation me
    
[^166]: Uconv-Conformer: 针对端到端语音识别的输入序列长度大幅缩减的新型架构

    Uconv-Conformer: High Reduction of Input Sequence Length for End-to-End Speech Recognition. (arXiv:2208.07657v3 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2208.07657](http://arxiv.org/abs/2208.07657)

    本文提出了一种新型Uconv-Conformer架构，可以将输入序列长度缩短16倍，加速中间层的工作，同时通过使用上采样块解决了收敛问题，表现出更好的WER和更快的训练和推理速度。

    The paper proposes a new Uconv-Conformer architecture that reduces the input sequence length by 16 times, speeds up the work of intermediate layers, and solves the convergence issue by using upsampling blocks. The Uconv-Conformer architecture shows better WER and faster training and inference speed.

    优化现代ASR架构是最高优先级的任务之一，因为它可以节省模型训练和推理的许多计算资源。本文提出了一种基于标准Conformer模型的新型Uconv-Conformer架构。它通过16倍的一致性缩短输入序列长度，从而加速了中间层的工作。为了解决与时间维度大幅缩减相关的收敛问题，我们使用了像U-Net架构中的上采样块来确保正确的CTC损失计算和稳定网络训练。Uconv-Conformer架构不仅在训练和推理速度方面更快，而且与基线Conformer相比，表现出更好的WER。我们最好的Uconv-Conformer模型在CPU和GPU上分别显示出47.8％和23.5％的推理加速。相对WER的减少分别为7.3％和9.2％。

    Optimization of modern ASR architectures is among the highest priority tasks since it saves many computational resources for model training and inference. The work proposes a new Uconv-Conformer architecture based on the standard Conformer model. It consistently reduces the input sequence length by 16 times, which results in speeding up the work of the intermediate layers. To solve the convergence issue connected with such a significant reduction of the time dimension, we use upsampling blocks like in the U-Net architecture to ensure the correct CTC loss calculation and stabilize network training. The Uconv-Conformer architecture appears to be not only faster in terms of training and inference speed but also shows better WER compared to the baseline Conformer. Our best Uconv-Conformer model shows 47.8% and 23.5% inference acceleration on the CPU and GPU, respectively. Relative WER reduction is 7.3% and 9.2% on LibriSpeech test_clean and test_other respectively.
    
[^167]: 物理约束深度学习用于气候降尺度

    Physics-Constrained Deep Learning for Climate Downscaling. (arXiv:2208.05424v6 [physics.ao-ph] UPDATED)

    [http://arxiv.org/abs/2208.05424](http://arxiv.org/abs/2208.05424)

    本文提出了一种物理约束深度学习降尺度模型的方法，以保证模型在预测物理变量时满足守恒定律，并提高其性能。

    This paper proposes a method for physics-constrained deep learning downscaling models to ensure that the models satisfy conservation laws when predicting physical variables, while improving their performance according to traditional metrics.

    可靠的高分辨率气候和天气数据的可用性对于指导气候适应和减缓的长期决策以及指导对极端事件的快速响应至关重要。预测模型受计算成本限制，因此通常生成粗分辨率预测。统计降尺度，包括深度学习的超分辨率方法，可以提供一种有效的方法来上采样低分辨率数据。然而，尽管在某些情况下取得了视觉上令人信服的结果，但这些模型在预测物理变量时经常违反守恒定律。为了保持物理量的守恒，我们开发了一种方法，保证深度学习降尺度模型满足物理约束条件，同时根据传统指标提高其性能。我们比较了不同的约束方法，并展示了它们在不同的神经架构以及各种气候和天气数据上的适用性。

    The availability of reliable, high-resolution climate and weather data is important to inform long-term decisions on climate adaptation and mitigation and to guide rapid responses to extreme events. Forecasting models are limited by computational costs and, therefore, often generate coarse-resolution predictions. Statistical downscaling, including super-resolution methods from deep learning, can provide an efficient method of upsampling low-resolution data. However, despite achieving visually compelling results in some cases, such models frequently violate conservation laws when predicting physical variables. In order to conserve physical quantities, we develop methods that guarantee physical constraints are satisfied by a deep learning downscaling model while also improving their performance according to traditional metrics. We compare different constraining approaches and demonstrate their applicability across different neural architectures as well as a variety of climate and weather
    
[^168]: 离散度量的内在维度估计

    Intrinsic dimension estimation for discrete metrics. (arXiv:2207.09688v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.09688](http://arxiv.org/abs/2207.09688)

    本文介绍了一种算法，用于推断嵌入离散空间的数据集的内在维度（ID），并在物种指纹的代谢组学数据集上展示了其准确性，发现一个令人惊讶的小ID，约为2的数量级。

    This paper introduces an algorithm to estimate the intrinsic dimension (ID) of datasets embedded in discrete spaces, and demonstrates its accuracy on a metagenomic dataset for species fingerprinting, finding a surprisingly small ID of order 2, suggesting that evolutive pressure acts on a low-dimensional manifold despite the high-dimensionality of sequences' space.

    具有离散特征的真实世界数据集是无处不在的：从分类调查到临床问卷，从无权网络到DNA序列。然而，最常见的无监督降维方法是为连续空间设计的，它们在离散空间中的使用可能会导致错误和偏差。在本文中，我们介绍了一种算法，用于推断嵌入离散空间的数据集的内在维度（ID）。我们在基准数据集上展示了其准确性，并将其应用于分析用于物种指纹的代谢组学数据集，发现一个令人惊讶的小ID，约为2的数量级。这表明，尽管序列空间的高维度，进化压力仍然作用于低维流形上。

    Real world-datasets characterized by discrete features are ubiquitous: from categorical surveys to clinical questionnaires, from unweighted networks to DNA sequences. Nevertheless, the most common unsupervised dimensional reduction methods are designed for continuous spaces, and their use for discrete spaces can lead to errors and biases. In this letter we introduce an algorithm to infer the intrinsic dimension (ID) of datasets embedded in discrete spaces. We demonstrate its accuracy on benchmark datasets, and we apply it to analyze a metagenomic dataset for species fingerprinting, finding a surprisingly small ID, of order 2. This suggests that evolutive pressure acts on a low-dimensional manifold despite the high-dimensionality of sequences' space.
    
[^169]: XG-BoT：一种可解释的深度图神经网络用于僵尸网络检测和取证

    XG-BoT: An Explainable Deep Graph Neural Network for Botnet Detection and Forensics. (arXiv:2207.09088v5 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2207.09088](http://arxiv.org/abs/2207.09088)

    本文提出了一种名为XG-BoT的可解释的深度图神经网络模型，用于检测大规模网络中的恶意僵尸网络节点，并通过突出显示可疑的网络流和相关的僵尸网络节点来执行自动网络取证。该模型在关键评估指标方面优于现有的最先进方法。

    This paper proposes an explainable deep graph neural network model called XG-BoT for detecting malicious botnet nodes in large-scale networks and performing automatic network forensics by highlighting suspicious network flows and related botnet nodes. The model outperforms state-of-the-art approaches in terms of key evaluation metrics.

    本文提出了一种名为XG-BoT的可解释的深度图神经网络模型，用于检测僵尸网络节点。该模型包括一个僵尸网络检测器和一个自动取证的解释器。XG-BoT检测器可以有效地检测大规模网络中的恶意僵尸网络节点。具体而言，它利用分组可逆残差连接和图同构网络从僵尸网络通信图中学习表达性节点表示。基于GNNExplainer和XG-BoT中的显著性图，解释器可以通过突出显示可疑的网络流和相关的僵尸网络节点来执行自动网络取证。我们使用真实的大规模僵尸网络图数据集评估了XG-BoT。总体而言，XG-BoT在关键评估指标方面优于现有的最先进方法。此外，我们证明了XG-BoT解释器可以为自动网络取证生成有用的解释。

    In this paper, we propose XG-BoT, an explainable deep graph neural network model for botnet node detection. The proposed model comprises a botnet detector and an explainer for automatic forensics. The XG-BoT detector can effectively detect malicious botnet nodes in large-scale networks. Specifically, it utilizes a grouped reversible residual connection with a graph isomorphism network to learn expressive node representations from botnet communication graphs. The explainer, based on the GNNExplainer and saliency map in XG-BoT, can perform automatic network forensics by highlighting suspicious network flows and related botnet nodes. We evaluated XG-BoT using real-world, large-scale botnet network graph datasets. Overall, XG-BoT outperforms state-of-the-art approaches in terms of key evaluation metrics. Additionally, we demonstrate that the XG-BoT explainers can generate useful explanations for automatic network forensics.
    
[^170]: 基于联想记忆模型的真实世界数据分类和生成

    Classification and Generation of real-world data with an Associative Memory Model. (arXiv:2207.04827v3 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2207.04827](http://arxiv.org/abs/2207.04827)

    本文提出了一种基于联想记忆模型的多模态框架，可以以容错的方式存储和检索大量真实世界数据，并且可以用于推断缺失的模态。

    This paper proposes a multi-modality framework based on the associative memory model, which can store and retrieve a large amount of real-world data in a fault-tolerant manner, and can be used to infer missing modalities.

    回忆起多年未见的朋友的面孔是一项困难的任务。然而，如果你们偶然相遇，你们会轻易地认出彼此。生物记忆配备了一个令人印象深刻的压缩算法，可以存储必要的信息，然后推断细节以匹配感知。Willshaw Memory是一种用于皮层计算的简单抽象模型，实现了生物记忆的机制。使用我们最近提出的用于视觉模式的稀疏编码规则[34]，该模型可以以容错的方式存储和检索大量真实世界数据。在本文中，我们通过使用多模态框架扩展了基本联想记忆模型的能力。在这种设置中，记忆同时存储每个模式的几种模态（例如，视觉或文本）。训练后，当只感知到子集时，记忆可以用于推断缺失的模态。使用简单的编码器-记忆解码器，我们可以生成具有多个模态的数据。

    Drawing from memory the face of a friend you have not seen in years is a difficult task. However, if you happen to cross paths, you would easily recognize each other. The biological memory is equipped with an impressive compression algorithm that can store the essential, and then infer the details to match perception. The Willshaw Memory is a simple abstract model for cortical computations which implements mechanisms of biological memories. Using our recently proposed sparse coding prescription for visual patterns [34], this model can store and retrieve an impressive amount of real-world data in a fault-tolerant manner. In this paper, we extend the capabilities of the basic Associative Memory Model by using a Multiple-Modality framework. In this setting, the memory stores several modalities (e.g., visual, or textual) of each pattern simultaneously. After training, the memory can be used to infer missing modalities when just a subset is perceived. Using a simple encoder-memory decoder a
    
[^171]: 多频联合社区检测和相位同步

    Multi-Frequency Joint Community Detection and Phase Synchronization. (arXiv:2206.12276v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2206.12276](http://arxiv.org/abs/2206.12276)

    本文提出了两种简单而高效的算法，利用MLE公式并从多个频率的信息中受益，用于解决具有相对相位的随机块模型上的联合社区检测和相位同步问题。

    This paper proposes two simple and efficient algorithms that leverage the MLE formulation and benefit from the information across multiple frequencies to solve the joint community detection and phase synchronization problem on the stochastic block model with relative phase.

    本文研究了具有相对相位的随机块模型上的联合社区检测和相位同步问题，其中每个节点都与一个未知的相位角相关联。这个问题具有多种实际应用，旨在同时恢复簇结构和相关的相位角。我们通过仔细研究其最大似然估计（MLE）公式，展示了这个问题呈现出“多频”结构，而现有方法并非源于这个角度。为此，提出了两种简单而高效的算法，利用MLE公式并从多个频率的信息中受益。前者是基于新颖的多频列主元QR分解的谱方法。应用于观测矩阵的前几个特征向量的分解提供了有关簇结构和相关相位角的关键信息。第二种方法是迭代的多频率方法。

    This paper studies the joint community detection and phase synchronization problem on the stochastic block model with relative phase, where each node is associated with an unknown phase angle. This problem, with a variety of real-world applications, aims to recover the cluster structure and associated phase angles simultaneously. We show this problem exhibits a ``multi-frequency'' structure by closely examining its maximum likelihood estimation (MLE) formulation, whereas existing methods are not originated from this perspective. To this end, two simple yet efficient algorithms that leverage the MLE formulation and benefit from the information across multiple frequencies are proposed. The former is a spectral method based on the novel multi-frequency column-pivoted QR factorization. The factorization applied to the top eigenvectors of the observation matrix provides key information about the cluster structure and associated phase angles. The second approach is an iterative multi-frequen
    
[^172]: 最大间隔有效而大间隔无效：无需均匀收敛的泛化。 (arXiv:2206.07892v2 [cs.LG] UPDATED)

    Max-Margin Works while Large Margin Fails: Generalization without Uniform Convergence. (arXiv:2206.07892v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.07892](http://arxiv.org/abs/2206.07892)

    本文证明了在UC失败的情况下，最大间隔分类器可以实现几乎没有测试损失的泛化，提供了新的泛化界限。

    This paper proves that in cases where uniform convergence fails, the max-margin classifier can achieve almost no test loss in generalization, providing new generalization bounds.

    现代机器学习中的一个主要挑战是理论上理解过度参数化模型的泛化特性。许多现有工具依赖于均匀收敛（UC），当它成立时，保证测试损失在候选模型类上均匀接近于训练损失。Nagarajan和Kolter（2019）表明，在某些简单的线性和神经网络设置中，任何均匀收敛界限都将是无意义的，这引出了一个问题：如何在UC失败的情况下证明泛化。我们的主要贡献是在两个这样的设置中证明了新的泛化界限，一个是线性的，一个是非线性的。我们研究了Nagarajan和Kolter的线性分类设置，以及通过两层神经网络学习的二次基本事实函数在非线性区域。我们证明了一种新型的边界，表明在一定的信噪比阈值以上，任何接近最大间隔分类器都将在测试损失上几乎没有损失。

    A major challenge in modern machine learning is theoretically understanding the generalization properties of overparameterized models. Many existing tools rely on uniform convergence (UC), a property that, when it holds, guarantees that the test loss will be close to the training loss, uniformly over a class of candidate models. Nagarajan and Kolter (2019) show that in certain simple linear and neural-network settings, any uniform convergence bound will be vacuous, leaving open the question of how to prove generalization in settings where UC fails. Our main contribution is proving novel generalization bounds in two such settings, one linear, and one non-linear. We study the linear classification setting of Nagarajan and Kolter, and a quadratic ground truth function learned via a two-layer neural network in the non-linear regime. We prove a new type of margin bound showing that above a certain signal-to-noise threshold, any near-max-margin classifier will achieve almost no test loss in 
    
[^173]: 梯度提升执行高斯过程推断

    Gradient Boosting Performs Gaussian Process Inference. (arXiv:2206.05608v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.05608](http://arxiv.org/abs/2206.05608)

    本文表明，基于对称决策树的梯度提升可以等价地重构为一种核方法，该方法收敛于某个核岭回归问题的解，从而使我们能够轻松地将梯度提升转换为从后验中提供更好的知识不确定性估计的采样器，通过后验方差的蒙特卡罗估计，从而允许更好的知识不确定性估计，导致改进的域外检测。

    This paper shows that gradient boosting based on symmetric decision trees can be equivalently reformulated as a kernel method that converges to the solution of a certain Kernel Ridge Regression problem, which allows us to easily transform gradient boosting into a sampler from the posterior to provide better knowledge uncertainty estimates through Monte-Carlo estimation of the posterior variance, leading to improved out-of-domain detection.

    本文表明，基于对称决策树的梯度提升可以等价地重构为一种核方法，该方法收敛于某个核岭回归问题的解。因此，我们获得了收敛于高斯过程后验均值的收敛性，从而使我们能够轻松地将梯度提升转换为从后验中提供更好的知识不确定性估计的采样器，通过后验方差的蒙特卡罗估计。我们展示了所提出的采样器允许更好的知识不确定性估计，从而导致改进的域外检测。

    This paper shows that gradient boosting based on symmetric decision trees can be equivalently reformulated as a kernel method that converges to the solution of a certain Kernel Ridge Regression problem. Thus, we obtain the convergence to a Gaussian Process' posterior mean, which, in turn, allows us to easily transform gradient boosting into a sampler from the posterior to provide better knowledge uncertainty estimates through Monte-Carlo estimation of the posterior variance. We show that the proposed sampler allows for better knowledge uncertainty estimates leading to improved out-of-domain detection.
    
[^174]: 基于解剖感知对比蒸馏的半监督医学图像分割引导启动

    Bootstrapping Semi-supervised Medical Image Segmentation with Anatomical-aware Contrastive Distillation. (arXiv:2206.02307v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.02307](http://arxiv.org/abs/2206.02307)

    本文提出了一种基于解剖感知对比蒸馏的半监督医学图像分割引导启动方法，通过软标记负样本和捕获更多语义上相似的特征来解决医学图像数据不平衡的问题。

    This paper proposes a semi-supervised medical image segmentation bootstrapping method based on anatomical-aware contrastive distillation, which solves the problem of imbalanced medical image data by softly labeling negative samples and capturing more semantically similar features.

    对比学习已经在医学图像分割的注释稀缺问题上显示出了巨大的潜力。现有的方法通常假设标记和未标记的医学图像具有平衡的类分布。然而，现实中的医学图像数据通常是不平衡的（即多类标签不平衡），这自然地产生模糊的轮廓并通常错误地标记罕见的对象。此外，所有负样本是否同样负面仍不清楚。在这项工作中，我们提出了ACTION，一种解剖感知对比蒸馏框架，用于半监督医学图像分割。具体而言，我们首先通过软标记负样本而不是正负对之间的二元监督来开发迭代对比蒸馏算法。与正样本相比，我们还从随机选择的负样本集中捕获更多语义上相似的特征，以强制执行采样数据的多样性。其次，我们提出了一种基于解剖感知的启动方法，以更好地利用有限的标记数据。

    Contrastive learning has shown great promise over annotation scarcity problems in the context of medical image segmentation. Existing approaches typically assume a balanced class distribution for both labeled and unlabeled medical images. However, medical image data in reality is commonly imbalanced (i.e., multi-class label imbalance), which naturally yields blurry contours and usually incorrectly labels rare objects. Moreover, it remains unclear whether all negative samples are equally negative. In this work, we present ACTION, an Anatomical-aware ConTrastive dIstillatiON framework, for semi-supervised medical image segmentation. Specifically, we first develop an iterative contrastive distillation algorithm by softly labeling the negatives rather than binary supervision between positive and negative pairs. We also capture more semantically similar features from the randomly chosen negative set compared to the positives to enforce the diversity of the sampled data. Second, we raise a m
    
[^175]: 具备辅助信息的优化

    Optimization with access to auxiliary information. (arXiv:2206.00395v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.00395](http://arxiv.org/abs/2206.00395)

    本文研究了在计算目标函数梯度很昂贵或有限的情况下，给定一些辅助函数的情况下，如何最小化目标函数。作者提出了两种通用的新算法，并证明了这个框架可以受益于目标和辅助信息之间的Hessian相似性假设。

    This paper investigates the fundamental optimization question of minimizing a target function with expensive or limited gradient computation, given access to some auxiliary side function with cheaper or more available gradients. The authors propose two generic new algorithms and prove that this framework can benefit from the Hessian similarity assumption between the target and side information.

    我们研究了基本的优化问题，即在计算目标函数$f(x)$的梯度很昂贵或有限的情况下，给定一些辅助函数$h(x)$的情况下，如何最小化目标函数。这个公式涵盖了许多实际相关的设置，如i）在SGD中重复使用批次，ii）迁移学习，iii）联邦学习，iv）使用压缩模型/丢弃等进行训练。我们提出了两种通用的新算法，适用于所有这些设置，并证明仅使用目标和辅助信息之间的Hessian相似性假设，我们可以从这个框架中受益。

    We investigate the fundamental optimization question of minimizing a target function $f(x)$ whose gradients are expensive to compute or have limited availability, given access to some auxiliary side function $h(x)$ whose gradients are cheap or more available. This formulation captures many settings of practical relevance such as i) re-using batches in SGD, ii) transfer learning, iii) federated learning, iv) training with compressed models/dropout, etc. We propose two generic new algorithms which are applicable in all these settings and prove using only an assumption on the Hessian similarity between the target and side information that we can benefit from this framework.
    
[^176]: DM$^2$: 基于分布匹配的去中心化多智能体强化学习

    DM$^2$: Decentralized Multi-Agent Reinforcement Learning for Distribution Matching. (arXiv:2206.00233v3 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2206.00233](http://arxiv.org/abs/2206.00233)

    本文提出了一种基于分布匹配的去中心化多智能体强化学习方法，每个智能体独立地最小化与目标访问分布的相应分量的分布不匹配，可以实现收敛到生成目标分布的联合策略。

    This paper proposes a decentralized multi-agent reinforcement learning method based on distribution matching, where each agent independently minimizes the distribution mismatch to the corresponding component of a target visitation distribution, achieving convergence to the joint policy that generated the target distribution.

    当前的多智能体协作方法往往依赖于集中式机制或显式通信协议以确保收敛。本文研究了分布匹配在不依赖于集中式组件或显式通信的分布式多智能体学习中的应用。在所提出的方案中，每个智能体独立地最小化与目标访问分布的相应分量的分布不匹配。理论分析表明，在某些条件下，每个智能体最小化其个体分布不匹配可以实现收敛到生成目标分布的联合策略。此外，如果目标分布来自优化合作任务的联合策略，则该任务奖励和分布匹配奖励的组合的最优策略是相同的联合策略。这一见解被用来制定一个实用的算法。

    Current approaches to multi-agent cooperation rely heavily on centralized mechanisms or explicit communication protocols to ensure convergence. This paper studies the problem of distributed multi-agent learning without resorting to centralized components or explicit communication. It examines the use of distribution matching to facilitate the coordination of independent agents. In the proposed scheme, each agent independently minimizes the distribution mismatch to the corresponding component of a target visitation distribution. The theoretical analysis shows that under certain conditions, each agent minimizing its individual distribution mismatch allows the convergence to the joint policy that generated the target distribution. Further, if the target distribution is from a joint policy that optimizes a cooperative task, the optimal policy for a combination of this task reward and the distribution matching reward is the same joint policy. This insight is used to formulate a practical al
    
[^177]: 基于指数窗口的最大均值差异在线变化检测

    Maximum Mean Discrepancy on Exponential Windows for Online Change Detection. (arXiv:2205.12706v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.12706](http://arxiv.org/abs/2205.12706)

    本文提出了一种基于指数窗口的最大均值差异在线变化检测算法，能够有效地检测数据流中的变化。

    This paper proposes a Maximum Mean Discrepancy on Exponential Windows (MMDEW) algorithm for online change detection, which efficiently detects changes in data streams.

    在分析数据流时，检测变化是非常重要的，具有许多应用，例如预测性维护、欺诈检测或医学。一种检测变化的原则方法是通过假设检验将流中观测值的分布相互比较。最大均值差异（MMD；也称为能量距离）是概率分布空间上众所周知的（半）度量。在温和条件下，MMD在核富集域上产生了强大的非参数两样本检验，这使得它在变化检测中的应用变得可取。然而，经典的MMD估计器具有二次复杂度，这禁止了它们在在线变化检测设置中的应用。我们提出了一种通用的变化检测算法，基于指数窗口的最大均值差异（MMDEW），它利用MMD两样本检验，在任何核富集域上促进其有效的在线计算，并能够检测到变化。

    Detecting changes is of fundamental importance when analyzing data streams and has many applications, e.g., predictive maintenance, fraud detection, or medicine. A principled approach to detect changes is to compare the distributions of observations within the stream to each other via hypothesis testing. Maximum mean discrepancy (MMD; also called energy distance) is a well-known (semi-)metric on the space of probability distributions. MMD gives rise to powerful non-parametric two-sample tests on kernel-enriched domains under mild conditions, which makes its deployment for change detection desirable. However, the classic MMD estimators suffer quadratic complexity, which prohibits their application in the online change detection setting. We propose a general-purpose change detection algorithm, Maximum Mean Discrepancy on Exponential Windows (MMDEW), which leverages the MMD two-sample test, facilitates its efficient online computation on any kernel-enriched domain, and is able to detect a
    
[^178]: 对抗随机森林用于密度估计和生成建模

    Adversarial random forests for density estimation and generative modeling. (arXiv:2205.09435v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.09435](http://arxiv.org/abs/2205.09435)

    本文提出了一种使用对抗随机森林进行密度估计和数据合成的方法，该方法可以提供平滑的（非）条件密度，并允许完全合成数据生成，同时在各种表格数据基准测试中实现了与最先进的概率电路和深度学习模型相当或更好的性能，平均执行速度快了两个数量级。

    This paper proposes a method for density estimation and data synthesis using adversarial random forests, which provides smooth (un)conditional densities and allows for fully synthetic data generation. The method achieves comparable or superior performance to state-of-the-art probabilistic circuits and deep learning models on various tabular data benchmarks while executing about two orders of magnitude faster on average.

    我们提出了一种使用新型无监督随机森林进行密度估计和数据合成的方法。受生成对抗网络的启发，我们实现了一种递归过程，其中树通过交替的生成和判别轮次逐渐学习数据的结构特性。该方法在最小假设下可以被证明是一致的。与经典的基于树的替代方法不同，我们的方法提供平滑的（非）条件密度，并允许完全合成数据生成。我们在各种表格数据基准测试中实现了与最先进的概率电路和深度学习模型相当或更好的性能，同时平均执行速度快了两个数量级。附带的R包arf可在CRAN上获得。

    We propose methods for density estimation and data synthesis using a novel form of unsupervised random forests. Inspired by generative adversarial networks, we implement a recursive procedure in which trees gradually learn structural properties of the data through alternating rounds of generation and discrimination. The method is provably consistent under minimal assumptions. Unlike classic tree-based alternatives, our approach provides smooth (un)conditional densities and allows for fully synthetic data generation. We achieve comparable or superior performance to state-of-the-art probabilistic circuits and deep learning models on various tabular data benchmarks while executing about two orders of magnitude faster on average. An accompanying $\texttt{R}$ package, $\texttt{arf}$, is available on $\texttt{CRAN}$.
    
[^179]: 使用物体感知表示在多物体场景中进行视觉运动控制

    Visuomotor Control in Multi-Object Scenes Using Object-Aware Representations. (arXiv:2205.06333v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2205.06333](http://arxiv.org/abs/2205.06333)

    本文探讨了使用物体感知表示学习技术进行机器人任务的有效性，以解决当前方法学习任务特定表示不能很好地转移到其他任务的问题，以及由监督方法学习的表示需要大量标记数据集的问题。

    This paper explores the effectiveness of using object-aware representation learning techniques for robotic tasks, to address the problem that current methodologies learn task specific representations that do not necessarily transfer well to other tasks, and that representations learned by supervised methods require large labeled datasets for each task that are expensive to collect in the real world.

    场景的感知理解以及其不同组件之间的关系对于成功完成机器人任务至关重要。表示学习已被证明是一种强大的技术，但大多数当前的方法学习任务特定的表示，不一定能够很好地转移到其他任务。此外，由监督方法学习的表示需要大量标记数据集，这在现实世界中收集起来很昂贵。使用自监督学习从未标记的数据中获取表示可以缓解这个问题。然而，当前的自监督表示学习方法大多是物体无关的，我们证明了由此得到的表示对于具有许多组件的场景的通用机器人任务是不足够的。在本文中，我们探讨了使用物体感知表示学习技术进行机器人任务的有效性。

    Perceptual understanding of the scene and the relationship between its different components is important for successful completion of robotic tasks. Representation learning has been shown to be a powerful technique for this, but most of the current methodologies learn task specific representations that do not necessarily transfer well to other tasks. Furthermore, representations learned by supervised methods require large labeled datasets for each task that are expensive to collect in the real world. Using self-supervised learning to obtain representations from unlabeled data can mitigate this problem. However, current self-supervised representation learning methods are mostly object agnostic, and we demonstrate that the resulting representations are insufficient for general purpose robotics tasks as they fail to capture the complexity of scenes with many components. In this paper, we explore the effectiveness of using object-aware representation learning techniques for robotic tasks. 
    
[^180]: Vine copula结构的矩阵和图形表示

    Matrix and graph representations of vine copula structures. (arXiv:2205.04783v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.04783](http://arxiv.org/abs/2205.04783)

    本文研究了Vine copula结构的矩阵和图形表示，证明了它们之间的等价性，并提出了一种新的方法来构建矩阵。这些算法的运行时间也被计算了。

    

    Vine copula可以有效地建模多元概率分布。本文重点研究它们的结构，因为在文献中，vine copula的表示经常是模糊的。图形表示包括原始的、cherry和chordal图形序列结构，我们证明了它们之间的等价性。重要的是，我们还展示了一个新的结果，即当给出vine结构的完美消除排序时，它总是可以用矩阵唯一表示。O. M. N\'apoles已经展示了一种在矩阵中表示vine的方法，我们对这种先前的方法进行了算法化，同时还展示了一种通过cherry树序列构建这种矩阵的新方法。我们还计算了这些算法的运行时间。最后，我们证明了这两种矩阵构建算法在使用相同的完美消除排序时是等价的。

    Vine copulas can efficiently model multivariate probability distributions. This paper focuses on a more thorough understanding of their structures, since in the literature, vine copula representations are often ambiguous. The graph representations include the original, cherry and chordal graph sequence structures, which we show equivalence between. Importantly we also show a new result, namely that when a perfect elimination ordering of a vine structure is given, then it can always be uniquely represented with a matrix. O. M. N\'apoles has shown a way to represent vines in a matrix, and we algorithmify this previous approach, while also showing a new method for constructing such a matrix, through cherry tree sequences. We also calculate the runtime of these algorithms. Lastly, we prove that these two matrix-building algorithms are equivalent if the same perfect elimination ordering is being used.
    
[^181]: 非平稳赌博机学习的预测抽样方法

    Non-Stationary Bandit Learning via Predictive Sampling. (arXiv:2205.01970v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.01970](http://arxiv.org/abs/2205.01970)

    本文提出了一种预测抽样算法，用于解决非平稳赌博机学习问题。该算法通过降低获取信息的优先级，解决了Thompson抽样在非平稳环境下表现不佳的问题，并在所有非平稳环境中优于Thompson抽样。

    This paper proposes a predictive sampling algorithm to solve the non-stationary bandit learning problem. By deprioritizing the acquisition of information that quickly loses usefulness, the algorithm outperforms Thompson sampling in all non-stationary environments examined.

    Thompson抽样已经在广泛的平稳赌博机环境中证明了其有效性。然而，正如我们在本文中所展示的，当应用于非平稳环境时，它的表现可能很差。我们表明，这样的失败是由于在探索时，算法没有根据由于非平稳性导致信息快速失去有用性的速度区分行动。基于这一洞见，我们提出了预测抽样算法，该算法降低了获取信息的优先级，这些信息由于快速失去有用性而不再重要。通过贝叶斯遗憾界，我们建立了预测抽样性能的理论保证。我们提供了预测抽样的版本，其计算可扩展到实际感兴趣的复杂赌博机环境。通过数值模拟，我们证明了预测抽样在所有非平稳环境中都优于Thompson抽样。

    Thompson sampling has proven effective across a wide range of stationary bandit environments. However, as we demonstrate in this paper, it can perform poorly when applied to non-stationary environments. We show that such failures are attributed to the fact that, when exploring, the algorithm does not differentiate actions based on how quickly the information acquired loses its usefulness due to non-stationarity. Building upon this insight, we propose predictive sampling, an algorithm that deprioritizes acquiring information that quickly loses usefulness. Theoretical guarantee on the performance of predictive sampling is established through a Bayesian regret bound. We provide versions of predictive sampling for which computations tractably scale to complex bandit environments of practical interest. Through numerical simulations, we demonstrate that predictive sampling outperforms Thompson sampling in all non-stationary environments examined.
    
[^182]: 低压负荷伯恩斯坦多项式归一化流的短期密度预测

    Short-Term Density Forecasting of Low-Voltage Load using Bernstein-Polynomial Normalizing Flows. (arXiv:2204.13939v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.13939](http://arxiv.org/abs/2204.13939)

    本文提出了一种基于伯恩斯坦多项式归一化流的灵活条件密度预测方法，用于短期低压负荷预测，相比传统方法表现更好，可用于规划和运营低碳能源系统。

    This paper proposes a flexible conditional density forecasting method based on Bernstein polynomial normalizing flows for short-term low-voltage load forecasting, which outperforms traditional methods and can be used for planning and operating low-carbon energy systems.

    实现全面可再生能源电网的转型需要更好地预测低压水平的需求，以提高效率并确保可靠的控制。然而，高波动性和不断增加的电气化导致巨大的预测变异性，这在传统的点估计中没有反映出来。概率负载预测考虑未来的不确定性，因此允许更明智的决策，用于规划和运营低碳能源系统。我们提出了一种基于伯恩斯坦多项式归一化流的灵活条件密度预测方法，其中神经网络控制流的参数。在一项包括363个智能电表客户的实证研究中，我们的密度预测与高斯和高斯混合密度相比表现出优势。此外，对于两种不同的神经网络架构，它们在24小时前的负载预测中优于基于针球损失的非参数方法。

    The transition to a fully renewable energy grid requires better forecasting of demand at the low-voltage level to increase efficiency and ensure reliable control. However, high fluctuations and increasing electrification cause huge forecast variability, not reflected in traditional point estimates. Probabilistic load forecasts take future uncertainties into account and thus allow more informed decision-making for the planning and operation of low-carbon energy systems. We propose an approach for flexible conditional density forecasting of short-term load based on Bernstein polynomial normalizing flows, where a neural network controls the parameters of the flow. In an empirical study with 363 smart meter customers, our density predictions compare favorably against Gaussian and Gaussian mixture densities. Also, they outperform a non-parametric approach based on the pinball loss for 24h-ahead load forecasting for two different neural network architectures.
    
[^183]: 通过检测错误的位置嵌入进行表示学习

    Representation Learning by Detecting Incorrect Location Embeddings. (arXiv:2204.04788v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2204.04788](http://arxiv.org/abs/2204.04788)

    本文提出了一种新的自监督学习（SSL）损失，用于图像表示学习。通过检测错误的位置嵌入，我们可以提高深度神经网络的泛化能力，使其更加鲁棒。我们称这种方法为DILEMMA，将其应用于MoCoV3、DINO和SimCLR，分别显示它们的性能提高了4.41%、3.97%和0.5%。

    

    本文提出了一种新的自监督学习（SSL）损失，用于图像表示学习。我们认为深度神经网络的泛化能力与其区分对象形状的能力有关。由于对象形状与其部件的位置有关，因此我们提出检测那些被人为移位的部件。我们用图像令牌表示对象部件，并训练ViT检测哪个令牌与错误的位置嵌入组合。然后，我们引入输入的稀疏性，使模型更加鲁棒，以应对遮挡并加速训练。我们称这种方法为DILEMMA，即检测错误位置嵌入和掩蔽输入。我们将DILEMMA应用于MoCoV3、DINO和SimCLR，并在相同的训练时间内，在ImageNet-1K上进行线性探测转移，分别显示它们的性能提高了4.41%、3.97%和0.5%。我们还展示了MAE与我们的完全微调改进的结果。

    In this paper, we introduce a novel self-supervised learning (SSL) loss for image representation learning. There is a growing belief that generalization in deep neural networks is linked to their ability to discriminate object shapes. Since object shape is related to the location of its parts, we propose to detect those that have been artificially misplaced. We represent object parts with image tokens and train a ViT to detect which token has been combined with an incorrect positional embedding. We then introduce sparsity in the inputs to make the model more robust to occlusions and to speed up the training. We call our method DILEMMA, which stands for Detection of Incorrect Location EMbeddings with MAsked inputs. We apply DILEMMA to MoCoV3, DINO and SimCLR and show an improvement in their performance of respectively 4.41%, 3.97%, and 0.5% under the same training time and with a linear probing transfer on ImageNet-1K. We also show full fine-tuning improvements of MAE combined with our 
    
[^184]: DynLight: 多级交通信号控制实现动态相位时长

    DynLight: Realize dynamic phase duration with multi-level traffic signal control. (arXiv:2204.03471v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2204.03471](http://arxiv.org/abs/2204.03471)

    本文已被撤回，原因是语言和理论描述不够令人满意，作者已经进行了修订和更新。

    The article has been withdrawn due to unsatisfactory language and theoretical description, and the authors have revised and updated it.

    我们因以下原因撤回本文：1.本文的语言和理论描述不够令人满意；2.我们在其他作者的帮助下丰富和修订了本文；3.我们必须更新作者贡献信息。

    We would like to withdraw this article for the following reasons: 1 this article is not satisfactory for limited language and theoretical description; 2 we have enriched and revised this article with the help of other authors; 3 we must update the author contribution information.
    
[^185]: 深度特征筛选：通过深度神经网络进行超高维数据的特征选择

    Deep Feature Screening: Feature Selection for Ultra High-Dimensional Data via Deep Neural Networks. (arXiv:2204.01682v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2204.01682](http://arxiv.org/abs/2204.01682)

    本文提出了一种新的两步非参数方法，称为深度特征筛选（DeepFS），可以克服高维、低样本数据上的困难和挑战，并对超高维、低样本数据进行高精度的特征筛选。

    This paper proposes a novel two-step nonparametric approach called Deep Feature Screening (DeepFS) that can overcome the challenges of high-dimensional, low-sample-size data and identify significant features with high precision for ultra high-dimensional, low-sample-size data.

    传统的统计特征选择方法在高维、低样本数据上的应用经常遇到困难和挑战，如过拟合、维数灾难、计算不可行和强模型假设。本文提出了一种新的两步非参数方法，称为深度特征筛选（DeepFS），可以克服这些问题，并对超高维、低样本数据进行高精度的特征筛选。该方法首先提取输入数据的低维表示，然后应用基于Deb和Sen（2021）最近开发的多元秩距相关性的特征筛选。该方法结合了深度神经网络和特征筛选的优点，除了处理具有少量样本的超高维数据的能力外，还具有以下吸引人的特点：（1）它是模型自由和分布自由的；（2）它可以

    The applications of traditional statistical feature selection methods to high-dimension, low sample-size data often struggle and encounter challenging problems, such as overfitting, curse of dimensionality, computational infeasibility, and strong model assumption. In this paper, we propose a novel two-step nonparametric approach called Deep Feature Screening (DeepFS) that can overcome these problems and identify significant features with high precision for ultra high-dimensional, low-sample-size data. This approach first extracts a low-dimensional representation of input data and then applies feature screening based on multivariate rank distance correlation recently developed by Deb and Sen (2021). This approach combines the strengths of both deep neural networks and feature screening, and thereby has the following appealing features in addition to its ability of handling ultra high-dimensional data with small number of samples: (1) it is model free and distribution free; (2) it can be
    
[^186]: 分离征服启发式算法允许在分类、回归和生存数据中进行强大的对比集挖掘

    Separate and conquer heuristic allows robust mining of contrast sets in classification, regression, and survival data. (arXiv:2204.00497v3 [cs.DB] UPDATED)

    [http://arxiv.org/abs/2204.00497](http://arxiv.org/abs/2204.00497)

    本文提出了一种基于分离征服的对比集挖掘算法RuleKit-CS，该算法通过多次通过伴随属性惩罚方案提供描述具有不同属性的相同示例的对比集，区别于标准的分离征服。该算法还被推广到回归和生存数据，允许识别标签属性/生存预测与预定义对比组的标签/预测一致的对比集。

    This paper proposes a contrast set mining algorithm, RuleKit-CS, based on the separate and conquer heuristic, which provides contrast sets describing the same examples with different attributes through multiple passes accompanied with an attribute penalization scheme. The algorithm is also generalized for regression and survival data, allowing identification of contrast sets whose label attribute/survival prognosis is consistent with the label/prognosis for the predefined contrast groups.

    识别群体之间的差异是最重要的知识发现问题之一。该过程，也称为对比集挖掘，在医学、工业或经济等广泛领域中应用。在本文中，我们提出了RuleKit-CS，一种基于分离征服的对比集挖掘算法——一种用于决策规则归纳的成熟启发式算法。多次通过伴随属性惩罚方案提供描述具有不同属性的相同示例的对比集，区别于标准的分离征服。该算法还被推广到回归和生存数据，允许识别标签属性/生存预测与预定义对比组的标签/预测一致的对比集。这个特性，不是现有方法所提供的，进一步扩展了RuleKit-CS的可用性。在来自各个领域的130多个数据集上进行的实验和详细分析。

    Identifying differences between groups is one of the most important knowledge discovery problems. The procedure, also known as contrast sets mining, is applied in a wide range of areas like medicine, industry, or economics.  In the paper we present RuleKit-CS, an algorithm for contrast set mining based on separate and conquer - a well established heuristic for decision rule induction. Multiple passes accompanied with an attribute penalization scheme provide contrast sets describing same examples with different attributes, distinguishing presented approach from the standard separate and conquer. The algorithm was also generalized for regression and survival data allowing identification of contrast sets whose label attribute/survival prognosis is consistent with the label/prognosis for the predefined contrast groups. This feature, not provided by the existing approaches, further extends the usability of RuleKit-CS.  Experiments on over 130 data sets from various areas and detailed analys
    
[^187]: 生成建模有助于弱监督（反之亦然）

    Generative Modeling Helps Weak Supervision (and Vice Versa). (arXiv:2203.12023v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.12023](http://arxiv.org/abs/2203.12023)

    本文提出了一种融合程序弱监督和生成对抗网络的模型，通过对齐离散潜在变量和弱监督派生的标签估计，改善了未观察到的标签的估计，实现了数据增强。

    This paper proposes a model that fuses programmatic weak supervision and generative adversarial networks, improving the estimate of unobserved labels by aligning discrete latent variables and weak supervision derived label estimate, and enabling data augmentation through weak supervision.

    许多有前途的监督式机器学习应用在获取足够数量和质量的标记数据方面面临困难，从而造成昂贵的瓶颈。为了克服这些限制，研究了不依赖于基本真实标签的技术，包括弱监督和生成建模。虽然这些技术似乎可以共同使用，相互改进，但如何在它们之间建立接口尚不为人所知。在这项工作中，我们提出了一种融合程序弱监督和生成对抗网络的模型，并提供了理论上的理由来支持这种融合。所提出的方法捕捉数据中的离散潜在变量以及弱监督派生的标签估计。两者的对齐允许更好地建模弱监督来源的样本相关准确性，从而改善未观察到的标签的估计。这是第一种通过弱监督实现数据增强的方法。

    Many promising applications of supervised machine learning face hurdles in the acquisition of labeled data in sufficient quantity and quality, creating an expensive bottleneck. To overcome such limitations, techniques that do not depend on ground truth labels have been studied, including weak supervision and generative modeling. While these techniques would seem to be usable in concert, improving one another, how to build an interface between them is not well-understood. In this work, we propose a model fusing programmatic weak supervision and generative adversarial networks and provide theoretical justification motivating this fusion. The proposed approach captures discrete latent variables in the data alongside the weak supervision derived label estimate. Alignment of the two allows for better modeling of sample-dependent accuracies of the weak supervision sources, improving the estimate of unobserved labels. It is the first approach to enable data augmentation through weakly supervi
    
[^188]: 癫痫样活动对危重病患者出院结局的影响

    Effects of Epileptiform Activity on Discharge Outcome in Critically Ill Patients. (arXiv:2203.04920v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2203.04920](http://arxiv.org/abs/2203.04920)

    本研究旨在探讨癫痫样活动对危重病患者出院结局的影响，通过回顾性横断面研究发现，如果每个人都经历了某种EA负荷并且未接受治疗，出院mRS会发生变化。

    This study aims to explore the effects of epileptiform activity on discharge outcomes in critically ill patients. Through a retrospective cross-sectional study, it was found that the discharge mRS would change if everyone had experienced a certain EA burden and were untreated.

    癫痫样活动（EA）与更差的结局相关，包括增加残疾和死亡的风险。然而，EA对神经系统结局的影响受到抗癫痫药物（ASM）治疗和EA负荷之间的反馈的干扰。由于EA-ASM反馈的顺序性以及伦理原因，随机临床试验具有挑战性。然而，一些机制知识是可用的，例如药物的吸收方式。这些知识与观察数据结合起来，可以使用因果推断提供更准确的效应估计。我们进行了一项回顾性横断面研究，共有995名患者，以出院时的修正Rankin量表（mRS）为结果，以在第一次脑电图的前24小时内每个六小时窗口中EA负荷的平均或最大比例为暴露。我们估计了如果数据集中的每个人都经历了某种EA负荷并且未接受治疗，出院mRS的变化。

    Epileptiform activity (EA) is associated with worse outcomes including increased risk of disability and death. However, the effect of EA on the neurologic outcome is confounded by the feedback between treatment with anti-seizure medications (ASM) and EA burden. A randomized clinical trial is challenging due to the sequential nature of EA-ASM feedback, as well as ethical reasons. However, some mechanistic knowledge is available, e.g., how drugs are absorbed. This knowledge together with observational data could provide a more accurate effect estimate using causal inference. We performed a retrospective cross-sectional study with 995 patients with the modified Rankin Scale (mRS) at discharge as the outcome and the EA burden defined as the mean or maximum proportion of time spent with EA in six-hour windows in the first 24 hours of electroencephalography as the exposure. We estimated the change in discharge mRS if everyone in the dataset had experienced a certain EA burden and were untrea
    
[^189]: 面向异构遥感图像的目标变化检测，用于森林死亡率映射

    Towards Targeted Change Detection with Heterogeneous Remote Sensing Images for Forest Mortality Mapping. (arXiv:2203.00049v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.00049](http://arxiv.org/abs/2203.00049)

    本文提出了一种基于图像到图像转换和单类分类的新方法，用于检测生态系统某些干扰的弱信号，以绘制由几何蛾爆发引起的森林死亡率在稀疏森林-苔原生态过渡带中的地图。

    

    最近已经开发了几种用于异构遥感数据（例如合成孔径雷达（SAR）和多光谱辐射计）的变化检测的通用方法。然而，这些方法不适合检测生态系统某些干扰的弱信号。为了解决这个问题，我们提出了一种基于图像到图像转换和单类分类（OCC）的新方法。我们旨在使用多源卫星图像绘制由几何蛾爆发引起的森林死亡率在稀疏森林-苔原生态过渡带中的地图。事件前和事件后的图像分别由Landsat-5和RADARSAT-2收集。使用最近的深度学习方法进行变化感知图像转换，我们在两个卫星各自的域中计算差异图像。这些差异与原始的事件前和事件后的图像堆叠，并传递给在目标变化类的小样本上训练的OCC。分类器产生一个

    Several generic methods have recently been developed for change detection in heterogeneous remote sensing data, such as images from synthetic aperture radar (SAR) and multispectral radiometers. However, these are not well suited to detect weak signatures of certain disturbances of ecological systems. To resolve this problem we propose a new approach based on image-to-image translation and one-class classification (OCC). We aim to map forest mortality caused by an outbreak of geometrid moths in a sparsely forested forest-tundra ecotone using multisource satellite images. The images preceding and following the event are collected by Landsat-5 and RADARSAT-2, respectively. Using a recent deep learning method for change-aware image translation, we compute difference images in both satellites' respective domains. These differences are stacked with the original pre- and post-event images and passed to an OCC trained on a small sample from the targeted change class. The classifier produces a 
    
[^190]: 随机拉普拉斯特征用于超几何空间学习

    Random Laplacian Features for Learning with Hyperbolic Space. (arXiv:2202.06854v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.06854](http://arxiv.org/abs/2202.06854)

    本文提出了一种简单的方法，通过学习输入的超几何嵌入，使用一种编码几何先验的映射将其映射到欧几里得空间，并最终使用标准欧几里得网络。关键的洞见是使用拉普拉斯算子的特征函数的随机特征映射，我们展示了它可以近似超几何空间上的任何等度量不变核。

    This paper proposes a simple approach to learning hyperbolic embeddings by mapping them to Euclidean space using a mapping that encodes geometric priors and then using a standard Euclidean network. The key insight is to use a random feature mapping via the eigenfunctions of the Laplace operator, which can approximate any isometry-invariant kernel on hyperbolic space.

    由于其几何特性，超几何空间可以支持树形和图形结构数据的高保真嵌入，基于此，各种超几何网络已经被开发出来。现有的超几何网络不仅对输入进行几何先验编码，而且在网络的每一层都进行编码。这种方法涉及到反复映射到和从超几何空间，使得这些网络难以实现，计算成本高，训练数值不稳定。在本文中，我们提出了一种更简单的方法：学习输入的超几何嵌入，然后使用一种编码几何先验的映射将其映射到欧几里得空间，并最终使用标准欧几里得网络。关键的洞见是使用拉普拉斯算子的特征函数的随机特征映射，我们展示了它可以近似超几何空间上的任何等度量不变核。我们的方法可以与任何图形神经网络一起使用。

    Due to its geometric properties, hyperbolic space can support high-fidelity embeddings of tree- and graph-structured data, upon which various hyperbolic networks have been developed. Existing hyperbolic networks encode geometric priors not only for the input, but also at every layer of the network. This approach involves repeatedly mapping to and from hyperbolic space, which makes these networks complicated to implement, computationally expensive to scale, and numerically unstable to train. In this paper, we propose a simpler approach: learn a hyperbolic embedding of the input, then map once from it to Euclidean space using a mapping that encodes geometric priors by respecting the isometries of hyperbolic space, and finish with a standard Euclidean network. The key insight is to use a random feature mapping via the eigenfunctions of the Laplace operator, which we show can approximate any isometry-invariant kernel on hyperbolic space. Our method can be used together with any graph neura
    
[^191]: PGMax: 用于离散概率图模型和JAX中的循环置信传播的因子图

    PGMax: Factor Graphs for Discrete Probabilistic Graphical Models and Loopy Belief Propagation in JAX. (arXiv:2202.04110v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.04110](http://arxiv.org/abs/2202.04110)

    PGMax是一个用于离散概率图模型的因子图工具，可以在JAX中自动运行高效且可扩展的循环置信传播，与现有替代方案相比，PGMax获得了更高质量的推理结果，推理时间加速高达三个数量级。

    PGMax is a factor graph tool for discrete probabilistic graphical models that automatically runs efficient and scalable loopy belief propagation in JAX. Compared with existing alternatives, PGMax obtains higher-quality inference results with up to three orders-of-magnitude inference time speedups.

    PGMax是一个开源的Python包，用于轻松指定离散概率图模型（PGMs）作为因子图，并在JAX中自动运行高效且可扩展的循环置信传播（LBP）。PGMax支持具有可处理因子的一般因子图，并利用现代加速器（如GPU）进行推理。与现有替代方案相比，PGMax获得了更高质量的推理结果，推理时间加速高达三个数量级。PGMax还与快速增长的JAX生态系统无缝交互，开启了新的研究可能性。我们的源代码、示例和文档可在https://github.com/deepmind/PGMax上获得。

    PGMax is an open-source Python package for (a) easily specifying discrete Probabilistic Graphical Models (PGMs) as factor graphs; and (b) automatically running efficient and scalable loopy belief propagation (LBP) in JAX. PGMax supports general factor graphs with tractable factors, and leverages modern accelerators like GPUs for inference. Compared with existing alternatives, PGMax obtains higher-quality inference results with up to three orders-of-magnitude inference time speedups. PGMax additionally interacts seamlessly with the rapidly growing JAX ecosystem, opening up new research possibilities. Our source code, examples and documentation are available at https://github.com/deepmind/PGMax.
    
[^192]: 基于多尺度CNN的体积模拟相似度度量学习

    Learning Similarity Metrics for Volumetric Simulations with Multiscale CNNs. (arXiv:2202.04109v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.04109](http://arxiv.org/abs/2202.04109)

    本文提出了一种基于熵的相似度模型，用于评估基于运输和运动的模拟产生的标量和矢量数据的相似度，并提出了一种多尺度CNN架构，用于计算体积相似度度量（VolSiM）。

    This paper proposes a similarity model based on entropy for assessing the similarity of scalar and vectorial data produced from transport and motion-based simulations, and a multiscale CNN architecture for computing a volumetric similarity metric (VolSiM).

    三维数据模拟在科学中应用广泛，从流体流动到等离子物理。本文提出了一种基于熵的相似度模型，允许创建物理上有意义的基准距离，用于评估基于运输和运动的模拟产生的标量和矢量数据的相似度。利用从该模型导出的两种数据采集方法，我们创建了从数值PDE求解器和现有模拟数据存储库中收集的场集合。此外，我们提出了一种多尺度CNN架构，用于计算体积相似度度量（VolSiM）。据我们所知，这是第一种天然设计用于解决高维模拟数据相似度评估挑战的学习方法。此外，我们还研究了基于相关损失函数的大批量大小和准确相关计算之间的权衡，并研究了该度量的不变性。

    Simulations that produce three-dimensional data are ubiquitous in science, ranging from fluid flows to plasma physics. We propose a similarity model based on entropy, which allows for the creation of physically meaningful ground truth distances for the similarity assessment of scalar and vectorial data, produced from transport and motion-based simulations. Utilizing two data acquisition methods derived from this model, we create collections of fields from numerical PDE solvers and existing simulation data repositories. Furthermore, a multiscale CNN architecture that computes a volumetric similarity metric (VolSiM) is proposed. To the best of our knowledge this is the first learning method inherently designed to address the challenges arising for the similarity assessment of high-dimensional simulation data. Additionally, the tradeoff between a large batch size and an accurate correlation computation for correlation-based loss functions is investigated, and the metric's invariance with 
    
[^193]: 我的深度网络的表现是否过于优秀？一种直接估计二元分类中贝叶斯误差的方法

    Is the Performance of My Deep Network Too Good to Be True? A Direct Approach to Estimating the Bayes Error in Binary Classification. (arXiv:2202.00395v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.00395](http://arxiv.org/abs/2202.00395)

    本文提出了一种简单直接的贝叶斯误差估计器，可以用于评估具有最先进性能的分类器的标准，并可用于检测测试集过拟合。与其他方法相比，我们的方法是无模型的，甚至是无实例的。此外，它没有超参数，并且在实证上比几个基线给出了更准确的贝叶斯误差估计。

    This paper proposes a simple and direct Bayes error estimator, which can be used as a criterion to evaluate classifiers with state-of-the-art performance and can be used to detect test set overfitting. Our method is model-free and even instance-free, and gives a more accurate estimate of the Bayes error than several baselines empirically.

    机器学习模型的预测性能存在根本限制，这是由于预测目标的不可避免的不确定性所致。在分类问题中，这可以通过贝叶斯误差来描述，它是任何分类器可以达到的最佳误差。贝叶斯误差可以用作评估具有最先进性能的分类器的标准，并可用于检测测试集过拟合。我们提出了一种简单直接的贝叶斯误差估计器，其中我们只需取显示类别分配不确定性的标签的平均值。我们的灵活方法使我们能够即使对于弱监督数据也进行贝叶斯误差估计。与其他方法相比，我们的方法是无模型的，甚至是无实例的。此外，它没有超参数，并且在实证上比几个基线给出了更准确的贝叶斯误差估计。使用我们的方法进行的实验表明，最近提出的深度网络（如Vision Transformer m）的表现可能过于优秀。

    There is a fundamental limitation in the prediction performance that a machine learning model can achieve due to the inevitable uncertainty of the prediction target. In classification problems, this can be characterized by the Bayes error, which is the best achievable error with any classifier. The Bayes error can be used as a criterion to evaluate classifiers with state-of-the-art performance and can be used to detect test set overfitting. We propose a simple and direct Bayes error estimator, where we just take the mean of the labels that show \emph{uncertainty} of the class assignments. Our flexible approach enables us to perform Bayes error estimation even for weakly supervised data. In contrast to others, our method is model-free and even instance-free. Moreover, it has no hyperparameters and gives a more accurate estimate of the Bayes error than several baselines empirically. Experiments using our method suggest that recently proposed deep networks such as the Vision Transformer m
    
[^194]: 状态条件对抗子目标生成

    State-Conditioned Adversarial Subgoal Generation. (arXiv:2201.09635v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.09635](http://arxiv.org/abs/2201.09635)

    本文提出了一种新的分层强化学习方法，通过对抗性学习来减轻高层策略不稳定的问题，提高了学习效率和性能。

    This paper proposes a novel hierarchical reinforcement learning approach that mitigates the problem of non-stationary high-level policy by adversarially enforcing the generation of subgoals compatible with the current instantiation of the low-level policy, resulting in improved learning efficiency and performance in challenging continuous control tasks.

    分层强化学习（HRL）提出通过在时间抽象的逐步更高的层次上执行决策和控制来解决困难任务。然而，离线HRL经常遭受高层策略不稳定的问题，因为低层策略不断变化。在本文中，我们提出了一种新的HRL方法，通过对抗性地强制高层策略生成与当前低层策略实例兼容的子目标来减轻不稳定性问题。在实践中，对抗性学习是通过同时训练一个简单的状态条件鉴别器网络和决定子目标兼容性水平的高层策略来实现的。与最先进的算法相比，我们的方法在具有挑战性的连续控制任务中提高了学习效率和性能。

    Hierarchical reinforcement learning (HRL) proposes to solve difficult tasks by performing decision-making and control at successively higher levels of temporal abstraction. However, off-policy HRL often suffers from the problem of a non-stationary high-level policy since the low-level policy is constantly changing. In this paper, we propose a novel HRL approach for mitigating the non-stationarity by adversarially enforcing the high-level policy to generate subgoals compatible with the current instantiation of the low-level policy. In practice, the adversarial learning is implemented by training a simple state-conditioned discriminator network concurrently with the high-level policy which determines the compatibility level of subgoals. Comparison to state-of-the-art algorithms shows that our approach improves both learning efficiency and performance in challenging continuous control tasks.
    
[^195]: 可解释神经网络在滑坡易发性建模中的应用

    Landslide Susceptibility Modeling by Interpretable Neural Network. (arXiv:2201.06837v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.06837](http://arxiv.org/abs/2201.06837)

    本文介绍了一种可解释神经网络（SNN）优化框架，用于评估滑坡易发性。SNN模型发现坡度和降水的乘积以及坡向是高滑坡易发性的重要主要贡献因素。

    This paper introduces an interpretable neural network framework, called superposable neural network (SNN) optimization, for assessing landslide susceptibility. The SNN models found the product of slope and precipitation and hillslope aspect to be important primary contributors to high landslide susceptibility.

    滑坡由于许多时空变化因素影响而难以预测。人工神经网络（ANN）已被证明可以提高预测准确性，但缺乏可解释性。本文介绍了一种可加性ANN优化框架，用于评估滑坡易发性，以及数据集划分和结果解释技术。我们将这种具有完全可解释性、高准确性、高泛化性和低模型复杂度的方法称为可叠加神经网络（SNN）优化。我们通过对来自三个不同东喜马拉雅地区的滑坡清单进行模型训练来验证我们的方法。我们的SNN优于基于物理和统计模型，并实现了类似于最先进的深度神经网络的性能。SNN模型发现坡度和降水的乘积以及坡向是高滑坡易发性的重要主要贡献因素。

    Landslides are notoriously difficult to predict because numerous spatially and temporally varying factors contribute to slope stability. Artificial neural networks (ANN) have been shown to improve prediction accuracy but are largely uninterpretable. Here we introduce an additive ANN optimization framework to assess landslide susceptibility, as well as dataset division and outcome interpretation techniques. We refer to our approach, which features full interpretability, high accuracy, high generalizability and low model complexity, as superposable neural network (SNN) optimization. We validate our approach by training models on landslide inventory from three different easternmost Himalaya regions. Our SNN outperformed physically-based and statistical models and achieved similar performance to state-of-the-art deep neural networks. The SNN models found the product of slope and precipitation and hillslope aspect to be important primary contributors to high landslide susceptibility, which 
    
[^196]: Egeria: 基于知识引导的层冻结技术实现高效DNN训练

    Egeria: Efficient DNN Training with Knowledge-Guided Layer Freezing. (arXiv:2201.06227v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.06227](http://arxiv.org/abs/2201.06227)

    Egeria是一种基于知识引导的DNN训练系统，通过跳过DNN层的计算和通信来实现高效训练，利用参考模型中的语义知识准确评估单个层的训练可塑性，并安全地冻结已收敛的层，从而节省相应的反向计算和通信。

    Egeria is a knowledge-guided DNN training system that skips computing and communication through DNN layer freezing, accurately evaluates individual layers' training plasticity using semantic knowledge from a reference model, and safely freezes the converged ones, saving their corresponding backward computation and communication.

    训练深度神经网络（DNN）是一项耗时的任务。本文提出了一种基于知识引导的DNN训练系统Egeria，通过跳过DNN层的计算和通信来实现高效训练。我们的关键洞察是，内部DNN层的训练进度存在显著差异，前层通常比深层更早地得到很好的训练。为了探索这一点，我们首先引入了训练可塑性的概念，以量化内部DNN层的训练进度。然后，我们设计了Egeria，一种基于知识引导的DNN训练系统，利用参考模型中的语义知识准确评估单个层的训练可塑性，并安全地冻结已收敛的层，从而节省相应的反向计算和通信。

    Training deep neural networks (DNNs) is time-consuming. While most existing solutions try to overlap/schedule computation and communication for efficient training, this paper goes one step further by skipping computing and communication through DNN layer freezing. Our key insight is that the training progress of internal DNN layers differs significantly, and front layers often become well-trained much earlier than deep layers. To explore this, we first introduce the notion of training plasticity to quantify the training progress of internal DNN layers. Then we design Egeria, a knowledge-guided DNN training system that employs semantic knowledge from a reference model to accurately evaluate individual layers' training plasticity and safely freeze the converged ones, saving their corresponding backward computation and communication. Our reference model is generated on the fly using quantization techniques and runs forward operations asynchronously on available CPUs to minimize the overhe
    
[^197]: PRONTO：基于神经网络的粗同步中的前导开销减少

    PRONTO: Preamble Overhead Reduction with Neural Networks for Coarse Synchronization. (arXiv:2112.10885v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.10885](http://arxiv.org/abs/2112.10885)

    本文提出了一种名为PRONTO的基于神经网络的方案，用于在WiFi基础波形中执行粗略的时间和频率同步，以减少前导开销。该方案通过消除传统短训练场（L-STF）来缩短前导长度，并使用其他前导字段（特别是传统的长训练场（L-LTF））执行估计。

    

    在IEEE 802.11 WiFi基础波形中，接收器使用前导的第一个字段（称为传统短训练场（L-STF））执行粗略的时间和频率同步。 L-STF占据了前导长度的高达40％，需要长达32微秒的空气时间。为了减少通信开销，我们提出了一种修改后的波形，其中通过消除L-STF来缩短前导长度。为了解码这种修改后的波形，我们提出了一种基于神经网络（NN）的方案，称为PRONTO，它使用其他前导字段（特别是传统的长训练场（L-LTF））执行粗略的时间和频率估计。我们的贡献有三个：（i）我们提出了PRONTO，其中包括用于数据检测和粗略载波频率偏移（CFO）估计的定制卷积神经网络（CNN），以及用于强化训练的数据增强步骤。 （ii）我们提出了一种广义决策流程，使PRONTO与包括th在内的传统波形兼容

    In IEEE 802.11 WiFi-based waveforms, the receiver performs coarse time and frequency synchronization using the first field of the preamble known as the legacy short training field (L-STF). The L-STF occupies upto 40% of the preamble length and takes upto 32 us of airtime. With the goal of reducing communication overhead, we propose a modified waveform, where the preamble length is reduced by eliminating the L-STF. To decode this modified waveform, we propose a neural network (NN)-based scheme called PRONTO that performs coarse time and frequency estimations using other preamble fields, specifically the legacy long training field (L-LTF). Our contributions are threefold: (i) We present PRONTO featuring customized convolutional neural networks (CNNs) for packet detection and coarse carrier frequency offset (CFO) estimation, along with data augmentation steps for robust training. (ii) We propose a generalized decision flow that makes PRONTO compatible with legacy waveforms that include th
    
[^198]: 两视角图神经网络用于知识图谱补全

    Two-view Graph Neural Networks for Knowledge Graph Completion. (arXiv:2112.09231v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2112.09231](http://arxiv.org/abs/2112.09231)

    本文提出了一种名为WGE的图神经网络模型，通过两个单一的实体和关系为中心的图来学习实体和关系的向量表示，并在知识图谱补全任务上取得了优异的性能。

    This paper proposes a graph neural network model named WGE, which learns vector representations of entities and relations from two single entity- and relation-focused graphs, and achieves excellent performance on knowledge graph completion task.

    我们提出了一种有效的基于图神经网络（GNN）的知识图谱嵌入模型，称为WGE，以捕捉实体和关系为中心的图结构。给定一个知识图谱，WGE构建一个单一的无向实体为中心的图，将实体视为节点。WGE还从关系为中心的约束条件构建另一个单一的无向图，将实体和关系视为节点。然后，WGE提出了一种基于GNN的架构，从这两个单一的实体和关系为中心的图中更好地学习实体和关系的向量表示。WGE将学习到的实体和关系表示馈送到加权得分函数中，以返回知识图谱补全的三元组得分。实验结果表明，WGE在七个知识图谱补全基准数据集上优于强基线模型。

    We present an effective graph neural network (GNN)-based knowledge graph embedding model, which we name WGE, to capture entity- and relation-focused graph structures. Given a knowledge graph, WGE builds a single undirected entity-focused graph that views entities as nodes. WGE also constructs another single undirected graph from relation-focused constraints, which views entities and relations as nodes. WGE then proposes a GNN-based architecture to better learn vector representations of entities and relations from these two single entity- and relation-focused graphs. WGE feeds the learned entity and relation representations into a weighted score function to return the triple scores for knowledge graph completion. Experimental results show that WGE outperforms strong baselines on seven benchmark datasets for knowledge graph completion.
    
[^199]: 联邦信号地图中的位置泄露问题

    Location Leakage in Federated Signal Maps. (arXiv:2112.03452v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.03452](http://arxiv.org/abs/2112.03452)

    本文研究了在联邦学习框架下，通过梯度泄漏攻击推断用户位置的问题，并提出了一种保护位置隐私的方法。

    This paper studies the problem of inferring user location through gradient leakage attacks in the federated learning framework, and proposes a method to protect location privacy.

    本文考虑了从多个移动设备收集的测量数据中预测蜂窝网络性能（信号地图）的问题。我们在在线联邦学习框架内制定了问题：（i）联邦学习（FL）使用户能够协作训练模型，同时保留其训练数据在其设备上；（ii）测量数据是随着用户随时间移动而收集的，并以在线方式用于本地训练。我们考虑一个诚实但好奇的服务器，观察参与FL的目标用户的更新并使用梯度泄漏（DLG）类型的攻击推断他们的位置，该攻击最初是为重构DNN图像分类器的训练数据而开发的。我们做出了关键观察，即DLG攻击应用于我们的设置，可以推断出本地数据批次的平均位置，并因此可以用于在粗略粒度上重构目标用户的轨迹。我们基于这个观察来保护位置隐私，在我们的s中。

    We consider the problem of predicting cellular network performance (signal maps) from measurements collected by several mobile devices. We formulate the problem within the online federated learning framework: (i) federated learning (FL) enables users to collaboratively train a model, while keeping their training data on their devices; (ii) measurements are collected as users move around over time and are used for local training in an online fashion. We consider an honest-but-curious server, who observes the updates from target users participating in FL and infers their location using a deep leakage from gradients (DLG) type of attack, originally developed to reconstruct training data of DNN image classifiers. We make the key observation that a DLG attack, applied to our setting, infers the average location of a batch of local data, and can thus be used to reconstruct the target users' trajectory at a coarse granularity. We build on this observation to protect location privacy, in our s
    
[^200]: FLSys：面向联邦学习移动应用的开放生态系统

    FLSys: Toward an Open Ecosystem for Federated Learning Mobile Apps. (arXiv:2111.09445v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.09445](http://arxiv.org/abs/2111.09445)

    本文介绍了FLSys，一个移动-云联邦学习（FL）系统，可以成为FL模型和应用程序开放生态系统的关键组成部分。FLSys旨在在智能手机上使用移动感测数据。它平衡了模型性能和资源消耗，容忍通信故障，并实现了可扩展性。FLSys提供了先进的隐私保护机制和一个通用的API，供第三方应用程序开发人员访问FL模型。

    This article introduces FLSys, a mobile-cloud federated learning (FL) system that can be a key component for an open ecosystem of FL models and apps. FLSys is designed to work on smart phones with mobile sensing data. It balances model performance with resource consumption, tolerates communication failures, and achieves scalability. FLSys provides advanced privacy preserving mechanisms and a common API for third-party app developers to access FL models.

    本文介绍了FLSys的设计、实现和评估，这是一个移动-云联邦学习（FL）系统，可以成为FL模型和应用程序开放生态系统的关键组成部分。FLSys旨在在智能手机上使用移动感测数据。它平衡了模型性能和资源消耗，容忍通信故障，并实现了可扩展性。在FLSys中，不同的DL模型和不同的FL聚合方法可以同时被不同的应用程序训练和访问。此外，FLSys提供了先进的隐私保护机制和一个通用的API，供第三方应用程序开发人员访问FL模型。FLSys采用模块化设计，实现在Android和AWS云中。我们与人类活动识别（HAR）模型共同设计了FLSys。在4个月的时间里，从100多名大学生中收集了HAR感测数据。我们实现了HAR-Wild，这是一个针对移动设备量身定制的CNN模型，具有数据增强机制以减轻p

    This article presents the design, implementation, and evaluation of FLSys, a mobile-cloud federated learning (FL) system, which can be a key component for an open ecosystem of FL models and apps. FLSys is designed to work on smart phones with mobile sensing data. It balances model performance with resource consumption, tolerates communication failures, and achieves scalability. In FLSys, different DL models with different FL aggregation methods can be trained and accessed concurrently by different apps. Furthermore, FLSys provides advanced privacy preserving mechanisms and a common API for third-party app developers to access FL models. FLSys adopts a modular design and is implemented in Android and AWS cloud. We co-designed FLSys with a human activity recognition (HAR) model. HAR sensing data was collected in the wild from 100+ college students during a 4-month period. We implemented HAR-Wild, a CNN model tailored to mobile devices, with a data augmentation mechanism to mitigate the p
    
[^201]: 一种统一且快速可解释的预测分析模型

    A Unified and Fast Interpretable Model for Predictive Analytics. (arXiv:2111.08255v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.08255](http://arxiv.org/abs/2111.08255)

    本文提出了一种名为FXAM的统一且快速可解释的预测分析模型，通过统一的可加模型扩展了GAM的建模能力，用于数值、分类和时间特征。FXAM进行了一种新颖的训练过程，称为三阶段迭代（TSI），分别对数值、分类和时间特征进行学习，具有较高的准确性和训练效率。

    This paper proposes a unified and fast interpretable model for predictive analytics called FXAM, which extends GAM's modeling capability with a unified additive model for numerical, categorical, and temporal features. FXAM conducts a novel training procedure called Three-Stage Iteration (TSI) and achieves high accuracy and training efficiency.

    预测分析旨在构建机器学习模型以预测行为模式并使用预测指导决策。由于预测分析需要人类参与，因此机器学习模型最好是可解释的。在文献中，广义可加模型（GAM）是解释性的标准。然而，由于在现实场景中常见的一对多和多对一现象，现有的GAM在准确性和训练效率方面都存在局限性。在本文中，我们提出了FXAM（快速且可解释的可加模型），这是一种统一且快速可解释的预测分析模型。FXAM通过统一的可加模型扩展了GAM的建模能力，用于数值、分类和时间特征。FXAM进行了一种新颖的训练过程，称为三阶段迭代（TSI）。TSI分别对数值、分类和时间特征进行学习。每个阶段学习局部最优解。

    Predictive analytics aims to build machine learning models to predict behavior patterns and use predictions to guide decision-making. Predictive analytics is human involved, thus the machine learning model is preferred to be interpretable. In literature, Generalized Additive Model (GAM) is a standard for interpretability. However, due to the one-to-many and many-to-one phenomena which appear commonly in real-world scenarios, existing GAMs have limitations to serve predictive analytics in terms of both accuracy and training efficiency. In this paper, we propose FXAM (Fast and eXplainable Additive Model), a unified and fast interpretable model for predictive analytics. FXAM extends GAM's modeling capability with a unified additive model for numerical, categorical, and temporal features. FXAM conducts a novel training procedure called Three-Stage Iteration (TSI). TSI corresponds to learning over numerical, categorical, and temporal features respectively. Each stage learns a local optimum 
    
[^202]: 利用行动影响规律和外生状态变量进行离线强化学习

    Exploiting Action Impact Regularity and Exogenous State Variables for Offline Reinforcement Learning. (arXiv:2111.08066v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.08066](http://arxiv.org/abs/2111.08066)

    本文提出了一种利用行动影响规律和外生状态变量进行离线强化学习的算法，该算法在现实世界的许多领域中成立，包括金融市场，并在模拟和真实世界中的不同数据收集策略中优于现有的离线强化学习算法。

    This paper proposes an algorithm for offline reinforcement learning that exploits the Action Impact Regularity (AIR) property, which holds in many real-world domains including financial markets, and outperforms existing algorithms across different data collection policies in simulated and real world.

    离线强化学习——从一批数据中学习策略——已知对于一般的MDP来说是困难的。这些结果促使我们需要关注离线强化学习可能可行的特定类别的MDP。在这项工作中，我们探索了一类受限制的MDP，以获得离线强化学习的保证。我们称之为行动影响规律（AIR）的关键属性是，行动主要影响状态的一部分（内生组件），并且对状态的其余部分（外生组件）影响有限。AIR是一个强假设，但它在许多现实世界的领域中仍然成立，包括金融市场。我们讨论了利用AIR属性的算法，并为基于Fitted-Q迭代的算法提供了理论分析。最后，我们证明了该算法在模拟和真实世界中的不同数据收集策略中优于现有的离线强化学习算法。

    Offline reinforcement learning -- learning a policy from a batch of data -is known to be hard for general MDPs. These results motivate the need to look at specific classes of MDPs where offline reinforcement learning might be feasible. In this work, we explore a restricted class of MDPs to obtain guarantees for offline reinforcement learning. The key property, which we call Action Impact Regularity (AIR), is that actions primarily impact a part of the state (an endogenous component) and have limited impact on the remaining part of the state (an exogenous component). AIR is a strong assumption, but it nonetheless holds in a number of real-world domains including financial markets. We discuss algorithms that exploit the AIR property, and provide a theoretical analysis for an algorithm based on Fitted-Q Iteration. Finally, we demonstrate that the algorithm outperforms existing offline reinforcement learning algorithms across different data collection policies in simulated and real world
    
[^203]: 如何学会不再担心并热爱重新训练

    How I Learned to Stop Worrying and Love Retraining. (arXiv:2111.00843v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.00843](http://arxiv.org/abs/2111.00843)

    本文提出了一种简单的线性学习率时间表，可以大大缩短重新训练阶段，同时提出了一种方法来自适应地选择线性时间表的初始值，并对初始密集训练阶段施加预算，从而改进了现有的重新训练方法。

    This paper proposes a simple linear learning rate schedule that can significantly shorten the retraining phase, and a method to adaptively select the initial value of the linear schedule, as well as imposing a budget on the initial dense training phase, improving on existing retraining approaches.

    许多神经网络剪枝方法包括几个迭代的训练和剪枝步骤，看似在剪枝后失去了大量性能，然后在随后的重新训练阶段恢复了它。最近的Renda等人（2020）和Le＆Hua（2021）的作品展示了重新训练阶段中学习率调度的重要性，并提出了选择IMP（Han等人，2015）的这种调度的特定启发式方法。我们将这些发现置于Li等人（2020）关于在固定训练预算内训练模型的结果的背景下，并证明，因此，可以使用简单的线性学习率时间表大大缩短重新训练阶段。在现有的重新训练方法的基础上，我们还提出了一种方法来自适应地选择线性时间表的初始值。更进一步，我们提出类似地对初始密集训练阶段施加预算，并展示了由此产生的简单方法。

    Many Neural Network Pruning approaches consist of several iterative training and pruning steps, seemingly losing a significant amount of their performance after pruning and then recovering it in the subsequent retraining phase. Recent works of Renda et al. (2020) and Le & Hua (2021) demonstrate the significance of the learning rate schedule during the retraining phase and propose specific heuristics for choosing such a schedule for IMP (Han et al., 2015). We place these findings in the context of the results of Li et al. (2020) regarding the training of models within a fixed training budget and demonstrate that, consequently, the retraining phase can be massively shortened using a simple linear learning rate schedule. Improving on existing retraining approaches, we additionally propose a method to adaptively select the initial value of the linear schedule. Going a step further, we propose similarly imposing a budget on the initial dense training phase and show that the resulting simple
    
[^204]: 全面深度学习

    Holistic Deep Learning. (arXiv:2110.15829v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.15829](http://arxiv.org/abs/2110.15829)

    本文提出了一种全面深度学习框架，通过解决输入扰动的脆弱性、过度参数化和性能不稳定性等挑战，全面提高了准确性、鲁棒性、稀疏性和稳定性，适用于表格和图像数据集。提供了选择适当的训练损失函数的建议。

    This paper proposes a holistic deep learning framework that addresses the challenges of vulnerability to input perturbations, overparametrization, and performance instability from different train-validation splits. The proposed framework improves accuracy, robustness, sparsity, and stability over standard deep learning models, as demonstrated by extensive experiments on both tabular and image data sets. A prescriptive approach is provided to support practitioners in selecting an appropriate training loss function based on their specific objectives.

    本文提出了一种新颖的全面深度学习框架，同时解决了对输入扰动的脆弱性、过度参数化和来自不同训练验证拆分的性能不稳定性等挑战。所提出的框架在标准深度学习模型上全面提高了准确性、鲁棒性、稀疏性和稳定性，这在对表格和图像数据集进行广泛实验中得到了证明。结果进一步通过消融实验和SHAP值分析进行验证，揭示了不同评估指标之间的交互作用和权衡。为了支持实践者应用我们的框架，我们提供了一种指导性方法，根据他们的具体目标，提供选择适当的训练损失函数的建议。所有用于重现结果的代码都可以在https://github.com/kimvc7/HDL找到。

    This paper presents a novel holistic deep learning framework that simultaneously addresses the challenges of vulnerability to input perturbations, overparametrization, and performance instability from different train-validation splits. The proposed framework holistically improves accuracy, robustness, sparsity, and stability over standard deep learning models, as demonstrated by extensive experiments on both tabular and image data sets. The results are further validated by ablation experiments and SHAP value analysis, which reveal the interactions and trade-offs between the different evaluation metrics. To support practitioners applying our framework, we provide a prescriptive approach that offers recommendations for selecting an appropriate training loss function based on their specific objectives. All the code to reproduce the results can be found at https://github.com/kimvc7/HDL.
    
[^205]: ProxyBO: 通过零成本代理加速神经架构搜索的贝叶斯优化

    ProxyBO: Accelerating Neural Architecture Search via Bayesian Optimization with Zero-cost Proxies. (arXiv:2110.10423v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.10423](http://arxiv.org/abs/2110.10423)

    本文提出了一种名为ProxyBO的高效贝叶斯优化框架，利用零成本代理加速神经架构搜索，通过广义能力测量估计代理在任务上的适应性，并设计了一种新的收购函数，基于它们的动态影响将BO与零成本代理相结合，实验证明ProxyBO在五个任务上始终优于竞争基线。

    This paper proposes an efficient Bayesian optimization framework called ProxyBO, which utilizes zero-cost proxies to accelerate neural architecture search, estimates the fitness of proxies on the task during each iteration using generalization ability measurement, and designs a novel acquisition function to combine BO with zero-cost proxies based on their dynamic influence. Extensive empirical studies show that ProxyBO consistently outperforms competitive baselines on five tasks from three public benchmarks.

    设计神经架构需要巨大的人工努力。这促进了神经架构搜索（NAS）的发展，以自动化设计。虽然以前的NAS方法取得了有希望的结果，但运行速度较慢，而零成本代理运行极快，但前景不太乐观。因此，通过这些零成本代理加速NAS具有巨大的潜力。现有方法有两个限制，即不可预见的可靠性和一次性使用。为了解决这些限制，我们提出了ProxyBO，一种高效的贝叶斯优化（BO）框架，利用零成本代理加速神经架构搜索。我们应用广义能力测量来估计每次迭代中代理在任务上的适应性，并设计了一种新的收购函数，基于它们的动态影响将BO与零成本代理相结合。广泛的实证研究表明，ProxyBO在来自三个公共基准测试的五个任务上始终优于竞争基线。

    Designing neural architectures requires immense manual efforts. This has promoted the development of neural architecture search (NAS) to automate the design. While previous NAS methods achieve promising results but run slowly, zero-cost proxies run extremely fast but are less promising. Therefore, it is of great potential to accelerate NAS via those zero-cost proxies. The existing method has two limitations, which are unforeseeable reliability and one-shot usage. To address the limitations, we present ProxyBO, an efficient Bayesian optimization (BO) framework that utilizes the zero-cost proxies to accelerate neural architecture search. We apply the generalization ability measurement to estimate the fitness of proxies on the task during each iteration and design a novel acquisition function to combine BO with zero-cost proxies based on their dynamic influence. Extensive empirical studies show that ProxyBO consistently outperforms competitive baselines on five tasks from three public ben
    
[^206]: 为什么对抗性重编程有效，何时会失败，以及如何区分二者

    Why Adversarial Reprogramming Works, When It Fails, and How to Tell the Difference. (arXiv:2108.11673v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.11673](http://arxiv.org/abs/2108.11673)

    对抗性重编程可以重新利用机器学习模型执行不同的任务，成功取决于平均输入梯度的大小，当输入梯度更加对齐且输入具有更高的维度时，平均输入梯度会增长。

    Adversarial reprogramming can repurpose machine-learning models to perform different tasks, and its success depends on the size of the average input gradient, which grows when input gradients are more aligned and when inputs have higher dimensionality.

    对抗性重编程允许重新利用机器学习模型执行不同的任务。例如，通过在提供的数字图像中嵌入对抗性程序，可以将训练用于识别动物的模型重新编程为识别数字。最近的研究表明，对抗性重编程不仅可以用于滥用作为服务提供的机器学习模型，而且在训练数据稀缺时，还可以有益地改善迁移学习。然而，影响其成功的因素仍然大多未被解释。在本文中，我们开发了对抗性重编程的一阶线性模型，以表明其成功本质上取决于平均输入梯度的大小，当输入梯度更加对齐且输入具有更高的维度时，平均输入梯度会增长。我们的实验分析结果涉及14个不同的重编程任务，表明上述因素与对抗性重编程的成功和失败相关。

    Adversarial reprogramming allows repurposing a machine-learning model to perform a different task. For example, a model trained to recognize animals can be reprogrammed to recognize digits by embedding an adversarial program in the digit images provided as input. Recent work has shown that adversarial reprogramming may not only be used to abuse machine-learning models provided as a service, but also beneficially, to improve transfer learning when training data is scarce. However, the factors affecting its success are still largely unexplained. In this work, we develop a first-order linear model of adversarial reprogramming to show that its success inherently depends on the size of the average input gradient, which grows when input gradients are more aligned, and when inputs have higher dimensionality. The results of our experimental analysis, involving fourteen distinct reprogramming tasks, show that the above factors are correlated with the success and the failure of adversarial repro
    
[^207]: 对抗正则化图注意力网络用于部分标记图的归纳学习

    Adversarially Regularized Graph Attention Networks for Inductive Learning on Partially Labeled Graphs. (arXiv:2106.03393v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.03393](http://arxiv.org/abs/2106.03393)

    本文提出了一种对抗正则化图注意力模型，用于在部分标记的图中分类新添加的节点，通过聚合其相邻节点的信息生成节点的表示，从而自然地推广到以前未见过的节点。

    This paper proposes an adversarially regularized graph attention model for classifying newly added nodes in a partially labeled graph, which generates the representation of a node by aggregating information from its neighboring nodes and naturally generalizes to previously unseen nodes. Adversarial training is employed to improve the model's robustness and generalization ability.

    在实际应用中，数据标记的高成本经常导致节点标记短缺。为了提高节点分类准确性，基于图的半监督学习利用丰富的未标记节点与稀缺的可用标记节点一起训练。然而，大多数现有方法在模型训练期间需要所有节点的信息，包括要预测的节点，这在具有新添加节点的动态图中不实用。为了解决这个问题，提出了一种对抗正则化图注意力模型，用于在部分标记的图中分类新添加的节点。设计了一种基于注意力的聚合器，通过聚合其相邻节点的信息生成节点的表示，从而自然地推广到以前未见过的节点。此外，采用对抗性训练，通过强制节点表示匹配先验分布来提高模型的鲁棒性和泛化能力。在真实世界的数据集上进行了实验。

    The high cost of data labeling often results in node label shortage in real applications. To improve node classification accuracy, graph-based semi-supervised learning leverages the ample unlabeled nodes to train together with the scarce available labeled nodes. However, most existing methods require the information of all nodes, including those to be predicted, during model training, which is not practical for dynamic graphs with newly added nodes. To address this issue, an adversarially regularized graph attention model is proposed to classify newly added nodes in a partially labeled graph. An attention-based aggregator is designed to generate the representation of a node by aggregating information from its neighboring nodes, thus naturally generalizing to previously unseen nodes. In addition, adversarial training is employed to improve the model's robustness and generalization ability by enforcing node representations to match a prior distribution. Experiments on real-world datasets
    
[^208]: 从嘈杂的数据中进行数据驱动的可达性分析

    Data-Driven Reachability Analysis from Noisy Data. (arXiv:2105.07229v3 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2105.07229](http://arxiv.org/abs/2105.07229)

    本文提出了一种从嘈杂的数据中计算可达集的算法，适用于不同类型的系统，包括线性、多项式和非线性系统。算法基于矩阵zonotope，可以提供较少保守的可达集，并且可以将关于未知系统模型的先前知识纳入计算。算法具有理论保证，并在多个数值示例和实际实验中得到了验证。

    This paper proposes an algorithm for computing reachable sets directly from noisy data without a given system model, which is applicable to different types of systems including linear, polynomial, and nonlinear systems. The algorithm is based on matrix zonotopes and can provide less conservative reachable sets while incorporating prior knowledge about the unknown system model. Theoretical guarantees are given and the applicability of the algorithm is demonstrated through numerical examples and real experiments.

    我们考虑在没有给定系统模型的情况下直接从嘈杂的数据中计算可达集的问题。我们提出了几种适用于生成数据的不同类型系统的可达性算法。首先，我们提出了一种基于矩阵zonotope的算法，用于计算线性系统的过估计可达集。引入了约束矩阵zonotope以提供较少保守的可达集，但代价是增加计算开销，并用于将关于未知系统模型的先前知识纳入计算。然后，我们将这种方法扩展到多项式系统，并在Lipschitz连续性的假设下扩展到非线性系统。这些算法的理论保证是它们给出一个包含真实可达集的适当过估计可达集。多个数值示例和实际实验显示了引入算法的适用性，并进行了算法之间的比较。

    We consider the problem of computing reachable sets directly from noisy data without a given system model. Several reachability algorithms are presented for different types of systems generating the data. First, an algorithm for computing over-approximated reachable sets based on matrix zonotopes is proposed for linear systems. Constrained matrix zonotopes are introduced to provide less conservative reachable sets at the cost of increased computational expenses and utilized to incorporate prior knowledge about the unknown system model. Then we extend the approach to polynomial systems and, under the assumption of Lipschitz continuity, to nonlinear systems. Theoretical guarantees are given for these algorithms in that they give a proper over-approximate reachable set containing the true reachable set. Multiple numerical examples and real experiments show the applicability of the introduced algorithms, and comparisons are made between algorithms.
    
[^209]: NOMU: 基于神经优化的模型不确定性

    NOMU: Neural Optimization-based Model Uncertainty. (arXiv:2102.13640v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2102.13640](http://arxiv.org/abs/2102.13640)

    NOMU是一种新的神经网络模型，可以在无噪声设置下，通过设计一个由两个连接的子NN组成的网络架构，并使用精心设计的损失函数进行训练，来捕捉NN的模型不确定性。该模型满足五个关于模型不确定性的重要愿望。

    NOMU is a new neural network model that captures model uncertainty for neural networks (NNs) in regression by designing a network architecture consisting of two connected sub-NNs, one for model prediction and one for model uncertainty, and training it using a carefully-designed loss function. The model satisfies five important desiderata regarding model uncertainty.

    我们研究了神经网络（NN）回归中模型不确定性的估计方法。为了隔离模型不确定性的影响，我们专注于稀缺训练数据的无噪声设置。我们提出了五个关于模型不确定性的重要愿望，任何方法都应该满足这些愿望。然而，我们发现，即使是贝叶斯理论所要求的一些愿望，已经建立的基准测试也经常无法可靠地捕捉到。为了解决这个问题，我们引入了一种新的方法来捕捉NN的模型不确定性，称为神经优化模型不确定性（NOMU）。 NOMU的主要思想是设计一个由两个连接的子NN组成的网络架构，一个用于模型预测，一个用于模型不确定性，并使用精心设计的损失函数进行训练。重要的是，我们的设计强制NOMU满足我们的五个愿望。由于其模块化架构，如果给定访问权限，NOMU可以为任何给定的（先前训练的）NN提供模型不确定性。

    We study methods for estimating model uncertainty for neural networks (NNs) in regression. To isolate the effect of model uncertainty, we focus on a noiseless setting with scarce training data. We introduce five important desiderata regarding model uncertainty that any method should satisfy. However, we find that established benchmarks often fail to reliably capture some of these desiderata, even those that are required by Bayesian theory. To address this, we introduce a new approach for capturing model uncertainty for NNs, which we call Neural Optimization-based Model Uncertainty (NOMU). The main idea of NOMU is to design a network architecture consisting of two connected sub-NNs, one for model prediction and one for model uncertainty, and to train it using a carefully-designed loss function. Importantly, our design enforces that NOMU satisfies our five desiderata. Due to its modular architecture, NOMU can provide model uncertainty for any given (previously trained) NN if given access
    
[^210]: 不确定性的不平等影响：平权行动与平权信息

    The Disparate Impact of Uncertainty: Affirmative Action vs. Affirmative Information. (arXiv:2102.10019v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2102.10019](http://arxiv.org/abs/2102.10019)

    本文证明了不确定性对不同人群的影响是不平等的，虽然它会在所有人口群体中产生误差，但误差的类型会有系统性的变化。我们提出了一种名为平权信息的策略，可以消除这种差异并扩大机会的获取，这可以作为平权行动的替代方案。

    This paper proves that uncertainty has a disparate impact on different demographic groups, with varying types of errors. The proposed strategy, called Affirmative Information, can eliminate this disparity and broaden access to opportunity, serving as an alternative to Affirmative Action.

    像贷款批准、医疗干预和大学录取这样的关键决策是在存在不确定性的情况下进行预测的。在本文中，我们证明了不确定性具有不平等的影响。虽然它会在所有人口群体中产生误差，但误差的类型会有系统性的变化：平均结果较高的群体通常被分配更高的假阳性率，而平均结果较低的群体则被分配更高的假阴性率。我们展示了额外的数据获取可以消除这种差异并扩大机会的获取。我们称之为平权信息的策略可以作为平权行动的替代方案。

    Critical decisions like loan approvals, medical interventions, and college admissions are guided by predictions made in the presence of uncertainty. In this paper, we prove that uncertainty has a disparate impact. While it imparts errors across all demographic groups, the types of errors vary systematically: Groups with higher average outcomes are typically assigned higher false positive rates, while those with lower average outcomes are assigned higher false negative rates. We show that additional data acquisition can eliminate the disparity and broaden access to opportunity. The strategy, which we call Affirmative Information, could stand as an alternative to Affirmative Action.
    
[^211]: SoK: 隐私保护下多源训练机器学习模型

    SoK: Training Machine Learning Models over Multiple Sources with Privacy Preservation. (arXiv:2012.03386v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2012.03386](http://arxiv.org/abs/2012.03386)

    本文综述了隐私保护下多源训练机器学习模型的两种主流解决方案：安全多方学习和联邦学习，并对它们的安全性、效率、数据分布、训练模型的准确性和应用场景进行了比较和讨论，同时探讨了未来的研究方向。

    This paper reviews two mainstream solutions for training machine learning models over multiple sources with privacy preservation: Secure Multi-party Learning (MPL) and Federated Learning (FL). The security, efficiency, data distribution, accuracy of trained models, and application scenarios of these two solutions are compared and discussed, and future research directions are explored.

    如今，从多个数据源中收集高质量的训练数据并保护隐私是训练高性能机器学习模型的关键挑战。潜在的解决方案可以打破孤立数据语料库之间的障碍，从而扩大可用于处理的数据范围。为此，学术研究人员和工业供应商最近强烈动力，提出了两种主流解决方案，主要基于软件构造：1）安全多方学习（MPL）；和2）联邦学习（FL）。当我们根据以下五个标准评估它们时，上述两个技术文件夹都有其优点和局限性：安全性，效率，数据分布，训练模型的准确性和应用场景。为了展示研究进展并讨论未来方向的见解，我们彻底调查了这些协议和MPL和FL的框架，并总结了该领域的最新研究进展。我们还对这两个技术文件夹进行了全面比较，并讨论了开放的挑战和未来的研究方向。

    Nowadays, gathering high-quality training data from multiple data sources with privacy preservation is a crucial challenge to training high-performance machine learning models. The potential solutions could break the barriers among isolated data corpus, and consequently enlarge the range of data available for processing. To this end, both academic researchers and industrial vendors are recently strongly motivated to propose two main-stream folders of solutions mainly based on software constructions: 1) Secure Multi-party Learning (MPL for short); and 2) Federated Learning (FL for short). The above two technical folders have their advantages and limitations when we evaluate them according to the following five criteria: security, efficiency, data distribution, the accuracy of trained models, and application scenarios.  Motivated to demonstrate the research progress and discuss the insights on the future directions, we thoroughly investigate these protocols and frameworks of both MPL and
    
[^212]: LOCUS：一种使用低秩结构和均匀稀疏性的大脑网络连接矩阵分解新方法

    LOCUS: A Novel Decomposition Method for Brain Network Connectivity Matrices using Low-rank Structure with Uniform Sparsity. (arXiv:2008.08915v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2008.08915](http://arxiv.org/abs/2008.08915)

    LOCUS是一种新的大脑网络连接矩阵分解方法，使用低秩结构和均匀稀疏性，能够更有效和准确地分离连接矩阵源，有望成为理解大脑组织的关键。

    LOCUS is a novel method for decomposing brain network connectivity matrices using low-rank structure and uniform sparsity, which achieves more efficient and accurate source separation and has the potential to serve as a key for understanding brain organizations.

    网络导向研究在许多科学领域中越来越受欢迎。在神经科学研究中，基于成像的网络连接度量已成为理解大脑组织的关键，可能作为个体神经指纹。分析连接矩阵存在主要挑战，包括大脑网络的高维度，观察到的连接下面的未知潜在源以及导致虚假发现的大量脑连接。在本文中，我们提出了一种新的盲源分离方法，即具有低秩结构和均匀稀疏性（LOCUS）的完全数据驱动分解方法，用于网络度量。与将连接矩阵向量化并忽略大脑网络拓扑的现有方法相比，LOCUS使用低秩结构实现了更有效和准确的连接矩阵源分离。我们提出了一种新的基于角度的均匀稀疏正则化，证明了其在连接矩阵分解中的优越性。

    Network-oriented research has been increasingly popular in many scientific areas. In neuroscience research, imaging-based network connectivity measures have become the key for understanding brain organizations, potentially serving as individual neural fingerprints. There are major challenges in analyzing connectivity matrices including the high dimensionality of brain networks, unknown latent sources underlying the observed connectivity, and the large number of brain connections leading to spurious findings. In this paper, we propose a novel blind source separation method with low-rank structure and uniform sparsity (LOCUS) as a fully data-driven decomposition method for network measures. Compared with the existing method that vectorizes connectivity matrices ignoring brain network topology, LOCUS achieves more efficient and accurate source separation for connectivity matrices using low-rank structure. We propose a novel angle-based uniform sparsity regularization that demonstrates bet
    
[^213]: FRMDN: 基于流的循环混合密度网络

    FRMDN: Flow-based Recurrent Mixture Density Network. (arXiv:2008.02144v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2008.02144](http://arxiv.org/abs/2008.02144)

    本文提出了一种基于流的循环混合密度网络，通过在每个时间步上定义一个高斯混合模型，将循环混合密度网络推广到非线性变换的目标序列上，该模型在图像序列的拟合度上表现显著，具有显著的建模能力，在对数似然度量方面优于其他最先进的方法。

    This paper proposes a flow-based recurrent mixture density network (FRMDN) that generalizes recurrent mixture density networks by defining a Gaussian mixture model on a non-linearly transformed target sequence in each time-step. The model significantly improves the fit to image sequences and outperforms other state-of-the-art methods in terms of the log-likelihood.

    循环混合密度网络是一类重要的概率模型，广泛应用于序列建模和序列到序列映射应用中。在这类模型中，目标序列在每个时间步的密度由具有循环神经网络参数的高斯混合模型建模。本文通过在每个时间步上定义一个高斯混合模型，将循环混合密度网络推广到非线性变换的目标序列上。非线性变换空间是通过归一化流创建的。我们观察到，该模型显著提高了图像序列的拟合度，用对数似然度量。我们还将所提出的模型应用于一些语音和图像数据，并观察到该模型具有显著的建模能力，在对数似然度量方面优于其他最先进的方法。

    The class of recurrent mixture density networks is an important class of probabilistic models used extensively in sequence modeling and sequence-to-sequence mapping applications. In this class of models, the density of a target sequence in each time-step is modeled by a Gaussian mixture model with the parameters given by a recurrent neural network. In this paper, we generalize recurrent mixture density networks by defining a Gaussian mixture model on a non-linearly transformed target sequence in each time-step. The non-linearly transformed space is created by normalizing flow. We observed that this model significantly improves the fit to image sequences measured by the log-likelihood. We also applied the proposed model on some speech and image data, and observed that the model has significant modeling power outperforming other state-of-the-art methods in terms of the log-likelihood.
    
[^214]: 异常感知

    Anomaly Awareness. (arXiv:2007.14462v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2007.14462](http://arxiv.org/abs/2007.14462)

    该论文提出了一种新的异常检测算法，称为异常感知。该算法通过修改成本函数来学习正常事件，并了解异常事件。该方法在不同的粒子物理情况和标准计算机视觉任务中的应用，能够有效地识别以前未见过的异常，并在了解足够多的异常时变得更加稳健。

    The paper proposes a new anomaly detection algorithm called Anomaly Awareness, which learns about normal events while being made aware of the anomalies through a modification of the cost function. The method is effective in identifying anomalies not seen before and becomes more robust as it is made aware of a varied-enough set of anomalies. It is applied in different Particle Physics situations and in standard Computer Vision tasks.

    我们提出了一种新的异常检测算法，称为异常感知。该算法通过修改成本函数来学习正常事件，并了解异常事件。我们展示了该方法在不同的粒子物理情况和标准计算机视觉任务中的应用。例如，我们将该方法应用于由标准模型顶夸克和QCD事件生成的Fat Jet拓扑的图像，并针对一系列新物理场景进行测试，包括具有EFT效应的希格斯产生和衰变成两个、三个或四个子喷注的共振。我们发现该算法能够有效地识别以前未见过的异常，并在我们让它了解足够多的异常时变得更加稳健。

    We present a new algorithm for anomaly detection called Anomaly Awareness. The algorithm learns about normal events while being made aware of the anomalies through a modification of the cost function. We show how this method works in different Particle Physics situations and in standard Computer Vision tasks. For example, we apply the method to images from a Fat Jet topology generated by Standard Model Top and QCD events, and test it against an array of new physics scenarios, including Higgs production with EFT effects and resonances decaying into two, three or four subjets. We find that the algorithm is effective identifying anomalies not seen before, and becomes robust as we make it aware of a varied-enough set of anomalies.
    
[^215]: COMET: 卷积维度交互用于协同过滤

    COMET: Convolutional Dimension Interaction for Collaborative Filtering. (arXiv:2007.14129v6 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2007.14129](http://arxiv.org/abs/2007.14129)

    COMET是一种新的基于表示学习的模型，它可以同时模拟历史交互和嵌入维度之间的高阶交互模式。

    COMET is a novel representation learning-based model that can simultaneously model the high-order interaction patterns among historical interactions and embedding dimensions.

    基于表示学习的推荐模型在推荐技术中扮演着主导角色。然而，大多数现有方法假设历史交互和嵌入维度相互独立，因此遗憾地忽略了历史交互和嵌入维度之间的高阶交互信息。在本文中，我们提出了一种新的基于表示学习的模型，称为COMET（COnvolutional diMEnsion inTeraction），它同时模拟历史交互和嵌入维度之间的高阶交互模式。具体而言，COMET首先将历史交互的嵌入水平堆叠，从而产生两个“嵌入映射”。通过卷积神经网络（CNN）同时使用不同大小的内部交互和维度交互内核，可以利用内部交互和维度交互。然后应用全连接的多层感知器（MLP）来获得两个交互向量。

    Representation learning-based recommendation models play a dominant role among recommendation techniques. However, most of the existing methods assume both historical interactions and embedding dimensions are independent of each other, and thus regrettably ignore the high-order interaction information among historical interactions and embedding dimensions. In this paper, we propose a novel representation learning-based model called COMET (COnvolutional diMEnsion inTeraction), which simultaneously models the high-order interaction patterns among historical interactions and embedding dimensions. To be specific, COMET stacks the embeddings of historical interactions horizontally at first, which results in two "embedding maps". In this way, internal interactions and dimensional interactions can be exploited by convolutional neural networks (CNN) with kernels of different sizes simultaneously. A fully-connected multi-layer perceptron (MLP) is then applied to obtain two interaction vectors. 
    
[^216]: 基于FPGA的在线顺序学习强化学习方法

    An FPGA-Based On-Device Reinforcement Learning Approach using Online Sequential Learning. (arXiv:2005.04646v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2005.04646](http://arxiv.org/abs/2005.04646)

    本文提出了一种基于FPGA的轻量级强化学习方法，利用OS-ELM算法进行训练，避免了DQN需要大量缓冲区和批处理的问题，并通过L2正则化和谱归一化的组合使得强化学习更加稳定。

    This paper proposes a lightweight on-device reinforcement learning approach for low-cost FPGA devices, which uses OS-ELM algorithm for training and avoids the problem of requiring large buffers and batch processing in DQN. The combination of L2 regularization and spectral normalization is used to make the reinforcement learning more stable.

    DQN是一种使用深度神经网络进行强化学习的Q学习方法。DQN需要大量的缓冲区和批处理进行经验重放，并依赖于基于反向传播的迭代优化，使它们难以在资源有限的边缘设备上实现。本文提出了一种轻量级的基于设备的强化学习方法，用于低成本的FPGA设备。它利用了最近提出的基于神经网络的设备学习方法，该方法不依赖于反向传播方法，而是使用基于OS-ELM（在线顺序极限学习机）的训练算法。此外，我们提出了一种L2正则化和谱归一化的组合，用于设备上的强化学习，以便将神经网络的输出值适合于某个范围，并使强化学习变得稳定。所提出的强化学习方法是为PYNQ-Z1板设计的，作为低成本的FPGA平台。

    DQN (Deep Q-Network) is a method to perform Q-learning for reinforcement learning using deep neural networks. DQNs require a large buffer and batch processing for an experience replay and rely on a backpropagation based iterative optimization, making them difficult to be implemented on resource-limited edge devices. In this paper, we propose a lightweight on-device reinforcement learning approach for low-cost FPGA devices. It exploits a recently proposed neural-network based on-device learning approach that does not rely on the backpropagation method but uses OS-ELM (Online Sequential Extreme Learning Machine) based training algorithm. In addition, we propose a combination of L2 regularization and spectral normalization for the on-device reinforcement learning so that output values of the neural network can be fit into a certain range and the reinforcement learning becomes stable. The proposed reinforcement learning approach is designed for PYNQ-Z1 board as a low-cost FPGA platform. Th
    
[^217]: 基于特征的生成模型潜空间插值和测地线

    Feature-Based Interpolation and Geodesics in the Latent Spaces of Generative Models. (arXiv:1904.03445v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1904.03445](http://arxiv.org/abs/1904.03445)

    本文提出了一种通用和统一的插值方法，它同时允许我们在任意密度的情况下搜索测地线和插值曲线。最大化曲线的质量度量可以等价地理解为在空间上某种重新定义的黎曼度量下搜索测地线。

    This paper proposes a general and unified approach to interpolation, which simultaneously allows us to search for geodesics and interpolating curves in latent space in the case of arbitrary density. Maximizing the quality measure of the curve can be equivalently understood as a search of geodesic for a certain redefinition of the Riemannian metric on the space.

    插值问题同时涉及到测地线和生成模型的研究。在测地线的情况下，我们寻找长度最短的曲线，而在生成模型的情况下，我们通常在潜空间中应用线性插值。然而，这种插值隐含地使用了高斯分布是单峰的事实。因此，在潜在密度为非高斯分布的情况下插值的问题是一个开放的问题。在本文中，我们提出了一种通用和统一的插值方法，它同时允许我们在任意密度的情况下搜索测地线和插值曲线。我们的结果具有强大的理论背景，基于引入的插值曲线质量度量。特别地，我们展示了最大化曲线的质量度量可以等价地理解为在空间上某种重新定义的黎曼度量下搜索测地线。

    Interpolating between points is a problem connected simultaneously with finding geodesics and study of generative models. In the case of geodesics, we search for the curves with the shortest length, while in the case of generative models we typically apply linear interpolation in the latent space. However, this interpolation uses implicitly the fact that Gaussian is unimodal. Thus the problem of interpolating in the case when the latent density is non-Gaussian is an open problem.  In this paper, we present a general and unified approach to interpolation, which simultaneously allows us to search for geodesics and interpolating curves in latent space in the case of arbitrary density. Our results have a strong theoretical background based on the introduced quality measure of an interpolating curve. In particular, we show that maximising the quality measure of the curve can be equivalently understood as a search of geodesic for a certain redefinition of the Riemannian metric on the space. 
    
[^218]: 在多个统计相关方向上进行梯度的在线线性回归以提高SGD的收敛性

    Improving SGD convergence by online linear regression of gradients in multiple statistically relevant directions. (arXiv:1901.11457v11 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1901.11457](http://arxiv.org/abs/1901.11457)

    本文提出了一种在线线性回归的方法，通过在多个统计相关方向上进行梯度的回归来提高SGD的收敛性，解决了标准方法只考虑单个方向的问题，同时避免了二阶方法的成本和数值不稳定性。

    This paper proposes an online linear regression method that improves the convergence of SGD by regressing gradients in multiple statistically relevant directions, addressing the issue of standard methods only considering a single direction and avoiding the cost and numerical instability of second order methods.

    深度神经网络通常使用随机梯度下降（SGD）进行训练，该方法仅使用梯度的非常粗略的近似值来最小化目标函数。标准方法（如动量或ADAM）仅考虑单个方向，并且不尝试模拟到极值的距离，忽略了从计算的梯度序列中获得的有价值信息，往往停滞在某些次优平台上。二阶方法可以利用这些错过的机会，但除了遭受非常大的成本和数值不稳定性外，其中许多方法由于忽略曲率的符号（作为Hessian的特征值）而吸引到像鞍点这样的次优点。无鞍牛顿法是解决这个问题的一个罕见例子，它将鞍点吸引力转变为排斥力，并被证明在这种方式下提供了最终值的重要改进。然而，它在模拟二阶行为时忽略了噪声，专注于Krylov子空间以进行数值计算。

    Deep neural networks are usually trained with stochastic gradient descent (SGD), which minimizes objective function using very rough approximations of gradient, only averaging to the real gradient. Standard approaches like momentum or ADAM only consider a single direction, and do not try to model distance from extremum - neglecting valuable information from calculated sequence of gradients, often stagnating in some suboptimal plateau. Second order methods could exploit these missed opportunities, however, beside suffering from very large cost and numerical instabilities, many of them attract to suboptimal points like saddles due to negligence of signs of curvatures (as eigenvalues of Hessian).  Saddle-free Newton method is a rare example of addressing this issue changes saddle attraction into repulsion, and was shown to provide essential improvement for final value this way. However, it neglects noise while modelling second order behavior, focuses on Krylov subspace for numerical rea
    

