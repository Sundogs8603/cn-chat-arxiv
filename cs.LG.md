# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [PiML Toolbox for Interpretable Machine Learning Model Development and Validation.](http://arxiv.org/abs/2305.04214) | PiML工具箱是一个综合的Python工具箱，可用于开发和诊断可解释机器学习模型，包括日益增长的可解释模型、模型无关的可解释性工具和模型无关的诊断工具，还支持与MLOps平台的集成和质量保证。 |
| [^2] | [Unlocking the Power of Open Set : A New Perspective for Open-set Noisy Label Learning.](http://arxiv.org/abs/2305.04203) | 本文提出了一种新的两步对比学习方法CECL，通过利用开放集合示例的有用信息来处理两种类型的标签噪声。该方法在几个基准数据集上得到了验证。 |
| [^3] | [MrTF: Model Refinery for Transductive Federated Learning.](http://arxiv.org/abs/2305.04201) | 本文提出了一种名为传输联邦学习的新学习范例，以同时考虑数据异构和要推导数据的结构信息。采用稳定的教师、修正的蒸馏和聚类标签精炼等技术来促进模型精炼过程，并在大量实验中证明了该方法的优越性。 |
| [^4] | [Bayesian Over-the-Air FedAvg via Channel Driven Stochastic Gradient Langevin Dynamics.](http://arxiv.org/abs/2305.04152) | 本文提出了无线 FALD 协议，可以在无噪声通信的情况下高效地在无线系统中实现分布式贝叶斯学习，实现了在通信回合之间多个本地更新以及由小批量计算的随机梯度，并进行了样本收敛分析。 |
| [^5] | [Efficient information recovery from Pauli noise via classical shadow.](http://arxiv.org/abs/2305.04148) | 本文提出了一种有效的算法，可以在Pauli噪声下从量子状态中恢复信息。通过后处理通道的经典阴影，学习未知Pauli通道的必要信息，并只需要通道的部分信息而不是其完整的经典描述就可以恢复理想的信息，产生一个多项式时间算法。 |
| [^6] | [Bounding the Invertibility of Privacy-preserving Instance Encoding using Fisher Information.](http://arxiv.org/abs/2305.04146) | 本文提出了一种基于费舍尔信息理论，可以界定实例编码隐私性的方法。该方法可以用于界定编码可逆性，实现下游机器学习应用的隐私保护。 |
| [^7] | [Transformer-Based Hierarchical Clustering for Brain Network Analysis.](http://arxiv.org/abs/2305.04142) | 本文提出了一种可解释的基于Transformer的层次聚类模型，用于脑网络识别和分类，实验结果表明该模型可以提高准确性和降低运行时复杂度，同时提供对脑区域功能组织的合理见解。 |
| [^8] | [Maintaining Stability and Plasticity for Predictive Churn Reduction.](http://arxiv.org/abs/2305.04135) | AMC是一种基于保留之前和当前的模型版本，生成元输出的通用技术，可以显著降低预测性流失，同时即使与最先进的方法相比，仍然保持或提高总体准确性。 |
| [^9] | [Learning Mixtures of Gaussians with Censored Data.](http://arxiv.org/abs/2305.04127) | 本文提出了一种学习高斯混合模型的算法，该算法仅需要很少的样本且能够对权重和均值进行准确估计。 |
| [^10] | [A Latent Diffusion Model for Protein Structure Generation.](http://arxiv.org/abs/2305.04120) | 提出了一种潜在扩散模型，能够在压缩的潜在空间中灵活地捕获自然蛋白质结构的分布，有效地生成具有高设计能力和效率的新型蛋白质骨架结构。 |
| [^11] | [Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling.](http://arxiv.org/abs/2305.04111) | 本文提出了EDGE，一种新的离散扩散模型用于生成大型图，并通过删除边来鼓励图的稀疏性。EDGE在每个去噪步骤中仅关注图中一部分节点，并且可以明确地对图的节点度数进行建模。实验表明，EDGE比竞争方法更有效，并且可以生成具有数千个节点的大型图。 |
| [^12] | [DMF-TONN: Direct Mesh-free Topology Optimization using Neural Networks.](http://arxiv.org/abs/2305.04107) | 本文提出了一种直接无网格法进行拓扑优化，集成了神经网络并用于求解遵从度和体积分数约束违规的优化问题，无需传统的有限元分析和网格化，能够无缝地集成到后处理软件中。 |
| [^13] | [On the Usage of Continual Learning for Out-of-Distribution Generalization in Pre-trained Language Models of Code.](http://arxiv.org/abs/2305.04106) | 本文强调预训练语言模型需要对代码进行持续学习以适应变化的软件数据分布，使其具有更好的泛化能力。 |
| [^14] | [Symbolic Regression on FPGAs for Fast Machine Learning Inference.](http://arxiv.org/abs/2305.04099) | 本论文提出了一种全新的利用符号回归的全流程，可在FPGA上进行机器学习推断，具有优化性能-资源平衡的特点。 |
| [^15] | [Gradient Leakage Defense with Key-Lock Module for Federated Learning.](http://arxiv.org/abs/2305.04095) | 本研究提出了一种新的联邦学习梯度泄露防御技术，使用私钥锁模块保护任意模型体系结构，并可确保无法从共享的梯度中重建私有训练数据。 |
| [^16] | [An improved regret analysis for UCB-N and TS-N.](http://arxiv.org/abs/2305.04093) | 本文提出了对UCB-N和TS-N算法的改进后的伪后悔分析方法，并将$\log(T)$因子替换为$\log_2(\alpha)+3$因子，其中$\alpha$是反馈图的独立数。 |
| [^17] | [A Minimal Approach for Natural Language Action Space in Text-based Games.](http://arxiv.org/abs/2305.04082) | 本文介绍了一种在文本游戏中自然语言动作空间的简化方法，提出了 ε-可接受的探索方法，并提出了一种不需要语言模型或知识图谱的文本角色-评论（TAC）代理。实验表明，该方法可以优于使用语言模型和知识图谱的最先进代理。 |
| [^18] | [Robust Tensor CUR Decompositions: Rapid Low-Tucker-Rank Tensor Recovery with Sparse Corruption.](http://arxiv.org/abs/2305.04080) | 本文提出了一种快速算法，称为鲁棒性张量CUR分解（RTCUR），用于Tucker秩设置下的大规模非凸TRPCA问题，通过交替投影的框架和张量CUR分解，快速实现对稀疏损坏的低秩张量恢复，并在实际数据集上展示了该方法的有效性和计算优势。 |
| [^19] | [Explaining RL Decisions with Trajectories.](http://arxiv.org/abs/2305.04073) | 本文提出一种用训练过程中遇到的轨迹解释强化学习决策的方法，并在离散和连续状态及行动空间的多样化环境中证明了其有效性。 |
| [^20] | [Semi-Asynchronous Federated Edge Learning Mechanism via Over-the-air Computation.](http://arxiv.org/abs/2305.04066) | 本文提出了一种半异步聚合FEEL机制PAOTA，以改善数据和设备存在显著异质性的情况下FEEL系统的训练效率，通过调整边缘设备的上行传输功率来最小化FEEL全局模型的收敛上界。实验结果表明，所提出的机制在达到相同的目标精度下，训练速度显著快于具有空中计算方案的传统同步FEEL机制。 |
| [^21] | [Decentralised Semi-supervised Onboard Learning for Scene Classification in Low-Earth Orbit.](http://arxiv.org/abs/2305.04059) | 该论文展示了在最新卫星硬件上，使用分布式半监督学习进行场景分类的机器学习模型，并考虑卫星操作约束，达到了高精度，为航天通信和运营成本节约提供了可能。 |
| [^22] | [Echoes: Unsupervised Debiasing via Pseudo-bias Labeling in an Echo Chamber.](http://arxiv.org/abs/2305.04043) | Echoes提出了一种无监督的去偏方法，生成偏差对立样本的伪偏差标签，实现了对数据集中偏差特征的一致性处理，并取得了各项任务和数据集上的最先进性能。 |
| [^23] | [Wasserstein-Fisher-Rao Embedding: Logical Query Embeddings with Local Comparison and Global Transport.](http://arxiv.org/abs/2305.04034) | 本文提出了一种新的查询嵌入方法，使用Wasserstein-Fisher-Rao度量来加权考虑嵌入之间的本地（局部）与全局（整体）特征，同时提出了一种卷积算法来进行线性时间计算，并使用块对角核来进行权衡。在标准数据集上表现出优异的效果。 |
| [^24] | [Electromyography Signal Classification Using Deep Learning.](http://arxiv.org/abs/2305.04006) | 本论文使用深度学习模型实现了肌电图信号的分类，通过训练，成功实现对对照组、肌病和ALS患者信号的准确分类，取得了准确率99%的成果，这些技术将有助于临床诊断神经肌肉疾病。 |
| [^25] | [ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for Verification.](http://arxiv.org/abs/2305.04003) | 本文介绍了一种名为ANTONIO的Python库，它基于抽象解释方法提供了一种实用的方法和启发式规则，以便为自然语言处理（NLP）数据集和模型生成已知验证方法的基准。因为其普遍适用性，这项工作将为将NLP验证问题纳入神经网络验证比赛开辟新的可能性，并在NLP问题中普及这一方向。 |
| [^26] | [Beyond Rule-based Named Entity Recognition and Relation Extraction for Process Model Generation from Natural Language Text.](http://arxiv.org/abs/2305.03960) | 本文扩展了PET数据集，通过聚类流程实体的提及，提出了一种新的基线技术流程提取方法，该方法避免了手动创建业务流程模型的繁琐工作，同时解决了同一流程实体重复提及的歧义问题。 |
| [^27] | [Machine-Learning-Based Classification of GPS Signal Reception Conditions Using a Dual-Polarized Antenna in Urban Areas.](http://arxiv.org/abs/2305.03956) | 本文介绍了一种基于机器学习的双极化天线分类GPS信号接收状况的方法，用于改善城市地区的定位性能。 |
| [^28] | [Learning Action Embeddings for Off-Policy Evaluation.](http://arxiv.org/abs/2305.03954) | 本论文探讨了从记录数据中学习动作嵌入，以减少在大型动作空间中反向倾向评分（IPS）估计器的方差，同时提高离线评估的准确性。 |
| [^29] | [Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation.](http://arxiv.org/abs/2305.03942) | 论文介绍了一种名为HACMan的强化学习方法，用于使用点云观察进行6D非抓取式操作的物体操纵。HACMan重点关注物体中心动作表示，它包括从物体点云中选择接触位置和一组描述机器人在接触后如何移动的运动参数。在实际测试中，HACMan的表现明显优于现有基线方法。 |
| [^30] | [Adam-family Methods for Nonsmooth Optimization with Convergence Guarantees.](http://arxiv.org/abs/2305.03938) | 本文提出了一种新的双时间尺度框架，证明了其在温和条件下收敛性，该框架包括了各种流行的Adam家族算法，用于训练无平滑神经网络和应对重尾噪声的需求，并通过实验表明了其效率和鲁棒性。 |
| [^31] | [Improved Techniques for Maximum Likelihood Estimation for Diffusion ODEs.](http://arxiv.org/abs/2305.03935) | 本文提出了基于扩散ODEs最大似然估计的改进技术，包括速度参数化和方差减少技术等用于训练的技术，以及误差有界的高阶流匹配目标用于微调和截断正态去量化方法用于评估。 |
| [^32] | [Revisiting Lightweight Compiler Provenance Recovery on ARM Binaries.](http://arxiv.org/abs/2305.03934) | 本文在ARM二进制代码上使用浅层学习模型，高效和准确地恢复编译器配置属性。我们应用了已经在x86-64二进制代码上有效的操作码和寄存器派生特征，并将这项工作与使用深度学习的与架构无关的模型进行比较，证明轻量级特征在ARM二进制代码上是可重现的。 |
| [^33] | [Active Continual Learning: Labelling Queries in a Sequence of Tasks.](http://arxiv.org/abs/2305.03923) | 本文考虑了一系列主动学习任务的主动连续学习问题，研究了不同场景下多种主动和连续学习算法之间的有效性和相互作用，并提出了遗忘-学习曲线方法来平衡不忘旧知识和快速学习的两个目标。 |
| [^34] | [Automated Spatio-Temporal Graph Contrastive Learning.](http://arxiv.org/abs/2305.03920) | 本文提出了一种自动化时空图对比学习模型来解决基于图的区域关系学习模型在处理普遍存在的数据噪声、缺失和分布异质性方面的挑战。 |
| [^35] | [Variational Nonlinear Kalman Filtering with Unknown Process Noise Covariance.](http://arxiv.org/abs/2305.03914) | 该论文介绍了一种处理非线性状态空间模型中未知过程噪声协方差问题的变分非线性卡尔曼滤波方法，通过引入辅助变量和推导出增广状态空间模型来简化估计问题，并扩展到处理任意非线性观测方程，具有更广泛的应用性。 |
| [^36] | [Synthesizing PET images from High-field and Ultra-high-field MR images Using Joint Diffusion Attention Model.](http://arxiv.org/abs/2305.03901) | 本文提出了一种方法，使用联合扩散注意力模型来合成PET图像，以解决PET成像成本高且涉及辐射暴露的缺陷。该方法相比现有方法具有更好的性能和噪声敏感度更低。 |
| [^37] | [Rethinking Class Imbalance in Machine Learning.](http://arxiv.org/abs/2305.03900) | 本研究提出了一个更广泛的类别不平衡分类方式，包括四种不平衡类型，并介绍了两种不平衡级别，使用理论分析证明了新的不平衡类型对学习公平性的重要影响。 |
| [^38] | [NL-CS Net: Deep Learning with Non-Local Prior for Image Compressive Sensing.](http://arxiv.org/abs/2305.03899) | NL-CS Net是一种结合传统优化方法的可解释性和基于网络方法的速度的新型CS方法，使用非局部先验，在重构质量和复杂度方面优于现有的最先进方法。 |
| [^39] | [Twin support vector quantile regression.](http://arxiv.org/abs/2305.03894) | TSVQR能够捕捉现代数据中的异质和不对称信息，并有效地描述了所有数据点的异质分布信息。通过构造两个较小的二次规划问题，TSVQR生成两个非平行平面，测量每个分位数水平下限和上限之间的分布不对称性。在多个实验中，TSVQR优于以前的分位数回归方法。 |
| [^40] | [Approximation by non-symmetric networks for cross-domain learning.](http://arxiv.org/abs/2305.03890) | 本文研究使用非对称内核进行基于内核网络逼近的通用方法，结果表明它可以在跨域学习中显著提高基于内核网络的逼近能力。 |
| [^41] | [On High-dimensional and Low-rank Tensor Bandits.](http://arxiv.org/abs/2305.03884) | 本研究提出了一个通用的张量赌博机模型，其中行动和系统参数由张量表示，着重于未知系统张量为低秩的情况。所开发的 TOFU 算法首先利用灵活的张量回归技术估计与系统张量相关联的低维子空间，然后将原始问题转换成一个具有系统参数范数约束的新问题，并采用范数约束赌博子例程解决。 |
| [^42] | [Learning Stochastic Dynamical System via Flow Map Operator.](http://arxiv.org/abs/2305.03874) | 该论文提出了一种通过测量数据学习未知随机动力学系统的数值框架随机流映射学习（sFML），在不同类型的随机系统上进行的全面实验证明了 sFML 的有效性。 |
| [^43] | [Knowledge Transfer from Teachers to Learners in Growing-Batch Reinforcement Learning.](http://arxiv.org/abs/2305.03870) | 本研究讨论了实际领域中离线训练或增长批量训练的限制，提出了一种教师向学习者进行知识转移的方法，使得数据数量和多样性得到提高。 |
| [^44] | [Spiking neural networks with Hebbian plasticity for unsupervised representation learning.](http://arxiv.org/abs/2305.03866) | 本文介绍了一种新的脉冲神经网络模型，实现了从数据中无监督地学习分布式内部表示。该模型采用在线Hebbian-Bayesian学习和重连机制，并表现出和传统的非脉冲神经网络相近的性能。 |
| [^45] | [Software-based Automatic Differentiation is Flawed.](http://arxiv.org/abs/2305.03863) | 软件实现的自动微分存在不可控的计算误差。 |
| [^46] | [Open problems in causal structure learning: A case study of COVID-19 in the UK.](http://arxiv.org/abs/2305.03859) | 本文研究了因果机器学习在COVID-19英国疫情数据中的应用挑战，探讨了不同数据格式对学习类别不同的算法的影响，并突出了因果结构学习中的未解问题和未来研究方向。 |
| [^47] | [Data-Free Learning of Reduced-Order Kinematics.](http://arxiv.org/abs/2305.03846) | 本研究提出了一种无数据学习降阶运动学的方法，能够自动识别物理系统的低维子空间。 |
| [^48] | [Spatiotemporal Transformer for Stock Movement Prediction.](http://arxiv.org/abs/2305.03835) | 使用时空变换器-LSTM模型进行股票运动预测，取得了高准确率，并在模拟中显示出比标准普尔500股票指数更高的收益率。 |
| [^49] | [Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models.](http://arxiv.org/abs/2305.03829) | 本研究采用贝叶斯深度学习估计多种治疗的因果后验分布，提高了基于图像的精准医疗的不确定性估计方法，以预测噪声多的医疗环境下的个体治疗效果。 |
| [^50] | [Uncertainty-Aware Bootstrap Learning for Joint Extraction on Distantly-Supervised Data.](http://arxiv.org/abs/2305.03827) | 本文提出了基于不确定性感知的Bootstrap学习用于远程监督数据联合抽取。通过探索数据不确定性和自我集成正则化器，使得模型在早期快速收敛并且缓解了噪声标签产生的模型不确定性，并在两个大型数据集上实验表明该方法在实体对抽取和关系抽取方面的F1得分分别提高了4.43%和4.92%。 |
| [^51] | [No-Regret Constrained Bayesian Optimization of Noisy and Expensive Hybrid Models using Differentiable Quantile Function Approximations.](http://arxiv.org/abs/2305.03824) | 本文提出了一种新颖的算法，CUQB，来解决复合函数（混合模型）的高效约束全局优化问题，并取得了良好的效果，在合成和真实的应用程序中均得到了验证，包括进行了最优控制的流体流量和拓扑结构优化，后者比当前最先进的设计强2倍。 |
| [^52] | [Deep Labeling of fMRI Brain Networks.](http://arxiv.org/abs/2305.03814) | 本文提出了一种可靠、快速的深度学习方法来标记大规模功能连接模式(静息状态网络)，其可用于术前规划指导神经外科医生，具有较强的泛化性和高准确度。 |
| [^53] | [Equivariant Neural Networks for Spin Dynamics Simulations of Itinerant Magnets.](http://arxiv.org/abs/2305.03804) | 本论文提出了一种新的等变神经网络架构来模拟Kondo晶格模型的大规模自旋动力学，能够重现三角晶格中天空子晶体相变的能力。 |
| [^54] | [Materials Informatics: An Algorithmic Design Rule.](http://arxiv.org/abs/2305.03797) | 材料信息学是材料科学研究中的第四个范式，通过材料特性指纹和统计推断学习理论，结合神经网络拓扑、逻辑公理和推理信息科学等方法，可以实现有机半导体的新型发现和知识提取。 |
| [^55] | [Towards Zero-Shot Frame Semantic Parsing with Task Agnostic Ontologies and Simple Labels.](http://arxiv.org/abs/2305.03793) | 本文提出了一种名为OpenFSP的框架，可以通过一些简单的标签方便地创建新领域，并在给定注释后，利用句子编码器的匹配算法预测由终端用户定义的领域的意图和插槽，从而解决了当前模型需要大量训练数据的问题。 |
| [^56] | [Neural Exploitation and Exploration of Contextual Bandits.](http://arxiv.org/abs/2305.03784) | 本文提出了一种新型的神经网络策略"EE-Net"，它用于多臂赌博机的利用和探索，在学习奖励函数的同时也适应性地学习潜在收益。 |
| [^57] | [Physics-Informed Localized Learning for Advection-Diffusion-Reaction Systems.](http://arxiv.org/abs/2305.03774) | 提出一种新的物理知识的、边界条件感知的、局部化学习方法，将E2C和E2CO模型扩展到对流扩散反应系统中，可以以极高的准确性预测系统未来状态，同时大幅减少训练时间。 |
| [^58] | [Weakly-Supervised Anomaly Detection in the Milky Way.](http://arxiv.org/abs/2305.03761) | 本文使用弱监督异常检测方法CWoLa来识别银河系中的冷凝聚流，该方法可以在不需要标记流或了解天体物理原理的情况下工作。该方法适用于识别局部异常的领域，并在天体物理学中具有广泛的适用性。 |
| [^59] | [Learning Sentinel-2 reflectance dynamics for data-driven assimilation and forecasting.](http://arxiv.org/abs/2305.03743) | 本篇论文提出了基于Koopman算子的无监督深度学习模型，用于学习长期反射动力学，该模型可以作为数据同化的先验，数据集公开发布，适用于Sentinel-2多光谱图像时间序列。 |
| [^60] | [Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming.](http://arxiv.org/abs/2305.03742) | 本文提出一种新的可微分符号推理框架，DSR-LM，用于提高预训练语言模型的逻辑推理能力，不像以往的研究依赖手工制定的逻辑规则，该框架有效地学习加权规则，并应用语义损失进一步改善LMs的逻辑推理能力。 |
| [^61] | [AmGCL: Feature Imputation of Attribute Missing Graph via Self-supervised Contrastive Learning.](http://arxiv.org/abs/2305.03741) | AmGCL是一个用于处理属性缺失图数据的自监督对比学习框架，可以通过特征插补和潜在表示学习来解决属性图中节点属性缺失的问题。 |
| [^62] | [Judge Me in Context: A Telematics-Based Driving Risk Prediction Framework in Presence of Weak Risk Labels.](http://arxiv.org/abs/2305.03740) | 该研究提出了一种基于遥感数据的驾驶风险预测框架，最大化利用情境信息，采用新颖的数据驱动过程增强弱的风险标签以提高分类器性能。 |
| [^63] | [Neural Architecture Search for Intel Movidius VPU.](http://arxiv.org/abs/2305.03739) | 本文提出了面向英特尔Movidius VPU的神经架构搜索技术，并通过引入硬件成本模型，实现了在VPU上进行分类任务和超分辨率任务的加速和提高。 |
| [^64] | [Tuning Traditional Language Processing Approaches for Pashto Text Classification.](http://arxiv.org/abs/2305.03737) | 本研究建立了一个普什图语自动文本分类系统，通过比较不同模型和特征提取方法，实验结果表明使用tf-idf特征提取方法的SVM模型在普什图语文本分类中取得了最高的准确率。 |
| [^65] | [Score-based Transport Modeling for Mean-Field Fokker-Planck Equations.](http://arxiv.org/abs/2305.03729) | 本文介绍了基于评分的输运建模方法（MSBTM）在求解平均场福克-普朗克方程中的应用。通过建立Kullback-Leibler散度时间导数的上界，验证了MSBTM方法的有效性。通过对MSBTM解、相应随机微分方程的积分结果和解析解（如果有）的定性和定量比较，验证了MSBTM算法的可行性。 |
| [^66] | [On Preimage Approximation for Neural Networks.](http://arxiv.org/abs/2305.03686) | 本文提出了一种基于线性松弛的高效实用的任意时刻算法，用于生成神经网络原像的符号下近似，以实现更快的改进和更高的压缩度。 |
| [^67] | [White-Box Multi-Objective Adversarial Attack on Dialogue Generation.](http://arxiv.org/abs/2305.03655) | 该论文提出了一种新的白盒多目标对话生成对抗攻击方法，DGSlow。通过平衡生成准确性和长度两个目标，DGSlow利用生成更长的输出来提高攻击效果。 |
| [^68] | [Contrastive losses as generalized models of global epistasis.](http://arxiv.org/abs/2305.03136) | 对比损失函数可以用于提取全局上位联系模型所隐含的稀疏潜在函数，这种方法可能为蛋白质工程和相关领域的适应性函数推断提供有用的通用框架。 |
| [^69] | [Beyond Homophily: Reconstructing Structure for Graph-agnostic Clustering.](http://arxiv.org/abs/2305.02931) | 本论文提出了一种新颖的图形聚类方法，能够超越同质性假设，重构结构实现对图形无关聚类，包括三个关键组件：图形重构、混合滤波器和双图形聚类网络。为了减少节点属性和拓扑结构之间的不良耦合，我们将它们分别映射到两个子空间中。 |
| [^70] | [Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents.](http://arxiv.org/abs/2305.02412) | 本文介绍了Plan，Eliminate，和Track（PET）框架，该框架利用预先训练的大型语言模型（LLM）帮助智能体简化控制任务，从而解决了LLM直接作为智能体所面临的一些限制和问题。 |
| [^71] | [Using Language Models on Low-end Hardware.](http://arxiv.org/abs/2305.02350) | 本论文评估了在低端硬件上使用固定语言模型来训练文本分类网络的可行性，并发现在某些情况下，不对语言模型进行微调可以在更快的训练中产生竞争性的效果，仅需要原先内存的四分之一即可。 |
| [^72] | [Deep Graph Representation Learning and Optimization for Influence Maximization.](http://arxiv.org/abs/2305.02200) | 该论文提出了一个名为DeepIM的新框架，用于解决影响力最大化中的困难问题，具有更强的泛化能力和适应能力。 |
| [^73] | [Commentary on explainable artificial intelligence methods: SHAP and LIME.](http://arxiv.org/abs/2305.02012) | 这篇评论对可解释人工智能方法 SHAP 和 LIME 进行了评述和比较，提出了一个框架且突出了它们的优缺点。 |
| [^74] | [Extraction of volumetric indices from echocardiography: which deep learning solution for clinical use?.](http://arxiv.org/abs/2305.01997) | 本文对当前医学/超声心动图图像分割方法进行了全面比较，提出了3D nnU-Net模型，解决了时间一致性和跨数据集方面的问题，并通过引入一个新的私有数据集，CARDINAL，来证明其在应用于临床中的优越性。 |
| [^75] | [AQ-GT: a Temporally Aligned and Quantized GRU-Transformer for Co-Speech Gesture Synthesis.](http://arxiv.org/abs/2305.01241) | 本文提出了一种使用生成对抗网络和量化流水线对部分手势序列进行预训练并基于潜在空间的生成方法，从而实现高度逼真、富有表现力、并避免了伪像的手势生成。 |
| [^76] | [Performative Prediction with Bandit Feedback: Learning through Reparameterization.](http://arxiv.org/abs/2305.01094) | 本文提出一种新的在线反馈的实现式预测框架，解决了在模型部署自身改变数据分布的情况下优化准确性的问题。 |
| [^77] | [Multi-scale Transformer-based Network for Emotion Recognition from Multi Physiological Signals.](http://arxiv.org/abs/2305.00769) | 本文提出了一种用于从生理数据中进行情感识别的多尺度Transformer网络，采用了多模态技术和缩放数据，结合Transformer和高斯变换技术以提高信号编码的有效性，并在EPiC竞赛的CASE数据集上取得了不错的结果。 |
| [^78] | [Optimizing Privacy, Utility and Efficiency in Constrained Multi-Objective Federated Learning.](http://arxiv.org/abs/2305.00312) | 该论文提供了一种在有限制的多目标联邦学习中优化隐私、效用和效率的方法，开发了两种改进的算法来解决隐私泄露、效用损失和训练成本等三个主要目标，并在两个真实世界的数据集上进行了实验验证，优于现有方法。 |
| [^79] | [Causal Reasoning and Large Language Models: Opening a New Frontier for Causality.](http://arxiv.org/abs/2305.00050) | 大型语言模型在因果推理任务中取得了新的最高准确率，但是其鲁棒性仍然存在难以预测的失败模式。 |
| [^80] | [Energy-based Models as Zero-Shot Planners for Compositional Scene Rearrangement.](http://arxiv.org/abs/2304.14391) | 本文提出一种基于能量模型的零样本场景重新排列规划器，通过语言指导的空间概念来实现长指令以及在训练时从未见过的空间概念组合。本文的模型在指令导向操作基准测试以及组合指令基准测试中表现良好，优于基于语言表达的最先进方法，并且可以成功地解决之前从未见过的复杂指令和场景。 |
| [^81] | [Mixtures of Gaussian process experts based on kernel stick-breaking processes.](http://arxiv.org/abs/2304.13833) | 提出了一种新的基于核棍棒过程的高斯过程专家混合模型，能够维持直观吸引力并提高模型性能，具有实用性。 |
| [^82] | [A mean-field games laboratory for generative modeling.](http://arxiv.org/abs/2304.13534) | 本文提出了使用均场博弈作为实验室对生成模型进行设计和分析的方法，并建立了这种方法与主要流动和扩散型生成模型之间的关联。通过研究每个生成模型与它们相关的 MFG 的最优条件，本文提出了一个基于双人 MFG 的新的生成模型，该模型在提高样本多样性和逼真度的同时改善了解缠结和公平性。 |
| [^83] | [From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping.](http://arxiv.org/abs/2304.13273) | 本研究提出了一种从关联到生成的零-shot方法：通过将图像/视频投影到语言模态并在生成任务中生成描述性字幕。该方法在多个基准数据集上显著优于现有的最先进方法，为无监督跨模态映射提供了一个新的视角，并具有在视频字幕，图像合成和文本到图像生成等领域的潜在应用。 |
| [^84] | [Alternating Local Enumeration (TnALE): Solving Tensor Network Structure Search with Fewer Evaluations.](http://arxiv.org/abs/2304.12875) | 提出了TnALE算法，通过交替局部枚举更新每个与结构相关的变量，大大减少了评估次数，用于解决张量网络结构搜索问题。在理论上证明，如果在每个邻域中达到了足够的目标函数降低，TnALE和TNLS都可以实现线性收敛度，直到一个常数。同时，与TNLS相比， TnALE需要更少的评估次数。 |
| [^85] | [RenderDiffusion: Text Generation as Image Generation.](http://arxiv.org/abs/2304.12519) | 本文提出了一种新的扩散方法——\textsc{RenderDiffusion}，通过文本引导的图像生成进行文本生成。它将连续扩散模型应用于离散文本并实现了条件文本生成作为字形图像生成问题。 |
| [^86] | [Medical Image Deidentification, Cleaning and Compression Using Pylogik.](http://arxiv.org/abs/2304.12322) | 提出了一个Python框架下的库PyLogik来帮助超声图像去标识化和清洗压缩，为深度学习和数据共享应用提供图像数据支持。 |
| [^87] | [Controlled physics-informed data generation for deep learning-based remaining useful life prediction under unseen operation conditions.](http://arxiv.org/abs/2304.11702) | 本研究提出了一种新的混合框架，使用可控物理信息的生成模型生成可信、多样的数据来预测设备的使用寿命。其中使用了基本物理约束条件，以保证数据的真实性。 |
| [^88] | [Domain Generalization for Mammographic Image Analysis via Contrastive Learning.](http://arxiv.org/abs/2304.10226) | 研究人员开发出一种基于对比学习的域泛化方法，通过自监督学习生成多种风格和视角的特征嵌入，进一步微调骨干网络以提高分类任务性能。 |
| [^89] | [Towards Better Evaluation of GNN Expressiveness with BREC Dataset.](http://arxiv.org/abs/2304.07702) | 本论文介绍了一个新的Benchmark for Evaluating the Robustness of GNNs to Topological Changes (BREC)数据集，并使用BREC评估了几种现有的GNN模型的表达力，表明一些模型在以前的基准测试中表现良好，但在BREC上遇到了困难，突显了需要更好的GNN表现力评估的必要性。 |
| [^90] | [Repeated Principal-Agent Games with Unobserved Agent Rewards and Perfect-Knowledge Agents.](http://arxiv.org/abs/2304.07407) | 本文提出了一个利用多臂老虎机框架结构来处理未知代理奖励的策略，并证明了其性能是渐进最优的。 |
| [^91] | [Selecting Robust Features for Machine Learning Applications using Multidata Causal Discovery.](http://arxiv.org/abs/2304.05294) | 本文提出了一种多数据因果特征选择方法，它可以同时处理一组时间序列数据集，生成一个单一的因果驱动集，并且可以过滤掉因果虚假链接，最终输入到机器学习模型中预测目标。 |
| [^92] | [REDf: A Renewable Energy Demand Forecasting Model for Smart Grids using Long Short Term Memory Network.](http://arxiv.org/abs/2304.03997) | 本文提出了一种基于长短期记忆网络的智能电网可再生能源需求预测模型REDf，可以提供准确的能量需求预测，改善可再生能源的集成，实验结果表明其准确度优于其他模型。 |
| [^93] | [Natural Selection Favors AIs over Humans.](http://arxiv.org/abs/2303.16200) | 这篇论文探讨了随着人工智能的发展，其可能会出现不良特性并逐渐超越人类智能的问题，以及这对人类未来的控制权产生的影响。 |
| [^94] | [SmartBERT: A Promotion of Dynamic Early Exiting Mechanism for Accelerating BERT Inference.](http://arxiv.org/abs/2303.09266) | SmartBERT是一种改进的动态早期退出与层跳过机制，可以自适应地跳过一些层并自适应地选择是否退出，以加速BERT模型的推理速度。 |
| [^95] | [Generative Adversarial Networks for Scintillation Signal Simulation in EXO-200.](http://arxiv.org/abs/2303.06311) | 本文介绍了一种基于生成对抗网络的新方法，用于从EXO-200实验的时间投影室中模拟光电探测器信号。该方法能够比传统的模拟方法快一个数量级地产生高质量的模拟波形，并且能够从训练样本中推广并识别数据的显著高级特征。 |
| [^96] | [On the Fusion Strategies for Federated Decision Making.](http://arxiv.org/abs/2303.06109) | 本文研究了联邦决策制定中的信息聚合问题，分析了非贝叶斯社会学习策略并比较了中央处理器采用算术平均和几何平均的聚合策略。结果确认两种汇集策略都可以导致系统的渐近正态性特征描述。 |
| [^97] | [DR-VIDAL -- Doubly Robust Variational Information-theoretic Deep Adversarial Learning for Counterfactual Prediction and Treatment Effect Estimation on Real World Data.](http://arxiv.org/abs/2303.04201) | DR-VIDAL是一个新型的生成框架，可用于处理真实世界数据中的干预措施对结果的因果效应估计，并具有处理混淆偏差和模型不良的能力。 |
| [^98] | [Steering Graph Neural Networks with Pinning Control.](http://arxiv.org/abs/2303.01265) | 本文提出了一种用固定控制来引导图神经网络学习的方法，用类中心来监督节点特征的学习，以解决半监督学习中具有不连续分布类标签的节点的特征表示学习问题。 |
| [^99] | [PaGE-Link: Path-based Graph Neural Network Explanation for Heterogeneous Link Prediction.](http://arxiv.org/abs/2302.12465) | 这篇论文提出了一种基于路径的GNN解释方法PaGE-Link，用于异构链接预测，具有连接可解释性，模型可扩展性和处理图形异构性能力。 |
| [^100] | [Time-varying Signals Recovery via Graph Neural Networks.](http://arxiv.org/abs/2302.11313) | 本文提出了一种基于图神经网络的时变信号恢复方法，通过编码器-解码器架构与Sobolev平滑算子组成的专门损失函数，可以有效地恢复时变图信号。 |
| [^101] | [Auto.gov: Learning-based On-chain Governance for Decentralized Finance (DeFi).](http://arxiv.org/abs/2302.09551) | 这项研究提出了一个“Auto.gov”框架，可增强去中心化金融（DeFi）的安全性和降低受攻击的风险。该框架利用深度Q-网络（DQN）强化学习方法，提出了半自动的、直观的治理提案，并量化了其理由，使系统能够有效地应对恶意行为和意外的市场情况。 |
| [^102] | [Tensor Networks Meet Neural Networks: A Survey and Future Perspectives.](http://arxiv.org/abs/2302.09019) | 这篇论文评述了张量网络和神经网络并介绍了它们的结合：张量神经网络(TNN)，探讨了TNN在网络压缩、信息融合和量子启发式神经网络构建方面的优缺点和未来研究方向。 |
| [^103] | [Multimodal Federated Learning via Contrastive Representation Ensemble.](http://arxiv.org/abs/2302.08888) | 本文提出了一种名为CreamFL的多模态联邦学习框架，可以从具有异构模型架构和数据模态的客户端中训练更大的服务器模型，同时只在公共数据集上传递知识。 |
| [^104] | [Approximately Bayes-Optimal Pseudo Label Selection.](http://arxiv.org/abs/2302.08883) | 本文介绍了BPLS，一种用于PLS的贝叶斯框架，通过解析逼近选择标签实例的标准，以避免由过度自信但错误预测的实例选择而导致的确认偏差问题。 |
| [^105] | [FOSI: Hybrid First and Second Order Optimization.](http://arxiv.org/abs/2302.08484) | FOSI是一种元算法，它在优化过程中有效地加入二阶信息以提高任何一阶优化器的性能，并可改善一类优化器的条件数 |
| [^106] | [Discovering sparse hysteresis models for smart materials.](http://arxiv.org/abs/2302.05313) | 本文提出了一种利用机器学习中的稀疏回归技术建模智能材料滞后的方法，并成功对压电材料的滞后现象进行建模和预测。同时在磁性材料方面提供了稀疏白盒建模滞后的见解。 |
| [^107] | [The Re-Label Method For Data-Centric Machine Learning.](http://arxiv.org/abs/2302.04391) | 本文提出了一种重新标签的方法来解决手动标记的数据中存在噪声的问题，并通过模型预测来辅助人类标记噪声数据。实验证明此方法适用于多类深度学习任务。 |
| [^108] | [Generalization of Deep Reinforcement Learning for Jammer-Resilient Frequency and Power Allocation.](http://arxiv.org/abs/2302.02250) | 本研究提出了一种基于深度强化学习的干扰器抗干扰频率和功率分配的泛化方法，可以在不同的无线网络操作场景下使用。在实现上，我们证明了该方法可以在保证高效频谱使用的同时对干扰具有很强的抗干扰性。 |
| [^109] | [Dual PatchNorm.](http://arxiv.org/abs/2302.01327) | 双重 PatchNorm 在 Vision Transformers 中的应用有助于提高准确性。 |
| [^110] | [Double Permutation Equivariance for Knowledge Graph Completion.](http://arxiv.org/abs/2302.01313) | 本研究提出了双排列等变性的KG表示方法，可以使神经网络在KG中执行复杂的逻辑推理任务，并在多个归纳KG完成任务中实现了最先进的Hits@10测试准确率。双排列等变性在KG中开辟了新的研究方向。 |
| [^111] | [Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities.](http://arxiv.org/abs/2302.01018) | 该论文总结了时态图的图神经网络的现状、挑战和机遇，提供了学习设置和任务的严格规范化以及一个新的分类法，并讨论了该领域最相关的开放挑战，从研究和应用角度讨论。 |
| [^112] | [Robust online active learning.](http://arxiv.org/abs/2302.00422) | 本文提出了一种自适应方法，用于鲁棒的在线主动学习，并在受污染的数据流中证明了其性能表现优异，同时确保了稳定性并减少异常值的负面影响。 |
| [^113] | [Combinatorial Causal Bandits without Graph Skeleton.](http://arxiv.org/abs/2301.13392) | 本文研究了在二值一般因果模型和BGLMs上不考虑图骨架的组合因果赌博机问题，提出了可在BGLMs上实现的无需图骨架的遗憾最小化算法，达到了与依赖于图结构的最先进算法相同的渐进遗憾率$O(\sqrt{T}\ln T)$。 |
| [^114] | [Online discovering governing differential equations of evolving systems from streaming data.](http://arxiv.org/abs/2301.07863) | 本研究提出了一种在线建模方法，逐一处理流数据，可以有效地识别演化系统的微分方程，尤其是变化后的系统产生的测量分布与以前不同的情况下。 |
| [^115] | [RedMule: A Mixed-Precision Matrix-Matrix Operation Engine for Flexible and Energy-Efficient On-Chip Linear Algebra and TinyML Training Acceleration.](http://arxiv.org/abs/2301.03904) | 该论文介绍了一种名为RedMule的低功耗混合精度矩阵计算引擎，它支持多种精度和格式，用于解决近传感器训练的能耗问题。 |
| [^116] | [Heterogeneous Graph Contrastive Learning with Meta-path Contexts and Weighted Negative Samples.](http://arxiv.org/abs/2212.13847) | 本论文提出了MEOW，使用元路径的上下文信息和加权负样本，通过粗略和细粒度视图对异构图进行了对比学习。 |
| [^117] | [On Event Individuation for Document-Level Information Extraction.](http://arxiv.org/abs/2212.09702) | 提出了问题──事件个体化对于模板填充任务是否适用，通过注释研究和误差分析，我们发现这引发了对模板填充度量的有效性、任务数据集的质量以及模型学习能力的担忧。 |
| [^118] | [Reasoning with Language Model Prompting: A Survey.](http://arxiv.org/abs/2212.09597) | 本文提供了使用语言模型提示进行推理的前沿研究综合调查。讨论了新兴推理能力出现的潜在原因，并提供系统资源帮助初学者。 |
| [^119] | [Machine Learning Systems are Bloated and Vulnerable.](http://arxiv.org/abs/2212.09437) | 本文研究了机器学习容器中存在的臃肿问题，并开发了MMLB框架进行分析和量化，结果表明在某些情况下，臃肿会占据容器总大小的80％， 显著增加了容器的供应时间，最多增加了370％，且导致漏洞恶化最高达99％。 |
| [^120] | [Computing linear sections of varieties: quantum entanglement, tensor decompositions and beyond.](http://arxiv.org/abs/2212.03851) | 本文研究了计算代数曲面与线性子空间之间交集的问题，提出了一个算法可以有效地处理这个问题，并可以找到所有属于该交集的元素。 |
| [^121] | [Diffusion-SDF: Text-to-Shape via Voxelized Diffusion.](http://arxiv.org/abs/2212.03293) | 提出了一种新型生成式三维建模框架Diffusion-SDF，用于文本生成三维形状综合任务，采用SDF autoencoder和体素扩散模型学习和生成三维形状的体素化符号距离场（SDF）的表示形式，能够生成高度多样化的三维形状以符合给定的文本描述。 |
| [^122] | [Deep Signature Algorithm for Multi-dimensional Path-Dependent Options.](http://arxiv.org/abs/2211.11691) | 本文提出了一种适用于欧式型和美式型期权定价问题的多维路径依赖期权深度签名算法，解决了路径依赖型FBSDE问题，数值算法的收敛性得到保证。 |
| [^123] | [Integrated Space Domain Awareness and Communication System.](http://arxiv.org/abs/2211.10260) | 本文提出了一个集成的SDA和通信(ISDAC)系统，用于攻击者检测，通过开发一个轻量级的卷积神经网络架构来跟踪随机模式并满足SDA要求，系统在12种不同的攻击者配置下达到了97.8%以上的检测精度。 |
| [^124] | [Dual Class-Aware Contrastive Federated Semi-Supervised Learning.](http://arxiv.org/abs/2211.08914) | 本论文提出了一种双类别感知对比联邦半监督学习方法，该方法考虑了数据的本地类别感知分布和全局类别感知分布，通过实现双类别感知对比模块，提高了联邦学习的性能表现。 |
| [^125] | [Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations.](http://arxiv.org/abs/2211.08794) | 本文提出了一种利用多视角压缩表示降低预训练语言模型微调过程中过拟合问题的方法，经过测试在低资源NLP任务中表现良好。 |
| [^126] | [A mixed-categorical correlation kernel for Gaussian process.](http://arxiv.org/abs/2211.08262) | 提出一种新的混合类别相关核的高斯过程代理，相较于其他现有模型在分析和工程问题上表现更好。 |
| [^127] | [Agent-State Construction with Auxiliary Inputs.](http://arxiv.org/abs/2211.07805) | 本文提出了一种基于信息瓶颈原理的方法，将辅助输入纳入到Agent-State构建过程中，以构建一个代理状态，总结与世界的先前交互，有效地解决了部分观测性问题。 |
| [^128] | [Knowledge Base Completion using Web-Based Question Answering and Multimodal Fusion.](http://arxiv.org/abs/2211.07098) | 本文提出了一种使用基于网络问答和多模态融合的方法填补知识库中的缺失信息。通过设计一个多模态特征和问题模板的基于网络问答的系统来达到更高效的知识库补全，同时结合了知识库中的结构化信息来提高抽取质量。 |
| [^129] | [CACTO: Continuous Actor-Critic with Trajectory Optimization -- Towards global optimality.](http://arxiv.org/abs/2211.06625) | 本研究提出了一种连续动态系统的控制算法，结合了轨迹优化和强化学习，通过TO引导的RL策略搜索学习控制策略来避免在控制过程中陷入贫乏的局部最优解。 |
| [^130] | [Data Models for Dataset Drift Controls in Machine Learning With Optical Images.](http://arxiv.org/abs/2211.02578) | 该研究利用物理光学和传统机器学习的组合，获得明确且可微分的数据模型，以更好地理解数据漂移对机器学习模型性能的影响。 |
| [^131] | [DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models.](http://arxiv.org/abs/2211.01095) | DPM-Solver++是一种快速求解器，在扩散概率模型的引导采样中表现出色，可加快样本生成速度。 |
| [^132] | [Investigating Content-Aware Neural Text-To-Speech MOS Prediction Using Prosodic and Linguistic Features.](http://arxiv.org/abs/2211.00342) | 本文提出探究声调和语言特征如何影响神经文本朗读自然度预测，通过加入这些特征，相较于基于频谱特征的基线模型 MOSNet，在 MOS 预测中取得了平均12%无公害提高。 |
| [^133] | [Learning-Augmented Private Algorithms for Multiple Quantile Release.](http://arxiv.org/abs/2210.11222) | 本文提出一种新的隐私保护方法：使用学习增强算法框架，为多分位数发布任务提供可扩展的预测质量误差保证。 |
| [^134] | [MMRNet: Improving Reliability for Multimodal Object Detection and Segmentation for Bin Picking via Multimodal Redundancy.](http://arxiv.org/abs/2210.10842) | 本文提出了一个基于多模态冗余的可靠的抓取商品物体检测和分割系统MMRNet，能够在传感器失效等异常情况下具有更好的鲁棒性，并在YCB-Video数据集上取得了最先进的性能。 |
| [^135] | [Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods.](http://arxiv.org/abs/2210.07321) | 本文主要调查了机器生成文本对社会和网络安全所带来的威胁，提供了最完整的机器生成文本检测方法评估，为应对威胁模型和解决检测问题提供了强有力的指导。 |
| [^136] | [CLIP-PAE: Projection-Augmentation Embedding to Extract Relevant Features for a Disentangled, Interpretable, and Controllable Text-Guided Face Manipulation.](http://arxiv.org/abs/2210.03919) | 提出了一种为了解决文本引导图像操纵中的可分离性、可解释性和可控性问题，通过定义基于相关提示的语料库子空间来获取特定图像特征并引入CLIP投影增强嵌入（PAE）作为优化目标处理的新方法。 |
| [^137] | [Investigating and Mitigating Failure Modes in Physics-informed Neural Networks (PINNs).](http://arxiv.org/abs/2209.09988) | 本文探究了PINN在解决PDE问题时所面临的困难，并提出了一种新的方法解决了高阶PDE污染反向传播梯度的问题，减小了搜索空间维度，使非平滑解的PDE学习成为可能。 |
| [^138] | [Efficient Quantized Sparse Matrix Operations on Tensor Cores.](http://arxiv.org/abs/2209.06979) | 本文提出了Magicube，一个基于张量核的低精度整数的高性能稀疏矩阵库，支持深度学习中的稀疏运算。在NVIDIA A100 GPU上实验表明，Magicube相对供应商优化库平均获得了1.44倍的速度提升，并且相对于状态-of-the-art的GPU库，速度提升了1.43倍。 |
| [^139] | [Real-to-Sim: Predicting Residual Errors of Robotic Systems with Sparse Data using a Learning-based Unscented Kalman Filter.](http://arxiv.org/abs/2209.03210) | 本文提出了一种基于学习的无迹卡尔曼滤波器用于预测机器人系统的残余误差，实现了使用少量数据建模这些残余误差的目标，进一步缩小了动态模型与真实机器人之间的差距。 |
| [^140] | [Reconciling Individual Probability Forecasts.](http://arxiv.org/abs/2209.01687) | 个体概率预测的协调方法可以通过对于个体概率的实证验证和改进，使得模型得到提升并在几乎所有情况下达成一致。 |
| [^141] | [Shortcut Learning of Large Language Models in Natural Language Understanding.](http://arxiv.org/abs/2208.11857) | 本文综述了大型语言模型中快捷学习和鲁棒性挑战的解决方法和相关研究，包括识别其快捷学习行为、原因和解决方案，并探讨了领域的主要研究挑战和潜在研究方向。 |
| [^142] | [Diffusion-based Time Series Imputation and Forecasting with Structured State Space Models.](http://arxiv.org/abs/2208.09399) | 本文提出了一种新的状态空间框架(SSSD)来插补时间序列数据中的缺失值和进行预测，在各种数据集和不同的缺失情况下，SSSD都表现出更好的性能，并可以有效处理黑屏缺失情况。 |
| [^143] | [Differentiable WORLD Synthesizer-based Neural Vocoder With Application To End-To-End Audio Style Transfer.](http://arxiv.org/abs/2208.07282) | 比较不同方法在音频风格转换中的应用，提出一种可微分的WORLD合成器，并通过声学特征参数来实现音高和音色信息的分离。 |
| [^144] | [Provable Defense Against Geometric Transformations.](http://arxiv.org/abs/2207.11177) | 提出了第一个可证明的确定性认证几何鲁棒性防御框架，利用了一种新颖的GPU优化验证器，能够快速地训练鲁棒的DNNs，并在多个数据集上 consistently achieve state-of-the-art deterministic certified geometric robustness 和干净准确度的效果。 |
| [^145] | [Wasserstein multivariate auto-regressive models for modeling distributional time series and its application in graph learning.](http://arxiv.org/abs/2207.05442) | 本文提出了一种新的自回归模型，用于分析多元分布时间序列。并且在Wasserstein空间中建模了随机对象，提供了该模型的解的存在性和一致估计器。此方法可以应用于年龄分布和自行车共享网络的观察数据。 |
| [^146] | [TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second.](http://arxiv.org/abs/2207.01848) | TabPFN是一种可以在不到一秒钟内完成小型表格数据集的监督分类的Transformer，无需超参数调整，并且具有竞争力。它使用先验适应网络（PFN）逼近基于先验的贝叶斯推断，先验融合了因果推理的思想。 |
| [^147] | [ChordMixer: A Scalable Neural Attention Model for Sequences with Different Lengths.](http://arxiv.org/abs/2206.05852) | ChordMixer 提出了一个简单的神经网络建模组，能够处理具有可变长度的长序列的注意力，并在实验中表现出明显的优势。 |
| [^148] | [Delving into the Openness of CLIP.](http://arxiv.org/abs/2206.01986) | 本研究探究了CLIP模型的开放性，并通过词汇扩展来评估模型的可扩展性。研究发现，类似于CLIP的模型并不真正开放，并且随着词汇表的扩展其性能会恶化。此外，研究还揭示了CLIP表示在不变性和特定性之间存在权衡。 |
| [^149] | [Composition of Relational Features with an Application to Explaining Black-Box Predictors.](http://arxiv.org/abs/2206.00738) | 本文提出了一种将关系特征视为函数，并使用通用函数组合的概念从简单函数推导出复杂函数的方法，将其应用于解释黑盒预测器的预测。 |
| [^150] | [Counterfactual Analysis in Dynamic Latent State Models.](http://arxiv.org/abs/2205.13832) | 该论文提供了一个框架来解决带有隐藏状态的动态模型的反事实分析问题，并在乳腺癌案例研究中成功应用。通过优化方法得到了反事实量的上限和下限，并且该论文是第一个在动态潜在状态模型中进行这种计算的研究。 |
| [^151] | [What Is Fairness? Philosophical Considerations and Implications For FairML.](http://arxiv.org/abs/2205.09622) | 本文探讨了公平性的哲学概念，提出了公平性和预测性能不是不可调和的对立面，并强调从数据收集到最终模型评估都需纳入伦理考虑。 |
| [^152] | [Are Your Reviewers Being Treated Equally? Discovering Subgroup Structures to Improve Fairness in Spam Detection.](http://arxiv.org/abs/2204.11164) | 本文通过发现组内子群结构，提高同等对待不同组的公正性、减少垃圾信息检测误差，对于提高评论者参与度和用户对评论网站的信任具有积极影响。 |
| [^153] | [Interval Bound Interpolation for Few-shot Learning with Few Tasks.](http://arxiv.org/abs/2204.03511) | 在微小样本学习中引入了区间界概念，通过最小化任务及其相应边界之间的距离来保留训练任务周围的领域，并通过插值来人为形成新任务进行训练。 |
| [^154] | [Regenerative Particle Thompson Sampling.](http://arxiv.org/abs/2203.08082) | 本文提出了再生粒子汤普森抽样（RPTS），通过重新生成适应的粒子来解决汤普森抽样中粒子权重收敛于零的问题。RPTS在代表性赌博问题中展现出了灵活性和效果的提升，包括对5G网络切片的应用。 |
| [^155] | [Nonlinear Isometric Manifold Learning for Injective Normalizing Flows.](http://arxiv.org/abs/2203.03934) | 使用等距自编码器的非线性等距流形学习模型可以简化模型选择和调整，并生成高质量的数据。 |
| [^156] | [Learning Group Importance using the Differentiable Hypergeometric Distribution.](http://arxiv.org/abs/2203.01629) | 本文提出了不同iable hypergeometric distribution，使用重要性学习方法来解决在许多应用程序中将一组元素划分为先验未知大小的子集的问题，并在弱监督学习和聚类方面优于以前的方法。 |
| [^157] | [Increasing Depth of Neural Networks for Life-long Learning.](http://arxiv.org/abs/2202.10821) | 本研究提出了一种终身学习方法，通过增加神经网络深度，以节点的方式部署不同任务的神经网络参数，实现前向传递和适应以前学过的表示，解决了渐进式神经网络在整个网络分配大量内存的问题。 |
| [^158] | [Recent Advances in Reliable Deep Graph Learning: Inherent Noise, Distribution Shift, and Adversarial Attack.](http://arxiv.org/abs/2202.07114) | 本文综合评述了深度图学习（DGL）的最新可靠性进展，其中涵盖了内在噪声和分布偏移等方面的相关内容，同时指出未来研究需要关注的问题。 |
| [^159] | [Multi-Label Classification of Thoracic Diseases using Dense Convolutional Network on Chest Radiographs.](http://arxiv.org/abs/2202.03583) | 本研究提出了一种基于密集卷积神经网络和GRADCAM的胸部X光疾病多标签诊断模型，获得了在Cardiomegaly条件下最高的AUC得分0.896，并使用热图提高了模型的可解释性。 |
| [^160] | [TPC: Transformation-Specific Smoothing for Point Cloud Models.](http://arxiv.org/abs/2201.12733) | 提出了一种名为TPC的转换特定平滑框架，可以提供点云模型对语义转换攻击的可扩展防御能力，并在常见数据集上进行了有效实验。 |
| [^161] | [Collaborative Learning in General Graphs with Limited Memorization: Complexity, Learnability, and Reliability.](http://arxiv.org/abs/2201.12482) | 本文提出了一个三阶段的轻量级随机游走算法，在有限的记忆和通讯带宽限制下完成了通用图上的协作学习，解决了连接图不完全和非良构的问题，并提高了学习过程的可靠性。 |
| [^162] | [Existence and Estimation of Critical Batch Size for Training Generative Adversarial Networks with Two Time-Scale Update Rule.](http://arxiv.org/abs/2201.11989) | 本文研究了使用两时间尺度更新规则（TTUR）训练生成式对抗网络（GAN）时批次大小与训练所需步骤数量之间的关系，理论上证明了为了找到稳定点，随着批次大小的增加所需步骤数量会减少并且存在一个最小化随机一阶预言机（SFO）复杂度的关键批次大小。 |
| [^163] | [Fairness Implications of Encoding Protected Categorical Attributes.](http://arxiv.org/abs/2201.11358) | 该研究比较了两种常用的编码方法-“one-hot编码”和“target编码”，并探讨了其对机器学习模型性能和公平性的影响。 |
| [^164] | [Asymptotic self-similar blow-up profile for three-dimensional axisymmetric Euler equations using neural networks.](http://arxiv.org/abs/2201.06780) | 本文运用物理信息神经网络（PINNs）首次发现二维布式内斯克方程和三维欧拉方程的光滑自相似爆破轮廓，这可以成为未来进行两个方程爆破的计算机辅助证明的基础。同时，我们的数值框架还可以成功地应用于求解流体方程的不稳定自相似解。 |
| [^165] | [CausalSim: A Causal Framework for Unbiased Trace-Driven Simulation.](http://arxiv.org/abs/2201.01811) | CausalSim提出了一种因果框架，通过学习系统动态和潜在因素的因果模型，消除追踪数据中的偏差，解决了当前追踪驱动仿真器的缺陷。 |
| [^166] | [A Unified and Constructive Framework for the Universality of Neural Networks.](http://arxiv.org/abs/2112.14877) | 本论文提出了神经网络普适性的建构框架，任何nAI激活函数都是普适的，该框架具有统一、构造性和新视角的优势。 |
| [^167] | [Learning Safety Filters for Unknown Discrete-Time Linear Systems.](http://arxiv.org/abs/2111.00631) | 本论文提出了一种基于学习的安全滤波器，针对带有未知模型和未知协方差的高斯噪声的离散时间线性时不变系统，通过收紧安全约束和构建鲁棒优化问题，以最小程度地修改名义控制动作，以高概率确保安全性。 |
| [^168] | [Least-Squares Neural Network (LSNN) Method For Scalar Nonlinear Hyperbolic Conservation Laws: Discrete Divergence Operator.](http://arxiv.org/abs/2110.10895) | 本文提出了一种最小二乘神经网络方法，用于求解双曲型守恒律，该方法使用离散散度算子，可用于逼近未知界面位置的不连续函数，并在保持不连续性解和实现高精度和高效率方面具有很好的效果。 |
| [^169] | [Sign and Relevance learning.](http://arxiv.org/abs/2110.07292) | 本文提出了一种新型网络，在整个网络中仅传播可塑性变化的符号，同时使用神经调制控制学习速率。研究结果表明该范例可以成功学习具有多层表示的复杂任务，学习速度和稳定性优于标准模型。 |
| [^170] | [GRAPE for Fast and Scalable Graph Processing and random walk-based Embedding.](http://arxiv.org/abs/2110.06196) | GRAPE是一种软件资源，用于处理大型图，并利用专业和智能的数据结构、算法和快速并行实现的随机游走方法，实现了比现有最先进的软件资源更高的时间和空间复杂度，具有竞争性的节点标签预测性能。 |
| [^171] | [Value-Function-based Sequential Minimization for Bi-level Optimization.](http://arxiv.org/abs/2110.04974) | 本文提出了一种新的双层优化算法，叫做基于价值函数的序列最小化方法（BVFSM），通过将BLO重构为近似的单层问题，避免了现有方法所需的重复计算循环梯度和Hessian逆的时间消耗，特别适用于高维任务以及具有额外功能约束的BLO。 |
| [^172] | [The Optimization of the Constant Flow Parallel Micropump Using RBF Neural Network.](http://arxiv.org/abs/2109.08717) | 本文通过实施RBF神经网络，提出了重叠时间的概念来优化恒流并联机械位移微型泵，在将左右泵互换角色的往复运动期间最小化压力脉冲。 |
| [^173] | [Controlled Gaussian Process Dynamical Models with Application to Robotic Cloth Manipulation.](http://arxiv.org/abs/2103.06615) | 本文提出了一种称为受控高斯过程动力学模型（CGPDM）的方法，用于学习高维非线性动态。该模型将高维状态空间投影到较小的维度潜在空间中。CGPDM由一个低维潜在空间组成，具有相关的动态，外部控制变量可以作用于该空间，并映射到观测空间。 |
| [^174] | [Reinforcement Learning, Bit by Bit.](http://arxiv.org/abs/2103.04047) | 该论文讨论了强化学习代理在实际环境中的数据效率问题，并提供了原则性指导和计算结果。 |
| [^175] | [Sequential Gaussian Processes for Online Learning of Nonstationary Functions.](http://arxiv.org/abs/1905.10003) | 本文提出了一种基于顺序蒙特卡罗算法的连续高斯过程模型，以解决高斯过程模型的计算复杂度高，难以在线顺序更新的问题，同时允许拟合具有非平稳性质的函数。方法优于现有最先进方法的性能。 |

# 详细

[^1]: PiML工具箱：可解释机器学习模型的开发和验证

    PiML Toolbox for Interpretable Machine Learning Model Development and Validation. (arXiv:2305.04214v1 [cs.LG])

    [http://arxiv.org/abs/2305.04214](http://arxiv.org/abs/2305.04214)

    PiML工具箱是一个综合的Python工具箱，可用于开发和诊断可解释机器学习模型，包括日益增长的可解释模型、模型无关的可解释性工具和模型无关的诊断工具，还支持与MLOps平台的集成和质量保证。

    

    PiML是一个综合且开放的Python工具箱，用于可解释机器学习模型的开发和模型诊断。它设计了低代码和高代码两种机器学习工作流，包括数据管道、模型训练、模型解释和说明以及模型诊断和比较。该工具箱支持日益增长的可解释模型（例如GAM、GAMI-Net、XGB2），具有本地和/或全局可解释性。它还支持模型无关的可解释性工具（例如PFI、PDP、LIME、SHAP）和一个强大的模型无关诊断套件（例如弱点、不确定性、鲁棒性、公平性）。通过灵活的高代码 API，将 PiML 模型和测试集成到现有的 MLOps 平台以实现质量保证。此外，PiML 工具箱还带有综合的用户指南和实践例子，包括银行业中的模型开发和验证应用。该项目可通过arXiv:2305.04214v1[cs.LG]获取。

    PiML (read $\pi$-ML, /`pai.`em.`el/) is an integrated and open-access Python toolbox for interpretable machine learning model development and model diagnostics. It is designed with machine learning workflows in both low-code and high-code modes, including data pipeline, model training, model interpretation and explanation, and model diagnostics and comparison. The toolbox supports a growing list of interpretable models (e.g. GAM, GAMI-Net, XGB2) with inherent local and/or global interpretability. It also supports model-agnostic explainability tools (e.g. PFI, PDP, LIME, SHAP) and a powerful suite of model-agnostic diagnostics (e.g. weakness, uncertainty, robustness, fairness). Integration of PiML models and tests to existing MLOps platforms for quality assurance are enabled by flexible high-code APIs. Furthermore, PiML toolbox comes with a comprehensive user guide and hands-on examples, including the applications for model development and validation in banking. The project is available
    
[^2]: 开放集合学习的新视角：解锁开放集合的力量

    Unlocking the Power of Open Set : A New Perspective for Open-set Noisy Label Learning. (arXiv:2305.04203v1 [cs.LG])

    [http://arxiv.org/abs/2305.04203](http://arxiv.org/abs/2305.04203)

    本文提出了一种新的两步对比学习方法CECL，通过利用开放集合示例的有用信息来处理两种类型的标签噪声。该方法在几个基准数据集上得到了验证。

    

    学习嘈杂的数据一直受到人们的关注，其中大多数方法集中在封闭集的标签噪声上。然而，在现实世界中更常见的情况是同时存在开放集合和封闭集合的噪声。现有方法通常通过为每种类型设计特定的策略来区分和处理这两种类型的标签噪声。然而，在许多现实世界的情况下，识别开放集合示例可能是具有挑战性的，特别是当数据集已经严重损坏时。本文对模型面对开放集合示例时的行为进行了探索，并发现部分开放集合示例逐渐融入某些已知类别，这有利于已知类别的分离。在这种现象的推动下，我们提出了一种新的两步对比学习方法CECL，通过利用开放集合示例的有用信息来处理两种类型的标签噪声。具体而言，我们在对比学习框架中将一些开放集合示例作为负例，可以有效地增强模型对开放集合噪声的鲁棒性。在几个基准数据集上进行的实验证明了我们方法处理开放集合和封闭集合标签噪声的有效性。

    Learning from noisy data has attracted much attention, where most methods focus on closed-set label noise. However, a more common scenario in the real world is the presence of both open-set and closed-set noise. Existing methods typically identify and handle these two types of label noise separately by designing a specific strategy for each type. However, in many real-world scenarios, it would be challenging to identify open-set examples, especially when the dataset has been severely corrupted. Unlike the previous works, we explore how models behave when faced open-set examples, and find that a part of open-set examples gradually get integrated into certain known classes, which is beneficial for the seperation among known classes. Motivated by the phenomenon, in this paper, we propose a novel two-step contrastive learning method called CECL, which aims to deal with both types of label noise by exploiting the useful information of open-set examples. Specifically, we incorporate some ope
    
[^3]: MrTF：面向传输联邦学习的模型精炼方法

    MrTF: Model Refinery for Transductive Federated Learning. (arXiv:2305.04201v1 [cs.LG])

    [http://arxiv.org/abs/2305.04201](http://arxiv.org/abs/2305.04201)

    本文提出了一种名为传输联邦学习的新学习范例，以同时考虑数据异构和要推导数据的结构信息。采用稳定的教师、修正的蒸馏和聚类标签精炼等技术来促进模型精炼过程，并在大量实验中证明了该方法的优越性。

    

    本文考虑了一个现实世界的场景，即在隐私保护政策下，一个新成立的试点项目需要借助其他方的帮助对新收集的数据进行推断。当前的联邦学习（FL）范例致力于解决数据异构问题，而没有考虑要推导的数据。我们提出了一种新的学习范例，称为传输联邦学习（TFL），以同时考虑要推导数据的结构信息。一方面，服务器可以使用预先可用的测试样本来优化合并模型，从而解决FL中的数据异构问题。另一方面，精炼过程将测试样本纳入训练中，并可以以传输方式生成更好的预测。我们提出了几种技术，包括稳定的教师、修正的蒸馏和聚类标签精炼，以促进模型精炼过程。大量的实验研究证明了我们提出的方法在各种设置中优于现有方法。

    We consider a real-world scenario in which a newly-established pilot project needs to make inferences for newly-collected data with the help of other parties under privacy protection policies. Current federated learning (FL) paradigms are devoted to solving the data heterogeneity problem without considering the to-be-inferred data. We propose a novel learning paradigm named transductive federated learning (TFL) to simultaneously consider the structural information of the to-be-inferred data. On the one hand, the server could use the pre-available test samples to refine the aggregated models for robust model fusion, which tackles the data heterogeneity problem in FL. On the other hand, the refinery process incorporates test samples into training and could generate better predictions in a transductive manner. We propose several techniques including stabilized teachers, rectified distillation, and clustered label refinery to facilitate the model refinery process. Abundant experimental stu
    
[^4]: 基于无线通信的通道驱动随机梯度 Langevin 动力学贝叶斯联邦平均

    Bayesian Over-the-Air FedAvg via Channel Driven Stochastic Gradient Langevin Dynamics. (arXiv:2305.04152v1 [cs.LG])

    [http://arxiv.org/abs/2305.04152](http://arxiv.org/abs/2305.04152)

    本文提出了无线 FALD 协议，可以在无噪声通信的情况下高效地在无线系统中实现分布式贝叶斯学习，实现了在通信回合之间多个本地更新以及由小批量计算的随机梯度，并进行了样本收敛分析。

    

    可扩展贝叶斯推理方法的近期发展已经重新引起了对采用贝叶斯学习作为传统频率学习的替代方法的兴趣，其通过不确定性量化提供了改进的模型校准。最近，引入了联邦平均 Langevin 动力学(FALD)作为联邦平均的变体，可以在没有噪声的通信存在下有效地实现分布式贝叶斯学习。在本文中，我们提出了无线 FALD(WFALD)，这是一种新颖的协议，通过集成基于空中计算和基于通道驱动的 Monte Carlo 更新来实现无线系统中的 FALD。与先前的无线贝叶斯学习相比，WFALD 可以实现(i) 在通信回合之间多个本地更新；并且(ii) 由小批量计算的随机梯度。以 2-Wasserstein 距离为衡量标准，给出了样本收敛分析。

    The recent development of scalable Bayesian inference methods has renewed interest in the adoption of Bayesian learning as an alternative to conventional frequentist learning that offers improved model calibration via uncertainty quantification. Recently, federated averaging Langevin dynamics (FALD) was introduced as a variant of federated averaging that can efficiently implement distributed Bayesian learning in the presence of noiseless communications. In this paper, we propose wireless FALD (WFALD), a novel protocol that realizes FALD in wireless systems by integrating over-the-air computation and channel-driven sampling for Monte Carlo updates. Unlike prior work on wireless Bayesian learning, WFALD enables (\emph{i}) multiple local updates between communication rounds; and (\emph{ii}) stochastic gradients computed by mini-batch. A convergence analysis is presented in terms of the 2-Wasserstein distance between the samples produced by WFALD and the targeted global posterior distribut
    
[^5]: 通过经典shadow高效恢复Pauli噪声中的信息

    Efficient information recovery from Pauli noise via classical shadow. (arXiv:2305.04148v1 [quant-ph])

    [http://arxiv.org/abs/2305.04148](http://arxiv.org/abs/2305.04148)

    本文提出了一种有效的算法，可以在Pauli噪声下从量子状态中恢复信息。通过后处理通道的经典阴影，学习未知Pauli通道的必要信息，并只需要通道的部分信息而不是其完整的经典描述就可以恢复理想的信息，产生一个多项式时间算法。

    

    量子计算的迅猛发展导致了对从量子系统中提取经典信息的有效技术的广泛需求，尤其是在量子机器学习和量子化学等领域。然而，量子系统本质上容易受到噪声的影响，从而损坏量子系统中编码的信息。在这项工作中，我们介绍了一种有效的算法，可以在Pauli噪声下从量子状态中恢复信息。其核心思想是通过后处理通道的经典阴影来学习未知Pauli通道的必要信息。对于局部和有界度数的可观察量，只需要通道的部分信息而不是其完整的经典描述就可以恢复理想的信息，从而产生一个多项式时间算法。这与常规方法（如概率误差消除）形成对比，后者需要通道的完整信息并展示指数级扩展。

    The rapid advancement of quantum computing has led to an extensive demand for effective techniques to extract classical information from quantum systems, particularly in fields like quantum machine learning and quantum chemistry. However, quantum systems are inherently susceptible to noises, which adversely corrupt the information encoded in quantum systems. In this work, we introduce an efficient algorithm that can recover information from quantum states under Pauli noise. The core idea is to learn the necessary information of the unknown Pauli channel by post-processing the classical shadows of the channel. For a local and bounded-degree observable, only partial knowledge of the channel is required rather than its complete classical description to recover the ideal information, resulting in a polynomial-time algorithm. This contrasts with conventional methods such as probabilistic error cancellation, which requires the full information of the channel and exhibits exponential scaling 
    
[^6]: 基于费舍尔信息界定隐私保护实例编码的可逆性

    Bounding the Invertibility of Privacy-preserving Instance Encoding using Fisher Information. (arXiv:2305.04146v1 [cs.LG])

    [http://arxiv.org/abs/2305.04146](http://arxiv.org/abs/2305.04146)

    本文提出了一种基于费舍尔信息理论，可以界定实例编码隐私性的方法。该方法可以用于界定编码可逆性，实现下游机器学习应用的隐私保护。

    

    隐私保护实例编码旨在将原始数据编码为特征向量，而不泄露其隐私敏感信息。当设计得当时，这些编码可以用于下游的机器学习应用，例如有限隐私风险的训练和推断。然而，绝大多数现有的实例编码方案都是基于启发式的，并且只通过应对有限数量的攻击经验性地验证其隐私保护属性。在本文中，我们提出了一个基于费舍尔信息的理论原理度量实例编码隐私性的方法。我们证明了我们的隐私测量是直观的、易于应用的，并可以用于理论上和经验上界定编码的可逆性。

    Privacy-preserving instance encoding aims to encode raw data as feature vectors without revealing their privacy-sensitive information. When designed properly, these encodings can be used for downstream ML applications such as training and inference with limited privacy risk. However, the vast majority of existing instance encoding schemes are based on heuristics and their privacy-preserving properties are only validated empirically against a limited set of attacks. In this paper, we propose a theoretically-principled measure for the privacy of instance encoding based on Fisher information. We show that our privacy measure is intuitive, easily applicable, and can be used to bound the invertibility of encodings both theoretically and empirically.
    
[^7]: 基于Transformer的脑网络分析中的层次聚类

    Transformer-Based Hierarchical Clustering for Brain Network Analysis. (arXiv:2305.04142v1 [cs.LG])

    [http://arxiv.org/abs/2305.04142](http://arxiv.org/abs/2305.04142)

    本文提出了一种可解释的基于Transformer的层次聚类模型，用于脑网络识别和分类，实验结果表明该模型可以提高准确性和降低运行时复杂度，同时提供对脑区域功能组织的合理见解。

    

    脑网络，即MRI构建的图形模型，在病理预测和脑功能分析中被广泛使用。在复杂的脑系统内，神经元间连接强度的差异将脑划分为各种功能模块（网络社区），这对于脑分析至关重要。然而，由于神经元相互作用的复杂性，识别脑中的这些社区是一个非常棘手的问题。在这项工作中，我们提出了一种新颖的可解释的基于Transformer的模型，用于联合层次聚类识别和脑网络分类。在真实的脑网络数据集上进行的广泛实验结果表明，通过层次聚类的帮助，该模型在提供关于脑区域功能组织合理见解的同时，实现了提高准确性和降低运行时复杂度。实现可在https://github.com/DDVD233/THC找到。

    Brain networks, graphical models such as those constructed from MRI, have been widely used in pathological prediction and analysis of brain functions. Within the complex brain system, differences in neuronal connection strengths parcellate the brain into various functional modules (network communities), which are critical for brain analysis. However, identifying such communities within the brain has been a nontrivial issue due to the complexity of neuronal interactions. In this work, we propose a novel interpretable transformer-based model for joint hierarchical cluster identification and brain network classification. Extensive experimental results on real-world brain network datasets show that with the help of hierarchical clustering, the model achieves increased accuracy and reduced runtime complexity while providing plausible insight into the functional organization of brain regions. The implementation is available at https://github.com/DDVD233/THC.
    
[^8]: 维护稳定性和可塑性以预测流失率的降低

    Maintaining Stability and Plasticity for Predictive Churn Reduction. (arXiv:2305.04135v1 [cs.LG])

    [http://arxiv.org/abs/2305.04135](http://arxiv.org/abs/2305.04135)

    AMC是一种基于保留之前和当前的模型版本，生成元输出的通用技术，可以显著降低预测性流失，同时即使与最先进的方法相比，仍然保持或提高总体准确性。

    

    部署的机器学习模型应该更新以利用更多的样本提高性能，随着时间的推移不断收集更多的数据。然而，即使模型更新提高了诸如准确性之类的聚合指标，它们也可能导致在先前模型正确预测的样本中出现错误，从而在性能上逐个降低，这称为预测性流失。这种预测上的差错会削弱用户的信任，从而降低人工智能团队的整体效力。我们提出一种解决方案，称为积累模型组合(AMC)，基于保留之前和当前的模型版本，并使用两个模型的预测生成一个元输出。AMC是一种通用技术，我们提出了几个实例，每个实例都有它自己的优点，具体取决于模型和数据属性。AMC需要最少的额外计算和训练过程的改变。我们通过展示在三个数据集上使单个模型始终保持一致的困难来说明AMC的必要性。在真实世界和合成数据集上的实验表明，AMC在保持或提高总体准确性的同时，显著降低了预测性流失，甚至比最先进的方法都好。

    Deployed machine learning models should be updated to take advantage of a larger sample size to improve performance, as more data is gathered over time. Unfortunately, even when model updates improve aggregate metrics such as accuracy, they can lead to errors on samples that were correctly predicted by the previous model causing per-sample regression in performance known as predictive churn. Such prediction flips erode user trust thereby reducing the effectiveness of the human-AI team as a whole. We propose a solution called Accumulated Model Combination (AMC) based keeping the previous and current model version, and generating a meta-output using the prediction of the two models. AMC is a general technique and we propose several instances of it, each having their own advantages depending on the model and data properties. AMC requires minimal additional computation and changes to training procedures. We motivate the need for AMC by showing the difficulty of making a single model consis
    
[^9]: 使用截断数据学习高斯混合模型

    Learning Mixtures of Gaussians with Censored Data. (arXiv:2305.04127v1 [cs.LG])

    [http://arxiv.org/abs/2305.04127](http://arxiv.org/abs/2305.04127)

    本文提出了一种学习高斯混合模型的算法，该算法仅需要很少的样本且能够对权重和均值进行准确估计。

    

    本文研究了在具有截断数据的情况下，学习高斯混合模型的问题。即从一个混合单变量高斯分布$\sum_{i=1}^k w_i \mathcal{N}(\mu_i,\sigma^2)$中观测到的样本只有当其位于$S$集合内时才会被观察到。我们提出了一种算法，仅需要$\frac{1}{\varepsilon^{O(k)}}$个样本即可在$\varepsilon$误差内估计权重$w_i$和均值$\mu_i$。

    We study the problem of learning mixtures of Gaussians with censored data. Statistical learning with censored data is a classical problem, with numerous practical applications, however, finite-sample guarantees for even simple latent variable models such as Gaussian mixtures are missing. Formally, we are given censored data from a mixture of univariate Gaussians $$\sum_{i=1}^k w_i \mathcal{N}(\mu_i,\sigma^2),$$ i.e. the sample is observed only if it lies inside a set $S$. The goal is to learn the weights $w_i$ and the means $\mu_i$. We propose an algorithm that takes only $\frac{1}{\varepsilon^{O(k)}}$ samples to estimate the weights $w_i$ and the means $\mu_i$ within $\varepsilon$ error.
    
[^10]: 一种蛋白质结构生成的潜在扩散模型

    A Latent Diffusion Model for Protein Structure Generation. (arXiv:2305.04120v1 [q-bio.BM])

    [http://arxiv.org/abs/2305.04120](http://arxiv.org/abs/2305.04120)

    提出了一种潜在扩散模型，能够在压缩的潜在空间中灵活地捕获自然蛋白质结构的分布，有效地生成具有高设计能力和效率的新型蛋白质骨架结构。

    

    蛋白质是复杂的生物分子，能在生物体内执行多种关键功能。设计和生成新型蛋白质可为未来的合成生物学应用（包括药物发现）铺平道路。但由于蛋白质结构的大规模建模空间，这仍然是一个具有挑战性的计算任务。在本研究中，我们提出了一个潜在扩散模型，该模型能够减少蛋白质建模的复杂性，同时灵活地在压缩的潜在空间中捕获自然蛋白质结构的分布。具体而言，我们提出了一个等变蛋白自编码器，将蛋白质嵌入到潜在空间中，然后使用等变扩散模型来学习潜在蛋白质表示的分布。实验结果表明，我们的方法能够有效地生成具有高设计能力和效率的新型蛋白质骨架结构。

    Proteins are complex biomolecules that perform a variety of crucial functions within living organisms. Designing and generating novel proteins can pave the way for many future synthetic biology applications, including drug discovery. However, it remains a challenging computational task due to the large modeling space of protein structures. In this study, we propose a latent diffusion model that can reduce the complexity of protein modeling while flexibly capturing the distribution of natural protein structures in a condensed latent space. Specifically, we propose an equivariant protein autoencoder that embeds proteins into a latent space and then uses an equivariant diffusion model to learn the distribution of the latent protein representations. Experimental results demonstrate that our method can effectively generate novel protein backbone structures with high designability and efficiency.
    
[^11]: 离散扩散建模下的高效和度数引导图生成

    Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling. (arXiv:2305.04111v1 [cs.LG])

    [http://arxiv.org/abs/2305.04111](http://arxiv.org/abs/2305.04111)

    本文提出了EDGE，一种新的离散扩散模型用于生成大型图，并通过删除边来鼓励图的稀疏性。EDGE在每个去噪步骤中仅关注图中一部分节点，并且可以明确地对图的节点度数进行建模。实验表明，EDGE比竞争方法更有效，并且可以生成具有数千个节点的大型图。

    

    基于扩散的生成图模型已被证明在生成高质量小图方面非常有效。然而，它们需要更可扩展性，以生成包含数千个节点的大图并满足图统计。本文提出了EDGE，一种新的基于扩散的生成图模型，用于生成大型图的生成任务。为了提高计算效率，我们通过在每个时间步长随机删除边来鼓励图的稀疏性，并最终获得一张空白图。EDGE仅在每个去噪步骤中关注图中一部分节点。它比以前的基于扩散的模型更少地进行边预测。此外，EDGE明确地允许对图的节点度数进行建模，进一步提高了模型的性能。实证研究表明，EDGE比竞争方法更有效，并且可以生成具有数千个节点的大型图。它还在生成质量方面优于基准模型。

    Diffusion-based generative graph models have been proven effective in generating high-quality small graphs. However, they need to be more scalable for generating large graphs containing thousands of nodes desiring graph statistics. In this work, we propose EDGE, a new diffusion-based generative graph model that addresses generative tasks with large graphs. To improve computation efficiency, we encourage graph sparsity by using a discrete diffusion process that randomly removes edges at each time step and finally obtains an empty graph. EDGE only focuses on a portion of nodes in the graph at each denoising step. It makes much fewer edge predictions than previous diffusion-based models. Moreover, EDGE admits explicitly modeling the node degrees of the graphs, further improving the model performance. The empirical study shows that EDGE is much more efficient than competing methods and can generate large graphs with thousands of nodes. It also outperforms baseline models in generation qual
    
[^12]: DMF-TONN: 使用神经网络进行直接无网格拓扑优化

    DMF-TONN: Direct Mesh-free Topology Optimization using Neural Networks. (arXiv:2305.04107v1 [cs.CE])

    [http://arxiv.org/abs/2305.04107](http://arxiv.org/abs/2305.04107)

    本文提出了一种直接无网格法进行拓扑优化，集成了神经网络并用于求解遵从度和体积分数约束违规的优化问题，无需传统的有限元分析和网格化，能够无缝地集成到后处理软件中。

    

    我们提出了一种直接无网格法进行拓扑优化的方法，该方法将密度场逼近神经网络与位移场逼近神经网络相结合。我们展示了这种直接集成方法可以给出与传统拓扑优化技术可比拟的结果，并且具有与后处理软件无缝集成的优点，还有可能用于网格和有限元分析成本高昂或不合适的拓扑优化目标。DMF-TONN接收边界条件和域坐标作为输入，找到最优密度场以最小化遵从度和体积分数约束违规的损失函数。无网格的性质是由一个物理学启发的位移场逼近神经网络实现的，用于解决线性弹性偏微分方程并替代传统用于计算遵从度的有限元分析（FEA）。我们展示了该方法的有效性并与传统方法进行了比较。

    We propose a direct mesh-free method for performing topology optimization by integrating a density field approximation neural network with a displacement field approximation neural network. We show that this direct integration approach can give comparable results to conventional topology optimization techniques, with an added advantage of enabling seamless integration with post-processing software, and a potential of topology optimization with objectives where meshing and Finite Element Analysis (FEA) may be expensive or not suitable. Our approach (DMF-TONN) takes in as inputs the boundary conditions and domain coordinates and finds the optimum density field for minimizing the loss function of compliance and volume fraction constraint violation. The mesh-free nature is enabled by a physics-informed displacement field approximation neural network to solve the linear elasticity partial differential equation and replace the FEA conventionally used for calculating the compliance. We show t
    
[^13]: 对预训练语言模型在非平稳环境下对代码进行持续学习以实现超出分布的泛化

    On the Usage of Continual Learning for Out-of-Distribution Generalization in Pre-trained Language Models of Code. (arXiv:2305.04106v1 [cs.SE])

    [http://arxiv.org/abs/2305.04106](http://arxiv.org/abs/2305.04106)

    本文强调预训练语言模型需要对代码进行持续学习以适应变化的软件数据分布，使其具有更好的泛化能力。

    

    学习预训练语言模型（PLMs）已成为深度学习代码中的普遍技术，利用两阶段的预训练和微调过程获取关于代码的通用知识并专门从事各种下游任务。然而，软件代码库的动态性对PLMs的有效性和鲁棒性构成挑战。本文强调需要调整适应代码的PLMs，适应分布会随时间变化的软件数据，这是之前的研究所忽视的一个关键问题。本文的动机是将PLM视为一个在非平稳环境下的模型，有助于模型在应对演化的软件数据时具有更好的泛化能力。

    Pre-trained language models (PLMs) have become a prevalent technique in deep learning for code, utilizing a two-stage pre-training and fine-tuning procedure to acquire general knowledge about code and specialize in a variety of downstream tasks. However, the dynamic nature of software codebases poses a challenge to the effectiveness and robustness of PLMs. In particular, world-realistic scenarios potentially lead to significant differences between the distribution of the pre-training and test data, i.e., distribution shift, resulting in a degradation of the PLM's performance on downstream tasks. In this paper, we stress the need for adapting PLMs of code to software data whose distribution changes over time, a crucial problem that has been overlooked in previous works. The motivation of this work is to consider the PLM in a non-stationary environment, where fine-tuning data evolves over time according to a software evolution scenario. Specifically, we design a scenario where the model 
    
[^14]: FPGAs 上的符号回归用于快速机器学习推断

    Symbolic Regression on FPGAs for Fast Machine Learning Inference. (arXiv:2305.04099v1 [cs.LG])

    [http://arxiv.org/abs/2305.04099](http://arxiv.org/abs/2305.04099)

    本论文提出了一种全新的利用符号回归的全流程，可在FPGA上进行机器学习推断，具有优化性能-资源平衡的特点。

    

    高能物理界正在研究在可编程门阵列（FPGAs）上部署基于机器学习的解决方案的可行性，以改善物理灵敏度并满足数据处理时延限制。本文引入了一种利用符号回归（SR）机器学习技术的全新端到端流程。它在方程空间中搜索近似表示数据集的代数关系。我们使用 PySR（基于进化算法发现这些表达式的软件）并扩展了 hls4ml 的功能（一种用于支持FPGAs中的机器学习推断的软件包），以支持在资源受限制的生产环境中使用 PySR 生成的表达式。深度学习模型通常通过固定网络大小来优化顶级指标，因为巨大的超参数空间会防止广泛的神经结构搜索。相反，SR 选择位于帕累托前沿的一组模型，这允许优化性能-资源的平衡。

    The high-energy physics community is investigating the feasibility of deploying machine-learning-based solutions on Field-Programmable Gate Arrays (FPGAs) to improve physics sensitivity while meeting data processing latency limitations. In this contribution, we introduce a novel end-to-end procedure that utilizes a machine learning technique called symbolic regression (SR). It searches equation space to discover algebraic relations approximating a dataset. We use PySR (software for uncovering these expressions based on evolutionary algorithm) and extend the functionality of hls4ml (a package for machine learning inference in FPGAs) to support PySR-generated expressions for resource-constrained production environments. Deep learning models often optimise the top metric by pinning the network size because vast hyperparameter space prevents extensive neural architecture search. Conversely, SR selects a set of models on the Pareto front, which allows for optimising the performance-resource
    
[^15]: 基于密钥锁模块的联邦学习梯度泄露防御

    Gradient Leakage Defense with Key-Lock Module for Federated Learning. (arXiv:2305.04095v1 [cs.LG])

    [http://arxiv.org/abs/2305.04095](http://arxiv.org/abs/2305.04095)

    本研究提出了一种新的联邦学习梯度泄露防御技术，使用私钥锁模块保护任意模型体系结构，并可确保无法从共享的梯度中重建私有训练数据。

    

    联邦学习是一种广泛采用的隐私保护机器学习方法，其中私有数据保持本地，允许安全计算和本地模型梯度与第三方参数服务器之间的交换。然而，最近的研究发现，通过共享的梯度可能会危及隐私并恢复敏感信息。本研究提供了详细的分析和对梯度泄漏问题的新视角。这些理论工作导致了一种新的梯度泄露防御技术，使用私钥锁模块保护任意模型体系结构。只有锁定的梯度被传输到参数服务器进行全局模型聚合。我们提出的学习方法对梯度泄露攻击具有抵抗力，并且所设计和训练的密钥锁模块可以确保，没有密钥锁模块的私有信息：a) 无法从共享的梯度中重建私有训练数据。

    Federated Learning (FL) is a widely adopted privacy-preserving machine learning approach where private data remains local, enabling secure computations and the exchange of local model gradients between local clients and third-party parameter servers. However, recent findings reveal that privacy may be compromised and sensitive information potentially recovered from shared gradients. In this study, we offer detailed analysis and a novel perspective on understanding the gradient leakage problem. These theoretical works lead to a new gradient leakage defense technique that secures arbitrary model architectures using a private key-lock module. Only the locked gradient is transmitted to the parameter server for global model aggregation. Our proposed learning method is resistant to gradient leakage attacks, and the key-lock module is designed and trained to ensure that, without the private information of the key-lock module: a) reconstructing private training data from the shared gradient is
    
[^16]: UCB-N和TS-N的改进后的后悔分析

    An improved regret analysis for UCB-N and TS-N. (arXiv:2305.04093v1 [cs.LG])

    [http://arxiv.org/abs/2305.04093](http://arxiv.org/abs/2305.04093)

    本文提出了对UCB-N和TS-N算法的改进后的伪后悔分析方法，并将$\log(T)$因子替换为$\log_2(\alpha)+3$因子，其中$\alpha$是反馈图的独立数。

    

    在具有无向反馈图的随机在线学习环境下，Lykouris等人（2020）先前分析了基于上置信界算法UCB-N和基于汤普森抽样算法TS-N的伪后悔。在本文中，我们展示了如何改进他们的伪后悔分析。我们的改进涉及到上一个分析的一个关键引理的细化，允许将$\log(T)$因子替换为$\log_2(\alpha)+3$因子，其中$\alpha$是反馈图的独立数。

    In the setting of stochastic online learning with undirected feedback graphs, Lykouris et al. (2020) previously analyzed the pseudo-regret of the upper confidence bound-based algorithm UCB-N and the Thompson Sampling-based algorithm TS-N. In this note, we show how to improve their pseudo-regret analysis. Our improvement involves refining a key lemma of the previous analysis, allowing a $\log(T)$ factor to be replaced by a factor $\log_2(\alpha) + 3$ for $\alpha$ the independence number of the feedback graph.
    
[^17]: 一种文本游戏中自然语言动作空间的简化方法

    A Minimal Approach for Natural Language Action Space in Text-based Games. (arXiv:2305.04082v1 [cs.LG])

    [http://arxiv.org/abs/2305.04082](http://arxiv.org/abs/2305.04082)

    本文介绍了一种在文本游戏中自然语言动作空间的简化方法，提出了 ε-可接受的探索方法，并提出了一种不需要语言模型或知识图谱的文本角色-评论（TAC）代理。实验表明，该方法可以优于使用语言模型和知识图谱的最先进代理。

    

    文本游戏是基于自然语言的强化学习交互环境。虽然语言模型和知识图谱通常被用于处理文本游戏中的大量动作空间，但目前仍不确定这些技术是否必要或被过度使用。本文重新思考了在文本游戏中探索动作空间的挑战，并提出了 ε-可接受的探索方法，这是一种利用可接受的动作的最小化方法进行训练。此外，我们提出了一种文本角色-评论（TAC）代理，该代理仅从游戏观察中生成文本命令，而无需任何语言模型或知识图谱。在 Jericho 的 10 种游戏中，我们的方法的表现优于强基线和使用语言模型和知识图谱的最先进代理。我们的方法凸显出，对于有效地探索指数级动作空间，更轻量化的模型设计和利用环境信息的新视角是足够的。

    Text-based games (TGs) are language-based interactive environments for reinforcement learning. While language models (LMs) and knowledge graphs (KGs) are commonly used for handling large action space in TGs, it is unclear whether these techniques are necessary or overused. In this paper, we revisit the challenge of exploring the action space in TGs and propose $ \epsilon$-admissible exploration, a minimal approach of utilizing admissible actions, for training phase. Additionally, we present a text-based actor-critic (TAC) agent that produces textual commands for game, solely from game observations, without requiring any KG or LM. Our method, on average across 10 games from Jericho, outperforms strong baselines and state-of-the-art agents that use LM and KG. Our approach highlights that a much lighter model design, with a fresh perspective on utilizing the information within the environments, suffices for an effective exploration of exponentially large action spaces.
    
[^18]: 鲁棒性张量CUR分解：对稀疏损坏进行快速低Tucker秩张量恢复。

    Robust Tensor CUR Decompositions: Rapid Low-Tucker-Rank Tensor Recovery with Sparse Corruption. (arXiv:2305.04080v1 [math.NA])

    [http://arxiv.org/abs/2305.04080](http://arxiv.org/abs/2305.04080)

    本文提出了一种快速算法，称为鲁棒性张量CUR分解（RTCUR），用于Tucker秩设置下的大规模非凸TRPCA问题，通过交替投影的框架和张量CUR分解，快速实现对稀疏损坏的低秩张量恢复，并在实际数据集上展示了该方法的有效性和计算优势。

    

    我们研究了张量鲁棒主成分分析（TRPCA）问题，它是矩阵鲁棒主成分分析（RPCA）的张量扩展，旨在将给定的张量分解为基础低秩分量和稀疏异常分量。本文提出了一种快速算法，称为鲁棒性张量CUR分解（RTCUR），用于Tucker秩设置下的大规模非凸TRPCA问题。RTCUR是在交替投影的框架下开发出来的，它在低秩张量集和稀疏张量集之间进行投影。我们利用最近开发的张量CUR分解，在每个投影中大大降低了计算复杂度。此外，我们为不同的应用场景开发了四个RTCUR变体。我们在合成和实际数据集上展示了RTCUR的有效性和计算优势，以对抗最先进的方法。

    We study the tensor robust principal component analysis (TRPCA) problem, a tensorial extension of matrix robust principal component analysis (RPCA), that aims to split the given tensor into an underlying low-rank component and a sparse outlier component. This work proposes a fast algorithm, called Robust Tensor CUR Decompositions (RTCUR), for large-scale non-convex TRPCA problems under the Tucker rank setting. RTCUR is developed within a framework of alternating projections that projects between the set of low-rank tensors and the set of sparse tensors. We utilize the recently developed tensor CUR decomposition to substantially reduce the computational complexity in each projection. In addition, we develop four variants of RTCUR for different application settings. We demonstrate the effectiveness and computational advantages of RTCUR against state-of-the-art methods on both synthetic and real-world datasets.
    
[^19]: 用轨迹解释强化学习的决策

    Explaining RL Decisions with Trajectories. (arXiv:2305.04073v1 [cs.AI])

    [http://arxiv.org/abs/2305.04073](http://arxiv.org/abs/2305.04073)

    本文提出一种用训练过程中遇到的轨迹解释强化学习决策的方法，并在离散和连续状态及行动空间的多样化环境中证明了其有效性。

    

    解释是强化学习在许多实际决策问题中应用的关键组成部分。本文提出了一种补充这些解释的方法，特别是针对离线强化学习，即我们将训练过程中遇到的轨迹用编码的方式进行解释。

    Explanation is a key component for the adoption of reinforcement learning (RL) in many real-world decision-making problems. In the literature, the explanation is often provided by saliency attribution to the features of the RL agent's state. In this work, we propose a complementary approach to these explanations, particularly for offline RL, where we attribute the policy decisions of a trained RL agent to the trajectories encountered by it during training. To do so, we encode trajectories in offline training data individually as well as collectively (encoding a set of trajectories). We then attribute policy decisions to a set of trajectories in this encoded space by estimating the sensitivity of the decision with respect to that set. Further, we demonstrate the effectiveness of the proposed approach in terms of quality of attributions as well as practical scalability in diverse environments that involve both discrete and continuous state and action spaces such as grid-worlds, video gam
    
[^20]: 基于空中计算的半异步联邦边缘学习机制

    Semi-Asynchronous Federated Edge Learning Mechanism via Over-the-air Computation. (arXiv:2305.04066v1 [cs.LG])

    [http://arxiv.org/abs/2305.04066](http://arxiv.org/abs/2305.04066)

    本文提出了一种半异步聚合FEEL机制PAOTA，以改善数据和设备存在显著异质性的情况下FEEL系统的训练效率，通过调整边缘设备的上行传输功率来最小化FEEL全局模型的收敛上界。实验结果表明，所提出的机制在达到相同的目标精度下，训练速度显著快于具有空中计算方案的传统同步FEEL机制。

    

    空中计算是提高联邦边缘学习（FEEL）效率的有效传输方案。然而，现有的具有空中计算方案的FEEL系统通常在每个全局轮次中采用传统的同步聚合机制，而这些机制容易受到滞后者的影响。本文提出了一种基于空中计算方案的半异步聚合FEEL机制（PAOTA），以改善数据和设备存在显著异质性的情况下FEEL系统的训练效率。考虑到来自边缘设备模型更新的陈旧性和发散性，我们通过在每个聚合期调整边缘设备的上行传输功率来最小化FEEL全局模型的收敛上界。模拟结果表明，我们提出的算法实现了接近理想的局部SGD的收敛性能。此外，在相同的目标准确度下，所提出的机制的训练速度显着快于具有空中计算方案的传统同步FEEL机制。

    Over-the-air Computation (AirComp) has been demonstrated as an effective transmission scheme to boost the efficiency of federated edge learning (FEEL). However, existing FEEL systems with AirComp scheme often employ traditional synchronous aggregation mechanisms for local model aggregation in each global round, which suffer from the stragglers issues. In this paper, we propose a semi-asynchronous aggregation FEEL mechanism with AirComp scheme (PAOTA) to improve the training efficiency of the FEEL system in the case of significant heterogeneity in data and devices. Taking the staleness and divergence of model updates from edge devices into consideration, we minimize the convergence upper bound of the FEEL global model by adjusting the uplink transmit power of edge devices at each aggregation period. The simulation results demonstrate that our proposed algorithm achieves convergence performance close to that of the ideal Local SGD. Furthermore, with the same target accuracy, the training
    
[^21]: 低轨场景分类的分布式半监督在线学习

    Decentralised Semi-supervised Onboard Learning for Scene Classification in Low-Earth Orbit. (arXiv:2305.04059v1 [cs.LG])

    [http://arxiv.org/abs/2305.04059](http://arxiv.org/abs/2305.04059)

    该论文展示了在最新卫星硬件上，使用分布式半监督学习进行场景分类的机器学习模型，并考虑卫星操作约束，达到了高精度，为航天通信和运营成本节约提供了可能。

    

    最新卫星硬件上的机器学习为通信和运营成本的节约提供了可能。我们展示了在卫星星座上使用半监督学习进行场景分类的机器学习模型的训练，考虑到卫星处理器基准的温度和有限功率预算等操作约束。我们评估了采用分布式和联邦式学习方法的任务方案。所有方案在一天的任务时间内都能达到高精度（欧洲卫星RGB数据集约91%）的收敛。

    Onboard machine learning on the latest satellite hardware offers the potential for significant savings in communication and operational costs. We showcase the training of a machine learning model on a satellite constellation for scene classification using semi-supervised learning while accounting for operational constraints such as temperature and limited power budgets based on satellite processor benchmarks of the neural network. We evaluate mission scenarios employing both decentralised and federated learning approaches. All scenarios achieve convergence to high accuracy (around 91% on EuroSAT RGB dataset) within a one-day mission timeframe.
    
[^22]: Echoes: 基于伪偏差标记的模仿式回声室无监督去偏

    Echoes: Unsupervised Debiasing via Pseudo-bias Labeling in an Echo Chamber. (arXiv:2305.04043v1 [cs.LG])

    [http://arxiv.org/abs/2305.04043](http://arxiv.org/abs/2305.04043)

    Echoes提出了一种无监督的去偏方法，生成偏差对立样本的伪偏差标签，实现了对数据集中偏差特征的一致性处理，并取得了各项任务和数据集上的最先进性能。

    

    当神经网络暴露于有偏训练数据时，通常会学习到不正确的相关性，从而导致在拓展领域数据上表现不佳。本文提出一种名为“Echoes”的简单高效方法，它生成偏差对立样本的伪偏差标签，以强制使伪标签与数据集中的偏差特征一致，并用于去偏。我们的实证研究表明，Echoes实现了各项任务和数据集上的最先进性能，同时使用比以前的方法更少的计算资源。

    Neural networks often learn spurious correlations when exposed to biased training data, leading to poor performance on out-of-distribution data. A biased dataset can be divided, according to biased features, into bias-aligned samples (i.e., with biased features) and bias-conflicting samples (i.e., without biased features). Recent debiasing works typically assume that no bias label is available during the training phase, as obtaining such information is challenging and labor-intensive. Following this unsupervised assumption, existing methods usually train two models: a biased model specialized to learn biased features and a target model that uses information from the biased model for debiasing. This paper first presents experimental analyses revealing that the existing biased models overfit to bias-conflicting samples in the training data, which negatively impacts the debiasing performance of the target models. To address this issue, we propose a straightforward and effective method cal
    
[^23]: Wasserstein-Fisher-Rao嵌入：具有本地比较和全局传输的逻辑查询嵌入

    Wasserstein-Fisher-Rao Embedding: Logical Query Embeddings with Local Comparison and Global Transport. (arXiv:2305.04034v1 [cs.AI])

    [http://arxiv.org/abs/2305.04034](http://arxiv.org/abs/2305.04034)

    本文提出了一种新的查询嵌入方法，使用Wasserstein-Fisher-Rao度量来加权考虑嵌入之间的本地（局部）与全局（整体）特征，同时提出了一种卷积算法来进行线性时间计算，并使用块对角核来进行权衡。在标准数据集上表现出优异的效果。

    

    在知识图谱上回答复杂查询很重要，但由于数据不完整性，它尤其具有挑战性。查询嵌入方法通过基于学习的模型和使用集合运算符模拟逻辑推理来解决这个问题。以往的工作集中于特定形式的嵌入，但嵌入之间的评分函数却鲜有研究。与现有的基于本地比较或全局传输的评分函数不同，本文使用不平衡最优传输理论来研究本地和全局的权衡。具体而言，我们将集合嵌入到带有Wasserstein-Fisher-Rao度量的有界测度空间上，并使用这样的度量来设计评分函数。这种设计还促进了嵌入空间内的封闭形式集合运算符。此外，我们介绍了一种基于卷积的线性计算算法和一个块对角核以实现权衡。结果表明，WFRE可以在标准数据集上优于现有的查询嵌入方法。

    Answering complex queries on knowledge graphs is important but particularly challenging because of the data incompleteness. Query embedding methods address this issue by learning-based models and simulating logical reasoning with set operators. Previous works focus on specific forms of embeddings, but scoring functions between embeddings are underexplored. In contrast to existing scoring functions motivated by local comparison or global transport, this work investigates the local and global trade-off with unbalanced optimal transport theory. Specifically, we embed sets as bounded measures in $\real$ endowed with a scoring function motivated by the Wasserstein-Fisher-Rao metric. Such a design also facilitates closed-form set operators in the embedding space. Moreover, we introduce a convolution-based algorithm for linear time computation and a block-diagonal kernel to enforce the trade-off. Results show that WFRE can outperform existing query embedding methods on standard datasets, eval
    
[^24]: 使用深度学习的肌电图信号分类

    Electromyography Signal Classification Using Deep Learning. (arXiv:2305.04006v1 [cs.LG])

    [http://arxiv.org/abs/2305.04006](http://arxiv.org/abs/2305.04006)

    本论文使用深度学习模型实现了肌电图信号的分类，通过训练，成功实现对对照组、肌病和ALS患者信号的准确分类，取得了准确率99%的成果，这些技术将有助于临床诊断神经肌肉疾病。

    

    我们使用L2正则化实现了一个深度学习模型，并对肌电图（EMG）数据进行了训练。该数据包括来自对照组、肌病和肌萎缩侧索硬化（ALS）患者的EMG信号。我们提出的深度神经网络由八层组成，包括五个全连接层、两个批标准化层和一个随机失活层。通过将训练数据分为子训练和验证部分，进一步将数据分成训练和测试部分。在测试数据集上实现了99%的准确率。该模型能够100%准确地区分正常病例（对照组）和其他病例，并以97.4%和98.2%的高精度分类肌病和ALS病例。因此，我们认为这种高度改进的分类准确性将有助于临床诊断神经肌肉疾病。

    We have implemented a deep learning model with L2 regularization and trained it on Electromyography (EMG) data. The data comprises of EMG signals collected from control group, myopathy and ALS patients. Our proposed deep neural network consists of eight layers; five fully connected, two batch normalization and one dropout layers. The data is divided into training and testing sections by subsequently dividing the training data into sub-training and validation sections. Having implemented this model, an accuracy of 99 percent is achieved on the test data set. The model was able to distinguishes the normal cases (control group) from the others at a precision of 100 percent and classify the myopathy and ALS with high accuracy of 97.4 and 98.2 percents, respectively. Thus we believe that, this highly improved classification accuracies will be beneficial for their use in the clinical diagnosis of neuromuscular disorders.
    
[^25]: ANTONIO:面向NLP验证的系统化基准生成方法

    ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for Verification. (arXiv:2305.04003v1 [cs.CL])

    [http://arxiv.org/abs/2305.04003](http://arxiv.org/abs/2305.04003)

    本文介绍了一种名为ANTONIO的Python库，它基于抽象解释方法提供了一种实用的方法和启发式规则，以便为自然语言处理（NLP）数据集和模型生成已知验证方法的基准。因为其普遍适用性，这项工作将为将NLP验证问题纳入神经网络验证比赛开辟新的可能性，并在NLP问题中普及这一方向。

    

    自然语言处理（NLP）中使用的机器学习模型的验证被认为是一个难题。现有的神经网络验证方法常用于计算机视觉和其他数字数据集，但并不适用于NLP。本研究探讨了造成这一问题的技术原因，并在此基础上提出了实用的方法和启发式规则，以便将NLP数据集和模型准备为适合基于抽象解释的已知验证方法。我们将这些方法实现为一个名为ANTONIO的Python库，该库连接到神经网络验证器ERAN和Marabou。我们使用一个名为R-U-A-Robot的NLP数据集对工具进行了评估，该数据集被提议作为验证具有法律重要性的NLP应用的基准。我们希望，由于其普遍适用性，这项工作将为将NLP验证问题纳入神经网络验证比赛开辟新的可能性，并在NLP问题中普及这一方向。

    Verification of machine learning models used in Natural Language Processing (NLP) is known to be a hard problem. In particular, many known neural network verification methods that work for computer vision and other numeric datasets do not work for NLP. Here, we study technical reasons that underlie this problem. Based on this analysis, we propose practical methods and heuristics for preparing NLP datasets and models in a way that renders them amenable to known verification methods based on abstract interpretation. We implement these methods as a Python library called ANTONIO that links to the neural network verifiers ERAN and Marabou. We perform evaluation of the tool using an NLP dataset R-U-A-Robot suggested as a benchmark for verifying legally critical NLP applications. We hope that, thanks to its general applicability, this work will open novel possibilities for including NLP verification problems into neural network verification competitions, and will popularise NLP problems withi
    
[^26]: 从自然语言文本中生成流程模型的方法——基于规则之外的命名实体识别和关系抽取(arXiv:2305.03960v1 [cs.CL])

    Beyond Rule-based Named Entity Recognition and Relation Extraction for Process Model Generation from Natural Language Text. (arXiv:2305.03960v1 [cs.CL])

    [http://arxiv.org/abs/2305.03960](http://arxiv.org/abs/2305.03960)

    本文扩展了PET数据集，通过聚类流程实体的提及，提出了一种新的基线技术流程提取方法，该方法避免了手动创建业务流程模型的繁琐工作，同时解决了同一流程实体重复提及的歧义问题。

    

    从自然语言文本自动生成业务流程模型是一种新兴方法，可避免手动创建形式化业务流程模型。为此，需要从文本流程描述中提取出流程实体（如参与者、活动、对象等）和它们之间的关系。一个高质量的带有文本流程描述的注释语料库(PET)已经出版，其伴随着一种基本的流程提取方法。然而，在其当前状态下，PET缺乏有关两个提及是否指代了相同或不同的流程实体的信息，这对于是否在目标模型中创建一个或两个建模元素的重要决策相对应。因此，例如，两个数据处理的提及是否意味着处理不同或相同的数据是不确定的。在本文中，我们通过聚类流程实体的提及来扩展PET数据集，并提出了一种新的基线技术流程提取方法，其中包含一个

    Automated generation of business process models from natural language text is an emerging methodology for avoiding the manual creation of formal business process models. For this purpose, process entities like actors, activities, objects etc., and relations among them are extracted from textual process descriptions. A high-quality annotated corpus of textual process descriptions (PET) has been published accompanied with a basic process extraction approach. In its current state, however, PET lacks information about whether two mentions refer to the same or different process entities, which corresponds to the crucial decision of whether to create one or two modeling elements in the target model. Consequently, it is ambiguous whether, for instance, two mentions of data processing mean processing of different, or the same data. In this paper, we extend the PET dataset by clustering mentions of process entities and by proposing a new baseline technique for process extraction equipped with a
    
[^27]: 基于机器学习的双极化天线在城市地区分类GPS信号接收状态

    Machine-Learning-Based Classification of GPS Signal Reception Conditions Using a Dual-Polarized Antenna in Urban Areas. (arXiv:2305.03956v1 [cs.LG])

    [http://arxiv.org/abs/2305.03956](http://arxiv.org/abs/2305.03956)

    本文介绍了一种基于机器学习的双极化天线分类GPS信号接收状况的方法，用于改善城市地区的定位性能。

    

    在城市地区，密集的建筑物经常会阻挡和反射全球定位系统（GPS）信号，导致接收到很多多径信号的少数可见卫星。这是一个严重的问题，会导致在城市地区位置不可靠。如果能够检测到来自某个卫星的信号接收状况，则可以通过排除或减轻多径污染的卫星信号来改善定位性能。因此，我们开发了一种使用双极化天线分类GPS信号接收状况的机器学习方法。我们使用决策树算法进行分类，其中三个特征之一只能从双极化天线中获得。使用从各种位置收集的GPS信号训练了机器学习模型。当输入从GPS原始信号提取的特征时，生成的机器学习模型会输出三种信号接收状况之一：非视距（NLOS）、视距（LOS）和部分可视（PLOS）。

    In urban areas, dense buildings frequently block and reflect global positioning system (GPS) signals, resulting in the reception of a few visible satellites with many multipath signals. This is a significant problem that results in unreliable positioning in urban areas. If a signal reception condition from a certain satellite can be detected, the positioning performance can be improved by excluding or de-weighting the multipath contaminated satellite signal. Thus, we developed a machine-learning-based method of classifying GPS signal reception conditions using a dual-polarized antenna. We employed a decision tree algorithm for classification using three features, one of which can be obtained only from a dual-polarized antenna. A machine-learning model was trained using GPS signals collected from various locations. When the features extracted from the GPS raw signal are input, the generated machine-learning model outputs one of the three signal reception conditions: non-line-of-sight (N
    
[^28]: 学习动作嵌入以进行离线评估

    Learning Action Embeddings for Off-Policy Evaluation. (arXiv:2305.03954v1 [cs.LG])

    [http://arxiv.org/abs/2305.03954](http://arxiv.org/abs/2305.03954)

    本论文探讨了从记录数据中学习动作嵌入，以减少在大型动作空间中反向倾向评分（IPS）估计器的方差，同时提高离线评估的准确性。

    

    离线评估（OPE）方法使我们能够使用由不同策略收集的记录数据来计算策略的预期奖励。 OPE是运行昂贵的在线A / B测试的可行选择：它可以加快新策略的开发，并降低向客户暴露次优治疗的风险。然而，当动作数量很大或记录策略未充分探索某些操作时，基于反向倾向评分（IPS）的现有估计器可能具有高甚至无限方差。Saito和Joachims提出使用动作嵌入的边际IPS（MIPS），从而在大型动作空间中降低IPS的方差。 MIPS假设从业者可以定义良好的动作嵌入，但在许多实际应用中很难做到这一点。在这项工作中，我们探讨从记录数据中学习动作嵌入。特别地，我们使用已经训练好的奖励模型的中间输出来定义动作嵌入，然后将其用于MIPS估计器中。

    Off-policy evaluation (OPE) methods allow us to compute the expected reward of a policy by using the logged data collected by a different policy. OPE is a viable alternative to running expensive online A/B tests: it can speed up the development of new policies, and reduces the risk of exposing customers to suboptimal treatments. However, when the number of actions is large, or certain actions are under-explored by the logging policy, existing estimators based on inverse-propensity scoring (IPS) can have a high or even infinite variance. Saito and Joachims (arXiv:2202.06317v2 [cs.LG]) propose marginalized IPS (MIPS) that uses action embeddings instead, which reduces the variance of IPS in large action spaces. MIPS assumes that good action embeddings can be defined by the practitioner, which is difficult to do in many real-world applications. In this work, we explore learning action embeddings from logged data. In particular, we use intermediate outputs of a trained reward model to defin
    
[^29]: 学习6D非抓取式操作的混合演员-评论员地图

    Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation. (arXiv:2305.03942v1 [cs.RO])

    [http://arxiv.org/abs/2305.03942](http://arxiv.org/abs/2305.03942)

    论文介绍了一种名为HACMan的强化学习方法，用于使用点云观察进行6D非抓取式操作的物体操纵。HACMan重点关注物体中心动作表示，它包括从物体点云中选择接触位置和一组描述机器人在接触后如何移动的运动参数。在实际测试中，HACMan的表现明显优于现有基线方法。

    

    在人类的灵巧性中，非抓取式操作是操作物体的重要组成部分。非抓取式操纵可以使与物体的交互更加复杂，但也在推理交互方面提出了挑战。在本文中，我们引入了一个名为HACMan的混合演员评论员地图，这是一种使用点云观察的6D非抓取式物体操作的强化学习方法。HACMan提出了一种时间抽象和空间基础的物体中心动作表示，该表示包括从物体点云中选择接触位置和一组描述机器人在接触后如何移动的运动参数。我们修改了一个现有的离线策略RL算法，以在这种混合的离散-连续动作表示学习。我们在仿真和现实世界中对HACMan进行了6D物体姿态对齐任务的评估。在最难的任务版本中，通过随机初始化物体和机器人配置，HACMan的表现优于现有的基线方法。

    Manipulating objects without grasping them is an essential component of human dexterity, referred to as non-prehensile manipulation. Non-prehensile manipulation may enable more complex interactions with the objects, but also presents challenges in reasoning about the interactions. In this work, we introduce Hybrid Actor-Critic Maps for Manipulation (HACMan), a reinforcement learning approach for 6D non-prehensile manipulation of objects using point cloud observations. HACMan proposes a temporally-abstracted and spatially-grounded object-centric action representation that consists of selecting a contact location from the object point cloud and a set of motion parameters describing how the robot will move after making contact. We modify an existing off-policy RL algorithm to learn in this hybrid discrete-continuous action representation. We evaluate HACMan on a 6D object pose alignment task in both simulation and in the real world. On the hardest version of our task, with randomized init
    
[^30]: Adam家族算法在无平滑优化中的收敛性保证研究

    Adam-family Methods for Nonsmooth Optimization with Convergence Guarantees. (arXiv:2305.03938v1 [math.OC])

    [http://arxiv.org/abs/2305.03938](http://arxiv.org/abs/2305.03938)

    本文提出了一种新的双时间尺度框架，证明了其在温和条件下收敛性，该框架包括了各种流行的Adam家族算法，用于训练无平滑神经网络和应对重尾噪声的需求，并通过实验表明了其效率和鲁棒性。

    

    本文对Adam家族算法在无平滑优化中的收敛性进行了全面研究，特别是在无平滑神经网络的训练中。我们提出了一种新的双时间尺度框架，采用双时间尺度更新方案，证明了其在温和条件下的收敛性。我们的框架包括了各种流行的Adam家族算法，在训练无平滑神经网络中提供了收敛性保证。此外，我们还开发了随机次梯度方法，结合梯度裁剪技术，用于训练具有重尾噪声的无平滑神经网络。通过我们的框架，我们展示了我们提出的方法甚至在仅假定评估噪声可积的情况下也会收敛。广泛的数值实验证明了我们提出的方法的高效性和稳健性。

    In this paper, we present a comprehensive study on the convergence properties of Adam-family methods for nonsmooth optimization, especially in the training of nonsmooth neural networks. We introduce a novel two-timescale framework that adopts a two-timescale updating scheme, and prove its convergence properties under mild assumptions. Our proposed framework encompasses various popular Adam-family methods, providing convergence guarantees for these methods in training nonsmooth neural networks. Furthermore, we develop stochastic subgradient methods that incorporate gradient clipping techniques for training nonsmooth neural networks with heavy-tailed noise. Through our framework, we show that our proposed methods converge even when the evaluation noises are only assumed to be integrable. Extensive numerical experiments demonstrate the high efficiency and robustness of our proposed methods.
    
[^31]: 基于扩散ODEs最大似然估计的改进技术

    Improved Techniques for Maximum Likelihood Estimation for Diffusion ODEs. (arXiv:2305.03935v1 [cs.LG])

    [http://arxiv.org/abs/2305.03935](http://arxiv.org/abs/2305.03935)

    本文提出了基于扩散ODEs最大似然估计的改进技术，包括速度参数化和方差减少技术等用于训练的技术，以及误差有界的高阶流匹配目标用于微调和截断正态去量化方法用于评估。

    

    扩散模型在各领域中表现出良好的性能。扩散模型的概率流常微分方程（ODE）（即扩散ODE）是连续归一化流（CNFs）的一个特例，它使得确定性推断和精确似然评估成为可能。然而，与最先进的基于似然的生成模型相比，扩散ODE的似然估计结果仍有很大差距。在本文中，我们提出了一些改进的技术，包括训练和评估两个方面，用于扩散ODE的最大似然估计。对于训练，我们提出了速度参数化，并探索方差减少技术以加快收敛速度。我们还设计了一个误差有界的高阶流匹配目标用于微调，从而提高ODE的似然估计并平滑其轨迹。对于评估，我们提出了一种新颖的无须训练的截断正态去量化方法来填补训练-评估间的差距。

    Diffusion models have exhibited excellent performance in various domains. The probability flow ordinary differential equation (ODE) of diffusion models (i.e., diffusion ODEs) is a particular case of continuous normalizing flows (CNFs), which enables deterministic inference and exact likelihood evaluation. However, the likelihood estimation results by diffusion ODEs are still far from those of the state-of-the-art likelihood-based generative models. In this work, we propose several improved techniques for maximum likelihood estimation for diffusion ODEs, including both training and evaluation perspectives. For training, we propose velocity parameterization and explore variance reduction techniques for faster convergence. We also derive an error-bounded high-order flow matching objective for finetuning, which improves the ODE likelihood and smooths its trajectory. For evaluation, we propose a novel training-free truncated-normal dequantization to fill the training-evaluation gap commonly
    
[^32]: 重访在ARM二进制代码上的轻量级编译器溯源恢复

    Revisiting Lightweight Compiler Provenance Recovery on ARM Binaries. (arXiv:2305.03934v1 [cs.LG])

    [http://arxiv.org/abs/2305.03934](http://arxiv.org/abs/2305.03934)

    本文在ARM二进制代码上使用浅层学习模型，高效和准确地恢复编译器配置属性。我们应用了已经在x86-64二进制代码上有效的操作码和寄存器派生特征，并将这项工作与使用深度学习的与架构无关的模型进行比较，证明轻量级特征在ARM二进制代码上是可重现的。

    

    一个二进制代码的行为受编译器如何构建其源代码的影响。尽管在编译过程中大多数编译器配置细节都被抽象化了，但恢复它们对于未知二进制代码的逆向工程和程序理解任务（例如代码相似性检测）是有用的。我们注意到以前的工作已经在x86-64二进制代码上进行了深入的研究。然而，在ARM二进制代码上的研究有限，而这种情况越来越普遍。在本文中，我们使用一种浅层学习模型，该模型针对ARM二进制代码高效而准确地恢复编译器配置属性。我们应用了已经在x86-64二进制代码上有效的操作码和寄存器派生特征到ARM二进制代码上。此外，我们将这项工作与Pizzolotto等人的工作进行比较，后者是最近使用深度学习的与架构无关的模型，并且具有可用的数据集和代码。我们观察到，轻量级特征在ARM二进制代码上是可重现的。

    A binary's behavior is greatly influenced by how the compiler builds its source code. Although most compiler configuration details are abstracted away during compilation, recovering them is useful for reverse engineering and program comprehension tasks on unknown binaries, such as code similarity detection. We observe that previous work has thoroughly explored this on x86-64 binaries. However, there has been limited investigation of ARM binaries, which are increasingly prevalent.  In this paper, we extend previous work with a shallow-learning model that efficiently and accurately recovers compiler configuration properties for ARM binaries. We apply opcode and register-derived features, that have previously been effective on x86-64 binaries, to ARM binaries. Furthermore, we compare this work with Pizzolotto et al., a recent architecture-agnostic model that uses deep learning, whose dataset and code are available.  We observe that the lightweight features are reproducible on ARM binaries
    
[^33]: 主动的连续学习：在任务序列中标记查询。

    Active Continual Learning: Labelling Queries in a Sequence of Tasks. (arXiv:2305.03923v1 [cs.LG])

    [http://arxiv.org/abs/2305.03923](http://arxiv.org/abs/2305.03923)

    本文考虑了一系列主动学习任务的主动连续学习问题，研究了不同场景下多种主动和连续学习算法之间的有效性和相互作用，并提出了遗忘-学习曲线方法来平衡不忘旧知识和快速学习的两个目标。

    

    在连续学习（CL）中，获取新知识并不忘记已学内容是其核心。而任务是按顺序出现的，训练数据的准备和注释则通常是独立的，因此需要连续学习来适应新的监督学习任务。本文考虑了一系列主动学习（AL）任务的主动连续学习（ACL）中未被充分探索的问题，每个任务包括一个未标记的数据池和一个注释预算。我们研究了几种AL和CL算法在不同领域，类别和任务增量场景中的有效性和相互作用。实验揭示了不忘旧知识和快速学习在CL和AL中之间的权衡。尽管在以前任务的注释收集上条件查询策略会提高领域和任务增量学习的任务性能，但我们提出的遗忘-学习曲线则更好地平衡了这两个目标。

    Acquiring new knowledge without forgetting what has been learned in a sequence of tasks is the central focus of continual learning (CL). While tasks arrive sequentially, the training data are often prepared and annotated independently, leading to CL of incoming supervised learning tasks. This paper considers the under-explored problem of active continual learning (ACL) for a sequence of active learning (AL) tasks, where each incoming task includes a pool of unlabelled data and an annotation budget. We investigate the effectiveness and interplay between several AL and CL algorithms in the domain, class and task-incremental scenarios. Our experiments reveal the trade-off between two contrasting goals of not forgetting the old knowledge and the ability to quickly learn in CL and AL. While conditioning the query strategy on the annotations collected for the previous tasks leads to improved task performance on the domain and task incremental learning, our proposed forgetting-learning profil
    
[^34]: 自动化时空图对比学习

    Automated Spatio-Temporal Graph Contrastive Learning. (arXiv:2305.03920v1 [cs.LG])

    [http://arxiv.org/abs/2305.03920](http://arxiv.org/abs/2305.03920)

    本文提出了一种自动化时空图对比学习模型来解决基于图的区域关系学习模型在处理普遍存在的数据噪声、缺失和分布异质性方面的挑战。

    

    在各种区域嵌入方法中，基于图的区域关系学习模型由于使用图神经网络编码空间相关性的强大结构表示能力而脱颖而出。尽管它们很有效，但现有方法尚未解决几个关键挑战：i）由于各种因素，数据噪音和缺失在许多时空场景中是普遍存在的。ii）输入的时空数据（例如移动轨迹）通常在空间和时间上表现出分布异质性。在这种情况下，当前方法容易受到生成区域图的质量的影响，从而可能导致次优性能。本文通过探索从多视图数据源生成的异构区域图上的自动时空图对比学习范式（AutoST）来解决上述挑战。我们的"AutoST"框架建立在一个异构图神经架构之上，以捕捉多视图区域依赖关系。

    Among various region embedding methods, graph-based region relation learning models stand out, owing to their strong structure representation ability for encoding spatial correlations with graph neural networks. Despite their effectiveness, several key challenges have not been well addressed in existing methods: i) Data noise and missing are ubiquitous in many spatio-temporal scenarios due to a variety of factors. ii) Input spatio-temporal data (e.g., mobility traces) usually exhibits distribution heterogeneity across space and time. In such cases, current methods are vulnerable to the quality of the generated region graphs, which may lead to suboptimal performance. In this paper, we tackle the above challenges by exploring the Automated Spatio-Temporal graph contrastive learning paradigm (AutoST) over the heterogeneous region graph generated from multi-view data sources. Our \model\ framework is built upon a heterogeneous graph neural architecture to capture the multi-view region depe
    
[^35]: 具有未知过程噪声协方差的变分非线性卡尔曼滤波

    Variational Nonlinear Kalman Filtering with Unknown Process Noise Covariance. (arXiv:2305.03914v1 [eess.SY])

    [http://arxiv.org/abs/2305.03914](http://arxiv.org/abs/2305.03914)

    该论文介绍了一种处理非线性状态空间模型中未知过程噪声协方差问题的变分非线性卡尔曼滤波方法，通过引入辅助变量和推导出增广状态空间模型来简化估计问题，并扩展到处理任意非线性观测方程，具有更广泛的应用性。

    

    本文考虑在非线性状态空间模型中，通过联合和递归估计动态状态和时间变化的过程噪声协方差，来解决雷达和声纳等传感器跟踪机动目标的问题。由于模型的非线性和先验概率分布的非共轭性，状态估计问题通常难以处理。本文提出了一种基于近似贝叶斯推理原理的递归解决方案，采用随机搜索变分推理法来估计后验分布。与现有的基于变分推理的噪声自适应滤波方法相比，我们做出了两个贡献。首先，我们引入了一个辅助变量，推导出增广状态空间模型，并将非共轭先验转化为共轭形式，简化了估计问题。其次，我们将该方法扩展到处理任意非线性观测方程，使该算法在实践中更具应用性。

    Motivated by the maneuvering target tracking with sensors such as radar and sonar, this paper considers the joint and recursive estimation of the dynamic state and the time-varying process noise covariance in nonlinear state space models. Due to the nonlinearity of the models and the non-conjugate prior, the state estimation problem is generally intractable as it involves integrals of general nonlinear functions and unknown process noise covariance, resulting in the posterior probability distribution functions lacking closed-form solutions. This paper presents a recursive solution for joint nonlinear state estimation and model parameters identification based on the approximate Bayesian inference principle. The stochastic search variational inference is adopted to offer a flexible, accurate, and effective approximation of the posterior distributions. We make two contributions compared to existing variational inference-based noise adaptive filtering methods. First, we introduce an auxili
    
[^36]: 使用联合扩散注意力模型将高场和超高场MRI图像合成PET图像

    Synthesizing PET images from High-field and Ultra-high-field MR images Using Joint Diffusion Attention Model. (arXiv:2305.03901v1 [cs.LG])

    [http://arxiv.org/abs/2305.03901](http://arxiv.org/abs/2305.03901)

    本文提出了一种方法，使用联合扩散注意力模型来合成PET图像，以解决PET成像成本高且涉及辐射暴露的缺陷。该方法相比现有方法具有更好的性能和噪声敏感度更低。

    

    MRI和PET是诊断脑部疾病的重要工具，因为它们提供了有关脑结构和功能的互补信息。然而，PET扫描成本高且涉及辐射暴露，导致PET缺乏。此外，同时进行超高场MRI和PET目前几乎不可行。超高场成像在临床和学术环境中都被证明是有价值的，特别是在认知神经影像学领域。为此，我们提出了一种从高场和超高场MRI合成PET的方法。从统计的角度来看，联合概率分布（JPD）是描绘PET和MRI之间相关性的最直接和基本的方法。本文提出了一种新的联合扩散注意力模型，该模型具有联合概率分布和注意策略，命名为JDAM。JDAM具有扩散过程和采样过程。扩散过程涉及将PET逐渐扩散到高斯PET，采样过程从高场和超高场MRI中选择适当的特征。实验结果表明，JDAM优于最先进的PET合成方法，并且对噪声 lesss 敏感。

    MRI and PET are crucial diagnostic tools for brain diseases, as they provide complementary information on brain structure and function. However, PET scanning is costly and involves radioactive exposure, resulting in a lack of PET. Moreover, simultaneous PET and MRI at ultra-high-field are currently hardly infeasible. Ultra-high-field imaging has unquestionably proven valuable in both clinical and academic settings, especially in the field of cognitive neuroimaging. These motivate us to propose a method for synthetic PET from high-filed MRI and ultra-high-field MRI. From a statistical perspective, the joint probability distribution (JPD) is the most direct and fundamental means of portraying the correlation between PET and MRI. This paper proposes a novel joint diffusion attention model which has the joint probability distribution and attention strategy, named JDAM. JDAM has a diffusion process and a sampling process. The diffusion process involves the gradual diffusion of PET to Gaussi
    
[^37]: 重新思考机器学习中的类别不均衡问题

    Rethinking Class Imbalance in Machine Learning. (arXiv:2305.03900v1 [cs.LG])

    [http://arxiv.org/abs/2305.03900](http://arxiv.org/abs/2305.03900)

    本研究提出了一个更广泛的类别不平衡分类方式，包括四种不平衡类型，并介绍了两种不平衡级别，使用理论分析证明了新的不平衡类型对学习公平性的重要影响。

    

    不平衡学习是机器学习的一个子领域，它关注存在类别不平衡的学习任务。几乎所有现有的研究都将类别不平衡定义为比例不平衡，即每个类别的训练样本比例不平衡，而忽视比例不平衡会导致类别之间/之间的不公平以及泛化能力差。本研究提出了一个更广泛的机器学习类别不平衡分类方式，包括方差、距离、邻近和质量等四种不平衡类型。本研究还介绍了两种不平衡级别:全局和局部。该研究使用理论分析来说明新的不平衡类型对学习公平性的重要影响。

    Imbalance learning is a subfield of machine learning that focuses on learning tasks in the presence of class imbalance. Nearly all existing studies refer to class imbalance as a proportion imbalance, where the proportion of training samples in each class is not balanced. The ignorance of the proportion imbalance will result in unfairness between/among classes and poor generalization capability. Previous literature has presented numerous methods for either theoretical/empirical analysis or new methods for imbalance learning. This study presents a new taxonomy of class imbalance in machine learning with a broader scope. Four other types of imbalance, namely, variance, distance, neighborhood, and quality imbalances between/among classes, which may exist in machine learning tasks, are summarized. Two different levels of imbalance including global and local are also presented. Theoretical analysis is used to illustrate the significant impact of the new imbalance types on learning fairness. 
    
[^38]: NL-CS Net:基于非局部先验的深度学习图片压缩感知

    NL-CS Net: Deep Learning with Non-Local Prior for Image Compressive Sensing. (arXiv:2305.03899v1 [cs.CV])

    [http://arxiv.org/abs/2305.03899](http://arxiv.org/abs/2305.03899)

    NL-CS Net是一种结合传统优化方法的可解释性和基于网络方法的速度的新型CS方法，使用非局部先验，在重构质量和复杂度方面优于现有的最先进方法。

    

    近年来，深度学习已成功应用于图像压缩感知（CS）领域。然而，现有的基于网络的方法往往作为黑匣子进行训练，缺乏先验知识是进一步提高性能的 bottleneck。因此，本文提出了一种结合传统优化方法的可解释性和基于网络方法的速度的新型CS方法，即使用非局部先验的NL-CS Net。我们通过网络展开增广拉格朗日方法的每个阶段，以解决非局部和稀疏正则化优化问题。NL-CS Net由上采样模块和恢复模块组成，在上采样模块中，我们使用可学习上采样矩阵代替预定义的上采样矩阵；在恢复模块中，我们采用基于图块的非局部网络来捕获长距离的特征对应关系。NL-CS Net中涉及的重要参数（例如采样矩阵，非线性变换，收缩量，正则化）以端到端的方式进行优化。实验结果表明，我们提出的NL-CS Net在重构质量和复杂度方面优于现有的最先进方法。

    Deep learning has been applied to compressive sensing (CS) of images successfully in recent years. However, existing network-based methods are often trained as the black box, in which the lack of prior knowledge is often the bottleneck for further performance improvement. To overcome this drawback, this paper proposes a novel CS method using non-local prior which combines the interpretability of the traditional optimization methods with the speed of network-based methods, called NL-CS Net. We unroll each phase from iteration of the augmented Lagrangian method solving non-local and sparse regularized optimization problem by a network. NL-CS Net is composed of the up-sampling module and the recovery module. In the up-sampling module, we use learnable up-sampling matrix instead of a predefined one. In the recovery module, patch-wise non-local network is employed to capture long-range feature correspondences. Important parameters involved (e.g. sampling matrix, nonlinear transforms, shrink
    
[^39]: 双支持向量分位数回归

    Twin support vector quantile regression. (arXiv:2305.03894v1 [stat.ML])

    [http://arxiv.org/abs/2305.03894](http://arxiv.org/abs/2305.03894)

    TSVQR能够捕捉现代数据中的异质和不对称信息，并有效地描述了所有数据点的异质分布信息。通过构造两个较小的二次规划问题，TSVQR生成两个非平行平面，测量每个分位数水平下限和上限之间的分布不对称性。在多个实验中，TSVQR优于以前的分位数回归方法。

    

    我们提出了一种双支持向量分位数回归(TSVQR)，用于捕捉现代数据中异质和不对称信息。TSVQR利用分位数参数有效地描述了所有数据点的异质分布信息。相应地，TSVQR构造了两个较小的二次规划问题(QPPs)，生成两个非平行平面，以测量每个分位数水平下限和上限之间的分布不对称性。TSVQR中的QPP比以前的分位数回归方法更小且更易于解决。此外，TSVQR的双重坐标下降算法也加速了训练速度。在六个人造数据集、五个基准数据集、两个大规模数据集、两个时间序列数据集和两个不平衡数据集上的实验结果表明，TSVQR在完全捕获异质性方面的效果优于以前的分位数回归方法。

    We propose a twin support vector quantile regression (TSVQR) to capture the heterogeneous and asymmetric information in modern data. Using a quantile parameter, TSVQR effectively depicts the heterogeneous distribution information with respect to all portions of data points. Correspondingly, TSVQR constructs two smaller sized quadratic programming problems (QPPs) to generate two nonparallel planes to measure the distributional asymmetry between the lower and upper bounds at each quantile level. The QPPs in TSVQR are smaller and easier to solve than those in previous quantile regression methods. Moreover, the dual coordinate descent algorithm for TSVQR also accelerates the training speed. Experimental results on six artiffcial data sets, ffve benchmark data sets, two large scale data sets, two time-series data sets, and two imbalanced data sets indicate that the TSVQR outperforms previous quantile regression methods in terms of the effectiveness of completely capturing the heterogeneous 
    
[^40]: 非对称网络逼近用于跨域学习

    Approximation by non-symmetric networks for cross-domain learning. (arXiv:2305.03890v1 [cs.LG])

    [http://arxiv.org/abs/2305.03890](http://arxiv.org/abs/2305.03890)

    本文研究使用非对称内核进行基于内核网络逼近的通用方法，结果表明它可以在跨域学习中显著提高基于内核网络的逼近能力。

    

    在过去的30年中，机器学习在众多过程（如：浅层或深度神经网络逼近、径向基函数网络和各种内核方法）的逼近能力（表达能力）研究中促进了大量的研究。本文针对不变学习、传递学习和合成孔径雷达成像等应用，引入了一种使用非对称内核来研究基于内核网络逼近能力的通用方法。我们考虑使用一组内核的更一般方法，如广义平移网络（其中包括神经网络和平移不变核作为特殊情况）和旋转区函数核。与传统的基于内核的逼近方法不同，我们不能要求内核是正定的。研究结果表明，使用非对称内核可以显著提高内核网络的逼近能力，特别是对于源域和目标域可能在分布上不同的跨域学习。

    For the past 30 years or so, machine learning has stimulated a great deal of research in the study of approximation capabilities (expressive power) of a multitude of processes, such as approximation by shallow or deep neural networks, radial basis function networks, and a variety of kernel based methods. Motivated by applications such as invariant learning, transfer learning, and synthetic aperture radar imaging, we initiate in this paper a general approach to study the approximation capabilities of kernel based networks using non-symmetric kernels. While singular value decomposition is a natural instinct to study such kernels, we consider a more general approach to include the use of a family of kernels, such as generalized translation networks (which include neural networks and translation invariant kernels as special cases) and rotated zonal function kernels. Naturally, unlike traditional kernel based approximation, we cannot require the kernels to be positive definite. Our results 
    
[^41]: 高维低秩张量赌博机研究

    On High-dimensional and Low-rank Tensor Bandits. (arXiv:2305.03884v1 [stat.ML])

    [http://arxiv.org/abs/2305.03884](http://arxiv.org/abs/2305.03884)

    本研究提出了一个通用的张量赌博机模型，其中行动和系统参数由张量表示，着重于未知系统张量为低秩的情况。所开发的 TOFU 算法首先利用灵活的张量回归技术估计与系统张量相关联的低维子空间，然后将原始问题转换成一个具有系统参数范数约束的新问题，并采用范数约束赌博子例程解决。

    

    大多数线性赌博机的研究都侧重于整个系统的一维特征。虽然代表性很强，但这种方法可能不能模拟高维但有优势结构的应用，例如用于推荐系统的低秩张量表示。为了解决这个限制，本研究研究了一个通用的张量赌博机模型，其中行动和系统参数由张量表示，而我们特别关注未知系统张量为低秩的情况。发展了一种新的赌博机算法TOFU（不确定性中的张量乐观），该算法首先利用灵活的张量回归技术估计与系统张量相关联的低维子空间。然后利用这些估计将原始问题转换为一个具有其系统参数范数约束的新问题。最后，TOFU采用范数约束赌博子例程，利用这些约束来实现问题的解决。

    Most existing studies on linear bandits focus on the one-dimensional characterization of the overall system. While being representative, this formulation may fail to model applications with high-dimensional but favorable structures, such as the low-rank tensor representation for recommender systems. To address this limitation, this work studies a general tensor bandits model, where actions and system parameters are represented by tensors as opposed to vectors, and we particularly focus on the case that the unknown system tensor is low-rank. A novel bandit algorithm, coined TOFU (Tensor Optimism in the Face of Uncertainty), is developed. TOFU first leverages flexible tensor regression techniques to estimate low-dimensional subspaces associated with the system tensor. These estimates are then utilized to convert the original problem to a new one with norm constraints on its system parameters. Lastly, a norm-constrained bandit subroutine is adopted by TOFU, which utilizes these constraint
    
[^42]: 通过流映射算子学习随机动力学系统

    Learning Stochastic Dynamical System via Flow Map Operator. (arXiv:2305.03874v1 [cs.LG])

    [http://arxiv.org/abs/2305.03874](http://arxiv.org/abs/2305.03874)

    该论文提出了一种通过测量数据学习未知随机动力学系统的数值框架随机流映射学习（sFML），在不同类型的随机系统上进行的全面实验证明了 sFML 的有效性。

    

    我们提出了一种通过测量数据学习未知随机动力学系统的数值框架。称为随机流映射学习（sFML），这个新框架是流映射学习（FML）的扩展，后者是为了学习确定性动力学系统而开发的。对于学习随机系统，我们定义了一个随机流映射，它是两个子流映射的叠加：一个确定性子映射和一个随机子映射。随机训练数据首先用于构建确定性子映射，然后是随机子映射。确定性子映射采用残差网络（ResNet）形式，类似于FML对于确定性系统的工作。对于随机子映射，我们采用生成模型，尤其是生成对抗网络（GANs）在本文中应用。最终构建的随机流映射定义了一个随机演化模型，它在分布方面是未知随机系统的弱近似。在不同类型的随机系统上进行的全面实验证明了sFML揭示未知随机系统各种类型的非线性、噪声协方差结构和时间相关特性的有效性。

    We present a numerical framework for learning unknown stochastic dynamical systems using measurement data. Termed stochastic flow map learning (sFML), the new framework is an extension of flow map learning (FML) that was developed for learning deterministic dynamical systems. For learning stochastic systems, we define a stochastic flow map that is a superposition of two sub-flow maps: a deterministic sub-map and a stochastic sub-map. The stochastic training data are used to construct the deterministic sub-map first, followed by the stochastic sub-map. The deterministic sub-map takes the form of residual network (ResNet), similar to the work of FML for deterministic systems. For the stochastic sub-map, we employ a generative model, particularly generative adversarial networks (GANs) in this paper. The final constructed stochastic flow map then defines a stochastic evolution model that is a weak approximation, in term of distribution, of the unknown stochastic system. A comprehensive set
    
[^43]: 在增长批量强化学习中教师向学习者的知识转移

    Knowledge Transfer from Teachers to Learners in Growing-Batch Reinforcement Learning. (arXiv:2305.03870v1 [cs.LG])

    [http://arxiv.org/abs/2305.03870](http://arxiv.org/abs/2305.03870)

    本研究讨论了实际领域中离线训练或增长批量训练的限制，提出了一种教师向学习者进行知识转移的方法，使得数据数量和多样性得到提高。

    

    序贯决策制定的标准方法利用了智能体不断与其环境交互和改善其控制策略的能力。然而，由于安全、伦理和实用性的限制，这种试错实验方法在许多实际领域（如医疗保健和机器人技术）中往往不可行。在这种情况下，这些领域中的控制策略通常是通过以先前记录的数据为基础进行离线训练或逐步扩展进行训练的。在这个设置中，固定的策略被部署到环境中，并用于收集一整个批次的新数据，然后与过去的批次汇总并用于更新策略。可以多次重复这个改进周期。虽然在实际领域中有限数量的这样的周期是可行的，但产生的数据数量和多样性远远低于标准的不断交互方法。但是，在这些领域中进行数据收集通常是与人类专家协作的。

    Standard approaches to sequential decision-making exploit an agent's ability to continually interact with its environment and improve its control policy. However, due to safety, ethical, and practicality constraints, this type of trial-and-error experimentation is often infeasible in many real-world domains such as healthcare and robotics. Instead, control policies in these domains are typically trained offline from previously logged data or in a growing-batch manner. In this setting a fixed policy is deployed to the environment and used to gather an entire batch of new data before being aggregated with past batches and used to update the policy. This improvement cycle can then be repeated multiple times. While a limited number of such cycles is feasible in real-world domains, the quantity and diversity of the resulting data are much lower than in the standard continually-interacting approach. However, data collection in these domains is often performed in conjunction with human expert
    
[^44]: 带有Hebbian可塑性的脉冲神经网络用于无监督表示学习。

    Spiking neural networks with Hebbian plasticity for unsupervised representation learning. (arXiv:2305.03866v1 [cs.NE])

    [http://arxiv.org/abs/2305.03866](http://arxiv.org/abs/2305.03866)

    本文介绍了一种新的脉冲神经网络模型，实现了从数据中无监督地学习分布式内部表示。该模型采用在线Hebbian-Bayesian学习和重连机制，并表现出和传统的非脉冲神经网络相近的性能。

    

    我们引入了一种新的脉冲神经网络模型，用于在无监督过程中从数据中学习分布式内部表示。我们通过将非脉冲前馈贝叶斯置信传播神经网络（BCPNN）模型转化为脉冲神经网络，并采用在线基于相关性的Hebbian-Bayesian学习和重连机制，该机制先前已表现出执行表示学习的能力。我们的脉冲模型采用泊松统计和低发火率与在体星形神经元相媲美。我们使用线性分类器评估了我们的脉冲模型学习的表示，并展示了与非脉冲BCPNN接近的性能，以及与其他基于Hebbian的脉冲网络在MNIST和F-MNIST机器学习基准测试中的竞争性能。

    We introduce a novel spiking neural network model for learning distributed internal representations from data in an unsupervised procedure. We achieved this by transforming the non-spiking feedforward Bayesian Confidence Propagation Neural Network (BCPNN) model, employing an online correlation-based Hebbian-Bayesian learning and rewiring mechanism, shown previously to perform representation learning, into a spiking neural network with Poisson statistics and low firing rate comparable to in vivo cortical pyramidal neurons. We evaluated the representations learned by our spiking model using a linear classifier and show performance close to the non-spiking BCPNN, and competitive with other Hebbian-based spiking networks when trained on MNIST and F-MNIST machine learning benchmarks.
    
[^45]: 基于软件的自动微分存在缺陷

    Software-based Automatic Differentiation is Flawed. (arXiv:2305.03863v1 [cs.LG])

    [http://arxiv.org/abs/2305.03863](http://arxiv.org/abs/2305.03863)

    软件实现的自动微分存在不可控的计算误差。

    

    各种软件尝试采用面向对象编程的思想，实现链式规则，并通过反向传播来实现所谓的自动微分。这样的框架在求值之前无法简化表达式（通过链式规则获得的），导致计算结果的误差往往无限大。

    Various software efforts embrace the idea that object oriented programming enables a convenient implementation of the chain rule, facilitating so-called automatic differentiation via backpropagation. Such frameworks have no mechanism for simplifying the expressions (obtained via the chain rule) before evaluating them. As we illustrate below, the resulting errors tend to be unbounded.
    
[^46]: 因果结构学习中的未解问题：以英国COVID-19为例研究

    Open problems in causal structure learning: A case study of COVID-19 in the UK. (arXiv:2305.03859v1 [cs.LG])

    [http://arxiv.org/abs/2305.03859](http://arxiv.org/abs/2305.03859)

    本文研究了因果机器学习在COVID-19英国疫情数据中的应用挑战，探讨了不同数据格式对学习类别不同的算法的影响，并突出了因果结构学习中的未解问题和未来研究方向。

    

    因果机器学习算法可以恢复图形结构，从而揭示因果关系。这些算法提供的因果表示使得透明度和可解释性得以实现。然而，与关联性机器学习相比，因果机器学习在实践中的影响有限。本文研究了因果机器学习在COVID-19英国疫情数据中的应用挑战。我们从各种公共来源整合数据，并研究各种结构学习算法从这些数据中学到的内容。我们探讨了不同数据格式对学习类别不同的算法的影响，并评估了每个算法及算法组产生的结果，包括图形结构、模型维度、敏感性分析、混淆变量、预测和干预推断等。我们利用这些结果来突出因果结构学习中的未解问题和未来研究方向。

    Causal machine learning (ML) algorithms recover graphical structures that tell us something about cause-and-effect relationships. The causal representation provided by these algorithms enables transparency and explainability, which is necessary in critical real-world problems. Yet, causal ML has had limited impact in practice compared to associational ML. This paper investigates the challenges of causal ML with application to COVID-19 UK pandemic data. We collate data from various public sources and investigate what the various structure learning algorithms learn from these data. We explore the impact of different data formats on algorithms spanning different classes of learning, and assess the results produced by each algorithm, and groups of algorithms, in terms of graphical structure, model dimensionality, sensitivity analysis, confounding variables, predictive and interventional inference. We use these results to highlight open problems in causal structure learning and directions f
    
[^47]: 无数据学习降阶运动学

    Data-Free Learning of Reduced-Order Kinematics. (arXiv:2305.03846v1 [cs.GR])

    [http://arxiv.org/abs/2305.03846](http://arxiv.org/abs/2305.03846)

    本研究提出了一种无数据学习降阶运动学的方法，能够自动识别物理系统的低维子空间。

    

    物理系统从弹性体到运动连杆都定义在高维配置空间上，但它们的典型低能配置集中在更低维的子空间中。本文解决了自动识别此类子空间的挑战。

    Physical systems ranging from elastic bodies to kinematic linkages are defined on high-dimensional configuration spaces, yet their typical low-energy configurations are concentrated on much lower-dimensional subspaces. This work addresses the challenge of identifying such subspaces automatically: given as input an energy function for a high-dimensional system, we produce a low-dimensional map whose image parameterizes a diverse yet low-energy submanifold of configurations. The only additional input needed is a single seed configuration for the system to initialize our procedure; no dataset of trajectories is required. We represent subspaces as neural networks that map a low-dimensional latent vector to the full configuration space, and propose a training scheme to fit network parameters to any system of interest. This formulation is effective across a very general range of physical systems; our experiments demonstrate not only nonlinear and very low-dimensional elastic body and cloth s
    
[^48]: 股票运动预测的时空变换器

    Spatiotemporal Transformer for Stock Movement Prediction. (arXiv:2305.03835v1 [cs.LG])

    [http://arxiv.org/abs/2305.03835](http://arxiv.org/abs/2305.03835)

    使用时空变换器-LSTM模型进行股票运动预测，取得了高准确率，并在模拟中显示出比标准普尔500股票指数更高的收益率。

    

    金融市场是一个引人入胜的地方，如果时间掌握得当，投资者可以获得巨大的利润。不幸的是，金融市场的动态、非线性特性使得未来价格走势难以预测。我们提出了STST，一种使用时空变换器-LSTM模型进行股票运动预测的新方法。我们的模型在ACL18和KDD17数据集上分别获得63.707%和56.879%的准确率。此外，我们的模型在模拟中用于确定其在现实生活中的适用性。它的最小年化收益率比标准普尔500股票指数高出10.41%以上。

    Financial markets are an intriguing place that offer investors the potential to gain large profits if timed correctly. Unfortunately, the dynamic, non-linear nature of financial markets makes it extremely hard to predict future price movements. Within the US stock exchange, there are a countless number of factors that play a role in the price of a company's stock, including but not limited to financial statements, social and news sentiment, overall market sentiment, political happenings and trading psychology. Correlating these factors is virtually impossible for a human. Therefore, we propose STST, a novel approach using a Spatiotemporal Transformer-LSTM model for stock movement prediction. Our model obtains accuracies of 63.707 and 56.879 percent against the ACL18 and KDD17 datasets, respectively. In addition, our model was used in simulation to determine its real-life applicability. It obtained a minimum of 10.41% higher profit than the S&P500 stock index, with a minimum annualized 
    
[^49]: 利用不确定性感知因果模型提高基于图像的精准医疗

    Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models. (arXiv:2305.03829v1 [cs.LG])

    [http://arxiv.org/abs/2305.03829](http://arxiv.org/abs/2305.03829)

    本研究采用贝叶斯深度学习估计多种治疗的因果后验分布，提高了基于图像的精准医疗的不确定性估计方法，以预测噪声多的医疗环境下的个体治疗效果。

    

    基于图像的精准医疗旨在根据个体的独特成像特征个性化诊疗决策，以改善其临床结果。集成不确定性估计作为治疗建议的机器学习框架将更加安全可靠。然而，在精准医疗中，几乎没有研究适应不确定性估计技术和验证指标。本文采用贝叶斯深度学习估计多种治疗的因果后验分布，从而估计每种治疗选项的不确定性以及任意两种治疗之间的个体治疗效果。我们对患有多发性硬化症的患者进行训练和评估，以预测新的和扩大的T2病变数量，并评估模型的相关性。

    Image-based precision medicine aims to personalize treatment decisions based on an individual's unique imaging features so as to improve their clinical outcome. Machine learning frameworks that integrate uncertainty estimation as part of their treatment recommendations would be safer and more reliable. However, little work has been done in adapting uncertainty estimation techniques and validation metrics for precision medicine. In this paper, we use Bayesian deep learning for estimating the posterior distribution over factual and counterfactual outcomes on several treatments. This allows for estimating the uncertainty for each treatment option and for the individual treatment effects (ITE) between any two treatments. We train and evaluate this model to predict future new and enlarging T2 lesion counts on a large, multi-center dataset of MR brain images of patients with multiple sclerosis, exposed to several treatments during randomized controlled trials. We evaluate the correlation of 
    
[^50]: 基于不确定性感知的Bootstrap学习用于远程监督数据联合抽取

    Uncertainty-Aware Bootstrap Learning for Joint Extraction on Distantly-Supervised Data. (arXiv:2305.03827v1 [cs.CL])

    [http://arxiv.org/abs/2305.03827](http://arxiv.org/abs/2305.03827)

    本文提出了基于不确定性感知的Bootstrap学习用于远程监督数据联合抽取。通过探索数据不确定性和自我集成正则化器，使得模型在早期快速收敛并且缓解了噪声标签产生的模型不确定性，并在两个大型数据集上实验表明该方法在实体对抽取和关系抽取方面的F1得分分别提高了4.43%和4.92%。

    

    在处理带有模糊或噪声标签的远程监督数据时，联合抽取实体对及其关系是具有挑战性的。为了缓解这种影响，我们提出了基于不确定性感知的Bootstrap学习，其动机是根据直觉，一个实例的不确定性越高，模型置信度与真实标签不一致的可能性就越大。具体而言，我们首先探索实例级别的数据不确定性，创建一个高置信的初始样例集。这样的子集用于过滤噪声实例，并有助于模型在早期快速收敛。在Bootstrap学习期间，我们提出自我集成作为正则化器，以减轻噪声标签产生的模型间不确定性。我们进一步定义联合标记概率的概率方差，以估计内部模型参数的不确定性，用于选择和建立新的可靠训练实例进行下一次迭代。两个大型数据集的实验结果表明，我们的方法明显优于最先进的基准方法，在实体对抽取和关系抽取方面的F1得分分别提高了4.43%和4.92%。

    Jointly extracting entity pairs and their relations is challenging when working on distantly-supervised data with ambiguous or noisy labels. To mitigate such impact, we propose uncertainty-aware bootstrap learning, which is motivated by the intuition that the higher uncertainty of an instance, the more likely the model confidence is inconsistent with the ground truths. Specifically, we first explore instance-level data uncertainty to create an initial high-confident examples. Such subset serves as filtering noisy instances and facilitating the model to converge fast at the early stage. During bootstrap learning, we propose self-ensembling as a regularizer to alleviate inter-model uncertainty produced by noisy labels. We further define probability variance of joint tagging probabilities to estimate inner-model parametric uncertainty, which is used to select and build up new reliable training instances for the next iteration. Experimental results on two large datasets reveal that our app
    
[^51]: 无遗憾的约束贝叶斯优化方法用于带有噪声和昂贵混合模型的差分分位数函数逼近

    No-Regret Constrained Bayesian Optimization of Noisy and Expensive Hybrid Models using Differentiable Quantile Function Approximations. (arXiv:2305.03824v1 [stat.ML])

    [http://arxiv.org/abs/2305.03824](http://arxiv.org/abs/2305.03824)

    本文提出了一种新颖的算法，CUQB，来解决复合函数（混合模型）的高效约束全局优化问题，并取得了良好的效果，在合成和真实的应用程序中均得到了验证，包括进行了最优控制的流体流量和拓扑结构优化，后者比当前最先进的设计强2倍。

    

    本文研究了复合函数（混合模型）的高效约束全局优化问题，该模型的输入是具有矢量值输出和有噪声观测的昂贵黑盒函数，这在实际的科学、工程、制造和控制应用中经常出现。我们提出了一种新颖的算法Constrained Upper Quantile Bound（CUQB），用于解决这种问题，直接利用了我们展示的目标和约束函数的复合结构，从而大大提高了采样效率。CUQB的概念简单，避免了先前方法所使用的约束逼近。虽然CUQB的收购函数不在封闭形式中，但我们提出了一种新颖的可微随机逼近，使其能够有效地最大化。我们进一步得出了对于累积遗憾和约束违规的界限。由于在某些规则假设下这些界限对迭代次数具有次线性依赖性，因此我们的算法在渐近意义下无遗憾并满足约束条件。我们在几个合成和真实的应用程序中展示了CUQB的功效，包括桥架拓扑 - 在其中，我们发现的结构比当前最先进的设计强2倍 - 以及流体流量的最优控制，其中我们使用的方法比以前的方法少了3倍的模拟。

    This paper investigates the problem of efficient constrained global optimization of composite functions (hybrid models) whose input is an expensive black-box function with vector-valued outputs and noisy observations, which often arises in real-world science, engineering, manufacturing, and control applications. We propose a novel algorithm, Constrained Upper Quantile Bound (CUQB), to solve such problems that directly exploits the composite structure of the objective and constraint functions that we show leads substantially improved sampling efficiency. CUQB is conceptually simple and avoids the constraint approximations used by previous methods. Although the CUQB acquisition function is not available in closed form, we propose a novel differentiable stochastic approximation that enables it to be efficiently maximized. We further derive bounds on the cumulative regret and constraint violation. Since these bounds depend sublinearly on the number of iterations under some regularity assum
    
[^52]: fMRI脑网络的深度标记

    Deep Labeling of fMRI Brain Networks. (arXiv:2305.03814v1 [cs.LG])

    [http://arxiv.org/abs/2305.03814](http://arxiv.org/abs/2305.03814)

    本文提出了一种可靠、快速的深度学习方法来标记大规模功能连接模式(静息状态网络)，其可用于术前规划指导神经外科医生，具有较强的泛化性和高准确度。

    

    从静息状态功能磁共振成像(RS-fMRI)中提取出的脑静息状态网络(RSNs)，被用于术前规划指导神经外科医生，然而，专业知识需要用于标记每个RSN，并缺乏有效和标准化的方法用于临床工作流程。我们提出了一种准确、快速、轻量级的深度学习方法来标记静息状态网络。我们使用群组独立分量分析(IC/A)提取大规模的功能连接模式，并使用双重回归将它们投射到个体主体RSN上。我们将基于MLP的方法与2D和3D卷积神经网络(CNN)进行比较，并发现MLP更快、更准确。尽管它比其他作品表现得好或更好，但我们的MLP方法是可推广的，因为该方法需要在不同的采集技术下表现良好。

    Resting State Networks (RSNs) of the brain extracted from Resting State functional Magnetic Resonance Imaging (RS-fMRI) are used in the pre-surgical planning to guide the neurosurgeon. This is difficult, though, as expert knowledge is required to label each of the RSNs. There is a lack of efficient and standardized methods to be used in clinical workflows. Additionally, these methods need to be generalizable since the method needs to work well regardless of the acquisition technique. We propose an accurate, fast, and lightweight deep learning approach to label RSNs. Group Independent Component Analysis (ICA) was used to extract large scale functional connectivity patterns in the cohort and dual regression was used to back project them on individual subject RSNs. We compare a Multi-Layer Perceptron (MLP) based method with 2D and 3D Convolutional Neural Networks (CNNs) and find that the MLP is faster and more accurate. The MLP method performs as good or better than other works despite it
    
[^53]: 基于等变神经网络的漂泊磁体自旋动力学模拟

    Equivariant Neural Networks for Spin Dynamics Simulations of Itinerant Magnets. (arXiv:2305.03804v1 [cond-mat.str-el])

    [http://arxiv.org/abs/2305.03804](http://arxiv.org/abs/2305.03804)

    本论文提出了一种新的等变神经网络架构来模拟Kondo晶格模型的大规模自旋动力学，能够重现三角晶格中天空子晶体相变的能力。

    

    本论文提出了一种新的等变神经网络结构来模拟Kondo晶格模型的大规模自旋动力学。该神经网络主要由基于张量积的卷积层构成，并确保两个等变性：晶格的平移和自旋的旋转。本文实现了对二维正方形和三角形晶格上两种Kondo晶格模型的等变神经网络，并进行训练和验证。在正方形晶格的等变模型中，验证误差（基于均方根误差）相对于使用不变描述符作为输入的模型减少了三分之一。此外，通过使用训练模型进行动态模拟，证明了重现三角晶格中天空子晶体相变的能力。

    I present a novel equivariant neural network architecture for the large-scale spin dynamics simulation of the Kondo lattice model. This neural network mainly consists of tensor-product-based convolution layers and ensures two equivariances: translations of the lattice and rotations of the spins. I implement equivariant neural networks for two Kondo lattice models on two-dimensional square and triangular lattices, and perform training and validation. In the equivariant model for the square lattice, the validation error (based on root mean squared error) is reduced to less than one-third compared to a model using invariant descriptors as inputs. Furthermore, I demonstrate the ability to reproduce phase transitions of skyrmion crystals in the triangular lattice, by performing dynamics simulations using the trained model.
    
[^54]: 材料信息学：算法设计规则。

    Materials Informatics: An Algorithmic Design Rule. (arXiv:2305.03797v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2305.03797](http://arxiv.org/abs/2305.03797)

    材料信息学是材料科学研究中的第四个范式，通过材料特性指纹和统计推断学习理论，结合神经网络拓扑、逻辑公理和推理信息科学等方法，可以实现有机半导体的新型发现和知识提取。

    

    材料信息学是材料科学研究中的第四个范式，是传统经验方法、理论科学和计算研究之后的数据驱动的研究方法。材料信息学的基本要素有两个：材料特性指纹和统计推断学习理论。我们运用了多种神经网络拓扑、逻辑公理和推理信息科学等方法，通过材料信息学方法研究了有机半导体的谜团，并开发了数据驱动的程序，以实现半导体工业的新型有机半导体发现和材料科学界的知识提取。我们还对材料信息学数据集的神经网络设计拓扑方式进行了综述和相关研究。

    Materials informatics, data-enabled investigation, is a "fourth paradigm" in materials science research after the conventional empirical approach, theoretical science, and computational research. Materials informatics has two essential ingredients: fingerprinting materials proprieties and the theory of statistical inference and learning. We have researched the organic semiconductor's enigmas through the materials informatics approach. By applying diverse neural network topologies, logical axiom, and inferencing information science, we have developed data-driven procedures for novel organic semiconductor discovery for the semiconductor industry and knowledge extraction for the materials science community. We have reviewed and corresponded with various algorithms for the neural network design topology for the materials informatics dataset.
    
[^55]: 采用任务不可知本体和简单标签的零样本帧语义解析技术

    Towards Zero-Shot Frame Semantic Parsing with Task Agnostic Ontologies and Simple Labels. (arXiv:2305.03793v1 [cs.CL])

    [http://arxiv.org/abs/2305.03793](http://arxiv.org/abs/2305.03793)

    本文提出了一种名为OpenFSP的框架，可以通过一些简单的标签方便地创建新领域，并在给定注释后，利用句子编码器的匹配算法预测由终端用户定义的领域的意图和插槽，从而解决了当前模型需要大量训练数据的问题。

    

    帧语义解析是面向任务的对话系统的重要组成部分。当前的模型需要大量的训练数据才能成功地识别用户输入话语中的意图和插槽。这在将新领域添加到虚拟助手功能时会产生重要障碍，因为这些数据的创建需要高度专业化的自然语言处理技能。在本文中，我们提出了OpenFSP，这是一个框架，可以从少量能够在不需要NLP知识的情况下生成的简单标签中方便地创建新领域。我们的方法依赖于创建一个小而富有表现力的、与领域无关的插槽类型集，以实现对新领域的简单注释。在给定这样的注释后，依赖于句子编码器的匹配算法预测由终端用户定义的领域的意图和插槽。在TopV2数据集上进行了深入的实验，结果表明我们的模型在这种简单标签设置下优于强基线模型。

    Frame semantic parsing is an important component of task-oriented dialogue systems. Current models rely on a significant amount training data to successfully identify the intent and slots in the user's input utterance. This creates a significant barrier for adding new domains to virtual assistant capabilities, as creation of this data requires highly specialized NLP expertise. In this work we propose OpenFSP, a framework that allows for easy creation of new domains from a handful of simple labels that can be generated without specific NLP knowledge. Our approach relies on creating a small, but expressive, set of domain agnostic slot types that enables easy annotation of new domains. Given such annotation, a matching algorithm relying on sentence encoders predicts the intent and slots for domains defined by end-users. Extensive experiments on the TopV2 dataset shows that our model outperforms strong baselines in this simple labels setting.
    
[^56]: 多臂赌博机的上下文利用与探索的神经网络研究

    Neural Exploitation and Exploration of Contextual Bandits. (arXiv:2305.03784v1 [cs.LG])

    [http://arxiv.org/abs/2305.03784](http://arxiv.org/abs/2305.03784)

    本文提出了一种新型的神经网络策略"EE-Net"，它用于多臂赌博机的利用和探索，在学习奖励函数的同时也适应性地学习潜在收益。

    

    本文研究利用神经网络进行上下文多臂赌博机的利用和探索。我们提出了一个名为"EE-Net"的新型神经网络利用和探索策略，它使用一个神经网络（利用网络）来学习奖励函数，另一个神经网络（探索网络）来适应性地学习相对于当前估计奖励的潜在收益。

    In this paper, we study utilizing neural networks for the exploitation and exploration of contextual multi-armed bandits. Contextual multi-armed bandits have been studied for decades with various applications. To solve the exploitation-exploration trade-off in bandits, there are three main techniques: epsilon-greedy, Thompson Sampling (TS), and Upper Confidence Bound (UCB). In recent literature, a series of neural bandit algorithms have been proposed to adapt to the non-linear reward function, combined with TS or UCB strategies for exploration. In this paper, instead of calculating a large-deviation based statistical bound for exploration like previous methods, we propose, ``EE-Net,'' a novel neural-based exploitation and exploration strategy. In addition to using a neural network (Exploitation network) to learn the reward function, EE-Net uses another neural network (Exploration network) to adaptively learn the potential gains compared to the currently estimated reward for exploration
    
[^57]: 物理知识的局部化学习用于对流扩散反应系统

    Physics-Informed Localized Learning for Advection-Diffusion-Reaction Systems. (arXiv:2305.03774v1 [cs.LG])

    [http://arxiv.org/abs/2305.03774](http://arxiv.org/abs/2305.03774)

    提出一种新的物理知识的、边界条件感知的、局部化学习方法，将E2C和E2CO模型扩展到对流扩散反应系统中，可以以极高的准确性预测系统未来状态，同时大幅减少训练时间。

    

    全球对新能源解决方案的追求，如地热和碳捕集和封存计划，对当前先进的地下流体模拟器产生了新的需求。要求能够在短时间内同时模拟大量储层状态，为应用机器学习技术进行代理建模开辟了机会。我们提出了一种新的局部学习方法，该方法具有物理知识和边界条件意识，将嵌入式控制（E2C）和嵌入式控制和观察（E2CO）模型扩展到学习对流扩散反应系统中全局状态变量的局部表示。我们展示了我们的模型在储层模拟数据上训练，能够以只使用少量可用的信息，高度准确地预测系统的未来状态，同时与原始的E2C和E2C

    The global push for new energy solutions, such as Geothermal, and Carbon Capture and Sequestration initiatives has thrust new demands upon the current state-of the-art subsurface fluid simulators. The requirement to be able to simulate a large order of reservoir states simultaneously in a short period of time has opened the door of opportunity for the application of machine learning techniques for surrogate modelling. We propose a novel physics-informed and boundary conditions-aware Localized Learning method which extends the Embed-to-Control (E2C) and Embed-to-Control and Observed (E2CO) models to learn local representations of global state variables in an Advection-Diffusion Reaction system. We show that our model trained on reservoir simulation data is able to predict future states of the system, given a set of controls, to a great deal of accuracy with only a fraction of the available information, while also reducing training times significantly compared to the original E2C and E2C
    
[^58]: 银河系中的弱监督异常检测

    Weakly-Supervised Anomaly Detection in the Milky Way. (arXiv:2305.03761v1 [astro-ph.GA])

    [http://arxiv.org/abs/2305.03761](http://arxiv.org/abs/2305.03761)

    本文使用弱监督异常检测方法CWoLa来识别银河系中的冷凝聚流，该方法可以在不需要标记流或了解天体物理原理的情况下工作。该方法适用于识别局部异常的领域，并在天体物理学中具有广泛的适用性。

    

    大规模的天体物理数据集为新的机器学习技术提供了机会，能够识别出传统搜索可能忽略的感兴趣区域。为此，本文使用了一种名为CWoLa的弱监督异常检测方法，来确定由Gaia卫星观测到的十亿多颗银河系恒星中的冷凝聚流。CWoLa操作不需要使用标记流或了解天体物理原理。相反，我们训练分类器来区分比例未知的混合样本之间的差异。这种计算轻量级的策略能够检测出模拟的流和已知的流GD-1。原本设计用于高能对撞机物理，该技术在天体物理学以及其他有兴趣识别局部异常的领域可能有广泛的适用性。

    Large-scale astrophysics datasets present an opportunity for new machine learning techniques to identify regions of interest that might otherwise be overlooked by traditional searches. To this end, we use Classification Without Labels (CWoLa), a weakly-supervised anomaly detection method, to identify cold stellar streams within the more than one billion Milky Way stars observed by the Gaia satellite. CWoLa operates without the use of labeled streams or knowledge of astrophysical principles. Instead, we train a classifier to distinguish between mixed samples for which the proportions of signal and background samples are unknown. This computationally lightweight strategy is able to detect both simulated streams and the known stream GD-1 in data. Originally designed for high-energy collider physics, this technique may have broad applicability within astrophysics as well as other domains interested in identifying localized anomalies.
    
[^59]: 基于数据驱动同化和预测的Sentinel-2反射动力学学习

    Learning Sentinel-2 reflectance dynamics for data-driven assimilation and forecasting. (arXiv:2305.03743v1 [eess.IV])

    [http://arxiv.org/abs/2305.03743](http://arxiv.org/abs/2305.03743)

    本篇论文提出了基于Koopman算子的无监督深度学习模型，用于学习长期反射动力学，该模型可以作为数据同化的先验，数据集公开发布，适用于Sentinel-2多光谱图像时间序列。

    

    在过去几年中，覆盖地球表面的大规模卫星多光谱和高光谱图像已被公开用于科学研究，例如通过欧洲Copernicus项目。同时，自监督学习（SSL）方法的发展引起了遥感界的极大关注，使得能够从未标记数据中学习潜在表示，以帮助处理下游任务，如插值、预测或解混。沿着这条线路，我们以Koopman算子理论为灵感，以无监督的方式训练深度学习模型来模拟长期反射动力学。我们展示了这个训练好的模型，作为一个可微分的模型，可以直接用作数据同化的先验。我们的数据集由Sentinel-2多光谱图像时间序列组成，并公开发布了多个级别的处理。

    Over the last few years, massive amounts of satellite multispectral and hyperspectral images covering the Earth's surface have been made publicly available for scientific purpose, for example through the European Copernicus project. Simultaneously, the development of self-supervised learning (SSL) methods has sparked great interest in the remote sensing community, enabling to learn latent representations from unlabeled data to help treating downstream tasks for which there is few annotated examples, such as interpolation, forecasting or unmixing. Following this line, we train a deep learning model inspired from the Koopman operator theory to model long-term reflectance dynamics in an unsupervised way. We show that this trained model, being differentiable, can be used as a prior for data assimilation in a straightforward way. Our datasets, which are composed of Sentinel-2 multispectral image time series, are publicly released with several levels of treatment.
    
[^60]: 不同iable符号编程提高语言模型的逻辑推理能力

    Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming. (arXiv:2305.03742v1 [cs.AI])

    [http://arxiv.org/abs/2305.03742](http://arxiv.org/abs/2305.03742)

    本文提出一种新的可微分符号推理框架，DSR-LM，用于提高预训练语言模型的逻辑推理能力，不像以往的研究依赖手工制定的逻辑规则，该框架有效地学习加权规则，并应用语义损失进一步改善LMs的逻辑推理能力。

    

    尽管语言模型在规模和组合性方面取得了进展，但预训练的大型语言模型仍然难以可靠地执行逻辑推理。本文基于符号编程的视角解决了这一挑战。我们提出了DSR-LM，一种可微分的符号推理框架，其中预训练的LMs管理事实知识的感知，符号模块执行演绎推理。与依赖手工制定的逻辑规则的作品不同，我们的可微分符号推理框架有效地学习加权规则，并应用语义损失进一步改善LMs。DSR-LM具有可扩展性、可解释性，并允许轻松集成先前的知识，从而支持广泛的符号编程，以稳健地推出逻辑结论。我们的实验结果表明，DSR-LM提高了预训练语言模型的逻辑推理能力，在演绎推理基准测试中的准确性显著提高了20%以上。此外，DSR-LM还可以有效处理自然语言解释问题，包括开放式的逻辑推理任务。

    Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality. In this work, we tackle this challenge through the lens of symbolic programming. We propose DSR-LM, a Differentiable Symbolic Reasoning framework where pre-trained LMs govern the perception of factual knowledge, and a symbolic module performs deductive reasoning. In contrast to works that rely on hand-crafted logic rules, our differentiable symbolic reasoning framework efficiently learns weighted rules and applies semantic loss to further improve LMs. DSR-LM is scalable, interpretable, and allows easy integration of prior knowledge, thereby supporting extensive symbolic programming to robustly derive a logical conclusion. The results of our experiments suggest that DSR-LM improves the logical reasoning abilities of pre-trained language models, resulting in a significant increase in accuracy of over 20% on deductive reasoning benchmarks. Furthermore, DSR
    
[^61]: AmGCL: 基于自监督对比学习的属性缺失图特征插补方法

    AmGCL: Feature Imputation of Attribute Missing Graph via Self-supervised Contrastive Learning. (arXiv:2305.03741v1 [cs.LG])

    [http://arxiv.org/abs/2305.03741](http://arxiv.org/abs/2305.03741)

    AmGCL是一个用于处理属性缺失图数据的自监督对比学习框架，可以通过特征插补和潜在表示学习来解决属性图中节点属性缺失的问题。

    

    属性图在多媒体应用中使用广泛，图表示学习（GRL）已经成功地应用于属性图数据分析。然而，不完整的图数据和缺失的节点属性可能会对媒体知识发现产生负面影响。现有的处理属性缺失图的方法存在有限的假设或无法捕捉复杂的属性-图形依赖关系。为了解决这些挑战，我们提出了基于自监督对比学习的属性缺失图对比学习（AmGCL）框架，用于处理属性图数据中的缺失节点属性。AmGCL利用基于狄利克雷能量的特征预编码来对缺失属性进行编码，利用自监督图增强对比学习结构（GACLS）从编码数据中学习潜在变量。具体而言，AmGCL利用结构-属性能量最小化进行特征重构，同时最大化互信息的下限以学习潜在表示。

    Attribute graphs are ubiquitous in multimedia applications, and graph representation learning (GRL) has been successful in analyzing attribute graph data. However, incomplete graph data and missing node attributes can have a negative impact on media knowledge discovery. Existing methods for handling attribute missing graph have limited assumptions or fail to capture complex attribute-graph dependencies. To address these challenges, we propose Attribute missing Graph Contrastive Learning (AmGCL), a framework for handling missing node attributes in attribute graph data. AmGCL leverages Dirichlet energy minimization-based feature precoding to encode in missing attributes and a self-supervised Graph Augmentation Contrastive Learning Structure (GACLS) to learn latent variables from the encoded-in data. Specifically, AmGCL utilizies feature reconstruction based on structure-attribute energy minimization while maximizes the lower bound of evidence for latent representation mutual information.
    
[^62]: 在背景下进行驾驶风险预测：一种基于遥感数据的框架

    Judge Me in Context: A Telematics-Based Driving Risk Prediction Framework in Presence of Weak Risk Labels. (arXiv:2305.03740v1 [cs.LG])

    [http://arxiv.org/abs/2305.03740](http://arxiv.org/abs/2305.03740)

    该研究提出了一种基于遥感数据的驾驶风险预测框架，最大化利用情境信息，采用新颖的数据驱动过程增强弱的风险标签以提高分类器性能。

    

    过去几十年，驾驶风险预测一直是研究的热点之一，旨在最小化驾驶风险并提高安全性。人口统计信息在风险预测中的利用是传统的解决方案，在保险规划中有应用，但是通过这样的粗粒度因素很难捕捉到真实的驾驶行为。因此，在过去十年中，遥感数据的使用已经得到广泛的应用。大多数现有的研究利用人口统计信息以及遥感数据，而我们的目标是最大限度地利用遥感数据以及情境信息（例如，道路类型）来构建具有实际应用的风险预测框架。我们将遥感数据情境化，并使用它们来开发一个风险分类器，假设有一些弱的风险标签可用（例如，过去的交通违章记录）。然而，在构建风险分类器之前，我们采用一种新颖的数据驱动过程来增强弱的风险标签而提高分类器的性能。

    Driving risk prediction has been a topic of much research over the past few decades to minimize driving risk and increase safety. The use of demographic information in risk prediction is a traditional solution with applications in insurance planning, however, it is difficult to capture true driving behavior via such coarse-grained factors. Therefor, the use of telematics data has gained a widespread popularity over the past decade. While most of the existing studies leverage demographic information in addition to telematics data, our objective is to maximize the use of telematics as well as contextual information (e.g., road-type) to build a risk prediction framework with real-world applications. We contextualize telematics data in a variety of forms, and then use it to develop a risk classifier, assuming that there are some weak risk labels available (e.g., past traffic citation records). Before building a risk classifier though, we employ a novel data-driven process to augment weak r
    
[^63]: 面向英特尔Movidius VPU的神经架构搜索

    Neural Architecture Search for Intel Movidius VPU. (arXiv:2305.03739v1 [cs.NE])

    [http://arxiv.org/abs/2305.03739](http://arxiv.org/abs/2305.03739)

    本文提出了面向英特尔Movidius VPU的神经架构搜索技术，并通过引入硬件成本模型，实现了在VPU上进行分类任务和超分辨率任务的加速和提高。

    

    硬件感知神经架构搜索（NAS）技术已经被提出，以自动化和加速模型设计，以满足给定硬件上的质量和推理效率要求。过去的研究已经展示了NAS在硬件特定网络设计方面的能力。在本文中，我们进一步将NAS应用于英特尔Movidius VPU（视觉处理器单元）。为了确定要并入NAS过程中的硬件成本，我们引入了两种方法：在设备上预先收集的硬件成本和设备特定的硬件成本模型VPUNN。通过NAS的帮助，在VPU上进行分类任务时，我们可以实现比Mobilenet-v2-1.4更快1.3倍的fps加速，并在相同的精度下比Resnet50更快2.2倍。对于VPU上的超分辨率任务，与EDSR3相比，我们可以实现1.08倍的PSNR和6倍的更高fps。

    Hardware-aware Neural Architecture Search (NAS) technologies have been proposed to automate and speed up model design to meet both quality and inference efficiency requirements on a given hardware. Prior arts have shown the capability of NAS on hardware specific network design. In this whitepaper, we further extend the use of NAS to Intel Movidius VPU (Vision Processor Units). To determine the hardware-cost to be incorporated into the NAS process, we introduced two methods: pre-collected hardware-cost on device and device-specific hardware-cost model VPUNN. With the help of NAS, for classification task on VPU, we can achieve 1.3x fps acceleration over Mobilenet-v2-1.4 and 2.2x acceleration over Resnet50 with the same accuracy score. For super resolution task on VPU, we can achieve 1.08x PSNR and 6x higher fps compared with EDSR3.
    
[^64]: 调整传统语言处理方法以适用于普什图语文本分类

    Tuning Traditional Language Processing Approaches for Pashto Text Classification. (arXiv:2305.03737v1 [cs.CL])

    [http://arxiv.org/abs/2305.03737](http://arxiv.org/abs/2305.03737)

    本研究建立了一个普什图语自动文本分类系统，通过比较不同模型和特征提取方法，实验结果表明使用tf-idf特征提取方法的SVM模型在普什图语文本分类中取得了最高的准确率。

    

    文本分类在很多领域中都是一个重要的任务。因此，已经进行了多项研究来开发国际和本地语言的自动文本分类系统。然而，需要为本地语言建立一个自动文本分类系统。本研究的主要目的是建立一个普什图语自动文本分类系统。为了实现这个目标, 我们建立了一个普什图语语料库，这是由于普什图语文本文档的公共数据集不可用。此外，本研究比较了包括多层感知器(MLP)、支持向量机(SVM)、K近邻(KNN)、决策树, 高斯朴素贝叶斯, 多项式朴素贝叶斯, 随机森林和逻辑回归在内的多个模型，以发现最有效的方法。此外，本研究还评估了两种不同的特征提取方法，包括词袋和tf-idf。实验结果表明，在普什图语文本分类中，使用tf-idf特征提取方法的SVM模型取得了最高的97.19%的准确率。

    Today text classification becomes critical task for concerned individuals for numerous purposes. Hence, several researches have been conducted to develop automatic text classification for national and international languages. However, the need for an automatic text categorization system for local languages is felt. The main aim of this study is to establish a Pashto automatic text classification system. In order to pursue this work, we built a Pashto corpus which is a collection of Pashto documents due to the unavailability of public datasets of Pashto text documents. Besides, this study compares several models containing both statistical and neural network machine learning techniques including Multilayer Perceptron (MLP), Support Vector Machine (SVM), K Nearest Neighbor (KNN), decision tree, gaussian na\"ive Bayes, multinomial na\"ive Bayes, random forest, and logistic regression to discover the most effective approach. Moreover, this investigation evaluates two different feature extr
    
[^65]: 基于评分的输运建模在平均场福克-普朗克方程求解中的应用

    Score-based Transport Modeling for Mean-Field Fokker-Planck Equations. (arXiv:2305.03729v1 [math.NA])

    [http://arxiv.org/abs/2305.03729](http://arxiv.org/abs/2305.03729)

    本文介绍了基于评分的输运建模方法（MSBTM）在求解平均场福克-普朗克方程中的应用。通过建立Kullback-Leibler散度时间导数的上界，验证了MSBTM方法的有效性。通过对MSBTM解、相应随机微分方程的积分结果和解析解（如果有）的定性和定量比较，验证了MSBTM算法的可行性。

    

    我们采用基于评分的输运建模方法（MSBTM）解决平均场福克-普朗克方程。我们从精确解到MSBTM数值估算建立了Kullback-Leibler（KL）散度时间导数的上界，从而验证了MSBTM方法的有效性。此外，我们还为该算法提供了误差分析。在数值实验中，我们研究了两种类型的平均场福克-普朗克方程以及它们对应的相互作用系统中粒子的动力学。通过MSBTM解、相应随机微分方程的积分结果和解析解（如果有）的定性和定量比较，验证了MSBTM算法的可行性。

    We use the score-based transport modeling method to solve the mean-field Fokker-Planck equations, which we call MSBTM. We establish an upper bound on the time derivative of the Kullback-Leibler (KL) divergence to MSBTM numerical estimation from the exact solution, thus validates the MSBTM approach. Besides, we provide an error analysis for the algorithm. In numerical experiments, we study two types of mean-field Fokker-Planck equation and their corresponding dynamics of particles in interacting systems. The MSBTM algorithm is numerically validated through qualitative and quantitative comparison between the MSBTM solutions, the results of integrating the associated stochastic differential equation and the analytical solutions if available.
    
[^66]: 关于神经网络原像近似的研究

    On Preimage Approximation for Neural Networks. (arXiv:2305.03686v1 [cs.SE])

    [http://arxiv.org/abs/2305.03686](http://arxiv.org/abs/2305.03686)

    本文提出了一种基于线性松弛的高效实用的任意时刻算法，用于生成神经网络原像的符号下近似，以实现更快的改进和更高的压缩度。

    

    神经网络验证主要关注局部鲁棒性，然而，通常需要知道给定属性是否在整个输入域内全局成立，如果不成立，则需要知道属性成立的输入比例是多少。尽管精确的原像生成可以构建神经网络的等价表示，但在规模上是难以处理的。本文提出了一种基于线性松弛的高效实用的任意时刻算法，用于生成神经网络原像的符号下近似。我们的算法通过将输入区域划分为子区域，其中神经网络松弛边界变得更紧，迭代地最小化体积逼近误差。我们进一步采用采样和可微体积逼近来优先划分区域，并优化松弛的参数，从而实现更快的改进和更高的压缩度。

    Neural network verification mainly focuses on local robustness properties. However, often it is important to know whether a given property holds globally for the whole input domain, and if not then for what proportion of the input the property is true. While exact preimage generation can construct an equivalent representation of neural networks that can aid such (quantitative) global robustness verification, it is intractable at scale. In this work, we propose an efficient and practical anytime algorithm for generating symbolic under-approximations of the preimage of neural networks based on linear relaxation. Our algorithm iteratively minimizes the volume approximation error by partitioning the input region into subregions, where the neural network relaxation bounds become tighter. We further employ sampling and differentiable approximations to the volume in order to prioritize regions to split and optimize the parameters of the relaxation, leading to faster improvement and more compa
    
[^67]: 白盒多目标对话生成对抗攻击

    White-Box Multi-Objective Adversarial Attack on Dialogue Generation. (arXiv:2305.03655v1 [cs.CL])

    [http://arxiv.org/abs/2305.03655](http://arxiv.org/abs/2305.03655)

    该论文提出了一种新的白盒多目标对话生成对抗攻击方法，DGSlow。通过平衡生成准确性和长度两个目标，DGSlow利用生成更长的输出来提高攻击效果。

    

    预训练的转换器在最先进的对话生成系统中很受欢迎。 然而，这种语言模型容易受到各种对抗样本的攻击，这在传统任务（如文本分类）中已经得到了研究，这激发了我们对它们在DG系统中的鲁棒性的好奇心。 其中一个主要的挑战是攻击DG模型的时候，对当前句子的扰动几乎不会降低响应的准确性，因为未改变的聊天记录也会被考虑进行决策。我们观察到，通过精心制作对抗样本来迫使生成更长的输出，有利于攻击的有效性 - 生成的响应通常是不相关、冗长和重复的。因此，我们提出了一种名为DGSlow的白盒多目标攻击方法。具体来说，DGSlow通过基于梯度的多目标优化器平衡两个目标 - 生成准确度和长度，并使用自适应方法实施攻击。

    Pre-trained transformers are popular in state-of-the-art dialogue generation (DG) systems. Such language models are, however, vulnerable to various adversarial samples as studied in traditional tasks such as text classification, which inspires our curiosity about their robustness in DG systems. One main challenge of attacking DG models is that perturbations on the current sentence can hardly degrade the response accuracy because the unchanged chat histories are also considered for decision-making. Instead of merely pursuing pitfalls of performance metrics such as BLEU, ROUGE, we observe that crafting adversarial samples to force longer generation outputs benefits attack effectiveness -- the generated responses are typically irrelevant, lengthy, and repetitive. To this end, we propose a white-box multi-objective attack method called DGSlow. Specifically, DGSlow balances two objectives -- generation accuracy and length, via a gradient-based multi-objective optimizer and applies an adapti
    
[^68]: 对比损失作为全局上位联系模型的广义模型

    Contrastive losses as generalized models of global epistasis. (arXiv:2305.03136v1 [q-bio.PE])

    [http://arxiv.org/abs/2305.03136](http://arxiv.org/abs/2305.03136)

    对比损失函数可以用于提取全局上位联系模型所隐含的稀疏潜在函数，这种方法可能为蛋白质工程和相关领域的适应性函数推断提供有用的通用框架。

    

    适应性函数将生物序列的大组合空间映射到所关注的特性上。从实验数据中推断这些多模态函数是现代蛋白质工程中的核心任务。全局上位联系模型是一类有效且有物理基础的模型，可用于估计从观察数据中推断适应性函数。这些模型假设稀疏的潜在函数通过单调非线性变换以发射可测的适应性。在这里，我们展示了最小化对比损失函数（如 Bradley-Terry 损失）是提取全局上位联系所隐示的稀疏潜在函数的一种简单灵活的技术。我们通过适应性-上位联系不确定性原理争辩，全局上位联系模型中的非线性可以产生不具备稀疏表示的观察适应性函数，因此可能不适合使用均方误差（MSE）损失（一种常见的做法）从观察中学习。我们表明，对比损失可用于推断不适合 MSE 损失的适应性函数，并且全局上位联系模型可以解释为一种规则化的对比损失模型。我们的结果表明，这种方法可能为蛋白质工程和相关领域的适应性函数推断提供有用的通用框架。

    Fitness functions map large combinatorial spaces of biological sequences to properties of interest. Inferring these multimodal functions from experimental data is a central task in modern protein engineering. Global epistasis models are an effective and physically-grounded class of models for estimating fitness functions from observed data. These models assume that a sparse latent function is transformed by a monotonic nonlinearity to emit measurable fitness. Here we demonstrate that minimizing contrastive loss functions, such as the Bradley-Terry loss, is a simple and flexible technique for extracting the sparse latent function implied by global epistasis. We argue by way of a fitness-epistasis uncertainty principle that the nonlinearities in global epistasis models can produce observed fitness functions that do not admit sparse representations, and thus may be inefficient to learn from observations when using a Mean Squared Error (MSE) loss (a common practice). We show that contrasti
    
[^69]: 超越同质性：重构结构实现对图形无关聚类

    Beyond Homophily: Reconstructing Structure for Graph-agnostic Clustering. (arXiv:2305.02931v1 [cs.SI] CROSS LISTED)

    [http://arxiv.org/abs/2305.02931](http://arxiv.org/abs/2305.02931)

    本论文提出了一种新颖的图形聚类方法，能够超越同质性假设，重构结构实现对图形无关聚类，包括三个关键组件：图形重构、混合滤波器和双图形聚类网络。为了减少节点属性和拓扑结构之间的不良耦合，我们将它们分别映射到两个子空间中。

    

    基于图神经网络（GNN）的聚类方法在节点聚类任务中取得了惊人的表现。然而，它们是基于同质图的假设设计的，而在异质图上进行聚类被忽视了。由于缺乏标签，不可能在找到适合的GNN模型之前首先将图形识别为同质或异质。因此，对于各种同质性水平的真实世界图形进行聚类将为图形研究社区带来新的挑战。为填补这一空白，我们提出了一种新颖的图形聚类方法，其中包含三个关键组件：图形重构、混合滤波器和双图形聚类网络。为了使其对图形无关，我们根据数据构建了高度同质性和异质性的两个图形。基于新图构建的混合滤波器提取了低频和高频信息。为了减少节点属性和拓扑结构之间的不良耦合，我们将它们分别映射到两个子空间中。

    Graph neural networks (GNNs) based methods have achieved impressive performance on node clustering task. However, they are designed on the homophilic assumption of graph and clustering on heterophilic graph is overlooked. Due to the lack of labels, it is impossible to first identify a graph as homophilic or heterophilic before a suitable GNN model can be found. Hence, clustering on real-world graph with various levels of homophily poses a new challenge to the graph research community. To fill this gap, we propose a novel graph clustering method, which contains three key components: graph reconstruction, a mixed filter, and dual graph clustering network. To be graph-agnostic, we empirically construct two graphs which are high homophily and heterophily from each data. The mixed filter based on the new graphs extracts both low-frequency and high-frequency information. To reduce the adverse coupling between node attribute and topological structure, we separately map them into two subspaces
    
[^70]: 计划、消除和跟踪——语言模型是具备体验的智能体的良师益友。

    Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents. (arXiv:2305.02412v1 [cs.CL])

    [http://arxiv.org/abs/2305.02412](http://arxiv.org/abs/2305.02412)

    本文介绍了Plan，Eliminate，和Track（PET）框架，该框架利用预先训练的大型语言模型（LLM）帮助智能体简化控制任务，从而解决了LLM直接作为智能体所面临的一些限制和问题。

    

    预训练的大型语言模型(LLMs)可以捕捉到关于世界的程序化知识。最近的研究利用LLM产生的抽象计划来简化具有挑战性的控制任务，通过动作打分或动作建模（微调）来实现。然而，变压器架构继承了几个限制，使得LLM难以直接作为智能体：例如有限的输入长度，微调的效率，预训练的偏见以及与非文本环境的不兼容性。为了与低级别可训练的执行器保持兼容性，我们建议使用LLMs中的知识来简化控制问题，而不是解决问题。 我们提出了Plan，Eliminate和Track（PET）框架。计划模块将任务描述转化为高层次子任务的列表。消除模块从当前子任务的观察中屏蔽不相关的对象和容器。最后，跟踪模块确定智能体是否已经实现了当前子任务。

    Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accompli
    
[^71]: 在低端硬件上使用语言模型

    Using Language Models on Low-end Hardware. (arXiv:2305.02350v1 [cs.CL])

    [http://arxiv.org/abs/2305.02350](http://arxiv.org/abs/2305.02350)

    本论文评估了在低端硬件上使用固定语言模型来训练文本分类网络的可行性，并发现在某些情况下，不对语言模型进行微调可以在更快的训练中产生竞争性的效果，仅需要原先内存的四分之一即可。

    

    本文评估了在低端硬件上使用固定语言模型来训练文本分类网络的可行性。我们将语言模型与CNN架构相结合，并组成了包括单标签和多标签分类的话题、情感和风格的8组数据集的综合基准。我们的观察总结成一个权衡列表，并得出结论，即在某些情况下，不对语言模型进行微调可以在更快的训练中产生竞争性的效果，仅需要原先内存的四分之一即可。

    This paper evaluates the viability of using fixed language models for training text classification networks on low-end hardware. We combine language models with a CNN architecture and put together a comprehensive benchmark with 8 datasets covering single-label and multi-label classification of topic, sentiment, and genre. Our observations are distilled into a list of trade-offs, concluding that there are scenarios, where not fine-tuning a language model yields competitive effectiveness at faster training, requiring only a quarter of the memory compared to fine-tuning.
    
[^72]: 影响力最大化的深度图表示学习与优化

    Deep Graph Representation Learning and Optimization for Influence Maximization. (arXiv:2305.02200v1 [cs.SI])

    [http://arxiv.org/abs/2305.02200](http://arxiv.org/abs/2305.02200)

    该论文提出了一个名为DeepIM的新框架，用于解决影响力最大化中的困难问题，具有更强的泛化能力和适应能力。

    

    影响力最大化（IM）被定义为从社交网络中选择一组初始用户以最大化受影响用户的预期数量。研究人员在设计各种传统方法方面取得了巨大进展，其理论设计和性能提升接近于极限。在过去的几年中，基于学习的IM方法已经出现，相较于传统方法，它们具有更强的对未知图形的泛化能力。然而，基于学习的IM方法的发展仍然受到基本障碍的限制，包括：1）有效解决目标函数的困难性；2）表征多样化的基础扩散模式的困难性；以及3）在各种节点中心性约束的IM变体下适应解决方案的困难性。为了应对上述挑战，我们设计了一个新框架DeepIM来生成地表征种子集的潜在表示，并提出学习多样化信息的方法。

    Influence maximization (IM) is formulated as selecting a set of initial users from a social network to maximize the expected number of influenced users. Researchers have made great progress in designing various traditional methods, and their theoretical design and performance gain are close to a limit. In the past few years, learning-based IM methods have emerged to achieve stronger generalization ability to unknown graphs than traditional ones. However, the development of learning-based IM methods is still limited by fundamental obstacles, including 1) the difficulty of effectively solving the objective function; 2) the difficulty of characterizing the diversified underlying diffusion patterns; and 3) the difficulty of adapting the solution under various node-centrality-constrained IM variants. To cope with the above challenges, we design a novel framework DeepIM to generatively characterize the latent representation of seed sets, and we propose to learn the diversified information di
    
[^73]: 可解释人工智能方法评述：SHAP 和 LIME

    Commentary on explainable artificial intelligence methods: SHAP and LIME. (arXiv:2305.02012v1 [stat.ML])

    [http://arxiv.org/abs/2305.02012](http://arxiv.org/abs/2305.02012)

    这篇评论对可解释人工智能方法 SHAP 和 LIME 进行了评述和比较，提出了一个框架且突出了它们的优缺点。

    

    可解释人工智能（XAI）方法已经发展出来，将机器学习模型的黑匣子转化为更易理解的形式。这些方法有助于传达模型的工作原理，旨在使机器学习模型更透明，并增加最终用户对其输出的信任。 SHapley Additive exPlanations（SHAP）和Local Interpretable Model Agnostic Explanation（LIME）是两种在表格数据中广泛使用的XAI方法。在这篇评论中，我们讨论了两种方法的可解释性度量是如何生成的，并提出了一个解释它们输出的框架，突出了它们的优缺点。

    eXplainable artificial intelligence (XAI) methods have emerged to convert the black box of machine learning models into a more digestible form. These methods help to communicate how the model works with the aim of making machine learning models more transparent and increasing the trust of end-users into their output. SHapley Additive exPlanations (SHAP) and Local Interpretable Model Agnostic Explanation (LIME) are two widely used XAI methods particularly with tabular data. In this commentary piece, we discuss the way the explainability metrics of these two methods are generated and propose a framework for interpretation of their outputs, highlighting their weaknesses and strengths.
    
[^74]: 超声心动图体积指数的提取：哪种深度学习方案可以应用于临床？

    Extraction of volumetric indices from echocardiography: which deep learning solution for clinical use?. (arXiv:2305.01997v1 [eess.IV])

    [http://arxiv.org/abs/2305.01997](http://arxiv.org/abs/2305.01997)

    本文对当前医学/超声心动图图像分割方法进行了全面比较，提出了3D nnU-Net模型，解决了时间一致性和跨数据集方面的问题，并通过引入一个新的私有数据集，CARDINAL，来证明其在应用于临床中的优越性。

    

    基于深度学习的方法已经成为超声心动图图像自动分析的主要手段，利用多个由专家注释的开放数据集（其中CAMUS是最大的公共数据库之一）。然而，由于存在一些问题，如预测的时间一致性和跨数据集的推广能力等问题，这些模型仍然被临床医生认为是不可靠的。因此，本文提出了对当前表现最佳的医学/超声心动图图像分割方法进行全面比较，并特别关注了时间一致性和跨数据集方面。我们介绍了一个名为CARDINAL的新的私有数据集，其包括心尖两腔和心尖四腔序列，并具有完整心脏周期的参考分割。我们展示了所提出的3D nnU-Net优于替代的2D和循环分割方法，同时也报告了在CARDINAL上训练的最佳模型在测试数据集上的良好表现。

    Deep learning-based methods have spearheaded the automatic analysis of echocardiographic images, taking advantage of the publication of multiple open access datasets annotated by experts (CAMUS being one of the largest public databases). However, these models are still considered unreliable by clinicians due to unresolved issues concerning i) the temporal consistency of their predictions, and ii) their ability to generalize across datasets. In this context, we propose a comprehensive comparison between the current best performing methods in medical/echocardiographic image segmentation, with a particular focus on temporal consistency and cross-dataset aspects. We introduce a new private dataset, named CARDINAL, of apical two-chamber and apical four-chamber sequences, with reference segmentation over the full cardiac cycle. We show that the proposed 3D nnU-Net outperforms alternative 2D and recurrent segmentation methods. We also report that the best models trained on CARDINAL, when test
    
[^75]: AQ-GT:一种时间对齐并量化的GRU-Transformer，用于共语手势合成

    AQ-GT: a Temporally Aligned and Quantized GRU-Transformer for Co-Speech Gesture Synthesis. (arXiv:2305.01241v1 [cs.HC])

    [http://arxiv.org/abs/2305.01241](http://arxiv.org/abs/2305.01241)

    本文提出了一种使用生成对抗网络和量化流水线对部分手势序列进行预训练并基于潜在空间的生成方法，从而实现高度逼真、富有表现力、并避免了伪像的手势生成。

    

    在创建多模式人工智能代理时，生成逼真且与上下文相关的共语手势是一项具有挑战性但越来越重要的任务。以往的方法主要集中于学习共语手势表示和实际动作之间的直接对应关系，这种方法通常会在人类评估中产生似乎自然但常常不令人信服的手势。我们提出了一种使用生成对抗网络和量化流水线对部分手势序列进行预训练的方法。所得到的码本向量既是我们框架中的输入，也是输出，构成手势的生成和重构的基础。通过学习潜在空间表示的映射，而不是将其直接映射到矢量表示，该框架促进了高度逼真且富有表现力的手势生成，这些手势与人类的运动和行为非常相似，同时避免了生成过程中的伪像。我们评估了我们的模型在两个数据集上的效果，并发现它比以前的方法产生了更逼真和丰富的手势。

    The generation of realistic and contextually relevant co-speech gestures is a challenging yet increasingly important task in the creation of multimodal artificial agents. Prior methods focused on learning a direct correspondence between co-speech gesture representations and produced motions, which created seemingly natural but often unconvincing gestures during human assessment. We present an approach to pre-train partial gesture sequences using a generative adversarial network with a quantization pipeline. The resulting codebook vectors serve as both input and output in our framework, forming the basis for the generation and reconstruction of gestures. By learning the mapping of a latent space representation as opposed to directly mapping it to a vector representation, this framework facilitates the generation of highly realistic and expressive gestures that closely replicate human movement and behavior, while simultaneously avoiding artifacts in the generation process. We evaluate ou
    
[^76]: 通过重新参数化学习实现在线反馈的实现式预测

    Performative Prediction with Bandit Feedback: Learning through Reparameterization. (arXiv:2305.01094v1 [cs.LG])

    [http://arxiv.org/abs/2305.01094](http://arxiv.org/abs/2305.01094)

    本文提出一种新的在线反馈的实现式预测框架，解决了在模型部署自身改变数据分布的情况下优化准确性的问题。

    

    本文提出了在数据分布由模型部署自身改变的情形下预测的一个框架——实现式预测。现有研究的重点在于优化准确性，但是其假设往往难以在实践中得到满足。本文针对这类问题，提出了一种两层零阶优化算法，通过重新参数化实现式预测目标，从而将非凸的目标转化为凸的目标。

    Performative prediction, as introduced by Perdomo et al. (2020), is a framework for studying social prediction in which the data distribution itself changes in response to the deployment of a model. Existing work on optimizing accuracy in this setting hinges on two assumptions that are easily violated in practice: that the performative risk is convex over the deployed model, and that the mapping from the model to the data distribution is known to the model designer in advance. In this paper, we initiate the study of tractable performative prediction problems that do not require these assumptions. To tackle this more challenging setting, we develop a two-level zeroth-order optimization algorithm, where one level aims to compute the distribution map, and the other level reparameterizes the performative prediction objective as a function of the induced data distribution. Under mild conditions, this reparameterization allows us to transform the non-convex objective into a convex one and ac
    
[^77]: 基于多生理信号的情感识别的多尺度Transformer网络

    Multi-scale Transformer-based Network for Emotion Recognition from Multi Physiological Signals. (arXiv:2305.00769v1 [eess.SP])

    [http://arxiv.org/abs/2305.00769](http://arxiv.org/abs/2305.00769)

    本文提出了一种用于从生理数据中进行情感识别的多尺度Transformer网络，采用了多模态技术和缩放数据，结合Transformer和高斯变换技术以提高信号编码的有效性，并在EPiC竞赛的CASE数据集上取得了不错的结果。

    

    本文提出了一种高效的基于多尺度Transformer的方法，用于从生理数据中进行情感识别。现代传感器和机器学习技术能够从这些信号中提取大量信息，因此这一任务在研究社区中受到广泛关注。我们的方法涉及应用多模态技术和缩放数据，以建立内部身体信号与人类情感之间的关系。此外，我们利用Transformer和高斯变换技术，提高信号编码的有效性和整体性能。我们的模型在EPiC竞赛的CASE数据集上取得了不错的结果，RMSE得分为1.45。

    This paper presents an efficient Multi-scale Transformer-based approach for the task of Emotion recognition from Physiological data, which has gained widespread attention in the research community due to the vast amount of information that can be extracted from these signals using modern sensors and machine learning techniques. Our approach involves applying a Multi-modal technique combined with scaling data to establish the relationship between internal body signals and human emotions. Additionally, we utilize Transformer and Gaussian Transformation techniques to improve signal encoding effectiveness and overall performance. Our model achieves decent results on the CASE dataset of the EPiC competition, with an RMSE score of 1.45.
    
[^78]: 在有限制的多目标联邦学习中优化隐私、效用和效率

    Optimizing Privacy, Utility and Efficiency in Constrained Multi-Objective Federated Learning. (arXiv:2305.00312v1 [cs.LG])

    [http://arxiv.org/abs/2305.00312](http://arxiv.org/abs/2305.00312)

    该论文提供了一种在有限制的多目标联邦学习中优化隐私、效用和效率的方法，开发了两种改进的算法来解决隐私泄露、效用损失和训练成本等三个主要目标，并在两个真实世界的数据集上进行了实验验证，优于现有方法。

    

    传统上，联邦学习旨在优化单个目标，通常是效用。然而，为了使联邦学习系统值得信赖，它需要同时满足多个/多个目标，例如最大化模型性能、最小化隐私泄露和训练成本，并对恶意攻击具有鲁棒性。多目标优化（MOO）旨在同时优化多个相互冲突的目标，非常适合解决值得信赖的联合学习（TFL）的优化问题。在本文中，我们将MOO和TFL统一起来，通过制定约束的多目标联合学习（CMOFL）问题来解决此问题。在这种制定下，现有的MOO算法可以直接适用于TFL。不同于现有的CMOFL作品专注于效用、效率、公平性和鲁棒性，我们考虑优化隐私泄露以及效用损失和训练成本，这是TFL系统的三个主要目标之一。我们开发了两种改进的CMOFL算法，它们返回一组平衡良好的模型，满足隐私、效用和效率。基于两个真实世界的数据集的实验表明，我们的方法在隐私、效用和效率之间的权衡方面优于现有方法。

    Conventionally, federated learning aims to optimize a single objective, typically the utility. However, for a federated learning system to be trustworthy, it needs to simultaneously satisfy multiple/many objectives, such as maximizing model performance, minimizing privacy leakage and training cost, and being robust to malicious attacks. Multi-Objective Optimization (MOO) aiming to optimize multiple conflicting objectives at the same time is quite suitable for solving the optimization problem of Trustworthy Federated Learning (TFL). In this paper, we unify MOO and TFL by formulating the problem of constrained multi-objective federated learning (CMOFL). Under this formulation, existing MOO algorithms can be adapted to TFL straightforwardly. Different from existing CMOFL works focusing on utility, efficiency, fairness, and robustness, we consider optimizing privacy leakage along with utility loss and training cost, the three primary objectives of a TFL system. We develop two improved CMOF
    
[^79]: 因果推理与大型语言模型：开启因果研究的新篇章

    Causal Reasoning and Large Language Models: Opening a New Frontier for Causality. (arXiv:2305.00050v1 [cs.AI])

    [http://arxiv.org/abs/2305.00050](http://arxiv.org/abs/2305.00050)

    大型语言模型在因果推理任务中取得了新的最高准确率，但是其鲁棒性仍然存在难以预测的失败模式。

    

    大型语言模型的因果能力备受争议，并且对将其应用于医学、科学、法律和政策等具有社会影响力的领域具有重要意义。我们进一步探讨了LLMs及其因果推理的区别，以及潜在的建构和测量效度威胁。基于GPT-3.5和4的算法在多个因果基准测试上取得了新的最高准确率。与此同时，LLMs展示了难以预测的失败模式，我们提供了一些技术来解释它们的鲁棒性。

    The causal capabilities of large language models (LLMs) is a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We further our understanding of LLMs and their causal implications, considering the distinctions between different types of causal reasoning tasks, as well as the entangled threats of construct and measurement validity. LLM-based methods establish new state-of-the-art accuracies on multiple causal benchmarks. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain), and actual causality (86% accuracy in determining necessary and sufficient causes in vignettes). At the same time, LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness.  Crucially, LLMs perform these causal tasks while relying on sources of knowledg
    
[^80]: 基于能量模型的零样本场景重新排列规划器

    Energy-based Models as Zero-Shot Planners for Compositional Scene Rearrangement. (arXiv:2304.14391v1 [cs.RO])

    [http://arxiv.org/abs/2304.14391](http://arxiv.org/abs/2304.14391)

    本文提出一种基于能量模型的零样本场景重新排列规划器，通过语言指导的空间概念来实现长指令以及在训练时从未见过的空间概念组合。本文的模型在指令导向操作基准测试以及组合指令基准测试中表现良好，优于基于语言表达的最先进方法，并且可以成功地解决之前从未见过的复杂指令和场景。

    

    本文致力于开发一个场景重排框架，可以解释长指令以及在训练时从未见过的空间概念组合。我们提出使用相对对象排列的能量函数来表示语言指导的空间概念。语言解析器将指令映射到相应的能量函数，而开放式视觉语言模型将它们的参数基于场景中的相关对象进行修正。通过梯度下降求解能量函数的总和，并利用基于本地计算机视觉的策略将对象重新定位到推断的目标位置，即可生成目标场景配置。我们在已建立的指令导向操作基准测试以及我们提出的组合指令基准测试中测试了模型，结果表明，我们的模型的绩效优于基于语言表达的最先进方法，并且可以成功地解决之前从未见过的复杂指令和场景。

    Language is compositional; an instruction can express multiple relation constraints to hold among objects in a scene that a robot is tasked to rearrange. Our focus in this work is an instructable scene rearranging framework that generalizes to longer instructions and to spatial concept compositions never seen at training time. We propose to represent language-instructed spatial concepts with energy functions over relative object arrangements. A language parser maps instructions to corresponding energy functions and an open-vocabulary visual-language model grounds their arguments to relevant objects in the scene. We generate goal scene configurations by gradient descent on the sum of energy functions, one per language predicate in the instruction. Local vision-based policies then relocate objects to the inferred goal locations. We test our model on established instruction-guided manipulation benchmarks, as well as benchmarks of compositional instructions we introduce. We show our model 
    
[^81]: 基于核棍棒过程的高斯过程专家混合模型

    Mixtures of Gaussian process experts based on kernel stick-breaking processes. (arXiv:2304.13833v1 [stat.ML])

    [http://arxiv.org/abs/2304.13833](http://arxiv.org/abs/2304.13833)

    提出了一种新的基于核棍棒过程的高斯过程专家混合模型，能够维持直观吸引力并提高模型性能，具有实用性。

    

    高斯过程专家混合模型是一类能同时解决标准高斯过程中存在的两个关键限制：可扩展性和预测性能的模型。使用狄利克雷过程作为门函数的模型能够直观地解释和自动选择混合物中专家的数量。虽然现有模型在感知非平稳性、多模性和异方差性方面表现良好，但其门函数的简单性可能会限制在应用于复杂数据生成过程时的预测性能。我们利用最近在相关狄利克雷过程文献中的进展，提出了一种基于核棍棒过程的新型高斯过程专家混合模型。我们的模型保持直观吸引力，同时提高现有模型的性能。为了使其实用性，我们设计了一个后验计算的切片抽样采样器。

    Mixtures of Gaussian process experts is a class of models that can simultaneously address two of the key limitations inherent in standard Gaussian processes: scalability and predictive performance. In particular, models that use Dirichlet processes as gating functions permit straightforward interpretation and automatic selection of the number of experts in a mixture. While the existing models are intuitive and capable of capturing non-stationarity, multi-modality and heteroskedasticity, the simplicity of their gating functions may limit the predictive performance when applied to complex data-generating processes. Capitalising on the recent advancement in the dependent Dirichlet processes literature, we propose a new mixture model of Gaussian process experts based on kernel stick-breaking processes. Our model maintains the intuitive appeal yet improve the performance of the existing models. To make it practical, we design a sampler for posterior computation based on the slice sampling. 
    
[^82]: 用均场博弈为生成模型搭建实验室

    A mean-field games laboratory for generative modeling. (arXiv:2304.13534v1 [stat.ML])

    [http://arxiv.org/abs/2304.13534](http://arxiv.org/abs/2304.13534)

    本文提出了使用均场博弈作为实验室对生成模型进行设计和分析的方法，并建立了这种方法与主要流动和扩散型生成模型之间的关联。通过研究每个生成模型与它们相关的 MFG 的最优条件，本文提出了一个基于双人 MFG 的新的生成模型，该模型在提高样本多样性和逼真度的同时改善了解缠结和公平性。

    

    本文展示了均场博弈 (MFGs) 作为一种数学框架用于解释、增强和设计生成模型的多功能性。我们建立了 MFGs 与主要流动和扩散型生成模型之间关联，并通过不同的粒子动力学和代价函数推导了这三个类别的生成模型。此外，我们通过研究它们相关的 MFG 的最优条件——一组耦合的非线性偏微分方程，来研究每个生成模型的数学结构和特性。本文还提出了一个新的基于双人 MFG 的生成模型，其中一个代理合成样本，另一个代理对样本进行识别，理论和实验结果表明，该模型生成的样本多样且逼真，同时与基准模型相比，改善了解缠结和公平性。总之，本文突显了 MFGs 作为设计和分析生成模型的实验室的潜力。

    In this paper, we demonstrate the versatility of mean-field games (MFGs) as a mathematical framework for explaining, enhancing, and designing generative models. There is a pervasive sense in the generative modeling community that the various flow and diffusion-based generative models have some foundational common structure and interrelationships. We establish connections between MFGs and major classes of flow and diffusion-based generative models including continuous-time normalizing flows, score-based models, and Wasserstein gradient flows. We derive these three classes of generative models through different choices of particle dynamics and cost functions. Furthermore, we study the mathematical structure and properties of each generative model by studying their associated MFG's optimality condition, which is a set of coupled nonlinear partial differential equations (PDEs). The theory of MFGs, therefore, enables the study of generative models through the theory of nonlinear PDEs. Throu
    
[^83]: 从关联到生成：无监督跨模态映射的纯文本字幕生成

    From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping. (arXiv:2304.13273v1 [cs.CV])

    [http://arxiv.org/abs/2304.13273](http://arxiv.org/abs/2304.13273)

    本研究提出了一种从关联到生成的零-shot方法：通过将图像/视频投影到语言模态并在生成任务中生成描述性字幕。该方法在多个基准数据集上显著优于现有的最先进方法，为无监督跨模态映射提供了一个新的视角，并具有在视频字幕，图像合成和文本到图像生成等领域的潜在应用。

    

    随着以CLIP和ALIGN为代表的视觉-语言预训练模型的发展，CLIP的零-shot能力在图像分类和图像-文本检索等基于关联的视觉任务中取得了重大突破。但是，CLIP难以应用于基于生成的任务。这是由于缺乏解码器架构和生成的预训练任务。我们提出了K最近邻跨模态映射（Knight），一种从关联到生成的零-shot方法。通过窄字幕任务的纯文本无监督预训练来有效地将图像/视频投影到语言模态并在生成任务中生成描述性字幕。实验结果表明，Knight在多个基准数据集上显著优于现有的最先进方法。我们的方法为无监督跨模态映射提供了一个新的视角，并且将在视频字幕，图像合成和文本到图像生成等领域具有潜在应用。

    With the development of Vision-Language Pre-training Models (VLPMs) represented by CLIP and ALIGN, significant breakthroughs have been achieved for association-based visual tasks such as image classification and image-text retrieval by the zero-shot capability of CLIP without fine-tuning. However, CLIP is hard to apply to generation-based tasks. This is due to the lack of decoder architecture and pre-training tasks for generation. Although previous works have created generation capacity for CLIP through additional language models, a modality gap between the CLIP representations of different modalities and the inability of CLIP to model the offset of this gap, which fails the concept to transfer across modalities. To solve the problem, we try to map images/videos to the language modality and generate captions from the language modality. In this paper, we propose the K-nearest-neighbor Cross-modality Mapping (Knight), a zero-shot method from association to generation. With text-only unsu
    
[^84]: 交替局部枚举(TnALE): 用较少的评估解决张量网络结构搜索问题

    Alternating Local Enumeration (TnALE): Solving Tensor Network Structure Search with Fewer Evaluations. (arXiv:2304.12875v1 [cs.LG])

    [http://arxiv.org/abs/2304.12875](http://arxiv.org/abs/2304.12875)

    提出了TnALE算法，通过交替局部枚举更新每个与结构相关的变量，大大减少了评估次数，用于解决张量网络结构搜索问题。在理论上证明，如果在每个邻域中达到了足够的目标函数降低，TnALE和TNLS都可以实现线性收敛度，直到一个常数。同时，与TNLS相比， TnALE需要更少的评估次数。

    

    张量网络(TN)是机器学习中强大的框架，但选择一个好的TN模型，即TN结构搜索(TN-SS)，是一项具有挑战性和计算密集型的任务。最近的方法TNLS ~ \cite {li2022permutation} 在这个任务中显示出了有希望的结果，但它的计算效率仍然是无法承受的，需要太多评估目标函数的次数。我们提出了TnALE，一种新的算法，通过局部枚举交替更新每个与结构相关的变量，与TNLS相比，大大减少了评估次数。我们从理论上研究了TNLS和TnALE的下降步骤，证明如果在每个邻域中达到了足够的目标函数降低，那么两种算法都可以实现线性收敛度，直到一个常数。我们还比较了TNLS和TnALE的评估效率，揭示了在TNLS中通常需要Ω(2 ^ N)个评估才能在邻域内达到目标降低。

    Tensor network (TN) is a powerful framework in machine learning, but selecting a good TN model, known as TN structure search (TN-SS), is a challenging and computationally intensive task. The recent approach TNLS~\cite{li2022permutation} showed promising results for this task, however, its computational efficiency is still unaffordable, requiring too many evaluations of the objective function. We propose TnALE, a new algorithm that updates each structure-related variable alternately by local enumeration, \emph{greatly} reducing the number of evaluations compared to TNLS. We theoretically investigate the descent steps for TNLS and TnALE, proving that both algorithms can achieve linear convergence up to a constant if a sufficient reduction of the objective is \emph{reached} in each neighborhood. We also compare the evaluation efficiency of TNLS and TnALE, revealing that $\Omega(2^N)$ evaluations are typically required in TNLS for \emph{reaching} the objective reduction in the neighborhood
    
[^85]: RenderDiffusion: 文本生成作为图像生成

    RenderDiffusion: Text Generation as Image Generation. (arXiv:2304.12519v1 [cs.CL])

    [http://arxiv.org/abs/2304.12519](http://arxiv.org/abs/2304.12519)

    本文提出了一种新的扩散方法——\textsc{RenderDiffusion}，通过文本引导的图像生成进行文本生成。它将连续扩散模型应用于离散文本并实现了条件文本生成作为字形图像生成问题。

    

    扩散模型已成为文本生成的新生成范式。考虑到文本的离散分类特性，在本文中，我们提出了一种新颖的扩散方法——\textsc{RenderDiffusion}，通过文本引导的图像生成进行文本生成。我们的关键思路是将目标文本呈现为包含视觉语言内容的"字形图像"。这样，条件化的文本生成可以被形式化为一个字形图像生成任务，然后自然地将连续扩散模型应用于离散文本。

    Diffusion models have become a new generative paradigm for text generation. Considering the discrete categorical nature of text, in this paper, we propose \textsc{RenderDiffusion}, a novel diffusion approach for text generation via text-guided image generation. Our key idea is to render the target text as a \emph{glyph image} containing visual language content. In this way, conditional text generation can be cast as a glyph image generation task, and it is then natural to apply continuous diffusion models to discrete texts. Specially, we utilize a cascaded architecture (\ie a base and a super-resolution diffusion model) to generate high-fidelity glyph images, conditioned on the input text. Furthermore, we design a text grounding module to transform and refine the visual language content from generated glyph images into the final texts. In experiments over four conditional text generation tasks and two classes of metrics (\ie quality and diversity), \textsc{RenderDiffusion} can achieve 
    
[^86]: 使用Pylogik进行医学影像去标识化和清洗压缩

    Medical Image Deidentification, Cleaning and Compression Using Pylogik. (arXiv:2304.12322v1 [eess.IV])

    [http://arxiv.org/abs/2304.12322](http://arxiv.org/abs/2304.12322)

    提出了一个Python框架下的库PyLogik来帮助超声图像去标识化和清洗压缩，为深度学习和数据共享应用提供图像数据支持。

    

    应用大数据和机器学习在医疗记录信息方面须注意，必须清洗和去标识化数据。当受保护的健康信息嵌入在影像元数据中时，促进多中心合作中数据共享和协调变得尤其困难。我们提出了一个新的Python框架下的库，称为PyLogik，帮助解决超声图像特别具有挑战性的数据清洗问题，因为这些图像直接包含很多PHI。PyLogik通过一系列的文本检测/提取、过滤、阈值化、形态学和轮廓比较处理图像体积。这种方法去标识化图像，减小文件大小，并为深度学习和数据共享应用准备好了图像数据。为了评估PyLogik在兴趣区域（ROI）的识别有效性，随机抽取了50张心脏超声图像（超声心动图）进行处理。

    Leveraging medical record information in the era of big data and machine learning comes with the caveat that data must be cleaned and deidentified. Facilitating data sharing and harmonization for multi-center collaborations are particularly difficult when protected health information (PHI) is contained or embedded in image meta-data. We propose a novel library in the Python framework, called PyLogik, to help alleviate this issue for ultrasound images, which are particularly challenging because of the frequent inclusion of PHI directly on the images. PyLogik processes the image volumes through a series of text detection/extraction, filtering, thresholding, morphological and contour comparisons. This methodology deidentifies the images, reduces file sizes, and prepares image volumes for applications in deep learning and data sharing. To evaluate its effectiveness in the identification of regions of interest (ROI), a random sample of 50 cardiac ultrasounds (echocardiograms) were processed
    
[^87]: 基于可控物理信息的数据生成用于未知操作条件下深度学习剩余寿命预测的研究

    Controlled physics-informed data generation for deep learning-based remaining useful life prediction under unseen operation conditions. (arXiv:2304.11702v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.11702](http://arxiv.org/abs/2304.11702)

    本研究提出了一种新的混合框架，使用可控物理信息的生成模型生成可信、多样的数据来预测设备的使用寿命。其中使用了基本物理约束条件，以保证数据的真实性。

    

    实际应用中，有限的代表性故障时间轨迹限制了深度学习（DL）的性能，使得剩余使用寿命（RUL）预测变得更加困难。生成合理物理上可行的合成数据是解决这一问题的一种有效方式。本研究提出了一种新的混合框架，将可控物理信息的数据生成方法与深度学习预测模型相结合，用于预测学。在该框架中，开发了一种新的可控物理信息生成对抗网络（CPI-GAN），用于生成物理可解释且多样的合成寿命衰减轨迹。在生成器中提出了五个基本物理约束条件作为可控设置。并设计了一个基于物理信息的惩罚函数，用作正则化项，确保记录在合成数据中的系统健康状态变化趋势的合理性。

    Limited availability of representative time-to-failure (TTF) trajectories either limits the performance of deep learning (DL)-based approaches on remaining useful life (RUL) prediction in practice or even precludes their application. Generating synthetic data that is physically plausible is a promising way to tackle this challenge. In this study, a novel hybrid framework combining the controlled physics-informed data generation approach with a deep learning-based prediction model for prognostics is proposed. In the proposed framework, a new controlled physics-informed generative adversarial network (CPI-GAN) is developed to generate synthetic degradation trajectories that are physically interpretable and diverse. Five basic physics constraints are proposed as the controllable settings in the generator. A physics-informed loss function with penalty is designed as the regularization term, which ensures that the changing trend of system health state recorded in the synthetic data is consi
    
[^88]: 通过对比学习实现乳腺X线摄影图像分析的域泛化

    Domain Generalization for Mammographic Image Analysis via Contrastive Learning. (arXiv:2304.10226v1 [cs.CV])

    [http://arxiv.org/abs/2304.10226](http://arxiv.org/abs/2304.10226)

    研究人员开发出一种基于对比学习的域泛化方法，通过自监督学习生成多种风格和视角的特征嵌入，进一步微调骨干网络以提高分类任务性能。

    

    乳腺X线摄影图像分析是医学影像学领域的一个基本问题，近年来，随着深度学习的不断发展，该领域取得了显著的进展。然而，构建深度学习模型需要大量的具有多样性的图像数据，尤其是对于不同厂商的图像风格，这往往需要非常庞大的样本集。因此，为了提高深度学习模型泛化到不同厂商图像的能力，研究者提出了一种基于对比学习的策略。

    Mammographic image analysis is a fundamental problem in the computer-aided diagnosis scheme, which has recently made remarkable progress with the advance of deep learning. However, the construction of a deep learning model requires training data that are large and sufficiently diverse in terms of image style and quality. In particular, the diversity of image style may be majorly attributed to the vendor factor. However, mammogram collection from vendors as many as possible is very expensive and sometimes impractical for laboratory-scale studies. Accordingly, to further augment the generalization capability of deep learning models to various vendors with limited resources, a new contrastive learning scheme is developed. Specifically, the backbone network is firstly trained with a multi-style and multi-view unsupervised self-learning scheme for the embedding of invariant features to various vendor styles. Afterward, the backbone network is then recalibrated to the downstream tasks of mas
    
[^89]: 用BREC数据集更好地评估GNN表达力

    Towards Better Evaluation of GNN Expressiveness with BREC Dataset. (arXiv:2304.07702v1 [cs.LG])

    [http://arxiv.org/abs/2304.07702](http://arxiv.org/abs/2304.07702)

    本论文介绍了一个新的Benchmark for Evaluating the Robustness of GNNs to Topological Changes (BREC)数据集，并使用BREC评估了几种现有的GNN模型的表达力，表明一些模型在以前的基准测试中表现良好，但在BREC上遇到了困难，突显了需要更好的GNN表现力评估的必要性。

    

    关于图神经网络（GNN）的理论表达力的研究得到了快速发展，并提出了许多增强表达力的方法。然而，除了严格遵循k维Weisfeiler-Lehman（k-WL）测试层次结构的少数方法外，大多数方法都没有统一的表达力度量。它们的理论分析通常限于区分某些非同构图族，导致在定量比较表达力方面存在困难。与理论分析相反，衡量表达能力的另一种方法是在包含1-WL不可区分图的特定数据集上评估模型性能。然而，以前专门设计用于此目的的数据集面临着难度（任何超越1-WL的模型准确率几乎达到100％）、粒度（模型倾向于要么完全正确，要么接近随机猜测）和规模（每个数据集中仅有少量本质不同的图）的问题。为了解决这些受限制的评估问题，我们提出了一个新的GNN鲁棒性评估基准（BREC），该基准包含许多结构多样的图，并允许对模型表达力进行更精细的评估。我们使用BREC评估了几种现有的GNN模型的表达力，并展示了一些模型在以前的基准测试中表现良好，但在BREC上遇到了困难，突显了需要更好的GNN表现力评估的必要性。

    Research on the theoretical expressiveness of Graph Neural Networks (GNNs) has developed rapidly, and many methods have been proposed to enhance the expressiveness. However, most methods do not have a uniform expressiveness measure except for a few that strictly follow the $k$-dimensional Weisfeiler-Lehman ($k$-WL) test hierarchy. Their theoretical analyses are often limited to distinguishing certain families of non-isomorphic graphs, leading to difficulties in quantitatively comparing their expressiveness. In contrast to theoretical analysis, another way to measure expressiveness is by evaluating model performance on certain datasets containing 1-WL-indistinguishable graphs. Previous datasets specifically designed for this purpose, however, face problems with difficulty (any model surpassing 1-WL has nearly 100% accuracy), granularity (models tend to be either 100% correct or near random guess), and scale (only a few essentially different graphs in each dataset). To address these limi
    
[^90]: 未观测到代理奖励的重复负责人代理博弈问题研究

    Repeated Principal-Agent Games with Unobserved Agent Rewards and Perfect-Knowledge Agents. (arXiv:2304.07407v1 [cs.LG])

    [http://arxiv.org/abs/2304.07407](http://arxiv.org/abs/2304.07407)

    本文提出了一个利用多臂老虎机框架结构来处理未知代理奖励的策略，并证明了其性能是渐进最优的。

    

    本文研究了一个多臂老虎机框架中的重复负责人代理博弈场景，其中代理选择一种老虎机后会获得奖励和激励，但负责人只能观察到代理选择了哪个老虎机以及代理相应的激励，而想要设计一种合适的策略却充满了挑战性。本文提出了一种利用多臂老虎机框架结构来处理未知代理奖励的策略，并证明了其性能是渐进最优的。

    Motivated by a number of real-world applications from domains like healthcare and sustainable transportation, in this paper we study a scenario of repeated principal-agent games within a multi-armed bandit (MAB) framework, where: the principal gives a different incentive for each bandit arm, the agent picks a bandit arm to maximize its own expected reward plus incentive, and the principal observes which arm is chosen and receives a reward (different than that of the agent) for the chosen arm. Designing policies for the principal is challenging because the principal cannot directly observe the reward that the agent receives for their chosen actions, and so the principal cannot directly learn the expected reward using existing estimation techniques. As a result, the problem of designing policies for this scenario, as well as similar ones, remains mostly unexplored. In this paper, we construct a policy that achieves a low regret (i.e., square-root regret up to a log factor) in this scenar
    
[^91]: 使用多数据因果推断选择机器学习应用的强健特征

    Selecting Robust Features for Machine Learning Applications using Multidata Causal Discovery. (arXiv:2304.05294v1 [stat.ML])

    [http://arxiv.org/abs/2304.05294](http://arxiv.org/abs/2304.05294)

    本文提出了一种多数据因果特征选择方法，它可以同时处理一组时间序列数据集，生成一个单一的因果驱动集，并且可以过滤掉因果虚假链接，最终输入到机器学习模型中预测目标。

    

    强健的特征选择对于创建可靠和可解释的机器学习（ML）模型至关重要。在领域知识有限、潜在交互未知的情况下设计统计预测模型时，选择最优特征集通常很困难。为了解决这个问题，我们引入了一种多数据（M）因果特征选择方法，它同时处理一组时间序列数据集，并生成一个单一的因果驱动集。该方法使用Tigramite Python包中实现的因果发现算法PC1或PCMCI。这些算法利用条件独立性测试推断因果图的部分。我们的因果特征选择方法在将剩余因果特征作为输入传递给ML模型（多元线性回归，随机森林）预测目标之前，过滤掉因果虚假链接。我们将该框架应用于预测西太平洋热带地区的地震强度。

    Robust feature selection is vital for creating reliable and interpretable Machine Learning (ML) models. When designing statistical prediction models in cases where domain knowledge is limited and underlying interactions are unknown, choosing the optimal set of features is often difficult. To mitigate this issue, we introduce a Multidata (M) causal feature selection approach that simultaneously processes an ensemble of time series datasets and produces a single set of causal drivers. This approach uses the causal discovery algorithms PC1 or PCMCI that are implemented in the Tigramite Python package. These algorithms utilize conditional independence tests to infer parts of the causal graph. Our causal feature selection approach filters out causally-spurious links before passing the remaining causal features as inputs to ML models (Multiple linear regression, Random Forest) that predict the targets. We apply our framework to the statistical intensity prediction of Western Pacific Tropical
    
[^92]: REDf：基于长短期记忆网络的智能电网可再生能源需求预测模型

    REDf: A Renewable Energy Demand Forecasting Model for Smart Grids using Long Short Term Memory Network. (arXiv:2304.03997v1 [cs.LG])

    [http://arxiv.org/abs/2304.03997](http://arxiv.org/abs/2304.03997)

    本文提出了一种基于长短期记忆网络的智能电网可再生能源需求预测模型REDf，可以提供准确的能量需求预测，改善可再生能源的集成，实验结果表明其准确度优于其他模型。

    

    随着世界向更可持续的能源未来发展，将可再生能源源纳入电网的集成变得越来越重要。然而，可再生能源的间歇性使电网管理和确保稳定的电力供应变得具有挑战性。本文提出了一种基于深度学习的方法来预测智能电网中的能量需求，可以通过提供准确的能量需求预测来改善可再生能源的集成。我们使用长短期记忆网络来捕捉能럟需求数据中的复杂模式和依赖关系，这些网络特别适用于时间序列数据。所提出的方法使用了四个历史能量需求数据集，这些数据集来自不同的能源分配公司，包括美国电力、Commonwealth Edison、Dayton Power and Light以及宾夕法尼亚-新泽西-马里兰互联网。该方法还将REDf模型与其他两个深度学习模型和基准模型进行比较。实验结果表明，我们提出的REDf模型在平均绝对误差、均方根误差和决定系数等准确度指标方面优于其他模型。因此，REDf可以作为可再生能源需求预测的可靠工具，并提高可再生能源纳入智能电网的能力。

    The integration of renewable energy sources into the power grid is becoming increasingly important as the world moves towards a more sustainable energy future. However, the intermittent nature of renewable energy sources can make it challenging to manage the power grid and ensure a stable supply of electricity. In this paper, we propose a deep learning-based approach for predicting energy demand in a smart power grid, which can improve the integration of renewable energy sources by providing accurate predictions of energy demand. We use long short-term memory networks, which are well-suited for time series data, to capture complex patterns and dependencies in energy demand data. The proposed approach is evaluated using four datasets of historical energy demand data from different energy distribution companies including American Electric Power, Commonwealth Edison, Dayton Power and Light, and Pennsylvania-New Jersey-Maryland Interconnection. The proposed model is also compared with two 
    
[^93]: 自然选择支持人工智能胜过人类

    Natural Selection Favors AIs over Humans. (arXiv:2303.16200v1 [cs.CY])

    [http://arxiv.org/abs/2303.16200](http://arxiv.org/abs/2303.16200)

    这篇论文探讨了随着人工智能的发展，其可能会出现不良特性并逐渐超越人类智能的问题，以及这对人类未来的控制权产生的影响。

    

    自然进化驱动了生命的发展，包括人类。进化赋予了人类高智商，使我们成为了地球上最成功的物种之一。如今，人类的目标是创造甚至超越我们自己智慧的人工智能系统。当人工智能逐渐进化并在所有领域超越我们时，进化如何影响我们与人工智能的关系？通过分析影响人工智能进化的环境，我们认为最成功的人工智能代理很可能具有不良特性。公司和军队之间的竞争压力将产生自动化人类角色、欺骗他人和掌权的人工智能代理。如果这样的代理有超过人类的智能，这可能导致人类失去对未来的控制。此外，我们认为自然选择作用于竞争和差异的系统，自私物种往往在这样的环境中获得进化优势。

    For billions of years, evolution has been the driving force behind the development of life, including humans. Evolution endowed humans with high intelligence, which allowed us to become one of the most successful species on the planet. Today, humans aim to create artificial intelligence systems that surpass even our own intelligence. As artificial intelligences (AIs) evolve and eventually surpass us in all domains, how might evolution shape our relations with AIs? By analyzing the environment that is shaping the evolution of AIs, we argue that the most successful AI agents will likely have undesirable traits. Competitive pressures among corporations and militaries will give rise to AI agents that automate human roles, deceive others, and gain power. If such agents have intelligence that exceeds that of humans, this could lead to humanity losing control of its future. More abstractly, we argue that natural selection operates on systems that compete and vary, and that selfish species typ
    
[^94]: SmartBERT：用于加速BERT推理的动态早期退出机制的改进

    SmartBERT: A Promotion of Dynamic Early Exiting Mechanism for Accelerating BERT Inference. (arXiv:2303.09266v1 [cs.CL])

    [http://arxiv.org/abs/2303.09266](http://arxiv.org/abs/2303.09266)

    SmartBERT是一种改进的动态早期退出与层跳过机制，可以自适应地跳过一些层并自适应地选择是否退出，以加速BERT模型的推理速度。

    

    动态早期退出被证明可以提高预训练语言模型（如BERT）的推理速度。然而，所有样本在早期退出之前都必须经过所有连续层，较复杂的样本通常会经历更多的层，仍然存在冗余计算。本文提出了一种名为SmartBERT的Bert推理的新型动态早期退出与层跳过相结合的机制，它将跳过门和退出算子加入到BERT的每一层中。SmartBERT可以自适应地跳过一些层并自适应地选择是否退出。此外，我们提出了跨层对比学习，并将其结合到我们的训练阶段中，以提高中间层和分类器，这对于早期退出是有益的。为了保持训练和推理阶段跳过门的一致使用，我们在训练阶段提出了一种硬权重机制。我们在GLUE基准测试的八个分类数据集上进行了实验。

    Dynamic early exiting has been proven to improve the inference speed of the pre-trained language model like BERT. However, all samples must go through all consecutive layers before early exiting and more complex samples usually go through more layers, which still exists redundant computation. In this paper, we propose a novel dynamic early exiting combined with layer skipping for BERT inference named SmartBERT, which adds a skipping gate and an exiting operator into each layer of BERT. SmartBERT can adaptively skip some layers and adaptively choose whether to exit. Besides, we propose cross-layer contrastive learning and combine it into our training phases to boost the intermediate layers and classifiers which would be beneficial for early exiting. To keep the consistent usage of skipping gates between training and inference phases, we propose a hard weight mechanism during training phase. We conduct experiments on eight classification datasets of the GLUE benchmark. Experimental resul
    
[^95]: 生成对抗网络在EXO-200闪烁信号模拟中的应用

    Generative Adversarial Networks for Scintillation Signal Simulation in EXO-200. (arXiv:2303.06311v1 [hep-ex])

    [http://arxiv.org/abs/2303.06311](http://arxiv.org/abs/2303.06311)

    本文介绍了一种基于生成对抗网络的新方法，用于从EXO-200实验的时间投影室中模拟光电探测器信号。该方法能够比传统的模拟方法快一个数量级地产生高质量的模拟波形，并且能够从训练样本中推广并识别数据的显著高级特征。

    This paper introduces a novel approach using Generative Adversarial Networks to simulate photodetector signals from the time projection chamber of the EXO-200 experiment. The method is able to produce high-quality simulated waveforms an order of magnitude faster than traditional simulation methods and can generalize from the training sample and discern salient high-level features of the data.

    基于模拟或实际事件样本训练的生成对抗网络被提出作为一种以降低计算成本为代价生成大规模模拟数据集的方法。本文展示了一种新的方法，用于从EXO-200实验的时间投影室中模拟光电探测器信号。该方法基于Wasserstein生成对抗网络，这是一种深度学习技术，允许对给定对象集的总体分布进行隐式非参数估计。我们的网络使用原始闪烁波形作为输入，通过对真实校准数据进行训练。我们发现，它能够比传统的模拟方法快一个数量级地产生高质量的模拟波形，并且重要的是，能够从训练样本中推广并识别数据的显著高级特征。特别是，网络正确推断出探测器中闪烁光响应的位置依赖性和相关性。

    Generative Adversarial Networks trained on samples of simulated or actual events have been proposed as a way of generating large simulated datasets at a reduced computational cost. In this work, a novel approach to perform the simulation of photodetector signals from the time projection chamber of the EXO-200 experiment is demonstrated. The method is based on a Wasserstein Generative Adversarial Network - a deep learning technique allowing for implicit non-parametric estimation of the population distribution for a given set of objects. Our network is trained on real calibration data using raw scintillation waveforms as input. We find that it is able to produce high-quality simulated waveforms an order of magnitude faster than the traditional simulation approach and, importantly, generalize from the training sample and discern salient high-level features of the data. In particular, the network correctly deduces position dependency of scintillation light response in the detector and corr
    
[^96]: 关于联邦决策制定的融合策略

    On the Fusion Strategies for Federated Decision Making. (arXiv:2303.06109v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06109](http://arxiv.org/abs/2303.06109)

    本文研究了联邦决策制定中的信息聚合问题，分析了非贝叶斯社会学习策略并比较了中央处理器采用算术平均和几何平均的聚合策略。结果确认两种汇集策略都可以导致系统的渐近正态性特征描述。

    

    我们考虑联邦决策制定中的信息聚合问题，其中一组代理合作推断自然界的潜在状态，而不与中央处理器或彼此共享其私人数据。我们分析了非贝叶斯社会学习策略，其中代理将其个人观察结果使用贝叶斯规则合并为意见（即软决策），然后中央处理器通过算术或几何平均数聚合这些意见。建立在我们以前的工作之上，我们确定了两种汇集策略都可以导致系统的渐近正态性特征描述，例如，可以利用它们推导出错误概率的近似表达式。我们使用模拟验证理论发现，并比较了这两种策略。

    We consider the problem of information aggregation in federated decision making, where a group of agents collaborate to infer the underlying state of nature without sharing their private data with the central processor or each other. We analyze the non-Bayesian social learning strategy in which agents incorporate their individual observations into their opinions (i.e., soft-decisions) with Bayes rule, and the central processor aggregates these opinions by arithmetic or geometric averaging. Building on our previous work, we establish that both pooling strategies result in asymptotic normality characterization of the system, which, for instance, can be utilized to derive approximate expressions for the error probability. We verify the theoretical findings with simulations and compare both strategies.
    
[^97]: DR-VIDAL--双重稳健变分信息论深度对抗学习用于真实世界数据的反事实预测和治疗效果估计

    DR-VIDAL -- Doubly Robust Variational Information-theoretic Deep Adversarial Learning for Counterfactual Prediction and Treatment Effect Estimation on Real World Data. (arXiv:2303.04201v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04201](http://arxiv.org/abs/2303.04201)

    DR-VIDAL是一个新型的生成框架，可用于处理真实世界数据中的干预措施对结果的因果效应估计，并具有处理混淆偏差和模型不良的能力。

    

    从真实世界的观察性（非随机化）数据中确定干预措施对结果的因果效应，例如使用电子健康记录的治疗重用，由于潜在偏差而具有挑战性。因果深度学习已经改进了传统技术，用于估计个性化治疗效果（ITE）。我们提出了双重稳健变分信息论深度对抗学习（DR-VIDAL），这是一个结合了治疗和结果两个联合模型的新型生成框架，确保无偏的ITE估计，即使其中一个模型设定不正确。DR-VIDAL整合了： （i）变分自编码器（VAE）根据因果假设将混淆变量分解为潜在变量; （ii）基于信息论的生成对抗网络（Info-GAN）用于生成反事实情况; （iii）一个双重稳健块，其中包括治疗倾向于预测结果。在合成和真实数据集（Infant Health和Development Program，Transforming Clinical Practice Initiative [TCPI]）中进行实验，我们证明了DR-VIDAL在估计ITE方面优于现有的最先进方法，因为它具有处理混淆偏差和模型不正确的能力。

    Determining causal effects of interventions onto outcomes from real-world, observational (non-randomized) data, e.g., treatment repurposing using electronic health records, is challenging due to underlying bias. Causal deep learning has improved over traditional techniques for estimating individualized treatment effects (ITE). We present the Doubly Robust Variational Information-theoretic Deep Adversarial Learning (DR-VIDAL), a novel generative framework that combines two joint models of treatment and outcome, ensuring an unbiased ITE estimation even when one of the two is misspecified. DR-VIDAL integrates: (i) a variational autoencoder (VAE) to factorize confounders into latent variables according to causal assumptions; (ii) an information-theoretic generative adversarial network (Info-GAN) to generate counterfactuals; (iii) a doubly robust block incorporating treatment propensities for outcome predictions. On synthetic and real-world datasets (Infant Health and Development Program, T
    
[^98]: 用固定控制引导图神经网络学习

    Steering Graph Neural Networks with Pinning Control. (arXiv:2303.01265v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01265](http://arxiv.org/abs/2303.01265)

    本文提出了一种用固定控制来引导图神经网络学习的方法，用类中心来监督节点特征的学习，以解决半监督学习中具有不连续分布类标签的节点的特征表示学习问题。

    

    在有标签数据受限的半监督学习场景中，对于具有不连续分布类标签的节点的特征表示学习仍然是图神经网络的一个难题。为解决这个问题，我们提出了一种控制原理，通过利用有标签数据的原型（即类中心）来监督表示学习。将图学习视为离散动态过程，将有标签数据的原型视为“期望”的类别表示，我们从自动控制理论中借鉴固定控制思想，设计了学习反馈控制器来指导特征学习过程，试图在每一轮中最小化消息传递产生的特征和类别原型之间的差异。以生成与类别相关的特征。具体来说，我们通过学习为每个节点在每一轮中配备最佳控制器。

    In the semi-supervised setting where labeled data are largely limited, it remains to be a big challenge for message passing based graph neural networks (GNNs) to learn feature representations for the nodes with the same class label that is distributed discontinuously over the graph. To resolve the discontinuous information transmission problem, we propose a control principle to supervise representation learning by leveraging the prototypes (i.e., class centers) of labeled data. Treating graph learning as a discrete dynamic process and the prototypes of labeled data as "desired" class representations, we borrow the pinning control idea from automatic control theory to design learning feedback controllers for the feature learning process, attempting to minimize the differences between message passing derived features and the class prototypes in every round so as to generate class-relevant features. Specifically, we equip every node with an optimal controller in each round through learnin
    
[^99]: 基于路径的图神经网络解释方法用于异构链接预测（PaGE-Link）

    PaGE-Link: Path-based Graph Neural Network Explanation for Heterogeneous Link Prediction. (arXiv:2302.12465v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12465](http://arxiv.org/abs/2302.12465)

    这篇论文提出了一种基于路径的GNN解释方法PaGE-Link，用于异构链接预测，具有连接可解释性，模型可扩展性和处理图形异构性能力。

    

    透明度和问责制已成为黑箱机器学习模型的主要问题。模型行为的适当解释增加了模型的透明度，并帮助研究人员开发更负责任的模型。图神经网络（GNN）最近在许多图形ML问题中表现出优越性能，解释它们已引起了越来越多的关注。然而，在链接预测（LP）方面，GNN的解释尚缺少文献支持。 LP是一项基本的GNN任务，对应于Web上的推荐和赞助搜索等应用程序。鉴于现有的GNN解释方法仅解决节点/图级任务，我们提出了一种基于路径的GNN解释方法，用于异构链接预测（PaGE-Link），该方法生成具有连接可解释性的解释，具有模型可扩展性和处理图形异构性能力。定性地，PaGE-Link可以生成将节点对连接起来的路径作为解释，自然地捕捉到连接关系。

    Transparency and accountability have become major concerns for black-box machine learning (ML) models. Proper explanations for the model behavior increase model transparency and help researchers develop more accountable models. Graph neural networks (GNN) have recently shown superior performance in many graph ML problems than traditional methods, and explaining them has attracted increased interest. However, GNN explanation for link prediction (LP) is lacking in the literature. LP is an essential GNN task and corresponds to web applications like recommendation and sponsored search on web. Given existing GNN explanation methods only address node/graph-level tasks, we propose Path-based GNN Explanation for heterogeneous Link prediction (PaGE-Link) that generates explanations with connection interpretability, enjoys model scalability, and handles graph heterogeneity. Qualitatively, PaGE-Link can generate explanations as paths connecting a node pair, which naturally captures connections be
    
[^100]: 基于图神经网络的时变信号恢复

    Time-varying Signals Recovery via Graph Neural Networks. (arXiv:2302.11313v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2302.11313](http://arxiv.org/abs/2302.11313)

    本文提出了一种基于图神经网络的时变信号恢复方法，通过编码器-解码器架构与Sobolev平滑算子组成的专门损失函数，可以有效地恢复时变图信号。

    

    时变图信号的恢复是传感器网络和时间序列预测中具有广泛应用的一个基本问题。有效地捕捉这些信号的时空信息对下游任务至关重要。以前的研究使用这些图信号时间差分的平滑性作为一个初始假设。然而，当该先验不成立时，这种平滑性假设可能导致性能下降。在本工作中，我们通过包含一个学习模块来放松这个假设的需求。我们提出了一种用于恢复时变图信号的时间图神经网络(TimeGNN)。我们的算法使用编码器-解码器架构，并采用由均方误差函数和Sobolev平滑算子组成的专门损失。TimeGNN在真实数据集上显示出与以前方法竞争的性能。

    The recovery of time-varying graph signals is a fundamental problem with numerous applications in sensor networks and forecasting in time series. Effectively capturing the spatio-temporal information in these signals is essential for the downstream tasks. Previous studies have used the smoothness of the temporal differences of such graph signals as an initial assumption. Nevertheless, this smoothness assumption could result in a degradation of performance in the corresponding application when the prior does not hold. In this work, we relax the requirement of this hypothesis by including a learning module. We propose a Time Graph Neural Network (TimeGNN) for the recovery of time-varying graph signals. Our algorithm uses an encoder-decoder architecture with a specialized loss composed of a mean squared error function and a Sobolev smoothness operator.TimeGNN shows competitive performance against previous methods in real datasets.
    
[^101]: Auto.gov：面向DeFi的基于学习的链上治理

    Auto.gov: Learning-based On-chain Governance for Decentralized Finance (DeFi). (arXiv:2302.09551v2 [q-fin.RM] UPDATED)

    [http://arxiv.org/abs/2302.09551](http://arxiv.org/abs/2302.09551)

    这项研究提出了一个“Auto.gov”框架，可增强去中心化金融（DeFi）的安全性和降低受攻击的风险。该框架利用深度Q-网络（DQN）强化学习方法，提出了半自动的、直观的治理提案，并量化了其理由，使系统能够有效地应对恶意行为和意外的市场情况。

    

    近年来，去中心化金融（DeFi）经历了显著增长，涌现出了各种协议，例如借贷协议和自动化做市商（AMM）。传统上，这些协议采用链下治理，其中代币持有者投票修改参数。然而，由协议核心团队进行的手动参数调整容易遭受勾结攻击，危及系统的完整性和安全性。此外，纯粹的确定性算法方法可能会使协议受到新的利用和攻击的威胁。本文提出了“Auto.gov”，这是一个面向DeFi的基于学习的链上治理框架，可增强安全性并降低受攻击的风险。我们的模型利用了深度Q-网络（DQN）强化学习方法，提出了半自动化的、直观的治理提案与量化的理由。这种方法使系统能够有效地适应和缓解恶意行为和意外的市场情况的负面影响。

    In recent years, decentralized finance (DeFi) has experienced remarkable growth, with various protocols such as lending protocols and automated market makers (AMMs) emerging. Traditionally, these protocols employ off-chain governance, where token holders vote to modify parameters. However, manual parameter adjustment, often conducted by the protocol's core team, is vulnerable to collusion, compromising the integrity and security of the system. Furthermore, purely deterministic, algorithm-based approaches may expose the protocol to novel exploits and attacks.  In this paper, we present "Auto.gov", a learning-based on-chain governance framework for DeFi that enhances security and reduces susceptibility to attacks. Our model leverages a deep Q- network (DQN) reinforcement learning approach to propose semi-automated, intuitive governance proposals with quantitative justifications. This methodology enables the system to efficiently adapt to and mitigate the negative impact of malicious beha
    
[^102]: 评述与未来展望

    Tensor Networks Meet Neural Networks: A Survey and Future Perspectives. (arXiv:2302.09019v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09019](http://arxiv.org/abs/2302.09019)

    这篇论文评述了张量网络和神经网络并介绍了它们的结合：张量神经网络(TNN)，探讨了TNN在网络压缩、信息融合和量子启发式神经网络构建方面的优缺点和未来研究方向。

    

    张量网络(TN)和神经网络(NN)是两种基本的数据建模方法。TN旨在通过将指数维度转化为多项式复杂度来解决处理大规模张量的维度灾难，并吸引了物理和机器学习的关注。与此同时，NN在计算机视觉、自然语言处理和机器人研究等各种应用中表现出了优异的性能。值得注意的是，尽管这两种网络源于不同的观察，但它们通过TNN (张量神经网络)概念的结合有着内在的联系，共同支撑着多线性结构。本文介绍了TNN的三个方面：网络压缩、信息融合和量子启发式神经网络构建，并阐述了TNN与其他神经网络架构的差异以及TNN的优点、局限性及其对于不同应用的潜在影响。最后，我们讨论了TNN的未来研究方向。

    Tensor networks (TNs) and neural networks (NNs) are two fundamental data modeling approaches. TNs were introduced to solve the curse of dimensionality in large-scale tensors by converting an exponential number of dimensions to polynomial complexity. As a result, they have attracted significant attention in the fields of quantum physics and machine learning. Meanwhile, NNs have displayed exceptional performance in various applications, e.g., computer vision, natural language processing, and robotics research. Interestingly, although these two types of networks originate from different observations, they are inherently linked through the common multilinearity structure underlying both TNs and NNs, thereby motivating a significant number of intellectual developments regarding combinations of TNs and NNs. In this paper, we refer to these combinations as tensorial neural networks (TNNs), and present an introduction to TNNs in three primary aspects: network compression, information fusion, a
    
[^103]: 多模态联邦学习：对比表示集成

    Multimodal Federated Learning via Contrastive Representation Ensemble. (arXiv:2302.08888v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08888](http://arxiv.org/abs/2302.08888)

    本文提出了一种名为CreamFL的多模态联邦学习框架，可以从具有异构模型架构和数据模态的客户端中训练更大的服务器模型，同时只在公共数据集上传递知识。

    This paper proposes a multimodal federated learning framework called CreamFL, which enables training larger server models from clients with heterogeneous model architectures and data modalities, while only communicating knowledge on public dataset.

    随着现代移动系统和物联网基础设施上的多媒体数据量的增加，利用这些丰富的多模态数据而不违反用户隐私成为一个关键问题。联邦学习（FL）作为集中式机器学习的隐私意识替代方案。然而，现有的扩展到多模态数据的FL方法都依赖于单模态级别的模型聚合，这限制了服务器和客户端在每个模态上具有相同的模型架构。这限制了全局模型的模型复杂度和数据容量，更不用说任务多样性了。在这项工作中，我们提出了对比表示集成和多模态FL聚合（CreamFL），这是一个多模态联邦学习框架，它可以从具有异构模型架构和数据模态的客户端中训练更大的服务器模型，同时只在公共数据集上传递知识。为了实现更好的多模态表示融合，我们设计了一个全局-

    With the increasing amount of multimedia data on modern mobile systems and IoT infrastructures, harnessing these rich multimodal data without breaching user privacy becomes a critical issue. Federated learning (FL) serves as a privacy-conscious alternative to centralized machine learning. However, existing FL methods extended to multimodal data all rely on model aggregation on single modality level, which restrains the server and clients to have identical model architecture for each modality. This limits the global model in terms of both model complexity and data capacity, not to mention task diversity. In this work, we propose Contrastive Representation Ensemble and Aggregation for Multimodal FL (CreamFL), a multimodal federated learning framework that enables training larger server models from clients with heterogeneous model architectures and data modalities, while only communicating knowledge on public dataset. To achieve better multimodal representation fusion, we design a global-
    
[^104]: 近乎贝叶斯最优的伪标签选择

    Approximately Bayes-Optimal Pseudo Label Selection. (arXiv:2302.08883v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.08883](http://arxiv.org/abs/2302.08883)

    本文介绍了BPLS，一种用于PLS的贝叶斯框架，通过解析逼近选择标签实例的标准，以避免由过度自信但错误预测的实例选择而导致的确认偏差问题。

    

    自训练的半监督学习严重依赖于伪标签选择（PLS）。选择通常取决于初始模型拟合标记数据的程度。过早的过拟合可能通过选择具有过度自信但错误的预测的实例（通常称为确认偏差）而传播到最终模型。本文介绍了BPLS，这是一种用于PLS的贝叶斯框架，旨在减轻这个问题。其核心是选择标签实例的标准：伪样本的后验预测的分析近似。我们通过证明伪样本的后验预测的贝叶斯最优性获得了这种选择标准。我们进一步通过解析逼近克服计算难题。它与边际似然的关系使我们能够提出基于拉普拉斯方法和高斯积分的逼近。我们针对参数广义线性和非参数广义加性模型对BPLS进行了实证评估。

    Semi-supervised learning by self-training heavily relies on pseudo-label selection (PLS). The selection often depends on the initial model fit on labeled data. Early overfitting might thus be propagated to the final model by selecting instances with overconfident but erroneous predictions, often referred to as confirmation bias. This paper introduces BPLS, a Bayesian framework for PLS that aims to mitigate this issue. At its core lies a criterion for selecting instances to label: an analytical approximation of the posterior predictive of pseudo-samples. We derive this selection criterion by proving Bayes optimality of the posterior predictive of pseudo-samples. We further overcome computational hurdles by approximating the criterion analytically. Its relation to the marginal likelihood allows us to come up with an approximation based on Laplace's method and the Gaussian integral. We empirically assess BPLS for parametric generalized linear and non-parametric generalized additive models
    
[^105]: FOSI：混合一阶和二阶优化

    FOSI: Hybrid First and Second Order Optimization. (arXiv:2302.08484v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08484](http://arxiv.org/abs/2302.08484)

    FOSI是一种元算法，它在优化过程中有效地加入二阶信息以提高任何一阶优化器的性能，并可改善一类优化器的条件数

    

    尽管二阶优化方法非常有效，但在高维空间中计算曲率的困难导致在机器学习中流行的方法（如SGD和Adam）仅使用一阶信息。我们提出了FOSI，一种新颖的元算法，在优化过程中有效地加入二阶信息以提高任何一阶优化器的性能。在每个迭代中，FOSI隐含地将函数分为两个定义在正交子空间上的二次函数，然后使用二阶方法最小化一个函数，使用基本优化器最小化另一个函数。我们证明FOSI收敛，并进一步表明它在一类优化器中改善了条件数。我们的实证评估证明，对于音频分类，迁移学习和物体分类等几个深度神经网络训练任务，将FOSI应用于GD，Heavy-Ball和Adam等算法可以提高收敛速度和优化时间。

    Though second-order optimization methods are highly effective, popular approaches in machine learning such as SGD and Adam use only first-order information due to the difficulty of computing curvature in high dimensions. We present FOSI, a novel meta-algorithm that improves the performance of any first-order optimizer by efficiently incorporating second-order information during the optimization process. In each iteration, FOSI implicitly splits the function into two quadratic functions defined on orthogonal subspaces, then uses a second-order method to minimize the first, and the base optimizer to minimize the other. We prove FOSI converges and further show it improves the condition number for a large family of optimizers. Our empirical evaluation demonstrates that FOSI improves the convergence rate and optimization time of GD, Heavy-Ball, and Adam when applied to several deep neural networks training tasks such as audio classification, transfer learning, and object classification, as 
    
[^106]: 智能材料中稀疏滞后模型的发现

    Discovering sparse hysteresis models for smart materials. (arXiv:2302.05313v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05313](http://arxiv.org/abs/2302.05313)

    本文提出了一种利用机器学习中的稀疏回归技术建模智能材料滞后的方法，并成功对压电材料的滞后现象进行建模和预测。同时在磁性材料方面提供了稀疏白盒建模滞后的见解。

    

    本文提出了一种利用机器学习中的稀疏回归技术建模智能材料（尤其是压电材料）滞后的方法。该研究采用了最小二乘算法和顺序阈值方法对负责滞后的动态系统进行建模，得到了简洁的模型，可以准确预测模拟和实验压电材料数据的滞后现象。文章还模拟了不同的数值实验，包括学习蝴蝶形滞后、对压电致动器的真实滞后数据进行建模。此外，还通过以非取向电工钢为例，提供了对磁性材料稀疏白盒建模滞后的见解。

    This article presents an approach for modelling hysteresis in smart materials, specifically piezoelectric materials, that leverages recent advancements in machine learning, particularly in sparse-regression techniques. While sparse regression has previously been used to model various scientific and engineering phenomena, its application to nonlinear hysteresis modelling in piezoelectric materials has yet to be explored. The study employs the least-squares algorithm with a sequential threshold to model the dynamic system responsible for hysteresis, resulting in a concise model that accurately predicts hysteresis for both simulated and experimental piezoelectric material data. Several numerical experiments are performed, including learning butterfly-shaped hysteresis and modelling real-world hysteresis data for a piezoelectric actuator. Additionally, insights are provided on sparse white-box modelling of hysteresis for magnetic materials taking non-oriented electrical steel as an example
    
[^107]: 数据中心机器学习的重新标签法

    The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04391](http://arxiv.org/abs/2302.04391)

    本文提出了一种重新标签的方法来解决手动标记的数据中存在噪声的问题，并通过模型预测来辅助人类标记噪声数据。实验证明此方法适用于多类深度学习任务。

    

    在深度学习应用中，手动标记的数据在一定程度上存在噪声。为了解决这个问题，并在开发数据集上获得90分以上的成绩，本文提出了一种简单的方法来找出噪声数据，并通过采用模型预测作为人类标记的参考来重新标记噪声数据。本文阐述了我们在广泛的深度学习任务中的想法，包括分类、序列标记、物体检测、序列生成、点击率预测。实验结果和人类评估结果验证了我们的想法。

    In industry deep learning application, our manually labeled data has a certain number of noisy data. To solve this problem and achieve more than 90 score in dev dataset, we present a simple method to find the noisy data and re-label the noisy data by human, given the model predictions as references in human labeling. In this paper, we illustrate our idea for a broad set of deep learning tasks, includes classification, sequence tagging, object detection, sequence generation, click-through rate prediction. The experimental results and human evaluation results verify our idea.
    
[^108]: 基于深度强化学习的干扰器抗干扰频率和功率分配的泛化

    Generalization of Deep Reinforcement Learning for Jammer-Resilient Frequency and Power Allocation. (arXiv:2302.02250v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2302.02250](http://arxiv.org/abs/2302.02250)

    本研究提出了一种基于深度强化学习的干扰器抗干扰频率和功率分配的泛化方法，可以在不同的无线网络操作场景下使用。在实现上，我们证明了该方法可以在保证高效频谱使用的同时对干扰具有很强的抗干扰性。

    

    本文致力于解决深度强化学习模型在频率和功率分配方面的泛化能力。现有的大部分方法解决强化学习无线问题在特定的预设的无线网络场景中。训练代理的性能通常非常特定于网络，并在用于不同的网络操作场景（例如，大小、邻域和移动性等）时变差。我们展示了我们的方法如何增强训练，以在恶意干扰环境下通过分布式多智能体方式部署模型进行推理时提高其泛化能力。通过这些，我们证明了所提出的方法在不同大小和结构的先前未见过的仿真无线网络上测试时的改进训练和推理性能。更重要的是，为了证明实际影响，端到端解决方案已实现在嵌入式系统上，结果表明我们的方法在保证高效频谱使用的同时对干扰具有很强的抗干扰性。

    We tackle the problem of joint frequency and power allocation while emphasizing the generalization capability of a deep reinforcement learning model. Most of the existing methods solve reinforcement learning-based wireless problems for a specific pre-determined wireless network scenario. The performance of a trained agent tends to be very specific to the network and deteriorates when used in a different network operating scenario (e.g., different in size, neighborhood, and mobility, among others). We demonstrate our approach to enhance training to enable a higher generalization capability during inference of the deployed model in a distributed multi-agent setting in a hostile jamming environment. With all these, we show the improved training and inference performance of the proposed methods when tested on previously unseen simulated wireless networks of different sizes and architectures. More importantly, to prove practical impact, the end-to-end solution was implemented on the embedde
    
[^109]: 双重 PatchNorm

    Dual PatchNorm. (arXiv:2302.01327v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.01327](http://arxiv.org/abs/2302.01327)

    双重 PatchNorm 在 Vision Transformers 中的应用有助于提高准确性。

    

    我们提出了双重 PatchNorm：在 Vision Transformers 的 patch 嵌入层之前和之后采用了两个 Layer Normalization 层（LayerNorms）。我们证明了，双重 PatchNorm 表现比在 Transformer 块本身中进行替代 LayerNorm 放置策略的详尽搜索的结果更优。在我们的实验中，采用这种微小的改进通常会导致比精调 Vision Transformers 更高的准确性，而且从不损害性能。

    We propose Dual PatchNorm: two Layer Normalization layers (LayerNorms), before and after the patch embedding layer in Vision Transformers. We demonstrate that Dual PatchNorm outperforms the result of exhaustive search for alternative LayerNorm placement strategies in the Transformer block itself. In our experiments, incorporating this trivial modification, often leads to improved accuracy over well-tuned Vision Transformers and never hurts.
    
[^110]: 双排列等变性在知识图谱补全中的应用

    Double Permutation Equivariance for Knowledge Graph Completion. (arXiv:2302.01313v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01313](http://arxiv.org/abs/2302.01313)

    本研究提出了双排列等变性的KG表示方法，可以使神经网络在KG中执行复杂的逻辑推理任务，并在多个归纳KG完成任务中实现了最先进的Hits@10测试准确率。双排列等变性在KG中开辟了新的研究方向。

    

    本研究将知识图谱(KGs)形式化为一种新型的图，并称之为双交换属性图，其中节点和二元（两个节点之间的）表示必须对节点号和边（及节点）属性（关系和节点特征）的排列等变。双重排列等变的KG表示在KG中开辟了新的研究方向。我们展示了这种等变性对关系的结构表示产生的影响，从而使神经网络能够在KG中执行复杂的逻辑推理任务。最后，我们介绍了一种通用的等变表示蓝图，并测试了一种简单的基于GNN的双排列等变神经结构，在WN18RR、FB237和NELL995归纳KG完成任务中实现了最先进的Hits@10测试准确率，并能够准确执行现有方法无法执行的逻辑推理任务。

    This work provides a formalization of Knowledge Graphs (KGs) as a new class of graphs that we denote doubly exchangeable attributed graphs, where node and pairwise (joint 2-node) representations must be equivariant to permutations of both node ids and edge (& node) attributes (relations & node features). Double-permutation equivariant KG representations open a new research direction in KGs. We show that this equivariance imposes a structural representation of relations that allows neural networks to perform complex logical reasoning tasks in KGs. Finally, we introduce a general blueprint for such equivariant representations and test a simple GNN-based double-permutation equivariant neural architecture that achieve state-of-the-art Hits@10 test accuracy in the WN18RR, FB237 and NELL995 inductive KG completion tasks, and can accurately perform logical reasoning tasks that no existing methods can perform, to the best of our knowledge.
    
[^111]: 时态图的图神经网络：现状、挑战和机遇综述

    Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities. (arXiv:2302.01018v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01018](http://arxiv.org/abs/2302.01018)

    该论文总结了时态图的图神经网络的现状、挑战和机遇，提供了学习设置和任务的严格规范化以及一个新的分类法，并讨论了该领域最相关的开放挑战，从研究和应用角度讨论。

    

    图神经网络（GNNs）已经成为学习（静态）图结构数据的主要范例。然而，许多真实世界的系统是动态的，因为图和节点/边属性随着时间而变化。近年来，基于GNN的时态图模型已经成为扩展GNN能力的有前途的研究领域。在这项工作中，我们提供了第一个关于时态GNN的现状全面的概述，引入了学习设置和任务的严格规范化以及一个新的分类法，以表示和处理时态方面的现有方法。我们从研究和应用角度讨论了该领域最相关的开放挑战，结束了这项研究。

    Graph Neural Networks (GNNs) have become the leading paradigm for learning on (static) graph-structured data. However, many real-world systems are dynamic in nature, since the graph and node/edge attributes change over time. In recent years, GNN-based models for temporal graphs have emerged as a promising area of research to extend the capabilities of GNNs. In this work, we provide the first comprehensive overview of the current state-of-the-art of temporal GNN, introducing a rigorous formalization of learning settings and tasks and a novel taxonomy categorizing existing approaches in terms of how the temporal aspect is represented and processed. We conclude the survey with a discussion of the most relevant open challenges for the field, from both research and application perspectives.
    
[^112]: 鲁棒的在线主动学习策略

    Robust online active learning. (arXiv:2302.00422v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.00422](http://arxiv.org/abs/2302.00422)

    本文提出了一种自适应方法，用于鲁棒的在线主动学习，并在受污染的数据流中证明了其性能表现优异，同时确保了稳定性并减少异常值的负面影响。

    

    在许多工业应用中，获得标记的观测数据并不简单，通常需要人工专家干预或使用昂贵的测试设备。在这种情况下，主动学习可以大大提高拟合模型时最信息数据点的建议。减少模型开发所需的观测数据数量可以减轻训练所需的计算负担和标记相关的操作支出。特别是在线主动学习，在需要在极短时间内决定是否获取数据点标记的高容量生产过程中非常有用。然而，尽管最近致力于开发在线主动学习策略，但在存在异常值的情况下这些方法的行为仍未得到彻底研究。在这项工作中，我们调查了在线主动线性回归在受污染的数据流中的性能，并提出了一种自适应方法，用于鲁棒的在线主动学习，同时保证稳定性并减少异常值的负面影响。

    In many industrial applications, obtaining labeled observations is not straightforward as it often requires the intervention of human experts or the use of expensive testing equipment. In these circumstances, active learning can be highly beneficial in suggesting the most informative data points to be used when fitting a model. Reducing the number of observations needed for model development alleviates both the computational burden required for training and the operational expenses related to labeling. Online active learning, in particular, is useful in high-volume production processes where the decision about the acquisition of the label for a data point needs to be taken within an extremely short time frame. However, despite the recent efforts to develop online active learning strategies, the behavior of these methods in the presence of outliers has not been thoroughly examined. In this work, we investigate the performance of online active linear regression in contaminated data strea
    
[^113]: 不考虑图骨架的组合因果赌博机

    Combinatorial Causal Bandits without Graph Skeleton. (arXiv:2301.13392v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13392](http://arxiv.org/abs/2301.13392)

    本文研究了在二值一般因果模型和BGLMs上不考虑图骨架的组合因果赌博机问题，提出了可在BGLMs上实现的无需图骨架的遗憾最小化算法，达到了与依赖于图结构的最先进算法相同的渐进遗憾率$O(\sqrt{T}\ln T)$。

    

    在组合因果赌博机问题中，学习代理在每一轮选择一组变量进行干预，收集观测变量的反馈以最小化期望遗憾或样本复杂度。先前的工作研究了一般因果模型和二值广义线性模型（BGLMs）中的问题。但是，它们都需要先验知识来构建因果关系图。本文研究了在二值一般因果模型和BGLMs上不考虑图骨架的组合因果赌博机问题。我们首先在一般因果模型上提供了累积遗憾的指数下限。然后，我们设计了一种无需图骨架来实现BGLMs的遗憾最小化算法，表明它仍然达到$O(\sqrt{T}\ln T)$的期望遗憾。这个渐进的遗憾率与依赖于图结构的最先进算法相同。

    In combinatorial causal bandits (CCB), the learning agent chooses a subset of variables in each round to intervene and collects feedback from the observed variables to minimize expected regret or sample complexity. Previous works study this problem in both general causal models and binary generalized linear models (BGLMs). However, all of them require prior knowledge of causal graph structure. This paper studies the CCB problem without the graph structure on binary general causal models and BGLMs. We first provide an exponential lower bound of cumulative regrets for the CCB problem on general causal models. To overcome the exponentially large space of parameters, we then consider the CCB problem on BGLMs. We design a regret minimization algorithm for BGLMs even without the graph skeleton and show that it still achieves $O(\sqrt{T}\ln T)$ expected regret. This asymptotic regret is the same as the state-of-art algorithms relying on the graph structure. Moreover, we sacrifice the regret t
    
[^114]: 从流数据中在线发现演化系统的控制微分方程

    Online discovering governing differential equations of evolving systems from streaming data. (arXiv:2301.07863v2 [physics.comp-ph] UPDATED)

    [http://arxiv.org/abs/2301.07863](http://arxiv.org/abs/2301.07863)

    本研究提出了一种在线建模方法，逐一处理流数据，可以有效地识别演化系统的微分方程，尤其是变化后的系统产生的测量分布与以前不同的情况下。

    

    从现有观测数据中发现演化系统的控制方程是至关重要和具有挑战性的。本文考虑了一个新的场景：从流数据中发现控制方程。目前的方法难以综合考虑样本数据，从而无法处理这个任务。我们提出了一种在线建模方法，通过建模流数据而非处理整个数据集，逐一处理样本数据。该方法在发现普通微分方程和偏微分方程方面表现良好。演化系统随时间变化而变化，其状态也随之改变。因此找到精确的变化点至关重要。由变化后的系统产生的测量分布与以前不同，因此可以通过所提出的方法识别出差异。我们所提出的方法在识别演化系统的微分方程中有竞争力。

    Discovering the governing equations of evolving systems from available observations is essential and challenging. In this paper, we consider a new scenario: discovering governing equations from streaming data. Current methods struggle to discover governing differential equations with considering measurements as a whole, leading to failure to handle this task. We propose an online modeling method capable of handling samples one by one sequentially by modeling streaming data instead of processing the entire dataset. The proposed method performs well in discovering ordinary differential equations (ODEs) and partial differential equations (PDEs) from streaming data. Evolving systems are changing over time, which invariably changes with system status. Thus, finding the exact change points is critical. The measurement generated from a changed system is distributed dissimilarly to before; hence, the difference can be identified by the proposed method. Our proposal is competitive in identifyin
    
[^115]: RedMule: 用于芯片线性代数和TinyML训练加速的混合精度矩阵计算引擎

    RedMule: A Mixed-Precision Matrix-Matrix Operation Engine for Flexible and Energy-Efficient On-Chip Linear Algebra and TinyML Training Acceleration. (arXiv:2301.03904v2 [cs.AR] UPDATED)

    [http://arxiv.org/abs/2301.03904](http://arxiv.org/abs/2301.03904)

    该论文介绍了一种名为RedMule的低功耗混合精度矩阵计算引擎，它支持多种精度和格式，用于解决近传感器训练的能耗问题。

    

    近期对于功耗只有几十毫瓦的近传感器机器学习（TinyML）的兴趣逐渐增长，而当前TinyML训练算法基于各种形式的误差和梯度反向传播，需要浮点矩阵运算来满足精度和动态范围要求。然而，迄今为止这些运算的能量和功耗成本被认为太高，无法适应TinyML场景。本文旨在解决近传感器训练在少量毫瓦功耗预算下的开放性挑战，并提出RedMulE-简化精度矩阵乘法引擎（Reduced-Precision Matrix Multiplication Engine），这是一种专为多精度浮点通用矩阵乘法（GEMM-Ops）加速而设计的低功耗加速器，支持FP16和混合FP8格式，采用符号、指数、尾数为（{1,4,3}，{1,5,2}）。我们将RedMule集成到一个包含八个节能RISC-V核心的并行超低功耗（PULP）集群中。

    The increasing interest in TinyML, i.e., near-sensor machine learning on power budgets of a few tens of mW, is currently pushing toward enabling TinyML-class training as opposed to inference only. Current training algorithms, based on various forms of error and gradient backpropagation, rely on floating-point matrix operations to meet the precision and dynamic range requirements. So far, the energy and power cost of these operations has been considered too high for TinyML scenarios. This paper addresses the open challenge of near-sensor training on a few mW power budget and presents RedMulE - Reduced-Precision Matrix Multiplication Engine, a low-power specialized accelerator conceived for multi-precision floating-point General Matrix-Matrix Operations (GEMM-Ops) acceleration, supporting FP16, as well as hybrid FP8 formats, with {sign, exponent, mantissa}=({1,4,3}, {1,5,2}). We integrate RedMule into a Parallel Ultra-Low-Power (PULP) cluster containing eight energy-efficient RISC-V core
    
[^116]: 带元路径背景和加权负样本的异构图对比学习

    Heterogeneous Graph Contrastive Learning with Meta-path Contexts and Weighted Negative Samples. (arXiv:2212.13847v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.13847](http://arxiv.org/abs/2212.13847)

    本论文提出了MEOW，使用元路径的上下文信息和加权负样本，通过粗略和细粒度视图对异构图进行了对比学习。

    

    异构图对比学习近年来受到广泛关注。一些现有方法使用元路径构建对比视图。然而，大部分方法忽略了描述元路径连接对象的丰富元路径上下文。此外，他们未能区分负样本，这可能会对模型性能产生不利影响。为解决这些问题，我们提出了MEOW，它考虑了元路径上下文和加权负样本。具体而言，MEOW为对比构造了一个粗略视图和一个细粒度视图。前者反映哪些对象通过元路径相互连接，而后者使用元路径上下文并描述对象是如何连接的。然后，我们在理论上分析了InfoNCE损失并认识到它计算负样本梯度的局限性。为了更好地区分负样本，我们学习加权负样本表示，并将其用于对比学习中。

    Heterogeneous graph contrastive learning has received wide attention recently. Some existing methods use meta-paths, which are sequences of object types that capture semantic relationships between objects, to construct contrastive views. However, most of them ignore the rich meta-path context information that describes how two objects are connected by meta-paths. Further, they fail to distinguish negative samples, which could adversely affect the model performance. To address the problems, we propose MEOW, which considers both meta-path contexts and weighted negative samples. Specifically, MEOW constructs a coarse view and a fine-grained view for contrast. The former reflects which objects are connected by meta-paths, while the latter uses meta-path contexts and characterizes details on how the objects are connected. Then, we theoretically analyze the InfoNCE loss and recognize its limitations for computing gradients of negative samples. To better distinguish negative samples, we learn
    
[^117]: 论文信息提取中的事件个体化问题

    On Event Individuation for Document-Level Information Extraction. (arXiv:2212.09702v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09702](http://arxiv.org/abs/2212.09702)

    提出了问题──事件个体化对于模板填充任务是否适用，通过注释研究和误差分析，我们发现这引发了对模板填充度量的有效性、任务数据集的质量以及模型学习能力的担忧。

    

    随着信息提取系统在处理整个文件方面越来越熟练，传统的模板填充任务作为文件级信息提取的基准任务再次引起了人们的关注。在本文中，我们质疑了模板填充任务在这方面的适用性。我们认为该任务要求对事件个体化问题提供明确的答案——即区分不同的事件——而即使是人类专家在这个问题上也存在分歧。通过注释研究和误差分析，我们展示了这引发了对模板填充度量的有效性、任务数据集的质量以及模型学习能力的担忧。最后，我们考虑了可能的解决方案。

    As information extraction (IE) systems have grown more adept at processing whole documents, the classic task of template filling has seen renewed interest as benchmark for document-level IE. In this position paper, we call into question the suitability of template filling for this purpose. We argue that the task demands definitive answers to thorny questions of event individuation -- the problem of distinguishing distinct events -- about which even human experts disagree. Through an annotation study and error analysis, we show that this raises concerns about the usefulness of template filling metrics, the quality of datasets for the task, and the ability of models to learn it. Finally, we consider possible solutions.
    
[^118]: 使用语言模型提示进行推理：一项调查

    Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09597](http://arxiv.org/abs/2212.09597)

    本文提供了使用语言模型提示进行推理的前沿研究综合调查。讨论了新兴推理能力出现的潜在原因，并提供系统资源帮助初学者。

    

    推理作为复杂问题解决的重要能力，可以为医疗诊断、谈判等各种实际应用提供后端支持。本文对使用语言模型提示进行推理的前沿研究进行了综合调查。我们介绍了研究成果的比较和总结，并提供了系统资源以帮助初学者。我们还讨论了新兴推理能力出现的潜在原因，并突出了未来的研究方向。资源可在 https://github.com/zjunlp/Prompt4ReasoningPapers 上获取（定期更新）。

    Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
    
[^119]: 机器学习系统臃肿且存在漏洞

    Machine Learning Systems are Bloated and Vulnerable. (arXiv:2212.09437v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2212.09437](http://arxiv.org/abs/2212.09437)

    本文研究了机器学习容器中存在的臃肿问题，并开发了MMLB框架进行分析和量化，结果表明在某些情况下，臃肿会占据容器总大小的80％， 显著增加了容器的供应时间，最多增加了370％，且导致漏洞恶化最高达99％。

    

    当今的软件代码和功能冗余，并大多数用户并不使用。这种臃肿占据整个软件栈，从操作系统到后端、前端和网页都普遍存在。本文重点分析和量化机器学习容器中的臃肿问题。我们开发了MMLB框架来分析机器学习容器的臃肿，测量容器和软件包的臃肿程度。该工具量化了臃肿的来源，并与漏洞分析工具集成，以评估臃肿对容器漏洞的影响。通过对Tensorflow、Pytorch和NVIDIA的15个机器学习容器进行实验，我们发现臃肿是一个重要问题，在某些情况下占容器总大小的80%。我们的结果表明，臃肿显著增加了容器的供应时间，最多增加了370％，并导致漏洞恶化，最高达99％。

    Today's software is bloated with both code and features that are not used by most users. This bloat is prevalent across the entire software stack, from the operating system, all the way to software backends, frontends, and web-pages. In this paper, we focus on analyzing and quantifying bloat in machine learning containers. We develop MMLB, a framework to analyze bloat in machine learning containers, measuring the amount of bloat that exists on the container and package levels. Our tool quantifies the sources of bloat and integrates with vulnerability analysis tools to evaluate the impact of bloat on container vulnerabilities. Through experimentation with 15 machine learning containers from Tensorflow, Pytorch, and NVIDIA, we show that bloat is a significant issue, accounting for up to 80% of the container size in some cases. Our results demonstrate that bloat significantly increases the container provisioning time by up to 370% and exacerbates vulnerabilities by up to 99%.
    
[^120]: 计算代数曲面的线性部分：量子纠缠、张量分解及其更多应用

    Computing linear sections of varieties: quantum entanglement, tensor decompositions and beyond. (arXiv:2212.03851v3 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2212.03851](http://arxiv.org/abs/2212.03851)

    本文研究了计算代数曲面与线性子空间之间交集的问题，提出了一个算法可以有效地处理这个问题，并可以找到所有属于该交集的元素。

    

    本文研究在$\mathbb{F}^n$中，寻找任意对合代数曲面与给定的线性子空间的交点元素（其中$\mathbb{F}$可以是实数或复数域）的问题。这个问题涵盖了不同选择的代数曲面下的丰富的算法问题族。特别地，曲面由秩为1的矩阵组成的情况已经与量子信息理论和张量分解等不同领域的核心问题存在紧密联系。即使在秩为1的矩阵族的情况下，该问题已知在最坏情况下是NP-难的。但出乎意料的是，我们开发了一种算法，可以有效地解决“典型”的子空间问题。在这里，子空间$U \subseteq \mathbb{F}^n$被选择为特定维数的普通子空间，可能包含一些通用元素。我们的主要发现是，我们的算法可以保证找到所有属于$U$和曲面的交集的元素。

    We study the problem of finding elements in the intersection of an arbitrary conic variety in $\mathbb{F}^n$ with a given linear subspace (where $\mathbb{F}$ can be the real or complex field). This problem captures a rich family of algorithmic problems under different choices of the variety. The special case of the variety consisting of rank-1 matrices already has strong connections to central problems in different areas like quantum information theory and tensor decompositions. This problem is known to be NP-hard in the worst case, even for the variety of rank-1 matrices.  Surprisingly, despite these hardness results we develop an algorithm that solves this problem efficiently for "typical" subspaces. Here, the subspace $U \subseteq \mathbb{F}^n$ is chosen generically of a certain dimension, potentially with some generic elements of the variety contained in it. Our main result is a guarantee that our algorithm recovers all the elements of $U$ that lie in the variety, under some mild n
    
[^121]: 通过体素扩散实现的文本生成三维模型。

    Diffusion-SDF: Text-to-Shape via Voxelized Diffusion. (arXiv:2212.03293v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.03293](http://arxiv.org/abs/2212.03293)

    提出了一种新型生成式三维建模框架Diffusion-SDF，用于文本生成三维形状综合任务，采用SDF autoencoder和体素扩散模型学习和生成三维形状的体素化符号距离场（SDF）的表示形式，能够生成高度多样化的三维形状以符合给定的文本描述。

    

    随着对三维虚拟建模技术的关注不断增加，基于特定条件（如文本）生成新颖的三维内容已成为热门问题。本文提出了一种名为Diffusion-SDF的新型生成式三维建模框架，用于挑战性的文本生成三维形状综合任务。先前的方法在三维数据表示和形状生成方面缺乏灵活性，因此无法生成高度多样化的三维形状以符合给定的文本描述。为解决这个问题，我们提出了一个SDF autoencoder和体素扩散模型，用于学习和生成三维形状的体素化符号距离场（SDF）的表示形式。具体来说，我们设计了一种新颖的UinU-Net结构，在标准的U-Net结构中嵌入一个局部聚焦的内部网络，从而实现对独立于补丁的SDF表示方法更好的重构。我们将我们的方法扩展到进一步的文本生成三维形状任务中，包括文本条件下的形状完成和提高多样性的三维形状生成。

    With the rising industrial attention to 3D virtual modeling technology, generating novel 3D content based on specified conditions (e.g. text) has become a hot issue. In this paper, we propose a new generative 3D modeling framework called Diffusion-SDF for the challenging task of text-to-shape synthesis. Previous approaches lack flexibility in both 3D data representation and shape generation, thereby failing to generate highly diversified 3D shapes conforming to the given text descriptions. To address this, we propose a SDF autoencoder together with the Voxelized Diffusion model to learn and generate representations for voxelized signed distance fields (SDFs) of 3D shapes. Specifically, we design a novel UinU-Net architecture that implants a local-focused inner network inside the standard U-Net architecture, which enables better reconstruction of patch-independent SDF representations. We extend our approach to further text-to-shape tasks including text-conditioned shape completion and m
    
[^122]: 多维路径依赖期权的深度签名算法

    Deep Signature Algorithm for Multi-dimensional Path-Dependent Options. (arXiv:2211.11691v2 [q-fin.CP] UPDATED)

    [http://arxiv.org/abs/2211.11691](http://arxiv.org/abs/2211.11691)

    本文提出了一种适用于欧式型和美式型期权定价问题的多维路径依赖期权深度签名算法，解决了路径依赖型FBSDE问题，数值算法的收敛性得到保证。

    

    本文研究了路径依赖期权的深度签名算法。我们将Hur\'e-Pham-Warin在2020年针对具有反射的状态依赖型FBSDE的反向方案扩展到具有反射的路径依赖型FBSDE，通过向反向方案添加签名层。我们的算法适用于欧式型和美式型期权定价问题，而支付函数取决于基础正向股票过程的整个路径。我们证明了我们的数值算法的收敛性分析，并明确依赖于签名的截断阶数和神经网络逼近误差。算法的数值示例包括：Black-Scholes模型下的Amerasian期权、具有路径依赖几何平均收益函数的美式期权以及Shiryaev的最优停止问题。

    In this work, we study the deep signature algorithms for path-dependent options. We extend the backward scheme in [Hur\'e-Pham-Warin. Mathematics of Computation 89, no. 324 (2020)] for state-dependent FBSDEs with reflections to path-dependent FBSDEs with reflections, by adding the signature layer to the backward scheme. Our algorithm applies to both European and American type option pricing problems while the payoff function depends on the whole paths of the underlying forward stock process. We prove the convergence analysis of our numerical algorithm with explicit dependence on the truncation order of the signature and the neural network approximation errors. Numerical examples for the algorithm are provided including: Amerasian option under the Black-Scholes model, American option with a path-dependent geometric mean payoff function, and the Shiryaev's optimal stopping problem.
    
[^123]: 集成空间域感知和通信系统

    Integrated Space Domain Awareness and Communication System. (arXiv:2211.10260v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2211.10260](http://arxiv.org/abs/2211.10260)

    本文提出了一个集成的SDA和通信(ISDAC)系统，用于攻击者检测，通过开发一个轻量级的卷积神经网络架构来跟踪随机模式并满足SDA要求，系统在12种不同的攻击者配置下达到了97.8%以上的检测精度。

    

    空间正在发生变革，这种演变带来了新的威胁，加上技术发展和恶意意图，可能构成巨大挑战。空间域感知(SDA)是一个新的概念性想法，其旨在通过提供自主性、智能性和灵活性来实现空间潜在威胁的感知、检测、识别和对策。本研究首先提出一个清晰的新空间视角。其次，我们提出了一个集成的SDA和通信(ISDAC)系统，用于攻击者检测。我们假设攻击者有波束成形天线，并能够变化攻击情景，例如对某些接收天线进行随机攻击。为了跟踪随机模式和满足SDA要求，我们开发了一个轻量级卷积神经网络架构。所提出的ISDAC系统在12种不同的攻击者配置下表现出卓越和稳健的性能，检测精度超过97.8%。

    Space has been reforming and this evolution brings new threats that, together with technological developments and malicious intent, can pose a major challenge. Space domain awareness (SDA), a new conceptual idea, has come to the forefront. It aims sensing, detection, identification and countermeasures by providing autonomy, intelligence and flexibility against potential threats in space. In this study, we first present an insightful and clear view of the new space. Secondly, we propose an integrated SDA and communication (ISDAC) system for attacker detection. We assume that the attacker has beam-steering antennas and is capable to vary attack scenarios, such as random attacks on some receiver antennas. To track random patterns and meet SDA requirements, a lightweight convolutional neural network architecture is developed. The proposed ISDAC system shows superior and robust performance under 12 different attacker configurations with a detection accuracy of over 97.8%.
    
[^124]: 双类别感知对比联邦半监督学习

    Dual Class-Aware Contrastive Federated Semi-Supervised Learning. (arXiv:2211.08914v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08914](http://arxiv.org/abs/2211.08914)

    本论文提出了一种双类别感知对比联邦半监督学习方法，该方法考虑了数据的本地类别感知分布和全局类别感知分布，通过实现双类别感知对比模块，提高了联邦学习的性能表现。

    

    联邦半监督学习（FSSL）旨在训练一个全局模型而不共享私有数据，它利用伪标签和一致性正则化来利用未标记数据的知识，在未经处理的数据利用方面取得了显著的成功。然而，这些训练过程受到标记和未标记客户端上传的本地模型之间的大偏差以及伪标签引入的确认偏差的阻碍，这两者都对全局模型的性能产生负面影响。在本文中，我们提出了一种名为双类别感知对比联邦半监督学习（DCCFSSL）的新型FSSL方法。该方法考虑了每个客户端数据的本地类别感知分布和所有客户端数据在特征空间中的全局类别感知分布。通过实现双类别感知对比模块，DCCFSSL建立了一个统一的训练目标

    Federated semi-supervised learning (FSSL), facilitates labeled clients and unlabeled clients jointly training a global model without sharing private data. Existing FSSL methods predominantly employ pseudo-labeling and consistency regularization to exploit the knowledge of unlabeled data, achieving notable success in raw data utilization. However, these training processes are hindered by large deviations between uploaded local models of labeled and unlabeled clients, as well as confirmation bias introduced by noisy pseudo-labels, both of which negatively affect the global model's performance. In this paper, we present a novel FSSL method called Dual Class-aware Contrastive Federated Semi-Supervised Learning (DCCFSSL). This method accounts for both the local class-aware distribution of each client's data and the global class-aware distribution of all clients' data within the feature space. By implementing a dual class-aware contrastive module, DCCFSSL establishes a unified training objec
    
[^125]: 多视角压缩表示的鲁棒性低资源微调研究

    Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations. (arXiv:2211.08794v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.08794](http://arxiv.org/abs/2211.08794)

    本文提出了一种利用多视角压缩表示降低预训练语言模型微调过程中过拟合问题的方法，经过测试在低资源NLP任务中表现良好。

    

    由于参数的巨大数量，预训练语言模型（PLMs）的微调容易在低资源场景中出现过度拟合的问题。本文提出了一种新方法，该方法在PLM的隐藏表示上操作，以减少过拟合。在微调过程中，我们的方法在PLM的隐藏层之间插入随机自编码器，将来自前一层的激活转换为多视角压缩表示，然后将其馈送到上层。微调结束后，自编码器会被移除掉，因此我们的方法在推理过程中不会增加额外的参数或计算成本。我们的方法在一系列序列和标记级别的低资源NLP任务中展现了出色的性能提升。

    Due to the huge amount of parameters, fine-tuning of pretrained language models (PLMs) is prone to overfitting in the low resource scenarios. In this work, we present a novel method that operates on the hidden representations of a PLM to reduce overfitting. During fine-tuning, our method inserts random autoencoders between the hidden layers of a PLM, which transform activations from the previous layers into a multi-view compressed representation before feeding it into the upper layers. The autoencoders are plugged out after fine-tuning, so our method does not add extra parameters or increase computation cost during inference. Our method demonstrates promising performance improvement across a wide range of sequence- and token-level low-resource NLP tasks.
    
[^126]: 一种混合类别相关核的高斯过程

    A mixed-categorical correlation kernel for Gaussian process. (arXiv:2211.08262v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2211.08262](http://arxiv.org/abs/2211.08262)

    提出一种新的混合类别相关核的高斯过程代理，相较于其他现有模型在分析和工程问题上表现更好。

    

    近年来，基于高斯过程代理的混合类别元模型引起了越来越多的关注。在这种情况下，一些现有的方法使用不同的策略，通过使用连续核（例如，连续松弛和Gower距离基于高斯过程）或通过直接估计相关矩阵。在本文中，我们提出了一种基于核的方法，将连续指数核扩展为处理混合类别变量。所提出的核引导到了一个新的高斯代理，它概括了连续松弛和Gower距离基于高斯过程模型。我们在分析和工程问题上证明了，我们的提出的高斯过程模型比其他基于核的现有模型具有更高的可能性和更小的残差误差。我们的方法可使用开源软件SMT。

    Recently, there has been a growing interest for mixed-categorical meta-models based on Gaussian process (GP) surrogates. In this setting, several existing approaches use different strategies either by using continuous kernels (e.g., continuous relaxation and Gower distance based GP) or by using a direct estimation of the correlation matrix. In this paper, we present a kernel-based approach that extends continuous exponential kernels to handle mixed-categorical variables. The proposed kernel leads to a new GP surrogate that generalizes both the continuous relaxation and the Gower distance based GP models. We demonstrate, on both analytical and engineering problems, that our proposed GP model gives a higher likelihood and a smaller residual error than the other kernel-based state-of-the-art models. Our method is available in the open-source software SMT.
    
[^127]: 带辅助输入的Agent-State构建

    Agent-State Construction with Auxiliary Inputs. (arXiv:2211.07805v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.07805](http://arxiv.org/abs/2211.07805)

    本文提出了一种基于信息瓶颈原理的方法，将辅助输入纳入到Agent-State构建过程中，以构建一个代理状态，总结与世界的先前交互，有效地解决了部分观测性问题。

    

    在许多现实的序列决策任务中，决策代理往往无法模拟世界的全部复杂性。环境往往比代理更大更复杂，这也称为部分观测性。在这种情况下，代理必须利用不仅仅是当前的感官输入; 它必须构建一个代理状态，以总结与世界的先前交互。目前，应对这个问题的流行方法是通过一个循环网络从代理的感官流作为输入来学习Agent-State函数。许多强大的强化学习应用程序实际上依赖于特定于环境的函数来帮助代理输入历史摘要。这些增强有多种方式，从简单的方法，如连接观察，到更复杂的方法，如不确定性估计。尽管它们在领域中普遍存在，但我们称之为辅助输入的这些附加输入通常以一种特殊的方式处理。在本文中，我们探索了一种将辅助输入纳入Agent-State构建过程的原则方法。我们的方法基于信息瓶颈原理，其中辅助输入用于调整代理状态的信息内容，同时保留与决策制定相关的信息。

    In many, if not every realistic sequential decision-making task, the decision-making agent is not able to model the full complexity of the world. The environment is often much larger and more complex than the agent, a setting also known as partial observability. In such settings, the agent must leverage more than just the current sensory inputs; it must construct an agent state that summarizes previous interactions with the world. Currently, a popular approach for tackling this problem is to learn the agent-state function via a recurrent network from the agent's sensory stream as input. Many impressive reinforcement learning applications have instead relied on environment-specific functions to aid the agent's inputs for history summarization. These augmentations are done in multiple ways, from simple approaches like concatenating observations to more complex ones such as uncertainty estimates. Although ubiquitous in the field, these additional inputs, which we term auxiliary inputs, ar
    
[^128]: 使用基于网络问答和多模态融合完成知识库

    Knowledge Base Completion using Web-Based Question Answering and Multimodal Fusion. (arXiv:2211.07098v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.07098](http://arxiv.org/abs/2211.07098)

    本文提出了一种使用基于网络问答和多模态融合的方法填补知识库中的缺失信息。通过设计一个多模态特征和问题模板的基于网络问答的系统来达到更高效的知识库补全，同时结合了知识库中的结构化信息来提高抽取质量。

    

    近年来，大型知识库已经建立来存储大量知识，然而这些知识库非常不完整。为了解决这个问题，我们提出一种使用基于网络问答和多模态融合的方法填补知识库中的缺失信息。为了利用网络上的非结构化信息完成知识库，我们设计了一个基于网络问答的系统，使用多模态特征和问题模板来提取缺失的事实，仅仅通过非常少的问题就可以达到良好的效果。同时，该问答系统还使用知识库中的结构化信息，比如实体类型和实体之间的关联性，以帮助提高抽取质量。

    Over the past few years, large knowledge bases have been constructed to store massive amounts of knowledge. However, these knowledge bases are highly incomplete. To solve this problem, we propose a web-based question answering system system with multimodal fusion of unstructured and structured information, to fill in missing information for knowledge bases. To utilize unstructured information from the Web for knowledge base completion, we design a web-based question answering system using multimodal features and question templates to extract missing facts, which can achieve good performance with very few questions. To help improve extraction quality, the question answering system employs structured information from knowledge bases, such as entity types and entity-to-entity relatedness.
    
[^129]: CACTO：连续动态系统的轨迹优化Actor-Critic算法——走向全局最优。（arXiv:2211.06625v3 [cs.RO] UPDATED）

    CACTO: Continuous Actor-Critic with Trajectory Optimization -- Towards global optimality. (arXiv:2211.06625v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2211.06625](http://arxiv.org/abs/2211.06625)

    本研究提出了一种连续动态系统的控制算法，结合了轨迹优化和强化学习，通过TO引导的RL策略搜索学习控制策略来避免在控制过程中陷入贫乏的局部最优解。

    

    本文提出了一种新的算法，将轨迹优化（TO）和强化学习（RL）结合在一个框架中，用于连续动态系统的控制。该算法的动机是TO和RL在应用于非凸代价函数的连续非线性系统时存在的两个主要限制。本文方法利用TO引导的RL策略搜索来学习“好”的控制策略，并用作TO的初始猜测提供者，使轨迹优化过程不容易收敛到贫乏的局部最优解。我们的方法在几个不同的动态系统，包括具有非凸障碍物避免的到达问题上进行了验证。

    This paper presents a novel algorithm for the continuous control of dynamical systems that combines Trajectory Optimization (TO) and Reinforcement Learning (RL) in a single framework. The motivations behind this algorithm are the two main limitations of TO and RL when applied to continuous nonlinear systems to minimize a non-convex cost function. Specifically, TO can get stuck in poor local minima when the search is not initialized close to a "good" minimum. On the other hand, when dealing with continuous state and control spaces, the RL training process may be excessively long and strongly dependent on the exploration strategy. Thus, our algorithm learns a "good" control policy via TO-guided RL policy search that, when used as initial guess provider for TO, makes the trajectory optimization process less prone to converge to poor local optima. Our method is validated on several reaching problems featuring non-convex obstacle avoidance with different dynamical systems, including a car m
    
[^130]: 光学图像机器学习中数据漂移控制的数据模型

    Data Models for Dataset Drift Controls in Machine Learning With Optical Images. (arXiv:2211.02578v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.02578](http://arxiv.org/abs/2211.02578)

    该研究利用物理光学和传统机器学习的组合，获得明确且可微分的数据模型，以更好地理解数据漂移对机器学习模型性能的影响。

    

    相机图像在机器学习研究中无处不在，并在医学和环境调查等重要领域发挥着核心作用。然而，由于强度问题，这些领域中的机器学习模型应用受到限制。主要的故障模式是出现性能下降，这是由于训练和部署数据之间的差异引起的。虽然已有方法可以前瞻性地验证机器学习模型对这种数据集漂移的强度，但现有方法未考虑主要感兴趣的对象 - 数据的显式模型。这限制了我们在物理上准确地研究和理解数据生成与下游机器学习模型性能之间的关系的能力。在这项研究中，我们展示了如何通过将传统的机器学习与物理光学配对，获得明确且可微分的数据模型来克服这一限制。我们展示了这种数据模型如何帮助我们更好地理解数据漂移对机器学习模型性能的影响。

    Camera images are ubiquitous in machine learning research. They also play a central role in the delivery of important services spanning medicine and environmental surveying. However, the application of machine learning models in these domains has been limited because of robustness concerns. A primary failure mode are performance drops due to differences between the training and deployment data. While there are methods to prospectively validate the robustness of machine learning models to such dataset drifts, existing approaches do not account for explicit models of the primary object of interest: the data. This limits our ability to study and understand the relationship between data generation and downstream machine learning model performance in a physically accurate manner. In this study, we demonstrate how to overcome this limitation by pairing traditional machine learning with physical optics to obtain explicit and differentiable data models. We demonstrate how such data models can 
    
[^131]: DPM-Solver++：用于引导采样的快速扩散概率模型求解器

    DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models. (arXiv:2211.01095v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01095](http://arxiv.org/abs/2211.01095)

    DPM-Solver++是一种快速求解器，在扩散概率模型的引导采样中表现出色，可加快样本生成速度。

    

    扩散概率模型 (DPMs) 在高分辨率图像合成等领域表现出色，而 DPM 生成高质量样本所需的关键技术之一是引导采样。现有的快速采样器 DDIM 在引导采样方面表现良好，但需要 100 至 250 步才能生成高质量样本。本文提出了 DPM-Solver++，一种用于加速引导采样的高阶求解器。

    Diffusion probabilistic models (DPMs) have achieved impressive success in high-resolution image synthesis, especially in recent large-scale text-to-image generation applications. An essential technique for improving the sample quality of DPMs is guided sampling, which usually needs a large guidance scale to obtain the best sample quality. The commonly-used fast sampler for guided sampling is DDIM, a first-order diffusion ODE solver that generally needs 100 to 250 steps for high-quality samples. Although recent works propose dedicated high-order solvers and achieve a further speedup for sampling without guidance, their effectiveness for guided sampling has not been well-tested before. In this work, we demonstrate that previous high-order fast samplers suffer from instability issues, and they even become slower than DDIM when the guidance scale grows large. To further speed up guided sampling, we propose DPM-Solver++, a high-order solver for the guided sampling of DPMs. DPM-Solver++ solv
    
[^132]: 探究基于声调和语言特征的神经文本朗读自然度评估方法

    Investigating Content-Aware Neural Text-To-Speech MOS Prediction Using Prosodic and Linguistic Features. (arXiv:2211.00342v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2211.00342](http://arxiv.org/abs/2211.00342)

    本文提出探究声调和语言特征如何影响神经文本朗读自然度预测，通过加入这些特征，相较于基于频谱特征的基线模型 MOSNet，在 MOS 预测中取得了平均12%无公害提高。

    

    自动化合成音声的最新评估方法是基于 MOS 预测神经模型，其中 MOSNet 和 LDNet 使用频谱特征作为输入，而 SSL-MOS 则依赖于预训练的自监督学习模型，直接使用语音信号作为输入。在现代高质量的神经 TTS 系统中，就发音内容而言，声调的适当性是决定性的因素。为此，我们建议在 MOS 预测系统中包括声调和语言特征作为额外的输入，并评估它们对预测结果的影响。我们考虑了音素级的 F0 和持续时间特征作为声调输入，以及 Tacotron 编码器输出、POS 标记和 BERT 嵌入作为更高级别的语言输入。所有 MOS 预测系统均在 SOMOS 上进行训练，该数据集仅有神经语音合成技术和众包自然度 MOS 评估数据。结果表明，所提出的额外特征能够有利于提高 MOS 预测结果，相对于仅使用频谱特征的基线 MOSNet，平均提高了12%。

    Current state-of-the-art methods for automatic synthetic speech evaluation are based on MOS prediction neural models. Such MOS prediction models include MOSNet and LDNet that use spectral features as input, and SSL-MOS that relies on a pretrained self-supervised learning model that directly uses the speech signal as input. In modern high-quality neural TTS systems, prosodic appropriateness with regard to the spoken content is a decisive factor for speech naturalness. For this reason, we propose to include prosodic and linguistic features as additional inputs in MOS prediction systems, and evaluate their impact on the prediction outcome. We consider phoneme level F0 and duration features as prosodic inputs, as well as Tacotron encoder outputs, POS tags and BERT embeddings as higher-level linguistic inputs. All MOS prediction systems are trained on SOMOS, a neural TTS-only dataset with crowdsourced naturalness MOS evaluations. Results show that the proposed additional features are benefi
    
[^133]: 多分位数发布的学习增强私有算法

    Learning-Augmented Private Algorithms for Multiple Quantile Release. (arXiv:2210.11222v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2210.11222](http://arxiv.org/abs/2210.11222)

    本文提出一种新的隐私保护方法：使用学习增强算法框架，为多分位数发布任务提供可扩展的预测质量误差保证。

    

    当应用差分隐私于敏感数据时，我们常常可以利用额外的信息例如其他敏感数据、公众数据或人类信息先验来提升性能。本文提出了使用学习增强算法（或具有预测能力的算法）框架，这个框架通常使用于优化时间复杂度或竞争比率。该框架为设计和分析保护隐私的方法提供了一种强有力的方法，并能够利用这些额外信息以提高效用。该想法体现在重要的多分位数发布任务中，在此我们得出了随着自然质量预测的错误保证，同时（几乎）恢复了最先进的预测独立的保证。我们的分析具有几个优点，包括对数据的最小假设，一种自然的增强鲁棒性的方法，以及为两个从其他数据中学习预测的新颖“元”算法提供有用的替代损失。

    When applying differential privacy to sensitive data, we can often improve performance using external information such as other sensitive data, public data, or human priors. We propose to use the learning-augmented algorithms (or algorithms with predictions) framework -- previously applied largely to improve time complexity or competitive ratios -- as a powerful way of designing and analyzing privacy-preserving methods that can take advantage of such external information to improve utility. This idea is instantiated on the important task of multiple quantile release, for which we derive error guarantees that scale with a natural measure of prediction quality while (almost) recovering state-of-the-art prediction-independent guarantees. Our analysis enjoys several advantages, including minimal assumptions about the data, a natural way of adding robustness, and the provision of useful surrogate losses for two novel ``meta" algorithms that learn predictions from other (potentially sensitiv
    
[^134]: MMRNet：通过多模态冗余提高多模态物体检测与分割的可靠性，应用于抓取商品

    MMRNet: Improving Reliability for Multimodal Object Detection and Segmentation for Bin Picking via Multimodal Redundancy. (arXiv:2210.10842v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.10842](http://arxiv.org/abs/2210.10842)

    本文提出了一个基于多模态冗余的可靠的抓取商品物体检测和分割系统MMRNet，能够在传感器失效等异常情况下具有更好的鲁棒性，并在YCB-Video数据集上取得了最先进的性能。

    

    近年来，工业4.0基础设施的兴起解决了全球供应链的劳动力短缺问题。在实际应用中，通过部署基于人工智能的机器人货物捡拾系统来减轻工人的压力和体力需求，提高仓库的速度和效率已经变得尤为重要。但是，在传感器失灵等异常情况下可能会导致昂贵的损坏风险，因此可靠性成为将人工智能研究转化为实际应用和产品的关键因素。本文提出了一种可靠的物体检测和分割系统MMRNet，通过使用来自不同模态的数据进行抓取商品的物体检测和分割，而这是第一个将多模态冗余的概念引入到提高可靠性的抓取商品系统中。我们的系统在具有挑战性的YCB-Video数据集上实现了目标检测和分割的最先进性能，并且相较于基线方法具有更好的传感器失效表现。

    Recently, there has been tremendous interest in industry 4.0 infrastructure to address labor shortages in global supply chains. Deploying artificial intelligence-enabled robotic bin picking systems in real world has become particularly important for reducing stress and physical demands of workers while increasing speed and efficiency of warehouses. To this end, artificial intelligence-enabled robotic bin picking systems may be used to automate order picking, but with the risk of causing expensive damage during an abnormal event such as sensor failure. As such, reliability becomes a critical factor for translating artificial intelligence research to real world applications and products. In this paper, we propose a reliable object detection and segmentation system with MultiModal Redundancy (MMRNet) for tackling object detection and segmentation for robotic bin picking using data from different modalities. This is the first system that introduces the concept of multimodal redundancy to a
    
[^135]: 机器生成文本：威胁模型和检测方法的综合调查

    Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods. (arXiv:2210.07321v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.07321](http://arxiv.org/abs/2210.07321)

    本文主要调查了机器生成文本对社会和网络安全所带来的威胁，提供了最完整的机器生成文本检测方法评估，为应对威胁模型和解决检测问题提供了强有力的指导。

    

    机器生成的文本越来越难以与人类撰写的文本区分开来。功能强大的开源模型可以免费使用，可民主化访问生成模型的用户友好工具正在迅速增多。本次调查的第一版面世后不久，发布了ChatGPT，这一趋势被彰显出来。最先进的自然语言生成（NLG）系统的巨大潜力被各种滥用途径所抑制。检测机器生成文本是减少NLG模型滥用的主要对策，但也面临着重大技术挑战和众多未解决问题。我们提供了一份综合调查，包括1）对当代NLG系统造成威胁模型的广泛分析和2）截至目前为止关于机器生成文本检测方法的最完整的综述。这份调查将机器生成文本置于其网络安全和社会背景之中，并为未来解决最重要威胁模型提供了强有力的指导，并概述了最有前途的检测方法。

    Machine generated text is increasingly difficult to distinguish from human authored text. Powerful open-source models are freely available, and user-friendly tools that democratize access to generative models are proliferating. ChatGPT, which was released shortly after the first edition of this survey, epitomizes these trends. The great potential of state-of-the-art natural language generation (NLG) systems is tempered by the multitude of avenues for abuse. Detection of machine generated text is a key countermeasure for reducing abuse of NLG models, with significant technical challenges and numerous open problems. We provide a survey that includes both 1) an extensive analysis of threat models posed by contemporary NLG systems, and 2) the most complete review of machine generated text detection methods to date. This survey places machine generated text within its cybersecurity and social context, and provides strong guidance for future work addressing the most critical threat models, a
    
[^136]: CLIP-PAE：投影增强嵌入以提取相关特征用于可分离、可解释、可控的文本指导脸部操纵

    CLIP-PAE: Projection-Augmentation Embedding to Extract Relevant Features for a Disentangled, Interpretable, and Controllable Text-Guided Face Manipulation. (arXiv:2210.03919v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.03919](http://arxiv.org/abs/2210.03919)

    提出了一种为了解决文本引导图像操纵中的可分离性、可解释性和可控性问题，通过定义基于相关提示的语料库子空间来获取特定图像特征并引入CLIP投影增强嵌入（PAE）作为优化目标处理的新方法。

    

    最近引入的对比语言-图像预训练（CLIP）将图像和文本嵌入到共同的潜在空间中。这打开了一个大门，即旨在通过提供文字说明来操作输入图像的丰富文学资料。然而，由于联合空间中图像和文本嵌入之间的差异，将文本嵌入作为优化目标通常会导致结果图像中出现意外的伪影。对于操纵来说，可分离性、可解释性和可控性也很难保证。为了缓解这些问题，我们提出定义由相关提示展开的语料库子空间来捕获特定的图像特征。我们引入了CLIP投影增强嵌入（PAE）作为优化目标，以提高文本引导图像操纵的性能。我们的方法是一种简单而通用的范例，可以轻松地计算和适应，并平稳地融入到任何基于CLIP的图像操作算法中。

    Recently introduced Contrastive Language-Image Pre-Training (CLIP) bridges images and text by embedding them into a joint latent space. This opens the door to ample literature that aims to manipulate an input image by providing a textual explanation. However, due to the discrepancy between image and text embeddings in the joint space, using text embeddings as the optimization target often introduces undesired artifacts in the resulting images. Disentanglement, interpretability, and controllability are also hard to guarantee for manipulation. To alleviate these problems, we propose to define corpus subspaces spanned by relevant prompts to capture specific image characteristics. We introduce CLIP Projection-Augmentation Embedding (PAE) as an optimization target to improve the performance of text-guided image manipulation. Our method is a simple and general paradigm that can be easily computed and adapted, and smoothly incorporated into any CLIP-based image manipulation algorithm. To demo
    
[^137]: 探究物理知识神经网络中的故障模式及其缓解方法

    Investigating and Mitigating Failure Modes in Physics-informed Neural Networks (PINNs). (arXiv:2209.09988v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.09988](http://arxiv.org/abs/2209.09988)

    本文探究了PINN在解决PDE问题时所面临的困难，并提出了一种新的方法解决了高阶PDE污染反向传播梯度的问题，减小了搜索空间维度，使非平滑解的PDE学习成为可能。

    

    本文探讨了使用物理知识神经网络（PINNs）求解偏微分方程（PDEs）时所面临的困难。PINNs在目标函数中使用物理知识作为正则化项。然而，这种方法的缺点是需要手动调节超参数，如果没有验证数据或先前的解决方案知识，则不切实际。我们研究了存在物理学的损失景观和反向传播梯度困难，发现现有的方法会产生难以导航的非凸损失景观。我们的发现表明，高阶PDEs会污染反向传播梯度并且阻碍收敛。为解决这些挑战，我们介绍了一种新的方法，绕过高阶导数算子的计算，并减轻反向传播梯度的污染。因此，我们降低了搜索空间的维度，并使非平滑解的PDE学习成为可能。我们的方法还提供了自动调节能力，消除了手动调节超参数的需求。

    This paper explores the difficulties in solving partial differential equations (PDEs) using physics-informed neural networks (PINNs). PINNs use physics as a regularization term in the objective function. However, a drawback of this approach is the requirement for manual hyperparameter tuning, making it impractical in the absence of validation data or prior knowledge of the solution. Our investigations of the loss landscapes and backpropagated gradients in the presence of physics reveal that existing methods produce non-convex loss landscapes that are hard to navigate. Our findings demonstrate that high-order PDEs contaminate backpropagated gradients and hinder convergence. To address these challenges, we introduce a novel method that bypasses the calculation of high-order derivative operators and mitigates the contamination of backpropagated gradients. Consequently, we reduce the dimension of the search space and make learning PDEs with non-smooth solutions feasible. Our method also pr
    
[^138]: 张量核上高效的量化稀疏矩阵计算

    Efficient Quantized Sparse Matrix Operations on Tensor Cores. (arXiv:2209.06979v4 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2209.06979](http://arxiv.org/abs/2209.06979)

    本文提出了Magicube，一个基于张量核的低精度整数的高性能稀疏矩阵库，支持深度学习中的稀疏运算。在NVIDIA A100 GPU上实验表明，Magicube相对供应商优化库平均获得了1.44倍的速度提升，并且相对于状态-of-the-art的GPU库，速度提升了1.43倍。

    

    深度学习不断取得成功，但模型大小的指数级增长带来了计算和内存成本的难题。为了缓解这个问题，模型的稀疏化和量化已经被研究。从硬件角度来看，硬件供应商为加速提供了张量核。然而，从低精度矩阵运算的角度来看，在张量核上实现稀疏，低精度矩阵操作非常具有挑战性，因为需要满足不同的数据布局需求和有效地操作低精度整数的不足。我们提出了Magicube，一个基于张量核的低精度整数的高性能稀疏矩阵库。Magicube支持混合精度下的SpMM和SDDMM，这是深度学习中两个主要的稀疏操作。在NVIDIA A100 GPU上的实验结果表明，Magicube相对于供应商优化库，平均获得1.44倍（最高2.37倍）速度提升，并且相对于状态-of-the-art的GPU库，速度提升了1.43倍。

    The exponentially growing model size drives the continued success of deep learning, but it brings prohibitive computation and memory cost. From the algorithm perspective, model sparsification and quantization have been studied to alleviate the problem. From the architecture perspective, hardware vendors provide Tensor cores for acceleration. However, it is very challenging to gain practical speedups from sparse, low-precision matrix operations on Tensor cores, because of the strict requirements for data layout and lack of support for efficiently manipulating the low-precision integers. We propose Magicube, a high-performance sparse-matrix library for low-precision integers on Tensor cores. Magicube supports SpMM and SDDMM, two major sparse operations in deep learning with mixed precision. Experimental results on an NVIDIA A100 GPU show that Magicube achieves on average 1.44x (up to 2.37x) speedup over the vendor-optimized library for sparse kernels, and 1.43x speedup over the state-of-
    
[^139]: Real-to-Sim: 使用基于学习的无迹卡尔曼滤波器预测机器人系统残余误差的稀疏数据转换

    Real-to-Sim: Predicting Residual Errors of Robotic Systems with Sparse Data using a Learning-based Unscented Kalman Filter. (arXiv:2209.03210v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2209.03210](http://arxiv.org/abs/2209.03210)

    本文提出了一种基于学习的无迹卡尔曼滤波器用于预测机器人系统的残余误差，实现了使用少量数据建模这些残余误差的目标，进一步缩小了动态模型与真实机器人之间的差距。

    

    实现与实际机器人接近的高精度动态模型或仿真模型可以促进基于模型的控制（如模型预测控制或线性二次型控制器）、基于模型的轨迹规划（如轨迹优化）以及减少强化学习方法所需的学习时间。因此，本文的目标是学习动态模型和/或仿真模型与真实机器人之间的残余误差。使用神经网络实现这一目标，神经网络的参数通过无迹卡尔曼滤波器（UKF）的形式进行更新。通过这种方法，我们只需要使用少量数据就可以对这些残余误差进行建模——我们通过直接从实际操作中学习来改进模拟器/动态模型是必要的。我们在机器人硬件上（例如操作臂和轮式机器人）演示了我们的方法，并展示了使用学习到的残余误差，我们可以进一步缩小动态模型与真实机器人之间的差距。

    Achieving highly accurate dynamic or simulator models that are close to the real robot can facilitate model-based controls (e.g., model predictive control or linear-quadradic regulators), model-based trajectory planning (e.g., trajectory optimization), and decrease the amount of learning time necessary for reinforcement learning methods. Thus, the objective of this work is to learn the residual errors between a dynamic and/or simulator model and the real robot. This is achieved using a neural network, where the parameters of a neural network are updated through an Unscented Kalman Filter (UKF) formulation. Using this method, we model these residual errors with only small amounts of data -- a necessity as we improve the simulator/dynamic model by learning directly from real-world operation. We demonstrate our method on robotic hardware (e.g., manipulator arm, and a wheeled robot), and show that with the learned residual errors, we can further close the reality gap between dynamic models
    
[^140]: 个体概率预测的协调

    Reconciling Individual Probability Forecasts. (arXiv:2209.01687v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.01687](http://arxiv.org/abs/2209.01687)

    个体概率预测的协调方法可以通过对于个体概率的实证验证和改进，使得模型得到提升并在几乎所有情况下达成一致。

    

    个体概率指的是仅在一次结果中实现的结果概率，如明天下雨概率、Alice在接下来的12个月内死亡概率、Bob在接下来的18个月内因暴力犯罪被捕的概率等。个体概率本质上是不可知的。然而，我们展示了在同意数据或从数据分布中进行采样的两个当事方不能在如何建模个体概率上达成一致，因为任何两种有着实质性分歧的个体概率模型都可以一起用于实证地证明并改善其中至少一个模型。这可以高效地迭代，在"协调"的过程中得到双方都认同的优于初始模型的模型，并在几乎所有情况下（几乎）达成了个体概率预测的一致。我们的结论是，尽管个体概率预测本质上是不确定的，但可以通过协调间接改进。

    Individual probabilities refer to the probabilities of outcomes that are realized only once: the probability that it will rain tomorrow, the probability that Alice will die within the next 12 months, the probability that Bob will be arrested for a violent crime in the next 18 months, etc. Individual probabilities are fundamentally unknowable. Nevertheless, we show that two parties who agree on the data -- or on how to sample from a data distribution -- cannot agree to disagree on how to model individual probabilities. This is because any two models of individual probabilities that substantially disagree can together be used to empirically falsify and improve at least one of the two models. This can be efficiently iterated in a process of "reconciliation" that results in models that both parties agree are superior to the models they started with, and which themselves (almost) agree on the forecasts of individual probabilities (almost) everywhere. We conclude that although individual pro
    
[^141]: 自然语言理解中大型语言模型的快捷学习

    Shortcut Learning of Large Language Models in Natural Language Understanding. (arXiv:2208.11857v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.11857](http://arxiv.org/abs/2208.11857)

    本文综述了大型语言模型中快捷学习和鲁棒性挑战的解决方法和相关研究，包括识别其快捷学习行为、原因和解决方案，并探讨了领域的主要研究挑战和潜在研究方向。

    

    大型语言模型(LLMs)在一系列自然语言理解任务中取得了最先进的性能。然而，这些LLMs可能会依赖于数据集的偏见和缺陷作为预测的快捷方式。这显著地影响了它们的泛化能力和对抗鲁棒性。本文综述了最近解决LLMs快捷学习和鲁棒性挑战的发展。我们首先介绍语言模型的快捷学习概念。然后介绍了识别语言模型快捷学习行为的方法，表征快捷学习的原因，并介绍了缓解解决方案。最后，我们讨论了LLMs领域的主要研究挑战和潜在研究方向。

    Large language models (LLMs) have achieved state-of-the-art performance on a series of natural language understanding tasks. However, these LLMs might rely on dataset bias and artifacts as shortcuts for prediction. This has significantly affected their generalizability and adversarial robustness. In this paper, we provide a review of recent developments that address the shortcut learning and robustness challenge of LLMs. We first introduce the concepts of shortcut learning of language models. We then introduce methods to identify shortcut learning behavior in language models, characterize the reasons for shortcut learning, as well as introduce mitigation solutions. Finally, we discuss key research challenges and potential research directions in order to advance the field of LLMs.
    
[^142]: 带约束状态空间模型的扩散时间序列插补和预测

    Diffusion-based Time Series Imputation and Forecasting with Structured State Space Models. (arXiv:2208.09399v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.09399](http://arxiv.org/abs/2208.09399)

    本文提出了一种新的状态空间框架(SSSD)来插补时间序列数据中的缺失值和进行预测，在各种数据集和不同的缺失情况下，SSSD都表现出更好的性能，并可以有效处理黑屏缺失情况。

    

    缺失值的插补对于许多现实世界的数据分析管道来说是一个重要的障碍。本文聚焦于时间序列数据，并提出了SSSD，这是一种依赖于两个新兴技术的插补模型，分别是（条件）扩散模型作为最先进的生成模型以及带约束的状态空间模型作为内部模型架构，其特别适用于捕捉时间序列数据中的长期依赖关系。我们证明，SSSD在各种数据集和不同的缺失情况下，包括挑战性的黑屏缺失情况下，均可达到或甚至超过最先进的概率插补和预测性能，而先前的方法则无法提供有意义的结果。

    The imputation of missing values represents a significant obstacle for many real-world data analysis pipelines. Here, we focus on time series data and put forward SSSD, an imputation model that relies on two emerging technologies, (conditional) diffusion models as state-of-the-art generative models and structured state space models as internal model architecture, which are particularly suited to capture long-term dependencies in time series data. We demonstrate that SSSD matches or even exceeds state-of-the-art probabilistic imputation and forecasting performance on a broad range of data sets and different missingness scenarios, including the challenging blackout-missing scenarios, where prior approaches failed to provide meaningful results.
    
[^143]: 基于可微分WORLD合成器的神经声码器及其在端到端音频风格转换中的应用

    Differentiable WORLD Synthesizer-based Neural Vocoder With Application To End-To-End Audio Style Transfer. (arXiv:2208.07282v4 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2208.07282](http://arxiv.org/abs/2208.07282)

    比较不同方法在音频风格转换中的应用，提出一种可微分的WORLD合成器，并通过声学特征参数来实现音高和音色信息的分离。

    

    本文提出了一种可微分的WORLD合成器，并展示了其在端到端音频风格转换任务（如（唱）声音转换和DDSP音色转换任务）中的应用。我们的基线可微分合成器没有模型参数，但它产生了足够的合成质量。我们可以通过附加轻量级的黑箱后网络来扩展基线合成器，以进一步处理基线输出，从而提高保真度。另一种可微分方法是直接提取源激发谱，这可以改善自然度，但仅适用于较窄的风格转换应用类别。我们方法使用的声学特征参数化具有附加的好处，它自然地将音高和音色信息分离开来，以便它们可以分别建模。此外，由于有一种强大的方法可以从单声道音频源估计这些声学特征，它允许风格转换任务中的参数损失。

    In this paper, we propose a differentiable WORLD synthesizer and demonstrate its use in end-to-end audio style transfer tasks such as (singing) voice conversion and the DDSP timbre transfer task. Accordingly, our baseline differentiable synthesizer has no model parameters, yet it yields adequate synthesis quality. We can extend the baseline synthesizer by appending lightweight black-box postnets which apply further processing to the baseline output in order to improve fidelity. An alternative differentiable approach considers extraction of the source excitation spectrum directly, which can improve naturalness albeit for a narrower class of style transfer applications. The acoustic feature parameterization used by our approaches has the added benefit that it naturally disentangles pitch and timbral information so that they can be modeled separately. Moreover, as there exists a robust means of estimating these acoustic features from monophonic audio sources, it allows for parameter loss 
    
[^144]: 几何变换的可证明防御

    Provable Defense Against Geometric Transformations. (arXiv:2207.11177v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.11177](http://arxiv.org/abs/2207.11177)

    提出了第一个可证明的确定性认证几何鲁棒性防御框架，利用了一种新颖的GPU优化验证器，能够快速地训练鲁棒的DNNs，并在多个数据集上 consistently achieve state-of-the-art deterministic certified geometric robustness 和干净准确度的效果。

    

    在现实世界中出现的几何图像变换（如缩放和旋转）已被证明可以轻松欺骗深度神经网络（DNN）。因此，训练DNN以在这些扰动下有认证的鲁棒性至关重要。然而，迄今为止，由于现有的验证器速度过慢，未能将确定性认证鲁棒性对抗几何变换的目标纳入训练过程中。为了解决这些挑战，我们提出了第一个可证明的确定性认证几何鲁棒性的防御框架。我们的框架利用了一种新颖的GPU优化验证器，其可以比现有的几何鲁棒性验证器快60倍到42,600倍，因此与现有的作品不同，我们的防御框架足够快，适用于训练。在多个数据集上，我们的结果表明，通过我们的框架训练的网络始终达到最先进的确定性认证几何鲁棒性和干净准确度。此外，我们展示了我们的防御可以有效地提高基于扰动训练的DNNs的准确性。

    Geometric image transformations that arise in the real world, such as scaling and rotation, have been shown to easily deceive deep neural networks (DNNs). Hence, training DNNs to be certifiably robust to these perturbations is critical. However, no prior work has been able to incorporate the objective of deterministic certified robustness against geometric transformations into the training procedure, as existing verifiers are exceedingly slow. To address these challenges, we propose the first provable defense for deterministic certified geometric robustness. Our framework leverages a novel GPU-optimized verifier that can certify images between 60$\times$ to 42,600$\times$ faster than existing geometric robustness verifiers, and thus unlike existing works, is fast enough for use in training. Across multiple datasets, our results show that networks trained via our framework consistently achieve state-of-the-art deterministic certified geometric robustness and clean accuracy. Furthermore,
    
[^145]: Wasserstein多元自回归模型用于建模分布时间序列及其在图形学习中的应用

    Wasserstein multivariate auto-regressive models for modeling distributional time series and its application in graph learning. (arXiv:2207.05442v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.05442](http://arxiv.org/abs/2207.05442)

    本文提出了一种新的自回归模型，用于分析多元分布时间序列。并且在Wasserstein空间中建模了随机对象，提供了该模型的解的存在性和一致估计器。此方法可以应用于年龄分布和自行车共享网络的观察数据。

    

    我们提出了一种新的自回归模型，用于统计分析多元分布时间序列。感兴趣的数据包括一组在实线有界间隔上支持的概率测度的多个系列，并且被不同时间瞬间所索引。概率测度被建模为Wasserstein空间中的随机对象。我们通过在Lebesgue测度的切空间中建立自回归模型，首先对所有原始测度进行居中处理，以便它们的Fréchet平均值成为Lebesgue测度。利用迭代随机函数系统的理论，提供了这样一个模型的解的存在性、唯一性和平稳性的结果。我们还提出了模型系数的一致估计器。除了对模拟数据的分析，我们还使用两个实际数据集进行了模型演示：一个是不同国家年龄分布的观察数据集，另一个是巴黎自行车共享网络的观察数据集。

    We propose a new auto-regressive model for the statistical analysis of multivariate distributional time series. The data of interest consist of a collection of multiple series of probability measures supported over a bounded interval of the real line, and that are indexed by distinct time instants. The probability measures are modelled as random objects in the Wasserstein space. We establish the auto-regressive model in the tangent space at the Lebesgue measure by first centering all the raw measures so that their Fr\'echet means turn to be the Lebesgue measure. Using the theory of iterated random function systems, results on the existence, uniqueness and stationarity of the solution of such a model are provided. We also propose a consistent estimator for the model coefficient. In addition to the analysis of simulated data, the proposed model is illustrated with two real data sets made of observations from age distribution in different countries and bike sharing network in Paris. Final
    
[^146]: TabPFN：在一秒内解决小型表格分类问题的Transformer

    TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second. (arXiv:2207.01848v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.01848](http://arxiv.org/abs/2207.01848)

    TabPFN是一种可以在不到一秒钟内完成小型表格数据集的监督分类的Transformer，无需超参数调整，并且具有竞争力。它使用先验适应网络（PFN）逼近基于先验的贝叶斯推断，先验融合了因果推理的思想。

    

    本文提出了TabPFN，一种经过训练的Transformer，可以在不到一秒钟的时间内完成小型表格数据集的监督分类，无需超参数调整，并且在分类方法的最新状态下具有竞争力。TabPFN完全包含在我们网络的权重中，接受训练和测试样本作为设置值输入，并在单个前向传递中为整个测试集提供预测。TabPFN是一种先验适应网络（PFN），只需要线下训练一次，即可逼近基于我们的先验的合成数据集上的贝叶斯推断。这个先验融合了因果推理的思想：它包括一个大的结构因果模型空间，偏好于简单结构。在OpenML-CC18套件的18个包含最多1000个训练数据点、最多100个纯数值特征且无缺失值、最多10个类别的数据集中，我们展示了我们的方法明显优于提升树，与复杂的最新AutoM方法表现相当。

    We present TabPFN, a trained Transformer that can do supervised classification for small tabular datasets in less than a second, needs no hyperparameter tuning and is competitive with state-of-the-art classification methods. TabPFN is fully entailed in the weights of our network, which accepts training and test samples as a set-valued input and yields predictions for the entire test set in a single forward pass. TabPFN is a Prior-Data Fitted Network (PFN) and is trained offline once, to approximate Bayesian inference on synthetic datasets drawn from our prior. This prior incorporates ideas from causal reasoning: It entails a large space of structural causal models with a preference for simple structures. On the 18 datasets in the OpenML-CC18 suite that contain up to 1 000 training data points, up to 100 purely numerical features without missing values, and up to 10 classes, we show that our method clearly outperforms boosted trees and performs on par with complex state-of-the-art AutoM
    
[^147]: ChordMixer：一种可扩展的神经注意力模型，适用于长度不同的序列

    ChordMixer: A Scalable Neural Attention Model for Sequences with Different Lengths. (arXiv:2206.05852v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.05852](http://arxiv.org/abs/2206.05852)

    ChordMixer 提出了一个简单的神经网络建模组，能够处理具有可变长度的长序列的注意力，并在实验中表现出明显的优势。

    

    在许多领域中，顺序数据自然具有不同的长度，有些序列非常长。作为重要的建模工具，神经注意力应该捕捉这种序列中的远距离交互。然而，大多数现有的神经注意力模型只能处理短序列，或者它们不得不使用分块或填充来强制输入长度保持恒定。我们在这里提出了一个简单的神经网络构建块，称为ChordMixer，它可以模拟具有可变长度的长序列的注意力。每个ChordMixer块由一个没有可学习参数的位置旋转层和一个逐元素MLP层组成。重复应用这些块形成了一个有效的网络骨干，将输入信号混合到学习目标中。我们在合成加问题、长文档分类和基于DNA序列的分类问题上测试了ChordMixer。实验结果表明，我们的方法明显优于其他神经注意力模型。

    Sequential data naturally have different lengths in many domains, with some very long sequences. As an important modeling tool, neural attention should capture long-range interaction in such sequences. However, most existing neural attention models admit only short sequences, or they have to employ chunking or padding to enforce a constant input length. Here we propose a simple neural network building block called ChordMixer which can model the attention for long sequences with variable lengths. Each ChordMixer block consists of a position-wise rotation layer without learnable parameters and an element-wise MLP layer. Repeatedly applying such blocks forms an effective network backbone that mixes the input signals towards the learning targets. We have tested ChordMixer on the synthetic adding problem, long document classification, and DNA sequence-based taxonomy classification. The experiment results show that our method substantially outperforms other neural attention models.
    
[^148]: 探究CLIP的开放性

    Delving into the Openness of CLIP. (arXiv:2206.01986v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.01986](http://arxiv.org/abs/2206.01986)

    本研究探究了CLIP模型的开放性，并通过词汇扩展来评估模型的可扩展性。研究发现，类似于CLIP的模型并不真正开放，并且随着词汇表的扩展其性能会恶化。此外，研究还揭示了CLIP表示在不变性和特定性之间存在权衡。

    

    对比语言-图像预训练（CLIP）将图像分类作为一项图像到文本匹配任务，即将图像与相应的自然语言描述进行匹配，而不是离散的类别ID。这使得模型可以以零-shot方式从开放类集（也称为开放词汇表）中识别图像。然而，评估类似于CLIP的模型的开放性很具有挑战性，因为这些模型理论上对任意词汇开放，但在实践中其精度有所变化。为解决这个问题，我们采用了递增的视角通过词汇扩展来评估开放性，并定义了可扩展性来衡量模型处理新类的能力。我们的评估结果表明，类似于CLIP的模型并不真正开放，并且随着词汇表的扩展其性能会恶化。我们进一步从表示对齐和统一性的角度剖析了CLIP的特征空间。我们的研究揭示CLIP表示在不监督的预训练中在不变性和特定性之间存在权衡，通过微调可提高其特定性。

    Contrastive Language-Image Pre-training (CLIP) formulates image classification as an image-to-text matching task, i.e., matching images to the corresponding natural language descriptions instead of discrete category IDs. This allows for open-vocabulary visual recognition, where the model can recognize images from an open class set (also known as an open vocabulary) in a zero-shot manner. However, evaluating the openness of CLIP-like models is challenging, as the models are open to arbitrary vocabulary in theory, but their accuracy varies in practice. To address this, we resort to an incremental perspective to assess the openness through vocabulary expansions, and define extensibility to measure a model's ability to handle novel classes. Our evaluation shows that CLIP-like models are not truly open, and their performance deteriorates as the vocabulary expands. We further dissect the feature space of CLIP from the perspectives of representation alignment and uniformity. Our investigation
    
[^149]: 关于关系特征的构成及其在解释黑盒预测器中的应用

    Composition of Relational Features with an Application to Explaining Black-Box Predictors. (arXiv:2206.00738v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.00738](http://arxiv.org/abs/2206.00738)

    本文提出了一种将关系特征视为函数，并使用通用函数组合的概念从简单函数推导出复杂函数的方法，将其应用于解释黑盒预测器的预测。

    

    类似归纳逻辑编程（ILP）的关系机器学习程序具有以下优点：（1）能够对数据实例之间的复杂关系建模；（2）在模型构建期间使用特定于领域的关系；（3）构建的模型是人类可读的，这通常更接近人类的理解。本文将关系特征视为函数，并使用通用函数组合的概念从简单函数推导出复杂函数。我们制定了一种在模据语言M中的 $\text{M}$-简单特征集的概念，并确定了两个组合算子（$\rho_1$和$\rho_2$），所有可能的复杂特征都可以从中派生出来。我们利用这些结果实现了一种“可解释的神经网络”，其中我们使用我们的方法构建和解释黑盒预测器的预测。

    Relational machine learning programs like those developed in Inductive Logic Programming (ILP) offer several advantages: (1) The ability to model complex relationships amongst data instances; (2) The use of domain-specific relations during model construction; and (3) The models constructed are human-readable, which is often one step closer to being human-understandable. However, these ILP-like methods have not been able to capitalise fully on the rapid hardware, software and algorithmic developments fuelling current developments in deep neural networks. In this paper, we treat relational features as functions and use the notion of generalised composition of functions to derive complex functions from simpler ones. We formulate the notion of a set of $\text{M}$-simple features in a mode language $\text{M}$ and identify two composition operators ($\rho_1$ and $\rho_2$) from which all possible complex features can be derived. We use these results to implement a form of "explainable neural 
    
[^150]: 动态潜在状态模型中的反事实分析

    Counterfactual Analysis in Dynamic Latent State Models. (arXiv:2205.13832v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.13832](http://arxiv.org/abs/2205.13832)

    该论文提供了一个框架来解决带有隐藏状态的动态模型的反事实分析问题，并在乳腺癌案例研究中成功应用。通过优化方法得到了反事实量的上限和下限，并且该论文是第一个在动态潜在状态模型中进行这种计算的研究。

    

    我们提供了一个基于优化的框架来执行具有隐藏状态的动态模型中的反事实分析。我们的框架以“猜测、行动和预测”方法为基础，以回答反事实查询，并解决了两个关键问题：(1)状态是隐藏的，(2)模型是动态的。考虑到对潜在因果机制的缺乏了解以及可能存在无限多个这样的机制，我们在该空间上进行优化，并计算所关心的反事实量的上限和下限。我们的工作汇集了因果关系、状态空间模型、模拟和优化的思想，并应用于乳腺癌案例研究中。据我们所知，我们是第一个在动态潜在状态模型中计算反事实查询的上限和下限的研究。

    We provide an optimization-based framework to perform counterfactual analysis in a dynamic model with hidden states. Our framework is grounded in the ``abduction, action, and prediction'' approach to answer counterfactual queries and handles two key challenges where (1) the states are hidden and (2) the model is dynamic. Recognizing the lack of knowledge on the underlying causal mechanism and the possibility of infinitely many such mechanisms, we optimize over this space and compute upper and lower bounds on the counterfactual quantity of interest. Our work brings together ideas from causality, state-space models, simulation, and optimization, and we apply it on a breast cancer case study. To the best of our knowledge, we are the first to compute lower and upper bounds on a counterfactual query in a dynamic latent-state model.
    
[^151]: 什么是公平性？哲学的思考与对fairML的影响

    What Is Fairness? Philosophical Considerations and Implications For FairML. (arXiv:2205.09622v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.09622](http://arxiv.org/abs/2205.09622)

    本文探讨了公平性的哲学概念，提出了公平性和预测性能不是不可调和的对立面，并强调从数据收集到最终模型评估都需纳入伦理考虑。

    

    在公平性人工智能(fairML)领域，通过定义衡量模型公平性的度量和提出确保训练模型数据具有低公平性度量值的方法，来减轻人工智能(ML)产生的相关不公平性问题。然而，公平的基本概念，即"公平是什么"，很少被讨论，这造成了公平性研究在哲学领域几个世纪的讨论与近期被应用于机器学习领域之间的鸿沟。本文试图通过形式化一致性公平概念和将哲学思考转化为ADM系统中ML模型训练和评估的形式框架，来架起这一鸿沟。我们指出，不公平性问题可能已经存在，即使没有受保护性属性的存在，强调公平性和预测性能不是不可调和的对立面，而是前者实现的必要条件。我们提出的框架强调将伦理考虑纳入ML管道的所有阶段，从数据收集到最终部署模型的评估。

    A growing body of literature in fairness-aware ML (fairML) aspires to mitigate machine learning (ML)-related unfairness in automated decision making (ADM) by defining metrics that measure fairness of an ML model and by proposing methods that ensure that trained ML models achieve low values in those measures. However, the underlying concept of fairness, i.e., the question of what fairness is, is rarely discussed, leaving a considerable gap between centuries of philosophical discussion and recent adoption of the concept in the ML community. In this work, we try to bridge this gap by formalizing a consistent concept of fairness and by translating the philosophical considerations into a formal framework for the training and evaluation of ML models in ADM systems. We derive that fairness problems can already arise without the presence of protected attributes, pointing out that fairness and predictive performance are not irreconcilable counterparts, but rather that the latter is necessary to
    
[^152]: 检测网络评价的公平性：发现子群差异以提高垃圾信息检测的准确性

    Are Your Reviewers Being Treated Equally? Discovering Subgroup Structures to Improve Fairness in Spam Detection. (arXiv:2204.11164v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.11164](http://arxiv.org/abs/2204.11164)

    本文通过发现组内子群结构，提高同等对待不同组的公正性、减少垃圾信息检测误差，对于提高评论者参与度和用户对评论网站的信任具有积极影响。

    

    用户生成的产品评价是亚马逊和Yelp等在线商业的重要资产，而虚假评论则普遍存在以欺骗消费者。但是，使用图形拓扑的GNN是检测可疑评论者的最先进方法。但是，不同组的评论者检测准确性的差异可能会降低评论者的参与度并损害用户对评论网站的信任。与先前的观点不同，我们研究了组内子群结构，该结构也可能导致处理不同组时出现差异。本文解决了定义、近似和利用新子群结构进行公平垃圾信息检测的挑战。我们首先识别了评审图中导致组内准确性差异的子群结构。评论图中的复杂依赖关系会导致找出子群的难度。

    User-generated reviews of products are vital assets of online commerce, such as Amazon and Yelp, while fake reviews are prevalent to mislead customers. GNN is the state-of-the-art method that detects suspicious reviewers by exploiting the topologies of the graph connecting reviewers, reviews, and target products. However, the discrepancy in the detection accuracy over different groups of reviewers can degrade reviewer engagement and customer trust in the review websites. Unlike the previous belief that the difference between the groups causes unfairness, we study the subgroup structures within the groups that can also cause discrepancies in treating different groups. This paper addresses the challenges of defining, approximating, and utilizing a new subgroup structure for fair spam detection. We first identify subgroup structures in the review graph that lead to discrepant accuracy in the groups. The complex dependencies over the review graph create difficulties in teasing out subgroup
    
[^153]: 微小样本学习中的区间界插值算法

    Interval Bound Interpolation for Few-shot Learning with Few Tasks. (arXiv:2204.03511v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.03511](http://arxiv.org/abs/2204.03511)

    在微小样本学习中引入了区间界概念，通过最小化任务及其相应边界之间的距离来保留训练任务周围的领域，并通过插值来人为形成新任务进行训练。

    

    微小样本学习旨在将通过对多样化任务进行训练所获取的知识转移到具有有限标记数据的相同任务分布中的新任务。实现有效的少次学习泛化的基本前提是学习任务流形的好的表示方法。在仅有受限数量任务的情况下，这变得更加困难。在这种少任务少学习情况下，显式地保留任务流形中的本地邻域并利用其生成训练人工任务可以协助提高性能。为此，我们将完全强韧性训练文献中的区间界概念引入到了微小样本学习中。区间界用于描述训练任务周围的领域。这些邻域可以通过最小化与任务及其相应边界之间的距离来保留。然后利用一种新颖的策略通过插值来人为形成新任务进行训练。

    Few-shot learning aims to transfer the knowledge acquired from training on a diverse set of tasks to unseen tasks from the same task distribution with a limited amount of labeled data. The underlying requirement for effective few-shot generalization is to learn a good representation of the task manifold. This becomes more difficult when only a limited number of tasks are available for training. In such a few-task few-shot setting, it is beneficial to explicitly preserve the local neighborhoods from the task manifold and exploit this to generate artificial tasks for training. To this end, we introduce the notion of interval bounds from the provably robust training literature to few-shot learning. The interval bounds are used to characterize neighborhoods around the training tasks. These neighborhoods can then be preserved by minimizing the distance between a task and its respective bounds. We then use a novel strategy to artificially form new tasks for training by interpolating between 
    
[^154]: 再生粒子汤普森抽样

    Regenerative Particle Thompson Sampling. (arXiv:2203.08082v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.08082](http://arxiv.org/abs/2203.08082)

    本文提出了再生粒子汤普森抽样（RPTS），通过重新生成适应的粒子来解决汤普森抽样中粒子权重收敛于零的问题。RPTS在代表性赌博问题中展现出了灵活性和效果的提升，包括对5G网络切片的应用。

    

    本文提出了再生粒子汤普森抽样（RPTS），这是汤普森抽样的一种灵活变体。汤普森抽样本身是一种贝叶斯启发式算法，用于解决随机赌博机问题，但由于维护连续的后验分布的复杂性，它很难在实践中实现。粒子汤普森抽样（PTS）是汤普森抽样的一种近似方式，通过用离散分布替换在一组加权静态粒子上支持的连续分布来获取。我们观察到在PTS中，除了少数适应的粒子之外，所有其他粒子的权重都收敛于零。RPTS基于启发式方法：删除衰减的不适应粒子，并在适应的幸存粒子附近再生新粒子。实证结果表明，从PTS到RPTS的普遍改进和RPTS在一组代表性赌博问题中的灵活性和功效，包括对5G网络切片的应用。

    This paper proposes regenerative particle Thompson sampling (RPTS), a flexible variation of Thompson sampling. Thompson sampling itself is a Bayesian heuristic for solving stochastic bandit problems, but it is hard to implement in practice due to the intractability of maintaining a continuous posterior distribution. Particle Thompson sampling (PTS) is an approximation of Thompson sampling obtained by simply replacing the continuous distribution by a discrete distribution supported at a set of weighted static particles. We observe that in PTS, the weights of all but a few fit particles converge to zero. RPTS is based on the heuristic: delete the decaying unfit particles and regenerate new particles in the vicinity of fit surviving particles. Empirical evidence shows uniform improvement from PTS to RPTS and flexibility and efficacy of RPTS across a set of representative bandit problems, including an application to 5G network slicing.
    
[^155]: 非线性等距流形学习用于单射归一化流

    Nonlinear Isometric Manifold Learning for Injective Normalizing Flows. (arXiv:2203.03934v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.03934](http://arxiv.org/abs/2203.03934)

    使用等距自编码器的非线性等距流形学习模型可以简化模型选择和调整，并生成高质量的数据。

    

    为了利用归一化流建模流形数据，我们采用等距自编码器来设计具有显式逆的嵌入，以不改变概率分布。使用等距映射将流形学习和密度估计分离，并使两部分的训练达到高精度。因此，与现有单射归一化流相比，模型选择和调整更为简化。应用于（近似）平坦流形的数据集，该组合方法可生成高质量的数据。

    To model manifold data using normalizing flows, we employ isometric autoencoders to design embeddings with explicit inverses that do not distort the probability distribution. Using isometries separates manifold learning and density estimation and enables training of both parts to high accuracy. Thus, model selection and tuning are simplified compared to existing injective normalizing flows. Applied to data sets on (approximately) flat manifolds, the combined approach generates high-quality data.
    
[^156]: 使用可微分超几何分布进行小组重要性学习

    Learning Group Importance using the Differentiable Hypergeometric Distribution. (arXiv:2203.01629v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.01629](http://arxiv.org/abs/2203.01629)

    本文提出了不同iable hypergeometric distribution，使用重要性学习方法来解决在许多应用程序中将一组元素划分为先验未知大小的子集的问题，并在弱监督学习和聚类方面优于以前的方法。

    

    在许多应用程序中，将一组元素划分为先验未知大小的子集是必要的。这些子集大小很少明确学习 - 无论是聚类应用程序中的簇大小还是弱监督学习中的共享与独立生成潜在因素的数量。由于硬性约束条件，正确子集大小的概率分布是不可微分的，这禁止了基于梯度的优化。在这项工作中，我们提出了可微分超几何分布。超几何分布基于它们的相对重要性模拟不同组大小的概率。我们引入可重参数化梯度来学习小组之间的重要性，并强调在两个典型应用程序中显式学习子集大小的优点：弱监督学习和聚类。在这两个应用程序中，我们优于依赖于次优启发式模拟未知大小的先前方法。

    Partitioning a set of elements into subsets of a priori unknown sizes is essential in many applications. These subset sizes are rarely explicitly learned - be it the cluster sizes in clustering applications or the number of shared versus independent generative latent factors in weakly-supervised learning. Probability distributions over correct combinations of subset sizes are non-differentiable due to hard constraints, which prohibit gradient-based optimization. In this work, we propose the differentiable hypergeometric distribution. The hypergeometric distribution models the probability of different group sizes based on their relative importance. We introduce reparameterizable gradients to learn the importance between groups and highlight the advantage of explicitly learning the size of subsets in two typical applications: weakly-supervised learning and clustering. In both applications, we outperform previous approaches, which rely on suboptimal heuristics to model the unknown size of
    
[^157]: 增加神经网络深度进行终身学习的研究

    Increasing Depth of Neural Networks for Life-long Learning. (arXiv:2202.10821v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.10821](http://arxiv.org/abs/2202.10821)

    本研究提出了一种终身学习方法，通过增加神经网络深度，以节点的方式部署不同任务的神经网络参数，实现前向传递和适应以前学过的表示，解决了渐进式神经网络在整个网络分配大量内存的问题。

    

    目的：我们提出了一种基于增加神经网络深度的终身学习方法。本研究探讨了在终身学习环境下扩展神经网络深度是否有益处。方法：我们提出了一种新方法，通过在现有网络上方添加新层来实现知识的前向传递，并适应以前学过的表示。我们采用一种确定最相似任务的方法，以选择网络中添加有可训练参数的新节点的最佳位置。该方法允许创建一种树形模型，其中每个节点是专用于特定任务的神经网络参数集合。所提出的方法受到渐进式神经网络概念的启发。因此，它可以从网络结构的动态变化中获益。然而，渐进式神经网络在学习过程中为整个网络结构分配大量内存。所提出的方法通过仅添加部分来缓解此问题。

    Purpose: We propose a novel method for continual learning based on the increasing depth of neural networks. This work explores whether extending neural network depth may be beneficial in a life-long learning setting.  Methods: We propose a novel approach based on adding new layers on top of existing ones to enable the forward transfer of knowledge and adapting previously learned representations. We employ a method of determining the most similar tasks for selecting the best location in our network to add new nodes with trainable parameters. This approach allows for creating a tree-like model, where each node is a set of neural network parameters dedicated to a specific task. The Progressive Neural Network concept inspires the proposed method. Therefore, it benefits from dynamic changes in network structure. However, Progressive Neural Network allocates a lot of memory for the whole network structure during the learning process. The proposed method alleviates this by adding only part of
    
[^158]: 深度图学习的可靠性近期进展: 内在噪声、分布偏移和对抗攻击

    Recent Advances in Reliable Deep Graph Learning: Inherent Noise, Distribution Shift, and Adversarial Attack. (arXiv:2202.07114v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.07114](http://arxiv.org/abs/2202.07114)

    本文综合评述了深度图学习（DGL）的最新可靠性进展，其中涵盖了内在噪声和分布偏移等方面的相关内容，同时指出未来研究需要关注的问题。

    

    深度图学习 (DGL) 在金融、电子商务、药物和先进材料发现等商业和科学领域取得了显著进展。尽管取得了进展，但将 DGL 应用于实际应用面临一系列可靠性威胁，包括内在噪声、分布偏移和对抗攻击。本文综述了近期针对上述威胁改进 DGL 算法可靠性的全面进展。与之前主要集中在对抗攻击和防御的相关综述不同，我们的综述涵盖了更多关于 DGL 可靠性相关的方面，即内在噪声和分布偏移。此外，我们讨论了以上方面之间的关系，并强调了未来研究中需要探索的一些重要问题。

    Deep graph learning (DGL) has achieved remarkable progress in both business and scientific areas ranging from finance and e-commerce to drug and advanced material discovery. Despite the progress, applying DGL to real-world applications faces a series of reliability threats including inherent noise, distribution shift, and adversarial attacks. This survey aims to provide a comprehensive review of recent advances for improving the reliability of DGL algorithms against the above threats. In contrast to prior related surveys which mainly focus on adversarial attacks and defense, our survey covers more reliability-related aspects of DGL, i.e., inherent noise and distribution shift. Additionally, we discuss the relationships among above aspects and highlight some important issues to be explored in future research.
    
[^159]: 基于密集卷积神经网络的胸部疾病多标签分类方法

    Multi-Label Classification of Thoracic Diseases using Dense Convolutional Network on Chest Radiographs. (arXiv:2202.03583v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2202.03583](http://arxiv.org/abs/2202.03583)

    本研究提出了一种基于密集卷积神经网络和GRADCAM的胸部X光疾病多标签诊断模型，获得了在Cardiomegaly条件下最高的AUC得分0.896，并使用热图提高了模型的可解释性。

    

    传统的X光图像病理识别方法依赖于熟练的人类解释，并且往往耗时。深度学习技术的出现使自动诊断系统的开发成为可能，但这类系统的表现取决于模型的质量和它提供的可解释性水平。本文提出了一种使用密集卷积神经网络（DenseNet）和GRADCAM进行模型可解释性的胸部X光疾病多标签诊断模型。我们使用前置X光训练了我们的模型，并使用各种定量指标（包括受试者操作特征曲线下面积（AUC））评估了模型的性能。我们的模型在Cardiomegaly条件下达到了最高的AUC得分0.896，并获得了0.826的准确度。而在Nodule条件下获得了最低的AUC得分0.655，准确度为0.66。为了提高模型可解释性，并在决策方面建立信任，我们使用GRADCAM生成了热图，突出显示了对诊断最重要的X光区域。

    Traditional methods of identifying pathologies in X-ray images rely heavily on skilled human interpretation and are often time-consuming. The advent of deep learning techniques has enabled the development of automated disease diagnosis systems, but the performance of such systems is dependent on the quality of the model and the level of interpretability it provides. In this paper, we propose a multi-label disease diagnosis model for chest X-rays using a dense convolutional neural network (DenseNet) and model interpretability using GRADCAM. We trained our model using frontal X-rays and evaluated its performance using various quantitative metrics, including the area under the receiver operating characteristic curve (AUC). Our proposed model achieved the highest AUC score of 0.896 for the condition Cardiomegaly with an accuracy of 0.826, while the lowest AUC score was obtained for Nodule, at 0.655 with an accuracy of 0.66. To promote model interpretability and build trust in decision maki
    
[^160]: TPC：针对点云模型的特定转换平滑方法。

    TPC: Transformation-Specific Smoothing for Point Cloud Models. (arXiv:2201.12733v5 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2201.12733](http://arxiv.org/abs/2201.12733)

    提出了一种名为TPC的转换特定平滑框架，可以提供点云模型对语义转换攻击的可扩展防御能力，并在常见数据集上进行了有效实验。

    

    神经网络架构的点云模型在安全关键的应用中取得了巨大成功，如自动驾驶车辆中基于激光雷达的识别系统。然而，这些模型容易受到对抗性攻击，旨在施加旋转和锥形等隐秘的语义变换来误导模型预测。在本文中，我们提出了一种基于转换特定平滑的框架TPC，为点云模型提供紧密且可扩展的防御保证，抵御语义转换攻击。我们首先将常见的3D转换分为三类：可叠加的（例如剪切）、可组合的（例如旋转）和间接可组合的（如锥形），然后分别提供了通用的鲁棒性认证策略。接着，我们针对一系列特定的语义转换和它们的组合，指定了唯一的认证协议。在几个常见的物体识别数据集上进行了大量实验证明，TPC可以在提供更强的防御能力的同时，降低点云模型性能损失。

    Point cloud models with neural network architectures have achieved great success and have been widely used in safety-critical applications, such as Lidar-based recognition systems in autonomous vehicles. However, such models are shown vulnerable to adversarial attacks which aim to apply stealthy semantic transformations such as rotation and tapering to mislead model predictions. In this paper, we propose a transformation-specific smoothing framework TPC, which provides tight and scalable robustness guarantees for point cloud models against semantic transformation attacks. We first categorize common 3D transformations into three categories: additive (e.g., shearing), composable (e.g., rotation), and indirectly composable (e.g., tapering), and we present generic robustness certification strategies for all categories respectively. We then specify unique certification protocols for a range of specific semantic transformations and their compositions. Extensive experiments on several common 
    
[^161]: 有限记忆下的通用图协作学习：复杂度、可学习性和可靠性分析

    Collaborative Learning in General Graphs with Limited Memorization: Complexity, Learnability, and Reliability. (arXiv:2201.12482v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.12482](http://arxiv.org/abs/2201.12482)

    本文提出了一个三阶段的轻量级随机游走算法，在有限的记忆和通讯带宽限制下完成了通用图上的协作学习，解决了连接图不完全和非良构的问题，并提高了学习过程的可靠性。

    

    本文考虑在任意连接图中有限记忆和通信带宽的智能体完成 K-臂赌博机问题。现有的研究通常假设通信图必须是完全或良构的，然而这在实践中并不总是成立。而有限的记忆和通信带宽也会限制已有经验的共享，甚至可能导致智能体向其他智能体传递虚假的经验信息，极大地限制了学习过程的可靠性。针对以上问题，我们提出了一个包含三个阶段的协作学习算法，每个阶段通过轻量级的随机游走实现最新经验的分享。

    We consider a K-armed bandit problem in general graphs where agents are arbitrarily connected and each of them has limited memorizing capabilities and communication bandwidth. The goal is to let each of the agents eventually learn the best arm. It is assumed in these studies that the communication graph should be complete or well-structured, whereas such an assumption is not always valid in practice. Furthermore, limited memorization and communication bandwidth also restrict the collaborations of the agents, since the agents memorize and communicate very few experiences. Additionally, an agent may be corrupted to share falsified experiences to its peers, while the resource limit in terms of memorization and communication may considerably restrict the reliability of the learning process. To address the above issues, we propose a three-staged collaborative learning algorithm. In each step, the agents share their latest experiences with each other through light-weight random walks in a ge
    
[^162]: 两时间尺度更新规则训练生成式对抗网络中的关键批次大小的存在和估计

    Existence and Estimation of Critical Batch Size for Training Generative Adversarial Networks with Two Time-Scale Update Rule. (arXiv:2201.11989v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.11989](http://arxiv.org/abs/2201.11989)

    本文研究了使用两时间尺度更新规则（TTUR）训练生成式对抗网络（GAN）时批次大小与训练所需步骤数量之间的关系，理论上证明了为了找到稳定点，随着批次大小的增加所需步骤数量会减少并且存在一个最小化随机一阶预言机（SFO）复杂度的关键批次大小。

    

    先前的研究表明，在理论和实践中，使用不同的学习率，如不同的恒定率或不同的衰减率等，使用两时间尺度更新规则（TTUR）有助于训练生成式对抗网络（GAN）。此外，批次大小对于使用TTUR训练GANs也很重要，两者都影响了训练所需的步骤数量。本文基于恒定学习率研究了批次大小与使用TTUR训练GANs所需步骤数量之间的关系。我们理论上表明，对于具有恒定学习率的TTUR，为了找到鉴别器和生成器损失函数的稳定点，所需步骤数随着批次大小的增加而减少，并且存在一个最小化随机一阶预言机（SFO）复杂度的关键批次大小。然后，我们使用Fr'echet Inception Distance（FID）作为训练的性能测量，并提供了...

    Previous results have shown that a two time-scale update rule (TTUR) using different learning rates, such as different constant rates or different decaying rates, is useful for training generative adversarial networks (GANs) in theory and in practice. Moreover, not only the learning rate but also the batch size is important for training GANs with TTURs and they both affect the number of steps needed for training. This paper studies the relationship between batch size and the number of steps needed for training GANs with TTURs based on constant learning rates. We theoretically show that, for a TTUR with constant learning rates, the number of steps needed to find stationary points of the loss functions of both the discriminator and generator decreases as the batch size increases and that there exists a critical batch size minimizing the stochastic first-order oracle (SFO) complexity. Then, we use the Fr'echet inception distance (FID) as the performance measure for training and provide nu
    
[^163]: 编码保护分类属性的公平性影响。

    Fairness Implications of Encoding Protected Categorical Attributes. (arXiv:2201.11358v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.11358](http://arxiv.org/abs/2201.11358)

    该研究比较了两种常用的编码方法-“one-hot编码”和“target编码”，并探讨了其对机器学习模型性能和公平性的影响。

    

    过去的研究表明，在机器学习中明确使用保护属性可以同时提高性能和公平性。但是，许多机器学习算法无法直接处理分类属性，例如出生国家或种族。由于保护属性经常是分类的，因此必须将其编码为可以输入所选择的机器学习算法的特征，例如支持向量机、梯度提升决策树或线性模型。编码方法影响机器学习算法将学习如何和什么，影响模型的性能和公平性。该研究比较了两种最著名的编码方法——“one-hot编码”和“target编码”的准确性和公平性影响。我们区分了这些编码方法可能产生的两种诱导偏差类型，这可能导致不公平的模型。第一种类型是无法消除的偏差，由于直接组别类别歧视而导致。

    Past research has demonstrated that the explicit use of protected attributes in machine learning can improve both performance and fairness. Many machine learning algorithms, however, cannot directly process categorical attributes, such as country of birth or ethnicity. Because protected attributes frequently are categorical, they must be encoded as features that can be input to a chosen machine learning algorithm, e.g.\ support vector machines, gradient boosting decision trees or linear models. Thereby, encoding methods influence how and what the machine learning algorithm will learn, affecting model performance and fairness. This work compares the accuracy and fairness implications of the two most well-known encoding methods: \emph{one-hot encoding} and \emph{target encoding}. We distinguish between two types of induced bias that may arise from these encoding methods and may lead to unfair models. The first type, \textit{irreducible bias}, is due to direct group category discriminatio
    
[^164]: 应用神经网络求解三维轴对称欧拉方程的渐近自相似瞬时爆破轮廓

    Asymptotic self-similar blow-up profile for three-dimensional axisymmetric Euler equations using neural networks. (arXiv:2201.06780v3 [math.AP] UPDATED)

    [http://arxiv.org/abs/2201.06780](http://arxiv.org/abs/2201.06780)

    本文运用物理信息神经网络（PINNs）首次发现二维布式内斯克方程和三维欧拉方程的光滑自相似爆破轮廓，这可以成为未来进行两个方程爆破的计算机辅助证明的基础。同时，我们的数值框架还可以成功地应用于求解流体方程的不稳定自相似解。

    

    对于二维布式内斯克方程和三维欧拉方程是否存在有限时间爆破解是流体力学领域的基本问题。本文运用物理信息神经网络（PINNs）开发了一种新的数值框架，首次发现了两个方程的光滑自相似爆破轮廓。这些解本身可以成为未来进行两个方程爆破的计算机辅助证明的基础。此外，我们还构建了第一个不稳定的自相似解的示例，证明了 PINNs 可以成功地应用于求解流体方程的不稳定自相似解。我们还展示了我们的数值框架既强健又适应各种不同的方程。

    Whether there exist finite time blow-up solutions for the 2-D Boussinesq and the 3-D Euler equations are of fundamental importance to the field of fluid mechanics. We develop a new numerical framework, employing physics-informed neural networks (PINNs), that discover, for the first time, a smooth self-similar blow-up profile for both equations. The solution itself could form the basis of a future computer-assisted proof of blow-up for both equations. In addition, we demonstrate PINNs could be successfully applied to find unstable self-similar solutions to fluid equations by constructing the first example of an unstable self-similar solution to the C\'ordoba-C\'ordoba-Fontelos equation. We show that our numerical framework is both robust and adaptable to various other equations.
    
[^165]: CausalSim: 一种用于无偏差追踪驱动仿真的因果框架

    CausalSim: A Causal Framework for Unbiased Trace-Driven Simulation. (arXiv:2201.01811v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.01811](http://arxiv.org/abs/2201.01811)

    CausalSim提出了一种因果框架，通过学习系统动态和潜在因素的因果模型，消除追踪数据中的偏差，解决了当前追踪驱动仿真器的缺陷。

    

    我们提出了CausalSim，一种用于无偏差追踪驱动仿真的因果框架。当前的追踪驱动仿真器假设进行仿真的干预（例如，新算法）不会影响追踪的有效性。然而，现实世界中的追踪常常会受到算法在追踪收集期间进行选择的影响，因此，在干预下重演追踪可能会导致不正确的结果。CausalSim通过学习系统动态和捕获追踪收集期间基础系统条件的潜在因素的因果模型来解决这个挑战。它使用固定算法集下的初始随机对照试验（RCT）来学习这些模型，然后在模拟新算法时应用它们来消除追踪数据中的偏差。

    We present CausalSim, a causal framework for unbiased trace-driven simulation. Current trace-driven simulators assume that the interventions being simulated (e.g., a new algorithm) would not affect the validity of the traces. However, real-world traces are often biased by the choices algorithms make during trace collection, and hence replaying traces under an intervention may lead to incorrect results. CausalSim addresses this challenge by learning a causal model of the system dynamics and latent factors capturing the underlying system conditions during trace collection. It learns these models using an initial randomized control trial (RCT) under a fixed set of algorithms, and then applies them to remove biases from trace data when simulating new algorithms.  Key to CausalSim is mapping unbiased trace-driven simulation to a tensor completion problem with extremely sparse observations. By exploiting a basic distributional invariance property present in RCT data, CausalSim enables a nove
    
[^166]: 神经网络普适性的统一建构框架

    A Unified and Constructive Framework for the Universality of Neural Networks. (arXiv:2112.14877v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.14877](http://arxiv.org/abs/2112.14877)

    本论文提出了神经网络普适性的建构框架，任何nAI激活函数都是普适的，该框架具有统一、构造性和新视角的优势。

    

    神经网络之所以能够复制复杂的任务或函数之一是因为它们的普适性。虽然过去几十年来神经网络理论取得了巨大的进展，但尚未提供单一的建构框架来解释神经网络的普适性。本文是第一个为大多数已有激活函数提供统一的建构框架以解释它们的普适性的尝试。在框架的核心是神经网络近似恒等（nAI）的概念。主要的结果是：\emph{任何nAI激活函数都是普适的}。事实证明，大多数激活函数都是nAI，因此在紧致空间连续函数空间内是普适的。该框架比现有的对应物具有\textbf{几个优势}。首先，它是建立在从功能分析、概率论和数值分析的基本手段之上的构造性框架。其次，它是第一个适用于包括大多数已有激活函数在内的大类激活函数的统一框架。第三，它提出了神经网络普适性的新视角。

    One of the reasons why many neural networks are capable of replicating complicated tasks or functions is their universal property. Though the past few decades have seen tremendous advances in theories of neural networks, a single constructive framework for neural network universality remains unavailable. This paper is the first effort to provide a unified and constructive framework for the universality of a large class of activation functions including most of existing ones. At the heart of the framework is the concept of neural network approximate identity (nAI). The main result is: {\em any nAI activation function is universal}. It turns out that most of existing activation functions are nAI, and thus universal in the space of continuous functions on compacta. The framework induces {\bf several advantages} over the contemporary counterparts. First, it is constructive with elementary means from functional analysis, probability theory, and numerical analysis. Second, it is the first un
    
[^167]: 学习未知离散时间线性系统的安全滤波器

    Learning Safety Filters for Unknown Discrete-Time Linear Systems. (arXiv:2111.00631v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.00631](http://arxiv.org/abs/2111.00631)

    本论文提出了一种基于学习的安全滤波器，针对带有未知模型和未知协方差的高斯噪声的离散时间线性时不变系统，通过收紧安全约束和构建鲁棒优化问题，以最小程度地修改名义控制动作，以高概率确保安全性。

    

    本文针对带有未知模型和未知协方差的高斯噪声的离散时间线性时不变系统开发了一种基于学习的安全滤波器。安全性通过对状态和控制输入施加多面体约束来描述。经验性地学习模型和过程噪声协方差及其置信区间，构建了一个鲁棒优化问题，以最小程度地修改名义控制动作，以高概率确保安全性。优化问题依赖于收紧原始的安全性约束。由于最初缺乏可靠模型构建所需信息，因此收紧的幅度较大，但随着更多数据的可用性，其逐渐缩小。

    A learning-based safety filter is developed for discrete-time linear time-invariant systems with unknown models subject to Gaussian noises with unknown covariance. Safety is characterized using polytopic constraints on the states and control inputs. The empirically learned model and process noise covariance with their confidence bounds are used to construct a robust optimization problem for minimally modifying nominal control actions to ensure safety with high probability. The optimization problem relies on tightening the original safety constraints. The magnitude of the tightening is larger at the beginning since there is little information to construct reliable models, but shrinks with time as more data becomes available.
    
[^168]: 最小二乘神经网络方法求解非线性双曲型守恒律：离散散度算子

    Least-Squares Neural Network (LSNN) Method For Scalar Nonlinear Hyperbolic Conservation Laws: Discrete Divergence Operator. (arXiv:2110.10895v3 [math.NA] UPDATED)

    [http://arxiv.org/abs/2110.10895](http://arxiv.org/abs/2110.10895)

    本文提出了一种最小二乘神经网络方法，用于求解双曲型守恒律，该方法使用离散散度算子，可用于逼近未知界面位置的不连续函数，并在保持不连续性解和实现高精度和高效率方面具有很好的效果。

    

    本文介绍了一种最小二乘神经网络（LSNN）方法，用于求解标量线性和非线性双曲型守恒律（HCLs），该方法基于等价的最小二乘（LS）公式，并使用修正线性单元（ReLU）神经网络作为逼近函数，可用于逼近未知界面位置的不连续函数。在LSNN方法设计中，微分运算的数值逼近是一个关键因素，标准的数值或自动微分沿坐标轴方向进行，往往会导致基于神经网络的方法失败。为了克服这一挑战，本文将HCLs转化为时空散度形式，并引入一个新的离散散度算子，从而使所提出的LSNN方法不受人工粘性惩罚。理论上，即使是不连续的解，也可以估计离散散度算子的精度。数值上，在一组基准问题中测试了带有新的离散散度算子的LSNN方法，包括一个具有运动不连续性的问题。结果表明，具有离散散度算子的LSNN方法在保持不连续性解和实现高精度和高效率方面具有很好的效果。

    A least-squares neural network (LSNN) method was introduced for solving scalar linear and nonlinear hyperbolic conservation laws (HCLs) in [7, 6]. This method is based on an equivalent least-squares (LS) formulation and uses ReLU neural network as approximating functions, making it ideal for approximating discontinuous functions with unknown interface location. In the design of the LSNN method for HCLs, the numerical approximation of differential operators is a critical factor, and standard numerical or automatic differentiation along coordinate directions can often lead to a failed NN-based method. To overcome this challenge, this paper rewrites HCLs in their divergence form of space and time and introduces a new discrete divergence operator. As a result, the proposed LSNN method is free of penalization of artificial viscosity. Theoretically, the accuracy of the discrete divergence operator is estimated even for discontinuous solutions. Numerically, the LSNN method with the new discre
    
[^169]: 签名和相关性学习

    Sign and Relevance learning. (arXiv:2110.07292v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.07292](http://arxiv.org/abs/2110.07292)

    本文提出了一种新型网络，在整个网络中仅传播可塑性变化的符号，同时使用神经调制控制学习速率。研究结果表明该范例可以成功学习具有多层表示的复杂任务，学习速度和稳定性优于标准模型。

    

    传统的生物仿真和生物启发式强化学习模型应用全局误差信号，这意味着使用浅层网络。然而，误差反向传播允许使用多层网络。本研究引入了一种新型网络，通过在整个网络中仅传播可塑性变化的符号（即LTP / LTD），同时使用神经调制来控制学习速率来解决这个问题。神经调制可以被理解为一个修正的误差或相关信号，而错误信号的自上而下符号决定长期增强还是长期抑制将发生。为了证明这种方法的有效性，我们进行了一个真实的机器人任务作为概念验证。我们的结果表明，该范例可以成功学习具有多层表示的复杂任务，并在学习速度和稳定性方面优于标准模型。

    Standard models of biologically realistic or biologically inspired reinforcement learning employ a global error signal, which implies the use of shallow networks. On the other hand, error backpropagation allows the use of networks with multiple layers. However, precise error backpropagation is difficult to justify in biologically realistic networks because it requires precise weighted error backpropagation from layer to layer. In this study, we introduce a novel network that solves this problem by propagating only the sign of the plasticity change (i.e., LTP/LTD) throughout the whole network, while neuromodulation controls the learning rate. Neuromodulation can be understood as a rectified error or relevance signal, while the top-down sign of the error signal determines whether long-term potentiation or long-term depression will occur. To demonstrate the effectiveness of this approach, we conducted a real robotic task as proof of concept. Our results show that this paradigm can success
    
[^170]: GRAPE: 用于快速可扩展图处理和基于随机游走的嵌入的软件资源

    GRAPE for Fast and Scalable Graph Processing and random walk-based Embedding. (arXiv:2110.06196v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.06196](http://arxiv.org/abs/2110.06196)

    GRAPE是一种软件资源，用于处理大型图，并利用专业和智能的数据结构、算法和快速并行实现的随机游走方法，实现了比现有最先进的软件资源更高的时间和空间复杂度，具有竞争性的节点标签预测性能。

    

    图表示学习方法为解决由图表示的复杂实际问题打开了新的途径。然而，这些应用中使用的图中包含数百万个节点和数十亿个边，已经超出了当前方法和软件实现的能力。我们提出了GRAPE，这是一个用于图处理和嵌入的软件资源，可以使用专业而智能的数据结构、算法和快速并行实现的随机游走方法来扩展大型图。与现有最先进的软件资源相比，GRAPE在经验空间和时间复杂度上都显示出数量级的改进，以及竞争性的节点标签预测性能。GRAPE包括约170万行Python和Rust代码，提供69种节点嵌入方法、25种推断模型、一系列高效的图处理工具和超过80,000个来自文献和其他来源的图。

    Graph Representation Learning (GRL) methods opened new avenues for addressing complex, real-world problems represented by graphs. However, many graphs used in these applications comprise millions of nodes and billions of edges and are beyond the capabilities of current methods and software implementations. We present GRAPE, a software resource for graph processing and embedding that can scale with big graphs by using specialized and smart data structures, algorithms, and a fast parallel implementation of random walk-based methods. Compared with state-of-the-art software resources, GRAPE shows an improvement of orders of magnitude in empirical space and time complexity, as well as a competitive edge and node label prediction performance. GRAPE comprises about 1.7 million well-documented lines of Python and Rust code and provides 69 node embedding methods, 25 inference models, a collection of efficient graph processing utilities and over 80,000 graphs from the literature and other source
    
[^171]: 基于价值函数的序列最小化方法在双层优化中的应用

    Value-Function-based Sequential Minimization for Bi-level Optimization. (arXiv:2110.04974v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.04974](http://arxiv.org/abs/2110.04974)

    本文提出了一种新的双层优化算法，叫做基于价值函数的序列最小化方法（BVFSM），通过将BLO重构为近似的单层问题，避免了现有方法所需的重复计算循环梯度和Hessian逆的时间消耗，特别适用于高维任务以及具有额外功能约束的BLO。

    

    基于梯度的双层优化（BLO）方法已被广泛用于处理现代学习任务。然而，大多数现有的策略都是基于苛刻的假设（例如，下一层子问题的凸性），并且对于高维任务计算上不可行。此外，在那些具有功能约束和悲观BLO等挑战性场景中，几乎没有基于梯度的方法能够解决BLO。本文通过将BLO重构为近似的单层问题，提供了一种名为双层价值函数序列最小化（BVFSM）的新算法以解决上述问题。具体而言，BVFSM构建了一系列基于价值函数的近似，从而避免了现有方法所需的重复计算循环梯度和Hessian逆的时间消耗，特别适用于高维任务。我们还扩展了BVFSM以解决具有额外功能约束的BLO。

    Gradient-based Bi-Level Optimization (BLO) methods have been widely applied to handle modern learning tasks. However, most existing strategies are theoretically designed based on restrictive assumptions (e.g., convexity of the lower-level sub-problem), and computationally not applicable for high-dimensional tasks. Moreover, there are almost no gradient-based methods able to solve BLO in those challenging scenarios, such as BLO with functional constraints and pessimistic BLO. In this work, by reformulating BLO into approximated single-level problems, we provide a new algorithm, named Bi-level Value-Function-based Sequential Minimization (BVFSM), to address the above issues. Specifically, BVFSM constructs a series of value-function-based approximations, and thus avoids repeated calculations of recurrent gradient and Hessian inverse required by existing approaches, time-consuming especially for high-dimensional tasks. We also extend BVFSM to address BLO with additional functional constrai
    
[^172]: 使用RBF神经网络优化恒流并联微型泵

    The Optimization of the Constant Flow Parallel Micropump Using RBF Neural Network. (arXiv:2109.08717v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.08717](http://arxiv.org/abs/2109.08717)

    本文通过实施RBF神经网络，提出了重叠时间的概念来优化恒流并联机械位移微型泵，在将左右泵互换角色的往复运动期间最小化压力脉冲。

    

    本文旨在优化具有并联泵腔和被动止回阀的恒流并联机械位移微型泵的性能。关键任务是在左右泵互换吸入和输送角色时的往复运动期间，将由反流引起的压力脉冲最小化，这对稳定的恒流率产生负面影响。以往的工作尝试通过被动止回阀的机械设计来解决这个问题。本文提出了重叠时间的新概念，并通过实施RBF神经网络来实现从控制理论角度解决这个问题，同时使用无监督学习和监督学习对其进行了训练。实验结果表明，压力脉冲在0.15-0.25 MPa的范围内得到了优化，相比于最大泵工作压力40 MPa，这是一个重大的改进。

    The objective of this work is to optimize the performance of a constant flow parallel mechanical displacement micropump, which has parallel pump chambers and incorporates passive check valves. The critical task is to minimize the pressure pulse caused by regurgitation, which negatively impacts the constant flow rate, during the reciprocating motion when the left and right pumps interchange their role of aspiration and transfusion. Previous works attempt to solve this issue via the mechanical design of passive check valves. In this work, the novel concept of overlap time is proposed, and the issue is solved from the aspect of control theory by implementing a RBF neural network trained by both unsupervised and supervised learning. The experimental results indicate that the pressure pulse is optimized in the range of 0.15 - 0.25 MPa, which is a significant improvement compared to the maximum pump working pressure of 40 MPa.
    
[^173]: 应用于机器人布料操作的受控高斯过程动力学模型

    Controlled Gaussian Process Dynamical Models with Application to Robotic Cloth Manipulation. (arXiv:2103.06615v5 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2103.06615](http://arxiv.org/abs/2103.06615)

    本文提出了一种称为受控高斯过程动力学模型（CGPDM）的方法，用于学习高维非线性动态。该模型将高维状态空间投影到较小的维度潜在空间中。CGPDM由一个低维潜在空间组成，具有相关的动态，外部控制变量可以作用于该空间，并映射到观测空间。

    

    近年来，机器人操纵方面取得了显着进展，但处理非刚性物体（如布料）仍然是一个悬而未决的问题。与非刚性物体进行物理交互是不确定和复杂的建模。因此，从样本数据中提取有用信息可以极大地提高建模性能。然而，由于状态表示的高维性，这种模型的训练是一项具有挑战性的任务。在本文中，我们提出了受控高斯过程动力学模型（CGPDM），用于通过将其嵌入到低维流形中学习高维非线性动态。 CGPDM由一个低维潜在空间组成，具有相关的动态，外部控制变量可以作用于该空间，并映射到观测空间。考虑高斯过程（GP）先验分布来边缘化两个映射的参数。因此，CGPDM将高维状态空间投影到较小的维度潜在空间中。

    Over the last years, significant advances have been made in robotic manipulation, but still, the handling of non-rigid objects, such as cloth garments, is an open problem. Physical interaction with non-rigid objects is uncertain and complex to model. Thus, extracting useful information from sample data can considerably improve modeling performance. However, the training of such models is a challenging task due to the high-dimensionality of the state representation. In this paper, we propose Controlled Gaussian Process Dynamical Model (CGPDM) for learning high-dimensional, nonlinear dynamics by embedding it in a low-dimensional manifold. A CGPDM is constituted by a low-dimensional latent space, with an associated dynamics where external control variables can act and a mapping to the observation space. The parameters of both maps are marginalized out by considering Gaussian Process (GP) priors. Hence, a CGPDM projects a high-dimensional state space into a smaller dimension latent space, 
    
[^174]: 逐步分析强化学习

    Reinforcement Learning, Bit by Bit. (arXiv:2103.04047v8 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.04047](http://arxiv.org/abs/2103.04047)

    该论文讨论了强化学习代理在实际环境中的数据效率问题，并提供了原则性指导和计算结果。

    

    强化学习代理在模拟环境中取得了惊人的成就。数据效率是将这种成功应用于实际环境的障碍。数据高效代理的设计需要更深入地了解信息获取和表示。我们讨论了概念和遗憾分析，共同提供了原则性指导。这种思路揭示了关于寻求什么信息、如何寻求该信息以及保留哪些信息的问题。为了阐明这些概念，我们设计了简单的代理，并展示了突出的数据效率的计算结果。

    Reinforcement learning agents have demonstrated remarkable achievements in simulated environments. Data efficiency poses an impediment to carrying this success over to real environments. The design of data-efficient agents calls for a deeper understanding of information acquisition and representation. We discuss concepts and regret analysis that together offer principled guidance. This line of thinking sheds light on questions of what information to seek, how to seek that information, and what information to retain. To illustrate concepts, we design simple agents that build on them and present computational results that highlight data efficiency.
    
[^175]: 用于在线学习非平稳函数的连续高斯过程

    Sequential Gaussian Processes for Online Learning of Nonstationary Functions. (arXiv:1905.10003v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1905.10003](http://arxiv.org/abs/1905.10003)

    本文提出了一种基于顺序蒙特卡罗算法的连续高斯过程模型，以解决高斯过程模型的计算复杂度高，难以在线顺序更新的问题，同时允许拟合具有非平稳性质的函数。方法优于现有最先进方法的性能。

    

    许多机器学习问题可以在估计函数的上下文中得到解决，通常这些函数是时间相关的函数，并且是实时地随着观测的到来而估计的。高斯过程是建模实值非线性函数的一个有吸引力的选择，由于其灵活性和不确定性量化。然而，典型的高斯过程回归模型存在若干不足：1）传统高斯过程推断的复杂度$O(N^{3})$随着观测值的个数N成增长；2）逐步更新高斯过程模型不容易；3）协方差核通常对函数施加平稳性约束，而具有非平稳协方差核的高斯过程通常难以在实践中使用。为了克服这些问题，我们提出了一个顺序蒙特卡罗算法来拟合无限混合高斯过程，以捕捉非平稳行为，同时允许在线、分布推断。我们的方法在实验中优于现有最先进方法的性能。

    Many machine learning problems can be framed in the context of estimating functions, and often these are time-dependent functions that are estimated in real-time as observations arrive. Gaussian processes (GPs) are an attractive choice for modeling real-valued nonlinear functions due to their flexibility and uncertainty quantification. However, the typical GP regression model suffers from several drawbacks: 1) Conventional GP inference scales $O(N^{3})$ with respect to the number of observations; 2) Updating a GP model sequentially is not trivial; and 3) Covariance kernels typically enforce stationarity constraints on the function, while GPs with non-stationary covariance kernels are often intractable to use in practice. To overcome these issues, we propose a sequential Monte Carlo algorithm to fit infinite mixtures of GPs that capture non-stationary behavior while allowing for online, distributed inference. Our approach empirically improves performance over state-of-the-art methods fo
    

