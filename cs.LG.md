# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Photo-zSNthesis: Converting Type Ia Supernova Lightcurves to Redshift Estimates via Deep Learning.](http://arxiv.org/abs/2305.11869) | 本研究提出了一种基于卷积神经网络的方法"Photo-zSNthesis"，用于从多波段超新星光变曲线预测完整的红移概率分布，无需光谱信息。该方法在模拟和真实观测中都取得了重大提升，能极大地约束宇宙学。 |
| [^2] | [Q-malizing flow and infinitesimal density ratio estimation.](http://arxiv.org/abs/2305.11857) | 研究提出了一种可以从一个数据分布P传输到任意访问通过有限样本的Q的流模型。这个模型通过神经ODE模型进行，可以进行无穷小DRE。 |
| [^3] | [Multimodal Web Navigation with Instruction-Finetuned Foundation Models.](http://arxiv.org/abs/2305.11854) | 本文研究使用视觉语言基础模型进行数据驱动离线训练的 Web 代理，提出了一个指令跟随多模态代理WebGUM，将微调指令微调语言模型和视觉转换器，能够有效提高代理的基于视觉感知、HTML 理解和多步推理的能力。 |
| [^4] | [Any-to-Any Generation via Composable Diffusion.](http://arxiv.org/abs/2305.11846) | CoDi是一种生成模型，通过在输入空间和输出空间中进行模态对齐，实现了可组合生成策略，从而可以生成任意组合的输出模态。 CoDi 非常灵活，能够生成多个模态，如图像、视频、语言和音频，甚至在训练数据中不存在的模态组合。 |
| [^5] | [AI's Regimes of Representation: A Community-centered Study of Text-to-Image Models in South Asia.](http://arxiv.org/abs/2305.11844) | 本文探讨南亚文化背景下文字到图像模型的文化限制，并将其归因于全球和地区权力不平等所塑造的外来视角。通过将社区视为专家，对T2I的限制进行研究，深入了解文化特定的AI技术在非西方和南球地区上失败的原因。建议负责任地开发T2I模型，以允许对结构性不平等性的认识。 |
| [^6] | [Complexity of Neural Network Training and ETR: Extensions with Effectively Continuous Functions.](http://arxiv.org/abs/2305.11833) | 本文研究了神经网络训练问题的复杂性，证明了sigmoid激活函数与实数存在性理论和指数函数相关，这使得神经网络在使用sigmoid激活函数时算法可解性存在疑问。同时，使用正弦激活函数时训练问题是不可判定的。 |
| [^7] | [Improving Multimodal Joint Variational Autoencoders through Normalizing Flows and Correlation Analysis.](http://arxiv.org/abs/2305.11832) | 本文提出了一种改进的多模态联合变分自编码器，利用正则化流和相关性分析技术，实现了更加连贯的跨模态生成，更多样化的数据生成，同时可扩展到任意数量的模态。 |
| [^8] | [Regularization of Soft Actor-Critic Algorithms with Automatic Temperature Adjustment.](http://arxiv.org/abs/2305.11831) | 本文提出了正则化自动温度调整的软性演员评论算法，增加了对原理的明确性。 |
| [^9] | [On the Fairness Impacts of Private Ensembles Models.](http://arxiv.org/abs/2305.11807) | 本文探讨了私有教师集成（PATE）模型是否会导致不公平性，并证明它可能导致不同群体之间的准确性差异。建议在PATE的应用中加入公平性考虑，以减少不公平性的影响。 |
| [^10] | [PANNA 2.0: Efficient neural network interatomic potentials and new architectures.](http://arxiv.org/abs/2305.11805) | PANNA 2.0是一个基于神经网络的原子间势生成代码，具有更好的GPU支持和自定义工具，并通过变分电荷均衡方案解决了长程静电相互作用问题。 |
| [^11] | [The probability flow ODE is provably fast.](http://arxiv.org/abs/2305.11798) | 首次提供概率流ODE实现得到多项式时间收敛保证的证明，使用欠阻尼Langevin扩散的特殊选择的校正步骤，获得了更好的维度依赖性，凸显了ODE框架的潜在优势。 |
| [^12] | [Implicit Bias of Gradient Descent for Logistic Regression at the Edge of Stability.](http://arxiv.org/abs/2305.11788) | 本文研究了逻辑回归常数步长梯度下降在稳定性边缘的收敛性和隐式偏差，证明了逻辑损失可以通过任何常数步长的梯度下降进行最小化，同时也发现了指数损失下的发散性问题，强调了稳定性边缘下梯度下降的不稳定性。 |
| [^13] | [Cross-Lingual Supervision improves Large Language Models Pre-training.](http://arxiv.org/abs/2305.11778) | 本文表明，将大型语言模型的预训练中使用跨语种并行数据能提高其上下文学习能力；同时，提出了一种简单且有效的策略来学习两个目标之间的最佳混合比例。 |
| [^14] | [Multi-Objective Optimization Using the R2 Utility.](http://arxiv.org/abs/2305.11774) | 本文提出将多目标优化问题转化为一组单目标问题进行解决，并介绍了R2效用函数作为适当的目标函数。该效用函数单调且次模，可以使用贪心优化算法计算全局最优解。 |
| [^15] | [Transfer operators on graphs: Spectral clustering and beyond.](http://arxiv.org/abs/2305.11766) | 本文介绍了在图上定义的转移算子及其谱特性，提出了基于广义转移算子的有向图聚类算法，并证明了算法有效性。 |
| [^16] | [Tester-Learners for Halfspaces: Universal Algorithms.](http://arxiv.org/abs/2305.11765) | 本文提出了第一个成功的通用半空间测试学习器，可以在广泛结构化的分布上工作，实现误差$ O（\mathrm {opt}）+\ \epsilon $。 |
| [^17] | [Marginalized Beam Search Algorithms for Hierarchical HMMs.](http://arxiv.org/abs/2305.11752) | 本文提出了两种新算法，分别是贪心边缘束搜索算法和局部焦点束搜索算法，用于解决应用于层次隐藏马尔可夫模型时维特比算法和束搜索算法的局限性，它们能够更好地近似最可能的外部状态序列。 |
| [^18] | [MedLens: Improve mortality prediction via medical signs selecting and regression interpolation.](http://arxiv.org/abs/2305.11742) | 本文介绍了一个自动选择医学体征和回归插值的方法（MedLens），用于解决电子病历中医学体征数据缺失率过高导致预测性能降低的问题。 |
| [^19] | [Non-stationary Projection-free Online Learning with Dynamic and Adaptive Regret Guarantees.](http://arxiv.org/abs/2305.11726) | 本文研究了非平稳无投影在线学习，并提出了动态遗憾和自适应遗憾来衡量性能，首次提出了该方法的动态遗憾边界和自适应遗憾上限。 |
| [^20] | [What Comes Next? Evaluating Uncertainty in Neural Text Generators Against Human Production Variability.](http://arxiv.org/abs/2305.11707) | 本文分析了神经文本生成器与人类生产变异性之间的不确定性，通过探测生成器的输出空间来测量其对人类生产变异性的校准程度，并证明用多个样本和多个参考可以更好地了解模型的不确定性表示。 |
| [^21] | [S-JEA: Stacked Joint Embedding Architectures for Self-Supervised Visual Representation Learning.](http://arxiv.org/abs/2305.11701) | 本文提出了使用堆叠联合嵌入结构学习高度可分离的分层语义表示，显示出更明显的语义概念子类，并且与传统方法相似。 |
| [^22] | [RGCVAE: Relational Graph Conditioned Variational Autoencoder for Molecule Design.](http://arxiv.org/abs/2305.11699) | 本文提出了RGCVAE，一种基于关系图条件化的可变自编码器，可以有效、高效地进行分子设计，并在两个数据集上表现出了先进的生成性能和快速的训练速度。 |
| [^23] | [Surgical-VQLA: Transformer with Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery.](http://arxiv.org/abs/2305.11692) | 本文提出了Surgical-VQLA方法，结合Transformer模型和门控视觉-语言嵌入，解决了手术VQA中对象检测稀缺、异构模态融合策略不足、定位答案缺失等问题，并在测试中实现了最好的表现。 |
| [^24] | [Recycle-and-Distill: Universal Compression Strategy for Transformer-based Speech SSL Models with Attention Map Reusing and Masking Distillation.](http://arxiv.org/abs/2305.11685) | 本研究提出了一种基于Transformer的语音自监督学习模型的通用压缩策略，通过重用注意力映射和蒸馏屏蔽来提高学生模型的语音表示质量，实现了较低的错误率。 |
| [^25] | [Self-Reinforcement Attention Mechanism For Tabular Learning.](http://arxiv.org/abs/2305.11684) | 本文提出了自我强化注意机制（SRA）用于处理表格学习，该机制通过逐元素向量乘法以加强或减弱原始输入的某些组件来学习可理解的特征表示。 |
| [^26] | [Sensing of inspiration events from speech: comparison of deep learning and linguistic methods.](http://arxiv.org/abs/2305.11683) | 本研究比较了使用深度学习和基于语言的方法来感知胸带传感器数据中的吸气事件，结果表明VRB方法优于传统方法。同时该研究还发现朗读和自发语言内容都具有显着的非语法性呼吸，为开发VRB方法提供了新的见解。 |
| [^27] | [Probabilistic Lexicase Selection.](http://arxiv.org/abs/2305.11681) | 本文介绍了一种新的亲代选择算法——概率词典选择（plexicase selection），它有效地近似了传统词典选择的概率分布。该方法具有语意感知的选择能力，同时也提高了效率和灵活性。 |
| [^28] | [A Generic Performance Model for Deep Learning in a Distributed Environment.](http://arxiv.org/abs/2305.11665) | 本文提出了一个适用于分布式环境下各类深度学习应用程序的通用性能模型，可解决现有性能模型特定于案例的问题，并将内在与外在因素考虑在内。通过正则化和差分进化算法，找到最佳拟合常数值的通用表达式，以提高和量化模型框架的效率。 |
| [^29] | [V2X-Boosted Federated Learning for Cooperative Intelligent Transportation Systems with Contextual Client Selection.](http://arxiv.org/abs/2305.11654) | 基于V2X消息进行联邦学习，提出了上下文客户端选择流程，解决了车辆动态状态和网络连接质量等问题，实验结果优于各种数据集 |
| [^30] | [Moment Matching Denoising Gibbs Sampling.](http://arxiv.org/abs/2305.11650) | 本文提出了动量匹配去噪Gibbs采样方法，可以在给定‘嘈杂’的模型的情况下，从干净的模型中有效地进行采样。 |
| [^31] | [Distribution-Free Matrix Prediction Under Arbitrary Missing Pattern.](http://arxiv.org/abs/2305.11640) | 本文提出了两种实用算法，能够在任意丢失模式下有效地保证覆盖率的有效性，并量化了缺失对预测精度的影响。 |
| [^32] | [A Path to Holistic Privacy in Stream Processing Systems.](http://arxiv.org/abs/2305.11638) | 本文系统地研究了流处理系统与物联网交叉引起的隐私问题，并提出解决方案，旨在实现全面的隐私保护。 |
| [^33] | [Goal-Oriented Communications in Federated Learning via Feedback on Risk-Averse Participation.](http://arxiv.org/abs/2305.11633) | 本文基于风险规避机制，通过PS反馈和语义信息，解决了联邦学习中客户端选择问题，以获得通讯高效的设备性能。 |
| [^34] | [Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD Detection, Calibration, and Accuracy.](http://arxiv.org/abs/2305.11616) | 这项研究提出了一种使用显著性图来促进深度集成多样性的方法，用于改善OOD检测、校准和准确性，能够优于传统的集成技术，并在OpenOOD基准测试上证明了其有效性。 |
| [^35] | [SFP: Spurious Feature-targeted Pruning for Out-of-Distribution Generalization.](http://arxiv.org/abs/2305.11615) | SFP提出了一种针对模型子结构的修剪框架，SFP可以自动探索不变的子结构，而不考虑对完全暴露于域外数据的依赖性以及对整个数据分布进行同样特征未命中修剪带来的缺点。这种方法的核心在于，利用ID数据中的伪特征来降低风险。 |
| [^36] | [MIDI-Draw: Sketching to Control Melody Generation.](http://arxiv.org/abs/2305.11605) | MIDI-Draw是一种可以通过画曲线来控制旋律生成的方法，相比于现有的方法，这种方法可以让用户更快速地表达他们的音乐意图。 |
| [^37] | [Latent Imitator: Generating Natural Individual Discriminatory Instances for Black-Box Fairness Testing.](http://arxiv.org/abs/2305.11602) | 本论文提出了潜在模拟器方法，使用生成对抗网络在真实数据分布上生成自然鉴别实例，用于黑盒公平性测试。 |
| [^38] | [Vision-based DRL Autonomous Driving Agent with Sim2Real Transfer.](http://arxiv.org/abs/2305.11589) | 本研究提出了一种基于视觉的深度强化学习代理，可以同时执行保持车道和跟车操作，并且展示了其在真实情况下的模型迁移能力，是第一个具有此能力的代理。 |
| [^39] | [Bayesian approach to Gaussian process regression with uncertain inputs.](http://arxiv.org/abs/2305.11586) | 本文提出了一种新的高斯过程回归技术，通过贝叶斯方法将输入数据的不确定性纳入回归模型预测中。在数值实验中展示了该方法具有普适性和不错的表现。 |
| [^40] | [Dynamic Regularized Sharpness Aware Minimization in Federated Learning: Approaching Global Consistency and Smooth Landscape.](http://arxiv.org/abs/2305.11584) | 本文提出了一种动态正则化锐度感知联邦学习方法，通过同时考虑优化和泛化目标来高效地提高联邦学习的性能。 |
| [^41] | [What You Hear Is What You See: Audio Quality Metrics From Image Quality Metrics.](http://arxiv.org/abs/2305.11582) | 该研究探讨了利用图像感知度量方法来评估音频信号的可行性，并通过定制一个心理声学合理结构的度量方法来解决声音信号的特殊性，并在音乐数据集上展现出了令人满意的结果。 |
| [^42] | [The Deep Promotion Time Cure Model.](http://arxiv.org/abs/2305.11575) | 该研究提出了一种新方法，将灵活的生存模型集成进深度神经网络框架中，实现预测存在治愈分数时的时间至事件。该方法可适用于大规模应用，允许协变量和生存之间的非线性关系和高维交互，从而获得更好的预测性能和更真实的协变量效应。 |
| [^43] | [TSGM: A Flexible Framework for Generative Modeling of Synthetic Time Series.](http://arxiv.org/abs/2305.11567) | TSGM提供了一种生成合成时间序列数据的灵活框架，使研究人员能够快速实现自己的方法并在可共享的环境中进行比较，从而有助于生成大规模的合成时间序列数据集，以用于训练和验证各种机器学习模型。 |
| [^44] | [ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings.](http://arxiv.org/abs/2305.11554) | 本论文提出了一种名为ToolkenGPT的方法，将大型语言模型（LLMs）与外部工具相结合，引入了toolken的概念，利用tool embeddings实现无缝交互，同时在各种下游任务上展示出了良好的效果。 |
| [^45] | [Generalizing to new calorimeter geometries with Geometry-Aware Autoregressive Models (GAAMs) for fast calorimeter simulation.](http://arxiv.org/abs/2305.11531) | 基于几何感知的自回归模型能够学习电磁量计响应如何随几何形状变化，能够快速有效地模拟非环形的电磁量计。 |
| [^46] | [A Sequence-to-Sequence Approach for Arabic Pronoun Resolution.](http://arxiv.org/abs/2305.11529) | 本文提出了一种序列到序列学习方法，用于解决阿拉伯语代词消解问题。该模型在AnATAr数据集上优于传统的机器学习模型和手工特征模型。研究者还探讨了一些对模型的修改，这些修改显著提高了模型的性能。 |
| [^47] | [InstructIE: A Chinese Instruction-based Information Extraction Dataset.](http://arxiv.org/abs/2305.11527) | 介绍了一份中文的基于指令的信息提取数据集InstructIE，其中包括了270,000个弱监督的数据和1,000个高质量注释实例。实验结果表明当前的模型表现有待改进，该任务仍存在挑战。 |
| [^48] | [Enhancing Short-Term Wind Speed Forecasting using Graph Attention and Frequency-Enhanced Mechanisms.](http://arxiv.org/abs/2305.11526) | 本文提出了一种基于图注意力和频率增强机制的风速预测模型GFST-WSF，能够有效提高短期风速预测的准确性。 |
| [^49] | [Enriching Disentanglement: Definitions to Metrics.](http://arxiv.org/abs/2305.11512) | 本文探究了解缠结表示学习的度量标准，并提出了基于丰富范畴论的系统方法，将方程定义转化为可比较的度量标准，我们推导出应用于测量解缠结属性的度量标准，并在合成数据上证明其有效性。 |
| [^50] | [From Random Search to Bandit Learning in Metric Measure Spaces.](http://arxiv.org/abs/2305.11509) | 本文介绍了随机搜索及其性能，引入了“散射维度”的概念，描述了底层函数的状态，量化了随机搜索的性能，并证明了在无噪声和有界噪声情况下的输出分别以一定概率收敛到最优值。 |
| [^51] | [JOINEDTrans: Prior Guided Multi-task Transformer for Joint Optic Disc/Cup Segmentation and Fovea Detection.](http://arxiv.org/abs/2305.11504) | 本文提出JOINDEDTrans框架用于联合OD/OC 分割及黄斑检测, 采用先验信息引导多任务Transformer， 取得更好的分割和检测效果。 |
| [^52] | [Nonconvex Robust High-Order Tensor Completion Using Randomized Low-Rank Approximation.](http://arxiv.org/abs/2305.11495) | 本文提出了两种高效低秩张量逼近方法和双非凸模型及其相应的快速优化算法，用于解决鲁棒高阶张量完成问题，并在大规模合成和真实世界的任务上证明了其优越性能。 |
| [^53] | [LLM Itself Can Read and Generate CXR Images.](http://arxiv.org/abs/2305.11490) | 该论文提出了一种新方法，可以在不需要进行结构更改、额外训练、或训练专门网络的情况下，通过微调预先训练的LLM来读取和生成像文本一样的图像，并应用于胸部X线（CXR）图像的生成任务中。 |
| [^54] | [Incomplete Multi-view Clustering via Diffusion Completion.](http://arxiv.org/abs/2305.11489) | 本文提出一种扩散补全方法，将缺失视角恢复到不完整多视角聚类框架中，并通过对比学习来学习多视角数据的一致性信息，从而提高了多视角聚类的性能。 |
| [^55] | [Learning Diverse Risk Preferences in Population-based Self-play.](http://arxiv.org/abs/2305.11476) | RPPO是一种新颖的强化学习算法，通过代理程序在面对不确定性时具备多样的风险偏好，从而增加自我对抗算法中的策略多样性，并提高代理程序面对不同对手的鲁棒性。 |
| [^56] | [Curve Your Enthusiasm: Concurvity Regularization in Differentiable Generalized Additive Models.](http://arxiv.org/abs/2305.11475) | 本文提供了一种共曲抑制正则化器，用于应对广义加性模型易受共错性的问题，通过惩罚非线性转换的特征变量的成对相关性，增强了模型的解释性。 |
| [^57] | [Generative Sliced MMD Flows with Riesz Kernels.](http://arxiv.org/abs/2305.11463) | 本文使用Riesz核展示了生成式分割MMD流的高效计算方法，实现了在大规模应用中通过神经网络训练生成模型。 |
| [^58] | [Shattering the Agent-Environment Interface for Fine-Tuning Inclusive Language Models.](http://arxiv.org/abs/2305.11455) | 该论文提出了一种新颖的思路，将预训练的语言模型本身同时作为策略、奖励函数和转移函数，可以直接进行奖励学习和语言模型微调，可以带来巨大的统计收益。 |
| [^59] | [Zero-Shot Text Classification via Self-Supervised Tuning.](http://arxiv.org/abs/2305.11442) | 本文提出了一种基于自监督调整的零样本文本分类算法，通过使用无标签数据来调整语言模型，通过学习预测段落中的第一句话，实现了对未见过任务的零样本推断，模型不需要注释数据进行元调整，对模板的选择不敏感，并在实验中取得不错的结果。 |
| [^60] | [PS-FedGAN: An Efficient Federated Learning Framework Based on Partially Shared Generative Adversarial Networks For Data Privacy.](http://arxiv.org/abs/2305.11437) | 本文提出了一种高效的联合学习框架 PS-FedGAN，通过部分共享生成式对抗网络以保护数据隐私，实现了在分布数据环境下捕捉本地数据总体特征，相比于现有框架具有更好的收敛速度、通信开销和隐私保护效果。 |
| [^61] | [TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks.](http://arxiv.org/abs/2305.11430) | 本文提出了一个通用分类法，可以用来设计具有特定属性的提示来执行各种复杂任务，从而解决了LLM在执行复杂任务方面的性能变异巨大的问题。 |
| [^62] | [Graph Propagation Transformer for Graph Representation Learning.](http://arxiv.org/abs/2305.11424) | 本文提出了一种新的变换器架构 GPTrans，以图传播注意力为基础，可以更好地学习图形模型，并在多个基准测试集上超过了其他最先进的基于变换器的图形模型。 |
| [^63] | [Beyond Exponential Graph: Communication-Efficient Topologies for Decentralized Learning via Finite-time Convergence.](http://arxiv.org/abs/2305.11420) | 本文介绍了一种新型拓扑——基础$(k+1)$图，其中节点在有限的迭代次数后能达到确切的共识，具有快速共识率和小的最大度数，从而可以用于分散式SGD。 |
| [^64] | [JetSeg: Efficient Real-Time Semantic Segmentation Model for Low-Power GPU-Embedded Systems.](http://arxiv.org/abs/2305.11419) | JetSeg是一个专为GPU-嵌入式系统设计的高效实时语义分割模型，通过新型的轻量级高效块JetBlock和结合了不对称和非对称卷积、深度空洞卷积、通道混洗操作、轻量级激活函数和适用于嵌入式系统的方便数量的组卷积的策略JetConv以及创新的损失函数JetLoss，在保持高精度的同时，明显减少了内存使用量和推理时间，优于最先进的模型。 |
| [^65] | [Complexity of Feed-Forward Neural Networks from the Perspective of Functional Equivalence.](http://arxiv.org/abs/2305.11417) | 本文从功能等价的角度出发研究前馈神经网络的复杂性，发现利用置换不变性的特性可以降低网络的复杂度，通过过参数化可以增加训练网络的容易程度，并对深度学习中的优化和泛化理解具有重要意义。 |
| [^66] | [Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models.](http://arxiv.org/abs/2305.11414) | 本文提出了联邦基础模型（FFMs）的概念，结合了基础模型和联邦学习的优势，可实现跨多个机构的隐私保护和协作学习。 |
| [^67] | [AlignAtt: Using Attention-based Audio-Translation Alignments as a Guide for Simultaneous Speech Translation.](http://arxiv.org/abs/2305.11408) | AlignAtt是一种新型的SimulST策略，使用基于注意力的音频翻译对齐来指导模型，在BLEU和延迟方面均优于之前的策略。 |
| [^68] | [Few-Shot Continual Learning for Conditional Generative Adversarial Networks.](http://arxiv.org/abs/2305.11400) | 本文提出了一种新的连续学习方法，适用于条件生成对抗网络，根据cGAN的判别器数据识别出最接近目标的现有模式，并通过扩展连续学习模型，使用回放生成的数据来训练目标模式的cGAN模型，以避免灾难性遗忘，提高了生成性能。 |
| [^69] | [A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation.](http://arxiv.org/abs/2305.11391) | 通过验证和验证的视角对大型语言模型的安全性和可信度进行调查，分类它们的已知漏洞，将其分为固有问题、有意攻击和意外错误。同时，考虑四种互补技术以提供LLM及其应用的安全和可信度保障。 |
| [^70] | [ALT: An Automatic System for Long Tail Scenario Modeling.](http://arxiv.org/abs/2305.11390) | 本文提出了一种名为ALT的自动化系统，用于解决长尾场景建模问题，实现了对于模型训练和推理阶段人力和资源有限的情况下的有效建模需求。 |
| [^71] | [Domain Generalization Deep Graph Transformation.](http://arxiv.org/abs/2305.11389) | 本文提出了一种面向领域泛化的深度图形转换方法，使用超网络进行多输入多输出的图神经网络预测，通过加入潜在变量来训练泛化模型，在多个基准数据集中取得了优异表现。 |
| [^72] | [Justices for Information Bottleneck Theory.](http://arxiv.org/abs/2305.11387) | 本论文从引入一个辅助函数，证明深度学习中ReLU激活下互信息下降的悖论，挑战了对信息瓶颈理论适用性的质疑，提供了使用该理论解释DL网络内部组织的新方法。 |
| [^73] | [Improving Fairness in AI Models on Electronic Health Records: The Case for Federated Learning Methods.](http://arxiv.org/abs/2305.11386) | 本研究提出了一种利用联邦学习方法来提高医疗AI模型公平性的方法，包括对抗去偏见和公平聚合方法，适用于各种公平度量标准，使医疗机构可以有效合作。 |
| [^74] | [Online Learning in a Creator Economy.](http://arxiv.org/abs/2305.11381) | 本文探讨了如何应用在线学习的方法优化创作者经济学中的平台收益，分析和比较了基于回报的和基于特征的两种合同类型。 |
| [^75] | [Generalized Precision Matrix for Scalable Estimation of Nonparametric Markov Networks.](http://arxiv.org/abs/2305.11379) | 本文提出了一种广义精度矩阵（GPM）用于描述所有数据类型的条件独立结构，并允许变量之间的一般功能关系。同时，提出了一种马尔科夫网络结构学习算法，在处理大图时，使用了统一的正则化得分匹配框架以提高可伸缩性。 |
| [^76] | [GraphFC: Customs Fraud Detection with Label Scarcity.](http://arxiv.org/abs/2305.11377) | GraphFC是一个模型不可知、领域特定、半监督图神经网络框架，用于少量标签数据的海关欺诈检测。它利用了图神经网络和半监督学习，有效地结合了标记和未标记的数据，提高欺诈检测的性能。 |
| [^77] | [Smart Pressure e-Mat for Human Sleeping Posture and Dynamic Activity Recognition.](http://arxiv.org/abs/2305.11367) | 本文介绍了一种基于Velostat的智能压力电子垫系统，可用于识别人体姿势和运动，具有高精度。 |
| [^78] | [Differentially Private Online Item Pricing.](http://arxiv.org/abs/2305.11362) | 本文介绍了一种差分隐私算法，可在保护买家隐私的同时实现重复、不限供应物品拍卖中的收益最大化，是第一个提供隐私保证的$O(\sqrt{T}\log{T})$亏损子线性的方法。 |
| [^79] | [Differentially Private Adapters for Parameter Efficient Acoustic Modeling.](http://arxiv.org/abs/2305.11360) | 本研究提出一种高效的方案，将差分隐私保证引入跨语言语音分类器的适配中，使用教师-学生集成和残差适配器可以显著降低训练成本和时间，同时保持DP保证，使得可训练参数减少97.5%且性能基本不受影响。 |
| [^80] | [Understanding the World to Solve Social Dilemmas Using Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2305.11358) | 本文通过研究自利理性代理在多智能体强化学习环境中学习世界模型的行为，发现具有世界模型的代理群体在处理可能出现的社会困境时表现出色。这是第一项展示代理可以通过学习现实世界来解决社会困境的工作。 |
| [^81] | [Meta-learning for heterogeneous treatment effect estimation with closed-form solvers.](http://arxiv.org/abs/2305.11353) | 本文提出了一种元学习方法，用于从少量的观测数据中估计条件平均处理效应（CATE），该方法通过神经网络模型对CATE估计问题进行分解并使用闭式求解器获得参数，最终实现了任务之间的共享和优化CATE估计表现提升。 |
| [^82] | [Data Redaction from Conditional Generative Models.](http://arxiv.org/abs/2305.11351) | 本文研究如何对已训练好的条件生成模型进行后期编辑，以便编辑掉某些条件分支，这些条件分支很可能会生成不良内容。通过精简模型中的条件网络实现，提出的解决方案有效、高效、具有可控性和普适性，在文本到图像和文本到语音生成模型中取得了良好效果。 |
| [^83] | [Unsupervised Domain-agnostic Fake News Detection using Multi-modal Weak Signals.](http://arxiv.org/abs/2305.11349) | 本文提出了一种新的、无需监督、跨领域的虚假新闻检测框架，通过嵌入多模态信息和自监督学习技术实现，同时还提出了一种新的数据集构建技术，有效避免了现有数据集中的潜在偏见。 |
| [^84] | [In the Name of Fairness: Assessing the Bias in Clinical Record De-identification.](http://arxiv.org/abs/2305.11348) | 本文研究了临床记录去识别系统在不同人口群体中的表现差异，揭示了其在名称去识别方面存在显著的偏见。 |
| [^85] | [Quantifying the robustness of deep multispectral segmentation models against natural perturbations and data poisoning.](http://arxiv.org/abs/2305.11347) | 本研究通过对多光谱图像分割模型进行实验，发现多光谱数据不能提高模型对自然扰动的鲁棒性，同时模型对抗攻击的鲁棒性取决于攻击方法和使用的特定光谱波段。 |
| [^86] | [Bayesian Reparameterization of Reward-Conditioned Reinforcement Learning with Energy-based Models.](http://arxiv.org/abs/2305.11340) | 该论文提出了BR-RCRL，它是一种贝叶斯重参数化算法，能够解决奖励条件强化学习中的泛化能力和样本外查询问题。 |
| [^87] | [MALM: Mask Augmentation based Local Matching for Food-Recipe Retrieval.](http://arxiv.org/abs/2305.11327) | 提出了一种基于口罩增强的局部匹配网络(MALM)，用于图像到食谱的检索，学习可泛化的跨模态表示。 |
| [^88] | [SpikeCP: Delay-Adaptive Reliable Spiking Neural Networks via Conformal Prediction.](http://arxiv.org/abs/2305.11322) | 这篇论文提出了一种新的脉冲神经网络模型，能够通过极限预测实现自适应的推断延迟，从而节约能源与提高可靠性。 |
| [^89] | [BELLA: Black box model Explanations by Local Linear Approximations.](http://arxiv.org/abs/2305.11311) | 本文提出了一种确定性的、与模型无关的事后方法BELLA，用于解释回归黑盒模型的个别预测。该方法通过特征空间中训练的线性模型提供解释，使得该模型的系数可以直接用于计算特征值的预测值。此外，BELLA最大化了线性模型适用的领域范围。 |
| [^90] | [AMII: Adaptive Multimodal Inter-personal and Intra-personal Model for Adapted Behavior Synthesis.](http://arxiv.org/abs/2305.11310) | AMII是一种面部手势合成方法，通过模态记忆编码模式和注意机制，实现了对自我和人际关系的捕捉，从而适应性地显示行为。 |
| [^91] | [pTSE: A Multi-model Ensemble Method for Probabilistic Time Series Forecasting.](http://arxiv.org/abs/2305.11304) | 提出了pTSE，一种基于隐马尔可夫模型的概率预测的多模型分布集成方法，实现了对时间序列的鲁棒性和准确性的提高。 |
| [^92] | [Multi-Fidelity Machine Learning for Excited State Energies of Molecules.](http://arxiv.org/abs/2305.11292) | 本文提出了一种多精度机器学习方法来预测分子的激发态能量，该方法将高精度的数据与成本更低、精度更低的数据相结合，从而提高了预测的精度。 |
| [^93] | [Massively Scalable Inverse Reinforcement Learning in Google Maps.](http://arxiv.org/abs/2305.11290) | 本文提出了一种新的逆强化学习算法（RHIP），通过图压缩、并行化和基于主特征向量的问题初始化解决了全球规模的MDPs、大型数据集和高度参数化的模型的问题，在谷歌地图中实现了16-24%的全球路线质量改进。 |
| [^94] | [Riemannian Multiclass Logistics Regression for SPD Neural Networks.](http://arxiv.org/abs/2305.11288) | 本论文提出了一种新的Riemannian多类Logistic回归（RMLR）分类器用于学习对称正定矩阵的神经网络，通过内在捕捉SPD流形几何的方式，在流行的SPD学习基准测试中证明了其优越性。 |
| [^95] | [Federated learning for secure development of AI models for Parkinson's disease detection using speech from different languages.](http://arxiv.org/abs/2305.11284) | 本论文利用联邦学习方法，无需共享患者数据，实现在德语、西班牙语和捷克语三种语言数据集上进行帕金森病检测，取得了优于本地模型的诊断准确性。 |
| [^96] | [On the Statistical Efficiency of Mean Field Reinforcement Learning with General Function Approximation.](http://arxiv.org/abs/2305.11283) | 本文研究了一般函数逼近下的均场控制(MFC)和均场博弈(MFG)中的强化学习的统计效率，提出了基于乐观最大似然估计的算法，并仅对转移动力学具有Lipschitz连续性的假设，最后建立了一个指数级的下界支持MFC设置。 |
| [^97] | [SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models.](http://arxiv.org/abs/2305.11281) | 本文提出了一种名为SlotDiffusion的对象中心潜在扩散模型，它具有强大的建模能力，能够提高物体中心槽到图像解码的质量，超越了先前的槽模型。 |
| [^98] | [Real-Time Variational Method for Learning Neural Trajectory and its Dynamics.](http://arxiv.org/abs/2305.11278) | 本论文介绍了一种实时的递归贝叶斯方法用于推断神经轨迹及其动力学，能够广泛适用于任意似然，同时有效跟踪神经元中钙成像数据的动态。 |
| [^99] | [Towards Collaborative Plan Acquisition through Theory of Mind Modeling in Situated Dialogue.](http://arxiv.org/abs/2305.11271) | 本文提出了一种协作计划获取方法，通过丰富的感知和对话历史，让代理人预测他们自己和合作伙伴缺失的任务知识，实现联合任务的完整计划获取。 |
| [^100] | [Constrained Environment Optimization for Prioritized Multi-Agent Navigation.](http://arxiv.org/abs/2305.11260) | 本文将环境视为决策变量，提出了优先级环境优化的问题，并分析了智能体优先级在环境优化中的作用。 |
| [^101] | [Brain-inspired learning in artificial neural networks: a review.](http://arxiv.org/abs/2305.11252) | 本文综述了当前人工神经网络中的脑启发式学习表示，找出了未来研究的有前途的方向，这可能使我们更加接近理解智能的本质。 |
| [^102] | [A Parameter-Efficient Learning Approach to Arabic Dialect Identification with Pre-Trained General-Purpose Speech Model.](http://arxiv.org/abs/2305.11244) | 本文介绍了一种利用预训练通用语音模型进行阿拉伯方言识别的参数高效学习方法，通过残差适配器和模型重编程，设计了一个基于记号的标签映射，并在ADI-17数据集上实现了最高精度，同时使用PEL方法进一步减少了训练成本。 |
| [^103] | [Evidence Networks: simple losses for fast, amortized, neural Bayesian model comparison.](http://arxiv.org/abs/2305.11241) | 本论文提出了一种名为证据网络的方法，能够在处理似然函数或先验函数与嵌套抽样无法胜任的情况下实现贝叶斯模型比较。与传统方法不同的是，该方法使用了新的损失函数，使得我们能够更快速地、更有效地估算贝叶斯因子。 |
| [^104] | [Efficient Vertical Federated Learning with Secure Aggregation.](http://arxiv.org/abs/2305.11236) | 本文提出了一种安全有效的竖向联邦学习方法，通过使用安全模块进行聚合，解决了竖直数据集下的隐私泄露问题，并在不降低性能的情况下获得了大量加速。 |
| [^105] | [Information-Ordered Bottlenecks for Adaptive Semantic Compression.](http://arxiv.org/abs/2305.11213) | 本文提出了信息排序瓶颈（IOB）技术，可以在不重新训练的情况下将数据自适应地压缩为按顺序排列的潜在变量，具有高效压缩图像和文本数据的能力，并可以排序信号并对全局固有维度进行估计。 |
| [^106] | [LIMA: Less Is More for Alignment.](http://arxiv.org/abs/2305.11206) | 该论文介绍了一种使用无声调学习预训练语言模型和标准监督损失微调的方法（不使用强化学习或人类模型），并展示了在复杂任务上也有出色的表现。 |
| [^107] | [Assessing Exoplanet Habitability through Data-driven Approaches: A Comprehensive Literature Review.](http://arxiv.org/abs/2305.11204) | 本文献综述旨在阐明新兴趋势和进展，特别关注机器学习和计算模型在系外行星研究中的重要角色，揭示如何应用机器学习技术来预测系外行星的宜居性。 |
| [^108] | [PDP: Parameter-free Differentiable Pruning is All You Need.](http://arxiv.org/abs/2305.11203) | PDP提出了一种无需参数的可微剪枝方案，具有最先进的模型大小、准确性和训练成本，适用于各种视觉和自然语言任务。 |
| [^109] | [At-Admission Prediction of Mortality and Pulmonary Embolism in COVID-19 Patients Using Statistical and Machine Learning Methods: An International Cohort Study.](http://arxiv.org/abs/2305.11199) | 本篇文章提出了一种成本敏感的梯度提升机器学习模型，用于预测COVID-19患者入院时的PE事件和死亡风险。该模型在接受血栓预防治疗的患者子群中表现优于现有的PE风险评分系统，为早期识别和管理高危患者提供了新的准确工具。 |
| [^110] | [Prediction with Incomplete Data under Agnostic Mask Distribution Shift.](http://arxiv.org/abs/2305.11197) | 本研究考虑预测不完整数据的情况，在缺失模式分布可能发生偏移的情况下，我们利用掩码的不变最优预测器实现泛化，通过双参数化技术联合近似最优预测器避免指数爆炸，同时引入正则化项保证预测器具有鲁棒性。 |
| [^111] | [DClEVerNet: Deep Combinatorial Learning for Efficient EV Charging Scheduling in Large-scale Networked Facilities.](http://arxiv.org/abs/2305.11195) | 本文提出了一种基于深度学习和近似算法技术的数据驱动优化框架DClEVerNet，可以优化大规模网络化的EV充电站的预约管理程序，最大化EV用户的总福利收益，同时考虑到网络的可用功率容量和站点的入住限制。 |
| [^112] | [Vaxformer: Antigenicity-controlled Transformer for Vaccine Design Against SARS-CoV-2.](http://arxiv.org/abs/2305.11194) | Vaxformer是一种新型的条件蛋白质语言模型结构，超过了现有的条件变分自编码器模型，在生成抗原性受控的SARS-CoV-2刺突蛋白方面表现更好，为疫苗设计提供了有前途的机会。 |
| [^113] | [Taxonomy of AISecOps Threat Modeling for Cloud Based Medical Chatbots.](http://arxiv.org/abs/2305.11189) | 这篇论文介绍了使用 AISecOps 对云医疗聊天机器人进行监控的方法，解释了 AISecOps 是如何将 IT 运营、人工智能和安全三个领域整合起来协同运作以确保聊天机器人的机密性、完整性和可用性的。 |
| [^114] | [Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt.](http://arxiv.org/abs/2305.11186) | 本文提出了使用可转移提示来优化压缩的LLMs的准确性和效率的平衡问题。该方法通过选择精度更高的提示显著提高了压缩的LLM在特定查询方面的生成质量，并实现了4倍推理时间加速。 |
| [^115] | [Assessing the predicting power of GPS data for aftershocks forecasting.](http://arxiv.org/abs/2305.11183) | 这篇论文提出了一种基于GPS数据和机器学习的方法，可以对地震余震进行准确预测，但其预测能力依赖于GPS站的密度。 |
| [^116] | [Comparison of Transfer Learning based Additive Manufacturing Models via A Case Study.](http://arxiv.org/abs/2305.11181) | 本文制定了一个基于开源数据集的案例研究，通过比较不同的迁移学习方法，回答了应用迁移学习在增材制造建模中的固有挑战问题，并构建了基于迁移学习的模型，用于提升建模性能。 |
| [^117] | [Vanishing Activations: A Symptom of Deep Capsule Networks.](http://arxiv.org/abs/2305.11178) | 本文探讨了胶囊网络结构的缺陷，证明这些问题不仅限于原始设计，而是存在于许多领先的胶囊网络架构中。这种内在的设计相似性可能会限制其可扩展性。 |
| [^118] | [Generating coherent comic with rich story using ChatGPT and Stable Diffusion.](http://arxiv.org/abs/2305.11067) | 本文介绍了一种利用ChatGPT和Stable Diffusion生成连贯漫画故事的方法，通过引入新的评估AI故事的方式，并使用LoRA、ControlNet等方法进行fine-tuning，取得了在角色忠实度和艺术风格上的最先进表现。 |
| [^119] | [Free Lunch for Privacy Preserving Distributed Graph Learning.](http://arxiv.org/abs/2305.10869) | 该论文提出了一种能够保护隐私的分布式图学习框架，通过学习特征和距离，而不需要实际的特征，来执行图形学习和其他下游任务。这是一种通用的框架。 |
| [^120] | [A Subabdominal MRI Image Segmentation Algorithm Based on Multi-Scale Feature Pyramid Network and Dual Attention Mechanism.](http://arxiv.org/abs/2305.10631) | 提出了一种基于多尺度特征金字塔网络和双重注意力机制的子腹部MRI图像分割算法，使用空洞卷积和多尺度特征金字塔编码以避免语义差距，设计双重注意力机制以保持空间信息并减少错位。 |
| [^121] | [Measuring and Mitigating Local Instability in Deep Neural Networks.](http://arxiv.org/abs/2305.10625) | 深度神经网络中，训练过程中的随机性可能导致模型的输出不稳定，作者提出了基于原则的指标来量化不稳定性并发现不稳定的预测并不是随机出现的，而是以数据相关的方式聚集在一起。作者研究了数据无关正则化方法来减轻这种不稳定性，并表明一些方法可以显着提高不稳定性，甚至在某些情况下优于更广泛使用的正则化方法。 |
| [^122] | [Active Learning in Symbolic Regression Performance with Physical Constraints.](http://arxiv.org/abs/2305.10379) | 本文探讨了利用进化符号回归作为主动学习中的方法来提出哪些数据应该被采集，通过“委员会查询”来减少所需数据，并在重新发现已知方程所需的数据方面实现最新的结果。 |
| [^123] | [A Survey of Federated Evaluation in Federated Learning.](http://arxiv.org/abs/2305.08070) | 本篇论文对现有联邦评估方法进行全面综述，阐述了联邦评估在客户端选择、激励机制设计、恶意攻击检测等方面的重要作用，探讨了联邦评估在增强FL性能方面的各种应用，并提出了未来的研究方向。 |
| [^124] | [Provable Multi-instance Deep AUC Maximization with Stochastic Pooling.](http://arxiv.org/abs/2305.08040) | 本文提出了在多实例学习中使用深度AUC最大化（DAM）的方法，并根据包含大量实例的情况下训练的计算挑战，提出了一种基于方差减少的随机池化方法，使得只需对每个包进行少量采样即可计算MIDAM模型，提高了效率和准确性。 |
| [^125] | [Neural operator for structural simulation and bridge health monitoring.](http://arxiv.org/abs/2305.07889) | 本论文提出了结构模拟和桥梁健康监测的神经运算器VINO，通过学习结构响应场和损伤场之间的映射，在前向预测和反向确定损伤区域和程度方面可以比传统有限元模型更准确地预测和判断。 |
| [^126] | [Implicitly normalized forecaster with clipping for linear and non-linear heavy-tailed multi-armed bandits.](http://arxiv.org/abs/2305.06743) | 本文提出了一种针对奖励分布重尾的MAB问题的隐式规范化预测器，证明该方法在线性和非线性重尾随机MAB问题上是最优的。 |
| [^127] | [Towards Achieving Near-optimal Utility for Privacy-Preserving Federated Learning via Data Generation and Parameter Distortion.](http://arxiv.org/abs/2305.04288) | 本论文提出了一种用数据生成和参数畸变实现隐私保护联邦学习接近最优效用的上限方法，其中通过降低方差和模型参数差异来衡量效用损失。 |
| [^128] | [Explainable Reinforcement Learning via a Causal World Model.](http://arxiv.org/abs/2305.02749) | 本文提出了一种新的可解释强化学习框架，通过学习因果世界模型来解释行动的长期影响以及教学习者如何影响环境变量并最终导致奖励。 |
| [^129] | [Dynamic Sparse Training with Structured Sparsity.](http://arxiv.org/abs/2305.02299) | 本文提出了一种结构化稀疏动态训练（DST）方法，学习一种变体的结构化 N:M 稀疏性，其加速在一般情况下通常被支持，可缩减参数和内存占用，同时相较于密集模型，具有减少推理时间的优势。 |
| [^130] | [Towards Computational Architecture of Liberty: A Comprehensive Survey on Deep Learning for Generating Virtual Architecture in the Metaverse.](http://arxiv.org/abs/2305.00510) | 本文综述了当前最新的深度学习生成模型用于建筑形式的3D对象生成方法，强调了尚未充分探讨的问题，并提出了未来研究的重点议程。 |
| [^131] | [Segment Anything Model for Medical Images?.](http://arxiv.org/abs/2304.14660) | “Segment Anything Model”（SAM）是适用于常规图像分割的基础模型，可以实现零样本图像分割，但在医学图像分割方面具有更高的挑战性。作者通过构建一个大型医学分割数据集来验证SAM在该领域的潜力。 |
| [^132] | [Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction: A Unified Library and Performance Benchmark.](http://arxiv.org/abs/2304.14343) | 本研究提出了一种称为原子文件的统一空间时间数据存储格式，开发了一个名为LibCity的开源库，重新构建了65个空间时间预测模型，并收集了55个空间时间数据集。同时还提出了城市时空预测模型的性能基准，为这一领域提供了一个可靠的评估工具。 |
| [^133] | [Categorical Foundations of Explainable AI: A Unifying Formalism of Structures and Semantics.](http://arxiv.org/abs/2304.14094) | 本文采用范畴理论的框架，提出了可解释AI的统一理论体系，为领域中所有重要术语提供了清晰的形式定义，并提供了遵循所提出结构的领域分类法。 |
| [^134] | [Variational Diffusion Auto-encoder: Deep Latent Variable Model with Unconditional Diffusion Prior.](http://arxiv.org/abs/2304.12141) | 本文提出了一种基于扩散模型对条件数据分布进行建模的变分扩散自编码器方法，它避免了对参数形式做出强烈假设，可以显著提高生成图像的质量。 |
| [^135] | [Denial-of-Service or Fine-Grained Control: Towards Flexible Model Poisoning Attacks on Federated Learning.](http://arxiv.org/abs/2304.10783) | 本文提出了一种灵活的联邦学习模型毒化攻击策略，既可以实现拒绝服务(Dos)目标，也可以精确控制全局准确性，具有高效和隐形的特点。 |
| [^136] | [A Scalable Test Problem Generator for Sequential Transfer Optimization.](http://arxiv.org/abs/2304.08503) | STO中已有的测试问题设计不完善，难以代表真实问题多样化关系，限制了算法的表现。本文介绍了一种可扩展的序列转移优化问题生成器。 |
| [^137] | [Probably Approximately Correct Federated Learning.](http://arxiv.org/abs/2304.04641) | 本文提出了FedPAC框架，利用PAC学习理论推导出一个解析解，可以保证FL之间隐私、效用和效率的最佳权衡。 |
| [^138] | [TransPimLib: A Library for Efficient Transcendental Functions on Processing-in-Memory Systems.](http://arxiv.org/abs/2304.01951) | TransPimLib提供了处理器内存系统上高效的超越函数计算方法，有助于提高PIM系统的计算能力和支持更广泛的工作负载，特别是机器学习应用中的激活函数。 |
| [^139] | [Incorporating Unlabelled Data into Bayesian Neural Networks.](http://arxiv.org/abs/2304.01762) | 该论文提出了一种利用未标记数据学习贝叶斯神经网络（BNNs）的对比框架，通过该框架提出了一种同时具备自监督学习的标签效率和贝叶斯方法中的不确定性估计的实用BNN算法。最后，该方法在半监督和低预算主动学习问题中展现出了数据高效学习的优势。 |
| [^140] | [Explicit Planning Helps Language Models in Logical Reasoning.](http://arxiv.org/abs/2303.15714) | 本文提出了一个新的系统，使用语言模型进行多步逻辑推理，采用了显式规划来帮助做出更明智的决策，比其他竞争系统表现更好，显式规划在系统性能中起着关键作用。 |
| [^141] | [Adaptive Riemannian Metrics on SPD Manifolds.](http://arxiv.org/abs/2303.15477) | 本文提出了自适应黎曼度量来改进SPD神经网络的次优性能，实验结果表明该度量能使网络表现更好。 |
| [^142] | [On Statistical Properties of Sharpness-Aware Minimization: Provable Guarantees.](http://arxiv.org/abs/2302.11836) | SAM是一种优化框架，旨在通过获得更平坦（即更不锐利）的解来改善深度神经网络的泛化能力。我们研究两个统计问题，在某些条件下，证明了SAM在预测误差方面比梯度下降有更小的误差，并适用于非凸问题。此外，我们的设置表明，SAM的解更不锐利，证明了我们的结论。 |
| [^143] | [Reinforcement Learning with Function Approximation: From Linear to Nonlinear.](http://arxiv.org/abs/2302.09703) | 本文回顾了近年来在线性和非线性逼近环境下强化学习算法的错误分析，并强调了逼近误差和估计误差/样本复杂度。在线性问题结构的假设下，近期的算法实现了多项式样本复杂度，然而尚未实现最小最大速率。 |
| [^144] | [Zero-Shot Batch-Level Anomaly Detection.](http://arxiv.org/abs/2302.07849) | 本文提出了一种名为“自适应中心表示”的方法，用于零样本批次级异常检测。该方法利用批量归一化来训练现成的深度异常检测器，可以自动零样本泛化为未见过的AD任务。在实验中，该方法显示出了在多种数据集上的优秀表现，对表格数据进行了零样本AD。 |
| [^145] | [The Geometry of Neural Nets' Parameter Spaces Under Reparametrization.](http://arxiv.org/abs/2302.07384) | 研究了神经网络在重参数化下的不变性，如果显式地表示度量并使用正确的相关变换规则，则不变性是任何神经网络的固有属性。 |
| [^146] | [Neural Capacitated Clustering.](http://arxiv.org/abs/2302.05134) | 本论文提出了“神经容量聚类”方法，利用神经网络预测数据点分配到簇中心的概率，结合一种类似于K均值的迭代过程，在容量约束下对聚类问题进行求解。通过人造数据和实际数据集的实验，该方法在性能上优于文献中的多个最先进的数学和启发式求解器。 |
| [^147] | [Quadratic Memory is Necessary for Optimal Query Complexity in Convex Optimization: Center-of-Mass is Pareto-Optimal.](http://arxiv.org/abs/2302.04963) | 本文证明了在凸优化中，实现最优 oracle 复杂性所必需要的内存为二次，并且在处理 1-Lipschitz 凸函数时，使用 $d^{2-\delta}$ 内存的任何算法都需要进行 $\tilde\Omega(d^{1+\delta/3})$ 次查询。此外，在可行性问题中，使用至多 $d^{2-\delta}$ 存储器容量的分离 oracle 需要进行 $\tilde\Omega(d^{1+\delta})$ 次查询。 |
| [^148] | [Migration Reframed? A multilingual analysis on the stance shift in Europe during the Ukrainian crisis.](http://arxiv.org/abs/2302.02813) | 乌克兰危机引起了欧洲对移民议题态度的变化，特别是对来自乌克兰的难民。研究者运用多语言分析技术对新闻和社交媒体上的相关报道进行研究，发现了一种对移民议题讨论的重构。 |
| [^149] | [PubGraph: A Large-Scale Scientific Knowledge Graph.](http://arxiv.org/abs/2302.02231) | PubGraph是一个大规模的、全面的科学知识图谱，包含超过3.85亿个实体和130亿个主要边缘，可以支持对科学网络进行推理研究。 |
| [^150] | [Salient Conditional Diffusion for Defending Against Backdoor Attacks.](http://arxiv.org/abs/2301.13862) | Sancdifi是一种有效防御后门攻击的新算法，通过生成基于salience map的masks调节去噪扩散概率模型，能够有效去除被后门攻击污染的数据中的触发器，同时在干净数据上也能恢复出突出特征，而且无需使用特洛伊网络模型参数，作为一种黑盒防御机制发挥作用。 |
| [^151] | [Cause-Effect Inference in Location-Scale Noise Models: Maximum Likelihood vs. Independence Testing.](http://arxiv.org/abs/2301.12930) | 通过引入异方差位置-尺度噪声函数模型，该论文在正确说明噪声分布的情况下，通过最大似然实现了最先进的准确性。但是，在用户错误指定噪声分布的形式时，分析表明因果推断的精度会急剧下降。因此，该论文提出通过因果模型选择实现稳定而准确的因果推断。 |
| [^152] | [Zero-shot causal learning.](http://arxiv.org/abs/2301.12292) | 无先验因果学习是一个解决预测新型干预措施个性化影响的框架，并通过元学习对任务的处理达成了目的，能够将干预措施的知识传输到未见过的干预措施中，并在合成和真实数据集上表现出了优越性能。 |
| [^153] | [Is TinyML Sustainable? Assessing the Environmental Impacts of Machine Learning on Microcontrollers.](http://arxiv.org/abs/2301.11899) | TinyML部署了机器学习算法到低成本的微控制器系统上，可以解锁无数始终处于开启状态的机器学习应用，这项新兴技术有助于解决可持续发展挑战，但需要评估和缓解其环境影响以确保可持续性。 |
| [^154] | [One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER.](http://arxiv.org/abs/2301.10410) | 本论文提出了基于协作域前缀调整的跨领域实体识别，使用文本到文本生成的支撑领域相关指导来将知识转移至新域NER任务，避免了先前的为每个领域结束一个全新的NER模型的问题。 |
| [^155] | [Open-Set Likelihood Maximization for Few-Shot Learning.](http://arxiv.org/abs/2301.08390) | 本文提出了一种针对少样本开放集识别问题的开放集似然最大化方法，可以在利用未标记查询实例进行推理时提高模型的鲁棒性和准确性。 |
| [^156] | [Deep reinforcement learning for irrigation scheduling using high-dimensional sensor feedback.](http://arxiv.org/abs/2301.00899) | 本文介绍了一个使用深度强化学习进行灌溉调度的原则性框架和可行的程序，并在澳大利亚一个产出高的地区使用灌溉小麦的案例研究中证明了其有效性。 |
| [^157] | [Your diffusion model secretly knows the dimension of the data manifold.](http://arxiv.org/abs/2212.12611) | 本研究提出了一种新的方法，利用扩散模型估算数据流形的维度并且在实验中表现出色。 |
| [^158] | [Foveate, Attribute, and Rationalize: Towards Physically Safe and Trustworthy AI.](http://arxiv.org/abs/2212.09667) | 研究提出了一种新颖的FARM框架，通过利用外部知识生成能够被信任的原理，解决了不安全文本检测的问题，并能够帮助利益相关者和政策制定者保障消费者的安全。 |
| [^159] | [Causes and Cures for Interference in Multilingual Translation.](http://arxiv.org/abs/2212.07530) | 研究探究了多语言机器翻译中干扰的主要因素，通过系统化试验发现使用不到10亿参数的标准Transformer配置可以在很大程度上缓解干扰并促进协同，同时发现调整采样温度以控制数据中每个语言对所占比例的方法是平衡语言对之间关系的关键。 |
| [^160] | [ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages.](http://arxiv.org/abs/2212.06742) | ERNIE-Code是一个适用于116种自然语言和6种编程语言的统一预训练语言模型，采用了跨度损坏语言建模和基于桥接的翻译语言建模两种跨语言预训练方法，并在广泛的代码智能终端任务中优于以前的多语言LLMs。 |
| [^161] | [Copula Conformal Prediction for Multi-step Time Series Forecasting.](http://arxiv.org/abs/2212.03281) | 本文提出了一种 Copula 联合预测算法 CopulaCPTS，用于多元、多步时间序列预测，经过实验验证，其置信区间比现有技术更精准和更锐利。 |
| [^162] | [SODA: A Natural Language Processing Package to Extract Social Determinants of Health for Cancer Studies.](http://arxiv.org/abs/2212.03000) | 本文介绍了一个开源的自然语言处理包SODA，可用于提取癌症患者的社会健康决定因素。该包在泛化能力方面表现良好，可以用于新的疾病领域。研究结果表明，该包在癌症人群中提取SDoH的提取率较高。 |
| [^163] | [Fast Inference from Transformers via Speculative Decoding.](http://arxiv.org/abs/2211.17192) | 本文介绍了一种基于投机解码的算法，可以在不更改输出的情况下更快地从大型自回归模型（如Transformer）中采样，加速了现有的模型，而无需重新训练或进行架构更改。 |
| [^164] | [On the Complexity of Counterfactual Reasoning.](http://arxiv.org/abs/2211.13447) | 该研究发现，与在完全指定的结构因果模型上进行关联或干预推理相比，反事实推理的计算复杂性并不更高，两者的复杂性可以通过关于树宽的边界界定得到较好的处理。 |
| [^165] | [Modeling Temporal Data as Continuous Functions with Stochastic Process Diffusion.](http://arxiv.org/abs/2211.02590) | 本文提出了用随机过程扩散方法将时间数据建模为连续函数，并实现了适用于多元概率预测和插补的新颖模型。 |
| [^166] | [Balancing Utility and Fairness in Submodular Maximization (Technical Report).](http://arxiv.org/abs/2211.00980) | 本文提出了一个新的问题，称为“二标准子模最大化”，以平衡效用和公平性。该问题要求找到一个固定大小的解，以最大化效用函数为目标。 |
| [^167] | [Differentiable Model Selection for Ensemble Learning.](http://arxiv.org/abs/2211.00251) | 本文提出了一种可微分模型选择框架，专为集成学习而设计，通过将集成学习任务转化为可微分选择程序，在集成学习模型内端到端地训练学习为特定输入样本选择合适的集成成员，有效性和多功能性均优于传统和先进的共识规则。 |
| [^168] | [Understanding HTML with Large Language Models.](http://arxiv.org/abs/2210.03945) | 本研究使用大型语言模型探索了对HTML的理解，提出了HTML理解模型，通过微调使其在语义分类、描述生成和自主网络导航三个任务上表现出良好的性能，显示出大型语言模型在HTML任务上表现出色。 |
| [^169] | [Benign Autoencoders.](http://arxiv.org/abs/2210.00637) | 本文正式化了用于生成式人工智能中编码器-解码器对的最佳选择问题并提出了良性自编码器（BAE），BAE能够将数据投射到最优的流型上，实现了数据压缩和更加稳定的梯度下降。 |
| [^170] | [Neural Integral Equations.](http://arxiv.org/abs/2209.15190) | 本文介绍了神经积分方程（NIE）和自注意神经积分方程（ANIE）的方法，它们可以在无监督情况下通过学习数据中的积分算子进行模型建立，并且在合成和真实世界数据的ODE、PDE和IE系统中的基准任务上表现出较高的速度和准确性。 |
| [^171] | [On the Optimization Landscape of Dynamic Output Feedback: A Case Study for Linear Quadratic Regulator.](http://arxiv.org/abs/2209.05042) | 本文研究了线性二次调节器中动态输出反馈策略的优化景观，推导了最优变换并证明了当其可观测时静止点的唯一性，从而为使用策略梯度方法解决动态控制器提供了最优性证明。 |
| [^172] | [Online Decision Making for Trading Wind Energy.](http://arxiv.org/abs/2209.02009) | 本文提出了一种在线报价方法，能够在非稳态及不确定参数下实现更好的适应性与显著的经济收益。 |
| [^173] | [ID and OOD Performance Are Sometimes Inversely Correlated on Real-world Datasets.](http://arxiv.org/abs/2209.00613) | 本文发现在现实世界数据中，ID性能和OOD性能之间存在反相关关系，提示需要在两者之间进行权衡，单纯关注ID性能可能无法达到最佳性能。 |
| [^174] | [Federated Learning via Decentralized Dataset Distillation in Resource-Constrained Edge Environments.](http://arxiv.org/abs/2208.11311) | 本论文介绍了一种名为FedD3的联邦学习框架，通过集成数据集提炼实例仅需要一次通信，与其他联邦学习方法相比，在需要通信的数据量方面表现显著更好，同时通过平衡准确性和通信成本来适应使用场景。 |
| [^175] | [Algebraic Reduction of Hidden Markov Models.](http://arxiv.org/abs/2208.05968) | 本论文提出了两种算法用于将隐马尔可夫模型简化为维度更小的模型，并精确复现其边缘分布，首次扩展了实现理论工具到这个应用领域。 |
| [^176] | [Conditioning Normalizing Flows for Rare Event Sampling.](http://arxiv.org/abs/2207.14530) | 该论文提出了一种基于神经网络生成配置的转换路径采样方案，使用归一化流消除采样路径之间的相关性，易于并行化采样过程，通过条件设置将配置采样引导到感兴趣的区域。 |
| [^177] | [An Experimental Evaluation of Machine Learning Training on a Real Processing-in-Memory System.](http://arxiv.org/abs/2207.07886) | 该研究评估了在处理内存系统上训练机器学习算法的潜能，并证明基于PIM的ML训练实现了显着的加速和能量效率。 |
| [^178] | [Towards the Practical Utility of Federated Learning in the Medical Domain.](http://arxiv.org/abs/2207.03075) | 本研究提出了应用联邦学习于医学领域的实用指南，包括三个具有代表性的医学数据集的实验，旨在提高医保业的数据效率，并形成适用于全行业的标准。 |
| [^179] | [Confident Sinkhorn Allocation for Pseudo-Labeling.](http://arxiv.org/abs/2206.05880) | 本文提出了一种基于置信泰森堡分配的伪标签方法，通过最优传输仅对高置信度样本进行伪标签分配，在半监督学习方面取得了目前最好的表现。 |
| [^180] | [DELTA: Diverse Client Sampling for Fasting Federated Learning.](http://arxiv.org/abs/2205.13925) | DELTA 提出了一个无偏抽样方案来减少部分客户端参与所引起的方差，以缓解现有抽样方法可能导致性能下降的问题，它考虑了客户端的多样性和局部方差的影响，并选择具有全局模型更新所需有价值信息的代表性客户端。实验结果表明，DELTA 可以优于其他无偏抽样方案并加速模型收敛速度。 |
| [^181] | [Probabilistic Symmetry for Multi-Agent Dynamics.](http://arxiv.org/abs/2205.01927) | 该论文提出了PECCO模型，通过利用多智能体间的对称性和能量评分规则，可以更准确地预测多智能体轨迹并量化不确定性，为下游决策提供重要支持。 |
| [^182] | [Distributionally Robust Bayesian Optimization with $\phi$-divergences.](http://arxiv.org/abs/2203.02128) | 本研究提出了一种基于$\phi$-离散度的分布鲁棒贝叶斯优化算法。 |
| [^183] | [Are Transformers More Robust? Towards Exact Robustness Verification for Transformers.](http://arxiv.org/abs/2202.03932) | 本文研究了基于Sparsemax的Transformers的稳健性问题，并发现Transformer不一定比传统的多层感知器更加稳健，这对于选择适用于安全关键领域应用的NN架构方面有深刻的考虑。 |
| [^184] | [Anticorrelated Noise Injection for Improved Generalization.](http://arxiv.org/abs/2202.02831) | 本文发现，在一些目标函数中，抗相关噪声的梯度下降方法比传统的梯度下降和常规扰动梯度下降有更好的泛化性能。理论分析证明了这是因为 Anti-PGD 能够移动到更宽的最小值点，而 GD 和 PGD 会停滞在次优区域甚至发散。 |
| [^185] | [On the Noise Stability and Robustness of Adversarially Trained Networks on NVM Crossbars.](http://arxiv.org/abs/2109.09060) | 本文研究结合对抗训练和NVM交叉型存储器内在鲁棒性的设计方法，探索如何设计鲁棒的DNN。对网络未受干扰输入数据下的噪声稳定性进行了研究，并发现对抗训练的网络具有更低的S值。 |
| [^186] | [Semi-verified PAC Learning from the Crowd.](http://arxiv.org/abs/2106.07080) | 本文提出了一种同样具有显著挑战性的半验证模型，在该模型下，即使大多数工人的行为是对抗的，并且其余的人会像Massart噪声一样工作，但众包PAC学习阈值函数的假设类仍然是可行的，并且标注成本可以通过比较查询大大减少。 |
| [^187] | [A multi-centre polyp detection and segmentation dataset for generalisability assessment.](http://arxiv.org/abs/2106.04463) | 这是一个由六个医疗中心提供的包含超过300名患者数据的多中心息肉检测和分割数据集，具有像素级分割和详细的息肉标注，可用于严格测试自动化的息肉检测和分割方法。 |
| [^188] | [A Lightweight and Gradient-Stable Nerual Layer.](http://arxiv.org/abs/2106.04088) | Han层是一种梯度稳定、参数更少的神经层结构，可以替换全连接层来优化神经网络模型。 |

# 详细

[^1]: Photo-zSNthesis: 通过深度学习将Ia型超新星光变曲线转化为红移估计

    Photo-zSNthesis: Converting Type Ia Supernova Lightcurves to Redshift Estimates via Deep Learning. (arXiv:2305.11869v1 [astro-ph.CO])

    [http://arxiv.org/abs/2305.11869](http://arxiv.org/abs/2305.11869)

    本研究提出了一种基于卷积神经网络的方法"Photo-zSNthesis"，用于从多波段超新星光变曲线预测完整的红移概率分布，无需光谱信息。该方法在模拟和真实观测中都取得了重大提升，能极大地约束宇宙学。

    

    即将到来的光度巡天将会发现数以万计的Ia型超新星(SNe Ia)，远远超过了我们的光谱资源容量。为了在没有光谱信息的情况下最大化观测数据的科学回报，我们必须只利用光度信息准确地提取关键参数，例如SN红移。我们提出了一种基于卷积神经网络的方法Photo-zSNthesis，用于从多波段超新星光变曲线预测完整的红移概率分布，并在模拟的SDSS和LSST数据以及观测到的SDSS SNe上进行了测试。我们展示了对现有方法进行的预测的显著改进，无论是在模拟还是实际观测中，且存在极少的红移相关偏差，这是由于选择效应(例如Malmquist偏差)而带来的挑战。该方法产生的概率密度函数能够得到很好的限制，并将最大程度地约束宇宙学。

    Upcoming photometric surveys will discover tens of thousands of Type Ia supernovae (SNe Ia), vastly outpacing the capacity of our spectroscopic resources. In order to maximize the science return of these observations in the absence of spectroscopic information, we must accurately extract key parameters, such as SN redshifts, with photometric information alone. We present Photo-zSNthesis, a convolutional neural network-based method for predicting full redshift probability distributions from multi-band supernova lightcurves, tested on both simulated Sloan Digital Sky Survey (SDSS) and Vera C. Rubin Legacy Survey of Space and Time (LSST) data as well as observed SDSS SNe. We show major improvements over predictions from existing methods on both simulations and real observations as well as minimal redshift-dependent bias, which is a challenge due to selection effects, e.g. Malmquist bias. The PDFs produced by this method are well-constrained and will maximize the cosmological constraining 
    
[^2]: Q-malizing流和无穷小密度比估计

    Q-malizing flow and infinitesimal density ratio estimation. (arXiv:2305.11857v1 [stat.ML])

    [http://arxiv.org/abs/2305.11857](http://arxiv.org/abs/2305.11857)

    研究提出了一种可以从一个数据分布P传输到任意访问通过有限样本的Q的流模型。这个模型通过神经ODE模型进行，可以进行无穷小DRE。

    

    连续的正则化流在生成任务中被广泛使用，其中流网络从数据分布P传输到正态分布。一种能够从P传输到任意Q的流模型，其中P和Q都可通过有限样本访问，将在各种应用兴趣中使用，特别是在最近开发的望远镜密度比估计中（DRE），它需要构建中间密度以在P和Q之间建立桥梁。在这项工作中，我们提出了这样的“Q-malizing流”，通过神经ODE模型进行，该模型通过经验样本的可逆传输从P到Q（反之亦然），并通过最小化传输成本进行正则化。训练好的流模型使我们能够沿与时间参数化的log密度进行无穷小DRE，通过训练附加的连续时间流网络使用分类损失来估计log密度的时间偏导数。通过积分时间得分网络

    Continuous normalizing flows are widely used in generative tasks, where a flow network transports from a data distribution $P$ to a normal distribution. A flow model that can transport from $P$ to an arbitrary $Q$, where both $P$ and $Q$ are accessible via finite samples, would be of various application interests, particularly in the recently developed telescoping density ratio estimation (DRE) which calls for the construction of intermediate densities to bridge between $P$ and $Q$. In this work, we propose such a ``Q-malizing flow'' by a neural-ODE model which is trained to transport invertibly from $P$ to $Q$ (and vice versa) from empirical samples and is regularized by minimizing the transport cost. The trained flow model allows us to perform infinitesimal DRE along the time-parametrized $\log$-density by training an additional continuous-time flow network using classification loss, which estimates the time-partial derivative of the $\log$-density. Integrating the time-score network
    
[^3]: 使用指令微调基础模型的多模态 Web 导航。

    Multimodal Web Navigation with Instruction-Finetuned Foundation Models. (arXiv:2305.11854v1 [cs.LG])

    [http://arxiv.org/abs/2305.11854](http://arxiv.org/abs/2305.11854)

    本文研究使用视觉语言基础模型进行数据驱动离线训练的 Web 代理，提出了一个指令跟随多模态代理WebGUM，将微调指令微调语言模型和视觉转换器，能够有效提高代理的基于视觉感知、HTML 理解和多步推理的能力。

    

    自主 Web 导航的进展受到了依赖数十亿次在线强化学习的探索性交互和具有领域特定模型设计的影响，这使得难以利用来自丰富领域外数据的泛化。在本工作中，我们研究了基于数据驱动的脱机训练，用于使用视觉语言基础模型的 Web 代理。我们提出了一个指令跟随多模态代理， WebGUM，它观察了网页截图和 HTML 页面，并输出 Web 导航操作，如单击和输入。WebGUM 是通过联合微调指令微调语言模型和视觉转换器在大量的演示语料库上训练的。我们凭经验证明，这种方法可以提高代理的基于视觉感知、HTML 理解和多步推理的能力，明显优于之前的工作。在 MiniWoB 基准测试中，我们超过之前最佳脱机方法 31.9% 以上，接近实现在线交互的表现。

    The progress of autonomous web navigation has been hindered by the dependence on billions of exploratory interactions via online reinforcement learning, and domain-specific model designs that make it difficult to leverage generalization from rich out-of-domain data. In this work, we study data-driven offline training for web agents with vision-language foundation models. We propose an instruction-following multimodal agent, WebGUM, that observes both webpage screenshots and HTML pages and outputs web navigation actions, such as click and type. WebGUM is trained by jointly finetuning an instruction-finetuned language model and a vision transformer on a large corpus of demonstrations. We empirically demonstrate this recipe improves the agent's ability of grounded visual perception, HTML comprehension and multi-step reasoning, outperforming prior works by a significant margin. On the MiniWoB benchmark, we improve over the previous best offline methods by more than 31.9%, being close to re
    
[^4]: 通过可组合扩散实现任意输入与输出之间的生成

    Any-to-Any Generation via Composable Diffusion. (arXiv:2305.11846v1 [cs.CV])

    [http://arxiv.org/abs/2305.11846](http://arxiv.org/abs/2305.11846)

    CoDi是一种生成模型，通过在输入空间和输出空间中进行模态对齐，实现了可组合生成策略，从而可以生成任意组合的输出模态。 CoDi 非常灵活，能够生成多个模态，如图像、视频、语言和音频，甚至在训练数据中不存在的模态组合。

    

    本文提出了一种新的生成模型，称之为可组合扩散（CoDi）。这个模型能够从任意输入模态和任意组合中生成各种输出模态，如语言、图像、视频或音频。与现有的生成 AI 系统不同，CoDi 可以同时生成多个模态，并且其输入不局限于文本或图像的子集。尽管很多模态的组合缺乏训练数据集，但我们提出了在输入空间和输出空间中进行模态对齐的方法，从而使 CoDi 可以自由地对任何输入组合进行条件生成，并生成任何模态组合，即使这些组合不在训练数据中。CoDi采用了一种新的可组合生成策略，通过在扩散过程中构建共享的多模态空间来实现对齐，从而实现了交织模态的同步生成，例如时间上对齐的视频和音频。高度可定制和灵活，CoDi 实现了强大的联合模态生成。

    We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality gen
    
[^5]: AI 的表现形式：南亚文字到图像模型的社区中心研究

    AI's Regimes of Representation: A Community-centered Study of Text-to-Image Models in South Asia. (arXiv:2305.11844v1 [cs.CY])

    [http://arxiv.org/abs/2305.11844](http://arxiv.org/abs/2305.11844)

    本文探讨南亚文化背景下文字到图像模型的文化限制，并将其归因于全球和地区权力不平等所塑造的外来视角。通过将社区视为专家，对T2I的限制进行研究，深入了解文化特定的AI技术在非西方和南球地区上失败的原因。建议负责任地开发T2I模型，以允许对结构性不平等性的认识。

    

    本文提出了一种社区中心的研究方法，来探讨南亚文化背景下文字到图像（T2I）模型的文化限制。我们使用支配性的媒体表现形式的学术理论来阐述这些失败，并将它们定位于参与者对他们现有社会边缘化的报告上。因此，我们展示了生成AI会再现一种外部人眼中看待南亚文化的视角，这种视角是由全球和地区权力不平等所塑造的。通过将社区视为专家并征求他们对T2I限制的看法，我们的研究为现有的评估框架增添了丰富的细节，并加深了我们对文化特定的AI技术在非西方和南球地区上失败的理解。我们总结出了负责任地开发T2I模型的教训，推荐具体的前进路径，以允许对结构性不平等性的认识。

    This paper presents a community-centered study of cultural limitations of text-to-image (T2I) models in the South Asian context. We theorize these failures using scholarship on dominant media regimes of representations and locate them within participants' reporting of their existing social marginalizations. We thus show how generative AI can reproduce an outsiders gaze for viewing South Asian cultures, shaped by global and regional power inequities. By centering communities as experts and soliciting their perspectives on T2I limitations, our study adds rich nuance into existing evaluative frameworks and deepens our understanding of the culturally-specific ways AI technologies can fail in non-Western and Global South settings. We distill lessons for responsible development of T2I models, recommending concrete pathways forward that can allow for recognition of structural inequalities.
    
[^6]: 神经网络训练的复杂性及ETR: 有效连续函数拓展

    Complexity of Neural Network Training and ETR: Extensions with Effectively Continuous Functions. (arXiv:2305.11833v1 [cs.LO])

    [http://arxiv.org/abs/2305.11833](http://arxiv.org/abs/2305.11833)

    本文研究了神经网络训练问题的复杂性，证明了sigmoid激活函数与实数存在性理论和指数函数相关，这使得神经网络在使用sigmoid激活函数时算法可解性存在疑问。同时，使用正弦激活函数时训练问题是不可判定的。

    

    本文研究了通过不同激活函数定义的神经网络训练问题的复杂性。对于线性和ReLU激活函数而言，已知训练问题是存在R-完备的。我们考虑了sigmoid激活函数和其他有效连续函数的问题复杂性。我们证明了这些训练问题可在多项式时间的 many-one 可还原到关于相应激活函数拓展的实数存在性理论上。特别地，我们证明了sigmoid激活函数导致了带有指数函数的实数存在性理论。因此，使用sigmoid激活函数训练神经网络是否具有算法可解性以及实数存在性理论和指数函数的可判定性等问题都是开放的。相反，在使用正弦激活函数时，我们发现训练问题是不可判定的。

    We study the complexity of the problem of training neural networks defined via various activation functions. The training problem is known to be existsR-complete with respect to linear activation functions and the ReLU activation function. We consider the complexity of the problem with respect to the sigmoid activation function and other effectively continuous functions. We show that these training problems are polynomial-time many-one bireducible to the existential theory of the reals extended with the corresponding activation functions. In particular, we establish that the sigmoid activation function leads to the existential theory of the reals with the exponential function. It is thus open, and equivalent with the decidability of the existential theory of the reals with the exponential function, whether training neural networks using the sigmoid activation function is algorithmically solvable. In contrast, we obtain that the training problem is undecidable if sinusoidal activation f
    
[^7]: 通过正则化流和相关性分析改进多模态联合变分自编码器

    Improving Multimodal Joint Variational Autoencoders through Normalizing Flows and Correlation Analysis. (arXiv:2305.11832v1 [stat.ML])

    [http://arxiv.org/abs/2305.11832](http://arxiv.org/abs/2305.11832)

    本文提出了一种改进的多模态联合变分自编码器，利用正则化流和相关性分析技术，实现了更加连贯的跨模态生成，更多样化的数据生成，同时可扩展到任意数量的模态。

    

    我们提出了一种新的多模态变分自编码器，能够从联合分布生成并针对任意数量的复杂模态进行条件生成。单模后验分布是基于保留跨模态共享信息的深度典型相关分析嵌入进行条件生成的，从而实现了更连贯的跨模态生成。此外，我们使用正则化流来丰富单模后验分布，实现了更多样化的数据生成。最后，我们提出使用专家乘积来从多个模态中推断一个模态，从而使得模型可扩展到任意数量的模态。我们在几个数据集上证明了我们的方法改善了似然度估计、代表性的生成和在条件生成中特别是连贯性指标的性能。

    We propose a new multimodal variational autoencoder that enables to generate from the joint distribution and conditionally to any number of complex modalities. The unimodal posteriors are conditioned on the Deep Canonical Correlation Analysis embeddings which preserve the shared information across modalities leading to more coherent cross-modal generations. Furthermore, we use Normalizing Flows to enrich the unimodal posteriors and achieve more diverse data generation. Finally, we propose to use a Product of Experts for inferring one modality from several others which makes the model scalable to any number of modalities. We demonstrate that our method improves likelihood estimates, diversity of the generations and in particular coherence metrics in the conditional generations on several datasets.
    
[^8]: 自动温度调整的软性演员评论算法的正则化

    Regularization of Soft Actor-Critic Algorithms with Automatic Temperature Adjustment. (arXiv:2305.11831v1 [cs.LG])

    [http://arxiv.org/abs/2305.11831](http://arxiv.org/abs/2305.11831)

    本文提出了正则化自动温度调整的软性演员评论算法，增加了对原理的明确性。

    

    本文提出了一种使用自动温度调整的软性演员评论（SAC）算法的正则化方法，并对策略评估、策略改进和温度调整进行重新定义和修改，以更加明确地阐述原理。

    This work presents a comprehensive analysis to regularize the Soft Actor-Critic (SAC) algorithm with automatic temperature adjustment. The the policy evaluation, the policy improvement and the temperature adjustment are reformulated, addressing certain modification and enhancing the clarity of the original theory in a more explicit manner.
    
[^9]: 私有集成模型的公平影响研究

    On the Fairness Impacts of Private Ensembles Models. (arXiv:2305.11807v1 [cs.LG])

    [http://arxiv.org/abs/2305.11807](http://arxiv.org/abs/2305.11807)

    本文探讨了私有教师集成（PATE）模型是否会导致不公平性，并证明它可能导致不同群体之间的准确性差异。建议在PATE的应用中加入公平性考虑，以减少不公平性的影响。

    

    私有教师集成（PATE）是一种机器学习框架，通过多个“教师”模型和一个“学生”模型的组合来创建私有模型。学生模型学习预测基于教师的投票的输出，生成的模型满足差分隐私。已经证明PATE在半监督设置或保护数据标签是优先的情况下创建私有模型是有效的。本文探讨了使用PATE是否会导致不公平，并证明它可能导致不同群体之间的准确性差异。本文还分析了算法和数据属性对这些不成比例的影响的贡献，以及为什么这些方面会不成比例地影响不同的群体，并提出了缓解这些影响的建议。

    The Private Aggregation of Teacher Ensembles (PATE) is a machine learning framework that enables the creation of private models through the combination of multiple "teacher" models and a "student" model. The student model learns to predict an output based on the voting of the teachers, and the resulting model satisfies differential privacy. PATE has been shown to be effective in creating private models in semi-supervised settings or when protecting data labels is a priority. This paper explores whether the use of PATE can result in unfairness, and demonstrates that it can lead to accuracy disparities among groups of individuals. The paper also analyzes the algorithmic and data properties that contribute to these disproportionate impacts, why these aspects are affecting different groups disproportionately, and offers recommendations for mitigating these effects
    
[^10]: PANNA 2.0: 高效的神经网络原子间势及其新架构（arXiv:2305.11805v1 [physics.comp-ph]）

    PANNA 2.0: Efficient neural network interatomic potentials and new architectures. (arXiv:2305.11805v1 [physics.comp-ph])

    [http://arxiv.org/abs/2305.11805](http://arxiv.org/abs/2305.11805)

    PANNA 2.0是一个基于神经网络的原子间势生成代码，具有更好的GPU支持和自定义工具，并通过变分电荷均衡方案解决了长程静电相互作用问题。

    

    我们介绍了PANNA 2.0（基于局部原子描述符和多层感知机的神经网络原子间势生成代码）的最新版本。该版本基于新的后端构建，具有改进的工具，可自定义和监测网络训练，支持更好的GPU，包括快速描述符计算器，针对外部代码的新插件以及通过变分电荷均衡方案包括长程静电相互作用的新架构。我们总结了新代码的主要特点，并对 PANNA 模型的精度进行了若干基准测试，比较其与现有技术的表现，包括常用基准数据集和丰富数据集。

    We present the latest release of PANNA 2.0 (Properties from Artificial Neural Network Architectures), a code for the generation of neural network interatomic potentials based on local atomic descriptors and multilayer perceptrons. Built on a new back end, this new release of PANNA features improved tools for customizing and monitoring network training, better GPU support including a fast descriptor calculator, new plugins for external codes and a new architecture for the inclusion of long-range electrostatic interactions through a variational charge equilibration scheme. We present an overview of the main features of the new code, and several benchmarks comparing the accuracy of PANNA models to the state of the art, on commonly used benchmarks as well as richer datasets.
    
[^11]: 概率流ODE可证明速度快

    The probability flow ODE is provably fast. (arXiv:2305.11798v1 [cs.LG])

    [http://arxiv.org/abs/2305.11798](http://arxiv.org/abs/2305.11798)

    首次提供概率流ODE实现得到多项式时间收敛保证的证明，使用欠阻尼Langevin扩散的特殊选择的校正步骤，获得了更好的维度依赖性，凸显了ODE框架的潜在优势。

    

    我们首次提供概率流ODE实现（连同校正步骤）得到多项式时间收敛保证的证明，用于基于分数生成建模。我们的分析是在最近的结果基础上进行的，该结果获得了基于SDE的实现（即去噪扩散概率建模或DDPM）的这样的保证，但需要开发新的技术来研究无收缩的确定性动态。通过使用基于欠阻尼Langevin扩散的特殊选择的校正步骤，我们获得了比DDPM之前作品更好的维度依赖性（假设数据分布平滑，为$ O（\sqrt {d}）$而不是$ O（d）$），凸显了ODE框架的潜在优势。

    We provide the first polynomial-time convergence guarantees for the probability flow ODE implementation (together with a corrector step) of score-based generative modeling. Our analysis is carried out in the wake of recent results obtaining such guarantees for the SDE-based implementation (i.e., denoising diffusion probabilistic modeling or DDPM), but requires the development of novel techniques for studying deterministic dynamics without contractivity. Through the use of a specially chosen corrector step based on the underdamped Langevin diffusion, we obtain better dimension dependence than prior works on DDPM ($O(\sqrt{d})$ vs. $O(d)$, assuming smoothness of the data distribution), highlighting potential advantages of the ODE framework.
    
[^12]: 稳定性边缘处的逻辑回归梯度下降的隐式偏差

    Implicit Bias of Gradient Descent for Logistic Regression at the Edge of Stability. (arXiv:2305.11788v1 [cs.LG])

    [http://arxiv.org/abs/2305.11788](http://arxiv.org/abs/2305.11788)

    本文研究了逻辑回归常数步长梯度下降在稳定性边缘的收敛性和隐式偏差，证明了逻辑损失可以通过任何常数步长的梯度下降进行最小化，同时也发现了指数损失下的发散性问题，强调了稳定性边缘下梯度下降的不稳定性。

    

    最近的研究表明，在机器学习优化中，梯度下降 (GD) 经常在稳定性边缘 (EoS) [Cohen 等，2021] 运行，其中步长被设置为大，导致由 GD 迭代引起的非单调损失。本文研究在 EoS 区域内使用常数步长 GD 进行逻辑回归的收敛性和隐式偏差，对于线性可分的数据。尽管存在局部振荡，我们证明逻辑损失可以通过任何常数步长的 GD 在长时间尺度上进行最小化。此外，我们证明，在任何常数步长下，当投影到最大边际方向 (硬边 SVM 方向) 时，GD 迭代趋向于无穷大，并在投影到最大边缘的正交补空间时，收敛于最小化强凸势能的固定向量。相反，我们也表明，在 EoS 区域，GD 迭代可能在指数损失下发生灾难性发散，突显了 EoS 区域中 GD 的不稳定性。

    Recent research has observed that in machine learning optimization, gradient descent (GD) often operates at the edge of stability (EoS) [Cohen, et al., 2021], where the stepsizes are set to be large, resulting in non-monotonic losses induced by the GD iterates. This paper studies the convergence and implicit bias of constant-stepsize GD for logistic regression on linearly separable data in the EoS regime. Despite the presence of local oscillations, we prove that the logistic loss can be minimized by GD with any constant stepsize over a long time scale. Furthermore, we prove that with any constant stepsize, the GD iterates tend to infinity when projected to a max-margin direction (the hard-margin SVM direction) and converge to a fixed vector that minimizes a strongly convex potential when projected to the orthogonal complement of the max-margin direction. In contrast, we also show that in the EoS regime, GD iterates may diverge catastrophically under the exponential loss, highlighting t
    
[^13]: 《跨语种监督提高大型语言模型预训练质量》

    Cross-Lingual Supervision improves Large Language Models Pre-training. (arXiv:2305.11778v1 [cs.CL])

    [http://arxiv.org/abs/2305.11778](http://arxiv.org/abs/2305.11778)

    本文表明，将大型语言模型的预训练中使用跨语种并行数据能提高其上下文学习能力；同时，提出了一种简单且有效的策略来学习两个目标之间的最佳混合比例。

    

    近期大型语言模型的迅速进展主要依赖于使用自监督语言建模目标（如下一个标记预测或跨度损坏）。与此相反，机器翻译系统主要使用需要源语言和目标语言之间对齐数据的跨语种监督进行训练。我们证明，在预训练过程中，将大型语言模型使用自监督语言建模目标和受监督的机器翻译目标混合，因此在预训练过程中包含跨语种并行数据，可产生更好的上下文学习能力。由于预训练是非常资源密集的过程，而在两个目标之间进行最佳混合比例的网格搜索是非常昂贵的，我们提出了一种简单而有效的策略来在预训练中学习这些知识。

    The recent rapid progress in pre-training Large Language Models has relied on using self-supervised language modeling objectives like next token prediction or span corruption. On the other hand, Machine Translation Systems are mostly trained using cross-lingual supervision that requires aligned data between source and target languages. We demonstrate that pre-training Large Language Models on a mixture of a self-supervised Language Modeling objective and the supervised Machine Translation objective, therefore including cross-lingual parallel data during pre-training, yields models with better in-context learning abilities. As pre-training is a very resource-intensive process and a grid search on the best mixing ratio between the two objectives is prohibitively expensive, we propose a simple yet effective strategy to learn it during pre-training.
    
[^14]: 使用R2效用的多目标优化

    Multi-Objective Optimization Using the R2 Utility. (arXiv:2305.11774v1 [math.OC])

    [http://arxiv.org/abs/2305.11774](http://arxiv.org/abs/2305.11774)

    本文提出将多目标优化问题转化为一组单目标问题进行解决，并介绍了R2效用函数作为适当的目标函数。该效用函数单调且次模，可以使用贪心优化算法计算全局最优解。

    

    多目标优化的目标是确定描述多目标之间最佳权衡的点集合。为了解决这个矢量值优化问题，从业者常常使用标量化函数将多目标问题转化为一组单目标问题。这组标量化问题可以使用传统的单目标优化技术来解决。在这项工作中，我们将这个约定形式化为一个通用的数学框架。我们展示了这种策略如何有效地将原始的多目标优化问题重新转化为定义在集合上的单目标优化问题。针对这个新问题的适当类别的目标函数是R2效用函数，它被定义为标量化优化问题的加权积分。我们证明了这个效用函数是单调的和次模的集合函数，可以通过贪心优化算法有效地计算出全局最优解。

    The goal of multi-objective optimization is to identify a collection of points which describe the best possible trade-offs between the multiple objectives. In order to solve this vector-valued optimization problem, practitioners often appeal to the use of scalarization functions in order to transform the multi-objective problem into a collection of single-objective problems. This set of scalarized problems can then be solved using traditional single-objective optimization techniques. In this work, we formalise this convention into a general mathematical framework. We show how this strategy effectively recasts the original multi-objective optimization problem into a single-objective optimization problem defined over sets. An appropriate class of objective functions for this new problem is the R2 utility function, which is defined as a weighted integral over the scalarized optimization problems. We show that this utility function is a monotone and submodular set function, which can be op
    
[^15]: 图上的转移算子：谱聚类及其扩展

    Transfer operators on graphs: Spectral clustering and beyond. (arXiv:2305.11766v1 [stat.ML])

    [http://arxiv.org/abs/2305.11766](http://arxiv.org/abs/2305.11766)

    本文介绍了在图上定义的转移算子及其谱特性，提出了基于广义转移算子的有向图聚类算法，并证明了算法有效性。

    

    图和网络在建模和分析复杂的相关系统中发挥着重要作用，例如交通网络，集成电路，电力网格，引文图以及生物和人工神经网络。本文在图上定义了转移算子，如Koopman算子和Perron-Frobenius算子，研究了它们的谱特性，引入了这些算子的Galerkin投影，并说明了如何从数据中估计降低表示。特别地，我们展示了无向图谱聚类可以被解释为Koopman算子的特征函数，并提出了基于广义转移算子的有向图聚类算法。我们在几个基准问题上证明了所得算法的有效性，并提供了不同聚类的解释。

    Graphs and networks play an important role in modeling and analyzing complex interconnected systems such as transportation networks, integrated circuits, power grids, citation graphs, and biological and artificial neural networks. Graph clustering algorithms can be used to detect groups of strongly connected vertices and to derive coarse-grained models. We define transfer operators such as the Koopman operator and the Perron-Frobenius operator on graphs, study their spectral properties, introduce Galerkin projections of these operators, and illustrate how reduced representations can be estimated from data. In particular, we show that spectral clustering of undirected graphs can be interpreted in terms of eigenfunctions of the Koopman operator and propose novel clustering algorithms for directed graphs based on generalized transfer operators. We demonstrate the efficacy of the resulting algorithms on several benchmark problems and provide different interpretations of clusters.
    
[^16]: 半空间的测试学习器：通用算法

    Tester-Learners for Halfspaces: Universal Algorithms. (arXiv:2305.11765v1 [cs.LG])

    [http://arxiv.org/abs/2305.11765](http://arxiv.org/abs/2305.11765)

    本文提出了第一个成功的通用半空间测试学习器，可以在广泛结构化的分布上工作，实现误差$ O（\mathrm {opt}）+\ \epsilon $。

    

    本文提出了第一个在广泛结构化分布上成功的半空间测试学习器，该通用测试学习器在完全多项式时间内运行，并具有以下保证：学习器在测试器接受的任何标记分布上实现错误$ O（\mathrm {opt}）+\  \epsilon $，此外，测试器在边缘分布是满足Poincar\'e不等式的任何分布时都可以接受。与之前在可测试学习方面的工作不同的是，我们的测试器没有针对任何单一目标分布进行调整，而是对一整个目标分布类成功。Poincar\'e分布类包括所有强对数凹分布，并且，如果假设Kannan-L\'{o}vasz-Simonovits（KLS）猜想，则包括所有对数凹分布。在标签噪声已知为Massart的特殊情况下，我们的测试学习器在不受条件限制的情况下接受所有对数凹分布，并实现误差$ \mathrm {opt} +\  \epsilon $。

    We give the first tester-learner for halfspaces that succeeds universally over a wide class of structured distributions. Our universal tester-learner runs in fully polynomial time and has the following guarantee: the learner achieves error $O(\mathrm{opt}) + \epsilon$ on any labeled distribution that the tester accepts, and moreover, the tester accepts whenever the marginal is any distribution that satisfies a Poincar\'e inequality. In contrast to prior work on testable learning, our tester is not tailored to any single target distribution but rather succeeds for an entire target class of distributions. The class of Poincar\'e distributions includes all strongly log-concave distributions, and, assuming the Kannan--L\'{o}vasz--Simonovits (KLS) conjecture, includes all log-concave distributions. In the special case where the label noise is known to be Massart, our tester-learner achieves error $\mathrm{opt} + \epsilon$ while accepting all log-concave distributions unconditionally (withou
    
[^17]: 分类隐藏马尔可夫模型的边际束搜索算法

    Marginalized Beam Search Algorithms for Hierarchical HMMs. (arXiv:2305.11752v1 [cs.LG])

    [http://arxiv.org/abs/2305.11752](http://arxiv.org/abs/2305.11752)

    本文提出了两种新算法，分别是贪心边缘束搜索算法和局部焦点束搜索算法，用于解决应用于层次隐藏马尔可夫模型时维特比算法和束搜索算法的局限性，它们能够更好地近似最可能的外部状态序列。

    

    从一系列测量数据中推断状态序列是生物信息学和自然语言处理中的基本问题。维特比算法和束搜索算法是流行的推断方法，但是当应用于层次隐藏马尔可夫模型（HHMM）时，它们具有局限性，因为感兴趣的是外部状态序列。维特比算法无法在没有内部状态的情况下推断外部状态，而束搜索算法需要对过多的状态空间进行边际化。我们提出了两种新算法来克服这些局限性：贪心边缘束搜索算法和局部焦点束搜索算法。我们证明它们比维特比算法更优秀地近似了最可能的外部状态序列，并且我们使用模拟和纳米孔基调用数据评估了这些算法的性能。

    Inferring a state sequence from a sequence of measurements is a fundamental problem in bioinformatics and natural language processing. The Viterbi and the Beam Search (BS) algorithms are popular inference methods, but they have limitations when applied to Hierarchical Hidden Markov Models (HHMMs), where the interest lies in the outer state sequence. The Viterbi algorithm can not infer outer states without inner states, while the BS algorithm requires marginalization over prohibitively large state spaces. We propose two new algorithms to overcome these limitations: the greedy marginalized BS algorithm and the local focus BS algorithm. We show that they approximate the most likely outer state sequence with higher performance than the Viterbi algorithm, and we evaluate the performance of these algorithms on an explicit duration HMM with simulation and nanopore base calling data.
    
[^18]: MedLens: 通过选择医学体征和回归插值来提高死亡率预测

    MedLens: Improve mortality prediction via medical signs selecting and regression interpolation. (arXiv:2305.11742v1 [cs.LG])

    [http://arxiv.org/abs/2305.11742](http://arxiv.org/abs/2305.11742)

    本文介绍了一个自动选择医学体征和回归插值的方法（MedLens），用于解决电子病历中医学体征数据缺失率过高导致预测性能降低的问题。

    

    监测患者的健康状况并提前预测死亡率对及时提供患者护理和治疗至关重要。电子病历中的大量医学体征被用于先进的机器学习模型来进行预测。然而，原始临床体征的数据质量问题在文献中被较少讨论。通过对各种医学体征和大量患者住院记录中的缺失率和相关分数进行深入测量，我们发现综合缺失率非常高，大量无用的体征可能会损害预测模型的性能。我们得出结论，只有改善数据质量才能提高不同预测算法的基线准确性。我们设计了MedLens，通过统计自动选择重要医学体征，并使用灵活的插值方法处理高缺失率时间序列。

    Monitoring the health status of patients and predicting mortality in advance is vital for providing patients with timely care and treatment. Massive medical signs in electronic health records (EHR) are fitted into advanced machine learning models to make predictions. However, the data-quality problem of original clinical signs is less discussed in the literature. Based on an in-depth measurement of the missing rate and correlation score across various medical signs and a large amount of patient hospital admission records, we discovered the comprehensive missing rate is extremely high, and a large number of useless signs could hurt the performance of prediction models. Then we concluded that only improving data-quality could improve the baseline accuracy of different prediction algorithms. We designed MEDLENS, with an automatic vital medical signs selection approach via statistics and a flexible interpolation approach for high missing rate time series. After augmenting the data-quality 
    
[^19]: 动态和自适应遗憾保证的非平稳无投影在线学习

    Non-stationary Projection-free Online Learning with Dynamic and Adaptive Regret Guarantees. (arXiv:2305.11726v1 [cs.LG])

    [http://arxiv.org/abs/2305.11726](http://arxiv.org/abs/2305.11726)

    本文研究了非平稳无投影在线学习，并提出了动态遗憾和自适应遗憾来衡量性能，首次提出了该方法的动态遗憾边界和自适应遗憾上限。

    

    由于其解决具有复杂约束的高维问题的效率，无投影在线学习引起了越来越多的关注。然而，大多数现有的无投影在线方法都集中在最小化静态遗憾上，这很不幸地无法捕捉到变化环境的挑战。在本文中，我们研究了非平稳的无投影在线学习，并选择动态遗憾和自适应遗憾来衡量性能。具体而言，我们首先为现有的名为$ \text {BOGD} _ \text {IP} $的无投影在线方法提供了一种新的动态遗憾分析，并建立了一个$ \mathcal {O}（T ^ {3/4}（1 + P_T））$的动态遗憾上限，其中$ P_T $表示比较器序列的路径长度。然后，我们通过并行运行多个具有不同步长的$ \text {BOGD} _ \text {IP} $算法，并实时跟踪最佳算法，将上限改进为$ \mathcal {O}（T ^ {3/4}（1 + P_T）^ {1/4}）$。我们的结果是投影-free在线学习方法的第一个一般情况的动态遗憾边界。最后，我们将我们的方法扩展到自适应遗憾，它容忍比较器序列和损失函数的动态变化。我们的方法实现了$ \mathcal {O}（T ^ {2/3}（1 + P_T）^ {1/ 3}）$的自适应遗憾界限，几乎与下限相匹配，最多只相差一个对数因子。

    Projection-free online learning has drawn increasing interest due to its efficiency in solving high-dimensional problems with complicated constraints. However, most existing projection-free online methods focus on minimizing the static regret, which unfortunately fails to capture the challenge of changing environments. In this paper, we investigate non-stationary projection-free online learning, and choose dynamic regret and adaptive regret to measure the performance. Specifically, we first provide a novel dynamic regret analysis for an existing projection-free method named $\text{BOGD}_\text{IP}$, and establish an $\mathcal{O}(T^{3/4}(1+P_T))$ dynamic regret bound, where $P_T$ denotes the path-length of the comparator sequence. Then, we improve the upper bound to $\mathcal{O}(T^{3/4}(1+P_T)^{1/4})$ by running multiple $\text{BOGD}_\text{IP}$ algorithms with different step sizes in parallel, and tracking the best one on the fly. Our results are the first general-case dynamic regret bou
    
[^20]: 下一个会是什么？评估神经文本生成器的不确定性与人类生产变异性对比

    What Comes Next? Evaluating Uncertainty in Neural Text Generators Against Human Production Variability. (arXiv:2305.11707v1 [cs.CL])

    [http://arxiv.org/abs/2305.11707](http://arxiv.org/abs/2305.11707)

    本文分析了神经文本生成器与人类生产变异性之间的不确定性，通过探测生成器的输出空间来测量其对人类生产变异性的校准程度，并证明用多个样本和多个参考可以更好地了解模型的不确定性表示。

    

    在自然语言生成（NLG）任务中，针对任何输入，存在多个可行的交际目标，并且可以用多种方式将任何目标用语言表达出来或进行生产。我们表征了人类生产在四个NLG任务中词汇、句法和语义方面的变异程度，并将人类生产变异性与不确定性联系起来。然后，我们检查了生成系统预测的概率分布和解码算法所形成的输出字符串空间，以探究其不确定性。针对每个测试输入，我们测量了生成器对人类生产变异性的校准程度。通过这种基于实例级别的方法，我们分析了NLG模型和解码策略，证明用多个样本和多个参考对生成器进行探测，提供了理解模型不确定性表示所必需的详细级别。

    In Natural Language Generation (NLG) tasks, for any input, multiple communicative goals are plausible, and any goal can be put into words, or produced, in multiple ways. We characterise the extent to which human production varies lexically, syntactically, and semantically across four NLG tasks, connecting human production variability to aleatoric or data uncertainty. We then inspect the space of output strings shaped by a generation system's predicted probability distribution and decoding algorithm to probe its uncertainty. For each test input, we measure the generator's calibration to human production variability. Following this instance-level approach, we analyse NLG models and decoding strategies, demonstrating that probing a generator with multiple samples and, when possible, multiple references, provides the level of detail necessary to gain understanding of a model's representation of uncertainty.
    
[^21]: S-JEA: 堆叠联合嵌入结构用于自监督视觉表示学习

    S-JEA: Stacked Joint Embedding Architectures for Self-Supervised Visual Representation Learning. (arXiv:2305.11701v1 [cs.CV])

    [http://arxiv.org/abs/2305.11701](http://arxiv.org/abs/2305.11701)

    本文提出了使用堆叠联合嵌入结构学习高度可分离的分层语义表示，显示出更明显的语义概念子类，并且与传统方法相似。

    

    自监督学习作为学习图像表示的基本范式，近年来已经在各种任务中展示了高度的实证成功。然而，大多数自监督学习方法未能学习到捕获分层语义概念的嵌入，这些概念是可分离和可解释的。本文旨在通过堆叠联合嵌入结构（JEA）来学习高度可分离的分层语义表示，其中较高级别的JEA使用较低级别JEA的表示结果作为输入。这导致表示空间表现出更明显的语义概念子类（如车辆的型号和颜色）在较高级别的JEA中。我们实验性地展示了堆叠JEA的表示与传统JEA相似，并展示了表示空间以验证语义分层结构。

    The recent emergence of Self-Supervised Learning (SSL) as a fundamental paradigm for learning image representations has, and continues to, demonstrate high empirical success in a variety of tasks. However, most SSL approaches fail to learn embeddings that capture hierarchical semantic concepts that are separable and interpretable. In this work, we aim to learn highly separable semantic hierarchical representations by stacking Joint Embedding Architectures (JEA) where higher-level JEAs are input with representations of lower-level JEA. This results in a representation space that exhibits distinct sub-categories of semantic concepts (e.g., model and colour of vehicles) in higher-level JEAs. We empirically show that representations from stacked JEA perform on a similar level as traditional JEA with comparative parameter counts and visualise the representation spaces to validate the semantic hierarchies.
    
[^22]: RGCVAE：基于关系图条件化可变自编码器的分子设计

    RGCVAE: Relational Graph Conditioned Variational Autoencoder for Molecule Design. (arXiv:2305.11699v1 [cs.LG])

    [http://arxiv.org/abs/2305.11699](http://arxiv.org/abs/2305.11699)

    本文提出了RGCVAE，一种基于关系图条件化的可变自编码器，可以有效、高效地进行分子设计，并在两个数据集上表现出了先进的生成性能和快速的训练速度。

    

    确定表现出某些预定特性的分子是难以解决的问题。近年来，深度生成模型已被用于分子生成。深度图变分自编码器是最强大的机器学习工具之一，可用于解决此问题。然而，现有方法往往难以捕捉真实的数据分布，并且倾向于计算成本高。在本文中，我们提出了一种高效且有效的基于关系图同构网络的图变分自编码器RGCVAE：（i）利用全新的强大关系图同构网络的编码网络；（ii）一种新颖的概率解码组件。在两个广泛采用的数据集上，与数种最先进的VAE方法相比，RGCVAE表现出最先进的分子生成性能，同时训练速度显著加快。

    Identifying molecules that exhibit some pre-specified properties is a difficult problem to solve. In the last few years, deep generative models have been used for molecule generation. Deep Graph Variational Autoencoders are among the most powerful machine learning tools with which it is possible to address this problem. However, existing methods struggle in capturing the true data distribution and tend to be computationally expensive. In this work, we propose RGCVAE, an efficient and effective Graph Variational Autoencoder based on: (i) an encoding network exploiting a new powerful Relational Graph Isomorphism Network; (ii) a novel probabilistic decoding component. Compared to several state-of-the-art VAE methods on two widely adopted datasets, RGCVAE shows state-of-the-art molecule generation performance while being significantly faster to train.
    
[^23]: 带有门控视觉-语言嵌入的Transformer用于机器人手术中的视觉问答

    Surgical-VQLA: Transformer with Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery. (arXiv:2305.11692v1 [cs.CV])

    [http://arxiv.org/abs/2305.11692](http://arxiv.org/abs/2305.11692)

    本文提出了Surgical-VQLA方法，结合Transformer模型和门控视觉-语言嵌入，解决了手术VQA中对象检测稀缺、异构模态融合策略不足、定位答案缺失等问题，并在测试中实现了最好的表现。

    

    尽管存在着计算机辅助模拟器和手术过程的录制视频，但初级住院医师仍然严重依赖专家来回答他们的问题。然而，专家外科医生通常承担着临床和学术工作，限制了他们回答问题的时间。为此，我们开发了一种手术问答系统，以便从录制的视频中促进机器人辅助手术场景和活动理解。本文提出了一种将Transformer模型与门控视觉-语言嵌入相结合的机器人手术视觉问答方法，解决了手术对象检测模型稀缺、异构模态融合策略不足、缺失定位答案等问题，并在基准手术VQA数据集上实现了最先进的性能。

    Despite the availability of computer-aided simulators and recorded videos of surgical procedures, junior residents still heavily rely on experts to answer their queries. However, expert surgeons are often overloaded with clinical and academic workloads and limit their time in answering. For this purpose, we develop a surgical question-answering system to facilitate robot-assisted surgical scene and activity understanding from recorded videos. Most of the existing VQA methods require an object detector and regions based feature extractor to extract visual features and fuse them with the embedded text of the question for answer generation. However, (1) surgical object detection model is scarce due to smaller datasets and lack of bounding box annotation; (2) current fusion strategy of heterogeneous modalities like text and image is naive; (3) the localized answering is missing, which is crucial in complex surgical scenarios. In this paper, we propose Visual Question Localized-Answering in
    
[^24]: 回收和精馏：带有注意力映射重用和蒸馏屏蔽的基于Transformer的语音自监督学习模型的通用压缩策略

    Recycle-and-Distill: Universal Compression Strategy for Transformer-based Speech SSL Models with Attention Map Reusing and Masking Distillation. (arXiv:2305.11685v1 [eess.AS])

    [http://arxiv.org/abs/2305.11685](http://arxiv.org/abs/2305.11685)

    本研究提出了一种基于Transformer的语音自监督学习模型的通用压缩策略，通过重用注意力映射和蒸馏屏蔽来提高学生模型的语音表示质量，实现了较低的错误率。

    

    基于Transformer的语音自监督学习模型在各种语音处理任务中表现出惊人的性能。然而，语音 SSL 模型中庞大的参数数量需要压缩成更紧凑的模型，以便在学术界或小公司中更广泛地使用。本研究建议重用Transformer层之间的注意力映射，因此可以删除键和查询参数，同时保留层数。此外，我们提出了一种新的蒸馏策略，以提高学生模型的语音表示质量。我们扩展了蒸馏损失，利用遮罩和未遮罩的语音帧，充分利用教师模型的高质量表示。我们的通用压缩策略产生的学生模型在SUPERB基准测试中实现了7.72%的音素误差率（PER）和9.96%的单词错误率（WER）。

    Transformer-based speech self-supervised learning (SSL) models, such as HuBERT, show surprising performance in various speech processing tasks. However, huge number of parameters in speech SSL models necessitate the compression to a more compact model for wider usage in academia or small companies. In this study, we suggest to reuse attention maps across the Transformer layers, so as to remove key and query parameters while retaining the number of layers. Furthermore, we propose a novel masking distillation strategy to improve the student model's speech representation quality. We extend the distillation loss to utilize both masked and unmasked speech frames to fully leverage the teacher model's high-quality representation. Our universal compression strategy yields the student model that achieves phoneme error rate (PER) of 7.72% and word error rate (WER) of 9.96% on the SUPERB benchmark.
    
[^25]: 自我强化注意机制用于表格学习

    Self-Reinforcement Attention Mechanism For Tabular Learning. (arXiv:2305.11684v1 [cs.LG])

    [http://arxiv.org/abs/2305.11684](http://arxiv.org/abs/2305.11684)

    本文提出了自我强化注意机制（SRA）用于处理表格学习，该机制通过逐元素向量乘法以加强或减弱原始输入的某些组件来学习可理解的特征表示。

    

    除了机器学习模型的高精度外，许多研究者在处理现实中的问题（如欺诈检测、信用评分）时，更关注于发现数据中的隐藏模式，特别是当面对具有挑战性的不平衡特征时。解释能力也是一个关键要求，必须伴随使用的机器学习模型。在这方面，通常会优先选择本质上具有可解释性的模型，而不是那些在大多数情况下都是黑匣子模型的复杂模型。即使需要牺牲性能，一些高风险领域也使用线性模型来处理表格数据。本文介绍了自我强化注意机制（Self-Reinforcement Attention, SRA），它提供了一种将相关性转化为特征权重向量的新颖注意机制，该权重向量用于学习可理解的表达。然后，该权重用于通过逐元素向量乘法来增强或减少原始输入的某些组件。我们在合成和真实的不平衡数据集上得到了实验结果。

    Apart from the high accuracy of machine learning models, what interests many researchers in real-life problems (e.g., fraud detection, credit scoring) is to find hidden patterns in data; particularly when dealing with their challenging imbalanced characteristics. Interpretability is also a key requirement that needs to accompany the used machine learning model. In this concern, often, intrinsically interpretable models are preferred to complex ones, which are in most cases black-box models. Also, linear models are used in some high-risk fields to handle tabular data, even if performance must be sacrificed. In this paper, we introduce Self-Reinforcement Attention (SRA), a novel attention mechanism that provides a relevance of features as a weight vector which is used to learn an intelligible representation. This weight is then used to reinforce or reduce some components of the raw input through element-wise vector multiplication. Our results on synthetic and real-world imbalanced data s
    
[^26]: 基于语音的灵感事件感知：深度学习与语言方法的比较

    Sensing of inspiration events from speech: comparison of deep learning and linguistic methods. (arXiv:2305.11683v1 [cs.SD])

    [http://arxiv.org/abs/2305.11683](http://arxiv.org/abs/2305.11683)

    本研究比较了使用深度学习和基于语言的方法来感知胸带传感器数据中的吸气事件，结果表明VRB方法优于传统方法。同时该研究还发现朗读和自发语言内容都具有显着的非语法性呼吸，为开发VRB方法提供了新的见解。

    

    呼吸胸带传感器可以用于测量呼吸率和其他呼吸健康参数。虚拟呼吸胸带（VRB）算法可以从语音音频估算出胸带传感器波形。本文比较了使用一种新颖的神经网络VRB算法和基于时间对齐语言内容的检测来自胸带传感器数据的吸气事件（IE）的能力。结果表明，VRB方法优于单词暂停检测或语法内容分割。方法的比较显示，朗读和自发语言内容都具有显着的非语法性呼吸，即呼吸事件不与语言中的语法正确的位置对齐。该研究为VRB方法的开发提供了新的见解，并增加了对语音呼吸行为的普遍理解。此外，演示了一种连续呼吸波形的重建新VRB方法VRBOLA。

    Respiratory chest belt sensor can be used to measure the respiratory rate and other respiratory health parameters. Virtual Respiratory Belt, VRB, algorithms estimate the belt sensor waveform from speech audio. In this paper we compare the detection of inspiration events (IE) from respiratory belt sensor data using a novel neural VRB algorithm and the detections based on time-aligned linguistic content. The results show the superiority of the VRB method over word pause detection or grammatical content segmentation. The comparison of the methods show that both read and spontaneous speech content has a significant amount of ungrammatical breathing, that is, breathing events that are not aligned with grammatically appropriate places in language. This study gives new insights into the development of VRB methods and adds to the general understanding of speech breathing behavior. Moreover, a new VRB method, VRBOLA, for the reconstruction of the continuous breathing waveform is demonstrated.
    
[^27]: 概率词典选择

    Probabilistic Lexicase Selection. (arXiv:2305.11681v1 [cs.NE])

    [http://arxiv.org/abs/2305.11681](http://arxiv.org/abs/2305.11681)

    本文介绍了一种新的亲代选择算法——概率词典选择（plexicase selection），它有效地近似了传统词典选择的概率分布。该方法具有语意感知的选择能力，同时也提高了效率和灵活性。

    

    词典选择是遗传编程中广泛使用的亲代选择算法，在程序合成、符号回归和机器学习等多个任务领域取得成功。由于其非参数和递归本质，计算每个个体被词典选择选择的概率被证明是NP难问题，这阻碍了对算法更深入的理论理解和实际改进。在本文中，我们介绍了概率词典选择（plexicase selection），一种新颖的亲代选择算法，它有效地近似了词典选择的概率分布。我们的方法不仅作为一个语意感知的选择方法展示了卓越的问题解决能力，而且由于具有选择过程的概率表示，从而获得了增强的效率和灵活性。在遗传编程的两个普遍领域（程序合成和符号回归）进行了实验。

    Lexicase selection is a widely used parent selection algorithm in genetic programming, known for its success in various task domains such as program synthesis, symbolic regression, and machine learning. Due to its non-parametric and recursive nature, calculating the probability of each individual being selected by lexicase selection has been proven to be an NP-hard problem, which discourages deeper theoretical understanding and practical improvements to the algorithm. In this work, we introduce probabilistic lexicase selection (plexicase selection), a novel parent selection algorithm that efficiently approximates the probability distribution of lexicase selection. Our method not only demonstrates superior problem-solving capabilities as a semantic-aware selection method, but also benefits from having a probabilistic representation of the selection process for enhanced efficiency and flexibility. Experiments are conducted in two prevalent domains in genetic programming: program synthesi
    
[^28]: 一种深度学习分布式环境下的通用性能模型

    A Generic Performance Model for Deep Learning in a Distributed Environment. (arXiv:2305.11665v1 [cs.DC])

    [http://arxiv.org/abs/2305.11665](http://arxiv.org/abs/2305.11665)

    本文提出了一个适用于分布式环境下各类深度学习应用程序的通用性能模型，可解决现有性能模型特定于案例的问题，并将内在与外在因素考虑在内。通过正则化和差分进化算法，找到最佳拟合常数值的通用表达式，以提高和量化模型框架的效率。

    

    对深度学习应用的性能建模对于改进和量化模型框架的效率至关重要。然而，现有的性能模型大多都是特定于案例的，对于新的深度学习框架/应用程序的能力有限。本文提出了一种分布式环境下应用程序的通用性能模型，其中包含了应用程序执行时间的通用表达式，考虑了内在因素/操作（例如算法参数/内部操作）和外在扩展因素（例如处理器数量、数据块和批次大小）的影响。我们将其制定为全局优化问题，并使用正则化成本函数和差分进化算法来解决，以找到最佳拟合常数值的通用表达式，以匹配实验确定的计算时间。我们已经在三个深度学习框架（TensorFlow、MXnet、PyTorch）上评估了所提出的模型。

    Performance modelling of a deep learning application is essential to improve and quantify the efficiency of the model framework. However, existing performance models are mostly case-specific, with limited capability for the new deep learning frameworks/applications. In this paper, we propose a generic performance model of an application in a distributed environment with a generic expression of the application execution time that considers the influence of both intrinsic factors/operations (e.g. algorithmic parameters/internal operations) and extrinsic scaling factors (e.g. the number of processors, data chunks and batch size). We formulate it as a global optimization problem and solve it using regularization on a cost function and differential evolution algorithm to find the best-fit values of the constants in the generic expression to match the experimentally determined computation time. We have evaluated the proposed model on three deep learning frameworks (i.e., TensorFlow, MXnet, a
    
[^29]: 基于V2X的联邦学习在协同智能运输系统中的应用与上下文客户端选择

    V2X-Boosted Federated Learning for Cooperative Intelligent Transportation Systems with Contextual Client Selection. (arXiv:2305.11654v1 [cs.LG])

    [http://arxiv.org/abs/2305.11654](http://arxiv.org/abs/2305.11654)

    基于V2X消息进行联邦学习，提出了上下文客户端选择流程，解决了车辆动态状态和网络连接质量等问题，实验结果优于各种数据集

    

    机器学习已经彻底改变了运输系统，使自主驾驶和智能交通服务成为可能。联邦学习通过在分布式系统中训练机器学习模型，交换模型参数而不是原始数据，克服了隐私约束。 然而，连接车辆的动态状态会影响网络连接质量并影响联邦学习的性能。为解决这一挑战，我们提出了一个上下文客户端选择流程，利用车辆间通信（V2X）消息根据预测的通信延迟选择客户端。

    Machine learning (ML) has revolutionized transportation systems, enabling autonomous driving and smart traffic services. Federated learning (FL) overcomes privacy constraints by training ML models in distributed systems, exchanging model parameters instead of raw data. However, the dynamic states of connected vehicles affect the network connection quality and influence the FL performance. To tackle this challenge, we propose a contextual client selection pipeline that uses Vehicle-to-Everything (V2X) messages to select clients based on the predicted communication latency. The pipeline includes: (i) fusing V2X messages, (ii) predicting future traffic topology, (iii) pre-clustering clients based on local data distribution similarity, and (iv) selecting clients with minimal latency for future model aggregation. Experiments show that our pipeline outperforms baselines on various datasets, particularly in non-iid settings.
    
[^30]: 动量匹配去噪Gibbs采样

    Moment Matching Denoising Gibbs Sampling. (arXiv:2305.11650v1 [stat.ML])

    [http://arxiv.org/abs/2305.11650](http://arxiv.org/abs/2305.11650)

    本文提出了动量匹配去噪Gibbs采样方法，可以在给定‘嘈杂’的模型的情况下，从干净的模型中有效地进行采样。

    

    能量基模型（EBMs）为建模复杂数据分布提供了一个通用的框架。然而，EBMs 的训练和采样仍然面临重大挑战。用于可扩展 EBM 训练的广泛使用的去噪分数匹配（DSM）方法存在不一致性问题，导致能量模型学习到“嘈杂”的数据分布。在本文中，我们提出了一种有效的采样框架：（伪）Gibbs采样与动量匹配，可以在给定经过DSM训练良好的“嘈杂”模型的情况下，从基础“干净”模型中有效地进行采样。我们探讨了我们的方法相对于相关方法的优势，并展示了如何将该方法扩展到高维数据集。

    Energy-Based Models (EBMs) offer a versatile framework for modeling complex data distributions. However, training and sampling from EBMs continue to pose significant challenges. The widely-used Denoising Score Matching (DSM) method for scalable EBM training suffers from inconsistency issues, causing the energy model to learn a `noisy' data distribution. In this work, we propose an efficient sampling framework: (pseudo)-Gibbs sampling with moment matching, which enables effective sampling from the underlying clean model when given a `noisy' model that has been well-trained via DSM. We explore the benefits of our approach compared to related methods and demonstrate how to scale the method to high-dimensional datasets.
    
[^31]: 任意缺失模式下的无分布矩阵预测

    Distribution-Free Matrix Prediction Under Arbitrary Missing Pattern. (arXiv:2305.11640v1 [cs.LG])

    [http://arxiv.org/abs/2305.11640](http://arxiv.org/abs/2305.11640)

    本文提出了两种实用算法，能够在任意丢失模式下有效地保证覆盖率的有效性，并量化了缺失对预测精度的影响。

    

    本文研究了在行/列可交换矩阵中预测缺失条目的问题。虽然矩阵设置提出了新颖和独特的挑战，但是在这个有趣的主题上存在很少的工作。我们精细地定义了问题，将其与密切相关的问题区分开来，并严格划分了可达成和不可能的目标的边界。然后我们提出了两种实用算法。第一种方法提供了全面的预测的快速仿真，而第二种方法利用算法稳定性技术加速计算。这两种方法计算效率高，能够在任意丢失模式下有效地保证覆盖率的有效性。此外，我们量化了缺失对预测精度的影响，并建立了基本的极限结果。来自合成和真实数据集的经验证据证实了我们提出的方法的卓越性能。

    This paper studies the open problem of conformalized entry prediction in a row/column-exchangeable matrix. The matrix setting presents novel and unique challenges, but there exists little work on this interesting topic. We meticulously define the problem, differentiate it from closely related problems, and rigorously delineate the boundary between achievable and impossible goals. We then propose two practical algorithms. The first method provides a fast emulation of the full conformal prediction, while the second method leverages the technique of algorithmic stability for acceleration. Both methods are computationally efficient and can effectively safeguard coverage validity in presence of arbitrary missing pattern. Further, we quantify the impact of missingness on prediction accuracy and establish fundamental limit results. Empirical evidence from synthetic and real-world data sets corroborates the superior performance of our proposed methods.
    
[^32]: 流处理系统中实现全面隐私保护的路径

    A Path to Holistic Privacy in Stream Processing Systems. (arXiv:2305.11638v1 [cs.CR])

    [http://arxiv.org/abs/2305.11638](http://arxiv.org/abs/2305.11638)

    本文系统地研究了流处理系统与物联网交叉引起的隐私问题，并提出解决方案，旨在实现全面的隐私保护。

    

    互联网物联网（IoT）数据的海量流需要及时分析以保留数据的价值。流处理系统（SPSs）可以完成这项任务，在实时中从IoT数据中推导知识。这样的实时分析有利于许多应用程序，但也可能用于侵犯用户的隐私，因为从用户或其附近收集的IoT数据本身就是敏感的。在本文中，我们系统地研究了SPSs和IoT的交叉所引起的隐私问题，确定了实现SPSs全面隐私保护的关键研究挑战，并提出了解决方案。

    The massive streams of Internet of Things (IoT) data require a timely analysis to retain data usefulness. Stream processing systems (SPSs) enable this task, deriving knowledge from the IoT data in real-time. Such real-time analytics benefits many applications but can also be used to violate user privacy, as the IoT data collected from users or their vicinity is inherently sensitive. In this paper, we present our systematic look into privacy issues arising from the intersection of SPSs and IoT, identifying key research challenges towards achieving holistic privacy protection in SPSs and proposing the solutions.
    
[^33]: 在风险规避参与反馈下的目标导向联邦学习通信

    Goal-Oriented Communications in Federated Learning via Feedback on Risk-Averse Participation. (arXiv:2305.11633v1 [cs.DC])

    [http://arxiv.org/abs/2305.11633](http://arxiv.org/abs/2305.11633)

    本文基于风险规避机制，通过PS反馈和语义信息，解决了联邦学习中客户端选择问题，以获得通讯高效的设备性能。

    

    本文讨论了联邦学习中客户端选择问题，利用学习目标和本地参与者的激励制定目标导向通信问题。具体来说，我们考虑参与者的风险规避本质，通过来自参数服务器的反馈获得通信高效的设备性能。客户端必须基于它的内在激励进行传输计划，该激励是此客户端参与训练全局模型的价值。本地更新的相关性被视为开发本地传输策略的语义信息，即决定“不传输”的时间。设备使用有关PS状态的反馈，来发展其策略，以在不降低全局模型性能的情况下，减少通信成本。

    We treat the problem of client selection in a Federated Learning (FL) setup, where the learning objective and the local incentives of the participants are used to formulate a goal-oriented communication problem. Specifically, we incorporate the risk-averse nature of participants and obtain a communication-efficient on-device performance, while relying on feedback from the Parameter Server (\texttt{PS}). A client has to decide its transmission plan on when not to participate in FL. This is based on its intrinsic incentive, which is the value of the trained global model upon participation by this client. Poor updates not only plunge the performance of the global model with added communication cost but also propagate the loss in performance on other participating devices. We cast the relevance of local updates as \emph{semantic information} for developing local transmission strategies, i.e., making a decision on when to ``not transmit". The devices use feedback about the state of the PS a
    
[^34]: 多样化深度集成：一种使用显著性图的方法以增强OOD检测、校准和准确性

    Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD Detection, Calibration, and Accuracy. (arXiv:2305.11616v1 [cs.CV])

    [http://arxiv.org/abs/2305.11616](http://arxiv.org/abs/2305.11616)

    这项研究提出了一种使用显著性图来促进深度集成多样性的方法，用于改善OOD检测、校准和准确性，能够优于传统的集成技术，并在OpenOOD基准测试上证明了其有效性。

    

    深度集成在分类和 OOD 检测方面取得了最先进的成果；然而，由于集成中学习的模式的同质性，它们的效果仍然有限。为了克服这一挑战，本研究引入了一种促进集成成员之间多样性的新方法，该方法利用显著性图。通过整合显著性图多样化，我们的方法在多个分类和OOD检测任务中优于传统的集成技术，同时也提高了校准性。在已建立的OpenOOD基准测试上的实验凸显了我们的方法在实际应用中的潜力。

    Deep ensembles achieved state-of-the-art results in classification and out-of-distribution (OOD) detection; however, their effectiveness remains limited due to the homogeneity of learned patterns within the ensemble. To overcome this challenge, our study introduces a novel approach that promotes diversity among ensemble members by leveraging saliency maps. By incorporating saliency map diversification, our method outperforms conventional ensemble techniques in multiple classification and OOD detection tasks, while also improving calibration. Experiments on well-established OpenOOD benchmarks highlight the potential of our method in practical applications.
    
[^35]: SFP: 针对伪特征的修剪方法，用于识别无分布概括问题

    SFP: Spurious Feature-targeted Pruning for Out-of-Distribution Generalization. (arXiv:2305.11615v1 [cs.LG])

    [http://arxiv.org/abs/2305.11615](http://arxiv.org/abs/2305.11615)

    SFP提出了一种针对模型子结构的修剪框架，SFP可以自动探索不变的子结构，而不考虑对完全暴露于域外数据的依赖性以及对整个数据分布进行同样特征未命中修剪带来的缺点。这种方法的核心在于，利用ID数据中的伪特征来降低风险。

    

    模型子结构学习旨在找到一个不变的网络子结构，可以比原始的完整结构更好地进行超出分布范围（OOD）概括。现有的工作通常使用完全暴露的域外数据来搜索不变的子结构，从而可能带来两个缺点：1）不公平，因为完全暴露出域外数据的依赖性；和2）次优的OOD概括，由于对整个数据分布进行了同样的特征未命中修剪。基于ID数据中的伪特征可能具有更低的体验风险的想法，在本文中，我们提出了一种新的伪特征定向的模型修剪框架，称为SFP，以自动探索不变的子结构，而不考虑上述缺点。具体而言，SFP在培训过程中使用我们在理论上验证的任务丢失识别ID实例中的伪特征，基于此，SFP减弱了相应的特征作用，以提高OOD泛化能力。

    Model substructure learning aims to find an invariant network substructure that can have better out-of-distribution (OOD) generalization than the original full structure. Existing works usually search the invariant substructure using modular risk minimization (MRM) with fully exposed out-domain data, which may bring about two drawbacks: 1) Unfairness, due to the dependence of the full exposure of out-domain data; and 2) Sub-optimal OOD generalization, due to the equally feature-untargeted pruning on the whole data distribution. Based on the idea that in-distribution (ID) data with spurious features may have a lower experience risk, in this paper, we propose a novel Spurious Feature-targeted model Pruning framework, dubbed SFP, to automatically explore invariant substructures without referring to the above drawbacks. Specifically, SFP identifies spurious features within ID instances during training using our theoretically verified task loss, upon which, SFP attenuates the corresponding 
    
[^36]: MIDI-Draw: 用绘画控制旋律生成

    MIDI-Draw: Sketching to Control Melody Generation. (arXiv:2305.11605v1 [cs.SD])

    [http://arxiv.org/abs/2305.11605](http://arxiv.org/abs/2305.11605)

    MIDI-Draw是一种可以通过画曲线来控制旋律生成的方法，相比于现有的方法，这种方法可以让用户更快速地表达他们的音乐意图。

    

    我们描述了一个概念验证实现的系统，该系统通过旋律轮廓将音符级别的输入表示抽象化。其目的是允许用户表达他们的音乐意图，而无需事先知道音符如何和谐地结合在一起。目前，可控旋律生成的当前方法通常要求用户选择整个序列中静态的参数，通过按钮或滑块进行选择。相比之下，我们的方法允许用户通过绘制轮廓快速指定参数如何随时间变化。

    We describe a proof-of-principle implementation of a system for drawing melodies that abstracts away from a note-level input representation via melodic contours. The aim is to allow users to express their musical intentions without requiring prior knowledge of how notes fit together melodiously. Current approaches to controllable melody generation often require users to choose parameters that are static across a whole sequence, via buttons or sliders. In contrast, our method allows users to quickly specify how parameters should change over time by drawing a contour.
    
[^37]: 潜在模拟器: 生成黑盒公平性测试的自然鉴别实例

    Latent Imitator: Generating Natural Individual Discriminatory Instances for Black-Box Fairness Testing. (arXiv:2305.11602v1 [cs.SE])

    [http://arxiv.org/abs/2305.11602](http://arxiv.org/abs/2305.11602)

    本论文提出了潜在模拟器方法，使用生成对抗网络在真实数据分布上生成自然鉴别实例，用于黑盒公平性测试。

    

    机器学习(ML)系统在许多应用领域取得了显著的性能。然而，在敏感的应用领域中，它们经常展现出不公平的行为，引发了严重的公平性关注。为了评估和测试公平性，工程师经常生成鉴别实例来暴露模型部署之前的不公平行为。然而，现有的基线忽略了生成的自然性，并产生偏离真实数据分布的实例，可能无法揭示实际的模型公平性，因为这些不自然的鉴别实例不可能出现在实践中。为了解决这个问题，本文提出了一个名为潜在模拟器(LIMI)的框架，利用生成对抗网络(GAN)来生成更自然的个体鉴别实例，其中我们在GAN的语义潜在空间中模拟目标模型的决策边界，并进一步对其进行样本潜在实例的采样。

    Machine learning (ML) systems have achieved remarkable performance across a wide area of applications. However, they frequently exhibit unfair behaviors in sensitive application domains, raising severe fairness concerns. To evaluate and test fairness, engineers often generate individual discriminatory instances to expose unfair behaviors before model deployment. However, existing baselines ignore the naturalness of generation and produce instances that deviate from the real data distribution, which may fail to reveal the actual model fairness since these unnatural discriminatory instances are unlikely to appear in practice. To address the problem, this paper proposes a framework named Latent Imitator (LIMI) to generate more natural individual discriminatory instances with the help of a generative adversarial network (GAN), where we imitate the decision boundary of the target model in the semantic latent space of GAN and further samples latent instances on it. Specifically, we first der
    
[^38]: 基于视觉的深度强化学习自动驾驶系统及Sim2Real转移

    Vision-based DRL Autonomous Driving Agent with Sim2Real Transfer. (arXiv:2305.11589v1 [cs.RO])

    [http://arxiv.org/abs/2305.11589](http://arxiv.org/abs/2305.11589)

    本研究提出了一种基于视觉的深度强化学习代理，可以同时执行保持车道和跟车操作，并且展示了其在真实情况下的模型迁移能力，是第一个具有此能力的代理。

    

    要实现完全自动驾驶，车辆必须能够持续执行各种驾驶任务，包括保持车道和跟车，这两个任务是基本的并且研究得很充分。然而，以前的研究主要集中在单个任务上，而跟车任务通常依赖完整的领导-跟随者信息来实现最佳性能。为解决这一限制，我们提出了一种基于视觉的深度强化学习（DRL）代理，可以同时执行保持车道和跟车操作。为了评估我们的DRL代理的性能，我们将其与基线控制器进行比较，并使用各种性能指标进行定量分析。此外，我们进行了现实世界的评估，以证明训练的DRL代理的Sim2Real转移能力。据我们所知，我们的基于视觉的保持车道和跟车代理及其Sim2Real转移能力是第一个这样的代理。

    To achieve fully autonomous driving, vehicles must be capable of continuously performing various driving tasks, including lane keeping and car following, both of which are fundamental and well-studied driving ones. However, previous studies have mainly focused on individual tasks, and car following tasks have typically relied on complete leader-follower information to attain optimal performance. To address this limitation, we propose a vision-based deep reinforcement learning (DRL) agent that can simultaneously perform lane keeping and car following maneuvers. To evaluate the performance of our DRL agent, we compare it with a baseline controller and use various performance metrics for quantitative analysis. Furthermore, we conduct a real-world evaluation to demonstrate the Sim2Real transfer capability of the trained DRL agent. To the best of our knowledge, our vision-based car following and lane keeping agent with Sim2Real transfer capability is the first of its kind.
    
[^39]: 高斯过程回归的贝叶斯方法中融入不确定输入

    Bayesian approach to Gaussian process regression with uncertain inputs. (arXiv:2305.11586v1 [cs.LG])

    [http://arxiv.org/abs/2305.11586](http://arxiv.org/abs/2305.11586)

    本文提出了一种新的高斯过程回归技术，通过贝叶斯方法将输入数据的不确定性纳入回归模型预测中。在数值实验中展示了该方法具有普适性和不错的表现。

    

    传统高斯过程回归仅假设模型观测数据的输出具有噪声。然而，在许多科学和工程应用中，由于建模假设、测量误差等因素，观测数据的输入位置可能也存在不确定性。在本文中，我们提出了一种贝叶斯方法，将输入数据的可变性融入到高斯过程回归中。考虑两种可观测量——具有固定输入的噪声污染输出和具有先验分布定义的不确定输入，通过贝叶斯框架估计后验分布以推断不确定的数据位置。然后，利用边际化方法将这些输入的量化不确定性纳入高斯过程预测中。通过几个数值实验，展示了这种新回归技术的有效性，在其中观察到不同水平输入数据不确定性下的普适良好表现。

    Conventional Gaussian process regression exclusively assumes the existence of noise in the output data of model observations. In many scientific and engineering applications, however, the input locations of observational data may also be compromised with uncertainties owing to modeling assumptions, measurement errors, etc. In this work, we propose a Bayesian method that integrates the variability of input data into Gaussian process regression. Considering two types of observables -- noise-corrupted outputs with fixed inputs and those with prior-distribution-defined uncertain inputs, a posterior distribution is estimated via a Bayesian framework to infer the uncertain data locations. Thereafter, such quantified uncertainties of inputs are incorporated into Gaussian process predictions by means of marginalization. The effectiveness of this new regression technique is demonstrated through several numerical examples, in which a consistently good performance of generalization is observed, w
    
[^40]: 动态正则化锐度感知联邦学习中的全局一致性和平滑场景方法

    Dynamic Regularized Sharpness Aware Minimization in Federated Learning: Approaching Global Consistency and Smooth Landscape. (arXiv:2305.11584v1 [cs.LG])

    [http://arxiv.org/abs/2305.11584](http://arxiv.org/abs/2305.11584)

    本文提出了一种动态正则化锐度感知联邦学习方法，通过同时考虑优化和泛化目标来高效地提高联邦学习的性能。

    

    在联邦学习（FL）中，一组本地客户端在全局服务器的协调下协作训练带有隐私保护的模型。由于多个本地更新和隔离的非 iid 数据集，客户端容易过度拟合到自己的自身最优解，这极大地偏离了全局目标并严重削弱了性能。本文提出了一种新颖的通用算法 FedSMOO，通过同时考虑优化和泛化目标来高效地提高 FL 中的性能。

    In federated learning (FL), a cluster of local clients are chaired under the coordination of the global server and cooperatively train one model with privacy protection. Due to the multiple local updates and the isolated non-iid dataset, clients are prone to overfit into their own optima, which extremely deviates from the global objective and significantly undermines the performance. Most previous works only focus on enhancing the consistency between the local and global objectives to alleviate this prejudicial client drifts from the perspective of the optimization view, whose performance would be prominently deteriorated on the high heterogeneity. In this work, we propose a novel and general algorithm {\ttfamily FedSMOO} by jointly considering the optimization and generalization targets to efficiently improve the performance in FL. Concretely, {\ttfamily FedSMOO} adopts a dynamic regularizer to guarantee the local optima towards the global objective, which is meanwhile revised by the 
    
[^41]: 你听到的正是你看到的：从图像质量评价中获得的音频质量评价方法

    What You Hear Is What You See: Audio Quality Metrics From Image Quality Metrics. (arXiv:2305.11582v1 [cs.SD])

    [http://arxiv.org/abs/2305.11582](http://arxiv.org/abs/2305.11582)

    该研究探讨了利用图像感知度量方法来评估音频信号的可行性，并通过定制一个心理声学合理结构的度量方法来解决声音信号的特殊性，并在音乐数据集上展现出了令人满意的结果。

    

    本研究旨在探究利用最先进的图像感知度量方法，通过将音频信号表示成频谱图以评估音频信号的可行性。所提出的方法基于听觉和视觉通路中神经机制的相似性，取得了鼓舞人心的结果。此外，我们定制了一个具有心理声学合理结构的度量方法，以解决声音信号的特殊性。我们使用音乐数据集评估了我们提出的度量方法和几个基准度量方法的有效性，并取得了令人满意的结果，即度量方法与人类评估者所评估的音频质量之间的相关性。

    In this study, we investigate the feasibility of utilizing state-of-the-art image perceptual metrics for evaluating audio signals by representing them as spectrograms. The encouraging outcome of the proposed approach is based on the similarity between the neural mechanisms in the auditory and visual pathways. Furthermore, we customise one of the metrics which has a psychoacoustically plausible architecture to account for the peculiarities of sound signals. We evaluate the effectiveness of our proposed metric and several baseline metrics using a music dataset, with promising results in terms of the correlation between the metrics and the perceived quality of audio as rated by human evaluators.
    
[^42]: 深度推进时间治愈模型

    The Deep Promotion Time Cure Model. (arXiv:2305.11575v1 [stat.ML])

    [http://arxiv.org/abs/2305.11575](http://arxiv.org/abs/2305.11575)

    该研究提出了一种新方法，将灵活的生存模型集成进深度神经网络框架中，实现预测存在治愈分数时的时间至事件。该方法可适用于大规模应用，允许协变量和生存之间的非线性关系和高维交互，从而获得更好的预测性能和更真实的协变量效应。

    

    我们提出了一种基于灵活的生存模型集成到深度神经网络框架中用于预测存在治愈分数时的时间至事件的新方法。我们的方法允许协变量和生存之间的非线性关系和高维交互，并适用于大规模应用。此外，我们允许该方法合并一个由可解释的线性和非线性效应的附加分解形成的已识别的预测器，并添加正交化层以捕获潜在的更高维交互。我们通过模拟证明了我们的方法的有用性和计算效率，并将其应用于大量的美国抵押贷款组合。在此，我们不仅发现了我们框架更好的预测性能，而且发现了更真实的协变量效应。

    We propose a novel method for predicting time-to-event in the presence of cure fractions based on flexible survivals models integrated into a deep neural network framework. Our approach allows for non-linear relationships and high-dimensional interactions between covariates and survival and is suitable for large-scale applications. Furthermore, we allow the method to incorporate an identified predictor formed of an additive decomposition of interpretable linear and non-linear effects and add an orthogonalization layer to capture potential higher dimensional interactions. We demonstrate the usefulness and computational efficiency of our method via simulations and apply it to a large portfolio of US mortgage loans. Here, we find not only a better predictive performance of our framework but also a more realistic picture of covariate effects.
    
[^43]: TSGM：一种生成合成时间序列数据的灵活框架

    TSGM: A Flexible Framework for Generative Modeling of Synthetic Time Series. (arXiv:2305.11567v1 [cs.LG])

    [http://arxiv.org/abs/2305.11567](http://arxiv.org/abs/2305.11567)

    TSGM提供了一种生成合成时间序列数据的灵活框架，使研究人员能够快速实现自己的方法并在可共享的环境中进行比较，从而有助于生成大规模的合成时间序列数据集，以用于训练和验证各种机器学习模型。

    

    时间序列数据在各个领域中非常重要，对机器学习研究者也很有兴趣。然而，时间序列数据通常很少或高度敏感，这使得数据在研究者和工业组织之间的共享以及现有和新的数据密集型 ML 方法的应用受到限制。解决这一难题的可能方法是生成合成数据。在这项工作中，我们介绍了时间序列生成模型（TSGM），这是一种用于生成合成时间序列数据的开源框架。TSGM包括广泛的机器学习方法：生成模型、概率模型和基于模拟器的方法。该框架使用户能够从不同的角度评估生成的数据的质量：相似性、下游效果、预测一致性、多样性和隐私。该框架是可扩展的，这使得研究人员能够快速实现自己的方法并在可共享的环境中进行比较。TSGM将有助于生成大规模的合成时间序列数据集，这些数据集可以用于训练和验证各种机器学习模型。

    Temporally indexed data are essential in a wide range of fields and of interest to machine learning researchers. Time series data, however, are often scarce or highly sensitive, which precludes the sharing of data between researchers and industrial organizations and the application of existing and new data-intensive ML methods. A possible solution to this bottleneck is to generate synthetic data. In this work, we introduce Time Series Generative Modeling (TSGM), an open-source framework for the generative modeling of synthetic time series. TSGM includes a broad repertoire of machine learning methods: generative models, probabilistic, and simulator-based approaches. The framework enables users to evaluate the quality of the produced data from different angles: similarity, downstream effectiveness, predictive consistency, diversity, and privacy. The framework is extensible, which allows researchers to rapidly implement their own methods and compare them in a shareable environment. TSGM w
    
[^44]: ToolkenGPT：通过工具嵌入扩充冻结语言模型

    ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings. (arXiv:2305.11554v1 [cs.CL])

    [http://arxiv.org/abs/2305.11554](http://arxiv.org/abs/2305.11554)

    本论文提出了一种名为ToolkenGPT的方法，将大型语言模型（LLMs）与外部工具相结合，引入了toolken的概念，利用tool embeddings实现无缝交互，同时在各种下游任务上展示出了良好的效果。

    

    将大型语言模型与外部工具结合起来解决复杂问题已成为一种有前途的方法。然而，传统方法需要用工具演示数据对LLM进行微调，既费时又受限于预定义的工具集。最近的上下文学习范例缓解了这些问题，但是有限的上下文长度只允许演示几次，导致对工具的理解不够充分。此外，当有大量工具可供选择时，上下文学习可能完全无法正常工作。在本文中，我们提出了一种$\textbf{ToolkenGPT}$的替代方法，将两种方法的优点结合起来。我们的方法将每个$\underline{工具}$表示为一个$\underline{token}$（$\textit{toolken}$），并为其学习一个嵌入，使得工具调用与生成常规单词标记的方式相同。一旦触发了toolken，LLM被提示完成工具执行所需的参数。ToolkenGPT提供了以下贡献：1）引入了toolken的概念，以扩充LLM与外部工具的交互，2）提出了一种新的学习范例，利用tool embeddings实现无缝交互，3）在各种下游任务上展示了我们方法的有效性。

    Augmenting large language models (LLMs) with external tools has emerged as a promising approach to solving complex problems. However, traditional methods, which finetune LLMs with tool demonstration data, can be both costly and restricted to a predefined set of tools. Recent in-context learning paradigm alleviates these issues, but the limited context length only allows for a few shots of demonstrations, leading to suboptimal understandings of the tools. Moreover, when there are numerous tools to choose from, in-context learning could completely fail to work. In this paper, we propose an alternative approach, $\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our approach represents each $\underline{tool}$ as a to$\underline{ken}$ ($\textit{toolken}$) and learns an embedding for it, enabling tool calls in the same way as generating a regular word token. Once a toolken is triggered, the LLM is prompted to complete arguments for the tool to execute. ToolkenGPT offers the f
    
[^45]: 基于几何感知的自回归模型用于新型电磁量计几何模拟的泛化研究

    Generalizing to new calorimeter geometries with Geometry-Aware Autoregressive Models (GAAMs) for fast calorimeter simulation. (arXiv:2305.11531v1 [physics.ins-det])

    [http://arxiv.org/abs/2305.11531](http://arxiv.org/abs/2305.11531)

    基于几何感知的自回归模型能够学习电磁量计响应如何随几何形状变化，能够快速有效地模拟非环形的电磁量计。

    

    在粒子物理数据分析中，生成对撞产物的模拟探测器响应至关重要，但计算非常昂贵。其中一个子探测器，电磁量计由于其单元格的高粒度和复杂的相互作用而占据了计算时间的主导地位。生成模型可以提供更快的样本生成，但目前需要大量努力来优化特定探测器几何形状的性能，通常需要许多网络来描述不同的单元格大小和排列方式，这些模型不能推广到其他几何形状。我们开发了一种“几何感知”自回归模型，学习电磁量计响应如何随几何形状变化，能够生成看不见的几何形状的模拟响应而无需其他训练。该几何感知模型在涉及关键响应的生成和真实分布之间的Wasserstein距离等指标上比基线模型优越50％。我们通过在二维空间中模拟具有前所未有的细粒度，并扩展到非平面几何形状，展示了该方法的可行性和速度。

    Generation of simulated detector response to collision products is crucial to data analysis in particle physics, but computationally very expensive. One subdetector, the calorimeter, dominates the computational time due to the high granularity of its cells and complexity of the interaction. Generative models can provide more rapid sample production, but currently require significant effort to optimize performance for specific detector geometries, often requiring many networks to describe the varying cell sizes and arrangements, which do not generalize to other geometries. We develop a {\it geometry-aware} autoregressive model, which learns how the calorimeter response varies with geometry, and is capable of generating simulated responses to unseen geometries without additional training. The geometry-aware model outperforms a baseline, unaware model by 50\% in metrics such as the Wasserstein distance between generated and true distributions of key quantities which summarize the simulate
    
[^46]: 一种用于解决阿拉伯语代词消解问题的序列到序列方法

    A Sequence-to-Sequence Approach for Arabic Pronoun Resolution. (arXiv:2305.11529v1 [cs.CL])

    [http://arxiv.org/abs/2305.11529](http://arxiv.org/abs/2305.11529)

    本文提出了一种序列到序列学习方法，用于解决阿拉伯语代词消解问题。该模型在AnATAr数据集上优于传统的机器学习模型和手工特征模型。研究者还探讨了一些对模型的修改，这些修改显著提高了模型的性能。

    

    本文提出了一种序列到序列学习方法，探索了使用先进的自然语言处理(NLP)技术，特别是Bi-LSTM和BERT预训练语言模型，在解决阿拉伯语代词消解问题方面的有效性。该方法在AnATAr数据集上进行评估，并与几种基线模型进行比较，包括传统的机器学习模型和手工特征模型。我们的结果表明，该模型在所有指标上都优于基线模型，包括KNN、逻辑回归和SVM。此外，我们探讨了对模型的各种修改的有效性，包括将指代词文本与段落文本连接作为输入、添加掩码以关注候选分数以及基于指代词的性别和数量协议来过滤候选项。我们的结果表明，这些修改显著提高了模型的性能。

    This paper proposes a sequence-to-sequence learning approach for Arabic pronoun resolution, which explores the effectiveness of using advanced natural language processing (NLP) techniques, specifically Bi-LSTM and the BERT pre-trained Language Model, in solving the pronoun resolution problem in Arabic. The proposed approach is evaluated on the AnATAr dataset, and its performance is compared to several baseline models, including traditional machine learning models and handcrafted feature-based models. Our results demonstrate that the proposed model outperforms the baseline models, which include KNN, logistic regression, and SVM, across all metrics. In addition, we explore the effectiveness of various modifications to the model, including concatenating the anaphor text beside the paragraph text as input, adding a mask to focus on candidate scores, and filtering candidates based on gender and number agreement with the anaphor. Our results show that these modifications significantly improv
    
[^47]: InstructIE: 一份基于指令的中文信息提取数据集

    InstructIE: A Chinese Instruction-based Information Extraction Dataset. (arXiv:2305.11527v1 [cs.CL])

    [http://arxiv.org/abs/2305.11527](http://arxiv.org/abs/2305.11527)

    介绍了一份中文的基于指令的信息提取数据集InstructIE，其中包括了270,000个弱监督的数据和1,000个高质量注释实例。实验结果表明当前的模型表现有待改进，该任务仍存在挑战。

    

    我们引入了一项新的信息提取任务，称为基于指令的信息提取 (Instruction-based IE)，它旨在要求系统遵循特定的指令或指南来提取信息。为了促进该领域的研究，我们构建了一个数据集，称为InstructIE，其中包括来自中文维基百科的 270,000 个弱监督数据和 1,000 个高质量众包注释实例。我们进一步评估了各种基线模型在InstructIE数据集上的表现。结果表明，尽管当前的模型表现很有希望，但仍有改进的空间。此外，我们进行了全面的案例研究分析，强调了基于指令的信息提取任务中固有的挑战。代码和数据集可在 https://github.com/zjunlp/DeepKE/tree/main/example/llm 找到。

    We introduce a new Information Extraction (IE) task dubbed Instruction-based IE, which aims to ask the system to follow specific instructions or guidelines to extract information. To facilitate research in this area, we construct a dataset called InstructIE, consisting of 270,000 weakly supervised data from Chinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We further evaluate the performance of various baseline models on the InstructIE dataset. The results reveal that although current models exhibit promising performance, there is still room for improvement. Furthermore, we conduct a comprehensive case study analysis, underlining the challenges inherent in the Instruction-based IE task. Code and dataset are available at https://github.com/zjunlp/DeepKE/tree/main/example/llm.
    
[^48]: 基于图注意力和频率增强机制的短期风速预测方法

    Enhancing Short-Term Wind Speed Forecasting using Graph Attention and Frequency-Enhanced Mechanisms. (arXiv:2305.11526v1 [cs.LG])

    [http://arxiv.org/abs/2305.11526](http://arxiv.org/abs/2305.11526)

    本文提出了一种基于图注意力和频率增强机制的风速预测模型GFST-WSF，能够有效提高短期风速预测的准确性。

    

    在风电大规模集成电网中，风力的高可变性和随机性对电力系统的安全和稳定运行带来了巨大挑战。风力预测是解决这个问题的有效方法，其中风速预测是至关重要的方面。本文提出了一种基于图注意力和频率增强机制的图注意力频率增强时空风速预测模型（GFST-WSF），以提高短期风速预测的准确性。GFST-WSF包括用于提取时间特征的Transformer架构和用于提取空间特征的图注意力网络（GAT）。GAT被专门设计用于捕捉风速站之间的复杂空间依赖关系，从而有效地聚合图中相邻节点的信息，从而增强数据的空间表示。为了模拟邻近风场之间的风速相关的时间滞后

    The safe and stable operation of power systems is greatly challenged by the high variability and randomness of wind power in large-scale wind-power-integrated grids. Wind power forecasting is an effective solution to tackle this issue, with wind speed forecasting being an essential aspect. In this paper, a Graph-attentive Frequency-enhanced Spatial-Temporal Wind Speed Forecasting model based on graph attention and frequency-enhanced mechanisms, i.e., GFST-WSF, is proposed to improve the accuracy of short-term wind speed forecasting. The GFST-WSF comprises a Transformer architecture for temporal feature extraction and a Graph Attention Network (GAT) for spatial feature extraction. The GAT is specifically designed to capture the complex spatial dependencies among wind speed stations to effectively aggregate information from neighboring nodes in the graph, thus enhancing the spatial representation of the data. To model the time lag in wind speed correlation between adjacent wind farms cau
    
[^49]: 丰富解缠结：从定义到度量的探究

    Enriching Disentanglement: Definitions to Metrics. (arXiv:2305.11512v1 [cs.LG])

    [http://arxiv.org/abs/2305.11512](http://arxiv.org/abs/2305.11512)

    本文探究了解缠结表示学习的度量标准，并提出了基于丰富范畴论的系统方法，将方程定义转化为可比较的度量标准，我们推导出应用于测量解缠结属性的度量标准，并在合成数据上证明其有效性。

    

    解缠结表示学习是一项具有挑战性的任务，涉及到在复杂数据中分离多个变化因素。虽然已经提出了各种用于学习和评估解缠结表示的度量标准，但这些度量标准真正量化了什么以及如何比较它们仍然不清楚。在本文中，我们研究了利用一阶方程谓词定义的解缠结，并介绍了一种基于丰富范畴论的系统方法，将方程定义转化为兼容的定量度量标准。具体而言，我们展示了如何用度量或离散度替换(i) 等式，用排序操作替换 (ii) 逻辑联结词，用聚合替换 (iii) 通用量词，用最佳逼近替换 (iv) 存在量词。使用这种方法，我们推导出用于测量解缠结表示提取器所需属性的度量标准，并在合成数据上展示它们的有效性。我们提出的方法提供了一种框架，能够将解缠结表示定义转化为可比较的，并衡量一种方法中解缠结属性的有效性。

    Disentangled representation learning is a challenging task that involves separating multiple factors of variation in complex data. Although various metrics for learning and evaluating disentangled representations have been proposed, it remains unclear what these metrics truly quantify and how to compare them. In this work, we study the definitions of disentanglement given by first-order equational predicates and introduce a systematic approach for transforming an equational definition into a compatible quantitative metric based on enriched category theory. Specifically, we show how to replace (i) equality with metric or divergence, (ii) logical connectives with order operations, (iii) universal quantifier with aggregation, and (iv) existential quantifier with the best approximation. Using this approach, we derive metrics for measuring the desired properties of a disentangled representation extractor and demonstrate their effectiveness on synthetic data. Our proposed approach provides p
    
[^50]: 从随机搜索到度量测度空间中的赌博学习

    From Random Search to Bandit Learning in Metric Measure Spaces. (arXiv:2305.11509v1 [cs.LG])

    [http://arxiv.org/abs/2305.11509](http://arxiv.org/abs/2305.11509)

    本文介绍了随机搜索及其性能，引入了“散射维度”的概念，描述了底层函数的状态，量化了随机搜索的性能，并证明了在无噪声和有界噪声情况下的输出分别以一定概率收敛到最优值。

    

    随机搜索是超参数优化中最常用的方法之一，对于深度学习模型的成功至关重要。尽管其性能令人惊叹，但很少有非启发式的理论用于描述其工作机制。本文给出了关于随机搜索的理论解释。我们引入了“散射维度”的概念，描述了底层函数的状态，并量化了随机搜索的性能。我们表明，当环境没有噪声时，随机搜索的输出以概率收敛到最优值，其速率为$ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \right)^{ \frac{1}{d_s} } \right) $，其中$ d_s \ge 0 $是底层函数的散射维度。当观察到的函数值受到有界的独立同分布噪声影响时，随机搜索的输出以概率收敛到最优值，速率为$ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \right)^{ \frac{2}{2+d_s} } \right) $。

    Random Search is one of the most widely-used method for Hyperparameter Optimization, and is critical to the success of deep learning models. Despite its astonishing performance, little non-heuristic theory has been developed to describe the underlying working mechanism. This paper gives a theoretical accounting of Random Search. We introduce the concept of \emph{scattering dimension} that describes the landscape of the underlying function, and quantifies the performance of random search. We show that, when the environment is noise-free, the output of random search converges to the optimal value in probability at rate $ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \right)^{ \frac{1}{d_s} } \right) $, where $ d_s \ge 0 $ is the scattering dimension of the underlying function. When the observed function values are corrupted by bounded $iid$ noise, the output of random search converges to the optimal value in probability at rate $ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \rig
    
[^51]: JOINEDTrans: 优先引导的多任务Transformer用于联合视盘/杯分割和黄斑检测。

    JOINEDTrans: Prior Guided Multi-task Transformer for Joint Optic Disc/Cup Segmentation and Fovea Detection. (arXiv:2305.11504v1 [eess.IV])

    [http://arxiv.org/abs/2305.11504](http://arxiv.org/abs/2305.11504)

    本文提出JOINDEDTrans框架用于联合OD/OC 分割及黄斑检测, 采用先验信息引导多任务Transformer， 取得更好的分割和检测效果。

    

    基于深度学习的图像分割和检测模型在分析视网膜标志物（如视盘（OD），视杯（OC）和黄斑）方面的效率已大大提高。然而，眼科疾病相关病变和低图像质量问题等因素可能严重复杂化自动OD / OC分割和黄斑检测。大多数现有作品将每个标志物的识别视为单个任务，并考虑先前的信息。为了解决这些问题，我们提出了一个先验引导多任务Transformer框架，用于联合OD / OC分割和黄斑检测，命名为JOINEDTrans。JOINEDTrans有效地结合了视网膜图像的各种空间特征，缓解了病变和其他成像问题引起的结构失真。它包含一个分割分支和一个检测分支。值得注意的是，我们采用了在血管分割任务中预训练的编码器，以有效利用血管，OD / OC和黄斑之间的位置关系。

    Deep learning-based image segmentation and detection models have largely improved the efficiency of analyzing retinal landmarks such as optic disc (OD), optic cup (OC), and fovea. However, factors including ophthalmic disease-related lesions and low image quality issues may severely complicate automatic OD/OC segmentation and fovea detection. Most existing works treat the identification of each landmark as a single task, and take into account no prior information. To address these issues, we propose a prior guided multi-task transformer framework for joint OD/OC segmentation and fovea detection, named JOINEDTrans. JOINEDTrans effectively combines various spatial features of the fundus images, relieving the structural distortions induced by lesions and other imaging issues. It contains a segmentation branch and a detection branch. To be noted, we employ an encoder pretrained in a vessel segmentation task to effectively exploit the positional relationship among vessel, OD/OC, and fovea, 
    
[^52]: 基于随机低秩逼近的非凸鲁棒高阶张量完成

    Nonconvex Robust High-Order Tensor Completion Using Randomized Low-Rank Approximation. (arXiv:2305.11495v1 [cs.LG])

    [http://arxiv.org/abs/2305.11495](http://arxiv.org/abs/2305.11495)

    本文提出了两种高效低秩张量逼近方法和双非凸模型及其相应的快速优化算法，用于解决鲁棒高阶张量完成问题，并在大规模合成和真实世界的任务上证明了其优越性能。

    

    在张量奇异值分解（T-SVD）框架下，现有的稳健低秩张量完成方法在科学和工程的各个领域取得了巨大的成就。然而，这些方法涉及基于T-SVD的低秩逼近，在处理大规模张量数据时面临高计算成本的问题。此外，它们中的大多数仅适用于三阶张量。针对这些问题，在本文中，我们首先在d阶（d>=3）T-SVD框架下设计了两种融合随机技术的高效低秩张量逼近方法。在此基础上，我们进一步探讨了鲁棒高阶张量完成（RHTC）问题，开发了双非凸模型及其相应的快速优化算法，并提供了收敛保证。据我们所知，这是第一次将随机低秩逼近纳入RHTC问题的研究。对大规模合成和真实世界的高阶张量完成任务的实证研究表明，与现有方法相比，所提出的算法具有卓越的性能。

    Within the tensor singular value decomposition (T-SVD) framework, existing robust low-rank tensor completion approaches have made great achievements in various areas of science and engineering. Nevertheless, these methods involve the T-SVD based low-rank approximation, which suffers from high computational costs when dealing with large-scale tensor data. Moreover, most of them are only applicable to third-order tensors. Against these issues, in this article, two efficient low-rank tensor approximation approaches fusing randomized techniques are first devised under the order-d (d >= 3) T-SVD framework. On this basis, we then further investigate the robust high-order tensor completion (RHTC) problem, in which a double nonconvex model along with its corresponding fast optimization algorithms with convergence guarantees are developed. To the best of our knowledge, this is the first study to incorporate the randomized low-rank approximation into the RHTC problem. Empirical studies on large-
    
[^53]: LLM自身可读取和生成CXR图像

    LLM Itself Can Read and Generate CXR Images. (arXiv:2305.11490v1 [cs.CV])

    [http://arxiv.org/abs/2305.11490](http://arxiv.org/abs/2305.11490)

    该论文提出了一种新方法，可以在不需要进行结构更改、额外训练、或训练专门网络的情况下，通过微调预先训练的LLM来读取和生成像文本一样的图像，并应用于胸部X线（CXR）图像的生成任务中。

    

    借助于近期大语言模型（LLMs）的显著发展，人们正积极尝试将LLMs的实用性扩展到多模态任务。已经有人尝试连接语言和视觉信息，并且也在不断尝试为LLMs添加视觉能力。然而，现有的尝试只使用LLMs作为图像解码器，没有尝试通过自然语言来生成图像。通过采用VQ-GAN框架，将图像的潜在表示视为一种文本标记，我们提出了一种新方法，可以微调预先训练的LLM，以像文本一样读取和生成图像，而无需进行结构更改、额外的训练目标或训练专门的网络，同时仍保留LLM的指令跟随能力。我们将此框架应用于胸部X线（CXR）图像的生成任务中，因为这是一个复杂信息在视觉和语言之间翻译的领域。

    Building on the recent remarkable development of large language models (LLMs), active attempts are being made to extend the utility of LLMs to multimodal tasks. There have been previous efforts to link language and visual information, and attempts to add visual capabilities to LLMs are ongoing as well. However, existing attempts use LLMs only as image decoders and no attempt has been made to generate images in the same line as the natural language. By adopting a VQ-GAN framework in which latent representations of images are treated as a kind of text tokens, we present a novel method to fine-tune a pre-trained LLM to read and generate images like text without any structural changes, extra training objectives, or the need for training an ad-hoc network while still preserving the of the instruction-following capability of the LLM. We apply this framework to chest X-ray (CXR) image and report generation tasks as it is a domain in which translation of complex information between visual and 
    
[^54]: 通过扩散补全实现不完整多视角聚类

    Incomplete Multi-view Clustering via Diffusion Completion. (arXiv:2305.11489v1 [cs.LG])

    [http://arxiv.org/abs/2305.11489](http://arxiv.org/abs/2305.11489)

    本文提出一种扩散补全方法，将缺失视角恢复到不完整多视角聚类框架中，并通过对比学习来学习多视角数据的一致性信息，从而提高了多视角聚类的性能。

    

    在真实世界中，为了对大量未标记的数据进行有效的数据分析，不完整多视角聚类是一项具有挑战性和非常规的任务。所有的不完整多视角聚类方法都需要解决如何减少缺失视角的影响问题。为了解决这个问题，我们提出了一种扩散补全方法，将缺失视角恢复到不完整多视角聚类框架中。基于可观察视角信息，扩散模型用于恢复缺失的视角，然后通过对比学习来学习多视角数据的一致性信息，以提高多视角聚类的性能。据我们所知，这可能是将扩散模型纳入不完整多视角聚类框架的首个工作。实验结果表明，与最先进的方法相比，所提方法在恢复缺失的视角的同时实现了优越的聚类性能。

    Incomplete multi-view clustering is a challenging and non-trivial task to provide effective data analysis for large amounts of unlabeled data in the real world. All incomplete multi-view clustering methods need to address the problem of how to reduce the impact of missing views. To address this issue, we propose diffusion completion to recover the missing views integrated into an incomplete multi-view clustering framework. Based on the observable views information, the diffusion model is used to recover the missing views, and then the consistency information of the multi-view data is learned by contrastive learning to improve the performance of multi-view clustering. To the best of our knowledge, this may be the first work to incorporate diffusion models into an incomplete multi-view clustering framework. Experimental results show that the proposed method performs well in recovering the missing views while achieving superior clustering performance compared to state-of-the-art methods.
    
[^55]: 基于人群自我对抗学习多样风险偏好

    Learning Diverse Risk Preferences in Population-based Self-play. (arXiv:2305.11476v1 [cs.LG])

    [http://arxiv.org/abs/2305.11476](http://arxiv.org/abs/2305.11476)

    RPPO是一种新颖的强化学习算法，通过代理程序在面对不确定性时具备多样的风险偏好，从而增加自我对抗算法中的策略多样性，并提高代理程序面对不同对手的鲁棒性。

    

    在强化学习的成功案例中，自我对抗算法在解决竞争性游戏中发挥了重要作用。然而当前的自我对抗算法在优化代理程序以最大化预期胜率时，往往会陷入局部最优并产生单一同质化的策略。为了打破僵局并增强代理程序面对不同对手的鲁棒性，解决方法可能在于增加策略的多样性。然而，在自我对抗算法中增加多样性并不是易如反掌的。本文试图从代理程序在面对不确定性时可以具备多样的风险偏好这一视角出发增加策略多样性。具体来说，我们设计了一种新颖的强化学习算法，称为风险敏感近端策略优化(RPPO)，它在最坏和最好的策略学习之间平滑地插值，允许具有所需风险偏好的策略学习。

    Among the great successes of Reinforcement Learning (RL), self-play algorithms play an essential role in solving competitive games. Current self-play algorithms optimize the agent to maximize expected win-rates against its current or historical copies, making it often stuck in the local optimum and its strategy style simple and homogeneous. A possible solution is to improve the diversity of policies, which helps the agent break the stalemate and enhances its robustness when facing different opponents. However, enhancing diversity in the self-play algorithms is not trivial. In this paper, we aim to introduce diversity from the perspective that agents could have diverse risk preferences in the face of uncertainty. Specifically, we design a novel reinforcement learning algorithm called Risk-sensitive Proximal Policy Optimization (RPPO), which smoothly interpolates between worst-case and best-case policy learning and allows for policy learning with desired risk preferences. Seamlessly inte
    
[^56]: 曲线上扬：在可微广义加性模型中的共曲抑制正则化

    Curve Your Enthusiasm: Concurvity Regularization in Differentiable Generalized Additive Models. (arXiv:2305.11475v1 [cs.LG])

    [http://arxiv.org/abs/2305.11475](http://arxiv.org/abs/2305.11475)

    本文提供了一种共曲抑制正则化器，用于应对广义加性模型易受共错性的问题，通过惩罚非线性转换的特征变量的成对相关性，增强了模型的解释性。

    

    最近，由于广义加性模型（GAM）可表达目标变量为特征的非线性变换和解释性，其再次受到欢迎。尽管GAM目前备受热捧，但其易受共错性，即特征之间的（可能是非线性的）依赖性迄今为止大多被忽视。在这里，我们展示了共错性如何严重破坏GAM的解释性，并提出了一个解决方法：一个在非线性转换的特征变量的成对相关性上进行惩罚的概念简单但有效的正则化器。该过程适用于任何可微的加性模型，如神经加性模型或神经预言。并且通过消除自我抵消的特征贡献的歧义，增强了解释性。我们在合成和真实时间序列和表格数据集上验证了我们的正则化器的有效性。

    Generalized Additive Models (GAMs) have recently experienced a resurgence in popularity due to their interpretability, which arises from expressing the target value as a sum of non-linear transformations of the features. Despite the current enthusiasm for GAMs, their susceptibility to concurvity - i.e., (possibly non-linear) dependencies between the features - has hitherto been largely overlooked. Here, we demonstrate how concurvity can severly impair the interpretability of GAMs and propose a remedy: a conceptually simple, yet effective regularizer which penalizes pairwise correlations of the non-linearly transformed feature variables. This procedure is applicable to any differentiable additive model, such as Neural Additive Models or NeuralProphet, and enhances interpretability by eliminating ambiguities due to self-canceling feature contributions. We validate the effectiveness of our regularizer in experiments on synthetic as well as real-world datasets for time-series and tabular d
    
[^57]: 利用Riesz核的生成式分割MMD流

    Generative Sliced MMD Flows with Riesz Kernels. (arXiv:2305.11463v1 [cs.LG])

    [http://arxiv.org/abs/2305.11463](http://arxiv.org/abs/2305.11463)

    本文使用Riesz核展示了生成式分割MMD流的高效计算方法，实现了在大规模应用中通过神经网络训练生成模型。

    

    在大规模计算中，最大平均差异度(MMD)流的计算成本很高。在本文中，我们展示了使用Riesz核$K(x,y)=-\|x-y\|^r$，$r \in (0,2)$的MMD流具有杰出的性质，可允许其进行高效计算。首先，Riesz核的MMD与其分割版本的MMD重合。因此，可以在一维设置中进行MMD梯度的计算。在此处，对于$r=1$，可以应用简单的排序算法将两个经验度量的复杂度从$O(MN+N^2)$降低到$O((M+N)\log(M+N))$，其中$M$和$N$是支持点。对于实现，我们通过仅使用有限数量的$P$个切片来近似分割MMD的梯度。我们展示了由此产生的误差具有$O(\sqrt{d/P})$的复杂度，其中$d$是数据维度。这些结果使我们能够通过神经网络近似MMD梯度流来训练生成模型，甚至用于大规模应用。

    Maximum mean discrepancy (MMD) flows suffer from high computational costs in large scale computations. In this paper, we show that MMD flows with Riesz kernels $K(x,y) = - \|x-y\|^r$, $r \in (0,2)$ have exceptional properties which allow for their efficient computation. First, the MMD of Riesz kernels coincides with the MMD of their sliced version. As a consequence, the computation of gradients of MMDs can be performed in the one-dimensional setting. Here, for $r=1$, a simple sorting algorithm can be applied to reduce the complexity from $O(MN+N^2)$ to $O((M+N)\log(M+N))$ for two empirical measures with $M$ and $N$ support points. For the implementations we approximate the gradient of the sliced MMD by using only a finite number $P$ of slices. We show that the resulting error has complexity $O(\sqrt{d/P})$, where $d$ is the data dimension. These results enable us to train generative models by approximating MMD gradient flows by neural networks even for large scale applications. We demo
    
[^58]: 突破智能体-环境界面，优化具有包容性的语言模型的微调

    Shattering the Agent-Environment Interface for Fine-Tuning Inclusive Language Models. (arXiv:2305.11455v1 [cs.CL])

    [http://arxiv.org/abs/2305.11455](http://arxiv.org/abs/2305.11455)

    该论文提出了一种新颖的思路，将预训练的语言模型本身同时作为策略、奖励函数和转移函数，可以直接进行奖励学习和语言模型微调，可以带来巨大的统计收益。

    

    在微调自回归语言模型中，从人类反馈的增强学习方法（RLHF）的重要组成部分是显式训练一个奖励模型来模拟人类反馈，而不是语言模型本身。然后，将这个奖励模型与策略梯度方法耦合起来，从而显著提高语言模型输出与期望响应之间的一致性。在这项工作中，我们采取了一种新颖的观点，即预训练的语言模型本身同时是策略、奖励函数和转移函数。这个观点的一个直接结果是，可以同时直接进行奖励学习和语言模型微调，无需进一步的下游策略优化。虽然这个观点确实打破了传统智能体-环境界面，但我们仍然认为，将强化学习的传统算法概念运用于这种方法中可以带来巨大的统计收益。

    A centerpiece of the ever-popular reinforcement learning from human feedback (RLHF) approach to fine-tuning autoregressive language models is the explicit training of a reward model to emulate human feedback, distinct from the language model itself. This reward model is then coupled with policy-gradient methods to dramatically improve the alignment between language model outputs and desired responses. In this work, we adopt a novel perspective wherein a pre-trained language model is itself simultaneously a policy, reward function, and transition function. An immediate consequence of this is that reward learning and language model fine-tuning can be performed jointly and directly, without requiring any further downstream policy optimization. While this perspective does indeed break the traditional agent-environment interface, we nevertheless maintain that there can be enormous statistical benefits afforded by bringing to bear traditional algorithmic concepts from reinforcement learning.
    
[^59]: 基于自监督调整的零样本文本分类算法

    Zero-Shot Text Classification via Self-Supervised Tuning. (arXiv:2305.11442v1 [cs.CL])

    [http://arxiv.org/abs/2305.11442](http://arxiv.org/abs/2305.11442)

    本文提出了一种基于自监督调整的零样本文本分类算法，通过使用无标签数据来调整语言模型，通过学习预测段落中的第一句话，实现了对未见过任务的零样本推断，模型不需要注释数据进行元调整，对模板的选择不敏感，并在实验中取得不错的结果。

    

    现有的零样本文本分类方法要么使用预训练语言模型进行提示，但这种方法对模板的选择非常敏感；要么依赖于大量相关任务的注释数据进行元调整。本文提出了一种基于自监督学习的新范式，通过使用无标签数据来调整语言模型，称为自监督调整。通过探索自由文本的内在结构，我们提出了一种新的学习目标，称为首句预测，以弥合无标签数据和文本分类任务之间的差距。调整模型以学习根据剩余文本来预测段落中的第一句话后，该模型能够推断出未见过的任务，如主题分类和情感分析。实验结果表明，我们的模型在10个任务中的7个任务上优于现有基准线。此外，分析表明，我们的模型对模板的选择不敏感，并且不需要注释数据进行元调整。

    Existing solutions to zero-shot text classification either conduct prompting with pre-trained language models, which is sensitive to the choices of templates, or rely on large-scale annotated data of relevant tasks for meta-tuning. In this work, we propose a new paradigm based on self-supervised learning to solve zero-shot text classification tasks by tuning the language models with unlabeled data, called self-supervised tuning. By exploring the inherent structure of free texts, we propose a new learning objective called first sentence prediction to bridge the gap between unlabeled data and text classification tasks. After tuning the model to learn to predict the first sentence in a paragraph based on the rest, the model is able to conduct zero-shot inference on unseen tasks such as topic classification and sentiment analysis. Experimental results show that our model outperforms the state-of-the-art baselines on 7 out of 10 tasks. Moreover, the analysis reveals that our model is less s
    
[^60]: 基于部分共享生成式对抗网络的高效联合学习框架 PS-FedGAN 用于数据隐私保护

    PS-FedGAN: An Efficient Federated Learning Framework Based on Partially Shared Generative Adversarial Networks For Data Privacy. (arXiv:2305.11437v1 [cs.LG])

    [http://arxiv.org/abs/2305.11437](http://arxiv.org/abs/2305.11437)

    本文提出了一种高效的联合学习框架 PS-FedGAN，通过部分共享生成式对抗网络以保护数据隐私，实现了在分布数据环境下捕捉本地数据总体特征，相比于现有框架具有更好的收敛速度、通信开销和隐私保护效果。

    

    联邦学习（FL）因在保护数据隐私的同时更好地捕捉基础数据统计信息而成为分布式计算的有效学习范例。然而，在FL客户之间存在实际数据异构性的情况下，现有FL框架在捕捉展现不同分布的本地用户数据的总体特征性能上仍然存在不足。为此，本文提出了一种新的FL框架PS-FedGAN，只需要部分的GAN模型共享，其中包括全局鉴别网络和部分共享的生成网络，可以以部分合作的方式协作学习全局数据统计并基于部分共享GAN进行本地数据再生。

    Federated Learning (FL) has emerged as an effective learning paradigm for distributed computation owing to its strong potential in capturing underlying data statistics while preserving data privacy. However, in cases of practical data heterogeneity among FL clients, existing FL frameworks still exhibit deficiency in capturing the overall feature properties of local client data that exhibit disparate distributions. In response, generative adversarial networks (GANs) have recently been exploited in FL to address data heterogeneity since GANs can be integrated for data regeneration without exposing original raw data. Despite some successes, existing GAN-related FL frameworks often incur heavy communication cost and also elicit other privacy concerns, which limit their applications in real scenarios. To this end, this work proposes a novel FL framework that requires only partial GAN model sharing. Named as PS-FedGAN, this new framework enhances the GAN releasing and training mechanism to a
    
[^61]: TELeR：用于基准测试复杂任务的LLM提示的通用分类法

    TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks. (arXiv:2305.11430v1 [cs.AI])

    [http://arxiv.org/abs/2305.11430](http://arxiv.org/abs/2305.11430)

    本文提出了一个通用分类法，可以用来设计具有特定属性的提示来执行各种复杂任务，从而解决了LLM在执行复杂任务方面的性能变异巨大的问题。

    

    尽管LLM在传统对话环境中理解和生成文本时取得了巨大成功，但它们在执行不明确的复杂任务方面的潜力仍然受到很少的研究。本文提出了一种通用分类法，可以用来设计具有特定属性的提示，以执行各种复杂任务，从而解决了使用不同提示类型/风格和提示提供的不同详细程度时LLM性能变化巨大的问题。这个分类法将使未来的基准测试研究能够报告研究中使用的特定提示类别，从而实现跨不同研究的有意义的比较。

    While LLMs have shown great success in understanding and generating text in traditional conversational settings, their potential for performing ill-defined complex tasks is largely under-studied. Indeed, we are yet to conduct comprehensive benchmarking studies with multiple LLMs that are exclusively focused on a complex task. However, conducting such benchmarking studies is challenging because of the large variations in LLMs' performance when different prompt types/styles are used and different degrees of detail are provided in the prompts. To address this issue, the paper proposes a general taxonomy that can be used to design prompts with specific properties in order to perform a wide range of complex tasks. This taxonomy will allow future benchmarking studies to report the specific categories of prompts used as part of the study, enabling meaningful comparisons across different studies. Also, by establishing a common standard through this taxonomy, researchers will be able to draw mo
    
[^62]: 图传播变换器用于图表示学习

    Graph Propagation Transformer for Graph Representation Learning. (arXiv:2305.11424v1 [cs.LG])

    [http://arxiv.org/abs/2305.11424](http://arxiv.org/abs/2305.11424)

    本文提出了一种新的变换器架构 GPTrans，以图传播注意力为基础，可以更好地学习图形模型，并在多个基准测试集上超过了其他最先进的基于变换器的图形模型。

    

    本文提出了一种用于图表示学习的新型变换器架构。我们的方法的核心见解是在构建变换器块中的注意力模块时，充分考虑图中节点和边之间的信息传播。具体而言，我们提出了一种新的注意力机制称为图传播注意力（GPA），它将信息在节点和边之间以三种方式明确传递，即从节点到节点，从节点到边和从边到节点，这对于学习图结构数据至关重要。在此基础上，我们设计了一种名为图传播变换器（GPTrans）的有效变换器架构，进一步帮助学习图数据。我们在几个基准数据集上的广泛图学习实验中验证了GPTrans的性能。这些结果表明，我们的方法以更好的性能超过了许多最先进的基于变换器的图形模型。代码将在https://github.com/czczup/GPTrans上发布。

    This paper presents a novel transformer architecture for graph representation learning. The core insight of our method is to fully consider the information propagation among nodes and edges in a graph when building the attention module in the transformer blocks. Specifically, we propose a new attention mechanism called Graph Propagation Attention (GPA). It explicitly passes the information among nodes and edges in three ways, i.e. node-to-node, node-to-edge, and edge-to-node, which is essential for learning graph-structured data. On this basis, we design an effective transformer architecture named Graph Propagation Transformer (GPTrans) to further help learn graph data. We verify the performance of GPTrans in a wide range of graph learning experiments on several benchmark datasets. These results show that our method outperforms many state-of-the-art transformer-based graph models with better performance. The code will be released at https://github.com/czczup/GPTrans.
    
[^63]: 超越指数图：有限时间收敛的通信效率拓扑用于分散学习

    Beyond Exponential Graph: Communication-Efficient Topologies for Decentralized Learning via Finite-time Convergence. (arXiv:2305.11420v1 [cs.LG])

    [http://arxiv.org/abs/2305.11420](http://arxiv.org/abs/2305.11420)

    本文介绍了一种新型拓扑——基础$(k+1)$图，其中节点在有限的迭代次数后能达到确切的共识，具有快速共识率和小的最大度数，从而可以用于分散式SGD。

    

    近年来越来越多的研究关注于分散式学习在并行计算和隐私保护中的应用。许多最近的研究指出，具有更快共识率（即谱间隙）的底层网络拓扑可导致分散式学习的更好收敛速度和准确性。然而，具有快速共识率的拓扑，如指数图，通常具有较大的最大度数，这会导致重要的通信成本。因此，寻求既具有快速共识率又具有小的最大度数的拓扑是重要的。在本研究中，我们提出了一种结合快速共识率和小最大度的新型拓扑，称为基础$(k+1)$ 图。与现有的拓扑不同，基础$(k+1)$ 图使所有节点在有限的迭代次数后都能达到确切的共识，对于任何节点数和最大度k都适用。得益于这个有利的属性，基础$(k+1)$ 图赋予了分散式SGD

    Decentralized learning has recently been attracting increasing attention for its applications in parallel computation and privacy preservation. Many recent studies stated that the underlying network topology with a faster consensus rate (a.k.a. spectral gap) leads to a better convergence rate and accuracy for decentralized learning. However, a topology with a fast consensus rate, e.g., the exponential graph, generally has a large maximum degree, which incurs significant communication costs. Thus, seeking topologies with both a fast consensus rate and small maximum degree is important. In this study, we propose a novel topology combining both a fast consensus rate and small maximum degree called the Base-$(k + 1)$ Graph. Unlike the existing topologies, the Base-$(k + 1)$ Graph enables all nodes to reach the exact consensus after a finite number of iterations for any number of nodes and maximum degree k. Thanks to this favorable property, the Base-$(k + 1)$ Graph endows Decentralized SGD
    
[^64]: JetSeg: 低功耗GPU嵌入式系统的高效实时语义分割模型

    JetSeg: Efficient Real-Time Semantic Segmentation Model for Low-Power GPU-Embedded Systems. (arXiv:2305.11419v1 [cs.CV])

    [http://arxiv.org/abs/2305.11419](http://arxiv.org/abs/2305.11419)

    JetSeg是一个专为GPU-嵌入式系统设计的高效实时语义分割模型，通过新型的轻量级高效块JetBlock和结合了不对称和非对称卷积、深度空洞卷积、通道混洗操作、轻量级激活函数和适用于嵌入式系统的方便数量的组卷积的策略JetConv以及创新的损失函数JetLoss，在保持高精度的同时，明显减少了内存使用量和推理时间，优于最先进的模型。

    

    实时语义分割是一项具有挑战性的任务，需要精准和具有低推理时间的模型。在嵌入式系统上实现这些模型受到硬件能力和内存使用的限制，导致了瓶颈。我们提出了一种名为JetSeg的实时语义分割高效模型，由一个称为JetNet的编码器和一个改进的RegSeg解码器组成。JetNet专为GPU嵌入式系统设计，并包括两个主要组件：一个称为JetBlock的新型轻量级高效块，通过减少参数数量来最小化内存使用和推理时间，而不会牺牲准确性；一种称为JetConv 的新策略，它结合了不对称和非对称卷积、深度空洞卷积、通道混洗操作、轻量级激活函数和适用于嵌入式系统的方便数量的组卷积，并引入一种创新的损失函数JetLoss，它集成了精确度、召回率和F1分值。实验结果表明，JetSeg在保持高精度的同时，明显减少了内存使用量和推理时间，并优于最先进的模型。

    Real-time semantic segmentation is a challenging task that requires high-accuracy models with low-inference times. Implementing these models on embedded systems is limited by hardware capability and memory usage, which produces bottlenecks. We propose an efficient model for real-time semantic segmentation called JetSeg, consisting of an encoder called JetNet, and an improved RegSeg decoder. The JetNet is designed for GPU-Embedded Systems and includes two main components: a new light-weight efficient block called JetBlock, that reduces the number of parameters minimizing memory usage and inference time without sacrificing accuracy; a new strategy that involves the combination of asymmetric and non-asymmetric convolutions with depthwise-dilated convolutions called JetConv, a channel shuffle operation, light-weight activation functions, and a convenient number of group convolutions for embedded systems, and an innovative loss function named JetLoss, which integrates the Precision, Recall,
    
[^65]: 从功能等价的角度看前馈神经网络的复杂性。

    Complexity of Feed-Forward Neural Networks from the Perspective of Functional Equivalence. (arXiv:2305.11417v1 [cs.LG])

    [http://arxiv.org/abs/2305.11417](http://arxiv.org/abs/2305.11417)

    本文从功能等价的角度出发研究前馈神经网络的复杂性，发现利用置换不变性的特性可以降低网络的复杂度，通过过参数化可以增加训练网络的容易程度，并对深度学习中的优化和泛化理解具有重要意义。

    

    本文通过考察功能等价的概念来研究前馈神经网络的复杂性，该概念表明不同的网络参数化可以导致相同的函数。我们利用置换不变性的特性为前馈神经网络类导出了一个新的覆盖数上界，发现利用该性质可以降低神经网络的复杂度。此外，基于参数空间的对称结构，我们证明适当的随机参数初始化策略可以增加优化收敛的概率。我们发现，过参数化的网络往往更容易训练，即增加神经网络的宽度会导致有效参数空间的体积趋近于零。本研究结果揭示了过参数化的新见解，并对深度学习中的泛化和优化理解具有重要意义。

    In this paper, we investigate the complexity of feed-forward neural networks by examining the concept of functional equivalence, which suggests that different network parameterizations can lead to the same function. We utilize the permutation invariance property to derive a novel covering number bound for the class of feedforward neural networks, which reveals that the complexity of a neural network can be reduced by exploiting this property. Furthermore, based on the symmetric structure of parameter space, we demonstrate that an appropriate strategy of random parameter initialization can increase the probability of convergence for optimization. We found that overparameterized networks tend to be easier to train in the sense that increasing the width of neural networks leads to a vanishing volume of the effective parameter space. Our findings offer new insights into overparameterization and have significant implications for understanding generalization and optimization in deep learning
    
[^66]: 联邦基础模型：用于大模型的隐私保护协作学习

    Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models. (arXiv:2305.11414v1 [cs.LG])

    [http://arxiv.org/abs/2305.11414](http://arxiv.org/abs/2305.11414)

    本文提出了联邦基础模型（FFMs）的概念，结合了基础模型和联邦学习的优势，可实现跨多个机构的隐私保护和协作学习。

    

    基础模型通常需要大量数据进行预训练，如BERT、GPT、ViT和CLIP，但其优化通常需要访问敏感数据，引发隐私问题并限制其适用性。为解决这一问题，本文提出了联邦基础模型（FFMs）的概念，这是一种新颖的方法，结合了基础模型和联邦学习的优势，可实现跨多个机构的隐私保护和协作学习。

    Foundation Models (FMs), such as BERT, GPT, ViT, and CLIP, have demonstrated remarkable success in a wide range of applications, driven by their ability to leverage vast amounts of data for pre-training. However, optimizing FMs often requires access to sensitive data, raising privacy concerns and limiting their applicability in certain domains. In this paper, we introduce the concept of Federated Foundation Models (FFMs), a novel approach that combines the benefits of FMs and Federated Learning (FL) to enable privacy-preserving and collaborative learning across multiple institutions. We discuss the potential benefits and challenges of integrating FL into the lifespan of FMs, covering pre-training, fine-tuning, and application. We further provide formal definitions of FFM tasks, including FFM pre-training, FFM fine-tuning, and federated prompt engineering, allowing for more personalized and context-aware models while maintaining data privacy. Moreover, we explore the possibility of cont
    
[^67]: AlignAtt：使用基于注意力的音频翻译对齐作为同时语音翻译的指导

    AlignAtt: Using Attention-based Audio-Translation Alignments as a Guide for Simultaneous Speech Translation. (arXiv:2305.11408v1 [cs.CL])

    [http://arxiv.org/abs/2305.11408](http://arxiv.org/abs/2305.11408)

    AlignAtt是一种新型的SimulST策略，使用基于注意力的音频翻译对齐来指导模型，在BLEU和延迟方面均优于之前的策略。

    

    注意力是当今自然语言处理中最常用的架构的核心机制，并已从许多角度进行分析，包括其在机器翻译相关任务中的有效性。在这些研究中，注意力在输入文本被替换为音频片段的情况下，也是获取有关单词对齐的有用信息的一种方式，例如语音翻译（ST）任务。在本文中，我们提出了AlignAtt，一种新颖的同时ST（SimulST）策略，它利用注意力信息来生成源-目标对齐，以在推理过程中指导模型。通过对MuST-C v1.0的8种语言对的实验，我们发现，在线下训练的模型上应用先前的最新SimulST策略，AlignAtt在BLEU方面获得了2个分数的提高，并且8种语言的延迟缩减在0.5秒到0.8秒之间。

    Attention is the core mechanism of today's most used architectures for natural language processing and has been analyzed from many perspectives, including its effectiveness for machine translation-related tasks. Among these studies, attention resulted to be a useful source of information to get insights about word alignment also when the input text is substituted with audio segments, as in the case of the speech translation (ST) task. In this paper, we propose AlignAtt, a novel policy for simultaneous ST (SimulST) that exploits the attention information to generate source-target alignments that guide the model during inference. Through experiments on the 8 language pairs of MuST-C v1.0, we show that AlignAtt outperforms previous state-of-the-art SimulST policies applied to offline-trained models with gains in terms of BLEU of 2 points and latency reductions ranging from 0.5s to 0.8s across the 8 languages.
    
[^68]: 面向有条件生成对抗网络的少样本连续学习

    Few-Shot Continual Learning for Conditional Generative Adversarial Networks. (arXiv:2305.11400v1 [cs.LG])

    [http://arxiv.org/abs/2305.11400](http://arxiv.org/abs/2305.11400)

    本文提出了一种新的连续学习方法，适用于条件生成对抗网络，根据cGAN的判别器数据识别出最接近目标的现有模式，并通过扩展连续学习模型，使用回放生成的数据来训练目标模式的cGAN模型，以避免灾难性遗忘，提高了生成性能。

    

    在生成模型的少样本连续学习中，必须学习目标模式，并在不影响先前学习到的模式的情况下仅使用有限的样本。本文针对条件生成对抗网络提出了一种新的连续学习方法，基于一种新的用于生成建模的模式亲和力量度。我们的度量完全基于cGAN的判别器，可以识别最接近目标的现有模式。随后，我们通过包含基于最接近模式的加权标签来扩展连续学习模型。为了预防灾难性遗忘，我们首先使用cGAN的生成器生成带标签的数据样本，然后通过回放生成的数据来训练目标模式的cGAN模型。我们的实验结果证明了我们的方法在提高生成性能方面的有效性，超越了各种标准和最先进的方法。

    In few-shot continual learning for generative models, a target mode must be learned with limited samples without adversely affecting the previously learned modes. In this paper, we propose a new continual learning approach for conditional generative adversarial networks (cGAN) based on a new mode-affinity measure for generative modeling. Our measure is entirely based on the cGAN's discriminator and can identify the existing modes that are most similar to the target. Subsequently, we expand the continual learning model by including the target mode using a weighted label derived from those of the closest modes. To prevent catastrophic forgetting, we first generate labeled data samples using the cGAN's generator, and then train the cGAN model for the target mode while memory replaying with the generated data. Our experimental results demonstrate the efficacy of our approach in improving the generation performance over the baselines and the state-of-the-art approaches for various standard 
    
[^69]: 通过验证和验证的视角对大型语言模型的安全性和可信度进行调查

    A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation. (arXiv:2305.11391v1 [cs.AI])

    [http://arxiv.org/abs/2305.11391](http://arxiv.org/abs/2305.11391)

    通过验证和验证的视角对大型语言模型的安全性和可信度进行调查，分类它们的已知漏洞，将其分为固有问题、有意攻击和意外错误。同时，考虑四种互补技术以提供LLM及其应用的安全和可信度保障。

    

    大型语言模型（LLM）以其在许多知识领域中为终端用户提供详细和有条理的答案，并能够进行人类级别的对话能力，引发了AI的一波新热潮。为了应对它们在许多工业应用中的快速采用，本次调查关注它们的安全性和可信度。首先，我们回顾LLM的已知漏洞，将它们分类为固有问题、有意攻击和意外错误。然后，我们考虑是否以及如何将已被广泛用于传统软件和深度学习模型（如卷积神经网络）的验证和验证（V＆V）技术，集成并进一步扩展到LLM的整个生命周期中，以提供严格的分析，确保LLM及其应用的安全和可信度。具体而言，我们考虑四种互补技术：虚假性和评估、验证、运行时监视和道德使用。考虑到LLM的快速发展，

    Large Language Models (LLMs) have exploded a new heatwave of AI, for their ability to engage end-users in human-level conversations with detailed and articulate answers across many knowledge domains. In response to their fast adoption in many industrial applications, this survey concerns their safety and trustworthiness. First, we review known vulnerabilities of the LLMs, categorising them into inherent issues, intended attacks, and unintended bugs. Then, we consider if and how the Verification and Validation (V&V) techniques, which have been widely developed for traditional software and deep learning models such as convolutional neural networks, can be integrated and further extended throughout the lifecycle of the LLMs to provide rigorous analysis to the safety and trustworthiness of LLMs and their applications. Specifically, we consider four complementary techniques: falsification and evaluation, verification, runtime monitoring, and ethical use. Considering the fast development of 
    
[^70]: ALT: 一种用于长尾场景建模的自动化系统

    ALT: An Automatic System for Long Tail Scenario Modeling. (arXiv:2305.11390v1 [cs.LG])

    [http://arxiv.org/abs/2305.11390](http://arxiv.org/abs/2305.11390)

    本文提出了一种名为ALT的自动化系统，用于解决长尾场景建模问题，实现了对于模型训练和推理阶段人力和资源有限的情况下的有效建模需求。

    

    本文考虑了长尾场景建模的问题，其中包括对于模型训练阶段人力资源不足以及模型推理阶段时间和计算资源有限的情况。我们提出了一种名为ALT的自动化系统来解决这个问题。我们采用了多种自动化机器学习相关技术，采用元学习哲学，并提出了一种基于有限预算的神经架构搜索方法等，以改进系统中使用的算法。此外，从系统的角度进行了多项优化，并加入了必要的模块，使系统更具可行性和效率。我们进行了大量实验证明我们的系统的有效性，并展示了我们系统中关键模块的实用性。

    In this paper, we consider the problem of long tail scenario modeling with budget limitation, i.e., insufficient human resources for model training stage and limited time and computing resources for model inference stage. This problem is widely encountered in various applications, yet has received deficient attention so far. We present an automatic system named ALT to deal with this problem. Several efforts are taken to improve the algorithms used in our system, such as employing various automatic machine learning related techniques, adopting the meta learning philosophy, and proposing an essential budget-limited neural architecture search method, etc. Moreover, to build the system, many optimizations are performed from a systematic perspective, and essential modules are armed, making the system more feasible and efficient. We perform abundant experiments to validate the effectiveness of our system and demonstrate the usefulness of the critical modules in our system. Moreover, online r
    
[^71]: 面向领域泛化的深度图形转换方法

    Domain Generalization Deep Graph Transformation. (arXiv:2305.11389v1 [cs.LG])

    [http://arxiv.org/abs/2305.11389](http://arxiv.org/abs/2305.11389)

    本文提出了一种面向领域泛化的深度图形转换方法，使用超网络进行多输入多输出的图神经网络预测，通过加入潜在变量来训练泛化模型，在多个基准数据集中取得了优异表现。

    

    预测图形从一种模式转变为另一种模式的图形转换是一个重要和常见的问题。近年来，虽然在开发先进的图形转换技术方面取得了很多进展，但通常需要机器学习模型所需的基本假设是测试和训练数据保持相同的分布，并不总是成立。因此，预测在训练数据中不可用的图形的领域泛化图形转换是一个未被充分探索的领域，需要解决多个关键挑战，包括（1）当训练所有输入-输出模式组合时的极端空间复杂度、（2）输入和输出模式之间的图形拓扑差异，以及(3)如何将模型泛化到在训练数据中不存在的（未见过的）目标域。为了填补这一空白，我们提出了一种基于深度图神经网络的多输入-多输出、超网络的图形转换方法（MultiHyperGNN），它利用编码器和解码器来编码输入和输出图的拓扑结构，并生成输出图。我们还设计了一个领域泛化框架，通过加入捕捉领域特定信息的潜在变量，来以有限的标记数据训练所提出的模型。实验结果表明，我们提出的方法在几个基准数据集上优于基线方法。

    Graph transformation that predicts graph transition from one mode to another is an important and common problem. Despite much progress in developing advanced graph transformation techniques in recent years, the fundamental assumption typically required in machine-learning models that the testing and training data preserve the same distribution does not always hold. As a result, domain generalization graph transformation that predicts graphs not available in the training data is under-explored, with multiple key challenges to be addressed including (1) the extreme space complexity when training on all input-output mode combinations, (2) difference of graph topologies between the input and the output modes, and (3) how to generalize the model to (unseen) target domains that are not in the training data. To fill the gap, we propose a multi-input, multi-output, hypernetwork-based graph neural network (MultiHyperGNN) that employs a encoder and a decoder to encode topologies of both input an
    
[^72]: 信息瓶颈理论的公正之声

    Justices for Information Bottleneck Theory. (arXiv:2305.11387v1 [cs.LG])

    [http://arxiv.org/abs/2305.11387](http://arxiv.org/abs/2305.11387)

    本论文从引入一个辅助函数，证明深度学习中ReLU激活下互信息下降的悖论，挑战了对信息瓶颈理论适用性的质疑，提供了使用该理论解释DL网络内部组织的新方法。

    

    本研究对信息瓶颈（IB）理论不断增加的质疑做出及时回应，注入新的观点以纠正误解并重申其有效性。首先，我们引入一种辅助函数，将最大编码率降低法解释为信息瓶颈理论的特殊但局部最优情况。通过这种辅助函数，我们澄清了在深度学习（DL）网络中应用ReLU激活时互信息下降的悖论。其次，我们通过辅助函数的视角，证明了IB理论解释线性激活函数在隐藏层中没有压缩阶段的能力，挑战了对IB理论适用性的质疑。最后，通过提出一种新的理论观点，我们使用IB理论提供了一种解释DL网络内部组织的新方法，并与最近的实验证据相一致。因此，本文是对IB理论的公正之声，也为未来该理论的发展提供了前瞻性的思考。

    This study comes as a timely response to mounting criticism of the information bottleneck (IB) theory, injecting fresh perspectives to rectify misconceptions and reaffirm its validity. Firstly, we introduce an auxiliary function to reinterpret the maximal coding rate reduction method as a special yet local optimal case of IB theory. Through this auxiliary function, we clarify the paradox of decreasing mutual information during the application of ReLU activation in deep learning (DL) networks. Secondly, we challenge the doubts about IB theory's applicability by demonstrating its capacity to explain the absence of a compression phase with linear activation functions in hidden layers, when viewed through the lens of the auxiliary function. Lastly, by taking a novel theoretical stance, we provide a new way to interpret the inner organizations of DL networks by using IB theory, aligning them with recent experimental evidence. Thus, this paper serves as an act of justice for IB theory, poten
    
[^73]: 在电子健康记录中提高AI模型公平性：联邦学习方法的案例研究

    Improving Fairness in AI Models on Electronic Health Records: The Case for Federated Learning Methods. (arXiv:2305.11386v1 [cs.LG])

    [http://arxiv.org/abs/2305.11386](http://arxiv.org/abs/2305.11386)

    本研究提出了一种利用联邦学习方法来提高医疗AI模型公平性的方法，包括对抗去偏见和公平聚合方法，适用于各种公平度量标准，使医疗机构可以有效合作。

    

    开发能够保持公平性的AI工具是至关重要的，特别是在医疗保健等高风险应用中。然而，医疗AI模型的整体预测性能通常优先于这些模型可能存在的偏见。在这项研究中，我们展示了一种可能的方法，通过医疗机构通过联邦学习范式（FL）合作来缓解偏见问题（这是医疗保健领域中广泛使用的一种选择）。虽然先前已经提出了注重公平性的FL方法，但它们的基本模型和本地实施技术，以及它们可能应用于医疗领域，仍然广泛未经研究。因此，我们在使用电子健康记录的医疗领域提出了一种全面的FL方法，其中包括对抗去偏见和公平聚合方法，适用于各种公平度量标准。我们的方法不仅明确地缓解了偏见作为优化的一部分，还提高了预测性能。

    Developing AI tools that preserve fairness is of critical importance, specifically in high-stakes applications such as those in healthcare. However, health AI models' overall prediction performance is often prioritized over the possible biases such models could have. In this study, we show one possible approach to mitigate bias concerns by having healthcare institutions collaborate through a federated learning paradigm (FL; which is a popular choice in healthcare settings). While FL methods with an emphasis on fairness have been previously proposed, their underlying model and local implementation techniques, as well as their possible applications to the healthcare domain remain widely underinvestigated. Therefore, we propose a comprehensive FL approach with adversarial debiasing and a fair aggregation method, suitable to various fairness metrics, in the healthcare domain where electronic health records are used. Not only our approach explicitly mitigates bias as part of the optimizatio
    
[^74]: 创作者经济学中的在线学习方法

    Online Learning in a Creator Economy. (arXiv:2305.11381v1 [cs.GT])

    [http://arxiv.org/abs/2305.11381](http://arxiv.org/abs/2305.11381)

    本文探讨了如何应用在线学习的方法优化创作者经济学中的平台收益，分析和比较了基于回报的和基于特征的两种合同类型。

    

    创作者经济学革新了通过在线平台获取利润的方式。本文通过将创作者经济学建模为用户、平台和内容创作者之间的三方博弈，探讨了如何应用在线学习的方法，通过合同和推荐系统优化平台收益。该研究主要分析和比较了两种类型的合同：基于回报的合同和基于特征的合同。

    The creator economy has revolutionized the way individuals can profit through online platforms. In this paper, we initiate the study of online learning in the creator economy by modeling the creator economy as a three-party game between the users, platform, and content creators, with the platform interacting with the content creator under a principal-agent model through contracts to encourage better content. Additionally, the platform interacts with the users to recommend new content, receive an evaluation, and ultimately profit from the content, which can be modeled as a recommender system.  Our study aims to explore how the platform can jointly optimize the contract and recommender system to maximize the utility in an online learning fashion. We primarily analyze and compare two families of contracts: return-based contracts and feature-based contracts. Return-based contracts pay the content creator a fraction of the reward the platform gains. In contrast, feature-based contracts pay 
    
[^75]: 广义精度矩阵用于可伸缩估计非参数马尔科夫网络

    Generalized Precision Matrix for Scalable Estimation of Nonparametric Markov Networks. (arXiv:2305.11379v1 [cs.LG])

    [http://arxiv.org/abs/2305.11379](http://arxiv.org/abs/2305.11379)

    本文提出了一种广义精度矩阵（GPM）用于描述所有数据类型的条件独立结构，并允许变量之间的一般功能关系。同时，提出了一种马尔科夫网络结构学习算法，在处理大图时，使用了统一的正则化得分匹配框架以提高可伸缩性。

    

    马尔科夫网络给出了一组随机变量的条件独立性结构，现有的工作侧重于特定的分布族（例如指数族）和/或特定的图结构，而且大多数只能处理同一种数据类型的变量（连续或离散）。在本文中，我们使用广义精度矩阵（GPM）在所有数据类型（即连续、离散和混合类型）的一般分布中描述条件独立结构。此外，我们还允许变量之间的一般功能关系，从而产生一种最一般的马尔科夫网络结构学习算法。为了处理问题的计算挑战，尤其是对于大图，我们将所有情况统一到一个正则化得分匹配框架下。我们验证了理论结果并在各种设置下演示了可伸缩性。

    A Markov network characterizes the conditional independence structure, or Markov property, among a set of random variables. Existing work focuses on specific families of distributions (e.g., exponential families) and/or certain structures of graphs, and most of them can only handle variables of a single data type (continuous or discrete). In this work, we characterize the conditional independence structure in general distributions for all data types (i.e., continuous, discrete, and mixed-type) with a Generalized Precision Matrix (GPM). Besides, we also allow general functional relations among variables, thus giving rise to a Markov network structure learning algorithm in one of the most general settings. To deal with the computational challenge of the problem, especially for large graphs, we unify all cases under the same umbrella of a regularized score matching framework. We validate the theoretical results and demonstrate the scalability empirically in various settings.
    
[^76]: GraphFC: 带有少量标签数据的海关欺诈检测

    GraphFC: Customs Fraud Detection with Label Scarcity. (arXiv:2305.11377v1 [cs.LG])

    [http://arxiv.org/abs/2305.11377](http://arxiv.org/abs/2305.11377)

    GraphFC是一个模型不可知、领域特定、半监督图神经网络框架，用于少量标签数据的海关欺诈检测。它利用了图神经网络和半监督学习，有效地结合了标记和未标记的数据，提高欺诈检测的性能。

    

    全球的海关官员每年面对着海量的交易。随着连通性和全球化的增加，海关交易持续增长。与海关交易相关的是海关欺诈——即有意修改货物申报以避免税款和关税。由于人手不足，海关办公室只能对有限数量的申报进行手动检查。这需要通过机器学习（ML）技术自动化海关欺诈检测。然而，由于新进申报检查的标签数据受限，ML方法应具有稳健的性能。本文提出了$\textbf{GraphFC}$（海关欺诈$\textbf{GNN}$（$\textbf{G}$raph $\textbf{N}$eural $\textbf{N}$etworks）），这是一个模型不可知、领域特定、半监督图神经网络框架，用于少量标签数据的欺诈检测。$\textbf{GraphFC}$利用图神经网络和半监督学习的最新进展，有效地结合标记和未标记数据，提高了欺诈检测的性能。我们在实际的海关交易数据集上评估了我们的提议框架，并展示了它在标签数据稀缺的情况下检测海关欺诈的有效性。

    Custom officials across the world encounter huge volumes of transactions. With increased connectivity and globalization, the customs transactions continue to grow every year. Associated with customs transactions is the customs fraud - the intentional manipulation of goods declarations to avoid the taxes and duties. With limited manpower, the custom offices can only undertake manual inspection of a limited number of declarations. This necessitates the need for automating the customs fraud detection by machine learning (ML) techniques. Due the limited manual inspection for labeling the new-incoming declarations, the ML approach should have robust performance subject to the scarcity of labeled data. However, current approaches for customs fraud detection are not well suited and designed for this real-world setting. In this work, we propose $\textbf{GraphFC}$ ($\textbf{Graph}$ neural networks for $\textbf{C}$ustoms $\textbf{F}$raud), a model-agnostic, domain-specific, semi-supervised graph
    
[^77]: 智能压力电子垫用于人类睡眠姿势和动态活动识别

    Smart Pressure e-Mat for Human Sleeping Posture and Dynamic Activity Recognition. (arXiv:2305.11367v1 [cs.CV])

    [http://arxiv.org/abs/2305.11367](http://arxiv.org/abs/2305.11367)

    本文介绍了一种基于Velostat的智能压力电子垫系统，可用于识别人体姿势和运动，具有高精度。

    

    在强调医疗保健、早期教育和健身方面，越来越多的非侵入式测量和识别方法受到关注。压力感应由于其简单的结构、易于访问、可视化应用和无害性而得到广泛研究。本文介绍了一种基于压敏材料Velostat的智能压力电子垫(SP e-Mat)系统，用于人体监测应用，包括睡眠姿势、运动和瑜伽识别。在子系统扫描电子垫读数并处理信号后，它生成一个压力图像流。采用深度神经网络(DNNs)来拟合和训练压力图像流，并识别相应的人类行为。四种睡眠姿势和受Nintendo Switch Ring Fit Adventure(RFA)启发的五种动态活动被用作拟议的SPeM系统的初步验证。SPeM系统在两种应用中均达到了较高的准确性，这证明了其高精度和。

    With the emphasis on healthcare, early childhood education, and fitness, non-invasive measurement and recognition methods have received more attention. Pressure sensing has been extensively studied due to its advantages of simple structure, easy access, visualization application, and harmlessness. This paper introduces a smart pressure e-mat (SPeM) system based on a piezoresistive material Velostat for human monitoring applications, including sleeping postures, sports, and yoga recognition. After a subsystem scans e-mat readings and processes the signal, it generates a pressure image stream. Deep neural networks (DNNs) are used to fit and train the pressure image stream and recognize the corresponding human behavior. Four sleeping postures and five dynamic activities inspired by Nintendo Switch Ring Fit Adventure (RFA) are used as a preliminary validation of the proposed SPeM system. The SPeM system achieves high accuracies on both applications, which demonstrates the high accuracy and
    
[^78]: 差分隐私在线物品定价

    Differentially Private Online Item Pricing. (arXiv:2305.11362v1 [cs.GT])

    [http://arxiv.org/abs/2305.11362](http://arxiv.org/abs/2305.11362)

    本文介绍了一种差分隐私算法，可在保护买家隐私的同时实现重复、不限供应物品拍卖中的收益最大化，是第一个提供隐私保证的$O(\sqrt{T}\log{T})$亏损子线性的方法。

    

    本文探讨了在保护买方隐私的同时，实现重复、不限供应物品拍卖中的收益最大化问题。我们提出了一种新颖的算法，它与买方的输入对（商品选择和出价）具有差分隐私性质。值得注意的是，我们的算法是第一个提供隐私保证的$O(\sqrt{T}\log{T})$亏损子线性(regret)的方法。我们的方法基于指数权重元算法，通过小的随机扰动缓解了收益函数不连续的问题。由于与指数机制的结构相似，因此我们的方法固有地保证了差分隐私。我们还将我们的算法扩展到逐轮策略性出价的情况。内在的差分隐私性质使我们能够在最小修改的情况下适应这种情况，并确保其亏损子线性。

    This work addresses the problem of revenue maximization in a repeated, unlimited supply item-pricing auction while preserving buyer privacy. We present a novel algorithm that provides differential privacy with respect to the buyer's input pair: item selection and bid. Notably, our algorithm is the first to offer a sublinear $O(\sqrt{T}\log{T})$ regret with a privacy guarantee. Our method is based on an exponential weights meta-algorithm, and we mitigate the issue of discontinuities in revenue functions via small random perturbations. As a result of its structural similarity to the exponential mechanism, our method inherently secures differential privacy. We also extend our algorithm to accommodate scenarios where buyers strategically bid over successive rounds. The inherent differential privacy allows us to adapt our algorithm with minimal modification to ensure a sublinear regret in this setting.
    
[^79]: 差分隐私适配器与参数高效声学建模

    Differentially Private Adapters for Parameter Efficient Acoustic Modeling. (arXiv:2305.11360v1 [cs.SD])

    [http://arxiv.org/abs/2305.11360](http://arxiv.org/abs/2305.11360)

    本研究提出一种高效的方案，将差分隐私保证引入跨语言语音分类器的适配中，使用教师-学生集成和残差适配器可以显著降低训练成本和时间，同时保持DP保证，使得可训练参数减少97.5%且性能基本不受影响。

    

    本文提出了一种高效的方案，将差分隐私（DP）保证引入跨语言语音分类器的适配中。我们探索了一种新的冻结预训练适配框架，用于保持DP保护语音建模，无需全模型微调。首先，我们将一个有噪声的教师-学生集成引入传统的适配方案中，利用冻结的预训练声学模型，并取得比基于随机梯度下降的DP（DPSGD）更好的性能。接着，我们在冻结的预训练声学模型的层之间插入残差适配器（RA）。RA显著降低了训练成本和时间，同时性能下降可忽略不计。在公开的多语言口语词语（MLSW）数据集上进行评估，我们的方案使用RA降低了97.5%的可训练参数，而与微调跨语言语音分类器相比仅有4%的性能下降，同时保持DP保证。

    In this work, we devise a parameter-efficient solution to bring differential privacy (DP) guarantees into adaptation of a cross-lingual speech classifier. We investigate a new frozen pre-trained adaptation framework for DP-preserving speech modeling without full model fine-tuning. First, we introduce a noisy teacher-student ensemble into a conventional adaptation scheme leveraging a frozen pre-trained acoustic model and attain superior performance than DP-based stochastic gradient descent (DPSGD). Next, we insert residual adapters (RA) between layers of the frozen pre-trained acoustic model. The RAs reduce training cost and time significantly with a negligible performance drop. Evaluated on the open-access Multilingual Spoken Words (MLSW) dataset, our solution reduces the number of trainable parameters by 97.5% using the RAs with only a 4% performance drop with respect to fine-tuning the cross-lingual speech classifier while preserving DP guarantees.
    
[^80]: 通过多智能体强化学习理解世界以解决社会困境

    Understanding the World to Solve Social Dilemmas Using Multi-Agent Reinforcement Learning. (arXiv:2305.11358v1 [cs.LG])

    [http://arxiv.org/abs/2305.11358](http://arxiv.org/abs/2305.11358)

    本文通过研究自利理性代理在多智能体强化学习环境中学习世界模型的行为，发现具有世界模型的代理群体在处理可能出现的社会困境时表现出色。这是第一项展示代理可以通过学习现实世界来解决社会困境的工作。

    

    社会困境是一种情况，个体团体之间能够从相互合作中获益，但冲突的利益阻碍了他们的合作。这种情况类似于人类许多最关键的挑战，发现促进合作行为出现的机制仍然是一个未解决的问题。本文研究自利理性代理在多智能体强化学习环境中学习世界模型的行为，并共存于可能出现社会困境的环境中。我们的模拟结果表明，具有世界模型的代理群体在处理可能出现社会困境的情况时表现出色。我们利用世界模型结构定性评估所学动态，并确认每个代理的世界模型能够编码变化环境和其他代理的行动信息。这是第一项展示代理可以通过学习现实世界来解决社会困境的工作。

    Social dilemmas are situations where groups of individuals can benefit from mutual cooperation but conflicting interests impede them from doing so. This type of situations resembles many of humanity's most critical challenges, and discovering mechanisms that facilitate the emergence of cooperative behaviors is still an open problem. In this paper, we study the behavior of self-interested rational agents that learn world models in a multi-agent reinforcement learning (RL) setting and that coexist in environments where social dilemmas can arise. Our simulation results show that groups of agents endowed with world models outperform all the other tested ones when dealing with scenarios where social dilemmas can arise. We exploit the world model architecture to qualitatively assess the learnt dynamics and confirm that each agent's world model is capable to encode information of the behavior of the changing environment and the other agent's actions. This is the first work that shows that wor
    
[^81]: 具有闭式求解器的异质性处理效应估计元学习方法

    Meta-learning for heterogeneous treatment effect estimation with closed-form solvers. (arXiv:2305.11353v1 [stat.ML])

    [http://arxiv.org/abs/2305.11353](http://arxiv.org/abs/2305.11353)

    本文提出了一种元学习方法，用于从少量的观测数据中估计条件平均处理效应（CATE），该方法通过神经网络模型对CATE估计问题进行分解并使用闭式求解器获得参数，最终实现了任务之间的共享和优化CATE估计表现提升。

    

    本文提出了一种元学习方法，用于从少量的观察数据中估计条件平均处理效应（CATE）。所提出的方法学习如何从多个任务中估计CATE，并使用这些知识来进行未见过的任务。在该方法中，基于元学习框架，我们将CATE估计问题分解为子问题。对于每个子问题，我们使用具有任务共享和任务特定参数的神经网络来构建我们的估计模型。通过我们的公式化，我们可以获得可微分的闭式的最优任务特定参数，这些参数能够相对于任务共享参数进行有效的元学习。我们训练任务共享参数，以使少示点设置下的CATE估计表现通过将使用大量数据估计的CATE与仅使用少量数据估计的CATE之间的差异最小化得到改善。我们的实验结果表明，我们的方法在CATE估计方面具有优越性。

    This article proposes a meta-learning method for estimating the conditional average treatment effect (CATE) from a few observational data. The proposed method learns how to estimate CATEs from multiple tasks and uses the knowledge for unseen tasks. In the proposed method, based on the meta-learner framework, we decompose the CATE estimation problem into sub-problems. For each sub-problem, we formulate our estimation models using neural networks with task-shared and task-specific parameters. With our formulation, we can obtain optimal task-specific parameters in a closed form that are differentiable with respect to task-shared parameters, making it possible to perform effective meta-learning. The task-shared parameters are trained such that the expected CATE estimation performance in few-shot settings is improved by minimizing the difference between a CATE estimated with a large amount of data and one estimated with just a few data. Our experimental results demonstrate that our method o
    
[^82]: 有条件生成模型中的数据编辑

    Data Redaction from Conditional Generative Models. (arXiv:2305.11351v1 [cs.LG])

    [http://arxiv.org/abs/2305.11351](http://arxiv.org/abs/2305.11351)

    本文研究如何对已训练好的条件生成模型进行后期编辑，以便编辑掉某些条件分支，这些条件分支很可能会生成不良内容。通过精简模型中的条件网络实现，提出的解决方案有效、高效、具有可控性和普适性，在文本到图像和文本到语音生成模型中取得了良好效果。

    

    深度生成模型因生成不良内容而受到批评。传统的缓解方法包括重新训练、过滤或编辑；然而这些方法要么计算成本高，要么会被第三方回避。本文提出一种不同的方法，研究如何后期编辑已经训练好的条件生成模型，使其编辑掉某些条件分支，这些条件分支很可能会生成不良内容。这是通过精简模型中的条件网络来实现的，提出的解决方案既有效又高效、具有可控性和普适性，能用于一类深度生成模型。我们在文本到图像生成模型和文本到语音生成模型中进行了数据编辑实验，并表明我们的方法计算成本较低，相比基线方法具有更好的编辑质量和鲁棒性，同时仍保持高生成质量。

    Deep generative models are known to produce undesirable samples such as harmful content. Traditional mitigation methods include re-training from scratch, filtering, or editing; however, these are either computationally expensive or can be circumvented by third parties. In this paper, we take a different approach and study how to post-edit an already-trained conditional generative model so that it redacts certain conditionals that will, with high probability, lead to undesirable content. This is done by distilling the conditioning network in the models, giving a solution that is effective, efficient, controllable, and universal for a class of deep generative models. We conduct experiments on redacting prompts in text-to-image models and redacting voices in text-to-speech models. Our method is computationally light, leads to better redaction quality and robustness than baseline methods while still retaining high generation quality.
    
[^83]: 无监督跨领域的多模态弱信号假新闻检测

    Unsupervised Domain-agnostic Fake News Detection using Multi-modal Weak Signals. (arXiv:2305.11349v1 [cs.LG])

    [http://arxiv.org/abs/2305.11349](http://arxiv.org/abs/2305.11349)

    本文提出了一种新的、无需监督、跨领域的虚假新闻检测框架，通过嵌入多模态信息和自监督学习技术实现，同时还提出了一种新的数据集构建技术，有效避免了现有数据集中的潜在偏见。

    

    社交媒体作为人们获取新闻的主要平台之一的出现，促进了虚假新闻的广泛传播。这激发了大量的自动化识别虚假新闻的研究。尽管已经有了一些未经监督的虚假新闻检测尝试，但它们的性能受到未利用与新闻记录相关的各种模态知识和现有新闻数据集中存在的各种潜在偏见的影响。为了解决这些问题，本文提出了一种有效的无监督虚假新闻检测框架，首先将新闻记录中四种模态的知识嵌入，然后提出一种新颖的噪声鲁棒的自监督学习技术，从多模态嵌入中识别新闻记录的真实性。此外，我们提出了一种新颖的构建新闻数据集的技术，最小化现有新闻数据集中的潜在偏见。按照所提出的数据集构建方法，我们制作一个基于语言的新闻数据集，用于评估所提出的框架的有效性。我们的实验表明，所提出的框架显著优于最先进的无监督虚假新闻检测技术。

    The emergence of social media as one of the main platforms for people to access news has enabled the wide dissemination of fake news. This has motivated numerous studies on automating fake news detection. Although there have been limited attempts at unsupervised fake news detection, their performance suffers due to not exploiting the knowledge from various modalities related to news records and due to the presence of various latent biases in the existing news datasets. To address these limitations, this work proposes an effective framework for unsupervised fake news detection, which first embeds the knowledge available in four modalities in news records and then proposes a novel noise-robust self-supervised learning technique to identify the veracity of news records from the multi-modal embeddings. Also, we propose a novel technique to construct news datasets minimizing the latent biases in existing news datasets. Following the proposed approach for dataset construction, we produce a L
    
[^84]: 以公平名义：评估临床记录去识别中的偏见

    In the Name of Fairness: Assessing the Bias in Clinical Record De-identification. (arXiv:2305.11348v1 [cs.LG])

    [http://arxiv.org/abs/2305.11348](http://arxiv.org/abs/2305.11348)

    本文研究了临床记录去识别系统在不同人口群体中的表现差异，揭示了其在名称去识别方面存在显著的偏见。

    

    数据共享对于开放科学和可重复研究至关重要，但合法共享临床数据需要从电子健康记录中删除受保护的健康信息。这个过程，称为去识别，通常通过许多商业和开源系统使用机器学习算法来实现。虽然这些系统在平均水平上已经显示出令人信服的结果，但它们在不同的人口群体中的表现差异还没有得到彻底的检查。在这项工作中，我们通过大规模实证分析，研究了临床笔记中的名称去识别系统的偏见。为了实现这一目的，我们创建了16个名称集，涵盖了四个人口统计学维度：性别、种族、名称流行度和流行的十年。我们将这些名称插入到100个手动筛选的临床模板中，并评估了九种公共和私人去识别方法的性能。我们的发现表明，在临床记录去识别系统的名称方面存在统计显著的偏见。

    Data sharing is crucial for open science and reproducible research, but the legal sharing of clinical data requires the removal of protected health information from electronic health records. This process, known as de-identification, is often achieved through the use of machine learning algorithms by many commercial and open-source systems. While these systems have shown compelling results on average, the variation in their performance across different demographic groups has not been thoroughly examined. In this work, we investigate the bias of de-identification systems on names in clinical notes via a large-scale empirical analysis. To achieve this, we create 16 name sets that vary along four demographic dimensions: gender, race, name popularity, and the decade of popularity. We insert these names into 100 manually curated clinical templates and evaluate the performance of nine public and private de-identification methods. Our findings reveal that there are statistically significant p
    
[^85]: 深度多光谱分割模型对自然扰动和数据污染的鲁棒性量化研究

    Quantifying the robustness of deep multispectral segmentation models against natural perturbations and data poisoning. (arXiv:2305.11347v1 [cs.CV])

    [http://arxiv.org/abs/2305.11347](http://arxiv.org/abs/2305.11347)

    本研究通过对多光谱图像分割模型进行实验，发现多光谱数据不能提高模型对自然扰动的鲁棒性，同时模型对抗攻击的鲁棒性取决于攻击方法和使用的特定光谱波段。

    

    在航空图像分割任务中，除了传统的RGB通道之外，包括更多光谱波段可以提高模型性能。然而，将这些额外数据纳入模型对对抗性攻击和自然扰动的抵抗力如何影响仍不清楚。本文旨在表征多光谱（RGB和近红外）图像分割模型在面对对抗攻击和自然扰动时的性能和鲁棒性。我们的实验表明，虽然多光谱数据能提高模型性能，但并不一定能提高其对自然扰动的鲁棒性。此外，我们发现，模型对抗攻击的鲁棒性很大程度上取决于所使用的攻击方法和使用的特定光谱波段。

    In overhead image segmentation tasks, including additional spectral bands beyond the traditional RGB channels can improve model performance. However, it is still unclear how incorporating this additional data impacts model robustness to adversarial attacks and natural perturbations. For adversarial robustness, the additional information could improve the model's ability to distinguish malicious inputs, or simply provide new attack avenues and vulnerabilities. For natural perturbations, the additional information could better inform model decisions and weaken perturbation effects or have no significant influence at all. In this work, we seek to characterize the performance and robustness of a multispectral (RGB and near infrared) image segmentation model subjected to adversarial attacks and natural perturbations. While existing adversarial and natural robustness research has focused primarily on digital perturbations, we prioritize on creating realistic perturbations designed with physi
    
[^86]: 能量模型下的贝叶斯重参数化奖励条件强化学习

    Bayesian Reparameterization of Reward-Conditioned Reinforcement Learning with Energy-based Models. (arXiv:2305.11340v1 [cs.LG])

    [http://arxiv.org/abs/2305.11340](http://arxiv.org/abs/2305.11340)

    该论文提出了BR-RCRL，它是一种贝叶斯重参数化算法，能够解决奖励条件强化学习中的泛化能力和样本外查询问题。

    

    最近，由于其简单灵活和离线策略特性，奖励条件强化学习（RCRL）变得越来越受欢迎。然而，我们将展示当前的RCRL方法存在根本性局限性，并未解决两个关键的RCRL挑战 - 如何改善高奖励输出的泛化能力以及如何避免测试期间的样本外奖励查询。为了解决这些问题，我们提出了贝叶斯重参数化RCRL（BR-RCRL），这是一种新颖的RCRL归纳偏置设计，灵感来自于贝叶斯定理。

    Recently, reward-conditioned reinforcement learning (RCRL) has gained popularity due to its simplicity, flexibility, and off-policy nature. However, we will show that current RCRL approaches are fundamentally limited and fail to address two critical challenges of RCRL -- improving generalization on high reward-to-go (RTG) inputs, and avoiding out-of-distribution (OOD) RTG queries during testing time. To address these challenges when training vanilla RCRL architectures, we propose Bayesian Reparameterized RCRL (BR-RCRL), a novel set of inductive biases for RCRL inspired by Bayes' theorem. BR-RCRL removes a core obstacle preventing vanilla RCRL from generalizing on high RTG inputs -- a tendency that the model treats different RTG inputs as independent values, which we term ``RTG Independence". BR-RCRL also allows us to design an accompanying adaptive inference method, which maximizes total returns while avoiding OOD queries that yield unpredictable behaviors in vanilla RCRL methods. We s
    
[^87]: 基于口罩增强的局部匹配的食谱图像检索方法

    MALM: Mask Augmentation based Local Matching for Food-Recipe Retrieval. (arXiv:2305.11327v1 [cs.CV])

    [http://arxiv.org/abs/2305.11327](http://arxiv.org/abs/2305.11327)

    提出了一种基于口罩增强的局部匹配网络(MALM)，用于图像到食谱的检索，学习可泛化的跨模态表示。

    

    图像到食谱的检索是一项具有重要实用价值的视觉到语言的挑战性任务。该任务的主要挑战在于食谱长度超高冗余和反映在食品组合和外观上的大变化。为了解决这个问题，我们提出了一种基于口罩增强的局部匹配网络(MALM)，其中图像-文本匹配模块和掩码自蒸馏模块相互受益，以学习可泛化的跨模态表示。

    Image-to-recipe retrieval is a challenging vision-to-language task of significant practical value. The main challenge of the task lies in the ultra-high redundancy in the long recipe and the large variation reflected in both food item combination and food item appearance. A de-facto idea to address this task is to learn a shared feature embedding space in which a food image is aligned better to its paired recipe than other recipes. However, such supervised global matching is prone to supervision collapse, i.e., only partial information that is necessary for distinguishing training pairs can be identified, while other information that is potentially useful in generalization could be lost. To mitigate such a problem, we propose a mask-augmentation-based local matching network (MALM), where an image-text matching module and a masked self-distillation module benefit each other mutually to learn generalizable cross-modality representations. On one hand, we perform local matching between the
    
[^88]: SpikeCP: 通过极限预测实现延迟自适应可靠脉冲神经网络

    SpikeCP: Delay-Adaptive Reliable Spiking Neural Networks via Conformal Prediction. (arXiv:2305.11322v1 [cs.NE])

    [http://arxiv.org/abs/2305.11322](http://arxiv.org/abs/2305.11322)

    这篇论文提出了一种新的脉冲神经网络模型，能够通过极限预测实现自适应的推断延迟，从而节约能源与提高可靠性。

    

    脉冲神经网络（SNN）通过内部事件驱动的神经动态处理时间序列数据，其能量消耗取决于输入演示期间神经元之间交换的脉冲数量。在典型的SNN分类器实现中，决策是在整个输入序列被处理后产生的，导致延迟和能量消耗水平在输入之间是相对均匀的。最近引入的延迟自适应SNN可根据每个示例的难度来定制推断延迟 - 以及随之而来的能耗 - 通过在SNN模型足够“自信”时产生早期决策来实现。

    Spiking neural networks (SNNs) process time-series data via internal event-driven neural dynamics whose energy consumption depends on the number of spikes exchanged between neurons over the course of the input presentation. In typical implementations of an SNN classifier, decisions are produced after the entire input sequence has been processed, resulting in latency and energy consumption levels that are fairly uniform across inputs. Recently introduced delay-adaptive SNNs tailor the inference latency -- and, with it, the energy consumption -- to the difficulty of each example, by producing an early decision when the SNN model is sufficiently ``confident''. In this paper, we start by observing that, as an SNN processes input samples, its classification decisions tend to be first under-confident and then over-confident with respect to the decision's ground-truth, unknown, test accuracy. This makes it difficult to determine a stopping time that ensures a desired level of accuracy. To add
    
[^89]: BELLA: 通过本地线性逼近进行黑盒模型解释

    BELLA: Black box model Explanations by Local Linear Approximations. (arXiv:2305.11311v1 [cs.LG])

    [http://arxiv.org/abs/2305.11311](http://arxiv.org/abs/2305.11311)

    本文提出了一种确定性的、与模型无关的事后方法BELLA，用于解释回归黑盒模型的个别预测。该方法通过特征空间中训练的线性模型提供解释，使得该模型的系数可以直接用于计算特征值的预测值。此外，BELLA最大化了线性模型适用的领域范围。

    

    近年来，理解黑盒模型的决策过程不仅成为法律要求，也成为评估其性能的另一种方式。然而，现有的事后解释方法依赖于合成数据生成，这引入了不确定性并可能损害解释的可靠性，并且它们 tend to produce explanations that apply to only very few data points. This makes the explanations brittle and limited in scope. Finally, they provide scores that have no direct verifiable meaning. In this paper, we present BELLA, a deterministic model-agnostic post-hoc approach for explaining the individual predictions of regression black-box models. BELLA provides explanations in the form of a linear model trained in the feature space. Thus, its coefficients can be used directly to compute the predicted value from the feature values. Furthermore, BELLA maximizes the size of the neighborhood to which the linear model a

    In recent years, understanding the decision-making process of black-box models has become not only a legal requirement but also an additional way to assess their performance. However, the state of the art post-hoc interpretation approaches rely on synthetic data generation. This introduces uncertainty and can hurt the reliability of the interpretations. Furthermore, they tend to produce explanations that apply to only very few data points. This makes the explanations brittle and limited in scope. Finally, they provide scores that have no direct verifiable meaning. In this paper, we present BELLA, a deterministic model-agnostic post-hoc approach for explaining the individual predictions of regression black-box models. BELLA provides explanations in the form of a linear model trained in the feature space. Thus, its coefficients can be used directly to compute the predicted value from the feature values. Furthermore, BELLA maximizes the size of the neighborhood to which the linear model a
    
[^90]: AMII：自适应多模态人际和自我模型用于行为合成

    AMII: Adaptive Multimodal Inter-personal and Intra-personal Model for Adapted Behavior Synthesis. (arXiv:2305.11310v1 [cs.HC])

    [http://arxiv.org/abs/2305.11310](http://arxiv.org/abs/2305.11310)

    AMII是一种面部手势合成方法，通过模态记忆编码模式和注意机制，实现了对自我和人际关系的捕捉，从而适应性地显示行为。

    

    社交交互代理（SIAs）是显示与人类多模态行为类似的实体或虚拟化身的代理。对于建模SIAs的非语言行为（如语音和面部手势），始终是一项具有挑战性的任务，因为SIA可以扮演发言人或听众的角色。SIA在两种角色下都必须发出适当的行为，以适应其自己的语音、其以前的行为（自我）以及用户的行为（人际）。我们提出了AMII，一种新颖的方法，用于在SIAs与用户交互并交替扮演发言人或听众时合成自适应面部手势。AMII的特点是模态记忆编码模式，其中模态对应于语音或面部手势，并使用注意机制捕捉自我和人际关系。我们通过进行客观评估并与最先进的方法进行比较来验证我们的方法。

    Socially Interactive Agents (SIAs) are physical or virtual embodied agents that display similar behavior as human multimodal behavior. Modeling SIAs' non-verbal behavior, such as speech and facial gestures, has always been a challenging task, given that a SIA can take the role of a speaker or a listener. A SIA must emit appropriate behavior adapted to its own speech, its previous behaviors (intra-personal), and the User's behaviors (inter-personal) for both roles. We propose AMII, a novel approach to synthesize adaptive facial gestures for SIAs while interacting with Users and acting interchangeably as a speaker or as a listener. AMII is characterized by modality memory encoding schema - where modality corresponds to either speech or facial gestures - and makes use of attention mechanisms to capture the intra-personal and inter-personal relationships. We validate our approach by conducting objective evaluations and comparing it with the state-of-the-art approaches.
    
[^91]: pTSE:一种用于概率时间序列预测的多模型集成方法

    pTSE: A Multi-model Ensemble Method for Probabilistic Time Series Forecasting. (arXiv:2305.11304v1 [cs.LG])

    [http://arxiv.org/abs/2305.11304](http://arxiv.org/abs/2305.11304)

    提出了pTSE，一种基于隐马尔可夫模型的概率预测的多模型分布集成方法，实现了对时间序列的鲁棒性和准确性的提高。

    

    出现了各种概率时间序列预测模型并表现出了卓越的性能。然而，模型的选择高度依赖于输入时间序列的特征和模型基于的固定分布。由于概率分布不能直接平均不同模型，目前的时间序列模型集成方法不能直接用于提高预测的鲁棒性和准确性。为了解决这个问题，我们提出了pTSE，一种基于隐马尔可夫模型的概率预测的多模型分布集成方法。pTSE只需从成员模型获取现成输出，不需要进一步了解每个模型的信息。此外，我们对pTSE进行了完整的理论分析，证明了时间序列经HMM处理后的经验分布几乎一定收敛于稳态分布。基准实验显示，与单个模型和其他最新集成方法相比，pTSE在准确性和鲁棒性方面具有优越性。

    Various probabilistic time series forecasting models have sprung up and shown remarkably good performance. However, the choice of model highly relies on the characteristics of the input time series and the fixed distribution that the model is based on. Due to the fact that the probability distributions cannot be averaged over different models straightforwardly, the current time series model ensemble methods cannot be directly applied to improve the robustness and accuracy of forecasting. To address this issue, we propose pTSE, a multi-model distribution ensemble method for probabilistic forecasting based on Hidden Markov Model (HMM). pTSE only takes off-the-shelf outputs from member models without requiring further information about each model. Besides, we provide a complete theoretical analysis of pTSE to prove that the empirical distribution of time series subject to an HMM will converge to the stationary distribution almost surely. Experiments on benchmarks show the superiority of p
    
[^92]: 多精度机器学习用于分子激发态能量的预测

    Multi-Fidelity Machine Learning for Excited State Energies of Molecules. (arXiv:2305.11292v1 [physics.chem-ph])

    [http://arxiv.org/abs/2305.11292](http://arxiv.org/abs/2305.11292)

    本文提出了一种多精度机器学习方法来预测分子的激发态能量，该方法将高精度的数据与成本更低、精度更低的数据相结合，从而提高了预测的精度。

    

    分子激发态能量的准确但快速计算仍然是一个非常具有挑战性的课题。为了许多应用，对于较大分子聚合物中的能量漏斗的详细了解是至关重要的，需要高精度的激发态能量。为此，机器学习技术可以是一个极其有用的工具，尽管生成高精度的训练数据的成本仍然是一个严峻的挑战。为了克服这个障碍，本文提出了使用多精度机器学习的方法，其中大量来自高精度的训练数据与成本更低、精度更低的数据相结合，以达到更高的精度。在本研究中，该方法被用来预测三种分子的第一激发态能量，分别为苯、萘和蒽，并针对经典分子动力学模拟和实时密度泛函紧束缚截断中的构象进行了训练和测试。

    The accurate but fast calculation of molecular excited states is still a very challenging topic. For many applications, detailed knowledge of the energy funnel in larger molecular aggregates is of key importance requiring highly accurate excited state energies. To this end, machine learning techniques can be an extremely useful tool though the cost of generating highly accurate training datasets still remains a severe challenge. To overcome this hurdle, this work proposes the use of multi-fidelity machine learning where very little training data from high accuracies is combined with cheaper and less accurate data to achieve the accuracy of the costlier level. In the present study, the approach is employed to predict the first excited state energies for three molecules of increasing size, namely, benzene, naphthalene, and anthracene. The energies are trained and tested for conformations stemming from classical molecular dynamics simulations and from real-time density functional tight-bi
    
[^93]: 谷歌地图中的大规模可扩展逆强化学习

    Massively Scalable Inverse Reinforcement Learning in Google Maps. (arXiv:2305.11290v1 [cs.LG])

    [http://arxiv.org/abs/2305.11290](http://arxiv.org/abs/2305.11290)

    本文提出了一种新的逆强化学习算法（RHIP），通过图压缩、并行化和基于主特征向量的问题初始化解决了全球规模的MDPs、大型数据集和高度参数化的模型的问题，在谷歌地图中实现了16-24%的全球路线质量改进。

    

    优化人类潜在偏好是路线推荐中的巨大挑战，全球可扩展解决方案仍是一个未解决的问题。尽管过去的研究为逆强化学习的应用创建了越来越通用的解决方案，但这些解决方案尚未成功扩展到世界规模的MDP、大型数据集和高度参数化的模型，分别涉及数亿个状态、轨迹和参数。本文通过一系列的改进，聚焦于图压缩、并行化和基于主特征向量的问题初始化，突破以往的限制。我们引入了逆向规划递进地平面(RHIP)，它可以概括现有的工作，并通过其规划水平控制关键性能平衡。我们的策略在全球路线质量方面实现了16-24%的改进，就我们所知，它是迄今为止实现在真实世界环境下的最大的逆强化学习示例。我们的结果显示了更好的导航行为和路径规划。

    Optimizing for humans' latent preferences is a grand challenge in route recommendation, where globally-scalable solutions remain an open problem. Although past work created increasingly general solutions for the application of inverse reinforcement learning (IRL), these have not been successfully scaled to world-sized MDPs, large datasets, and highly parameterized models; respectively hundreds of millions of states, trajectories, and parameters. In this work, we surpass previous limitations through a series of advancements focused on graph compression, parallelization, and problem initialization based on dominant eigenvectors. We introduce Receding Horizon Inverse Planning (RHIP), which generalizes existing work and enables control of key performance trade-offs via its planning horizon. Our policy achieves a 16-24% improvement in global route quality, and, to our knowledge, represents the largest instance of IRL in a real-world setting to date. Our results show critical benefits to mor
    
[^94]: Riemannian多类Logistic回归用于SPD神经网络

    Riemannian Multiclass Logistics Regression for SPD Neural Networks. (arXiv:2305.11288v1 [cs.LG])

    [http://arxiv.org/abs/2305.11288](http://arxiv.org/abs/2305.11288)

    本论文提出了一种新的Riemannian多类Logistic回归（RMLR）分类器用于学习对称正定矩阵的神经网络，通过内在捕捉SPD流形几何的方式，在流行的SPD学习基准测试中证明了其优越性。

    

    在机器学习领域，学习对称正定（SPD）矩阵的深度神经网络越来越受到关注。尽管取得了显着进展，但大多数现有的SPD网络使用传统的欧几里得分类器在近似空间上而不是在准确捕捉SPD流形几何的内在分类器上。受超几何神经网络（HNN）的成功启发，我们提出了Riemannian多类Logistic回归（RMLR）用于SPD网络。我们介绍了一个SPD流形上一族Riemannian度量的通用统一框架，并展示了特定的 $\orth{n}$-不变的Log-Euclidean Metrics适用于SPD网络。此外，我们还将现有SPD网络中最流行的分类器作为我们框架的特殊情况。在流行的SPD学习基准测试上进行的广泛实验表明了我们分类器的优越性。

    Deep neural networks for learning symmetric positive definite (SPD) matrices are gaining increasing attention in machine learning. Despite the significant progress, most existing SPD networks use traditional Euclidean classifiers on approximated spaces rather than intrinsic classifiers that accurately capture the geometry of SPD manifolds. Inspired by the success of hyperbolic neural networks (HNNs), we propose Riemannian multiclass logistics regression (RMLR) for SPD networks. We introduce a general unified framework for a family of Riemannian metrics on SPD manifolds and showcase the specific $\orth{n}$-invariant Log-Euclidean Metrics for SPD networks. Moreover, we encompass the most popular classifier in existing SPD networks as a special case of our framework. Extensive experiments on popular SPD learning benchmarks demonstrate the superiority of our classifiers.
    
[^95]: 基于联邦学习的多语言帕金森病检测模型的安全开发

    Federated learning for secure development of AI models for Parkinson's disease detection using speech from different languages. (arXiv:2305.11284v1 [eess.AS])

    [http://arxiv.org/abs/2305.11284](http://arxiv.org/abs/2305.11284)

    本论文利用联邦学习方法，无需共享患者数据，实现在德语、西班牙语和捷克语三种语言数据集上进行帕金森病检测，取得了优于本地模型的诊断准确性。

    

    帕金森病是一种影响人类说话的神经系统疾病。深度学习模型在自动化帕金森病评估中表现出了出色的性能，但严格的患者数据隐私法规阻碍了机构间共享数据。本文在不共享患者数据的前提下，利用联邦学习在德语、西班牙语和捷克语等三种不同语言的真实数据集上进行了帕金森病检测，并取得了优于本地模型的诊断准确性。

    Parkinson's disease (PD) is a neurological disorder impacting a person's speech. Among automatic PD assessment methods, deep learning models have gained particular interest. Recently, the community has explored cross-pathology and cross-language models which can improve diagnostic accuracy even further. However, strict patient data privacy regulations largely prevent institutions from sharing patient speech data with each other. In this paper, we employ federated learning (FL) for PD detection using speech signals from 3 real-world language corpora of German, Spanish, and Czech, each from a separate institution. Our results indicate that the FL model outperforms all the local models in terms of diagnostic accuracy, while not performing very differently from the model based on centrally combined training sets, with the advantage of not requiring any data sharing among collaborators. This will simplify inter-institutional collaborations, resulting in enhancement of patient outcomes.
    
[^96]: 关于一般函数逼近下的均场强化学习的统计效率

    On the Statistical Efficiency of Mean Field Reinforcement Learning with General Function Approximation. (arXiv:2305.11283v1 [cs.LG])

    [http://arxiv.org/abs/2305.11283](http://arxiv.org/abs/2305.11283)

    本文研究了一般函数逼近下的均场控制(MFC)和均场博弈(MFG)中的强化学习的统计效率，提出了基于乐观最大似然估计的算法，并仅对转移动力学具有Lipschitz连续性的假设，最后建立了一个指数级的下界支持MFC设置。

    

    本文研究了一般函数逼近下的均场控制（MFC）和均场博弈（MFG）中强化学习的统计效率。引入了一种称为Mean-Field Model-Based Eluder Dimension (MBED)的新概念，包含了一系列丰富的均场强化学习问题。此外，我们提出了基于乐观最大似然估计的算法，可以返回一个$\epsilon$优的策略，适用于MFC或$\epsilon$纳什均衡策略适用于MFG，样本复杂度多项式与相关参数无关，与状态、动作和代理数量无关。值得注意的是，我们的结果仅对转移动力学具有Lipschitz连续性的假设，避免了以前的强结构假设。最后，在tabular设置下，假设有一个生成模型，我们建立了一个指数级的下界支持MFC设置，同时提供了一种新颖的样本高效的模型消除算法以逼近最优策略。

    In this paper, we study the statistical efficiency of Reinforcement Learning in Mean-Field Control (MFC) and Mean-Field Game (MFG) with general function approximation. We introduce a new concept called Mean-Field Model-Based Eluder Dimension (MBED), which subsumes a rich family of Mean-Field RL problems. Additionally, we propose algorithms based on Optimistic Maximal Likelihood Estimation, which can return an $\epsilon$-optimal policy for MFC or an $\epsilon$-Nash Equilibrium policy for MFG, with sample complexity polynomial w.r.t. relevant parameters and independent of the number of states, actions and the number of agents. Notably, our results only require a mild assumption of Lipschitz continuity on transition dynamics and avoid strong structural assumptions in previous work. Finally, in the tabular setting, given the access to a generative model, we establish an exponential lower bound for MFC setting, while providing a novel sample-efficient model elimination algorithm to approxim
    
[^97]: SlotDiffusion: 基于Diffusion模型的物体中心生成建模

    SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models. (arXiv:2305.11281v1 [cs.CV])

    [http://arxiv.org/abs/2305.11281](http://arxiv.org/abs/2305.11281)

    本文提出了一种名为SlotDiffusion的对象中心潜在扩散模型，它具有强大的建模能力，能够提高物体中心槽到图像解码的质量，超越了先前的槽模型。

    

    物体中心学习旨在用对象实体（也称为槽）表示视觉数据，提供结构化表示，从而实现系统化泛化。本文提出了一种名为SlotDiffusion的对象中心潜在扩散模型 (LDM)，旨在提高槽到图像解码的质量，是一种既可用于图像数据又可用于视频数据的新型生成模型。由于LDM的强大建模能力，SlotDiffusion在无监督物体分割和视觉生成方面超过了先前的槽模型。

    Object-centric learning aims to represent visual data with a set of object entities (a.k.a. slots), providing structured representations that enable systematic generalization. Leveraging advanced architectures like Transformers, recent approaches have made significant progress in unsupervised object discovery. In addition, slot-based representations hold great potential for generative modeling, such as controllable image generation and object manipulation in image editing. However, current slot-based methods often produce blurry images and distorted objects, exhibiting poor generative modeling capabilities. In this paper, we focus on improving slot-to-image decoding, a crucial aspect for high-quality visual generation. We introduce SlotDiffusion -- an object-centric Latent Diffusion Model (LDM) designed for both image and video data. Thanks to the powerful modeling capacity of LDMs, SlotDiffusion surpasses previous slot models in unsupervised object segmentation and visual generation a
    
[^98]: 实时变分方法学习神经轨迹及其动力学

    Real-Time Variational Method for Learning Neural Trajectory and its Dynamics. (arXiv:2305.11278v1 [stat.ML])

    [http://arxiv.org/abs/2305.11278](http://arxiv.org/abs/2305.11278)

    本论文介绍了一种实时的递归贝叶斯方法用于推断神经轨迹及其动力学，能够广泛适用于任意似然，同时有效跟踪神经元中钙成像数据的动态。

    

    潜变量模型在计算神经科学中已成为推理神经计算的重要工具。这促进了从神经记录中提取潜在神经轨迹的强大离线算法的发展。然而，尽管实时替代方案能够为实验者立即提供反馈并增强实验设计能力，但它们得到的关注要少得多。在本研究中，我们介绍了指数族变分卡尔曼滤波器（eVKF），这是一种在线递归贝叶斯方法，旨在推断潜在轨迹同时学习产生它们的动力系统。eVKF适用于任意似然，并利用常数基本测度指数族来模拟潜在状态的随机性。我们得出了卡尔曼滤波器预测步骤的闭式变分类比，它比另一种在线变分方法产生了可证明更紧的ELBO界限。我们在合成数据上验证了我们的方法，并展示了它在跟踪神经元中钙成像数据的动态方面的有效性。

    Latent variable models have become instrumental in computational neuroscience for reasoning about neural computation. This has fostered the development of powerful offline algorithms for extracting latent neural trajectories from neural recordings. However, despite the potential of real time alternatives to give immediate feedback to experimentalists, and enhance experimental design, they have received markedly less attention. In this work, we introduce the exponential family variational Kalman filter (eVKF), an online recursive Bayesian method aimed at inferring latent trajectories while simultaneously learning the dynamical system generating them. eVKF works for arbitrary likelihoods and utilizes the constant base measure exponential family to model the latent state stochasticity. We derive a closed-form variational analogue to the predict step of the Kalman filter which leads to a provably tighter bound on the ELBO compared to another online variational method. We validate our metho
    
[^99]: 面向情境对话中的心智建模，实现协同计划获取

    Towards Collaborative Plan Acquisition through Theory of Mind Modeling in Situated Dialogue. (arXiv:2305.11271v1 [cs.AI])

    [http://arxiv.org/abs/2305.11271](http://arxiv.org/abs/2305.11271)

    本文提出了一种协作计划获取方法，通过丰富的感知和对话历史，让代理人预测他们自己和合作伙伴缺失的任务知识，实现联合任务的完整计划获取。

    

    协作任务通常始于双方拥有不完全的任务知识和不完整的初始计划。为完成这些任务，代理人需要与合作伙伴进行实地交流，并协调他们的部分计划以实现联合任务目标。虽然这种协作在人与人的团队中似乎轻而易举，但对于人工智能的协作来说却具有很高的挑战性。为解决这个问题，本文提出了一种协作计划获取的方法，其中人类和代理人努力学习并相互交流，以获取联合任务的完整计划。具体地，本文提出了一种新颖的问题，让代理人基于丰富的感知和对话历史，预测他们自己和合作伙伴缺失的任务知识。我们在一个三维方块世界的对称协作任务中扩展了一个情境对话基准，并研究了计划获取的计算策略。我们的实证结果表明，预测任务知识是计划获取过程中的重点。

    Collaborative tasks often begin with partial task knowledge and incomplete initial plans from each partner. To complete these tasks, agents need to engage in situated communication with their partners and coordinate their partial plans towards a complete plan to achieve a joint task goal. While such collaboration seems effortless in a human-human team, it is highly challenging for human-AI collaboration. To address this limitation, this paper takes a step towards collaborative plan acquisition, where humans and agents strive to learn and communicate with each other to acquire a complete plan for joint tasks. Specifically, we formulate a novel problem for agents to predict the missing task knowledge for themselves and for their partners based on rich perceptual and dialogue history. We extend a situated dialogue benchmark for symmetric collaborative tasks in a 3D blocks world and investigate computational strategies for plan acquisition. Our empirical results suggest that predicting the
    
[^100]: 为优先排序的多智能体导航实现约束环境优化

    Constrained Environment Optimization for Prioritized Multi-Agent Navigation. (arXiv:2305.11260v1 [eess.SY])

    [http://arxiv.org/abs/2305.11260](http://arxiv.org/abs/2305.11260)

    本文将环境视为决策变量，提出了优先级环境优化的问题，并分析了智能体优先级在环境优化中的作用。

    

    传统的多智能体导航算法设计方法认为环境是固定的约束条件，但是空间约束对智能体性能有影响。手动设计适合环境的布局效率低下且成本高昂。本文的目标是将环境视为系统级优化问题的决策变量，考虑智能体性能和环境成本。为此，我们提出了无优先级环境优化和优先级环境优化的新问题，其中前者在公正地考虑智能体的情况下，后者考虑智能体的优先级。我们通过正式证明展示了在保证完整性的情况下（即所有智能体都能到达目标），环境可以如何改变，并分析了智能体优先级在环境优化中的作用。我们进一步对环境优化施加真实世界的约束，并将其数学化为带约束的环境优化问题。

    Traditional approaches to the design of multi-agent navigation algorithms consider the environment as a fixed constraint, despite the influence of spatial constraints on agents' performance. Yet hand-designing conducive environment layouts is inefficient and potentially expensive. The goal of this paper is to consider the environment as a decision variable in a system-level optimization problem, where both agent performance and environment cost are incorporated. Towards this end, we propose novel problems of unprioritized and prioritized environment optimization, where the former considers agents unbiasedly and the latter accounts for agent priorities. We show, through formal proofs, under which conditions the environment can change while guaranteeing completeness (i.e., all agents reach goals), and analyze the role of agent priorities in the environment optimization. We proceed to impose real-world constraints on the environment optimization and formulate it mathematically as a constr
    
[^101]: 人工神经网络中的脑启发式学习: 一篇综述

    Brain-inspired learning in artificial neural networks: a review. (arXiv:2305.11252v1 [cs.NE])

    [http://arxiv.org/abs/2305.11252](http://arxiv.org/abs/2305.11252)

    本文综述了当前人工神经网络中的脑启发式学习表示，找出了未来研究的有前途的方向，这可能使我们更加接近理解智能的本质。

    

    人工神经网络已成为机器学习中的必要工具，在图像和语音生成、游戏和机器人等多个领域取得了巨大的成功。然而，人工神经网络的运作机制与生物大脑存在根本差异，尤其是学习过程方面。本文综述了当前人工神经网络中的脑启发式学习表示，并探讨了整合更符合生物学原理的机制（如突触可塑性）以提高这些网络能力的潜在优势和挑战，找出未来研究的有前途的方向，这可能使我们更加接近理解智能的本质。

    Artificial neural networks (ANNs) have emerged as an essential tool in machine learning, achieving remarkable success across diverse domains, including image and speech generation, game playing, and robotics. However, there exist fundamental differences between ANNs' operating mechanisms and those of the biological brain, particularly concerning learning processes. This paper presents a comprehensive review of current brain-inspired learning representations in artificial neural networks. We investigate the integration of more biologically plausible mechanisms, such as synaptic plasticity, to enhance these networks' capabilities. Moreover, we delve into the potential advantages and challenges accompanying this approach. Ultimately, we pinpoint promising avenues for future research in this rapidly advancing field, which could bring us closer to understanding the essence of intelligence.
    
[^102]: 一种参数高效的学习方法，用于带有预训练通用语音模型的阿拉伯方言识别

    A Parameter-Efficient Learning Approach to Arabic Dialect Identification with Pre-Trained General-Purpose Speech Model. (arXiv:2305.11244v1 [cs.CL])

    [http://arxiv.org/abs/2305.11244](http://arxiv.org/abs/2305.11244)

    本文介绍了一种利用预训练通用语音模型进行阿拉伯方言识别的参数高效学习方法，通过残差适配器和模型重编程，设计了一个基于记号的标签映射，并在ADI-17数据集上实现了最高精度，同时使用PEL方法进一步减少了训练成本。

    

    本文研究了参数高效学习（PEL）技术，以重新利用通用语音模型（GSM）进行阿拉伯方言识别（ADI）。我们设计了一个基于记号的标签映射，将GSM适应于阿拉伯方言识别，通过残差适配器和模型重编程来实现。我们通过vanilla fine-tuning在ADI-17数据集上实现了新的最高精度。此外，我们通过PEL方法进一步减少了训练成本，使用额外2.5％的网络可训练参数即可达到fine-tuning精度的1.86％。我们的研究展示了如何使用小型数据集和有限的计算资源来识别阿拉伯方言。

    In this work, we explore Parameter-Efficient-Learning (PEL) techniques to repurpose a General-Purpose-Speech (GSM) model for Arabic dialect identification (ADI). Specifically, we investigate different setups to incorporate trainable features into a multi-layer encoder-decoder GSM formulation under frozen pre-trained settings. Our architecture includes residual adapter and model reprogramming (input-prompting). We design a token-level label mapping to condition the GSM for Arabic Dialect Identification (ADI). This is challenging due to the high variation in vocabulary and pronunciation among the numerous regional dialects. We achieve new state-of-the-art accuracy on the ADI-17 dataset by vanilla fine-tuning. We further reduce the training budgets with the PEL method, which performs within 1.86% accuracy to fine-tuning using only 2.5% of (extra) network trainable parameters. Our study demonstrates how to identify Arabic dialects using a small dataset and limited computation with open sou
    
[^103]: 证据网络：用简单的损失函数快速、分摊式地进行神经贝叶斯模型比较

    Evidence Networks: simple losses for fast, amortized, neural Bayesian model comparison. (arXiv:2305.11241v1 [cs.LG])

    [http://arxiv.org/abs/2305.11241](http://arxiv.org/abs/2305.11241)

    本论文提出了一种名为证据网络的方法，能够在处理似然函数或先验函数与嵌套抽样无法胜任的情况下实现贝叶斯模型比较。与传统方法不同的是，该方法使用了新的损失函数，使得我们能够更快速地、更有效地估算贝叶斯因子。

    

    证据网络可在当现有的方法（如嵌套抽样）失败、似然函数或先验函数难以处理或不知道的情况下实现贝叶斯模型比较。贝叶斯模型比较可看作一个优化问题。虽然用贝叶斯法进行最优分类的解释已经众所周知，但在这里，我们改变了视角，提出了一系列损失函数，以产生快速、分摊式的神经估计器，直接估算方便的贝叶斯因子的函数。这减少了估算单个模型概率时的数字不准确性。我们介绍了渗漏奇 parity-odd power（l-POP）变换，引导了新的“l-Pop-Exponential”的损失函数。我们探讨了在不同模型中对数据概率进行神经密度估计，结果表明这种方法比证据网络的精度和可扩展性都要低。多种实际和人造例子证明了证据网络的优越性。

    Evidence Networks can enable Bayesian model comparison when state-of-the-art methods (e.g. nested sampling) fail and even when likelihoods or priors are intractable or unknown. Bayesian model comparison, i.e. the computation of Bayes factors or evidence ratios, can be cast as an optimization problem. Though the Bayesian interpretation of optimal classification is well-known, here we change perspective and present classes of loss functions that result in fast, amortized neural estimators that directly estimate convenient functions of the Bayes factor. This mitigates numerical inaccuracies associated with estimating individual model probabilities. We introduce the leaky parity-odd power (l-POP) transform, leading to the novel ``l-POP-Exponential'' loss function. We explore neural density estimation for data probability in different models, showing it to be less accurate and scalable than Evidence Networks. Multiple real-world and synthetic examples illustrate that Evidence Networks are e
    
[^104]: 安全聚合的高效竖向联邦学习

    Efficient Vertical Federated Learning with Secure Aggregation. (arXiv:2305.11236v1 [cs.LG])

    [http://arxiv.org/abs/2305.11236](http://arxiv.org/abs/2305.11236)

    本文提出了一种安全有效的竖向联邦学习方法，通过使用安全模块进行聚合，解决了竖直数据集下的隐私泄露问题，并在不降低性能的情况下获得了大量加速。

    

    隐私保护联邦学习（FL）的大部分工作都集中在水平分区数据集上，其中客户共享相同的特征集并可以独立地训练完整的模型。然而，在许多有趣的问题中，例如金融欺诈和疾病检测，个别数据点散落在竖直联邦学习的不同客户/组织中。针对这种FL的解决方案需要参与者之间的梯度交换，很少考虑隐私和安全问题，可能存在隐私泄露的风险。在这项工作中，我们提出了一个新颖的设计，使用最先进的安全模块进行安全聚合，以安全有效地进行竖向FL训练。我们通过实验证明，与同态加密(HE)相比，我们的方法在不影响训练性能的情况下，获得了9.1e2~3.8e4的加速比。

    The majority of work in privacy-preserving federated learning (FL) has been focusing on horizontally partitioned datasets where clients share the same sets of features and can train complete models independently. However, in many interesting problems, such as financial fraud detection and disease detection, individual data points are scattered across different clients/organizations in vertical federated learning. Solutions for this type of FL require the exchange of gradients between participants and rarely consider privacy and security concerns, posing a potential risk of privacy leakage. In this work, we present a novel design for training vertical FL securely and efficiently using state-of-the-art security modules for secure aggregation. We demonstrate empirically that our method does not impact training performance whilst obtaining 9.1e2 ~3.8e4 speedup compared to homomorphic encryption (HE).
    
[^105]: 自适应语义压缩的信息排序瓶颈

    Information-Ordered Bottlenecks for Adaptive Semantic Compression. (arXiv:2305.11213v1 [cs.LG])

    [http://arxiv.org/abs/2305.11213](http://arxiv.org/abs/2305.11213)

    本文提出了信息排序瓶颈（IOB）技术，可以在不重新训练的情况下将数据自适应地压缩为按顺序排列的潜在变量，具有高效压缩图像和文本数据的能力，并可以排序信号并对全局固有维度进行估计。

    

    我们提出了信息排序瓶颈（IOB），这是一个神经层，旨在通过最大化可能性对数据进行自适应压缩，将其压缩成按顺序排列的潜在变量。在不重新训练的情况下，IOB节点可以在任何瓶颈宽度处截断，捕捉前几个潜在变量中最关键的信息。通过统一几种先前的方法，我们发现IOB实现了针对给定编码体系结构的近乎最优压缩，并可按含义有意义的方式对潜在信号进行排序。IOB展示了压缩图像和文本数据嵌入的卓越能力，利用了先进的架构（如CNN、transformer和扩散模型）的性能。此外，我们引入了一种用IOB估计全局固有维度的新理论，并展示了它们恢复复杂合成数据的SOTA维度估计。此外，我们展示了这些模型在异质数据的探索性分析中的实用性。

    We present the information-ordered bottleneck (IOB), a neural layer designed to adaptively compress data into latent variables ordered by likelihood maximization. Without retraining, IOB nodes can be truncated at any bottleneck width, capturing the most crucial information in the first latent variables. Unifying several previous approaches, we show that IOBs achieve near-optimal compression for a given encoding architecture and can assign ordering to latent signals in a manner that is semantically meaningful. IOBs demonstrate a remarkable ability to compress embeddings of image and text data, leveraging the performance of SOTA architectures such as CNNs, transformers, and diffusion models. Moreover, we introduce a novel theory for estimating global intrinsic dimensionality with IOBs and show that they recover SOTA dimensionality estimates for complex synthetic data. Furthermore, we showcase the utility of these models for exploratory analysis through applications on heterogeneous datas
    
[^106]: LIMA: 对齐的更少即为更优（Less Is More for Alignment）

    LIMA: Less Is More for Alignment. (arXiv:2305.11206v1 [cs.CL])

    [http://arxiv.org/abs/2305.11206](http://arxiv.org/abs/2305.11206)

    该论文介绍了一种使用无声调学习预训练语言模型和标准监督损失微调的方法（不使用强化学习或人类模型），并展示了在复杂任务上也有出色的表现。

    

    大型语言模型的训练通常分为两个阶段：(1)无监督的原始文本预训练，以学习通用表示；(2)大规模的指令微调和强化学习，以更好地对齐最终任务和用户偏好。我们通过训练LIMA，一个使用标准监督损失值进行的65B参数LLaMa语言模型，仅使用1000个经过筛选的提示和回复进行微调，而不使用任何强化学习或人类偏好建模，衡量了这两个阶段之间的相对重要性。 LIMA表现出了极强的性能，仅从训练数据中的少量示例中学习到如何遵循特定的响应格式，包括从规划旅行行程到推测替代历史的复杂查询。此外，该模型倾向于良好地推广到未在训练数据中出现的任务中。在一项控制的人类研究中，与GPT-4相比，LIMA的响应在43%的情况下等效或严格优先。

    Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of c
    
[^107]: 用数据驱动的方法评估系外行星的宜居性：一篇综合性文献综述

    Assessing Exoplanet Habitability through Data-driven Approaches: A Comprehensive Literature Review. (arXiv:2305.11204v1 [astro-ph.EP])

    [http://arxiv.org/abs/2305.11204](http://arxiv.org/abs/2305.11204)

    本文献综述旨在阐明新兴趋势和进展，特别关注机器学习和计算模型在系外行星研究中的重要角色，揭示如何应用机器学习技术来预测系外行星的宜居性。

    

    探索和研究系外行星仍然处于天文学研究的前沿，挑战着科学家们不断创新和改进方法来处理这些天体产生的浩瀚而复杂的数据。本文献综述旨在阐明这一领域内新兴趋势和进展，特别关注系外行星检测、分类和可视化之间的相互作用，以及机器学习和计算模型日益重要的角色。我们的探索之旅始于对该领域中15篇精选的重要论文进行全面分析。这些论文各自代表着系外行星研究的不同方面，共同呈现了当前领域的多维视角。它们提供了有价值的洞见，即如何创新地应用机器学习技术来克服分析和解释天文数据所面临的挑战，以及这些技术如何帮助预测系外行星的宜居性。

    The exploration and study of exoplanets remain at the frontier of astronomical research, challenging scientists to continuously innovate and refine methodologies to navigate the vast, complex data these celestial bodies produce. This literature the review aims to illuminate the emerging trends and advancements within this sphere, specifically focusing on the interplay between exoplanet detection, classification, and visualization, and the the increasingly pivotal role of machine learning and computational models. Our journey through this realm of exploration commences with a comprehensive analysis of fifteen meticulously selected, seminal papers in the field. These papers, each representing a distinct facet of exoplanet research, collectively offer a multi-dimensional perspective on the current state of the field. They provide valuable insights into the innovative application of machine learning techniques to overcome the challenges posed by the analysis and interpretation of astronomi
    
[^108]: PDP：无需参数的可微剪枝即可搞定

    PDP: Parameter-free Differentiable Pruning is All You Need. (arXiv:2305.11203v1 [cs.LG])

    [http://arxiv.org/abs/2305.11203](http://arxiv.org/abs/2305.11203)

    PDP提出了一种无需参数的可微剪枝方案，具有最先进的模型大小、准确性和训练成本，适用于各种视觉和自然语言任务。

    

    DNN剪枝是一种常用的方法，可以减少模型的大小，提高推理延迟，并最小化DNN加速器上的功耗。然而，现有的方法可能过于复杂、昂贵或无法适用于各种视觉/语言任务、DNN体系结构并遵守结构化剪枝约束。在本文中，我们提出了一种高效而有效的训练时间剪枝方案——PDP（参数自由可微剪枝），它在模型大小、准确性和训练成本方面具有最先进的性能。PDP在训练过程中使用权重的动态函数，以参数无关的方式为给定的剪枝目标生成软剪枝掩码。虽然是可微的，但是PDP的简单和高效使其足够普遍，以在各种视觉和自然语言任务上提供最先进的随机/结构化/通道剪枝结果。例如，对于MobileNet-v1，PDP可以在86.6%的稀疏度下达到68.2%的ImageNet1k top-1准确率。

    DNN pruning is a popular way to reduce the size of a model, improve the inference latency, and minimize the power consumption on DNN accelerators. However, existing approaches might be too complex, expensive or ineffective to apply to a variety of vision/language tasks, DNN architectures and to honor structured pruning constraints. In this paper, we propose an efficient yet effective train-time pruning scheme, Parameter-free Differentiable Pruning (PDP), which offers state-of-the-art qualities in model size, accuracy, and training cost. PDP uses a dynamic function of weights during training to generate soft pruning masks for the weights in a parameter-free manner for a given pruning target. While differentiable, the simplicity and efficiency of PDP make it universal enough to deliver state-of-the-art random/structured/channel pruning results on various vision and natural language tasks. For example, for MobileNet-v1, PDP can achieve 68.2% top-1 ImageNet1k accuracy at 86.6% sparsity, wh
    
[^109]: 使用统计学和机器学习方法对COVID-19患者的死亡率和肺栓塞进行入院预测：一项国际队列研究

    At-Admission Prediction of Mortality and Pulmonary Embolism in COVID-19 Patients Using Statistical and Machine Learning Methods: An International Cohort Study. (arXiv:2305.11199v1 [q-bio.QM])

    [http://arxiv.org/abs/2305.11199](http://arxiv.org/abs/2305.11199)

    本篇文章提出了一种成本敏感的梯度提升机器学习模型，用于预测COVID-19患者入院时的PE事件和死亡风险。该模型在接受血栓预防治疗的患者子群中表现优于现有的PE风险评分系统，为早期识别和管理高危患者提供了新的准确工具。

    

    截至2022年9月，全球已报告超过6亿多例SARS-CoV-2感染，造成超过650万人死亡。然而，COVID-19死亡风险估计器通常是基于小型不具代表性的样本以及方法学限制而开发的。开发COVID-19患者肺栓塞的预测工具是非常重要的。本研究利用国际队列超过80万COVID-19患者的数据集，提出一种成本敏感的梯度提升机器学习模型，用于预测入院时的PE事件和死亡风险。采用逻辑回归、Cox比例风险模型和Shapley值等方法，识别PE和死亡的关键预测因子。我们的预测模型在高度多样化的测试集上的测试AUROC分别为75.9％和74.2％，并且在PE和所有原因的死亡方面的敏感性分别为67.5％和72.7％。PE预测模型还在接受血栓预防治疗的患者子群中进行了评估，表现优于现有的PE风险评分系统。本研究提供了一种新颖且准确的工具，可用于在入院时预测COVID-19患者的死亡率和PE, 有助于早期识别和管理高危患者。

    By September, 2022, more than 600 million cases of SARS-CoV-2 infection have been reported globally, resulting in over 6.5 million deaths. COVID-19 mortality risk estimators are often, however, developed with small unrepresentative samples and with methodological limitations. It is highly important to develop predictive tools for pulmonary embolism (PE) in COVID-19 patients as one of the most severe preventable complications of COVID-19. Using a dataset of more than 800,000 COVID-19 patients from an international cohort, we propose a cost-sensitive gradient-boosted machine learning model that predicts occurrence of PE and death at admission. Logistic regression, Cox proportional hazards models, and Shapley values were used to identify key predictors for PE and death. Our prediction model had a test AUROC of 75.9% and 74.2%, and sensitivities of 67.5% and 72.7% for PE and all-cause mortality respectively on a highly diverse and held-out test set. The PE prediction model was also evaluat
    
[^110]: 在不知道遮盖分布移位的情况下预测不完整数据

    Prediction with Incomplete Data under Agnostic Mask Distribution Shift. (arXiv:2305.11197v1 [cs.LG])

    [http://arxiv.org/abs/2305.11197](http://arxiv.org/abs/2305.11197)

    本研究考虑预测不完整数据的情况，在缺失模式分布可能发生偏移的情况下，我们利用掩码的不变最优预测器实现泛化，通过双参数化技术联合近似最优预测器避免指数爆炸，同时引入正则化项保证预测器具有鲁棒性。

    

    在许多应用程序中，存在缺失值的数据是普遍存在的。最近几年，人们对仅使用观察到的特征和指示缺失模式的掩码的不完整数据进行预测的关注不断增加。现有方法假设训练和测试分布相同，在实际环境中可能会被违反。本文考虑在存在分布偏移的情况下预测不完整数据。我们专注于基础完整特征和标签的联合分布不变，但缺失模式即掩码分布可能会在训练和测试之间不知道地发生变化。为了实现泛化，我们利用这个观察结果，即对于每个掩码，都有一个不变的最优预测器。为了避免在单独学习它们时出现指数爆炸，在使用双参数化技术联合近似最优预测器。这具有不良的副作用，即允许学习到的预测器对参数化的选择敏感。为了解决这个问题，我们引入了一个正则化项，促进对参数化的选择具有鲁棒性的预测器。对合成和真实数据集的实验表明了我们提出的方法的有效性。

    Data with missing values is ubiquitous in many applications. Recent years have witnessed increasing attention on prediction with only incomplete data consisting of observed features and a mask that indicates the missing pattern. Existing methods assume that the training and testing distributions are the same, which may be violated in real-world scenarios. In this paper, we consider prediction with incomplete data in the presence of distribution shift. We focus on the case where the underlying joint distribution of complete features and label is invariant, but the missing pattern, i.e., mask distribution may shift agnostically between training and testing. To achieve generalization, we leverage the observation that for each mask, there is an invariant optimal predictor. To avoid the exponential explosion when learning them separately, we approximate the optimal predictors jointly using a double parameterization technique. This has the undesirable side effect of allowing the learned pred
    
[^111]: DClEVerNet: 深度组合学习优化大规模网络化充电设施的高效电动汽车充电调度

    DClEVerNet: Deep Combinatorial Learning for Efficient EV Charging Scheduling in Large-scale Networked Facilities. (arXiv:2305.11195v1 [cs.LG])

    [http://arxiv.org/abs/2305.11195](http://arxiv.org/abs/2305.11195)

    本文提出了一种基于深度学习和近似算法技术的数据驱动优化框架DClEVerNet，可以优化大规模网络化的EV充电站的预约管理程序，最大化EV用户的总福利收益，同时考虑到网络的可用功率容量和站点的入住限制。

    

    随着交通电气化，电动汽车（EV）的普及可能会显着增加配电网络的压力，导致其性能下降和稳定性受到威胁。为了以经济有效的方式容纳这些新负载，现代电力网络需要协调或“智能”充电策略，能够在一个可伸缩和高效的方式下优化EV充电调度。为此，本文重点研究大规模、网络化EV充电站的预约管理程序。我们制定了一个时耦合的二进制优化问题，最大化EV用户的总福利收益，同时考虑到网络的可用功率容量和站点的入住限制。为了在保持高解决质量的同时大规模解决问题，引入了一个基于深度学习和近似算法技术的数据驱动优化框架。该框架的关键因素是一种新颖的输入输出处理方案。

    With the electrification of transportation, the rising uptake of electric vehicles (EVs) might stress distribution networks significantly, leaving their performance degraded and stability jeopardized. To accommodate these new loads cost-effectively, modern power grids require coordinated or ``smart'' charging strategies capable of optimizing EV charging scheduling in a scalable and efficient fashion. With this in view, the present work focuses on reservation management programs for large-scale, networked EV charging stations. We formulate a time-coupled binary optimization problem that maximizes EV users' total welfare gain while accounting for the network's available power capacity and stations' occupancy limits. To tackle the problem at scale while retaining high solution quality, a data-driven optimization framework combining techniques from the fields of Deep Learning and Approximation Algorithms is introduced. The framework's key ingredient is a novel input-output processing schem
    
[^112]: Vaxformer：针对SARS-CoV-2疫苗设计的抗原性控制Transformer

    Vaxformer: Antigenicity-controlled Transformer for Vaccine Design Against SARS-CoV-2. (arXiv:2305.11194v1 [q-bio.BM])

    [http://arxiv.org/abs/2305.11194](http://arxiv.org/abs/2305.11194)

    Vaxformer是一种新型的条件蛋白质语言模型结构，超过了现有的条件变分自编码器模型，在生成抗原性受控的SARS-CoV-2刺突蛋白方面表现更好，为疫苗设计提供了有前途的机会。

    

    SARS-CoV-2大流行强调了开发一种通用疫苗以保护免受当前和未来变种病毒的重要性。本研究提出了一种新型的条件蛋白质语言模型结构，称为Vaxformer，它旨在产生外观自然的抗原性控制的SARS-CoV-2刺突蛋白。我们使用DDGun蛋白质稳定性测量、netMHCpan抗原性评分和带有AlphaFold的结构保真度评分评估了Vaxformer模型的生成蛋白质序列，以衡量其用于疫苗开发的可能性。结果表明，Vaxformer比现有的最先进的条件变分自编码器模型表现更好，可以生成抗原性受控的SARS-CoV-2刺突蛋白。这些发现表明，条件Transformer模型为扩展我们对疫苗设计的理解以及它们在缓解全球健康挑战中的作用提供了有前途的机会。此研究使用的代码可在https://git上获取。

    The SARS-CoV-2 pandemic has emphasised the importance of developing a universal vaccine that can protect against current and future variants of the virus. The present study proposes a novel conditional protein Language Model architecture, called Vaxformer, which is designed to produce natural-looking antigenicity-controlled SARS-CoV-2 spike proteins. We evaluate the generated protein sequences of the Vaxformer model using DDGun protein stability measure, netMHCpan antigenicity score, and a structure fidelity score with AlphaFold to gauge its viability for vaccine development. Our results show that Vaxformer outperforms the existing state-of-the-art Conditional Variational Autoencoder model to generate antigenicity-controlled SARS-CoV-2 spike proteins. These findings suggest promising opportunities for conditional Transformer models to expand our understanding of vaccine design and their role in mitigating global health challenges. The code used in this study is available at https://git
    
[^113]: 云医疗聊天机器人的 AISecOps 威胁建模分类

    Taxonomy of AISecOps Threat Modeling for Cloud Based Medical Chatbots. (arXiv:2305.11189v1 [cs.DC])

    [http://arxiv.org/abs/2305.11189](http://arxiv.org/abs/2305.11189)

    这篇论文介绍了使用 AISecOps 对云医疗聊天机器人进行监控的方法，解释了 AISecOps 是如何将 IT 运营、人工智能和安全三个领域整合起来协同运作以确保聊天机器人的机密性、完整性和可用性的。

    

    人工智能在技术的各个方面都扮演着至关重要的角色，包括网络安全。医疗领域中越来越流行的聊天机器人等交互式人工智能应用程序，为需要及时获得医疗援助的患者提供方便。由于医疗聊天机器人涉及大量的敏感信息，因此这些机器人的安全性至关重要。为了确保云主机资产的机密性，完整性和可用性，可以使用 AISecOps（为安全 IT 运营而设计的人工智能）对医疗聊天机器人进行监控。 AISecOps 是一个新兴的领域，将 IT 运营，人工智能和安全三个不同但密切相关的领域整合为一个域，在此域中，来自这三个领域的专业知识被协同使用以保护网络安全资产。它考虑到云操作和安全性因素，采用综合框架收集评估安全威胁所需的度量标准，并训练 AI 模型以立即采取行动。

    Artificial Intelligence (AI) is playing a vital role in all aspects of technology including cyber security. Application of Conversational AI like the chatbots are also becoming very popular in the medical field to provide timely and immediate medical assistance to patients in need. As medical chatbots deal with a lot of sensitive information, the security of these chatbots is crucial. To secure the confidentiality, integrity, and availability of cloud-hosted assets like these, medical chatbots can be monitored using AISecOps (Artificial Intelligence for Secure IT Operations). AISecOPs is an emerging field that integrates three different but interrelated domains like the IT operation, AI, and security as one domain, where the expertise from all these three domains are used cohesively to secure the cyber assets. It considers cloud operations and security in a holistic framework to collect the metrics required to assess the security threats and train the AI models to take immediate action
    
[^114]: 压缩，然后提示：使用可转移提示来改善LLM推理的准确性和效率平衡

    Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt. (arXiv:2305.11186v1 [cs.CL])

    [http://arxiv.org/abs/2305.11186](http://arxiv.org/abs/2305.11186)

    本文提出了使用可转移提示来优化压缩的LLMs的准确性和效率的平衡问题。该方法通过选择精度更高的提示显著提高了压缩的LLM在特定查询方面的生成质量，并实现了4倍推理时间加速。

    

    大型语言模型（LLMs）具有数十亿的参数，表现出在各种自然语言处理（NLP）任务中的卓越性能。然而，它们在推理过程中会带来显着的计算挑战，尤其是在常见的硬件上部署（例如单个GPU）时。因此，通过压缩来最小化LLM推理的延迟，即减少计算和内存需求，变得至关重要。但是，此过程必然引发效率和精度之间的平衡，因为压缩的LLMs通常会经历预测精度下降。在这项研究中，我们提出了一个创新的视角：为了优化这种平衡，压缩的LLMs需要一种不同于原始模型的独特输入格式。我们的研究结果表明，通过选择具有精度的提示，可以显著改善压缩的LLM在特定查询方面的生成质量。基于这一发现，我们提出了一个可转移提示的方法，该方法训练模型预测有效提示。实证上，我们的方法可以在各种语言任务上生成高质量的输出，并实现了4倍速度的推理时间加速，同时保持竞争性的准确性。

    Large Language Models (LLMs), armed with billions of parameters, exhibit exceptional performance across a wide range of Natural Language Processing (NLP) tasks. However, they present a significant computational challenge during inference, especially when deploying on common hardware such as single GPUs. As such, minimizing the latency of LLM inference by curtailing computational and memory requirements, though achieved through compression, becomes critically important. However, this process inevitably instigates a trade-off between efficiency and accuracy, as compressed LLMs typically experience a reduction in predictive precision. In this research, we introduce an innovative perspective: to optimize this trade-off, compressed LLMs require a unique input format that varies from that of the original models. Our findings indicate that the generation quality in a compressed LLM can be markedly improved for specific queries by selecting prompts with precision. Capitalizing on this insight,
    
[^115]: 利用GPS数据评估地震余震预测的准确性

    Assessing the predicting power of GPS data for aftershocks forecasting. (arXiv:2305.11183v1 [physics.geo-ph])

    [http://arxiv.org/abs/2305.11183](http://arxiv.org/abs/2305.11183)

    这篇论文提出了一种基于GPS数据和机器学习的方法，可以对地震余震进行准确预测，但其预测能力依赖于GPS站的密度。

    

    我们提出了一种基于机器学习的方法，利用2015年至2019年日本地震目录中全球定位系统（GPS）站在主震发生当天测量到的地面形变作为唯一输入，并使用卷积神经网络（CNN）对其进行处理，从而捕捉输入的空间相关性，对地震余震进行预测。虽然数据量适中，但这种新方法的性能非常有前途。预测的准确性严重依赖于GPS站的密度：当主震发生在离测量站远的离岸地区时，预测能力就会丧失。

    We present a machine learning approach for the aftershock forecasting of Japanese earthquake catalogue from 2015 to 2019. Our method takes as sole input the ground surface deformation as measured by Global Positioning System (GPS) stations at the day of the mainshock, and processes it with a Convolutional Neural Network (CNN), thus capturing the input's spatial correlations. Despite the moderate amount of data the performance of this new approach is very promising. The accuracy of the prediction heavily relies on the density of GPS stations: the predictive power is lost when the mainshocks occur far from measurement stations, as in offshore regions.
    
[^116]: 基于迁移学习的增材制造模型比较：以案例研究为例

    Comparison of Transfer Learning based Additive Manufacturing Models via A Case Study. (arXiv:2305.11181v1 [cs.LG])

    [http://arxiv.org/abs/2305.11181](http://arxiv.org/abs/2305.11181)

    本文制定了一个基于开源数据集的案例研究，通过比较不同的迁移学习方法，回答了应用迁移学习在增材制造建模中的固有挑战问题，并构建了基于迁移学习的模型，用于提升建模性能。

    

    基于迁移学习的增材制造建模是一种新兴的领域，其可以重复使用历史产品的数据，并减少建模新产品时数据量不足的问题。虽然最近进行了一些尝试，但是应用迁移学习在增材制造建模方面的固有挑战却很少被讨论，例如使用哪个源域，需要多少目标数据，以及是否应用数据预处理技术等。本文旨在通过基于开源数据集制定的案例研究来回答这些问题。在该案例研究中，将五种迁移学习方法与决策树回归和人工神经网络相结合，构建了六个基于迁移学习的模型，并将其性能与基准决策树回归和人工神经网络在一个提出的验证框架中进行比较。比较结果用于量化所应用的迁移学习方法的性能，并从相似性、训练数据大小和数据预处理的角度进行讨论。

    Transfer learning (TL) based additive manufacturing (AM) modeling is an emerging field to reuse the data from historical products and mitigate the data insufficiency in modeling new products. Although some trials have been conducted recently, the inherent challenges of applying TL in AM modeling are seldom discussed, e.g., which source domain to use, how much target data is needed, and whether to apply data preprocessing techniques. This paper aims to answer those questions through a case study defined based on an open-source dataset about metal AM products. In the case study, five TL methods are integrated with decision tree regression (DTR) and artificial neural network (ANN) to construct six TL-based models, whose performances are then compared with the baseline DTR and ANN in a proposed validation framework. The comparisons are used to quantify the performance of applied TL methods and are discussed from the perspective of similarity, training data size, and data preprocessing. Fin
    
[^117]: 消失的激活：深度胶囊网络的症状。

    Vanishing Activations: A Symptom of Deep Capsule Networks. (arXiv:2305.11178v1 [cs.CV])

    [http://arxiv.org/abs/2305.11178](http://arxiv.org/abs/2305.11178)

    本文探讨了胶囊网络结构的缺陷，证明这些问题不仅限于原始设计，而是存在于许多领先的胶囊网络架构中。这种内在的设计相似性可能会限制其可扩展性。

    

    胶囊网络是一种使用向量或矩阵表示而不是标量的神经网络扩展。最初开发胶囊网络的目的是创建一个动态解析树，其中视觉概念从部分逐渐发展成为完整的对象。早期的胶囊网络实现在各种数据集上取得和保持最先进的结果。然而，最近的研究揭示了原始胶囊网络结构的缺陷，特别是在构建解析树和在深度网络中易受消失梯度的方面存在问题。本文将调查一系列领先的胶囊网络架构，证明这些问题不仅限于原始设计。我们认为，大部分的胶囊网络研究已经产生了架构，虽然在某种程度上有所不同，但仍保留了基本相似的结构。我们提出，这种内在的设计相似性可能会限制可扩展性。

    Capsule Networks, an extension to Neural Networks utilizing vector or matrix representations instead of scalars, were initially developed to create a dynamic parse tree where visual concepts evolve from parts to complete objects. Early implementations of Capsule Networks achieved and maintain state-of-the-art results on various datasets. However, recent studies have revealed shortcomings in the original Capsule Network architecture, notably its failure to construct a parse tree and its susceptibility to vanishing gradients when deployed in deeper networks. This paper extends the investigation to a range of leading Capsule Network architectures, demonstrating that these issues are not confined to the original design. We argue that the majority of Capsule Network research has produced architectures that, while modestly divergent from the original Capsule Network, still retain a fundamentally similar structure. We posit that this inherent design similarity might be impeding the scalabilit
    
[^118]: 利用ChatGPT和Stable Diffusion生成内容丰富、故事连贯的漫画

    Generating coherent comic with rich story using ChatGPT and Stable Diffusion. (arXiv:2305.11067v1 [cs.CV])

    [http://arxiv.org/abs/2305.11067](http://arxiv.org/abs/2305.11067)

    本文介绍了一种利用ChatGPT和Stable Diffusion生成连贯漫画故事的方法，通过引入新的评估AI故事的方式，并使用LoRA、ControlNet等方法进行fine-tuning，取得了在角色忠实度和艺术风格上的最先进表现。

    

    过去的研究表明，使用神经网络可以在保持音乐家音乐风格的基础上，扩展未完成的音乐作品。最近大型语言模型和扩散模型的进展使得我们能够生成有趣的漫画故事，并保持艺术家的艺术风格。在本文中，我们使用ChatGPT生成情节和对话，然后使用stable diffusion生成漫画。我们介绍了一种评估AI生成故事的新方法，并通过使用LoRA、ControlNet等方法对stable diffusion进行fine-tuning，达到了在角色忠实度和艺术风格上的SOTA表现。

    Past work demonstrated that using neural networks, we can extend unfinished music pieces while maintaining the music style of the musician. With recent advancements in large language models and diffusion models, we are now capable of generating comics with an interesting storyline while maintaining the art style of the artist. In this paper, we used ChatGPT to generate storylines and dialogue and then generated the comic using stable diffusion. We introduced a novel way to evaluate AI-generated stories, and we achieved SOTA performance on character fidelity and art style by fine-tuning stable diffusion using LoRA, ControlNet, etc.
    
[^119]: 面向隐私保护的分布式图学习免费午餐

    Free Lunch for Privacy Preserving Distributed Graph Learning. (arXiv:2305.10869v1 [cs.LG])

    [http://arxiv.org/abs/2305.10869](http://arxiv.org/abs/2305.10869)

    该论文提出了一种能够保护隐私的分布式图学习框架，通过学习特征和距离，而不需要实际的特征，来执行图形学习和其他下游任务。这是一种通用的框架。

    

    在社交网络、机器人、通信、医学等各种应用中，图形学习正在变得越来越普遍。这些属于不同实体的数据集通常包含关键的私人信息。然而，数据共享的隐私问题使得应用图形学习变得困难。现有的隐私保护方法通过提取用户侧的特征来预处理数据，并仅使用这些特征进行下一步的学习。然而，这些方法容易受到对私人属性进行推断的攻击。我们提出了一个新颖的隐私保护框架，用于分布式图形学习和基于图形的机器学习。为了在服务器端执行图形学习和其他下游任务，该框架旨在学习特征和距离，而不需要实际的特征，同时保留原始数据的结构特性。所提出的框架非常通用且高度适用。

    Learning on graphs is becoming prevalent in a wide range of applications including social networks, robotics, communication, medicine, etc. These datasets belonging to entities often contain critical private information. The utilization of data for graph learning applications is hampered by the growing privacy concerns from users on data sharing. Existing privacy-preserving methods pre-process the data to extract user-side features, and only these features are used for subsequent learning. Unfortunately, these methods are vulnerable to adversarial attacks to infer private attributes. We present a novel privacy-respecting framework for distributed graph learning and graph-based machine learning. In order to perform graph learning and other downstream tasks on the server side, this framework aims to learn features as well as distances without requiring actual features while preserving the original structural properties of the raw data. The proposed framework is quite generic and highly a
    
[^120]: 基于多尺度特征金字塔网络和双重注意力机制的腹部MRI图像分割算法

    A Subabdominal MRI Image Segmentation Algorithm Based on Multi-Scale Feature Pyramid Network and Dual Attention Mechanism. (arXiv:2305.10631v1 [eess.IV])

    [http://arxiv.org/abs/2305.10631](http://arxiv.org/abs/2305.10631)

    提出了一种基于多尺度特征金字塔网络和双重注意力机制的子腹部MRI图像分割算法，使用空洞卷积和多尺度特征金字塔编码以避免语义差距，设计双重注意力机制以保持空间信息并减少错位。

    

    本研究旨在解决U-Net在分割直肠癌治疗期间的子腹部MRI图像时，由于多次卷积和池化操作导致编码和解码之间存在语义差距和错位问题。提出了一种基于多尺度特征金字塔网络和双重注意力机制的MRI图像分割方法。我们的创新在于设计了两个模块：1）在编码中使用了空洞卷积和多尺度特征金字塔网络以避免语义差距。2）设计了双重注意力机制，以保持U-Net的空间信息并减少错位。对子腹部MRI图像数据集的实验表明，该方法比其他方法表现更好。总之，多尺度特征金字塔网络可以减少语义差距，双重注意力机制可以使编码和解码之间的特征对齐。

    This study aimed to solve the semantic gap and misalignment issue between encoding and decoding because of multiple convolutional and pooling operations in U-Net when segmenting subabdominal MRI images during rectal cancer treatment. A MRI Image Segmentation is proposed based on a multi-scale feature pyramid network and dual attention mechanism. Our innovation is the design of two modules: 1) a dilated convolution and multi-scale feature pyramid network are used in the encoding to avoid the semantic gap. 2) a dual attention mechanism is designed to maintain spatial information of U-Net and reduce misalignment. Experiments on a subabdominal MRI image dataset show the proposed method achieves better performance than others methods. In conclusion, a multi-scale feature pyramid network can reduce the semantic gap, and the dual attention mechanism can make an alignment of features between encoding and decoding.
    
[^121]: 深度神经网络中的局部不稳定性测量和减少方法

    Measuring and Mitigating Local Instability in Deep Neural Networks. (arXiv:2305.10625v1 [cs.LG])

    [http://arxiv.org/abs/2305.10625](http://arxiv.org/abs/2305.10625)

    深度神经网络中，训练过程中的随机性可能导致模型的输出不稳定，作者提出了基于原则的指标来量化不稳定性并发现不稳定的预测并不是随机出现的，而是以数据相关的方式聚集在一起。作者研究了数据无关正则化方法来减轻这种不稳定性，并表明一些方法可以显着提高不稳定性，甚至在某些情况下优于更广泛使用的正则化方法。

    

    深度神经网络已经成为数百万用户依赖的实际场景应用的重要组成部分。然而，这些系统的构建者往往很难确保可靠的性能，因为像随机初始化这样的无关细节可能意外地改变训练系统的输出，可能带来灾难性的后果。我们通过研究模型稳定性问题，研究模型在训练过程中的随机性对模型预测结果的影响，即使在同一数据上重新训练，预测仍然会发生变化。对于自然语言理解（NLU）任务，我们发现其中相当一部分查询的预测存在不稳定性。我们提出了一些基于原则的指标，如跨训练运行或单次训练内的每个样本的“标签熵”，来量化这种现象。有趣的是，我们发现不稳定的预测并不是随机出现的，而是以数据相关的方式聚集在一起。我们研究了数据无关正则化方法来减轻这种不稳定性，并表明一些方法可以显着提高不稳定性，甚至在某些情况下优于更广泛使用的正则化方法。

    Deep Neural Networks (DNNs) are becoming integral components of real world services relied upon by millions of users. Unfortunately, architects of these systems can find it difficult to ensure reliable performance as irrelevant details like random initialization can unexpectedly change the outputs of a trained system with potentially disastrous consequences. We formulate the model stability problem by studying how the predictions of a model change, even when it is retrained on the same data, as a consequence of stochasticity in the training process. For Natural Language Understanding (NLU) tasks, we find instability in predictions for a significant fraction of queries. We formulate principled metrics, like per-sample ``label entropy'' across training runs or within a single training run, to quantify this phenomenon. Intriguingly, we find that unstable predictions do not appear at random, but rather appear to be clustered in data-specific ways. We study data-agnostic regularization meth
    
[^122]: 基于物理约束的符号回归中主动学习的表现

    Active Learning in Symbolic Regression Performance with Physical Constraints. (arXiv:2305.10379v1 [cs.LG])

    [http://arxiv.org/abs/2305.10379](http://arxiv.org/abs/2305.10379)

    本文探讨了利用进化符号回归作为主动学习中的方法来提出哪些数据应该被采集，通过“委员会查询”来减少所需数据，并在重新发现已知方程所需的数据方面实现最新的结果。

    

    进化符号回归（SR）是一种将符号方程拟合到数据中的方法，可以得到简洁易懂的模型。本文探讨使用SR作为主动学习中的方法来提出哪些数据应该被采集，在此过程中考虑物理约束。基于主动学习的SR通过“委员会查询”来提出下一步实验。物理约束可以在非常低的数据情况下改善所建议的方程。这些方法可以减少SR所需的数据，并在重新发现已知方程所需的数据方面实现最新的结果。

    Evolutionary symbolic regression (SR) fits a symbolic equation to data, which gives a concise interpretable model. We explore using SR as a method to propose which data to gather in an active learning setting with physical constraints. SR with active learning proposes which experiments to do next. Active learning is done with query by committee, where the Pareto frontier of equations is the committee. The physical constraints improve proposed equations in very low data settings. These approaches reduce the data required for SR and achieves state of the art results in data required to rediscover known equations.
    
[^123]: 联邦学习中的联邦评估综述

    A Survey of Federated Evaluation in Federated Learning. (arXiv:2305.08070v1 [cs.LG])

    [http://arxiv.org/abs/2305.08070](http://arxiv.org/abs/2305.08070)

    本篇论文对现有联邦评估方法进行全面综述，阐述了联邦评估在客户端选择、激励机制设计、恶意攻击检测等方面的重要作用，探讨了联邦评估在增强FL性能方面的各种应用，并提出了未来的研究方向。

    

    在传统机器学习中，由于所有数据样本都由服务器进行集中管理，因此进行模型评估非常简单。然而，在联邦学习（FL）中，模型评估变得更加具有挑战性，这被称为本文中的联邦评估。这是因为客户端不会公开其原始数据以保护数据隐私。联邦评估在客户端选择、激励机制设计、恶意攻击检测等方面发挥着重要作用。在本文中，我们首次全面调查了现有的联邦评估方法。此外，我们探讨了联邦评估在增强FL性能方面的各种应用，并最终通过展望一些挑战提出了未来的研究方向。

    In traditional machine learning, it is trivial to conduct model evaluation since all data samples are managed centrally by a server. However, model evaluation becomes a challenging problem in federated learning (FL), which is called federated evaluation in this work. This is because clients do not expose their original data to preserve data privacy. Federated evaluation plays a vital role in client selection, incentive mechanism design, malicious attack detection, etc. In this paper, we provide the first comprehensive survey of existing federated evaluation methods. Moreover, we explore various applications of federated evaluation for enhancing FL performance and finally present future research directions by envisioning some challenges.
    
[^124]: 基于随机池化的可证明多实例深度AUC最大化方法

    Provable Multi-instance Deep AUC Maximization with Stochastic Pooling. (arXiv:2305.08040v1 [cs.LG])

    [http://arxiv.org/abs/2305.08040](http://arxiv.org/abs/2305.08040)

    本文提出了在多实例学习中使用深度AUC最大化（DAM）的方法，并根据包含大量实例的情况下训练的计算挑战，提出了一种基于方差减少的随机池化方法，使得只需对每个包进行少量采样即可计算MIDAM模型，提高了效率和准确性。

    

    本文提出了一种深度AUC最大化（DAM）的新型应用，用于多实例学习（MIL），其中将单个类标签分配给一组实例（例如，患者的多个CT扫描的多个2D切片）。我们在DAM的背景下解决了MIL中被忽略但非常重要的计算挑战，即包大小过大，无法在反向传播时加载到GPU内存中，这是MIL标准池化方法所必需的。为了解决这个问题，我们提出了一种基于方差减少的随机池化方法，这种方法可以将关于汇聚预测的损失函数构造为多级组合函数。通过综合随机组合优化和非凸极小最大优化技术，我们提出了一种统一且可证明的多实例DAM（MIDAM）算法，其使用随机平滑最大池化或随机注意力池化，仅对每个包对应的实例进行少量采样来计算 sto。

    This paper considers a novel application of deep AUC maximization (DAM) for multi-instance learning (MIL), in which a single class label is assigned to a bag of instances (e.g., multiple 2D slices of a CT scan for a patient). We address a neglected yet non-negligible computational challenge of MIL in the context of DAM, i.e., bag size is too large to be loaded into {GPU} memory for backpropagation, which is required by the standard pooling methods of MIL. To tackle this challenge, we propose variance-reduced stochastic pooling methods in the spirit of stochastic optimization by formulating the loss function over the pooled prediction as a multi-level compositional function. By synthesizing techniques from stochastic compositional optimization and non-convex min-max optimization, we propose a unified and provable muli-instance DAM (MIDAM) algorithm with stochastic smoothed-max pooling or stochastic attention-based pooling, which only samples a few instances for each bag to compute a sto
    
[^125]: 结构模拟和桥梁健康监测的神经运算器

    Neural operator for structural simulation and bridge health monitoring. (arXiv:2305.07889v1 [cs.LG])

    [http://arxiv.org/abs/2305.07889](http://arxiv.org/abs/2305.07889)

    本论文提出了结构模拟和桥梁健康监测的神经运算器VINO，通过学习结构响应场和损伤场之间的映射，在前向预测和反向确定损伤区域和程度方面可以比传统有限元模型更准确地预测和判断。

    

    将深度学习与结构工程相结合已经受到了广泛关注，用于前向问题（结构模拟）和反向问题（结构健康监测）。本研究基于傅里叶神经运算器，提出了VINO（车辆-桥梁相互作用神经运算器），作为桥梁结构的数字孪生。VINO学习结构响应场和损伤场之间的映射。本研究通过运行参数有限元（FE）模拟，考虑结构初始损伤场的随机分布，建立了VBI-FE数据集。随后，在四种损伤情况下进行了实验研究，产生了VBI-EXP数据集。在VINO通过VBI-FE预训练并在健康状态下通过VBI-EXP微调后，模型实现了以下两个改进。首先，前向的VINO比FE模型更准确地从损伤场输入预测结构响应。其次，反向的VINO可以确定损伤区域和程度。

    Infusing deep learning with structural engineering has received widespread attention for both forward problems (structural simulation) and inverse problems (structural health monitoring). Based on Fourier Neural Operator, this study proposes VINO (Vehicle-bridge Interaction Neural Operator) to serve as the digital twin of bridge structures. VINO learns mappings between structural response fields and damage fields. In this study, VBI-FE dataset was established by running parametric finite element (FE) simulations considering a random distribution of structural initial damage field. Subsequently, VBI-EXP dataset was produced by conducting an experimental study under four damage scenarios. After VINO was pre-trained by VBI-FE and fine-tuned by VBI-EXP from the bridge at the healthy state, the model achieved the following two improvements. First, forward VINO can predict structural responses from damage field inputs more accurately than the FE model. Second, inverse VINO can determine, loc
    
[^126]: 针对线性和非线性重尾多臂老虎机的隐式范数预测器的修剪

    Implicitly normalized forecaster with clipping for linear and non-linear heavy-tailed multi-armed bandits. (arXiv:2305.06743v1 [cs.LG])

    [http://arxiv.org/abs/2305.06743](http://arxiv.org/abs/2305.06743)

    本文提出了一种针对奖励分布重尾的MAB问题的隐式规范化预测器，证明该方法在线性和非线性重尾随机MAB问题上是最优的。

    

    已知隐式范数预测器（在线镜像下降，以Tsallis熵作为prox函数）是对抗性多臂老虎机问题（MAB）的最佳算法。但是，大多数复杂性结果都依赖于有界奖励或其他限制性假设。最近有关最佳二者结合算法的研究已经针对对手性和随机重尾MAB设置进行了探讨。这个算法在这两种情况下都是最优的，但不能充分利用数据。在本文中，我们针对奖励分布重尾的MAB问题提出了带剪辑的隐式规范化预测器。我们在奖励分布上提出渐进收敛性结果，并证明所提出的方法对于线性和非线性重尾随机MAB问题是最优的。我们还证明了与最好的二者结合算法相比，该算法通常表现更好。

    Implicitly Normalized Forecaster (online mirror descent with Tsallis entropy as prox-function) is known to be an optimal algorithm for adversarial multi-armed problems (MAB). However, most of the complexity results rely on bounded rewards or other restrictive assumptions. Recently closely related best-of-both-worlds algorithm were proposed for both adversarial and stochastic heavy-tailed MAB settings. This algorithm is known to be optimal in both settings, but fails to exploit data fully. In this paper, we propose Implicitly Normalized Forecaster with clipping for MAB problems with heavy-tailed distribution on rewards. We derive convergence results under mild assumptions on rewards distribution and show that the proposed method is optimal for both linear and non-linear heavy-tailed stochastic MAB problems. Also we show that algorithm usually performs better compared to best-of-two-worlds algorithm.
    
[^127]: 通过数据生成和参数畸变实现隐私保护联邦学习的接近最优效用

    Towards Achieving Near-optimal Utility for Privacy-Preserving Federated Learning via Data Generation and Parameter Distortion. (arXiv:2305.04288v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.04288](http://arxiv.org/abs/2305.04288)

    本论文提出了一种用数据生成和参数畸变实现隐私保护联邦学习接近最优效用的上限方法，其中通过降低方差和模型参数差异来衡量效用损失。

    

    联邦学习（FL）使参与方能够协作构建具有提高效用的全局模型，而不泄露私有数据信息。必须采用适当的保护机制来满足保护隐私和维护高模型效用的要求。目前采用的保护机制的本质，包括“随机化机制”和“压缩机制”，是通过畸变模型参数来保护隐私。我们通过原始模型参数和畸变模型参数之间的差距来衡量效用。我们想要确定在什么普遍条件下，通过数据生成和参数畸变，隐私保护的联邦学习可以实现接近最优的效用。为了提供接近最优效用的途径，我们提出了一个效用损失的上限，用两个主要项称为降低方差和模型参数差异来衡量。

    Federated learning (FL) enables participating parties to collaboratively build a global model with boosted utility without disclosing private data information. Appropriate protection mechanisms have to be adopted to fulfill the requirements in preserving \textit{privacy} and maintaining high model \textit{utility}. The nature of the widely-adopted protection mechanisms including \textit{Randomization Mechanism} and \textit{Compression Mechanism} is to protect privacy via distorting model parameter. We measure the utility via the gap between the original model parameter and the distorted model parameter. We want to identify under what general conditions privacy-preserving federated learning can achieve near-optimal utility via data generation and parameter distortion. To provide an avenue for achieving near-optimal utility, we present an upper bound for utility loss, which is measured using two main terms called variance-reduction and model parameter discrepancy separately. Our analysis
    
[^128]: 通过因果世界模型实现可解释强化学习

    Explainable Reinforcement Learning via a Causal World Model. (arXiv:2305.02749v1 [cs.LG])

    [http://arxiv.org/abs/2305.02749](http://arxiv.org/abs/2305.02749)

    本文提出了一种新的可解释强化学习框架，通过学习因果世界模型来解释行动的长期影响以及教学习者如何影响环境变量并最终导致奖励。

    

    给强化学习提供解释是一项挑战，因为行动可能对未来产生长期影响。本文提出了一种新的可解释强化学习框架：通过学习一个因果世界模型而不预先知道环境的因果结构。该模型捕捉到动作的影响，使我们能够通过因果链来解释行动的长期影响，从而揭示出行动是如何影响环境变量并最终导致奖励的。与大多数解释性模型的低准确性不同，我们的模型保持高准确性的同时提高了解释性，使其适用于基于模型的学习。因此，我们证明了我们的因果模型可以成为解释性和学习之间的桥梁。

    Generating explanations for reinforcement learning (RL) is challenging as actions may produce long-term effects on the future. In this paper, we develop a novel framework for explainable RL by learning a causal world model without prior knowledge of the causal structure of the environment. The model captures the influence of actions, allowing us to interpret the long-term effects of actions through causal chains, which present how actions influence environmental variables and finally lead to rewards. Different from most explanatory models which suffer from low accuracy, our model remains accurate while improving explainability, making it applicable in model-based learning. As a result, we demonstrate that our causal model can serve as the bridge between explainability and learning.
    
[^129]: 结构化稀疏动态训练

    Dynamic Sparse Training with Structured Sparsity. (arXiv:2305.02299v1 [cs.LG])

    [http://arxiv.org/abs/2305.02299](http://arxiv.org/abs/2305.02299)

    本文提出了一种结构化稀疏动态训练（DST）方法，学习一种变体的结构化 N:M 稀疏性，其加速在一般情况下通常被支持，可缩减参数和内存占用，同时相较于密集模型，具有减少推理时间的优势。

    

    动态稀疏训练在稀疏神经网络训练中取得了最先进的结果，并匹配了密集模型的泛化性，同时使得稀疏训练和推理成为可能。尽管得到的模型高度稀疏，理论上训练更便宜，但在实际硬件上，使用非结构化稀疏性加速依然具有人们所面临的挑战。在本文中，我们提出一种 DST 方法，学习一种变体的结构化 N:M 稀疏性，其加速在一般情况下通常被支持。此外，我们通过理论分析和实证结果，证明了特定 N:M 稀疏方法（常数扇入）的泛化性能，并展示了一种缩减参数和内存占用的紧凑表示。经过对 PyTorch CPU 实现的简单表示进行推断，我们证明了相较于密集模型，该方法减少了推理时间。我们的源代码可在 https://github.com/calgaryml/condensed-sparsity 上获得。

    DST methods achieve state-of-the-art results in sparse neural network training, matching the generalization of dense models while enabling sparse training and inference. Although the resulting models are highly sparse and theoretically cheaper to train, achieving speedups with unstructured sparsity on real-world hardware is challenging. In this work we propose a DST method to learn a variant of structured N:M sparsity, the acceleration of which in general is commonly supported in commodity hardware. Furthermore, we motivate with both a theoretical analysis and empirical results, the generalization performance of our specific N:M sparsity (constant fan-in), present a condensed representation with a reduced parameter and memory footprint, and demonstrate reduced inference time compared to dense models with a naive PyTorch CPU implementation of the condensed representation Our source code is available at https://github.com/calgaryml/condensed-sparsity
    
[^130]: 通向自由计算架构: 关于深度学习生成元宇宙虚拟建筑的综合调研

    Towards Computational Architecture of Liberty: A Comprehensive Survey on Deep Learning for Generating Virtual Architecture in the Metaverse. (arXiv:2305.00510v1 [cs.HC])

    [http://arxiv.org/abs/2305.00510](http://arxiv.org/abs/2305.00510)

    本文综述了当前最新的深度学习生成模型用于建筑形式的3D对象生成方法，强调了尚未充分探讨的问题，并提出了未来研究的重点议程。

    

    利用深度学习的3D形状生成技术正在受到计算机视觉和建筑设计两方的越来越多的关注。本综合调查旨在调查和比较当前最新的基于深度生成模型（DGMs）的3D对象生成方法，包括生成对抗网络（GANs）、变分自动编码器（VAEs）、3D感知图像和扩散模型。我们调查了187篇文章(占2018-2022年间发表文章的80.7%)，以回顾在虚拟环境下建筑生成可能性的领域，限于建筑形式。我们提供了建筑研究、虚拟环境和相关技术方法的概述，接着回顾了离散体素生成、由2D图像生成的3D模型以及条件参数的最近趋势。我们强调了3D生成和参数化控制中尚未充分探讨的问题值得进一步研究。此外，我们推测包括生成多样性、新型输出和嵌入式构建等四个研究议程可能会成为未来研究的重点。

    3D shape generation techniques utilizing deep learning are increasing attention from both computer vision and architectural design. This survey focuses on investigating and comparing the current latest approaches to 3D object generation with deep generative models (DGMs), including Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), 3D-aware images, and diffusion models. We discuss 187 articles (80.7% of articles published between 2018-2022) to review the field of generated possibilities of architecture in virtual environments, limited to the architecture form. We provide an overview of architectural research, virtual environment, and related technical approaches, followed by a review of recent trends in discrete voxel generation, 3D models generated from 2D images, and conditional parameters. We highlight under-explored issues in 3D generation and parameterized control that is worth further investigation. Moreover, we speculate that four research agendas including
    
[^131]: 医学图像的“Segment Anything Model”模型？

    Segment Anything Model for Medical Images?. (arXiv:2304.14660v1 [eess.IV])

    [http://arxiv.org/abs/2304.14660](http://arxiv.org/abs/2304.14660)

    “Segment Anything Model”（SAM）是适用于常规图像分割的基础模型，可以实现零样本图像分割，但在医学图像分割方面具有更高的挑战性。作者通过构建一个大型医学分割数据集来验证SAM在该领域的潜力。

    

    “Segment Anything Model”（SAM）是第一个适用于常规图像分割的基础模型。它设计了一种新颖的可推广分割任务，通过自动和手动两种模式实现了使用预训练模型进行零样本图像分割。SAM在各种自然图像分割任务中取得了显着的成果。然而，由于复杂的模态、细微的解剖结构、不确定的复杂对象边界和广泛的对象尺度，医学图像分割（MIS）更具挑战性。SAM在各种自然图像分割任务中取得了显着的成果。同时，零样本和高效的MIS可以很好地减少注释时间并促进医学图像分析的发展。因此，SAM似乎是一个潜在的工具，并且其在大型医学数据集上的表现应该进一步验证。我们收集和整理了52个开源数据集，并建立了一个具有16个模态和68个对象的大型医学分割数据集。

    The Segment Anything Model (SAM) is the first foundation model for general image segmentation. It designed a novel promotable segmentation task, ensuring zero-shot image segmentation using the pre-trained model via two main modes including automatic everything and manual prompt. SAM has achieved impressive results on various natural image segmentation tasks. However, medical image segmentation (MIS) is more challenging due to the complex modalities, fine anatomical structures, uncertain and complex object boundaries, and wide-range object scales. SAM has achieved impressive results on various natural image segmentation tasks. Meanwhile, zero-shot and efficient MIS can well reduce the annotation time and boost the development of medical image analysis. Hence, SAM seems to be a potential tool and its performance on large medical datasets should be further validated. We collected and sorted 52 open-source datasets, and build a large medical segmentation dataset with 16 modalities, 68 obje
    
[^132]: 实现高效和全面的城市时空预测：一个统一的库和性能基准

    Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction: A Unified Library and Performance Benchmark. (arXiv:2304.14343v1 [cs.LG])

    [http://arxiv.org/abs/2304.14343](http://arxiv.org/abs/2304.14343)

    本研究提出了一种称为原子文件的统一空间时间数据存储格式，开发了一个名为LibCity的开源库，重新构建了65个空间时间预测模型，并收集了55个空间时间数据集。同时还提出了城市时空预测模型的性能基准，为这一领域提供了一个可靠的评估工具。

    

    随着深度学习技术的不断推进和城市时空数据的积累，越来越多的深度学习模型被提出来解决城市时空预测问题。然而，现有领域存在许多限制，包括开放数据以各种格式存在，使用困难，极少数论文公开其代码和数据，以及开源模型经常使用不同的框架和平台，使得比较具有挑战性。迫切需要一个统一的框架来实施和评估这些方法。为解决这些问题，我们提供了一个城市时空预测的综合评估，并提出了一种称为原子文件的统一空间时间数据存储格式。我们还提出了一个名为LibCity的开源库，为研究人员提供了一个可靠的实验工具和一个方便的开发框架。在这个库中，我们已经重新构建了65个空间时间预测模型，并收集了55个空间时间数据集。此外，我们还引入了一个城市时空预测模型性能基准，包括效率和有效性度量，以进行公平比较。在这个基准上的实验结果证明了我们提出的统一库和基准的有用性和有效性。

    As deep learning technology advances and more urban spatial-temporal data accumulates, an increasing number of deep learning models are being proposed to solve urban spatial-temporal prediction problems. However, there are limitations in the existing field, including open-source data being in various formats and difficult to use, few papers making their code and data openly available, and open-source models often using different frameworks and platforms, making comparisons challenging. A standardized framework is urgently needed to implement and evaluate these methods. To address these issues, we provide a comprehensive review of urban spatial-temporal prediction and propose a unified storage format for spatial-temporal data called atomic files. We also propose LibCity, an open-source library that offers researchers a credible experimental tool and a convenient development framework. In this library, we have reproduced 65 spatial-temporal prediction models and collected 55 spatial-temp
    
[^133]: 可解释人工智能的范畴基础：一种统一的结构和语义形式体系。

    Categorical Foundations of Explainable AI: A Unifying Formalism of Structures and Semantics. (arXiv:2304.14094v1 [cs.AI])

    [http://arxiv.org/abs/2304.14094](http://arxiv.org/abs/2304.14094)

    本文采用范畴理论的框架，提出了可解释AI的统一理论体系，为领域中所有重要术语提供了清晰的形式定义，并提供了遵循所提出结构的领域分类法。

    

    可解释人工智能（XAI）旨在回答与AI模型部署相关的伦理和法律问题。然而，相当数量的领域特定评论强调需要一个数学基础来定义领域中的关键概念，即使“解释”这个术语还缺乏精确定义。这些评论还主张建立一个健全而统一的可解释AI形式体系，以避免出现不良提出问题，帮助研究人员浏览一个快速增长的知识体系。据作者所知，该论文是填补该空白的首次尝试，通过形式化一个可解释AI的统一理论。采用范畴理论的框架，特别是反馈单调范畴，我们首先提供了可解释AI中所有重要术语的形式定义。然后，我们提出了一个遵循提出结构的领域分类法，展示了如何使用引入的理论来对当前研究的所有主要XAI系统类进行分类。

    Explainable AI (XAI) aims to answer ethical and legal questions associated with the deployment of AI models. However, a considerable number of domain-specific reviews highlight the need of a mathematical foundation for the key notions in the field, considering that even the term "explanation" still lacks a precise definition. These reviews also advocate for a sound and unifying formalism for explainable AI, to avoid the emergence of ill-posed questions, and to help researchers navigate a rapidly growing body of knowledge. To the authors knowledge, this paper is the first attempt to fill this gap by formalizing a unifying theory of XAI. Employing the framework of category theory, and feedback monoidal categories in particular, we first provide formal definitions for all essential terms in explainable AI. Then we propose a taxonomy of the field following the proposed structure, showing how the introduced theory can be used to categorize all the main classes of XAI systems currently studi
    
[^134]: 变分扩散自编码器：具有无条件扩散先验的深层潜变量模型

    Variational Diffusion Auto-encoder: Deep Latent Variable Model with Unconditional Diffusion Prior. (arXiv:2304.12141v1 [cs.LG])

    [http://arxiv.org/abs/2304.12141](http://arxiv.org/abs/2304.12141)

    本文提出了一种基于扩散模型对条件数据分布进行建模的变分扩散自编码器方法，它避免了对参数形式做出强烈假设，可以显著提高生成图像的质量。

    

    变分自编码器是深度生成建模的一种最流行的方法。尽管取得了成功，但因为高度不现实的建模假设，即条件数据分布p(x|z)可以近似为各向同性高斯分布，所以由变分自编码器生成的图像是模糊的。在本文中，我们引入了一种基于扩散模型对条件数据分布p(x|z)进行建模的原则性方法。我们证明了可以创建类似变分自编码器的深潜变量模型，而无需对p(x|z)做高斯假设，甚至不需要训练解码器网络。通过Bayes'规则，可以将经过训练的编码器和无条件扩散模型组合到一起，以获得一个表达丰富的p(x|z)模型。我们的方法避免了对参数形式p(x|z)做出强烈假设，因此可以显著提高生成图像的质量。

    Variational auto-encoders (VAEs) are one of the most popular approaches to deep generative modeling. Despite their success, images generated by VAEs are known to suffer from blurriness, due to a highly unrealistic modeling assumption that the conditional data distribution $ p(\textbf{x} | \textbf{z})$ can be approximated as an isotropic Gaussian. In this work we introduce a principled approach to modeling the conditional data distribution $p(\textbf{x} | \textbf{z})$ by incorporating a diffusion model. We show that it is possible to create a VAE-like deep latent variable model without making the Gaussian assumption on $ p(\textbf{x} | \textbf{z}) $ or even training a decoder network. A trained encoder and an unconditional diffusion model can be combined via Bayes' rule for score functions to obtain an expressive model for $ p(\textbf{x} | \textbf{z}) $. Our approach avoids making strong assumptions on the parametric form of $ p(\textbf{x} | \textbf{z}) $, and thus allows to significant
    
[^135]: 拒绝服务或细粒度控制：面向联邦学习的灵活模型毒化攻击

    Denial-of-Service or Fine-Grained Control: Towards Flexible Model Poisoning Attacks on Federated Learning. (arXiv:2304.10783v1 [cs.LG])

    [http://arxiv.org/abs/2304.10783](http://arxiv.org/abs/2304.10783)

    本文提出了一种灵活的联邦学习模型毒化攻击策略，既可以实现拒绝服务(Dos)目标，也可以精确控制全局准确性，具有高效和隐形的特点。

    

    联邦学习容易受到毒化攻击，敌对方会破坏全局聚合结果并造成拒绝服务。本文提出了一种灵活模型毒化攻击(FMPA)，旨在实现多功能攻击目标。本文考虑如下实际情景：敌对方没有关于FL系统的额外信息（例如，聚合规则或良性设备上的更新）。FMPA利用全局历史信息构建估计器，将下一轮全局模型预测为良性参考模型，并微调参考模型以获得所需的精度低和扰动小的毒化模型。FMPA不仅可以达到DoS的目标，还可以自然地扩展到启动细粒度可控攻击，从而精确降低全局准确性。本文进一步探索了FMPA在几种FL场景下的攻击性能，包括二元分类和图像分类，在不同的攻击目标和攻击知识水平下。实验结果表明，FMPA可以有效而高效地实现所需的攻击目标，同时保持隐形和不可感知。

    Federated learning (FL) is vulnerable to poisoning attacks, where adversaries corrupt the global aggregation results and cause denial-of-service (DoS). Unlike recent model poisoning attacks that optimize the amplitude of malicious perturbations along certain prescribed directions to cause DoS, we propose a Flexible Model Poisoning Attack (FMPA) that can achieve versatile attack goals. We consider a practical threat scenario where no extra knowledge about the FL system (e.g., aggregation rules or updates on benign devices) is available to adversaries. FMPA exploits the global historical information to construct an estimator that predicts the next round of the global model as a benign reference. It then fine-tunes the reference model to obtain the desired poisoned model with low accuracy and small perturbations. Besides the goal of causing DoS, FMPA can be naturally extended to launch a fine-grained controllable attack, making it possible to precisely reduce the global accuracy. Armed wi
    
[^136]: 一种可扩展的序列转移优化问题生成器

    A Scalable Test Problem Generator for Sequential Transfer Optimization. (arXiv:2304.08503v1 [cs.NE])

    [http://arxiv.org/abs/2304.08503](http://arxiv.org/abs/2304.08503)

    STO中已有的测试问题设计不完善，难以代表真实问题多样化关系，限制了算法的表现。本文介绍了一种可扩展的序列转移优化问题生成器。

    

    近年来，序列转移优化(STO)受到越来越多的研究关注，旨在利用储存在数据库中以前求解的优化任务的知识来提高优化性能。然而，尽管算法设计已有重大进展，但STO中的测试问题设计并不完善。它们往往是由其他基准函数随机组合而成，这些基准函数具有相同的最佳值，或者生成自表现出有限变化的实际问题。这些问题中源任务和目标任务的最优解之间的关系是手动配置的，因此单调，限制了它们表征真实问题多样化关系的能力。因此，许多算法在这些问题上取得的有前途的结果具有高度的偏见，并且难以推广到其他问题。鉴于此，我们首先引入了一些表征STO问题的基本概念。

    Sequential transfer optimization (STO), which aims to improve optimization performance by exploiting knowledge captured from previously-solved optimization tasks stored in a database, has been gaining increasing research attention in recent years. However, despite significant advancements in algorithm design, the test problems in STO are not well designed. Oftentimes, they are either randomly assembled by other benchmark functions that have identical optima or are generated from practical problems that exhibit limited variations. The relationships between the optimal solutions of source and target tasks in these problems are manually configured and thus monotonous, limiting their ability to represent the diverse relationships of real-world problems. Consequently, the promising results achieved by many algorithms on these problems are highly biased and difficult to be generalized to other problems. In light of this, we first introduce a few rudimentary concepts for characterizing STO pr
    
[^137]: 可能大致正确联邦学习

    Probably Approximately Correct Federated Learning. (arXiv:2304.04641v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.04641](http://arxiv.org/abs/2304.04641)

    本文提出了FedPAC框架，利用PAC学习理论推导出一个解析解，可以保证FL之间隐私、效用和效率的最佳权衡。

    

    联邦学习是一种新的分布式学习范例，其主要支柱为隐私、效用和效率。现有研究表明，同时实现无穷小隐私泄露、效用损失和效率是不可能的。因此，在设计联邦学习算法时，如何找到最佳权衡解决方案是关键考虑因素。一种常见的方法是将权衡问题视为多目标优化问题，即目标是在约束隐私泄露不超过预定值的情况下最小化效用损失和效率降低。然而，现有的多目标优化框架非常耗时，并且不能保证帕累托前沿的存在性，这激励我们寻求一种方法，将多目标问题转化为单目标问题，因为它更高效、更容易被解决。为此，本文提出了FedPAC，这是一个统一的框架，利用PAC学习理论推导出一个解析解，可以保证FL之间隐私、效用和效率的最佳权衡。具体而言，我们首先将FL问题公式化为一个二分类任务，然后设计一个自适应FL算法，动态调整每个客户端的采样比率，以平衡全局和本地的隐私-效用权衡，最后证明FedPAC可以在温和的假设下高概率地实现最优的隐私-效用权衡。基准数据集上的大量实验证明了我们提出的FedPAC框架的功效和效率。

    Federated learning (FL) is a new distributed learning paradigm, with privacy, utility, and efficiency as its primary pillars. Existing research indicates that it is unlikely to simultaneously attain infinitesimal privacy leakage, utility loss, and efficiency. Therefore, how to find an optimal trade-off solution is the key consideration when designing the FL algorithm. One common way is to cast the trade-off problem as a multi-objective optimization problem, i.e., the goal is to minimize the utility loss and efficiency reduction while constraining the privacy leakage not exceeding a predefined value. However, existing multi-objective optimization frameworks are very time-consuming, and do not guarantee the existence of the Pareto frontier, this motivates us to seek a solution to transform the multi-objective problem into a single-objective problem because it is more efficient and easier to be solved. To this end, in this paper, we propose FedPAC, a unified framework that leverages PAC l
    
[^138]: TransPimLib：用于处理器内存系统上高效的超越函数的库

    TransPimLib: A Library for Efficient Transcendental Functions on Processing-in-Memory Systems. (arXiv:2304.01951v1 [cs.MS])

    [http://arxiv.org/abs/2304.01951](http://arxiv.org/abs/2304.01951)

    TransPimLib提供了处理器内存系统上高效的超越函数计算方法，有助于提高PIM系统的计算能力和支持更广泛的工作负载，特别是机器学习应用中的激活函数。

    

    处理器内存系统（PIM）承诺减轻现代计算系统中的数据移动瓶颈。然而，现有的真实PIM系统有一个内在的劣势，即它们的硬件比传统的处理器（CPU、GPU）更加受限，因为在内存附近或内部构建处理元件的难度和成本很高。因此，通用的PIM架构支持相当有限的指令集，并且难以执行复杂的操作，例如超越函数和其他难以计算的操作（例如平方根）。这些操作对于一些现代工作负载尤其重要，例如机器学习应用中的激活函数。为了在通用的PIM系统中提供对超越（和其他难以计算）函数的支持，我们介绍了TransPimLib，这是一个库，提供基于CORDIC和LUT的三角函数、双曲函数、指数、对数、平方根等难以计算的函数的方法。

    Processing-in-memory (PIM) promises to alleviate the data movement bottleneck in modern computing systems. However, current real-world PIM systems have the inherent disadvantage that their hardware is more constrained than in conventional processors (CPU, GPU), due to the difficulty and cost of building processing elements near or inside the memory. As a result, general-purpose PIM architectures support fairly limited instruction sets and struggle to execute complex operations such as transcendental functions and other hard-to-calculate operations (e.g., square root). These operations are particularly important for some modern workloads, e.g., activation functions in machine learning applications.  In order to provide support for transcendental (and other hard-to-calculate) functions in general-purpose PIM systems, we present \emph{TransPimLib}, a library that provides CORDIC-based and LUT-based methods for trigonometric functions, hyperbolic functions, exponentiation, logarithm, squar
    
[^139]: 将未标记数据纳入贝叶斯神经网络中

    Incorporating Unlabelled Data into Bayesian Neural Networks. (arXiv:2304.01762v1 [cs.LG])

    [http://arxiv.org/abs/2304.01762](http://arxiv.org/abs/2304.01762)

    该论文提出了一种利用未标记数据学习贝叶斯神经网络（BNNs）的对比框架，通过该框架提出了一种同时具备自监督学习的标签效率和贝叶斯方法中的不确定性估计的实用BNN算法。最后，该方法在半监督和低预算主动学习问题中展现出了数据高效学习的优势。

    

    我们提出了一个对贝叶斯神经网络（BNNs）中先验分布进行学习的对比框架，利用未标记数据来优化。基于该框架，我们提出了一种实用的BNN算法，同时具备自监督学习的标签效率和贝叶斯方法中的根据原则的不确定性估计。最后，我们展示了我们的方法在半监督和低预算主动学习问题中的数据高效学习优势。

    We develop a contrastive framework for learning better prior distributions for Bayesian Neural Networks (BNNs) using unlabelled data. With this framework, we propose a practical BNN algorithm that offers the label-efficiency of self-supervised learning and the principled uncertainty estimates of Bayesian methods. Finally, we demonstrate the advantages of our approach for data-efficient learning in semi-supervised and low-budget active learning problems.
    
[^140]: 显式规划有助于语言模型进行逻辑推理

    Explicit Planning Helps Language Models in Logical Reasoning. (arXiv:2303.15714v1 [cs.CL])

    [http://arxiv.org/abs/2303.15714](http://arxiv.org/abs/2303.15714)

    本文提出了一个新的系统，使用语言模型进行多步逻辑推理，采用了显式规划来帮助做出更明智的决策，比其他竞争系统表现更好，显式规划在系统性能中起着关键作用。

    

    语言模型在各种自然语言处理任务中表现出色。本文提出了一个新颖的系统，采用语言模型进行多步逻辑推理。我们的系统将显式规划纳入到推理过程中，因此可以通过展望未来的效果来做出更明智的决策。在实验中，我们的全套系统在多项选择题答题任务中明显优于其他竞争系统，尽管只有约15亿个参数，但与GPT-3-davinci表现相当。我们进行了多个消融研究以证明显式规划在系统性能中起着关键作用。

    Language models have been shown to perform remarkably well on a wide range of natural language processing tasks. In this paper, we propose a novel system that uses language models to perform multi-step logical reasoning. Our system incorporates explicit planning into its inference procedure, thus able to make more informed reasoning decisions at each step by looking ahead into their future effects. In our experiments, our full system significantly outperforms other competing systems. On a multiple-choice question answering task, our system performs competitively compared to GPT-3-davinci despite having only around 1.5B parameters. We conduct several ablation studies to demonstrate that explicit planning plays a crucial role in the system's performance.
    
[^141]: 对称正定矩阵上的自适应黎曼度量

    Adaptive Riemannian Metrics on SPD Manifolds. (arXiv:2303.15477v1 [cs.LG])

    [http://arxiv.org/abs/2303.15477](http://arxiv.org/abs/2303.15477)

    本文提出了自适应黎曼度量来改进SPD神经网络的次优性能，实验结果表明该度量能使网络表现更好。

    

    由于其内在能够编码数据中的潜在结构相关性，对称正定（SPD）矩阵在机器学习中受到广泛关注。为了反映SPD流形的非欧几里得几何，已经提出了许多成功的黎曼度量。然而，现有的固定度量张量可能会导致SPD矩阵学习的次优性能，特别是对于SPD神经网络。为了解决这个限制，我们利用拉回的思想，提出了自适应SPD流形的黎曼度量。此外，我们还对我们的度量提出了全面的理论。三个数据集上的实验表明，配备了我们提出的度量的SPD网络可以展现出优越的性能。

    Symmetric Positive Definite (SPD) matrices have received wide attention in machine learning due to their intrinsic capacity of encoding underlying structural correlation in data. To reflect the non-Euclidean geometry of SPD manifolds, many successful Riemannian metrics have been proposed. However, existing fixed metric tensors might lead to sub-optimal performance for SPD matrices learning, especially for SPD neural networks. To remedy this limitation, we leverage the idea of pullback and propose adaptive Riemannian metrics for SPD manifolds. Moreover, we present comprehensive theories for our metrics. Experiments on three datasets demonstrate that equipped with the proposed metrics, SPD networks can exhibit superior performance.
    
[^142]: 关于锐度感知最小化的统计性质：可证明的保证

    On Statistical Properties of Sharpness-Aware Minimization: Provable Guarantees. (arXiv:2302.11836v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.11836](http://arxiv.org/abs/2302.11836)

    SAM是一种优化框架，旨在通过获得更平坦（即更不锐利）的解来改善深度神经网络的泛化能力。我们研究两个统计问题，在某些条件下，证明了SAM在预测误差方面比梯度下降有更小的误差，并适用于非凸问题。此外，我们的设置表明，SAM的解更不锐利，证明了我们的结论。

    

    锐度感知最小化 (SAM) 是一种旨在通过获得更平坦（即更不锐利）的解来改善深度神经网络泛化能力的最新优化框架。由于SAM在数值上十分成功，因此最近的论文研究了该框架的理论方面，并表明SAM的解确实是平坦的。然而，在SAM的统计性质方面，理论探索有限。本文直接研究SAM的统计性能，并提出了一个新的理论解释，解释了为什么SAM能够进行良好的泛化。为此，我们研究了两个统计问题，包括具有隐藏层的神经网络和核回归，并证明在某些条件下，SAM对于梯度下降(GD)相比有更小的预测误差。我们的结果涉及凸和非凸设置，并表明SAM特别适用于非凸问题。此外，我们还证明，在我们的设置中，SAM的解也更不锐利，证明了我们的结论。

    Sharpness-Aware Minimization (SAM) is a recent optimization framework aiming to improve the deep neural network generalization, through obtaining flatter (i.e. less sharp) solutions. As SAM has been numerically successful, recent papers have studied the theoretical aspects of the framework and have shown SAM solutions are indeed flat. However, there has been limited theoretical exploration regarding statistical properties of SAM. In this work, we directly study the statistical performance of SAM, and present a new theoretical explanation of why SAM generalizes well. To this end, we study two statistical problems, neural networks with a hidden layer and kernel regression, and prove under certain conditions, SAM has smaller prediction error over Gradient Descent (GD). Our results concern both convex and non-convex settings, and show that SAM is particularly well-suited for non-convex problems. Additionally, we prove that in our setup, SAM solutions are less sharp as well, showing our res
    
[^143]: 基于函数逼近的强化学习：从线性到非线性

    Reinforcement Learning with Function Approximation: From Linear to Nonlinear. (arXiv:2302.09703v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09703](http://arxiv.org/abs/2302.09703)

    本文回顾了近年来在线性和非线性逼近环境下强化学习算法的错误分析，并强调了逼近误差和估计误差/样本复杂度。在线性问题结构的假设下，近期的算法实现了多项式样本复杂度，然而尚未实现最小最大速率。

    

    函数逼近是现代强化学习算法中不可或缺的组成部分，其旨在解决高维度大状态空间问题。本文回顾了近年来在线性和非线性逼近环境下强化学习算法的错误分析，并强调了逼近误差和估计误差/样本复杂度。我们讨论了与逼近误差有关的各种性质，并给出了转移概率和奖励函数的具体条件，使得这些性质成立。在强化学习中，样本复杂度的分析比监督学习更为复杂，主要是由于分布不匹配现象。在线性问题结构的假设下，文献中的许多算法都可以实现多项式样本复杂度，与特征数量、剧集长度和准确性有关，尽管尚未实现最小最大速率。

    Function approximation has been an indispensable component in modern reinforcement learning algorithms designed to tackle problems with large state spaces in high dimensions. This paper reviews recent results on error analysis for these reinforcement learning algorithms in linear or nonlinear approximation settings, emphasizing approximation error and estimation error/sample complexity. We discuss various properties related to approximation error and present concrete conditions on transition probability and reward function under which these properties hold true. Sample complexity analysis in reinforcement learning is more complicated than in supervised learning, primarily due to the distribution mismatch phenomenon. With assumptions on the linear structure of the problem, numerous algorithms in the literature achieve polynomial sample complexity with respect to the number of features, episode length, and accuracy, although the minimax rate has not been achieved yet. These results rely 
    
[^144]: 零样本批次级异常检测

    Zero-Shot Batch-Level Anomaly Detection. (arXiv:2302.07849v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07849](http://arxiv.org/abs/2302.07849)

    本文提出了一种名为“自适应中心表示”的方法，用于零样本批次级异常检测。该方法利用批量归一化来训练现成的深度异常检测器，可以自动零样本泛化为未见过的AD任务。在实验中，该方法显示出了在多种数据集上的优秀表现，对表格数据进行了零样本AD。

    

    异常检测（AD）在许多安全关键的应用领域中发挥着关键作用。适应正常数据分布漂移的异常检测器调整，特别是当没有针对“新正常”进行训练的数据时，这一挑战导致产生了零样本AD技术。在本文中，我们提出了一种名为自适应中心表示（ACR）的简单而有效的方法，用于零样本批次级AD。我们的方法使用批量归一化来训练现成的深度异常检测器（例如深度SVDD）来适应一组相互关联的训练数据分布，使其能够自动零样本泛化为未见过的AD任务。这个简单的方法，批量归一化加元训练，是一种非常有效和多功能的工具。我们的结果展示了对表格数据的第一个零样本AD结果，并在来自专业领域的图像数据的零样本异常检测和分段方面优于现有方法。

    Anomaly detection (AD) plays a crucial role in many safety-critical application domains. The challenge of adapting an anomaly detector to drift in the normal data distribution, especially when no training data is available for the "new normal," has led to the development of zero-shot AD techniques. In this paper, we propose a simple yet effective method called Adaptive Centered Representations (ACR) for zero-shot batch-level AD. Our approach trains off-the-shelf deep anomaly detectors (such as deep SVDD) to adapt to a set of inter-related training data distributions in combination with batch normalization, enabling automatic zero-shot generalization for unseen AD tasks. This simple recipe, batch normalization plus meta-training, is a highly effective and versatile tool. Our results demonstrate the first zero-shot AD results for tabular data and outperform existing methods in zero-shot anomaly detection and segmentation on image data from specialized domains.
    
[^145]: 重参数化下神经网络参数空间的几何学

    The Geometry of Neural Nets' Parameter Spaces Under Reparametrization. (arXiv:2302.07384v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07384](http://arxiv.org/abs/2302.07384)

    研究了神经网络在重参数化下的不变性，如果显式地表示度量并使用正确的相关变换规则，则不变性是任何神经网络的固有属性。

    

    模型重参数化是改善神经网络训练的一种流行方法，但也可能存在问题，如在Hessian平坦度测量、优化轨迹和概率密度模式等方面引入不一致性。这使得下游分析变得更为复杂：例如，由于任意的重参数化都可以改变二者之间的关系，因此无法明确地将平坦度与泛化联系起来。在本文中，我们从黎曼几何的角度研究了神经网络在重参数化下的不变性。从这个角度来看，如果我们显式地表示度量并使用正确的相关变换规则，那么不变性是任何神经网络的固有属性。这一点很重要，因为尽管度量始终存在，但通常被隐式地假定为单位矩阵，并因此从符号中省略，然后在重参数化下丢失了。我们讨论了衡量平坦度所带来的启示。

    Model reparametrization, which follows the change-of-variable rule of calculus, is a popular way to improve the training of neural nets. But it can also be problematic since it can induce inconsistencies in, e.g., Hessian-based flatness measures, optimization trajectories, and modes of probability densities. This complicates downstream analyses: e.g. one cannot definitively relate flatness with generalization since arbitrary reparametrization changes their relationship. In this work, we study the invariance of neural nets under reparametrization from the perspective of Riemannian geometry. From this point of view, invariance is an inherent property of any neural net if one explicitly represents the metric and uses the correct associated transformation rules. This is important since although the metric is always present, it is often implicitly assumed as identity, and thus dropped from the notation, then lost under reparametrization. We discuss implications for measuring the flatness of
    
[^146]: 神经容量聚类

    Neural Capacitated Clustering. (arXiv:2302.05134v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05134](http://arxiv.org/abs/2302.05134)

    本论文提出了“神经容量聚类”方法，利用神经网络预测数据点分配到簇中心的概率，结合一种类似于K均值的迭代过程，在容量约束下对聚类问题进行求解。通过人造数据和实际数据集的实验，该方法在性能上优于文献中的多个最先进的数学和启发式求解器。

    

    最近的深度聚类研究发现了一些有前途的方法用于约束聚类问题。它们通常可以使用成对约束来指导数据的分组。然而，许多问题具有聚类级别的约束，例如容量约束聚类问题（CCP），其中每个点具有权重，每个簇中所有点的权重总和由规定的容量限制。本文提出了一种新的CCP方法“神经容量聚类”，该方法学习一个神经网络来预测数据点分配到簇中心的概率，该神经网络以其他问题实例的最优或近似最优解的数据集为基础。在推理过程中，使用得分在容量约束下迭代地类似于K均值的过程来改进分配。在人造数据和两个真实数据集上的实验中，我们的方法优于文献中的多个最先进的数学和启发式求解器。

    Recent work on deep clustering has found new promising methods also for constrained clustering problems. Their typically pairwise constraints often can be used to guide the partitioning of the data. Many problems however, feature cluster-level constraints, e.g. the Capacitated Clustering Problem (CCP), where each point has a weight and the total weight sum of all points in each cluster is bounded by a prescribed capacity. In this paper we propose a new method for the CCP, Neural Capacited Clustering, that learns a neural network to predict the assignment probabilities of points to cluster centers from a data set of optimal or near optimal past solutions of other problem instances. During inference, the resulting scores are then used in an iterative k-means like procedure to refine the assignment under capacity constraints. In our experiments on artificial data and two real world datasets our approach outperforms several state-of-the-art mathematical and heuristic solvers from the liter
    
[^147]: 二次内存是实现凸优化最优查询复杂度所必需的：质心是帕累托优化

    Quadratic Memory is Necessary for Optimal Query Complexity in Convex Optimization: Center-of-Mass is Pareto-Optimal. (arXiv:2302.04963v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04963](http://arxiv.org/abs/2302.04963)

    本文证明了在凸优化中，实现最优 oracle 复杂性所必需要的内存为二次，并且在处理 1-Lipschitz 凸函数时，使用 $d^{2-\delta}$ 内存的任何算法都需要进行 $\tilde\Omega(d^{1+\delta/3})$ 次查询。此外，在可行性问题中，使用至多 $d^{2-\delta}$ 存储器容量的分离 oracle 需要进行 $\tilde\Omega(d^{1+\delta})$ 次查询。

    

    我们给出了凸优化及相关可行性问题的查询复杂性下界。我们表明，实现凸优化的一阶最优性的最优 oracle 复杂性所必需的内存是二次的。特别地，这表明在维度 $d$ 中使用 $\tilde O(d^2)$ 内存和 $\tilde O(d)$ 查询的质心切平面算法对凸优化和可行性问题来说都是帕累托最优的，精度为 $1/d^4$，上限为对数因子。确切地说，我们证明为了在单位球上将 $1$-Lipschitz 凸函数最小化到 $1/d^4$ 的精度，任何使用至多 $d^{2-\delta}$ 个内存位的确定性一阶算法都必须进行 $\tilde\Omega(d^{1+\delta/3})$ 次查询，其中 $\delta\in[0,1]$。对于可行性问题，在其只有访问分离 oracle 的情况下，我们展示了更强的权衡：对于至多 $d^{2-\delta}$ 的存储器容量，所需的查询数量为 $\tilde\Omega(d^{1+\delta})$。这解决了 COLT 2019 的一个未解决问题。

    We give query complexity lower bounds for convex optimization and the related feasibility problem. We show that quadratic memory is necessary to achieve the optimal oracle complexity for first-order convex optimization. In particular, this shows that center-of-mass cutting-planes algorithms in dimension $d$ which use $\tilde O(d^2)$ memory and $\tilde O(d)$ queries are Pareto-optimal for both convex optimization and the feasibility problem, up to logarithmic factors. Precisely, we prove that to minimize $1$-Lipschitz convex functions over the unit ball to $1/d^4$ accuracy, any deterministic first-order algorithms using at most $d^{2-\delta}$ bits of memory must make $\tilde\Omega(d^{1+\delta/3})$ queries, for any $\delta\in[0,1]$. For the feasibility problem, in which an algorithm only has access to a separation oracle, we show a stronger trade-off: for at most $d^{2-\delta}$ memory, the number of queries required is $\tilde\Omega(d^{1+\delta})$. This resolves a COLT 2019 open problem 
    
[^148]: 移民议题的再定义？乌克兰危机期间欧洲态度变化的多语言分析

    Migration Reframed? A multilingual analysis on the stance shift in Europe during the Ukrainian crisis. (arXiv:2302.02813v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2302.02813](http://arxiv.org/abs/2302.02813)

    乌克兰危机引起了欧洲对移民议题态度的变化，特别是对来自乌克兰的难民。研究者运用多语言分析技术对新闻和社交媒体上的相关报道进行研究，发现了一种对移民议题讨论的重构。

    

    乌克兰战争似乎积极改变了欧洲对移民这一关键社会议题的态度——至少对来自乌克兰的难民来说如此。本研究调查了该议题在网络新闻和社交媒体上的反映，以此将网上的议题表达与社会对其的感知联系起来。为此，我们结合并改编了领先的自动文本处理技术，采用新型的多语言立场检测方法。从2021年9月开始的一年内，我们获得了565家欧洲新闻机构发布的550万条推特帖子和回复，进行了关于移民相关的媒体报道和相关社交媒体互动的多语言分析。我们的分析结果表明，实际上存在一种讨论重塑，可以通过术语的变化来说明，例如，从“移民”到“难民”，甚至常常强调“真正的难民”。

    The war in Ukraine seems to have positively changed the attitude toward the critical societal topic of migration in Europe -- at least towards refugees from Ukraine. We investigate whether this impression is substantiated by how the topic is reflected in online news and social media, thus linking the representation of the issue on the Web to its perception in society. For this purpose, we combine and adapt leading-edge automatic text processing for a novel multilingual stance detection approach. Starting from 5.5M Twitter posts published by 565 European news outlets in one year, beginning September 2021, plus replies, we perform a multilingual analysis of migration-related media coverage and associated social media interaction for Europe and selected European countries.  The results of our analysis show that there is actually a reframing of the discussion illustrated by the terminology change, e.g., from "migrant" to "refugee", often even accentuated with phrases such as "real refugees
    
[^149]: PubGraph: 一个大规模的科学知识图谱

    PubGraph: A Large-Scale Scientific Knowledge Graph. (arXiv:2302.02231v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.02231](http://arxiv.org/abs/2302.02231)

    PubGraph是一个大规模的、全面的科学知识图谱，包含超过3.85亿个实体和130亿个主要边缘，可以支持对科学网络进行推理研究。

    

    科研出版物是分享新发现、新方法、新技术和洞见的主要方式。然而，缺乏一个大规模、全面且易于使用的资源来捕捉出版物、作者和期刊之间的各种关系，给对科学有更深入理解的应用带来了障碍。在本文中，我们介绍了PubGraph，它是一个新的研究科学进展的资源，以大规模的知识图谱（KG）的形式出现，具有超过3.85亿个实体、130亿个主要边缘和15亿个限定词边缘。 PubGraph是全面的，并使用Wikidata本体论统一来自不同来源（包括Wikidata、OpenAlex和Semantic Scholar）的数据。除了这些来源的元数据外，PubGraph还包括来自辅助社区检测算法和大型语言模型的输出。为了进一步支持关于科学网络推理的研究，我们创建了几个大规模​​的基准。

    Research publications are the primary vehicle for sharing scientific progress in the form of new discoveries, methods, techniques, and insights. Unfortunately, the lack of a large-scale, comprehensive, and easy-to-use resource capturing the myriad relationships between publications, their authors, and venues presents a barrier to applications for gaining a deeper understanding of science. In this paper, we present PubGraph, a new resource for studying scientific progress that takes the form of a large-scale knowledge graph (KG) with more than 385M entities, 13B main edges, and 1.5B qualifier edges. PubGraph is comprehensive and unifies data from various sources, including Wikidata, OpenAlex, and Semantic Scholar, using the Wikidata ontology. Beyond the metadata available from these sources, PubGraph includes outputs from auxiliary community detection algorithms and large language models. To further support studies on reasoning over scientific networks, we create several large-scale ben
    
[^150]: 防御后门攻击的显著条件扩散算法

    Salient Conditional Diffusion for Defending Against Backdoor Attacks. (arXiv:2301.13862v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13862](http://arxiv.org/abs/2301.13862)

    Sancdifi是一种有效防御后门攻击的新算法，通过生成基于salience map的masks调节去噪扩散概率模型，能够有效去除被后门攻击污染的数据中的触发器，同时在干净数据上也能恢复出突出特征，而且无需使用特洛伊网络模型参数，作为一种黑盒防御机制发挥作用。

    

    本文提出了一种新颖的算法——显著条件扩散（Sancdifi），是目前针对后门攻击的最先进防御机制。Sancdifi利用去噪扩散概率模型（DDPM）对一张加入噪声的图像进行降质处理，然后再利用学习到的逆扩散重构该图像。重要的是，我们基于显著性图生成掩码来调节扩散，允许DDPM对最突出的像素进行更强的扩散。因此，Sancdifi在去除受到后门攻击污染的数据中的触发器时非常有效。与此同时，Sancdifi可以可靠地在干净数据上应用，并恢复突出特征。这种性能是在无需使用特洛伊网络模型参数的情况下实现的，这意味着Sancdifi作为一种黑盒防御机制发挥作用。

    We propose a novel algorithm, Salient Conditional Diffusion (Sancdifi), a state-of-the-art defense against backdoor attacks. Sancdifi uses a denoising diffusion probabilistic model (DDPM) to degrade an image with noise and then recover said image using the learned reverse diffusion. Critically, we compute saliency map-based masks to condition our diffusion, allowing for stronger diffusion on the most salient pixels by the DDPM. As a result, Sancdifi is highly effective at diffusing out triggers in data poisoned by backdoor attacks. At the same time, it reliably recovers salient features when applied to clean data. This performance is achieved without requiring access to the model parameters of the Trojan network, meaning Sancdifi operates as a black-box defense.
    
[^151]: 位置-尺度噪声模型中因果推断的最大似然与独立性检验比较研究

    Cause-Effect Inference in Location-Scale Noise Models: Maximum Likelihood vs. Independence Testing. (arXiv:2301.12930v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12930](http://arxiv.org/abs/2301.12930)

    通过引入异方差位置-尺度噪声函数模型，该论文在正确说明噪声分布的情况下，通过最大似然实现了最先进的准确性。但是，在用户错误指定噪声分布的形式时，分析表明因果推断的精度会急剧下降。因此，该论文提出通过因果模型选择实现稳定而准确的因果推断。

    

    因果发现的一个基本问题是推断两个随机变量之间的正确因果方向。最近引入的异方差位置-尺度噪声函数模型 (LSNM) 结合了表达能力和可识别性保证，在正确指定噪声分布的情况下，通过最大似然实现了最先进的准确性。然而，我们通过广泛的实证评估表明，当用户错误指定噪声分布的形式时，精度会急剧下降。我们的分析表明，这种失败主要发生在反因果方向的条件方差小于因果方向的条件方差的情况下。作为一种替代方案，发现通过因果模型选择可以在缺乏噪声分布知识的情况下，实现稳定而准确的因果推断。

    A fundamental problem of causal discovery is cause-effect inference, learning the correct causal direction between two random variables. Significant progress has been made through modelling the effect as a function of its cause and a noise term, which allows us to leverage assumptions about the generating function class. The recently introduced heteroscedastic location-scale noise functional models (LSNMs) combine expressive power with identifiability guarantees. LSNM model selection based on maximizing likelihood achieves state-of-the-art accuracy, when the noise distributions are correctly specified. However, through an extensive empirical evaluation, we demonstrate that the accuracy deteriorates sharply when the form of the noise distribution is misspecified by the user. Our analysis shows that the failure occurs mainly when the conditional variance in the anti-causal direction is smaller than that in the causal direction. As an alternative, we find that causal model selection throu
    
[^152]: 无先验因果学习

    Zero-shot causal learning. (arXiv:2301.12292v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12292](http://arxiv.org/abs/2301.12292)

    无先验因果学习是一个解决预测新型干预措施个性化影响的框架，并通过元学习对任务的处理达成了目的，能够将干预措施的知识传输到未见过的干预措施中，并在合成和真实数据集上表现出了优越性能。

    

    在个性化医疗、公共政策和在线营销等领域，预测不同干预措施对特定个体的因果影响非常重要。预测现有干预措施的影响有许多方法，这些方法基于接受过干预措施的个体的历史数据。然而，在许多场景中，预测新型干预措施的影响也很重要，这些方法无法解决。在这里，我们考虑了无先验因果学习：预测新型干预措施的个性化影响。我们提出了CaML，这是一个因果元学习框架，它将每个干预措施的个性化预测效果作为一个任务来进行处理。CaML在数千个任务中训练单一的元模型，每个任务都是通过抽样生成一个干预措施及其接收者和非接收者来构建的。通过利用干预信息（例如，药物的属性）和个体特征（例如，特定个体的医疗记录），CaML学习如何将已观察到的干预措施的知识有效地传输给未见过的干预措施。我们在合成和真实数据集上展示了我们方法的有效性，展示了该方法具有推广到未见过干预措施并胜过现有方法的能力。

    Predicting how different interventions will causally affect a specific individual is important in a variety of domains such as personalized medicine, public policy, and online marketing. There are a large number of methods to predict the effect of an existing intervention based on historical data from individuals who received it. However, in many settings it is important to predict the effects of novel interventions (\emph{e.g.}, a newly invented drug), which these methods do not address. Here, we consider zero-shot causal learning: predicting the personalized effects of a novel intervention. We propose CaML, a causal meta-learning framework which formulates the personalized prediction of each intervention's effect as a task. CaML trains a single meta-model across thousands of tasks, each constructed by sampling an intervention, along with its recipients and nonrecipients. By leveraging both intervention information (\emph{e.g.}, a drug's attributes) and individual features~(\emph{e.g.
    
[^153]: TinyML的可持续性评估：评估机器学习对微控制器的环境影响

    Is TinyML Sustainable? Assessing the Environmental Impacts of Machine Learning on Microcontrollers. (arXiv:2301.11899v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11899](http://arxiv.org/abs/2301.11899)

    TinyML部署了机器学习算法到低成本的微控制器系统上，可以解锁无数始终处于开启状态的机器学习应用，这项新兴技术有助于解决可持续发展挑战，但需要评估和缓解其环境影响以确保可持续性。

    

    持续增长的碳排放和全球垃圾问题引起了人们对环境未来的可持续性关注。快速增长的物联网可能会加剧这一问题。然而，Tiny Machine Learning（TinyML）这一新兴领域有机会通过可持续计算实践来帮助解决这些环境挑战。TinyML是将机器学习（ML）算法部署到低成本、低功耗的微控制器系统上，实现了设备上的传感器分析，从而释放出无数的始终处于开启状态的ML应用。本文讨论了这些TinyML应用应对关键可持续性挑战的潜力，以及这一新兴技术的环境足迹。通过完全的生命周期分析（LCA），我们发现，TinyML系统通过启用减少其他行业排放的应用程序，为抵消碳排放提供了机会。然而，如果不可持续地扩大规模，TinyML的增长可能会给环境造成重大负担。我们得出结论：TinyML社区的利益相关者必须积极评估和缓解这项技术的环境影响，以确保其可持续性。

    The sustained growth of carbon emissions and global waste elicits significant sustainability concerns for our environment's future. The growing Internet of Things (IoT) has the potential to exacerbate this issue. However, an emerging area known as Tiny Machine Learning (TinyML) has the opportunity to help address these environmental challenges through sustainable computing practices. TinyML, the deployment of machine learning (ML) algorithms onto low-cost, low-power microcontroller systems, enables on-device sensor analytics that unlocks numerous always-on ML applications. This article discusses both the potential of these TinyML applications to address critical sustainability challenges, as well as the environmental footprint of this emerging technology. Through a complete life cycle analysis (LCA), we find that TinyML systems present opportunities to offset their carbon emissions by enabling applications that reduce the emissions of other sectors. Nevertheless, when globally scaled, 
    
[^154]: 适用于所有领域的一个模型：基于协作域前缀调整的跨领域实体识别

    One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER. (arXiv:2301.10410v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.10410](http://arxiv.org/abs/2301.10410)

    本论文提出了基于协作域前缀调整的跨领域实体识别，使用文本到文本生成的支撑领域相关指导来将知识转移至新域NER任务，避免了先前的为每个领域结束一个全新的NER模型的问题。

    

    解决实际场景中低资源问题是跨领域实体识别的一个挑战性任务。先前典型的解决方案主要通过使用来自丰富资源领域的数据进行预训练语言模型(PLMs)获得NER模型并将其适应于目标领域。由于不同领域实体类型之间的不匹配问题，先前的方法通常调整所有PLMs的参数，从而为每个领域结束一个全新的NER模型。此外，当前的模型只关注于利用一个普通来源领域中的知识，而未能成功地将来自多个来源领域的知识转移到目标上。为了解决这些问题，我们基于文本到文本生成的PLM引入了协作域前缀调整跨领域NER(CP-NER)。具体来说，我们呈现了用于文本到文本生成的支撑领域相关指导来将知识转移至新域NER任务而无需结构修改。我们利用冻结的PLMs并进行协作域前缀调整。

    Cross-domain NER is a challenging task to address the low-resource problem in practical scenarios. Previous typical solutions mainly obtain a NER model by pre-trained language models (PLMs) with data from a rich-resource domain and adapt it to the target domain. Owing to the mismatch issue among entity types in different domains, previous approaches normally tune all parameters of PLMs, ending up with an entirely new NER model for each domain. Moreover, current models only focus on leveraging knowledge in one general source domain while failing to successfully transfer knowledge from multiple sources to the target. To address these issues, we introduce Collaborative Domain-Prefix Tuning for cross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically, we present text-to-text generation grounding domain-related instructors to transfer knowledge to new domain NER tasks without structural modifications. We utilize frozen PLMs and conduct collaborative domain-prefix tuning
    
[^155]: 针对少样本开放集识别问题的开放集似然最大化方法

    Open-Set Likelihood Maximization for Few-Shot Learning. (arXiv:2301.08390v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.08390](http://arxiv.org/abs/2301.08390)

    本文提出了一种针对少样本开放集识别问题的开放集似然最大化方法，可以在利用未标记查询实例进行推理时提高模型的鲁棒性和准确性。

    

    本文研究了少样本开放集识别问题，即在只有很少标记样本的情况下对一组类别中的实例进行分类，并同时检测不属于任何已知类别的实例。我们探索了流行的传导设置，利用推理时未标记的查询实例。由于现有的传导方法在开放集场景下表现不佳，我们提出了一种广义最大似然原则，其中除了通常的参数模型外，还引入了下调潜在异常值影响的潜在得分。我们的公式从支持集嵌入监督约束和附加惩罚，以防止对查询集的过度自信的预测。我们采用块坐标下降，轮流共同优化潜在得分和参数模型，从而互相受益。我们称之为Open-Set Likelihood Optimization（OSLO），并在少样本开放集基准测试中证明了其与基线方法相比的有效性。

    We tackle the Few-Shot Open-Set Recognition (FSOSR) problem, i.e. classifying instances among a set of classes for which we only have a few labeled samples, while simultaneously detecting instances that do not belong to any known class. We explore the popular transductive setting, which leverages the unlabelled query instances at inference. Motivated by the observation that existing transductive methods perform poorly in open-set scenarios, we propose a generalization of the maximum likelihood principle, in which latent scores down-weighing the influence of potential outliers are introduced alongside the usual parametric model. Our formulation embeds supervision constraints from the support set and additional penalties discouraging overconfident predictions on the query set. We proceed with a block-coordinate descent, with the latent scores and parametric model co-optimized alternately, thereby benefiting from each other. We call our resulting formulation \textit{Open-Set Likelihood Op
    
[^156]: 使用高维传感器反馈的深度强化学习进行灌溉调度

    Deep reinforcement learning for irrigation scheduling using high-dimensional sensor feedback. (arXiv:2301.00899v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.00899](http://arxiv.org/abs/2301.00899)

    本文介绍了一个使用深度强化学习进行灌溉调度的原则性框架和可行的程序，并在澳大利亚一个产出高的地区使用灌溉小麦的案例研究中证明了其有效性。

    

    深度强化学习在许多作物系统中应用于根据多个时间点上的各种测量值自适应地施加水分，有潜力显著改善灌溉调度。本文提出了一个原则性框架和可行的程序，使研究人员可以制定自己的优化问题并基于深度强化学习实现解决方案算法，以加快技术进步。其有效性已在澳大利亚一个产出高的地区使用灌溉小麦的案例研究中得到证明。

    Deep reinforcement learning has considerable potential to improve irrigation scheduling in many cropping systems by applying adaptive amounts of water based on various measurements over time. The goal is to discover an intelligent decision rule that processes information available to growers and prescribes sensible irrigation amounts for the time steps considered. Due to the technical novelty, however, the research on the technique remains sparse and impractical. To accelerate the progress, the paper proposes a principled framework and actionable procedure that allow researchers to formulate their own optimisation problems and implement solution algorithms based on deep reinforcement learning. The effectiveness of the framework was demonstrated using a case study of irrigated wheat grown in a productive region of Australia where profits were maximised. Specifically, the decision rule takes nine state variable inputs: crop phenological stage, leaf area index, extractable soil water for 
    
[^157]: 扩散模型暗中识别数据流形的维度

    Your diffusion model secretly knows the dimension of the data manifold. (arXiv:2212.12611v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.12611](http://arxiv.org/abs/2212.12611)

    本研究提出了一种新的方法，利用扩散模型估算数据流形的维度并且在实验中表现出色。

    

    本研究提出了一种使用训练过的扩散模型估算数据流形维度的新框架。扩散模型逐渐逼近目标分布的梯度，即噪声污染版本的对数密度的梯度，不同级别的污染程度对应不同的梯度。我们证明，如果数据集聚焦于高维环境空间中嵌入的流形，那么随着噪声污染程度的降低，梯度会指向流形，因为这个方向是最大似然增加的方向。因此，在污染程度较低时，扩散模型为我们提供了数据流形正常向量的逼近。这使我们能够估计切空间的维度，也就是数据流形的内在维度。据我们所知，我们的方法是基于扩散模型的数据流形维度的第一个估算器，并且胜过了已经成熟的统计方法。

    In this work, we propose a novel framework for estimating the dimension of the data manifold using a trained diffusion model. A diffusion model approximates the score function i.e. the gradient of the log density of a noise-corrupted version of the target distribution for varying levels of corruption. We prove that, if the data concentrates around a manifold embedded in the high-dimensional ambient space, then as the level of corruption decreases, the score function points towards the manifold, as this direction becomes the direction of maximal likelihood increase. Therefore, for small levels of corruption, the diffusion model provides us with access to an approximation of the normal bundle of the data manifold. This allows us to estimate the dimension of the tangent space, thus, the intrinsic dimension of the data manifold. To the best of our knowledge, our method is the first estimator of the data manifold dimension based on diffusion models and it outperforms well established statis
    
[^158]: 注重视觉、属性和理性：迈向物理安全和可信的人工智能

    Foveate, Attribute, and Rationalize: Towards Physically Safe and Trustworthy AI. (arXiv:2212.09667v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09667](http://arxiv.org/abs/2212.09667)

    研究提出了一种新颖的FARM框架，通过利用外部知识生成能够被信任的原理，解决了不安全文本检测的问题，并能够帮助利益相关者和政策制定者保障消费者的安全。

    

    随着智能系统市场的不断增长，用户的身体安全越来越受到关注。不受限制的系统可能会向用户推荐危险的行为，导致严重的伤害。隐蔽的不安全文本是一个特别关注的领域，因为这样的文本可能会出现在日常场景中，并且很难被检测为有害。我们提出了FARM，这是一个新颖的框架，利用外部知识在安全上下文中生成可信的原理。具体而言，FARM注重于缺失的知识，以确认在特定情境中进行推理所需的信息，并通过可信源进行归因以获取此信息。这些知识用于分类原始文本的安全性并生成人类可解释的原理，揭示系统对特定用户群体的风险，并帮助利益相关者管理其系统的风险，帮助政策制定者为消费者安全提供具体的保障。我们的实验表明，FARM在识别不安全文本和生成可信的原理方面优于现有方法。

    Users' physical safety is an increasing concern as the market for intelligent systems continues to grow, where unconstrained systems may recommend users dangerous actions that can lead to serious injury. Covertly unsafe text is an area of particular interest, as such text may arise from everyday scenarios and are challenging to detect as harmful. We propose FARM, a novel framework leveraging external knowledge for trustworthy rationale generation in the context of safety. In particular, FARM foveates on missing knowledge to qualify the information required to reason in specific scenarios and retrieves this information with attribution to trustworthy sources. This knowledge is used to both classify the safety of the original text and generate human-interpretable rationales, shedding light on the risk of systems to specific user groups and helping both stakeholders manage the risks of their systems and policymakers to provide concrete safeguards for consumer safety. Our experiments show 
    
[^159]: 多语言翻译中干扰的原因和解决方法探究

    Causes and Cures for Interference in Multilingual Translation. (arXiv:2212.07530v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.07530](http://arxiv.org/abs/2212.07530)

    研究探究了多语言机器翻译中干扰的主要因素，通过系统化试验发现使用不到10亿参数的标准Transformer配置可以在很大程度上缓解干扰并促进协同，同时发现调整采样温度以控制数据中每个语言对所占比例的方法是平衡语言对之间关系的关键。

    

    多语言机器翻译模型可以从不同语言对之间的协同中获益，但同时也会受到干扰的影响。虽然目前有越来越多的先进方法旨在消除干扰，但我们对干扰现象的理解仍然有限。本研究确定了导致多语言机器翻译中干扰的主要因素。通过系统化试验，我们发现干扰（或协同）主要由模型大小、数据大小和每个语言对在总数据集中所占比例来决定。我们观察到，当模型相对于可用的训练数据非常小的时候，会出现严重的干扰，而使用不到10亿参数的标准Transformer配置可以在很大程度上缓解干扰并促进协同。此外，我们还展示了通过调整采样温度以控制数据中每个语言对所占比例的方法是平衡语言对之间关系的关键。

    Multilingual machine translation models can benefit from synergy between different language pairs, but also suffer from interference. While there is a growing number of sophisticated methods that aim to eliminate interference, our understanding of interference as a phenomenon is still limited. This work identifies the main factors that contribute to interference in multilingual machine translation. Through systematic experimentation, we find that interference (or synergy) are primarily determined by model size, data size, and the proportion of each language pair within the total dataset. We observe that substantial interference occurs mainly when the model is very small with respect to the available training data, and that using standard transformer configurations with less than one billion parameters largely alleviates interference and promotes synergy. Moreover, we show that tuning the sampling temperature to control the proportion of each language pair in the data is key to balancin
    
[^160]: ERNIE-Code: 超越英语为中心的跨语言编程预训练

    ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages. (arXiv:2212.06742v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.06742](http://arxiv.org/abs/2212.06742)

    ERNIE-Code是一个适用于116种自然语言和6种编程语言的统一预训练语言模型，采用了跨度损坏语言建模和基于桥接的翻译语言建模两种跨语言预训练方法，并在广泛的代码智能终端任务中优于以前的多语言LLMs。

    

    软件工程师使用同一种编程语言可能使用不同的自然语言，这会导致沟通和工作效率的巨大障碍。最近的研究表明，在计算机程序中使用生成式预训练是有效的，然而它们总是以英语为中心。在这项工作中，我们迈出了迈向为大型语言模型（LLM）建立多语言自然语言和多语言编程语言之间桥梁的一步。我们发布了ERNIE-Code，这是一个适用于116种自然语言和6种编程语言的统一预训练语言模型。我们采用两种普遍的跨语言预训练方法：跨度损坏语言建模从单语言自然语言或编程语言中学习模式；基于桥接的翻译语言建模依靠多种自然语言和编程语言的平行数据。广泛的结果表明，ERNIE-Code在代码智能的广泛终端任务中优于以前的多语言LLMs，包括多语言代码到文本，文本到代码，代码到代码和文本到文本的转换。

    Software engineers working with the same programming language (PL) may speak different natural languages (NLs) and vice versa, erecting huge barriers to communication and working efficiency. Recent studies have demonstrated the effectiveness of generative pre-training in computer programs, yet they are always English-centric. In this work, we step towards bridging the gap between multilingual NLs and multilingual PLs for large language models (LLMs). We release ERNIE-Code, a unified pre-trained language model for 116 NLs and 6 PLs. We employ two methods for universal cross-lingual pre-training: span-corruption language modeling that learns patterns from monolingual NL or PL; and pivot-based translation language modeling that relies on parallel data of many NLs and PLs. Extensive results show that ERNIE-Code outperforms previous multilingual LLMs for PL or NL across a wide range of end tasks of code intelligence, including multilingual code-to-text, text-to-code, code-to-code, and text-
    
[^161]: Copula联合预测用于多步时间序列预测

    Copula Conformal Prediction for Multi-step Time Series Forecasting. (arXiv:2212.03281v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.03281](http://arxiv.org/abs/2212.03281)

    本文提出了一种 Copula 联合预测算法 CopulaCPTS，用于多元、多步时间序列预测，经过实验验证，其置信区间比现有技术更精准和更锐利。

    

    精确的不确定性度量是构建强大可靠的机器学习系统的关键步骤。拟合预测是一种流行的无分布不确定性量化算法，因其易于实现、统计覆盖保证和对底层预测算法的多样性而受到欢迎。然而，现有的时间序列置信预测算法仅限于单步预测，未考虑时序依赖。本文提出一种 Copula 联合预测算法，用于多元、多步时间序列预测 CopulaCPTS。我们证明了 CopulaCPTS 具有有限的样本*有效性保证。在多个合成和真实世界的多元时间序列数据集上，我们展示了 CopulaCPTS 的多步预测可产生比现有技术更精准和更锐利的置信区间。

    Accurate uncertainty measurement is a key step to building robust and reliable machine learning systems. Conformal prediction is a distribution-free uncertainty quantification algorithm popular for its ease of implementation, statistical coverage guarantees, and versatility for underlying forecasters. However, existing conformal prediction algorithms for time series are limited to single-step prediction without considering the temporal dependency. In this paper we propose a Copula Conformal Prediction algorithm for multivariate, multi-step Time Series forecasting, CopulaCPTS. We prove that CopulaCPTS has finite sample validity guarantee. On several synthetic and real-world multivariate time series datasets, we show that CopulaCPTS produces more calibrated and sharp confidence intervals for multi-step prediction tasks than existing techniques.
    
[^162]: SODA：一种自然语言处理包，用于提取癌症研究中的社会健康决定因素

    SODA: A Natural Language Processing Package to Extract Social Determinants of Health for Cancer Studies. (arXiv:2212.03000v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.03000](http://arxiv.org/abs/2212.03000)

    本文介绍了一个开源的自然语言处理包SODA，可用于提取癌症患者的社会健康决定因素。该包在泛化能力方面表现良好，可以用于新的疾病领域。研究结果表明，该包在癌症人群中提取SDoH的提取率较高。

    

    本文介绍了一个名为SODA的自然语言处理包，其中含有预训练的转换器模型，可用于提取癌症患者的社会健康决定因素（SDoH），并检验了SODA在新的疾病领域（如使用阿片类药物）的泛化能力，并评估了在癌症人群中提取SDoH的提取率。

    Objective: We aim to develop an open-source natural language processing (NLP) package, SODA (i.e., SOcial DeterminAnts), with pre-trained transformer models to extract social determinants of health (SDoH) for cancer patients, examine the generalizability of SODA to a new disease domain (i.e., opioid use), and evaluate the extraction rate of SDoH using cancer populations.  Methods: We identified SDoH categories and attributes and developed an SDoH corpus using clinical notes from a general cancer cohort. We compared four transformer-based NLP models to extract SDoH, examined the generalizability of NLP models to a cohort of patients prescribed with opioids, and explored customization strategies to improve performance. We applied the best NLP model to extract 19 categories of SDoH from the breast (n=7,971), lung (n=11,804), and colorectal cancer (n=6,240) cohorts.  Results and Conclusion: We developed a corpus of 629 cancer patients notes with annotations of 13,193 SDoH concepts/attribut
    
[^163]: 基于投机解码的Transformer快速推理

    Fast Inference from Transformers via Speculative Decoding. (arXiv:2211.17192v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.17192](http://arxiv.org/abs/2211.17192)

    本文介绍了一种基于投机解码的算法，可以在不更改输出的情况下更快地从大型自回归模型（如Transformer）中采样，加速了现有的模型，而无需重新训练或进行架构更改。

    

    从Transformer等大型自回归模型中进行推理是缓慢的，因为解码K个标记需要运行K次模型。本文介绍了一种名为“投机解码”的算法，它可以在不改变输出的情况下更快地从自回归模型中采样，通过并行计算多个标记实现。我们的方法可以加速现有的模型，而无需重新训练或进行架构更改。我们在T5-XXL上进行了演示，并显示相对于标准T5X实现，其加速了2X-3X，输出相同。

    Inference from large autoregressive models like Transformers is slow decoding K tokens takes K serial runs of the model. In this work we introduce speculative decoding - an algorithm to sample from autoregressive models faster without any changes to the outputs, by computing several tokens in parallel. At the heart of our approach lie the observations that (1) hard language-modeling tasks often include easier subtasks that can be approximated well by more efficient models, and (2) using speculative execution and a novel sampling method, we can make exact decoding from the large models faster, by running them in parallel on the outputs of the approximation models, potentially generating several tokens concurrently, and without changing the distribution. Our method can accelerate existing off-the-shelf models without retraining or architecture changes. We demonstrate it on T5-XXL and show a 2X-3X acceleration compared to the standard T5X implementation, with identical outputs.
    
[^164]: 论反事实推理的复杂性

    On the Complexity of Counterfactual Reasoning. (arXiv:2211.13447v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.13447](http://arxiv.org/abs/2211.13447)

    该研究发现，与在完全指定的结构因果模型上进行关联或干预推理相比，反事实推理的计算复杂性并不更高，两者的复杂性可以通过关于树宽的边界界定得到较好的处理。

    

    我们研究了反事实推理的计算复杂性，以及它与在结构因果模型（SCMs）上进行关联和干预推理的复杂性之间的关系。我们表明，在两个计算框架的背景下，反事实推理并不比在完全指定的SCMs上进行关联或干预推理更难。第一个框架基于树宽的概念，并包括经典的变量消除和联结树算法。第二个框架则基于更近期且更精细的因果树宽概念，针对具有功能依赖性的模型，如SCMs。我们的结果是建设性的，基于界定双网络（用于标准反事实推理，包括现实和想象两种情况）的（因果）树宽，到基础SCM结构的（因果）树宽之间的关系。特别地，我们表明后者（因果）树宽不会超过前者的两倍加一。因此，如果一个。

    We study the computational complexity of counterfactual reasoning in relation to the complexity of associational and interventional reasoning on structural causal models (SCMs). We show that counterfactual reasoning is no harder than associational or interventional reasoning on fully specified SCMs in the context of two computational frameworks. The first framework is based on the notion of treewidth and includes the classical variable elimination and jointree algorithms. The second framework is based on the more recent and refined notion of causal treewidth which is directed towards models with functional dependencies such as SCMs. Our results are constructive and based on bounding the (causal) treewidth of twin networks -- used in standard counterfactual reasoning that contemplates two worlds, real and imaginary -- to the (causal) treewidth of the underlying SCM structure. In particular, we show that the latter (causal) treewidth is no more than twice the former plus one. Hence, if a
    
[^165]: 用随机过程扩散方法将时间数据建模为连续函数

    Modeling Temporal Data as Continuous Functions with Stochastic Process Diffusion. (arXiv:2211.02590v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.02590](http://arxiv.org/abs/2211.02590)

    本文提出了用随机过程扩散方法将时间数据建模为连续函数，并实现了适用于多元概率预测和插补的新颖模型。

    

    时间序列等时间数据可以被看作是潜在函数的离散化测量值。为了为这样的数据建立生成模型，我们必须对控制它的随机过程进行建模。我们提出了一种通过在函数空间中定义去噪扩散模型的解决方案，同时也可以自然地处理不规则采样的数据。前向过程逐渐向函数添加噪声，保持其连续性，而学习到的反向过程则移除噪声并返回函数作为新的样本。为此，我们定义了适当的噪声源，并引入了新颖的去噪和分值匹配模型。我们展示了我们的方法如何用于多元概率预测和插补，以及如何将我们的模型解释为神经过程。

    Temporal data such as time series can be viewed as discretized measurements of the underlying function. To build a generative model for such data we have to model the stochastic process that governs it. We propose a solution by defining the denoising diffusion model in the function space which also allows us to naturally handle irregularly-sampled observations. The forward process gradually adds noise to functions, preserving their continuity, while the learned reverse process removes the noise and returns functions as new samples. To this end, we define suitable noise sources and introduce novel denoising and score-matching models. We show how our method can be used for multivariate probabilistic forecasting and imputation, and how our model can be interpreted as a neural process.
    
[^166]: 在子模最大化中平衡效用和公平性（技术报告）

    Balancing Utility and Fairness in Submodular Maximization (Technical Report). (arXiv:2211.00980v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2211.00980](http://arxiv.org/abs/2211.00980)

    本文提出了一个新的问题，称为“二标准子模最大化”，以平衡效用和公平性。该问题要求找到一个固定大小的解，以最大化效用函数为目标。

    

    子模函数最大化是一个基本的组合优化问题，具有许多应用，包括数据汇总、影响力最大化和推荐等。在许多问题中，目标是找到一解，使得对于每个用户，效用函数是单调子模的情况下，平均效用最大化。然而，当用户群体由几个人口统计学分组组成时，另一个关键问题是效用是否公平地分配在不同的群体中。虽然效用和公平目标都是可取的，但它们可能互相矛盾，并且据我们所知，很少有人关注如何一起优化它们。在本文中，我们提出了一个新问题，称为“二标准子模最大化”（BSM），以在效用和公平性之间取得平衡，具体而言，它要求找到一个固定大小的解，以最大化效用函数为目标。

    Submodular function maximization is a fundamental combinatorial optimization problem with plenty of applications -- including data summarization, influence maximization, and recommendation. In many of these problems, the goal is to find a solution that maximizes the average utility over all users, for each of whom the utility is defined by a monotone submodular function. However, when the population of users is composed of several demographic groups, another critical problem is whether the utility is fairly distributed across different groups. Although the \emph{utility} and \emph{fairness} objectives are both desirable, they might contradict each other, and, to the best of our knowledge, little attention has been paid to optimizing them jointly.  In this paper, we propose a new problem called \emph{Bicriteria Submodular Maximization} (BSM) to strike a balance between utility and fairness. Specifically, it requires finding a fixed-size solution to maximize the utility function, subject
    
[^167]: 集成学习的可微分模型选择

    Differentiable Model Selection for Ensemble Learning. (arXiv:2211.00251v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.00251](http://arxiv.org/abs/2211.00251)

    本文提出了一种可微分模型选择框架，专为集成学习而设计，通过将集成学习任务转化为可微分选择程序，在集成学习模型内端到端地训练学习为特定输入样本选择合适的集成成员，有效性和多功能性均优于传统和先进的共识规则。

    

    模型选择是创造准确和稳健模型的策略。设计这些算法的一个关键挑战是确定任何特定输入样本的最佳分类模型。本文解决了这一挑战，提出了一种新颖的可微分模型选择框架，整合了机器学习和组合优化。该框架专为集成学习而设计，该策略结合了单个预训练模型的输出，并通过将集成学习任务转化为可微分选择程序，在集成学习模型内端到端地训练学习为特定输入样本选择合适的集成成员。在各种任务上测试，所提出的框架展示了其多功能性和有效性，且在各种设置和学习任务中均优于传统和先进的共识规则。

    Model selection is a strategy aimed at creating accurate and robust models. A key challenge in designing these algorithms is identifying the optimal model for classifying any particular input sample. This paper addresses this challenge and proposes a novel framework for differentiable model selection integrating machine learning and combinatorial optimization. The framework is tailored for ensemble learning, a strategy that combines the outputs of individually pre-trained models, and learns to select appropriate ensemble members for a particular input sample by transforming the ensemble learning task into a differentiable selection program trained end-to-end within the ensemble learning model. Tested on various tasks, the proposed framework demonstrates its versatility and effectiveness, outperforming conventional and advanced consensus rules across a variety of settings and learning tasks.
    
[^168]: 用大型语言模型理解HTML

    Understanding HTML with Large Language Models. (arXiv:2210.03945v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.03945](http://arxiv.org/abs/2210.03945)

    本研究使用大型语言模型探索了对HTML的理解，提出了HTML理解模型，通过微调使其在语义分类、描述生成和自主网络导航三个任务上表现出良好的性能，显示出大型语言模型在HTML任务上表现出色。

    

    大型语言模型（LLMs）在各种自然语言任务中表现出了卓越的性能。然而，它们在HTML理解方面的能力——即解析网页的原始HTML，应用于自动化网络任务、爬取和浏览器辅助检索等方面——尚未得到完全的探索。我们提出了HTML理解模型（微调LLMs），并深入分析了它们在三个任务下的能力：（i）HTML元素的语义分类，（ii）HTML输入的描述生成，以及（iii）HTML页面的自主网络导航。虽然先前的工作已经为HTML理解开发了专用的架构和训练程序，但我们表明，预训练于标准自然语言语料库的LLMs非常适用于HTML理解任务。例如，微调后的LLMs在语义分类方面比仅基于任务数据集训练的模型准确率高12%。此外，当它们被微调于MiniW的数据时，LLMs的描述生成在人类主观质量评估中表现出与基于Transformer编码器—解码器的基线模型相当的质量，而且它们能够成功地自主地浏览HTML页面，执行各种任务。

    Large language models (LLMs) have shown exceptional performance on a variety of natural language tasks. Yet, their capabilities for HTML understanding -i.e., parsing the raw HTML of a webpage, with applications to automation of web-based tasks, crawling, and browser-assisted retrieval -- have not been fully explored. We contribute HTML understanding models (fine-tuned LLMs) and an in-depth analysis of their capabilities under three tasks: (i) Semantic Classification of HTML elements, (ii) Description Generation for HTML inputs, and (iii) Autonomous Web Navigation of HTML pages. While previous work has developed dedicated architectures and training procedures for HTML understanding, we show that LLMs pretrained on standard natural language corpora transfer remarkably well to HTML understanding tasks. For instance, fine-tuned LLMs are 12% more accurate at semantic classification compared to models trained exclusively on the task dataset. Moreover, when fine-tuned on data from the MiniW
    
[^169]: 良性自编码器

    Benign Autoencoders. (arXiv:2210.00637v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00637](http://arxiv.org/abs/2210.00637)

    本文正式化了用于生成式人工智能中编码器-解码器对的最佳选择问题并提出了良性自编码器（BAE），BAE能够将数据投射到最优的流型上，实现了数据压缩和更加稳定的梯度下降。

    

    最近，生成式人工智能取得了很多进展，其中常采用编码器-解码器架构来实现数据的高效表示。本论文正式化了寻找最佳编码器-解码器对的数学问题并表征其解决方案，我们将其命名为“良性自编码器”（BAE）。我们证明BAE将数据投射到一个流型上，其维数为生成问题的最佳可压缩维度。我们强调BAE与人工智能中几个最近发展的方向之间的惊人联系，如有条件的GAN，上下文编码器，稳定扩散，堆叠自编码器和生成模型的学习能力。我们展示了BAE如何找到最优的低维潜在表示，从而在分布转移下提高鉴别器的性能。通过压缩“恶性”数据维度，BAE导致梯度更加平滑和稳定。

    Recent progress in Generative Artificial Intelligence (AI) relies on efficient data representations, often featuring encoder-decoder architectures. We formalize the mathematical problem of finding the optimal encoder-decoder pair and characterize its solution, which we name the "benign autoencoder" (BAE). We prove that BAE projects data onto a manifold whose dimension is the optimal compressibility dimension of the generative problem. We highlight surprising connections between BAE and several recent developments in AI, such as conditional GANs, context encoders, stable diffusion, stacked autoencoders, and the learning capabilities of generative models. As an illustration, we show how BAE can find optimal, low-dimensional latent representations that improve the performance of a discriminator under a distribution shift. By compressing "malignant" data dimensions, BAE leads to smoother and more stable gradients.
    
[^170]: 神经积分方程

    Neural Integral Equations. (arXiv:2209.15190v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.15190](http://arxiv.org/abs/2209.15190)

    本文介绍了神经积分方程（NIE）和自注意神经积分方程（ANIE）的方法，它们可以在无监督情况下通过学习数据中的积分算子进行模型建立，并且在合成和真实世界数据的ODE、PDE和IE系统中的基准任务上表现出较高的速度和准确性。

    

    积分方程 (IEs) 是用于建模具有非局部相互作用的时空系统的方程。它们已经在理论和应用科学中找到了重要应用，包括物理学、化学、生物学和工程学。虽然存在有效的算法来解决给定的IEs，但不存在可以仅从数据中学习IE和其相关动态的方法。在本文中，我们介绍了神经积分方程 (NIE)，这种方法通过IE求解器从数据中学习未知的积分算子。我们还介绍了自注意神经积分方程 (ANIE)，其中积分被自注意力替换，这提高了可扩展性、容量，并产生了一个可解释的模型。我们证明(A)NIE在ODE、PDE和IE系统中的多个基准任务上的速度和准确性都优于其他方法，并且适用于合成和真实世界数据。

    Integral equations (IEs) are equations that model spatiotemporal systems with non-local interactions. They have found important applications throughout theoretical and applied sciences, including in physics, chemistry, biology, and engineering. While efficient algorithms exist for solving given IEs, no method exists that can learn an IE and its associated dynamics from data alone. In this paper, we introduce Neural Integral Equations (NIE), a method that learns an unknown integral operator from data through an IE solver. We also introduce Attentional Neural Integral Equations (ANIE), where the integral is replaced by self-attention, which improves scalability, capacity, and results in an interpretable model. We demonstrate that (A)NIE outperforms other methods in both speed and accuracy on several benchmark tasks in ODE, PDE, and IE systems of synthetic and real-world data.
    
[^171]: 关于动态输出反馈的优化景观: 基于线性二次调节器的案例研究

    On the Optimization Landscape of Dynamic Output Feedback: A Case Study for Linear Quadratic Regulator. (arXiv:2209.05042v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.05042](http://arxiv.org/abs/2209.05042)

    本文研究了线性二次调节器中动态输出反馈策略的优化景观，推导了最优变换并证明了当其可观测时静止点的唯一性，从而为使用策略梯度方法解决动态控制器提供了最优性证明。

    

    策略梯度算法的收敛取决于基础最优控制问题的优化景观。通过分析线性二次控制的优化景观，我们可以获得这些算法的理论洞见。然而，现有文献大多只考虑静态全状态或输出反馈策略（控制器）的优化景观。我们研究了线性二次调节器中动态输出反馈策略（简称 dLQR）的更具挑战性的情况，这在实践中普遍存在，但其优化景观相对复杂。我们首先展示了 dLQR 成本如何随动态控制器的坐标变换而变化，然后推导了给定可观控稳定控制器的最优变换。我们的一个核心结果是 dLQR 在可观测时静止点的唯一性，这为使用策略梯度方法解决动态控制器提供了最优性证明。

    The convergence of policy gradient algorithms hinges on the optimization landscape of the underlying optimal control problem. Theoretical insights into these algorithms can often be acquired from analyzing those of linear quadratic control. However, most of the existing literature only considers the optimization landscape for static full-state or output feedback policies (controllers). We investigate the more challenging case of dynamic output-feedback policies for linear quadratic regulation (abbreviated as dLQR), which is prevalent in practice but has a rather complicated optimization landscape. We first show how the dLQR cost varies with the coordinate transformation of the dynamic controller and then derive the optimal transformation for a given observable stabilizing controller. One of our core results is the uniqueness of the stationary point of dLQR when it is observable, which provides an optimality certificate for solving dynamic controllers using policy gradient methods. More
    
[^172]: 风能交易的在线决策制定

    Online Decision Making for Trading Wind Energy. (arXiv:2209.02009v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.02009](http://arxiv.org/abs/2209.02009)

    本文提出了一种在线报价方法，能够在非稳态及不确定参数下实现更好的适应性与显著的经济收益。

    

    我们提出并发展了一种新的算法，在在线学习和优化的框架内，用于在电力市场中交易风能。特别地，我们将梯度下降算法的逐分量自适应变体与最新进展的基于特征的新闻供应商模型相结合。这导致一种在线报价方法，能够利用数据丰富的环境，同时适应能源产生和电力市场的不稳定特征，而且计算负担最小。我们的方法的性能基于几个数值实验进行分析，显示出较好的适应能力和显著的经济收益。

    We propose and develop a new algorithm for trading wind energy in electricity markets, within an online learning and optimization framework. In particular, we combine a component-wise adaptive variant of the gradient descent algorithm with recent advances in the feature-driven newsvendor model. This results in an online offering approach capable of leveraging data-rich environments, while adapting to the nonstationary characteristics of energy generation and electricity markets, also with a minimal computational burden. The performance of our approach is analyzed based on several numerical experiments, showing both better adaptability to nonstationary uncertain parameters and significant economic gains.
    
[^173]: ID和OOD性能在现实世界的数据集中有时是反相关的

    ID and OOD Performance Are Sometimes Inversely Correlated on Real-world Datasets. (arXiv:2209.00613v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.00613](http://arxiv.org/abs/2209.00613)

    本文发现在现实世界数据中，ID性能和OOD性能之间存在反相关关系，提示需要在两者之间进行权衡，单纯关注ID性能可能无法达到最佳性能。

    

    许多研究比较了计算机视觉和NLP模型的分布内（ID）和分布外（OOD）性能。它们报道了频繁的正相关性，并有一些惊人的研究甚至没有观察到反相关，表明必须进行权衡。确定反向模式的可能性很重要，以确定ID性能是否可以作为OOD泛化能力的代理。本文显示了多个数据集中ID性能和OOD性能之间的反相关关系在现实世界数据中确实存在 - 不仅在理论最坏情况下。我们还从理论上解释了即使在最小线性设置中也可以出现这些情况以及为什么以前的研究可能由于模型选择的偏见而忽略了这些情况。我们的观察结果导致的建议与当前文献中的大部分相反。 - 高OOD性能有时需要牺牲ID性能。- 单纯关注ID性能可能无法达到最佳性能。

    Several studies have compared the in-distribution (ID) and out-of-distribution (OOD) performance of models in computer vision and NLP. They report a frequent positive correlation and some surprisingly never even observe an inverse correlation indicative of a necessary trade-off. The possibility of inverse patterns is important to determine whether ID performance can serve as a proxy for OOD generalization capabilities.  This paper shows with multiple datasets that inverse correlations between ID and OOD performance do happen in real-world data - not only in theoretical worst-case settings. We also explain theoretically how these cases can arise even in a minimal linear setting, and why past studies could miss such cases due to a biased selection of models.  Our observations lead to recommendations that contradict those found in much of the current literature. - High OOD performance sometimes requires trading off ID performance. - Focusing on ID performance alone may not lead to optimal
    
[^174]: 基于分散式数据集提炼的边缘资源受限环境下联邦学习

    Federated Learning via Decentralized Dataset Distillation in Resource-Constrained Edge Environments. (arXiv:2208.11311v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.11311](http://arxiv.org/abs/2208.11311)

    本论文介绍了一种名为FedD3的联邦学习框架，通过集成数据集提炼实例仅需要一次通信，与其他联邦学习方法相比，在需要通信的数据量方面表现显著更好，同时通过平衡准确性和通信成本来适应使用场景。

    

    在联邦学习中，所有的联网客户端协作地进行模型训练。然而，随着模型大小的增加，即使在迭代通信中共享已训练的部分模型，也往往会导致底层网络中的严重通信瓶颈。本文介绍了一种联邦学习框架FedD3，它只需要一次通信，并集成了数据集提炼实例。FedD3不同于其他联邦学习方法中的共享模型更新，它允许连接的客户端独立地提炼本地数据集，然后从网络中聚合那些分散的提炼数据集（例如，一些无法识别的图像）并用于模型训练。我们的实验结果表明，与其他联邦学习方法相比，FedD3在需要通信的数据量方面表现显著更好，同时它能够在使用场景中平衡准确性和通信成本。

    In federated learning, all networked clients contribute to the model training cooperatively. However, with model sizes increasing, even sharing the trained partial models often leads to severe communication bottlenecks in underlying networks, especially when communicated iteratively. In this paper, we introduce a federated learning framework FedD3 requiring only one-shot communication by integrating dataset distillation instances. Instead of sharing model updates in other federated learning approaches, FedD3 allows the connected clients to distill the local datasets independently, and then aggregates those decentralized distilled datasets (e.g. a few unrecognizable images) from networks for model training. Our experimental results show that FedD3 significantly outperforms other federated learning frameworks in terms of needed communication volumes, while it provides the additional benefit to be able to balance the trade-off between accuracy and communication cost, depending on usage sc
    
[^175]: 隐马尔可夫模型的代数约简

    Algebraic Reduction of Hidden Markov Models. (arXiv:2208.05968v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.05968](http://arxiv.org/abs/2208.05968)

    本论文提出了两种算法用于将隐马尔可夫模型简化为维度更小的模型，并精确复现其边缘分布，首次扩展了实现理论工具到这个应用领域。

    

    通过使用系统理论方法，利用适当的概率空间的代数表示，我们解决了将隐马尔可夫模型(HMM)简化为一个更小维度的问题，以完全再现相同边缘的问题。我们提出了两种算法，通过随机投影算子返回粗粒度等效的HMM: 第一种返回精确复制给定输出过程的单时间分布的模型，而第二种则保留完整(多时间)分布。该简化方法不仅利用了观测输出的结构，而且还利用其初始条件，只要后者已知或属于给定的子类。对于可观测类隐马尔可夫模型，推导出了最优算法。

    The problem of reducing a Hidden Markov Model (HMM) to one of smaller dimension that exactly reproduces the same marginals is tackled by using a system-theoretic approach. Realization theory tools are extended to HMMs by leveraging suitable algebraic representations of probability spaces. We propose two algorithms that return coarse-grained equivalent HMMs obtained by stochastic projection operators: the first returns models that exactly reproduce the single-time distribution of a given output process, while in the second the full (multi-time) distribution is preserved. The reduction method exploits not only the structure of the observed output, but also its initial condition, whenever the latter is known or belongs to a given subclass. Optimal algorithms are derived for a class of HMM, namely observable ones.
    
[^176]: 利用归一化流进行稀有事件采样的条件化方法

    Conditioning Normalizing Flows for Rare Event Sampling. (arXiv:2207.14530v2 [physics.comp-ph] UPDATED)

    [http://arxiv.org/abs/2207.14530](http://arxiv.org/abs/2207.14530)

    该论文提出了一种基于神经网络生成配置的转换路径采样方案，使用归一化流消除采样路径之间的相关性，易于并行化采样过程，通过条件设置将配置采样引导到感兴趣的区域。

    

    理解复杂分子过程的动力学通常涉及研究长寿命稳定态之间的不经常转换。通常采用的稀有事件采样方法是使用轨迹空间中的随机游走生成转换路径的集合。但是，这种方法存在随后采样路径之间的强相关性以及在并行化采样过程中存在固有难度。我们提出了一种基于神经网络生成配置的转换路径采样方案。这些配置是通过归一化流获得的，这是一种神经网络类，能够从给定的分布生成统计独立的样本。采用这种方法，不仅消除了所访问路径之间的相关性，而且采样过程易于并行化。此外，通过对归一化流进行条件设置，可以将配置的采样引导到感兴趣的区域。我们演示了这种方法使得...

    Understanding the dynamics of complex molecular processes is often linked to the study of infrequent transitions between long-lived stable states. The standard approach to the sampling of such rare events is to generate an ensemble of transition paths using a random walk in trajectory space. This, however, comes with the drawback of strong correlations between subsequently sampled paths and with an intrinsic difficulty in parallelizing the sampling process. We propose a transition path sampling scheme based on neural-network generated configurations. These are obtained employing normalizing flows, a neural network class able to generate statistically independent samples from a given distribution. With this approach, not only are correlations between visited paths removed, but the sampling process becomes easily parallelizable. Moreover, by conditioning the normalizing flow, the sampling of configurations can be steered towards regions of interest. We show that this approach enables the
    
[^177]: 基于处理内存系统的机器学习训练的实验评估

    An Experimental Evaluation of Machine Learning Training on a Real Processing-in-Memory System. (arXiv:2207.07886v2 [cs.AR] UPDATED)

    [http://arxiv.org/abs/2207.07886](http://arxiv.org/abs/2207.07886)

    该研究评估了在处理内存系统上训练机器学习算法的潜能，并证明基于PIM的ML训练实现了显着的加速和能量效率。

    

    训练机器学习算法是一种计算密集型的过程，由于不断访问大型训练数据集，这种过程通常会受到内存限制。因此，以处理器为中心的系统（例如CPU，GPU）在内存单元和处理单元之间的数据传输方面存在昂贵的瓶颈，这会消耗大量的能量和执行周期。具有处理内存（PIM）功能的内存中心计算系统可以缓解这种数据移动瓶颈。我们的目标是了解现代通用PIM架构加速ML训练的潜力。为此，我们（1）在实际通用PIM架构上实现了几种代表性的传统ML算法（即线性回归，逻辑回归，决策树，K-Means聚类），（2）严格评估和表征这些算法的准确性，性能和扩展性，并且（3）与它们在CPU和GPU上的相应实现进行比较。我们在实际内存中心计算平台上的评估表明，与相应的CPU和GPU方法相比，基于PIM的ML训练实现了显着的加速和能量效率。

    Training machine learning (ML) algorithms is a computationally intensive process, which is frequently memory-bound due to repeatedly accessing large training datasets. As a result, processor-centric systems (e.g., CPU, GPU) suffer from costly data movement between memory units and processing units, which consumes large amounts of energy and execution cycles. Memory-centric computing systems, i.e., with processing-in-memory (PIM) capabilities, can alleviate this data movement bottleneck.  Our goal is to understand the potential of modern general-purpose PIM architectures to accelerate ML training. To do so, we (1) implement several representative classic ML algorithms (namely, linear regression, logistic regression, decision tree, K-Means clustering) on a real-world general-purpose PIM architecture, (2) rigorously evaluate and characterize them in terms of accuracy, performance and scaling, and (3) compare to their counterpart implementations on CPU and GPU. Our evaluation on a real mem
    
[^178]: 在医学领域中实用联邦学习的探索

    Towards the Practical Utility of Federated Learning in the Medical Domain. (arXiv:2207.03075v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.03075](http://arxiv.org/abs/2207.03075)

    本研究提出了应用联邦学习于医学领域的实用指南，包括三个具有代表性的医学数据集的实验，旨在提高医保业的数据效率，并形成适用于全行业的标准。

    

    联邦学习（FL）是一个活跃的研究领域。医学领域是采用FL的最适合领域之一，因为必须尊重患者隐私。然而，以往的研究并没有提供在医学领域中应用FL的实用指南。本文针对三个代表性的医学数据集，即长期的电子健康记录、皮肤癌图像和心电图信号，提出经验基准和实验设置。潜在的FL用户，如医疗机构和IT公司，可以将这些基准作为采用FL的指南，并尽可能减少试错。对于每个数据集，每个客户端数据来自不同的来源，以保留现实世界的异质性。我们评估了六种针对客户端数据异质性问题的FL算法，以及一种将两种典型FL算法的优点结合起来的混合算法。基于三种类型数据的实验结果，我们发现简单的FL算法可以达到与更复杂算法相当的性能。我们的工作为医疗机构和IT公司提供了在安全高效的方式下，应用FL从而改善医疗保健的实用指南。

    Federated learning (FL) is an active area of research. One of the most suitable areas for adopting FL is the medical domain, where patient privacy must be respected. Previous research, however, does not provide a practical guide to applying FL in the medical domain. We propose empirical benchmarks and experimental settings for three representative medical datasets with different modalities: longitudinal electronic health records, skin cancer images, and electrocardiogram signals. The likely users of FL such as medical institutions and IT companies can take these benchmarks as guides for adopting FL and minimize their trial and error. For each dataset, each client data is from a different source to preserve real-world heterogeneity. We evaluate six FL algorithms designed for addressing data heterogeneity among clients, and a hybrid algorithm combining the strengths of two representative FL algorithms. Based on experiment results from three modalities, we discover that simple FL algorith
    
[^179]: 基于置信泰森堡分配的伪标签方法

    Confident Sinkhorn Allocation for Pseudo-Labeling. (arXiv:2206.05880v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.05880](http://arxiv.org/abs/2206.05880)

    本文提出了一种基于置信泰森堡分配的伪标签方法，通过最优传输仅对高置信度样本进行伪标签分配，在半监督学习方面取得了目前最好的表现。

    

    半监督学习是减少机器学习对标记数据依赖的重要工具。它通过利用预训练模型或数据增强的内在空间和语义结构成功应用于结构化数据，例如图像和自然语言。然而，当数据没有适当的结构或不变性时，这些方法不适用。由于其简单性，伪标签（PL）方法可以在没有任何领域假设的情况下广泛使用。然而，PL 对阈值敏感，如果由于过度自信导致错误分配，则可能表现不佳。本文从理论上研究了不确定性对伪标签的作用，并提出了基于置信泰森堡分配的方法(CSA)，通过最优传输仅对高置信度样本进行伪标签分配。CSA 在半监督学习这一实际重要领域中胜过了当前的最先进方法。此外，所提出的方法具有可扩展性，高效性和易实现性。

    Semi-supervised learning is a critical tool in reducing machine learning's dependence on labeled data. It has been successfully applied to structured data, such as images and natural language, by exploiting the inherent spatial and semantic structure therein with pretrained models or data augmentation. These methods are not applicable, however, when the data does not have the appropriate structure, or invariances. Due to their simplicity, pseudo-labeling (PL) methods can be widely used without any domain assumptions. However, PL is sensitive to a threshold and can perform poorly if wrong assignments are made due to overconfidence. This paper studies theoretically the role of uncertainty to pseudo-labeling and proposes Confident Sinkhorn Allocation (CSA), which identifies the best pseudo-label allocation via optimal transport to only samples with high confidence scores. CSA outperforms the current state-of-the-art in this practically important area of semi-supervised learning. Additiona
    
[^180]: DELTA: 多样化客户抽样用于快速联邦学习

    DELTA: Diverse Client Sampling for Fasting Federated Learning. (arXiv:2205.13925v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.13925](http://arxiv.org/abs/2205.13925)

    DELTA 提出了一个无偏抽样方案来减少部分客户端参与所引起的方差，以缓解现有抽样方法可能导致性能下降的问题，它考虑了客户端的多样性和局部方差的影响，并选择具有全局模型更新所需有价值信息的代表性客户端。实验结果表明，DELTA 可以优于其他无偏抽样方案并加速模型收敛速度。

    

    在联邦学习中，部分客户端参与已被广泛应用以高效减少通信负担。然而，不合适的客户端抽样方案可能导致选择出不具代表性子集，从而导致模型更新的显著方差和收敛速度的减慢。现有的抽样方法可能会有偏差，或者可以进一步优化以实现更快的收敛。本文介绍了 DELTA，这是一种设计用于缓解这些问题的无偏抽样方案。DELTA 刻画了客户端的多样性和局部方差的影响，并选择具有全局模型更新所需有价值信息的代表性客户端。此外，DELTA 是一种经过证明的最优无偏抽样方案，在减少由部分客户端参与引起的方差方面优于其他无偏抽样方案。此外，为解决全客户端梯度依赖性，我们提供了一个实用版本的 DELTA，它取决于可用客户端的信息。

    Partial client participation has been widely adopted in Federated Learning (FL) to reduce the communication burden efficiently. However, an inadequate client sampling scheme can lead to the selection of unrepresentative subsets, resulting in significant variance in model updates and slowed convergence. Existing sampling methods are either biased or can be further optimized for faster convergence.In this paper, we present DELTA, an unbiased sampling scheme designed to alleviate these issues. DELTA characterizes the effects of client diversity and local variance, and samples representative clients with valuable information for global model updates. In addition, DELTA is a proven optimal unbiased sampling scheme that minimizes variance caused by partial client participation and outperforms other unbiased sampling schemes in terms of convergence. Furthermore, to address full-client gradient dependence,we provide a practical version of DELTA depending on the available clients' information, 
    
[^181]: 多智能体动力学的概率对称性

    Probabilistic Symmetry for Multi-Agent Dynamics. (arXiv:2205.01927v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.01927](http://arxiv.org/abs/2205.01927)

    该论文提出了PECCO模型，通过利用多智能体间的对称性和能量评分规则，可以更准确地预测多智能体轨迹并量化不确定性，为下游决策提供重要支持。

    

    学习多智能体动态是人工智能的核心问题，广泛应用于机器人和自主驾驶等领域。多数现有的研究侧重于确定性预测，而产生概率预测以量化不确定性与评估风险方面对下游决策制定任务，如运动规划与避碰至关重要。多智能体动态通常包含内部对称性。通过利用对称性，特别是旋转等变性，我们不仅可以提高预测精度，还可以改善不确定性校准。我们引入能量评分作为适当的评分规则来评估概率预测表现。我们提出了一种新颖的深度动力学模型，即概率等变连续卷积（PECCO），用于多智能体轨迹的概率预测。PECCO将等变的连续卷积扩展到多智能体的联合速度分布建模上。它使用动态积分将不确定性从速度传播到位置。我们在合成和现实场景的结果中验证了PECCO模型的性能。

    Learning multi-agent dynamics is a core AI problem with broad applications in robotics and autonomous driving. While most existing works focus on deterministic prediction, producing probabilistic forecasts to quantify uncertainty and assess risks is critical for downstream decision-making tasks such as motion planning and collision avoidance. Multi-agent dynamics often contains internal symmetry. By leveraging symmetry, specifically rotation equivariance, we can improve not only the prediction accuracy but also uncertainty calibration. We introduce Energy Score, a proper scoring rule, to evaluate probabilistic predictions. We propose a novel deep dynamics model, Probabilistic Equivariant Continuous COnvolution (PECCO) for probabilistic prediction of multi-agent trajectories. PECCO extends equivariant continuous convolution to model the joint velocity distribution of multiple agents. It uses dynamics integration to propagate the uncertainty from velocity to position. On both synthetic a
    
[^182]: 基于$\phi$-离散度的分布鲁棒贝叶斯优化

    Distributionally Robust Bayesian Optimization with $\phi$-divergences. (arXiv:2203.02128v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.02128](http://arxiv.org/abs/2203.02128)

    本研究提出了一种基于$\phi$-离散度的分布鲁棒贝叶斯优化算法。

    

    鲁棒性研究因其在面对不确定性的许多系统中不可避免而受到广泛关注。其中一个例子是贝叶斯优化，它面临着多方面的不确定性，但仅有少量的研究致力于这个方向。在现有研究的基础上，我们提出了一种基于$\phi$-离散度的分布鲁棒贝叶斯优化算法。

    The study of robustness has received much attention due to its inevitability in data-driven settings where many systems face uncertainty. One such example of concern is Bayesian Optimization (BO), where uncertainty is multi-faceted, yet there only exists a limited number of works dedicated to this direction. In particular, there is the work of Kirschner et al. (2020), which bridges the existing literature of Distributionally Robust Optimization (DRO) by casting the BO problem from the lens of DRO. While this work is pioneering, it admittedly suffers from various practical shortcomings such as finite contexts assumptions, leaving behind the main question Can one devise a computationally tractable algorithm for solving this DRO-BO problem? In this work, we tackle this question to a large degree of generality by considering robustness against data-shift in $\phi$-divergences, which subsumes many popular choices, such as the $\chi^2$-divergence, Total Variation, and the extant Kullback-Lei
    
[^183]: Transformer更加稳健吗？面向Transformer的确切稳健性验证

    Are Transformers More Robust? Towards Exact Robustness Verification for Transformers. (arXiv:2202.03932v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.03932](http://arxiv.org/abs/2202.03932)

    本文研究了基于Sparsemax的Transformers的稳健性问题，并发现Transformer不一定比传统的多层感知器更加稳健，这对于选择适用于安全关键领域应用的NN架构方面有深刻的考虑。

    

    作为新兴的神经网络类型，Transformer被应用于众多领域，从自然语言处理到自动驾驶。本论文研究Transformers的稳健性问题，这是一个关键特性，低稳健性可能会引起安全问题。具体而言，我们关注基于Sparsemax的Transformers，将找到它们的最大稳健性降低为一个混合整数二次约束编程（MIQCP）问题。我们还设计了两个可嵌入MIQCP编码并大幅加速其求解的预处理启发式方法。然后，我们使用Land Departure Warning应用程序进行实验，比较基于Sparsemax的Transformers与更传统的多层感知器（MLP）神经网络的稳健性。令我们惊讶的是，Transformer并不一定更加稳健，这引发了在选择适用于安全关键领域应用的NN架构方面的深刻考虑。

    As an emerging type of Neural Networks (NNs), Transformers are used in many domains ranging from Natural Language Processing to Autonomous Driving. In this paper, we study the robustness problem of Transformers, a key characteristic as low robustness may cause safety concerns. Specifically, we focus on Sparsemax-based Transformers and reduce the finding of their maximum robustness to a Mixed Integer Quadratically Constrained Programming (MIQCP) problem. We also design two pre-processing heuristics that can be embedded in the MIQCP encoding and substantially accelerate its solving. We then conduct experiments using the application of Land Departure Warning to compare the robustness of Sparsemax-based Transformers against that of the more conventional Multi-Layer-Perceptron (MLP) NNs. To our surprise, Transformers are not necessarily more robust, leading to profound considerations in selecting appropriate NN architectures for safety-critical domain applications.
    
[^184]: 抗相关噪声注入用于提高泛化性能

    Anticorrelated Noise Injection for Improved Generalization. (arXiv:2202.02831v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.02831](http://arxiv.org/abs/2202.02831)

    本文发现，在一些目标函数中，抗相关噪声的梯度下降方法比传统的梯度下降和常规扰动梯度下降有更好的泛化性能。理论分析证明了这是因为 Anti-PGD 能够移动到更宽的最小值点，而 GD 和 PGD 会停滞在次优区域甚至发散。

    

    将人工噪声注入梯度下降常常被用于改善机器学习模型的性能。通常，这种扰动的梯度下降方法使用的是不相关的噪声。然而，目前尚不清楚是否使用不同类型的噪声能够提供更好的泛化性能。本文聚焦于相关的扰动。我们研究了各种目标函数，发现带有抗相关扰动的梯度下降（"Anti-PGD"）比传统的梯度下降和常规的（不相关的）扰动梯度下降有着更好的泛化性能。为了支持这些实验结果，我们还进行了理论分析，证明了 Anti-PGD 能够移动到更宽的最小值点，而 GD 和 PGD 会停滞在次优区域甚至发散。这一新颖的抗相关噪声与泛化性能的联系为训练机器学习模型提供了新的方法。

    Injecting artificial noise into gradient descent (GD) is commonly employed to improve the performance of machine learning models. Usually, uncorrelated noise is used in such perturbed gradient descent (PGD) methods. It is, however, not known if this is optimal or whether other types of noise could provide better generalization performance. In this paper, we zoom in on the problem of correlating the perturbations of consecutive PGD steps. We consider a variety of objective functions for which we find that GD with anticorrelated perturbations ("Anti-PGD") generalizes significantly better than GD and standard (uncorrelated) PGD. To support these experimental findings, we also derive a theoretical analysis that demonstrates that Anti-PGD moves to wider minima, while GD and PGD remain stuck in suboptimal regions or even diverge. This new connection between anticorrelated noise and generalization opens the field to novel ways to exploit noise for training machine learning models.
    
[^185]: 关于NVM交叉型存储器上对抗训练网络的噪声稳定性和鲁棒性

    On the Noise Stability and Robustness of Adversarially Trained Networks on NVM Crossbars. (arXiv:2109.09060v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.09060](http://arxiv.org/abs/2109.09060)

    本文研究结合对抗训练和NVM交叉型存储器内在鲁棒性的设计方法，探索如何设计鲁棒的DNN。对网络未受干扰输入数据下的噪声稳定性进行了研究，并发现对抗训练的网络具有更低的S值。

    

    深度神经网络应用在过去的十年中呈指数级增长。为了满足这些网络日益增长的计算需求，提出了几种基于非易失性存储器（NVM）交叉型存储器的加速器。最近的研究表明，除了显著提高能效和性能外，这种近似硬件还具有内在的鲁棒性，可以抵御对抗攻击。先前的研究针对未受干扰输入数据训练的基本DNN对其内在鲁棒性进行了量化。然而，对抗训练DNN已成为鲁棒性的基准技术，仅依靠硬件的内在鲁棒性可能不足以应对此类攻击。本文通过将对抗训练和NVM交互式存储器模拟硬件的内在鲁棒性相结合，探索了鲁棒DNN的设计。首先，我们研究了这些网络在未受干扰输入下的噪声稳定性，并观察到对抗训练的网络的内部激活拥有更低的S

    Applications based on Deep Neural Networks (DNNs) have grown exponentially in the past decade. To match their increasing computational needs, several Non-Volatile Memory (NVM) crossbar based accelerators have been proposed. Recently, researchers have shown that apart from improved energy efficiency and performance, such approximate hardware also possess intrinsic robustness for defense against adversarial attacks. Prior works quantified this intrinsic robustness for vanilla DNNs trained on unperturbed inputs. However, adversarial training of DNNs is the benchmark technique for robustness, and sole reliance on intrinsic robustness of the hardware may not be sufficient. In this work, we explore the design of robust DNNs through the amalgamation of adversarial training and intrinsic robustness of NVM crossbar-based analog hardware. First, we study the noise stability of such networks on unperturbed inputs and observe that internal activations of adversarially trained networks have lower S
    
[^186]: 众包PAC学习中的半验证方法

    Semi-verified PAC Learning from the Crowd. (arXiv:2106.07080v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.07080](http://arxiv.org/abs/2106.07080)

    本文提出了一种同样具有显著挑战性的半验证模型，在该模型下，即使大多数工人的行为是对抗的，并且其余的人会像Massart噪声一样工作，但众包PAC学习阈值函数的假设类仍然是可行的，并且标注成本可以通过比较查询大大减少。

    

    本文研究了阈值函数的众包PAC学习问题。在一般假设下，只有在有显著比例的工人是完美的情况下，才能确立具有查询效率的算法。本文探讨了更具挑战性的情况——大多数人可能会行事对抗，并且剩下的人会像Massart噪声一样，从而得到了更普遍的打破完美性假设的结果。我们展示了在Charikar等人提出的半验证模型下，只要有(有限的)访问受信任的oracle就能够对基本的假设类进行PAC学习，并且能够在可管理的标签查询量下实现。此外，我们还表明可以通过更容易获得的比较查询来极大地减少标记成本。我们的PAC保证与半验证或列表可解决学习中最近的发展不同，这些发展关键地依赖于数据分布假设。

    We study the problem of crowdsourced PAC learning of threshold functions. This is a challenging problem and only recently have query-efficient algorithms been established under the assumption that a noticeable fraction of the workers are perfect. In this work, we investigate a more challenging case where the majority may behave adversarially and the rest behave as the Massart noise - a significant generalization of the perfectness assumption. We show that under the {semi-verified model} of Charikar et al. (2017), where we have (limited) access to a trusted oracle who always returns correct annotations, it is possible to PAC learn the underlying hypothesis class with a manageable amount of label queries. Moreover, we show that the labeling cost can be drastically mitigated via the more easily obtained comparison queries. Orthogonal to recent developments in semi-verified or list-decodable learning that crucially rely on data distributional assumptions, our PAC guarantee holds by explori
    
[^187]: 一份用于广泛评估的多中心息肉检测和分割数据集

    A multi-centre polyp detection and segmentation dataset for generalisability assessment. (arXiv:2106.04463v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2106.04463](http://arxiv.org/abs/2106.04463)

    这是一个由六个医疗中心提供的包含超过300名患者数据的多中心息肉检测和分割数据集，具有像素级分割和详细的息肉标注，可用于严格测试自动化的息肉检测和分割方法。

    

    结肠息肉是结肠镜检查识别的已知癌症前体。虽然大多数息肉是良性的，但息肉的数量、大小和表面结构与结肠癌的风险相关。已经开发了几种自动化息肉检测和分割方法。然而，主要问题在于它们尚未在大型多中心专门构建的数据集上进行严格测试，其中一个原因是缺乏综合公共数据集。因此，开发的方法可能无法推广到不同的人群数据集。为此，我们从六个独特的中心策划了一个数据集，涵盖了300多名患者。该数据集包括单帧和序列数据，具有3762个注释的息肉标签，精确勾画了六位资深胃肠病学家验证的息肉边界。据我们所知，这是由计算科学家团队策划的最全面的检测和像素级分割数据集（称为PolypGen）。

    Polyps in the colon are widely known cancer precursors identified by colonoscopy. Whilst most polyps are benign, the polyp's number, size and surface structure are linked to the risk of colon cancer. Several methods have been developed to automate polyp detection and segmentation. However, the main issue is that they are not tested rigorously on a large multicentre purpose-built dataset, one reason being the lack of a comprehensive public dataset. As a result, the developed methods may not generalise to different population datasets. To this extent, we have curated a dataset from six unique centres incorporating more than 300 patients. The dataset includes both single frame and sequence data with 3762 annotated polyp labels with precise delineation of polyp boundaries verified by six senior gastroenterologists. To our knowledge, this is the most comprehensive detection and pixel-level segmentation dataset (referred to as \textit{PolypGen}) curated by a team of computational scientists 
    
[^188]: 一种轻量级且梯度稳定的神经层

    A Lightweight and Gradient-Stable Nerual Layer. (arXiv:2106.04088v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.04088](http://arxiv.org/abs/2106.04088)

    Han层是一种梯度稳定、参数更少的神经层结构，可以替换全连接层来优化神经网络模型。

    

    我们提出了一种基于Householder权重和绝对值激活的神经层结构，因此被称为Householder-absolute神经层或简称Han层。与具有$d$个神经元和$d$个输出的全连接层相比，Han层将参数数量和相应的复杂度从$O（d ^ 2）$降低到$O（d）$。Han层结构保证了两个理想属性：（1）梯度稳定性（不会出现梯度消失或梯度爆炸），以及（2）1-Lipschitz连续性。广泛的数值实验表明，可以有策略地使用Han层替换全连接（FC）层，从而减少模型参数的数量，同时保持或甚至提高泛化性能。我们将展示Han层结构在一些小型化的模型上的能力，同时讨论其当前的限制。

    We propose a neural-layer architecture based on Householder weighting and absolute-value activating, hence called Householder-absolute neural layer or simply Han-layer. Compared to a fully-connected layer with $d$-neurons and $d$ outputs, a Han-layer reduces the number of parameters and the corresponding complexity from $O(d^2)$ to $O(d)$. The Han-layer structure guarantees two desirable properties: (1) gradient stability (free of vanishing or exploding gradient), and (2) 1-Lipschitz continuity. Extensive numerical experiments show that one can strategically use Han-layers to replace fully-connected (FC) layers, reducing the number of model parameters while maintaining or even improving the generalization performance. We will showcase the capabilities of the Han-layer architecture on a few small stylized models, and also discuss its current limitations.
    

