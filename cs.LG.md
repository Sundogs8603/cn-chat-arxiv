# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Learning Generalized Policies for Fully Observable Non-Deterministic Planning Domains](https://arxiv.org/abs/2404.02499) | 本研究扩展了学习完全可观察、非确定性计划领域的泛化策略的方法，并通过实验评估了在一些 FOND 计划基准领域中产生的泛化策略的正确性。 |
| [^2] | [Conjugate-Gradient-like Based Adaptive Moment Estimation Optimization Algorithm for Deep Learning](https://arxiv.org/abs/2404.01714) | 提出一种基于共轭梯度样式的新优化算法CG-like-Adam，用于深度学习，并在收敛分析和数值实验中展示了其优越性 |
| [^3] | [DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning](https://arxiv.org/abs/2403.14421) | 开发了第一个差分隐私检索增强生成算法，能够在生成高质量图像样本的同时提供可证明的隐私保证 |
| [^4] | [Impact Assessment of Missing Data in Model Predictions for Earth Observation Applications](https://arxiv.org/abs/2403.14297) | 本研究评估了在地球观测应用中缺失数据对训练模型的影响，发现集成策略可以实现高达100%的预测稳健性，同时揭示了缺失情景在回归任务中比分类任务更具挑战性，且光学视角是最关键的。 |
| [^5] | [STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model](https://arxiv.org/abs/2403.12418) | STG-Mamba 是首个利用选择性状态空间模型进行时空图学习的研究，将STG网络视为系统，并采用图选择性状态空间模块（GS3B）精确表征STG的动态演化。 |
| [^6] | [DTOR: Decision Tree Outlier Regressor to explain anomalies](https://arxiv.org/abs/2403.10903) | DTOR是一种决策树异常值回归器，通过估计异常检测模型生成的异常分数来产生基于规则的解释，具有鲁棒性，适用于具有大量特征数据集。 |
| [^7] | [A Framework for Strategic Discovery of Credible Neural Network Surrogate Models under Uncertainty](https://arxiv.org/abs/2403.08901) | 提出了一个名为Occam Plausibility Algorithm for surrogate models (OPAL-surrogate)的框架，通过基于神经网络的代理模型，采用层次贝叶斯推理和模型验证测试来评估代理模型的可信度和预测可靠性。 |
| [^8] | [C2P-GCN: Cell-to-Patch Graph Convolutional Network for Colorectal Cancer Grading](https://arxiv.org/abs/2403.04962) | C2P-GCN提出了一种新颖的细胞到补丁图卷积网络方法，通过两阶段图形成，在第一阶段形成补丁级图，第二阶段形成图像级图，从而更好地捕捉结直肠癌组织结构信息。 |
| [^9] | [Learning to Defer to a Population: A Meta-Learning Approach](https://arxiv.org/abs/2403.02683) | 通过元学习，本研究提出一种学习推迟对人群的方法，该方法可以在测试时适应前所未见的专家，从而更好地面对困难决策。 |
| [^10] | [Physics-constrained polynomial chaos expansion for scientific machine learning and uncertainty quantification](https://arxiv.org/abs/2402.15115) | 提出一种物理约束的多项式混沌展开方法，将科学机器学习与不确定性量化无缝集成，有效地实现SciML任务中的不确定性量化和在UQ任务中利用SciML提高不确定性评估。 |
| [^11] | [ACE : Off-Policy Actor-Critic with Causality-Aware Entropy Regularization](https://arxiv.org/abs/2402.14528) | 该论文提出了ACE算法，通过引入因果感知熵正则化，有效评估不同行为的重要性，并分析梯度休眠现象，引入休眠引导复位机制，在多个连续控制任务中取得显著性能优势。 |
| [^12] | [HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement Learning Framework for Complex Environments](https://arxiv.org/abs/2402.10228) | HyperAgent提出了一种简单、高效、可扩展的强化学习框架，在复杂环境下能够实现高效的计算和数据选择，是首个达到可证明可扩展的每步计算复杂度以及次线性后悔的方法。 |
| [^13] | [Human Curriculum Effects Emerge with In-Context Learning in Neural Networks](https://arxiv.org/abs/2402.08674) | 人类学习对示例课程和任务结构有敏感性。研究发现，在神经网络和语言模型中，通过上下文学习方法可以同时获得分组和交错训练的优势。 |
| [^14] | [A systematic investigation of learnability from single child linguistic input](https://arxiv.org/abs/2402.07899) | 我们的研究探索了用单个儿童的语言输入训练语言模型的可学习性，我们发现这种设置下的语言模型能够形成句法和语义词群，并对某些语言现象具有敏感性。 |
| [^15] | [Hydragen: High-Throughput LLM Inference with Shared Prefixes](https://arxiv.org/abs/2402.05099) | Hydragen是一种具有共享前缀的高吞吐量LLM推理方法，通过将注意力计算分解为共享前缀和唯一后缀，来提高推理效率，并能够提高端到端LLM吞吐量多达32倍。 |
| [^16] | [Fast Timing-Conditioned Latent Audio Diffusion](https://arxiv.org/abs/2402.04825) | 本研究提出了一种名为Stable Audio的生成模型，该模型利用潜在扩散和条件化技术，实现了高效生成44.1kHz立体声音乐和音效。与其他最先进的模型不同，Stable Audio能够生成具有结构和立体声的音乐，并在性能评估上表现出色。 |
| [^17] | [Bayesian Uncertainty for Gradient Aggregation in Multi-Task Learning](https://arxiv.org/abs/2402.04005) | 这篇论文提出了一种利用贝叶斯推断的梯度聚合方法，通过引入概率分布来量化梯度维度的不确定性，在多任务学习中获得更好的效果。 |
| [^18] | [Fine-tuning Reinforcement Learning Models is Secretly a Forgetting Mitigation Problem](https://arxiv.org/abs/2402.02868) | 细调强化学习模型中的遗忘问题会导致转移效果差，研究发现常见且具有灾难性后果。通过使用标准的知识保留技术可以缓解这个问题并最大程度地利用细调的优势。 |
| [^19] | [Revisiting the Power of Prompt for Visual Tuning](https://arxiv.org/abs/2402.02382) | 本研究提出了一种改进的视觉调整方法，通过采用下游令牌原型对提示进行初始化，进一步优化令牌构造，有效解决了视觉提示调整中的挑战，并取得了比其他方法更好的性能。 |
| [^20] | [A Survey of Data-Efficient Graph Learning](https://arxiv.org/abs/2402.00447) | 这项研究提出了数据高效图学习（DEGL）的概念，并总结了近期在这一领域的进展。DEGL的目标是在资源有限的场景下提高图机器学习的性能，通过探索各种最小监督方法来解决大规模标记数据的挑战。 |
| [^21] | [Dataset Condensation Driven Machine Unlearning](https://arxiv.org/abs/2402.00195) | 这篇论文通过引入数据集精简作为机器遗忘的重要组成部分，在图像分类中解决了持久的计算挑战，并改进了近似遗忘的计算复杂性。 |
| [^22] | [Deeper or Wider: A Perspective from Optimal Generalization Error with Sobolev Loss](https://arxiv.org/abs/2402.00152) | 本文比较了更深的神经网络和更宽的神经网络在Sobolev损失的最优泛化误差方面的表现，研究发现神经网络的架构受多种因素影响，参数数量更多倾向于选择更宽的网络，而样本点数量和损失函数规则性更高倾向于选择更深的网络。 |
| [^23] | [Zero-Shot Reinforcement Learning via Function Encoders](https://arxiv.org/abs/2401.17173) | 本论文提出了一种用于实现零-shot迁移的函数编码器，通过将函数表示为学习到的非线性基函数的加权组合，代理程序通过连贯的向量表示了当前任务与先前看到任务的关联信息，从而实现了在相关任务之间的迁移，无需额外训练。 |
| [^24] | [The Selected-completely-at-random Complementary Label is a Practical Weak Supervision for Multi-class Classification](https://arxiv.org/abs/2311.15502) | 提出了一种不依赖于均匀分布假设的互补标签学习方法，基于完全随机选择假设的无偏风险估计器，以及风险校正方法来解决过拟合问题。 |
| [^25] | [Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space](https://arxiv.org/abs/2310.09656) | 本文介绍了一种名为Tabsyn的方法，利用潜在空间中的扩散模型和变分自编码器生成具有混合类型数据的表格数据。Tabsyn具有通用性、质量和速度等关键优势，通过将不同类型的数据转换为统一空间并捕捉列间关系，优化潜在嵌入的分布来生成高质量的合成数据，同时生成速度更快。 |
| [^26] | [Unveiling the Pitfalls of Knowledge Editing for Large Language Models](https://arxiv.org/abs/2310.02129) | 这篇论文探讨了大型语言模型知识编辑的潜在陷阱，提出了新的评估方法，发现知识冲突和知识扭曲是两个重要问题。 |
| [^27] | [OMPGPT: A Generative Pre-trained Transformer Model for OpenMP.](http://arxiv.org/abs/2401.16445) | OMPGPT是一种为了OpenMP pragma生成而设计的生成式预训练Transformer模型，采用了来自NLP领域的提示工程技术，并创建了一种创新的策略chain-of-OMP。 |
| [^28] | [CompactifAI: Extreme Compression of Large Language Models using Quantum-Inspired Tensor Networks.](http://arxiv.org/abs/2401.14109) | CompactifAI是一种使用量子启发的张量网络对大型语言模型进行极压缩的创新方法，相比于传统的压缩方法，它更注重模型的相关空间，实现更加可控和精细的压缩。 |
| [^29] | [LocMoE: A Low-overhead MoE for Large Language Model Training.](http://arxiv.org/abs/2401.13920) | LocMoE提出了一种新的路由策略，通过将部分节点间通信转换为节点内通信，结合负载平衡和局部性，以提高大型语言模型训练的性能。 |
| [^30] | [Convergence Rate Maximization for Split Learning-based Control of EMG Prosthetic Devices.](http://arxiv.org/abs/2401.03233) | 本文介绍了一种用于最大化模型收敛速率的Split Learning肌电假肢控制中的切层选择算法，通过加速收敛过程提高了假肢控制的性能。 |
| [^31] | [Approximating Numerical Flux by Fourier Neural Operators for the Hyperbolic Conservation Laws.](http://arxiv.org/abs/2401.01783) | 该研究通过在传统的数值方案中运用神经网络来替换数值通量，解决了神经网络方法缺乏鲁棒性和泛化能力的问题，并展示了该方法在超声古典守恒定律方面具有优势。 |
| [^32] | [Convergence Rates for Stochastic Approximation: Biased Noise with Unbounded Variance, and Applications.](http://arxiv.org/abs/2312.02828) | 本论文研究了带有无界方差的有偏噪声对随机逼近算法的收敛速度的影响，并介绍了该算法在各个领域的应用。 |
| [^33] | [A Mass-Conserving-Perceptron for Machine Learning-Based Modeling of Geoscientific Systems.](http://arxiv.org/abs/2310.08644) | 这篇论文提出了一种质量保持感知器（MCP）用于将物理-概念模型和机器学习模型结合起来建模地球科学系统，通过利用机器学习技术从数据中学习物理过程的功能性和质量保持性。 |
| [^34] | [Generative Modeling with Phase Stochastic Bridges.](http://arxiv.org/abs/2310.07805) | 通过在相位空间中构建路径测度，我们提出了一种新颖的生成建模框架，可以在动力传播的早期阶段生成逼真的数据点，并利用额外的速度信息实现高效的数据生成。 |
| [^35] | [How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English ChatGPT Responses.](http://arxiv.org/abs/2310.03031) | 本文系统地分析并探索了德语和英语的ChatGPT回应中可能存在的问题，特别关注了性别偏见。我们发现，在对系统多次提供相同指令的情况下，回应存在差异。使用ChatGPT来帮助非IT用户撰写工作文本非常有用，但用户需要充分考虑系统的固有限制。 |
| [^36] | [Generative Modeling of Regular and Irregular Time Series Data via Koopman VAEs.](http://arxiv.org/abs/2310.02619) | 本文介绍了一种基于Koopman VAEs的新型生成框架，可以用于生成规则和非规则时间序列数据，解决了GANs训练不稳定和模式崩溃的问题，并通过利用谱工具对线性映射的特征值施加约束，实现了领域知识的整合和对定性行为和稳定性的研究。 |
| [^37] | [zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated Learning.](http://arxiv.org/abs/2310.02554) | zkFL是一种基于零知识证明的联邦学习梯度聚合方法，通过提供每轮的证明来解决协调者恶意行为的问题。 |
| [^38] | [G4SATBench: Benchmarking and Advancing SAT Solving with Graph Neural Networks.](http://arxiv.org/abs/2309.16941) | G4SATBench是第一个为基于图神经网络的SAT求解器提供全面评估框架的基准研究。它提供了一个大而多样的SAT数据集，并基于各种预测任务、训练目标和推理算法对广泛的GNN模型进行了测试和比较。通过与搜索型SAT求解器中的启发式方法进行比较，它还揭示了基于GNN的SAT求解器的学习能力以及其优缺点。 |
| [^39] | [Multi-unit soft sensing permits few-shot learning.](http://arxiv.org/abs/2309.15828) | 多单元软测量是利用可转移性学习算法改进软测量的一种方法，能够通过解决多个任务来增强软测量的性能，并且特别适用于具有多个实现的进程。 |
| [^40] | [Density Estimation via Measure Transport: Outlook for Applications in the Biological Sciences.](http://arxiv.org/abs/2309.15366) | 通过测度传递方法进行密度估计在生物科学中具有广阔的应用前景，尤其是在处理稀疏数据的情况下，使用稀疏传递映射可以揭示数据中隐藏的信息。 |
| [^41] | [MoDem-V2: Visuo-Motor World Models for Real-World Robot Manipulation.](http://arxiv.org/abs/2309.14236) | MoDem-V2是一个能够在非仪器化的真实世界中直接学习接触丰富操作的系统。 |
| [^42] | [Fully Embedded Time-Series Generative Adversarial Networks.](http://arxiv.org/abs/2308.15730) | 这篇论文提出了一种完全嵌入的时间序列生成对抗网络（FETSGAN），通过对抗性训练匹配特征空间和低维度采样空间的训练分布，确保合成样本的时间分布不会崩溃。 |
| [^43] | [Temporal Interest Network for Click-Through Rate Prediction.](http://arxiv.org/abs/2308.08487) | 本文提出了时间兴趣网络（TIN），用于捕捉行为与目标之间的四重语义和时间相关性，以预测点击率的效果和已有方法对这种相关性的学习程度尚不清楚。 |
| [^44] | [Properties of Discrete Sliced Wasserstein Losses.](http://arxiv.org/abs/2307.10352) | 本文研究了离散切割Wasserstein损失的性质，并探讨了其正则性和优化性质以及通过蒙特卡洛近似的方法。 |
| [^45] | [DeepMem: ML Models as storage channels and their (mis-)applications.](http://arxiv.org/abs/2307.08811) | 本文提出了将机器学习模型作为存储通道的新视角，通过过度参数化来增加通道容量。通过在训练时嵌入信息，并利用黑盒访问实现信息的存储和提取。 |
| [^46] | [Intuitionistic Fuzzy Broad Learning System: Enhancing Robustness Against Noise and Outliers.](http://arxiv.org/abs/2307.08713) | 该论文提出了一种直觉模糊广泛学习系统（IF-BLS），用于增强对噪声和异常值的稳健性，通过为每个训练点分配模糊隶属度来减小噪声和异常值的影响。 |
| [^47] | [DIGEST: Fast and Communication Efficient Decentralized Learning with Local Updates.](http://arxiv.org/abs/2307.07652) | 本文提出了一种名为DIGEST的快速和通信高效的异步分散学习机制，通过结合Gossip和随机游走的思想，并专注于随机梯度下降（SGD），实现了在分散学习中较低的通信成本和较快的收敛时间。 |
| [^48] | [Learning Active Subspaces and Discovering Important Features with Gaussian Radial Basis Functions Neural Networks.](http://arxiv.org/abs/2307.05639) | 本论文提出了一种修改的径向基函数神经网络模型，通过学习精度矩阵，从训练完成后的模型中提取有用信息，包括活跃子空间的方向和输入变量重要性的排序。 |
| [^49] | [Bayesian Optimization with Formal Safety Guarantees via Online Conformal Prediction.](http://arxiv.org/abs/2306.17815) | 本文提出了一种基于贝叶斯优化的方法，无论约束函数的特性如何，都能满足安全要求。 |
| [^50] | [PeFLL: A Lifelong Learning Approach to Personalized Federated Learning.](http://arxiv.org/abs/2306.05515) | PeFLL是个性化联邦学习的一种新方法，通过联合训练嵌入网络和超网络，PeFLL能够学习输出特定于每个客户端的模型，并且在其它新出现的客户端上表现良好。 |
| [^51] | [Posterior Collapse in Linear Conditional and Hierarchical Variational Autoencoders.](http://arxiv.org/abs/2306.05023) | 本文研究了高度相似的变分后验分布和先验分布之间的后验崩溃现象，特别地，通过对线性条件VAE和分层VAE进行分析，证明了这种现象是由于潜在变量层次关系不清晰而引起的。 |
| [^52] | [Transition role of entangled data in quantum machine learning.](http://arxiv.org/abs/2306.03481) | 本研究证明了纠缠数据对量子机器学习的性能具有双重效应，有助于减少预测误差和减小训练数据大小，为量子机器学习模型设计提供了指南。 |
| [^53] | [Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic.](http://arxiv.org/abs/2306.02865) | 该论文提出了 BEE 操作符，通过充分利用过去的成功经验，并保持探索乐观性，解决了离线策略演员-评论家中 Q 值高估与低估问题，提高了策略学习和样本效率。 |
| [^54] | [Simplifying Full Waveform Inversion via Domain-Independent Self-Supervised Learning.](http://arxiv.org/abs/2305.13314) | 本文提出了基于领域无关自监督学习的SimFWI算法，其两个步骤分别是分别通过多个数据集上的遮盖图像建模学习编码器和解码器，然后为每个数据集学习一个线性映射。该算法可用于预测地震数据中的地下速度图，可极大地简化全波形反演任务，并连接多个FWI数据集。 |
| [^55] | [MAGDiff: Covariate Data Set Shift Detection via Activation Graphs of Deep Neural Networks.](http://arxiv.org/abs/2305.13271) | MAGDiff是一种新的表示法，可以从神经网络分类器中提取，可以有效地检测协变数据集转移，而不需要训练新模型。这可以通过对比神经网络激活图来计算，并通过两个样本 Kolmogorov-Smirnov 测试进行实证验证。 |
| [^56] | [The Update Equivalence Framework for Decision-Time Planning.](http://arxiv.org/abs/2304.13138) | 该论文提出了一个基于更新等价的决策时间规划框架，使得决策时间规划算法不依赖于公共信息，在更大范围的不完全信息决策环境中实现超人类表现。 |
| [^57] | [BanditQ -- No-Regret Learning with Guaranteed Per-User Rewards in Adversarial Environments.](http://arxiv.org/abs/2304.05219) | 提出了一种名为BanditQ的新的在线预测策略，在敌对设置中以公平的方式达到目标速率约束，并实现了$O(T^{3/4})$的遗憾。 |
| [^58] | [A Billion-scale Foundation Model for Remote Sensing Images.](http://arxiv.org/abs/2304.05215) | 本文介绍了一个用于遥感图像的十亿级基础模型，并研究了增加模型参数数量对该模型在下游任务中的性能影响，实验显示增加模型参数数量可以显著提高性能。 |
| [^59] | [Generalization with quantum geometry for learning unitaries.](http://arxiv.org/abs/2303.13462) | 本文研究了量子机器学习模型的泛化能力，使用数据的量子费舍尔信息度量来评估成功训练和泛化所需的电路参数和训练数据的数量，并展示通过去除对称性来提高泛化能力，同时发现超出分布泛化能力可以比使用相同分布更优。 |
| [^60] | [Post-Episodic Reinforcement Learning Inference.](http://arxiv.org/abs/2302.08854) | 我们提出了一种后期情节式强化学习推断的方法，能够评估反事实的自适应策略并估计动态处理效应，通过重新加权的$Z$-估计方法稳定情节变化的估计方差。 |
| [^61] | [Archetypal Analysis++: Rethinking the Initialization Strategy.](http://arxiv.org/abs/2301.13748) | 本文提出了一种针对原型分析的概率初始化策略 AA ++，能够在13个不同大小和维度的实际数据集上表现优异。 |
| [^62] | [On the Security Vulnerabilities of Text-to-SQL Models.](http://arxiv.org/abs/2211.15363) | 该论文揭示了Text-to-SQL模型存在的安全漏洞，并证明了这些漏洞能够被恶意利用产生攻击，通过对商业应用和开源语言模型的实验验证。该研究意在引起学术界对NLP算法相关的软件安全问题的关注和进一步研究。 |
| [^63] | [Log-linear Guardedness and its Implications.](http://arxiv.org/abs/2210.10012) | 本研究介绍了对数线性保护性及其对下游分类器行为的影响。在二元情况下，下游对数线性模型无法恢复被删除的概念，但在某些情况下，可以通过构建多类对数线性模型间接恢复概念。这些结果揭示了线性删除方法的局限性，并强调了进一步研究的需求。 |
| [^64] | [Multi-scale Wasserstein Shortest-path Filtration Kernels on Graphs.](http://arxiv.org/abs/2206.00979) | 这篇论文提出了一种名为多尺度Wasserstein最短路径过滤图核心（MWSPF）的新型最短路径图核心，解决了传统核心的信息丢失和缺乏多个尺度考虑的问题。 |
| [^65] | [Reinforcement Learning in Modern Biostatistics: Constructing Optimal Adaptive Interventions.](http://arxiv.org/abs/2203.02605) | 本文是关于将强化学习应用于自适应干预中的第一份统一调查，强化学习在动态治疗方案和移动健康中即时自适应干预这两个领域中都具有很大的应用潜力。在这两个领域之间存在相似和不同之处需要考虑，并且这里存在巨大的合作机会。 |
| [^66] | [Efficient Direct-Connect Topologies for Collective Communications.](http://arxiv.org/abs/2202.03356) | 本论文提出了一种算法框架，可以构建高效的直连网络拓扑结构，以优化节点延迟和带宽权衡，适用于集体通信负载。 |
| [^67] | [Rule Generation for Classification: Scalability, Interpretability, and Fairness.](http://arxiv.org/abs/2104.10751) | 这项研究介绍了一种新的基于规则的分类优化方法，利用列生成线性规划实现可扩展性，并通过分配成本系数和引入额外约束解决了解释性和公平性问题。该方法在局部解释性和公平性之间取得了良好的平衡。 |

# 详细

[^1]: 学习面向完全可观察非确定性计划领域的泛化策略

    Learning Generalized Policies for Fully Observable Non-Deterministic Planning Domains

    [https://arxiv.org/abs/2404.02499](https://arxiv.org/abs/2404.02499)

    本研究扩展了学习完全可观察、非确定性计划领域的泛化策略的方法，并通过实验评估了在一些 FOND 计划基准领域中产生的泛化策略的正确性。

    

    泛化策略代表解决大量计划问题的反应性策略，例如从给定领域中无限可解实例的集合。 提出了一种从一系列小训练实例中学习这种策略的方法，已成功应用于经典领域。 本文扩展了学习面向完全可观察、非确定性（FOND）领域的泛化策略的公式和导致的组合方法，通过一系列 FOND 计划基准领域的实验评估了生成的方法，展示了一些领域中产生的泛化策略，并证明了其正确性。 学习 FOND 计划的泛化策略方法实际上可以被看作是一种搜索结果的另一种 FOND 计划方法，这种方法不是在给定状态空间中搜索解决方案，而是在由必须学习的特征定义的抽象空间中搜索解决方案。

    arXiv:2404.02499v1 Announce Type: new  Abstract: General policies represent reactive strategies for solving large families of planning problems like the infinite collection of solvable instances from a given domain. Methods for learning such policies from a collection of small training instances have been developed successfully for classical domains. In this work, we extend the formulations and the resulting combinatorial methods for learning general policies over fully observable, non-deterministic (FOND) domains. We also evaluate the resulting approach experimentally over a number of benchmark domains in FOND planning, present the general policies that result in some of these domains, and prove their correctness. The method for learning general policies for FOND planning can actually be seen as an alternative FOND planning method that searches for solutions, not in the given state space but in an abstract space defined by features that must be learned as well.
    
[^2]: 基于共轭梯度的自适应矩估计优化算法用于深度学习

    Conjugate-Gradient-like Based Adaptive Moment Estimation Optimization Algorithm for Deep Learning

    [https://arxiv.org/abs/2404.01714](https://arxiv.org/abs/2404.01714)

    提出一种基于共轭梯度样式的新优化算法CG-like-Adam，用于深度学习，并在收敛分析和数值实验中展示了其优越性

    

    训练深度神经网络是一项具有挑战性的任务。为加快培训速度并增强深度神经网络的性能，我们将传统的共轭梯度修正为共轭梯度样式，并将其并入通用Adam中，因此提出了一种名为CG-like-Adam的新优化算法用于深度学习。具体而言，通用Adam的一阶和二阶矩估计均由共轭梯度样式替换。收敛分析处理了一阶矩估计的指数移动平均系数为常数且一阶矩估计无偏的情况。数值实验显示了基于CIFAR10/100数据集的所提算法的优越性。

    arXiv:2404.01714v1 Announce Type: cross  Abstract: Training deep neural networks is a challenging task. In order to speed up training and enhance the performance of deep neural networks, we rectify the vanilla conjugate gradient as conjugate-gradient-like and incorporate it into the generic Adam, and thus propose a new optimization algorithm named CG-like-Adam for deep learning. Specifically, both the first-order and the second-order moment estimation of generic Adam are replaced by the conjugate-gradient-like. Convergence analysis handles the cases where the exponential moving average coefficient of the first-order moment estimation is constant and the first-order moment estimation is unbiased. Numerical experiments show the superiority of the proposed algorithm based on the CIFAR10/100 dataset.
    
[^3]: 将扩散模型调整到私有领域而无需微调

    DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning

    [https://arxiv.org/abs/2403.14421](https://arxiv.org/abs/2403.14421)

    开发了第一个差分隐私检索增强生成算法，能够在生成高质量图像样本的同时提供可证明的隐私保证

    

    arXiv:2403.14421v1 公告类型：新的 文摘：文本到图像的扩散模型已经被证明存在样本级别的记忆化问题，可能会复制出与其训练图像几乎完全相同的副本，这可能是不希望看到的。为了解决这个问题，我们开发了第一个能够生成高质量图像样本并提供可证明的隐私保证的差分隐私检索增强生成算法。具体而言，我们假设可以访问一个在少量公共数据上训练的文本到图像扩散模型，并设计一个DP检索机制，以从私有检索数据集中检索的样本来增强文本提示。我们的\emph{差分隐私检索增强扩散模型}（DP-RDM）在适应另一个领域时无需对检索数据集进行微调，并且可以使用最先进的生成模型生成高质量的图像样本，同时满足严格的差分隐私保证。例如，在评估时

    arXiv:2403.14421v1 Announce Type: new  Abstract: Text-to-image diffusion models have been shown to suffer from sample-level memorization, possibly reproducing near-perfect replica of images that they are trained on, which may be undesirable. To remedy this issue, we develop the first differentially private (DP) retrieval-augmented generation algorithm that is capable of generating high-quality image samples while providing provable privacy guarantees. Specifically, we assume access to a text-to-image diffusion model trained on a small amount of public data, and design a DP retrieval mechanism to augment the text prompt with samples retrieved from a private retrieval dataset. Our \emph{differentially private retrieval-augmented diffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset to adapt to another domain, and can use state-of-the-art generative models to generate high-quality image samples while satisfying rigorous DP guarantees. For instance, when evaluated on M
    
[^4]: 地球观测应用中缺失数据对模型预测的影响评估

    Impact Assessment of Missing Data in Model Predictions for Earth Observation Applications

    [https://arxiv.org/abs/2403.14297](https://arxiv.org/abs/2403.14297)

    本研究评估了在地球观测应用中缺失数据对训练模型的影响，发现集成策略可以实现高达100%的预测稳健性，同时揭示了缺失情景在回归任务中比分类任务更具挑战性，且光学视角是最关键的。

    

    地球观测（EO）应用涉及复杂和异构数据源，通常采用机器学习模型进行处理。然而，人们普遍假设数据源将持续可用。不同情况可能影响EO数据源的可用性，如噪声、云层或卫星任务失败。本研究评估了在四个数据集上进行的分类和回归任务中缺失时间性和静态EO数据源对训练模型的影响。我们比较了不同方法的预测质量，并发现一些方法在面对缺失数据时自然更加稳健。特别是集成策略实现了高达100%的预测稳健性。我们发现缺失情景在回归任务中比分类任务更具挑战性。最后，我们发现光学视角在单独缺失时是最关键的视角。

    arXiv:2403.14297v1 Announce Type: cross  Abstract: Earth observation (EO) applications involving complex and heterogeneous data sources are commonly approached with machine learning models. However, there is a common assumption that data sources will be persistently available. Different situations could affect the availability of EO sources, like noise, clouds, or satellite mission failures. In this work, we assess the impact of missing temporal and static EO sources in trained models across four datasets with classification and regression tasks. We compare the predictive quality of different methods and find that some are naturally more robust to missing data. The Ensemble strategy, in particular, achieves a prediction robustness up to 100%. We evidence that missing scenarios are significantly more challenging in regression than classification tasks. Finally, we find that the optical view is the most critical view when it is missing individually.
    
[^5]: STG-Mamba: 通过选择性状态空间模型进行时空图学习

    STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model

    [https://arxiv.org/abs/2403.12418](https://arxiv.org/abs/2403.12418)

    STG-Mamba 是首个利用选择性状态空间模型进行时空图学习的研究，将STG网络视为系统，并采用图选择性状态空间模块（GS3B）精确表征STG的动态演化。

    

    Spatial-Temporal Graph（STG）数据具有动态性、异质性和非平稳性特点，导致时空图学习持续面临挑战。近年来，提出了各种基于GNN的方法，主要集中于模拟STG网络中节点个体之间的关系，忽略了随时间存在的STG系统本质特征的建模重要性。相反，现代选择性状态空间模型（SSSMs）提出了一种将STG网络视为系统的新方法，并精心探索了STG系统在时间维度上的动态状态演变。在本工作中，我们引入了Spatial-Temporal Graph Mamba（STG-Mamba），作为首个利用强大的选择性状态空间模型进行STG学习的研究，将STG网络视为系统，并采用图选择性状态空间模块（GS3B）精确表征STG的动态演化。

    arXiv:2403.12418v1 Announce Type: cross  Abstract: Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous, and non-stationary, leading to the continuous challenge of spatial-temporal graph learning. In the past few years, various GNN-based methods have been proposed to solely focus on mimicking the relationships among node individuals of the STG network, ignoring the significance of modeling the intrinsic features that exist in STG system over time. In contrast, modern Selective State Space Models (SSSMs) present a new approach which treat STG Network as a system, and meticulously explore the STG system's dynamic state evolution across temporal dimension. In this work, we introduce Spatial-Temporal Graph Mamba (STG-Mamba) as the first exploration of leveraging the powerful selective state space models for STG learning by treating STG Network as a system, and employing the Graph Selective State Space Block (GS3B) to precisely characterize the dynamic evolution of ST
    
[^6]: DTOR：决策树异常值回归器用于解释异常

    DTOR: Decision Tree Outlier Regressor to explain anomalies

    [https://arxiv.org/abs/2403.10903](https://arxiv.org/abs/2403.10903)

    DTOR是一种决策树异常值回归器，通过估计异常检测模型生成的异常分数来产生基于规则的解释，具有鲁棒性，适用于具有大量特征数据集。

    

    解释异常值的出现以及其产生机制在各种领域中可能非常重要。故障、欺诈、威胁等问题，除了被正确识别之外，通常需要有效的解释以有效执行可操作的对抗措施。越来越广泛地使用复杂的机器学习方法来识别异常值，使得这样的解释更具挑战性。我们提出了决策树异常值回归器（DTOR），这是一种通过估计异常检测模型生成的异常分数来为单个数据点生成基于规则的解释的技术。这是通过首先应用决策树回归器来计算估计分数，然后提取与数据点分数相关联的相对路径来实现的。我们的结果表明，即使在具有大量特征的数据集中，DTOR的鲁棒性也得到了证实。此外，与其他基于规则的方法相比

    arXiv:2403.10903v1 Announce Type: cross  Abstract: Explaining outliers occurrence and mechanism of their occurrence can be extremely important in a variety of domains. Malfunctions, frauds, threats, in addition to being correctly identified, oftentimes need a valid explanation in order to effectively perform actionable counteracts. The ever more widespread use of sophisticated Machine Learning approach to identify anomalies make such explanations more challenging. We present the Decision Tree Outlier Regressor (DTOR), a technique for producing rule-based explanations for individual data points by estimating anomaly scores generated by an anomaly detection model. This is accomplished by first applying a Decision Tree Regressor, which computes the estimation score, and then extracting the relative path associated with the data point score. Our results demonstrate the robustness of DTOR even in datasets with a large number of features. Additionally, in contrast to other rule-based approac
    
[^7]: 一个用于在不确定性下发现可信神经网络代理模型的战略框架

    A Framework for Strategic Discovery of Credible Neural Network Surrogate Models under Uncertainty

    [https://arxiv.org/abs/2403.08901](https://arxiv.org/abs/2403.08901)

    提出了一个名为Occam Plausibility Algorithm for surrogate models (OPAL-surrogate)的框架，通过基于神经网络的代理模型，采用层次贝叶斯推理和模型验证测试来评估代理模型的可信度和预测可靠性。

    

    深度神经网络在开发复杂物理系统高保真仿真的数据驱动代理模型中的广泛整合，凸显了稳健不确定性量化技术和可信度评估方法的重要性，确保代理模型可可靠地用于重要决策。本研究提出了用于代理模型的Occam Plausibility Algorithm（OPAL-surrogate），提供了一个系统框架，以在大量潜在模型（包括各种神经网络类别以及架构和超参数选择）中揭示具有预测能力的基于神经网络的代理模型。框架基于层次贝叶斯推理，并采用模型验证测试来评估在不确定性下代理模型的可信度和预测可靠性。通过利用这些原则，OPAL-surrogate引入了一种系统性和

    arXiv:2403.08901v1 Announce Type: cross  Abstract: The widespread integration of deep neural networks in developing data-driven surrogate models for high-fidelity simulations of complex physical systems highlights the critical necessity for robust uncertainty quantification techniques and credibility assessment methodologies, ensuring the reliable deployment of surrogate models in consequential decision-making. This study presents the Occam Plausibility Algorithm for surrogate models (OPAL-surrogate), providing a systematic framework to uncover predictive neural network-based surrogate models within the large space of potential models, including various neural network classes and choices of architecture and hyperparameters. The framework is grounded in hierarchical Bayesian inferences and employs model validation tests to evaluate the credibility and prediction reliability of the surrogate models under uncertainty. Leveraging these principles, OPAL-surrogate introduces a systematic and
    
[^8]: C2P-GCN：用于结直肠癌分级的细胞到补丁图卷积网络

    C2P-GCN: Cell-to-Patch Graph Convolutional Network for Colorectal Cancer Grading

    [https://arxiv.org/abs/2403.04962](https://arxiv.org/abs/2403.04962)

    C2P-GCN提出了一种新颖的细胞到补丁图卷积网络方法，通过两阶段图形成，在第一阶段形成补丁级图，第二阶段形成图像级图，从而更好地捕捉结直肠癌组织结构信息。

    

    基于图的学习方法越来越受欢迎，因为它们能够编码组织/器官结构信息，逐渐成为结直肠癌组织学图像分级的首选。最近的基于图的技术涉及将整张幻灯片图像（WSIs）分成更小或中等大小的补丁，然后在每个补丁上构建图以直接用于训练。然而，这种方法无法捕捉整个WSI中存在的组织结构信息，并依赖于来自大量图像补丁的训练。在本文中，我们提出了一种新颖的细胞到补丁图卷积网络（C2P-GCN），这是一种基于两阶段图形成的方法。在第一阶段，它基于WSI上每个补丁上的细胞组织形成一个补丁级图。在第二阶段，它基于WSI中每个补丁之间的相似度量形成一个图像级图，将每个补丁视为图的节点。这种图表示是t

    arXiv:2403.04962v1 Announce Type: cross  Abstract: Graph-based learning approaches, due to their ability to encode tissue/organ structure information, are increasingly favored for grading colorectal cancer histology images. Recent graph-based techniques involve dividing whole slide images (WSIs) into smaller or medium-sized patches, and then building graphs on each patch for direct use in training. This method, however, fails to capture the tissue structure information present in an entire WSI and relies on training from a significantly large dataset of image patches. In this paper, we propose a novel cell-to-patch graph convolutional network (C2P-GCN), which is a two-stage graph formation-based approach. In the first stage, it forms a patch-level graph based on the cell organization on each patch of a WSI. In the second stage, it forms an image-level graph based on a similarity measure between patches of a WSI considering each patch as a node of a graph. This graph representation is t
    
[^9]: 学习推迟对人群的学习：一种元学习方法

    Learning to Defer to a Population: A Meta-Learning Approach

    [https://arxiv.org/abs/2403.02683](https://arxiv.org/abs/2403.02683)

    通过元学习，本研究提出一种学习推迟对人群的方法，该方法可以在测试时适应前所未见的专家，从而更好地面对困难决策。

    

    arXiv:2403.02683v1 公告类型：新 摘要：学习推迟（L2D）框架允许自主系统通过将困难决策委托给人类专家来保持安全和健壮。所有现有的关于L2D的工作都假设每个专家都可以很好地确定，并且如果任何专家发生变化，系统应该重新训练。在这项工作中，我们减轻了这一限制，制定了一个L2D系统，它可以在测试时应对前所未见的专家。我们通过使用元学习来实现这一点，考虑了基于优化和基于模型的变体。给定一个小的上下文集来描述当前可用的专家，我们的框架可以快速调整它的推迟策略。对于基于模型的方法，我们采用了一个注意力机制，能够寻找上下文集中与给定测试点相似的点，从而更精确地评估专家的能力。

    arXiv:2403.02683v1 Announce Type: new  Abstract: The learning to defer (L2D) framework allows autonomous systems to be safe and robust by allocating difficult decisions to a human expert. All existing work on L2D assumes that each expert is well-identified, and if any expert were to change, the system should be re-trained. In this work, we alleviate this constraint, formulating an L2D system that can cope with never-before-seen experts at test-time. We accomplish this by using meta-learning, considering both optimization- and model-based variants. Given a small context set to characterize the currently available expert, our framework can quickly adapt its deferral policy. For the model-based approach, we employ an attention mechanism that is able to look for points in the context set that are similar to a given test point, leading to an even more precise assessment of the expert's abilities. In the experiments, we validate our methods on image recognition, traffic sign detection, and s
    
[^10]: 物理约束的多项式混沌展开用于科学机器学习和不确定性量化

    Physics-constrained polynomial chaos expansion for scientific machine learning and uncertainty quantification

    [https://arxiv.org/abs/2402.15115](https://arxiv.org/abs/2402.15115)

    提出一种物理约束的多项式混沌展开方法，将科学机器学习与不确定性量化无缝集成，有效地实现SciML任务中的不确定性量化和在UQ任务中利用SciML提高不确定性评估。

    

    我们提出了一种新颖的物理约束的多项式混沌展开作为一种替代建模方法，能够执行科学机器学习（SciML）和不确定性量化（UQ）任务。所提出的方法具有独特的能力：将SciML与UQ无缝集成，从而能够有效地量化SciML任务中的不确定性，并利用SciML来改善UQ相关任务中的不确定性评估。该替代模型可以有效地纳入多种物理约束，如支配偏微分方程（PDEs）及其相关的初始和边界条件约束，不等式型约束（如单调性，凸性，非负性等），以及在训练过程中添加额外先验信息以辅助有限数据。这确保了物理上合理的预测，并显著减少了昂贵计算的需求。

    arXiv:2402.15115v1 Announce Type: cross  Abstract: We present a novel physics-constrained polynomial chaos expansion as a surrogate modeling method capable of performing both scientific machine learning (SciML) and uncertainty quantification (UQ) tasks. The proposed method possesses a unique capability: it seamlessly integrates SciML into UQ and vice versa, which allows it to quantify the uncertainties in SciML tasks effectively and leverage SciML for improved uncertainty assessment during UQ-related tasks. The proposed surrogate model can effectively incorporate a variety of physical constraints, such as governing partial differential equations (PDEs) with associated initial and boundary conditions constraints, inequality-type constraints (e.g., monotonicity, convexity, non-negativity, among others), and additional a priori information in the training process to supplement limited data. This ensures physically realistic predictions and significantly reduces the need for expensive comp
    
[^11]: ACE：具有因果感知熵正则化的离策略演员-评论家算法

    ACE : Off-Policy Actor-Critic with Causality-Aware Entropy Regularization

    [https://arxiv.org/abs/2402.14528](https://arxiv.org/abs/2402.14528)

    该论文提出了ACE算法，通过引入因果感知熵正则化，有效评估不同行为的重要性，并分析梯度休眠现象，引入休眠引导复位机制，在多个连续控制任务中取得显著性能优势。

    

    先前的无模型强化学习算法忽视了策略学习过程中不同原始行为的变化重要性。利用这一观点，我们探讨了不同动作维度和奖励之间的因果关系，以评估训练过程中各种原始行为的重要性。我们引入了一种因果感知熵项，有效地识别并优先处理具有高潜在影响的行动，以实现有效的探索。此外，为了防止对特定原始行为过度关注，我们分析了梯度休眠现象，并引入了一种休眠引导复位机制，进一步增强了我们的方法的功效。我们提出的算法ACE：具有因果感知熵正则化的离策演员-评论家，在跨7个领域的29个不同连续控制任务中，相较于无模型强化学习基线，表现出显著的性能优势。

    arXiv:2402.14528v1 Announce Type: cross  Abstract: The varying significance of distinct primitive behaviors during the policy learning process has been overlooked by prior model-free RL algorithms. Leveraging this insight, we explore the causal relationship between different action dimensions and rewards to evaluate the significance of various primitive behaviors during training. We introduce a causality-aware entropy term that effectively identifies and prioritizes actions with high potential impacts for efficient exploration. Furthermore, to prevent excessive focus on specific primitive behaviors, we analyze the gradient dormancy phenomenon and introduce a dormancy-guided reset mechanism to further enhance the efficacy of our method. Our proposed algorithm, ACE: Off-policy Actor-critic with Causality-aware Entropy regularization, demonstrates a substantial performance advantage across 29 diverse continuous control tasks spanning 7 domains compared to model-free RL baselines, which un
    
[^12]: HyperAgent：一种简单、可扩展、高效且可证明用于复杂环境的强化学习框架

    HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement Learning Framework for Complex Environments

    [https://arxiv.org/abs/2402.10228](https://arxiv.org/abs/2402.10228)

    HyperAgent提出了一种简单、高效、可扩展的强化学习框架，在复杂环境下能够实现高效的计算和数据选择，是首个达到可证明可扩展的每步计算复杂度以及次线性后悔的方法。

    

    为了在资源约束下解决复杂任务，强化学习（RL）代理需要简单、高效、可扩展、具有大状态空间和不断积累的交互数据。我们提出了HyperAgent，这是一个具有超模型、索引抽样方案和增量更新机制的RL框架，可以在一般价值函数逼近中进行计算高效的顺序后验逼近和数据高效的动作选择，超越了共轭性。HyperAgent的实现简单，只需要在DDQN中添加一个模块和一行额外代码。在实践中，HyperAgent在大规模深度RL基准测试中表现出稳健的性能，无论是在数据还是计算方面都获得了显着的效率提升。在理论上，在实际可扩展的算法中，HyperAgent是第一个能够实现可证明可扩展的每步计算复杂度以及次线性后悔的方法。

    arXiv:2402.10228v1 Announce Type: cross  Abstract: To solve complex tasks under resource constraints, reinforcement learning (RL) agents need to be simple, efficient, and scalable with (1) large state space and (2) increasingly accumulated data of interactions. We propose the HyperAgent, a RL framework with hypermodel, index sampling schemes and incremental update mechanism, enabling computation-efficient sequential posterior approximation and data-efficient action selection under general value function approximation beyond conjugacy. The implementation of \HyperAgent is simple as it only adds one module and one line of code additional to DDQN. Practically, HyperAgent demonstrates its robust performance in large-scale deep RL benchmarks with significant efficiency gain in terms of both data and computation. Theoretically, among the practically scalable algorithms, HyperAgent is the first method to achieve provably scalable per-step computational complexity as well as sublinear regret u
    
[^13]: 使用上下文学习的神经网络中出现人类课程效应

    Human Curriculum Effects Emerge with In-Context Learning in Neural Networks

    [https://arxiv.org/abs/2402.08674](https://arxiv.org/abs/2402.08674)

    人类学习对示例课程和任务结构有敏感性。研究发现，在神经网络和语言模型中，通过上下文学习方法可以同时获得分组和交错训练的优势。

    

    人类学习对规则结构和训练中所使用的示例课程非常敏感。在由简洁规则控制的任务中，当相关示例在多次试验中被分组时，学习更加稳健；但在缺乏这样的规则的情况下，交错训练更加有效。迄今为止，没有神经模型能够同时捕捉到这些看似矛盾的效应。在本文中，我们展示了“上下文学习”（ICL）在使用元学习进行训练的神经网络和大型语言模型（LLMs）中自发产生了同样的权衡。ICL是通过内层循环算法在激活动力学中实现的一种“上下文内学习”（in-context learning）的能力，可以在没有权重更改的情况下学习新任务。对预训练的LLMs和元学习变压器进行的实验表明，ICL在涉及规则结构的任务中展示出了人类所示的分组优势，而同时进行权重学习则复制了人类在缺少这样结构的任务上所观察到的交错优势。

    Human learning is sensitive to rule-like structure and the curriculum of examples used for training. In tasks governed by succinct rules, learning is more robust when related examples are blocked across trials, but in the absence of such rules, interleaving is more effective. To date, no neural model has simultaneously captured these seemingly contradictory effects. Here we show that this same tradeoff spontaneously emerges with "in-context learning" (ICL) both in neural networks trained with metalearning and in large language models (LLMs). ICL is the ability to learn new tasks "in context" - without weight changes - via an inner-loop algorithm implemented in activation dynamics. Experiments with pretrained LLMs and metalearning transformers show that ICL exhibits the blocking advantage demonstrated in humans on a task involving rule-like structure, and conversely, that concurrent in-weight learning reproduces the interleaving advantage observed in humans on tasks lacking such structu
    
[^14]: 从单一儿童语言输入的可学习性的系统调查

    A systematic investigation of learnability from single child linguistic input

    [https://arxiv.org/abs/2402.07899](https://arxiv.org/abs/2402.07899)

    我们的研究探索了用单个儿童的语言输入训练语言模型的可学习性，我们发现这种设置下的语言模型能够形成句法和语义词群，并对某些语言现象具有敏感性。

    

    语言模型（LM）在生成语言连贯文本方面表现出了 remarkable proficiency，引发了关于它们与人类语言可学习性的相关讨论。然而，这些模型的训练数据与儿童接收到的语言输入之间存在着显著差距。LMs通常在数量级上更大且本质与儿童语言输入不同的数据上进行训练。针对这一差距，我们的研究侧重于在单个儿童语言输入的子集上训练LMs。先前的研究发现，在这种设置下训练的LMs可以形成句法和语义词群，并对某些语言现象具有敏感性。然而，这些研究仅考虑了仅使用一个单一儿童数据集训练的LSTMs和更简单的神经网络。为了检验从单一儿童输入可学习性的鲁棒性，我们系统地…

    Language models (LMs) have demonstrated remarkable proficiency in generating linguistically coherent text, sparking discussions about their relevance to understanding human language learnability. However, a significant gap exists between the training data for these models and the linguistic input a child receives. LMs are typically trained on data that is orders of magnitude larger and fundamentally different from child-directed speech (Warstadt and Bowman, 2022; Warstadt et al., 2023; Frank, 2023a). Addressing this discrepancy, our research focuses on training LMs on subsets of a single child's linguistic input. Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in this setting can form syntactic and semantic word clusters and develop sensitivity to certain linguistic phenomena, but they only considered LSTMs and simpler neural networks trained from just one single-child dataset. Here, to examine the robustness of learnability from single-child input, we systematicall
    
[^15]: Hydragen：共享前缀的高吞吐量LLM推理

    Hydragen: High-Throughput LLM Inference with Shared Prefixes

    [https://arxiv.org/abs/2402.05099](https://arxiv.org/abs/2402.05099)

    Hydragen是一种具有共享前缀的高吞吐量LLM推理方法，通过将注意力计算分解为共享前缀和唯一后缀，来提高推理效率，并能够提高端到端LLM吞吐量多达32倍。

    

    基于Transformer的大规模语言模型（LLM）现在已经部署到数亿用户上。LLM推理通常在共享前缀的序列批次上执行，例如少量样本示例或聊天机器人系统提示。在这种大批量设置下，解码可能会受到注意操作的瓶颈，该操作从内存中读取大型键值（KV）缓存，并为批次中的每个序列计算低效的矩阵-向量乘积。在这项工作中，我们介绍了Hydragen，一种具有共享前缀的硬件感知精确实现的注意力。Hydragen将注意力分别计算在共享前缀和唯一后缀上。这种分解通过在序列之间批量查询一起减少冗余内存读取，从而实现了高效的前缀注意力，并使得可以使用硬件友好的矩阵乘法。我们的方法可以将端到端的LLM吞吐量提高多达32倍，超过竞争基线，并且随着批次大小和共享前缀的长度增加，速度提高的幅度也增加。

    Transformer-based large language models (LLMs) are now deployed to hundreds of millions of users. LLM inference is commonly performed on batches of sequences that share a prefix, such as few-shot examples or a chatbot system prompt. Decoding in this large-batch setting can be bottlenecked by the attention operation, which reads large key-value (KV) caches from memory and computes inefficient matrix-vector products for every sequence in the batch. In this work, we introduce Hydragen, a hardware-aware exact implementation of attention with shared prefixes. Hydragen computes attention over the shared prefix and unique suffixes separately. This decomposition enables efficient prefix attention by batching queries together across sequences, reducing redundant memory reads and enabling the use of hardware-friendly matrix multiplications. Our method can improve end-to-end LLM throughput by up to 32x against competitive baselines, with speedup growing with the batch size and shared prefix lengt
    
[^16]: 快速定时条件下的潜在音频扩散

    Fast Timing-Conditioned Latent Audio Diffusion

    [https://arxiv.org/abs/2402.04825](https://arxiv.org/abs/2402.04825)

    本研究提出了一种名为Stable Audio的生成模型，该模型利用潜在扩散和条件化技术，实现了高效生成44.1kHz立体声音乐和音效。与其他最先进的模型不同，Stable Audio能够生成具有结构和立体声的音乐，并在性能评估上表现出色。

    

    从文本提示生成长篇44.1kHz立体声音频可能对计算要求很高。此外，大多数先前的工作并没有解决音乐和音效在持续时间上的自然变化问题。我们的研究专注于使用生成模型以高效方式生成长篇、可变长度的44.1kHz立体声音乐和音效。Stable Audio基于潜在扩散，其潜在性质由一个全卷积变分自编码器定义。它不仅基于文本提示进行条件化，还基于时间嵌入，使得可以对生成的音乐和音效的内容和长度进行精细控制。在A100 GPU上，Stable Audio能够在8秒内以44.1kHz的速度渲染长达95秒的立体声信号。尽管计算效率高且推理速度快，但它在两个公开的文本-音乐和音频基准中仍然是最好的，并且与最先进的模型不同，它可以生成具有结构和立体声音乐。

    Generating long-form 44.1kHz stereo audio from text prompts can be computationally demanding. Further, most previous works do not tackle that music and sound effects naturally vary in their duration. Our research focuses on the efficient generation of long-form, variable-length stereo music and sounds at 44.1kHz using text prompts with a generative model. Stable Audio is based on latent diffusion, with its latent defined by a fully-convolutional variational autoencoder. It is conditioned on text prompts as well as timing embeddings, allowing for fine control over both the content and length of the generated music and sounds. Stable Audio is capable of rendering stereo signals of up to 95 sec at 44.1kHz in 8 sec on an A100 GPU. Despite its compute efficiency and fast inference, it is one of the best in two public text-to-music and -audio benchmarks and, differently from state-of-the-art models, can generate music with structure and stereo sounds.
    
[^17]: 贝叶斯不确定性用于多任务学习中的梯度聚合

    Bayesian Uncertainty for Gradient Aggregation in Multi-Task Learning

    [https://arxiv.org/abs/2402.04005](https://arxiv.org/abs/2402.04005)

    这篇论文提出了一种利用贝叶斯推断的梯度聚合方法，通过引入概率分布来量化梯度维度的不确定性，在多任务学习中获得更好的效果。

    

    随着机器学习的日益突出，需要同时执行多个推理任务的需求也在增长。为每个任务运行专用模型在计算上十分昂贵，因此对多任务学习（MTL）的兴趣也越来越大。MTL的目标是学习一个能高效解决多个任务的单个模型。通过计算每个任务的单一梯度并将它们聚合起来以获得结合的更新方向来优化MTL模型。然而，这些方法并没有考虑到一个重要的方面，即梯度维度的敏感性。在这里，我们引入了一种新颖的利用贝叶斯推断的梯度聚合方法。我们为任务特定参数放置一个概率分布，这又引起了任务梯度的分布。这些额外的有价值的信息使我们能够量化每个梯度维度中的不确定性，从而在聚合它们时将其纳入考虑。我们通过实证方法证明了我们的方法的效果。

    As machine learning becomes more prominent there is a growing demand to perform several inference tasks in parallel. Running a dedicated model for each task is computationally expensive and therefore there is a great interest in multi-task learning (MTL). MTL aims at learning a single model that solves several tasks efficiently. Optimizing MTL models is often achieved by computing a single gradient per task and aggregating them for obtaining a combined update direction. However, these approaches do not consider an important aspect, the sensitivity in the gradient dimensions. Here, we introduce a novel gradient aggregation approach using Bayesian inference. We place a probability distribution over the task-specific parameters, which in turn induce a distribution over the gradients of the tasks. This additional valuable information allows us to quantify the uncertainty in each of the gradients dimensions, which can then be factored in when aggregating them. We empirically demonstrate the
    
[^18]: 细调强化学习模型暗地里是一种遗忘缓解问题

    Fine-tuning Reinforcement Learning Models is Secretly a Forgetting Mitigation Problem

    [https://arxiv.org/abs/2402.02868](https://arxiv.org/abs/2402.02868)

    细调强化学习模型中的遗忘问题会导致转移效果差，研究发现常见且具有灾难性后果。通过使用标准的知识保留技术可以缓解这个问题并最大程度地利用细调的优势。

    

    细调是一种广泛应用的技术，允许从预训练模型中转移能力，最近基础模型的成功应用就证明了这一点。然而，细调强化学习（RL）模型仍然是一个挑战。本研究从动作和观察之间的相互作用的角度，将细调阶段未访问到的下游任务状态子空间中的预训练能力遗忘问题作为导致转移效果差的一个具体原因进行了概念化。模型在这个未访问到的状态子空间中的表现良好，但由于预训练使其失去了期望的转移优势。我们确定了该问题发生的条件，表明它是普遍存在的，并且在许多情况下是灾难性的。通过对具有挑战性的NetHack和Montezuma's Revenge环境进行详细的经验分析，我们展示了标准的知识保留技术如何缓解这个问题，从而使我们能充分利用细调的优势。

    Fine-tuning is a widespread technique that allows practitioners to transfer pre-trained capabilities, as recently showcased by the successful applications of foundation models. However, fine-tuning reinforcement learning (RL) models remains a challenge. This work conceptualizes one specific cause of poor transfer, accentuated in the RL setting by the interplay between actions and observations: forgetting of pre-trained capabilities. Namely, a model deteriorates on the state subspace of the downstream task not visited in the initial phase of fine-tuning, on which the model behaved well due to pre-training. This way, we lose the anticipated transfer benefits. We identify conditions when this problem occurs, showing that it is common and, in many cases, catastrophic. Through a detailed empirical analysis of the challenging NetHack and Montezuma's Revenge environments, we show that standard knowledge retention techniques mitigate the problem and thus allow us to take full advantage of the 
    
[^19]: 重新审视视觉调整中提示词的力量

    Revisiting the Power of Prompt for Visual Tuning

    [https://arxiv.org/abs/2402.02382](https://arxiv.org/abs/2402.02382)

    本研究提出了一种改进的视觉调整方法，通过采用下游令牌原型对提示进行初始化，进一步优化令牌构造，有效解决了视觉提示调整中的挑战，并取得了比其他方法更好的性能。

    

    视觉提示调整（VPT）是一种很有前景的解决方案，它利用可学习的提示词来定制预训练模型，用于下游任务。然而，VPT及其变种经常遇到诸如提示初始化、提示长度和自监督预训练中性能不佳等挑战，阻碍了成功的上下文适应。本研究从探索训练过程中提示词与补丁令牌之间的相关性演变开始。受到提示令牌与补丁令牌之间往往具有高互信息的观察启发，我们提出利用下游令牌原型对提示进行初始化。该策略性初始化明显提高了微调性能，相比于VPT，无需增加计算开销，我们进一步优化令牌构造，使用简化的流程保持了出色的性能。详尽的实验表明，我们提出的方法胜过了其他方法。

    Visual prompt tuning (VPT) is a promising solution incorporating learnable prompt tokens to customize pre-trained models for downstream tasks. However, VPT and its variants often encounter challenges like prompt initialization, prompt length, and subpar performance in self-supervised pretraining, hindering successful contextual adaptation. This study commences by exploring the correlation evolvement between prompts and patch tokens during proficient training. Inspired by the observation that the prompt tokens tend to share high mutual information with patch tokens, we propose initializing prompts with downstream token prototypes. The strategic initialization, a stand-in for the previous initialization, substantially improves performance in fine-tuning. To refine further, we optimize token construction with a streamlined pipeline that maintains excellent performance with almost no increase in computational expenses compared to VPT. Exhaustive experiments show our proposed approach outpe
    
[^20]: 数据高效图学习的综述

    A Survey of Data-Efficient Graph Learning

    [https://arxiv.org/abs/2402.00447](https://arxiv.org/abs/2402.00447)

    这项研究提出了数据高效图学习（DEGL）的概念，并总结了近期在这一领域的进展。DEGL的目标是在资源有限的场景下提高图机器学习的性能，通过探索各种最小监督方法来解决大规模标记数据的挑战。

    

    图结构化数据在社交网络到生物化学分析等领域中广泛存在，是各种现实世界系统的基础。虽然图神经网络在建模这种数据方面表现出色，但它们的成功往往依赖于大量标记数据，这在标注资源有限的实际场景中构成了挑战。为了解决这个问题，我们致力于通过探索各种最小监督方法来提高低资源设置下的图机器学习性能。本文介绍了一种新颖的数据高效图学习(DEGL)的研究前沿，并提供了对DEGL当前进展的首次综述。我们首先强调了使用大规模标记数据训练模型所固有的挑战，为我们对DEGL的探索铺平了道路。接下来，我们从几个关键方面系统地回顾了这一主题的最新进展，其中包括...

    Graph-structured data, prevalent in domains ranging from social networks to biochemical analysis, serve as the foundation for diverse real-world systems. While graph neural networks demonstrate proficiency in modeling this type of data, their success is often reliant on significant amounts of labeled data, posing a challenge in practical scenarios with limited annotation resources. To tackle this problem, tremendous efforts have been devoted to enhancing graph machine learning performance under low-resource settings by exploring various approaches to minimal supervision. In this paper, we introduce a novel concept of Data-Efficient Graph Learning (DEGL) as a research frontier, and present the first survey that summarizes the current progress of DEGL. We initiate by highlighting the challenges inherent in training models with large labeled data, paving the way for our exploration into DEGL. Next, we systematically review recent advances on this topic from several key aspects, including 
    
[^21]: 数据集精简驱动的机器遗忘

    Dataset Condensation Driven Machine Unlearning

    [https://arxiv.org/abs/2402.00195](https://arxiv.org/abs/2402.00195)

    这篇论文通过引入数据集精简作为机器遗忘的重要组成部分，在图像分类中解决了持久的计算挑战，并改进了近似遗忘的计算复杂性。

    

    数据监管要求和注重隐私保护的机器学习当前的趋势强调了机器遗忘的重要性。通过重新训练忘记样本的补集来遗忘训练数据的朴素方法容易受到计算挑战的影响。这些挑战已经通过一系列属于机器遗忘范畴的技术得到了有效解决。然而，在处理持久性计算挑战与未遗忘模型的实用性和隐私性之间仍然存在不足。我们将其归因于从训练数据集的角度改进近似遗忘的计算复杂性方面的工作不足。在本文中，我们旨在填补这一空白，引入数据集精简作为机器遗忘在图像分类上的重要组成部分。为了实现这个目标，我们提出了新的数据集精简技术和创新的遗忘方案。

    The current trend in data regulation requirements and privacy-preserving machine learning has emphasized the importance of machine unlearning. The naive approach to unlearning training data by retraining over the complement of the forget samples is susceptible to computational challenges. These challenges have been effectively addressed through a collection of techniques falling under the umbrella of machine unlearning. However, there still exists a lack of sufficiency in handling persistent computational challenges in harmony with the utility and privacy of unlearned model. We attribute this to the lack of work on improving the computational complexity of approximate unlearning from the perspective of the training dataset. In this paper, we aim to fill this gap by introducing dataset condensation as an essential component of machine unlearning in the context of image classification. To achieve this goal, we propose new dataset condensation techniques and an innovative unlearning schem
    
[^22]: 更深还是更宽: 从Sobolev损失的最优泛化误差角度看

    Deeper or Wider: A Perspective from Optimal Generalization Error with Sobolev Loss

    [https://arxiv.org/abs/2402.00152](https://arxiv.org/abs/2402.00152)

    本文比较了更深的神经网络和更宽的神经网络在Sobolev损失的最优泛化误差方面的表现，研究发现神经网络的架构受多种因素影响，参数数量更多倾向于选择更宽的网络，而样本点数量和损失函数规则性更高倾向于选择更深的网络。

    

    构建神经网络的架构是机器学习界一个具有挑战性的追求，到底是更深还是更宽一直是一个持续的问题。本文探索了更深的神经网络（DeNNs）和具有有限隐藏层的更宽的神经网络（WeNNs）在Sobolev损失的最优泛化误差方面的比较。通过分析研究发现，神经网络的架构可以受到多种因素的显著影响，包括样本点的数量，神经网络内的参数以及损失函数的规则性。具体而言，更多的参数倾向于选择WeNNs，而更多的样本点和更高的损失函数规则性倾向于选择DeNNs。最后，我们将这个理论应用于使用深度Ritz和物理感知神经网络（PINN）方法解决偏微分方程的问题。

    Constructing the architecture of a neural network is a challenging pursuit for the machine learning community, and the dilemma of whether to go deeper or wider remains a persistent question. This paper explores a comparison between deeper neural networks (DeNNs) with a flexible number of layers and wider neural networks (WeNNs) with limited hidden layers, focusing on their optimal generalization error in Sobolev losses. Analytical investigations reveal that the architecture of a neural network can be significantly influenced by various factors, including the number of sample points, parameters within the neural networks, and the regularity of the loss function. Specifically, a higher number of parameters tends to favor WeNNs, while an increased number of sample points and greater regularity in the loss function lean towards the adoption of DeNNs. We ultimately apply this theory to address partial differential equations using deep Ritz and physics-informed neural network (PINN) methods,
    
[^23]: 通过函数编码器实现零-shot强化学习

    Zero-Shot Reinforcement Learning via Function Encoders

    [https://arxiv.org/abs/2401.17173](https://arxiv.org/abs/2401.17173)

    本论文提出了一种用于实现零-shot迁移的函数编码器，通过将函数表示为学习到的非线性基函数的加权组合，代理程序通过连贯的向量表示了当前任务与先前看到任务的关联信息，从而实现了在相关任务之间的迁移，无需额外训练。

    

    尽管强化学习（RL）可以解决许多具有挑战性的序列决策问题，但在相关任务之间实现零-shot迁移仍然是一个挑战。难点在于寻找一个良好的表示来表达当前任务，以便代理程序理解它与先前看到的任务的关系。为了实现零-shot迁移，我们引入了函数编码器，一种表示学习算法，它将函数表示为学习到的非线性基函数的加权组合。通过使用函数编码器来表示奖励函数或转移函数，代理程序通过一个连贯的向量表示有关当前任务与先前看到的任务的关联信息。因此，代理能够在运行时在相关任务之间实现迁移，而无需进行额外的训练。通过将基本RL算法与函数编码器结合，我们在三个RL领域中展示了最先进的数据效率、渐近性能和训练稳定性。

    Although reinforcement learning (RL) can solve many challenging sequential decision making problems, achieving zero-shot transfer across related tasks remains a challenge. The difficulty lies in finding a good representation for the current task so that the agent understands how it relates to previously seen tasks. To achieve zero-shot transfer, we introduce the function encoder, a representation learning algorithm which represents a function as a weighted combination of learned, non-linear basis functions. By using a function encoder to represent the reward function or the transition function, the agent has information on how the current task relates to previously seen tasks via a coherent vector representation. Thus, the agent is able to achieve transfer between related tasks at run time with no additional training. We demonstrate state-of-the-art data efficiency, asymptotic performance, and training stability in three RL fields by augmenting basic RL algorithms with a function encod
    
[^24]: 选取完全随机的互补标签是多类别分类的实用弱监督方法

    The Selected-completely-at-random Complementary Label is a Practical Weak Supervision for Multi-class Classification

    [https://arxiv.org/abs/2311.15502](https://arxiv.org/abs/2311.15502)

    提出了一种不依赖于均匀分布假设的互补标签学习方法，基于完全随机选择假设的无偏风险估计器，以及风险校正方法来解决过拟合问题。

    

    互补标签学习是一个弱监督学习问题，每个训练样本关联着一个或多个互补标签，指示其不属于的类别。现有的一致方法依赖于均匀分布假设来模拟互补标签的生成，或者依赖于一个普通标签的训练集来估计非均匀情况下的转移矩阵。然而，实际情况下这两个条件可能不会被满足。在本文中，我们提出了一种新颖的一致方法，不依赖于这些条件。受到PU学习文献的启发，我们提出了基于完全随机选择假设的无偏风险估计器，用于互补标签学习。然后，我们引入了一种风险校正方法来解决过拟合问题。此外，我们发现互补标签学习可以被表达为一组...

    arXiv:2311.15502v2 Announce Type: replace  Abstract: Complementary-label learning is a weakly supervised learning problem in which each training example is associated with one or multiple complementary labels indicating the classes to which it does not belong. Existing consistent approaches have relied on the uniform distribution assumption to model the generation of complementary labels, or on an ordinary-label training set to estimate the transition matrix in non-uniform cases. However, either condition may not be satisfied in real-world scenarios. In this paper, we propose a novel consistent approach that does not rely on these conditions. Inspired by the positive-unlabeled (PU) learning literature, we propose an unbiased risk estimator based on the Selected Completely At Random assumption for complementary-label learning. We then introduce a risk-correction approach to address overfitting problems. Furthermore, we find that complementary-label learning can be expressed as a set of 
    
[^25]: 基于得分的扩散在潜在空间中生成混合类型的表格数据

    Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space

    [https://arxiv.org/abs/2310.09656](https://arxiv.org/abs/2310.09656)

    本文介绍了一种名为Tabsyn的方法，利用潜在空间中的扩散模型和变分自编码器生成具有混合类型数据的表格数据。Tabsyn具有通用性、质量和速度等关键优势，通过将不同类型的数据转换为统一空间并捕捉列间关系，优化潜在嵌入的分布来生成高质量的合成数据，同时生成速度更快。

    

    最近在表格数据生成领域的进展极大地提高了合成数据的质量。然而，由于表格数据的复杂多样的分布和融合的数据类型，将扩散模型扩展到表格数据是具有挑战性的。本文介绍了Tabsyn，一种利用潜在空间中的扩散模型和变分自编码器（VAE）生成表格数据的方法。所提出的Tabsyn的主要优势包括：（1）通用性：能够处理各种数据类型，并将其转换为单一的统一空间，并明确捕捉列间关系；（2）质量：通过优化潜在嵌入的分布以增强后续扩散模型的训练，从而生成高质量的合成数据；（3）速度：比现有的基于扩散的方法具有更少的反向步骤和更快的生成速度。对于六个数据集的大量实验证明，Tabsyn优于现有的方法。

    Recent advances in tabular data generation have greatly enhanced synthetic data quality. However, extending diffusion models to tabular data is challenging due to the intricately varied distributions and a blend of data types of tabular data. This paper introduces Tabsyn, a methodology that synthesizes tabular data by leveraging a diffusion model within a variational autoencoder (VAE) crafted latent space. The key advantages of the proposed Tabsyn include (1) Generality: the ability to handle a broad spectrum of data types by converting them into a single unified space and explicitly capture inter-column relations; (2) Quality: optimizing the distribution of latent embeddings to enhance the subsequent training of diffusion models, which helps generate high-quality synthetic data, (3) Speed: much fewer number of reverse steps and faster synthesis speed than existing diffusion-based methods. Extensive experiments on six datasets with five metrics demonstrate that Tabsyn outperforms exist
    
[^26]: 揭示大语言模型知识编辑的陷阱

    Unveiling the Pitfalls of Knowledge Editing for Large Language Models

    [https://arxiv.org/abs/2310.02129](https://arxiv.org/abs/2310.02129)

    这篇论文探讨了大型语言模型知识编辑的潜在陷阱，提出了新的评估方法，发现知识冲突和知识扭曲是两个重要问题。

    

    随着调整大型语言模型（LLMs）成本不断上升，最近的研究工作已经转向开发编辑LLMs内在知识的方法。然而，仍有一个阴云悬在头顶上 - 知识编辑是否会触发蝴蝶效应？因为目前尚不清楚知识编辑是否会引入可能带来潜在风险的副作用。本文首次探讨了与LLMs知识编辑相关的潜在陷阱。为实现此目的，我们引入了新的基准数据集并提出了创新性的评估指标。我们的结果强调了两个关键问题：（1）知识冲突：编辑逻辑冲突的事实组可能会放大LLMs固有的不一致性 - 这是以前方法忽略的一个方面。（2）知识扭曲：为了编辑事实知识而更改参数可能会不可逆地扭曲

    arXiv:2310.02129v3 Announce Type: replace-cross  Abstract: As the cost associated with fine-tuning Large Language Models (LLMs) continues to rise, recent research efforts have pivoted towards developing methodologies to edit implicit knowledge embedded within LLMs. Yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not. This paper pioneers the investigation into the potential pitfalls associated with knowledge editing for LLMs. To achieve this, we introduce new benchmark datasets and propose innovative evaluation metrics. Our results underline two pivotal concerns: (1) Knowledge Conflict: Editing groups of facts that logically clash can magnify the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2) Knowledge Distortion: Altering parameters with the aim of editing factual knowledge can irrevocably warp 
    
[^27]: OMPGPT: 一种用于OpenMP的生成式预训练Transformer模型

    OMPGPT: A Generative Pre-trained Transformer Model for OpenMP. (arXiv:2401.16445v1 [cs.SE])

    [http://arxiv.org/abs/2401.16445](http://arxiv.org/abs/2401.16445)

    OMPGPT是一种为了OpenMP pragma生成而设计的生成式预训练Transformer模型，采用了来自NLP领域的提示工程技术，并创建了一种创新的策略chain-of-OMP。

    

    大型语言模型（LLMs），如ChatGPT等模型，已经在自然语言处理领域引起了革命。随着这一趋势，基于代码的大型语言模型，如StarCoder、WizardCoder和CodeLlama等，已经涌现出来，在大量的代码数据库上进行了广泛的训练。然而，由于设计固有的原因，这些模型主要关注代码生成、代码完成和注释生成等生成任务，以及对多种编程语言的一般支持。虽然代码LLMs的通用能力对许多程序员来说很有用，但高性能计算（HPC）领域具有更窄的需求集，使得更小、更具领域特定的LM成为一个更明智的选择。本文介绍了OMPGPT，这是一种精心设计的新型模型，旨在充分利用语言模型在OpenMP pragma生成方面的固有优势。此外，我们采用并改进了来自NLP领域的提示工程技术，创建了链式OMP（chain-of-OMP），这是一种创新策略。

    Large language models (LLMs), as epitomized by models like ChatGPT, have revolutionized the field of natural language processing (NLP). Along with this trend, code-based large language models such as StarCoder, WizardCoder, and CodeLlama have emerged, trained extensively on vast repositories of code data. Yet, inherent in their design, these models primarily focus on generative tasks like code generation, code completion, and comment generation, and general support for multiple programming languages. While the generic abilities of code LLMs are useful for many programmers, the area of high-performance computing (HPC) has a narrower set of requirements that make a smaller and more domain-specific LM a smarter choice. This paper introduces OMPGPT, a novel model meticulously designed to harness the inherent strengths of language models for OpenMP pragma generation. Furthermore, we adopt and adapt prompt engineering techniques from the NLP domain to create chain-of-OMP, an innovative strat
    
[^28]: CompactifAI: 使用量子启发的张量网络对大型语言模型进行极压缩

    CompactifAI: Extreme Compression of Large Language Models using Quantum-Inspired Tensor Networks. (arXiv:2401.14109v1 [cs.CL])

    [http://arxiv.org/abs/2401.14109](http://arxiv.org/abs/2401.14109)

    CompactifAI是一种使用量子启发的张量网络对大型语言模型进行极压缩的创新方法，相比于传统的压缩方法，它更注重模型的相关空间，实现更加可控和精细的压缩。

    

    大型语言模型（LLM）如ChatGPT和LlaMA在生成人工智能（AI）方面取得了快速进展，但其庞大的规模带来了重要挑战，如巨大的训练和推断成本、较大的能源需求以及现场部署的限制。传统的压缩方法如剪枝、蒸馏和低秩逼近主要关注减少网络中神经元的有效数量，而量化方法则侧重于降低单个权重的数值精度，以减小模型大小同时保持神经元数目不变。虽然这些压缩方法在实践中取得了相对成功，但没有令人信服的理由认为截断神经元的数量是一种最优策略。本文介绍了一种创新的LLM压缩方法CompactifAI，它使用量子启发的张量网络，而不是传统的压缩方法，更注重模型的相关空间，实现更加可控和精细的压缩。

    Large Language Models (LLMs) such as ChatGPT and LlaMA are advancing rapidly in generative Artificial Intelligence (AI), but their immense size poses significant challenges, such as huge training and inference costs, substantial energy demands, and limitations for on-site deployment. Traditional compression methods such as pruning, distillation, and low-rank approximation focus on reducing the effective number of neurons in the network, while quantization focuses on reducing the numerical precision of individual weights to reduce the model size while keeping the number of neurons fixed. While these compression methods have been relatively successful in practice, there's no compelling reason to believe that truncating the number of neurons is an optimal strategy. In this context, this paper introduces CompactifAI, an innovative LLM compression approach using quantum-inspired Tensor Networks that focuses on the model's correlation space instead, allowing for a more controlled, refined an
    
[^29]: LocMoE: 一种用于大型语言模型训练的低开销MoE

    LocMoE: A Low-overhead MoE for Large Language Model Training. (arXiv:2401.13920v1 [cs.LG])

    [http://arxiv.org/abs/2401.13920](http://arxiv.org/abs/2401.13920)

    LocMoE提出了一种新的路由策略，通过将部分节点间通信转换为节点内通信，结合负载平衡和局部性，以提高大型语言模型训练的性能。

    

    混合专家模型（MoE）是一种广泛采用的分布式和集成学习方法，用于大型语言模型（LLM），由于其能够有效稀疏和扩展模型，因此备受青睐。然而，MoE的性能受到负载不平衡和全对全通信的高延迟的限制，同时由于大量的专家容量导致相对冗余的计算。负载不平衡可能是由于现有路由策略始终倾向于选择特定的专家导致的。全对全过程中频繁的节点间通信也显著延长了训练时间。为了缓解上述性能问题，我们提出了一种新的路由策略，通过将部分节点间通信转换为节点内通信，结合负载平衡和局部性。值得注意的是，我们阐明了专家容量的最小阈值，通过将专家的门控权重与分配的标记之间的最大角偏差计算出来。

    The Mixtures-of-Experts (MoE) model is a widespread distributed and integrated learning method for large language models (LLM), which is favored due to its ability to sparsify and expand models efficiently. However, the performance of MoE is limited by load imbalance and high latency of All-To-All communication, along with relatively redundant computation owing to large expert capacity. Load imbalance may result from existing routing policies that consistently tend to select certain experts. The frequent inter-node communication in the All-To-All procedure also significantly prolongs the training time. To alleviate the above performance problems, we propose a novel routing strategy that combines load balance and locality by converting partial inter-node communication to that of intra-node. Notably, we elucidate that there is a minimum threshold for expert capacity, calculated through the maximal angular deviation between the gating weights of the experts and the assigned tokens. We por
    
[^30]: 基于Split Learning的肌电假肢控制中的收敛速率最大化

    Convergence Rate Maximization for Split Learning-based Control of EMG Prosthetic Devices. (arXiv:2401.03233v1 [cs.LG])

    [http://arxiv.org/abs/2401.03233](http://arxiv.org/abs/2401.03233)

    本文介绍了一种用于最大化模型收敛速率的Split Learning肌电假肢控制中的切层选择算法，通过加速收敛过程提高了假肢控制的性能。

    

    Split Learning (SL)是一种有前途的分布式学习方法，可以在资源有限的环境中应用于基于肌电的假肢控制。与深度学习和联邦学习等其他学习方法相比，SL能够提供更优的解决方案，因为假肢设备在处理能力和电池寿命方面非常有限。在这些情况下，实现SL的可行性源于其固有的模型分割，其中客户端执行较小的模型部分。然而，选择不恰当的切层会阻碍SL系统的训练过程。本文提出了一种用于最大化模型收敛速率的切层选择算法。性能评估表明，所提出的算法在改善假肢控制的肌电模式识别任务中显著加速了收敛过程。

    Split Learning (SL) is a promising Distributed Learning approach in electromyography (EMG) based prosthetic control, due to its applicability within resource-constrained environments. Other learning approaches, such as Deep Learning and Federated Learning (FL), provide suboptimal solutions, since prosthetic devices are extremely limited in terms of processing power and battery life. The viability of implementing SL in such scenarios is caused by its inherent model partitioning, with clients executing the smaller model segment. However, selecting an inadequate cut layer hinders the training process in SL systems. This paper presents an algorithm for optimal cut layer selection in terms of maximizing the convergence rate of the model. The performance evaluation demonstrates that the proposed algorithm substantially accelerates the convergence in an EMG pattern recognition task for improving prosthetic device control.
    
[^31]: 使用傅里叶神经算子逼近数值通量的超波古典守恒定律

    Approximating Numerical Flux by Fourier Neural Operators for the Hyperbolic Conservation Laws. (arXiv:2401.01783v1 [math.NA])

    [http://arxiv.org/abs/2401.01783](http://arxiv.org/abs/2401.01783)

    该研究通过在传统的数值方案中运用神经网络来替换数值通量，解决了神经网络方法缺乏鲁棒性和泛化能力的问题，并展示了该方法在超声古典守恒定律方面具有优势。

    

    传统的数值方案用于数值解PDE，最近发展了基于神经网络的方法。然而，使用神经网络的方法，如PINN和神经算子，缺乏鲁棒性和泛化能力。为了弥补这些缺点，有很多种类型的研究将传统的数值方案和机器学习方法结合起来，通过用神经网络替代数值方案中的一小部分来实现。在本文中，我们专注于超声古典守恒定律，将数值方案中的数值通量替换为神经算子。为此，我们构造了受数值方案启发的损失函数，并通过FNO逼近数值通量。通过实验证明，我们的方法通过与原始方法的比较具有数值方案和FNO的优势。例如，我们演示了我们的方法具有鲁棒性，分辨率不变性和数据驱动方法的可行性。

    Classical numerical schemes exist for solving PDEs numerically, and recently, neural network-based methods have been developed. However, methodologies using neural networks, such as PINN and neural operators, lack robustness and generalization power. To compensate for such drawbacks, there are many types of research combining classical numerical schemes and machine learning methods by replacing a small portion of the numerical schemes with neural networks. In this work, we focus on hyperbolic conservation laws and replace numerical fluxes in the numerical schemes by neural operator. For this, we construct losses that are motivated by numerical schemes for conservation laws and approximate numerical flux by FNO. Through experiments, we show that our methodology has advantages of both numerical schemes and FNO by comparing with original methods. For instance, we demonstrate our method gains robustness, resolution invariance property, and feasibility of a data-driven method. Our method es
    
[^32]: 随机逼近的收敛速度：带有无界方差的有偏噪声和应用

    Convergence Rates for Stochastic Approximation: Biased Noise with Unbounded Variance, and Applications. (arXiv:2312.02828v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2312.02828](http://arxiv.org/abs/2312.02828)

    本论文研究了带有无界方差的有偏噪声对随机逼近算法的收敛速度的影响，并介绍了该算法在各个领域的应用。

    

    1951年罗宾斯和莫洛引入的随机逼近（SA）算法已经成为解方程$\mathbf{f}({\boldsymbol{\theta}}) = \mathbf{0}$的标准方法，当只有$\mathbf{f}(\cdot)$的带噪声测量可用时。如果对于某个函数$J(\cdot)$，$\mathbf{f}({\boldsymbol{\theta}}) = \nabla J({\boldsymbol{\theta}})$，那么SA也可以用来寻找$J(\cdot)$的一个稳定点。在每个时间$t$，当前的猜测${\boldsymbol{\theta}}_t$通过形式为$\mathbf{f}({\boldsymbol{\theta}}_t) + {\boldsymbol{\xi}}_{t+1}$的带噪声测量更新为${\boldsymbol{\theta}}_{t+1}$。在许多文献中，假设误差项${\boldsymbol{\xi}}_{t+1}$的条件均值为零，和/或者它的条件方差随$t$（而不是${\boldsymbol{\theta}}_t$）被限制。多年来，SA已经应用于各种领域，本文重点研究其中一个领域。

    The Stochastic Approximation (SA) algorithm introduced by Robbins and Monro in 1951 has been a standard method for solving equations of the form $\mathbf{f}({\boldsymbol {\theta}}) = \mathbf{0}$, when only noisy measurements of $\mathbf{f}(\cdot)$ are available. If $\mathbf{f}({\boldsymbol {\theta}}) = \nabla J({\boldsymbol {\theta}})$ for some function $J(\cdot)$, then SA can also be used to find a stationary point of $J(\cdot)$. At each time $t$, the current guess ${\boldsymbol {\theta}}_t$ is updated to ${\boldsymbol {\theta}}_{t+1}$ using a noisy measurement of the form $\mathbf{f}({\boldsymbol {\theta}}_t) + {\boldsymbol {\xi}}_{t+1}$. In much of the literature, it is assumed that the error term ${\boldsymbol {\xi}}_{t+1}$ has zero conditional mean, and/or that its conditional variance is bounded as a function of $t$ (though not necessarily with respect to ${\boldsymbol {\theta}}_t$). Over the years, SA has been applied to a variety of areas, out of which the focus in this paper i
    
[^33]: 机器学习模型地球科学系统建模中的质量保持感知器

    A Mass-Conserving-Perceptron for Machine Learning-Based Modeling of Geoscientific Systems. (arXiv:2310.08644v1 [cs.LG])

    [http://arxiv.org/abs/2310.08644](http://arxiv.org/abs/2310.08644)

    这篇论文提出了一种质量保持感知器（MCP）用于将物理-概念模型和机器学习模型结合起来建模地球科学系统，通过利用机器学习技术从数据中学习物理过程的功能性和质量保持性。

    

    虽然数十年来致力于构建用于预测地球科学系统时间序列演化的物理-概念 (PC) 模型，但最近的研究表明，基于机器学习 (ML) 的门控循环神经网络技术可以用于开发更准确的模型。然而，从ML基础模型中提取物理理解的困难使得其在增强对系统结构和功能的科学知识方面的应用变得复杂。在这里，我们提出了一个理解物理性的质量保持感知器 (MCP) 作为弥合PC模型和ML模型的方法。MCP利用PC模型和GRNNs背后的有向图结构的内在同构性，以可解释的方式明确表示物理过程的质量保持性质，同时利用现有数据和现成的ML技术直接学习这种过程的功能性（可解释性）。

    Although decades of effort have been devoted to building Physical-Conceptual (PC) models for predicting the time-series evolution of geoscientific systems, recent work shows that Machine Learning (ML) based Gated Recurrent Neural Network technology can be used to develop models that are much more accurate. However, the difficulty of extracting physical understanding from ML-based models complicates their utility for enhancing scientific knowledge regarding system structure and function. Here, we propose a physically-interpretable Mass Conserving Perceptron (MCP) as a way to bridge the gap between PC-based and ML-based modeling approaches. The MCP exploits the inherent isomorphism between the directed graph structures underlying both PC models and GRNNs to explicitly represent the mass-conserving nature of physical processes while enabling the functional nature of such processes to be directly learned (in an interpretable manner) from available data using off-the-shelf ML technology. As
    
[^34]: 具有相位随机桥的生成建模

    Generative Modeling with Phase Stochastic Bridges. (arXiv:2310.07805v1 [cs.LG])

    [http://arxiv.org/abs/2310.07805](http://arxiv.org/abs/2310.07805)

    通过在相位空间中构建路径测度，我们提出了一种新颖的生成建模框架，可以在动力传播的早期阶段生成逼真的数据点，并利用额外的速度信息实现高效的数据生成。

    

    扩散模型（DMs）是用于连续输入的最先进的生成模型。DMs通过在输入空间（即位置空间）中构建随机微分方程（SDE），并使用神经网络进行反演来工作。在这项工作中，我们介绍了一种基于相位空间动力学的新型生成建模框架，其中相位空间被定义为一个包括位置和速度的增强空间。利用随机最优控制的洞察力，我们构建了相位空间中的路径测度，实现了高效的采样。与DMs相比，我们的框架在动力传播的早期阶段就能够生成逼真的数据点。这种早期预测为通过沿轨迹利用额外的速度信息实现高效的数据生成奠定了基础。在标准图像生成基准测试中，我们的模型在小函数评估数量的范围内表现出优秀的性能。

    Diffusion models (DMs) represent state-of-the-art generative models for continuous inputs. DMs work by constructing a Stochastic Differential Equation (SDE) in the input space (ie, position space), and using a neural network to reverse it. In this work, we introduce a novel generative modeling framework grounded in \textbf{phase space dynamics}, where a phase space is defined as {an augmented space encompassing both position and velocity.} Leveraging insights from Stochastic Optimal Control, we construct a path measure in the phase space that enables efficient sampling. {In contrast to DMs, our framework demonstrates the capability to generate realistic data points at an early stage of dynamics propagation.} This early prediction sets the stage for efficient data generation by leveraging additional velocity information along the trajectory. On standard image generation benchmarks, our model yields favorable performance over baselines in the regime of small Number of Function Evaluation
    
[^35]: ChatGPT中的性别偏见有多普遍？—— 探索德语和英语ChatGPT的回应

    How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English ChatGPT Responses. (arXiv:2310.03031v1 [cs.CL])

    [http://arxiv.org/abs/2310.03031](http://arxiv.org/abs/2310.03031)

    本文系统地分析并探索了德语和英语的ChatGPT回应中可能存在的问题，特别关注了性别偏见。我们发现，在对系统多次提供相同指令的情况下，回应存在差异。使用ChatGPT来帮助非IT用户撰写工作文本非常有用，但用户需要充分考虑系统的固有限制。

    

    随着ChatGPT的推出，OpenAI使得大型语言模型（LLM）可供具有有限IT专业知识的用户使用。然而，没有自然语言处理（NLP）背景的用户可能缺乏对LLM的适当理解。因此，在处理系统输出时，缺乏对其固有限制的意识，将接受系统输出的表面价值。在本文中，我们系统地分析输入提示和生成的回应，以识别可能存在的问题，特别关注性别偏见问题，用户在处理系统输出时需要意识到这一点。我们探索了ChatGPT在英语和德语中的反应，并提供了女性、男性或中立角度的指令时，回复的是否有差异。通过深入调查，我们研究了一些选择的提示，并分析了系统在相同方式下多次提供指令时回应的差异程度。在此基础上，我们展示了对于帮助非IT用户撰写日常工作文本，ChatGPT确实非常有用。然而，当然至关重要的是要意识到，当处理系统输出时，用户需要充分考虑到其固有限制。

    With the introduction of ChatGPT, OpenAI made large language models (LLM) accessible to users with limited IT expertise. However, users with no background in natural language processing (NLP) might lack a proper understanding of LLMs. Thus the awareness of their inherent limitations, and therefore will take the systems' output at face value. In this paper, we systematically analyse prompts and the generated responses to identify possible problematic issues with a special focus on gender biases, which users need to be aware of when processing the system's output. We explore how ChatGPT reacts in English and German if prompted to answer from a female, male, or neutral perspective. In an in-depth investigation, we examine selected prompts and analyse to what extent responses differ if the system is prompted several times in an identical way. On this basis, we show that ChatGPT is indeed useful for helping non-IT users draft texts for their daily work. However, it is absolutely crucial to 
    
[^36]: 通过Koopman VAEs生成规则和非规则时间序列数据的生成建模

    Generative Modeling of Regular and Irregular Time Series Data via Koopman VAEs. (arXiv:2310.02619v1 [cs.LG])

    [http://arxiv.org/abs/2310.02619](http://arxiv.org/abs/2310.02619)

    本文介绍了一种基于Koopman VAEs的新型生成框架，可以用于生成规则和非规则时间序列数据，解决了GANs训练不稳定和模式崩溃的问题，并通过利用谱工具对线性映射的特征值施加约束，实现了领域知识的整合和对定性行为和稳定性的研究。

    

    生成真实的时间序列数据对于许多工程和科学应用非常重要。现有的工作使用生成对抗网络（GANs）来解决这个问题。然而，GANs在训练过程中常常不稳定，并且可能出现模式崩溃的问题。而变分自动编码器（VAEs）被认为对这些问题更具鲁棒性，但却很少被考虑用于时间序列生成。在这项工作中，我们引入了基于新型模型先验的生成框架Koopman VAE（KVAE），可以为规则和非规则训练数据进行优化。受Koopman理论的启发，我们使用线性映射来表示潜在条件先验动态。我们的方法增强了生成建模的两个期望特性：（i）通过利用谱工具对线性映射的特征值施加约束，可以实现领域知识的整合；（ii）研究定性行为和稳定性。

    Generating realistic time series data is important for many engineering and scientific applications. Existing work tackles this problem using generative adversarial networks (GANs). However, GANs are often unstable during training, and they can suffer from mode collapse. While variational autoencoders (VAEs) are known to be more robust to these issues, they are (surprisingly) less often considered for time series generation. In this work, we introduce Koopman VAE (KVAE), a new generative framework that is based on a novel design for the model prior, and that can be optimized for either regular and irregular training data. Inspired by Koopman theory, we represent the latent conditional prior dynamics using a linear map. Our approach enhances generative modeling with two desired features: (i) incorporating domain knowledge can be achieved by leverageing spectral tools that prescribe constraints on the eigenvalues of the linear map; and (ii) studying the qualitative behavior and stablity 
    
[^37]: zkFL: 基于零知识证明的联邦学习梯度聚合

    zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated Learning. (arXiv:2310.02554v1 [cs.AI])

    [http://arxiv.org/abs/2310.02554](http://arxiv.org/abs/2310.02554)

    zkFL是一种基于零知识证明的联邦学习梯度聚合方法，通过提供每轮的证明来解决协调者恶意行为的问题。

    

    联邦学习是一种机器学习范式，使多个分散的客户端在中央协调者的组织下共同训练一个模型。传统的联邦学习解决方案依赖于对中央协调者的信任，它以公平诚实的方式形成客户端的群体。然而，在现实中，恶意的协调者可能会放弃并替换客户端的训练模型，或者发动虚假客户端的肆意攻击。这种恶意行为让协调者在联邦学习环境中拥有更多控制客户端和决定最终训练结果的权力。本文介绍了zkFL，它利用零知识证明(ZKPs)来解决训练模型聚合过程中的恶意协调者问题。为了保证正确的聚合结果，协调者需要每轮提供一个证明。这个证明可以向客户端证明协调者忠实执行预期行为。为了进一步保护客户端隐私和数据安全，我们还引入了差分隐私机制，并对zkFL进行了实验评估。

    Federated Learning (FL) is a machine learning paradigm, which enables multiple and decentralized clients to collaboratively train a model under the orchestration of a central aggregator. Traditional FL solutions rely on the trust assumption of the centralized aggregator, which forms cohorts of clients in a fair and honest manner. However, a malicious aggregator, in reality, could abandon and replace the client's training models, or launch Sybil attacks to insert fake clients. Such malicious behaviors give the aggregator more power to control clients in the FL setting and determine the final training results. In this work, we introduce zkFL, which leverages zero-knowledge proofs (ZKPs) to tackle the issue of a malicious aggregator during the training model aggregation process. To guarantee the correct aggregation results, the aggregator needs to provide a proof per round. The proof can demonstrate to the clients that the aggregator executes the intended behavior faithfully. To further r
    
[^38]: G4SATBench: 用图神经网络评估和改进SAT求解的基准研究

    G4SATBench: Benchmarking and Advancing SAT Solving with Graph Neural Networks. (arXiv:2309.16941v1 [cs.LG])

    [http://arxiv.org/abs/2309.16941](http://arxiv.org/abs/2309.16941)

    G4SATBench是第一个为基于图神经网络的SAT求解器提供全面评估框架的基准研究。它提供了一个大而多样的SAT数据集，并基于各种预测任务、训练目标和推理算法对广泛的GNN模型进行了测试和比较。通过与搜索型SAT求解器中的启发式方法进行比较，它还揭示了基于GNN的SAT求解器的学习能力以及其优缺点。

    

    最近，图神经网络(GNNs)作为解决布尔可满足性问题(SAT)的一种有希望的方法出现了，为传统的回溯或局部搜索SAT求解器提供了潜在的替代方案。然而，尽管这个领域的文献数量不断增长，但仍然缺乏一个统一的数据集和公正的基准来评估和比较现有的方法。为了填补这个关键的空白，我们提出了G4SATBench，这是第一个为基于GNN的SAT求解器建立全面评估框架的基准研究。在G4SATBench中，我们精心策划了一个包含7个问题和3个难度级别的大型多样化SAT数据集，并在各种预测任务、训练目标和推理算法下对广泛的GNN模型进行基准测试。为了探索基于GNN的SAT求解器的学习能力，并理解其优点和局限性，我们还将它们的求解过程与基于搜索的SAT求解器中的启发式方法进行比较。

    Graph neural networks (GNNs) have recently emerged as a promising approach for solving the Boolean Satisfiability Problem (SAT), offering potential alternatives to traditional backtracking or local search SAT solvers. However, despite the growing volume of literature in this field, there remains a notable absence of a unified dataset and a fair benchmark to evaluate and compare existing approaches. To address this crucial gap, we present G4SATBench, the first benchmark study that establishes a comprehensive evaluation framework for GNN-based SAT solvers. In G4SATBench, we meticulously curate a large and diverse set of SAT datasets comprising 7 problems with 3 difficulty levels and benchmark a broad range of GNN models across various prediction tasks, training objectives, and inference algorithms. To explore the learning abilities and comprehend the strengths and limitations of GNN-based SAT solvers, we also compare their solving processes with the heuristics in search-based SAT solvers
    
[^39]: 多单元软测量允许少样本学习

    Multi-unit soft sensing permits few-shot learning. (arXiv:2309.15828v1 [stat.ML])

    [http://arxiv.org/abs/2309.15828](http://arxiv.org/abs/2309.15828)

    多单元软测量是利用可转移性学习算法改进软测量的一种方法，能够通过解决多个任务来增强软测量的性能，并且特别适用于具有多个实现的进程。

    

    近期的研究探索了利用具有可转移性的学习算法来改进软测量的各种方法。总体来说，当一个软测量通过解决多个任务来学习时，其性能可以得到加强。可转移性的有用性取决于所设计的学习任务的相关性。在软测量要应用于有多个实现的进程（例如，有多个可用数据的系统或设备）时，尤其相关。然后，每个实现都提供一个软测量学习任务，并且合理地期望这些不同任务之间具有强相关性。在这种设置中应用可转移性导致了我们所称的多单元软测量，其中软测量通过从所有实现的数据中学习来建模一个进程。本文探讨了多单元软测量的学习能力，它被构建为一个分层模型，并使用...

    Recent literature has explored various ways to improve soft sensors using learning algorithms with transferability. Broadly put, the performance of a soft sensor may be strengthened when it is learned by solving multiple tasks. The usefulness of transferability depends on how strongly related the devised learning tasks are. A particularly relevant case for transferability, is when a soft sensor is to be developed for a process of which there are many realizations, e.g. system or device with many implementations from which data is available. Then, each realization presents a soft sensor learning task, and it is reasonable to expect that the different tasks are strongly related. Applying transferability in this setting leads to what we call multi-unit soft sensing, where a soft sensor models a process by learning from data from all of its realizations.  This paper explores the learning abilities of a multi-unit soft sensor, which is formulated as a hierarchical model and implemented usin
    
[^40]: 通过测度传递进行密度估计：生物科学中的应用展望

    Density Estimation via Measure Transport: Outlook for Applications in the Biological Sciences. (arXiv:2309.15366v1 [q-bio.QM])

    [http://arxiv.org/abs/2309.15366](http://arxiv.org/abs/2309.15366)

    通过测度传递方法进行密度估计在生物科学中具有广阔的应用前景，尤其是在处理稀疏数据的情况下，使用稀疏传递映射可以揭示数据中隐藏的信息。

    

    测度传递方法的一个优势是其允许对根据广泛概率测度分布的数据进行统一的处理和分析。在这个框架下，我们通过计算研究的结果来评估测度传递技术的潜力，特别是三角传递映射的使用，作为支持生物科学研究的工作流的一部分。稀疏数据场景在辐射生物学等领域很常见。我们发现，在数据稀缺时，稀疏传递映射是有优势的。具体而言，通过计算一系列（稀疏的）自适应传递映射的统计信息，这些映射是在随机选择的一系列可用数据样本子集上进行训练的，可以揭示数据中隐藏的信息。因此，在本文考虑的辐射生物学应用中，此方法为生成假设提供了一个工具。

    One among several advantages of measure transport methods is that they allow for a unified framework for processing and analysis of data distributed according to a wide class of probability measures. Within this context, we present results from computational studies aimed at assessing the potential of measure transport techniques, specifically, the use of triangular transport maps, as part of a workflow intended to support research in the biological sciences. Scarce data scenarios, which are common in domains such as radiation biology, are of particular interest. We find that when data is scarce, sparse transport maps are advantageous. In particular, statistics gathered from computing series of (sparse) adaptive transport maps, trained on a series of randomly chosen subsets of the set of available data samples, leads to uncovering information hidden in the data. As a result, in the radiation biology application considered here, this approach provides a tool for generating hypotheses ab
    
[^41]: MoDem-V2: 面向真实世界机器人操作的视觉-运动世界模型

    MoDem-V2: Visuo-Motor World Models for Real-World Robot Manipulation. (arXiv:2309.14236v1 [cs.RO] CROSS LISTED)

    [http://arxiv.org/abs/2309.14236](http://arxiv.org/abs/2309.14236)

    MoDem-V2是一个能够在非仪器化的真实世界中直接学习接触丰富操作的系统。

    

    希望在非仪器化的真实世界环境中运行的机器人系统必须通过机载传感器直接感知世界。基于视觉的学习系统旨在通过基于原始像素的隐式对世界的理解，消除环境装置的需求，但仅仅依靠稀疏的视觉奖励信号在接触丰富的高维搜索空间中导航，显著加剧了探索的难度。因此，这种系统的适用性通常局限于模拟或严格工程化的环境，因为在没有明确的状态估计和稠密奖励的指导下，在真实世界中进行代理的探索可能导致不安全行为和重大安全故障。在本研究中，我们分离了这些限制的根本原因，开发了一个名为MoDem-V2的系统，能够直接在非仪器化的真实世界中学习接触丰富的操作。在最新的算法进展的基础上构建，

    Robotic systems that aspire to operate in uninstrumented real-world environments must perceive the world directly via onboard sensing. Vision-based learning systems aim to eliminate the need for environment instrumentation by building an implicit understanding of the world based on raw pixels, but navigating the contact-rich high-dimensional search space from solely sparse visual reward signals significantly exacerbates the challenge of exploration. The applicability of such systems is thus typically restricted to simulated or heavily engineered environments since agent exploration in the real-world without the guidance of explicit state estimation and dense rewards can lead to unsafe behavior and safety faults that are catastrophic. In this study, we isolate the root causes behind these limitations to develop a system, called MoDem-V2, capable of learning contact-rich manipulation directly in the uninstrumented real world. Building on the latest algorithmic advancements in model-based
    
[^42]: 完全嵌入的时间序列生成对抗网络

    Fully Embedded Time-Series Generative Adversarial Networks. (arXiv:2308.15730v1 [cs.LG])

    [http://arxiv.org/abs/2308.15730](http://arxiv.org/abs/2308.15730)

    这篇论文提出了一种完全嵌入的时间序列生成对抗网络（FETSGAN），通过对抗性训练匹配特征空间和低维度采样空间的训练分布，确保合成样本的时间分布不会崩溃。

    

    生成对抗网络（GANs）应产生与所建模数据的潜在分布相符的合成数据。针对实值时间序列数据，这意味着需要同时捕获数据的静态分布以及任何潜在时间范围的完整时间分布。这个时间要素产生了一个更复杂的问题，可能导致当前的解决方案受限、训练不稳定或易于发生模式崩溃。在完全嵌入的时间序列生成对抗网络（FETSGAN）中，整个序列通过一个seq2seq风格的对抗性自编码器（AAE）直接转换为生成器的采样空间，其中对抗性训练用于在特征空间和较低维度的采样空间匹配训练分布。这个额外的约束条件提供了对合成样本的时间分布不会崩溃的松散保证。此外，引入了第一个超过阈值（FAT）操作符。

    Generative Adversarial Networks (GANs) should produce synthetic data that fits the underlying distribution of the data being modeled. For real valued time-series data, this implies the need to simultaneously capture the static distribution of the data, but also the full temporal distribution of the data for any potential time horizon. This temporal element produces a more complex problem that can potentially leave current solutions under-constrained, unstable during training, or prone to varying degrees of mode collapse. In FETSGAN, entire sequences are translated directly to the generator's sampling space using a seq2seq style adversarial auto encoder (AAE), where adversarial training is used to match the training distribution in both the feature space and the lower dimensional sampling space. This additional constraint provides a loose assurance that the temporal distribution of the synthetic samples will not collapse. In addition, the First Above Threshold (FAT) operator is introduc
    
[^43]: 点击率预测的时间兴趣网络

    Temporal Interest Network for Click-Through Rate Prediction. (arXiv:2308.08487v1 [cs.IR])

    [http://arxiv.org/abs/2308.08487](http://arxiv.org/abs/2308.08487)

    本文提出了时间兴趣网络（TIN），用于捕捉行为与目标之间的四重语义和时间相关性，以预测点击率的效果和已有方法对这种相关性的学习程度尚不清楚。

    

    用户行为的历史是预测点击率最重要的特征之一，因为它们与目标项目具有强烈的语义和时间相关性。虽然已有文献分别研究了这些相关性，但尚未分析它们的组合，即行为语义、目标语义、行为时间和目标时间的四重相关性。这种相关性对性能的影响以及现有方法学习这种相关性的程度尚不清楚。为了填补这一空白，我们在实践中测量了四重相关性，并观察到直观而强大的四重模式。我们测量了几种代表性的用户行为方法的学习相关性，但令人惊讶的是，它们都没有学习到这样的模式，特别是时间模式。在本文中，我们提出了时间兴趣网络（TIN）来捕捉行为与目标之间的四重语义和时间相关性。

    The history of user behaviors constitutes one of the most significant characteristics in predicting the click-through rate (CTR), owing to their strong semantic and temporal correlation with the target item. While the literature has individually examined each of these correlations, research has yet to analyze them in combination, that is, the quadruple correlation of (behavior semantics, target semantics, behavior temporal, and target temporal). The effect of this correlation on performance and the extent to which existing methods learn it remain unknown. To address this gap, we empirically measure the quadruple correlation and observe intuitive yet robust quadruple patterns. We measure the learned correlation of several representative user behavior methods, but to our surprise, none of them learn such a pattern, especially the temporal one.  In this paper, we propose the Temporal Interest Network (TIN) to capture the quadruple semantic and temporal correlation between behaviors and th
    
[^44]: 离散切割Wasserstein损失的性质

    Properties of Discrete Sliced Wasserstein Losses. (arXiv:2307.10352v1 [stat.ML])

    [http://arxiv.org/abs/2307.10352](http://arxiv.org/abs/2307.10352)

    本文研究了离散切割Wasserstein损失的性质，并探讨了其正则性和优化性质以及通过蒙特卡洛近似的方法。

    

    切割Wasserstein（SW）距离已成为比较概率测度的Wasserstein距离的一种流行替代方法。广泛应用包括图像处理、领域自适应和生成建模，常常需要优化一些参数以最小化SW，该参数充当离散概率测度之间的损失函数（因为具有密度的测度在数值上是无法实现的）。所有这些优化问题都存在相同的子问题，即最小化切割Wasserstein能量。在本文中，我们研究了$\mathcal{E}: Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$的属性，即两个具有与一个测度的支撑相同数量的离散均匀测度之间的SW距离作为支撑$Y \in \mathbb{R}^{n \times d}$函数的能量。我们研究了这个能量的正则性和优化性质，以及其通过蒙特卡洛近似$\mathcal{E}_p$（使用SW中的期望估计）。

    The Sliced Wasserstein (SW) distance has become a popular alternative to the Wasserstein distance for comparing probability measures. Widespread applications include image processing, domain adaptation and generative modelling, where it is common to optimise some parameters in order to minimise SW, which serves as a loss function between discrete probability measures (since measures admitting densities are numerically unattainable). All these optimisation problems bear the same sub-problem, which is minimising the Sliced Wasserstein energy. In this paper we study the properties of $\mathcal{E}: Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$, i.e. the SW distance between two uniform discrete measures with the same amount of points as a function of the support $Y \in \mathbb{R}^{n \times d}$ of one of the measures. We investigate the regularity and optimisation properties of this energy, as well as its Monte-Carlo approximation $\mathcal{E}_p$ (estimating the expectation in SW using 
    
[^45]: DeepMem: 将机器学习模型用作存储通道及其（误用）应用

    DeepMem: ML Models as storage channels and their (mis-)applications. (arXiv:2307.08811v1 [cs.LG])

    [http://arxiv.org/abs/2307.08811](http://arxiv.org/abs/2307.08811)

    本文提出了将机器学习模型作为存储通道的新视角，通过过度参数化来增加通道容量。通过在训练时嵌入信息，并利用黑盒访问实现信息的存储和提取。

    

    机器学习（ML）模型为了支持通用性和避免过拟合而过度参数化。之前的研究表明，这些额外的参数既可以用于恶意目的（例如，在经过训练的模型中隐藏一个模型），也可以用于有益目的（例如，给模型加上水印）。在本文中，我们提出了一个新的信息论视角；我们将ML模型视为一个存储通道，其容量随着过度参数化而增加。具体而言，我们考虑一个发送方，在训练时将任意信息嵌入模型中，接收方可以通过对部署模型的黑盒访问来提取信息。我们根据可用参数的数量推导出通道容量的上界。然后，我们探索了黑盒写入和读取原语，允许攻击者：（i）通过在发射机端扩充训练数据的方式以优化地将数据存储在模型中，以及（ii）通过查询模型来读取数据。

    Machine learning (ML) models are overparameterized to support generality and avoid overfitting. Prior works have shown that these additional parameters can be used for both malicious (e.g., hiding a model covertly within a trained model) and beneficial purposes (e.g., watermarking a model). In this paper, we propose a novel information theoretic perspective of the problem; we consider the ML model as a storage channel with a capacity that increases with overparameterization. Specifically, we consider a sender that embeds arbitrary information in the model at training time, which can be extracted by a receiver with a black-box access to the deployed model. We derive an upper bound on the capacity of the channel based on the number of available parameters. We then explore black-box write and read primitives that allow the attacker to: (i) store data in an optimized way within the model by augmenting the training data at the transmitter side, and (ii) to read it by querying the model afte
    
[^46]: 直觉模糊广泛学习系统：增强对噪声和异常值的稳健性

    Intuitionistic Fuzzy Broad Learning System: Enhancing Robustness Against Noise and Outliers. (arXiv:2307.08713v1 [cs.LG])

    [http://arxiv.org/abs/2307.08713](http://arxiv.org/abs/2307.08713)

    该论文提出了一种直觉模糊广泛学习系统（IF-BLS），用于增强对噪声和异常值的稳健性，通过为每个训练点分配模糊隶属度来减小噪声和异常值的影响。

    

    在数据分类领域，广泛学习系统（BLS）已被证明是一种有效的工具，它利用逐层前馈神经网络。它包括特征学习和增强部分，共同从输入数据中提取复杂的特征。传统的BLS将所有样本视为同等重要，这使得它在处理带有噪声和异常值的真实数据集时不够稳健和有效。为了解决这个问题，我们提出了模糊BLS（F-BLS）模型，它为每个训练点分配模糊隶属度，以减小噪声和异常值的影响。在分配隶属度的过程中，F-BLS模型仅考虑样本到类别中心之间的距离，而不考虑不属于类别的程度。我们进一步提出了一种基于直觉模糊理论的新型BLS（IF-BLS）。所提出的IF-BLS利用基于模糊隶属度和非隶属度的直觉模糊数。

    In the realm of data classification, broad learning system (BLS) has proven to be a potent tool that utilizes a layer-by-layer feed-forward neural network. It consists of feature learning and enhancement segments, working together to extract intricate features from input data. The traditional BLS treats all samples as equally significant, which makes it less robust and less effective for real-world datasets with noises and outliers. To address this issue, we propose the fuzzy BLS (F-BLS) model, which assigns a fuzzy membership value to each training point to reduce the influence of noises and outliers. In assigning the membership value, the F-BLS model solely considers the distance from samples to the class center in the original feature space without incorporating the extent of non-belongingness to a class. We further propose a novel BLS based on intuitionistic fuzzy theory (IF-BLS). The proposed IF-BLS utilizes intuitionistic fuzzy numbers based on fuzzy membership and non-membership
    
[^47]: DIGEST: 快速和通信高效的分散学习与本地更新

    DIGEST: Fast and Communication Efficient Decentralized Learning with Local Updates. (arXiv:2307.07652v1 [cs.LG])

    [http://arxiv.org/abs/2307.07652](http://arxiv.org/abs/2307.07652)

    本文提出了一种名为DIGEST的快速和通信高效的异步分散学习机制，通过结合Gossip和随机游走的思想，并专注于随机梯度下降（SGD），实现了在分散学习中较低的通信成本和较快的收敛时间。

    

    两种广泛考虑的分散学习算法是Gossip和基于随机游走的学习。Gossip算法（同步和异步版本）存在较高的通信成本，而基于随机游走的学习则会增加收敛时间。在本文中，我们设计了一种快速和通信有效的异步分散学习机制DIGEST，利用了Gossip和随机游走的思想，并专注于随机梯度下降（SGD）。DIGEST是一个基于本地SGD算法的异步分散算法，它最初是为通信高效的集中式学习而设计的。我们设计了单流和多流的DIGEST，当流的数量增加时通信开销可能会增加，并且有一种收敛和通信开销的权衡可以利用。我们分析了单流和多流DIGEST的收敛性，并证明了两种算法都接近最优解。

    Two widely considered decentralized learning algorithms are Gossip and random walk-based learning. Gossip algorithms (both synchronous and asynchronous versions) suffer from high communication cost, while random-walk based learning experiences increased convergence time. In this paper, we design a fast and communication-efficient asynchronous decentralized learning mechanism DIGEST by taking advantage of both Gossip and random-walk ideas, and focusing on stochastic gradient descent (SGD). DIGEST is an asynchronous decentralized algorithm building on local-SGD algorithms, which are originally designed for communication efficient centralized learning. We design both single-stream and multi-stream DIGEST, where the communication overhead may increase when the number of streams increases, and there is a convergence and communication overhead trade-off which can be leveraged. We analyze the convergence of single- and multi-stream DIGEST, and prove that both algorithms approach to the optima
    
[^48]: 使用高斯径向基函数神经网络学习活跃子空间并发现重要特征

    Learning Active Subspaces and Discovering Important Features with Gaussian Radial Basis Functions Neural Networks. (arXiv:2307.05639v1 [cs.LG])

    [http://arxiv.org/abs/2307.05639](http://arxiv.org/abs/2307.05639)

    本论文提出了一种修改的径向基函数神经网络模型，通过学习精度矩阵，从训练完成后的模型中提取有用信息，包括活跃子空间的方向和输入变量重要性的排序。

    

    提供一个既能达到强大预测性能，又能被人类解释的模型是机器学习研究中最困难的挑战之一，由于这两个目标的冲突性。为解决这个挑战，我们提出了一种修改的径向基函数神经网络模型，通过为其高斯核添加可学习的精度矩阵。我们展示了训练完成后可以从精度矩阵的谱中提取宝贵的信息。特别是，特征向量解释了模型最敏感的方向，揭示了活跃子空间，并提出了用于监督降维的潜在应用。同时，特征向量凸显了输入和潜在变量之间的绝对变化关系，从而使我们能够基于其对预测的重要性提取输入变量的排序。

    Providing a model that achieves a strong predictive performance and at the same time is interpretable by humans is one of the most difficult challenges in machine learning research due to the conflicting nature of these two objectives. To address this challenge, we propose a modification of the Radial Basis Function Neural Network model by equipping its Gaussian kernel with a learnable precision matrix. We show that precious information is contained in the spectrum of the precision matrix that can be extracted once the training of the model is completed. In particular, the eigenvectors explain the directions of maximum sensitivity of the model revealing the active subspace and suggesting potential applications for supervised dimensionality reduction. At the same time, the eigenvectors highlight the relationship in terms of absolute variation between the input and the latent variables, thereby allowing us to extract a ranking of the input variables based on their importance to the predi
    
[^49]: 通过在线信心预测实现具备形式安全保证的贝叶斯优化

    Bayesian Optimization with Formal Safety Guarantees via Online Conformal Prediction. (arXiv:2306.17815v1 [cs.LG])

    [http://arxiv.org/abs/2306.17815](http://arxiv.org/abs/2306.17815)

    本文提出了一种基于贝叶斯优化的方法，无论约束函数的特性如何，都能满足安全要求。

    

    黑盒零阶优化是金融、物理和工程等领域应用的核心基本操作。在这个问题的常见形式中，设计者顺序尝试候选解，并从系统中接收到关于每个尝试值的噪声反馈。本文研究了在这些场景中还提供了有关尝试解的安全性的反馈，并且优化器被限制在整个优化过程中尝试的不安全解的数量上。在基于贝叶斯优化（BO）的方法上，先前的研究引入了一种被称为SAFEOPT的优化方案，只要满足对安全约束函数的严格假设，就能够以可控的概率在反馈噪声上避免选择任何不安全的解。本文介绍了一种新的基于BO的方法，无论约束函数的特性如何，都能满足安全要求。

    Black-box zero-th order optimization is a central primitive for applications in fields as diverse as finance, physics, and engineering. In a common formulation of this problem, a designer sequentially attempts candidate solutions, receiving noisy feedback on the value of each attempt from the system. In this paper, we study scenarios in which feedback is also provided on the safety of the attempted solution, and the optimizer is constrained to limit the number of unsafe solutions that are tried throughout the optimization process. Focusing on methods based on Bayesian optimization (BO), prior art has introduced an optimization scheme -- referred to as SAFEOPT -- that is guaranteed not to select any unsafe solution with a controllable probability over feedback noise as long as strict assumptions on the safety constraint function are met. In this paper, a novel BO-based approach is introduced that satisfies safety requirements irrespective of properties of the constraint function. This s
    
[^50]: 一种个性化联邦学习的终身学习方法

    PeFLL: A Lifelong Learning Approach to Personalized Federated Learning. (arXiv:2306.05515v1 [cs.LG])

    [http://arxiv.org/abs/2306.05515](http://arxiv.org/abs/2306.05515)

    PeFLL是个性化联邦学习的一种新方法，通过联合训练嵌入网络和超网络，PeFLL能够学习输出特定于每个客户端的模型，并且在其它新出现的客户端上表现良好。

    

    个性化联邦学习（pFL）已成为应对参与客户端数据分布的统计异质性挑战的常用方法。pFL不是学习单个全局模型，而是旨在学习每个客户端的个体模型，同时仍然利用其他客户端可用的数据。在这项工作中，我们提出了PeFLL，这是一种根植于终身学习的新型pFL方法，不仅在训练阶段存在的客户端上表现良好，而且在未来可能出现的客户端上也表现良好。PeFLL通过联合训练嵌入网络和超网络来学习输出特定于客户端的模型。嵌入网络学习以一种反映它们之间相似性的潜在描述符空间中表示客户端。超网络学习从这个潜在空间到可能的客户模型空间的映射。我们的实验证明，与先前的方法相比，PeFLL产生了更高准确率的模型。

    Personalized federated learning (pFL) has emerged as a popular approach to dealing with the challenge of statistical heterogeneity between the data distributions of the participating clients. Instead of learning a single global model, pFL aims to learn an individual model for each client while still making use of the data available at other clients. In this work, we present PeFLL, a new pFL approach rooted in lifelong learning that performs well not only on clients present during its training phase, but also on any that may emerge in the future. PeFLL learns to output client specific models by jointly training an embedding network and a hypernetwork. The embedding network learns to represent clients in a latent descriptor space in a way that reflects their similarity to each other. The hypernetwork learns a mapping from this latent space to the space of possible client models. We demonstrate experimentally that PeFLL produces models of superior accuracy compared to previous methods, es
    
[^51]: 线性条件VAE和分层VAE中的后验崩溃现象

    Posterior Collapse in Linear Conditional and Hierarchical Variational Autoencoders. (arXiv:2306.05023v1 [stat.ML])

    [http://arxiv.org/abs/2306.05023](http://arxiv.org/abs/2306.05023)

    本文研究了高度相似的变分后验分布和先验分布之间的后验崩溃现象，特别地，通过对线性条件VAE和分层VAE进行分析，证明了这种现象是由于潜在变量层次关系不清晰而引起的。

    

    在变分自编码器（VAE）中，后验崩溃现象指的是变分后验分布与先验分布的相似度过高，导致编码器提取的潜在变量保存的输入数据信息较少，无法为解码器的数据重建过程产生有意义的表示。尽管该现象一直是VAEs性能的研究热点，但是对于后验崩溃的理论却相对薄弱，特别是在非标准的VAEs中。本文通过对两类重要而常见又较少研究的VAEs进行非平凡的理论分析，即具有两个潜在变量层次的线性条件VAE和分层VAE，提升了对后验崩溃的理论认识，证明了其成因。

    The posterior collapse phenomenon in variational autoencoders (VAEs), where the variational posterior distribution closely matches the prior distribution, can hinder the quality of the learned latent variables. As a consequence of posterior collapse, the latent variables extracted by the encoder in VAEs preserve less information from the input data and thus fail to produce meaningful representations as input to the reconstruction process in the decoder. While this phenomenon has been an actively addressed topic related to VAEs performance, the theory for posterior collapse remains underdeveloped, especially beyond the standard VAEs. In this work, we advance the theoretical understanding of posterior collapse to two important and prevalent yet less studied classes of VAEs: conditional VAEs and hierarchical VAEs. Specifically, via a non-trivial theoretical analysis of linear conditional VAEs and hierarchical VAEs with two levels of latent, we prove that the cause of posterior collapses i
    
[^52]: 量子机器学习中纠缠数据的转换作用

    Transition role of entangled data in quantum machine learning. (arXiv:2306.03481v1 [quant-ph])

    [http://arxiv.org/abs/2306.03481](http://arxiv.org/abs/2306.03481)

    本研究证明了纠缠数据对量子机器学习的性能具有双重效应，有助于减少预测误差和减小训练数据大小，为量子机器学习模型设计提供了指南。

    

    纠缠作为增强量子计算的资源，已经在学习量子动力学中得到了应用。将纠缠融入到量子机器学习模型的操作或测量中，可以显著降低训练数据大小，同时在达到指定预测误差阈值时取得了更优的结果。然而，关于纠缠程度对模型性能的影响，目前仍缺乏分析性理解。本研究通过在学习量子动力学中使用纠缠数据，建立了量子不免费午餐定理。与以往发现的结果相反，我们证明了纠缠数据对预测误差的影响呈现出双重效应，取决于允许的测量次数。在有充分的测量次数的情况下，增加训练数据的纠缠度可以持续降低预测误差，或减少达到给定误差阈值所需的训练数据大小。本研究阐明了纠缠数据在量子机器学习中的关键转换作用，并提供了改进性能的量子机器学习模型设计指南。

    Entanglement serves as the resource to empower quantum computing. Recent progress has highlighted its positive impact on learning quantum dynamics, wherein the integration of entanglement into quantum operations or measurements of quantum machine learning (QML) models leads to substantial reductions in training data size, surpassing a specified prediction error threshold. However, an analytical understanding of how the entanglement degree in data affects model performance remains elusive. In this study, we address this knowledge gap by establishing a quantum no-free-lunch (NFL) theorem for learning quantum dynamics using entangled data. Contrary to previous findings, we prove that the impact of entangled data on prediction error exhibits a dual effect, depending on the number of permitted measurements. With a sufficient number of measurements, increasing the entanglement of training data consistently reduces the prediction error or decreases the required size of the training data to ac
    
[^53]: 抓住意外收获：在离线策略演员-评论家中利用过去成功的价值(arXiv:2306.02865v2 [cs.LG]已更新)

    Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic. (arXiv:2306.02865v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02865](http://arxiv.org/abs/2306.02865)

    该论文提出了 BEE 操作符，通过充分利用过去的成功经验，并保持探索乐观性，解决了离线策略演员-评论家中 Q 值高估与低估问题，提高了策略学习和样本效率。

    

    学习高质量的 Q 值函数在许多现代离线深度强化学习 (RL) 算法的成功中起着关键作用。之前的研究集中解决采用函数逼近器和离线学习所导致的值过高的问题。与这种普遍观点不同，我们观察到 Q 值在 RL 训练过程的后期实际上被低估了，主要是由于贝尔曼更新中，当前策略使用比回放缓冲区中更优的动作样本差。我们假设这个长期被忽视的现象可能阻碍了策略学习，降低了样本效率。我们的想法是在保持探索乐观性的同时，结合充分利用过去成功的经验。我们提出了混合利用和探索 (BEE) 操作符，这是一种简单而有效的方法，使用历史上表现最佳的动作和当前策略生成的动作来更新 Q 值。

    Learning high-quality Q-value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. Previous works focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. Deviating from the common viewpoint, we observe that Q-values are indeed underestimated in the latter stage of the RL training process, primarily related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer. We hypothesize that this long-neglected phenomenon potentially hinders policy learning and reduces sample efficiency. Our insight to address this issue is to incorporate sufficient exploitation of past successes while maintaining exploration optimism. We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates Q-value using both historical best-performing actions and
    
[^54]: 基于领域无关自监督学习简化全波形反演

    Simplifying Full Waveform Inversion via Domain-Independent Self-Supervised Learning. (arXiv:2305.13314v1 [physics.geo-ph])

    [http://arxiv.org/abs/2305.13314](http://arxiv.org/abs/2305.13314)

    本文提出了基于领域无关自监督学习的SimFWI算法，其两个步骤分别是分别通过多个数据集上的遮盖图像建模学习编码器和解码器，然后为每个数据集学习一个线性映射。该算法可用于预测地震数据中的地下速度图，可极大地简化全波形反演任务，并连接多个FWI数据集。

    

    地球物理学中的深度学习在全波形反演(FWI)中应用得到成功，用于预测地震数据中的地下速度图。本文报告了一个惊人的现象：通过自监督学习，在各自的领域中分别训练编码器和解码器，可以在潜在空间中观察到跨领域的线性关系。基于这些发现，我们开发了SimFWI，一个包括两个步骤的新范式：(a)通过多个数据集上的遮盖图像建模分别学习地震编码器和速度解码器；(b)为每个数据集学习一个线性映射。实验结果显示

    Geophysics has witnessed success in applying deep learning to one of its core problems: full waveform inversion (FWI) to predict subsurface velocity maps from seismic data. It is treated as an image-to-image translation problem, jointly training an encoder for seismic data and a decoder for the velocity map from seismic-velocity pairs. In this paper, we report a surprising phenomenon: when training an encoder and decoder separately in their own domains via self-supervised learning, a linear relationship is observed across domains in the latent spaces. Moreover, this phenomenon connects multiple FWI datasets in an elegant manner: these datasets can share the self-learned encoder and decoder with different linear mappings.  Based on these findings, we develop SimFWI, a new paradigm that includes two steps: (a) learning a seismic encoder and a velocity decoder separately by masked image modeling over multiple datasets; (b) learning a linear mapping per dataset. Experimental results show t
    
[^55]: MAGDiff: 利用深度神经网络激活图检测协变数据集转移

    MAGDiff: Covariate Data Set Shift Detection via Activation Graphs of Deep Neural Networks. (arXiv:2305.13271v1 [stat.ML])

    [http://arxiv.org/abs/2305.13271](http://arxiv.org/abs/2305.13271)

    MAGDiff是一种新的表示法，可以从神经网络分类器中提取，可以有效地检测协变数据集转移，而不需要训练新模型。这可以通过对比神经网络激活图来计算，并通过两个样本 Kolmogorov-Smirnov 测试进行实证验证。

    

    尽管神经网络被广泛应用于各种任务，但像其他机器学习方法一样，它们受到数据转移的影响，在训练数据与实际应用数据之间分布存在差异时，其性能会受到严重影响。本文提出了一种称为 MAGDiff 的新表示方法，可以从任何给定的神经网络分类器中提取出来，并且可以有效地检测协变数据集转移，而不需要训练专门用于此任务的新模型。这些表示形式是通过比较神经网络激活图来计算的，对于属于训练分布和目标分布的样本，提供了强大的数据和任务自适应统计量，用于检测常用的数据集转移的两个样本测试。我们通过测量两个样本 Kolmogorov-Smirnov（KS）测试的统计功率进行了实证验证。

    Despite their successful application to a variety of tasks, neural networks remain limited, like other machine learning methods, by their sensitivity to shifts in the data: their performance can be severely impacted by differences in distribution between the data on which they were trained and that on which they are deployed. In this article, we propose a new family of representations, called MAGDiff, that we extract from any given neural network classifier and that allows for efficient covariate data shift detection without the need to train a new model dedicated to this task. These representations are computed by comparing the activation graphs of the neural network for samples belonging to the training distribution and to the target distribution, and yield powerful data- and task-adapted statistics for the two-sample tests commonly used for data set shift detection. We demonstrate this empirically by measuring the statistical powers of two-sample Kolmogorov-Smirnov (KS) tests on sev
    
[^56]: 决策时间规划的更新等价框架

    The Update Equivalence Framework for Decision-Time Planning. (arXiv:2304.13138v1 [cs.AI])

    [http://arxiv.org/abs/2304.13138](http://arxiv.org/abs/2304.13138)

    该论文提出了一个基于更新等价的决策时间规划框架，使得决策时间规划算法不依赖于公共信息，在更大范围的不完全信息决策环境中实现超人类表现。

    

    在棋类游戏等完全信息环境中，即时修正（或构建）策略的决策时间规划是实现超人类表现的关键。一些研究将决策时间规划扩展到更普遍的不完全信息环境，从而实现了扑克中的超人类表现。但是，这些方法需要考虑随着非公共信息量的增加而快速增长的子游戏，使得它们在非公共信息量较大时不起作用。为了解决这个问题，我们引入了一种基于更新等价而不是子游戏概念的决策时间规划框架。在这个框架中，决策时间规划算法模拟同步学习算法的更新。这个框架使我们能够引入一系列原则上的决策时间规划算法，这些算法不依赖于公共信息，并为新的一个系列的决策时间规划算法打开了大门。

    The process of revising (or constructing) a policy immediately prior to execution -- known as decision-time planning -- is key to achieving superhuman performance in perfect-information settings like chess and Go. A recent line of work has extended decision-time planning to more general imperfect-information settings, leading to superhuman performance in poker. However, these methods requires considering subgames whose sizes grow quickly in the amount of non-public information, making them unhelpful when the amount of non-public information is large. Motivated by this issue, we introduce an alternative framework for decision-time planning that is not based on subgames but rather on the notion of update equivalence. In this framework, decision-time planning algorithms simulate updates of synchronous learning algorithms. This framework enables us to introduce a new family of principled decision-time planning algorithms that do not rely on public information, opening the door to sound and
    
[^57]: BanditQ - 在敌对环境中保证用户每次奖励的无悔学习

    BanditQ -- No-Regret Learning with Guaranteed Per-User Rewards in Adversarial Environments. (arXiv:2304.05219v1 [cs.LG])

    [http://arxiv.org/abs/2304.05219](http://arxiv.org/abs/2304.05219)

    提出了一种名为BanditQ的新的在线预测策略，在敌对设置中以公平的方式达到目标速率约束，并实现了$O(T^{3/4})$的遗憾。

    

    经典的在线预测算法如Hedge在设计上具有不公平性，因为它们尝试尽可能多地玩最具回报的臂而忽略次优臂，以实现亚线性遗憾。本文考虑在具有对所有臂累积奖励速率下界的敌对设置中，以公平的在线预测问题。通过将基本排队论与在线学习相结合，我们提出了一种名为BanditQ的新的在线预测策略，它在全信息设置下实现了目标速率约束，并实现了$O(T^{3/4})$的遗憾。BanditQ的设计和分析涉及潜在函数方法的新颖应用，并具有独立的兴趣。

    Classic online prediction algorithms, such as Hedge, are inherently unfair by design, as they try to play the most rewarding arm as many times as possible while ignoring the sub-optimal arms to achieve sublinear regret. In this paper, we consider a fair online prediction problem in the adversarial setting with hard lower bounds on the rate of accrual of rewards for all arms. By combining elementary queueing theory with online learning, we propose a new online prediction policy, called BanditQ, that achieves the target rate constraints while achieving a regret of $O(T^{3/4})$ in the full-information setting. The design and analysis of BanditQ involve a novel use of the potential function method and are of independent interest.
    
[^58]: 一种用于遥感图像的十亿级基础模型

    A Billion-scale Foundation Model for Remote Sensing Images. (arXiv:2304.05215v1 [cs.CV])

    [http://arxiv.org/abs/2304.05215](http://arxiv.org/abs/2304.05215)

    本文介绍了一个用于遥感图像的十亿级基础模型，并研究了增加模型参数数量对该模型在下游任务中的性能影响，实验显示增加模型参数数量可以显著提高性能。

    

    随着基础模型在视觉任务中的潜力引起了广泛关注，先对这些模型进行预训练已成为一个关键步骤。预训练基础模型的三个关键因素是预训练方法、预训练数据集的大小以及模型参数的数量。最近，遥感领域的研究主要关注预训练方法和数据集的大小，对模型参数的数量关注较少。本文通过研究增加模型参数数量对基础模型在旋转目标检测和语义分割等下游任务中性能的影响来弥补这一空白。我们使用不同数量参数（包括86M、605.26M、1.3B和2.4B）的基础模型进行预训练，以确定参数增加是否会提高下游任务的性能。据我们所知，这是第一个用于遥感图像的十亿级基础模型。我们的实验表明，增加模型参数数量可以显著提高下游任务的性能。此外，我们还介绍了一个包含10亿个遥感图像的新的预训练数据集，并向研究社区公开。

    As the potential of foundation models in visual tasks has garnered significant attention, pretraining these models before downstream tasks has become a crucial step. The three key factors in pretraining foundation models are the pretraining method, the size of the pretraining dataset, and the number of model parameters. Recently, research in the remote sensing field has focused primarily on the pretraining method and the size of the dataset, with limited emphasis on the number of model parameters. This paper addresses this gap by examining the effect of increasing the number of model parameters on the performance of foundation models in downstream tasks such as rotated object detection and semantic segmentation. We pretrained foundation models with varying numbers of parameters, including 86M, 605.26M, 1.3B, and 2.4B, to determine whether performance in downstream tasks improved with an increase in parameters. To the best of our knowledge, this is the first billion-scale foundation mod
    
[^59]: 利用量子几何进行学习幺正变换的泛化

    Generalization with quantum geometry for learning unitaries. (arXiv:2303.13462v1 [quant-ph])

    [http://arxiv.org/abs/2303.13462](http://arxiv.org/abs/2303.13462)

    本文研究了量子机器学习模型的泛化能力，使用数据的量子费舍尔信息度量来评估成功训练和泛化所需的电路参数和训练数据的数量，并展示通过去除对称性来提高泛化能力，同时发现超出分布泛化能力可以比使用相同分布更优。

    

    泛化是量子机器学习模型从训练数据学习准确预测新数据的能力。在这里，我们引入数据的量子费舍尔信息度量(DQFIM)来确定模型何时能够泛化。对于幺正变换的可变学习，DQFIM量化了成功训练和泛化所需的电路参数和训练数据的数量。我们应用DQFIM来解释何时恒定数量的训练状态和多项式数量的参数足以实现泛化。此外，通过从训练数据中删除对称性，可以提高泛化能力。最后，我们显示，使用不同数据分布进行训练和测试的超出分布泛化能力可以比使用相同分布的能力更优。我们的研究为提高量子机器学习中的泛化能力开辟了新的方法。

    Generalization is the ability of quantum machine learning models to make accurate predictions on new data by learning from training data. Here, we introduce the data quantum Fisher information metric (DQFIM) to determine when a model can generalize. For variational learning of unitaries, the DQFIM quantifies the amount of circuit parameters and training data needed to successfully train and generalize. We apply the DQFIM to explain when a constant number of training states and polynomial number of parameters are sufficient for generalization. Further, we can improve generalization by removing symmetries from training data. Finally, we show that out-of-distribution generalization, where training and testing data are drawn from different data distributions, can be better than using the same distribution. Our work opens up new approaches to improve generalization in quantum machine learning.
    
[^60]: 后期情节式强化学习推断

    Post-Episodic Reinforcement Learning Inference. (arXiv:2302.08854v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.08854](http://arxiv.org/abs/2302.08854)

    我们提出了一种后期情节式强化学习推断的方法，能够评估反事实的自适应策略并估计动态处理效应，通过重新加权的$Z$-估计方法稳定情节变化的估计方差。

    

    我们考虑从情节式强化学习算法收集的数据进行估计和推断；即在每个时期（也称为情节）以顺序方式与单个受试单元多次交互的自适应试验算法。我们的目标是在收集数据后能够评估反事实的自适应策略，并估计结构参数，如动态处理效应，这可以用于信用分配（例如，第一个时期的行动对最终结果的影响）。这些感兴趣的参数可以构成矩方程的解，但不是总体损失函数的最小化器，在静态数据情况下导致了$Z$-估计方法。然而，这样的估计量在自适应数据收集的情况下不能渐近正态。我们提出了一种重新加权的$Z$-估计方法，使用精心设计的自适应权重来稳定情节变化的估计方差，这是由非...

    We consider estimation and inference with data collected from episodic reinforcement learning (RL) algorithms; i.e. adaptive experimentation algorithms that at each period (aka episode) interact multiple times in a sequential manner with a single treated unit. Our goal is to be able to evaluate counterfactual adaptive policies after data collection and to estimate structural parameters such as dynamic treatment effects, which can be used for credit assignment (e.g. what was the effect of the first period action on the final outcome). Such parameters of interest can be framed as solutions to moment equations, but not minimizers of a population loss function, leading to $Z$-estimation approaches in the case of static data. However, such estimators fail to be asymptotically normal in the case of adaptive data collection. We propose a re-weighted $Z$-estimation approach with carefully designed adaptive weights to stabilize the episode-varying estimation variance, which results from the non
    
[^61]: 原型分析++：重新思考初始化策略

    Archetypal Analysis++: Rethinking the Initialization Strategy. (arXiv:2301.13748v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13748](http://arxiv.org/abs/2301.13748)

    本文提出了一种针对原型分析的概率初始化策略 AA ++，能够在13个不同大小和维度的实际数据集上表现优异。

    

    原型分析是一种带有凸性约束的矩阵分解方法。由于局部最小值的存在，好的初始化非常重要，但是经常使用的初始化方法要么产生次优的起始点，要么容易陷入不良的局部最小值。在本文中，我们提出了原型分析++（AA ++），这是一种针对原型分析的概率初始化策略，它根据点对目标的影响顺序地进行采样，类似于$k$-means++。实际上，我们认为$k$-means++已近逼近了所提出的初始化方法。此外，我们建议将$k$-means++的高效蒙特卡罗近似方法应用于AA++。在对13个不同大小和维度的实际数据集进行广泛的实证评估并考虑两个预处理策略的情况下，我们表明AA++几乎总是优于所有的基线方法，包括最常用的方法。

    Archetypal analysis is a matrix factorization method with convexity constraints. Due to local minima, a good initialization is essential, but frequently used initialization methods yield either sub-optimal starting points or are prone to get stuck in poor local minima. In this paper, we propose archetypal analysis++ (AA++), a probabilistic initialization strategy for archetypal analysis that sequentially samples points based on their influence on the objective, similar to $k$-means++. In fact, we argue that $k$-means++ already approximates the proposed initialization method. Furthermore, we suggest to adapt an efficient Monte Carlo approximation of $k$-means++ to AA++. In an extensive empirical evaluation of 13 real-world data sets of varying sizes and dimensionalities and considering two pre-processing strategies, we show that AA++ nearly always outperforms all baselines, including the most frequently used ones.
    
[^62]: 关于Text-to-SQL模型的安全漏洞

    On the Security Vulnerabilities of Text-to-SQL Models. (arXiv:2211.15363v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.15363](http://arxiv.org/abs/2211.15363)

    该论文揭示了Text-to-SQL模型存在的安全漏洞，并证明了这些漏洞能够被恶意利用产生攻击，通过对商业应用和开源语言模型的实验验证。该研究意在引起学术界对NLP算法相关的软件安全问题的关注和进一步研究。

    

    尽管已经证明自然语言处理（NLP）算法容易受到恶意攻击，但这些弱点是否可能导致软件安全威胁尚未深入研究。为了弥补这一差距，我们对常用于创建自然语言数据库接口的Text-to-SQL系统进行了漏洞测试。我们展示了六个商业应用中的Text-to-SQL模块可以被操纵以产生恶意代码，潜在地导致数据泄漏和拒绝服务攻击。这是第一个证明NLP模型可以被利用为攻击向量的示例。此外，使用四个开源语言模型的实验验证了对Text-to-SQL系统进行直接后门攻击可以达到100％的成功率，而不影响其性能。本研究旨在引起学术界对与NLP算法相关的潜在软件安全问题的关注，并鼓励进一步探索。

    Although it has been demonstrated that Natural Language Processing (NLP) algorithms are vulnerable to deliberate attacks, the question of whether such weaknesses can lead to software security threats is under-explored. To bridge this gap, we conducted vulnerability tests on Text-to-SQL systems that are commonly used to create natural language interfaces to databases. We showed that the Text-to-SQL modules within six commercial applications can be manipulated to produce malicious code, potentially leading to data breaches and Denial of Service attacks. This is the first demonstration that NLP models can be exploited as attack vectors in the wild. In addition, experiments using four open-source language models verified that straightforward backdoor attacks on Text-to-SQL systems achieve a 100% success rate without affecting their performance. The aim of this work is to draw the community's attention to potential software security issues associated with NLP algorithms and encourage explor
    
[^63]: 对数线性保护性及其影响的研究

    Log-linear Guardedness and its Implications. (arXiv:2210.10012v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10012](http://arxiv.org/abs/2210.10012)

    本研究介绍了对数线性保护性及其对下游分类器行为的影响。在二元情况下，下游对数线性模型无法恢复被删除的概念，但在某些情况下，可以通过构建多类对数线性模型间接恢复概念。这些结果揭示了线性删除方法的局限性，并强调了进一步研究的需求。

    

    已经发现，在假设可线性的神经表示中，从中删除可人解释的概念的方法是可行和有用的。然而，这种删除对于基于修改后表示进行训练的下游分类器行为的影响尚未完全理解。在这项工作中，我们正式定义了对数线性保护性的概念，即对手无法直接从表示中预测概念的能力，并研究其影响。我们证明，在二元情况下，在某些假设下，下游对数线性模型无法恢复被删除的概念。然而，我们证明，在某些情况下，可以构建一个多类对数线性模型，间接恢复概念，这指出了对数线性保护性作为下游偏差缓解技术的内在局限性。这些发现揭示了线性删除方法的理论限制，并强调了进一步研究可解释神经表示与分类器之间的联系的需要。

    Methods for erasing human-interpretable concepts from neural representations that assume linearity have been found to be tractable and useful. However, the impact of this removal on the behavior of downstream classifiers trained on the modified representations is not fully understood. In this work, we formally define the notion of log-linear guardedness as the inability of an adversary to predict the concept directly from the representation, and study its implications. We show that, in the binary case, under certain assumptions, a downstream log-linear model cannot recover the erased concept. However, we demonstrate that a multiclass log-linear model \emph{can} be constructed that indirectly recovers the concept in some cases, pointing to the inherent limitations of log-linear guardedness as a downstream bias mitigation technique. These findings shed light on the theoretical limitations of linear erasure methods and highlight the need for further research on the connections between int
    
[^64]: 图上的多尺度Wasserstein最短路径过滤核心

    Multi-scale Wasserstein Shortest-path Filtration Kernels on Graphs. (arXiv:2206.00979v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.00979](http://arxiv.org/abs/2206.00979)

    这篇论文提出了一种名为多尺度Wasserstein最短路径过滤图核心（MWSPF）的新型最短路径图核心，解决了传统核心的信息丢失和缺乏多个尺度考虑的问题。

    

    传统的最短路径图核心（SP）是最受欢迎的图核心之一。它将图分解为最短路径，并计算每个图中最短路径的频率。然而，SP面临两个主要挑战：首先，最短路径的三元表示失去了信息。其次，SP比较图时没有考虑到图结构的多个不同尺度，而这在现实世界的图中很常见，例如社交网络中的链状结构、环状结构和星状结构。为了克服这两个挑战，我们开发了一种新颖的最短路径图核心，称为多尺度Wasserstein最短路径过滤图核心（MWSPF）。它使用以每个顶点为根的某个深度的BFS树来限制考虑最短路径的最大长度，考虑到小世界特性。它考虑了最短路径中所有顶点的标签。为了方便在多个不同尺度上比较图，它从顶点和

    The traditional shortest-path graph kernel (SP) is one of the most popular graph kernels. It decomposes graphs into shortest paths and computes their frequencies in each graph. However, SP has two main challenges: Firstly, the triplet representation of the shortest path loses information. Secondly, SP compares graphs without considering the multiple different scales of the graph structure which is common in real-world graphs, e.g., the chain-, ring-, and star-structures in social networks. To overcome these two challenges, we develop a novel shortest-path graph kernel called the Multi-scale Wasserstein Shortest-Path Filtration graph kernel (MWSPF). It uses a BFS tree of a certain depth rooted at each vertex to restrict the maximum length of the shortest path considering the small world property. It considers the labels of all the vertices in the shortest path. To facilitate the comparison of graphs at multiple different scales, it augments graphs from both the aspects of the vertex and
    
[^65]: 现代生物统计中的强化学习：构建最优自适应干预

    Reinforcement Learning in Modern Biostatistics: Constructing Optimal Adaptive Interventions. (arXiv:2203.02605v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2203.02605](http://arxiv.org/abs/2203.02605)

    本文是关于将强化学习应用于自适应干预中的第一份统一调查，强化学习在动态治疗方案和移动健康中即时自适应干预这两个领域中都具有很大的应用潜力。在这两个领域之间存在相似和不同之处需要考虑，并且这里存在巨大的合作机会。

    

    近年来，强化学习（RL）在与健康相关的序列性决策中占据了重要地位，成为交付自适应干预（AIs）的越来越流行的工具。然而，尽管具有潜在优势，但其现实应用仍然受到限制，部分是由于方法论和应用社区之间的协同不足。在这项工作中，我们提供了关于学习AIs的RL方法的第一份统一调查，利用RL的通用方法论伞来桥接动态治疗方案和移动健康中即时自适应干预这两个AI领域。我们概述了这两个AI领域之间的异同，并讨论了它们对使用RL的影响。最后，我们利用自己在两个领域中设计案例研究的经验，说明了在AIs领域中，统计学、RL和医疗研究人员之间的巨大合作机会。

    In recent years, reinforcement learning (RL) has acquired a prominent position in the space of health-related sequential decision-making, becoming an increasingly popular tool for delivering adaptive interventions (AIs). However, despite potential benefits, its real-life application is still limited, partly due to a poor synergy between the methodological and the applied communities. In this work, we provide the first unified survey on RL methods for learning AIs, using the common methodological umbrella of RL to bridge the two AI areas of dynamic treatment regimes and just-in-time adaptive interventions in mobile health. We outline similarities and differences between these two AI domains and discuss their implications for using RL. Finally, we leverage our experience in designing case studies in both areas to illustrate the tremendous collaboration opportunities between statistical, RL, and healthcare researchers in the space of AIs.
    
[^66]: 高效直连拓扑结构用于集体通信

    Efficient Direct-Connect Topologies for Collective Communications. (arXiv:2202.03356v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2202.03356](http://arxiv.org/abs/2202.03356)

    本论文提出了一种算法框架，可以构建高效的直连网络拓扑结构，以优化节点延迟和带宽权衡，适用于集体通信负载。

    

    本文研究了如何构建适用于集体通信的高效网络拓扑结构。我们提出了一种算法框架，用于构建针对节点延迟与带宽权衡优化的直连拓扑结构。这个算法框架从小的基础拓扑结构和相关的通信进度开始，并使用一组可以迭代应用的技术来派生更大的拓扑结构。这些衍生的拓扑结构的时间表可以与扩展一起合成，也可以使用优化公式计算。我们的方法允许我们为给定的集群大小和度数合成许多不同的拓扑结构和时间表，然后为给定的工作负载确定适当的拓扑和时间表。我们在使用补丁面板配置所需拓扑结构的12节点光学实验平台上评估了我们的方法，并增加了基于分析模型的评估，用于更大的部署。

    We consider the problem of distilling efficient network topologies for collective communications. We provide an algorithmic framework for constructing direct-connect topologies optimized for the node latency vs bandwidth trade-off given a collective communication workload. Our algorithmic framework allows us to start from small base topologies and associated communication schedules and use a set of techniques that can be iteratively applied to derive much larger topologies. The schedules for these derived topologies are either synthesized along with the expansions or computed using an optimization formulation. Our approach allows us to synthesize many different topologies and schedules for a given cluster size and degree, and then identify the appropriate topology and schedule for a given workload. We evaluate our approach on a 12-node optical testbed that uses patch panels for configuring the desired topology and augment it with an analytical-model-based evaluation for larger deployme
    
[^67]: 分类规则生成：可扩展性，解释性和公平性

    Rule Generation for Classification: Scalability, Interpretability, and Fairness. (arXiv:2104.10751v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2104.10751](http://arxiv.org/abs/2104.10751)

    这项研究介绍了一种新的基于规则的分类优化方法，利用列生成线性规划实现可扩展性，并通过分配成本系数和引入额外约束解决了解释性和公平性问题。该方法在局部解释性和公平性之间取得了良好的平衡。

    

    我们引入了一种新的基于规则的分类优化方法，具有约束条件。所提出的方法利用列生成线性规划，因此可扩展到大型数据集。所得定价子问题被证明是NP难问题。我们采用基于决策树的启发式方法，并解决了一个代理定价子问题以加速。该方法返回一组规则以及它们的最优权重，指示每个规则对学习的重要性。我们通过为规则分配成本系数和引入额外约束来解决解释性和公平性问题。具体而言，我们关注局部解释性，并将公平性的一般分离准则推广到多个敏感属性和类别。我们在一系列数据集上测试了所提出方法的性能，并提供了一个案例研究来详细阐述其不同方面。所提出的基于规则的学习方法在局部解释性和公平性之间达到了良好的平衡点。

    We introduce a new rule-based optimization method for classification with constraints. The proposed method leverages column generation for linear programming, and hence, is scalable to large datasets. The resulting pricing subproblem is shown to be NP-Hard. We recourse to a decision tree-based heuristic and solve a proxy pricing subproblem for acceleration. The method returns a set of rules along with their optimal weights indicating the importance of each rule for learning. We address interpretability and fairness by assigning cost coefficients to the rules and introducing additional constraints. In particular, we focus on local interpretability and generalize separation criterion in fairness to multiple sensitive attributes and classes. We test the performance of the proposed methodology on a collection of datasets and present a case study to elaborate on its different aspects. The proposed rule-based learning method exhibits a good compromise between local interpretability and fairn
    

