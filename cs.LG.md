# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [A Proactive and Dual Prevention Mechanism against Illegal Song Covers empowered by Singing Voice Conversion.](http://arxiv.org/abs/2401.17133) | 这项工作提出了一种主动性的双重防护机制，通过引入人类无法察觉的扰动，干扰歌唱声音转换的生成过程，防止未经授权的基于歌唱声音转换的非法歌曲翻唱。该机制既扰乱了歌手身份，又扰乱了歌词，使得歌唱声音既不模仿目标歌手，也不保留原始歌词。 |
| [^2] | [Personalized Differential Privacy for Ridge Regression.](http://arxiv.org/abs/2401.17127) | 该论文提出了一种个性化差分隐私输出扰动方法（PDP-OP），可以在Ridge回归中训练具有每个数据点个性化隐私水平的模型，并提供了相应的隐私证明和准确度保证。 |
| [^3] | [Spectral Co-Distillation for Personalized Federated Learning.](http://arxiv.org/abs/2401.17124) | 本论文提出了一种基于模型谱信息的光谱共蒸馏方法，用于个性化联邦学习。通过建立一个共蒸馏框架，建立了通用和个性化模型训练之间的双向桥梁。 |
| [^4] | [Unsupervised Discovery of Steerable Factors When Graph Deep Generative Models Are Entangled.](http://arxiv.org/abs/2401.17123) | 这项工作提出了GraphCG方法，用于在预训练图深度生成模型的潜在空间中无监督发现可操纵因素，通过最大化语义丰富方向之间的互信息来学习这些可操纵因素。实验证明GraphCG优于其他竞争方法。 |
| [^5] | [Explainable data-driven modeling via mixture of experts: towards effective blending of grey and black-box models.](http://arxiv.org/abs/2401.17118) | 这个论文提出了一种基于“专家混合”原理的综合框架，通过将灰盒和黑盒模型进行数据驱动的融合，实现了对复杂系统的准确建模，并提高了可解释性。 |
| [^6] | [Quantum error mitigation and correction mediated by Yang-Baxter equation and artificial neural network.](http://arxiv.org/abs/2401.17116) | 该论文介绍了一种新策略，通过人工神经网络和Yang-Baxter方程来缓解和修正量子计算中的错误。研究表明，通过控制噪声，我们可以使用经典计算进行错误缓解，并且通过训练神经网络模型可以有效地纠正时间演化的量子态中的错误。 |
| [^7] | [Evaluation in Neural Style Transfer: A Review.](http://arxiv.org/abs/2401.17109) | 神经风格迁移领域对于评估方法的选择缺乏共识，本综述通过对现有评估技术的分析，提出了标准化评估实践的建议。 |
| [^8] | [CharNet: Generalized Approach for High-Complexity Character Classification.](http://arxiv.org/abs/2401.17098) | 这是一篇关于手写字符识别问题的论文，提出了一个广义的方法来解决高复杂性字符分类的挑战。 |
| [^9] | [Traffic estimation in unobserved network locations using data-driven macroscopic models.](http://arxiv.org/abs/2401.17095) | 本文提出了一种利用宏观模型和多源时空数据的方法来估计无法观测到的网络位置的交通流量和行驶时间。该方法可以在传感器覆盖范围有限的情况下进行准确的估计，并满足基本的流量守恒约束条件。 |
| [^10] | [Dynamical Survival Analysis with Controlled Latent States.](http://arxiv.org/abs/2401.17077) | 本论文提出了一种动态生存分析方法，通过控制潜在状态来学习个体特定的计数过程强度。研究者设计了一个神经控制微分方程模型，并证明了在足够正则条件下，可以在签名空间中线性化模型，得到一种基于签名的估计器。通过对金融、预测性维护和食品供应链管理等数据集的实验，验证了模型的性能。 |
| [^11] | [Outline of an Independent Systematic Blackbox Test for ML-based Systems.](http://arxiv.org/abs/2401.17062) | 本文提出了一种独立测试ML模型和基于ML的系统的方法，可以独立验证其黑盒特性和随机属性。建议扩展现有测试方法以更好地反映ML系统的特点。 |
| [^12] | [Making Parametric Anomaly Detection on Tabular Data Non-Parametric Again.](http://arxiv.org/abs/2401.17052) | 本论文研究了在表格数据上使用检索增强模型进行异常检测的方法，通过重建被屏蔽特征，结合KNN和注意力机制选择相关样本来帮助目标样本的重建过程。实验证明通过使用非参数化关系进行检索增强的异常检测方法能取得良好的效果。 |
| [^13] | [Forecasting VIX using Bayesian Deep Learning.](http://arxiv.org/abs/2401.17042) | 本文利用贝叶斯深度学习方法预测波动性指数VIX，并采用概率对应模型和标准差缩放方法提高预测准确性和不确定性估计。具有高斯先验的MNF模型表现最优，使得TCN和WaveNet网络可以良好推断VIX值。 |
| [^14] | [Bayesian Optimization with Noise-Free Observations: Improved Regret Bounds via Random Exploration.](http://arxiv.org/abs/2401.17037) | 该论文研究了基于无噪声观测的贝叶斯优化问题，提出了一种基于散乱数据逼近的新算法，并引入随机探索步骤以实现接近最优填充距离的速率衰减。该算法在实现的易用性和累积遗憾边界的性能上超过了传统的GP-UCB算法，并在多个示例中优于其他贝叶斯优化策略。 |
| [^15] | [Intrinsic Data Constraints and Upper Bounds in Binary Classification Performance.](http://arxiv.org/abs/2401.17036) | 我们研究了二元分类性能的内在数据限制和上界，提供了一个理论框架并进行了理论推理和实证检验，发现理论上限是可以被达到的，并计算出了三个常用评估指标的精确上限。 |
| [^16] | [Robust Kernel Sparse Subspace Clustering.](http://arxiv.org/abs/2401.17035) | 这篇论文提出了一种用于处理具有粗疏损坏数据的坚固核稀疏子空间聚类算法，并验证了其有效性。 |
| [^17] | [M2CURL: Sample-Efficient Multimodal Reinforcement Learning via Self-Supervised Representation Learning for Robotic Manipulation.](http://arxiv.org/abs/2401.17032) | M2CURL是一种样本高效的多模态强化学习方法，通过自监督表示学习从视触觉数据中学习出高效的表示，并加速强化学习算法的收敛。 |
| [^18] | [LADDER: Revisiting the Cosmic Distance Ladder with Deep Learning Approaches and Exploring its Applications.](http://arxiv.org/abs/2401.17029) | LADDER是一个新颖的深度学习框架，通过学习宇宙的“距离梯度”，实现了预测宇宙距离并探索了多个宇宙学应用。这项研究表明在机器学习应用中需要进行有趣但谨慎的考虑。 |
| [^19] | [Heterogeneous treatment effect estimation with subpopulation identification for personalized medicine in opioid use disorder.](http://arxiv.org/abs/2401.17027) | 本论文介绍了一种名为SubgroupTE的神经网络框架，它结合了亚群识别和治疗效果估计，通过考虑治疗反应的异质性，提高了个性化治疗建议的准确性。 |
| [^20] | [Evaluation of Out-of-Distribution Detection Performance on Autonomous Driving Datasets.](http://arxiv.org/abs/2401.17013) | 本研究评估了在自动驾驶数据集上的越界检测性能，发现通过使用Mahalanobis距离来拒绝输出可以显著降低分类风险，即使应用在未见过的数据集上。 |
| [^21] | [Finetuning Large Language Models for Vulnerability Detection.](http://arxiv.org/abs/2401.17010) | 本文优化了大规模语言模型用于源代码中的漏洞检测任务，通过微调最先进的代码语言模型WizardCoder并改进其训练过程和策略，实现了对漏洞数据集的分类性能的提升。 |
| [^22] | [Causal Machine Learning for Cost-Effective Allocation of Development Aid.](http://arxiv.org/abs/2401.16986) | 本文提出了一个因果机器学习框架，用于预测援助分配的异质化治疗效果，以支持有效的援助分配决策。 |
| [^23] | [Multiple Yield Curve Modeling and Forecasting using Deep Learning.](http://arxiv.org/abs/2401.16985) | 本文介绍了一种使用深度学习模型同时描述多种收益率曲线动态的方法，并通过结合自注意机制和非参数分位数回归，生成未来收益率的点预测和区间预测。实验证实了该方法的有效性，并且提出了深度集成和迁移学习的扩展和改进。 |
| [^24] | [Selection of gamma events from IACT images with deep learning methods.](http://arxiv.org/abs/2401.16981) | 本研究展示了在TAIGA-IACT的Monte Carlo（MC）图像上通过神经网络进行图像分类任务的结果，同时考虑了交错观测模式和适应NN分析的图像调整。 |
| [^25] | [CORE: Towards Scalable and Efficient Causal Discovery with Reinforcement Learning.](http://arxiv.org/abs/2401.16974) | CORE是一种基于深度强化学习的因果推断和干预规划方法，可以高效地揭示因果结构，并在结构估计准确性和样本效率方面表现优于现有方法。 |
| [^26] | [Online Resource Allocation with Non-Stationary Customers.](http://arxiv.org/abs/2401.16945) | 本文提出了一种用于在线资源分配的新算法，适用于非平稳顾客到达和未知的点击率。通过充分利用随机上下文摇臂和具有对抗性到达的在线匹配的结果，我们的方案实现了在顾客到达接近平稳时具有次线性遗憾，并在一般（非平稳）顾客到达分布下享受最优的竞争比率。我们通过大量的数值实验证明了我们的方法在各种不同的顾客场景下生成接近最优的收益。 |
| [^27] | [Segmentation and Characterization of Macerated Fibers and Vessels Using Deep Learning.](http://arxiv.org/abs/2401.16937) | 这项工作开发了一种利用深度学习进行浸软纤维和导管分割和特征提取的自动方法，并且在显微镜图像中取得了快速而准确的分割效果。 |
| [^28] | [Multi-modal Representation Learning for Cross-modal Prediction of Continuous Weather Patterns from Discrete Low-Dimensional Data.](http://arxiv.org/abs/2401.16936) | 本研究提出了一种基于深度学习的方法，用于从离散低维数据中预测连续分辨率风数据。该方法解决了改善数据分辨率、降低数据维度、以及推断不同空间规格风数据的三个挑战。 |
| [^29] | [Sparse Portfolio Selection via Topological Data Analysis based Clustering.](http://arxiv.org/abs/2401.16920) | 本文使用拓扑数据分析工具提出了一种基于聚类的稀疏投资组合选择策略，通过利用股票价格波动的拓扑特征，在持续图和景观空间上引入新的距离度量，并与聚类算法相结合，显著提升了多样市场情景下稀疏投资组合的绩效。 |
| [^30] | [Energy-conserving equivariant GNN for elasticity of lattice architected metamaterials.](http://arxiv.org/abs/2401.16914) | 本研究提出了一种节能的等变图神经网络模型，用于预测周期性支撑结构的刚度张量。通过编码的等变性和能量守恒定律的应用，该模型在预测性能和训练需求方面具有明显优势。 |
| [^31] | [Zero-shot Classification using Hyperdimensional Computing.](http://arxiv.org/abs/2401.16876) | 本论文提出了一种使用超维计算的零样本分类方法，通过在属性编码器内使用符号-样分布表示的二进制编码表来紧凑地表示模型，可以实现零样本属性提取和零样本分类任务。 |
| [^32] | [Checkmating One, by Using Many: Combining Mixture of Experts with MCTS to Improve in Chess.](http://arxiv.org/abs/2401.16852) | 通过将混合专家方法和MCTS相结合，本研究在国际象棋中显著提升了下棋水平，验证了集成方法的有效性并展示了融入专家知识和战略原则到神经网络中的潜力。 |
| [^33] | [Evaluating ML-Based Anomaly Detection Across Datasets of Varied Integrity: A Case Study.](http://arxiv.org/abs/2401.16843) | 本研究评估了基于机器学习的异常检测在数据完整性不同的数据集上的表现，并发现Random Forest算法在各种数据集上都展现出了卓越的稳健性。 |
| [^34] | [Coseparable Nonnegative Tensor Factorization With T-CUR Decomposition.](http://arxiv.org/abs/2401.16836) | 本文提出了一种基于T-CUR分解的可分离非负张量分解方法，用于在多维数据中提取有意义的特征。 |
| [^35] | [Analysis of Knowledge Tracing performance on synthesised student data.](http://arxiv.org/abs/2401.16832) | 通过合成数据进行训练可以达到与真实数据相似的知识追踪性能。 |
| [^36] | [H2O-Danube-1.8B Technical Report.](http://arxiv.org/abs/2401.16818) | H2O-Danube-1.8B 是一个在 1T 个标记上训练的 18 亿语言模型，具有高度竞争力的指标。同时，他们还发布了一个经过微调和优化训练的聊天模型，进一步推动语言模型的经济民主化。 |
| [^37] | [Encoding Temporal Statistical-space Priors via Augmented Representation.](http://arxiv.org/abs/2401.16808) | 通过增加表示的方式编码时间统计空间先验，以应对时间序列数据建模中的挑战。我们的方法在两个数据集上的实证推广性能明显优于五个最新的基准方法。具有高度模块化性质的方法适用于各种场景。 |
| [^38] | [PBSCSR: The Piano Bootleg Score Composer Style Recognition Dataset.](http://arxiv.org/abs/2401.16803) | 本研究介绍了PBSCSR数据集，用于研究钢琴乐谱作曲家风格识别。数据集包含了盗版乐谱图像和相关元数据，可以进行多个研究任务。 |
| [^39] | [Online Algorithm for Node Feature Forecasting in Temporal Graphs.](http://arxiv.org/abs/2401.16800) | 本文提出了一种在线算法"mspace"，适用于预测时态图中的节点特征。与其他基线方法相比，mspace表现出与最先进方法相当甚至更好的性能，并且在训练样本有限的情况下依然具有鲁棒性。 |
| [^40] | [Learnable Prompt as Pseudo-Imputation: Reassessing the Necessity of Traditional EHR Data Imputation in Downstream Clinical Prediction.](http://arxiv.org/abs/2401.16796) | 本研究提出了一种新的训练协议，可学习提示作为伪插补（PAI），通过构建可学习的提示来模拟下游模型对缺失值的隐含偏好，显著提升所有电子健康记录（EHR）分析模型的性能。 |
| [^41] | [Performance Insights-based AI-driven Football Transfer Fee Prediction.](http://arxiv.org/abs/2401.16795) | 该研究开发了一种利用人工智能的方法，可以预测足球球员的转会费用。这可以帮助俱乐部做出更好的决策，提高表现并增加俱乐部的预算。 |
| [^42] | [Accelerated Cloud for Artificial Intelligence (ACAI).](http://arxiv.org/abs/2401.16791) | ACAI是一个基于云端的机器学习平台，通过实现基于云端的存储和索引的应用，支持模型管理，帮助提高机器学习从业者的生产力。 |
| [^43] | [Enhancing Efficiency and Robustness in Support Vector Regression with HawkEye Loss.](http://arxiv.org/abs/2401.16785) | 通过引入名为HawkEye损失函数的新的对称损失函数，本文解决了支持向量回归在处理离群值和噪声时遇到的挑战，并提供了增强的泛化性能和鲁棒性。 |
| [^44] | [Graph Fairness Learning under Distribution Shifts.](http://arxiv.org/abs/2401.16784) | 论文主要研究了在分布变化下的图公平学习，通过理论分析和实证研究发现了决定图中偏差的因素，并探索了训练图和测试图之间表示距离的影响，对于在图结构数据上确保公平性具有重要意义。 |
| [^45] | [A Literature Review on Fetus Brain Motion Correction in MRI.](http://arxiv.org/abs/2401.16782) | 本文综述了胎儿磁共振成像中脑运动校正的最新进展，包括传统的校正方法和基于深度学习的技术，并提供对潜在解决方案和未来改进的合理观点。 |
| [^46] | [Addressing Distribution Shift in Time Series Forecasting with Instance Normalization Flows.](http://arxiv.org/abs/2401.16777) | 本文提出了一种通过实例规范化流解决时间序列预测中的分布偏移问题的方法，该方法不依赖于固定统计数据，也不限制于预测架构。通过双层优化问题实现转换和预测的联合学习，并提出了实例规范化流作为一种新颖的可逆网络用于时间序列转换。实验证明该方法在合成数据和真实数据上优于最先进的基线模型。 |
| [^47] | [Leveraging Nested MLMC for Sequential Neural Posterior Estimation with Intractable Likelihoods.](http://arxiv.org/abs/2401.16776) | 本研究提出一种嵌套APT方法来解决顺序神经后验估计中的嵌套期望计算问题，从而实现了收敛性分析。 |
| [^48] | [Activity Detection for Massive Connectivity in Cell-free Networks with Unknown Large-scale Fading, Channel Statistics, Noise Variance, and Activity Probability: A Bayesian Approach.](http://arxiv.org/abs/2401.16775) | 本研究探讨了在没有关于网络的重要信息的情况下进行活动检测的问题，并通过使用贝叶斯方法来处理大量未知参数。研究提出了最大后验估计器和变分方法来解决这个问题。 |
| [^49] | [Extrinsicaly Rewarded Soft Q Imitation Learning with Discriminator.](http://arxiv.org/abs/2401.16772) | 本论文提出了一种基于外部奖励的鉴别器的软Q模仿学习方法，旨在解决在少量专家数据和采样数据中进行模仿学习时遇到的困难，同时通过添加基于对抗的奖励函数，使算法更加稳健和高效。 |
| [^50] | [MolPLA: A Molecular Pretraining Framework for Learning Cores, R-Groups and their Linker Joints.](http://arxiv.org/abs/2401.16771) | MolPLA是一个用于学习分子核心、R-基和连接点的分子预训练框架，通过掩码图对比学习可以深入理解分子的可分解部分，同时还可以帮助化学家在先导优化场景中找到可替代的R-基。 |
| [^51] | [Detection and Recovery Against Deep Neural Network Fault Injection Attacks Based on Contrastive Learning.](http://arxiv.org/abs/2401.16766) | 本文提出了一个基于对比学习的深度神经网络故障注入攻击检测和恢复框架（CFDR），通过将对比学习应用于训练和推理流程中，实现了具有自适应能力的深度神经网络推理引擎，在只有一个批次的测试数据和少量无标签测试数据的情况下，能够实时检测并快速恢复多种类型的故障注入攻击。 |
| [^52] | [One-Step Forward and Backtrack: Overcoming Zig-Zagging in Loss-Aware Quantization Training.](http://arxiv.org/abs/2401.16760) | 本文提出了一种新的损失感知量化方法，通过一步向前搜索和回溯的方式解决了梯度下降中出现的曲线行进问题。 |
| [^53] | [SwapNet: Efficient Swapping for DNN Inference on Edge AI Devices Beyond the Memory Budget.](http://arxiv.org/abs/2401.16757) | SwapNet是一种高效的边缘AI设备DNN块交换中间件，在超出内存预算的情况下，通过分解DNN为块并进行交换，实现了大型DNN的高效执行。 |
| [^54] | [Diffusion model for relational inference.](http://arxiv.org/abs/2401.16755) | 这项研究提出了一种关系推理的扩散模型(DiffRI)，通过条件扩散建模学习推断组件之间连接存在的概率，并在无监督方式下发现地面真实相互作用方面具有很高的能力。 |
| [^55] | [AI Oversight and Human Mistakes: Evidence from Centre Court.](http://arxiv.org/abs/2401.16754) | 人工智能系统在纠正人类错误方面起到了积极作用，但此举也潜在导致心理成本，并影响人的决策。通过研究网球比赛中的Hawk-Eye审查系统，我们发现引入AI监督后，裁判员的错误率下降，心理成本导致他们更倾向于将球判为进界，从而产生了类型错判的转变。 |
| [^56] | [Generative AI-based closed-loop fMRI system.](http://arxiv.org/abs/2401.16742) | DecNefGAN是一种基于生成型人工智能的闭环fMRI系统，通过结合生成对抗系统和神经强化模型，研究人类如何对抗和应对生成型人工智能的潜在影响。 |
| [^57] | [Engineering A Large Language Model From Scratch.](http://arxiv.org/abs/2401.16736) | Atinuke是一种基于Transformer的神经网络，通过在处理时序数据的层与注意机制交织在一起，模拟人类语言，从而优化各种语言任务的性能。 |
| [^58] | [Widely Linear Matched Filter: A Lynchpin towards the Interpretability of Complex-valued CNNs.](http://arxiv.org/abs/2401.16729) | 本研究提出了广义线性匹配滤波器（WLMF）范例来实现复杂值CNN的可解释性，解决了在复杂值数据中匹配滤波的难题，并分析了其性能。与标准线性对应物（SLMF）相比，WLMF在输出信噪比方面具有理论上的优势。 |
| [^59] | [SmartFRZ: An Efficient Training Framework using Attention-Based Layer Freezing.](http://arxiv.org/abs/2401.16720) | SmartFRZ是一种通用而智能的层冻结方法，能够自动执行“现场”层冻结，在提高训练效率的同时兼顾通用性和准确性。 |
| [^60] | [OptiState: State Estimation of Legged Robots using Gated Networks with Transformer-based Vision and Kalman Filtering.](http://arxiv.org/abs/2401.16719) | 本研究提出了一种名为OptiState的腿式机器人状态估计方法，该方法通过整合Kalman滤波、优化和学习模式的混合解决方案，结合本体感和外感信息，以精确估计机器人主体的状态。该方法利用关节编码器、IMU测量和基于凸规划的模型预测控制优化，通过Gate循环单元和Vision Transformer自编码器改进了估计结果。研究结果表明，该方法能够提供准确的机器人状态估计，并减小传感器测量和模型简化引起的非线性误差。 |
| [^61] | [Multivariate Beta Mixture Model: Probabilistic Clustering With Flexible Cluster Shapes.](http://arxiv.org/abs/2401.16708) | 本文提出了一种名为多元贝塔混合模型（MBMM）的新的概率模型，用于软聚类。MBMM通过其灵活的多元贝塔分布的概率密度函数适应不同的聚类形状，并在合成和真实数据集上展示了其适应性。 |
| [^62] | [EdgeOL: Efficient in-situ Online Learning on Edge Devices.](http://arxiv.org/abs/2401.16694) | 本文提出了EdgeOL，一种边缘在线学习框架，通过内部和外部调优来优化推理准确性、微调执行时间和能量效率，在边缘设备上实现了显著的性能提升。 |
| [^63] | [Calibration-then-Calculation: A Variance Reduced Metric Framework in Deep Click-Through Rate Prediction Models.](http://arxiv.org/abs/2401.16692) | 本文提出一种新的度量框架，通过减少方差来改进深度学习流水线的性能评估，具有更高的准确性来检测有效建模改进。 |
| [^64] | [Revisiting Gradient Pruning: A Dual Realization for Defending against Gradient Attacks.](http://arxiv.org/abs/2401.16687) | 提出了一种基于梯度剪枝的防御方法，双重梯度剪枝（DGP），可以提高协作学习的通信效率同时保护隐私。 |
| [^65] | [Communication-Efficient Multimodal Federated Learning: Joint Modality and Client Selection.](http://arxiv.org/abs/2401.16685) | 本文提出了一种新的多模态联邦学习方法，通过联合模态和客户选择来解决多样的模态集合和通信限制的挑战。 |
| [^66] | [Polynomial Chaos Expansions on Principal Geodesic Grassmannian Submanifolds for Surrogate Modeling and Uncertainty Quantification.](http://arxiv.org/abs/2401.16683) | 本论文提出了一种基于流形学习的代理建模框架，用于高维随机系统中的不确定性量化。通过在Grassmann流形上进行主测地分析，识别出一组潜在的低维描述符，然后利用多项式混沌展开构建映射。 |
| [^67] | [The Detection and Understanding of Fictional Discourse.](http://arxiv.org/abs/2401.16678) | 本文介绍了虚构话语检测的分类实验，利用了多样的数据集和新的特征集。虚构话语的检测有助于丰富大型文化遗产档案，并帮助更广泛地理解虚构叙事的特质。 |
| [^68] | [T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives.](http://arxiv.org/abs/2401.16677) | T3提出了一种透明跟踪和触发的硬件-软件协同设计方法，用于解决大型语言模型在分布式训练和推理中的通信效率问题。 |
| [^69] | [Is Artificial Intelligence Providing the Second Revolution for Weather Forecasting?.](http://arxiv.org/abs/2401.16669) | 人工智能技术在天气预报领域的快速发展代表了一个重大突破，它克服了传统模型的局限性，有潜力引领天气预报的第二次革命。 |
| [^70] | [Fast Dual-Regularized Autoencoder for Sparse Biological Data.](http://arxiv.org/abs/2401.16664) | 本文提出了一种快速双正则自编码器用于稀疏生物数据的问题。该方法相对于现有最先进方法在预测药物靶点相互作用和药物疾病关联方面具有速度和准确性的优势。 |
| [^71] | [Generalization of LiNGAM that allows confounding.](http://arxiv.org/abs/2401.16661) | 本文提出了一种名为LiNGAM-MMI的方法，可以增强LiNGAM模型以处理混淆问题。该方法使用KL散度量化混淆程度，并通过最短路径问题解决方案高效地确定变量顺序，不论是否存在混淆情况。实验证明，LiNGAM-MMI可以更准确地识别正确的变量顺序。 |
| [^72] | [Rademacher Complexity of Neural ODEs via Chen-Fliess Series.](http://arxiv.org/abs/2401.16655) | 本文通过Chen-Fliess序列展开将连续深度神经ODE模型转化为单层、无限宽度的网络，并利用此框架推导出了将初始条件映射到某个终端时间的ODE模型的Rademacher复杂度的紧凑表达式。 |
| [^73] | [Augmenting Replay in World Models for Continual Reinforcement Learning.](http://arxiv.org/abs/2401.16650) | 本研究通过在回放缓冲区中应用增强方法，成功地解决了增强连续强化学习中的内存限制问题，并在世界模型中有效防止灾难性遗忘。 |
| [^74] | [Using Motion Forecasting for Behavior-Based Virtual Reality (VR) Authentication.](http://arxiv.org/abs/2401.16649) | 本研究提出了一种利用动作预测进行基于行为的虚拟现实认证的方法，通过预测用户的未来行为轨迹并进行认证，解决了现有技术在使用较小轨迹段时的性能下降问题。 |
| [^75] | [Speeding up and reducing memory usage for scientific machine learning via mixed precision.](http://arxiv.org/abs/2401.16645) | 通过使用混合精度训练神经网络，可以加速科学机器学习并减少内存使用，但对于PINNs和DeepONets等方法，半精度不适用。 |
| [^76] | [TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese.](http://arxiv.org/abs/2401.16640) | 这篇论文开发了用于低资源环境中的开放式基础模型，以巴西葡萄牙语为例，发布在GitHub和Hugging Face上供社区使用和进一步开发。 |
| [^77] | [Improving Reinforcement Learning from Human Feedback with Efficient Reward Model Ensemble.](http://arxiv.org/abs/2401.16635) | 本论文提出一种通过高效的奖励模型集成来改进人工反馈强化学习的方法，以解决由于奖励模型预测不准确而导致RLHF输出与人类价值观不一致的问题。 |
| [^78] | [The Why, When, and How to Use Active Learning in Large-Data-Driven 3D Object Detection for Safe Autonomous Driving: An Empirical Exploration.](http://arxiv.org/abs/2401.16634) | 本研究探究了在自主驾驶数据集中使用主动学习进行3D物体检测的方法。通过使用熵查询选择信息样本，可以降低标注成本并提高模型性能，特别是在减少多数类和少数类之间的性能差距方面表现优异。研究结果表明，选择多样且信息丰富的数据对于模型训练至关重要，而熵查询是在资源有限的环境中选择增强模型学习的数据的一种有前景的策略。 |
| [^79] | [Algebraic Complexity and Neurovariety of Linear Convolutional Networks.](http://arxiv.org/abs/2401.16613) | 本文研究了具有一维滤波器和任意步幅的线性卷积网络的代数复杂度和神经多样性。通过引入递归算法和度量代数几何的工具，我们发现这些网络优化中的关键点数量显著超过了完全连接的线性网络。 |
| [^80] | [Learning a Gaussian Mixture for Sparsity Regularization in Inverse Problems.](http://arxiv.org/abs/2401.16612) | 本研究提出了一种基于高斯混合模型的稀疏正则化方法，通过神经网络进行贝叶斯估计，有效地解决了逆问题中的稀疏建模和参数估计问题。 |
| [^81] | [Accelerating superconductor discovery through tempered deep learning of the electron-phonon spectral function.](http://arxiv.org/abs/2401.16611) | 通过采用非传统的训练策略，结合调控深度学习模型，成功预测了电子声子谱函数，并得到了超导材料的关键参数，加速了超导体的发现。 |
| [^82] | [Consistent algorithms for multi-label classification with macro-at-$k$ metrics.](http://arxiv.org/abs/2401.16594) | 该研究提出了一种针对多标签分类的一致算法，主要解决了宏观at-$k$度量的优化问题。通过在人口效用框架下考虑复杂性能度量的优化，该算法在极端分类问题中表现出色。 |
| [^83] | [Autoencoder-Based Domain Learning for Semantic Communication with Conceptual Spaces.](http://arxiv.org/abs/2401.16569) | 这篇论文研究了基于概念空间的语义通信，提出了一个自编码器学习的框架，解决了无法捕捉和量化“意义”的问题。 |
| [^84] | [Deep Learning for Multi-Label Learning: A Comprehensive Survey.](http://arxiv.org/abs/2401.16549) | 深度学习在多标签学习中的综合调研，旨在审视深度学习在解决多标签分类中的挑战方面的最新进展。 |
| [^85] | [Efficient Observation Time Window Segmentation for Administrative Data Machine Learning.](http://arxiv.org/abs/2401.16537) | 本文研究了如何将机器学习模型的观察窗口划分为时间段，通过优化高优先级特征的时间bin大小，可以实现更简单、更快速训练的机器学习模型，并且能够达到与更复杂模型相似甚至更好的性能。 |
| [^86] | [Validation, Robustness, and Accuracy of Perturbation-Based Sensitivity Analysis Methods for Time-Series Deep Learning Models.](http://arxiv.org/abs/2401.16521) | 本研究验证了时间序列深度学习模型的扰动敏感性分析方法的可靠性和准确性，并比较了不同方法和模型的影响。 |
| [^87] | [MT-HCCAR: Multi-Task Deep Learning with Hierarchical Classification and Attention-based Regression for Cloud Property Retrieval.](http://arxiv.org/abs/2401.16520) | 这篇论文提出了一种名为MT-HCCAR的多任务深度学习模型，用于云属性检索。该模型考虑了云属性检索任务之间的层级关系，并具有对不同传感器数据集具有健壮泛化能力的特点。 |
| [^88] | [AFSD-Physics: Exploring the governing equations of temperature evolution during additive friction stir deposition by a human-AI teaming approach.](http://arxiv.org/abs/2401.16501) | 本文使用人工智能与人类协作的方法，提出了AFSD-Physics模型，通过学习实验数据，得到了添加摩擦搅拌堆积过程中温度演变的控制方程。该模型具有物理解释性、计算成本低且准确度高，与实际测量结果吻合较好。 |
| [^89] | [A Discriminative Bayesian Gaussian Process Latent Variable Model for High-Dimensional Data.](http://arxiv.org/abs/2401.16497) | 本研究提出了一个名为LDGD的判别贝叶斯高斯过程潜变量模型，能够有效地从高维数据中提取信息，并具有较高的预测准确性和鲁棒性。 |
| [^90] | [GPU Cluster Scheduling for Network-Sensitive Deep Learning.](http://arxiv.org/abs/2401.16492) | 我们提出了一种GPU集群调度器，用于分布式深度学习任务，根据任务对通信网络延迟的敏感性进行GPU资源的邻近基础一致性。相比传统的调度方法，我们的调度器可以提供高达69％的端到端Makespan提升。 |
| [^91] | [High-Quality Image Restoration Following Human Instructions.](http://arxiv.org/abs/2401.16468) | 本论文提出了一种使用人类编写的指令来指导图像恢复模型的方法，并在多个恢复任务上取得了最先进的结果，为基于文本指导的图像恢复和增强研究提供了一个新的基准。 |
| [^92] | [ReGAL: Refactoring Programs to Discover Generalizable Abstractions.](http://arxiv.org/abs/2401.16467) | ReGAL提出了一种用于发现通用抽象的程序重构方法，可以通过重构代码学习可重用的函数库，利用这些共享函数库可以更准确地预测程序。 |
| [^93] | [Towards Regret Free Slot Allocation in Billboard Advertisement.](http://arxiv.org/abs/2401.16464) | 本文提出了一种无遗憾的广告牌广告时段分配方法，旨在最大化广告商与顾客之间的影响力，并通过四种高效的解决方案来减少遗憾的发生。 |
| [^94] | [Supervised Contrastive Learning based Dual-Mixer Model for Remaining Useful Life Prediction.](http://arxiv.org/abs/2401.16462) | 本文提出了一种基于监督对比学习的双混合模型，用于剩余寿命预测。该模型通过灵活的特征融合和特征空间全局关系不变性训练方法，提高了预测准确性。 |
| [^95] | [Norm Enforcement with a Soft Touch: Faster Emergence, Happier Agents.](http://arxiv.org/abs/2401.16461) | 通过温和的规范执行，该研究提出了一种新的方法，通过智能体之间的交流推动合作并促进规范的出现。 |
| [^96] | [Credit Risk Meets Large Language Models: Building a Risk Indicator from Loan Descriptions in P2P Lending.](http://arxiv.org/abs/2401.16458) | 本文研究了如何利用P2P借贷平台上借款人提供的文本描述来构建风险指标。结果显示，利用大型语言模型生成的风险评分可以明显提高信用风险分类器的性能。 |
| [^97] | [Effective Controllable Bias Mitigation for Classification and Retrieval using Gate Adapters.](http://arxiv.org/abs/2401.16457) | 本文引入了可控门适配器（ConGater），一种具有可调节敏感性参数的新颖模块化门机制，可在推理时逐渐过渡从模型的偏向状态到完全去偏的版本，并通过实验证明了其在分类和检索任务中的性能。 |
| [^98] | [Hybrid Transformer and Spatial-Temporal Self-Supervised Learning for Long-term Traffic Prediction.](http://arxiv.org/abs/2401.16453) | 本文提出了一个将混合Transformer和时空自监督学习相结合的模型，通过应用自适应数据增强技术和Chebyshev多项式图卷积来提高模型的鲁棒性和对复杂空间依赖性的捕捉能力，并设计了两个自监督学习任务来建模时空异质性，从而提高了模型的准确性和泛化能力。 |
| [^99] | [Context-Former: Stitching via Latent Conditioned Sequence Modeling.](http://arxiv.org/abs/2401.16452) | Context-Former是一种集成了基于情境信息的模仿学习和序列建模的方法，通过拼接次优轨迹片段来改善决策，并提高了Decision Transformer的性能。 |
| [^100] | [AI in Energy Digital Twining: A Reinforcement Learning-based Adaptive Digital Twin Model for Green Cities.](http://arxiv.org/abs/2401.16449) | 本研究提出了一种基于强化学习的自适应数字双胞胎模型，利用时空图和深度Q网络来支持动态的智能城市环境。实验结果表明，该模型在准确性、同步性、资源优化和能源效率方面具有明显的优势，同时在图数据库中实现时表现出更高的查询性能。此外，该模型还能够实现实时数据捕获，降低开销和能源消耗。 |
| [^101] | [OMPGPT: A Generative Pre-trained Transformer Model for OpenMP.](http://arxiv.org/abs/2401.16445) | OMPGPT是一种为了OpenMP pragma生成而设计的生成式预训练Transformer模型，采用了来自NLP领域的提示工程技术，并创建了一种创新的策略chain-of-OMP。 |
| [^102] | [Evaluating Deep Networks for Detecting User Familiarity with VR from Hand Interactions.](http://arxiv.org/abs/2401.16443) | 通过使用深度分类器和手部追踪技术，本文提出了一种评估用户对虚拟现实熟悉程度的方法，以便在用户不熟悉虚拟现实时为其提供按需培训，从而提高其在虚拟环境中的任务完成效率。 |
| [^103] | [FaKnow: A Unified Library for Fake News Detection.](http://arxiv.org/abs/2401.16441) | FaKnow是一个统一的虚假新闻检测算法库，包含多种常用的模型和工具，并解决了不同框架下的可重复性和冗余问题。 |
| [^104] | [Beyond Eviction Prediction: Leveraging Local Spatiotemporal Public Records to Inform Action.](http://arxiv.org/abs/2401.16440) | 该研究利用当地时空公共记录来预测驱逐风险，并证明这些预测对于指导有针对性的外展政策是有用的。 |
| [^105] | [Polynomial time auditing of statistical subgroup fairness for Gaussian data.](http://arxiv.org/abs/2401.16439) | 这篇论文研究了使用统计子组不公平性概念对分类器进行审计的问题，并给出了对高斯分布的审计结果。他们提供了一种替代方法来利用无偏学习的进展。 |
| [^106] | [Do deep neural networks utilize the weight space efficiently?.](http://arxiv.org/abs/2401.16438) | 该论文介绍了一种利用权重矩阵的列空间和行空间的新概念，可以大幅减少深度学习模型的参数而不影响性能。实验证明该方法能够在资源有限的情况下实现参数高效的深度学习模型，并在ImageNet数据集上展现了竞争性的性能。 |
| [^107] | [A Benchmark Dataset for Tornado Detection and Prediction using Full-Resolution Polarimetric Weather Radar Data.](http://arxiv.org/abs/2401.16437) | 本研究提出了一个新的基准数据集TorNet，用于支持龙卷风检测和预测中的机器学习算法的发展。该数据集包含了十年的具有全分辨率和极化特性的天气雷达数据，为训练和评估机器学习算法提供了重要的资源。 |
| [^108] | [A novel ANROA based control approach for grid-tied multi-functional solar energy conversion system.](http://arxiv.org/abs/2401.16434) | 本文提出了一种基于ANROA方法的多功能并网太阳能转换系统的自适应控制方法，使用了自适应神经模糊推理系统和Rain优化算法，并成功实现了电力质量问题的避免和单位功率因数运行模式。 |
| [^109] | [Within-basket Recommendation via Neural Pattern Associator.](http://arxiv.org/abs/2401.16433) | 本文介绍了一种称为神经模式关联器（NPA）的深度商品关联挖掘模型，该模型能够明确地建模购物过程中的复杂用户行为，并通过注意力驱动的查找来识别用户的购物意图。 |
| [^110] | [Improving conversion rate prediction via self-supervised pre-training in online advertising.](http://arxiv.org/abs/2401.16432) | 这项研究通过自监督预训练方法，改进了在线广告系统中的转化率预测。由于数据稀疏性的挑战，添加非点击归因的转化会损坏模型的校准，而自监督预训练能够解决这个问题。 |
| [^111] | [Combining topic modelling and citation network analysis to study case law from the European Court on Human Rights on the right to respect for private and family life.](http://arxiv.org/abs/2401.16429) | 本文研究了结合主题建模和引用网络分析的方法，用来研究欧洲人权法院关于尊重私密和家庭生活的案例法。通过这种方法，可以找到和组织具有相似主题和引用模式的案例法，并且通过结合这两种技术能够得到更好的结果。 |
| [^112] | [Informal Safety Guarantees for Simulated Optimizers Through Extrapolation from Partial Simulations.](http://arxiv.org/abs/2401.16426) | 通过从部分模拟中推断出的方式，为模拟优化器提供非正式的安全保证。 |
| [^113] | [cDVGAN: One Flexible Model for Multi-class Gravitational Wave Signal and Glitch Generation.](http://arxiv.org/abs/2401.16356) | cDVGAN是一个灵活的生成对抗网络模型，用于模拟多类引力波信号和探测器故障，并通过引入辅助鉴别器分析一阶导数时间序列来更好地捕捉原始数据特征。 |
| [^114] | [Cross-silo Federated Learning with Record-level Personalized Differential Privacy.](http://arxiv.org/abs/2401.16251) | 本文研究了跨领域联合学习中基于记录级个性化差分隐私的问题，设计了一个名为rPDP-FL的新型框架，并提出了多功能解决方案“模拟-曲线拟合”，以满足不同记录的隐私需求。 |
| [^115] | [Self-Supervised Learning in Event Sequences: A Comparative Study and Hybrid Approach of Generative Modeling and Contrastive Learning.](http://arxiv.org/abs/2401.15935) | 本研究通过比较研究和混合方法，调查了事件序列的自我监督学习技术，并引入了一种新的方法，将生成模型和对比嵌入进行对齐。结果显示，这种对齐模型在各种任务上表现优越，为预测事件序列中的信息提供了潜在的好处。 |
| [^116] | [Bayesian Nonparametrics meets Data-Driven Robust Optimization.](http://arxiv.org/abs/2401.15771) | 本文提出了一种将贝叶斯非参数方法与最新的决策理论模型相结合的鲁棒优化准则，通过这种方法，可以在线性回归问题中获得有稳定性和优越性能的结果。 |
| [^117] | [FDR-Controlled Portfolio Optimization for Sparse Financial Index Tracking.](http://arxiv.org/abs/2401.15139) | 本论文提出了一种扩展的T-Rex框架，用于在稀疏金融指数跟踪中选择少数相关变量，并通过集成最近邻惩罚机制，可靠控制误发现率（FDR）。实验证明了该方法在过去20年内基于少量股票准确跟踪标准普尔500指数的能力。 |
| [^118] | [Discovering Mathematical Formulas from Data via GPT-guided Monte Carlo Tree Search.](http://arxiv.org/abs/2401.14424) | 通过结合MCTS和生成式预训练模型，我们提出了一种新的符号回归算法SR-GPT，在发现数据中的数学公式方面取得了显著的改进。 |
| [^119] | [Prompt Design and Engineering: Introduction and Advanced Methods.](http://arxiv.org/abs/2401.14423) | 本文介绍了提示设计与工程的主要概念，并回顾了基本和更高级的方法。 |
| [^120] | [Investigating the Efficacy of Large Language Models for Code Clone Detection.](http://arxiv.org/abs/2401.13802) | 这项研究探索了大型语言模型在代码克隆检测任务中的应用。 |
| [^121] | [In-Context Language Learning: Architectures and Algorithms.](http://arxiv.org/abs/2401.12973) | 本文通过研究一个新的问题家族——上下文语言学习（ICLL），探讨了大规模神经语言模型在上下文学习中的能力。在ICLL中，模型通过生成与给定形式语言相同的字符串来进行上下文学习。研究结果对于理解真实场景中的上下文学习以及神经语言模型的发展具有重要意义。 |
| [^122] | [Consistency Enhancement-Based Deep Multiview Clustering via Contrastive Learning.](http://arxiv.org/abs/2401.12648) | 本文提出了一种基于一致性增强的深度多视图聚类方法通过对比学习（CCEC）。该方法通过引入语义连接块并入特征表示中，以保持多个视图间的一致信息，并通过谱聚类改善聚类的表示过程。实验结果显示，该方法在多个数据集上的表现优于其他现有方法。 |
| [^123] | [Tensor-view Topological Graph Neural Network.](http://arxiv.org/abs/2401.12007) | 提出了一种新颖的Tensor视图拓扑图神经网络（TTG-NN），该方法结合了持久同调、图卷积和张量运算，同时捕捉了局部和全局层面上的Tensor视图拓扑（TT）和Tensor视图图（TG）结构信息。 |
| [^124] | [A Systematic Evaluation of Euclidean Alignment with Deep Learning for EEG Decoding.](http://arxiv.org/abs/2401.10746) | 本研究系统评估了使用深度学习和欧几里得对齐对脑电解码的影响。结果表明，欧几里得对齐能够显著提高解码率，并且减少了收敛时间。 |
| [^125] | [Noise Contrastive Estimation-based Matching Framework for Low-resource Security Attack Pattern Recognition.](http://arxiv.org/abs/2401.10337) | 该论文提出了一种基于噪声对比估计的低资源安全攻击模式识别匹配框架，通过直接语义相似度决定文本与攻击模式之间的关联，以降低大量类别、标签分布不均和标签空间复杂性带来的学习难度。 |
| [^126] | [Comparison analysis between standard polysomnographic data and in-ear-EEG signals: A preliminary study.](http://arxiv.org/abs/2401.10107) | 本研究通过比较标准多导睡眠图（PSG）和耳内脑电信号的相似性，旨在探索一种更少侵入、成本效益高和便携的替代方法。研究确定了评估方法，并通过提取特征进行分析。 |
| [^127] | [Rethinking Spectral Graph Neural Networks with Spatially Adaptive Filtering.](http://arxiv.org/abs/2401.09071) | 本文重新思考了谱图神经网络，并揭示了谱滤波和空间聚合之间的联系。该研究发现，谱滤波在隐含地将原始图转换成适应性新图，并明确计算用于空间聚合的新图。适应性新图展现出非局部性，并能够反映节点之间的标签一致性。 |
| [^128] | [Augmenting Math Word Problems via Iterative Question Composing.](http://arxiv.org/abs/2401.09003) | 本研究通过引入MMIQC数据集和迭代组合问题(IQC)的新颖增强方法，成功提高了大型语言模型的数学推理能力，在竞赛级数学问题上取得了优于先前最佳结果的准确率。 |
| [^129] | [Evaluating the Utility of Conformal Prediction Sets for AI-Advised Image Labeling.](http://arxiv.org/abs/2401.08876) | 本研究评估了符合预测集在AI辅助图像标注中的效用，发现对于简单图像，预测集与Top-1和Top-k显示的准确性相当，但在标记分布外图像时特别有效，尤其是集合大小较小时。 |
| [^130] | [RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture.](http://arxiv.org/abs/2401.08406) | 本文评估了检索增强生成（RAG）和微调两种方法在大型语言模型上的性能差异，并提出了适用于农业数据集的管道和权衡。 |
| [^131] | [TwinBooster: Synergising Large Language Models with Barlow Twins and Gradient Boosting for Enhanced Molecular Property Prediction.](http://arxiv.org/abs/2401.04478) | TwinBooster结合了大语言模型、Barlow Twins和梯度提升，通过整合生物检测方法和分子指纹，实现了对未见过的生物检测方法和分子属性的精确预测，该方法在数据稀缺的情况下展现出了优秀的性能。 |
| [^132] | [Generating Non-Stationary Textures using Self-Rectification.](http://arxiv.org/abs/2401.02847) | 本文提出了一种使用自校正来生成非平稳纹理的方法，通过使用预训练扩散网络和自注意机制，可以将用户修改的参考纹理细化为一种连贯、无缝的纹理，并保留参考样本的独特视觉特征。实验证实表明，该方法在处理非平稳纹理方面具有卓越的能力，相比现有技术在纹理合成方面取得了显著的进展。 |
| [^133] | [Powerformer: A Section-adaptive Transformer for Power Flow Adjustment.](http://arxiv.org/abs/2401.02771) | Powerformer是一种适应不同传输区段的变压器架构，用于学习稳健电力系统状态表示。它通过开发专用的区段自适应注意机制，并引入图神经网络传播和多因素注意机制来提供更加稳健的状态表示。在三个不同的电力系统场景上进行了广泛评估。 |
| [^134] | [HAAQI-Net: A non-intrusive neural music quality assessment model for hearing aids.](http://arxiv.org/abs/2401.01145) | HAAQI-Net是一种适用于助听器用户的非侵入性神经音质评估模型，通过使用BLSTM和注意力机制，以及预训练的BEATs进行声学特征提取，能够快速且准确地预测音乐的HAAQI得分，相比传统方法具有更高的性能和更低的推理时间。 |
| [^135] | [Reinforcement Unlearning.](http://arxiv.org/abs/2312.15910) | 强化学习中的消除学习是一种新兴的研究领域，旨在解决环境所有者有权撤销智能体训练数据的隐私问题。该领域面临三个主要挑战。 |
| [^136] | [Causal Forecasting for Pricing.](http://arxiv.org/abs/2312.15282) | 本文提出了一种在定价环境下进行需求预测的新方法，通过将因果推断的双重机器学习方法和最先进的基于变压器的预测模型结合在一起，我们的方法在完全控制的情况下更好地估计因果效应，并在离线政策设置中优于其他预测方法。 |
| [^137] | [Auto311: A Confidence-guided Automated System for Non-emergency Calls.](http://arxiv.org/abs/2312.14185) | Auto311是第一个处理非紧急电话的自动化系统，它通过减轻非紧急电话负担，提供快速有效的响应。通过预测事件类型并生成个性化的案件报告，并从对话上下文中提取关键信息来完善报告，系统与主叫人之间的对话结构得到优化。 |
| [^138] | [Weighted least-squares approximation with determinantal point processes and generalized volume sampling.](http://arxiv.org/abs/2312.14057) | 该论文研究了使用行列式点过程和广义体积取样进行加权最小二乘逼近的问题，提出了广义版本的体积标准化取样算法，并证明了该算法在期望上的准最优性以及在某些规范向量空间中的逼近结果。 |
| [^139] | [Locating Factual Knowledge in Large Language Models: Exploring the Residual Stream and Analyzing Subvalues in Vocabulary Space.](http://arxiv.org/abs/2312.12141) | 通过探索剩余流和分析词汇空间中的子值，我们定位了大型语言模型中的事实知识，并找到了存储了有关“法国，首都，巴黎”的知识的位置。 |
| [^140] | [Toward A Reinforcement-Learning-Based System for Adjusting Medication to Minimize Speech Disfluency.](http://arxiv.org/abs/2312.11509) | 这个论文介绍了一种基于强化学习的系统，该系统可以根据患者言语不流畅程度自动调整药物，通过对药物组合的强化学习算法的优化，能够收敛到良好的用药方案。 |
| [^141] | [Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs.](http://arxiv.org/abs/2312.05934) | 该研究比较了无监督的微调和检索增强生成（RAG）这两种常见方法在LLMs中的应用。结果发现，RAG在现有知识和新知识上表现出更好的性能，而LLMs通过无监督的微调学习新的事实信息较困难。 |
| [^142] | [MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs.](http://arxiv.org/abs/2312.03731) | 本文提出了一种名为MultiGPrompt的多任务预训练和提示框架，用于在图形表示学习中提高鲁棒性和减少标注成本。 |
| [^143] | [Self-Infilling Code Generation.](http://arxiv.org/abs/2311.17972) | 本文介绍了自补代码生成的通用框架，利用自补机制实现了中断和循环机制，使传统解码进程变得非单调。利用中断机制可以推迟生成代码，增强对输出的控制；利用循环机制可以循环更新和同步生成的每个部分。 |
| [^144] | [Enhancing Low-Order Discontinuous Galerkin Methods with Neural Ordinary Differential Equations for Compressible Navier--Stokes Equations.](http://arxiv.org/abs/2310.18897) | 本研究提出了一种方法，通过在不连续Galerkin方法中加入神经常微分方程，学习子网格尺度模型的效果，从而提高模拟的准确性和加速计算过程。 |
| [^145] | [Clover: Closed-Loop Verifiable Code Generation.](http://arxiv.org/abs/2310.17807) | Clover是一种闭环可验证代码生成的范式，通过在代码、docstrings和形式注释之间进行一致性检查，确保生成的代码的正确性。 |
| [^146] | [Solving large flexible job shop scheduling instances by generating a diverse set of scheduling policies with deep reinforcement learning.](http://arxiv.org/abs/2310.15706) | 本文提出了一种能够通过生成多样化的调度策略来解决大型柔性车间调度实例的方法，并应用深度强化学习来优化调度质量。 |
| [^147] | [Towards Zero Shot Learning in Restless Multi-armed Bandits.](http://arxiv.org/abs/2310.14526) | 通过开发一个基于神经网络的预训练模型，我们实现了在不断变化的多臂赌博机中的零样本学习，该模型具有泛化能力，并且能够在特定实例上进行高效微调，同时适用于多行为设置和离散或连续状态空间。 |
| [^148] | [Learning Interpretable Rules for Scalable Data Representation and Classification.](http://arxiv.org/abs/2310.14336) | 这项研究提出了一种名为RRL的新型分类器，通过自动学习可解释的非模糊规则，实现了数据表示和分类的良好可扩展性和解释性。 |
| [^149] | [Graph Neural Networks with polynomial activations have limited expressivity.](http://arxiv.org/abs/2310.13139) | 本文证明了具有多项式激活函数的图神经网络无法表达GC2查询，与常用的非多项式激活函数存在分离，这回答了一个开放问题。 |
| [^150] | [Equivariant Matrix Function Neural Networks.](http://arxiv.org/abs/2310.10434) | 矩阵函数神经网络（MFNs）是一种通过解析矩阵等变函数来参数化非局部相互作用的新型架构，能够在各种应用中实现最先进的性能。 |
| [^151] | [Ring-A-Bell! How Reliable are Concept Removal Methods for Diffusion Models?.](http://arxiv.org/abs/2310.10012) | 本文研究了对于文本到图像合成的扩散模型中的潜在滥用问题的安全措施的有效性，并提出了一个用于评估的新颖概念检索算法。我们引入了一个名为Ring-A-Bell的模型无关的红队工具，可以事先准备整个评估过程，而无需先验知识。 |
| [^152] | [Data-centric Graph Learning: A Survey.](http://arxiv.org/abs/2310.04987) | 本综述从数据中心化的角度全面评估了图学习方法，回答了何时修改图数据、图数据的哪一部分需要修改以及如何保护图模型的关键问题。 |
| [^153] | [Dynamic DAG Discovery for Interpretable Imitation Learning.](http://arxiv.org/abs/2310.00489) | 提出了一种用于解释模仿学习中神经代理的动态DAG发现方法，通过有向无环因果图展现其捕获的知识，以增加透明度和可解释性。 |
| [^154] | [Adversarial Machine Learning in Latent Representations of Neural Networks.](http://arxiv.org/abs/2309.17401) | 这项研究通过分析分布式深度神经网络对抗性行为的韧性填补了现有研究空白，并发现潜在特征在相同信息失真水平下比输入表示更加韧性，并且对抗性韧性由特征维度和神经网络的泛化能力共同决定。 |
| [^155] | [Automatically Testing Functional Properties of Code Translation Models.](http://arxiv.org/abs/2309.12813) | 本研究介绍了一种自动、功能性的代码翻译模型测试方法，能够捕捉各种属性从纯语法到纯语义的相关信息，并在实验中证明其有效性。 |
| [^156] | [Towards Differential Privacy in Sequential Recommendation: A Noisy Graph Neural Network Approach.](http://arxiv.org/abs/2309.11515) | 这项工作提出了一种新颖的差分隐私顺序推荐框架，采用了噪声图神经网络方法，解决了现有差分隐私推荐系统在动态和依赖关系方面的局限性，同时也关注了敏感用户特征的隐私风险。 |
| [^157] | [Circuit Breaking: Removing Model Behaviors with Targeted Ablation.](http://arxiv.org/abs/2309.05973) | 本论文提出了一种通过有针对性的消融模型组件之间的因果路径来去除语言模型中不良行为的新方法。在减少GPT-2毒性语言生成方面，仅消融12条因果边中的11.6K可以有效减轻毒性生成，并在其他输入上的性能下降很小。 |
| [^158] | [Viewing the process of generating counterfactuals as a source of knowledge -- Application to the Naive Bayes classifier.](http://arxiv.org/abs/2309.04284) | 将生成对立假设的过程视为知识来源，并应用于朴素贝叶斯分类器，展示其有趣属性。 |
| [^159] | [Learning Hybrid Dynamics Models With Simulator-Informed Latent States.](http://arxiv.org/abs/2309.02873) | 本文提出了一种通过用模拟器更新潜在状态的方法来学习混合动力学模型的新方法，以控制预测并防止累积误差的发生。 |
| [^160] | [Exact Inference for Continuous-Time Gaussian Process Dynamics.](http://arxiv.org/abs/2309.02351) | 本论文提出了一种对连续时间高斯过程动力学进行精确推断的方法，解决了在离散时间下进行预测可能带来的问题，并利用高阶数值积分器进行动力学函数的离散化，避免了传统方法中的近似推断的限制。 |
| [^161] | [Benchmarking Autoregressive Conditional Diffusion Models for Turbulent Flow Simulation.](http://arxiv.org/abs/2309.01745) | 这项工作研究了机器学习求解器在模拟湍流流场时如何实现时间稳定性，并发现基于条件扩散模型的自回归演化方法在准确性和稳定性方面可以超越其他流场预测方法。 |
| [^162] | [Data-Driven Projection for Reducing Dimensionality of Linear Programs: Generalization Bound and Learning Methods.](http://arxiv.org/abs/2309.00203) | 本文研究了一种简单的数据驱动方法，通过学习投影矩阵来降低高维线性规划问题的维数，实现更快的求解速度。基于“数据驱动算法设计”，提出了泛化保证的数据量与性能指标的伪维度的上界和下界。 |
| [^163] | [TransGNN: Harnessing the Collaborative Power of Transformers and Graph Neural Networks for Recommender Systems.](http://arxiv.org/abs/2308.14355) | TransGNN是一种将Transformer和GNN层交替结合以相互增强其能力的新型模型，用于解决当前基于GNN的推荐系统面临的感受域有限和存在噪音连接的挑战。 |
| [^164] | [Efficient Benchmarking (of Language Models).](http://arxiv.org/abs/2308.11696) | 本研究提出了一种名为"Efficient Benchmarking"的问题，旨在智能地减少语言模型评估的计算成本而不降低可靠性，并使用一种名为Decision Impact on Reliability（DIoR）的新度量来评估决策的可靠性。通过HELM基准测试的案例研究，发现只需删除一个低排名模型即可改变领先者，并仅需少量示例即可得到正确的基准测试排名。 |
| [^165] | [Convergence of SGD for Training Neural Networks with Sliced Wasserstein Losses.](http://arxiv.org/abs/2307.11714) | 本论文研究了使用切片Wasserstein损失训练神经网络时，随机梯度下降算法的收敛性，并证明了在特定条件下，SGD轨迹逼近了梯度流方程的集合。 |
| [^166] | [ENN: A Neural Network with DCT Adaptive Activation Functions.](http://arxiv.org/abs/2307.00673) | ENN是一种具有DCT自适应激活函数的神经网络模型，通过使用反向传播自适应地调整激活函数，提供了高度的灵活性和表现力。在解释网络收敛过程中，我们恢复了每个激活函数在输出空间中的响应，即“凸起”。通过实验证明了该模型在多个任务上的性能优势。 |
| [^167] | [Unified Transfer Learning Models for High-Dimensional Linear Regression.](http://arxiv.org/abs/2307.00238) | UTrans是一种统一转移学习模型，它能检测可转移变量和源数据，并具有较低的估计和预测误差，同时保持可解释性。 |
| [^168] | [MILD: Modeling the Instance Learning Dynamics for Learning with Noisy Labels.](http://arxiv.org/abs/2306.11560) | MILD模型化了学习动态，通过基于Weibull混合模型的迭代选择方法，识别干净的数据实例，以减少网络对带有噪声标签的数据的记忆影响。 |
| [^169] | [Are ChatGPT and Other Similar Systems the Modern Lernaean Hydras of AI?.](http://arxiv.org/abs/2306.09267) | 生成式人工智能系统的崛起引发了版权和创新保护的问题。建议对开源代码许可证进行更改，限制AI系统对代码的访问和使用，并探讨与AI和版权之间的关系引发的问题。 |
| [^170] | [Vector-Quantized Graph Auto-Encoder.](http://arxiv.org/abs/2306.07735) | 本文介绍了一种用于建模图分布的置换等变离散自编码器(VQ-GAE)，它采用向量量化防止将离散对象映射到连续潜在空间中，并利用自回归模型捕获了图的全局结构，实验表明具有优异性能。 |
| [^171] | [Simple and Controllable Music Generation.](http://arxiv.org/abs/2306.05284) | 本文提出了 MusicGen，一个单一的语言模型，可以在条件描述或旋律特征控制下生成高质量的样本，并且在标准的文本到音乐基准上的实证研究中，该方法优于其他基线模型。 |
| [^172] | [Zero-Shot Blind Audio Bandwidth Extension.](http://arxiv.org/abs/2306.01433) | 本文提出了一种名为BABE的新方法，在零样本情况下解决了具有挑战性的盲音频带宽扩展问题，实现了比最先进的盲带宽扩展基线更好的性能，并且表现出了强大的泛化能力。 |
| [^173] | [DoWG Unleashed: An Efficient Universal Parameter-Free Gradient Descent Method.](http://arxiv.org/abs/2305.16284) | 本文提出了一种名为DoWG的无参数梯度下降方法，它是第一个既高效又通用的算法，能够自适应于平稳和非平稳问题，并且无需回溯搜索过程。 |
| [^174] | [Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image Super-Resolution.](http://arxiv.org/abs/2305.15357) | 本文提出了一种求解最优边界条件解决扩散ODE问题的有效采样方法，以稳定地从预训练的基于扩散的超分辨率模型中采样高质量的超分辨率图像。 |
| [^175] | [Proximal Policy Gradient Arborescence for Quality Diversity Reinforcement Learning.](http://arxiv.org/abs/2305.13795) | 本论文提出了一种将近端策略优化(PPO)方法与质量多样性(QD)相结合的新型QD-RL方法，用于在高吞吐量、大规模并行化机器人模拟器环境下训练能够在未知动态环境中表现出色的机器人学习智能体。 |
| [^176] | [Textually Pretrained Speech Language Models.](http://arxiv.org/abs/2305.13009) | 本论文提出了一种使用预训练的文本语言模型训练语音语言模型的方法，通过对模型设计选择和数据集规模的经验性分析，构建了参数数量和训练数据最多的语音语言模型，并引入了两个Spoken版本的文本基准，以进一步改善模型评估和推动未来研究。 |
| [^177] | [Incorporating Attribution Importance for Improving Faithfulness Metrics.](http://arxiv.org/abs/2305.10496) | 本研究提出了一种软删除标准来评估归因方法的忠实度，该方法随机遮盖标记的部分向量表示，这种方法比现有的硬删除标准更准确。 |
| [^178] | [FedPDD: A Privacy-preserving Double Distillation Framework for Cross-silo Federated Recommendation.](http://arxiv.org/abs/2305.06272) | 本文提出了一个名为FedPDD的隐私保护双重蒸馏框架，用于跨平台联邦推荐。该框架包括教师蒸馏和学生蒸馏两个阶段，在不传输模型信息的情况下，通过有效地转移知识和使用一种新的蒸馏损失函数来构建全局模型，实现了最先进的性能。 |
| [^179] | [Active Continual Learning: Labelling Queries in a Sequence of Tasks.](http://arxiv.org/abs/2305.03923) | 本文考虑了一系列主动学习任务的主动连续学习问题，研究了不同场景下多种主动和连续学习算法之间的有效性和相互作用，并提出了遗忘-学习曲线方法来平衡不忘旧知识和快速学习的两个目标。 |
| [^180] | [Deep Neural-network Prior for Orbit Recovery from Method of Moments.](http://arxiv.org/abs/2304.14604) | 该论文提出了一种基于深度神经网络先验的矩法轨道恢复方法，可用于解决多参照面对齐和单颗粒冷冻电镜建模等问题，具有抑制噪声的优势. |
| [^181] | [Exploring the flavor structure of quarks and leptons with reinforcement learning.](http://arxiv.org/abs/2304.14176) | 通过利用强化学习，探索了具有 $U(1)$ 味道对称性的模型的味道结构，找到了21个与实验测量值一致的模型，预测了无中微子双贝塔衰变的有效质量和可观的轻子 CP 破坏。 |
| [^182] | [Machine-learned Adversarial Attacks against Fault Prediction Systems in Smart Electrical Grids.](http://arxiv.org/abs/2303.18136) | 该论文提出了针对智能电网故障预测系统的机器学习对抗攻击的研究，证明智能电网中使用的深度神经网络方法容易受到对抗性攻击，并突出了目前在智能电网中的机器学习算法存在对各种对抗性攻击的弱点。 |
| [^183] | [Federated Stochastic Bandit Learning with Unobserved Context.](http://arxiv.org/abs/2303.17043) | 本文提出了一种联邦随机多臂上下文赌博算法以最大化累积奖励，针对未知上下文的情况通过执行特征向量转换解决问题。 |
| [^184] | [A Unified Single-stage Learning Model for Estimating Fiber Orientation Distribution Functions on Heterogeneous Multi-shell Diffusion-weighted MRI.](http://arxiv.org/abs/2303.16376) | 该论文提出了一种适用于异质多壳扩散加权MRI估计纤维定向分布函数的单阶段学习模型，使用深度学习技术可以提高推理速度和扫描一致性。 |
| [^185] | [Inverse Reinforcement Learning without Reinforcement Learning.](http://arxiv.org/abs/2303.14623) | 该论文提出了一种新的逆强化学习简化方法，通过利用专家的状态分布来减少强化学习子例程的全局探索部分，实现了指数级的加速，大大提高了样本复杂度和时间复杂度的效果。 |
| [^186] | [Data-dependent Generalization Bounds via Variable-Size Compressibility.](http://arxiv.org/abs/2303.05369) | 本文通过引入可变大小压缩性框架，建立了一种新的数据相关的泛化误差上界。该方法将算法的泛化误差与其输入数据的可变大小压缩率相关联，并提供了依赖于经验分布而非未知分布的界限。此外，该方法还可以推导出输入数据和输出假设随机变量的任何函数的泛化界限，并包含并可能优于现有的基于PAC-Bayes和数据相关内在维度的界限。 |
| [^187] | [Approximating the Shapley Value without Marginal Contributions.](http://arxiv.org/abs/2302.00736) | 本文提出了两种无参数、领域无关的Shapley值近似算法SVARM和Stratified SVARM，它们基于一种与边际贡献概念脱钩的Shapley值表示。我们证明了它们在近似质量方面具有无与伦比的理论保证，并通过实证结果将其与合成游戏和常见可解释性用例进行了比较。 |
| [^188] | [Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing.](http://arxiv.org/abs/2212.10789) | 本论文介绍了一种名为MoleculeSTM的多模态分子结构-文本模型，通过联合学习化学结构和文本描述，可以实现基于文本的检索和编辑。通过构建大型的多模态数据集，并设计挑战性的零样本任务进行验证，该模型展示了开放词汇和组合性的特性。 |
| [^189] | [Doubly robust nearest neighbors in factor models.](http://arxiv.org/abs/2211.14297) | 该论文介绍了一种在潜在因子模型中处理缺失数据的双重稳健最近邻方法，可以提供一致的估计，并在存在良好的行和列邻居时提供（近似）二次改进非渐近性能。 |
| [^190] | [Estimating counterfactual treatment outcomes over time in complex multi-agent scenarios.](http://arxiv.org/abs/2206.01900) | 本论文提出了一个可解释的反事实循环网络，用于在复杂的多智能体场景中估计干预效果。该模型考虑了时间变化的多智能体关系和协变量反事实预测的复杂结构，能够准确评估个体治疗效果，并提供解释性。 |
| [^191] | [TracInAD: Measuring Influence for Anomaly Detection.](http://arxiv.org/abs/2205.01362) | 本论文提出了一种新的方法TracInAD，用于根据影响力测量标记异常，并且在医疗和网络安全表格数据上实现了与最先进方法相当或更好的检测准确性。 |
| [^192] | [Optimal service resource management strategy for IoT-based health information system considering value co-creation of users.](http://arxiv.org/abs/2204.02521) | 本文研究了基于物联网的健康信息系统中考虑用户价值共创的最佳服务资源管理策略，通过嵌入深度强化学习算法，实现了服务资源的优化分配和用户参与行为的控制。 |

# 详细

[^1]: 一种针对非法歌曲翻唱的主动性双重防护机制：基于歌唱声音转换的能力

    A Proactive and Dual Prevention Mechanism against Illegal Song Covers empowered by Singing Voice Conversion. (arXiv:2401.17133v1 [cs.SD])

    [http://arxiv.org/abs/2401.17133](http://arxiv.org/abs/2401.17133)

    这项工作提出了一种主动性的双重防护机制，通过引入人类无法察觉的扰动，干扰歌唱声音转换的生成过程，防止未经授权的基于歌唱声音转换的非法歌曲翻唱。该机制既扰乱了歌手身份，又扰乱了歌词，使得歌唱声音既不模仿目标歌手，也不保留原始歌词。

    

    歌唱声音转换(SVC)通过将一个歌手的歌唱声音转换成另一个目标歌手的歌唱声音，并使用原始歌词和旋律，自动化了歌曲翻唱。然而，这引发了对版权和公民权利的严重担忧。本研究提出了 SongBsAb，这是第一个主动性方法，用于减轻未经授权的基于 SVC 的非法歌曲翻唱。SongBsAb 在发布歌唱声音之前引入了人类无法察觉的扰动，这样当它们被使用时，SVC 的生成过程将被干扰，导致意外的歌唱声音。 SongBsAb 具有双重预防效果，引起歌手身份和歌词的混乱，即 SVC 覆盖的歌唱声音既不模仿目标歌手，也不保留原始歌词。为了提高扰动的不可察觉性，我们使用了一个以伴奏曲作为额外掩蔽者的基于心理声学模型的损失模型。

    Singing voice conversion (SVC) automates song covers by converting one singer's singing voice into another target singer's singing voice with the original lyrics and melody. However, it raises serious concerns about copyright and civil right infringements to multiple entities. This work proposes SongBsAb, the first proactive approach to mitigate unauthorized SVC-based illegal song covers. SongBsAb introduces human-imperceptible perturbations to singing voices before releasing them, so that when they are used, the generation process of SVC will be interfered, resulting in unexpected singing voices. SongBsAb features a dual prevention effect by causing both (singer) identity disruption and lyric disruption, namely, the SVC-covered singing voice neither imitates the target singer nor preserves the original lyrics. To improve the imperceptibility of perturbations, we refine a psychoacoustic model-based loss with the backing track as an additional masker, a unique accompanying element for s
    
[^2]: 个性化差分隐私在Ridge回归中的应用

    Personalized Differential Privacy for Ridge Regression. (arXiv:2401.17127v1 [cs.LG])

    [http://arxiv.org/abs/2401.17127](http://arxiv.org/abs/2401.17127)

    该论文提出了一种个性化差分隐私输出扰动方法（PDP-OP），可以在Ridge回归中训练具有每个数据点个性化隐私水平的模型，并提供了相应的隐私证明和准确度保证。

    

    在敏感领域中机器学习的应用增加，需要通过差分隐私等隐私框架保护训练数据。差分隐私要求指定一个统一的隐私水平ε，以表示每个数据点在整个数据集中愿意容忍的最大隐私损失。然而，在实践中，不同的数据点通常有不同的隐私需求。只设置一个统一的隐私水平通常过于限制，不允许学习器以大量的精度开销来保证严格的隐私要求。为了克服这个限制，我们引入了我们的个性化差分隐私输出扰动方法（PDP-OP），它可以训练具有每个数据点个性化隐私水平的Ridge回归模型。我们提供了对我们的PDP-OP的严格隐私证明以及生成模型的准确度保证。这项工作是首次在个性化差分隐私方面提供了这样的理论准确度保证。

    The increased application of machine learning (ML) in sensitive domains requires protecting the training data through privacy frameworks, such as differential privacy (DP). DP requires to specify a uniform privacy level $\varepsilon$ that expresses the maximum privacy loss that each data point in the entire dataset is willing to tolerate. Yet, in practice, different data points often have different privacy requirements. Having to set one uniform privacy level is usually too restrictive, often forcing a learner to guarantee the stringent privacy requirement, at a large cost to accuracy. To overcome this limitation, we introduce our novel Personalized-DP Output Perturbation method (PDP-OP) that enables to train Ridge regression models with individual per data point privacy levels. We provide rigorous privacy proofs for our PDP-OP as well as accuracy guarantees for the resulting model. This work is the first to provide such theoretical accuracy guarantees when it comes to personalized DP 
    
[^3]: 个性化联邦学习中的光谱共蒸馏

    Spectral Co-Distillation for Personalized Federated Learning. (arXiv:2401.17124v1 [cs.LG])

    [http://arxiv.org/abs/2401.17124](http://arxiv.org/abs/2401.17124)

    本论文提出了一种基于模型谱信息的光谱共蒸馏方法，用于个性化联邦学习。通过建立一个共蒸馏框架，建立了通用和个性化模型训练之间的双向桥梁。

    

    个性化联邦学习（PFL）被广泛研究以解决数据异构性的挑战，尤其是当单个通用模型无法满足本地客户端的不同性能要求时。现有的PFL方法本质上基于通用全局模型和个性化本地模型之间的关系由模型权重的相似性捕获的思想。这种相似性主要基于将模型架构划分为通用与个性化组件，或通过模型权重建模客户关系。为了更好地捕获相似（但不同的）通用与个性化模型表示，我们提出了一种基于模型谱信息的新型蒸馏方法：光谱共蒸馏。在光谱共蒸馏的基础上，我们还引入了一个共蒸馏框架，建立了通用和个性化模型训练之间的双向桥梁。

    Personalized federated learning (PFL) has been widely investigated to address the challenge of data heterogeneity, especially when a single generic model is inadequate in satisfying the diverse performance requirements of local clients simultaneously. Existing PFL methods are inherently based on the idea that the relations between the generic global and personalized local models are captured by the similarity of model weights. Such a similarity is primarily based on either partitioning the model architecture into generic versus personalized components, or modeling client relationships via model weights. To better capture similar (yet distinct) generic versus personalized model representations, we propose \textit{spectral distillation}, a novel distillation method based on model spectrum information. Building upon spectral distillation, we also introduce a co-distillation framework that establishes a two-way bridge between generic and personalized model training. Moreover, to utilize th
    
[^4]: 无监督发现当图深度生成模型出现交织时的可操纵因素

    Unsupervised Discovery of Steerable Factors When Graph Deep Generative Models Are Entangled. (arXiv:2401.17123v1 [cs.LG])

    [http://arxiv.org/abs/2401.17123](http://arxiv.org/abs/2401.17123)

    这项工作提出了GraphCG方法，用于在预训练图深度生成模型的潜在空间中无监督发现可操纵因素，通过最大化语义丰富方向之间的互信息来学习这些可操纵因素。实验证明GraphCG优于其他竞争方法。

    

    深度生成模型(DGMs)广泛用于图数据。然而，对于这种预训练图DGMs的潜在空间的理解研究相对较少。这些理解有潜力为重要任务提供有益的指导，例如图的可控制生成。因此，在这项工作中，我们对这个问题很感兴趣，并提出GraphCG，一种用于在预训练图DGMs的潜在空间中无监督发现可操纵因素的方法。我们首先使用六个解缠度度量标准检验了三个预训练图DGMs的表示空间，并观察到预训练表示空间是交织的。受这个观察的启发，GraphCG通过最大化语义丰富方向之间的互信息来学习可操纵因素，沿着相同方向移动的图将共享相同的可操纵因素。我们定量验证了GraphCG优于其他四个竞争方法。

    Deep generative models (DGMs) have been widely developed for graph data. However, much less investigation has been carried out on understanding the latent space of such pretrained graph DGMs. These understandings possess the potential to provide constructive guidelines for crucial tasks, such as graph controllable generation. Thus in this work, we are interested in studying this problem and propose GraphCG, a method for the unsupervised discovery of steerable factors in the latent space of pretrained graph DGMs. We first examine the representation space of three pretrained graph DGMs with six disentanglement metrics, and we observe that the pretrained representation space is entangled. Motivated by this observation, GraphCG learns the steerable factors via maximizing the mutual information between semantic-rich directions, where the controlled graph moving along the same direction will share the same steerable factors. We quantitatively verify that GraphCG outperforms four competitive 
    
[^5]: 通过专家混合实现可解释的数据驱动建模：实现灰盒和黑盒模型的有效融合

    Explainable data-driven modeling via mixture of experts: towards effective blending of grey and black-box models. (arXiv:2401.17118v1 [cs.LG])

    [http://arxiv.org/abs/2401.17118](http://arxiv.org/abs/2401.17118)

    这个论文提出了一种基于“专家混合”原理的综合框架，通过将灰盒和黑盒模型进行数据驱动的融合，实现了对复杂系统的准确建模，并提高了可解释性。

    

    随着系统的复杂性增加，传统的基于第一原理的模型常常在准确性上遇到困难。与此相反，机器学习方法虽然功能强大，但在可解释性和处理物理约束方面面临挑战。将这些模型结合起来的努力往往很难在准确性和复杂性之间找到平衡。为了解决这些问题，我们提出了一个基于“专家混合”原理的综合框架。这种方法可以对多样化的局部模型进行基于数据的融合，利用基于第一原理的先验知识的全部潜力。我们的解决方案允许专家的独立训练，借鉴了机器学习和系统识别的技术，并支持协作和竞争学习范式。为了增强可解释性，我们对专家的组合中的突变进行了惩罚。实验结果验证了我们的方法在生成可解释的组合方面的有效性。

    Traditional models grounded in first principles often struggle with accuracy as the system's complexity increases. Conversely, machine learning approaches, while powerful, face challenges in interpretability and in handling physical constraints. Efforts to combine these models often often stumble upon difficulties in finding a balance between accuracy and complexity. To address these issues, we propose a comprehensive framework based on a "mixture of experts" rationale. This approach enables the data-based fusion of diverse local models, leveraging the full potential of first-principle-based priors. Our solution allows independent training of experts, drawing on techniques from both machine learning and system identification, and it supports both collaborative and competitive learning paradigms. To enhance interpretability, we penalize abrupt variations in the expert's combination. Experimental results validate the effectiveness of our approach in producing an interpretable combination
    
[^6]: 通过Yang-Baxter方程和人工神经网络介导的量子错误缓解和修正

    Quantum error mitigation and correction mediated by Yang-Baxter equation and artificial neural network. (arXiv:2401.17116v1 [quant-ph])

    [http://arxiv.org/abs/2401.17116](http://arxiv.org/abs/2401.17116)

    该论文介绍了一种新策略，通过人工神经网络和Yang-Baxter方程来缓解和修正量子计算中的错误。研究表明，通过控制噪声，我们可以使用经典计算进行错误缓解，并且通过训练神经网络模型可以有效地纠正时间演化的量子态中的错误。

    

    量子计算显示出巨大的潜力，但错误是一个重大挑战。本研究探索了使用人工神经网络（ANN）和Yang-Baxter方程（YBE）来缓解量子错误的新策略。与传统的错误修正方法不同，这些方法计算量很大，我们研究了人工错误缓解。本文介绍了量子错误来源的基础知识，并探讨了使用经典计算来进行错误缓解的潜力。Yang-Baxter方程起着关键作用，使我们能够将时间动力学模拟压缩到恒定深度的电路中。通过引入通过YBE控制的噪声，我们增强了用于错误缓解的数据集。我们使用部分量子模拟数据训练了一个ANN模型，证明了其在纠正时间演化的量子态中的错误方面的有效性。

    Quantum computing shows great potential, but errors pose a significant challenge. This study explores new strategies for mitigating quantum errors using artificial neural networks (ANN) and the Yang-Baxter equation (YBE). Unlike traditional error correction methods, which are computationally intensive, we investigate artificial error mitigation. The manuscript introduces the basics of quantum error sources and explores the potential of using classical computation for error mitigation. The Yang-Baxter equation plays a crucial role, allowing us to compress time dynamics simulations into constant-depth circuits. By introducing controlled noise through the YBE, we enhance the dataset for error mitigation. We train an ANN model on partial data from quantum simulations, demonstrating its effectiveness in correcting errors in time-evolving quantum states.
    
[^7]: 神经风格迁移评估：一项综述

    Evaluation in Neural Style Transfer: A Review. (arXiv:2401.17109v1 [cs.CV])

    [http://arxiv.org/abs/2401.17109](http://arxiv.org/abs/2401.17109)

    神经风格迁移领域对于评估方法的选择缺乏共识，本综述通过对现有评估技术的分析，提出了标准化评估实践的建议。

    

    过去几年，神经风格迁移领域取得了显著的进展，方法能够合成出艺术性和逼真度极高的图像和视频。为了评估这些结果，使用了各种各样的评估方法和指标，包括基于比较的作者观点、量化参与者主观判断的人类评估研究，以及量化计算指标，客观评估算法性能的不同方面。然而，关于最适合和最有效的评估程序以确保结果的可靠性并没有一致的共识。在这篇综述中，我们对现有的评估技术进行了深入分析，发现了当前评估方法的不一致性和局限性，并给出了标准化评估实践的建议。我们认为，建立一个强大的评估框架是必要的。

    The field of Neural Style Transfer (NST) has witnessed remarkable progress in the past few years, with approaches being able to synthesize artistic and photorealistic images and videos of exceptional quality. To evaluate such results, a diverse landscape of evaluation methods and metrics is used, including authors' opinions based on side-by-side comparisons, human evaluation studies that quantify the subjective judgements of participants, and a multitude of quantitative computational metrics which objectively assess the different aspects of an algorithm's performance. However, there is no consensus regarding the most suitable and effective evaluation procedure that can guarantee the reliability of the results. In this review, we provide an in-depth analysis of existing evaluation techniques, identify the inconsistencies and limitations of current evaluation methods, and give recommendations for standardized evaluation practices. We believe that the development of a robust evaluation fr
    
[^8]: CharNet：高复杂性字符分类的广义方法

    CharNet: Generalized Approach for High-Complexity Character Classification. (arXiv:2401.17098v1 [cs.CV])

    [http://arxiv.org/abs/2401.17098](http://arxiv.org/abs/2401.17098)

    这是一篇关于手写字符识别问题的论文，提出了一个广义的方法来解决高复杂性字符分类的挑战。

    

    对于机器学习研究人员来说，手写字符识别（HCR）是一个具有挑战性的问题。与打印文本数据不同，手写字符数据集由于人类导致的偏差而具有更多的变化。基于独特字符类的数据，例如象形文字或汉字韩字字符序列，给HCR问题带来新的复杂性。在这种数据集上的分类任务要求模型学习具有相似特征的图像的高复杂性细节。随着计算资源的可用性的最新进展以及进一步的计算机视觉理论的发展，一些研究团队已经有效地解决了出现的挑战。尽管以高效率而闻名，但许多常见的方法仍然不具有通用性，并使用数据集特定的解决方案来取得更好的结果。由于复杂的结构和高计算需求，现有方法经常阻止解决方案流行起来。本文提出了一种方法。

    Handwritten character recognition (HCR) is a challenging problem for machine learning researchers. Unlike printed text data, handwritten character datasets have more variation due to human-introduced bias. With numerous unique character classes present, some data, such as Logographic Scripts or Sino-Korean character sequences, bring new complications to the HCR problem. The classification task on such datasets requires the model to learn high-complexity details of the images that share similar features. With recent advances in computational resource availability and further computer vision theory development, some research teams have effectively addressed the arising challenges. Although known for achieving high efficiency, many common approaches are still not generalizable and use dataset-specific solutions to achieve better results. Due to complex structure and high computing demands, existing methods frequently prevent the solutions from gaining popularity. This paper proposes a str
    
[^9]: 在未观测到的网络位置使用数据驱动的宏观模型进行交通估计

    Traffic estimation in unobserved network locations using data-driven macroscopic models. (arXiv:2401.17095v1 [cs.LG])

    [http://arxiv.org/abs/2401.17095](http://arxiv.org/abs/2401.17095)

    本文提出了一种利用宏观模型和多源时空数据的方法来估计无法观测到的网络位置的交通流量和行驶时间。该方法可以在传感器覆盖范围有限的情况下进行准确的估计，并满足基本的流量守恒约束条件。

    

    本文利用宏观模型和自动交通计数器和探测车辆收集的多源时空数据，准确估计无法获得这些测量数据的路段的交通流量和行驶时间。这个问题在交通规划应用中是关键的，因为传感器覆盖范围有限，规划的干预措施会对整个网络产生影响。提出的模型命名为宏观交通估计器（MaTE），可以仅使用这些数量的观测测量来进行网络范围的交通流量和行驶时间估计。由于MaTE基于宏观流量理论，所有参数和变量都是可以解释的。估计的交通流量满足基本的流量守恒约束条件，并且与估计的行驶时间呈递增的单调关系。将基于logit的随机交通分配作为流量行为路由的原则使得模型在可微方面具有完全可区分性。

    This paper leverages macroscopic models and multi-source spatiotemporal data collected from automatic traffic counters and probe vehicles to accurately estimate traffic flow and travel time in links where these measurements are unavailable. This problem is critical in transportation planning applications where the sensor coverage is low and the planned interventions have network-wide impacts. The proposed model, named the Macroscopic Traffic Estimator (MaTE), can perform network-wide estimations of traffic flow and travel time only using the set of observed measurements of these quantities. Because MaTE is grounded in macroscopic flow theory, all parameters and variables are interpretable. The estimated traffic flow satisfies fundamental flow conservation constraints and exhibits an increasing monotonic relationship with the estimated travel time. Using logit-based stochastic traffic assignment as the principle for routing flow behavior makes the model fully differentiable with respect
    
[^10]: 动态生存分析与控制潜在状态

    Dynamical Survival Analysis with Controlled Latent States. (arXiv:2401.17077v1 [stat.ML])

    [http://arxiv.org/abs/2401.17077](http://arxiv.org/abs/2401.17077)

    本论文提出了一种动态生存分析方法，通过控制潜在状态来学习个体特定的计数过程强度。研究者设计了一个神经控制微分方程模型，并证明了在足够正则条件下，可以在签名空间中线性化模型，得到一种基于签名的估计器。通过对金融、预测性维护和食品供应链管理等数据集的实验，验证了模型的性能。

    

    我们考虑从一组静态变量和不规则采样的时间序列中学习个体特定的计数过程强度的任务。我们引入一种新颖的建模方法，其中强度是控制微分方程的解。首先，我们通过构建神经控制微分方程来设计一个神经估计器。然后，我们证明在足够正则条件下，我们的模型可以在签名空间中线性化，得到一种基于签名的估计器，我们称之为CoxSig。我们为这两种估计器提供理论学习保证，并展示了我们的模型在金融、预测性维护和食品供应链管理等各种模拟和真实数据集上的性能。

    We consider the task of learning individual-specific intensities of counting processes from a set of static variables and irregularly sampled time series. We introduce a novel modelization approach in which the intensity is the solution to a controlled differential equation. We first design a neural estimator by building on neural controlled differential equations. In a second time, we show that our model can be linearized in the signature space under sufficient regularity conditions, yielding a signature-based estimator which we call CoxSig. We provide theoretical learning guarantees for both estimators, before showcasing the performance of our models on a vast array of simulated and real-world datasets from finance, predictive maintenance and food supply chain management.
    
[^11]: 一种独立的系统化黑盒测试ML系统的概要

    Outline of an Independent Systematic Blackbox Test for ML-based Systems. (arXiv:2401.17062v1 [cs.LG])

    [http://arxiv.org/abs/2401.17062](http://arxiv.org/abs/2401.17062)

    本文提出了一种独立测试ML模型和基于ML的系统的方法，可以独立验证其黑盒特性和随机属性。建议扩展现有测试方法以更好地反映ML系统的特点。

    

    本文提出了一种测试程序，可用于独立于实际训练过程测试ML模型和基于ML的系统。通过考虑其黑盒特性和ML模型及其训练数据的固有随机属性，可以独立验证这些模型和系统的典型质量参数，如准确性和精度。文章介绍了一系列测试实验的初步结果，并提出了扩展现有测试方法以反映ML模型和基于ML的系统的随机特性的建议。

    This article proposes a test procedure that can be used to test ML models and ML-based systems independently of the actual training process. In this way, the typical quality statements such as accuracy and precision of these models and system can be verified independently, taking into account their black box character and the immanent stochastic properties of ML models and their training data. The article presents first results from a set of test experiments and suggest extensions to existing test methods reflecting the stochastic nature of ML models and ML-based systems.
    
[^12]: 使参数异常检测再次变得非参数化的表格数据

    Making Parametric Anomaly Detection on Tabular Data Non-Parametric Again. (arXiv:2401.17052v1 [cs.LG])

    [http://arxiv.org/abs/2401.17052](http://arxiv.org/abs/2401.17052)

    本论文研究了在表格数据上使用检索增强模型进行异常检测的方法，通过重建被屏蔽特征，结合KNN和注意力机制选择相关样本来帮助目标样本的重建过程。实验证明通过使用非参数化关系进行检索增强的异常检测方法能取得良好的效果。

    

    近年来，深度学习在表格数据上的应用引起了越来越多的关注，但是在处理结构化数据时，使用深度模型仍然具有挑战性。虽然这些模型在处理非结构化数据时表现出色，但在处理结构化数据时效果有限。最近的研究引入了检索增强模型来填补这一差距，在监督任务（如分类和回归）中展示了有希望的结果。在这项工作中，我们研究了在表格数据上使用检索增强模型进行异常检测。我们提出了一种基于重建的方法，其中一个Transformer模型学习重建“正常”样本的被屏蔽特征。我们测试了基于KNN和注意力机制的模块的有效性，以选择相关样本来帮助目标样本的重建过程。我们在一个包含31个表格数据集的基准测试上进行了实验，结果显示通过使用非参数化关系进行检索增强的异常检测（AD）方法能取得良好的效果。

    Deep learning for tabular data has garnered increasing attention in recent years, yet employing deep models for structured data remains challenging. While these models excel with unstructured data, their efficacy with structured data has been limited. Recent research has introduced retrieval-augmented models to address this gap, demonstrating promising results in supervised tasks such as classification and regression. In this work, we investigate using retrieval-augmented models for anomaly detection on tabular data. We propose a reconstruction-based approach in which a transformer model learns to reconstruct masked features of \textit{normal} samples. We test the effectiveness of KNN-based and attention-based modules to select relevant samples to help in the reconstruction process of the target sample. Our experiments on a benchmark of 31 tabular datasets reveal that augmenting this reconstruction-based anomaly detection (AD) method with non-parametric relationships via retrieval modu
    
[^13]: 使用贝叶斯深度学习预测VIX指数

    Forecasting VIX using Bayesian Deep Learning. (arXiv:2401.17042v1 [cs.LG])

    [http://arxiv.org/abs/2401.17042](http://arxiv.org/abs/2401.17042)

    本文利用贝叶斯深度学习方法预测波动性指数VIX，并采用概率对应模型和标准差缩放方法提高预测准确性和不确定性估计。具有高斯先验的MNF模型表现最优，使得TCN和WaveNet网络可以良好推断VIX值。

    

    最近，深度学习技术逐渐代替传统统计和机器学习模型成为价格预测任务的首选。本文利用概率深度学习推断波动性指数VIX。我们使用WaveNet、Temporal Convolutional Network (TCN)和Transformers的概率对应模型。我们发现，TCN在RMSE约为0.189时胜过其他模型。此外，现代神经网络提供的不准确的不确定性估计问题已被广泛知晓。为解决这个问题，我们使用标准差缩放来校准网络。此外，我们发现，具有高斯先验的MNF模型在精度和不确定性预测方面优于重新参数化技巧和Flipout模型。最后，我们声称具有柯西和对数均匀先验分布的MNF可以最好地推断VIX值，并且能够使TCN和WaveNet网络进行良好的校准。

    Recently, deep learning techniques are gradually replacing traditional statistical and machine learning models as the first choice for price forecasting tasks. In this paper, we leverage probabilistic deep learning for inferring the volatility index VIX. We employ the probabilistic counterpart of WaveNet, Temporal Convolutional Network (TCN), and Transformers. We show that TCN outperforms all models with an RMSE around 0.189. In addition, it has been well known that modern neural networks provide inaccurate uncertainty estimates. For solving this problem, we use the standard deviation scaling to calibrate the networks. Furthermore, we found out that MNF with Gaussian prior outperforms Reparameterization Trick and Flipout models in terms of precision and uncertainty predictions. Finally, we claim that MNF with Cauchy and LogUniform prior distributions yield well calibrated TCN and WaveNet networks being the former that best infer the VIX values.
    
[^14]: 基于无噪声观测的贝叶斯优化：通过随机探索改善遗憾边界

    Bayesian Optimization with Noise-Free Observations: Improved Regret Bounds via Random Exploration. (arXiv:2401.17037v1 [cs.LG])

    [http://arxiv.org/abs/2401.17037](http://arxiv.org/abs/2401.17037)

    该论文研究了基于无噪声观测的贝叶斯优化问题，提出了一种基于散乱数据逼近的新算法，并引入随机探索步骤以实现接近最优填充距离的速率衰减。该算法在实现的易用性和累积遗憾边界的性能上超过了传统的GP-UCB算法，并在多个示例中优于其他贝叶斯优化策略。

    

    本文研究了基于无噪声观测的贝叶斯优化。我们引入了新的基于散乱数据逼近的算法，并通过随机探索步骤确保查询点的填充距离以接近最优的速率衰减。我们的算法保留了经典的GP-UCB算法的易实现性，并满足了几乎与arXiv:2002.05096中的猜想相匹配的累积遗憾边界，从而解决了COLT的一个开放问题。此外，新算法在几个示例中优于GP-UCB和其他流行的贝叶斯优化策略。

    This paper studies Bayesian optimization with noise-free observations. We introduce new algorithms rooted in scattered data approximation that rely on a random exploration step to ensure that the fill-distance of query points decays at a near-optimal rate. Our algorithms retain the ease of implementation of the classical GP-UCB algorithm and satisfy cumulative regret bounds that nearly match those conjectured in arXiv:2002.05096, hence solving a COLT open problem. Furthermore, the new algorithms outperform GP-UCB and other popular Bayesian optimization strategies in several examples.
    
[^15]: 二元分类性能中的内在数据限制和上界

    Intrinsic Data Constraints and Upper Bounds in Binary Classification Performance. (arXiv:2401.17036v1 [cs.LG])

    [http://arxiv.org/abs/2401.17036](http://arxiv.org/abs/2401.17036)

    我们研究了二元分类性能的内在数据限制和上界，提供了一个理论框架并进行了理论推理和实证检验，发现理论上限是可以被达到的，并计算出了三个常用评估指标的精确上限。

    

    数据组织的结构被广泛认为对机器学习算法的有效性有重要影响，尤其是在二元分类任务中。我们的研究提供了一个理论框架，认为给定数据集上二元分类器的最大潜力主要受到数据的内在特性的限制。通过理论推理和实证检验，我们使用了标准目标函数、评估指标和二元分类器，得出了两个主要结论。首先，我们证明了在实际数据集上二元分类性能的理论上限是可以被理论上达到的。这个上限代表了学习损失和评估指标之间的可计算平衡。其次，我们计算了三个常用评估指标的精确上限，揭示了与我们总体论点的根本一致性：上界与内在数据限制密切相关。

    The structure of data organization is widely recognized as having a substantial influence on the efficacy of machine learning algorithms, particularly in binary classification tasks. Our research provides a theoretical framework suggesting that the maximum potential of binary classifiers on a given dataset is primarily constrained by the inherent qualities of the data. Through both theoretical reasoning and empirical examination, we employed standard objective functions, evaluative metrics, and binary classifiers to arrive at two principal conclusions. Firstly, we show that the theoretical upper bound of binary classification performance on actual datasets can be theoretically attained. This upper boundary represents a calculable equilibrium between the learning loss and the metric of evaluation. Secondly, we have computed the precise upper bounds for three commonly used evaluation metrics, uncovering a fundamental uniformity with our overarching thesis: the upper bound is intricately 
    
[^16]: 坚固的核稀疏子空间聚类

    Robust Kernel Sparse Subspace Clustering. (arXiv:2401.17035v1 [cs.LG])

    [http://arxiv.org/abs/2401.17035](http://arxiv.org/abs/2401.17035)

    这篇论文提出了一种用于处理具有粗疏损坏数据的坚固核稀疏子空间聚类算法，并验证了其有效性。

    

    核方法被应用于模式识别中的许多问题，包括子空间聚类（SC）。通过这种方式，输入数据空间中的非线性问题在映射到高维特征空间后变成了线性问题。因此，通过核技巧的隐式映射使得计算上可行的非线性算法得以实现。然而，只有在相关的优化问题中使用误差项的Froebenious范数的平方时，线性算法的核化才是可能的。然而，这意味着误差项的正态分布。这对于如粗疏损坏这样的非高斯误差是不合适的，而这些误差是由l1范数建模的。据我们所知，在这里，我们首次提出了用于具有粗疏损坏的数据的坚固核稀疏聚类（RKSSC）算法。该概念原则上可以应用于其他SC算法，以实现对此类损坏的鲁棒性。我们在两个知名的数据集上验证了所提出的方法。

    Kernel methods are applied to many problems in pattern recognition, including subspace clustering (SC). That way, nonlinear problems in the input data space become linear in mapped high-dimensional feature space. Thereby, computationally tractable nonlinear algorithms are enabled through implicit mapping by the virtue of kernel trick. However, kernelization of linear algorithms is possible only if square of the Froebenious norm of the error term is used in related optimization problem. That, however, implies normal distribution of the error. That is not appropriate for non-Gaussian errors such as gross sparse corruptions that are modeled by -norm. Herein, to the best of our knowledge, we propose for the first time robust kernel sparse SC (RKSSC) algorithm for data with gross sparse corruptions. The concept, in principle, can be applied to other SC algorithms to achieve robustness to the presence of such type of corruption. We validated proposed approach on two well-known datasets with 
    
[^17]: M2CURL: 通过自监督表示学习实现样本高效的多模态强化学习，用于机器人操纵

    M2CURL: Sample-Efficient Multimodal Reinforcement Learning via Self-Supervised Representation Learning for Robotic Manipulation. (arXiv:2401.17032v1 [cs.RO])

    [http://arxiv.org/abs/2401.17032](http://arxiv.org/abs/2401.17032)

    M2CURL是一种样本高效的多模态强化学习方法，通过自监督表示学习从视触觉数据中学习出高效的表示，并加速强化学习算法的收敛。

    

    多模态强化学习中最重要的方面之一是有效地整合不同的观测模态。从这些模态中得到稳健准确的表示对于提升强化学习算法的鲁棒性和样本效率至关重要。然而，在视触觉数据的强化学习环境中学习表示面临着重要挑战，特别是由于数据的高维度和将视触觉输入与动态环境和任务目标进行相关性分析的复杂性。为了解决这些挑战，我们提出了多模态对比无监督强化学习（M2CURL）。我们的方法采用了一种新颖的多模态自监督学习技术，学习出高效的表示并加速了强化学习算法的收敛。我们的方法与强化学习算法无关，因此可以与任何可用的强化学习算法进行整合。我们在Tactile Gym 2模拟器上评估了M2CURL。

    One of the most critical aspects of multimodal Reinforcement Learning (RL) is the effective integration of different observation modalities. Having robust and accurate representations derived from these modalities is key to enhancing the robustness and sample efficiency of RL algorithms. However, learning representations in RL settings for visuotactile data poses significant challenges, particularly due to the high dimensionality of the data and the complexity involved in correlating visual and tactile inputs with the dynamic environment and task objectives. To address these challenges, we propose Multimodal Contrastive Unsupervised Reinforcement Learning (M2CURL). Our approach employs a novel multimodal self-supervised learning technique that learns efficient representations and contributes to faster convergence of RL algorithms. Our method is agnostic to the RL algorithm, thus enabling its integration with any available RL algorithm. We evaluate M2CURL on the Tactile Gym 2 simulator 
    
[^18]: LADDER: 深度学习方法重新探索宇宙距离梯度并探索其应用

    LADDER: Revisiting the Cosmic Distance Ladder with Deep Learning Approaches and Exploring its Applications. (arXiv:2401.17029v1 [astro-ph.CO])

    [http://arxiv.org/abs/2401.17029](http://arxiv.org/abs/2401.17029)

    LADDER是一个新颖的深度学习框架，通过学习宇宙的“距离梯度”，实现了预测宇宙距离并探索了多个宇宙学应用。这项研究表明在机器学习应用中需要进行有趣但谨慎的考虑。

    

    我们通过一种名为LADDER（深度学习算法用于距离估计和重建）的新颖深度学习框架，研究了使用“宇宙距离梯度”重建宇宙的前景。LADDER使用了来自Pantheon Type Ia超新星编译的视星等数据，并将数据点之间的全部协方差信息进行了融合，以生成具有相应误差的预测结果。通过对多个深度学习模型进行了多个验证实验后，我们选择了表现最佳的LADDER模型。然后，我们演示了我们的方法在宇宙学上的应用，包括作为独立于模型的一致性检查工具，用于其他数据集（如重子声学振荡）的校准，用于高红移数据集（如伽玛射线暴）的校准，以及用作未来探测的独立于模型的模拟目录生成器等等。我们的分析给出了关于在机器学习应用中要进行有趣而谨慎的考虑的支持。

    We investigate the prospect of reconstructing the ``cosmic distance ladder'' of the Universe using a novel deep learning framework called LADDER - Learning Algorithm for Deep Distance Estimation and Reconstruction. LADDER is trained on the apparent magnitude data from the Pantheon Type Ia supernovae compilation, incorporating the full covariance information among data points, to produce predictions along with corresponding errors. After employing several validation tests with a number of deep learning models, we pick LADDER as the best performing one. We then demonstrate applications of our method in the cosmological context, that include serving as a model-independent tool for consistency checks for other datasets like baryon acoustic oscillations, calibration of high-redshift datasets such as gamma ray bursts, use as a model-independent mock catalog generator for future probes, etc. Our analysis advocates for interesting yet cautious consideration of machine learning applications in 
    
[^19]: 用于个性化治疗的亚群识别的异质治疗效果估计（opioid use disorder中的个人医学）。

    Heterogeneous treatment effect estimation with subpopulation identification for personalized medicine in opioid use disorder. (arXiv:2401.17027v1 [cs.LG])

    [http://arxiv.org/abs/2401.17027](http://arxiv.org/abs/2401.17027)

    本论文介绍了一种名为SubgroupTE的神经网络框架，它结合了亚群识别和治疗效果估计，通过考虑治疗反应的异质性，提高了个性化治疗建议的准确性。

    

    深度学习模型在治疗效果估计方面已经显示出有希望的结果。然而，大多数模型忽视了不同特征亚群之间的治疗结果变化。这个限制阻碍了它们提供准确估计和特定亚群的治疗建议的能力。在这项研究中，我们引入了一种新颖的基于神经网络的框架，名为SubgroupTE，它结合了亚群识别和治疗效果估计。SubgroupTE对各个亚群进行识别并同时估计每个亚群的治疗效果，通过考虑治疗反应的异质性改进了治疗效果估计。对合成数据的比较实验表明，SubgroupTE在治疗效果估计方面优于现有模型。此外，对与阿片类药物使用障碍（OUD）相关的真实数据集的实验证明了我们方法提高个性化治疗建议的潜力。

    Deep learning models have demonstrated promising results in estimating treatment effects (TEE). However, most of them overlook the variations in treatment outcomes among subgroups with distinct characteristics. This limitation hinders their ability to provide accurate estimations and treatment recommendations for specific subgroups. In this study, we introduce a novel neural network-based framework, named SubgroupTE, which incorporates subgroup identification and treatment effect estimation. SubgroupTE identifies diverse subgroups and simultaneously estimates treatment effects for each subgroup, improving the treatment effect estimation by considering the heterogeneity of treatment responses. Comparative experiments on synthetic data show that SubgroupTE outperforms existing models in treatment effect estimation. Furthermore, experiments on a real-world dataset related to opioid use disorder (OUD) demonstrate the potential of our approach to enhance personalized treatment recommendatio
    
[^20]: 在自动驾驶数据集上评估对于模型的越界检测性能

    Evaluation of Out-of-Distribution Detection Performance on Autonomous Driving Datasets. (arXiv:2401.17013v1 [cs.LG])

    [http://arxiv.org/abs/2401.17013](http://arxiv.org/abs/2401.17013)

    本研究评估了在自动驾驶数据集上的越界检测性能，发现通过使用Mahalanobis距离来拒绝输出可以显著降低分类风险，即使应用在未见过的数据集上。

    

    需要系统地研究安全措施，以评估深度神经网络（DNN）在关键应用中的预期性能程度。由于缺乏用于高维DNN的验证方法，需要在接受的性能和处理越界样本之间进行权衡。本文通过应用基于最可能的类条件高斯分布的Mahalanobis距离（MD）作为越界评分，评估拒绝语义分割DNN的输出。评估过程涉及三个在Cityscapes数据集上训练并在四个汽车数据集上测试的DNN，并发现即使应用在未见过的数据集上，通过减少像素覆盖率可以大幅降低分类风险。我们的研究结果的适用性将支持合法化安全措施并推动其在汽车感知中的安全使用。

    Safety measures need to be systemically investigated to what extent they evaluate the intended performance of Deep Neural Networks (DNNs) for critical applications. Due to a lack of verification methods for high-dimensional DNNs, a trade-off is needed between accepted performance and handling of out-of-distribution (OOD) samples.  This work evaluates rejecting outputs from semantic segmentation DNNs by applying a Mahalanobis distance (MD) based on the most probable class-conditional Gaussian distribution for the predicted class as an OOD score. The evaluation follows three DNNs trained on the Cityscapes dataset and tested on four automotive datasets and finds that classification risk can drastically be reduced at the cost of pixel coverage, even when applied on unseen datasets. The applicability of our findings will support legitimizing safety measures and motivate their usage when arguing for safe usage of DNNs in automotive perception.
    
[^21]: 优化大规模语言模型用于漏洞检测

    Finetuning Large Language Models for Vulnerability Detection. (arXiv:2401.17010v1 [cs.CR])

    [http://arxiv.org/abs/2401.17010](http://arxiv.org/abs/2401.17010)

    本文优化了大规模语言模型用于源代码中的漏洞检测任务，通过微调最先进的代码语言模型WizardCoder并改进其训练过程和策略，实现了对漏洞数据集的分类性能的提升。

    

    本文介绍了对大规模语言模型进行微调，并将其用于源代码中的漏洞检测的结果。我们利用最先进的语言模型StarCoder的改进版本WizardCoder，并通过进一步微调将其适应于漏洞检测任务。为了加速训练，我们修改了WizardCoder的训练过程，并探究了最佳的训练策略。针对负样本远多于正样本的不平衡数据集，我们还尝试了不同的技术来提高分类性能。微调后的WizardCoder模型在平衡和不平衡的漏洞数据集上在ROC AUC和F1度量上实现了改进，证明了将预训练的语言模型用于源代码中的漏洞检测的有效性。主要贡献包括对最先进的代码语言模型WizardCoder进行微调，提高其训练速度而不影响性能，并对训练过程和策略进行了优化。

    This paper presents the results of finetuning large language models (LLMs) for the task of detecting vulnerabilities in source code. We leverage WizardCoder, a recent improvement of the state-of-the-art LLM StarCoder, and adapt it for vulnerability detection through further finetuning. To accelerate training, we modify WizardCoder's training procedure, also we investigate optimal training regimes. For the imbalanced dataset with many more negative examples than positive, we also explore different techniques to improve classification performance. The finetuned WizardCoder model achieves improvement in ROC AUC and F1 measures on balanced and imbalanced vulnerability datasets over CodeBERT-like model, demonstrating the effectiveness of adapting pretrained LLMs for vulnerability detection in source code. The key contributions are finetuning the state-of-the-art code LLM, WizardCoder, increasing its training speed without the performance harm, optimizing the training procedure and regimes, 
    
[^22]: 用于成本效益优化的因果机器学习在发展援助分配中的应用

    Causal Machine Learning for Cost-Effective Allocation of Development Aid. (arXiv:2401.16986v1 [stat.ML])

    [http://arxiv.org/abs/2401.16986](http://arxiv.org/abs/2401.16986)

    本文提出了一个因果机器学习框架，用于预测援助分配的异质化治疗效果，以支持有效的援助分配决策。

    

    联合国的可持续发展目标提供了“无人被遗弃”的更美好未来蓝图，为了在2030年之前实现这些目标，贫穷国家需要大量的发展援助。本文提出了一个因果机器学习框架，用于预测援助分配的异质化治疗效果，以支持有效的援助分配决策。具体而言，我们的框架包括三个组成部分：（i）一个平衡自编码器，利用表示学习将高维国家特征嵌入，同时解决治疗选择偏差问题；（ii）一个反事实生成器，用于计算在不同援助规模下的反事实结果，以解决小样本问题；（iii）一个推断模型，用于预测异质化的治疗效果曲线。我们使用105个国家战略性发展援助数据（总额超过52亿美元），以结束HIV/AIDS为目标，证明了我们的框架的有效性。

    The Sustainable Development Goals (SDGs) of the United Nations provide a blueprint of a better future by 'leaving no one behind', and, to achieve the SDGs by 2030, poor countries require immense volumes of development aid. In this paper, we develop a causal machine learning framework for predicting heterogeneous treatment effects of aid disbursements to inform effective aid allocation. Specifically, our framework comprises three components: (i) a balancing autoencoder that uses representation learning to embed high-dimensional country characteristics while addressing treatment selection bias; (ii) a counterfactual generator to compute counterfactual outcomes for varying aid volumes to address small sample-size settings; and (iii) an inference model that is used to predict heterogeneous treatment-response curves. We demonstrate the effectiveness of our framework using data with official development aid earmarked to end HIV/AIDS in 105 countries, amounting to more than USD 5.2 billion. F
    
[^23]: 使用深度学习进行多种收益率曲线建模和预测

    Multiple Yield Curve Modeling and Forecasting using Deep Learning. (arXiv:2401.16985v1 [stat.ML])

    [http://arxiv.org/abs/2401.16985](http://arxiv.org/abs/2401.16985)

    本文介绍了一种使用深度学习模型同时描述多种收益率曲线动态的方法，并通过结合自注意机制和非参数分位数回归，生成未来收益率的点预测和区间预测。实验证实了该方法的有效性，并且提出了深度集成和迁移学习的扩展和改进。

    

    本文介绍了一种使用深度学习模型同时描述多种收益率曲线动态的方法。我们旨在学习金融市场全球化引起的不同收益率曲线之间的依赖结构，并利用它来产生更准确的预测。通过结合自注意机制和非参数分位数回归，我们的模型可以生成未来收益率的点预测和区间预测。该框架的设计旨在避免影响多个分位数回归模型的分位数交叉问题。对两个不同数据集进行的数值实验证实了我们方法的有效性。最后，我们通过结合深度集成方法和迁移学习机制，探讨了潜在的扩展和改进。

    This manuscript introduces deep learning models that simultaneously describe the dynamics of several yield curves. We aim to learn the dependence structure among the different yield curves induced by the globalization of financial markets and exploit it to produce more accurate forecasts. By combining the self-attention mechanism and nonparametric quantile regression, our model generates both point and interval forecasts of future yields. The architecture is designed to avoid quantile crossing issues affecting multiple quantile regression models. Numerical experiments conducted on two different datasets confirm the effectiveness of our approach. Finally, we explore potential extensions and enhancements by incorporating deep ensemble methods and transfer learning mechanisms.
    
[^24]: 使用深度学习方法从IACT图像中选择伽玛事件

    Selection of gamma events from IACT images with deep learning methods. (arXiv:2401.16981v1 [astro-ph.IM])

    [http://arxiv.org/abs/2401.16981](http://arxiv.org/abs/2401.16981)

    本研究展示了在TAIGA-IACT的Monte Carlo（MC）图像上通过神经网络进行图像分类任务的结果，同时考虑了交错观测模式和适应NN分析的图像调整。

    

    伽玛射线观测站TAIGA的成像大气切伦科夫望远镜（IACTs）探测源自宇宙射线或伽玛射线与大气相互作用的广大空气阵射（EASs）。因此，望远镜获取EASs的图像。将伽玛射线图像与宇宙射线背景进行分离是该类型探测器的主要特点之一。然而，在实际的IACT观测中，需要同时观测背景和伽玛射线源。这种观测模式（称为交错观测）会改变事件的图像，从而影响了神经网络的选择质量。因此，在本文中，我们提出了在TAIGA-IACT的Monte Carlo（MC）图像上应用神经网络（NN）进行图像分类任务的结果。同时考虑交错观测模式和适应NN分析的图像调整。我们还探索了几种神经网络结构，可以直接从图像或进过预处理的图像分类事件。

    Imaging Atmospheric Cherenkov Telescopes (IACTs) of gamma ray observatory TAIGA detect the Extesnive Air Showers (EASs) originating from the cosmic or gamma rays interactions with the atmosphere. Thereby, telescopes obtain images of the EASs. The ability to segregate gamma rays images from the hadronic cosmic ray background is one of the main features of this type of detectors. However, in actual IACT observations simultaneous observation of the background and the source of gamma ray is needed. This observation mode (called wobbling) modifies images of events, which affects the quality of selection by neural networks.  Thus, in this work, the results of the application of neural networks (NN) for image classification task on Monte Carlo (MC) images of TAIGA-IACTs are presented. The wobbling mode is considered together with the image adaptation for adequate analysis by NNs. Simultaneously, we explore several neural network structures that classify events both directly from images or thr
    
[^25]: CORE: 通过强化学习实现可扩展高效的因果推断

    CORE: Towards Scalable and Efficient Causal Discovery with Reinforcement Learning. (arXiv:2401.16974v1 [cs.LG])

    [http://arxiv.org/abs/2401.16974](http://arxiv.org/abs/2401.16974)

    CORE是一种基于深度强化学习的因果推断和干预规划方法，可以高效地揭示因果结构，并在结构估计准确性和样本效率方面表现优于现有方法。

    

    因果推断是从数据中推断因果结构的具有挑战性的任务。受到Pearl的因果层次结构（Causal Hierarchy）的启发，该论文呼吁将干预引入到机器学习研究中。强化学习提供了一个方便的框架，用于实现这种主动的学习方法。本文提出了CORE，一种基于深度强化学习的因果推断和干预规划方法。CORE学习从数据中顺序重建因果图，并学习执行信息丰富的干预。我们的结果表明，CORE可以推广到未见过的图，并高效地揭示因果结构。此外，CORE可以扩展到具有多达10个变量的更大的图，并在结构估计准确性和样本效率方面优于现有方法。

    Causal discovery is the challenging task of inferring causal structure from data. Motivated by Pearl's Causal Hierarchy (PCH), which tells us that passive observations alone are not enough to distinguish correlation from causation, there has been a recent push to incorporate interventions into machine learning research. Reinforcement learning provides a convenient framework for such an active approach to learning. This paper presents CORE, a deep reinforcement learning-based approach for causal discovery and intervention planning. CORE learns to sequentially reconstruct causal graphs from data while learning to perform informative interventions. Our results demonstrate that CORE generalizes to unseen graphs and efficiently uncovers causal structures. Furthermore, CORE scales to larger graphs with up to 10 variables and outperforms existing approaches in structure estimation accuracy and sample efficiency. All relevant code and supplementary material can be found at https://github.com/s
    
[^26]: 在线资源分配与非平稳顾客

    Online Resource Allocation with Non-Stationary Customers. (arXiv:2401.16945v1 [cs.LG])

    [http://arxiv.org/abs/2401.16945](http://arxiv.org/abs/2401.16945)

    本文提出了一种用于在线资源分配的新算法，适用于非平稳顾客到达和未知的点击率。通过充分利用随机上下文摇臂和具有对抗性到达的在线匹配的结果，我们的方案实现了在顾客到达接近平稳时具有次线性遗憾，并在一般（非平稳）顾客到达分布下享受最优的竞争比率。我们通过大量的数值实验证明了我们的方法在各种不同的顾客场景下生成接近最优的收益。

    

    本文提出了一种用于在线资源分配的新算法，适用于非平稳顾客到达和未知的点击率。我们假设多种类型的顾客以非平稳随机方式到达，每个时期都有未知的到达率，并且顾客的点击率未知，只能在线学习。通过利用基于随机上下文摇臂和具有对抗性到达的在线匹配的结果，我们开发了一个在线方案，将资源分配给非平稳顾客。我们证明，在温和条件下，我们的方案实现了“两全其美”的效果：当顾客到达接近平稳时，方案具有次线性遗憾，并在一般（非平稳）顾客到达分布下享受最优的竞争比率。最后，我们进行了大量的数值实验，展示了我们的方法在所有不同顾客场景下生成接近最优的收益。

    We propose a novel algorithm for online resource allocation with non-stationary customer arrivals and unknown click-through rates. We assume multiple types of customers arrive in a nonstationary stochastic fashion, with unknown arrival rates in each period, and that customers' click-through rates are unknown and can only be learned online. By leveraging results from the stochastic contextual bandit with knapsack and online matching with adversarial arrivals, we develop an online scheme to allocate the resources to nonstationary customers. We prove that under mild conditions, our scheme achieves a ``best-of-both-world'' result: the scheme has a sublinear regret when the customer arrivals are near-stationary, and enjoys an optimal competitive ratio under general (non-stationary) customer arrival distributions. Finally, we conduct extensive numerical experiments to show our approach generates near-optimal revenues for all different customer scenarios.
    
[^27]: 利用深度学习对浸软纤维和导管进行分割和特征提取

    Segmentation and Characterization of Macerated Fibers and Vessels Using Deep Learning. (arXiv:2401.16937v1 [cs.CV])

    [http://arxiv.org/abs/2401.16937](http://arxiv.org/abs/2401.16937)

    这项工作开发了一种利用深度学习进行浸软纤维和导管分割和特征提取的自动方法，并且在显微镜图像中取得了快速而准确的分割效果。

    

    目的：木材由纤维和导管等不同种类的细胞组成，这些细胞的形状、大小和排列对于理解木材样本至关重要。通常，这涉及将样本浸泡在溶液中以分离细胞，然后将它们分散在载玻片上，用显微镜进行广域成像，捕捉数千个细胞。然而，这些细胞在图像中经常聚集和重叠，使用标准图像处理方法进行分割变得困难且耗时。结果：在这项工作中，我们开发了一种自动深度学习分割方法，利用一阶YOLOv8模型来快速而准确地对显微镜图像中的纤维和导管进行分割和特征提取。该模型可以分析32640 x 25920像素的图像，并展示出有效的细胞检测和分割，达到78%的mAP_0.5-0.95。为了评估模型的鲁棒性，我们对经过基因改造的纤维进行了研究。

    Purpose: Wood comprises different cell types, such as fibers and vessels, defining its properties. Studying their shape, size, and arrangement in microscopic images is crucial for understanding wood samples. Typically, this involves macerating (soaking) samples in a solution to separate cells, then spreading them on slides for imaging with a microscope that covers a wide area, capturing thousands of cells. However, these cells often cluster and overlap in images, making the segmentation difficult and time-consuming using standard image-processing methods. Results: In this work, we develop an automatic deep learning segmentation approach that utilizes the one-stage YOLOv8 model for fast and accurate fiber and vessel segmentation and characterization in microscopy images. The model can analyze 32640 x 25920 pixels images and demonstrate effective cell detection and segmentation, achieving a mAP_0.5-0.95 of 78 %. To assess the model's robustness, we examined fibers from a genetically modi
    
[^28]: 跨模态预测离散低维数据中连续气候模式的多模态表示学习

    Multi-modal Representation Learning for Cross-modal Prediction of Continuous Weather Patterns from Discrete Low-Dimensional Data. (arXiv:2401.16936v1 [cs.LG])

    [http://arxiv.org/abs/2401.16936](http://arxiv.org/abs/2401.16936)

    本研究提出了一种基于深度学习的方法，用于从离散低维数据中预测连续分辨率风数据。该方法解决了改善数据分辨率、降低数据维度、以及推断不同空间规格风数据的三个挑战。

    

    世界正在寻找不污染环境的清洁可再生能源，以减少对全球变暖起因二氧化碳排放的贡献。风能不仅有潜力减少温室气体排放，还可以满足不断增长的能源需求。为了有效利用风能，解决风数据分析中的以下三个挑战至关重要。首先，在各种气候条件下改善数据分辨率，以确保足够的信息用于评估潜在能源资源。其次，使用降维技术对从传感器/模拟中收集的数据进行高效管理和存储。第三，从一个空间规格推断风数据，特别是在数据获取可能不实用或昂贵的情况下。我们提出了一种基于深度学习的方法，实现了从离散低维数据中多模态连续分辨率风数据的预测。

    World is looking for clean and renewable energy sources that do not pollute the environment, in an attempt to reduce greenhouse gas emissions that contribute to global warming. Wind energy has significant potential to not only reduce greenhouse emission, but also meet the ever increasing demand for energy. To enable the effective utilization of wind energy, addressing the following three challenges in wind data analysis is crucial. Firstly, improving data resolution in various climate conditions to ensure an ample supply of information for assessing potential energy resources. Secondly, implementing dimensionality reduction techniques for data collected from sensors/simulations to efficiently manage and store large datasets. Thirdly, extrapolating wind data from one spatial specification to another, particularly in cases where data acquisition may be impractical or costly. We propose a deep learning based approach to achieve multi-modal continuous resolution wind data prediction from d
    
[^29]: 通过基于拓扑数据分析的聚类实现稀疏投资组合选择

    Sparse Portfolio Selection via Topological Data Analysis based Clustering. (arXiv:2401.16920v1 [q-fin.PM])

    [http://arxiv.org/abs/2401.16920](http://arxiv.org/abs/2401.16920)

    本文使用拓扑数据分析工具提出了一种基于聚类的稀疏投资组合选择策略，通过利用股票价格波动的拓扑特征，在持续图和景观空间上引入新的距离度量，并与聚类算法相结合，显著提升了多样市场情景下稀疏投资组合的绩效。

    

    本文使用拓扑数据分析工具，引入了一种针对稀疏投资组合构建的数据驱动聚类型股票选择策略。我们的资产选择策略利用股票价格波动的拓扑特征，选择一组拓扑类似（不同）的资产用于稀疏指数追踪（马科维茨）投资组合。我们引入了在持续图和景观空间上考虑时间成分的新距离度量，作为聚类算法的输入。我们对2009年至2020年的S\&P指数进行了实证分析，包括对COVID-19数据的研究，以验证我们方法的稳健性。我们将拓扑数据分析与聚类算法相结合的策略显著提升了不同市场情景下稀疏投资组合的综合绩效。

    This paper uses topological data analysis (TDA) tools and introduces a data-driven clustering-based stock selection strategy tailored for sparse portfolio construction. Our asset selection strategy exploits the topological features of stock price movements to select a subset of topologically similar (different) assets for a sparse index tracking (Markowitz) portfolio. We introduce new distance measures, which serve as an input to the clustering algorithm, on the space of persistence diagrams and landscapes that consider the time component of a time series. We conduct an empirical analysis on the S\&P index from 2009 to 2020, including a study on the COVID-19 data to validate the robustness of our methodology. Our strategy to integrate TDA with the clustering algorithm significantly enhanced the performance of sparse portfolios across various performance measures in diverse market scenarios.
    
[^30]: 节能的等变图神经网络在格子结构的亚波长型材料的弹性性质中的应用

    Energy-conserving equivariant GNN for elasticity of lattice architected metamaterials. (arXiv:2401.16914v1 [cs.LG])

    [http://arxiv.org/abs/2401.16914](http://arxiv.org/abs/2401.16914)

    本研究提出了一种节能的等变图神经网络模型，用于预测周期性支撑结构的刚度张量。通过编码的等变性和能量守恒定律的应用，该模型在预测性能和训练需求方面具有明显优势。

    

    格子是一种结构化的亚波长型材料，其性质强烈依赖于其几何设计。格子和图之间的类比使得可以使用图神经网络（GNN）作为一个比传统方法（如有限元建模）更快速的替代模型。本文提出了一个高阶GNN模型，用于预测周期性支撑结构的四阶刚度张量。该模型的关键特点是（i）SE（3）等变性和（ii）能量守恒定律的一致性。我们通过一些误差度量指标将该模型与非等变模型进行比较，展示了编码的等变性和能量守恒在预测性能和减少训练需求方面的好处。

    Lattices are architected metamaterials whose properties strongly depend on their geometrical design. The analogy between lattices and graphs enables the use of graph neural networks (GNNs) as a faster surrogate model compared to traditional methods such as finite element modelling. In this work we present a higher-order GNN model trained to predict the fourth-order stiffness tensor of periodic strut-based lattices. The key features of the model are (i) SE(3) equivariance, and (ii) consistency with the thermodynamic law of conservation of energy. We compare the model to non-equivariant models based on a number of error metrics and demonstrate the benefits of the encoded equivariance and energy conservation in terms of predictive performance and reduced training requirements.
    
[^31]: 使用超维计算的零样本分类

    Zero-shot Classification using Hyperdimensional Computing. (arXiv:2401.16876v1 [cs.CV])

    [http://arxiv.org/abs/2401.16876](http://arxiv.org/abs/2401.16876)

    本论文提出了一种使用超维计算的零样本分类方法，通过在属性编码器内使用符号-样分布表示的二进制编码表来紧凑地表示模型，可以实现零样本属性提取和零样本分类任务。

    

    零样本学习（ZSL）基于模型能够将输入分类到模型之前没有见过的新类别的能力。在解决这一挑战性任务时，提供一个辅助描述符，以属性集的形式描述ZSL分类中涉及的新类别，是一种被青睐的方法。受超维计算（HDC）启发，本文提出了在属性编码器内使用符号-样分布表示的二进制编码表来紧凑地表示一个计算简单的端到端可训练模型，我们将其命名为超维计算零样本分类器（HDC-ZSC）。它包括一个可训练的图像编码器、一个基于HDC的属性编码器和一个相似性核。我们展示了HDC-ZSC可以用于首先执行零样本属性提取任务，并且可以在最小架构改变的情况下，被重新用于零样本分类任务。

    Classification based on Zero-shot Learning (ZSL) is the ability of a model to classify inputs into novel classes on which the model has not previously seen any training examples. Providing an auxiliary descriptor in the form of a set of attributes describing the new classes involved in the ZSL-based classification is one of the favored approaches to solving this challenging task. In this work, inspired by Hyperdimensional Computing (HDC), we propose the use of stationary binary codebooks of symbol-like distributed representations inside an attribute encoder to compactly represent a computationally simple end-to-end trainable model, which we name Hyperdimensional Computing Zero-shot Classifier~(HDC-ZSC). It consists of a trainable image encoder, an attribute encoder based on HDC, and a similarity kernel. We show that HDC-ZSC can be used to first perform zero-shot attribute extraction tasks and, can later be repurposed for Zero-shot Classification tasks with minimal architectural changes
    
[^32]: 通过多种方式，将混合专家与MCTS相结合以提高国际象棋中的校验

    Checkmating One, by Using Many: Combining Mixture of Experts with MCTS to Improve in Chess. (arXiv:2401.16852v1 [cs.LG])

    [http://arxiv.org/abs/2401.16852](http://arxiv.org/abs/2401.16852)

    通过将混合专家方法和MCTS相结合，本研究在国际象棋中显著提升了下棋水平，验证了集成方法的有效性并展示了融入专家知识和战略原则到神经网络中的潜力。

    

    本文提出了一种新的方法，将深度学习与计算机棋盘相结合，同时使用混合专家方法和蒙特卡罗树搜索方法。我们的方法采用一套专门设计的模型，每个模型都针对游戏输入数据的特定变化做出响应。这导致了一个稀疏激活模型的框架，提供了显著的计算优势。我们的框架将混合专家方法与蒙特卡罗树搜索方法结合起来，以使其与国际象棋的战略阶段相一致，从而摆脱传统的“一刀切”的模型。相反，我们利用不同的游戏阶段定义，将计算任务有效地分配给多个专家神经网络。我们的实证研究显示，在游戏实力方面有了显著改进，超过了传统的单模型框架。这证实了我们集成方法的功效，并凸显了将专家知识和战略原则纳入神经网络中的潜力。

    This paper presents a new approach that integrates deep learning with computational chess, using both the Mixture of Experts (MoE) method and Monte-Carlo Tree Search (MCTS). Our methodology employs a suite of specialized models, each designed to respond to specific changes in the game's input data. This results in a framework with sparsely activated models, which provides significant computational benefits. Our framework combines the MoE method with MCTS, in order to align it with the strategic phases of chess, thus departing from the conventional ``one-for-all'' model. Instead, we utilize distinct game phase definitions to effectively distribute computational tasks across multiple expert neural networks. Our empirical research shows a substantial improvement in playing strength, surpassing the traditional single-model framework. This validates the efficacy of our integrated approach and highlights the potential of incorporating expert knowledge and strategic principles into neural net
    
[^33]: 评估基于机器学习的异常检测在数据完整性不同的数据集上的表现：一个案例研究

    Evaluating ML-Based Anomaly Detection Across Datasets of Varied Integrity: A Case Study. (arXiv:2401.16843v1 [cs.LG])

    [http://arxiv.org/abs/2401.16843](http://arxiv.org/abs/2401.16843)

    本研究评估了基于机器学习的异常检测在数据完整性不同的数据集上的表现，并发现Random Forest算法在各种数据集上都展现出了卓越的稳健性。

    

    在这项研究中，我们解决了网络流量异常检测中数据完整性的普遍问题，这对于开发机器学习模型进行异常检测至关重要。我们介绍了两个经过改进的CICIDS-2017数据集的版本，NFS-2023-nTE和NFS-2023-TE，利用NFStream进行方法上合理的流量到期和标记处理。我们的研究对比了Random Forest（RF）算法在原始的CICIDS-2017数据集、其改进版本WTMC-2021和CRiSIS-2022以及我们基于NFStream生成的数据集上的性能，在二分类和多分类上下文中。我们观察到，RF模型表现出异常的稳健性，无论底层数据集的质量如何，在性能指标上都能保持一致的高水平，这引发了对实际影响的重要讨论。

    Cybersecurity remains a critical challenge in the digital age, with network traffic flow anomaly detection being a key pivotal instrument in the fight against cyber threats. In this study, we address the prevalent issue of data integrity in network traffic datasets, which are instrumental in developing machine learning (ML) models for anomaly detection. We introduce two refined versions of the CICIDS-2017 dataset, NFS-2023-nTE and NFS-2023-TE, processed using NFStream to ensure methodologically sound flow expiration and labeling. Our research contrasts the performance of the Random Forest (RF) algorithm across the original CICIDS-2017, its refined counterparts WTMC-2021 and CRiSIS-2022, and our NFStream-generated datasets, in both binary and multi-class classification contexts. We observe that the RF model exhibits exceptional robustness, achieving consistent high-performance metrics irrespective of the underlying dataset quality, which prompts a critical discussion on the actual impac
    
[^34]: 基于T-CUR分解的可分离非负张量分解

    Coseparable Nonnegative Tensor Factorization With T-CUR Decomposition. (arXiv:2401.16836v1 [cs.LG])

    [http://arxiv.org/abs/2401.16836](http://arxiv.org/abs/2401.16836)

    本文提出了一种基于T-CUR分解的可分离非负张量分解方法，用于在多维数据中提取有意义的特征。

    

    非负矩阵分解(NMF)是一种重要的无监督学习方法，用于从数据中提取有意义的特征。为了在多项式时间框架内解决NMF问题，研究人员引入了可分离性假设，最近演变为可分离的概念。这一进展为原始数据提供了更高效的核心表示。然而，在现实世界中，数据更自然地被表示为多维数组，如图像或视频。将NMF应用于高维数据涉及向量化，会导致丢失关键的多维度相关性。为了保留数据中这些固有的相关性，我们转向张量(多维数组)并利用张量t乘积。这种方法将可分离的NMF扩展到张量设置，从而创建了我们所称的可分离非负张量分解(NTF)。在这项工作中，我们提供了一种交替索引选择方法来选择cos

    Nonnegative Matrix Factorization (NMF) is an important unsupervised learning method to extract meaningful features from data. To address the NMF problem within a polynomial time framework, researchers have introduced a separability assumption, which has recently evolved into the concept of coseparability. This advancement offers a more efficient core representation for the original data. However, in the real world, the data is more natural to be represented as a multi-dimensional array, such as images or videos. The NMF's application to high-dimensional data involves vectorization, which risks losing essential multi-dimensional correlations. To retain these inherent correlations in the data, we turn to tensors (multidimensional arrays) and leverage the tensor t-product. This approach extends the coseparable NMF to the tensor setting, creating what we term coseparable Nonnegative Tensor Factorization (NTF). In this work, we provide an alternating index selection method to select the cos
    
[^35]: 对合成学生数据的知识追踪性能分析

    Analysis of Knowledge Tracing performance on synthesised student data. (arXiv:2401.16832v1 [cs.CY])

    [http://arxiv.org/abs/2401.16832](http://arxiv.org/abs/2401.16832)

    通过合成数据进行训练可以达到与真实数据相似的知识追踪性能。

    

    知识追踪旨在通过跟踪学生的知识状态的发展来预测他们未来的表现。尽管在这一领域取得了一些进展，但由于数据保护问题，KT模型在教育系统中的应用仍受到数据限制：1）由于数据保护问题，无法获得现实生活数据；2）公共数据集中缺乏多样性；3）基准数据集中存在重复记录的噪音。为解决这些问题，我们使用三种基于公共数据集的统计策略模拟了学生数据，并测试了它们在两个KT基准上的性能。虽然我们观察到额外的合成数据只带来了轻微的性能改进，但我们的研究表明，仅使用合成数据进行训练可以达到与真实数据相似的性能水平。

    Knowledge Tracing (KT) aims to predict the future performance of students by tracking the development of their knowledge states. Despite all the recent progress made in this field, the application of KT models in education systems is still restricted from the data perspectives: 1) limited access to real life data due to data protection concerns, 2) lack of diversity in public datasets, 3) noises in benchmark datasets such as duplicate records. To resolve these problems, we simulated student data with three statistical strategies based on public datasets and tested their performance on two KT baselines. While we observe only minor performance improvement with additional synthetic data, our work shows that using only synthetic data for training can lead to similar performance as real data.
    
[^36]: H2O-Danube-1.8B 技术报告

    H2O-Danube-1.8B Technical Report. (arXiv:2401.16818v1 [cs.CL])

    [http://arxiv.org/abs/2401.16818](http://arxiv.org/abs/2401.16818)

    H2O-Danube-1.8B 是一个在 1T 个标记上训练的 18 亿语言模型，具有高度竞争力的指标。同时，他们还发布了一个经过微调和优化训练的聊天模型，进一步推动语言模型的经济民主化。

    

    我们介绍了 H2O-Danube-1.8B，这是一个在 1T 个标记上训练的 18 亿语言模型，遵循 LLama 2 和 Mistral 的核心原则。我们利用和改进了各种大规模语言模型预训练的技术。尽管我们的模型训练所使用的总标记数量明显少于相似规模的参考模型，但它在众多基准测试中展现出了高度竞争力的指标。我们还发布了一个经过监督微调和直接偏好优化训练的聊天模型。我们以 Apache 2.0 许可证将 H2O-Danube-1.8B 开放，进一步推动 LLMs 的经济民主化，让更广泛的受众受益。

    We present H2O-Danube-1.8B, a 1.8B language model trained on 1T tokens following the core principles of LLama 2 and Mistral. We leverage and refine various techniques for pre-training large language models. Although our model is trained on significantly fewer total tokens compared to reference models of similar size, it exhibits highly competitive metrics across a multitude of benchmarks. We additionally release a chat model trained with supervised fine-tuning followed by direct preference optimization. We make H2O-Danube-1.8B openly available under Apache 2.0 license further democratizing LLMs to a wider audience economically.
    
[^37]: 通过增加表示来编码时间统计空间先验

    Encoding Temporal Statistical-space Priors via Augmented Representation. (arXiv:2401.16808v1 [cs.LG])

    [http://arxiv.org/abs/2401.16808](http://arxiv.org/abs/2401.16808)

    通过增加表示的方式编码时间统计空间先验，以应对时间序列数据建模中的挑战。我们的方法在两个数据集上的实证推广性能明显优于五个最新的基准方法。具有高度模块化性质的方法适用于各种场景。

    

    时间序列数据建模仍然是一个普遍存在的问题，因为时间维度与许多领域密切相关。尽管在时间序列预测方面取得了显著进展，但高噪声信号比、非正态性、非平稳性和数据缺乏仍然是挑战从业者的问题。为此，我们利用一种简单的表示增强技术来克服这些挑战。我们的增强表示在每个时间步骤上作为统计空间先验进行编码。作为响应，我们将我们的方法命名为统计空间增强表示（SSAR）。基于高维数据生成过程，启发了我们的表示增强。我们在两个数据集上对两个下游时间学习算法的经验泛化性能进行了严格的检查。我们的方法明显击败了五个最新的基准线。此外，我们的方法具有高度模块化的性质，可以轻松应用于各种情况。最后，我们提供了全面的理论视角。

    Modeling time series data remains a pervasive issue as the temporal dimension is inherent to numerous domains. Despite significant strides in time series forecasting, high noise-to-signal ratio, non-normality, non-stationarity, and lack of data continue challenging practitioners. In response, we leverage a simple representation augmentation technique to overcome these challenges. Our augmented representation acts as a statistical-space prior encoded at each time step. In response, we name our method Statistical-space Augmented Representation (SSAR). The underlying high-dimensional data-generating process inspires our representation augmentation. We rigorously examine the empirical generalization performance on two data sets with two downstream temporal learning algorithms. Our approach significantly beats all five up-to-date baselines. Moreover, the highly modular nature of our approach can easily be applied to various settings. Lastly, fully-fledged theoretical perspectives are availa
    
[^38]: PBSCSR：钢琴黑市乐谱作曲家风格识别数据集

    PBSCSR: The Piano Bootleg Score Composer Style Recognition Dataset. (arXiv:2401.16803v1 [cs.SD])

    [http://arxiv.org/abs/2401.16803](http://arxiv.org/abs/2401.16803)

    本研究介绍了PBSCSR数据集，用于研究钢琴乐谱作曲家风格识别。数据集包含了盗版乐谱图像和相关元数据，可以进行多个研究任务。

    

    本文介绍了PBSCSR数据集，用于研究钢琴乐谱作曲家风格识别。我们的目标是创建一个研究作曲家风格识别的数据集，它既像MNIST一样易于获取，又像ImageNet一样具有挑战性。为了实现这个目标，我们从IMSLP的钢琴乐谱图像中采样固定长度的盗版乐谱片段。数据集本身包含40,000个62x64的盗版乐谱图像，用于进行9分类任务，以及100,000个62x64的盗版乐谱图像，用于进行100分类任务，还有29,310个无标签的可变长度的盗版乐谱图像，用于预训练。标记数据以与MNIST图像类似的形式呈现，以便极其方便地可视化、操作和训练模型。此外，我们还包括相关的元数据，以允许访问IMSLP上的原始乐谱图像和其他相关数据。我们描述了几个可以使用该数据进行研究的任务。

    This article motivates, describes, and presents the PBSCSR dataset for studying composer style recognition of piano sheet music. Our overarching goal was to create a dataset for studying composer style recognition that is "as accessible as MNIST and as challenging as ImageNet." To achieve this goal, we sample fixed-length bootleg score fragments from piano sheet music images on IMSLP. The dataset itself contains 40,000 62x64 bootleg score images for a 9-way classification task, 100,000 62x64 bootleg score images for a 100-way classification task, and 29,310 unlabeled variable-length bootleg score images for pretraining. The labeled data is presented in a form that mirrors MNIST images, in order to make it extremely easy to visualize, manipulate, and train models in an efficient manner. Additionally, we include relevant metadata to allow access to the underlying raw sheet music images and other related data on IMSLP. We describe several research tasks that could be studied with the data
    
[^39]: 在时态图中的节点特征预测的在线算法

    Online Algorithm for Node Feature Forecasting in Temporal Graphs. (arXiv:2401.16800v1 [cs.LG])

    [http://arxiv.org/abs/2401.16800](http://arxiv.org/abs/2401.16800)

    本文提出了一种在线算法"mspace"，适用于预测时态图中的节点特征。与其他基线方法相比，mspace表现出与最先进方法相当甚至更好的性能，并且在训练样本有限的情况下依然具有鲁棒性。

    

    本文提出了一种名为"mspace"的在线算法，用于预测时态图中的节点特征，该算法能够灵活地捕捉不同节点之间的空间交叉相关性以及节点内的时间自相关性。该算法可用于概率和确定性的多步预测，适用于估计和生成任务。与基于图神经网络（GNN）和经典卡尔曼滤波器的各种基线方法进行比较评估，结果表明mspace与最先进的方法水平相当，甚至在某些数据集上超过它们。重要的是，mspace在具有不同训练样本大小的数据集上表现出一致的鲁棒性，这是与GNN方法相比的一个显著优势，后者需要丰富的训练样本来有效地学习数据中的时空趋势。因此，在训练样本有限的情况下，采用mspace具有优势。此外，我们建立了理论模型来证明该算法的性能保证，进一步验证了mspace的有效性。

    In this paper, we propose an online algorithm "mspace" for forecasting node features in temporal graphs, which adeptly captures spatial cross-correlation among different nodes as well as the temporal autocorrelation within a node. The algorithm can be used for both probabilistic and deterministic multi-step forecasting, making it applicable for estimation and generation tasks. Comparative evaluations against various baselines, including graph neural network (GNN) based models and classical Kalman filters, demonstrate that mspace performs at par with the state-of-the-art and even surpasses them on some datasets. Importantly, mspace demonstrates consistent robustness across datasets with varying training sizes, a notable advantage over GNN-based methods requiring abundant training samples to learn the spatiotemporal trends in the data effectively. Therefore, employing mspace is advantageous in scenarios where the training sample availability is limited. Additionally, we establish theoret
    
[^40]: 可学习提示作为伪插补方法：重新评估传统电子健康记录数据插补在下游临床预测中的必要性

    Learnable Prompt as Pseudo-Imputation: Reassessing the Necessity of Traditional EHR Data Imputation in Downstream Clinical Prediction. (arXiv:2401.16796v1 [cs.LG])

    [http://arxiv.org/abs/2401.16796](http://arxiv.org/abs/2401.16796)

    本研究提出了一种新的训练协议，可学习提示作为伪插补（PAI），通过构建可学习的提示来模拟下游模型对缺失值的隐含偏好，显著提升所有电子健康记录（EHR）分析模型的性能。

    

    基于电子健康记录（EHR）分析患者的健康状况是医学信息学中的一个基本研究问题。EHR中存在大量缺失值，这使得深度神经网络难以直接基于EHR模型化患者的健康状况。现有的深度学习训练协议需要使用统计信息或插补模型来重构缺失值，然而，这些协议会将非现实的数据注入到下游EHR分析模型中，极大地限制了模型的性能。本文介绍了一种新的训练协议——可学习提示作为伪插补（PAI）。PAI不再引入任何插补数据，而是构建一个可学习的提示来模拟下游模型对缺失值的隐含偏好，从而显著提高了所有EHR分析模型的性能。此外，我们的实验结果表明，在数据不足和高噪声的情况下，PAI表现出更高的鲁棒性。

    Analyzing the health status of patients based on Electronic Health Records (EHR) is a fundamental research problem in medical informatics. The presence of extensive missing values in EHR makes it challenging for deep neural networks to directly model the patient's health status based on EHR. Existing deep learning training protocols require the use of statistical information or imputation models to reconstruct missing values; however, the protocols inject non-realistic data into downstream EHR analysis models, significantly limiting model performance. This paper introduces Learnable Prompt as Pseudo Imputation (PAI) as a new training protocol. PAI no longer introduces any imputed data but constructs a learnable prompt to model the implicit preferences of the downstream model for missing values, resulting in a significant performance improvement for all EHR analysis models. Additionally, our experiments show that PAI exhibits higher robustness in situations of data insufficiency and hig
    
[^41]: 基于性能洞察的人工智能驱动的足球转会费用预测

    Performance Insights-based AI-driven Football Transfer Fee Prediction. (arXiv:2401.16795v1 [cs.LG])

    [http://arxiv.org/abs/2401.16795](http://arxiv.org/abs/2401.16795)

    该研究开发了一种利用人工智能的方法，可以预测足球球员的转会费用。这可以帮助俱乐部做出更好的决策，提高表现并增加俱乐部的预算。

    

    我们开发了一种人工智能方法来预测足球球员的转会费用。该模型可以帮助俱乐部在购买和出售球员时做出更好的决策，从而提高表现和增加俱乐部预算。通过收集球员表现、转会费用和其他可能影响球员价值的因素的数据，我们训练了一个能够准确预测球员对比赛影响的机器学习模型。我们进一步将所得结果作为转会费用预测器的特征之一。该模型可以帮助俱乐部识别被低估的球员，并在出售时获得利润。它还可以帮助俱乐部避免为球员支付过高费用。我们相信我们的模型可以成为足球俱乐部的有价值工具，为他们在球员招募和转会方面做出更好的决策。

    We developed an artificial intelligence approach to predict the transfer fee of a football player. This model can help clubs make better decisions about which players to buy and sell, which can lead to improved performance and increased club budgets. Having collected data on player performance, transfer fees, and other factors that might affect a player's value, we then used this data to train a machine learning model that can accurately predict a player's impact on the game. We further passed the obtained results as one of the features to the predictor of transfer fees. The model can help clubs identify players who are undervalued and who could be sold for a profit. It can also help clubs avoid overpaying for players. We believe that our model can be a valuable tool for football clubs. It can help them make better decisions about player recruitment and transfers.
    
[^42]: 加速云端人工智能 (ACAI)

    Accelerated Cloud for Artificial Intelligence (ACAI). (arXiv:2401.16791v1 [cs.LG])

    [http://arxiv.org/abs/2401.16791](http://arxiv.org/abs/2401.16791)

    ACAI是一个基于云端的机器学习平台，通过实现基于云端的存储和索引的应用，支持模型管理，帮助提高机器学习从业者的生产力。

    

    训练有效的机器学习 (ML) 模型是一个需要在多个维度上付出努力的迭代过程。纵向上，一个单一的管道通常包括原始数据集的初始ETL（提取、变换、加载），模型训练阶段以及从中获取模型性能统计的评估阶段。横向上，在模型配置的搜索空间中可能需要多个这样的管道来找到最佳模型。许多从业者常常手动维护日志并编写简单的粘合代码以自动化工作流程。然而，在云端进行这个过程并不是一件简单的任务，它涉及资源供应、数据管理以及作业历史记录的记录，以确保结果可重现。我们提出了一个端到端的基于云端的机器学习平台，加速云端人工智能（ACAI），以帮助提高机器学习从业者的生产力。ACAI通过实现基于云端的存储和索引的应用，支持模型管理，简化了这个过程。

    Training an effective Machine learning (ML) model is an iterative process that requires effort in multiple dimensions. Vertically, a single pipeline typically includes an initial ETL (Extract, Transform, Load) of raw datasets, a model training stage, and an evaluation stage where the practitioners obtain statistics of the model performance. Horizontally, many such pipelines may be required to find the best model within a search space of model configurations. Many practitioners resort to maintaining logs manually and writing simple glue code to automate the workflow. However, carrying out this process on the cloud is not a trivial task in terms of resource provisioning, data management, and bookkeeping of job histories to make sure the results are reproducible. We propose an end-to-end cloud-based machine learning platform, Accelerated Cloud for AI (ACAI), to help improve the productivity of ML practitioners. ACAI achieves this goal by enabling cloud-based storage of indexed, labeled, a
    
[^43]: 通过HawkEye Loss在支持向量回归中提高效率和鲁棒性

    Enhancing Efficiency and Robustness in Support Vector Regression with HawkEye Loss. (arXiv:2401.16785v1 [cs.LG])

    [http://arxiv.org/abs/2401.16785](http://arxiv.org/abs/2401.16785)

    通过引入名为HawkEye损失函数的新的对称损失函数，本文解决了支持向量回归在处理离群值和噪声时遇到的挑战，并提供了增强的泛化性能和鲁棒性。

    

    支持向量回归（SVR）由于其在各个领域的广泛应用而受到了显著的关注，在面对离群值和噪声时，SVR遇到了挑战，主要是由于使用了ε-insensitive损失函数。为了解决这个限制，具有有界损失函数的SVR已成为一种吸引人的替代方案，提供了增强的泛化性能和鲁棒性。值得注意的是，最近的研究关注于设计具有平滑特性的有界损失函数，促进了梯度优化算法的采用。然而，需要强调的是，这些有界和平滑的损失函数不具有一个不敏感的区域。在本文中，我们通过引入一种名为HawkEye损失函数的新的对称损失函数来解决上述约束。值得注意的是，HawkEye损失函数作为SVR中的第一个损失函数突出显示出来。

    Support vector regression (SVR) has garnered significant popularity over the past two decades owing to its wide range of applications across various fields. Despite its versatility, SVR encounters challenges when confronted with outliers and noise, primarily due to the use of the $\varepsilon$-insensitive loss function. To address this limitation, SVR with bounded loss functions has emerged as an appealing alternative, offering enhanced generalization performance and robustness. Notably, recent developments focus on designing bounded loss functions with smooth characteristics, facilitating the adoption of gradient-based optimization algorithms. However, it's crucial to highlight that these bounded and smooth loss functions do not possess an insensitive zone. In this paper, we address the aforementioned constraints by introducing a novel symmetric loss function named the HawkEye loss function. It is worth noting that the HawkEye loss function stands out as the first loss function in SVR
    
[^44]: 在分布变化下的图公平学习

    Graph Fairness Learning under Distribution Shifts. (arXiv:2401.16784v1 [cs.LG])

    [http://arxiv.org/abs/2401.16784](http://arxiv.org/abs/2401.16784)

    论文主要研究了在分布变化下的图公平学习，通过理论分析和实证研究发现了决定图中偏差的因素，并探索了训练图和测试图之间表示距离的影响，对于在图结构数据上确保公平性具有重要意义。

    

    图神经网络（GNNs）在图结构数据上取得了显著的性能。然而，GNNs可能会从训练数据中继承偏见，并根据敏感属性（如性别和种族）做出歧视性预测。最近，越来越多的人关注在GNNs上确保公平性，但所有这些方法都基于训练数据和测试数据在相同分布下的假设，即训练数据和测试数据来自同一个图。在分布变化下，图的公平性性能是否会降低？分布变化如何影响图的公平学习？所有这些开放问题从理论上很大程度上未被探索。为了回答这些问题，我们首先在理论上确定了决定图中偏差的因素。随后，我们探索了影响测试图公平性的因素，其中一个值得注意的因素是训练图和测试图之间某些群体的表示距离。

    Graph neural networks (GNNs) have achieved remarkable performance on graph-structured data. However, GNNs may inherit prejudice from the training data and make discriminatory predictions based on sensitive attributes, such as gender and race. Recently, there has been an increasing interest in ensuring fairness on GNNs, but all of them are under the assumption that the training and testing data are under the same distribution, i.e., training data and testing data are from the same graph. Will graph fairness performance decrease under distribution shifts? How does distribution shifts affect graph fairness learning? All these open questions are largely unexplored from a theoretical perspective. To answer these questions, we first theoretically identify the factors that determine bias on a graph. Subsequently, we explore the factors influencing fairness on testing graphs, with a noteworthy factor being the representation distances of certain groups between the training and testing graph. M
    
[^45]: 在磁共振成像中胎儿脑运动校正的文献综述

    A Literature Review on Fetus Brain Motion Correction in MRI. (arXiv:2401.16782v1 [eess.IV])

    [http://arxiv.org/abs/2401.16782](http://arxiv.org/abs/2401.16782)

    本文综述了胎儿磁共振成像中脑运动校正的最新进展，包括传统的校正方法和基于深度学习的技术，并提供对潜在解决方案和未来改进的合理观点。

    

    本文综述了胎儿磁共振成像中脑运动校正的最新进展。我们深入探讨了各种当代的方法和技术进步，旨在克服这些挑战。其中包括传统的三维胎儿磁共振成像校正方法，如切片到体积配准（SVR），基于深度学习的技术，如卷积神经网络（CNNs），长短期记忆网络（LSTM），变压器（Transformers），生成对抗网络（GANs）以及最近的扩散模型的进展。从这篇文献综述中得出的见解反映了对胎儿磁共振成像中脑运动技术复杂性和实际影响的深入理解，为潜在解决方案和未来改进提供了合理的观点。

    This paper provides a comprehensive review of the latest advancements in fetal motion correction in MRI. We delve into various contemporary methodologies and technological advancements aimed at overcoming these challenges. It includes traditional 3D fetal MRI correction methods like Slice to Volume Registration (SVR), deep learning-based techniques such as Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM) Networks, Transformers, Generative Adversarial Networks (GANs) and most recent advancements of Diffusion Models. The insights derived from this literature review reflect a thorough understanding of both the technical intricacies and practical implications of fetal motion in MRI studies, offering a reasoned perspective on potential solutions and future improvements in this field.
    
[^46]: 通过实例规范化流解决时间序列预测中的分布偏移问题

    Addressing Distribution Shift in Time Series Forecasting with Instance Normalization Flows. (arXiv:2401.16777v1 [cs.LG])

    [http://arxiv.org/abs/2401.16777](http://arxiv.org/abs/2401.16777)

    本文提出了一种通过实例规范化流解决时间序列预测中的分布偏移问题的方法，该方法不依赖于固定统计数据，也不限制于预测架构。通过双层优化问题实现转换和预测的联合学习，并提出了实例规范化流作为一种新颖的可逆网络用于时间序列转换。实验证明该方法在合成数据和真实数据上优于最先进的基线模型。

    

    由于时间序列的非平稳性，分布偏移问题很大程度上阻碍了时间序列预测的性能。现有的解决方案要么无法处理超出简单统计的偏移，要么与预测模型的兼容性有限。在本文中，我们提出了一种针对时间序列预测的通用解耦公式，不依赖于固定统计数据，也不限制于预测架构。然后，我们将这种公式形式化为一个双层优化问题，以实现转换（外循环）和预测（内循环）的联合学习。此外，对于转换而言，对表达能力和双向性的特殊要求促使我们提出了实例规范化流（IN-Flow），一种新颖的可逆网络用于时间序列转换。大量实验证明我们的方法在合成数据和真实数据上始终优于最先进的基线模型。

    Due to non-stationarity of time series, the distribution shift problem largely hinders the performance of time series forecasting. Existing solutions either fail for the shifts beyond simple statistics or the limited compatibility with forecasting models. In this paper, we propose a general decoupled formulation for time series forecasting, with no reliance on fixed statistics and no restriction on forecasting architectures. Then, we make such a formulation formalized into a bi-level optimization problem, to enable the joint learning of the transformation (outer loop) and forecasting (inner loop). Moreover, the special requirements of expressiveness and bi-direction for the transformation motivate us to propose instance normalization flows (IN-Flow), a novel invertible network for time series transformation. Extensive experiments demonstrate our method consistently outperforms state-of-the-art baselines on both synthetic and real-world data.
    
[^47]: 利用嵌套MLMC对具有难以处理的似然函数的顺序神经后验估计进行优化

    Leveraging Nested MLMC for Sequential Neural Posterior Estimation with Intractable Likelihoods. (arXiv:2401.16776v1 [stat.CO])

    [http://arxiv.org/abs/2401.16776](http://arxiv.org/abs/2401.16776)

    本研究提出一种嵌套APT方法来解决顺序神经后验估计中的嵌套期望计算问题，从而实现了收敛性分析。

    

    最近提出了顺序神经后验估计（SNPE）技术，用于处理具有难以处理的似然函数的基于模拟的模型。它们致力于通过使用基于神经网络的条件密度估计器自适应地生成的模拟来学习后验。作为一种SNPE技术，Greenberg等人（2019）提出的自动后验变换（APT）方法表现出色，并可应用于高维数据。然而，APT方法包含计算难以处理的归一化常数的对数的期望，即嵌套期望。尽管原子APT通过离散化归一化常数来解决这个问题，但分析学习的收敛性仍然具有挑战性。在本文中，我们提出了一种嵌套APT方法来估计相关的嵌套期望。这有助于建立收敛性分析。由于损失函数及其梯度的嵌套估计是有偏的，我们进行了

    Sequential neural posterior estimation (SNPE) techniques have been recently proposed for dealing with simulation-based models with intractable likelihoods. They are devoted to learning the posterior from adaptively proposed simulations using neural network-based conditional density estimators. As a SNPE technique, the automatic posterior transformation (APT) method proposed by Greenberg et al. (2019) performs notably and scales to high dimensional data. However, the APT method bears the computation of an expectation of the logarithm of an intractable normalizing constant, i.e., a nested expectation. Although atomic APT was proposed to solve this by discretizing the normalizing constant, it remains challenging to analyze the convergence of learning. In this paper, we propose a nested APT method to estimate the involved nested expectation instead. This facilitates establishing the convergence analysis. Since the nested estimators for the loss function and its gradient are biased, we make
    
[^48]: 对于具有未知大规模衰落、信道统计、噪音方差和活动概率的无线单元网络中的大规模连接活动检测: 一种贝叶斯方法的研究

    Activity Detection for Massive Connectivity in Cell-free Networks with Unknown Large-scale Fading, Channel Statistics, Noise Variance, and Activity Probability: A Bayesian Approach. (arXiv:2401.16775v1 [cs.LG])

    [http://arxiv.org/abs/2401.16775](http://arxiv.org/abs/2401.16775)

    本研究探讨了在没有关于网络的重要信息的情况下进行活动检测的问题，并通过使用贝叶斯方法来处理大量未知参数。研究提出了最大后验估计器和变分方法来解决这个问题。

    

    活动检测是下一代免授权多址中的重要任务。虽然有许多现有的算法专门设计用于此目的，但它们大多需要精确的网络信息，如大规模衰落系数、小规模衰落信道统计、访问点的噪音方差和用户活动概率。获取这些信息将会耗费大量开销，并且其估计值可能不准确。在无线单元网络中，这个问题更加严重，因为存在许多这些参数需要获取。因此，本文旨在研究在没有上述信息的情况下的活动检测问题。为了处理这么多未知参数，本文采用了贝叶斯方法，其中未知变量赋予了先验分布，有效地起到了正则化的作用。结合似然函数，使用了最大后验（MAP）估计器和变分

    Activity detection is an important task in the next generation grant-free multiple access. While there are a number of existing algorithms designed for this purpose, they mostly require precise information about the network, such as large-scale fading coefficients, small-scale fading channel statistics, noise variance at the access points, and user activity probability. Acquiring these information would take a significant overhead and their estimated values might not be accurate. This problem is even more severe in cell-free networks as there are many of these parameters to be acquired. Therefore, this paper sets out to investigate the activity detection problem without the above-mentioned information. In order to handle so many unknown parameters, this paper employs the Bayesian approach, where the unknown variables are endowed with prior distributions which effectively act as regularizations. Together with the likelihood function, a maximum a posteriori (MAP) estimator and a variatio
    
[^49]: 基于外部奖励的鉴别器的软Q模仿学习

    Extrinsicaly Rewarded Soft Q Imitation Learning with Discriminator. (arXiv:2401.16772v1 [cs.LG])

    [http://arxiv.org/abs/2401.16772](http://arxiv.org/abs/2401.16772)

    本论文提出了一种基于外部奖励的鉴别器的软Q模仿学习方法，旨在解决在少量专家数据和采样数据中进行模仿学习时遇到的困难，同时通过添加基于对抗的奖励函数，使算法更加稳健和高效。

    

    在难以设计奖励或奖励稀疏的环境中，模仿学习常常与强化学习结合使用，但在少量专家数据和采样数据中很难在未知状态中良好地进行模仿。行为克隆等监督学习方法不需要采样数据，但通常会受到分布偏移的困扰。基于强化学习的方法，如逆向强化学习和生成对抗模仿学习（GAIL），可以从少量专家数据中进行学习，但通常需要与环境进行交互。软Q模仿学习（SQIL）解决了这些问题，并通过将行为克隆和常数奖励的软Q学习相结合，表明能够高效学习。为了使该算法对分布偏移更加稳健，我们提出了一种更高效和更稳健的算法，通过在该方法中添加基于对抗的奖励函数。

    Imitation learning is often used in addition to reinforcement learning in environments where reward design is difficult or where the reward is sparse, but it is difficult to be able to imitate well in unknown states from a small amount of expert data and sampling data. Supervised learning methods such as Behavioral Cloning do not require sampling data, but usually suffer from distribution shift. The methods based on reinforcement learning, such as inverse reinforcement learning and Generative Adversarial imitation learning (GAIL), can learn from only a few expert data. However, they often need to interact with the environment. Soft Q imitation learning (SQIL) addressed the problems, and it was shown that it could learn efficiently by combining Behavioral Cloning and soft Q-learning with constant rewards. In order to make this algorithm more robust to distribution shift, we propose more efficient and robust algorithm by adding to this method a reward function based on adversarial invers
    
[^50]: MolPLA: 一个用于学习核心、R-基和连接点的分子预训练框架

    MolPLA: A Molecular Pretraining Framework for Learning Cores, R-Groups and their Linker Joints. (arXiv:2401.16771v1 [cs.LG])

    [http://arxiv.org/abs/2401.16771](http://arxiv.org/abs/2401.16771)

    MolPLA是一个用于学习分子核心、R-基和连接点的分子预训练框架，通过掩码图对比学习可以深入理解分子的可分解部分，同时还可以帮助化学家在先导优化场景中找到可替代的R-基。

    

    分子核心结构和R-基是药物开发中的重要概念。将这些概念与传统的图预训练方法相结合，可以促进对分子的更深入理解。我们提出了MolPLA，一种新颖的预训练框架，它使用掩码图对比学习来理解分子中的可分解部分，以揭示其核心结构和外围R-基。此外，我们还制定了一个额外的框架，使MolPLA能够帮助化学家在先导优化场景中找到可替代的R-基。分子性质预测的实验结果表明，MolPLA展现出与当前最先进模型相当的可预测性。定性分析表明，MolPLA能够区分核心和R-基亚结构，识别分子中的可分解区域，并通过理性建议不同查询核心温度下的R-基替换，为先导优化场景做出贡献。

    Molecular core structures and R-groups are essential concepts in drug development. Integration of these concepts with conventional graph pre-training approaches can promote deeper understanding in molecules. We propose MolPLA, a novel pre-training framework that employs masked graph contrastive learning in understanding the underlying decomposable parts inmolecules that implicate their core structure and peripheral R-groups. Furthermore, we formulate an additional framework that grants MolPLA the ability to help chemists find replaceable R-groups in lead optimization scenarios. Experimental results on molecular property prediction show that MolPLA exhibits predictability comparable to current state-of-the-art models. Qualitative analysis implicate that MolPLA is capable of distinguishing core and R-group sub-structures, identifying decomposable regions in molecules and contributing to lead optimization scenarios by rationally suggesting R-group replacements given various query core tem
    
[^51]: 基于对比学习的深度神经网络故障注入攻击检测和恢复

    Detection and Recovery Against Deep Neural Network Fault Injection Attacks Based on Contrastive Learning. (arXiv:2401.16766v1 [cs.LG])

    [http://arxiv.org/abs/2401.16766](http://arxiv.org/abs/2401.16766)

    本文提出了一个基于对比学习的深度神经网络故障注入攻击检测和恢复框架（CFDR），通过将对比学习应用于训练和推理流程中，实现了具有自适应能力的深度神经网络推理引擎，在只有一个批次的测试数据和少量无标签测试数据的情况下，能够实时检测并快速恢复多种类型的故障注入攻击。

    

    深度神经网络在执行设备上作为推理引擎实施时容易受到故障注入攻击，这些攻击操纵模型参数以破坏推理执行的性能。本文将对比学习应用于深度学习的训练和推理流程中，以实现具有自适应能力的深度神经网络推理引擎，以应对故障注入攻击。我们提出的基于对比学习的故障注入攻击检测和恢复（CFDR）框架具有以下特点：（i）仅需一个批次的测试数据进行实时检测，（ii）即使仅有少量无标签测试数据，也能实现快速的恢复效果。在CIFAR-10数据集上对多种类型的故障注入攻击进行评估，我们的CFDR展现出了良好的检测和恢复效果。

    Deep Neural Network (DNN) models when implemented on executing devices as the inference engines are susceptible to Fault Injection Attacks (FIAs) that manipulate model parameters to disrupt inference execution with disastrous performance. This work introduces Contrastive Learning (CL) of visual representations i.e., a self-supervised learning approach into the deep learning training and inference pipeline to implement DNN inference engines with self-resilience under FIAs. Our proposed CL based FIA Detection and Recovery (CFDR) framework features (i) real-time detection with only a single batch of testing data and (ii) fast recovery effective even with only a small amount of unlabeled testing data. Evaluated with the CIFAR-10 dataset on multiple types of FIAs, our CFDR shows promising detection and recovery effectiveness.
    
[^52]: 一步向前和回溯：克服损失感知量化训练中的曲线行进问题

    One-Step Forward and Backtrack: Overcoming Zig-Zagging in Loss-Aware Quantization Training. (arXiv:2401.16760v1 [cs.LG])

    [http://arxiv.org/abs/2401.16760](http://arxiv.org/abs/2401.16760)

    本文提出了一种新的损失感知量化方法，通过一步向前搜索和回溯的方式解决了梯度下降中出现的曲线行进问题。

    

    权重量化是一种有效的方法，用于在资源有限的边缘设备上部署深度神经网络。传统的损失感知量化方法常常使用量化梯度来替代全精度梯度。然而，我们发现梯度误差会导致梯度下降学习过程中出现意想不到的曲线行进问题，其中梯度方向迅速振荡或曲线行进，这个问题严重减慢了模型的收敛速度。因此，本文提出了一种一步向前和回溯的损失感知量化方法，以获得更准确稳定的梯度方向来克服此问题。在梯度下降学习过程中，设计了一步向前搜索来寻找下一步的试验梯度，该梯度被用来调整当前步骤的梯度，朝着快速收敛的方向调整。之后，我们回溯当前步骤，更新全精度和量化权重。

    Weight quantization is an effective technique to compress deep neural networks for their deployment on edge devices with limited resources. Traditional loss-aware quantization methods commonly use the quantized gradient to replace the full-precision gradient. However, we discover that the gradient error will lead to an unexpected zig-zagging-like issue in the gradient descent learning procedures, where the gradient directions rapidly oscillate or zig-zag, and such issue seriously slows down the model convergence. Accordingly, this paper proposes a one-step forward and backtrack way for loss-aware quantization to get more accurate and stable gradient direction to defy this issue. During the gradient descent learning, a one-step forward search is designed to find the trial gradient of the next-step, which is adopted to adjust the gradient of current step towards the direction of fast convergence. After that, we backtrack the current step to update the full-precision and quantized weights
    
[^53]: SwapNet: 超出内存预算的边缘AI设备上进行DNN推理的高效交换技术

    SwapNet: Efficient Swapping for DNN Inference on Edge AI Devices Beyond the Memory Budget. (arXiv:2401.16757v1 [cs.LG])

    [http://arxiv.org/abs/2401.16757](http://arxiv.org/abs/2401.16757)

    SwapNet是一种高效的边缘AI设备DNN块交换中间件，在超出内存预算的情况下，通过分解DNN为块并进行交换，实现了大型DNN的高效执行。

    

    在边缘人工智能（AI）设备上执行深度神经网络（DNN）可以实现各种自主移动计算应用。然而，边缘AI设备的内存预算限制了这些应用中允许的DNN数量和复杂性。现有的解决方案，如模型压缩或云卸载，减少了DNN推理的内存占用，但同时也降低了模型准确度或自主性。为了避免这些缺点，我们将DNN分解成块，并按顺序互相交换，以便在较小的内存预算下执行大型DNN。然而，在边缘AI设备上进行简单交换会引起显著的延迟，因为在边缘AI设备的DNN开发生态系统中存在冗余的内存操作。为此，我们开发了SwapNet，一种高效的边缘AI设备DNN块交换中间件。我们系统地消除了块交换过程中不必要的内存操作，同时保持与深度学习框架和GPU后端的兼容性。

    Executing deep neural networks (DNNs) on edge artificial intelligence (AI) devices enables various autonomous mobile computing applications. However, the memory budget of edge AI devices restricts the number and complexity of DNNs allowed in such applications. Existing solutions, such as model compression or cloud offloading, reduce the memory footprint of DNN inference at the cost of decreased model accuracy or autonomy. To avoid these drawbacks, we divide DNN into blocks and swap them in and out in order, such that large DNNs can execute within a small memory budget. Nevertheless, naive swapping on edge AI devices induces significant delays due to the redundant memory operations in the DNN development ecosystem for edge AI devices. To this end, we develop SwapNet, an efficient DNN block swapping middleware for edge AI devices. We systematically eliminate the unnecessary memory operations during block swapping while retaining compatible with the deep learning frameworks, GPU backends,
    
[^54]: 关系推理的扩散模型

    Diffusion model for relational inference. (arXiv:2401.16755v1 [cs.LG])

    [http://arxiv.org/abs/2401.16755](http://arxiv.org/abs/2401.16755)

    这项研究提出了一种关系推理的扩散模型(DiffRI)，通过条件扩散建模学习推断组件之间连接存在的概率，并在无监督方式下发现地面真实相互作用方面具有很高的能力。

    

    复杂相互作用系统的动态行为，包括大脑活动、金融价格波动和物理集体现象，与系统组成部分之间的相互作用相关。利用可观测的动态来发现这些系统中的相互作用关系被称为关系推理。在本研究中，我们提出了一种关系推理的扩散模型(DiffRI)，它借鉴了一种自监督的概率时间序列插值方法。DiffRI通过条件扩散建模学习推断组件之间连接存在的概率。对于模拟和准真实数据集的实验证明，DiffRI在无监督方式下发现地面真实相互作用方面与其他最先进的模型相比具有很高的能力。我们的代码将很快公开。

    Dynamical behaviors of complex interacting systems, including brain activities, financial price movements, and physical collective phenomena, are associated with underlying interactions between the system's components. The issue of uncovering interaction relations in such systems using observable dynamics is called relational inference. In this study, we propose a Diffusion model for Relational Inference (DiffRI), inspired by a self-supervised method for probabilistic time series imputation. DiffRI learns to infer the probability of the presence of connections between components through conditional diffusion modeling. Experiments on both simulated and quasi-real datasets show that DiffRI is highly competent compared with other state-of-the-art models in discovering ground truth interactions in an unsupervised manner. Our code will be made public soon.
    
[^55]: AI监督和人类错误：来自中心法庭的证据

    AI Oversight and Human Mistakes: Evidence from Centre Court. (arXiv:2401.16754v1 [cs.LG])

    [http://arxiv.org/abs/2401.16754](http://arxiv.org/abs/2401.16754)

    人工智能系统在纠正人类错误方面起到了积极作用，但此举也潜在导致心理成本，并影响人的决策。通过研究网球比赛中的Hawk-Eye审查系统，我们发现引入AI监督后，裁判员的错误率下降，心理成本导致他们更倾向于将球判为进界，从而产生了类型错判的转变。

    

    在机器学习算法不断提升的驱动下，人工智能（AI）系统已经开始在许多场合用于纠正人类错误。我们提供了首个实地证据，证明这种AI监督会产生心理成本，影响人的决策。我们调查了AI监督发生的最高可见性场景之一：顶级网球比赛中裁判的Hawk-Eye审查。我们发现，引入Hawk-Eye审查后，裁判的整体错误率降低，符合心理成本被AI否定的合理忽视现象。我们还发现，裁判增加了对球入内的判定率，从而产生了从II类错误（将球判为出界，实际上是进界）到I类错误（将球判为进界，实际上是出界）的转变。通过对理性不注意的裁判模型进行心理成本的结构估计，我们的结果表明，由于AI否定的心理成本，裁判员降低了错误判定的风险并提高了球入内的判定率。

    Powered by the increasing predictive capabilities of machine learning algorithms, artificial intelligence (AI) systems have begun to be used to overrule human mistakes in many settings. We provide the first field evidence this AI oversight carries psychological costs that can impact human decision-making. We investigate one of the highest visibility settings in which AI oversight has occurred: the Hawk-Eye review of umpires in top tennis tournaments. We find that umpires lowered their overall mistake rate after the introduction of Hawk-Eye review, in line with rational inattention given psychological costs of being overruled by AI. We also find that umpires increased the rate at which they called balls in, which produced a shift from making Type II errors (calling a ball out when in) to Type I errors (calling a ball in when out). We structurally estimate the psychological costs of being overruled by AI using a model of rational inattentive umpires, and our results suggest that because 
    
[^56]: 基于生成型人工智能的闭环fMRI系统

    Generative AI-based closed-loop fMRI system. (arXiv:2401.16742v1 [cs.HC])

    [http://arxiv.org/abs/2401.16742](http://arxiv.org/abs/2401.16742)

    DecNefGAN是一种基于生成型人工智能的闭环fMRI系统，通过结合生成对抗系统和神经强化模型，研究人类如何对抗和应对生成型人工智能的潜在影响。

    

    虽然生成型人工智能现在在社会中广泛应用，并且具有很大的用处，但是存在潜在的滥用风险，例如，无意识地影响认知过程或决策。尽管这在认知领域中引起了安全问题，但至今没有关于在人类身上对抗恶意生成型人工智能影响的神经和计算机机制的研究。我们提出了DecNefGAN，一种结合了生成对抗系统和神经强化模型的新框架。具体而言，DecNefGAN在一个闭环系统中连接了人类和生成型人工智能，其中人工智能创建诱发特定心理状态的刺激，从而对神经活动施加外部控制。人类的目标相反，要竞争并达到一个正交的心理状态。这个框架可以有助于阐明人脑如何对抗和应对生成型人工智能的潜在影响。

    While generative AI is now widespread and useful in society, there are potential risks of misuse, e.g., unconsciously influencing cognitive processes or decision-making. Although this causes a security problem in the cognitive domain, there has been no research about neural and computational mechanisms counteracting the impact of malicious generative AI in humans. We propose DecNefGAN, a novel framework that combines a generative adversarial system and a neural reinforcement model. More specifically, DecNefGAN bridges human and generative AI in a closed-loop system, with the AI creating stimuli that induce specific mental states, thus exerting external control over neural activity. The objective of the human is the opposite, to compete and reach an orthogonal mental state. This framework can contribute to elucidating how the human brain responds to and counteracts the potential influence of generative AI.
    
[^57]: 从零开始构建一个大型语言模型

    Engineering A Large Language Model From Scratch. (arXiv:2401.16736v1 [cs.CL])

    [http://arxiv.org/abs/2401.16736](http://arxiv.org/abs/2401.16736)

    Atinuke是一种基于Transformer的神经网络，通过在处理时序数据的层与注意机制交织在一起，模拟人类语言，从而优化各种语言任务的性能。

    

    深度学习在自然语言处理（NLP）领域的普及导致了能够理解和生成人类语言的创新技术的开发和发布。Atinuke是一种基于Transformer的神经网络，通过利用独特的配置，在各种语言任务上优化性能。该架构通过将处理时序数据的层与注意机制交织在一起，从而在输入和输出之间建立有意义的关联。由于其拓扑结构和超参数调整的配置，它可以提取特征并学习复杂的映射，从而模仿人类语言。Atinuke是模块化、可扩展的，并可以与现有的机器学习流程无缝集成。softmax、嵌入和多头注意力等高级矩阵操作使得对文本、声音和视觉信号的细致处理成为可能。通过将现代深度学习技术与软件设计原则和数学方法相结合

    The proliferation of deep learning in natural language processing (NLP) has led to the development and release of innovative technologies capable of understanding and generating human language with remarkable proficiency. Atinuke, a Transformer-based neural network, optimises performance across various language tasks by utilising a unique configuration. The architecture interweaves layers for processing sequential data with attention mechanisms to draw meaningful affinities between inputs and outputs. Due to the configuration of its topology and hyperparameter tuning, it can emulate human-like language by extracting features and learning complex mappings. Atinuke is modular, extensible, and integrates seamlessly with existing machine learning pipelines. Advanced matrix operations like softmax, embeddings, and multi-head attention enable nuanced handling of textual, acoustic, and visual signals. By unifying modern deep learning techniques with software design principles and mathematical
    
[^58]: 广义线性匹配滤波器：实现复杂值CNN可解释性之关键

    Widely Linear Matched Filter: A Lynchpin towards the Interpretability of Complex-valued CNNs. (arXiv:2401.16729v1 [cs.LG])

    [http://arxiv.org/abs/2401.16729](http://arxiv.org/abs/2401.16729)

    本研究提出了广义线性匹配滤波器（WLMF）范例来实现复杂值CNN的可解释性，解决了在复杂值数据中匹配滤波的难题，并分析了其性能。与标准线性对应物（SLMF）相比，WLMF在输出信噪比方面具有理论上的优势。

    

    最近关于实值卷积神经网络（CNNs）可解释性的研究揭示了通过匹配滤波器在数据中找到特征的任务与其直接和有物理含义的联系。然而，将这一范式应用于揭示复杂值CNNs可解释性遇到了一个巨大的障碍：广义非循环复杂值数据的匹配滤波器扩展，即广义线性匹配滤波器（WLMF），在文献中仅仅是隐含的。为了实现复杂值CNNs操作的可解释性，我们引入了一种广义WLMF范例，提供了解决方案并对其性能进行了分析。为了保证严谨性，我们的WLMF解决方案不对噪声的概率密度做任何假设。WLMF在输出信噪比方面相对于其标准严格线性对应物（SLMF）的理论优势被提供。

    A recent study on the interpretability of real-valued convolutional neural networks (CNNs) \cite{Stankovic_Mandic_2023CNN} has revealed a direct and physically meaningful link with the task of finding features in data through matched filters. However, applying this paradigm to illuminate the interpretability of complex-valued CNNs meets a formidable obstacle: the extension of matched filtering to a general class of noncircular complex-valued data, referred to here as the widely linear matched filter (WLMF), has been only implicit in the literature. To this end, to establish the interpretability of the operation of complex-valued CNNs, we introduce a general WLMF paradigm, provide its solution and undertake analysis of its performance. For rigor, our WLMF solution is derived without imposing any assumption on the probability density of noise. The theoretical advantages of the WLMF over its standard strictly linear counterpart (SLMF) are provided in terms of their output signal-to-noise-
    
[^59]: SmartFRZ:一种使用基于注意力的层冻结的高效训练框架

    SmartFRZ: An Efficient Training Framework using Attention-Based Layer Freezing. (arXiv:2401.16720v1 [cs.LG])

    [http://arxiv.org/abs/2401.16720](http://arxiv.org/abs/2401.16720)

    SmartFRZ是一种通用而智能的层冻结方法，能够自动执行“现场”层冻结，在提高训练效率的同时兼顾通用性和准确性。

    

    人工智能应用正不断增加，模型训练对于提供高质量服务至关重要。然而，模型训练过程既耗时又耗能，不可避免地影响到用户对应用效率的需求。层冻结作为一种高效的模型训练技术被提出来提高训练效率。虽然现有的层冻结方法展示了减少模型训练成本的巨大潜力，但它们仍然存在缺点，如缺乏通用性和降低准确性。例如，现有的层冻结方法要么需要在训练之前手动定义冻结配置，这不适用于不同的网络，要么使用启发式的冻结标准，在不同场景下很难保证准确性。因此，缺乏一种能够自动执行“现场”层冻结的通用而智能的方法。

    There has been a proliferation of artificial intelligence applications, where model training is key to promising high-quality services for these applications. However, the model training process is both time-intensive and energy-intensive, inevitably affecting the user's demand for application efficiency. Layer freezing, an efficient model training technique, has been proposed to improve training efficiency. Although existing layer freezing methods demonstrate the great potential to reduce model training costs, they still remain shortcomings such as lacking generalizability and compromised accuracy. For instance, existing layer freezing methods either require the freeze configurations to be manually defined before training, which does not apply to different networks, or use heuristic freezing criteria that is hard to guarantee decent accuracy in different scenarios. Therefore, there lacks a generic and smart layer freezing method that can automatically perform ``in-situation'' layer fr
    
[^60]: OptiState：基于门控网络、Transformer视觉和卡尔曼滤波的腿式机器人状态估计

    OptiState: State Estimation of Legged Robots using Gated Networks with Transformer-based Vision and Kalman Filtering. (arXiv:2401.16719v1 [cs.RO])

    [http://arxiv.org/abs/2401.16719](http://arxiv.org/abs/2401.16719)

    本研究提出了一种名为OptiState的腿式机器人状态估计方法，该方法通过整合Kalman滤波、优化和学习模式的混合解决方案，结合本体感和外感信息，以精确估计机器人主体的状态。该方法利用关节编码器、IMU测量和基于凸规划的模型预测控制优化，通过Gate循环单元和Vision Transformer自编码器改进了估计结果。研究结果表明，该方法能够提供准确的机器人状态估计，并减小传感器测量和模型简化引起的非线性误差。

    

    由于腿式机器人的高动态运动和传感器精度的局限性，腿式机器人的状态估计具有挑战性。通过整合卡尔曼滤波、优化和基于学习的模态，我们提出了一种混合解决方案，结合了本体感和外感信息，用于估计机器人主体的状态。借助关节编码器和IMU测量，我们的卡尔曼滤波器通过单刚体模型进行增强，该模型还结合了基于凸规划的模型预测控制优化的接地反力控制输出。通过门控循环单元进一步改进估计结果，该方法还考虑了从深度图像上应用视觉Transformer自编码器获得的语义洞察和机器人高度。该框架不仅提供准确的机器人状态估计，包括不确定性评估，还可以通过学习来减小传感器测量和模型简化引起的非线性误差。所提出的方法经过评估。

    State estimation for legged robots is challenging due to their highly dynamic motion and limitations imposed by sensor accuracy. By integrating Kalman filtering, optimization, and learning-based modalities, we propose a hybrid solution that combines proprioception and exteroceptive information for estimating the state of the robot's trunk. Leveraging joint encoder and IMU measurements, our Kalman filter is enhanced through a single-rigid body model that incorporates ground reaction force control outputs from convex Model Predictive Control optimization. The estimation is further refined through Gated Recurrent Units, which also considers semantic insights and robot height from a Vision Transformer autoencoder applied on depth images. This framework not only furnishes accurate robot state estimates, including uncertainty evaluations, but can minimize the nonlinear errors that arise from sensor measurements and model simplifications through learning. The proposed methodology is evaluated
    
[^61]: 多元贝塔混合模型：具有灵活聚类形状的概率聚类方法

    Multivariate Beta Mixture Model: Probabilistic Clustering With Flexible Cluster Shapes. (arXiv:2401.16708v1 [cs.LG])

    [http://arxiv.org/abs/2401.16708](http://arxiv.org/abs/2401.16708)

    本文提出了一种名为多元贝塔混合模型（MBMM）的新的概率模型，用于软聚类。MBMM通过其灵活的多元贝塔分布的概率密度函数适应不同的聚类形状，并在合成和真实数据集上展示了其适应性。

    

    本文介绍了多元贝塔混合模型（MBMM），这是一种新的概率模型用于软聚类。MBMM通过多元贝塔分布的灵活概率密度函数适应不同的聚类形状。我们介绍了MBMM的属性，描述了参数学习过程，并展示了在合成和真实数据集上适合各种聚类形状的实验结果。代码匿名发布在\url{https://github.com/hhchen1105/mbmm/}上。

    This paper introduces the multivariate beta mixture model (MBMM), a new probabilistic model for soft clustering. MBMM adapts to diverse cluster shapes because of the flexible probability density function of the multivariate beta distribution. We introduce the properties of MBMM, describe the parameter learning procedure, and present the experimental results, showing that MBMM fits diverse cluster shapes on synthetic and real datasets. The code is released anonymously at \url{https://github.com/hhchen1105/mbmm/}.
    
[^62]: EdgeOL: 边缘设备上高效的原位在线学习

    EdgeOL: Efficient in-situ Online Learning on Edge Devices. (arXiv:2401.16694v1 [cs.LG])

    [http://arxiv.org/abs/2401.16694](http://arxiv.org/abs/2401.16694)

    本文提出了EdgeOL，一种边缘在线学习框架，通过内部和外部调优来优化推理准确性、微调执行时间和能量效率，在边缘设备上实现了显著的性能提升。

    

    新兴应用，如机器人辅助养老和物体识别，通常采用深度学习神经网络模型，并且自然需要：i) 处理实时推理请求和ii) 适应可能的部署场景变化。在线模型微调被广泛采用以满足这些需求。然而，微调会导致显著的能量消耗，使其难以部署在边缘设备上。在本文中，我们提出了EdgeOL，一种边缘在线学习框架，通过内部和外部调优来优化推理准确性、微调执行时间和能量效率。实验结果显示，EdgeOL平均减少了82%的微调执行时间，74%的能量消耗，并提高了平均推理准确率1.70%，相对于即时在线学习策略。

    Emerging applications, such as robot-assisted eldercare and object recognition, generally employ deep learning neural networks (DNNs) models and naturally require: i) handling streaming-in inference requests and ii) adapting to possible deployment scenario changes. Online model fine-tuning is widely adopted to satisfy these needs. However, fine-tuning involves significant energy consumption, making it challenging to deploy on edge devices. In this paper, we propose EdgeOL, an edge online learning framework that optimizes inference accuracy, fine-tuning execution time, and energy efficiency through both inter-tuning and intra-tuning optimizations. Experimental results show that, on average, EdgeOL reduces overall fine-tuning execution time by 82%, energy consumption by 74%, and improves average inference accuracy by 1.70% over the immediate online learning strategy.
    
[^63]: 预校准和计算：深度点击率预测模型中一种方差减少的度量框架

    Calibration-then-Calculation: A Variance Reduced Metric Framework in Deep Click-Through Rate Prediction Models. (arXiv:2401.16692v1 [cs.LG])

    [http://arxiv.org/abs/2401.16692](http://arxiv.org/abs/2401.16692)

    本文提出一种新的度量框架，通过减少方差来改进深度学习流水线的性能评估，具有更高的准确性来检测有效建模改进。

    

    深度学习已经在各个领域得到广泛应用，但对于深度学习流水线的性能评估关注较少。随着大型数据集和复杂模型的使用增加，通常只运行一次训练过程并与之前的基准进行比较。然而，由于神经网络评估指标的方差，这种过程可能导致不精确的比较。指标方差来自深度学习流水线训练过程中固有的随机性。传统解决方案如多次运行训练过程在深度学习中往往不可行，因为计算限制。在本文中，我们提出了一种新的度量框架，称为校准损失度量，通过减少其基准模型中的方差来解决这个问题。结果，这个新的度量方法具有更高的准确性来检测有效建模改进。我们的方法得到了理论上的证明和实验证明。

    Deep learning has been widely adopted across various fields, but there has been little focus on evaluating the performance of deep learning pipelines. With the increased use of large datasets and complex models, it has become common to run the training process only once and compare the result to previous benchmarks. However, this procedure can lead to imprecise comparisons due to the variance in neural network evaluation metrics. The metric variance comes from the randomness inherent in the training process of deep learning pipelines. Traditional solutions such as running the training process multiple times are usually not feasible in deep learning due to computational limitations. In this paper, we propose a new metric framework, Calibrated Loss Metric, that addresses this issue by reducing the variance in its vanilla counterpart. As a result, the new metric has a higher accuracy to detect effective modeling improvement. Our approach is supported by theoretical justifications and exte
    
[^64]: 重新审视梯度剪枝：一种用于抵御梯度攻击的双重实现

    Revisiting Gradient Pruning: A Dual Realization for Defending against Gradient Attacks. (arXiv:2401.16687v1 [cs.CR])

    [http://arxiv.org/abs/2401.16687](http://arxiv.org/abs/2401.16687)

    提出了一种基于梯度剪枝的防御方法，双重梯度剪枝（DGP），可以提高协作学习的通信效率同时保护隐私。

    

    协作学习是一种分布式学习框架，通过共享梯度更新训练模型，旨在保护用户隐私。然而，梯度反演攻击可以从共享的梯度中恢复用户的训练数据，对协作学习构成严重的隐私威胁。现有的防御方法采用不同的技术，如差分隐私、密码学和扰动防御，以抵御梯度反演攻击。然而，所有的现有防御方法在隐私、效用和效率之间存在较差的平衡。为了缓解现有解决方案的弱点，我们提出了一种新颖的防御方法，即双重梯度剪枝（DGP），基于梯度剪枝，它可以提高通信效率同时保持协作学习的效用和隐私。具体而言，DGP稍微改变了梯度剪枝，并提供了更强的隐私保证。同时，DGP还可以通过理论分析显著提高通信效率。

    Collaborative learning (CL) is a distributed learning framework that aims to protect user privacy by allowing users to jointly train a model by sharing their gradient updates only. However, gradient inversion attacks (GIAs), which recover users' training data from shared gradients, impose severe privacy threats to CL. Existing defense methods adopt different techniques, e.g., differential privacy, cryptography, and perturbation defenses, to defend against the GIAs. Nevertheless, all current defense methods suffer from a poor trade-off between privacy, utility, and efficiency. To mitigate the weaknesses of existing solutions, we propose a novel defense method, Dual Gradient Pruning (DGP), based on gradient pruning, which can improve communication efficiency while preserving the utility and privacy of CL. Specifically, DGP slightly changes gradient pruning with a stronger privacy guarantee. And DGP can also significantly improve communication efficiency with a theoretical analysis of its
    
[^65]: 通信高效的多模态联邦学习：联合模态和客户选择

    Communication-Efficient Multimodal Federated Learning: Joint Modality and Client Selection. (arXiv:2401.16685v1 [cs.LG])

    [http://arxiv.org/abs/2401.16685](http://arxiv.org/abs/2401.16685)

    本文提出了一种新的多模态联邦学习方法，通过联合模态和客户选择来解决多样的模态集合和通信限制的挑战。

    

    多模态联邦学习旨在丰富在客户端收集多模态测量的联邦学习环境中的模型训练。然而，多模态联邦学习面临一些尚未解决的关键挑战，特别是在异构网络环境中：(i)每个客户端收集的模态集合将是多样的，(ii)通信限制阻止客户端将其所有本地训练的模态模型上传到服务器。在本文中，我们提出了多模态联邦学习与联合模态和客户选择(mmFedMC)，一种新的联邦学习方法，可以解决多模态环境中的上述挑战。联合选择算法包含两个主要组成部分：(a)为每个客户端设计的模态选择方法，根据Shapley值分析评估模态的影响，根据通信开销的模态模型大小，结合模态模型更新频率（称为最近更新）作为权重，以增强模态选择的效果。

    Multimodal federated learning (FL) aims to enrich model training in FL settings where clients are collecting measurements across multiple modalities. However, key challenges to multimodal FL remain unaddressed, particularly in heterogeneous network settings where: (i) the set of modalities collected by each client will be diverse, and (ii) communication limitations prevent clients from uploading all their locally trained modality models to the server. In this paper, we propose multimodal Federated learning with joint Modality and Client selection (mmFedMC), a new FL methodology that can tackle the above-mentioned challenges in multimodal settings. The joint selection algorithm incorporates two main components: (a) A modality selection methodology for each client, which weighs (i) the impact of the modality, gauged by Shapley value analysis, (ii) the modality model size as a gauge of communication overhead, against (iii) the frequency of modality model updates, denoted recency, to enhan
    
[^66]: 在主测地Grassmannian子流形上的多项式混沌展开用于代理建模和不确定性量化

    Polynomial Chaos Expansions on Principal Geodesic Grassmannian Submanifolds for Surrogate Modeling and Uncertainty Quantification. (arXiv:2401.16683v1 [stat.ML])

    [http://arxiv.org/abs/2401.16683](http://arxiv.org/abs/2401.16683)

    本论文提出了一种基于流形学习的代理建模框架，用于高维随机系统中的不确定性量化。通过在Grassmann流形上进行主测地分析，识别出一组潜在的低维描述符，然后利用多项式混沌展开构建映射。

    

    在这项工作中，我们介绍了一种基于流形学习的代理建模框架，用于高维随机系统中的不确定性量化。我们的首要目标是对可用的模拟数据进行数据挖掘，以确定能够高效参数化高维计算模型响应的一组低维（潜在）描述符。为此，我们采用Grassmann流形上的主测地分析，识别出一组可能具有不同维度的不相交主测地子流形，以捕捉数据的变化。由于Grassmann上的操作需要数据集中，我们提出了一种基于Riemanniann K-means和Grassmann流形上样本Frechet方差最小化的自适应算法，用于识别代表参数空间中不同系统行为的“本地”主测地子流形。然后使用多项式混沌展开构建映射

    In this work we introduce a manifold learning-based surrogate modeling framework for uncertainty quantification in high-dimensional stochastic systems. Our first goal is to perform data mining on the available simulation data to identify a set of low-dimensional (latent) descriptors that efficiently parameterize the response of the high-dimensional computational model. To this end, we employ Principal Geodesic Analysis on the Grassmann manifold of the response to identify a set of disjoint principal geodesic submanifolds, of possibly different dimension, that captures the variation in the data. Since operations on the Grassmann require the data to be concentrated, we propose an adaptive algorithm based on Riemanniann K-means and the minimization of the sample Frechet variance on the Grassmann manifold to identify "local" principal geodesic submanifolds that represent different system behavior across the parameter space. Polynomial chaos expansion is then used to construct a mapping bet
    
[^67]: 虚构话语的检测与理解

    The Detection and Understanding of Fictional Discourse. (arXiv:2401.16678v1 [cs.CL])

    [http://arxiv.org/abs/2401.16678](http://arxiv.org/abs/2401.16678)

    本文介绍了虚构话语检测的分类实验，利用了多样的数据集和新的特征集。虚构话语的检测有助于丰富大型文化遗产档案，并帮助更广泛地理解虚构叙事的特质。

    

    本文介绍了与虚构话语检测任务相关的分类实验。我们利用了各种各样的数据集，包括当代专业出版的小说、来自Hathi Trust的历史小说、粉丝所写文、来自Reddit的故事、民间传说、GPT生成的故事以及英语世界文学作品。此外，我们还引入了一组新的“超词义”特征集，以促进语义概括的目标。虚构话语的检测可以帮助丰富我们对大型文化遗产档案的了解，并有助于更广泛地理解虚构叙事的独特特质。

    In this paper, we present a variety of classification experiments related to the task of fictional discourse detection. We utilize a diverse array of datasets, including contemporary professionally published fiction, historical fiction from the Hathi Trust, fanfiction, stories from Reddit, folk tales, GPT-generated stories, and anglophone world literature. Additionally, we introduce a new feature set of word "supersenses" that facilitate the goal of semantic generalization. The detection of fictional discourse can help enrich our knowledge of large cultural heritage archives and assist with the process of understanding the distinctive qualities of fictional storytelling more broadly.
    
[^68]: T3：面向计算和集合细粒度重叠的透明跟踪和触发

    T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives. (arXiv:2401.16677v1 [cs.AR])

    [http://arxiv.org/abs/2401.16677](http://arxiv.org/abs/2401.16677)

    T3提出了一种透明跟踪和触发的硬件-软件协同设计方法，用于解决大型语言模型在分布式训练和推理中的通信效率问题。

    

    大型语言模型越来越多地依赖于分布式技术进行训练和推理。这些技术需要设备之间的通信，随着设备数量的增加，通信会降低扩展效率。虽然一些分布式技术可以重叠并隐藏这种通信与独立计算，但张量并行技术(TP)本质上将通信与模型执行串行化。隐藏这种串行通信的一种方法是以细粒度的方式将其与生产操作(通信数据)交替进行。然而，软件中的这种细粒度通信和计算交错可能很困难。此外，与任何并发执行一样，它需要计算和内存资源在计算和通信之间共享，导致资源争用减少重叠效果。为了克服这些挑战，我们提出了T3，它应用硬件-软件协同设计来实现透明的跟踪和触发。

    Large Language Models increasingly rely on distributed techniques for their training and inference. These techniques require communication across devices which can reduce scaling efficiency as the number of devices increases. While some distributed techniques can overlap, and thus, hide this communication with independent computations, techniques such as Tensor Parallelism (TP) inherently serialize communication with model execution. One approach to hide this serialized communication is to interleave it with the producer operation (of the communicated data) in a fine-grained manner. However, this fine-grained interleaving of communication and computation in software can be difficult. Furthermore, as with any concurrent execution, it requires compute and memory resources to be shared between computation and communication, causing resource contention that reduces overlapping efficacy.  To overcome these challenges, we propose T3 which applies hardware-software co-design to transparently 
    
[^69]: 人工智能是否为天气预报带来了第二次革命？

    Is Artificial Intelligence Providing the Second Revolution for Weather Forecasting?. (arXiv:2401.16669v1 [cs.LG])

    [http://arxiv.org/abs/2401.16669](http://arxiv.org/abs/2401.16669)

    人工智能技术在天气预报领域的快速发展代表了一个重大突破，它克服了传统模型的局限性，有潜力引领天气预报的第二次革命。

    

    人工智能技术的快速发展，特别是近年来，导致了几种大参数人工智能天气预报模型的出现。这些模型代表了一个重大突破，克服了传统数值天气预报模型的局限性，并表明了天气预报可能迎来第二次革命的潜力。本研究探讨了这些先进人工智能预报模型的演变，并在确定的共同点的基础上，提出了它们的发展的“三大规则”。我们讨论了人工智能在革命数值天气预报中的潜力，并简要概述了潜在的原因。此外，我们还探讨了大型人工智能天气预报模型未来发展前景的关键领域，将整个数值预报过程进行整合。通过将大型人工智能模型与其他信息综合，给出了一个应用实例。

    The rapid advancement of artificial intelligence technologies, particularly in recent years, has led to the emergence of several large parameter artificial intelligence weather forecast models. These models represent a significant breakthrough, overcoming the limitations of traditional numerical weather prediction models and indicating a potential second revolution for weather forecast. This study explores the evolution of these advanced artificial intelligence forecast models, and based on the identified commonalities, proposes the "Three Large Rules" for their development. We discuss the potential of artificial intelligence in revolutionizing numerical weather prediction, briefly outlining the underlying reasons for this potential. Additionally, we explore key areas for future development prospects for large artificial intelligence weather forecast models, integrating the entire numerical prediction process. Through an example that combines a large artificial intelligence model with 
    
[^70]: 快速双正则自编码器用于稀疏生物数据

    Fast Dual-Regularized Autoencoder for Sparse Biological Data. (arXiv:2401.16664v1 [cs.LG])

    [http://arxiv.org/abs/2401.16664](http://arxiv.org/abs/2401.16664)

    本文提出了一种快速双正则自编码器用于稀疏生物数据的问题。该方法相对于现有最先进方法在预测药物靶点相互作用和药物疾病关联方面具有速度和准确性的优势。

    

    从稀疏数据中推断关系是一个重要的任务，应用范围从产品推荐到药物发现。最近提出的稀疏矩阵补全的线性模型在速度和准确性上相对于更复杂的推荐系统算法具有惊人的优势。在这里，我们扩展了线性模型，开发了一个浅层自编码器来解决双邻域正则化矩阵补全问题。我们证明了我们的方法在预测药物靶点相互作用和药物疾病关联方面相对于现有最先进方法的速度和准确性优势。

    Relationship inference from sparse data is an important task with applications ranging from product recommendation to drug discovery. A recently proposed linear model for sparse matrix completion has demonstrated surprising advantage in speed and accuracy over more sophisticated recommender systems algorithms. Here we extend the linear model to develop a shallow autoencoder for the dual neighborhood-regularized matrix completion problem. We demonstrate the speed and accuracy advantage of our approach over the existing state-of-the-art in predicting drug-target interactions and drug-disease associations.
    
[^71]: 允许混淆的LiNGAM的泛化

    Generalization of LiNGAM that allows confounding. (arXiv:2401.16661v1 [cs.LG])

    [http://arxiv.org/abs/2401.16661](http://arxiv.org/abs/2401.16661)

    本文提出了一种名为LiNGAM-MMI的方法，可以增强LiNGAM模型以处理混淆问题。该方法使用KL散度量化混淆程度，并通过最短路径问题解决方案高效地确定变量顺序，不论是否存在混淆情况。实验证明，LiNGAM-MMI可以更准确地识别正确的变量顺序。

    

    LiNGAM使用加性噪声模型来确定因果关系的变量顺序，但在混淆方面面临挑战。先前的方法在保持LiNGAM的基本结构的同时，试图识别和处理受混淆影响的变量。结果是，不论是否存在混淆，这些方法都需要大量的计算资源，并且不能确保检测到所有的混淆类型。相比之下，本文通过引入LiNGAM-MMI对LiNGAM进行了增强，该方法使用KL散度量化混淆程度，并安排变量以最小化其影响。该方法通过最短路径问题的形式高效地实现全局最优的变量顺序。在无混淆的情况下，LiNGAM-MMI的处理数据效率与传统LiNGAM相当，同时有效处理混淆情况。我们的实验结果表明，LiNGAM-MMI更准确地确定了正确的变量顺序...

    LiNGAM determines the variable order from cause to effect using additive noise models, but it faces challenges with confounding. Previous methods maintained LiNGAM's fundamental structure while trying to identify and address variables affected by confounding. As a result, these methods required significant computational resources regardless of the presence of confounding, and they did not ensure the detection of all confounding types. In contrast, this paper enhances LiNGAM by introducing LiNGAM-MMI, a method that quantifies the magnitude of confounding using KL divergence and arranges the variables to minimize its impact. This method efficiently achieves a globally optimal variable order through the shortest path problem formulation. LiNGAM-MMI processes data as efficiently as traditional LiNGAM in scenarios without confounding while effectively addressing confounding situations. Our experimental results suggest that LiNGAM-MMI more accurately determines the correct variable order, bo
    
[^72]: 通过Chen-Fliess序列，我们展示了如何将连续深度神经ODE模型构建为单层、无限宽度的网络。

    Rademacher Complexity of Neural ODEs via Chen-Fliess Series. (arXiv:2401.16655v1 [stat.ML])

    [http://arxiv.org/abs/2401.16655](http://arxiv.org/abs/2401.16655)

    本文通过Chen-Fliess序列展开将连续深度神经ODE模型转化为单层、无限宽度的网络，并利用此框架推导出了将初始条件映射到某个终端时间的ODE模型的Rademacher复杂度的紧凑表达式。

    

    本文将连续深度神经ODE模型使用Chen-Fliess序列展开为单层、无限宽度的网络。在这个网络中，输出的“权重”来自控制输入的特征序列，它由控制输入在单纯形上的迭代积分构成。而“特征”则基于受控ODE模型中输出函数相对于向量场的迭代李导数。本文的主要结果是，应用这个框架推导出了将初始条件映射到某个终端时间的ODE模型的Rademacher复杂度的紧凑表达式。这一结果利用了单层结构所带来的直接分析性质。最后，我们通过一些具体系统的例子实例化该界，并讨论了可能的后续工作。

    We show how continuous-depth neural ODE models can be framed as single-layer, infinite-width nets using the Chen--Fliess series expansion for nonlinear ODEs. In this net, the output ''weights'' are taken from the signature of the control input -- a tool used to represent infinite-dimensional paths as a sequence of tensors -- which comprises iterated integrals of the control input over a simplex. The ''features'' are taken to be iterated Lie derivatives of the output function with respect to the vector fields in the controlled ODE model. The main result of this work applies this framework to derive compact expressions for the Rademacher complexity of ODE models that map an initial condition to a scalar output at some terminal time. The result leverages the straightforward analysis afforded by single-layer architectures. We conclude with some examples instantiating the bound for some specific systems and discuss potential follow-up work.
    
[^73]: 增强连续强化学习中的回放在世界模型中

    Augmenting Replay in World Models for Continual Reinforcement Learning. (arXiv:2401.16650v1 [cs.LG])

    [http://arxiv.org/abs/2401.16650](http://arxiv.org/abs/2401.16650)

    本研究通过在回放缓冲区中应用增强方法，成功地解决了增强连续强化学习中的内存限制问题，并在世界模型中有效防止灾难性遗忘。

    

    在连续强化学习中，强化学习代理的环境会发生变化。成功的系统应该适当平衡保持已学习任务上的代理性能、稳定性和学习新任务的可塑性之间的矛盾要求。首进先出缓冲区通常用于增强此类设置中的学习，但需要大量内存。我们探索了将增强方法应用于此缓冲区中，以缓解内存限制，并与基于世界模型的强化学习算法一起使用，评估其在促进连续学习方面的效果。我们在Procgen和Atari强化学习基准测试中评估了我们方法的有效性，并证明了在潜在世界模型的背景下，回放缓冲区中的分布匹配增强可以成功防止灾难性遗忘，并显著降低计算开销。然而，我们也发现这种解决方案并非完全无懈可击，

    In continual RL, the environment of a reinforcement learning (RL) agent undergoes change. A successful system should appropriately balance the conflicting requirements of retaining agent performance on already learned tasks, stability, whilst learning new tasks, plasticity. The first-in-first-out buffer is commonly used to enhance learning in such settings but requires significant memory. We explore the application of an augmentation to this buffer which alleviates the memory constraints, and use it with a world model model-based reinforcement learning algorithm, to evaluate its effectiveness in facilitating continual learning. We evaluate the effectiveness of our method in Procgen and Atari RL benchmarks and show that the distribution matching augmentation to the replay-buffer used in the context of latent world models can successfully prevent catastrophic forgetting with significantly reduced computational overhead. Yet, we also find such a solution to not be entirely infallible, and
    
[^74]: 利用动作预测进行基于行为的虚拟现实（VR）认证

    Using Motion Forecasting for Behavior-Based Virtual Reality (VR) Authentication. (arXiv:2401.16649v1 [cs.LG])

    [http://arxiv.org/abs/2401.16649](http://arxiv.org/abs/2401.16649)

    本研究提出了一种利用动作预测进行基于行为的虚拟现实认证的方法，通过预测用户的未来行为轨迹并进行认证，解决了现有技术在使用较小轨迹段时的性能下降问题。

    

    在虚拟现实（VR）环境中，基于任务的行为生物特征认证可以通过仅使用人体的运动轨迹作为唯一签名实现无缝连续认证。基于深度学习的行为生物特征认证方法在使用完整或接近完整的用户轨迹时表现出较高的准确性，但在使用任务开始时的较小段落时表现较差。因此，使用现有技术设计的任何系统在等待未来的运动轨迹段可用时都存在漏洞。在这项工作中，我们提出了一种利用基于Transformer的预测方法来预测未来用户行为，并使用预测的轨迹进行用户认证。我们的工作利用了这样的观点：在任务环境中，根据用户当前的轨迹，我们可以预测用户的未来轨迹，因为他们不太可能在行为上发生剧变。

    Task-based behavioral biometric authentication of users interacting in virtual reality (VR) environments enables seamless continuous authentication by using only the motion trajectories of the person's body as a unique signature. Deep learning-based approaches for behavioral biometrics show high accuracy when using complete or near complete portions of the user trajectory, but show lower performance when using smaller segments from the start of the task. Thus, any systems designed with existing techniques are vulnerable while waiting for future segments of motion trajectories to become available. In this work, we present the first approach that predicts future user behavior using Transformer-based forecasting and using the forecasted trajectory to perform user authentication. Our work leverages the notion that given the current trajectory of a user in a task-based environment we can predict the future trajectory of the user as they are unlikely to dramatically shift their behavior sinc
    
[^75]: 通过混合精度加速科学机器学习并减少内存使用

    Speeding up and reducing memory usage for scientific machine learning via mixed precision. (arXiv:2401.16645v1 [cs.LG])

    [http://arxiv.org/abs/2401.16645](http://arxiv.org/abs/2401.16645)

    通过使用混合精度训练神经网络，可以加速科学机器学习并减少内存使用，但对于PINNs和DeepONets等方法，半精度不适用。

    

    科学机器学习（SciML）已成为解决复杂的计算科学和工程问题的多功能方法。在这个领域中，基于物理信息的神经网络（PINNs）和深度算子网络（DeepONets）以其结合物理方程和实验数据的能力而脱颖而出，成为解决偏微分方程的主要技术。然而，训练PINNs和DeepONets需要大量的计算资源，包括长时间的计算和大量的内存。为了提高计算效率，使用半精度（float16）而不是传统的单精度（float32）或双精度（float64）来训练神经网络已经引起了广泛关注，因为它具有减少计算时间和内存消耗的潜在优势。然而，我们发现float16不能应用于SciML方法，因为训练开始时梯度发散，权重更新变为零，无法收敛。

    Scientific machine learning (SciML) has emerged as a versatile approach to address complex computational science and engineering problems. Within this field, physics-informed neural networks (PINNs) and deep operator networks (DeepONets) stand out as the leading techniques for solving partial differential equations by incorporating both physical equations and experimental data. However, training PINNs and DeepONets requires significant computational resources, including long computational times and large amounts of memory. In search of computational efficiency, training neural networks using half precision (float16) rather than the conventional single (float32) or double (float64) precision has gained substantial interest, given the inherent benefits of reduced computational time and memory consumed. However, we find that float16 cannot be applied to SciML methods, because of gradient divergence at the start of training, weight updates going to zero, and the inability to converge to a 
    
[^76]: TeenyTinyLlama：基于巴西葡萄牙语训练的开源微型语言模型

    TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese. (arXiv:2401.16640v1 [cs.CL])

    [http://arxiv.org/abs/2401.16640](http://arxiv.org/abs/2401.16640)

    这篇论文开发了用于低资源环境中的开放式基础模型，以巴西葡萄牙语为例，发布在GitHub和Hugging Face上供社区使用和进一步开发。

    

    大型语言模型（LLMs）在自然语言处理方面取得了显著的进展，但在各种语言中的进展还不平衡。虽然大多数LLMs是在像英语这样的高资源语言中训练的，但多语言模型通常比单语言模型表现稍差。此外，它们的多语言基础有时会限制它们产生的副产品，如计算需求和许可制度。在本研究中，我们记录了为在低资源环境中使用而量身定制的开放式基础模型的开发过程、其局限性和优势。这就是TeenyTinyLlama：两个用于巴西葡萄牙语文本生成的紧凑型模型。我们在GitHub和Hugging Face上以宽松的Apache 2.0许可证发布它们，供社区使用和进一步开发。详见https://github.com/Nkluge-correa/TeenyTinyLlama

    Large language models (LLMs) have significantly advanced natural language processing, but their progress has yet to be equal across languages. While most LLMs are trained in high-resource languages like English, multilingual models generally underperform monolingual ones. Additionally, aspects of their multilingual foundation sometimes restrict the byproducts they produce, like computational demands and licensing regimes. In this study, we document the development of open-foundation models tailored for use in low-resource settings, their limitations, and their benefits. This is the TeenyTinyLlama pair: two compact models for Brazilian Portuguese text generation. We release them under the permissive Apache 2.0 license on GitHub and Hugging Face for community use and further development. See https://github.com/Nkluge-correa/TeenyTinyLlama
    
[^77]: 通过高效的奖励模型集成改进人工反馈强化学习

    Improving Reinforcement Learning from Human Feedback with Efficient Reward Model Ensemble. (arXiv:2401.16635v1 [cs.LG])

    [http://arxiv.org/abs/2401.16635](http://arxiv.org/abs/2401.16635)

    本论文提出一种通过高效的奖励模型集成来改进人工反馈强化学习的方法，以解决由于奖励模型预测不准确而导致RLHF输出与人类价值观不一致的问题。

    

    人工反馈强化学习（RLHF）是一种广泛使用的方法，用于将大型语言模型与人类价值观对齐。然而，RLHF依赖于通过有限的人类偏好数据训练的奖励模型，这可能导致不准确的预测。因此，RLHF可能产生与人类价值观不一致的输出。为了缓解这个问题，我们提出了一种奖励集成方法，可以使奖励模型做出更准确的预测。考虑到使用基于大型语言模型的奖励模型集成可能具有计算和资源昂贵的问题，我们探索了包括线性层集成和基于LoRA的集成在内的高效集成方法。实证上，我们使用我们的集成奖励模型运行Best-of-$n$和Proximal Policy Optimization，并验证我们的集成方法有助于改善RLHF输出的对齐性能。

    Reinforcement Learning from Human Feedback (RLHF) is a widely adopted approach for aligning large language models with human values. However, RLHF relies on a reward model that is trained with a limited amount of human preference data, which could lead to inaccurate predictions. As a result, RLHF may produce outputs that are misaligned with human values. To mitigate this issue, we contribute a reward ensemble method that allows the reward model to make more accurate predictions. As using an ensemble of large language model-based reward models can be computationally and resource-expensive, we explore efficient ensemble methods including linear-layer ensemble and LoRA-based ensemble. Empirically, we run Best-of-$n$ and Proximal Policy Optimization with our ensembled reward models, and verify that our ensemble methods help improve the alignment performance of RLHF outputs.
    
[^78]: 在以大数据为驱动的自主驾驶中使用主动学习进行3D物体检测：一个实证探究

    The Why, When, and How to Use Active Learning in Large-Data-Driven 3D Object Detection for Safe Autonomous Driving: An Empirical Exploration. (arXiv:2401.16634v1 [cs.CV])

    [http://arxiv.org/abs/2401.16634](http://arxiv.org/abs/2401.16634)

    本研究探究了在自主驾驶数据集中使用主动学习进行3D物体检测的方法。通过使用熵查询选择信息样本，可以降低标注成本并提高模型性能，特别是在减少多数类和少数类之间的性能差距方面表现优异。研究结果表明，选择多样且信息丰富的数据对于模型训练至关重要，而熵查询是在资源有限的环境中选择增强模型学习的数据的一种有前景的策略。

    

    在自主驾驶数据集中，使用主动学习策略进行3D物体检测可以解决数据不平衡、冗余和高维数据等挑战。我们通过使用熵查询来选择信息样本，旨在降低标注成本并提高模型性能。我们使用BEVFusion模型在nuScenes数据集上进行实验，将主动学习与随机采样进行比较，并证明熵查询在大多数情况下表现优异。该方法在减少多数类和少数类之间的性能差距方面特别有效。类别特定分析揭示了资源有限的情况下，为模型训练分配注释资源的高效方法，强调了选择多样且信息丰富的数据对模型训练的重要性。我们的研究结果表明，熵查询是在资源有限的环境中选择增强模型学习的数据的一种有前景的策略。

    Active learning strategies for 3D object detection in autonomous driving datasets may help to address challenges of data imbalance, redundancy, and high-dimensional data. We demonstrate the effectiveness of entropy querying to select informative samples, aiming to reduce annotation costs and improve model performance. We experiment using the BEVFusion model for 3D object detection on the nuScenes dataset, comparing active learning to random sampling and demonstrating that entropy querying outperforms in most cases. The method is particularly effective in reducing the performance gap between majority and minority classes. Class-specific analysis reveals efficient allocation of annotated resources for limited data budgets, emphasizing the importance of selecting diverse and informative data for model training. Our findings suggest that entropy querying is a promising strategy for selecting data that enhances model learning in resource-constrained environments.
    
[^79]: 线性卷积网络的代数复杂度和神经多样性研究

    Algebraic Complexity and Neurovariety of Linear Convolutional Networks. (arXiv:2401.16613v1 [math.AG])

    [http://arxiv.org/abs/2401.16613](http://arxiv.org/abs/2401.16613)

    本文研究了具有一维滤波器和任意步幅的线性卷积网络的代数复杂度和神经多样性。通过引入递归算法和度量代数几何的工具，我们发现这些网络优化中的关键点数量显著超过了完全连接的线性网络。

    

    本文研究了具有一维滤波器和任意步幅的线性卷积网络。这种网络的神经流形是一个半代数集，由具有特定分解的多项式空间表示。通过引入递归算法，我们生成多项式方程，其共同零点对应于相应神经流形的Zariski闭包。此外，我们还利用度量代数几何的工具探索了训练这些网络的代数复杂度。我们的研究结果揭示，优化这种网络时所有复杂关键点的数量等于一个Segre簇的通用欧几里德距离度。值得注意的是，这个数量显著超过了使用相同参数个数训练完全连接的线性网络时遇到的关键点数量。

    In this paper, we study linear convolutional networks with one-dimensional filters and arbitrary strides. The neuromanifold of such a network is a semialgebraic set, represented by a space of polynomials admitting specific factorizations. Introducing a recursive algorithm, we generate polynomial equations whose common zero locus corresponds to the Zariski closure of the corresponding neuromanifold. Furthermore, we explore the algebraic complexity of training these networks employing tools from metric algebraic geometry. Our findings reveal that the number of all complex critical points in the optimization of such a network is equal to the generic Euclidean distance degree of a Segre variety. Notably, this count significantly surpasses the number of critical points encountered in the training of a fully connected linear network with the same number of parameters.
    
[^80]: 在逆问题中学习高斯混合物进行稀疏正则化

    Learning a Gaussian Mixture for Sparsity Regularization in Inverse Problems. (arXiv:2401.16612v1 [stat.ML])

    [http://arxiv.org/abs/2401.16612](http://arxiv.org/abs/2401.16612)

    本研究提出了一种基于高斯混合模型的稀疏正则化方法，通过神经网络进行贝叶斯估计，有效地解决了逆问题中的稀疏建模和参数估计问题。

    

    在逆问题中，广泛认为引入稀疏先验对解决方案具有正则化效果。这种方法是基于一个先验假设，即未知量可以在一个有限数量的显著成分的基础上适当表示，而大多数系数接近于零。这种情况在现实世界中经常出现，比如分段平滑信号。在本研究中，我们提出了一种以高斯退化混合物形式表述的概率稀疏先验，能够对于任意基进行稀疏建模。在这个前提下，我们设计了一个可以解释为线性逆问题的贝叶斯估计器的神经网络。此外，我们提出了一种有监督和无监督的训练策略来估计这个网络的参数。为了评估我们方法的有效性，我们进行了与常用的稀疏正则化方法的数值比较。

    In inverse problems, it is widely recognized that the incorporation of a sparsity prior yields a regularization effect on the solution. This approach is grounded on the a priori assumption that the unknown can be appropriately represented in a basis with a limited number of significant components, while most coefficients are close to zero. This occurrence is frequently observed in real-world scenarios, such as with piecewise smooth signals. In this study, we propose a probabilistic sparsity prior formulated as a mixture of degenerate Gaussians, capable of modeling sparsity with respect to a generic basis. Under this premise, we design a neural network that can be interpreted as the Bayes estimator for linear inverse problems. Additionally, we put forth both a supervised and an unsupervised training strategy to estimate the parameters of this network. To evaluate the effectiveness of our approach, we conduct a numerical comparison with commonly employed sparsity-promoting regularization
    
[^81]: 通过调控深度学习的电子声子谱函数来加速超导体发现

    Accelerating superconductor discovery through tempered deep learning of the electron-phonon spectral function. (arXiv:2401.16611v1 [cond-mat.supr-con])

    [http://arxiv.org/abs/2401.16611](http://arxiv.org/abs/2401.16611)

    通过采用非传统的训练策略，结合调控深度学习模型，成功预测了电子声子谱函数，并得到了超导材料的关键参数，加速了超导体的发现。

    

    将深度学习与寻找新的电子声子超导体结合起来，是一个蓬勃发展的研究领域，其中主要挑战在于计算电子声子谱函数$\alpha^2F(\omega)$的计算强度，这是 Midgal-Eliashberg 超导理论的基本因素。为了克服这个挑战，我们采用了一个两步方法。首先，我们对818个动态稳定的材料计算了$\alpha^2F(\omega)$。然后，我们采用一种非传统的训练策略，使用混合模型训练神经网络模型来预测$\alpha^2F(\omega)$，以调节模型的过度拟合，提高预测准确性。具体而言，我们使用 TEMNets（温度均衡下的 Bootstrapped Ensemble of Equivariant graph neural networks）模型进行训练，得到了来自$\alpha^2F(\omega)$的 Eliashberg 矩的 MAE，分别为0.21，45 K 和 43 K: $\lambda$，$\omega_{\log}$ 和 $\omega_{2}$，对应的临界温度$T_c$的 MAE 为2.5 K。此外，我们还将领域信息结合进来利用。

    Integrating deep learning with the search for new electron-phonon superconductors represents a burgeoning field of research, where the primary challenge lies in the computational intensity of calculating the electron-phonon spectral function, $\alpha^2F(\omega)$, the essential ingredient of Midgal-Eliashberg theory of superconductivity. To overcome this challenge, we adopt a two-step approach. First, we compute $\alpha^2F(\omega)$ for 818 dynamically stable materials. We then train a deep-learning model to predict $\alpha^2F(\omega)$, using an unconventional training strategy to temper the model's overfitting, enhancing predictions. Specifically, we train a Bootstrapped Ensemble of Tempered Equivariant graph neural NETworks (BETE-NET), obtaining an MAE of 0.21, 45 K, and 43 K for the Eliashberg moments derived from $\alpha^2F(\omega)$: $\lambda$, $\omega_{\log}$, and $\omega_{2}$, respectively, yielding an MAE of 2.5 K for the critical temperature, $T_c$. Further, we incorporate domain
    
[^82]: 一种多标签分类的一致算法: 宏观at-$k$度量.

    Consistent algorithms for multi-label classification with macro-at-$k$ metrics. (arXiv:2401.16594v1 [cs.LG])

    [http://arxiv.org/abs/2401.16594](http://arxiv.org/abs/2401.16594)

    该研究提出了一种针对多标签分类的一致算法，主要解决了宏观at-$k$度量的优化问题。通过在人口效用框架下考虑复杂性能度量的优化，该算法在极端分类问题中表现出色。

    

    我们在人口效用框架下考虑了多标签分类中复杂性能度量的优化问题。我们主要关注的是将度量线性分解为每个标签分别应用的二分类效用的总和，并对每个实例预测恰好有$k$个标签的额外要求。“宏观at-$k$”度量在具有长尾标签的极端分类问题中具有理想的属性。不幸的是，at-$k$约束将原本独立的二分类任务耦合在一起，导致比标准宏平均更具挑战性的优化问题。我们提供了一个统计框架来研究这个问题，证明了最优分类器的存在和形式，并基于Frank-Wolfe方法提出了一种统计一致且实用的学习算法。有趣的是，我们的主要结果还涉及非线性函数的更一般度量，这些函数是按标签进行的混淆矩阵。实证结果表明，我们的算法在多个数据集上都取得了很好的性能。

    We consider the optimization of complex performance metrics in multi-label classification under the population utility framework. We mainly focus on metrics linearly decomposable into a sum of binary classification utilities applied separately to each label with an additional requirement of exactly $k$ labels predicted for each instance. These "macro-at-$k$" metrics possess desired properties for extreme classification problems with long tail labels. Unfortunately, the at-$k$ constraint couples the otherwise independent binary classification tasks, leading to a much more challenging optimization problem than standard macro-averages. We provide a statistical framework to study this problem, prove the existence and the form of the optimal classifier, and propose a statistically consistent and practical learning algorithm based on the Frank-Wolfe method. Interestingly, our main results concern even more general metrics being non-linear functions of label-wise confusion matrices. Empirical
    
[^83]: 基于自编码器的概念空间语义通信的领域学习

    Autoencoder-Based Domain Learning for Semantic Communication with Conceptual Spaces. (arXiv:2401.16569v1 [cs.LG])

    [http://arxiv.org/abs/2401.16569](http://arxiv.org/abs/2401.16569)

    这篇论文研究了基于概念空间的语义通信，提出了一个自编码器学习的框架，解决了无法捕捉和量化“意义”的问题。

    

    与准确传递符号相比，以准确传递意义为目标的语义通信已成为一个越来越受关注的领域。这种范式通常利用人工智能和机器学习的现代发展，以提高通信系统的效率和鲁棒性。然而，对于捕捉和量化“意义”的细节缺乏一个标准模型，许多领先的语义通信方法采用黑盒框架，对模型的具体学习内容知之甚少。一种解决方案是利用概念空间框架，以几何方式明确建模意义。虽然以前使用概念空间研究语义通信的工作已经取得了良好的结果，但这些先前的尝试涉及手工制作概念空间模型，严重限制了该方法的可扩展性和实用性。在这项工作中，我们开发了一个学习框架，用于学习一个自编码器，实现概念空间语义通信。

    Communication with the goal of accurately conveying meaning, rather than accurately transmitting symbols, has become an area of growing interest. This paradigm, termed semantic communication, typically leverages modern developments in artificial intelligence and machine learning to improve the efficiency and robustness of communication systems. However, a standard model for capturing and quantifying the details of "meaning" is lacking, with many leading approaches to semantic communication adopting a black-box framework with little understanding of what exactly the model is learning. One solution is to utilize the conceptual spaces framework, which models meaning explicitly in a geometric manner. Though prior work studying semantic communication with conceptual spaces has shown promising results, these previous attempts involve hand-crafting a conceptual space model, severely limiting the scalability and practicality of the approach. In this work, we develop a framework for learning a 
    
[^84]: 深度学习在多标签学习中的应用：一项全面调研

    Deep Learning for Multi-Label Learning: A Comprehensive Survey. (arXiv:2401.16549v1 [cs.LG])

    [http://arxiv.org/abs/2401.16549](http://arxiv.org/abs/2401.16549)

    深度学习在多标签学习中的综合调研，旨在审视深度学习在解决多标签分类中的挑战方面的最新进展。

    

    多标签学习是一个快速发展的研究领域，旨在从单个输入数据点中预测多个标签。在大数据时代，涉及多标签分类或排名的任务带来了重大而复杂的挑战，在各个领域引起了极大关注。多标签分类面临的困难包括处理高维数据、解决标签相关性和处理部分标签，传统方法在这方面表现不佳。近年来，人们越来越多地采用深度学习技术来更有效地应对多标签分类中的这些挑战。值得注意的是，针对深度学习在多标签学习中的综合研究还比较有限。因此，本调研旨在全面审视深度学习在多标签学习中的最新进展。

    Multi-label learning is a rapidly growing research area that aims to predict multiple labels from a single input data point. In the era of big data, tasks involving multi-label classification (MLC) or ranking present significant and intricate challenges, capturing considerable attention in diverse domains. Inherent difficulties in MLC include dealing with high-dimensional data, addressing label correlations, and handling partial labels, for which conventional methods prove ineffective. Recent years have witnessed a notable increase in adopting deep learning (DL) techniques to address these challenges more effectively in MLC. Notably, there is a burgeoning effort to harness the robust learning capabilities of DL for improved modelling of label dependencies and other challenges in MLC. However, it is noteworthy that comprehensive studies specifically dedicated to DL for multi-label learning are limited. Thus, this survey aims to thoroughly review recent progress in DL for multi-label lea
    
[^85]: 高效的行政数据机器学习观察时间窗口分割

    Efficient Observation Time Window Segmentation for Administrative Data Machine Learning. (arXiv:2401.16537v1 [cs.LG])

    [http://arxiv.org/abs/2401.16537](http://arxiv.org/abs/2401.16537)

    本文研究了如何将机器学习模型的观察窗口划分为时间段，通过优化高优先级特征的时间bin大小，可以实现更简单、更快速训练的机器学习模型，并且能够达到与更复杂模型相似甚至更好的性能。

    

    利用行政数据预测结果是机器学习的一个重要应用领域，尤其在医疗保健领域。大多数行政数据记录都有时间戳，记录随时间的模式是机器学习模型的关键输入。本文探讨了如何将机器学习模型的观察窗口划分为时间段或“bins”。提出了一种计算上高效的过程，可以确定哪些数据特征最适合较小的，更高分辨率的时间段。在医疗保健和住房/无家可归的行政数据上产生的结果表明，在其他特征使用单个时间bin的情况下，优化这些高优先级特征的时间bin大小可以实现更简单，更快速训练的机器学习模型。这种方法还可以实现与更复杂的模型相似甚至更好的性能，这些模型默认将所有数据特征用相同的时间分辨率表示。

    Utilizing administrative data to predict outcomes is an important application area of machine learning, particularly in healthcare. Most administrative data records are timestamped and the pattern of records over time is a key input for machine learning models. This paper explores how best to divide the observation window of a machine learning model into time segments or "bins". A computationally efficient process is presented that identifies which data features benefit most from smaller, higher resolution time segments. Results generated on healthcare and housing/homelessness administrative data demonstrate that optimizing the time bin size of these high priority features while using a single time bin for the other features achieves machine learning models that are simpler and quicker to train. This approach also achieves similar and sometimes better performance than more complex models that default to representing all data features with the same time resolution.
    
[^86]: 时间序列深度学习模型的扰动敏感性分析方法的验证、健壮性和准确性

    Validation, Robustness, and Accuracy of Perturbation-Based Sensitivity Analysis Methods for Time-Series Deep Learning Models. (arXiv:2401.16521v1 [cs.LG])

    [http://arxiv.org/abs/2401.16521](http://arxiv.org/abs/2401.16521)

    本研究验证了时间序列深度学习模型的扰动敏感性分析方法的可靠性和准确性，并比较了不同方法和模型的影响。

    

    本研究对时间序列深度学习的可解释性方法进行评估。敏感性分析评估输入变化对输出的影响，是解释的关键组成部分。在后期解释方法中，如反向传播、扰动和近似法中，本研究将调查现代Transformer模型上的基于扰动的敏感性分析方法，以评估其性能。具体而言，本研究回答了三个研究问题：1）不同的敏感性分析方法是否产生可比较的输出和属性重要性排序？2）使用相同的敏感性分析方法，不同的深度学习模型是否对敏感性分析的输出产生影响？3）敏感性分析方法的结果与基本事实的一致性如何？

    This work undertakes studies to evaluate Interpretability Methods for Time-Series Deep Learning. Sensitivity analysis assesses how input changes affect the output, constituting a key component of interpretation. Among the post-hoc interpretation methods such as back-propagation, perturbation, and approximation, my work will investigate perturbation-based sensitivity Analysis methods on modern Transformer models to benchmark their performances. Specifically, my work answers three research questions: 1) Do different sensitivity analysis (SA) methods yield comparable outputs and attribute importance rankings? 2) Using the same sensitivity analysis method, do different Deep Learning (DL) models impact the output of the sensitivity analysis? 3) How well do the results from sensitivity analysis methods align with the ground truth?
    
[^87]: MT-HCCAR: 多任务深度学习与层级分类的注意力回归用于云属性检索

    MT-HCCAR: Multi-Task Deep Learning with Hierarchical Classification and Attention-based Regression for Cloud Property Retrieval. (arXiv:2401.16520v1 [cs.LG])

    [http://arxiv.org/abs/2401.16520](http://arxiv.org/abs/2401.16520)

    这篇论文提出了一种名为MT-HCCAR的多任务深度学习模型，用于云属性检索。该模型考虑了云属性检索任务之间的层级关系，并具有对不同传感器数据集具有健壮泛化能力的特点。

    

    在地球科学领域中，有效的云属性检索包括云遮蔽、云相分类和云光学厚度（COT）预测仍然至关重要。传统方法需要针对每个传感器仪器使用不同的模型，因为它们具有独特的光谱特征。最近，在地球科学研究中采用了机器学习和深度学习技术从卫星数据集的光谱观测中提取特征。然而，现有方法缺乏考虑检索任务之间层级关系的创新架构。此外，考虑到现有传感器之间的光谱多样性，开发具有对不同传感器数据集具有健壮泛化能力的模型是必要的。令人惊讶的是，目前缺乏解决多样数据集下选择最优模型的方法。为此，本文引入了MT-HCCAR，这是一种端到端的深度学习模型，采用多任务学习和基于注意力的回归方法。

    In the realm of Earth science, effective cloud property retrieval, encompassing cloud masking, cloud phase classification, and cloud optical thickness (COT) prediction, remains pivotal. Traditional methodologies necessitate distinct models for each sensor instrument due to their unique spectral characteristics. Recent strides in Earth Science research have embraced machine learning and deep learning techniques to extract features from satellite datasets' spectral observations. However, prevailing approaches lack novel architectures accounting for hierarchical relationships among retrieval tasks. Moreover, considering the spectral diversity among existing sensors, the development of models with robust generalization capabilities over different sensor datasets is imperative. Surprisingly, there is a dearth of methodologies addressing the selection of an optimal model for diverse datasets. In response, this paper introduces MT-HCCAR, an end-to-end deep learning model employing multi-task 
    
[^88]: AFSD-Physics：通过人工智能与人类协作方法探索添加摩擦搅拌堆积过程中温度演变的控制方程

    AFSD-Physics: Exploring the governing equations of temperature evolution during additive friction stir deposition by a human-AI teaming approach. (arXiv:2401.16501v1 [cs.LG])

    [http://arxiv.org/abs/2401.16501](http://arxiv.org/abs/2401.16501)

    本文使用人工智能与人类协作的方法，提出了AFSD-Physics模型，通过学习实验数据，得到了添加摩擦搅拌堆积过程中温度演变的控制方程。该模型具有物理解释性、计算成本低且准确度高，与实际测量结果吻合较好。

    

    本文提出了一种模型方法，通过人工智能与人类协作方法研究添加摩擦搅拌堆积（AFSD）过程中温度演变的物理原理。AFSD是一种新兴的固态增材制造技术，可以在没有熔融的情况下进行材料堆积。然而，目前对于该过程的建模以及AFSD工具的建模还处于早期阶段。本文提出了一种人工智能与人类协作的方法，将基于第一原理的模型与人工智能相结合。得到的人工智能学习方法被命名为AFSD-Physics，能够有效地学习工具和堆积过程中温度演变的控制方程，通过过程中的测量数据进行学习。设计并进行了实验，采集了30层铝7075材料堆积过程中的测量数据。得到的控制方程是具有物理解释性、计算成本低且准确度高的模型。模型预测结果与测量结果具有良好的一致性。

    This paper presents a modeling effort to explore the underlying physics of temperature evolution during additive friction stir deposition (AFSD) by a human-AI teaming approach. AFSD is an emerging solid-state additive manufacturing technology that deposits materials without melting. However, both process modeling and modeling of the AFSD tool are at an early stage. In this paper, a human-AI teaming approach is proposed to combine models based on first principles with AI. The resulting human-informed machine learning method, denoted as AFSD-Physics, can effectively learn the governing equations of temperature evolution at the tool and the build from in-process measurements. Experiments are designed and conducted to collect in-process measurements for the deposition of aluminum 7075 with a total of 30 layers. The acquired governing equations are physically interpretable models with low computational cost and high accuracy. Model predictions show good agreement with the measurements. Expe
    
[^89]: 一个用于高维数据的判别贝叶斯高斯过程潜变量模型

    A Discriminative Bayesian Gaussian Process Latent Variable Model for High-Dimensional Data. (arXiv:2401.16497v1 [cs.LG])

    [http://arxiv.org/abs/2401.16497](http://arxiv.org/abs/2401.16497)

    本研究提出了一个名为LDGD的判别贝叶斯高斯过程潜变量模型，能够有效地从高维数据中提取信息，并具有较高的预测准确性和鲁棒性。

    

    从高维数据中提取有意义的信息是一个具有挑战性的建模问题，特别是当数据被噪声干扰或以不同的模态表示时。在这项研究中，我们提出了一种新颖的非参数建模方法，利用高斯过程（GP）将高维数据映射到潜在的低维流形上。这个模型被命名为潜在判别生成解码器（LDGD），它在流形发现过程中利用了数据（或其特征）和相关标签（如类别或刺激）。为了推断潜在变量，我们提供了一个贝叶斯解，使得LDGD能够有效地捕捉数据中的内在不确定性，同时提高模型的预测准确性和鲁棒性。我们在合成数据集和基准数据集上演示了LDGD的应用。LDGD不仅能准确地推断流形，而且在预测标签方面的准确性超过了最先进的方法。

    Extracting meaningful information from high-dimensional data poses a formidable modeling challenge, particularly when the data is obscured by noise or represented through different modalities. In this research, we propose a novel non-parametric modeling approach, leveraging the Gaussian Process (GP), to characterize high-dimensional data by mapping it to a latent low-dimensional manifold. This model, named the Latent Discriminative Generative Decoder (LDGD), utilizes both the data (or its features) and associated labels (such as category or stimulus) in the manifold discovery process. To infer the latent variables, we derive a Bayesian solution, allowing LDGD to effectively capture inherent uncertainties in the data while enhancing the model's predictive accuracy and robustness. We demonstrate the application of LDGD on both synthetic and benchmark datasets. Not only does LDGD infer the manifold accurately, but its prediction accuracy in anticipating labels surpasses state-of-the-art a
    
[^90]: GPU集群调度对网络敏感的深度学习

    GPU Cluster Scheduling for Network-Sensitive Deep Learning. (arXiv:2401.16492v1 [cs.PF])

    [http://arxiv.org/abs/2401.16492](http://arxiv.org/abs/2401.16492)

    我们提出了一种GPU集群调度器，用于分布式深度学习任务，根据任务对通信网络延迟的敏感性进行GPU资源的邻近基础一致性。相比传统的调度方法，我们的调度器可以提供高达69％的端到端Makespan提升。

    

    我们提出了一种新颖的GPU集群调度器，用于分布式DL（DDL）工作负载，以基于DDL作业对预期通信网络延迟的敏感性进行GPU资源的邻近基础一致性。我们的调度器由三个主要组成部分组成：（i）一个经典的延迟调度算法，用于促进作业放置和一致性；（ii）一个对网络敏感的作业抢占策略；和（iii）一种“自动调整器”机制，用于优化延迟计时器以实现有效的延迟调度。另外，为了实现大规模实验的成本效益方法，我们开发了一个数据驱动的DDL集群仿真平台。通过使用仿真平台，我们在实际工作负载跟踪中与几种最先进的替代方法进行了比较，以展示我们设计的优势。与传统的基于一致性调度方法相比，我们的调度器可以提供高达69％的端到端Makespan提升，同时减少了平均j

    We propose a novel GPU-cluster scheduler for distributed DL (DDL) workloads that enables proximity based consolidation of GPU resources based on the DDL jobs' sensitivities to the anticipated communication-network delays. Our scheduler consists of three major components: (i) a classical delay scheduling algorithm to facilitate job placement and consolidation; (ii) a network-sensitive job preemption strategy; and (iii) an "auto-tuner" mechanism to optimize delay timers for effective delay scheduling. Additionally, to enable a cost-effective methodology for large-scale experiments, we develop a data-driven DDL cluster simulation platform. Employing the simulation platform we compare against several state-of-the-art alternatives on real-world workload traces to demonstrate the benefits of our design. Our scheduler can provide improvement of up to 69% in end-to-end Makespan for training all jobs compared to the prevailing consolidation-based scheduling methods, while reducing the average j
    
[^91]: 遵循人类指令的高质量图像恢复

    High-Quality Image Restoration Following Human Instructions. (arXiv:2401.16468v1 [cs.CV])

    [http://arxiv.org/abs/2401.16468](http://arxiv.org/abs/2401.16468)

    本论文提出了一种使用人类编写的指令来指导图像恢复模型的方法，并在多个恢复任务上取得了最先进的结果，为基于文本指导的图像恢复和增强研究提供了一个新的基准。

    

    图像恢复是一个基本问题，涉及从退化观测中恢复出高质量的干净图像。全能图像恢复模型可以通过使用特定于退化类型的信息作为提示来有效地恢复各种类型和级别的退化图像，并引导恢复模型。我们提出了一种使用人类编写的指令来指导图像恢复模型的方法。在给定自然语言提示的情况下，我们的模型可以从退化图像中恢复出高质量的图像，并考虑多种退化类型。我们的方法InstructIR在图像去噪、雨水去除、去模糊、去雾和(低光)图像增强等多个恢复任务上取得了最先进的结果。InstructIR在之前的全能恢复方法上提高了1dB。此外，我们的数据集和结果为基于文本指导的图像恢复和增强的新研究提供了一个新的基准。我们提供了代码、数据集和模型。

    Image restoration is a fundamental problem that involves recovering a high-quality clean image from its degraded observation. All-In-One image restoration models can effectively restore images from various types and levels of degradation using degradation-specific information as prompts to guide the restoration model. In this work, we present the first approach that uses human-written instructions to guide the image restoration model. Given natural language prompts, our model can recover high-quality images from their degraded counterparts, considering multiple degradation types. Our method, InstructIR, achieves state-of-the-art results on several restoration tasks including image denoising, deraining, deblurring, dehazing, and (low-light) image enhancement. InstructIR improves +1dB over previous all-in-one restoration methods. Moreover, our dataset and results represent a novel benchmark for new research on text-guided image restoration and enhancement. Our code, datasets and models a
    
[^92]: ReGAL: 用于发现通用抽象的程序重构方法

    ReGAL: Refactoring Programs to Discover Generalizable Abstractions. (arXiv:2401.16467v1 [cs.SE])

    [http://arxiv.org/abs/2401.16467](http://arxiv.org/abs/2401.16467)

    ReGAL提出了一种用于发现通用抽象的程序重构方法，可以通过重构代码学习可重用的函数库，利用这些共享函数库可以更准确地预测程序。

    

    虽然大型语言模型（LLMs）越来越多地被用于程序合成，但它们缺乏开发有用抽象所需的全局视角；它们通常一次预测一个程序，经常重复相同的功能。从头开始生成冗余代码既低效又容易出错。为了解决这个问题，我们提出了用于通用抽象学习的重构方法（ReGAL），通过代码重构来学习可重用函数库，即在不改变代码执行输出的情况下重组代码。ReGAL从一小组现有程序中学习，通过执行验证和细化抽象。我们发现，ReGAL发现的共享函数库使得在不同领域预测程序变得更加容易。在三个数据集（LOGO图形生成、日期推理和基于Minecraft的文字游戏TextCraft）上，开源和专有的LLMs在使用ReGAL函数库预测程序时准确性得到提高。

    While large language models (LLMs) are increasingly being used for program synthesis, they lack the global view needed to develop useful abstractions; they generally predict programs one at a time, often repeating the same functionality. Generating redundant code from scratch is both inefficient and error-prone. To address this, we propose Refactoring for Generalizable Abstraction Learning (ReGAL), a gradient-free method for learning a library of reusable functions via code refactorization, i.e. restructuring code without changing its execution output. ReGAL learns from a small set of existing programs, iteratively verifying and refining its abstractions via execution. We find that the shared function libraries discovered by ReGAL make programs easier to predict across diverse domains. On three datasets (LOGO graphics generation, Date reasoning, and TextCraft, a Minecraft-based text game), both open-source and proprietary LLMs improve in accuracy when predicting programs with ReGAL fun
    
[^93]: 实现无遗憾的广告牌广告时段分配

    Towards Regret Free Slot Allocation in Billboard Advertisement. (arXiv:2401.16464v1 [cs.IR])

    [http://arxiv.org/abs/2401.16464](http://arxiv.org/abs/2401.16464)

    本文提出了一种无遗憾的广告牌广告时段分配方法，旨在最大化广告商与顾客之间的影响力，并通过四种高效的解决方案来减少遗憾的发生。

    

    在广告牌广告中，为了创造和最大化对顾客的影响力，广告商需要找到具有一定影响力的人提供一定数量的广告播放，并基于观看数量收费。本文针对广告播放者，提出了一种解决该问题的离散优化方法，并通过四种高效的解决方案分析了它们的时间和空间复杂性。最终目标是最小化决策带来的损失（遗憾）。

    Creating and maximizing influence among the customers is one of the central goals of an advertiser, and hence, remains an active area of research in recent times. In this advertisement technique, the advertisers approach an influence provider for a specific number of views of their content on a payment basis. Now, if the influence provider can provide the required number of views or more, he will receive the full, else a partial payment. In the context of an influence provider, it is a loss for him if he offers more or less views. This is formalized as 'Regret', and naturally, in the context of the influence provider, the goal will be to minimize this quantity. In this paper, we solve this problem in the context of billboard advertisement and pose it as a discrete optimization problem. We propose four efficient solution approaches for this problem and analyze them to understand their time and space complexity. We implement all the solution methodologies with real-life datasets and comp
    
[^94]: 基于监督对比学习的双混合模型用于剩余寿命预测

    Supervised Contrastive Learning based Dual-Mixer Model for Remaining Useful Life Prediction. (arXiv:2401.16462v1 [cs.LG])

    [http://arxiv.org/abs/2401.16462](http://arxiv.org/abs/2401.16462)

    本文提出了一种基于监督对比学习的双混合模型，用于剩余寿命预测。该模型通过灵活的特征融合和特征空间全局关系不变性训练方法，提高了预测准确性。

    

    近年来，剩余寿命（RUL）预测问题已经引起了研究人员的广泛关注，旨在准确估计从当前预测时刻到设备完全失效的剩余时间。为了克服现有RUL预测方法中时间和空间特征刚性组合的缺点，本文首次提出了一种名为双混合模型的时空同质特征提取器。采用灵活的逐层递进特征融合，以确保时空特征的同质性并提高预测准确性。其次，基于监督对比学习引入了特征空间全局关系不变性（FSGRI）训练方法。该方法在模型训练过程中维持样本特征与其退化模式之间的关系一致性，简化了输出层中的回归任务，提高了预测性能。

    The problem of the Remaining Useful Life (RUL) prediction, aiming at providing an accurate estimate of the remaining time from the current predicting moment to the complete failure of the device, has gained significant attention from researchers in recent years. In this paper, to overcome the shortcomings of rigid combination for temporal and spatial features in most existing RUL prediction approaches, a spatial-temporal homogeneous feature extractor, named Dual-Mixer model, is firstly proposed. Flexible layer-wise progressive feature fusion is employed to ensure the homogeneity of spatial-temporal features and enhance the prediction accuracy. Secondly, the Feature Space Global Relationship Invariance (FSGRI) training method is introduced based on supervised contrastive learning. This method maintains the consistency of relationships among sample features with their degradation patterns during model training, simplifying the subsequently regression task in the output layer and improvin
    
[^95]: 温和的规范执行：更快的出现，更快乐的智能体

    Norm Enforcement with a Soft Touch: Faster Emergence, Happier Agents. (arXiv:2401.16461v1 [cs.MA])

    [http://arxiv.org/abs/2401.16461](http://arxiv.org/abs/2401.16461)

    通过温和的规范执行，该研究提出了一种新的方法，通过智能体之间的交流推动合作并促进规范的出现。

    

    多智能体系统可视为一个自主智能体的社会，通过社会规范可以有效地调控智能体的交互。一般来说，一个社会的规范并不是硬编码的，而是从智能体的交互中产生的。具体来说，一个社会中的智能体对另一个智能体的行为作出的反应以及对他人反应的回应，决定了社会中出现哪些规范。我们将一个智能体对另一个智能体的满意或不满意行为的反应视为第一个智能体向第二个智能体的交流。理解这些交流是一种社会智能：这些交流通过推动智能体朝着某些行为进行，从而促进规范的出现。虽然众所周知惩罚可以导致规范的出现，但我们认为更宽泛的社会智能可能在促进多智能体系统中的合作方面更有效。因此，我们开发了一种被称为Ne的方法

    A multiagent system can be viewed as a society of autonomous agents, whose interactions can be effectively regulated via social norms. In general, the norms of a society are not hardcoded but emerge from the agents' interactions. Specifically, how the agents in a society react to each other's behavior and respond to the reactions of others determines which norms emerge in the society. We think of these reactions by an agent to the satisfactory or unsatisfactory behaviors of another agent as communications from the first agent to the second agent. Understanding these communications is a kind of social intelligence: these communications provide natural drivers for norm emergence by pushing agents toward certain behaviors, which can become established as norms. Whereas it is well-known that sanctioning can lead to the emergence of norms, we posit that a broader kind of social intelligence can prove more effective in promoting cooperation in a multiagent system.  Accordingly, we develop Ne
    
[^96]: 信用风险与大型语言模型相结合：从P2P借贷的贷款描述中构建风险指标。

    Credit Risk Meets Large Language Models: Building a Risk Indicator from Loan Descriptions in P2P Lending. (arXiv:2401.16458v1 [q-fin.RM])

    [http://arxiv.org/abs/2401.16458](http://arxiv.org/abs/2401.16458)

    本文研究了如何利用P2P借贷平台上借款人提供的文本描述来构建风险指标。结果显示，利用大型语言模型生成的风险评分可以明显提高信用风险分类器的性能。

    

    P2P借贷作为一种独特的融资机制，通过在线平台将借款人与放款人联系起来。然而，P2P借贷面临信息不对称的挑战，因为放款人往往缺乏足够的数据来评估借款人的信用价值。本文提出了一种新颖的方法来解决这个问题，即利用借款人在贷款申请过程中提供的文本描述。我们的方法涉及使用大型语言模型（LLM）处理这些文本描述，LLM是一种能够识别文本中的模式和语义的强大工具。将迁移学习应用于将LLM适应特定任务。我们从Lending Club数据集的分析结果显示，BERT生成的风险评分显著提高了信用风险分类器的性能。然而，基于LLM的系统固有的不透明性，以及潜在偏差的不确定性，限制了其应用。

    Peer-to-peer (P2P) lending has emerged as a distinctive financing mechanism, linking borrowers with lenders through online platforms. However, P2P lending faces the challenge of information asymmetry, as lenders often lack sufficient data to assess the creditworthiness of borrowers. This paper proposes a novel approach to address this issue by leveraging the textual descriptions provided by borrowers during the loan application process. Our methodology involves processing these textual descriptions using a Large Language Model (LLM), a powerful tool capable of discerning patterns and semantics within the text. Transfer learning is applied to adapt the LLM to the specific task at hand.  Our results derived from the analysis of the Lending Club dataset show that the risk score generated by BERT, a widely used LLM, significantly improves the performance of credit risk classifiers. However, the inherent opacity of LLM-based systems, coupled with uncertainties about potential biases, unders
    
[^97]: 有效的可控偏差缓解方法，利用门适配器进行分类和检索。(arXiv:2401.16457v1 [cs.LG])

    Effective Controllable Bias Mitigation for Classification and Retrieval using Gate Adapters. (arXiv:2401.16457v1 [cs.LG])

    [http://arxiv.org/abs/2401.16457](http://arxiv.org/abs/2401.16457)

    本文引入了可控门适配器（ConGater），一种具有可调节敏感性参数的新颖模块化门机制，可在推理时逐渐过渡从模型的偏向状态到完全去偏的版本，并通过实验证明了其在分类和检索任务中的性能。

    

    语言模型的偏差缓解已经成为许多研究的主题，最近关注的焦点是学习独立的模块，例如适配器进行按需去偏。除了优化模块化去偏模型外，在实践中通常需要在推理时控制偏差减少的程度，例如，为了在搜索结果中调整期望的性能-公平性权衡或在分类任务中控制去偏的强度。在本文中，我们引入了可控门适配器（ConGater），一种具有可调节敏感性参数的新颖模块化门机制，允许在推理时从模型的偏向状态逐渐过渡到完全去偏的版本。通过在三个分类任务上对三个不同模型进行对抗性去偏实验，并通过公平性列表正则化来减少搜索结果的偏差，我们展示了ConGater的性能。

    Bias mitigation of Language Models has been the topic of many studies with a recent focus on learning separate modules like adapters for on-demand debiasing. Besides optimizing for a modularized debiased model, it is often critical in practice to control the degree of bias reduction at inference time, e.g., in order to tune for a desired performance-fairness trade-off in search results or to control the strength of debiasing in classification tasks. In this paper, we introduce Controllable Gate Adapter (ConGater), a novel modular gating mechanism with adjustable sensitivity parameters, which allows for a gradual transition from the biased state of the model to the fully debiased version at inference time. We demonstrate ConGater performance by (1) conducting adversarial debiasing experiments with three different models on three classification tasks with four protected attributes, and (2) reducing the bias of search results through fairness list-wise regularization to enable adjusting a
    
[^98]: 混合Transformer和时空自监督学习用于长期交通预测

    Hybrid Transformer and Spatial-Temporal Self-Supervised Learning for Long-term Traffic Prediction. (arXiv:2401.16453v1 [cs.LG])

    [http://arxiv.org/abs/2401.16453](http://arxiv.org/abs/2401.16453)

    本文提出了一个将混合Transformer和时空自监督学习相结合的模型，通过应用自适应数据增强技术和Chebyshev多项式图卷积来提高模型的鲁棒性和对复杂空间依赖性的捕捉能力，并设计了两个自监督学习任务来建模时空异质性，从而提高了模型的准确性和泛化能力。

    

    长期交通预测一直是一项具有挑战性的任务，由于其动态时间依赖性和复杂的空间依赖性。本文提出了一种将混合Transformer和时空自监督学习相结合的模型。该模型通过在交通数据的序列级和图级应用自适应数据增强技术，增强了其鲁棒性。它利用Transformer克服了循环神经网络在捕捉长期序列方面的局限性，并采用Chebyshev多项式图卷积来捕捉复杂的空间依赖性。此外，考虑到时空异质性对交通速度的影响，我们设计了两个自监督学习任务来建模时空异质性，从而提高了模型的准确性和泛化能力。在两个实际数据集PeMS04和PeMS08上进行了实验评估，并进行了可视化和分析，展示了模型的效果。

    Long-term traffic prediction has always been a challenging task due to its dynamic temporal dependencies and complex spatial dependencies. In this paper, we propose a model that combines hybrid Transformer and spatio-temporal self-supervised learning. The model enhances its robustness by applying adaptive data augmentation techniques at the sequence-level and graph-level of the traffic data. It utilizes Transformer to overcome the limitations of recurrent neural networks in capturing long-term sequences, and employs Chebyshev polynomial graph convolution to capture complex spatial dependencies. Furthermore, considering the impact of spatio-temporal heterogeneity on traffic speed, we design two self-supervised learning tasks to model the temporal and spatial heterogeneity, thereby improving the accuracy and generalization ability of the model. Experimental evaluations are conducted on two real-world datasets, PeMS04 and PeMS08, and the results are visualized and analyzed, demonstrating 
    
[^99]: Context-Former：基于潜在条件序列建模的拼接技术

    Context-Former: Stitching via Latent Conditioned Sequence Modeling. (arXiv:2401.16452v1 [cs.LG])

    [http://arxiv.org/abs/2401.16452](http://arxiv.org/abs/2401.16452)

    Context-Former是一种集成了基于情境信息的模仿学习和序列建模的方法，通过拼接次优轨迹片段来改善决策，并提高了Decision Transformer的性能。

    

    离线强化学习（RL）算法可以通过拼接次优轨迹来改善决策，从而获得更优的结果。这种能力是使RL能够学习优于行为策略的策略的关键因素。另一方面，Decision Transformer（DT）将决策建模为序列建模，展示了在离线RL基准测试中竞争性的性能，然而，最近的研究表明DT缺乏拼接能力，因此提高DT性能需要利用拼接能力。为了赋予DT拼接能力，我们将轨迹拼接抽象为专家匹配，并引入了我们的方法ContextFormer，通过模拟有限数量的专家轨迹的表示来集成基于情境信息的模仿学习（IL）和序列建模，以拼接次优轨迹片段。为了验证我们的观点，我们从两个角度进行实验证明：

    Offline reinforcement learning (RL) algorithms can improve the decision making via stitching sub-optimal trajectories to obtain more optimal ones. This capability is a crucial factor in enabling RL to learn policies that are superior to the behavioral policy. On the other hand, Decision Transformer (DT) abstracts the decision-making as sequence modeling, showcasing competitive performance on offline RL benchmarks, however, recent studies demonstrate that DT lacks of stitching capability, thus exploit stitching capability for DT is vital to further improve its performance. In order to endow stitching capability to DT, we abstract trajectory stitching as expert matching and introduce our approach, ContextFormer, which integrates contextual information-based imitation learning (IL) and sequence modeling to stitch sub-optimal trajectory fragments by emulating the representations of a limited number of expert trajectories. To validate our claim, we conduct experiments from two perspectives:
    
[^100]: 能源数字化双胞胎中的人工智能: 基于强化学习的自适应数字双胞胎模型用于绿色城市

    AI in Energy Digital Twining: A Reinforcement Learning-based Adaptive Digital Twin Model for Green Cities. (arXiv:2401.16449v1 [cs.LG])

    [http://arxiv.org/abs/2401.16449](http://arxiv.org/abs/2401.16449)

    本研究提出了一种基于强化学习的自适应数字双胞胎模型，利用时空图和深度Q网络来支持动态的智能城市环境。实验结果表明，该模型在准确性、同步性、资源优化和能源效率方面具有明显的优势，同时在图数据库中实现时表现出更高的查询性能。此外，该模型还能够实现实时数据捕获，降低开销和能源消耗。

    

    数字双胞胎(DT)对于实现可持续和有效的智能城市解决方案至关重要。然而，当前的DT建模技术无法支持这些智能城市环境的动态性。传统方法中缺乏实时数据捕获导致建模不准确以及资源和能源消耗挑战。为了填补这一空白，我们探索了时空图，并提出了基于强化学习的自适应双胞胎（RL-AT）机制与深度Q网络（DQN）。通过这样做，我们的研究有助于推进绿色城市，并展示准确性、同步性、资源优化和能源效率等方面的实际好处。结果显示，当使用图数据库进行实现时，时空图能够提供一致的准确性和55%更高的查询性能。此外，我们的模型表现出实时数据捕获能力，开销降低20%，能源消耗降低25%。

    Digital Twins (DT) have become crucial to achieve sustainable and effective smart urban solutions. However, current DT modelling techniques cannot support the dynamicity of these smart city environments. This is caused by the lack of right-time data capturing in traditional approaches, resulting in inaccurate modelling and high resource and energy consumption challenges. To fill this gap, we explore spatiotemporal graphs and propose the Reinforcement Learning-based Adaptive Twining (RL-AT) mechanism with Deep Q Networks (DQN). By doing so, our study contributes to advancing Green Cities and showcases tangible benefits in accuracy, synchronisation, resource optimization, and energy efficiency. As a result, we note the spatiotemporal graphs are able to offer a consistent accuracy and 55% higher querying performance when implemented using graph databases. In addition, our model demonstrates right-time data capturing with 20% lower overhead and 25% lower energy consumption.
    
[^101]: OMPGPT: 一种用于OpenMP的生成式预训练Transformer模型

    OMPGPT: A Generative Pre-trained Transformer Model for OpenMP. (arXiv:2401.16445v1 [cs.SE])

    [http://arxiv.org/abs/2401.16445](http://arxiv.org/abs/2401.16445)

    OMPGPT是一种为了OpenMP pragma生成而设计的生成式预训练Transformer模型，采用了来自NLP领域的提示工程技术，并创建了一种创新的策略chain-of-OMP。

    

    大型语言模型（LLMs），如ChatGPT等模型，已经在自然语言处理领域引起了革命。随着这一趋势，基于代码的大型语言模型，如StarCoder、WizardCoder和CodeLlama等，已经涌现出来，在大量的代码数据库上进行了广泛的训练。然而，由于设计固有的原因，这些模型主要关注代码生成、代码完成和注释生成等生成任务，以及对多种编程语言的一般支持。虽然代码LLMs的通用能力对许多程序员来说很有用，但高性能计算（HPC）领域具有更窄的需求集，使得更小、更具领域特定的LM成为一个更明智的选择。本文介绍了OMPGPT，这是一种精心设计的新型模型，旨在充分利用语言模型在OpenMP pragma生成方面的固有优势。此外，我们采用并改进了来自NLP领域的提示工程技术，创建了链式OMP（chain-of-OMP），这是一种创新策略。

    Large language models (LLMs), as epitomized by models like ChatGPT, have revolutionized the field of natural language processing (NLP). Along with this trend, code-based large language models such as StarCoder, WizardCoder, and CodeLlama have emerged, trained extensively on vast repositories of code data. Yet, inherent in their design, these models primarily focus on generative tasks like code generation, code completion, and comment generation, and general support for multiple programming languages. While the generic abilities of code LLMs are useful for many programmers, the area of high-performance computing (HPC) has a narrower set of requirements that make a smaller and more domain-specific LM a smarter choice. This paper introduces OMPGPT, a novel model meticulously designed to harness the inherent strengths of language models for OpenMP pragma generation. Furthermore, we adopt and adapt prompt engineering techniques from the NLP domain to create chain-of-OMP, an innovative strat
    
[^102]: 评估深度网络用于通过手部交互检测用户对虚拟现实的熟悉程度

    Evaluating Deep Networks for Detecting User Familiarity with VR from Hand Interactions. (arXiv:2401.16443v1 [cs.HC])

    [http://arxiv.org/abs/2401.16443](http://arxiv.org/abs/2401.16443)

    通过使用深度分类器和手部追踪技术，本文提出了一种评估用户对虚拟现实熟悉程度的方法，以便在用户不熟悉虚拟现实时为其提供按需培训，从而提高其在虚拟环境中的任务完成效率。

    

    随着虚拟现实设备在消费领域的普及，对于对虚拟现实不熟悉的用户而言，虚拟现实应用的使用可能越来越普遍。检测用户对虚拟现实的熟悉程度作为交互媒介，具有通过提供按需培训进行适应和防止用户在完成任务时被虚拟现实环境所拖累的潜力。本文介绍了使用深度分类器进行自动检测用户对虚拟现实的熟悉程度的初步结果，通过用户使用手部与虚拟现实门锁数字密码输入面板进行交互来解锁虚拟现实门。我们将虚拟现实门作为企业虚拟空间的第一入口点，例如会议室、办公室或诊所。对于不熟悉虚拟现实的用户而言，在现实世界中已经使用过手部打开带有密码输入面板的门。因此，虽然用户可能对虚拟现实不熟悉，但他们对打开门的任务应该是熟悉的。使用 pilot d

    As VR devices become more prevalent in the consumer space, VR applications are likely to be increasingly used by users unfamiliar with VR. Detecting the familiarity level of a user with VR as an interaction medium provides the potential of providing on-demand training for acclimatization and prevents the user from being burdened by the VR environment in accomplishing their tasks. In this work, we present preliminary results of using deep classifiers to conduct automatic detection of familiarity with VR by using hand tracking of the user as they interact with a numeric passcode entry panel to unlock a VR door. We use a VR door as we envision it to the first point of entry to collaborative virtual spaces, such as meeting rooms, offices, or clinics. Users who are unfamiliar with VR will have used their hands to open doors with passcode entry panels in the real world. Thus, while the user may not be familiar with VR, they would be familiar with the task of opening the door. Using a pilot d
    
[^103]: FaKnow: 一个用于虚假新闻检测的统一库

    FaKnow: A Unified Library for Fake News Detection. (arXiv:2401.16441v1 [cs.LG])

    [http://arxiv.org/abs/2401.16441](http://arxiv.org/abs/2401.16441)

    FaKnow是一个统一的虚假新闻检测算法库，包含多种常用的模型和工具，并解决了不同框架下的可重复性和冗余问题。

    

    在过去的几年中，基于深度学习的大量虚假新闻检测算法应运而生。然而，它们往往在不同的框架下开发，每个框架又要求使用不同的方法，因此阻碍了可重复性。此外，这些虚假新闻检测模型的代码开发中存在大量的冗余。为了解决这些问题，我们提出了FaKnow，一个统一且全面的虚假新闻检测算法库。它涵盖了多种常用的虚假新闻检测模型，包括基于内容和基于社会环境的方法。该库涵盖了模型训练和评估流程的完整范围，在一个统一框架内有效组织了数据、模型和训练程序。此外，它还提供了一系列辅助功能和工具，包括可视化和日志记录。我们的工作为虚假新闻检测的标准化和统一化做出了贡献。

    Over the past years, a large number of fake news detection algorithms based on deep learning have emerged. However, they are often developed under different frameworks, each mandating distinct utilization methodologies, consequently hindering reproducibility. Additionally, a substantial amount of redundancy characterizes the code development of such fake news detection models. To address these concerns, we propose FaKnow, a unified and comprehensive fake news detection algorithm library. It encompasses a variety of widely used fake news detection models, categorized as content-based and social context-based approaches. This library covers the full spectrum of the model training and evaluation process, effectively organizing the data, models, and training procedures within a unified framework. Furthermore, it furnishes a series of auxiliary functionalities and tools, including visualization, and logging. Our work contributes to the standardization and unification of fake news detection 
    
[^104]: 超越驱逐预测：利用当地时空公共记录来指导行动

    Beyond Eviction Prediction: Leveraging Local Spatiotemporal Public Records to Inform Action. (arXiv:2401.16440v1 [cs.LG])

    [http://arxiv.org/abs/2401.16440](http://arxiv.org/abs/2401.16440)

    该研究利用当地时空公共记录来预测驱逐风险，并证明这些预测对于指导有针对性的外展政策是有用的。

    

    近年来，基于驱逐风险对房产进行评分引起了相当大的关注。驱逐预测方法的成功通常是通过不同的预测准确度指标来评估的。然而，这种预测的根本目标是为了向可能面临更大风险的家庭提供适当的帮助，以保持住房稳定。因此，我们必须问一个问题，那就是这样的预测在指导外展行动方面有多大的用处。本文利用一个新颖的数据集，将房产、驱逐和业主的信息进行匹配，研究这个问题。我们进行了一项驱逐预测任务，生成风险得分，然后利用这些风险得分来规划有针对性的外展政策。我们显示这些风险得分实际上是有用的，能够使一个理论上的工作人员团队在相同的时间内接触更多容易发生驱逐的房产，相比于以街区为基础或关注特定建筑物的外展政策。

    There has been considerable recent interest in scoring properties on the basis of eviction risk. The success of methods for eviction prediction is typically evaluated using different measures of predictive accuracy. However, the underlying goal of such prediction is to direct appropriate assistance to households that may be at greater risk so they remain stably housed. Thus, we must ask the question of how useful such predictions are in targeting outreach efforts - informing action. In this paper, we investigate this question using a novel dataset that matches information on properties, evictions, and owners. We perform an eviction prediction task to produce risk scores and then use these risk scores to plan targeted outreach policies. We show that the risk scores are, in fact, useful, enabling a theoretical team of caseworkers to reach more eviction-prone properties in the same amount of time, compared to outreach policies that are either neighborhood-based or focus on buildings with 
    
[^105]: 多项式时间下对高斯数据进行统计子组不公平性审计的研究

    Polynomial time auditing of statistical subgroup fairness for Gaussian data. (arXiv:2401.16439v1 [cs.LG])

    [http://arxiv.org/abs/2401.16439](http://arxiv.org/abs/2401.16439)

    这篇论文研究了使用统计子组不公平性概念对分类器进行审计的问题，并给出了对高斯分布的审计结果。他们提供了一种替代方法来利用无偏学习的进展。

    

    我们研究了使用统计子组不公平性概念对分类器进行审计的问题。Kearns等人（2018）已经表明，审计组合子组公平性的问题与无偏学习一样困难。尽管对于这个问题没有已知的高效算法，但几乎所有解决对子组的统计歧视度量的工作都假设可以访问此问题的预言机。如果我们假设数据分布是高斯分布，甚至仅是寻常对数凹曲分布，那么最近的一系列工作已经发现了高效的无偏学习算法来学习半空间。不幸的是，Kearns等人给出的提升风格的规约要求无偏学习算法在可能不是寻常对数凹的重新加权分布上成功，即使原始数据分布是寻常对数凹的。在这项工作中，我们对高斯分布的审计结果给出了正面和负面的结果：在正面方面，我们提供了一个替代方法来利用这些无偏学习的进展。

    We study the problem of auditing classifiers with the notion of statistical subgroup fairness. Kearns et al. (2018) has shown that the problem of auditing combinatorial subgroups fairness is as hard as agnostic learning. Essentially all work on remedying statistical measures of discrimination against subgroups assumes access to an oracle for this problem, despite the fact that no efficient algorithms are known for it. If we assume the data distribution is Gaussian, or even merely log-concave, then a recent line of work has discovered efficient agnostic learning algorithms for halfspaces. Unfortunately, the boosting-style reductions given by Kearns et al. required the agnostic learning algorithm to succeed on reweighted distributions that may not be log-concave, even if the original data distribution was. In this work, we give positive and negative results on auditing for the Gaussian distribution: On the positive side, we an alternative approach to leverage these advances in agnostic l
    
[^106]: 深度神经网络是否高效利用了权重空间？

    Do deep neural networks utilize the weight space efficiently?. (arXiv:2401.16438v1 [cs.LG])

    [http://arxiv.org/abs/2401.16438](http://arxiv.org/abs/2401.16438)

    该论文介绍了一种利用权重矩阵的列空间和行空间的新概念，可以大幅减少深度学习模型的参数而不影响性能。实验证明该方法能够在资源有限的情况下实现参数高效的深度学习模型，并在ImageNet数据集上展现了竞争性的性能。

    

    深度学习模型如Transformer和卷积神经网络（CNN）已经在各个领域引起了革命，但是它们参数密集的特性限制了在资源有限的情况下的应用。在本文中，我们引入一种新的概念，利用权重矩阵的列空间和行空间，可以大幅减少模型参数而不影响性能。利用这种范式，我们实现了参数高效的深度学习模型。我们的方法适用于瓶颈层和注意力层，可以将参数减半，仅带来轻微的性能降低。我们在ImageNet数据集上使用ViT和ResNet50进行了大量实验，证明了我们方法的有效性，在与传统模型的比较中展示了竞争性的性能。这种方法不仅解决了对参数高效的深度学习解决方案的紧迫需求，而且在真实场景的实际部署中具有巨大的潜力。

    Deep learning models like Transformers and Convolutional Neural Networks (CNNs) have revolutionized various domains, but their parameter-intensive nature hampers deployment in resource-constrained settings. In this paper, we introduce a novel concept utilizes column space and row space of weight matrices, which allows for a substantial reduction in model parameters without compromising performance. Leveraging this paradigm, we achieve parameter-efficient deep learning models.. Our approach applies to both Bottleneck and Attention layers, effectively halving the parameters while incurring only minor performance degradation. Extensive experiments conducted on the ImageNet dataset with ViT and ResNet50 demonstrate the effectiveness of our method, showcasing competitive performance when compared to traditional models. This approach not only addresses the pressing demand for parameter efficient deep learning solutions but also holds great promise for practical deployment in real-world scena
    
[^107]: 用全分辨率极化天气雷达数据进行龙卷风检测和预测的基准数据集

    A Benchmark Dataset for Tornado Detection and Prediction using Full-Resolution Polarimetric Weather Radar Data. (arXiv:2401.16437v1 [physics.ao-ph])

    [http://arxiv.org/abs/2401.16437](http://arxiv.org/abs/2401.16437)

    本研究提出了一个新的基准数据集TorNet，用于支持龙卷风检测和预测中的机器学习算法的发展。该数据集包含了十年的具有全分辨率和极化特性的天气雷达数据，为训练和评估机器学习算法提供了重要的资源。

    

    天气雷达是检测和预警龙卷风的主要工具。为了帮助预报员警告公众，已开发了几种算法以自动检测天气雷达观测中的龙卷风迹象。最近，机器学习（ML）算法直接从大量标记数据中学习，已被证明对此目的非常有效。由于龙卷风在所有可用雷达观测数据中非常罕见，因此选择和设计用于ML应用的训练数据集对于ML算法的性能、稳健性和最终接受度至关重要。本研究介绍了一个新的基准数据集TorNet，以支持龙卷风检测和预测中的ML算法的开发。TorNet包含从10年报告的风暴事件中采样的全分辨率、极化的Level-II WSR-88D数据。开发了许多龙卷风检测的ML基准，并进行了比较。

    Weather radar is the primary tool used by forecasters to detect and warn for tornadoes in near-real time. In order to assist forecasters in warning the public, several algorithms have been developed to automatically detect tornadic signatures in weather radar observations. Recently, Machine Learning (ML) algorithms, which learn directly from large amounts of labeled data, have been shown to be highly effective for this purpose. Since tornadoes are extremely rare events within the corpus of all available radar observations, the selection and design of training datasets for ML applications is critical for the performance, robustness, and ultimate acceptance of ML algorithms. This study introduces a new benchmark dataset, TorNet to support development of ML algorithms in tornado detection and prediction. TorNet contains full-resolution, polarimetric, Level-II WSR-88D data sampled from 10 years of reported storm events. A number of ML baselines for tornado detection are developed and compa
    
[^108]: 基于ANROA的多功能并网太阳能转换系统的新控制方法

    A novel ANROA based control approach for grid-tied multi-functional solar energy conversion system. (arXiv:2401.16434v1 [eess.SY])

    [http://arxiv.org/abs/2401.16434](http://arxiv.org/abs/2401.16434)

    本文提出了一种基于ANROA方法的多功能并网太阳能转换系统的自适应控制方法，使用了自适应神经模糊推理系统和Rain优化算法，并成功实现了电力质量问题的避免和单位功率因数运行模式。

    

    本文提出并讨论了一种基于新的神经模糊推理系统与Rain优化算法（ANROA）方法的三相并网太阳能光伏系统的自适应控制方法。该方法将自适应神经模糊推理系统（ANFIS）与Rain优化算法（ROA）结合起来，具有良好的最大跟踪能力。ROA技术负责控制电压源变换器的开关，以避免电压波动、谐波和闪烁等电力质量问题，同时实现负载平衡和无功功率使用的目标。此外，所提出的方法可以在零电压调节和单位功率因数模式下运行。该方法已经建模和仿真，并使用现有的替代方法进行了性能评估。

    An adaptive control approach for a three-phase grid-interfaced solar photovoltaic system based on the new Neuro-Fuzzy Inference System with Rain Optimization Algorithm (ANROA) methodology is proposed and discussed in this manuscript. This method incorporates an Adaptive Neuro-fuzzy Inference System (ANFIS) with a Rain Optimization Algorithm (ROA). The ANFIS controller has excellent maximum tracking capability because it includes features of both neural and fuzzy techniques. The ROA technique is in charge of controlling the voltage source converter switching. Avoiding power quality problems including voltage fluctuations, harmonics, and flickers as well as unbalanced loads and reactive power usage is the major goal. Besides, the proposed method performs at zero voltage regulation and unity power factor modes. The suggested control approach has been modeled and simulated, and its performance has been assessed using existing alternative methods. A statistical analysis of proposed and exis
    
[^109]: 通过神经模式关联器进行篮内推荐

    Within-basket Recommendation via Neural Pattern Associator. (arXiv:2401.16433v1 [cs.IR])

    [http://arxiv.org/abs/2401.16433](http://arxiv.org/abs/2401.16433)

    本文介绍了一种称为神经模式关联器（NPA）的深度商品关联挖掘模型，该模型能够明确地建模购物过程中的复杂用户行为，并通过注意力驱动的查找来识别用户的购物意图。

    

    篮内推荐（WBR）是指在购物过程中为了完成一个非空购物篮而推荐商品的任务。尽管这个领域的最新创新在基准数据集上表现出了显著的性能提升，但它们常常忽视了实际用户行为的复杂性，比如1）多个购物意图的共存，2）这些意图的多粒度和3）购物过程中的交织行为（切换意图）。本文提出了一种名为神经模式关联器（NPA）的深度商品关联挖掘模型，明确地建模了上述因素。具体来说，受到向量量化的启发，NPA模型学习将常见的用户意图（或商品组合模式）编码为量化表示（也称为码本），这允许在推理阶段通过注意力驱动的查找来识别用户的购物意图。这样产生的推荐结果连贯且自解释。

    Within-basket recommendation (WBR) refers to the task of recommending items to the end of completing a non-empty shopping basket during a shopping session. While the latest innovations in this space demonstrate remarkable performance improvement on benchmark datasets, they often overlook the complexity of user behaviors in practice, such as 1) co-existence of multiple shopping intentions, 2) multi-granularity of such intentions, and 3) interleaving behavior (switching intentions) in a shopping session. This paper presents Neural Pattern Associator (NPA), a deep item-association-mining model that explicitly models the aforementioned factors. Specifically, inspired by vector quantization, the NPA model learns to encode common user intentions (or item-combination patterns) as quantized representations (a.k.a. codebook), which permits identification of users's shopping intentions via attention-driven lookup during the reasoning phase. This yields coherent and self-interpretable recommendat
    
[^110]: 在在线广告中通过自监督预训练改进转化率预测

    Improving conversion rate prediction via self-supervised pre-training in online advertising. (arXiv:2401.16432v1 [cs.IR])

    [http://arxiv.org/abs/2401.16432](http://arxiv.org/abs/2401.16432)

    这项研究通过自监督预训练方法，改进了在线广告系统中的转化率预测。由于数据稀疏性的挑战，添加非点击归因的转化会损坏模型的校准，而自监督预训练能够解决这个问题。

    

    预测转化率是在线广告系统中优化投标以满足广告主性能要求的关键任务。尽管深度神经网络的崛起，但这些预测通常由分解机（FM）进行，特别是在推理延迟至关重要的商业环境中。这些模型使用逻辑回归框架训练，利用与任务相关的过去用户活动形成的标记表格数据。许多广告主只关心被点击属性的转化。预测给定点击的转化模型训练的主要挑战来自数据稀疏性 - 点击很少，点击归因的转化更少。然而，在训练集中添加非点击归因的转化来减轻稀疏性会损坏模型的校准。由于校准对实现广告主目标至关重要，这是不可行的。在这项工作中，我们使用了自监督预训练的众所周知的思想来解决这个问题。

    The task of predicting conversion rates (CVR) lies at the heart of online advertising systems aiming to optimize bids to meet advertiser performance requirements. Even with the recent rise of deep neural networks, these predictions are often made by factorization machines (FM), especially in commercial settings where inference latency is key. These models are trained using the logistic regression framework on labeled tabular data formed from past user activity that is relevant to the task at hand.  Many advertisers only care about click-attributed conversions. A major challenge in training models that predict conversions-given-clicks comes from data sparsity - clicks are rare, conversions attributed to clicks are even rarer. However, mitigating sparsity by adding conversions that are not click-attributed to the training set impairs model calibration. Since calibration is critical to achieving advertiser goals, this is infeasible.  In this work we use the well-known idea of self-supervi
    
[^111]: 结合主题建模和引用网络分析研究欧洲人权法院关于尊重私人和家庭生活权利的案例法

    Combining topic modelling and citation network analysis to study case law from the European Court on Human Rights on the right to respect for private and family life. (arXiv:2401.16429v1 [cs.IR])

    [http://arxiv.org/abs/2401.16429](http://arxiv.org/abs/2401.16429)

    本文研究了结合主题建模和引用网络分析的方法，用来研究欧洲人权法院关于尊重私密和家庭生活的案例法。通过这种方法，可以找到和组织具有相似主题和引用模式的案例法，并且通过结合这两种技术能够得到更好的结果。

    

    随着HUDOC等法律案例法数据库的快速增长，为了处理如此大规模的数据集，法律研究人员找到高效的方法变得至关重要。这种案例法数据库通常包含案件的文本内容以及它们之间的引用。本文重点研究了来自欧洲人权法院关于欧洲人权公约第8条关于尊重私人和家庭生活、家庭和通信权利的案例法。在本研究中，我们演示并比较了主题建模和引用网络在根据一般主题和引用模式找到和组织第8条案例法方面的潜力。另外，我们还探索了结合这两种技术是否比仅应用其中一种方法效果更好。我们在一组手工收集和注释的关于驱逐的第8条案例法独特数据集上评估了组合方法的有效性。我们的结果表明，结合使用这两种方法能够取得更好的效果。

    As legal case law databases such as HUDOC continue to grow rapidly, it has become essential for legal researchers to find efficient methods to handle such large-scale data sets. Such case law databases usually consist of the textual content of cases together with the citations between them. This paper focuses on case law from the European Court of Human Rights on Article 8 of the European Convention of Human Rights, the right to respect private and family life, home and correspondence. In this study, we demonstrate and compare the potential of topic modelling and citation network to find and organize case law on Article 8 based on their general themes and citation patterns, respectively. Additionally, we explore whether combining these two techniques leads to better results compared to the application of only one of the methods. We evaluate the effectiveness of the combined method on a unique manually collected and annotated dataset of Aricle 8 case law on evictions. The results of our
    
[^112]: 通过从部分模拟中推断出的方式，为模拟优化器提供非正式的安全保证

    Informal Safety Guarantees for Simulated Optimizers Through Extrapolation from Partial Simulations. (arXiv:2401.16426v1 [cs.LG])

    [http://arxiv.org/abs/2401.16426](http://arxiv.org/abs/2401.16426)

    通过从部分模拟中推断出的方式，为模拟优化器提供非正式的安全保证。

    

    自我监督学习是现代语言模型的基础。有人认为，在自我监督数据集上使用预测损失进行训练会导致内部表示真实世界系统可能配置的实体，即模拟器。在这个假设下，基于嵌入式代理的笛卡尔坐标系模型构建了模拟器的数学模型，并通过将二维坐标系扩展到任意维度来实现多代理世界，先前的文献选择使用在坐标系上操作的方法。这种利用尺度尺寸变化的变体被称为笛卡尔对象，并用于表示模拟（其中个体的模拟是该对象中的代理和设备）。在笛卡尔对象周围，通过考虑令牌选择和模拟复杂性，对模拟器的行为进行形式化，并使用Lobian障碍证明了通过检查模拟之间的一致性的对齐证明。

    Self-supervised learning is the backbone of state of the art language modeling. It has been argued that training with predictive loss on a self-supervised dataset causes simulators: entities that internally represent possible configurations of real-world systems. Under this assumption, a mathematical model for simulators is built based in the Cartesian frames model of embedded agents, which is extended to multi-agent worlds through scaling a two-dimensional frame to arbitrary dimensions, where literature prior chooses to instead use operations on frames. This variant leveraging scaling dimensionality is named the Cartesian object, and is used to represent simulations (where individual simulacra are the agents and devices in that object). Around the Cartesian object, functions like token selection and simulation complexity are accounted for in formalizing the behavior of a simulator, and used to show (through the L\"obian obstacle) that a proof of alignment between simulacra by inspecti
    
[^113]: cDVGAN: 一个灵活的模型用于多类引力波信号和故障生成

    cDVGAN: One Flexible Model for Multi-class Gravitational Wave Signal and Glitch Generation. (arXiv:2401.16356v2 [physics.ins-det] UPDATED)

    [http://arxiv.org/abs/2401.16356](http://arxiv.org/abs/2401.16356)

    cDVGAN是一个灵活的生成对抗网络模型，用于模拟多类引力波信号和探测器故障，并通过引入辅助鉴别器分析一阶导数时间序列来更好地捕捉原始数据特征。

    

    模拟真实的时间域引力波（GWs）观测和GW探测器故障可以帮助推进GW数据分析。模拟数据可以通过增加用于信号搜索的数据集，平衡用于机器学习的数据集，以及验证检测方案，在下游任务中使用。在这项工作中，我们提出了cDVGAN，这是一种基于生成对抗网络框架的新型条件模型，用于模拟代表引力波（GWs）和探测器故障的多种类别的时间域观测。cDVGAN还可以通过在条件类别向量中进行插值生成跨类别变化的广义混合样本。cDVGAN在典型的GANs的二人对抗博弈中引入了一个额外的参与者，其中一个辅助鉴别器分析一阶导数时间序列。我们的结果表明，这提供了更好地捕捉原始数据特征的合成数据。

    Simulating realistic time-domain observations of gravitational waves (GWs) and GW detector glitches can help in advancing GW data analysis. Simulated data can be used in downstream tasks by augmenting datasets for signal searches, balancing data sets for machine learning, and validating detection schemes. In this work, we present Conditional Derivative GAN (cDVGAN), a novel conditional model in the Generative Adversarial Network framework for simulating multiple classes of time-domain observations that represent gravitational waves (GWs) and detector glitches. cDVGAN can also generate generalized hybrid samples that span the variation between classes through interpolation in the conditioned class vector. cDVGAN introduces an additional player into the typical 2-player adversarial game of GANs, where an auxiliary discriminator analyzes the first-order derivative time-series. Our results show that this provides synthetic data that better captures the features of the original data. cDVGAN
    
[^114]: 跨领域联合学习中基于记录级个性化差分隐私的研究

    Cross-silo Federated Learning with Record-level Personalized Differential Privacy. (arXiv:2401.16251v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2401.16251](http://arxiv.org/abs/2401.16251)

    本文研究了跨领域联合学习中基于记录级个性化差分隐私的问题，设计了一个名为rPDP-FL的新型框架，并提出了多功能解决方案“模拟-曲线拟合”，以满足不同记录的隐私需求。

    

    基于差分隐私增强的联合学习成为了保护客户端数据隐私的常用方法，但现有方案通常假设所有记录的隐私预算均相同，提供一种适用于所有记录的通用解决方案，可能无法满足每个记录的隐私需求。本文探讨了跨领域联合学习中基于记录级个性化差分隐私的未知领域。我们设计了一个名为rPDP-FL的新型框架，采用两阶段混合抽样方案，既包括客户端级别抽样，又包括非均匀记录级别抽样，以适应不同的隐私需求。一个关键且非平凡的问题是在给定个性化隐私预算ε的情况下选择理想的每记录抽样概率q。我们提出了一个多功能解决方案“模拟-曲线拟合”，使我们能够揭示非线性相关性的重要见解。

    Federated learning enhanced by differential privacy has emerged as a popular approach to better safeguard the privacy of client-side data by protecting clients' contributions during the training process. Existing solutions typically assume a uniform privacy budget for all records and provide one-size-fits-all solutions that may not be adequate to meet each record's privacy requirement. In this paper, we explore the uncharted territory of cross-silo FL with record-level personalized differential privacy. We devise a novel framework named rPDP-FL, employing a two-stage hybrid sampling scheme with both client-level sampling and non-uniform record-level sampling to accommodate varying privacy requirements. A critical and non-trivial problem is to select the ideal per-record sampling probability q given the personalized privacy budget {\epsilon}. We introduce a versatile solution named Simulation-CurveFitting, allowing us to uncover a significant insight into the nonlinear correlation betwe
    
[^115]: 事件序列的自我监督学习：生成建模和对比学习的比较研究和混合方法的应用

    Self-Supervised Learning in Event Sequences: A Comparative Study and Hybrid Approach of Generative Modeling and Contrastive Learning. (arXiv:2401.15935v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.15935](http://arxiv.org/abs/2401.15935)

    本研究通过比较研究和混合方法，调查了事件序列的自我监督学习技术，并引入了一种新的方法，将生成模型和对比嵌入进行对齐。结果显示，这种对齐模型在各种任务上表现优越，为预测事件序列中的信息提供了潜在的好处。

    

    本研究调查了获取事件序列表示的自我监督学习技术。这是各种应用中的关键模态，包括但不限于银行、电子商务和医疗保健。我们对自我监督学习中的生成模型和对比方法进行了全面的研究，并分别应用了它们。我们发现没有一种绝对优越的方法。因此，我们探讨了结合这些方法的潜在好处。为了实现这个目标，我们引入了一种新的方法，将生成模型和对比嵌入作为不同的模态进行对齐，从当代多模态研究中汲取灵感。生成模型和对比方法通常被视为互斥的，因此存在它们的联合探索的空白。我们的结果表明，这种对齐模型在至少与现有方法持平，并且在各种任务上更加普适。此外，我们证明了自我监督学习在预测事件序列中包含的信息方面的潜力。

    This study investigates self-supervised learning techniques to obtain representations of Event Sequences. It is a key modality in various applications, including but not limited to banking, e-commerce, and healthcare.  We perform a comprehensive study of generative and contrastive approaches in self-supervised learning, applying them both independently. We find that there is no single supreme method. Consequently, we explore the potential benefits of combining these approaches. To achieve this goal, we introduce a novel method that aligns generative and contrastive embeddings as distinct modalities, drawing inspiration from contemporary multimodal research.  Generative and contrastive approaches are often treated as mutually exclusive, leaving a gap for their combined exploration. Our results demonstrate that this aligned model performs at least on par with, and mostly surpasses, existing methods and is more universal across a variety of tasks. Furthermore, we demonstrate that self-sup
    
[^116]: 贝叶斯非参数方法与数据驱动鲁棒优化的结合

    Bayesian Nonparametrics meets Data-Driven Robust Optimization. (arXiv:2401.15771v1 [stat.ML])

    [http://arxiv.org/abs/2401.15771](http://arxiv.org/abs/2401.15771)

    本文提出了一种将贝叶斯非参数方法与最新的决策理论模型相结合的鲁棒优化准则，通过这种方法，可以在线性回归问题中获得有稳定性和优越性能的结果。

    

    训练机器学习和统计模型通常涉及优化数据驱动的风险准则。风险通常是根据经验数据分布计算的，但由于分布不确定性，这可能导致性能不稳定和不好的样本外表现。在分布鲁棒优化的精神下，我们提出了一个新颖的鲁棒准则，将贝叶斯非参数（即狄利克雷过程）理论和最近的平滑模糊规避偏好的决策理论模型的见解相结合。首先，我们强调了与标准正则化经验风险最小化技术的新连接，其中包括岭回归和套索回归。然后，我们从理论上证明了鲁棒优化过程在有限样本和渐近统计保证方面的有利性存在。对于实际实施，我们提出并研究了基于众所周知的狄利克雷过程表示的可行近似准则。

    Training machine learning and statistical models often involves optimizing a data-driven risk criterion. The risk is usually computed with respect to the empirical data distribution, but this may result in poor and unstable out-of-sample performance due to distributional uncertainty. In the spirit of distributionally robust optimization, we propose a novel robust criterion by combining insights from Bayesian nonparametric (i.e., Dirichlet Process) theory and recent decision-theoretic models of smooth ambiguity-averse preferences. First, we highlight novel connections with standard regularized empirical risk minimization techniques, among which Ridge and LASSO regressions. Then, we theoretically demonstrate the existence of favorable finite-sample and asymptotic statistical guarantees on the performance of the robust optimization procedure. For practical implementation, we propose and study tractable approximations of the criterion based on well-known Dirichlet Process representations. 
    
[^117]: FDR控制的稀疏金融指数跟踪投资组合优化

    FDR-Controlled Portfolio Optimization for Sparse Financial Index Tracking. (arXiv:2401.15139v1 [q-fin.PM])

    [http://arxiv.org/abs/2401.15139](http://arxiv.org/abs/2401.15139)

    本论文提出了一种扩展的T-Rex框架，用于在稀疏金融指数跟踪中选择少数相关变量，并通过集成最近邻惩罚机制，可靠控制误发现率（FDR）。实验证明了该方法在过去20年内基于少量股票准确跟踪标准普尔500指数的能力。

    

    在高维数据分析中，如金融指数跟踪或生物医学应用中，关键是在保持对误发现率（FDR）的控制的同时选择少数相关变量。在这些应用中，变量之间经常存在强依赖关系（例如股票收益），这可能会削弱现有方法（如模型X knockoff方法或T-Rex选择器）的FDR控制特性。为了解决这个问题，我们扩展了T-Rex框架，以适应高度相关变量的重叠组。这是通过将最近邻惩罚机制集成到框架中实现的，该机制能够在用户定义的目标水平上可靠控制FDR。稀疏指数跟踪的实例展示了该方法在过去20年内基于少量股票准确跟踪标准普尔500指数的能力。在CRAN上提供了R包TRexSelector的开源实现。

    In high-dimensional data analysis, such as financial index tracking or biomedical applications, it is crucial to select the few relevant variables while maintaining control over the false discovery rate (FDR). In these applications, strong dependencies often exist among the variables (e.g., stock returns), which can undermine the FDR control property of existing methods like the model-X knockoff method or the T-Rex selector. To address this issue, we have expanded the T-Rex framework to accommodate overlapping groups of highly correlated variables. This is achieved by integrating a nearest neighbors penalization mechanism into the framework, which provably controls the FDR at the user-defined target level. A real-world example of sparse index tracking demonstrates the proposed method's ability to accurately track the S&P 500 index over the past 20 years based on a small number of stocks. An open-source implementation is provided within the R package TRexSelector on CRAN.
    
[^118]: 通过GPT引导的蒙特卡洛树搜索从数据中发现数学公式

    Discovering Mathematical Formulas from Data via GPT-guided Monte Carlo Tree Search. (arXiv:2401.14424v1 [cs.LG])

    [http://arxiv.org/abs/2401.14424](http://arxiv.org/abs/2401.14424)

    通过结合MCTS和生成式预训练模型，我们提出了一种新的符号回归算法SR-GPT，在发现数据中的数学公式方面取得了显著的改进。

    

    在科学研究和人工智能中，找到一个简洁且可解释的数学公式来准确描述数据中每个变量与预测值之间的关系是一个关键任务，也是一个重大挑战。这个问题被称为符号回归，是一个NP困难问题。去年，提出了一种基于蒙特卡洛树搜索（MCTS）的符号回归方法，并在多个数据集上获得了sota。虽然与以前的方法相比，该算法在恢复目标表达式方面显示出了相当大的改进，但是在MCTS过程中缺乏引导严重阻碍了其搜索效率。最近，一些算法在MCTS的搜索中添加了一个预训练的策略网络，但是这个预训练的策略网络的泛化能力很差。为了平衡效率和通用性，我们提出了SR-GPT，结合了AlphaZero的思想。SR-GPT是一种新的符号回归算法，将MCTS与一个通用性较好的生成式预训练模型相结合。

    Finding a concise and interpretable mathematical formula that accurately describes the relationship between each variable and the predicted value in the data is a crucial task in scientific research, as well as a significant challenge in artificial intelligence. This problem is referred to as symbolic regression, which is an NP-hard problem. Last year, a symbolic regression method based on Monte Carlo Tree Search (MCTS) was proposed and sota was obtained on multiple datasets. While this algorithm has shown considerable improvement in recovering target expressions compared to previous methods, the lack of guidance during the MCTS process severely hampers its search efficiency. Recently, some algorithms have added a pre-trained policy network to guide the search of MCTS, but the pre-trained policy network generalizes poorly. To balance efficiency and generality, we propose SR-GPT combining ideas from AlphaZero. SR-GPT is a new symbolic regression algorithm that combines MCTS with a Gener
    
[^119]: 提示设计与工程：介绍与高级方法

    Prompt Design and Engineering: Introduction and Advanced Methods. (arXiv:2401.14423v1 [cs.SE])

    [http://arxiv.org/abs/2401.14423](http://arxiv.org/abs/2401.14423)

    本文介绍了提示设计与工程的主要概念，并回顾了基本和更高级的方法。

    

    提示设计与工程在过去几个月中成为了一个重要的学科。在本文中，我们介绍了主要概念，并回顾了提示设计与工程的基本和更高级的方法。

    Prompt design and engineering has become an important discipline in just the past few months. In this paper, we provide an introduction to the main concepts as well as review basic and more advanced approaches to prompt design and engineering.
    
[^120]: 研究大型语言模型在代码克隆检测方面的功效

    Investigating the Efficacy of Large Language Models for Code Clone Detection. (arXiv:2401.13802v1 [cs.SE])

    [http://arxiv.org/abs/2401.13802](http://arxiv.org/abs/2401.13802)

    这项研究探索了大型语言模型在代码克隆检测任务中的应用。

    

    大型语言模型（LLMs）在各种自然语言处理和软件工程任务中表现出了显著的成功，例如代码生成。LLMs主要在基于提示的零/少样本范式中被用于指导模型完成任务。本研究探索了LLMs在代码克隆检测（CCD）这一非生成任务中的适用性。

    Large Language Models (LLMs) have demonstrated remarkable success in various natural language processing and software engineering tasks, such as code generation. The LLMs are mainly utilized in the prompt-based zero/few-shot paradigm to guide the model in accomplishing the task. %\textbf{Goal:} GPT-based models are one of the popular ones studied for tasks such as code comment generation or test generation. These tasks are `generative' tasks. However, there is limited research on the usage of LLMs for `non-generative' tasks such as classification using the prompt-based paradigm. In this preliminary exploratory study, we investigated the applicability of LLMs for Code Clone Detection (CCD), a non-generative task. %\textbf{Method:} By building a mono-lingual and cross-lingual CCD dataset derived from CodeNet, we first investigated two different prompts using ChatGPT to detect \textcolor{black}{Type-4} code clones in Java-Java and Java-Ruby pairs in a zero-shot setting. We \textcolor{blac
    
[^121]: 上下文语言学习：架构与算法

    In-Context Language Learning: Architectures and Algorithms. (arXiv:2401.12973v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.12973](http://arxiv.org/abs/2401.12973)

    本文通过研究一个新的问题家族——上下文语言学习（ICLL），探讨了大规模神经语言模型在上下文学习中的能力。在ICLL中，模型通过生成与给定形式语言相同的字符串来进行上下文学习。研究结果对于理解真实场景中的上下文学习以及神经语言模型的发展具有重要意义。

    

    大规模神经语言模型展现了在上下文学习中令人惊叹的能力：它们能够从输入的数据集中推断出新的函数。目前，我们对于上下文学习何时以及如何发生的了解主要来自于在极其简单的学习问题上训练的语言模型，例如线性回归和关联记忆。然而，这些模型问题与在大型文本语料库上训练的语言模型展现的“真正”上下文学习之间存在显著差距，后者不仅涉及检索和函数近似，还包括了自由生成语言和其他结构化输出。本文通过研究一个被称为上下文语言学习（ICLL）的新型问题家族，来探讨上下文学习。在ICLL中，语言模型被呈现一组来自形式语言的字符串，并需要生成与该语言相同的其他字符串。我们重点研究了通过随机有限自动机生成的正则语言的上下文学习。我们评估了多种神经序列模型（包括几种常用的模型）。

    Large-scale neural language models exhibit a remarkable capacity for in-context learning (ICL): they can infer novel functions from datasets provided as input. Most of our current understanding of when and how ICL arises comes from LMs trained on extremely simple learning problems like linear regression and associative recall. There remains a significant gap between these model problems and the "real" ICL exhibited by LMs trained on large text corpora, which involves not just retrieval and function approximation but free-form generation of language and other structured outputs. In this paper, we study ICL through the lens of a new family of model problems we term in context language learning (ICLL). In ICLL, LMs are presented with a set of strings from a formal language, and must generate additional strings from the same language. We focus on in-context learning of regular languages generated by random finite automata. We evaluate a diverse set of neural sequence models (including seve
    
[^122]: 基于一致性增强的深度多视图聚类方法通过对比学习

    Consistency Enhancement-Based Deep Multiview Clustering via Contrastive Learning. (arXiv:2401.12648v1 [cs.LG])

    [http://arxiv.org/abs/2401.12648](http://arxiv.org/abs/2401.12648)

    本文提出了一种基于一致性增强的深度多视图聚类方法通过对比学习（CCEC）。该方法通过引入语义连接块并入特征表示中，以保持多个视图间的一致信息，并通过谱聚类改善聚类的表示过程。实验结果显示，该方法在多个数据集上的表现优于其他现有方法。

    

    多视图聚类（MVC）通过综合多个视图的信息，将数据样本分为有意义的聚类。而基于深度学习的方法在MVC场景中展现了强大的特征学习能力。然而，有效地泛化特征表示并保持一致性仍然是一个棘手的问题。此外，大多数基于对比学习的现有深度聚类方法在聚类过程中忽略了聚类表示的一致性。本文展示了如何解决上述问题，并提出了一种通过对比学习的一致增强型深度MVC方法（CCEC）。具体而言，将语义连接块并入特征表示中，以保持多个视图间的一致信息。此外，通过谱聚类改善聚类的表示过程，并提高了多个视图间的一致性。实验结果显示，我们的方法在多个数据集上的表现优于其他现有方法。

    Multiview clustering (MVC) segregates data samples into meaningful clusters by synthesizing information across multiple views. Moreover, deep learning-based methods have demonstrated their strong feature learning capabilities in MVC scenarios. However, effectively generalizing feature representations while maintaining consistency is still an intractable problem. In addition, most existing deep clustering methods based on contrastive learning overlook the consistency of the clustering representations during the clustering process. In this paper, we show how the above problems can be overcome and propose a consistent enhancement-based deep MVC method via contrastive learning (CCEC). Specifically, semantic connection blocks are incorporated into a feature representation to preserve the consistent information among multiple views. Furthermore, the representation process for clustering is enhanced through spectral clustering, and the consistency across multiple views is improved. Experiment
    
[^123]: Tensor视图拓扑图神经网络

    Tensor-view Topological Graph Neural Network. (arXiv:2401.12007v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.12007](http://arxiv.org/abs/2401.12007)

    提出了一种新颖的Tensor视图拓扑图神经网络（TTG-NN），该方法结合了持久同调、图卷积和张量运算，同时捕捉了局部和全局层面上的Tensor视图拓扑（TT）和Tensor视图图（TG）结构信息。

    

    图分类是一项重要的图结构数据学习任务。图神经网络（GNNs）近年来在图学习中引起了越来越多的关注，并在许多重要的图问题上显示出显著的改进。尽管现有的GNNs在性能上处于最前沿，但它们只使用了每个节点周围非常有限的邻域的局部信息，导致了多模态信息的丢失和过多计算的开销。为了解决这些问题，我们提出了一种新颖的Tensor视图拓扑图神经网络（TTG-NN），这是一种简单而有效的基于持久同调、图卷积和张量运算的拓扑深度学习方法。这种新方法同时捕捉了局部和全局层面上的Tensor视图拓扑（TT）和Tensor视图图（TG）结构信息，并在计算上充分利用了图的拓扑和结构。

    Graph classification is an important learning task for graph-structured data. Graph neural networks (GNNs) have recently gained growing attention in graph learning and have shown significant improvements in many important graph problems. Despite their state-of-the-art performances, existing GNNs only use local information from a very limited neighborhood around each node, suffering from loss of multi-modal information and overheads of excessive computation. To address these issues, we propose a novel Tensor-view Topological Graph Neural Network (TTG-NN), a class of simple yet effective topological deep learning built upon persistent homology, graph convolution, and tensor operations. This new method incorporates tensor learning to simultaneously capture Tensor-view Topological (TT), as well as Tensor-view Graph (TG) structural information on both local and global levels. Computationally, to fully exploit graph topology and structure, we propose two flexible TT and TG representation lea
    
[^124]: 利用深度学习对脑电解码中的欧几里得对齐进行系统评估

    A Systematic Evaluation of Euclidean Alignment with Deep Learning for EEG Decoding. (arXiv:2401.10746v1 [eess.SP])

    [http://arxiv.org/abs/2401.10746](http://arxiv.org/abs/2401.10746)

    本研究系统评估了使用深度学习和欧几里得对齐对脑电解码的影响。结果表明，欧几里得对齐能够显著提高解码率，并且减少了收敛时间。

    

    脑电图（EEG）信号经常用于各种脑机接口（BCI）任务。尽管深度学习（DL）技术显示出有希望的结果，但它们受到大量数据要求的限制。通过利用来自多个受试者的数据，迁移学习能够更有效地训练DL模型。一种越来越受欢迎的技术是欧几里得对齐（EA），因为它易于使用、计算复杂度低并且与深度学习模型兼容。然而，很少有研究评估其对共享和个体DL模型的训练效果的影响。在这项工作中，我们系统地评估了EA与DL相结合在解码BCI信号中的效果。我们使用EA来训练来自多个受试者的共享模型，并评估其对新受试者的可迁移性。我们的实验结果表明，它将目标受试者的解码率提高了4.33％，并且收敛时间缩短了超过70％。我们还为个体模型进行了训练。

    Electroencephalography (EEG) signals are frequently used for various Brain-Computer Interface (BCI) tasks. While Deep Learning (DL) techniques have shown promising results, they are hindered by the substantial data requirements. By leveraging data from multiple subjects, transfer learning enables more effective training of DL models. A technique that is gaining popularity is Euclidean Alignment (EA) due to its ease of use, low computational complexity, and compatibility with Deep Learning models. However, few studies evaluate its impact on the training performance of shared and individual DL models. In this work, we systematically evaluate the effect of EA combined with DL for decoding BCI signals. We used EA to train shared models with data from multiple subjects and evaluated its transferability to new subjects. Our experimental results show that it improves decoding in the target subject by 4.33% and decreases convergence time by more than 70%. We also trained individual models for 
    
[^125]: 基于噪声对比估计的低资源安全攻击模式识别匹配框架

    Noise Contrastive Estimation-based Matching Framework for Low-resource Security Attack Pattern Recognition. (arXiv:2401.10337v1 [cs.LG])

    [http://arxiv.org/abs/2401.10337](http://arxiv.org/abs/2401.10337)

    该论文提出了一种基于噪声对比估计的低资源安全攻击模式识别匹配框架，通过直接语义相似度决定文本与攻击模式之间的关联，以降低大量类别、标签分布不均和标签空间复杂性带来的学习难度。

    

    战术、技术和程序（TTPs）是网络安全领域中复杂的攻击模式，在文本知识库中有详细的描述。在网络安全写作中识别TTPs，通常称为TTP映射，是一个重要而具有挑战性的任务。传统的学习方法通常以经典的多类或多标签分类设置为目标。由于存在大量的类别（即TTPs），标签分布的不均衡和标签空间的复杂层次结构，这种设置限制了模型的学习能力。我们采用了一种不同的学习范式来解决这个问题，其中将文本与TTP标签之间的直接语义相似度决定为文本分配给TTP标签，从而减少了仅仅在大型标签空间上竞争的复杂性。为此，我们提出了一种具有有效的基于采样的学习比较机制的神经匹配架构，促进学习过程。

    Tactics, Techniques and Procedures (TTPs) represent sophisticated attack patterns in the cybersecurity domain, described encyclopedically in textual knowledge bases. Identifying TTPs in cybersecurity writing, often called TTP mapping, is an important and challenging task. Conventional learning approaches often target the problem in the classical multi-class or multilabel classification setting. This setting hinders the learning ability of the model due to a large number of classes (i.e., TTPs), the inevitable skewness of the label distribution and the complex hierarchical structure of the label space. We formulate the problem in a different learning paradigm, where the assignment of a text to a TTP label is decided by the direct semantic similarity between the two, thus reducing the complexity of competing solely over the large labeling space. To that end, we propose a neural matching architecture with an effective sampling-based learn-to-compare mechanism, facilitating the learning pr
    
[^126]: 标准多导睡眠图与耳内脑电信号的比较分析：初步研究

    Comparison analysis between standard polysomnographic data and in-ear-EEG signals: A preliminary study. (arXiv:2401.10107v1 [eess.SP])

    [http://arxiv.org/abs/2401.10107](http://arxiv.org/abs/2401.10107)

    本研究通过比较标准多导睡眠图（PSG）和耳内脑电信号的相似性，旨在探索一种更少侵入、成本效益高和便携的替代方法。研究确定了评估方法，并通过提取特征进行分析。

    

    研究目的：多导睡眠图（PSG）目前被用作评估睡眠障碍的基准。其不舒适、不适合家庭使用以及在睡眠质量评估中引入偏差的问题需要探索更少侵入性、成本效益高和便携性的替代方法。一种有前景的候选方法是耳内脑电传感器，它在舒适性、固定电极位置、抗电磁干扰性和易于使用性方面均具有优势。本研究旨在建立一种评估耳内脑电信号与标准PSG之间相似性的方法。方法：我们评估PSG和耳内脑电推导的睡眠图之间的一致性。我们从PSG和耳内脑电信号的30秒时域和频域提取特征。我们只考虑在PSG评分员和耳内脑电评分员达成一致时的时段。我们引入一种方法来量化PSG推导和单通道耳内脑电之间的相似性。该方法包括...

    Study Objectives: Polysomnography (PSG) currently serves as the benchmark for evaluating sleep disorders. Its discomfort, impracticality for home-use, and introduction of bias in sleep quality assessment necessitate the exploration of less invasive, cost-effective, and portable alternatives. One promising contender is the in-ear-EEG sensor, which offers advantages in terms of comfort, fixed electrode positions, resistance to electromagnetic interference, and user-friendliness. This study aims to establish a methodology to assess the similarity between the in-ear-EEG signal and standard PSG.  Methods: We assess the agreement between the PSG and in-ear-EEG derived hypnograms. We extract features in the time- and frequency- domain from PSG and in-ear-EEG 30-second epochs. We only consider the epochs where the PSG-scorers and the in-ear-EEG-scorers were in agreement. We introduce a methodology to quantify the similarity between PSG derivations and the single-channel in-ear-EEG. The approac
    
[^127]: 用空间自适应滤波重新思考谱图神经网络

    Rethinking Spectral Graph Neural Networks with Spatially Adaptive Filtering. (arXiv:2401.09071v1 [cs.LG])

    [http://arxiv.org/abs/2401.09071](http://arxiv.org/abs/2401.09071)

    本文重新思考了谱图神经网络，并揭示了谱滤波和空间聚合之间的联系。该研究发现，谱滤波在隐含地将原始图转换成适应性新图，并明确计算用于空间聚合的新图。适应性新图展现出非局部性，并能够反映节点之间的标签一致性。

    

    尽管谱图神经网络（GNN）在理论上在谱域中有很好的基础，但它们实际上依赖于多项式逼近，意味着它们与空间域有着深刻的联系。由于以前的研究很少从空间角度研究谱图GNN，因此它们在空间域的可解释性仍然难以捉摸，例如，谱图GNN在空间域中实际上编码了哪些信息？为了回答这个问题，本文在谱滤波和空间聚合之间建立了一个理论上的联系，揭示了谱滤波隐含地将原始图转换成适应性新图的内在交互作用，并明确地计算用于空间聚合的适应性新图。理论和经验研究表明，适应性新图不仅表现出非局部性，还能够容纳有符号的边权重以反映节点之间的标签一致性。因此，这些发现突显了谱图GNN在空间中的可解释性角色。

    Whilst spectral Graph Neural Networks (GNNs) are theoretically well-founded in the spectral domain, their practical reliance on polynomial approximation implies a profound linkage to the spatial domain. As previous studies rarely examine spectral GNNs from the spatial perspective, their spatial-domain interpretability remains elusive, e.g., what information is essentially encoded by spectral GNNs in the spatial domain? In this paper, to answer this question, we establish a theoretical connection between spectral filtering and spatial aggregation, unveiling an intrinsic interaction that spectral filtering implicitly leads the original graph to an adapted new graph, explicitly computed for spatial aggregation. Both theoretical and empirical investigations reveal that the adapted new graph not only exhibits non-locality but also accommodates signed edge weights to reflect label consistency between nodes. These findings thus highlight the interpretable role of spectral GNNs in the spatial 
    
[^128]: 通过迭代组合问题来增强数学问题求解

    Augmenting Math Word Problems via Iterative Question Composing. (arXiv:2401.09003v1 [cs.CL])

    [http://arxiv.org/abs/2401.09003](http://arxiv.org/abs/2401.09003)

    本研究通过引入MMIQC数据集和迭代组合问题(IQC)的新颖增强方法，成功提高了大型语言模型的数学推理能力，在竞赛级数学问题上取得了优于先前最佳结果的准确率。

    

    尽管在改善大型语言模型(LLMs)的数学推理能力方面取得了一定进展，但在不使用外部工具的情况下解决竞赛级数学问题仍然对开源LLMs具有挑战性。在这项工作中，我们介绍了MMIQC数据集，这是一个混合处理的网络数据和合成问题-响应对的混合数据集，以提供基础模型更好的数学推理能力。通过在MMIQC上对Mistral-7B(arXiv:2310.06825)进行微调获得的模型Mistral-7B-MMIQC，在MATH(arXiv:2103.03874)上达到了36.0%的准确率，比之前(model size $\sim$7B)的最佳结果高出5.8%。我们的实验还表明，改进的一个重要部分归功于我们的新颖增强方法IQC(迭代组合问题)，其中我们迭代地要求LLM从给定的种子问题中组合新问题，并从另一个LLM中进行拒绝抽样。MMIQC现已在https://huggingface.co/datasets/Vivacem/MMIQC上发布。

    Despite recent progress in improving the mathematical reasoning ability of large language models(LLMs), solving competition-level math problems without the use of external tools remains challenging for open-source LLMs. In this work, we introduce the MMIQC dataset, a mixture of processed web data and synthetic question-response pairs, to equip base models with better mathematical reasoning skills. Mistral-7B-MMIQC, the model obtained by fine-tuning Mistral-7B(arXiv:2310.06825) on MMIQC, achieves 36.0\% accuracy on MATH(arXiv:2103.03874), 5.8\% higher than the previous (model size $\sim$7B) SOTA. Our experiments also show that a large part of the improvement attributes to our novel augmentation method IQC(Iterative Question Composing), where we iteratively ask an LLM to compose new questions from the given seed problems and do rejection sampling from another LLM. MMIQC has now been released on https://huggingface.co/datasets/Vivacem/MMIQC.
    
[^129]: 评估用于AI辅助图像标注的符合预测集的效用

    Evaluating the Utility of Conformal Prediction Sets for AI-Advised Image Labeling. (arXiv:2401.08876v1 [cs.HC])

    [http://arxiv.org/abs/2401.08876](http://arxiv.org/abs/2401.08876)

    本研究评估了符合预测集在AI辅助图像标注中的效用，发现对于简单图像，预测集与Top-1和Top-k显示的准确性相当，但在标记分布外图像时特别有效，尤其是集合大小较小时。

    

    随着深度神经网络在高风险领域中越来越常见，它们的缺乏可解释性使得不确定性量化变得具有挑战性。我们研究了用于表示AI辅助决策中的不确定性的符合预测集的效果。通过一项大型预注册实验，我们比较了符合预测集和显示Top-1和Top-k预测在AI辅助图像标注中的效用。我们发现，对于简单的图像，预测集的准确性与Top-1和Top-k显示相当或稍低，但在标记分布外（OOD）图像时，尤其是当集合大小较小时，预测集在辅助人类标注方面表现出色。我们的结果在实践中强调了符合预测集的实际挑战，并提供了相关建议。

    As deep neural networks are more commonly deployed in high-stakes domains, their lack of interpretability makes uncertainty quantification challenging. We investigate the effects of presenting conformal prediction sets$\unicode{x2013}$a method for generating valid confidence sets in distribution-free uncertainty quantification$\unicode{x2013}$to express uncertainty in AI-advised decision-making. Through a large pre-registered experiment, we compare the utility of conformal prediction sets to displays of Top-1 and Top-k predictions for AI-advised image labeling. We find that the utility of prediction sets for accuracy varies with the difficulty of the task: while they result in accuracy on par with or less than Top-1 and Top-k displays for easy images, prediction sets excel at assisting humans in labeling out-of-distribution (OOD) images especially when the set size is small. Our results empirically pinpoint the practical challenges of conformal prediction sets and provide implications 
    
[^130]: RAG vs Fine-tuning: 管道，权衡以及在农业上的个案研究

    RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture. (arXiv:2401.08406v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.08406](http://arxiv.org/abs/2401.08406)

    本文评估了检索增强生成（RAG）和微调两种方法在大型语言模型上的性能差异，并提出了适用于农业数据集的管道和权衡。

    

    在构建大型语言模型应用程序时，开发者通常有两种常见方法来整合专有和领域特定的数据：检索增强生成（RAG）和微调。RAG利用外部数据增强提示信息，而微调则将附加知识整合到模型中。然而，这两种方法的优缺点并不为人所理解。在本文中，我们提出了一个微调和RAG的管道，并对多种流行的大型语言模型（包括Llama2-13B，GPT-3.5和GPT-4）进行了权衡。我们的管道由多个阶段组成，包括从PDF中提取信息，生成问题和答案，将其用于微调，并利用GPT-4评估结果。我们提出了评估RAG和微调管道不同阶段性能的指标。我们对农业数据集进行了深入研究。作为一个产业，农业在人工智能的应用方面并没有得到很大的渗透。

    There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen much penetration of AI, an
    
[^131]: TwinBooster: 结合Barlow Twins和梯度提升的大语言模型协同增强分子属性预测

    TwinBooster: Synergising Large Language Models with Barlow Twins and Gradient Boosting for Enhanced Molecular Property Prediction. (arXiv:2401.04478v1 [q-bio.BM])

    [http://arxiv.org/abs/2401.04478](http://arxiv.org/abs/2401.04478)

    TwinBooster结合了大语言模型、Barlow Twins和梯度提升，通过整合生物检测方法和分子指纹，实现了对未见过的生物检测方法和分子属性的精确预测，该方法在数据稀缺的情况下展现出了优秀的性能。

    

    药物发现和开发的成功依赖于对分子活性和属性的精确预测。虽然基于计算的分子属性预测显示出了显著的潜力，但其使用迄今为止仅限于大量数据可用的检测方法。在本研究中，我们使用经过微调的大语言模型，结合了基于文本信息的生物检测方法，并使用了一种新颖的自监督学习方法的Siamese神经网络Barlow Twins。该架构利用检测方法信息和分子指纹提取真实的分子信息。TwinBooster通过提供最先进的零样本学习任务，实现了对未见过的生物检测方法和分子的属性预测。值得注意的是，我们的人工智能流水线在FS-Mol基准测试上表现出优秀的性能。这一突破展示了深度学习在通常数据稀缺的关键属性预测任务中的应用。

    The success of drug discovery and development relies on the precise prediction of molecular activities and properties. While in silico molecular property prediction has shown remarkable potential, its use has been limited so far to assays for which large amounts of data are available. In this study, we use a fine-tuned large language model to integrate biological assays based on their textual information, coupled with Barlow Twins, a Siamese neural network using a novel self-supervised learning approach. This architecture uses both assay information and molecular fingerprints to extract the true molecular information. TwinBooster enables the prediction of properties of unseen bioassays and molecules by providing state-of-the-art zero-shot learning tasks. Remarkably, our artificial intelligence pipeline shows excellent performance on the FS-Mol benchmark. This breakthrough demonstrates the application of deep learning to critical property prediction tasks where data is typically scarce.
    
[^132]: 使用自校正生成非平稳纹理

    Generating Non-Stationary Textures using Self-Rectification. (arXiv:2401.02847v1 [cs.CV])

    [http://arxiv.org/abs/2401.02847](http://arxiv.org/abs/2401.02847)

    本文提出了一种使用自校正来生成非平稳纹理的方法，通过使用预训练扩散网络和自注意机制，可以将用户修改的参考纹理细化为一种连贯、无缝的纹理，并保留参考样本的独特视觉特征。实验证实表明，该方法在处理非平稳纹理方面具有卓越的能力，相比现有技术在纹理合成方面取得了显著的进展。

    

    本文解决了基于示例的非平稳纹理合成的挑战。我们提出了一种新的两步方法，用户可以使用标准图像编辑工具修改参考纹理，得到合成的初始目标。随后，我们提出的方法“自校正”自动将这个目标细化为一种连贯、无缝的纹理，同时忠实地保留了参考样本的独特视觉特征。我们的方法利用预训练扩散网络，并使用自注意机制，逐渐将合成纹理与参考对齐，确保保留所提供目标中的结构。通过实验证实，我们的方法在处理非平稳纹理方面表现出卓越的能力，相比现有的最先进技术，在纹理合成方面取得了显著的进展。代码可在https://github.com/xiaorongjun000/Self-Rectific下载。

    This paper addresses the challenge of example-based non-stationary texture synthesis. We introduce a novel twostep approach wherein users first modify a reference texture using standard image editing tools, yielding an initial rough target for the synthesis. Subsequently, our proposed method, termed "self-rectification", automatically refines this target into a coherent, seamless texture, while faithfully preserving the distinct visual characteristics of the reference exemplar. Our method leverages a pre-trained diffusion network, and uses self-attention mechanisms, to gradually align the synthesized texture with the reference, ensuring the retention of the structures in the provided target. Through experimental validation, our approach exhibits exceptional proficiency in handling non-stationary textures, demonstrating significant advancements in texture synthesis when compared to existing state-of-the-art techniques. Code is available at https://github.com/xiaorongjun000/Self-Rectific
    
[^133]: Powerformer：适应不同传输区段的变压器架构用于电力流调整

    Powerformer: A Section-adaptive Transformer for Power Flow Adjustment. (arXiv:2401.02771v1 [cs.LG])

    [http://arxiv.org/abs/2401.02771](http://arxiv.org/abs/2401.02771)

    Powerformer是一种适应不同传输区段的变压器架构，用于学习稳健电力系统状态表示。它通过开发专用的区段自适应注意机制，并引入图神经网络传播和多因素注意机制来提供更加稳健的状态表示。在三个不同的电力系统场景上进行了广泛评估。

    

    本文提出了一种专为学习稳健电力系统状态表示而量身定制的变压器架构，旨在优化跨不同传输区段的电力调度以进行电力流调整。具体而言，我们的提出的方法名为Powerformer，开发了一种专用的区段自适应注意机制，与传统变压器中使用的自注意分离开来。该机制有效地将电力系统状态与传输区段信息整合在一起，有助于开发稳健的状态表示。此外，通过考虑电力系统的图拓扑和母线节点的电气属性，我们引入了两种定制策略来进一步增强表达能力：图神经网络传播和多因素注意机制。我们在三个电力系统场景（包括IEEE 118节点系统、中国实际300节点系统和一个大型系统）上进行了广泛的评估。

    In this paper, we present a novel transformer architecture tailored for learning robust power system state representations, which strives to optimize power dispatch for the power flow adjustment across different transmission sections. Specifically, our proposed approach, named Powerformer, develops a dedicated section-adaptive attention mechanism, separating itself from the self-attention used in conventional transformers. This mechanism effectively integrates power system states with transmission section information, which facilitates the development of robust state representations. Furthermore, by considering the graph topology of power system and the electrical attributes of bus nodes, we introduce two customized strategies to further enhance the expressiveness: graph neural network propagation and multi-factor attention mechanism. Extensive evaluations are conducted on three power system scenarios, including the IEEE 118-bus system, a realistic 300-bus system in China, and a large-
    
[^134]: HAAQI-Net: 一种适用于助听器的非侵入性神经音质评估模型

    HAAQI-Net: A non-intrusive neural music quality assessment model for hearing aids. (arXiv:2401.01145v1 [eess.AS])

    [http://arxiv.org/abs/2401.01145](http://arxiv.org/abs/2401.01145)

    HAAQI-Net是一种适用于助听器用户的非侵入性神经音质评估模型，通过使用BLSTM和注意力机制，以及预训练的BEATs进行声学特征提取，能够快速且准确地预测音乐的HAAQI得分，相比传统方法具有更高的性能和更低的推理时间。

    

    本文介绍了HAAQI-Net，一种针对助听器用户定制的非侵入性深度学习音质评估模型。与传统方法如Hearing Aid Audio Quality Index (HAAQI) 不同，HAAQI-Net采用了带有注意力机制的双向长短期记忆网络(BLSTM)。该模型以评估的音乐样本和听力损失模式作为输入，生成预测的HAAQI得分。模型采用了预训练的来自音频变换器(BEATs)的双向编码器表示进行声学特征提取。通过将预测分数与真实分数进行比较，HAAQI-Net达到了0.9257的长期一致性相关(LCC)，0.9394的斯皮尔曼等级相关系数(SRCC)，和0.0080的均方误差(MSE)。值得注意的是，这种高性能伴随着推理时间的大幅减少：从62.52秒(HAAQI)减少到2.71秒(HAAQI-Net)，为助听器用户提供了高效的音质评估模型。

    This paper introduces HAAQI-Net, a non-intrusive deep learning model for music quality assessment tailored to hearing aid users. In contrast to traditional methods like the Hearing Aid Audio Quality Index (HAAQI), HAAQI-Net utilizes a Bidirectional Long Short-Term Memory (BLSTM) with attention. It takes an assessed music sample and a hearing loss pattern as input, generating a predicted HAAQI score. The model employs the pre-trained Bidirectional Encoder representation from Audio Transformers (BEATs) for acoustic feature extraction. Comparing predicted scores with ground truth, HAAQI-Net achieves a Longitudinal Concordance Correlation (LCC) of 0.9257, Spearman's Rank Correlation Coefficient (SRCC) of 0.9394, and Mean Squared Error (MSE) of 0.0080. Notably, this high performance comes with a substantial reduction in inference time: from 62.52 seconds (by HAAQI) to 2.71 seconds (by HAAQI-Net), serving as an efficient music quality assessment model for hearing aid users.
    
[^135]: 强化学习中的消除学习

    Reinforcement Unlearning. (arXiv:2312.15910v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2312.15910](http://arxiv.org/abs/2312.15910)

    强化学习中的消除学习是一种新兴的研究领域，旨在解决环境所有者有权撤销智能体训练数据的隐私问题。该领域面临三个主要挑战。

    

    机器消除学习指的是根据数据所有者的请求，降低特定训练数据对机器学习模型的影响的过程。然而，在消除学习的研究中，一个重要的领域往往被忽视，那就是强化学习。强化学习旨在训练一个智能体在环境中做出最优决策以最大化累积奖励。在训练过程中，智能体往往会记忆环境的特征，这引发了一个重大的隐私问题。根据数据保护法规，环境的所有者有权撤销智能体的训练数据的访问权限，因此需要开展一个新颖且紧迫的研究领域，即“强化消除学习”。强化消除学习侧重于撤销整个环境而不是单个数据样本。这一独特特征带来了三个不同的挑战：1）如何提出消除学习方案

    Machine unlearning refers to the process of mitigating the influence of specific training data on machine learning models based on removal requests from data owners. However, one important area that has been largely overlooked in the research of unlearning is reinforcement learning. Reinforcement learning focuses on training an agent to make optimal decisions within an environment to maximize its cumulative rewards. During the training, the agent tends to memorize the features of the environment, which raises a significant concern about privacy. As per data protection regulations, the owner of the environment holds the right to revoke access to the agent's training data, thus necessitating the development of a novel and pressing research field, known as \emph{reinforcement unlearning}. Reinforcement unlearning focuses on revoking entire environments rather than individual data samples. This unique characteristic presents three distinct challenges: 1) how to propose unlearning schemes f
    
[^136]: 定价的因果预测方法

    Causal Forecasting for Pricing. (arXiv:2312.15282v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2312.15282](http://arxiv.org/abs/2312.15282)

    本文提出了一种在定价环境下进行需求预测的新方法，通过将因果推断的双重机器学习方法和最先进的基于变压器的预测模型结合在一起，我们的方法在完全控制的情况下更好地估计因果效应，并在离线政策设置中优于其他预测方法。

    

    本文提出了一种在定价环境下进行需求预测的新方法。在这种情况下，建模价格作为需求的输入变量之间的因果关系至关重要，因为零售商的目标是以（利润）最佳方式设定价格，以解决下游决策问题。我们的方法将因果推断的双重机器学习方法和最先进的基于变压器的预测模型结合在一起。通过大量的实证实验，我们一方面展示了我们的方法在完全控制的情况下对合成的、但现实的数据更好地估计因果效应。另一方面，我们还展示了在实际数据中，我们的方法在离线政策设置（即定价政策发生变化时）中优于其他预测方法，而在在线政策设置中略有落后。

    This paper proposes a novel method for demand forecasting in a pricing context. Here, modeling the causal relationship between price as an input variable to demand is crucial because retailers aim to set prices in a (profit) optimal manner in a downstream decision making problem. Our methods bring together the Double Machine Learning methodology for causal inference and state-of-the-art transformer-based forecasting models. In extensive empirical experiments, we show on the one hand that our method estimates the causal effect better in a fully controlled setting via synthetic, yet realistic data. On the other hand, we demonstrate on real-world data that our method outperforms forecasting methods in off-policy settings (i.e., when there's a change in the pricing policy) while only slightly trailing in the on-policy setting.
    
[^137]: Auto311: 一种基于信心指导的自动非紧急通话系统

    Auto311: A Confidence-guided Automated System for Non-emergency Calls. (arXiv:2312.14185v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.14185](http://arxiv.org/abs/2312.14185)

    Auto311是第一个处理非紧急电话的自动化系统，它通过减轻非紧急电话负担，提供快速有效的响应。通过预测事件类型并生成个性化的案件报告，并从对话上下文中提取关键信息来完善报告，系统与主叫人之间的对话结构得到优化。

    

    紧急和非紧急响应系统是地方政府提供的基本服务，对于保护生命、环境和财产至关重要。有效处理（非）紧急电话对公共安全和福祉至关重要。通过减轻非紧急电话的负担，亟需911求助的居民将获得快速有效的响应。我们与纳什维尔紧急通信部门合作，分析了11,796个非紧急呼叫录音，并开发了Auto311，第一个处理311非紧急呼叫的自动化系统，该系统（1）有效动态地预测正在进行的非紧急事件类型，以在通话过程中生成个性化的案件报告；（2）从对话上下文中提取关键信息，完成生成的报告；（3）以优化的信心水平安排系统和主叫人之间的对话结构。我们使用实际数据评估了该系统的有效性。

    Emergency and non-emergency response systems are essential services provided by local governments and critical to protecting lives, the environment, and property. The effective handling of (non-)emergency calls is critical for public safety and well-being. By reducing the burden through non-emergency callers, residents in critical need of assistance through 911 will receive a fast and effective response. Collaborating with the Department of Emergency Communications (DEC) in Nashville, we analyzed 11,796 non-emergency call recordings and developed Auto311, the first automated system to handle 311 non-emergency calls, which (1) effectively and dynamically predicts ongoing non-emergency incident types to generate tailored case reports during the call; (2) itemizes essential information from dialogue contexts to complete the generated reports; and (3) strategically structures system-caller dialogues with optimized confidence. We used real-world data to evaluate the system's effectiveness a
    
[^138]: 基于行列式点过程和广义体积取样的加权最小二乘逼近

    Weighted least-squares approximation with determinantal point processes and generalized volume sampling. (arXiv:2312.14057v2 [math.NA] UPDATED)

    [http://arxiv.org/abs/2312.14057](http://arxiv.org/abs/2312.14057)

    该论文研究了使用行列式点过程和广义体积取样进行加权最小二乘逼近的问题，提出了广义版本的体积标准化取样算法，并证明了该算法在期望上的准最优性以及在某些规范向量空间中的逼近结果。

    

    我们考虑使用给定的m维空间V_m中的元素，借助于一些特征映射φ，通过对随机点x_1，...，x_n处的函数进行评估，来逼近函数从L^2到函数。在回顾一些关于使用独立同分布点的最优加权最小二乘的结果之后，我们考虑使用投影行列式点过程（DPP）或体积取样的加权最小二乘。这些分布在选定的特征φ(x_i)中引入了点之间的依赖性，以促进多样性。我们首先提供了广义版本的体积标准化取样，使用样本数n = O(mlog(m))得到了期望上的准最优结果，这意味着期望的L^2误差受到一个常数乘以在L^2中的最佳逼近误差的限制。此外，进一步假设函数在某个嵌入在L^2中的规范向量空间H中，我们进一步证明了逼近的结果。

    We consider the problem of approximating a function from $L^2$ by an element of a given $m$-dimensional space $V_m$, associated with some feature map $\varphi$, using evaluations of the function at random points $x_1,\dots,x_n$. After recalling some results on optimal weighted least-squares using independent and identically distributed points, we consider weighted least-squares using projection determinantal point processes (DPP) or volume sampling. These distributions introduce dependence between the points that promotes diversity in the selected features $\varphi(x_i)$. We first provide a generalized version of volume-rescaled sampling yielding quasi-optimality results in expectation with a number of samples $n = O(m\log(m))$, that means that the expected $L^2$ error is bounded by a constant times the best approximation error in $L^2$. Also, further assuming that the function is in some normed vector space $H$ continuously embedded in $L^2$, we further prove that the approximation is
    
[^139]: 在大型语言模型中定位事实知识：探索剩余流和分析词汇空间中的子值。

    Locating Factual Knowledge in Large Language Models: Exploring the Residual Stream and Analyzing Subvalues in Vocabulary Space. (arXiv:2312.12141v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.12141](http://arxiv.org/abs/2312.12141)

    通过探索剩余流和分析词汇空间中的子值，我们定位了大型语言模型中的事实知识，并找到了存储了有关“法国，首都，巴黎”的知识的位置。

    

    通过探索剩余流和分析词汇空间中的子值，我们找到了大型语言模型中事实知识的位置。我们发现当投影到词汇空间时，子值具有可人类解释的概念的原因。子值的softmax之前的值通过一个加法函数相加，因此词汇空间中前几个标记的概率会增加。基于此，我们发现使用对数概率增加来计算层和子值的重要性比概率增加更好，因为对数概率增加的曲线呈线性单调增形状。此外，我们计算内积来评估前馈网络（FFN）的子值被前面的层激活的程度。根据我们的方法，我们找到了事实知识“法国，首都，巴黎”存储的位置。具体来说，注意力层存储“巴黎与法国相关”。FFN层存储“巴黎是一个首都/城市”，由注意力子值激活。

    We find the location of factual knowledge in large language models by exploring the residual stream and analyzing subvalues in vocabulary space. We find the reason why subvalues have human-interpretable concepts when projecting into vocabulary space. The before-softmax values of subvalues are added by an addition function, thus the probability of top tokens in vocabulary space will increase. Based on this, we find using log probability increase to compute the significance of layers and subvalues is better than probability increase, since the curve of log probability increase has a linear monotonically increasing shape. Moreover, we calculate the inner products to evaluate how much a feed-forward network (FFN) subvalue is activated by previous layers. Base on our methods, we find where factual knowledge <France, capital, Paris> is stored. Specifically, attention layers store "Paris is related to France". FFN layers store "Paris is a capital/city", activated by attention subvalues relate
    
[^140]: 面向基于强化学习的药物调整系统以减少言语不流畅的论文翻译

    Toward A Reinforcement-Learning-Based System for Adjusting Medication to Minimize Speech Disfluency. (arXiv:2312.11509v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.11509](http://arxiv.org/abs/2312.11509)

    这个论文介绍了一种基于强化学习的系统，该系统可以根据患者言语不流畅程度自动调整药物，通过对药物组合的强化学习算法的优化，能够收敛到良好的用药方案。

    

    我们提出了一种基于强化学习的系统，该系统可以自动为患有与心理健康相关的言语不流畅的虚拟患者开具药物处方，并根据零成本频繁测量结果，调整药物和剂量。我们展示了系统的两个组成部分：一个在我们构建的大型数据集上检测和评估言语不流畅的模块，以及一个可以自动找到良好药物组合的强化学习算法。为了支持这两个模块，我们从文献中收集了关于药物治疗言语不流畅的效果的数据，并建立了一个可信的患者模拟系统。我们证明了在某些情况下，强化学习系统能够收敛到一个良好的用药方案。我们收集并对可能存在言语不流畅的人群进行了数据标注，并使用该数据集演示了我们的方法。我们的工作是一个概念验证:

    We propose a Reinforcement-Learning-based system that would automatically prescribe a hypothetical patient medication that may help the patient with their mental-health-related speech disfluency, and adjust the medication and the dosages in response to zero-cost frequent measurement of the fluency of the patient. We demonstrate the components of the system: a module that detects and evaluates speech disfluency on a large dataset we built, and a Reinforcement Learning algorithm that automatically finds good combinations of medications. To support the two modules, we collect data on the effect of psychiatric medications for speech disfluency from the literature, and build a plausible patient simulation system. We demonstrate that the Reinforcement Learning system is, under some circumstances, able to converge to a good medication regime. We collect and label a dataset of people with possible speech disfluency and demonstrate our methods using that dataset. Our work is a proof of concept:
    
[^141]: Fine-Tuning还是检索？比较在LLMs中的知识注入

    Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs. (arXiv:2312.05934v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.05934](http://arxiv.org/abs/2312.05934)

    该研究比较了无监督的微调和检索增强生成（RAG）这两种常见方法在LLMs中的应用。结果发现，RAG在现有知识和新知识上表现出更好的性能，而LLMs通过无监督的微调学习新的事实信息较困难。

    

    大型语言模型（LLMs）在其预训练的权重中封装了大量的事实信息，正如它们能够在不同领域回答各种问题所证明的那样。然而，这种知识本质上是有限的，很大程度上依赖于训练数据的特性。因此，使用外部数据集来整合新的信息或改进LLMs在已见信息上的能力面临着重大挑战。在这个研究中，我们比较了两种常见的方法：无监督的微调和检索增强生成（RAG）。我们在不同主题的各种知识密集型任务上评估了这两种方法。我们的发现表明，虽然无监督的微调能够提供一定的改进，但RAG在现有知识和完全新知识上始终表现出更好的性能。此外，我们发现LLMs很难通过无监督的微调来学习新的事实信息，并且暴露

    Large language models (LLMs) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of LLMs on previously seen information poses a significant challenge. In this study, we compare two common approaches: unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while unsupervised fine-tuning offers some improvement, RAG consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing
    
[^142]: 多个任务预训练和图形提示的MultiGPrompt

    MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs. (arXiv:2312.03731v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.03731](http://arxiv.org/abs/2312.03731)

    本文提出了一种名为MultiGPrompt的多任务预训练和提示框架，用于在图形表示学习中提高鲁棒性和减少标注成本。

    

    图形可以固有地对Web上相互连接的对象进行建模，从而支持一系列Web应用，比如网络分析和内容推荐。最近，图神经网络（GNNs）已经成为图表示学习的主流技术。然而，在端到端监督框架中，它们的有效性与任务特定标签的可用性密切相关。为了减少标注成本并增强在少样本设置中的鲁棒性，基于自监督任务的预训练已经成为一种有前途的方法，而提示则被提出来进一步缩小预训练任务与下游任务之间的目标差距。虽然已经对基于提示的图形学习进行了初步的探索，但它们主要利用单个预训练任务，导致从预训练数据中可能学习的通用知识的子集受限。因此，在本文中，我们提出了一种新颖的多任务预训练和提示框架MultiGPrompt，用于进一步提高对图形的表示学习。

    Graphs can inherently model interconnected objects on the Web, thereby facilitating a series of Web applications, such as web analyzing and content recommendation. Recently, Graph Neural Networks (GNNs) have emerged as a mainstream technique for graph representation learning. However, their efficacy within an end-to-end supervised framework is significantly tied to the availabilityof task-specific labels. To mitigate labeling costs and enhance robustness in few-shot settings, pre-training on self-supervised tasks has emerged as a promising method, while prompting has been proposed to further narrow the objective gap between pretext and downstream tasks. Although there has been some initial exploration of prompt-based learning on graphs, they primarily leverage a single pretext task, resulting in a limited subset of general knowledge that could be learned from the pre-training data. Hence, in this paper, we propose MultiGPrompt, a novel multi-task pre-training and prompting framework to
    
[^143]: 自补代码生成

    Self-Infilling Code Generation. (arXiv:2311.17972v2 [cs.PL] UPDATED)

    [http://arxiv.org/abs/2311.17972](http://arxiv.org/abs/2311.17972)

    本文介绍了自补代码生成的通用框架，利用自补机制实现了中断和循环机制，使传统解码进程变得非单调。利用中断机制可以推迟生成代码，增强对输出的控制；利用循环机制可以循环更新和同步生成的每个部分。

    

    本文介绍了一种自补代码生成的通用框架，它将补充操作融入自回归解码中。我们的方法利用了最近的能够进行填充的代码语言模型可以自动进行填充的观察结果：补充操作旨在根据预定义的前缀和后缀填充中间内容，而自补机制顺序生成这些周围上下文和被填充内容。我们利用这种能力在传统解码中引入了新颖的中断和循环机制，使其进化为非单调过程。中断机制允许推迟生成特定的代码，直到确定的后缀建立，增强对输出的控制。同时，循环机制利用自补和从左到右解码的互补性，可以循环更新和同步每个生成部分。我们进行了大量实验来证明我们的方法的有效性。

    This work introduces self-infilling code generation, a general framework that incorporates infilling operations into auto-regressive decoding. Our approach capitalizes on the observation that recent infilling-capable code language models can self-infill: whereas infilling operations aim to fill in the middle based on a predefined prefix and suffix, self-infilling sequentially generates both such surrounding context and the infilled content. We utilize this capability to introduce novel interruption and looping mechanisms in conventional decoding, evolving it into a non-monotonic process. Interruptions allow for postponing the generation of specific code until a definitive suffix is established, enhancing control over the output. Meanwhile, the looping mechanism, which leverages the complementary nature of self-infilling and left-to-right decoding, can iteratively update and synchronize each piece of generation cyclically. Extensive experiments are conducted to demonstrate that our prop
    
[^144]: 使用神经常微分方程增强低阶不连续Galerkin方法在可压Navier-Stokes方程中的应用

    Enhancing Low-Order Discontinuous Galerkin Methods with Neural Ordinary Differential Equations for Compressible Navier--Stokes Equations. (arXiv:2310.18897v2 [physics.flu-dyn] UPDATED)

    [http://arxiv.org/abs/2310.18897](http://arxiv.org/abs/2310.18897)

    本研究提出了一种方法，通过在不连续Galerkin方法中加入神经常微分方程，学习子网格尺度模型的效果，从而提高模拟的准确性和加速计算过程。

    

    随着计算能力的增长，模拟变得更加复杂和准确。然而，高保真度的模拟需要巨大的计算资源。为了降低计算成本，通常会运行一个低保真度模型并采用子网格尺度模型，但选择适当的子网格尺度模型并对其进行调节是具有挑战性的。我们在不连续Galerkin（DG）空间离散化的背景下提出了一种新颖的方法，通过在偏微分方程模拟中引入神经常微分算子来学习子网格尺度模型的效果。我们的方法在连续级别上学习低阶DG求解器中缺失的尺度，从而提高低阶DG近似的准确性，同时以一定程度的精度加速滤波高阶DG模拟。我们通过实验证明了我们方法的性能。

    The growing computing power over the years has enabled simulations to become more complex and accurate. While immensely valuable for scientific discovery and problem-solving, however, high-fidelity simulations come with significant computational demands. As a result, it is common to run a low-fidelity model with a subgrid-scale model to reduce the computational cost, but selecting the appropriate subgrid-scale models and tuning them are challenging. We propose a novel method for learning the subgrid-scale model effects when simulating partial differential equations augmented by neural ordinary differential operators in the context of discontinuous Galerkin (DG) spatial discretization. Our approach learns the missing scales of the low-order DG solver at a continuous level and hence improves the accuracy of the low-order DG approximations as well as accelerates the filtered high-order DG simulations with a certain degree of precision. We demonstrate the performance of our approach throug
    
[^145]: Clover: 闭环可验证代码生成

    Clover: Closed-Loop Verifiable Code Generation. (arXiv:2310.17807v1 [cs.SE])

    [http://arxiv.org/abs/2310.17807](http://arxiv.org/abs/2310.17807)

    Clover是一种闭环可验证代码生成的范式，通过在代码、docstrings和形式注释之间进行一致性检查，确保生成的代码的正确性。

    

    在软件开发中，使用大型语言模型进行代码生成是一个快速增长的趋势。然而，如果没有有效的方法来确保生成的代码的正确性，这个趋势可能会导致许多不良结果。在本文中，我们提出了一个解决这个挑战的愿景：Clover范式，即闭环可验证代码生成，它将正确性检查简化为更可访问的一致性检查问题。在Clover的核心是一个检查器，它在代码、docstrings和形式注释之间进行一致性检查。该检查器使用了形式验证工具和大型语言模型的新颖集成实现。我们提供了理论分析来支持我们的论点，即Clover在一致性检查方面应该是有效的。我们还在一个由手工设计的数据集（CloverBench）上进行了实证调查，该数据集包含了注释的Dafny程序，难度水平与教科书相当。实验结果显示

    The use of large language models for code generation is a rapidly growing trend in software development. However, without effective methods for ensuring the correctness of generated code, this trend could lead to any number of undesirable outcomes. In this paper, we lay out a vision for addressing this challenge: the Clover paradigm, short for Closed-Loop Verifiable Code Generation, which reduces correctness checking to the more accessible problem of consistency checking. At the core of Clover lies a checker that performs consistency checks among code, docstrings, and formal annotations. The checker is implemented using a novel integration of formal verification tools and large language models. We provide a theoretical analysis to support our thesis that Clover should be effective at consistency checking. We also empirically investigate its feasibility on a hand-designed dataset (CloverBench) featuring annotated Dafny programs at a textbook level of difficulty. Experimental results sho
    
[^146]: 用深度强化学习生成多样化调度策略来解决大型柔性车间调度实例

    Solving large flexible job shop scheduling instances by generating a diverse set of scheduling policies with deep reinforcement learning. (arXiv:2310.15706v1 [cs.AI])

    [http://arxiv.org/abs/2310.15706](http://arxiv.org/abs/2310.15706)

    本文提出了一种能够通过生成多样化的调度策略来解决大型柔性车间调度实例的方法，并应用深度强化学习来优化调度质量。

    

    柔性车间调度问题（FJSSP）在文献中得到了广泛研究，提出了许多启发式、精确和元启发式方法。然而，工业对实时响应突发事件的需求产生了在几秒内生成新调度的必要性。在这些方法中，只有调度规则（DRs）能够在约束下生成调度，尽管其质量可以得到改进。为了改善结果，最近的方法将FJSSP建模为马尔可夫决策过程（MDP），并应用强化学习生成一个策略，将操作分配到机器上生成最优解。然而，在大型的FJSSP实例中仍然有改进的空间，而这在实际情况中很常见。因此，本文的目标是提出一种能够稳健解决大型FJSSP实例的方法。

    The Flexible Job Shop Scheduling Problem (FJSSP) has been extensively studied in the literature, and multiple approaches have been proposed within the heuristic, exact, and metaheuristic methods. However, the industry's demand to be able to respond in real-time to disruptive events has generated the necessity to be able to generate new schedules within a few seconds. Among these methods, under this constraint, only dispatching rules (DRs) are capable of generating schedules, even though their quality can be improved. To improve the results, recent methods have been proposed for modeling the FJSSP as a Markov Decision Process (MDP) and employing reinforcement learning to create a policy that generates an optimal solution assigning operations to machines. Nonetheless, there is still room for improvement, particularly in the larger FJSSP instances which are common in real-world scenarios. Therefore, the objective of this paper is to propose a method capable of robustly solving large insta
    
[^147]: 在不断变化的多臂赌博机中实现零样本学习

    Towards Zero Shot Learning in Restless Multi-armed Bandits. (arXiv:2310.14526v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.14526](http://arxiv.org/abs/2310.14526)

    通过开发一个基于神经网络的预训练模型，我们实现了在不断变化的多臂赌博机中的零样本学习，该模型具有泛化能力，并且能够在特定实例上进行高效微调，同时适用于多行为设置和离散或连续状态空间。

    

    近来，通过多智能体强化学习的视角研究了一类资源分配问题——不断变化的多臂赌博机（RMABs），该问题在医疗保健、在线广告和反盗猎等领域具有广泛应用。先前的RMAB研究存在一些限制，例如没有充分解决连续状态问题，并且在多个真实世界应用中，当赌博机的入选和退出不断发生时，需要从头开始重新训练，这是一个常见的挑战。为了解决这些限制，我们开发了一个基于神经网络的预训练模型（PreFeRMAB），该模型具有对之前未见过的广泛RMAB问题的零样本能力，并且可以比从头训练更加高效地对特定实例进行微调。此外，我们的模型还适用于一般的多行为设置和离散或连续状态空间。为了实现快速泛化，我们学习了一种新颖的单一策略网络模型，该模型利用特征信息并采用了一种新的训练方式。

    Restless multi-arm bandits (RMABs), a class of resource allocation problems with broad application in areas such as healthcare, online advertising, and anti-poaching, have recently been studied from a multi-agent reinforcement learning perspective. Prior RMAB research suffers from several limitations, e.g., it fails to adequately address continuous states, and requires retraining from scratch when arms opt-in and opt-out over time, a common challenge in many real world applications. We address these limitations by developing a neural network-based pre-trained model (PreFeRMAB) that has general zero-shot ability on a wide range of previously unseen RMABs, and which can be fine-tuned on specific instances in a more sample-efficient way than retraining from scratch. Our model also accommodates general multi-action settings and discrete or continuous state spaces. To enable fast generalization, we learn a novel single policy network model that utilizes feature information and employs a tra
    
[^148]: 学习可解释的规则以实现可扩展的数据表示和分类

    Learning Interpretable Rules for Scalable Data Representation and Classification. (arXiv:2310.14336v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.14336](http://arxiv.org/abs/2310.14336)

    这项研究提出了一种名为RRL的新型分类器，通过自动学习可解释的非模糊规则，实现了数据表示和分类的良好可扩展性和解释性。

    

    基于规则的模型（如决策树）在需要高模型解释性的场景中被广泛使用，因为它们具有透明的内部结构和良好的模型表达能力。然而，由于离散的参数和结构，基于规则的模型在优化方面很难应对大规模的数据集。集成方法和模糊/软规则通常用于提高性能，但会牺牲模型的解释性。为了获得良好的可扩展性和可解释性，我们提出了一种新的分类器，称为基于规则的表示学习器（RRL），它可以自动学习用于数据表示和分类的可解释的非模糊规则。为了有效训练不可微分的RRL，我们将其映射到连续空间，并提出一种称为梯度嵌入的新的训练方法，可以使用梯度下降直接优化离散模型。此外，还设计了一种新颖的逻辑激活函数，以增加RRL的可扩展性，并使其能够进行判别。

    Rule-based models, e.g., decision trees, are widely used in scenarios demanding high model interpretability for their transparent inner structures and good model expressivity. However, rule-based models are hard to optimize, especially on large data sets, due to their discrete parameters and structures. Ensemble methods and fuzzy/soft rules are commonly used to improve performance, but they sacrifice the model interpretability. To obtain both good scalability and interpretability, we propose a new classifier, named Rule-based Representation Learner (RRL), that automatically learns interpretable non-fuzzy rules for data representation and classification. To train the non-differentiable RRL effectively, we project it to a continuous space and propose a novel training method, called Gradient Grafting, that can directly optimize the discrete model using gradient descent. A novel design of logical activation functions is also devised to increase the scalability of RRL and enable it to discr
    
[^149]: 具有多项式激活函数的图神经网络具有有限的表达能力

    Graph Neural Networks with polynomial activations have limited expressivity. (arXiv:2310.13139v1 [cs.LG])

    [http://arxiv.org/abs/2310.13139](http://arxiv.org/abs/2310.13139)

    本文证明了具有多项式激活函数的图神经网络无法表达GC2查询，与常用的非多项式激活函数存在分离，这回答了一个开放问题。

    

    图神经网络（GNNs）的表达能力可以完全由适当的一阶逻辑片段来描述。换句话说，任何在标记图上解释的关于二元逻辑片段（GC2）的查询都可以使用一个大小仅取决于查询深度的GNN来表示。正如[Barcelo＆Al。，2020，Grohe，2021]指出的那样，这个描述适用于一组激活函数的家族，这表明GNN可以通过不同的激活函数选择来表达不同的逻辑层次结构。在本文中，我们证明了这样的层次结构的存在，证明了具有多项式激活函数的GNN无法表示GC2查询。这意味着多项式和常用的非多项式激活函数（如ReLU、sigmoid、双曲正切等）之间存在一个分离，并回答了[Grohe，2021]提出的一个悬而未决的问题。

    The expressivity of Graph Neural Networks (GNNs) can be entirely characterized by appropriate fragments of the first order logic. Namely, any query of the two variable fragment of graded modal logic (GC2) interpreted over labelled graphs can be expressed using a GNN whose size depends only on the depth of the query. As pointed out by [Barcelo & Al., 2020, Grohe, 2021 ], this description holds for a family of activation functions, leaving the possibibility for a hierarchy of logics expressible by GNNs depending on the chosen activation function. In this article, we show that such hierarchy indeed exists by proving that GC2 queries cannot be expressed by GNNs with polynomial activation functions. This implies a separation between polynomial and popular non polynomial activations (such as ReLUs, sigmoid and hyperbolic tan and others) and answers an open question formulated by [Grohe, 2021].
    
[^150]: 等变矩阵函数神经网络

    Equivariant Matrix Function Neural Networks. (arXiv:2310.10434v1 [stat.ML])

    [http://arxiv.org/abs/2310.10434](http://arxiv.org/abs/2310.10434)

    矩阵函数神经网络（MFNs）是一种通过解析矩阵等变函数来参数化非局部相互作用的新型架构，能够在各种应用中实现最先进的性能。

    

    图神经网络（GNNs），尤其是消息传递神经网络（MPNNs），已经成为在各种应用中学习图形的强大架构。然而，当建模非局部相互作用时，MPNNs在大共轭分子，金属或非晶态材料等系统中面临挑战。尽管谱GNN和传统的神经网络（例如循环神经网络和Transformer）可以缓解这些挑战，但它们常常缺乏扩展性，适应性，泛化能力，计算效率，或者不能捕捉数据中的详细结构关系或对称性。为了解决这些问题，我们引入了矩阵函数神经网络（MFNs），一种通过解析矩阵等变函数来参数化非局部相互作用的新型架构。采用解析矩阵展开提供了一种直接的实现方法，并具有随系统大小线性扩展的潜力。该MFN架构在标准任务中实现了最先进的性能。

    Graph Neural Networks (GNNs), especially message-passing neural networks (MPNNs), have emerged as powerful architectures for learning on graphs in diverse applications. However, MPNNs face challenges when modeling non-local interactions in systems such as large conjugated molecules, metals, or amorphous materials. Although Spectral GNNs and traditional neural networks such as recurrent neural networks and transformers mitigate these challenges, they often lack extensivity, adaptability, generalizability, computational efficiency, or fail to capture detailed structural relationships or symmetries in the data. To address these concerns, we introduce Matrix Function Neural Networks (MFNs), a novel architecture that parameterizes non-local interactions through analytic matrix equivariant functions. Employing resolvent expansions offers a straightforward implementation and the potential for linear scaling with system size. The MFN architecture achieves state-of-the-art performance in standa
    
[^151]: 响铃！概念去除方法在扩散模型中的可靠性如何？

    Ring-A-Bell! How Reliable are Concept Removal Methods for Diffusion Models?. (arXiv:2310.10012v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.10012](http://arxiv.org/abs/2310.10012)

    本文研究了对于文本到图像合成的扩散模型中的潜在滥用问题的安全措施的有效性，并提出了一个用于评估的新颖概念检索算法。我们引入了一个名为Ring-A-Bell的模型无关的红队工具，可以事先准备整个评估过程，而无需先验知识。

    

    文本到图像(T2I)合成的扩散模型，如稳定的扩散(SD)，最近展示出了生成高质量内容的卓越能力。然而，这一进展引发了对潜在滥用的几个关注，特别是在创建受版权限制、禁止和受限内容，或者不适宜工作的(NSFW)图片方面。虽然已经采取了一些措施来缓解这些问题，例如在评估阶段实施安全过滤器或通过微调模型来消除不受欢迎的概念或风格，但这些安全措施在处理各种提示方面的有效性仍然很少被探索。在这项工作中，我们旨在通过提出一种用于评估的新颖概念检索算法来研究这些安全机制。我们引入了Ring-A-Bell，这是一个面向T2I扩散模型的模型无关的红队工具，整个评估可以在没有目标模型先验知识的情况下提前准备好。具体来说，Ring-A-Bell首先对激励进行分解，然后通过去除候选概念和计算特定概念的相关度来设计筛选机制。

    Diffusion models for text-to-image (T2I) synthesis, such as Stable Diffusion (SD), have recently demonstrated exceptional capabilities for generating high-quality content. However, this progress has raised several concerns of potential misuse, particularly in creating copyrighted, prohibited, and restricted content, or NSFW (not safe for work) images. While efforts have been made to mitigate such problems, either by implementing a safety filter at the evaluation stage or by fine-tuning models to eliminate undesirable concepts or styles, the effectiveness of these safety measures in dealing with a wide range of prompts remains largely unexplored. In this work, we aim to investigate these safety mechanisms by proposing one novel concept retrieval algorithm for evaluation. We introduce Ring-A-Bell, a model-agnostic red-teaming tool for T2I diffusion models, where the whole evaluation can be prepared in advance without prior knowledge of the target model. Specifically, Ring-A-Bell first pe
    
[^152]: 数据中心化图学习：一份综述

    Data-centric Graph Learning: A Survey. (arXiv:2310.04987v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.04987](http://arxiv.org/abs/2310.04987)

    本综述从数据中心化的角度全面评估了图学习方法，回答了何时修改图数据、图数据的哪一部分需要修改以及如何保护图模型的关键问题。

    

    人工智能的历史见证了高质量数据对各种深度学习模型的重大影响，例如AlexNet和ResNet的ImageNet。最近，与以模型为中心的方法设计更复杂的神经结构不同，人工智能社区的关注重点转向了以数据为中心的方法，该方法侧重于更好地处理数据以增强神经模型的能力。而在深度学习时代，操作普遍存在的拓扑数据的图学习也发挥着重要作用。在本综述中，我们从数据中心化的角度全面评估了图学习方法，并旨在回答三个关键问题：（1）何时修改图数据，（2）图数据的哪一部分需要修改以释放各种图模型的潜力，以及（3）如何保护图模型免受有问题的数据的影响。因此，我们提出了一个基于图学习流程阶段的创新分类法，并突出了关键创新和贡献。

    The history of artificial intelligence (AI) has witnessed the significant impact of high-quality data on various deep learning models, such as ImageNet for AlexNet and ResNet. Recently, instead of designing more complex neural architectures as model-centric approaches, the attention of AI community has shifted to data-centric ones, which focuses on better processing data to strengthen the ability of neural models. Graph learning, which operates on ubiquitous topological data, also plays an important role in the era of deep learning. In this survey, we comprehensively review graph learning approaches from the data-centric perspective, and aim to answer three crucial questions: (1) when to modify graph data, (2) what part of the graph data needs modification to unlock the potential of various graph models, and (3) how to safeguard graph models from problematic data influence. Accordingly, we propose a novel taxonomy based on the stages in the graph learning pipeline, and highlight the pr
    
[^153]: 可解释性模仿学习的动态DAG发现

    Dynamic DAG Discovery for Interpretable Imitation Learning. (arXiv:2310.00489v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00489](http://arxiv.org/abs/2310.00489)

    提出了一种用于解释模仿学习中神经代理的动态DAG发现方法，通过有向无环因果图展现其捕获的知识，以增加透明度和可解释性。

    

    模仿学习通过模仿专家的示范来学习代理策略，在医疗治疗方案和自动驾驶等许多应用中显示出了有希望的结果。然而，解释代理学习到的控制策略仍然是一个困难的任务。困难主要来自两个方面：1）模仿学习中的代理通常实现为深度神经网络，这些模型是黑盒模型，缺乏可解释性；2）代理决策背后的潜在因果机制可能随着轨迹而变化，而不是在整个时间步骤中保持静态不变。为了增加神经代理的透明度和提供更好的可解释性，我们提出以有向无环因果图的形式展示其所捕获的知识，其中节点是动作和状态变量，边表示预测背后的因果关系。此外，我们设计这个因果发现过程是依赖状态的，使其能够对潜在因果图中的动态进行建模。

    Imitation learning, which learns agent policy by mimicking expert demonstration, has shown promising results in many applications such as medical treatment regimes and self-driving vehicles. However, it remains a difficult task to interpret control policies learned by the agent. Difficulties mainly come from two aspects: 1) agents in imitation learning are usually implemented as deep neural networks, which are black-box models and lack interpretability; 2) the latent causal mechanism behind agents' decisions may vary along the trajectory, rather than staying static throughout time steps. To increase transparency and offer better interpretability of the neural agent, we propose to expose its captured knowledge in the form of a directed acyclic causal graph, with nodes being action and state variables and edges denoting the causal relations behind predictions. Furthermore, we design this causal discovery process to be state-dependent, enabling it to model the dynamics in latent causal gr
    
[^154]: 神经网络潜在表示中的对抗性机器学习

    Adversarial Machine Learning in Latent Representations of Neural Networks. (arXiv:2309.17401v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.17401](http://arxiv.org/abs/2309.17401)

    这项研究通过分析分布式深度神经网络对抗性行为的韧性填补了现有研究空白，并发现潜在特征在相同信息失真水平下比输入表示更加韧性，并且对抗性韧性由特征维度和神经网络的泛化能力共同决定。

    

    分布式深度神经网络已被证明可以减轻移动设备的计算负担，并降低边缘计算场景中的端到端推理延迟。尽管已经对分布式深度神经网络进行了研究，但据我们所知，分布式深度神经网络对于对抗性行为的韧性仍然是一个开放问题。在本文中，我们通过严格分析分布式深度神经网络对抗性行为的韧性来填补现有的研究空白。我们将这个问题置于信息论的背景下，并引入了两个新的衡量指标来衡量失真和韧性。我们的理论发现表明：（i）在假设具有相同信息失真水平的情况下，潜在特征始终比输入表示更加韧性；（ii）对抗性韧性同时由特征维度和深度神经网络的泛化能力决定。为了验证我们的理论发现，我们进行了广泛的实验分析，考虑了6种不同的深度神经网络架构。

    Distributed deep neural networks (DNNs) have been shown to reduce the computational burden of mobile devices and decrease the end-to-end inference latency in edge computing scenarios. While distributed DNNs have been studied, to the best of our knowledge the resilience of distributed DNNs to adversarial action still remains an open problem. In this paper, we fill the existing research gap by rigorously analyzing the robustness of distributed DNNs against adversarial action. We cast this problem in the context of information theory and introduce two new measurements for distortion and robustness. Our theoretical findings indicate that (i) assuming the same level of information distortion, latent features are always more robust than input representations; (ii) the adversarial robustness is jointly determined by the feature dimension and the generalization capability of the DNN. To test our theoretical findings, we perform extensive experimental analysis by considering 6 different DNN arc
    
[^155]: 自动测试代码翻译模型的功能性质

    Automatically Testing Functional Properties of Code Translation Models. (arXiv:2309.12813v1 [cs.SE])

    [http://arxiv.org/abs/2309.12813](http://arxiv.org/abs/2309.12813)

    本研究介绍了一种自动、功能性的代码翻译模型测试方法，能够捕捉各种属性从纯语法到纯语义的相关信息，并在实验中证明其有效性。

    

    对于跨编程语言进行代码翻译的大型语言模型，即$transpiling$，正变得日益实用。尽管自动翻译显著提高了开发者的生产力，但一个关键问题是生成的代码是否正确。现有的工作最初使用手工制作的测试套件来测试小规模程序的翻译，后来又将这些测试套件自动化。相反，我们提出了一种用于自动化功能性属性测试的代码翻译模型的方法。我们关于转换后代码的一般用户提供的规范涵盖了一系列属性，从纯语法到纯语义。正如我们的实验所示，这种方法在检测流行的代码翻译模型中的属性违规方面非常有效，因此可以根据给定的属性评估模型质量。我们还进一步探索了用户只需获得

    Large language models are becoming increasingly practical for translating code across programming languages, a process known as $transpiling$. Even though automated transpilation significantly boosts developer productivity, a key concern is whether the generated code is correct. Existing work initially used manually crafted test suites to test the translations of a small corpus of programs; these test suites were later automated. In contrast, we devise the first approach for automated, functional, property-based testing of code translation models. Our general, user-provided specifications about the transpiled code capture a range of properties, from purely syntactic to purely semantic ones. As shown by our experiments, this approach is very effective in detecting property violations in popular code translation models, and therefore, in evaluating model quality with respect to given properties. We also go a step further and explore the usage scenario where a user simply aims to obtain a
    
[^156]: 运用噪声图神经网络方法实现差分隐私的顺序推荐研究

    Towards Differential Privacy in Sequential Recommendation: A Noisy Graph Neural Network Approach. (arXiv:2309.11515v1 [cs.CR])

    [http://arxiv.org/abs/2309.11515](http://arxiv.org/abs/2309.11515)

    这项工作提出了一种新颖的差分隐私顺序推荐框架，采用了噪声图神经网络方法，解决了现有差分隐私推荐系统在动态和依赖关系方面的局限性，同时也关注了敏感用户特征的隐私风险。

    

    随着各种在线平台中高调的隐私泄露事件频繁发生，用户对隐私越来越关注。推荐系统作为在线平台提供个性化服务的核心组件，其隐私保护引起了极大的关注。作为隐私保护的黄金标准，差分隐私已被广泛应用于推荐系统中的隐私保护。然而，现有的差分隐私推荐系统只考虑静态和独立的用户交互，因此无法应用于具有动态和依赖关系的顺序推荐。同时，对于敏感用户特征的隐私风险关注较少，大多数只保护用户的反馈数据。在这项工作中，我们提出了一个新颖的差分隐私顺序推荐框架，采用了噪声图神经网络方法（称为DIPSGNN）来解决这些局限性。

    With increasing frequency of high-profile privacy breaches in various online platforms, users are becoming more concerned about their privacy. And recommender system is the core component of online platforms for providing personalized service, consequently, its privacy preservation has attracted great attention. As the gold standard of privacy protection, differential privacy has been widely adopted to preserve privacy in recommender systems. However, existing differentially private recommender systems only consider static and independent interactions, so they cannot apply to sequential recommendation where behaviors are dynamic and dependent. Meanwhile, little attention has been paid on the privacy risk of sensitive user features, most of them only protect user feedbacks. In this work, we propose a novel DIfferentially Private Sequential recommendation framework with a noisy Graph Neural Network approach (denoted as DIPSGNN) to address these limitations. To the best of our knowledge, 
    
[^157]: 切断电路: 通过有针对性的消融去除模型行为

    Circuit Breaking: Removing Model Behaviors with Targeted Ablation. (arXiv:2309.05973v1 [cs.CL])

    [http://arxiv.org/abs/2309.05973](http://arxiv.org/abs/2309.05973)

    本论文提出了一种通过有针对性的消融模型组件之间的因果路径来去除语言模型中不良行为的新方法。在减少GPT-2毒性语言生成方面，仅消融12条因果边中的11.6K可以有效减轻毒性生成，并在其他输入上的性能下降很小。

    

    语言模型通常会表现出在预训练目标上提高性能但在下游任务上降低性能的行为。我们提出了一种新颖的方法，通过消融模型组件之间的一小部分因果路径，以禁用与不良行为有关的计算电路，从而去除不良行为。在拥有模型表现差的小型输入数据集的情况下，我们学会了消融一小部分重要的因果路径。在减少GPT-2毒性语言生成方面，我们发现消融仅仅12条因果边中的11.6K，可以减轻毒性生成，同时在其他输入上的性能下降很小。

    Language models often exhibit behaviors that improve performance on a pre-training objective but harm performance on downstream tasks. We propose a novel approach to removing undesirable behaviors by ablating a small number of causal pathways between model components, with the intention of disabling the computational circuit responsible for the bad behavior. Given a small dataset of inputs where the model behaves poorly, we learn to ablate a small number of important causal pathways. In the setting of reducing GPT-2 toxic language generation, we find ablating just 12 of the 11.6K causal edges mitigates toxic generation with minimal degradation of performance on other inputs.
    
[^158]: 将生成对立假设的过程视为知识来源 - 应用于朴素贝叶斯分类器

    Viewing the process of generating counterfactuals as a source of knowledge -- Application to the Naive Bayes classifier. (arXiv:2309.04284v1 [cs.LG])

    [http://arxiv.org/abs/2309.04284](http://arxiv.org/abs/2309.04284)

    将生成对立假设的过程视为知识来源，并应用于朴素贝叶斯分类器，展示其有趣属性。

    

    现在有许多理解算法可以理解机器学习算法的决策，其中包括基于生成对立假设示例的算法。本文提出将这个生成过程视为一种创造一定量知识的方法，这些知识可以存储并在以后以不同的方式使用。本文在加法模型中进行了说明，具体而言，是在朴素贝叶斯分类器的情况下，展示了其在此目的上的有趣属性。

    There are now many comprehension algorithms for understanding the decisions of a machine learning algorithm. Among these are those based on the generation of counterfactual examples. This article proposes to view this generation process as a source of creating a certain amount of knowledge that can be stored to be used, later, in different ways. This process is illustrated in the additive model and, more specifically, in the case of the naive Bayes classifier, whose interesting properties for this purpose are shown.
    
[^159]: 通过具备模拟器信息的潜在状态学习混合动力学模型

    Learning Hybrid Dynamics Models With Simulator-Informed Latent States. (arXiv:2309.02873v1 [cs.LG])

    [http://arxiv.org/abs/2309.02873](http://arxiv.org/abs/2309.02873)

    本文提出了一种通过用模拟器更新潜在状态的方法来学习混合动力学模型的新方法，以控制预测并防止累积误差的发生。

    

    动力学模型学习的任务是从测量数据中推断未知的系统动力学，并预测系统未来的行为。解决这个问题的一种常见方法是训练递归模型。然而，这些模型的预测通常在物理意义上不合理，并且由于累积误差的存在，随着时间推移，它们的行为会恶化。通常情况下，基于第一原理构建的模拟器是物理意义上合理的。然而，建模简化通常会导致这些模型的不准确性。因此，混合建模是一种新兴的趋势，旨在结合两者的优点。在本文中，我们提出了一种新的混合建模方法，通过黑盒模拟器将学习模型的潜在状态进行更新。通过模拟器来控制预测，防止累积误差的发生，这是一项特别具有挑战性的任务。

    Dynamics model learning deals with the task of inferring unknown dynamics from measurement data and predicting the future behavior of the system. A typical approach to address this problem is to train recurrent models. However, predictions with these models are often not physically meaningful. Further, they suffer from deteriorated behavior over time due to accumulating errors. Often, simulators building on first principles are available being physically meaningful by design. However, modeling simplifications typically cause inaccuracies in these models. Consequently, hybrid modeling is an emerging trend that aims to combine the best of both worlds. In this paper, we propose a new approach to hybrid modeling, where we inform the latent states of a learned model via a black-box simulator. This allows to control the predictions via the simulator preventing them from accumulating errors. This is especially challenging since, in contrast to previous approaches, access to the simulator's la
    
[^160]: 连续时间高斯过程动力学的精确推断

    Exact Inference for Continuous-Time Gaussian Process Dynamics. (arXiv:2309.02351v1 [cs.LG])

    [http://arxiv.org/abs/2309.02351](http://arxiv.org/abs/2309.02351)

    本论文提出了一种对连续时间高斯过程动力学进行精确推断的方法，解决了在离散时间下进行预测可能带来的问题，并利用高阶数值积分器进行动力学函数的离散化，避免了传统方法中的近似推断的限制。

    

    实际物理系统通常可以通过连续时间动力系统来描述。在实际应用中，真实系统通常是未知的，需要从测量数据中学习。由于数据通常以离散时间收集，例如通过传感器，高斯过程（GP）动力模型学习中的大多数方法都是针对一步预测进行训练的。在一些场景中，这可能会导致问题，例如如果测量结果以不规则的时间步长提供，或者物理系统属性需要保持不变。因此，我们的目标是建立对真实连续时间动力学的GP模型。高阶数值积分器提供了通过任意精度离散化动力学函数来解决这个问题的工具。许多高阶积分器需要在中间时间步骤进行动力学评估，这使得精确的GP推断变得难以处理。在先前的工作中，通常通过使用变分推断来近似GP后验来解决这个问题。然而，精确的GP推断是很困难的。

    Physical systems can often be described via a continuous-time dynamical system. In practice, the true system is often unknown and has to be learned from measurement data. Since data is typically collected in discrete time, e.g. by sensors, most methods in Gaussian process (GP) dynamics model learning are trained on one-step ahead predictions. This can become problematic in several scenarios, e.g. if measurements are provided at irregularly-sampled time steps or physical system properties have to be conserved. Thus, we aim for a GP model of the true continuous-time dynamics. Higher-order numerical integrators provide the necessary tools to address this problem by discretizing the dynamics function with arbitrary accuracy. Many higher-order integrators require dynamics evaluations at intermediate time steps making exact GP inference intractable. In previous work, this problem is often tackled by approximating the GP posterior with variational inference. However, exact GP inference is pre
    
[^161]: 用于湍流流场模拟的自回归条件扩散模型的基准测试

    Benchmarking Autoregressive Conditional Diffusion Models for Turbulent Flow Simulation. (arXiv:2309.01745v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.01745](http://arxiv.org/abs/2309.01745)

    这项工作研究了机器学习求解器在模拟湍流流场时如何实现时间稳定性，并发现基于条件扩散模型的自回归演化方法在准确性和稳定性方面可以超越其他流场预测方法。

    

    模拟湍流流场对于许多应用至关重要，而基于机器学习的求解器日益受到重视。然而，对于学习的PDE求解器来说，在推广到更长的演化时间中实现时间稳定性仍然是一个持久的挑战。在这项工作中，我们分析了完全数据驱动的流体求解器是否利用基于条件扩散模型的自回归演化是解决这一问题的可行选择。我们研究了准确性、后验采样、谱特性和时间稳定性，并要求这些方法能够推广到超出训练范围的流动参数。为了定量和定性地对一系列流场预测方法的性能进行基准测试，我们采用了三个具有挑战性的场景，包括不可压缩流动、跨音速流动和各向同性湍流。我们发现，即使是简单的基于扩散的方法在准确性和...

    Simulating turbulent flows is crucial for a wide range of applications, and machine learning-based solvers are gaining increasing relevance. However, achieving temporal stability when generalizing to longer rollout horizons remains a persistent challenge for learned PDE solvers. In this work, we analyze if fully data-driven fluid solvers that utilize an autoregressive rollout based on conditional diffusion models are a viable option to address this challenge. We investigate accuracy, posterior sampling, spectral behavior, and temporal stability, while requiring that methods generalize to flow parameters beyond the training regime. To quantitatively and qualitatively benchmark the performance of a range of flow prediction approaches, three challenging scenarios including incompressible and transonic flows, as well as isotropic turbulence are employed. We find that even simple diffusion-based approaches can outperform multiple established flow prediction methods in terms of accuracy and 
    
[^162]: 数据驱动的线性规划降维方法：泛化界限和学习方法

    Data-Driven Projection for Reducing Dimensionality of Linear Programs: Generalization Bound and Learning Methods. (arXiv:2309.00203v1 [cs.LG])

    [http://arxiv.org/abs/2309.00203](http://arxiv.org/abs/2309.00203)

    本文研究了一种简单的数据驱动方法，通过学习投影矩阵来降低高维线性规划问题的维数，实现更快的求解速度。基于“数据驱动算法设计”，提出了泛化保证的数据量与性能指标的伪维度的上界和下界。

    

    本文研究了一种简单的数据驱动方法来处理高维线性规划问题（LP）。给定过去的$n$维LP数据，我们学习一个$n\times k$的“投影矩阵”（$n > k$），将维数从$n$降低到$k$。然后，我们通过解决$k$维LP问题并通过乘以投影矩阵来恢复$n$维的解决方案来处理未来的LP实例。这个思想与任何用户首选的LP求解器兼容，因此是一种通用的加速LP求解的方法。一个自然的问题是：需要多少数据才能确保恢复的解决方案的质量？我们基于“数据驱动算法设计”的思想来回答这个问题，它将足够进行泛化保证的数据量与性能指标的“伪维度”联系起来。我们给出了伪维度的$\tilde{\mathrm{O}}(nk^2)$上界（$\tilde{\mathrm{O}}$压缩了对数因子），并通过一个$\Omega(nk)$下界来补充它，

    This paper studies a simple data-driven approach to high-dimensional linear programs (LPs). Given data of past $n$-dimensional LPs, we learn an $n\times k$ \textit{projection matrix} ($n > k$), which reduces the dimensionality from $n$ to $k$. Then, we address future LP instances by solving $k$-dimensional LPs and recovering $n$-dimensional solutions by multiplying the projection matrix. This idea is compatible with any user-preferred LP solvers, hence a versatile approach to faster LP solving. One natural question is: how much data is sufficient to ensure the recovered solutions' quality? We address this question based on the idea of \textit{data-driven algorithm design}, which relates the amount of data sufficient for generalization guarantees to the \textit{pseudo-dimension} of performance metrics. We present an $\tilde{\mathrm{O}}(nk^2)$ upper bound on the pseudo-dimension ($\tilde{\mathrm{O}}$ compresses logarithmic factors) and complement it by an $\Omega(nk)$ lower bound, hence 
    
[^163]: TransGNN: 利用Transformer和图神经网络的协同能力来做推荐系统

    TransGNN: Harnessing the Collaborative Power of Transformers and Graph Neural Networks for Recommender Systems. (arXiv:2308.14355v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.14355](http://arxiv.org/abs/2308.14355)

    TransGNN是一种将Transformer和GNN层交替结合以相互增强其能力的新型模型，用于解决当前基于GNN的推荐系统面临的感受域有限和存在噪音连接的挑战。

    

    图神经网络(GNNs)已经被证明是推荐系统中有前途的解决方案，通过对用户-物品交互图进行建模来进行协同过滤(CF)。现有基于GNN的推荐系统的核心是通过在用户-物品交互边上进行递归消息传递来改进编码嵌入。尽管它们已经证明是有效的，但是当前基于GNN的方法面临着有限的感受域和存在噪音 "兴趣无关" 连接的挑战。相比之下，基于Transformer的方法在自适应和全局信息聚合方面表现出色。然而，它们在捕捉复杂、纠缠的结构信息方面在大规模交互图中的应用受到困扰。在本文中，我们提出了TransGNN，这是一种新颖的模型，通过交替地结合Transformer和GNN层来相互增强它们的能力。

    Graph Neural Networks (GNNs) have emerged as promising solutions for collaborative filtering (CF) through the modeling of user-item interaction graphs. The nucleus of existing GNN-based recommender systems involves recursive message passing along user-item interaction edges to refine encoded embeddings. Despite their demonstrated effectiveness, current GNN-based methods encounter challenges of limited receptive fields and the presence of noisy ``interest-irrelevant'' connections. In contrast, Transformer-based methods excel in aggregating information adaptively and globally. Nevertheless, their application to large-scale interaction graphs is hindered by inherent complexities and challenges in capturing intricate, entangled structural information. In this paper, we propose TransGNN, a novel model that integrates Transformer and GNN layers in an alternating fashion to mutually enhance their capabilities. Specifically, TransGNN leverages Transformer layers to broaden the receptive field 
    
[^164]: 有效的语言模型基准测试

    Efficient Benchmarking (of Language Models). (arXiv:2308.11696v1 [cs.CL])

    [http://arxiv.org/abs/2308.11696](http://arxiv.org/abs/2308.11696)

    本研究提出了一种名为"Efficient Benchmarking"的问题，旨在智能地减少语言模型评估的计算成本而不降低可靠性，并使用一种名为Decision Impact on Reliability（DIoR）的新度量来评估决策的可靠性。通过HELM基准测试的案例研究，发现只需删除一个低排名模型即可改变领先者，并仅需少量示例即可得到正确的基准测试排名。

    

    语言模型的多功能性增加导致了一类全面评估广泛能力的基准测试的出现。这些基准测试与大规模计算成本相关，每个模型需要数千个GPU小时。然而，关于评估效率方面的问题在文献中讨论较少。本文提出了一种名为"Efficient Benchmarking"的问题，即在不损害可靠性的情况下智能地减少语言模型评估的计算成本。通过使用HELM基准测试作为示例，我们研究了不同基准测试设计选择如何影响计算-可靠性权衡。我们提出使用一种名为Decision Impact on Reliability（DIoR）的新度量来评估这些决策的可靠性。例如，我们发现仅通过从基准测试中删除一个低排名模型，当前在HELM上的领先者可能会改变，并且观察到只需一小部分示例即可获得正确的基准测试排名。

    The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark rank
    
[^165]: 使用切片Wasserstein损失来训练神经网络的SGD收敛性分析

    Convergence of SGD for Training Neural Networks with Sliced Wasserstein Losses. (arXiv:2307.11714v1 [cs.LG])

    [http://arxiv.org/abs/2307.11714](http://arxiv.org/abs/2307.11714)

    本论文研究了使用切片Wasserstein损失训练神经网络时，随机梯度下降算法的收敛性，并证明了在特定条件下，SGD轨迹逼近了梯度流方程的集合。

    

    最优输运近年来引发了广泛的兴趣，特别是由于Wasserstein距离，它提供了一种几何上合理和直观的比较概率测度的方法。出于计算原因，切片Wasserstein（SW）距离作为Wasserstein距离的一种替代方法被引入，并且已经被用于训练生成式神经网络（NNs）。虽然在这样的设置中实际观察到了随机梯度下降（SGD）的收敛性，但据我们所知，对于这一观察没有理论保证。借鉴Bianchi等人（2022）关于SGD在非光滑和非凸函数上收敛性的最新工作，我们旨在填补这一知识空白，并提供一个具有实际意义的上下文，使得SW损失对NN参数的固定步长SGD轨迹收敛到（次）梯度流方程的集合。更准确地说，我们证明了随着步长减小，这些轨迹逼近了梯度流方程的集合。

    Optimal Transport has sparked vivid interest in recent years, in particular thanks to the Wasserstein distance, which provides a geometrically sensible and intuitive way of comparing probability measures. For computational reasons, the Sliced Wasserstein (SW) distance was introduced as an alternative to the Wasserstein distance, and has seen uses for training generative Neural Networks (NNs). While convergence of Stochastic Gradient Descent (SGD) has been observed practically in such a setting, there is to our knowledge no theoretical guarantee for this observation. Leveraging recent works on convergence of SGD on non-smooth and non-convex functions by Bianchi et al. (2022), we aim to bridge that knowledge gap, and provide a realistic context under which fixed-step SGD trajectories for the SW loss on NN parameters converge. More precisely, we show that the trajectories approach the set of (sub)-gradient flow equations as the step decreases. Under stricter assumptions, we show a much st
    
[^166]: ENN: 一种具有DCT自适应激活函数的神经网络

    ENN: A Neural Network with DCT Adaptive Activation Functions. (arXiv:2307.00673v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2307.00673](http://arxiv.org/abs/2307.00673)

    ENN是一种具有DCT自适应激活函数的神经网络模型，通过使用反向传播自适应地调整激活函数，提供了高度的灵活性和表现力。在解释网络收敛过程中，我们恢复了每个激活函数在输出空间中的响应，即“凸起”。通过实验证明了该模型在多个任务上的性能优势。

    

    神经网络的表达能力高度取决于激活函数的性质，尽管这些通常在训练阶段被假定为预定义和固定的。在信号处理的视角下，本文提出了一种新颖的模型——表达神经网络(ENN)，其中非线性激活函数使用离散余弦变换(DCT)进行建模，并且在训练过程中使用反向传播进行自适应。这种参数化方法将可训练参数的数量保持较低，适合基于梯度的方案，并能适应不同的学习任务。这是第一个在激活函数方面依赖于信号处理视角的非线性模型，为网络提供了高度的灵活性和表现力。我们通过恢复“凸起”的概念来为网络在收敛时的可解释性提供了新的见解，即每个激活函数在输出空间中的响应。最后，通过详尽的实验，我们展示了该模型在多个任务上的性能优势。

    The expressiveness of neural networks highly depends on the nature of the activation function, although these are usually assumed predefined and fixed during the training stage. Under a signal processing perspective, in this paper we present Expressive Neural Network (ENN), a novel model in which the non-linear activation functions are modeled using the Discrete Cosine Transform (DCT) and adapted using backpropagation during training. This parametrization keeps the number of trainable parameters low, is appropriate for gradient-based schemes, and adapts to different learning tasks. This is the first non-linear model for activation functions that relies on a signal processing perspective, providing high flexibility and expressiveness to the network. We contribute with insights in the explainability of the network at convergence by recovering the concept of bump, this is, the response of each activation function in the output space. Finally, through exhaustive experiments we show that th
    
[^167]: 高维线性回归的统一转移学习模型

    Unified Transfer Learning Models for High-Dimensional Linear Regression. (arXiv:2307.00238v1 [stat.ML])

    [http://arxiv.org/abs/2307.00238](http://arxiv.org/abs/2307.00238)

    UTrans是一种统一转移学习模型，它能检测可转移变量和源数据，并具有较低的估计和预测误差，同时保持可解释性。

    

    在现代数据分析中，当目标数据稀缺而源数据充足，或者源数据和目标数据的分布不同的情况下，转移学习在发挥重要作用。本文提出了一种可解释的统一转移学习模型，称为UTrans，该模型能够检测可转移变量和源数据。具体来说，我们建立了估计误差界限，并证明我们的界限低于仅有目标数据的界限。此外，我们基于假设检验提出了一种源数据检测算法，用于排除不可转移的数据。我们在多个实验中评估和比较了UTrans与现有算法。结果显示，UTrans在保持可解释性的同时，比现有方法具有更低的估计和预测误差。最后，我们将其应用于美国代际流动数据，并将我们提出的算法与经典的机器学习算法进行比较。

    Transfer learning plays a key role in modern data analysis when: (1) the target data are scarce but the source data are sufficient; (2) the distributions of the source and target data are heterogeneous. This paper develops an interpretable unified transfer learning model, termed as UTrans, which can detect both transferable variables and source data. More specifically, we establish the estimation error bounds and prove that our bounds are lower than those with target data only. Besides, we propose a source detection algorithm based on hypothesis testing to exclude the nontransferable data. We evaluate and compare UTrans to the existing algorithms in multiple experiments. It is shown that UTrans attains much lower estimation and prediction errors than the existing methods, while preserving interpretability. We finally apply it to the US intergenerational mobility data and compare our proposed algorithms to the classical machine learning algorithms.
    
[^168]: MILD: 模型化学习动态，用于学习带有噪声标签的数据

    MILD: Modeling the Instance Learning Dynamics for Learning with Noisy Labels. (arXiv:2306.11560v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11560](http://arxiv.org/abs/2306.11560)

    MILD模型化了学习动态，通过基于Weibull混合模型的迭代选择方法，识别干净的数据实例，以减少网络对带有噪声标签的数据的记忆影响。

    

    尽管深度学习取得了巨大的成功，但它通常依赖于大量带有准确标签的训练数据，而这些数据的收集成本高且耗时。降低成本的一个重要方向是学习带噪声标签的数据，这在现实世界的应用中普遍存在。对于这样的学习任务，一个关键挑战是减少网络对错误标签数据的记忆效应。在这项工作中，我们提出了一种基于Weibull混合模型的迭代选择方法，通过考虑每个数据实例的整体学习动态来识别干净的数据。与先前的小损失启发式方法不同，我们利用了深度网络容易记忆和难以忘记干净数据的观察结果。特别地，我们通过训练期间被错误分类和被记忆之间的转换次数来测量每个实例的记忆和遗忘难度，并将它们整合到一个新的选择指标中。

    Despite deep learning has achieved great success, it often relies on a large amount of training data with accurate labels, which are expensive and time-consuming to collect. A prominent direction to reduce the cost is to learn with noisy labels, which are ubiquitous in the real-world applications. A critical challenge for such a learning task is to reduce the effect of network memorization on the falsely-labeled data. In this work, we propose an iterative selection approach based on the Weibull mixture model, which identifies clean data by considering the overall learning dynamics of each data instance. In contrast to the previous small-loss heuristics, we leverage the observation that deep network is easy to memorize and hard to forget clean data. In particular, we measure the difficulty of memorization and forgetting for each instance via the transition times between being misclassified and being memorized in training, and integrate them into a novel metric for selection. Based on th
    
[^169]: ChatGPT和其他类似系统是AI的现代勒纳恩九头蛇吗？

    Are ChatGPT and Other Similar Systems the Modern Lernaean Hydras of AI?. (arXiv:2306.09267v3 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2306.09267](http://arxiv.org/abs/2306.09267)

    生成式人工智能系统的崛起引发了版权和创新保护的问题。建议对开源代码许可证进行更改，限制AI系统对代码的访问和使用，并探讨与AI和版权之间的关系引发的问题。

    

    生成式人工智能系统的崛起引发了前所未有的社交参与。AI代码生成系统通过访问过去几十年开发人员创建的大量开源代码来提供回答或响应问题或请求。然而，它们被指控窃取存储在虚拟库中的开源代码。本文着重讨论此类问题，并探讨是否有解决方案来保护创新并避免多年的诉讼。对AI和版权之间的关系引发的一系列问题也进行了讨论。展望未来，我们提出以下建议：(a)对开发人员创建的开源代码的许可证进行即时更改，限制对任何开源代码的访问和/或使用仅限于人类；(b)建议对麻省理工学院（MIT）许可证进行修订，要求AI系统从开源代码部分获取适当的许可证。

    The rise of Generative Artificial Intelligence systems ("AI systems") has created unprecedented social engagement. AI code generation systems provide responses (output) to questions or requests by accessing the vast library of open-source code created by developers over the past few decades. However, they do so by allegedly stealing the open-source code stored in virtual libraries, known as repositories. This Article focuses on how this happens and whether there is a solution that protects innovation and avoids years of litigation. We also touch upon the array of issues raised by the relationship between AI and copyright. Looking ahead, we propose the following: (a) immediate changes to the licenses for open-source code created by developers that will limit access and/or use of any open-source code to humans only; (b) we suggest revisions to the Massachusetts Institute of Technology ("MIT") license so that AI systems are required to procure appropriate licenses from open-source code de
    
[^170]: 向量量化图自编码器

    Vector-Quantized Graph Auto-Encoder. (arXiv:2306.07735v1 [cs.LG])

    [http://arxiv.org/abs/2306.07735](http://arxiv.org/abs/2306.07735)

    本文介绍了一种用于建模图分布的置换等变离散自编码器(VQ-GAE)，它采用向量量化防止将离散对象映射到连续潜在空间中，并利用自回归模型捕获了图的全局结构，实验表明具有优异性能。

    

    本文研究了建模图的分布的问题。文章介绍了一种置换等变离散自编码器(VQ-GAE)，旨在设计用于建模图的分布。通过利用图神经网络(GNN)的置换等变性，我们的自编码器绕过了图表示的排序问题。我们利用GNN捕捉图的局部结构的能力，同时采用向量量化来防止将离散对象映射到连续的潜在空间中。此外，自回归模型的使用使我们能够通过潜在表示捕获图的全局结构。我们在用于图生成的标准数据集上评估了我们的模型，并观察到与现有最先进方法相比，在一些最突出的评估指标上取得了出色的性能。

    In this work, we addresses the problem of modeling distributions of graphs. We introduce the Vector-Quantized Graph Auto-Encoder (VQ-GAE), a permutation-equivariant discrete auto-encoder and designed to model the distribution of graphs. By exploiting the permutation-equivariance of graph neural networks (GNNs), our autoencoder circumvents the problem of the ordering of the graph representation. We leverage the capability of GNNs to capture local structures of graphs while employing vector-quantization to prevent the mapping of discrete objects to a continuous latent space. Furthermore, the use of autoregressive models enables us to capture the global structure of graphs via the latent representation. We evaluate our model on standard datasets used for graph generation and observe that it achieves excellent performance on some of the most salient evaluation metrics compared to the state-of-the-art.
    
[^171]: 简单且可控的音乐生成

    Simple and Controllable Music Generation. (arXiv:2306.05284v1 [cs.SD])

    [http://arxiv.org/abs/2306.05284](http://arxiv.org/abs/2306.05284)

    本文提出了 MusicGen，一个单一的语言模型，可以在条件描述或旋律特征控制下生成高质量的样本，并且在标准的文本到音乐基准上的实证研究中，该方法优于其他基线模型。

    

    本研究解决了条件音乐生成的问题。我们介绍了MusicGen，它是一个单一的语言模型，可以操作多个压缩离散音乐表示流，即令牌。与以往的工作不同，MusicGen由一个单一阶段的Transformer LM和高效的令牌交错模式组成，消除了级联多个模型的需要，例如分层或上采样。采用这种方法，我们展示了MusicGen如何在条件描述或旋律特征的控制下生成高质量的样本。我们进行了广泛的实证评估，考虑了自动和人为研究，展示了所提出的方法优于标准文本到音乐基准上评估的基线。通过消融研究，我们阐明了MusicGen所包含组件的重要性。音乐样本、代码和模型可以在https://github.com/fac找到。

    We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at https://github.com/fac
    
[^172]: 零样本盲音频带宽扩展

    Zero-Shot Blind Audio Bandwidth Extension. (arXiv:2306.01433v1 [eess.AS])

    [http://arxiv.org/abs/2306.01433](http://arxiv.org/abs/2306.01433)

    本文提出了一种名为BABE的新方法，在零样本情况下解决了具有挑战性的盲音频带宽扩展问题，实现了比最先进的盲带宽扩展基线更好的性能，并且表现出了强大的泛化能力。

    

    音频带宽扩展涉及从带宽受限的观测信号中实现高频谱的逼真重建。在低通信号退化未知的情况下，比如对历史音频记录的恢复，这便成了一个盲问题。本文提出了一种名为BABE的新方法（Blind Audio Bandwidth Extension），在零样本情况下解决了盲问题，利用了预训练的无条件扩散模型的生成先验。在推断过程中，BABE利用了一个广义版本的扩散后验采样，在其中退化算子是未知的，但是通过迭代进行参数化和推断。提出的方法的性能是用客观和主观指标评估的，结果表明BABE超过了最先进的盲带宽扩展基线，并且在测试合成数据时与非盲滤波器知情方法相比实现了有竞争力的性能。此外，BABE表现出了强大的泛化能力。

    Audio bandwidth extension involves the realistic reconstruction of high-frequency spectra from bandlimited observations. In cases where the lowpass degradation is unknown, such as in restoring historical audio recordings, this becomes a blind problem. This paper introduces a novel method called BABE (Blind Audio Bandwidth Extension) that addresses the blind problem in a zero-shot setting, leveraging the generative priors of a pre-trained unconditional diffusion model. During the inference process, BABE utilizes a generalized version of diffusion posterior sampling, where the degradation operator is unknown but parametrized and inferred iteratively. The performance of the proposed method is evaluated using objective and subjective metrics, and the results show that BABE surpasses state-of-the-art blind bandwidth extension baselines and achieves competitive performance compared to non-blind filter-informed methods when tested with synthetic data. Moreover, BABE exhibits robust generaliza
    
[^173]: DoWG展示：一种高效的通用无参数梯度下降方法

    DoWG Unleashed: An Efficient Universal Parameter-Free Gradient Descent Method. (arXiv:2305.16284v1 [cs.LG])

    [http://arxiv.org/abs/2305.16284](http://arxiv.org/abs/2305.16284)

    本文提出了一种名为DoWG的无参数梯度下降方法，它是第一个既高效又通用的算法，能够自适应于平稳和非平稳问题，并且无需回溯搜索过程。

    

    本文提出了一种新的易于实现的无参数梯度优化器：DoWG（Weighted Gradients的距离）。我们证明了该方法是高效的——在不调整任何参数的情况下，匹配优化凸优化中最优调的梯度下降的收敛速度，直到对数因子，并且是通用的——自动适应平滑和非平滑问题。与AdaGrad，Adam或DoG等流行算法计算平方梯度的运行平均值不同，DoWG保持运行平均值的一种新的基于距离的加权版本，这对于实现所需的性质至关重要。据我们所知，DoWG是第一个不需要回溯搜索过程的无参数，高效和通用算法。它还是第一个适应于平稳优化的无参数AdaGrad样式算法。为了补充我们的理论，我们还通过实验证明DoWG在稳定的边缘训练，并证明其在实践中的有效性。

    This paper proposes a new easy-to-implement parameter-free gradient-based optimizer: DoWG (Distance over Weighted Gradients). We prove that DoWG is efficient -- matching the convergence rate of optimally tuned gradient descent in convex optimization up to a logarithmic factor without tuning any parameters, and universal -- automatically adapting to both smooth and nonsmooth problems. While popular algorithms such as AdaGrad, Adam, or DoG compute a running average of the squared gradients, DoWG maintains a new distance-based weighted version of the running average, which is crucial to achieve the desired properties. To our best knowledge, DoWG is the first parameter-free, efficient, and universal algorithm that does not require backtracking search procedures. It is also the first parameter-free AdaGrad style algorithm that adapts to smooth optimization. To complement our theory, we also show empirically that DoWG trains at the edge of stability, and validate its effectiveness on practic
    
[^174]: 通过求解最优边界条件解决扩散ODE问题以实现更好的图像超分辨率

    Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image Super-Resolution. (arXiv:2305.15357v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2305.15357](http://arxiv.org/abs/2305.15357)

    本文提出了一种求解最优边界条件解决扩散ODE问题的有效采样方法，以稳定地从预训练的基于扩散的超分辨率模型中采样高质量的超分辨率图像。

    

    扩散模型作为一种强大的生成模型，已经在图像超分辨率任务中取得了令人印象深刻的结果。然而，由于扩散模型反向过程中引入的随机性，基于扩散的超分辨率模型在每次采样时性能波动很大，特别是对于具有少量重新采样步骤的采样器。扩散模型的这种固有随机性导致其无效和不稳定，使用户难以保证超分辨结果的质量。然而，我们的工作将这种随机性视为一种机遇：全面分析和利用它导致了构建一种有效的即插即用采样方法，具有潜力使一系列基于扩散的超分辨率方法受益。更详细地说，我们建议通过求解扩散普通微分方程（扩散ODE）和最优边界条件（BC），稳定地从预训练的基于扩散的超分辨率模型中采样高质量的超分辨率图像，并分析其特性。

    Diffusion models, as a kind of powerful generative model, have given impressive results on image super-resolution (SR) tasks. However, due to the randomness introduced in the reverse process of diffusion models, the performances of diffusion-based SR models are fluctuating at every time of sampling, especially for samplers with few resampled steps. This inherent randomness of diffusion models results in ineffectiveness and instability, making it challenging for users to guarantee the quality of SR results. However, our work takes this randomness as an opportunity: fully analyzing and leveraging it leads to the construction of an effective plug-and-play sampling method that owns the potential to benefit a series of diffusion-based SR methods. More in detail, we propose to steadily sample high-quality SR images from pretrained diffusion-based SR models by solving diffusion ordinary differential equations (diffusion ODEs) with optimal boundary conditions (BCs) and analyze the characterist
    
[^175]: 质量多样性强化学习中的近端策略梯度树枝方法

    Proximal Policy Gradient Arborescence for Quality Diversity Reinforcement Learning. (arXiv:2305.13795v1 [cs.LG])

    [http://arxiv.org/abs/2305.13795](http://arxiv.org/abs/2305.13795)

    本论文提出了一种将近端策略优化(PPO)方法与质量多样性(QD)相结合的新型QD-RL方法，用于在高吞吐量、大规模并行化机器人模拟器环境下训练能够在未知动态环境中表现出色的机器人学习智能体。

    

    培训通常能够在未知动态环境中表现良好的机器人学习智能体是一个长期目标。质量多样性强化学习(QD-RL)是一类新兴的强化学习算法，它将质量多样性(QD)和RL的见解相结合，产生一系列关于行为嵌入的高性能和行为多样性的策略集。然而，现有的QD-RL方法迄今为止利用了样本有效的离策略RL算法。然而，最近高吞吐量、大规模并行化的机器人模拟器的进步已经打开了能够利用这种并行性的算法的大门，而将现有的离策略QD-RL方法扩展到这些新的数据丰富的环境还不清楚。在这项工作中，我们首次采用了能够利用大规模并行性的近端策略优化(PPO)等策略方法与QD相结合，提出了一种新的QD-RL方法。

    Training generally capable agents that perform well in unseen dynamic environments is a long-term goal of robot learning. Quality Diversity Reinforcement Learning (QD-RL) is an emerging class of reinforcement learning (RL) algorithms that blend insights from Quality Diversity (QD) and RL to produce a collection of high performing and behaviorally diverse policies with respect to a behavioral embedding. Existing QD-RL approaches have thus far taken advantage of sample-efficient off-policy RL algorithms. However, recent advances in high-throughput, massively parallelized robotic simulators have opened the door for algorithms that can take advantage of such parallelism, and it is unclear how to scale existing off-policy QD-RL methods to these new data-rich regimes. In this work, we take the first steps to combine on-policy RL methods, specifically Proximal Policy Optimization (PPO), that can leverage massive parallelism, with QD, and propose a new QD-RL method with these high-throughput s
    
[^176]: 文本预训练的语音语言模型

    Textually Pretrained Speech Language Models. (arXiv:2305.13009v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13009](http://arxiv.org/abs/2305.13009)

    本论文提出了一种使用预训练的文本语言模型训练语音语言模型的方法，通过对模型设计选择和数据集规模的经验性分析，构建了参数数量和训练数据最多的语音语言模型，并引入了两个Spoken版本的文本基准，以进一步改善模型评估和推动未来研究。

    

    语音语言模型（SpeechLMs）仅处理和生成音频数据，没有文字监督。在这项工作中，我们提出了TWIST，一种使用预训练的文本语言模型进行SpeechLMs训练的方法。我们通过自动和人工评估表明，TWIST在各个方面都优于冷启动的SpeechLM。我们经验性地分析了不同的模型设计选择（如语音分词器、预训练的文本模型和数据集大小）的影响。我们发现模型和数据集规模在构建性能更好的SpeechLMs方面都起着重要作用。基于我们的观察，我们介绍了迄今为止参数数量和训练数据最多的SpeechLM（据我们所知）。此外，我们还引入了两个Spoken版本的StoryCloze文本基准，以进一步改善模型评估并推动该领域的未来研究。我们公开提供语音样本、代码和模型：https://pages.cs.huji.ac.il/

    Speech language models (SpeechLMs) process and generate acoustic data only, without textual supervision. In this work, we propose TWIST, a method for training SpeechLMs using a warm-start from a pretrained textual language models. We show using both automatic and human evaluations that TWIST outperforms a cold-start SpeechLM across the board. We empirically analyze the effect of different model design choices such as the speech tokenizer, the pretrained textual model, and the dataset size. We find that model and dataset scale both play an important role in constructing better-performing SpeechLMs. Based on our observations, we present the largest (to the best of our knowledge) SpeechLM both in terms of number of parameters and training data. We additionally introduce two spoken versions of the StoryCloze textual benchmark to further improve model evaluation and advance future research in the field. We make speech samples, code and models publicly available: https://pages.cs.huji.ac.il/
    
[^177]: 融合归因重要性以提高忠实度评估的方法

    Incorporating Attribution Importance for Improving Faithfulness Metrics. (arXiv:2305.10496v1 [cs.CL])

    [http://arxiv.org/abs/2305.10496](http://arxiv.org/abs/2305.10496)

    本研究提出了一种软删除标准来评估归因方法的忠实度，该方法随机遮盖标记的部分向量表示，这种方法比现有的硬删除标准更准确。

    

    特征归因方法是提供对模型推理过程进行预测的流行方法。一个更加准确的归因方法标志着它更加忠实，它可以更加准确地反映哪些部分的输入对预测更加重要。然而，现有的忠实度评估方法，如充分性和全面性，只使用一种硬删除标准，即完全删除或保留由给定归因方法排名最高的顶部标记，并观察预测可能性的变化。因此，这种硬删除标准忽略了每个标记的重要性，把它们全部等同地处理。在本文中，我们提出了一个简单而有效的软删除标准。我们不会完全删除或保留输入中的标记，而是随机地遮盖代表归因方法重要性的部分标记向量表示。基于各种自然语言处理任务和不同的归因方法进行的广泛实验表明，我们的方法显著优于现有的评估方法。

    Feature attribution methods (FAs) are popular approaches for providing insights into the model reasoning process of making predictions. The more faithful a FA is, the more accurately it reflects which parts of the input are more important for the prediction. Widely used faithfulness metrics, such as sufficiency and comprehensiveness use a hard erasure criterion, i.e. entirely removing or retaining the top most important tokens ranked by a given FA and observing the changes in predictive likelihood. However, this hard criterion ignores the importance of each individual token, treating them all equally for computing sufficiency and comprehensiveness. In this paper, we propose a simple yet effective soft erasure criterion. Instead of entirely removing or retaining tokens from the input, we randomly mask parts of the token vector representations proportionately to their FA importance. Extensive experiments across various natural language processing tasks and different FAs show that our sof
    
[^178]: FedPDD：用于跨平台联邦推荐的隐私保护双重蒸馏框架

    FedPDD: A Privacy-preserving Double Distillation Framework for Cross-silo Federated Recommendation. (arXiv:2305.06272v1 [cs.IR])

    [http://arxiv.org/abs/2305.06272](http://arxiv.org/abs/2305.06272)

    本文提出了一个名为FedPDD的隐私保护双重蒸馏框架，用于跨平台联邦推荐。该框架包括教师蒸馏和学生蒸馏两个阶段，在不传输模型信息的情况下，通过有效地转移知识和使用一种新的蒸馏损失函数来构建全局模型，实现了最先进的性能。

    

    跨平台推荐旨在通过从不同平台收集异构特征来提高推荐准确性。然而，越来越严格的隐私保护法规限制了这种平台间的跨界协作，因此不能聚合数据用于训练。联邦学习（FL）是推荐场景中处理数据孤岛问题的实用解决方案。现有的跨平台FL方法通过利用重叠用户的数据协同构建全局模型，传输模型信息。然而，在现实中，重叠用户的数量往往非常小，从而大大限制了此类方法的性能。此外，训练期间传输模型信息需要高通信成本，可能会造成严重的隐私泄露。本文提出了一种新的隐私保护双重蒸馏框架 FedPDD 用于跨平台联邦推荐，该框架通过有效地转移知识来保护隐私。FedPDD 包括两个阶段：教师蒸馏和学生蒸馏。在教师蒸馏阶段，每个平台在自己的数据上训练本地模型，并将来自这些模型的知识蒸馏到一个小的、带有噪声的教师模型中。然后，在学生蒸馏阶段，每个平台通过一种新的蒸馏损失函数，同时从教师模型和本地数据中学习，训练自己的学生模型。FedPDD 在两个真实世界的跨平台联邦推荐数据集上实现了最先进的性能，同时保护隐私。

    Cross-platform recommendation aims to improve recommendation accuracy by gathering heterogeneous features from different platforms. However, such cross-silo collaborations between platforms are restricted by increasingly stringent privacy protection regulations, thus data cannot be aggregated for training. Federated learning (FL) is a practical solution to deal with the data silo problem in recommendation scenarios. Existing cross-silo FL methods transmit model information to collaboratively build a global model by leveraging the data of overlapped users. However, in reality, the number of overlapped users is often very small, thus largely limiting the performance of such approaches. Moreover, transmitting model information during training requires high communication costs and may cause serious privacy leakage. In this paper, we propose a novel privacy-preserving double distillation framework named FedPDD for cross-silo federated recommendation, which efficiently transfers knowledge wh
    
[^179]: 主动的连续学习：在任务序列中标记查询。

    Active Continual Learning: Labelling Queries in a Sequence of Tasks. (arXiv:2305.03923v1 [cs.LG])

    [http://arxiv.org/abs/2305.03923](http://arxiv.org/abs/2305.03923)

    本文考虑了一系列主动学习任务的主动连续学习问题，研究了不同场景下多种主动和连续学习算法之间的有效性和相互作用，并提出了遗忘-学习曲线方法来平衡不忘旧知识和快速学习的两个目标。

    

    在连续学习（CL）中，获取新知识并不忘记已学内容是其核心。而任务是按顺序出现的，训练数据的准备和注释则通常是独立的，因此需要连续学习来适应新的监督学习任务。本文考虑了一系列主动学习（AL）任务的主动连续学习（ACL）中未被充分探索的问题，每个任务包括一个未标记的数据池和一个注释预算。我们研究了几种AL和CL算法在不同领域，类别和任务增量场景中的有效性和相互作用。实验揭示了不忘旧知识和快速学习在CL和AL中之间的权衡。尽管在以前任务的注释收集上条件查询策略会提高领域和任务增量学习的任务性能，但我们提出的遗忘-学习曲线则更好地平衡了这两个目标。

    Acquiring new knowledge without forgetting what has been learned in a sequence of tasks is the central focus of continual learning (CL). While tasks arrive sequentially, the training data are often prepared and annotated independently, leading to CL of incoming supervised learning tasks. This paper considers the under-explored problem of active continual learning (ACL) for a sequence of active learning (AL) tasks, where each incoming task includes a pool of unlabelled data and an annotation budget. We investigate the effectiveness and interplay between several AL and CL algorithms in the domain, class and task-incremental scenarios. Our experiments reveal the trade-off between two contrasting goals of not forgetting the old knowledge and the ability to quickly learn in CL and AL. While conditioning the query strategy on the annotations collected for the previous tasks leads to improved task performance on the domain and task incremental learning, our proposed forgetting-learning profil
    
[^180]: 基于深度神经网络先验的矩法轨道恢复

    Deep Neural-network Prior for Orbit Recovery from Method of Moments. (arXiv:2304.14604v1 [stat.ME] CROSS LISTED)

    [http://arxiv.org/abs/2304.14604](http://arxiv.org/abs/2304.14604)

    该论文提出了一种基于深度神经网络先验的矩法轨道恢复方法，可用于解决多参照面对齐和单颗粒冷冻电镜建模等问题，具有抑制噪声的优势.

    

    轨道恢复问题是一类经常出现的问题，有多种形式。在这些问题中，我们旨在在经过群作用扭曲并通过已知算子观察后，估计未知函数。通常情况下，观测值会受到非平凡水平的噪声污染。本文研究了两种特定的轨道恢复问题，即多参照面对齐和单颗粒冷冻电镜建模。为了抑制噪声，我们建议在两个问题中都使用矩法方法，并引入深度神经网络先验。特别地，我们的神经网络应输出信号和群元素的分布，而矩则为输入。在多参照面对齐的情况下，我们展示了使用神经网络提高从矩中重建信号的收敛速度的优势。最后，我们使用我们的方法重建了冷冻电镜模拟和生物体积。

    Orbit recovery problems are a class of problems that often arise in practice and in various forms. In these problems, we aim to estimate an unknown function after being distorted by a group action and observed via a known operator. Typically, the observations are contaminated with a non-trivial level of noise. Two particular orbit recovery problems of interest in this paper are multireference alignment and single-particle cryo-EM modeling. In order to suppress the noise, we suggest using the method of moments approach for both problems while introducing deep neural network priors. In particular, our neural networks should output the signals and the distribution of group elements, with moments being the input. In the multireference alignment case, we demonstrate the advantage of using the NN to accelerate the convergence for the reconstruction of signals from the moments. Finally, we use our method to reconstruct simulated and biological volumes in the cryo-EM setting.
    
[^181]: 基于强化学习探索夸克和轻子的味道结构

    Exploring the flavor structure of quarks and leptons with reinforcement learning. (arXiv:2304.14176v1 [hep-ph])

    [http://arxiv.org/abs/2304.14176](http://arxiv.org/abs/2304.14176)

    通过利用强化学习，探索了具有 $U(1)$ 味道对称性的模型的味道结构，找到了21个与实验测量值一致的模型，预测了无中微子双贝塔衰变的有效质量和可观的轻子 CP 破坏。

    

    我们提出了一种利用强化学习探索夸克和轻子味道结构的方法。作为具体模型，我们利用一个基本的基于策略的算法，针对具有 $U(1)$ 味道对称性的模型。通过训练神经网络对夸克和轻子的 $U(1)$ 荷进行学习，代理方案找到了21个与夸克和轻子的实验测量质量和混合角一致的模型。特别是，正序的固有值往往大于反序，正序与目前的实验数据相比更加符合。代理的自主行为根据无中微子双贝塔衰变的有效质量和叶子场的角成分引起的可观的轻子 CP 破坏来预测。

    We propose a method to explore the flavor structure of quarks and leptons with reinforcement learning. As a concrete model, we utilize a basic policy-based algorithm for models with $U(1)$ flavor symmetry. By training neural networks on the $U(1)$ charges of quarks and leptons, the agent finds 21 models to be consistent with experimentally measured masses and mixing angles of quarks and leptons. In particular, an intrinsic value of normal ordering tends to be larger than that of inverted ordering, and the normal ordering is well fitted with the current experimental data in contrast to the inverted ordering. A specific value of effective mass for the neutrinoless double beta decay and a sizable leptonic CP violation induced by an angular component of flavon field are predicted by autonomous behavior of the agent.
    
[^182]: 智能电网故障预测系统的机器学习对抗攻击

    Machine-learned Adversarial Attacks against Fault Prediction Systems in Smart Electrical Grids. (arXiv:2303.18136v1 [cs.CR])

    [http://arxiv.org/abs/2303.18136](http://arxiv.org/abs/2303.18136)

    该论文提出了针对智能电网故障预测系统的机器学习对抗攻击的研究，证明智能电网中使用的深度神经网络方法容易受到对抗性攻击，并突出了目前在智能电网中的机器学习算法存在对各种对抗性攻击的弱点。

    

    在智能电网中，由于经济和关键性的原因，故障检测任务可能会对社会产生很大的影响。近年来，许多智能电网应用程序，如缺陷检测和负载预测，已经采用了数据驱动的方法。本研究的目的是研究智能电网情况下机器学习（ML）应用的安全性挑战。事实上，这些数据驱动算法的鲁棒性和安全性尚未与所有电网应用程序相关地进行广泛研究。我们首先证明了智能电网中使用的深度神经网络方法容易受到对抗性攻击。接着，我们突出展示了故障定位和类型分类方面的研究，说明了目前在智能电网中的机器学习算法对各种对抗性攻击的弱点。

    In smart electrical grids, fault detection tasks may have a high impact on society due to their economic and critical implications. In the recent years, numerous smart grid applications, such as defect detection and load forecasting, have embraced data-driven methodologies. The purpose of this study is to investigate the challenges associated with the security of machine learning (ML) applications in the smart grid scenario. Indeed, the robustness and security of these data-driven algorithms have not been extensively studied in relation to all power grid applications. We demonstrate first that the deep neural network method used in the smart grid is susceptible to adversarial perturbation. Then, we highlight how studies on fault localization and type classification illustrate the weaknesses of present ML algorithms in smart grids to various adversarial attacks
    
[^183]: 无观测上下文的联邦随机赌博学习

    Federated Stochastic Bandit Learning with Unobserved Context. (arXiv:2303.17043v1 [cs.LG])

    [http://arxiv.org/abs/2303.17043](http://arxiv.org/abs/2303.17043)

    本文提出了一种联邦随机多臂上下文赌博算法以最大化累积奖励，针对未知上下文的情况通过执行特征向量转换解决问题。

    

    本文研究了具有未知上下文的联邦随机多臂上下文赌博问题，其中M个代理面临不同的赌博机并协作学习。通信模型由中央服务器组成，并且代理会定期与中央服务器共享其估计结果，以便选择最优动作以最小化总后悔。我们假设精确的上下文不可观察，代理仅观测上下文的分布。例如，当上下文本身是噪声测量或基于预测机制时，就会出现这种情况。我们的目标是开发一种分布式联邦算法，促进代理之间的协作学习，选择一系列最优动作以最大化累积奖励。通过执行特征向量转换，我们提出了一种基于消除的算法，并证明了线性参数化奖励函数的后悔界。最后，我们验证了算法的性能。

    We study the problem of federated stochastic multi-arm contextual bandits with unknown contexts, in which M agents are faced with different bandits and collaborate to learn. The communication model consists of a central server and the agents share their estimates with the central server periodically to learn to choose optimal actions in order to minimize the total regret. We assume that the exact contexts are not observable and the agents observe only a distribution of the contexts. Such a situation arises, for instance, when the context itself is a noisy measurement or based on a prediction mechanism. Our goal is to develop a distributed and federated algorithm that facilitates collaborative learning among the agents to select a sequence of optimal actions so as to maximize the cumulative reward. By performing a feature vector transformation, we propose an elimination-based algorithm and prove the regret bound for linearly parametrized reward functions. Finally, we validated the perfo
    
[^184]: 适用于异质多壳扩散加权MRI估计纤维定向分布函数的单阶段学习模型

    A Unified Single-stage Learning Model for Estimating Fiber Orientation Distribution Functions on Heterogeneous Multi-shell Diffusion-weighted MRI. (arXiv:2303.16376v1 [cs.LG])

    [http://arxiv.org/abs/2303.16376](http://arxiv.org/abs/2303.16376)

    该论文提出了一种适用于异质多壳扩散加权MRI估计纤维定向分布函数的单阶段学习模型，使用深度学习技术可以提高推理速度和扫描一致性。

    

    扩散加权MRI通过其在q空间中的频谱测量每个体素中本地扩散过程的方向和尺度，通常在一个或多个shell中获取。 最近在微结构成像和多组织分解方面的发展引起了人们对信号的径向b值依赖性的关注。 因此，在组织分类和微观结构估计方面，需要扩展径向和角向域的信号表示。 提出了多种方法，可以模拟DW-MRI信号与生物微结构之间的非线性关系。 在过去几年中，许多基于深度学习的方法已经开发出来，相比传统的基于模型的方法（例如多壳多组织约束球形去卷积）具有更快的推理速度和更高的扫描一致性。 然而，由于学习过程在很大程度上依赖于扩散过程和组织微结构的先验信息，因此通常需要多阶段的学习策略。

    Diffusion-weighted (DW) MRI measures the direction and scale of the local diffusion process in every voxel through its spectrum in q-space, typically acquired in one or more shells. Recent developments in micro-structure imaging and multi-tissue decomposition have sparked renewed attention to the radial b-value dependence of the signal. Applications in tissue classification and micro-architecture estimation, therefore, require a signal representation that extends over the radial as well as angular domain. Multiple approaches have been proposed that can model the non-linear relationship between the DW-MRI signal and biological microstructure. In the past few years, many deep learning-based methods have been developed towards faster inference speed and higher inter-scan consistency compared with traditional model-based methods (e.g., multi-shell multi-tissue constrained spherical deconvolution). However, a multi-stage learning strategy is typically required since the learning process rel
    
[^185]: 无需强化学习的逆强化学习

    Inverse Reinforcement Learning without Reinforcement Learning. (arXiv:2303.14623v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.14623](http://arxiv.org/abs/2303.14623)

    该论文提出了一种新的逆强化学习简化方法，通过利用专家的状态分布来减少强化学习子例程的全局探索部分，实现了指数级的加速，大大提高了样本复杂度和时间复杂度的效果。

    

    逆强化学习 (IRL) 是一种强大的模仿学习技术，旨在学习合乎逻辑的专家演示的奖励函数。然而，传统的IRL方法存在计算上的弱点：它们需要将解决难度高的强化学习（RL）问题作为子例程进行反复求解。这与归约的观点相矛盾：我们已将模仿学习的较易问题归约为反复解决强化学习的更难问题。另一方面的工作证明，访问强策略花费时间的状态分布的侧面信息可以大大降低解决RL问题的样本和计算复杂度。在本研究中，我们首次展示了一种更加明智的模仿学习简化方法，利用专家的状态分布来缓解RL子例程的全局探索部分，理论上提供了指数级的加速。实际上，我们的算法在多个基准任务中在样本复杂度和时间复杂度方面都显著优于现有的IRL方法。

    Inverse Reinforcement Learning (IRL) is a powerful set of techniques for imitation learning that aims to learn a reward function that rationalizes expert demonstrations. Unfortunately, traditional IRL methods suffer from a computational weakness: they require repeatedly solving a hard reinforcement learning (RL) problem as a subroutine. This is counter-intuitive from the viewpoint of reductions: we have reduced the easier problem of imitation learning to repeatedly solving the harder problem of RL. Another thread of work has proved that access to the side-information of the distribution of states where a strong policy spends time can dramatically reduce the sample and computational complexities of solving an RL problem. In this work, we demonstrate for the first time a more informed imitation learning reduction where we utilize the state distribution of the expert to alleviate the global exploration component of the RL subroutine, providing an exponential speedup in theory. In practice
    
[^186]: 通过可变大小的压缩性建立数据相关的泛化界限

    Data-dependent Generalization Bounds via Variable-Size Compressibility. (arXiv:2303.05369v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.05369](http://arxiv.org/abs/2303.05369)

    本文通过引入可变大小压缩性框架，建立了一种新的数据相关的泛化误差上界。该方法将算法的泛化误差与其输入数据的可变大小压缩率相关联，并提供了依赖于经验分布而非未知分布的界限。此外，该方法还可以推导出输入数据和输出假设随机变量的任何函数的泛化界限，并包含并可能优于现有的基于PAC-Bayes和数据相关内在维度的界限。

    

    本文通过引入“可变大小压缩性”框架，建立了一种新的数据相关泛化误差的上界。在这个框架中，算法的泛化误差与其输入数据的可变大小“压缩率”相关联。通过这种方式，我们得到的界限依赖于手头给定输入数据的经验分布，而不是其未知分布。我们建立的新的泛化界限包括尾部界限、期望值的尾部界限和期望界限。此外，我们的框架还可以推导出对输入数据和输出假设随机变量的任何函数的泛化界限。特别是，这些泛化界限包含并可能优于几种现有的基于PAC-Bayes和数据相关内在维度的界限，这些界限作为特殊情况得到复原，从而揭示出我们方法的统一特性。

    In this paper, we establish novel data-dependent upper bounds on the generalization error through the lens of a "variable-size compressibility" framework that we introduce newly here. In this framework, the generalization error of an algorithm is linked to a variable-size 'compression rate' of its input data. This is shown to yield bounds that depend on the empirical measure of the given input data at hand, rather than its unknown distribution. Our new generalization bounds that we establish are tail bounds, tail bounds on the expectation, and in-expectations bounds. Moreover, it is shown that our framework also allows to derive general bounds on any function of the input data and output hypothesis random variables. In particular, these general bounds are shown to subsume and possibly improve over several existing PAC-Bayes and data-dependent intrinsic dimension-based bounds that are recovered as special cases, thus unveiling a unifying character of our approach. For instance, a new da
    
[^187]: 不使用边际贡献近似计算Shapley值

    Approximating the Shapley Value without Marginal Contributions. (arXiv:2302.00736v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00736](http://arxiv.org/abs/2302.00736)

    本文提出了两种无参数、领域无关的Shapley值近似算法SVARM和Stratified SVARM，它们基于一种与边际贡献概念脱钩的Shapley值表示。我们证明了它们在近似质量方面具有无与伦比的理论保证，并通过实证结果将其与合成游戏和常见可解释性用例进行了比较。

    

    Shapley值是为合作博弈中的玩家分配有意义的贡献值的最流行方法，最近在可解释的人工智能中得到了广泛应用。Shapley值的有意义性源于仅有Shapley值满足的公理属性，然而，确切计算的代价是随着玩家数量指数级增长。因此，许多研究致力于高效近似Shapley值，其中大部分围绕着玩家的边际贡献的概念。在本文中，我们提出了两种基于与边际贡献概念脱钩的Shapley值表示的无参数、领域无关的近似算法SVARM和Stratified SVARM。我们证明了它们在近似质量方面的无与伦比的理论保证，并提供了包括合成游戏和常用可解释性用例的实证结果进行比较。

    The Shapley value is arguably the most popular approach for assigning a meaningful contribution value to players in a cooperative game, which has recently been used intensively in explainable artificial intelligence. The meaningfulness is due to axiomatic properties that only the Shapley value satisfies, which, however, comes at the expense of an exact computation growing exponentially with the number of agents. Accordingly, a number of works are devoted to the efficient approximation of the Shapley values, most of them revolve around the notion of an agent's marginal contribution. In this paper, we propose with SVARM and Stratified SVARM two parameter-free and domain-independent approximation algorithms based on a representation of the Shapley value detached from the notion of marginal contributions. We prove unmatched theoretical guarantees regarding their approximation quality and provide empirical results including synthetic games as well as common explainability use cases comparin
    
[^188]: 多模态分子结构-文本模型用于基于文本的检索和编辑

    Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing. (arXiv:2212.10789v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.10789](http://arxiv.org/abs/2212.10789)

    本论文介绍了一种名为MoleculeSTM的多模态分子结构-文本模型，通过联合学习化学结构和文本描述，可以实现基于文本的检索和编辑。通过构建大型的多模态数据集，并设计挑战性的零样本任务进行验证，该模型展示了开放词汇和组合性的特性。

    

    药物发现中正在越来越广泛地采用人工智能，然而，现有研究主要利用分子的化学结构，忽视了化学领域中可用的丰富文本知识。将文本知识纳入考虑可以实现新的药物设计目标，适应基于文本的指导和预测复杂的生物活性。在这里，我们提出了一种多模态的分子结构-文本模型MoleculeSTM，通过联合学习分子的化学结构和文本描述来实现，采用对比学习策略。为了训练MoleculeSTM，我们构建了一个大型的多模态数据集，名为PubChemSTM，包含超过28万个化学结构-文本对。为了展示MoleculeSTM的有效性和实用性，我们设计了两个基于文本指令的挑战性零样本任务，包括结构-文本检索和分子编辑。MoleculeSTM具有两个主要特性：开放词汇和通过自然语言实现组合性。

    There is increasing adoption of artificial intelligence in drug discovery. However, existing studies use machine learning to mainly utilize the chemical structures of molecules but ignore the vast textual knowledge available in chemistry. Incorporating textual knowledge enables us to realize new drug design objectives, adapt to text-based instructions and predict complex biological activities. Here we present a multi-modal molecule structure-text model, MoleculeSTM, by jointly learning molecules' chemical structures and textual descriptions via a contrastive learning strategy. To train MoleculeSTM, we construct a large multi-modal dataset, namely, PubChemSTM, with over 280,000 chemical structure-text pairs. To demonstrate the effectiveness and utility of MoleculeSTM, we design two challenging zero-shot tasks based on text instructions, including structure-text retrieval and molecule editing. MoleculeSTM has two main properties: open vocabulary and compositionality via natural language.
    
[^189]: 因子模型中的双重稳健最近邻方法

    Doubly robust nearest neighbors in factor models. (arXiv:2211.14297v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.14297](http://arxiv.org/abs/2211.14297)

    该论文介绍了一种在潜在因子模型中处理缺失数据的双重稳健最近邻方法，可以提供一致的估计，并在存在良好的行和列邻居时提供（近似）二次改进非渐近性能。

    

    我们介绍并分析了在潜在因子模型中处理缺失数据的改进最近邻（NN）方法。我们考虑一个带有缺失数据的矩阵补全问题，其中当被观察到时，第$(i, t)$个条目由其均值$f(u_i, v_t)$加上均值为零的噪声给出，其中$f$为未知函数，$u_i$和$v_t$为潜在因子。之前的NN策略，如单元-单元NN，用于估计均值$f(u_i, v_t)$，依赖于存在其他行$j$使得$u_j \approx u_i$。类似地，时间-时间NN策略依赖于存在列$t'$使得$v_{t'} \approx v_t$。当相似行或相似列不可用时，这些策略的性能较差。我们的估计在两个方面对这种不足是双重稳健的：(1) 只要存在良好的行或列邻居，我们的估计提供一致的估计。 (2) 此外，如果存在良好的行和列邻居，它提供了（近似）二次改进非渐近性能。

    We introduce and analyze an improved variant of nearest neighbors (NN) for estimation with missing data in latent factor models. We consider a matrix completion problem with missing data, where the $(i, t)$-th entry, when observed, is given by its mean $f(u_i, v_t)$ plus mean-zero noise for an unknown function $f$ and latent factors $u_i$ and $v_t$. Prior NN strategies, like unit-unit NN, for estimating the mean $f(u_i, v_t)$ relies on existence of other rows $j$ with $u_j \approx u_i$. Similarly, time-time NN strategy relies on existence of columns $t'$ with $v_{t'} \approx v_t$. These strategies provide poor performance respectively when similar rows or similar columns are not available. Our estimate is doubly robust to this deficit in two ways: (1) As long as there exist either good row or good column neighbors, our estimate provides a consistent estimate. (2) Furthermore, if both good row and good column neighbors exist, it provides a (near-)quadratic improvement in the non-asympto
    
[^190]: 在复杂的多智能体场景中估计反事实治疗结果的时间变化

    Estimating counterfactual treatment outcomes over time in complex multi-agent scenarios. (arXiv:2206.01900v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2206.01900](http://arxiv.org/abs/2206.01900)

    本论文提出了一个可解释的反事实循环网络，用于在复杂的多智能体场景中估计干预效果。该模型考虑了时间变化的多智能体关系和协变量反事实预测的复杂结构，能够准确评估个体治疗效果，并提供解释性。

    

    在各种工程和科学领域中，评估多智能体系统中的干预行为（例如，人类何时应该干预自动驾驶系统，何时球员应该传给队友进行好射门）是一项具有挑战性的任务。使用反事实的长期预测来估计个体治疗效果（ITE）是评估此类干预措施的实用方法。然而，大多数传统框架没有考虑到多智能体关系的时间变化和协变量反事实预测的复杂结构，这可能导致ITE的错误评估和解释困难。在这里，我们提出了一个可解释的反事实循环网络，用于估计干预的效果。我们的模型利用图形变分循环神经网络和基于领域知识的计算来进行基于多智能体协变量和结果的长期预测的ITE估计框架，能够确认循环结构。

    Evaluation of intervention in a multi-agent system, e.g., when humans should intervene in autonomous driving systems and when a player should pass to teammates for a good shot, is challenging in various engineering and scientific fields. Estimating the individual treatment effect (ITE) using counterfactual long-term prediction is practical to evaluate such interventions. However, most of the conventional frameworks did not consider the time-varying complex structure of multi-agent relationships and covariate counterfactual prediction. This may lead to erroneous assessments of ITE and difficulty in interpretation. Here we propose an interpretable, counterfactual recurrent network in multi-agent systems to estimate the effect of the intervention. Our model leverages graph variational recurrent neural networks and theory-based computation with domain knowledge for the ITE estimation framework based on long-term prediction of multi-agent covariates and outcomes, which can confirm the circu
    
[^191]: TracInAD：用于异常检测的影响力测量方法

    TracInAD: Measuring Influence for Anomaly Detection. (arXiv:2205.01362v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.01362](http://arxiv.org/abs/2205.01362)

    本论文提出了一种新的方法TracInAD，用于根据影响力测量标记异常，并且在医疗和网络安全表格数据上实现了与最先进方法相当或更好的检测准确性。

    

    与许多其他任务一样，神经网络在异常检测方面也非常有效。然而，很少有深度学习模型适用于检测表格数据集中的异常。本文提出了一种新的方法来根据TracIn（一种最初用于解释性目的的影响力测量）来标记异常。所提出的方法可以用于增强任何无监督的深度异常检测方法。我们使用变分自编码器对我们的方法进行了测试，并表明训练点对测试点的平均影响力可以用作异常的代理。我们的模型在医疗和网络安全表格基准数据上在检测准确性方面表现出与最先进方法相当或更好的性能。

    As with many other tasks, neural networks prove very effective for anomaly detection purposes. However, very few deep-learning models are suited for detecting anomalies on tabular datasets. This paper proposes a novel methodology to flag anomalies based on TracIn, an influence measure initially introduced for explicability purposes. The proposed methods can serve to augment any unsupervised deep anomaly detection method. We test our approach using Variational Autoencoders and show that the average influence of a subsample of training points on a test point can serve as a proxy for abnormality. Our model proves to be competitive in comparison with state-of-the-art approaches: it achieves comparable or better performance in terms of detection accuracy on medical and cyber-security tabular benchmark data.
    
[^192]: 基于物联网的健康信息系统中考虑用户价值共创的最佳服务资源管理策略

    Optimal service resource management strategy for IoT-based health information system considering value co-creation of users. (arXiv:2204.02521v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.02521](http://arxiv.org/abs/2204.02521)

    本文研究了基于物联网的健康信息系统中考虑用户价值共创的最佳服务资源管理策略，通过嵌入深度强化学习算法，实现了服务资源的优化分配和用户参与行为的控制。

    

    本文探讨了一种优化的服务资源管理策略，用于增强健康信息服务的性能，优化服务资源利用，并提供互动性健康信息服务。考虑到健康信息服务中的价值共创模型，开发了一种自适应的最佳服务资源管理策略，重点关注与用户的协作和互动。在基于物联网的健康信息服务系统（I-HISS）中嵌入了深度强化学习算法，通过控制服务提供和服务适应来分配服务资源，基于用户参与行为。通过模拟实验，评估了所提出算法在不同用户对健康信息服务的反应下的重要性。

    This paper explores optimal service resource management strategy, a continuous challenge for health information service to enhance service performance, optimise service resource utilisation and deliver interactive health information service. An adaptive optimal service resource management strategy was developed considering a value co-creation model in health information service with a focus on collaborative and interactive with users. The deep reinforcement learning algorithm was embedded in the Internet of Things (IoT)-based health information service system (I-HISS) to allocate service resources by controlling service provision and service adaptation based on user engagement behaviour. The simulation experiments were conducted to evaluate the significance of the proposed algorithm under different user reactions to the health information service.
    

