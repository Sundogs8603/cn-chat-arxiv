# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Minimizing the Thompson Sampling Regret-to-Sigma Ratio (TS-RSR): a provably efficient algorithm for batch Bayesian Optimization](https://arxiv.org/abs/2403.04764) | 该论文提出了一种用于批量贝叶斯优化的高效算法，通过最小化Thompson抽样近似的遗憾与不确定性比率，成功协调每个批次的动作选择，同时实现高概率的理论保证，并在非凸测试函数上表现出色. |
| [^2] | [BloomGML: Graph Machine Learning through the Lens of Bilevel Optimization](https://arxiv.org/abs/2403.04763) | 本文将各种图学习技术重新构建为双层优化的特例或简化形式，并提出了更灵活的能量函数以形成图神经网络消息传递层，同时揭示了残余近似误差的来源。 |
| [^3] | [iScore: Visual Analytics for Interpreting How Language Models Automatically Score Summaries](https://arxiv.org/abs/2403.04760) | 设计了iScore工具，通过用户中心设计过程解决了大型语言模型在自动评分摘要中的透明度和信任问题 |
| [^4] | [Lifelong Intelligence Beyond the Edge using Hyperdimensional Computing](https://arxiv.org/abs/2403.04759) | 本文设计并部署了名为LifeHD的首个面向通用物联网应用的边缘设备终生成长学习系统，基于新颖、轻量级的神经启发式学习范式超高维计算（HDC）。通过利用两层联想记忆组织智能存储和管理高维、低精度向量，实现了在变化环境中对历史模式进行无限学习的能力。 |
| [^5] | [KnowledgeVIS: Interpreting Language Models by Comparing Fill-in-the-Blank Prompts](https://arxiv.org/abs/2403.04758) | KnowledgeVIS是一个人机协作的可视分析系统，可以通过比较填空提示之间的预测结果来揭示语言模型学习到的联系，帮助用户理解语言模型的工作方式及原因。 |
| [^6] | [JAX-SPH: A Differentiable Smoothed Particle Hydrodynamics Framework](https://arxiv.org/abs/2403.04750) | JAX-SPH是一个在JAX中实现的平滑粒子流体动力学框架，扩展了LagrangeBench项目的代码，集成了关键的SPH算法，验证了梯度的准确性，展示了梯度在解决逆问题和Solver-i中的作用。 |
| [^7] | [GNN-VPA: A Variance-Preserving Aggregation Strategy for Graph Neural Networks](https://arxiv.org/abs/2403.04747) | 通过提出一种保持方差的聚合函数（VPA），该函数在维持图神经网络（GNNs）的表达能力的基础上，提高了前向和后向动力学，进而导致了增强的预测性能和改善的学习动态。 |
| [^8] | [LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error](https://arxiv.org/abs/2403.04746) | 提出了一种受生物启发的方法，即模拟试错（STE），为工具增强型LLMs编排了三个关键机制以实现成功的工具使用行为 |
| [^9] | [SQ Lower Bounds for Non-Gaussian Component Analysis with Weaker Assumptions](https://arxiv.org/abs/2403.04744) | 本研究证明在非高斯成分分析中，对于区分标准多元高斯和在随机隐藏方向上行为类似某一维分布而在正交补上行为类似标准高斯的问题，先前被要求的卡方条件实际上是不必要的。 |
| [^10] | [A Sub-Quadratic Time Algorithm for Robust Sparse Mean Estimation](https://arxiv.org/abs/2403.04726) | 提出了一种次二次时间算法，用于在存在恶意异常值的情况下进行鲁棒稀疏均值估计 |
| [^11] | [Rethinking of Encoder-based Warm-start Methods in Hyperparameter Optimization](https://arxiv.org/abs/2403.04720) | 提出了一种新的基于编码器的表格数据集表示方法，与现有方法不同，能够自动提取重要的元特征，同时在两个常见的元任务上进行了评估 |
| [^12] | [Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification](https://arxiv.org/abs/2403.04696) | 提出了一种基于标记级别不确定性量化的新型事实核查和幻觉检测流程，该方法能够检测大型语言模型输出中的不可靠预测。 |
| [^13] | [Analysis of Systems' Performance in Natural Language Processing Competitions](https://arxiv.org/abs/2403.04693) | 本文描述了一种用于统计分析自然语言处理竞赛结果的评估方法，通过八个竞赛案例研究展示了其普适性。 |
| [^14] | [Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level](https://arxiv.org/abs/2403.04690) | 该研究提出了一种更快的邻域注意力机制，通过将注意力限制在最近的邻居之间来降低自注意力的计算复杂度，实现了显著的性能提升。 |
| [^15] | [End-to-end Conditional Robust Optimization](https://arxiv.org/abs/2403.04670) | 提出了一种新颖的端到端方法，利用现代可微分优化方法训练CRO模型，以同时考虑所规定决策的经验风险和支持它们的语境不确定性集合的条件覆盖质量 |
| [^16] | [Telecom Language Models: Must They Be Large?](https://arxiv.org/abs/2403.04666) | 小型语言模型Phi-2在电信领域展示出与大型对应模型相媲美的性能，通过检索增强生成方法提升了其能力。 |
| [^17] | [Dynamic Cross Attention for Audio-Visual Person Verification](https://arxiv.org/abs/2403.04661) | 提出了一种动态交叉注意力（DCA）模型，根据音频和视频模态之间的强弱互补关系，动态选择交叉关注或不关注的特征。 |
| [^18] | [Context-Based Multimodal Fusion](https://arxiv.org/abs/2403.04650) | 提出一种基于上下文的多模态融合模型，结合了模态融合和数据分布对齐，通过特定上下文向量表示每个模态，并将其与每个模态的嵌入进行融合， |
| [^19] | [Teaching Large Language Models to Reason with Reinforcement Learning](https://arxiv.org/abs/2403.04642) | 强化学习从人类反馈中学习已成为将LLM输出与人类偏好对齐的主要方法，研究了多种算法在提升LLM推理能力方面的表现，发现在大多数情况下专家迭代效果最佳，且其样本复杂度与PPO类似。 |
| [^20] | [Entropy Aware Message Passing in Graph Neural Networks](https://arxiv.org/abs/2403.04636) | 该论文提出了一种新颖的物理启发的图神经网络模型，通过引入熵感知消息传递项来缓解深度图神经网络中的过度平滑问题。 |
| [^21] | [Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI Collaboration](https://arxiv.org/abs/2403.04629) | 提出了ShapleyBO框架，用Shapley值解释贝叶斯优化提议，量化每个参数对于优化过程的贡献，并能够区分不同类型的不确定性探索贡献。 |
| [^22] | [MedFLIP: Medical Vision-and-Language Self-supervised Fast Pre-Training with Masked Autoencoder](https://arxiv.org/abs/2403.04626) | MedFLIP是一种用于医学分析的快速语言-图像预训练方法，通过引入SVD损失增强医学图像特征表示学习，验证了用语言可以提高零样本医学图像分析的性能。 |
| [^23] | [In-n-Out: Calibrating Graph Neural Networks for Link Prediction](https://arxiv.org/abs/2403.04605) | IN-N-OUT提出了第一个用于链接预测的图神经网络校准方法，基于对GNN校准偏差的观察，通过简单直觉实现校准 |
| [^24] | [Contrastive Continual Learning with Importance Sampling and Prototype-Instance Relation Distillation](https://arxiv.org/abs/2403.04599) | 提出了Contrastive Continual Learning via Importance Sampling (CCLIS)以保留知识，其中通过重播缓冲区选择（RBS）策略恢复先前的数据分布，最小化估计方差以保存高质量的负样本，同时引入了原型-实例关系蒸馏（PRD）损失来维护原型和样本表示之间的关系。 |
| [^25] | [Learning Agility Adaptation for Flight in Clutter](https://arxiv.org/abs/2403.04586) | 本文旨在使飞行器在未知且部分可观测的杂乱环境中具有敏捷性调整能力，提出了一种利用分层学习和规划框架的方法，通过在线无模型强化学习和预训练微调奖励方案获得可部署的策略，在仿真中显示出比恒定敏捷性基线和另一种替代方法更优越的飞行效率和安全性。 |
| [^26] | [Beyond Major Product Prediction: Reproducing Reaction Mechanisms with Machine Learning Models Trained on a Large-Scale Mechanistic Dataset](https://arxiv.org/abs/2403.04580) | 该研究使用大规模机制数据集和机器学习模型成功复现有机反应机理，展示了预测反应途径和杂质的潜力。 |
| [^27] | [Improved Algorithm for Adversarial Linear Mixture MDPs with Bandit Feedback and Unknown Transition](https://arxiv.org/abs/2403.04568) | 通过引入新的最小二乘估计器和self-normalized技术，我们提出了一个新算法，显著改进了线性混合MDPs中的遗憾上界。 |
| [^28] | [Reducing self-supervised learning complexity improves weakly-supervised classification performance in computational pathology](https://arxiv.org/abs/2403.04558) | 本研究探讨了在计算病理学中减少对比自监督学习复杂性对分类性能的改善，通过利用消费级硬件。 |
| [^29] | [Improvements & Evaluations on the MLCommons CloudMask Benchmark](https://arxiv.org/abs/2403.04553) | 本文报告了在MLCommons CloudMask基准测试上使用深度学习模型的性能基准测试结果，包括最佳模型、最高准确性和平均运行时间。 |
| [^30] | [Dissecting Sample Hardness: A Fine-Grained Analysis of Hardness Characterization Methods for Data-Centric AI](https://arxiv.org/abs/2403.04551) | 该论文提出了一个细致的难度类型分类系统，并引入了难度表征分析工具包（H-CAT），支持全面和定量评估不同难度类型下的HCMs，从而填补了当前对“难度”定义和评估的定量识别任务的空白。 |
| [^31] | [CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?](https://arxiv.org/abs/2403.04547) | 数据平衡在对比语言-图像预训练（CLIP）中可以部分改善偏见问题，然而会对质量产生复杂影响。 |
| [^32] | [Architectural Blueprint For Heterogeneity-Resilient Federated Learning](https://arxiv.org/abs/2403.04546) | 提出了一个三层架构用于优化边缘计算环境中的联邦学习，解决了客户端数据异构性和计算约束挑战，提高了非独立同分布数据集管理效率，显著改善了模型准确性，减少了通信开销，促进了联邦学习技术更广泛的应用。 |
| [^33] | [Improve Generalization Ability of Deep Wide Residual Network with A Suitable Scaling Factor](https://arxiv.org/abs/2403.04545) | 通过在深广残差网络中使用适当的缩放因子，可以提高泛化能力，即使允许缩放因子随深度减小，也可以实现最小最大速率。 |
| [^34] | [Enhancing Data Quality in Federated Fine-Tuning of Foundation Models](https://arxiv.org/abs/2403.04529) | 提出了一个用于联邦微调基础模型的数据质量控制管道，可以提高全局性能。 |
| [^35] | [Hyperspectral unmixing for Raman spectroscopy via physics-constrained autoencoders](https://arxiv.org/abs/2403.04526) | 通过物理约束自动编码器开发了高光谱解混算法，提供了在复杂混合情况下更好的准确性、鲁棒性和效率，展示了在复杂生物环境中的应用。 |
| [^36] | [T-TAME: Trainable Attention Mechanism for Explaining Convolutional Networks and Vision Transformers](https://arxiv.org/abs/2403.04523) | 本文提出了T-TAME，一种适用于卷积网络和视觉Transformer的可训练注意机制，为解释深度神经网络在图像分类任务中的应用提供了通用方法。 |
| [^37] | [What makes an image realistic?](https://arxiv.org/abs/2403.04493) | 论文讨论了如何设计能够可靠区分真实数据和不真实数据的函数，提出了通用评论者的概念作为一个新的解决方案。 |
| [^38] | [Source Matters: Source Dataset Impact on Model Robustness in Medical Imaging](https://arxiv.org/abs/2403.04484) | 该研究调查了在医学成像中，源数据集的选择如何影响模型的鲁棒性，指出ImageNet预训练模型更容易过拟合混杂因素，建议研究人员重新评估模型的鲁棒性。 |
| [^39] | [On the Topology Awareness and Generalization Performance of Graph Neural Networks](https://arxiv.org/abs/2403.04482) | 这篇论文介绍了一个全面的框架，用于对图神经网络在任何拓扑特征上的拓扑感知进行表征。 |
| [^40] | [Hyperparameter Tuning MLPs for Probabilistic Time Series Forecasting](https://arxiv.org/abs/2403.04477) | 该研究通过大量实验调查时间序列相关的特定超参数对MLP模型在时间序列预测中性能的影响，引入了迄今为止最大的用于时间序列预测的元数据集TSBench，强调调整这些参数的重要性。 |
| [^41] | [A Survey of Graph Neural Networks in Real world: Imbalance, Noise, Privacy and OOD Challenges](https://arxiv.org/abs/2403.04468) | 本文调查了图神经网络在现实世界中面临的不平衡、噪声、隐私和OOD挑战，并致力于提高模型性能、可靠性和鲁棒性。 |
| [^42] | [Vlearn: Off-Policy Learning with Efficient State-Value Function Estimation](https://arxiv.org/abs/2403.04453) | Vlearn提出了一种新颖的离策略优化方法，称为Vlearn，它通过仅利用状态值函数作为评论家，消除了对明确状态-动作-值函数的需求，从而解决了高维动作空间中的计算挑战。 |
| [^43] | [Membership Inference Attacks and Privacy in Topic Modeling](https://arxiv.org/abs/2403.04451) | 主题建模中提出了会员推理攻击，通过差分隐私词汇选择来改善隐私风险 |
| [^44] | [FRRI: a novel algorithm for fuzzy-rough rule induction](https://arxiv.org/abs/2403.04447) | 结合模糊与粗糙集理论，提出一种新颖的模糊-粗糙规则归纳算法 FRRI。 |
| [^45] | [Cooperative Bayesian Optimization for Imperfect Agents](https://arxiv.org/abs/2403.04442) | 这项研究提出了一种合作的贝叶斯优化方法，用于优化黑匣子函数，在这种方法中两个代理者协作选择查询函数的点，通过战略规划实现更好地识别全局最大值。 |
| [^46] | [Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation](https://arxiv.org/abs/2403.04436) | 提出了基于强化学习的框架"Human to Humanoid (H2O)"，实现了利用单个RGB相机对全尺寸人形机器人进行实时全身遥操作，并成功在真实场景中实现了动态全身运动遥操作。 |
| [^47] | [Boosting Fairness and Robustness in Over-the-Air Federated Learning](https://arxiv.org/abs/2403.04431) | 提出了一种通过极大极小优化实现公平性和鲁棒性的无线联邦学习算法，不需要复杂的编码解码方案，提高了效率和隐私性 |
| [^48] | [On-demand Quantization for Green Federated Generative Diffusion in Mobile Edge Networks](https://arxiv.org/abs/2403.04430) | 在移动边缘网络中，提出了一种按需量化的能效联合生成扩散方法以解决大型GAI模型训练中的通信消耗和能量消耗挑战。 |
| [^49] | [Exploring the Influence of Dimensionality Reduction on Anomaly Detection Performance in Multivariate Time Series](https://arxiv.org/abs/2403.04429) | 降维技术与先进的无监督时间序列异常检测模型的整合能够显著提高特定场景下的异常检测性能，并且可以大幅减少训练时间。 |
| [^50] | [Signature Isolation Forest](https://arxiv.org/abs/2403.04405) | 介绍了一种新颖的异常检测算法"Signature Isolation Forest"，利用粗路径理论的签名变换去除了Functional Isolation Forest的线性内积和词典选择方面的限制。 |
| [^51] | [Impacts of Color and Texture Distortions on Earth Observation Data in Deep Learning](https://arxiv.org/abs/2403.04385) | 本研究系统地研究了深度学习模型对地球观测数据中颜色和纹理失真的敏感性，发现模型对纹理失真比颜色失真更敏感 |
| [^52] | [LoCoDL: Communication-Efficient Distributed Learning with Local Training and Compression](https://arxiv.org/abs/2403.04348) | LoCoDL是一种通信高效的分布式学习算法，结合了本地训练和压缩技术，具有双倍加速的通信复杂度优势，特别适用于一般异构条件下的强凸函数。 |
| [^53] | [RL-CFR: Improving Action Abstraction for Imperfect Information Extensive-Form Games with Reinforcement Learning](https://arxiv.org/abs/2403.04344) | RL-CFR是一种用于动态动作抽象的强化学习方法，通过创新的MDP公式构建游戏树，并利用CFR进行策略推导，能够在不完全信息广义形式博弈中取得优秀性能。 |
| [^54] | [Explainable AI for Embedded Systems Design: A Case Study of Static Redundant NVM Memory Write Prediction](https://arxiv.org/abs/2403.04337) | 本研究提出了解释性人工智能在嵌入式系统设计中的应用，并通过静态冗余NVM内存写预测案例研究，证明了XAI在提高性能和能效方面的有效性。 |
| [^55] | [A mechanism-informed reinforcement learning framework for shape optimization of airfoils](https://arxiv.org/abs/2403.04329) | 提出了一种基于机制的强化学习框架，用于处理流体动力学控制下的气动外型优化，并整合了多种策略以保证精确性和高效性。 |
| [^56] | [Edge-based Parametric Digital Twins for Intelligent Building Indoor Climate Modeling](https://arxiv.org/abs/2403.04326) | 使用边缘计算、数字孪生体和深度学习相结合的方法，创建参数化数字孪生体用于建筑气候建模，并在边缘部署深度学习模型，以优化建筑物运营。 |
| [^57] | [Memetic Differential Evolution Methods for Semi-Supervised Clustering](https://arxiv.org/abs/2403.04322) | 本文提出了一种基于差分进化范式的新颖遗传模拟策略，用于解决半监督聚类问题，是第一次在这个领域尝试定义这样的方法。 |
| [^58] | [Online Adaptation of Language Models with a Memory of Amortized Contexts](https://arxiv.org/abs/2403.04317) | 提出了一种带有分摊上下文记忆的在线适应框架，可有效地提取、压缩并存储信息以保持强大的知识保留能力 |
| [^59] | [Effectiveness Assessment of Recent Large Vision-Language Models](https://arxiv.org/abs/2403.04306) | 本文评估了最近出现的大型视觉-语言模型在专业和通用任务中的表现，旨在全面了解这些创新方法的能力。 |
| [^60] | [MedM2G: Unifying Medical Multi-Modal Generation via Cross-Guided Diffusion with Visual Invariant](https://arxiv.org/abs/2403.04290) | MedM2G提出了一种医学多模态生成框架，通过统一模型对医学多模态进行对齐、提取和生成，提取有价值的临床知识并增强特定医学信息。 |
| [^61] | [Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model with Proxy](https://arxiv.org/abs/2403.04283) | 该论文提出Proxy-RLHF方法，通过将大型语言模型的生成和对齐过程解耦，实现了以更低计算成本对齐人类价值观，仅使用其他方法的1%训练参数即可达到可比水平的对齐度。 |
| [^62] | [Qubit-Wise Architecture Search Method for Variational Quantum Circuits](https://arxiv.org/abs/2403.04268) | 提出了一种新颖的QWAS方法，通过逐步搜索每个阶段的一量子位配置，并结合蒙特卡罗树搜索算法，实现了在一些实际任务中平衡电路性能和大小的探索与开发。 |
| [^63] | [Can Small Language Models be Good Reasoners for Sequential Recommendation?](https://arxiv.org/abs/2403.04260) | 提出了逐步知识提取框架（SLIM），为顺序推荐系统解决了大型语言模型（LLMs）高资源需求的难题，使其能以资源高效的方式享受LLMs的出色推理能力。 |
| [^64] | [Decentralized and Equitable Optimal Transport](https://arxiv.org/abs/2403.04259) | 本文提出了分散的最优输运问题和分散公平最优输运问题，引入了具有迭代复杂度为O(1/{\epsilon})的单循环分散算法，以匹配现有的集中式一阶方法，并改进了现有集中式算法的迭代复杂度。 |
| [^65] | [Mastering Memory Tasks with World Models](https://arxiv.org/abs/2403.04253) | 通过将一类新的状态空间模型整合到基于模型的强化学习代理的世界模型中，提出了一种新方法，Recall to Imagine（R2I），旨在增强长期记忆和长距离奖励分配，实现了在记忆任务和奖励分配方面的超越表现。 |
| [^66] | [Efficient CNN-LSTM based Parameter Estimation of Levy Driven Stochastic Differential Equations](https://arxiv.org/abs/2403.04246) | 本研究提出了一种新颖的CNN-LSTM三阶段模型PEnet，通过CNN对数据特征进行浓缩，提供了一种端到端方法，具有高准确性和适应性，以及长序列观测的增强推断速度和高泛化能力，从而在估计SDE方面取得了显著优势。 |
| [^67] | [A Study of Dropout-Induced Modality Bias on Robustness to Missing Video Frames for Audio-Visual Speech Recognition](https://arxiv.org/abs/2403.04245) | 本文研究了在音视频语音识别中丢失视频帧导致的辅助模态偏差对系统稳健性的影响，并提出了模态偏差假设，进一步提出了一种多模态分布近似框架，以减少对音频模态的过度依赖，并保持性能和稳健性。 |
| [^68] | [Regularized DeepIV with Model Selection](https://arxiv.org/abs/2403.04236) | 本文提出了一种名为正则化DeepIV（RDIV）回归的新方法，能够避免IV回归唯一标识、极小极大计算预言和缺乏模型选择过程等限制，同时实现了通用函数逼近。 |
| [^69] | [Fundamental limits of Non-Linear Low-Rank Matrix Estimation](https://arxiv.org/abs/2403.04234) | 证明了贝叶斯最优性能由等效高斯模型和有效先验确定，信号重构需要增长的信噪比条件，并提供了针对非线性低秩矩阵估计问题的渐近误差特征化和近似传递消息算法。 |
| [^70] | [Generalizing Cooperative Eco-driving via Multi-residual Task Learning](https://arxiv.org/abs/2403.04232) | 介绍了基于多残差任务学习的泛化协同生态驾驶方法，通过分解控制并运用学习来解决多个交通场景的挑战 |
| [^71] | [Aligners: Decoupling LLMs and Alignment](https://arxiv.org/abs/2403.04224) | 提出了一种通过训练对齐器模型来解耦大型语言模型（LLMs）和对齐，以减少对齐对性能的潜在负面影响。 |
| [^72] | [Why Online Reinforcement Learning is Causal](https://arxiv.org/abs/2403.04221) | 强化学习和因果建模相互补充，论文主要指出在线学习环境下，条件概率具有因果性，离线RL是因果学习潜力最大的环境。 |
| [^73] | [HeteroSwitch: Characterizing and Taming System-Induced Data Heterogeneity in Federated Learning](https://arxiv.org/abs/2403.04207) | 该论文研究了在联邦学习中由于设备硬件和软件配置的不同导致的系统诱导数据异质性，证明了这种异质性对模型的性能产生负面影响，并提出了名为HeteroSwitch的解决方案。 |
| [^74] | [GRAWA: Gradient-based Weighted Averaging for Distributed Training of Deep Learning Models](https://arxiv.org/abs/2403.04206) | 提出了一种新的基于梯度的加权平均算法，以恢复优化景观中的平坦区域为优先，实验结果表明该算法在分布式训练中取得了更快的收敛速度和更好的质量 |
| [^75] | [Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents](https://arxiv.org/abs/2403.04202) | 在多代理环境中，研究人员探讨了不同道德类型的学习代理之间的互动，发现道德异质性可能对代理的共同发展产生影响。 |
| [^76] | [Fill-and-Spill: Deep Reinforcement Learning Policy Gradient Methods for Reservoir Operation Decision and Control](https://arxiv.org/abs/2403.04195) | 深度强化学习方法应用于水库运行决策与控制，克服了动态规划和随机动态规划在高分辨率下面临的挑战，解决了“维度诅咒”问题。 |
| [^77] | [Generative AI for Synthetic Data Generation: Methods, Challenges and the Future](https://arxiv.org/abs/2403.04190) | 这里是中文总结出的一句话要点 |
| [^78] | [Silicon Photonic 2.5D Interposer Networks for Overcoming Communication Bottlenecks in Scale-out Machine Learning Hardware Accelerators](https://arxiv.org/abs/2403.04189) | 这项研究提出了在2.5D平台中利用光通信和计算的方法，实现能效高、吞吐量高的机器学习加速器体系结构。 |
| [^79] | [RATSF: Empowering Customer Service Volume Management through Retrieval-Augmented Time-Series Forecasting](https://arxiv.org/abs/2403.04180) | 提出了一种检索增强时序序列预测框架(RATSF)，通过引入交叉注意力模块(RACA)及知识库设计，有效利用历史数据段进行客服量预测，在非平稳数据情况下显著提升了性能。 |
| [^80] | [Noisy Spiking Actor Network for Exploration](https://arxiv.org/abs/2403.04162) | 提出了一种噪声尖峰演员网络（NoisySAN）以解决尖峰神经网络（SNNs）在探索中的弱点，并在连续控制任务上取得了优于最先进方法的表现 |
| [^81] | [SWAP-NAS: Sample-Wise Activation Patterns For Ultra-Fast NAS](https://arxiv.org/abs/2403.04161) | 提出了一种新颖的高性能无需训练的度量SWAP-Score，能够在不同搜索空间和任务中测量网络在一批输入样本上的表现能力，并通过正则化进一步提高相关性，实现模型大小的控制。 |
| [^82] | [Stabilizing Policy Gradients for Stochastic Differential Equations via Consistency with Perturbation Process](https://arxiv.org/abs/2403.04154) | 通过与SDE相关的扰动过程一致性，稳定策略梯度，提高样本效率。 |
| [^83] | [FL-GUARD: A Holistic Framework for Run-Time Detection and Recovery of Negative Federated Learning](https://arxiv.org/abs/2403.04146) | FL-GUARD提出了一个全面框架，用于在负面联邦学习情况下检测和恢复，解决了先前解决方案中的额外成本和浪费学习轮次的问题 |
| [^84] | [FedClust: Optimizing Federated Learning on Non-IID Data through Weight-Driven Client Clustering](https://arxiv.org/abs/2403.04144) | 该论文提出了FedClust，一种新颖的集群化联邦学习方法，通过利用本地模型权重和客户端数据分布之间的相关性，在一次性的操作中将客户端分组成集群。 |
| [^85] | [Exploring LLM-based Agents for Root Cause Analysis](https://arxiv.org/abs/2403.04123) | 探索基于LLM的代理用于根本原因分析，以解决自动化RCA中无法动态收集额外诊断信息的限制。 |
| [^86] | [Can Large Language Models Reason and Plan?](https://arxiv.org/abs/2403.04121) | 大型语言模型缺乏自我批评能力，无法像人类一样纠正错误。 |
| [^87] | [Globally Stable Neural Imitation Policies](https://arxiv.org/abs/2403.04118) | 提出了稳定神经动力系统（SNDS）的仿真学习制度，可生成具有正式稳定性保证的政策，并通过联合训练政策和其对应的李亚普诺夫候选者确保全局稳定性。 |
| [^88] | [Closing the Visual Sim-to-Real Gap with Object-Composable NeRFs](https://arxiv.org/abs/2403.04114) | COV-NeRF是一种新型的神经辅助射线体模型，可以从真实图像中提取对象并合成新场景，有效地解决了视觉从虚拟到实际中的数据匹配问题。 |
| [^89] | [Using Causal Trees to Estimate Personalized Task Difficulty in Post-Stroke Individuals](https://arxiv.org/abs/2403.04109) | 提出了一种基于个体表现自动生成不同任务难度级别区域的方法，能够更好地解释中风患者在达到任务时的表现变异。 |
| [^90] | [Many-Objective Multi-Solution Transport](https://arxiv.org/abs/2403.04099) | 提出了Many-objective multi-solution Transport (MosT) 框架，能够在众多目标的帕累托前沿找到多个多样化解决方案，通过优化每个解决方案的加权目标来实现。 |
| [^91] | [Automated Multi-Task Learning for Joint Disease Prediction on Electronic Health Records](https://arxiv.org/abs/2403.04086) | 该研究提出了一种名为AutoDP的自动化方法，用于在电子健康记录数据中为联合疾病预测搜索最佳配置。 |
| [^92] | [Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference](https://arxiv.org/abs/2403.04082) | 通过对比学习学到的时间序列数据表示遵循高斯马尔可夫链，从而启用规划和推断 |
| [^93] | [Directional Smoothness and Gradient Methods: Convergence and Adaptivity](https://arxiv.org/abs/2403.04081) | 提出了一种新的次优性界限方法以解决梯度下降问题，利用方向平滑度开发上界并获得收敛保证，在实验中证明优于传统基于L平滑度的理论。 |
| [^94] | [Forecasting and Mitigating Disruptions in Public Bus Transit Services](https://arxiv.org/abs/2403.04072) | 引入数据驱动的统计和机器学习模型，以解决公共公交服务中替代车辆最佳位置选择的挑战性问题 |
| [^95] | [Improving Adversarial Training using Vulnerability-Aware Perturbation Budget](https://arxiv.org/abs/2403.04070) | 提出了两种基于脆弱性感知的重新加权函数，用于为对抗训练中的对抗示例分配扰动界限，从而提高了对抗训练的有效性。 |
| [^96] | [Belief-Enriched Pessimistic Q-Learning against Adversarial State Perturbations](https://arxiv.org/abs/2403.04050) | 提出了一种新的鲁棒RL算法，通过引入信念状态推理和基于扩散的状态净化，推导出一种悲观策略，以对抗代理对真实状态的不确定性。 |
| [^97] | [Sample size planning for conditional counterfactual mean estimation with a K-armed randomized experiment](https://arxiv.org/abs/2403.04039) | 论文讨论了如何确定足够大的样本规模，以在数据驱动的子组中估计条件反事实期望，通过将原始目标转化为同时推断问题来解决估计误差增加的可能性，并且允许在固定的样本大小预算下逆转问题以确定可行的治疗手臂数量或分区复杂性。 |
| [^98] | [OCD-FL: A Novel Communication-Efficient Peer Selection-based Decentralized Federated Learning](https://arxiv.org/abs/2403.04037) | 提出了一种名为OCD-FL的新方案，通过系统化的FL对等选择进行协作，旨在在减少能耗的同时实现最大的FL知识增益 |
| [^99] | [Unsupervised Contrastive Learning for Robust RF Device Fingerprinting Under Time-Domain Shift](https://arxiv.org/abs/2403.04036) | 本论文引入了对比学习来解决射频设备指纹识别中的领域漂移问题，通过学习距离度量处理RF信号，从而提高稳健性。 |
| [^100] | [Online Learning with Unknown Constraints](https://arxiv.org/abs/2403.04033) | 在线学习中通过元算法结合在线回归预测器估计未知安全约束，并将在线学习预测转换为符合约束的预测，同时保证在每一轮高概率地满足安全约束。算法的后悔受到在线回归和在线学习预测器的限制，模型类中未知安全约束的逃避维度，以及捕捉安全学习困难的新颖复杂度度量的约束。 |
| [^101] | [Learning Guided Automated Reasoning: A Brief Survey](https://arxiv.org/abs/2403.04017) | 机器学习可引导自动推理系统，通过逻辑有效证明概念支持演绎搜索，从大型推理体中培训机器学习系统，打破推理系统的组合爆炸问题，并促进推理链的长远发展和新颖证明思路的产生 |
| [^102] | [Knockoff-Guided Feature Selection via A Single Pre-trained Reinforced Agent](https://arxiv.org/abs/2403.04015) | 通过引入模拟特征指导和强化学习优化的创新框架，提出了一种用于特征选择的方法，以识别最佳有效的特征子集。 |
| [^103] | [Temporal Cross-Attention for Dynamic Embedding and Tokenization of Multimodal Electronic Health Records](https://arxiv.org/abs/2403.04012) | 提出了一种用于精确表示多模态临床时间序列的动态嵌入和标记框架 |
| [^104] | [Three Revisits to Node-Level Graph Anomaly Detection: Outliers, Message Passing and Hyperbolic Neural Networks](https://arxiv.org/abs/2403.04010) | 本文从离群值注入、消息传递和双曲神经网络三个方面重新审视节点级图异常检测任务，揭示了对提高性能的通用策略 |
| [^105] | [Sampling-based Safe Reinforcement Learning for Nonlinear Dynamical Systems](https://arxiv.org/abs/2403.04007) | 该论文提出了一种基于采样的安全强化学习方法，可以在控制非线性动力系统时同时满足硬性安全约束和收敛性保证。 |
| [^106] | [On the Efficient Marginalization of Probabilistic Sequence Models](https://arxiv.org/abs/2403.04005) | 论文提出了一系列新颖高效的逼近技术，用于序贯模型的边缘化，这些技术是模型无关的，仅依赖于预训练自回归模型的下一步条件分布的访问和采样。 |
| [^107] | [Bidirectional Progressive Neural Networks with Episodic Return Progress for Emergent Task Sequencing and Robotic Skill Transfer](https://arxiv.org/abs/2403.04001) | 提出了一种名为具有双向进步神经网络的情节回归进展（ERP-BPNN）的新型多任务强化学习框架，通过人类般交叉的方式学习，实现自主任务切换，并允许在任务之间进行双向技能传递。 |
| [^108] | [Video Relationship Detection Using Mixture of Experts](https://arxiv.org/abs/2403.03994) | 使用专家混合模型进行视频关系检测，解决了连接视觉和语言的计算和推理差距，提高了稳定性和泛化能力。 |
| [^109] | [Effect of Ambient-Intrinsic Dimension Gap on Adversarial Vulnerability](https://arxiv.org/abs/2403.03967) | 通过环境-固有维度差异的概念，论文证明了维度差异使干净训练的模型更容易受到数据空间脱离流形方向的对抗扰动攻击。 |
| [^110] | [Assessing the Extrapolation Capability of Template-Free Retrosynthesis Models](https://arxiv.org/abs/2403.03960) | 评估了无模板逆合成模型的外推能力，发现尽管其在预测新合成规则的前体方面表现出潜力，但在超出分布反应中的精准匹配准确率很低，且其预测的新颖反应大部分都是不切实际的。 |
| [^111] | [Black-Box $k$-to-$1$-PCA Reductions: Theory and Applications](https://arxiv.org/abs/2403.03905) | 我们的主要贡献是对于$k$-PCA算法中近似参数退化的边界得到了显著更为精确的界限 |
| [^112] | [Spectral Algorithms on Manifolds through Diffusion](https://arxiv.org/abs/2403.03669) | 本文提出了一种新的视角，将输入数据视为嵌入到更高维欧几里得空间中的低维流形，并研究了在RKHS中谱算法的收敛性能，特别是热核生成的扩散空间，通过积分算子技术推导了关于广义范数的紧收敛上界，使估计器在强意义下收敛到目标函数。 |
| [^113] | [A Survey on Applications of Reinforcement Learning in Spatial Resource Allocation](https://arxiv.org/abs/2403.03643) | 运用强化学习解决空间资源分配问题的新方法具有快速解决方法收敛和强大的模型泛化能力等优势，为这一问题领域提供了新的视角。 |
| [^114] | [DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training](https://arxiv.org/abs/2403.03542) | 本文提出了一种新的自回归去噪预训练策略，可以更稳定、更高效地在PDE数据上进行预训练，并且通过基于傅里叶注意力的模型架构设计，实现了在大规模预训练中轻松扩展模型，该模型在多个PDE数据集上取得了SOTA表现。 |
| [^115] | [CoRMF: Criticality-Ordered Recurrent Mean Field Ising Solver](https://arxiv.org/abs/2403.03391) | CoRMF是一种基于RNN的高效伊辛模型求解器，利用关键有序自旋序列和循环神经网络来实现变分均场和 RNN 之间的统一，从而实现了对通常难以处理的伊辛模型的高效探索。 |
| [^116] | [Exact Enforcement of Temporal Continuity in Sequential Physics-Informed Neural Networks](https://arxiv.org/abs/2403.03223) | 精确执⾏时间连续性是本论⽂的⼀项重要创新，简化了解决时间相关问题动态⾏为预测困难的挑战。 |
| [^117] | [The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning](https://arxiv.org/abs/2403.03218) | WMDP基准是一个公开发布的数据集，包含4157个多项选择问题，用作生物安全、网络安全和化学安全危险知识的代理测量。 |
| [^118] | [State-Constrained Zero-Sum Differential Games with One-Sided Information](https://arxiv.org/abs/2403.02741) | 我们将零和微分博弈的理论扩展到具有状态约束，并提出了计算行为策略所需的原始和对偶子动态原则。 |
| [^119] | [False Positive Sampling-based Data Augmentation for Enhanced 3D Object Detection Accuracy](https://arxiv.org/abs/2403.02639) | 本研究提出了一种名为虚假阳性采样的新增强技术，通过重新训练模型使用被识别为虚假阳性的点云，以提高3D物体检测模型的性能。 |
| [^120] | [Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2403.01112) | 提出了用于合作多智能体强化学习的高效情节记忆利用（EMU），利用语义一致内存加速学习，有选择地促进理想的转换，避免陷入局部最优解。 |
| [^121] | [LAB: Large-Scale Alignment for ChatBots](https://arxiv.org/abs/2403.01081) | 介绍了一种名为LAB的方法，旨在克服大型语言模型训练中的可扩展性挑战，通过分类法指导的合成数据生成和多阶段调整框架，实现了对昂贵人工标注和GPT-4等专有模型依赖较少的大规模对齐，提供了一种可扩展、具有成本效益的解决方案，不会出现灾难性遗忘情况，进一步增强了LLM的训练效率。 |
| [^122] | [Merging Text Transformer Models from Different Initializations](https://arxiv.org/abs/2403.00986) | 研究了合并不同初始化的Transformer模型的技术，提出了一种模型合并技术以研究这些模型极小值之间的关系，并发现与模型平均相比，通过我们的方法合并这些模型始终可以获得较低的损失障碍。 |
| [^123] | [Disaggregated Multi-Tower: Topology-aware Modeling Technique for Efficient Large-Scale Recommendation](https://arxiv.org/abs/2403.00877) | Disaggregated Multi-Tower提出了一种面向拓扑感知的建模技术，通过SPTT、TM和TP三个组件实现了高效的大规模推荐，加速性能提升了1.9倍。 |
| [^124] | [Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs](https://arxiv.org/abs/2403.00858) | 通过提出的框架，我们训练了一种用于Llama 2 Chat 7B或更大模型的草案模型，实现了加速推理，仅占原始大小的1.64％。 |
| [^125] | [Mitigating Reversal Curse via Semantic-aware Permutation Training](https://arxiv.org/abs/2403.00758) | 逆转诅咒问题是导致因果语言模型无法进行双向推理的根本原因之一，在这篇论文中，我们提出了通过语义感知的置换训练来缓解这一问题。 |
| [^126] | [Deep Reinforcement Learning: A Convex Optimization Approach](https://arxiv.org/abs/2402.19212) | 本文提出了一种深度强化学习的凸优化方法，通过每集使用凸优化来训练神经网络近似最优$Q$-函数，确保收敛参数可以无限接近最优参数。 |
| [^127] | [Automated Testing of Spatially-Dependent Environmental Hypotheses through Active Transfer Learning](https://arxiv.org/abs/2402.18064) | 结合了迁移学习和主动学习的方法，通过多任务高斯过程和基于信息的目标函数，可以在实时评估假设的数量之间的关系，从而提高规划效率。 |
| [^128] | [Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models](https://arxiv.org/abs/2402.18059) | 提出一种利用多目标优化方法的水印技术，通过轻量级网络生成特定令牌水印logits和分割比率，在保证检测性的同时提升了文本的语义完整性。 |
| [^129] | [Discovering Symmetry Group Structures via Implicit Orthogonality Bias](https://arxiv.org/abs/2402.17002) | HyperCube网络通过独特的因式分解架构和正则化器，成功学习了对称群的操作，能够高效地恢复完整操作表，并形成广义傅里叶基进行群卷积。 |
| [^130] | [A prior Estimates for Deep Residual Network in Continuous-time Reinforcement Learning](https://arxiv.org/abs/2402.16899) | 本研究针对连续时间控制问题，提出了一种可以直接分析Bellman最优损失\emph{先验}泛化误差的方法，避免了有界性假设，并通过最大算子的分解方法实现了损失函数的转换。 |
| [^131] | [On Trojan Signatures in Large Language Models of Code](https://arxiv.org/abs/2402.16896) | 本文研究了在大型代码语言模型中木马签名的问题，并发现木马签名无法推广到代码语言模型，具有重要的研究意义。 |
| [^132] | [Graph Learning under Distribution Shifts: A Comprehensive Survey on Domain Adaptation, Out-of-distribution, and Continual Learning](https://arxiv.org/abs/2402.16374) | 该综述调查了解决图学习中分布变化问题的最新方法、策略和见解。 |
| [^133] | [Ranking Large Language Models without Ground Truth](https://arxiv.org/abs/2402.14860) | 不需要基准实况或参考响应的条件下，通过考虑模型的三元组来排名大型语言模型，并提出了两种排名方法。 |
| [^134] | [Reinforcement learning-assisted quantum architecture search for variational quantum algorithms](https://arxiv.org/abs/2402.13754) | 通过强化学习自动搜索变分电路的最佳结构，改善了VQAs的性能。 |
| [^135] | [Revitalizing Multivariate Time Series Forecasting: Learnable Decomposition with Inter-Series Dependencies and Intra-Series Variations Modeling](https://arxiv.org/abs/2402.12694) | 引入可学习分解策略和双注意力模块，同时捕捉跨系列依赖和内部变化，以应对复杂的多变量时间序列预测挑战。 |
| [^136] | [AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling](https://arxiv.org/abs/2402.12226) | AnyGPT是一个统一的多模态语言模型，通过离散表示实现各种模态的统一处理，能够在不改变大型语言模型架构或训练方式的情况下稳定训练，为新模态的无缝整合提供了可能。 |
| [^137] | [MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data](https://arxiv.org/abs/2402.08957) | 这项工作介绍了MUSTARD，一种掌握定理和证明数据统一合成的数据生成框架，通过三个阶段的合成，实现了高质量和多样化的问题和推理步骤的生成。 |
| [^138] | [Pathformer: Multi-scale transformers with Adaptive Pathways for Time Series Forecasting](https://arxiv.org/abs/2402.05956) | 本文提出了一种名为Pathformer的多尺度自适应路径的Transformer模型，用于时间序列预测。通过整合时间分辨率和时间距离进行多尺度建模，并使用自适应路径来优化建模过程，可以提高预测准确性和泛化能力。 |
| [^139] | [Scalable Wasserstein Gradient Flow for Generative Modeling through Unbalanced Optimal Transport](https://arxiv.org/abs/2402.05443) | 本文介绍了一种可扩展的生成模型，称为Semi-dual JKO (S-JKO)，通过采用半对偶形式的JKO步骤，降低了训练复杂性，并在WGF上有显著的性能改进。 |
| [^140] | [Closing the Gap Between SGP4 and High-Precision Propagation via Differentiable Programming](https://arxiv.org/abs/2402.04830) | 本研究介绍了dSGP4，一种使用PyTorch实现的可微版本的SGP4。通过可微化，dSGP4实现了轨道传播的高精度，并且适用于各种与太空相关的应用，包括卫星轨道确定、状态转换、协方差传播等。 |
| [^141] | [Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction](https://arxiv.org/abs/2402.04154) | 本论文探索了为智能体提供增强形式的任务指导，使其能够理解游戏指导并实现"读玩游戏"的能力。通过将多模态指导调优的成功应用于视觉任务中的强化学习任务，构建了一组... (内容太长，无法继续显示) |
| [^142] | [Standard Gaussian Process is All You Need for High-Dimensional Bayesian Optimization](https://arxiv.org/abs/2402.02746) | 标准 Gaussian 过程在高维贝叶斯优化中表现优秀，经验证据显示其在函数估计和协方差建模中克服了高维输入困难，比专门为高维优化设计的方法表现更好。 |
| [^143] | [Multi-Region Markovian Gaussian Process: An Efficient Method to Discover Directional Communications Across Multiple Brain Regions](https://arxiv.org/abs/2402.02686) | 本研究提出了一种名为多区域马尔可夫高斯过程的方法，将高斯过程和线性动态系统相结合，有效地发现了多个脑区之间的方向性通讯。通过建立LDS与多输出GP之间的联系，该模型实现了线性推断并提供了可解释的低维表示。 |
| [^144] | [Can Large Language Models Replace Economic Choice Prediction Labs?](https://arxiv.org/abs/2401.17435) | 该论文研究大型语言模型是否能够取代经济实验室进行选择预测，并通过相关实验证明了其可行性。 |
| [^145] | [Player Pressure Map -- A Novel Representation of Pressure in Soccer for Evaluating Player Performance in Different Game Contexts](https://arxiv.org/abs/2401.16235) | 本文提出了一种球员压力地图来捕捉足球比赛中球控球队所经历的压力，为教练和分析人员提供了更深入的理解。 |
| [^146] | [Transformer Multivariate Forecasting: Less is More?](https://arxiv.org/abs/2401.00230) | 本文致力于通过引入主成分分析（PCA）来优化Transformer预测框架，以减少冗余信息并提高预测准确性和运行效率。 |
| [^147] | [Multi-Agent Reinforcement Learning for Assessing False-Data Injection Attacks on Transportation Networks](https://arxiv.org/abs/2312.14625) | 引入了一个计算框架，利用多智能体强化学习来找到针对交通网络的最坏情况数据注入攻击，以评估虚假数据注入攻击对交通网络的威胁。 |
| [^148] | [Value Explicit Pretraining for Learning Transferable Representations](https://arxiv.org/abs/2312.12339) | 提出了价值显性预训练（VEP）方法，通过学习编码器来实现学习可迁移的表示，使得能够在新任务中表现优异，对不同任务之间的状态进行关联，实现在Atari和视觉导航中获得多达2倍奖励改善。 |
| [^149] | [Harnessing Inherent Noises for Privacy Preservation in Quantum Machine Learning](https://arxiv.org/abs/2312.11126) | 本文提出利用固有的量子噪声来保护量子机器学习中的数据隐私，对量子电路参数的梯度进行数学分析，并推导其方差的上下界。 |
| [^150] | [DTP-Net: Learning to Reconstruct EEG signals in Time-Frequency Domain by Multi-scale Feature Reuse](https://arxiv.org/abs/2312.09417) | 该论文提出了一种全卷积神经网络架构DTP-Net，通过多尺度特征重用学习在时频域中重建脑电图信号并去除噪声。 |
| [^151] | [Steering Llama 2 via Contrastive Activation Addition](https://arxiv.org/abs/2312.06681) | 引入Contrastive Activation Addition（CAA）方法，通过修改语言模型的激活来精确控制目标行为的程度，显著改变模型行为并在微调和系统提示设计的基础上提供额外有效性。 |
| [^152] | [Efficient Out-of-Distribution Detection with Prototypical Semi-Supervised Learning and Foundation Models](https://arxiv.org/abs/2311.17093) | 本文介绍了一种新的改进的半监督学习方法，利用冻结的基础模型作为神经网络骨干，在半监督学习和超出分布检测方面取得了优越的表现，并引入了新的预训练技术、损失函数和原型选择方法。 |
| [^153] | [Mitigating Biases with Diverse Ensembles and Diffusion Models](https://arxiv.org/abs/2311.16176) | 通过利用扩散概率模型（DPMs）生成新特征组合的图像，可以在集成模型中增加模型多样性，并减轻捷径偏见，而无需额外监督信号。 |
| [^154] | [The Selected-completely-at-random Complementary Label is a Practical Weak Supervision for Multi-class Classification](https://arxiv.org/abs/2311.15502) | 提出了一种不依赖于均匀分布假设的互补标签学习方法，基于完全随机选择假设的无偏风险估计器，以及风险校正方法来解决过拟合问题。 |
| [^155] | [InteRACT: Transformer Models for Human Intent Prediction Conditioned on Robot Actions](https://arxiv.org/abs/2311.12943) | InteRACT通过在大型人类-人类数据集上预训练条件意图预测模型，并在小型人机数据集上微调，解决了人机交互中的先有鸡还是先有蛋问题。 |
| [^156] | [On the Out-of-Distribution Coverage of Combining Split Conformal Prediction and Bayesian Deep Learning](https://arxiv.org/abs/2311.12688) | 结合拆分置信预测和贝叶斯深度学习在多类图像分类中的分布外覆盖率基于模型在校准集上的信心水平，可能会影响模型对分布外数据的处理。 |
| [^157] | [SkelVIT: Consensus of Vision Transformers for a Lightweight Skeleton-Based Action Recognition System](https://arxiv.org/abs/2311.08094) | 本研究考察了VIT对基于骨骼的动作识别的有效性，并提出了一种三级架构SkelVit，该架构能够形成一组伪图像，并在每个表示上应用分类器，然后结合它们的结果来找到动作识别系统中的最佳表现。 |
| [^158] | [Can LLMs Follow Simple Rules?](https://arxiv.org/abs/2311.04235) | 提出了一个名为RuLES的程序框架，用于衡量LLMs在与用户交互时遵守规则的能力。 |
| [^159] | [AI Hazard Management: A framework for the systematic management of root causes for AI risks](https://arxiv.org/abs/2310.16727) | 介绍了AI危险管理框架（AIHM），旨在系统地识别、评估和处理AI危险，并在系统开发过程中进行，以确保尽早发现任何AI危险。 |
| [^160] | [Multi-Factor Spatio-Temporal Prediction based on Graph Decomposition Learning](https://arxiv.org/abs/2310.10374) | 提出了一个多因素时空预测任务，通过预测不同因素下的部分时空数据演变，并将它们结合起来进行最终预测，提出了有效的理论解决方案和可移植的实例化框架。 |
| [^161] | [VeCLIP: Improving CLIP Training via Visual-enriched Captions](https://arxiv.org/abs/2310.07699) | 本研究提出了一种通过将视觉概念融入标题中的方式来改进CLIP训练的方法，名为VeCLIP，该方法在大规模网络爬取数据集上展示了良好的性能。 |
| [^162] | [Does Writing with Language Models Reduce Content Diversity?](https://arxiv.org/abs/2309.05196) | 写作时使用InstructGPT（而不是GPT3）会显著降低内容多样性，增加不同作者之间的相似性，并减少整体的词汇和内容多样性。 |
| [^163] | [DWA: Differential Wavelet Amplifier for Image Super-Resolution](https://arxiv.org/abs/2307.04593) | 差分小波放大器（DWA）为基于小波的图像超分辨率引入了新的模块，通过利用两个卷积滤波器的差异改进了特征提取，强调了局部对比度并抑制了常见噪声，从而在经典超分辨率任务中取得了明显的改进。 |
| [^164] | [Identifying Spurious Biases Early in Training through the Lens of Simplicity Bias](https://arxiv.org/abs/2305.18761) | 本研究通过理论分析揭示了简单性偏见对学习虚假相关性的影响，提出了早期识别和缓解虚假相关性的方法SPARE，并在实验证明其优于目前最先进的方法。 |
| [^165] | [Sparse Representer Theorems for Learning in Reproducing Kernel Banach Spaces](https://arxiv.org/abs/2305.12584) | 该论文研究了在再生核Banach空间中促进稀疏学习解的条件，并建立了相应的表示定理和转换机制。 |
| [^166] | [Misspecification-robust Sequential Neural Likelihood for Simulation-based Inference](https://arxiv.org/abs/2301.13368) | 本文提出了一种新的顺序神经似然方法，通过纳入额外的调整参数，使其对模型错误规定具有鲁棒性，并能够识别数据的特征。 |
| [^167] | [PDFormer: Propagation Delay-Aware Dynamic Long-Range Transformer for Traffic Flow Prediction](https://arxiv.org/abs/2301.07945) | PDFormer是一种考虑传播时延的动态远程Transformer，用于解决交通流预测中静态空间依赖、短程信息和时间延迟的挑战。 |
| [^168] | [First-order penalty methods for bilevel optimization](https://arxiv.org/abs/2301.01716) | 本文研究双层优化问题中的一阶惩罚方法，提出了寻找$\varepsilon$-KKT解的方法，并在适当假设下确立了其操作复杂度。 |
| [^169] | [Self-supervised Trajectory Representation Learning with Temporal Regularities and Travel Semantics](https://arxiv.org/abs/2211.09510) | 提出了一种新颖的自监督轨迹表示学习框架START，充分利用时间规律和行程语义，通过轨迹模式增强图注意力网络(TPE-GAT)将道路网络特征和行程语义转换为道路段的表示向量。 |
| [^170] | [Climbing Routes Clustering Using Energy-Efficient Accelerometers Attached to the Quickdraws](https://arxiv.org/abs/2211.02680) | 利用节能加速计对攀岩路线进行聚类，为攀岩健身房提供了改善服务和最大程度利用基础设施的新方法 |
| [^171] | [FIRE: A Failure-Adaptive Reinforcement Learning Framework for Edge Computing Migrations](https://arxiv.org/abs/2209.14399) | 提出了一个面向边缘计算迁移的故障自适应强化学习框架 FIRE，引入ImRE算法，通过在边缘计算数字孪生环境中训练RL策略来适应罕见事件，解决了RL框架在处理偶发服务器故障方面的挑战。 |
| [^172] | [Regression modelling of spatiotemporal extreme U.S. wildfires via partially-interpretable neural networks](https://arxiv.org/abs/2208.07581) | 提出了一种新的方法框架，利用神经网络进行极端分位数回归，以更好地理解和量化美国极端森林火灾的风险 |
| [^173] | [Can phones, syllables, and words emerge as side-products of cross-situational audiovisual learning? -- A computational investigation](https://arxiv.org/abs/2109.14200) | 研究探讨了语言学习中，通过跨情境视听学习，音素、音节和单词能否作为副产品出现，并支持不同形式表征间的转换。 |
| [^174] | [On the expressivity of bi-Lipschitz normalizing flows](https://arxiv.org/abs/2107.07232) | 讨论了双Lipschitz正规化流的表达能力，发现了一些难以逼近的目标分布，并通过给出下界刻画了它们之间的距离，最后讨论了使用更复杂的潜在分布等潜在改进方法。 |
| [^175] | [On the consistency of supervised learning with missing values](https://arxiv.org/abs/1902.06931) | 两种方法在带缺失值的监督学习中表现出一致性，当缺失值不具信息性时，使用常数进行插补是一种简单且重要的实践方法。 |
| [^176] | [Who Are We Missing? A Principled Approach to Characterizing the Underrepresented Population.](http://arxiv.org/abs/2401.14512) | 本文提出了一种基于优化的方法，Rashomon Set of Optimal Trees (ROOT)，用于识别和描述随机对照试验中的少数人群。该方法通过最小化目标平均处理效应估计的方差来优化目标子群体分布，从而提供更精确和可解释的处理效应估计。与其他方法相比，该方法具有更高的精度和可解释性，通过合成数据实验进行了验证。 |
| [^177] | [Graph Contrastive Invariant Learning from the Causal Perspective.](http://arxiv.org/abs/2401.12564) | 本文从因果关系的角度研究了图对比不变学习，并提出了一种新的GCL方法，通过引入谱图扩增和设计不变性目标和独立性目标来更好地学习不变表示。 |
| [^178] | [AutoFT: Robust Fine-Tuning by Optimizing Hyperparameters on OOD Data.](http://arxiv.org/abs/2401.10220) | AutoFT是一种通过优化超参数在OOD数据上进行基础模型微调的方法，以实现鲁棒性能。 |
| [^179] | [AST-T5: Structure-Aware Pretraining for Code Generation and Understanding.](http://arxiv.org/abs/2401.03003) | AST-T5是一种结构感知的预训练模型，通过利用抽象语法树（AST）来增强代码生成、转换和理解的能力。它优于其他同等大小的语言模型，并在代码到代码任务中表现出色。 |
| [^180] | [Diffusion Model with Perceptual Loss.](http://arxiv.org/abs/2401.00110) | 本研究介绍了一种使用感知损失的扩散模型，通过无分类器指导实现了生成更真实样本的目的。 |
| [^181] | [Is Scaling Learned Optimizers Worth It? Evaluating The Value of VeLO's 4000 TPU Months.](http://arxiv.org/abs/2310.18191) | VeLO是迄今为止规模最大的训练通用“基础”优化器的尝试，但我们的评估发现它需要问题特定的调优，并不一定优于竞争对手的解决方案质量和训练误差降低速度，这对于VeLO的通用性和培训投资的价值提出了质疑。 |
| [^182] | [When Machine Learning Models Leak: An Exploration of Synthetic Training Data.](http://arxiv.org/abs/2310.08775) | 本论文研究了针对一个预测人员或家庭是否会在接下来的两年内搬迁的机器学习模型的攻击，攻击者利用模型的预测以及公开的训练数据边际分布来推断目标个体的敏感属性值，同时探讨了用合成数据替代原始数据训练模型对攻击的影响。 |
| [^183] | [GenTKG: Generative Forecasting on Temporal Knowledge Graph.](http://arxiv.org/abs/2310.07793) | 研究提出了一种名为GenTKG的生成模型，用于在时间知识图谱上进行预测。该模型通过结合基于时间逻辑规则的检索策略和轻量级的参数效率指导，克服了复杂的时间图数据结构和庞大的数据量所带来的挑战。 |
| [^184] | [Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment.](http://arxiv.org/abs/2310.07229) | 本论文提出了一种通过蛋白质片段和周围对齐的自监督口袋预训练方法，用于模拟并生成大量的配体-受体相互作用复合物。 |
| [^185] | [A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence Classes with the same Skeleton.](http://arxiv.org/abs/2310.04218) | 本文提出了一个固定参数可处理算法，用于计数具有相同骨架的马尔可夫等价类。 |
| [^186] | [Adapting Large Language Models for Content Moderation: Pitfalls in Data Engineering and Supervised Fine-tuning.](http://arxiv.org/abs/2310.03400) | 本文介绍了如何对LLM模型进行微调以实现内容审查的私下部署，讨论了引入原因的微调过程和直接分类任务的区别，并研究了利用更强大的LLM生成的原因对微调的影响。 |
| [^187] | [Analyzing and Improving OT-based Adversarial Networks.](http://arxiv.org/abs/2310.02611) | 本文分析和改进了基于OT的对抗网络，首先在一个统一的框架中统一了这些方法，然后通过全面分析展示了各组件在训练中的作用，最后提出了一个简单但新颖的方法以改进最优生成模型，该方法通过逐步调整生成分布逐渐使其与数据分布对齐 |
| [^188] | [Implicit regularization of multi-task learning and finetuning in overparameterized neural networks.](http://arxiv.org/abs/2310.02396) | 本文研究了在过参数化神经网络中，多任务学习和微调所带来的隐式正则化效果。在简化的线性网络环境中，我们发现了多任务学习和微调所对特征共享和学习特定特征稀疏性的鼓励作用，并发现微调过程同时具有内核和特征学习的混合状态。此外，微调还可以展现一种嵌套特征学习行为，使其偏向于提取一组稀疏的特征子集。 |
| [^189] | [Score dynamics: scaling molecular dynamics with picosecond timesteps via conditional diffusion model.](http://arxiv.org/abs/2310.01678) | 该论文提出了Score dynamics (SD) 方法，通过条件扩散模型，可以使用1 ps的时间步长进行分子动力学模拟。基于图神经网络的Score dynamics模型展示了在丙氨酸二肽和短链烷烃案例中的效果。 |
| [^190] | [RECOMBINER: Robust and Enhanced Compression with Bayesian Implicit Neural Representations.](http://arxiv.org/abs/2309.17182) | RECOMBINER是一种鲁棒性和增强的贝叶斯隐式神经表示的压缩方法，通过丰富变分逼近、增加位置编码和分割高分辨率数据来解决COMBINER存在的局限性。 |
| [^191] | [Unsupervised Fact Verification by Language Model Distillation.](http://arxiv.org/abs/2309.16540) | 本文提出了一种名为SFAVEL的无监督框架，通过语言模型蒸馏将自监督特征转化为高质量的主张-事实对齐，实现无监督事实验证。这通过一种新颖的对比损失函数实现，同时保留语料库间的语义关系。 |
| [^192] | [Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks.](http://arxiv.org/abs/2309.16347) | 本文提出了基于大型语言模型的内在引导探索（IGE-LLMs）框架，通过利用LLMs作为辅助内在奖励，解决了复杂长视程机器人操作任务中奖励稀疏问题，并在实验中展示了其较高的性能和模块化特性。 |
| [^193] | [PointSSC: A Cooperative Vehicle-Infrastructure Point Cloud Benchmark for Semantic Scene Completion.](http://arxiv.org/abs/2309.12708) | PointSSC是第一个为了语义场景补全而引入的车辆基础设施点云合作基准，具备长距离感知和最小遮挡。通过使用Segment Anything进行自动化注释，我们提出了一种基于激光雷达的模型，结合补全和分割的合作模块，来推动语义点云补全在真实世界导航中的发展。 |
| [^194] | [Tackling the Non-IID Issue in Heterogeneous Federated Learning by Gradient Harmonization.](http://arxiv.org/abs/2309.06692) | 本研究通过梯度协调方法解决了异构联邦学习中的非独立同分布问题，提出了FedGH，通过减轻本地漂移来增强性能。实验证明，在多个基准和非独立同分布场景下，FedGH始终能够显著提升联邦学习的性能。 |
| [^195] | [Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning.](http://arxiv.org/abs/2309.06553) | 这项工作介绍了一种基于离线逆向强化学习的提示评估与优化方法，通过利用离线数据集和逆向强化学习，预测提示性能、提高成本效益、生成易读的结果。 |
| [^196] | [Crystal Structure Prediction by Joint Equivariant Diffusion.](http://arxiv.org/abs/2309.04475) | 本文提出了DiffCSP，一种利用周期-E(3)-等变去噪模型的扩散模型，用于通过联合生成晶体的晶格和原子坐标，以解决晶体结构预测中的对称性挑战。 |
| [^197] | [A correlation-based fuzzy cluster validity index with secondary options detector.](http://arxiv.org/abs/2308.14785) | 本研究提出了一种基于相关性的模糊聚类有效性指标，该指标考虑了在聚类数量选择时可能存在的多个选项，并通过评估在多种数据集上的性能，与现有指标进行比较。 |
| [^198] | [Resource-Efficient Federated Learning for Heterogenous and Resource-Constrained Environments.](http://arxiv.org/abs/2308.13662) | 提出了一种新的资源高效联邦学习方法，通过可变剪枝技术和知识蒸馏来解决资源受限设备中的计算和通信挑战，并在保持数据隐私和性能的同时适应异构模型架构。 |
| [^199] | [Unified Data Management and Comprehensive Performance Evaluation for Urban Spatial-Temporal Prediction [Experiment, Analysis & Benchmark].](http://arxiv.org/abs/2308.12899) | 该论文提出了针对城市时空预测的统一数据管理和综合性能评估方法。其贡献包括引入“原子文件”作为统一存储格式，提供了城市时空预测模型技术进展的全面概述，并建立了性能排行榜和鉴定了有潜力的模型和数据集。 |
| [^200] | [A Benchmark Study on Calibration.](http://arxiv.org/abs/2308.11838) | 这项研究提出了一个模型校准的基准研究，利用神经架构搜索空间探索了模型校准属性。研究结果显示，模型校准可以在不同任务中泛化，并可以同时兼顾模型的准确性和校准性能。 |
| [^201] | [Learning Abstract Visual Reasoning via Task Decomposition: A Case Study in Raven Progressive Matrices.](http://arxiv.org/abs/2308.06528) | 通过任务分解学习抽象视觉推理，提出了一种基于变形器蓝图的深度学习架构，该架构预测单个对象及其排列的视觉特性，通过多维预测来选择答案。 |
| [^202] | [Efficient Variational Inference for Large Skew-t Copulas with Application to Intraday Equity Returns.](http://arxiv.org/abs/2308.05564) | 本研究提出一种快速而准确的贝叶斯变分推理方法，用于估计大规模偏t乌鸦因子勾结模型。该方法能够捕捉到金融数据中的不对称和极端尾部相关性，以及股票对之间的异质性非对称依赖。 |
| [^203] | [Generative Adversarial Networks for Stain Normalisation in Histopathology.](http://arxiv.org/abs/2308.02851) | 本论文研究了生成对抗网络在数字病理学染色标准化中的应用，发现基于GAN的方法在染色标准化方面表现出色，但需要更大的计算资源。 |
| [^204] | [Predicting small molecules solubilities on endpoint devices using deep ensemble neural networks.](http://arxiv.org/abs/2307.05318) | 这项工作提出了一种使用深度集成神经网络在端点设备上预测小分子溶解度的方法，通过静态网站运行，同时具备预测不确定性，并实现了令人满意的结果。 |
| [^205] | [Polynomial Width is Sufficient for Set Representation with High-dimensional Features.](http://arxiv.org/abs/2307.04001) | 本研究通过两种集合元素嵌入层的探索，证明了多项式宽度对于高维特征的集合表示足够，并揭示了之前分析中的局限性。 |
| [^206] | [Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework.](http://arxiv.org/abs/2307.01715) | 本文提出了一个通用的插入式框架，用于优化CTC模型中的所需属性。该框架通过补充额外的损失项来优先考虑符合所需属性的对齐，并不需要修改CTC损失函数。 |
| [^207] | [Accelerating Sampling and Aggregation Operations in GNN Frameworks with GPU Initiated Direct Storage Accesses.](http://arxiv.org/abs/2306.16384) | 本论文提出了一种通过利用GPU发起直接存储访问来加速GNN框架中的采样和聚合操作的方法，解决了在训练大规模图上时CPU无法充分利用GPU资源的问题。 |
| [^208] | [Safety-Critical Scenario Generation Via Reinforcement Learning Based Editing.](http://arxiv.org/abs/2306.14131) | 提出了一种基于强化学习的场景生成方法，通过顺序编辑生成安全关键场景，克服了维度挑战，并生成了质量更高的场景。 |
| [^209] | [EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations.](http://arxiv.org/abs/2306.12059) | 本文提出了EquiformerV2，通过使用新的卷积类型和架构改进，扩展了等变Transformer到更高的等变表示，在处理大型数据集时表现更好，能量和力的表现也得到了提高，计算效率也得到了提升。 |
| [^210] | [Towards Faster Non-Asymptotic Convergence for Diffusion-Based Generative Models.](http://arxiv.org/abs/2306.09251) | 该论文针对扩散生成模型设计了非渐进理论，提出了针对两种主流采样器的新的收敛速度，提高了总步数与收敛速度的比例。 |
| [^211] | [Tackling Heavy-Tailed Rewards in Reinforcement Learning with Function Approximation: Minimax Optimal and Instance-Dependent Regret Bounds.](http://arxiv.org/abs/2306.06836) | 本文解决了强化学习中当奖励呈“重尾”分布时的问题，提出了第一种处理这种情况的实例相关算法，并得到了极小最大化的遗憾界。 |
| [^212] | [DOCTOR: A Multi-Disease Detection Continual Learning Framework Based on Wearable Medical Sensors.](http://arxiv.org/abs/2305.05738) | DOCTOR是一种基于可穿戴医疗传感器的多疾病检测持续学习框架，采用了多头深度神经网络和Exemplar-replay风格的CL算法。它可以不断地学习新任务，并在内存使用、电池消耗和检测复杂度方面优于传统的ML驱动疾病检测方法。 |
| [^213] | [Policy Gradient Methods in the Presence of Symmetries and State Abstractions.](http://arxiv.org/abs/2305.05666) | 本文研究了在连续控制环境中的抽象，提出了一种策略梯度定理，允许利用环境的近似对称性进行策略优化，并提出了一系列演员-评论家算法进行策略和MDP同态映射的学习，最后展示了算法在连续对称性环境和视觉控制任务中的有效性。 |
| [^214] | [Towards Achieving Near-optimal Utility for Privacy-Preserving Federated Learning via Data Generation and Parameter Distortion.](http://arxiv.org/abs/2305.04288) | 本论文提出了一种用数据生成和参数畸变实现隐私保护联邦学习接近最优效用的上限方法，其中通过降低方差和模型参数差异来衡量效用损失。 |
| [^215] | [Diffusion Models for Constrained Domains.](http://arxiv.org/abs/2304.05364) | 本研究提出了两种方法来创建约束域的降噪扩散模型。第一种方法基于不等式约束诱导的对数障碍度量，第二种方法基于反射布朗运动。这些方法将扩散模型的应用范围扩展到了机器人和蛋白设计等领域。 |
| [^216] | [Forecasting localized weather impacts on vegetation as seen from space with meteo-guided video prediction.](http://arxiv.org/abs/2303.16198) | 本文提出一种利用气象引导的视频预测方法，从卫星视角预测欧洲地区植被对天气的响应，建立了相应的模型，并证明了该方法在卫星图像预测中具有优越性能。此外，该模型还可用于下游任务，如碳监测的总初级生产力的推断。 |
| [^217] | [Uni-RXN: A Unified Framework Bridging the Gap between Chemical Reaction Pretraining and Conditional Molecule Generation.](http://arxiv.org/abs/2303.06965) | 本文提出了Uni-RXN框架，在化学反应Pretraining和分子生成任务中都取得了最先进的结果。通过具备化学知识，克服了当前分子生成模型仅依赖少量反应模板的限制，生成质量高、可合成的药物类分子结构。 |
| [^218] | [FOSI: Hybrid First and Second Order Optimization.](http://arxiv.org/abs/2302.08484) | FOSI是一种元算法，它在优化过程中有效地加入二阶信息以提高任何一阶优化器的性能，并可改善一类优化器的条件数 |
| [^219] | [Hierarchical Data-efficient Representation Learning for Tertiary Structure-based RNA Design.](http://arxiv.org/abs/2301.10774) | 本研究提出了一个基于层次数据有效表示学习的RNA设计流程，通过构建大型数据集并设计全面的结构建模方法，实现了更高效的RNA序列设计。 |
| [^220] | [Label Alignment Regularization for Distribution Shift.](http://arxiv.org/abs/2211.14960) | 这篇论文提出了一种用于无监督领域自适应的正则化方法，通过鼓励目标域中的预测与其前几个奇异向量对齐来实现。与传统方法不同的是，这个方法通过正则化分类器与无监督目标数据对齐，而不是正则化表示。通过消除对最优联合风险假设的依赖，该方法展示了很好的效果。 |

# 详细

[^1]: 将Thompson抽样遗憾与Sigma比率（TS-RSR）最小化：一种用于批量贝叶斯优化的经过证明的高效算法

    Minimizing the Thompson Sampling Regret-to-Sigma Ratio (TS-RSR): a provably efficient algorithm for batch Bayesian Optimization

    [https://arxiv.org/abs/2403.04764](https://arxiv.org/abs/2403.04764)

    该论文提出了一种用于批量贝叶斯优化的高效算法，通过最小化Thompson抽样近似的遗憾与不确定性比率，成功协调每个批次的动作选择，同时实现高概率的理论保证，并在非凸测试函数上表现出色.

    

    本文提出了一个新的方法，用于批量贝叶斯优化（BO），其中抽样通过最小化Thompson抽样方法的遗憾与不确定性比率来进行。我们的目标是能够协调每个批次中选择的动作，以最小化点之间的冗余，同时关注具有高预测均值或高不确定性的点。我们对算法的遗憾提供了高概率的理论保证。最后，从数字上看，我们证明了我们的方法在一系列非凸测试函数上达到了最先进的性能，在平均值上比几个竞争对手的基准批量BO算法表现提高了一个数量级。

    arXiv:2403.04764v1 Announce Type: new  Abstract: This paper presents a new approach for batch Bayesian Optimization (BO), where the sampling takes place by minimizing a Thompson Sampling approximation of a regret to uncertainty ratio. Our objective is able to coordinate the actions chosen in each batch in a way that minimizes redundancy between points whilst focusing on points with high predictive means or high uncertainty. We provide high-probability theoretical guarantees on the regret of our algorithm. Finally, numerically, we demonstrate that our method attains state-of-the-art performance on a range of nonconvex test functions, where it outperforms several competitive benchmark batch BO algorithms by an order of magnitude on average.
    
[^2]: BloomGML: 透过双层优化镜头的图机器学习

    BloomGML: Graph Machine Learning through the Lens of Bilevel Optimization

    [https://arxiv.org/abs/2403.04763](https://arxiv.org/abs/2403.04763)

    本文将各种图学习技术重新构建为双层优化的特例或简化形式，并提出了更灵活的能量函数以形成图神经网络消息传递层，同时揭示了残余近似误差的来源。

    

    双层优化是指在一个低层次能量函数的最优解用作感兴趣的上层目标的输入特征的情况。这些最优特征通常依赖于低层次能量的可调参数，使得整个双层管道可以端对端训练。尽管通常未被提出，本文演示了如何将各种图学习技术重新构建为双层优化的特例或简化形式。简而言之，基于先前工作，我们首先推导出一类更灵活的能量函数，当与各种下降步骤配对（例如梯度下降、近端方法、动量等），形成图神经网络（GNN）消息传递层；关键是，我们还仔细解释了与底层传递函数有关的任何剩余近似误差所在。然后，我们探究了几个相

    arXiv:2403.04763v1 Announce Type: new  Abstract: Bilevel optimization refers to scenarios whereby the optimal solution of a lower-level energy function serves as input features to an upper-level objective of interest. These optimal features typically depend on tunable parameters of the lower-level energy in such a way that the entire bilevel pipeline can be trained end-to-end. Although not generally presented as such, this paper demonstrates how a variety of graph learning techniques can be recast as special cases of bilevel optimization or simplifications thereof. In brief, building on prior work we first derive a more flexible class of energy functions that, when paired with various descent steps (e.g., gradient descent, proximal methods, momentum, etc.), form graph neural network (GNN) message-passing layers; critically, we also carefully unpack where any residual approximation error lies with respect to the underlying constituent message-passing functions. We then probe several sim
    
[^3]: iScore: 用于解释语言模型如何自动评分摘要的可视化分析

    iScore: Visual Analytics for Interpreting How Language Models Automatically Score Summaries

    [https://arxiv.org/abs/2403.04760](https://arxiv.org/abs/2403.04760)

    设计了iScore工具，通过用户中心设计过程解决了大型语言模型在自动评分摘要中的透明度和信任问题

    

    近年来，大型语言模型（LLMs）的普及急剧增长，激发了学习工程师将它们纳入自适应教育工具中，用于自动评分摘要写作。在将它们部署到关键学习环境之前，了解和评估LLMs至关重要，然而它们数量庞大的参数和日益增加的规模阻碍了透明度，当它们表现不佳时还会影响信任。通过与多位构建和部署摘要评分LLMs的学习工程师展开协作的用户中心设计过程，我们界定了解释其模型的基本设计挑战和目标，包括整合大量文本输入、跟踪得分来源以及扩展LLM可解释性方法。为了解决他们的关切，我们开发了iScore，一款互动式可视化分析工具，供学习工程师同时上传、评分和比较多个摘要。与此同时紧密集成的视图

    arXiv:2403.04760v1 Announce Type: cross  Abstract: The recent explosion in popularity of large language models (LLMs) has inspired learning engineers to incorporate them into adaptive educational tools that automatically score summary writing. Understanding and evaluating LLMs is vital before deploying them in critical learning environments, yet their unprecedented size and expanding number of parameters inhibits transparency and impedes trust when they underperform. Through a collaborative user-centered design process with several learning engineers building and deploying summary scoring LLMs, we characterized fundamental design challenges and goals around interpreting their models, including aggregating large text inputs, tracking score provenance, and scaling LLM interpretability methods. To address their concerns, we developed iScore, an interactive visual analytics tool for learning engineers to upload, score, and compare multiple summaries simultaneously. Tightly integrated views
    
[^4]: 利用超高维计算实现边缘以外的终生智能

    Lifelong Intelligence Beyond the Edge using Hyperdimensional Computing

    [https://arxiv.org/abs/2403.04759](https://arxiv.org/abs/2403.04759)

    本文设计并部署了名为LifeHD的首个面向通用物联网应用的边缘设备终生成长学习系统，基于新颖、轻量级的神经启发式学习范式超高维计算（HDC）。通过利用两层联想记忆组织智能存储和管理高维、低精度向量，实现了在变化环境中对历史模式进行无限学习的能力。

    

    On-device learning已经成为一种盛行趋势，它避免了云端学习的响应时间缓慢和沟通成本高昂。在变化环境中连续、无限地学习，并且受到资源约束，对于真实传感器部署至关重要。然而，现有设计对于具有(i)数据流输入、(ii)缺乏监督和(iii)有限板载资源的实际场景是不足够的。在本文中，我们设计并部署了第一个面向具有有限监理的一般物联网应用的边缘设备终生成长学习系统LifeHD。LifeHD 基于一种名为超高维计算（HDC）的新颖、轻量级的神经启发式学习范式而设计。我们利用两层联想记忆组织智能存储和管理高维、低精度向量，这些向量代表历史模式作为簇质心。我们还提出了两种变体

    arXiv:2403.04759v1 Announce Type: new  Abstract: On-device learning has emerged as a prevailing trend that avoids the slow response time and costly communication of cloud-based learning. The ability to learn continuously and indefinitely in a changing environment, and with resource constraints, is critical for real sensor deployments. However, existing designs are inadequate for practical scenarios with (i) streaming data input, (ii) lack of supervision and (iii) limited on-board resources. In this paper, we design and deploy the first on-device lifelong learning system called LifeHD for general IoT applications with limited supervision. LifeHD is designed based on a novel neurally-inspired and lightweight learning paradigm called Hyperdimensional Computing (HDC). We utilize a two-tier associative memory organization to intelligently store and manage high-dimensional, low-precision vectors, which represent the historical patterns as cluster centroids. We additionally propose two varian
    
[^5]: 通过比较填空提示解释语言模型的KnowledgeVIS

    KnowledgeVIS: Interpreting Language Models by Comparing Fill-in-the-Blank Prompts

    [https://arxiv.org/abs/2403.04758](https://arxiv.org/abs/2403.04758)

    KnowledgeVIS是一个人机协作的可视分析系统，可以通过比较填空提示之间的预测结果来揭示语言模型学习到的联系，帮助用户理解语言模型的工作方式及原因。

    

    近年来，大型语言模型的流行增长导致它们在总结、预测和生成文本方面的使用增加，因此帮助研究人员和工程师理解其工作方式及原因变得至关重要。我们提出了KnowledgeVis，这是一个人机协作的可视分析系统，用于使用填空句作为提示来解释语言模型。通过比较句子之间的预测结果，KnowledgeVis展示了在训练过程中语言模型学习到的联系，这直观地将语言模型学到的内容与自然语言下游任务联系起来，帮助用户创建和测试多个提示变体，使用新颖的语义聚类技术分析预测的单词，并使用互动可视化发现见解。总的来说，这些可视化帮助用户确定单个预测的可能性和独特性，比较不同提示之间的一组预测，并总结模式和联系。

    arXiv:2403.04758v1 Announce Type: cross  Abstract: Recent growth in the popularity of large language models has led to their increased usage for summarizing, predicting, and generating text, making it vital to help researchers and engineers understand how and why they work. We present KnowledgeVis, a human-in-the-loop visual analytics system for interpreting language models using fill-in-the-blank sentences as prompts. By comparing predictions between sentences, KnowledgeVis reveals learned associations that intuitively connect what language models learn during training to natural language tasks downstream, helping users create and test multiple prompt variations, analyze predicted words using a novel semantic clustering technique, and discover insights using interactive visualizations. Collectively, these visualizations help users identify the likelihood and uniqueness of individual predictions, compare sets of predictions between prompts, and summarize patterns and relationships betw
    
[^6]: JAX-SPH：一种可微平滑粒子流体动力学框架

    JAX-SPH: A Differentiable Smoothed Particle Hydrodynamics Framework

    [https://arxiv.org/abs/2403.04750](https://arxiv.org/abs/2403.04750)

    JAX-SPH是一个在JAX中实现的平滑粒子流体动力学框架，扩展了LagrangeBench项目的代码，集成了关键的SPH算法，验证了梯度的准确性，展示了梯度在解决逆问题和Solver-i中的作用。

    

    基于粒子的流体模拟已经成为解决Navier-Stokes方程的强大工具，特别是在涉及复杂物理和自由表面的情况下。最近将机器学习方法添加到解决这类问题的工具箱中正在推动这些数值模拟的质量与速度之间的权衡边界。在这项工作中，我们引领了与深度学习框架兼容的拉格朗日流体模拟器的新方向，并提出了JAX-SPH——一个在JAX中实现的平滑粒子流体动力学（SPH）框架。JAX-SPH基于从LagrangeBench项目（Toshev等人，2023年）中生成数据集的代码，并通过多种方式扩展了此代码：(a)集成了进一步的关键SPH算法，(b)将代码重组为Python库，(c)通过求解器验证梯度，以及(d)演示了这些梯度在解决逆问题和Solver-i中的效用。

    arXiv:2403.04750v1 Announce Type: cross  Abstract: Particle-based fluid simulations have emerged as a powerful tool for solving the Navier-Stokes equations, especially in cases that include intricate physics and free surfaces. The recent addition of machine learning methods to the toolbox for solving such problems is pushing the boundary of the quality vs. speed tradeoff of such numerical simulations. In this work, we lead the way to Lagrangian fluid simulators compatible with deep learning frameworks, and propose JAX-SPH - a Smoothed Particle Hydrodynamics (SPH) framework implemented in JAX. JAX-SPH builds on the code for dataset generation from the LagrangeBench project (Toshev et al., 2023) and extends this code in multiple ways: (a) integration of further key SPH algorithms, (b) restructuring the code toward a Python library, (c) verification of the gradients through the solver, and (d) demonstration of the utility of the gradients for solving inverse problems as well as a Solver-i
    
[^7]: GNN-VPA: 一种用于图神经网络的方差保持聚合策略

    GNN-VPA: A Variance-Preserving Aggregation Strategy for Graph Neural Networks

    [https://arxiv.org/abs/2403.04747](https://arxiv.org/abs/2403.04747)

    通过提出一种保持方差的聚合函数（VPA），该函数在维持图神经网络（GNNs）的表达能力的基础上，提高了前向和后向动力学，进而导致了增强的预测性能和改善的学习动态。

    

    图神经网络（GNNs），特别是消息传递神经网络，在物理学、药物发现和分子建模等各个领域表现出色。GNNs的表达能力，特别是在区分非同构图的能力，关键取决于用于消息聚合和图级读出的函数。通过应用信号传播理论，我们提出了一种保持方差的聚合函数（VPA），该函数保持了表达能力，同时提高了前向和后向动力学。实验证明，VPA导致了流行的GNN架构的预测性能提高，同时改善了学习动态。我们的结果可能为无归一化或自归一化的GNNs铺平道路。

    arXiv:2403.04747v1 Announce Type: cross  Abstract: Graph neural networks (GNNs), and especially message-passing neural networks, excel in various domains such as physics, drug discovery, and molecular modeling. The expressivity of GNNs with respect to their ability to discriminate non-isomorphic graphs critically depends on the functions employed for message aggregation and graph-level readout. By applying signal propagation theory, we propose a variance-preserving aggregation function (VPA) that maintains expressivity, but yields improved forward and backward dynamics. Experiments demonstrate that VPA leads to increased predictive performance for popular GNN architectures as well as improved learning dynamics. Our results could pave the way towards normalizer-free or self-normalizing GNNs.
    
[^8]: 在幻境中的大型语言模型：通过模拟试错学习工具

    LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error

    [https://arxiv.org/abs/2403.04746](https://arxiv.org/abs/2403.04746)

    提出了一种受生物启发的方法，即模拟试错（STE），为工具增强型LLMs编排了三个关键机制以实现成功的工具使用行为

    

    工具对于大型语言模型（LLMs）获取最新信息并在外部环境中采取重要行动至关重要。现有关于工具增强型LLMs的工作主要集中在工具的广泛覆盖范围和灵活性上。然而，一个被人意外忽视的关键方面是LLM在经过训练后如何准确使用工具。我们发现，包括GPT-4和专门为工具使用进行微调的开源LLMs在正确率方面仅达到30%到60%的范围，远不足以在实践中可靠使用。我们提出了一种受生物启发的方法，即模拟试错（STE），为工具增强型LLMs编排了三个关键机制以实现成功的工具使用行为：试错、想象和记忆。具体而言，STE利用LLM的“想象力”来模拟使用工具的可能场景，

    arXiv:2403.04746v1 Announce Type: cross  Abstract: Tools are essential for large language models (LLMs) to acquire up-to-date information and take consequential actions in external environments. Existing work on tool-augmented LLMs primarily focuses on the broad coverage of tools and the flexibility of adding new tools. However, a critical aspect that has surprisingly been understudied is simply how accurately an LLM uses tools for which it has been trained. We find that existing LLMs, including GPT-4 and open-source LLMs specifically fine-tuned for tool use, only reach a correctness rate in the range of 30% to 60%, far from reliable use in practice. We propose a biologically inspired method for tool-augmented LLMs, simulated trial and error (STE), that orchestrates three key mechanisms for successful tool use behaviors in the biological system: trial and error, imagination, and memory. Specifically, STE leverages an LLM's 'imagination' to simulate plausible scenarios for using a tool,
    
[^9]: SQ在较弱假设下用于非高斯成分分析的下限

    SQ Lower Bounds for Non-Gaussian Component Analysis with Weaker Assumptions

    [https://arxiv.org/abs/2403.04744](https://arxiv.org/abs/2403.04744)

    本研究证明在非高斯成分分析中，对于区分标准多元高斯和在随机隐藏方向上行为类似某一维分布而在正交补上行为类似标准高斯的问题，先前被要求的卡方条件实际上是不必要的。

    

    我们研究了统计查询（SQ）模型中非高斯成分分析（NGCA）的复杂性。 先前的工作开发了一种一般方法，用于证明这一任务的SQ下界，并适用于广泛的上下文。 特别地，已知对于满足某些条件的任何一维分布$A$，区分标准多元高斯和在随机隐藏方向上的行为类似$A$而在正交补上行为类似标准高斯的分布是SQ-hard的。 所需条件是（1）$A$与标准一维高斯匹配许多低阶矩，和（2）$A$相对于标准高斯的卡方范数是有限的。 虽然满足矩匹配条件对于难度是必要的，但卡方条件仅出于技术原因而被要求。 在这项工作中，我们证明了后一个条件实际上并非必要。

    arXiv:2403.04744v1 Announce Type: new  Abstract: We study the complexity of Non-Gaussian Component Analysis (NGCA) in the Statistical Query (SQ) model. Prior work developed a general methodology to prove SQ lower bounds for this task that have been applicable to a wide range of contexts. In particular, it was known that for any univariate distribution $A$ satisfying certain conditions, distinguishing between a standard multivariate Gaussian and a distribution that behaves like $A$ in a random hidden direction and like a standard Gaussian in the orthogonal complement, is SQ-hard. The required conditions were that (1) $A$ matches many low-order moments with the standard univariate Gaussian, and (2) the chi-squared norm of $A$ with respect to the standard Gaussian is finite. While the moment-matching condition is necessary for hardness, the chi-squared condition was only required for technical reasons. In this work, we establish that the latter condition is indeed not necessary. In partic
    
[^10]: 一种用于鲁棒稀疏均值估计的次二次时间算法

    A Sub-Quadratic Time Algorithm for Robust Sparse Mean Estimation

    [https://arxiv.org/abs/2403.04726](https://arxiv.org/abs/2403.04726)

    提出了一种次二次时间算法，用于在存在恶意异常值的情况下进行鲁棒稀疏均值估计

    

    我们研究了在存在恶意异常值的情况下稀疏均值估计的算法问题。具体来说，算法观测到从$\mathcal{N}(\mu,\mathbf{I}_d)$中的\emph{受损害}样本集，其中未知均值$\mu \in \mathbb{R}^d$被限制为$k$-稀疏。一系列先前的研究工作已经开发出了具有样本复杂度$\mathrm{poly}(k,\log d, 1/\epsilon)$和运行时间$d^2 \mathrm{poly}(k,\log d,1/\epsilon)$的高效算法，其中$\epsilon$是污染分数。特别是，现有算法的最快运行时间是二次的($\Omega(d^2)$)，在高维情况下可能是禁止性的。现有算法运行时间的二次障碍源于这些算法对样本协方差矩阵的依赖，其大小为$d^2$。我们的主要贡献是一种用于鲁棒稀疏均值估计的算法，它在\emph{次二次}时间内运行，使用$\mathrm{poly}

    arXiv:2403.04726v1 Announce Type: cross  Abstract: We study the algorithmic problem of sparse mean estimation in the presence of adversarial outliers. Specifically, the algorithm observes a \emph{corrupted} set of samples from $\mathcal{N}(\mu,\mathbf{I}_d)$, where the unknown mean $\mu \in \mathbb{R}^d$ is constrained to be $k$-sparse. A series of prior works has developed efficient algorithms for robust sparse mean estimation with sample complexity $\mathrm{poly}(k,\log d, 1/\epsilon)$ and runtime $d^2 \mathrm{poly}(k,\log d,1/\epsilon)$, where $\epsilon$ is the fraction of contamination. In particular, the fastest runtime of existing algorithms is quadratic ($\Omega(d^2)$), which can be prohibitive in high dimensions. This quadratic barrier in the runtime stems from the reliance of these algorithms on the sample covariance matrix, which is of size $d^2$. Our main contribution is an algorithm for robust sparse mean estimation which runs in \emph{subquadratic} time using $\mathrm{poly
    
[^11]: 在超参数优化中重新思考基于编码器的热启动方法

    Rethinking of Encoder-based Warm-start Methods in Hyperparameter Optimization

    [https://arxiv.org/abs/2403.04720](https://arxiv.org/abs/2403.04720)

    提出了一种新的基于编码器的表格数据集表示方法，与现有方法不同，能够自动提取重要的元特征，同时在两个常见的元任务上进行了评估

    

    有效地表示异质性表格数据集以用于元学习仍然是一个未解决的问题。以往的方法依赖于预定义的元特征，例如，统计量或标志点。基于编码器的模型，如Dataset2Vec，使我们能够在无人干预的情况下自动提取重要的元特征。本研究介绍了一个新颖的基于编码器的表格数据集表示，实现在liltab包中，该包可在GitHub上找到https://github.com/azoz01/liltab。我们的包基于[Iwata and Kumagai, 2020]提出的一个用于异质表格数据的已建立模型。所提出的方法采用一种不同于现有方法如Dataset2Vec 的编码特征关系的模型，生成与现有方法不同的替代表示。它们都利用了数据集相似性学习的基本假设。在这项工作中，我们在两个常见的元任务上评价了Dataset2Vec和liltab

    arXiv:2403.04720v1 Announce Type: new  Abstract: Effectively representing heterogeneous tabular datasets for meta-learning remains an open problem. Previous approaches rely on predefined meta-features, for example, statistical measures or landmarkers. Encoder-based models, such as Dataset2Vec, allow us to extract significant meta-features automatically without human intervention. This research introduces a novel encoder-based representation of tabular datasets implemented within the liltab package available on GitHub https://github.com/azoz01/liltab. Our package is based on an established model for heterogeneous tabular data proposed in [Iwata and Kumagai, 2020]. The proposed approach employs a different model for encoding feature relationships, generating alternative representations compared to existing methods like Dataset2Vec. Both of them leverage the fundamental assumption of dataset similarity learning. In this work, we evaluate Dataset2Vec and liltab on two common meta-tasks - r
    
[^12]: 通过标记级别的不确定性量化检验大型语言模型的输出

    Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification

    [https://arxiv.org/abs/2403.04696](https://arxiv.org/abs/2403.04696)

    提出了一种基于标记级别不确定性量化的新型事实核查和幻觉检测流程，该方法能够检测大型语言模型输出中的不可靠预测。

    

    大型语言模型(LLMs)以产生错误的声明而臭名昭著。这种幻觉可能很危险，因为在生成的文本中偶尔出现的事实不准确可能会被整体上是事实的文本掩盖，这使得用户极其难以发现。利用LLMs的当前服务通常不提供检测不可靠生成的方式。在这里，我们旨在弥补这一空白。具体而言，我们提出了一种基于标记级别的不确定性量化的新型事实核查和幻觉检测流程。不确定性分数利用了神经网络或其层输出中包含的信息来检测不可靠的预测，并我们展示它们可以用于核查LLM输出中的各种声明。此外，我们提出了一种新型的标记级别不确定性量化方法，消除了对事实提出怀疑的影响。

    arXiv:2403.04696v1 Announce Type: cross  Abstract: Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output. Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factual, making it extremely hard for the users to spot them. Current services that leverage LLMs usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what c
    
[^13]: 自然语言处理竞赛中系统性能分析

    Analysis of Systems' Performance in Natural Language Processing Competitions

    [https://arxiv.org/abs/2403.04693](https://arxiv.org/abs/2403.04693)

    本文描述了一种用于统计分析自然语言处理竞赛结果的评估方法，通过八个竞赛案例研究展示了其普适性。

    

    合作竞赛在科学技术领域变得越来越受欢迎。这些竞赛涉及定义任务，选择评估分数和设计结果验证方法。参与者通常会收到一个训练集，并被要求为主办方保留的一个未公开数据集提供解决方案。本文描述了一种用于统计分析竞赛结果和竞争的评估方法。这种方法被设计成通用的，然而，在本文中使用了八个自然语言处理竞赛作为案例研究，涉及分类和回归。

    arXiv:2403.04693v1 Announce Type: new  Abstract: Collaborative competitions have gained popularity in the scientific and technological fields. These competitions involve defining tasks, selecting evaluation scores, and devising result verification methods. In the standard scenario, participants receive a training set and are expected to provide a solution for a held-out dataset kept by organizers. An essential challenge for organizers arises when comparing algorithms' performance, assessing multiple participants, and ranking them. Statistical tools are often used for this purpose; however, traditional statistical methods often fail to capture decisive differences between systems' performance. This manuscript describes an evaluation methodology for statistically analyzing competition results and competition. The methodology is designed to be universally applicable; however, it is illustrated using eight natural language competitions as case studies involving classification and regressio
    
[^14]: 更快的邻域注意力: 在线程块级别减少自注意力的O(n^2)成本

    Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level

    [https://arxiv.org/abs/2403.04690](https://arxiv.org/abs/2403.04690)

    该研究提出了一种更快的邻域注意力机制，通过将注意力限制在最近的邻居之间来降低自注意力的计算复杂度，实现了显著的性能提升。

    

    邻域注意力通过限制每个标记的注意力范围为其最近的邻居来降低自注意力的成本。该限制由窗口大小和扩张因子参数化，介于线性投影和自注意力之间绘制了可能的注意力模式谱。邻域注意力，以及更一般地滑动窗口注意力模式，在基础设施方面长期受到限制，特别是在更高秩的空间（2-D和3-D），促使开发定制内核的发展，这些内核在功能或性能方面受限，如果不是两者都有。在这项工作中，我们首先展示邻域注意力可以表示为批量化的GEMM问题，类似于标准注意力，并为1-D和2-D邻域注意力实现它。与现有的简单内核相比，这些内核平均提供了分别是1-D和2-D邻域注意力的全精度延迟改进分别为895%和272%。

    arXiv:2403.04690v1 Announce Type: cross  Abstract: Neighborhood attention reduces the cost of self attention by restricting each token's attention span to its nearest neighbors. This restriction, parameterized by a window size and dilation factor, draws a spectrum of possible attention patterns between linear projection and self attention. Neighborhood attention, and more generally sliding window attention patterns, have long been bounded by infrastructure, particularly in higher-rank spaces (2-D and 3-D), calling for the development of custom kernels, which have been limited in either functionality, or performance, if not both. In this work, we first show that neighborhood attention can be represented as a batched GEMM problem, similar to standard attention, and implement it for 1-D and 2-D neighborhood attention. These kernels on average provide 895% and 272% improvement in full precision latency compared to existing naive kernels for 1-D and 2-D neighborhood attention respectively. 
    
[^15]: 端到端条件鲁棒优化

    End-to-end Conditional Robust Optimization

    [https://arxiv.org/abs/2403.04670](https://arxiv.org/abs/2403.04670)

    提出了一种新颖的端到端方法，利用现代可微分优化方法训练CRO模型，以同时考虑所规定决策的经验风险和支持它们的语境不确定性集合的条件覆盖质量

    

    语境优化（CO）领域整合了机器学习和优化以解决不确定性下的决策问题。最近，一种风险敏感的CO变体，称为条件鲁棒优化（CRO），结合了不确定性量化和鲁棒优化，以在高风险应用中促进安全性和可靠性。利用现代可微分优化方法，我们提出了一种新颖的端到端方法来训练CRO模型，以考虑所规定决策的经验风险和支持它们的语境不确定性集合的条件覆盖质量。尽管从合规预测理论的角度来看，无法获得对后者目标成功的保证，但通过巧妙地在覆盖质量的计算中使用逻辑回归可微分层，可以在经验上实现高质量的条件覆盖。

    arXiv:2403.04670v1 Announce Type: new  Abstract: The field of Contextual Optimization (CO) integrates machine learning and optimization to solve decision making problems under uncertainty. Recently, a risk sensitive variant of CO, known as Conditional Robust Optimization (CRO), combines uncertainty quantification with robust optimization in order to promote safety and reliability in high stake applications. Exploiting modern differentiable optimization methods, we propose a novel end-to-end approach to train a CRO model in a way that accounts for both the empirical risk of the prescribed decisions and the quality of conditional coverage of the contextual uncertainty set that supports them. While guarantees of success for the latter objective are impossible to obtain from the point of view of conformal prediction theory, high quality conditional coverage is achieved empirically by ingeniously employing a logistic regression differentiable layer within the calculation of coverage quality
    
[^16]: 电信语言模型：它们必须庞大吗？

    Telecom Language Models: Must They Be Large?

    [https://arxiv.org/abs/2403.04666](https://arxiv.org/abs/2403.04666)

    小型语言模型Phi-2在电信领域展示出与大型对应模型相媲美的性能，通过检索增强生成方法提升了其能力。

    

    电信部门对庞大语言模型（LLMs）的日益关注凸显了它们在改变运营效率方面的潜力。然而，部署这些复杂模型往往受到其巨大体积和计算需求的影响，引发了对它们在资源受限环境中可行性的担忧。为了解决这一挑战，最近的进展出现了一批小型语言模型，令人惊讶的是它们在许多任务中表现与其较大对应物相当，比如编码和常识推理。Phi-2是一种紧凑但功能强大的模型，它体现了这一系列高效小型语言模型的新浪潮。本文对Phi-2在电信领域内在本质上的理解进行了全面评估。鉴于规模相关限制，我们通过检索增强生成方法，精心增强了Phi-2的能力。

    arXiv:2403.04666v1 Announce Type: new  Abstract: The increasing interest in Large Language Models (LLMs) within the telecommunications sector underscores their potential to revolutionize operational efficiency. However, the deployment of these sophisticated models is often hampered by their substantial size and computational demands, raising concerns about their viability in resource-constrained environments. Addressing this challenge, recent advancements have seen the emergence of small language models that surprisingly exhibit performance comparable to their larger counterparts in many tasks, such as coding and common-sense reasoning. Phi-2, a compact yet powerful model, exemplifies this new wave of efficient small language models. This paper conducts a comprehensive evaluation of Phi-2's intrinsic understanding of the telecommunications domain. Recognizing the scale-related limitations, we enhance Phi-2's capabilities through a Retrieval-Augmented Generation approach, meticulously i
    
[^17]: 面向音频-视频人员验证的动态交叉注意力

    Dynamic Cross Attention for Audio-Visual Person Verification

    [https://arxiv.org/abs/2403.04661](https://arxiv.org/abs/2403.04661)

    提出了一种动态交叉注意力（DCA）模型，根据音频和视频模态之间的强弱互补关系，动态选择交叉关注或不关注的特征。

    

    尽管人员或身份验证通常使用个体模态（如面部和声音）进行探索，但最近显示出巨大潜力的音视频融合方法可以胜过单模态方法。音频和视觉模态通常被期望具有强烈的互补关系，在有效的音视频融合中起着至关重要的作用。然而，它们并不总是强烈相互补充，它们也可能展现出弱的互补关系，导致音视频特征表示不佳。本文提出了一种动态交叉注意力（DCA）模型，可以根据跨音频和视觉模态之间的强弱互补关系，动态选择交叉关注或不关注的特征。特别地，设计了一个条件门控层来评估交叉注意力机制的贡献，并仅选择跨模态关注的特征。

    arXiv:2403.04661v1 Announce Type: cross  Abstract: Although person or identity verification has been predominantly explored using individual modalities such as face and voice, audio-visual fusion has recently shown immense potential to outperform unimodal approaches. Audio and visual modalities are often expected to pose strong complementary relationships, which plays a crucial role in effective audio-visual fusion. However, they may not always strongly complement each other, they may also exhibit weak complementary relationships, resulting in poor audio-visual feature representations. In this paper, we propose a Dynamic Cross-Attention (DCA) model that can dynamically select the cross-attended or unattended features on the fly based on the strong or weak complementary relationships, respectively, across audio and visual modalities. In particular, a conditional gating layer is designed to evaluate the contribution of the cross-attention mechanism and choose cross-attended features only
    
[^18]: 基于上下文的多模态融合

    Context-Based Multimodal Fusion

    [https://arxiv.org/abs/2403.04650](https://arxiv.org/abs/2403.04650)

    提出一种基于上下文的多模态融合模型，结合了模态融合和数据分布对齐，通过特定上下文向量表示每个模态，并将其与每个模态的嵌入进行融合，

    

    融合模型广泛应用于解决多模态任务，但在不同模态之间数据分布对齐方面存在明显局限性。针对这一挑战，我们提出了一种创新模型称为基于上下文的多模态融合（CBMF），结合了模态融合和数据分布对齐，通过特定上下文向量表示每个模态，并将其与每个模态的嵌入进行融合。

    arXiv:2403.04650v1 Announce Type: cross  Abstract: The fusion models, which effectively combine information from different sources, are widely used in solving multimodal tasks. However, they have significant limitations related to aligning data distributions across different modalities. This challenge can lead to inconsistencies and difficulties in learning robust representations. Alignment models, while specifically addressing this issue, often require training "from scratch" with large datasets to achieve optimal results, which can be costly in terms of resources and time. To overcome these limitations, we propose an innovative model called Context-Based Multimodal Fusion (CBMF), which combines both modality fusion and data distribution alignment. In CBMF, each modality is represented by a specific context vector, fused with the embedding of each modality. This enables the use of large pre-trained models that can be frozen, reducing the computational and training data requirements. A
    
[^19]: 通过强化学习教授大型语言模型进行推理

    Teaching Large Language Models to Reason with Reinforcement Learning

    [https://arxiv.org/abs/2403.04642](https://arxiv.org/abs/2403.04642)

    强化学习从人类反馈中学习已成为将LLM输出与人类偏好对齐的主要方法，研究了多种算法在提升LLM推理能力方面的表现，发现在大多数情况下专家迭代效果最佳，且其样本复杂度与PPO类似。

    

    强化学习从人类反馈中学习（\textbf{RLHF}）已经成为将LLM输出与人类偏好对齐的主要方法。受RLHF成功启发，我们研究了多种从反馈中学习的算法（专家迭代、近端策略优化（\textbf{PPO}）、有条件回报的RL）在提升LLM推理能力方面的表现。我们研究了启发式和通过学习奖励模型提供给LLM的稀疏和稠密奖励。此外，我们从多个模型大小和初始化开始，包括有和没有监督微调（\textbf{SFT}）数据。总体而言，我们发现所有算法的表现基本相当，专家迭代在大多数情况下表现最佳。令人惊讶的是，我们发现专家迭代的样本复杂度与PPO相似，从预训练检查点收敛需要最多大约$10^6$个样本。我们探讨了这种情况的原因，

    arXiv:2403.04642v1 Announce Type: new  Abstract: Reinforcement Learning from Human Feedback (\textbf{RLHF}) has emerged as a dominant approach for aligning LLM outputs with human preferences. Inspired by the success of RLHF, we study the performance of multiple algorithms that learn from feedback (Expert Iteration, Proximal Policy Optimization (\textbf{PPO}), Return-Conditioned RL) on improving LLM reasoning capabilities. We investigate both sparse and dense rewards provided to the LLM both heuristically and via a learned reward model. We additionally start from multiple model sizes and initializations both with and without supervised fine-tuning (\textbf{SFT}) data. Overall, we find all algorithms perform comparably, with Expert Iteration performing best in most cases. Surprisingly, we find the sample complexity of Expert Iteration is similar to that of PPO, requiring at most on the order of $10^6$ samples to converge from a pretrained checkpoint. We investigate why this is the case, 
    
[^20]: 图神经网络中的熵感知消息传递

    Entropy Aware Message Passing in Graph Neural Networks

    [https://arxiv.org/abs/2403.04636](https://arxiv.org/abs/2403.04636)

    该论文提出了一种新颖的物理启发的图神经网络模型，通过引入熵感知消息传递项来缓解深度图神经网络中的过度平滑问题。

    

    深度图神经网络在过度平滑方面存在困难。本文引入了一种新颖的、受物理启发的GNN模型，旨在缓解这一问题。我们的方法与现有的GNN架构集成，引入了一个熵感知消息传递项。该项在节点聚合过程中对熵进行梯度上升，从而在嵌入中保留一定程度的熵。我们对我们的模型在各种常见数据集上与最先进的GNN进行了对比分析。

    arXiv:2403.04636v1 Announce Type: new  Abstract: Deep Graph Neural Networks struggle with oversmoothing. This paper introduces a novel, physics-inspired GNN model designed to mitigate this issue. Our approach integrates with existing GNN architectures, introducing an entropy-aware message passing term. This term performs gradient ascent on the entropy during node aggregation, thereby preserving a certain degree of entropy in the embeddings. We conduct a comparative analysis of our model against state-of-the-art GNNs across various common datasets.
    
[^21]: 用Shapley值解释贝叶斯优化促进人工智能与人类协作

    Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI Collaboration

    [https://arxiv.org/abs/2403.04629](https://arxiv.org/abs/2403.04629)

    提出了ShapleyBO框架，用Shapley值解释贝叶斯优化提议，量化每个参数对于优化过程的贡献，并能够区分不同类型的不确定性探索贡献。

    

    贝叶斯优化（BO）与高斯过程（GP）已成为解决黑匣子优化问题的不可或缺的算法。然而，BO本身也常常被认为是一个黑匣子，缺乏提供为何提议评估某些参数的理由的方法。我们通过提出ShapleyBO来解决这个问题，这是一个用博弈论Shapley值解释BO提议的框架。它量化了每个参数对BO的收获函数的贡献。利用Shapley值的线性性，我们能够进一步确定每个参数对于像置信边界这样的加法收获函数推动BO的探索和开发的强度。我们还展示了ShapleyBO能够解决探索对于勘探aleatoric和认识epistemic不确定性的贡献。

    arXiv:2403.04629v1 Announce Type: cross  Abstract: Bayesian optimization (BO) with Gaussian processes (GP) has become an indispensable algorithm for black box optimization problems. Not without a dash of irony, BO is often considered a black box itself, lacking ways to provide reasons as to why certain parameters are proposed to be evaluated. This is particularly relevant in human-in-the-loop applications of BO, such as in robotics. We address this issue by proposing ShapleyBO, a framework for interpreting BO's proposals by game-theoretic Shapley values.They quantify each parameter's contribution to BO's acquisition function. Exploiting the linearity of Shapley values, we are further able to identify how strongly each parameter drives BO's exploration and exploitation for additive acquisition functions like the confidence bound. We also show that ShapleyBO can disentangle the contributions to exploration into those that explore aleatoric and epistemic uncertainty. Moreover, our method 
    
[^22]: MedFLIP：医学视觉与语言自监督快速预训练与掩蔽自编码器

    MedFLIP: Medical Vision-and-Language Self-supervised Fast Pre-Training with Masked Autoencoder

    [https://arxiv.org/abs/2403.04626](https://arxiv.org/abs/2403.04626)

    MedFLIP是一种用于医学分析的快速语言-图像预训练方法，通过引入SVD损失增强医学图像特征表示学习，验证了用语言可以提高零样本医学图像分析的性能。

    

    在医学分析领域，广泛的研究探讨了掩蔽自编码器（MAEs）和多模态数据之间互相学习的潜力。然而，MAEs对跨模态学习的影响仍然是一个关键挑战。我们引入了MedFLIP，一种用于医学分析的快速语言-图像预训练方法。我们探索使用MAEs进行跨领域零样本学习，从而增强模型在医学诊断中常见的有限数据中学习的能力。我们验证了对图像进行掩蔽不会影响跨模态学习。此外，我们提出了SVD损失以增强医学图像特征的表示学习，旨在通过利用这类数据的结构复杂性来提高分类准确性。最后，我们验证了使用语言将提高医学图像分析的零样本性能。MedFLIP对掩蔽过程的扩展标志着该领域的进步。

    arXiv:2403.04626v1 Announce Type: cross  Abstract: Within the domain of medical analysis, extensive research has explored the potential of mutual learning between Masked Autoencoders(MAEs) and multimodal data. However, the impact of MAEs on intermodality remains a key challenge. We introduce MedFLIP, a Fast Language-Image Pre-training method for Medical analysis. We explore MAEs for zero-shot learning with crossed domains, which enhances the model ability to learn from limited data, a common scenario in medical diagnostics. We verify that masking an image does not affect intermodal learning. Furthermore, we propose the SVD loss to enhance the representation learning for characteristics of medical images, aiming to improve classification accuracy by leveraging the structural intricacies of such data. Lastly, we validate using language will improve the zero-shot performance for the medical image analysis. MedFLIP scaling of the masking process marks an advancement in the field, offering 
    
[^23]: In-n-Out: 用于链接预测的图神经网络校准

    In-n-Out: Calibrating Graph Neural Networks for Link Prediction

    [https://arxiv.org/abs/2403.04605](https://arxiv.org/abs/2403.04605)

    IN-N-OUT提出了第一个用于链接预测的图神经网络校准方法，基于对GNN校准偏差的观察，通过简单直觉实现校准

    

    深度神经网络通常存在校准偏差，即它们的输出不能反映我们打算预测的事件的真实概率。我们展示了在链接预测中，图神经网络通常表现出混合的行为，即在负预测上可能过于自信，在正预测上可能不够自信。基于这一观察，我们提出了IN-N-OUT，这是第一个用于链接预测的校准图神经网络的方法。IN-N-OUT基于两个简单的直觉：i) 给边标注真/假标签，同时遵循GNN的预测应导致该边嵌入的微小波动；ii) 相反地，如果我们标记相同的边与我们的GNN预测相悖，那么嵌入应该发生更大的变化。

    arXiv:2403.04605v1 Announce Type: new  Abstract: Deep neural networks are notoriously miscalibrated, i.e., their outputs do not reflect the true probability of the event we aim to predict. While networks for tabular or image data are usually overconfident, recent works have shown that graph neural networks (GNNs) show the opposite behavior for node-level classification. But what happens when we are predicting links? We show that, in this case, GNNs often exhibit a mixed behavior. More specifically, they may be overconfident in negative predictions while being underconfident in positive ones. Based on this observation, we propose IN-N-OUT, the first-ever method to calibrate GNNs for link prediction. IN-N-OUT is based on two simple intuitions: i) attributing true/false labels to an edge while respecting a GNNs prediction should cause but small fluctuations in that edge's embedding; and, conversely, ii) if we label that same edge contradicting our GNN, embeddings should change more substa
    
[^24]: 具有重要性采样和原型实例关系蒸馏的对比持续学习

    Contrastive Continual Learning with Importance Sampling and Prototype-Instance Relation Distillation

    [https://arxiv.org/abs/2403.04599](https://arxiv.org/abs/2403.04599)

    提出了Contrastive Continual Learning via Importance Sampling (CCLIS)以保留知识，其中通过重播缓冲区选择（RBS）策略恢复先前的数据分布，最小化估计方差以保存高质量的负样本，同时引入了原型-实例关系蒸馏（PRD）损失来维护原型和样本表示之间的关系。

    

    最近，由于对比学习方法的高质量表示，提出了基于重复训练的对比持续学习，以探索如何持续学习可传递的表示嵌入，避免传统持续设置中的灾难性遗忘问题。基于这一框架，我们提出了通过重要性采样进行对比持续学习（CCLIS），通过一种新的重播缓冲区选择（RBS）策略来恢复先前的数据分布，最小化估计方差以保存高质量的用于表示学习的难负样本。此外，我们提出了原型-实例关系蒸馏（PRD）损失，一种通过自蒸馏过程来维护原型和样本表示之间关系的技术。在标准持续学习基准上的实验表明，我们的方法明显优于现有方法。

    arXiv:2403.04599v1 Announce Type: new  Abstract: Recently, because of the high-quality representations of contrastive learning methods, rehearsal-based contrastive continual learning has been proposed to explore how to continually learn transferable representation embeddings to avoid the catastrophic forgetting issue in traditional continual settings. Based on this framework, we propose Contrastive Continual Learning via Importance Sampling (CCLIS) to preserve knowledge by recovering previous data distributions with a new strategy for Replay Buffer Selection (RBS), which minimize estimated variance to save hard negative samples for representation learning with high quality. Furthermore, we present the Prototype-instance Relation Distillation (PRD) loss, a technique designed to maintain the relationship between prototypes and sample representations using a self-distillation process. Experiments on standard continual learning benchmarks reveal that our method notably outperforms existing
    
[^25]: 在杂乱环境中学习飞行敏捷性调整

    Learning Agility Adaptation for Flight in Clutter

    [https://arxiv.org/abs/2403.04586](https://arxiv.org/abs/2403.04586)

    本文旨在使飞行器在未知且部分可观测的杂乱环境中具有敏捷性调整能力，提出了一种利用分层学习和规划框架的方法，通过在线无模型强化学习和预训练微调奖励方案获得可部署的策略，在仿真中显示出比恒定敏捷性基线和另一种替代方法更优越的飞行效率和安全性。

    

    动物学习适应其运动能力和操作环境的敏捷性。移动机器人也应展示这种能力，将敏捷性和安全性结合起来。本文旨在赋予飞行器在未知且部分可观测的杂乱环境中适应敏捷性的能力。我们提出了一种分层学习和规划框架，结合试错学习和基于模型的轨迹生成方法来全面学习敏捷性策略。我们使用在线无模型强化学习和预训练微调奖励方案来获得可部署的策略。在仿真中的统计结果显示，相较于恒定敏捷性基线和另一种替代方法，我们的方法在飞行效率和安全性方面具有优势。特别是，该策略导致

    arXiv:2403.04586v1 Announce Type: cross  Abstract: Animals learn to adapt agility of their movements to their capabilities and the environment they operate in. Mobile robots should also demonstrate this ability to combine agility and safety. The aim of this work is to endow flight vehicles with the ability of agility adaptation in prior unknown and partially observable cluttered environments. We propose a hierarchical learning and planning framework where we utilize both trial and error to comprehensively learn an agility policy with the vehicle's observation as the input, and well-established methods of model-based trajectory generation. Technically, we use online model-free reinforcement learning and a pre-training-fine-tuning reward scheme to obtain the deployable policy. The statistical results in simulation demonstrate the advantages of our method over the constant agility baselines and an alternative method in terms of flight efficiency and safety. In particular, the policy leads
    
[^26]: 超越主要产物预测：使用大规模机制数据集训练的机器学习模型复现反应机理

    Beyond Major Product Prediction: Reproducing Reaction Mechanisms with Machine Learning Models Trained on a Large-Scale Mechanistic Dataset

    [https://arxiv.org/abs/2403.04580](https://arxiv.org/abs/2403.04580)

    该研究使用大规模机制数据集和机器学习模型成功复现有机反应机理，展示了预测反应途径和杂质的潜力。

    

    有机反应的机理理解可以促进反应开发、杂质预测，并且从原理上讲，有助于反应的发现。本研究通过使用专家反应模板在实验报告的反应物和产物之间插值出中间体，构建了一个包含5,184,184个基本步骤的数据集，并在该数据集上训练了几种机器学习模型。我们探讨了这些模型的性能和能力，着重于它们预测反应途径并重现催化剂和试剂的作用。此外，我们展示了机制模型在预测杂质方面的潜力，这在传统模型中经常被忽视。我们通过评估

    arXiv:2403.04580v1 Announce Type: new  Abstract: Mechanistic understanding of organic reactions can facilitate reaction development, impurity prediction, and in principle, reaction discovery. While several machine learning models have sought to address the task of predicting reaction products, their extension to predicting reaction mechanisms has been impeded by the lack of a corresponding mechanistic dataset. In this study, we construct such a dataset by imputing intermediates between experimentally reported reactants and products using expert reaction templates and train several machine learning models on the resulting dataset of 5,184,184 elementary steps. We explore the performance and capabilities of these models, focusing on their ability to predict reaction pathways and recapitulate the roles of catalysts and reagents. Additionally, we demonstrate the potential of mechanistic models in predicting impurities, often overlooked by conventional models. We conclude by evaluating the 
    
[^27]: 改进的算法用于带有匪夷所思反馈和未知转移的敌对线性混合MDPs

    Improved Algorithm for Adversarial Linear Mixture MDPs with Bandit Feedback and Unknown Transition

    [https://arxiv.org/abs/2403.04568](https://arxiv.org/abs/2403.04568)

    通过引入新的最小二乘估计器和self-normalized技术，我们提出了一个新算法，显著改进了线性混合MDPs中的遗憾上界。

    

    我们研究具有线性函数逼近、未知转移和在匪夷所思反馈设置中的敌对损失的强化学习。具体而言，我们专注于转移核为线性混合模型的线性混合MDPs。我们提出了一种新算法，该算法在高概率下达到了$\widetilde{O}(d\sqrt{HS^3K} + \sqrt{HSAK})$的遗憾值，其中$d$是特征映射的维度，$S$是状态空间的大小，$A$是动作空间的大小，$H$是每集长度，$K$是集数。我们的结果严格改进了Zhao等人(2023a)中已知的最佳$\widetilde{O}(dS^2 \sqrt{K} + \sqrt{HSAK})$结果，因为$H \leq S$由层次MDP结构成立。我们的进展主要归因于(i)一种新的最小二乘估计器，用于转移参数，利用了所有状态的访问信息，而不像以前的工作只用一个状态，以及(ii)一种新的self-normalized分。。

    arXiv:2403.04568v1 Announce Type: new  Abstract: We study reinforcement learning with linear function approximation, unknown transition, and adversarial losses in the bandit feedback setting. Specifically, we focus on linear mixture MDPs whose transition kernel is a linear mixture model. We propose a new algorithm that attains an $\widetilde{O}(d\sqrt{HS^3K} + \sqrt{HSAK})$ regret with high probability, where $d$ is the dimension of feature mappings, $S$ is the size of state space, $A$ is the size of action space, $H$ is the episode length and $K$ is the number of episodes. Our result strictly improves the previous best-known $\widetilde{O}(dS^2 \sqrt{K} + \sqrt{HSAK})$ result in Zhao et al. (2023a) since $H \leq S$ holds by the layered MDP structure. Our advancements are primarily attributed to (i) a new least square estimator for the transition parameter that leverages the visit information of all states, as opposed to only one state in prior work, and (ii) a new self-normalized conc
    
[^28]: 减少自监督学习复杂性改善计算病理学中的弱监督分类性能

    Reducing self-supervised learning complexity improves weakly-supervised classification performance in computational pathology

    [https://arxiv.org/abs/2403.04558](https://arxiv.org/abs/2403.04558)

    本研究探讨了在计算病理学中减少对比自监督学习复杂性对分类性能的改善，通过利用消费级硬件。

    

    深度学习模型已成功应用于从常规可用的组织学数据中提取临床可操作见解。通常，这些模型需要临床医生进行的标注，这种标注稀缺且昂贵。自监督学习（SSL）方法的出现消除了这一障碍，允许对非标注数据进行大规模分析。然而，最近的SSL方法采用日益庞大的模型架构和更大的数据集，导致数据量迅速增加，硬件要求和整体成本增加，使得很少机构能够获得这些资源。因此，我们研究了对比自监督学习在计算病理学中的复杂性与分类性能之间的关系，利用消费级硬件。具体而言，我们分析了数据量、架构和算法的调整对下游分类任务的影响。

    arXiv:2403.04558v1 Announce Type: cross  Abstract: Deep Learning models have been successfully utilized to extract clinically actionable insights from routinely available histology data. Generally, these models require annotations performed by clinicians, which are scarce and costly to generate. The emergence of self-supervised learning (SSL) methods remove this barrier, allowing for large-scale analyses on non-annotated data. However, recent SSL approaches apply increasingly expansive model architectures and larger datasets, causing the rapid escalation of data volumes, hardware prerequisites, and overall expenses, limiting access to these resources to few institutions. Therefore, we investigated the complexity of contrastive SSL in computational pathology in relation to classification performance with the utilization of consumer-grade hardware. Specifically, we analyzed the effects of adaptations in data volume, architecture, and algorithms on downstream clas- sification tasks, empha
    
[^29]: MLCommons CloudMask基准测试的改进与评估

    Improvements & Evaluations on the MLCommons CloudMask Benchmark

    [https://arxiv.org/abs/2403.04553](https://arxiv.org/abs/2403.04553)

    本文报告了在MLCommons CloudMask基准测试上使用深度学习模型的性能基准测试结果，包括最佳模型、最高准确性和平均运行时间。

    

    在本文中，我们报告了在纽约大学（NYU）的高性能计算集群NYU Greene上对MLCommons科学云遮蔽基准测试上使用深度学习模型的性能基准测试结果。MLCommons是一个开发和维护几个科学基准测试的联盟，可以从人工智能的发展中受益。我们提供了云遮蔽基准测试任务的描述，更新的代码以及在使用我们选择的超参数设置时针对该基准测试的最佳模型。我们的基准测试结果包括在NYU系统上实现的最高准确性，以及在多次运行/种子中进行训练和推理所需的平均时间。我们的代码可以在GitHub上找到。MLCommons团队已经了解到我们的进展，并可能在他们的未来工作中使用开发的代码。

    arXiv:2403.04553v1 Announce Type: cross  Abstract: In this paper, we report the performance benchmarking results of deep learning models on MLCommons' Science cloud-masking benchmark using a high-performance computing cluster at New York University (NYU): NYU Greene. MLCommons is a consortium that develops and maintains several scientific benchmarks that can benefit from developments in AI. We provide a description of the cloud-masking benchmark task, updated code, and the best model for this benchmark when using our selected hyperparameter settings. Our benchmarking results include the highest accuracy achieved on the NYU system as well as the average time taken for both training and inference on the benchmark across several runs/seeds. Our code can be found on GitHub. MLCommons team has been kept informed about our progress and may use the developed code for their future work.
    
[^30]: 细究样本难度: 针对数据中心人工智能的难度表征方法的精细分析

    Dissecting Sample Hardness: A Fine-Grained Analysis of Hardness Characterization Methods for Data-Centric AI

    [https://arxiv.org/abs/2403.04551](https://arxiv.org/abs/2403.04551)

    该论文提出了一个细致的难度类型分类系统，并引入了难度表征分析工具包（H-CAT），支持全面和定量评估不同难度类型下的HCMs，从而填补了当前对“难度”定义和评估的定量识别任务的空白。

    

    对于难以学习的样本进行表征对于开发高性能机器学习模型至关重要。这导致了许多旨在识别“难”样本的难度表征方法（HCMs）。然而，关于“难度”的定义和评估缺乏共识。不幸的是，当前的HCMs仅在特定类型的难度上进行评估，通常仅在质量或下游性能方面定性地评估，忽视了基本的定量识别任务。我们通过提出一个细粒度的难度类型分类来解决这一问题。此外，我们提出了难度表征分析工具包（H-CAT），支持对难度分类下的HCMs进行全面和定量的基准评估，并可以轻松扩展到新的HCMs、难度类型和数据集。我们使用H-CAT来评估8种难度类型中的13种不同HCMs。

    arXiv:2403.04551v1 Announce Type: new  Abstract: Characterizing samples that are difficult to learn from is crucial to developing highly performant ML models. This has led to numerous Hardness Characterization Methods (HCMs) that aim to identify "hard" samples. However, there is a lack of consensus regarding the definition and evaluation of "hardness". Unfortunately, current HCMs have only been evaluated on specific types of hardness and often only qualitatively or with respect to downstream performance, overlooking the fundamental quantitative identification task. We address this gap by presenting a fine-grained taxonomy of hardness types. Additionally, we propose the Hardness Characterization Analysis Toolkit (H-CAT), which supports comprehensive and quantitative benchmarking of HCMs across the hardness taxonomy and can easily be extended to new HCMs, hardness types, and datasets. We use H-CAT to evaluate 13 different HCMs across 8 hardness types. This comprehensive evaluation encomp
    
[^31]: CLIP去偏见：在多模态学习中平衡数据有多大用处？

    CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?

    [https://arxiv.org/abs/2403.04547](https://arxiv.org/abs/2403.04547)

    数据平衡在对比语言-图像预训练（CLIP）中可以部分改善偏见问题，然而会对质量产生复杂影响。

    

    我们研究了数据平衡对于减轻对比语言-图像预训练（CLIP）中的偏见的有效性，确定了其优势和局限性。首先，我们重申了以前的结论，即CLIP模型可能会无意中吸收社会刻板印象。为了应对这一问题，我们提出了一种新的算法，称为多模态时刻匹配（M4），旨在减少多模态数据中的表示和关联偏见（即一阶和二阶统计）。我们使用M4进行了深入分析，考虑了模型、表示和数据大小等各种因素。我们的研究还探讨了CLIP学习和消除偏见的动态特性。特别是，我们发现微调可以有效地抵制表示偏见，但对关联偏见的影响逐渐减弱。此外，数据平衡对质量有着复杂的影响：它倾向于改善分类，但可能会损害检索。

    arXiv:2403.04547v1 Announce Type: cross  Abstract: We study the effectiveness of data-balancing for mitigating biases in contrastive language-image pretraining (CLIP), identifying areas of strength and limitation. First, we reaffirm prior conclusions that CLIP models can inadvertently absorb societal stereotypes. To counter this, we present a novel algorithm, called Multi-Modal Moment Matching (M4), designed to reduce both representation and association biases (i.e. in first- and second-order statistics) in multimodal data. We use M4 to conduct an in-depth analysis taking into account various factors, such as the model, representation, and data size. Our study also explores the dynamic nature of how CLIP learns and unlearns biases. In particular, we find that fine-tuning is effective in countering representation biases, though its impact diminishes for association biases. Also, data balancing has a mixed impact on quality: it tends to improve classification but can hurt retrieval. Inte
    
[^32]: 面向异构鲁棒联邦学习的架构蓝图

    Architectural Blueprint For Heterogeneity-Resilient Federated Learning

    [https://arxiv.org/abs/2403.04546](https://arxiv.org/abs/2403.04546)

    提出了一个三层架构用于优化边缘计算环境中的联邦学习，解决了客户端数据异构性和计算约束挑战，提高了非独立同分布数据集管理效率，显著改善了模型准确性，减少了通信开销，促进了联邦学习技术更广泛的应用。

    

    本文提出了一种新颖的三层架构用于优化边缘计算环境中的联邦学习。该架构解决了与客户端数据异构性和计算约束相关的挑战。它引入了一个可扩展的、隐私保护的框架，增强了分布式机器学习的效率。通过实验，本文展示了该架构相对于传统联邦学习模型更有效地管理非独立同分布数据集的能力。此外，本文强调了这种创新方法显著提高模型准确性、减少通信开销，并促进联邦学习技术的更广泛应用的潜力。

    arXiv:2403.04546v1 Announce Type: new  Abstract: This paper proposes a novel three tier architecture for federated learning to optimize edge computing environments. The proposed architecture addresses the challenges associated with client data heterogeneity and computational constraints. It introduces a scalable, privacy preserving framework that enhances the efficiency of distributed machine learning. Through experimentation, the paper demonstrates the architecture capability to manage non IID data sets more effectively than traditional federated learning models. Additionally, the paper highlights the potential of this innovative approach to significantly improve model accuracy, reduce communication overhead, and facilitate broader adoption of federated learning technologies.
    
[^33]: 通过合适的缩放因子提高深广残差网络的泛化能力

    Improve Generalization Ability of Deep Wide Residual Network with A Suitable Scaling Factor

    [https://arxiv.org/abs/2403.04545](https://arxiv.org/abs/2403.04545)

    通过在深广残差网络中使用适当的缩放因子，可以提高泛化能力，即使允许缩放因子随深度减小，也可以实现最小最大速率。

    

    深残差神经网络（ResNets）在广泛的实际应用中取得了显著的成功。在本文中，我们确定了深广残差网络中残差分支上的合适缩放因子（用$\alpha$表示），以实现良好的泛化能力。我们展示了如果$\alpha$是一个常数，由残差神经切向核（RNTK）引发的函数类在深度趋近无穷时是渐近不可学习的。我们还强调了一个令人惊讶的现象：即使我们允许$\alpha$随着深度$L$的增加而减小，退化现象仍可能发生。然而，当$\alpha$与$L$快速减小时，使用深层RNTK进行核回归，并且在早停条件下可以实现最小最大速率，前提是目标回归函数落在与无限深度RNTK相关的再生核希尔伯特空间中。我们对合成数据和真实分类任务进行了模拟研究。

    arXiv:2403.04545v1 Announce Type: new  Abstract: Deep Residual Neural Networks (ResNets) have demonstrated remarkable success across a wide range of real-world applications. In this paper, we identify a suitable scaling factor (denoted by $\alpha$) on the residual branch of deep wide ResNets to achieve good generalization ability. We show that if $\alpha$ is a constant, the class of functions induced by Residual Neural Tangent Kernel (RNTK) is asymptotically not learnable, as the depth goes to infinity. We also highlight a surprising phenomenon: even if we allow $\alpha$ to decrease with increasing depth $L$, the degeneration phenomenon may still occur. However, when $\alpha$ decreases rapidly with $L$, the kernel regression with deep RNTK with early stopping can achieve the minimax rate provided that the target regression function falls in the reproducing kernel Hilbert space associated with the infinite-depth RNTK. Our simulation studies on synthetic data and real classification task
    
[^34]: 在联邦微调基础模型中提升数据质量

    Enhancing Data Quality in Federated Fine-Tuning of Foundation Models

    [https://arxiv.org/abs/2403.04529](https://arxiv.org/abs/2403.04529)

    提出了一个用于联邦微调基础模型的数据质量控制管道，可以提高全局性能。

    

    基础模型训练的当前情景中，存在着对公共领域数据的显著依赖，而根据最近的研究，这些数据已接近枯竭。为了进一步扩大规模，将多个专业化和高质量的私有领域数据源进行协作是至关重要的。然而，在不共享私有数据的情况下地方性训练模型所面临的数据质量控制问题是具有挑战性的。为了解决这一问题，我们提出了一个用于联邦微调基础模型的数据质量控制管道。该管道计算反映训练数据质量的分数，并确定一个全局阈值以实现统一标准，旨在提高全局性能。我们的实验表明，所提出的质量控制管道促进了模型训练的有效性和可靠性，从而带来更好的性能。

    arXiv:2403.04529v1 Announce Type: cross  Abstract: In the current landscape of foundation model training, there is a significant reliance on public domain data, which is nearing exhaustion according to recent research. To further scale up, it is crucial to incorporate collaboration among multiple specialized and high-quality private domain data sources. However, the challenge of training models locally without sharing private data presents numerous obstacles in data quality control. To tackle this issue, we propose a data quality control pipeline for federated fine-tuning of foundation models. This pipeline computes scores reflecting the quality of training data and determines a global threshold for a unified standard, aiming for improved global performance. Our experiments show that the proposed quality control pipeline facilitates the effectiveness and reliability of the model training, leading to better performance.
    
[^35]: 通过物理约束自动编码器进行拉曼光谱的高光谱解混

    Hyperspectral unmixing for Raman spectroscopy via physics-constrained autoencoders

    [https://arxiv.org/abs/2403.04526](https://arxiv.org/abs/2403.04526)

    通过物理约束自动编码器开发了高光谱解混算法，提供了在复杂混合情况下更好的准确性、鲁棒性和效率，展示了在复杂生物环境中的应用。

    

    拉曼光谱广泛应用于科学领域，以非破坏性、无标记的方式表征样品的化学组成。许多应用需要从混合分子物种的信号中解混，以识别出现的个体组分及其比例，然而传统的化学计量学方法常常难以处理实践中遇到的复杂混合情况。在这里，我们基于自动编码器神经网络开发了基于高光谱解混的算法，并使用内部创建的合成和实验基准数据集对它们进行系统验证。我们的结果表明，与标准解混方法相比，解混自动编码器提供了更高的准确性、鲁棒性和效率。我们还展示了自动编码器在复杂生物环境中的适用性，通过展示从单核细胞的体积拉曼成像数据中改善的生物化学表征。

    arXiv:2403.04526v1 Announce Type: cross  Abstract: Raman spectroscopy is widely used across scientific domains to characterize the chemical composition of samples in a non-destructive, label-free manner. Many applications entail the unmixing of signals from mixtures of molecular species to identify the individual components present and their proportions, yet conventional methods for chemometrics often struggle with complex mixture scenarios encountered in practice. Here, we develop hyperspectral unmixing algorithms based on autoencoder neural networks, and we systematically validate them using both synthetic and experimental benchmark datasets created in-house. Our results demonstrate that unmixing autoencoders provide improved accuracy, robustness and efficiency compared to standard unmixing methods. We also showcase the applicability of autoencoders to complex biological settings by showing improved biochemical characterization of volumetric Raman imaging data from a monocytic cell.
    
[^36]: T-TAME：用于解释卷积网络和视觉Transformer的可训练注意机制

    T-TAME: Trainable Attention Mechanism for Explaining Convolutional Networks and Vision Transformers

    [https://arxiv.org/abs/2403.04523](https://arxiv.org/abs/2403.04523)

    本文提出了T-TAME，一种适用于卷积网络和视觉Transformer的可训练注意机制，为解释深度神经网络在图像分类任务中的应用提供了通用方法。

    

    Vision Transformers和其他用于图像分类任务的深度学习架构的发展和应用快速增长。然而，神经网络的“黑匣子”特性是在需要解释性的应用中采用的障碍。虽然已经提出了一些用于生成解释的技术，主要用于卷积神经网络，但是将这些技术适应到视觉Transformer的新范式是非平凡的。本文提出了T-TAME，Transformer兼容的可训练注意机制用于解释，这是一种说明用于图像分类任务中的深度神经网络的通用方法。所提出的架构和训练技术可以轻松应用于任何卷积或类似Vision Transformer的神经网络，使用精简的训练方法。训练后，解释图可以在单次前向传递中计算出；这些解释图可以与Convolutional Neural Networks中生成的解释图相媲美或者

    arXiv:2403.04523v1 Announce Type: cross  Abstract: The development and adoption of Vision Transformers and other deep-learning architectures for image classification tasks has been rapid. However, the "black box" nature of neural networks is a barrier to adoption in applications where explainability is essential. While some techniques for generating explanations have been proposed, primarily for Convolutional Neural Networks, adapting such techniques to the new paradigm of Vision Transformers is non-trivial. This paper presents T-TAME, Transformer-compatible Trainable Attention Mechanism for Explanations, a general methodology for explaining deep neural networks used in image classification tasks. The proposed architecture and training technique can be easily applied to any convolutional or Vision Transformer-like neural network, using a streamlined training approach. After training, explanation maps can be computed in a single forward pass; these explanation maps are comparable to or 
    
[^37]: 使图像真实的因素是什么？

    What makes an image realistic?

    [https://arxiv.org/abs/2403.04493](https://arxiv.org/abs/2403.04493)

    论文讨论了如何设计能够可靠区分真实数据和不真实数据的函数，提出了通用评论者的概念作为一个新的解决方案。

    

    在过去的十年里，我们在生成看起来真实的数据方面取得了巨大进展，无论是图像、文本、音频还是视频。在这里，我们讨论了与之密切相关的问题，即量化现实主义，即设计能够可靠地区分真实数据和不真实数据的函数。从算法信息理论的观点出发，我们讨论了为什么这个问题很具挑战性，为什么一个好的生成模型单独不能解决它，以及一个好的解决方案应该是什么样的。特别是，我们引入了通用评论者的概念，不像对抗性评论者那样需要对抗性训练。尽管通用评论者并不立即实用，但它们既可以作为引导实际实现的北极星，也可以作为一个工具。

    arXiv:2403.04493v1 Announce Type: new  Abstract: The last decade has seen tremendous progress in our ability to generate realistic-looking data, be it images, text, audio, or video. Here, we discuss the closely related problem of quantifying realism, that is, designing functions that can reliably tell realistic data from unrealistic data. This problem turns out to be significantly harder to solve and remains poorly understood, despite its prevalence in machine learning and recent breakthroughs in generative AI. Drawing on insights from algorithmic information theory, we discuss why this problem is challenging, why a good generative model alone is insufficient to solve it, and what a good solution would look like. In particular, we introduce the notion of a universal critic, which unlike adversarial critics does not require adversarial training. While universal critics are not immediately practical, they can serve both as a North Star for guiding practical implementations and as a tool 
    
[^38]: 来源至关重要：医学成像中数据集对模型鲁棒性的影响

    Source Matters: Source Dataset Impact on Model Robustness in Medical Imaging

    [https://arxiv.org/abs/2403.04484](https://arxiv.org/abs/2403.04484)

    该研究调查了在医学成像中，源数据集的选择如何影响模型的鲁棒性，指出ImageNet预训练模型更容易过拟合混杂因素，建议研究人员重新评估模型的鲁棒性。

    

    迁移学习已成为医学成像分类算法中不可或缺的一部分，通常利用ImageNet权重。然而，从自然到医学图像的领域转变促使了诸如RadImageNet 等替代方案的出现，往往展示出可比的分类性能。然而，目前尚不清楚迁移学习中性能提升是来自于改善的泛化还是快捷学习。为了解决这个问题，我们研究了两个公开的胸部X光片和CT数据集之间的潜在混杂因素--无论是合成的还是从数据中抽取的。我们发现ImageNet 和 RadImageNet 实现了可比的分类性能，然而 ImageNet 更容易过拟合混杂因素。我们建议使用ImageNet预训练模型的研究人员通过开展类似实验来重新审视模型的鲁棒性。我们的代码和实验可在https://github.com/DovileDo/source-mat 获取。

    arXiv:2403.04484v1 Announce Type: cross  Abstract: Transfer learning has become an essential part of medical imaging classification algorithms, often leveraging ImageNet weights. However, the domain shift from natural to medical images has prompted alternatives such as RadImageNet, often demonstrating comparable classification performance. However, it remains unclear whether the performance gains from transfer learning stem from improved generalization or shortcut learning. To address this, we investigate potential confounders -- whether synthetic or sampled from the data -- across two publicly available chest X-ray and CT datasets. We show that ImageNet and RadImageNet achieve comparable classification performance, yet ImageNet is much more prone to overfitting to confounders. We recommend that researchers using ImageNet-pretrained models reexamine their model robustness by conducting similar experiments. Our code and experiments are available at https://github.com/DovileDo/source-mat
    
[^39]: 关于图神经网络的拓扑感知和泛化性能

    On the Topology Awareness and Generalization Performance of Graph Neural Networks

    [https://arxiv.org/abs/2403.04482](https://arxiv.org/abs/2403.04482)

    这篇论文介绍了一个全面的框架，用于对图神经网络在任何拓扑特征上的拓扑感知进行表征。

    

    许多计算机视觉和机器学习问题被建模为在图上的学习任务，图神经网络(GNNs)已经成为学习图结构数据表示的主要工具。GNNs的一个关键特征是它们利用图结构作为输入，从而能够利用图的固有拓扑属性，即GNNs的拓扑感知。尽管GNNs在实践中取得了成功，但拓扑感知对泛化性能的影响仍未被探讨，特别是对于与数据独立同分布(I.I.D.)的假设背道而驰的节点级任务。对于GNNs的拓扑感知的精确定义和表征，特别是涉及不同拓扑特征的情况，仍不清楚。本文引入了一个全面的框架，用于对GNNs在任何拓扑特征上的拓扑感知进行表征。

    arXiv:2403.04482v1 Announce Type: new  Abstract: Many computer vision and machine learning problems are modelled as learning tasks on graphs, where graph neural networks (GNNs) have emerged as a dominant tool for learning representations of graph-structured data. A key feature of GNNs is their use of graph structures as input, enabling them to exploit the graphs' inherent topological properties-known as the topology awareness of GNNs. Despite the empirical successes of GNNs, the influence of topology awareness on generalization performance remains unexplored, particularly for node-level tasks that diverge from the assumption of data being independent and identically distributed (I.I.D.). The precise definition and characterization of the topology awareness of GNNs, especially concerning different topological features, are still unclear. This paper introduces a comprehensive framework to characterize the topology awareness of GNNs across any topological feature. Using this framework, we
    
[^40]: 超参数调整MLP用于概率时间序列预测

    Hyperparameter Tuning MLPs for Probabilistic Time Series Forecasting

    [https://arxiv.org/abs/2403.04477](https://arxiv.org/abs/2403.04477)

    该研究通过大量实验调查时间序列相关的特定超参数对MLP模型在时间序列预测中性能的影响，引入了迄今为止最大的用于时间序列预测的元数据集TSBench，强调调整这些参数的重要性。

    

    时间序列预测试图通过分析过去的趋势和模式来预测未来事件。尽管经过深入研究，但关于在时间序列预测中使用深度学习的某些关键方面仍不明确。我们的研究主要集中在考察与时间序列相关的特定超参数（如上下文长度和验证策略）对时间序列预测中现代MLP模型性能的影响。我们进行了一系列全面的实验，涉及20个时间序列预测数据集中每个数据集4800个配置，并且我们的研究结果表明调整这些参数的重要性。此外，在这项工作中，我们引入了迄今为止用于时间序列预测的最大元数据集，名为TSBench，包括97200次评估，比该领域先前工作增加了20倍。最后，我们展示了所创建模型的效用。

    arXiv:2403.04477v1 Announce Type: new  Abstract: Time series forecasting attempts to predict future events by analyzing past trends and patterns. Although well researched, certain critical aspects pertaining to the use of deep learning in time series forecasting remain ambiguous. Our research primarily focuses on examining the impact of specific hyperparameters related to time series, such as context length and validation strategy, on the performance of the state-of-the-art MLP model in time series forecasting. We have conducted a comprehensive series of experiments involving 4800 configurations per dataset across 20 time series forecasting datasets, and our findings demonstrate the importance of tuning these parameters. Furthermore, in this work, we introduce the largest metadataset for timeseries forecasting to date, named TSBench, comprising 97200 evaluations, which is a twentyfold increase compared to previous works in the field. Finally, we demonstrate the utility of the created m
    
[^41]: 关于图神经网络在现实世界中的调查：不平衡、噪声、隐私和OOD挑战

    A Survey of Graph Neural Networks in Real world: Imbalance, Noise, Privacy and OOD Challenges

    [https://arxiv.org/abs/2403.04468](https://arxiv.org/abs/2403.04468)

    本文调查了图神经网络在现实世界中面临的不平衡、噪声、隐私和OOD挑战，并致力于提高模型性能、可靠性和鲁棒性。

    

    arXiv:2403.04468v1 发布类型: 跨域 摘要: 图结构化数据表现出普适性和广泛适用性，涵盖社交网络分析、生物化学、金融欺诈检测和网络安全等多个领域。在利用图神经网络（GNNs）取得显著成功方面已经取得了重要进展。然而，在实际应用场景中，模型的训练环境往往远非理想，由于各种不利因素，包括数据分布不平衡、错误数据中存在噪声、敏感信息的隐私保护以及对于OOD场景的泛化能力，导致GNN模型的性能大幅下降。为解决这些问题，人们致力于改善GNN模型在实际应用场景中的性能，提高其可靠性和鲁棒性。本文全面调查了...

    arXiv:2403.04468v1 Announce Type: cross  Abstract: Graph-structured data exhibits universality and widespread applicability across diverse domains, such as social network analysis, biochemistry, financial fraud detection, and network security. Significant strides have been made in leveraging Graph Neural Networks (GNNs) to achieve remarkable success in these areas. However, in real-world scenarios, the training environment for models is often far from ideal, leading to substantial performance degradation of GNN models due to various unfavorable factors, including imbalance in data distribution, the presence of noise in erroneous data, privacy protection of sensitive information, and generalization capability for out-of-distribution (OOD) scenarios. To tackle these issues, substantial efforts have been devoted to improving the performance of GNN models in practical real-world scenarios, as well as enhancing their reliability and robustness. In this paper, we present a comprehensive surv
    
[^42]: Vlearn：使用高效状态值函数估计的离策略学习

    Vlearn: Off-Policy Learning with Efficient State-Value Function Estimation

    [https://arxiv.org/abs/2403.04453](https://arxiv.org/abs/2403.04453)

    Vlearn提出了一种新颖的离策略优化方法，称为Vlearn，它通过仅利用状态值函数作为评论家，消除了对明确状态-动作-值函数的需求，从而解决了高维动作空间中的计算挑战。

    

    存在的离策略强化学习算法通常需要明确状态-动作-值函数表示，这在高维动作空间中变得棘手。这些算法经常遇到挑战，即它们在处理维度灾难时遇到困难，因为在这样的空间中维护状态-动作-值函数变得数据效率低下。在这项工作中，我们提出了一种名为Vlearn的新型离策略信任区域优化方法，它消除了对明确状态-动作-值函数的要求。相反，我们展示了如何有效地利用状态值函数作为评论家，从而克服现有方法的几个局限性。通过这样做，Vlearn解决了高维动作空间所带来的计算挑战。此外，Vlearn引入了一种有效的方法来解决与纯状态值函数学习相关的离策略学习中的挑战。

    arXiv:2403.04453v1 Announce Type: new  Abstract: Existing off-policy reinforcement learning algorithms typically necessitate an explicit state-action-value function representation, which becomes problematic in high-dimensional action spaces. These algorithms often encounter challenges where they struggle with the curse of dimensionality, as maintaining a state-action-value function in such spaces becomes data-inefficient. In this work, we propose a novel off-policy trust region optimization approach, called Vlearn, that eliminates the requirement for an explicit state-action-value function. Instead, we demonstrate how to efficiently leverage just a state-value function as the critic, thus overcoming several limitations of existing methods. By doing so, Vlearn addresses the computational challenges posed by high-dimensional action spaces. Furthermore, Vlearn introduces an efficient approach to address the challenges associated with pure state-value function learning in the off-policy se
    
[^43]: 会员推理攻击与主题建模中的隐私

    Membership Inference Attacks and Privacy in Topic Modeling

    [https://arxiv.org/abs/2403.04451](https://arxiv.org/abs/2403.04451)

    主题建模中提出了会员推理攻击，通过差分隐私词汇选择来改善隐私风险

    

    最近的研究表明，大型语言模型容易受到推理训练数据方面的隐私攻击。然而，目前还不清楚更简单的生成模型，例如主题模型，是否存在类似的漏洞。在这项工作中，我们提出了一种针对主题模型的攻击，可以自信地识别Latent Dirichlet Allocation中训练数据的成员。我们的结果表明，与大型神经模型相关联的隐私风险并不仅限于大型神经模型。此外，为了减轻这些漏洞，我们探讨了差分隐私（DP）主题建模。我们提出了一个私密主题建模框架，将DP词汇选择作为预处理步骤，并展示它不仅改善了隐私性，而且在实用性方面的影响有限。

    arXiv:2403.04451v1 Announce Type: cross  Abstract: Recent research shows that large language models are susceptible to privacy attacks that infer aspects of the training data. However, it is unclear if simpler generative models, like topic models, share similar vulnerabilities. In this work, we propose an attack against topic models that can confidently identify members of the training data in Latent Dirichlet Allocation. Our results suggest that the privacy risks associated with generative modeling are not restricted to large neural models. Additionally, to mitigate these vulnerabilities, we explore differentially private (DP) topic modeling. We propose a framework for private topic modeling that incorporates DP vocabulary selection as a pre-processing step, and show that it improves privacy while having limited effects on practical utility.
    
[^44]: FRRI：一种新颖的模糊-粗糙规则归纳算法

    FRRI: a novel algorithm for fuzzy-rough rule induction

    [https://arxiv.org/abs/2403.04447](https://arxiv.org/abs/2403.04447)

    结合模糊与粗糙集理论，提出一种新颖的模糊-粗糙规则归纳算法 FRRI。

    

    可解释性是机器学习研究的下一个前沿。在寻找白盒模型的过程中-与随机森林或神经网络等黑盒模型相对应，规则归纳算法是一个合乎逻辑且有希望的选择，因为规则可以被人类轻松理解。模糊和粗糙集理论已成功应用于这种原型，几乎总是分开应用。由于规则归纳的两种方法均涉及基于等价类概念的粒计算，将它们结合是自然的选择。QuickRules算法是利用模糊粗糙集理论进行规则归纳的第一次尝试。它基于QuickReduct，这是一个用于构建决策约简的贪婪算法。QuickRules 已经展示了相比其他规则归纳方法的改进。然而，要评估模糊-粗糙规则归纳算法的全部潜力，就需要从基础开始。在本文中，

    arXiv:2403.04447v1 Announce Type: cross  Abstract: Interpretability is the next frontier in machine learning research. In the search for white box models - as opposed to black box models, like random forests or neural networks - rule induction algorithms are a logical and promising option, since the rules can easily be understood by humans. Fuzzy and rough set theory have been successfully applied to this archetype, almost always separately. As both approaches to rule induction involve granular computing based on the concept of equivalence classes, it is natural to combine them. The QuickRules\cite{JensenCornelis2009} algorithm was a first attempt at using fuzzy rough set theory for rule induction. It is based on QuickReduct, a greedy algorithm for building decision reducts. QuickRules already showed an improvement over other rule induction methods. However, to evaluate the full potential of a fuzzy rough rule induction algorithm, one needs to start from the foundations. In this paper,
    
[^45]: 不完全代理的合作贝叶斯优化

    Cooperative Bayesian Optimization for Imperfect Agents

    [https://arxiv.org/abs/2403.04442](https://arxiv.org/abs/2403.04442)

    这项研究提出了一种合作的贝叶斯优化方法，用于优化黑匣子函数，在这种方法中两个代理者协作选择查询函数的点，通过战略规划实现更好地识别全局最大值。

    

    我们引入了一个合作的贝叶斯优化问题，用于优化两个变量的黑匣子函数，其中两个代理者一起选择在哪些点上查询函数，但每个只控制一个变量。这个设置受到人工智能与人类团队合作的启发，其中人工智能助手帮助其用户解决问题，在这种简单的情况下是协作优化。我们将解决方案构建为顺序决策，其中我们控制的代理把用户模拟为对函数具有先验知识的计算有理代理。我们展示了对查询的战略规划能够更好地识别全局最大值，只要用户避免过度探索。这种规划是通过使用贝叶斯自适应蒙特卡洛规划以及赋予代理具有考虑保守信念更新和探索性采样的用户模型而实现的。

    arXiv:2403.04442v1 Announce Type: cross  Abstract: We introduce a cooperative Bayesian optimization problem for optimizing black-box functions of two variables where two agents choose together at which points to query the function but have only control over one variable each. This setting is inspired by human-AI teamwork, where an AI-assistant helps its human user solve a problem, in this simplest case, collaborative optimization. We formulate the solution as sequential decision-making, where the agent we control models the user as a computationally rational agent with prior knowledge about the function. We show that strategic planning of the queries enables better identification of the global maximum of the function as long as the user avoids excessive exploration. This planning is made possible by using Bayes Adaptive Monte Carlo planning and by endowing the agent with a user model that accounts for conservative belief updates and exploratory sampling of the points to query.
    
[^46]: 学习人机器实时全身遥操作

    Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation

    [https://arxiv.org/abs/2403.04436](https://arxiv.org/abs/2403.04436)

    提出了基于强化学习的框架"Human to Humanoid (H2O)"，实现了利用单个RGB相机对全尺寸人形机器人进行实时全身遥操作，并成功在真实场景中实现了动态全身运动遥操作。

    

    我们提出了"Human to Humanoid (H2O)"这一基于强化学习（RL）的框架，可以利用仅有的一个RGB相机实现对全尺寸人形机器人的实时全身遥操作。为了为人形机器人创建一个大规模的人类运动重定向数据集，我们提出了一种可扩展的“从模拟到数据”的流程，使用一个特权运动模仿器来筛选和选择可行的动作。随后，我们在模拟环境中使用这些精细的动作训练一个稳健的实时人形机器人动作模仿器，并以零次试验的方式将其迁移到真实人形机器人上。我们成功实现了在真实环境中对动态全身运动的遥操作，包括行走、后跳、踢球、转身、挥手、推动、拳击等。据我们所知，这是实现基于学习的实时全身人形机器人遥控的首次演示。

    arXiv:2403.04436v1 Announce Type: cross  Abstract: We present Human to Humanoid (H2O), a reinforcement learning (RL) based framework that enables real-time whole-body teleoperation of a full-sized humanoid robot with only an RGB camera. To create a large-scale retargeted motion dataset of human movements for humanoid robots, we propose a scalable "sim-to-data" process to filter and pick feasible motions using a privileged motion imitator. Afterwards, we train a robust real-time humanoid motion imitator in simulation using these refined motions and transfer it to the real humanoid robot in a zero-shot manner. We successfully achieve teleoperation of dynamic whole-body motions in real-world scenarios, including walking, back jumping, kicking, turning, waving, pushing, boxing, etc. To the best of our knowledge, this is the first demonstration to achieve learning-based real-time whole-body humanoid teleoperation.
    
[^47]: 在无线联邦学习中增强公平性和鲁棒性

    Boosting Fairness and Robustness in Over-the-Air Federated Learning

    [https://arxiv.org/abs/2403.04431](https://arxiv.org/abs/2403.04431)

    提出了一种通过极大极小优化实现公平性和鲁棒性的无线联邦学习算法，不需要复杂的编码解码方案，提高了效率和隐私性

    

    近期研究表明，通过无线计算在分散式训练机器学习模型方面是有效的，本文提出了一种旨在通过极大极小优化提供公平性和鲁棒性的无线联邦学习算法。通过使用问题的外容形式，我们展示了所提算法收敛于极大极小问题的最优解。此外，与最先进方法相反，所提方法不需要通过复杂的编码解码方案对信道系数进行重建，从而提高了效率和隐私性。

    arXiv:2403.04431v1 Announce Type: new  Abstract: Over-the-Air Computation is a beyond-5G communication strategy that has recently been shown to be useful for the decentralized training of machine learning models due to its efficiency. In this paper, we propose an Over-the-Air federated learning algorithm that aims to provide fairness and robustness through minmax optimization. By using the epigraph form of the problem at hand, we show that the proposed algorithm converges to the optimal solution of the minmax problem. Moreover, the proposed approach does not require reconstructing channel coefficients by complex encoding-decoding schemes as opposed to state-of-the-art approaches. This improves both efficiency and privacy.
    
[^48]: 移动边缘网络中绿色联合生成扩散的按需量化

    On-demand Quantization for Green Federated Generative Diffusion in Mobile Edge Networks

    [https://arxiv.org/abs/2403.04430](https://arxiv.org/abs/2403.04430)

    在移动边缘网络中，提出了一种按需量化的能效联合生成扩散方法以解决大型GAI模型训练中的通信消耗和能量消耗挑战。

    

    arXiv:2403.04430v1 公告类型:新     摘要:生成人工智能 (GAI) 在移动边缘网络中表现出了显著的生产力和创造力，例如元宇宙和工业物联网。联合学习是在移动边缘网络中有效训练GAI模型的一种有前途的技术，因为它的数据分布。然而，当在移动边缘网络中训练大型GAI模型（如生成扩散模型）时，存在一个与通信消耗相关的显着问题。此外，基于扩散模型的训练所消耗的实际能量，加上边缘设备的有限资源和网络环境的复杂性，为提高GAI模型的训练效率带来了挑战。为解决这一挑战，我们提出了一种面向移动边缘网络的按需量化能效联合扩散方法。具体地，我们首先设计了一种考虑到网络动态量化的联合扩散训练方案。

    arXiv:2403.04430v1 Announce Type: new  Abstract: Generative Artificial Intelligence (GAI) shows remarkable productivity and creativity in Mobile Edge Networks, such as the metaverse and the Industrial Internet of Things. Federated learning is a promising technique for effectively training GAI models in mobile edge networks due to its data distribution. However, there is a notable issue with communication consumption when training large GAI models like generative diffusion models in mobile edge networks. Additionally, the substantial energy consumption associated with training diffusion-based models, along with the limited resources of edge devices and complexities of network environments, pose challenges for improving the training efficiency of GAI models. To address this challenge, we propose an on-demand quantized energy-efficient federated diffusion approach for mobile edge networks. Specifically, we first design a dynamic quantized federated diffusion training scheme considering va
    
[^49]: 探究降维对多元时间序列异常检测性能的影响

    Exploring the Influence of Dimensionality Reduction on Anomaly Detection Performance in Multivariate Time Series

    [https://arxiv.org/abs/2403.04429](https://arxiv.org/abs/2403.04429)

    降维技术与先进的无监督时间序列异常检测模型的整合能够显著提高特定场景下的异常检测性能，并且可以大幅减少训练时间。

    

    本文针对MUTANT和Anomaly-Transformer模型，对降维技术与先进的无监督时间序列异常检测模型的整合进行了广泛的实证研究。研究涉及对三个不同数据集（MSL、SMAP和SWaT）进行全面评估。每个数据集都提出了独特的挑战，有助于在不同环境下全面评估模型的能力。所研究的降维技术包括PCA、UMAP、随机投影和t-SNE，每种技术在简化高维数据方面都具有独特优势。我们的研究结果显示，降维不仅有助于减少计算复杂度，而且在某些情况下还显著增强了异常检测性能。此外，在观察到训练时间显著缩短的同时，当维数减半时，训练时间分别减少了约300\%和650\%。

    arXiv:2403.04429v1 Announce Type: new  Abstract: This paper presents an extensive empirical study on the integration of dimensionality reduction techniques with advanced unsupervised time series anomaly detection models, focusing on the MUTANT and Anomaly-Transformer models. The study involves a comprehensive evaluation across three different datasets: MSL, SMAP, and SWaT. Each dataset poses unique challenges, allowing for a robust assessment of the models' capabilities in varied contexts. The dimensionality reduction techniques examined include PCA, UMAP, Random Projection, and t-SNE, each offering distinct advantages in simplifying high-dimensional data. Our findings reveal that dimensionality reduction not only aids in reducing computational complexity but also significantly enhances anomaly detection performance in certain scenarios. Moreover, a remarkable reduction in training times was observed, with reductions by approximately 300\% and 650\% when dimensionality was halved and m
    
[^50]: Signature Isolation Forest

    Signature Isolation Forest

    [https://arxiv.org/abs/2403.04405](https://arxiv.org/abs/2403.04405)

    介绍了一种新颖的异常检测算法"Signature Isolation Forest"，利用粗路径理论的签名变换去除了Functional Isolation Forest的线性内积和词典选择方面的限制。

    

    Functional Isolation Forest (FIF)是一种针对功能数据设计的最新一流异常检测(AD)算法。它依赖于一种树分区过程，通过将每个曲线观测投影到通过线性内积绘制的词典上来计算异常得分。本文通过引入“Signature Isolation Forest”，一种利用粗路径理论签名变换的新颖AD算法类，来解决这些挑战。我们的目标是通过提出两种算法来消除FIF施加的限制，这两种算法特别针对FIF内积的线性性和词典的选择。

    arXiv:2403.04405v1 Announce Type: cross  Abstract: Functional Isolation Forest (FIF) is a recent state-of-the-art Anomaly Detection (AD) algorithm designed for functional data. It relies on a tree partition procedure where an abnormality score is computed by projecting each curve observation on a drawn dictionary through a linear inner product. Such linear inner product and the dictionary are a priori choices that highly influence the algorithm's performances and might lead to unreliable results, particularly with complex datasets. This work addresses these challenges by introducing \textit{Signature Isolation Forest}, a novel AD algorithm class leveraging the rough path theory's signature transform. Our objective is to remove the constraints imposed by FIF through the proposition of two algorithms which specifically target the linearity of the FIF inner product and the choice of the dictionary. We provide several numerical experiments, including a real-world applications benchmark sho
    
[^51]: 深度学习中颜色和纹理失真对地球观测数据的影响

    Impacts of Color and Texture Distortions on Earth Observation Data in Deep Learning

    [https://arxiv.org/abs/2403.04385](https://arxiv.org/abs/2403.04385)

    本研究系统地研究了深度学习模型对地球观测数据中颜色和纹理失真的敏感性，发现模型对纹理失真比颜色失真更敏感

    

    地物覆盖分类和变化检测是遥感和地球观测（EO）的两个重要应用领域，在深度学习的进展中得到了极大的益处。卷积和基于Transformer的U-net模型是这些任务的最先进架构，它们的性能得到了大规模标注EO数据集的增加的提升。然而，对输入EO数据的不同视觉特征对模型预测的影响尚不明确。在这项工作中，我们系统地研究了在推断期间对输入EO数据进行几种基于颜色和纹理的失真时模型的敏感性，考虑到这些模型是在没有这种失真的情况下进行训练的。我们对多个最先进的地物覆盖分类网络进行实验，结果表明它们通常对纹理失真比颜色失真更为敏感。

    arXiv:2403.04385v1 Announce Type: cross  Abstract: Land cover classification and change detection are two important applications of remote sensing and Earth observation (EO) that have benefited greatly from the advances of deep learning. Convolutional and transformer-based U-net models are the state-of-the-art architectures for these tasks, and their performances have been boosted by an increased availability of large-scale annotated EO datasets. However, the influence of different visual characteristics of the input EO data on a model's predictions is not well understood. In this work we systematically examine model sensitivities with respect to several color- and texture-based distortions on the input EO data during inference, given models that have been trained without such distortions. We conduct experiments with multiple state-of-the-art segmentation networks for land cover classification and show that they are in general more sensitive to texture than to color distortions. Beyond
    
[^52]: LoCoDL: 具有本地训练和压缩的通信高效分布式学习

    LoCoDL: Communication-Efficient Distributed Learning with Local Training and Compression

    [https://arxiv.org/abs/2403.04348](https://arxiv.org/abs/2403.04348)

    LoCoDL是一种通信高效的分布式学习算法，结合了本地训练和压缩技术，具有双倍加速的通信复杂度优势，特别适用于一般异构条件下的强凸函数。

    

    在分布式优化和学习中，甚至在现代联邦学习框架中，由于通信速度慢且成本高，通信至关重要。我们介绍了LoCoDL，这是一种通信高效的算法，它利用了本地训练和压缩这两种流行且有效的技术，本地训练降低了通信频率，压缩则是发送短的比特流而不是完整的浮点数向量。LoCoDL适用于大类别的无偏压缩器，其中包括广泛使用的稀疏化和量化方法。LoCoDL在一般异构条件下具有双倍加速的通信复杂度优势，这取决于函数的条件数和模型维度，特别是在强凸函数的情况下。在实践中得到了验证，LoCoDL胜过了现有的算法。

    arXiv:2403.04348v1 Announce Type: cross  Abstract: In Distributed optimization and Learning, and even more in the modern framework of federated learning, communication, which is slow and costly, is critical. We introduce LoCoDL, a communication-efficient algorithm that leverages the two popular and effective techniques of Local training, which reduces the communication frequency, and Compression, in which short bitstreams are sent instead of full-dimensional vectors of floats. LoCoDL works with a large class of unbiased compressors that includes widely-used sparsification and quantization methods. LoCoDL provably benefits from local training and compression and enjoys a doubly-accelerated communication complexity, with respect to the condition number of the functions and the model dimension, in the general heterogenous regime with strongly convex functions. This is confirmed in practice, with LoCoDL outperforming existing algorithms.
    
[^53]: RL-CFR: 使用强化学习改进不完全信息广义形式博弈的动作抽象

    RL-CFR: Improving Action Abstraction for Imperfect Information Extensive-Form Games with Reinforcement Learning

    [https://arxiv.org/abs/2403.04344](https://arxiv.org/abs/2403.04344)

    RL-CFR是一种用于动态动作抽象的强化学习方法，通过创新的MDP公式构建游戏树，并利用CFR进行策略推导，能够在不完全信息广义形式博弈中取得优秀性能。

    

    有效的动作抽象在应对不完全信息广义形式博弈（IIEFGs）中大规模动作空间所带来的挑战方面至关重要。然而，由于IIEFGs中庞大的状态空间和计算复杂性，现有方法通常依赖于固定的抽象，导致性能次优。为此，我们引入RL-CFR，这是一种新颖的强化学习（RL）方法，用于动态动作抽象。RL-CFR基于我们创新的马尔可夫决策过程（MDP）的表述，其中状态对应于公共信息，动作表示为指示特定动作抽象的特征向量。奖励被定义为所选动作抽象和默认动作抽象之间的预期收益差异。RL-CFR构建一个带有RL引导动作抽象的游戏树，并利用对盘悔恨最小化（CFR）进行策略推导。令人印象深刻的是，它可以从头开始进行训练，取得了优秀的性能。

    arXiv:2403.04344v1 Announce Type: cross  Abstract: Effective action abstraction is crucial in tackling challenges associated with large action spaces in Imperfect Information Extensive-Form Games (IIEFGs). However, due to the vast state space and computational complexity in IIEFGs, existing methods often rely on fixed abstractions, resulting in sub-optimal performance. In response, we introduce RL-CFR, a novel reinforcement learning (RL) approach for dynamic action abstraction. RL-CFR builds upon our innovative Markov Decision Process (MDP) formulation, with states corresponding to public information and actions represented as feature vectors indicating specific action abstractions. The reward is defined as the expected payoff difference between the selected and default action abstractions. RL-CFR constructs a game tree with RL-guided action abstractions and utilizes counterfactual regret minimization (CFR) for strategy derivation. Impressively, it can be trained from scratch, achievin
    
[^54]: 解释性人工智能在嵌入式系统设计中的应用：静态冗余NVM内存写预测案例研究

    Explainable AI for Embedded Systems Design: A Case Study of Static Redundant NVM Memory Write Prediction

    [https://arxiv.org/abs/2403.04337](https://arxiv.org/abs/2403.04337)

    本研究提出了解释性人工智能在嵌入式系统设计中的应用，并通过静态冗余NVM内存写预测案例研究，证明了XAI在提高性能和能效方面的有效性。

    

    这篇论文研究了解释性人工智能（XAI）在使用机器学习（ML）设计嵌入式系统中的应用。作为一个案例研究，它解决了静态静默存储预测的挑战性问题。这涉及仅基于静态程序特征识别冗余内存写入。消除这种存储会通过减少内存访问和总线流量提高性能和能效，尤其是在新兴非易失性内存技术存在的情况下。为了实现这一目标，我们提出了一个方法，包括：1）为解释静默存储预测开发相关的ML模型，以及2）应用XAI来解释这些模型。我们采用两种最先进的模型无关的XAI方法来分析静默存储的原因。通过案例研究，我们评估了这些方法的有效性。我们发现这些方法为静默存储预测提供了解释，

    arXiv:2403.04337v1 Announce Type: new  Abstract: This paper investigates the application of eXplainable Artificial Intelligence (XAI) in the design of embedded systems using machine learning (ML). As a case study, it addresses the challenging problem of static silent store prediction. This involves identifying redundant memory writes based only on static program features. Eliminating such stores enhances performance and energy efficiency by reducing memory access and bus traffic, especially in the presence of emerging non-volatile memory technologies. To achieve this, we propose a methodology consisting of: 1) the development of relevant ML models for explaining silent store prediction, and 2) the application of XAI to explain these models. We employ two state-of-the-art model-agnostic XAI methods to analyze the causes of silent stores. Through the case study, we evaluate the effectiveness of the methods. We find that these methods provide explanations for silent store predictions, whi
    
[^55]: 一种基于机制的强化学习框架用于机翼气动外型优化

    A mechanism-informed reinforcement learning framework for shape optimization of airfoils

    [https://arxiv.org/abs/2403.04329](https://arxiv.org/abs/2403.04329)

    提出了一种基于机制的强化学习框架，用于处理流体动力学控制下的气动外型优化，并整合了多种策略以保证精确性和高效性。

    

    在本研究中，我们提出了一种基于机制的强化学习框架，用于机翼气动外型优化。通过利用双延迟深度确定性策略梯度算法的显著稳定性，我们的方法解决了受流体动力学控制的外型优化的复杂性。采用基于PDEs的求解器以保证准确性，即使在探索过程中配置和几何形状发生了极大变化。我们采用双加权残差网格细化策略，以确保目标函数的精确计算。为了简化迭代优化过程并处理几何变形，我们的方法集成了拉普拉斯平滑、自适应细化和B\'ezier拟合策略。这种组合不仅消除了网格纠缠，还保证了对机翼几何的精确操作。我们的神经网络架构利用B\'ezier曲线实现了高效的表示。

    arXiv:2403.04329v1 Announce Type: cross  Abstract: In this study, we present the mechanism-informed reinforcement learning framework for airfoil shape optimization. By leveraging the twin delayed deep deterministic policy gradient algorithm for its notable stability, our approach addresses the complexities of optimizing shapes governed by fluid dynamics. The PDEs-based solver is adopted for its accuracy even when the configurations and geometries are extraordinarily changed during the exploration. Dual-weighted residual-based mesh refinement strategy is applied to ensure the accurate calculation of target functionals. To streamline the iterative optimization process and handle geometric deformations, our approach integrates Laplacian smoothing, adaptive refinement, and a B\'ezier fitting strategy. This combination not only remits mesh tangling but also guarantees a precise manipulation of the airfoil geometry. Our neural network architecture leverages B\'ezier curves for efficient dime
    
[^56]: 基于边缘的参数化数字孪生体用于智能建筑室内气候建模

    Edge-based Parametric Digital Twins for Intelligent Building Indoor Climate Modeling

    [https://arxiv.org/abs/2403.04326](https://arxiv.org/abs/2403.04326)

    使用边缘计算、数字孪生体和深度学习相结合的方法，创建参数化数字孪生体用于建筑气候建模，并在边缘部署深度学习模型，以优化建筑物运营。

    

    arXiv:2403.04326v1 公告类型：跨领域 摘要：建筑环境中的数字化转型产生大量数据，用于开发数据驱动模型以优化建筑运营。本研究提出了一种利用边缘计算、数字孪生体和深度学习的综合解决方案，以增强对建筑物气候的理解。使用本体创建的参数化数字孪生体确保跨不同建筑设备的多样服务系统中保持一致的数据表示。基于创建的数字孪生体和收集的数据，采用深度学习方法开发预测模型，用于识别室内气候中的模式并提供见解。参数化数字孪生体和深度学习模型均部署在边缘上，以实现低延迟和隐私合规性。作为示范，对瑞典奥斯泰约特兰地区的一座历史建筑进行了案例研究，以比较五种深度学习架构的性能。结果表明，时序...

    arXiv:2403.04326v1 Announce Type: cross  Abstract: Digital transformation in the built environment generates vast data for developing data-driven models to optimize building operations. This study presents an integrated solution utilizing edge computing, digital twins, and deep learning to enhance the understanding of climate in buildings. Parametric digital twins, created using an ontology, ensure consistent data representation across diverse service systems equipped by different buildings. Based on created digital twins and collected data, deep learning methods are employed to develop predictive models for identifying patterns in indoor climate and providing insights. Both the parametric digital twin and deep learning models are deployed on edge for low latency and privacy compliance. As a demonstration, a case study was conducted in a historic building in \"Osterg\"otland, Sweden, to compare the performance of five deep learning architectures. The results indicate that the time-seri
    
[^57]: 基于遗传模拟的差分进化方法用于半监督聚类

    Memetic Differential Evolution Methods for Semi-Supervised Clustering

    [https://arxiv.org/abs/2403.04322](https://arxiv.org/abs/2403.04322)

    本文提出了一种基于差分进化范式的新颖遗传模拟策略，用于解决半监督聚类问题，是第一次在这个领域尝试定义这样的方法。

    

    在本文中，我们处理半监督最小平方和聚类(MSSC)问题，其中背景知识以实例级约束的形式给定。我们特别考虑“必连接”和“非连接”约束，每个约束指示两个数据集点是否应该关联到同一个或不同的簇中。这些约束的存在使得问题至少与其无监督版本一样困难：不再每个点都关联到其最近的簇中心，因此需要在关键操作（如分配步骤）中进行一些修改。在这种情况下，我们提出了一种基于差分进化范式的新颖遗传模拟策略，直接扩展了最近在无监督聚类文献中提出的最新框架。据我们所知，我们的贡献代表了第一次尝试定义一个旨在生成一个

    arXiv:2403.04322v1 Announce Type: cross  Abstract: In this paper, we deal with semi-supervised Minimum Sum-of-Squares Clustering (MSSC) problems where background knowledge is given in the form of instance-level constraints. In particular, we take into account "must-link" and "cannot-link" constraints, each of which indicates if two dataset points should be associated to the same or to a different cluster. The presence of such constraints makes the problem at least as hard as its unsupervised version: it is no more true that each point is associated to its nearest cluster center, thus requiring some modifications in crucial operations, such as the assignment step. In this scenario, we propose a novel memetic strategy based on the Differential Evolution paradigm, directly extending a state-of-the-art framework recently proposed in the unsupervised clustering literature. As far as we know, our contribution represents the first attempt to define a memetic methodology designed to generate a
    
[^58]: 带有分摊上下文记忆的语言模型的在线适应

    Online Adaptation of Language Models with a Memory of Amortized Contexts

    [https://arxiv.org/abs/2403.04317](https://arxiv.org/abs/2403.04317)

    提出了一种带有分摊上下文记忆的在线适应框架，可有效地提取、压缩并存储信息以保持强大的知识保留能力

    

    由于信息的快速生成和传播，即使开发成本巨大，大型语言模型（LLMs）也很快过时。鉴于保持模型更新的重要性，当在现实世界应用LLMs时，在线学习已成为一项至关重要的需求。然而，鉴于不断扩大的未见文档语料库和现代LLMs的大参数空间，高效的适应至关重要。为了解决这些挑战，我们提出了Memory of Amortized Contexts（MAC），这是一个针对LLMs的高效且有效的在线适应框架，具有较强的知识保留能力。我们提出了一种摊销特征提取和记忆增强方法，将新文档中的信息压缩并提取为存储在记忆库中的紧凑调制。在回答问题时，我们的模型关注并从该记忆库中提取相关知识。为了有效地学习有信息量的调制…

    arXiv:2403.04317v1 Announce Type: cross  Abstract: Due to the rapid generation and dissemination of information, large language models (LLMs) quickly run out of date despite enormous development costs. Due to this crucial need to keep models updated, online learning has emerged as a critical necessity when utilizing LLMs for real-world applications. However, given the ever-expanding corpus of unseen documents and the large parameter space of modern LLMs, efficient adaptation is essential. To address these challenges, we propose Memory of Amortized Contexts (MAC), an efficient and effective online adaptation framework for LLMs with strong knowledge retention. We propose an amortized feature extraction and memory-augmentation approach to compress and extract information from new documents into compact modulations stored in a memory bank. When answering questions, our model attends to and extracts relevant knowledge from this memory bank. To learn informative modulations in an efficient m
    
[^59]: 最近大型视觉-语言模型的有效性评估

    Effectiveness Assessment of Recent Large Vision-Language Models

    [https://arxiv.org/abs/2403.04306](https://arxiv.org/abs/2403.04306)

    本文评估了最近出现的大型视觉-语言模型在专业和通用任务中的表现，旨在全面了解这些创新方法的能力。

    

    大型视觉-语言模型(LVLMs)的出现代表着迈向人工通用智能的重要进步。然而，它们在专业和通用任务中的有效性程度需要进一步调查。本文旨在评估流行的LVLMs在专业和通用任务中的能力，旨在提供对这些创新方法的全面理解。为了评估它们在专业任务中的有效性，我们量身定制了一个包含自然、医疗和工业三种不同场景的全面测试平台，涵盖六项具有挑战性的任务。这些任务包括显著、伪装和透明物体检测，以及息肉和皮肤病变检测，以及工业异常检测。我们检验了最近三种开源LVLMs--MiniGPT-v2、LLaVA-1.5和Shikra--在视觉识别和定位领域的表现。

    arXiv:2403.04306v1 Announce Type: cross  Abstract: The advent of large vision-language models (LVLMs) represents a noteworthy advancement towards the pursuit of artificial general intelligence. However, the extent of their efficacy across both specialized and general tasks warrants further investigation. This article endeavors to evaluate the competency of popular LVLMs in specialized and general tasks, respectively, aiming to offer a comprehensive comprehension of these innovative methodologies. To gauge their efficacy in specialized tasks, we tailor a comprehensive testbed comprising three distinct scenarios: natural, healthcare, and industrial, encompassing six challenging tasks. These tasks include salient, camouflaged, and transparent object detection, as well as polyp and skin lesion detection, alongside industrial anomaly detection. We examine the performance of three recent open-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of visual recognition and localiza
    
[^60]: MedM2G：通过交叉引导扩散统一医学多模态生成与视觉不变性

    MedM2G: Unifying Medical Multi-Modal Generation via Cross-Guided Diffusion with Visual Invariant

    [https://arxiv.org/abs/2403.04290](https://arxiv.org/abs/2403.04290)

    MedM2G提出了一种医学多模态生成框架，通过统一模型对医学多模态进行对齐、提取和生成，提取有价值的临床知识并增强特定医学信息。

    

    医学生成模型以其高质量样本生成能力而闻名，加速了医学应用的快速发展。然而，最近的研究集中于针对不同医学任务的单独医学生成模型，并且受到不足的医学多模态知识的限制，从而限制了医学全面诊断。在本文中，我们提出了MedM2G，一种医学多模态生成框架，其关键创新在于通过统一模型对医学多模态进行对齐、提取和生成。我们通过统一空间中的中心对齐方法高效地对医学多模态进行对齐，不仅限于单一或两种医学模态。值得注意的是，我们的框架通过保留每种成像模态的医学视觉不变性来提取有价值的临床知识，从而增强多模态生成的特定医学信息。通过调节自适应的交叉引导参数

    arXiv:2403.04290v1 Announce Type: cross  Abstract: Medical generative models, acknowledged for their high-quality sample generation ability, have accelerated the fast growth of medical applications. However, recent works concentrate on separate medical generation models for distinct medical tasks and are restricted to inadequate medical multi-modal knowledge, constraining medical comprehensive diagnosis. In this paper, we propose MedM2G, a Medical Multi-Modal Generative framework, with the key innovation to align, extract, and generate medical multi-modal within a unified model. Extending beyond single or two medical modalities, we efficiently align medical multi-modal through the central alignment approach in the unified space. Significantly, our framework extracts valuable clinical knowledge by preserving the medical visual invariant of each imaging modal, thereby enhancing specific medical information for multi-modal generation. By conditioning the adaptive cross-guided parameters i
    
[^61]: Proxy-RLHF：在大型语言模型中通过代理解耦生成和对齐

    Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model with Proxy

    [https://arxiv.org/abs/2403.04283](https://arxiv.org/abs/2403.04283)

    该论文提出Proxy-RLHF方法，通过将大型语言模型的生成和对齐过程解耦，实现了以更低计算成本对齐人类价值观，仅使用其他方法的1%训练参数即可达到可比水平的对齐度。

    

    强化学习从人类反馈中学习（RLHF）是确保大型语言模型（LLMs）与人类价值观保持一致的主流方法。然而，现有的RLHF方法需要高昂的计算成本，主要原因之一是RLHF同时将生成和对齐任务分配给LLM。本文介绍了Proxy-RLHF，它解耦了LLMs的生成和对齐流程，以更低的计算成本实现与人类价值的对齐。我们从为对齐过程设计的新型马尔可夫决策过程（MDP）开始，并使用强化学习（RL）训练了一个简化的代理模型，监督LLM的标记生成，而不改变LLM本身。实验证明，我们的方法仅使用其他方法的1%训练参数即可实现可比水平的对齐度。

    arXiv:2403.04283v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is the prevailing approach to ensure Large Language Models (LLMs) align with human values. However, existing RLHF methods require a high computational cost, one main reason being that RLHF assigns both the generation and alignment tasks to the LLM simultaneously. In this paper, we introduce Proxy-RLHF, which decouples the generation and alignment processes of LLMs, achieving alignment with human values at a much lower computational cost. We start with a novel Markov Decision Process (MDP) designed for the alignment process and employ Reinforcement Learning (RL) to train a streamlined proxy model that oversees the token generation of the LLM, without altering the LLM itself. Experiments show that our method achieves a comparable level of alignment with only 1\% of the training parameters of other methods.
    
[^62]: Qubit-Wise架构搜索方法用于变分量子电路

    Qubit-Wise Architecture Search Method for Variational Quantum Circuits

    [https://arxiv.org/abs/2403.04268](https://arxiv.org/abs/2403.04268)

    提出了一种新颖的QWAS方法，通过逐步搜索每个阶段的一量子位配置，并结合蒙特卡罗树搜索算法，实现了在一些实际任务中平衡电路性能和大小的探索与开发。

    

    考虑到噪音级别的限制，量子机器学习的一个关键方面是设计一个具有较少量子门的高性能变分量子电路架构。与经典神经架构搜索（NAS）类似，量子架构搜索方法（QAS）采用强化学习、进化算法和超网优化等方法来提高搜索效率。本文提出了一种新颖的Qubit-Wise架构搜索（QWAS）方法，逐步搜索每个阶段的一量子位配置，并结合蒙特卡罗树搜索算法通过将搜索空间划分为几个良好和不良子区域来找到良好的量子架构。数值实验结果表明，我们提出的方法可以在一些现实任务中（如MNIST、时尚和MOSI等）平衡电路性能和大小的探索与开发。据我们所知，QWAS达到

    arXiv:2403.04268v1 Announce Type: cross  Abstract: Considering the noise level limit, one crucial aspect for quantum machine learning is to design a high-performing variational quantum circuit architecture with small number of quantum gates. As the classical neural architecture search (NAS), quantum architecture search methods (QAS) employ methods like reinforcement learning, evolutionary algorithms and supernet optimiza-tion to improve the search efficiency. In this paper, we propose a novel qubit-wise architec-ture search (QWAS) method, which progres-sively search one-qubit configuration per stage, and combine with Monte Carlo Tree Search al-gorithm to find good quantum architectures by partitioning the search space into several good and bad subregions. The numerical experimental results indicate that our proposed method can balance the exploration and exploitation of cir-cuit performance and size in some real-world tasks, such as MNIST, Fashion and MOSI. As far as we know, QWAS achi
    
[^63]: 小型语言模型能成为顺序推荐系统的良好推理者吗？

    Can Small Language Models be Good Reasoners for Sequential Recommendation?

    [https://arxiv.org/abs/2403.04260](https://arxiv.org/abs/2403.04260)

    提出了逐步知识提取框架（SLIM），为顺序推荐系统解决了大型语言模型（LLMs）高资源需求的难题，使其能以资源高效的方式享受LLMs的出色推理能力。

    

    大型语言模型（LLMs）由于其出色的语言理解和生成能力，为顺序推荐开拓了新的领域。然而，要成功实现由LLMs赋能的顺序推荐还有许多挑战需要解决。首先，用户行为模式通常复杂，仅仅依靠LLMs的一步推理可能会导致错误或与任务无关的响应。其次，LLMs（例如ChatGPT-175B）极高的资源需求是难以承受且在实际顺序推荐系统中不切实际的。本文提出了一个新颖的逐步知识提取框架用于推荐（SLIM），为顺序推荐器以“瘦”（即资源高效）的方式享受LLMs出色的推理能力铺平了一条有前途的道路。我们引入基于用户行为序列的CoT提示来实现更好的推荐。

    arXiv:2403.04260v1 Announce Type: cross  Abstract: Large language models (LLMs) open up new horizons for sequential recommendations, owing to their remarkable language comprehension and generation capabilities. However, there are still numerous challenges that should be addressed to successfully implement sequential recommendations empowered by LLMs. Firstly, user behavior patterns are often complex, and relying solely on one-step reasoning from LLMs may lead to incorrect or task-irrelevant responses. Secondly, the prohibitively resource requirements of LLM (e.g., ChatGPT-175B) are overwhelmingly high and impractical for real sequential recommender systems. In this paper, we propose a novel Step-by-step knowLedge dIstillation fraMework for recommendation (SLIM), paving a promising path for sequential recommenders to enjoy the exceptional reasoning capabilities of LLMs in a "slim" (i.e., resource-efficient) manner. We introduce CoT prompting based on user behavior sequences for the larg
    
[^64]: 分散且公平的最优输运

    Decentralized and Equitable Optimal Transport

    [https://arxiv.org/abs/2403.04259](https://arxiv.org/abs/2403.04259)

    本文提出了分散的最优输运问题和分散公平最优输运问题，引入了具有迭代复杂度为O(1/{\epsilon})的单循环分散算法，以匹配现有的集中式一阶方法，并改进了现有集中式算法的迭代复杂度。

    

    本文考虑了分散（离散）最优输运（D-OT）问题。在这种情况下，一组代理人共同设计运输方案，其中成本函数是每个代理人持有的成本之和。我们将D-OT问题重新表述为约束耦合优化问题，并提出了一种具有迭代复杂度为O(1/{\epsilon})的单循环分散算法，与现有的集中式一阶方法相匹配。此外，我们提出了分散公平最优输运（DE-OT）问题。在DE-OT中，代理不仅协作设计最小化运输成本的运输计划，还努力确保各自成本的公平性。解决DE-OT的方法的迭代复杂度也是O(1/{\epsilon})，这一速率提高了现有的集中式算法，其中获得的最佳迭代复杂度为O(1/{\epsilon}^2)。

    arXiv:2403.04259v1 Announce Type: cross  Abstract: This paper considers the decentralized (discrete) optimal transport (D-OT) problem. In this setting, a network of agents seeks to design a transportation plan jointly, where the cost function is the sum of privately held costs for each agent. We reformulate the D-OT problem as a constraint-coupled optimization problem and propose a single-loop decentralized algorithm with an iteration complexity of O(1/{\epsilon}) that matches existing centralized first-order approaches. Moreover, we propose the decentralized equitable optimal transport (DE-OT) problem. In DE-OT, in addition to cooperatively designing a transportation plan that minimizes transportation costs, agents seek to ensure equity in their individual costs. The iteration complexity of the proposed method to solve DE-OT is also O(1/{\epsilon}). This rate improves existing centralized algorithms, where the best iteration complexity obtained is O(1/{\epsilon}^2).
    
[^65]: 用世界模型掌握记忆任务

    Mastering Memory Tasks with World Models

    [https://arxiv.org/abs/2403.04253](https://arxiv.org/abs/2403.04253)

    通过将一类新的状态空间模型整合到基于模型的强化学习代理的世界模型中，提出了一种新方法，Recall to Imagine（R2I），旨在增强长期记忆和长距离奖励分配，实现了在记忆任务和奖励分配方面的超越表现。

    

    当前基于模型的强化学习（MBRL）代理在处理长期依赖性方面存在困难，这限制了它们有效解决涉及动作和结果之间存在时间间隔或需要回想远距离观察以指导当前动作的任务的能力。为了改善时间上的一致性，我们将一类新的状态空间模型（SSMs）整合到MBRL代理的世界模型中，提出了一种新方法，Recall to Imagine（R2I）。这种整合旨在增强长期记忆和长距离奖励分配。通过一系列多样化的示例任务，我们系统地展示了R2I不仅在具有挑战性的记忆和奖励分配强化学习任务中确立了一个新的最先进水平，如BSuite和POPGym，还展示了在Memory Maze这一复杂的记忆领域中超越人类的表现。同时，在经典的强化学习任务，例如Atari和DMC中，它保持了可以媲美的性能。

    arXiv:2403.04253v1 Announce Type: new  Abstract: Current model-based reinforcement learning (MBRL) agents struggle with long-term dependencies. This limits their ability to effectively solve tasks involving extended time gaps between actions and outcomes, or tasks demanding the recalling of distant observations to inform current actions. To improve temporal coherence, we integrate a new family of state space models (SSMs) in world models of MBRL agents to present a new method, Recall to Imagine (R2I). This integration aims to enhance both long-term memory and long-horizon credit assignment. Through a diverse set of illustrative tasks, we systematically demonstrate that R2I not only establishes a new state-of-the-art for challenging memory and credit assignment RL tasks, such as BSuite and POPGym, but also showcases superhuman performance in the complex memory domain of Memory Maze. At the same time, it upholds comparable performance in classic RL tasks, such as Atari and DMC, suggestin
    
[^66]: 基于CNN-LSTM的Levy驱动随机微分方程参数估计的高效性研究

    Efficient CNN-LSTM based Parameter Estimation of Levy Driven Stochastic Differential Equations

    [https://arxiv.org/abs/2403.04246](https://arxiv.org/abs/2403.04246)

    本研究提出了一种新颖的CNN-LSTM三阶段模型PEnet，通过CNN对数据特征进行浓缩，提供了一种端到端方法，具有高准确性和适应性，以及长序列观测的增强推断速度和高泛化能力，从而在估计SDE方面取得了显著优势。

    

    本研究解决了由非高斯噪声驱动的随机微分方程参数估计中的挑战，这对于理解价格波动和传染病传播等动态现象至关重要。先前的研究强调了LSTM网络在估计alpha稳定Levy驱动的SDE参数方面的潜力，但面临高时间复杂度和LSTM链接属性的限制。为了缓解这些问题，我们引入了PEnet，这是一种基于CNN-LSTM的三阶段模型，提供了一种端到端方法，具有优越的准确性和适应不同数据结构的能力，通过CNN对初始数据特征进行浓缩，为长序列观测提供增强的推断速度，并具有高泛化能力，允许其应用于各种复杂的SDE场景。对合成数据集的实验验证了PEnet在估计SDE中的显著优势。

    arXiv:2403.04246v1 Announce Type: cross  Abstract: This study addresses the challenges in parameter estimation of stochastic differential equations driven by non-Gaussian noises, which are critical in understanding dynamic phenomena such as price fluctuations and the spread of infectious diseases. Previous research highlighted the potential of LSTM networks in estimating parameters of alpha stable Levy driven SDEs but faced limitations including high time complexity and constraints of the LSTM chaining property. To mitigate these issues, we introduce the PEnet, a novel CNN-LSTM-based three-stage model that offers an end to end approach with superior accuracy and adaptability to varying data structures, enhanced inference speed for long sequence observations through initial data feature condensation by CNN, and high generalization capability, allowing its application to various complex SDE scenarios. Experiments on synthetic datasets confirm PEnet significant advantage in estimating SDE
    
[^67]: 研究丢失视频帧导致的辅助模态偏差对音视频语音识别的影响

    A Study of Dropout-Induced Modality Bias on Robustness to Missing Video Frames for Audio-Visual Speech Recognition

    [https://arxiv.org/abs/2403.04245](https://arxiv.org/abs/2403.04245)

    本文研究了在音视频语音识别中丢失视频帧导致的辅助模态偏差对系统稳健性的影响，并提出了模态偏差假设，进一步提出了一种多模态分布近似框架，以减少对音频模态的过度依赖，并保持性能和稳健性。

    

    高级音视频语音识别系统已被观察到对丢失视频帧敏感，甚至表现比单模态模型更差。尽管将辍学技术应用于视频模态可以增强对丢失帧的稳健性，但同时在处理完整数据输入时会导致性能损失。本文从模态偏差的角度调查了这一矛盾现象，并揭示了辍学导致的对音频的过度模态偏差是潜在原因。此外，我们提出了模态偏差假设（MBH），系统地描述了多模态系统中模态偏差与对丢失模态的稳健性之间的关系。基于这些发现，我们提出了一种新颖的基于知识蒸馏的多模态分布近似（MDA-KD）框架，以减少对音频模态的过度依赖，并保持性能和稳健性。

    arXiv:2403.04245v1 Announce Type: cross  Abstract: Advanced Audio-Visual Speech Recognition (AVSR) systems have been observed to be sensitive to missing video frames, performing even worse than single-modality models. While applying the dropout technique to the video modality enhances robustness to missing frames, it simultaneously results in a performance loss when dealing with complete data input. In this paper, we investigate this contrasting phenomenon from the perspective of modality bias and reveal that an excessive modality bias on the audio caused by dropout is the underlying reason. Moreover, we present the Modality Bias Hypothesis (MBH) to systematically describe the relationship between modality bias and robustness against missing modality in multimodal systems. Building on these findings, we propose a novel Multimodal Distribution Approximation with Knowledge Distillation (MDA-KD) framework to reduce over-reliance on the audio modality and to maintain performance and robust
    
[^68]: 具有模型选择的正则化DeepIV

    Regularized DeepIV with Model Selection

    [https://arxiv.org/abs/2403.04236](https://arxiv.org/abs/2403.04236)

    本文提出了一种名为正则化DeepIV（RDIV）回归的新方法，能够避免IV回归唯一标识、极小极大计算预言和缺乏模型选择过程等限制，同时实现了通用函数逼近。

    

    在这篇论文中，我们研究了工具变量（IV）回归的非参数估计。虽然机器学习的最新进展已经引入了灵活的IV估计方法，但它们往往会遇到以下一个或多个限制：（1）将IV回归限制为唯一标识；（2）需要极小极大计算预言，这在实践中非常不稳定；（3）缺乏模型选择过程。在本文中，我们提出了一种可以避免所有三个限制的第一种方法和分析，同时仍然能够实现通用函数逼近。具体而言，我们提出了一种名为正则化DeepIV（RDIV）回归的无极小极大计算预言的方法，可以收敛到最小范数IV解。我们的方法分为两个阶段：首先，我们学习协变量的条件分布，并通过利用所学到的分布，通过最小化Tikhonov正则化损失函数来学习估计值。我们进一步...

    arXiv:2403.04236v1 Announce Type: new  Abstract: In this paper, we study nonparametric estimation of instrumental variable (IV) regressions. While recent advancements in machine learning have introduced flexible methods for IV estimation, they often encounter one or more of the following limitations: (1) restricting the IV regression to be uniquely identified; (2) requiring minimax computation oracle, which is highly unstable in practice; (3) absence of model selection procedure. In this paper, we present the first method and analysis that can avoid all three limitations, while still enabling general function approximation. Specifically, we propose a minimax-oracle-free method called Regularized DeepIV (RDIV) regression that can converge to the least-norm IV solution. Our method consists of two stages: first, we learn the conditional distribution of covariates, and by utilizing the learned distribution, we learn the estimator by minimizing a Tikhonov-regularized loss function. We furth
    
[^69]: 非线性低秩矩阵估计的基本限制

    Fundamental limits of Non-Linear Low-Rank Matrix Estimation

    [https://arxiv.org/abs/2403.04234](https://arxiv.org/abs/2403.04234)

    证明了贝叶斯最优性能由等效高斯模型和有效先验确定，信号重构需要增长的信噪比条件，并提供了针对非线性低秩矩阵估计问题的渐近误差特征化和近似传递消息算法。

    

    我们考虑从非线性和嘈杂观测中估计低秩矩阵的任务。我们证明了一个强大的普适性结果，表明贝叶斯最优表现由一个等效高斯模型和一个有效先验所决定，其参数完全由非线性函数的展开确定。具体而言，我们表明为了准确重构信号，信噪比需要增长为$N^{\frac 12 (1-1/k_F)}$，其中$k_F$是函数的第一个非零费舍尔信息系数。我们提供了最小可达均方误差（MMSE）的渐近特征化和一种近似传递消息算法，该算法在类似于问题的线性版本的情况下达到了MMSE。我们还提供了通过方法例如主成分分析结合贝叶斯降噪实现的渐近误差，并将它们与贝叶斯最优MMSE进行比较。

    arXiv:2403.04234v1 Announce Type: cross  Abstract: We consider the task of estimating a low-rank matrix from non-linear and noisy observations. We prove a strong universality result showing that Bayes-optimal performances are characterized by an equivalent Gaussian model with an effective prior, whose parameters are entirely determined by an expansion of the non-linear function. In particular, we show that to reconstruct the signal accurately, one requires a signal-to-noise ratio growing as $N^{\frac 12 (1-1/k_F)}$, where $k_F$ is the first non-zero Fisher information coefficient of the function. We provide asymptotic characterization for the minimal achievable mean squared error (MMSE) and an approximate message-passing algorithm that reaches the MMSE under conditions analogous to the linear version of the problem. We also provide asymptotic errors achieved by methods such as principal component analysis combined with Bayesian denoising, and compare them with Bayes-optimal MMSE.
    
[^70]: 通过多残差任务学习实现协同生态驾驶的泛化

    Generalizing Cooperative Eco-driving via Multi-residual Task Learning

    [https://arxiv.org/abs/2403.04232](https://arxiv.org/abs/2403.04232)

    介绍了基于多残差任务学习的泛化协同生态驾驶方法，通过分解控制并运用学习来解决多个交通场景的挑战

    

    传统控制，如基于模型的控制，在自动驾驶中通常被使用，因为其效率和可靠性。然而，现实世界中的自动驾驶要应对各种具有挑战性的不同交通场景，这对这些规划算法是具有挑战性的。基于模型无关的深度强化学习（DRL）在这方面提供了一个有前景的途径，但学习DRL控制策略以泛化到多个交通场景仍然是一个挑战。为了解决这个问题，我们引入了一种名为多残差任务学习（MRTL）的通用学习框架，它是基于多任务学习的，针对一组任务场景，将控制分解为有效由传统控制方法解决的名义分量和由学习解决的残差项。我们使用MRTL来在混合交通中通过使用自动驾驶车辆来实现车队级别的排放减少作为系统控制手段。通过分析MR性能

    arXiv:2403.04232v1 Announce Type: cross  Abstract: Conventional control, such as model-based control, is commonly utilized in autonomous driving due to its efficiency and reliability. However, real-world autonomous driving contends with a multitude of diverse traffic scenarios that are challenging for these planning algorithms. Model-free Deep Reinforcement Learning (DRL) presents a promising avenue in this direction, but learning DRL control policies that generalize to multiple traffic scenarios is still a challenge. To address this, we introduce Multi-residual Task Learning (MRTL), a generic learning framework based on multi-task learning that, for a set of task scenarios, decomposes the control into nominal components that are effectively solved by conventional control methods and residual terms which are solved using learning. We employ MRTL for fleet-level emission reduction in mixed traffic using autonomous vehicles as a means of system control. By analyzing the performance of MR
    
[^71]: Aligners: 解耦LLMs和对齐

    Aligners: Decoupling LLMs and Alignment

    [https://arxiv.org/abs/2403.04224](https://arxiv.org/abs/2403.04224)

    提出了一种通过训练对齐器模型来解耦大型语言模型（LLMs）和对齐，以减少对齐对性能的潜在负面影响。

    

    大型语言模型（LLMs）需要与人类期望对齐，以确保它们在大多数应用中的安全性和实用性。对齐具有挑战性，成本高昂，并且需要为每个LLM和对齐标准重复进行。我们建议通过训练可以根据需要用于对齐给定标准的任何LLM的对齐模型来解耦LLMs和对齐，从而在一定程度上减少对性能的潜在负面影响。我们提出的对齐模型训练配方仅依赖于使用（提示的）LLM 生成的合成数据，并且可以轻松调整以适应各种对齐标准。我们通过训练一个“道德”对齐器并在实验上验证其有效性来阐明我们的方法。

    arXiv:2403.04224v1 Announce Type: cross  Abstract: Large Language Models (LLMs) need to be aligned with human expectations to ensure their safety and utility in most applications. Alignment is challenging, costly, and needs to be repeated for every LLM and alignment criterion. We propose to decouple LLMs and alignment by training aligner models that can be used to align any LLM for a given criteria on an as-needed basis, thus also reducing the potential negative impacts of alignment on performance. Our recipe for training the aligner models solely relies on synthetic data generated with a (prompted) LLM and can be easily adjusted for a variety of alignment criteria. We illustrate our method by training an "ethical" aligner and verify its efficacy empirically.
    
[^72]: 在线强化学习为何具有因果性

    Why Online Reinforcement Learning is Causal

    [https://arxiv.org/abs/2403.04221](https://arxiv.org/abs/2403.04221)

    强化学习和因果建模相互补充，论文主要指出在线学习环境下，条件概率具有因果性，离线RL是因果学习潜力最大的环境。

    

    强化学习（RL）和因果建模自然互补。因果建模的目标是预测在环境中进行干预的效果，而强化学习的目标是选择最大化代理从环境中接收的奖励的干预。强化学习包括用于估计因果关系的两个最强大信息源：时间顺序和对环境进行操作的能力。本文研究了我们可以期望在哪些强化学习设置中从因果建模中受益，以及如何受益。在线学习中，代理有能力直接与环境进行交互，并从探索中学习。我们的主要论点是，在在线学习中，条件概率是因果的，因此离线RL是因果学习有潜力产生差异的环境。基本上，原因在于当代理与环境互动时，代理的行为是由其对环境的认识所推动的。

    arXiv:2403.04221v1 Announce Type: cross  Abstract: Reinforcement learning (RL) and causal modelling naturally complement each other. The goal of causal modelling is to predict the effects of interventions in an environment, while the goal of reinforcement learning is to select interventions that maximize the rewards the agent receives from the environment. Reinforcement learning includes the two most powerful sources of information for estimating causal relationships: temporal ordering and the ability to act on an environment. This paper examines which reinforcement learning settings we can expect to benefit from causal modelling, and how. In online learning, the agent has the ability to interact directly with their environment, and learn from exploring it. Our main argument is that in online learning, conditional probabilities are causal, and therefore offline RL is the setting where causal learning has the most potential to make a difference. Essentially, the reason is that when an a
    
[^73]: HeteroSwitch：对联邦学习中系统诱导数据异质性的表征与调控

    HeteroSwitch: Characterizing and Taming System-Induced Data Heterogeneity in Federated Learning

    [https://arxiv.org/abs/2403.04207](https://arxiv.org/abs/2403.04207)

    该论文研究了在联邦学习中由于设备硬件和软件配置的不同导致的系统诱导数据异质性，证明了这种异质性对模型的性能产生负面影响，并提出了名为HeteroSwitch的解决方案。

    

    联邦学习（FL）是一种实用的方法，可以跨用户设备协作训练深度学习模型，通过在设备上保留原始数据来保护用户隐私。在FL中，参与的用户设备在硬件和软件配置方面高度碎片化。这种碎片化引入了一种新型数据异质性，即\textit{系统诱导的数据异质性}，因为每个设备根据其硬件和软件配置生成不同的数据。在本文中，我们首先表征了系统诱导数据异质性对FL模型性能的影响。我们使用具有跨供应商和性能层级变化的异构设备收集了数据集。通过使用这个数据集，我们证明了\textit{系统诱导的数据异质性}对准确性产生负面影响，并恶化了FL中的公平性和域泛化问题。为了解决这些挑战，我们提出了HeteroSwitch，

    arXiv:2403.04207v1 Announce Type: new  Abstract: Federated Learning (FL) is a practical approach to train deep learning models collaboratively across user-end devices, protecting user privacy by retaining raw data on-device. In FL, participating user-end devices are highly fragmented in terms of hardware and software configurations. Such fragmentation introduces a new type of data heterogeneity in FL, namely \textit{system-induced data heterogeneity}, as each device generates distinct data depending on its hardware and software configurations. In this paper, we first characterize the impact of system-induced data heterogeneity on FL model performance. We collect a dataset using heterogeneous devices with variations across vendors and performance tiers. By using this dataset, we demonstrate that \textit{system-induced data heterogeneity} negatively impacts accuracy, and deteriorates fairness and domain generalization problems in FL. To address these challenges, we propose HeteroSwitch, 
    
[^74]: GRAWA: 基于梯度的加权平均用于深度学习模型的分布式训练

    GRAWA: Gradient-based Weighted Averaging for Distributed Training of Deep Learning Models

    [https://arxiv.org/abs/2403.04206](https://arxiv.org/abs/2403.04206)

    提出了一种新的基于梯度的加权平均算法，以恢复优化景观中的平坦区域为优先，实验结果表明该算法在分布式训练中取得了更快的收敛速度和更好的质量

    

    我们研究了在时间受限的环境中的深度学习模型的分布式训练。我们提出了一种新算法，周期性地将工作者拉向作为工作者加权平均值计算的中心变量，其中权重与工作者的梯度范数成反比，从而优先恢复优化景观中的平坦区域。我们开发了所提算法的两种异步变体，分别称为模型级和层级梯度加权平均（MGRAWA和LGRAWA），在加权方案上有所不同，要么是针对整个模型进行，要么是逐层应用。在理论上，我们证明了所提方法在凸性和非凸性环境中的收敛保证。然后，我们通过实验证明了我们的算法通过实现更快的收敛速度和恢复更好的质量而胜过竞争方法。

    arXiv:2403.04206v1 Announce Type: new  Abstract: We study distributed training of deep learning models in time-constrained environments. We propose a new algorithm that periodically pulls workers towards the center variable computed as a weighted average of workers, where the weights are inversely proportional to the gradient norms of the workers such that recovering the flat regions in the optimization landscape is prioritized. We develop two asynchronous variants of the proposed algorithm that we call Model-level and Layer-level Gradient-based Weighted Averaging (resp. MGRAWA and LGRAWA), which differ in terms of the weighting scheme that is either done with respect to the entire model or is applied layer-wise. On the theoretical front, we prove the convergence guarantee for the proposed approach in both convex and non-convex settings. We then experimentally demonstrate that our algorithms outperform the competitor methods by achieving faster convergence and recovering better quality
    
[^75]: 异质学习代理群体中道德行为动态

    Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents

    [https://arxiv.org/abs/2403.04202](https://arxiv.org/abs/2403.04202)

    在多代理环境中，研究人员探讨了不同道德类型的学习代理之间的互动，发现道德异质性可能对代理的共同发展产生影响。

    

    arXiv:2403.04202v1 公告类型：交叉领域 摘要：日益关注AI系统安全和对齐性的问题突显了在人工代理中嵌入道德能力的重要性。一种有前途的解决方案是利用经验学习，即强化学习。在多代理（社会）环境中，个体学习代理之间的交互可能产生复杂的群体层面现象。许多现有研究依赖于模拟的社会困境环境来研究独立学习代理的互动。然而，它们往往忽视了实践中代理社会中可能存在的道德异质性。例如，在不同时间点，单个学习代理可能面对后果主义者（即关心随时间最大化某种结果）或基于规范的对手（即专注于立即遵守特定规范） 。代理的共同发展在多大程度上可能受到这种道德异质性的影响。

    arXiv:2403.04202v1 Announce Type: cross  Abstract: Growing concerns about safety and alignment of AI systems highlight the importance of embedding moral capabilities in artificial agents. A promising solution is the use of learning from experience, i.e., Reinforcement Learning. In multi-agent (social) environments, complex population-level phenomena may emerge from interactions between individual learning agents. Many of the existing studies rely on simulated social dilemma environments to study the interactions of independent learning agents. However, they tend to ignore the moral heterogeneity that is likely to be present in societies of agents in practice. For example, at different points in time a single learning agent may face opponents who are consequentialist (i.e., caring about maximizing some outcome over time) or norm-based (i.e., focusing on conforming to a specific norm here and now). The extent to which agents' co-development may be impacted by such moral heterogeneity in 
    
[^76]: 《装填与溢出：用于水库运行决策与控制的深度强化学习策略梯度方法》

    Fill-and-Spill: Deep Reinforcement Learning Policy Gradient Methods for Reservoir Operation Decision and Control

    [https://arxiv.org/abs/2403.04195](https://arxiv.org/abs/2403.04195)

    深度强化学习方法应用于水库运行决策与控制，克服了动态规划和随机动态规划在高分辨率下面临的挑战，解决了“维度诅咒”问题。

    

    需求变化、各种水文输入和环境压力是水资源管理者和决策者经常面临的问题之一。 这些问题引起人们对应用不同技术来确定水库运行政策决策的兴趣。 随着分析的分辨率提高，使用传统方法如动态规划（DP）和随机动态规划（SDP）来确定最佳水库运行政策越来越困难，因为这些方法难以有效地表示真实世界系统。 其中一个挑战是“维度诅咒”，这意味着估计具有给定精度水平任意函数所需的样本数量随着函数的输入变量数量（即维数）呈指数级增长。 深度强化学习（DRL）是一种智能方法，可以克服水库随机优化问题的诅咒。

    arXiv:2403.04195v1 Announce Type: new  Abstract: Changes in demand, various hydrological inputs, and environmental stressors are among the issues that water managers and policymakers face on a regular basis. These concerns have sparked interest in applying different techniques to determine reservoir operation policy decisions. As the resolution of the analysis increases, it becomes more difficult to effectively represent a real-world system using traditional methods such as Dynamic Programming (DP) and Stochastic Dynamic Programming (SDP) for determining the best reservoir operation policy. One of the challenges is the "curse of dimensionality," which means the number of samples needed to estimate an arbitrary function with a given level of accuracy grows exponentially with respect to the number of input variables (i.e., dimensionality) of the function. Deep Reinforcement Learning (DRL) is an intelligent approach to overcome the curses of stochastic optimization problems for reservoir 
    
[^77]: 生成式人工智能用于合成数据生成：方法、挑战和未来展望

    Generative AI for Synthetic Data Generation: Methods, Challenges and the Future

    [https://arxiv.org/abs/2403.04190](https://arxiv.org/abs/2403.04190)

    这里是中文总结出的一句话要点

    

    最近，关于从大型语言模型（LLMs）生成合成数据的研究急剧增加，特别是针对数据有限的情况，标志着生成式人工智能（AI）领域的一个显著转变。它们能够表现出与真实世界数据相媲美的能力，将这种方法定位为解决低资源挑战的一个引人注目的解决方案。本文深入探讨了利用这些庞大的LLMs生成特定任务训练数据的先进技术。我们概述了方法论、评估技术和实际应用，讨论了当前的局限性，并提出了未来研究的潜在途径。

    arXiv:2403.04190v1 Announce Type: cross  Abstract: The recent surge in research focused on generating synthetic data from large language models (LLMs), especially for scenarios with limited data availability, marks a notable shift in Generative Artificial Intelligence (AI). Their ability to perform comparably to real-world data positions this approach as a compelling solution to low-resource challenges. This paper delves into advanced technologies that leverage these gigantic LLMs for the generation of task-specific training data. We outline methodologies, evaluation techniques, and practical applications, discuss the current limitations, and suggest potential pathways for future research.
    
[^78]: 克服规模化机器学习硬件加速器通信瓶颈的硅光子2.5D互连网络

    Silicon Photonic 2.5D Interposer Networks for Overcoming Communication Bottlenecks in Scale-out Machine Learning Hardware Accelerators

    [https://arxiv.org/abs/2403.04189](https://arxiv.org/abs/2403.04189)

    这项研究提出了在2.5D平台中利用光通信和计算的方法，实现能效高、吞吐量高的机器学习加速器体系结构。

    

    现代机器学习（ML）应用程序变得越来越复杂和单片加速器体系结构无法满足其能效和吞吐要求。尽管现代数字电子加速器逐渐采用具有多个较小芯片组的2.5D体系结构以提高可扩展性，但由于依赖缓慢的金属互连而面临基本限制。本文概述了如何在2.5D平台中利用光通信和计算，实现能效高、吞吐量高的2.5D ML加速器体系结构。

    arXiv:2403.04189v1 Announce Type: cross  Abstract: Modern machine learning (ML) applications are becoming increasingly complex and monolithic (single chip) accelerator architectures cannot keep up with their energy efficiency and throughput demands. Even though modern digital electronic accelerators are gradually adopting 2.5D architectures with multiple smaller chiplets to improve scalability, they face fundamental limitations due to a reliance on slow metallic interconnects. This paper outlines how optical communication and computation can be leveraged in 2.5D platforms to realize energy-efficient and high throughput 2.5D ML accelerator architectures.
    
[^79]: RATSF：通过检索增强时间序列预测来赋能客服量管理

    RATSF: Empowering Customer Service Volume Management through Retrieval-Augmented Time-Series Forecasting

    [https://arxiv.org/abs/2403.04180](https://arxiv.org/abs/2403.04180)

    提出了一种检索增强时序序列预测框架(RATSF)，通过引入交叉注意力模块(RACA)及知识库设计，有效利用历史数据段进行客服量预测，在非平稳数据情况下显著提升了性能。

    

    一个高效的客服管理系统取决于对服务量的精确预测。在这种数据非平稳性明显的情况下，成功的预测严重依赖于识别和利用类似的历史数据，而不仅仅是总结周期性模式。现有基于RNN或Transformer架构的模型通常在灵活和有效利用方面存在挑战。为了解决这一挑战，我们提出了一种高效且可适应的交叉注意力模块，称为RACA，它在预测任务中有效利用了历史段，并设计了一个精确的历史序列查询表示方案，结合了知识库的设计。这些关键组件共同构成了我们的检索增强时序序列预测框架（RATSF）。RATSF不仅在菲鸡酒店服务量预测环境中显著增强了性能，而且...

    arXiv:2403.04180v1 Announce Type: new  Abstract: An efficient customer service management system hinges on precise forecasting of service volume. In this scenario, where data non-stationarity is pronounced, successful forecasting heavily relies on identifying and leveraging similar historical data rather than merely summarizing periodic patterns. Existing models based on RNN or Transformer architectures often struggle with this flexible and effective utilization. To address this challenge, we propose an efficient and adaptable cross-attention module termed RACA, which effectively leverages historical segments in forecasting task, and we devised a precise representation scheme for querying historical sequences, coupled with the design of a knowledge repository. These critical components collectively form our Retrieval-Augmented Temporal Sequence Forecasting framework (RATSF). RATSF not only significantly enhances performance in the context of Fliggy hotel service volume forecasting but,
    
[^80]: 用于探索的噪声尖峰演员网络

    Noisy Spiking Actor Network for Exploration

    [https://arxiv.org/abs/2403.04162](https://arxiv.org/abs/2403.04162)

    提出了一种噪声尖峰演员网络（NoisySAN）以解决尖峰神经网络（SNNs）在探索中的弱点，并在连续控制任务上取得了优于最先进方法的表现

    

    作为深度强化学习中探索的一种通用方法，NoisyNet能够产生特定于问题的探索策略。由于尖峰神经网络（SNNs）具有二进制发放机制，对于噪声具有很强的鲁棒性，因此难以通过局部干扰实现高效探索。为了解决这个探索问题，我们提出了一种引入时间相关噪声的噪声尖峰演员网络（NoisySAN）。此外，还提出了一种噪声减少方法来为代理找到稳定策略。大量实验结果表明，我们的方法在来自OpenAI gym的广泛连续控制任务上优于最先进的性能。

    arXiv:2403.04162v1 Announce Type: new  Abstract: As a general method for exploration in deep reinforcement learning (RL), NoisyNet can produce problem-specific exploration strategies. Spiking neural networks (SNNs), due to their binary firing mechanism, have strong robustness to noise, making it difficult to realize efficient exploration with local disturbances. To solve this exploration problem, we propose a noisy spiking actor network (NoisySAN) that introduces time-correlated noise during charging and transmission. Moreover, a noise reduction method is proposed to find a stable policy for the agent. Extensive experimental results demonstrate that our method outperforms the state-of-the-art performance on a wide range of continuous control tasks from OpenAI gym.
    
[^81]: SWAP-NAS: 适用于超快速NAS的样本级激活模式

    SWAP-NAS: Sample-Wise Activation Patterns For Ultra-Fast NAS

    [https://arxiv.org/abs/2403.04161](https://arxiv.org/abs/2403.04161)

    提出了一种新颖的高性能无需训练的度量SWAP-Score，能够在不同搜索空间和任务中测量网络在一批输入样本上的表现能力，并通过正则化进一步提高相关性，实现模型大小的控制。

    

    无需训练的度量（即零成本代理）被广泛用于避免资源密集型的神经网络训练，尤其是在神经结构搜索（NAS）中。最近的研究表明，现有的无需训练的度量存在一些局限，比如在不同搜索空间和任务之间存在有限的关联性和差劲的泛化能力。因此，我们提出了样本级激活模式及其衍生物SWAP-Score，这是一种新颖的高性能无需训练的度量。它测量了网络在一批输入样本上的表现能力。SWAP-Score与不同搜索空间和任务中的真实性能强相关，在NAS-Bench-101/201/301和TransNAS-Bench-101上胜过了15种现有的无需训练的度量。SWAP-Score可以通过正则化进一步增强，这在基于单元的搜索空间中可以实现更高的相关性，并且在搜索过程中实现模型大小控制。例如，Spearman的排序

    arXiv:2403.04161v1 Announce Type: new  Abstract: Training-free metrics (a.k.a. zero-cost proxies) are widely used to avoid resource-intensive neural network training, especially in Neural Architecture Search (NAS). Recent studies show that existing training-free metrics have several limitations, such as limited correlation and poor generalisation across different search spaces and tasks. Hence, we propose Sample-Wise Activation Patterns and its derivative, SWAP-Score, a novel high-performance training-free metric. It measures the expressivity of networks over a batch of input samples. The SWAP-Score is strongly correlated with ground-truth performance across various search spaces and tasks, outperforming 15 existing training-free metrics on NAS-Bench-101/201/301 and TransNAS-Bench-101. The SWAP-Score can be further enhanced by regularisation, which leads to even higher correlations in cell-based search space and enables model size control during the search. For example, Spearman's rank
    
[^82]: 通过与扰动过程一致性稳定随机微分方程的策略梯度

    Stabilizing Policy Gradients for Stochastic Differential Equations via Consistency with Perturbation Process

    [https://arxiv.org/abs/2403.04154](https://arxiv.org/abs/2403.04154)

    通过与SDE相关的扰动过程一致性，稳定策略梯度，提高样本效率。

    

    考虑产生高回报样本，我们专注于优化参数化为深度神经网络的随机微分方程（SDEs），这是具有高可表达性的先进生成模型，利用强化学习中的主导算法策略梯度。然而，当将策略梯度应用于SDE时，由于策略梯度是在有限轨迹集上估计的，它可能是不明确定义的，并且数据稀缺区域的策略行为可能无法控制。这一挑战影响了策略梯度的稳定性，并对样本复杂度产生了负面影响。为了解决这些问题，我们提出将SDE限制为与其相关的扰动过程保持一致。由于扰动过程覆盖整个空间且易于抽样，我们可以缓解前述问题。我们的框架提供了一种通用方法，允许灵活选择策略梯度方法以有效。

    arXiv:2403.04154v1 Announce Type: new  Abstract: Considering generating samples with high rewards, we focus on optimizing deep neural networks parameterized stochastic differential equations (SDEs), the advanced generative models with high expressiveness, with policy gradient, the leading algorithm in reinforcement learning. Nevertheless, when applying policy gradients to SDEs, since the policy gradient is estimated on a finite set of trajectories, it can be ill-defined, and the policy behavior in data-scarce regions may be uncontrolled. This challenge compromises the stability of policy gradients and negatively impacts sample complexity. To address these issues, we propose constraining the SDE to be consistent with its associated perturbation process. Since the perturbation process covers the entire space and is easy to sample, we can mitigate the aforementioned problems. Our framework offers a general approach allowing for a versatile selection of policy gradient methods to effective
    
[^83]: FL-GUARD: 一个用于负面联邦学习运行时检测和恢复的全面框架

    FL-GUARD: A Holistic Framework for Run-Time Detection and Recovery of Negative Federated Learning

    [https://arxiv.org/abs/2403.04146](https://arxiv.org/abs/2403.04146)

    FL-GUARD提出了一个全面框架，用于在负面联邦学习情况下检测和恢复，解决了先前解决方案中的额外成本和浪费学习轮次的问题

    

    联邦学习（FL）是一种从分布在大量客户端上的数据中学习模型且不暴露数据隐私的有前途的方法。在理想的联邦学习中效果显著，其中客户端分享同质的数据分布和学习行为。然而，当联邦学习不理想时，FL可能无法正常运作，导致负面联邦学习（NFL）的不良状态。许多研究尝试解决NFL问题。然而，它们的解决方案要么（1）预先防止整个学习生命周期中的NFL，要么（2）在大量学习轮次之后解决NFL。因此，它们要么（1）在FL没有这些成本的情况下无差别增加额外成本，要么（2）浪费大量学习轮次。另外，先前的工作没有考虑可能不愿意/无法遵循提出的NFL解决方案的客户端。

    arXiv:2403.04146v1 Announce Type: cross  Abstract: Federated learning (FL) is a promising approach for learning a model from data distributed on massive clients without exposing data privacy. It works effectively in the ideal federation where clients share homogeneous data distribution and learning behavior. However, FL may fail to function appropriately when the federation is not ideal, amid an unhealthy state called Negative Federated Learning (NFL), in which most clients gain no benefit from participating in FL. Many studies have tried to address NFL. However, their solutions either (1) predetermine to prevent NFL in the entire learning life-cycle or (2) tackle NFL in the aftermath of numerous learning rounds. Thus, they either (1) indiscriminately incur extra costs even if FL can perform well without such costs or (2) waste numerous learning rounds. Additionally, none of the previous work takes into account the clients who may be unwilling/unable to follow the proposed NFL solution
    
[^84]: FedClust：通过基于权重驱动的客户端聚类优化非独立同分布数据上的联邦学习

    FedClust: Optimizing Federated Learning on Non-IID Data through Weight-Driven Client Clustering

    [https://arxiv.org/abs/2403.04144](https://arxiv.org/abs/2403.04144)

    该论文提出了FedClust，一种新颖的集群化联邦学习方法，通过利用本地模型权重和客户端数据分布之间的相关性，在一次性的操作中将客户端分组成集群。

    

    联邦学习（FL）是一种新兴的分布式机器学习范式，实现了在分散设备上进行协作模型训练，并不会暴露它们的本地数据。在FL中，一个关键挑战是客户端设备之间的数据分布不均匀，违反了传统机器学习中独立同分布（IID）训练样本的假设。集群化的联邦学习（CFL）通过根据数据分布的相似性对客户端进行分组来解决这一挑战。然而，现有的CFL方法需要大量的通信往返来稳定集群形成，并且依赖预定义的集群数量，从而限制了它们的灵活性和适应性。本文提出了FedClust，一种新颖的CFL方法，利用本地模型权重与客户端数据分布之间的相关性。FedClust使用策略性选择的参数一次性将客户端分组成集群。

    arXiv:2403.04144v1 Announce Type: cross  Abstract: Federated learning (FL) is an emerging distributed machine learning paradigm enabling collaborative model training on decentralized devices without exposing their local data. A key challenge in FL is the uneven data distribution across client devices, violating the well-known assumption of independent-and-identically-distributed (IID) training samples in conventional machine learning. Clustered federated learning (CFL) addresses this challenge by grouping clients based on the similarity of their data distributions. However, existing CFL approaches require a large number of communication rounds for stable cluster formation and rely on a predefined number of clusters, thus limiting their flexibility and adaptability. This paper proposes FedClust, a novel CFL approach leveraging correlations between local model weights and client data distributions. FedClust groups clients into clusters in a one-shot manner using strategically selected pa
    
[^85]: 探索基于LLM的代理用于根本原因分析

    Exploring LLM-based Agents for Root Cause Analysis

    [https://arxiv.org/abs/2403.04123](https://arxiv.org/abs/2403.04123)

    探索基于LLM的代理用于根本原因分析，以解决自动化RCA中无法动态收集额外诊断信息的限制。

    

    云软件系统越来越复杂，导致事件管理已成为软件开发生命周期的一个重要组成部分。根本原因分析（RCA）是事件管理过程的关键部分，对值班工程师来说是一项严峻的任务，需要深入的领域知识和对团队特定服务的广泛经验。自动化RCA可以显著节省时间，并减轻值班工程师在事件管理上的负担。最近，研究人员利用大型语言模型（LLMs）执行RCA，并取得了令人期待的成果。然而，这些方法无法动态收集额外的诊断信息，如与事件相关的日志、指标或数据库，严重限制了它们诊断根本原因的能力。在这项工作中，我们探讨了LLM代理用于RCA以解决此限制的使用。我们提出了一项彻底的实证评估。

    arXiv:2403.04123v1 Announce Type: cross  Abstract: The growing complexity of cloud based software systems has resulted in incident management becoming an integral part of the software development lifecycle. Root cause analysis (RCA), a critical part of the incident management process, is a demanding task for on-call engineers, requiring deep domain knowledge and extensive experience with a team's specific services. Automation of RCA can result in significant savings of time, and ease the burden of incident management on on-call engineers. Recently, researchers have utilized Large Language Models (LLMs) to perform RCA, and have demonstrated promising results. However, these approaches are not able to dynamically collect additional diagnostic information such as incident related logs, metrics or databases, severely restricting their ability to diagnose root causes. In this work, we explore the use of LLM based agents for RCA to address this limitation. We present a thorough empirical eva
    
[^86]: 大型语言模型能够进行推理和规划吗？

    Can Large Language Models Reason and Plan?

    [https://arxiv.org/abs/2403.04121](https://arxiv.org/abs/2403.04121)

    大型语言模型缺乏自我批评能力，无法像人类一样纠正错误。

    

    虽然人类有时候表现出能够通过自我批评纠正自己错误猜测的能力，但似乎在大型语言模型的情况下没有依据支持这一假设。

    arXiv:2403.04121v1 Announce Type: new  Abstract: While humans sometimes do show the capability of correcting their own erroneous guesses with self-critiquing, there seems to be no basis for that assumption in the case of LLMs.
    
[^87]: 全局稳定的神经仿真政策

    Globally Stable Neural Imitation Policies

    [https://arxiv.org/abs/2403.04118](https://arxiv.org/abs/2403.04118)

    提出了稳定神经动力系统（SNDS）的仿真学习制度，可生成具有正式稳定性保证的政策，并通过联合训练政策和其对应的李亚普诺夫候选者确保全局稳定性。

    

    仿真学习提供了一种有效的方法，可以缓解从头开始在解决空间中学习政策的资源密集和耗时的特性。尽管结果政策可以可靠地模仿专家演示，但在状态空间的未探索区域中常常缺乏可预测性，这给在面对扰动时带来了重大安全问题。为了解决这些挑战，我们引入了稳定神经动力系统（SNDS），一种生成具有正式稳定性保证的政策的仿真学习制度。我们使用神经政策架构，促进基于李亚普诺夫定理的稳定性表示，并联合训练政策及其相应的李亚普诺夫候选者，以确保全局稳定性。我们通过在仿真中进行大量实验来验证我们的方法，并成功将经过训练的政策部署到现实世界的机械手臂上。实验结果表明，我们的SNDS方法相比现有方法具有更好的全局稳定性和鲁棒性。

    arXiv:2403.04118v1 Announce Type: cross  Abstract: Imitation learning presents an effective approach to alleviate the resource-intensive and time-consuming nature of policy learning from scratch in the solution space. Even though the resulting policy can mimic expert demonstrations reliably, it often lacks predictability in unexplored regions of the state-space, giving rise to significant safety concerns in the face of perturbations. To address these challenges, we introduce the Stable Neural Dynamical System (SNDS), an imitation learning regime which produces a policy with formal stability guarantees. We deploy a neural policy architecture that facilitates the representation of stability based on Lyapunov theorem, and jointly train the policy and its corresponding Lyapunov candidate to ensure global stability. We validate our approach by conducting extensive experiments in simulation and successfully deploying the trained policies on a real-world manipulator arm. The experimental resu
    
[^88]: 用可组合物体的神经辅助射线体逼真地缩小视觉从虚拟到实际的差距

    Closing the Visual Sim-to-Real Gap with Object-Composable NeRFs

    [https://arxiv.org/abs/2403.04114](https://arxiv.org/abs/2403.04114)

    COV-NeRF是一种新型的神经辅助射线体模型，可以从真实图像中提取对象并合成新场景，有效地解决了视觉从虚拟到实际中的数据匹配问题。

    

    感知的深度学习方法是许多机器人系统的基石。尽管这些方法具有令人印象深刻的性能潜力，但获取真实世界的训练数据成本高，对于一些任务可能难以实际操作。通过领域随机化进行从虚拟到实际的转移提供了一个潜在的解决方法，但往往需要大量手动调整，并导致模型对于虚拟和真实之间的分布变化很脆弱。在这项工作中，我们介绍了可组合对象体积神经辅助射线体（COV-NeRF），这是一个以真实到虚拟管道为中心的物体可组合的NeRF模型，用于合成面向来自真实世界的场景和对象的训练数据。COV-NeRF从真实图像中提取对象，并将它们合成为新场景，生成逼真的渲染以及多种类型的2D和3D监督，包括深度图，分割遮罩和网格。我们展示COV-NeRF与现代NeRF的渲染质量相匹配

    arXiv:2403.04114v1 Announce Type: cross  Abstract: Deep learning methods for perception are the cornerstone of many robotic systems. Despite their potential for impressive performance, obtaining real-world training data is expensive, and can be impractically difficult for some tasks. Sim-to-real transfer with domain randomization offers a potential workaround, but often requires extensive manual tuning and results in models that are brittle to distribution shift between sim and real. In this work, we introduce Composable Object Volume NeRF (COV-NeRF), an object-composable NeRF model that is the centerpiece of a real-to-sim pipeline for synthesizing training data targeted to scenes and objects from the real world. COV-NeRF extracts objects from real images and composes them into new scenes, generating photorealistic renderings and many types of 2D and 3D supervision, including depth maps, segmentation masks, and meshes. We show that COV-NeRF matches the rendering quality of modern NeRF 
    
[^89]: 使用因果树估计中风后个体的个性化任务难度

    Using Causal Trees to Estimate Personalized Task Difficulty in Post-Stroke Individuals

    [https://arxiv.org/abs/2403.04109](https://arxiv.org/abs/2403.04109)

    提出了一种基于个体表现自动生成不同任务难度级别区域的方法，能够更好地解释中风患者在达到任务时的表现变异。

    

    自适应训练计划对中风后的康复至关重要。然而，开发能够自动调整的计划取决于如何量化任务对特定个体在康复过程中的难度。在本研究中，我们提出一种方法，根据个体的表现自动生成不同任务难度级别的区域。我们展示了这种技术比先前估计任务难度的方法更好地解释了达到任务的用户表现的变异。

    arXiv:2403.04109v1 Announce Type: cross  Abstract: Adaptive training programs are crucial for recovery post stroke. However, developing programs that automatically adapt depends on quantifying how difficult a task is for a specific individual at a particular stage of their recovery. In this work, we propose a method that automatically generates regions of different task difficulty levels based on an individual's performance. We show that this technique explains the variance in user performance for a reaching task better than previous approaches to estimating task difficulty.
    
[^90]: 多目标多解传输

    Many-Objective Multi-Solution Transport

    [https://arxiv.org/abs/2403.04099](https://arxiv.org/abs/2403.04099)

    提出了Many-objective multi-solution Transport (MosT) 框架，能够在众多目标的帕累托前沿找到多个多样化解决方案，通过优化每个解决方案的加权目标来实现。

    

    优化许多目标（由任务或客户端实例化）与少数帕累托固定解决方案（模型）的性能对于机器学习至关重要。然而，先前的多目标优化方法通常关注少量目标，无法扩展到多个数目超过解决方案的目标，导致性能不佳或被忽略的目标。我们提出了Many-objective multi-solution Transport（MosT），这是一个框架，可以在许多目标的帕累托前沿找到多个多样化解决方案。我们的见解是寻找多个解决方案，每个解决方案都像领域专家一样执行，并专注于特定子目标集，同时共同涵盖所有目标。MosT将问题表述为每个解决方案的加权目标的双层优化，其中权重由目标和解决方案之间的最优传输定义。我们的算法确保收敛到帕累托站点。

    arXiv:2403.04099v1 Announce Type: new  Abstract: Optimizing the performance of many objectives (instantiated by tasks or clients) jointly with a few Pareto stationary solutions (models) is critical in machine learning. However, previous multi-objective optimization methods often focus on a few number of objectives and cannot scale to many objectives that outnumber the solutions, leading to either subpar performance or ignored objectives. We introduce Many-objective multi-solution Transport (MosT), a framework that finds multiple diverse solutions in the Pareto front of many objectives. Our insight is to seek multiple solutions, each performing as a domain expert and focusing on a specific subset of objectives while collectively covering all of them. MosT formulates the problem as a bi-level optimization of weighted objectives for each solution, where the weights are defined by an optimal transport between the objectives and solutions. Our algorithm ensures convergence to Pareto station
    
[^91]: 基于电子健康记录的自动化多任务学习用于联合疾病预测

    Automated Multi-Task Learning for Joint Disease Prediction on Electronic Health Records

    [https://arxiv.org/abs/2403.04086](https://arxiv.org/abs/2403.04086)

    该研究提出了一种名为AutoDP的自动化方法，用于在电子健康记录数据中为联合疾病预测搜索最佳配置。

    

    在大数据和数字化医疗领域，电子健康记录（EHR）已成为一个丰富的信息来源，有潜力改善患者护理和医学研究。近年来，机器学习模型在分析EHR数据以预测患者未来健康状况方面得到了广泛应用。在其中，一些研究提倡使用多任务学习（MTL）来联合预测多个目标疾病，以提高单任务学习的预测性能。然而，由于当前EHR数据的MTL框架严重依赖于人工专家来识别用于联合训练的任务组和设计模型架构，存在显著局限性。为减少人为干预并改进框架设计，我们提出了一种名为AutoDP的自动化方法，可以同时搜索任务分组和架构的最佳配置。为了解决涵盖任务组合和架构的广泛联合搜索空间，

    arXiv:2403.04086v1 Announce Type: new  Abstract: In the realm of big data and digital healthcare, Electronic Health Records (EHR) have become a rich source of information with the potential to improve patient care and medical research. In recent years, machine learning models have proliferated for analyzing EHR data to predict patients future health conditions. Among them, some studies advocate for multi-task learning (MTL) to jointly predict multiple target diseases for improving the prediction performance over single task learning. Nevertheless, current MTL frameworks for EHR data have significant limitations due to their heavy reliance on human experts to identify task groups for joint training and design model architectures. To reduce human intervention and improve the framework design, we propose an automated approach named AutoDP, which can search for the optimal configuration of task grouping and architectures simultaneously. To tackle the vast joint search space encompassing ta
    
[^92]: 通过插值进行推断：对比表示可证明启用规划和推断

    Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference

    [https://arxiv.org/abs/2403.04082](https://arxiv.org/abs/2403.04082)

    通过对比学习学到的时间序列数据表示遵循高斯马尔可夫链，从而启用规划和推断

    

    给定时间序列数据，我们如何回答诸如“未来会发生什么？”和“我们是如何到达这里的？”这类概率推断问题在观测值为高维时具有挑战性。本文展示了这些问题如何通过学习表示的紧凑闭式解决方案。关键思想是将对比学习的变体应用于时间序列数据。之前的工作已经表明，通过对比学习学到的表示编码了概率比。通过将之前的工作扩展以表明表示的边际分布是高斯分布，我们随后证明表示的联合分布也是高斯分布。这些结果共同表明，通过时间对比学习学到的表示遵循高斯马尔可夫链，一种图形模型，其中对表示进行的推断（例如预测、规划）对应于反演低维分布。

    arXiv:2403.04082v1 Announce Type: new  Abstract: Given time series data, how can we answer questions like "what will happen in the future?" and "how did we get here?" These sorts of probabilistic inference questions are challenging when observations are high-dimensional. In this paper, we show how these questions can have compact, closed form solutions in terms of learned representations. The key idea is to apply a variant of contrastive learning to time series data. Prior work already shows that the representations learned by contrastive learning encode a probability ratio. By extending prior work to show that the marginal distribution over representations is Gaussian, we can then prove that joint distribution of representations is also Gaussian. Taken together, these results show that representations learned via temporal contrastive learning follow a Gauss-Markov chain, a graphical model where inference (e.g., prediction, planning) over representations corresponds to inverting a low-
    
[^93]: 方向平滑度与梯度方法：收敛性和自适应性

    Directional Smoothness and Gradient Methods: Convergence and Adaptivity

    [https://arxiv.org/abs/2403.04081](https://arxiv.org/abs/2403.04081)

    提出了一种新的次优性界限方法以解决梯度下降问题，利用方向平滑度开发上界并获得收敛保证，在实验中证明优于传统基于L平滑度的理论。

    

    我们为梯度下降（GD）开发了一种新的次优性界限，其取决于沿优化路径的目标条件性，而不是全局最坏情况常数。我们证明的关键是方向平滑度，这是我们用来开发目标上界的梯度变化度量。最小化这些上界需要解决隐式方程以获得一系列强适应的步长；我们展示了对于凸二次函数, 这些方程很容易求解，并为两种经典步长提供了新的保证。对于一般函数, 我们证明Polyak步长和归一化GD获得快速的、路径相关的速率，尽管不使用方向平滑度的任何知识。 Logistic回归实验表明，我们的收敛保证比基于L平滑度的经典理论更紧密。

    arXiv:2403.04081v1 Announce Type: new  Abstract: We develop new sub-optimality bounds for gradient descent (GD) that depend on the conditioning of the objective along the path of optimization, rather than on global, worst-case constants. Key to our proofs is directional smoothness, a measure of gradient variation that we use to develop upper-bounds on the objective. Minimizing these upper-bounds requires solving implicit equations to obtain a sequence of strongly adapted step-sizes; we show that these equations are straightforward to solve for convex quadratics and lead to new guarantees for two classical step-sizes. For general functions, we prove that the Polyak step-size and normalized GD obtain fast, path-dependent rates despite using no knowledge of the directional smoothness. Experiments on logistic regression show our convergence guarantees are tighter than the classical theory based on L-smoothness.
    
[^94]: 预测和缓解公共公交服务中的中断

    Forecasting and Mitigating Disruptions in Public Bus Transit Services

    [https://arxiv.org/abs/2403.04072](https://arxiv.org/abs/2403.04072)

    引入数据驱动的统计和机器学习模型，以解决公共公交服务中替代车辆最佳位置选择的挑战性问题

    

    公共交通系统经常遭受需求的意外波动和中断，例如机械故障和医疗紧急情况。这些波动和中断导致延误和拥挤，这对乘客的体验和公共交通服务的整体性能都是有害的。为了积极减少这类事件，许多运输机构在其服务范围内设置替代（备用）车辆，它们可以调度这些车辆以增加或替代遭受拥挤或中断的路线上的车辆。然而，由于中断的固有随机性和在城市各地选择位置的组合性质，确定替代车辆应该设置的最佳位置是一个具有挑战性的问题。我们与田纳西州纳什维尔的运输机构合作，通过引入基于数据驱动的统计和机器学习模型来解决这一问题。

    arXiv:2403.04072v1 Announce Type: new  Abstract: Public transportation systems often suffer from unexpected fluctuations in demand and disruptions, such as mechanical failures and medical emergencies. These fluctuations and disruptions lead to delays and overcrowding, which are detrimental to the passengers' experience and to the overall performance of the transit service. To proactively mitigate such events, many transit agencies station substitute (reserve) vehicles throughout their service areas, which they can dispatch to augment or replace vehicles on routes that suffer overcrowding or disruption. However, determining the optimal locations where substitute vehicles should be stationed is a challenging problem due to the inherent randomness of disruptions and due to the combinatorial nature of selecting locations across a city. In collaboration with the transit agency of Nashville, TN, we address this problem by introducing data-driven statistical and machine-learning models for fo
    
[^95]: 利用脆弱性感知扰动预算改进对抗训练

    Improving Adversarial Training using Vulnerability-Aware Perturbation Budget

    [https://arxiv.org/abs/2403.04070](https://arxiv.org/abs/2403.04070)

    提出了两种基于脆弱性感知的重新加权函数，用于为对抗训练中的对抗示例分配扰动界限，从而提高了对抗训练的有效性。

    

    对抗训练(Adversarial Training, AT)有效地提高了深度神经网络(DNNs)对敌对攻击的鲁棒性。通常，AT涉及使用在预定义、固定扰动界限内获取的对抗示例来训练DNN模型。值得注意的是，用于制作这些对抗示例的个别自然示例展示出不同程度的固有脆弱性，因此，为所有实例设定固定扰动半径来制作对抗示例可能不足以充分发挥AT的有效性。受到这一观察的启发，我们提出了两种简单、计算廉价的基于脆弱性感知的重新加权函数，用于为用于AT的对抗示例分配扰动界限，分别命名为边际加权扰动预算（MWPB）和标准差加权扰动预算（SDWPB）。所提出的方法根据其对应脆弱性为单个对抗样本分配扰动半径。

    arXiv:2403.04070v1 Announce Type: cross  Abstract: Adversarial Training (AT) effectively improves the robustness of Deep Neural Networks (DNNs) to adversarial attacks. Generally, AT involves training DNN models with adversarial examples obtained within a pre-defined, fixed perturbation bound. Notably, individual natural examples from which these adversarial examples are crafted exhibit varying degrees of intrinsic vulnerabilities, and as such, crafting adversarial examples with fixed perturbation radius for all instances may not sufficiently unleash the potency of AT. Motivated by this observation, we propose two simple, computationally cheap vulnerability-aware reweighting functions for assigning perturbation bounds to adversarial examples used for AT, named Margin-Weighted Perturbation Budget (MWPB) and Standard-Deviation-Weighted Perturbation Budget (SDWPB). The proposed methods assign perturbation radii to individual adversarial samples based on the vulnerability of their correspon
    
[^96]: 基于信念丰富的悲观Q学习抵抗对抗性状态扰动

    Belief-Enriched Pessimistic Q-Learning against Adversarial State Perturbations

    [https://arxiv.org/abs/2403.04050](https://arxiv.org/abs/2403.04050)

    提出了一种新的鲁棒RL算法，通过引入信念状态推理和基于扩散的状态净化，推导出一种悲观策略，以对抗代理对真实状态的不确定性。

    

    强化学习在各个领域取得了巨大成功。然而，其数据驱动的特性也引入了新的漏洞，可以被恶意对手利用。最近的研究表明，通过在测试阶段有策略地扰乱其状态观察，一个训练良好的RL代理很容易被操纵。现有解决方案要么引入正则化项以改善受扰动影响训练策略的平滑性，要么分别训练代理的策略和攻击者的策略。然而，前者不能提供足够的保护来抵御强攻击，而后者对于大环境而言在计算上是禁止的。在本文中，我们提出了一种新的鲁棒RL算法，用于推导出一种悲观策略，以防范代理对真实状态的不确定性。这种方法进一步结合了信念状态推理和基于扩散的状态净化，以减少不确定性。

    arXiv:2403.04050v1 Announce Type: new  Abstract: Reinforcement learning (RL) has achieved phenomenal success in various domains. However, its data-driven nature also introduces new vulnerabilities that can be exploited by malicious opponents. Recent work shows that a well-trained RL agent can be easily manipulated by strategically perturbing its state observations at the test stage. Existing solutions either introduce a regularization term to improve the smoothness of the trained policy against perturbations or alternatively train the agent's policy and the attacker's policy. However, the former does not provide sufficient protection against strong attacks, while the latter is computationally prohibitive for large environments. In this work, we propose a new robust RL algorithm for deriving a pessimistic policy to safeguard against an agent's uncertainty about true states. This approach is further enhanced with belief state inference and diffusion-based state purification to reduce unc
    
[^97]: 用于条件反事实均值估计的样本规模规划: 一个K臂随机实验

    Sample size planning for conditional counterfactual mean estimation with a K-armed randomized experiment

    [https://arxiv.org/abs/2403.04039](https://arxiv.org/abs/2403.04039)

    论文讨论了如何确定足够大的样本规模，以在数据驱动的子组中估计条件反事实期望，通过将原始目标转化为同时推断问题来解决估计误差增加的可能性，并且允许在固定的样本大小预算下逆转问题以确定可行的治疗手臂数量或分区复杂性。

    

    我们讨论如何确定足够大的样本规模，以估计数据驱动子组中的条件反事实期望，该子组可以由任何特征空间划分算法输出，包括根据预测分数相似的用户进行分组或根据学习的策略树进行分组。在仔细规定推断目标、最小置信水平和最大误差边际后，关键是将原始目标转化为一个同时推断问题，推荐的样本大小以抵消估计误差的增加可能性直接与要进行的推断数量相关。在给定固定样本规模预算的情况下，我们的结果使我们能够将问题反转为关于可行治疗手臂数量或分区复杂性（例如，决策树叶子数量）的问题。使用策略树学习子组，我们评估...

    arXiv:2403.04039v1 Announce Type: new  Abstract: We cover how to determine a sufficiently large sample size for a $K$-armed randomized experiment in order to estimate conditional counterfactual expectations in data-driven subgroups. The sub-groups can be output by any feature space partitioning algorithm, including as defined by binning users having similar predictive scores or as defined by a learned policy tree. After carefully specifying the inference target, a minimum confidence level, and a maximum margin of error, the key is to turn the original goal into a simultaneous inference problem where the recommended sample size to offset an increased possibility of estimation error is directly related to the number of inferences to be conducted. Given a fixed sample size budget, our result allows us to invert the question to one about the feasible number of treatment arms or partition complexity (e.g. number of decision tree leaves). Using policy trees to learn sub-groups, we evaluate o
    
[^98]: OCD-FL: 一种基于点对点选择的通信高效去中心化联邦学习

    OCD-FL: A Novel Communication-Efficient Peer Selection-based Decentralized Federated Learning

    [https://arxiv.org/abs/2403.04037](https://arxiv.org/abs/2403.04037)

    提出了一种名为OCD-FL的新方案，通过系统化的FL对等选择进行协作，旨在在减少能耗的同时实现最大的FL知识增益

    

    边缘智能和不断增长的物联网网络的结合开创了协作机器学习的新时代，联邦学习(FL)作为最突出的范式出现。随着人们对这些学习方案越来越感兴趣，研究人员开始解决它们最基本的一些限制。事实上，具有中心聚合器的传统FL存在单点故障和网络瓶颈。为了规避这个问题，提出了节点在点对点网络中协作的去中心化FL。尽管后者效率高，但在去中心化FL中，通信成本和数据异质性仍然是关键挑战。在这种背景下，我们提出了一种名为机会主义通信高效的去中心化联邦学习(OCD-FL)的新方案，其中包括系统化的FL对等选择以进行协作，旨在实现最大的FL知识增益同时减少能耗。

    arXiv:2403.04037v1 Announce Type: new  Abstract: The conjunction of edge intelligence and the ever-growing Internet-of-Things (IoT) network heralds a new era of collaborative machine learning, with federated learning (FL) emerging as the most prominent paradigm. With the growing interest in these learning schemes, researchers started addressing some of their most fundamental limitations. Indeed, conventional FL with a central aggregator presents a single point of failure and a network bottleneck. To bypass this issue, decentralized FL where nodes collaborate in a peer-to-peer network has been proposed. Despite the latter's efficiency, communication costs and data heterogeneity remain key challenges in decentralized FL. In this context, we propose a novel scheme, called opportunistic communication-efficient decentralized federated learning, a.k.a., OCD-FL, consisting of a systematic FL peer selection for collaboration, aiming to achieve maximum FL knowledge gain while reducing energy co
    
[^99]: 无监督对比学习用于抵抗时域漂移下的稳健射频设备指纹识别

    Unsupervised Contrastive Learning for Robust RF Device Fingerprinting Under Time-Domain Shift

    [https://arxiv.org/abs/2403.04036](https://arxiv.org/abs/2403.04036)

    本论文引入了对比学习来解决射频设备指纹识别中的领域漂移问题，通过学习距离度量处理RF信号，从而提高稳健性。

    

    射频（RF）设备指纹识别被认为是实现自动无线设备识别和分类的潜在技术。然而，由于信道条件和环境设置的变化可能引起的领域漂移，可能会降低基于RF的设备分类的准确性，特别是当测试和训练数据在不同领域收集时。本文介绍了一种新颖的解决方案，该方案利用对比学习来缓解这种领域漂移问题。对比学习是一种来自深度学习的最先进的自监督学习方法，学习一个距离度量，使得正对组在学习的度量空间中比负对更接近（即更相似）。当应用于RF指纹识别时，我们的模型将来自相同传输的RF信号视为正对，将来自不同传输的信号视为负对。

    arXiv:2403.04036v1 Announce Type: cross  Abstract: Radio Frequency (RF) device fingerprinting has been recognized as a potential technology for enabling automated wireless device identification and classification. However, it faces a key challenge due to the domain shift that could arise from variations in the channel conditions and environmental settings, potentially degrading the accuracy of RF-based device classification when testing and training data is collected in different domains. This paper introduces a novel solution that leverages contrastive learning to mitigate this domain shift problem. Contrastive learning, a state-of-the-art self-supervised learning approach from deep learning, learns a distance metric such that positive pairs are closer (i.e. more similar) in the learned metric space than negative pairs. When applied to RF fingerprinting, our model treats RF signals from the same transmission as positive pairs and those from different transmissions as negative pairs. T
    
[^100]: 具有未知约束的在线学习

    Online Learning with Unknown Constraints

    [https://arxiv.org/abs/2403.04033](https://arxiv.org/abs/2403.04033)

    在线学习中通过元算法结合在线回归预测器估计未知安全约束，并将在线学习预测转换为符合约束的预测，同时保证在每一轮高概率地满足安全约束。算法的后悔受到在线回归和在线学习预测器的限制，模型类中未知安全约束的逃避维度，以及捕捉安全学习困难的新颖复杂度度量的约束。

    

    我们考虑在线学习中的问题，其中学习者在每一轮必须遵守一个未知的安全约束。目标是在同时满足安全约束并在后视下最小化对最佳安全动作的后悔。我们提供了一个通用的元算法，利用在线回归预测器来估计未知安全约束，并将在线学习预测转换为符合未知安全约束的预测。在理论方面，我们的算法的后悔可以通过在线回归和在线学习预测器的后悔、包含未知安全约束的模型类的逃避维度，以及一个捕捉安全学习困难程度的新颖复杂度度量来界定。

    arXiv:2403.04033v1 Announce Type: cross  Abstract: We consider the problem of online learning where the sequence of actions played by the learner must adhere to an unknown safety constraint at every round. The goal is to minimize regret with respect to the best safe action in hindsight while simultaneously satisfying the safety constraint with high probability on each round. We provide a general meta-algorithm that leverages an online regression oracle to estimate the unknown safety constraint, and converts the predictions of an online learning oracle to predictions that adhere to the unknown safety constraint. On the theoretical side, our algorithm's regret can be bounded by the regret of the online regression and online learning oracles, the eluder dimension of the model class containing the unknown safety constraint, and a novel complexity measure that captures the difficulty of safe learning. We complement our result with an asymptotic lower bound that shows that the aforementioned
    
[^101]: 学习引导的自动推理：简要调查

    Learning Guided Automated Reasoning: A Brief Survey

    [https://arxiv.org/abs/2403.04017](https://arxiv.org/abs/2403.04017)

    机器学习可引导自动推理系统，通过逻辑有效证明概念支持演绎搜索，从大型推理体中培训机器学习系统，打破推理系统的组合爆炸问题，并促进推理链的长远发展和新颖证明思路的产生

    

    自动定理证明器和形式化证明助手是通用的推理系统，理论上能够证明任意困难的定理，从而解决可归约为数学和逻辑推理的任意问题。在实践中，这些系统面临着极大的组合爆炸，因此包含许多启发式方法和选择点，这些方法和选择点极大地影响其性能。这为受过训练的机器学习预测器提供了机会，可以引导这些推理系统的工作。相反，由逻辑上有效证明概念支持的演绎搜索允许在大型推理语料库上训练机器学习系统。这些证明体通常是通过构建正确的，当与更加精确的训练引导相结合时，可以将它们提升为非常庞大的语料库，具有越来越长的推理链，并可能产生新颖的证明思路。在本文中，我们概述了...

    arXiv:2403.04017v1 Announce Type: new  Abstract: Automated theorem provers and formal proof assistants are general reasoning systems that are in theory capable of proving arbitrarily hard theorems, thus solving arbitrary problems reducible to mathematics and logical reasoning. In practice, such systems however face large combinatorial explosion, and therefore include many heuristics and choice points that considerably influence their performance. This is an opportunity for trained machine learning predictors, which can guide the work of such reasoning systems. Conversely, deductive search supported by the notion of logically valid proof allows one to train machine learning systems on large reasoning corpora. Such bodies of proof are usually correct by construction and when combined with more and more precise trained guidance they can be boostrapped into very large corpora, with increasingly long reasoning chains and possibly novel proof ideas. In this paper we provide an overview of se
    
[^102]: 通过单个预训练的增强型代理引导的模拟特征选择

    Knockoff-Guided Feature Selection via A Single Pre-trained Reinforced Agent

    [https://arxiv.org/abs/2403.04015](https://arxiv.org/abs/2403.04015)

    通过引入模拟特征指导和强化学习优化的创新框架，提出了一种用于特征选择的方法，以识别最佳有效的特征子集。

    

    特征选择通过消除冗余特征来准备数据的AI可用性。先前的研究主要分为两类：i）监督特征选择，根据特征与目标变量的相关性识别最佳特征子集；ii）无监督特征选择，通过捕获特征集中的基本信息而非使用目标变量来减少特征空间的维度。然而，监督特征选择方法由于依赖目标变量和下游ML任务而导致耗时且泛化能力有限。无监督特征选择方法受限于推导出的特征空间是潜在且不可追踪的。为解决这些挑战，我们引入一种新颖的特征选择框架，通过模拟特征指导并通过强化学习进行优化，以识别最佳有效的特征子集。

    arXiv:2403.04015v1 Announce Type: cross  Abstract: Feature selection prepares the AI-readiness of data by eliminating redundant features. Prior research falls into two primary categories: i) Supervised Feature Selection, which identifies the optimal feature subset based on their relevance to the target variable; ii) Unsupervised Feature Selection, which reduces the feature space dimensionality by capturing the essential information within the feature set instead of using target variable. However, SFS approaches suffer from time-consuming processes and limited generalizability due to the dependence on the target variable and downstream ML tasks. UFS methods are constrained by the deducted feature space is latent and untraceable. To address these challenges, we introduce an innovative framework for feature selection, which is guided by knockoff features and optimized through reinforcement learning, to identify the optimal and effective feature subset. In detail, our method involves gener
    
[^103]: 用于多模态电子健康记录动态嵌入和标记的时间交叉注意力

    Temporal Cross-Attention for Dynamic Embedding and Tokenization of Multimodal Electronic Health Records

    [https://arxiv.org/abs/2403.04012](https://arxiv.org/abs/2403.04012)

    提出了一种用于精确表示多模态临床时间序列的动态嵌入和标记框架

    

    现代电子健康记录（EHR）系统的广度、规模和时间粒度为使用顺序深度学习估计个性化和背景患者健康轨迹提供了巨大潜力。然而，由于其高维度、稀疏性、多模态性、不规则和变量特定的记录频率以及在同时记录多个测量时戳重复，学习有用的EHR数据表示具有挑战性。尽管最近的努力将结构化EHR和非结构化临床笔记融合，表明了更准确预测临床结果的潜力，但对直接解决时间EHR挑战的EHR嵌入方法的关注较少——即通过学习来自多模态患者时间序列的具有时间感知的表示。在本文中，我们介绍了一个用于精确表示多模态临床时间序列的动态嵌入和标记框架

    arXiv:2403.04012v1 Announce Type: new  Abstract: The breadth, scale, and temporal granularity of modern electronic health records (EHR) systems offers great potential for estimating personalized and contextual patient health trajectories using sequential deep learning. However, learning useful representations of EHR data is challenging due to its high dimensionality, sparsity, multimodality, irregular and variable-specific recording frequency, and timestamp duplication when multiple measurements are recorded simultaneously. Although recent efforts to fuse structured EHR and unstructured clinical notes suggest the potential for more accurate prediction of clinical outcomes, less focus has been placed on EHR embedding approaches that directly address temporal EHR challenges by learning time-aware representations from multimodal patient time series. In this paper, we introduce a dynamic embedding and tokenization framework for precise representation of multimodal clinical time series that
    
[^104]: 三次重温节点级图异常检测：离群值、消息传递和双曲神经网络

    Three Revisits to Node-Level Graph Anomaly Detection: Outliers, Message Passing and Hyperbolic Neural Networks

    [https://arxiv.org/abs/2403.04010](https://arxiv.org/abs/2403.04010)

    本文从离群值注入、消息传递和双曲神经网络三个方面重新审视节点级图异常检测任务，揭示了对提高性能的通用策略

    

    图异常检测在识别复杂网络中的异常实例方面发挥着至关重要的作用。尽管近年来基于深度学习的方法取得了进展，但现有的基准方法存在局限性，阻碍了全面比较。本文从三个方面重新审视了用于无监督节点级图异常检测任务的数据集和方法。首先，我们引入了异常注入方法，通过在图数据集中创建更多不同的基于图的异常。其次，我们比较了使用消息传递的方法与不使用消息传递的方法，揭示了与消息传递相关的性能意外下降。第三，我们探索了使用双曲神经网络的方法，指定了关键的架构和损失设计，有助于提高性能。通过严格的实验和评估，我们的研究揭示了改进节点级图异常的一般策略。

    arXiv:2403.04010v1 Announce Type: new  Abstract: Graph anomaly detection plays a vital role for identifying abnormal instances in complex networks. Despite advancements of methodology based on deep learning in recent years, existing benchmarking approaches exhibit limitations that hinder a comprehensive comparison. In this paper, we revisit datasets and approaches for unsupervised node-level graph anomaly detection tasks from three aspects. Firstly, we introduce outlier injection methods that create more diverse and graph-based anomalies in graph datasets. Secondly, we compare methods employing message passing against those without, uncovering the unexpected decline in performance associated with message passing. Thirdly, we explore the use of hyperbolic neural networks, specifying crucial architecture and loss design that contribute to enhanced performance. Through rigorous experiments and evaluations, our study sheds light on general strategies for improving node-level graph anomaly 
    
[^105]: 基于采样的非线性动力系统安全强化学习

    Sampling-based Safe Reinforcement Learning for Nonlinear Dynamical Systems

    [https://arxiv.org/abs/2403.04007](https://arxiv.org/abs/2403.04007)

    该论文提出了一种基于采样的安全强化学习方法，可以在控制非线性动力系统时同时满足硬性安全约束和收敛性保证。

    

    我们为控制非线性动力系统开发了具有可证明安全性和收敛性的强化学习（RL）算法，弥合了控制理论的严格安全性保证和RL理论的收敛性保证之间的差距。我们发展了一种单阶段、基于采样的方法，用于硬约束满足，学习RL控制器在整个训练和部署过程中满足严格的安全约束，同时享有经典收敛保证。

    arXiv:2403.04007v1 Announce Type: new  Abstract: We develop provably safe and convergent reinforcement learning (RL) algorithms for control of nonlinear dynamical systems, bridging the gap between the hard safety guarantees of control theory and the convergence guarantees of RL theory. Recent advances at the intersection of control and RL follow a two-stage, safety filter approach to enforcing hard safety constraints: model-free RL is used to learn a potentially unsafe controller, whose actions are projected onto safe sets prescribed, for example, by a control barrier function. Though safe, such approaches lose any convergence guarantees enjoyed by the underlying RL methods. In this paper, we develop a single-stage, sampling-based approach to hard constraint satisfaction that learns RL controllers enjoying classical convergence guarantees while satisfying hard safety constraints throughout training and deployment. We validate the efficacy of our approach in simulation, including safe c
    
[^106]: 关于概率序列模型边缘化的高效性

    On the Efficient Marginalization of Probabilistic Sequence Models

    [https://arxiv.org/abs/2403.04005](https://arxiv.org/abs/2403.04005)

    论文提出了一系列新颖高效的逼近技术，用于序贯模型的边缘化，这些技术是模型无关的，仅依赖于预训练自回归模型的下一步条件分布的访问和采样。

    

    现实世界中的数据经常表现出序列依赖性，涵盖人类行为、医学、金融和气候模拟等各个领域。概率方法捕捉了这些背景下预测相关的固有不确定性，其中自回归模型尤为突出。本论文着重于使用自回归模型回答超出单步预测范围的复杂概率查询，比如未来事件的时间安排或某一事件发生在另一事件之前的可能性。具体来说，我们开发了一系列新颖高效的逼近技术，用于序贯模型的边缘化，这些技术是模型无关的，仅依赖于预训练自回归模型的下一步条件分布的访问和采样，包括传统参数模型以及最新的神经自回归模型。

    arXiv:2403.04005v1 Announce Type: cross  Abstract: Real-world data often exhibits sequential dependence, across diverse domains such as human behavior, medicine, finance, and climate modeling. Probabilistic methods capture the inherent uncertainty associated with prediction in these contexts, with autoregressive models being especially prominent. This dissertation focuses on using autoregressive models to answer complex probabilistic queries that go beyond single-step prediction, such as the timing of future events or the likelihood of a specific event occurring before another. In particular, we develop a broad class of novel and efficient approximation techniques for marginalization in sequential models that are model-agnostic. These techniques rely solely on access to and sampling from next-step conditional distributions of a pre-trained autoregressive model, including both traditional parametric models as well as more recent neural autoregressive models. Specific approaches are pres
    
[^107]: 具有双向进步神经网络和情节回归进展的新兴任务排序和机器人技能迁移

    Bidirectional Progressive Neural Networks with Episodic Return Progress for Emergent Task Sequencing and Robotic Skill Transfer

    [https://arxiv.org/abs/2403.04001](https://arxiv.org/abs/2403.04001)

    提出了一种名为具有双向进步神经网络的情节回归进展（ERP-BPNN）的新型多任务强化学习框架，通过人类般交叉的方式学习，实现自主任务切换，并允许在任务之间进行双向技能传递。

    

    人类大脑和行为提供了一个丰富的场景，可以启发机器人的新型控制和学习方法。为了通过激发人类如何获取知识并在任务之间传递技能来展示这样一种发展，我们引入了一种名为具有双向进步神经网络的情节回归进展（ERP-BPNN）的新型多任务强化学习框架。提出的ERP-BPNN模型（1）通过人类般交叉的方式学习，通过一种新颖的内在激励信号实现自主任务切换，并且与现有方法不同，（3）允许在任务之间进行双向技能传递。ERP-BPNN是一个通用架构，适用于几种多任务学习设置；在本文中，我们介绍了其神经结构的细节，并展示了它在不同机器人形态的触碰任务中实现有效学习和技能传递的能力。

    arXiv:2403.04001v1 Announce Type: cross  Abstract: Human brain and behavior provide a rich venue that can inspire novel control and learning methods for robotics. In an attempt to exemplify such a development by inspiring how humans acquire knowledge and transfer skills among tasks, we introduce a novel multi-task reinforcement learning framework named Episodic Return Progress with Bidirectional Progressive Neural Networks (ERP-BPNN). The proposed ERP-BPNN model (1) learns in a human-like interleaved manner by (2) autonomous task switching based on a novel intrinsic motivation signal and, in contrast to existing methods, (3) allows bidirectional skill transfer among tasks. ERP-BPNN is a general architecture applicable to several multi-task learning settings; in this paper, we present the details of its neural architecture and show its ability to enable effective learning and skill transfer among morphologically different robots in a reaching task. The developed Bidirectional Progressiv
    
[^108]: 使用专家混合模型进行视频关系检测

    Video Relationship Detection Using Mixture of Experts

    [https://arxiv.org/abs/2403.03994](https://arxiv.org/abs/2403.03994)

    使用专家混合模型进行视频关系检测，解决了连接视觉和语言的计算和推理差距，提高了稳定性和泛化能力。

    

    从图像和视频中通过神经网络对视觉信息进行机器理解面临两个主要挑战。首先，连接视觉和语言存在计算和推理差距，难以精确确定给定代理作用于哪个对象并通过语言表示。其次，由单一的整体神经网络训练的分类器通常缺乏稳定性和泛化能力。为了克服这些挑战，我们引入MoE-VRD，一种利用专家混合模型进行视觉关系检测的新方法。MoE-VRD通过在视觉处理中提取以<主语，谓词，宾语>元组形式的语言三元组来识别关系。利用视觉关系检测的最新进展，MoE-VRD解决了在建立主体（执行动作）和客体（受到作用）之间的关系时对动作识别的要求。与单一整体网络相比，

    arXiv:2403.03994v1 Announce Type: cross  Abstract: Machine comprehension of visual information from images and videos by neural networks faces two primary challenges. Firstly, there exists a computational and inference gap in connecting vision and language, making it difficult to accurately determine which object a given agent acts on and represent it through language. Secondly, classifiers trained by a single, monolithic neural network often lack stability and generalization. To overcome these challenges, we introduce MoE-VRD, a novel approach to visual relationship detection utilizing a mixture of experts. MoE-VRD identifies language triplets in the form of < subject, predicate, object> tuples to extract relationships from visual processing. Leveraging recent advancements in visual relationship detection, MoE-VRD addresses the requirement for action recognition in establishing relationships between subjects (acting) and objects (being acted upon). In contrast to single monolithic net
    
[^109]: 环境-固有维度差异对对抗脆弱性的影响

    Effect of Ambient-Intrinsic Dimension Gap on Adversarial Vulnerability

    [https://arxiv.org/abs/2403.03967](https://arxiv.org/abs/2403.03967)

    通过环境-固有维度差异的概念，论文证明了维度差异使干净训练的模型更容易受到数据空间脱离流形方向的对抗扰动攻击。

    

    论文介绍了对机器学习模型的对抗攻击存在且对人类来说几乎无法察觉这一事实，在理论上仍然相当神秘。文章引入了两种对抗攻击的概念：自然或在流形上的攻击，这些攻击是可以被人类/神谕感知到的；非自然或脱离流形的攻击，这些攻击则无法被感知到。文章认为脱离流形的攻击存在是数据固有维度与环境维度之间的差异的必然结果。对于2层ReLU网络，我们证明了即使维度差异不影响从观测数据空间中抽取样本的泛化性能，它仍会使干净训练的模型更容易受到数据空间脱离流形方向的对抗扰动攻击。我们的主要结果提供了在/脱离流形攻击的$\ell_2,\ell_{\infty}$攻击强度与维度差异之间明确的关系。

    arXiv:2403.03967v1 Announce Type: new  Abstract: The existence of adversarial attacks on machine learning models imperceptible to a human is still quite a mystery from a theoretical perspective. In this work, we introduce two notions of adversarial attacks: natural or on-manifold attacks, which are perceptible by a human/oracle, and unnatural or off-manifold attacks, which are not. We argue that the existence of the off-manifold attacks is a natural consequence of the dimension gap between the intrinsic and ambient dimensions of the data. For 2-layer ReLU networks, we prove that even though the dimension gap does not affect generalization performance on samples drawn from the observed data space, it makes the clean-trained model more vulnerable to adversarial perturbations in the off-manifold direction of the data space. Our main results provide an explicit relationship between the $\ell_2,\ell_{\infty}$ attack strength of the on/off-manifold attack and the dimension gap.
    
[^110]: 评估无模板逆合成模型的外推能力

    Assessing the Extrapolation Capability of Template-Free Retrosynthesis Models

    [https://arxiv.org/abs/2403.03960](https://arxiv.org/abs/2403.03960)

    评估了无模板逆合成模型的外推能力，发现尽管其在预测新合成规则的前体方面表现出潜力，但在超出分布反应中的精准匹配准确率很低，且其预测的新颖反应大部分都是不切实际的。

    

    尽管无模板模型在探索逆合成预测中未见反应空间方面具有公认的能力，相较于基于模板的模型，它们超越已建立边界的能力仍相对未知。在本研究中，我们通过精心组装大量的超出分布（OOD）反应，从经验上评估了最先进的无模板模型的外推能力。我们的研究结果表明，尽管无模板模型在预测具有新合成规则的前体方面表现出潜力，但在OOD反应中的前十个精准匹配准确率令人惊讶地不足（<1%）。此外，尽管能够生成新颖的反应，我们的研究突显了一个反复出现的问题，即无模板模型预测的一半以上新颖反应在化学上是不切实际的。因此，我们倡导未来开发能够...

    arXiv:2403.03960v1 Announce Type: cross  Abstract: Despite the acknowledged capability of template-free models in exploring unseen reaction spaces compared to template-based models for retrosynthesis prediction, their ability to venture beyond established boundaries remains relatively uncharted. In this study, we empirically assess the extrapolation capability of state-of-the-art template-free models by meticulously assembling an extensive set of out-of-distribution (OOD) reactions. Our findings demonstrate that while template-free models exhibit potential in predicting precursors with novel synthesis rules, their top-10 exact-match accuracy in OOD reactions is strikingly modest (< 1%). Furthermore, despite the capability of generating novel reactions, our investigation highlights a recurring issue where more than half of the novel reactions predicted by template-free models are chemically implausible. Consequently, we advocate for the future development of template-free models that in
    
[^111]: 黑盒$k$-to-$1$-PCA降维：理论与应用

    Black-Box $k$-to-$1$-PCA Reductions: Theory and Applications

    [https://arxiv.org/abs/2403.03905](https://arxiv.org/abs/2403.03905)

    我们的主要贡献是对于$k$-PCA算法中近似参数退化的边界得到了显著更为精确的界限

    

    $k$-主成分分析（$k$-PCA）问题是一种基本的算法原语，在数据分析和降维应用中被广泛使用。在统计环境中，$k$-PCA的目标是识别一个分布的协方差矩阵的顶部特征空间，我们只能通过样本隐式访问这个矩阵。受这些隐式设置的启发，我们分析黑盒缩减方法作为设计$k$-PCA算法的框架，其中我们通过黑盒$1$-PCA预言模拟对未知目标矩阵的访问，该预言返回一个近似的顶部特征向量，根据两个流行的近似概念。尽管这种黑盒方法可能是设计$k$-PCA算法中最自然的基于降维的方法，这种方法，即通过递归调用$1$-PCA预言调用了$k$次，以前很难理解。

    arXiv:2403.03905v1 Announce Type: cross  Abstract: The $k$-principal component analysis ($k$-PCA) problem is a fundamental algorithmic primitive that is widely-used in data analysis and dimensionality reduction applications. In statistical settings, the goal of $k$-PCA is to identify a top eigenspace of the covariance matrix of a distribution, which we only have implicit access to via samples. Motivated by these implicit settings, we analyze black-box deflation methods as a framework for designing $k$-PCA algorithms, where we model access to the unknown target matrix via a black-box $1$-PCA oracle which returns an approximate top eigenvector, under two popular notions of approximation. Despite being arguably the most natural reduction-based approach to $k$-PCA algorithm design, such black-box methods, which recursively call a $1$-PCA oracle $k$ times, were previously poorly-understood.   Our main contribution is significantly sharper bounds on the approximation parameter degradation of
    
[^112]: 通过扩散在流形上的谱算法

    Spectral Algorithms on Manifolds through Diffusion

    [https://arxiv.org/abs/2403.03669](https://arxiv.org/abs/2403.03669)

    本文提出了一种新的视角，将输入数据视为嵌入到更高维欧几里得空间中的低维流形，并研究了在RKHS中谱算法的收敛性能，特别是热核生成的扩散空间，通过积分算子技术推导了关于广义范数的紧收敛上界，使估计器在强意义下收敛到目标函数。

    

    在重现核希尔伯特空间（RKHS）中应用的谱算法的现有研究主要集中在一般核函数上，经常忽略输入特征空间的固有结构。我们的论文引入了一个新的视角，主张输入数据位于一个嵌入到更高维欧几里得空间中的低维流形内。我们研究了RKHS中谱算法的收敛性能，特别是那些由热核生成的，被称为扩散空间的空间。通过结合输入的流形结构，我们采用积分算子技术推导了关于广义范数的紧收敛上界，这表明估计器在强意义下收敛到目标函数，意味着函数本身及其导数同时收敛。这些界提供了两个重要优势：首先，它们是完全连续的。

    arXiv:2403.03669v1 Announce Type: cross  Abstract: The existing research on spectral algorithms, applied within a Reproducing Kernel Hilbert Space (RKHS), has primarily focused on general kernel functions, often neglecting the inherent structure of the input feature space. Our paper introduces a new perspective, asserting that input data are situated within a low-dimensional manifold embedded in a higher-dimensional Euclidean space. We study the convergence performance of spectral algorithms in the RKHSs, specifically those generated by the heat kernels, known as diffusion spaces. Incorporating the manifold structure of the input, we employ integral operator techniques to derive tight convergence upper bounds concerning generalized norms, which indicates that the estimators converge to the target function in strong sense, entailing the simultaneous convergence of the function itself and its derivatives. These bounds offer two significant advantages: firstly, they are exclusively contin
    
[^113]: 强化学习在空间资源分配中的应用调查

    A Survey on Applications of Reinforcement Learning in Spatial Resource Allocation

    [https://arxiv.org/abs/2403.03643](https://arxiv.org/abs/2403.03643)

    运用强化学习解决空间资源分配问题的新方法具有快速解决方法收敛和强大的模型泛化能力等优势，为这一问题领域提供了新的视角。

    

    空间资源分配的挑战在交通运输、工业和日常生活等各个领域普遍存在。随着现实世界问题规模不断扩大以及对实时解决方案的需求增加，传统算法面临着巨大的计算压力，难以实现最佳效率和实时能力。近年来，随着计算机计算能力的不断提升，强化学习在诸如围棋和机器人领域取得了显著成就，展示了其强大的学习和序贯决策能力。鉴于这些进展，近年来出现了大量运用强化学习解决空间资源分配问题的新方法。这些方法具有快速解决方法收敛和强大的模型泛化能力等优势，为解决空间资源分配问题提供了新的视角。

    arXiv:2403.03643v1 Announce Type: cross  Abstract: The challenge of spatial resource allocation is pervasive across various domains such as transportation, industry, and daily life. As the scale of real-world issues continues to expand and demands for real-time solutions increase, traditional algorithms face significant computational pressures, struggling to achieve optimal efficiency and real-time capabilities. In recent years, with the escalating computational power of computers, the remarkable achievements of reinforcement learning in domains like Go and robotics have demonstrated its robust learning and sequential decision-making capabilities. Given these advancements, there has been a surge in novel methods employing reinforcement learning to tackle spatial resource allocation problems. These methods exhibit advantages such as rapid solution convergence and strong model generalization abilities, offering a new perspective on resolving spatial resource allocation problems. Therefor
    
[^114]: DPOT: 自回归去噪运算器变换器用于大规模PDE预训练

    DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training

    [https://arxiv.org/abs/2403.03542](https://arxiv.org/abs/2403.03542)

    本文提出了一种新的自回归去噪预训练策略，可以更稳定、更高效地在PDE数据上进行预训练，并且通过基于傅里叶注意力的模型架构设计，实现了在大规模预训练中轻松扩展模型，该模型在多个PDE数据集上取得了SOTA表现。

    

    预训练已经被研究用来提高在数据稀缺环境中训练神经算子的效率和性能。然而，由于偏微分方程（PDE）数据的固有复杂性和多样性，如长轨迹、多个尺度和不同维度，它在很大程度上还处于起步阶段。在本文中，我们提出了一种新的自回归去噪预训练策略，这种策略能够更稳定、更高效地在PDE数据上进行预训练，并且可以泛化到各种下游任务。此外，通过基于傅里叶注意力的灵活可扩展模型架构的设计，我们可以轻松地将模型扩展到大规模预训练。我们在10+个PDE数据集上训练了具有超过0.5B参数的PDE基础模型，包括超过100k轨迹。大量实验证明我们在这些基准上取得了SOTA，并验证了我们的模型对显著提升性能的强大泛化能力。

    arXiv:2403.03542v1 Announce Type: new  Abstract: Pre-training has been investigated to improve the efficiency and performance of training neural operators in data-scarce settings. However, it is largely in its infancy due to the inherent complexity and diversity, such as long trajectories, multiple scales and varying dimensions of partial differential equations (PDEs) data. In this paper, we present a new auto-regressive denoising pre-training strategy, which allows for more stable and efficient pre-training on PDE data and generalizes to various downstream tasks. Moreover, by designing a flexible and scalable model architecture based on Fourier attention, we can easily scale up the model for large-scale pre-training. We train our PDE foundation model with up to 0.5B parameters on 10+ PDE datasets with more than 100k trajectories. Extensive experiments show that we achieve SOTA on these benchmarks and validate the strong generalizability of our model to significantly enhance performanc
    
[^115]: CoRMF: 临界有序循环均场伊辛求解器

    CoRMF: Criticality-Ordered Recurrent Mean Field Ising Solver

    [https://arxiv.org/abs/2403.03391](https://arxiv.org/abs/2403.03391)

    CoRMF是一种基于RNN的高效伊辛模型求解器，利用关键有序自旋序列和循环神经网络来实现变分均场和 RNN 之间的统一，从而实现了对通常难以处理的伊辛模型的高效探索。

    

    我们提出了一种基于RNN的高效伊辛模型求解器，称为Criticality-ordered Recurrent Mean Field (CoRMF)，用于前向伊辛问题。在其核心部分，通过贪婪算法对N个自旋的伊辛模型进行了关键有序自旋序列的引入，从而可以利用自回归均场因子分解，并通过循环神经网络(RNNs)进行优化。我们的方法具有两个显著特点：(i)通过利用底层伊辛图的近似树结构，新获得的关键性顺序使变分均场和RNN之间得以统一，从而允许有效地利用概率推断来探究通常难以处理的伊辛模型;(ii)它具有良好的模块化、独立于模型而又足够表达能力，因此可以完全适用于任何前向伊辛推理问题，而且工作量极小。计算上，通过使用一种方差减少

    arXiv:2403.03391v1 Announce Type: cross  Abstract: We propose an RNN-based efficient Ising model solver, the Criticality-ordered Recurrent Mean Field (CoRMF), for forward Ising problems. In its core, a criticality-ordered spin sequence of an $N$-spin Ising model is introduced by sorting mission-critical edges with greedy algorithm, such that an autoregressive mean-field factorization can be utilized and optimized with Recurrent Neural Networks (RNNs). Our method has two notable characteristics: (i) by leveraging the approximated tree structure of the underlying Ising graph, the newly-obtained criticality order enables the unification between variational mean-field and RNN, allowing the generally intractable Ising model to be efficiently probed with probabilistic inference; (ii) it is well-modulized, model-independent while at the same time expressive enough, and hence fully applicable to any forward Ising inference problems with minimal effort. Computationally, by using a variance-redu
    
[^116]: 在顺序物理信息神经网络中精确执⾏时间连续性

    Exact Enforcement of Temporal Continuity in Sequential Physics-Informed Neural Networks

    [https://arxiv.org/abs/2403.03223](https://arxiv.org/abs/2403.03223)

    精确执⾏时间连续性是本论⽂的⼀项重要创新，简化了解决时间相关问题动态⾏为预测困难的挑战。

    

    科学计算中深度学习方法的应⽤代表了⼯程问题解决⽅法的潜在范式转变。最显著的发展之⼀是物理信息神经⽹络（PINNs），其中神经⽹络被训练以满⾜偏微分⽅程（PDEs）和/或观察数据。尽管此⽅法有希望，但标准版本已被证明在准确预测时间相关问题的动态⾏为⽅⾯存在困难。为了解决这⼀挑战，已经提出⽅法将时间域分解为多个段，每个段中使⽤⼀个不同的神经⽹络，并直接在最⼩化问题的损失函数中将它们之间的连续性纳⼊其中。在本⼯作中，我们引⼊⼀种通过解析解精确强制实现连续性的⽅法。这种⽅法简单易⽤，能够消除

    arXiv:2403.03223v1 Announce Type: new  Abstract: The use of deep learning methods in scientific computing represents a potential paradigm shift in engineering problem solving. One of the most prominent developments is Physics-Informed Neural Networks (PINNs), in which neural networks are trained to satisfy partial differential equations (PDEs) and/or observed data. While this method shows promise, the standard version has been shown to struggle in accurately predicting the dynamic behavior of time-dependent problems. To address this challenge, methods have been proposed that decompose the time domain into multiple segments, employing a distinct neural network in each segment and directly incorporating continuity between them in the loss function of the minimization problem. In this work we introduce a method to exactly enforce continuity between successive time segments via a solution ansatz. This hard constrained sequential PINN (HCS-PINN) method is simple to implement and eliminates 
    
[^117]: WMDP基准：通过遗忘测量和减少恶意使用

    The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning

    [https://arxiv.org/abs/2403.03218](https://arxiv.org/abs/2403.03218)

    WMDP基准是一个公开发布的数据集，包含4157个多项选择问题，用作生物安全、网络安全和化学安全危险知识的代理测量。

    

    arXiv:2403.03218v1 公告类型：交叉领域 摘要：白宫关于人工智能的行政命令强调了大型语言模型(LLMs)赋予恶意行为者开发生物、网络和化学武器的风险。为了衡量这些恶意使用的风险，政府机构和主要人工智能实验室正在开发LLMs的危险能力评估。然而，当前的评估是私人的，阻碍了进一步研究如何减少风险。此外，它们仅专注于几条高度特定的恶意使用途径。为了填补这些空白，我们公开发布了大规模杀伤性武器代理（WMDP）基准，这是一个包含4157个多项选择问题的数据集，作为生物安全、网络安全和化学安全危险知识的代理测量。WMDP由一组学术界和技术顾问联合开发，并在公开发布前严格过滤以消除敏感信息。WMDP有两个服务

    arXiv:2403.03218v1 Announce Type: cross  Abstract: The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 4,157 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two r
    
[^118]: 具有单边信息的状态约束零和微分博弈

    State-Constrained Zero-Sum Differential Games with One-Sided Information

    [https://arxiv.org/abs/2403.02741](https://arxiv.org/abs/2403.02741)

    我们将零和微分博弈的理论扩展到具有状态约束，并提出了计算行为策略所需的原始和对偶子动态原则。

    

    我们研究了具有状态约束和单边信息的零和微分博弈，其中知情玩家（玩家1）具有对于未知于不知情玩家（玩家2）的分类支付类型。玩家1的目标是在不违反约束的情况下最小化他的支付，而玩家2的目标是要么违反状态约束，要么最大化支付。我们的理论贡献是将Cardaliaguet（2007）对于没有状态约束的类似博弈价值存在性和对于玩家的共同信仰凸性的结果扩展到具有状态约束的微分博弈，并推导了用于计算行为策略的原始和对偶子动态原则。

    arXiv:2403.02741v1 Announce Type: cross  Abstract: We study zero-sum differential games with state constraints and one-sided information, where the informed player (Player 1) has a categorical payoff type unknown to the uninformed player (Player 2). The goal of Player 1 is to minimize his payoff without violating the constraints, while that of Player 2 is to either violate the state constraints, or otherwise, to maximize the payoff. One example of the game is a man-to-man matchup in football. Without state constraints, Cardaliaguet (2007) showed that the value of such a game exists and is convex to the common belief of players. Our theoretical contribution is an extension of this result to differential games with state constraints and the derivation of the primal and dual subdynamic principles necessary for computing the behavioral strategies. Compared with existing works on imperfect-information dynamic games that focus on scalability and generalization, our focus is instead on reveal
    
[^119]: 基于虚假阳性采样的数据增强方法提高3D物体检测准确性

    False Positive Sampling-based Data Augmentation for Enhanced 3D Object Detection Accuracy

    [https://arxiv.org/abs/2403.02639](https://arxiv.org/abs/2403.02639)

    本研究提出了一种名为虚假阳性采样的新增强技术，通过重新训练模型使用被识别为虚假阳性的点云，以提高3D物体检测模型的性能。

    

    近期的研究集中在提升3D物体检测模型的性能上。在各种方法中，地面真实数据采样被提出作为一种增强技术来解决有限的地面真实数据带来的挑战。然而，地面真实数据采样的一个固有问题是其倾向增加虚假阳性。因此，本研究旨在克服地面真实数据采样的局限，并通过开发一种名为虚假阳性采样的新增强技术来提高3D物体检测模型的性能。虚假阳性采样涉及重新训练模型，使用在模型预测中被识别为虚假阳性的点云。我们提出了一个同时利用地面真实数据和虚假阳性采样的算法，以及一个建立虚假阳性样本数据库的算法。此外，我们分析了由于虚假阳性采样导致的性能提升背后的原理。

    arXiv:2403.02639v1 Announce Type: cross  Abstract: Recent studies have focused on enhancing the performance of 3D object detection models. Among various approaches, ground-truth sampling has been proposed as an augmentation technique to address the challenges posed by limited ground-truth data. However, an inherent issue with ground-truth sampling is its tendency to increase false positives. Therefore, this study aims to overcome the limitations of ground-truth sampling and improve the performance of 3D object detection models by developing a new augmentation technique called false-positive sampling. False-positive sampling involves retraining the model using point clouds that are identified as false positives in the model's predictions. We propose an algorithm that utilizes both ground-truth and false-positive sampling and an algorithm for building the false-positive sample database. Additionally, we analyze the principles behind the performance enhancement due to false-positive sampl
    
[^120]: 高效的合作多智能体强化学习的情节记忆利用

    Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning

    [https://arxiv.org/abs/2403.01112](https://arxiv.org/abs/2403.01112)

    提出了用于合作多智能体强化学习的高效情节记忆利用（EMU），利用语义一致内存加速学习，有选择地促进理想的转换，避免陷入局部最优解。

    

    在合作多智能体强化学习（MARL）中，智能体旨在实现共同目标，比如击败敌人或进球。现有的MARL算法虽然有效，但仍需要大量学习时间，通常会在复杂任务中陷入局部最优解，随后未能发现达成目标的策略。为了解决这一问题，我们引入了用于MARL的高效情节记忆利用（EMU），其两个主要目标是：（a）通过利用来自情节缓冲区的语义一致内存加速强化学习，以及（b）有选择地促进理想的转换以防止局部收敛。为实现（a），EMU在MARL旁引入了可训练的编码器/解码器结构，创建了有助于探索性内存回忆的连贯记忆嵌入。为实现（b），EMU引入了一种基于状态愿望性的新颖奖励结构，称为情节激励。这种奖励改善了TD

    arXiv:2403.01112v1 Announce Type: new  Abstract: In cooperative multi-agent reinforcement learning (MARL), agents aim to achieve a common goal, such as defeating enemies or scoring a goal. Existing MARL algorithms are effective but still require significant learning time and often get trapped in local optima by complex tasks, subsequently failing to discover a goal-reaching policy. To address this, we introduce Efficient episodic Memory Utilization (EMU) for MARL, with two primary objectives: (a) accelerating reinforcement learning by leveraging semantically coherent memory from an episodic buffer and (b) selectively promoting desirable transitions to prevent local convergence. To achieve (a), EMU incorporates a trainable encoder/decoder structure alongside MARL, creating coherent memory embeddings that facilitate exploratory memory recall. To achieve (b), EMU introduces a novel reward structure called episodic incentive based on the desirability of states. This reward improves the TD 
    
[^121]: LAB：针对ChatBots的大规模对齐

    LAB: Large-Scale Alignment for ChatBots

    [https://arxiv.org/abs/2403.01081](https://arxiv.org/abs/2403.01081)

    介绍了一种名为LAB的方法，旨在克服大型语言模型训练中的可扩展性挑战，通过分类法指导的合成数据生成和多阶段调整框架，实现了对昂贵人工标注和GPT-4等专有模型依赖较少的大规模对齐，提供了一种可扩展、具有成本效益的解决方案，不会出现灾难性遗忘情况，进一步增强了LLM的训练效率。

    

    这项工作介绍了LAB（ChatBots的大规模对齐），这是一种旨在克服大型语言模型（LLM）训练中指令调整阶段的可扩展性挑战的创新方法。通过利用基于分类法的合成数据生成过程和多阶段调整框架，LAB显著减少对昂贵的人类注释和诸如GPT-4之类的专有模型的依赖。我们证明，使用LAB训练的模型在几个基准测试中的性能可以与使用传统人类注释或GPT-4生成的合成数据训练的模型相比具有竞争力。因此，在不会出现灾难性遗忘的情况下，提供了一种可扩展、具有成本效益的解决方案，以增强LLM的能力和指令遵循行为，标志着在高效训练各种应用的LLM方面迈出了一步。

    arXiv:2403.01081v1 Announce Type: new  Abstract: This work introduces LAB (Large-scale Alignment for chatBots), a novel methodology designed to overcome the scalability challenges in the instruction-tuning phase of large language model (LLM) training. Leveraging a taxonomy-guided synthetic data generation process and a multi-phase tuning framework, LAB significantly reduces reliance on expensive human annotations and proprietary models like GPT-4. We demonstrate that LAB-trained models can achieve competitive performance across several benchmarks compared to models trained with traditional human-annotated or GPT-4 generated synthetic data. Thus offering a scalable, cost-effective solution for enhancing LLM capabilities and instruction-following behaviors without the drawbacks of catastrophic forgetting, marking a step forward in the efficient training of LLMs for a wide range of applications.
    
[^122]: 合并来自不同初始化的文本变换器模型

    Merging Text Transformer Models from Different Initializations

    [https://arxiv.org/abs/2403.00986](https://arxiv.org/abs/2403.00986)

    研究了合并不同初始化的Transformer模型的技术，提出了一种模型合并技术以研究这些模型极小值之间的关系，并发现与模型平均相比，通过我们的方法合并这些模型始终可以获得较低的损失障碍。

    

    最近关于一次性基于排列的模型合并的工作表明，不同初始化的模型之间存在令人印象深刻的低或零障碍模连接。然而，尽管Transformer架构在语言领域中占主导地位，但这一领域的研究尚未延伸到Transformer架构。因此，在这项工作中，我们调查了独立Transformer极小值学习类似特征的程度，并提出了一种模型合并技术，以研究损失景观中这些极小值之间的关系。架构的具体细节，如其残差连接、多头注意力和离散的顺序输入，需要特定的干预措施，以便计算留在相同功能等价类中的模型排列。通过我们的方法合并这些模型，我们发现与对几个在一个maske上训练的模型进行模型平均相比，最小值之间的损失障碍一直较低。

    arXiv:2403.00986v1 Announce Type: cross  Abstract: Recent work on one-shot permutation-based model merging has shown impressive low- or zero-barrier mode connectivity between models from completely different initializations. However, this line of work has not yet extended to the Transformer architecture, despite its dominant popularity in the language domain. Therefore, in this work, we investigate the extent to which separate Transformer minima learn similar features, and propose a model merging technique to investigate the relationship between these minima in the loss landscape. The specifics of the architecture, like its residual connections, multi-headed attention, and discrete, sequential input, require specific interventions in order to compute model permutations that remain within the same functional equivalence class. In merging these models with our method, we consistently find lower loss barriers between minima compared to model averaging for several models trained on a maske
    
[^123]: Disaggregated Multi-Tower: 面向拓扑感知的高效大规模推荐建模技术

    Disaggregated Multi-Tower: Topology-aware Modeling Technique for Efficient Large-Scale Recommendation

    [https://arxiv.org/abs/2403.00877](https://arxiv.org/abs/2403.00877)

    Disaggregated Multi-Tower提出了一种面向拓扑感知的建模技术，通过SPTT、TM和TP三个组件实现了高效的大规模推荐，加速性能提升了1.9倍。

    

    我们研究了深度学习推荐模型的扁平架构、常见的分布式训练模式和分层数据中心拓扑之间的不匹配。为了解决相关的低效性，我们提出了Disaggregated Multi-Tower（DMT），这是一种建模技术，包括（1）语义保留的Tower Transform（SPTT），一个将单片全局嵌入查找过程分解为不相交塔以利用数据中心位置关系的新型训练模式；（2）Tower Module（TM），一个附加到每个塔的协同稠密组件，通过分层特征交互降低模型复杂性和通信量；和（3）Tower Partitioner（TP），一个特征分区器，系统地创建具有有意义特征交互和负载平衡分配的塔，通过学习的嵌入来保持模型质量和训练吞吐量。我们展示了DMT相比于最新的方法可以实现高达1.9倍的加速。

    arXiv:2403.00877v1 Announce Type: new  Abstract: We study a mismatch between the deep learning recommendation models' flat architecture, common distributed training paradigm and hierarchical data center topology. To address the associated inefficiencies, we propose Disaggregated Multi-Tower (DMT), a modeling technique that consists of (1) Semantic-preserving Tower Transform (SPTT), a novel training paradigm that decomposes the monolithic global embedding lookup process into disjoint towers to exploit data center locality; (2) Tower Module (TM), a synergistic dense component attached to each tower to reduce model complexity and communication volume through hierarchical feature interaction; and (3) Tower Partitioner (TP), a feature partitioner to systematically create towers with meaningful feature interactions and load balanced assignments to preserve model quality and training throughput via learned embeddings. We show that DMT can achieve up to 1.9x speedup compared to the state-of-th
    
[^124]: 直接与Chat-Fine-Tuned LLMs的草案模型对齐

    Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs

    [https://arxiv.org/abs/2403.00858](https://arxiv.org/abs/2403.00858)

    通过提出的框架，我们训练了一种用于Llama 2 Chat 7B或更大模型的草案模型，实现了加速推理，仅占原始大小的1.64％。

    

    文本生成与大型语言模型（LLMs）由于其自回归本质、巨大的参数数量和有限的内存带宽而被认为是内存密集型，通常导致低令牌速率。猜测解码已被提出作为LLM推理加速的解决方案。然而，在现代开源LLM系列中，例如Llama 2 7B，由于草案模型通常不可用，因此需要训练高质量的草案模型以通过猜测解码实现推理加速。在本文中，我们提出了一个简单的草案模型训练框架，用于直接与Chat-capable目标模型对齐。通过我们提出的框架，我们训练出Llama 2 Chat Drafter 115M，这是一个适用于Llama 2 Chat 7B或更大模型的草案模型，仅占原始大小的1.64％。我们的训练框架仅包括预训练、蒸馏数据集生成和使用知识蒸馏进行微调，没有额外的对齐步骤。

    arXiv:2403.00858v1 Announce Type: cross  Abstract: Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates. Speculative decoding has been proposed as a solution for LLM inference acceleration. However, since draft models are often unavailable in the modern open-source LLM families, e.g., for Llama 2 7B, training a high-quality draft model is required to enable inference acceleration via speculative decoding. In this paper, we propose a simple draft model training framework for direct alignment to chat-capable target models. With the proposed framework, we train Llama 2 Chat Drafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\% of the original size. Our training framework only consists of pretraining, distillation dataset generation, and finetuning with knowledge distillation, with no additional align
    
[^125]: 通过语义感知置换训练来缓解逆转诅咒

    Mitigating Reversal Curse via Semantic-aware Permutation Training

    [https://arxiv.org/abs/2403.00758](https://arxiv.org/abs/2403.00758)

    逆转诅咒问题是导致因果语言模型无法进行双向推理的根本原因之一，在这篇论文中，我们提出了通过语义感知的置换训练来缓解这一问题。

    

    大型语言模型（LLM）在各种任务中取得了令人印象深刻的表现，然而最近的研究表明，因果关系的LLM遭遇了“逆转诅咒”。一个典型的例子是，模型知道“A的父亲是B”，但无法推理出“B的孩子是A”。这一局限性对人工通用智能（AGI）的进展构成了挑战，因为它暗示了模型在理解和应用双向推理方面存在差距。本文首先进行了大量评估，并确定了逆转诅咒的根本原因在于训练和推断阶段之间的词序不同，即因果语言模型在训练数据中预测先行词的能力不足。因此，考虑到在训练数据上进行排列可以被视为潜在解决方案，因为这可以使模型预测先行词或标记。然而，先前的排列方法可能受到截断影响。

    arXiv:2403.00758v1 Announce Type: cross  Abstract: While large language models (LLMs) have achieved impressive performance across diverse tasks, recent studies showcase that causal LLMs suffer from the "reversal curse". It is a typical example that the model knows "A's father is B", but is unable to reason "B's child is A". This limitation poses a challenge to the advancement of artificial general intelligence (AGI), as it suggests a gap in the models' ability to comprehend and apply bidirectional reasoning. In this paper, we first conduct substantial evaluation and identify that the root cause of the reversal curse lies in the different word order between the training and inference stage, namely, the poor ability of causal language models to predict antecedent words within the training data. Accordingly, permutation on the training data is considered as a potential solution, since this can make the model predict antecedent words or tokens. However, previous permutation methods may dis
    
[^126]: 深度强化学习：一个凸优化方法

    Deep Reinforcement Learning: A Convex Optimization Approach

    [https://arxiv.org/abs/2402.19212](https://arxiv.org/abs/2402.19212)

    本文提出了一种深度强化学习的凸优化方法，通过每集使用凸优化来训练神经网络近似最优$Q$-函数，确保收敛参数可以无限接近最优参数。

    

    在本文中，我们考虑了具有连续状态和动作空间的非线性系统的强化学习。我们提出了一种分集学习算法，其中我们在每个集中使用凸优化来找到最优$Q$-函数的两层神经网络近似。凸优化方法确保每个集合中计算的权重是最优的，关于当前集合的采样状态和动作。对于稳定的非线性系统，我们证明了算法收敛，并且经过训练的神经网络的收敛参数可以与最优神经网络参数无限接近。特别是，如果正则化参数为$\rho$，时间长度为$T$，那么经过训练的神经网络的参数收敛到$w$，其中$w$与最优参数$w^\star$之间的距离受到$\mathcal{O}(\rho T^{-1})$的限制。

    arXiv:2402.19212v1 Announce Type: cross  Abstract: In this paper, we consider reinforcement learning of nonlinear systems with continuous state and action spaces. We present an episodic learning algorithm, where we for each episode use convex optimization to find a two-layer neural network approximation of the optimal $Q$-function. The convex optimization approach guarantees that the weights calculated at each episode are optimal, with respect to the given sampled states and actions of the current episode. For stable nonlinear systems, we show that the algorithm converges and that the converging parameters of the trained neural network can be made arbitrarily close to the optimal neural network parameters. In particular, if the regularization parameter is $\rho$ and the time horizon is $T$, then the parameters of the trained neural network converge to $w$, where the distance between $w$ from the optimal parameters $w^\star$ is bounded by $\mathcal{O}(\rho T^{-1})$. That is, when the nu
    
[^127]: 通过主动迁移学习自动测试空间相关环境假设

    Automated Testing of Spatially-Dependent Environmental Hypotheses through Active Transfer Learning

    [https://arxiv.org/abs/2402.18064](https://arxiv.org/abs/2402.18064)

    结合了迁移学习和主动学习的方法，通过多任务高斯过程和基于信息的目标函数，可以在实时评估假设的数量之间的关系，从而提高规划效率。

    

    有效采样对户外信息收集应用至关重要，因为高昂的采样成本，如时间、能量，以及潜在的环境破坏。利用现有的先验数据可以是提高效率的强大工具。然而，这些数据与感兴趣的数量之间的关系通常事先未知，从而限制了利用此知识进行改进规划效率的能力。为此，这项工作通过多任务高斯过程和基于信息的目标函数结合了迁移学习和主动学习。通过这种组合，它可以研究假设的数量之间的关系空间，并即时评估这些假设，使此新知识能够立即为未来计划所利用。所提出方法的性能针对合成数据进行评估，并表明可以评估

    arXiv:2402.18064v1 Announce Type: cross  Abstract: The efficient collection of samples is an important factor in outdoor information gathering applications on account of high sampling costs such as time, energy, and potential destruction to the environment. Utilization of available a-priori data can be a powerful tool for increasing efficiency. However, the relationships of this data with the quantity of interest are often not known ahead of time, limiting the ability to leverage this knowledge for improved planning efficiency. To this end, this work combines transfer learning and active learning through a Multi-Task Gaussian Process and an information-based objective function. Through this combination it can explore the space of hypothetical inter-quantity relationships and evaluate these hypotheses in real-time, allowing this new knowledge to be immediately exploited for future plans. The performance of the proposed method is evaluated against synthetic data and is shown to evaluate 
    
[^128]: 具有增强可检测性和语义连贯性的大型语言模型的特定令牌水印技术

    Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models

    [https://arxiv.org/abs/2402.18059](https://arxiv.org/abs/2402.18059)

    提出一种利用多目标优化方法的水印技术，通过轻量级网络生成特定令牌水印logits和分割比率，在保证检测性的同时提升了文本的语义完整性。

    

    大型语言模型生成高质量的响应，潜在地存在误导信息的问题，强调了通过区分人工智能生成和人类撰写的文本来加以规范的必要性。水印技术在这种情况下至关重要，它涉及在LLM推理阶段向文本中嵌入隐藏标记，而这对人类来说是不可感知的。然而，当前的水印算法面临着实现插入水印的可检测性和生成文本的语义完整性两方面的挑战，增强其中一个方面常常会损害另一个方面。为了克服这一问题，我们引入了一种新颖的多目标优化（MOO）方法，用于水印技术，利用轻量级网络生成特定令牌水印logits和分割比率。通过利用MOO来优化检测和语义目标函数，我们的方法同时实现了可检测性和语义完整性。实验结果表明，我们的方法在...

    arXiv:2402.18059v1 Announce Type: cross  Abstract: Large language models generate high-quality responses with potential misinformation, underscoring the need for regulation by distinguishing AI-generated and human-written texts. Watermarking is pivotal in this context, which involves embedding hidden markers in texts during the LLM inference phase, which is imperceptible to humans. Current watermarking algorithms, however, face the challenge of achieving both the detectability of inserted watermarks and the semantic integrity of generated texts, where enhancing one aspect often undermines the other. To overcome this, we introduce a novel multi-objective optimization (MOO) approach for watermarking that utilizes lightweight networks to generate token-specific watermarking logits and splitting ratios. By leveraging MOO to optimize for both detection and semantic objective functions, our method simultaneously achieves detectability and semantic integrity. Experimental results show that ou
    
[^129]: 通过隐含正交偏置发现对称群结构

    Discovering Symmetry Group Structures via Implicit Orthogonality Bias

    [https://arxiv.org/abs/2402.17002](https://arxiv.org/abs/2402.17002)

    HyperCube网络通过独特的因式分解架构和正则化器，成功学习了对称群的操作，能够高效地恢复完整操作表，并形成广义傅里叶基进行群卷积。

    

    我们介绍了HyperCube网络，一个用于自动发现数据中对称群结构的新方法。关键创新在于独特的因式分解架构，结合一种新颖的正则化器，向学习正交表示灌输了强大的归纳偏差。这利用了表示理论的一个基本定理，即所有紧致/有限群都可以由正交矩阵表示。HyperCube能够高效地从部分观测数据中学习通用群操作，成功恢复完整的操作表。值得注意的是，所学习出的因素直接对应于底层群的精确矩阵表示。此外，这些因素捕捉到了群的完整不可约表示集合，形成了执行群卷积的广义傅里叶基。在对群和非群符号操作进行的大量实验证明，HyperCube展示了10

    arXiv:2402.17002v1 Announce Type: new  Abstract: We introduce the HyperCube network, a novel approach for autonomously discovering symmetry group structures within data. The key innovation is a unique factorization architecture coupled with a novel regularizer that instills a powerful inductive bias towards learning orthogonal representations. This leverages a fundamental theorem of representation theory that all compact/finite groups can be represented by orthogonal matrices. HyperCube efficiently learns general group operations from partially observed data, successfully recovering complete operation tables. Remarkably, the learned factors correspond directly to exact matrix representations of the underlying group. Moreover, these factors capture the group's complete set of irreducible representations, forming the generalized Fourier basis for performing group convolutions. In extensive experiments with both group and non-group symbolic operations, HyperCube demonstrates a dramatic 10
    
[^130]: 连续时间强化学习中深度残差网络的\emph{先验估计}

    A prior Estimates for Deep Residual Network in Continuous-time Reinforcement Learning

    [https://arxiv.org/abs/2402.16899](https://arxiv.org/abs/2402.16899)

    本研究针对连续时间控制问题，提出了一种可以直接分析Bellman最优损失\emph{先验}泛化误差的方法，避免了有界性假设，并通过最大算子的分解方法实现了损失函数的转换。

    

    深度强化学习在许多大规模实际应用中表现出色。然而，现有的性能分析忽略了连续时间控制问题的独特特征，无法直接估计Bellman最优损失的泛化误差，并且需要一个有界性假设。我们的工作侧重于连续时间控制问题，并提出了一种适用于所有满足半群和Lipschitz性质的问题的方法。在该方法下，我们能够直接分析Bellman最优损失的\emph{先验}泛化误差。该方法的核心在于损失函数的两次转换。为了完成转换，我们提出了最大算子的分解方法。此外，这个分析方法不需要有界性假设。最终我们维得到了一个没有“维度诅咒”的\emph{先验}泛化误差。

    arXiv:2402.16899v1 Announce Type: cross  Abstract: Deep reinforcement learning excels in numerous large-scale practical applications. However, existing performance analyses ignores the unique characteristics of continuous-time control problems, is unable to directly estimate the generalization error of the Bellman optimal loss and require a boundedness assumption. Our work focuses on continuous-time control problems and proposes a method that is applicable to all such problems where the transition function satisfies semi-group and Lipschitz properties. Under this method, we can directly analyze the \emph{a priori} generalization error of the Bellman optimal loss. The core of this method lies in two transformations of the loss function. To complete the transformation, we propose a decomposition method for the maximum operator. Additionally, this analysis method does not require a boundedness assumption. Finally, we obtain an \emph{a priori} generalization error without the curse of dime
    
[^131]: 关于代码大语言模型中的木马签名

    On Trojan Signatures in Large Language Models of Code

    [https://arxiv.org/abs/2402.16896](https://arxiv.org/abs/2402.16896)

    本文研究了在大型代码语言模型中木马签名的问题，并发现木马签名无法推广到代码语言模型，具有重要的研究意义。

    

    木马签名是由Fields等人(2021)描述的，是被感染类别参数（权重）与未被感染类别参数之间分布的显著差异，可以用于检测被感染的模型。Fields等人(2021)发现了计算机视觉分类任务中的木马签名，比如Resnet、WideResnet、Densenet和VGG。本文研究了源代码大语言模型中分类层参数的这种签名。我们的结果表明，木马签名不能泛化到代码的大语言模型。我们发现，即使在更明确的设置下对模型进行了中毒（用预训练的权重冻结进行微调），被感染的代码模型也仍然固执。我们对两个二进制分类任务进行了九个感染模型的分析：克隆和缺陷检测。据我们所知，这是第一个检查基于权重的木马的工作。

    arXiv:2402.16896v1 Announce Type: cross  Abstract: Trojan signatures, as described by Fields et al. (2021), are noticeable differences in the distribution of the trojaned class parameters (weights) and the non-trojaned class parameters of the trojaned model, that can be used to detect the trojaned model. Fields et al. (2021) found trojan signatures in computer vision classification tasks with image models, such as, Resnet, WideResnet, Densenet, and VGG. In this paper, we investigate such signatures in the classifier layer parameters of large language models of source code.   Our results suggest that trojan signatures could not generalize to LLMs of code. We found that trojaned code models are stubborn, even when the models were poisoned under more explicit settings (finetuned with pre-trained weights frozen). We analyzed nine trojaned models for two binary classification tasks: clone and defect detection. To the best of our knowledge, this is the first work to examine weight-based troj
    
[^132]: 图学习在分布变化下的研究：领域自适应、外部分布和持续学习的综述

    Graph Learning under Distribution Shifts: A Comprehensive Survey on Domain Adaptation, Out-of-distribution, and Continual Learning

    [https://arxiv.org/abs/2402.16374](https://arxiv.org/abs/2402.16374)

    该综述调查了解决图学习中分布变化问题的最新方法、策略和见解。

    

    图学习在各种应用场景中发挥着关键作用，并且因其在建模由图结构数据表示的复杂数据关系方面的有效性而引起了广泛关注，从社交网络分析到推荐系统。现实中，真实世界的图数据通常随时间动态变化，节点属性和边结构也会发生变化，导致严重的图数据分布转移问题。这个问题受到分布转移多样性和复杂性的影响，这些转移可以显著影响图学习方法在降低泛化和适应能力方面的性能，给它们的有效性带来了重大挑战。在这项调查中，我们对解决图学习背景下的分布变化的最新方法、策略和见解进行了全面回顾和总结。

    arXiv:2402.16374v1 Announce Type: new  Abstract: Graph learning plays a pivotal role and has gained significant attention in various application scenarios, from social network analysis to recommendation systems, for its effectiveness in modeling complex data relations represented by graph structural data. In reality, the real-world graph data typically show dynamics over time, with changing node attributes and edge structure, leading to the severe graph data distribution shift issue. This issue is compounded by the diverse and complex nature of distribution shifts, which can significantly impact the performance of graph learning methods in degraded generalization and adaptation capabilities, posing a substantial challenge to their effectiveness. In this survey, we provide a comprehensive review and summary of the latest approaches, strategies, and insights that address distribution shifts within the context of graph learning. Concretely, according to the observability of distributions 
    
[^133]: 在没有基准实况的情况下对大型语言模型进行排名

    Ranking Large Language Models without Ground Truth

    [https://arxiv.org/abs/2402.14860](https://arxiv.org/abs/2402.14860)

    不需要基准实况或参考响应的条件下，通过考虑模型的三元组来排名大型语言模型，并提出了两种排名方法。

    

    随着大型语言模型（LLMs）的普及和影响力的增强，评估和排名LLMs已成为一个重要问题。现有的评估方法要么需要获取昂贵的人类响应，要么使用LLMs成对地互相评估，这可能不够可靠。本文提供了一个新的视角，在给定一组提示数据集（比如问题、说明等）和一组LLMs的情况下，我们在没有任何基准实况或参考响应的情况下对它们进行排名。受到现实生活的启发，其中专家和有知识的人都能识别一个新手，我们的主要思路是考虑模型的三元组，其中每个模型评估其他两个模型，能够以很高的概率正确识别最差的模型。我们还分析了我们的想法并提供了成功的充分条件。通过反复应用这一想法，我们提出了两种对LLMs进行排名的方法。

    arXiv:2402.14860v1 Announce Type: cross  Abstract: Evaluation and ranking of large language models (LLMs) has become an important problem with the proliferation of these models and their impact. Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable. In this paper, we provide a novel perspective where, given a dataset of prompts (viz. questions, instructions, etc.) and a set of LLMs, we rank them without access to any ground truth or reference responses. Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability. We also analyze our idea and provide sufficient conditions for it to succeed. Applying this idea repeatedly, we propose two methods to rank LLMs. In experiments on different generati
    
[^134]: 强化学习辅助的变分量子算法量子架构搜索

    Reinforcement learning-assisted quantum architecture search for variational quantum algorithms

    [https://arxiv.org/abs/2402.13754](https://arxiv.org/abs/2402.13754)

    通过强化学习自动搜索变分电路的最佳结构，改善了VQAs的性能。

    

    在嘈杂中等规模量子（NISQ）时代，一个重要障碍是确定功能性量子电路。这些电路必须同时符合当前量子硬件限制所施加的约束。变分量子算法（VQA）是一类量子-经典优化算法，旨在解决当前可用量子设备中的这些挑战。本论文侧重于电路结构，通过使用强化学习（RL）自动搜索变分电路的最优结构，改善了VQAs的性能。论文内通过评估电路的深度、门和参数的总数以及准确性来确定电路的优越性。

    arXiv:2402.13754v1 Announce Type: cross  Abstract: A significant hurdle in the noisy intermediate-scale quantum (NISQ) era is identifying functional quantum circuits. These circuits must also adhere to the constraints imposed by current quantum hardware limitations. Variational quantum algorithms (VQAs), a class of quantum-classical optimization algorithms, were developed to address these challenges in the currently available quantum devices. However, the overall performance of VQAs depends on the initialization strategy of the variational circuit, the structure of the circuit (also known as ansatz), and the configuration of the cost function. Focusing on the structure of the circuit, in this thesis, we improve the performance of VQAs by automating the search for an optimal structure for the variational circuits using reinforcement learning (RL). Within the thesis, the optimality of a circuit is determined by evaluating its depth, the overall count of gates and parameters, and its accu
    
[^135]: 复兴多变量时间序列预测：可学习分解与跨系列依赖关系和内部变化建模

    Revitalizing Multivariate Time Series Forecasting: Learnable Decomposition with Inter-Series Dependencies and Intra-Series Variations Modeling

    [https://arxiv.org/abs/2402.12694](https://arxiv.org/abs/2402.12694)

    引入可学习分解策略和双注意力模块，同时捕捉跨系列依赖和内部变化，以应对复杂的多变量时间序列预测挑战。

    

    预测多变量时间序列是至关重要的，要求精确建模错综复杂模式，包括跨时间序列的依赖关系和内部变动。每个时间序列具有独特的趋势特征带来挑战，现有方法依赖基本的移动平均核可能难以处理现实数据中的非线性结构和复杂趋势。基于此，我们引入了一个可学习的分解策略，更合理地捕捉动态趋势信息。此外，我们提出了一个双注意力模块，专门用于同时捕捉跨系列依赖关系和内部变化，以实现更好的时间序列预测，其中通过通道自注意力和自回归自注意力实现。为了评估我们方法的有效性，我们在八个开源数据集上进行了实验，并将其与最先进的方法进行了比较。通过比较结果，我们的 Leddam...

    arXiv:2402.12694v1 Announce Type: new  Abstract: Predicting multivariate time series is crucial, demanding precise modeling of intricate patterns, including inter-series dependencies and intra-series variations. Distinctive trend characteristics in each time series pose challenges, and existing methods, relying on basic moving average kernels, may struggle with the non-linear structure and complex trends in real-world data. Given that, we introduce a learnable decomposition strategy to capture dynamic trend information more reasonably. Additionally, we propose a dual attention module tailored to capture inter-series dependencies and intra-series variations simultaneously for better time series forecasting, which is implemented by channel-wise self-attention and autoregressive self-attention. To evaluate the effectiveness of our method, we conducted experiments across eight open-source datasets and compared it with the state-of-the-art methods. Through the comparison results, our Leddam
    
[^136]: AnyGPT：统一的多模式离散序列建模语言模型

    AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling

    [https://arxiv.org/abs/2402.12226](https://arxiv.org/abs/2402.12226)

    AnyGPT是一个统一的多模态语言模型，通过离散表示实现各种模态的统一处理，能够在不改变大型语言模型架构或训练方式的情况下稳定训练，为新模态的无缝整合提供了可能。

    

    我们介绍了 AnyGPT，这是一个任意多模式语言模型，利用离散表示统一处理各种模态，包括语音、文本、图像和音乐。AnyGPT 可以稳定训练，无需对当前大型语言模型（LLM）架构或训练范式进行任何改动。相反，它仅依赖于数据级预处理，促进了新模态的无缝集成到LLM中，类似于新语言的整合。我们构建了一个多模式文本中心的数据集，用于多模式对齐预训练。利用生成模型，我们合成了第一个大规模任意多模式指令数据集。它包括108k个多轮对话示例，精细地交织各种模态，从而使模型能够处理多模态输入和输出的任意组合。实验结果表明，AnyGPT能够促进...

    arXiv:2402.12226v1 Announce Type: cross  Abstract: We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitat
    
[^137]: MUSTARD：掌握定理和证明数据的统一合成

    MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data

    [https://arxiv.org/abs/2402.08957](https://arxiv.org/abs/2402.08957)

    这项工作介绍了MUSTARD，一种掌握定理和证明数据统一合成的数据生成框架，通过三个阶段的合成，实现了高质量和多样化的问题和推理步骤的生成。

    

    最近，大型语言模型（LLMs）在各种任务中取得了显著进展，包括数学推理和定理证明。由于这两个任务需要严格和形式化的多步推理，它们是探索LLMs推理能力的吸引领域，但仍面临重要挑战。以前的研究如Chain-of-Thought（CoT）揭示了中间步骤指导的有效性。然而，这种逐步注释需要大量的劳动力，导致当前基准测试的训练步骤不足。为了填补这一空白，本研究引入了MUSTARD，一种数据生成框架，可以主导高质量和多样化的定理和证明数据的统一合成。MUSTARD通过三个阶段合成数据：（1）它随机选择几个数学概念作为问题的类别。（2）然后，它使用选定的概念提示生成性语言模型，以获得问题和它们的推理步骤。

    arXiv:2402.08957v1 Announce Type: new Abstract: Recent large language models (LLMs) have witnessed significant advancement in various tasks, including mathematical reasoning and theorem proving. As these two tasks require strict and formal multi-step inference, they are appealing domains for exploring the reasoning ability of LLMs but still face important challenges. Previous studies such as Chain-of-Thought (CoT) have revealed the effectiveness of intermediate steps guidance. However, such step-wise annotation requires heavy labor, leading to insufficient training steps for current benchmarks. To fill this gap, this work introduces MUSTARD, a data generation framework that masters uniform synthesis of theorem and proof data of high quality and diversity. MUSTARD synthesizes data in three stages: (1) It samples a few mathematical concept seeds as the problem category. (2) Then, it prompts a generative language model with the sampled concepts to obtain both the problems and their step-w
    
[^138]: Pathformer: 多尺度自适应路径的时间序列预测模型

    Pathformer: Multi-scale transformers with Adaptive Pathways for Time Series Forecasting

    [https://arxiv.org/abs/2402.05956](https://arxiv.org/abs/2402.05956)

    本文提出了一种名为Pathformer的多尺度自适应路径的Transformer模型，用于时间序列预测。通过整合时间分辨率和时间距离进行多尺度建模，并使用自适应路径来优化建模过程，可以提高预测准确性和泛化能力。

    

    基于Transformer的模型在时间序列预测中取得了一些成功。现有的方法主要从有限或固定尺度对时间序列进行建模，这使得捕捉跨多个尺度的不同特征变得具有挑战性。本文提出了一种多尺度自适应路径（Pathformer）的Transformer模型。该模型同时整合了时间分辨率和时间距离进行多尺度建模。多尺度划分运用不同大小的数据块将时间序列分割成不同的时间分辨率。基于每个尺度的划分，对这些数据块进行双重注意力机制，以捕捉全局相关性和局部细节作为时间依赖关系。我们进一步通过自适应路径来丰富多尺度Transformer，该路径可以根据输入时间序列中不断变化的时间动态调整多尺度建模过程，提高Pathformer的预测准确性和泛化能力。在11个真实数据集上进行了大量实验。

    Transformer-based models have achieved some success in time series forecasting. Existing methods mainly model time series from limited or fixed scales, making it challenging to capture different characteristics spanning various scales. In this paper, we propose multi-scale transformers with adaptive pathways (Pathformer). The proposed Transformer integrates both temporal resolution and temporal distance for multi-scale modeling. Multi-scale division divides the time series into different temporal resolutions using patches of various sizes. Based on the division of each scale, dual attention is performed over these patches to capture global correlations and local details as temporal dependencies. We further enrich the multi-scale transformer with adaptive pathways, which adaptively adjust the multi-scale modeling process based on the varying temporal dynamics in the input time series, improving the prediction accuracy and generalization of Pathformer. Extensive experiments on eleven rea
    
[^139]: 可扩展的不平衡最优输运生成建模中的Wasserstein渐变流

    Scalable Wasserstein Gradient Flow for Generative Modeling through Unbalanced Optimal Transport

    [https://arxiv.org/abs/2402.05443](https://arxiv.org/abs/2402.05443)

    本文介绍了一种可扩展的生成模型，称为Semi-dual JKO (S-JKO)，通过采用半对偶形式的JKO步骤，降低了训练复杂性，并在WGF上有显著的性能改进。

    

    Wasserstein渐变流（WGF）描述了Wasserstein空间中概率密度的梯度动力学。WGF提供了在概率分布上进行优化的有希望的方法。数值上近似连续的WGF需要时间离散化方法。其中最著名的方法是JKO方案。在这方面，以前的WGF模型采用JKO方案，并为每个JKO步骤参数化传输映射。然而，这种方法导致了与JKO步骤数量K成二次增长的训练复杂性$O(K^2)$。这严重限制了WGF模型的可扩展性。在本文中，我们介绍了一种可扩展的基于WGF的生成模型，称为半对偶JKO（S-JKO）。我们的模型基于JKO步骤的半对偶形式，通过JKO步骤与不平衡最优输运之间的等价性得到。我们的方法将训练复杂性降低到$O(K)$。我们证明了我们的模型明显优于现有的基于WGF的生成模型。

    Wasserstein Gradient Flow (WGF) describes the gradient dynamics of probability density within the Wasserstein space. WGF provides a promising approach for conducting optimization over the probability distributions. Numerically approximating the continuous WGF requires the time discretization method. The most well-known method for this is the JKO scheme. In this regard, previous WGF models employ the JKO scheme and parametrize transport map for each JKO step. However, this approach results in quadratic training complexity $O(K^2)$ with the number of JKO step $K$. This severely limits the scalability of WGF models. In this paper, we introduce a scalable WGF-based generative model, called Semi-dual JKO (S-JKO). Our model is based on the semi-dual form of the JKO step, derived from the equivalence between the JKO step and the Unbalanced Optimal Transport. Our approach reduces the training complexity to $O(K)$. We demonstrate that our model significantly outperforms existing WGF-based gener
    
[^140]: 缩小SGP4和高精度传播之间的差距：通过可微编程

    Closing the Gap Between SGP4 and High-Precision Propagation via Differentiable Programming

    [https://arxiv.org/abs/2402.04830](https://arxiv.org/abs/2402.04830)

    本研究介绍了dSGP4，一种使用PyTorch实现的可微版本的SGP4。通过可微化，dSGP4实现了轨道传播的高精度，并且适用于各种与太空相关的应用，包括卫星轨道确定、状态转换、协方差传播等。

    

    简化的第四级摄动(SGP4)轨道传播方法被广泛用于快速可靠地预测地球轨道物体的位置和速度。尽管不断改进，SGP模型仍然缺乏数值传播器的精度，后者的误差显著较小。本研究提出了dSGP4，一种使用PyTorch实现的新型可微版本的SGP4。通过使SGP4可微化，dSGP4便于进行各种与太空相关的应用，包括航天器轨道确定、状态转换、协方差转换、状态转移矩阵计算和协方差传播。此外，dSGP4的PyTorch实现允许在批量的TLE（两行参数）集上进行尴尬的并行轨道传播，利用CPU、GPU和分布式预测卫星位置的高级硬件的计算能力。此外，dSGP4的可微性使其能与模式集成。

    The Simplified General Perturbations 4 (SGP4) orbital propagation method is widely used for predicting the positions and velocities of Earth-orbiting objects rapidly and reliably. Despite continuous refinement, SGP models still lack the precision of numerical propagators, which offer significantly smaller errors. This study presents dSGP4, a novel differentiable version of SGP4 implemented using PyTorch. By making SGP4 differentiable, dSGP4 facilitates various space-related applications, including spacecraft orbit determination, state conversion, covariance transformation, state transition matrix computation, and covariance propagation. Additionally, dSGP4's PyTorch implementation allows for embarrassingly parallel orbital propagation across batches of Two-Line Element Sets (TLEs), leveraging the computational power of CPUs, GPUs, and advanced hardware for distributed prediction of satellite positions at future times. Furthermore, dSGP4's differentiability enables integration with mode
    
[^141]: 《读玩游戏（R2-Play）: 多模态游戏指导下的决策 Transformer》

    Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction

    [https://arxiv.org/abs/2402.04154](https://arxiv.org/abs/2402.04154)

    本论文探索了为智能体提供增强形式的任务指导，使其能够理解游戏指导并实现"读玩游戏"的能力。通过将多模态指导调优的成功应用于视觉任务中的强化学习任务，构建了一组... (内容太长，无法继续显示)

    

    在人工智能领域，开发一款通用智能体一直是一个长期的目标。先前的研究利用来自各种任务的大量离线数据集，在强化学习的多任务场景中表现出了出色的性能。然而，这些工作在扩展到新任务方面面临挑战。最近的方法将文本指导或视觉轨迹整合到决策网络中，提供任务特定的上下文提示，代表了一个有前途的方向。然而，观察到仅依赖于文本指导或视觉轨迹对于准确传达任务的上下文信息是不足够的。本文探索了增强智能体任务指导的形式，使其能够理解游戏指导，从而实现"读玩游戏"的能力。受到多模态指导调优在视觉任务中的成功启发，我们将基于视觉的强化学习任务视为一个长期视觉任务，并构建了一组... (内容太长，无法继续显示)

    Developing a generalist agent is a longstanding objective in artificial intelligence. Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning.However, these works encounter challenges in extending their capabilities to new tasks.Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction.However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks.This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a "read-to-play" capability.Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set 
    
[^142]: 标准 Gaussian 过程在高维贝叶斯优化中足以应对

    Standard Gaussian Process is All You Need for High-Dimensional Bayesian Optimization

    [https://arxiv.org/abs/2402.02746](https://arxiv.org/abs/2402.02746)

    标准 Gaussian 过程在高维贝叶斯优化中表现优秀，经验证据显示其在函数估计和协方差建模中克服了高维输入困难，比专门为高维优化设计的方法表现更好。

    

    长期以来，人们普遍认为使用标准 Gaussian 过程（GP）进行贝叶斯优化（BO），即标准 BO，在高维优化问题中效果不佳。这种观念可以部分归因于 Gaussian 过程在协方差建模和函数估计中对高维输入的困难。虽然这些担忧看起来合理，但缺乏支持这种观点的经验证据。本文系统地研究了在各种合成和真实世界基准问题上，使用标准 GP 回归进行高维优化的贝叶斯优化。令人惊讶的是，标准 GP 的表现始终位于最佳范围内，往往比专门为高维优化设计的现有 BO 方法表现更好。与刻板印象相反，我们发现标准 GP 可以作为学习高维目标函数的能力强大的代理。在没有强结构假设的情况下，使用标准 GP 进行 BO 可以获得非常好的性能。

    There has been a long-standing and widespread belief that Bayesian Optimization (BO) with standard Gaussian process (GP), referred to as standard BO, is ineffective in high-dimensional optimization problems. This perception may partly stem from the intuition that GPs struggle with high-dimensional inputs for covariance modeling and function estimation. While these concerns seem reasonable, empirical evidence supporting this belief is lacking. In this paper, we systematically investigated BO with standard GP regression across a variety of synthetic and real-world benchmark problems for high-dimensional optimization. Surprisingly, the performance with standard GP consistently ranks among the best, often outperforming existing BO methods specifically designed for high-dimensional optimization by a large margin. Contrary to the stereotype, we found that standard GP can serve as a capable surrogate for learning high-dimensional target functions. Without strong structural assumptions, BO wit
    
[^143]: 多区域马尔可夫高斯过程：一种发现多个脑区之间方向性通讯的高效方法

    Multi-Region Markovian Gaussian Process: An Efficient Method to Discover Directional Communications Across Multiple Brain Regions

    [https://arxiv.org/abs/2402.02686](https://arxiv.org/abs/2402.02686)

    本研究提出了一种名为多区域马尔可夫高斯过程的方法，将高斯过程和线性动态系统相结合，有效地发现了多个脑区之间的方向性通讯。通过建立LDS与多输出GP之间的联系，该模型实现了线性推断并提供了可解释的低维表示。

    

    研究不同脑区之间复杂的相互作用对神经科学至关重要。各种统计方法已经探索了多个脑区之间潜在的通讯。两个主要的类别是高斯过程（GP）和线性动态系统（LDS），每个方法都具有独特的优势。基于GP的方法有效地发现了潜在变量，如频带和通讯方向。相反，基于LDS的方法在计算效率上高，但在潜在表示方面缺乏强大的表达能力。在本研究中，我们通过创建一个与多输出GP相对应的LDS，即多区域马尔可夫高斯过程（MRM-GP），将这两种方法合二为一。我们的工作首次建立了LDS和多输出GP之间的联系，在神经记录的潜在空间中明确建模了频率和相位延迟。因此，该模型在时间点上实现了线性推断成本，并提供了可解释的低维表示。

    Studying the complex interactions between different brain regions is crucial in neuroscience. Various statistical methods have explored the latent communication across multiple brain regions. Two main categories are the Gaussian Process (GP) and Linear Dynamical System (LDS), each with unique strengths. The GP-based approach effectively discovers latent variables such as frequency bands and communication directions. Conversely, the LDS-based approach is computationally efficient but lacks powerful expressiveness in latent representation. In this study, we merge both methodologies by creating an LDS mirroring a multi-output GP, termed Multi-Region Markovian Gaussian Process (MRM-GP). Our work is the first to establish a connection between an LDS and a multi-output GP that explicitly models frequencies and phase delays within the latent space of neural recordings. Consequently, the model achieves a linear inference cost over time points and provides an interpretable low-dimensional repre
    
[^144]: 大型语言模型能否取代经济选择预测实验室？

    Can Large Language Models Replace Economic Choice Prediction Labs?

    [https://arxiv.org/abs/2401.17435](https://arxiv.org/abs/2401.17435)

    该论文研究大型语言模型是否能够取代经济实验室进行选择预测，并通过相关实验证明了其可行性。

    

    经济选择预测是一项具有挑战性的重要任务，往往受限于获取人类选择数据的困难。实验经济学研究在很大程度上专注于简单的选择环境。最近，人工智能界以两种方式为该努力做出了贡献：考虑大型语言模型是否可以代替人类在上述简单选择预测环境中，以及通过机器学习视角研究更复杂但仍严格的实验经济学环境，包括不完全信息、重复博弈和基于自然语言交流的说服游戏。这引发了一个重要的灵感：大型语言模型是否能够完全模拟经济环境，并生成用于高效人类选择预测的数据，替代复杂的经济实验室研究？我们在这个主题上开创了研究，并展示了其可行性。特别是，我们表明仅在大型语言模型生成的数据上训练的模型可以有效地进行预测。

    Economic choice prediction is an essential challenging task, often constrained by the difficulties in acquiring human choice data. Indeed, experimental economics studies had focused mostly on simple choice settings. The AI community has recently contributed to that effort in two ways: considering whether LLMs can substitute for humans in the above-mentioned simple choice prediction settings, and the study through ML lens of more elaborated but still rigorous experimental economics settings, employing incomplete information, repetitive play, and natural language communication, notably language-based persuasion games. This leaves us with a major inspiration: can LLMs be used to fully simulate the economic environment and generate data for efficient human choice prediction, substituting for the elaborated economic lab studies? We pioneer the study of this subject, demonstrating its feasibility. In particular, we show that a model trained solely on LLM-generated data can effectively predic
    
[^145]: 球员压力地图—一种新颖的足球压力表征方法，用于评估不同比赛环境下球员表现

    Player Pressure Map -- A Novel Representation of Pressure in Soccer for Evaluating Player Performance in Different Game Contexts

    [https://arxiv.org/abs/2401.16235](https://arxiv.org/abs/2401.16235)

    本文提出了一种球员压力地图来捕捉足球比赛中球控球队所经历的压力，为教练和分析人员提供了更深入的理解。

    

    在足球中，上下文的球员表现度量对教练们非常宝贵。例如，能够在比赛中承受压力的能力将精英和普通球员区分开来。正确的压力度量可以使球队准确评估球员在压力下的表现，并设计针对性的训练场景来弥补他们的弱点。本文的主要目标是利用追踪和事件数据以及比赛录像来捕捉足球比赛场景中球控球队所经历的压力。我们提出了一种球员压力地图来表示给定的比赛场景，它降低了原始数据的维度，同时仍包含丰富的上下文信息。它不仅作为一种有效的工具用于可视化和评估球队和每个个体面临的压力，还可以作为访问球员表现的基础。总的来说，我们的模型为教练和分析人员提供了更深入的理解。

    arXiv:2401.16235v2 Announce Type: replace  Abstract: In soccer, contextual player performance metrics are invaluable to coaches. For example, the ability to perform under pressure during matches distinguishes the elite from the average. Appropriate pressure metric enables teams to assess players' performance accurately under pressure and design targeted training scenarios to address their weaknesses. The primary objective of this paper is to leverage both tracking and event data and game footage to capture the pressure experienced by the possession team in a soccer game scene. We propose a player pressure map to represent a given game scene, which lowers the dimension of raw data and still contains rich contextual information. Not only does it serve as an effective tool for visualizing and evaluating the pressure on the team and each individual, but it can also be utilized as a backbone for accessing players' performance. Overall, our model provides coaches and analysts with a deeper u
    
[^146]: Transformer多元预测：少即是多？

    Transformer Multivariate Forecasting: Less is More?

    [https://arxiv.org/abs/2401.00230](https://arxiv.org/abs/2401.00230)

    本文致力于通过引入主成分分析（PCA）来优化Transformer预测框架，以减少冗余信息并提高预测准确性和运行效率。

    

    在多元预测领域，Transformer模型作为强大的工具脱颖而出，展现出在处理来自真实场景中杂乱数据集方面的异常能力。然而，这些数据集的固有复杂性，以众多变量和漫长时间序列为特征，带来挑战，包括增加的噪音和延长的模型运行时间。本文旨在通过减少冗余信息来提高预测精度，同时优化运行时间效率。我们提出了一种新颖的Transformer预测框架，通过主成分分析（PCA）来解决这一挑战。该框架通过五种最先进的模型和四个不同的真实数据集进行评估。我们的实验结果表明，该框架能够在所有模型和数据集上减少预测误差的能力，同时显著降低运行时间。

    arXiv:2401.00230v2 Announce Type: replace-cross  Abstract: In the domain of multivariate forecasting, transformer models stand out as powerful apparatus, displaying exceptional capabilities in handling messy datasets from real-world contexts. However, the inherent complexity of these datasets, characterized by numerous variables and lengthy temporal sequences, poses challenges, including increased noise and extended model runtime. This paper focuses on reducing redundant information to elevate forecasting accuracy while optimizing runtime efficiency. We propose a novel transformer forecasting framework enhanced by Principal Component Analysis (PCA) to tackle this challenge. The framework is evaluated by five state-of-the-art (SOTA) models and four diverse real-world datasets. Our experimental results demonstrate the framework's ability to minimize prediction errors across all models and datasets while significantly reducing runtime. From the model perspective, one of the PCA-enhanced m
    
[^147]: 多智能体强化学习用于评估交通网络中的虚假数据注入攻击

    Multi-Agent Reinforcement Learning for Assessing False-Data Injection Attacks on Transportation Networks

    [https://arxiv.org/abs/2312.14625](https://arxiv.org/abs/2312.14625)

    引入了一个计算框架，利用多智能体强化学习来找到针对交通网络的最坏情况数据注入攻击，以评估虚假数据注入攻击对交通网络的威胁。

    

    驾驶员对导航应用程序的日益依赖使得交通网络更容易受到恶意行为者的数据篡改攻击的影响。对手可能利用导航服务的数据收集或处理方面的漏洞来注入虚假信息，从而干扰驾驶员的路径选择。这种攻击可能显著增加交通拥堵，导致时间和资源的大量浪费，甚至可能破坏依赖道路网络的基本服务。为了评估此类攻击带来的威胁，我们引入了一个计算框架来发现针对交通网络的最坏情况数据注入攻击。首先，我们设计了一个拥有威胁行为者的对抗模型，该行为者可以通过增加驾驶员在某些道路上感知的行驶时间来操纵他们。然后，我们采用分层多智能体强化学习来找到一种近似最佳的对抗策略。

    arXiv:2312.14625v2 Announce Type: replace  Abstract: The increasing reliance of drivers on navigation applications has made transportation networks more susceptible to data-manipulation attacks by malicious actors. Adversaries may exploit vulnerabilities in the data collection or processing of navigation services to inject false information, and to thus interfere with the drivers' route selection. Such attacks can significantly increase traffic congestions, resulting in substantial waste of time and resources, and may even disrupt essential services that rely on road networks. To assess the threat posed by such attacks, we introduce a computational framework to find worst-case data-injection attacks against transportation networks. First, we devise an adversarial model with a threat actor who can manipulate drivers by increasing the travel times that they perceive on certain roads. Then, we employ hierarchical multi-agent reinforcement learning to find an approximate optimal adversaria
    
[^148]: 为学习可迁移表示提出价值显性预训练

    Value Explicit Pretraining for Learning Transferable Representations

    [https://arxiv.org/abs/2312.12339](https://arxiv.org/abs/2312.12339)

    提出了价值显性预训练（VEP）方法，通过学习编码器来实现学习可迁移的表示，使得能够在新任务中表现优异，对不同任务之间的状态进行关联，实现在Atari和视觉导航中获得多达2倍奖励改善。

    

    我们提出一种名为价值显性预训练（Value Explicit Pretraining，VEP）的方法，用于学习可迁移的表示，以进行强化学习的迁移。VEP通过学习为与先前学习任务共享类似目标的新任务学习编码器来实现，无论外观变化和环境动态如何，都能学习到目标条件表示。为了从一系列观察中预训练编码器，我们使用了一种自监督对比损失，导致学习到时间上平滑的表示。VEP学习将基于反映任务进展的贝尔曼回报估计来关联不同任务之间的状态。在使用真实导航模拟器和Atari基准进行实验后，结果显示我们方法产生的预训练编码器在泛化到未见任务的能力上优于当前最先进的预训练方法。VEP在Atari和视觉导航上的奖励上获得了多达2倍的改善。

    arXiv:2312.12339v2 Announce Type: replace  Abstract: We propose Value Explicit Pretraining (VEP), a method that learns generalizable representations for transfer reinforcement learning. VEP enables learning of new tasks that share similar objectives as previously learned tasks, by learning an encoder for objective-conditioned representations, irrespective of appearance changes and environment dynamics. To pre-train the encoder from a sequence of observations, we use a self-supervised contrastive loss that results in learning temporally smooth representations. VEP learns to relate states across different tasks based on the Bellman return estimate that is reflective of task progress. Experiments using a realistic navigation simulator and Atari benchmark show that the pretrained encoder produced by our method outperforms current SoTA pretraining methods on the ability to generalize to unseen tasks. VEP achieves up to a 2 times improvement in rewards on Atari and visual navigation, and up 
    
[^149]: 利用固有噪声保护量子机器学习中的隐私

    Harnessing Inherent Noises for Privacy Preservation in Quantum Machine Learning

    [https://arxiv.org/abs/2312.11126](https://arxiv.org/abs/2312.11126)

    本文提出利用固有的量子噪声来保护量子机器学习中的数据隐私，对量子电路参数的梯度进行数学分析，并推导其方差的上下界。

    

    量子计算彻底改变了解决复杂问题和处理海量数据的方式，显示出加速机器学习过程的巨大潜力。然而，在量子机器学习（QML）中的数据泄露可能带来隐私风险。尽管差分隐私（DP）是一种通过注入人工噪声保护隐私的成熟方法，但其在QML领域的应用仍未得到充分探讨。本文提出利用固有的量子噪声保护QML中的数据隐私。尤其是考虑到存在噪声的中等规模量子（NISQ）设备，我们利用量子计算中不可避免的射线噪声和非相干噪声来保护二元分类的QML模型的隐私。我们在数学上分析了QML中量子电路参数的梯度满足高斯分布，并推导了其方差的上下界，

    arXiv:2312.11126v2 Announce Type: replace-cross  Abstract: Quantum computing revolutionizes the way of solving complex problems and handling vast datasets, which shows great potential to accelerate the machine learning process. However, data leakage in quantum machine learning (QML) may present privacy risks. Although differential privacy (DP), which protects privacy through the injection of artificial noise, is a well-established approach, its application in the QML domain remains under-explored. In this paper, we propose to harness inherent quantum noises to protect data privacy in QML. Especially, considering the Noisy Intermediate-Scale Quantum (NISQ) devices, we leverage the unavoidable shot noise and incoherent noise in quantum computing to preserve the privacy of QML models for binary classification. We mathematically analyze that the gradient of quantum circuit parameters in QML satisfies a Gaussian distribution, and derive the upper and lower bounds on its variance, which can 
    
[^150]: DTP-Net：通过多尺度特征重用学习在时频域中重建脑电图信号

    DTP-Net: Learning to Reconstruct EEG signals in Time-Frequency Domain by Multi-scale Feature Reuse

    [https://arxiv.org/abs/2312.09417](https://arxiv.org/abs/2312.09417)

    该论文提出了一种全卷积神经网络架构DTP-Net，通过多尺度特征重用学习在时频域中重建脑电图信号并去除噪声。

    

    脑电图（EEG）信号很容易受到各种伪迹的影响，使得伪迹去除对于改善信号质量在疾病诊断和脑机接口（BCI）等场景中至关重要。本文提出了一种全卷积神经架构，称为DTP-Net，它由一个稠密连接的时间金字塔（DTP）和一对可学习的时频变换组成，用于端到端脑电图（EEG）去噪。所提方法首先通过编码器层将任意长度的单通道EEG信号转换成时频域。然后，DTP以多尺度的方式提取和减少噪声，如眼部和肌肉伪迹。最后，使用一个解码器层来重建去噪后的EEG信号。此外，我们对DTP-Net中每个模块的表示学习行为进行了深入分析。

    arXiv:2312.09417v2 Announce Type: replace-cross  Abstract: Electroencephalography (EEG) signals are easily corrupted by various artifacts, making artifact removal crucial for improving signal quality in scenarios such as disease diagnosis and brain-computer interface (BCI). In this paper, we present a fully convolutional neural architecture, called DTP-Net, which consists of a Densely Connected Temporal Pyramid (DTP) sandwiched between a pair of learnable time-frequency transformations for end-to-end electroencephalogram (EEG) denoising. The proposed method first transforms a single-channel EEG signal of arbitrary length into the time-frequency domain via an Encoder layer. Then, noises, such as ocular and muscle artifacts, are extracted by DTP in a multi-scale fashion and reduced. Finally, a Decoder layer is employed to reconstruct the artifact-reduced EEG signal. Additionally, we conduct an in-depth analysis of the representation learning behavior of each module in DTP-Net to substant
    
[^151]: 通过对比激活加法指导Llama 2

    Steering Llama 2 via Contrastive Activation Addition

    [https://arxiv.org/abs/2312.06681](https://arxiv.org/abs/2312.06681)

    引入Contrastive Activation Addition（CAA）方法，通过修改语言模型的激活来精确控制目标行为的程度，显著改变模型行为并在微调和系统提示设计的基础上提供额外有效性。

    

    我们引入了一种创新的方法Contrastive Activation Addition（CAA），用于通过在前向传递过程中修改其激活来指导语言模型。CAA通过对某种行为的正面和负面示例之间残差流激活的差异求平均，计算出“指导向量”。在推断过程中，在用户提示后的所有token位置上以正负系数添加这些指导向量，从而精确控制目标行为的程度。我们通过使用多项选择行为问题数据集和开放式生成任务在Llama 2 Chat上评估了CAA的有效性。我们证明CAA显着改变了模型行为，不仅在传统方法如微调和系统提示设计的基础上有效，而且最小程度地降低了功能。此外，我们对模型的行为做出了更深入的洞察。

    arXiv:2312.06681v3 Announce Type: replace-cross  Abstract: We introduce Contrastive Activation Addition (CAA), an innovative method for steering language models by modifying their activations during forward passes. CAA computes "steering vectors" by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user's prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA's effectiveness on Llama 2 Chat using multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, is effective over and on top of traditional methods like finetuning and system prompt design, and minimally reduces capabilities. Moreover, we gain deeper insights in
    
[^152]: 用原型半监督学习和基础模型实现高效的超出分布检测

    Efficient Out-of-Distribution Detection with Prototypical Semi-Supervised Learning and Foundation Models

    [https://arxiv.org/abs/2311.17093](https://arxiv.org/abs/2311.17093)

    本文介绍了一种新的改进的半监督学习方法，利用冻结的基础模型作为神经网络骨干，在半监督学习和超出分布检测方面取得了优越的表现，并引入了新的预训练技术、损失函数和原型选择方法。

    

    本文介绍了PAWS-VMK，一种改进的原型半监督学习方法，专门设计用于利用冻结的基础模型作为神经网络骨干，该方法在计算机视觉领域中优于以往的半监督学习和超出分布（OOD）检测结果，改进了Predicting View-Assignments With Support Samples（PAWS）半监督学习方法。我们引入了(1) 参数化von-Mises Fisher随机邻域嵌入（vMF-SNE）来预训练投影头，使用基础模型的高质量嵌入;(2) 受MixMatch启发的损失，通过对多视图的预测进行平均，提供比PAWS中使用的一致性损失更可靠的监督信号;和(3) 简单k-Means原型选择（SKMPS），一种比其他无监督标签选择方法提供更优越性能的技术。

    arXiv:2311.17093v2 Announce Type: replace-cross  Abstract: This paper describes PAWS-VMK, an improved approach to prototypical semi-supervised learning in the field of computer vision, specifically designed to utilize a frozen foundation model as the neural network backbone. This method outperforms previous results in semi-supervised learning and out-of-distribution (OOD) detection, improving upon the Predicting View-Assignments With Support Samples (PAWS) semi-supervised learning method. We introduce (1) parametric von-Mises Fisher Stochastic Neighbour Embedding (vMF-SNE) to pretrain the projection head using the high-quality embeddings of the foundation model; (2) a MixMatch inspired loss, where predictions across multiple views are averaged to provide a more reliable supervision signal compared to the consistency loss used in PAWS and (3) simple $k$-Means prototype selection (SKMPS), a technique that provides superior performance to other unsupervised label selection approaches in t
    
[^153]: 通过多样化合成和扩散模型减轻偏见

    Mitigating Biases with Diverse Ensembles and Diffusion Models

    [https://arxiv.org/abs/2311.16176](https://arxiv.org/abs/2311.16176)

    通过利用扩散概率模型（DPMs）生成新特征组合的图像，可以在集成模型中增加模型多样性，并减轻捷径偏见，而无需额外监督信号。

    

    数据中的虚假相关性，即多个线索可以预测目标标签，常常导致一种称为捷径偏见的现象，即模型依赖于错误的、易学的线索，而忽略可靠的线索。在这项工作中，我们提出了一种利用扩散概率模型（DPMs）的集成多样化框架，用于减轻捷径偏见。我们展示了在特定的训练间隔中，DPMs可以生成具有新特征组合的图像，即使在显示相关输入特征的样本上进行训练。我们利用这一关键属性通过集成不一致性生成合成反事实来增加模型的多样性。我们展示了DPM引导的多样化足以消除对主要捷径线索的依赖，无需额外的监督信号。我们进一步在几个多样化目标上在实证上量化其有效性，并最终展示了改进的泛化性能。

    arXiv:2311.16176v2 Announce Type: replace-cross  Abstract: Spurious correlations in the data, where multiple cues are predictive of the target labels, often lead to a phenomenon known as shortcut bias, where a model relies on erroneous, easy-to-learn cues while ignoring reliable ones. In this work, we propose an ensemble diversification framework exploiting Diffusion Probabilistic Models (DPMs) for shortcut bias mitigation. We show that at particular training intervals, DPMs can generate images with novel feature combinations, even when trained on samples displaying correlated input features. We leverage this crucial property to generate synthetic counterfactuals to increase model diversity via ensemble disagreement. We show that DPM-guided diversification is sufficient to remove dependence on primary shortcut cues, without a need for additional supervised signals. We further empirically quantify its efficacy on several diversification objectives, and finally show improved generalizati
    
[^154]: 选取完全随机的互补标签是多类别分类的实用弱监督方法

    The Selected-completely-at-random Complementary Label is a Practical Weak Supervision for Multi-class Classification

    [https://arxiv.org/abs/2311.15502](https://arxiv.org/abs/2311.15502)

    提出了一种不依赖于均匀分布假设的互补标签学习方法，基于完全随机选择假设的无偏风险估计器，以及风险校正方法来解决过拟合问题。

    

    互补标签学习是一个弱监督学习问题，每个训练样本关联着一个或多个互补标签，指示其不属于的类别。现有的一致方法依赖于均匀分布假设来模拟互补标签的生成，或者依赖于一个普通标签的训练集来估计非均匀情况下的转移矩阵。然而，实际情况下这两个条件可能不会被满足。在本文中，我们提出了一种新颖的一致方法，不依赖于这些条件。受到PU学习文献的启发，我们提出了基于完全随机选择假设的无偏风险估计器，用于互补标签学习。然后，我们引入了一种风险校正方法来解决过拟合问题。此外，我们发现互补标签学习可以被表达为一组...

    arXiv:2311.15502v2 Announce Type: replace  Abstract: Complementary-label learning is a weakly supervised learning problem in which each training example is associated with one or multiple complementary labels indicating the classes to which it does not belong. Existing consistent approaches have relied on the uniform distribution assumption to model the generation of complementary labels, or on an ordinary-label training set to estimate the transition matrix in non-uniform cases. However, either condition may not be satisfied in real-world scenarios. In this paper, we propose a novel consistent approach that does not rely on these conditions. Inspired by the positive-unlabeled (PU) learning literature, we propose an unbiased risk estimator based on the Selected Completely At Random assumption for complementary-label learning. We then introduce a risk-correction approach to address overfitting problems. Furthermore, we find that complementary-label learning can be expressed as a set of 
    
[^155]: InteRACT：基于机器人动作的人类意图预测的Transformer模型

    InteRACT: Transformer Models for Human Intent Prediction Conditioned on Robot Actions

    [https://arxiv.org/abs/2311.12943](https://arxiv.org/abs/2311.12943)

    InteRACT通过在大型人类-人类数据集上预训练条件意图预测模型，并在小型人机数据集上微调，解决了人机交互中的先有鸡还是先有蛋问题。

    

    在协作的人机操纵中，机器人必须预测人类意图并相应调整其行动，以平稳执行任务。然而，人类的意图反过来又取决于机器人采取的动作，造成了一个先有鸡还是先有蛋的问题。先前的方法忽略了这种相互依赖关系，而是训练独立于机器人行动的边际意图预测模型。这是因为在缺乏配对的人机交互数据集的情况下，训练条件模型是困难的。我们能否转而利用更容易获取的大规模人类-人类交互数据？我们的关键见解是利用人类和机器人行动之间的对应关系，实现从人类-人类到人类-机器人数据的迁移学习。我们提出了一种新颖的架构InteRACT，该架构在大型人类-人类数据集上预训练条件意图预测模型，并在小型人机数据集上进行微调。我们在一组真实世界的协作数据上进行评估。

    arXiv:2311.12943v2 Announce Type: replace-cross  Abstract: In collaborative human-robot manipulation, a robot must predict human intents and adapt its actions accordingly to smoothly execute tasks. However, the human's intent in turn depends on actions the robot takes, creating a chicken-or-egg problem. Prior methods ignore such inter-dependency and instead train marginal intent prediction models independent of robot actions. This is because training conditional models is hard given a lack of paired human-robot interaction datasets. Can we instead leverage large-scale human-human interaction data that is more easily accessible? Our key insight is to exploit a correspondence between human and robot actions that enables transfer learning from human-human to human-robot data. We propose a novel architecture, InteRACT, that pre-trains a conditional intent prediction model on large human-human datasets and fine-tunes on a small human-robot dataset. We evaluate on a set of real-world collabo
    
[^156]: 关于结合拆分置信预测和贝叶斯深度学习的分布外覆盖率

    On the Out-of-Distribution Coverage of Combining Split Conformal Prediction and Bayesian Deep Learning

    [https://arxiv.org/abs/2311.12688](https://arxiv.org/abs/2311.12688)

    结合拆分置信预测和贝叶斯深度学习在多类图像分类中的分布外覆盖率基于模型在校准集上的信心水平，可能会影响模型对分布外数据的处理。

    

    贝叶斯深度学习和置信预测是两种用于传达不确定性并增加机器学习系统安全性的方法。我们关注将贝叶斯深度学习与拆分置信预测相结合对分布外覆盖率的影响；特别是在多类图像分类情况下。我们建议，如果模型在校准集上通常缺乏信心，那么得到的置信集可能与简单预测可信集相比，表现出更糟糕的分布外覆盖。相反，如果模型在校准集上过于自信，使用置信预测可能会改善分布外覆盖。我们评估将拆分置信方法与随机梯度下降、深度集合和均场变分推理训练的神经网络相结合的预测集的结果。我们的结果表明，结合B

    arXiv:2311.12688v2 Announce Type: replace  Abstract: Bayesian deep learning and conformal prediction are two methods that have been used to convey uncertainty and increase safety in machine learning systems. We focus on combining Bayesian deep learning with split conformal prediction and how this combination effects out-of-distribution coverage; particularly in the case of multiclass image classification. We suggest that if the model is generally underconfident on the calibration set, then the resultant conformal sets may exhibit worse out-of-distribution coverage compared to simple predictive credible sets. Conversely, if the model is overconfident on the calibration set, the use of conformal prediction may improve out-of-distribution coverage. We evaluate prediction sets as a result of combining split conformal methods and neural networks trained with (i) stochastic gradient descent, (ii) deep ensembles, and (iii) mean-field variational inference. Our results suggest that combining B
    
[^157]: SkelVIT：轻量级基于骨骼的动作识别系统的视觉变压器共识

    SkelVIT: Consensus of Vision Transformers for a Lightweight Skeleton-Based Action Recognition System

    [https://arxiv.org/abs/2311.08094](https://arxiv.org/abs/2311.08094)

    本研究考察了VIT对基于骨骼的动作识别的有效性，并提出了一种三级架构SkelVit，该架构能够形成一组伪图像，并在每个表示上应用分类器，然后结合它们的结果来找到动作识别系统中的最佳表现。

    

    骨骼基础动作识别引起了许多研究人员的关注，因为它对视角和光照变化具有鲁棒性，并且其处理比视频帧的处理效率高得多。随着深度学习模型的出现，以伪图像形式表示骨架数据并应用CNN进行动作识别变得非常流行。因此，研究集中在寻找形成伪图像的有效方法上。最近，注意力网络，更具体地说是变压器，在各种视觉问题中提供了有希望的结果。本研究考察了VIT对基于骨架的动作识别的有效性，并研究了其在伪图像表示方案上的稳健性。为此，提出了一个三级架构，SkelVit，它形成一组伪图像，对每个表示应用分类器，并结合它们的结果以找到f

    arXiv:2311.08094v2 Announce Type: replace-cross  Abstract: Skeleton-based action recognition receives the attention of many researchers as it is robust to viewpoint and illumination changes, and its processing is much more efficient than the processing of video frames. With the emergence of deep learning models, it has become very popular to represent the skeleton data in pseudo-image form and apply CNN for action recognition. Thereafter, studies concentrated on finding effective methods for forming pseudo-images. Recently, attention networks, more specifically transformers have provided promising results in various vision problems. In this study, the effectiveness of VIT for skeleton-based action recognition is examined and its robustness on the pseudo-image representation scheme is investigated. To this end, a three-level architecture, SkelVit is proposed, which forms a set of pseudo images, applies a classifier on each of the representations, and combines their results to find the f
    
[^158]: LLM能遵守简单规则吗?

    Can LLMs Follow Simple Rules?

    [https://arxiv.org/abs/2311.04235](https://arxiv.org/abs/2311.04235)

    提出了一个名为RuLES的程序框架，用于衡量LLMs在与用户交互时遵守规则的能力。

    

    随着大型语言模型（LLMs）在现实世界中承担越来越多的责任，能够以可靠的方式指定和约束这些系统的行为变得至关重要。我们提出了规则遵循语言评估场景（RuLES），这是一个测量LLMs遵循规则能力的程序框架，包括14个简单的文本场景，模型在与用户交互时被指示遵守各种规则。

    arXiv:2311.04235v2 Announce Type: replace  Abstract: As Large Language Models (LLMs) are deployed with increasing real-world responsibilities, it is important to be able to specify and constrain the behavior of these systems in a reliable manner. Model developers may wish to set explicit rules for the model, such as "do not generate abusive content", but these may be circumvented by jailbreaking techniques. Existing evaluations of adversarial attacks and defenses on LLMs generally require either expensive manual review or unreliable heuristic checks. To address this issue, we propose Rule-following Language Evaluation Scenarios (RuLES), a programmatic framework for measuring rule-following ability in LLMs. RuLES consists of 14 simple text scenarios in which the model is instructed to obey various rules while interacting with the user. Each scenario has a programmatic evaluation function to determine whether the model has broken any rules in a conversation. Our evaluations of proprietar
    
[^159]: AI风险管理: 用于系统管理AI风险根本原因的框架

    AI Hazard Management: A framework for the systematic management of root causes for AI risks

    [https://arxiv.org/abs/2310.16727](https://arxiv.org/abs/2310.16727)

    介绍了AI危险管理框架（AIHM），旨在系统地识别、评估和处理AI危险，并在系统开发过程中进行，以确保尽早发现任何AI危险。

    

    人工智能（AI）领域的最新进展奠定了解决挑战性任务的基础。然而，随着AI的整合，新的风险也出现。因此，为了从其优势中受益，必须充分处理与AI相关的风险。现有领域中的风险管理流程，如软件系统，需要充分考虑AI的特定问题。一个关键挑战是系统地和透明地识别和解决AI风险的根本原因，也称为AI危险。本文介绍了AI危险管理（AIHM）框架，提供了一个结构化过程来系统地识别、评估和处理AI危险。提出的过程与开发同时进行，以确保任何AI危险在AI系统生命周期的最早可能阶段被捕获。此外，为了确保AI系统的可审计性，提出的框架系统化。

    arXiv:2310.16727v2 Announce Type: replace  Abstract: Recent advancements in the field of Artificial Intelligence (AI) establish the basis to address challenging tasks. However, with the integration of AI, new risks arise. Therefore, to benefit from its advantages, it is essential to adequately handle the risks associated with AI. Existing risk management processes in related fields, such as software systems, need to sufficiently consider the specifics of AI. A key challenge is to systematically and transparently identify and address AI risks' root causes - also called AI hazards. This paper introduces the AI Hazard Management (AIHM) framework, which provides a structured process to systematically identify, assess, and treat AI hazards. The proposed process is conducted in parallel with the development to ensure that any AI hazard is captured at the earliest possible stage of the AI system's life cycle. In addition, to ensure the AI system's auditability, the proposed framework systemat
    
[^160]: 基于图分解学习的多因素时空预测

    Multi-Factor Spatio-Temporal Prediction based on Graph Decomposition Learning

    [https://arxiv.org/abs/2310.10374](https://arxiv.org/abs/2310.10374)

    提出了一个多因素时空预测任务，通过预测不同因素下的部分时空数据演变，并将它们结合起来进行最终预测，提出了有效的理论解决方案和可移植的实例化框架。

    

    arXiv:2310.10374v2 公告类型: 替换 摘要: 时空（ST）预测是数据挖掘和分析中的一项重要且广泛应用的技术，特别适用于城市系统中的ST数据，如交通数据。在实践中，ST数据生成通常受到与自然现象或人类社会经济活动相关的各种潜在因素的影响，这些因素会选择性地影响特定的空间区域。然而，现有的ST预测方法通常不会细化不同因素的影响，而是直接模型化多个因素的交织影响。这增加了ST数据建模的复杂性，并影响模型的可解释性。因此，我们提出了一个多因素ST预测任务，该任务在不同因素下预测部分ST数据演变，并将它们结合起来进行最终预测。我们在这项任务中做出了两个贡献: 一个有效的理论解决方案和一个可移植的实例化框架。具体来说，我们首先提出了一个理论解决方.

    arXiv:2310.10374v2 Announce Type: replace  Abstract: Spatio-temporal (ST) prediction is an important and widely used technique in data mining and analytics, especially for ST data in urban systems such as transportation data. In practice, the ST data generation is usually influenced by various latent factors tied to natural phenomena or human socioeconomic activities, impacting specific spatial areas selectively. However, existing ST prediction methods usually do not refine the impacts of different factors, but directly model the entangled impacts of multiple factors. This amplifies the modeling complexity of ST data and compromises model interpretability. To this end, we propose a multi-factor ST prediction task that predicts partial ST data evolution under different factors, and combines them for a final prediction. We make two contributions to this task: an effective theoretical solution and a portable instantiation framework. Specifically, we first propose a theoretical solution ca
    
[^161]: VeCLIP：通过富含视觉信息的标题改进CLIP训练

    VeCLIP: Improving CLIP Training via Visual-enriched Captions

    [https://arxiv.org/abs/2310.07699](https://arxiv.org/abs/2310.07699)

    本研究提出了一种通过将视觉概念融入标题中的方式来改进CLIP训练的方法，名为VeCLIP，该方法在大规模网络爬取数据集上展示了良好的性能。

    

    大规模网络爬取数据集对于预训练视觉-语言模型（如CLIP）的成功至关重要。然而，网络爬取的AltTexts存在固有的噪音和潜在的不相关性，造成了精确的图像-文字对齐方面的挑战。本研究引入了一种适用于嘈杂标题重写的可扩展流程。与利用大型语言模型（LLMs）进行标题重写的现有方法在小型策划数据集（如CC3M和CC12M）上已经显示出了希望。我们强调将视觉概念融入标题中，称为富含视觉信息的标题（VeCap），以确保数据多样性。为了优化AltTexts与新生成的VeCap的利用，我们提出了一种新颖的混合训练方案。我们展示了该方法在大规模网络爬取数据集上训练CLIP的适应性，称为VeCLIP。通过使用这种经济有效的流程，我们轻松扩展了我们的实验。

    arXiv:2310.07699v2 Announce Type: replace-cross  Abstract: Large-scale web-crawled datasets are fundamental for the success of pre-training vision-language models, such as CLIP. However, the inherent noise and potential irrelevance of web-crawled AltTexts pose challenges in achieving precise image-text alignment. Existing methods utilizing large language models (LLMs) for caption rewriting have shown promise on small, curated datasets like CC3M and CC12M. This study introduces a scalable pipeline for noisy caption rewriting. Unlike recent LLM rewriting techniques, we emphasize the incorporation of visual concepts into captions, termed as Visual-enriched Captions (VeCap). To ensure data diversity, we propose a novel mixed training scheme that optimizes the utilization of AltTexts alongside newly generated VeCap. We showcase the adaptation of this method for training CLIP on large-scale web-crawled datasets, termed VeCLIP. Employing this cost-effective pipeline, we effortlessly scale our
    
[^162]: 语言模型写作是否会降低内容多样性？

    Does Writing with Language Models Reduce Content Diversity?

    [https://arxiv.org/abs/2309.05196](https://arxiv.org/abs/2309.05196)

    写作时使用InstructGPT（而不是GPT3）会显著降低内容多样性，增加不同作者之间的相似性，并减少整体的词汇和内容多样性。

    

    大型语言模型（LLMs）引发了与模型辅助合作写作的激增。当不同用户纳入同一模型的建议时，会存在内容多样性减少的风险，可能限制公共话语中的多元观点。本研究通过控制实验测量了协同写作对多样性的影响，在该实验中，用户以三种设置撰写议论性文章--使用基本LLM（GPT3）、经过反馈调整的LLM（InstructGPT）以及不使用模型帮助写作。我们开发了一组多样性指标，并发现使用InstructGPT进行写作（而不是GPT3）会导致多样性明显降低。具体而言，它增加了不同作者的写作之间的相似性，减少了整体的词汇和内容多样性。此外，我们还发现这种影响主要来源于InstructGPT对共同撰写的文本贡献较少。

    arXiv:2309.05196v2 Announce Type: replace  Abstract: Large language models (LLMs) have led to a surge in collaborative writing with model assistance. As different users incorporate suggestions from the same model, there is a risk of decreased diversity in the produced content, potentially limiting diverse perspectives in public discourse. In this work, we measure the impact of co-writing on diversity via a controlled experiment, where users write argumentative essays in three setups -- using a base LLM (GPT3), a feedback-tuned LLM (InstructGPT), and writing without model help. We develop a set of diversity metrics and find that writing with InstructGPT (but not the GPT3) results in a statistically significant reduction in diversity. Specifically, it increases the similarity between the writings of different authors and reduces the overall lexical and content diversity. We additionally find that this effect is mainly attributable to InstructGPT contributing less diverse text to co-writt
    
[^163]: DWA：图像超分辨率的差分小波放大器

    DWA: Differential Wavelet Amplifier for Image Super-Resolution

    [https://arxiv.org/abs/2307.04593](https://arxiv.org/abs/2307.04593)

    差分小波放大器（DWA）为基于小波的图像超分辨率引入了新的模块，通过利用两个卷积滤波器的差异改进了特征提取，强调了局部对比度并抑制了常见噪声，从而在经典超分辨率任务中取得了明显的改进。

    

    这项工作介绍了差分小波放大器（Differential Wavelet Amplifier，DWA），这是一个用于基于小波的图像超分辨率（SR）的插入模块。DWA激发了一种近来受到较少关注的方法，即离散小波变换（DWT）。DWT实现了一种高效的图像表示方式，用于SR，并将其输入的空间区域减小了4倍，模型大小和计算成本，将其作为可持续ML的一种具有吸引力的方法。我们提出的DWA模型通过利用两个卷积滤波器之间的差异来改进基于小波的SR模型，在小波域中精炼相关特征提取，强调局部对比度并抑制输入信号中的常见噪声。我们通过将其整合到现有的SR模型中（如DWSR和MWCNN），展示了其有效性，并在经典SR任务中展示了明显的改进。此外，DWA使得DWSR和MWCNN直接应用于输入图像空间，减少了

    arXiv:2307.04593v1 Announce Type: cross  Abstract: This work introduces Differential Wavelet Amplifier (DWA), a drop-in module for wavelet-based image Super-Resolution (SR). DWA invigorates an approach recently receiving less attention, namely Discrete Wavelet Transformation (DWT). DWT enables an efficient image representation for SR and reduces the spatial area of its input by a factor of 4, the overall model size, and computation cost, framing it as an attractive approach for sustainable ML. Our proposed DWA model improves wavelet-based SR models by leveraging the difference between two convolutional filters to refine relevant feature extraction in the wavelet domain, emphasizing local contrasts and suppressing common noise in the input signals. We show its effectiveness by integrating it into existing SR models, e.g., DWSR and MWCNN, and demonstrate a clear improvement in classical SR tasks. Moreover, DWA enables a direct application of DWSR and MWCNN to input image space, reducing 
    
[^164]: 透过简单性偏见的视角早期识别训练中的虚假偏见

    Identifying Spurious Biases Early in Training through the Lens of Simplicity Bias

    [https://arxiv.org/abs/2305.18761](https://arxiv.org/abs/2305.18761)

    本研究通过理论分析揭示了简单性偏见对学习虚假相关性的影响，提出了早期识别和缓解虚假相关性的方法SPARE，并在实验证明其优于目前最先进的方法。

    

    使用（随机）梯度下降训练的神经网络具有朝向学习更简单解决方案的归纳偏见。这使得它们极易于学习训练数据中的虚假相关性，在测试时可能不成立。在这项工作中，我们首次提供了对简单性偏见对学习虚假相关性的影响的理论分析。值得注意的是，我们展示了早期训练中基于模型输出可以明确分离出具有虚假特征的示例。我们进一步说明，如果虚假特征的噪声信号比足够小，网络在大多数示例上的输出几乎完全由虚假特征决定，导致较差的最坏组测试精度。最后，我们提出了SPARE，它可以在训练早期识别虚假相关性并利用重要性抽样来减轻其影响。实验证明，SPARE的表现优于最先进的方法。

    arXiv:2305.18761v2 Announce Type: replace  Abstract: Neural networks trained with (stochastic) gradient descent have an inductive bias towards learning simpler solutions. This makes them highly prone to learning spurious correlations in the training data, that may not hold at test time. In this work, we provide the first theoretical analysis of the effect of simplicity bias on learning spurious correlations. Notably, we show that examples with spurious features are provably separable based on the model's output early in training. We further illustrate that if spurious features have a small enough noise-to-signal ratio, the network's output on the majority of examples is almost exclusively determined by the spurious features, leading to poor worst-group test accuracy. Finally, we propose SPARE, which identifies spurious correlations early in training and utilizes importance sampling to alleviate their effect. Empirically, we demonstrate that SPARE outperforms state-of-the-art methods by
    
[^165]: 稀疏表示定理用于在再生核Banach空间中学习

    Sparse Representer Theorems for Learning in Reproducing Kernel Banach Spaces

    [https://arxiv.org/abs/2305.12584](https://arxiv.org/abs/2305.12584)

    该论文研究了在再生核Banach空间中促进稀疏学习解的条件，并建立了相应的表示定理和转换机制。

    

    学习解的稀疏性是机器学习中一种理想的特征。某些再生核Banach空间(RKBSs)是稀疏学习方法的适当假设空间。本文旨在了解什么样的RKBS可以促进学习解的稀疏性。我们考虑RKBS中的两种典型学习模型：最小范数插值(MNI)问题和正则化问题。我们首先为这些问题的解建立了一个显式的表示定理，该定理通过极值点的线性组合来表示解集的极值点，即规范函数的次梯度集，这是依赖于数据的。然后我们提出了对RKBS的充分条件，可以将解的显式表示转换为具有比观测数据数量更少项的稀疏核表示。在所提出的充分条件下，我们进行了研究

    arXiv:2305.12584v2 Announce Type: replace-cross  Abstract: Sparsity of a learning solution is a desirable feature in machine learning. Certain reproducing kernel Banach spaces (RKBSs) are appropriate hypothesis spaces for sparse learning methods. The goal of this paper is to understand what kind of RKBSs can promote sparsity for learning solutions. We consider two typical learning models in an RKBS: the minimum norm interpolation (MNI) problem and the regularization problem. We first establish an explicit representer theorem for solutions of these problems, which represents the extreme points of the solution set by a linear combination of the extreme points of the subdifferential set, of the norm function, which is data-dependent. We then propose sufficient conditions on the RKBS that can transform the explicit representation of the solutions to a sparse kernel representation having fewer terms than the number of the observed data. Under the proposed sufficient conditions, we investiga
    
[^166]: 适用于基于模拟推断的误差鲁棒顺序神经似然方法

    Misspecification-robust Sequential Neural Likelihood for Simulation-based Inference

    [https://arxiv.org/abs/2301.13368](https://arxiv.org/abs/2301.13368)

    本文提出了一种新的顺序神经似然方法，通过纳入额外的调整参数，使其对模型错误规定具有鲁棒性，并能够识别数据的特征。

    

    模拟推断技术对于具有难以处理的似然函数的机械和可模拟模型的参数估计至关重要。在传统统计方法(如近似贝叶斯计算和贝叶斯合成似然)的研究中，这些方法通常在规定良好和错误规定的设置下进行研究，但由于浪费模型模拟而导致效率低下。神经网络方法，如顺序神经似然(SNL)，通过利用所有模型模拟来训练神经代理似然函数，避免了这种浪费。然而，SNL在模型错误规定的情况下的性能不可靠，可能导致围绕不准确参数估计的过于自信的后验分布。本文提出了一种新的SNL方法，通过纳入额外的调整参数，使其对模型错误规定具有鲁棒性，并能够识别数据的特征。

    arXiv:2301.13368v2 Announce Type: replace-cross  Abstract: Simulation-based inference techniques are indispensable for parameter estimation of mechanistic and simulable models with intractable likelihoods. While traditional statistical approaches like approximate Bayesian computation and Bayesian synthetic likelihood have been studied under well-specified and misspecified settings, they often suffer from inefficiencies due to wasted model simulations. Neural approaches, such as sequential neural likelihood (SNL) avoid this wastage by utilising all model simulations to train a neural surrogate for the likelihood function. However, the performance of SNL under model misspecification is unreliable and can result in overconfident posteriors centred around an inaccurate parameter estimate. In this paper, we propose a novel SNL method, which through the incorporation of additional adjustment parameters, is robust to model misspecification and capable of identifying features of the data that 
    
[^167]: PDFormer：考虑传播时延的动态远程Transformer用于交通流预测

    PDFormer: Propagation Delay-Aware Dynamic Long-Range Transformer for Traffic Flow Prediction

    [https://arxiv.org/abs/2301.07945](https://arxiv.org/abs/2301.07945)

    PDFormer是一种考虑传播时延的动态远程Transformer，用于解决交通流预测中静态空间依赖、短程信息和时间延迟的挑战。

    

    作为智能交通系统的核心技术，交通流预测具有广泛的应用。交通流预测面临的基本挑战是有效地对交通数据中复杂的时空依赖关系进行建模。时空图神经网络（GNN）模型已成为解决这一问题的最具前景的方法之一。然而，基于GNN的模型在交通预测中存在三个主要限制：i）大多数方法以静态方式建模空间依赖关系，这限制了学习动态城市交通模式的能力；ii）大多数方法仅考虑短程空间信息，无法捕捉长程空间依赖关系；iii）这些方法忽略了交通状况在位置之间的传播存在时间延迟。为此，我们提出了一种新颖的考虑传播时延的动态远程Transformer，即PDFormer。

    arXiv:2301.07945v3 Announce Type: replace  Abstract: As a core technology of Intelligent Transportation System, traffic flow prediction has a wide range of applications. The fundamental challenge in traffic flow prediction is to effectively model the complex spatial-temporal dependencies in traffic data. Spatial-temporal Graph Neural Network (GNN) models have emerged as one of the most promising methods to solve this problem. However, GNN-based models have three major limitations for traffic prediction: i) Most methods model spatial dependencies in a static manner, which limits the ability to learn dynamic urban traffic patterns; ii) Most methods only consider short-range spatial information and are unable to capture long-range spatial dependencies; iii) These methods ignore the fact that the propagation of traffic conditions between locations has a time delay in traffic systems. To this end, we propose a novel Propagation Delay-aware dynamic long-range transFormer, namely PDFormer, fo
    
[^168]: 双层优化的一阶惩罚方法

    First-order penalty methods for bilevel optimization

    [https://arxiv.org/abs/2301.01716](https://arxiv.org/abs/2301.01716)

    本文研究双层优化问题中的一阶惩罚方法，提出了寻找$\varepsilon$-KKT解的方法，并在适当假设下确立了其操作复杂度。

    

    本文研究了一类无约束和有约束的双层优化问题，其中下层是可能是非光滑凸优化问题，而上层是可能是非凸优化问题。我们引入了$\varepsilon$-KKT解的概念，并展示了一个$\varepsilon$-KKT解在适当假设下会导致一个$O(\sqrt{\varepsilon})$或$O(\varepsilon)$-基于超梯度的稳定点。我们还提出了一阶惩罚方法来寻找它们的$\varepsilon$-KKT解，其子问题实际上是一个结构化的极小极大问题，可以通过作者最近开发的一阶方法适当地解决。在适当假设下，通过它们的基本操作衡量，我们建立了所提出的惩罚方法的“操作复杂度”分别为$O(\varepsilon^{-4}\log\varepsilon^{-1})$和$O(\varepsilon^{-7}\log\varepsilon^{-1})$。

    arXiv:2301.01716v2 Announce Type: replace-cross  Abstract: In this paper we study a class of unconstrained and constrained bilevel optimization problems in which the lower level is a possibly nonsmooth convex optimization problem, while the upper level is a possibly nonconvex optimization problem. We introduce a notion of $\varepsilon$-KKT solution for them and show that an $\varepsilon$-KKT solution leads to an $O(\sqrt{\varepsilon})$- or $O(\varepsilon)$-hypergradient based stionary point under suitable assumptions. We also propose first-order penalty methods for finding an $\varepsilon$-KKT solution of them, whose subproblems turn out to be a structured minimax problem and can be suitably solved by a first-order method recently developed by the authors. Under suitable assumptions, an \emph{operation complexity} of $O(\varepsilon^{-4}\log\varepsilon^{-1})$ and $O(\varepsilon^{-7}\log\varepsilon^{-1})$, measured by their fundamental operations, is established for the proposed penalty 
    
[^169]: 具有时间规律和行程语义的自监督轨迹表示学习

    Self-supervised Trajectory Representation Learning with Temporal Regularities and Travel Semantics

    [https://arxiv.org/abs/2211.09510](https://arxiv.org/abs/2211.09510)

    提出了一种新颖的自监督轨迹表示学习框架START，充分利用时间规律和行程语义，通过轨迹模式增强图注意力网络(TPE-GAT)将道路网络特征和行程语义转换为道路段的表示向量。

    

    轨迹表示学习（TRL）是空间 - 时间数据分析和管理的强大工具。TRL的目标是将复杂的原始轨迹转换为低维表示向量，可应用于各种下游任务，如轨迹分类、聚类和相似性计算。现有的TRL作品通常将轨迹视为普通的序列数据，而一些重要的空间 - 时间特征，如时间规律和行程语义，未被充分利用。为了填补这一空白，我们提出了一种新颖的具有时间规律和行程语义的自监督轨迹表示学习框架，即START。该方法由两个阶段组成。第一阶段是轨迹模式增强图注意力网络（TPE-GAT），将道路网络特征和行程语义转换为道路段的表示向量。

    arXiv:2211.09510v4 Announce Type: replace  Abstract: Trajectory Representation Learning (TRL) is a powerful tool for spatial-temporal data analysis and management. TRL aims to convert complicated raw trajectories into low-dimensional representation vectors, which can be applied to various downstream tasks, such as trajectory classification, clustering, and similarity computation. Existing TRL works usually treat trajectories as ordinary sequence data, while some important spatial-temporal characteristics, such as temporal regularities and travel semantics, are not fully exploited. To fill this gap, we propose a novel Self-supervised trajectory representation learning framework with TemporAl Regularities and Travel semantics, namely START. The proposed method consists of two stages. The first stage is a Trajectory Pattern-Enhanced Graph Attention Network (TPE-GAT), which converts the road network features and travel semantics into representation vectors of road segments. The second stag
    
[^170]: 利用节能加速计对攀岩路线进行聚类

    Climbing Routes Clustering Using Energy-Efficient Accelerometers Attached to the Quickdraws

    [https://arxiv.org/abs/2211.02680](https://arxiv.org/abs/2211.02680)

    利用节能加速计对攀岩路线进行聚类，为攀岩健身房提供了改善服务和最大程度利用基础设施的新方法

    

    攀岩健身房面临的挑战之一是找出受欢迎的攀登路线，以改善他们的服务并最大程度地利用他们的基础设施。为了解决这个问题，开发了一种硬件原型来收集数据，该原型使用附加到墙壁上的攀岩设备的加速计传感器，称为快挂，将攀岩绳连接到螺栓锚点。相应的传感器被配置为节能，因此在攀岩健身房大量使用时，在费用和更换时间上变得实用。本文描述了硬件规格，研究了传感器在超低功耗模式下测量的数据，检测了攀登不同路线过程中数据中的模式，并开发了一种无监督的路线聚类方法。

    arXiv:2211.02680v2 Announce Type: replace-cross  Abstract: One of the challenges for climbing gyms is to find out popular routes for the climbers to improve their services and optimally use their infrastructure. This problem must be addressed preserving both the privacy and convenience of the climbers and the costs of the gyms. To this aim, a hardware prototype is developed to collect data using accelerometer sensors attached to a piece of climbing equipment mounted on the wall, called quickdraw, that connects the climbing rope to the bolt anchors. The corresponding sensors are configured to be energy-efficient, hence becoming practical in terms of expenses and time consumption for replacement when used in large quantities in a climbing gym. This paper describes hardware specifications, studies data measured by the sensors in ultra-low power mode, detect patterns in data during climbing different routes, and develops an unsupervised approach for route clustering.
    
[^171]: FIRE：面向边缘计算迁移的故障自适应强化学习框架

    FIRE: A Failure-Adaptive Reinforcement Learning Framework for Edge Computing Migrations

    [https://arxiv.org/abs/2209.14399](https://arxiv.org/abs/2209.14399)

    提出了一个面向边缘计算迁移的故障自适应强化学习框架 FIRE，引入ImRE算法，通过在边缘计算数字孪生环境中训练RL策略来适应罕见事件，解决了RL框架在处理偶发服务器故障方面的挑战。

    

    在边缘计算中，用户服务配置文件由于用户移动而进行迁移。已经提出了强化学习（RL）框架来进行迁移，通常是在模拟数据上进行训练。然而，现有的RL框架忽视了偶发的服务器故障，尽管罕见，但会影响到像自动驾驶和实时障碍检测等对延迟敏感的应用。因此，这些（罕见事件）故障虽然在历史训练数据中没有得到充分代表，却对基于数据驱动的RL算法构成挑战。由于在实际应用中调整故障频率进行训练是不切实际的，我们引入了FIRE，这是一个通过在边缘计算数字孪生环境中训练RL策略来适应罕见事件的框架。我们提出了ImRE，一种基于重要性抽样的Q-learning算法，它根据罕见事件对值函数的影响进行比例抽样。FIRE考虑了延迟、迁移、故障和备份pl

    arXiv:2209.14399v2 Announce Type: replace-cross  Abstract: In edge computing, users' service profiles are migrated due to user mobility. Reinforcement learning (RL) frameworks have been proposed to do so, often trained on simulated data. However, existing RL frameworks overlook occasional server failures, which although rare, impact latency-sensitive applications like autonomous driving and real-time obstacle detection. Nevertheless, these failures (rare events), being not adequately represented in historical training data, pose a challenge for data-driven RL algorithms. As it is impractical to adjust failure frequency in real-world applications for training, we introduce FIRE, a framework that adapts to rare events by training a RL policy in an edge computing digital twin environment. We propose ImRE, an importance sampling-based Q-learning algorithm, which samples rare events proportionally to their impact on the value function. FIRE considers delay, migration, failure, and backup pl
    
[^172]: 美国极端森林火灾的时空回归建模：部分可解释神经网络

    Regression modelling of spatiotemporal extreme U.S. wildfires via partially-interpretable neural networks

    [https://arxiv.org/abs/2208.07581](https://arxiv.org/abs/2208.07581)

    提出了一种新的方法框架，利用神经网络进行极端分位数回归，以更好地理解和量化美国极端森林火灾的风险

    

    许多环境背景下的风险管理需要理解驱动极端事件的机制。用于量化这种风险的有用指标是响应变量的极端分位数，其条件是描述气候、生物圈和环境状态等预测变量。典型情况下，这些分位数位于可观测数据范围之外，因此，为了进行估计，需要在回归框架内指定参数极值模型。在这篇论文中，我们提出了一种新的方法框架，用于利用人工神经网络进行极端分位数回归。

    arXiv:2208.07581v4 Announce Type: replace-cross  Abstract: Risk management in many environmental settings requires an understanding of the mechanisms that drive extreme events. Useful metrics for quantifying such risk are extreme quantiles of response variables conditioned on predictor variables that describe, e.g., climate, biosphere and environmental states. Typically these quantiles lie outside the range of observable data and so, for estimation, require specification of parametric extreme value models within a regression framework. Classical approaches in this context utilise linear or additive relationships between predictor and response variables and suffer in either their predictive capabilities or computational efficiency; moreover, their simplicity is unlikely to capture the truly complex structures that lead to the creation of extreme wildfires. In this paper, we propose a new methodological framework for performing extreme quantile regression using artificial neutral network
    
[^173]: 手机、音节和单词能否作为跨情境视听学习的副产品而出现？-- 一项计算研究

    Can phones, syllables, and words emerge as side-products of cross-situational audiovisual learning? -- A computational investigation

    [https://arxiv.org/abs/2109.14200](https://arxiv.org/abs/2109.14200)

    研究探讨了语言学习中，通过跨情境视听学习，音素、音节和单词能否作为副产品出现，并支持不同形式表征间的转换。

    

    数十年的研究探讨了语言学习婴儿如何学会区分语音，分割单词，以及将单词与其含义关联起来。尽管这些能力的逐渐发展是毋庸置疑的，但这些技能的确切性质和潜在的心理表征仍然不清楚。与此同时，计算研究表明，通过语音和同时具有指称不明确的视觉输入之间的统计学习，可以实现对基本语音的理解。这些模型可以在没有诸如语言单位的表征，以及没有专门针对这些单位的学习机制的情况下运行。这引发了一个问题，即在多大程度上，类似音素、音节和单词的语言单位的知识实际上能够作为潜在表征出现，支持语音与其他形式表征之间的转换，而不需要专门的学习机制来实现。

    arXiv:2109.14200v2 Announce Type: replace-cross  Abstract: Decades of research has studied how language learning infants learn to discriminate speech sounds, segment words, and associate words with their meanings. While gradual development of such capabilities is unquestionable, the exact nature of these skills and the underlying mental representations yet remains unclear. In parallel, computational studies have shown that basic comprehension of speech can be achieved by statistical learning between speech and concurrent referentially ambiguous visual input. These models can operate without prior linguistic knowledge such as representations of linguistic units, and without learning mechanisms specifically targeted at such units. This has raised the question of to what extent knowledge of linguistic units, such as phone(me)s, syllables, and words, could actually emerge as latent representations supporting the translation between speech and representations in other modalities, and withou
    
[^174]: 论双Lipschitz正规化流的表达能力

    On the expressivity of bi-Lipschitz normalizing flows

    [https://arxiv.org/abs/2107.07232](https://arxiv.org/abs/2107.07232)

    讨论了双Lipschitz正规化流的表达能力，发现了一些难以逼近的目标分布，并通过给出下界刻画了它们之间的距离，最后讨论了使用更复杂的潜在分布等潜在改进方法。

    

    一个可逆函数若其自身和其逆函数均具有有界Lipschitz常数，则称之为双Lipschitz函数。 如今，大多数正规化流通过设计或训练而成为双Lipschitz，以限制数值误差 (等等)。 本文讨论了双Lipschitz正规化流的表达能力，并确定了一些难以用这种模型逼近的目标分布。 其次，我们通过给出若干特别不利分布与其最佳逼近之间的总变分距离的若干下界，刻画了双Lipschitz正规化流的表达能力。 最后，我们讨论了一些潜在的解决方法，其中包括使用更复杂的潜在分布。

    arXiv:2107.07232v3 Announce Type: replace  Abstract: An invertible function is bi-Lipschitz if both the function and its inverse have bounded Lipschitz constants. Nowadays, most Normalizing Flows are bi-Lipschitz by design or by training to limit numerical errors (among other things). In this paper, we discuss the expressivity of bi-Lipschitz Normalizing Flows and identify several target distributions that are difficult to approximate using such models. Then, we characterize the expressivity of bi-Lipschitz Normalizing Flows by giving several lower bounds on the Total Variation distance between these particularly unfavorable distributions and their best possible approximation. Finally, we discuss potential remedies which include using more complex latent distributions.
    
[^175]: 关于带缺失值的监督学习的一致性

    On the consistency of supervised learning with missing values

    [https://arxiv.org/abs/1902.06931](https://arxiv.org/abs/1902.06931)

    两种方法在带缺失值的监督学习中表现出一致性，当缺失值不具信息性时，使用常数进行插补是一种简单且重要的实践方法。

    

    在许多应用设置中，数据存在缺失值，这使得分析变得具有挑战性。丰富的文献涉及缺失值在推断框架中的处理：从不完整的表中估计参数及其方差。在这里，我们考虑监督学习设置：在训练和测试数据中出现缺失值时预测目标。我们表明了两种方法在预测中的一致性。一个引人注目的结果是，当缺失值不具信息性时，使用常数进行插补，例如在学习之前使用均值，是一致的。这与推断设置形成鲜明对比，推断设置中常用的均值插补方法被指责扭曲数据的分布。这样一个简单的方法在实践中能够保持一致性是很重要的。我们还展示了适用于完整观测的预测器可以通过多重插补在不完整数据上进行最佳预测。最后，为了比较插补

    arXiv:1902.06931v4 Announce Type: replace-cross  Abstract: In many application settings, the data have missing entries which make analysis challenging. An abundant literature addresses missing values in an inferential framework: estimating parameters and their variance from incomplete tables. Here, we consider supervised-learning settings: predicting a target when missing values appear in both training and testing data. We show the consistency of two approaches in prediction. A striking result is that the widely-used method of imputing with a constant, such as the mean prior to learning is consistent when missing values are not informative. This contrasts with inferential settings where mean imputation is pointed at for distorting the distribution of the data. That such a simple approach can be consistent is important in practice. We also show that a predictor suited for complete observations can predict optimally on incomplete data,through multiple imputation.Finally, to compare imput
    
[^176]: 我们错过了谁？一种基于原则的揭示少数人群特征的方法

    Who Are We Missing? A Principled Approach to Characterizing the Underrepresented Population. (arXiv:2401.14512v1 [stat.ME])

    [http://arxiv.org/abs/2401.14512](http://arxiv.org/abs/2401.14512)

    本文提出了一种基于优化的方法，Rashomon Set of Optimal Trees (ROOT)，用于识别和描述随机对照试验中的少数人群。该方法通过最小化目标平均处理效应估计的方差来优化目标子群体分布，从而提供更精确和可解释的处理效应估计。与其他方法相比，该方法具有更高的精度和可解释性，通过合成数据实验进行了验证。

    

    随机对照试验在理解因果效应方面起到了关键作用，然而将推论扩展到目标人群时面临效应异质性和代表性不足的挑战。我们的论文解决了在随机对照试验中识别和描述少数人群的关键问题，提出了一种改进目标人群以提升普适性的创新框架。我们引入了一种基于优化的方法——Rashomon Set of Optimal Trees (ROOT)，来描述少数人群。ROOT通过最小化目标平均处理效应估计的方差来优化目标子群体分布，从而确保更精确的处理效应估计。值得注意的是，ROOT生成可解释的少数人群特征，有助于研究人员有效沟通。我们的方法在精度和可解释性方面相对于其他方法展现了改进，通过合成数据实验进行了验证。

    Randomized controlled trials (RCTs) serve as the cornerstone for understanding causal effects, yet extending inferences to target populations presents challenges due to effect heterogeneity and underrepresentation. Our paper addresses the critical issue of identifying and characterizing underrepresented subgroups in RCTs, proposing a novel framework for refining target populations to improve generalizability. We introduce an optimization-based approach, Rashomon Set of Optimal Trees (ROOT), to characterize underrepresented groups. ROOT optimizes the target subpopulation distribution by minimizing the variance of the target average treatment effect estimate, ensuring more precise treatment effect estimations. Notably, ROOT generates interpretable characteristics of the underrepresented population, aiding researchers in effective communication. Our approach demonstrates improved precision and interpretability compared to alternatives, as illustrated with synthetic data experiments. We ap
    
[^177]: 从因果关系的角度看图对比不变学习

    Graph Contrastive Invariant Learning from the Causal Perspective. (arXiv:2401.12564v1 [cs.LG])

    [http://arxiv.org/abs/2401.12564](http://arxiv.org/abs/2401.12564)

    本文从因果关系的角度研究了图对比不变学习，并提出了一种新的GCL方法，通过引入谱图扩增和设计不变性目标和独立性目标来更好地学习不变表示。

    

    图对比学习（GCL）是一种通过对比两个扩增图来自我监督学习节点表示的方法，已引起了相当大的关注。GCL通常被认为是学习不变表示的方法。然而，这种理解在实践中总是正确的吗？本文首先从因果关系的角度研究了GCL。通过使用结构因果模型（SCM）分析GCL，我们发现传统的GCL由于图中包含的非因果信息，可能无法很好地学习不变表示。那么，我们如何修复并促使当前的GCL学习更好的不变表示呢？SCM提供了两个要求并激励我们提出了一种新的GCL方法。特别地，我们引入了谱图扩增来模拟对非因果因素的干预。然后，我们设计了不变性目标和独立性目标，以更好地捕捉因果因素。具体来说，（i）不变性目标鼓励e

    Graph contrastive learning (GCL), learning the node representation by contrasting two augmented graphs in a self-supervised way, has attracted considerable attention. GCL is usually believed to learn the invariant representation. However, does this understanding always hold in practice? In this paper, we first study GCL from the perspective of causality. By analyzing GCL with the structural causal model (SCM), we discover that traditional GCL may not well learn the invariant representations due to the non-causal information contained in the graph. How can we fix it and encourage the current GCL to learn better invariant representations? The SCM offers two requirements and motives us to propose a novel GCL method. Particularly, we introduce the spectral graph augmentation to simulate the intervention upon non-causal factors. Then we design the invariance objective and independence objective to better capture the causal factors. Specifically, (i) the invariance objective encourages the e
    
[^178]: AutoFT: 通过优化OOD数据上的超参数实现鲁棒的微调

    AutoFT: Robust Fine-Tuning by Optimizing Hyperparameters on OOD Data. (arXiv:2401.10220v1 [cs.CV])

    [http://arxiv.org/abs/2401.10220](http://arxiv.org/abs/2401.10220)

    AutoFT是一种通过优化超参数在OOD数据上进行基础模型微调的方法，以实现鲁棒性能。

    

    基础模型可以通过在特定任务的数据上进行微调来适应所需任务，但是在一种特定数据分布上微调模型往往会损害模型在其他分布上的原始性能。目前的鲁棒微调方法利用手工制定的正则化技术来约束微调过程，以保留基础模型的特征。然而，很难准确指定在微调过程中应保留哪些基础模型的特征，因为这取决于预训练、微调和评估数据分布之间的关系。我们提出了AutoFT，一种数据驱动的方法来引导基础模型的微调。AutoFT通过优化微调超参数来在小的ODD验证集上最大化性能。为了以细粒度的方式指导微调，AutoFT搜索一个高度表达的超参数空间，其中包括许多不同的权重系数。

    Foundation models encode rich representations that can be adapted to a desired task by fine-tuning on task-specific data. However, fine-tuning a model on one particular data distribution often compromises the model's original performance on other distributions. Current methods for robust fine-tuning utilize hand-crafted regularization techniques to constrain the fine-tuning process towards the base foundation model. Yet, it is hard to precisely specify what characteristics of the foundation model to retain during fine-tuning, as this depends on how the pre-training, fine-tuning, and evaluation data distributions relate to each other. We propose AutoFT, a data-driven approach for guiding foundation model fine-tuning. AutoFT optimizes fine-tuning hyperparameters to maximize performance on a small out-of-distribution (OOD) validation set. To guide fine-tuning in a granular way, AutoFT searches a highly expressive hyperparameter space that includes weight coefficients for many different lo
    
[^179]: AST-T5：面向代码生成和理解的结构感知预训练模型

    AST-T5: Structure-Aware Pretraining for Code Generation and Understanding. (arXiv:2401.03003v1 [cs.SE])

    [http://arxiv.org/abs/2401.03003](http://arxiv.org/abs/2401.03003)

    AST-T5是一种结构感知的预训练模型，通过利用抽象语法树（AST）来增强代码生成、转换和理解的能力。它优于其他同等大小的语言模型，并在代码到代码任务中表现出色。

    

    大型语言模型在代码相关任务中取得了显著进展，然而许多模型将代码视为简单序列，忽略了其结构化特性。我们引入了AST-T5，一种新颖的预训练范式，利用抽象语法树（AST）增强了代码生成、转换和理解。通过动态规划，我们的AST感知分割保留了代码结构，而AST感知跨度破坏目标使模型能够重建各种代码结构。与其他模型不同，AST-T5避免了复杂的程序分析或架构更改，因此可以与任何编码器-解码器Transformer无缝集成。评估结果显示，AST-T5在各种代码相关任务中始终优于同等大小的语言模型。结构感知使得AST-T5在代码到代码任务中特别强大，在Bugs2Fix任务的精确匹配得分上超过CodeT5 2个点，并在CodeXGLUE中的Java-C#转换任务的精确匹配得分上超过CodeT5 3个点。

    Large language models (LLMs) have made significant advancements in code-related tasks, yet many LLMs treat code as simple sequences, neglecting its structured nature. We introduce AST-T5, a novel pretraining paradigm that leverages the Abstract Syntax Tree (AST) for enhanced code generation, transpilation, and understanding. Using dynamic programming, our AST-Aware Segmentation retains code structure, while our AST-Aware Span Corruption objective equips the model to reconstruct various code structures. Unlike other models, AST-T5 avoids intricate program analyses or architectural changes, so it integrates seamlessly with any encoder-decoder Transformer. Evaluations show that AST-T5 consistently outperforms similar-sized LMs across various code-related tasks. Structure-awareness makes AST-T5 particularly powerful in code-to-code tasks, surpassing CodeT5 by 2 points in exact match score for the Bugs2Fix task and by 3 points in exact match score for Java-C# Transpilation in CodeXGLUE. Our
    
[^180]: 使用感知损失的扩散模型

    Diffusion Model with Perceptual Loss. (arXiv:2401.00110v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2401.00110](http://arxiv.org/abs/2401.00110)

    本研究介绍了一种使用感知损失的扩散模型，通过无分类器指导实现了生成更真实样本的目的。

    

    使用均方误差损失训练的扩散模型倾向于生成不真实的样本。目前的最先进模型依靠无分类器指导来改善样本质量，然而其惊人的效果尚未完全理解。本文中，我们展示了无分类器指导的有效性在一定程度上源自其作为一种隐式感知指导的形式。因此，我们可以直接在扩散训练中加入感知损失来提高样本质量。由于扩散训练中使用的分数匹配目标与无监督训练感知网络时使用的去噪自动编码器目标非常相似，因此扩散模型本身就是一个感知网络，并可以用于生成有意义的感知损失。我们提出了一种新颖的自感知目标，其结果是扩散模型能够生成更真实的样本。对于条件生成，我们的方法仅改善样本质量，而不与条件绑定。

    Diffusion models trained with mean squared error loss tend to generate unrealistic samples. Current state-of-the-art models rely on classifier-free guidance to improve sample quality, yet its surprising effectiveness is not fully understood. In this paper, We show that the effectiveness of classifier-free guidance partly originates from it being a form of implicit perceptual guidance. As a result, we can directly incorporate perceptual loss in diffusion training to improve sample quality. Since the score matching objective used in diffusion training strongly resembles the denoising autoencoder objective used in unsupervised training of perceptual networks, the diffusion model itself is a perceptual network and can be used to generate meaningful perceptual loss. We propose a novel self-perceptual objective that results in diffusion models capable of generating more realistic samples. For conditional generation, our method only improves sample quality without entanglement with the condit
    
[^181]: 缩放学习优化器是否值得？评估 VeLO 的 4000 个 TPU 月的价值。

    Is Scaling Learned Optimizers Worth It? Evaluating The Value of VeLO's 4000 TPU Months. (arXiv:2310.18191v1 [cs.LG])

    [http://arxiv.org/abs/2310.18191](http://arxiv.org/abs/2310.18191)

    VeLO是迄今为止规模最大的训练通用“基础”优化器的尝试，但我们的评估发现它需要问题特定的调优，并不一定优于竞争对手的解决方案质量和训练误差降低速度，这对于VeLO的通用性和培训投资的价值提出了质疑。

    

    我们分析了 VeLO（万能学习优化器），这是迄今为止规模最大的训练通用“基础”优化器的尝试。VeLO 使用超过 4000 个 TPU 月的机器学习任务进行训练，目标是产生一个能够推广到新问题并且不需要超参数调整，并且超过 Adam 等行业标准的优化器。我们对 MLCommons 优化器基准套件独立评估了 VeLO。我们发现与初步声明相反：（1）VeLO有一个关键的超参数需要根据具体问题进行调整，（2）VeLO在找到的解的质量上不一定优于竞争对手，（3）VeLO在降低训练误差上并不比竞争优化器更快。这些观察结果对 VeLO 的通用性和培训投资的价值提出了质疑。

    We analyze VeLO (versatile learned optimizer), the largest scale attempt to train a general purpose "foundational" optimizer to date. VeLO was trained on thousands of machine learning tasks using over 4000 TPU months with the goal of producing an optimizer capable of generalizing to new problems while being hyperparameter free, and outperforming industry standards such as Adam. We independently evaluate VeLO on the MLCommons optimizer benchmark suite. We find that, contrary to initial claims: (1) VeLO has a critical hyperparameter that needs problem-specific tuning, (2) VeLO does not necessarily outperform competitors in quality of solution found, and (3) VeLO is not faster than competing optimizers at reducing the training loss. These observations call into question VeLO's generality and the value of the investment in training it.
    
[^182]: 当机器学习模型泄漏：合成训练数据的探索

    When Machine Learning Models Leak: An Exploration of Synthetic Training Data. (arXiv:2310.08775v1 [cs.LG])

    [http://arxiv.org/abs/2310.08775](http://arxiv.org/abs/2310.08775)

    本论文研究了针对一个预测人员或家庭是否会在接下来的两年内搬迁的机器学习模型的攻击，攻击者利用模型的预测以及公开的训练数据边际分布来推断目标个体的敏感属性值，同时探讨了用合成数据替代原始数据训练模型对攻击的影响。

    

    我们研究了对一个机器学习模型的攻击，该模型用于预测一个人或家庭在接下来的两年内是否会搬迁，即迁移倾向分类器。攻击假设攻击者可以查询模型以获取预测，并且模型训练时使用的数据的边际分布是公开可用的。攻击还假设攻击者已经获取了一定数量目标个体的非敏感属性值。攻击的目标是推断这些目标个体的敏感属性值。我们探讨了在训练模型时用合成数据替代原始数据对攻击者成功推断敏感属性的影响。

    We investigate an attack on a machine learning model that predicts whether a person or household will relocate in the next two years, i.e., a propensity-to-move classifier. The attack assumes that the attacker can query the model to obtain predictions and that the marginal distribution of the data on which the model was trained is publicly available. The attack also assumes that the attacker has obtained the values of non-sensitive attributes for a certain number of target individuals. The objective of the attack is to infer the values of sensitive attributes for these target individuals. We explore how replacing the original data with synthetic data when training the model impacts how successfully the attacker can infer sensitive attributes.\footnote{Original paper published at PSD 2022. The paper was subsequently updated.}
    
[^183]: GenTKG: 基于生成模型的时间知识图谱预测

    GenTKG: Generative Forecasting on Temporal Knowledge Graph. (arXiv:2310.07793v1 [cs.CL])

    [http://arxiv.org/abs/2310.07793](http://arxiv.org/abs/2310.07793)

    研究提出了一种名为GenTKG的生成模型，用于在时间知识图谱上进行预测。该模型通过结合基于时间逻辑规则的检索策略和轻量级的参数效率指导，克服了复杂的时间图数据结构和庞大的数据量所带来的挑战。

    

    大规模语言模型(LLM)的快速发展引发了对时间知识图谱(tKG)领域的兴趣，其中传统的基于嵌入和规则的模型占主导地位。目前仍然存在一个问题，即预训练的LLM是否能够理解结构化的时间关系数据，并取代它们成为时间关系预测的基础模型。因此，我们将时间知识预测引入生成模式。然而，在复杂的时间图数据结构和LLM可以处理的序列自然表达之间存在巨大的鸿沟，在tKG的庞大数据量和微调LLM的巨大计算成本之间也存在挑战。为了解决这些挑战，我们提出了一种新颖的检索增强生成框架，称为GenTKG，它在tKG上执行生成式预测，结合了基于时间逻辑规则的检索策略和轻量级的参数效率指导。通过大量实验证明了GenTKG的有效性。

    The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional carefully designed embedding-based and rule-based models dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval augmented generation framework that performs generative forecasting on tKGs named GenTKG, which combines a temporal logical rule-based retrieval strategy and lightweight parameter-efficient instruction tuning. Extensive experiments hav
    
[^184]: 通过蛋白质片段周围对齐实现自监督口袋预训练

    Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment. (arXiv:2310.07229v1 [cs.LG])

    [http://arxiv.org/abs/2310.07229](http://arxiv.org/abs/2310.07229)

    本论文提出了一种通过蛋白质片段和周围对齐的自监督口袋预训练方法，用于模拟并生成大量的配体-受体相互作用复合物。

    

    口袋表示在各种生物医学应用中起着重要作用，如药物可裸露性估计、配体亲和力预测和新药设计等。然而，现有的几何特征和预训练表示通常将口袋独立于配体处理，忽略了它们之间的基本相互作用。我们提出了一种新颖的口袋预训练方法，借助高分辨率原子蛋白质结构的知识，并辅以高效的预训练小分子表示。将蛋白质结构分段为类似药物的片段及其相应的口袋，我们得到了一种合理的配体-受体相互作用模拟，从而生成了超过500万个复合物。

    Pocket representations play a vital role in various biomedical applications, such as druggability estimation, ligand affinity prediction, and de novo drug design. While existing geometric features and pretrained representations have demonstrated promising results, they usually treat pockets independent of ligands, neglecting the fundamental interactions between them. However, the limited pocket-ligand complex structures available in the PDB database (less than 100 thousand non-redundant pairs) hampers large-scale pretraining endeavors for interaction modeling. To address this constraint, we propose a novel pocket pretraining approach that leverages knowledge from high-resolution atomic protein structures, assisted by highly effective pretrained small molecule representations. By segmenting protein structures into drug-like fragments and their corresponding pockets, we obtain a reasonable simulation of ligand-receptor interactions, resulting in the generation of over 5 million complexes
    
[^185]: 一个可计数具有相同骨架的马尔可夫等价类的固定参数可处理算法

    A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence Classes with the same Skeleton. (arXiv:2310.04218v1 [cs.DS])

    [http://arxiv.org/abs/2310.04218](http://arxiv.org/abs/2310.04218)

    本文提出了一个固定参数可处理算法，用于计数具有相同骨架的马尔可夫等价类。

    

    因果有向无环图（也称为贝叶斯网络）是编码随机变量之间条件依赖关系的流行工具。在因果有向无环图中，随机变量被建模为有向图中的顶点，并且规定每个随机变量在给定其父节点的情况下与其祖先节点无关。然而，对于同一组随机变量上的两个不同的因果有向无环图可以准确编码相同的一组条件依赖关系。这样的因果有向无环图被称为马尔可夫等价，马尔可夫等价的因果有向无环图的等价类被称为马尔可夫等价类（MEC）。在过去几十年中，对于MEC已经创建了一些美丽的组合特征，并且已知，特别是在同一MEC中的所有因果有向无环图必须具有相同的“骨架”（底层无向图）和v-结构（形式为$a\rightarrow b \leftarrow c$的诱导子图）。这些组合特征还提出了几个自然的算法问题。

    Causal DAGs (also known as Bayesian networks) are a popular tool for encoding conditional dependencies between random variables. In a causal DAG, the random variables are modeled as vertices in the DAG, and it is stipulated that every random variable is independent of its ancestors conditioned on its parents. It is possible, however, for two different causal DAGs on the same set of random variables to encode exactly the same set of conditional dependencies. Such causal DAGs are said to be Markov equivalent, and equivalence classes of Markov equivalent DAGs are known as Markov Equivalent Classes (MECs). Beautiful combinatorial characterizations of MECs have been developed in the past few decades, and it is known, in particular that all DAGs in the same MEC must have the same ''skeleton'' (underlying undirected graph) and v-structures (induced subgraph of the form $a\rightarrow b \leftarrow c$).  These combinatorial characterizations also suggest several natural algorithmic questions. On
    
[^186]: 将大型语言模型应用于内容审查：数据工程和监督微调中的陷阱

    Adapting Large Language Models for Content Moderation: Pitfalls in Data Engineering and Supervised Fine-tuning. (arXiv:2310.03400v1 [cs.LG])

    [http://arxiv.org/abs/2310.03400](http://arxiv.org/abs/2310.03400)

    本文介绍了如何对LLM模型进行微调以实现内容审查的私下部署，讨论了引入原因的微调过程和直接分类任务的区别，并研究了利用更强大的LLM生成的原因对微调的影响。

    

    如今，数十亿人每天在互联网上进行沟通并表达自己的观点。不幸的是，并非所有这些表达都友好或合规，这使得内容审查成为一项不可或缺的任务。随着近年来大型语言模型（LLM）的成功发展，基于LLM的方法已成为处理各个领域任务的可行解决方案。然而，在内容审查领域，仍缺乏详细的工作系统地介绍实施细节。本文介绍了如何对LLM模型进行微调，以便可以私下部署用于内容审查。具体而言，我们讨论了在微调过程中是否应该引入原因，以及将其视为直接分类任务是否更好。我们还探讨了利用更强大的LLM生成的原因对私下部署模型进行微调的好处，以及在回答生成过程中不同处理方法的影响。

    Nowadays, billions of people engage in communication and express their opinions on the internet daily. Unfortunately, not all of these expressions are friendly or compliant, making content moderation an indispensable task. With the successful development of Large Language Models (LLMs) in recent years, LLM-based methods have become a feasible solution for handling tasks in various domains. However, in the field of content moderation, there is still a lack of detailed work that systematically introduces implementation details. In this paper, we introduce how to fine-tune an LLM model that can be privately deployed for content moderation. Specifically, we discuss whether incorporating reasons during the fine-tuning process would be better or if it should be treated as a classification task directly. We also explore the benefits of utilizing reasons generated by more powerful LLMs for fine-tuning privately deployed models and the impact of different processing approaches when the answers 
    
[^187]: 分析和改进基于OT的对抗网络

    Analyzing and Improving OT-based Adversarial Networks. (arXiv:2310.02611v1 [cs.LG])

    [http://arxiv.org/abs/2310.02611](http://arxiv.org/abs/2310.02611)

    本文分析和改进了基于OT的对抗网络，首先在一个统一的框架中统一了这些方法，然后通过全面分析展示了各组件在训练中的作用，最后提出了一个简单但新颖的方法以改进最优生成模型，该方法通过逐步调整生成分布逐渐使其与数据分布对齐

    

    最优输运（OT）问题旨在找到一种输运方案，它在最小化给定的成本函数的同时连接两个分布。OT理论已经广泛应用于生成模型。最初，OT距离被用作评估数据和生成分布之间的距离的一种度量。最近，OT传输映射在数据和先验分布之间被用作生成模型。这些基于OT的生成模型具有相似的对抗训练目标。在本文中，我们首先在一个统一的框架中统一了这些基于OT的对抗方法。然后，通过对这个统一框架的全面分析，我们阐明了每个组件在训练动力学中的作用。此外，我们提出了一个简单但新颖的方法，改进了先前表现最佳的基于OT的模型。直观地说，我们的方法通过逐步调整生成分布，逐渐使其与数据分布对齐，实现了渐进的改进。

    Optimal Transport (OT) problem aims to find a transport plan that bridges two distributions while minimizing a given cost function. OT theory has been widely utilized in generative modeling. In the beginning, OT distance has been used as a measure for assessing the distance between data and generated distributions. Recently, OT transport map between data and prior distributions has been utilized as a generative model. These OT-based generative models share a similar adversarial training objective. In this paper, we begin by unifying these OT-based adversarial methods within a single framework. Then, we elucidate the role of each component in training dynamics through a comprehensive analysis of this unified framework. Moreover, we suggest a simple but novel method that improves the previously best-performing OT-based model. Intuitively, our approach conducts a gradual refinement of the generated distribution, progressively aligning it with the data distribution. Our approach achieves a
    
[^188]: 用过参数化的神经网络中的隐式正则化方法进行多任务学习和微调

    Implicit regularization of multi-task learning and finetuning in overparameterized neural networks. (arXiv:2310.02396v1 [cs.LG])

    [http://arxiv.org/abs/2310.02396](http://arxiv.org/abs/2310.02396)

    本文研究了在过参数化神经网络中，多任务学习和微调所带来的隐式正则化效果。在简化的线性网络环境中，我们发现了多任务学习和微调所对特征共享和学习特定特征稀疏性的鼓励作用，并发现微调过程同时具有内核和特征学习的混合状态。此外，微调还可以展现一种嵌套特征学习行为，使其偏向于提取一组稀疏的特征子集。

    

    在深度学习中，常常使用训练辅助任务的方法来期望学习可以部分地转移到其他感兴趣的任务上。本研究探讨了学习辅助任务所产生的归纳偏置，包括同时学习（多任务学习，MTL）和依序学习（预训练和随后微调，PT+FT）。在使用梯度下降法训练两层对角线线性网络的简化环境中，我们发现了与MTL和PT+FT相关的隐式正则化惩罚，两者都鼓励任务之间的特征共享和学习任务特定特征的稀疏性。值得注意的是，我们的结果表明，在微调过程中，网络在先前研究中确定的内核（或“惰性”）状态和特征学习（“丰富”）状态之间具有混合状态。此外，PT+FT还可以展现一种新颖的“嵌套特征学习”行为，该行为无法被任何状态所捕捉，使其偏向于提取一组稀疏的特征子集。

    It is common in deep learning to train networks on auxiliary tasks with the expectation that the learning will transfer, at least partially, to another task of interest. In this work, we investigate the inductive biases that result from learning auxiliary tasks, either simultaneously (multi-task learning, MTL) or sequentially (pretraining and subsequent finetuning, PT+FT). In the simplified setting of two-layer diagonal linear networks trained with gradient descent, we identify implicit regularization penalties associated with MTL and PT+FT, both of which incentivize feature sharing between tasks and sparsity in learned task-specific features. Notably, our results imply that during finetuning, networks operate in a hybrid of the kernel (or "lazy") regime and the feature learning ("rich") regime identified in prior work. Moreover, PT+FT can exhibit a novel "nested feature learning" behavior not captured by either regime, which biases it to extract a sparse subset of the features learned
    
[^189]: Score dynamics: 使用条件扩散模型通过皮秒时间步提高分子动力学的规模化

    Score dynamics: scaling molecular dynamics with picosecond timesteps via conditional diffusion model. (arXiv:2310.01678v1 [physics.comp-ph])

    [http://arxiv.org/abs/2310.01678](http://arxiv.org/abs/2310.01678)

    该论文提出了Score dynamics (SD) 方法，通过条件扩散模型，可以使用1 ps的时间步长进行分子动力学模拟。基于图神经网络的Score dynamics模型展示了在丙氨酸二肽和短链烷烃案例中的效果。

    

    我们提出了一种称为Score dynamics (SD) 的通用框架，用于从分子动力学模拟中学习有效的演化算子，用于原子级和粗粒化动力学。SD以分数为中心，即与动态自由度的转换对数概率导数相关的量。后者在分数时间步中起到与MD中力场相同的作用，但在去噪扩散概率模型中用于生成动态变量的离散转变。这种时间步长可以比典型的MD时间步长大几个数量级。在这项工作中，我们构建了基于图神经网络的Score dynamics模型，用于演化以1~ps时间步长的现实分子体系。我们通过丙氨酸二肽和水溶液中的短链烷烃的案例研究证明了Score dynamics的效能。通过从条件概率的平稳分布中推导出的平衡预测和对转换速率和转换的动力学预测进行演示。

    We propose score dynamics (SD), a general framework for learning effective evolution operators for atomistic as well as coarse-grained dynamics from molecular-dynamics (MD) simulations. SD is centered around scores, or derivatives of the transition log-probability with respect to the dynamical degrees of freedom. The latter play the same role as force fields in MD but are used in denoising diffusion probability models to generate discrete transitions of the dynamical variables in an SD timestep, which can be orders of magnitude larger than a typical MD timestep. In this work, we construct graph neural network based score dynamics models of realistic molecular systems that are evolved with 1~ps timesteps. We demonstrate the efficacy of score dynamics with case studies of alanine dipeptide and short alkanes in aqueous solution. Both equilibrium predictions derived from the stationary distributions of the conditional probability and kinetic predictions for the transition rates and transit
    
[^190]: RECOMBINER：鲁棒性和增强的贝叶斯隐式神经表示的压缩方法

    RECOMBINER: Robust and Enhanced Compression with Bayesian Implicit Neural Representations. (arXiv:2309.17182v1 [cs.LG])

    [http://arxiv.org/abs/2309.17182](http://arxiv.org/abs/2309.17182)

    RECOMBINER是一种鲁棒性和增强的贝叶斯隐式神经表示的压缩方法，通过丰富变分逼近、增加位置编码和分割高分辨率数据来解决COMBINER存在的局限性。

    

    COMBINER是一种最近提出的数据压缩方法，它解决了先前基于隐式神经表示的方法存在的关键效率问题：避免了量化，并实现了对速率-失真性能的直接优化。然而，COMBINER仍然存在明显的局限性：1）使用因式化的先验和后验逼近，缺乏灵活性；2）不能有效地适应数据中的局部偏离全局模式；3）其性能易受建模选择和变分参数初始化的影响。我们提出的方法，鲁棒和增强的COMBINER(RECOMBINER)，通过以下方式解决了这些问题：1）通过对INR权重进行线性参数化，丰富变分逼近，并保持计算成本；2）通过增加可学习的位置编码来增强我们的INR，使其适应局部细节；3）将高分辨率数据分割成...

    COMpression with Bayesian Implicit NEural Representations (COMBINER) is a recent data compression method that addresses a key inefficiency of previous Implicit Neural Representation (INR)-based approaches: it avoids quantization and enables direct optimization of the rate-distortion performance. However, COMBINER still has significant limitations: 1) it uses factorized priors and posterior approximations that lack flexibility; 2) it cannot effectively adapt to local deviations from global patterns in the data; and 3) its performance can be susceptible to modeling choices and the variational parameters' initializations. Our proposed method, Robust and Enhanced COMBINER (RECOMBINER), addresses these issues by 1) enriching the variational approximation while maintaining its computational cost via a linear reparameterization of the INR weights, 2) augmenting our INRs with learnable positional encodings that enable them to adapt to local details and 3) splitting high-resolution data into pa
    
[^191]: 无监督语言模型蒸馏的事实验证

    Unsupervised Fact Verification by Language Model Distillation. (arXiv:2309.16540v1 [cs.CL])

    [http://arxiv.org/abs/2309.16540](http://arxiv.org/abs/2309.16540)

    本文提出了一种名为SFAVEL的无监督框架，通过语言模型蒸馏将自监督特征转化为高质量的主张-事实对齐，实现无监督事实验证。这通过一种新颖的对比损失函数实现，同时保留语料库间的语义关系。

    

    无监督事实验证旨在通过可靠知识库中的证据来验证主张，而无需任何形式的数据注释。为了解决这个挑战，算法必须为每个主张生成既语义明确又紧凑的特征，以便与源信息进行语义对齐。与之前的工作不同，前者通过学习包含主张及其相应标签的注释语料库来解决对齐问题。我们提出了SFAVEL（通过语言模型蒸馏的自监督事实验证），这是一个新颖的无监督框架，利用预训练的语言模型将自监督特征蒸馏为高质量的主张-事实对齐，而无需注释。这是通过一种新颖的对比损失函数实现的，该函数鼓励特征在保持语料库间的语义关系的同时实现高质量的主张和证据对齐。值得注意的是，我们展示了达到新颖的状态一.

    Unsupervised fact verification aims to verify a claim using evidence from a trustworthy knowledge base without any kind of data annotation. To address this challenge, algorithms must produce features for every claim that are both semantically meaningful, and compact enough to find a semantic alignment with the source information. In contrast to previous work, which tackled the alignment problem by learning over annotated corpora of claims and their corresponding labels, we propose SFAVEL (Self-supervised Fact Verification via Language Model Distillation), a novel unsupervised framework that leverages pre-trained language models to distil self-supervised features into high-quality claim-fact alignments without the need for annotations. This is enabled by a novel contrastive loss function that encourages features to attain high-quality claim and evidence alignments whilst preserving the semantic relationships across the corpora. Notably, we present results that achieve a new state-of-the
    
[^192]: 复杂长视程机器人操作任务的内在语言引导探索

    Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks. (arXiv:2309.16347v1 [cs.RO])

    [http://arxiv.org/abs/2309.16347](http://arxiv.org/abs/2309.16347)

    本文提出了基于大型语言模型的内在引导探索（IGE-LLMs）框架，通过利用LLMs作为辅助内在奖励，解决了复杂长视程机器人操作任务中奖励稀疏问题，并在实验中展示了其较高的性能和模块化特性。

    

    当前的强化学习算法在稀疏和复杂的环境中面临困境，尤其是在涉及众多不同序列的长视程操作任务中。在本研究中，我们提出了基于大型语言模型的内在引导探索（IGE-LLMs）框架。通过利用LLMs作为辅助内在奖励，IGE-LLMs引导强化学习中的探索过程，以解决复杂的长视程操作任务中奖励稀疏问题。我们在一个具有探索挑战的环境和一个同时面临探索和长视程挑战的复杂机器人操作任务中评估了我们的框架和相关的内在学习方法。结果显示，IGE-LLMs(i)在相关的内在方法和直接使用LLMs进行决策的性能上表现出明显的较高水平，(ii)可以与现有的学习方法相结合和互补，突出其模块化性能，(iii)对于不同的内在缩放参数比较不敏感。

    Current reinforcement learning algorithms struggle in sparse and complex environments, most notably in long-horizon manipulation tasks entailing a plethora of different sequences. In this work, we propose the Intrinsically Guided Exploration from Large Language Models (IGE-LLMs) framework. By leveraging LLMs as an assistive intrinsic reward, IGE-LLMs guides the exploratory process in reinforcement learning to address intricate long-horizon with sparse rewards robotic manipulation tasks. We evaluate our framework and related intrinsic learning methods in an environment challenged with exploration, and a complex robotic manipulation task challenged by both exploration and long-horizons. Results show IGE-LLMs (i) exhibit notably higher performance over related intrinsic methods and the direct use of LLMs in decision-making, (ii) can be combined and complement existing learning methods highlighting its modularity, (iii) are fairly insensitive to different intrinsic scaling parameters, and 
    
[^193]: PointSSC：一个用于语义场景补全的车辆基础设施点云合作基准

    PointSSC: A Cooperative Vehicle-Infrastructure Point Cloud Benchmark for Semantic Scene Completion. (arXiv:2309.12708v1 [cs.CV])

    [http://arxiv.org/abs/2309.12708](http://arxiv.org/abs/2309.12708)

    PointSSC是第一个为了语义场景补全而引入的车辆基础设施点云合作基准，具备长距离感知和最小遮挡。通过使用Segment Anything进行自动化注释，我们提出了一种基于激光雷达的模型，结合补全和分割的合作模块，来推动语义点云补全在真实世界导航中的发展。

    

    语义场景补全旨在为复杂的3D场景生成空间占用和语义标签。大多数现有的语义场景补全模型都集中在体素表示上，对于大型室外空间来说存在内存效率低下的问题。点云提供了一种轻量级的替代方案，但现有基准缺乏带有语义标签的室外点云场景。为了解决这个问题，我们引入了第一个用于语义场景补全的车辆基础设施点云合作基准PointSSC。这些场景具有长距离感知和最小的遮挡。我们利用Segment Anything开发了一个自动化注释流程，以高效地分配语义标签。为了评估进展，我们提出了一个基于激光雷达的模型，其中包括一个空间感知变换器用于全局和局部特征提取，以及一个补全和分割合作模块用于联合补全和分割。PointSSC提供了一个具有挑战性的测试平台，推动了语义点云补全在真实世界导航中的进展。

    Semantic Scene Completion (SSC) aims to jointly generate space occupancies and semantic labels for complex 3D scenes. Most existing SSC models focus on volumetric representations, which are memory-inefficient for large outdoor spaces. Point clouds provide a lightweight alternative but existing benchmarks lack outdoor point cloud scenes with semantic labels. To address this, we introduce PointSSC, the first cooperative vehicle-infrastructure point cloud benchmark for semantic scene completion. These scenes exhibit long-range perception and minimal occlusion. We develop an automated annotation pipeline leveraging Segment Anything to efficiently assign semantics. To benchmark progress, we propose a LiDAR-based model with a Spatial-Aware Transformer for global and local feature extraction and a Completion and Segmentation Cooperative Module for joint completion and segmentation. PointSSC provides a challenging testbed to drive advances in semantic point cloud completion for real-world navi
    
[^194]: 解决异构联邦学习中非独立同分布问题的梯度协调方法

    Tackling the Non-IID Issue in Heterogeneous Federated Learning by Gradient Harmonization. (arXiv:2309.06692v1 [cs.LG])

    [http://arxiv.org/abs/2309.06692](http://arxiv.org/abs/2309.06692)

    本研究通过梯度协调方法解决了异构联邦学习中的非独立同分布问题，提出了FedGH，通过减轻本地漂移来增强性能。实验证明，在多个基准和非独立同分布场景下，FedGH始终能够显著提升联邦学习的性能。

    

    联邦学习是一种保护隐私的范式，用于从分散的客户端协作训练全局模型。然而，联邦学习的性能受到非独立同分布的数据和设备异构性的影响。在本研究中，我们通过服务器端的梯度冲突视角重新思考这个关键挑战。具体而言，我们首先调查了多个客户端之间的梯度冲突现象，并揭示了更强的异构性会导致更严重的梯度冲突。为了解决这个问题，我们提出了FedGH，一种简单而有效的方法，通过梯度协调来减轻本地漂移。这种技术将一个梯度向量投影到与其他冲突客户端对之间的正交平面上。广泛的实验表明，FedGH在不同基准和非独立同分布场景下始终能够显著提升多个最先进的联邦学习基线。值得注意的是，FedGH在特定场景中取得了更显著的改进。

    Federated learning (FL) is a privacy-preserving paradigm for collaboratively training a global model from decentralized clients. However, the performance of FL is hindered by non-independent and identically distributed (non-IID) data and device heterogeneity. In this work, we revisit this key challenge through the lens of gradient conflicts on the server side. Specifically, we first investigate the gradient conflict phenomenon among multiple clients and reveal that stronger heterogeneity leads to more severe gradient conflicts. To tackle this issue, we propose FedGH, a simple yet effective method that mitigates local drifts through Gradient Harmonization. This technique projects one gradient vector onto the orthogonal plane of the other within conflicting client pairs. Extensive experiments demonstrate that FedGH consistently enhances multiple state-of-the-art FL baselines across diverse benchmarks and non-IID scenarios. Notably, FedGH yields more significant improvements in scenarios 
    
[^195]: 离线逆向强化学习下的提示评估与优化

    Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning. (arXiv:2309.06553v1 [cs.CL])

    [http://arxiv.org/abs/2309.06553](http://arxiv.org/abs/2309.06553)

    这项工作介绍了一种基于离线逆向强化学习的提示评估与优化方法，通过利用离线数据集和逆向强化学习，预测提示性能、提高成本效益、生成易读的结果。

    

    最近，像ChatGPT这样的大型语言模型（LLM）的发展取得了显著的性能，通过利用人类专业知识。然而，充分揭示LLMs在复杂任务中的潜力需要在自然语言提示的广阔搜索空间中进行导航。虽然提示工程显示出潜力，但试错尝试中所需的人工设计提示和相关成本带来了重大挑战。关键是，提示优化的效率取决于昂贵的提示评估过程。本工作介绍了Prompt-OIRL，这是一种基于离线逆向强化学习的方法，旨在弥合有效提示评估和可负担性之间的差距。我们的方法利用专家评估的离线数据集，运用逆向强化学习获得一个针对离线、查询依赖型提示评估的奖励模型。Prompt-OIRL的优点是多方面的：它预测提示的性能，成本高效，生成易读的结果。

    The recent advances in the development of Large Language Models (LLMs) like ChatGPT have achieved remarkable performance by leveraging human expertise. Yet, fully eliciting LLMs' potential for complex tasks requires navigating the vast search space of natural language prompts. While prompt engineering has shown promise, the requisite human-crafted prompts in trial-and-error attempts and the associated costs pose significant challenges. Crucially, the efficiency of prompt optimization hinges on the costly procedure of prompt evaluation. This work introduces Prompt-OIRL, an approach rooted in offline inverse reinforcement learning that seeks to bridge the gap between effective prompt evaluation and affordability. Our method draws on offline datasets from expert evaluations, employing Inverse-RL to derive a reward model for offline, query-dependent prompt evaluations. The advantages of Prompt-OIRL are manifold: it predicts prompt performance, is cost-efficient, produces human-readable res
    
[^196]: 通过联合等变扩散进行晶体结构预测

    Crystal Structure Prediction by Joint Equivariant Diffusion. (arXiv:2309.04475v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2309.04475](http://arxiv.org/abs/2309.04475)

    本文提出了DiffCSP，一种利用周期-E(3)-等变去噪模型的扩散模型，用于通过联合生成晶体的晶格和原子坐标，以解决晶体结构预测中的对称性挑战。

    

    晶体结构预测在各个科学学科中至关重要。尽管目前流行的生成模型（如扩散模型）可以用于解决晶体结构预测的任务，但由于晶体结构的对称几何特性（平移、旋转和周期性的不变性），该任务面临着独特的挑战。为了融入以上对称性，本文提出了DiffCSP，一种新颖的扩散模型，通过采用周期-E(3)-等变去噪模型，联合生成每个晶体的晶格和原子坐标，以更好地建模晶体几何。值得注意的是，与相关的等变生成方法不同，DiffCSP利用分数坐标而不是笛卡尔坐标来表示晶体，显著提升了原子位置的扩散和生成过程。大量实验证明，我们的DiffCSP明显优于现有方法。

    Crystal Structure Prediction (CSP) is crucial in various scientific disciplines. While CSP can be addressed by employing currently-prevailing generative models (e.g. diffusion models), this task encounters unique challenges owing to the symmetric geometry of crystal structures -- the invariance of translation, rotation, and periodicity. To incorporate the above symmetries, this paper proposes DiffCSP, a novel diffusion model to learn the structure distribution from stable crystals. To be specific, DiffCSP jointly generates the lattice and atom coordinates for each crystal by employing a periodic-E(3)-equivariant denoising model, to better model the crystal geometry. Notably, different from related equivariant generative approaches, DiffCSP leverages fractional coordinates other than Cartesian coordinates to represent crystals, remarkably promoting the diffusion and the generation process of atom positions. Extensive experiments verify that our DiffCSP significantly outperforms existing
    
[^197]: 基于相关性的模糊聚类有效性指标与次要选项检测器

    A correlation-based fuzzy cluster validity index with secondary options detector. (arXiv:2308.14785v1 [stat.ML])

    [http://arxiv.org/abs/2308.14785](http://arxiv.org/abs/2308.14785)

    本研究提出了一种基于相关性的模糊聚类有效性指标，该指标考虑了在聚类数量选择时可能存在的多个选项，并通过评估在多种数据集上的性能，与现有指标进行比较。

    

    应用聚类分析时，最佳聚类数量是主要关注点之一。已经引入了多个聚类有效性指标来解决这个问题。然而，在某些情况下，有多个选项可以作为最终的聚类数量。大多数现有工作在这个领域忽视了这一方面。在本研究中，我们引入了一种基于相关性的模糊聚类有效性指标，称为Wiroonsri-Preedasawakul（WP）指标。该指标根据一对数据点的实际距离与相应对的调整质心之间的距离之间的相关性来定义。我们评估并比较了我们的指标与Xie-Beni，Pakhira-Bandyopadhyay-Maulik，Tang，Wu-Li，广义C和Kwon2等几个现有指标的性能。我们在四种类型的数据集上进行了评估：人工数据集，现实世界数据集，带有等级的模拟数据集和图像数据集，使用模糊c-mea算法。

    The optimal number of clusters is one of the main concerns when applying cluster analysis. Several cluster validity indexes have been introduced to address this problem. However, in some situations, there is more than one option that can be chosen as the final number of clusters. This aspect has been overlooked by most of the existing works in this area. In this study, we introduce a correlation-based fuzzy cluster validity index known as the Wiroonsri-Preedasawakul (WP) index. This index is defined based on the correlation between the actual distance between a pair of data points and the distance between adjusted centroids with respect to that pair. We evaluate and compare the performance of our index with several existing indexes, including Xie-Beni, Pakhira-Bandyopadhyay-Maulik, Tang, Wu-Li, generalized C, and Kwon2. We conduct this evaluation on four types of datasets: artificial datasets, real-world datasets, simulated datasets with ranks, and image datasets, using the fuzzy c-mea
    
[^198]: 异构和资源受限环境中的资源高效联邦学习

    Resource-Efficient Federated Learning for Heterogenous and Resource-Constrained Environments. (arXiv:2308.13662v1 [cs.LG])

    [http://arxiv.org/abs/2308.13662](http://arxiv.org/abs/2308.13662)

    提出了一种新的资源高效联邦学习方法，通过可变剪枝技术和知识蒸馏来解决资源受限设备中的计算和通信挑战，并在保持数据隐私和性能的同时适应异构模型架构。

    

    联邦学习 (FL) 是机器学习中的一种保护隐私的子领域，它将模型带到用户设备进行训练，避免了与中央服务器共享个人数据的需求。虽然现有的工作解决了数据异质性，但忽视了FL中的其他挑战，如设备异质性和通信效率。在本文中，我们提出了RE-FL，这是一种面向资源受限设备的新方法，解决了计算和通信挑战。我们的可变剪枝技术通过根据每个客户端的计算能力进行剪枝优化资源利用。我们还采用知识蒸馏来减少带宽消耗和通信轮数。图像分类任务的实验结果证明了我们的方法在资源受限环境中的有效性，同时满足异构模型架构的数据隐私和性能需求。

    Federated Learning (FL) is a privacy-enforcing sub-domain of machine learning that brings the model to the user's device for training, avoiding the need to share personal data with a central server. While existing works address data heterogeneity, they overlook other challenges in FL, such as device heterogeneity and communication efficiency. In this paper, we propose RE-FL, a novel approach that tackles computational and communication challenges in resource-constrained devices. Our variable pruning technique optimizes resource utilization by adapting pruning to each client's computational capabilities. We also employ knowledge distillation to reduce bandwidth consumption and communication rounds. Experimental results on image classification tasks demonstrate the effectiveness of our approach in resource-constrained environments, maintaining data privacy and performance while accommodating heterogeneous model architectures.
    
[^199]: 统一的数据管理和综合性能评估用于城市时空预测[实验，分析和基准]的论文

    Unified Data Management and Comprehensive Performance Evaluation for Urban Spatial-Temporal Prediction [Experiment, Analysis & Benchmark]. (arXiv:2308.12899v1 [cs.LG])

    [http://arxiv.org/abs/2308.12899](http://arxiv.org/abs/2308.12899)

    该论文提出了针对城市时空预测的统一数据管理和综合性能评估方法。其贡献包括引入“原子文件”作为统一存储格式，提供了城市时空预测模型技术进展的全面概述，并建立了性能排行榜和鉴定了有潜力的模型和数据集。

    

    随着深度学习技术的发展和大规模数据集的可用性，城市时空预测领域正在迅速发展。然而，从不同来源和以不同格式存储的多样化城市时空数据的访问和利用仍然存在挑战，而在深度学习模型大量增加的情况下，确定有效的模型结构和组件也是一个挑战。本文解决了这些挑战，并提供了三个重要的贡献。首先，我们引入了“原子文件”，这是一种为城市时空大数据设计的统一存储格式，并在40个不同的数据集上验证了其有效性，简化了数据管理。其次，我们全面概述了城市时空预测模型的技术进展，指导了强大模型的发展。第三，我们使用不同的模型和数据集进行了大量实验，建立了性能排行榜并确定了有潜力的模型和数据集。

    The field of urban spatial-temporal prediction is advancing rapidly with the development of deep learning techniques and the availability of large-scale datasets. However, challenges persist in accessing and utilizing diverse urban spatial-temporal datasets from different sources and stored in different formats, as well as determining effective model structures and components with the proliferation of deep learning models. This work addresses these challenges and provides three significant contributions. Firstly, we introduce "atomic files", a unified storage format designed for urban spatial-temporal big data, and validate its effectiveness on 40 diverse datasets, simplifying data management. Secondly, we present a comprehensive overview of technological advances in urban spatial-temporal prediction models, guiding the development of robust models. Thirdly, we conduct extensive experiments using diverse models and datasets, establishing a performance leaderboard and identifying promis
    
[^200]: 一个关于校准的基准研究

    A Benchmark Study on Calibration. (arXiv:2308.11838v1 [cs.LG])

    [http://arxiv.org/abs/2308.11838](http://arxiv.org/abs/2308.11838)

    这项研究提出了一个模型校准的基准研究，利用神经架构搜索空间探索了模型校准属性。研究结果显示，模型校准可以在不同任务中泛化，并可以同时兼顾模型的准确性和校准性能。

    

    深度神经网络在各种机器学习任务中的应用越来越广泛。然而，随着这些模型复杂性的增加，它们往往面临校准问题，尽管预测准确性有所提高。许多研究通过数据预处理、使用特定损失函数和训练框架来改善校准性能。然而，对校准属性的研究有点被忽视了。我们的研究利用神经架构搜索（NAS）搜索空间，在全面探索校准属性的模型架构空间中提供了一个详尽的模型架构空间。我们特别创建了一个模型校准数据集。该数据集在广泛使用的NATS-Bench搜索空间中评估了90个基于区间的校准度量和12个其他校准度量，涵盖了117,702个独特的神经网络。我们的分析旨在通过我们提出的数据集回答该领域一些长期存在的问题：（i）模型校准能否在不同任务中泛化？（ii）能否同时兼顾模型的准确性和校准性能？

    Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through data preprocessing, the use of specific loss functions, and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. Our study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We specifically create a model calibration dataset. This dataset evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. Our analysis aims to answer several longstanding questions in the field, using our proposed dataset: (i) Can model calibration be generalized across different tasks? (ii) Can rob
    
[^201]: 通过任务分解学习抽象视觉推理：基于Raven渐进矩阵的案例研究

    Learning Abstract Visual Reasoning via Task Decomposition: A Case Study in Raven Progressive Matrices. (arXiv:2308.06528v1 [cs.AI])

    [http://arxiv.org/abs/2308.06528](http://arxiv.org/abs/2308.06528)

    通过任务分解学习抽象视觉推理，提出了一种基于变形器蓝图的深度学习架构，该架构预测单个对象及其排列的视觉特性，通过多维预测来选择答案。

    

    学习进行抽象推理的挑战之一是问题通常被提出为整体任务，没有中间目标。在Raven渐进矩阵（RPM）中，任务是在给定上下文的情况下选择一个可用答案，其中上下文和答案都是复合图像，具有多个对象以及各种空间安排。由于只有这个高级目标作为指导，学习变得具有挑战性，大多数现代解决方案往往不透明。在本研究中，我们提出了一种基于变形器蓝图的深度学习架构，该架构不直接进行上述选择，而是预测单个对象及其排列的视觉特性。通过这种方式获得的多维预测直接并置以选择答案。我们考虑了模型将视觉输入解析为令牌的几种方式，并采用了几种自监督训练中输入的屏蔽方法。

    One of the challenges in learning to perform abstract reasoning is that problems are often posed as monolithic tasks, with no intermediate subgoals. In Raven Progressive Matrices (RPM), the task is to choose one of the available answers given a context, where both contexts and answers are composite images featuring multiple objects in various spatial arrangements. As this high-level goal is the only guidance available, learning is challenging and most contemporary solvers tend to be opaque. In this study, we propose a deep learning architecture based on the transformer blueprint which, rather than directly making the above choice, predicts the visual properties of individual objects and their arrangements. The multidimensional predictions obtained in this way are then directly juxtaposed to choose the answer. We consider a few ways in which the model parses the visual input into tokens and several regimes of masking parts of the input in self-supervised training. In experimental assess
    
[^202]: 大规模偏t乌鸦勾结的高效变分推理及其在股票收益率中的应用

    Efficient Variational Inference for Large Skew-t Copulas with Application to Intraday Equity Returns. (arXiv:2308.05564v1 [econ.EM])

    [http://arxiv.org/abs/2308.05564](http://arxiv.org/abs/2308.05564)

    本研究提出一种快速而准确的贝叶斯变分推理方法，用于估计大规模偏t乌鸦因子勾结模型。该方法能够捕捉到金融数据中的不对称和极端尾部相关性，以及股票对之间的异质性非对称依赖。

    

    大规模偏t乌鸦因子勾结模型对金融数据建模具有吸引力，因为它们允许不对称和极端的尾部相关性。我们展示了Azzalini和Capitanio（2003）所隐含的乌鸦勾结在成对非对称依赖性方面比两种流行的乌鸦勾结更高。在高维情况下，对该乌鸦勾结的估计具有挑战性，我们提出了一种快速而准确的贝叶斯变分推理方法来解决这个问题。该方法使用条件高斯生成表示法定义了一个可以准确近似的附加后验。使用快速随机梯度上升算法来解决变分优化。这种新的方法被用来估计2017年至2021年间93个美国股票的股票收益率的勾结模型。除了成对相关性的变化外，该勾结还捕捉到了股票对之间的非对称依赖的大量异质性。

    Large skew-t factor copula models are attractive for the modeling of financial data because they allow for asymmetric and extreme tail dependence. We show that the copula implicit in the skew-t distribution of Azzalini and Capitanio (2003) allows for a higher level of pairwise asymmetric dependence than two popular alternative skew-t copulas. Estimation of this copula in high dimensions is challenging, and we propose a fast and accurate Bayesian variational inference (VI) approach to do so. The method uses a conditionally Gaussian generative representation of the skew-t distribution to define an augmented posterior that can be approximated accurately. A fast stochastic gradient ascent algorithm is used to solve the variational optimization. The new methodology is used to estimate copula models for intraday returns from 2017 to 2021 on 93 U.S. equities. The copula captures substantial heterogeneity in asymmetric dependence over equity pairs, in addition to the variability in pairwise co
    
[^203]: 生成对抗网络在组织病理学染色标准化中的应用

    Generative Adversarial Networks for Stain Normalisation in Histopathology. (arXiv:2308.02851v1 [eess.IV])

    [http://arxiv.org/abs/2308.02851](http://arxiv.org/abs/2308.02851)

    本论文研究了生成对抗网络在数字病理学染色标准化中的应用，发现基于GAN的方法在染色标准化方面表现出色，但需要更大的计算资源。

    

    近年来，数字病理学的快速发展为基于人工智能的工具改善临床诊断的准确性和效率提供了理想的机会。然而，当前研究的一个重要障碍是数字病理学图像之间存在高水平的视觉变异性，导致模型对未见过的数据泛化能力较差。染色标准化旨在在不改变图像结构内容的情况下标准化数字病理学图像的视觉特征。在本章中，我们探讨了用于数字病理学染色标准化的不同技术，重点介绍了利用生成对抗网络（GANs）的方法。通常，基于GAN的方法优于非生成方法，但代价是更大的计算需求。然而，目前尚不清楚哪种方法对染色标准化更好，不同的GAN和非GAN方法相互之间表现出色。

    The rapid growth of digital pathology in recent years has provided an ideal opportunity for the development of artificial intelligence-based tools to improve the accuracy and efficiency of clinical diagnoses. One of the significant roadblocks to current research is the high level of visual variability across digital pathology images, causing models to generalise poorly to unseen data. Stain normalisation aims to standardise the visual profile of digital pathology images without changing the structural content of the images. In this chapter, we explore different techniques which have been used for stain normalisation in digital pathology, with a focus on approaches which utilise generative adversarial networks (GANs). Typically, GAN-based methods outperform non-generative approaches but at the cost of much greater computational requirements. However, it is not clear which method is best for stain normalisation in general, with different GAN and non-GAN approaches outperforming each othe
    
[^204]: 使用深度集成神经网络在端点设备上预测小分子的溶解度

    Predicting small molecules solubilities on endpoint devices using deep ensemble neural networks. (arXiv:2307.05318v1 [physics.chem-ph])

    [http://arxiv.org/abs/2307.05318](http://arxiv.org/abs/2307.05318)

    这项工作提出了一种使用深度集成神经网络在端点设备上预测小分子溶解度的方法，通过静态网站运行，同时具备预测不确定性，并实现了令人满意的结果。

    

    水溶解度是一种有价值但难以预测的性质。使用一级原理方法计算溶解度需要考虑熵和焓的竞争效应，导致计算时间较长且准确性相对较差。基于数据驱动的方法，如深度学习，提供了更高的准确性和计算效率，但通常缺乏不确定性量化。此外，任何计算技术的易用性仍然是一个问题，导致群体贡献方法的持续流行。在这项工作中，我们使用一种具有预测不确定性的深度学习模型来解决这些问题，该模型在静态网站上运行（无需服务器）。这种方法将计算需求转移到网站访问者身上，而不需要安装，消除了支付和维护服务器的需求。我们的模型在溶解度预测上取得了令人满意的结果。此外，我们展示了如何创建平衡溶解度预测模型。

    Aqueous solubility is a valuable yet challenging property to predict. Computing solubility using first-principles methods requires accounting for the competing effects of entropy and enthalpy, resulting in long computations for relatively poor accuracy. Data-driven approaches, such as deep learning, offer improved accuracy and computational efficiency but typically lack uncertainty quantification. Additionally, ease of use remains a concern for any computational technique, resulting in the sustained popularity of group-based contribution methods. In this work, we addressed these problems with a deep learning model with predictive uncertainty that runs on a static website (without a server). This approach moves computing needs onto the website visitor without requiring installation, removing the need to pay for and maintain servers. Our model achieves satisfactory results in solubility prediction. Furthermore, we demonstrate how to create molecular property prediction models that balanc
    
[^205]: 多项式宽度对于具有高维特征的集合表示足够

    Polynomial Width is Sufficient for Set Representation with High-dimensional Features. (arXiv:2307.04001v1 [cs.LG])

    [http://arxiv.org/abs/2307.04001](http://arxiv.org/abs/2307.04001)

    本研究通过两种集合元素嵌入层的探索，证明了多项式宽度对于高维特征的集合表示足够，并揭示了之前分析中的局限性。

    

    集合表示在深度学习中已经变得普遍，用于建模神经网络对输入顺序不敏感的归纳偏差。DeepSets是最常用的集合表示神经网络架构，它将每个集合元素嵌入到具有维度L的潜在空间中，然后进行求和池化以获得整个集合的嵌入，最后将整个集合的嵌入映射到输出。在这项工作中，我们研究了维度L对DeepSets表达能力的影响。之前的分析要么将高维特征过于简化为一维特征，要么局限于分析激活函数，从而脱离实际应用或导致L随着集合大小N和特征维度D呈指数增长。为了研究达到足够表达能力的最小L值，我们提出了两种集合元素嵌入层：（a）线性+幂激活（LP）和（b）线性+指数激活。

    Set representation has become ubiquitous in deep learning for modeling the inductive bias of neural networks that are insensitive to the input order. DeepSets is the most widely used neural network architecture for set representation. It involves embedding each set element into a latent space with dimension $L$, followed by a sum pooling to obtain a whole-set embedding, and finally mapping the whole-set embedding to the output. In this work, we investigate the impact of the dimension $L$ on the expressive power of DeepSets. Previous analyses either oversimplified high-dimensional features to be one-dimensional features or were limited to analytic activations, thereby diverging from practical use or resulting in $L$ that grows exponentially with the set size $N$ and feature dimension $D$. To investigate the minimal value of $L$ that achieves sufficient expressive power, we present two set-element embedding layers: (a) linear + power activation (LP) and (b) linear + exponential activatio
    
[^206]: 符合目标：使用通用的插入式框架在CTC模型中优化所需属性

    Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework. (arXiv:2307.01715v1 [cs.CL])

    [http://arxiv.org/abs/2307.01715](http://arxiv.org/abs/2307.01715)

    本文提出了一个通用的插入式框架，用于优化CTC模型中的所需属性。该框架通过补充额外的损失项来优先考虑符合所需属性的对齐，并不需要修改CTC损失函数。

    

    连接主义时间分类（CTC）是训练监督序列到序列模型广泛使用的准则。它通过将完美对齐（产生基本事实）的边际化来学习输入和输出序列之间的关系，称为对其，以代价不完美对齐。这种对完美和不完美对齐的二元区分无法捕捉到在其他实际应用中具有重要意义的其他关键对齐属性。在这里，我们提出了$\textit{Align With Purpose}$，这是一个用于增强CTC条件下训练模型中所需属性的$\textbf{通用插入式框架}$。我们通过使用额外的损失项来补充CTC来优先考虑符合所需属性的对齐。我们的方法不需要干预CTC损失函数，能够轻松优化各种属性，并且可以区分完美和不完美的对齐。

    Connectionist Temporal Classification (CTC) is a widely used criterion for training supervised sequence-to-sequence (seq2seq) models. It enables learning the relations between input and output sequences, termed alignments, by marginalizing over perfect alignments (that yield the ground truth), at the expense of imperfect alignments. This binary differentiation of perfect and imperfect alignments falls short of capturing other essential alignment properties that hold significance in other real-world applications. Here we propose $\textit{Align With Purpose}$, a $\textbf{general Plug-and-Play framework}$ for enhancing a desired property in models trained with the CTC criterion. We do that by complementing the CTC with an additional loss term that prioritizes alignments according to a desired property. Our method does not require any intervention in the CTC loss function, enables easy optimization of a variety of properties, and allows differentiation between both perfect and imperfect al
    
[^207]: 加速GNN框架中的采样和聚合操作：利用GPU发起直接存储访问

    Accelerating Sampling and Aggregation Operations in GNN Frameworks with GPU Initiated Direct Storage Accesses. (arXiv:2306.16384v1 [cs.DC])

    [http://arxiv.org/abs/2306.16384](http://arxiv.org/abs/2306.16384)

    本论文提出了一种通过利用GPU发起直接存储访问来加速GNN框架中的采样和聚合操作的方法，解决了在训练大规模图上时CPU无法充分利用GPU资源的问题。

    

    图神经网络（GNNs）正在成为学习图结构数据和进行复杂推理任务的一个强大工具，适用于各个应用领域。尽管已经证明GNNs在中等规模的图上具有有效性，但在大规模图上训练仍然面临着数据访问和数据移动方法的不足。现有的GNN训练框架使用CPU进行图采样和特征聚合，而模型权重的训练和更新则由GPU执行。然而，我们深入分析发现CPU无法实现所需的吞吐量以充分利用昂贵的GPU资源。此外，当图和其嵌入不能适应CPU内存时，操作系统引入的开销，如处理页面错误，会成为关键路径的瓶颈。为了解决这些问题，我们提出了GPU发起的直接存储访问方法。

    Graph Neural Networks (GNNs) are emerging as a powerful tool for learning from graph-structured data and performing sophisticated inference tasks in various application domains. Although GNNs have been shown to be effective on modest-sized graphs, training them on large-scale graphs remains a significant challenge due to lack of efficient data access and data movement methods. Existing frameworks for training GNNs use CPUs for graph sampling and feature aggregation, while the training and updating of model weights are executed on GPUs. However, our in-depth profiling shows the CPUs cannot achieve the throughput required to saturate GNN model training throughput, causing gross under-utilization of expensive GPU resources. Furthermore, when the graph and its embeddings do not fit in the CPU memory, the overhead introduced by the operating system, say for handling page-faults, comes in the critical path of execution.  To address these issues, we propose the GPU Initiated Direct Storage Ac
    
[^208]: 通过强化学习的编辑生成安全关键场景

    Safety-Critical Scenario Generation Via Reinforcement Learning Based Editing. (arXiv:2306.14131v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.14131](http://arxiv.org/abs/2306.14131)

    提出了一种基于强化学习的场景生成方法，通过顺序编辑生成安全关键场景，克服了维度挑战，并生成了质量更高的场景。

    

    生成安全关键场景对于测试和验证自动驾驶汽车的安全性至关重要。传统的优化技术在维度诅咒和搜索空间的限制上存在问题。为了解决这些挑战，我们提出了一种基于深度强化学习的方法，通过顺序编辑生成场景，例如添加新的代理或修改现有代理的轨迹。我们的框架采用了包含风险和可行性目标的奖励函数。可行性目标利用生成模型，如变分自动编码器，从训练数据集中学习生成参数的可能性；它惩罚生成不太可能的场景。我们的方法克服了维度挑战，并探索了广泛的安全关键场景。我们的评估表明，与以前的方法相比，所提出的方法生成了质量更高的安全关键场景。

    Generating safety-critical scenarios is essential for testing and verifying the safety of autonomous vehicles. Traditional optimization techniques suffer from the curse of dimensionality and limit the search space to fixed parameter spaces. To address these challenges, we propose a deep reinforcement learning approach that generates scenarios by sequential editing, such as adding new agents or modifying the trajectories of the existing agents. Our framework employs a reward function consisting of both risk and plausibility objectives. The plausibility objective leverages generative models, such as a variational autoencoder, to learn the likelihood of the generated parameters from the training datasets; It penalizes the generation of unlikely scenarios. Our approach overcomes the dimensionality challenge and explores a wide range of safety-critical scenarios. Our evaluation demonstrates that the proposed method generates safety-critical scenarios of higher quality compared with previous
    
[^209]: EquiformerV2: 改进的等变Transformer，用于扩展到更高次表示

    EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations. (arXiv:2306.12059v1 [cs.LG])

    [http://arxiv.org/abs/2306.12059](http://arxiv.org/abs/2306.12059)

    本文提出了EquiformerV2，通过使用新的卷积类型和架构改进，扩展了等变Transformer到更高的等变表示，在处理大型数据集时表现更好，能量和力的表现也得到了提高，计算效率也得到了提升。

    

    等变Transformer（例如Equiformer）已经证明了将Transformer应用于3D原子系统领域的功效。但是，由于计算复杂性，它们仍然局限于小数次等变表示。在本文中，我们调查了这些架构是否能够很好地扩展到更高的次数。从Equiformer开始，我们首先用eSCN卷积替换了$SO(3)$卷积，以有效地合并更高次的张量。然后，为了更好地利用更高次的能力，我们提出了三个架构改进——注意力重标准化、可分离的$S^2$激活和可分离层归一化。将这一切放在一起，我们提出了EquiformerV2，在大型OC20数据集上的表现优于以前的最先进方法，在力上提高了最多$12\%$，能量上提高了$4\%$，提供更好的速度-准确性权衡，并且在计算吸附能所需的DFT计算量方面缩减了2倍。

    Equivariant Transformers such as Equiformer have demonstrated the efficacy of applying Transformers to the domain of 3D atomistic systems. However, they are still limited to small degrees of equivariant representations due to their computational complexity. In this paper, we investigate whether these architectures can scale well to higher degrees. Starting from Equiformer, we first replace $SO(3)$ convolutions with eSCN convolutions to efficiently incorporate higher-degree tensors. Then, to better leverage the power of higher degrees, we propose three architectural improvements -- attention re-normalization, separable $S^2$ activation and separable layer normalization. Putting this all together, we propose EquiformerV2, which outperforms previous state-of-the-art methods on the large-scale OC20 dataset by up to $12\%$ on forces, $4\%$ on energies, offers better speed-accuracy trade-offs, and $2\times$ reduction in DFT calculations needed for computing adsorption energies.
    
[^210]: 面向扩散式生成模型的非渐进快速收敛方法

    Towards Faster Non-Asymptotic Convergence for Diffusion-Based Generative Models. (arXiv:2306.09251v1 [stat.ML])

    [http://arxiv.org/abs/2306.09251](http://arxiv.org/abs/2306.09251)

    该论文针对扩散生成模型设计了非渐进理论，提出了针对两种主流采样器的新的收敛速度，提高了总步数与收敛速度的比例。

    

    扩散模型通过学习反转马尔可夫扩散过程将噪音转化为新数据实例，在当代生成建模领域中已成为基石。虽然它们的实用性现在已被广泛认可，但其理论基础仍然不够成熟。在这项工作中，我们开发了一套非渐进理论，以理解离散时间下扩散模型的数据生成过程，假设可以获得（Stein）得分函数的可靠估计。针对一种流行的确定性采样器（基于概率流ODE），我们建立了一个与 $T$（总步数）成比例的收敛速度，改进了过去的结果；对于另一种主流的随机采样器（即一种去噪扩散概率模型（DDPM）），我们导出了一个与 $1/\sqrt{T}$ 成比例的收敛速度，与最先进的理论相匹配。我们的理论对目标数据分布只作出最小的假设（例如，没有平滑）。

    Diffusion models, which convert noise into new data instances by learning to reverse a Markov diffusion process, have become a cornerstone in contemporary generative modeling. While their practical power has now been widely recognized, the theoretical underpinnings remain far from mature. In this work, we develop a suite of non-asymptotic theory towards understanding the data generation process of diffusion models in discrete time, assuming access to reliable estimates of the (Stein) score functions. For a popular deterministic sampler (based on the probability flow ODE), we establish a convergence rate proportional to $1/T$ (with $T$ the total number of steps), improving upon past results; for another mainstream stochastic sampler (i.e., a type of the denoising diffusion probabilistic model (DDPM)), we derive a convergence rate proportional to $1/\sqrt{T}$, matching the state-of-the-art theory. Our theory imposes only minimal assumptions on the target data distribution (e.g., no smoot
    
[^211]: 用函数逼近解决强化学习中重尾奖励问题的极小最大化算法和实例相关遗憾度量

    Tackling Heavy-Tailed Rewards in Reinforcement Learning with Function Approximation: Minimax Optimal and Instance-Dependent Regret Bounds. (arXiv:2306.06836v1 [cs.LG])

    [http://arxiv.org/abs/2306.06836](http://arxiv.org/abs/2306.06836)

    本文解决了强化学习中当奖励呈“重尾”分布时的问题，提出了第一种处理这种情况的实例相关算法，并得到了极小最大化的遗憾界。

    

    虽然有许多工作都专注于为有界奖励的强化学习设计有效算法，但当奖励呈现“重尾”分布时——即存在某个 $\epsilon\in(0,1]$ 使得仅有有限的$(1+\epsilon)$-阶矩——是否存在对大状态-动作空间进行采样或时效性算法仍然是一个未解决的问题。 在本文中，我们解决了具有线性函数逼近的 RL 中的这种奖励机制的挑战。我们首先为重尾线性赌臂设计了一种算法——\textsc{Heavy-OFUL}，其实现了一种实例相关的 $T$-round 遗憾度量，为 $\tilde{O}\big(d T^{\frac{1-\epsilon}{2(1+\epsilon)}} \sqrt{\sum_{t=1}^T \nu_t^2} + d T^{\frac{1-\epsilon}{2(1+\epsilon)}}\big)$，这是这种类型的\emph{第一篇}文章。$\nu_t^{1+\epsilon}$是第 $t$ 轮奖励的 $(1+\epsilon)$-阶中心矩。我们进一步证明了在应用于 st 的最坏情况时，上述界是极小值的最优解。

    While numerous works have focused on devising efficient algorithms for reinforcement learning (RL) with uniformly bounded rewards, it remains an open question whether sample or time-efficient algorithms for RL with large state-action space exist when the rewards are \emph{heavy-tailed}, i.e., with only finite $(1+\epsilon)$-th moments for some $\epsilon\in(0,1]$. In this work, we address the challenge of such rewards in RL with linear function approximation. We first design an algorithm, \textsc{Heavy-OFUL}, for heavy-tailed linear bandits, achieving an \emph{instance-dependent} $T$-round regret of $\tilde{O}\big(d T^{\frac{1-\epsilon}{2(1+\epsilon)}} \sqrt{\sum_{t=1}^T \nu_t^2} + d T^{\frac{1-\epsilon}{2(1+\epsilon)}}\big)$, the \emph{first} of this kind. Here, $d$ is the feature dimension, and $\nu_t^{1+\epsilon}$ is the $(1+\epsilon)$-th central moment of the reward at the $t$-th round. We further show the above bound is minimax optimal when applied to the worst-case instances in st
    
[^212]: DOCTOR：基于可穿戴医疗传感器的多疾病检测持续学习框架

    DOCTOR: A Multi-Disease Detection Continual Learning Framework Based on Wearable Medical Sensors. (arXiv:2305.05738v1 [cs.LG])

    [http://arxiv.org/abs/2305.05738](http://arxiv.org/abs/2305.05738)

    DOCTOR是一种基于可穿戴医疗传感器的多疾病检测持续学习框架，采用了多头深度神经网络和Exemplar-replay风格的CL算法。它可以不断地学习新任务，并在内存使用、电池消耗和检测复杂度方面优于传统的ML驱动疾病检测方法。

    

    现代机器学习（ML）和边缘设备中的可穿戴医疗传感器（WMS）的进步使得智能医疗的ML驱动疾病检测成为可能。传统的ML驱动疾病检测方法依赖于为每种疾病和相应的WMS数据定制个别模型。然而，这种方法缺乏对分布变化和新任务分类的适应性。同时，为了检测每个新疾病，需要从头开始重新构建和训练模型。针对这些挑战，我们提出了基于WMS的多疾病检测持续学习框架DOCTOR。它采用了多头深度神经网络（DNN）和一种Exemplar-replay风格的CL算法。CL算法使得框架能够不断地学习新任务，其中涉及不同的数据分布、分类类别和疾病检测任务。DOCTOR在使用来自实际WMS的公共数据集进行四种常见疾病检测方面取得了最先进的性能。同时，在内存使用、电池消耗和检测复杂度方面，DOCTOR也优于基线方法。

    Modern advances in machine learning (ML) and wearable medical sensors (WMSs) in edge devices have enabled ML-driven disease detection for smart healthcare. Conventional ML-driven disease detection methods rely on customizing individual models for each disease and its corresponding WMS data. However, such methods lack adaptability to distribution shifts and new task classification classes. Also, they need to be rearchitected and retrained from scratch for each new disease. Moreover, installing multiple ML models in an edge device consumes excessive memory, drains the battery faster, and complicates the detection process. To address these challenges, we propose DOCTOR, a multi-disease detection continual learning (CL) framework based on WMSs. It employs a multi-headed deep neural network (DNN) and an exemplar-replay-style CL algorithm. The CL algorithm enables the framework to continually learn new missions where different data distributions, classification classes, and disease detection
    
[^213]: 存在对称性和状态抽象的政策梯度方法

    Policy Gradient Methods in the Presence of Symmetries and State Abstractions. (arXiv:2305.05666v1 [cs.LG])

    [http://arxiv.org/abs/2305.05666](http://arxiv.org/abs/2305.05666)

    本文研究了在连续控制环境中的抽象，提出了一种策略梯度定理，允许利用环境的近似对称性进行策略优化，并提出了一系列演员-评论家算法进行策略和MDP同态映射的学习，最后展示了算法在连续对称性环境和视觉控制任务中的有效性。

    

    针对高维度和复杂问题，强化学习依靠抽象来提高效率和泛化性能。本文研究了在连续控制环境中的抽象，并将MDP同态的定义扩展到连续状态和动作空间的情况。我们针对抽象MDP的随机和确定性策略导出了一种策略梯度定理。我们的策略梯度结果允许利用环境的近似对称性进行策略优化。基于这些定理，我们提出了一系列演员-评论家算法，这些算法能够同时学习策略和MDP同态映射，使用松散双仿射度量。最后，我们引入了一系列具有连续对称性的环境，以进一步展示我们的算法在存在这些对称性的情况下进行动作抽象的能力。我们在这些环境以及具有挑战性的视觉控制任务中展示了我们方法的有效性。

    Reinforcement learning on high-dimensional and complex problems relies on abstraction for improved efficiency and generalization. In this paper, we study abstraction in the continuous-control setting, and extend the definition of MDP homomorphisms to the setting of continuous state and action spaces. We derive a policy gradient theorem on the abstract MDP for both stochastic and deterministic policies. Our policy gradient results allow for leveraging approximate symmetries of the environment for policy optimization. Based on these theorems, we propose a family of actor-critic algorithms that are able to learn the policy and the MDP homomorphism map simultaneously, using the lax bisimulation metric. Finally, we introduce a series of environments with continuous symmetries to further demonstrate the ability of our algorithm for action abstraction in the presence of such symmetries. We demonstrate the effectiveness of our method on our environments, as well as on challenging visual contro
    
[^214]: 通过数据生成和参数畸变实现隐私保护联邦学习的接近最优效用

    Towards Achieving Near-optimal Utility for Privacy-Preserving Federated Learning via Data Generation and Parameter Distortion. (arXiv:2305.04288v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.04288](http://arxiv.org/abs/2305.04288)

    本论文提出了一种用数据生成和参数畸变实现隐私保护联邦学习接近最优效用的上限方法，其中通过降低方差和模型参数差异来衡量效用损失。

    

    联邦学习（FL）使参与方能够协作构建具有提高效用的全局模型，而不泄露私有数据信息。必须采用适当的保护机制来满足保护隐私和维护高模型效用的要求。目前采用的保护机制的本质，包括“随机化机制”和“压缩机制”，是通过畸变模型参数来保护隐私。我们通过原始模型参数和畸变模型参数之间的差距来衡量效用。我们想要确定在什么普遍条件下，通过数据生成和参数畸变，隐私保护的联邦学习可以实现接近最优的效用。为了提供接近最优效用的途径，我们提出了一个效用损失的上限，用两个主要项称为降低方差和模型参数差异来衡量。

    Federated learning (FL) enables participating parties to collaboratively build a global model with boosted utility without disclosing private data information. Appropriate protection mechanisms have to be adopted to fulfill the requirements in preserving \textit{privacy} and maintaining high model \textit{utility}. The nature of the widely-adopted protection mechanisms including \textit{Randomization Mechanism} and \textit{Compression Mechanism} is to protect privacy via distorting model parameter. We measure the utility via the gap between the original model parameter and the distorted model parameter. We want to identify under what general conditions privacy-preserving federated learning can achieve near-optimal utility via data generation and parameter distortion. To provide an avenue for achieving near-optimal utility, we present an upper bound for utility loss, which is measured using two main terms called variance-reduction and model parameter discrepancy separately. Our analysis
    
[^215]: 约束域的扩散模型

    Diffusion Models for Constrained Domains. (arXiv:2304.05364v1 [cs.LG])

    [http://arxiv.org/abs/2304.05364](http://arxiv.org/abs/2304.05364)

    本研究提出了两种方法来创建约束域的降噪扩散模型。第一种方法基于不等式约束诱导的对数障碍度量，第二种方法基于反射布朗运动。这些方法将扩散模型的应用范围扩展到了机器人和蛋白设计等领域。

    

    降噪扩散模型是新近涌现的一种生成模型，它在无条件图像生成和语音生成等众多领域实现了最先进的成果。它们由破坏数据的加噪过程和定义为加噪扩散的时间反演的后向阶段组成。以这些成功为基础，扩散模型最近扩展到了黎曼流形设置。然而，这些黎曼扩散模型要求在所有时间上定义测地线。虽然该设置包括许多重要应用，但不包括由不等式约束集定义的流形，这在许多科学领域，如机器人和蛋白设计中是普遍存在的。在本文中，我们介绍了两种方法来弥合这个差距。首先，我们设计了一个基于不等式约束诱导的对数障碍度量的加噪过程。其次，我们介绍了一种基于反射布朗运动的加噪过程。由于现有的扩散模型不能直接应用于约束域，因此本文提出了两种方法来创建约束域的降噪扩散模型。

    Denoising diffusion models are a recent class of generative models which achieve state-of-the-art results in many domains such as unconditional image generation and text-to-speech tasks. They consist of a noising process destroying the data and a backward stage defined as the time-reversal of the noising diffusion. Building on their success, diffusion models have recently been extended to the Riemannian manifold setting. Yet, these Riemannian diffusion models require geodesics to be defined for all times. While this setting encompasses many important applications, it does not include manifolds defined via a set of inequality constraints, which are ubiquitous in many scientific domains such as robotics and protein design. In this work, we introduce two methods to bridge this gap. First, we design a noising process based on the logarithmic barrier metric induced by the inequality constraints. Second, we introduce a noising process based on the reflected Brownian motion. As existing diffu
    
[^216]: 利用气象引导的视频预测，从卫星视角预测局部气象对植被的影响

    Forecasting localized weather impacts on vegetation as seen from space with meteo-guided video prediction. (arXiv:2303.16198v1 [cs.CV])

    [http://arxiv.org/abs/2303.16198](http://arxiv.org/abs/2303.16198)

    本文提出一种利用气象引导的视频预测方法，从卫星视角预测欧洲地区植被对天气的响应，建立了相应的模型，并证明了该方法在卫星图像预测中具有优越性能。此外，该模型还可用于下游任务，如碳监测的总初级生产力的推断。

    

    本文提出了一种新的方法，通过Sentinel 2卫星测量欧洲地区的天气来建立植被对天气的响应模型。现有的卫星图像预测方法重点关注多光谱图像的真实感，而衍生的植被动态尚未得到太多关注。我们通过将气象指导应用于最先进的视频预测方法，扩展了空间和时间上下文。我们通过引入一个学习的云层掩模和适当的评估方案，扩展了EarthNet2021数据集以适合植被建模。定量和定性实验证明了我们的方法在包括领先的卫星图像预测方法在内的各种基线方法中具有卓越的性能。此外，我们展示了如何将我们建立的植被动态应用于下游任务：用于碳监测的总初级生产力的推断。据我们所知，这是第一次建立基于气象引导的视频预测模型。

    We present a novel approach for modeling vegetation response to weather in Europe as measured by the Sentinel 2 satellite. Existing satellite imagery forecasting approaches focus on photorealistic quality of the multispectral images, while derived vegetation dynamics have not yet received as much attention. We leverage both spatial and temporal context by extending state-of-the-art video prediction methods with weather guidance. We extend the EarthNet2021 dataset to be suitable for vegetation modeling by introducing a learned cloud mask and an appropriate evaluation scheme. Qualitative and quantitative experiments demonstrate superior performance of our approach over a wide variety of baseline methods, including leading approaches to satellite imagery forecasting. Additionally, we show how our modeled vegetation dynamics can be leveraged in a downstream task: inferring gross primary productivity for carbon monitoring. To the best of our knowledge, this work presents the first models fo
    
[^217]: Uni-RXN: 一种统一的框架，弥合化学反应Pretraining和条件分子生成之间的差距

    Uni-RXN: A Unified Framework Bridging the Gap between Chemical Reaction Pretraining and Conditional Molecule Generation. (arXiv:2303.06965v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06965](http://arxiv.org/abs/2303.06965)

    本文提出了Uni-RXN框架，在化学反应Pretraining和分子生成任务中都取得了最先进的结果。通过具备化学知识，克服了当前分子生成模型仅依赖少量反应模板的限制，生成质量高、可合成的药物类分子结构。

    

    化学反应是药物设计和有机化学研究的基本构建模块。近年来，对于一个可以有效捕捉化学反应基本规则的大规模深度学习框架的需求不断增长。在本文中，我们提出了一个统一的框架，解决了反应表示学习和分子生成任务，允许更整体的方法。受有机化学机制的启发，我们开发了一种新的预训练框架，使我们能够将归纳偏见纳入模型。我们的框架在具有挑战性的下游任务上取得了最先进的结果。通过具备化学知识，该框架可应用于基于反应的生成模型，克服了当前分子生成模型仅依赖少量反应模板的限制。在广泛的实验中，我们的模型生成了高质量的可合成药物类分子结构。

    Chemical reactions are the fundamental building blocks of drug design and organic chemistry research. In recent years, there has been a growing need for a large-scale deep-learning framework that can efficiently capture the basic rules of chemical reactions. In this paper, we have proposed a unified framework that addresses both the reaction representation learning and molecule generation tasks, which allows for a more holistic approach. Inspired by the organic chemistry mechanism, we develop a novel pretraining framework that enables us to incorporate inductive biases into the model. Our framework achieves state-of-the-art results on challenging downstream tasks. By possessing chemical knowledge, this framework can be applied to reaction-based generative models, overcoming the limitations of current molecule generation models that rely on a small number of reaction templates. In the extensive experiments, our model generates synthesizable drug-like structures of high quality. Overall,
    
[^218]: FOSI：混合一阶和二阶优化

    FOSI: Hybrid First and Second Order Optimization. (arXiv:2302.08484v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08484](http://arxiv.org/abs/2302.08484)

    FOSI是一种元算法，它在优化过程中有效地加入二阶信息以提高任何一阶优化器的性能，并可改善一类优化器的条件数

    

    尽管二阶优化方法非常有效，但在高维空间中计算曲率的困难导致在机器学习中流行的方法（如SGD和Adam）仅使用一阶信息。我们提出了FOSI，一种新颖的元算法，在优化过程中有效地加入二阶信息以提高任何一阶优化器的性能。在每个迭代中，FOSI隐含地将函数分为两个定义在正交子空间上的二次函数，然后使用二阶方法最小化一个函数，使用基本优化器最小化另一个函数。我们证明FOSI收敛，并进一步表明它在一类优化器中改善了条件数。我们的实证评估证明，对于音频分类，迁移学习和物体分类等几个深度神经网络训练任务，将FOSI应用于GD，Heavy-Ball和Adam等算法可以提高收敛速度和优化时间。

    Though second-order optimization methods are highly effective, popular approaches in machine learning such as SGD and Adam use only first-order information due to the difficulty of computing curvature in high dimensions. We present FOSI, a novel meta-algorithm that improves the performance of any first-order optimizer by efficiently incorporating second-order information during the optimization process. In each iteration, FOSI implicitly splits the function into two quadratic functions defined on orthogonal subspaces, then uses a second-order method to minimize the first, and the base optimizer to minimize the other. We prove FOSI converges and further show it improves the condition number for a large family of optimizers. Our empirical evaluation demonstrates that FOSI improves the convergence rate and optimization time of GD, Heavy-Ball, and Adam when applied to several deep neural networks training tasks such as audio classification, transfer learning, and object classification, as 
    
[^219]: 基于层次数据有效表示学习的RNA三级结构设计

    Hierarchical Data-efficient Representation Learning for Tertiary Structure-based RNA Design. (arXiv:2301.10774v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2301.10774](http://arxiv.org/abs/2301.10774)

    本研究提出了一个基于层次数据有效表示学习的RNA设计流程，通过构建大型数据集并设计全面的结构建模方法，实现了更高效的RNA序列设计。

    

    尽管人工智能已在揭示生物大分子的一级序列与三级结构之间的关系方面取得了显着进展，但基于特定三级结构设计RNA序列仍然具有挑战性。虽然蛋白质设计中的现有方法已经彻底探索了蛋白质中结构到序列的依赖性，但RNA设计仍面临结构复杂性和数据稀缺性的困难。与此同时，虽然RNA与蛋白质共享类似的结构组分，但直接将蛋白质设计方法移植到RNA设计中却无法取得令人满意的结果。本研究旨在系统构建数据驱动的RNA设计流程。我们构建了一个大型、精心策划的基准数据集，并设计了一个全面的结构建模方法来表示复杂的RNA三级结构。更重要的是，我们提出了一个层次数据有效表示学习框架，学习结构表示的多个层次特征，以实现更高效的RNA序列设计。

    While artificial intelligence has made remarkable strides in revealing the relationship between biological macromolecules' primary sequence and tertiary structure, designing RNA sequences based on specified tertiary structures remains challenging. Though existing approaches in protein design have thoroughly explored structure-to-sequence dependencies in proteins, RNA design still confronts difficulties due to structural complexity and data scarcity. Adding to the problem, direct transplantation of protein design methodologies into RNA design fails to achieve satisfactory outcomes although sharing similar structural components. In this study, we aim to systematically construct a data-driven RNA design pipeline. We crafted a large, well-curated benchmark dataset and designed a comprehensive structural modeling approach to represent the complex RNA tertiary structure. More importantly, we proposed a hierarchical data-efficient representation learning framework that learns structural repre
    
[^220]: 分布偏移的标签对齐正则化

    Label Alignment Regularization for Distribution Shift. (arXiv:2211.14960v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14960](http://arxiv.org/abs/2211.14960)

    这篇论文提出了一种用于无监督领域自适应的正则化方法，通过鼓励目标域中的预测与其前几个奇异向量对齐来实现。与传统方法不同的是，这个方法通过正则化分类器与无监督目标数据对齐，而不是正则化表示。通过消除对最优联合风险假设的依赖，该方法展示了很好的效果。

    

    最近的研究强调了监督学习中的标签对齐属性（LAP），即数据集中所有标签的向量大部分在数据矩阵的前几个奇异向量的张成空间内。受到这一观察的启发，我们提出了一种无监督领域自适应的正则化方法，鼓励目标域中的预测与其前几个奇异向量对齐。与传统的领域适应方法专注于正则化表示不同，我们相反，通过在源域和目标域中使用LAP，用正则化分类器与无监督目标数据对齐。理论分析表明，在一定的假设下，我们的解决方案位于目标域数据的前几个右奇异向量的张成空间内，并与最优解对齐。通过消除经典领域适应理论中常见的最优联合风险假设的依赖，我们展示了该方法的有效性。

    Recent work has highlighted the label alignment property (LAP) in supervised learning, where the vector of all labels in the dataset is mostly in the span of the top few singular vectors of the data matrix. Drawing inspiration from this observation, we propose a regularization method for unsupervised domain adaptation that encourages alignment between the predictions in the target domain and its top singular vectors. Unlike conventional domain adaptation approaches that focus on regularizing representations, we instead regularize the classifier to align with the unsupervised target data, guided by the LAP in both the source and target domains. Theoretical analysis demonstrates that, under certain assumptions, our solution resides within the span of the top right singular vectors of the target domain data and aligns with the optimal solution. By removing the reliance on the commonly used optimal joint risk assumption found in classic domain adaptation theory, we showcase the effectivene
    

