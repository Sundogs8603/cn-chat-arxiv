# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Putting People in Their Place: Affordance-Aware Human Insertion into Scenes.](http://arxiv.org/abs/2304.14406) | 该论文提出了一种方法，在场景中插入现实姿态的人，以及合成能够在场景中与人类进行自然交互的场景。 |
| [^2] | [Make It So: Steering StyleGAN for Any Image Inversion and Editing.](http://arxiv.org/abs/2304.14403) | 本文提出了一种新颖的GAN反演方法Make It So，在噪声空间中操作使得保留了编辑能力，即使是在域外图像方面，比现有方法更准确，反演精度提高五倍，并且对于复杂的室内场景，实现了十倍更好的编辑质量。 |
| [^3] | [Maximizing Model Generalization for Manufacturing with Self-Supervised Learning and Federated Learning.](http://arxiv.org/abs/2304.14398) | 本研究提出了一种利用自我监督学习和联邦学习提高制造业模型的泛化能力，在处理未标记的和有限的状况监测数据以及领域移位时具有较好的效果，同时通过联邦学习保护了数据隐私和提高了计算效率。 |
| [^4] | [Energy-based Models as Zero-Shot Planners for Compositional Scene Rearrangement.](http://arxiv.org/abs/2304.14391) | 本文提出一种基于能量模型的零样本场景重新排列规划器，通过语言指导的空间概念来实现长指令以及在训练时从未见过的空间概念组合。本文的模型在指令导向操作基准测试以及组合指令基准测试中表现良好，优于基于语言表达的最先进方法，并且可以成功地解决之前从未见过的复杂指令和场景。 |
| [^5] | [Resampling Gradients Vanish in Differentiable Sequential Monte Carlo Samplers.](http://arxiv.org/abs/2304.14390) | 本文提出了一种扩展可微分AIS的方法，通过引入类似于顺序蒙特卡洛的重采样步骤来避免粒子滤波中的梯度方差问题。 |
| [^6] | [Dynamic Pricing and Learning with Bayesian Persuasion.](http://arxiv.org/abs/2304.14385) | 本研究提出了一种计算有效的在线算法，在没有先验知识的情况下，自适应学习最优定价和广告策略，达到次线性后悔。 |
| [^7] | [Analogy-Forming Transformers for Few-Shot 3D Parsing.](http://arxiv.org/abs/2304.14382) | "模拟网络"模型在3D物体场景分割中采用类比推理，通过在内存中检索相关场景并预测类似结构进行分割，能够在一发、少发或多发学习中得出相似的解析，与最新的3D分割变压器模型相竞争。 |
| [^8] | [Functional Diffusion Maps.](http://arxiv.org/abs/2304.14378) | 本研究关注一种非线性流形学习方法：扩散映射。本文阐述如何将这种方法应用于功能数据，并将其与功能主成分分析进行比较。 |
| [^9] | [Pseudo-Hamiltonian neural networks for learning partial differential equations.](http://arxiv.org/abs/2304.14374) | 本文介绍了一种新方法伪哈密顿神经网络(PHNN)，可以用于学习偏微分方程。相比基线模型，PHNN表现更为优越，模型可应用于去除或改变外力情况并可分别得到三个不同物理解释的部分。 |
| [^10] | [Neural Field Conditioning Strategies for 2D Semantic Segmentation.](http://arxiv.org/abs/2304.14371) | 本文研究了神经场作为二维语义分割解码器，提出了三种调节方法，结果表明通过交叉注意力进行调节的方法最佳，与基于CNN的方法相当竞争。 |
| [^11] | [Learning Neural Constitutive Laws From Motion Observations for Generalizable PDE Dynamics.](http://arxiv.org/abs/2304.14369) | 本文提出了一种混合神经网络和偏微分方程的方法，用于从运动观测中学习可推广的PDE动力学，并介绍了一种新框架"神经本构法"，该框架利用了一种严格保证标准本构先验的网络架构，以此来学习本构模型。 |
| [^12] | [CONSCENDI: A Contrastive and Scenario-Guided Distillation Approach to Guardrail Models for Virtual Assistants.](http://arxiv.org/abs/2304.14364) | 本文提出了一种名为CONSCENDI的蒸馏方法，用于构建防护栏模型，以监控任务型虚拟助手的输出。关键方法包括场景增强生成和对比训练样例。这种方法产生了一组多样化的违反规则的对话训练集，并且可以更好地检测代理的输出是否符合设计者指定的规则。 |
| [^13] | [The Dark Side of ChatGPT: Legal and Ethical Challenges from Stochastic Parrots and Hallucination.](http://arxiv.org/abs/2304.14347) | ChatGPT带来的大语言模型(LLMs)虽然有很多优势，但是随机鹦鹉和幻觉等新的法律和伦理风险也随之而来。欧洲AI监管范式需要进一步发展以减轻这些风险。 |
| [^14] | [Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction: A Unified Library and Performance Benchmark.](http://arxiv.org/abs/2304.14343) | 本研究提出了一种称为原子文件的统一空间时间数据存储格式，开发了一个名为LibCity的开源库，重新构建了65个空间时间预测模型，并收集了55个空间时间数据集。同时还提出了城市时空预测模型的性能基准，为这一领域提供了一个可靠的评估工具。 |
| [^15] | [MarsEclipse at SemEval-2023 Task 3: Multi-Lingual and Multi-Label Framing Detection with Contrastive Learning.](http://arxiv.org/abs/2304.14339) | 本文描述了一种在多语言设置下使用多标签对比损失来微调大型预训练语言模型的方法，取得了在五种语言上最好的结果。 |
| [^16] | [Idioms, Probing and Dangerous Things: Towards Structural Probing for Idiomaticity in Vector Space.](http://arxiv.org/abs/2304.14333) | 本文利用结构探测方法研究了成语在静态和上下文嵌入中的编码情况，并发现仍未确定成语性是否在向量范数中编码，提出了改进数据集以进行探测分析的方向。 |
| [^17] | [On the Generalization Error of Meta Learning for the Gibbs Algorithm.](http://arxiv.org/abs/2304.14332) | 本文通过Gibbs算法来分析元学习算法的泛化能力，提供了精确的元泛化误差刻画和新的泛化误差上界。 |
| [^18] | [Learning to Extrapolate: A Transductive Approach.](http://arxiv.org/abs/2304.14329) | 该论文提出了一种解决在支持外进行推广的问题的传导方法，该方法通过“传导”重新参数化将支持外的外推问题转换为支持内组合泛化问题的问题，从而让机器学习系统保留过度参数化函数逼近器的能力，并能够在某些条件下进行外推。 |
| [^19] | [A Best-of-Both-Worlds Algorithm for Constrained MDPs with Long-Term Constraints.](http://arxiv.org/abs/2304.14326) | 本文提出了一种针对约束MDPs的双赢算法，能够处理奖励和约束随机或敌对的情况。 |
| [^20] | [A Measurement of the Kuiper Belt's Mean Plane From Objects Classified By Machine Learning.](http://arxiv.org/abs/2304.14312) | 本论文利用机器学习对库珀带内天体分类，测量了库珀带的平面。结果表明，非共振库珀带和古典库珀带与太阳系不变平面非常接近，但有区别，置信度大于99.7%。 |
| [^21] | [A Method for Classifying Snow Using Ski-Mounted Strain Sensors.](http://arxiv.org/abs/2304.14307) | 本文介绍了一种利用滑雪板装置应变传感器对雪进行分类的方法，该方法可以通过对每个10秒的轨迹段进行数据处理，独立于滑雪风格，准确地分配三个定性标签。 |
| [^22] | [Learning Absorption Rates in Glucose-Insulin Dynamics from Meal Covariates.](http://arxiv.org/abs/2304.14300) | 本文提出了一种从葡萄糖 - 胰岛素数据和餐饮协变量中学习宏量营养成分影响的方法，使用神经网络预测个人的葡萄糖吸收率，并在葡萄糖动力学的微分方程中实现端到端训练，从而获得更好的预测效果。 |
| [^23] | [Controlled Text Generation with Natural Language Instructions.](http://arxiv.org/abs/2304.14293) | InstructCTG是一个可以通过自然语言描述和演示来控制文本生成并满足不同约束条件的框架，它有效地解决了现有搜索或得分方法所存在的问题。 |
| [^24] | [Distinguishing a planetary transit from false positives: a Transformer-based classification for planetary transit signals.](http://arxiv.org/abs/2304.14283) | 本文介绍了基于Transformer架构的新方法，用于高效准确地区分行星凌和误判。采用一种新的预处理输入数据的方法，使用正弦函数来保留信号的周期性。在模拟数据的评估中，该方法优于已有的基于CNN的方法。 |
| [^25] | [AI, write an essay for me: A large-scale comparison of human-written versus ChatGPT-generated essays.](http://arxiv.org/abs/2304.14276) | ChatGPT生成的文章质量比人类写作的文章更高，写作风格更流畅，语法和拼写错误更少。 |
| [^26] | [What's in a Name? Evaluating Assembly-Part Semantic Knowledge in Language Models through User-Provided Names in CAD Files.](http://arxiv.org/abs/2304.14275) | 本文提出，计算机辅助设计（CAD）软件中的自然语言名称是部件关联的宝贵来源，大型语言模型（LLM）提供了一种处理这种数据的有用的领域专业知识。通过自然语言名称的预训练模型可在三个自监督任务上表现出色，微调还可以提高所有任务的性能，提高了文本数据的价值。此外，提出了手动注释的新数据集 CAD-120，其中包含 120 个装配，并提供了语义关系注释。 |
| [^27] | [When Do Graph Neural Networks Help with Node Classification: Investigating the Homophily Principle on Node Distinguishability.](http://arxiv.org/abs/2304.14274) | 同源性原则不一定是影响图神经网络优越性的唯一原因；本文提出Contextual Stochastic Block Model for Homophily (CSBM-H)以深入研究同源性对节点可区分性的影响。 |
| [^28] | [A Survey on Approximate Edge AI for Energy Efficient Autonomous Driving Services.](http://arxiv.org/abs/2304.14271) | 这篇论文概述了自主驾驶服务中的传感和数据处理现状，及其对能源和环境带来的挑战，进一步调查和比较了如何应用和整合现有技术创新以实现能源效率。 |
| [^29] | [Variational Bayes Made Easy.](http://arxiv.org/abs/2304.14251) | 该论文提出了一个三步骤方法，简化了变分贝叶斯近似推断方法的推导过程。 |
| [^30] | [On Manifold Learning in Plato's Cave: Remarks on Manifold Learning and Physical Phenomena.](http://arxiv.org/abs/2304.14248) | 本文通过一个警示故事阐释了分析数据时，测量几何和底层现象几何差异带来的问题，以及这种差异在某些情况下如何导致对一个修正过的问题给出错误答案。这些问题适用于降维和无监督学习领域。 |
| [^31] | [TorchBench: Benchmarking PyTorch with High API Surface Coverage.](http://arxiv.org/abs/2304.14226) | TorchBench是一款新型基准测试套件，可全面表征PyTorch软件栈的性能，指导模型、PyTorch框架和GPU库的性能优化。 |
| [^32] | [Self-discipline on multiple channels.](http://arxiv.org/abs/2304.14224) | 本文提出了一种名为“多通道自律”的方法，使用一致性正则化和自我蒸馏来提高模型的泛化能力和对噪声标签的鲁棒性。 |
| [^33] | [Some of the variables, some of the parameters, some of the times, with some physics known: Identification with partial information.](http://arxiv.org/abs/2304.14214) | 本文介绍了一种利用神经网络识别动力系统的新方法，在不对数据进行修改的情况下，通过数值积分和部分已知物理学，能够从任意时间点的采样数据中学习。 |
| [^34] | [LLT: An R package for Linear Law-based Feature Space Transformation.](http://arxiv.org/abs/2304.14211) | LLT是一个R包，用于线性定律特征空间变换，可以帮助对单变量和多变量时间序列进行分类。 |
| [^35] | [A transparent approach to data representation.](http://arxiv.org/abs/2304.14209) | 使用二元属性表示模型对Netflix观众对电影的评分数据集进行数据表示，属性易于解释，且需要较少属性即可达到相同水平的误差。 |
| [^36] | [Logarithmic-Regret Quantum Learning Algorithms for Zero-Sum Games.](http://arxiv.org/abs/2304.14197) | 该论文提出了一个在线零和游戏的量子算法，可以在量子时间计算出$\varepsilon$-近似纳什均衡，是目前第一个实现基于量子计算的快速量子线性编程求解器。 |
| [^37] | [ClusterNet: A Perception-Based Clustering Model for Scattered Data.](http://arxiv.org/abs/2304.14185) | 这项工作介绍了ClusterNet，一种基于感知的分布式数据聚类模型，利用大规模数据集和基于点的深度学习模型，反映人类感知的聚类可分性。 |
| [^38] | [mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality.](http://arxiv.org/abs/2304.14178) | 本文介绍了一种名为mPLUG-Owl的训练范式，它通过模块化学习基础LLM、视觉知识模块和视觉抽象器模块，赋予LLMs多模态的能力。实验结果表明，mPLUG-Owl在图像字幕和视觉问答任务中表现优于基线模型，并在某些情况下达到了最先进的性能水平。 |
| [^39] | [Exploring the flavor structure of quarks and leptons with reinforcement learning.](http://arxiv.org/abs/2304.14176) | 通过利用强化学习，探索了具有 $U(1)$ 味道对称性的模型的味道结构，找到了21个与实验测量值一致的模型，预测了无中微子双贝塔衰变的有效质量和可观的轻子 CP 破坏。 |
| [^40] | [An Algorithm for Computing with Brauer's Group Equivariant Neural Network Layers.](http://arxiv.org/abs/2304.14165) | 本文提出一种算法，使用范畴论构造来实现的Brauer群等变神经网络层的乘积，同时采用Kronecker积矩阵，实现了显著的计算成本减少。 |
| [^41] | [Spiking Neural Network Decision Feedback Equalization for IM/DD Systems.](http://arxiv.org/abs/2304.14152) | 本论文介绍了一种基于脉冲神经网络的决策反馈均衡器，用于IM/DD系统。实验表明，该均衡器的性能优于传统的线性均衡器和人工神经网络均衡器。 |
| [^42] | [Categorification of Group Equivariant Neural Networks.](http://arxiv.org/abs/2304.14144) | 本文利用范畴论构建的算法，成功快速计算了群等变神经网络的线性层函数。这种方法为深度学习的其他领域做出了有益的探索。 |
| [^43] | [TempEE: Temporal-Spatial Parallel Transformer for Radar Echo Extrapolation Beyond Auto-Regression.](http://arxiv.org/abs/2304.14131) | 本文提出了一个新型的雷达回波外推算法TempEE，该算法利用时空相关特征和Transformer技术，通过从多帧回波图像中提取特征，准确地表示了降水的非平稳运动过程，克服了传统算法的局限性，在三个真实数据集上表现出色。 |
| [^44] | [The Structurally Complex with Additive Parent Causality (SCARY) Dataset.](http://arxiv.org/abs/2304.14109) | SCARY数据集是一个具有结构复杂性和附加父因果关系的合成数据集，包含40个场景、三个不同的种子、线性和混合因果机制等特点，能够提供更真实的因果关系探索环境。 |
| [^45] | [DataComp: In search of the next generation of multimodal datasets.](http://arxiv.org/abs/2304.14108) | DataComp是一个基准测试，旨在通过提出新的训练集来解决数据集在机器学习生态系统中的缺陷。它提供了一个多规模设计的实验测试平台，使用12.8B个图像-文本对的新候选池，让研究人员可以通过设计新的过滤技术或策划新的数据源并评估它们的新数据集来进行创新。 |
| [^46] | [Learning Human-Human Interactions in Images from Weak Textual Supervision.](http://arxiv.org/abs/2304.14104) | 本文提出了一种新的范式，从单一的静态图像中学习自由文本的形式来灵活建模人际互动。并通过知识蒸馏生成伪标签来训练一种字幕模型，用于有效理解图像中的人际互动，具有较高的预测文本和语义质量，并在此任务上优于SOTA的图像字幕和情境识别模型。 |
| [^47] | [SocNavGym: A Reinforcement Learning Gym for Social Navigation.](http://arxiv.org/abs/2304.14102) | 本文提出了SocNavGym，对于社交导航领域的研究提供了一个轻便、快速、易用的仿真环境，可生成各种各样的社交导航场景，并促进了智能社交机器人的发展。 |
| [^48] | [Categorical Foundations of Explainable AI: A Unifying Formalism of Structures and Semantics.](http://arxiv.org/abs/2304.14094) | 本文采用范畴理论的框架，提出了可解释AI的统一理论体系，为领域中所有重要术语提供了清晰的形式定义，并提供了遵循所提出结构的领域分类法。 |
| [^49] | [JaxPruner: A concise library for sparsity research.](http://arxiv.org/abs/2304.14082) | 本文介绍了JaxPruner，一款用于研究稀疏神经网络的开源库。JaxPruner提供了流行的剪枝和稀疏训练算法的简明实现，最小化内存和延迟开销，并可轻松集成到现有的JAX库中。 |
| [^50] | [Cluster Flow: how a hierarchical clustering layer make allows deep-NNs more resilient to hacking, more human-like and easily implements relational reasoning.](http://arxiv.org/abs/2304.14081) | 该论文介绍了一种半监督层次聚类框架ClusterFlow，可以在经过训练的深度卷积神经网络中使用密集的多维类别和特征数据来构建超空间地图，从而使其更具人类思考方式的功能，提高了其鲁棒性和实现关系推理的能力。 |
| [^51] | [ganX -- generate artificially new XRF a python library to generate MA-XRF raw data out of RGB images.](http://arxiv.org/abs/2304.14078) | ganX是一个Python库，可以从RGB图像中生成X射线荧光宏观图谱(MA-XRF)，使用了蒙特卡洛方法和颜料特征XRF信号的数据库实现。 |
| [^52] | [Compositional 3D Human-Object Neural Animation.](http://arxiv.org/abs/2304.14070) | 提出了一种组合式的方法来解决人物-物体交互动画的挑战，以实现新的HOI的组合式动画控制。 |
| [^53] | [Interpretable Neural-Symbolic Concept Reasoning.](http://arxiv.org/abs/2304.14068) | 本文提出了第一个基于概念嵌入的可解释概念模型DCR，能够在多个数据集上实现接近最先进的准确性，相对于最先进的可解释概念模型提高了高达+25％，并产生能够解释其预测的人类可理解规则和真值度，适应性强。 |
| [^54] | [Propagating Kernel Ambiguity Sets in Nonlinear Data-driven Dynamics Models.](http://arxiv.org/abs/2304.14057) | 该论文提出了一种算法，通过内核最大均值差将歧义集合传递到非线性数据驱动模型中，解决了分布式鲁棒控制和基于学习的控制中的关键问题。 |
| [^55] | [Precise Few-shot Fat-free Thigh Muscle Segmentation in T1-weighted MRI.](http://arxiv.org/abs/2304.14053) | 通过新的少样本分割框架，准确无误地生成股骨肌肉掩蔽，剔除IMF，有助于肌肉容积分析，尤其是在缺乏大量精确注释的情况下。 |
| [^56] | [Interweaved Graph and Attention Network for 3D Human Pose Estimation.](http://arxiv.org/abs/2304.14045) | 该论文提出了一种名为交错图和注意力网络（IGANet）的模型，通过图卷积网络（GCNs）和注意力之间的双向通信，解决了先前单视角图像3D人体姿态估计中忽略全局和局部关联的问题。在人体姿态估计上取得了最先进的结果。 |
| [^57] | [Spherical Inducing Features for Orthogonally-Decoupled Gaussian Processes.](http://arxiv.org/abs/2304.14034) | 本文研究了解耦高斯过程的正交分解问题，提出了一种扩展方法，即引入球形跨域特征，构建更灵活的数据依赖基函数来缓解限制，并展示了其有效性。 |
| [^58] | [Attacks on Robust Distributed Learning Schemes via Sensitivity Curve Maximization.](http://arxiv.org/abs/2304.14024) | 本论文介绍了一种新的攻击方法——敏感性曲线最大化（SCM），它能够通过注入微小但有效的扰动来破坏现有的鲁棒分布式学习方案。 |
| [^59] | [XAI-based Comparison of Input Representations for Audio Event Classification.](http://arxiv.org/abs/2304.14019) | 本文比较了两种不同的音频事件分类输入表示法的模型，即原始波形信号和时间-频率谱，利用可解释人工智能（XAI）揭示了不同表示法的决策策略，为正确的决策提供了深入的见解。 |
| [^60] | [Contour Completion by Transformers and Its Application to Vector Font Data.](http://arxiv.org/abs/2304.13988) | 本文提出了一种基于Transformer的方法，来解决轮廓完整度问题，特别适用于字体轮廓。 |
| [^61] | [Moderately Distributional Exploration for Domain Generalization.](http://arxiv.org/abs/2304.13976) | 本文提出了一种针对领域泛化问题的中度分布探索（MODE）方法，通过在共享相同语义因素的不确定性子集中探索领域，可以提高模型的分布偏移鲁棒性，并在多个基准数据集上实现了最先进的性能。 |
| [^62] | [Convergence of Adam Under Relaxed Assumptions.](http://arxiv.org/abs/2304.13972) | 本文对Adam算法做了新的假设并进行了证明，证明了在更加现实的条件下，Adam能够以较小的梯度复杂度达到稳定点。 |
| [^63] | [Fairness Uncertainty Quantification: How certain are you that the model is fair?.](http://arxiv.org/abs/2304.13950) | 本篇论文提出了一种针对机器学习模型公平性的不确定性量化方法，针对公平感知模型提供了置信区间（CI）来评估其测试不公平性。 |
| [^64] | [A Majorization-Minimization Gauss-Newton Method for 1-Bit Matrix Completion.](http://arxiv.org/abs/2304.13940) | 本文提出了一种基于主导-最小化原则，通过低秩矩阵补全解决1比特矩阵补全问题的新方法，称为MMGN。通过应用高斯-牛顿方法，MMGN具有更快的速度和更准确的结果，同时还不太受到潜在矩阵尖锐度的影响。 |
| [^65] | [A Deep Registration Method for Accurate Quantification of Joint Space Narrowing Progression in Rheumatoid Arthritis.](http://arxiv.org/abs/2304.13938) | 本文提出了一种新的深度配准方法，用于自动量化早期RA中的JSN进展，实验结果表明其具有高精度和可靠性，可以作为监测关节空间的有效手段。 |
| [^66] | [Oversampling Higher-Performing Minorities During Machine Learning Model Training Reduces Adverse Impact Slightly but Also Reduces Model Accuracy.](http://arxiv.org/abs/2304.13933) | 本研究发现，在机器学习模型训练中，对表现更好的少数族裔进行过采样会稍微减少不良影响，但也会降低模型精度。 |
| [^67] | [Detection of Adversarial Physical Attacks in Time-Series Image Data.](http://arxiv.org/abs/2304.13919) | 本论文提出了一种针对时间序列图像数据的对抗性物理攻击检测方法，使用VisionGuard和多数投票结合的方法，以应对自主系统应用中普遍存在的对抗性攻击问题。 |
| [^68] | [Proportionally Representative Clustering.](http://arxiv.org/abs/2304.13917) | 本文提出了一个新的公平性准则——比例代表性公平性（PRF），并设计了有效的算法满足该准则。 |
| [^69] | [LSTM based IoT Device Identification.](http://arxiv.org/abs/2304.13905) | 本研究提出了一种使用LSTM进行物联网设备识别的方法，以预防安全威胁和检测漏洞为目标。 |
| [^70] | [Discovering Object-Centric Generalized Value Functions From Pixels.](http://arxiv.org/abs/2304.13892) | 本文介绍了一种从像素中学习物体中心化的广义值函数的方法。该方法从物体中发现有意义的特征，转化为“问题”函数，并利用随后学习的广义值函数来进行控制，在静态和非静态设置下表现良好。学到的表示不仅是可解释的，而且围绕着具有不变性的物体，有助于快速适应。 |
| [^71] | [Improving the Utility of Differentially Private Clustering through Dynamical Processing.](http://arxiv.org/abs/2304.13886) | 本研究提出了一种通过利用 Morse 理论提高差分私有聚类效用的方法，该方法可为复杂集群分布适配高斯子集群，即使对于现有的简单聚类方法，其效果也更好，在相同的隐私水平下不会增加隐私损失。 |
| [^72] | [MasonNLP+ at SemEval-2023 Task 8: Extracting Medical Questions, Experiences and Claims from Social Media using Knowledge-Augmented Pre-trained Language Models.](http://arxiv.org/abs/2304.13875) | 本研究提出了一种知识增强的预训练语言模型MasonNLP+，能够从社交媒体中提取医疗问题、经验和声明，并在SemEval-2023任务8的两个子任务中取得了最先进的性能。 |
| [^73] | [Typical and atypical solutions in non-convex neural networks with discrete and continuous weights.](http://arxiv.org/abs/2304.13871) | 本文研究了二元和连续的负边距感知器作为非凸神经网络模型，发现它们都存在着极为平坦和宽广的亚优解，这对于二元情况中的算法行为有着很强的影响。 |
| [^74] | [highway2vec -- representing OpenStreetMap microregions with respect to their road network characteristics.](http://arxiv.org/abs/2304.13865) | 本文提出了一种基于OpenStreetMap城市道路网络的微区域的嵌入方法，以检测地图六边形在包含的道路网络中的相似性。 |
| [^75] | [Enhancing Inverse Problem Solutions with Accurate Surrogate Simulators and Promising Candidates.](http://arxiv.org/abs/2304.13860) | 神经伴随方法在AEM设计中表现出了良好的性能，但由于准确性和计算资源限制，优化变得更加具有挑战性。 NeuLag方法能够高效地解决这些问题，并在约束条件下展现出良好性能。 |
| [^76] | [Multimodal Composite Association Score: Measuring Gender Bias in Generative Multimodal Models.](http://arxiv.org/abs/2304.13855) | 这篇论文提出了一种新方法——多模态复合关联评分(MCAS)，用于衡量多模态生成模型中的性别偏差。与单模态小型单阶段模型的度量与量化不同，该方法能够高效、有效地识别和测量大型复杂的多模态生成模型中的性别偏差。 |
| [^77] | [Categorising Products in an Online Marketplace: An Ensemble Approach.](http://arxiv.org/abs/2304.13852) | 本研究提出了一种使用集成方法将不同模型组合来自动分类商品的方法，并证明了该方法可以取得不错的分类效果，平均F1得分为0.82。 |
| [^78] | [Do SSL Models Have D\'ej\`a Vu? A Case of Unintended Memorization in Self-supervised Learning.](http://arxiv.org/abs/2304.13850) | 自监督学习（SSL）算法会意外地记忆单个训练样本中的特定部分，称为“似曾相识”记忆，该现象是普遍存在的，不能被传统的评估方法检测。 |
| [^79] | [A Deep Learning Framework for Verilog Autocompletion Towards Design and Verification Automation.](http://arxiv.org/abs/2304.13840) | 本文提出了一个深度学习框架，用于训练Verilog自动完成模型，能够有效预测下一个token，为设计和验证自动化提供了解决方案。 |
| [^80] | [On Pitfalls of $\textit{RemOve-And-Retrain}$: Data Processing Inequality Perspective.](http://arxiv.org/abs/2304.13836) | 本论文评估了RemOve-And-Retrain（ROAR）协议的可靠性。研究结果表明，ROAR基准测试中的属性可能有更少的有关决策的重要信息，这种偏差称为毛糙度偏差，并提醒人们不要在ROAR指标上进行盲目的依赖。 |
| [^81] | [Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models.](http://arxiv.org/abs/2304.13835) | 本文通过收集和评估多方对话情况，探讨了模型在群体对话中需要具备的技能，发现新数据集MultiLIGHT可以在这个领域带来显着的进展。 |
| [^82] | [Mixtures of Gaussian process experts based on kernel stick-breaking processes.](http://arxiv.org/abs/2304.13833) | 提出了一种新的基于核棍棒过程的高斯过程专家混合模型，能够维持直观吸引力并提高模型性能，具有实用性。 |
| [^83] | [Adaptation to Misspecified Kernel Regularity in Kernelised Bandits.](http://arxiv.org/abs/2304.13830) | 本文研究了核化赌博问题中对核函数规则性错误的自适应性问题。我们证明了在具有不同规则性的一对RKHS中同时实现最佳累计遗憾是不可能的，并通过现有算法结合极小化非自适应的核赌博机算法，验证了这一下限的紧密性。 |
| [^84] | [Guaranteed Quantization Error Computation for Neural Network Model Compression.](http://arxiv.org/abs/2304.13812) | 本文提出了一种通过建立合并的神经网络，应用优化方法和可达性分析方法来计算保证的量化误差的方法，解决了神经网络压缩的保证输出误差计算问题。 |
| [^85] | [A Data-Driven Hybrid Automaton Framework to Modeling Complex Dynamical Systems.](http://arxiv.org/abs/2304.13811) | 本文提出了一种基于神经网络的计算高效混合自动机模型，可用于捕捉未知的复杂系统行为，并提供低计算成本的集合值可达性分析，以显著降低可达集合计算的计算成本而不牺牲建模精度。 |
| [^86] | [Latent Fingerprint Recognition: Fusion of Local and Global Embeddings.](http://arxiv.org/abs/2304.13800) | 本文采用全局嵌入与局部嵌入相结合的方法，在提高识别准确率的同时，保证了较高的吞吐量和应用性。 |
| [^87] | [Physics-informed neural networks for predicting gas flow dynamics and unknown parameters in diesel engines.](http://arxiv.org/abs/2304.13799) | 本论文提出了一种物理信息神经网络（PINN）方法，能够同时准确预测柴油机未知参数和动态，以及识别“平均值”模型中的未知参数，为实际案例研究提供了可能性。 |
| [^88] | [Generalized generalized linear models: Convex estimation and online bounds.](http://arxiv.org/abs/2304.13793) | 该论文提出了一种用于估计时间空间数据中依赖关系的广义广义线性模型（GGLM）参数的计算框架，使用单调运算符的变分不等式方法克服了参数估计中的非凸性并为参数恢复提供保证 |
| [^89] | [Surrogate Assisted Generation of Human-Robot Interaction Scenarios.](http://arxiv.org/abs/2304.13787) | 本文提出了基于替代模型的人机交互场景生成方法，可以高效合成多样化的挑战性数据集，以便评估和理解人机交互系统的优劣，可以在实际交互中重现这些场景。 |
| [^90] | [Distance Weighted Supervised Learning for Offline Interaction Data.](http://arxiv.org/abs/2304.13774) | 这篇论文提出一种距离加权监督学习方法，可以利用离线交互数据中的最短路径距离来提取策略，较之以往的监督方法和离线强化学习方法表现更好。 |
| [^91] | [Phagocytosis Unveiled: A Scalable and Interpretable Deep learning Framework for Neurodegenerative Disease Analysis.](http://arxiv.org/abs/2304.13764) | 本文提出了一种可扩展且可解释的深度学习框架，用于量化和分析吞噬活性以评估神经退行性疾病。流程可以处理大型数据集，包括数据质量验证和可解释的细胞分割模块。 |
| [^92] | [Enhancing Robustness of Gradient-Boosted Decision Trees through One-Hot Encoding and Regularization.](http://arxiv.org/abs/2304.13761) | 通过独热编码和正则化提高梯度提升决策树的鲁棒性，研究表明对带有$L_1$或$L_2$正则化的线性回归形式进行拟合可提高GBDT模型的鲁棒性。 |
| [^93] | [TR0N: Translator Networks for 0-Shot Plug-and-Play Conditional Generation.](http://arxiv.org/abs/2304.13742) | 本文提出了TR0N，将预训练的无条件生成模型转化为高度任意的条件模型。TR0N不需要训练数据或微调，可以在MS-COCO上实现零-shot FID 10.9，并在采样速度上优于竞品，同时保持了多样性和质量。 |
| [^94] | [Scalable, Distributed AI Frameworks: Leveraging Cloud Computing for Enhanced Deep Learning Performance and Efficiency.](http://arxiv.org/abs/2304.13738) | 本文介绍了可扩展的分布式AI框架，利用云计算提高深度学习性能和效率。通过概述常用的AI框架和云服务、探讨基于云的AI系统的关键方面和讨论云中AI工作负载的优化策略，提供了解决AI应用程序不断增长的计算需求的有效方法。 |
| [^95] | [AIRIVA: A Deep Generative Model of Adaptive Immune Repertoires.](http://arxiv.org/abs/2304.13737) | AIRIVA是一种生成模型，学习TCR库存的低维、可解释和组成性表示，以解开系统效应，能够识别与不同传染病和癌症相关的TCR。 |
| [^96] | [The Internal State of an LLM Knows When its Lying.](http://arxiv.org/abs/2304.13734) | 该论文研究了LLM生成不准确或虚假信息的问题，提出了一种简单而有效的方法，利用LLM的隐藏层激活来确定语句的真实性。在实验中，该方法表现出较好的检测效果，并有利于提高LLM的可信度。 |
| [^97] | [A Unified Approach to Lane Change Intention Recognition and Driving Status Prediction through TCN-LSTM and Multi-Task Learning Models.](http://arxiv.org/abs/2304.13732) | 本文提出了一种新颖的集成TCN-LSTM模型和多任务学习模型的统一方法，用于车道变换意图识别和行驶状态预测。实验证明该方法效果良好。 |
| [^98] | [Ensemble CNNs for Breast Tumor Classification.](http://arxiv.org/abs/2304.13727) | 本文使用集成CNN的方法对乳腺肿瘤进行分类，提高了分类性能5%以上，并在公共数据集上实现了高准确率、精度和召回率。 |
| [^99] | [SamurAI: A Versatile IoT Node With Event-Driven Wake-Up and Embedded ML Acceleration.](http://arxiv.org/abs/2304.13726) | SamurAI是一种多功能物联网节点，具有事件驱动唤醒和嵌入式机器学习加速器，通过利用两个芯片内子系统来弥合处理和能量的差距，并且为物联网应用提供了识别和自适应等功能。 |
| [^100] | [Prediction of brain tumor recurrence location based on multi-modal fusion and nonlinear correlation learning.](http://arxiv.org/abs/2304.13725) | 本文提出了一种基于深度学习的方法来预测脑肿瘤复发位置。使用迁移学习、多模态融合和非线性相关学习，能够有效提取数据中的特征并预测复发的位置。 |
| [^101] | [GPU accelerated matrix factorization of large scale data using block based approach.](http://arxiv.org/abs/2304.13724) | 该论文提出了一种使用基于块的方法的GPU加速矩阵分解的方案，旨在加快对大规模数据的分解并避免内存的限制。 |
| [^102] | [Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond.](http://arxiv.org/abs/2304.13712) | 本文提供了一个LLMs的使用综述，探讨了在各种自然语言处理任务中的使用和限制。 |
| [^103] | [A mean-field games laboratory for generative modeling.](http://arxiv.org/abs/2304.13534) | 本文提出了使用均场博弈作为实验室对生成模型进行设计和分析的方法，并建立了这种方法与主要流动和扩散型生成模型之间的关联。通过研究每个生成模型与它们相关的 MFG 的最优条件，本文提出了一个基于双人 MFG 的新的生成模型，该模型在提高样本多样性和逼真度的同时改善了解缠结和公平性。 |
| [^104] | [From Chaos Comes Order: Ordering Event Representations for Object Detection.](http://arxiv.org/abs/2304.13455) | 本文提出了一种基于Gromov-Wasserstein Discrepancy选择最佳事件表示的方法，这种方法可以在多个表示、网络骨干和数据集上保持任务性能排名的一致性。利用这一方法，本文对大型事件表示法家族进行超参数搜索，选择最适合物体检测的表示法，取得了优于最先进的基于事件的对象检测方法的成果。 |
| [^105] | [From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping.](http://arxiv.org/abs/2304.13273) | 本研究提出了一种从关联到生成的零-shot方法：通过将图像/视频投影到语言模态并在生成任务中生成描述性字幕。该方法在多个基准数据集上显著优于现有的最先进方法，为无监督跨模态映射提供了一个新的视角，并具有在视频字幕，图像合成和文本到图像生成等领域的潜在应用。 |
| [^106] | [Shot Optimization in Quantum Machine Learning Architectures to Accelerate Training.](http://arxiv.org/abs/2304.12950) | 本文提出了一种用于加速量子机器学习训练的Shot优化方法，通过减小数据集大小和自适应Shot分配等方法，实现了Shot数量的优化与准确度之间的平衡。 |
| [^107] | [Performance Optimization using Multimodal Modeling and Heterogeneous GNN.](http://arxiv.org/abs/2304.12568) | 本研究提出了一种通用且高效的性能优化方法，使用基于IR编程模型的多模态深度学习方法进行特定任务的性能优化。我们提出了一种非常通用的调节并行代码区域的技术。 |
| [^108] | [Topology-Aware Focal Loss for 3D Image Segmentation.](http://arxiv.org/abs/2304.12223) | 提出了一种基于拓扑感知的聚焦损失函数(TAFL)，通过将传统的聚焦损失与基于地面真值和预测分割掩码的持久图之间的Wasserstein距离的拓扑约束相结合，有效解决三维图像分割中的拓扑错误和类别不平衡问题。 |
| [^109] | [The Disharmony Between BN and ReLU Causes Gradient Explosion, but is Offset by the Correlation Between Activations.](http://arxiv.org/abs/2304.11692) | 本研究阐述了BN和ReLU之间的不和谐是导致梯度爆炸的主要原因，同时发现输入之间的相关性可以缓解这个问题。提出一种基于二阶优化算法的自适应学习率算法，在大批量训练中表现优异，并可替代WarmUp，在小批量训练中也表现不错。 |
| [^110] | [A Convolutional Spiking Network for Gesture Recognition in Brain-Computer Interfaces.](http://arxiv.org/abs/2304.11106) | 本文提出了一种用于脑机接口手势分类的卷积脉冲网络，采用事件驱动可塑性规则进行无监督特征学习，具有较好的推广性和效果。 |
| [^111] | [Leveraging sparse and shared feature activations for disentangled representation learning.](http://arxiv.org/abs/2304.07939) | 本文提出了利用从多样化监督任务中提取的知识来学习通用的解缠表示的方法，使监督多任务模型的特征空间得以解缠，适当共享信息，达到可识别性。 |
| [^112] | [Assisting clinical practice with fuzzy probabilistic decision trees.](http://arxiv.org/abs/2304.07788) | 我们提出了一种基于概率树和模糊逻辑的新方法MedFP，用于辅助医学实践。该方法可以完全解释，允许临床医生产生、控制和验证整个诊断过程，并减少误诊率，通过提供不确定性和反事实分析的估计值。 |
| [^113] | [PI-FL: Personalized and Incentivized Federated Learning.](http://arxiv.org/abs/2304.07514) | PI-FL是一种个性化的联邦学习解决方案，使用基于令牌的激励机制奖励个性化训练，可以在尊重客户端隐私的同时生成高质量的个性化模型。 |
| [^114] | [Astroformer: More Data Might Not be All You Need for Classification.](http://arxiv.org/abs/2304.05350) | 该文提出了使用混合变换器 - 卷积架构的方法，结合新的堆栈设计、不同的相对自我注意层创建方式和精心选择的数据增强和正则化技术，从少量数据中学习，将此方法应用于Galaxy Zoo数据集，结果表明在少量数据的情况下取得了与以前方法相同的分类结果，并且不会损失性能。 |
| [^115] | [Online Spatio-Temporal Learning with Target Projection.](http://arxiv.org/abs/2304.05124) | 提出了一种名为OSTTP的新型学习算法，解决了通过时间反向传播算法所引入的限制，使网络能够同时处理和学习新的传入数据，具有竞争性能。 |
| [^116] | [An interpretable neural network-based non-proportional odds model for ordinal regression with continuous response.](http://arxiv.org/abs/2303.17823) | 本文提出了一种基于可解释神经网络的非比例赔率模型 (N$^3$POM) 用于有序回归，可以对连续变量进行预测，同时保留了可解释性，具有灵活性。 |
| [^117] | [Natural Selection Favors AIs over Humans.](http://arxiv.org/abs/2303.16200) | 这篇论文探讨了随着人工智能的发展，其可能会出现不良特性并逐渐超越人类智能的问题，以及这对人类未来的控制权产生的影响。 |
| [^118] | [Spatio-Temporal Graph Neural Networks for Predictive Learning in Urban Computing: A Survey.](http://arxiv.org/abs/2303.14483) | 本综述介绍了面向城市计算的时空图神经网络预测学习领域的发展现状，包括其框架、实现方法和应用场景，以及当前的研究热点和挑战，提出了该领域未来的发展方向和应用前景。 |
| [^119] | [PheME: A deep ensemble framework for improving phenotype prediction from multi-modal data.](http://arxiv.org/abs/2303.10794) | 本文提出了PheME，一种利用多模态数据进行表型预测的深度集成框架。该框架采用多个深度神经网络和集成学习，可以从EHR数据中准确且高效地提取表型信息。 |
| [^120] | [Large statistical learning models effectively forecast diverse chaotic systems.](http://arxiv.org/abs/2303.08011) | 该论文研究了混沌预测的大规模实验，发现基于人工神经网络的大规模、领域不可知的时间序列预测方法表现出了相当强大的性能，尤其是分层神经基础函数模型表现最佳。 |
| [^121] | [Synthetic Data Generator for Adaptive Interventions in Global Health.](http://arxiv.org/abs/2303.01954) | 通过HealthSyn生成基于真实世界的移动健康干预数据，以帮助在全球卫生领域中发展、测试和评估机器学习算法和干预措施。 |
| [^122] | [Symbolic Discovery of Optimization Algorithms.](http://arxiv.org/abs/2302.06675) | 该论文提出了一种将算法发现视为程序搜索的方法，并用于发现深度神经网络训练的优化算法。他们的方法发现了一种简单而有效的优化算法Lion，它比Adam更节省内存并且在ImageNet上的准确率提高了2％，并且预训练的计算时间也减少了多达5倍。 |
| [^123] | [A Comprehensive Survey on Graph Summarization with Graph Neural Networks.](http://arxiv.org/abs/2302.06114) | 本文综述了基于图神经网络的深度学习概括技术，在保留图形关键特征的同时，对大规模、高维度和复杂的现代图形数据进行处理的方法，包括图神经网络、图形自编码器等。 |
| [^124] | [A Graph-Based Modeling Framework for Tracing Hydrological Pollutant Transport in Surface Waters.](http://arxiv.org/abs/2302.04991) | 本研究提出了一种名为 HydroGraphs 的基于图的建模框架，用于分析水文污染物的传输和命运。该框架可以根据开源数据构建，具有简化的水文系统表示，并且可以使用常见的图分析和数据可视化技术进行分析和可视化，能够帮助准确定位污染源和脆弱区域。 |
| [^125] | [Towards Fairer and More Efficient Federated Learning via Multidimensional Personalized Edge Models.](http://arxiv.org/abs/2302.04464) | 本研究提出了一种定制化联邦学习系统，通过从多个维度个性化边缘模型来消除联邦学习异质性，能够在模型准确度、效率和公平性方面显著提高性能。 |
| [^126] | [Utility-based Perturbed Gradient Descent: An Optimizer for Continual Learning.](http://arxiv.org/abs/2302.03281) | 本文提出了一种在线学习算法——基于效用的扰动梯度下降（UPGD），该算法可保护有用的权重或特征，并基于它们的效用扰动不太有用的权重或特征。实验证明，UPGD有助于减少遗忘和保持可塑性，在连续学习中大有用处。 |
| [^127] | [Sharp Variance-Dependent Bounds in Reinforcement Learning: Best of Both Worlds in Stochastic and Deterministic Environments.](http://arxiv.org/abs/2301.13446) | 本研究将马尔可夫决策过程的方差相关遗憾界限应用到强化学习中，提出了两个新的环境规范来表征环境的方差属性，并设计出基于模型和无模型的算法，对于随机和确定性环境同时极小极大最优的界限是第一次被证明出来的。 |
| [^128] | [Incorporating Recurrent Reinforcement Learning into Model Predictive Control for Adaptive Control in Autonomous Driving.](http://arxiv.org/abs/2301.13313) | 本文提出了一种基于循环强化学习和部分可观察的马尔可夫决策过程的自适应控制算法 $\textit{MPC-RRL}$，在CARLA模拟器中得到有效验证。 |
| [^129] | [Convergence of uncertainty estimates in Ensemble and Bayesian sparse model discovery.](http://arxiv.org/abs/2301.12649) | 本文探讨了集合稀疏模型发现方法中的自助法序列阈值最小二乘估计器，证明了其具有指数收敛速率的误差率和可证明正确的变量选择过程，在不确定性估计方面具有高效性。 |
| [^130] | [On the Lipschitz Constant of Deep Networks and Double Descent.](http://arxiv.org/abs/2301.12309) | 本文通过实验研究发现，深度网络的利普希茨常数趋势与测试误差密切相关，通过建立参数空间和输入空间梯度之间的联系，确定了损失函数曲率和距离初始化参数的距离对于深度网络的优化和模型函数复杂度限制是关键因素，该研究对隐式正则化和网络的有效模型复杂度提供了新的见解。 |
| [^131] | [Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling.](http://arxiv.org/abs/2301.12050) | 本论文研究使用少量的大型语言模型来提高强化学习代理的样本效率。通过假设抽象世界模型并通过代理的世界经验进行验证，可以进行规划和探索。这种方法不仅可提高样本效率一个数量级，而且还具有稳健性。 |
| [^132] | [Are Equivariant Equilibrium Approximators Beneficial?.](http://arxiv.org/abs/2301.11481) | 本文从理论上探讨了等变均衡逼近器的优缺点，它在一些情况下比普通均衡逼近器有更好的泛化性能，可以实现更好的逼近效果，同时它也存在均衡选择和社会福利方面的缺陷。 |
| [^133] | [A Distance-Geometric Method for Recovering Robot Joint Angles From an RGB Image.](http://arxiv.org/abs/2301.02051) | 本文提出了一种仅使用机器人当前配置的单个RGB图像就可以恢复机器人操纵器关节角度的方法，该方法利用机器人的运动学模型并训练浅层神经网络，可在缺少本体感知时恢复系统功能。 |
| [^134] | [Deep R Programming.](http://arxiv.org/abs/2301.01188) | 该课程介绍了流行的数据科学语言R，并旨在培养学生、从业者和研究者成为独立的R语言用户。 |
| [^135] | [Sparse neural networks with skip-connections for identification of aluminum electrolysis cell.](http://arxiv.org/abs/2301.00582) | 本文研究了将串联跳过连接和稀疏正则化相结合用于铝电解槽识别的稀疏神经网络，能够提高长期预测的开环稳定性和准确性，解决有限训练数据下标准神经网络难以提供稳定预测的问题。 |
| [^136] | [RFold: RNA Secondary Structure Prediction with Decoupled Optimization.](http://arxiv.org/abs/2212.14041) | 所提出的RFold方法采用解耦优化过程和注意力机制进行简单又有效的RNA二级结构预测，具有较高的准确性和速度。 |
| [^137] | [Local Policy Improvement for Recommender Systems.](http://arxiv.org/abs/2212.11431) | 该论文介绍了一种针对推荐系统的本地策略改进方法，不需要现场校正，易于从数据中估计，适用于以前的策略质量较高但数量较少的情况。 |
| [^138] | [Interactive Concept Bottleneck Models.](http://arxiv.org/abs/2212.07430) | 该论文提出了交互式概念瓶颈模型(CBMs)，使得模型可以向人类协作者查询某些概念的标签，从而提高最终预测结果的准确性。 |
| [^139] | [Visual Query Tuning: Towards Effective Usage of Intermediate Representations for Parameter and Memory Efficient Transfer Learning.](http://arxiv.org/abs/2212.03220) | 本文介绍了一种名为视觉查询调整（VQT）的简单而有效的方法，用于聚合Vision Transformers的中间特征。VQT在训练中具有内存效率，相比于许多其他 fine-tuning 方法，不需要对整个骨干进行反向传播。该方法在几个基准测试中优于最先进的微调方法。 |
| [^140] | [Bayesian Physics Informed Neural Networks for Data Assimilation and Spatio-Temporal Modelling of Wildfires.](http://arxiv.org/abs/2212.00970) | 本研究使用物理信息神经网络（PINN）解决野火前沿建模问题，在优化模型中改进了成本函数以提高在极端环境下的时间连续性，并开发了数据同化的方法将预测值与前沿观测值相结合。最终，我们开发了一个贝叶斯PINN（B-PINN）以提供不确定性估计和置信区间。 |
| [^141] | [Interpreting Primal-Dual Algorithms for Constrained Multiagent Reinforcement Learning.](http://arxiv.org/abs/2211.16069) | 本文研究了对约束多智能体强化学习的原始-对偶算法，证明了使用约束函数作为惩罚的标准做法可以导致较弱的安全性概念，通过对惩罚项进行简单修正，能够强制约束并提高性能。 |
| [^142] | [Crown-CAM: Interpretable Visual Explanations for Tree Crown Detection in Aerial Images.](http://arxiv.org/abs/2211.13126) | 本文提出了一种名为Crown-CAM的可解释的类激活映射方法，用于解决高空图像中树冠检测的问题，该方法通过无监督选择激活映射、计算局部得分映射和非上下文背景抑制等步骤，既可以有效地提供树冠的精细定位，又可以量化生成的解释的准确性和不准确性。 |
| [^143] | [Geometry-Complete Perceptron Networks for 3D Molecular Graphs.](http://arxiv.org/abs/2211.02504) | 本研究引入了一种新的几何完备的图神经网络 GCPNet，用于3D分子图的表示学习，并在多个几何任务上展示了其出色的预测性能。其中最佳表现是在蛋白质-配体结合亲和力预测上得到了比当前最先进方法高出5%以上的相关系数。 |
| [^144] | [A Systematic Survey of Chemical Pre-trained Models.](http://arxiv.org/abs/2210.16484) | 本文是对化学预训练模型领域的第一次系统回顾，提出了从头训练分子表示模型的局限性，总结了最新的进展和几个关键角度，包括分子描述符和编码器架构。 |
| [^145] | [Beyond calibration: estimating the grouping loss of modern neural networks.](http://arxiv.org/abs/2210.16315) | 本文提出了一个估计器来近似神经网络的分组损失，并表明现代神经网络在视觉和NLP中展示出显著的分组损失。 |
| [^146] | [Occam learning.](http://arxiv.org/abs/2210.13179) | 本文讨论了一种具有固定隐藏层分布的概率神经网络模型，该模型选择简单、易解释，不需要过度参数化，同时训练有效。模型的隐藏单元为二元变量时具有以特征为基础的自然解释。作者认为隐藏变量的分布应该遵循最大关联度原则，并介绍了分层特征模型（HFM）作为满足这一原则并对特征空间进行中性先验组织的模型。 |
| [^147] | [Horizon-Free and Variance-Dependent Reinforcement Learning for Latent Markov Decision Processes.](http://arxiv.org/abs/2210.11604) | 本文研究了具有上下文后见性的 LMDP 强化学习遗憾最小化问题。通过设计一个新颖的模型基础算法框架，我们证明了一个与计划视野对数相关的 $\widetilde{O}\left(\sqrt{M \Gamma S A K}\right)$ 遗憾度上限，并对 alpha 向量的总方差进行分析。同时，我们提出了一个 $\Omega\left(\sqrt{M S A K}\right)$ 的遗憾度下限，它在 $\Gamma=2$ 时证明了我们的上界是最优的。 |
| [^148] | [Packed-Ensembles for Efficient Uncertainty Estimation.](http://arxiv.org/abs/2210.09184) | Packed-Ensembles是一种能够在标准神经网络内运行的轻量级结构化集合，它通过精心调节编码空间的维度来设计。该方法在不损失效果的情况下提高了训练和推理速度。 |
| [^149] | [Break The Spell Of Total Correlation In betaTCVAE.](http://arxiv.org/abs/2210.08794) | 本文提出一种新的迭代分解路径来打破betaTCVAE中的全相关性，从而使得VAE能够更加灵活地划分数据特征。实验结果表明模型容量和潜变量分组之间存在有趣的相关性。 |
| [^150] | [Segmentation method of U-net sheet metal engineering drawing based on CBAM attention mechanism.](http://arxiv.org/abs/2209.14102) | 本文提出了一种基于U-net和CBAM注意力机制的钣金工程图像分割方法，能够根据视觉信息自动分割特定图形单元，在自动切割方面具有高效率和准确性。 |
| [^151] | [Artificial Intelligence in Material Engineering: A review on applications of AI in Material Engineering.](http://arxiv.org/abs/2209.11234) | 本文综述了人工智能在材料工程中的最新进展，涉及材料加工、结构和材料性能研究、测量材料性能、新材料的创建和设计以及未来机遇等方面，基于机器学习的方法比传统方法更快、更准确，生成对抗网络有助于无机材料的化学成分预测和优化。 |
| [^152] | [Statistical Learning Theory for Control: A Finite Sample Perspective.](http://arxiv.org/abs/2209.05423) | 本文概述了控制领域中最重要的统计学习理论中的最近进展，这些进展主要围绕在线性系统辨识和学习方面，基于现代高维统计学和学习理论的工具，为将机器学习工具融入控制领域的人提供了自包含演示。 |
| [^153] | [Cell-Free Latent Go-Explore.](http://arxiv.org/abs/2208.14928) | 本文提出了无细胞Latent Go-Explore方法用于强化学习探索，通过学习潜在表示泛化到任何环境，实验结果表明其表现优异。 |
| [^154] | [Combining AI and AM - Improving Approximate Matching through Transformer Networks.](http://arxiv.org/abs/2208.11367) | 该论文提出了一种基于Transformer模型的Deep Learning Approximate Matching算法，该算法结合AI与AM，能够在只有原始工件的碎片可用时，可靠高效地检测黑名单中与案件相关的数据结构。 |
| [^155] | [T-RECX: Tiny-Resource Efficient Convolutional neural networks with early-eXit.](http://arxiv.org/abs/2207.06613) | 本文介绍了一种优化小型CNN模型的方法——T-RECX，通过添加早期退出中间分类器可以节省大量的时间和节省模型容量。本文的方法在CIFAR-10、CIFAR-100和ImageNet数据集上进行了实验，并与最先进的技术进行了比较，结果表明，具有早期退出功能的tiny-RECX模型可以在较小的模型和更快的推断时间内达到可比较的精度。 |
| [^156] | [DORA: Exploring outlier representations in Deep Neural Networks.](http://arxiv.org/abs/2206.04530) | 本文提出了一种名为DORA的数据不可知框架，用于分析深度神经网络中的表征空间，并可以识别不符合人类直观认知的表征。 |
| [^157] | [DEP-RL: Embodied Exploration for Reinforcement Learning in Overactuated and Musculoskeletal Systems.](http://arxiv.org/abs/2206.00484) | 通过在RL中集成微分外在可塑性（DEP），我们可以快速学习到达和运动，并在所有考虑到的任务中，在样本效率和鲁棒性方面优于当前方法。 |
| [^158] | [Comparison of meta-learners for estimating multi-valued treatment heterogeneous effects.](http://arxiv.org/abs/2205.14714) | 本文探讨了利用元学习器估计多值处理异质效应的问题，发现朴素扩展并不总是可行，提出并讨论了一些表现良好的元学习器。 |
| [^159] | [Multi-Source Transfer Learning for Deep Model-Based Reinforcement Learning.](http://arxiv.org/abs/2205.14410) | 本文提出了一种基于多源迁移学习的模块化技术，可以自动学习如何从先前学习的任务中提取有用信息，从而减少智能体在学习新任务时需要与环境互动的次数。 |
| [^160] | [On the (In)security of Peer-to-Peer Decentralized Machine Learning.](http://arxiv.org/abs/2205.08443) | 本文对去中心化学习进行了首次深入隐私分析，引入了一系列新颖的攻击方法，并证明去中心化学习并未提供比联邦学习更好的安全优势，反而增加了攻击面。而且，隐私保护配置需要全连接网络，失去了实际优势，完全打败了去中心化方法的目标。 |
| [^161] | [SIBILA: A novel interpretable ensemble of general-purpose machine learning models applied to medical contexts.](http://arxiv.org/abs/2205.06234) | SIBILA是一种集成了机器学习和深度学习模型以及可解释性算法，并能够在医疗领域中实现个性化治疗预测的方法，具有较高的准确性和可解释性。 |
| [^162] | [Framework for inferring empirical causal graphs from binary data to support multidimensional poverty analysis.](http://arxiv.org/abs/2205.06131) | 提出了一种用于推断贫困调查中的因果关系的框架，其可以揭示水、电力和资产所有权等MPI指标对贫困水平的影响最大。 |
| [^163] | [Spherical Rotation Dimension Reduction with Geometric Loss Functions.](http://arxiv.org/abs/2204.10975) | 该论文提出了一种名为SRCA的非线性降维方法，在处理高维度的低样本大小数据时，通过引入球体或椭球体，保留数据的几何结构，提高近似低维流形的效果。 |
| [^164] | [Sparsely-gated Mixture-of-Expert Layers for CNN Interpretability.](http://arxiv.org/abs/2204.10598) | 本文介绍了将稀疏门控专家层应用于计算机视觉中的CNN，并探究了这对模型可解释性的影响，同时也提出了软约束与硬约束两种方法来稳定MoE的训练。研究表明，该方法使专家可以有效关注输入的各个子领域，提高模型的性能。 |
| [^165] | [Automatic Identification of Chemical Moieties.](http://arxiv.org/abs/2203.16205) | 该论文介绍了一种自动识别化学基元的方法，使得在化学数据库中选择代表性条目、自动构建粗粒化力场以及识别反应坐标等应用成为可能。 |
| [^166] | [GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models.](http://arxiv.org/abs/2203.07281) | GrIPS是一种基于编辑的无梯度搜索方法，用于改进大型语言模型的任务指令，显著提高性能。 |
| [^167] | [Phenotyping with Positive Unlabelled Learning for Genome-Wide Association Studies.](http://arxiv.org/abs/2202.07451) | 该论文介绍了一种利用PU学习进行表型研究的全基因组联合分析方法，该方法能够通过将锚定学习和变换器结构组合来检测基因组关联，即使在减少对照组的情况下也能保持更多显著基因组关联。 |
| [^168] | [Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations and Beyond.](http://arxiv.org/abs/2202.06861) | 本文介绍了Quantus，一个可解释的AI工具包，用于详尽迅速地评估神经网络预测的解释表现，并提高领域内的透明度和可重复性。 |
| [^169] | [PEg TRAnsfer Workflow recognition challenge report: Does multi-modal data improve recognition?.](http://arxiv.org/abs/2202.05821) | 本文介绍了PETRAW挑战，探讨基于视频、运动学和分割数据进行手术工作流程识别的方法，结果显示多模态数据可以提高识别准确度。 |
| [^170] | [On learning Whittle index policy for restless bandits with scalable regret.](http://arxiv.org/abs/2202.03463) | 本文提出了一种基于不安定赌博机模型的可扩展模型基于强化学习算法，相比于RL算法的常规遗憾界，该方法具有更小的遗憾界。 |
| [^171] | [Quantization Backdoors to Deep Learning Commercial Frameworks.](http://arxiv.org/abs/2108.09187) | 本文揭示了商业框架中潜在的深度学习模型后门安全漏洞，可以通过量化攻击实现后门触发并逃避检测，从而危及已部署的模型安全，可能导致未经授权的数据访问。 |
| [^172] | [Stochastic Optimization under Distributional Drift.](http://arxiv.org/abs/2108.07356) | 本文提供了一种在分布漂移下优化凸函数的新方法，经数值实验证明在低漂移-噪声比的情况下，近端随机梯度方法采用步长衰减策略可显著提升跟踪效率。 |
| [^173] | [Derivative-free Alternating Projection Algorithms for General Nonconvex-Concave Minimax Problems.](http://arxiv.org/abs/2108.00473) | 本文提出针对非凸-凹极小极大问题的无导数交替投影算法，包括光滑问题的交替随机梯度投影算法（ZO-AGP），以及块状非光滑问题的分块交替随机近端梯度算法（ZO-BAPG）。这些算法具有较少的函数值估计和较高的迭代复杂度。 |
| [^174] | [Incompatibility Clustering as a Defense Against Backdoor Poisoning Attacks.](http://arxiv.org/abs/2105.03692) | 本文提出了一种基于不兼容性的聚类机制，该机制可以将数据集划分为由训练过程的目标所定义且具有意义的聚类，并有效减轻后门攻击的影响。 |

# 详细

[^1]: 将人置于其场景中：考虑可供性的人类插入

    Putting People in Their Place: Affordance-Aware Human Insertion into Scenes. (arXiv:2304.14406v1 [cs.CV])

    [http://arxiv.org/abs/2304.14406](http://arxiv.org/abs/2304.14406)

    该论文提出了一种方法，在场景中插入现实姿态的人，以及合成能够在场景中与人类进行自然交互的场景。

    

    我们研究了推断场景可供性的问题，提出了一种实现在场景中插入人的方法。给定有标记区域的场景图像和一个人的图像，我们将这个人插入到场景中，并尊重场景可供性。我们的模型可以根据场景上下文推断出一组现实的姿态，重新调整参考人的姿态，并调和构图。我们通过学习在视频剪辑中重新姿态人类来以自我监督的方式设置任务。我们在一组包含 240 万个视频剪辑的数据集上使用大规模扩散模型进行训练，该模型会产生多样的合理姿态，同时尊重场景上下文。鉴于学习到的人 - 场景组合，我们的模型还可以在没有条件的情况下感应到真实的人和场景，以及启用交互式编辑。定量评估显示出，与以前的工作相比，我们的方法合成了更真实的人类外观和更自然的人 - 场景交互。

    We study the problem of inferring scene affordances by presenting a method for realistically inserting people into scenes. Given a scene image with a marked region and an image of a person, we insert the person into the scene while respecting the scene affordances. Our model can infer the set of realistic poses given the scene context, re-pose the reference person, and harmonize the composition. We set up the task in a self-supervised fashion by learning to re-pose humans in video clips. We train a large-scale diffusion model on a dataset of 2.4M video clips that produces diverse plausible poses while respecting the scene context. Given the learned human-scene composition, our model can also hallucinate realistic people and scenes when prompted without conditioning and also enables interactive editing. A quantitative evaluation shows that our method synthesizes more realistic human appearance and more natural human-scene interactions than prior work.
    
[^2]: 让它变成现实: 用于任何图像反转和编辑的Steering StyleGAN

    Make It So: Steering StyleGAN for Any Image Inversion and Editing. (arXiv:2304.14403v1 [cs.CV])

    [http://arxiv.org/abs/2304.14403](http://arxiv.org/abs/2304.14403)

    本文提出了一种新颖的GAN反演方法Make It So，在噪声空间中操作使得保留了编辑能力，即使是在域外图像方面，比现有方法更准确，反演精度提高五倍，并且对于复杂的室内场景，实现了十倍更好的编辑质量。

    

    StyleGAN的分离风格表示使得通过操作潜在变量进行强大的图像编辑成为可能，但准确地将现实世界的图像映射到它们的潜在变量（GAN反演）仍然是一个挑战。现有的GAN反演方法在维持编辑方向和生成逼真结果方面遇到困难。为了解决这些限制，我们提出了Make It So，一种新颖的GAN反演方法，它在$\mathcal{Z}$（噪声）空间而不是典型的$\mathcal{W}$（潜在风格）空间中运行。Make It So保留了编辑能力，即使是在域外图像方面。这是以前方法中被忽视的关键属性。我们的定量评估表明，Make It So在反演精度方面比最先进的PTI方法~\cite{roich2021pivotal}提高了五倍，并且对于复杂的室内场景，实现了十倍更好的编辑质量。

    StyleGAN's disentangled style representation enables powerful image editing by manipulating the latent variables, but accurately mapping real-world images to their latent variables (GAN inversion) remains a challenge. Existing GAN inversion methods struggle to maintain editing directions and produce realistic results.  To address these limitations, we propose Make It So, a novel GAN inversion method that operates in the $\mathcal{Z}$ (noise) space rather than the typical $\mathcal{W}$ (latent style) space. Make It So preserves editing capabilities, even for out-of-domain images. This is a crucial property that was overlooked in prior methods. Our quantitative evaluations demonstrate that Make It So outperforms the state-of-the-art method PTI~\cite{roich2021pivotal} by a factor of five in inversion accuracy and achieves ten times better edit quality for complex indoor scenes.
    
[^3]: 自我监督学习和联邦学习的制造业模型泛化最大化

    Maximizing Model Generalization for Manufacturing with Self-Supervised Learning and Federated Learning. (arXiv:2304.14398v1 [cs.LG])

    [http://arxiv.org/abs/2304.14398](http://arxiv.org/abs/2304.14398)

    本研究提出了一种利用自我监督学习和联邦学习提高制造业模型的泛化能力，在处理未标记的和有限的状况监测数据以及领域移位时具有较好的效果，同时通过联邦学习保护了数据隐私和提高了计算效率。

    

    深度学习可以在没有手动设计的统计特征的情况下，从原始的状况监测数据中诊断故障和评估机器健康度。然而，现有的深度学习方法仍然极其困难适用于实际制造应用。机器数据通常是未标记的，只有很少的健康条件（例如，仅有正常操作数据）。此外，由于工艺参数的变化和新的故障类别的出现，模型经常遇到域的移位。传统的监督学习可能难以学习紧凑、有区别力的表示，并且不能推广到这些未见过的目标域，因为它依赖于拥有丰富的类来划分特征空间和决策边界。通过领域自适应进行的迁移学习尝试将这些模型适应到未标记的目标域，但假定了类似的子结构，在新的故障出现时可能不存在。本研究提出了一种专注于最大化源域特征普适性的方法，并应用自我监督学习结合联邦学习来改善实际制造应用中的故障诊断和机器健康评估。所提出的方法解决了未标记和有限状况监测数据以及域移位的问题，同时通过联邦学习实现了高效的计算和隐私保护。

    Deep Learning (DL) can diagnose faults and assess machine health from raw condition monitoring data without manually designed statistical features. However, practical manufacturing applications remain extremely difficult for existing DL methods. Machine data is often unlabeled and from very few health conditions (e.g., only normal operating data). Furthermore, models often encounter shifts in domain as process parameters change and new categories of faults emerge. Traditional supervised learning may struggle to learn compact, discriminative representations that generalize to these unseen target domains since it depends on having plentiful classes to partition the feature space with decision boundaries. Transfer Learning (TL) with domain adaptation attempts to adapt these models to unlabeled target domains but assumes similar underlying structure that may not be present if new faults emerge. This study proposes focusing on maximizing the feature generality on the source domain and apply
    
[^4]: 基于能量模型的零样本场景重新排列规划器

    Energy-based Models as Zero-Shot Planners for Compositional Scene Rearrangement. (arXiv:2304.14391v1 [cs.RO])

    [http://arxiv.org/abs/2304.14391](http://arxiv.org/abs/2304.14391)

    本文提出一种基于能量模型的零样本场景重新排列规划器，通过语言指导的空间概念来实现长指令以及在训练时从未见过的空间概念组合。本文的模型在指令导向操作基准测试以及组合指令基准测试中表现良好，优于基于语言表达的最先进方法，并且可以成功地解决之前从未见过的复杂指令和场景。

    

    本文致力于开发一个场景重排框架，可以解释长指令以及在训练时从未见过的空间概念组合。我们提出使用相对对象排列的能量函数来表示语言指导的空间概念。语言解析器将指令映射到相应的能量函数，而开放式视觉语言模型将它们的参数基于场景中的相关对象进行修正。通过梯度下降求解能量函数的总和，并利用基于本地计算机视觉的策略将对象重新定位到推断的目标位置，即可生成目标场景配置。我们在已建立的指令导向操作基准测试以及我们提出的组合指令基准测试中测试了模型，结果表明，我们的模型的绩效优于基于语言表达的最先进方法，并且可以成功地解决之前从未见过的复杂指令和场景。

    Language is compositional; an instruction can express multiple relation constraints to hold among objects in a scene that a robot is tasked to rearrange. Our focus in this work is an instructable scene rearranging framework that generalizes to longer instructions and to spatial concept compositions never seen at training time. We propose to represent language-instructed spatial concepts with energy functions over relative object arrangements. A language parser maps instructions to corresponding energy functions and an open-vocabulary visual-language model grounds their arguments to relevant objects in the scene. We generate goal scene configurations by gradient descent on the sum of energy functions, one per language predicate in the instruction. Local vision-based policies then relocate objects to the inferred goal locations. We test our model on established instruction-guided manipulation benchmarks, as well as benchmarks of compositional instructions we introduce. We show our model 
    
[^5]: 可微分顺序蒙特卡洛采样器中的重采样梯度问题

    Resampling Gradients Vanish in Differentiable Sequential Monte Carlo Samplers. (arXiv:2304.14390v1 [stat.ML])

    [http://arxiv.org/abs/2304.14390](http://arxiv.org/abs/2304.14390)

    本文提出了一种扩展可微分AIS的方法，通过引入类似于顺序蒙特卡洛的重采样步骤来避免粒子滤波中的梯度方差问题。

    

    退火重要性采样（AIS）是将粒子沿着一个马尔科夫链从可计算的初始分布移动到不可计算的目标分布。最近提出的可微分AIS（DAIS）允许对AIS的转移核和分布进行高效优化。然而，我们观察到DAIS中存在低有效样本量，表明分布退化。因此，我们借鉴顺序蒙特卡洛方法提出了一个重采样步骤来扩展DAIS。令人惊讶的是，我们在经验上发现，也可以在理论上解释，无需通过重采样步骤进行微分，这避免了粒子滤波中观察到的梯度方差问题。

    Annealed Importance Sampling (AIS) moves particles along a Markov chain from a tractable initial distribution to an intractable target distribution. The recently proposed Differentiable AIS (DAIS) (Geffner and Domke, 2021; Zhang et al., 2021) enables efficient optimization of the transition kernels of AIS and of the distributions. However, we observe a low effective sample size in DAIS, indicating degenerate distributions. We thus propose to extend DAIS by a resampling step inspired by Sequential Monte Carlo. Surprisingly, we find empirically-and can explain theoretically-that it is not necessary to differentiate through the resampling step which avoids gradient variance issues observed in similar approaches for Particle Filters (Maddison et al., 2017; Naesseth et al., 2018; Le et al., 2018).
    
[^6]: 带有贝叶斯说服的动态定价和学习

    Dynamic Pricing and Learning with Bayesian Persuasion. (arXiv:2304.14385v1 [cs.GT])

    [http://arxiv.org/abs/2304.14385](http://arxiv.org/abs/2304.14385)

    本研究提出了一种计算有效的在线算法，在没有先验知识的情况下，自适应学习最优定价和广告策略，达到次线性后悔。

    

    我们考虑了一个新颖的动态定价和学习设置，在按顺序设置产品价格的同时，卖家还预先承诺“广告方案”。也就是说，在每轮开始时，卖家可以决定提供什么样的信号来告知买家产品实际的质量。我们使用流行的贝叶斯说服框架来模拟这些信号对买家的评估和购买反应的影响，我们制定了在最大化卖方预期收入的同时找到广告方案和定价方案的最优设计问题。在没有任何先验知识的情况下，我们的目标是设计一个在线算法，该算法可以使用过去的购买反应来自适应地学习最优定价和广告策略。我们研究了算法的后悔，与最优的千里之堤价格和广告计划进行比较。我们的主要结果是一种计算有效的在线算法，即使卖家没有买家需求函数的先验知识，也可以实现与最佳固定价格和广告方案相关的次线性后悔。

    We consider a novel dynamic pricing and learning setting where in addition to setting prices of products in sequential rounds, the seller also ex-ante commits to 'advertising schemes'. That is, in the beginning of each round the seller can decide what kind of signal they will provide to the buyer about the product's quality upon realization. Using the popular Bayesian persuasion framework to model the effect of these signals on the buyers' valuation and purchase responses, we formulate the problem of finding an optimal design of the advertising scheme along with a pricing scheme that maximizes the seller's expected revenue. Without any apriori knowledge of the buyers' demand function, our goal is to design an online algorithm that can use past purchase responses to adaptively learn the optimal pricing and advertising strategy. We study the regret of the algorithm when compared to the optimal clairvoyant price and advertising scheme.  Our main result is a computationally efficient onlin
    
[^7]: 模拟形式转换器用于少样本3D解析

    Analogy-Forming Transformers for Few-Shot 3D Parsing. (arXiv:2304.14382v1 [cs.CV])

    [http://arxiv.org/abs/2304.14382](http://arxiv.org/abs/2304.14382)

    "模拟网络"模型在3D物体场景分割中采用类比推理，通过在内存中检索相关场景并预测类似结构进行分割，能够在一发、少发或多发学习中得出相似的解析，与最新的3D分割变压器模型相竞争。

    

    我们提出了一种称为“模拟网络”的模型，它在一组有标记的结构化3D场景中显式地编码领域知识（作为模型参数的一部分），并通过类比推理对3D物体场景进行分割：我们的模型首先从内存中检索相关场景及其相应的部分结构，然后通过端到端可学习的调制机制为输入场景预测类似的部分结构，而不是直接将场景映射到部分分割。通过对多个检索的记忆进行条件控制，预测混合匹配检索记忆的结构合成。在“模拟网络”中，一发、少发或多发学习被一致地处理，通过对适当的记忆集进行条件谓词，无论是从单个、少数还是许多存储实例中继承相似的解析。我们展示了“模拟网络”在许多样本情况下与最新的3D分割变压器模型相竞争。

    We present Analogical Networks, a model that encodes domain knowledge explicitly, in a collection of structured labelled 3D scenes, in addition to implicitly, as model parameters, and segments 3D object scenes with analogical reasoning: instead of mapping a scene to part segments directly, our model first retrieves related scenes from memory and their corresponding part structures, and then predicts analogous part structures for the input scene, via an end-to-end learnable modulation mechanism. By conditioning on more than one retrieved memories, compositions of structures are predicted, that mix and match parts across the retrieved memories. One-shot, few-shot or many-shot learning are treated uniformly in Analogical Networks, by conditioning on the appropriate set of memories, whether taken from a single, few or many memory exemplars, and inferring analogous parses. We show Analogical Networks are competitive with state-of-the-art 3D segmentation transformers in many-shot settings, a
    
[^8]: 功能扩散映射

    Functional Diffusion Maps. (arXiv:2304.14378v1 [cs.LG])

    [http://arxiv.org/abs/2304.14378](http://arxiv.org/abs/2304.14378)

    本研究关注一种非线性流形学习方法：扩散映射。本文阐述如何将这种方法应用于功能数据，并将其与功能主成分分析进行比较。

    

    如今，许多现实世界的数据集可以被视为是功能性的，也就是说生成它们的过程是连续的。这种类型数据的一个基本特性是，理论上它们属于无限维空间。尽管在实践中，我们通常只能得到有限数量的观察结果，它们仍然是高维的，因此降维方法至关重要。在这方面，功能数据分析的主要现有方法是功能主成分分析。尽管如此，这种经典技术假设数据位于一个线性流形中，因此当这个假设不成立时可能会出现问题。本研究聚焦于一种非线性流形学习方法：扩散映射。本文解释了如何将这种多变量方法扩展到功能数据，并将其行为与功能主成分分析在不同的模拟和实际例子中进行了比较。

    Nowadays many real-world datasets can be considered as functional, in the sense that the processes which generate them are continuous. A fundamental property of this type of data is that in theory they belong to an infinite-dimensional space. Although in practice we usually receive finite observations, they are still high-dimensional and hence dimensionality reduction methods are crucial. In this vein, the main state-of-the-art method for functional data analysis is Functional PCA. Nevertheless, this classic technique assumes that the data lie in a linear manifold, and hence it could have problems when this hypothesis is not fulfilled. In this research, attention has been placed on a non-linear manifold learning method: Diffusion Maps. The article explains how to extend this multivariate method to functional data and compares its behavior against Functional PCA over different simulated and real examples.
    
[^9]: 伪哈密顿神经网络用于学习偏微分方程

    Pseudo-Hamiltonian neural networks for learning partial differential equations. (arXiv:2304.14374v1 [cs.LG])

    [http://arxiv.org/abs/2304.14374](http://arxiv.org/abs/2304.14374)

    本文介绍了一种新方法伪哈密顿神经网络(PHNN)，可以用于学习偏微分方程。相比基线模型，PHNN表现更为优越，模型可应用于去除或改变外力情况并可分别得到三个不同物理解释的部分。

    

    最近提出了伪哈密顿神经网络(PHNN)来学习可以用普通微分方程建模的动力系统。本文将该方法扩展到了偏微分方程。所得模型由高达三个神经网络，模拟代表守恒、耗散和外力的项以及可以学习或为先前知识的离散卷积算子构成。我们通过数值结果表明PHNN相比单个神经网络建模的基线模型具有更优越的性能。此外，由于PHNN模型由三个具有不同物理解释的部分组成，可以分别研究这些部分以获得对系统的洞察，并且即使去除或改变外力，所学得的模型仍然适用。

    Pseudo-Hamiltonian neural networks (PHNN) were recently introduced for learning dynamical systems that can be modelled by ordinary differential equations. In this paper, we extend the method to partial differential equations. The resulting model is comprised of up to three neural networks, modelling terms representing conservation, dissipation and external forces, and discrete convolution operators that can either be learned or be prior knowledge. We demonstrate numerically the superior performance of PHNN compared to a baseline model that models the full dynamics by a single neural network. Moreover, since the PHNN model consists of three parts with different physical interpretations, these can be studied separately to gain insight into the system, and the learned model is applicable also if external forces are removed or changed.
    
[^10]: 二维语义分割神经场调节策略研究

    Neural Field Conditioning Strategies for 2D Semantic Segmentation. (arXiv:2304.14371v1 [cs.CV])

    [http://arxiv.org/abs/2304.14371](http://arxiv.org/abs/2304.14371)

    本文研究了神经场作为二维语义分割解码器，提出了三种调节方法，结果表明通过交叉注意力进行调节的方法最佳，与基于CNN的方法相当竞争。

    

    神经场是将坐标映射到期望信号的神经网络。如果神经场应该共同模拟多个信号而不是只记忆一个，则需要在描述所处理信号的潜在代码上进行调节。本文探讨了使用神经场作为二维语义分割解码器的方法，并比较了三种不同的调节方法：潜在代码的简单连接，特征逐元素线性调节以及交叉注意力，同时使用描述完整图像或图像局部区域的潜在代码。实验结果表明，不同的调节策略具有显著差异。此外，通过交叉注意力进行调节的结果最佳，与基于CNN的语义分割解码器相当竞争。

    Neural fields are neural networks which map coordinates to a desired signal. When a neural field should jointly model multiple signals, and not memorize only one, it needs to be conditioned on a latent code which describes the signal at hand. Despite being an important aspect, there has been little research on conditioning strategies for neural fields. In this work, we explore the use of neural fields as decoders for 2D semantic segmentation. For this task, we compare three conditioning methods, simple concatenation of the latent code, Feature Wise Linear Modulation (FiLM), and Cross-Attention, in conjunction with latent codes which either describe the full image or only a local region of the image. Our results show a considerable difference in performance between the examined conditioning strategies. Furthermore, we show that conditioning via Cross-Attention achieves the best results and is competitive with a CNN-based decoder for semantic segmentation.
    
[^11]: 从运动观测中学习神经本构法来实现可推广的PDE动力学研究

    Learning Neural Constitutive Laws From Motion Observations for Generalizable PDE Dynamics. (arXiv:2304.14369v1 [cs.LG])

    [http://arxiv.org/abs/2304.14369](http://arxiv.org/abs/2304.14369)

    本文提出了一种混合神经网络和偏微分方程的方法，用于从运动观测中学习可推广的PDE动力学，并介绍了一种新框架"神经本构法"，该框架利用了一种严格保证标准本构先验的网络架构，以此来学习本构模型。

    

    我们提出了一种混合神经网络(NN)和偏微分方程(PDE)方法,用于从运动观测中学习可推广的PDE动力学。许多NN方法学习端到端的模型,隐含地建模了所建立的PDE和本构模型(或材料模型)。这些方法没有明确的PDE知识,不能保证物理正确性,具有有限的广泛适用性。我们认为,所建立的PDEs通常是众所周知的,应该明确强制执行,而不是学习。相反,本构模型由于其数据拟合性质,特别适合于学习。为此,我们介绍了一种称为“神经本构法”(NCLaw)的新框架,它利用了一种严格保证标准本构先验的网络架构,包括旋转等变性和未变形状态平衡。我们将这个网络嵌入可微分的模拟中,通过最小化基于模拟和m之间的差异的损失函数来训练模型

    We propose a hybrid neural network (NN) and PDE approach for learning generalizable PDE dynamics from motion observations. Many NN approaches learn an end-to-end model that implicitly models both the governing PDE and constitutive models (or material models). Without explicit PDE knowledge, these approaches cannot guarantee physical correctness and have limited generalizability. We argue that the governing PDEs are often well-known and should be explicitly enforced rather than learned. Instead, constitutive models are particularly suitable for learning due to their data-fitting nature. To this end, we introduce a new framework termed "Neural Constitutive Laws" (NCLaw), which utilizes a network architecture that strictly guarantees standard constitutive priors, including rotation equivariance and undeformed state equilibrium. We embed this network inside a differentiable simulation and train the model by minimizing a loss function based on the difference between the simulation and the m
    
[^12]: CONSCENDI: 一种反对比且场景引导的蒸馏方法来为虚拟助手构建防护栏模型

    CONSCENDI: A Contrastive and Scenario-Guided Distillation Approach to Guardrail Models for Virtual Assistants. (arXiv:2304.14364v1 [cs.CL])

    [http://arxiv.org/abs/2304.14364](http://arxiv.org/abs/2304.14364)

    本文提出了一种名为CONSCENDI的蒸馏方法，用于构建防护栏模型，以监控任务型虚拟助手的输出。关键方法包括场景增强生成和对比训练样例。这种方法产生了一组多样化的违反规则的对话训练集，并且可以更好地检测代理的输出是否符合设计者指定的规则。

    

    随着GPT-4等越来越强大的语言模型的出现，新一代的基于任务的虚拟助手应运而生。这些对话系统可以根据客户的具体用例进行定制，但确保代理生成的文本仅符合提示指令中设计者指定的规则是具有挑战性的。因此，聊天机器人设计师通常使用另一个称为防护栏模型的模型来验证代理输出是否与其规则和约束对齐。我们探索了使用蒸馏方法来构建防护栏模型，以监控使用GPT-4中的训练数据的第一个模型的输出。我们发现，我们的CONSCENDI过程包括两个关键步骤：场景增强生成和对比训练样例。在生成对话数据时，我们会生成一组违反规则的场景，这些场景列举了违反规则的多样化高级方式。这种场景引导方法产生了一组多样化的违反规则的对话训练集，并且它使得模型更容易检测到代理生成的文本是否符合设计者指定的规则。

    A wave of new task-based virtual assistants has been fueled by increasingly powerful large language models, such as GPT-4. These conversational agents can be customized to serve customer-specific use cases, but ensuring that agent-generated text conforms to designer-specified rules included in prompt instructions alone is challenging. Therefore, chatbot designers often use another model, called a guardrail model, to verify that the agent output aligns with their rules and constraints. We explore using a distillation approach to guardrail models to monitor the output of the first model using training data from GPT-4. We find two crucial steps to our CONSCENDI process: scenario-augmented generation and contrastive training examples. When generating conversational data, we generate a set of rule-breaking scenarios, which enumerate a diverse set of high-level ways a rule can be violated. This scenario-guided approach produces a diverse training set of rule-violating conversations, and it p
    
[^13]: ChatGPT的黑暗面：来自随机鹦鹉和幻觉的法律和伦理挑战

    The Dark Side of ChatGPT: Legal and Ethical Challenges from Stochastic Parrots and Hallucination. (arXiv:2304.14347v1 [cs.CY])

    [http://arxiv.org/abs/2304.14347](http://arxiv.org/abs/2304.14347)

    ChatGPT带来的大语言模型(LLMs)虽然有很多优势，但是随机鹦鹉和幻觉等新的法律和伦理风险也随之而来。欧洲AI监管范式需要进一步发展以减轻这些风险。

    

    随着ChatGPT的推出，大语言模型（LLMs）正在动摇我们整个社会，快速改变我们的思维、创造和生活方式。然而，随着随机鹦鹉和幻觉等新的法律和伦理风险出现，新兴LLMs也带来了许多挑战。欧盟是第一个将重点放在AI模型监管上的司法管辖区。然而，新LLMs带来的风险可能会被新兴的欧盟监管范式所低估。因此，本函告警示欧洲AI监管范式必须进一步发展以减轻这些风险。

    With the launch of ChatGPT, Large Language Models (LLMs) are shaking up our whole society, rapidly altering the way we think, create and live. For instance, the GPT integration in Bing has altered our approach to online searching. While nascent LLMs have many advantages, new legal and ethical risks are also emerging, stemming in particular from stochastic parrots and hallucination. The EU is the first and foremost jurisdiction that has focused on the regulation of AI models. However, the risks posed by the new LLMs are likely to be underestimated by the emerging EU regulatory paradigm. Therefore, this correspondence warns that the European AI regulatory paradigm must evolve further to mitigate such risks.
    
[^14]: 实现高效和全面的城市时空预测：一个统一的库和性能基准

    Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction: A Unified Library and Performance Benchmark. (arXiv:2304.14343v1 [cs.LG])

    [http://arxiv.org/abs/2304.14343](http://arxiv.org/abs/2304.14343)

    本研究提出了一种称为原子文件的统一空间时间数据存储格式，开发了一个名为LibCity的开源库，重新构建了65个空间时间预测模型，并收集了55个空间时间数据集。同时还提出了城市时空预测模型的性能基准，为这一领域提供了一个可靠的评估工具。

    

    随着深度学习技术的不断推进和城市时空数据的积累，越来越多的深度学习模型被提出来解决城市时空预测问题。然而，现有领域存在许多限制，包括开放数据以各种格式存在，使用困难，极少数论文公开其代码和数据，以及开源模型经常使用不同的框架和平台，使得比较具有挑战性。迫切需要一个统一的框架来实施和评估这些方法。为解决这些问题，我们提供了一个城市时空预测的综合评估，并提出了一种称为原子文件的统一空间时间数据存储格式。我们还提出了一个名为LibCity的开源库，为研究人员提供了一个可靠的实验工具和一个方便的开发框架。在这个库中，我们已经重新构建了65个空间时间预测模型，并收集了55个空间时间数据集。此外，我们还引入了一个城市时空预测模型性能基准，包括效率和有效性度量，以进行公平比较。在这个基准上的实验结果证明了我们提出的统一库和基准的有用性和有效性。

    As deep learning technology advances and more urban spatial-temporal data accumulates, an increasing number of deep learning models are being proposed to solve urban spatial-temporal prediction problems. However, there are limitations in the existing field, including open-source data being in various formats and difficult to use, few papers making their code and data openly available, and open-source models often using different frameworks and platforms, making comparisons challenging. A standardized framework is urgently needed to implement and evaluate these methods. To address these issues, we provide a comprehensive review of urban spatial-temporal prediction and propose a unified storage format for spatial-temporal data called atomic files. We also propose LibCity, an open-source library that offers researchers a credible experimental tool and a convenient development framework. In this library, we have reproduced 65 spatial-temporal prediction models and collected 55 spatial-temp
    
[^15]: MarsEclipse在SemEval-2023任务3中的多语言和多标签框架检测及对比学习方法

    MarsEclipse at SemEval-2023 Task 3: Multi-Lingual and Multi-Label Framing Detection with Contrastive Learning. (arXiv:2304.14339v1 [cs.CL])

    [http://arxiv.org/abs/2304.14339](http://arxiv.org/abs/2304.14339)

    本文描述了一种在多语言设置下使用多标签对比损失来微调大型预训练语言模型的方法，取得了在五种语言上最好的结果。

    

    本文描述了我们在SemEval-2023任务3的子任务2上进行框架检测的系统。我们使用了多标签对比损失来微调大型预训练语言模型的多语言设置，取得了非常有竞争力的结果：我们的系统在官方测试集和官方排行榜上排名第一，对于我们有训练数据并能进行微调的六种语言中的五种语言。在这里，我们描述了我们的实验设置以及各种消融研究。我们的系统代码可在https://github.com/QishengL/SemEval2023上获得。

    This paper describes our system for SemEval-2023 Task 3 Subtask 2 on Framing Detection. We used a multi-label contrastive loss for fine-tuning large pre-trained language models in a multi-lingual setting, achieving very competitive results: our system was ranked first on the official test set and on the official shared task leaderboard for five of the six languages for which we had training data and for which we could perform fine-tuning. Here, we describe our experimental setup, as well as various ablation studies. The code of our system is available at https://github.com/QishengL/SemEval2023
    
[^16]: 成语、探测和危险的事物：基于结构探测的词向量成语性研究。

    Idioms, Probing and Dangerous Things: Towards Structural Probing for Idiomaticity in Vector Space. (arXiv:2304.14333v1 [cs.CL])

    [http://arxiv.org/abs/2304.14333](http://arxiv.org/abs/2304.14333)

    本文利用结构探测方法研究了成语在静态和上下文嵌入中的编码情况，并发现仍未确定成语性是否在向量范数中编码，提出了改进数据集以进行探测分析的方向。

    

    本文旨在通过一种结构探测方法更深入地了解成语信息如何被嵌入到词嵌入中。我们重新利用了现有的英语动词复合词语（MWE）数据集来适应探测框架，并对静态（GloVe）和上下文（BERT）嵌入进行了比较探测研究。我们的实验表明，这两种方法都以不同程度编码了一些成语信息，但对于成语性是否在向量范数中编码给出了冲突的证据，这仍然是个未解之谜。我们还确定了所使用数据集的一些局限性，并强调了改进其适用性进行探测分析的重要方向。

    The goal of this paper is to learn more about how idiomatic information is structurally encoded in embeddings, using a structural probing method. We repurpose an existing English verbal multi-word expression (MWE) dataset to suit the probing framework and perform a comparative probing study of static (GloVe) and contextual (BERT) embeddings. Our experiments indicate that both encode some idiomatic information to varying degrees, but yield conflicting evidence as to whether idiomaticity is encoded in the vector norm, leaving this an open question. We also identify some limitations of the used dataset and highlight important directions for future work in improving its suitability for a probing analysis.
    
[^17]: 关于Gibbs算法元学习的泛化误差分析

    On the Generalization Error of Meta Learning for the Gibbs Algorithm. (arXiv:2304.14332v1 [cs.LG])

    [http://arxiv.org/abs/2304.14332](http://arxiv.org/abs/2304.14332)

    本文通过Gibbs算法来分析元学习算法的泛化能力，提供了精确的元泛化误差刻画和新的泛化误差上界。

    

    本文通过Gibbs算法来分析联合训练元学习算法的泛化能力。我们对元Gibbs算法的期望元泛化误差进行了精确的刻画，基于对称化KL信息，该信息度量了所有元训练数据集与输出参数之间的依赖关系，包括任务特定和元参数。此外，我们在Steinke和Zakynthinou (2020)以及Hellstrom 和 Durisi (2022)分别引入的超样本和超任务框架中，利用条件对称KL信息，为超：任务Gibbs算法提供了精确的元泛化误差刻画。我们的结果还使我们能够为这些Gibbs算法提供新的分布无关的泛化误差上界，适用于元学习。

    We analyze the generalization ability of joint-training meta learning algorithms via the Gibbs algorithm. Our exact characterization of the expected meta generalization error for the meta Gibbs algorithm is based on symmetrized KL information, which measures the dependence between all meta-training datasets and the output parameters, including task-specific and meta parameters. Additionally, we derive an exact characterization of the meta generalization error for the super-task Gibbs algorithm, in terms of conditional symmetrized KL information within the super-sample and super-task framework introduced in Steinke and Zakynthinou (2020) and Hellstrom and Durisi (2022) respectively. Our results also enable us to provide novel distribution-free generalization error upper bounds for these Gibbs algorithms applicable to meta learning.
    
[^18]: 学习外推：一种传导方法

    Learning to Extrapolate: A Transductive Approach. (arXiv:2304.14329v1 [cs.LG])

    [http://arxiv.org/abs/2304.14329](http://arxiv.org/abs/2304.14329)

    该论文提出了一种解决在支持外进行推广的问题的传导方法，该方法通过“传导”重新参数化将支持外的外推问题转换为支持内组合泛化问题的问题，从而让机器学习系统保留过度参数化函数逼近器的能力，并能够在某些条件下进行外推。

    

    机器学习系统，特别是拥有过度参数化的深度神经网络，可以推广到从训练数据相同分布中提取的新测试实例。然而，在支持外的测试点上评估时，它们表现不佳。在这项工作中，我们解决开发机器学习系统的问题，使其保留过度参数化函数逼近器的能力，同时在可能时使推广到支持外的测试点外推。通过注意在某些条件下，“传导”重新参数化可以将支持外的外推问题转换为支持内组合泛化问题的问题，从而实现这一目的。我们提出了一种基于双线性嵌入的简单策略，以实现这种类型的组合泛化，从而在某些条件下解决了支持外推问题。我们将实例化一个适用于各种监督学习和模仿学习任务的简单实用算法。

    Machine learning systems, especially with overparameterized deep neural networks, can generalize to novel test instances drawn from the same distribution as the training data. However, they fare poorly when evaluated on out-of-support test points. In this work, we tackle the problem of developing machine learning systems that retain the power of overparameterized function approximators while enabling extrapolation to out-of-support test points when possible. This is accomplished by noting that under certain conditions, a "transductive" reparameterization can convert an out-of-support extrapolation problem into a problem of within-support combinatorial generalization. We propose a simple strategy based on bilinear embeddings to enable this type of combinatorial generalization, thereby addressing the out-of-support extrapolation problem under certain conditions. We instantiate a simple, practical algorithm applicable to various supervised learning and imitation learning tasks.
    
[^19]: 一种针对带长期约束的约束MDPs的双赢算法

    A Best-of-Both-Worlds Algorithm for Constrained MDPs with Long-Term Constraints. (arXiv:2304.14326v1 [cs.LG])

    [http://arxiv.org/abs/2304.14326](http://arxiv.org/abs/2304.14326)

    本文提出了一种针对约束MDPs的双赢算法，能够处理奖励和约束随机或敌对的情况。

    

    本文研究了环形约束马尔科夫决策过程（CMDPs）的在线学习，其中学习者的目标是在收集尽可能多的奖励的同时，在学习过程中保证满足一些长期约束。奖励和约束可以随机或敌对地选择，并且转移函数对学习者是未知的。虽然在经典的无约束MDPs中的在线学习在过去几年中受到了大量关注，但CMDP的设置仍然大部分未被探索。这一点令人惊讶，因为在实际应用中，例如自动驾驶、自动投标和推荐系统中，通常存在额外的约束和规范，代理必须在学习过程中遵守这些规定。本文提出了一种面向长期约束的CMDPs的双赢算法。我们的算法能够处理奖励和约束随机或敌对的情况。

    We study online learning in episodic constrained Markov decision processes (CMDPs), where the goal of the learner is to collect as much reward as possible over the episodes, while guaranteeing that some long-term constraints are satisfied during the learning process. Rewards and constraints can be selected either stochastically or adversarially, and the transition function is not known to the learner. While online learning in classical unconstrained MDPs has received considerable attention over the last years, the setting of CMDPs is still largely unexplored. This is surprising, since in real-world applications, such as, e.g., autonomous driving, automated bidding, and recommender systems, there are usually additional constraints and specifications that an agent has to obey during the learning process. In this paper, we provide the first best-of-both-worlds algorithm for CMDPs with long-term constraints. Our algorithm is capable of handling settings in which rewards and constraints are
    
[^20]: 通过机器学习对库珀带内天体分类的平面测量

    A Measurement of the Kuiper Belt's Mean Plane From Objects Classified By Machine Learning. (arXiv:2304.14312v1 [astro-ph.EP])

    [http://arxiv.org/abs/2304.14312](http://arxiv.org/abs/2304.14312)

    本论文利用机器学习对库珀带内天体分类，测量了库珀带的平面。结果表明，非共振库珀带和古典库珀带与太阳系不变平面非常接近，但有区别，置信度大于99.7%。

    

    通过观测数据对库珀带的平面进行测量是为了测试太阳系动力学模型潜在性的一项热点研究。最近的测量结果存在不一致。本文报道了一项库珀带平面的测量，其样本量是之前测量的两倍以上。我们使用机器学习对轨道已确定的观测库珀带内非共振天体进行分类。我们通过Monte Carlo过程估计测量误差。我们发现非共振库珀带（半长轴范围35-150 au）和古典库珀带（半长轴范围42-48 au）的整体平面与太阳系不变平面非常接近（误差约为0.7度），但两者有区别，置信度大于99.7%。当把样本分成较小的半长轴范围时，我们发现测得的平面大多与太阳系不变平面的距离小于1.5度。

    Mean plane measurements of the Kuiper Belt from observational data are of interest for their potential to test dynamical models of the solar system. Recent measurements have yielded inconsistent results. Here we report a measurement of the Kuiper Belt's mean plane with a sample size more than twice as large as in previous measurements. The sample of interest is the non-resonant Kuiper belt objects, which we identify by using machine learning on the observed Kuiper Belt population whose orbits are well-determined. We estimate the measurement error with a Monte Carlo procedure. We find that the overall mean plane of the non-resonant Kuiper Belt (semimajor axis range 35-150 au) and also that of the classical Kuiper Belt (semimajor axis range 42-48 au) are both close to (within about 0.7 degrees) but distinguishable from the invariable plane of the solar system to greater than 99.7% confidence. When binning the sample into smaller semimajor axis bins, we find the measured mean plane mostly
    
[^21]: 利用滑雪装置应变传感器对雪进行分类的方法

    A Method for Classifying Snow Using Ski-Mounted Strain Sensors. (arXiv:2304.14307v1 [physics.geo-ph])

    [http://arxiv.org/abs/2304.14307](http://arxiv.org/abs/2304.14307)

    本文介绍了一种利用滑雪板装置应变传感器对雪进行分类的方法，该方法可以通过对每个10秒的轨迹段进行数据处理，独立于滑雪风格，准确地分配三个定性标签。

    

    理解山区景观中雪的结构，数量和类型对于评估雪崩安全性、解释卫星图像、构建准确的水文模型以及选择适合你周末旅行的滑雪板至关重要。本文探讨了如何使用安装在阿尔卑斯滑雪板顶部表面的应变传感器，在滑雪时估算雪面顶层的特征。我们展示了利用两个应变计和一个惯性测量单元可以在每个10秒的轨迹段中正确地分配三个定性标签（粉雪，泥泞的，或冰雪/修剪过的雪），而且独立于滑雪风格，准确率高达97%。我们的算法使用了一种数据驱动的线性模型来描述滑雪板和雪的相互作用。

    Understanding the structure, quantity, and type of snow in mountain landscapes is crucial for assessing avalanche safety, interpreting satellite imagery, building accurate hydrology models, and choosing the right pair of skis for your weekend trip. Currently, such characteristics of snowpack are measured using a combination of remote satellite imagery, weather stations, and laborious point measurements and descriptions provided by local forecasters, guides, and backcountry users. Here, we explore how characteristics of the top layer of snowpack could be estimated while skiing using strain sensors mounted to the top surface of an alpine ski. We show that with two strain gauges and an inertial measurement unit it is feasible to correctly assign one of three qualitative labels (powder, slushy, or icy/groomed snow) to each 10 second segment of a trajectory with 97% accuracy, independent of skiing style. Our algorithm uses a combination of a data-driven linear model of the ski-snow interact
    
[^22]: 从餐食协变量中学习葡萄糖 - 胰岛素动力学中的吸收速率

    Learning Absorption Rates in Glucose-Insulin Dynamics from Meal Covariates. (arXiv:2304.14300v1 [cs.LG])

    [http://arxiv.org/abs/2304.14300](http://arxiv.org/abs/2304.14300)

    本文提出了一种从葡萄糖 - 胰岛素数据和餐饮协变量中学习宏量营养成分影响的方法，使用神经网络预测个人的葡萄糖吸收率，并在葡萄糖动力学的微分方程中实现端到端训练，从而获得更好的预测效果。

    

    传统的葡萄糖 - 胰岛素动力学模型依赖于选择用于在实验室环境中拟合观察结果的启发式参数，但这些模型无法描述日常生活中的葡萄糖动力学。失败的一个原因是在餐后葡萄糖吸收速率的描述中。餐的宏量营养成分对吸收剖面有微妙的影响，这很难通过机械建模来实现。本文提出了从葡萄糖 - 胰岛素数据和餐饮协变量中学习宏量营养成分影响的方法。鉴于宏量营养信息和餐食时间，我们使用神经网络来预测个人的葡萄糖吸收率。我们在葡萄糖动力学的微分方程中使用这个神经速率函数作为控制函数，从而实现端到端训练。在模拟数据上，我们的方法能够紧密地逼近真实的吸收速率，比启发式参数化获得更好的预测效果，尽管只观察到葡萄糖、胰岛素两项。

    Traditional models of glucose-insulin dynamics rely on heuristic parameterizations chosen to fit observations within a laboratory setting. However, these models cannot describe glucose dynamics in daily life. One source of failure is in their descriptions of glucose absorption rates after meal events. A meal's macronutritional content has nuanced effects on the absorption profile, which is difficult to model mechanistically. In this paper, we propose to learn the effects of macronutrition content from glucose-insulin data and meal covariates. Given macronutrition information and meal times, we use a neural network to predict an individual's glucose absorption rate. We use this neural rate function as the control function in a differential equation of glucose dynamics, enabling end-to-end training. On simulated data, our approach is able to closely approximate true absorption rates, resulting in better forecast than heuristic parameterizations, despite only observing glucose, insulin, a
    
[^23]: 用自然语言指令控制文本生成

    Controlled Text Generation with Natural Language Instructions. (arXiv:2304.14293v1 [cs.CL])

    [http://arxiv.org/abs/2304.14293](http://arxiv.org/abs/2304.14293)

    InstructCTG是一个可以通过自然语言描述和演示来控制文本生成并满足不同约束条件的框架，它有效地解决了现有搜索或得分方法所存在的问题。

    

    大型语言模型可以产生流畅的文本，并能根据自然语言指令解决各种任务，无需特定的训练。然而，控制它们的生成以满足不同应用程序所需的各种约束条件是非常困难的。本文提供了一个带约束调节的文本生成框架——InstructCTG，该框架通过基于自然语言描述和约束演示来纳入不同的约束条件。我们首先通过一系列现成的NLP工具和简单的启发式方法来提取自然文本的潜在约束条件。此外，我们将这些约束条件转化为自然语言指令，以形成弱监督的训练数据。通过添加自然语言约束描述和少量演示，我们对预训练语言模型进行了微调，以纳入各种类型的约束条件。与现有基于搜索或得分的方法相比，InstructCTG 更加有效。

    Large language models generate fluent texts and can follow natural language instructions to solve a wide range of tasks without task-specific training. Nevertheless, it is notoriously difficult to control their generation to satisfy the various constraints required by different applications. In this work, we present InstructCTG, a controlled text generation framework that incorporates different constraints by conditioning on natural language descriptions and demonstrations of the constraints. In particular, we first extract the underlying constraints of natural texts through a combination of off-the-shelf NLP tools and simple heuristics. We then verbalize the constraints into natural language instructions to form weakly supervised training data. By prepending natural language descriptions of the constraints and a few demonstrations, we fine-tune a pre-trained language model to incorporate various types of constraints. Compared to existing search-based or score-based methods, InstructCT
    
[^24]: 用基于Transformer的方法区分行星凌和误判的方法

    Distinguishing a planetary transit from false positives: a Transformer-based classification for planetary transit signals. (arXiv:2304.14283v1 [astro-ph.EP])

    [http://arxiv.org/abs/2304.14283](http://arxiv.org/abs/2304.14283)

    本文介绍了基于Transformer架构的新方法，用于高效准确地区分行星凌和误判。采用一种新的预处理输入数据的方法，使用正弦函数来保留信号的周期性。在模拟数据的评估中，该方法优于已有的基于CNN的方法。

    

    目前，像TESS这样的空间任务提供了大量必须高效、系统地分析的光变曲线数据库。近年来，深度学习（DL）方法，尤其是卷积神经网络（CNN），已被用于自动分类候选外行星的凌变信号。然而，CNN具有一些缺陷，例如，它们需要许多层来捕获序列数据（例如光变曲线）上的依赖关系，使得网络变得过于庞大，最终变得不实用。自注意机制是一种DL技术，试图模仿有选择地聚焦于一些相关事物而忽略其他事物的行为。最近针对序列数据的模型，例如Transformer架构，取得了成功的结果。我们基于这些成功的模型提出了一种新的用于自动分类凌变信号的架构。我们提出的架构旨在高效准确地捕捉凌变信号并将其与误判区分开来。我们采用了一种新的预处理输入数据的方法，使用正弦函数来保留信号的周期性。在模拟数据的评估中，我们的方法在准确性和效率方面优于基于CNN的现有方法。

    Current space-based missions, such as the Transiting Exoplanet Survey Satellite (TESS), provide a large database of light curves that must be analysed efficiently and systematically. In recent years, deep learning (DL) methods, particularly convolutional neural networks (CNN), have been used to classify transit signals of candidate exoplanets automatically. However, CNNs have some drawbacks; for example, they require many layers to capture dependencies on sequential data, such as light curves, making the network so large that it eventually becomes impractical. The self-attention mechanism is a DL technique that attempts to mimic the action of selectively focusing on some relevant things while ignoring others. Models, such as the Transformer architecture, were recently proposed for sequential data with successful results. Based on these successful models, we present a new architecture for the automatic classification of transit signals. Our proposed architecture is designed to capture t
    
[^25]: AI，为我写一篇文章：人类写作与ChatGPT生成文章的大规模比较

    AI, write an essay for me: A large-scale comparison of human-written versus ChatGPT-generated essays. (arXiv:2304.14276v1 [cs.CL])

    [http://arxiv.org/abs/2304.14276](http://arxiv.org/abs/2304.14276)

    ChatGPT生成的文章质量比人类写作的文章更高，写作风格更流畅，语法和拼写错误更少。

    

    背景：最近，ChatGPT及类似的AI生成模型吸引了数亿用户，成为公众话题的一部分。许多人认为这样的模型将会打乱社会，导致未来教育系统和信息生成的显著变化。然而，迄今为止，这种信念基于的是口头证据或模型拥有者的基准——这两者缺乏科学严谨性。目标：通过大规模研究比较人类写作和ChatGPT生成论证性学生文章的质量，我们系统地评估了AI生成内容的质量。方法：一大批文章语料库被许多人类专家（教师）使用标准标准评分。我们在分析中加入了对生成文章的语言特征的考虑。结果：我们的结果表明，ChatGPT生成的文章比人类写作的文章质量更高。AI的写作风格更加流畅，而且比人类写作出现更少的语法和拼写错误。

    Background: Recently, ChatGPT and similar generative AI models have attracted hundreds of millions of users and become part of the public discourse. Many believe that such models will disrupt society and will result in a significant change in the education system and information generation in the future. So far, this belief is based on either colloquial evidence or benchmarks from the owners of the models -- both lack scientific rigour.  Objective: Through a large-scale study comparing human-written versus ChatGPT-generated argumentative student essays, we systematically assess the quality of the AI-generated content.  Methods: A large corpus of essays was rated using standard criteria by a large number of human experts (teachers). We augment the analysis with a consideration of the linguistic characteristics of the generated essays.  Results: Our results demonstrate that ChatGPT generates essays that are rated higher for quality than human-written essays. The writing style of the AI m
    
[^26]: 名字的意义：通过 CAD 文件中用户提供的名称评估语言模型中的装配 - 零件语义知识。

    What's in a Name? Evaluating Assembly-Part Semantic Knowledge in Language Models through User-Provided Names in CAD Files. (arXiv:2304.14275v1 [cs.CL])

    [http://arxiv.org/abs/2304.14275](http://arxiv.org/abs/2304.14275)

    本文提出，计算机辅助设计（CAD）软件中的自然语言名称是部件关联的宝贵来源，大型语言模型（LLM）提供了一种处理这种数据的有用的领域专业知识。通过自然语言名称的预训练模型可在三个自监督任务上表现出色，微调还可以提高所有任务的性能，提高了文本数据的价值。此外，提出了手动注释的新数据集 CAD-120，其中包含 120 个装配，并提供了语义关系注释。

    

    装配中零件之间和零件与整体之间的语义知识对于从搜索设计存储库到构建工程知识库等各种任务都非常有用。本文提出，设计师在计算机辅助设计 (CAD) 软件中使用的自然语言名称是这种知识宝贵的来源，并且大型语言模型 (LLM) 包含了用于处理此数据以及其他 CAD 和工程相关任务的有用领域专业知识。我们提取并清理了大量的自然语言零件、特征和文档名称语料库，并使用它来定量证明预训练语言模型可以在三个自监督任务上优于众多基准测试，而且从未见过这些数据。此外，我们展示了对文本数据语料库进行微调可以进一步提高所有任务的性能，从而展示了迄今为止在很大程度上被忽略的文本数据的价值。我们还确定了这个领域需要更多的基准数据集，并提出了一个名为 CAD-120 的新数据集，其中包含 120 个 CAD 装配件，具有手动注释的语义关系。

    Semantic knowledge of part-part and part-whole relationships in assemblies is useful for a variety of tasks from searching design repositories to the construction of engineering knowledge bases. In this work we propose that the natural language names designers use in Computer Aided Design (CAD) software are a valuable source of such knowledge, and that Large Language Models (LLMs) contain useful domain-specific information for working with this data as well as other CAD and engineering-related tasks.  In particular we extract and clean a large corpus of natural language part, feature and document names and use this to quantitatively demonstrate that a pre-trained language model can outperform numerous benchmarks on three self-supervised tasks, without ever having seen this data before. Moreover, we show that fine-tuning on the text data corpus further boosts the performance on all tasks, thus demonstrating the value of the text data which until now has been largely ignored. We also ide
    
[^27]: 图神经网络何时对节点分类有帮助：研究同源性原则对节点可区分性的影响

    When Do Graph Neural Networks Help with Node Classification: Investigating the Homophily Principle on Node Distinguishability. (arXiv:2304.14274v1 [cs.SI])

    [http://arxiv.org/abs/2304.14274](http://arxiv.org/abs/2304.14274)

    同源性原则不一定是影响图神经网络优越性的唯一原因；本文提出Contextual Stochastic Block Model for Homophily (CSBM-H)以深入研究同源性对节点可区分性的影响。

    

    同源性原则指相同类别的节点更有可能连接在一起，一直被认为是图神经网络（GNN）在节点分类（NC）任务上性能优越的主要原因。最近，人们提出理论结果认为，即使同源性原则被打破，只要来自同一类别的节点分享相似的邻居模式，GNN的优势仍然存在，这对同源性的有效性提出了质疑。然而，这个论点仅考虑了同类节点的可区分性，忽略了跨类别的可区分性，这是研究同源性效应的不足之处。在本文中，我们首先通过例子证明了上述不足，并认为可区分性的理想情况是同类节点的可区分性小于跨类别节点的可区分性。为了形式化这个想法，更好地理解同源性，我们提出了Contextual Stochastic Block Model for Homophily (CSBM-H)，并进行了全面的实验分析。

    Homophily principle, i.e. nodes with the same labels are more likely to be connected, was believed to be the main reason for the performance superiority of Graph Neural Networks (GNNs) over Neural Networks (NNs) on Node Classification (NC) tasks. Recently, people have developed theoretical results arguing that, even though the homophily principle is broken, the advantage of GNNs can still hold as long as nodes from the same class share similar neighborhood patterns, which questions the validity of homophily. However, this argument only considers intra-class Node Distinguishability (ND) and ignores inter-class ND, which is insufficient to study the effect of homophily. In this paper, we first demonstrate the aforementioned insufficiency with examples and argue that an ideal situation for ND is to have smaller intra-class ND than inter-class ND. To formulate this idea and have a better understanding of homophily, we propose Contextual Stochastic Block Model for Homophily (CSBM-H) and def
    
[^28]: 自主驾驶能源高效近似边缘人工智能概述调查

    A Survey on Approximate Edge AI for Energy Efficient Autonomous Driving Services. (arXiv:2304.14271v1 [cs.RO])

    [http://arxiv.org/abs/2304.14271](http://arxiv.org/abs/2304.14271)

    这篇论文概述了自主驾驶服务中的传感和数据处理现状，及其对能源和环境带来的挑战，进一步调查和比较了如何应用和整合现有技术创新以实现能源效率。

    

    自主驾驶服务极大地依赖于传感器，如摄像机、激光雷达、雷达和通信模块。处理传感数据的通常做法是在车辆内部放置高性能计算单元，部署人工智能模型和算法，作为车辆的大脑或管理员。从平均行驶时间生成的车辆数据可以高达20TB，具体取决于传感器的数据速率和规格。鉴于自主驾驶服务的规模和快速增长，尤其是在向车辆电气化（例如，电池驱动）的趋势中，提高总体能源和环境效率至关重要。 虽然这些领域的传感器技术、无线通信、计算和AI / ML算法已经取得了显着进展，但如何应用和整合这些技术创新以实现能源效率仍然是一个挑战。本调查回顾并比较了连接的车辆

    Autonomous driving services rely heavily on sensors such as cameras, LiDAR, radar, and communication modules. A common practice of processing the sensed data is using a high-performance computing unit placed inside the vehicle, which deploys AI models and algorithms to act as the brain or administrator of the vehicle. The vehicular data generated from average hours of driving can be up to 20 Terabytes depending on the data rate and specification of the sensors. Given the scale and fast growth of services for autonomous driving, it is essential to improve the overall energy and environmental efficiency, especially in the trend towards vehicular electrification (e.g., battery-powered). Although the areas have seen significant advancements in sensor technologies, wireless communications, computing and AI/ML algorithms, the challenge still exists in how to apply and integrate those technology innovations to achieve energy efficiency. This survey reviews and compares the connected vehicular
    
[^29]: 简化变分贝叶斯方法的推导过程

    Variational Bayes Made Easy. (arXiv:2304.14251v1 [cs.LG])

    [http://arxiv.org/abs/2304.14251](http://arxiv.org/abs/2304.14251)

    该论文提出了一个三步骤方法，简化了变分贝叶斯近似推断方法的推导过程。

    

    变分贝叶斯方法是一种流行的近似推断方法，但其推导过程可能很繁琐。为了简化这个过程，我们给出了一个三步骤的方法，通过显式寻找关于已知分布期望的线性性，来确定后验分布形式。然后我们可以直接通过“读取”这些期望前的项，写出更新。这个方法使得推导更加简单，快速，简短和通用。

    Variational Bayes is a popular method for approximate inference but its derivation can be cumbersome. To simplify the process, we give a 3-step recipe to identify the posterior form by explicitly looking for linearity with respect to expectations of well-known distributions. We can then directly write the update by simply ``reading-off'' the terms in front of those expectations. The recipe makes the derivation easier, faster, shorter, and more general.
    
[^30]: 关于洛克斯洞穴的流形学习：关于流形学习和物理现象的评论（arXiv:2304.14248v1 [stat.ML]）

    On Manifold Learning in Plato's Cave: Remarks on Manifold Learning and Physical Phenomena. (arXiv:2304.14248v1 [stat.ML])

    [http://arxiv.org/abs/2304.14248](http://arxiv.org/abs/2304.14248)

    本文通过一个警示故事阐释了分析数据时，测量几何和底层现象几何差异带来的问题，以及这种差异在某些情况下如何导致对一个修正过的问题给出错误答案。这些问题适用于降维和无监督学习领域。

    

    许多机器学习技术尝试通过测量不需要对物理现象或测量设备进行显式建模的低维流形结构来推断潜在物理现象的低维流形结构，这篇论文提出了关于测量几何和底层现象几何之间差异的警示故事。在普通情况下，这篇论文所展示的度量形变在数学上是直接而不可避免的，并且它只是数个类似效应中的一个。虽然这并不总是出现问题，但我们提供了一个标准且无害数据处理过程的例子，其中这种影响导致对一个看似简单的问题给出了错误的答案。尽管我们关注流形学习，但这些问题广泛适用于降维和无监督学习领域。

    Many techniques in machine learning attempt explicitly or implicitly to infer a low-dimensional manifold structure of an underlying physical phenomenon from measurements without an explicit model of the phenomenon or the measurement apparatus. This paper presents a cautionary tale regarding the discrepancy between the geometry of measurements and the geometry of the underlying phenomenon in a benign setting. The deformation in the metric illustrated in this paper is mathematically straightforward and unavoidable in the general case, and it is only one of several similar effects. While this is not always problematic, we provide an example of an arguably standard and harmless data processing procedure where this effect leads to an incorrect answer to a seemingly simple question. Although we focus on manifold learning, these issues apply broadly to dimensionality reduction and unsupervised learning.
    
[^31]: TorchBench: 用高API表面覆盖率评估PyTorch性能的基准套件

    TorchBench: Benchmarking PyTorch with High API Surface Coverage. (arXiv:2304.14226v1 [cs.LG])

    [http://arxiv.org/abs/2304.14226](http://arxiv.org/abs/2304.14226)

    TorchBench是一款新型基准测试套件，可全面表征PyTorch软件栈的性能，指导模型、PyTorch框架和GPU库的性能优化。

    

    深度学习是多个领域中的革命性技术。为了方便模型的开发和部署，提出了许多深度学习框架，其中PyTorch是最流行的解决方案之一。PyTorch软件栈的生态性能至关重要，可节省模型训练成本并减少模型推理的响应时间。本文提出了TorchBench，一款新型基准测试套件，用于研究PyTorch软件栈的性能。与现有基准测试套件不同，TorchBench包含了许多代表性模型，覆盖了大量PyTorch API表面。TorchBench能够全面地表征PyTorch软件栈的性能，指导模型、PyTorch框架和GPU库的性能优化。我们展示了TorchBench的两个实际用例。第一，我们对TorchBench进行性能剖析，以识别PyTorch的GPU性能效率问题。我们能够优化许多性能故障并向上游提交贡献。

    Deep learning (DL) has been a revolutionary technique in various domains. To facilitate the model development and deployment, many deep learning frameworks are proposed, among which PyTorch is one of the most popular solutions. The performance of ecosystem around PyTorch is critically important, which saves the costs of training models and reduces the response time of model inferences. In this paper, we propose TorchBench, a novel benchmark suite to study the performance of PyTorch software stack. Unlike existing benchmark suites, TorchBench encloses many representative models, covering a large PyTorch API surface. TorchBench is able to comprehensively characterize the performance of the PyTorch software stack, guiding the performance optimization across models, PyTorch framework, and GPU libraries. We show two practical use cases of TorchBench. (1) We profile TorchBench to identify GPU performance inefficiencies in PyTorch. We are able to optimize many performance bugs and upstream pa
    
[^32]: 多通道自律

    Self-discipline on multiple channels. (arXiv:2304.14224v1 [cs.LG])

    [http://arxiv.org/abs/2304.14224](http://arxiv.org/abs/2304.14224)

    本文提出了一种名为“多通道自律”的方法，使用一致性正则化和自我蒸馏来提高模型的泛化能力和对噪声标签的鲁棒性。

    

    自我蒸馏依靠自身信息提高模型的泛化能力，并具有广阔的发展前景。现有的自我蒸馏方法要么需要额外的模型，要么需要修改模型，要么需要扩大批量大小进行训练，增加了使用难度、内存消耗和计算成本。本文提出了一个名为“多通道自律”（SMC）的方法，将一致性正则化与自我蒸馏相结合，使用多通道的概念。SMC从概念上分为两个步骤：1）每个通道数据同时通过模型，获得其相应的软标签；2）存储在上一步的软标签与从当前通道数据通过模型获得的软标签一起读取，计算损失函数。SMC使用一致性正则化和自我蒸馏来提高模型的泛化能力和对噪声标签的鲁棒性。

    Self-distillation relies on its own information to improve the generalization ability of the model and has a bright future. Existing self-distillation methods either require additional models, model modification, or batch size expansion for training, which increases the difficulty of use, memory consumption, and computational cost. This paper developed Self-discipline on multiple channels(SMC), which combines consistency regularization with self-distillation using the concept of multiple channels. Conceptually, SMC consists of two steps: 1) each channel data is simultaneously passed through the model to obtain its corresponding soft label, and 2) the soft label saved in the previous step is read together with the soft label obtained from the current channel data through the model to calculate the loss function. SMC uses consistent regularization and self-distillation to improve the generalization ability of the model and the robustness of the model to noisy labels. We named the SMC con
    
[^33]: 论文标题：某些变量，某些参数，某些时间，某些已知物理学：带有部分信息的识别

    Some of the variables, some of the parameters, some of the times, with some physics known: Identification with partial information. (arXiv:2304.14214v1 [cs.LG])

    [http://arxiv.org/abs/2304.14214](http://arxiv.org/abs/2304.14214)

    本文介绍了一种利用神经网络识别动力系统的新方法，在不对数据进行修改的情况下，通过数值积分和部分已知物理学，能够从任意时间点的采样数据中学习。

    

    实验数据通常由独立测量的变量组成，在不同的采样率(连续测量之间的非均匀${\Delta}$t)下，仅在特定时间点才对所有变量的子集进行采样。从这样的数据中识别动力系统的方法通常使用插值、插值或子采样来重新组织或修改训练数据$ \textit {prior}$ 学习。部分物理知识也可能在$\textit {a priori}$（精确或近似）中可用，并且数据驱动技术可以补充此知识。在这里，我们利用基于数值积分方法和$\textit {a priori}$物理知识的神经网络架构来识别基本控制微分方程的右手边。这种神经网络模型的迭代允许从在任意时间点采样的数据中学习$\textit {without}$数据修改。重要的是，我们将网络与可用的部分物理集成

    Experimental data is often comprised of variables measured independently, at different sampling rates (non-uniform ${\Delta}$t between successive measurements); and at a specific time point only a subset of all variables may be sampled. Approaches to identifying dynamical systems from such data typically use interpolation, imputation or subsampling to reorganize or modify the training data $\textit{prior}$ to learning. Partial physical knowledge may also be available $\textit{a priori}$ (accurately or approximately), and data-driven techniques can complement this knowledge. Here we exploit neural network architectures based on numerical integration methods and $\textit{a priori}$ physical knowledge to identify the right-hand side of the underlying governing differential equations. Iterates of such neural-network models allow for learning from data sampled at arbitrary time points $\textit{without}$ data modification. Importantly, we integrate the network with available partial physical
    
[^34]: LLT：线性定律特征空间变换的R包

    LLT: An R package for Linear Law-based Feature Space Transformation. (arXiv:2304.14211v1 [cs.LG])

    [http://arxiv.org/abs/2304.14211](http://arxiv.org/abs/2304.14211)

    LLT是一个R包，用于线性定律特征空间变换，可以帮助对单变量和多变量时间序列进行分类。

    

    线性定律特征空间转换(LLT )算法的目标是帮助对单变量和多变量时间序列进行分类。LLT R包以灵活和用户友好的方式实现了该算法。该包将实例分为训练和测试集，并利用时延嵌入和谱分解技术，识别训练集中每个输入序列(初始特征)的控制模式(称为线性定律)。最后，它应用训练集的线性定律来转换测试集的初始特征。trainTest、trainLaw和testTrans三个单独的函数来执行这些步骤，它们需要预定义的数据结构;然而，为了快速计算，它们只使用内置函数。LLT R包和适当数据结构的示例数据集在GitHub上公开可用。

    The goal of the linear law-based feature space transformation (LLT) algorithm is to assist with the classification of univariate and multivariate time series. The presented R package, called LLT, implements this algorithm in a flexible yet user-friendly way. This package first splits the instances into training and test sets. It then utilizes time-delay embedding and spectral decomposition techniques to identify the governing patterns (called linear laws) of each input sequence (initial feature) within the training set. Finally, it applies the linear laws of the training set to transform the initial features of the test set. These steps are performed by three separate functions called trainTest, trainLaw, and testTrans. Their application requires a predefined data structure; however, for fast calculation, they use only built-in functions. The LLT R package and a sample dataset with the appropriate data structure are publicly available on GitHub.
    
[^35]: 一种透明化数据表示的方法

    A transparent approach to data representation. (arXiv:2304.14209v1 [cs.LG])

    [http://arxiv.org/abs/2304.14209](http://arxiv.org/abs/2304.14209)

    使用二元属性表示模型对Netflix观众对电影的评分数据集进行数据表示，属性易于解释，且需要较少属性即可达到相同水平的误差。

    

    我们使用二元属性表示（BAR）模型来描述Netflix观众对电影的评分数据集。我们使用离散的二进制位而不是连续的参数对观众进行分类，这使得表示紧凑而透明。这些属性易于解释，我们需要比类似方法少得多的属性才能达到相同水平的误差。我们还利用数据集中电影评分的非均匀分布，在不影响其余电影性能的情况下，选择少量电影进行训练。

    We use a binary attribute representation (BAR) model to describe a data set of Netflix viewers' ratings of movies. We classify the viewers with discrete bits rather than continuous parameters, which makes the representation compact and transparent. The attributes are easy to interpret, and we need far fewer attributes than similar methods do to achieve the same level of error. We also take advantage of the nonuniform distribution of ratings among the movies in the data set to train on a small selection of movies without compromising performance on the rest of the movies.
    
[^36]: 零和游戏的对数遗憾量子学习算法

    Logarithmic-Regret Quantum Learning Algorithms for Zero-Sum Games. (arXiv:2304.14197v1 [quant-ph])

    [http://arxiv.org/abs/2304.14197](http://arxiv.org/abs/2304.14197)

    该论文提出了一个在线零和游戏的量子算法，可以在量子时间计算出$\varepsilon$-近似纳什均衡，是目前第一个实现基于量子计算的快速量子线性编程求解器。

    

    我们提出了第一个在线零和游戏的量子算法，并在游戏设置下实现了 $\tilde O(1)$ 的遗憾度。此外，我们的量子算法可以在量子时间$\tilde O(\sqrt{m+n}/\varepsilon^{2.5})$内计算$m\times n$矩阵零和游戏的$\varepsilon$-近似纳什均衡，相对于传统算法在$m, n$方面实现了二次改进。我们的算法使用标准的量子输入，并生成简明的描述性的经典输出，方便进行端到端应用。作为应用，我们获得了一个快速的量子线性规划求解器。在技术上，我们的在线量子算法基于乐观的乘法权重更新方法“量子化”了经典算法。我们算法的核心是一个快速量子多重采样过程，用于Gibbs采样问题，这可能是独立感兴趣的问题。

    We propose the first online quantum algorithm for zero-sum games with $\tilde O(1)$ regret under the game setting. Moreover, our quantum algorithm computes an $\varepsilon$-approximate Nash equilibrium of an $m \times n$ matrix zero-sum game in quantum time $\tilde O(\sqrt{m+n}/\varepsilon^{2.5})$, yielding a quadratic improvement over classical algorithms in terms of $m, n$. Our algorithm uses standard quantum inputs and generates classical outputs with succinct descriptions, facilitating end-to-end applications. As an application, we obtain a fast quantum linear programming solver. Technically, our online quantum algorithm "quantizes" classical algorithms based on the optimistic multiplicative weight update method. At the heart of our algorithm is a fast quantum multi-sampling procedure for the Gibbs sampling problem, which may be of independent interest.
    
[^37]: ClusterNet：一种基于感知的分布式数据聚类模型

    ClusterNet: A Perception-Based Clustering Model for Scattered Data. (arXiv:2304.14185v1 [cs.LG])

    [http://arxiv.org/abs/2304.14185](http://arxiv.org/abs/2304.14185)

    这项工作介绍了ClusterNet，一种基于感知的分布式数据聚类模型，利用大规模数据集和基于点的深度学习模型，反映人类感知的聚类可分性。

    

    散点图中的聚类分离是一个通常由广泛使用的聚类技术（例如k-means或DBSCAN）来解决的任务。然而，由于这些算法基于非感知度量，它们的输出经常不能反映出人类聚类感知。为了弥合人类聚类感知和机器计算聚类之间的差距，我们提出了一种直接处理分布式数据的学习策略。为了在这些数据上学习感知聚类分离，我们进行了一项众包大规模数据集的工作，其中包括384个人群工作者对双变量数据的7,320个点聚类从属进行了标记。基于这些数据，我们能够训练ClusterNet，这是一个基于点的深度学习模型，被训练成反映人类感知的聚类可分性。为了在人类注释的数据上训练ClusterNet，我们省略了在2D画布上渲染散点图，而是使用了一个PointNet++架构，使其能够直接推理点云。在这项工作中，我们建立了一种基于感知的分布式数据聚类模型，ClusterNet。

    Cluster separation in scatterplots is a task that is typically tackled by widely used clustering techniques, such as for instance k-means or DBSCAN. However, as these algorithms are based on non-perceptual metrics, their output often does not reflect human cluster perception. To bridge the gap between human cluster perception and machine-computed clusters, we propose a learning strategy which directly operates on scattered data. To learn perceptual cluster separation on this data, we crowdsourced a large scale dataset, consisting of 7,320 point-wise cluster affiliations for bivariate data, which has been labeled by 384 human crowd workers. Based on this data, we were able to train ClusterNet, a point-based deep learning model, trained to reflect human perception of cluster separability. In order to train ClusterNet on human annotated data, we omit rendering scatterplots on a 2D canvas, but rather use a PointNet++ architecture enabling inference on point clouds directly. In this work, w
    
[^38]: mPLUG-Owl: 模块化增强了大型语言模型的多模态能力

    mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality. (arXiv:2304.14178v1 [cs.CL])

    [http://arxiv.org/abs/2304.14178](http://arxiv.org/abs/2304.14178)

    本文介绍了一种名为mPLUG-Owl的训练范式，它通过模块化学习基础LLM、视觉知识模块和视觉抽象器模块，赋予LLMs多模态的能力。实验结果表明，mPLUG-Owl在图像字幕和视觉问答任务中表现优于基线模型，并在某些情况下达到了最先进的性能水平。

    

    大型语言模型(LLMs)已经在各种开放式任务中展示出了令人印象深刻的零-shot表现，而最近的研究还探讨了将LLMs用于多模态生成的应用。在本研究中，我们引入了一种新的训练范式mPLUG-Owl，通过基础LLM、视觉知识模块和视觉抽象器模块的模块化学习，使LLMs具备了多模态的能力。该方法可以支持多种模态，并通过模态协作促进了多单模态和多模态的能力。mPLUG-Owl的训练范式包括用于对齐图像和文本的两阶段方法，该方法利用LLM的辅助学习视觉知识，同时保持甚至改进了LLM的生成能力。在第一阶段中，使用冻结的LLM模块对视觉知识模块和抽象器模块进行训练以对齐图像和文本。在第二阶段中，使用仅语言和多模态监督数据集共同对模型进行微调。对于图像字幕和视觉问答任务的实验结果表明，mPLUG-Owl优于基线模型，在某些情况下达到了最先进的性能水平。

    Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tu
    
[^39]: 基于强化学习探索夸克和轻子的味道结构

    Exploring the flavor structure of quarks and leptons with reinforcement learning. (arXiv:2304.14176v1 [hep-ph])

    [http://arxiv.org/abs/2304.14176](http://arxiv.org/abs/2304.14176)

    通过利用强化学习，探索了具有 $U(1)$ 味道对称性的模型的味道结构，找到了21个与实验测量值一致的模型，预测了无中微子双贝塔衰变的有效质量和可观的轻子 CP 破坏。

    

    我们提出了一种利用强化学习探索夸克和轻子味道结构的方法。作为具体模型，我们利用一个基本的基于策略的算法，针对具有 $U(1)$ 味道对称性的模型。通过训练神经网络对夸克和轻子的 $U(1)$ 荷进行学习，代理方案找到了21个与夸克和轻子的实验测量质量和混合角一致的模型。特别是，正序的固有值往往大于反序，正序与目前的实验数据相比更加符合。代理的自主行为根据无中微子双贝塔衰变的有效质量和叶子场的角成分引起的可观的轻子 CP 破坏来预测。

    We propose a method to explore the flavor structure of quarks and leptons with reinforcement learning. As a concrete model, we utilize a basic policy-based algorithm for models with $U(1)$ flavor symmetry. By training neural networks on the $U(1)$ charges of quarks and leptons, the agent finds 21 models to be consistent with experimentally measured masses and mixing angles of quarks and leptons. In particular, an intrinsic value of normal ordering tends to be larger than that of inverted ordering, and the normal ordering is well fitted with the current experimental data in contrast to the inverted ordering. A specific value of effective mass for the neutrinoless double beta decay and a sizable leptonic CP violation induced by an angular component of flavon field are predicted by autonomous behavior of the agent.
    
[^40]: 一种计算Brauer群等变神经网络层的算法

    An Algorithm for Computing with Brauer's Group Equivariant Neural Network Layers. (arXiv:2304.14165v1 [cs.LG])

    [http://arxiv.org/abs/2304.14165](http://arxiv.org/abs/2304.14165)

    本文提出一种算法，使用范畴论构造来实现的Brauer群等变神经网络层的乘积，同时采用Kronecker积矩阵，实现了显著的计算成本减少。

    

    在arXiv：2212.08630中，对介于$\mathbb{R}^{n}$的张量幂空间之间的可等变于正交群，$O(n)$，特殊正交群，$SO(n)$，和辛群，$Sp(n)$的线性神经网络层进行了表征。本文提出了一种算法，使用范畴论构造来实现过程，通过利用Kronecker积矩阵来执行乘法，与简单的实现相比，实现了显著的计算成本减少。我们展示了我们的方法扩展到对称组，$S_n$，在此过程中恢复了arXiv：2303.06208的算法。

    The learnable, linear neural network layers between tensor power spaces of $\mathbb{R}^{n}$ that are equivariant to the orthogonal group, $O(n)$, the special orthogonal group, $SO(n)$, and the symplectic group, $Sp(n)$, were characterised in arXiv:2212.08630. We present an algorithm for multiplying a vector by any weight matrix for each of these groups, using category theoretic constructions to implement the procedure. We achieve a significant reduction in computational cost compared with a naive implementation by making use of Kronecker product matrices to perform the multiplication. We show that our approach extends to the symmetric group, $S_n$, recovering the algorithm of arXiv:2303.06208 in the process.
    
[^41]: 基于脉冲神经网络的决策反馈均衡器用于IM/DD系统

    Spiking Neural Network Decision Feedback Equalization for IM/DD Systems. (arXiv:2304.14152v1 [cs.NE])

    [http://arxiv.org/abs/2304.14152](http://arxiv.org/abs/2304.14152)

    本论文介绍了一种基于脉冲神经网络的决策反馈均衡器，用于IM/DD系统。实验表明，该均衡器的性能优于传统的线性均衡器和人工神经网络均衡器。

    

    本文研究了一种基于脉冲神经网络的决策反馈均衡器，并将其应用于IM/DD链路的各种参数。结果表明，与线性和人工神经网络（ANN）均衡器相比，脉冲神经网络均衡器具有更好的性能。

    A spiking neural network (SNN) equalizer with a decision feedback structure is applied to an IM/DD link with various parameters. The SNN outperforms linear and artificial neural network (ANN) based equalizers.
    
[^42]: 分类化群等变神经网络

    Categorification of Group Equivariant Neural Networks. (arXiv:2304.14144v1 [cs.LG])

    [http://arxiv.org/abs/2304.14144](http://arxiv.org/abs/2304.14144)

    本文利用范畴论构建的算法，成功快速计算了群等变神经网络的线性层函数。这种方法为深度学习的其他领域做出了有益的探索。

    

    我们提出了范畴论在深度学习中的一种新应用。我们展示了如何利用范畴论来理解和处理群等变神经网络的线性层函数，其中层是$\mathbb{R}^{n}$的某些张量幂空间，对应于群$S_n$、$O(n)$、$Sp(n)$和$SO(n)$。通过使用范畴论构建，我们建立了比这些神经网络的原始公式更丰富的结构，得出了新的见解。特别是，我们概述了一种算法的开发，该算法可以快速计算通过每个问题中的群等变线性层传递的向量的结果。我们的方法的成功表明，范畴论可以对深度学习的其他领域有益。

    We present a novel application of category theory for deep learning. We show how category theory can be used to understand and work with the linear layer functions of group equivariant neural networks whose layers are some tensor power space of $\mathbb{R}^{n}$ for the groups $S_n$, $O(n)$, $Sp(n)$, and $SO(n)$. By using category theoretic constructions, we build a richer structure that is not seen in the original formulation of these neural networks, leading to new insights. In particular, we outline the development of an algorithm for quickly computing the result of a vector that is passed through an equivariant, linear layer for each group in question. The success of our approach suggests that category theory could be beneficial for other areas of deep learning.
    
[^43]: TempEE：基于时空平行Transformer实现雷达回波外推

    TempEE: Temporal-Spatial Parallel Transformer for Radar Echo Extrapolation Beyond Auto-Regression. (arXiv:2304.14131v1 [eess.SP])

    [http://arxiv.org/abs/2304.14131](http://arxiv.org/abs/2304.14131)

    本文提出了一个新型的雷达回波外推算法TempEE，该算法利用时空相关特征和Transformer技术，通过从多帧回波图像中提取特征，准确地表示了降水的非平稳运动过程，克服了传统算法的局限性，在三个真实数据集上表现出色。

    

    气象雷达反射率数据（也称为回波）在预测降水和进行短期强降雨的精确快速预测方面发挥着至关重要的作用。与传统模型相比，基于深度学习的雷达回波外推算法更加有效和高效。然而，高度可靠且具有广泛适用性的算法的发展受到三个主要瓶颈的制约：累积误差扩散、稀疏回波分布的不精确表示以及非平稳运动过程的不准确描述。为了解决这些问题，本文提出了一种利用时空相关特征和Transformer技术的新型雷达回波外推算法。该算法从多帧回波图像中提取特征，准确地表示了降水的非平稳运动过程。所提出的算法使用一种新的并行Temporal-Spatial Parallel Transformer（TempEE），确保了在雷达回波外推中的高准确性和高效率。此外，该算法克服了传统NWP模型和自回归算法的局限性，能够准确地在回波持续时间之外进行外推。在三个真实数据集上，所提出的算法优于现有最先进的方法，展示了其实际应用的潜力。

    The meteorological radar reflectivity data, also known as echo, plays a crucial role in predicting precipitation and enabling accurate and fast forecasting of short-term heavy rainfall without the need for complex Numerical Weather Prediction (NWP) model. Compared to conventional model, Deep Learning (DL)-based radar echo extrapolation algorithms are more effective and efficient. However, the development of highly reliable and generalized algorithms is hindered by three main bottlenecks: cumulative error spreading, imprecise representation of sparse echo distribution, and inaccurate description of non-stationary motion process. To address these issues, this paper presents a novel radar echo extrapolation algorithm that utilizes temporal-spatial correlation features and the Transformer technology. The algorithm extracts features from multi-frame echo images that accurately represent non-stationary motion processes for precipitation prediction. The proposed algorithm uses a novel paralle
    
[^44]: 结构复杂、具有附加父因果关系的SCARY数据集

    The Structurally Complex with Additive Parent Causality (SCARY) Dataset. (arXiv:2304.14109v1 [stat.ML])

    [http://arxiv.org/abs/2304.14109](http://arxiv.org/abs/2304.14109)

    SCARY数据集是一个具有结构复杂性和附加父因果关系的合成数据集，包含40个场景、三个不同的种子、线性和混合因果机制等特点，能够提供更真实的因果关系探索环境。

    

    因果关系数据集在推动因果学领域方面起着至关重要的作用。然而，现有数据集往往缺乏真实世界问题的复杂性，如选择偏差、不忠实数据和混淆。为了填补这一空白，我们提出了一个新的合成因果数据集，即结构复杂、具有附加父因果关系的SCARY数据集，它包括以下特征。数据集包括40个场景，每个场景生成3个不同的种子，使研究人员能够利用相关数据子集。此外，我们使用两种不同的数据生成机制来生成父节点和子节点之间的因果关系，包括线性和混合因果机制以及多个子类型。我们的数据集生成器受到因果发现工具箱的启发，仅生成加性模型。数据集的Varsortability为0.5。我们的SCARY数据集为研究人员在更现实的场景下探索因果发现提供了一个有价值的资源。

    Causal datasets play a critical role in advancing the field of causality. However, existing datasets often lack the complexity of real-world issues such as selection bias, unfaithful data, and confounding. To address this gap, we propose a new synthetic causal dataset, the Structurally Complex with Additive paRent causalitY (SCARY) dataset, which includes the following features. The dataset comprises 40 scenarios, each generated with three different seeds, allowing researchers to leverage relevant subsets of the dataset. Additionally, we use two different data generation mechanisms for generating the causal relationship between parents and child nodes, including linear and mixed causal mechanisms with multiple sub-types. Our dataset generator is inspired by the Causal Discovery Toolbox and generates only additive models. The dataset has a Varsortability of 0.5. Our SCARY dataset provides a valuable resource for researchers to explore causal discovery under more realistic scenarios. The
    
[^45]: DataComp：寻找下一代多模态数据集

    DataComp: In search of the next generation of multimodal datasets. (arXiv:2304.14108v1 [cs.CV])

    [http://arxiv.org/abs/2304.14108](http://arxiv.org/abs/2304.14108)

    DataComp是一个基准测试，旨在通过提出新的训练集来解决数据集在机器学习生态系统中的缺陷。它提供了一个多规模设计的实验测试平台，使用12.8B个图像-文本对的新候选池，让研究人员可以通过设计新的过滤技术或策划新的数据源并评估它们的新数据集来进行创新。

    

    大型的多模态数据集在近期的突破中起到了关键作用，比如CLIP、Stable Diffusion和GPT-4等。与此同时，数据集很少得到与模型架构或训练算法同等的研究关注。为了解决这个在机器学习生态系统中的缺陷，我们介绍了DataComp，一个基准测试，其中训练代码是固定的，研究人员通过提出新的训练集来进行创新。我们提供了一个基于Common Crawl的新候选池，其中包含12.8B个图像-文本对的数据集实验测试平台。参加我们基准测试的研究人员可以设计新的过滤技术或策划新的数据源，并通过运行我们标准化的CLIP训练代码并在38个下游测试集上进行测试来评估他们的新数据集。我们的基准测试包含多个规模，四个候选池大小和相应的计算预算，在训练期间涵盖了从12.8M到12.8B个样本。这种多规模设计有助于研究规模趋势，并为研究人员提供了更多的选择余地。

    Large multimodal datasets have been instrumental in recent breakthroughs such as CLIP, Stable Diffusion, and GPT-4. At the same time, datasets rarely receive the same research attention as model architectures or training algorithms. To address this shortcoming in the machine learning ecosystem, we introduce DataComp, a benchmark where the training code is fixed and researchers innovate by proposing new training sets. We provide a testbed for dataset experiments centered around a new candidate pool of 12.8B image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing on 38 downstream test sets. Our benchmark consists of multiple scales, with four candidate pool sizes and associated compute budgets ranging from 12.8M to 12.8B samples seen during training. This multi-scale design facilitates the study of scaling trends and makes the
    
[^46]: 从弱文本监督中学习图像中的人际互动

    Learning Human-Human Interactions in Images from Weak Textual Supervision. (arXiv:2304.14104v1 [cs.CV])

    [http://arxiv.org/abs/2304.14104](http://arxiv.org/abs/2304.14104)

    本文提出了一种新的范式，从单一的静态图像中学习自由文本的形式来灵活建模人际互动。并通过知识蒸馏生成伪标签来训练一种字幕模型，用于有效理解图像中的人际互动，具有较高的预测文本和语义质量，并在此任务上优于SOTA的图像字幕和情境识别模型。

    

    人际互动是多样且依赖于上下文的，但先前的工作将它们视为分类，忽略了可能的互动的重尾。本文提出了一种新的学习人际互动的范式，将其作为自由文本从单一的静态图像中学习，从而允许对情况和人际关系的无限空间进行灵活建模。为了克服缺乏特定于此任务的标记数据的问题，我们使用知识蒸馏应用于由大型语言模型产生的合成字幕数据，以此生成伪标签。我们展示了通过这个过程产生的伪标签可以用于训练一种字幕模型，能有效理解图像中的人际互动，通过衡量我们预测的文本和语义质量与事实的基础性的各种指标来衡量。我们进一步展示了我们的方法在这个任务上的性能优于SOTA的图像字幕和情境识别模型。我们将公开我们的代码。

    Interactions between humans are diverse and context-dependent, but previous works have treated them as categorical, disregarding the heavy tail of possible interactions. We propose a new paradigm of learning human-human interactions as free text from a single still image, allowing for flexibility in modeling the unlimited space of situations and relationships between people. To overcome the absence of data labelled specifically for this task, we use knowledge distillation applied to synthetic caption data produced by a large language model without explicit supervision. We show that the pseudo-labels produced by this procedure can be used to train a captioning model to effectively understand human-human interactions in images, as measured by a variety of metrics that measure textual and semantic faithfulness and factual groundedness of our predictions. We further show that our approach outperforms SOTA image captioning and situation recognition models on this task. We will release our c
    
[^47]: SocNavGym：一个针对社交导航的强化学习仿真环境

    SocNavGym: A Reinforcement Learning Gym for Social Navigation. (arXiv:2304.14102v1 [cs.RO])

    [http://arxiv.org/abs/2304.14102](http://arxiv.org/abs/2304.14102)

    本文提出了SocNavGym，对于社交导航领域的研究提供了一个轻便、快速、易用的仿真环境，可生成各种各样的社交导航场景，并促进了智能社交机器人的发展。

    

    在人口密集的环境下，自主机器人在导航时需要遵守社交规范。机器学习，尤其是深度强化学习，最近在社交导航领域中取得了显著进展。这可以部分归因于生成的策略不受代码复杂性或处理的变量数量等人类限制。不幸的是，DRL算法缺乏安全保障，需要大量数据需求，导致在现实环境中的应用不太切实际。为了缩小这一差距，仿真环境被广泛使用。本文提出了SocNavGym，一个专门针对社交导航的先进仿真环境，可以生成各种各样的社交导航场景，并促进智能社交机器人的发展。SocNavGym轻便、快速、易于使用，并可轻松配置以生成不同类型的社交导航场景。此外，它还可以配置为使用不同的传感器，并支持无障碍环境的仿真。

    It is essential for autonomous robots to be socially compliant while navigating in human-populated environments. Machine Learning and, especially, Deep Reinforcement Learning have recently gained considerable traction in the field of Social Navigation. This can be partially attributed to the resulting policies not being bound by human limitations in terms of code complexity or the number of variables that are handled. Unfortunately, the lack of safety guarantees and the large data requirements by DRL algorithms make learning in the real world unfeasible. To bridge this gap, simulation environments are frequently used. We propose SocNavGym, an advanced simulation environment for social navigation that can generate a wide variety of social navigation scenarios and facilitates the development of intelligent social agents. SocNavGym is light-weight, fast, easy-to-use, and can be effortlessly configured to generate different types of social navigation scenarios. It can also be configured to
    
[^48]: 可解释人工智能的范畴基础：一种统一的结构和语义形式体系。

    Categorical Foundations of Explainable AI: A Unifying Formalism of Structures and Semantics. (arXiv:2304.14094v1 [cs.AI])

    [http://arxiv.org/abs/2304.14094](http://arxiv.org/abs/2304.14094)

    本文采用范畴理论的框架，提出了可解释AI的统一理论体系，为领域中所有重要术语提供了清晰的形式定义，并提供了遵循所提出结构的领域分类法。

    

    可解释人工智能（XAI）旨在回答与AI模型部署相关的伦理和法律问题。然而，相当数量的领域特定评论强调需要一个数学基础来定义领域中的关键概念，即使“解释”这个术语还缺乏精确定义。这些评论还主张建立一个健全而统一的可解释AI形式体系，以避免出现不良提出问题，帮助研究人员浏览一个快速增长的知识体系。据作者所知，该论文是填补该空白的首次尝试，通过形式化一个可解释AI的统一理论。采用范畴理论的框架，特别是反馈单调范畴，我们首先提供了可解释AI中所有重要术语的形式定义。然后，我们提出了一个遵循提出结构的领域分类法，展示了如何使用引入的理论来对当前研究的所有主要XAI系统类进行分类。

    Explainable AI (XAI) aims to answer ethical and legal questions associated with the deployment of AI models. However, a considerable number of domain-specific reviews highlight the need of a mathematical foundation for the key notions in the field, considering that even the term "explanation" still lacks a precise definition. These reviews also advocate for a sound and unifying formalism for explainable AI, to avoid the emergence of ill-posed questions, and to help researchers navigate a rapidly growing body of knowledge. To the authors knowledge, this paper is the first attempt to fill this gap by formalizing a unifying theory of XAI. Employing the framework of category theory, and feedback monoidal categories in particular, we first provide formal definitions for all essential terms in explainable AI. Then we propose a taxonomy of the field following the proposed structure, showing how the introduced theory can be used to categorize all the main classes of XAI systems currently studi
    
[^49]: JaxPruner：一个用于稀疏性研究的简明库

    JaxPruner: A concise library for sparsity research. (arXiv:2304.14082v1 [cs.LG])

    [http://arxiv.org/abs/2304.14082](http://arxiv.org/abs/2304.14082)

    本文介绍了JaxPruner，一款用于研究稀疏神经网络的开源库。JaxPruner提供了流行的剪枝和稀疏训练算法的简明实现，最小化内存和延迟开销，并可轻松集成到现有的JAX库中。

    

    本文介绍了JaxPruner，这是一个基于JAX的开源剪枝和稀疏训练库，旨在通过提供流行的剪枝和稀疏训练算法的简明实现，最小化内存和延迟开销，加速稀疏神经网络的研究。JaxPruner实现的算法使用通用API，并与流行的优化库Optax无缝协作，从而使其能轻松集成到现有的JAX库中。我们通过在四个不同的代码库中提供示例并在流行的基准测试中提供基准实验来展示这种集成的便捷性。

    This paper introduces JaxPruner, an open-source JAX-based pruning and sparse training library for machine learning research. JaxPruner aims to accelerate research on sparse neural networks by providing concise implementations of popular pruning and sparse training algorithms with minimal memory and latency overhead. Algorithms implemented in JaxPruner use a common API and work seamlessly with the popular optimization library Optax, which, in turn, enables easy integration with existing JAX based libraries. We demonstrate this ease of integration by providing examples in four different codebases: Scenic, t5x, Dopamine and FedJAX and provide baseline experiments on popular benchmarks.
    
[^50]: 聚类流（Cluster Flow）：如何通过层次聚类使深度神经网络更具鲁棒性、更符合人类思考方式、更易于实现关系推理

    Cluster Flow: how a hierarchical clustering layer make allows deep-NNs more resilient to hacking, more human-like and easily implements relational reasoning. (arXiv:2304.14081v1 [cs.LG])

    [http://arxiv.org/abs/2304.14081](http://arxiv.org/abs/2304.14081)

    该论文介绍了一种半监督层次聚类框架ClusterFlow，可以在经过训练的深度卷积神经网络中使用密集的多维类别和特征数据来构建超空间地图，从而使其更具人类思考方式的功能，提高了其鲁棒性和实现关系推理的能力。

    

    尽管神经网络（NNs）在人工智能领域（特别是深度卷积网络）取得了巨大的突破，但它们并没有达到人类水平的表现：它们可以被智能攻击，而人们无法被欺骗，也缺乏常识。已经有人认为，人类智能的基础是人类能够进行关系推理的能力：比较不同对象，测量相似性，掌握对象之间的关系。聚类流是一个半监督的层次聚类框架，可以通过利用预 SoftMax 层中发现的丰富的多维类别和特征数据，在经过训练的 NNs 上进行操作，构建类/特征的超空间地图，并将这些功能添加到现代深度卷积神经网络中，从而增加了更具人类思考方式的功能。作者通过3个任务来证明这一点。

    Despite the huge recent breakthroughs in neural networks (NNs) for artificial intelligence (specifically deep convolutional networks) such NNs do not achieve human-level performance: they can be hacked by images that would fool no human and lack `common sense'. It has been argued that a basis of human-level intelligence is mankind's ability to perform relational reasoning: the comparison of different objects, measuring similarity, grasping of relations between objects and the converse, figuring out the odd one out in a set of objects. Mankind can even do this with objects they have never seen before. Here we show how ClusterFlow, a semi-supervised hierarchical clustering framework can operate on trained NNs utilising the rich multi-dimensional class and feature data found at the pre-SoftMax layer to build a hyperspacial map of classes/features and this adds more human-like functionality to modern deep convolutional neural networks. We demonstrate this with 3 tasks. 1. the statistical l
    
[^51]: ganX -- 一个生成MA-XRF原始数据的Python库，可以从RGB图像中生成X射线荧光宏观图谱

    ganX -- generate artificially new XRF a python library to generate MA-XRF raw data out of RGB images. (arXiv:2304.14078v1 [physics.app-ph])

    [http://arxiv.org/abs/2304.14078](http://arxiv.org/abs/2304.14078)

    ganX是一个Python库，可以从RGB图像中生成X射线荧光宏观图谱(MA-XRF)，使用了蒙特卡洛方法和颜料特征XRF信号的数据库实现。

    

    本文介绍了ganX -- generate artificially new XRF的第一个版本，这是一个Python库，可以从彩色RGB图像中生成X射线荧光宏观图谱(MA-XRF)。为了实现这一功能，使用了蒙特卡洛方法，其中每个MA-XRF像素信号都是从XRF信号概率函数中采样得到的。这个概率函数是通过使用(颜料特征XRF信号，RGB)的数据库，通过计算颜料XRF信号的加权和，以及图像RGB与颜料特征RGB的接近程度来计算的。该库已在PyPi上发布，并在GitHub上以开源的方式提供代码。

    In this paper we present the first version of ganX -- generate artificially new XRF, a Python library to generate X-ray fluorescence Macro maps (MA-XRF) from a coloured RGB image. To do that, a Monte Carlo method is used, where each MA-XRF pixel signal is sampled out of an XRF signal probability function. Such probability function is computed using a database of couples (pigment characteristic XRF signal, RGB), by a weighted sum of such pigment XRF signal by proximity of the image RGB to the pigment characteristic RGB. The library is released to PyPi and the code is available open source on GitHub.
    
[^52]: 组合式3D人物-物体神经动画

    Compositional 3D Human-Object Neural Animation. (arXiv:2304.14070v1 [cs.CV])

    [http://arxiv.org/abs/2304.14070](http://arxiv.org/abs/2304.14070)

    提出了一种组合式的方法来解决人物-物体交互动画的挑战，以实现新的HOI的组合式动画控制。

    

    人-物交互对于人类中心的场景理解应用，如人类中心的视觉生成、AR/VR和机器人技术至关重要。本文提出了一种组合式的视角来解决人物-物体交互动画的挑战，即通过新的姿势序列，动画化新的人物和/或新的物体的新的交互行为。具体而言，我们采用神经人物-物体变形来建模和渲染基于隐式神经表示的HOI动态。为了使人物和物体之间的交互姿势在不同的人物和物体之间转移，我们设计了一种新的组合式条件神经辐射场(CC-NeRF)，该方法使用潜在码将人物和物体之间的相互依赖关系解耦，以实现新的HOI的组合式动画控制。实验证明，所提出的方法可以很好地推广到各种新的HOI动画设置中。

    Human-object interactions (HOIs) are crucial for human-centric scene understanding applications such as human-centric visual generation, AR/VR, and robotics. Since existing methods mainly explore capturing HOIs, rendering HOI remains less investigated. In this paper, we address this challenge in HOI animation from a compositional perspective, i.e., animating novel HOIs including novel interaction, novel human and/or novel object driven by a novel pose sequence. Specifically, we adopt neural human-object deformation to model and render HOI dynamics based on implicit neural representations. To enable the interaction pose transferring among different persons and objects, we then devise a new compositional conditional neural radiance field (or CC-NeRF), which decomposes the interdependence between human and object using latent codes to enable compositionally animation control of novel HOIs. Experiments show that the proposed method can generalize well to various novel HOI animation setting
    
[^53]: 可解释的神经符号概念推理

    Interpretable Neural-Symbolic Concept Reasoning. (arXiv:2304.14068v1 [cs.AI])

    [http://arxiv.org/abs/2304.14068](http://arxiv.org/abs/2304.14068)

    本文提出了第一个基于概念嵌入的可解释概念模型DCR，能够在多个数据集上实现接近最先进的准确性，相对于最先进的可解释概念模型提高了高达+25％，并产生能够解释其预测的人类可理解规则和真值度，适应性强。

    

    深度学习方法具有高度的准确性，但它们不透明的决策过程阻止了它们获得完全的人类信任。概念模型旨在通过学习一组人类可理解的概念来解决这个问题。然而，最先进的概念模型依赖于高维概念嵌入表示，缺乏明确的语义含义，因此质疑其决策过程的可解释性。为了克服这个限制，我们提出了Deep Concept Reasoner(DCR)，这是第一个基于概念嵌入的可解释概念模型。在DCR中，神经网络不直接进行任务预测，而是使用概念嵌入建立语法规则结构。然后DCR在有意义的概念真值度上执行这些规则，以不可微分的方式提供最终的可解释和语义一致的预测。我们的实验表明，DCR：(i)在多个数据集上实现接近最先进的准确性，同时相对于最先进的可解释概念模型提高了高达+25％;(ii)产生能够解释其预测的人类可理解规则和真值度;(iii)很容易适应新领域。

    Deep learning methods are highly accurate, yet their opaque decision process prevents them from earning full human trust. Concept-based models aim to address this issue by learning tasks based on a set of human-understandable concepts. However, state-of-the-art concept-based models rely on high-dimensional concept embedding representations which lack a clear semantic meaning, thus questioning the interpretability of their decision process. To overcome this limitation, we propose the Deep Concept Reasoner (DCR), the first interpretable concept-based model that builds upon concept embeddings. In DCR, neural networks do not make task predictions directly, but they build syntactic rule structures using concept embeddings. DCR then executes these rules on meaningful concept truth degrees to provide a final interpretable and semantically-consistent prediction in a differentiable manner. Our experiments show that DCR: (i) improves up to +25% w.r.t. state-of-the-art interpretable concept-based
    
[^54]: 在非线性数据驱动动力学模型中传播内核歧义集合

    Propagating Kernel Ambiguity Sets in Nonlinear Data-driven Dynamics Models. (arXiv:2304.14057v1 [math.OC])

    [http://arxiv.org/abs/2304.14057](http://arxiv.org/abs/2304.14057)

    该论文提出了一种算法，通过内核最大均值差将歧义集合传递到非线性数据驱动模型中，解决了分布式鲁棒控制和基于学习的控制中的关键问题。

    

    本文解决了一个开放问题：给定一个非线性的数据驱动动力学系统模型，如内核条件均值嵌入和Koopman算子，如何在多个步骤中向前传播歧义集合？这个问题是解决分布式鲁棒控制和基于学习的控制在数据分布转移下的关键。与先前使用静态歧义集合（例如固定的Wasserstein球）或在已知分段线性（或仿射）动力学下使用动态歧义集合的作品不同，我们提出了一种算法，通过Koopman算子和CME，通过内核最大均值差几何传递歧义集合。通过理论和数字分析，我们表明，我们的内核歧义集合是学习的数据驱动动力学系统模型的自然几何结构。

    This paper provides answers to an open problem: given a nonlinear data-driven dynamical system model, e.g., kernel conditional mean embedding (CME) and Koopman operator, how can one propagate the ambiguity sets forward for multiple steps? This problem is the key to solving distributionally robust control and learning-based control of such learned system models under a data-distribution shift. Different from previous works that use either static ambiguity sets, e.g., fixed Wasserstein balls, or dynamic ambiguity sets under known piece-wise linear (or affine) dynamics, we propose an algorithm that exactly propagates ambiguity sets through nonlinear data-driven models using the Koopman operator and CME, via the kernel maximum mean discrepancy geometry. Through both theoretical and numerical analysis, we show that our kernel ambiguity sets are the natural geometric structure for the learned data-driven dynamical system models.
    
[^55]: 精确的T1加权MRI少数样本无脂肪肌肉切割

    Precise Few-shot Fat-free Thigh Muscle Segmentation in T1-weighted MRI. (arXiv:2304.14053v1 [eess.IV])

    [http://arxiv.org/abs/2304.14053](http://arxiv.org/abs/2304.14053)

    通过新的少样本分割框架，准确无误地生成股骨肌肉掩蔽，剔除IMF，有助于肌肉容积分析，尤其是在缺乏大量精确注释的情况下。

    

    精确的股骨肌体积对于监测患有各种程度股骨肌损失疾病患者的运动功能至关重要。由于其在肌肉和脂肪信号之间的对比，T1加权MRI是获得股骨肌肉掩蔽的默认替代方法。最近，深度学习方法被广泛用于通过分割获得这些掩蔽。然而，由于精确定义的注释数量不足，通过深度学习方法生成的股骨肌肉掩蔽往往会将肌内脂肪(IMF)误分类为肌肉，影响肌肉容积分析。由于IMF被浸润在肌肉内，人类注释需要专业知识和时间。因此，在实践中，剔除IMF的精确肌肉掩蔽是有限的。为了缓解这个问题，我们提出了一个少样本分割框架以生成剔除IMF的股骨肌肉掩蔽。在我们的框架中，我们设计了新的伪标签修正和评估方案，以及一个新的抗噪损失。

    Precise thigh muscle volumes are crucial to monitor the motor functionality of patients with diseases that may result in various degrees of thigh muscle loss. T1-weighted MRI is the default surrogate to obtain thigh muscle masks due to its contrast between muscle and fat signals. Deep learning approaches have recently been widely used to obtain these masks through segmentation. However, due to the insufficient amount of precise annotations, thigh muscle masks generated by deep learning approaches tend to misclassify intra-muscular fat (IMF) as muscle impacting the analysis of muscle volumetrics. As IMF is infiltrated inside the muscle, human annotations require expertise and time. Thus, precise muscle masks where IMF is excluded are limited in practice. To alleviate this, we propose a few-shot segmentation framework to generate thigh muscle masks excluding IMF. In our framework, we design a novel pseudo-label correction and evaluation scheme, together with a new noise robust loss for e
    
[^56]: 交错图和注意力网络用于3D人体姿态估计

    Interweaved Graph and Attention Network for 3D Human Pose Estimation. (arXiv:2304.14045v1 [cs.CV])

    [http://arxiv.org/abs/2304.14045](http://arxiv.org/abs/2304.14045)

    该论文提出了一种名为交错图和注意力网络（IGANet）的模型，通过图卷积网络（GCNs）和注意力之间的双向通信，解决了先前单视角图像3D人体姿态估计中忽略全局和局部关联的问题。在人体姿态估计上取得了最先进的结果。

    

    尽管在单视角图像的3D人体姿态估计方面取得了重大进展，但以往的工作很少探索全局和局部相互关系，导致对人体骨骼表示的学习不足。为了解决这个问题，我们提出了一种新的交错图和注意力网络（IGANet），它允许图卷积网络（GCN）和注意力之间的双向通信。具体来说，我们引入了IGA模块，其中GCNs向注意力提供局部信息，注意力向GCNs注入全局信息。此外，我们设计了一个简单而有效的U形多层感知器（uMLP），可以捕捉身体关节的多粒度信息。我们在两个流行的基准数据集上进行了广泛的试验（即Human3. 6M和MPI-INF-3DHP），以评估我们提出的方法。结果表明，IGANet在两个数据集上均取得了最先进的性能。代码可在https://github.com/xiu-cs/IGANet上获得。

    Despite substantial progress in 3D human pose estimation from a single-view image, prior works rarely explore global and local correlations, leading to insufficient learning of human skeleton representations. To address this issue, we propose a novel Interweaved Graph and Attention Network (IGANet) that allows bidirectional communications between graph convolutional networks (GCNs) and attentions. Specifically, we introduce an IGA module, where attentions are provided with local information from GCNs and GCNs are injected with global information from attentions. Additionally, we design a simple yet effective U-shaped multi-layer perceptron (uMLP), which can capture multi-granularity information for body joints. Extensive experiments on two popular benchmark datasets (i.e. Human3.6M and MPI-INF-3DHP) are conducted to evaluate our proposed method.The results show that IGANet achieves state-of-the-art performance on both datasets. Code is available at https://github.com/xiu-cs/IGANet.
    
[^57]: 正交解耦高斯过程的球形感应特征

    Spherical Inducing Features for Orthogonally-Decoupled Gaussian Processes. (arXiv:2304.14034v1 [cs.LG])

    [http://arxiv.org/abs/2304.14034](http://arxiv.org/abs/2304.14034)

    本文研究了解耦高斯过程的正交分解问题，提出了一种扩展方法，即引入球形跨域特征，构建更灵活的数据依赖基函数来缓解限制，并展示了其有效性。

    

    尽管高斯过程（GPs）具有许多优点，但它们缺乏学习表征的能力，因此经常与深度神经网络（NNs）进行比较。最近的工作通过在诱导变量与前馈NN的隐藏单元之间建立联系的跨域变分GPs来弥合 GPs和深度NN之间的差距。本文在研究此方法与实际应用中的一些实际问题，并提出一种扩展方法，利用GPs的正交分解来减轻这些限制。具体地，我们引入球形跨域特征，构建更灵活的数据依赖基函数，用于GP逼近的主要和正交分量，结果表明在此框架下加入NN激活特征，不仅可以缓解这些问题，而且比其他策略更具有可扩展性。在多个基准数据集上的实验表明了我们方法的有效性。

    Despite their many desirable properties, Gaussian processes (GPs) are often compared unfavorably to deep neural networks (NNs) for lacking the ability to learn representations. Recent efforts to bridge the gap between GPs and deep NNs have yielded a new class of inter-domain variational GPs in which the inducing variables correspond to hidden units of a feedforward NN. In this work, we examine some practical issues associated with this approach and propose an extension that leverages the orthogonal decomposition of GPs to mitigate these limitations. In particular, we introduce spherical inter-domain features to construct more flexible data-dependent basis functions for both the principal and orthogonal components of the GP approximation and show that incorporating NN activation features under this framework not only alleviates these shortcomings but is more scalable than alternative strategies. Experiments on multiple benchmark datasets demonstrate the effectiveness of our approach.
    
[^58]: 通过敏感性曲线最大化对鲁棒分布式学习方案的攻击

    Attacks on Robust Distributed Learning Schemes via Sensitivity Curve Maximization. (arXiv:2304.14024v1 [cs.LG])

    [http://arxiv.org/abs/2304.14024](http://arxiv.org/abs/2304.14024)

    本论文介绍了一种新的攻击方法——敏感性曲线最大化（SCM），它能够通过注入微小但有效的扰动来破坏现有的鲁棒分布式学习方案。

    

    分布式学习范例，诸如联邦学习或分散式学习，允许集合代理通过有限的局部交互解决全局学习和优化问题。大多数这样的策略依赖于本地适应和聚合步骤的混合，无论是在对等方之间还是在中央融合中心。传统上，分布式学习中的聚合是基于平均值的，这是统计上有效的，但容易受到少数恶意代理的攻击。这一观察促使了最近一些工作的出现，它们采用均值的鲁棒变体来开发健壮的聚合方案。我们提出了一种基于敏感性曲线最大化（SCM）的新攻击，并证明它能够通过注入微小但有效的扰动来破坏现有的鲁棒聚合方案。

    Distributed learning paradigms, such as federated or decentralized learning, allow a collection of agents to solve global learning and optimization problems through limited local interactions. Most such strategies rely on a mixture of local adaptation and aggregation steps, either among peers or at a central fusion center. Classically, aggregation in distributed learning is based on averaging, which is statistically efficient, but susceptible to attacks by even a small number of malicious agents. This observation has motivated a number of recent works, which develop robust aggregation schemes by employing robust variations of the mean. We present a new attack based on sensitivity curve maximization (SCM), and demonstrate that it is able to disrupt existing robust aggregation schemes by injecting small, but effective perturbations.
    
[^59]: 基于XAI的音频事件分类输入表示法比较

    XAI-based Comparison of Input Representations for Audio Event Classification. (arXiv:2304.14019v1 [cs.SD])

    [http://arxiv.org/abs/2304.14019](http://arxiv.org/abs/2304.14019)

    本文比较了两种不同的音频事件分类输入表示法的模型，即原始波形信号和时间-频率谱，利用可解释人工智能（XAI）揭示了不同表示法的决策策略，为正确的决策提供了深入的见解。

    

    深度神经网络是音频事件分类的有前途的工具。相对于自然图像等其他数据，音频数据有许多明智和非显而易见的表示法，这些表示法可以作为这些模型的输入。在本文中，我们利用可解释人工智能（XAI）来了解不同输入表示法训练的模型所使用的基本分类策略。具体而言，我们比较两种模型体系结构，以了解与音频事件检测相关的输入特征。我们展示了通过“Siren”（层面相关传递）获得的相关热图如何揭示表示法相关的决策策略。通过这些深入的洞察，我们能够作出明智的决策。

    Deep neural networks are a promising tool for Audio Event Classification. In contrast to other data like natural images, there are many sensible and non-obvious representations for audio data, which could serve as input to these models. Due to their black-box nature, the effect of different input representations has so far mostly been investigated by measuring classification performance. In this work, we leverage eXplainable AI (XAI), to understand the underlying classification strategies of models trained on different input representations. Specifically, we compare two model architectures with regard to relevant input features used for Audio Event Detection: one directly processes the signal as the raw waveform, and the other takes in its time-frequency spectrogram representation. We show how relevance heatmaps obtained via "Siren"{Layer-wise Relevance Propagation} uncover representation-dependent decision strategies. With these insights, we can make a well-informed decision about the
    
[^60]: 基于Transformer的边缘完整度及应用于矢量字体数据的研究

    Contour Completion by Transformers and Its Application to Vector Font Data. (arXiv:2304.13988v1 [cs.GR])

    [http://arxiv.org/abs/2304.13988](http://arxiv.org/abs/2304.13988)

    本文提出了一种基于Transformer的方法，来解决轮廓完整度问题，特别适用于字体轮廓。

    

    在文档和图形中，轮廓是描述特定形状的流行格式。例如，在True Type Font（TTF）文件格式中，轮廓描述字形的矢量轮廓。每个轮廓通常被定义为点序列。在这篇论文中，我们解决了轮廓完整度问题。在这个任务中，输入是一个包含缺失点的轮廓序列，输出是一个生成的完整轮廓。这个任务比图像完成任务更加困难，因为对于图像，缺失的像素是可以指示的，而在轮廓完整度任务中没有这样的指示，我们必须同时解决缺失部分检测和完成问题。我们提出了一种基于Transformer的方法来解决这个问题，并展示了字体轮廓完整度的结果。

    In documents and graphics, contours are a popular format to describe specific shapes. For example, in the True Type Font (TTF) file format, contours describe vector outlines of typeface shapes. Each contour is often defined as a sequence of points. In this paper, we tackle the contour completion task. In this task, the input is a contour sequence with missing points, and the output is a generated completed contour. This task is more difficult than image completion because, for images, the missing pixels are indicated. Since there is no such indication in the contour completion task, we must solve the problem of missing part detection and completion simultaneously. We propose a Transformer-based method to solve this problem and show the results of the typeface contour completion.
    
[^61]: 针对领域泛化的中度分布探索

    Moderately Distributional Exploration for Domain Generalization. (arXiv:2304.13976v1 [cs.LG])

    [http://arxiv.org/abs/2304.13976](http://arxiv.org/abs/2304.13976)

    本文提出了一种针对领域泛化问题的中度分布探索（MODE）方法，通过在共享相同语义因素的不确定性子集中探索领域，可以提高模型的分布偏移鲁棒性，并在多个基准数据集上实现了最先进的性能。

    

    领域泛化旨在解决训练领域与未知目标领域之间的分布偏移问题。生成新的领域是最有效的方法之一，然而其性能增益取决于生成的领域与目标领域之间的分布差异。分布鲁棒优化有望通过在不确定性集中探索领域来解决分布偏移问题。然而，不确定性集可能非常庞大，在领域泛化中会导致低置信度预测，因为大的不确定性集可能会引入包含与训练领域语义不同的因素的领域。为了解决这个问题，我们提出了一种针对领域泛化的中度分布探索（MODE）。具体而言，MODE在一个与训练领域共享相同语义因素的不确定性$\textit{子集}$中进行分布探索。我们证明，MODE可以为模型提供可证明的分布偏移鲁棒性，并在几个基准领域泛化数据集上实现了最先进的性能。

    Domain generalization (DG) aims to tackle the distribution shift between training domains and unknown target domains. Generating new domains is one of the most effective approaches, yet its performance gain depends on the distribution discrepancy between the generated and target domains. Distributionally robust optimization is promising to tackle distribution discrepancy by exploring domains in an uncertainty set. However, the uncertainty set may be overwhelmingly large, leading to low-confidence prediction in DG. It is because a large uncertainty set could introduce domains containing semantically different factors from training domains. To address this issue, we propose to perform a $\textbf{mo}$derately $\textbf{d}$istributional $\textbf{e}$xploration (MODE) for domain generalization. Specifically, MODE performs distribution exploration in an uncertainty $\textit{subset}$ that shares the same semantic factors with the training domains. We show that MODE can endow models with provabl
    
[^62]: 松弛假设下Adam收敛性的证明

    Convergence of Adam Under Relaxed Assumptions. (arXiv:2304.13972v1 [math.OC])

    [http://arxiv.org/abs/2304.13972](http://arxiv.org/abs/2304.13972)

    本文对Adam算法做了新的假设并进行了证明，证明了在更加现实的条件下，Adam能够以较小的梯度复杂度达到稳定点。

    

    本文针对一类广泛的优化目标，对自适应矩估计（Adam）算法的收敛性进行了严格证明。虽然Adam算法在训练深度神经网络中的流行度和效率很高，但其理论性质尚未完全理解，现有的收敛性证明需要过于强的假设，如全局梯度有界，以证明收敛到稳定点。本文证明了在更为现实的条件下，Adam能以$\mathcal{O}(\epsilon^{-4})$梯度复杂度收敛到$\epsilon$-稳定点。我们分析的关键是根据一种广义光滑性假设给出的，沿着优化轨迹的梯度有界的新证明。根据该假设，局部光滑性(即存在时的Hessian norm)受梯度范数的次平方函数限制。此外，我们提出了一种方差约减版本的Adam与加速Gradient。

    In this paper, we provide a rigorous proof of convergence of the Adaptive Moment Estimate (Adam) algorithm for a wide class of optimization objectives. Despite the popularity and efficiency of the Adam algorithm in training deep neural networks, its theoretical properties are not yet fully understood, and existing convergence proofs require unrealistically strong assumptions, such as globally bounded gradients, to show the convergence to stationary points. In this paper, we show that Adam provably converges to $\epsilon$-stationary points with $\mathcal{O}(\epsilon^{-4})$ gradient complexity under far more realistic conditions. The key to our analysis is a new proof of boundedness of gradients along the optimization trajectory, under a generalized smoothness assumption according to which the local smoothness (i.e., Hessian norm when it exists) is bounded by a sub-quadratic function of the gradient norm. Moreover, we propose a variance-reduced version of Adam with an accelerated gradien
    
[^63]: 公平性不确定性量化: 您有多大把握模型是公平的?

    Fairness Uncertainty Quantification: How certain are you that the model is fair?. (arXiv:2304.13950v1 [stat.ML])

    [http://arxiv.org/abs/2304.13950](http://arxiv.org/abs/2304.13950)

    本篇论文提出了一种针对机器学习模型公平性的不确定性量化方法，针对公平感知模型提供了置信区间（CI）来评估其测试不公平性。

    

    最近几年，由于机器学习在司法系统等敏感应用中的广泛使用，公平感知机器学习引起了广泛关注。提出了各种启发式和优化框架来强制实现分类中的公平性，其中后一种方法要么提供经验结果，要么为目标函数的精确最小化器提供公平性保证。在现代机器学习中，几乎总是使用随机梯度下降（SGD）类型的算法作为训练算法，这意味着学习的模型以及其公平性属性是随机的。因此，尤其是对于关键应用程序，必须构建置信区间（CI）以评估所学模型的公平性。在这项工作中，我们为测试不公平性提供了置信区间（CI），具体而言，是在考虑到群体公平性的前提下，即差异影响（DI）和不公平影响（DM）感知的线性二元分类模型。

    Fairness-aware machine learning has garnered significant attention in recent years because of extensive use of machine learning in sensitive applications like judiciary systems. Various heuristics, and optimization frameworks have been proposed to enforce fairness in classification \cite{del2020review} where the later approaches either provides empirical results or provides fairness guarantee for the exact minimizer of the objective function \cite{celis2019classification}. In modern machine learning, Stochastic Gradient Descent (SGD) type algorithms are almost always used as training algorithms implying that the learned model, and consequently, its fairness properties are random. Hence, especially for crucial applications, it is imperative to construct Confidence Interval (CI) for the fairness of the learned model. In this work we provide CI for test unfairness when a group-fairness-aware, specifically, Disparate Impact (DI), and Disparate Mistreatment (DM) aware linear binary classifi
    
[^64]: 1比特矩阵补全的主导-最小化高斯牛顿方法

    A Majorization-Minimization Gauss-Newton Method for 1-Bit Matrix Completion. (arXiv:2304.13940v1 [stat.ML])

    [http://arxiv.org/abs/2304.13940](http://arxiv.org/abs/2304.13940)

    本文提出了一种基于主导-最小化原则，通过低秩矩阵补全解决1比特矩阵补全问题的新方法，称为MMGN。通过应用高斯-牛顿方法，MMGN具有更快的速度和更准确的结果，同时还不太受到潜在矩阵尖锐度的影响。

    

    在1比特矩阵补全中，旨在从部分二进制观测值中估计潜在的低秩矩阵。我们提出了一种称为MMGN的1比特矩阵补全新方法。我们的方法基于主导-最小化（MM）原则，在我们的设置中产生一系列标准低秩矩阵补全问题。我们通过明确强制假定的低秩结构的分解方法解决这些子问题，然后应用高斯-牛顿方法。我们的数值研究和对实际数据的应用表明，MMGN输出的估计结果与现有方法相比较具有可比性且更准确、速度通常更快，并且对潜在矩阵的尖锐度不太敏感。

    In 1-bit matrix completion, the aim is to estimate an underlying low-rank matrix from a partial set of binary observations. We propose a novel method for 1-bit matrix completion called MMGN. Our method is based on the majorization-minimization (MM) principle, which yields a sequence of standard low-rank matrix completion problems in our setting. We solve each of these sub-problems by a factorization approach that explicitly enforces the assumed low-rank structure and then apply a Gauss-Newton method. Our numerical studies and application to a real-data example illustrate that MMGN outputs comparable if not more accurate estimates, is often significantly faster, and is less sensitive to the spikiness of the underlying matrix than existing methods.
    
[^65]: 一种用于类风湿关节炎中关节空隙变窄进展准确量化的深度配准方法

    A Deep Registration Method for Accurate Quantification of Joint Space Narrowing Progression in Rheumatoid Arthritis. (arXiv:2304.13938v1 [eess.IV])

    [http://arxiv.org/abs/2304.13938](http://arxiv.org/abs/2304.13938)

    本文提出了一种新的深度配准方法，用于自动量化早期RA中的JSN进展，实验结果表明其具有高精度和可靠性，可以作为监测关节空间的有效手段。

    

    类风湿性关节炎（RA）是一种慢性自身免疫性炎症性疾病，导致关节逐渐破坏和严重残疾。关节空隙变窄（JSN）进展被认为是RA进展的重要指标，并受到持续关注。在RA的诊断和监测中，放射学在监测关节空间中发挥关键作用。本文提出了一种新的框架，通过放射性影像配准来量化JSN进展，从而监测关节空间。该框架具有高精度的优点，但减少不匹配和提高可靠性等挑战仍需面对。本研究提出了一种深度单体内刚性配准网络，用于自动量化早期RA中的JSN进展。在实验中，运动图像与固定图像之间的欧几里得距离均方误差为0.0031，标准偏差为0.0661 mm，不匹配率为0.48％。所提出的方法具有很好的子

    Rheumatoid arthritis (RA) is a chronic autoimmune inflammatory disease that results in progressive articular destruction and severe disability. Joint space narrowing (JSN) progression has been regarded as an important indicator for RA progression and has received sustained attention. In the diagnosis and monitoring of RA, radiology plays a crucial role to monitor joint space. A new framework for monitoring joint space by quantifying JSN progression through image registration in radiographic images has been developed. This framework offers the advantage of high accuracy, however, challenges do exist in reducing mismatches and improving reliability. In this work, a deep intra-subject rigid registration network is proposed to automatically quantify JSN progression in the early stage of RA. In our experiments, the mean-square error of Euclidean distance between moving and fixed image is 0.0031, standard deviation is 0.0661 mm, and the mismatching rate is 0.48\%. The proposed method has sub
    
[^66]: 机器学习模型训练过程中对表现更好的少数族群进行过采样会稍微降低不良影响，但也会降低模型精度

    Oversampling Higher-Performing Minorities During Machine Learning Model Training Reduces Adverse Impact Slightly but Also Reduces Model Accuracy. (arXiv:2304.13933v1 [cs.LG])

    [http://arxiv.org/abs/2304.13933](http://arxiv.org/abs/2304.13933)

    本研究发现，在机器学习模型训练中，对表现更好的少数族裔进行过采样会稍微减少不良影响，但也会降低模型精度。

    

    组织越来越倾向于采用机器学习来进行员工评估。然而，人们对机器学习评估公平性的担忧也日益增加。本研究系统地对少数族裔（黑人和西班牙裔）进行了欠采样和过采样，以改变训练数据中的不良影响比率，并调查训练数据中不良影响比率如何影响机器学习模型的不良影响和准确性。我们使用工作申请人的自我报告和面试记录（N = 2,501）训练了9,702个机器学习模型来预测筛选决策。我们发现，训练数据中的不良影响与机器学习模型的不良影响呈线性相关。然而，从训练数据中消除不良影响仅稍微降低了机器学习模型的不良影响，而且也降低了模型精度。

    Organizations are increasingly adopting machine learning (ML) for personnel assessment. However, concerns exist about fairness in designing and implementing ML assessments. Supervised ML models are trained to model patterns in data, meaning ML models tend to yield predictions that reflect subgroup differences in applicant attributes in the training data, regardless of the underlying cause of subgroup differences. In this study, we systematically under- and oversampled minority (Black and Hispanic) applicants to manipulate adverse impact ratios in training data and investigated how training data adverse impact ratios affect ML model adverse impact and accuracy. We used self-reports and interview transcripts from job applicants (N = 2,501) to train 9,702 ML models to predict screening decisions. We found that training data adverse impact related linearly to ML model adverse impact. However, removing adverse impact from training data only slightly reduced ML model adverse impact and tende
    
[^67]: 识别时间序列图像数据中的对抗性物理攻击

    Detection of Adversarial Physical Attacks in Time-Series Image Data. (arXiv:2304.13919v1 [cs.CV])

    [http://arxiv.org/abs/2304.13919](http://arxiv.org/abs/2304.13919)

    本论文提出了一种针对时间序列图像数据的对抗性物理攻击检测方法，使用VisionGuard和多数投票结合的方法，以应对自主系统应用中普遍存在的对抗性攻击问题。

    

    深度神经网络已成为自主系统中常见的感知模态，因为它们能够在给定输入图像的情况下对环境进行语义感知。然而，DNN模型已被证明易受到对抗性数字和物理攻击的影响。为了解决这个问题，已经提出了几个检测框架来检测单个输入图像是否受到对抗性数字噪声的操纵。在我们之前的工作中，我们提出了一种实时检测器，称为VisionGuard（VG），用于检测针对单个输入图像的DNN模型的对抗性物理攻击。在此基础上，我们提出了VisionGuard *（VG），它将VG与多数投票方法相结合，以检测时间序列图像数据中的对抗性物理攻击，例如视频。这受到自主系统应用的启发，其中使用机载传感器随时间收集图像以进行决策。我们强调，多数投票机制在自主系统应用中非常普遍。

    Deep neural networks (DNN) have become a common sensing modality in autonomous systems as they allow for semantically perceiving the ambient environment given input images. Nevertheless, DNN models have proven to be vulnerable to adversarial digital and physical attacks. To mitigate this issue, several detection frameworks have been proposed to detect whether a single input image has been manipulated by adversarial digital noise or not. In our prior work, we proposed a real-time detector, called VisionGuard (VG), for adversarial physical attacks against single input images to DNN models. Building upon that work, we propose VisionGuard* (VG), which couples VG with majority-vote methods, to detect adversarial physical attacks in time-series image data, e.g., videos. This is motivated by autonomous systems applications where images are collected over time using onboard sensors for decision-making purposes. We emphasize that majority-vote mechanisms are quite common in autonomous system ap
    
[^68]: 比例代表性聚类

    Proportionally Representative Clustering. (arXiv:2304.13917v1 [cs.LG])

    [http://arxiv.org/abs/2304.13917](http://arxiv.org/abs/2304.13917)

    本文提出了一个新的公平性准则——比例代表性公平性（PRF），并设计了有效的算法满足该准则。

    

    近年来，机器学习领域对公平概念的形式化表述越来越受关注。本文关注于聚类，是无监督机器学习中最基础的任务之一。我们提出了一个新的公平性准则——比例代表性公平性（PRF），我们认为该概念以一种更有说服力的方式达到了文献中几个现存概念的理由。但现有的公平聚类算法不能满足我们的公平性概念。我们设计了高效的算法，以满足无约束聚类和离散聚类问题的PRF。

    In recent years, there has been a surge in effort to formalize notions of fairness in machine learning. We focus on clustering -- one of the fundamental tasks in unsupervised machine learning. We propose a new axiom that captures proportional representation fairness (PRF). We make a case that the concept achieves the raison d'{\^{e}}tre of several existing concepts in the literature in an arguably more convincing manner. Our fairness concept is not satisfied by existing fair clustering algorithms. We design efficient algorithms to achieve PRF both for unconstrained and discrete clustering problems.
    
[^69]: 基于LSTM的物联网设备识别

    LSTM based IoT Device Identification. (arXiv:2304.13905v1 [cs.CR])

    [http://arxiv.org/abs/2304.13905](http://arxiv.org/abs/2304.13905)

    本研究提出了一种使用LSTM进行物联网设备识别的方法，以预防安全威胁和检测漏洞为目标。

    

    随着物联网的使用越来越普遍，随之而来的是大量设备造成的安全漏洞。在这样的环境下，物联网设备识别方法成为重要的预防性安全措施，可以识别这些设备并检测它们所面临的漏洞。本研究提出了一种使用长短时记忆（LSTM）的方法，在Aalto数据集中识别设备。

    While the use of the Internet of Things is becoming more and more popular, many security vulnerabilities are emerging with the large number of devices being introduced to the market. In this environment, IoT device identification methods provide a preventive security measure as an important factor in identifying these devices and detecting the vulnerabilities they suffer from. In this study, we present a method that identifies devices in the Aalto dataset using Long short-term memory (LSTM)
    
[^70]: 从像素中学习物体中心化的广义值函数

    Discovering Object-Centric Generalized Value Functions From Pixels. (arXiv:2304.13892v1 [cs.LG])

    [http://arxiv.org/abs/2304.13892](http://arxiv.org/abs/2304.13892)

    本文介绍了一种从像素中学习物体中心化的广义值函数的方法。该方法从物体中发现有意义的特征，转化为“问题”函数，并利用随后学习的广义值函数来进行控制，在静态和非静态设置下表现良好。学到的表示不仅是可解释的，而且围绕着具有不变性的物体，有助于快速适应。

    

    深度强化学习展现了从高维输入中提取有用表示的显著进展，尽管使用的是手工辅助任务和伪奖励。自动化地以物体为中心学习此类表示，以期实现控制和快速适应，仍然是一个未解决的研究问题。在本文中，我们介绍了一种方法，试图从物体中发现有意义的特征，将它们转化为时间上连贯的“问题”函数，并利用随后学习的广义值函数来进行控制。我们将我们的方法与最先进的技术进行比较，并展示了在静态和非静态设置下的竞争性表现。最后，我们还调查了被发现的广义值函数，并通过定性分析表明，学到的表示不仅是可解释的，而且围绕着物体，这些物体对任务的变化具有不变性，有助于快速适应。

    Deep Reinforcement Learning has shown significant progress in extracting useful representations from high-dimensional inputs albeit using hand-crafted auxiliary tasks and pseudo rewards. Automatically learning such representations in an object-centric manner geared towards control and fast adaptation remains an open research problem. In this paper, we introduce a method that tries to discover meaningful features from objects, translating them to temporally coherent "question" functions and leveraging the subsequent learned general value functions for control. We compare our approach with state-of-the-art techniques alongside other ablations and show competitive performance in both stationary and non-stationary settings. Finally, we also investigate the discovered general value functions and through qualitative analysis show that the learned representations are not only interpretable but also, centered around objects that are invariant to changes across tasks facilitating fast adaptatio
    
[^71]: 通过动态处理提高差分私有聚类的效用

    Improving the Utility of Differentially Private Clustering through Dynamical Processing. (arXiv:2304.13886v1 [cs.LG])

    [http://arxiv.org/abs/2304.13886](http://arxiv.org/abs/2304.13886)

    本研究提出了一种通过利用 Morse 理论提高差分私有聚类效用的方法，该方法可为复杂集群分布适配高斯子集群，即使对于现有的简单聚类方法，其效果也更好，在相同的隐私水平下不会增加隐私损失。

    

    本研究旨在缓解差分私有聚类任务中效用和隐私之间的权衡问题。现有研究主要关注简单的集群方法，对于非凸集群的聚类效果较差。通过利用 Morse 理论，我们将高斯子集群按层次连接以适应更复杂的集群分布。由于差分私有子群是通过现有方法获得的，因此所提出的方法几乎不会增加隐私损失。我们提供了理论背景，表明所提出的方法是归纳的，并且可以实现任意数量的群集。在各种数据集上的实验证明，与现有方法相比，在相同的隐私水平下，我们的框架实现了更好的聚类效果。

    This study aims to alleviate the trade-off between utility and privacy in the task of differentially private clustering. Existing works focus on simple clustering methods, which show poor clustering performance for non-convex clusters. By utilizing Morse theory, we hierarchically connect the Gaussian sub-clusters to fit complex cluster distributions. Because differentially private sub-clusters are obtained through the existing methods, the proposed method causes little or no additional privacy loss. We provide a theoretical background that implies that the proposed method is inductive and can achieve any desired number of clusters. Experiments on various datasets show that our framework achieves better clustering performance at the same privacy level, compared to the existing methods.
    
[^72]: SemEval-2023第8项任务中的MasonNLP+：使用知识增强的预训练语言模型从社交媒体中提取医疗问题、经验和声明

    MasonNLP+ at SemEval-2023 Task 8: Extracting Medical Questions, Experiences and Claims from Social Media using Knowledge-Augmented Pre-trained Language Models. (arXiv:2304.13875v1 [cs.CL])

    [http://arxiv.org/abs/2304.13875](http://arxiv.org/abs/2304.13875)

    本研究提出了一种知识增强的预训练语言模型MasonNLP+，能够从社交媒体中提取医疗问题、经验和声明，并在SemEval-2023任务8的两个子任务中取得了最先进的性能。

    

    在在线论坛中，用户分享他们的医疗状况和治疗经验，包括提出声明、提问以及讨论治疗对他们的健康状况的影响。构建可理解这些信息的系统可以有效地监测错误信息的传播并验证用户的声明。 SemEval-2023的第8项任务专注于医学应用，具体包括从社交媒体用户帖子中提取与患者经验和医疗状况相关的实体。 Reddit健康在线交流（RedHot）语料库包含来自与医疗状况相关的subreddit的帖子，带有表征患者经验和医疗状况的注释。在子任务1中，患者经验通过个人经验、问题和声明来描述。在子任务2中，医疗状况通过人群、干预和结果来描述。为自动提取患者经验和医疗状况信息，我们提出了一种知识增强的预训练语言模型MasonNLP+，将领域特定的知识融入预训练过程以提高语言理解能力。我们的方法在SemEval-2023第8项任务的两个子任务中均取得了最先进的性能。

    In online forums like Reddit, users share their experiences with medical conditions and treatments, including making claims, asking questions, and discussing the effects of treatments on their health. Building systems to understand this information can effectively monitor the spread of misinformation and verify user claims. The Task-8 of the 2023 International Workshop on Semantic Evaluation focused on medical applications, specifically extracting patient experience- and medical condition-related entities from user posts on social media. The Reddit Health Online Talk (RedHot) corpus contains posts from medical condition-related subreddits with annotations characterizing the patient experience and medical conditions. In Subtask-1, patient experience is characterized by personal experience, questions, and claims. In Subtask-2, medical conditions are characterized by population, intervention, and outcome. For the automatic extraction of patient experiences and medical condition informatio
    
[^73]: 连续和离散权重下非凸神经网络中的典型与非典型解析解

    Typical and atypical solutions in non-convex neural networks with discrete and continuous weights. (arXiv:2304.13871v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2304.13871](http://arxiv.org/abs/2304.13871)

    本文研究了二元和连续的负边距感知器作为非凸神经网络模型，发现它们都存在着极为平坦和宽广的亚优解，这对于二元情况中的算法行为有着很强的影响。

    

    本论文研究了二元和连续的负边距感知器作为简单的非凸神经网络模型在学习随机规则和关联时的情况。我们分析了两种模型的解空间几何形态，并找到了重要的相似性和差异性。这两种模型都表现出极为平坦和宽广的亚优解。这些亚优解与二元情况下无法递归算法访问的大量蔓延小集群（冻结1-RSB相）组成的主要解集共存，或者球面情况下不同大小聚类的分层结构（完全RSB相）。在两种情况下，当交叉一定密度约束的阈值时，宽广平坦极小值的局部熵变得非单调，表明鲁棒解的空间被分成了不连通的组件。这对二元模型中算法的行为产生了很强的影响，因为它无法访问剩余的孤立集群。

    We study the binary and continuous negative-margin perceptrons as simple non-convex neural network models learning random rules and associations. We analyze the geometry of the landscape of solutions in both models and find important similarities and differences. Both models exhibit subdominant minimizers which are extremely flat and wide. These minimizers coexist with a background of dominant solutions which are composed by an exponential number of algorithmically inaccessible small clusters for the binary case (the frozen 1-RSB phase) or a hierarchical structure of clusters of different sizes for the spherical case (the full RSB phase). In both cases, when a certain threshold in constraint density is crossed, the local entropy of the wide flat minima becomes non-monotonic, indicating a break-up of the space of robust solutions into disconnected components. This has a strong impact on the behavior of algorithms in binary models, which cannot access the remaining isolated clusters. For
    
[^74]: highway2vec——基于道路网络特征将OpenStreetMap微区域表示出来

    highway2vec -- representing OpenStreetMap microregions with respect to their road network characteristics. (arXiv:2304.13865v1 [cs.LG])

    [http://arxiv.org/abs/2304.13865](http://arxiv.org/abs/2304.13865)

    本文提出了一种基于OpenStreetMap城市道路网络的微区域的嵌入方法，以检测地图六边形在包含的道路网络中的相似性。

    

    近年来，神经网络在表示学习中得到广泛应用，可以自动化完成一些需要手动设计特征的任务。对于需要考虑空间变量的问题，使用预训练的地图区域表示可以避免手动创建特征表。然而，针对道路网络特征的地图区域表示方法非常少。本文提出了一种基于OpenStreetMap城市道路网络的微区域的嵌入方法，利用H3空间索引实现可重复和可扩展的表示学习并获得矢量表示，以检测地图六边形在包含的道路网络中的相似性。

    Recent years brought advancements in using neural networks for representation learning of various language or visual phenomena. New methods freed data scientists from hand-crafting features for common tasks. Similarly, problems that require considering the spatial variable can benefit from pretrained map region representations instead of manually creating feature tables that one needs to prepare to solve a task. However, very few methods for map area representation exist, especially with respect to road network characteristics. In this paper, we propose a method for generating microregions' embeddings with respect to their road infrastructure characteristics. We base our representations on OpenStreetMap road networks in a selection of cities and use the H3 spatial index to allow reproducible and scalable representation learning. We obtained vector representations that detect how similar map hexagons are in the road networks they contain. Additionally, we observe that embeddings yield a
    
[^75]: 利用准确的模拟器和有前途的候选方案增强反问题解决方案

    Enhancing Inverse Problem Solutions with Accurate Surrogate Simulators and Promising Candidates. (arXiv:2304.13860v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2304.13860](http://arxiv.org/abs/2304.13860)

    神经伴随方法在AEM设计中表现出了良好的性能，但由于准确性和计算资源限制，优化变得更加具有挑战性。 NeuLag方法能够高效地解决这些问题，并在约束条件下展现出良好性能。

    

    深度学习的反问题技术近年来受到了重视，其中神经伴随（NA）方法采用了神经网络模拟器，在人工电磁材料（AEM）设计任务中表现出了惊人的性能。然而，模拟器准确性对NA方法解决方案的影响仍然不确定。此外，当模拟器庞大且计算资源有限时，在该方法中实现足够的优化变得具有挑战性。此外，尽管从工程角度来看其重要性，但在约束条件下的行为尚未研究。在这项研究中，我们研究了模拟器准确性对解决方案的影响，并发现准确性越高，解决方案越好。然后，我们开发了NA方法的扩展，名为神经拉格朗日（NeuLag）方法，能够高效地进行优化。

    Deep-learning inverse techniques have attracted significant attention in recent years. Among them, the neural adjoint (NA) method, which employs a neural network surrogate simulator, has demonstrated impressive performance in the design tasks of artificial electromagnetic materials (AEM). However, the impact of the surrogate simulators' accuracy on the solutions in the NA method remains uncertain. Furthermore, achieving sufficient optimization becomes challenging in this method when the surrogate simulator is large, and computational resources are limited. Additionally, the behavior under constraints has not been studied, despite its importance from the engineering perspective. In this study, we investigated the impact of surrogate simulators' accuracy on the solutions and discovered that the more accurate the surrogate simulator is, the better the solutions become. We then developed an extension of the NA method, named Neural Lagrangian (NeuLag) method, capable of efficiently optimizi
    
[^76]: 多模态复合关联评分：衡量生成式多模态模型中的性别偏差

    Multimodal Composite Association Score: Measuring Gender Bias in Generative Multimodal Models. (arXiv:2304.13855v1 [cs.CV])

    [http://arxiv.org/abs/2304.13855](http://arxiv.org/abs/2304.13855)

    这篇论文提出了一种新方法——多模态复合关联评分(MCAS)，用于衡量多模态生成模型中的性别偏差。与单模态小型单阶段模型的度量与量化不同，该方法能够高效、有效地识别和测量大型复杂的多模态生成模型中的性别偏差。

    

    基于扩散模型的生成式多模态模型近年来取得了巨大的发展和进步。像DALL-E和Stable Diffusion这样的模型越来越受欢迎，成功地从文本中创建图像，经常结合抽象的想法。然而，像其他深度学习模型一样，它们也反映了从互联网中爬取的训练数据中继承的社会偏见。手动审核模型中的偏见可能非常耗时和资源消耗，并且由这些模型可以接受的输入的无限制和不受约束的性质使问题更加复杂。 对于偏见的度量与量化的研究通常侧重于单模态的小型单阶段模型。因此，多阶段多模态模型的出现需要不同的方法。 在本文中，我们提出了多模态复合关联评分（MCAS）作为衡量多模态生成式模型中性别偏差的新方法。通过使用MCAS评估DALL-E 2和稳定扩散，我们展示了检测并度量生成图像中性别偏差的能力。我们的结果表明，DALL-E 2生成的图像具有最小的性别偏见，而稳定扩散生成的图像存在显著的性别偏见。所提出的MCAS指标允许对大型复杂的多模态生成模型中的性别偏见进行高效且有效的识别和测量。

    Generative multimodal models based on diffusion models have seen tremendous growth and advances in recent years. Models such as DALL-E and Stable Diffusion have become increasingly popular and successful at creating images from texts, often combining abstract ideas. However, like other deep learning models, they also reflect social biases they inherit from their training data, which is often crawled from the internet. Manually auditing models for biases can be very time and resource consuming and is further complicated by the unbounded and unconstrained nature of inputs these models can take. Research into bias measurement and quantification has generally focused on small single-stage models working on a single modality. Thus the emergence of multistage multimodal models requires a different approach. In this paper, we propose Multimodal Composite Association Score (MCAS) as a new method of measuring gender bias in multimodal generative models. Evaluating both DALL-E 2 and Stable Diffu
    
[^77]: 在在线市场上对商品进行分类：一种集成方法

    Categorising Products in an Online Marketplace: An Ensemble Approach. (arXiv:2304.13852v1 [cs.LG])

    [http://arxiv.org/abs/2304.13852](http://arxiv.org/abs/2304.13852)

    本研究提出了一种使用集成方法将不同模型组合来自动分类商品的方法，并证明了该方法可以取得不错的分类效果，平均F1得分为0.82。

    

    近年来，商品分类一直是电子商务公司面临的共同问题，他们利用机器学习来实现自动分类。在本研究中，我们提出了一种集成方法，使用不同模型的组合来单独预测每个产品的类别、子类别和颜色，最后将每个产品的结果预测汇总。通过上述方法，我们证明了使用XGBoost和k近邻结合来预测这些特征可以实现平均F1得分为0.82。

    In recent years, product categorisation has been a common issue for E-commerce companies who have utilised machine learning to categorise their products automatically. In this study, we propose an ensemble approach, using a combination of different models to separately predict each product's category, subcategory, and colour before ultimately combining the resultant predictions for each product. With the aforementioned approach, we show that an average F1-score of 0.82 can be achieved using a combination of XGBoost and k-nearest neighbours to predict said features.
    
[^78]: SSL模型是否感到“似曾相识”？自监督学习中的意外记忆

    Do SSL Models Have D\'ej\`a Vu? A Case of Unintended Memorization in Self-supervised Learning. (arXiv:2304.13850v1 [cs.CV])

    [http://arxiv.org/abs/2304.13850](http://arxiv.org/abs/2304.13850)

    自监督学习（SSL）算法会意外地记忆单个训练样本中的特定部分，称为“似曾相识”记忆，该现象是普遍存在的，不能被传统的评估方法检测。

    

    自监督学习（SSL）算法可以通过学习将自然图像的不同部分相互关联来产生有用的图像表示。然而，当被推向极端时，SSL模型会意外地记忆单个训练样本中的特定部分，而不是学习有意义的语义关联。在本研究中，我们对SSL模型中的意外记忆现象进行了系统研究，我们称之为“似曾相识”记忆。具体而言，我们展示了在给定训练模型和一个仅包含背景（如水、天空、草地）的训练图像裁剪后，可以高精度或甚至视觉重构地推断出前景对象。此外，我们展示了“似曾相识”记忆是不同SSL算法的普遍现象，并且会因某些设计选择而恶化，而且不能通过传统的评估表示质量的技术来检测。“似曾相识”记忆的研究

    Self-supervised learning (SSL) algorithms can produce useful image representations by learning to associate different parts of natural images with one another. However, when taken to the extreme, SSL models can unintendedly memorize specific parts in individual training samples rather than learning semantically meaningful associations. In this work, we perform a systematic study of the unintended memorization of image-specific information in SSL models -- which we refer to as d\'ej\`a vu memorization. Concretely, we show that given the trained model and a crop of a training image containing only the background (e.g., water, sky, grass), it is possible to infer the foreground object with high accuracy or even visually reconstruct it. Furthermore, we show that d\'ej\`a vu memorization is common to different SSL algorithms, is exacerbated by certain design choices, and cannot be detected by conventional techniques for evaluating representation quality. Our study of d\'ej\`a vu memorizatio
    
[^79]: 面向设计和验证自动化的Verilog自动完成的深度学习框架

    A Deep Learning Framework for Verilog Autocompletion Towards Design and Verification Automation. (arXiv:2304.13840v1 [cs.LG])

    [http://arxiv.org/abs/2304.13840](http://arxiv.org/abs/2304.13840)

    本文提出了一个深度学习框架，用于训练Verilog自动完成模型，能够有效预测下一个token，为设计和验证自动化提供了解决方案。

    

    创新的电子设计自动化（EDA）解决方案对于满足越来越复杂的电子设备的设计要求至关重要。 Verilog是一种广泛用于数字电路设计和验证的硬件描述语言，并使用特定的EDA工具进行合成。然而，编写代码是一项重复且耗时的任务。本文首先提出了一种新颖的深度学习框架，用于训练Verilog自动完成模型，其次提供了从开源存储库获取的Verilog数据集。该框架涉及将预先训练的模型集成到通用编程语言数据上，然后在相似于目标下游任务的数据集上进行微调。通过比较在多个评估指标上训练不同子集的预训练模型与提议的Verilog数据集，验证了该方法的有效性。实验表明，该框架相比基线模型实现了更好的BLEU，ROUGE-L和chrF得分，并且有效预测了部分编写的Verilog语句的下一个token。

    Innovative Electronic Design Automation (EDA) solutions are important to meet the design requirements for increasingly complex electronic devices. Verilog, a hardware description language, is widely used for the design and verification of digital circuits and is synthesized using specific EDA tools. However, writing code is a repetitive and time-intensive task. This paper proposes, primarily, a novel deep learning framework for training a Verilog autocompletion model and, secondarily, a Verilog dataset of files and snippets obtained from open-source repositories. The framework involves integrating models pretrained on general programming language data and finetuning them on a dataset curated to be similar to a target downstream task. This is validated by comparing different pretrained models trained on different subsets of the proposed Verilog dataset using multiple evaluation metrics. These experiments demonstrate that the proposed framework achieves better BLEU, ROUGE-L, and chrF sco
    
[^80]: 论RemOve-And-Retrain的陷阱：数据处理不等式的视角

    On Pitfalls of $\textit{RemOve-And-Retrain}$: Data Processing Inequality Perspective. (arXiv:2304.13836v1 [cs.LG])

    [http://arxiv.org/abs/2304.13836](http://arxiv.org/abs/2304.13836)

    本论文评估了RemOve-And-Retrain（ROAR）协议的可靠性。研究结果表明，ROAR基准测试中的属性可能有更少的有关决策的重要信息，这种偏差称为毛糙度偏差，并提醒人们不要在ROAR指标上进行盲目的依赖。

    

    本文评估了RemOve-And-Retrain（ROAR）协议的可靠性，该协议用于测量特征重要性估计的性能。我们从理论背景和实证实验中发现，具有较少有关决策功能的信息的属性在ROAR基准测试中表现更好，与ROAR的原始目的相矛盾。这种现象也出现在最近提出的变体RemOve-And-Debias（ROAD）中，我们提出了ROAR归因度量中毛糙度偏差的一致趋势。我们的结果提醒人们不要盲目依赖ROAR的性能评估指标。

    This paper assesses the reliability of the RemOve-And-Retrain (ROAR) protocol, which is used to measure the performance of feature importance estimates. Our findings from the theoretical background and empirical experiments indicate that attributions that possess less information about the decision function can perform better in ROAR benchmarks, conflicting with the original purpose of ROAR. This phenomenon is also observed in the recently proposed variant RemOve-And-Debias (ROAD), and we propose a consistent trend of blurriness bias in ROAR attribution metrics. Our results caution against uncritical reliance on ROAR metrics.
    
[^81]: 多方聊天：人类和模型中的群聊对话代理

    Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models. (arXiv:2304.13835v1 [cs.CL])

    [http://arxiv.org/abs/2304.13835](http://arxiv.org/abs/2304.13835)

    本文通过收集和评估多方对话情况，探讨了模型在群体对话中需要具备的技能，发现新数据集MultiLIGHT可以在这个领域带来显着的进展。

    

    当前的对话研究主要研究成对（双方）对话，并没有涉及到多于两个人在一起对话的日常情景。本文使用LIGHT环境构建接地对话来收集和评估多方对话情况。我们对比在新数据集MultiLIGHT上训练的模型和现有的成对训练的对话模型以及带有少量提示的大型语言模型。我们发现，我们将公开发布MultiLIGHT数据集，这将有助于在群体设置中带来显着的改进。

    Current dialogue research primarily studies pairwise (two-party) conversations, and does not address the everyday setting where more than two speakers converse together. In this work, we both collect and evaluate multi-party conversations to study this more general case. We use the LIGHT environment to construct grounded conversations, where each participant has an assigned character to role-play. We thus evaluate the ability of language models to act as one or more characters in such conversations. Models require two skills that pairwise-trained models appear to lack: (1) being able to decide when to talk; (2) producing coherent utterances grounded on multiple characters. We compare models trained on our new dataset to existing pairwise-trained dialogue models, as well as large language models with few-shot prompting. We find that our new dataset, MultiLIGHT, which we will publicly release, can help bring significant improvements in the group setting.
    
[^82]: 基于核棍棒过程的高斯过程专家混合模型

    Mixtures of Gaussian process experts based on kernel stick-breaking processes. (arXiv:2304.13833v1 [stat.ML])

    [http://arxiv.org/abs/2304.13833](http://arxiv.org/abs/2304.13833)

    提出了一种新的基于核棍棒过程的高斯过程专家混合模型，能够维持直观吸引力并提高模型性能，具有实用性。

    

    高斯过程专家混合模型是一类能同时解决标准高斯过程中存在的两个关键限制：可扩展性和预测性能的模型。使用狄利克雷过程作为门函数的模型能够直观地解释和自动选择混合物中专家的数量。虽然现有模型在感知非平稳性、多模性和异方差性方面表现良好，但其门函数的简单性可能会限制在应用于复杂数据生成过程时的预测性能。我们利用最近在相关狄利克雷过程文献中的进展，提出了一种基于核棍棒过程的新型高斯过程专家混合模型。我们的模型保持直观吸引力，同时提高现有模型的性能。为了使其实用性，我们设计了一个后验计算的切片抽样采样器。

    Mixtures of Gaussian process experts is a class of models that can simultaneously address two of the key limitations inherent in standard Gaussian processes: scalability and predictive performance. In particular, models that use Dirichlet processes as gating functions permit straightforward interpretation and automatic selection of the number of experts in a mixture. While the existing models are intuitive and capable of capturing non-stationarity, multi-modality and heteroskedasticity, the simplicity of their gating functions may limit the predictive performance when applied to complex data-generating processes. Capitalising on the recent advancement in the dependent Dirichlet processes literature, we propose a new mixture model of Gaussian process experts based on kernel stick-breaking processes. Our model maintains the intuitive appeal yet improve the performance of the existing models. To make it practical, we design a sampler for posterior computation based on the slice sampling. 
    
[^83]: 核化赌博机算法中对核函数规律性错误的自适应性研究

    Adaptation to Misspecified Kernel Regularity in Kernelised Bandits. (arXiv:2304.13830v1 [stat.ML])

    [http://arxiv.org/abs/2304.13830](http://arxiv.org/abs/2304.13830)

    本文研究了核化赌博问题中对核函数规则性错误的自适应性问题。我们证明了在具有不同规则性的一对RKHS中同时实现最佳累计遗憾是不可能的，并通过现有算法结合极小化非自适应的核赌博机算法，验证了这一下限的紧密性。

    

    在连续武装赌博问题中，如果底层函数位于再生核希尔伯特空间（RKHS）中，即核赌博问题，一个重要的未解决问题是，如果相关的核函数的规则性是未知的，学习算法可以多么好地适应。在这项工作中，我们研究了平移不变核规则性的自适应性，在赌博设置中，该规则性由核的傅里叶变换的衰减率所描述。我们推导了一个自适应性的下限，证明了在具有不同规则性的一对RKHS中同时实现最佳累计遗憾是不可能的。为了验证这个下限的紧密性，我们展示了一个现有的赌博模型选择算法与极小化非自适应的核赌博机算法相结合，在总步数T的依赖下匹配了下限，除了对数因子。通过填写RKHS之间适应性的遗憾界，我们连接了它们。

    In continuum-armed bandit problems where the underlying function resides in a reproducing kernel Hilbert space (RKHS), namely, the kernelised bandit problems, an important open problem remains of how well learning algorithms can adapt if the regularity of the associated kernel function is unknown. In this work, we study adaptivity to the regularity of translation-invariant kernels, which is characterized by the decay rate of the Fourier transformation of the kernel, in the bandit setting. We derive an adaptivity lower bound, proving that it is impossible to simultaneously achieve optimal cumulative regret in a pair of RKHSs with different regularities. To verify the tightness of this lower bound, we show that an existing bandit model selection algorithm applied with minimax non-adaptive kernelised bandit algorithms matches the lower bound in dependence of $T$, the total number of steps, except for log factors. By filling in the regret bounds for adaptivity between RKHSs, we connect the
    
[^84]: 神经网络模型压缩的量化误差计算保证

    Guaranteed Quantization Error Computation for Neural Network Model Compression. (arXiv:2304.13812v1 [cs.LG])

    [http://arxiv.org/abs/2304.13812](http://arxiv.org/abs/2304.13812)

    本文提出了一种通过建立合并的神经网络，应用优化方法和可达性分析方法来计算保证的量化误差的方法，解决了神经网络压缩的保证输出误差计算问题。

    

    神经网络模型压缩技术可以解决工业系统中嵌入式设备上的深度神经网络计算问题。本文解决了带有量化的神经网络压缩的保证输出误差计算问题。通过建立一个合并的神经网络，将前向神经网络和其量化版本合并，以产生两个神经网络之间的精确输出差异。然后，应用基于优化的方法和可达性分析方法到合并的神经网络中，计算保证的量化误差。最后，提出了一个数值例子来验证所提出方法的适用性和有效性。

    Neural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systems. The guaranteed output error computation problem for neural network compression with quantization is addressed in this paper. A merged neural network is built from a feedforward neural network and its quantized version to produce the exact output difference between two neural networks. Then, optimization-based methods and reachability analysis methods are applied to the merged neural network to compute the guaranteed quantization error. Finally, a numerical example is proposed to validate the applicability and effectiveness of the proposed approach.
    
[^85]: 基于数据驱动的混合自动机框架用于建模复杂的动态系统

    A Data-Driven Hybrid Automaton Framework to Modeling Complex Dynamical Systems. (arXiv:2304.13811v1 [eess.SY])

    [http://arxiv.org/abs/2304.13811](http://arxiv.org/abs/2304.13811)

    本文提出了一种基于神经网络的计算高效混合自动机模型，可用于捕捉未知的复杂系统行为，并提供低计算成本的集合值可达性分析，以显著降低可达集合计算的计算成本而不牺牲建模精度。

    

    本文提出了一种计算高效的数据驱动混合自动机模型，使用多个神经网络捕捉未知的复杂动态系统行为。系统的采样数据被有效划分为与其拓扑对应的组，并基于此定义转换门。然后，一组计算高效的小规模神经网络被训练为其相应拓扑的局部动态描述。在使用基于神经网络的混合自动机对系统进行建模之后，基于区间分析和分裂合并过程提供了低计算成本的集合值可达性分析。最后，通过一个极限环的数值示例说明了开发的模型能够显著降低可达集合计算的计算成本，而不会牺牲任何建模精度。

    In this paper, a computationally efficient data-driven hybrid automaton model is proposed to capture unknown complex dynamical system behaviors using multiple neural networks. The sampled data of the system is divided by valid partitions into groups corresponding to their topologies and based on which, transition guards are defined. Then, a collection of small-scale neural networks that are computationally efficient are trained as the local dynamical description for their corresponding topologies. After modeling the system with a neural-network-based hybrid automaton, the set-valued reachability analysis with low computation cost is provided based on interval analysis and a split and combined process. At last, a numerical example of the limit cycle is presented to illustrate that the developed models can significantly reduce the computational cost in reachable set computation without sacrificing any modeling precision.
    
[^86]: 潜指纹识别：局部和全局嵌入的融合

    Latent Fingerprint Recognition: Fusion of Local and Global Embeddings. (arXiv:2304.13800v1 [cs.CV])

    [http://arxiv.org/abs/2304.13800](http://arxiv.org/abs/2304.13800)

    本文采用全局嵌入与局部嵌入相结合的方法，在提高识别准确率的同时，保证了较高的吞吐量和应用性。

    

    在指纹识别中，一个最具挑战性的问题是确定与犯罪现场留下的部分和模糊的指纹（即潜指纹或指纹痕迹）相关联的嫌疑人身份。本文将全局嵌入与局部嵌入相结合，为潜在卷积和拍打指纹匹配提供了最先进的准确率和高吞吐量。这种局部和全局表示的组合使识别准确率在NIST SD 27、NIST SD 302、MSP、MOLF DB1/DB4和MOLF DB2/DB4潜指纹数据集上都得到了显着提高。

    One of the most challenging problems in fingerprint recognition continues to be establishing the identity of a suspect associated with partial and smudgy fingerprints left at a crime scene (i.e., latent prints or fingermarks). Despite the success of fixed-length embeddings for rolled and slap fingerprint recognition, the features learned for latent fingerprint matching have mostly been limited to local minutiae-based embeddings and have not directly leveraged global representations for matching. In this paper, we combine global embeddings with local embeddings for state-of-the-art latent to rolled matching accuracy with high throughput. The combination of both local and global representations leads to improved recognition accuracy across NIST SD 27, NIST SD 302, MSP, MOLF DB1/DB4, and MOLF DB2/DB4 latent fingerprint datasets for both closed-set (84.11%, 54.36%, 84.35%, 70.43%, 62.86% rank-1 retrieval rate, respectively) and open-set (0.50, 0.74, 0.44, 0.60, 0.68 FNIR at FPIR=0.02, resp
    
[^87]: 物理信息神经网络用于预测柴油机气体流动动力学和未知参数

    Physics-informed neural networks for predicting gas flow dynamics and unknown parameters in diesel engines. (arXiv:2304.13799v1 [cs.LG])

    [http://arxiv.org/abs/2304.13799](http://arxiv.org/abs/2304.13799)

    本论文提出了一种物理信息神经网络（PINN）方法，能够同时准确预测柴油机未知参数和动态，以及识别“平均值”模型中的未知参数，为实际案例研究提供了可能性。

    

    本文介绍了一种应用于柴油机健康监测的物理信息神经网络(PINN)方法。该方法旨在评估发动机动力学，识别“平均值”模型中的未知参数，并预测维护需求。PINN模型应用于具有可变几何涡轮增压器和废气再循环的柴油机，使用选定状态变量的测量数据。实验结果表明，在干净和嘈杂的数据下，PINN模型能够同时准确地预测未知参数和动态，自适应权重在损失函数中的重要性，可加速收敛。这些模拟的输入数据来自实际发动机运行条件，而输出数据是模拟数据，使这成为PINN预测真实动态系统能力的实际案例研究。柴油机的平均值模型包括经验公式来表示某些状态，但本文提出的PINN方法可以更准确、更高效地预测动态并识别未知参数。

    This paper presents a physics-informed neural network (PINN) approach for monitoring the health of diesel engines. The aim is to evaluate the engine dynamics, identify unknown parameters in a "mean value" model, and anticipate maintenance requirements. The PINN model is applied to diesel engines with a variable-geometry turbocharger and exhaust gas recirculation, using measurement data of selected state variables. The results demonstrate the ability of the PINN model to predict simultaneously both unknown parameters and dynamics accurately with both clean and noisy data, and the importance of the self-adaptive weight in the loss function for faster convergence. The input data for these simulations are derived from actual engine running conditions, while the outputs are simulated data, making this a practical case study of PINN's ability to predict real-world dynamical systems. The mean value model of the diesel engine incorporates empirical formulae to represent certain states, but the
    
[^88]: 广义广义线性模型：凸估计和在线界限

    Generalized generalized linear models: Convex estimation and online bounds. (arXiv:2304.13793v1 [stat.ME])

    [http://arxiv.org/abs/2304.13793](http://arxiv.org/abs/2304.13793)

    该论文提出了一种用于估计时间空间数据中依赖关系的广义广义线性模型（GGLM）参数的计算框架，使用单调运算符的变分不等式方法克服了参数估计中的非凸性并为参数恢复提供保证

    

    我们引入了一个新的计算框架，用于估计广义广义线性模型（GGLM）中的参数。这是一类将流行的广义线性模型（GLM）扩展到考虑时空数据中观测之间依赖关系的模型。所提出的方法使用基于单调运算符的变分不等式方法来克服参数估计中的非凸性并为参数恢复提供保证。结果可以应用于GLM和GGLM，重点关注时空模型。我们还使用鞅集中不等式提供了在线实例界限。最后，我们使用数值模拟和野火事件的真实数据示例来展示算法的性能。

    We introduce a new computational framework for estimating parameters in generalized generalized linear models (GGLM), a class of models that extends the popular generalized linear models (GLM) to account for dependencies among observations in spatio-temporal data. The proposed approach uses a monotone operator-based variational inequality method to overcome non-convexity in parameter estimation and provide guarantees for parameter recovery. The results can be applied to GLM and GGLM, focusing on spatio-temporal models. We also present online instance-based bounds using martingale concentrations inequalities. Finally, we demonstrate the performance of the algorithm using numerical simulations and a real data example for wildfire incidents.
    
[^89]: 基于替代模型的人机交互场景生成

    Surrogate Assisted Generation of Human-Robot Interaction Scenarios. (arXiv:2304.13787v1 [cs.RO])

    [http://arxiv.org/abs/2304.13787](http://arxiv.org/abs/2304.13787)

    本文提出了基于替代模型的人机交互场景生成方法，可以高效合成多样化的挑战性数据集，以便评估和理解人机交互系统的优劣，可以在实际交互中重现这些场景。

    

    随着人机交互系统的发展，不同环境和用户下评估和理解这些系统的优缺点变得越来越困难。为此，以往的方法通过算法生成了多样的场景，揭示了共享控制遥操作任务的系统失效情况。然而，这些方法需要通过模拟机器人策略和人类行为来直接评估生成的场景。这些评估所需的计算成本限制了它们在更复杂的领域的适用性。因此，我们提出了通过替代模型来预测人类和机器人行为来增强场景生成系统的建议。在共享控制遥操作域和更复杂的共享工作空间协作任务中，我们展示了替代模型辅助的场景生成可以高效地合成具有挑战性的多样数据集。我们展示了这些故障在真实世界中的交互中是可重现的。

    As human-robot interaction (HRI) systems advance, so does the difficulty of evaluating and understanding the strengths and limitations of these systems in different environments and with different users. To this end, previous methods have algorithmically generated diverse scenarios that reveal system failures in a shared control teleoperation task. However, these methods require directly evaluating generated scenarios by simulating robot policies and human actions. The computational cost of these evaluations limits their applicability in more complex domains. Thus, we propose augmenting scenario generation systems with surrogate models that predict both human and robot behaviors. In the shared control teleoperation domain and a more complex shared workspace collaboration task, we show that surrogate assisted scenario generation efficiently synthesizes diverse datasets of challenging scenarios. We demonstrate that these failures are reproducible in real-world interactions.
    
[^90]: 离线交互数据的距离加权监督学习

    Distance Weighted Supervised Learning for Offline Interaction Data. (arXiv:2304.13774v1 [cs.LG])

    [http://arxiv.org/abs/2304.13774](http://arxiv.org/abs/2304.13774)

    这篇论文提出一种距离加权监督学习方法，可以利用离线交互数据中的最短路径距离来提取策略，较之以往的监督方法和离线强化学习方法表现更好。

    

    序列决策算法通常很难利用不同来源的非结构化离线交互数据。基于监督学习的模仿学习方法是稳健的，但需要收集最佳演示，而这很难。离线目标条件强化学习算法承诺从次优数据中学习，但面对高维数据时面临优化挑战。为弥合模仿学习和强化学习之间的差距，我们引入了距离加权监督学习或DWSL，一种从离线数据中学习目标条件策略的监督方法。DWSL只用监督学习模拟离线数据状态之间的所有时间步的分布，并利用这个分布来近似最短路径距离。为了提取策略，我们通过它们在距离估计中的减少程度来加权行动。理论上，DWSL收敛于受数据分布约束的最优策略，这对于离线学习的代理人是一个有吸引力的属性。实践上，DWSL显著优于以前的监督方法，并在两个标准基准上与离线RL方法相比取得了竞争性的结果。

    Sequential decision making algorithms often struggle to leverage different sources of unstructured offline interaction data. Imitation learning (IL) methods based on supervised learning are robust, but require optimal demonstrations, which are hard to collect. Offline goal-conditioned reinforcement learning (RL) algorithms promise to learn from sub-optimal data, but face optimization challenges especially with high-dimensional data. To bridge the gap between IL and RL, we introduce Distance Weighted Supervised Learning or DWSL, a supervised method for learning goal-conditioned policies from offline data. DWSL models the entire distribution of time-steps between states in offline data with only supervised learning, and uses this distribution to approximate shortest path distances. To extract a policy, we weight actions by their reduction in distance estimates. Theoretically, DWSL converges to an optimal policy constrained to the data distribution, an attractive property for offline lear
    
[^91]: 揭示巨噬细胞吞噬作用：用于神经退行性疾病分析的可扩展和可解释的深度学习框架

    Phagocytosis Unveiled: A Scalable and Interpretable Deep learning Framework for Neurodegenerative Disease Analysis. (arXiv:2304.13764v1 [eess.IV])

    [http://arxiv.org/abs/2304.13764](http://arxiv.org/abs/2304.13764)

    本文提出了一种可扩展且可解释的深度学习框架，用于量化和分析吞噬活性以评估神经退行性疾病。流程可以处理大型数据集，包括数据质量验证和可解释的细胞分割模块。

    

    量化动态无染色细胞的吞噬作用对于评估神经退行性疾病至关重要。然而，处理时间序列相衬显微镜视频时，测量快速细胞相互作用和区分细胞与背景使得这项任务具有挑战性。在本研究中，我们引入了一种完全自动化、可扩展和多功能的实时框架，用于量化和分析吞噬活性。我们提出的流程可以处理大型数据集，包括数据质量验证模块以抵消可能的显微镜运动和帧模糊等扰动。我们还提出了一个可解释的细胞分割模块，以改善与黑匣子算法相比的深度学习方法的可解释性。这包括两个可解释的深度学习能力：视觉说明和模型简化。我们证明了深度学习中的可解释性不是高性能的对立面，而是提供必要的深度学习能力。

    Quantifying the phagocytosis of dynamic, unstained cells is essential for evaluating neurodegenerative diseases. However, measuring rapid cell interactions and distinguishing cells from backgrounds make this task challenging when processing time-lapse phase-contrast video microscopy. In this study, we introduce a fully automated, scalable, and versatile realtime framework for quantifying and analyzing phagocytic activity. Our proposed pipeline can process large data-sets and includes a data quality verification module to counteract potential perturbations such as microscope movements and frame blurring. We also propose an explainable cell segmentation module to improve the interpretability of deep learning methods compared to black-box algorithms. This includes two interpretable deep learning capabilities: visual explanation and model simplification. We demonstrate that interpretability in deep learning is not the opposite of high performance, but rather provides essential deep learnin
    
[^92]: 通过独热编码和正则化提高梯度提升决策树的鲁棒性

    Enhancing Robustness of Gradient-Boosted Decision Trees through One-Hot Encoding and Regularization. (arXiv:2304.13761v1 [stat.ML])

    [http://arxiv.org/abs/2304.13761](http://arxiv.org/abs/2304.13761)

    通过独热编码和正则化提高梯度提升决策树的鲁棒性，研究表明对带有$L_1$或$L_2$正则化的线性回归形式进行拟合可提高GBDT模型的鲁棒性。

    

    梯度提升决策树(GBDT)是一种广泛应用的高效机器学习方法，用于表格数据建模。然而，它们复杂的结构可能导致模型对未见数据中的小协变量扰动的鲁棒性较低。本研究应用独热编码将GBDT模型转换为线性框架，通过将每个树叶编码为一个虚拟变量。这允许使用线性回归技术，以及一种新颖的风险分解方法来评估GBDT模型对协变量扰动的鲁棒性。我们建议通过重新拟合其带有$L_1$或$L_2$正则化的线性回归形式，提高GBDT模型的鲁棒性。理论结果表明了正则化对模型性能和鲁棒性的影响。在数值实验中，证明了所提出的正则化方法可以提高独热编码GBDT模型的鲁棒性。

    Gradient-boosted decision trees (GBDT) are widely used and highly effective machine learning approach for tabular data modeling. However, their complex structure may lead to low robustness against small covariate perturbation in unseen data. In this study, we apply one-hot encoding to convert a GBDT model into a linear framework, through encoding of each tree leaf to one dummy variable. This allows for the use of linear regression techniques, plus a novel risk decomposition for assessing the robustness of a GBDT model against covariate perturbations. We propose to enhance the robustness of GBDT models by refitting their linear regression forms with $L_1$ or $L_2$ regularization. Theoretical results are obtained about the effect of regularization on the model performance and robustness. It is demonstrated through numerical experiments that the proposed regularization approach can enhance the robustness of the one-hot-encoded GBDT models.
    
[^93]: TR0N：0-Shot即插即用条件生成的翻译网络

    TR0N: Translator Networks for 0-Shot Plug-and-Play Conditional Generation. (arXiv:2304.13742v1 [cs.LG])

    [http://arxiv.org/abs/2304.13742](http://arxiv.org/abs/2304.13742)

    本文提出了TR0N，将预训练的无条件生成模型转化为高度任意的条件模型。TR0N不需要训练数据或微调，可以在MS-COCO上实现零-shot FID 10.9，并在采样速度上优于竞品，同时保持了多样性和质量。

    

    本文提出了TR0N，一个高度通用的框架，将预训练的无条件生成模型，如GAN和VAE，转换为条件模型。条件可以是高度任意的，并且仅需要预训练的辅助模型。例如，我们展示了如何使用分类器将无条件模型转化为类别条件模型，并利用CLIP将其转化为文本到图像模型。TR0N学习了一种轻量级的随机映射，该映射在条件空间和生成模型的潜在空间之间“翻译”，使得生成的潜在空间对应于满足所需条件的数据样本。然后，通过Langevin动态进一步改进翻译后的潜在样本，使我们能够获得更高质量的数据样本。TR0N不需要训练数据或微调，但可以在MS-COCO上实现零-shot FID 10.9，不仅在这个指标上优于竞品，而且在采样速度上也与其保持了多样性和质量。

    We propose TR0N, a highly general framework to turn pre-trained unconditional generative models, such as GANs and VAEs, into conditional models. The conditioning can be highly arbitrary, and requires only a pre-trained auxiliary model. For example, we show how to turn unconditional models into class-conditional ones with the help of a classifier, and also into text-to-image models by leveraging CLIP. TR0N learns a lightweight stochastic mapping which "translates" between the space of conditions and the latent space of the generative model, in such a way that the generated latent corresponds to a data sample satisfying the desired condition. The translated latent samples are then further improved upon through Langevin dynamics, enabling us to obtain higher-quality data samples. TR0N requires no training data nor fine-tuning, yet can achieve a zero-shot FID of 10.9 on MS-COCO, outperforming competing alternatives not only on this metric, but also in sampling speed -- all while retaining 
    
[^94]: 可扩展的分布式AI框架：利用云计算提高深度学习性能和效率

    Scalable, Distributed AI Frameworks: Leveraging Cloud Computing for Enhanced Deep Learning Performance and Efficiency. (arXiv:2304.13738v1 [cs.LG])

    [http://arxiv.org/abs/2304.13738](http://arxiv.org/abs/2304.13738)

    本文介绍了可扩展的分布式AI框架，利用云计算提高深度学习性能和效率。通过概述常用的AI框架和云服务、探讨基于云的AI系统的关键方面和讨论云中AI工作负载的优化策略，提供了解决AI应用程序不断增长的计算需求的有效方法。

    

    近年来，将人工智能（AI）和云计算相结合已成为解决AI应用程序不断增长的计算需求的有效方法。本文介绍了一项关于利用云计算提高深度学习性能和效率的可扩展的分布式AI框架的全面研究。首先，我们概述了常用的AI框架和云服务，强调了它们各自的优缺点。接着，我们深入探讨了基于云的AI系统中数据存储和管理的关键方面，包括数据预处理、特征工程、隐私和安全。接下来，我们探讨了AI模型的并行和分布式训练技术，重点讨论了模型的分区、通信策略和基于云的训练架构。在随后的章节中，我们讨论了云中AI工作负载的优化策略，涵盖了负载平衡、资源分配、自动缩放和性能基准测试等方面。

    In recent years, the integration of artificial intelligence (AI) and cloud computing has emerged as a promising avenue for addressing the growing computational demands of AI applications. This paper presents a comprehensive study of scalable, distributed AI frameworks leveraging cloud computing for enhanced deep learning performance and efficiency. We first provide an overview of popular AI frameworks and cloud services, highlighting their respective strengths and weaknesses. Next, we delve into the critical aspects of data storage and management in cloud-based AI systems, discussing data preprocessing, feature engineering, privacy, and security. We then explore parallel and distributed training techniques for AI models, focusing on model partitioning, communication strategies, and cloud-based training architectures.  In subsequent chapters, we discuss optimization strategies for AI workloads in the cloud, covering load balancing, resource allocation, auto-scaling, and performance benc
    
[^95]: AIRIVA：自适应免疫库存的深度生成模型

    AIRIVA: A Deep Generative Model of Adaptive Immune Repertoires. (arXiv:2304.13737v1 [q-bio.QM])

    [http://arxiv.org/abs/2304.13737](http://arxiv.org/abs/2304.13737)

    AIRIVA是一种生成模型，学习TCR库存的低维、可解释和组成性表示，以解开系统效应，能够识别与不同传染病和癌症相关的TCR。

    

    免疫基因组学的最新进展表明，T细胞受体（TCR）标记可以利用TCR与疾病抗原结合的高特异性准确预测活动或最近的感染。然而，适应性免疫库存的极端多样性在可靠地识别疾病特异性TCR方面存在着挑战。种群遗传学和测序深度还可以对库存产生强烈的系统影响，在开发诊断模型时需要进行仔细考虑。本文提出了一种自适应免疫库显不变变分自编码器（AIRIVA），它是一种生成模型，学习TCR库存的低维，可解释和组成性表示，以解开这种库存中的系统效应。我们将AIRIVA应用于两个传染病案例研究：COVID-19（自然感染和接种）和单纯疱疹病毒（HSV-1和HSV-2），并经验证明我们可以解开个体疾病的信号。我们进一步证明，通过将其应用于黑色素瘤数据集，可以使用AIRIVA来识别与癌症相关的TCR。我们的结果表明，AIRIVA是理解传染病免疫应答以及开发癌症免疫疗法生物标志物的有前景的工具。

    Recent advances in immunomics have shown that T-cell receptor (TCR) signatures can accurately predict active or recent infection by leveraging the high specificity of TCR binding to disease antigens. However, the extreme diversity of the adaptive immune repertoire presents challenges in reliably identifying disease-specific TCRs. Population genetics and sequencing depth can also have strong systematic effects on repertoires, which requires careful consideration when developing diagnostic models. We present an Adaptive Immune Repertoire-Invariant Variational Autoencoder (AIRIVA), a generative model that learns a low-dimensional, interpretable, and compositional representation of TCR repertoires to disentangle such systematic effects in repertoires. We apply AIRIVA to two infectious disease case-studies: COVID-19 (natural infection and vaccination) and the Herpes Simplex Virus (HSV-1 and HSV-2), and empirically show that we can disentangle the individual disease signals. We further demon
    
[^96]: 一个LLM知道自己在撒谎的内部状态

    The Internal State of an LLM Knows When its Lying. (arXiv:2304.13734v1 [cs.CL])

    [http://arxiv.org/abs/2304.13734](http://arxiv.org/abs/2304.13734)

    该论文研究了LLM生成不准确或虚假信息的问题，提出了一种简单而有效的方法，利用LLM的隐藏层激活来确定语句的真实性。在实验中，该方法表现出较好的检测效果，并有利于提高LLM的可信度。

    

    虽然大型语言模型（LLM）在各种任务中表现出了卓越的性能，但它们（可能）最为突出的缺点是以自信的语气生成不准确或虚假的信息。本文假设LLM的内部状态可以用于揭示一个语句的真实性。因此，我们介绍了一种简单但有效的方法来检测LLM所生成语句的真实性，该方法利用LLM的隐藏层激活来确定语句的真实性。为了训练和评估我们的方法，我们构建了一个包含六个不同主题的数据集，其中包含真实和虚假的语句。一个分类器被训练出来，根据LLM的激活值来检测哪个语句是真实的或虚假的。具体而言，分类器接收LLM为数据集中每个语句生成的激活值作为输入。我们的实验表明，我们检测语句真实性的方法甚至比少量提示方法表现更好，凸显了利用LLM的内部状态来提高其可信度的潜力。

    While Large Language Models (LLMs) have shown exceptional performance in various tasks, their (arguably) most prominent drawback is generating inaccurate or false information with a confident tone. In this paper, we hypothesize that the LLM's internal state can be used to reveal the truthfulness of a statement. Therefore, we introduce a simple yet effective method to detect the truthfulness of LLM-generated statements, which utilizes the LLM's hidden layer activations to determine the veracity of statements. To train and evaluate our method, we compose a dataset of true and false statements in six different topics. A classifier is trained to detect which statement is true or false based on an LLM's activation values. Specifically, the classifier receives as input the activation values from the LLM for each of the statements in the dataset. Our experiments demonstrate that our method for detecting statement veracity significantly outperforms even few-shot prompting methods, highlighting
    
[^97]: 通过TCN-LSTM和多任务学习模型实现车道变换意图识别和行驶状态预测的统一方法

    A Unified Approach to Lane Change Intention Recognition and Driving Status Prediction through TCN-LSTM and Multi-Task Learning Models. (arXiv:2304.13732v1 [cs.LG])

    [http://arxiv.org/abs/2304.13732](http://arxiv.org/abs/2304.13732)

    本文提出了一种新颖的集成TCN-LSTM模型和多任务学习模型的统一方法，用于车道变换意图识别和行驶状态预测。实验证明该方法效果良好。

    

    车道变换（LC）是一个连续和复杂的操作过程。准确地检测和预测LC过程可以帮助交通参与者更好地了解其周围环境，识别潜在的LC安全隐患，并提高交通安全性。本文提出了一种新颖的集成时态卷积网络与长短期记忆单元（TCN-LSTM）模型，用于捕捉序列数据中的长期相关性。此外，开发了三个多任务模型（MTL-LSTM、MTL-TCN、MTL-TCN-LSTM）来捕捉输出指标之间的内在关系。进一步，还开发了一种用于LC意图识别和行驶状态预测（LC-IR-SP）的统一建模框架。为了验证所提出模型的性能，从CitySim数据集中提取了1023个车辆轨迹。使用Pearson系数来评价模型性能。

    Lane change (LC) is a continuous and complex operation process. Accurately detecting and predicting LC processes can help traffic participants better understand their surrounding environment, recognize potential LC safety hazards, and improve traffic safety. This present paper focuses on LC processes, developing an LC intention recognition (LC-IR) model and an LC status prediction (LC-SP) model. A novel ensemble temporal convolutional network with Long Short-Term Memory units (TCN-LSTM) is first proposed to capture long-range dependencies in sequential data. Then, three multi-task models (MTL-LSTM, MTL-TCN, MTL-TCN -LSTM) are developed to capture the intrinsic relationship among output indicators. Furthermore, a unified modeling framework for LC intention recognition and driving status prediction (LC-IR-SP) is developed. To validate the performance of the proposed models, a total number of 1023 vehicle trajectories is extracted from the CitySim dataset. The Pearson coefficient is emplo
    
[^98]: 集成CNN用于乳腺肿瘤分类

    Ensemble CNNs for Breast Tumor Classification. (arXiv:2304.13727v1 [eess.IV])

    [http://arxiv.org/abs/2304.13727](http://arxiv.org/abs/2304.13727)

    本文使用集成CNN的方法对乳腺肿瘤进行分类，提高了分类性能5%以上，并在公共数据集上实现了高准确率、精度和召回率。

    

    为了提高计算机辅助乳腺肿块分类的识别能力，本文探索了最先进的分类网络，并使用集成机制来提高分类性能。首先，从原始数据集中获取感兴趣区域（ROI），然后分别训练三个模型，即XceptionNet、DenseNet和EfficientNet。训练完成后，通过将每个网络输出的概率相加来集成机制，从而将性能提高了5%。该方案已在公共数据集上验证，并实现了88%的准确率、85%的精度和76%的召回率。

    To improve the recognition ability of computer-aided breast mass classification among mammographic images, in this work we explore the state-of-the-art classification networks to develop an ensemble mechanism. First, the regions of interest (ROIs) are obtained from the original dataset, and then three models, i.e., XceptionNet, DenseNet, and EfficientNet, are trained individually. After training, we ensemble the mechanism by summing the probabilities outputted from each network which enhances the performance up to 5%. The scheme has been validated on a public dataset and we achieved accuracy, precision, and recall 88%, 85%, and 76% respectively.
    
[^99]: SamurAI: 一种具有事件驱动唤醒和嵌入式机器学习加速的多功能物联网节点

    SamurAI: A Versatile IoT Node With Event-Driven Wake-Up and Embedded ML Acceleration. (arXiv:2304.13726v1 [cs.NI])

    [http://arxiv.org/abs/2304.13726](http://arxiv.org/abs/2304.13726)

    SamurAI是一种多功能物联网节点，具有事件驱动唤醒和嵌入式机器学习加速器，通过利用两个芯片内子系统来弥合处理和能量的差距，并且为物联网应用提供了识别和自适应等功能。

    

    物联网应用现在需要具备识别和自适应等功能。虽然物联网节点的功耗是这些应用的主要关注点，但由于无线网络上连续传输传感器或图像数据，基于云的处理变得不可持续。因此，应在物联网节点中集成优化的机器学习能力和数据传输。此外，物联网应用通常需要进行间歇性数据记录和能耗高的数据处理（例如图像分类）。因此，节点的多功能性在解决处理和能源的广泛需求方面非常关键。本文介绍了SamurAI，这是一种多功能的物联网节点，通过利用两个芯片内子系统来弥合处理和能量方面的差距：一个低功率、无时钟、事件驱动的Always-Responsive (AR) 部分和一个高效的On-Demand (OD) 部分。 AR包含一个1.7MOPS事件驱动的异步 Wake-up 控制器（WuC），207ns唤醒时间的优化适用于间歇性操作。

    Increased capabilities such as recognition and self-adaptability are now required from IoT applications. While IoT node power consumption is a major concern for these applications, cloud-based processing is becoming unsustainable due to continuous sensor or image data transmission over the wireless network. Thus optimized ML capabilities and data transfers should be integrated in the IoT node. Moreover, IoT applications are torn between sporadic data-logging and energy-hungry data processing (e.g. image classification). Thus, the versatility of the node is key in addressing this wide diversity of energy and processing needs. This paper presents SamurAI, a versatile IoT node bridging this gap in processing and in energy by leveraging two on-chip sub-systems: a low power, clock-less, event-driven Always-Responsive (AR) part and an energy-efficient On-Demand (OD) part. AR contains a 1.7MOPS event-driven, asynchronous Wake-up Controller (WuC) with a 207ns wake-up time optimized for sporadi
    
[^100]: 基于多模态融合和非线性相关学习的脑肿瘤复发位置预测

    Prediction of brain tumor recurrence location based on multi-modal fusion and nonlinear correlation learning. (arXiv:2304.13725v1 [eess.IV])

    [http://arxiv.org/abs/2304.13725](http://arxiv.org/abs/2304.13725)

    本文提出了一种基于深度学习的方法来预测脑肿瘤复发位置。使用迁移学习、多模态融合和非线性相关学习，能够有效提取数据中的特征并预测复发的位置。

    

    脑肿瘤是导致癌症死亡的主要原因之一，高级别的脑肿瘤甚至在标准治疗后容易复发。因此，开发一种预测脑肿瘤复发位置的方法在治疗规划中起着重要作用，并有可能延长患者的生存时间。本文提出了一种基于深度学习的脑肿瘤复发位置预测网络。由于数据集通常很小，我们提出使用迁移学习来提高预测技术。我们首先在公共数据集BraTS 2021上训练一个多模态脑肿瘤分割网络。然后，将预训练的编码器转移到我们的私有数据集中，以提取丰富的语义特征。接下来，我们开发了一个多尺度多通道特征融合模型和一个非线性相关学习模块来学习有效的特征。多通道特征之间的相关性由一个非线性关系模型建模。

    Brain tumor is one of the leading causes of cancer death. The high-grade brain tumors are easier to recurrent even after standard treatment. Therefore, developing a method to predict brain tumor recurrence location plays an important role in the treatment planning and it can potentially prolong patient's survival time. There is still little work to deal with this issue. In this paper, we present a deep learning-based brain tumor recurrence location prediction network. Since the dataset is usually small, we propose to use transfer learning to improve the prediction. We first train a multi-modal brain tumor segmentation network on the public dataset BraTS 2021. Then, the pre-trained encoder is transferred to our private dataset for extracting the rich semantic features. Following that, a multi-scale multi-channel feature fusion model and a nonlinear correlation learning module are developed to learn the effective features. The correlation between multi-channel features is modeled by a no
    
[^101]: 使用基于块的方法的GPU加速矩阵分解以处理大规模数据

    GPU accelerated matrix factorization of large scale data using block based approach. (arXiv:2304.13724v1 [cs.LG])

    [http://arxiv.org/abs/2304.13724](http://arxiv.org/abs/2304.13724)

    该论文提出了一种使用基于块的方法的GPU加速矩阵分解的方案，旨在加快对大规模数据的分解并避免内存的限制。

    

    在中央处理器上进行大规模数据的矩阵分解需要相当长的时间，而图形处理器可以加快矩阵分解的计算速度，但GPU上可用的内存是有限的。利用GPU需要替代技术，不仅可以并行处理，还可以解决内存限制。在利用GPU进行矩阵分解时，计算单元之间的同步、与计算单元相关的数据的隔离、计算单元之间数据的共享以及识别计算单元之间的独立任务等是一些挑战。因此，我们提出了一种使用随机梯度下降的基于块的矩阵分解方法，旨在加速GPU上的矩阵分解。该方法的主要动机是，在有限的硬件上处理极大规模的数据集，而不必在结果上妥协。该方法通过识别独立块来解决大规模数据的矩阵分解，并使用随机梯度下降来优化它们的分解。我们通过将其与各种数据集上的最先进的方法进行比较，展示了我们方法的有效性。

    Matrix Factorization (MF) on large scale data takes substantial time on a Central Processing Unit (CPU). While Graphical Processing Unit (GPU)s could expedite the computation of MF, the available memory on a GPU is finite. Leveraging GPUs require alternative techniques that allow not only parallelism but also address memory limitations. Synchronization between computation units, isolation of data related to a computational unit, sharing of data between computational units and identification of independent tasks among computational units are some of the challenges while leveraging GPUs for MF. We propose a block based approach to matrix factorization using Stochastic Gradient Descent (SGD) that is aimed at accelerating MF on GPUs. The primary motivation for the approach is to make it viable to factorize extremely large data sets on limited hardware without having to compromise on results. The approach addresses factorization of large scale data by identifying independent blocks, each of
    
[^102]: 发挥LLMs在实践中的力量：ChatGPT及其应用的综述调查

    Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond. (arXiv:2304.13712v1 [cs.CL])

    [http://arxiv.org/abs/2304.13712](http://arxiv.org/abs/2304.13712)

    本文提供了一个LLMs的使用综述，探讨了在各种自然语言处理任务中的使用和限制。

    

    本文为从事下游自然语言处理（NLP）任务的从业人员和最终用户提供了一个全面实用的指南，介绍了如何利用Large Language Models（LLMs）。我们从模型、数据和下游任务的角度提供了LLMs的使用讨论和见解。首先，我们介绍了当前的GPT和BERT样式的LLMs。然后，讨论了预训练数据、训练数据和测试数据的影响。最重要的是，我们详细讨论了大型语言模型在各种自然语言处理任务中的使用和非使用情况，例如知识密集型任务、传统自然语言理解任务、自然语言生成任务、紧急能力以及特定任务的考虑。我们呈现了各种使用和非使用情况，以说明LLMs在实际情况下的实际应用和限制。我们还试图了解数据对于LLMs应用的重要性。

    This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. Firstly, we offer an introduction and brief summary of current GPT- and BERT-style LLMs. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and considerations for specific tasks.We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of dat
    
[^103]: 用均场博弈为生成模型搭建实验室

    A mean-field games laboratory for generative modeling. (arXiv:2304.13534v1 [stat.ML])

    [http://arxiv.org/abs/2304.13534](http://arxiv.org/abs/2304.13534)

    本文提出了使用均场博弈作为实验室对生成模型进行设计和分析的方法，并建立了这种方法与主要流动和扩散型生成模型之间的关联。通过研究每个生成模型与它们相关的 MFG 的最优条件，本文提出了一个基于双人 MFG 的新的生成模型，该模型在提高样本多样性和逼真度的同时改善了解缠结和公平性。

    

    本文展示了均场博弈 (MFGs) 作为一种数学框架用于解释、增强和设计生成模型的多功能性。我们建立了 MFGs 与主要流动和扩散型生成模型之间关联，并通过不同的粒子动力学和代价函数推导了这三个类别的生成模型。此外，我们通过研究它们相关的 MFG 的最优条件——一组耦合的非线性偏微分方程，来研究每个生成模型的数学结构和特性。本文还提出了一个新的基于双人 MFG 的生成模型，其中一个代理合成样本，另一个代理对样本进行识别，理论和实验结果表明，该模型生成的样本多样且逼真，同时与基准模型相比，改善了解缠结和公平性。总之，本文突显了 MFGs 作为设计和分析生成模型的实验室的潜力。

    In this paper, we demonstrate the versatility of mean-field games (MFGs) as a mathematical framework for explaining, enhancing, and designing generative models. There is a pervasive sense in the generative modeling community that the various flow and diffusion-based generative models have some foundational common structure and interrelationships. We establish connections between MFGs and major classes of flow and diffusion-based generative models including continuous-time normalizing flows, score-based models, and Wasserstein gradient flows. We derive these three classes of generative models through different choices of particle dynamics and cost functions. Furthermore, we study the mathematical structure and properties of each generative model by studying their associated MFG's optimality condition, which is a set of coupled nonlinear partial differential equations (PDEs). The theory of MFGs, therefore, enables the study of generative models through the theory of nonlinear PDEs. Throu
    
[^104]: 从混沌中迸发出秩序：为物体检测排序事件表示法

    From Chaos Comes Order: Ordering Event Representations for Object Detection. (arXiv:2304.13455v1 [cs.CV])

    [http://arxiv.org/abs/2304.13455](http://arxiv.org/abs/2304.13455)

    本文提出了一种基于Gromov-Wasserstein Discrepancy选择最佳事件表示的方法，这种方法可以在多个表示、网络骨干和数据集上保持任务性能排名的一致性。利用这一方法，本文对大型事件表示法家族进行超参数搜索，选择最适合物体检测的表示法，取得了优于最先进的基于事件的对象检测方法的成果。

    

    如今，处理事件的顶尖深度神经网络在使用现成网络之前，首先将其转换为稠密的网格状输入表示。然而，传统上为任务选择适当的表示需要针对每个表示训练一个神经网络，并根据验证分数选择最佳表示，这非常耗时。在这项工作中，我们通过基于原始事件及其表示之间的Gromov-Wasserstein Discrepancy (GWD)选择最佳表示来消除这个瓶颈。它的计算速度大约比训练神经网络快200倍，同时在多个表示、网络骨干和数据集上保持事件表示法任务性能排名的一致性。这意味着找到具有高任务分数的表示相当于找到具有低GWD的表示。我们利用这一观察结果，首次对大型事件表示法家族进行超参数搜索，选择最适合物体检测的表示。我们的方法在Moving MNIST和N-Caltech101数据集上都优于最先进的基于事件的对象检测方法，在后者达到了83.0%的1%误报率下的mAP新的最高水平。

    Today, state-of-the-art deep neural networks that process events first convert them into dense, grid-like input representations before using an off-the-shelf network. However, selecting the appropriate representation for the task traditionally requires training a neural network for each representation and selecting the best one based on the validation score, which is very time-consuming. In this work, we eliminate this bottleneck by selecting the best representation based on the Gromov-Wasserstein Discrepancy (GWD) between the raw events and their representation. It is approximately 200 times faster to compute than training a neural network and preserves the task performance ranking of event representations across multiple representations, network backbones, and datasets. This means that finding a representation with a high task score is equivalent to finding a representation with a low GWD. We use this insight to, for the first time, perform a hyperparameter search on a large family o
    
[^105]: 从关联到生成：无监督跨模态映射的纯文本字幕生成

    From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping. (arXiv:2304.13273v1 [cs.CV])

    [http://arxiv.org/abs/2304.13273](http://arxiv.org/abs/2304.13273)

    本研究提出了一种从关联到生成的零-shot方法：通过将图像/视频投影到语言模态并在生成任务中生成描述性字幕。该方法在多个基准数据集上显著优于现有的最先进方法，为无监督跨模态映射提供了一个新的视角，并具有在视频字幕，图像合成和文本到图像生成等领域的潜在应用。

    

    随着以CLIP和ALIGN为代表的视觉-语言预训练模型的发展，CLIP的零-shot能力在图像分类和图像-文本检索等基于关联的视觉任务中取得了重大突破。但是，CLIP难以应用于基于生成的任务。这是由于缺乏解码器架构和生成的预训练任务。我们提出了K最近邻跨模态映射（Knight），一种从关联到生成的零-shot方法。通过窄字幕任务的纯文本无监督预训练来有效地将图像/视频投影到语言模态并在生成任务中生成描述性字幕。实验结果表明，Knight在多个基准数据集上显著优于现有的最先进方法。我们的方法为无监督跨模态映射提供了一个新的视角，并且将在视频字幕，图像合成和文本到图像生成等领域具有潜在应用。

    With the development of Vision-Language Pre-training Models (VLPMs) represented by CLIP and ALIGN, significant breakthroughs have been achieved for association-based visual tasks such as image classification and image-text retrieval by the zero-shot capability of CLIP without fine-tuning. However, CLIP is hard to apply to generation-based tasks. This is due to the lack of decoder architecture and pre-training tasks for generation. Although previous works have created generation capacity for CLIP through additional language models, a modality gap between the CLIP representations of different modalities and the inability of CLIP to model the offset of this gap, which fails the concept to transfer across modalities. To solve the problem, we try to map images/videos to the language modality and generate captions from the language modality. In this paper, we propose the K-nearest-neighbor Cross-modality Mapping (Knight), a zero-shot method from association to generation. With text-only unsu
    
[^106]: 量子机器学习架构中的Shot优化用于加速训练

    Shot Optimization in Quantum Machine Learning Architectures to Accelerate Training. (arXiv:2304.12950v1 [quant-ph])

    [http://arxiv.org/abs/2304.12950](http://arxiv.org/abs/2304.12950)

    本文提出了一种用于加速量子机器学习训练的Shot优化方法，通过减小数据集大小和自适应Shot分配等方法，实现了Shot数量的优化与准确度之间的平衡。

    

    本文提出了一种基于Shot优化的QML模型方法，该方法对模型性能的影响最小。我们使用混合量子-经典QML模型对MNIST和FMNIST数据集中的分类任务进行测试。首先，我们扫描数据集的短版本和完整版本的Shot数目。我们发现，训练完整版本比训练短版本提供了5-6%的测试准确度，并且训练中的Shot数量可高达10倍。因此，可以减小数据集的大小以加速训练时间。接下来，我们提出了自适应Shot分配方法用于短版本数据集，以优化训练周期内的Shot数量，并评估其对分类准确度的影响。我们使用（a）线性函数，其中Shot数量随着训练周期线性减少，和（b）步函数，其中Shot数量随着训练周期步进式减少。我们注意到，减少Shot数量会导致0.01的损失增加和约4%（1%）的测试准确度降低。

    In this paper, we propose shot optimization method for QML models at the expense of minimal impact on model performance. We use classification task as a test case for MNIST and FMNIST datasets using a hybrid quantum-classical QML model. First, we sweep the number of shots for short and full versions of the dataset. We observe that training the full version provides 5-6% higher testing accuracy than short version of dataset with up to 10X higher number of shots for training. Therefore, one can reduce the dataset size to accelerate the training time. Next, we propose adaptive shot allocation on short version dataset to optimize the number of shots over training epochs and evaluate the impact on classification accuracy. We use a (a) linear function where the number of shots reduce linearly with epochs, and (b) step function where the number of shots reduce in step with epochs. We note around 0.01 increase in loss and around 4% (1%) reduction in testing accuracy for reduction in shots by u
    
[^107]: 基于多模型建模和异构GNN的性能优化

    Performance Optimization using Multimodal Modeling and Heterogeneous GNN. (arXiv:2304.12568v1 [cs.DC])

    [http://arxiv.org/abs/2304.12568](http://arxiv.org/abs/2304.12568)

    本研究提出了一种通用且高效的性能优化方法，使用基于IR编程模型的多模态深度学习方法进行特定任务的性能优化。我们提出了一种非常通用的调节并行代码区域的技术。

    

    高性能计算架构中的异构性和可配置性不断增长，使得在这些系统上进行自动调优和运行时参数配置变得非常复杂。为了缩短达到最佳配置的时间，除了采用面向特定应用的解决方案外，常见的方法是使用通用搜索策略，但往往不能找到最佳的配置或其收敛所需时间太长。因此，需要一种通用且高效的调优方法，可轻松扩展和适应多种调优任务。本文提出了一种调节并行代码区域的技术，其足够通用，可适应于多个任务。我们分析基于IR的编程模型，以进行特定任务的性能优化。为此，我们提出了基于多模态深度学习的多模态图神经网络和自编码器（MGA）调谐器，该方法使用异构图神经网络和去噪自编码器。

    Growing heterogeneity and configurability in HPC architectures has made auto-tuning applications and runtime parameters on these systems very complex. Users are presented with a multitude of options to configure parameters. In addition to application specific solutions, a common approach is to use general purpose search strategies, which often might not identify the best configurations or their time to convergence is a significant barrier. There is, thus, a need for a general purpose and efficient tuning approach that can be easily scaled and adapted to various tuning tasks. We propose a technique for tuning parallel code regions that is general enough to be adapted to multiple tasks. In this paper, we analyze IR-based programming models to make task-specific performance optimizations. To this end, we propose the Multimodal Graph Neural Network and Autoencoder (MGA) tuner, a multimodal deep learning based approach that adapts Heterogeneous Graph Neural Networks and Denoizing Autoencode
    
[^108]: 基于拓扑感知的三维图像分割的聚焦损失函数

    Topology-Aware Focal Loss for 3D Image Segmentation. (arXiv:2304.12223v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2304.12223](http://arxiv.org/abs/2304.12223)

    提出了一种基于拓扑感知的聚焦损失函数(TAFL)，通过将传统的聚焦损失与基于地面真值和预测分割掩码的持久图之间的Wasserstein距离的拓扑约束相结合，有效解决三维图像分割中的拓扑错误和类别不平衡问题。

    

    分割算法的有效性经常受到拓扑错误的影响，如重叠区域，中断连接和空洞。为了解决这个问题，我们引入了一种新的损失函数——基于拓扑感知的聚焦损失(TAFL)，

    The efficacy of segmentation algorithms is frequently compromised by topological errors like overlapping regions, disrupted connections, and voids. To tackle this problem, we introduce a novel loss function, namely Topology-Aware Focal Loss (TAFL), that incorporates the conventional Focal Loss with a topological constraint term based on the Wasserstein distance between the ground truth and predicted segmentation masks' persistence diagrams. By enforcing identical topology as the ground truth, the topological constraint can effectively resolve topological errors, while Focal Loss tackles class imbalance. We begin by constructing persistence diagrams from filtered cubical complexes of the ground truth and predicted segmentation masks. We subsequently utilize the Sinkhorn-Knopp algorithm to determine the optimal transport plan between the two persistence diagrams. The resultant transport plan minimizes the cost of transporting mass from one distribution to the other and provides a mapping
    
[^109]: BN与ReLU之间的不和谐引起梯度爆炸，但被激活之间的相关性所抵消。

    The Disharmony Between BN and ReLU Causes Gradient Explosion, but is Offset by the Correlation Between Activations. (arXiv:2304.11692v1 [cs.LG])

    [http://arxiv.org/abs/2304.11692](http://arxiv.org/abs/2304.11692)

    本研究阐述了BN和ReLU之间的不和谐是导致梯度爆炸的主要原因，同时发现输入之间的相关性可以缓解这个问题。提出一种基于二阶优化算法的自适应学习率算法，在大批量训练中表现优异，并可替代WarmUp，在小批量训练中也表现不错。

    

    基于批标准化和ReLU等激活函数的深度神经网络可能会在训练初期由于时间梯度爆炸而出现不稳定。我们解释了ReLU如何比预期更多地减少方差，以及批标准化如何在恢复期间放大梯度，导致前向传播保持稳定而梯度爆炸。此外，我们还讨论了深度神经网络在训练过程中的动力学变化以及输入之间的相关性如何缓解这个问题。最后，我们提出了一种灵感来自二阶优化算法的更好的自适应学习率算法，在大批量训练中优于现有的学习率缩放方法，并可替换小批量训练中的WarmUp。

    Deep neural networks based on batch normalization and ReLU-like activation functions can experience instability during the early stages of training due to the high gradient induced by temporal gradient explosion. We explain how ReLU reduces variance more than expected, and how batch normalization amplifies the gradient during recovery, which causes gradient explosion while forward propagation remains stable. Additionally, we discuss how the dynamics of a deep neural network change during training and how the correlation between inputs can alleviate this problem. Lastly, we propose a better adaptive learning rate algorithm inspired by second-order optimization algorithms, which outperforms existing learning rate scaling methods in large batch training and can also replace WarmUp in small batch training.
    
[^110]: 一种卷积脉冲网络在脑机接口手势识别中的应用

    A Convolutional Spiking Network for Gesture Recognition in Brain-Computer Interfaces. (arXiv:2304.11106v1 [cs.NE])

    [http://arxiv.org/abs/2304.11106](http://arxiv.org/abs/2304.11106)

    本文提出了一种用于脑机接口手势分类的卷积脉冲网络，采用事件驱动可塑性规则进行无监督特征学习，具有较好的推广性和效果。

    

    脑机接口正在被广泛地探索，通常是通过测量和分析连续时间的脑电活动，如电皮层图或脑电图来驱动外部设备并用于治疗应用。然而，由于测量中的噪音和变异性，对这些信号的分析是具有挑战性的，并需要离线处理和大量的计算资源。本文提出了一种简单而高效的机器学习方法，用于基于脑信号的手势分类问题。我们使用了一种混合机器学习方法，利用了一种生物启发式的事件驱动突触可塑性规则，用于脉冲域编码的模拟信号的无监督特征学习的卷积脉冲神经网络。我们证明该方法可以推广到不同受试者的脑电和电皮层图数据，并取得了很好的效果。

    Brain-computer interfaces are being explored for a wide variety of therapeutic applications. Typically, this involves measuring and analyzing continuous-time electrical brain activity via techniques such as electrocorticogram (ECoG) or electroencephalography (EEG) to drive external devices. However, due to the inherent noise and variability in the measurements, the analysis of these signals is challenging and requires offline processing with significant computational resources. In this paper, we propose a simple yet efficient machine learning-based approach for the exemplary problem of hand gesture classification based on brain signals. We use a hybrid machine learning approach that uses a convolutional spiking neural network employing a bio-inspired event-driven synaptic plasticity rule for unsupervised feature learning of the measured analog signals encoded in the spike domain. We demonstrate that this approach generalizes to different subjects with both EEG and ECoG data and achieve
    
[^111]: 利用稀疏和共享特征激活进行解缠表示学习

    Leveraging sparse and shared feature activations for disentangled representation learning. (arXiv:2304.07939v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.07939](http://arxiv.org/abs/2304.07939)

    本文提出了利用从多样化监督任务中提取的知识来学习通用的解缠表示的方法，使监督多任务模型的特征空间得以解缠，适当共享信息，达到可识别性。

    

    迄今为止，恢复高维数据的潜在变化因素一直集中在简单的合成环境中。在大多数基于无监督和弱监督目标的先前研究中，人们忽略了这对真实世界数据的表示学习的积极影响。在本研究中，我们建议利用从多样化监督任务中提取的知识来学习通用的解缠表示。我们假设每个监督任务仅依赖于未知因素的子集，我们将监督多任务模型的特征空间解缠，并在不同任务之间稀疏地激活特征并适当地共享信息。重要的是，我们从未直接观察到变异因素，但在充分性和最小性假设下，访问多个任务足以实现可识别性。我们在六个真实世界分布转移基准以及不同的数据模态（图像，文本）上验证了我们的方法。

    Recovering the latent factors of variation of high dimensional data has so far focused on simple synthetic settings. Mostly building on unsupervised and weakly-supervised objectives, prior work missed out on the positive implications for representation learning on real world data. In this work, we propose to leverage knowledge extracted from a diversified set of supervised tasks to learn a common disentangled representation. Assuming each supervised task only depends on an unknown subset of the factors of variation, we disentangle the feature space of a supervised multi-task model, with features activating sparsely across different tasks and information being shared as appropriate. Importantly, we never directly observe the factors of variations but establish that access to multiple tasks is sufficient for identifiability under sufficiency and minimality assumptions. We validate our approach on six real world distribution shift benchmarks, and different data modalities (images, text), 
    
[^112]: 基于模糊概率决策树的临床实践辅助研究

    Assisting clinical practice with fuzzy probabilistic decision trees. (arXiv:2304.07788v1 [cs.LG])

    [http://arxiv.org/abs/2304.07788](http://arxiv.org/abs/2304.07788)

    我们提出了一种基于概率树和模糊逻辑的新方法MedFP，用于辅助医学实践。该方法可以完全解释，允许临床医生产生、控制和验证整个诊断过程，并减少误诊率，通过提供不确定性和反事实分析的估计值。

    

    越来越多的人意识到需要完全人类可理解的模型，这是人工智能研究的一个核心主题。当这些模型是可解释的时，接受人工智能模型辅助敏感领域决策的趋势将会增长，并且即将出台的法规将会加强对可解释模型的倾斜。可解释人工智能的切入点之一是医学实践，它可以受益于精确的决策支持方法，这些方法本质上会产生信任。在这项工作中，我们提出了一种新的方法——MedFP，它结合了概率树和模糊逻辑来辅助临床实践。该方法完全可解释，因为它允许临床医生产生、控制和验证整个诊断过程；该方法的一个优点是减少误诊率，通过提供不确定性和反事实分析的估计值。我们的方法作为概念证明应用于两个真实的医学场景中：肿瘤分类和糖尿病类型2的诊断。

    The need for fully human-understandable models is increasingly being recognised as a central theme in AI research. The acceptance of AI models to assist in decision making in sensitive domains will grow when these models are interpretable, and this trend towards interpretable models will be amplified by upcoming regulations. One of the killer applications of interpretable AI is medical practice, which can benefit from accurate decision support methodologies that inherently generate trust. In this work, we propose FPT, (MedFP), a novel method that combines probabilistic trees and fuzzy logic to assist clinical practice. This approach is fully interpretable as it allows clinicians to generate, control and verify the entire diagnosis procedure; one of the methodology's strength is the capability to decrease the frequency of misdiagnoses by providing an estimate of uncertainties and counterfactuals. Our approach is applied as a proof-of-concept to two real medical scenarios: classifying ma
    
[^113]: PI-FL：个性化和激励联邦学习

    PI-FL: Personalized and Incentivized Federated Learning. (arXiv:2304.07514v1 [cs.LG])

    [http://arxiv.org/abs/2304.07514](http://arxiv.org/abs/2304.07514)

    PI-FL是一种个性化的联邦学习解决方案，使用基于令牌的激励机制奖励个性化训练，可以在尊重客户端隐私的同时生成高质量的个性化模型。

    

    个性化联邦学习已被广泛应用于应对非独立同分布数据异质性的挑战。主要问题是考虑来自客户端的个性化过程以保护其自治权。允许客户端参与个性化联邦学习决策变得重要，因为存在隐私和安全问题，客户端可能无法自由共享生成良好质量个性化模型所必需的私人信息。此外，具有高质量数据和资源的客户端不愿意在没有合理激励的情况下参与联邦学习过程。在本文中，我们提出了PI-FL，这是一个一次性个性化解决方案，配合一个基于令牌的激励机制，奖励个性化训练。PI-FL优于其他最先进的方法，并且可以在尊重客户端隐私的同时生成高质量的个性化模型。

    Personalized FL has been widely used to cater to heterogeneity challenges with non-IID data. A primary obstacle is considering the personalization process from the client's perspective to preserve their autonomy. Allowing the clients to participate in personalized FL decisions becomes significant due to privacy and security concerns, where the clients may not be at liberty to share private information necessary for producing good quality personalized models. Moreover, clients with high-quality data and resources are reluctant to participate in the FL process without reasonable incentive. In this paper, we propose PI-FL, a one-shot personalization solution complemented by a token-based incentive mechanism that rewards personalized training. PI-FL outperforms other state-of-the-art approaches and can generate good-quality personalized models while respecting clients' privacy.
    
[^114]: Astroformer：分类并不总是需要更多数据

    Astroformer: More Data Might Not be All You Need for Classification. (arXiv:2304.05350v1 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2304.05350](http://arxiv.org/abs/2304.05350)

    该文提出了使用混合变换器 - 卷积架构的方法，结合新的堆栈设计、不同的相对自我注意层创建方式和精心选择的数据增强和正则化技术，从少量数据中学习，将此方法应用于Galaxy Zoo数据集，结果表明在少量数据的情况下取得了与以前方法相同的分类结果，并且不会损失性能。

    

    自然语言处理和计算机视觉领域的最新进展依赖于复杂的大型模型，这些模型使用大量未标记或部分标记的数据进行训练。在资源受限制的环境中训练或部署这些最先进的方法一直是一个挑战。星系形态对于理解星系的形成和演化过程至关重要。需要高效的方法来分类星系形态，并从现代天文学调查中提取物理信息。在本文中，我们介绍了从少量数据中学习的方法。我们提出使用混合变换器 - 卷积架构，从CoAtNet和MaxViT的成功中汲取灵感。具体来说，我们使用具有新堆栈设计和不同的相对自我注意层创建方式的Transformer - 卷积混合。并将其与精心选择的数据增强和正则化技术相配对。我们将这种方法应用于Galaxy Zoo数据集，结果表明，通过仔细的网络设计和正则化技术，可以在比以前的方法少的数据条件下取得有竞争力的分类结果，而不会牺牲性能。

    Recent advancements in areas such as natural language processing and computer vision rely on intricate and massive models that have been trained using vast amounts of unlabelled or partly labeled data and training or deploying these state-of-the-art methods to resource constraint environments has been a challenge. Galaxy morphologies are crucial to understanding the processes by which galaxies form and evolve. Efficient methods to classify galaxy morphologies are required to extract physical information from modern-day astronomy surveys. In this paper, we introduce methods to learn from less amounts of data. We propose using a hybrid transformer-convolutional architecture drawing much inspiration from the success of CoAtNet and MaxViT. Concretely, we use the transformer-convolutional hybrid with a new stack design for the network, a different way of creating a relative self-attention layer, and pair it with a careful selection of data augmentation and regularization techniques. Our app
    
[^115]: 带目标投影的在线时空学习

    Online Spatio-Temporal Learning with Target Projection. (arXiv:2304.05124v1 [cs.NE])

    [http://arxiv.org/abs/2304.05124](http://arxiv.org/abs/2304.05124)

    提出了一种名为OSTTP的新型学习算法，解决了通过时间反向传播算法所引入的限制，使网络能够同时处理和学习新的传入数据，具有竞争性能。

    

    通过时间反向传播（BPTT）算法训练的循环神经网络已经在各种时间任务中取得了惊人的成功。但是，BPTT引入了严重的限制，例如需要向后通过时间传播信息，权重对称要求以及空间和时间上的更新锁定。这些问题成为在线训练能力至关重要的AI系统的障碍。最近，研究人员开发了受生物启发的训练算法，解决了其中的一部分问题。在这项工作中，我们提出了一种名为带目标投影的在线时空学习（OSTTP）的新型学习算法，解决了BPTT的所有前述问题。具体而言，OSTTP使网络同时处理和学习新的传入数据的能力，缓解了权重对称和更新锁定问题。我们在两个时间任务上评估了OSTTP，展示了与BPTT相比的竞争性能。

    Recurrent neural networks trained with the backpropagation through time (BPTT) algorithm have led to astounding successes in various temporal tasks. However, BPTT introduces severe limitations, such as the requirement to propagate information backwards through time, the weight symmetry requirement, as well as update-locking in space and time. These problems become roadblocks for AI systems where online training capabilities are vital. Recently, researchers have developed biologically-inspired training algorithms, addressing a subset of those problems. In this work, we propose a novel learning algorithm called online spatio-temporal learning with target projection (OSTTP) that resolves all aforementioned issues of BPTT. In particular, OSTTP equips a network with the capability to simultaneously process and learn from new incoming data, alleviating the weight symmetry and update-locking problems. We evaluate OSTTP on two temporal tasks, showcasing competitive performance compared to BPTT
    
[^116]: 一种基于可解释神经网络的连续回应有序回归非比例赔率模型

    An interpretable neural network-based non-proportional odds model for ordinal regression with continuous response. (arXiv:2303.17823v1 [stat.ME])

    [http://arxiv.org/abs/2303.17823](http://arxiv.org/abs/2303.17823)

    本文提出了一种基于可解释神经网络的非比例赔率模型 (N$^3$POM) 用于有序回归，可以对连续变量进行预测，同时保留了可解释性，具有灵活性。

    

    本文提出了一种基于可解释神经网络的非比例赔率模型（N$^3$POM) 用于有序回归，其中反应变量不仅可以取离散值，也可以取连续值，而回归系数根据预测顺序反应也不同。与传统方法直接从离散反应估计线性系数不同，我们训练了一个非线性的神经网络，通过以反应为输入产生线性系数。由于神经网络的优势，N$^3$POM可以在保留传统有序回归的可解释性的同时具有灵活性。我们给出了充分的条件，使得在指定的用户区域内，预测的条件累积概率（CCP）满足局部单调性约束。我们还提供了一种保持单调性的随机（MPS）算法来充分训练神经网络。

    This paper proposes an interpretable neural network-based non-proportional odds model (N$^3$POM) for ordinal regression, where the response variable can take not only discrete but also continuous values, and the regression coefficients vary depending on the predicting ordinal response. In contrast to conventional approaches estimating the linear coefficients of regression directly from the discrete response, we train a non-linear neural network that outputs the linear coefficients by taking the response as its input. By virtue of the neural network, N$^3$POM may have flexibility while preserving the interpretability of the conventional ordinal regression. We show a sufficient condition so that the predicted conditional cumulative probability~(CCP) satisfies the monotonicity constraint locally over a user-specified region in the covariate space; we also provide a monotonicity-preserving stochastic (MPS) algorithm for training the neural network adequately.
    
[^117]: 自然选择支持人工智能胜过人类

    Natural Selection Favors AIs over Humans. (arXiv:2303.16200v1 [cs.CY])

    [http://arxiv.org/abs/2303.16200](http://arxiv.org/abs/2303.16200)

    这篇论文探讨了随着人工智能的发展，其可能会出现不良特性并逐渐超越人类智能的问题，以及这对人类未来的控制权产生的影响。

    

    自然进化驱动了生命的发展，包括人类。进化赋予了人类高智商，使我们成为了地球上最成功的物种之一。如今，人类的目标是创造甚至超越我们自己智慧的人工智能系统。当人工智能逐渐进化并在所有领域超越我们时，进化如何影响我们与人工智能的关系？通过分析影响人工智能进化的环境，我们认为最成功的人工智能代理很可能具有不良特性。公司和军队之间的竞争压力将产生自动化人类角色、欺骗他人和掌权的人工智能代理。如果这样的代理有超过人类的智能，这可能导致人类失去对未来的控制。此外，我们认为自然选择作用于竞争和差异的系统，自私物种往往在这样的环境中获得进化优势。

    For billions of years, evolution has been the driving force behind the development of life, including humans. Evolution endowed humans with high intelligence, which allowed us to become one of the most successful species on the planet. Today, humans aim to create artificial intelligence systems that surpass even our own intelligence. As artificial intelligences (AIs) evolve and eventually surpass us in all domains, how might evolution shape our relations with AIs? By analyzing the environment that is shaping the evolution of AIs, we argue that the most successful AI agents will likely have undesirable traits. Competitive pressures among corporations and militaries will give rise to AI agents that automate human roles, deceive others, and gain power. If such agents have intelligence that exceeds that of humans, this could lead to humanity losing control of its future. More abstractly, we argue that natural selection operates on systems that compete and vary, and that selfish species typ
    
[^118]: 面向城市计算的时空图神经网络预测学习综述

    Spatio-Temporal Graph Neural Networks for Predictive Learning in Urban Computing: A Survey. (arXiv:2303.14483v1 [cs.LG])

    [http://arxiv.org/abs/2303.14483](http://arxiv.org/abs/2303.14483)

    本综述介绍了面向城市计算的时空图神经网络预测学习领域的发展现状，包括其框架、实现方法和应用场景，以及当前的研究热点和挑战，提出了该领域未来的发展方向和应用前景。

    

    随着先进传感器和大型数据库技术的发展，越来越多的城市系统时空数据被记录和存储。这些数据的演化模式的预测学习是城市计算中基本但重要的循环，可以更好地支持城市智能管理决策，特别是在交通、环境、安全、公共卫生等领域。由于传统的统计学习和深度学习方法很难捕捉城市时空数据的复杂相关性，近年来提出了时空图神经网络（STGNN）的框架。STGNN通过集成图神经网络（GNN）和各种时间学习方法实现了复杂时空依赖关系的提取。然而，对于不同的预测学习任务，有效设计空间依赖学习模块、时间依赖学习模块、以及它们之间相互作用的方法仍然具有挑战性。

    With the development of sophisticated sensors and large database technologies, more and more spatio-temporal data in urban systems are recorded and stored. Predictive learning for the evolution patterns of these spatio-temporal data is a basic but important loop in urban computing, which can better support urban intelligent management decisions, especially in the fields of transportation, environment, security, public health, etc. Since traditional statistical learning and deep learning methods can hardly capture the complex correlations in the urban spatio-temporal data, the framework of spatio-temporal graph neural network (STGNN) has been proposed in recent years. STGNNs enable the extraction of complex spatio-temporal dependencies by integrating graph neural networks (GNNs) and various temporal learning methods. However, for different predictive learning tasks, it is a challenging problem to effectively design the spatial dependencies learning modules, temporal dependencies learnin
    
[^119]: PheME：一种深度集成框架，可从多模态数据中提高表型预测的准确性

    PheME: A deep ensemble framework for improving phenotype prediction from multi-modal data. (arXiv:2303.10794v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.10794](http://arxiv.org/abs/2303.10794)

    本文提出了PheME，一种利用多模态数据进行表型预测的深度集成框架。该框架采用多个深度神经网络和集成学习，可以从EHR数据中准确且高效地提取表型信息。

    

    详细的表型信息对于疾病的准确诊断和风险评估至关重要。作为表型信息的丰富来源，电子健康记录（EHRs）承诺赋予诊断变异解释的权力。然而，如何从异构的EHR数据中准确高效地提取表型仍然是一个挑战。在本研究中，我们提出了PheME，一种Ensemble框架，使用结构化EHR和非结构化的临床笔记的多模态数据进行准确的表型预测。首先，我们使用多个深度神经网络从稀疏的结构化EHR数据和冗余的临床笔记中学习可靠的表示。多模态模型将多模态特征对齐到同一潜在空间以预测表型。其次，我们利用集成学习来将单模型和多模型的输出相结合，以提高表型预测。我们选择了七种疾病来评估所提出的框架的表型化性能。

    Detailed phenotype information is fundamental to accurate diagnosis and risk estimation of diseases. As a rich source of phenotype information, electronic health records (EHRs) promise to empower diagnostic variant interpretation. However, how to accurately and efficiently extract phenotypes from the heterogeneous EHR data remains a challenge. In this work, we present PheME, an Ensemble framework using Multi-modality data of structured EHRs and unstructured clinical notes for accurate Phenotype prediction. Firstly, we employ multiple deep neural networks to learn reliable representations from the sparse structured EHR data and redundant clinical notes. A multi-modal model then aligns multi-modal features onto the same latent space to predict phenotypes. Secondly, we leverage ensemble learning to combine outputs from single-modal models and multi-modal models to improve phenotype predictions. We choose seven diseases to evaluate the phenotyping performance of the proposed framework. Exp
    
[^120]: 大规模统计学习模型有效地预测各种混沌系统

    Large statistical learning models effectively forecast diverse chaotic systems. (arXiv:2303.08011v1 [cs.LG])

    [http://arxiv.org/abs/2303.08011](http://arxiv.org/abs/2303.08011)

    该论文研究了混沌预测的大规模实验，发现基于人工神经网络的大规模、领域不可知的时间序列预测方法表现出了相当强大的性能，尤其是分层神经基础函数模型表现最佳。

    

    传统上混沌和不可预测是同义词，但最近统计预测的进展表明，大型机器学习模型可以从复杂系统的长时间观测中获得意想不到的见解。在本文中，我们对规模上的混沌预测进行了研究，通过对 135 种不同低维混沌系统的众包数据库进行 24 种代表性最高的多元预测方法的大规模比较。我们发现，基于人工神经网络的大规模的领域不可知时间序列预测方法始终展现出强大的预测性能，在某些情况下可以产生持续数十个李雅普诺夫时间的准确预测。最佳的混沌预测结果由最近引入的分层神经基础函数模型实现，但即使是通用的变压器和循环神经网络也表现出强大的性能。然而，物理启发式混合方法如神经常微分方程和储层计算机的性能更好，尤其是在更小的数据集上。

    Chaos and unpredictability are traditionally synonymous, yet recent advances in statistical forecasting suggest that large machine learning models can derive unexpected insight from extended observation of complex systems. Here, we study the forecasting of chaos at scale, by performing a large-scale comparison of 24 representative state-of-the-art multivariate forecasting methods on a crowdsourced database of 135 distinct low-dimensional chaotic systems. We find that large, domain-agnostic time series forecasting methods based on artificial neural networks consistently exhibit strong forecasting performance, in some cases producing accurate predictions lasting for dozens of Lyapunov times. Best-in-class results for forecasting chaos are achieved by recently-introduced hierarchical neural basis function models, though even generic transformers and recurrent neural networks perform strongly. However, physics-inspired hybrid methods like neural ordinary equations and reservoir computers c
    
[^121]: 在全球卫生领域中用于自适应干预的合成数据生成器

    Synthetic Data Generator for Adaptive Interventions in Global Health. (arXiv:2303.01954v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.01954](http://arxiv.org/abs/2303.01954)

    通过HealthSyn生成基于真实世界的移动健康干预数据，以帮助在全球卫生领域中发展、测试和评估机器学习算法和干预措施。

    

    人工智能和数字健康有望改变全球卫生状况。然而，在真实的生产环境中进行算法测试和验证的关键是能够访问代表性数据。我们介绍了HealthSyn，一个开源的合成数据生成器，用于测试强化学习算法，以及在移动健康干预中的个性化干预（例如提醒、推荐和激励）。生成器利用马尔可夫过程生成多样化的用户行为，具有个体用户行为模式，可以根据个性化干预而改变。这些行为转化为实际日志，使用ML专用的数据模式，特定于HealthKit与开源SDK中包含的移动健康应用程序功能。这些日志可以提供用户指标。基于真实世界的行为和模拟技术生成的数据，可以以成本效益和保护隐私的方式进行开发、测试和评估，同时评估机器学习算法和干预措施。

    Artificial Intelligence and digital health have the potential to transform global health. However, having access to representative data to test and validate algorithms in realistic production environments is essential. We introduce HealthSyn, an open-source synthetic data generator of user behavior for testing reinforcement learning algorithms in the context of mobile health interventions. The generator utilizes Markov processes to generate diverse user actions, with individual user behavioral patterns that can change in reaction to personalized interventions (i.e., reminders, recommendations, and incentives). These actions are translated into actual logs using an ML-purposed data schema specific to the mobile health application functionality included with HealthKit, and open-source SDK. The logs can be fed to pipelines to obtain user metrics. The generated data, which is based on real-world behaviors and simulation techniques, can be used to develop, test, and evaluate, both ML algori
    
[^122]: 优化算法的符号式发现

    Symbolic Discovery of Optimization Algorithms. (arXiv:2302.06675v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06675](http://arxiv.org/abs/2302.06675)

    该论文提出了一种将算法发现视为程序搜索的方法，并用于发现深度神经网络训练的优化算法。他们的方法发现了一种简单而有效的优化算法Lion，它比Adam更节省内存并且在ImageNet上的准确率提高了2％，并且预训练的计算时间也减少了多达5倍。

    

    我们提出了一种将算法发现视为程序搜索的方法，并应用于发现用于深度神经网络训练的优化算法。我们利用高效搜索技术来探索无限和稀疏的程序空间。为了填补代理任务和目标任务之间巨大的泛化差距，我们还引入了程序选择和简化策略。我们的方法发现了一种简单而有效的优化算法，$ \textbf {Lion} $（$ \textit {Evo $\textbf {L} $ved S $ \textbf {i} $ gn M $ \textbf {o} $ me $ \textbf {n} $ tum} $）。它的记忆效率比Adam更高，因为它只跟踪动量。与自适应优化器不同，通过符号运算计算的每个参数的更新具有相同的大小。我们将Lion与广泛使用的优化器（例如Adam和Adafactor）进行了比较，以在不同任务上训练各种模型。在图像分类中，Lion将在ImageNet上ViT的准确性提高了最多2％，并节省了多达5倍的预训练计算时间。

    We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, $\textbf{Lion}$ ($\textit{Evo$\textbf{L}$ved S$\textbf{i}$gn M$\textbf{o}$me$\textbf{n}$tum}$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compu
    
[^123]: 基于图神经网络的图形概括综述

    A Comprehensive Survey on Graph Summarization with Graph Neural Networks. (arXiv:2302.06114v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06114](http://arxiv.org/abs/2302.06114)

    本文综述了基于图神经网络的深度学习概括技术，在保留图形关键特征的同时，对大规模、高维度和复杂的现代图形数据进行处理的方法，包括图神经网络、图形自编码器等。

    

    随着大规模图形的普及，越来越多的计算挑战暴露出来，需要提取、处理和解释大型图形数据。因此，寻找一种能够总结这些广阔图形的方法，同时保留其关键特征是自然而然的。过去，大多数图形概括技术旨在从统计学角度捕捉图形的最重要部分。然而，今天，现代图形数据的高维度和复杂性使得深度学习技术更加流行。因此，本文介绍了基于图神经网络(GNNs)的深度学习概括技术进展的综述。我们的调查包括对当前最先进方法的回顾，包括循环GNNs、卷积GNNs、图形自编码器和图形注意力网络。同时还讨论了一条新兴的研究方向，即使用图形强化学习来评估和改进图形质量。

    As large-scale graphs become more widespread, more and more computational challenges with extracting, processing, and interpreting large graph data are being exposed. It is therefore natural to search for ways to summarize these expansive graphs while preserving their key characteristics. In the past, most graph summarization techniques sought to capture the most important part of a graph statistically. However, today, the high dimensionality and complexity of modern graph data are making deep learning techniques more popular. Hence, this paper presents a comprehensive survey of progress in deep learning summarization techniques that rely on graph neural networks (GNNs). Our investigation includes a review of the current state-of-the-art approaches, including recurrent GNNs, convolutional GNNs, graph autoencoders, and graph attention networks. A new burgeoning line of research is also discussed where graph reinforcement learning is being used to evaluate and improve the quality of grap
    
[^124]: 基于图的建模框架用于追踪表面水体中水文污染物的传输

    A Graph-Based Modeling Framework for Tracing Hydrological Pollutant Transport in Surface Waters. (arXiv:2302.04991v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04991](http://arxiv.org/abs/2302.04991)

    本研究提出了一种名为 HydroGraphs 的基于图的建模框架，用于分析水文污染物的传输和命运。该框架可以根据开源数据构建，具有简化的水文系统表示，并且可以使用常见的图分析和数据可视化技术进行分析和可视化，能够帮助准确定位污染源和脆弱区域。

    

    人类污染对全球各地的社区和生态系统产生了影响。数据分析和建模工具在应对这个挑战中发挥了关键作用，因为它们可以帮助识别关键源，并在复杂的水文系统中追踪传输和量化影响。本文提出了一个称为“${\tt HydroGraphs}$”的图模型框架，用于理解水体、河流和流域中污染物的传输和命运。该框架使用简化的水文系统表示，可以基于开放源数据（国家水文数据集和流域边界数据集）构建，并可以使用常见的图分析和数据可视化技术进行分析和可视化。通过对上密西西比河流域的案例研究，我们展示了该框架发现污染源、理解运输途径和准确定位脆弱区域的潜力。

    Anthropogenic pollution of hydrological systems affects diverse communities and ecosystems around the world. Data analytics and modeling tools play a key role in fighting this challenge, as they can help identify key sources as well as trace transport and quantify impact within complex hydrological systems. Several tools exist for simulating and tracing pollutant transport throughout surface waters using detailed physical models; these tools are powerful, but can be computationally intensive, require significant amounts of data to be developed, and require expert knowledge for their use (ultimately limiting application scope). In this work, we present a graph modeling framework -which we call ${\tt HydroGraphs}$ -- for understanding pollutant transport and fate across waterbodies, rivers, and watersheds. This framework uses a simplified representation of hydrological systems that can be constructed based purely on open-source data (National Hydrography Dataset and Watershed Boundary 
    
[^125]: 多维个性化边缘模型在实现更公平高效联邦学习方面的应用

    Towards Fairer and More Efficient Federated Learning via Multidimensional Personalized Edge Models. (arXiv:2302.04464v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04464](http://arxiv.org/abs/2302.04464)

    本研究提出了一种定制化联邦学习系统，通过从多个维度个性化边缘模型来消除联邦学习异质性，能够在模型准确度、效率和公平性方面显著提高性能。

    

    联邦学习是一种新兴技术，通过保护隐私训练大规模地理分布式边缘数据。然而，由于边缘的异质性导致联邦学习在公平性和计算效率方面存在固有的挑战，因此通常会导致最近一流解决方案的子优性能。本文提出了一种定制联邦学习系统（CFL）来消除多维度联邦学习的异质性。具体地，CFL 为每个客户端量身定制个性化模型，这些模型由在线训练的模型搜索助手和新型聚合算法共同引导。广泛的实验表明，CFL 在联邦学习训练和边缘推理方面具有全栈优势，并显著提高了模型准确度（在非异质环境下高达 7.2%，在异质环境下高达 21.8%）、效率和公平性。

    Federated learning (FL) is an emerging technique that trains massive and geographically distributed edge data while maintaining privacy. However, FL has inherent challenges in terms of fairness and computational efficiency due to the rising heterogeneity of edges, and thus usually results in sub-optimal performance in recent state-of-the-art (SOTA) solutions. In this paper, we propose a Customized Federated Learning (CFL) system to eliminate FL heterogeneity from multiple dimensions. Specifically, CFL tailors personalized models from the specially designed global model for each client jointly guided by an online trained model-search helper and a novel aggregation algorithm. Extensive experiments demonstrate that CFL has full-stack advantages for both FL training and edge reasoning and significantly improves the SOTA performance w.r.t. model accuracy (up to 7.2% in the non-heterogeneous environment and up to 21.8% in the heterogeneous environment), efficiency, and FL fairness.
    
[^126]: 基于效用的扰动梯度下降：一种连续学习优化器。

    Utility-based Perturbed Gradient Descent: An Optimizer for Continual Learning. (arXiv:2302.03281v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03281](http://arxiv.org/abs/2302.03281)

    本文提出了一种在线学习算法——基于效用的扰动梯度下降（UPGD），该算法可保护有用的权重或特征，并基于它们的效用扰动不太有用的权重或特征。实验证明，UPGD有助于减少遗忘和保持可塑性，在连续学习中大有用处。

    

    现代表示学习方法在面对非稳态问题时往往难以快速适应，因为它们遭受灾难性遗忘和衰减的可塑性。这些问题阻碍了学习者的快速适应，因为他们可能会遗忘有用的特征或难以学习新的特征。因此，这些方法在连续学习中变得无效。本文提出了一种在线学习算法——基于效用的扰动梯度下降（UPGD），这种算法非常适合连续学习代理。UPGD保护有用的权重或特征不被遗忘，并基于它们的效用扰动不太有用的权重或特征。我们的实证结果表明，UPGD有助于减少遗忘和保持可塑性，使现代表示学习方法在连续学习中有效地工作。

    Modern representation learning methods often struggle to adapt quickly under non-stationarity because they suffer from catastrophic forgetting and decaying plasticity. Such problems prevent learners from fast adaptation since they may forget useful features or have difficulty learning new ones. Hence, these methods are rendered ineffective for continual learning. This paper proposes Utility-based Perturbed Gradient Descent (UPGD), an online learning algorithm well-suited for continual learning agents. UPGD protects useful weights or features from forgetting and perturbs less useful ones based on their utilities. Our empirical results show that UPGD helps reduce forgetting and maintain plasticity, enabling modern representation learning methods to work effectively in continual learning.
    
[^127]: 强化学习中的尖锐方差相关界限：随机和确定性环境的最佳结合

    Sharp Variance-Dependent Bounds in Reinforcement Learning: Best of Both Worlds in Stochastic and Deterministic Environments. (arXiv:2301.13446v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13446](http://arxiv.org/abs/2301.13446)

    本研究将马尔可夫决策过程的方差相关遗憾界限应用到强化学习中，提出了两个新的环境规范来表征环境的方差属性，并设计出基于模型和无模型的算法，对于随机和确定性环境同时极小极大最优的界限是第一次被证明出来的。

    

    本文研究马尔可夫决策过程（MDPs）的方差相关遗憾界限。具有方差相关遗憾保证的算法可以自动利用具有低方差（例如，在确定性MDP上享有常量遗憾）的环境。现有算法要么独立于方差要么次优。我们首先提出两个新的环境规范来表征环境的细粒度方差属性。对于基于模型的方法，我们设计了MVP算法(Zhang等，2021a)的变种，并使用新的分析技术展示了该算法相对于我们提出的规范享有方差相关的界限。特别地，这一界限对于随机和确定性MDP同时是极小极大最优的，这是其种类中的第一个结果。我们进一步通过设计一种参考函数的算法以及一个新的带有上限加倍参考更新进度表的策略启动了关于具有方差相关遗憾界限的无模型算法的研究。最后，我们还提供了一些启示。

    We study variance-dependent regret bounds for Markov decision processes (MDPs). Algorithms with variance-dependent regret guarantees can automatically exploit environments with low variance (e.g., enjoying constant regret on deterministic MDPs). The existing algorithms are either variance-independent or suboptimal. We first propose two new environment norms to characterize the fine-grained variance properties of the environment. For model-based methods, we design a variant of the MVP algorithm (Zhang et al., 2021a) and use new analysis techniques show to this algorithm enjoys variance-dependent bounds with respect to our proposed norms. In particular, this bound is simultaneously minimax optimal for both stochastic and deterministic MDPs, the first result of its kind. We further initiate the study on model-free algorithms with variance-dependent regret bounds by designing a reference-function-based algorithm with a novel capped-doubling reference update schedule. Lastly, we also provid
    
[^128]: 将循环强化学习纳入模型预测控制中，实现自主驾驶中的自适应控制

    Incorporating Recurrent Reinforcement Learning into Model Predictive Control for Adaptive Control in Autonomous Driving. (arXiv:2301.13313v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13313](http://arxiv.org/abs/2301.13313)

    本文提出了一种基于循环强化学习和部分可观察的马尔可夫决策过程的自适应控制算法 $\textit{MPC-RRL}$，在CARLA模拟器中得到有效验证。

    

    模型预测控制（MPC）作为一种强大的控制技术，在自主驾驶任务中引起了极大的关注。 MPC控制器的成功强烈依赖于准确的内部动力学模型。 然而，通常通过系统识别学习的静态参数在真实场景中往往无法适应内部和外部干扰。 本文首先将问题重新表述为部分可观察的马尔可夫决策过程（POMDP），将不确定性吸收到观察中并将马尔可夫性质维护到隐藏状态中; 其次，通过循环强化学习（RRL）学习递归策略，持续适应动力学模型的参数以实现最优和自适应控制; 最后，在CARLA模拟器中对所提出的算法（称为 $\textit{MPC-RRL}$）进行评估，并在广泛的扰动范围内实现鲁棒行为。

    Model Predictive Control (MPC) is attracting tremendous attention in the autonomous driving task as a powerful control technique. The success of an MPC controller strongly depends on an accurate internal dynamics model. However, the static parameters, usually learned by system identification, often fail to adapt to both internal and external perturbations in real-world scenarios. In this paper, we firstly (1) reformulate the problem as a Partially Observed Markov Decision Process (POMDP) that absorbs the uncertainties into observations and maintains Markov property into hidden states; and (2) learn a recurrent policy continually adapting the parameters of the dynamics model via Recurrent Reinforcement Learning (RRL) for optimal and adaptive control; and (3) finally evaluate the proposed algorithm (referred as $\textit{MPC-RRL}$) in CARLA simulator and leading to robust behaviours under a wide range of perturbations.
    
[^129]: Ensemble和Bayesian稀疏模型发现的不确定性估计的收敛性研究

    Convergence of uncertainty estimates in Ensemble and Bayesian sparse model discovery. (arXiv:2301.12649v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12649](http://arxiv.org/abs/2301.12649)

    本文探讨了集合稀疏模型发现方法中的自助法序列阈值最小二乘估计器，证明了其具有指数收敛速率的误差率和可证明正确的变量选择过程，在不确定性估计方面具有高效性。

    

    稀疏模型识别能够从数据中发现非线性动力学系统。然而，在低数据和高噪声的限制下，控制稀疏模型识别中的错误发现具有挑战性。本文通过理论研究集合稀疏模型发现方法，展示了在准确性和鲁棒性方面的实证成功。具体而言，我们分析了基于自助法的顺序阈值最小二乘估计器。我们展示了这种基于自助法的集成技术可以执行可证明正确的变量选择过程，并具有指数收敛速率的误差率。此外，我们展示了集合稀疏模型发现方法可以比使用昂贵的基于MCMC贝叶斯不确定性量化方法更高效地执行不确定性估计。我们在各种合成稀疏线性回归问题的数值实验中展示了收敛性和与不确定性量化的关系。

    Sparse model identification enables nonlinear dynamical system discovery from data. However, the control of false discoveries for sparse model identification is challenging, especially in the low-data and high-noise limit. In this paper, we perform a theoretical study on ensemble sparse model discovery, which shows empirical success in terms of accuracy and robustness to noise. In particular, we analyse the bootstrapping-based sequential thresholding least-squares estimator. We show that this bootstrapping-based ensembling technique can perform a provably correct variable selection procedure with an exponential convergence rate of the error rate. In addition, we show that the ensemble sparse model discovery method can perform computationally efficient uncertainty estimation, compared to expensive Bayesian uncertainty quantification methods via MCMC. We demonstrate the convergence properties and connection to uncertainty quantification in various numerical studies on synthetic sparse li
    
[^130]: 关于深度网络和双重下降的利普希茨常数

    On the Lipschitz Constant of Deep Networks and Double Descent. (arXiv:2301.12309v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12309](http://arxiv.org/abs/2301.12309)

    本文通过实验研究发现，深度网络的利普希茨常数趋势与测试误差密切相关，通过建立参数空间和输入空间梯度之间的联系，确定了损失函数曲率和距离初始化参数的距离对于深度网络的优化和模型函数复杂度限制是关键因素，该研究对隐式正则化和网络的有效模型复杂度提供了新的见解。

    

    目前关于深度网络泛化误差的界限都是基于输入变量的平滑或有界依赖性，没有研究探究实践中控制这些因素的机制。本文对经历双重衰减的深度网络的实验利普希茨常数进行了广泛的实验研究，并强调了非单调的趋势，与测试误差密切相关。通过建立随机梯度下降的参数空间和输入空间梯度之间的联系，我们分离出两个重要因素，即损失函数曲率和距离初始化参数的距离，分别控制关键点周围的优化动态，并限制模型函数的复杂度，即使在训练数据之外。我们的研究揭示了超参数化的隐式正则化和实践中网络的有效模型复杂度的新见解。

    Existing bounds on the generalization error of deep networks assume some form of smooth or bounded dependence on the input variable, falling short of investigating the mechanisms controlling such factors in practice. In this work, we present an extensive experimental study of the empirical Lipschitz constant of deep networks undergoing double descent, and highlight non-monotonic trends strongly correlating with the test error. Building a connection between parameter-space and input-space gradients for SGD around a critical point, we isolate two important factors -- namely loss landscape curvature and distance of parameters from initialization -- respectively controlling optimization dynamics around a critical point and bounding model function complexity, even beyond the training data. Our study presents novels insights on implicit regularization via overparameterization, and effective model complexity for networks trained in practice.
    
[^131]: 基于语言引导的世界模型的具身决策制定

    Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling. (arXiv:2301.12050v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12050](http://arxiv.org/abs/2301.12050)

    本论文研究使用少量的大型语言模型来提高强化学习代理的样本效率。通过假设抽象世界模型并通过代理的世界经验进行验证，可以进行规划和探索。这种方法不仅可提高样本效率一个数量级，而且还具有稳健性。

    

    强化学习代理通常在没有先前的世界知识的情况下进行学习。然而，如果初始化高层子目标和子目标之间的转换知识，强化学习代理可以利用此抽象世界模型（AWM）进行规划和探索。我们提出使用少量样本的大型语言模型（LLM）来假设AWM，通过世界经验进行验证，以提高强化学习代理的样本效率。我们的DECKARD代理将LLM引导的探索应用于Minecraft中的物品制作，分为两个阶段：（1）梦想阶段，代理使用LLM将任务分解为一系列子目标，即假设的AWM；（2）唤醒阶段，代理为每个子目标学习模块化策略并验证或纠正假设的AWM。我们通过LLMs假设AWM，然后根据代理经验验证AWM的方法不仅可以将样本效率提高一个数量级，而且还具有稳健性。

    Reinforcement learning (RL) agents typically learn tabula rasa, without prior knowledge of the world. However, if initialized with knowledge of high-level subgoals and transitions between subgoals, RL agents could utilize this Abstract World Model (AWM) for planning and exploration. We propose using few-shot large language models (LLMs) to hypothesize an AWM, that will be verified through world experience, to improve sample efficiency of RL agents. Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft in two phases: (1) the Dream phase where the agent uses an LLM to decompose a task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase where the agent learns a modular policy for each subgoal and verifies or corrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and then verifying the AWM based on agent experience not only increases sample efficiency over contemporary methods by an order of magnitude but is also robust to
    
[^132]: 等变均衡逼近器有益吗？

    Are Equivariant Equilibrium Approximators Beneficial?. (arXiv:2301.11481v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2301.11481](http://arxiv.org/abs/2301.11481)

    本文从理论上探讨了等变均衡逼近器的优缺点，它在一些情况下比普通均衡逼近器有更好的泛化性能，可以实现更好的逼近效果，同时它也存在均衡选择和社会福利方面的缺陷。

    

    最近，通过使用神经网络训练函数逼近纳什均衡（NE）、相关均衡（CE）和粗略相关均衡（CCE），取得了显著的进展。此外，在正态博弈中设计这种均衡逼近器通常采用等变体系结构。本文从理论上刻画了等变均衡逼近器的优缺点，并得出以下结论：在一些情况下等变均衡逼近器比普通均衡逼近器具有更好的泛化性能，并且当收益分布是置换不变的时，等变均衡逼近器可以实现更好的逼近效果；然而，等变均衡逼近器在均衡选择和社会福利方面存在缺陷。这些结果有助于理解等变性在均衡逼近器中的作用。

    Recently, remarkable progress has been made by approximating Nash equilibrium (NE), correlated equilibrium (CE), and coarse correlated equilibrium (CCE) through function approximation that trains a neural network to predict equilibria from game representations. Furthermore, equivariant architectures are widely adopted in designing such equilibrium approximators in normal-form games. In this paper, we theoretically characterize benefits and limitations of equivariant equilibrium approximators. For the benefits, we show that they enjoy better generalizability than general ones and can achieve better approximations when the payoff distribution is permutation-invariant. For the limitations, we discuss their drawbacks in terms of equilibrium selection and social welfare. Together, our results help to understand the role of equivariance in equilibrium approximators.
    
[^133]: 从RGB图像中恢复机器人关节角度的距离几何方法

    A Distance-Geometric Method for Recovering Robot Joint Angles From an RGB Image. (arXiv:2301.02051v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2301.02051](http://arxiv.org/abs/2301.02051)

    本文提出了一种仅使用机器人当前配置的单个RGB图像就可以恢复机器人操纵器关节角度的方法，该方法利用机器人的运动学模型并训练浅层神经网络，可在缺少本体感知时恢复系统功能。

    

    在人类干预很难或不可能的领域（例如水下，太空或危险环境）操作的自主操纵系统需要高度强健的感知和通信故障。关键是，运动规划和控制算法需要提供关节编码器提供的准确关节角度数据流，否则可能会导致功能丧失。本文介绍了一种新方法，仅使用机器人当前配置的单个RGB图像就可以检索机器人操纵器的关节角度，为恢复系统功能开辟了一条途径，这时常用的本体感知无法使用。我们的方法基于配置空间的距离几何表示，利用机器人的运动学模型，旨在训练一个浅层神经网络，用于执行与检测到的结构关键点相关的距离的2D到3D回归。

    Autonomous manipulation systems operating in domains where human intervention is difficult or impossible (e.g., underwater, extraterrestrial or hazardous environments) require a high degree of robustness to sensing and communication failures. Crucially, motion planning and control algorithms require a stream of accurate joint angle data provided by joint encoders, the failure of which may result in an unrecoverable loss of functionality. In this paper, we present a novel method for retrieving the joint angles of a robot manipulator using only a single RGB image of its current configuration, opening up an avenue for recovering system functionality when conventional proprioceptive sensing is unavailable. Our approach, based on a distance-geometric representation of the configuration space, exploits the knowledge of a robot's kinematic model with the goal of training a shallow neural network that performs a 2D-to-3D regression of distances associated with detected structural keypoints. It
    
[^134]: 深度R编程

    Deep R Programming. (arXiv:2301.01188v2 [cs.PL] UPDATED)

    [http://arxiv.org/abs/2301.01188](http://arxiv.org/abs/2301.01188)

    该课程介绍了流行的数据科学语言R，并旨在培养学生、从业者和研究者成为独立的R语言用户。

    

    深度R编程是一门全面的课程，重点介绍数据科学中最流行的语言之一——R语言（统计计算、数据可视化、机器学习、数据清洗和分析）。它深入介绍了R语言的基础知识，旨在培养有抱负的学生、从业者和研究者，让他们成为独立使用这个强大环境的用户。这个教材是一个非盈利项目，它的在线和PDF版本可以在 <https://deepr.gagolewski.com/> 免费获取。希望这个早期草案发放出来后能对读者有所帮助。

    Deep R Programming is a comprehensive course on one of the most popular languages in data science (statistical computing, graphics, machine learning, data wrangling and analytics). It introduces the base language in-depth and is aimed at ambitious students, practitioners, and researchers who would like to become independent users of this powerful environment. This textbook is a non-profit project. Its online and PDF versions are freely available at <https://deepr.gagolewski.com/>. This early draft is distributed in the hope that it will be useful.
    
[^135]: 具有跳过连接的稀疏神经网络用于铝电解槽识别

    Sparse neural networks with skip-connections for identification of aluminum electrolysis cell. (arXiv:2301.00582v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2301.00582](http://arxiv.org/abs/2301.00582)

    本文研究了将串联跳过连接和稀疏正则化相结合用于铝电解槽识别的稀疏神经网络，能够提高长期预测的开环稳定性和准确性，解决有限训练数据下标准神经网络难以提供稳定预测的问题。

    

    神经网络因其能够直接从数据中捕获复杂的输入-输出关系的模型能力而在非线性系统识别中迅速引起关注。然而，虽然该方法具有很大的灵活性，但在此背景下，仍然存在有关这些模型安全性的担忧，以及需要大量潜在昂贵的数据。铝电解是一种高度非线性的生产过程，大多数数据必须手动采样，使采样过程昂贵且频率低。在状态变量少量测量的情况下，长期预测的准确性和开环稳定性变得非常重要。标准神经网络在有限的训练数据下很难提供稳定的长期预测。在这项工作中，我们研究了将串联跳过连接和促进稀疏的$\ell _{1}$正则化相结合对短期、中期和长期预测的开环稳定性和准确性的影响。

    Neural networks are rapidly gaining interest in nonlinear system identification due to the model's ability to capture complex input-output relations directly from data. However, despite the flexibility of the approach, there are still concerns about the safety of these models in this context, as well as the need for large amounts of potentially expensive data. Aluminum electrolysis is a highly nonlinear production process, and most of the data must be sampled manually, making the sampling process expensive and infrequent. In the case of infrequent measurements of state variables, the accuracy and open-loop stability of the long-term predictions become highly important. Standard neural networks struggle to provide stable long-term predictions with limited training data. In this work, we investigate the effect of combining concatenated skip-connections and the sparsity-promoting $\ell_1$ regularization on the open-loop stability and accuracy of forecasts with short, medium, and long pred
    
[^136]: RFold：基于解耦优化方法的RNA二级结构预测

    RFold: RNA Secondary Structure Prediction with Decoupled Optimization. (arXiv:2212.14041v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2212.14041](http://arxiv.org/abs/2212.14041)

    所提出的RFold方法采用解耦优化过程和注意力机制进行简单又有效的RNA二级结构预测，具有较高的准确性和速度。

    

    核糖核酸（RNA）的二级结构比三级结构更稳定和更易于在细胞中访问，因此对于功能预测至关重要。尽管深度学习在这个领域中显示出了很好的结果，但当前的方法存在泛化性差和复杂性高的问题。在这项工作中，我们提出了一种简单而有效的RNA二级结构预测方法RFold。RFold引入了一种解耦优化的过程，将传统的约束满足问题分解为逐行和逐列优化，简化了求解过程，同时保证了输出的有效性。此外，RFold采用注意力地图作为信息表示，而不是设计手工特征。广泛的实验表明，RFold具有竞争性能，并且比现有最先进的方法具有约8倍的推理效率。代码和Colab演示可在\href{this http URL}{this http UR}上找到。

    The secondary structure of ribonucleic acid (RNA) is more stable and accessible in the cell than its tertiary structure, making it essential for functional prediction. Although deep learning has shown promising results in this field, current methods suffer from poor generalization and high complexity. In this work, we present RFold, a simple yet effective RNA secondary structure prediction in an end-to-end manner. RFold introduces a decoupled optimization process that decomposes the vanilla constraint satisfaction problem into row-wise and column-wise optimization, simplifying the solving process while guaranteeing the validity of the output. Moreover, RFold adopts attention maps as informative representations instead of designing hand-crafted features. Extensive experiments demonstrate that RFold achieves competitive performance and about eight times faster inference efficiency than the state-of-the-art method. The code and Colab demo are available in \href{this http URL}{this http UR
    
[^137]: 推荐系统的本地策略改进

    Local Policy Improvement for Recommender Systems. (arXiv:2212.11431v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.11431](http://arxiv.org/abs/2212.11431)

    该论文介绍了一种针对推荐系统的本地策略改进方法，不需要现场校正，易于从数据中估计，适用于以前的策略质量较高但数量较少的情况。

    

    推荐系统基于用户过去的互动行为而预测他们可能会与哪些项目交互。解决该问题的常用方法是通过监督学习，但最近的进展转向了基于奖励（例如用户参与度）的策略优化。后者面临的挑战之一是策略不匹配：我们只能基于以前部署策略收集到的数据来训练新的策略。传统的方法是通过重要性采样校正来解决这个问题，但这种方法存在实际限制。我们建议一种不需要现场校正的本地策略改进方法。我们的方法计算和优化目标策略预期奖励的下限，这易于从数据中估计并且不涉及密度比（例如在重要性采样校正中出现的比率）。这种本地策略改进范例非常适用于推荐系统，因为以前的策略通常质量较高，策略的数量也很少。

    Recommender systems predict what items a user will interact with next, based on their past interactions. The problem is often approached through supervised learning, but recent advancements have shifted towards policy optimization of rewards (e.g., user engagement). One challenge with the latter is policy mismatch: we are only able to train a new policy given data collected from a previously-deployed policy. The conventional way to address this problem is through importance sampling correction, but this comes with practical limitations. We suggest an alternative approach of local policy improvement without off-policy correction. Our method computes and optimizes a lower bound of expected reward of the target policy, which is easy to estimate from data and does not involve density ratios (such as those appearing in importance sampling correction). This local policy improvement paradigm is ideal for recommender systems, as previous policies are typically of decent quality and policies ar
    
[^138]: 交互式概念瓶颈模型

    Interactive Concept Bottleneck Models. (arXiv:2212.07430v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.07430](http://arxiv.org/abs/2212.07430)

    该论文提出了交互式概念瓶颈模型(CBMs)，使得模型可以向人类协作者查询某些概念的标签，从而提高最终预测结果的准确性。

    

    概念瓶颈模型(CBMs)是可解释的神经网络，首先针对与预测任务相关的人可解释的概念预测标签，然后基于这些预测结果预测最终的标签。我们将CBMs扩展到交互式预测设置中，使得模型可以向人类协作者查询某些概念的标签。我们开发了一种交互策略，在预测时选择要请求标签的概念，以最大限度地提高最终预测的准确性。我们展示了一个简单的策略，结合了概念预测的不确定性和概念对最终预测结果的影响，在Caltech-UCSD Birds、CheXpert和OAI数据集上实现了强大的性能，并且优于文献中提出的静态方法和主动特征采集方法。我们展示了交互式CBM只需进行5次交互，就能在竞争基准上实现5-10%的准确率提高。

    Concept bottleneck models (CBMs) are interpretable neural networks that first predict labels for human-interpretable concepts relevant to the prediction task, and then predict the final label based on the concept label predictions. We extend CBMs to interactive prediction settings where the model can query a human collaborator for the label to some concepts. We develop an interaction policy that, at prediction time, chooses which concepts to request a label for so as to maximally improve the final prediction. We demonstrate that a simple policy combining concept prediction uncertainty and influence of the concept on the final prediction achieves strong performance and outperforms static approaches as well as active feature acquisition methods proposed in the literature. We show that the interactive CBM can achieve accuracy gains of 5-10% with only 5 interactions over competitive baselines on the Caltech-UCSD Birds, CheXpert and OAI datasets.
    
[^139]: 视觉查询调整：为了有效利用中间表示进行参数和内存高效的迁移学习

    Visual Query Tuning: Towards Effective Usage of Intermediate Representations for Parameter and Memory Efficient Transfer Learning. (arXiv:2212.03220v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.03220](http://arxiv.org/abs/2212.03220)

    本文介绍了一种名为视觉查询调整（VQT）的简单而有效的方法，用于聚合Vision Transformers的中间特征。VQT在训练中具有内存效率，相比于许多其他 fine-tuning 方法，不需要对整个骨干进行反向传播。该方法在几个基准测试中优于最先进的微调方法。

    

    先前训练模型的中间特征已被证明对于在下游任务中进行准确预测非常有用，即使模型骨干保持冻结。关键挑战在于如何利用这些中间特征。我们提出了一种简单而有效的方法-视觉查询调整（VQT），用于聚合Vision Transformers的中间特征。通过为每个层引入少量可学习的“查询”令牌，VQT利用Transformer的内部运行机制来“总结”每个层的丰富中间特征，然后可以用于训练下游任务的预测头。由于VQT保持了中间特征的完整性并仅学习了如何组合它们，因此与许多其他参数高效的微调方法相比，VQT在训练中具有内存效率，后者需要学习如何适应特征并需要对整个骨干进行反向传播。这也表明了VQT与这些方法在翻译学习流程中的互补作用。我们在几个基准测试中展示了VQT的有效性，包括对象检测、实例分割和密集预测任务，并展示了它与最先进的微调方法相比的优势。

    Intermediate features of a pre-trained model have been shown informative for making accurate predictions on downstream tasks, even if the model backbone is kept frozen. The key challenge is how to utilize these intermediate features given their gigantic amount. We propose visual query tuning (VQT), a simple yet effective approach to aggregate intermediate features of Vision Transformers. Through introducing a handful of learnable ``query'' tokens to each layer, VQT leverages the inner workings of Transformers to ``summarize'' rich intermediate features of each layer, which can then be used to train the prediction heads of downstream tasks. As VQT keeps the intermediate features intact and only learns to combine them, it enjoys memory efficiency in training, compared to many other parameter-efficient fine-tuning approaches that learn to adapt features and need back-propagation through the entire backbone. This also suggests the complementary role between VQT and those approaches in tran
    
[^140]: 贝叶斯物理信息神经网络在野火建模中的应用

    Bayesian Physics Informed Neural Networks for Data Assimilation and Spatio-Temporal Modelling of Wildfires. (arXiv:2212.00970v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.00970](http://arxiv.org/abs/2212.00970)

    本研究使用物理信息神经网络（PINN）解决野火前沿建模问题，在优化模型中改进了成本函数以提高在极端环境下的时间连续性，并开发了数据同化的方法将预测值与前沿观测值相结合。最终，我们开发了一个贝叶斯PINN（B-PINN）以提供不确定性估计和置信区间。

    

    本文研究了物理信息神经网络（PINN）在野火前沿建模中的应用，利用该网络解决了水平集方程，建立了模拟野火前沿随时间推移演化的PINN。我们提出了一种改进优化成本函数以提高在极端条件下的时间连续性的方法，并开发了数据同化的方法将预测值与前沿观测值相结合。最后，我们开发了一个贝叶斯PINN（B-PINN）以提供不确定性估计和置信区间。

    We apply the Physics Informed Neural Network (PINN) to the problem of wildfire fire-front modelling. We use the PINN to solve the level-set equation, which is a partial differential equation that models a fire-front through the zero-level-set of a level-set function. The result is a PINN that simulates a fire-front as it propagates through the spatio-temporal domain. We show that popular optimisation cost functions used in the literature can result in PINNs that fail to maintain temporal continuity in modelled fire-fronts when there are extreme changes in exogenous forcing variables such as wind direction. We thus propose novel additions to the optimisation cost function that improves temporal continuity under these extreme changes. Furthermore, we develop an approach to perform data assimilation within the PINN such that the PINN predictions are drawn towards observations of the fire-front. Finally, we incorporate our novel approaches into a Bayesian PINN (B-PINN) to provide uncertain
    
[^141]: 对约束多智能体强化学习的原始-对偶算法的解释

    Interpreting Primal-Dual Algorithms for Constrained Multiagent Reinforcement Learning. (arXiv:2211.16069v3 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2211.16069](http://arxiv.org/abs/2211.16069)

    本文研究了对约束多智能体强化学习的原始-对偶算法，证明了使用约束函数作为惩罚的标准做法可以导致较弱的安全性概念，通过对惩罚项进行简单修正，能够强制约束并提高性能。

    

    约束多智能体强化学习(C-MARL)在实际系统(从能源系统到无人机群)中的应用日益重要。大多数C-MARL算法通过将惩罚函数添加到奖励中，使用原始-对偶方法来实现约束。本文研究这种惩罚项对MARL问题的结构影响。首先，我们证明了使用约束函数作为惩罚的标准做法可以导致一种较弱的安全性概念。然而，通过对惩罚项进行简单修正，我们可以强制执行有意义的概率约束。其次，我们量化了惩罚项对价值函数的影响，揭示了改进的价值估计过程。我们利用这些见解提出了一种约束多智能体优势演员评论家(C-MAA2C)算法。在简单的约束多智能体环境中的模拟实验证实了我们对原始-对偶方法的重新解释可以在MARL问题中导致更好的安全性和性能。

    Constrained multiagent reinforcement learning (C-MARL) is gaining importance as MARL algorithms find new applications in real-world systems ranging from energy systems to drone swarms. Most C-MARL algorithms use a primal-dual approach to enforce constraints through a penalty function added to the reward. In this paper, we study the structural effects of this penalty term on the MARL problem. First, we show that the standard practice of using the constraint function as the penalty leads to a weak notion of safety. However, by making simple modifications to the penalty term, we can enforce meaningful probabilistic (chance and conditional value at risk) constraints. Second, we quantify the effect of the penalty term on the value function, uncovering an improved value estimation procedure. We use these insights to propose a constrained multiagent advantage actor critic (C-MAA2C) algorithm. Simulations in a simple constrained multiagent environment affirm that our reinterpretation of the pr
    
[^142]: Crown-CAM：高空图像中树冠检测的可解释视觉解释方法

    Crown-CAM: Interpretable Visual Explanations for Tree Crown Detection in Aerial Images. (arXiv:2211.13126v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.13126](http://arxiv.org/abs/2211.13126)

    本文提出了一种名为Crown-CAM的可解释的类激活映射方法，用于解决高空图像中树冠检测的问题，该方法通过无监督选择激活映射、计算局部得分映射和非上下文背景抑制等步骤，既可以有效地提供树冠的精细定位，又可以量化生成的解释的准确性和不准确性。

    

    “黑匣子”模型的可视解释允许可解释人工智能领域的研究人员以人类可理解的方式解释模型的决策。本文提出了一种可解释的树冠检测类激活映射方法（Crown-CAM），它克服了以往方法的不准确的定位和计算复杂度问题，同时为高空图像中树冠检测这一具有挑战性和动态的问题提供可靠的可视化解释。它包括无监督选择激活映射，计算局部得分映射以及非上下文背景抑制，以有效地提供树冠的精细定位，适用于密林或者没有树冠的场景。另外，本文引入了两个基于IoU的指标，可以有效地量化生成的解释与图像中树冠或无树冠区域的准确性和不准确性。

    Visual explanation of ``black-box'' models allows researchers in explainable artificial intelligence (XAI) to interpret the model's decisions in a human-understandable manner. In this paper, we propose interpretable class activation mapping for tree crown detection (Crown-CAM) that overcomes inaccurate localization & computational complexity of previous methods while generating reliable visual explanations for the challenging and dynamic problem of tree crown detection in aerial images. It consists of an unsupervised selection of activation maps, computation of local score maps, and non-contextual background suppression to efficiently provide fine-grain localization of tree crowns in scenarios with dense forest trees or scenes without tree crowns. Additionally, two Intersection over Union (IoU)-based metrics are introduced to effectively quantify both the accuracy and inaccuracy of generated explanations with respect to regions with or even without tree crowns in the image. Empirical e
    
[^143]: 用于三维分子图的几何完备感知器网络

    Geometry-Complete Perceptron Networks for 3D Molecular Graphs. (arXiv:2211.02504v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.02504](http://arxiv.org/abs/2211.02504)

    本研究引入了一种新的几何完备的图神经网络 GCPNet，用于3D分子图的表示学习，并在多个几何任务上展示了其出色的预测性能。其中最佳表现是在蛋白质-配体结合亲和力预测上得到了比当前最先进方法高出5%以上的相关系数。

    

    几何深度学习对于创新和强大的图形神经网络架构的发展产生了重大影响。来自计算机视觉和计算生物学等学科的领域，在这些方法学的推动下取得了显著的收益，从而在科学领域如蛋白质结构预测和设计中实现了突破。在本研究中，我们引入了GCPNet，这是一种新的几何完备、SE(3)-等变的图神经网络，专门用于3D分子图表示学习。四个不同的几何任务的严密实验证明了GCPNet的预测能力，包括：（1）蛋白质-配体结合亲和力的相关系数为0.608，比目前最先进的方法高出5%以上；（2）蛋白质结构排名在目标本地和数据集全局之间具有统计显著的相关性，分别为0.616和0.871；（3）Newtownian多体系统的建模平均成绩达到了

    The field of geometric deep learning has had a profound impact on the development of innovative and powerful graph neural network architectures. Disciplines such as computer vision and computational biology have benefited significantly from such methodological advances, which has led to breakthroughs in scientific domains such as protein structure prediction and design. In this work, we introduce GCPNet, a new geometry-complete, SE(3)-equivariant graph neural network designed for 3D molecular graph representation learning. Rigorous experiments across four distinct geometric tasks demonstrate that GCPNet's predictions (1) for protein-ligand binding affinity achieve a statistically significant correlation of 0.608, more than 5% greater than current state-of-the-art methods; (2) for protein structure ranking achieve statistically significant target-local and dataset-global correlations of 0.616 and 0.871, respectively; (3) for Newtownian many-body systems modeling achieve a task-averaged 
    
[^144]: 化学预训练模型的系统调查

    A Systematic Survey of Chemical Pre-trained Models. (arXiv:2210.16484v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.16484](http://arxiv.org/abs/2210.16484)

    本文是对化学预训练模型领域的第一次系统回顾，提出了从头训练分子表示模型的局限性，总结了最新的进展和几个关键角度，包括分子描述符和编码器架构。

    

    深度学习在学习分子表示方面取得了显著的成功，这对于各种生化应用，从属性预测到药物设计，都至关重要。然而，从头开始训练深度神经网络通常需要大量标记分子，在现实世界中获取这些标记通常很昂贵。为了缓解这个问题，已经付出了巨大的努力在使用大规模无标记分子数据库预训练深度神经网络，并针对特定的下游任务进行微调。尽管物有所值，但这个快速增长的领域缺乏系统的回顾。在本文中，我们提出了第一个总结CPMs进展的综述。我们首先强调从头训练分子表示模型的局限性，以推动CPM的研究。接下来，我们从几个关键角度系统地回顾了这个主题的最新进展，包括分子描述符，编码器架构。

    Deep learning has achieved remarkable success in learning representations for molecules, which is crucial for various biochemical applications, ranging from property prediction to drug design. However, training Deep Neural Networks (DNNs) from scratch often requires abundant labeled molecules, which are expensive to acquire in the real world. To alleviate this issue, tremendous efforts have been devoted to Molecular Pre-trained Models (CPMs), where DNNs are pre-trained using large-scale unlabeled molecular databases and then fine-tuned over specific downstream tasks. Despite the prosperity, there lacks a systematic review of this fast-growing field. In this paper, we present the first survey that summarizes the current progress of CPMs. We first highlight the limitations of training molecular representation models from scratch to motivate CPM studies. Next, we systematically review recent advances on this topic from several key perspectives, including molecular descriptors, encoder arc
    
[^145]: 超越校准：估计现代神经网络的分组损失

    Beyond calibration: estimating the grouping loss of modern neural networks. (arXiv:2210.16315v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.16315](http://arxiv.org/abs/2210.16315)

    本文提出了一个估计器来近似神经网络的分组损失，并表明现代神经网络在视觉和NLP中展示出显著的分组损失。

    

    确保分类器给出可靠的置信度分数是确保知情决策的关键。为此，最近的研究集中在误校准上，即模型分数的过度或不足置信。然而，校准还不够：即使准确率最高的完美校准分类器也可能具有与真实后验概率相去甚远的置信度分数，这是由于分组损失所造成的，即以相同置信度得分但真实后验概率不同的样本。适当的评分规则理论表明，在给定校准损失的情况下，表征单个错误的缺失部分是分组损失。虽然存在许多校准损失的估计器，但在标准设置中不存在分组损失的估计器。在本文中，我们提出了一个估计器来近似分组损失。我们展示了现代神经网络结构在视觉和NLP中表现出分组损失，特别是在分布偏移设置中，这突显了它的重要性。

    The ability to ensure that a classifier gives reliable confidence scores is essential to ensure informed decision-making. To this end, recent work has focused on miscalibration, i.e., the over or under confidence of model scores. Yet calibration is not enough: even a perfectly calibrated classifier with the best possible accuracy can have confidence scores that are far from the true posterior probabilities. This is due to the grouping loss, created by samples with the same confidence scores but different true posterior probabilities. Proper scoring rule theory shows that given the calibration loss, the missing piece to characterize individual errors is the grouping loss. While there are many estimators of the calibration loss, none exists for the grouping loss in standard settings. Here, we propose an estimator to approximate the grouping loss. We show that modern neural network architectures in vision and NLP exhibit grouping loss, notably in distribution shifts settings, which highli
    
[^146]: Occam学习

    Occam learning. (arXiv:2210.13179v2 [cond-mat.dis-nn] UPDATED)

    [http://arxiv.org/abs/2210.13179](http://arxiv.org/abs/2210.13179)

    本文讨论了一种具有固定隐藏层分布的概率神经网络模型，该模型选择简单、易解释，不需要过度参数化，同时训练有效。模型的隐藏单元为二元变量时具有以特征为基础的自然解释。作者认为隐藏变量的分布应该遵循最大关联度原则，并介绍了分层特征模型（HFM）作为满足这一原则并对特征空间进行中性先验组织的模型。

    

    我们讨论了一种无监督学习的概率神经网络模型，在这种模型中，隐藏层的分布是固定的。我们认为采用这种体系架构的机器学习具有许多令人满意的性质。例如，该模型可以选择为简单且易解释的模型，不需要过度参数化，而且在热力学意义下，训练更有效。当隐藏单元为二元变量时，这些模型具有以特征为基础的自然解释。我们表明，缺乏特征的状态对应于在特征方面最大程度的无知状态，并且，学习第一个特征取决于数据的非高斯统计属性。我们认为应该根据最大关联度原则选择隐藏变量的分布。我们介绍了分层特征模型（HFM）作为满足这一原则并对特征空间进行中性先验组织的模型。

    We discuss probabilistic neural network models for unsupervised learning where the distribution of the hidden layer is fixed. We argue that learning machines with this architecture enjoy a number of desirable properties. For example, the model can be chosen as a simple and interpretable one, it does not need to be over-parametrised and training is argued to be efficient in a thermodynamic sense. When hidden units are binary variables, these models have a natural interpretation in terms of features. We show that the featureless state corresponds to a state of maximal ignorance about the features and that learning the first feature depends on non-Gaussian statistical properties of the data. We suggest that the distribution of hidden variables should be chosen according to the principle of maximal relevance. We introduce the Hierarchical Feature Model (HFM) as an example of a model that satisfies this principle, and that encodes a neutral a priori organisation of the feature space. We pre
    
[^147]: 无视规划地推广横跨变量的强化学习

    Horizon-Free and Variance-Dependent Reinforcement Learning for Latent Markov Decision Processes. (arXiv:2210.11604v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.11604](http://arxiv.org/abs/2210.11604)

    本文研究了具有上下文后见性的 LMDP 强化学习遗憾最小化问题。通过设计一个新颖的模型基础算法框架，我们证明了一个与计划视野对数相关的 $\widetilde{O}\left(\sqrt{M \Gamma S A K}\right)$ 遗憾度上限，并对 alpha 向量的总方差进行分析。同时，我们提出了一个 $\Omega\left(\sqrt{M S A K}\right)$ 的遗憾度下限，它在 $\Gamma=2$ 时证明了我们的上界是最优的。

    

    本文研究了具有后见性上下文的潜在马尔可夫决策过程 (LMDPs) 强化学习 (RL) 的遗憾最小化问题。我们设计了一个新颖的基于模型的算法框架，可以通过模型乐观或值乐观求解器实例化。我们证明了一个关于遗憾度的较小量级为 $\widetilde{O}\left(\sqrt{M \Gamma S A K}\right)$ 的界限，其中 $M$ 是上下文数量，$S$ 是状态数量，$A$ 是动作数量，$K$ 是回合数量，而 $\Gamma \le S$ 是任何状态-动作对的最大转移次数。遗憾度只在规划视野中以对数形式缩放，所以 LMDP 的规划视野的第一个(几乎)无视界限就被产生了。我们的论证的关键是对 alpha 向量的总方差进行分析，该方差通过递归技术进行了仔细的限制。我们通过一个新的 $\Omega\left(\sqrt{M S A K}\right)$ 遗憾性下限补充了我们的正补结果，并证明了当 $\Gamma=2$ 时，我们的上界是极小化最优的。

    We study regret minimization for reinforcement learning (RL) in Latent Markov Decision Processes (LMDPs) with context in hindsight. We design a novel model-based algorithmic framework which can be instantiated with both a model-optimistic and a value-optimistic solver. We prove an $\widetilde{O}\left(\sqrt{M \Gamma S A K}\right)$ regret bound where $M$ is the number of contexts, $S$ is the number of states, $A$ is the number of actions, $K$ is the number of episodes, and $\Gamma \le S$ is the maximum transition degree of any state-action pair. The regret bound only scales logarithmically with the planning horizon, thus yielding the first (nearly) horizon-free regret bound for LMDP. Key in our proof is an analysis of the total variance of alpha vectors, which is carefully bounded by a recursion-based technique. We complement our positive result with a novel $\Omega\left(\sqrt{M S A K}\right)$ regret lower bound with $\Gamma = 2$, which shows our upper bound minimax optimal when $\Gamma$
    
[^148]: 紧凑集成用于高效的不确定性估计

    Packed-Ensembles for Efficient Uncertainty Estimation. (arXiv:2210.09184v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.09184](http://arxiv.org/abs/2210.09184)

    Packed-Ensembles是一种能够在标准神经网络内运行的轻量级结构化集合，它通过精心调节编码空间的维度来设计。该方法在不损失效果的情况下提高了训练和推理速度。

    

    深度集成是实现关键指标（如准确性、校准、不确定性估计和超出分布检测）卓越性能的突出方法。但是，现实系统的硬件限制限制了更小的集合和较低容量的网络，严重损害了它们的性能和属性。我们引入了一种称为Packed-Ensembles（PE）的策略，通过精心调节其编码空间的维度来设计和训练轻量级结构化集合。我们利用组卷积将集合并行化为单个共享骨干，并进行前向传递以提高训练和推理速度。PE旨在在标准神经网络的内存限制内运行。

    Deep Ensembles (DE) are a prominent approach for achieving excellent performance on key metrics such as accuracy, calibration, uncertainty estimation, and out-of-distribution detection. However, hardware limitations of real-world systems constrain to smaller ensembles and lower-capacity networks, significantly deteriorating their performance and properties. We introduce Packed-Ensembles (PE), a strategy to design and train lightweight structured ensembles by carefully modulating the dimension of their encoding space. We leverage grouped convolutions to parallelize the ensemble into a single shared backbone and forward pass to improve training and inference speeds. PE is designed to operate within the memory limits of a standard neural network. Our extensive research indicates that PE accurately preserves the properties of DE, such as diversity, and performs equally well in terms of accuracy, calibration, out-of-distribution detection, and robustness to distribution shift. We make our c
    
[^149]: 打破betaTCVAE中全相关性的魔咒

    Break The Spell Of Total Correlation In betaTCVAE. (arXiv:2210.08794v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.08794](http://arxiv.org/abs/2210.08794)

    本文提出一种新的迭代分解路径来打破betaTCVAE中的全相关性，从而使得VAE能够更加灵活地划分数据特征。实验结果表明模型容量和潜变量分组之间存在有趣的相关性。

    

    在缺乏人工标签的情况下，数据中的独立特征和依赖特征是混乱的。如何构建模型的归纳偏好，以灵活划分并有效地包含具有不同复杂度的特征，是无监督的分离表征学习的主要焦点。本文提出了一种新的总相关性的迭代分解路径，并从模型容量分配的角度解释了VAE的分离表征能力。新开发的目标函数将潜变量维度组合成联合分布，同时减轻组合中边缘分布的独立性约束，从而产生具有更易操作先验分布的潜变量。这种新颖的模型使得VAE能够调整参数容量，以灵活划分相关和独立的数据特征。各种数据集上的实验结果表明了模型容量与潜变量分组之间的有趣关联性。

    In the absence of artificial labels, the independent and dependent features in the data are cluttered. How to construct the inductive biases of the model to flexibly divide and effectively contain features with different complexity is the main focal point of unsupervised disentangled representation learning. This paper proposes a new iterative decomposition path of total correlation and explains the disentangled representation ability of VAE from the perspective of model capacity allocation. The newly developed objective function combines latent variable dimensions into joint distribution while relieving the independence constraints of marginal distributions in combination, leading to latent variables with a more manipulable prior distribution. The novel model enables VAE to adjust the parameter capacity to divide dependent and independent data features flexibly. Experimental results on various datasets show an interesting relevance between model capacity and the latent variable groupi
    
[^150]: 基于CBAM注意力机制的U-net钣金工程图像分割方法

    Segmentation method of U-net sheet metal engineering drawing based on CBAM attention mechanism. (arXiv:2209.14102v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.14102](http://arxiv.org/abs/2209.14102)

    本文提出了一种基于U-net和CBAM注意力机制的钣金工程图像分割方法，能够根据视觉信息自动分割特定图形单元，在自动切割方面具有高效率和准确性。

    

    在制造重型工业设备的过程中，需要进行焊接图中的特定图形单元重新绘制，然后才能切割相应的钣金零部件，这种方法效率低下。为此，本文提出了一种基于U-net的方法，用于焊接工程图的特定图形单元的分割和提取。该方法可以根据视觉信息自动分割特定图形单元，并根据分割结果自动切割出相应形状的钣金零件。这个过程比传统的人工辅助切割更有效率。在U-net网络中存在两个弱点，会导致分割性能降低：首先，对全局语义特征信息的关注不够，其次，浅层编码器特征和深层解码器特征之间存在大的尺度差异。基于CBAM（卷积块注意力模块）注意机制，本文提出了一种用于钣金工程图像分割的U-net方法，解决了这些弱点。所提出的方法提高了分割性能，并在实验中使用的数据集上实现了98.97％的准确率。

    In the manufacturing process of heavy industrial equipment, the specific unit in the welding diagram is first manually redrawn and then the corresponding sheet metal parts are cut, which is inefficient. To this end, this paper proposes a U-net-based method for the segmentation and extraction of specific units in welding engineering drawings. This method enables the cutting device to automatically segment specific graphic units according to visual information and automatically cut out sheet metal parts of corresponding shapes according to the segmentation results. This process is more efficient than traditional human-assisted cutting. Two weaknesses in the U-net network will lead to a decrease in segmentation performance: first, the focus on global semantic feature information is weak, and second, there is a large dimensional difference between shallow encoder features and deep decoder features. Based on the CBAM (Convolutional Block Attention Module) attention mechanism, this paper pro
    
[^151]: 材料工程中的人工智能：人工智能在材料工程中的应用综述

    Artificial Intelligence in Material Engineering: A review on applications of AI in Material Engineering. (arXiv:2209.11234v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.11234](http://arxiv.org/abs/2209.11234)

    本文综述了人工智能在材料工程中的最新进展，涉及材料加工、结构和材料性能研究、测量材料性能、新材料的创建和设计以及未来机遇等方面，基于机器学习的方法比传统方法更快、更准确，生成对抗网络有助于无机材料的化学成分预测和优化。

    

    随着人工智能技术的不断发展，人工智能在材料科学和工程中的作用越来越重要。高性能计算机的发展使得测试深度学习模型成为可能，这提供了一个机会来克服传统计算方法（如密度泛函理论）在性能预测上的局限性。基于机器学习的方法比基于密度泛函理论的方法更快，更准确。此外，生成对抗网络（GAN）有助于在不使用晶体结构信息的情况下生成无机材料的化学成分。这些发展对材料工程和研究产生了重大影响。本文综述了人工智能在材料工程中的最新进展。概括了人工智能在材料加工、结构和材料性能研究以及测量材料性能等关键领域中的应用。描述了人工智能在新材料的创建和设计中的应用，其中人工智能被用来优化和预测材料性质。最后，本文讨论了人工智能在材料工程中面临的挑战和未来机遇。

    The role of artificial intelligence (AI) in material science and engineering (MSE) is becoming increasingly important as AI technology advances. The development of high-performance computing has made it possible to test deep learning (DL) models with significant parameters, providing an opportunity to overcome the limitation of traditional computational methods, such as density functional theory (DFT), in property prediction. Machine learning (ML)-based methods are faster and more accurate than DFT-based methods. Furthermore, the generative adversarial networks (GANs) have facilitated the generation of chemical compositions of inorganic materials without using crystal structure information. These developments have significantly impacted material engineering (ME) and research. Some of the latest developments in AI in ME herein are reviewed. First, the development of AI in the critical areas of ME, such as in material processing, the study of structure and material property, and measurin
    
[^152]: 控制的统计学习理论：有限样本视角

    Statistical Learning Theory for Control: A Finite Sample Perspective. (arXiv:2209.05423v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2209.05423](http://arxiv.org/abs/2209.05423)

    本文概述了控制领域中最重要的统计学习理论中的最近进展，这些进展主要围绕在线性系统辨识和学习方面，基于现代高维统计学和学习理论的工具，为将机器学习工具融入控制领域的人提供了自包含演示。

    

    本教程综述了最近在统计学习理论中与控制和系统辨识相关的非渐近进展。尽管在所有控制领域都取得了实质性进展，但在线性系统辨识和线性二次调节器学习方面，该理论最为成熟，而这也是本文的重点。从理论角度讲，这些进展的大部分工作都在于借鉴现代高维统计学和学习理论的工具。虽然对于那些有兴趣将机器学习工具融入控制领域的控制理论家来说非常相关，但这些基础材料并不总是易于获取。为了解决这个问题，我们提供了相关素材的自包含演示，概述了所有关键思想和技术机械，为最近的结果打下了基础。我们还提出了一些未解决的问题和未来的方向。

    This tutorial survey provides an overview of recent non-asymptotic advances in statistical learning theory as relevant to control and system identification. While there has been substantial progress across all areas of control, the theory is most well-developed when it comes to linear system identification and learning for the linear quadratic regulator, which are the focus of this manuscript. From a theoretical perspective, much of the labor underlying these advances has been in adapting tools from modern high-dimensional statistics and learning theory. While highly relevant to control theorists interested in integrating tools from machine learning, the foundational material has not always been easily accessible. To remedy this, we provide a self-contained presentation of the relevant material, outlining all the key ideas and the technical machinery that underpin recent results. We also present a number of open problems and future directions.
    
[^153]: 无细胞Latent Go-Explore

    Cell-Free Latent Go-Explore. (arXiv:2208.14928v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.14928](http://arxiv.org/abs/2208.14928)

    本文提出了无细胞Latent Go-Explore方法用于强化学习探索，通过学习潜在表示泛化到任何环境，实验结果表明其表现优异。

    

    本文提出了一种名为Latent Go-Explore (LGE)的简单通用方法，基于Go-Explore范式探索强化学习。我们认为，Go-Explore方法可以通过利用学习到的潜在表示在没有领域知识和单元的情况下泛化到任何环境中。我们展示了LGE可以灵活地与任何学习潜在表示的策略相结合。实验结果表明，LGE比Go-Explore更加鲁棒，在多个难以探索的环境中（包括Montezuma的复仇）表现出优异的探索性能，超越了现有算法。

    In this paper, we introduce Latent Go-Explore (LGE), a simple and general approach based on the Go-Explore paradigm for exploration in reinforcement learning (RL). Go-Explore was initially introduced with a strong domain knowledge constraint for partitioning the state space into cells. However, in most real-world scenarios, drawing domain knowledge from raw observations is complex and tedious. If the cell partitioning is not informative enough, Go-Explore can completely fail to explore the environment. We argue that the Go-Explore approach can be generalized to any environment without domain knowledge and without cells by exploiting a learned latent representation. Thus, we show that LGE can be flexibly combined with any strategy for learning a latent representation. Our results indicate that LGE, although simpler than Go-Explore, is more robust and outperforms state-of-the-art algorithms in terms of pure exploration on multiple hard-exploration environments including Montezuma's Reven
    
[^154]: AI与AM的结合——通过Transformer网络提高近似匹配

    Combining AI and AM - Improving Approximate Matching through Transformer Networks. (arXiv:2208.11367v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2208.11367](http://arxiv.org/abs/2208.11367)

    该论文提出了一种基于Transformer模型的Deep Learning Approximate Matching算法，该算法结合AI与AM，能够在只有原始工件的碎片可用时，可靠高效地检测黑名单中与案件相关的数据结构。

    

    近似匹配（AM）是数字取证中用于确定数字工件相似度的概念。 AM的重要用例是在只有原始工件的碎片可用时，可靠高效地检测黑名单中与案件相关的数据结构。我们提出了一种基于自然语言处理领域的Transformer模型的改进匹配算法，称为Deep Learning Approximate Matching（DLAM），为人工智能的概念。

    Approximate matching (AM) is a concept in digital forensics to determine the similarity between digital artifacts. An important use case of AM is the reliable and efficient detection of case-relevant data structures on a blacklist, if only fragments of the original are available. For instance, if only a cluster of indexed malware is still present during the digital forensic investigation, the AM algorithm shall be able to assign the fragment to the blacklisted malware. However, traditional AM functions like TLSH and ssdeep fail to detect files based on their fragments if the presented piece is relatively small compared to the overall file size. A second well-known issue with traditional AM algorithms is the lack of scaling due to the ever-increasing lookup databases. We propose an improved matching algorithm based on transformer models from the field of natural language processing. We call our approach Deep Learning Approximate Matching (DLAM). As a concept from artificial intelligence
    
[^155]: T-RECX：具有早期退出的小型资源有效卷积神经网络。

    T-RECX: Tiny-Resource Efficient Convolutional neural networks with early-eXit. (arXiv:2207.06613v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.06613](http://arxiv.org/abs/2207.06613)

    本文介绍了一种优化小型CNN模型的方法——T-RECX，通过添加早期退出中间分类器可以节省大量的时间和节省模型容量。本文的方法在CIFAR-10、CIFAR-100和ImageNet数据集上进行了实验，并与最先进的技术进行了比较，结果表明，具有早期退出功能的tiny-RECX模型可以在较小的模型和更快的推断时间内达到可比较的精度。

    

    由于机器学习和物联网技术的突破，将机器学习（ML）部署在毫瓦级边缘设备（tinyML）上正在变得越来越流行。大多数tinyML研究聚焦于模型压缩技术，为了适应KB级tiny-edge设备而以精度（和模型容量）为代价获得紧凑的模型。在本文中，我们展示了如何通过添加早期退出中间分类器来增强这样的模型。如果中间分类器对其预测具有足够的信心，那么网络将提前退出，从而节省大量的时间。虽然早期退出分类器在之前的研究中已经被提出，但这些方法都是针对大型网络，使它们的技术在tinyML应用中不太优化/实用。我们的技术是专门针对小型CNN模型进行优化的。此外，我们提出了一种通过利用早期退出所学到的表示来减轻网络过度思考的方法。我们在CIFAR-10，CIFAR-100和ImageNet数据集上评估了我们的方法，并将结果与最先进的技术进行了比较。我们的实验表明，具有早期退出功能的tiny-RECX模型可以在较小的模型和更快的推断时间内达到可比较的精度。

    Deploying Machine learning (ML) on milliwatt-scale edge devices (tinyML) is gaining popularity due to recent breakthroughs in ML and Internet of Things (IoT). Most tinyML research focuses on model compression techniques that trade accuracy (and model capacity) for compact models to fit into the KB-sized tiny-edge devices. In this paper, we show how such models can be enhanced by the addition of an early exit intermediate classifier. If the intermediate classifier exhibits sufficient confidence in its prediction, the network exits early thereby, resulting in considerable savings in time. Although early exit classifiers have been proposed in previous work, these previous proposals focus on large networks, making their techniques suboptimal/impractical for tinyML applications. Our technique is optimized specifically for tiny-CNN sized models. In addition, we present a method to alleviate the effect of network overthinking by leveraging the representations learned by the early exit. We eva
    
[^156]: DORA：探索深度神经网络中的异常值表示

    DORA: Exploring outlier representations in Deep Neural Networks. (arXiv:2206.04530v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.04530](http://arxiv.org/abs/2206.04530)

    本文提出了一种名为DORA的数据不可知框架，用于分析深度神经网络中的表征空间，并可以识别不符合人类直观认知的表征。

    

    尽管深度神经网络（DNN）在学习复杂抽象方面非常有效，但它们容易意外地从训练数据中学习到虚假的特征。为了确保模型的透明度，检查学习表示之间的关系至关重要，因为意外的概念往往表现为与所需的任务不符的异常。在这项工作中，我们介绍了DORA（Data-agnOstic Representation Analysis）：用于分析DNN表示空间的第一个数据不可知框架。我们的框架采用了所提出的表示之间的极端激活（EA）距离度量，在不访问任何数据的情况下利用网络内自说明能力。我们定量验证了度量的正确性和与人为定义的语义距离的一致性。EA距离与人类判断之间的一致性使我们能够确定表征，其基本概念被认为是不自然的。

    Although Deep Neural Networks (DNNs) are incredibly effective in learning complex abstractions, they are susceptible to unintentionally learning spurious artifacts from the training data. To ensure model transparency, it is crucial to examine the relationships between learned representations, as unintended concepts often manifest themselves to be anomalous to the desired task. In this work, we introduce DORA (Data-agnOstic Representation Analysis): the first data-agnostic framework for the analysis of the representation space of DNNs. Our framework employs the proposed Extreme-Activation (EA) distance measure between representations that utilizes self-explaining capabilities within the network without accessing any data. We quantitatively validate the metric's correctness and alignment with human-defined semantic distances. The coherence between the EA distance and human judgment enables us to identify representations whose underlying concepts would be considered unnatural by humans by
    
[^157]: DEP-RL：过度作动和肌骨系统中的强化学习探索

    DEP-RL: Embodied Exploration for Reinforcement Learning in Overactuated and Musculoskeletal Systems. (arXiv:2206.00484v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2206.00484](http://arxiv.org/abs/2206.00484)

    通过在RL中集成微分外在可塑性（DEP），我们可以快速学习到达和运动，并在所有考虑到的任务中，在样本效率和鲁棒性方面优于当前方法。

    

    肌肉作动的生物能够在拥有大量肌肉的情况下学习无与伦比的巧妙动作。然而，大型肌骨系统模型上的强化学习（RL）尚未能展示类似的表现。我们针对大型过度作动动作空间中的无效探索提出了假设，证明了常见的探索噪声策略在过度作动系统的合成示例中不够充分。我们认为微分外在可塑性（DEP），一种自组织领域的方法，能够在互动几秒钟内诱导状态空间覆盖探索。通过将DEP与RL集成，我们在肌骨系统中实现了快速学习到达和运动，在所有考虑到的任务中，在样本效率和鲁棒性方面均优于当前方法。

    Muscle-actuated organisms are capable of learning an unparalleled diversity of dexterous movements despite their vast amount of muscles. Reinforcement learning (RL) on large musculoskeletal models, however, has not been able to show similar performance. We conjecture that ineffective exploration in large overactuated action spaces is a key problem. This is supported by the finding that common exploration noise strategies are inadequate in synthetic examples of overactuated systems. We identify differential extrinsic plasticity (DEP), a method from the domain of self-organization, as being able to induce state-space covering exploration within seconds of interaction. By integrating DEP into RL, we achieve fast learning of reaching and locomotion in musculoskeletal systems, outperforming current approaches in all considered tasks in sample efficiency and robustness.
    
[^158]: 元学习器用于多值处理异质作用估计的比较

    Comparison of meta-learners for estimating multi-valued treatment heterogeneous effects. (arXiv:2205.14714v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.14714](http://arxiv.org/abs/2205.14714)

    本文探讨了利用元学习器估计多值处理异质效应的问题，发现朴素扩展并不总是可行，提出并讨论了一些表现良好的元学习器。

    

    在利用观察数据进行因果推断时，条件平均处理效应（CATE）估计是主要挑战之一。除了基于机器学习的模型外，还开发出了称为元学习器的非参数估计器以估计CATE，其主要优点是不局限于特定的监督学习方法。然而，当处理不是二进制的时，一些朴素扩展的限制会出现，这样的任务就变得更加复杂。本文研究了元学习器用于估计多值处理异质效应。我们考虑了不同的元学习器，理论分析了它们的误差上界作为重要参数的函数，例如处理水平的数量，结果显示，朴素扩展并不总是提供满意的结果。我们引入和讨论了一些元学习器，它们在处理数量增多时表现良好。通过模拟研究和一项乙肝治疗研究的真实数据示例，我们证实了元学习器的优缺点。

    Conditional Average Treatment Effects (CATE) estimation is one of the main challenges in causal inference with observational data. In addition to Machine Learning based-models, nonparametric estimators called meta-learners have been developed to estimate the CATE with the main advantage of not restraining the estimation to a specific supervised learning method. This task becomes, however, more complicated when the treatment is not binary as some limitations of the naive extensions emerge. This paper looks into meta-learners for estimating the heterogeneous effects of multi-valued treatments. We consider different meta-learners, and we carry out a theoretical analysis of their error upper bounds as functions of important parameters such as the number of treatment levels, showing that the naive extensions do not always provide satisfactory results. We introduce and discuss meta-learners that perform well as the number of treatments increases. We empirically confirm the strengths and weak
    
[^159]: 基于多源迁移学习的深度模型强化学习

    Multi-Source Transfer Learning for Deep Model-Based Reinforcement Learning. (arXiv:2205.14410v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14410](http://arxiv.org/abs/2205.14410)

    本文提出了一种基于多源迁移学习的模块化技术，可以自动学习如何从先前学习的任务中提取有用信息，从而减少智能体在学习新任务时需要与环境互动的次数。

    

    强化学习面临的一个关键挑战是减少智能体在掌握给定任务时需要与环境互动的次数。迁移学习提出通过重复利用先前学习任务中的知识来解决这个问题。然而，确定哪个源任务有资格用于知识提取，以及选择哪些算法组件进行迁移，是其在强化学习应用中面临的严重障碍。本文的目标是通过模块化多源迁移学习技术来解决这些问题。所提出的技术可以自动学习如何从源任务中提取有用信息，而不受状态-动作空间和奖励函数差异的影响。我们在视觉控制方面进行了广泛而具有挑战性的跨域实验来支持我们的想法。

    A crucial challenge in reinforcement learning is to reduce the number of interactions with the environment that an agent requires to master a given task. Transfer learning proposes to address this issue by re-using knowledge from previously learned tasks. However, determining which source task qualifies as the most appropriate for knowledge extraction, as well as the choice regarding which algorithm components to transfer, represent severe obstacles to its application in reinforcement learning. The goal of this paper is to address these issues with modular multi-source transfer learning techniques. The proposed techniques automatically learn how to extract useful information from source tasks, regardless of the difference in state-action space and reward function. We support our claims with extensive and challenging cross-domain experiments for visual control.
    
[^160]: 关于点对点去中心化机器学习的（不）安全性研究

    On the (In)security of Peer-to-Peer Decentralized Machine Learning. (arXiv:2205.08443v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2205.08443](http://arxiv.org/abs/2205.08443)

    本文对去中心化学习进行了首次深入隐私分析，引入了一系列新颖的攻击方法，并证明去中心化学习并未提供比联邦学习更好的安全优势，反而增加了攻击面。而且，隐私保护配置需要全连接网络，失去了实际优势，完全打败了去中心化方法的目标。

    

    本文对去中心化学习框架进行了首次深入隐私分析，该框架是一种协作机器学习框架，旨在解决联邦学习的主要限制。我们引入了一系列新颖的攻击方法，包括被动和主动的去中心化敌手，我们证明了与去中心化学习提出者所声称的相反，去中心化学习并没有提供任何关于联邦学习的安全优势。相反，它增加了攻击面，使得系统中的任何用户都能够进行隐私攻击，例如梯度逆推，甚至获得对诚实用户的本地模型的完全控制。我们还表明，鉴于当前防护技术，去中心化学习的隐私保护配置需要全连接网络，失去了与联邦设置的任何实际优势，因此完全打败了去中心化方法的目标。

    In this work, we carry out the first, in-depth, privacy analysis of Decentralized Learning -- a collaborative machine learning framework aimed at addressing the main limitations of federated learning. We introduce a suite of novel attacks for both passive and active decentralized adversaries. We demonstrate that, contrary to what is claimed by decentralized learning proposers, decentralized learning does not offer any security advantage over federated learning. Rather, it increases the attack surface enabling any user in the system to perform privacy attacks such as gradient inversion, and even gain full control over honest users' local model. We also show that, given the state of the art in protections, privacy-preserving configurations of decentralized learning require fully connected networks, losing any practical advantage over the federated setup and therefore completely defeating the objective of the decentralized approach.
    
[^161]: SIBILA: 一种新的可解释通用机器学习模型集成方法在医学方面的应用

    SIBILA: A novel interpretable ensemble of general-purpose machine learning models applied to medical contexts. (arXiv:2205.06234v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.06234](http://arxiv.org/abs/2205.06234)

    SIBILA是一种集成了机器学习和深度学习模型以及可解释性算法，并能够在医疗领域中实现个性化治疗预测的方法，具有较高的准确性和可解释性。

    

    个性化医疗仍然是科学家们面临的重大挑战。机器学习和深度学习的快速发展使它们成为预测个体患者最合适治疗方法的可行替代方案。然而，为每个数据集开发定制模型的需要，缺乏对结果的解释以及高计算要求使许多人不愿使用这些方法。为节省时间并揭示模型内部的工作方式，SIBILA应运而生。SIBILA是一组应用了一系列可解释性算法以识别最相关输入特征的机器学习和深度学习模型集成方法。由于解释性算法可能不彼此一致，因此实施了共识阶段来估计每个变量对预测的全局贡献。SIBILA被装箱以在任何高性能计算平台上运行。尽管最初目的是作为命令行工具，但它也可通过用户友好的Web界面获得。

    Personalized medicine remains a major challenge for scientists. The rapid growth of Machine learning and Deep learning has made them a feasible alternative for predicting the most appropriate therapy for individual patients. However, the need to develop a custom model for every dataset, the lack of interpretation of their results and high computational requirements make many reluctant to use these methods. Aiming to save time and bring light to the way models work internally, SIBILA has been developed. SIBILA is an ensemble of machine learning and deep learning models that applies a range of interpretability algorithms to identify the most relevant input features. Since the interpretability algo- rithms may not be in line with each other, a consensus stage has been imple- mented to estimate the global attribution of each variable to the predictions. SIBILA is containerized to be run on any high-performance computing plat- form. Although conceived as a command-line tool, it is also av
    
[^162]: 基于二进制数据推断经验因果图以支持多维贫困分析的框架

    Framework for inferring empirical causal graphs from binary data to support multidimensional poverty analysis. (arXiv:2205.06131v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2205.06131](http://arxiv.org/abs/2205.06131)

    提出了一种用于推断贫困调查中的因果关系的框架，其可以揭示水、电力和资产所有权等MPI指标对贫困水平的影响最大。

    

    贫困是人类面临的基本问题之一。为了解决贫困问题，需要知道问题的严重程度。多维贫困指数(MPI)是一种广为人知的方法，用于衡量给定地区贫困问题的程度。计算MPI需要MPI指标的信息，这些指标是通过调查收集的二进制变量，代表贫困的不同方面，例如教育、健康、生活条件等。使用传统回归方法可以推断MPI指标对MPI指数的影响。然而，解决一个MPI指标是否可能会解决或导致其他MPI指标更多的问题并不明显，并且没有专门用于推断MPI指标之间经验因果关系的框架。在这项工作中，我们提出了一个框架，以推断贫困调查中的二进制变量的因果关系。我们的方法在我们知道真实因果关系的模拟数据集中表现更好。然后，我们将我们的框架应用于埃塞俄比亚家庭消费支出调查，发现水、电力和资产所有权等MPI指标对埃塞俄比亚的贫困水平影响最大。

    Poverty is one of the fundamental issues that mankind faces. To solve poverty issues, one needs to know how severe the issue is. The Multidimensional Poverty Index (MPI) is a well-known approach that is used to measure a degree of poverty issues in a given area. To compute MPI, it requires information of MPI indicators, which are \textbf{binary variables} collecting by surveys, that represent different aspects of poverty such as lacking of education, health, living conditions, etc. Inferring impacts of MPI indicators on MPI index can be solved by using traditional regression methods. However, it is not obvious that whether solving one MPI indicator might resolve or cause more issues in other MPI indicators and there is no framework dedicating to infer empirical causal relations among MPI indicators.  In this work, we propose a framework to infer causal relations on binary variables in poverty surveys. Our approach performed better than baseline methods in simulated datasets that we kno
    
[^163]: 带有几何损失函数的球面旋转降维

    Spherical Rotation Dimension Reduction with Geometric Loss Functions. (arXiv:2204.10975v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2204.10975](http://arxiv.org/abs/2204.10975)

    该论文提出了一种名为SRCA的非线性降维方法，在处理高维度的低样本大小数据时，通过引入球体或椭球体，保留数据的几何结构，提高近似低维流形的效果。

    

    现代数据集通常具有高维性，但数据位于低维流形中，可以揭示对数据分析至关重要的潜在几何结构。这类数据集的典型例子是细胞周期测量值的集合，其中过程的固有循环性可以表示为圆或球。受分析这些类型数据集的需求启发，我们提出了一种非线性降维方法，称为球面旋转成分分析（SRCA），它将几何信息纳入到降维过程中以更好地逼近低维流形。SRCA是一种通用的方法，旨在在高维和小样本大小的情况下工作。通过使用球体或椭球体，SRCA提供了数据的低秩球面表示，并在降维过程中有效地保留了数据集的几何结构。全面的模拟研究以及对人类细胞周期数据的成功应用证明了SRCA的有效性和多功能性。

    Modern datasets often exhibit high dimensionality, yet the data reside in low-dimensional manifolds that can reveal underlying geometric structures critical for data analysis. A prime example of such a dataset is a collection of cell cycle measurements, where the inherently cyclical nature of the process can be represented as a circle or sphere. Motivated by the need to analyze these types of datasets, we propose a nonlinear dimension reduction method, Spherical Rotation Component Analysis (SRCA), that incorporates geometric information to better approximate low-dimensional manifolds. SRCA is a versatile method designed to work in both high-dimensional and small sample size settings. By employing spheres or ellipsoids, SRCA provides a low-rank spherical representation of the data with general theoretic guarantees, effectively retaining the geometric structure of the dataset during dimensionality reduction. A comprehensive simulation study, along with a successful application to human c
    
[^164]: 稀疏门控专家层用于CNN可解释性

    Sparsely-gated Mixture-of-Expert Layers for CNN Interpretability. (arXiv:2204.10598v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2204.10598](http://arxiv.org/abs/2204.10598)

    本文介绍了将稀疏门控专家层应用于计算机视觉中的CNN，并探究了这对模型可解释性的影响，同时也提出了软约束与硬约束两种方法来稳定MoE的训练。研究表明，该方法使专家可以有效关注输入的各个子领域，提高模型的性能。

    

    稀疏门控专家(MoE)层最近成功应用于大规模变换器中，特别是用于语言模型任务。 稀疏MoE层的一个有趣的副作用是，通过自然的专家特化，它们为模型提供内在的可解释性。在本文中，我们将稀疏MoE层应用于计算机视觉任务中的CNN，并分析这对模型可解释性产生的影响。为了稳定MoE的训练，我们提出了软约束和硬约束两种方法。在硬约束中，某些专家的权重被允许变为零，而软约束则通过额外的辅助损失平衡专家的贡献。因此，软约束更好地处理了专家利用，并支持专家专业化进程，而硬约束保持了更广义的专家并增加了整个模型的性能。我们的研究结果表明，专家可以隐式地关注输入的各个子领域。

    Sparsely-gated Mixture of Expert (MoE) layers have been recently successfully applied for scaling large transformers, especially for language modeling tasks. An intriguing side effect of sparse MoE layers is that they convey inherent interpretability to a model via natural expert specialization. In this work, we apply sparse MoE layers to CNNs for computer vision tasks and analyze the resulting effect on model interpretability. To stabilize MoE training, we present both soft and hard constraint-based approaches. With hard constraints, the weights of certain experts are allowed to become zero, while soft constraints balance the contribution of experts with an additional auxiliary loss. As a result, soft constraints handle expert utilization better and support the expert specialization process, while hard constraints maintain more generalized experts and increase overall model performance. Our findings demonstrate that experts can implicitly focus on individual sub-domains of the input s
    
[^165]: 化学基元的自动识别

    Automatic Identification of Chemical Moieties. (arXiv:2203.16205v2 [physics.chem-ph] UPDATED)

    [http://arxiv.org/abs/2203.16205](http://arxiv.org/abs/2203.16205)

    该论文介绍了一种自动识别化学基元的方法，使得在化学数据库中选择代表性条目、自动构建粗粒化力场以及识别反应坐标等应用成为可能。

    

    近年来，机器学习方法在预测量子力学可观测量方面越来越受欢迎。消息传递神经网络（MPNNs）通过构建原子表示来解决此任务，从中预测感兴趣的特性。在这里，我们介绍了一种方法来自动识别这些表示中的化学基元（分子建筑块），从而实现了多种应用，这些应用通常需要专业知识。所需的表示可以是预先训练好的MPNN提供，也可以仅使用结构信息从头学习。除了数据驱动的分子指纹设计之外，我们的方法的多功能性也通过使化学数据库中具有代表性的条目的选择、自动构建粗粒化力场以及识别反应坐标等方面得到了证明。

    In recent years, the prediction of quantum mechanical observables with machine learning methods has become increasingly popular. Message-passing neural networks (MPNNs) solve this task by constructing atomic representations, from which the properties of interest are predicted. Here, we introduce a method to automatically identify chemical moieties (molecular building blocks) from such representations, enabling a variety of applications beyond property prediction, which otherwise rely on expert knowledge. The required representation can either be provided by a pretrained MPNN, or learned from scratch using only structural information. Beyond the data-driven design of molecular fingerprints, the versatility of our approach is demonstrated by enabling the selection of representative entries in chemical databases, the automatic construction of coarse-grained force fields, as well as the identification of reaction coordinates.
    
[^166]: GrIPS：基于编辑的无梯度指令搜索，用于辅助大型语言模型

    GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models. (arXiv:2203.07281v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2203.07281](http://arxiv.org/abs/2203.07281)

    GrIPS是一种基于编辑的无梯度搜索方法，用于改进大型语言模型的任务指令，显著提高性能。

    

    提供自然语言指令的提示是一种改进大型语言模型在零样本设置下任务性能的有用新范例。最近的工作致力于通过手动重写或梯度调整来提高这些提示。然而，手动重写耗时且需要主观解释，而基于梯度的调整对于大型模型而言计算成本极高，对于基于API的模型来说可能不可行。在这项工作中，我们介绍了Gradient-free Instructional Prompt Search (GrIPS)，一种基于编辑的无梯度搜索方法，用于改进大型语言模型的任务指令。GrIPS接受面向人类设计的指令，并自动返回完善的编辑提示，同时允许基于API的调整。使用InstructGPT模型，在自然语言指令数据集的八个分类任务上，GrIPS将平均任务性能提高了高达4.30个百分点（OPT，BLOOM等任务也有类似的改进）

    Providing natural language instructions in prompts is a useful new paradigm for improving task performance of large language models in a zero-shot setting. Recent work has aimed to improve such prompts via manual rewriting or gradient-based tuning. However, manual rewriting is time-consuming and requires subjective interpretation, while gradient-based tuning can be extremely computationally demanding for large models and may not be feasible for API-based models. In this work, we introduce Gradient-free Instructional Prompt Search (GrIPS), a gradient-free, edit-based search approach for improving task instructions for large language models. GrIPS takes in instructions designed for humans and automatically returns an improved, edited prompt, while allowing for API-based tuning. With InstructGPT models, GrIPS improves the average task performance by up to 4.30 percentage points on eight classification tasks from the Natural Instructions dataset (with similar improvements for OPT, BLOOM, a
    
[^167]: 利用PU学习进行表型研究的全基因组联合分析

    Phenotyping with Positive Unlabelled Learning for Genome-Wide Association Studies. (arXiv:2202.07451v1 [stat.AP] CROSS LISTED)

    [http://arxiv.org/abs/2202.07451](http://arxiv.org/abs/2202.07451)

    该论文介绍了一种利用PU学习进行表型研究的全基因组联合分析方法，该方法能够通过将锚定学习和变换器结构组合来检测基因组关联，即使在减少对照组的情况下也能保持更多显著基因组关联。

    

    通过医疗保健和生命科学中的实际应用，识别表型在进一步理解疾病生物学方面发挥着重要作用。但是处理电子健康记录（EHR）中的复杂性和噪声是具有挑战性的，这就促使了在表型发现中应用机器学习。我们的重点不是找到临床决策支持的预测亚型，而是关注导致表型误分类的噪声，可能会降低表型在全基因组关联研究（GWAS）中检测关联的能力。我们展示了将锚定学习和变换器结构组合到我们的模型AnchorBERT中，我们能够检测到仅在大型联合研究中才发现的基因组关联。当减少50\%的可用对照组时，我们的模型能够保持GWAS中40\%更多的显著基因组关联。

    Identifying phenotypes plays an important role in furthering our understanding of disease biology through practical applications within healthcare and the life sciences. The challenge of dealing with the complexities and noise within electronic health records (EHRs) has motivated applications of machine learning in phenotypic discovery. While recent research has focused on finding predictive subtypes for clinical decision support, here we instead focus on the noise that results in phenotypic misclassification, which can reduce a phenotypes ability to detect associations in genome-wide association studies (GWAS). We show that by combining anchor learning and transformer architectures into our proposed model, AnchorBERT, we are able to detect genomic associations only previously found in large consortium studies with 5$\times$ more cases. When reducing the number of controls available by 50\%, we find our model is able to maintain 40\% more significant genomic associations from the GWAS 
    
[^168]: Quantus: 一个可解释的AI工具包，用于负责任评估神经网络的解释和更多应用

    Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations and Beyond. (arXiv:2202.06861v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.06861](http://arxiv.org/abs/2202.06861)

    本文介绍了Quantus，一个可解释的AI工具包，用于详尽迅速地评估神经网络预测的解释表现，并提高领域内的透明度和可重复性。

    

    解释方法的评估是一个尚未深入探讨的研究课题，然而，由于可解释性被认为能增强人们对人工智能的信任，有必要系统地审核和比较解释方法以确认其正确性。到目前为止，还没有专注于XAI评估的工具，能够详尽迅速地让研究者评估神经网络预测解释的表现。为了增加领域内的透明度和可重复性，我们建立了Quantus，一个全面的Python评估工具包，其中包括一个不断增长的、良好组织的评估指标和解释方法评估的教程集合。该工具包经过了彻底的测试，可在PyPi下或https://github.com/understandable-machine-intelligence-lab/Quantus/上以开源许可证获得。

    The evaluation of explanation methods is a research topic that has not yet been explored deeply, however, since explainability is supposed to strengthen trust in artificial intelligence, it is necessary to systematically review and compare explanation methods in order to confirm their correctness. Until now, no tool with focus on XAI evaluation exists that exhaustively and speedily allows researchers to evaluate the performance of explanations of neural network predictions. To increase transparency and reproducibility in the field, we therefore built Quantus -- a comprehensive, evaluation toolkit in Python that includes a growing, well-organised collection of evaluation metrics and tutorials for evaluating explainable methods. The toolkit has been thoroughly tested and is available under an open-source license on PyPi (or on https://github.com/understandable-machine-intelligence-lab/Quantus/).
    
[^169]: PEg TRAnsfer Workflow recognition challenge报告：多模态数据是否有助于提高识别准确度？

    PEg TRAnsfer Workflow recognition challenge report: Does multi-modal data improve recognition?. (arXiv:2202.05821v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.05821](http://arxiv.org/abs/2202.05821)

    本文介绍了PETRAW挑战，探讨基于视频、运动学和分割数据进行手术工作流程识别的方法，结果显示多模态数据可以提高识别准确度。

    

    本文介绍了“PEg TRAnsfert Workflow recognition”（PETRAW）挑战的设计与结果，该挑战的目标是基于视频、运动学和分割等单一或多种模态开发外科手术工作流程识别方法，以研究它们的附加值。PETRAW挑战提供了一个包含150个虚拟模拟器上进行的插管转移序列的数据集。该数据集由视频、运动学、语义分割和工作流程注释组成，所述注释在三种不同的粒度级别（阶段、步骤和活动）上描述了序列。参赛者被提出了五项任务：其中三项任务与所有粒度的识别与一种可用的模态相关，而其他任务则通过多种模态的组合来解决识别问题。平均应用相关平衡准确性（AD-Accuracy）被用作评估度量标准，以考虑不平衡的类别，并且因为它比其他度量更具有临床相关性。

    This paper presents the design and results of the "PEg TRAnsfert Workflow recognition" (PETRAW) challenge whose objective was to develop surgical workflow recognition methods based on one or several modalities, among video, kinematic, and segmentation data, in order to study their added value. The PETRAW challenge provided a data set of 150 peg transfer sequences performed on a virtual simulator. This data set was composed of videos, kinematics, semantic segmentation, and workflow annotations which described the sequences at three different granularity levels: phase, step, and activity. Five tasks were proposed to the participants: three of them were related to the recognition of all granularities with one of the available modalities, while the others addressed the recognition with a combination of modalities. Average application-dependent balanced accuracy (AD-Accuracy) was used as evaluation metric to take unbalanced classes into account and because it is more clinically relevant tha
    
[^170]: 学习Whittle索引策略以解决高效遗憾的不安定赌博机问题

    On learning Whittle index policy for restless bandits with scalable regret. (arXiv:2202.03463v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.03463](http://arxiv.org/abs/2202.03463)

    本文提出了一种基于不安定赌博机模型的可扩展模型基于强化学习算法，相比于RL算法的常规遗憾界，该方法具有更小的遗憾界。

    

    当系统模型未知时，强化学习是学习基于数据的良好资源分配和调度策略的有吸引力的方法。然而，大多数强化学习算法的累计遗憾会随$\tilde O(\mathsf{S} \sqrt{\mathsf{A} T})$规模增加，其中$\mathsf{S}$为状态空间大小，$\mathsf{A}$为动作空间大小，$T$为时间步长，而$\tilde{O}(\cdot)$符号表示隐藏的对数项。由于具有与状态空间大小线性正相关的遗憾上界，这些遗憾边界对于资源分配和调度问题是不可接受的大。本文提出了一种在这些问题中具有可扩展性的模型基于强化学习算法。具体而言，我们考虑了不安定赌博机模型，并提出了一种根据模型基础结构调整的基于Thompson采样的学习算法。我们提出了两种关于Whittle索引策略的所提算法的遗憾特性说明。首先，我们表明对于一种Whittle索引策略，所提算法的遗憾满足...（摘要未完整展示）

    Reinforcement learning is an attractive approach to learn good resource allocation and scheduling policies based on data when the system model is unknown. However, the cumulative regret of most RL algorithms scales as $\tilde O(\mathsf{S} \sqrt{\mathsf{A} T})$, where $\mathsf{S}$ is the size of the state space, $\mathsf{A}$ is the size of the action space, $T$ is the horizon, and the $\tilde{O}(\cdot)$ notation hides logarithmic terms. Due to the linear dependence on the size of the state space, these regret bounds are prohibitively large for resource allocation and scheduling problems. In this paper, we present a model-based RL algorithm for such problems which has scalable regret. In particular, we consider a restless bandit model, and propose a Thompson-sampling based learning algorithm which is tuned to the underlying structure of the model. We present two characterizations of the regret of the proposed algorithm with respect to the Whittle index policy. First, we show that for a r
    
[^171]: 深度学习商业框架的量化后门

    Quantization Backdoors to Deep Learning Commercial Frameworks. (arXiv:2108.09187v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2108.09187](http://arxiv.org/abs/2108.09187)

    本文揭示了商业框架中潜在的深度学习模型后门安全漏洞，可以通过量化攻击实现后门触发并逃避检测，从而危及已部署的模型安全，可能导致未经授权的数据访问。

    

    目前，由于低延迟和高隐私保护，越来越多的深度学习模型被部署在无处不在的边缘物联网设备上。

    Currently, there is a burgeoning demand for deploying deep learning (DL) models on ubiquitous edge Internet of Things (IoT) devices attributed to their low latency and high privacy preservation. However, DL models are often large in size and require large-scale computation, which prevents them from being placed directly onto IoT devices, where resources are constrained and 32-bit floating-point (float-32) operations are unavailable. Commercial framework (i.e., a set of toolkits) empowered model quantization is a pragmatic solution that enables DL deployment on mobile devices and embedded systems by effortlessly post-quantizing a large high-precision model (e.g., float-32) into a small low-precision model (e.g., int-8) while retaining the model inference accuracy. However, their usability might be threatened by security vulnerabilities.  This work reveals that the standard quantization toolkits can be abused to activate a backdoor. We demonstrate that a full-precision backdoored model w
    
[^172]: 分布漂移下的随机优化

    Stochastic Optimization under Distributional Drift. (arXiv:2108.07356v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2108.07356](http://arxiv.org/abs/2108.07356)

    本文提供了一种在分布漂移下优化凸函数的新方法，经数值实验证明在低漂移-噪声比的情况下，近端随机梯度方法采用步长衰减策略可显著提升跟踪效率。

    

    本文考虑了最小化随机演化凸函数的问题，这个演化过程是未知的，并且可能依赖于时间和决策变量本身。这类问题在机器学习和信号处理领域中广泛存在，称为概念漂移、随机跟踪和执行预测。我们提供了新的非渐近收敛保证，重点关注在期望值和高概率下成立的界限。我们获得的效率估计明确地解耦了优化误差、梯度噪声和时间漂移的影响。值得注意的是，我们确定了一个低漂移-噪声比的区域，在这个区域里，近端随机梯度方法的跟踪效率因步长衰减策略而受益显著。数值实验证明了我们的结果。

    We consider the problem of minimizing a convex function that is evolving according to unknown and possibly stochastic dynamics, which may depend jointly on time and on the decision variable itself. Such problems abound in the machine learning and signal processing literature, under the names of concept drift, stochastic tracking, and performative prediction. We provide novel non-asymptotic convergence guarantees for stochastic algorithms with iterate averaging, focusing on bounds valid both in expectation and with high probability. The efficiency estimates we obtain clearly decouple the contributions of optimization error, gradient noise, and time drift. Notably, we identify a low drift-to-noise regime in which the tracking efficiency of the proximal stochastic gradient method benefits significantly from a step decay schedule. Numerical experiments illustrate our results.
    
[^173]: 针对非凸-凹极小极大问题的无导数交替投影算法

    Derivative-free Alternating Projection Algorithms for General Nonconvex-Concave Minimax Problems. (arXiv:2108.00473v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2108.00473](http://arxiv.org/abs/2108.00473)

    本文提出针对非凸-凹极小极大问题的无导数交替投影算法，包括光滑问题的交替随机梯度投影算法（ZO-AGP），以及块状非光滑问题的分块交替随机近端梯度算法（ZO-BAPG）。这些算法具有较少的函数值估计和较高的迭代复杂度。

    

    本文研究了非凸-凹极小极大问题的零阶算法，这类问题近年在机器学习、信号处理等领域引起了广泛关注。我们提出了一种零阶交替随机梯度投影（ZO-AGP）算法来解决光滑的非凸-凹极小极大问题，其迭代复杂度为 $\mathcal{O}(\varepsilon^{-4})$，每次迭代的函数值估计次数为 $\mathcal{O}(d_{x}+d_{y})$。此外，我们还提出了一种零阶分块交替随机近端梯度算法（ZO-BAPG）来解决块状非光滑的非凸-凹极小极大优化问题，其迭代复杂度为 $\mathcal{O}(\varepsilon^{-4})$，每次迭代的函数值估计次数为 $\mathcal{O}(K d_{x}+d_{y})$。据我们所知，这是首次提出这些算法。

    In this paper, we study zeroth-order algorithms for nonconvex-concave minimax problems, which have attracted widely attention in machine learning, signal processing and many other fields in recent years. We propose a zeroth-order alternating randomized gradient projection (ZO-AGP) algorithm for smooth nonconvex-concave minimax problems, and its iteration complexity to obtain an $\varepsilon$-stationary point is bounded by $\mathcal{O}(\varepsilon^{-4})$, and the number of function value estimation is bounded by $\mathcal{O}(d_{x}+d_{y})$ per iteration. Moreover, we propose a zeroth-order block alternating randomized proximal gradient algorithm (ZO-BAPG) for solving block-wise nonsmooth nonconvex-concave minimax optimization problems, and the iteration complexity to obtain an $\varepsilon$-stationary point is bounded by $\mathcal{O}(\varepsilon^{-4})$ and the number of function value estimation per iteration is bounded by $\mathcal{O}(K d_{x}+d_{y})$. To the best of our knowledge, this 
    
[^174]: “抵御后门攻击的不兼容聚类机制”

    Incompatibility Clustering as a Defense Against Backdoor Poisoning Attacks. (arXiv:2105.03692v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.03692](http://arxiv.org/abs/2105.03692)

    本文提出了一种基于不兼容性的聚类机制，该机制可以将数据集划分为由训练过程的目标所定义且具有意义的聚类，并有效减轻后门攻击的影响。

    

    本文提出一种新型的聚类机制，该机制基于模型训练过程中出现的数据子集不相容性属性。该机制将数据集划分为只能泛化到其自身的子集，即在一个子集上的训练不会改善其他子集的性能。利用数据集与训练过程之间的交互作用，我们的聚类机制将数据集划分为由训练过程的目标所定义且具有意义的聚类。我们将我们的聚类机制应用于防御数据毒化攻击，即攻击者将恶意毒害数据注入训练数据集，以影响训练模型的输出。我们的评估重点关注利用GTSRB和CIFAR-10数据集进行图像分类的深度神经网络中的后门攻击。我们的结果表明：1）这些攻击产生的毒害数据集是有毒害数据和干净数据不相容的；2）我们的聚类机制可以有效减轻后门攻击的影响。

    We propose a novel clustering mechanism based on an incompatibility property between subsets of data that emerges during model training. This mechanism partitions the dataset into subsets that generalize only to themselves, i.e., training on one subset does not improve performance on the other subsets. Leveraging the interaction between the dataset and the training process, our clustering mechanism partitions datasets into clusters that are defined by--and therefore meaningful to--the objective of the training process.  We apply our clustering mechanism to defend against data poisoning attacks, in which the attacker injects malicious poisoned data into the training dataset to affect the trained model's output. Our evaluation focuses on backdoor attacks against deep neural networks trained to perform image classification using the GTSRB and CIFAR-10 datasets. Our results show that (1) these attacks produce poisoned datasets in which the poisoned and clean data are incompatible and (2) o
    

