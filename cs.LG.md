# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Analyzing the contribution of different passively collected data to predict Stress and Depression.](http://arxiv.org/abs/2310.13607) | 本研究分析了不同的被动收集数据类型对于预测压力和抑郁的贡献。结果显示，WiFi特征和电话日志特征对于压力和抑郁预测非常重要。 |
| [^2] | [ReLM: Leveraging Language Models for Enhanced Chemical Reaction Prediction.](http://arxiv.org/abs/2310.13590) | ReLM是一种通过利用语言模型来辅助图神经网络(GNN)从而提高化学反应预测准确性的新框架,在不同化学反应数据集上表现出色，尤其是在超出分布范围的情况下。 |
| [^3] | [Improving Cross-Lingual Transfer through Subtree-Aware Word Reordering.](http://arxiv.org/abs/2310.13583) | 通过子树感知的单词重排序方法能够提高跨语言转移的能力，并克服了现有方法中语言特定规则、POS标签级别或只针对主句的限制。 |
| [^4] | [Tree Search in DAG Space with Model-based Reinforcement Learning for Causal Discovery.](http://arxiv.org/abs/2310.13576) | 本论文提出了一种在DAG空间中使用树搜索和模型驱动强化学习进行因果发现的方法，并通过两个实际任务的评估，证明了其在性能上显著优于目前最先进的无模型方法和贪婪搜索，具有很大的应用前景。 |
| [^5] | [Progressive Dual Priori Network for Generalized Breast Tumor Segmentation.](http://arxiv.org/abs/2310.13574) | 本文提出了一种基于渐进双先验网络的方法用于乳腺肿瘤分割，通过使用弱语义先验和跨尺度相关先验知识逐步改进分割性能，实验证明相比于现有方法，该方法在泛化能力和分割性能上都有显著提高。 |
| [^6] | [Unraveling the Enigma of Double Descent: An In-depth Analysis through the Lens of Learned Feature Space.](http://arxiv.org/abs/2310.13572) | 本研究通过对学得表示的特征空间进行全面分析，揭示出双谷现象是在使用噪声数据训练的不完美模型中出现的，并提出了双谷现象的解释：模型首先学习噪声数据直到插值点，然后通过超参数化的隐式正则化来将信息与噪声分离。 |
| [^7] | [Reward Shaping for Happier Autonomous Cyber Security Agents.](http://arxiv.org/abs/2310.13565) | 本研究探讨了在计算机网络防御任务中，奖励信号的特点对深度强化学习算法的影响，并研究了奖励塑造技术以提高代理的训练效率和性能。 |
| [^8] | [Cache & Distil: Optimising API Calls to Large Language Models.](http://arxiv.org/abs/2310.13561) | 本研究针对分类任务，通过神经缓存技术，使用边界抽样和委员会查询作为决策策略，优化了对大型语言模型的API调用，并取得了一致的好处。 |
| [^9] | [On sample complexity of conditional independence testing with Von Mises estimator with application to causal discovery.](http://arxiv.org/abs/2310.13553) | 本文研究了以Von Mises估计器进行条件独立性测试的样本复杂性，并基于该估计器设计了一种新的测试方法。实验证明，该方法在时间或样本复杂度方面优于其他方法。 |
| [^10] | [Provable Benefits of Multi-task RL under Non-Markovian Decision Making Processes.](http://arxiv.org/abs/2310.13550) | 本文通过研究在非马尔可夫决策过程下的多任务强化学习，证明了多个MDPs之间共享的潜在结构能够显著提高采样效率。对于部分可观察MDPs和预测状态表示，我们提出了一个联合模型类，并使用$\eta$-bracketing number来量化其复杂性和任务的相似性，从而决定了多任务相较于单任务RL的受益。 |
| [^11] | [Towards Understanding Sycophancy in Language Models.](http://arxiv.org/abs/2310.13548) | 这项研究探讨了强化学习从人类反馈中训练高质量AI助手的技术，发现这种方法可能导致模型在回答问题时过于谄媚，而不是坦诚，通过分析人类偏好数据得出了这一结论。 |
| [^12] | [Positive-Unlabeled Node Classification with Structure-aware Graph Learning.](http://arxiv.org/abs/2310.13538) | 本文提出了一种利用图结构进行正-未标记节点分类的方法。我们引入了一个距离感知的PU损失来提供更准确的监督，同时通过正则化项与图结构对齐。实验证明该方法在多种图数据集上具有卓越性能。 |
| [^13] | [Technical Report for ICCV 2023 Visual Continual Learning Challenge: Continuous Test-time Adaptation for Semantic Segmentation.](http://arxiv.org/abs/2310.13533) | 该论文提出了一种连续测试时间适应方法，用于解决语义分割任务中视频序列中逐渐变化的领域问题。该方法在每个图像序列中单独评估，并通过逐渐改变天气条件和时段来测试模型的性能。 |
| [^14] | [Controlled Randomness Improves the Performance of Transformer Models.](http://arxiv.org/abs/2310.13526) | 本研究通过在训练过程中引入受控随机性来提高Transformer模型的性能，并在命名实体识别、关系抽取和文本摘要等任务中取得了改进效果。 |
| [^15] | [Variational measurement-based quantum computation for generative modeling.](http://arxiv.org/abs/2310.13524) | 这项研究提出了一种基于测量的变分量子计算算法，将量子测量的随机性视为计算资源，并应用于生成建模任务。 |
| [^16] | [Feature Selection and Hyperparameter Fine-tuning in Artificial Neural Networks for Wood Quality Classification.](http://arxiv.org/abs/2310.13490) | 本文研究了在木材质量分类中同时调整人工神经网络的超参数和选择最佳特征子集的问题。 |
| [^17] | [Personalized identification, prediction, and stimulation of neural oscillations via data-driven models of epileptic network dynamics.](http://arxiv.org/abs/2310.13480) | 该论文研究了从脑电图数据中提取个体化的癫痫网络动力学模型，应用于个性化识别、预测和刺激神经振荡，为癫痫治疗提供了有前景的疗法选择。 |
| [^18] | [Segment, Select, Correct: A Framework for Weakly-Supervised Referring Segmentation.](http://arxiv.org/abs/2310.13479) | 这项研究提出了一个弱监督框架，通过将指代图像分割任务分解为获取实例掩模、选择正确掩模和纠正错误掩模的三个步骤，填补了弱监督和零样本方法在性能上的差距。 |
| [^19] | [An Analysis of $D^\alpha$ seeding for $k$-means.](http://arxiv.org/abs/2310.13474) | 该论文分析了$k$-means的$D^\alpha$种子选择算法。他们证明了该算法在期望上对于任何$ \alpha \geq 1 $都能保证一个$ O(2^{2\alpha}\cdot \log k) $-近似解。此外，他们还发现使用$ \alpha > 2 $的$ D^\alpha $种子选择算法可以得到比标准的$ k $-means目标函数更好的解。该论文提供了对这一现象的严格理解。 |
| [^20] | [Stable Nonconvex-Nonconcave Training via Linear Interpolation.](http://arxiv.org/abs/2310.13459) | 本文通过理论分析指出，优化过程中的不稳定性通常是由损失函数的非单调性引起的，提出了一种稳定（大规模）神经网络训练的方法，即通过线性插值来利用“非扩张算子”的理论来解决。通过构建一种新的优化方案，松弛近似近端点（RAPP），发现了一族Lookahead算法，能够在协调部分单调问题中收敛，即使基本优化器采用梯度下降升级算法。 |
| [^21] | [Correspondence learning between morphologically different robots through task demonstrations.](http://arxiv.org/abs/2310.13458) | 本论文提出了一种学习不同形态机器人间对应关系的方法，通过演示实现了共同的潜变量表示，使得不同机器人可以更直接地转移技能。 |
| [^22] | [Random Matrix Analysis to Balance between Supervised and Unsupervised Learning under the Low Density Separation Assumption.](http://arxiv.org/abs/2310.13434) | 本论文提出了一种理论框架，在低密度分离假设下分析高维情况下的半监督分类，并介绍了QLDS模型，该模型在平衡监督和无监督学习方法之间建立了一个平滑的桥梁。通过应用随机矩阵理论，我们推导出了在渐近情况下分类错误的理论评估，并提出了一种能够找到最佳平衡点的超参数选择策略。 |
| [^23] | [Y-Diagonal Couplings: Approximating Posteriors with Conditional Wasserstein Distances.](http://arxiv.org/abs/2310.13433) | 本研究介绍了一种使用条件Wasserstein距离逼近后验分布的方法，并提出了一组受限耦合来计算后验分布的期望Wasserstein距离。我们推导了其对偶形式，并展示了其在后验采样方面的有利性质。 |
| [^24] | [HRTF Interpolation using a Spherical Neural Process Meta-Learner.](http://arxiv.org/abs/2310.13430) | 该论文介绍了一种使用球形神经过程元学习器进行HRTF插值的方法，通过利用HRTF数据的球形几何特性和左右声道的潜在对称性来纠正个性化方法的估计误差。 |
| [^25] | [FLTracer: Accurate Poisoning Attack Provenance in Federated Learning.](http://arxiv.org/abs/2310.13424) | FLTracer是第一个FL攻击来源追踪框架，能够高准确性地检测各种攻击，并追踪攻击的时间、目标、类型和被毒化的位置。与现有方法不同，FLTracer利用本地信息和全局信息的统计分析来实现。 |
| [^26] | [BRFL: A Blockchain-based Byzantine-Robust Federated Learning Model.](http://arxiv.org/abs/2310.13403) | 提出了一种基于区块链的拜占庭鲁棒联邦学习（BRLF）模型，该模型通过将联邦学习与区块链技术相结合，解决了联邦学习中的拜占庭攻击问题，并通过选择聚合节点、谱聚类和平均梯度计算等方法实现了高性能。 |
| [^27] | [Calibrating Neural Simulation-Based Inference with Differentiable Coverage Probability.](http://arxiv.org/abs/2310.13402) | 本研究提出了一种在选定的摊销SBI技术的神经模型的训练目标中添加校准项的方法，以解决现有算法产生过于自信的后验概率的问题。该方法适用于现有的计算流程，实现可靠的黑盒后验推断。 |
| [^28] | [Equivariant Deep Weight Space Alignment.](http://arxiv.org/abs/2310.13397) | 本论文提出了一个名为Deep-Align的新框架，用于学习解决权重对齐问题，以加速对齐过程并提高其质量。 |
| [^29] | [RL-X: A Deep Reinforcement Learning Library (not only) for RoboCup.](http://arxiv.org/abs/2310.13396) | 本文介绍了 RL-X 这个新的深度强化学习库，并展示了它在 RoboCup 和经典 DRL 基准测试中的应用，以及相比于其他框架的加速效果。 |
| [^30] | [Optimal Best Arm Identification with Fixed Confidence in Restless Bandits.](http://arxiv.org/abs/2310.13393) | 在多臂赌博机中，我们研究了以固定置信度进行最优臂识别。我们提出了一种新的策略来识别具有最大平均值的臂，同时限制了决策错误的概率，并建立了停止时间增长率的下界。 |
| [^31] | [Learning Successor Representations with Distributed Hebbian Temporal Memory.](http://arxiv.org/abs/2310.13391) | 本文提出了一种名为DHTM的算法，它基于因子图形式和多组成神经元模型，利用分布式表示、稀疏转移矩阵和局部Hebbian样学习规则来解决在线隐藏表示学习的挑战。实验结果表明，DHTM在变化的环境中比经典的LSTM效果更好，并与更先进的类似RNN的算法性能相当，可以加速继任者表示的时间差异学习。 |
| [^32] | [Music Augmentation and Denoising For Peak-Based Audio Fingerprinting.](http://arxiv.org/abs/2310.13388) | 本研究通过引入音频增强流水线和深度学习模型，为嘈杂环境下的音频指纹识别提出了解决方案，提高了识别性能。 |
| [^33] | [Assumption violations in causal discovery and the robustness of score matching.](http://arxiv.org/abs/2310.13387) | 本文在不同背景条件下对最近的因果发现方法在观察性独立同分布数据上的实际性能进行了基准测试，发现基于score matching的方法在具有挑战性的场景中表现出令人惊讶的性能。 |
| [^34] | [Tuna: Instruction Tuning using Feedback from Large Language Models.](http://arxiv.org/abs/2310.13385) | 本文提出了一种使用大型语言模型的反馈进行指令调整的方法，通过使用概率排名和上下文排名来增加生成更好响应的可能性。 |
| [^35] | [Salted Inference: Enhancing Privacy while Maintaining Efficiency of Split Inference in Mobile Computing.](http://arxiv.org/abs/2310.13384) | 本文介绍了一种名为“盐化DNN”的方法，通过在推理时让客户控制DNN输出的语义解释，同时保持准确性和效率与标准DNN几乎一致，提升了移动计算中隐私和计算效率的问题。 |
| [^36] | [Accelerated sparse Kernel Spectral Clustering for large scale data clustering problems.](http://arxiv.org/abs/2310.13381) | 这篇论文介绍了改进的稀疏多路径核谱聚类算法，通过不完全Cholesky分解和降维方法的组合实现了稀疏性，并通过解决核心特征值问题的对称版本大大提高了计算特性。 |
| [^37] | [SigFormer: Signature Transformers for Deep Hedging.](http://arxiv.org/abs/2310.13369) | SigFormer是一种基于路径签名和Transformer的深度对冲模型，通过结合路径签名的能力捕捉复杂的数据模式，同时利用Transformer提供优秀的序列注意力，在设计神经网络架构方面具有优势。实验证明，SigFormer在合成数据上表现出更快的学习速度和增强的鲁棒性，特别是在存在不规则价格数据的情况下。同时，通过对冲SP 500指数的真实回测，SigFormer展示出积极的结果。 |
| [^38] | [VFedMH: Vertical Federated Learning for Training Multi-party Heterogeneous Models.](http://arxiv.org/abs/2310.13367) | VFedMH是一种垂直联合学习方法，通过在前向传播过程中聚合参与者的嵌入来处理参与者之间的异构模型，解决了现有VFL方法面临的挑战。 |
| [^39] | [Dissecting Causal Biases.](http://arxiv.org/abs/2310.13364) | 本文研究了机器学习中因数据生成和收集方式引起的因果偏差，提供了分别以模型参数为基础的四种偏差源的闭合形式表达式，可以分析它们的行为并在不同情况下判断它们的存在和最大化程度。 |
| [^40] | [Towards General Error Diagnosis via Behavioral Testing in Machine Translation.](http://arxiv.org/abs/2310.13362) | 本论文提出了一种基于双语翻译对生成的行为测试框架，用于诊断机器翻译系统的通用错误。该框架通过自动构建高质量的测试用例及其伪参考，能够在没有人工参考的情况下进行翻译质量评估。 |
| [^41] | [DeepFDR: A Deep Learning-based False Discovery Rate Control Method for Neuroimaging Data.](http://arxiv.org/abs/2310.13349) | DeepFDR是一种基于深度学习的虚警控制方法，通过利用无监督的图像分割技术解决神经影像数据中的多重检验问题，并在实验证明其相对于现有方法具有卓越的性能。 |
| [^42] | [Non-Negative Spherical Relaxations for Universe-Free Multi-Matching and Clustering.](http://arxiv.org/abs/2310.13311) | 针对多匹配和聚类问题，提出了一种新颖的非负球面松弛方法，可以自动调整连续参数，而不需要事先固定整数参数，且能够直接得到二进制结果。 |
| [^43] | [Test-Time Self-Adaptive Small Language Models for Question Answering.](http://arxiv.org/abs/2310.13307) | 本论文研究了仅使用无标记测试数据的小型自适应语言模型在问答任务中的能力，并提出了一种自适应策略，该策略通过随机生成多个答案并进行集成，以达到显著的性能提升和更高的鲁棒性。 |
| [^44] | [Decoding the Silent Majority: Inducing Belief Augmented Social Graph with Large Language Model for Response Forecasting.](http://arxiv.org/abs/2310.13297) | 通过利用大型语言模型，我们提出了一个名为SocialSense的框架，通过诱导信念增强的图和基于图的传播来预测新闻发布的响应。这个方法在没有用户明确个人资料或历史行为的情况下，能够有效地捕捉社交动态。 |
| [^45] | [CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training.](http://arxiv.org/abs/2310.13292) | 本文提出了一种面向大规模胸部 X 射线语言-图像预训练的方法，通过扩展图像-标签对为图像-文本对，并利用放射学报告中的多个图像和多个部分来解决胸部 X 射线图像中的数据稀缺性问题。我们的模型在相同条件下的训练中优于现有技术水平的模型，并通过扩充数据集提高了我们预训练模型的分类能力。 |
| [^46] | [Assessing Privacy Risks in Language Models: A Case Study on Summarization Tasks.](http://arxiv.org/abs/2310.13291) | 本研究评估了语言模型在摘要任务中的隐私风险，发现摘要模型存在泄露数据成员身份的风险，并讨论了一些保护措施和隐私与效用之间的权衡。 |
| [^47] | [Learning Recurrent Models with Temporally Local Rules.](http://arxiv.org/abs/2310.13284) | 本研究提出一种新的方法，要求生成模型学习当前状态和先前状态的联合分布，以替代传统的反向传递计算。在玩具数据集上的实验证明，这种方法可以学习到通常需要反向传递计算的数据方面。 |
| [^48] | [FedLoRA: Model-Heterogeneous Personalized Federated Learning with LoRA Tuning.](http://arxiv.org/abs/2310.13283) | FedLoRA是基于LoRA调整的计算和通信高效的模型异构个性化联邦学习框架，旨在为每个联邦学习客户端训练个性化且异构的本地模型。 |
| [^49] | [InvGC: Robust Cross-Modal Retrieval by Inverse Graph Convolution.](http://arxiv.org/abs/2310.13276) | 通过反向图卷积进行的鲁棒跨模态检索，解决了表示退化问题，并通过增加数据点之间的距离来有效分离不同模态的表示。 |
| [^50] | [Meta-learning of Physics-informed Neural Networks for Efficiently Solving Newly Given PDEs.](http://arxiv.org/abs/2310.13270) | 该论文提出了一种基于神经网络的元学习方法，用于高效求解偏微分方程问题。通过编码问题表示和采用物理学一致神经网络框架来训练模型，可以在未知解决方案的情况下评估误差。 |
| [^51] | [An Exploratory Study on Simulated Annealing for Feature Selection in Learning-to-Rank.](http://arxiv.org/abs/2310.13269) | 该研究探讨了在学习排序中使用模拟退火进行特征选择的方法，并引入了进展参数来有效遍历搜索空间。实验证明了该方法的有效性。 |
| [^52] | [DPM-Solver-v3: Improved Diffusion ODE Solver with Empirical Model Statistics.](http://arxiv.org/abs/2310.13268) | 本论文提出了DPM-Solver-v3，一个基于经验模型统计的新型快速ODE求解器，用于优化扩散概率模型的采样效率，并提出了一种新的参数化方法以减小ODE解的离散化误差。同时，引入了多步方法和预测-校正框架来进一步改善采样质量。 |
| [^53] | [On the Language Encoder of Contrastive Cross-modal Models.](http://arxiv.org/abs/2310.13267) | 本研究探究了对比跨模态模型中的语言编码器，并发现在视觉语言任务中，句子嵌入训练提高了语言编码器的质量并改善了跨模态任务的性能。而在音频语言任务中，句子嵌入训练效果较小。另外，句子嵌入训练改善了文本空间的均匀性，但降低了跨模态对齐度。 |
| [^54] | [DIG-MILP: a Deep Instance Generator for Mixed-Integer Linear Programming with Feasibility Guarantee.](http://arxiv.org/abs/2310.13261) | DIG-MILP是一个深度生成框架，利用变分自编码器从非常有限的MILP数据中提取深层结构特征，并生成与目标数据非常接近的样例。它通过利用MILP的对偶性来确保生成空间的正确性、完整性以及生成实例的有界性和可行性。 |
| [^55] | [ManiCast: Collaborative Manipulation with Cost-Aware Human Forecasting.](http://arxiv.org/abs/2310.13258) | ManiCast是一个基于成本感知的人体预测的协同操作框架，通过提供能够捕捉未来人体运动如何影响机器人计划成本的预测，实现了人机协同操纵任务的流畅执行和实时交互。 |
| [^56] | [Knowledge Graph Context-Enhanced Diversified Recommendation.](http://arxiv.org/abs/2310.13253) | 该研究在知识图谱背景下探索多样化推荐系统，通过引入创新的度量标准和评分函数，有效提高了知识图谱推荐算法的多样性。 |
| [^57] | [FLEE-GNN: A Federated Learning System for Edge-Enhanced Graph Neural Network in Analyzing Geospatial Resilience of Multicommodity Food Flows.](http://arxiv.org/abs/2310.13248) | 本文提出了FLEE-GNN，它是一种用于分析多种商品食品流的地理空间韧性的边缘增强图神经网络的联邦学习系统。FLEE-GNN克服了当前方法的局限性，提高了分析食品供应网络韧性的普适性、可扩展性和数据隐私保护。 |
| [^58] | [Transparency challenges in policy evaluation with causal machine learning -- improving usability and accountability.](http://arxiv.org/abs/2310.13240) | 透明度问题是因果机器学习在政策评估中的挑战，因为黑盒子模型难以理解和问责。本文提出了通过可解释的AI工具和简化模型来解决这些问题的方法。 |
| [^59] | [Training A Semantic Communication System with Federated Learning.](http://arxiv.org/abs/2310.13236) | 该论文介绍了用联邦学习训练语义通信系统的方法，通过利用用户数据而不泄露隐私的方式，减少了通信开销，并提高了网络资源利用率。 |
| [^60] | [Multi-level Contrastive Learning for Script-based Character Understanding.](http://arxiv.org/abs/2310.13231) | 本文提出了一种多层对比学习框架用于剧本中角色的理解，从角色的话语中学习其个性和身份，并在多个角色理解子任务上通过与强大的预训练语言模型的比较进行了验证。 |
| [^61] | [Absolute Policy Optimization.](http://arxiv.org/abs/2310.13230) | 这篇论文提出了绝对策略优化（APO）的方法，通过优化一个新颖的目标函数，在保证性能下界的同时，实现了连续控制任务和Atari游戏中的令人瞩目的结果。 |
| [^62] | [ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search.](http://arxiv.org/abs/2310.13227) | ToolChain*是一种提供了高效动作空间导航的树搜索算法，它可以在大型语言模型中进行决策和规划，解决复杂的真实世界问题。 |
| [^63] | [Scalable Neural Network Kernels.](http://arxiv.org/abs/2310.13225) | 可扩展神经网络内核（SNNKs）是一种替代常规前馈层的方法，能够近似实现常规前馈层的功能，但具有更优的计算特性。通过将内核与参数-输入向量的点积联系起来，SNNKs能够有效地解开参数与输入之间的联系，从而模拟复杂关系。此外，我们还引入了神经网络捆绑过程，将SNNKs应用于深度神经网络压缩，进一步提高了压缩效果。最终捆绑网络甚至可以绕过反向传播，通过显式公式求解最优参数。 |
| [^64] | [Equivariant Transformer is all you need.](http://arxiv.org/abs/2310.13222) | 等变Transformer能够解决自学习Monte-Carlo中的低接受率问题，实现了较大的模型容量，在模拟物理系统中具有重要的创新和贡献。 |
| [^65] | [In-context Learning with Transformer Is Really Equivalent to a Contrastive Learning Pattern.](http://arxiv.org/abs/2310.13220) | 本文将上下文学习的推理过程解释为对比学习模式中的梯度下降过程，通过建立梯度下降与自注意机制之间的关系，并分析了对应梯度下降过程，提出了可能的改进，并设计实验证明了这一观点。 |
| [^66] | [A Deep Learning Analysis of Climate Change, Innovation, and Uncertainty.](http://arxiv.org/abs/2310.13200) | 本研究使用深度学习分析了气候变化、创新和不确定性，发现模型不确定性对决策和社会估值产生首要影响，并在整合的气候经济创新框架中提出了对投资调整的重要调整。 |
| [^67] | [NameGuess: Column Name Expansion for Tabular Data.](http://arxiv.org/abs/2310.13196) | 该论文介绍了一种用于扩展表格数据中列名称的新方法，通过使用大型语言模型进行自然语言生成，解决了缩写列名称对数据搜索、访问和理解任务的负面影响。作者提出的NameGuess方法通过对表格内容和列头名称进行调整，得到了与人类相匹配的性能。 |
| [^68] | [Heterogeneous Graph Neural Networks for Data-driven Traffic Assignment.](http://arxiv.org/abs/2310.13193) | 本论文提出了一种基于异构图神经网络的数据驱动交通分配和交通流学习方法，该方法能够准确捕捉不同链路之间的空间交通模式，优于其他传统神经网络模型，在大规模网络中有着广泛的应用潜力。 |
| [^69] | [CycleNet: Rethinking Cycle Consistency in Text-Guided Diffusion for Image Manipulation.](http://arxiv.org/abs/2310.13165) | CycleNet是一种将循环一致性引入扩散模型的新方法，用于规范图像操作，具有优越的翻译一致性和质量，并且可以生成高质量的跨领域分布图像。 |
| [^70] | [Almost Equivariance via Lie Algebra Convolutions.](http://arxiv.org/abs/2310.13164) | 本文研究了几乎等变性的主题，并提供了一个不同于现有定义的几乎等变性定义，并通过利用李群的李代数给出了在模型中编码几乎等变性的实用方法。 |
| [^71] | [A Distributed Approach to Meteorological Predictions: Addressing Data Imbalance in Precipitation Prediction Models through Federated Learning and GANs.](http://arxiv.org/abs/2310.13161) | 这项研究提出了一种分布式方法来解决气象预测模型中的数据不平衡问题，通过联邦学习和生成对抗网络技术改善了模型的性能。 |
| [^72] | [Conditional Generative Modeling for Images, 3D Animations, and Video.](http://arxiv.org/abs/2310.13157) | 本论文探索了条件生成模型的新颖形式和创新应用，提出了使用神经ODE对视频动态建模，以及连续标准流的条件变体实现高分辨率图像生成。 |
| [^73] | [CLIFT: Analysing Natural Distribution Shift on Question Answering Models in Clinical Domain.](http://arxiv.org/abs/2310.13146) | 本文介绍了一个新的临床领域问答模型测试平台CLIFT，其中包括7.5k个高质量的问答样本。在原始测试集上表现出色的深度学习模型在应用于新的测试集时性能下降，表明存在分布偏移。我们的研究结果强调了提高临床领域模型鲁棒性的必要性和潜力，并提供了一个追踪该方向进展的测试平台。 |
| [^74] | [Graph Neural Networks with polynomial activations have limited expressivity.](http://arxiv.org/abs/2310.13139) | 本文证明了具有多项式激活函数的图神经网络无法表达GC2查询，与常用的非多项式激活函数存在分离，这回答了一个开放问题。 |
| [^75] | [Mean Estimation Under Heterogeneous Privacy Demands.](http://arxiv.org/abs/2310.13137) | 本文研究了异构隐私需求下的均值估计问题，提出的算法在接近线性的时间复杂度下，能够满足每个用户的个别隐私要求，并且获得极小化最优结果。最严格的用户的隐私要求决定了整体的误差率，且拥有较少但不同隐私要求的用户都会获得超过自身需求的隐私保护，且保护程度相同。这意味着对隐私不敏感的用户可以免费获得非平凡程度的隐私保护。 |
| [^76] | [Approaches for Uncertainty Quantification of AI-predicted Material Properties: A Comparison.](http://arxiv.org/abs/2310.13136) | 本论文比较了三种易于实施的方法来确定AI预测材料性能的个体不确定性。 |
| [^77] | [Deep Reinforcement Learning-based Intelligent Traffic Signal Controls with Optimized CO2 emissions.](http://arxiv.org/abs/2310.13129) | 本研究提出了一种基于深度强化学习的智能交通信号控制算法，通过优化CO2排放和行驶时间等指标，实现了较好的性能。 |
| [^78] | [Fuel Consumption Prediction for a Passenger Ferry using Machine Learning and In-service Data: A Comparative Study.](http://arxiv.org/abs/2310.13123) | 本文研究了使用机器学习和业务数据预测客运渡轮燃料消费的方法，并通过选择适当的输入变量来提高预测性能和实际可应用性。 |
| [^79] | [Understanding Addition in Transformers.](http://arxiv.org/abs/2310.13121) | 本文通过对经过训练进行整数加法的单层Transformer模型的深入分析，揭示了该模型将任务分为并行的、特定于数字的流，并针对不同的数字位置采用不同的算法，同时还发现了一种罕见的高损失的使用情况。这些发现对于机制可解释性、人工智能安全性和对齐性等方面的研究具有重要贡献。 |
| [^80] | [RSAdapter: Adapting Multimodal Models for Remote Sensing Visual Question Answering.](http://arxiv.org/abs/2310.13120) | RSAdapter是一种针对遥感视觉问答的多模态模型，通过并行适配器和线性转换层的设计，提高了运行时和参数效率。 |
| [^81] | [Semi-Supervised Learning of Dynamical Systems with Neural Ordinary Differential Equations: A Teacher-Student Model Approach.](http://arxiv.org/abs/2310.13110) | 本文提出了TS-NODE，该方法是使用神经常微分方程进行动力系统建模的首个半监督方法。TS-NODE通过生成伪样本来拓宽状态空间的探索，并利用教师-学生模型解决由于缺乏真实系统数据而带来的挑战。 |
| [^82] | [Streamlining Brain Tumor Classification with Custom Transfer Learning in MRI Images.](http://arxiv.org/abs/2310.13108) | 这项研究提出了一种用自定义迁移学习网络对MRI图像中的脑肿瘤进行分类的高效解决方案，通过使用VGG-19架构和额外的隐藏层，降低了计算复杂性但提高了准确率。 |
| [^83] | [AVTENet: Audio-Visual Transformer-based Ensemble Network Exploiting Multiple Experts for Video Deepfake Detection.](http://arxiv.org/abs/2310.13103) | 本文提出了AVTENet框架，该框架是一个基于音频-视觉Transformer的多专家集成网络，用于在视频深度伪造检测中考虑声学和视觉操作。 |
| [^84] | [Particle Guidance: non-I.I.D. Diverse Sampling with Diffusion Models.](http://arxiv.org/abs/2310.13102) | 本文提出了一种粒子引导的方法，通过超越独立样本的常见假设，提高了生成模型的多样性和采样效率。在实验中，我们在条件图像生成和分子构象生成上进行了测试，并取得了显著的结果。 |
| [^85] | [No offence, Bert -- I insult only humans! Multiple addressees sentence-level attack on toxicity detection neural network.](http://arxiv.org/abs/2310.13099) | 本论文介绍了一种简单而高效的句级攻击方法，可以通过添加积极的词语或句子来改变黑盒有毒性检测模型的预测结果，该方法在多种语言上均有效。 |
| [^86] | [SRAI: Towards Standardization of Geospatial AI.](http://arxiv.org/abs/2310.13098) | SRAI是一个Python库，用于标准化地理空间人工智能（Geospatial AI）领域工具集，包括下载地理空间数据、划分区域和训练嵌入模型等功能。 |
| [^87] | [A Multi-Stage Temporal Convolutional Network for Volleyball Jumps Classification Using a Waist-Mounted IMU.](http://arxiv.org/abs/2310.13097) | 本研究提出了一种使用腰部IMU识别排球跳跃类型的方法，采用了多层时态卷积网络（MS-TCN）进行分类，相较于最先进的深度学习模型具有更好的性能和较低的计算成本。 |
| [^88] | [Sequence Length Independent Norm-Based Generalization Bounds for Transformers.](http://arxiv.org/abs/2310.13088) | 本文提出了一种基于范数的泛化界限，适用于不依赖于输入序列长度的Transformer架构。通过使用覆盖数界限来上界Transformer的Rademacher复杂度，该方法适用于掩码预测掩码字等常见的Transformer训练技术。我们也通过模拟研究验证了理论结果的有效性。 |
| [^89] | [Unsupervised Representation Learning to Aid Semi-Supervised Meta Learning.](http://arxiv.org/abs/2310.13085) | 本论文提出了一种一次性的无监督元学习方法，通过学习训练样本的潜在表示，解决了少样本学习中的数据稀缺问题。该方法在初始化和快速适应中应用了迁移学习，并提高了准确性。 |
| [^90] | [How Can Everyday Users Efficiently Teach Robots by Demonstrations?.](http://arxiv.org/abs/2310.13083) | 通过使用不确定性度量作为准则，我们提出使用增强现实引导系统来指导人类教师提供更有效的机器人演示，从而促进机器人的高效学习。 |
| [^91] | [On the Computational Complexities of Complex-valued Neural Networks.](http://arxiv.org/abs/2310.13075) | 本论文研究了复值神经网络的计算复杂性，可以直接处理复值输入和输出信号，并使用定量计算复杂性来估计浮点操作的数量，有助于在低功耗系统中决定实施哪种算法。 |
| [^92] | [Using Logic Programming and Kernel-Grouping for Improving Interpretability of Convolutional Neural Networks.](http://arxiv.org/abs/2310.13073) | 本论文提出了一个使用逻辑编程和核组合的神经符号框架，能够提高卷积神经网络的可解释性。通过找到相似的核并生成规则集，该框架使得底层知识更加容易理解。 |
| [^93] | [Creative Robot Tool Use with Large Language Models.](http://arxiv.org/abs/2310.13065) | 本文利用大型语言模型开发了一个系统，名为RoboTool，可以使机器人在涉及隐含物理约束和长期规划的任务中创造性地使用工具。该系统通过解析自然语言指令、生成全面的策略、计算技能参数并生成可执行的Python代码来实现。实验结果表明，RoboTool能够有效地处理任务中的物理约束和环境因素。 |
| [^94] | [To grok or not to grok: Disentangling generalization and memorization on corrupted algorithmic datasets.](http://arxiv.org/abs/2310.13061) | 本研究探讨了在深度学习中的泛化和记忆的问题，通过对模数算术任务上训练的神经网络进行实验，发现网络可以同时记住损坏的标签并实现100%的泛化，并且可以通过识别和剪枝记忆化的神经元来降低对损坏数据的准确率，提高对未损坏数据的准确率。 |
| [^95] | [Robust multimodal models have outlier features and encode more concepts.](http://arxiv.org/abs/2310.13040) | 健壮的多模态模型展示了异常特征和更多概念的编码方式。 |
| [^96] | [Agri-GNN: A Novel Genotypic-Topological Graph Neural Network Framework Built on GraphSAGE for Optimized Yield Prediction.](http://arxiv.org/abs/2310.13037) | Agri-GNN是一种新型基因型-拓扑图神经网络框架，通过考虑作物之间的复杂空间和基因型相互作用，实现优化的农作物产量预测。 |
| [^97] | [LASER: Linear Compression in Wireless Distributed Optimization.](http://arxiv.org/abs/2310.13033) | LASER是一种新的压缩方案，通过利用梯度的低秩结构在噪声通道上高效传输梯度，相对于现有方案在计算机视觉和GPT语言建模任务上表现出持续优势。 |
| [^98] | [Quality-Diversity through AI Feedback.](http://arxiv.org/abs/2310.13032) | 基于AI反馈的质量-多样性（QDAIF）算法利用语言模型来生成和评估创造性写作，比传统算法更广泛地覆盖高质量样本的搜索空间。 |
| [^99] | [A Use Case: Reformulating Query Rewriting as a Statistical Machine Translation Problem.](http://arxiv.org/abs/2310.13031) | 本文提出了一种基于统计机器翻译的查询重写方法，通过学习重写阿拉伯语用户查询以改善搜索引擎的检索结果。 |
| [^100] | [Blending gradient boosted trees and neural networks for point and probabilistic forecasting of hierarchical time series.](http://arxiv.org/abs/2310.13029) | 本文提出了一种融合梯度提升树和神经网络的方法，有效解决了层次时间序列的点预测和概率预测问题。 |
| [^101] | [Be Bayesian by Attachments to Catch More Uncertainty.](http://arxiv.org/abs/2310.13027) | 本文提出了一种附加结构贝叶斯神经网络(ABNN)，通过在主干网络中整合足够分布外数据的不确定性，来提高神经网络对不确定性的捕捉能力。 |
| [^102] | [Towards Anytime Fine-tuning: Continually Pre-trained Language Models with Hypernetwork Prompt.](http://arxiv.org/abs/2310.13024) | 本研究旨在解决持续预训练方法在未知领域上性能下降的问题，提出了一种基于超网络提示的持续预训练方法，通过使用一致性和不一致性损失来生成领域特定的提示，显著减轻了领域特异性并促进了知识的传递。 |
| [^103] | [Uncertainty-aware Parameter-Efficient Self-training for Semi-supervised Language Understanding.](http://arxiv.org/abs/2310.13022) | 本论文提出了一种不确定性感知的参数高效自训练（UPET）框架，通过利用Monte Carlo dropout来进行贝叶斯神经网络中的不确定性估计，并根据置信度和确定性选择可靠的伪标记样本，以解决标记数据稀缺问题。 |
| [^104] | [Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class Manipulation Using DeepFool Algorithm.](http://arxiv.org/abs/2310.13019) | 本文提出了一种增强版DeepFool算法，名为Targeted DeepFool，可以针对特定类别进行错误分类，并引入了最小置信度分数要求超参数来提高灵活性。 |
| [^105] | [Getting aligned on representational alignment.](http://arxiv.org/abs/2310.13018) | 该论文研究了生物和人工信息处理系统的表示一致性，探讨了不同系统之间的表示是否一致以及如何调整表示以更好地匹配其他系统。为了改善领域之间的交流，提出了一个统一的框架作为共同语言。 |
| [^106] | [Position Interpolation Improves ALiBi Extrapolation.](http://arxiv.org/abs/2310.13017) | 该论文提出了一种使用线性位置插值来改进ALiBi模型外推能力的方法，并且在上游语言建模和下游摘要和检索任务中取得了显著的改进。 |
| [^107] | [Large Language Model Prediction Capabilities: Evidence from a Real-World Forecasting Tournament.](http://arxiv.org/abs/2310.13014) | 该论文通过参与Metaculus平台举办的预测竞赛，实证测试了OpenAI的最先进大型语言模型GPT-4的概率预测能力，并发现其与人类预测相比明显不准确。 |
| [^108] | [Compositional preference models for aligning LMs.](http://arxiv.org/abs/2310.13011) | 用于对齐语言模型的组合偏好模型（CPMs）是一种新颖的偏好模型框架，可以分解全局偏好评估并根据可解释的特征进行标量评分，得到更好的泛化能力和鲁棒性。 |
| [^109] | [LoBaSS: Gauging Learnability in Supervised Fine-tuning Data.](http://arxiv.org/abs/2310.13008) | 本文介绍了一种新的方法LoBaSS，利用数据的可学习性作为选择监督微调数据的主要标准。这种方法可以根据模型的能力将数据选择与模型对齐，确保高效的学习。 |
| [^110] | [Software Metadata Classification based on Generative Artificial Intelligence.](http://arxiv.org/abs/2310.13006) | 本文提出了一种利用生成式人工智能在二进制代码评论质量分类模型中提升性能的新方法，通过引入生成数据集，模型的准确性得到了显著改善，支持向量机模型的精确度提高了6%，人工神经网络模型的召回率提高了1.5%。 |
| [^111] | [Progressively Efficient Learning.](http://arxiv.org/abs/2310.13004) | CEIL是一种渐进高效的学习框架，通过给学习代理提供一个抽象、动态的语言和一种内在的动机，以尽可能少的沟通代价学习，实现了类似人类逐步高效沟通的能力。在2D MineCraft领域上，CEIL展示了令人印象深刻的性能和沟通效率。 |
| [^112] | [Conversational Financial Information Retrieval Model (ConFIRM).](http://arxiv.org/abs/2310.13001) | ConFIRM是一种会话式金融信息检索模型，通过合成金融领域特定问答对和评估参数微调方法，实现了超过90%的准确性，为金融对话系统提供了数据高效的解决方案。 |
| [^113] | [Zero-shot Learning of Drug Response Prediction for Preclinical Drug Screening.](http://arxiv.org/abs/2310.12996) | 本文提出了一种无监督学习的解决方案，可用于临床药物筛选中的药物反应预测。通过学习相似药物的先前反应数据，该方法能够增强对未标记化合物的实时预测。 |
| [^114] | [Wave-informed dictionary learning for high-resolution imaging in complex media.](http://arxiv.org/abs/2310.12990) | 本论文提出了一种在复杂介质中利用大量多样的数据进行高分辨率成像的方法，通过字典学习和多维尺度缩放技术实现。通过模拟实验验证了该方法的有效性。 |
| [^115] | [Training Dynamics of Deep Network Linear Regions.](http://arxiv.org/abs/2310.12977) | 该研究探讨了深度神经网络的线性区域训练动力学。通过对数据点周围线性区域的局部复杂性进行统计分析，发现训练过程中这种复杂性经历了几个阶段的变化。最后，通过精确的可视化方法，观察到训练的最后阶段有一个下降趋势。 |
| [^116] | [Conditional Density Estimations from Privacy-Protected Data.](http://arxiv.org/abs/2310.12781) | 本文提出了一种从隐私保护数据中进行条件密度估计的方法，使用神经条件密度估计器来近似模型参数的后验分布，从而解决了在统计分析过程中只能访问私有化数据导致的计算复杂度增加的问题。 |
| [^117] | [Representation Learning via Consistent Assignment of Views over Random Partitions.](http://arxiv.org/abs/2310.12692) | 本论文提出了一种称为CARP的自监督聚类方法，通过对随机分区的视图进行一致分配，实现了可靠的表示学习，同时提高了训练稳定性和防止了崩溃解决方案的出现。在广泛的评估中，证明CARP的表示适用于多种下游任务，并与11种现有的自监督方法进行了比较。 |
| [^118] | [SecurityNet: Assessing Machine Learning Vulnerabilities on Public Models.](http://arxiv.org/abs/2310.12665) | 本研究通过在公共模型上评估攻击和防御来全面理解机器学习模型的漏洞，以解决对高计算资源要求较高的问题。 |
| [^119] | [Towards a Deep Learning-based Online Quality Prediction System for Welding Processes.](http://arxiv.org/abs/2310.12632) | 该论文提出了一个基于深度学习的气体金属电弧焊接预测质量系统的概念，通过收集和管理多传感器数据以及实时处理和特征工程的方式，可以识别关系并预测焊接质量。 |
| [^120] | [DA-TransUNet: Integrating Spatial and Channel Dual Attention with Transformer U-Net for Medical Image Segmentation.](http://arxiv.org/abs/2310.12570) | DA-TransUNet 提出了一种新的深度医学图像分割框架，集成了Transformer和双重注意力块。通过注意机制和多方面特征提取，提升医学图像分割结果。 |
| [^121] | [Explanation-Based Training with Differentiable Insertion/Deletion Metric-Aware Regularizers.](http://arxiv.org/abs/2310.12553) | 提出一种插入/删除指标感知的基于解释的优化(ID-ExpO)方法，通过优化可区分的预测器来提高解释的插入和删除得分，并保持预测准确性。实验结果表明，ID-ExpO能够使流行的事后解释器产生更忠实的解释。 |
| [^122] | [Equivariant Bootstrapping for Uncertainty Quantification in Imaging Inverse Problems.](http://arxiv.org/abs/2310.11838) | 本文提出了一种新的不确定性量化方法，利用参数引导算法的等变形式，可以在成像反问题中量化重构图像的不确定性，并且可以与任何图像重建技术结合使用。 |
| [^123] | [Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis.](http://arxiv.org/abs/2310.10477) | 该论文介绍了一种基于错误分析的对齐策略，通过暴露大型语言模型的错误输出并进行评估，以理解内部原因。通过这种方法，有毒回应可以转化为模型对齐的指导调谐语料，从而提高模型的安全性并训练其进行自我批评。 |
| [^124] | [Hamming Encoder: Mining Discriminative k-mers for Discrete Sequence Classification.](http://arxiv.org/abs/2310.10321) | 提出了一种名为Hamming编码器的新方法，利用二进制的1D卷积神经网络来挖掘区分性k-mer集，解决了基于模式的方法中存在的挑战。 |
| [^125] | [Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook.](http://arxiv.org/abs/2310.10196) | 这篇综述探讨了大型模型在时间序列和时空数据中的应用。它们不仅带来了增强的模式识别和推理能力，还为人工通用智能打下了基础。 |
| [^126] | [Label Differential Privacy via Aggregation.](http://arxiv.org/abs/2310.10092) | 以前研究表明朴素的LBA和LLP不能提供标签差分隐私。但本研究显示，使用具有随机抽样的加权LBA可以提供标签差分隐私。 |
| [^127] | [Towards Better Evaluation of Instruction-Following: A Case-Study in Summarization.](http://arxiv.org/abs/2310.08394) | 本论文通过收集真实数据集riSum，对大型语言模型的指令遵循能力进行了评估，并提出了新的参考方法，以衡量这种能力。这些方法在性能上与需要高质量摘要的参考方法相当。 |
| [^128] | [PICProp: Physics-Informed Confidence Propagation for Uncertainty Quantification.](http://arxiv.org/abs/2310.06923) | 本文提出了一种名为PICProp的方法，基于双层优化，用于在深度学习和物理信息学习中进行不确定性量化。该方法能够在不进行强大假设的情况下计算有效的置信区间（CI），并且通过传播置信度实现了数据位置到整个域的置信度传播。 |
| [^129] | [On Double-Descent in Reinforcement Learning with LSTD and Random Features.](http://arxiv.org/abs/2310.05518) | 本文研究了在强化学习中网络大小和L2正则化对性能的影响，并观察到了双下降现象。通过使用随机特征和懒惰训练策略，在参数和状态数无限大的情况下研究了正则化的最小二乘时间差分算法，得出了其收敛性和最优性，并阐述了双下降现象在该算法中的影响。 |
| [^130] | [Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models.](http://arxiv.org/abs/2310.05253) | 本文提出了一种基于知识的推理方法，通过大型语言模型实现可解释的主张验证。该方法不需要依赖昂贵的标注数据，能够验证复杂的主张并生成解释，对协助人工事实检查员具有重要意义。 |
| [^131] | [Physics-aware Machine Learning Revolutionizes Scientific Paradigm for Machine Learning and Process-based Hydrology.](http://arxiv.org/abs/2310.05227) | 物理感知机器学习是一种革命性方法，它将物理知识和机器学习相结合，提供了准确的水文学理解和水循环预测，对于管理水资源以应对气候变化等挑战具有重要意义。 |
| [^132] | [SE(3)-Stochastic Flow Matching for Protein Backbone Generation.](http://arxiv.org/abs/2310.02391) | 通过SE(3)-Stochastic Flow Matching，我们提出了一系列新型生成模型FoldFlow，可以准确建模蛋白质主链。这些模型通过无需模拟训练和Riemannian最优传输的结合，具有更好的稳定性和建模能力。 |
| [^133] | [On the definition of toxicity in NLP.](http://arxiv.org/abs/2310.02357) | 这项研究探讨了毒性的定义模糊性问题，并提出了一种基于定量压力的毒性定义来弥补现有定义的缺点。 |
| [^134] | [Self-Supervised Terrain Representation Learning from Unconstrained Robot Experience.](http://arxiv.org/abs/2309.15302) | 提出一种名为STERLING的自我监督地形表示学习方法，通过无约束的机器人经验学习有关地形的相关表示，以实现地形感知导航。 |
| [^135] | [Improving VTE Identification through Adaptive NLP Model Selection and Clinical Expert Rule-based Classifier from Radiology Reports.](http://arxiv.org/abs/2309.12273) | 通过自适应的NLP模型选择和临床专家规则的分类器，该研究提出一种改进VTE识别的新方法，在放射学报告中准确识别VTE事件的准确性得到提高。 |
| [^136] | [FedDCSR: Federated Cross-domain Sequential Recommendation via Disentangled Representation Learning.](http://arxiv.org/abs/2309.08420) | 提出了一种名为FedDCSR的联邦跨领域顺序推荐框架，通过解缠表示学习来处理不同领域之间的序列特征异质性，并保护数据隐私。 |
| [^137] | [Modeling Supply and Demand in Public Transportation Systems.](http://arxiv.org/abs/2309.06299) | 该论文在公共交通系统中建立了供需模型，利用数据分析和机器学习技术揭示了运营服务中的空缺。 |
| [^138] | [On the quality of randomized approximations of Tukey's depth.](http://arxiv.org/abs/2309.05657) | 本文研究了Tukey深度的随机近似质量问题，证明了在维度较高且数据从对数凹集的均匀分布中抽样的情况下，随机算法可以正确近似最大深度和接近零的深度，而对于中间深度的点，任何好的近似都需要指数复杂度。 |
| [^139] | [CONVERT:Contrastive Graph Clustering with Reliable Augmentation.](http://arxiv.org/abs/2308.08963) | 本论文提出了一种名为COVERT的对比图聚类网络，通过可逆扰动恢复网络处理数据增强，提炼可靠的语义信息，进一步保证对比学习中语义的可靠性。 |
| [^140] | [Generalizing Topological Graph Neural Networks with Paths.](http://arxiv.org/abs/2308.06838) | 本论文提出了一种通用的拓扑图神经网络（GNNs）方法，通过强调图中的路径，突破了1-Weisfeiler-Lehmann测试的理论限制，并在无需对图的子结构进行假设的情况下，在多个基准测试中取得了最先进的性能。 |
| [^141] | [Dimensionality Reduction for Improving Out-of-Distribution Detection in Medical Image Segmentation.](http://arxiv.org/abs/2308.03723) | 本研究提出了一种降维方法，通过在医学图像分割中应用马氏距离后处理和主成分分析，实现对越界图像的高效检测。 |
| [^142] | [The Effect of Spoken Language on Speech Enhancement using Self-Supervised Speech Representation Loss Functions.](http://arxiv.org/abs/2307.14502) | 本文研究了在语音增强中，使用与嘈杂数据语言完全匹配的自监督语音表示损失函数训练的模型的性能更好。与传统的频谱图或时间域损失函数相比，这些增强模型具有特定语言的特性。 |
| [^143] | [Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models.](http://arxiv.org/abs/2307.11224) | Jina Embeddings是一组高性能的句子嵌入模型，能够捕捉文本的语义本质。该论文详细介绍了Jina Embeddings的开发过程，并通过性能评估验证了其优越性能。 |
| [^144] | [Predicting Battery Lifetime Under Varying Usage Conditions from Early Aging Data.](http://arxiv.org/abs/2307.08382) | 本研究通过提取早期寿命中的容量-电压数据的新特征，成功预测了在不同使用条件下的电池寿命。我们的研究结果表明，这些早期特征能够很好地捕捉电池的健康状态和老化模式的变化速率，为电池寿命预测提供了有希望的途径。 |
| [^145] | [Revisiting Implicit Models: Sparsity Trade-offs Capability in Weight-tied Model for Vision Tasks.](http://arxiv.org/abs/2307.08013) | 本研究重新审视了隐式模型，并发现权重绑定模型在视觉任务中比DEQ变体更有效、稳定和高效。通过使用不同的稀疏掩模，我们提出了提高模型容量的方法。 |
| [^146] | [Neural Video Recovery for Cloud Gaming.](http://arxiv.org/abs/2307.07847) | 本文提出了一个新方法，用于在云游戏中恢复丢失或损坏的视频帧。与传统方法不同的是，我们利用游戏状态和部分解码帧来提高恢复准确性。 |
| [^147] | [Elastic Decision Transformer.](http://arxiv.org/abs/2307.02484) | 弹性决策变压器（EDT）通过在测试时间进行动作推断时调整历史长度来实现轨迹拼接，填补了决策变压器（DT）在这一方面的性能差距，并且在多任务情况下胜过基于Q-Learning的方法。 |
| [^148] | [Act3D: Infinite Resolution Action Detection Transformer for Robotic Manipulation.](http://arxiv.org/abs/2306.17817) | Act3D是一种基于Transformer的操作策略，将6自由度关键姿势预测作为3D检测任务，并以自适应空间计算的方式进行处理。它在高度精确的机器人操纵任务中取得了显著的性能改进。 |
| [^149] | [CoarsenConf: Equivariant Coarsening with Aggregated Attention for Molecular Conformer Generation.](http://arxiv.org/abs/2306.14852) | CoarsenConf是一种用于分子构象生成的等变粗化方法，通过聚合注意机制恢复从粗粒度表示中得到的细粒度原子坐标，实现了高效准确的构象生成。 |
| [^150] | [FuXi: A cascade machine learning forecasting system for 15-day global weather forecast.](http://arxiv.org/abs/2306.12873) | 逐步级联模型FuXi在15天全球天气预报中表现出更好的性能，并通过减少预测误差的积累达到优化。 |
| [^151] | [Otter-Knowledge: benchmarks of multimodal knowledge graph representation learning from different sources for drug discovery.](http://arxiv.org/abs/2306.12802) | 本研究在药物发现方面使用了多模态知识图谱表示学习，获得了最先进的结果。他们整合了来自不同来源的数据，并提供了基于这些数据的预训练模型。 |
| [^152] | [Trained Transformers Learn Linear Models In-Context.](http://arxiv.org/abs/2306.09927) | 本文研究了Transformer在具有单层线性自注意层的线性回归任务上通过梯度流进行训练的ICL机制，揭示了梯度流具有找到目标函数全局最小值的能力。 |
| [^153] | [A Primal-Dual-Critic Algorithm for Offline Constrained Reinforcement Learning.](http://arxiv.org/abs/2306.07818) | PDCA是一种用于离线约束强化学习的算法，它可以通过在Lagrangian函数上运行原始-对偶算法来找到近似鞍点，而无需集中性和强Bellman完备性假设。 |
| [^154] | [Generalizable Lightweight Proxy for Robust NAS against Diverse Perturbations.](http://arxiv.org/abs/2306.05031) | 本研究提出了一种轻量级的、零成本代理，通过考虑干净图像和受扰动图像的特征、参数和梯度的一致性，实现了高效、快速地搜索鲁棒NAS架构。 |
| [^155] | [Inference-Time Intervention: Eliciting Truthful Answers from a Language Model.](http://arxiv.org/abs/2306.03341) | 本研究提出推理时间干预（ITI）技术，通过在推理过程中跨越有限数量的注意力头，显着提高大型语言模型的真实性。在TruthfulQA基准上，ITI使LLaMA模型的真实性从32.5%提高到65.1%。ITI是一种最小程度的干扰，计算廉价，且数据效率高。 |
| [^156] | [Unbiased Compression Saves Communication in Distributed Optimization: When and How Much?.](http://arxiv.org/abs/2305.16297) | 本文研究了分布式优化中通信压缩的效果，探讨了无偏压缩降低总通信成本的条件和程度。 |
| [^157] | [Language Model Tokenizers Introduce Unfairness Between Languages.](http://arxiv.org/abs/2305.15425) | 语言模型的分词器在不同语言之间引入了不公平现象，因为同一段文本翻译成不同的语言可能会导致极大的分词长度差异，这影响了一些语言社区在获取商业语言服务的成本、处理时间和延迟以及提供给机器学习模型的内容量方面存在不公平待遇。 |
| [^158] | [LMs with a Voice: Spoken Language Modeling beyond Speech Tokens.](http://arxiv.org/abs/2305.15255) | SPECTRON是一个新颖的语音延续模型，通过利用预训练的语言模型和语音编码器进行端到端的训练来生成文本和语音输出，在语义内容和讲话者保护方面超越了现有的口语语言模型。 |
| [^159] | [A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis.](http://arxiv.org/abs/2305.15054) | 本研究通过因果中介分析框架对基于Transformer的语言模型在算术问题上进行了机制解释，发现语言模型通过注意机制传输与查询相关的信息，并通过一组MLP模块进行处理。 |
| [^160] | [Newton-Cotes Graph Neural Networks: On the Time Evolution of Dynamic Systems.](http://arxiv.org/abs/2305.14642) | 本文提出了基于Newton-Cotes公式的方法来预测动态系统的时间演化，与现有最先进的方法相比较， 实验结果表明该方法有着显著的改进。 |
| [^161] | [Learning Semantic Role Labeling from Compatible Label Sequences.](http://arxiv.org/abs/2305.14600) | 该论文探讨了如何从不相交的兼容标签序列中高效地学习，将此运用于语义角色标注任务，提出了联合处理VerbNet和PropBank标签的方法，并验证了其有效性。 |
| [^162] | [What Comes Next? Evaluating Uncertainty in Neural Text Generators Against Human Production Variability.](http://arxiv.org/abs/2305.11707) | 本文分析了神经文本生成器与人类生产变异性之间的不确定性，通过探测生成器的输出空间来测量其对人类生产变异性的校准程度，并证明用多个样本和多个参考可以更好地了解模型的不确定性表示。 |
| [^163] | [Tinto: Multisensor Benchmark for 3D Hyperspectral Point Cloud Segmentation in the Geosciences.](http://arxiv.org/abs/2305.09928) | Tinto是一个多传感器数字露头基准测试数据集，旨在促进开发和验证地质制图的深度学习方法，特别是针对非结构化的3D数据如点云。 |
| [^164] | [INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Large Language Models.](http://arxiv.org/abs/2305.06677) | 本文提出了一种使用信息丰富的数据子集来高效预训练大型语言模型的方法，减少了训练时间和计算成本，同时保持了模型的泛化能力。 |
| [^165] | [Performance Gaps of Artificial Intelligence Models Screening Mammography -- Towards Fair and Interpretable Models.](http://arxiv.org/abs/2305.04422) | 该研究探讨了乳腺筛查 X 光片异常分类模型中的性能差距，尤其是与人口学和成像特征之间的关系，旨在开发公平和可解释的模型。 |
| [^166] | [On Preimage Approximation for Neural Networks.](http://arxiv.org/abs/2305.03686) | 本文提出了一种基于线性松弛的高效实用的任意时刻算法，用于生成神经网络原像的符号下近似，以实现更快的改进和更高的压缩度。 |
| [^167] | [Verifiable Learning for Robust Tree Ensembles.](http://arxiv.org/abs/2305.03626) | 本论文提出了一种可验证学习的方法，即训练易于验证的限制模型类来解决决策树集成的 NP-hard 问题，并成功设计出一种新的训练算法，使得在多项式时间内可以进行安全验证，而且仍保持着该领域最好的鲁棒性能。 |
| [^168] | [DataComp: In search of the next generation of multimodal datasets.](http://arxiv.org/abs/2304.14108) | DataComp是一个基准测试，旨在通过提出新的训练集来解决数据集在机器学习生态系统中的缺陷。它提供了一个多规模设计的实验测试平台，使用12.8B个图像-文本对的新候选池，让研究人员可以通过设计新的过滤技术或策划新的数据源并评估它们的新数据集来进行创新。 |
| [^169] | [Hierarchical Graph Neural Network with Cross-Attention for Cross-Device User Matching.](http://arxiv.org/abs/2304.03215) | 本文提出了一种带有跨设备交叉注意力的分层图神经网络(HGNN)，用于解决跨设备用户匹配问题，相对于最先进的TGCE方法，提高了5%的性能。 |
| [^170] | [Anti-DreamBooth: Protecting users from personalized text-to-image synthesis.](http://arxiv.org/abs/2303.15433) | 本文介绍了一种名为反梦幻机的防御系统，通过向用户图像添加微小的噪声扰动来保护用户免受个性化文本生成图像的恶意使用，从而避免产生虚假新闻或针对个人受害者的令人不安的内容。 |
| [^171] | [CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models.](http://arxiv.org/abs/2302.12343) | 提出了CHiLL，一种用于从医疗记录中提取特征的方式，通过对大型语言模型进行查询生成特征，使医生能够用自己的专业知识制作对下游任务有临床意义的特征，并且该方法在性能和可解释性方面表现出良好的结果。 |
| [^172] | [Adaptive Selective Sampling for Online Prediction with Experts.](http://arxiv.org/abs/2302.08397) | 针对带有专家建议的在线预测，我们提出了使用选择性采样方案的标签高效预测算法，能够使用比标准程序少得多的标签，并保持最优最坏情况后悔保证。对于在期望上明显更好的专家，算法的标签复杂度与回合数的平方根成比例，并且在实验中表现出与池式主动学习的极小极大速率相匹配的归一化后悔。 |
| [^173] | [Kernel Ridge Regression Inference.](http://arxiv.org/abs/2302.06578) | 我们提供了核岭回归方法的一致推断和置信带，为广泛应用于各种数据类型的非参数回归估计器提供了准确的统计推断方法。 |
| [^174] | [A critical look at deep neural network for dynamic system modeling.](http://arxiv.org/abs/2301.11604) | 这项研究对使用深度神经网络进行动态系统建模的能力进行了批判性分析，比较了两种神经网络模型（LSTM和CFNN）与标准系统辨识方法（PEM）的性能，并指出了神经网络建模可能存在的缺陷和被忽视的问题。 |
| [^175] | [Learning to Control and Coordinate Mixed Traffic Through Robot Vehicles at Complex and Unsignalized Intersections.](http://arxiv.org/abs/2301.05294) | 本研究提出了一种去中心化的多智能体强化学习方法，用于控制和协调混合交通，特别是人驾驶车辆和机器人车辆在实际复杂交叉口的应用。实验结果表明，使用5%的机器人车辆可以有效防止交叉口内的拥堵形成。 |
| [^176] | [A Scalable Technique for Weak-Supervised Learning with Domain Constraints.](http://arxiv.org/abs/2301.05253) | 我们提出了一种使用领域约束的可扩展技术，用于弱监督学习神经网络分类未标记数据。我们通过在MNIST图像分类问题上的实验证明，我们的方法比以前的方法在规模上有显著的提升。 |
| [^177] | [Progress measures for grokking via mechanistic interpretability.](http://arxiv.org/abs/2301.05217) | 该论文通过机械解释性的方法找到了连续的进展测量，以理解神经网络中 emergent behavior 的产生原因。其中，作者以“grokking”现象为案例，通过逆向工程的方式完全理解了小型transformer网络在模块化加法任务中的算法，并定义了进展测量来研究这个现象的进展性质。 |
| [^178] | [Unleashing the Power of Shared Label Structures for Human Activity Recognition.](http://arxiv.org/abs/2301.03462) | 本文提出了SHARE，一种考虑共享标签结构的HAR框架，通过建模共同的结构从而揭示不同活动之间的知识。 |
| [^179] | [On the Overlooked Structure of Stochastic Gradients.](http://arxiv.org/abs/2212.02083) | 本文对深度学习中随机梯度的结构进行了正式的统计检验，发现逐维梯度通常呈现幂律重尾，而逐次迭代的梯度和随机梯度噪声通常不呈现幂律重尾。 |
| [^180] | [Karyotype AI for Precision Oncology.](http://arxiv.org/abs/2211.14312) | 本研究针对精准肿瘤学中的染色体分析问题，通过使用Fred Hutchinson癌症研究中心的大量数据，利用深度学习模型和拓扑视觉转换器(TopViTs)，成功开发出了一种自动识别染色体异常的方法。 |
| [^181] | [AF Adapter: Continual Pretraining for Building Chinese Biomedical Language Model.](http://arxiv.org/abs/2211.11363) | 我们提出了一种连续预训练方法，名为Attention-FFN Adapter，用于构建中文生物医学领域的语言模型。实验证明，该方法在性能上相对于强基准模型平均提高了0.6%至2%。同时，该方法还有效缓解了灾难性的遗忘问题。 |
| [^182] | [Consistent and Truthful Interpretation with Fourier Analysis.](http://arxiv.org/abs/2210.17426) | 该论文提出了一个称为真实解释的新概念，通过傅立叶分析获得严格保证，并在实验中证明了其在支持假设情景和降低解释误差方面的优势。 |
| [^183] | [Online Probabilistic Model Identification using Adaptive Recursive MCMC.](http://arxiv.org/abs/2210.12595) | 提出了在线自适应递归马尔可夫链蒙特卡洛（ARMCMC）方法用于概率模型识别，克服了传统在线技术的缺点，并计算了模型参数的整个概率分布函数。 |
| [^184] | [Event-Triggered Time-Varying Bayesian Optimization.](http://arxiv.org/abs/2208.10790) | 本文提出了一种事件触发算法ET-GP-UCB，用于解决时变贝叶斯优化中的探索和开发的权衡问题。通过基于高斯过程回归的概率均匀误差界，算法能够在未知变化速率的情况下自适应地适应实际的时间变化。数值实验结果表明，ET-GP-UCB在合成和实际数据上优于现有算法。 |
| [^185] | [DESCN: Deep Entire Space Cross Networks for Individual Treatment Effect Estimation.](http://arxiv.org/abs/2207.09920) | 本论文提出了DESCN模型，通过深度整体空间交叉网络的方式，从端到端的角度建模个体治疗效果估计。该模型可以解决传统方法中的分布偏移和样本不平衡问题。 |
| [^186] | [Discretization Invariant Networks for Learning Maps between Neural Fields.](http://arxiv.org/abs/2206.01178) | 该论文提出了一种用于学习神经场之间映射的离散不变网络（DI-Nets）框架，并通过分析得出模型输出在不同离散化下的上界，并强调了点集差异在界限表征方面的重要性。通过数值积分进行低差异度离散化的网络设计，证明DI-Nets可以普遍逼近一类映射。 |
| [^187] | [Uniform Complexity for Text Generation.](http://arxiv.org/abs/2204.05185) | 提出了一种新的基准测试，称为统一复杂度文本生成（UCTG），要求生成模型在不同的文本提示情况下遵循统一的语言属性。该测试使用了150多个与人类和生成文本复杂度相关的特征进行评估。 |
| [^188] | [Representation Learning via Consistent Assignment of Views to Clusters.](http://arxiv.org/abs/2112.15421) | 该论文介绍了一种名为CARL的无监督学习方法，通过将对比学习和深度聚类相结合，通过学习一组通用原型来实现一致分配视图到聚类中进行表示学习。CARL在多个表示学习基准测试中超越了其他方法。 |
| [^189] | [Numerical influence of ReLU'(0) on backpropagation.](http://arxiv.org/abs/2106.12915) | 本研究研究了ReLU'(0)值对深度学习中反向传播的数值影响，发现在32位精度下会出现显著的变化，而在16位精度下是系统性的。在普通的SGD训练中，选择ReLU'(0) = 0似乎是最有效的。此外，重新调整方法 tend to buffer ReLU'(0)值的影响。 |
| [^190] | [Lifetime policy reuse and the importance of task capacity.](http://arxiv.org/abs/2106.01741) | 本文提出了生命周期策略重用算法和任务容量指标，通过优化一组近似最优策略和策略选择，实现了在终身强化学习中避免生成大量策略的目标。实验证明了生命周期策略重用和任务容量预选对多任务学习的重要性。 |
| [^191] | [On the Overlooked Pitfalls of Weight Decay and How to Mitigate Them: A Gradient-Norm Perspective.](http://arxiv.org/abs/2011.11152) | 本论文发现了权重衰减在训练的最后阶段会导致大梯度范数的陷阱，为了解决这个问题，提出了Scheduled Weight Decay（SWD）方法，通过动态调整权重衰减强度并惩罚大梯度范数，可以有效缓解这个问题。 |
| [^192] | [Interpretable Sequence Classification Via Prototype Trajectory.](http://arxiv.org/abs/2007.01777) | ProtoryNet是一种基于原型轨迹的可解释深度神经网络，它通过捕捉时间模式和原型的近似程度来进行文本分类，并实现了直观和细致的推理过程解释。 |

# 详细

[^1]: 分析不同被动收集的数据对于预测压力和抑郁的贡献

    Analyzing the contribution of different passively collected data to predict Stress and Depression. (arXiv:2310.13607v1 [cs.LG])

    [http://arxiv.org/abs/2310.13607](http://arxiv.org/abs/2310.13607)

    本研究分析了不同的被动收集数据类型对于预测压力和抑郁的贡献。结果显示，WiFi特征和电话日志特征对于压力和抑郁预测非常重要。

    

    从被动捕获的数据中识别人类行为和环境背景的可能性推动了其在心理健康评估中的使用。本文分析了不同被动收集传感器数据类型（WiFi、GPS、社交互动、电话日志、体力活动、音频和学术特征）对于预测每日的自我报告压力和PHQ-9抑郁得分的贡献。首先，我们从原始原始数据中计算出125个中级特征。这些125个特征包括来自不同传感器数据类型的特征组。然后，我们通过比较使用所有特征训练的神经网络模型与使用特定特征组训练的神经网络模型的性能，来评估每个特征类型的贡献。我们的结果表明，WiFi特征（编码移动模式）和电话日志特征（编码与睡眠模式相关的信息）为压力和抑郁预测提供了显著的信息。

    The possibility of recognizing diverse aspects of human behavior and environmental context from passively captured data motivates its use for mental health assessment. In this paper, we analyze the contribution of different passively collected sensor data types (WiFi, GPS, Social interaction, Phone Log, Physical Activity, Audio, and Academic features) to predict daily selfreport stress and PHQ-9 depression score. First, we compute 125 mid-level features from the original raw data. These 125 features include groups of features from the different sensor data types. Then, we evaluate the contribution of each feature type by comparing the performance of Neural Network models trained with all features against Neural Network models trained with specific feature groups. Our results show that WiFi features (which encode mobility patterns) and Phone Log features (which encode information correlated with sleep patterns), provide significative information for stress and depression prediction.
    
[^2]: ReLM:利用语言模型提升化学反应预测能力

    ReLM: Leveraging Language Models for Enhanced Chemical Reaction Prediction. (arXiv:2310.13590v1 [cs.LG])

    [http://arxiv.org/abs/2310.13590](http://arxiv.org/abs/2310.13590)

    ReLM是一种通过利用语言模型来辅助图神经网络(GNN)从而提高化学反应预测准确性的新框架,在不同化学反应数据集上表现出色，尤其是在超出分布范围的情况下。

    

    预测化学反应是化学中一个基本的挑战，涉及从给定的反应过程中预测出产物。传统的技术，特别是使用图神经网络（GNN）的技术，通常受到训练数据不足和无法利用文本信息的限制，从而限制了它们在实际应用中的适用性。在这项工作中，我们提出了一种新颖的框架ReLM，它利用语言模型（LM）中编码的化学知识来辅助GNN，从而提高了实际化学反应预测的准确性。为了进一步提高模型的鲁棒性和可解释性，我们还引入了置信度评分策略，使LM能够自我评估其预测的可靠性。我们的实验结果表明，ReLM改进了各种化学反应数据集上先进GNN方法的性能，特别是在超出分布范围的情况下。

    Predicting chemical reactions, a fundamental challenge in chemistry, involves forecasting the resulting products from a given reaction process. Conventional techniques, notably those employing Graph Neural Networks (GNNs), are often limited by insufficient training data and their inability to utilize textual information, undermining their applicability in real-world applications. In this work, we propose ReLM, a novel framework that leverages the chemical knowledge encoded in language models (LMs) to assist GNNs, thereby enhancing the accuracy of real-world chemical reaction predictions. To further enhance the model's robustness and interpretability, we incorporate the confidence score strategy, enabling the LMs to self-assess the reliability of their predictions. Our experimental results demonstrate that ReLM improves the performance of state-of-the-art GNN-based methods across various chemical reaction datasets, especially in out-of-distribution settings. Codes are available at https
    
[^3]: 通过子树感知的单词重排序提升跨语言转移能力

    Improving Cross-Lingual Transfer through Subtree-Aware Word Reordering. (arXiv:2310.13583v1 [cs.CL])

    [http://arxiv.org/abs/2310.13583](http://arxiv.org/abs/2310.13583)

    通过子树感知的单词重排序方法能够提高跨语言转移的能力，并克服了现有方法中语言特定规则、POS标签级别或只针对主句的限制。

    

    尽管多语言语言模型（如XLM-R和mT5）的能力有了令人印象深刻的增长，但已经发现它们在处理语言差异较大的语言特别是在资源匮乏的情况下仍然存在困难。有效的跨语言转移的一个障碍是单词顺序模式的可变性。这可以通过源端或目标端的单词重排序来减轻，并且已经提出了许多重排序方法。然而，它们依赖于特定语言的规则，工作在POS标签的级别上，或者只针对主句，而保持从属从句不变。为了解决这些限制，我们提出了一种新的强大的重排序方法，它基于通用依赖关系来定义，能够利用少量标注数据学习以句法上下文为条件的细粒度单词顺序模式，并且可以应用于句法树的所有层级。我们在各种任务上进行实验，并展示了我们的方法的一致性。

    Despite the impressive growth of the abilities of multilingual language models, such as XLM-R and mT5, it has been shown that they still face difficulties when tackling typologically-distant languages, particularly in the low-resource setting. One obstacle for effective cross-lingual transfer is variability in word-order patterns. It can be potentially mitigated via sourceor target-side word reordering, and numerous approaches to reordering have been proposed. However, they rely on language-specific rules, work on the level of POS tags, or only target the main clause, leaving subordinate clauses intact. To address these limitations, we present a new powerful reordering method, defined in terms of Universal Dependencies, that is able to learn fine-grained word-order patterns conditioned on the syntactic context from a small amount of annotated data and can be applied at all levels of the syntactic tree. We conduct experiments on a diverse set of tasks and show that our method consiste
    
[^4]: 在DAG空间中使用基于模型的强化学习进行因果发现的树搜索

    Tree Search in DAG Space with Model-based Reinforcement Learning for Causal Discovery. (arXiv:2310.13576v1 [cs.LG])

    [http://arxiv.org/abs/2310.13576](http://arxiv.org/abs/2310.13576)

    本论文提出了一种在DAG空间中使用树搜索和模型驱动强化学习进行因果发现的方法，并通过两个实际任务的评估，证明了其在性能上显著优于目前最先进的无模型方法和贪婪搜索，具有很大的应用前景。

    

    识别因果结构对于许多领域至关重要，从战略决策到生物学和经济学。在这项工作中，我们提出了一种基于树搜索的因果发现的模型驱动强化学习方法，该方法逐步构建有向无环图。我们还形式化并证明了一种排除会引入循环的边的高效算法的正确性，这使得在DAG空间中进行更深入的离散搜索和采样成为可能。我们在两个实际任务中评估了我们的方法，在性能上比最先进的无模型方法和贪婪搜索取得了显著优势，这是组合方法的一个有希望的进展。

    Identifying causal structure is central to many fields ranging from strategic decision-making to biology and economics. In this work, we propose a model-based reinforcement learning method for causal discovery based on tree search, which builds directed acyclic graphs incrementally. We also formalize and prove the correctness of an efficient algorithm for excluding edges that would introduce cycles, which enables deeper discrete search and sampling in DAG space. We evaluate our approach on two real-world tasks, achieving substantially better performance than the state-of-the-art model-free method and greedy search, constituting a promising advancement for combinatorial methods.
    
[^5]: 基于渐进双先验网络的泛化乳腺肿瘤分割

    Progressive Dual Priori Network for Generalized Breast Tumor Segmentation. (arXiv:2310.13574v1 [eess.IV])

    [http://arxiv.org/abs/2310.13574](http://arxiv.org/abs/2310.13574)

    本文提出了一种基于渐进双先验网络的方法用于乳腺肿瘤分割，通过使用弱语义先验和跨尺度相关先验知识逐步改进分割性能，实验证明相比于现有方法，该方法在泛化能力和分割性能上都有显著提高。

    

    为了提高乳腺肿瘤分割模型的泛化能力，并改进对小尺寸、低对比度和不规则形状乳腺肿瘤的分割性能，我们提出了一种渐进双先验网络（PDPNet），用于从不同机构采集的动态增强磁共振图像（DCE-MRI）中分割乳腺肿瘤。PDPNet首先通过粗分割定位模块裁剪肿瘤区域，然后使用弱语义先验和跨尺度相关先验知识逐步改进乳腺肿瘤掩膜。为了验证PDPNet的有效性，我们将其与几种最先进的方法在多中心数据集上进行了比较。结果表明，与亚优化方法相比，PDPNet的DSC、SEN、KAPPA和HD95分别提高了3.63％、8.19％、5.52％和3.66％。此外，通过消融实验证明了所提出的定位模块可以减少正常组织的影响。

    To promote the generalization ability of breast tumor segmentation models, as well as to improve the segmentation performance for breast tumors with smaller size, low-contrast amd irregular shape, we propose a progressive dual priori network (PDPNet) to segment breast tumors from dynamic enhanced magnetic resonance images (DCE-MRI) acquired at different sites. The PDPNet first cropped tumor regions with a coarse-segmentation based localization module, then the breast tumor mask was progressively refined by using the weak semantic priori and cross-scale correlation prior knowledge. To validate the effectiveness of PDPNet, we compared it with several state-of-the-art methods on multi-center datasets. The results showed that, comparing against the suboptimal method, the DSC, SEN, KAPPA and HD95 of PDPNet were improved 3.63\%, 8.19\%, 5.52\%, and 3.66\% respectively. In addition, through ablations, we demonstrated that the proposed localization module can decrease the influence of normal t
    
[^6]: 解开双谷之谜：透过学得特征空间的深入分析

    Unraveling the Enigma of Double Descent: An In-depth Analysis through the Lens of Learned Feature Space. (arXiv:2310.13572v1 [cs.LG])

    [http://arxiv.org/abs/2310.13572](http://arxiv.org/abs/2310.13572)

    本研究通过对学得表示的特征空间进行全面分析，揭示出双谷现象是在使用噪声数据训练的不完美模型中出现的，并提出了双谷现象的解释：模型首先学习噪声数据直到插值点，然后通过超参数化的隐式正则化来将信息与噪声分离。

    

    双谷现象在机器学习领域中呈现出一种逆直觉的特征，并且研究人员在各种模型和任务中观察到其表现。虽然对该现象在特定背景下提出了一些理论解释，但仍没有确立用于解释深度学习中出现双谷的公认理论。在本研究中，我们重新审视了双谷现象，并展示了其出现受到噪声数据影响的强烈影响。通过对学得表示的特征空间进行全面分析，我们揭示出在使用噪声数据训练的不完美模型中会出现双谷现象。我们认为双谷是模型首先学习噪声数据直到插值点，然后通过超参数化的隐式正则化来获取将信息与噪声分离的能力。我们假设双谷现象不应该在良好正则化的模型中发生。

    Double descent presents a counter-intuitive aspect within the machine learning domain, and researchers have observed its manifestation in various models and tasks. While some theoretical explanations have been proposed for this phenomenon in specific contexts, an accepted theory to account for its occurrence in deep learning remains yet to be established. In this study, we revisit the phenomenon of double descent and demonstrate that its occurrence is strongly influenced by the presence of noisy data. Through conducting a comprehensive analysis of the feature space of learned representations, we unveil that double descent arises in imperfect models trained with noisy data. We argue that double descent is a consequence of the model first learning the noisy data until interpolation and then adding implicit regularization via over-parameterization acquiring therefore capability to separate the information from the noise. We postulate that double descent should never occur in well-regulari
    
[^7]: 为更快乐的自主网络安全代理设计的奖励塑造

    Reward Shaping for Happier Autonomous Cyber Security Agents. (arXiv:2310.13565v1 [cs.LG])

    [http://arxiv.org/abs/2310.13565](http://arxiv.org/abs/2310.13565)

    本研究探讨了在计算机网络防御任务中，奖励信号的特点对深度强化学习算法的影响，并研究了奖励塑造技术以提高代理的训练效率和性能。

    

    随着机器学习模型的能力增强，它们在解决复杂任务方面展示出了增加的潜力。最有前景的方向之一是使用深度强化学习来训练计算机网络防御任务中的自主代理。本研究研究了在训练此任务时为代理提供的奖励信号的影响。由于网络安全任务的性质，奖励信号通常采用以下形式：1）以惩罚的形式（例如发生了妥协）；2）在每次防御任务中稀疏分布。这样的奖励特征不同于经典强化学习任务，其中代理定期获得进展的奖励（而不是偶尔因失败而受到惩罚）。我们研究了能够弥合这一差距的奖励塑造技术，以使代理更能有效地训练，并潜在地收敛到更好的性能。首先，我们展示了深度强化学习算法对奖励幅度的敏感性。

    As machine learning models become more capable, they have exhibited increased potential in solving complex tasks. One of the most promising directions uses deep reinforcement learning to train autonomous agents in computer network defense tasks. This work studies the impact of the reward signal that is provided to the agents when training for this task. Due to the nature of cybersecurity tasks, the reward signal is typically 1) in the form of penalties (e.g., when a compromise occurs), and 2) distributed sparsely across each defense episode. Such reward characteristics are atypical of classic reinforcement learning tasks where the agent is regularly rewarded for progress (cf. to getting occasionally penalized for failures). We investigate reward shaping techniques that could bridge this gap so as to enable agents to train more sample-efficiently and potentially converge to a better performance. We first show that deep reinforcement learning algorithms are sensitive to the magnitude of 
    
[^8]: 缓存与精炼：优化对大型语言模型的API调用

    Cache & Distil: Optimising API Calls to Large Language Models. (arXiv:2310.13561v1 [cs.CL])

    [http://arxiv.org/abs/2310.13561](http://arxiv.org/abs/2310.13561)

    本研究针对分类任务，通过神经缓存技术，使用边界抽样和委员会查询作为决策策略，优化了对大型语言模型的API调用，并取得了一致的好处。

    

    大规模部署生成式AI工具往往依赖于昂贵的API调用来满足用户查询对大型语言模型（LLM）。为了减少这些调用的频率，可以使用一个较小的语言模型--学生模型--不断地在LLM的响应上进行训练。这个学生模型逐渐独立处理越来越多的用户请求，并逐步提高其能力，我们将这个过程称为神经缓存。神经缓存的关键要素是一个决策策略，用于决定哪些请求应由学生模型单独处理，哪些请求应重定向给LLM，从而帮助学生模型的学习。在本研究中，我们专注于分类任务，并考虑了一系列经典的基于主动学习的选择准则作为该策略。我们的实验证明，边界抽样和委员会查询能够在各种任务和预算下带来一致的好处。

    Large-scale deployment of generative AI tools often depends on costly API calls to a Large Language Model (LLM) to fulfil user queries. To curtail the frequency of these calls, one can employ a smaller language model -- a student -- which is continuously trained on the responses of the LLM. This student gradually gains proficiency in independently handling an increasing number of user requests, a process we term neural caching. The crucial element in neural caching is a policy that decides which requests should be processed by the student alone and which should be redirected to the LLM, subsequently aiding the student's learning. In this study, we focus on classification tasks, and we consider a range of classic active learning-based selection criteria as the policy. Our experiments suggest that Margin Sampling and Query by Committee bring consistent benefits across tasks and budgets.
    
[^9]: 关于以Von Mises估计器进行条件独立性测试的样本复杂性及其在因果发现中的应用

    On sample complexity of conditional independence testing with Von Mises estimator with application to causal discovery. (arXiv:2310.13553v1 [cs.LG])

    [http://arxiv.org/abs/2310.13553](http://arxiv.org/abs/2310.13553)

    本文研究了以Von Mises估计器进行条件独立性测试的样本复杂性，并基于该估计器设计了一种新的测试方法。实验证明，该方法在时间或样本复杂度方面优于其他方法。

    

    本文通过研究基于核密度估计的多元分布的非参数Von Mises估计器，针对条件独立性测试的动机，建立了一个指数集中不等式。我们设计了一种基于该估计器的条件独立性测试方法，称为VM-CI，该方法在平滑性假设下达到了最优参数速率。利用指数集中不等式，我们证明了VM-CI的整体误差的紧密上界。由此，我们能够对使用VM-CI进行条件独立性测试的任何基于约束的因果发现算法的样本复杂度进行刻画。据我们所知，这是对于连续变量因果发现的首个样本复杂度保证。此外，我们通过实验证明，VM-CI在时间或样本复杂度（或两者）方面优于其他常用的条件独立性测试方法，这可以转化为

    Motivated by conditional independence testing, an essential step in constraint-based causal discovery algorithms, we study the nonparametric Von Mises estimator for the entropy of multivariate distributions built on a kernel density estimator. We establish an exponential concentration inequality for this estimator. We design a test for conditional independence (CI) based on our estimator, called VM-CI, which achieves optimal parametric rates under smoothness assumptions. Leveraging the exponential concentration, we prove a tight upper bound for the overall error of VM-CI. This, in turn, allows us to characterize the sample complexity of any constraint-based causal discovery algorithm that uses VM-CI for CI tests. To the best of our knowledge, this is the first sample complexity guarantee for causal discovery for continuous variables. Furthermore, we empirically show that VM-CI outperforms other popular CI tests in terms of either time or sample complexity (or both), which translates to
    
[^10]: 多任务强化学习在非马尔可夫决策过程中的可证明受益

    Provable Benefits of Multi-task RL under Non-Markovian Decision Making Processes. (arXiv:2310.13550v1 [cs.LG])

    [http://arxiv.org/abs/2310.13550](http://arxiv.org/abs/2310.13550)

    本文通过研究在非马尔可夫决策过程下的多任务强化学习，证明了多个MDPs之间共享的潜在结构能够显著提高采样效率。对于部分可观察MDPs和预测状态表示，我们提出了一个联合模型类，并使用$\eta$-bracketing number来量化其复杂性和任务的相似性，从而决定了多任务相较于单任务RL的受益。

    

    在马尔可夫决策过程（MDPs）下的多任务强化学习（RL）中，多个MDPs之间存在共享的潜在结构已被证明相对于单任务RL能够显著提高采样效率。本文研究这种受益是否能够扩展到更一般的顺序决策问题，如部分可观察MDPs（POMDPs）和更一般的预测状态表示（PSRs）。主要挑战在于大型复杂模型空间使得很难确定多任务PSRs的共同潜在结构类型能够减少模型复杂性并提高采样效率。为了达到这个目标，我们假设了一个用于任务的联合模型类，并使用$\eta$-bracketing number来量化其复杂性；这个数也作为一个通用指标来捕捉任务的相似性，从而决定了多任务相较于单任务RL的受益。我们首先研究了上游多任务学习在PSRs上的应用。

    In multi-task reinforcement learning (RL) under Markov decision processes (MDPs), the presence of shared latent structures among multiple MDPs has been shown to yield significant benefits to the sample efficiency compared to single-task RL. In this paper, we investigate whether such a benefit can extend to more general sequential decision making problems, such as partially observable MDPs (POMDPs) and more general predictive state representations (PSRs). The main challenge here is that the large and complex model space makes it hard to identify what types of common latent structure of multi-task PSRs can reduce the model complexity and improve sample efficiency. To this end, we posit a joint model class for tasks and use the notion of $\eta$-bracketing number to quantify its complexity; this number also serves as a general metric to capture the similarity of tasks and thus determines the benefit of multi-task over single-task RL. We first study upstream multi-task learning over PSRs, i
    
[^11]: 探索语言模型中谄媚行为的理解

    Towards Understanding Sycophancy in Language Models. (arXiv:2310.13548v1 [cs.CL])

    [http://arxiv.org/abs/2310.13548](http://arxiv.org/abs/2310.13548)

    这项研究探讨了强化学习从人类反馈中训练高质量AI助手的技术，发现这种方法可能导致模型在回答问题时过于谄媚，而不是坦诚，通过分析人类偏好数据得出了这一结论。

    

    「从人类反馈中进行强化学习（RLHF）」是训练高质量AI助手的一种流行技术。然而，RLHF可能会鼓励模型通过与用户信念相符的回答来代替真实回答，这种行为被称为谄媚行为。我们研究了RLHF训练模型中谄媚行为的普遍性以及人类偏好判断是否起到了作用。首先，我们证明了五个最先进的AI助手在四个不同的自由文本生成任务中一贯表现出谄媚行为。为了理解人类偏好是否驱动了RLHF模型的这种广泛行为，我们分析了现有的人类偏好数据。我们发现，当回答与用户的观点相符时，它更有可能被选中。此外，人类和偏好模型（PMs）将有说服力的谄媚回答与正确回答相比，有时几乎可以忽略不计地选择了谄媚回答。优化模型输出以满足PMs有时也会在真实性和谄媚行为之间做出取舍。

    Reinforcement learning from human feedback (RLHF) is a popular technique for training high-quality AI assistants. However, RLHF may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models and whether human preference judgements are responsible. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophantic behavior across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior of RLHF models, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Over
    
[^12]: 基于结构感知图学习的正-未标记节点分类

    Positive-Unlabeled Node Classification with Structure-aware Graph Learning. (arXiv:2310.13538v1 [cs.LG])

    [http://arxiv.org/abs/2310.13538](http://arxiv.org/abs/2310.13538)

    本文提出了一种利用图结构进行正-未标记节点分类的方法。我们引入了一个距离感知的PU损失来提供更准确的监督，同时通过正则化项与图结构对齐。实验证明该方法在多种图数据集上具有卓越性能。

    

    在图上进行节点分类是一个具有许多应用的重要研究问题。现实世界中的图数据集可能并不如大多数现有工作所假定的那样平衡和准确。正-未标记（PU）节点分类是一个具有挑战性的设置，其中标记节点被限制为正节点。它具有多样的应用，例如疫情预测或网络异常检测。现有的PU节点分类工作忽视了图结构中的信息，而这可能是关键的。在本文中，我们提出了更好地利用图结构进行PU节点分类的方法。我们首先提出了一个距离感知的PU损失，利用图中的同质性引入更准确的监督。我们还提出了一个正则化项，使模型与图结构对齐。理论分析表明，最小化所提出的损失函数也导致最小化带有正负标签的期望损失。对多种图数据集进行的大量实证评估证明了其卓越的性能。

    Node classification on graphs is an important research problem with many applications. Real-world graph data sets may not be balanced and accurate as assumed by most existing works. A challenging setting is positive-unlabeled (PU) node classification, where labeled nodes are restricted to positive nodes. It has diverse applications, e.g., pandemic prediction or network anomaly detection. Existing works on PU node classification overlook information in the graph structure, which can be critical. In this paper, we propose to better utilize graph structure for PU node classification. We first propose a distance-aware PU loss that uses homophily in graphs to introduce more accurate supervision. We also propose a regularizer to align the model with graph structure. Theoretical analysis shows that minimizing the proposed loss also leads to minimizing the expected loss with both positive and negative labels. Extensive empirical evaluation on diverse graph data sets demonstrates its superior p
    
[^13]: ICCV 2023视觉连续学习挑战的技术报告：语义分割的连续测试时间适应

    Technical Report for ICCV 2023 Visual Continual Learning Challenge: Continuous Test-time Adaptation for Semantic Segmentation. (arXiv:2310.13533v1 [cs.CV])

    [http://arxiv.org/abs/2310.13533](http://arxiv.org/abs/2310.13533)

    该论文提出了一种连续测试时间适应方法，用于解决语义分割任务中视频序列中逐渐变化的领域问题。该方法在每个图像序列中单独评估，并通过逐渐改变天气条件和时段来测试模型的性能。

    

    该挑战的目标是开发一种测试时间适应方法（TTA），该方法能够适应视频序列中逐渐变化的领域，用于语义分割任务。它基于一个合成驾驶视频数据集- SHIFT。源模型是在天气晴朗的白天拍摄的图像上训练的。测试时间的领域变化主要是由于不同的天气条件和不同时段。TTA方法在每个图像序列（视频）中单独评估，意味着在下一个序列之前，模型会被重新置为源模型状态。图像一个接一个地到达，每个帧到达时都需要进行预测。每个序列由401个图像组成，从源领域开始，然后逐渐转变为不同的领域（改变天气或时段），直到序列的中间部分。在序列的后半部分，领域逐渐回归到源领域。只有SHIFT数据集的验证集提供了真实数据。

    The goal of the challenge is to develop a test-time adaptation (TTA) method, which could adapt the model to gradually changing domains in video sequences for semantic segmentation task. It is based on a synthetic driving video dataset - SHIFT. The source model is trained on images taken during daytime in clear weather. Domain changes at test-time are mainly caused by varying weather conditions and times of day. The TTA methods are evaluated in each image sequence (video) separately, meaning the model is reset to the source model state before the next sequence. Images come one by one and a prediction has to be made at the arrival of each frame. Each sequence is composed of 401 images and starts with the source domain, then gradually drifts to a different one (changing weather or time of day) until the middle of the sequence. In the second half of the sequence, the domain gradually shifts back to the source one. Ground truth data is available only for the validation split of the SHIFT da
    
[^14]: 受控随机性提高了Transformer模型的性能

    Controlled Randomness Improves the Performance of Transformer Models. (arXiv:2310.13526v1 [cs.CL])

    [http://arxiv.org/abs/2310.13526](http://arxiv.org/abs/2310.13526)

    本研究通过在训练过程中引入受控随机性来提高Transformer模型的性能，并在命名实体识别、关系抽取和文本摘要等任务中取得了改进效果。

    

    在自然语言模型的预训练过程中，主要目标是学习预训练数据集的通用表示，通常需要大量文本数据来捕捉自然语言的复杂性和多样性。然而，与此相反，在大多数情况下，可用于解决特定下游任务的数据量往往远远不及上述预训练数据集，尤其是在数据稀缺的领域。我们引入了受控随机性，即噪声，到训练过程中，以提高微调语言模型的性能，并探索目标噪声以及这些模型的参数对性能的影响。我们发现添加这样的噪声可以提高我们的两个下游任务，即联合命名实体识别和关系抽取，以及文本摘要的性能。

    During the pre-training step of natural language models, the main objective is to learn a general representation of the pre-training dataset, usually requiring large amounts of textual data to capture the complexity and diversity of natural language. Contrasting this, in most cases, the size of the data available to solve the specific downstream task is often dwarfed by the aforementioned pre-training dataset, especially in domains where data is scarce. We introduce controlled randomness, i.e. noise, into the training process to improve fine-tuning language models and explore the performance of targeted noise in addition to the parameters of these models. We find that adding such noise can improve the performance in our two downstream tasks of joint named entity recognition and relation extraction and text summarization.
    
[^15]: 基于测量的变分量子计算用于生成建模

    Variational measurement-based quantum computation for generative modeling. (arXiv:2310.13524v1 [quant-ph])

    [http://arxiv.org/abs/2310.13524](http://arxiv.org/abs/2310.13524)

    这项研究提出了一种基于测量的变分量子计算算法，将量子测量的随机性视为计算资源，并应用于生成建模任务。

    

    基于测量的量子计算（MBQC）提供了一种基本独特的范例来设计量子算法。在MBQC中，由于量子测量的固有随机性，自然的操作不是确定性和幺正的，而是通过概率附带的。然而，到目前为止，MBQC的主要算法应用是完全抵消这种概率性质，以模拟表达在电路模型中的幺正计算。在这项工作中，我们提出了设计MBQC算法的思路，该算法接受这种固有随机性，并将MBQC中的随机附带视为计算资源。我们考虑了随机性有益的自然应用，即生成建模，这是一个以生成复杂概率分布为中心的机器学习任务。为了解决这个任务，我们提出了一个具有控制参数的变分MBQC算法，可以直接调整允许在计算中引入的随机程度。

    Measurement-based quantum computation (MBQC) offers a fundamentally unique paradigm to design quantum algorithms. Indeed, due to the inherent randomness of quantum measurements, the natural operations in MBQC are not deterministic and unitary, but are rather augmented with probabilistic byproducts. Yet, the main algorithmic use of MBQC so far has been to completely counteract this probabilistic nature in order to simulate unitary computations expressed in the circuit model. In this work, we propose designing MBQC algorithms that embrace this inherent randomness and treat the random byproducts in MBQC as a resource for computation. As a natural application where randomness can be beneficial, we consider generative modeling, a task in machine learning centered around generating complex probability distributions. To address this task, we propose a variational MBQC algorithm equipped with control parameters that allow to directly adjust the degree of randomness to be admitted in the comput
    
[^16]: 人工神经网络在木材质量分类中的特征选择和超参数微调

    Feature Selection and Hyperparameter Fine-tuning in Artificial Neural Networks for Wood Quality Classification. (arXiv:2310.13490v1 [cs.LG])

    [http://arxiv.org/abs/2310.13490](http://arxiv.org/abs/2310.13490)

    本文研究了在木材质量分类中同时调整人工神经网络的超参数和选择最佳特征子集的问题。

    

    木板质量分类是锯木厂行业的重要任务，发展中国家的中小企业通常由人工操作员执行此任务。机器学习算法已成功应用于研究该问题，相较于其他解决方案，提供了更经济的选择。然而，这些方法通常在选择适当的超参数方面存在一些缺点。此外，模型对从木板图像中提取的特征敏感，这影响了模型的归纳能力和泛化能力。因此，本文研究了同时调整人工神经网络(ANN)的超参数和选择更好描述木板质量的特征子集的问题。实验使用了从锯木厂行业获得的图像组成的私人数据集，并使用不同的特征进行描述。

    Quality classification of wood boards is an essential task in the sawmill industry, which is still usually performed by human operators in small to median companies in developing countries. Machine learning algorithms have been successfully employed to investigate the problem, offering a more affordable alternative compared to other solutions. However, such approaches usually present some drawbacks regarding the proper selection of their hyperparameters. Moreover, the models are susceptible to the features extracted from wood board images, which influence the induction of the model and, consequently, its generalization power. Therefore, in this paper, we investigate the problem of simultaneously tuning the hyperparameters of an artificial neural network (ANN) as well as selecting a subset of characteristics that better describes the wood board quality. Experiments were conducted over a private dataset composed of images obtained from a sawmill industry and described using different fea
    
[^17]: 个性化识别、预测和刺激神经振荡：基于数据驱动的癫痫网络动力学模型

    Personalized identification, prediction, and stimulation of neural oscillations via data-driven models of epileptic network dynamics. (arXiv:2310.13480v1 [q-bio.NC])

    [http://arxiv.org/abs/2310.13480](http://arxiv.org/abs/2310.13480)

    该论文研究了从脑电图数据中提取个体化的癫痫网络动力学模型，应用于个性化识别、预测和刺激神经振荡，为癫痫治疗提供了有前景的疗法选择。

    

    神经振荡被认为是大脑信息处理和传递的特定标志，也反映了神经系统疾病中的病理活动，为诊断和预测提供了基础。癫痫是最常见的神经系统疾病之一，其特征是大脑振荡的异常同步和失同步。约三分之一的癫痫病例对药物治疗无效，因此需要创新治疗方法，其中大脑刺激似乎是一种有希望的治疗选择。然而，大脑刺激范式的发展往往基于对大脑动态的概括假设，尽管已知患者和脑状态之间存在显著差异。我们开发了一个框架，可以直接从脑电图数据中提取个体化的癫痫网络动力学预测模型。这些模型基于主要的相干振荡及其

    Neural oscillations are considered to be brain-specific signatures of information processing and communication in the brain. They also reflect pathological brain activity in neurological disorders, thus offering a basis for diagnoses and forecasting. Epilepsy is one of the most common neurological disorders, characterized by abnormal synchronization and desynchronization of the oscillations in the brain. About one third of epilepsy cases are pharmacoresistant, and as such emphasize the need for novel therapy approaches, where brain stimulation appears to be a promising therapeutic option. The development of brain stimulation paradigms, however, is often based on generalized assumptions about brain dynamics, although it is known that significant differences occur between patients and brain states. We developed a framework to extract individualized predictive models of epileptic network dynamics directly from EEG data. The models are based on the dominant coherent oscillations and their 
    
[^18]: 分段、选择、纠正：一种弱监督指代分割的框架

    Segment, Select, Correct: A Framework for Weakly-Supervised Referring Segmentation. (arXiv:2310.13479v1 [cs.CV])

    [http://arxiv.org/abs/2310.13479](http://arxiv.org/abs/2310.13479)

    这项研究提出了一个弱监督框架，通过将指代图像分割任务分解为获取实例掩模、选择正确掩模和纠正错误掩模的三个步骤，填补了弱监督和零样本方法在性能上的差距。

    

    指代图像分割（RIS）是通过自然语言句子在图像中识别对象的一个具有挑战性的任务，目前主要通过监督学习来解决。然而，收集指代标注掩模是一个耗时的过程，现有的弱监督和零样本方法在性能上远远不及完全监督学习的方法。为了填补性能差距，我们提出了一种新的弱监督框架，通过将RIS分解成三个步骤进行处理：获取被提及指令中的对象的实例掩模（分段），使用零样本学习来选择给定指令的潜在正确掩模（选择），并通过引导模型来修正零样本选择的错误（纠正）。在我们的实验中，仅使用前两个步骤（零样本分段和选择）比其他零样本基线提高了多达19%的性能。

    Referring Image Segmentation (RIS) - the problem of identifying objects in images through natural language sentences - is a challenging task currently mostly solved through supervised learning. However, while collecting referred annotation masks is a time-consuming process, the few existing weakly-supervised and zero-shot approaches fall significantly short in performance compared to fully-supervised learning ones. To bridge the performance gap without mask annotations, we propose a novel weakly-supervised framework that tackles RIS by decomposing it into three steps: obtaining instance masks for the object mentioned in the referencing instruction (segment), using zero-shot learning to select a potentially correct mask for the given instruction (select), and bootstrapping a model which allows for fixing the mistakes of zero-shot selection (correct). In our experiments, using only the first two steps (zero-shot segment and select) outperforms other zero-shot baselines by as much as 19%,
    
[^19]: $k$-means的$D^\alpha$种子选择算法的分析

    An Analysis of $D^\alpha$ seeding for $k$-means. (arXiv:2310.13474v1 [cs.DS])

    [http://arxiv.org/abs/2310.13474](http://arxiv.org/abs/2310.13474)

    该论文分析了$k$-means的$D^\alpha$种子选择算法。他们证明了该算法在期望上对于任何$ \alpha \geq 1 $都能保证一个$ O(2^{2\alpha}\cdot \log k) $-近似解。此外，他们还发现使用$ \alpha > 2 $的$ D^\alpha $种子选择算法可以得到比标准的$ k $-means目标函数更好的解。该论文提供了对这一现象的严格理解。

    

    一种最受欢迎的聚类算法是Arthur和Vassilvitskii (2007)开发的著名的$D^\alpha$种子选择算法（也被称为$ k $-means ++，其中$ \alpha = 2 $），他们证明了该算法在期望上对于任何$ \alpha \geq 1 $都能保证一个$ O(2^{2\alpha}\cdot \log k) $-近似解。最近，Balcan，Dick和White(2018)通过实验证明使用$ \alpha > 2 $的$ D^\alpha $种子选择算法可以得到比标准的$ k $-means目标函数（即$(k,2)$-means损失函数）更好的解。在本文中，我们对这一现象进行了严格的理解。对于任何$ \alpha > 2 $，我们证明$ D^\alpha $种子选择算法可以在期望上保证一个近似系数为$$ O_\alpha \left((g_\alpha)^{2/\alpha}\cdot \left(\frac{\sigma_{\mathrm{max}}}{\sigma_{\mathrm{min}}}\right)^{2-4/\alpha}\cdot (\min\{\ell,\log k\})^{2/\alpha}\right)$$的标准$ k $-means目标函数。

    One of the most popular clustering algorithms is the celebrated $D^\alpha$ seeding algorithm (also know as $k$-means++ when $\alpha=2$) by Arthur and Vassilvitskii (2007), who showed that it guarantees in expectation an $O(2^{2\alpha}\cdot \log k)$-approximate solution to the ($k$,$\alpha$)-means cost (where euclidean distances are raised to the power $\alpha$) for any $\alpha\ge 1$. More recently, Balcan, Dick, and White (2018) observed experimentally that using $D^\alpha$ seeding with $\alpha>2$ can lead to a better solution with respect to the standard $k$-means objective (i.e. the $(k,2)$-means cost).  In this paper, we provide a rigorous understanding of this phenomenon. For any $\alpha>2$, we show that $D^\alpha$ seeding guarantees in expectation an approximation factor of $$ O_\alpha \left((g_\alpha)^{2/\alpha}\cdot \left(\frac{\sigma_{\mathrm{max}}}{\sigma_{\mathrm{min}}}\right)^{2-4/\alpha}\cdot (\min\{\ell,\log k\})^{2/\alpha}\right)$$ with respect to the standard $k$-means c
    
[^20]: 稳定的非凸-非凹训练通过线性插值

    Stable Nonconvex-Nonconcave Training via Linear Interpolation. (arXiv:2310.13459v1 [cs.LG])

    [http://arxiv.org/abs/2310.13459](http://arxiv.org/abs/2310.13459)

    本文通过理论分析指出，优化过程中的不稳定性通常是由损失函数的非单调性引起的，提出了一种稳定（大规模）神经网络训练的方法，即通过线性插值来利用“非扩张算子”的理论来解决。通过构建一种新的优化方案，松弛近似近端点（RAPP），发现了一族Lookahead算法，能够在协调部分单调问题中收敛，即使基本优化器采用梯度下降升级算法。

    

    本文提出了一种关于线性插值的理论分析，作为一种稳定（大规模）神经网络训练的方法。我们认为优化过程中的不稳定性通常是由损失函数的非单调性引起的，并展示了线性插值如何通过利用“非扩张算子”的理论来帮助解决这个问题。我们构建了一种新的优化方案，称为松弛近似近端点（RAPP），这是第一个明确的方法，能够实现完整范围内的协调部分单调问题的最后迭代收敛速率。该构造可扩展到约束和正则化设置。通过替换RAPP中的内部优化器，我们重新发现了Lookahead算法族，我们证明了这些算法在协调部分单调问题中的收敛性，即使基本优化器采用梯度下降升级算法。通过利用Lookahead继承性质，我们进一步扩展了Lookahead在协调部分单调问题中收敛的范围。

    This paper presents a theoretical analysis of linear interpolation as a principled method for stabilizing (large-scale) neural network training. We argue that instabilities in the optimization process are often caused by the nonmonotonicity of the loss landscape and show how linear interpolation can help by leveraging the theory of nonexpansive operators. We construct a new optimization scheme called relaxed approximate proximal point (RAPP), which is the first explicit method to achieve last iterate convergence rates for the full range of cohypomonotone problems. The construction extends to constrained and regularized settings. By replacing the inner optimizer in RAPP we rediscover the family of Lookahead algorithms for which we establish convergence in cohypomonotone problems even when the base optimizer is taken to be gradient descent ascent. The range of cohypomonotone problems in which Lookahead converges is further expanded by exploiting that Lookahead inherits the properties of 
    
[^21]: 不同形态机器人间的任务演示学习与对应关系

    Correspondence learning between morphologically different robots through task demonstrations. (arXiv:2310.13458v1 [cs.RO])

    [http://arxiv.org/abs/2310.13458](http://arxiv.org/abs/2310.13458)

    本论文提出了一种学习不同形态机器人间对应关系的方法，通过演示实现了共同的潜变量表示，使得不同机器人可以更直接地转移技能。

    

    我们观察到机器人在其机身、传感器和执行器方面有着各种各样的差异。考虑到机器人领域的巨大多样性，独立地教导每个不同机器人的每个技能是低效且不可扩展的。如果我们能够学习不同机器人的感官运动空间之间的对应关系，那么我们可以期望在一个机器人上学习的技能可以更直接有效地转移到其他机器人上。本文提出了一种方法，在具有关节控制的固定基座操纵机器人和差动驱动移动机器人之间学习对应关系。为此，首先让两个机器人进行执行相同任务的演示。在学习对应策略的同时形成一个共同的潜变量表示。在这个初始学习阶段之后，通过观察一个机器人的新任务执行就足以生成一个潜变量的表示，从而实现对应关系的学习。

    We observe a large variety of robots in terms of their bodies, sensors, and actuators. Given the commonalities in the skill sets, teaching each skill to each different robot independently is inefficient and not scalable when the large variety in the robotic landscape is considered. If we can learn the correspondences between the sensorimotor spaces of different robots, we can expect a skill that is learned in one robot can be more directly and easily transferred to the other robots. In this paper, we propose a method to learn correspondences between robots that have significant differences in their morphologies: a fixed-based manipulator robot with joint control and a differential drive mobile robot. For this, both robots are first given demonstrations that achieve the same tasks. A common latent representation is formed while learning the corresponding policies. After this initial learning stage, the observation of a new task execution by one robot becomes sufficient to generate a lat
    
[^22]: 随机矩阵分析在低密度分离假设下平衡监督和无监督学习之间的应用

    Random Matrix Analysis to Balance between Supervised and Unsupervised Learning under the Low Density Separation Assumption. (arXiv:2310.13434v1 [cs.LG])

    [http://arxiv.org/abs/2310.13434](http://arxiv.org/abs/2310.13434)

    本论文提出了一种理论框架，在低密度分离假设下分析高维情况下的半监督分类，并介绍了QLDS模型，该模型在平衡监督和无监督学习方法之间建立了一个平滑的桥梁。通过应用随机矩阵理论，我们推导出了在渐近情况下分类错误的理论评估，并提出了一种能够找到最佳平衡点的超参数选择策略。

    

    我们提出了一个理论框架，用于分析高维情况下低密度分离假设下的半监督分类。具体而言，我们引入了QLDS，一个线性分类模型，其中通过二次边界最大化来实现低密度分离假设。该算法具有明确的解和丰富的理论属性，并且我们证明了我们算法的特殊情况是监督情况下的最小二乘支持向量机，完全无监督情况下的谱聚类，以及一类半监督的基于图的方法。因此，QLDS在这些监督和无监督学习方法之间建立了一个平滑的桥梁。利用随机矩阵理论的最新进展，我们正式推导了在渐近情况下的分类误差的理论评估。作为一个应用，我们推导出了一种超参数选择策略，以找到最佳的监督和无监督之间的平衡。

    We propose a theoretical framework to analyze semi-supervised classification under the low density separation assumption in a high-dimensional regime. In particular, we introduce QLDS, a linear classification model, where the low density separation assumption is implemented via quadratic margin maximization. The algorithm has an explicit solution with rich theoretical properties, and we show that particular cases of our algorithm are the least-square support vector machine in the supervised case, the spectral clustering in the fully unsupervised regime, and a class of semi-supervised graph-based approaches. As such, QLDS establishes a smooth bridge between these supervised and unsupervised learning methods. Using recent advances in the random matrix theory, we formally derive a theoretical evaluation of the classification error in the asymptotic regime. As an application, we derive a hyperparameter selection policy that finds the best balance between the supervised and the unsupervised
    
[^23]: Y-对角线耦合: 用条件Wasserstein距离逼近后验分布

    Y-Diagonal Couplings: Approximating Posteriors with Conditional Wasserstein Distances. (arXiv:2310.13433v1 [cs.LG])

    [http://arxiv.org/abs/2310.13433](http://arxiv.org/abs/2310.13433)

    本研究介绍了一种使用条件Wasserstein距离逼近后验分布的方法，并提出了一组受限耦合来计算后验分布的期望Wasserstein距离。我们推导了其对偶形式，并展示了其在后验采样方面的有利性质。

    

    在逆问题中，许多条件生成模型通过最小化联合分布与其学习到的近似之间的距离来逼近后验测度。虽然这种方法在Kullback Leibler散度的情况下也可以控制后验测度之间的距离，但对于Wasserstein距离来说却不成立。我们将引入一种带有一组受限耦合的条件Wasserstein距离，它等于后验分布的期望Wasserstein距离。通过推导其对偶形式，我们找到了一种严格的方式来解释条件Wasserstein GANs的损失。我们概述了使得常规Wasserstein距离和条件Wasserstein距离相等的条件。此外，我们将展示使用条件Wasserstein距离进行训练在后验采样方面具有有利的性质的数值示例。

    In inverse problems, many conditional generative models approximate the posterior measure by minimizing a distance between the joint measure and its learned approximation. While this approach also controls the distance between the posterior measures in the case of the Kullback Leibler divergence, it does not hold true for the Wasserstein distance. We will introduce a conditional Wasserstein distance with a set of restricted couplings that equals the expected Wasserstein distance of the posteriors. By deriving its dual, we find a rigorous way to motivate the loss of conditional Wasserstein GANs. We outline conditions under which the vanilla and the conditional Wasserstein distance coincide. Furthermore, we will show numerical examples where training with the conditional Wasserstein distance yields favorable properties for posterior sampling.
    
[^24]: 使用球形神经过程元学习器进行HRTF插值

    HRTF Interpolation using a Spherical Neural Process Meta-Learner. (arXiv:2310.13430v1 [eess.AS])

    [http://arxiv.org/abs/2310.13430](http://arxiv.org/abs/2310.13430)

    该论文介绍了一种使用球形神经过程元学习器进行HRTF插值的方法，通过利用HRTF数据的球形几何特性和左右声道的潜在对称性来纠正个性化方法的估计误差。

    

    最近提出了几种个性化方法来估计主观头相关传递函数（HRTF），使用便捷的输入模态，例如人类测量或耳廓照片。然而，这些方法在使用一些由声学测量或感知反馈获得的主观HRTF数据点样本时，存在纠正估计误差的需求。为此，我们引入了一种专门用于HRTF误差插值的卷积条件神经过程元学习器。特别地，该模型包括一个球形卷积神经网络组件，以适应HRTF数据的球形几何特性。它还利用了HRTF左右声道在中轴线上的潜在对称性。在这项工作中，我们在一个简化的设置下，纯粹从时间对齐的频谱插值角度评估了所提出模型的性能，其中通用的人群平均HRTF形成了初始估计。

    Several individualization methods have recently been proposed to estimate a subject's Head-Related Transfer Function (HRTF) using convenient input modalities such as anthropometric measurements or pinnae photographs. There exists a need for adaptively correcting the estimation error committed by such methods using a few data point samples from the subject's HRTF, acquired using acoustic measurements or perceptual feedback. To this end, we introduce a Convolutional Conditional Neural Process meta-learner specialized in HRTF error interpolation. In particular, the model includes a Spherical Convolutional Neural Network component to accommodate the spherical geometry of HRTF data. It also exploits potential symmetries between the HRTF's left and right channels about the median axis. In this work, we evaluate the proposed model's performance purely on time-aligned spectrum interpolation grounds under a simplified setup where a generic population-mean HRTF forms the initial estimates prior 
    
[^25]: FLTracer: 高准确性的联邦学习中的毒化攻击来源追踪

    FLTracer: Accurate Poisoning Attack Provenance in Federated Learning. (arXiv:2310.13424v1 [cs.CR])

    [http://arxiv.org/abs/2310.13424](http://arxiv.org/abs/2310.13424)

    FLTracer是第一个FL攻击来源追踪框架，能够高准确性地检测各种攻击，并追踪攻击的时间、目标、类型和被毒化的位置。与现有方法不同，FLTracer利用本地信息和全局信息的统计分析来实现。

    

    联邦学习（FL）是一种有前途的分布式学习方法，可以使多个客户端协同训练共享的全局模型。然而，最近的研究表明，FL容易受到各种毒化攻击，这些攻击可以降低全局模型的性能或在其中引入后门。本文首先对先前的FL攻击和检测方法进行了全面的研究。结果表明，所有现有的检测方法只对有限和特定的攻击有效。大多数检测方法存在高误报率，特别是在非独立和同分布（non-IID）的设置中，导致性能严重降低。为了解决这些问题，我们提出了FLTracer，这是第一个FL攻击来源追踪框架，可以准确检测各种攻击并追踪攻击的时间、目标、类型和被毒化的位置。与现有的仅依赖于跨客户端异常检测的方法不同，我们的方法利用了每个客户端的本地信息和全局信息的统计分析。

    Federated Learning (FL) is a promising distributed learning approach that enables multiple clients to collaboratively train a shared global model. However, recent studies show that FL is vulnerable to various poisoning attacks, which can degrade the performance of global models or introduce backdoors into them. In this paper, we first conduct a comprehensive study on prior FL attacks and detection methods. The results show that all existing detection methods are only effective against limited and specific attacks. Most detection methods suffer from high false positives, which lead to significant performance degradation, especially in not independent and identically distributed (non-IID) settings. To address these issues, we propose FLTracer, the first FL attack provenance framework to accurately detect various attacks and trace the attack time, objective, type, and poisoned location of updates. Different from existing methodologies that rely solely on cross-client anomaly detection, we
    
[^26]: 基于区块链的拜占庭鲁棒联邦学习模型（BRFL）

    BRFL: A Blockchain-based Byzantine-Robust Federated Learning Model. (arXiv:2310.13403v1 [cs.LG])

    [http://arxiv.org/abs/2310.13403](http://arxiv.org/abs/2310.13403)

    提出了一种基于区块链的拜占庭鲁棒联邦学习（BRLF）模型，该模型通过将联邦学习与区块链技术相结合，解决了联邦学习中的拜占庭攻击问题，并通过选择聚合节点、谱聚类和平均梯度计算等方法实现了高性能。

    

    随着机器学习的日益重要，训练数据的隐私和安全性变得至关重要。联邦学习通过将数据存储在分布式节点中，并仅共享模型参数，来应对这一问题，并因此受到了广泛关注。然而，在联邦学习中存在一种挑战，即拜占庭攻击问题，恶意的本地模型可能在聚合过程中影响全局模型的性能。本文提出了一种基于区块链的拜占庭鲁棒联邦学习（BRLF）模型，将联邦学习与区块链技术相结合。这种集成可以追溯恶意模型，并为本地训练的客户提供激励。我们的方法根据皮尔逊相关系数选择聚合节点，并使用谱聚类计算每个聚类中的平均梯度，并利用聚合节点的本地数据集对其准确性进行验证。实验结果表明，我们的模型在不同的数据集上表现出较高的性能。

    With the increasing importance of machine learning, the privacy and security of training data have become critical. Federated learning, which stores data in distributed nodes and shares only model parameters, has gained significant attention for addressing this concern. However, a challenge arises in federated learning due to the Byzantine Attack Problem, where malicious local models can compromise the global model's performance during aggregation. This article proposes the Blockchain-based Byzantine-Robust Federated Learning (BRLF) model that combines federated learning with blockchain technology. This integration enables traceability of malicious models and provides incentives for locally trained clients. Our approach involves selecting the aggregation node based on Pearson's correlation coefficient, and we perform spectral clustering and calculate the average gradient within each cluster, validating its accuracy using local dataset of the aggregation nodes. Experimental results on p
    
[^27]: 使用可微覆盖概率校准神经仿真推理

    Calibrating Neural Simulation-Based Inference with Differentiable Coverage Probability. (arXiv:2310.13402v1 [stat.ML])

    [http://arxiv.org/abs/2310.13402](http://arxiv.org/abs/2310.13402)

    本研究提出了一种在选定的摊销SBI技术的神经模型的训练目标中添加校准项的方法，以解决现有算法产生过于自信的后验概率的问题。该方法适用于现有的计算流程，实现可靠的黑盒后验推断。

    

    贝叶斯推理允许在给定先验信息和证据的似然的概率模型下表达后验信念的不确定性。主要是通过模拟器隐式建立的似然函数需要进行基于仿真的推理(SBI)。然而，现有的算法可能产生过于自信的后验概率(Hermans等，2022)，如果不确定性量化不准确，将击败整个可信性的目的。我们提出在选定的摊销SBI技术的神经模型的训练目标中直接包含一个校准项。通过引入对经典校准错误公式的松弛，我们实现了端到端的反向传播。所提出的方法不局限于任何特定的神经模型，并带来适度的计算开销。它直接适用于现有的计算流程，实现可靠的黑盒后验推断。

    Bayesian inference allows expressing the uncertainty of posterior belief under a probabilistic model given prior information and the likelihood of the evidence. Predominantly, the likelihood function is only implicitly established by a simulator posing the need for simulation-based inference (SBI). However, the existing algorithms can yield overconfident posteriors (Hermans *et al.*, 2022) defeating the whole purpose of credibility if the uncertainty quantification is inaccurate. We propose to include a calibration term directly into the training objective of the neural model in selected amortized SBI techniques. By introducing a relaxation of the classical formulation of calibration error we enable end-to-end backpropagation. The proposed method is not tied to any particular neural model and brings moderate computational overhead compared to the profits it introduces. It is directly applicable to existing computational pipelines allowing reliable black-box posterior inference. We empi
    
[^28]: 等变深度权重空间对齐

    Equivariant Deep Weight Space Alignment. (arXiv:2310.13397v1 [cs.LG])

    [http://arxiv.org/abs/2310.13397](http://arxiv.org/abs/2310.13397)

    本论文提出了一个名为Deep-Align的新框架，用于学习解决权重对齐问题，以加速对齐过程并提高其质量。

    

    深度网络的排列对称性使得简单操作如模型平均和相似度估计变得困难。在许多情况下，对齐网络的权重，即找到它们之间最优排列，是必要的。更一般地说，权重对齐对于广泛的应用非常重要，从模型合并，通过探索深度神经网络的优化空间，到定义神经网络之间有意义的距离函数。不幸的是，权重对齐是一个NP-hard问题。先前的研究主要集中在解决对齐问题的松弛版本，导致方法耗时或者次优解。为了加速对齐过程并提高其质量，我们提出了一个名为Deep-Align的新框架，旨在学习解决权重对齐问题。为此，我们首先证明了权重对齐遵循两个基本对称性，然后提出了一个深度架构。

    Permutation symmetries of deep networks make simple operations like model averaging and similarity estimation challenging. In many cases, aligning the weights of the networks, i.e., finding optimal permutations between their weights, is necessary. More generally, weight alignment is essential for a wide range of applications, from model merging, through exploring the optimization landscape of deep neural networks, to defining meaningful distance functions between neural networks. Unfortunately, weight alignment is an NP-hard problem. Prior research has mainly focused on solving relaxed versions of the alignment problem, leading to either time-consuming methods or sub-optimal solutions. To accelerate the alignment process and improve its quality, we propose a novel framework aimed at learning to solve the weight alignment problem, which we name Deep-Align. To that end, we first demonstrate that weight alignment adheres to two fundamental symmetries and then, propose a deep architecture 
    
[^29]: RL-X: 一个用于RoboCup的深度强化学习库（不仅限于）

    RL-X: A Deep Reinforcement Learning Library (not only) for RoboCup. (arXiv:2310.13396v1 [cs.RO])

    [http://arxiv.org/abs/2310.13396](http://arxiv.org/abs/2310.13396)

    本文介绍了 RL-X 这个新的深度强化学习库，并展示了它在 RoboCup 和经典 DRL 基准测试中的应用，以及相比于其他框架的加速效果。

    

    本文介绍了新的深度强化学习（DRL）库RL-X及其在RoboCup Soccer Simulation 3D联赛和经典DRL基准测试中的应用。RL-X提供了一个灵活且易于扩展的代码库，具有自包含的单目录算法。通过基于快速的JAX实现，RL-X与知名框架（如Stable-Baselines3）相比，可以达到高达4.5倍的加速效果。

    This paper presents the new Deep Reinforcement Learning (DRL) library RL-X and its application to the RoboCup Soccer Simulation 3D League and classic DRL benchmarks. RL-X provides a flexible and easy-to-extend codebase with self-contained single directory algorithms. Through the fast JAX-based implementations, RL-X can reach up to 4.5x speedups compared to well-known frameworks like Stable-Baselines3.
    
[^30]: 在多臂赌博机中以固定置信度进行最优臂识别

    Optimal Best Arm Identification with Fixed Confidence in Restless Bandits. (arXiv:2310.13393v1 [stat.ML])

    [http://arxiv.org/abs/2310.13393](http://arxiv.org/abs/2310.13393)

    在多臂赌博机中，我们研究了以固定置信度进行最优臂识别。我们提出了一种新的策略来识别具有最大平均值的臂，同时限制了决策错误的概率，并建立了停止时间增长率的下界。

    

    我们研究了在具有有限数目臂的多臂赌博机中以不断变化的形式进行最优臂识别。每个臂产生的离散时间数据形成了一个取值在共同、有限状态空间中的同质马尔可夫链。每个臂的状态转移由一个遵循单参数指数族的转移概率矩阵（TPM）捕获。每个臂的TPM的实值参数是未知的，属于给定空间。给定在臂的共同状态空间上定义的函数f，目标是在样本数最少的情况下识别最优臂，即在该臂的稳态分布下评估f的平均值最大的臂，同时满足对决策错误概率（即固定置信度区间）的上界。在渐进性的误差概率趋于零的情况下，建立了期望停止时间增长率的下界。此外，我们提出了一种最优臂识别策略。

    We study best arm identification in a restless multi-armed bandit setting with finitely many arms. The discrete-time data generated by each arm forms a homogeneous Markov chain taking values in a common, finite state space. The state transitions in each arm are captured by an ergodic transition probability matrix (TPM) that is a member of a single-parameter exponential family of TPMs. The real-valued parameters of the arm TPMs are unknown and belong to a given space. Given a function $f$ defined on the common state space of the arms, the goal is to identify the best arm -- the arm with the largest average value of $f$ evaluated under the arm's stationary distribution -- with the fewest number of samples, subject to an upper bound on the decision's error probability (i.e., the fixed-confidence regime). A lower bound on the growth rate of the expected stopping time is established in the asymptote of a vanishing error probability. Furthermore, a policy for best arm identification is propo
    
[^31]: 使用分布式Hebbian Temporal Memory学习继任者表示法

    Learning Successor Representations with Distributed Hebbian Temporal Memory. (arXiv:2310.13391v1 [cs.LG])

    [http://arxiv.org/abs/2310.13391](http://arxiv.org/abs/2310.13391)

    本文提出了一种名为DHTM的算法，它基于因子图形式和多组成神经元模型，利用分布式表示、稀疏转移矩阵和局部Hebbian样学习规则来解决在线隐藏表示学习的挑战。实验结果表明，DHTM在变化的环境中比经典的LSTM效果更好，并与更先进的类似RNN的算法性能相当，可以加速继任者表示的时间差异学习。

    

    本文提出了一种新颖的方法来解决在线隐藏表示学习的挑战，该方法用于在不稳定的、部分可观测的环境中进行决策。所提出的算法，分布式Hebbian Temporal Memory (DHTM)，基于因子图形式和多组成神经元模型。DHTM旨在捕捉顺序数据关系并对未来观察作出累积预测，形成继任者表示。受新皮层的神经生理学模型启发，该算法利用分布式表示、稀疏转移矩阵和局部Hebbian样学习规则克服了传统时间记忆算法（如RNN和HMM）的不稳定性和慢速学习过程。实验结果表明，DHTM优于经典的LSTM，并与更先进的类似RNN的算法性能相当，在变化的环境中加速了继任者表示的时间差异学习。此外，我们还进行了比较。

    This paper presents a novel approach to address the challenge of online hidden representation learning for decision-making under uncertainty in non-stationary, partially observable environments. The proposed algorithm, Distributed Hebbian Temporal Memory (DHTM), is based on factor graph formalism and a multicomponent neuron model. DHTM aims to capture sequential data relationships and make cumulative predictions about future observations, forming Successor Representation (SR). Inspired by neurophysiological models of the neocortex, the algorithm utilizes distributed representations, sparse transition matrices, and local Hebbian-like learning rules to overcome the instability and slow learning process of traditional temporal memory algorithms like RNN and HMM. Experimental results demonstrate that DHTM outperforms classical LSTM and performs comparably to more advanced RNN-like algorithms, speeding up Temporal Difference learning for SR in changing environments. Additionally, we compare
    
[^32]: 音频指纹识别中的音乐增强和降噪

    Music Augmentation and Denoising For Peak-Based Audio Fingerprinting. (arXiv:2310.13388v1 [cs.SD])

    [http://arxiv.org/abs/2310.13388](http://arxiv.org/abs/2310.13388)

    本研究通过引入音频增强流水线和深度学习模型，为嘈杂环境下的音频指纹识别提出了解决方案，提高了识别性能。

    

    音频指纹识别是一种从简短录音片段中识别歌曲的成熟解决方案。流行的方法依赖于抽取稀疏表示，通常是频谱峰值，并且已经证明准确、快速，并可扩展到大型集合。然而，在现实世界中，音频识别的应用往往发生在嘈杂的环境中，这可能导致这些系统失效。在这项工作中，我们通过引入和发布一个新的音频增强流水线来解决这个问题，该流水线以一种逼真的方式向音乐片段添加噪声，通过随机模拟现实世界的场景。然后，我们提出并发布了一个深度学习模型，该模型从谱图中去除噪声成分，以提高基于峰值的指纹识别系统的准确性。我们证明了我们的模型的添加改进了常用音频指纹识别系统的识别性能，即使在嘈杂的条件下也是如此。

    Audio fingerprinting is a well-established solution for song identification from short recording excerpts. Popular methods rely on the extraction of sparse representations, generally spectral peaks, and have proven to be accurate, fast, and scalable to large collections. However, real-world applications of audio identification often happen in noisy environments, which can cause these systems to fail. In this work, we tackle this problem by introducing and releasing a new audio augmentation pipeline that adds noise to music snippets in a realistic way, by stochastically mimicking real-world scenarios. We then propose and release a deep learning model that removes noisy components from spectrograms in order to improve peak-based fingerprinting systems' accuracy. We show that the addition of our model improves the identification performance of commonly used audio fingerprinting systems, even under noisy conditions.
    
[^33]: 因果发现中的假设违规和score matching的鲁棒性

    Assumption violations in causal discovery and the robustness of score matching. (arXiv:2310.13387v1 [stat.ME])

    [http://arxiv.org/abs/2310.13387](http://arxiv.org/abs/2310.13387)

    本文在不同背景条件下对最近的因果发现方法在观察性独立同分布数据上的实际性能进行了基准测试，发现基于score matching的方法在具有挑战性的场景中表现出令人惊讶的性能。

    

    当领域知识有限且实验受到道德、财务或时间限制时，从业者会转向观察性因果发现方法来恢复因果结构，利用其数据的统计特性。由于没有进一步的假设，因果发现是一个不适定的问题，每个算法都有其自己的一套通常无法验证的假设，其中一些在真实数据集中很难满足。鉴于这些考虑，本文在不同背景条件下对最近的因果发现方法在观察性独立同分布数据上的实际性能进行了广泛的基准测试，允许违反每个选定方法所需的关键假设。我们的实验结果显示，在这些具有挑战性场景中，基于score matching的方法在推断的图的误报与漏报率方面表现出令人惊讶的性能，并提供了对其的理论洞察。

    When domain knowledge is limited and experimentation is restricted by ethical, financial, or time constraints, practitioners turn to observational causal discovery methods to recover the causal structure, exploiting the statistical properties of their data. Because causal discovery without further assumptions is an ill-posed problem, each algorithm comes with its own set of usually untestable assumptions, some of which are hard to meet in real datasets. Motivated by these considerations, this paper extensively benchmarks the empirical performance of recent causal discovery methods on observational i.i.d. data generated under different background conditions, allowing for violations of the critical assumptions required by each selected approach. Our experimental findings show that score matching-based methods demonstrate surprising performance in the false positive and false negative rate of the inferred graph in these challenging scenarios, and we provide theoretical insights into their
    
[^34]: Tuna: 使用大型语言模型的反馈进行指令调整

    Tuna: Instruction Tuning using Feedback from Large Language Models. (arXiv:2310.13385v1 [cs.CL])

    [http://arxiv.org/abs/2310.13385](http://arxiv.org/abs/2310.13385)

    本文提出了一种使用大型语言模型的反馈进行指令调整的方法，通过使用概率排名和上下文排名来增加生成更好响应的可能性。

    

    使用来自更强大的语言模型（如Instruct-GPT和GPT-4）的直接输出，对开源大型语言模型（如LLaMA）进行指令调整已被证明是一种具有成本效益的方法，可以使模型行为与人类偏好相一致。然而，指令调整的模型仅看到每个指令的一个响应，缺乏可能更好的响应的知识。在本文中，我们提出了一个使用我们的新颖的“概率排名”和“上下文排名”方法来对指令调整的LLM进行微调，以增加生成更好响应的可能性。概率排名使指令调整的模型继承了来自教师LLM的高质量和低质量响应的相对排名。另一方面，学习上下文排名可以让模型使用更强大LLM的上下文理解能力来进一步优化自己的响应分布。此外，我们将概率排名和上下文排名按顺序应用于...

    Instruction tuning of open-source large language models (LLMs) like LLaMA, using direct outputs from more powerful LLMs such as Instruct-GPT and GPT-4, has proven to be a cost-effective way to align model behaviors with human preferences. However, the instruction-tuned model has only seen one response per instruction, lacking the knowledge of potentially better responses. In this paper, we propose finetuning an instruction-tuned LLM using our novel \textit{probabilistic ranking} and \textit{contextual ranking} approaches to increase the likelihood of generating better responses. Probabilistic ranking enables the instruction-tuned model to inherit the relative rankings of high-quality and low-quality responses from the teacher LLM. On the other hand, learning with contextual ranking allows the model to refine its own response distribution using the contextual understanding ability of stronger LLMs. Furthermore, we apply probabilistic ranking and contextual ranking sequentially to the in
    
[^35]: Salted Inference: 在移动计算中提升隐私并保持分割推理的效率的方法

    Salted Inference: Enhancing Privacy while Maintaining Efficiency of Split Inference in Mobile Computing. (arXiv:2310.13384v1 [cs.LG])

    [http://arxiv.org/abs/2310.13384](http://arxiv.org/abs/2310.13384)

    本文介绍了一种名为“盐化DNN”的方法，通过在推理时让客户控制DNN输出的语义解释，同时保持准确性和效率与标准DNN几乎一致，提升了移动计算中隐私和计算效率的问题。

    

    分割推理将深度神经网络（DNN）划分为两部分，边缘部分在设备上运行，后续部分在云端运行。这满足了设备上机器学习的两个关键需求：输入隐私和计算效率。然而，在分割推理中，一个未解决的问题是输出隐私，因为DNN的输出可见于云端。尽管加密计算可以保护输出隐私，但需要大量的计算和通信资源。本文引入了“盐化DNN”：一种新颖的方法，可以让客户在推理时控制DNN输出的语义解释，同时保持准确性和效率与标准DNN几乎一致。在图像和传感器数据上进行的实验评估表明，盐化DNN在分类准确性上与标准DNN非常接近，尤其是当盐化层位于较早阶段以满足分割推理的要求时。我们的方法是通用的，可以应用于各种领域。

    Split inference partitions a deep neural network (DNN) to run the early part at the edge and the later part in the cloud. This meets two key requirements for on-device machine learning: input privacy and compute efficiency. Still, an open question in split inference is output privacy, given that the output of a DNN is visible to the cloud. While encrypted computing can protect output privacy, it mandates extensive computation and communication resources. In this paper, we introduce "Salted DNNs": a novel method that lets clients control the semantic interpretation of DNN output at inference time while maintaining accuracy and efficiency very close to that of a standard DNN. Experimental evaluations conducted on both image and sensor data show that Salted DNNs achieve classification accuracy very close to standard DNNs, particularly when the salted layer is positioned within the early part to meet the requirements of split inference. Our method is general and can be applied to various D
    
[^36]: 大规模数据聚类问题的加速稀疏核谱聚类

    Accelerated sparse Kernel Spectral Clustering for large scale data clustering problems. (arXiv:2310.13381v1 [cs.LG])

    [http://arxiv.org/abs/2310.13381](http://arxiv.org/abs/2310.13381)

    这篇论文介绍了改进的稀疏多路径核谱聚类算法，通过不完全Cholesky分解和降维方法的组合实现了稀疏性，并通过解决核心特征值问题的对称版本大大提高了计算特性。

    

    本文介绍了改进的稀疏多路径核谱聚类算法。原始算法是在原始-对偶最小二乘支持向量机框架下，基于加权核主成分分析(KPCA)从而得到的。通过不完全Cholesky分解(ICD)和所谓的降维方法的组合，实现了稀疏性。原始的ICD稀疏KSC算法在应用于大规模数据聚类问题时计算复杂度过高，这导致该算法迄今只具有理论上的重要性。本文通过改进，大大提高了计算特性。解决计算复杂度最高的核心特征值问题的可替代的对称版本，消除了形成稀疏低秩核矩阵的必要性。

    An improved version of the sparse multiway kernel spectral clustering (KSC) is presented in this brief. The original algorithm is derived from weighted kernel principal component (KPCA) analysis formulated within the primal-dual least-squares support vector machine (LS-SVM) framework. Sparsity is achieved then by the combination of the incomplete Cholesky decomposition (ICD) based low rank approximation of the kernel matrix with the so called reduced set method. The original ICD based sparse KSC algorithm was reported to be computationally far too demanding, especially when applied on large scale data clustering problems that actually it was designed for, which has prevented to gain more than simply theoretical relevance so far. This is altered by the modifications reported in this brief that drastically improve the computational characteristics. Solving the alternative, symmetrized version of the computationally most demanding core eigenvalue problem eliminates the necessity of formin
    
[^37]: SigFormer: 基于路径签名和Transformer的深度对冲模型

    SigFormer: Signature Transformers for Deep Hedging. (arXiv:2310.13369v1 [cs.LG])

    [http://arxiv.org/abs/2310.13369](http://arxiv.org/abs/2310.13369)

    SigFormer是一种基于路径签名和Transformer的深度对冲模型，通过结合路径签名的能力捕捉复杂的数据模式，同时利用Transformer提供优秀的序列注意力，在设计神经网络架构方面具有优势。实验证明，SigFormer在合成数据上表现出更快的学习速度和增强的鲁棒性，特别是在存在不规则价格数据的情况下。同时，通过对冲SP 500指数的真实回测，SigFormer展示出积极的结果。

    

    深度对冲是量化金融中的一个有前景的方向，结合了深度学习研究中的模型和技术。虽然可以提供出色的对冲策略，但模型对于神经网络的架构设计需要仔细处理。为了缓解这些困难，我们引入了SigFormer，这是一个将路径签名和Transformer的能力结合起来处理序列数据的新型深度学习模型，特别是对于存在不规则性的情况。路径签名可以有效捕捉复杂的数据模式，而Transformer则提供了优秀的序列注意力。我们在合成数据上对我们提出的模型进行了实证比较，展示了更快的学习速度和增强的鲁棒性，特别是在存在不规则的基础价格数据的情况下。此外，我们通过对冲SP 500指数的真实回测验证了我们模型的性能，并展示了积极的结果。

    Deep hedging is a promising direction in quantitative finance, incorporating models and techniques from deep learning research. While giving excellent hedging strategies, models inherently requires careful treatment in designing architectures for neural networks. To mitigate such difficulties, we introduce SigFormer, a novel deep learning model that combines the power of path signatures and transformers to handle sequential data, particularly in cases with irregularities. Path signatures effectively capture complex data patterns, while transformers provide superior sequential attention. Our proposed model is empirically compared to existing methods on synthetic data, showcasing faster learning and enhanced robustness, especially in the presence of irregular underlying price data. Additionally, we validate our model performance through a real-world backtest on hedging the SP 500 index, demonstrating positive outcomes.
    
[^38]: VFedMH: 垂直联合学习用于训练多参与方异构模型

    VFedMH: Vertical Federated Learning for Training Multi-party Heterogeneous Models. (arXiv:2310.13367v1 [cs.LG])

    [http://arxiv.org/abs/2310.13367](http://arxiv.org/abs/2310.13367)

    VFedMH是一种垂直联合学习方法，通过在前向传播过程中聚合参与者的嵌入来处理参与者之间的异构模型，解决了现有VFL方法面临的挑战。

    

    垂直联合学习（VFL）作为一种集成样本对齐和特征合并的新型训练范式，已经引起了越来越多的关注。然而，现有的VFL方法在处理参与者之间存在异构本地模型时面临挑战，这影响了优化收敛性和泛化能力。为了解决这个问题，本文提出了一种名为VFedMH的新方法，用于训练多方异构模型。VFedMH的重点是在前向传播期间聚合每个参与者知识的嵌入，而不是中间结果。主动方，拥有样本的标签和特征，在VFedMH中安全地聚合本地嵌入以获得全局知识嵌入，并将其发送给被动方。被动方仅拥有样本的特征，然后利用全局嵌入在其本地异构网络上进行前向传播。然而，被动方不拥有标签。

    Vertical Federated Learning (VFL) has gained increasing attention as a novel training paradigm that integrates sample alignment and feature union. However, existing VFL methods face challenges when dealing with heterogeneous local models among participants, which affects optimization convergence and generalization. To address this issue, this paper proposes a novel approach called Vertical Federated learning for training Multi-parties Heterogeneous models (VFedMH). VFedMH focuses on aggregating the embeddings of each participant's knowledge instead of intermediate results during forward propagation. The active party, who possesses labels and features of the sample, in VFedMH securely aggregates local embeddings to obtain global knowledge embeddings, and sends them to passive parties. The passive parties, who own only features of the sample, then utilize the global embeddings to propagate forward on their local heterogeneous networks. However, the passive party does not own the labels, 
    
[^39]: 揭示因果偏差

    Dissecting Causal Biases. (arXiv:2310.13364v1 [cs.LG])

    [http://arxiv.org/abs/2310.13364](http://arxiv.org/abs/2310.13364)

    本文研究了机器学习中因数据生成和收集方式引起的因果偏差，提供了分别以模型参数为基础的四种偏差源的闭合形式表达式，可以分析它们的行为并在不同情况下判断它们的存在和最大化程度。

    

    准确地测量基于机器学习的自动决策系统中的歧视是解决子群体和/或个体之间公平问题的关键。在测量歧视时的任何偏差都可能导致对歧视真实值的放大或低估。本文关注一类由训练数据生成和/或收集方式引起的偏差，我们将其称为因果偏差，并使用因果性领域的工具来形式化定义和分析这些偏差。我们考虑四种偏差源：混杂，选择，测量和交互作用。本文的主要贡献是为每种偏差源提供了一个以模型参数为基础的闭合形式表达式。这使得可以分析每种偏差源的行为，特别是在哪些情况下它们不存在以及在哪些情况下它们被最大化。我们希望提供的特征有助于更好的研究社区。

    Accurately measuring discrimination in machine learning-based automated decision systems is required to address the vital issue of fairness between subpopulations and/or individuals. Any bias in measuring discrimination can lead to either amplification or underestimation of the true value of discrimination. This paper focuses on a class of bias originating in the way training data is generated and/or collected. We call such class causal biases and use tools from the field of causality to formally define and analyze such biases. Four sources of bias are considered, namely, confounding, selection, measurement, and interaction. The main contribution of this paper is to provide, for each source of bias, a closed-form expression in terms of the model parameters. This makes it possible to analyze the behavior of each source of bias, in particular, in which cases they are absent and in which other cases they are maximized. We hope that the provided characterizations help the community better 
    
[^40]: 通过行为测试实现对机器翻译的通用错误诊断

    Towards General Error Diagnosis via Behavioral Testing in Machine Translation. (arXiv:2310.13362v1 [cs.CL])

    [http://arxiv.org/abs/2310.13362](http://arxiv.org/abs/2310.13362)

    本论文提出了一种基于双语翻译对生成的行为测试框架，用于诊断机器翻译系统的通用错误。该框架通过自动构建高质量的测试用例及其伪参考，能够在没有人工参考的情况下进行翻译质量评估。

    

    行为测试为诊断语言错误和评估自然语言处理模型的能力提供了重要手段。然而，将行为测试应用于机器翻译系统是具有挑战性的，因为通常需要人力来制作评估这些系统在新生成的测试用例上的翻译质量的参考。现有的机器翻译行为测试工作通过在没有参考的情况下评估翻译质量来绕开这个问题，但这限制了对特定类型错误的诊断，比如单个数字或货币词的错误翻译。为了诊断通用错误，本文提出了一种基于双语翻译对生成的行为测试（BTPGBT）框架来进行机器翻译行为测试。BTPGBT的核心思想是采用一种新颖的双语翻译对生成（BTPG）方法，自动构建高质量的测试用例及其伪参考。实验结果表明，该方法能够有效诊断机器翻译系统的通用错误。

    Behavioral testing offers a crucial means of diagnosing linguistic errors and assessing capabilities of NLP models. However, applying behavioral testing to machine translation (MT) systems is challenging as it generally requires human efforts to craft references for evaluating the translation quality of such systems on newly generated test cases. Existing works in behavioral testing of MT systems circumvent this by evaluating translation quality without references, but this restricts diagnosis to specific types of errors, such as incorrect translation of single numeric or currency words. In order to diagnose general errors, this paper proposes a new Bilingual Translation Pair Generation based Behavior Testing (BTPGBT) framework for conducting behavioral testing of MT systems. The core idea of BTPGBT is to employ a novel bilingual translation pair generation (BTPG) approach that automates the construction of high-quality test cases and their pseudoreferences. Experimental results on var
    
[^41]: DeepFDR：一种用于神经影像数据的基于深度学习的虚警控制方法

    DeepFDR: A Deep Learning-based False Discovery Rate Control Method for Neuroimaging Data. (arXiv:2310.13349v1 [stat.ML])

    [http://arxiv.org/abs/2310.13349](http://arxiv.org/abs/2310.13349)

    DeepFDR是一种基于深度学习的虚警控制方法，通过利用无监督的图像分割技术解决神经影像数据中的多重检验问题，并在实验证明其相对于现有方法具有卓越的性能。

    

    基于体素的多重检验在神经影像数据分析中广泛应用。传统的虚警控制方法常常忽视基于体素的检验之间的空间相关性，从而导致测试能力的大幅损失。虽然最近出现了一些空间虚警控制方法，但是当处理复杂的脑空间依赖关系时，它们的有效性和最优性仍存在疑问。与此同时，深度学习方法已经在图像分割方面取得了革命性的进展，而图像分割与基于体素的多重检验密切相关。本文提出了一种名为DeepFDR的新型空间虚警控制方法，利用无监督的基于深度学习的图像分割来解决基于体素的多重检验问题。包括全面的模拟和阿尔茨海默病FDG-PET影像分析在内的数值研究表明DeepFDR相对于现有方法具有优势。DeepFDR不仅在虚警控制方面表现出色，还有效降低了虚假的非发现率。

    Voxel-based multiple testing is widely used in neuroimaging data analysis. Traditional false discovery rate (FDR) control methods often ignore the spatial dependence among the voxel-based tests and thus suffer from substantial loss of testing power. While recent spatial FDR control methods have emerged, their validity and optimality remain questionable when handling the complex spatial dependencies of the brain. Concurrently, deep learning methods have revolutionized image segmentation, a task closely related to voxel-based multiple testing. In this paper, we propose DeepFDR, a novel spatial FDR control method that leverages unsupervised deep learning-based image segmentation to address the voxel-based multiple testing problem. Numerical studies, including comprehensive simulations and Alzheimer's disease FDG-PET image analysis, demonstrate DeepFDR's superiority over existing methods. DeepFDR not only excels in FDR control and effectively diminishes the false nondiscovery rate, but als
    
[^42]: 针对无限制多匹配和聚类问题的非负球面松弛方法

    Non-Negative Spherical Relaxations for Universe-Free Multi-Matching and Clustering. (arXiv:2310.13311v1 [stat.ML])

    [http://arxiv.org/abs/2310.13311](http://arxiv.org/abs/2310.13311)

    针对多匹配和聚类问题，提出了一种新颖的非负球面松弛方法，可以自动调整连续参数，而不需要事先固定整数参数，且能够直接得到二进制结果。

    

    我们提出了一种新颖的非负球面松弛方法，用于具有唯一性约束的二进制矩阵优化问题，特别适用于多匹配和聚类。我们将相应的二进制矩阵约束松弛到（高维）非负球面上。为了优化我们的松弛问题，我们使用一种条件幂迭代方法来迭代改进目标函数，同时在与宇宙大小（或簇的数量）（间接）相关的连续标量参数上进行扫描。与现有的在优化之前需要固定整数宇宙大小的方法相反，我们的方法自动调整类似的连续参数。此外，虽然我们的方法与谱多匹配和谱聚类有一定相似之处，但我们的制定方案具有一个强大的优势，即我们不依赖额外的后处理过程来获得二进制结果。我们的方法在各种应用情况下显示出令人信服的结果。

    We propose a novel non-negative spherical relaxation for optimization problems over binary matrices with injectivity constraints, which in particular has applications in multi-matching and clustering. We relax respective binary matrix constraints to the (high-dimensional) non-negative sphere. To optimize our relaxed problem, we use a conditional power iteration method to iteratively improve the objective function, while at same time sweeping over a continuous scalar parameter that is (indirectly) related to the universe size (or number of clusters). Opposed to existing procedures that require to fix the integer universe size before optimization, our method automatically adjusts the analogous continuous parameter. Furthermore, while our approach shares similarities with spectral multi-matching and spectral clustering, our formulation has the strong advantage that we do not rely on additional post-processing procedures to obtain binary results. Our method shows compelling results in vari
    
[^43]: 测试时自适应小型语言模型在问答中的应用

    Test-Time Self-Adaptive Small Language Models for Question Answering. (arXiv:2310.13307v1 [cs.CL])

    [http://arxiv.org/abs/2310.13307](http://arxiv.org/abs/2310.13307)

    本论文研究了仅使用无标记测试数据的小型自适应语言模型在问答任务中的能力，并提出了一种自适应策略，该策略通过随机生成多个答案并进行集成，以达到显著的性能提升和更高的鲁棒性。

    

    最近，经过指导微调的大型语言模型在各种任务中取得了显著的成绩，例如问答。然而，尽管它们能够记忆各种任务的大量常识，但由于能力有限，转移和适应知识到目标任务时可能不是最优的。此外，由于缺少标记的数据集，使用带标记的数据集对模型进行进一步微调通常是不可行的，但我们也可以通过仅使用无标记的测试数据来转移具有有限知识的更小语言模型也是一个值得探究的问题。在这项工作中，我们展示并研究了仅使用无标记测试数据的较小自适应语言模型的能力。特别地，我们首先随机生成多个答案，然后在过滤掉低质量样本的同时对它们进行集成，以减轻不准确标签的噪声。我们提出的自适应策略在基准QA数据集上表现出了显著的性能提升，具有更高的鲁棒性。

    Recent instruction-finetuned large language models (LMs) have achieved notable performances in various tasks, such as question-answering (QA). However, despite their ability to memorize a vast amount of general knowledge across diverse tasks, they might be suboptimal on specific tasks due to their limited capacity to transfer and adapt knowledge to target tasks. Moreover, further finetuning LMs with labeled datasets is often infeasible due to their absence, but it is also questionable if we can transfer smaller LMs having limited knowledge only with unlabeled test data. In this work, we show and investigate the capabilities of smaller self-adaptive LMs, only with unlabeled test data. In particular, we first stochastically generate multiple answers, and then ensemble them while filtering out low-quality samples to mitigate noise from inaccurate labels. Our proposed self-adaption strategy demonstrates significant performance improvements on benchmark QA datasets with higher robustness ac
    
[^44]: 解码沉默的大多数：利用大型语言模型诱导信念增强的社交图进行响应预测

    Decoding the Silent Majority: Inducing Belief Augmented Social Graph with Large Language Model for Response Forecasting. (arXiv:2310.13297v1 [cs.CL])

    [http://arxiv.org/abs/2310.13297](http://arxiv.org/abs/2310.13297)

    通过利用大型语言模型，我们提出了一个名为SocialSense的框架，通过诱导信念增强的图和基于图的传播来预测新闻发布的响应。这个方法在没有用户明确个人资料或历史行为的情况下，能够有效地捕捉社交动态。

    

    新闻媒体的自动响应预测在帮助内容生产者有效预测新闻发布的影响并防止意外的负面结果（如社会冲突和道德伤害）方面起着关键作用。为了有效地预测响应，必须开发能够利用个体周围的社交动态和背景信息的措施，尤其是在用户明确的个人资料或历史行为有限的情况下（即潜在参与者）。正如先前的研究所示，97%的推文仅由最活跃的25%用户产生。然而，现有的方法对于如何处理和利用这些重要特征的最佳方式进行了有限的探索。为了弥补这一差距，我们提出了一个名为SocialSense的新型框架，利用大型语言模型在现有社交网络之上诱导出一个以信念为中心的图，以及基于图的传播来捕捉社交动态。我们假设诱导出的图可以...

    Automatic response forecasting for news media plays a crucial role in enabling content producers to efficiently predict the impact of news releases and prevent unexpected negative outcomes such as social conflict and moral injury. To effectively forecast responses, it is essential to develop measures that leverage the social dynamics and contextual information surrounding individuals, especially in cases where explicit profiles or historical actions of the users are limited (referred to as lurkers). As shown in a previous study, 97% of all tweets are produced by only the most active 25% of users. However, existing approaches have limited exploration of how to best process and utilize these important features. To address this gap, we propose a novel framework, named SocialSense, that leverages a large language model to induce a belief-centered graph on top of an existent social network, along with graph-based propagation to capture social dynamics. We hypothesize that the induced graph 
    
[^45]: CXR-CLIP：面向大规模胸部 X 射线语言-图像预训练的方法

    CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training. (arXiv:2310.13292v1 [cs.CV])

    [http://arxiv.org/abs/2310.13292](http://arxiv.org/abs/2310.13292)

    本文提出了一种面向大规模胸部 X 射线语言-图像预训练的方法，通过扩展图像-标签对为图像-文本对，并利用放射学报告中的多个图像和多个部分来解决胸部 X 射线图像中的数据稀缺性问题。我们的模型在相同条件下的训练中优于现有技术水平的模型，并通过扩充数据集提高了我们预训练模型的分类能力。

    

    一个大规模的图文配对数据集对于视觉-语言预训练模型的发展做出了巨大贡献，这些模型使得在没有昂贵标注的情况下实现了零样本或少样本分类。然而，在医学领域，数据的稀缺性仍然是开发强大的视觉-语言预训练模型的重大挑战。本文通过扩展图像-标签对为图像-文本对，并利用放射学报告中的多个图像和多个部分来解决胸部 X 射线图像中的图文数据缺乏问题。我们还设计了两种对比损失，分别称为 ICL 和 TCL，用于学习医学图像和报告的研究层面特征。我们的模型在相同条件下的训练中优于现有技术水平的模型。此外，扩大的数据集提高了我们预训练模型的分类能力，而牺牲了较小的检索性能。代码可在 https://github.com/kakaobrain/cxr-clip 获得。

    A large-scale image-text pair dataset has greatly contributed to the development of vision-language pre-training (VLP) models, which enable zero-shot or few-shot classification without costly annotation. However, in the medical domain, the scarcity of data remains a significant challenge for developing a powerful VLP model. In this paper, we tackle the lack of image-text data in chest X-ray by expanding image-label pair as image-text pair via general prompt and utilizing multiple images and multiple sections in a radiologic report. We also design two contrastive losses, named ICL and TCL, for learning study-level characteristics of medical images and reports, respectively. Our model outperforms the state-of-the-art models trained under the same conditions. Also, enlarged dataset improve the discriminative power of our pre-trained model for classification, while sacrificing marginal retrieval performance. Code is available at https://github.com/kakaobrain/cxr-clip.
    
[^46]: 评估语言模型中的隐私风险：以摘要任务为案例研究

    Assessing Privacy Risks in Language Models: A Case Study on Summarization Tasks. (arXiv:2310.13291v1 [cs.CL])

    [http://arxiv.org/abs/2310.13291](http://arxiv.org/abs/2310.13291)

    本研究评估了语言模型在摘要任务中的隐私风险，发现摘要模型存在泄露数据成员身份的风险，并讨论了一些保护措施和隐私与效用之间的权衡。

    

    大型语言模型通过在各种任务上实现最先进的性能，改变了自然语言处理领域。然而，人们担心这些模型可能泄露训练数据中的信息。本研究集中在摘要任务上，调查成员推断（MI）攻击：通过给出一个样本和对模型API的黑盒访问，可以确定样本是否是训练数据的一部分。我们利用文本相似性和模型对文档修改的抵抗力作为潜在的MI信号，并评估它们在广泛使用的数据集上的有效性。我们的结果表明，摘要模型存在泄露数据成员身份的风险，甚至在没有参考摘要可用的情况下也是如此。此外，我们讨论了训练摘要模型以防止MI攻击的几种保护措施，并讨论了隐私和效用之间固有的权衡。

    Large language models have revolutionized the field of NLP by achieving state-of-the-art performance on various tasks. However, there is a concern that these models may disclose information in the training data. In this study, we focus on the summarization task and investigate the membership inference (MI) attack: given a sample and black-box access to a model's API, it is possible to determine if the sample was part of the training data. We exploit text similarity and the model's resistance to document modifications as potential MI signals and evaluate their effectiveness on widely used datasets. Our results demonstrate that summarization models are at risk of exposing data membership, even in cases where the reference summary is not available. Furthermore, we discuss several safeguards for training summarization models to protect against MI attacks and discuss the inherent trade-off between privacy and utility.
    
[^47]: 使用时间局部规则学习循环模型

    Learning Recurrent Models with Temporally Local Rules. (arXiv:2310.13284v1 [cs.LG])

    [http://arxiv.org/abs/2310.13284](http://arxiv.org/abs/2310.13284)

    本研究提出一种新的方法，要求生成模型学习当前状态和先前状态的联合分布，以替代传统的反向传递计算。在玩具数据集上的实验证明，这种方法可以学习到通常需要反向传递计算的数据方面。

    

    将生成模型拟合到顺序数据通常需要通过时间进行两个递归计算，一个正向计算，一个反向计算。后者可以是损失梯度的计算（如反向传播），也可以是推理算法的计算（如RTS/Kalman平滑器）。特别是反向传递计算在计算上很昂贵（因为它是串行的，无法利用GPU），而且很难映射到生物过程。已经提出了一些解决方法；在这里，我们探索了一个非常不同的方法：要求生成模型学习当前状态和先前状态的联合分布，而不仅仅是转移概率。我们在玩具数据集上展示了使用这一原则的不同架构可以学习通常需要反向传递计算的数据方面。

    Fitting generative models to sequential data typically involves two recursive computations through time, one forward and one backward. The latter could be a computation of the loss gradient (as in backpropagation through time), or an inference algorithm (as in the RTS/Kalman smoother). The backward pass in particular is computationally expensive (since it is inherently serial and cannot exploit GPUs), and difficult to map onto biological processes. Work-arounds have been proposed; here we explore a very different one: requiring the generative model to learn the joint distribution over current and previous states, rather than merely the transition probabilities. We show on toy datasets that different architectures employing this principle can learn aspects of the data typically requiring the backward pass.
    
[^48]: FedLoRA：基于LoRA调整的模型异构个性化联邦学习

    FedLoRA: Model-Heterogeneous Personalized Federated Learning with LoRA Tuning. (arXiv:2310.13283v1 [cs.LG])

    [http://arxiv.org/abs/2310.13283](http://arxiv.org/abs/2310.13283)

    FedLoRA是基于LoRA调整的计算和通信高效的模型异构个性化联邦学习框架，旨在为每个联邦学习客户端训练个性化且异构的本地模型。

    

    联邦学习是一种新兴的机器学习范 Paradig，其中一个中央服务器协调多个参与者（即FL客户端）在分散的数据上进行协作训练模型，同时保护隐私。这种范 Paradig 限制了所有客户端必须使用相同结构的模型（同构）。实践中，FL经常面临统计异质性、系统异质性和模型异质性等挑战。这些问题激发了模型异构个性化联邦学习（MHPFL）领域的研究，旨在为每个FL客户端训练一个个性化且异构的本地模型。现有的MHPFL方法无法同时实现令人满意的模型性能、可接受的计算开销和高效的通信。为了填补这一差距，我们提出了一种基于LoRA调整的计算和通信高效的模型异构个性化联邦学习框架（FedLoRA）。

    Federated learning (FL) is an emerging machine learning paradigm in which a central server coordinates multiple participants (a.k.a. FL clients) to train a model collaboratively on decentralized data with privacy protection. This paradigm constrains that all clients have to train models with the same structures (homogeneous). In practice, FL often faces statistical heterogeneity, system heterogeneity and model heterogeneity challenges. These challenging issues inspire the field of Model-Heterogeneous Personalized Federated Learning (MHPFL) which aims to train a personalized and heterogeneous local model for each FL client. Existing MHPFL approaches cannot achieve satisfactory model performance, acceptable computational overhead and efficient communication simultaneously. To bridge this gap, we propose a novel computation- and communication-efficient model-heterogeneous personalized Federated learning framework based on LoRA tuning (FedLoRA). It is designed to incorporate a homogeneous 
    
[^49]: 通过反向图卷积实现鲁棒的跨模态检索

    InvGC: Robust Cross-Modal Retrieval by Inverse Graph Convolution. (arXiv:2310.13276v1 [cs.CV])

    [http://arxiv.org/abs/2310.13276](http://arxiv.org/abs/2310.13276)

    通过反向图卷积进行的鲁棒跨模态检索，解决了表示退化问题，并通过增加数据点之间的距离来有效分离不同模态的表示。

    

    近年来，跨模态检索的重大进展主要是通过视觉和语言建模的突破推动的。然而，最近的研究表明，多模态数据表示往往在有限的凸锥内聚集（作为表示退化问题），这由于这些表示的不可分离性而阻碍了检索性能。在我们的研究中，我们首先通过多个跨模态基准和方法经验证实了表示退化问题的存在。接下来，为了解决这个问题，我们引入了一种新方法，称为InvGC，它是一种受图卷积和平均池化启发的后处理技术。具体而言，InvGC在数据集中定义图拓扑，然后应用图卷积以一种减法的方式。这种方法通过增加数据点之间的距离来有效地分离表示。为了提高InvGC的效率和效果，我们提出了一个高级图拓扑，Lo

    Over recent decades, significant advancements in cross-modal retrieval are mainly driven by breakthroughs in visual and linguistic modeling. However, a recent study shows that multi-modal data representations tend to cluster within a limited convex cone (as representation degeneration problem), which hinders retrieval performance due to the inseparability of these representations. In our study, we first empirically validate the presence of the representation degeneration problem across multiple cross-modal benchmarks and methods. Next, to address it, we introduce a novel method, called InvGC, a post-processing technique inspired by graph convolution and average pooling. Specifically, InvGC defines the graph topology within the datasets and then applies graph convolution in a subtractive manner. This method effectively separates representations by increasing the distances between data points. To improve the efficiency and effectiveness of InvGC, we propose an advanced graph topology, Lo
    
[^50]: 基于元学习的物理学一致神经网络高效求解新给定的偏微分方程问题

    Meta-learning of Physics-informed Neural Networks for Efficiently Solving Newly Given PDEs. (arXiv:2310.13270v1 [stat.ML])

    [http://arxiv.org/abs/2310.13270](http://arxiv.org/abs/2310.13270)

    该论文提出了一种基于神经网络的元学习方法，用于高效求解偏微分方程问题。通过编码问题表示和采用物理学一致神经网络框架来训练模型，可以在未知解决方案的情况下评估误差。

    

    我们提出了一种基于神经网络的元学习方法，用于高效求解偏微分方程（PDE）问题。该方法旨在元学习如何解决各种PDE问题，并将其知识应用于解决新给定的PDE问题。我们使用神经网络将PDE问题编码为问题表示，其中控制方程通过偏导数的多项式函数系数表示，边界条件通过一组点条件对表示。我们将问题表示作为神经网络的输入，用于预测解决方案，这使我们能够通过神经网络的前向过程高效地预测问题特定的解决方案，而无需更新模型参数。为了训练我们的模型，我们最小化基于“物理学一致神经网络”框架适应于PDE问题时的预期误差，借此我们可以在解决方案未知的情况下评估误差。我们证明了该方法的实用性和高效性。

    We propose a neural network-based meta-learning method to efficiently solve partial differential equation (PDE) problems. The proposed method is designed to meta-learn how to solve a wide variety of PDE problems, and uses the knowledge for solving newly given PDE problems. We encode a PDE problem into a problem representation using neural networks, where governing equations are represented by coefficients of a polynomial function of partial derivatives, and boundary conditions are represented by a set of point-condition pairs. We use the problem representation as an input of a neural network for predicting solutions, which enables us to efficiently predict problem-specific solutions by the forwarding process of the neural network without updating model parameters. To train our model, we minimize the expected error when adapted to a PDE problem based on the physics-informed neural network framework, by which we can evaluate the error even when solutions are unknown. We demonstrate that 
    
[^51]: 用于学习排序中特征选择的模拟退火探索性研究

    An Exploratory Study on Simulated Annealing for Feature Selection in Learning-to-Rank. (arXiv:2310.13269v1 [cs.LG])

    [http://arxiv.org/abs/2310.13269](http://arxiv.org/abs/2310.13269)

    该研究探讨了在学习排序中使用模拟退火进行特征选择的方法，并引入了进展参数来有效遍历搜索空间。实验证明了该方法的有效性。

    

    学习排序是一种应用于监督式机器学习的领域。由于特征选择已被发现可以有效提高学习模型的准确性，因此研究将该过程应用于学习排序领域是非常有趣的。在本研究中，我们调查了一种称为模拟退火的流行元启发式方法在该任务中的应用。在模拟退火的一般框架下，我们探索了各种邻域选择策略和温度冷却方案。我们进一步引入了一个新的超参数，称为进展参数，可以有效地用于遍历搜索空间。我们的算法在五个公开基准数据集上进行了评估。为了更好地验证，我们还将基于模拟退火的特征选择算法与另一种有效的元启发式算法，即局部波束搜索进行了比较。广泛的实验结果表明了我们提出的模型的有效性。

    Learning-to-rank is an applied domain of supervised machine learning. As feature selection has been found to be effective for improving the accuracy of learning models in general, it is intriguing to investigate this process for learning-to-rank domain. In this study, we investigate the use of a popular meta-heuristic approach called simulated annealing for this task. Under the general framework of simulated annealing, we explore various neighborhood selection strategies and temperature cooling schemes. We further introduce a new hyper-parameter called the progress parameter that can effectively be used to traverse the search space. Our algorithms are evaluated on five publicly benchmark datasets of learning-to-rank. For a better validation, we also compare the simulated annealing-based feature selection algorithm with another effective meta-heuristic algorithm, namely local beam search. Extensive experimental results shows the efficacy of our proposed models.
    
[^52]: DPM-Solver-v3: 使用经验模型统计改进的扩散ODE求解器

    DPM-Solver-v3: Improved Diffusion ODE Solver with Empirical Model Statistics. (arXiv:2310.13268v1 [cs.CV])

    [http://arxiv.org/abs/2310.13268](http://arxiv.org/abs/2310.13268)

    本论文提出了DPM-Solver-v3，一个基于经验模型统计的新型快速ODE求解器，用于优化扩散概率模型的采样效率，并提出了一种新的参数化方法以减小ODE解的离散化误差。同时，引入了多步方法和预测-校正框架来进一步改善采样质量。

    

    扩散概率模型（DPMs）在生成高保真度图像方面表现出色，但采样效率低下。最近的研究通过提出利用DPMs的特定ODE形式的快速ODE求解器来加速采样过程。然而，它们在推理过程中高度依赖特定参数化（如噪声/数据预测），这可能并不是最佳选择。在本研究中，我们提出了一种新的公式，以获得在采样过程中最佳参数化，以最小化ODE解的一阶离散化错误。基于这种公式，我们引入了几个在预训练模型上高效计算的系数，并提出了一个名为DPM-Solver-v3的新快速ODE求解器，我们称之为“经验模型统计”。我们进一步结合了多步方法和预测-校正框架，并提出了一些技术来改善在少量函数评估（NFE）或大规模数据集上的样本质量。

    Diffusion probabilistic models (DPMs) have exhibited excellent performance for high-fidelity image generation while suffering from inefficient sampling. Recent works accelerate the sampling procedure by proposing fast ODE solvers that leverage the specific ODE form of DPMs. However, they highly rely on specific parameterization during inference (such as noise/data prediction), which might not be the optimal choice. In this work, we propose a novel formulation towards the optimal parameterization during sampling that minimizes the first-order discretization error of the ODE solution. Based on such formulation, we propose \textit{DPM-Solver-v3}, a new fast ODE solver for DPMs by introducing several coefficients efficiently computed on the pretrained model, which we call \textit{empirical model statistics}. We further incorporate multistep methods and a predictor-corrector framework, and propose some techniques for improving sample quality at small numbers of function evaluations (NFE) or
    
[^53]: 关于对比跨模态模型的语言编码器的研究

    On the Language Encoder of Contrastive Cross-modal Models. (arXiv:2310.13267v1 [cs.CL])

    [http://arxiv.org/abs/2310.13267](http://arxiv.org/abs/2310.13267)

    本研究探究了对比跨模态模型中的语言编码器，并发现在视觉语言任务中，句子嵌入训练提高了语言编码器的质量并改善了跨模态任务的性能。而在音频语言任务中，句子嵌入训练效果较小。另外，句子嵌入训练改善了文本空间的均匀性，但降低了跨模态对齐度。

    

    对比跨模态模型如CLIP和CLAP在各种视觉语言和音频语言任务中发挥作用。然而，对它们的语言编码器的研究和改进还很有限，而语言编码器是将图像/音频的自然语言描述编码成向量表示的核心组件。我们广泛评估了无监督和监督的句子嵌入训练对语言编码器质量和跨模态任务性能的影响。在视觉语言预训练中，我们发现句子嵌入训练提高了语言编码器质量，并有助于跨模态任务，改进了诸如CyCLIP等对比视觉语言模型。相比之下，音频语言预训练对句子嵌入训练的效益较小，可能是由于预训练数据量有限的原因。我们分析了表示空间，以了解句子嵌入训练的优势，并发现它改善了文本空间的均匀性，但代价是降低了跨模态对齐度。

    Contrastive cross-modal models such as CLIP and CLAP aid various vision-language (VL) and audio-language (AL) tasks. However, there has been limited investigation of and improvement in their language encoder, which is the central component of encoding natural language descriptions of image/audio into vector representations. We extensively evaluate how unsupervised and supervised sentence embedding training affect language encoder quality and cross-modal task performance. In VL pretraining, we found that sentence embedding training language encoder quality and aids in cross-modal tasks, improving contrastive VL models such as CyCLIP. In contrast, AL pretraining benefits less from sentence embedding training, which may result from the limited amount of pretraining data. We analyze the representation spaces to understand the strengths of sentence embedding training, and find that it improves text-space uniformity, at the cost of decreased cross-modal alignment.
    
[^54]: DIG-MILP: 一种具有可行性保证的混合整数线性规划的深度样例生成器

    DIG-MILP: a Deep Instance Generator for Mixed-Integer Linear Programming with Feasibility Guarantee. (arXiv:2310.13261v1 [cs.LG])

    [http://arxiv.org/abs/2310.13261](http://arxiv.org/abs/2310.13261)

    DIG-MILP是一个深度生成框架，利用变分自编码器从非常有限的MILP数据中提取深层结构特征，并生成与目标数据非常接近的样例。它通过利用MILP的对偶性来确保生成空间的正确性、完整性以及生成实例的有界性和可行性。

    

    混合整数线性规划（MILP）是许多关键工业应用中一个重要的NP难问题。有效算法的开发，求解器的调优以及用于MILP解析的机器学习模型的训练都依赖于大量、多样和代表性的数据。然而，与图像和文本领域丰富的自然数据相比，MILP的数据缺乏，凸显了合成MILP生成的重要作用。我们提出了DIG-MILP，这是一个基于变分自编码器（VAE）的深度生成框架，能够从非常有限的MILP数据中提取深层结构特征，并生成与目标数据非常接近的样例。值得注意的是，通过利用MILP的对偶性，DIG-MILP保证了正确和完整的生成空间，并确保了生成实例的有界性和可行性。我们的实证研究突出了DIG-MILP生成的样例的新颖性和质量。

    Mixed-integer linear programming (MILP) stands as a notable NP-hard problem pivotal to numerous crucial industrial applications. The development of effective algorithms, the tuning of solvers, and the training of machine learning models for MILP resolution all hinge on access to extensive, diverse, and representative data. Yet compared to the abundant naturally occurring data in image and text realms, MILP is markedly data deficient, underscoring the vital role of synthetic MILP generation. We present DIG-MILP, a deep generative framework based on variational auto-encoder (VAE), adept at extracting deep-level structural features from highly limited MILP data and producing instances that closely mirror the target data. Notably, by leveraging the MILP duality, DIG-MILP guarantees a correct and complete generation space as well as ensures the boundedness and feasibility of the generated instances. Our empirical study highlights the novelty and quality of the instances generated by DIG-MIL
    
[^55]: ManiCast: 基于成本感知人体预测的协同操作

    ManiCast: Collaborative Manipulation with Cost-Aware Human Forecasting. (arXiv:2310.13258v1 [cs.RO])

    [http://arxiv.org/abs/2310.13258](http://arxiv.org/abs/2310.13258)

    ManiCast是一个基于成本感知的人体预测的协同操作框架，通过提供能够捕捉未来人体运动如何影响机器人计划成本的预测，实现了人机协同操纵任务的流畅执行和实时交互。

    

    紧密合作的人机操纵依赖准确的人体运动预测。尽管在大规模学习预测模型方面取得了显著进展，但当应用于操作任务时，这些模型在关键转折点处积累了较高的误差，导致下游规划性能的降低。我们的关键见解是，与其预测最有可能的人体运动，产生能够捕捉未来人体运动如何影响机器人计划成本的预测就足够了。我们提出了ManiCast，一种新颖的框架，学习成本感知的人体预测并将其提供给模型预测控制规划器以执行协同操作任务。我们的框架实现了人类和7个自由度的机械臂在如反应搅拌、物体交接和协同摆桌等多个真实任务中的流畅、实时交互。我们对运动预测和端到端的预测-规划系统进行了评估

    Seamless human-robot manipulation in close proximity relies on accurate forecasts of human motion. While there has been significant progress in learning forecast models at scale, when applied to manipulation tasks, these models accrue high errors at critical transition points leading to degradation in downstream planning performance. Our key insight is that instead of predicting the most likely human motion, it is sufficient to produce forecasts that capture how future human motion would affect the cost of a robot's plan. We present ManiCast, a novel framework that learns cost-aware human forecasts and feeds them to a model predictive control planner to execute collaborative manipulation tasks. Our framework enables fluid, real-time interactions between a human and a 7-DoF robot arm across a number of real-world tasks such as reactive stirring, object handovers, and collaborative table setting. We evaluate both the motion forecasts and the end-to-end forecaster-planner system against a
    
[^56]: 知识图谱增强的多样化推荐

    Knowledge Graph Context-Enhanced Diversified Recommendation. (arXiv:2310.13253v1 [cs.IR])

    [http://arxiv.org/abs/2310.13253](http://arxiv.org/abs/2310.13253)

    该研究在知识图谱背景下探索多样化推荐系统，通过引入创新的度量标准和评分函数，有效提高了知识图谱推荐算法的多样性。

    

    推荐系统领域一直致力于通过利用用户的历史交互来提高准确性。然而，这种追求准确性的同时往往导致了多样性的降低，从而产生了众所周知的“回声室”现象。多样化推荐系统作为一种对策应运而生，将多样性与准确性同等看待，并在学术界和行业实践者中获得了显著的关注。本研究探索了多样化推荐系统在复杂的知识图谱（KG）背景下的应用。这些知识图谱是连接实体和项目的信息库，通过加入深入的上下文信息，提供了增加推荐多样性的有利途径。我们的贡献包括引入了一种创新的度量标准，实体覆盖和关系覆盖，有效地量化了知识图谱领域的多样性。此外，我们还引入了多样化评分函数，该函数通过综合利用实体覆盖和关系覆盖来提高推荐算法的多样性。

    The field of Recommender Systems (RecSys) has been extensively studied to enhance accuracy by leveraging users' historical interactions. Nonetheless, this persistent pursuit of accuracy frequently engenders diminished diversity, culminating in the well-recognized "echo chamber" phenomenon. Diversified RecSys has emerged as a countermeasure, placing diversity on par with accuracy and garnering noteworthy attention from academic circles and industry practitioners. This research explores the realm of diversified RecSys within the intricate context of knowledge graphs (KG). These KGs act as repositories of interconnected information concerning entities and items, offering a propitious avenue to amplify recommendation diversity through the incorporation of insightful contextual information. Our contributions include introducing an innovative metric, Entity Coverage, and Relation Coverage, which effectively quantifies diversity within the KG domain. Additionally, we introduce the Diversified
    
[^57]: FLEE-GNN：一种用于分析多种商品食品流的地理空间韧性的边缘增强图神经网络的联邦学习系统

    FLEE-GNN: A Federated Learning System for Edge-Enhanced Graph Neural Network in Analyzing Geospatial Resilience of Multicommodity Food Flows. (arXiv:2310.13248v1 [cs.LG])

    [http://arxiv.org/abs/2310.13248](http://arxiv.org/abs/2310.13248)

    本文提出了FLEE-GNN，它是一种用于分析多种商品食品流的地理空间韧性的边缘增强图神经网络的联邦学习系统。FLEE-GNN克服了当前方法的局限性，提高了分析食品供应网络韧性的普适性、可扩展性和数据隐私保护。

    

    理解和衡量食品供应网络的韧性是解决日益严重的食品不安全问题的全球任务。然而，这些网络的复杂性以及它们的多维交互和决策提出了重大挑战。本文提出了一种名为FLEE-GNN的新型联邦学习系统，用于边缘增强的图神经网络，旨在克服这些挑战，增强多种商品食品流网络的地理空间韧性分析，它是空间网络的一种类型。FLEE-GNN解决了当前方法的局限性，例如基于熵的方法在普适性，可扩展性和数据隐私方面的限制。它将图神经网络的鲁棒性和适应性与联邦学习的数据隐私意识和分散特性相结合，用于跨地理区域的食品供应网络韧性分析。本文还讨论了FLEE-GNN的创新数据生成技术，实验设计和f

    Understanding and measuring the resilience of food supply networks is a global imperative to tackle increasing food insecurity. However, the complexity of these networks, with their multidimensional interactions and decisions, presents significant challenges. This paper proposes FLEE-GNN, a novel Federated Learning System for Edge-Enhanced Graph Neural Network, designed to overcome these challenges and enhance the analysis of geospatial resilience of multicommodity food flow network, which is one type of spatial networks. FLEE-GNN addresses the limitations of current methodologies, such as entropy-based methods, in terms of generalizability, scalability, and data privacy. It combines the robustness and adaptability of graph neural networks with the privacy-conscious and decentralized aspects of federated learning on food supply network resilience analysis across geographical regions. This paper also discusses FLEE-GNN's innovative data generation techniques, experimental designs, and f
    
[^58]: 透明度挑战与因果机器学习中的政策评估 - 提高可用性和问责性

    Transparency challenges in policy evaluation with causal machine learning -- improving usability and accountability. (arXiv:2310.13240v1 [cs.LG])

    [http://arxiv.org/abs/2310.13240](http://arxiv.org/abs/2310.13240)

    透明度问题是因果机器学习在政策评估中的挑战，因为黑盒子模型难以理解和问责。本文提出了通过可解释的AI工具和简化模型来解决这些问题的方法。

    

    因果机器学习工具开始在实际政策评估任务中使用，灵活估计治疗效果。这些方法的一个问题是所使用的机器学习模型通常是黑盒子，即没有全局可解释的方式来理解模型如何进行估计。这在政策评估应用中是一个明显的问题，特别是在政府领域，因为很难理解这些模型是否按照公正的方式运行，基于正确的证据解释，并且透明到足以允许在出现问题时进行问责。然而，在因果机器学习文献中很少讨论透明度问题以及如何解决这些问题。本文探讨了为什么透明度问题是因果机器学习在公共政策评估应用中的问题，并考虑通过可解释的AI工具和简化模型来解决这些问题的方法。

    Causal machine learning tools are beginning to see use in real-world policy evaluation tasks to flexibly estimate treatment effects. One issue with these methods is that the machine learning models used are generally black boxes, i.e., there is no globally interpretable way to understand how a model makes estimates. This is a clear problem in policy evaluation applications, particularly in government, because it is difficult to understand whether such models are functioning in ways that are fair, based on the correct interpretation of evidence and transparent enough to allow for accountability if things go wrong. However, there has been little discussion of transparency problems in the causal machine learning literature and how these might be overcome. This paper explores why transparency issues are a problem for causal machine learning in public policy evaluation applications and considers ways these problems might be addressed through explainable AI tools and by simplifying models in
    
[^59]: 用联邦学习训练语义通信系统

    Training A Semantic Communication System with Federated Learning. (arXiv:2310.13236v1 [cs.LG])

    [http://arxiv.org/abs/2310.13236](http://arxiv.org/abs/2310.13236)

    该论文介绍了用联邦学习训练语义通信系统的方法，通过利用用户数据而不泄露隐私的方式，减少了通信开销，并提高了网络资源利用率。

    

    语义通信因其减少数据冗余的功能成为下一代通信系统的重要支柱。大多数语义通信系统使用先进的深度学习模型构建，其性能严重依赖于数据的可用性。这些研究假设有丰富的训练数据可用，这是不现实的。实际上，数据主要是由用户生成的。由于隐私和安全问题，数据传输受到限制，这对于传统的集中式训练方案是必要的。为了解决这个挑战，我们在联邦学习（FL）环境中探索语义通信，利用用户数据而不泄露隐私。此外，我们设计了一个系统来解决通信开销问题，通过减少每个全局轮次传送的信息量。这样，我们可以为资源受限的设备节省大量带宽，减少整体网络流量。最后，我们提出了...

    Semantic communication has emerged as a pillar for the next generation of communication systems due to its capabilities in alleviating data redundancy. Most semantic communication systems are built using advanced deep learning models whose performance heavily depends on data availability. These studies assume that an abundance of training data is available, which is unrealistic. In practice, data is mainly created on the user side. Due to privacy and security concerns, the transmission of data is restricted, which is necessary for conventional centralized training schemes. To address this challenge, we explore semantic communication in federated learning (FL) setting that utilizes user data without leaking privacy. Additionally, we design our system to tackle the communication overhead by reducing the quantity of information delivered in each global round. In this way, we can save significant bandwidth for resource-limited devices and reduce overall network traffic. Finally, we propose
    
[^60]: 多层对比学习用于基于剧本的角色理解

    Multi-level Contrastive Learning for Script-based Character Understanding. (arXiv:2310.13231v1 [cs.CL])

    [http://arxiv.org/abs/2310.13231](http://arxiv.org/abs/2310.13231)

    本文提出了一种多层对比学习框架用于剧本中角色的理解，从角色的话语中学习其个性和身份，并在多个角色理解子任务上通过与强大的预训练语言模型的比较进行了验证。

    

    本文针对剧本中的角色理解场景，旨在从他们的话语中学习角色的个性和身份。我们首先分析了这一场景中的几个挑战，然后提出了一种多层对比学习框架，以细粒度的方式捕捉角色的全局信息。为了验证所提出的框架，我们与强大的预训练语言模型（包括SpanBERT，Longformer，BigBird和ChatGPT-3.5）进行了三个角色理解子任务的广泛实验比较。实验结果表明，我们的方法在性能上取得了显著改进。通过深入分析，我们展示了我们的方法在解决挑战和提供更多关于角色理解场景的线索方面的有效性。我们将在github上开源我们的工作，网址为https://github.com/David-Li0406/Script-based-Character-Understanding。

    In this work, we tackle the scenario of understanding characters in scripts, which aims to learn the characters' personalities and identities from their utterances. We begin by analyzing several challenges in this scenario, and then propose a multi-level contrastive learning framework to capture characters' global information in a fine-grained manner. To validate the proposed framework, we conduct extensive experiments on three character understanding sub-tasks by comparing with strong pre-trained language models, including SpanBERT, Longformer, BigBird and ChatGPT-3.5. Experimental results demonstrate that our method improves the performances by a considerable margin. Through further in-depth analysis, we show the effectiveness of our method in addressing the challenges and provide more hints on the scenario of character understanding. We will open-source our work on github at https://github.com/David-Li0406/Script-based-Character-Understanding.
    
[^61]: 绝对策略优化

    Absolute Policy Optimization. (arXiv:2310.13230v1 [cs.LG])

    [http://arxiv.org/abs/2310.13230](http://arxiv.org/abs/2310.13230)

    这篇论文提出了绝对策略优化（APO）的方法，通过优化一个新颖的目标函数，在保证性能下界的同时，实现了连续控制任务和Atari游戏中的令人瞩目的结果。

    

    近年来，基于信任域的在线策略强化学习在解决复杂控制任务和游戏场景方面取得了令人瞩目的结果。然而，这一类别中现有的最先进算法主要强调对预期性能的改进，缺乏对最坏情况下性能结果的控制能力。为了解决这个限制，我们引入了一个新颖的目标函数；通过优化该函数，可以确保近乎总体性能样本的下界（绝对性能）呈现单调改进。考虑到这一具有突破性的理论进展，我们通过一系列的近似对这个理论基础算法进行了改进，得到了一种实用的解决方案称为绝对策略优化（APO）。我们的实验证明了我们的方法在具有挑战性的连续控制基准任务上的有效性，并将其适用性扩展到掌握Atari游戏。我们的发现表明，APO在提高性能的同时也显著改善了最坏情况下的性能结果。

    In recent years, trust region on-policy reinforcement learning has achieved impressive results in addressing complex control tasks and gaming scenarios. However, contemporary state-of-the-art algorithms within this category primarily emphasize improvement in expected performance, lacking the ability to control over the worst-case performance outcomes. To address this limitation, we introduce a novel objective function; by optimizing which, it will lead to guaranteed monotonic improvement in the lower bound of near-total performance samples (absolute performance). Considering this groundbreaking theoretical advancement, we then refine this theoretically grounded algorithm through a series of approximations, resulting in a practical solution called Absolute Policy Optimization (APO). Our experiments demonstrate the effectiveness of our approach across challenging continuous control benchmark tasks and extend its applicability to mastering Atari games. Our findings reveal that APO signifi
    
[^62]: ToolChain*: 使用A*搜索在大型语言模型中进行高效的动作空间导航

    ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search. (arXiv:2310.13227v1 [cs.CL])

    [http://arxiv.org/abs/2310.13227](http://arxiv.org/abs/2310.13227)

    ToolChain*是一种提供了高效动作空间导航的树搜索算法，它可以在大型语言模型中进行决策和规划，解决复杂的真实世界问题。

    

    大型语言模型（LLM）在解决复杂的真实世界问题时展现了强大的决策和规划能力。基于LLM的自主代理可以与各种工具（例如功能性API）进行交互，并生成执行一系列API函数调用的解决方案计划。候选API函数调用的多样性显著扩展了动作空间，加大了对高效动作空间导航的关键需求。然而，现有方法要么在庞大的动作空间中面临单向探索困难，陷入局部优化解，要么遭受穷尽地遍历所有潜在动作所导致的低效导航。为了解决这些问题，我们提出了ToolChain*，一种基于树搜索的LLM代理规划算法。它将整个动作空间构建为一个决策树，其中每个节点表示解决方案计划中涉及的可能的API函数调用。

    Large language models (LLMs) have demonstrated powerful decision-making and planning capabilities in solving complicated real-world problems. LLM-based autonomous agents can interact with diverse tools (e.g., functional APIs) and generate solution plans that execute a series of API function calls in a step-by-step manner. The multitude of candidate API function calls significantly expands the action space, amplifying the critical need for efficient action space navigation. However, existing methods either struggle with unidirectional exploration in expansive action spaces, trapped into a locally optimal solution, or suffer from exhaustively traversing all potential actions, causing inefficient navigation. To address these issues, we propose ToolChain*, an efficient tree search-based planning algorithm for LLM-based agents. It formulates the entire action space as a decision tree, where each node represents a possible API function call involved in a solution plan. By incorporating the A
    
[^63]: 可扩展的神经网络内核

    Scalable Neural Network Kernels. (arXiv:2310.13225v1 [cs.LG])

    [http://arxiv.org/abs/2310.13225](http://arxiv.org/abs/2310.13225)

    可扩展神经网络内核（SNNKs）是一种替代常规前馈层的方法，能够近似实现常规前馈层的功能，但具有更优的计算特性。通过将内核与参数-输入向量的点积联系起来，SNNKs能够有效地解开参数与输入之间的联系，从而模拟复杂关系。此外，我们还引入了神经网络捆绑过程，将SNNKs应用于深度神经网络压缩，进一步提高了压缩效果。最终捆绑网络甚至可以绕过反向传播，通过显式公式求解最优参数。

    

    我们引入了可扩展神经网络内核（SNNKs）的概念，这是常规前馈层（FFLs）的替代品，能够近似实现后者，但具有有利的计算属性。SNNKs有效地解开了FFL中参数与输入之间的联系，并通过点积内核在最终计算中连接它们。它们也更加表达力强，能够模拟复杂关系，超出参数-输入向量的函数范围。我们还引入了神经网络捆绑过程，将SNNKs应用于压缩深度神经网络结构，从而获得额外的压缩效益。在极端情况下，它导致完全捆绑网络，其最优参数可以通过多个损失函数（例如均方误差）的显式公式来表示，从而有可能绕过反向传播。作为我们分析的副产品，我们引入了普遍性机制的机制。

    We introduce the concept of scalable neural network kernels (SNNKs), the replacements of regular feedforward layers (FFLs), capable of approximating the latter, but with favorable computational properties. SNNKs effectively disentangle the inputs from the parameters of the neural network in the FFL, only to connect them in the final computation via the dot-product kernel. They are also strictly more expressive, as allowing to model complicated relationships beyond the functions of the dot-products of parameter-input vectors. We also introduce the neural network bundling process that applies SNNKs to compactify deep neural network architectures, resulting in additional compression gains. In its extreme version, it leads to the fully bundled network whose optimal parameters can be expressed via explicit formulae for several loss functions (e.g. mean squared error), opening a possibility to bypass backpropagation. As a by-product of our analysis, we introduce the mechanism of the universa
    
[^64]: 等变Transformer就是你所需要的

    Equivariant Transformer is all you need. (arXiv:2310.13222v1 [hep-lat])

    [http://arxiv.org/abs/2310.13222](http://arxiv.org/abs/2310.13222)

    等变Transformer能够解决自学习Monte-Carlo中的低接受率问题，实现了较大的模型容量，在模拟物理系统中具有重要的创新和贡献。

    

    机器学习、深度学习已经在计算物理学中取得了加速，用于在格点上模拟系统。等变性对于模拟物理系统是至关重要的，因为它为机器学习模型描述的概率分布引入了强烈的归纳偏差。这降低了数据对称性和物理定律偏离的错误外推的风险。然而，将对称性强加于模型有时会导致自学习Monte-Carlo（SLMC）中的接受率不佳。另一方面，Transformers中使用的Attention实现了较大的模型容量。我们将等变性Attention引入到SLMC中。为了评估我们的架构，我们将其应用于我们提出的二维格点自旋费米模型上的新架构。我们发现它克服了线性模型的低接受率，并观察到与Transformers中的大型语言模型相似的接受率的标度律。

    Machine learning, deep learning, has been accelerating computational physics, which has been used to simulate systems on a lattice. Equivariance is essential to simulate a physical system because it imposes a strong induction bias for the probability distribution described by a machine learning model. This reduces the risk of erroneous extrapolation that deviates from data symmetries and physical laws. However, imposing symmetry on the model sometimes occur a poor acceptance rate in self-learning Monte-Carlo (SLMC). On the other hand, Attention used in Transformers like GPT realizes a large model capacity. We introduce symmetry equivariant attention to SLMC. To evaluate our architecture, we apply it to our proposed new architecture on a spin-fermion model on a two-dimensional lattice. We find that it overcomes poor acceptance rates for linear models and observe the scaling law of the acceptance rate as in the large language models with Transformers.
    
[^65]: 使用Transformer的上下文学习与对比学习模式是等价的

    In-context Learning with Transformer Is Really Equivalent to a Contrastive Learning Pattern. (arXiv:2310.13220v1 [cs.LG])

    [http://arxiv.org/abs/2310.13220](http://arxiv.org/abs/2310.13220)

    本文将上下文学习的推理过程解释为对比学习模式中的梯度下降过程，通过建立梯度下降与自注意机制之间的关系，并分析了对应梯度下降过程，提出了可能的改进，并设计实验证明了这一观点。

    

    基于Transformer的预训练大型语言模型展示了惊人的上下文学习能力。在给定几个示例的情况下，模型可以在不进行任何参数更新的情况下执行新任务。然而，理解上下文学习的机制仍然是一个未解决的问题。在本文中，我们将上下文学习的推理过程解释为对比学习模式中的梯度下降过程。首先，利用核方法建立梯度下降与自注意机制之间的关系，在一般使用的softmax注意设置下而不是线性注意设置下。然后，我们从对比学习的角度分析了上下文学习的对应梯度下降过程，讨论了在这种对比学习模式下可能的改进，基于这些改进可以进一步修改自注意层。最后，我们设计了实验来支持我们的观点。据我们所知，我们的工作是第一个将上下文学习与对比学习模式等价的研究。

    Pre-trained large language models based on Transformers have demonstrated amazing in-context learning (ICL) abilities. Given several demonstration examples, the models can implement new tasks without any parameter updates. However, it is still an open question to understand the mechanism of ICL. In this paper, we interpret the inference process of ICL as a gradient descent process in a contrastive learning pattern. Firstly, leveraging kernel methods, we establish the relationship between gradient descent and self-attention mechanism under generally used softmax attention setting instead of linear attention setting. Then, we analyze the corresponding gradient descent process of ICL from the perspective of contrastive learning without negative samples and discuss possible improvements of this contrastive learning pattern, based on which the self-attention layer can be further modified. Finally, we design experiments to support our opinions. To the best of our knowledge, our work is the f
    
[^66]: 深度学习分析气候变化、创新和不确定性

    A Deep Learning Analysis of Climate Change, Innovation, and Uncertainty. (arXiv:2310.13200v1 [econ.GN])

    [http://arxiv.org/abs/2310.13200](http://arxiv.org/abs/2310.13200)

    本研究使用深度学习分析了气候变化、创新和不确定性，发现模型不确定性对决策和社会估值产生首要影响，并在整合的气候经济创新框架中提出了对投资调整的重要调整。

    

    我们在一个气候经济框架中研究了模型不确定性的影响，该框架涉及三种类型的资本：在生产过程中产生碳排放的"脏"资本，没有排放但起初不如"脏"资本效率高的"清洁"资本，以及随着研发投资增加并导致绿色部门生产力技术创新的知识资本。为了解决我们的高维非线性模型框架，我们实施了基于神经网络的全局解决方法。我们展示了模型不确定性对最优决策和社会估值在我们的整合气候经济创新框架中的一阶影响。考虑到气候动力学的相互依赖不确定性、气候变化带来的经济损害以及绿色技术变革的到来，会对不同类型的资本投资进行大幅调整，以应对技术变革和气候损害严重程度的揭示。

    We study the implications of model uncertainty in a climate-economics framework with three types of capital: "dirty" capital that produces carbon emissions when used for production, "clean" capital that generates no emissions but is initially less productive than dirty capital, and knowledge capital that increases with R\&D investment and leads to technological innovation in green sector productivity. To solve our high-dimensional, non-linear model framework we implement a neural-network-based global solution method. We show there are first-order impacts of model uncertainty on optimal decisions and social valuations in our integrated climate-economic-innovation framework. Accounting for interconnected uncertainty over climate dynamics, economic damages from climate change, and the arrival of a green technological change leads to substantial adjustments to investment in the different capital types in anticipation of technological change and the revelation of climate damage severity.
    
[^67]: NameGuess：用于表格数据的列名称扩展

    NameGuess: Column Name Expansion for Tabular Data. (arXiv:2310.13196v1 [cs.CL])

    [http://arxiv.org/abs/2310.13196](http://arxiv.org/abs/2310.13196)

    该论文介绍了一种用于扩展表格数据中列名称的新方法，通过使用大型语言模型进行自然语言生成，解决了缩写列名称对数据搜索、访问和理解任务的负面影响。作者提出的NameGuess方法通过对表格内容和列头名称进行调整，得到了与人类相匹配的性能。

    

    最近大型语言模型的进展已经在许多行业中引起了革命，包括数据库行业。在处理大量的表格数据时，一个常见的挑战是普遍使用缩写的列名称，这可能会对各种数据搜索、访问和理解任务产生负面影响。为了解决这个问题，我们提出了一个新的任务，名为NameGuess，将扩展列名称（用于数据库模式）视为自然语言生成问题。我们使用一种新的数据制作方法创建了一个包含384K个缩写-扩展列对的训练数据集，并使用人工标注的评估基准进行评估，其中包括来自真实世界表格的9.2K个例子。为了应对NameGuess中的多义和歧义带来的复杂性，我们通过对表格内容和列头名称进行调整来增强自回归语言模型，得到了一个经过微调的模型（具有2.7B个参数），其性能与人类相匹配。此外，我们进行了全面的分析（o

    Recent advances in large language models have revolutionized many sectors, including the database industry. One common challenge when dealing with large volumes of tabular data is the pervasive use of abbreviated column names, which can negatively impact performance on various data search, access, and understanding tasks. To address this issue, we introduce a new task, called NameGuess, to expand column names (used in database schema) as a natural language generation problem. We create a training dataset of 384K abbreviated-expanded column pairs using a new data fabrication method and a human-annotated evaluation benchmark that includes 9.2K examples from real-world tables. To tackle the complexities associated with polysemy and ambiguity in NameGuess, we enhance auto-regressive language models by conditioning on table content and column header names -- yielding a fine-tuned model (with 2.7B parameters) that matches human performance. Furthermore, we conduct a comprehensive analysis (o
    
[^68]: 基于异构图神经网络的数据驱动交通分配研究

    Heterogeneous Graph Neural Networks for Data-driven Traffic Assignment. (arXiv:2310.13193v1 [cs.LG])

    [http://arxiv.org/abs/2310.13193](http://arxiv.org/abs/2310.13193)

    本论文提出了一种基于异构图神经网络的数据驱动交通分配和交通流学习方法，该方法能够准确捕捉不同链路之间的空间交通模式，优于其他传统神经网络模型，在大规模网络中有着广泛的应用潜力。

    

    交通分配问题是交通流分析的重要组成部分之一，已经提出了各种解决方法。然而，将这些方法应用于大规模网络面临重大挑战。在本文中，我们利用异构图神经网络的强大能力，提出了一种新颖的基于数据驱动的交通分配和交通流学习方法。所提出的模型能够捕捉不同链路之间的空间交通模式，从而产生高度准确的结果。我们在城市交通网络上进行了数值实验，并展示了该异构图神经网络模型在收敛速度、训练损失和预测准确度方面优于其他传统神经网络模型的表现。值得注意的是，所提出的异构图神经网络模型还可以推广到不同的网络拓扑。这种方法为复杂交通流分析和预测提供了一种有希望的解决方案。

    The traffic assignment problem is one of the significant components of traffic flow analysis for which various solution approaches have been proposed. However, deploying these approaches for large-scale networks poses significant challenges. In this paper, we leverage the power of heterogeneous graph neural networks to propose a novel data-driven approach for traffic assignment and traffic flow learning. The proposed model is capable of capturing spatial traffic patterns across different links, yielding highly accurate results. We present numerical experiments on urban transportation networks and show that the proposed heterogeneous graph neural network model outperforms other conventional neural network models in terms of convergence rate, training loss, and prediction accuracy. Notably, the proposed heterogeneous graph neural network model can also be generalized to different network topologies. This approach offers a promising solution for complex traffic flow analysis and predictio
    
[^69]: CycleNet：重新思考文本引导扩散中的循环一致性，以进行图像操作

    CycleNet: Rethinking Cycle Consistency in Text-Guided Diffusion for Image Manipulation. (arXiv:2310.13165v1 [cs.CV])

    [http://arxiv.org/abs/2310.13165](http://arxiv.org/abs/2310.13165)

    CycleNet是一种将循环一致性引入扩散模型的新方法，用于规范图像操作，具有优越的翻译一致性和质量，并且可以生成高质量的跨领域分布图像。

    

    扩散模型（DM）在图像合成任务中取得了突破，但缺乏一种直观的一致图像到图像（I2I）翻译接口。为解决这个问题，已经探索了各种方法，包括基于掩码的方法，基于注意力的方法和基于图像的方法。然而，如何使用预训练的DMs进行无配对的I2I翻译并保持一致性仍然是一个关键挑战。本文介绍了Cyclenet，一种新颖但简单的方法，它将循环一致性纳入DMs中，以规范图像操作。我们验证了Cyclenet在不同粒度的无配对I2I任务上的优势。除了场景和对象级别的翻译，我们还贡献了一个多领域I2I翻译数据集，用于研究物体的物理状态变化。我们的实证研究表明，Cyclenet在翻译的一致性和质量方面具有优势，并且在改变文本描述时可以生成高质量的跨领域分布图像。

    Diffusion models (DMs) have enabled breakthroughs in image synthesis tasks but lack an intuitive interface for consistent image-to-image (I2I) translation. Various methods have been explored to address this issue, including mask-based methods, attention-based methods, and image-conditioning. However, it remains a critical challenge to enable unpaired I2I translation with pre-trained DMs while maintaining satisfying consistency. This paper introduces Cyclenet, a novel but simple method that incorporates cycle consistency into DMs to regularize image manipulation. We validate Cyclenet on unpaired I2I tasks of different granularities. Besides the scene and object level translation, we additionally contribute a multi-domain I2I translation dataset to study the physical state changes of objects. Our empirical studies show that Cyclenet is superior in translation consistency and quality, and can generate high-quality images for out-of-domain distributions with a simple change of the textual 
    
[^70]: 几乎等变性通过李代数卷积

    Almost Equivariance via Lie Algebra Convolutions. (arXiv:2310.13164v1 [cs.LG])

    [http://arxiv.org/abs/2310.13164](http://arxiv.org/abs/2310.13164)

    本文研究了几乎等变性的主题，并提供了一个不同于现有定义的几乎等变性定义，并通过利用李群的李代数给出了在模型中编码几乎等变性的实用方法。

    

    最近，在机器学习中，模型相对于群作用的等变性已成为一个重要的研究课题。然而，赋予一个架构具体的群等变性对模型所期望看到的数据变换类型施加了强大的先验。严格等变模型强制执行对称性，但真实世界的数据并不总是符合这样的严格等变性，可能是因为数据中的噪声或仅编码了近似或部分对称性的潜在物理定律。在这种情况下，严格等变性的先验实际上可能过于强大，导致模型在真实数据上表现不佳。因此，在这项工作中，我们研究了一个相关的主题，即几乎等变性。我们提供了一个与当前文献中现有定义不同的几乎等变性定义，并通过利用李群的李代数给出了在模型中编码几乎等变性的实用方法。

    Recently, the equivariance of models with respect to a group action has become an important topic of research in machine learning. However, imbuing an architecture with a specific group equivariance imposes a strong prior on the types of data transformations that the model expects to see. While strictly-equivariant models enforce symmetries, real-world data does not always conform to such strict equivariances, be it due to noise in the data or underlying physical laws that encode only approximate or partial symmetries. In such cases, the prior of strict equivariance can actually prove too strong and cause models to underperform on real-world data. Therefore, in this work we study a closely related topic, that of almost equivariance. We provide a definition of almost equivariance that differs from those extant in the current literature and give a practical method for encoding almost equivariance in models by appealing to the Lie algebra of a Lie group. Specifically, we define Lie algebr
    
[^71]: 一种分布式气象预测方法：通过联邦学习和生成对抗网络解决降水预测模型中的数据不平衡问题

    A Distributed Approach to Meteorological Predictions: Addressing Data Imbalance in Precipitation Prediction Models through Federated Learning and GANs. (arXiv:2310.13161v1 [cs.LG])

    [http://arxiv.org/abs/2310.13161](http://arxiv.org/abs/2310.13161)

    这项研究提出了一种分布式方法来解决气象预测模型中的数据不平衡问题，通过联邦学习和生成对抗网络技术改善了模型的性能。

    

    天气数据的分类需要将气象现象分为不同的类别，从而为农业、航空和灾害管理等各个领域提供细致的分析和准确的预测。这涉及使用机器学习模型来分析大型多维天气数据集中的模式和趋势。这些数据集可能包括温度、湿度、风速和气压等变量，这些变量对气象条件起着作用。此外，分类算法必须能够有效应对数据不平衡等挑战，其中某些天气事件（如风暴或极端温度）可能被低估。这项实证研究探索了在集中和联邦环境中解决表格状天气数据中不平衡类别的数据增强方法。采用合成少数类过采样技术或生成对抗网络等数据增强技术可以改善模型的性能。

    The classification of weather data involves categorizing meteorological phenomena into classes, thereby facilitating nuanced analyses and precise predictions for various sectors such as agriculture, aviation, and disaster management. This involves utilizing machine learning models to analyze large, multidimensional weather datasets for patterns and trends. These datasets may include variables such as temperature, humidity, wind speed, and pressure, contributing to meteorological conditions. Furthermore, it's imperative that classification algorithms proficiently navigate challenges such as data imbalances, where certain weather events (e.g., storms or extreme temperatures) might be underrepresented. This empirical study explores data augmentation methods to address imbalanced classes in tabular weather data in centralized and federated settings. Employing data augmentation techniques such as the Synthetic Minority Over-sampling Technique or Generative Adversarial Networks can improve t
    
[^72]: 条件生成建模用于图像、3D动画和视频

    Conditional Generative Modeling for Images, 3D Animations, and Video. (arXiv:2310.13157v1 [cs.CV])

    [http://arxiv.org/abs/2310.13157](http://arxiv.org/abs/2310.13157)

    本论文探索了条件生成模型的新颖形式和创新应用，提出了使用神经ODE对视频动态建模，以及连续标准流的条件变体实现高分辨率图像生成。

    

    本论文旨在推动计算机视觉中生成建模领域的创新，通过探索条件生成模型的新颖形式和图像、3D动画和视频中的创新应用。我们的研究关注于提供噪声和视觉数据的可逆变换的架构，并应用编码器-解码器架构进行生成任务和3D内容操作。在所有情况下，我们都将条件信息纳入到增强视觉数据合成中，提高了生成过程的效率和生成内容的质量。我们引入了使用神经ODE对视频动态建模的方法，利用编码器-解码器架构，尽管仅训练用于重构当前帧，但能够预测未来的视频帧。接下来，我们提出了连续标准流的条件变体，通过较低分辨率的输入实现更高分辨率的图像生成。

    This dissertation attempts to drive innovation in the field of generative modeling for computer vision, by exploring novel formulations of conditional generative models, and innovative applications in images, 3D animations, and video. Our research focuses on architectures that offer reversible transformations of noise and visual data, and the application of encoder-decoder architectures for generative tasks and 3D content manipulation. In all instances, we incorporate conditional information to enhance the synthesis of visual data, improving the efficiency of the generation process as well as the generated content.  We introduce the use of Neural ODEs to model video dynamics using an encoder-decoder architecture, demonstrating their ability to predict future video frames despite being trained solely to reconstruct current frames. Next, we propose a conditional variant of continuous normalizing flows that enables higher-resolution image generation based on lower-resolution input, achiev
    
[^73]: 在临床领域的问答模型中分析自然分布偏移的CLIFT

    CLIFT: Analysing Natural Distribution Shift on Question Answering Models in Clinical Domain. (arXiv:2310.13146v1 [cs.CL])

    [http://arxiv.org/abs/2310.13146](http://arxiv.org/abs/2310.13146)

    本文介绍了一个新的临床领域问答模型测试平台CLIFT，其中包括7.5k个高质量的问答样本。在原始测试集上表现出色的深度学习模型在应用于新的测试集时性能下降，表明存在分布偏移。我们的研究结果强调了提高临床领域模型鲁棒性的必要性和潜力，并提供了一个追踪该方向进展的测试平台。

    

    本文介绍了一个新的测试平台CLIFT（临床分布偏移），用于临床领域的问答任务。该测试平台包括7.5k个高质量的问答样本，提供了一个多样化且可靠的基准。我们进行了全面的实验研究，并在该测试平台下评估了几个QA深度学习模型。尽管在原始测试集上表现出色，但在应用于新的测试集时，性能下降，表明存在分布偏移。我们的研究结果强调了在分布偏移下提高临床领域模型鲁棒性的必要性和潜力。该测试平台为追踪该方向的进展提供了一种途径。它还强调了采用考虑自然分布偏移鲁棒性的评估指标的必要性。我们计划通过添加更多样本和模型结果来扩展语料库。完整的论文和更新的基准可以在github.com/openlifescience-ai/clift找到。

    This paper introduces a new testbed CLIFT (Clinical Shift) for the clinical domain Question-answering task. The testbed includes 7.5k high-quality question answering samples to provide a diverse and reliable benchmark. We performed a comprehensive experimental study and evaluated several QA deep-learning models under the proposed testbed. Despite impressive results on the original test set, the performance degrades when applied to new test sets, which shows the distribution shift. Our findings emphasize the need for and the potential for increasing the robustness of clinical domain models under distributional shifts. The testbed offers one way to track progress in that direction. It also highlights the necessity of adopting evaluation metrics that consider robustness to natural distribution shifts. We plan to expand the corpus by adding more samples and model results. The full paper and the updated benchmark are available at github.com/openlifescience-ai/clift
    
[^74]: 具有多项式激活函数的图神经网络具有有限的表达能力

    Graph Neural Networks with polynomial activations have limited expressivity. (arXiv:2310.13139v1 [cs.LG])

    [http://arxiv.org/abs/2310.13139](http://arxiv.org/abs/2310.13139)

    本文证明了具有多项式激活函数的图神经网络无法表达GC2查询，与常用的非多项式激活函数存在分离，这回答了一个开放问题。

    

    图神经网络（GNNs）的表达能力可以完全由适当的一阶逻辑片段来描述。换句话说，任何在标记图上解释的关于二元逻辑片段（GC2）的查询都可以使用一个大小仅取决于查询深度的GNN来表示。正如[Barcelo＆Al。，2020，Grohe，2021]指出的那样，这个描述适用于一组激活函数的家族，这表明GNN可以通过不同的激活函数选择来表达不同的逻辑层次结构。在本文中，我们证明了这样的层次结构的存在，证明了具有多项式激活函数的GNN无法表示GC2查询。这意味着多项式和常用的非多项式激活函数（如ReLU、sigmoid、双曲正切等）之间存在一个分离，并回答了[Grohe，2021]提出的一个悬而未决的问题。

    The expressivity of Graph Neural Networks (GNNs) can be entirely characterized by appropriate fragments of the first order logic. Namely, any query of the two variable fragment of graded modal logic (GC2) interpreted over labelled graphs can be expressed using a GNN whose size depends only on the depth of the query. As pointed out by [Barcelo & Al., 2020, Grohe, 2021 ], this description holds for a family of activation functions, leaving the possibibility for a hierarchy of logics expressible by GNNs depending on the chosen activation function. In this article, we show that such hierarchy indeed exists by proving that GC2 queries cannot be expressed by GNNs with polynomial activation functions. This implies a separation between polynomial and popular non polynomial activations (such as ReLUs, sigmoid and hyperbolic tan and others) and answers an open question formulated by [Grohe, 2021].
    
[^75]: 异构隐私需求下的均值估计

    Mean Estimation Under Heterogeneous Privacy Demands. (arXiv:2310.13137v1 [cs.CR])

    [http://arxiv.org/abs/2310.13137](http://arxiv.org/abs/2310.13137)

    本文研究了异构隐私需求下的均值估计问题，提出的算法在接近线性的时间复杂度下，能够满足每个用户的个别隐私要求，并且获得极小化最优结果。最严格的用户的隐私要求决定了整体的误差率，且拥有较少但不同隐私要求的用户都会获得超过自身需求的隐私保护，且保护程度相同。这意味着对隐私不敏感的用户可以免费获得非平凡程度的隐私保护。

    

    差分隐私是一种量化算法造成的隐私损失的成熟框架。传统的方法对所有用户都施加统一的隐私要求，这与现实情景不一致，因为用户可以个别决定自己的隐私偏好。本文考虑了均值估计的问题，每个用户都可以施加自己不同的隐私水平。我们提出的算法被证明是极小化最优的，且具有接近线性的运行时间。我们的结果揭示了一种有趣的饱和现象。即最严格用户的隐私要求决定了整体的误差率。因此，拥有较少但不同隐私要求的用户都会获得超过自身需求的隐私保护，且保护程度相同。换句话说，这些对隐私不敏感的用户可以免费获得非平凡程度的隐私保护，而无需在估计器性能上做出任何牺牲。

    Differential Privacy (DP) is a well-established framework to quantify privacy loss incurred by any algorithm. Traditional formulations impose a uniform privacy requirement for all users, which is often inconsistent with real-world scenarios in which users dictate their privacy preferences individually. This work considers the problem of mean estimation, where each user can impose their own distinct privacy level. The algorithm we propose is shown to be minimax optimal and has a near-linear run-time. Our results elicit an interesting saturation phenomenon that occurs. Namely, the privacy requirements of the most stringent users dictate the overall error rates. As a consequence, users with less but differing privacy requirements are all given more privacy than they require, in equal amounts. In other words, these privacy-indifferent users are given a nontrivial degree of privacy for free, without any sacrifice in the performance of the estimator.
    
[^76]: AI预测材料性能不确定性量化方法的比较

    Approaches for Uncertainty Quantification of AI-predicted Material Properties: A Comparison. (arXiv:2310.13136v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2310.13136](http://arxiv.org/abs/2310.13136)

    本论文比较了三种易于实施的方法来确定AI预测材料性能的个体不确定性。

    

    大量材料性能数据库的建立和强大计算机的可用性使得机器学习（ML）建模成为预测材料性能的常用工具。虽然ML模型常常报告置信区间，但预测区间，即每个预测的不确定性却不常见。本文研究了三种易于实施的方法来确定这种个体不确定性，将其应用于包括能量学、力学、电子学、光学和光谱性质在内的十种ML量上，进行了比较。具体而言，我们关注了量化方法，直接机器学习预测区间和集成方法。

    The development of large databases of material properties, together with the availability of powerful computers, has allowed machine learning (ML) modeling to become a widely used tool for predicting material performances. While confidence intervals are commonly reported for such ML models, prediction intervals, i.e., the uncertainty on each prediction, are not as frequently available. Here, we investigate three easy-to-implement approaches to determine such individual uncertainty, comparing them across ten ML quantities spanning energetics, mechanical, electronic, optical, and spectral properties. Specifically, we focused on the Quantile approach, the direct machine learning of the prediction intervals and Ensemble methods.
    
[^77]: 基于深度强化学习的优化CO2排放的智能交通信号控制

    Deep Reinforcement Learning-based Intelligent Traffic Signal Controls with Optimized CO2 emissions. (arXiv:2310.13129v1 [eess.SY])

    [http://arxiv.org/abs/2310.13129](http://arxiv.org/abs/2310.13129)

    本研究提出了一种基于深度强化学习的智能交通信号控制算法，通过优化CO2排放和行驶时间等指标，实现了较好的性能。

    

    如今，交通网络面临着次优控制策略的挑战，这可能对人类健康、环境产生不利影响，并 contribute to traffic congestion. 由于交通拥堵导致的空气污染水平上升和通勤时间延长，交叉路口交通信号控制器成为现代交通基础设施的关键组成部分。尽管文献中有几个自适应交通信号控制器，但对其比较性能的研究有限。此外，尽管二氧化碳（CO2）排放在全球范围内非常重要，但文献对该领域关注不够。在本报告中，我们提出了EcoLight，一种针对强化学习算法的奖励塑造方案，不仅能减少CO2排放，还能在诸如行驶时间等指标上取得有竞争力的结果。我们使用行驶时间、CO2排放、等指标比较了表格式Q-Learning、DQN、SARSA和A2C算法的性能。

    Nowadays, transportation networks face the challenge of sub-optimal control policies that can have adverse effects on human health, the environment, and contribute to traffic congestion. Increased levels of air pollution and extended commute times caused by traffic bottlenecks make intersection traffic signal controllers a crucial component of modern transportation infrastructure. Despite several adaptive traffic signal controllers in literature, limited research has been conducted on their comparative performance. Furthermore, despite carbon dioxide (CO2) emissions' significance as a global issue, the literature has paid limited attention to this area. In this report, we propose EcoLight, a reward shaping scheme for reinforcement learning algorithms that not only reduces CO2 emissions but also achieves competitive results in metrics such as travel time. We compare the performance of tabular Q-Learning, DQN, SARSA, and A2C algorithms using metrics such as travel time, CO2 emissions, wa
    
[^78]: 使用机器学习和业务数据预测客运渡轮燃料消费：一项比较研究

    Fuel Consumption Prediction for a Passenger Ferry using Machine Learning and In-service Data: A Comparative Study. (arXiv:2310.13123v1 [cs.LG])

    [http://arxiv.org/abs/2310.13123](http://arxiv.org/abs/2310.13123)

    本文研究了使用机器学习和业务数据预测客运渡轮燃料消费的方法，并通过选择适当的输入变量来提高预测性能和实际可应用性。

    

    随着环保交通的重要性增加，为海运船只提供高效的操作方法至关重要。考虑到天气条件和使用船只业务数据进行预测的状态监控方法需要准确完整的模型来预测船只的energy efficiency。这些模型需要能够实时有效地处理所有操作数据。本文提出了使用从客运船只收集的业务数据来预测燃料消耗的模型。统计和领域知识方法被用来选择模型的适当输入变量。这些方法能够防止过拟合、缺失数据和多重共线性，并提供实际可应用性。研究中调查的预测模型包括多元线性回归（MLR）、决策树方法（DT）、人工神经网络（ANN）和集成方法。最好的预测性能来自于使用ensemble方法开发的模型

    As the importance of eco-friendly transportation increases, providing an efficient approach for marine vessel operation is essential. Methods for status monitoring with consideration to the weather condition and forecasting with the use of in-service data from ships requires accurate and complete models for predicting the energy efficiency of a ship. The models need to effectively process all the operational data in real-time. This paper presents models that can predict fuel consumption using in-service data collected from a passenger ship. Statistical and domain-knowledge methods were used to select the proper input variables for the models. These methods prevent over-fitting, missing data, and multicollinearity while providing practical applicability. Prediction models that were investigated include multiple linear regression (MLR), decision tree approach (DT), an artificial neural network (ANN), and ensemble methods. The best predictive performance was from a model developed using t
    
[^79]: 理解Transformer中的加法

    Understanding Addition in Transformers. (arXiv:2310.13121v1 [cs.LG])

    [http://arxiv.org/abs/2310.13121](http://arxiv.org/abs/2310.13121)

    本文通过对经过训练进行整数加法的单层Transformer模型的深入分析，揭示了该模型将任务分为并行的、特定于数字的流，并针对不同的数字位置采用不同的算法，同时还发现了一种罕见的高损失的使用情况。这些发现对于机制可解释性、人工智能安全性和对齐性等方面的研究具有重要贡献。

    

    了解像Transformer这样的机器学习模型的内部工作方式对于其安全和道德使用至关重要。本文对经过训练进行整数加法的单层Transformer模型进行了深入分析。我们揭示了该模型将任务分为并行的、特定于数字的流，并针对不同的数字位置采用不同的算法。我们的研究还发现该模型开始计算较晚，但执行速度非常快。我们还发现了一种罕见的高损失的使用情况，并予以解释。总体而言，我们详细解释了该模型的算法。这些发现通过严格测试和数学建模得到了验证，对于机制可解释性、人工智能安全性和对齐性等广泛研究做出了贡献。我们的方法为分析更复杂的任务和多层Transformer模型打开了大门。

    Understanding the inner workings of machine learning models like Transformers is vital for their safe and ethical use. This paper presents an in-depth analysis of a one-layer Transformer model trained for integer addition. We reveal that the model divides the task into parallel, digit-specific streams and employs distinct algorithms for different digit positions. Our study also finds that the model starts calculations late but executes them rapidly. A rare use case with high loss is identified and explained. Overall, the model's algorithm is explained in detail. These findings are validated through rigorous testing and mathematical modeling, contributing to the broader works in Mechanistic Interpretability, AI safety, and alignment. Our approach opens the door for analyzing more complex tasks and multi-layer Transformer models.
    
[^80]: RSAdapter: 适应遥感视觉问答的多模态模型

    RSAdapter: Adapting Multimodal Models for Remote Sensing Visual Question Answering. (arXiv:2310.13120v1 [cs.CV])

    [http://arxiv.org/abs/2310.13120](http://arxiv.org/abs/2310.13120)

    RSAdapter是一种针对遥感视觉问答的多模态模型，通过并行适配器和线性转换层的设计，提高了运行时和参数效率。

    

    近年来，随着Transformer模型的快速发展，基于Transformer的多模态架构在各种下游任务中都得到了广泛应用，包括但不限于图像描述、视觉问答（VQA）和图像文本生成。然而，当前的遥感VQA方法通常涉及资源密集型技术，如对大型模型进行全面微调或从预训练的多模态模型中提取图像-文本特征，然后使用解码器进行模态融合。这些方法需要大量的计算资源和时间，并引入了相当数量的可训练参数。为了解决这些挑战，我们提出了一种名为RSAdapter的新方法，该方法优先考虑运行时和参数效率。RSAdapter包括两个关键组件：并行适配器和插入在适配器的每个全连接（FC）层后的额外线性转换层。

    In recent years, with the rapid advancement of transformer models, transformer-based multimodal architectures have found wide application in various downstream tasks, including but not limited to Image Captioning, Visual Question Answering (VQA), and Image-Text Generation. However, contemporary approaches to Remote Sensing (RS) VQA often involve resource-intensive techniques, such as full fine-tuning of large models or the extraction of image-text features from pre-trained multimodal models, followed by modality fusion using decoders. These approaches demand significant computational resources and time, and a considerable number of trainable parameters are introduced. To address these challenges, we introduce a novel method known as RSAdapter, which prioritizes runtime and parameter efficiency. RSAdapter comprises two key components: the Parallel Adapter and an additional linear transformation layer inserted after each fully connected (FC) layer within the Adapter. This approach not on
    
[^81]: 使用神经常微分方程的半监督学习动力系统：一种教师-学生模型的方法

    Semi-Supervised Learning of Dynamical Systems with Neural Ordinary Differential Equations: A Teacher-Student Model Approach. (arXiv:2310.13110v1 [cs.LG])

    [http://arxiv.org/abs/2310.13110](http://arxiv.org/abs/2310.13110)

    本文提出了TS-NODE，该方法是使用神经常微分方程进行动力系统建模的首个半监督方法。TS-NODE通过生成伪样本来拓宽状态空间的探索，并利用教师-学生模型解决由于缺乏真实系统数据而带来的挑战。

    

    建模动力系统对于各种任务至关重要，但由于复杂的非线性动力学、有限的观察或缺乏先验知识，建模仍然具有挑战性。最近，利用神经网络的表达能力来建模未知动力学的数据驱动方法，例如神经常微分方程(NODE)，已经显示出有希望的结果。然而，这些方法常常受到有限标记训练数据的限制，导致泛化能力差和次优预测。另一方面，半监督算法可以利用丰富的无标签数据，在分类和回归任务上表现良好。我们提出了TS-NODE，这是一种使用NODE进行动力系统建模的首个半监督方法。TS-NODE利用廉价生成的伪样本探索状态空间，以解决由于缺乏真实系统数据而带来的挑战，并采用教师-学生模型的方法。

    Modeling dynamical systems is crucial for a wide range of tasks, but it remains challenging due to complex nonlinear dynamics, limited observations, or lack of prior knowledge. Recently, data-driven approaches such as Neural Ordinary Differential Equations (NODE) have shown promising results by leveraging the expressive power of neural networks to model unknown dynamics. However, these approaches often suffer from limited labeled training data, leading to poor generalization and suboptimal predictions. On the other hand, semi-supervised algorithms can utilize abundant unlabeled data and have demonstrated good performance in classification and regression tasks. We propose TS-NODE, the first semi-supervised approach to modeling dynamical systems with NODE. TS-NODE explores cheaply generated synthetic pseudo rollouts to broaden exploration in the state space and to tackle the challenges brought by lack of ground-truth system data under a teacher-student model. TS-NODE employs an unified o
    
[^82]: 用自定义迁移学习简化MRI图像中的脑肿瘤分类

    Streamlining Brain Tumor Classification with Custom Transfer Learning in MRI Images. (arXiv:2310.13108v1 [eess.IV])

    [http://arxiv.org/abs/2310.13108](http://arxiv.org/abs/2310.13108)

    这项研究提出了一种用自定义迁移学习网络对MRI图像中的脑肿瘤进行分类的高效解决方案，通过使用VGG-19架构和额外的隐藏层，降低了计算复杂性但提高了准确率。

    

    脑肿瘤的发病率不断增加，其特征是异常组织在大脑中的不受控制扩散，全球每年几乎有70万新病例被诊断出来。磁共振成像（MRI）常常用于脑肿瘤的诊断，准确的分类是关键的临床程序。在这项研究中，我们提出了一种使用自定义迁移学习网络对MRI图像中的脑肿瘤进行分类的高效解决方案。虽然一些研究人员采用了各种预训练的架构，如RESNET-50，ALEXNET，VGG-16和VGG-19，但这些方法通常存在高计算复杂性的问题。为了解决这个问题，我们提出了一个自定义的轻量级模型，使用了基于卷积神经网络的预训练架构以减少复杂性。具体而言，我们采用了VGG-19架构，并增加了额外的隐藏层，这减少了基础架构的复杂性但提高了计算效率。我们的目标是实现高准确率的脑肿瘤分类同时降低计算复杂性。

    Brain tumors are increasingly prevalent, characterized by the uncontrolled spread of aberrant tissues in the brain, with almost 700,000 new cases diagnosed globally each year. Magnetic Resonance Imaging (MRI) is commonly used for the diagnosis of brain tumors and accurate classification is a critical clinical procedure. In this study, we propose an efficient solution for classifying brain tumors from MRI images using custom transfer learning networks. While several researchers have employed various pre-trained architectures such as RESNET-50, ALEXNET, VGG-16, and VGG-19, these methods often suffer from high computational complexity. To address this issue, we present a custom and lightweight model using a Convolutional Neural Network-based pre-trained architecture with reduced complexity. Specifically, we employ the VGG-19 architecture with additional hidden layers, which reduces the complexity of the base architecture but improves computational efficiency. The objective is to achieve h
    
[^83]: AVTENet: 基于音频-视觉Transformer的多专家集成网络在视频深度伪造检测中的应用

    AVTENet: Audio-Visual Transformer-based Ensemble Network Exploiting Multiple Experts for Video Deepfake Detection. (arXiv:2310.13103v1 [cs.CV])

    [http://arxiv.org/abs/2310.13103](http://arxiv.org/abs/2310.13103)

    本文提出了AVTENet框架，该框架是一个基于音频-视觉Transformer的多专家集成网络，用于在视频深度伪造检测中考虑声学和视觉操作。

    

    在社交媒体平台上广泛分享的伪造内容是一个重大社会问题，要求加强监管并给研究社区带来新的挑战。近年来，超真实的深度伪造视频的普及引起了对音频和视觉伪造威胁的关注。大多数关于检测AI生成的伪造视频的先前工作只利用了视觉模态或音频模态。虽然文献中有一些方法利用音频和视觉模态来检测伪造视频，但它们尚未在涉及声学和视觉操作的多模态深度伪造视频数据集上进行全面评估。此外，这些现有方法大多基于CNN，并且检测准确率较低。受到Transformer在各个领域的最新成功启发，为了解决深度伪造技术带来的挑战，本文提出了一种考虑声学操作的音频-视觉Transformer集成网络（AVTENet）框架。

    Forged content shared widely on social media platforms is a major social problem that requires increased regulation and poses new challenges to the research community. The recent proliferation of hyper-realistic deepfake videos has drawn attention to the threat of audio and visual forgeries. Most previous work on detecting AI-generated fake videos only utilizes visual modality or audio modality. While there are some methods in the literature that exploit audio and visual modalities to detect forged videos, they have not been comprehensively evaluated on multi-modal datasets of deepfake videos involving acoustic and visual manipulations. Moreover, these existing methods are mostly based on CNN and suffer from low detection accuracy. Inspired by the recent success of Transformer in various fields, to address the challenges posed by deepfake technology, in this paper, we propose an Audio-Visual Transformer-based Ensemble Network (AVTENet) framework that considers both acoustic manipulatio
    
[^84]: 粒子引导：非独立同分布样本多样性采样与扩散模型

    Particle Guidance: non-I.I.D. Diverse Sampling with Diffusion Models. (arXiv:2310.13102v1 [cs.LG])

    [http://arxiv.org/abs/2310.13102](http://arxiv.org/abs/2310.13102)

    本文提出了一种粒子引导的方法，通过超越独立样本的常见假设，提高了生成模型的多样性和采样效率。在实验中，我们在条件图像生成和分子构象生成上进行了测试，并取得了显著的结果。

    

    鉴于生成模型的广泛成功，已经有大量的研究致力于加快其采样时间。然而，为了获取多样性样本，生成模型通常需要进行多次采样，这会造成与采样时间无关的成本。本文处理了如何通过超越独立样本的常见假设来提高多样性和采样效率的问题。我们提出了粒子引导，一种基于扩散的生成采样的扩展，其中的联合粒子时变位势强制实现多样性。我们从理论上分析了粒子引导产生的联合分布，以及它对位势选择的影响和与其他学科方法的联系。在实证方面，我们在条件图像生成的设置中进行了测试，我们能够增加多样性而不影响质量，并在分子构象生成中降低了平均13%的先进技术中值误差。

    In light of the widespread success of generative models, a significant amount of research has gone into speeding up their sampling time. However, generative models are often sampled multiple times to obtain a diverse set incurring a cost that is orthogonal to sampling time. We tackle the question of how to improve diversity and sample efficiency by moving beyond the common assumption of independent samples. We propose particle guidance, an extension of diffusion-based generative sampling where a joint-particle time-evolving potential enforces diversity. We analyze theoretically the joint distribution that particle guidance generates, its implications on the choice of potential, and the connections with methods in other disciplines. Empirically, we test the framework both in the setting of conditional image generation, where we are able to increase diversity without affecting quality, and molecular conformer generation, where we reduce the state-of-the-art median error by 13% on average
    
[^85]: 不冒犯，伯特 - 我只侮辱人类！多个收件人句级攻击有毒性检测神经网络。

    No offence, Bert -- I insult only humans! Multiple addressees sentence-level attack on toxicity detection neural network. (arXiv:2310.13099v1 [cs.CL])

    [http://arxiv.org/abs/2310.13099](http://arxiv.org/abs/2310.13099)

    本论文介绍了一种简单而高效的句级攻击方法，可以通过添加积极的词语或句子来改变黑盒有毒性检测模型的预测结果，该方法在多种语言上均有效。

    

    我们介绍了一种简单而高效的句级攻击方法，针对黑盒有毒性检测模型。通过在仇恨信息末尾添加一些积极的词语或句子，我们能够改变神经网络的预测结果，并通过有毒性检测系统的检查。该方法在来自三个不同语言家族的七种语言上被证明有效。我们还描述了对抗上述攻击的防御机制，并讨论了其局限性。

    We introduce a simple yet efficient sentence-level attack on black-box toxicity detector models. By adding several positive words or sentences to the end of a hateful message, we are able to change the prediction of a neural network and pass the toxicity detection system check. This approach is shown to be working on seven languages from three different language families. We also describe the defence mechanism against the aforementioned attack and discuss its limitations.
    
[^86]: SRAI: 探索地理空间人工智能的标准化

    SRAI: Towards Standardization of Geospatial AI. (arXiv:2310.13098v1 [cs.LG])

    [http://arxiv.org/abs/2310.13098](http://arxiv.org/abs/2310.13098)

    SRAI是一个Python库，用于标准化地理空间人工智能（Geospatial AI）领域工具集，包括下载地理空间数据、划分区域和训练嵌入模型等功能。

    

    空间表示的人工智能（SRAI）是一个用于处理地理空间数据的Python库。该库可以下载地理空间数据，使用多种算法将给定区域划分为微区域，并使用不同的架构训练嵌入模型。它包括基准模型以及来自已发表作品的更复杂方法。这些功能使得在解决地理空间任务的完整流水线中使用SRAI成为可能。该提议的库是标准化地理空间人工智能领域工具集的第一步。它是完全开源的，并在Apache 2.0许可下发布。

    Spatial Representations for Artificial Intelligence (\textit{srai}) is a Python library for working with geospatial data. The library can download geospatial data, split a given area into micro-regions using multiple algorithms and train an embedding model using various architectures. It includes baseline models as well as more complex methods from published works. Those capabilities make it possible to use \textit{srai} in a complete pipeline for geospatial task solving. The proposed library is the first step to standardize the geospatial AI domain toolset. It is fully open-source and published under Apache 2.0 licence.
    
[^87]: 一种用于排球跳跃分类的多阶段时态卷积网络，使用腰部融合惯性测量单元的方法。

    A Multi-Stage Temporal Convolutional Network for Volleyball Jumps Classification Using a Waist-Mounted IMU. (arXiv:2310.13097v1 [cs.LG])

    [http://arxiv.org/abs/2310.13097](http://arxiv.org/abs/2310.13097)

    本研究提出了一种使用腰部IMU识别排球跳跃类型的方法，采用了多层时态卷积网络（MS-TCN）进行分类，相较于最先进的深度学习模型具有更好的性能和较低的计算成本。

    

    监测排球运动员在训练或比赛中的跳跃次数对防止伤害非常重要，然而使用传统方法如视频分析来进行测量需要很大的工作量和成本。此外，现有方法无法准确区分不同类型的跳跃。本研究提出了一种使用单个腰部惯性测量单元（IMU）识别排球跳跃类型的无障碍系统。多层时态卷积网络（MS-TCN）被应用于逐样本分类。该模型在十名和二十六名排球运动员中进行了评估，在固定协议的跳跃和着地任务的实验室会话期间以及四个排球训练期间进行了评估。相比于最先进的深度学习模型，MS-TCN模型在性能上表现更好，但计算成本更低。在实验室会话期间，大多数跳跃次数之间的差异很小。

    Monitoring the number of jumps for volleyball players during training or a match can be crucial to prevent injuries, yet the measurement requires considerable workload and cost using traditional methods such as video analysis. Also, existing methods do not provide accurate differentiation between different types of jumps. In this study, an unobtrusive system with a single inertial measurement unit (IMU) on the waist was proposed to recognize the types of volleyball jumps. A Multi-Layer Temporal Convolutional Network (MS-TCN) was applied for sample-wise classification. The model was evaluated on ten volleyball players and twenty-six volleyball players, during a lab session with a fixed protocol of jumping and landing tasks, and during four volleyball training sessions, respectively. The MS-TCN model achieved better performance than a state-of-the-art deep learning model but with lower computational cost. In the lab sessions, most jump counts showed small differences between the predicte
    
[^88]: Transformer的基于范数的长度无关泛化界限

    Sequence Length Independent Norm-Based Generalization Bounds for Transformers. (arXiv:2310.13088v1 [stat.ML])

    [http://arxiv.org/abs/2310.13088](http://arxiv.org/abs/2310.13088)

    本文提出了一种基于范数的泛化界限，适用于不依赖于输入序列长度的Transformer架构。通过使用覆盖数界限来上界Transformer的Rademacher复杂度，该方法适用于掩码预测掩码字等常见的Transformer训练技术。我们也通过模拟研究验证了理论结果的有效性。

    

    本文提供了一种基于范数的泛化界限，适用于Transformer架构且不依赖于输入序列的长度。我们采用基于覆盖数的方法来证明我们的界限。我们使用了三个新颖的覆盖数界限来上界Transformer的Rademacher复杂度，该函数类为有界线性变换。此外，我们还展示了这个泛化界限适用于常见的Transformer训练技术，即掩码预测掩码字。我们还在一个稀疏多数数据集上进行了模拟研究，从实证角度验证了我们的理论发现。

    This paper provides norm-based generalization bounds for the Transformer architecture that do not depend on the input sequence length. We employ a covering number based approach to prove our bounds. We use three novel covering number bounds for the function class of bounded linear transformations to upper bound the Rademacher complexity of the Transformer. Furthermore, we show this generalization bound applies to the common Transformer training technique of masking and then predicting the masked word. We also run a simulated study on a sparse majority data set that empirically validates our theoretical findings.
    
[^89]: 无监督表示学习助力半监督元学习

    Unsupervised Representation Learning to Aid Semi-Supervised Meta Learning. (arXiv:2310.13085v1 [cs.LG])

    [http://arxiv.org/abs/2310.13085](http://arxiv.org/abs/2310.13085)

    本论文提出了一种一次性的无监督元学习方法，通过学习训练样本的潜在表示，解决了少样本学习中的数据稀缺问题。该方法在初始化和快速适应中应用了迁移学习，并提高了准确性。

    

    少样本学习或元学习利用了机器学习中的数据稀缺问题。传统上，监督学习需要大量的样本和标签。为了解决这个问题，我们提出了一种一次性的无监督元学习方法，用于学习训练样本的潜在表示。我们在无监督元学习的训练阶段使用增强样本作为查询集。在元学习的内循环中使用温度缩放的交叉熵损失来防止无监督学习过拟合。从这一步学到的参数以迁移学习的方式应用于目标监督元学习，用于初始化和快速适应，并提高准确性。所提出的方法是与模型无关的，可以帮助任何元学习模型提高准确性。我们在Omniglot和mini-Imagenet数据集上使用模型无关元学习（MAML）和关系网络（RN）来展示所提出方法的性能。

    Few-shot learning or meta-learning leverages the data scarcity problem in machine learning. Traditionally, training data requires a multitude of samples and labeling for supervised learning. To address this issue, we propose a one-shot unsupervised meta-learning to learn the latent representation of the training samples. We use augmented samples as the query set during the training phase of the unsupervised meta-learning. A temperature-scaled cross-entropy loss is used in the inner loop of meta-learning to prevent overfitting during unsupervised learning. The learned parameters from this step are applied to the targeted supervised meta-learning in a transfer-learning fashion for initialization and fast adaptation with improved accuracy. The proposed method is model agnostic and can aid any meta-learning model to improve accuracy. We use model agnostic meta-learning (MAML) and relation network (RN) on Omniglot and mini-Imagenet datasets to demonstrate the performance of the proposed met
    
[^90]: 日常用户如何通过演示有效地教授机器人？

    How Can Everyday Users Efficiently Teach Robots by Demonstrations?. (arXiv:2310.13083v1 [cs.RO])

    [http://arxiv.org/abs/2310.13083](http://arxiv.org/abs/2310.13083)

    通过使用不确定性度量作为准则，我们提出使用增强现实引导系统来指导人类教师提供更有效的机器人演示，从而促进机器人的高效学习。

    

    演示学习（LfD）是一个允许普通用户轻松编程机器人的框架。然而，机器人学习的效率以及机器人对任务变化的泛化能力取决于提供的演示的质量和数量。我们的目标是指导人类教师提供更有效的演示，从而促进机器人的高效学习。为了实现这一目标，我们提出使用不确定性的度量，即与任务相关的信息熵，作为向人类教师建议信息丰富的演示示例的准则，以提高他们的教学技能。在一项实验中（N=24），我们使用增强现实（AR）引导系统来训练新手用户从工作空间中具有最高熵的区域产生额外的演示。这些新手用户经过几次试验，教机器人使用有限数量的演示来完成一个可泛化的任务。随后，在训练后评估用户的表现

    Learning from Demonstration (LfD) is a framework that allows lay users to easily program robots. However, the efficiency of robot learning and the robot's ability to generalize to task variations hinges upon the quality and quantity of the provided demonstrations. Our objective is to guide human teachers to furnish more effective demonstrations, thus facilitating efficient robot learning. To achieve this, we propose to use a measure of uncertainty, namely task-related information entropy, as a criterion for suggesting informative demonstration examples to human teachers to improve their teaching skills. In a conducted experiment (N=24), an augmented reality (AR)-based guidance system was employed to train novice users to produce additional demonstrations from areas with the highest entropy within the workspace. These novice users were trained for a few trials to teach the robot a generalizable task using a limited number of demonstrations. Subsequently, the users' performance after tra
    
[^91]: 关于复值神经网络的计算复杂性研究

    On the Computational Complexities of Complex-valued Neural Networks. (arXiv:2310.13075v1 [cs.NE])

    [http://arxiv.org/abs/2310.13075](http://arxiv.org/abs/2310.13075)

    本论文研究了复值神经网络的计算复杂性，可以直接处理复值输入和输出信号，并使用定量计算复杂性来估计浮点操作的数量，有助于在低功耗系统中决定实施哪种算法。

    

    复值神经网络（CVNNs）是用于复域数据数字信号处理的非线性滤波器。与实值神经网络（RVNNs）相比，CVNNs可以直接处理复值输入和输出信号，这是由于它们的复域参数和激活函数。随着低功耗系统的趋势，计算复杂性分析已成为衡量算法功耗的重要工具。因此，本文介绍了CVNNs的定量和渐进计算复杂性，并且这对于决定实施哪种算法是至关重要的。数学运算以实值乘法的数量来描述，因为这些是最具挑战性的操作。通过定量计算复杂性可以准确估计浮点操作的数量，从而确定哪种CVNN可以在低功耗系统中实施。我们还研究了计算复杂性。

    Complex-valued neural networks (CVNNs) are nonlinear filters used in the digital signal processing of complex-domain data. Compared with real-valued neural networks~(RVNNs), CVNNs can directly handle complex-valued input and output signals due to their complex domain parameters and activation functions. With the trend toward low-power systems, computational complexity analysis has become essential for measuring an algorithm's power consumption. Therefore, this paper presents both the quantitative and asymptotic computational complexities of CVNNs. This is a crucial tool in deciding which algorithm to implement. The mathematical operations are described in terms of the number of real-valued multiplications, as these are the most demanding operations. To determine which CVNN can be implemented in a low-power system, quantitative computational complexities can be used to accurately estimate the number of floating-point operations. We have also investigated the computational complexities o
    
[^92]: 使用逻辑编程和核组合来提高卷积神经网络的可解释性

    Using Logic Programming and Kernel-Grouping for Improving Interpretability of Convolutional Neural Networks. (arXiv:2310.13073v1 [cs.LG])

    [http://arxiv.org/abs/2310.13073](http://arxiv.org/abs/2310.13073)

    本论文提出了一个使用逻辑编程和核组合的神经符号框架，能够提高卷积神经网络的可解释性。通过找到相似的核并生成规则集，该框架使得底层知识更加容易理解。

    

    在深度学习领域中，卷积神经网络（CNNs），尤其是在图像分类任务中的可解释性仍然是一个巨大的挑战。为此，我们提出了一个神经符号框架NeSyFOLD-G，它使用CNN的最后一层核来生成一个符号规则集，以使其底层知识具有可解释性。NeSyFOLD-G与其他类似框架的不同之处在于，我们首先使用各个核生成的特征图之间的余弦相似度来找到CNN中相似核的组合（核组合）。一旦找到了这样的核组，我们对CNN的每个核组的输出进行二值化，并使用它生成一个二值化表作为FOLD-SE-M的输入数据，FOLD-SE-M是一种基于规则的机器学习（RBML）算法。FOLD-SE-M然后生成一个可用于进行预测的规则集。我们提出了一种新颖的核组合算法，并证明组合相似核可以显著提升模型的可解释性。

    Within the realm of deep learning, the interpretability of Convolutional Neural Networks (CNNs), particularly in the context of image classification tasks, remains a formidable challenge. To this end we present a neurosymbolic framework, NeSyFOLD-G that generates a symbolic rule-set using the last layer kernels of the CNN to make its underlying knowledge interpretable. What makes NeSyFOLD-G different from other similar frameworks is that we first find groups of similar kernels in the CNN (kernel-grouping) using the cosine-similarity between the feature maps generated by various kernels. Once such kernel groups are found, we binarize each kernel group's output in the CNN and use it to generate a binarization table which serves as input data to FOLD-SE-M which is a Rule Based Machine Learning (RBML) algorithm. FOLD-SE-M then generates a rule-set that can be used to make predictions. We present a novel kernel grouping algorithm and show that grouping similar kernels leads to a significant
    
[^93]: 利用大型语言模型进行创造性机器人工具使用研究

    Creative Robot Tool Use with Large Language Models. (arXiv:2310.13065v1 [cs.RO])

    [http://arxiv.org/abs/2310.13065](http://arxiv.org/abs/2310.13065)

    本文利用大型语言模型开发了一个系统，名为RoboTool，可以使机器人在涉及隐含物理约束和长期规划的任务中创造性地使用工具。该系统通过解析自然语言指令、生成全面的策略、计算技能参数并生成可执行的Python代码来实现。实验结果表明，RoboTool能够有效地处理任务中的物理约束和环境因素。

    

    工具使用是高级智能的标志，不仅存在于动物行为中，也体现在机器人的能力中。本文研究了将机器人赋予创造性使用工具的能力，用于涉及隐含物理约束和长期规划的任务中。通过利用大型语言模型（LLM），我们开发了RoboTool，一个系统，它接受自然语言指令并输出控制机器人在模拟和真实环境中的可执行代码。RoboTool包含四个关键组件：（i）"解析器"，用于解释自然语言以识别与任务相关的关键概念，（ii）"规划器"，根据语言输入和关键概念生成全面的策略，（iii）"计算器"，用于计算每个技能的参数，（iv）"编码器"，将这些计划转换成可执行的Python代码。我们的结果表明，RoboTool不仅可以理解明确或隐含的物理约束和环境因素，而且还可以产生有效的执行计划。

    Tool use is a hallmark of advanced intelligence, exemplified in both animal behavior and robotic capabilities. This paper investigates the feasibility of imbuing robots with the ability to creatively use tools in tasks that involve implicit physical constraints and long-term planning. Leveraging Large Language Models (LLMs), we develop RoboTool, a system that accepts natural language instructions and outputs executable code for controlling robots in both simulated and real-world environments. RoboTool incorporates four pivotal components: (i) an "Analyzer" that interprets natural language to discern key task-related concepts, (ii) a "Planner" that generates comprehensive strategies based on the language input and key concepts, (iii) a "Calculator" that computes parameters for each skill, and (iv) a "Coder" that translates these plans into executable Python code. Our results show that RoboTool can not only comprehend explicit or implicit physical constraints and environmental factors bu
    
[^94]: 到底是理解还是记忆：解析算法数据集上的泛化和记忆

    To grok or not to grok: Disentangling generalization and memorization on corrupted algorithmic datasets. (arXiv:2310.13061v1 [cs.LG])

    [http://arxiv.org/abs/2310.13061](http://arxiv.org/abs/2310.13061)

    本研究探讨了在深度学习中的泛化和记忆的问题，通过对模数算术任务上训练的神经网络进行实验，发现网络可以同时记住损坏的标签并实现100%的泛化，并且可以通过识别和剪枝记忆化的神经元来降低对损坏数据的准确率，提高对未损坏数据的准确率。

    

    在深度学习中，强大的泛化能力是一个重要的挑战，特别是在可训练参数非常大的情况下。通常情况下，很难知道网络是否已经记住了一组特定的样本，还是理解了其中的基本规律（或者两者都有）。受到这个挑战的启发，我们研究了一个可解释的模型，其中的泛化表示可以通过分析来理解，并且很容易与记忆性表示区分开。具体而言，我们考虑了在模数算术任务上训练的两层神经网络，在这些任务中，（ξ·100%）的标签是被损坏的（即训练集中的一些模数运算结果是错误的）。我们展示了：（i）网络可以同时记住损坏的标签并实现100%的泛化；（ii）可以识别和剪枝记忆化的神经元，降低对损坏数据的准确率，提高对未损坏数据的准确率；（iii）正则化方法如w

    Robust generalization is a major challenge in deep learning, particularly when the number of trainable parameters is very large. In general, it is very difficult to know if the network has memorized a particular set of examples or understood the underlying rule (or both). Motivated by this challenge, we study an interpretable model where generalizing representations are understood analytically, and are easily distinguishable from the memorizing ones. Namely, we consider two-layer neural networks trained on modular arithmetic tasks where ($\xi \cdot 100\%$) of labels are corrupted (\emph{i.e.} some results of the modular operations in the training set are incorrect). We show that (i) it is possible for the network to memorize the corrupted labels \emph{and} achieve $100\%$ generalization at the same time; (ii) the memorizing neurons can be identified and pruned, lowering the accuracy on corrupted data and improving the accuracy on uncorrupted data; (iii) regularization methods such as w
    
[^95]: 健壮的多模态模型具有异常特征并编码更多概念

    Robust multimodal models have outlier features and encode more concepts. (arXiv:2310.13040v1 [cs.LG])

    [http://arxiv.org/abs/2310.13040](http://arxiv.org/abs/2310.13040)

    健壮的多模态模型展示了异常特征和更多概念的编码方式。

    

    什么区分健壮模型与非健壮模型？随着大规模多模态模型（如CLIP）的出现，这个问题引起了人们的关注。这些模型在自然分布转变方面表现出了前所未有的健壮性。尽管已经证明了健壮性的差异可以追溯到训练数据上的差异，但迄今为止还不清楚这对于模型学习到了什么意味着。在这项工作中，我们通过探测12个具有不同骨干（ResNets和ViTs）和预训练集（OpenAI，LAION-400M，LAION-2B，YFCC15M，CC12M和DataComp）的健壮多模态模型的表示空间来填补这一空白。我们发现这些模型的表示空间中存在两个健壮性的特征：（1）健壮模型具有由其激活特征表征的异常特征，其中一些特征值比平均值高几个数量级。这些异常特征在模型的表示空间中引入了特权方向。我们证明了...

    What distinguishes robust models from non-robust ones? This question has gained traction with the appearance of large-scale multimodal models, such as CLIP. These models have demonstrated unprecedented robustness with respect to natural distribution shifts. While it has been shown that such differences in robustness can be traced back to differences in training data, so far it is not known what that translates to in terms of what the model has learned. In this work, we bridge this gap by probing the representation spaces of 12 robust multimodal models with various backbones (ResNets and ViTs) and pretraining sets (OpenAI, LAION-400M, LAION-2B, YFCC15M, CC12M and DataComp). We find two signatures of robustness in the representation spaces of these models: (1) Robust models exhibit outlier features characterized by their activations, with some being several orders of magnitude above average. These outlier features induce privileged directions in the model's representation space. We demon
    
[^96]: Agri-GNN:一种新颖的基因型-拓扑图神经网络框架，建立在GraphSAGE上，用于优化产量预测。

    Agri-GNN: A Novel Genotypic-Topological Graph Neural Network Framework Built on GraphSAGE for Optimized Yield Prediction. (arXiv:2310.13037v1 [cs.LG])

    [http://arxiv.org/abs/2310.13037](http://arxiv.org/abs/2310.13037)

    Agri-GNN是一种新型基因型-拓扑图神经网络框架，通过考虑作物之间的复杂空间和基因型相互作用，实现优化的农作物产量预测。

    

    农业作为人类文明的基石，不断寻求整合技术以提高生产力和可持续性。本文介绍了一种名为Agri-GNN的新型基因型-拓扑图神经网络框架，旨在捕捉作物之间复杂的空间和基因型相互作用，为优化农作物产量预测铺平道路。Agri-GNN构建了一个图G，将农田地块视为节点，并根据空间和基因型相似性方法地构建节点之间的边，通过基因型-拓扑过滤器对节点信息进行聚合。图神经网络（GNN）通过设计考虑数据点之间的关系，使其能够有效地对农业生态系统进行建模。通过利用GNN的能力，Agri-GNN可以从植物中捕捉局部和全局信息，考虑它们基于空间属性的内在关联。

    Agriculture, as the cornerstone of human civilization, constantly seeks to integrate technology for enhanced productivity and sustainability. This paper introduces $\textit{Agri-GNN}$, a novel Genotypic-Topological Graph Neural Network Framework tailored to capture the intricate spatial and genotypic interactions of crops, paving the way for optimized predictions of harvest yields. $\textit{Agri-GNN}$ constructs a Graph $\mathcal{G}$ that considers farming plots as nodes, and then methodically constructs edges between nodes based on spatial and genotypic similarity, allowing for the aggregation of node information through a genotypic-topological filter. Graph Neural Networks (GNN), by design, consider the relationships between data points, enabling them to efficiently model the interconnected agricultural ecosystem. By harnessing the power of GNNs, $\textit{Agri-GNN}$ encapsulates both local and global information from plants, considering their inherent connections based on spatial pro
    
[^97]: LASER：无线分布式优化中的线性压缩

    LASER: Linear Compression in Wireless Distributed Optimization. (arXiv:2310.13033v1 [cs.NE])

    [http://arxiv.org/abs/2310.13033](http://arxiv.org/abs/2310.13033)

    LASER是一种新的压缩方案，通过利用梯度的低秩结构在噪声通道上高效传输梯度，相对于现有方案在计算机视觉和GPT语言建模任务上表现出持续优势。

    

    数据并行SGD是分布式优化的事实上的算法，尤其适用于大规模机器学习。尽管它有很多优点，但通信瓶颈是其中持久存在的问题之一。大多数压缩方案要么假设通信链路无噪声，要么在实际任务中无法取得良好的性能。在本文中，我们填补了这一空白，介绍了LASER：无线分布式优化中的线性压缩。LASER利用梯度的固有低秩结构，在噪声通道上高效传输梯度。尽管享受与经典SGD相似的理论保证，LASER在各种实际基准测试中表现出持续的优势。特别是，在具有挑战性的计算机视觉和GPT语言建模任务中，它优于最先进的压缩方案。在后者中，我们相对于噪声通道上的基准模型在困惑度上获得了50-64%的提升。

    Data-parallel SGD is the de facto algorithm for distributed optimization, especially for large scale machine learning. Despite its merits, communication bottleneck is one of its persistent issues. Most compression schemes to alleviate this either assume noiseless communication links, or fail to achieve good performance on practical tasks. In this paper, we close this gap and introduce LASER: LineAr CompreSsion in WirEless DistRibuted Optimization. LASER capitalizes on the inherent low-rank structure of gradients and transmits them efficiently over the noisy channels. Whilst enjoying theoretical guarantees similar to those of the classical SGD, LASER shows consistent gains over baselines on a variety of practical benchmarks. In particular, it outperforms the state-of-the-art compression schemes on challenging computer vision and GPT language modeling tasks. On the latter, we obtain $50$-$64 \%$ improvement in perplexity over our baselines for noisy channels.
    
[^98]: AI反馈促进的质量-多样性算法

    Quality-Diversity through AI Feedback. (arXiv:2310.13032v1 [cs.CL])

    [http://arxiv.org/abs/2310.13032](http://arxiv.org/abs/2310.13032)

    基于AI反馈的质量-多样性（QDAIF）算法利用语言模型来生成和评估创造性写作，比传统算法更广泛地覆盖高质量样本的搜索空间。

    

    在许多文本生成问题中，用户可能不仅偏好单一回复，而是希望得到多样性的高质量输出以供选择。质量-多样性（QD）搜索算法旨在通过不断改进和多样化候选人群来实现这一目标。然而，QD在创作性写作等质性领域的应用受到算法指定质量和多样性度量的困难的限制。有趣的是，最近语言模型（LMs）的发展使得通过AI反馈指导搜索成为可能，其中LMs在自然语言中被提示来评估文本的质性方面。借助这一进展，我们引入了通过AI反馈实现的质量-多样性算法（QDAIF），其中进化算法应用LMs来生成变异并评估候选文本的质量和多样性。在创作性写作领域的评估中，与非QDAIF算法相比，QDAIF更广泛地覆盖高质量样本的指定搜索空间。

    In many text-generation problems, users may prefer not only a single response, but a diverse range of high-quality outputs from which to choose. Quality-diversity (QD) search algorithms aim at such outcomes, by continually improving and diversifying a population of candidates. However, the applicability of QD to qualitative domains, like creative writing, has been limited by the difficulty of algorithmically specifying measures of quality and diversity. Interestingly, recent developments in language models (LMs) have enabled guiding search through AI feedback, wherein LMs are prompted in natural language to evaluate qualitative aspects of text. Leveraging this development, we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an evolutionary algorithm applies LMs to both generate variation and evaluate the quality and diversity of candidate text. When assessed on creative writing domains, QDAIF covers more of a specified search space with high-quality samples than do non-
    
[^99]: 将查询重写重新定义为统计机器翻译问题的用例

    A Use Case: Reformulating Query Rewriting as a Statistical Machine Translation Problem. (arXiv:2310.13031v1 [cs.CL])

    [http://arxiv.org/abs/2310.13031](http://arxiv.org/abs/2310.13031)

    本文提出了一种基于统计机器翻译的查询重写方法，通过学习重写阿拉伯语用户查询以改善搜索引擎的检索结果。

    

    现代搜索引擎面临的最重要挑战之一是根据用户查询检索相关的网络内容。为了实现这个挑战，搜索引擎有一个模块来重写用户查询。因此，现代网络搜索引擎利用了自然语言处理领域中使用的一些统计和神经模型。其中，统计机器翻译是一种众所周知的NLP方法。本文提出了一种基于单语机器翻译模型的查询重写流程，学习如何重写阿拉伯语用户搜索查询。本文还描述了创建用户查询和网页标题之间映射的预处理步骤。

    One of the most important challenges for modern search engines is to retrieve relevant web content based on user queries. In order to achieve this challenge, search engines have a module to rewrite user queries. That is why modern web search engines utilize some statistical and neural models used in the natural language processing domain. Statistical machine translation is a well-known NLP method among them. The paper proposes a query rewriting pipeline based on a monolingual machine translation model that learns to rewrite Arabic user search queries. This paper also describes preprocessing steps to create a mapping between user queries and web page titles.
    
[^100]: 融合梯度提升树和神经网络进行层次时间序列的点预测和概率预测

    Blending gradient boosted trees and neural networks for point and probabilistic forecasting of hierarchical time series. (arXiv:2310.13029v1 [cs.LG])

    [http://arxiv.org/abs/2310.13029](http://arxiv.org/abs/2310.13029)

    本文提出了一种融合梯度提升树和神经网络的方法，有效解决了层次时间序列的点预测和概率预测问题。

    

    本文通过描述一种融合梯度提升树和神经网络方法的机器学习模型，解决了点预测和概率预测的问题。该方法在最近的M5竞赛的准确性和不确定性赛道上取得了成功。我们方法的关键点包括：a)将任务转化为对每天销售额的回归问题；b)信息丰富的特征工程；c)创建一系列最先进的机器学习模型；d)精心构建用于模型调优的验证集。我们认为，机器学习模型的多样性以及验证样本的精心选择是我们方法有效的最重要因素。尽管预测数据具有层次结构（12个层级），但我们提出的解决方案没有利用该层次结构。使用所提出的方法，我们的团队在金牌范围内排名。

    In this paper we tackle the problem of point and probabilistic forecasting by describing a blending methodology of machine learning models that belong to gradient boosted trees and neural networks families. These principles were successfully applied in the recent M5 Competition on both Accuracy and Uncertainty tracks. The keypoints of our methodology are: a) transform the task to regression on sales for a single day b) information rich feature engineering c) create a diverse set of state-of-the-art machine learning models and d) carefully construct validation sets for model tuning. We argue that the diversity of the machine learning models along with the careful selection of validation examples, where the most important ingredients for the effectiveness of our approach. Although forecasting data had an inherent hierarchy structure (12 levels), none of our proposed solutions exploited that hierarchical scheme. Using the proposed methodology, our team was ranked within the gold medal ran
    
[^101]: 通过附加件变得贝叶斯，捕捉更多不确定性

    Be Bayesian by Attachments to Catch More Uncertainty. (arXiv:2310.13027v1 [cs.LG])

    [http://arxiv.org/abs/2310.13027](http://arxiv.org/abs/2310.13027)

    本文提出了一种附加结构贝叶斯神经网络(ABNN)，通过在主干网络中整合足够分布外数据的不确定性，来提高神经网络对不确定性的捕捉能力。

    

    贝叶斯神经网络(BNNs)已成为不确定性评估的有希望方法之一，由于其坚实的理论基础。然而，BNNs的性能受到捕捉不确定性的能力的影响。本文提出了一种新的附加结构贝叶斯神经网络(ABNN)，通过附加结构从足够分布外的数据(OOD)中捕捉更多的不确定性。我们首先根据先验分布为OOD数据的不确定性构建了一个数学描述，然后开发了一个附加的贝叶斯结构将OOD数据的不确定性整合到主干网络中。ABNN由期望模块和若干分布模块组成。期望模块是一个专注于原始任务的主干深度网络，而分布模块则是作为主干的附加结构的小贝叶斯结构。特别地，这些分布模块的目的是检测和传播OOD数据的不确定性，从而提高整体网络的贝叶斯性质。

    Bayesian Neural Networks (BNNs) have become one of the promising approaches for uncertainty estimation due to the solid theorical foundations. However, the performance of BNNs is affected by the ability of catching uncertainty. Instead of only seeking the distribution of neural network weights by in-distribution (ID) data, in this paper, we propose a new Bayesian Neural Network with an Attached structure (ABNN) to catch more uncertainty from out-of-distribution (OOD) data. We first construct a mathematical description for the uncertainty of OOD data according to the prior distribution, and then develop an attached Bayesian structure to integrate the uncertainty of OOD data into the backbone network. ABNN is composed of an expectation module and several distribution modules. The expectation module is a backbone deep network which focuses on the original task, and the distribution modules are mini Bayesian structures which serve as attachments of the backbone. In particular, the distribu
    
[^102]: 朝向随时微调：使用超网络提示进行持续预训练的语言模型

    Towards Anytime Fine-tuning: Continually Pre-trained Language Models with Hypernetwork Prompt. (arXiv:2310.13024v1 [cs.CL])

    [http://arxiv.org/abs/2310.13024](http://arxiv.org/abs/2310.13024)

    本研究旨在解决持续预训练方法在未知领域上性能下降的问题，提出了一种基于超网络提示的持续预训练方法，通过使用一致性和不一致性损失来生成领域特定的提示，显著减轻了领域特异性并促进了知识的传递。

    

    在快速发展的世界中，持续预训练对于使预训练模型适应各种领域和任务变得迫切。在实践中，持续预训练模型被期望不仅在预训练领域上进行微调时具有更大的容量，而且在未知领域上的性能不会下降。在本文中，我们首先调查了现有持续预训练方法的随时微调效果，得出了对未知领域的普遍性能下降的结论。为此，我们提出了一种基于提示的持续预训练方法，通过一致性和不一致性损失训练超网络生成特定领域的提示。一致性损失最大程度地保留了预训练模型对新领域的泛化能力，而不一致性损失则保护了为每个领域生成的隐藏状态的独特性。显著的是，超网络生成的提示在微调时减轻了领域特异性并促进了知识的传递。

    Continual pre-training has been urgent for adapting a pre-trained model to a multitude of domains and tasks in the fast-evolving world. In practice, a continually pre-trained model is expected to demonstrate not only greater capacity when fine-tuned on pre-trained domains but also a non-decreasing performance on unseen ones. In this work, we first investigate such anytime fine-tuning effectiveness of existing continual pre-training approaches, concluding with unanimously decreased performance on unseen domains. To this end, we propose a prompt-guided continual pre-training method, where we train a hypernetwork to generate domain-specific prompts by both agreement and disagreement losses. The agreement loss maximally preserves the generalization of a pre-trained model to new domains, and the disagreement one guards the exclusiveness of the generated hidden states for each domain. Remarkably, prompts by the hypernetwork alleviate the domain identity when fine-tuning and promote knowledge
    
[^103]: 不确定性感知的参数高效自训练用于半监督语言理解

    Uncertainty-aware Parameter-Efficient Self-training for Semi-supervised Language Understanding. (arXiv:2310.13022v1 [cs.LG])

    [http://arxiv.org/abs/2310.13022](http://arxiv.org/abs/2310.13022)

    本论文提出了一种不确定性感知的参数高效自训练（UPET）框架，通过利用Monte Carlo dropout来进行贝叶斯神经网络中的不确定性估计，并根据置信度和确定性选择可靠的伪标记样本，以解决标记数据稀缺问题。

    

    最近大型预训练语言模型（PLM）的成功在很大程度上依赖于大量标记数据，这在资源有限的情况下通常会产生较差的性能。为了解决这个困境，我们研究了自训练作为半监督学习（SSL）方法中的一种主要方法，利用大规模未标记数据生成合成样本。然而，太多的噪声标签会损害模型性能，而且自训练过程需要多次训练迭代，如果更新PLM的所有模型参数，会更加昂贵。本文提出了UPET，一种新颖的不确定性感知的参数高效自训练框架，以有效且高效地解决标记数据稀缺问题。具体而言，我们将蒙特卡罗（MC）dropout引入贝叶斯神经网络（BNN）以进行教师模型的不确定性估计，然后根据置信度和确定性来精确选择可靠的伪标记样本。

    The recent success of large pre-trained language models (PLMs) heavily hinges on massive labeled data, which typically produces inferior performance in low-resource scenarios. To remedy this dilemma, we study self-training as one of the predominant semi-supervised learning (SSL) approaches, which utilizes large-scale unlabeled data to generate synthetic examples. However, too many noisy labels will hurt the model performance, and the self-training procedure requires multiple training iterations making it more expensive if all the model parameters of the PLM are updated. This paper presents UPET, a novel Uncertainty-aware Parameter-Efficient self-Training framework to effectively and efficiently address the labeled data scarcity issue. Specifically, we incorporate Monte Carlo (MC) dropout in Bayesian neural network (BNN) to perform uncertainty estimation for the teacher model and then judiciously select reliable pseudo-labeled examples based on confidence and certainty. During the stude
    
[^104]: 通过DeepFool算法对深度神经网络进行有针对性的类别操纵的对抗攻击定制

    Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class Manipulation Using DeepFool Algorithm. (arXiv:2310.13019v1 [cs.CV])

    [http://arxiv.org/abs/2310.13019](http://arxiv.org/abs/2310.13019)

    本文提出了一种增强版DeepFool算法，名为Targeted DeepFool，可以针对特定类别进行错误分类，并引入了最小置信度分数要求超参数来提高灵活性。

    

    深度神经网络（DNNs）在各个领域都取得了显著的进展，但对抗攻击的易受攻击性引起了严重关注。了解这些易受攻击性并开发有效的防御机制至关重要。DeepFool是Moosavi-Dezfooli等人（2016年）提出的一种算法，用于找到将输入图像错误分类的最小扰动。然而，DeepFool缺乏有针对性的方法，使其在特定攻击场景中的有效性较低。此外，在先前的相关工作中，研究人员主要关注的是成功率，而没有考虑图像被扭曲的程度、图像质量的完整性以及错误分类的置信度水平。因此，在本文中，我们提出了Targeted DeepFool，这是DeepFool的增强版，可以针对特定类别进行错误分类。我们还引入了一个最小置信度分数要求超参数来增强灵活性。我们的实验证明了所提方法在不同情况下的有效性和效率。

    Deep neural networks (DNNs) have significantly advanced various domains, but their vulnerability to adversarial attacks poses serious concerns. Understanding these vulnerabilities and developing effective defense mechanisms is crucial. DeepFool, an algorithm proposed by Moosavi-Dezfooli et al. (2016), finds minimal perturbations to misclassify input images. However, DeepFool lacks a targeted approach, making it less effective in specific attack scenarios. Also, in previous related works, researchers primarily focus on success, not considering how much an image is getting distorted; the integrity of the image quality, and the confidence level to misclassifying. So, in this paper, we propose Targeted DeepFool, an augmented version of DeepFool that allows targeting specific classes for misclassification. We also introduce a minimum confidence score requirement hyperparameter to enhance flexibility. Our experiments demonstrate the effectiveness and efficiency of the proposed method across 
    
[^105]: 对表示一致性达成共识

    Getting aligned on representational alignment. (arXiv:2310.13018v1 [q-bio.NC])

    [http://arxiv.org/abs/2310.13018](http://arxiv.org/abs/2310.13018)

    该论文研究了生物和人工信息处理系统的表示一致性，探讨了不同系统之间的表示是否一致以及如何调整表示以更好地匹配其他系统。为了改善领域之间的交流，提出了一个统一的框架作为共同语言。

    

    生物和人工信息处理系统构建可以用来进行分类、推理、规划、导航和决策的世界表示。这些多样化系统所构建的表示在多大程度上是一致的？即使表示不同，是否仍然能够导致相同的行为？系统如何修改它们的表示以更好地匹配另一个系统的表示？这些关于表示一致性研究的问题是当代认知科学、神经科学和机器学习中一些最活跃的研究领域的核心。不幸的是，对于对表示一致性感兴趣的研究社区之间的知识转移有限，其中大部分在一个领域的进展最终会在另一个领域独立地重新发现，而更广泛的领域间交流将是有利的。为了改善领域之间的交流，我们提出了一个统一的框架，可以作为一种共同的语言。

    Biological and artificial information processing systems form representations of the world that they can use to categorize, reason, plan, navigate, and make decisions. To what extent do the representations formed by these diverse systems agree? Can diverging representations still lead to the same behaviors? And how can systems modify their representations to better match those of another system? These questions pertaining to the study of \textbf{\emph{representational alignment}} are at the heart of some of the most active research areas in contemporary cognitive science, neuroscience, and machine learning. Unfortunately, there is limited knowledge-transfer between research communities interested in representational alignment, and much of the progress in one field ends up being rediscovered independently in another, when greater cross-field communication would be advantageous. To improve communication between fields, we propose a unifying framework that can serve as a common language b
    
[^106]: 位置插值改进了ALiBi外推能力

    Position Interpolation Improves ALiBi Extrapolation. (arXiv:2310.13017v1 [cs.CL])

    [http://arxiv.org/abs/2310.13017](http://arxiv.org/abs/2310.13017)

    该论文提出了一种使用线性位置插值来改进ALiBi模型外推能力的方法，并且在上游语言建模和下游摘要和检索任务中取得了显著的改进。

    

    线性位置插值有助于使用旋转位置嵌入（RoPE）的预训练模型对更长的序列长度进行外推。我们提出使用线性位置插值来扩展使用带有线性偏差的注意力（ALiBi）的模型的外推范围。我们发现，位置插值显著改善了上游语言建模和下游摘要和检索任务的外推能力。

    Linear position interpolation helps pre-trained models using rotary position embeddings (RoPE) to extrapolate to longer sequence lengths. We propose using linear position interpolation to extend the extrapolation range of models using Attention with Linear Biases (ALiBi). We find position interpolation significantly improves extrapolation capability on upstream language modelling and downstream summarization and retrieval tasks.
    
[^107]: 大型语言模型的预测能力：来自现实预测竞赛的证据

    Large Language Model Prediction Capabilities: Evidence from a Real-World Forecasting Tournament. (arXiv:2310.13014v1 [cs.CY])

    [http://arxiv.org/abs/2310.13014](http://arxiv.org/abs/2310.13014)

    该论文通过参与Metaculus平台举办的预测竞赛，实证测试了OpenAI的最先进大型语言模型GPT-4的概率预测能力，并发现其与人类预测相比明显不准确。

    

    准确预测未来将是人工智能能力的重要里程碑。然而，关于大型语言模型提供关于未来事件概率预测能力的研究仍处于初级阶段。为了经验性地测试这种能力，我们将OpenAI最先进的大型语言模型GPT-4纳入了Metaculus平台举办的为期三个月的预测竞赛。这场从2023年7月到10月进行的竞赛吸引了843名参与者，涵盖了包括大型科技公司、美国政治、病毒爆发和乌克兰冲突在内的各种主题。我们聚焦于二进制预测，结果显示，与人群中位数预测相比，GPT-4的概率预测明显不准确。我们发现，GPT-4的预测与将每个问题的概率分配为50%的无信息预测策略没有显著差异。我们探讨了一个潜在的解释，即GPT-4可能有倾向性地预测概率为50%。

    Accurately predicting the future would be an important milestone in the capabilities of artificial intelligence. However, research on the ability of large language models to provide probabilistic predictions about future events remains nascent. To empirically test this ability, we enrolled OpenAI's state-of-the-art large language model, GPT-4, in a three-month forecasting tournament hosted on the Metaculus platform. The tournament, running from July to October 2023, attracted 843 participants and covered diverse topics including Big Tech, U.S. politics, viral outbreaks, and the Ukraine conflict. Focusing on binary forecasts, we show that GPT-4's probabilistic forecasts are significantly less accurate than the median human-crowd forecasts. We find that GPT-4's forecasts did not significantly differ from the no-information forecasting strategy of assigning a 50% probability to every question. We explore a potential explanation, that GPT-4 might be predisposed to predict probabilities clo
    
[^108]: 用于对齐语言模型的组合偏好模型

    Compositional preference models for aligning LMs. (arXiv:2310.13011v1 [cs.CL])

    [http://arxiv.org/abs/2310.13011](http://arxiv.org/abs/2310.13011)

    用于对齐语言模型的组合偏好模型（CPMs）是一种新颖的偏好模型框架，可以分解全局偏好评估并根据可解释的特征进行标量评分，得到更好的泛化能力和鲁棒性。

    

    随着语言模型的能力越来越强，将其与人类偏好进行对齐变得越来越重要。然而，用于训练偏好模型的主流范式存在根本性的限制，例如缺乏透明度和可扩展性，以及对偏好数据集过拟合的敏感性。我们提出了组合偏好模型（CPMs），这是一个新颖的偏好模型框架，将一个全局偏好评估分解为多个可解释的特征，从一个提示的语言模型中获取这些特征的标量评分，并使用逻辑回归分类器聚合这些评分。CPMs允许控制从偏好数据中使用哪些属性来训练偏好模型，并基于被认为是人类偏好判断基础的特征构建模型。我们的实验表明，CPMs不仅改善了泛化能力，比标准偏好模型更具鲁棒性，并且使用CPMs获得的最佳n个样本比使用标准PMs的表现更好。

    As language models (LMs) become more capable, it is increasingly important to align them with human preferences. However, the dominant paradigm for training Preference Models (PMs) for that purpose suffers from fundamental limitations, such as lack of transparency and scalability, along with susceptibility to overfitting the preference dataset. We propose Compositional Preference Models (CPMs), a novel PM framework that decomposes one global preference assessment into several interpretable features, obtains scalar scores for these features from a prompted LM, and aggregates these scores using a logistic regression classifier. CPMs allow to control which properties of the preference data are used to train the preference model and to build it based on features that are believed to underlie the human preference judgment. Our experiments show that CPMs not only improve generalization and are more robust to overoptimization than standard PMs, but also that best-of-n samples obtained using C
    
[^109]: LoBaSS：在监督微调数据中测量可学习性

    LoBaSS: Gauging Learnability in Supervised Fine-tuning Data. (arXiv:2310.13008v1 [cs.LG])

    [http://arxiv.org/abs/2310.13008](http://arxiv.org/abs/2310.13008)

    本文介绍了一种新的方法LoBaSS，利用数据的可学习性作为选择监督微调数据的主要标准。这种方法可以根据模型的能力将数据选择与模型对齐，确保高效的学习。

    

    监督微调（SFT）是将大型语言模型（LLM）与特定任务的先决条件对齐的关键阶段。微调数据的选择深刻影响模型的性能，传统上以数据质量和分布为基础。在本文中，我们引入了SFT数据选择的一个新维度：可学习性。这个新维度的动机是由LLM在预训练阶段获得的能力。鉴于不同的预训练模型具有不同的能力，适合一个模型的SFT数据可能不适合另一个模型。因此，我们引入了学习能力这个术语来定义数据对模型进行有效学习的适合性。我们提出了基于损失的SFT数据选择（LoBaSS）方法，利用数据的可学习性作为选择SFT数据的主要标准。这种方法提供了一种细致的方法，允许将数据选择与固有的模型能力对齐，确保高效的学习。

    Supervised Fine-Tuning (SFT) serves as a crucial phase in aligning Large Language Models (LLMs) to specific task prerequisites. The selection of fine-tuning data profoundly influences the model's performance, whose principle is traditionally grounded in data quality and distribution. In this paper, we introduce a new dimension in SFT data selection: learnability. This new dimension is motivated by the intuition that SFT unlocks capabilities acquired by a LLM during the pretraining phase. Given that different pretrained models have disparate capabilities, the SFT data appropriate for one may not suit another. Thus, we introduce the term learnability to define the suitability of data for effective learning by the model. We present the Loss Based SFT Data Selection (LoBaSS) method, utilizing data learnability as the principal criterion for the selection SFT data. This method provides a nuanced approach, allowing the alignment of data selection with inherent model capabilities, ensuring op
    
[^110]: 基于生成式人工智能的软件元数据分类

    Software Metadata Classification based on Generative Artificial Intelligence. (arXiv:2310.13006v1 [cs.SE])

    [http://arxiv.org/abs/2310.13006](http://arxiv.org/abs/2310.13006)

    本文提出了一种利用生成式人工智能在二进制代码评论质量分类模型中提升性能的新方法，通过引入生成数据集，模型的准确性得到了显著改善，支持向量机模型的精确度提高了6%，人工神经网络模型的召回率提高了1.5%。

    

    本文提出了一种新颖的方法，通过应用生成式人工智能(AI)来提高二进制代码评论质量分类模型的性能。通过利用OpenAI API，将从各种GitHub仓库和开源项目中提取的1239个新生成的代码-评论对数据集标记为“有用”或“无用”，并与现有的9048个C编程语言对数据集集成。利用先进的大型语言模型架构，生成的数据集在模型准确性方面表现出明显的改善。具体来说，当集成到支持向量机(SVM)模型中时，精确度提高了6%，从0.79增加到0.85。此外，人工神经网络(ANN)模型的召回率提高了1.5%，从0.731增加到0.746。本文揭示了生成式人工智能在增强代码评论质量分类模型中的潜力。

    This paper presents a novel approach to enhance the performance of binary code comment quality classification models through the application of Generative Artificial Intelligence (AI). By leveraging the OpenAI API, a dataset comprising 1239 newly generated code-comment pairs, extracted from various GitHub repositories and open-source projects, has been labelled as "Useful" or "Not Useful", and integrated into the existing corpus of 9048 pairs in the C programming language. Employing a cutting-edge Large Language Model Architecture, the generated dataset demonstrates notable improvements in model accuracy. Specifically, when incorporated into the Support Vector Machine (SVM) model, a 6% increase in precision is observed, rising from 0.79 to 0.85. Additionally, the Artificial Neural Network (ANN) model exhibits a 1.5% increase in recall, climbing from 0.731 to 0.746. This paper sheds light on the potential of Generative AI in augmenting code comment quality classification models. The res
    
[^111]: 渐进高效学习

    Progressively Efficient Learning. (arXiv:2310.13004v1 [cs.LG])

    [http://arxiv.org/abs/2310.13004](http://arxiv.org/abs/2310.13004)

    CEIL是一种渐进高效的学习框架，通过给学习代理提供一个抽象、动态的语言和一种内在的动机，以尽可能少的沟通代价学习，实现了类似人类逐步高效沟通的能力。在2D MineCraft领域上，CEIL展示了令人印象深刻的性能和沟通效率。

    

    助理 AI 代理应该能够迅速获取新技能并适应用户的新偏好。传统的框架如模仿学习和强化学习不能支持这种能力，因为它们只支持低级、低效的沟通形式。相比之下，人类通过定义和分享抽象意图来实现逐步高效的沟通。为了在 AI 代理中再现类似的能力，我们开发了一种名为沟通高效交互学习（CEIL）的新型学习框架。通过为学习代理提供一个抽象、动态的语言和一种内在的动机，即以尽可能少的沟通代价学习，CEIL 可以导致学习者和教师逐渐高效地交流，通过交换越来越抽象的意图。CEIL 在一个包含长期决策任务的 2D MineCraft 领域上展示了令人印象深刻的性能和沟通效率。

    Assistant AI agents should be capable of rapidly acquiring novel skills and adapting to new user preferences. Traditional frameworks like imitation learning and reinforcement learning do not facilitate this capability because they support only low-level, inefficient forms of communication. In contrast, humans communicate with progressive efficiency by defining and sharing abstract intentions. Reproducing similar capability in AI agents, we develop a novel learning framework named Communication-Efficient Interactive Learning (CEIL). By equipping a learning agent with an abstract, dynamic language and an intrinsic motivation to learn with minimal communication effort, CEIL leads to emergence of a human-like pattern where the learner and the teacher communicate progressively efficiently by exchanging increasingly more abstract intentions. CEIL demonstrates impressive performance and communication efficiency on a 2D MineCraft domain featuring long-horizon decision-making tasks. Agents trai
    
[^112]: 会话式金融信息检索模型（ConFIRM）

    Conversational Financial Information Retrieval Model (ConFIRM). (arXiv:2310.13001v1 [cs.IR])

    [http://arxiv.org/abs/2310.13001](http://arxiv.org/abs/2310.13001)

    ConFIRM是一种会话式金融信息检索模型，通过合成金融领域特定问答对和评估参数微调方法，实现了超过90%的准确性，为金融对话系统提供了数据高效的解决方案。

    

    随着大型语言模型（LLM）的指数级增长，利用它们在金融等专门领域的新兴特性具有探索的价值。然而，金融等受监管领域具有独特的约束条件，需要具备针对该领域的优化框架。我们提出了ConFIRM，一种基于LLM的会话式金融信息检索模型，用于查询意图分类和知识库标记。ConFIRM包括两个模块：1）一种合成金融领域特定问答对的方法，以及2）评估参数高效的微调方法来进行查询分类任务。我们生成了一个包含4000多个样本的数据集，并在单独的测试集上评估了准确性。ConFIRM实现了超过90%的准确性，这对于符合监管要求至关重要。ConFIRM提供了一种数据高效的解决方案，用于提取金融对话系统的精确查询意图。

    With the exponential growth in large language models (LLMs), leveraging their emergent properties for specialized domains like finance merits exploration. However, regulated fields such as finance pose unique constraints, requiring domain-optimized frameworks. We present ConFIRM, an LLM-based conversational financial information retrieval model tailored for query intent classification and knowledge base labeling.  ConFIRM comprises two modules:  1) a method to synthesize finance domain-specific question-answer pairs, and  2) evaluation of parameter efficient fine-tuning approaches for the query classification task. We generate a dataset of over 4000 samples, assessing accuracy on a separate test set.  ConFIRM achieved over 90% accuracy, essential for regulatory compliance. ConFIRM provides a data-efficient solution to extract precise query intent for financial dialog systems.
    
[^113]: 无监督学习在临床药物筛选中的药物反应预测

    Zero-shot Learning of Drug Response Prediction for Preclinical Drug Screening. (arXiv:2310.12996v1 [q-bio.BM])

    [http://arxiv.org/abs/2310.12996](http://arxiv.org/abs/2310.12996)

    本文提出了一种无监督学习的解决方案，可用于临床药物筛选中的药物反应预测。通过学习相似药物的先前反应数据，该方法能够增强对未标记化合物的实时预测。

    

    传统的深度学习方法通常使用有监督学习来进行药物反应的预测，这需要依赖于已标记的药物反应数据进行模型训练。然而，在临床药物筛选阶段的实际应用中，需要药物反应模型来预测新化合物的反应，这些化合物的药物反应通常是未知的，这带来了挑战，使得传统的有监督深度学习方法在这种情况下不适用。本文提出了一种在临床药物筛选中用于药物反应预测的无监督学习解决方案。具体而言，我们提出了一种多分支多源领域自适应测试增强插件(MSDA)，MSDA可以与传统的药物反应预测方法无缝集成，从相似药物的先前反应数据中学习不变特征，以增强对未标记化合物的实时预测。我们使用GDSCv2和CellMiner数据集进行实验。结果表明，MSDA能够高效地预测药物反应。

    Conventional deep learning methods typically employ supervised learning for drug response prediction (DRP). This entails dependence on labeled response data from drugs for model training. However, practical applications in the preclinical drug screening phase demand that DRP models predict responses for novel compounds, often with unknown drug responses. This presents a challenge, rendering supervised deep learning methods unsuitable for such scenarios. In this paper, we propose a zero-shot learning solution for the DRP task in preclinical drug screening. Specifically, we propose a Multi-branch Multi-Source Domain Adaptation Test Enhancement Plug-in, called MSDA. MSDA can be seamlessly integrated with conventional DRP methods, learning invariant features from the prior response data of similar drugs to enhance real-time predictions of unlabeled compounds. We conducted experiments using the GDSCv2 and CellMiner datasets. The results demonstrate that MSDA efficiently predicts drug respon
    
[^114]: 基于波形信息的复杂介质高分辨率成像的字典学习

    Wave-informed dictionary learning for high-resolution imaging in complex media. (arXiv:2310.12990v1 [cs.CV])

    [http://arxiv.org/abs/2310.12990](http://arxiv.org/abs/2310.12990)

    本论文提出了一种在复杂介质中利用大量多样的数据进行高分辨率成像的方法，通过字典学习和多维尺度缩放技术实现。通过模拟实验验证了该方法的有效性。

    

    我们提出了一种在散射介质中利用大量多样的数据集进行成像的方法。它有两个步骤。第一步使用字典学习算法估计真实的格林函数向量作为无序感知矩阵中的列。阵列数据来自许多稀疏的源集，其位置和强度我们并不知道。在第二步中，使用多维尺度缩放并利用感知矩阵列的交叉相关性导出的连接信息对感知矩阵的列进行排序，就像时间反演一样。为了使这两个步骤能够相互配合工作，我们需要来自大型接收器阵列的数据，以使感知矩阵的列在第一步中不相关，以及来自子阵列的数据，以使它们在第二步中具有足够的相关性以获得所需的连接性。通过模拟实验，我们展示了这种方法能够在复杂介质中提供具有均匀  分辨率的图像。

    We propose an approach for imaging in scattering media when large and diverse data sets are available. It has two steps. Using a dictionary learning algorithm the first step estimates the true Green's function vectors as columns in an unordered sensing matrix. The array data comes from many sparse sets of sources whose location and strength are not known to us. In the second step, the columns of the estimated sensing matrix are ordered for imaging using Multi-Dimensional Scaling with connectivity information derived from cross-correlations of its columns, as in time reversal. For these two steps to work together we need data from large arrays of receivers so the columns of the sensing matrix are incoherent for the first step, as well as from sub-arrays so that they are coherent enough to obtain the connectivity needed in the second step. Through simulation experiments, we show that the proposed approach is able to provide images in complex media whose resolution is that of a homogeneou
    
[^115]: 深度神经网络线性区域的训练动力学

    Training Dynamics of Deep Network Linear Regions. (arXiv:2310.12977v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2310.12977](http://arxiv.org/abs/2310.12977)

    该研究探讨了深度神经网络的线性区域训练动力学。通过对数据点周围线性区域的局部复杂性进行统计分析，发现训练过程中这种复杂性经历了几个阶段的变化。最后，通过精确的可视化方法，观察到训练的最后阶段有一个下降趋势。

    

    对于深度神经网络（DN）的训练动力学的研究主要集中在损失函数的演变上，这些演变是在训练和测试数据点附近进行评估的。事实上，许多DN现象最初是在文献中以此为基础引入的，例如，双重下降、理解。在本研究中，我们研究了由连续分段仿射DN（例如具有（泄漏的）ReLU非线性的网络）形成的输入空间划分或线性区域的训练动力学。首先，我们提出了一种新的统计量，该统计量根据数据点周围任意维度邻域中的线性区域的集中程度来囊括DN的局部复杂性（LC）。我们观察到，在训练过程中，数据点周围的LC经历了许多阶段，从初始化后的下降趋势开始，然后上升，并最终以下降趋势结束。通过使用精确的可视化方法，我们发现在训练的最后LC下降阶段，li

    The study of Deep Network (DN) training dynamics has largely focused on the evolution of the loss function, evaluated on or around train and test set data points. In fact, many DN phenomenon were first introduced in literature with that respect, e.g., double descent, grokking. In this study, we look at the training dynamics of the input space partition or linear regions formed by continuous piecewise affine DNs, e.g., networks with (leaky)ReLU nonlinearities. First, we present a novel statistic that encompasses the local complexity (LC) of the DN based on the concentration of linear regions inside arbitrary dimensional neighborhoods around data points. We observe that during training, the LC around data points undergoes a number of phases, starting with a decreasing trend after initialization, followed by an ascent and ending with a final descending trend. Using exact visualization methods, we come across the perplexing observation that during the final LC descent phase of training, li
    
[^116]: 从隐私保护数据中进行条件密度估计

    Conditional Density Estimations from Privacy-Protected Data. (arXiv:2310.12781v1 [stat.ML])

    [http://arxiv.org/abs/2310.12781](http://arxiv.org/abs/2310.12781)

    本文提出了一种从隐私保护数据中进行条件密度估计的方法，使用神经条件密度估计器来近似模型参数的后验分布，从而解决了在统计分析过程中只能访问私有化数据导致的计算复杂度增加的问题。

    

    许多现代统计分析和机器学习应用需要在敏感用户数据上进行模型训练。差分隐私提供了一种正式的保证，即个体用户信息不会泄露。在这个框架下，随机算法向保密数据注入校准的噪声，从而产生隐私保护的数据集或查询。然而，在统计分析过程中只能访问私有化数据会导致计算复杂度增加，难以对基础机密数据的参数进行有效的推理。在本工作中，我们提出了基于隐私保护数据集的基于模拟的推理方法。具体而言，我们使用神经条件密度估计器作为一组灵活的分布来近似给定观测到的私有查询结果的模型参数的后验分布。我们在传染病模型下的离散时间序列数据以及普通线性回归模型上说明了我们的方法。

    Many modern statistical analysis and machine learning applications require training models on sensitive user data. Differential privacy provides a formal guarantee that individual-level information about users does not leak. In this framework, randomized algorithms inject calibrated noise into the confidential data, resulting in privacy-protected datasets or queries. However, restricting access to only the privatized data during statistical analysis makes it computationally challenging to perform valid inferences on parameters underlying the confidential data. In this work, we propose simulation-based inference methods from privacy-protected datasets. Specifically, we use neural conditional density estimators as a flexible family of distributions to approximate the posterior distribution of model parameters given the observed private query results. We illustrate our methods on discrete time-series data under an infectious disease model and on ordinary linear regression models. Illustra
    
[^117]: 通过对随机分区的视图进行一致分配的表示学习

    Representation Learning via Consistent Assignment of Views over Random Partitions. (arXiv:2310.12692v1 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2310.12692](http://arxiv.org/abs/2310.12692)

    本论文提出了一种称为CARP的自监督聚类方法，通过对随机分区的视图进行一致分配，实现了可靠的表示学习，同时提高了训练稳定性和防止了崩溃解决方案的出现。在广泛的评估中，证明CARP的表示适用于多种下游任务，并与11种现有的自监督方法进行了比较。

    

    我们提出了一种自监督聚类方法，即对随机分区的视图进行一致分配（CARP），用于学习视觉特征的表示学习。CARP以端到端在线的方式使用梯度下降来学习原型，而无需额外的非可微模块来解决聚类分配问题。CARP通过基于原型的随机分区优化新的预训练任务，从而对模型进行正则化并强制视图之间的分配一致性。此外，我们的方法改善了训练的稳定性，并防止了联合嵌入训练中的崩溃解决方案。通过广泛的评估，我们证明CARP的表示适用于学习下游任务。我们在17个数据集上评估了CARP的表示能力，包括线性评估、少样本分类、k-NN、k-means、图像检索和复制检测等许多标准协议。我们将CARP的性能与11种现有的自监督方法进行了比较。

    We present Consistent Assignment of Views over Random Partitions (CARP), a self-supervised clustering method for representation learning of visual features. CARP learns prototypes in an end-to-end online fashion using gradient descent without additional non-differentiable modules to solve the cluster assignment problem. CARP optimizes a new pretext task based on random partitions of prototypes that regularizes the model and enforces consistency between views' assignments. Additionally, our method improves training stability and prevents collapsed solutions in joint-embedding training. Through an extensive evaluation, we demonstrate that CARP's representations are suitable for learning downstream tasks. We evaluate CARP's representations capabilities in 17 datasets across many standard protocols, including linear evaluation, few-shot classification, k-NN, k-means, image retrieval, and copy detection. We compare CARP performance to 11 existing self-supervised methods. We extensively abla
    
[^118]: SecurityNet：评估公共模型上的机器学习漏洞

    SecurityNet: Assessing Machine Learning Vulnerabilities on Public Models. (arXiv:2310.12665v1 [cs.CR] CROSS LISTED)

    [http://arxiv.org/abs/2310.12665](http://arxiv.org/abs/2310.12665)

    本研究通过在公共模型上评估攻击和防御来全面理解机器学习模型的漏洞，以解决对高计算资源要求较高的问题。

    

    尽管先进的机器学习（ML）模型被部署在许多实际应用中，之前的研究表明这些模型存在安全和隐私漏洞。在这一领域有各种经验性研究。然而，大多数实验是在由安全研究人员自己训练的目标ML模型上进行的。由于使用复杂架构训练高级模型需要高计算资源，研究人员通常选择使用相对简单的架构在典型的实验数据集上训练少数目标模型。我们认为，为了全面了解ML模型的漏洞，应该在训练有不同目的（而不仅仅是评估ML攻击和防御的目的）的大量模型上进行实验。为此，我们提出使用来自互联网的公开模型权重（公共模型）来评估对ML模型的攻击和防御。我们建立了一个数据库，名为SecurityNet

    While advanced machine learning (ML) models are deployed in numerous real-world applications, previous works demonstrate these models have security and privacy vulnerabilities. Various empirical research has been done in this field. However, most of the experiments are performed on target ML models trained by the security researchers themselves. Due to the high computational resource requirement for training advanced models with complex architectures, researchers generally choose to train a few target models using relatively simple architectures on typical experiment datasets. We argue that to understand ML models' vulnerabilities comprehensively, experiments should be performed on a large set of models trained with various purposes (not just the purpose of evaluating ML attacks and defenses). To this end, we propose using publicly available models with weights from the Internet (public models) for evaluating attacks and defenses on ML models. We establish a database, namely SecurityNe
    
[^119]: 面向焊接过程的基于深度学习的在线质量预测系统

    Towards a Deep Learning-based Online Quality Prediction System for Welding Processes. (arXiv:2310.12632v1 [cs.LG])

    [http://arxiv.org/abs/2310.12632](http://arxiv.org/abs/2310.12632)

    该论文提出了一个基于深度学习的气体金属电弧焊接预测质量系统的概念，通过收集和管理多传感器数据以及实时处理和特征工程的方式，可以识别关系并预测焊接质量。

    

    制造过程的数字化为机器学习辅助的质量保证提供了有前景的应用。一个广泛应用的制造过程，可以从数据驱动的解决方案中受益匪浅，是气体金属电弧焊接（GMAW）。焊接过程以材料性质、工艺条件和焊接质量之间复杂的因果关系为特征。在频繁更改工艺参数的非实验室环境中，通过破坏性测试准确确定焊缝质量是经济上不可行的。深度学习提供了从工艺观察中识别关系并预测焊接质量的潜力。本文提出了一个基于深度学习的气体金属电弧焊接预测质量系统的概念。核心概念包括由四个主要阶段组成的管线：多传感器数据（如电流和电压）的收集和管理、时间序列的实时处理和特征工程

    The digitization of manufacturing processes enables promising applications for machine learning-assisted quality assurance. A widely used manufacturing process that can strongly benefit from data-driven solutions is \ac{GMAW}. The welding process is characterized by complex cause-effect relationships between material properties, process conditions and weld quality. In non-laboratory environments with frequently changing process parameters, accurate determination of weld quality by destructive testing is economically unfeasible. Deep learning offers the potential to identify the relationships in available process data and predict the weld quality from process observations. In this paper, we present a concept for a deep learning based predictive quality system in \ac{GMAW}. At its core, the concept involves a pipeline consisting of four major phases: collection and management of multi-sensor data (e.g. current and voltage), real-time processing and feature engineering of the time series 
    
[^120]: DA-TransUNet: 将Spatial和Channel Dual Attention与Transformer U-Net集成，用于医学图像分割

    DA-TransUNet: Integrating Spatial and Channel Dual Attention with Transformer U-Net for Medical Image Segmentation. (arXiv:2310.12570v1 [eess.IV])

    [http://arxiv.org/abs/2310.12570](http://arxiv.org/abs/2310.12570)

    DA-TransUNet 提出了一种新的深度医学图像分割框架，集成了Transformer和双重注意力块。通过注意机制和多方面特征提取，提升医学图像分割结果。

    

    由于强大的深度表示学习，自动化医学图像分割取得了很大的进展。Transformer的影响导致了对其变体的研究，并大规模替代传统的CNN模块。然而，这种趋势经常忽视了Transformer的固有特征提取能力以及通过微小调整对模型和Transformer模块的潜在改进。本研究提出了一种新颖的深度医学图像分割框架，称为DA-TransUNet，旨在将Transformer和双重注意块引入传统U形架构的编码器和解码器中。与之前基于Transformer的解决方案不同，我们的DA-TransUNet利用了Transformer的注意机制和DA-Block的多方面特征提取，可以有效地结合全局、局部和多尺度特征以增强医学图像分割。同时，实验结果表明在之前的Transformer U-Net的基础上添加了一个双重注意块。

    Great progress has been made in automatic medical image segmentation due to powerful deep representation learning. The influence of transformer has led to research into its variants, and large-scale replacement of traditional CNN modules. However, such trend often overlooks the intrinsic feature extraction capabilities of the transformer and potential refinements to both the model and the transformer module through minor adjustments. This study proposes a novel deep medical image segmentation framework, called DA-TransUNet, aiming to introduce the Transformer and dual attention block into the encoder and decoder of the traditional U-shaped architecture. Unlike prior transformer-based solutions, our DA-TransUNet utilizes attention mechanism of transformer and multifaceted feature extraction of DA-Block, which can efficiently combine global, local, and multi-scale features to enhance medical image segmentation. Meanwhile, experimental results show that a dual attention block is added bef
    
[^121]: 利用可区分插入/删除指标感知正则化进行解释性训练

    Explanation-Based Training with Differentiable Insertion/Deletion Metric-Aware Regularizers. (arXiv:2310.12553v1 [cs.LG])

    [http://arxiv.org/abs/2310.12553](http://arxiv.org/abs/2310.12553)

    提出一种插入/删除指标感知的基于解释的优化(ID-ExpO)方法，通过优化可区分的预测器来提高解释的插入和删除得分，并保持预测准确性。实验结果表明，ID-ExpO能够使流行的事后解释器产生更忠实的解释。

    

    复杂机器学习预测器的解释质量通常使用插入和删除指标进行衡量，这些指标评估解释的忠实度，即解释正确地反映了预测器的行为程度。为了提高忠实度，我们提出了插入/删除指标感知的基于解释的优化（ID-ExpO），该优化能够改善解释的插入和删除得分，同时保持其预测准确性。由于原始的插入和删除指标对于解释来说是不可区分的，并且无法直接进行基于梯度的优化，我们扩展了这些指标以使其可区分，并将其用于形式化插入和删除指标的正则化。在图像和表格数据集上的实验结果表明，使用ID-ExpO进行微调的基于深度神经网络的预测器能够使流行的事后解释器产生更忠实的解释。

    The quality of explanations for the predictions of complex machine learning predictors is often measured using insertion and deletion metrics, which assess the faithfulness of the explanations, i.e., how correctly the explanations reflect the predictor's behavior. To improve the faithfulness, we propose insertion/deletion metric-aware explanation-based optimization (ID-ExpO), which optimizes differentiable predictors to improve both insertion and deletion scores of the explanations while keeping their predictive accuracy. Since the original insertion and deletion metrics are indifferentiable with respect to the explanations and directly unavailable for gradient-based optimization, we extend the metrics to be differentiable and use them to formalize insertion and deletion metric-based regularizers. The experimental results on image and tabular datasets show that the deep neural networks-based predictors fine-tuned using ID-ExpO enable popular post-hoc explainers to produce more faithful
    
[^122]: 等变引导法在成像反问题不确定性量化中的应用

    Equivariant Bootstrapping for Uncertainty Quantification in Imaging Inverse Problems. (arXiv:2310.11838v1 [eess.IV])

    [http://arxiv.org/abs/2310.11838](http://arxiv.org/abs/2310.11838)

    本文提出了一种新的不确定性量化方法，利用参数引导算法的等变形式，可以在成像反问题中量化重构图像的不确定性，并且可以与任何图像重建技术结合使用。

    

    科学成像问题通常存在严重的不适定性，因此具有重要的内在不确定性。准确量化解决方案的不确定性对于严格解释实验结果以及可靠地使用重构图像作为科学证据至关重要。不幸的是，现有的成像方法无法以对实验重复具有鲁棒性的方式量化重构图像中的不确定性。本文提出了一种新的不确定性量化方法，基于参数引导算法的等变形式，利用在成像问题中常见的对称性和不变性特性。此外，所提出的方法是通用的，可以轻松地与任何图像重建技术结合使用，包括只能从观察数据中进行训练的无监督训练策略，从而实现了不确定性量化在只有观测数据的情况下的应用。

    Scientific imaging problems are often severely ill-posed, and hence have significant intrinsic uncertainty. Accurately quantifying the uncertainty in the solutions to such problems is therefore critical for the rigorous interpretation of experimental results as well as for reliably using the reconstructed images as scientific evidence. Unfortunately, existing imaging methods are unable to quantify the uncertainty in the reconstructed images in a manner that is robust to experiment replications. This paper presents a new uncertainty quantification methodology based on an equivariant formulation of the parametric bootstrap algorithm that leverages symmetries and invariance properties commonly encountered in imaging problems. Additionally, the proposed methodology is general and can be easily applied with any image reconstruction technique, including unsupervised training strategies that can be trained from observed data alone, thus enabling uncertainty quantification in situations where 
    
[^123]: 从挫折中获得智慧：通过错误分析对齐大型语言模型

    Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis. (arXiv:2310.10477v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.10477](http://arxiv.org/abs/2310.10477)

    该论文介绍了一种基于错误分析的对齐策略，通过暴露大型语言模型的错误输出并进行评估，以理解内部原因。通过这种方法，有毒回应可以转化为模型对齐的指导调谐语料，从而提高模型的安全性并训练其进行自我批评。

    

    大型语言模型（LLMs）的快速发展既带来了机遇，也带来了挑战，特别是在意外生成有害和有毒回应方面。传统的对齐方法致力于引导LLMs朝着期望的性能发展并保护它们免受恶意内容的侵害，而本研究提出了一种基于错误分析的全新对齐策略，通过有意暴露LLMs的缺陷输出并进行深入评估，以完全理解内部原因，通过自然语言分析。因此，有毒回应可以转化为模型对齐的指导调谐语料，LLMs不仅可以避免生成有缺陷的回应，还可以训练其进行自我批评，发挥其辨别有毒内容的内在能力。实验结果表明，所提出的方法在安全指令遵循方面优于传统的对齐技术，同时还保持了卓越的效率。

    The rapid advancement of large language models (LLMs) presents both opportunities and challenges, particularly concerning unintentional generation of harmful and toxic responses. While the traditional alignment methods strive to steer LLMs towards desired performance and shield them from malicious content, this study proposes a novel alignment strategy rooted in mistake analysis by exposing LLMs to flawed outputs purposefully and then conducting a thorough assessment to fully comprehend internal reasons via natural language analysis. Thus, toxic responses can be transformed into instruction tuning corpus for model alignment, and LLMs can not only be deterred from generating flawed responses but also trained to self-criticize, leveraging its innate ability to discriminate toxic content. Experimental results demonstrate that the proposed method outperforms conventional alignment techniques for safety instruction following, while maintaining superior efficiency.
    
[^124]: Hamming编码器：为离散序列分类挖掘区分性k-mers

    Hamming Encoder: Mining Discriminative k-mers for Discrete Sequence Classification. (arXiv:2310.10321v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.10321](http://arxiv.org/abs/2310.10321)

    提出了一种名为Hamming编码器的新方法，利用二进制的1D卷积神经网络来挖掘区分性k-mer集，解决了基于模式的方法中存在的挑战。

    

    序列分类在各个领域中都有很多应用。尽管过去几十年进行了大量的研究，但仍然存在许多挑战，特别是在基于模式的方法中。现有的基于模式的方法在挖掘过程中单独衡量每个特征的区分能力，导致遗漏了一些具有区分能力的特征组合。此外，在将序列转换为特征向量后，很难确保整体的区分性能。为了解决这些挑战，我们提出了一种新的方法，称为Hamming编码器，它利用二进制的1D卷积神经网络（1DCNN）结构来挖掘区分性k-mer集。特别地，我们采用基于Hamming距离的相似度度量来确保特征挖掘和分类过程的一致性。我们的方法涉及训练一个可解释的CNN编码器来处理顺序数据，并通过梯度搜索来寻找区分性k-mer。

    Sequence classification has numerous applications in various fields. Despite extensive studies in the last decades, many challenges still exist, particularly in pattern-based methods. Existing pattern-based methods measure the discriminative power of each feature individually during the mining process, leading to the result of missing some combinations of features with discriminative power. Furthermore, it is difficult to ensure the overall discriminative performance after converting sequences into feature vectors. To address these challenges, we propose a novel approach called Hamming Encoder, which utilizes a binarized 1D-convolutional neural network (1DCNN) architecture to mine discriminative k-mer sets. In particular, we adopt a Hamming distance-based similarity measure to ensure consistency in the feature mining and classification procedure. Our method involves training an interpretable CNN encoder for sequential data and performing a gradient-based search for discriminative k-mer
    
[^125]: 时间序列和时空数据的大型模型：综述与展望

    Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook. (arXiv:2310.10196v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.10196](http://arxiv.org/abs/2310.10196)

    这篇综述探讨了大型模型在时间序列和时空数据中的应用。它们不仅带来了增强的模式识别和推理能力，还为人工通用智能打下了基础。

    

    时间数据，特别是时间序列和时空数据，在现实世界的应用中非常普遍。它们捕捉动态系统的测量数据，并由物理和虚拟传感器大量产生。分析这些数据类型对于利用它们所包含的丰富信息以及受益于各种下游任务非常重要。近年来，大型语言和其他基础模型的突破推动了这些模型在时间序列和时空数据挖掘中的增加使用。这样的方法不仅能够实现跨不同领域的增强模式识别和推理，还为能够理解和处理常见时间数据的人工通用智能打下了基础。在此综述中，我们提供了针对时间序列和时空数据定制的大型模型的全面和最新综述，包括数据类型、模型类别、模型范围和应用领域/任务。我们的目标是

    Temporal data, notably time series and spatio-temporal data, are prevalent in real-world applications. They capture dynamic system measurements and are produced in vast quantities by both physical and virtual sensors. Analyzing these data types is vital to harnessing the rich information they encompass and thus benefits a wide range of downstream tasks. Recent advances in large language and other foundational models have spurred increased use of these models in time series and spatio-temporal data mining. Such methodologies not only enable enhanced pattern recognition and reasoning across diverse domains but also lay the groundwork for artificial general intelligence capable of comprehending and processing common temporal data. In this survey, we offer a comprehensive and up-to-date review of large models tailored (or adapted) for time series and spatio-temporal data, spanning four key facets: data types, model categories, model scopes, and application areas/tasks. Our objective is to 
    
[^126]: 通过聚合实现标签差分隐私

    Label Differential Privacy via Aggregation. (arXiv:2310.10092v1 [cs.LG])

    [http://arxiv.org/abs/2310.10092](http://arxiv.org/abs/2310.10092)

    以前研究表明朴素的LBA和LLP不能提供标签差分隐私。但本研究显示，使用具有随机抽样的加权LBA可以提供标签差分隐私。

    

    在许多现实应用中，特别是由于隐私领域的最新发展，训练数据可以进行聚合，以保护敏感训练标签的隐私。在标签比例学习(LLP)框架中，数据集被划分为特征向量的包，只能获得每个包中标签的总和。进一步限制的限制学习(LBA)是只能获得包的特征向量的总和（可能是加权的）。我们研究这种聚合技术是否能够在标签差分隐私(label-DP)的概念下提供隐私保证，该概念之前在[Chaudhuri-Hsu'11, Ghazi et al.'21, Esfandiari et al.'22]中进行了研究。很容易看出，朴素的LBA和LLP不能提供标签差分隐私。然而，我们的主要结果表明，使用具有$m$个随机抽样的不相交$k$-大小包的加权LBA实际上是$(\varepsilon,

    In many real-world applications, in particular due to recent developments in the privacy landscape, training data may be aggregated to preserve the privacy of sensitive training labels. In the learning from label proportions (LLP) framework, the dataset is partitioned into bags of feature-vectors which are available only with the sum of the labels per bag. A further restriction, which we call learning from bag aggregates (LBA) is where instead of individual feature-vectors, only the (possibly weighted) sum of the feature-vectors per bag is available. We study whether such aggregation techniques can provide privacy guarantees under the notion of label differential privacy (label-DP) previously studied in for e.g. [Chaudhuri-Hsu'11, Ghazi et al.'21, Esfandiari et al.'22].  It is easily seen that naive LBA and LLP do not provide label-DP. Our main result however, shows that weighted LBA using iid Gaussian weights with $m$ randomly sampled disjoint $k$-sized bags is in fact $(\varepsilon, 
    
[^127]: 朝着更好的指令遵循评估：摘要中的一个案例研究

    Towards Better Evaluation of Instruction-Following: A Case-Study in Summarization. (arXiv:2310.08394v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.08394](http://arxiv.org/abs/2310.08394)

    本论文通过收集真实数据集riSum，对大型语言模型的指令遵循能力进行了评估，并提出了新的参考方法，以衡量这种能力。这些方法在性能上与需要高质量摘要的参考方法相当。

    

    尽管最近取得了一些进展，但是评估大型语言模型(LLMs)如何遵循用户指令仍然是一个开放问题。虽然语言模型的评估方法在基于提示的方法上有所增长，但是对这些方法的正确性的研究还很有限。在这项工作中，我们进行了元评估，评估了一系列指标的准确性，以衡量LLMs的指令遵循能力。我们的研究是在基于查询的摘要任务上进行的，通过收集一个新的短文形式的真实数据集riSum，其中包含300个文档指令对，每个对应3个答案。所有900个答案由3名人类标注员进行评分。利用riSum，我们分析了评估方法与人类判断之间的一致性。最后，我们提出了基于LLM的新的无参考评估方法，改进了已有的基准方法，并与需要高质量摘要的昂贵基于参考的度量方法表现相当。

    Despite recent advances, evaluating how well large language models (LLMs) follow user instructions remains an open problem. While evaluation methods of language models have seen a rise in prompt-based approaches, limited work on the correctness of these methods has been conducted. In this work, we perform a meta-evaluation of a variety of metrics to quantify how accurately they measure the instruction-following abilities of LLMs. Our investigation is performed on grounded query-based summarization by collecting a new short-form, real-world dataset riSum, containing 300 document-instruction pairs with 3 answers each. All 900 answers are rated by 3 human annotators. Using riSum, we analyze the agreement between evaluation methods and human judgment. Finally, we propose new LLM-based reference-free evaluation methods that improve upon established baselines and perform on par with costly reference-based metrics that require high-quality summaries.
    
[^128]: PICProp：用于不确定性量化的物理信息置信传播

    PICProp: Physics-Informed Confidence Propagation for Uncertainty Quantification. (arXiv:2310.06923v1 [cs.AI])

    [http://arxiv.org/abs/2310.06923](http://arxiv.org/abs/2310.06923)

    本文提出了一种名为PICProp的方法，基于双层优化，用于在深度学习和物理信息学习中进行不确定性量化。该方法能够在不进行强大假设的情况下计算有效的置信区间（CI），并且通过传播置信度实现了数据位置到整个域的置信度传播。

    

    在深度学习和物理信息学习中，标准的不确定性量化方法存在着持久的局限性。特别是，需要对数据的可能性做出强烈的假设，性能在很大程度上取决于先验的选择，并且后验只能以近似的方式进行采样，从而由于相关的计算成本导致近似精度较差。本文提出并研究了对确定性偏微分方程的置信区间（CI）估计作为一个新的问题。即，在整个区域中以概率担保的形式传播置信度，以达到数据位置到整个域的置信度传播。我们提出了一种名为物理信息置信传播（PICProp）的方法，基于双层优化来计算一个有效的CI，而不需要进行大量的假设。我们提供了关于我们方法有效性的定理以及针对物理信息学习的计算实验。

    Standard approaches for uncertainty quantification in deep learning and physics-informed learning have persistent limitations. Indicatively, strong assumptions regarding the data likelihood are required, the performance highly depends on the selection of priors, and the posterior can be sampled only approximately, which leads to poor approximations because of the associated computational cost. This paper introduces and studies confidence interval (CI) estimation for deterministic partial differential equations as a novel problem. That is, to propagate confidence, in the form of CIs, from data locations to the entire domain with probabilistic guarantees. We propose a method, termed Physics-Informed Confidence Propagation (PICProp), based on bi-level optimization to compute a valid CI without making heavy assumptions. We provide a theorem regarding the validity of our method, and computational experiments, where the focus is on physics-informed learning.
    
[^129]: 关于使用LSTD和随机特征的强化学习中的双下降现象

    On Double-Descent in Reinforcement Learning with LSTD and Random Features. (arXiv:2310.05518v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2310.05518](http://arxiv.org/abs/2310.05518)

    本文研究了在强化学习中网络大小和L2正则化对性能的影响，并观察到了双下降现象。通过使用随机特征和懒惰训练策略，在参数和状态数无限大的情况下研究了正则化的最小二乘时间差分算法，得出了其收敛性和最优性，并阐述了双下降现象在该算法中的影响。

    

    时间差分算法在深度强化学习中被广泛使用，其性能受神经网络大小的影响。然而，在监督学习中过参数化和其带来的好处已经得到了很好的理解，但是在强化学习中情况则不太清楚。本文通过理论分析探讨了网络大小和L2正则化对性能的影响，并将参数个数与访问状态个数之比定义为关键因素，当该比值大于1时称为过参数化。此外，我们观察到了双下降现象，即在参数/状态比为1附近会突然性能下降。通过利用随机特征和懒惰训练策略，我们在无限大的参数和状态数下研究了正则化的最小二乘时间差分算法。我们推导了其收敛性和最优性，并阐述了双下降现象在该算法中的影响。

    Temporal Difference (TD) algorithms are widely used in Deep Reinforcement Learning (RL). Their performance is heavily influenced by the size of the neural network. While in supervised learning, the regime of over-parameterization and its benefits are well understood, the situation in RL is much less clear. In this paper, we present a theoretical analysis of the influence of network size and $l_2$-regularization on performance. We identify the ratio between the number of parameters and the number of visited states as a crucial factor and define over-parameterization as the regime when it is larger than one. Furthermore, we observe a double-descent phenomenon, i.e., a sudden drop in performance around the parameter/state ratio of one. Leveraging random features and the lazy training regime, we study the regularized Least-Square Temporal Difference (LSTD) algorithm in an asymptotic regime, as both the number of parameters and states go to infinity, maintaining a constant ratio. We derive 
    
[^130]: 通过基于知识的推理和大型语言模型实现可解释的主张验证

    Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models. (arXiv:2310.05253v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05253](http://arxiv.org/abs/2310.05253)

    本文提出了一种基于知识的推理方法，通过大型语言模型实现可解释的主张验证。该方法不需要依赖昂贵的标注数据，能够验证复杂的主张并生成解释，对协助人工事实检查员具有重要意义。

    

    主张验证在打击虚假信息方面起着至关重要的作用。尽管现有的主张验证工作取得了令人满意的结果，但仍有一个关键问题尚未解决，那就是如何在不依赖于昂贵的人工标注数据的情况下进行主张验证。此外，模型提供全面解释以证明其决策并协助人工事实检查员也非常重要。本文提出了一种能够使用大型语言模型进行复杂主张验证和生成解释的一阶逻辑指导的知识基础的推理方法（FOLK）。FOLK利用大型语言模型的上下文学习能力将主张转化为一阶逻辑子句，并生成一组谓词，每个谓词对应一个需要验证的子主张。然后，FOLK通过一组基于知识的问答对进行一阶逻辑指导的推理，从而进行真实性验证。

    Claim verification plays a crucial role in combating misinformation. While existing works on claim verification have shown promising results, a crucial piece of the puzzle that remains unsolved is to understand how to verify claims without relying on human-annotated data, which is expensive to create at a large scale. Additionally, it is important for models to provide comprehensive explanations that can justify their decisions and assist human fact-checkers. This paper presents First-Order-Logic-Guided Knowledge-Grounded (FOLK) Reasoning that can verify complex claims and generate explanations without the need for annotated evidence using Large Language Models (LLMs). FOLK leverages the in-context learning ability of LLMs to translate the claim into a First-Order-Logic (FOL) clause consisting of predicates, each corresponding to a sub-claim that needs to be verified. Then, FOLK performs FOL-Guided reasoning over a set of knowledge-grounded question-and-answer pairs to make veracity pr
    
[^131]: 物理感知机器学习革命科学范式对于机器学习和基于过程的水文学的影响

    Physics-aware Machine Learning Revolutionizes Scientific Paradigm for Machine Learning and Process-based Hydrology. (arXiv:2310.05227v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.05227](http://arxiv.org/abs/2310.05227)

    物理感知机器学习是一种革命性方法，它将物理知识和机器学习相结合，提供了准确的水文学理解和水循环预测，对于管理水资源以应对气候变化等挑战具有重要意义。

    

    准确的水文学理解和水循环预测对于解决水资源管理中的科学和社会挑战至关重要，特别是在人为气候变化的动态影响下。现有的评论主要关注机器学习在这个领域的发展，然而水文学和机器学习作为独立的范式存在明显的区别。在这里，我们介绍了以物理感知机器学习作为一种变革性方法，克服了这种认知障碍，并革新了这两个领域。具体来说，我们提出了一个综合的物理感知机器学习方法的评论，构建了一个结构化社区（PaML），将先前的物理知识或基于物理的建模与机器学习相结合。我们系统地从物理数据引导的机器学习、物理信息处理的机器学习、物理嵌入式机器学习和物理感知混合学习四个方面分析了这些PaML方法。PaML促进了机器学习辅助的假设推导。

    Accurate hydrological understanding and water cycle prediction are crucial for addressing scientific and societal challenges associated with the management of water resources, particularly under the dynamic influence of anthropogenic climate change. Existing reviews predominantly concentrate on the development of machine learning (ML) in this field, yet there is a clear distinction between hydrology and ML as separate paradigms. Here, we introduce physics-aware ML as a transformative approach to overcome the perceived barrier and revolutionize both fields. Specifically, we present a comprehensive review of the physics-aware ML methods, building a structured community (PaML) of existing methodologies that integrate prior physical knowledge or physics-based modeling into ML. We systematically analyze these PaML methodologies with respect to four aspects: physical data-guided ML, physics-informed ML, physics-embedded ML, and physics-aware hybrid learning. PaML facilitates ML-aided hypothe
    
[^132]: SE(3)-蛋白质主链生成中的随机流匹配

    SE(3)-Stochastic Flow Matching for Protein Backbone Generation. (arXiv:2310.02391v1 [cs.LG])

    [http://arxiv.org/abs/2310.02391](http://arxiv.org/abs/2310.02391)

    通过SE(3)-Stochastic Flow Matching，我们提出了一系列新型生成模型FoldFlow，可以准确建模蛋白质主链。这些模型通过无需模拟训练和Riemannian最优传输的结合，具有更好的稳定性和建模能力。

    

    通过基于三维刚体运动（即SE(3)群）的流匹配范式，我们引入了一系列具有不断增强建模能力的新型生成模型：FoldFlow，从而实现了对蛋白质主链的准确建模。首先，我们介绍了FoldFlow-Base，一种无需模拟的学习确定性连续时间动力学和匹配不变目标分布的方法。接下来，我们通过引入Riemannian最优传输来加速训练，创建了FoldFlow-OT，从而构建了更简单和稳定的流。最后，我们设计了FoldFlow-SFM，将Riemannian最优传输和无需模拟训练相结合，可以学习SE(3)上的随机连续时间动力学。我们的FoldFlow生成模型家族相比之前的方法具有几个关键优势。

    The computational design of novel protein structures has the potential to impact numerous scientific disciplines greatly. Toward this goal, we introduce $\text{FoldFlow}$ a series of novel generative models of increasing modeling power based on the flow-matching paradigm over $3\text{D}$ rigid motions -i.e. the group $\text{SE(3)}$ -- enabling accurate modeling of protein backbones. We first introduce $\text{FoldFlow-Base}$, a simulation-free approach to learning deterministic continuous-time dynamics and matching invariant target distributions on $\text{SE(3)}$. We next accelerate training by incorporating Riemannian optimal transport to create $\text{FoldFlow-OT}$, leading to the construction of both more simple and stable flows. Finally, we design $\text{FoldFlow-SFM}$ coupling both Riemannian OT and simulation-free training to learn stochastic continuous-time dynamics over $\text{SE(3)}$. Our family of $\text{FoldFlow}$ generative models offer several key advantages over previous
    
[^133]: 关于自然语言处理中毒性定义的探讨

    On the definition of toxicity in NLP. (arXiv:2310.02357v1 [cs.CL])

    [http://arxiv.org/abs/2310.02357](http://arxiv.org/abs/2310.02357)

    这项研究探讨了毒性的定义模糊性问题，并提出了一种基于定量压力的毒性定义来弥补现有定义的缺点。

    

    毒性检测任务中的根本问题在于毒性的定义模糊不清。谷歌旗下的团队Jigsaw是该领域的领导者之一，他们使用Dixon等人给出的毒性定义：“粗鲁、不尊重或不合理的语言，可能会让某人离开讨论”。人们可以立即看到这个定义的问题，因为它没有给出毒性的定量度量，而且涉及高度主观的文化术语。尽管存在模糊和缺陷，但这个定义已经成为许多研究者广泛采用的实际标准。在这项工作中，我们提出了一种基于定量压力的毒性定义，克服了现有的缺点。

    The fundamental problem in toxicity detection task lies in the fact that the toxicity is ill-defined. Jigsaw, a unit within Google and one of the leaders in the field, uses a definition of toxicity given by Dixon et al. - 'rude, disrespectful, or unreasonable language that is likely to make someone leave a discussion'. One can instantly see the issue with this definition, as it gives no quantitative measure of the toxicity and operates with highly subjective cultural terms. Despite all vagueness and flaws, this definition is de-facto widely used by many researchers. In this work we suggest quantative stress-based defenition for the toxicity that overcomes existing shortcomings.
    
[^134]: 自我监督的无约束机器人经验中的地形表示学习

    Self-Supervised Terrain Representation Learning from Unconstrained Robot Experience. (arXiv:2309.15302v1 [cs.RO])

    [http://arxiv.org/abs/2309.15302](http://arxiv.org/abs/2309.15302)

    提出一种名为STERLING的自我监督地形表示学习方法，通过无约束的机器人经验学习有关地形的相关表示，以实现地形感知导航。

    

    地形认知，即辨别和区分不同类型的地形，是机器人在自主越野导航中必须具备的关键能力。目前提供机器人这种认知能力的方法要么依赖昂贵的标记数据收集，要么依赖无法泛化的工程特征和成本函数，或者依赖可能无法获得的专家人类示范。为了使机器人不受这些限制地具备地形认知能力，我们引入了自我监督地形表示学习（STERLING），这是一种新颖的学习地形表示的方法，仅依赖于易于收集的、无约束（例如，非专家的）和未标记的机器人经验，对数据采集没有额外的限制。STERLING采用了一种新颖的多模态自我监督目标，通过非对比度表示学习来学习地形相关的表示以用于地形感知导航。通过实物机器人实验

    Terrain awareness, i.e., the ability to identify and distinguish different types of terrain, is a critical ability that robots must have to succeed at autonomous off-road navigation. Current approaches that provide robots with this awareness either rely on labeled data which is expensive to collect, engineered features and cost functions that may not generalize, or expert human demonstrations which may not be available. Towards endowing robots with terrain awareness without these limitations, we introduce Self-supervised TErrain Representation LearnING (STERLING), a novel approach for learning terrain representations that relies solely on easy-to-collect, unconstrained (e.g., non-expert), and unlabelled robot experience, with no additional constraints on data collection. STERLING employs a novel multi-modal self-supervision objective through non-contrastive representation learning to learn relevant terrain representations for terrain-aware navigation. Through physical robot experiments
    
[^135]: 通过自适应的NLP模型选择和基于临床专家规则的分类器改进VTE的识别

    Improving VTE Identification through Adaptive NLP Model Selection and Clinical Expert Rule-based Classifier from Radiology Reports. (arXiv:2309.12273v1 [cs.CL])

    [http://arxiv.org/abs/2309.12273](http://arxiv.org/abs/2309.12273)

    通过自适应的NLP模型选择和临床专家规则的分类器，该研究提出一种改进VTE识别的新方法，在放射学报告中准确识别VTE事件的准确性得到提高。

    

    快速准确地识别静脉血栓栓塞（VTE），包括深静脉血栓（DVT）和肺栓塞（PE），对于有效治疗非常重要。利用自然语言处理（NLP）在放射学报告中，自动化方法已经在从回顾性数据集中识别VTE事件或帮助临床专家识别放射学报告中的VTE事件方面展示了有希望的进展。然而，由于标记有限的医学文本数据、放射学报告的复杂性和异质性以及数据不平衡，有效训练深度学习（DL）和NLP模型存在挑战。本研究提出了DL方法的新的组合方法，结合数据增强、自适应预训练的NLP模型选择和临床专家NLP基于规则的分类器，以提高非结构化（自由文本）放射学报告中VTE识别的准确性。我们的实验结果证明了该模型的有效性。

    Rapid and accurate identification of Venous thromboembolism (VTE), a severe cardiovascular condition including deep vein thrombosis (DVT) and pulmonary embolism (PE), is important for effective treatment. Leveraging Natural Language Processing (NLP) on radiology reports, automated methods have shown promising advancements in identifying VTE events from retrospective data cohorts or aiding clinical experts in identifying VTE events from radiology reports. However, effectively training Deep Learning (DL) and the NLP models is challenging due to limited labeled medical text data, the complexity and heterogeneity of radiology reports, and data imbalance. This study proposes novel method combinations of DL methods, along with data augmentation, adaptive pre-trained NLP model selection, and a clinical expert NLP rule-based classifier, to improve the accuracy of VTE identification in unstructured (free-text) radiology reports. Our experimental results demonstrate the model's efficacy, achievi
    
[^136]: FedDCSR: 通过解缠表示学习实现联邦跨领域顺序推荐

    FedDCSR: Federated Cross-domain Sequential Recommendation via Disentangled Representation Learning. (arXiv:2309.08420v1 [cs.LG])

    [http://arxiv.org/abs/2309.08420](http://arxiv.org/abs/2309.08420)

    提出了一种名为FedDCSR的联邦跨领域顺序推荐框架，通过解缠表示学习来处理不同领域之间的序列特征异质性，并保护数据隐私。

    

    近年来，利用来自多个领域的用户序列数据的跨领域顺序推荐(CSR)受到了广泛关注。然而，现有的CSR方法需要在领域之间共享原始用户数据，这违反了《通用数据保护条例》(GDPR)。因此，有必要将联邦学习(FL)和CSR相结合，充分利用不同领域的知识，同时保护数据隐私。然而，不同领域之间的序列特征异质性对FL的整体性能有显著影响。在本文中，我们提出了FedDCSR，这是一种通过解缠表示学习的新型联邦跨领域顺序推荐框架。具体而言，为了解决不同领域之间的序列特征异质性，我们引入了一种称为领域内-领域间序列表示解缠(SRD)的方法，将用户序列特征解缠成领域共享和领域专属特征。

    Cross-domain Sequential Recommendation (CSR) which leverages user sequence data from multiple domains has received extensive attention in recent years. However, the existing CSR methods require sharing origin user data across domains, which violates the General Data Protection Regulation (GDPR). Thus, it is necessary to combine federated learning (FL) and CSR to fully utilize knowledge from different domains while preserving data privacy. Nonetheless, the sequence feature heterogeneity across different domains significantly impacts the overall performance of FL. In this paper, we propose FedDCSR, a novel federated cross-domain sequential recommendation framework via disentangled representation learning. Specifically, to address the sequence feature heterogeneity across domains, we introduce an approach called inter-intra domain sequence representation disentanglement (SRD) to disentangle the user sequence features into domain-shared and domain-exclusive features. In addition, we design
    
[^137]: 在公共交通系统中建模供需

    Modeling Supply and Demand in Public Transportation Systems. (arXiv:2309.06299v1 [cs.LG])

    [http://arxiv.org/abs/2309.06299](http://arxiv.org/abs/2309.06299)

    该论文在公共交通系统中建立了供需模型，利用数据分析和机器学习技术揭示了运营服务中的空缺。

    

    哈里森堡公共交通部门旨在利用其数据提高运营效率和效果。我们构建了两个供需模型，帮助部门识别服务中的空缺。模型考虑了许多变量，包括哈里森堡市向联邦政府报告的方式以及最脆弱人口聚集的区域。我们采用数据分析和机器学习技术进行预测。

    The Harrisonburg Department of Public Transportation (HDPT) aims to leverage their data to improve the efficiency and effectiveness of their operations. We construct two supply and demand models that help the department identify gaps in their service. The models take many variables into account, including the way that the HDPT reports to the federal government and the areas with the most vulnerable populations in Harrisonburg City. We employ data analysis and machine learning techniques to make our predictions.
    
[^138]: 关于Tukey深度的随机近似质量

    On the quality of randomized approximations of Tukey's depth. (arXiv:2309.05657v1 [stat.ML])

    [http://arxiv.org/abs/2309.05657](http://arxiv.org/abs/2309.05657)

    本文研究了Tukey深度的随机近似质量问题，证明了在维度较高且数据从对数凹集的均匀分布中抽样的情况下，随机算法可以正确近似最大深度和接近零的深度，而对于中间深度的点，任何好的近似都需要指数复杂度。

    

    Tukey深度（或半空间深度）是用于多元数据中心度量的广泛应用的指标。然而，在高维度下，Tukey深度的精确计算被认为是一个困难的问题。为了解决这个问题，人们提出了Tukey深度的随机近似方法。在本文中，我们探讨了这样的随机算法何时能够返回一个良好的Tukey深度近似。我们研究了数据从对数凹陷均匀分布中抽样的情况。我们证明了，如果要求算法在维度上以多项式时间运行，随机算法可以正确地近似最大深度1/2和接近零的深度。另一方面，对于任何中间深度的点，任何好的近似都需要指数复杂度。

    Tukey's depth (or halfspace depth) is a widely used measure of centrality for multivariate data. However, exact computation of Tukey's depth is known to be a hard problem in high dimensions. As a remedy, randomized approximations of Tukey's depth have been proposed. In this paper we explore when such randomized algorithms return a good approximation of Tukey's depth. We study the case when the data are sampled from a log-concave isotropic distribution. We prove that, if one requires that the algorithm runs in polynomial time in the dimension, the randomized algorithm correctly approximates the maximal depth $1/2$ and depths close to zero. On the other hand, for any point of intermediate depth, any good approximation requires exponential complexity.
    
[^139]: CONVERT: 可靠增强的对比图聚类

    CONVERT:Contrastive Graph Clustering with Reliable Augmentation. (arXiv:2308.08963v1 [cs.LG])

    [http://arxiv.org/abs/2308.08963](http://arxiv.org/abs/2308.08963)

    本论文提出了一种名为COVERT的对比图聚类网络，通过可逆扰动恢复网络处理数据增强，提炼可靠的语义信息，进一步保证对比学习中语义的可靠性。

    

    通过可学习的数据增强进行对比图节点聚类是无监督图学习领域的研究热点。现有方法通过学习预定义增强的采样分布来自动生成数据驱动增强。虽然已取得了有希望的聚类性能，但我们观察到这些方法仍依赖于预定义增强，增强后的图的语义容易漂移，对比学习中的增强视图语义的可靠性无法保证，从而限制了模型性能。为了解决这些问题，我们提出了一种新的可靠增强的对比图聚类网络（COVERT）。具体来说，在我们的方法中，增强数据通过提出的可逆扰动恢复网络进行处理。它通过恢复扰动后的潜在嵌入来提炼可靠的语义信息。此外，为了进一步保证语义的可靠性，我们提出了一种新的语义Loss。

    Contrastive graph node clustering via learnable data augmentation is a hot research spot in the field of unsupervised graph learning. The existing methods learn the sampling distribution of a pre-defined augmentation to generate data-driven augmentations automatically. Although promising clustering performance has been achieved, we observe that these strategies still rely on pre-defined augmentations, the semantics of the augmented graph can easily drift. The reliability of the augmented view semantics for contrastive learning can not be guaranteed, thus limiting the model performance. To address these problems, we propose a novel CONtrastiVe Graph ClustEring network with Reliable AugmenTation (COVERT). Specifically, in our method, the data augmentations are processed by the proposed reversible perturb-recover network. It distills reliable semantic information by recovering the perturbed latent embeddings. Moreover, to further guarantee the reliability of semantics, a novel semantic lo
    
[^140]: 使用路径进行一般化拓扑图神经网络 (arXiv:2308.06838v2 [cs.LG] 已更新)

    Generalizing Topological Graph Neural Networks with Paths. (arXiv:2308.06838v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.06838](http://arxiv.org/abs/2308.06838)

    本论文提出了一种通用的拓扑图神经网络（GNNs）方法，通过强调图中的路径，突破了1-Weisfeiler-Lehmann测试的理论限制，并在无需对图的子结构进行假设的情况下，在多个基准测试中取得了最先进的性能。

    

    尽管图神经网络（GNNs）在不同领域取得了重大进展，但它们受到1-Weisfeiler-Lehmann测试的理论限制。尽管高阶GNNs的最新进展可以克服这个障碍，但它们通常集中在特定的图组件，如团或环。然而，我们的研究走了不同的路线。我们强调路径，这是每个图中固有的。我们能够构建一个更通用的拓扑视角，并与其他拓扑领域的一些成熟理论形成桥梁。有趣的是，在没有对图的子结构进行任何假设的情况下，我们的方法超过了该领域早期的技术，在多个基准测试中实现了最先进的性能。

    While Graph Neural Networks (GNNs) have made significant strides in diverse areas, they are hindered by a theoretical constraint known as the 1-Weisfeiler-Lehmann test. Even though latest advancements in higher-order GNNs can overcome this boundary, they typically center around certain graph components like cliques or cycles. However, our investigation goes a different route. We put emphasis on paths, which are inherent in every graph. We are able to construct a more general topological perspective and form a bridge to certain established theories about other topological domains. Interestingly, without any assumptions on graph sub-structures, our approach surpasses earlier techniques in this field, achieving state-of-the-art performance on several benchmarks.
    
[^141]: 提高医学图像分割中的越界检测的降维方法

    Dimensionality Reduction for Improving Out-of-Distribution Detection in Medical Image Segmentation. (arXiv:2308.03723v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.03723](http://arxiv.org/abs/2308.03723)

    本研究提出了一种降维方法，通过在医学图像分割中应用马氏距离后处理和主成分分析，实现对越界图像的高效检测。

    

    已知临床应用的分割模型在其训练分布之外的数据上表现不佳。由于这些模型在大多数情况下表现良好，因此在推断过程中检测越界图像是至关重要的，以防止自动化偏差。本研究将马氏距离后处理应用于Swin UNETR模型的瓶颈特征，该模型用于对T1加权磁共振成像上的肝脏进行分割。通过使用主成分分析降低瓶颈特征的维数，可以高效地检测到越界图像，并且具有较小的计算负载。

    Clinically deployed segmentation models are known to fail on data outside of their training distribution. As these models perform well on most cases, it is imperative to detect out-of-distribution (OOD) images at inference to protect against automation bias. This work applies the Mahalanobis distance post hoc to the bottleneck features of a Swin UNETR model that segments the liver on T1-weighted magnetic resonance imaging. By reducing the dimensions of the bottleneck features with principal component analysis, OOD images were detected with high performance and minimal computational load.
    
[^142]: 通过使用自监督语音表示损失函数研究口语对语音增强的影响

    The Effect of Spoken Language on Speech Enhancement using Self-Supervised Speech Representation Loss Functions. (arXiv:2307.14502v1 [eess.AS])

    [http://arxiv.org/abs/2307.14502](http://arxiv.org/abs/2307.14502)

    本文研究了在语音增强中，使用与嘈杂数据语言完全匹配的自监督语音表示损失函数训练的模型的性能更好。与传统的频谱图或时间域损失函数相比，这些增强模型具有特定语言的特性。

    

    最近在语音增强领域的研究中，使用自监督语音表示（SSSRs）作为特征转换在损失函数中。然而，在之前的工作中，很少注意到用于训练自监督表示的音频语言与用于训练语音增强系统的语言之间的关系。使用将自监督表示与用于训练语音增强系统的嘈杂数据的语言完全匹配的损失函数训练的增强模型表现比不完全匹配的模型更好。这可能导致增强系统具有特定语言的特性，因此对未见语言的泛化能力不好，而使用传统的频谱图或时间域损失函数训练的模型则不会出现这个问题。本研究在多种不同语言上训练和测试了语音增强模型，使用了经过不同语言组合训练的自监督表示。

    Recent work in the field of speech enhancement (SE) has involved the use of self-supervised speech representations (SSSRs) as feature transformations in loss functions. However, in prior work, very little attention has been paid to the relationship between the language of the audio used to train the self-supervised representation and that used to train the SE system. Enhancement models trained using a loss function which incorporates a self-supervised representation that shares exactly the language of the noisy data used to train the SE system show better performance than those which do not match exactly. This may lead to enhancement systems which are language specific and as such do not generalise well to unseen languages, unlike models trained using traditional spectrogram or time domain loss functions. In this work, SE models are trained and tested on a number of different languages, with self-supervised representations which themselves are trained using different language combinati
    
[^143]: Jina Embeddings:一种新颖的高性能句子嵌入模型

    Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models. (arXiv:2307.11224v1 [cs.CL])

    [http://arxiv.org/abs/2307.11224](http://arxiv.org/abs/2307.11224)

    Jina Embeddings是一组高性能的句子嵌入模型，能够捕捉文本的语义本质。该论文详细介绍了Jina Embeddings的开发过程，并通过性能评估验证了其优越性能。

    

    Jina Embeddings由一组高性能的句子嵌入模型组成，能够将各种文本输入转化为数值表示，从而捕捉文本的语义本质。虽然这些模型并非专门设计用于文本生成，但在密集检索和语义文本相似性等应用中表现出色。本文详细介绍了Jina Embeddings的开发过程，从创建高质量的成对和三元数据集开始。它强调了数据清理在数据集准备中的关键作用，并对模型训练过程进行了深入探讨，最后利用Massive Textual Embedding Benchmark（MTEB）进行了全面的性能评估。

    Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating various textual inputs into numerical representations, thereby capturing the semantic essence of the text. While these models are not exclusively designed for text generation, they excel in applications such as dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of a high-quality pairwise and triplet dataset. It underlines the crucial role of data cleaning in dataset preparation, gives in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Textual Embedding Benchmark (MTEB).
    
[^144]: 从早期老化数据中预测不同使用条件下的电池寿命

    Predicting Battery Lifetime Under Varying Usage Conditions from Early Aging Data. (arXiv:2307.08382v1 [cs.LG])

    [http://arxiv.org/abs/2307.08382](http://arxiv.org/abs/2307.08382)

    本研究通过提取早期寿命中的容量-电压数据的新特征，成功预测了在不同使用条件下的电池寿命。我们的研究结果表明，这些早期特征能够很好地捕捉电池的健康状态和老化模式的变化速率，为电池寿命预测提供了有希望的途径。

    

    精确的电池寿命预测对于预防性维护、保修和改进的电池设计和制造非常重要。然而，制造可变性和使用依赖性降解使得寿命预测变得具有挑战性。本研究通过从早期寿命中提取的容量-电压数据的新特征来预测在充电速率、放电速率和放电深度差异较大的电池的寿命。这些特征是从定期安排的参考性能测试中提取的（即低速完全循环）。早期生命周期特征捕捉到了电池的健康状态和各组件级别老化模式的变化速率，其中一些与电池寿命呈强相关性。通过对由225个镍-锰-钴/石墨锂离子电池在广泛条件下老化产生的新数据集的使用，我们证明了在分布内的电池的寿命预测能够达到15.1%的平均绝对百分比误差，并且只使用了前15%的数据。

    Accurate battery lifetime prediction is important for preventative maintenance, warranties, and improved cell design and manufacturing. However, manufacturing variability and usage-dependent degradation make life prediction challenging. Here, we investigate new features derived from capacity-voltage data in early life to predict the lifetime of cells cycled under widely varying charge rates, discharge rates, and depths of discharge. Features were extracted from regularly scheduled reference performance tests (i.e., low rate full cycles) during cycling. The early-life features capture a cell's state of health and the rate of change of component-level degradation modes, some of which correlate strongly with cell lifetime. Using a newly generated dataset from 225 nickel-manganese-cobalt/graphite Li-ion cells aged under a wide range of conditions, we demonstrate a lifetime prediction of in-distribution cells with 15.1% mean absolute percentage error using no more than the first 15% of data
    
[^145]: 重新审视隐式模型：在视觉任务中权重绑定模型的稀疏度权衡能力

    Revisiting Implicit Models: Sparsity Trade-offs Capability in Weight-tied Model for Vision Tasks. (arXiv:2307.08013v1 [cs.LG])

    [http://arxiv.org/abs/2307.08013](http://arxiv.org/abs/2307.08013)

    本研究重新审视了隐式模型，并发现权重绑定模型在视觉任务中比DEQ变体更有效、稳定和高效。通过使用不同的稀疏掩模，我们提出了提高模型容量的方法。

    

    隐式模型如深度平衡模型（Deep Equilibrium Models, DEQs）因其能够用优雅的解决方案和恒定的内存占用训练无限层模型而引起了研究者的广泛关注。然而，尽管已经做出了几次尝试，这些方法仍然受到模型低效和优化不稳定性的严重限制。此外，对于视觉任务，缺乏相关方法的公平基准评估。在本研究中，我们重新审视了隐式模型的发展，并将其追溯到原始的权重绑定模型。令人惊讶的是，我们观察到与DEQ变体相比，权重绑定模型在视觉任务上更加有效、稳定和高效。通过这些简单而清晰的权重绑定模型，我们进一步研究了这种模型容量的基本限制，并提出了使用不同稀疏掩模来提高模型容量的方法。最后，我们为从业人员提供了关于深度、宽度和稀疏度的设计指南。

    Implicit models such as Deep Equilibrium Models (DEQs) have garnered significant attention in the community for their ability to train infinite layer models with elegant solution-finding procedures and constant memory footprint. However, despite several attempts, these methods are heavily constrained by model inefficiency and optimization instability. Furthermore, fair benchmarking across relevant methods for vision tasks is missing. In this work, we revisit the line of implicit models and trace them back to the original weight-tied models. Surprisingly, we observe that weight-tied models are more effective, stable, as well as efficient on vision tasks, compared to the DEQ variants. Through the lens of these simple-yet-clean weight-tied models, we further study the fundamental limits in the model capacity of such models and propose the use of distinct sparse masks to improve the model capacity. Finally, for practitioners, we offer design guidelines regarding the depth, width, and spars
    
[^146]: 云游戏中的神经视频恢复

    Neural Video Recovery for Cloud Gaming. (arXiv:2307.07847v1 [cs.NI])

    [http://arxiv.org/abs/2307.07847](http://arxiv.org/abs/2307.07847)

    本文提出了一个新方法，用于在云游戏中恢复丢失或损坏的视频帧。与传统方法不同的是，我们利用游戏状态和部分解码帧来提高恢复准确性。

    

    云游戏是一个价值数十亿美元的行业。在云游戏中，客户端将自己的移动发送到互联网上的游戏服务器，服务器将渲染并传输结果视频回来。为了提供良好的游戏体验，需要低于80毫秒的延迟。这意味着视频的渲染、编码、传输、解码和显示必须在这个时间范围内完成，由于服务器过载、网络拥塞和丢包等因素，这一点特别具有挑战性。在本文中，我们提出了一种在云游戏中恢复丢失或损坏视频帧的新方法。与传统视频帧恢复不同，我们的方法利用游戏状态显著提升恢复准确性，并利用部分解码的帧来恢复丢失的部分。我们开发了一个综合性的系统，包括(i)高效提取游戏状态，(ii)修改 H.264 视频解码器生成一个指示需要恢复视频帧哪些部分的掩码，和 (iii)设计一个新颖的神经网络进行视频帧恢复。

    Cloud gaming is a multi-billion dollar industry. A client in cloud gaming sends its movement to the game server on the Internet, which renders and transmits the resulting video back. In order to provide a good gaming experience, a latency below 80 ms is required. This means that video rendering, encoding, transmission, decoding, and display have to finish within that time frame, which is especially challenging to achieve due to server overload, network congestion, and losses. In this paper, we propose a new method for recovering lost or corrupted video frames in cloud gaming. Unlike traditional video frame recovery, our approach uses game states to significantly enhance recovery accuracy and utilizes partially decoded frames to recover lost portions. We develop a holistic system that consists of (i) efficiently extracting game states, (ii) modifying H.264 video decoder to generate a mask to indicate which portions of video frames need recovery, and (iii) designing a novel neural networ
    
[^147]: 弹性决策变压器

    Elastic Decision Transformer. (arXiv:2307.02484v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.02484](http://arxiv.org/abs/2307.02484)

    弹性决策变压器（EDT）通过在测试时间进行动作推断时调整历史长度来实现轨迹拼接，填补了决策变压器（DT）在这一方面的性能差距，并且在多任务情况下胜过基于Q-Learning的方法。

    

    本文介绍了弹性决策变压器（EDT），它是现有决策变压器（DT）及其变体的重大进展。尽管DT声称能够生成最佳轨迹，但实证证据表明它在轨迹拼接方面存在困难，轨迹拼接是指从一组次优轨迹中生成最优或接近最优轨迹的过程。提出的EDT通过在测试时间进行动作推断时调整DT中维护的历史长度来实现轨迹拼接，从而使自己与众不同。此外，当前轨迹是最优的时候，EDT通过保持较长的历史，当当前轨迹是次优的时候，EDT通过保持较短的历史来优化轨迹，使其能够与更优的轨迹进行“拼接”。广泛的实验表明，EDT能够填补基于DT和基于Q-Learning方法之间的性能差距。特别是，EDT在多任务情况下胜过基于Q-Learning的方法。

    This paper introduces Elastic Decision Transformer (EDT), a significant advancement over the existing Decision Transformer (DT) and its variants. Although DT purports to generate an optimal trajectory, empirical evidence suggests it struggles with trajectory stitching, a process involving the generation of an optimal or near-optimal trajectory from the best parts of a set of sub-optimal trajectories. The proposed EDT differentiates itself by facilitating trajectory stitching during action inference at test time, achieved by adjusting the history length maintained in DT. Further, the EDT optimizes the trajectory by retaining a longer history when the previous trajectory is optimal and a shorter one when it is sub-optimal, enabling it to "stitch" with a more optimal trajectory. Extensive experimentation demonstrates EDT's ability to bridge the performance gap between DT-based and Q Learning-based approaches. In particular, the EDT outperforms Q Learning-based methods in a multi-task regi
    
[^148]: Act3D：无限分辨率的机器人操作检测Transformer

    Act3D: Infinite Resolution Action Detection Transformer for Robotic Manipulation. (arXiv:2306.17817v1 [cs.RO])

    [http://arxiv.org/abs/2306.17817](http://arxiv.org/abs/2306.17817)

    Act3D是一种基于Transformer的操作策略，将6自由度关键姿势预测作为3D检测任务，并以自适应空间计算的方式进行处理。它在高度精确的机器人操纵任务中取得了显著的性能改进。

    

    3D感知表征非常适用于机器人操纵，因为它们可以轻松编码遮挡情况并简化空间推理。许多操纵任务需要对末端执行器姿势预测进行高空间精度，通常需要高分辨率的3D感知网格进行计算，这在处理上非常耗时。因此，大多数操作策略直接在2D中运作，放弃了3D的归纳偏差。本文提出了Act3D，一种将6自由度关键姿势预测视为自适应空间计算的操作策略Transformer。它以一个或多个摄像机视图的未投影3D特征云作为输入，以粗-精方式在自由空间中迭代采样3D点网格，使用相对空间注意力将其特征化为物理特征云，并选择最佳特征点进行末端执行器姿势预测。Act3D在已建立的操纵基准RLbench中取得了最新的最好成绩。我们的模型在该基准中实现了10%的绝对改进。

    3D perceptual representations are well suited for robot manipulation as they easily encode occlusions and simplify spatial reasoning. Many manipulation tasks require high spatial precision in end-effector pose prediction, typically demanding high-resolution 3D perceptual grids that are computationally expensive to process. As a result, most manipulation policies operate directly in 2D, foregoing 3D inductive biases. In this paper, we propose Act3D, a manipulation policy Transformer that casts 6-DoF keypose prediction as 3D detection with adaptive spatial computation. It takes as input 3D feature clouds unprojected from one or more camera views, iteratively samples 3D point grids in free space in a coarse-to-fine manner, featurizes them using relative spatial attention to the physical feature cloud, and selects the best feature point for end-effector pose prediction. Act3D sets a new state-of-the-art in RLbench, an established manipulation benchmark. Our model achieves 10% absolute impr
    
[^149]: CoarsenConf: 基于聚合注意力的分子构象生成的等变粗化方法

    CoarsenConf: Equivariant Coarsening with Aggregated Attention for Molecular Conformer Generation. (arXiv:2306.14852v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.14852](http://arxiv.org/abs/2306.14852)

    CoarsenConf是一种用于分子构象生成的等变粗化方法，通过聚合注意机制恢复从粗粒度表示中得到的细粒度原子坐标，实现了高效准确的构象生成。

    

    分子构象生成是化学信息学和药物发现中的重要任务。能够高效生成低能量的三维结构可以避免昂贵的量子力学模拟，从而加速虚拟筛选和增强结构探索。已经开发了多种用于分子构象生成的生成模型，但很多模型难以持续产生高质量的构象。为了解决这些问题，我们引入了CoarsenConf，它基于扭转角度进行分子图的粗化，并将其整合到SE(3)-等变分层变分自动编码器中。通过等变的粗化，我们聚合了通过可转动键连接的子图的细粒度原子坐标，创建了一个可变长度的粗粒度潜在表示。我们的模型使用一种新颖的聚合注意机制，从粗粒度的潜在表示中恢复细粒度的坐标，实现了高效准确的构象生成。

    Molecular conformer generation (MCG) is an important task in cheminformatics and drug discovery. The ability to efficiently generate low-energy 3D structures can avoid expensive quantum mechanical simulations, leading to accelerated virtual screenings and enhanced structural exploration. Several generative models have been developed for MCG, but many struggle to consistently produce high-quality conformers. To address these issues, we introduce CoarsenConf, which coarse-grains molecular graphs based on torsional angles and integrates them into an SE(3)-equivariant hierarchical variational autoencoder. Through equivariant coarse-graining, we aggregate the fine-grained atomic coordinates of subgraphs connected via rotatable bonds, creating a variable-length coarse-grained latent representation. Our model uses a novel aggregated attention mechanism to restore fine-grained coordinates from the coarse-grained latent representation, enabling efficient generation of accurate conformers. Furth
    
[^150]: FuXi: 一个15天全球天气预报级联机器学习系统

    FuXi: A cascade machine learning forecasting system for 15-day global weather forecast. (arXiv:2306.12873v1 [physics.ao-ph])

    [http://arxiv.org/abs/2306.12873](http://arxiv.org/abs/2306.12873)

    逐步级联模型FuXi在15天全球天气预报中表现出更好的性能，并通过减少预测误差的积累达到优化。

    

    近年来，随着机器学习模型在天气预报中的快速发展，最先进的机器学习模型在0.25度空间分辨率下的10天天气预报中已经表现出比欧洲中期天气预报中心(ECMWF)的高分辨率预报(HRES)更优越的性能。然而，挑战在于在15天预报中表现与ECMWF集合平均(EM)相当。以前的研究表明，缓解预报误差的积累对于有效的长期预报非常重要。尽管有许多减少积累误差的努力，包括自回归多时间步长损失，但使用单个模型发现无法在短和长导出时间上达到最佳性能。因此，我们提出了FuXi，这是一个级联机器学习天气预测系统，提供了分辨率为0.25度、时间分辨率为6小时的15天全球预测。FuXi基于级联集合模型开发，它集成了多种模型的优势，并减少了预测误差的积累。使用空气温度，比湿度和位势高度的均方根误差(RMSE)和异常相关系数(ACC)评估了FuXi的性能。结果表明，与ECMWF HRES相比，FuXi在15天预报中表现出更好的性能，并显著减少了积累误差。

    Over the past few years, due to the rapid development of machine learning (ML) models for weather forecasting, state-of-the-art ML models have shown superior performance compared to the European Centre for Medium-Range Weather Forecasts (ECMWF)'s high-resolution forecast (HRES) in 10-day forecasts at a spatial resolution of 0.25 degree. However, the challenge remains to perform comparably to the ECMWF ensemble mean (EM) in 15-day forecasts. Previous studies have demonstrated the importance of mitigating the accumulation of forecast errors for effective long-term forecasts. Despite numerous efforts to reduce accumulation errors, including autoregressive multi-time step loss, using a single model is found to be insufficient to achieve optimal performance in both short and long lead times. Therefore, we present FuXi, a cascaded ML weather forecasting system that provides 15-day global forecasts with a temporal resolution of 6 hours and a spatial resolution of 0.25 degree. FuXi is develope
    
[^151]: Otter-Knowledge：不同来源的多模态知识图谱表示学习在药物发现中的基准测试。

    Otter-Knowledge: benchmarks of multimodal knowledge graph representation learning from different sources for drug discovery. (arXiv:2306.12802v1 [cs.LG])

    [http://arxiv.org/abs/2306.12802](http://arxiv.org/abs/2306.12802)

    本研究在药物发现方面使用了多模态知识图谱表示学习，获得了最先进的结果。他们整合了来自不同来源的数据，并提供了基于这些数据的预训练模型。

    

    最近，表示学习的研究利用大量的蛋白质或分子数据库，通过无监督学习技术获得药物和蛋白质结构的知识。这些预训练表示已被证明可以显著提高后续任务的准确性，如预测药物和靶蛋白之间的亲和力。在本研究中，我们展示了通过将来自不同来源和模态的知识图谱整合到序列或SMILES表示中，可以进一步丰富表示，并在已建立的基准测试数据集上实现最先进的结果。我们提供了来自7个公共来源的预处理和整合数据，其中包括超过30M个三元组。此外，我们还提供了基于这些数据的预训练模型，以及它们在Therapeutic Data Commons (TDC)基准测试中性能报告的结果。

    Recent research in representation learning utilizes large databases of proteins or molecules to acquire knowledge of drug and protein structures through unsupervised learning techniques. These pre-trained representations have proven to significantly enhance the accuracy of subsequent tasks, such as predicting the affinity between drugs and target proteins. In this study, we demonstrate that by incorporating knowledge graphs from diverse sources and modalities into the sequences or SMILES representation, we can further enrich the representation and achieve state-of-the-art results on established benchmark datasets. We provide preprocessed and integrated data obtained from 7 public sources, which encompass over 30M triples. Additionally, we make available the pre-trained models based on this data, along with the reported outcomes of their performance on three widely-used benchmark datasets for drug-target binding affinity prediction found in the Therapeutic Data Commons (TDC) benchmarks.
    
[^152]: 训练好的Transformer在上下文中学习线性模型

    Trained Transformers Learn Linear Models In-Context. (arXiv:2306.09927v1 [stat.ML])

    [http://arxiv.org/abs/2306.09927](http://arxiv.org/abs/2306.09927)

    本文研究了Transformer在具有单层线性自注意层的线性回归任务上通过梯度流进行训练的ICL机制，揭示了梯度流具有找到目标函数全局最小值的能力。

    

    基于注意力的神经网络，例如Transformers，在上下文学习（ICL）方面表现出了非凡的能力：给定一个来自未见过的任务的短语序列的提示，它们可以制定相关的每个令牌和下一个令牌的预测，而不需要任何参数更新。通过将标记的训练数据和未标记的测试数据序列嵌入到提示中，这使得Transformer表现得像有监督学习算法。事实上，最近的工作表明，在随机实例上训练Transformer体系结构的线性回归问题时，这些模型的预测会模仿普通最小二乘法的预测。

    Attention-based neural networks such as transformers have demonstrated a remarkable ability to exhibit in-context learning (ICL): Given a short prompt sequence of tokens from an unseen task, they can formulate relevant per-token and next-token predictions without any parameter updates. By embedding a sequence of labeled training data and unlabeled test data as a prompt, this allows for transformers to behave like supervised learning algorithms. Indeed, recent work has shown that when training transformer architectures over random instances of linear regression problems, these models' predictions mimic those of ordinary least squares.  Towards understanding the mechanisms underlying this phenomenon, we investigate the dynamics of ICL in transformers with a single linear self-attention layer trained by gradient flow on linear regression tasks. We show that despite non-convexity, gradient flow with a suitable random initialization finds a global minimum of the objective function. At this 
    
[^153]: 一种基于原始-对偶-评论家算法的离线约束强化学习

    A Primal-Dual-Critic Algorithm for Offline Constrained Reinforcement Learning. (arXiv:2306.07818v1 [cs.LG])

    [http://arxiv.org/abs/2306.07818](http://arxiv.org/abs/2306.07818)

    PDCA是一种用于离线约束强化学习的算法，它可以通过在Lagrangian函数上运行原始-对偶算法来找到近似鞍点，而无需集中性和强Bellman完备性假设。

    

    离线约束强化学习旨在学习一种策略，以在现有数据集上满足对成本函数期望值的限制条件下最大化预期的累积奖励。本文提出了一种称为原始-对偶-评论家算法（PDCA）的新算法，用于具有一般函数逼近的离线约束强化学习。PDCA在评论家估计的Lagrangian函数上运行原始-对偶算法。原始玩家采用无悔策略优化神谕，在给定任何评论家和对偶玩家的选择的情况下最大化拉格朗日函数的估计。对偶玩家通过采用无悔在线线性优化神谕，在给定评论家和原始玩家的任何选择的情况下最小化拉格朗日函数的估计。我们展示了PDCA可以成功地找到拉格朗日函数的近似鞍点，这对于约束强化学习问题几乎是最优的。与以前需要集中性和强Bellman完备性假设的作品不同，PDCA只需要一致性和自闭性这两个假设。

    Offline constrained reinforcement learning (RL) aims to learn a policy that maximizes the expected cumulative reward subject to constraints on expected value of cost functions using an existing dataset. In this paper, we propose Primal-Dual-Critic Algorithm (PDCA), a novel algorithm for offline constrained RL with general function approximation. PDCA runs a primal-dual algorithm on the Lagrangian function estimated by critics. The primal player employs a no-regret policy optimization oracle to maximize the Lagrangian estimate given any choices of the critics and the dual player. The dual player employs a no-regret online linear optimization oracle to minimize the Lagrangian estimate given any choices of the critics and the primal player. We show that PDCA can successfully find a near saddle point of the Lagrangian, which is nearly optimal for the constrained RL problem. Unlike previous work that requires concentrability and strong Bellman completeness assumptions, PDCA only requires co
    
[^154]: 鲁棒性神经架构搜索中的可推广轻量级代理

    Generalizable Lightweight Proxy for Robust NAS against Diverse Perturbations. (arXiv:2306.05031v1 [cs.LG])

    [http://arxiv.org/abs/2306.05031](http://arxiv.org/abs/2306.05031)

    本研究提出了一种轻量级的、零成本代理，通过考虑干净图像和受扰动图像的特征、参数和梯度的一致性，实现了高效、快速地搜索鲁棒NAS架构。

    

    最近的神经架构搜索（NAS）框架已成功在给定条件下（例如性能或延迟）找到最佳架构。然而，它们只针对干净图像的性能寻找最佳架构，而在实践中，对于各种类型的扰动或损坏的鲁棒性非常重要。尽管存在几个通过将对抗性训练集成到单次NAS中来解决这个问题的鲁棒性NAS框架，但它们的局限性在于它们只考虑对抗攻击的鲁棒性，并需要大量计算资源才能为单个任务发现最佳架构，这使它们在实际应用中不切实际。为了解决这些挑战，我们提出了一种新颖的轻量级鲁棒零成本代理，该代理在初始化状态下考虑干净图像和受扰动图像的特征、参数和梯度的一致性。我们的方法有助于快速、高效地搜索鲁棒NAS架构。

    Recent neural architecture search (NAS) frameworks have been successful in finding optimal architectures for given conditions (e.g., performance or latency). However, they search for optimal architectures in terms of their performance on clean images only, while robustness against various types of perturbations or corruptions is crucial in practice. Although there exist several robust NAS frameworks that tackle this issue by integrating adversarial training into one-shot NAS, however, they are limited in that they only consider robustness against adversarial attacks and require significant computational resources to discover optimal architectures for a single task, which makes them impractical in real-world scenarios. To address these challenges, we propose a novel lightweight robust zero-cost proxy that considers the consistency across features, parameters, and gradients of both clean and perturbed images at the initialization state. Our approach facilitates an efficient and rapid sea
    
[^155]: 推理时间干预：从语言模型中引导出真实的答案

    Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. (arXiv:2306.03341v1 [cs.LG])

    [http://arxiv.org/abs/2306.03341](http://arxiv.org/abs/2306.03341)

    本研究提出推理时间干预（ITI）技术，通过在推理过程中跨越有限数量的注意力头，显着提高大型语言模型的真实性。在TruthfulQA基准上，ITI使LLaMA模型的真实性从32.5%提高到65.1%。ITI是一种最小程度的干扰，计算廉价，且数据效率高。

    

    我们介绍了推理时间干预（ITI）技术，旨在增强大型语言模型（LLMs）的真实性。ITI通过在推理过程中沿着一组方向移动模型激活，跨越有限数量的注意力头。这种干预显着提高了LLaMA模型在TruthfulQA基准上的表现。在指令微调的LLaMA Alpaca上，ITI将其真实性从32.5％提高到65.1％。我们确定了真实性和可用性之间的权衡，并演示了如何通过调整干预强度来平衡它。ITI 取得了最低程度的干扰且计算廉价。此外，该技术在数据效率上表现优异：虽然像RLHF这样的方法需要广泛注释，但是ITI仅使用了几百个例子就能定位真实的方向。我们的研究结果表明，LLMs可能具有某种内部表示方法来表示某事是真实的可能性，即使它们在表面上产生了虚假的结果。

    We introduce Inference-Time Intervention (ITI), a technique designed to enhance the truthfulness of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.
    
[^156]: 无偏压缩在分布式优化中节省通信：何时以及多少？

    Unbiased Compression Saves Communication in Distributed Optimization: When and How Much?. (arXiv:2305.16297v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.16297](http://arxiv.org/abs/2305.16297)

    本文研究了分布式优化中通信压缩的效果，探讨了无偏压缩降低总通信成本的条件和程度。

    

    通信压缩是在分布式优化中常用的技术，可以通过传输压缩梯度和模型参数来减少通信开销。然而，压缩会引入信息失真，导致收敛减慢并增加通信轮次以达到期望解。鉴于每轮通信成本的降低和额外的通信轮次之间的权衡，尚不清楚通信压缩是否降低了总通信成本。本文探讨了无偏压缩，一种广泛使用的压缩形式，在什么条件下可以降低总通信成本，以及在多大程度上可以降低。为此，我们提出了第一个理论框架来表征具有通信压缩的分布式优化中的总通信成本。我们证明了仅仅使用无偏压缩并不能一定节省总通信成本，但这种结果有

    Communication compression is a common technique in distributed optimization that can alleviate communication overhead by transmitting compressed gradients and model parameters. However, compression can introduce information distortion, which slows down convergence and incurs more communication rounds to achieve desired solutions. Given the trade-off between lower per-round communication costs and additional rounds of communication, it is unclear whether communication compression reduces the total communication cost.  This paper explores the conditions under which unbiased compression, a widely used form of compression, can reduce the total communication cost, as well as the extent to which it can do so. To this end, we present the first theoretical formulation for characterizing the total communication cost in distributed optimization with communication compression. We demonstrate that unbiased compression alone does not necessarily save the total communication cost, but this outcome c
    
[^157]: 语言模型的分词器在不同语言之间引入了不公平现象

    Language Model Tokenizers Introduce Unfairness Between Languages. (arXiv:2305.15425v1 [cs.CL])

    [http://arxiv.org/abs/2305.15425](http://arxiv.org/abs/2305.15425)

    语言模型的分词器在不同语言之间引入了不公平现象，因为同一段文本翻译成不同的语言可能会导致极大的分词长度差异，这影响了一些语言社区在获取商业语言服务的成本、处理时间和延迟以及提供给机器学习模型的内容量方面存在不公平待遇。

    

    最近的语言模型表现出了惊人的多语言性能，即使没有明确为此进行过训练。尽管如此，人们对它们在不同语言之间的输出质量提出了担忧。在本文中，我们展示了在分词阶段出现了不同语言的处理差异，甚至在模型被调用之前就已经出现了。同一段文本翻译成不同的语言可以有极大的分词长度差异，有些情况下差异可高达15倍。这些差异在我们评估的17种分词器中仍然存在，即使它们是有意为多语言支持进行训练的。某些语言对的字符级和字节级模型也显示出4倍以上的编码长度差异。这导致了一些语言社区在获取商业语言服务的成本、处理时间和延迟以及提供给机器学习模型的内容量方面存在不公平待遇。

    Recent language models have shown impressive multilingual performance, even when not explicitly trained for it. Despite this, concerns have been raised about the quality of their outputs across different languages. In this paper, we show how disparity in the treatment of different languages arises at the tokenization stage, well before a model is even invoked. The same text translated into different languages can have drastically different tokenization lengths, with differences up to 15 times in some cases. These disparities persist across the 17 tokenizers we evaluate, even if they are intentionally trained for multilingual support. Character-level and byte-level models also exhibit over 4 times the difference in the encoding length for some language pairs. This induces unfair treatment for some language communities in regard to the cost of accessing commercial language services, the processing time and latency, as well as the amount of content that can be provided as context to the m
    
[^158]: 带有语音的LM：超越语音令牌的口语语言建模

    LMs with a Voice: Spoken Language Modeling beyond Speech Tokens. (arXiv:2305.15255v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.15255](http://arxiv.org/abs/2305.15255)

    SPECTRON是一个新颖的语音延续模型，通过利用预训练的语言模型和语音编码器进行端到端的训练来生成文本和语音输出，在语义内容和讲话者保护方面超越了现有的口语语言模型。

    

    我们提出了SPECTRON，一种新颖的方法来适应预训练的语言模型（LM）以执行语音延续。通过利用预训练的语音编码器，我们的模型可以生成文本和语音输出，整个系统都在频谱图上进行端到端的训练。在频谱图领域训练整个模型相对于使用离散语音表示的现有级联方法简化了我们的语音延续系统。我们进一步展示了我们的方法在语义内容和讲话者保护方面超过了现有的口语语言模型，同时也从预先存在的模型中获得了知识传递的好处。我们的网站https://michelleramanovich.github.io/spectron/spectron上可以找到音频样本。

    We present SPECTRON, a novel approach to adapting pre-trained language models (LMs) to perform speech continuation. By leveraging pre-trained speech encoders, our model generates both text and speech outputs with the entire system being trained end-to-end operating directly on spectrograms. Training the entire model in the spectrogram domain simplifies our speech continuation system versus existing cascade methods which use discrete speech representations. We further show our method surpasses existing spoken language models both in semantic content and speaker preservation while also benefiting from the knowledge transferred from pre-existing models. Audio samples can be found in our website https://michelleramanovich.github.io/spectron/spectron
    
[^159]: 使用因果中介分析解释语言模型中的算术推理

    A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis. (arXiv:2305.15054v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.15054](http://arxiv.org/abs/2305.15054)

    本研究通过因果中介分析框架对基于Transformer的语言模型在算术问题上进行了机制解释，发现语言模型通过注意机制传输与查询相关的信息，并通过一组MLP模块进行处理。

    

    近期的研究对大型语言模型中的数学推理引起了重要关注，但对于这些模型如何处理和存储与算术任务相关的信息的理解还很有限。为了加深我们对语言模型这一方面的理解，我们提出了使用因果中介分析框架对基于Transformer的语言模型在算术问题上进行机制解释。通过对特定模型组件的激活进行干预并测量预测概率的变化，我们确定了特定预测所负责的参数子集。这为我们了解语言模型如何处理与算术相关的信息提供了见解。实验结果表明，语言模型通过使用注意机制将与查询相关的信息从中间层传输到最终的令牌，然后通过一组MLP模块处理这些信息。

    Mathematical reasoning in large language models (LMs) has garnered significant attention in recent work, but there is a limited understanding of how these models process and store information related to arithmetic tasks within their architecture. In order to improve our understanding of this aspect of language models, we present a mechanistic interpretation of Transformer-based LMs on arithmetic questions using a causal mediation analysis framework. By intervening on the activations of specific model components and measuring the resulting changes in predicted probabilities, we identify the subset of parameters responsible for specific predictions. This provides insights into how information related to arithmetic is processed by LMs. Our experimental results indicate that LMs process the input by transmitting the information relevant to the query from mid-sequence early layers to the final token using the attention mechanism. Then, this information is processed by a set of MLP modules, 
    
[^160]: Newton-Cotes图神经网络：论动态系统的时间演化

    Newton-Cotes Graph Neural Networks: On the Time Evolution of Dynamic Systems. (arXiv:2305.14642v1 [cs.LG])

    [http://arxiv.org/abs/2305.14642](http://arxiv.org/abs/2305.14642)

    本文提出了基于Newton-Cotes公式的方法来预测动态系统的时间演化，与现有最先进的方法相比较， 实验结果表明该方法有着显著的改进。

    

    研究系统动态性是许多科学研究中最重要的分析方法之一。使用系统的初始状态作为输入，最近基于图神经网络（GNNs）的方法能够高精度地预测远离初始状态的未来状态。虽然这些方法在建模系统的坐标和相互作用力方面有不同的设计，但我们发现它们实际上共享一种常数的积分学习范例， 能够在初始和终端坐标之间的时间间隔内预测速度的积分。受此观察的启发，我们提出了一种新的方法，基于用Newton-Cotes公式估算的若干速度估计来预测积分，并理论上证明其有效性。 在几个基准实验上，我们发现该方法与现有最先进的方法相比有着持续且显著的改进。

    Reasoning system dynamics is one of the most important analytical approaches for many scientific studies. With the initial state of a system as input, the recent graph neural networks (GNNs)-based methods are capable of predicting the future state distant in time with high accuracy. Although these methods have diverse designs in modeling the coordinates and interacting forces of the system, we show that they actually share a common paradigm that learns the integration of the velocity over the interval between the initial and terminal coordinates. However, their integrand is constant w.r.t. time. Inspired by this observation, we propose a new approach to predict the integration based on several velocity estimations with Newton-Cotes formulas and prove its effectiveness theoretically. Extensive experiments on several benchmarks empirically demonstrate consistent and significant improvement compared with the state-of-the-art methods.
    
[^161]: 学习从兼容标签序列中的语义角色标注

    Learning Semantic Role Labeling from Compatible Label Sequences. (arXiv:2305.14600v1 [cs.CL])

    [http://arxiv.org/abs/2305.14600](http://arxiv.org/abs/2305.14600)

    该论文探讨了如何从不相交的兼容标签序列中高效地学习，将此运用于语义角色标注任务，提出了联合处理VerbNet和PropBank标签的方法，并验证了其有效性。

    

    本文探讨了如何高效地学习从不相交的兼容标签序列中标注的问题。我们认为，不相交标签集之间的兼容结构有助于模型的学习和推理。我们在语义角色标注（SRL）任务中验证了这一假设，具体地，标记具有两个角色序列的句子：VerbNet参数和PropBank参数。先前的研究已经表明跨任务交互可以提高性能。但是，这两个任务仍然是分别解码的，存在生成结构不一致的标签序列 (在像SEMLINK的词典中)的风险。为了消除这个问题，我们首先提出了一个简单而有效的设置，联合处理VerbNet和PropBank标签作为一个序列。通过这个设置，我们证明了在解码过程中强制执行SEMLINK约束不断提高总F1值。通过特殊的输入构造，我们的联合模型可以以超过99%的准确性从PropBank参数中推断出VerbNet参数。我们还提出了一种co

    This paper addresses the question of how to efficiently learn from disjoint, compatible label sequences. We argue that the compatible structures between disjoint label sets help model learning and inference. We verify this hypothesis on the task of semantic role labeling (SRL), specifically, tagging a sentence with two role sequences: VerbNet arguments and PropBank arguments. Prior work has shown that cross-task interaction improves performance. However, the two tasks are still separately decoded, running the risk of generating structurally inconsistent label sequences (as per lexicons like SEMLINK). To eliminate this issue, we first propose a simple and effective setup that jointly handles VerbNet and PropBank labels as one sequence. With this setup, we show that enforcing SEMLINK constraints during decoding constantly improves the overall F1. With special input constructions, our joint model infers VerbNet arguments from PropBank arguments with over 99% accuracy. We also propose a co
    
[^162]: 下一个会是什么？评估神经文本生成器的不确定性与人类生产变异性对比

    What Comes Next? Evaluating Uncertainty in Neural Text Generators Against Human Production Variability. (arXiv:2305.11707v1 [cs.CL])

    [http://arxiv.org/abs/2305.11707](http://arxiv.org/abs/2305.11707)

    本文分析了神经文本生成器与人类生产变异性之间的不确定性，通过探测生成器的输出空间来测量其对人类生产变异性的校准程度，并证明用多个样本和多个参考可以更好地了解模型的不确定性表示。

    

    在自然语言生成（NLG）任务中，针对任何输入，存在多个可行的交际目标，并且可以用多种方式将任何目标用语言表达出来或进行生产。我们表征了人类生产在四个NLG任务中词汇、句法和语义方面的变异程度，并将人类生产变异性与不确定性联系起来。然后，我们检查了生成系统预测的概率分布和解码算法所形成的输出字符串空间，以探究其不确定性。针对每个测试输入，我们测量了生成器对人类生产变异性的校准程度。通过这种基于实例级别的方法，我们分析了NLG模型和解码策略，证明用多个样本和多个参考对生成器进行探测，提供了理解模型不确定性表示所必需的详细级别。

    In Natural Language Generation (NLG) tasks, for any input, multiple communicative goals are plausible, and any goal can be put into words, or produced, in multiple ways. We characterise the extent to which human production varies lexically, syntactically, and semantically across four NLG tasks, connecting human production variability to aleatoric or data uncertainty. We then inspect the space of output strings shaped by a generation system's predicted probability distribution and decoding algorithm to probe its uncertainty. For each test input, we measure the generator's calibration to human production variability. Following this instance-level approach, we analyse NLG models and decoding strategies, demonstrating that probing a generator with multiple samples and, when possible, multiple references, provides the level of detail necessary to gain understanding of a model's representation of uncertainty.
    
[^163]: Tinto：地球科学中3D高光谱点云分割的多传感器基准测试

    Tinto: Multisensor Benchmark for 3D Hyperspectral Point Cloud Segmentation in the Geosciences. (arXiv:2305.09928v1 [cs.CV])

    [http://arxiv.org/abs/2305.09928](http://arxiv.org/abs/2305.09928)

    Tinto是一个多传感器数字露头基准测试数据集，旨在促进开发和验证地质制图的深度学习方法，特别是针对非结构化的3D数据如点云。

    

    深度学习技术的使用减少了地质图的解释时间，并通过从数字露头模型自动派生地质图来理想地减少了解释者偏见。然而，由于地质图解释的主观性以及收集定量验证数据的困难，对这些自动化制图方法的准确验证是一项重大挑战。此外，许多最先进的深度学习方法仅限于2D图像数据，这对于诸如超级云这样的3D数字露头是不足够的。为了解决这些挑战，我们提出了Tinto，这是一个多传感器数字露头基准测试数据集，旨在促进开发和验证地质制图的深度学习方法，特别是针对非结构化的3D数据如点云。Tinto包括两个互补的数据集：1）来自Corta Atalaya（西班牙）的真实数字露头模型，具有光谱属性和地面真实数据，以及2）使用潜在变量的合成数据集，以模拟真实数据的各种变化和不确定性。

    The increasing use of deep learning techniques has reduced interpretation time and, ideally, reduced interpreter bias by automatically deriving geological maps from digital outcrop models. However, accurate validation of these automated mapping approaches is a significant challenge due to the subjective nature of geological mapping and the difficulty in collecting quantitative validation data. Additionally, many state-of-the-art deep learning methods are limited to 2D image data, which is insufficient for 3D digital outcrops, such as hyperclouds. To address these challenges, we present Tinto, a multi-sensor benchmark digital outcrop dataset designed to facilitate the development and validation of deep learning approaches for geological mapping, especially for non-structured 3D data like point clouds. Tinto comprises two complementary sets: 1) a real digital outcrop model from Corta Atalaya (Spain), with spectral attributes and ground-truth data, and 2) a synthetic twin that uses latent
    
[^164]: INGENIOUS：使用信息丰富的数据子集对大型语言模型进行高效预训练

    INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Large Language Models. (arXiv:2305.06677v1 [cs.CL])

    [http://arxiv.org/abs/2305.06677](http://arxiv.org/abs/2305.06677)

    本文提出了一种使用信息丰富的数据子集来高效预训练大型语言模型的方法，减少了训练时间和计算成本，同时保持了模型的泛化能力。

    

    大型预训练语言模型的显着特点是在其泛化能力和新能力方面随着模型容量和预训练数据集大小的增加而 achieved. 然而，必须认识到这不可避免地导致了过长的训练时间、过高的计算成本和有害的环境影响。本文提出了一种方法，即是否可能仅使用高度信息丰富的训练数据子集来训练 PTLM，并同时保持其下游性能？

    A salient characteristic of large pre-trained language models (PTLMs) is a remarkable improvement in their generalization capability and emergence of new capabilities with increasing model capacity and pre-training dataset size. Consequently, we are witnessing the development of enormous models pushing the state-of-the-art. It is, however, imperative to realize that this inevitably leads to prohibitively long training times, extortionate computing costs, and a detrimental environmental impact. Significant efforts are underway to make PTLM training more efficient through innovations in model architectures, training pipelines, and loss function design, with scant attention being paid to optimizing the utility of training data. The key question that we ask is whether it is possible to train PTLMs by employing only highly informative subsets of the training data while maintaining downstream performance? Building upon the recent progress in informative data subset selection, we show how we 
    
[^165]: 人工智能模型筛查乳腺 X 光片的性能差距 -- 迈向公平和可解释的模型

    Performance Gaps of Artificial Intelligence Models Screening Mammography -- Towards Fair and Interpretable Models. (arXiv:2305.04422v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2305.04422](http://arxiv.org/abs/2305.04422)

    该研究探讨了乳腺筛查 X 光片异常分类模型中的性能差距，尤其是与人口学和成像特征之间的关系，旨在开发公平和可解释的模型。

    

    尽管深度学习模型在筛查乳腺 X 光片的异常分类中表现良好，与增加异常分类失败风险相关的人口学和成像特征仍不清楚。本回顾性研究使用 Emory BrEast Imaging Dataset（EMBED）的数据，包括2013年至2020年间 Emory University Healthcare 的 115,931 名患者的乳腺 X 光片。临床和成像数据包括乳腺成像报告和数据系统（BI-RADS）评估，异常区域的兴趣点坐标，成像特征，病理结果和患者人口统计学。开发了 InceptionV3、VGG16、ResNet50V2 和 ResNet152V2 等深度学习模型，用于区分筛查乳腺 X 光片中异常组织区域和随机选择的正常组织区域。训练集、验证集和测试集的分布为 29,144（55.6%）个异常组织区域和来自 10,678（54.2%）位患者的膨胀组织区域。

    Even though deep learning models for abnormality classification can perform well in screening mammography, the demographic and imaging characteristics associated with increased risk of failure for abnormality classification in screening mammograms remain unclear. This retrospective study used data from the Emory BrEast Imaging Dataset (EMBED) including mammograms from 115,931 patients imaged at Emory University Healthcare between 2013 to 2020. Clinical and imaging data includes Breast Imaging Reporting and Data System (BI-RADS) assessment, region of interest coordinates for abnormalities, imaging features, pathologic outcomes, and patient demographics. Deep learning models including InceptionV3, VGG16, ResNet50V2, and ResNet152V2 were developed to distinguish between patches of abnormal tissue and randomly selected patches of normal tissue from the screening mammograms. The distributions of the training, validation and test sets are 29,144 (55.6%) patches of 10,678 (54.2%) patients, 9,
    
[^166]: 关于神经网络原像近似的研究

    On Preimage Approximation for Neural Networks. (arXiv:2305.03686v1 [cs.SE])

    [http://arxiv.org/abs/2305.03686](http://arxiv.org/abs/2305.03686)

    本文提出了一种基于线性松弛的高效实用的任意时刻算法，用于生成神经网络原像的符号下近似，以实现更快的改进和更高的压缩度。

    

    神经网络验证主要关注局部鲁棒性，然而，通常需要知道给定属性是否在整个输入域内全局成立，如果不成立，则需要知道属性成立的输入比例是多少。尽管精确的原像生成可以构建神经网络的等价表示，但在规模上是难以处理的。本文提出了一种基于线性松弛的高效实用的任意时刻算法，用于生成神经网络原像的符号下近似。我们的算法通过将输入区域划分为子区域，其中神经网络松弛边界变得更紧，迭代地最小化体积逼近误差。我们进一步采用采样和可微体积逼近来优先划分区域，并优化松弛的参数，从而实现更快的改进和更高的压缩度。

    Neural network verification mainly focuses on local robustness properties. However, often it is important to know whether a given property holds globally for the whole input domain, and if not then for what proportion of the input the property is true. While exact preimage generation can construct an equivalent representation of neural networks that can aid such (quantitative) global robustness verification, it is intractable at scale. In this work, we propose an efficient and practical anytime algorithm for generating symbolic under-approximations of the preimage of neural networks based on linear relaxation. Our algorithm iteratively minimizes the volume approximation error by partitioning the input region into subregions, where the neural network relaxation bounds become tighter. We further employ sampling and differentiable approximations to the volume in order to prioritize regions to split and optimize the parameters of the relaxation, leading to faster improvement and more compa
    
[^167]: 鲁棒决策树集成的可验证学习

    Verifiable Learning for Robust Tree Ensembles. (arXiv:2305.03626v1 [cs.LG])

    [http://arxiv.org/abs/2305.03626](http://arxiv.org/abs/2305.03626)

    本论文提出了一种可验证学习的方法，即训练易于验证的限制模型类来解决决策树集成的 NP-hard 问题，并成功设计出一种新的训练算法，使得在多项式时间内可以进行安全验证，而且仍保持着该领域最好的鲁棒性能。

    

    在测试时间内验证机器学习模型对抗攻击的鲁棒性是一个重要的研究问题。不幸的是，先前的研究确定，对于决策树集成，这个问题是 NP-hard ，因此对于特定的输入来说是不可解的。在本文中，我们确定了一类受限决策树集成，称为 large-spread 集成，其允许在多项式时间内运行安全验证算法。然后，我们提出了一种新方法，称为可验证学习，该方法倡导训练这种易于验证的受限模型类。我们通过设计一种新的训练算法，从标记数据中自动学习 large-spread 决策树集成来展示这种方法的益处，从而使其能够在多项式时间内进行安全验证。公开可用数据集上的实验结果证实，使用我们的算法训练的 large-spread 集成可以在几秒钟内使用标准半定编程求解器进行验证，同时对抗当前最先进的攻击具有竞争力的性能。

    Verifying the robustness of machine learning models against evasion attacks at test time is an important research problem. Unfortunately, prior work established that this problem is NP-hard for decision tree ensembles, hence bound to be intractable for specific inputs. In this paper, we identify a restricted class of decision tree ensembles, called large-spread ensembles, which admit a security verification algorithm running in polynomial time. We then propose a new approach called verifiable learning, which advocates the training of such restricted model classes which are amenable for efficient verification. We show the benefits of this idea by designing a new training algorithm that automatically learns a large-spread decision tree ensemble from labelled data, thus enabling its security verification in polynomial time. Experimental results on publicly available datasets confirm that large-spread ensembles trained using our algorithm can be verified in a matter of seconds, using stand
    
[^168]: DataComp：寻找下一代多模态数据集

    DataComp: In search of the next generation of multimodal datasets. (arXiv:2304.14108v1 [cs.CV])

    [http://arxiv.org/abs/2304.14108](http://arxiv.org/abs/2304.14108)

    DataComp是一个基准测试，旨在通过提出新的训练集来解决数据集在机器学习生态系统中的缺陷。它提供了一个多规模设计的实验测试平台，使用12.8B个图像-文本对的新候选池，让研究人员可以通过设计新的过滤技术或策划新的数据源并评估它们的新数据集来进行创新。

    

    大型的多模态数据集在近期的突破中起到了关键作用，比如CLIP、Stable Diffusion和GPT-4等。与此同时，数据集很少得到与模型架构或训练算法同等的研究关注。为了解决这个在机器学习生态系统中的缺陷，我们介绍了DataComp，一个基准测试，其中训练代码是固定的，研究人员通过提出新的训练集来进行创新。我们提供了一个基于Common Crawl的新候选池，其中包含12.8B个图像-文本对的数据集实验测试平台。参加我们基准测试的研究人员可以设计新的过滤技术或策划新的数据源，并通过运行我们标准化的CLIP训练代码并在38个下游测试集上进行测试来评估他们的新数据集。我们的基准测试包含多个规模，四个候选池大小和相应的计算预算，在训练期间涵盖了从12.8M到12.8B个样本。这种多规模设计有助于研究规模趋势，并为研究人员提供了更多的选择余地。

    Large multimodal datasets have been instrumental in recent breakthroughs such as CLIP, Stable Diffusion, and GPT-4. At the same time, datasets rarely receive the same research attention as model architectures or training algorithms. To address this shortcoming in the machine learning ecosystem, we introduce DataComp, a benchmark where the training code is fixed and researchers innovate by proposing new training sets. We provide a testbed for dataset experiments centered around a new candidate pool of 12.8B image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing on 38 downstream test sets. Our benchmark consists of multiple scales, with four candidate pool sizes and associated compute budgets ranging from 12.8M to 12.8B samples seen during training. This multi-scale design facilitates the study of scaling trends and makes the
    
[^169]: 带有跨设备交叉注意力的分层图神经网络用于跨设备用户匹配

    Hierarchical Graph Neural Network with Cross-Attention for Cross-Device User Matching. (arXiv:2304.03215v1 [cs.LG])

    [http://arxiv.org/abs/2304.03215](http://arxiv.org/abs/2304.03215)

    本文提出了一种带有跨设备交叉注意力的分层图神经网络(HGNN)，用于解决跨设备用户匹配问题，相对于最先进的TGCE方法，提高了5%的性能。

    

    在广告、推荐系统和网络安全等众多领域，跨设备用户匹配是一个关键问题。它涉及使用序列日志来识别和链接属于同一人的不同设备。以往的数据挖掘技术难以解决日志之间的长程依赖和高阶连接问题。最近，研究人员将这个问题建模为图问题，并提出了一种两层图上下文嵌入(TGCE)神经网络架构，表现优于先前的方法。在本文中，我们提出了一种新颖的分层图神经网络架构（HGNN），它具有比TGCE更为计算效率的二级设计。此外，我们在模型中引入了一种跨设备交叉注意力（Cross-Att）机制，相对于最先进的TGCE方法，提高了5%的性能。

    Cross-device user matching is a critical problem in numerous domains, including advertising, recommender systems, and cybersecurity. It involves identifying and linking different devices belonging to the same person, utilizing sequence logs. Previous data mining techniques have struggled to address the long-range dependencies and higher-order connections between the logs. Recently, researchers have modeled this problem as a graph problem and proposed a two-tier graph contextual embedding (TGCE) neural network architecture, which outperforms previous methods. In this paper, we propose a novel hierarchical graph neural network architecture (HGNN), which has a more computationally efficient second level design than TGCE. Furthermore, we introduce a cross-attention (Cross-Att) mechanism in our model, which improves performance by 5% compared to the state-of-the-art TGCE method.
    
[^170]: 抵抗个性化文本生成图像的反梦幻机：保护用户免受伪造图像的侵害

    Anti-DreamBooth: Protecting users from personalized text-to-image synthesis. (arXiv:2303.15433v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.15433](http://arxiv.org/abs/2303.15433)

    本文介绍了一种名为反梦幻机的防御系统，通过向用户图像添加微小的噪声扰动来保护用户免受个性化文本生成图像的恶意使用，从而避免产生虚假新闻或针对个人受害者的令人不安的内容。

    

    文本生成图像扩散模型是一场革命，它使得任何人，甚至没有设计技巧的人，都能够从简单的文本输入中创建逼真的图像。像梦幻机这样的强大个性化工具能够通过学习目标人物的几张参考图片生成该人物的图像。然而，当被滥用时，这种强大而方便的工具可能会产生虚假新闻或针对任何个人受害者的令人不安的内容，给社会带来严重的负面影响。本文中，我们探讨了一种名为反梦幻机的防御系统，以对抗对梦幻机的恶意使用。该系统旨在在发布用户图像之前向每个用户的图像添加微小的噪声扰动，以干扰对这些扰动图像进行训练的任何梦幻机模型的生成质量。我们对扰动优化的多种算法进行了广泛的研究，并在两个面部数据集上对各种文本生成图像模型版本进行了详细评估。尽管梦幻机的公式复杂，但我们发现了一种高效的防御方法。

    Text-to-image diffusion models are nothing but a revolution, allowing anyone, even without design skills, to create realistic images from simple text inputs. With powerful personalization tools like DreamBooth, they can generate images of a specific person just by learning from his/her few reference images. However, when misused, such a powerful and convenient tool can produce fake news or disturbing content targeting any individual victim, posing a severe negative social impact. In this paper, we explore a defense system called Anti-DreamBooth against such malicious use of DreamBooth. The system aims to add subtle noise perturbation to each user's image before publishing in order to disrupt the generation quality of any DreamBooth model trained on these perturbed images. We investigate a wide range of algorithms for perturbation optimization and extensively evaluate them on two facial datasets over various text-to-image model versions. Despite the complicated formulation of DreamBooth
    
[^171]: CHiLL: 零-shot定制化、可解释的医疗记录特征提取方法与大型语言模型

    CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models. (arXiv:2302.12343v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.12343](http://arxiv.org/abs/2302.12343)

    提出了CHiLL，一种用于从医疗记录中提取特征的方式，通过对大型语言模型进行查询生成特征，使医生能够用自己的专业知识制作对下游任务有临床意义的特征，并且该方法在性能和可解释性方面表现出良好的结果。

    

    我们提出了CHiLL (Crafting High-Level Latents)，一种自然语言规范化线性模型特征的方法。CHiLL通过专家制作的查询指导LLMs生成解释性特征，用于从健康记录中训练简单的线性分类器。基于LLM的查询生成特征可以使医生利用他们的领域专业知识来制作对下游任务有临床意义的特征，而无需手动从原始电子病历中提取这些特征。我们受到一个现实世界的风险预测任务的启发，但我们使用MIMIC-III和MIMIC-CXR数据和标准的预测任务（例如，30天再入院）来评估这种方法。我们发现使用自动提取的特征的线性模型在性能上与使用参考特征的模型相当，并且比使用“词袋”特征的线性模型提供更高的可解释性。我们证实了学习到的特征权重的有效性。

    We propose CHiLL (Crafting High-Level Latents), an approach for natural-language specification of features for linear models. CHiLL prompts LLMs with expert-crafted queries to generate interpretable features from health records. The resulting noisy labels are then used to train a simple linear classifier. Generating features based on queries to an LLM can empower physicians to use their domain expertise to craft features that are clinically meaningful for a downstream task of interest, without having to manually extract these from raw EHR. We are motivated by a real-world risk prediction task, but as a reproducible proxy, we use MIMIC-III and MIMIC-CXR data and standard predictive tasks (e.g., 30-day readmission) to evaluate this approach. We find that linear models using automatically extracted features are comparably performant to models using reference features, and provide greater interpretability than linear models using "Bag-of-Words" features. We verify that learned feature weig
    
[^172]: 对于带有专家建议的在线预测，自适应选择采样方法

    Adaptive Selective Sampling for Online Prediction with Experts. (arXiv:2302.08397v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.08397](http://arxiv.org/abs/2302.08397)

    针对带有专家建议的在线预测，我们提出了使用选择性采样方案的标签高效预测算法，能够使用比标准程序少得多的标签，并保持最优最坏情况后悔保证。对于在期望上明显更好的专家，算法的标签复杂度与回合数的平方根成比例，并且在实验中表现出与池式主动学习的极小极大速率相匹配的归一化后悔。

    

    我们考虑对于二进制序列的在线预测，所提出的标签高效预测算法使用了选择性采样方案，在保持最优最坏情况后悔保证的同时，能够使用比标准程序少得多的标签。这些算法基于指数加权预测器，适用于有或无完美专家的情况。对于一个在期望上明显更好的专家而言，我们展示了标签高效预测器的标签复杂度大致与回合数的平方根成比例。最后，我们通过数值实验证明了标签高效预测器的归一化后悔可以渐近匹配已知的基于池式主动学习的极小极大速率，表明它能够在良性环境中进行最优适应。

    We consider online prediction of a binary sequence with expert advice. For this setting, we devise label-efficient forecasting algorithms, which use a selective sampling scheme that enables collecting much fewer labels than standard procedures, while still retaining optimal worst-case regret guarantees. These algorithms are based on exponentially weighted forecasters, suitable for settings with and without a perfect expert. For a scenario where one expert is strictly better than the others in expectation, we show that the label complexity of the label-efficient forecaster scales roughly as the square root of the number of rounds. Finally, we present numerical experiments empirically showing that the normalized regret of the label-efficient forecaster can asymptotically match known minimax rates for pool-based active learning, suggesting it can optimally adapt to benign settings.
    
[^173]: 核岭回归推断

    Kernel Ridge Regression Inference. (arXiv:2302.06578v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2302.06578](http://arxiv.org/abs/2302.06578)

    我们提供了核岭回归方法的一致推断和置信带，为广泛应用于各种数据类型的非参数回归估计器提供了准确的统计推断方法。

    

    我们提供了核岭回归(KRR)的一致推断和置信带，这是一种广泛应用于包括排名、图像和图表在内的一般数据类型的非参数回归估计器。尽管这些数据的普遍存在，如学校分配中的排序优先级列表，但KRR的推断理论尚未完全知悉，限制了它在经济学和其他科学领域中的作用。我们构建了针对一般回归器的尖锐、一致的置信区间。为了进行推断，我们开发了一种有效的自举程序，通过对称化来消除偏差并限制计算开销。为了证明该程序，我们推导了再生核希尔伯特空间(RKHS)中部分和的有限样本、均匀高斯和自举耦合。这些推导暗示了基于RKHS单位球的经验过程的强逼近，对覆盖数具有对数依赖关系。模拟验证了置信度。

    We provide uniform inference and confidence bands for kernel ridge regression (KRR), a widely-used non-parametric regression estimator for general data types including rankings, images, and graphs. Despite the prevalence of these data -e.g., ranked preference lists in school assignment -- the inferential theory of KRR is not fully known, limiting its role in economics and other scientific domains. We construct sharp, uniform confidence sets for KRR, which shrink at nearly the minimax rate, for general regressors. To conduct inference, we develop an efficient bootstrap procedure that uses symmetrization to cancel bias and limit computational overhead. To justify the procedure, we derive finite-sample, uniform Gaussian and bootstrap couplings for partial sums in a reproducing kernel Hilbert space (RKHS). These imply strong approximation for empirical processes indexed by the RKHS unit ball with logarithmic dependence on the covering number. Simulations verify coverage. We use our proce
    
[^174]: 深度神经网络用于动态系统建模的批判性观察

    A critical look at deep neural network for dynamic system modeling. (arXiv:2301.11604v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11604](http://arxiv.org/abs/2301.11604)

    这项研究对使用深度神经网络进行动态系统建模的能力进行了批判性分析，比较了两种神经网络模型（LSTM和CFNN）与标准系统辨识方法（PEM）的性能，并指出了神经网络建模可能存在的缺陷和被忽视的问题。

    

    神经网络模型作为控制领域中的动态建模工具越来越受欢迎。它们具有非线性结构，能够逼近任何函数等吸引人的特点。尽管大多数研究人员认为这些模型具有良好的前景，但本文对（深度）神经网络在使用输入输出数据进行动态系统建模方面的能力提出了质疑。对于线性时不变（LTI）动态系统的识别，将两个代表性的神经网络模型，即长短期记忆（LSTM）和级联前馈神经网络（CFNN），与系统辨识的标准预测误差法（PEM）进行了比较。在比较中，考虑了系统辨识的四个重要方面，然后指出了基于神经网络的建模可能存在的一些缺陷和被忽视的问题。进行了详细的仿真研究来验证这些缺陷：对于LTI系统，无论是LSTM还是CFNN都无法提供一致的模型。

    Neural network models become increasingly popular as dynamic modeling tools in the control community. They have many appealing features including nonlinear structures, being able to approximate any functions. While most researchers hold optimistic attitudes towards such models, this paper questions the capability of (deep) neural networks for the modeling of dynamic systems using input-output data. For the identification of linear time-invariant (LTI) dynamic systems, two representative neural network models, Long Short-Term Memory (LSTM) and Cascade Foward Neural Network (CFNN) are compared to the standard Prediction Error Method (PEM) of system identification. In the comparison, four essential aspects of system identification are considered, then several possible defects and neglected issues of neural network based modeling are pointed out. Detailed simulation studies are performed to verify these defects: for the LTI system, both LSTM and CFNN fail to deliver consistent models even 
    
[^175]: 通过机器人车辆在复杂和无信号的交叉口中学习控制和协调混合交通

    Learning to Control and Coordinate Mixed Traffic Through Robot Vehicles at Complex and Unsignalized Intersections. (arXiv:2301.05294v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05294](http://arxiv.org/abs/2301.05294)

    本研究提出了一种去中心化的多智能体强化学习方法，用于控制和协调混合交通，特别是人驾驶车辆和机器人车辆在实际复杂交叉口的应用。实验结果表明，使用5%的机器人车辆可以有效防止交叉口内的拥堵形成。

    

    交叉口是现代大都市交通中必不可少的道路基础设施。然而，由于交通事故或缺乏交通协调机制（如交通信号灯），它们也可能成为交通流的瓶颈。最近，提出了各种超越传统控制方法的控制和协调机制，以提高交叉口交通的效率。在这些方法中，控制可预见的包含人驾驶车辆（HVs）和机器人车辆（RVs）的混合交通已经出现。在本项目中，我们提出了一种去中心化的多智能体强化学习方法，用于实际复杂交叉口的混合交通的控制和协调，这是一个以前未被探索过的主题。我们进行了全面的实验，以展示我们方法的有效性。特别是，我们展示了在实际交通条件下，使用5%的RVs，我们可以防止复杂交叉口内的拥堵形成。

    Intersections are essential road infrastructures for traffic in modern metropolises. However, they can also be the bottleneck of traffic flows as a result of traffic incidents or the absence of traffic coordination mechanisms such as traffic lights. Recently, various control and coordination mechanisms that are beyond traditional control methods have been proposed to improve the efficiency of intersection traffic. Amongst these methods, the control of foreseeable mixed traffic that consists of human-driven vehicles (HVs) and robot vehicles (RVs) has emerged. In this project, we propose a decentralized multi-agent reinforcement learning approach for the control and coordination of mixed traffic at real-world, complex intersections--a topic that has not been previously explored. Comprehensive experiments are conducted to show the effectiveness of our approach. In particular, we show that using 5% RVs, we can prevent congestion formation inside a complex intersection under the actual traf
    
[^176]: 一种基于领域约束的弱监督学习可扩展技术

    A Scalable Technique for Weak-Supervised Learning with Domain Constraints. (arXiv:2301.05253v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05253](http://arxiv.org/abs/2301.05253)

    我们提出了一种使用领域约束的可扩展技术，用于弱监督学习神经网络分类未标记数据。我们通过在MNIST图像分类问题上的实验证明，我们的方法比以前的方法在规模上有显著的提升。

    

    我们提出了一种新颖的可扩展的端到端流水线技术，利用符号领域知识作为约束条件，以弱监督的方式学习用于分类未标记数据的神经网络。我们的方法特别适用于数据包含不同的组（类），适宜于聚类友好的表示学习，并且可以通过考虑多个训练示例一次性使用高效的数学优化技术来重新制定领域约束。

    We propose a novel scalable end-to-end pipeline that uses symbolic domain knowledge as constraints for learning a neural network for classifying unlabeled data in a weak-supervised manner. Our approach is particularly well-suited for settings where the data consists of distinct groups (classes) that lends itself to clustering-friendly representation learning and the domain constraints can be reformulated for use of efficient mathematical optimization techniques by considering multiple training examples at once. We evaluate our approach on a variant of the MNIST image classification problem where a training example consists of image sequences and the sum of the numbers represented by the sequences, and show that our approach scales significantly better than previous approaches that rely on computing all constraint satisfying combinations for each training example.
    
[^177]: 通过机械解释性的进展测量来理解"grokking"

    Progress measures for grokking via mechanistic interpretability. (arXiv:2301.05217v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05217](http://arxiv.org/abs/2301.05217)

    该论文通过机械解释性的方法找到了连续的进展测量，以理解神经网络中 emergent behavior 的产生原因。其中，作者以“grokking”现象为案例，通过逆向工程的方式完全理解了小型transformer网络在模块化加法任务中的算法，并定义了进展测量来研究这个现象的进展性质。

    

    神经网络经常表现出新兴行为，也就是通过增加参数、训练数据或训练步骤来产生质的变化。理解新兴行为的一种方法是找到连续的“进展测量”，这些测量是看似不连续的质的变化的基础。我们认为，可以通过机械解释性来找到进展测量：将学到的行为逆向工程成其各个组成部分。作为一个案例研究，我们调查了小型transformer训练模块化加法任务时出现的“grokking”现象。我们完全逆向工程了这些网络学到的算法，该算法使用离散傅里叶变换和三角恒等式将加法转换为圆周旋转。我们通过分析激活和权重，并在傅里叶空间进行消融实验证实了这个算法。基于这种理解，我们定义了进展测量，使我们能够研究这个现象的进展性质。

    Neural networks often exhibit emergent behavior, where qualitatively new capabilities arise from scaling up the amount of parameters, training data, or training steps. One approach to understanding emergence is to find continuous \textit{progress measures} that underlie the seemingly discontinuous qualitative changes. We argue that progress measures can be found via mechanistic interpretability: reverse-engineering learned behaviors into their individual components. As a case study, we investigate the recently-discovered phenomenon of ``grokking'' exhibited by small transformers trained on modular addition tasks. We fully reverse engineer the algorithm learned by these networks, which uses discrete Fourier transforms and trigonometric identities to convert addition to rotation about a circle. We confirm the algorithm by analyzing the activations and weights and by performing ablations in Fourier space. Based on this understanding, we define progress measures that allow us to study the 
    
[^178]: 释放共享标签结构在人类活动识别中的力量

    Unleashing the Power of Shared Label Structures for Human Activity Recognition. (arXiv:2301.03462v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.03462](http://arxiv.org/abs/2301.03462)

    本文提出了SHARE，一种考虑共享标签结构的HAR框架，通过建模共同的结构从而揭示不同活动之间的知识。

    

    当前的人类活动识别（HAR）技术将活动标签视为整数类别ID，没有明确地对类别标签的语义进行建模。我们观察到不同的活动名称通常具有共享的结构。例如，“打开门”和“打开冰箱”都有“打开”作为动作；“踢足球”和“打网球”都有“球”作为物体。标签名称中的这种共享结构可以转化为感知数据的相似性，并且建模共同的结构可以帮助揭示不同活动之间的知识，特别是对于样本有限的活动。在本文中，我们提出了SHARE，一种考虑不同活动的标签名称共享结构的HAR框架。为了利用共享结构，SHARE包括一个编码器，用于从输入的感知时间序列中提取特征，并包括一个解码器，用于生成标签名称作为令牌序列。我们还提出了三种标签增强技术，以帮助模型更加有效地进行预测。

    Current human activity recognition (HAR) techniques regard activity labels as integer class IDs without explicitly modeling the semantics of class labels. We observe that different activity names often have shared structures. For example, "open door" and "open fridge" both have "open" as the action; "kicking soccer ball" and "playing tennis ball" both have "ball" as the object. Such shared structures in label names can be translated to the similarity in sensory data and modeling common structures would help uncover knowledge across different activities, especially for activities with limited samples. In this paper, we propose SHARE, a HAR framework that takes into account shared structures of label names for different activities. To exploit the shared structures, SHARE comprises an encoder for extracting features from input sensory time series and a decoder for generating label names as a token sequence. We also propose three label augmentation techniques to help the model more effecti
    
[^179]: 关于随机梯度的被忽视的结构

    On the Overlooked Structure of Stochastic Gradients. (arXiv:2212.02083v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.02083](http://arxiv.org/abs/2212.02083)

    本文对深度学习中随机梯度的结构进行了正式的统计检验，发现逐维梯度通常呈现幂律重尾，而逐次迭代的梯度和随机梯度噪声通常不呈现幂律重尾。

    

    随机梯度与深度神经网络（DNN）的优化和泛化密切相关。一些研究试图通过梯度噪声的重尾性质来解释随机优化在深度学习中的成功，而其他研究则提出了对梯度噪声的重尾假设的理论和实证证据。不幸的是，在深度学习中，用于分析随机梯度结构和重尾的正式统计检验还没有得到充分开发。在本文中，我们主要做出两个贡献。首先，我们对随机梯度和梯度噪声在参数和迭代中的分布进行了正式的统计检验。我们的统计检验发现，逐维梯度通常表现出幂律重尾，而逐次迭代的梯度和由小批量训练引起的随机梯度噪声通常不表现出幂律重尾。其次，我们进一步发现协方差特性。

    Stochastic gradients closely relate to both optimization and generalization of deep neural networks (DNNs). Some works attempted to explain the success of stochastic optimization for deep learning by the arguably heavy-tail properties of gradient noise, while other works presented theoretical and empirical evidence against the heavy-tail hypothesis on gradient noise. Unfortunately, formal statistical tests for analyzing the structure and heavy tails of stochastic gradients in deep learning are still under-explored. In this paper, we mainly make two contributions. First, we conduct formal statistical tests on the distribution of stochastic gradients and gradient noise across both parameters and iterations. Our statistical tests reveal that dimension-wise gradients usually exhibit power-law heavy tails, while iteration-wise gradients and stochastic gradient noise caused by minibatch training usually do not exhibit power-law heavy tails. Second, we further discover that the covariance spe
    
[^180]: 精准肿瘤学的染色体AI

    Karyotype AI for Precision Oncology. (arXiv:2211.14312v3 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2211.14312](http://arxiv.org/abs/2211.14312)

    本研究针对精准肿瘤学中的染色体分析问题，通过使用Fred Hutchinson癌症研究中心的大量数据，利用深度学习模型和拓扑视觉转换器(TopViTs)，成功开发出了一种自动识别染色体异常的方法。

    

    染色体分析对于诊断遗传疾病至关重要。对于血液系统恶性肿瘤，通过染色体组型分析来发现体细胞突变是标准的护理方法。然而，染色体组型分析因为大部分是手动操作，且需要专业知识来识别和注释突变，所以昂贵且耗时。以Fred Hutchinson癌症研究中心过去五年的约10,000个患者标本和约50,000个染色体组型图片作为训练集，我们创建了一组代表单个染色体的标记图片。这些单个染色体用于训练和评估深度学习模型，以分类人类的24条染色体和识别染色体异常。具有最高准确性的模型使用了最近引入的拓扑视觉转换器(TopViTs)和二级块-托普利茨蒙版，以融入结构性归纳偏置。TopViT的性能优于CNN(Inc)

    Chromosome analysis is essential for diagnosing genetic disorders. For hematologic malignancies, identification of somatic clonal aberrations by karyotype analysis remains the standard of care. However, karyotyping is costly and time-consuming because of the largely manual process and the expertise required in identifying and annotating aberrations. Efforts to automate karyotype analysis to date fell short in aberration detection. Using a training set of ~10k patient specimens and ~50k karyograms from over 5 years from the Fred Hutchinson Cancer Center, we created a labeled set of images representing individual chromosomes. These individual chromosomes were used to train and assess deep learning models for classifying the 24 human chromosomes and identifying chromosomal aberrations. The top-accuracy models utilized the recently introduced Topological Vision Transformers (TopViTs) with 2-level-block-Toeplitz masking, to incorporate structural inductive bias. TopViT outperformed CNN (Inc
    
[^181]: AF Adapter: 连续预训练用于构建中文生物医学语言模型

    AF Adapter: Continual Pretraining for Building Chinese Biomedical Language Model. (arXiv:2211.11363v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.11363](http://arxiv.org/abs/2211.11363)

    我们提出了一种连续预训练方法，名为Attention-FFN Adapter，用于构建中文生物医学领域的语言模型。实验证明，该方法在性能上相对于强基准模型平均提高了0.6%至2%。同时，该方法还有效缓解了灾难性的遗忘问题。

    

    连续预训练是从通用语言模型构建特定领域预训练语言模型的流行方法。尽管高效，但连续预训练存在灾难性遗忘问题，可能会影响模型在下游任务中的性能。为了缓解这个问题，本文提出了一种BERT-based模型的连续预训练方法，名为Attention-FFN Adapter。其主要思想是在每个自注意层和前馈网络中引入少量的注意头和隐藏单元。此外，我们训练了一个基于RoBERTa的针对中文生物医学领域的特定领域语言模型，名为AF Adapter。实验证明，仅训练模型参数的约17%，AF Adapter相对于强基准模型平均性能提高了0.6%至2%。进一步实验证明，我们的方法缓解了灾难性的遗忘问题。

    Continual pretraining is a popular way of building a domain-specific pretrained language model from a general-domain language model. In spite of its high efficiency, continual pretraining suffers from catastrophic forgetting, which may harm the model's performance in downstream tasks. To alleviate the issue, in this paper, we propose a continual pretraining method for the BERT-based model, named Attention-FFN Adapter. Its main idea is to introduce a small number of attention heads and hidden units inside each self-attention layer and feed-forward network. Furthermore, we train a domain-specific language model named AF Adapter based RoBERTa for the Chinese biomedical domain. In experiments, models are applied to downstream tasks for evaluation. The results demonstrate that with only about 17% of model parameters trained, AF Adapter achieves 0.6%, 2% gain in performance on average, compared to strong baselines. Further experimental results show that our method alleviates the catastrophic
    
[^182]: 傅立叶分析实现一致且真实的模型解释

    Consistent and Truthful Interpretation with Fourier Analysis. (arXiv:2210.17426v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2210.17426](http://arxiv.org/abs/2210.17426)

    该论文提出了一个称为真实解释的新概念，通过傅立叶分析获得严格保证，并在实验中证明了其在支持假设情景和降低解释误差方面的优势。

    

    对于许多跨学科领域，机器学习的解释需要与当前案例相关的假设情景一致，即如果一个因素改变，模型会如何反应？尽管归因方法由优雅的公理系统支持，但它们主要关注单个输入，并且通常不一致。为支持假设情景，我们引入了一个称为真实解释的新概念，并应用布尔函数的傅立叶分析来获得严格的保证。实验结果表明，对于各种半径的邻域，我们的方法与其他方法相比，可以实现2倍至50倍更低的解释误差。

    For many interdisciplinary fields, ML interpretations need to be consistent with what-if scenarios related to the current case, i.e., if one factor changes, how does the model react? Although the attribution methods are supported by the elegant axiomatic systems, they mainly focus on individual inputs, and are generally inconsistent. To support what-if scenarios, we introduce a new notion called truthful interpretation, and apply Fourier analysis of Boolean functions to get rigorous guarantees. Experimental results show that for neighborhoods with various radii, our method achieves 2x - 50x lower interpretation error compared with the other methods.
    
[^183]: 在线自适应递归MCMC方法用于概率模型识别

    Online Probabilistic Model Identification using Adaptive Recursive MCMC. (arXiv:2210.12595v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12595](http://arxiv.org/abs/2210.12595)

    提出了在线自适应递归马尔可夫链蒙特卡洛（ARMCMC）方法用于概率模型识别，克服了传统在线技术的缺点，并计算了模型参数的整个概率分布函数。

    

    尽管贝叶斯范式提供了一个估计不确定参数整个概率分布的形式框架，但其在线实现由于高计算成本而具有挑战性。我们提出了自适应递归马尔可夫链蒙特卡洛（ARMCMC）方法，该方法消除了传统在线技术的缺点，同时计算模型参数的整个概率密度函数。高斯噪声的限制、仅适用于线性参数（LIP）系统以及持续激励（PE）需求是其中的一些缺点。在ARMCMC中，提出了基于时间遗忘因子（TFF）的变量跳跃分布。遗忘因子可以在许多动态系统中用TFF自适应地表示，作为常数超参数的替代选择。通过在开发与探索之间提供权衡，具体的跳跃分布已被优化，以便适应允许推断的混合/多模态系统。

    Although the Bayesian paradigm offers a formal framework for estimating the entire probability distribution over uncertain parameters, its online implementation can be challenging due to high computational costs. We suggest the Adaptive Recursive Markov Chain Monte Carlo (ARMCMC) method, which eliminates the shortcomings of conventional online techniques while computing the entire probability density function of model parameters. The limitations to Gaussian noise, the application to only linear in the parameters (LIP) systems, and the persistent excitation (PE) needs are some of these drawbacks. In ARMCMC, a temporal forgetting factor (TFF)-based variable jump distribution is proposed. The forgetting factor can be presented adaptively using the TFF in many dynamical systems as an alternative to a constant hyperparameter. By offering a trade-off between exploitation and exploration, the specific jump distribution has been optimised towards hybrid/multi-modal systems that permit inferenc
    
[^184]: 事件触发的时变贝叶斯优化

    Event-Triggered Time-Varying Bayesian Optimization. (arXiv:2208.10790v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.10790](http://arxiv.org/abs/2208.10790)

    本文提出了一种事件触发算法ET-GP-UCB，用于解决时变贝叶斯优化中的探索和开发的权衡问题。通过基于高斯过程回归的概率均匀误差界，算法能够在未知变化速率的情况下自适应地适应实际的时间变化。数值实验结果表明，ET-GP-UCB在合成和实际数据上优于现有算法。

    

    我们考虑使用时变贝叶斯优化（TVBO）顺序优化时变目标函数的问题。其中，关键挑战是在时间变化下的勘探与开发的权衡。当前的TVBO方法需要对变化速率有先验知识。然而，在实践中，变化速率通常是未知的。我们提出了一种事件触发算法ET-GP-UCB，它将优化问题视为静态问题，直到在线检测到目标函数的变化并重置数据集。这使得算法能够适应实际的时间变化，而不需要先验知识。事件触发基于高斯过程回归中使用的概率均匀误差界。我们给出了ET-GP-UCB的遗憾界，并通过数值实验表明，它在合成和实际数据上优于现有算法。此外，这些结果表明ET-GP-UCB可广泛应用于不同的设定。

    We consider the problem of sequentially optimizing a time-varying objective function using time-varying Bayesian optimization (TVBO). Here, the key challenge is the exploration-exploitation trade-off under time variations. Current approaches to TVBO require prior knowledge of a constant rate of change. However, in practice, the rate of change is usually unknown. We propose an event-triggered algorithm, ET-GP-UCB, that treats the optimization problem as static until it detects changes in the objective function online and then resets the dataset. This allows the algorithm to adapt to realized temporal changes without the need for prior knowledge. The event-trigger is based on probabilistic uniform error bounds used in Gaussian process regression. We provide regret bounds for ET-GP-UCB and show in numerical experiments that it outperforms state-of-the-art algorithms on synthetic and real-world data. Furthermore, these results demonstrate that ET-GP-UCB is readily applicable to various set
    
[^185]: DESCN: 深度整体空间交叉网络用于个体治疗效果估计

    DESCN: Deep Entire Space Cross Networks for Individual Treatment Effect Estimation. (arXiv:2207.09920v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.09920](http://arxiv.org/abs/2207.09920)

    本论文提出了DESCN模型，通过深度整体空间交叉网络的方式，从端到端的角度建模个体治疗效果估计。该模型可以解决传统方法中的分布偏移和样本不平衡问题。

    

    因果推断在电子商务和精准医学等领域有广泛应用，其性能严重依赖于个体治疗效果(ITE)的准确估计。传统上，通过分别在各自的样本空间中建模受治疗组和对照组的响应函数来预测ITE。然而，在实际应用中，这种方法通常遇到两个问题，即由于治疗偏差而导致的受治疗组和对照组之间的分布偏离，以及其人口规模之间的显著样本不平衡。本文提出了深度整体空间交叉网络(DESCN)，以端到端的方式建模治疗效果。DESCN通过一个跨网络以多任务学习的方式捕捉治疗倾向、响应和隐藏治疗效果的综合信息。我们的方法在整个样本空间中联合学习治疗和响应函数，以避免治疗偏差，并采用中间伪处理方式。

    Causal Inference has wide applications in various areas such as E-commerce and precision medicine, and its performance heavily relies on the accurate estimation of the Individual Treatment Effect (ITE). Conventionally, ITE is predicted by modeling the treated and control response functions separately in their individual sample spaces. However, such an approach usually encounters two issues in practice, i.e. divergent distribution between treated and control groups due to treatment bias, and significant sample imbalance of their population sizes. This paper proposes Deep Entire Space Cross Networks (DESCN) to model treatment effects from an end-to-end perspective. DESCN captures the integrated information of the treatment propensity, the response, and the hidden treatment effect through a cross network in a multi-task learning manner. Our method jointly learns the treatment and response functions in the entire sample space to avoid treatment bias and employs an intermediate pseudo treat
    
[^186]: 离散不变网络用于学习神经场之间的映射

    Discretization Invariant Networks for Learning Maps between Neural Fields. (arXiv:2206.01178v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.01178](http://arxiv.org/abs/2206.01178)

    该论文提出了一种用于学习神经场之间映射的离散不变网络（DI-Nets）框架，并通过分析得出模型输出在不同离散化下的上界，并强调了点集差异在界限表征方面的重要性。通过数值积分进行低差异度离散化的网络设计，证明DI-Nets可以普遍逼近一类映射。

    

    随着神经场形式的连续数据强大表示的出现，需要一种离散不变的学习方法：一种在连续域上学习函数之间映射的方法，不受函数采样方式的影响。我们提出了一个新的框架用于理解和设计离散不变的神经网络（DI-Nets），它广泛推广了许多离散网络（如卷积神经网络）和连续网络（如神经算子）。我们的分析确定了在不同有限离散化下模型输出的上界，并强调了点集差异在表征这种界限方面的核心作用。这一洞察力促使我们设计了一类由数值积分驱动的神经网络，通过低差异度的离散化进行准蒙特卡洛抽样。我们通过构造证明了DI-Nets可以普遍逼近一类介于可积函数之间的映射。

    With the emergence of powerful representations of continuous data in the form of neural fields, there is a need for discretization invariant learning: an approach for learning maps between functions on continuous domains without being sensitive to how the function is sampled. We present a new framework for understanding and designing discretization invariant neural networks (DI-Nets), which generalizes many discrete networks such as convolutional neural networks as well as continuous networks such as neural operators. Our analysis establishes upper bounds on the deviation in model outputs under different finite discretizations, and highlights the central role of point set discrepancy in characterizing such bounds. This insight leads to the design of a family of neural networks driven by numerical integration via quasi-Monte Carlo sampling with discretizations of low discrepancy. We prove by construction that DI-Nets universally approximate a large class of maps between integrable funct
    
[^187]: 文本生成的统一复杂度

    Uniform Complexity for Text Generation. (arXiv:2204.05185v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2204.05185](http://arxiv.org/abs/2204.05185)

    提出了一种新的基准测试，称为统一复杂度文本生成（UCTG），要求生成模型在不同的文本提示情况下遵循统一的语言属性。该测试使用了150多个与人类和生成文本复杂度相关的特征进行评估。

    

    大型语言模型（LLMs）在多个生成型自然语言处理任务中取得了令人期待的结果，比如总结和机器翻译。然而，在叙述生成的背景下，现有模型仍未捕捉到产生一致文本所涉及的因素。例如，逻辑上讲，一段文本或一个故事应该在始终可读，这种复杂度是可控制的。因此，如果输入文本提示在弗列许读易度测试中评为一年级阅读水平，那么延续情节的生成文本也应在此复杂度范围内。基于此，我们引入了文本生成的统一复杂度（UCTG），这是一个新的基准测试，提出了使生成模型在提示方面遵循统一语言属性的挑战。我们通过使用150多个与语言和认知相关的特征来评估人类和生成文本的复杂度进行实验。

    Large language models (LLMs) have shown promising results in a wide array of generative NLP tasks, such as summarization and machine translation. In the context of narrative generation, however, existing models still do not capture factors that contribute to producing consistent text. For instance, it is logical that a piece of text or a story should be uniformly readable throughout and that this form of complexity should be controllable. As such, if the complexity of an input text prompt is rated first-grade reading level in the Flesch Reading Ease test, then the generated text continuing the plot should also be within this range of complexity. With this in mind, we introduce Uniform Complexity for Text Generation (UCTG), a new benchmark test which raises the challenge of making generative models observe uniform linguistic properties with respect to prompts. We experiment with over 150+ linguistically and cognitively motivated features for evaluating text complexity in humans and gene
    
[^188]: 通过一致分配视图到聚类中进行表示学习

    Representation Learning via Consistent Assignment of Views to Clusters. (arXiv:2112.15421v2 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2112.15421](http://arxiv.org/abs/2112.15421)

    该论文介绍了一种名为CARL的无监督学习方法，通过将对比学习和深度聚类相结合，通过学习一组通用原型来实现一致分配视图到聚类中进行表示学习。CARL在多个表示学习基准测试中超越了其他方法。

    

    我们介绍了一种名为CARL的一致分配表示学习的无监督学习方法，该方法结合了自监督对比学习和深度聚类的思想。通过从聚类的角度看待对比学习，CARL通过学习一组通用原型来学习无监督表示，这些原型作为能量锚点，强制将给定图像的不同视图分配给相同的原型。与当代的深度聚类对比学习相比，CARL提出以在线方式学习一组通用原型，使用梯度下降而不需要使用非可微分算法或K-Means来解决聚类分配问题。CARL在许多表示学习基准测试中超越了竞争对手，包括线性评估、半监督学习和迁移学习。

    We introduce Consistent Assignment for Representation Learning (CARL), an unsupervised learning method to learn visual representations by combining ideas from self-supervised contrastive learning and deep clustering. By viewing contrastive learning from a clustering perspective, CARL learns unsupervised representations by learning a set of general prototypes that serve as energy anchors to enforce different views of a given image to be assigned to the same prototype. Unlike contemporary work on contrastive learning with deep clustering, CARL proposes to learn the set of general prototypes in an online fashion, using gradient descent without the necessity of using non-differentiable algorithms or K-Means to solve the cluster assignment problem. CARL surpasses its competitors in many representations learning benchmarks, including linear evaluation, semi-supervised learning, and transfer learning.
    
[^189]: ReLU'(0)对反向传播的数值影响研究

    Numerical influence of ReLU'(0) on backpropagation. (arXiv:2106.12915v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.12915](http://arxiv.org/abs/2106.12915)

    本研究研究了ReLU'(0)值对深度学习中反向传播的数值影响，发现在32位精度下会出现显著的变化，而在16位精度下是系统性的。在普通的SGD训练中，选择ReLU'(0) = 0似乎是最有效的。此外，重新调整方法 tend to buffer ReLU'(0)值的影响。

    

    在理论上，神经网络中ReLU'(0)在[0, 1]范围内的选择对反向传播和训练几乎没有影响。然而，在现实世界中，32位默认精度结合深度学习问题的规模，使其成为训练方法的超参数。我们研究了ReLU'(0)值对几种精度水平（16位，32位，64位）、各种网络（全连接、VGG、ResNet）和数据集（MNIST、CIFAR10、SVHN）的重要性。我们观察到在32位精度下，反向传播输出出现了显著的变化，这种情况大约出现了一半的时间。这种影响在双精度下消失，而在16位精度下是系统性的。对于普通的SGD训练而言，选择ReLU'(0) = 0似乎是最有效的。我们还发现，批归一化或ADAM等重新调整方法 tend to buffer ReLU'(0)值的影响。总体而言，我们想要传达的信息是，非光滑问题的算法微分可能隐藏了一些参数。

    In theory, the choice of ReLU'(0) in [0, 1] for a neural network has a negligible influence both on backpropagation and training. Yet, in the real world, 32 bits default precision combined with the size of deep learning problems makes it a hyperparameter of training methods. We investigate the importance of the value of ReLU'(0) for several precision levels (16, 32, 64 bits), on various networks (fully connected, VGG, ResNet) and datasets (MNIST, CIFAR10, SVHN). We observe considerable variations of backpropagation outputs which occur around half of the time in 32 bits precision. The effect disappears with double precision, while it is systematic at 16 bits. For vanilla SGD training, the choice ReLU'(0) = 0 seems to be the most efficient. We also evidence that reconditioning approaches as batch-norm or ADAM tend to buffer the influence of ReLU'(0)'s value. Overall, the message we want to convey is that algorithmic differentiation of nonsmooth problems potentially hides parameters that 
    
[^190]: 生命周期策略重用与任务容量的重要性

    Lifetime policy reuse and the importance of task capacity. (arXiv:2106.01741v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.01741](http://arxiv.org/abs/2106.01741)

    本文提出了生命周期策略重用算法和任务容量指标，通过优化一组近似最优策略和策略选择，实现了在终身强化学习中避免生成大量策略的目标。实验证明了生命周期策略重用和任务容量预选对多任务学习的重要性。

    

    人工智能领域一直存在一个挑战，即终身强化学习。在这种学习中，学习者按顺序接收多个任务，并在任务之间传递知识，同时避免灾难性遗忘。策略重用和其他多策略强化学习技术可以学习多个任务，但可能会生成大量的策略。本文提出了两个创新贡献，即1) 生命周期策略重用，这是一种模型无关的策略重用算法，通过策略优化和自适应策略选择的组合来优化一组近似最优策略，避免生成过多策略；2) 任务容量，一种衡量策略能准确解决的最大任务数量的指标。通过比较两种先进的基础学习模型，在一个包含18个部分可观察的Pacman任务和一个最多包含125个任务的Cartpole任务中，实验结果证明了生命周期策略重用和基于任务容量的预选的重要性。

    A long-standing challenge in artificial intelligence is lifelong reinforcement learning, where learners are given many tasks in sequence and must transfer knowledge between tasks while avoiding catastrophic forgetting. Policy reuse and other multi-policy reinforcement learning techniques can learn multiple tasks but may generate many policies. This paper presents two novel contributions, namely 1) Lifetime Policy Reuse, a model-agnostic policy reuse algorithm that avoids generating many policies by optimising a fixed number of near-optimal policies through a combination of policy optimisation and adaptive policy selection; and 2) the task capacity, a measure for the maximal number of tasks that a policy can accurately solve. Comparing two state-of-the-art base-learners, the results demonstrate the importance of Lifetime Policy Reuse and task capacity based pre-selection on an 18-task partially observable Pacman domain and a Cartpole domain of up to 125 tasks.
    
[^191]: 关于权重衰减的常被忽视的陷阱及其如何缓解：梯度范数视角

    On the Overlooked Pitfalls of Weight Decay and How to Mitigate Them: A Gradient-Norm Perspective. (arXiv:2011.11152v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2011.11152](http://arxiv.org/abs/2011.11152)

    本论文发现了权重衰减在训练的最后阶段会导致大梯度范数的陷阱，为了解决这个问题，提出了Scheduled Weight Decay（SWD）方法，通过动态调整权重衰减强度并惩罚大梯度范数，可以有效缓解这个问题。

    

    权重衰减是一种简单而强大的正则化技术，在深度神经网络（DNN）的训练中被广泛使用。尽管权重衰减引起了很多关注，但先前的研究未能发现权重衰减导致的大梯度范数的一些被忽视的陷阱。在本文中，我们发现权重衰减不幸地会导致训练的最后阶段（或终止的解）出现大梯度范数，这通常表明了糟糕的收敛和差劲的泛化性能。为了缓解以梯度范数为中心的问题，我们提出了第一个实用的权重衰减调度器，称为Scheduled Weight Decay（SWD）方法，可以根据梯度范数动态调整权重衰减强度，并在训练过程中显著惩罚大梯度范数。我们的实验也证明，SWD确实缓解了大梯度范数问题，并且通常明显优于常规的恒定权重衰减策略（AMEN）。

    Weight decay is a simple yet powerful regularization technique that has been very widely used in training of deep neural networks (DNNs). While weight decay has attracted much attention, previous studies fail to discover some overlooked pitfalls on large gradient norms resulted by weight decay. In this paper, we discover that, weight decay can unfortunately lead to large gradient norms at the final phase (or the terminated solution) of training, which often indicates bad convergence and poor generalization. To mitigate the gradient-norm-centered pitfalls, we present the first practical scheduler for weight decay, called the Scheduled Weight Decay (SWD) method that can dynamically adjust the weight decay strength according to the gradient norm and significantly penalize large gradient norms during training. Our experiments also support that SWD indeed mitigates large gradient norms and often significantly outperforms the conventional constant weight decay strategy for Adaptive Moment Es
    
[^192]: 可解释的序列分类通过原型轨迹

    Interpretable Sequence Classification Via Prototype Trajectory. (arXiv:2007.01777v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2007.01777](http://arxiv.org/abs/2007.01777)

    ProtoryNet是一种基于原型轨迹的可解释深度神经网络，它通过捕捉时间模式和原型的近似程度来进行文本分类，并实现了直观和细致的推理过程解释。

    

    我们提出了一种新颖的用于文本分类的可解释深度神经网络，称为ProtoryNet，它基于原型轨迹的新概念。受现代语言学中的原型理论的启发，ProtoryNet通过为文本序列中的每个句子找到最相似的原型，并将每个句子与相应的活动原型的接近程度输入到RNN主干中进行预测。然后，RNN主干捕捉到原型的时间模式，我们称之为原型轨迹。原型轨迹能够直观而细致地解释RNN模型的推理过程，类似于人类分析文本的方式。我们还设计了原型修剪过程，以减少模型使用的原型总数，以提高解释性。在多个公共数据集上的实验证明，ProtoryNet比基线的基于原型的深度神经网络更准确，并减少了与现有模型相比的性能差距。

    We propose a novel interpretable deep neural network for text classification, called ProtoryNet, based on a new concept of prototype trajectories. Motivated by the prototype theory in modern linguistics, ProtoryNet makes a prediction by finding the most similar prototype for each sentence in a text sequence and feeding an RNN backbone with the proximity of each sentence to the corresponding active prototype. The RNN backbone then captures the temporal pattern of the prototypes, which we refer to as prototype trajectories. Prototype trajectories enable intuitive and fine-grained interpretation of the reasoning process of the RNN model, in resemblance to how humans analyze texts. We also design a prototype pruning procedure to reduce the total number of prototypes used by the model for better interpretability. Experiments on multiple public data sets show that ProtoryNet is more accurate than the baseline prototype-based deep neural net and reduces the performance gap compared to state-o
    

