# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Multivariate Probabilistic Time Series Forecasting with Correlated Errors](https://rss.arxiv.org/abs/2402.01000) | 本文提出了一种方法，基于低秩加对角线参数化协方差矩阵，可以有效地刻画时间序列预测中误差的自相关性，并具有复杂度低、校准预测准确性高等优点。 |
| [^2] | [Multi-Scale Protein Language Model for Unified Molecular Modeling](https://arxiv.org/abs/2403.12995) | 提出了一种多尺度蛋白质语言模型，能够在蛋白质和小分子相关任务中超越先前的方法，并且通过统一分子建模实现了充分利用蛋白质语言模型的潜力。 |
| [^3] | [The Power of Few: Accelerating and Enhancing Data Reweighting with Coreset Selection](https://arxiv.org/abs/2403.12166) | 提出一种利用核心子集选择进行数据重新加权的方法，有效优化了计算时间和模型性能，突显其作为模型训练的可扩展和精确解决方案的潜力。 |
| [^4] | [Solvent-Aware 2D NMR Prediction: Leveraging Multi-Tasking Training and Iterative Self-Training Strategies](https://arxiv.org/abs/2403.11353) | 该论文提出了一种利用迭代自我训练方法来训练深度学习模型，从而解决二维核磁共振（2D NMR）预测中的挑战，弥补了缺乏标注NMR训练数据集的不足。 |
| [^5] | [NLP Verification: Towards a General Methodology for Certifying Robustness](https://arxiv.org/abs/2403.10144) | 本文尝试总结和评估由该领域迄今进展而形成的NLP验证流程的一般组成部分，贡献在于提出了将句子嵌入连续空间得到的可验证子空间的一般描述。 |
| [^6] | [Quality-Diversity Actor-Critic: Learning High-Performing and Diverse Behaviors via Value and Successor Features Critics](https://arxiv.org/abs/2403.09930) | QDAC是一种基于离策略演员-评论家深度强化学习算法，通过价值函数评论家和继承特征评论家学习高性能和多样性行为。 |
| [^7] | [Unity by Diversity: Improved Representation Learning in Multimodal VAEs](https://arxiv.org/abs/2403.05300) | 通过软约束取代硬约束，提出了一种新的专家混合先验，改善了多模态VAEs中的表示学习。 |
| [^8] | [MedFLIP: Medical Vision-and-Language Self-supervised Fast Pre-Training with Masked Autoencoder](https://arxiv.org/abs/2403.04626) | MedFLIP是一种用于医学分析的快速语言-图像预训练方法，通过引入SVD损失增强医学图像特征表示学习，验证了用语言可以提高零样本医学图像分析的性能。 |
| [^9] | [GUIDE: Guidance-based Incremental Learning with Diffusion Models](https://arxiv.org/abs/2403.03938) | GUIDE利用扩散模型和分类器引导技术，针对持续训练模型遗忘的信息产生复习示例，显著减少了灾难性遗忘。 |
| [^10] | [Large-scale variational Gaussian state-space models](https://arxiv.org/abs/2403.01371) | 该论文介绍了一种针对具有高斯噪声驱动非线性动力学的状态空间模型的大规模变分算法和结构化逼近方法，可以有效评估ELBO和获取低方差的随机梯度估计，通过利用低秩蒙特卡罗逼近和推断网络的精度矩阵更新，将近似平滑问题转化为近似滤波问题。 |
| [^11] | [Scale-Invariant Gradient Aggregation for Constrained Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2403.00282) | 提出了一种名为CoMOGA的约束多目标梯度聚合算法，通过将目标转换为约束，实现了对帕累托最优策略的求解，同时满足预定义约束。 |
| [^12] | [BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning](https://arxiv.org/abs/2402.17810) | BioT5+是BioT5框架的扩展，通过整合IUPAC名称、包含广泛生物文本和分子数据、多任务指令调整以及新颖的数值标记技术，实现了分子表示与文本之间的联系。 |
| [^13] | [Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models](https://arxiv.org/abs/2402.15938) | 本文提出了一种通过LLMs输出分布进行污染检测的方法CDD，以及一种基于LLMs输出修正的可信评估方法TED，以应对大语言模型在数据污染和可信评估方面面临的挑战。 |
| [^14] | [Open Ad Hoc Teamwork with Cooperative Game Theory](https://arxiv.org/abs/2402.15259) | 提出了采用合作博弈论解释开放式即兴团队合作中联合Q值表示的新理论，为进一步发展这一研究方向和应用提供了新思路 |
| [^15] | [Quantum Theory and Application of Contextual Optimal Transport](https://arxiv.org/abs/2402.14991) | 提出了一种首创的量子计算公式，用于情境化输送计划的摊销优化，并通过预测背景情境中药物剂量参数化的细胞类型分布的变化来验证方法，展示了捕捉剂量引起的细胞分布变化的能力。 |
| [^16] | [Non-asymptotic Convergence of Discrete-time Diffusion Models: New Approach and Improved Rate](https://arxiv.org/abs/2402.13901) | 本文提出了离散时间扩散模型的新方法，改进了对更大类的分布的收敛保证，并提高了具有有界支撑的分布的收敛速率。 |
| [^17] | [Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization](https://arxiv.org/abs/2402.12550) | 多线性专家混合（MMoE）层通过因式分解针对视觉模型提供了一种可扩展的专家特化解决方案，避免了离散专家路由和过高推理时间成本。 |
| [^18] | [Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement](https://arxiv.org/abs/2402.12146) | Meta Ranking方法通过比较目标查询-响应对与参考查询-响应对来使较不具备能力的语言模型有效地评估单个响应的可靠性。 |
| [^19] | [Performative Reinforcement Learning in Gradually Shifting Environments](https://arxiv.org/abs/2402.09838) | 这项研究提出了一种在渐变环境中进行强化学习的框架，可以模拟部署策略对环境的影响，并提出了一种新的算法MDRR来应对这种情况。 |
| [^20] | [Best Arm Identification for Prompt Learning under a Limited Budget](https://arxiv.org/abs/2402.09723) | 这项工作提出了一种在提示学习中考虑有限预算约束的方法，通过建立提示学习和多臂赌博机中固定预算最佳臂识别之间的联系，提出了一个通用框架TRIPLE，通过利用聚类和嵌入思想实现了两个增强方法。 |
| [^21] | [API Pack: A Massive Multilingual Dataset for API Call Generation](https://arxiv.org/abs/2402.09615) | 这个论文介绍了一个名为API Pack的大规模多语言数据集，旨在提高大型语言模型的API调用生成能力，通过实验证明了其在生成未见过的API调用方面的高准确率，并实现了跨语言的API调用生成 |
| [^22] | [End-to-End Training Induces Information Bottleneck through Layer-Role Differentiation: A Comparative Analysis with Layer-wise Training](https://arxiv.org/abs/2402.09050) | 本文通过与逐层训练的比较，重新考虑了为什么端到端训练表现出优越的性能，分析了信息传播方面的优势，并对训练模型的性质差异进行了深入理解。 |
| [^23] | [An Accelerated Gradient Method for Simple Bilevel Optimization with Convex Lower-level Problem](https://arxiv.org/abs/2402.08097) | 本文提出了一种加速梯度方法来解决具有凸下层问题的简单双层优化问题，通过局部逼近下层问题的解集和加速梯度更新方法，在有限次迭代内找到一个具有一定精度的最优解。 |
| [^24] | [Resampling methods for Private Statistical Inference](https://arxiv.org/abs/2402.07131) | 这项研究提出了两种私有变体的非参数bootstrap方法，用于在差分隐私的情况下构建置信区间。方法在计算效率和置信区间长度上相比现有方法有显著改进。 |
| [^25] | [A Tale of Tails: Model Collapse as a Change of Scaling Laws](https://arxiv.org/abs/2402.07043) | 本文通过尺度律的视角，研究了AI模型大小增长和合成数据引入对模型性能的影响，发现模型可能会遭遇总体崩溃，并通过大规模实验证实了这一理论。 |
| [^26] | [Dirichlet Flow Matching with Applications to DNA Sequence Design](https://arxiv.org/abs/2402.05841) | 本文提出了一种使用Dirichlet流匹配的方法，在DNA序列设计中取得卓越的性能，比所有基线模型在分布度量和生成的序列设计目标方面表现更好。 |
| [^27] | [Online Cascade Learning for Efficient Inference over Streams](https://arxiv.org/abs/2402.04513) | 这项研究提出了在线级联学习的方法，通过学习一个“级联”模型，从容量较低的模型到强大的语言模型，以及推迟策略，可以在流数据处理中同时保证准确性和降低推理成本。 |
| [^28] | [GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models](https://arxiv.org/abs/2402.03299) | 本论文提出了一个通过角色扮演的系统，可以生成自然语言越狱，用于测试大型语言模型的指南遵循情况。系统通过收集现有越狱并将其组织成知识图来生成新的越狱，证明了其高效性和有效性。 |
| [^29] | [Graph Foundation Models](https://arxiv.org/abs/2402.02216) | 图基础模型是图领域中一个新兴的研究课题，旨在开发一个能够跨不同图和任务进行泛化的图模型。我们提出了一种新的 GFM 发展视角，通过倡导“图词汇表”来编码图的不变性，有助于推进未来的GFM设计。 |
| [^30] | [Random Linear Projections Loss for Hyperplane-Based Optimization in Neural Networks](https://arxiv.org/abs/2311.12356) | 本研究引入了一种名为随机线性投影（RLP）损失的新方法，通过利用数据中的几何关系来提高神经网络训练效率。实证评估表明，使用RLP损失训练的神经网络优于传统损失函数训练的网络，在更少的数据样本下实现更好的性能，并且对于添加噪声表现更强鲁棒性。 |
| [^31] | [Pre- to Post-Contrast Breast MRI Synthesis for Enhanced Tumour Segmentation](https://arxiv.org/abs/2311.10879) | 通过生成对抗网络（GAN）在乳腺MRI中合成对比增强，引入了Scaled Aggregate Measure (SAMe)进行量化评估，并成功应用于乳腺肿瘤分割任务。 |
| [^32] | [Towards probabilistic Weather Forecasting with Conditioned Spatio-Temporal Normalizing Flows](https://arxiv.org/abs/2311.06958) | 该论文提出了一种基于条件化归一化流的概率天气预测方法，通过实验证明其能够捕捉和良好外推空间-时间相关性。 |
| [^33] | [Learning to Learn for Few-shot Continual Active Learning](https://arxiv.org/abs/2311.03732) | 提出了一种简单而高效的方法，Meta-Continual Active Learning，通过元学习和经验重播解决少样本持续主动学习中的任务混淆和灾难性遗忘，进一步结合文本增强来确保泛化。 |
| [^34] | [Harmonic Self-Conditioned Flow Matching for Multi-Ligand Docking and Binding Site Design](https://arxiv.org/abs/2310.05764) | 该研究开发了一种谐波自调流匹配方法，在多配体对接和结合位点设计中表现出比现有方法更好的生成过程和设计效果。 |
| [^35] | [Identification and Estimation of Conditional Average Partial Causal Effects via Instrumental Variable.](http://arxiv.org/abs/2401.11130) | 本文提出了一种通过工具变量法识别和估计连续处理的因果效应异质性的方法，并开发了三类相应的估计器，并对其进行了统计性质分析。 |
| [^36] | [Relaxed Contrastive Learning for Federated Learning.](http://arxiv.org/abs/2401.04928) | 我们提出了一种放松的对比学习框架，用于解决联邦学习中的数据异构性挑战。我们的方法通过引入放松的对比学习损失，防止表示坍缩，增强特征的可传递性，从而实现了显著的性能提升。 |
| [^37] | [Structure-focused Neurodegeneration Convolutional Neural Network for Modeling and Classification of Alzheimer's Disease.](http://arxiv.org/abs/2401.03922) | 本论文提出了一种结构化神经退行性卷积神经网络，用于AD和MCI的识别。该网络考虑了局部结构特征，可以更准确地进行早期AD的诊断。 |
| [^38] | [Graph Convolutions Enrich the Self-Attention in Transformers!.](http://arxiv.org/abs/2312.04234) | 这项研究通过引入图卷积来改进Transformer模型中的自注意力机制，并在计算机视觉、自然语言处理等多个领域展示了其性能提升。 |
| [^39] | [Simplifying Transformer Blocks.](http://arxiv.org/abs/2311.01906) | 本论文通过结合信号传播理论和实证观察，提出了一种简化Transformer块的方法。这种简化方法可以去除许多不影响训练速度的组件，并且在保持性能的同时比标准Transformer块的训练吞吐量提高了15%。 |
| [^40] | [Detecting Out-of-Distribution Through the Lens of Neural Collapse.](http://arxiv.org/abs/2311.01479) | 通过观察到的群内特征聚集和群外特征离散的性质，本论文提出了一种基于特征和权重向量接近程度的神经坍塌（NC-OOD）检测器来提高OAD检测的泛化能力，并取得了最先进的效果。 |
| [^41] | [Equivariant Deep Weight Space Alignment.](http://arxiv.org/abs/2310.13397) | 本论文提出了一个名为Deep-Align的新框架，用于学习解决权重对齐问题，以加速对齐过程并提高其质量。 |
| [^42] | [Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers.](http://arxiv.org/abs/2310.02905) | 该论文提出了一种使用神经探测器和转换器优化指令的方法，以提高大型语言模型的性能。 |
| [^43] | [Data Cleaning and Machine Learning: A Systematic Literature Review.](http://arxiv.org/abs/2310.01765) | 本文系统综述了数据清洗与机器学习的关系，并总结了最新的数据清洗方法和ML的应用领域。在这两个领域存在着互相促进的关系，研究人员提出了未来的研究方向和建议。 |
| [^44] | [Primal-Dual Continual Learning: Stability and Plasticity through Lagrange Multipliers.](http://arxiv.org/abs/2310.00154) | 本文提出了原始-对偶持续学习方法，通过利用拉格朗日对偶解决受限学习问题，实现了稳定性和可塑性。作者通过分析任务层面和样本层面的约束，在基于记忆的方法中分配资源，取得了较好的效果。 |
| [^45] | [High-dimensional robust regression under heavy-tailed data: Asymptotics and Universality.](http://arxiv.org/abs/2309.16476) | 本文研究了在高维度和重尾干扰条件下的鲁棒回归估计器的性质，通过研究一类椭圆协变量和噪声数据分布的M-估计器，我们发现在存在重尾噪声的情况下，Huber损失需要进一步正则化才能达到最优性能。同时，我们还推导出了岭回归的超额风险的衰减速率。 |
| [^46] | [BayOTIDE: Bayesian Online Multivariate Time series Imputation with functional decomposition.](http://arxiv.org/abs/2308.14906) | BayOTIDE是一种基于贝叶斯方法的在线多元时间序列插补与函数分解模型，通过将多元时间序列视为低秩时序因子组的加权组合来进行插补，同时解决了全局趋势和周期性模式的忽略以及不规则采样时间序列的处理问题。 |
| [^47] | [Learning to Model the World with Language.](http://arxiv.org/abs/2308.01399) | 本论文提出了一种通过语言学习对世界进行建模的方法，利用语言帮助代理器预测未来并进行行动。通过学习多模态世界模型，代理器可以预测未来的文本和图像表示，并在模型回滚中进行行动。 |
| [^48] | [AMEE: A Robust Framework for Explanation Evaluation in Time Series Classification.](http://arxiv.org/abs/2306.05501) | AMEE是一个模型无关的解释评价框架，用于量化和比较时间序列分类中多种基于显著性的解释方法的信息价值，帮助解决在这一领域中解释方法选择的难题。 |
| [^49] | [Stochastic Multi-Level Compositional Optimization Algorithms over Networks with Level-Independent Convergence Rate.](http://arxiv.org/abs/2306.03322) | 本文提出了面向网络的随机多层组合优化算法，其中包括两种新的分散式优化算法，可实现独立于层数的收敛速度，理论结果和实验证明它们更加有效。 |
| [^50] | [Permutation Decision Trees.](http://arxiv.org/abs/2306.02617) | 该论文提出了一种称为排列决策树的模型，通过引入一种新的复杂度度量方法，能够捕捉到数据实例的顺序依赖性，在不同排列的数据实例上得到不同的决策树模型，从而克服了传统决策树模型在处理顺序相关数据时的限制。 |
| [^51] | [Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers.](http://arxiv.org/abs/2305.15805) | 本研究提出了一种动态上下文剪枝方法，可以在保持模型表现力的同时，动态减少无效信息，提高模型的效率和可解释性。该技术可以应用于现有的预训练模型，并且可以通过简单的微调过程实现。 |
| [^52] | [LMs with a Voice: Spoken Language Modeling beyond Speech Tokens.](http://arxiv.org/abs/2305.15255) | SPECTRON是一个新颖的语音延续模型，通过利用预训练的语言模型和语音编码器进行端到端的训练来生成文本和语音输出，在语义内容和讲话者保护方面超越了现有的口语语言模型。 |
| [^53] | [Characterizing Long-Tail Categories on Graphs.](http://arxiv.org/abs/2305.09938) | 该研究提出了图上长尾分类的第一个泛化边界，并提出了一种可表征长尾类别的行为并提高机器学习模型在现实世界网络中的泛化性能的新图表示学习框架。 |
| [^54] | [Understanding Model Averaging in Federated Learning on Heterogeneous Data.](http://arxiv.org/abs/2305.07845) | 本文研究了异构数据联邦学习中的模型平均技术，通过可视化损失/错误景观揭示了客户端模型环绕全局模型在一个共同的盆地内，并且发现全局模型在早期训练后的误差主要来自客户端数据集和全局数据集之间非重叠的数据及全局模型与客户端模型之间的最大距离两个因素。 |
| [^55] | [Dynamic Transfer Learning across Graphs.](http://arxiv.org/abs/2305.00664) | 该论文提出了一个新的问题：在动态图形环境下如何有效地进行跨图迁移学习，主要解决了领域演化对泛化性能的影响。 |
| [^56] | [Federated Compositional Deep AUC Maximization.](http://arxiv.org/abs/2304.10101) | 本论文介绍了一种通过直接优化AUC分数来解决联邦学习中不平衡数据问题的方法，并开发了一个随机组合梯度下降上升动量算法。通过广泛的实验验证，证实了该方法的效率和有效性。 |
| [^57] | [Improving Monte Carlo Evaluation with Offline Data.](http://arxiv.org/abs/2301.13734) | 本论文介绍了通过使用离线数据来提升蒙特卡罗评估方法，实现在保持相同估计准确度的前提下，减少在线样本数量的目的。通过使用一个定制的行为策略，可以比普通的 MC 估计器产生更小的方差。该行为策略可以从现有的离线数据中高效学习，我们的实验表明，相对于现有的最先进方法，我们的方法只需使用小部分在线样本就能实现相同的估计精度。 |
| [^58] | [RFold: RNA Secondary Structure Prediction with Decoupled Optimization.](http://arxiv.org/abs/2212.14041) | 所提出的RFold方法采用解耦优化过程和注意力机制进行简单又有效的RNA二级结构预测，具有较高的准确性和速度。 |
| [^59] | [Active Inference and Reinforcement Learning: A unified inference on continuous state and action spaces under partially observability.](http://arxiv.org/abs/2212.07946) | 本论文提出了一种能够在部分可观测的连续状态和动作空间下进行统一推理的框架，通过最小化期望自由能函数指导代理选择动作，以实现最大化奖励的决策制定。 |
| [^60] | [Accuracy Boosters: Epoch-Driven Mixed-Mantissa Block Floating-Point for DNN Training.](http://arxiv.org/abs/2211.10737) | 本文提出了一种基于时代驱动的混合尾数HBFP技术，通过对不同参数的探索和优化，实现了对DNN训练中算术操作的更小编码。使用分析模型表明，该方法能够将HBFP训练加速器的算术密度增加高达$21.3\times$。 |
| [^61] | [Efficient Video Representation Learning via Motion-Aware Token Selection.](http://arxiv.org/abs/2211.10636) | 该论文提出了一种新的运动感知标记选择方法，针对视频中不同补丁的信息密度，选择包含丰富动态特性的标记，放弃无效的标记，从而大大降低计算和存储需求，实现了在单台机器上进行预训练和微调而不影响性能。 |
| [^62] | [The Common Intuition to Transfer Learning Can Win or Lose: Case Studies for Linear Regression.](http://arxiv.org/abs/2103.05621) | 本论文研究了传输学习的基本过程，针对线性回归任务，通过利用源任务参数和目标任务训练数据，提出了一种传输学习方法。我们分析了该方法的泛化性能，并展示了其在解决线性回归中的泛化误差峰值方面的能力。此外，我们证明了在足够相关的任务中，该传输学习方法可以优于岭回归方法。 |

# 详细

[^1]: 多元概率时间序列预测与相关误差

    Multivariate Probabilistic Time Series Forecasting with Correlated Errors

    [https://rss.arxiv.org/abs/2402.01000](https://rss.arxiv.org/abs/2402.01000)

    本文提出了一种方法，基于低秩加对角线参数化协方差矩阵，可以有效地刻画时间序列预测中误差的自相关性，并具有复杂度低、校准预测准确性高等优点。

    

    建模误差之间的相关性与模型能够准确量化概率时间序列预测中的预测不确定性密切相关。最近的多元模型在考虑误差之间的同时相关性方面取得了显著进展，然而，对于统计简化的目的，对这些误差的常见假设是它们在时间上是独立的。然而，实际观测往往偏离了这个假设，因为误差通常由于各种因素（如排除时间相关的协变量）而表现出显著的自相关性。在这项工作中，我们提出了一种基于低秩加对角线参数化协方差矩阵的高效方法，可以有效地刻画误差的自相关性。所提出的方法具有几个可取的特性：复杂度不随时间序列数目增加，得到的协方差可以用于校准预测，且具有较好的性能。

    Modeling the correlations among errors is closely associated with how accurately the model can quantify predictive uncertainty in probabilistic time series forecasting. Recent multivariate models have made significant progress in accounting for contemporaneous correlations among errors, while a common assumption on these errors is that they are temporally independent for the sake of statistical simplicity. However, real-world observations often deviate from this assumption, since errors usually exhibit substantial autocorrelation due to various factors such as the exclusion of temporally correlated covariates. In this work, we propose an efficient method, based on a low-rank-plus-diagonal parameterization of the covariance matrix, which can effectively characterize the autocorrelation of errors. The proposed method possesses several desirable properties: the complexity does not scale with the number of time series, the resulting covariance can be used for calibrating predictions, and i
    
[^2]: 多尺度蛋白质语言模型用于统一分子建模

    Multi-Scale Protein Language Model for Unified Molecular Modeling

    [https://arxiv.org/abs/2403.12995](https://arxiv.org/abs/2403.12995)

    提出了一种多尺度蛋白质语言模型，能够在蛋白质和小分子相关任务中超越先前的方法，并且通过统一分子建模实现了充分利用蛋白质语言模型的潜力。

    

    蛋白质语言模型在蛋白质工程领域展现出了显著潜力。然而，当前的蛋白质语言模型主要在残基级别上运行，这限制了它们在原子水平提供信息的能力。这一限制阻碍了我们充分利用蛋白质语言模型在涉及蛋白质和小分子的应用中的能力。本文提出了ms-ESM（多尺度ESM），一种新颖的方法，实现了多尺度统一分子建模。ms-ESM通过预训练多尺度代码切换蛋白质序列并利用多尺度位置编码来捕获残基和原子之间的关系来实现这一目标。实验结果表明，ms-ESM在蛋白质-分子任务中超越了先前的方法，展示了对蛋白质语言模型的充分利用。进一步的研究揭示通过统一分子建模，ms-ESM不

    arXiv:2403.12995v1 Announce Type: cross  Abstract: Protein language models have demonstrated significant potential in the field of protein engineering. However, current protein language models primarily operate at the residue scale, which limits their ability to provide information at the atom level. This limitation prevents us from fully exploiting the capabilities of protein language models for applications involving both proteins and small molecules. In this paper, we propose ms-ESM (multi-scale ESM), a novel approach that enables multi-scale unified molecular modeling. ms-ESM achieves this by pre-training on multi-scale code-switch protein sequences and utilizing a multi-scale position encoding to capture relationships among residues and atoms. Experimental results indicate that ms-ESM surpasses previous methods in protein-molecule tasks, demonstrating the full utilization of protein language models. Further investigations reveal that through unified molecular modeling, ms-ESM not 
    
[^3]: 少数个体的力量：利用核心子集选择加速和优化数据重新加权

    The Power of Few: Accelerating and Enhancing Data Reweighting with Coreset Selection

    [https://arxiv.org/abs/2403.12166](https://arxiv.org/abs/2403.12166)

    提出一种利用核心子集选择进行数据重新加权的方法，有效优化了计算时间和模型性能，突显其作为模型训练的可扩展和精确解决方案的潜力。

    

    随着机器学习任务不断发展，趋势是收集更大的数据集并训练规模越来越大的模型。虽然这提高了准确性，但也将计算成本提高到不可持续的水平。针对这一问题，我们的工作旨在在计算效率和模型准确性之间取得微妙的平衡，这是该领域中一直存在的挑战。我们引入了一种利用核心子集选择进行重新加权的新方法，有效优化了计算时间和模型性能。通过专注于 strategically selected coreset，我们的方法提供了一个稳健的表示，因为它有效地最小化了异常值的影响。然后，重新校准的权重被映射回并传播到整个数据集。我们的实验结果证实了这种方法的有效性，突显了它作为模型训练的可扩展和精确解决方案的潜力。

    arXiv:2403.12166v1 Announce Type: new  Abstract: As machine learning tasks continue to evolve, the trend has been to gather larger datasets and train increasingly larger models. While this has led to advancements in accuracy, it has also escalated computational costs to unsustainable levels. Addressing this, our work aims to strike a delicate balance between computational efficiency and model accuracy, a persisting challenge in the field. We introduce a novel method that employs core subset selection for reweighting, effectively optimizing both computational time and model performance. By focusing on a strategically selected coreset, our approach offers a robust representation, as it efficiently minimizes the influence of outliers. The re-calibrated weights are then mapped back to and propagated across the entire dataset. Our experimental results substantiate the effectiveness of this approach, underscoring its potential as a scalable and precise solution for model training.
    
[^4]: 溶剂感知的2D核磁共振预测：利用多任务训练和迭代自训练策略

    Solvent-Aware 2D NMR Prediction: Leveraging Multi-Tasking Training and Iterative Self-Training Strategies

    [https://arxiv.org/abs/2403.11353](https://arxiv.org/abs/2403.11353)

    该论文提出了一种利用迭代自我训练方法来训练深度学习模型，从而解决二维核磁共振（2D NMR）预测中的挑战，弥补了缺乏标注NMR训练数据集的不足。

    

    核磁共振（NMR）光谱在各个科学领域中起着关键作用，提供了有关分子的结构信息、电子性质和动态行为的见解。准确的NMR光谱预测能够高效地生成候选分子，使化学家能够将它们与实际实验光谱进行比较。该过程有助于确认分子结构或指出差异，引导进一步的研究。机器学习（ML）已经成为一种有前途的替代方法，用于根据分子结构预测分子的原子NMR化学位移。虽然在预测一维（1D）NMR方面已经取得了显著进展，但通过机器学习进行二维（2D）NMR预测仍然是一项挑战，因为缺乏用于训练的标注的NMR数据集。为了解决这一差距，我们提出了一种迭代自训练（IST）方法，用于训练深度学习模型，以预测原子2DNMR位移。

    arXiv:2403.11353v1 Announce Type: cross  Abstract: Nuclear magnetic resonance (NMR) spectroscopy plays a pivotal role in various scientific fields, offering insights into structural information, electronic properties and dynamic behaviors of molecules. Accurate NMR spectrum prediction efficiently produces candidate molecules, enabling chemists to compare them with actual experimental spectra. This process aids in confirming molecular structures or pinpointing discrepancies, guiding further investigation. Machine Learning (ML) has then emerged as a promising alternative approach for predicting atomic NMR chemical shits of molecules given their structures. Although significant progresses have been made in predicting one-dimensional (1D) NMR, two-dimensional (2D) NMR prediction via ML remains a challenge due to the lack of annotated NMR training datasets. To address this gap, we propose an iterative self-training (IST) approach to train a deep learning model for predicting atomic 2DNMR sh
    
[^5]: NLP验证：走向一种通用的用于认证鲁棒性的方法论

    NLP Verification: Towards a General Methodology for Certifying Robustness

    [https://arxiv.org/abs/2403.10144](https://arxiv.org/abs/2403.10144)

    本文尝试总结和评估由该领域迄今进展而形成的NLP验证流程的一般组成部分，贡献在于提出了将句子嵌入连续空间得到的可验证子空间的一般描述。

    

    深度神经网络在自然语言处理（NLP）领域取得了显著成功，确保它们的安全性和可靠性至关重要：在安全关键的情境中，这些模型必须对变化或攻击具有鲁棒性，并能对其输出给出保证。与计算机视觉不同，NLP缺乏一个统一的验证方法论，尽管近年来文献中取得了一些进展，但对于NLP验证的实用问题常常涉及不深。在本文中，我们尝试提炼和评估一个NLP验证流程的一般组成部分，该流程来源于迄今为止该领域的进展。我们的贡献有两方面：首先，我们给出了将句子嵌入连续空间得到的可验证子空间的一般描述。我们确定了可验证子空间的语义泛化技术挑战，并提出了一种有效处理的方法。

    arXiv:2403.10144v1 Announce Type: cross  Abstract: Deep neural networks have exhibited substantial success in the field of Natural Language Processing (NLP) and ensuring their safety and reliability is crucial: there are safety critical contexts where such models must be robust to variability or attack, and give guarantees over their output. Unlike Computer Vision, NLP lacks a unified verification methodology and, despite recent advancements in literature, they are often light on the pragmatical issues of NLP verification. In this paper, we make an attempt to distil and evaluate general components of an NLP verification pipeline, that emerges from the progress in the field to date. Our contributions are two-fold. Firstly, we give a general characterisation of verifiable subspaces that result from embedding sentences into continuous spaces. We identify, and give an effective method to deal with, the technical challenge of semantic generalisability of verified subspaces; and propose it a
    
[^6]: 质量多样性演员-评论家：通过值和继承特征评论家学习高性能和多样性行为

    Quality-Diversity Actor-Critic: Learning High-Performing and Diverse Behaviors via Value and Successor Features Critics

    [https://arxiv.org/abs/2403.09930](https://arxiv.org/abs/2403.09930)

    QDAC是一种基于离策略演员-评论家深度强化学习算法，通过价值函数评论家和继承特征评论家学习高性能和多样性行为。

    

    智能的一个关键方面是表现出适应意外情况的广泛行为谱。过去十年，深度强化学习的进步取得了突破性成就，用于解决复杂的连续控制任务。然而，大多数方法只返回一个专门针对特定问题的解决方案。我们引入了质量多样性演员-评论家（QDAC），这是一种基于离策略演员-评论家深度强化学习算法，利用价值函数评论家和继承特征评论家学习高性能和多样性行为。在这个框架中，演员通过受限优化来最大化回报并执行多样性技能的客观函数，无缝统一了两个评论家。与其他质量多样性方法相比，QDAC在六个具有挑战性的连续控制运动任务上实现了显着更高的性能和更多样性的行为。

    arXiv:2403.09930v1 Announce Type: cross  Abstract: A key aspect of intelligence is the ability to demonstrate a broad spectrum of behaviors for adapting to unexpected situations. Over the past decade, advancements in deep reinforcement learning have led to groundbreaking achievements to solve complex continuous control tasks. However, most approaches return only one solution specialized for a specific problem. We introduce Quality-Diversity Actor-Critic (QDAC), an off-policy actor-critic deep reinforcement learning algorithm that leverages a value function critic and a successor features critic to learn high-performing and diverse behaviors. In this framework, the actor optimizes an objective that seamlessly unifies both critics using constrained optimization to (1) maximize return, while (2) executing diverse skills. Compared with other Quality-Diversity methods, QDAC achieves significantly higher performance and more diverse behaviors on six challenging continuous control locomotion 
    
[^7]: 多模态VAEs中的统一多样性：改进的表示学习

    Unity by Diversity: Improved Representation Learning in Multimodal VAEs

    [https://arxiv.org/abs/2403.05300](https://arxiv.org/abs/2403.05300)

    通过软约束取代硬约束，提出了一种新的专家混合先验，改善了多模态VAEs中的表示学习。

    

    多模态数据的变分自编码器在数据分析的许多任务中表现出潜力，如表示学习、有条件生成和填补。目前的架构要么跨模态共享编码器输出、解码器输入，要么两者都要学习共享表示。这样的架构对模型施加了严格约束。在这项工作中，我们展示了通过用软约束取代这些硬约束可以获得更好的潜在表示。我们提出了一种新的专家混合先验，软性地引导每个模态的潜在表示朝着共享的后验。这种方法导致了优秀的潜在表示，并允许每个编码保留来自其未压缩原始特征更好的信息。通过对多个基准数据集和一个具有挑战性的现实世界神经科学数据集进行的广泛实验，我们展示了改进的学习潜在表示和填补。

    arXiv:2403.05300v1 Announce Type: cross  Abstract: Variational Autoencoders for multimodal data hold promise for many tasks in data analysis, such as representation learning, conditional generation, and imputation. Current architectures either share the encoder output, decoder input, or both across modalities to learn a shared representation. Such architectures impose hard constraints on the model. In this work, we show that a better latent representation can be obtained by replacing these hard constraints with a soft constraint. We propose a new mixture-of-experts prior, softly guiding each modality's latent representation towards a shared aggregate posterior. This approach results in a superior latent representation and allows each encoding to preserve information from its uncompressed original features better. In extensive experiments on multiple benchmark datasets and a challenging real-world neuroscience data set, we show improved learned latent representations and imputation of m
    
[^8]: MedFLIP：医学视觉与语言自监督快速预训练与掩蔽自编码器

    MedFLIP: Medical Vision-and-Language Self-supervised Fast Pre-Training with Masked Autoencoder

    [https://arxiv.org/abs/2403.04626](https://arxiv.org/abs/2403.04626)

    MedFLIP是一种用于医学分析的快速语言-图像预训练方法，通过引入SVD损失增强医学图像特征表示学习，验证了用语言可以提高零样本医学图像分析的性能。

    

    在医学分析领域，广泛的研究探讨了掩蔽自编码器（MAEs）和多模态数据之间互相学习的潜力。然而，MAEs对跨模态学习的影响仍然是一个关键挑战。我们引入了MedFLIP，一种用于医学分析的快速语言-图像预训练方法。我们探索使用MAEs进行跨领域零样本学习，从而增强模型在医学诊断中常见的有限数据中学习的能力。我们验证了对图像进行掩蔽不会影响跨模态学习。此外，我们提出了SVD损失以增强医学图像特征的表示学习，旨在通过利用这类数据的结构复杂性来提高分类准确性。最后，我们验证了使用语言将提高医学图像分析的零样本性能。MedFLIP对掩蔽过程的扩展标志着该领域的进步。

    arXiv:2403.04626v1 Announce Type: cross  Abstract: Within the domain of medical analysis, extensive research has explored the potential of mutual learning between Masked Autoencoders(MAEs) and multimodal data. However, the impact of MAEs on intermodality remains a key challenge. We introduce MedFLIP, a Fast Language-Image Pre-training method for Medical analysis. We explore MAEs for zero-shot learning with crossed domains, which enhances the model ability to learn from limited data, a common scenario in medical diagnostics. We verify that masking an image does not affect intermodal learning. Furthermore, we propose the SVD loss to enhance the representation learning for characteristics of medical images, aiming to improve classification accuracy by leveraging the structural intricacies of such data. Lastly, we validate using language will improve the zero-shot performance for the medical image analysis. MedFLIP scaling of the masking process marks an advancement in the field, offering 
    
[^9]: GUIDE：基于引导的扩散模型增量学习

    GUIDE: Guidance-based Incremental Learning with Diffusion Models

    [https://arxiv.org/abs/2403.03938](https://arxiv.org/abs/2403.03938)

    GUIDE利用扩散模型和分类器引导技术，针对持续训练模型遗忘的信息产生复习示例，显著减少了灾难性遗忘。

    

    我们引入了GUIDE，一种新颖的持续学习方法，该方法指导扩散模型对有被遗忘风险的样本进行复习。现有的生成策略通过从生成模型中随机抽取复习样本来对抗灾难性遗忘。这种方法与基于缓冲区的方法相矛盾，其中采样策略起着重要作用。我们提出通过将扩散模型与分类器引导技术相结合，产生专门针对持续训练模型遗忘信息的复习示例，以弥合这一差距。这种方法使得能够从先前任务分布中生成样本，这些样本在最近遇到的类别情境下更有可能被误分类。我们的实验结果表明，GUIDE显著减少了灾难性遗忘，优于传统的随机抽样方法，并在持续学习方面超过了最近的先进方法。

    arXiv:2403.03938v1 Announce Type: new  Abstract: We introduce GUIDE, a novel continual learning approach that directs diffusion models to rehearse samples at risk of being forgotten. Existing generative strategies combat catastrophic forgetting by randomly sampling rehearsal examples from a generative model. Such an approach contradicts buffer-based approaches where sampling strategy plays an important role. We propose to bridge this gap by integrating diffusion models with classifier guidance techniques to produce rehearsal examples specifically targeting information forgotten by a continuously trained model. This approach enables the generation of samples from preceding task distributions, which are more likely to be misclassified in the context of recently encountered classes. Our experimental results show that GUIDE significantly reduces catastrophic forgetting, outperforming conventional random sampling approaches and surpassing recent state-of-the-art methods in continual learnin
    
[^10]: 大规模变分高斯状态空间模型

    Large-scale variational Gaussian state-space models

    [https://arxiv.org/abs/2403.01371](https://arxiv.org/abs/2403.01371)

    该论文介绍了一种针对具有高斯噪声驱动非线性动力学的状态空间模型的大规模变分算法和结构化逼近方法，可以有效评估ELBO和获取低方差的随机梯度估计，通过利用低秩蒙特卡罗逼近和推断网络的精度矩阵更新，将近似平滑问题转化为近似滤波问题。

    

    我们介绍了一种用于状态空间模型的嵌套变分推断算法和结构化变分逼近方法，其中非线性动力学由高斯噪声驱动。值得注意的是，所提出的框架允许在没有采用对角高斯逼近的情况下有效地评估ELBO和低方差随机梯度估计，通过利用（i）通过动力学对隐状态进行边缘化的蒙特卡罗逼近的低秩结构，（ii）一个推断网络，该网络通过低秩精度矩阵更新来近似更新步骤，（iii）将当前和未来观测编码为伪观测--将近似平滑问题转换为（更简单的）近似滤波问题。整体而言，必要的统计信息和ELBO可以在$O（TL（Sr+S^2+r^2））$时间内计算，其中$T$是系列长度，$L$是状态空间维数，$S$是用于逼近的样本数量。

    arXiv:2403.01371v1 Announce Type: cross  Abstract: We introduce an amortized variational inference algorithm and structured variational approximation for state-space models with nonlinear dynamics driven by Gaussian noise. Importantly, the proposed framework allows for efficient evaluation of the ELBO and low-variance stochastic gradient estimates without resorting to diagonal Gaussian approximations by exploiting (i) the low-rank structure of Monte-Carlo approximations to marginalize the latent state through the dynamics (ii) an inference network that approximates the update step with low-rank precision matrix updates (iii) encoding current and future observations into pseudo observations -- transforming the approximate smoothing problem into an (easier) approximate filtering problem. Overall, the necessary statistics and ELBO can be computed in $O(TL(Sr + S^2 + r^2))$ time where $T$ is the series length, $L$ is the state-space dimensionality, $S$ are the number of samples used to app
    
[^11]: 尺度不变梯度聚合用于受约束多目标强化学习

    Scale-Invariant Gradient Aggregation for Constrained Multi-Objective Reinforcement Learning

    [https://arxiv.org/abs/2403.00282](https://arxiv.org/abs/2403.00282)

    提出了一种名为CoMOGA的约束多目标梯度聚合算法，通过将目标转换为约束，实现了对帕累托最优策略的求解，同时满足预定义约束。

    

    多目标强化学习(MORL)的目标是找到一组帕累托最优策略，以涵盖各种偏好。然而，在实际应用中应用MORL，找到的策略不仅要帕累托最优，还要满足预定义的安全约束。为此，我们提出了一种名为约束多目标梯度聚合器(Constrained Multi-Objective Gradient Aggregator, CoMOGA)的约束MORL(CMORL)算法。CoMOGA意识到同时处理多个目标和约束的困难，通过将目标转换为额外的约束，将原始CMORL问题放松成一个约束优化问题。这种新颖的转换过程确保转换后的约束对目标尺度不变，同时具有与原始目标相同的效果。我们展示了所提方法收敛到一个局部帕累托最优策略，同时满足预定义约束。

    arXiv:2403.00282v1 Announce Type: new  Abstract: Multi-objective reinforcement learning (MORL) aims to find a set of Pareto optimal policies to cover various preferences. However, to apply MORL in real-world applications, it is important to find policies that are not only Pareto optimal but also satisfy pre-defined constraints for safety. To this end, we propose a constrained MORL (CMORL) algorithm called Constrained Multi-Objective Gradient Aggregator (CoMOGA). Recognizing the difficulty of handling multiple objectives and constraints concurrently, CoMOGA relaxes the original CMORL problem into a constrained optimization problem by transforming the objectives into additional constraints. This novel transformation process ensures that the converted constraints are invariant to the objective scales while having the same effect as the original objectives. We show that the proposed method converges to a local Pareto optimal policy while satisfying the predefined constraints. Empirical eva
    
[^12]: BioT5+: 通过IUPAC集成和多任务调整实现广义生物理解

    BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning

    [https://arxiv.org/abs/2402.17810](https://arxiv.org/abs/2402.17810)

    BioT5+是BioT5框架的扩展，通过整合IUPAC名称、包含广泛生物文本和分子数据、多任务指令调整以及新颖的数值标记技术，实现了分子表示与文本之间的联系。

    

    最近计算生物学的研究趋势越来越集中于整合文本和生物实体建模，特别是在分子和蛋白质的背景下。然而，类似于BioT5的先前工作在跨越多样化任务和缺乏对分子结构的细致理解方面面临挑战，特别是在它们的文本表示（例如IUPAC）方面。本文介绍了BioT5+，这是BioT5框架的一个扩展，旨在增强生物研究和药物发现。 BioT5+包含几个新颖的特性：整合IUPAC名称以加深对分子的理解，包括来自bioRxiv和PubChem等源的广泛生物文本和分子数据，多任务指令调整以跨越多个任务，以及一种用于改进数字数据处理的新颖数值标记技术。 这些增强功能使BioT5+能够弥合分子表示和它们的文本之间的差距。

    arXiv:2402.17810v1 Announce Type: cross  Abstract: Recent research trends in computational biology have increasingly focused on integrating text and bio-entity modeling, especially in the context of molecules and proteins. However, previous efforts like BioT5 faced challenges in generalizing across diverse tasks and lacked a nuanced understanding of molecular structures, particularly in their textual representations (e.g., IUPAC). This paper introduces BioT5+, an extension of the BioT5 framework, tailored to enhance biological research and drug discovery. BioT5+ incorporates several novel features: integration of IUPAC names for molecular understanding, inclusion of extensive bio-text and molecule data from sources like bioRxiv and PubChem, the multi-task instruction tuning for generality across tasks, and a novel numerical tokenization technique for improved processing of numerical data. These enhancements allow BioT5+ to bridge the gap between molecular representations and their text
    
[^13]: 大语言模型的泛化或记忆：数据污染与可信评估

    Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models

    [https://arxiv.org/abs/2402.15938](https://arxiv.org/abs/2402.15938)

    本文提出了一种通过LLMs输出分布进行污染检测的方法CDD，以及一种基于LLMs输出修正的可信评估方法TED，以应对大语言模型在数据污染和可信评估方面面临的挑战。

    

    最近关于大语言模型（LLMs）令人印象深刻能力的说法通常是通过在开放获取的基准上进行评估来支持的。考虑到LLMs的训练数据的庞大规模和广泛来源，它可能明确或隐含地包含测试数据，导致LLMs更容易受到数据污染的影响。然而，由于训练数据的不透明性、模型的黑盒访问以及合成训练数据的快速增长，对于LLMs来说检测和减轻数据污染面临着重大挑战。在本文中，我们提出了CDD，即通过LLMs输出分布进行污染检测的CDD。CDD仅需要采样文本来检测数据污染，通过识别LLMs输出分布的峰值来进行检测。为了减轻评估中数据污染的影响，我们还提出了TED：基于LLMs输出修正的可信评估。

    arXiv:2402.15938v1 Announce Type: cross  Abstract: Recent statements about the impressive capabilities of large language models (LLMs) are usually supported by evaluating on open-access benchmarks. Considering the vast size and wide-ranging sources of LLMs' training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination. However, due to the opacity of training data, the black-box access of models, and the rapid growth of synthetic training data, detecting and mitigating data contamination for LLMs faces significant challenges. In this paper, we propose CDD, which stands for Contamination Detection via output Distribution for LLMs. CDD necessitates only the sampled texts to detect data contamination, by identifying the peakedness of LLM's output distribution. To mitigate the impact of data contamination in evaluation, we also present TED: Trustworthy Evaluation via output Distribution, based on the correction of LLM's outp
    
[^14]: 采用合作博弈论的开放式即兴团队合作

    Open Ad Hoc Teamwork with Cooperative Game Theory

    [https://arxiv.org/abs/2402.15259](https://arxiv.org/abs/2402.15259)

    提出了采用合作博弈论解释开放式即兴团队合作中联合Q值表示的新理论，为进一步发展这一研究方向和应用提供了新思路

    

    即兴团队合作面临着一个具有挑战性的问题，需要设计一个能够与队友协作但没有先前协调或联合训练的智能体。开放式即兴团队合作进一步复杂化了这一挑战，考虑了具有不断变化的队友数量的环境，即开放式团队。现有解决这一问题的最先进方法是基于图神经网络的策略学习（GPL），利用了图神经网络的泛化能力来处理无限数量的智能体，有效应对开放式团队。GPL的性能优于其他方法，但其联合Q值表示对解释造成了挑战，阻碍了进一步发展这一研究方向和应用。本文建立了一种新的理论，从合作博弈论的角度为GPL中采用的联合Q值表示提供了一种解释。基于我们的理论，我们提出了一种基于

    arXiv:2402.15259v1 Announce Type: cross  Abstract: Ad hoc teamwork poses a challenging problem, requiring the design of an agent to collaborate with teammates without prior coordination or joint training. Open ad hoc teamwork further complicates this challenge by considering environments with a changing number of teammates, referred to as open teams. The state-of-the-art solution to this problem is graph-based policy learning (GPL), leveraging the generalizability of graph neural networks to handle an unrestricted number of agents and effectively address open teams. GPL's performance is superior to other methods, but its joint Q-value representation presents challenges for interpretation, hindering further development of this research line and applicability. In this paper, we establish a new theory to give an interpretation for the joint Q-value representation employed in GPL, from the perspective of cooperative game theory. Building on our theory, we propose a novel algorithm based on
    
[^15]: 量子理论与情境最优输运的应用

    Quantum Theory and Application of Contextual Optimal Transport

    [https://arxiv.org/abs/2402.14991](https://arxiv.org/abs/2402.14991)

    提出了一种首创的量子计算公式，用于情境化输送计划的摊销优化，并通过预测背景情境中药物剂量参数化的细胞类型分布的变化来验证方法，展示了捕捉剂量引起的细胞分布变化的能力。

    

    最优输运（Optimal Transport，OT）推动了机器学习在许多领域的应用。在测量数据（$\mu$，$\nu$）与上下文变量 $p_i$ 耦合的情况下，我们可以努力学习一个可以通过可能看不见的上下文参数化的全局输运映射。现有方法利用神经最优输运，并在很大程度上依赖于Brenier定理。在这里，我们提出了一种首创的量子计算公式，用于情境化输送计划的摊销优化。我们利用双随机矩阵和酉算符之间的直接联系，从而找到了最优输运和量子计算之间的自然联系。我们通过对合成和真实数据的验证，预测通过药物剂量参数化的细胞类型分布的变化作为背景情境。我们将我们的方法与几个基准进行了比较，结果显示我们的方法可以捕捉到剂量引起的细胞分布变化，甚至在一定程度上。

    arXiv:2402.14991v1 Announce Type: new  Abstract: Optimal Transport (OT) has fueled machine learning (ML) applications across many domains. In cases where paired data measurements ($\mu$, $\nu$) are coupled to a context variable $p_i$ , one may aspire to learn a global transportation map that can be parameterized through a potentially unseen con-text. Existing approaches utilize Neural OT and largely rely on Brenier's theorem. Here, we propose a first-of-its-kind quantum computing formulation for amortized optimization of contextualized transportation plans. We exploit a direct link between doubly stochastic matrices and unitary operators thus finding a natural connection between OT and quantum computation. We verify our method on synthetic and real data, by predicting variations in cell type distributions parameterized through drug dosage as context. Our comparisons to several baselines reveal that our method can capture dose-induced variations in cell distributions, even to some exten
    
[^16]: 离散时间扩散模型的非渐近收敛：新方法和改进速率

    Non-asymptotic Convergence of Discrete-time Diffusion Models: New Approach and Improved Rate

    [https://arxiv.org/abs/2402.13901](https://arxiv.org/abs/2402.13901)

    本文提出了离散时间扩散模型的新方法，改进了对更大类的分布的收敛保证，并提高了具有有界支撑的分布的收敛速率。

    

    最近，去噪扩散模型作为一种强大的生成技术出现，将噪声转化为数据。理论上主要研究了连续时间扩散模型的收敛性保证，并且仅在文献中对具有有界支撑的分布的离散时间扩散模型进行了获得。本文为更大类的分布建立了离散时间扩散模型的收敛性保证，并进一步改进了对具有有界支撑的分布的收敛速率。特别地，首先为具有有限二阶矩的平滑和一般（可能非光滑）分布建立了收敛速率。然后将结果专门应用于一些有明确参数依赖关系的有趣分布类别，包括具有Lipschitz分数、高斯混合分布和具有有界支撑的分布。

    arXiv:2402.13901v1 Announce Type: new  Abstract: The denoising diffusion model emerges recently as a powerful generative technique that converts noise into data. Theoretical convergence guarantee has been mainly studied for continuous-time diffusion models, and has been obtained for discrete-time diffusion models only for distributions with bounded support in the literature. In this paper, we establish the convergence guarantee for substantially larger classes of distributions under discrete-time diffusion models and further improve the convergence rate for distributions with bounded support. In particular, we first establish the convergence rates for both smooth and general (possibly non-smooth) distributions having finite second moment. We then specialize our results to a number of interesting classes of distributions with explicit parameter dependencies, including distributions with Lipschitz scores, Gaussian mixture distributions, and distributions with bounded support. We further 
    
[^17]: 多线性专家混合：通过因式分解实现可扩展的专家特化

    Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization

    [https://arxiv.org/abs/2402.12550](https://arxiv.org/abs/2402.12550)

    多线性专家混合（MMoE）层通过因式分解针对视觉模型提供了一种可扩展的专家特化解决方案，避免了离散专家路由和过高推理时间成本。

    

    专家混合（MoE）范式提供了一种强大的方法，将难以理解的密集层分解为更小、模块化的计算，通常更易于人类解释、调试和编辑。然而，一个主要问题在于扩展专家数量的计算成本，以实现足够精细的专业化。本文提出了多线性专家混合（MMoE）层来解决这个问题，重点放在视觉模型上。MMoE层完全以因式化形式对庞大的权重张量进行隐式计算。因此，MMoEs既避免了在流行的“稀疏”MoE模型中离散专家路由所造成的问题，又不会引起“软”MoE替代方案中过高的推理时间成本。我们通过可视化和反事实干预，提供了定性和定量证据，证明了扩展MMoE层的效果。

    arXiv:2402.12550v1 Announce Type: cross  Abstract: The Mixture of Experts (MoE) paradigm provides a powerful way to decompose inscrutable dense layers into smaller, modular computations often more amenable to human interpretation, debugging, and editability. A major problem however lies in the computational cost of scaling the number of experts to achieve sufficiently fine-grained specialization. In this paper, we propose the Multilinear Mixutre of Experts (MMoE) layer to address this, focusing on vision models. MMoE layers perform an implicit computation on prohibitively large weight tensors entirely in factorized form. Consequently, MMoEs both (1) avoid the issues incurred through the discrete expert routing in the popular 'sparse' MoE models, yet (2) do not incur the restrictively high inference-time costs of 'soft' MoE alternatives. We present both qualitative and quantitative evidence (through visualization and counterfactual interventions respectively) that scaling MMoE layers wh
    
[^18]: Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement

    Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement

    [https://arxiv.org/abs/2402.12146](https://arxiv.org/abs/2402.12146)

    Meta Ranking方法通过比较目标查询-响应对与参考查询-响应对来使较不具备能力的语言模型有效地评估单个响应的可靠性。

    

    尽管大型语言模型（LLMs）在广泛任务中展现强大性能，但仍面临幻觉等可靠性挑战。先前的研究表明，像GPT-4这样高能力的LLMs在评估单个响应的可靠性方面是有效的，而较不具备能力的LLMs通常被调整来评估对相同查询的响应的相对可靠性。为了使较不具备能力的LLMs有效地评估单个响应的可靠性，我们提出了一种名为$\textit{Meta}$ $\textit{Ranking}$（MR）的新方法。与先前直接评估响应的方法不同，我们通过将目标查询-响应对与参考查询-响应对进行比较来实现判断。我们发现在推理任务的LLM响应的错误检测中，MR表现出显著的有效性，即使在没有微调的情况下，较不具备能力的LLMs也能胜过强基线。我们进一步证明MR可以被用

    arXiv:2402.12146v1 Announce Type: cross  Abstract: Although Large Language Models (LLMs) have demonstrated strong performance on a wide range of tasks, they still face reliability challenges such as hallucination. Previous studies reveal that highly capable LLMs like GPT-4 are effective in judging the reliability of individual responses, while less capable ones are often tuned to evaluate the relative reliability of responses to the same query. To enable less capable LLMs to effectively judge the reliability of individual responses, we propose a novel method named $\textit{Meta}$ $\textit{Ranking}$ (MR). Unlike previous methods, which assess the response directly, we achieve the judgement by comparing the target query-response pair with reference query-response pairs. We found its remarkable effectiveness in error detection for LLM responses on reasoning tasks, where less capable LLMs could outperform strong baselines, even without fine-tuning. We further demonstrate that MR can be use
    
[^19]: 渐变环境中的表演性强化学习

    Performative Reinforcement Learning in Gradually Shifting Environments

    [https://arxiv.org/abs/2402.09838](https://arxiv.org/abs/2402.09838)

    这项研究提出了一种在渐变环境中进行强化学习的框架，可以模拟部署策略对环境的影响，并提出了一种新的算法MDRR来应对这种情况。

    

    当强化学习（RL）代理在实践中部署时，它们可能会影响环境并改变其动态。当前的研究试图形式化建模这种现象，并在这些模型中分析学习算法。为此，我们提出了一个框架，其中当前的环境取决于部署策略及其先前的动态。这是Performative RL（PRL）[Mandal et al., 2023]的一种泛化。与PRL不同，我们的框架允许对环境逐渐调整到部署策略的情景进行建模。我们将表演性预测文献中的两种算法适应到我们的设置，并提出了一种新的算法称为混合延迟重复训练（MDRR）。我们给出了这些算法收敛的条件，并使用三个指标进行比较：重训练次数，逼近保证和每次部署的样本数。与之前的方法不同，MDRR结合了样本

    arXiv:2402.09838v1 Announce Type: new  Abstract: When Reinforcement Learning (RL) agents are deployed in practice, they might impact their environment and change its dynamics. Ongoing research attempts to formally model this phenomenon and to analyze learning algorithms in these models. To this end, we propose a framework where the current environment depends on the deployed policy as well as its previous dynamics. This is a generalization of Performative RL (PRL) [Mandal et al., 2023]. Unlike PRL, our framework allows to model scenarios where the environment gradually adjusts to a deployed policy. We adapt two algorithms from the performative prediction literature to our setting and propose a novel algorithm called Mixed Delayed Repeated Retraining (MDRR). We provide conditions under which these algorithms converge and compare them using three metrics: number of retrainings, approximation guarantee, and number of samples per deployment. Unlike previous approaches, MDRR combines sample
    
[^20]: 有限预算下的迅速学习最佳臂识别

    Best Arm Identification for Prompt Learning under a Limited Budget

    [https://arxiv.org/abs/2402.09723](https://arxiv.org/abs/2402.09723)

    这项工作提出了一种在提示学习中考虑有限预算约束的方法，通过建立提示学习和多臂赌博机中固定预算最佳臂识别之间的联系，提出了一个通用框架TRIPLE，通过利用聚类和嵌入思想实现了两个增强方法。

    

    大型语言模型（LLMs）的显著指令跟随能力引发了对自动学习合适提示的兴趣。然而，虽然提出了许多有效的方法，但在学习过程中产生的成本（例如访问LLM和评估响应）尚未得到考虑。为克服这个限制，本工作在提示学习中明确引入了有限预算约束。为了开发有原则的解决方案，本研究在提示学习和多臂赌博机的固定预算最佳臂识别（BAI-FB）之间建立了一种新的联系。基于这种联系，提出了一个通用框架TRIPLE（用于提示学习的最佳臂识别），以系统地利用BAI-FB在提示学习中的力量。提示学习的独特特点进一步通过利用聚类和嵌入思想提出了TRIPLE的两个基于嵌入的增强方法。

    arXiv:2402.09723v1 Announce Type: cross  Abstract: The remarkable instruction-following capability of large language models (LLMs) has sparked a growing interest in automatically learning suitable prompts. However, while many effective methods have been proposed, the cost incurred during the learning process (e.g., accessing LLM and evaluating the responses) has not been considered. To overcome this limitation, this work explicitly incorporates a finite budget constraint into prompt learning. Towards developing principled solutions, a novel connection is established between prompt learning and fixed-budget best arm identification (BAI-FB) in multi-armed bandits (MAB). Based on this connection, a general framework TRIPLE (besT aRm Identification for Prompt LEarning) is proposed to harness the power of BAI-FB in prompt learning systematically. Unique characteristics of prompt learning further lead to two embedding-based enhancements of TRIPLE by exploiting the ideas of clustering and fun
    
[^21]: API Pack：一个用于API调用生成的大规模多语言数据集

    API Pack: A Massive Multilingual Dataset for API Call Generation

    [https://arxiv.org/abs/2402.09615](https://arxiv.org/abs/2402.09615)

    这个论文介绍了一个名为API Pack的大规模多语言数据集，旨在提高大型语言模型的API调用生成能力，通过实验证明了其在生成未见过的API调用方面的高准确率，并实现了跨语言的API调用生成

    

    我们介绍了API Pack，一个包含超过一百万个指令-API调用对的多语言数据集，旨在提高大型语言模型的API调用生成能力。通过实验，我们证明了API Pack在提升模型在这一特定任务上的效果的同时，保持其在一般编码方面的整体熟练程度。仅在20,000个Python实例上对CodeLlama-13B进行微调，其生成未见过的API调用的准确率比GPT-3.5和GPT-4分别高出10%和5%。扩展到100k个例子可以提高对训练期间未见过的新API的泛化能力。此外，实现了跨语言的API调用生成，而无需大量语言特定的数据。数据集、经过微调的模型和整体代码库可在https://github.com/anonymous_url上公开获取。

    arXiv:2402.09615v1 Announce Type: cross  Abstract: We introduce API Pack, a multilingual dataset featuring over one million instruction-API call pairs aimed at advancing large language models' API call generation capabilities. Through experiments, we demonstrate API Pack's efficacy in enhancing models for this specialized task while maintaining their overall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000 Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4 respectively in generating unseen API calls. Scaling to 100k examples improves generalization to new APIs not seen during training. In addition, cross-lingual API call generation is achieved without needing extensive data per language. The dataset, fine-tuned models, and overall code base are publicly available at https://github.com/anonymous_url.
    
[^22]: 通过层角色差异化，端到端训练引发信息瓶颈：与逐层训练的比较分析

    End-to-End Training Induces Information Bottleneck through Layer-Role Differentiation: A Comparative Analysis with Layer-wise Training

    [https://arxiv.org/abs/2402.09050](https://arxiv.org/abs/2402.09050)

    本文通过与逐层训练的比较，重新考虑了为什么端到端训练表现出优越的性能，分析了信息传播方面的优势，并对训练模型的性质差异进行了深入理解。

    

    端到端（E2E）训练通过误差反向传播优化整个模型，从根本上支持深度学习的进展。尽管其性能很高，但E2E训练面临内存消耗、并行计算和与实际大脑功能的不一致等问题。已经提出了各种替代方法来克服这些困难，然而目前还没有一种能够与E2E训练的性能匹配的方法，因此在实用性上存在不足。此外，对于训练模型性质的差异在性能差距之外缺乏深入的理解。在本文中，我们通过与逐层训练进行比较，重新考虑了为什么E2E训练表现出优越的性能，逐层训练是一种局部设置错误的非E2E方法。在观察到E2E训练在传播输入信息方面具有优势的基础上，我们分析了中间表示的信息平面动态。

    arXiv:2402.09050v1 Announce Type: new Abstract: End-to-end (E2E) training, optimizing the entire model through error backpropagation, fundamentally supports the advancements of deep learning. Despite its high performance, E2E training faces the problems of memory consumption, parallel computing, and discrepancy with the functionalities of the actual brain. Various alternative methods have been proposed to overcome these difficulties; however, no one can yet match the performance of E2E training, thereby falling short in practicality. Furthermore, there is no deep understanding regarding differences in the trained model properties beyond the performance gap. In this paper, we reconsider why E2E training demonstrates a superior performance through a comparison with layer-wise training, a non-E2E method that locally sets errors. On the basis of the observation that E2E training has an advantage in propagating input information, we analyze the information plane dynamics of intermediate rep
    
[^23]: 一种加速梯度方法求解具有凸下层问题的简单双层优化问题

    An Accelerated Gradient Method for Simple Bilevel Optimization with Convex Lower-level Problem

    [https://arxiv.org/abs/2402.08097](https://arxiv.org/abs/2402.08097)

    本文提出了一种加速梯度方法来解决具有凸下层问题的简单双层优化问题，通过局部逼近下层问题的解集和加速梯度更新方法，在有限次迭代内找到一个具有一定精度的最优解。

    

    本文主要研究简单的双层优化问题，即在另一个凸光滑约束优化问题的最优解集上最小化一个凸光滑目标函数。我们提出了一种新颖的双层优化方法，通过切平面方法局部逼近下层问题的解集，并采用加速梯度更新方法降低近似解集上的上层目标函数。我们通过子最优解和不可行误差度量我们方法的性能，并提供了对两个误差标准的非渐进收敛性保证。特别地，当可行集是紧致的时候，我们证明了我们的方法最多需要$\mathcal{O}(\max\{1/\sqrt{\epsilon_{f}}, 1/\epsilon_g\})$次迭代才能找到一个$\epsilon_f$-子最优且$\epsilon_g$-不可行的解。此外，在额外假设下，下层目标满足$r$阶H\"olderian误差时，我们给出了解的收敛速度估计。

    In this paper, we focus on simple bilevel optimization problems, where we minimize a convex smooth objective function over the optimal solution set of another convex smooth constrained optimization problem. We present a novel bilevel optimization method that locally approximates the solution set of the lower-level problem using a cutting plane approach and employs an accelerated gradient-based update to reduce the upper-level objective function over the approximated solution set. We measure the performance of our method in terms of suboptimality and infeasibility errors and provide non-asymptotic convergence guarantees for both error criteria. Specifically, when the feasible set is compact, we show that our method requires at most $\mathcal{O}(\max\{1/\sqrt{\epsilon_{f}}, 1/\epsilon_g\})$ iterations to find a solution that is $\epsilon_f$-suboptimal and $\epsilon_g$-infeasible. Moreover, under the additional assumption that the lower-level objective satisfies the $r$-th H\"olderian err
    
[^24]: 针对私有统计推断的重采样方法

    Resampling methods for Private Statistical Inference

    [https://arxiv.org/abs/2402.07131](https://arxiv.org/abs/2402.07131)

    这项研究提出了两种私有变体的非参数bootstrap方法，用于在差分隐私的情况下构建置信区间。方法在计算效率和置信区间长度上相比现有方法有显著改进。

    

    我们考虑使用差分隐私构建置信区间的任务。我们提出了两种私有变体的非参数bootstrap方法，该方法在数据的分区上私下计算多个“小”bootstrap的结果的中位数，并给出了得到的置信区间的渐进覆盖误差上界。对于固定的差分隐私参数ε，我们的方法在样本大小n上的误差率与非私有bootstrap相当，只是在对数因子内。我们使用真实数据和合成数据在均值估计、中位数估计和逻辑回归方面对我们的方法进行了经验验证。我们的方法在提供类似的覆盖精度的同时，比以前的方法提供了显著缩短（大约10倍）的置信区间。

    We consider the task of constructing confidence intervals with differential privacy. We propose two private variants of the non-parametric bootstrap, which privately compute the median of the results of multiple ``little'' bootstraps run on partitions of the data and give asymptotic bounds on the coverage error of the resulting confidence intervals. For a fixed differential privacy parameter $\epsilon$, our methods enjoy the same error rates as that of the non-private bootstrap to within logarithmic factors in the sample size $n$. We empirically validate the performance of our methods for mean estimation, median estimation, and logistic regression with both real and synthetic data. Our methods achieve similar coverage accuracy to existing methods (and non-private baselines) while providing notably shorter ($\gtrsim 10$ times) confidence intervals than previous approaches.
    
[^25]: 尾巴的故事：作为尺度律变化的模型崩溃

    A Tale of Tails: Model Collapse as a Change of Scaling Laws

    [https://arxiv.org/abs/2402.07043](https://arxiv.org/abs/2402.07043)

    本文通过尺度律的视角，研究了AI模型大小增长和合成数据引入对模型性能的影响，发现模型可能会遭遇总体崩溃，并通过大规模实验证实了这一理论。

    

    随着AI模型大小的增长，神经尺度律已成为预测大模型在扩容和原始（人类或自然）训练数据大小增加时改善的关键工具。然而，流行模型的广泛使用意味着在线数据和文本的生态系统将逐渐包含越来越多的合成数据。在本文中，我们问：当合成数据进入训练语料库时，尺度律会如何改变？未来的模型仍会改善，还是注定会完全崩溃（模型崩溃）？通过尺度律的视角，我们开发了一个模型崩溃的理论框架。我们发现了广泛的衰减现象，分析了尺度的丧失、与代数的变化尺度、技能的"遗忘"以及混合人类和合成数据时的理解能力。我们的理论通过对一个算术任务和文本生成的转换器进行大规模实验证实。

    As AI model size grows, neural scaling laws have become a crucial tool to predict the improvements of large models when increasing capacity and the size of original (human or natural) training data. Yet, the widespread use of popular models means that the ecosystem of online data and text will co-evolve to progressively contain increased amounts of synthesized data. In this paper we ask: How will the scaling laws change in the inevitable regime where synthetic data makes its way into the training corpus? Will future models, still improve, or be doomed to degenerate up to total (model) collapse? We develop a theoretical framework of model collapse through the lens of scaling laws. We discover a wide range of decay phenomena, analyzing loss of scaling, shifted scaling with number of generations, the ''un-learning" of skills, and grokking when mixing human and synthesized data. Our theory is validated by large-scale experiments with a transformer on an arithmetic task and text generation 
    
[^26]: 使用Dirichlet流匹配的应用于DNA序列设计

    Dirichlet Flow Matching with Applications to DNA Sequence Design

    [https://arxiv.org/abs/2402.05841](https://arxiv.org/abs/2402.05841)

    本文提出了一种使用Dirichlet流匹配的方法，在DNA序列设计中取得卓越的性能，比所有基线模型在分布度量和生成的序列设计目标方面表现更好。

    

    离散扩散或流模型可以比自回归模型更快速和更可控地生成序列。我们表明，在单纯形上的原始线性流匹配不足以实现这一目标，因为它在训练目标和进一步的路径上存在不连续性。为了克服这个问题，我们基于Dirichlet分布的混合来开发在单纯形上的Dirichlet流匹配作为概率路径。在这个框架下，我们推导了混合分数和流向量场之间的连接，从而实现了分类器和无分类器指导。此外，我们提供了蒸馏Dirichlet流匹配，它可以实现一步序列生成而只有最小的性能损失，与自回归模型相比，速度提高了O(L)倍。在复杂的DNA序列生成任务中，我们证明了相对于所有基线模型在分布度量和实现生成序列的期望设计目标方面具有更优的性能。

    Discrete diffusion or flow models could enable faster and more controllable sequence generation than autoregressive models. We show that na\"ive linear flow matching on the simplex is insufficient toward this goal since it suffers from discontinuities in the training target and further pathologies. To overcome this, we develop Dirichlet flow matching on the simplex based on mixtures of Dirichlet distributions as probability paths. In this framework, we derive a connection between the mixtures' scores and the flow's vector field that allows for classifier and classifier-free guidance. Further, we provide distilled Dirichlet flow matching, which enables one-step sequence generation with minimal performance hits, resulting in $O(L)$ speedups compared to autoregressive models. On complex DNA sequence generation tasks, we demonstrate superior performance compared to all baselines in distributional metrics and in achieving desired design targets for generated sequences. Finally, we show that
    
[^27]: 在流数据上进行高效推理的在线级联学习

    Online Cascade Learning for Efficient Inference over Streams

    [https://arxiv.org/abs/2402.04513](https://arxiv.org/abs/2402.04513)

    这项研究提出了在线级联学习的方法，通过学习一个“级联”模型，从容量较低的模型到强大的语言模型，以及推迟策略，可以在流数据处理中同时保证准确性和降低推理成本。

    

    大型语言模型 (LLM) 在回答关于数据流的复杂查询方面具有天然的优势，但是 LLM 推理的高计算成本使得它们在许多任务中不可行。我们提出了在线级联学习，这是首个解决这一挑战的方法。这里的目标是学习一个“级联”模型，从容量较低的模型（如逻辑回归器）开始，到强大的 LLM 结束，并配备一个决定在给定输入上使用哪个模型的推迟策略。我们将在线学习级联的任务公式化为一个模仿学习问题，并为该问题提供了无遗憾算法。在四个基准测试中的实验结果显示，我们的方法在准确性上与 LLM 相当，同时将推理成本削减了多达 90%，突显了它在流处理中的效能和适应能力。

    Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks. We propose online cascade learning, the first approach to addressing this challenge. The objective here is to learn a "cascade" of models, starting with lower-capacity models (such as logistic regressors) and ending with a powerful LLM, along with a deferral policy that determines the model that is used on a given input. We formulate the task of learning cascades online as an imitation-learning problem and give a no-regret algorithm for the problem. Experimental results across four benchmarks show that our method parallels LLMs in accuracy while cutting down inference costs by as much as 90%, underscoring its efficacy and adaptability in stream processing.
    
[^28]: GUARD: 通过角色扮演生成自然语言越狱来测试大型语言模型遵循指南的合规性

    GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models

    [https://arxiv.org/abs/2402.03299](https://arxiv.org/abs/2402.03299)

    本论文提出了一个通过角色扮演的系统，可以生成自然语言越狱，用于测试大型语言模型的指南遵循情况。系统通过收集现有越狱并将其组织成知识图来生成新的越狱，证明了其高效性和有效性。

    

    发现绕过大型语言模型（LLM）的安全过滤和有害回应的"越狱"已经鼓励社区采取安全措施。其中一个主要的安全措施是在发布之前用越狱主动测试LLM。因此，这样的测试将需要一种能够大规模且高效地生成越狱的方法。本文在追随一种新颖而直观的策略下，以人类生成的方式来生成越狱。我们提出了一个角色扮演系统，将四种不同角色分配给用户LLM，以便协作生成新的越狱。此外，我们收集现有的越狱，并通过句子逐句进行聚类频率和语义模式的划分，将它们分成不同的独立特征。我们将这些特征组织成一个知识图，使其更易于访问和检索。我们的角色系统将利用这个知识图来生成新的越狱，证明了其有效性。

    The discovery of "jailbreaks" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effec
    
[^29]: 图基础模型

    Graph Foundation Models

    [https://arxiv.org/abs/2402.02216](https://arxiv.org/abs/2402.02216)

    图基础模型是图领域中一个新兴的研究课题，旨在开发一个能够跨不同图和任务进行泛化的图模型。我们提出了一种新的 GFM 发展视角，通过倡导“图词汇表”来编码图的不变性，有助于推进未来的GFM设计。

    

    图基础模型（Graph Foundation Model，GFM）是图领域中一个新兴的研究课题，旨在开发一个能够跨不同图和任务进行泛化的图模型。然而，目前还没有实现一个多功能的GFM。构建GFM的关键挑战在于如何能在具有不同结构模式的图之间实现正向迁移。受计算机视觉（CV）和自然语言处理（NLP）领域基础模型的启发，我们提出了一种新的 GFM 发展视角，通过倡导“图词汇表”，即潜藏于图中的基本可迁移单元来编码图的不变性。我们从网络分析、理论基础和稳定性等重要方面来建立图词汇表。这种词汇表的视角有助于按照神经缩放定律推进未来的GFM设计。

    Graph Foundation Model (GFM) is a new trending research topic in the graph domain, aiming to develop a graph model capable of generalizing across different graphs and tasks. However, a versatile GFM has not yet been achieved. The key challenge in building GFM is how to enable positive transfer across graphs with diverse structural patterns. Inspired by the existing foundation models in the CV and NLP domains, we propose a novel perspective for the GFM development by advocating for a ``graph vocabulary'', in which the basic transferable units underlying graphs encode the invariance on graphs. We ground the graph vocabulary construction from essential aspects including network analysis, theoretical foundations, and stability. Such a vocabulary perspective can potentially advance the future GFM design following the neural scaling laws.
    
[^30]: 基于超平面优化的神经网络中的随机线性投影损失

    Random Linear Projections Loss for Hyperplane-Based Optimization in Neural Networks

    [https://arxiv.org/abs/2311.12356](https://arxiv.org/abs/2311.12356)

    本研究引入了一种名为随机线性投影（RLP）损失的新方法，通过利用数据中的几何关系来提高神经网络训练效率。实证评估表明，使用RLP损失训练的神经网络优于传统损失函数训练的网络，在更少的数据样本下实现更好的性能，并且对于添加噪声表现更强鲁棒性。

    

    提出了一种名为随机线性投影（RLP）损失的新方法，通过利用数据中的几何关系来提高训练效率。与传统的旨在最小化逐点误差的损失函数不同，RLP损失通过最小化连接固定大小的特征-预测对和特征-标签对的超平面集之间的距离来操作。我们通过在基准数据集和合成示例上进行的实证评估表明，使用RLP损失训练的神经网络优于使用传统损失函数训练的网络，可以在更少的数据样本下实现更好的性能，并且对于添加噪声表现更强鲁棒性。我们还提供了支持我们实证结果的理论分析。

    arXiv:2311.12356v2 Announce Type: replace  Abstract: Advancing loss function design is pivotal for optimizing neural network training and performance. This work introduces Random Linear Projections (RLP) loss, a novel approach that enhances training efficiency by leveraging geometric relationships within the data. Distinct from traditional loss functions that target minimizing pointwise errors, RLP loss operates by minimizing the distance between sets of hyperplanes connecting fixed-size subsets of feature-prediction pairs and feature-label pairs. Our empirical evaluations, conducted across benchmark datasets and synthetic examples, demonstrate that neural networks trained with RLP loss outperform those trained with traditional loss functions, achieving improved performance with fewer data samples, and exhibiting greater robustness to additive noise. We provide theoretical analysis supporting our empirical findings.
    
[^31]: 改进乳腺MRI肿瘤分割的前后对比合成

    Pre- to Post-Contrast Breast MRI Synthesis for Enhanced Tumour Segmentation

    [https://arxiv.org/abs/2311.10879](https://arxiv.org/abs/2311.10879)

    通过生成对抗网络（GAN）在乳腺MRI中合成对比增强，引入了Scaled Aggregate Measure (SAMe)进行量化评估，并成功应用于乳腺肿瘤分割任务。

    

    尽管动态增强MRI（DCE-MRI）中对比剂的使用对于肿瘤的检测和治疗有益处，但存在一系列问题，包括其侵入性、生物积累性及潜在的肾源性系统纤维化风险。本研究探讨了通过将术前对比前T1加权脂肪饱和乳腺MRI转换为其对应的首次DCE-MRI序列，利用生成对抗网络（GAN）的能力来产生合成对比增强的可行性。此外，我们引入了一种用于定量评估合成数据质量的比例聚合测量（SAMe），并作为选择最佳生成模型的基础。我们使用定量图像质量指标评估生成的DCE-MRI数据，并将其应用于3D乳腺肿瘤分割的下游任务。我们的结果突显了p的潜力

    arXiv:2311.10879v2 Announce Type: replace-cross  Abstract: Despite its benefits for tumour detection and treatment, the administration of contrast agents in dynamic contrast-enhanced MRI (DCE-MRI) is associated with a range of issues, including their invasiveness, bioaccumulation, and a risk of nephrogenic systemic fibrosis. This study explores the feasibility of producing synthetic contrast enhancements by translating pre-contrast T1-weighted fat-saturated breast MRI to their corresponding first DCE-MRI sequence leveraging the capabilities of a generative adversarial network (GAN). Additionally, we introduce a Scaled Aggregate Measure (SAMe) designed for quantitatively evaluating the quality of synthetic data in a principled manner and serving as a basis for selecting the optimal generative model. We assess the generated DCE-MRI data using quantitative image quality metrics and apply them to the downstream task of 3D breast tumour segmentation. Our results highlight the potential of p
    
[^32]: 基于条件化空间-时间归一化流的概率天气预测

    Towards probabilistic Weather Forecasting with Conditioned Spatio-Temporal Normalizing Flows

    [https://arxiv.org/abs/2311.06958](https://arxiv.org/abs/2311.06958)

    该论文提出了一种基于条件化归一化流的概率天气预测方法，通过实验证明其能够捕捉和良好外推空间-时间相关性。

    

    生成式归一化流能够建模多模态空间分布，已经成功地模拟了时间相关性。由于其训练稳定性、可逆性以及在采样和推断方面的高效性，这些模型比其他类型的生成模型提供了几项好处。这使它们成为随机空间-时间预测问题的合适候选者，在许多科学领域中都普遍存在，如地球科学、天体物理学或分子科学。本文介绍了用于随机空间-时间建模的条件化归一化流。该方法在从ERA5数据集进行的日温度和小时等压图预测任务上进行了评估。实验表明，我们的方法能够捕捉空间-时间相关性，并能够在训练期间使用的时间范围之外进行良好的外推。

    arXiv:2311.06958v2 Announce Type: replace-cross  Abstract: Generative normalizing flows are able to model multimodal spatial distributions, and they have been shown to model temporal correlations successfully as well. These models provide several benefits over other types of generative models due to their training stability, invertibility and efficiency in sampling and inference. This makes them a suitable candidate for stochastic spatio-temporal prediction problems, which are omnipresent in many fields of sciences, such as earth sciences, astrophysics or molecular sciences. In this paper, we present conditional normalizing flows for stochastic spatio-temporal modelling. The method is evaluated on the task of daily temperature and hourly geopotential map prediction from ERA5 datasets. Experiments show that our method is able to capture spatio-temporal correlations and extrapolates well beyond the time horizon used during training.
    
[^33]: 学习学习以进行少样本持久主动学习

    Learning to Learn for Few-shot Continual Active Learning

    [https://arxiv.org/abs/2311.03732](https://arxiv.org/abs/2311.03732)

    提出了一种简单而高效的方法，Meta-Continual Active Learning，通过元学习和经验重播解决少样本持续主动学习中的任务混淆和灾难性遗忘，进一步结合文本增强来确保泛化。

    

    持续学习旨在确保解决先前见过的任务的稳定性，同时展示对新领域的可塑性。最近在持续学习方面的进展主要局限于监督学习设置，尤其是在自然语言处理领域。在这项工作中，我们考虑了一种少样本持续主动学习（CAL）设置，其中标记数据不足，但未标记数据充足，但有限的注释预算。我们提出了一种简单而高效的方法，称为元持续主动学习。具体地，我们采用元学习和经验重播来解决任务之间的混淆和灾难性遗忘。我们进一步结合文本增强来确保泛化性能。我们在基准文本分类数据集上进行了大量实验，以验证所提方法的有效性，并分析了在少样本CAL设置中不同主动学习策略的影响。我们的实验结果表明

    arXiv:2311.03732v2 Announce Type: replace-cross  Abstract: Continual learning strives to ensure stability in solving previously seen tasks while demonstrating plasticity in a novel domain. Recent advances in CL are mostly confined to a supervised learning setting, especially in NLP domain. In this work, we consider a few-shot continual active learning (CAL) setting where labeled data are inadequate, and unlabeled data are abundant but with a limited annotation budget. We propose a simple but efficient method, called Meta-Continual Active Learning. Specifically, we employ meta-learning and experience replay to address inter-task confusion and catastrophic forgetting. We further incorporate textual augmentations to ensure generalization. We conduct extensive experiments on benchmark text classification datasets to validate the effectiveness of the proposed method and analyze the effect of different active learning strategies in few-shot CAL setting. Our experimental results demonstrate t
    
[^34]: 多配体对接和结合位点设计的谐波自调流匹配

    Harmonic Self-Conditioned Flow Matching for Multi-Ligand Docking and Binding Site Design

    [https://arxiv.org/abs/2310.05764](https://arxiv.org/abs/2310.05764)

    该研究开发了一种谐波自调流匹配方法，在多配体对接和结合位点设计中表现出比现有方法更好的生成过程和设计效果。

    

    大量蛋白质功能需要与小分子结合，包括酶催化。因此，为小分子设计结合口袋具有从药物合成到能量存储等多种影响深远的应用。为实现这一目标，我们首先开发了HarmonicFlow，这是一个改进的基于自调流匹配目标的3D蛋白质-配体结合结构生成过程。FlowSite将这种流模型扩展到联合生成蛋白质口袋的离散残基类型和分子的结合3D结构。我们展示了HarmonicFlow在口袋级对接中在简单性、普适性和平均样本质量上优于最先进的生成过程。借助于这种结构建模，FlowSite设计的结合位点明显优于基线方法。

    arXiv:2310.05764v3 Announce Type: replace-cross  Abstract: A significant amount of protein function requires binding small molecules, including enzymatic catalysis. As such, designing binding pockets for small molecules has several impactful applications ranging from drug synthesis to energy storage. Towards this goal, we first develop HarmonicFlow, an improved generative process over 3D protein-ligand binding structures based on our self-conditioned flow matching objective. FlowSite extends this flow model to jointly generate a protein pocket's discrete residue types and the molecule's binding 3D structure. We show that HarmonicFlow improves upon state-of-the-art generative processes for docking in simplicity, generality, and average sample quality in pocket-level docking. Enabled by this structure modeling, FlowSite designs binding sites substantially better than baseline approaches.
    
[^35]: 通过工具变量法识别和估计条件平均偏因果效应

    Identification and Estimation of Conditional Average Partial Causal Effects via Instrumental Variable. (arXiv:2401.11130v1 [cs.LG])

    [http://arxiv.org/abs/2401.11130](http://arxiv.org/abs/2401.11130)

    本文提出了一种通过工具变量法识别和估计连续处理的因果效应异质性的方法，并开发了三类相应的估计器，并对其进行了统计性质分析。

    

    近年来，对于估计异质因果效应的兴趣日益增加。本文引入了条件平均偏因果效应（CAPCE），以揭示连续处理的因果效应的异质性。我们给出了在工具变量设置下识别CAPCE的条件。我们开发了三类CAPCE估计器：筛选、参数化和再生核希尔伯特空间（RKHS）-基础，分析了它们的统计性质。我们通过合成和真实数据对提出的CAPCE估计器进行了说明。

    There has been considerable recent interest in estimating heterogeneous causal effects. In this paper, we introduce conditional average partial causal effects (CAPCE) to reveal the heterogeneity of causal effects with continuous treatment. We provide conditions for identifying CAPCE in an instrumental variable setting. We develop three families of CAPCE estimators: sieve, parametric, and reproducing kernel Hilbert space (RKHS)-based, and analyze their statistical properties. We illustrate the proposed CAPCE estimators on synthetic and real-world data.
    
[^36]: 放松的对比学习用于联邦学习

    Relaxed Contrastive Learning for Federated Learning. (arXiv:2401.04928v1 [cs.LG])

    [http://arxiv.org/abs/2401.04928](http://arxiv.org/abs/2401.04928)

    我们提出了一种放松的对比学习框架，用于解决联邦学习中的数据异构性挑战。我们的方法通过引入放松的对比学习损失，防止表示坍缩，增强特征的可传递性，从而实现了显著的性能提升。

    

    我们提出了一种新颖的对比学习框架，以有效解决联邦学习中的数据异构性挑战。我们首先分析了本地训练中客户端之间梯度更新的不一致性，并建立其与特征表示分布的依赖关系，从而导出了监督对比学习（SCL）目标来减轻局部偏差。此外，我们还展示了在联邦学习中对SCL的朴素应用会导致表示坍缩，导致收敛缓慢和性能提升有限。为了解决这个问题，我们引入了一种放松的对比学习损失，对每个类别内过于相似的样本对施加发散惩罚。这种策略可以防止表示坍缩，增强特征的可传递性，促进协作训练，并导致显著的性能提升。我们的框架在所有现有的联邦学习方法中表现出巨大的优势。

    We propose a novel contrastive learning framework to effectively address the challenges of data heterogeneity in federated learning. We first analyze the inconsistency of gradient updates across clients during local training and establish its dependence on the distribution of feature representations, leading to the derivation of the supervised contrastive learning (SCL) objective to mitigate local deviations. In addition, we show that a na\"ive adoption of SCL in federated learning leads to representation collapse, resulting in slow convergence and limited performance gains. To address this issue, we introduce a relaxed contrastive learning loss that imposes a divergence penalty on excessively similar sample pairs within each class. This strategy prevents collapsed representations and enhances feature transferability, facilitating collaborative training and leading to significant performance improvements. Our framework outperforms all existing federated learning approaches by huge marg
    
[^37]: 结构化神经退行性卷积神经网络用于阿尔茨海默病建模和分类

    Structure-focused Neurodegeneration Convolutional Neural Network for Modeling and Classification of Alzheimer's Disease. (arXiv:2401.03922v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2401.03922](http://arxiv.org/abs/2401.03922)

    本论文提出了一种结构化神经退行性卷积神经网络，用于AD和MCI的识别。该网络考虑了局部结构特征，可以更准确地进行早期AD的诊断。

    

    阿尔茨海默病（AD）作为主要的痴呆形式，对全球构成了一个不断增长的挑战，并强调了准确和早期诊断的紧迫性。临床技术中，放射科医生在使用磁共振成像（MRI）区分轻度认知障碍（MCI）和AD时遇到了困难，因为这些方法不一致且不可靠。已经证明，机器学习有望提供早期AD诊断的可能性。然而，现有的模型主要关注细粒度的局部特征，而没有考虑到可以提供大脑皮层神经退行信息的局部结构特征。因此，本文提出了一种机器学习（ML）框架，该框架集成了Gamma校正（一种图像增强技术），并使用一种名为SNeurodCNN的结构化神经退行性卷积神经网络（CNN）架构，用于区分AD和MCI。该ML框架利用了中矢状面和旁矢状面的大脑图像视点。

    Alzheimer's disease (AD), the predominant form of dementia, poses a growing global challenge and underscores the urgency of accurate and early diagnosis. The clinical technique radiologists adopt for distinguishing between mild cognitive impairment (MCI) and AD using Machine Resonance Imaging (MRI) encounter hurdles because they are not consistent and reliable. Machine learning has been shown to offer promise for early AD diagnosis. However, existing models focused on focal fine-grain features without considerations to focal structural features that give off information on neurodegeneration of the brain cerebral cortex. Therefore, this paper proposes a machine learning (ML) framework that integrates Gamma correction, an image enhancement technique, and includes a structure-focused neurodegeneration convolutional neural network (CNN) architecture called SNeurodCNN for discriminating between AD and MCI. The ML framework leverages the mid-sagittal and para-sagittal brain image viewpoints 
    
[^38]: 图卷积在Transformer的自注意力机制中起到了改进的作用！（arXiv：2312.04234v2 [cs.LG]已更新）

    Graph Convolutions Enrich the Self-Attention in Transformers!. (arXiv:2312.04234v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.04234](http://arxiv.org/abs/2312.04234)

    这项研究通过引入图卷积来改进Transformer模型中的自注意力机制，并在计算机视觉、自然语言处理等多个领域展示了其性能提升。

    

    Transformer因其自注意力机制而闻名，在自然语言处理、计算机视觉、时间序列建模等各种任务中取得了最先进的性能。然而，深度Transformer模型面临的挑战之一是过度平滑问题，即表示在各个层之间趋于无法区分的值，导致性能严重下降。我们将原始的自注意力机制解释为一种简单的图滤波器，并从图信号处理（GSP）的角度重新设计它。我们提出了基于图滤波器的自注意力机制（GFSA），以学习一种既通用又有效的机制，其复杂度略高于原始的自注意力机制。我们证明了GFSA在计算机视觉、自然语言处理、图模式分类、语音识别和代码分类等多个领域中改进了Transformer的性能。

    Transformers, renowned for their self-attention mechanism, have achieved state-of-the-art performance across various tasks in natural language processing, computer vision, time-series modeling, etc. However, one of the challenges with deep Transformer models is the oversmoothing problem, where representations across layers converge to indistinguishable values, leading to significant performance degradation. We interpret the original self-attention as a simple graph filter and redesign it from a graph signal processing (GSP) perspective. We propose graph-filter-based self-attention (GFSA) to learn a general yet effective one, whose complexity, however, is slightly larger than that of the original self-attention mechanism. We demonstrate that GFSA improves the performance of Transformers in various fields, including computer vision, natural language processing, graph pattern classification, speech recognition, and code classification.
    
[^39]: 简化Transformer块

    Simplifying Transformer Blocks. (arXiv:2311.01906v1 [cs.LG])

    [http://arxiv.org/abs/2311.01906](http://arxiv.org/abs/2311.01906)

    本论文通过结合信号传播理论和实证观察，提出了一种简化Transformer块的方法。这种简化方法可以去除许多不影响训练速度的组件，并且在保持性能的同时比标准Transformer块的训练吞吐量提高了15%。

    

    对于深度Transformer，一个简单的设计方法是组合相同的构建块。但是标准的Transformer块远非简单，它们将注意力和MLP子块与跳连接和标准化层以精确的方式交织在一起。这种复杂性导致了脆弱的架构，即似乎微小的改变可能会显著降低训练速度，或使模型无法训练。在这项工作中，我们探讨了标准Transformer块可以被简化到什么程度？结合信号传播理论和实证观察，我们提出了可以去除许多块组件而不损失训练速度的修改，包括跳连接、投影或值参数、序列子块和标准化层。在自回归解码器和BERT编码器模型的实验中，我们简化的Transformer块在保持训练速度和性能的基础上，比标准Transformer块的训练吞吐量提高了15%。

    A simple design recipe for deep Transformers is to compose identical building blocks. But standard transformer blocks are far from simple, interweaving attention and MLP sub-blocks with skip connections & normalisation layers in precise arrangements. This complexity leads to brittle architectures, where seemingly minor changes can significantly reduce training speed, or render models untrainable.  In this work, we ask to what extent the standard transformer block can be simplified? Combining signal propagation theory and empirical observations, we motivate modifications that allow many block components to be removed with no loss of training speed, including skip connections, projection or value parameters, sequential sub-blocks and normalisation layers. In experiments on both autoregressive decoder-only and BERT encoder-only models, our simplified transformers emulate the per-update training speed and performance of standard transformers, while enjoying 15% faster training throughput, 
    
[^40]: 通过神经坍塌的视角检测到群外分布

    Detecting Out-of-Distribution Through the Lens of Neural Collapse. (arXiv:2311.01479v1 [cs.LG])

    [http://arxiv.org/abs/2311.01479](http://arxiv.org/abs/2311.01479)

    通过观察到的群内特征聚集和群外特征离散的性质，本论文提出了一种基于特征和权重向量接近程度的神经坍塌（NC-OOD）检测器来提高OAD检测的泛化能力，并取得了最先进的效果。

    

    群外（OOD）检测对于安全部署人工智能至关重要。特别是，OOD检测器应该在各种场景中有效地泛化。为了改进现有OOD检测器的泛化能力，我们引入了一种高度灵活的OOD检测器，称为神经坍塌（NC-OOD）检测器。我们扩展了普遍观察到的群内（ID）特征倾向于形成簇，而群外特征则远离的观察。特别是基于最近的观察结果，神经坍塌，我们进一步证明ID特征倾向于在接近权重向量的位置聚集。根据我们的扩展观察，我们提出了一种基于特征与权重向量的接近程度来检测OOD的方法。为了进一步排除OOD样本，我们利用了OOD特征倾向于比ID特征更接近原点的观察结果。大量实验证明我们的方法增强了现有工作的泛化能力，并且在OOD检测方面始终能够达到最先进的水平。

    Out-of-distribution (OOD) detection is essential for the safe deployment of AI. Particularly, OOD detectors should generalize effectively across diverse scenarios. To improve upon the generalizability of existing OOD detectors, we introduce a highly versatile OOD detector, called Neural Collapse inspired OOD detector (NC-OOD). We extend the prevalent observation that in-distribution (ID) features tend to form clusters, whereas OOD features are far away. Particularly, based on the recent observation, Neural Collapse, we further demonstrate that ID features tend to cluster in proximity to weight vectors. From our extended observation, we propose to detect OOD based on feature proximity to weight vectors. To further rule out OOD samples, we leverage the observation that OOD features tend to reside closer to the origin than ID features. Extensive experiments show that our approach enhances the generalizability of existing work and can consistently achieve state-of-the-art OOD detection per
    
[^41]: 等变深度权重空间对齐

    Equivariant Deep Weight Space Alignment. (arXiv:2310.13397v1 [cs.LG])

    [http://arxiv.org/abs/2310.13397](http://arxiv.org/abs/2310.13397)

    本论文提出了一个名为Deep-Align的新框架，用于学习解决权重对齐问题，以加速对齐过程并提高其质量。

    

    深度网络的排列对称性使得简单操作如模型平均和相似度估计变得困难。在许多情况下，对齐网络的权重，即找到它们之间最优排列，是必要的。更一般地说，权重对齐对于广泛的应用非常重要，从模型合并，通过探索深度神经网络的优化空间，到定义神经网络之间有意义的距离函数。不幸的是，权重对齐是一个NP-hard问题。先前的研究主要集中在解决对齐问题的松弛版本，导致方法耗时或者次优解。为了加速对齐过程并提高其质量，我们提出了一个名为Deep-Align的新框架，旨在学习解决权重对齐问题。为此，我们首先证明了权重对齐遵循两个基本对称性，然后提出了一个深度架构。

    Permutation symmetries of deep networks make simple operations like model averaging and similarity estimation challenging. In many cases, aligning the weights of the networks, i.e., finding optimal permutations between their weights, is necessary. More generally, weight alignment is essential for a wide range of applications, from model merging, through exploring the optimization landscape of deep neural networks, to defining meaningful distance functions between neural networks. Unfortunately, weight alignment is an NP-hard problem. Prior research has mainly focused on solving relaxed versions of the alignment problem, leading to either time-consuming methods or sub-optimal solutions. To accelerate the alignment process and improve its quality, we propose a novel framework aimed at learning to solve the weight alignment problem, which we name Deep-Align. To that end, we first demonstrate that weight alignment adheres to two fundamental symmetries and then, propose a deep architecture 
    
[^42]: 使用您的本能：使用神经探测器与转换器进行指令优化

    Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers. (arXiv:2310.02905v1 [cs.LG])

    [http://arxiv.org/abs/2310.02905](http://arxiv.org/abs/2310.02905)

    该论文提出了一种使用神经探测器和转换器优化指令的方法，以提高大型语言模型的性能。

    

    大型语言模型(LLMs)在各种应用中展示了出色的指令跟随能力，并取得了令人瞩目的表现。然而，LLMs的性能严重依赖于给予它们的指令，这些指令通常需要大量人力进行手动调整。最近的研究使用了高效的贝叶斯优化（BO）算法来自动优化给予黑盒LLMs的指令。然而，在优化高度复杂（例如高维）的目标函数时，如将指令映射到LLM性能的函数，BO通常表现不佳。这主要是由于BO使用的高斯过程（GP）模型的表达能力有限，该模型被用作BO的代理来建模目标函数。与此同时，已经多次证明神经网络（NNs），尤其是预训练的转换器，具有很强的表达能力，可以建模高度复杂的函数。因此，我们采用了一种神经探测器算法。

    Large language models (LLMs) have shown remarkable instruction-following capabilities and achieved impressive performances in various applications. However, the performances of LLMs depend heavily on the instructions given to them, which are typically manually tuned with substantial human efforts. Recent work has used the query-efficient Bayesian optimization (BO) algorithm to automatically optimize the instructions given to black-box LLMs. However, BO usually falls short when optimizing highly sophisticated (e.g., high-dimensional) objective functions, such as the functions mapping an instruction to the performance of an LLM. This is mainly due to the limited expressive power of the Gaussian process (GP) model which is used by BO as a surrogate to model the objective function. Meanwhile, it has been repeatedly shown that neural networks (NNs), especially pre-trained transformers, possess strong expressive power and can model highly complex functions. So, we adopt a neural bandit algor
    
[^43]: 数据清洗与机器学习：系统性文献综述

    Data Cleaning and Machine Learning: A Systematic Literature Review. (arXiv:2310.01765v1 [cs.LG])

    [http://arxiv.org/abs/2310.01765](http://arxiv.org/abs/2310.01765)

    本文系统综述了数据清洗与机器学习的关系，并总结了最新的数据清洗方法和ML的应用领域。在这两个领域存在着互相促进的关系，研究人员提出了未来的研究方向和建议。

    

    背景：机器学习（ML）被整合到越来越多的系统中，用于各种应用。由于ML模型的性能高度依赖于其训练数据的质量，因此对于检测和修复数据错误（即数据清洗）的方法越来越受关注。研究人员还在探索如何使用ML进行数据清洗，从而在ML和数据清洗之间构建了双重关系。据我们所知，尚无对这种关系进行全面综述的研究。目标：本文的目标有两个。首先，总结了最新的数据清洗方法，包括ML用于数据清洗和数据清洗用于ML。其次，提出了未来的工作建议。方法：我们对2016年至2022年期间发表的论文进行了系统性文献综述。我们确定了不同类型的数据清洗活动，包括特征清洗、标签清洗、实体匹配、异常值检测、缺失数据填充等。

    Context: Machine Learning (ML) is integrated into a growing number of systems for various applications. Because the performance of an ML model is highly dependent on the quality of the data it has been trained on, there is a growing interest in approaches to detect and repair data errors (i.e., data cleaning). Researchers are also exploring how ML can be used for data cleaning; hence creating a dual relationship between ML and data cleaning. To the best of our knowledge, there is no study that comprehensively reviews this relationship. Objective: This paper's objectives are twofold. First, it aims to summarize the latest approaches for data cleaning for ML and ML for data cleaning. Second, it provides future work recommendations. Method: We conduct a systematic literature review of the papers published between 2016 and 2022 inclusively. We identify different types of data cleaning activities with and for ML: feature cleaning, label cleaning, entity matching, outlier detection, imputati
    
[^44]: 原始-对偶持续学习：通过拉格朗日乘子实现稳定性和可塑性

    Primal-Dual Continual Learning: Stability and Plasticity through Lagrange Multipliers. (arXiv:2310.00154v1 [cs.LG])

    [http://arxiv.org/abs/2310.00154](http://arxiv.org/abs/2310.00154)

    本文提出了原始-对偶持续学习方法，通过利用拉格朗日对偶解决受限学习问题，实现了稳定性和可塑性。作者通过分析任务层面和样本层面的约束，在基于记忆的方法中分配资源，取得了较好的效果。

    

    持续学习固有地是一个受限学习问题。目标是在“无遗忘”要求下学习一个预测器。尽管之前有几项研究将其形式化为这样一个问题，但它们没有明确解决这个受限问题。在这项工作中，我们展示了直接解决这个受限优化问题是可行且有益的。为此，我们利用了最近在限制性学习中的拉格朗日对偶的结果。我们聚焦于基于记忆的方法，其中可以将先前任务中的一小部分样本存储在回放缓冲区中。在这个设置中，我们分析了持续学习问题的两个版本：一个在任务层面上有约束的粗糙方法和一个在样本层面上有约束的精细方法。我们展示了对偶变量指示了最优值对于约束扰动的敏感性。然后，我们利用这个结果在粗糙方法中对缓冲区进行了划分，将更多资源分配给更难的任务。

    Continual learning is inherently a constrained learning problem. The goal is to learn a predictor under a \emph{no-forgetting} requirement. Although several prior studies formulate it as such, they do not solve the constrained problem explicitly. In this work, we show that it is both possible and beneficial to undertake the constrained optimization problem directly. To do this, we leverage recent results in constrained learning through Lagrangian duality. We focus on memory-based methods, where a small subset of samples from previous tasks can be stored in a replay buffer. In this setting, we analyze two versions of the continual learning problem: a coarse approach with constraints at the task level and a fine approach with constraints at the sample level. We show that dual variables indicate the sensitivity of the optimal value with respect to constraint perturbations. We then leverage this result to partition the buffer in the coarse approach, allocating more resources to harder task
    
[^45]: 高维度下重尾数据下的鲁棒回归: 渐近性和普适性

    High-dimensional robust regression under heavy-tailed data: Asymptotics and Universality. (arXiv:2309.16476v1 [math.ST])

    [http://arxiv.org/abs/2309.16476](http://arxiv.org/abs/2309.16476)

    本文研究了在高维度和重尾干扰条件下的鲁棒回归估计器的性质，通过研究一类椭圆协变量和噪声数据分布的M-估计器，我们发现在存在重尾噪声的情况下，Huber损失需要进一步正则化才能达到最优性能。同时，我们还推导出了岭回归的超额风险的衰减速率。

    

    我们研究了在协变量和响应函数都受重尾干扰的情况下，鲁棒回归估计量的高维性质。特别地，我们针对一类包含椭圆协变量和噪声数据分布的M-估计器提供了锐利的渐近特征化，包括二阶及以上矩不存在的情况。我们发现，在存在重尾噪声的高维情况下，尽管Huber损失通过最优调整的位置参数$\delta$是一致的，但其在性能上是次优的，突显了进一步正则化以达到最优性能的必要性。这个结果还揭示了$\delta$作为样本复杂度和污染的函数存在的一个有趣的转变。此外，我们还推导出岭回归中超额风险的衰减速率。我们发现，对于具有有限二阶矩的噪声分布，岭回归不仅是最优的，而且是普适的，但其衰减速率可以是...

    We investigate the high-dimensional properties of robust regression estimators in the presence of heavy-tailed contamination of both the covariates and response functions. In particular, we provide a sharp asymptotic characterisation of M-estimators trained on a family of elliptical covariate and noise data distributions including cases where second and higher moments do not exist. We show that, despite being consistent, the Huber loss with optimally tuned location parameter $\delta$ is suboptimal in the high-dimensional regime in the presence of heavy-tailed noise, highlighting the necessity of further regularisation to achieve optimal performance. This result also uncovers the existence of a curious transition in $\delta$ as a function of the sample complexity and contamination. Moreover, we derive the decay rates for the excess risk of ridge regression. We show that, while it is both optimal and universal for noise distributions with finite second moment, its decay rate can be consi
    
[^46]: BayOTIDE: 基于贝叶斯方法的在线多元时间序列插补与函数分解

    BayOTIDE: Bayesian Online Multivariate Time series Imputation with functional decomposition. (arXiv:2308.14906v1 [cs.LG])

    [http://arxiv.org/abs/2308.14906](http://arxiv.org/abs/2308.14906)

    BayOTIDE是一种基于贝叶斯方法的在线多元时间序列插补与函数分解模型，通过将多元时间序列视为低秩时序因子组的加权组合来进行插补，同时解决了全局趋势和周期性模式的忽略以及不规则采样时间序列的处理问题。

    

    在真实世界的场景中，如交通和能源，经常观察到具有缺失值和噪声的大规模时间序列数据，甚至是不规则采样。尽管已经提出了许多插补方法，但它们大多数只适用于局部视角，即将长序列拆分为适当大小的批次进行训练。这种局部视角可能使模型忽略全局趋势或周期性模式。更重要的是，几乎所有方法都假设观测值在规则的时间间隔进行采样，并且无法处理来自不同应用的复杂不规则采样时间序列。此外，大多数现有方法都是在离线状态下进行学习的。因此，对于那些有快速到达的流数据的应用来说，它们并不合适。为了克服这些局限性，我们提出了BayOTIDE：基于贝叶斯方法的在线多元时间序列插补与函数分解。

    In real-world scenarios like traffic and energy, massive time-series data with missing values and noises are widely observed, even sampled irregularly. While many imputation methods have been proposed, most of them work with a local horizon, which means models are trained by splitting the long sequence into batches of fit-sized patches. This local horizon can make models ignore global trends or periodic patterns. More importantly, almost all methods assume the observations are sampled at regular time stamps, and fail to handle complex irregular sampled time series arising from different applications. Thirdly, most existing methods are learned in an offline manner. Thus, it is not suitable for many applications with fast-arriving streaming data. To overcome these limitations, we propose \ours: Bayesian Online Multivariate Time series Imputation with functional decomposition. We treat the multivariate time series as the weighted combination of groups of low-rank temporal factors with dif
    
[^47]: 通过语言学习对世界建模

    Learning to Model the World with Language. (arXiv:2308.01399v1 [cs.CL])

    [http://arxiv.org/abs/2308.01399](http://arxiv.org/abs/2308.01399)

    本论文提出了一种通过语言学习对世界进行建模的方法，利用语言帮助代理器预测未来并进行行动。通过学习多模态世界模型，代理器可以预测未来的文本和图像表示，并在模型回滚中进行行动。

    

    为了与人类在世界中相互作用，代理器需要理解人们使用的多样化的语言类型，并将其与视觉世界关联起来，并基于语言行动。虽然当前的代理器可以通过任务奖励学习执行简单的语言指令，但我们的目标是建立可以利用传达一般知识、描述世界状态、提供互动反馈等多样化语言的代理器。我们的核心思想是语言帮助代理器预测未来：将会被观察到什么、世界将如何运行以及哪些情况将获得奖励。这个观点将语言理解与未来预测统一为一个强大的自监督学习目标。我们提出了Dynalang，一种学习多模态世界模型的代理器，它可以预测未来的文本和图像表示，并在想像的模型回滚中学习行动。与只使用语言预测动作的传统代理器不同，Dynalang通过过去的语言还可以获取丰富的语言理解能力。

    To interact with humans in the world, agents need to understand the diverse types of language that people use, relate them to the visual world, and act based on them. While current agents learn to execute simple language instructions from task rewards, we aim to build agents that leverage diverse language that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that language helps agents predict the future: what will be observed, how the world will behave, and which situations will be rewarded. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We present Dynalang, an agent that learns a multimodal world model that predicts future text and image representations and learns to act from imagined model rollouts. Unlike traditional agents that use language only to predict actions, Dynalang acquires rich language understanding by using past language also to 
    
[^48]: AMEE：时间序列分类的解释评价框架

    AMEE: A Robust Framework for Explanation Evaluation in Time Series Classification. (arXiv:2306.05501v1 [cs.LG])

    [http://arxiv.org/abs/2306.05501](http://arxiv.org/abs/2306.05501)

    AMEE是一个模型无关的解释评价框架，用于量化和比较时间序列分类中多种基于显著性的解释方法的信息价值，帮助解决在这一领域中解释方法选择的难题。

    

    本文旨在提供一个框架，用于定量评估和排名时间序列分类任务中的解释方法，该任务涉及到卫生保健和金融等关键领域的普遍数据类型。最近对时间序列分类解释方法的研究兴趣激增，提供了各种各样的解释技术。然而，当这些解释技术在特定问题上产生分歧时，仍然不清楚使用哪种技术。比较解释以找到正确答案并不容易。两个关键挑战仍然存在：如何定量和稳健地评估给定解释方法的信息价值（即与分类任务相关性），以及如何并排比较解释方法。我们提出了AMEE，一种模型无关的解释评价框架，用于量化和比较多种应用于时间序列分类的基于显著性的解释方法。在输入时间序列中增加扰动

    This paper aims to provide a framework to quantitatively evaluate and rank explanation methods for the time series classification task, which deals with a prevalent data type in critical domains such as healthcare and finance. The recent surge of research interest in explanation methods for time series classification has provided a great variety of explanation techniques. Nevertheless, when these explanation techniques disagree on a specific problem, it remains unclear which of them to use. Comparing the explanations to find the right answer is non-trivial. Two key challenges remain: how to quantitatively and robustly evaluate the informativeness (i.e., relevance for the classification task) of a given explanation method, and how to compare explanation methods side-by-side. We propose AMEE, a Model-Agnostic Explanation Evaluation framework for quantifying and comparing multiple saliency-based explanations for time series classification. Perturbation is added to the input time series gu
    
[^49]: 面向网络的随机多层组合优化算法，具有独立于层数的收敛速度

    Stochastic Multi-Level Compositional Optimization Algorithms over Networks with Level-Independent Convergence Rate. (arXiv:2306.03322v1 [cs.LG])

    [http://arxiv.org/abs/2306.03322](http://arxiv.org/abs/2306.03322)

    本文提出了面向网络的随机多层组合优化算法，其中包括两种新的分散式优化算法，可实现独立于层数的收敛速度，理论结果和实验证明它们更加有效。

    

    随机多层组合优化问题涵盖了很多新的机器学习范例，如多步骤模型无关元学习，需要大规模应用的高效优化算法。本文研究了分散随机多层优化算法，这是一项具有挑战性的任务，因为多层结构和分散式通讯方案可能增加层数对收敛速度的影响。为此，我们开发了两种新的分散式优化算法，来处理多层函数和其梯度。我们的理论结果表明，与现有的单机算法相比，这两种算法均能在非凸问题中实现独立于层数的收敛速度，条件更加宽松。据我们所知，这是首次在分散式设置下实现独立于层数的收敛速度的工作。此外，广泛的实验证实了其有效性。

    Stochastic multi-level compositional optimization problems cover many new machine learning paradigms, e.g., multi-step model-agnostic meta-learning, which require efficient optimization algorithms for large-scale applications. This paper studies the decentralized stochastic multi-level optimization algorithm, which is challenging because the multi-level structure and decentralized communication scheme may make the number of levels affect the order of the convergence rate. To this end, we develop two novel decentralized optimization algorithms to deal with the multi-level function and its gradient. Our theoretical results show that both algorithms can achieve the level-independent convergence rate for nonconvex problems under much milder conditions compared with existing single-machine algorithms. To the best of our knowledge, this is the first work that achieves the level-independent convergence rate under the decentralized setting. Moreover, extensive experiments confirm the efficacy 
    
[^50]: 排列决策树

    Permutation Decision Trees. (arXiv:2306.02617v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02617](http://arxiv.org/abs/2306.02617)

    该论文提出了一种称为排列决策树的模型，通过引入一种新的复杂度度量方法，能够捕捉到数据实例的顺序依赖性，在不同排列的数据实例上得到不同的决策树模型，从而克服了传统决策树模型在处理顺序相关数据时的限制。

    

    决策树是一种基于最小化内部节点中的不纯度的机器学习模型。最常见的不纯度度量是香农熵和基尼不纯度。这些不纯度度量对训练数据的顺序不敏感，因此得到的最终树对数据的任何排列都是不变的。这导致了在建模存在顺序依赖性的数据实例时的严重限制。在这项工作中，我们首次提出使用“压缩努力”(ETC) - 一种复杂度度量，作为不纯度度量。与香农熵和基尼不纯度不同，基于ETC的结构性不纯度能够捕捉到数据的顺序依赖性，从而为相同数据实例的不同排列获得潜在不同的决策树（排列决策树）。然后，我们引入了使用排列决策树实现排列Bagging的概念，而无需随机特征选择和子采样。我们比较了翻译包…(信息不全)

    Decision Tree is a well understood Machine Learning model that is based on minimizing impurities in the internal nodes. The most common impurity measures are Shannon entropy and Gini impurity. These impurity measures are insensitive to the order of training data and hence the final tree obtained is invariant to any permutation of the data. This leads to a serious limitation in modeling data instances that have order dependencies. In this work, we propose the use of Effort-To-Compress (ETC) - a complexity measure, for the first time, as an impurity measure. Unlike Shannon entropy and Gini impurity, structural impurity based on ETC is able to capture order dependencies in the data, thus obtaining potentially different decision trees for different permutations of the same data instances (Permutation Decision Trees). We then introduce the notion of Permutation Bagging achieved using permutation decision trees without the need for random feature selection and sub-sampling. We compare the pe
    
[^51]: 动态上下文剪枝用于高效和可解释的自回归变换器

    Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers. (arXiv:2305.15805v1 [cs.CL])

    [http://arxiv.org/abs/2305.15805](http://arxiv.org/abs/2305.15805)

    本研究提出了一种动态上下文剪枝方法，可以在保持模型表现力的同时，动态减少无效信息，提高模型的效率和可解释性。该技术可以应用于现有的预训练模型，并且可以通过简单的微调过程实现。

    

    大型语言模型中采用的自回归变换器难以扩展到长序列。尽管有几项工作试图减少它们的计算成本，但大多数LLM仍然在所有标记对之间采用注意层，从而产生二次成本。本研究提出了一种新方法，通过保留模型的表现力来动态修剪上下文信息，从而在推理过程中减少内存和计算要求。我们的方法使用可学习机制，在生成过程中确定哪些无关的标记可以从上下文中删除。通过这样做，我们的方法不仅解决了性能问题，而且增强了可解释性，为模型的决策过程提供了宝贵的洞察力。我们的技术可以通过简单的微调过程应用于现有的预训练模型，并且剪枝强度可以由稀疏度参数指定。

    Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity para
    
[^52]: 带有语音的LM：超越语音令牌的口语语言建模

    LMs with a Voice: Spoken Language Modeling beyond Speech Tokens. (arXiv:2305.15255v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.15255](http://arxiv.org/abs/2305.15255)

    SPECTRON是一个新颖的语音延续模型，通过利用预训练的语言模型和语音编码器进行端到端的训练来生成文本和语音输出，在语义内容和讲话者保护方面超越了现有的口语语言模型。

    

    我们提出了SPECTRON，一种新颖的方法来适应预训练的语言模型（LM）以执行语音延续。通过利用预训练的语音编码器，我们的模型可以生成文本和语音输出，整个系统都在频谱图上进行端到端的训练。在频谱图领域训练整个模型相对于使用离散语音表示的现有级联方法简化了我们的语音延续系统。我们进一步展示了我们的方法在语义内容和讲话者保护方面超过了现有的口语语言模型，同时也从预先存在的模型中获得了知识传递的好处。我们的网站https://michelleramanovich.github.io/spectron/spectron上可以找到音频样本。

    We present SPECTRON, a novel approach to adapting pre-trained language models (LMs) to perform speech continuation. By leveraging pre-trained speech encoders, our model generates both text and speech outputs with the entire system being trained end-to-end operating directly on spectrograms. Training the entire model in the spectrogram domain simplifies our speech continuation system versus existing cascade methods which use discrete speech representations. We further show our method surpasses existing spoken language models both in semantic content and speaker preservation while also benefiting from the knowledge transferred from pre-existing models. Audio samples can be found in our website https://michelleramanovich.github.io/spectron/spectron
    
[^53]: 图中长尾类别的特征化

    Characterizing Long-Tail Categories on Graphs. (arXiv:2305.09938v1 [cs.LG])

    [http://arxiv.org/abs/2305.09938](http://arxiv.org/abs/2305.09938)

    该研究提出了图上长尾分类的第一个泛化边界，并提出了一种可表征长尾类别的行为并提高机器学习模型在现实世界网络中的泛化性能的新图表示学习框架。

    

    长尾数据分布在许多现实世界的网络中普遍存在，包括金融交易网络、电子商务网络和合作网络。尽管最近取得了成功，但现有的作品主要集中于通过图增强或目标重新加权消除机器学习模型的偏见。然而，目前有限的文献提供理论工具来表征图上长尾类别的行为，并理解实际情况下的泛化性能。为填补这一空白，我们通过将问题形式化为多任务学习的方式，即每个任务对应于预测一个特定的类别，提出了图上长尾分类的第一个泛化边界。我们的理论结果表明，长尾分类的泛化性能受所有任务中的损失范围和任务总数的支配。在理论发现的基础上，我们提出了一种新的图表示学习框架，可表征长尾类别的行为，并提高机器学习模型在现实世界网络中的泛化性能。

    Long-tail data distributions are prevalent in many real-world networks, including financial transaction networks, e-commerce networks, and collaboration networks. Despite the success of recent developments, the existing works mainly focus on debiasing the machine learning models via graph augmentation or objective reweighting. However, there is limited literature that provides a theoretical tool to characterize the behaviors of long-tail categories on graphs and understand the generalization performance in real scenarios. To bridge this gap, we propose the first generalization bound for long-tail classification on graphs by formulating the problem in the fashion of multi-task learning, i.e., each task corresponds to the prediction of one particular category. Our theoretical results show that the generalization performance of long-tail classification is dominated by the range of losses across all tasks and the total number of tasks. Building upon the theoretical findings, we propose a n
    
[^54]: 理解异构数据联邦学习中的模型平均

    Understanding Model Averaging in Federated Learning on Heterogeneous Data. (arXiv:2305.07845v1 [cs.LG])

    [http://arxiv.org/abs/2305.07845](http://arxiv.org/abs/2305.07845)

    本文研究了异构数据联邦学习中的模型平均技术，通过可视化损失/错误景观揭示了客户端模型环绕全局模型在一个共同的盆地内，并且发现全局模型在早期训练后的误差主要来自客户端数据集和全局数据集之间非重叠的数据及全局模型与客户端模型之间的最大距离两个因素。

    

    模型平均是联邦学习中广泛采用的一种技术，它会聚集训练于异构数据上的多个客户端模型以获得表现良好的全局模型。然而，其成功背后的原理尚不是很清楚。本文通过可视化损失/错误景观来研究模型平均的几何特性，揭示了客户端模型环绕全局模型在一个共同的盆地内，并且即使全局模型表现优异，也可能偏离盆地底部。进一步的分析表明，全局模型在早期训练后的误差主要来自客户端数据集和全局数据集之间非重叠的数据及全局模型与客户端模型之间的最大距离两个因素。

    Model averaging, a widely adopted technique in federated learning (FL), aggregates multiple client models trained on heterogeneous data to obtain a well-performed global model. However, the rationale behind its success is not well understood. To shed light on this issue, we investigate the geometric properties of model averaging by visualizing the loss/error landscape. The geometrical visualization shows that the client models surround the global model within a common basin, and the global model may deviate from the bottom of the basin even though it performs better than the client models. To further understand this phenomenon, we decompose the expected prediction error of the global model into five factors related to client models. Specifically, we find that the global-model error after early training mainly comes from i) the client-model error on non-overlapping data between client datasets and the global dataset and ii) the maximal distance between the global and client models. Insp
    
[^55]: 跨图动态迁移学习

    Dynamic Transfer Learning across Graphs. (arXiv:2305.00664v1 [cs.LG])

    [http://arxiv.org/abs/2305.00664](http://arxiv.org/abs/2305.00664)

    该论文提出了一个新的问题：在动态图形环境下如何有效地进行跨图迁移学习，主要解决了领域演化对泛化性能的影响。

    

    在许多高风险领域中，跨图传输知识起着关键作用，包括运输网络、电子商务网络、神经科学和金融领域。我们提出了一个新问题：在动态设置下，考虑已观察到的具有标签的源图和标签稀疏的目标图，如何有效地表征不断变化的领域偏差，并优化目标域在下一个时间戳的泛化性能？为了回答这个问题，我们首次提出了跨图动态迁移学习设置下的一般化界限，这意味着泛化性能由领域演化控制。

    Transferring knowledge across graphs plays a pivotal role in many high-stake domains, ranging from transportation networks to e-commerce networks, from neuroscience to finance. To date, the vast majority of existing works assume both source and target domains are sampled from a universal and stationary distribution. However, many real-world systems are intrinsically dynamic, where the underlying domains are evolving over time. To bridge the gap, we propose to shift the problem to the dynamic setting and ask: given the label-rich source graphs and the label-scarce target graphs observed in previous T timestamps, how can we effectively characterize the evolving domain discrepancy and optimize the generalization performance of the target domain at the incoming T+1 timestamp? To answer the question, for the first time, we propose a generalization bound under the setting of dynamic transfer learning across graphs, which implies the generalization performance is dominated by domain evolution
    
[^56]: 联邦组合深度AUC最大化

    Federated Compositional Deep AUC Maximization. (arXiv:2304.10101v1 [cs.LG])

    [http://arxiv.org/abs/2304.10101](http://arxiv.org/abs/2304.10101)

    本论文介绍了一种通过直接优化AUC分数来解决联邦学习中不平衡数据问题的方法，并开发了一个随机组合梯度下降上升动量算法。通过广泛的实验验证，证实了该方法的效率和有效性。

    

    联邦学习由于平衡隐私和大规模学习的承诺而引起了越来越多的关注；已经提出了许多方法。然而，大多数现有方法都集中在处理平衡数据问题上，而在许多现实应用中，不同类别中的样本数量高度不平衡，导致预测性能远低于理想水平。为了解决这个具有挑战性的问题，我们开发了一种针对不平衡数据的新型联邦学习方法，通过直接优化曲线下面积（AUC）分数来提高预测性能。具体来说，我们将AUC最大化问题作为联邦组合最小最大优化问题进行了表述，并开发了本地随机组合梯度下降上升动量算法，并提供了我们算法的计算和通信复杂度的界限。据我们所知，这是第一个实现如此有利理论结果的工作。最后，广泛的实验结果证实了我们提出的方法的效率和有效性。

    Federated learning has attracted increasing attention due to the promise of balancing privacy and large-scale learning; numerous approaches have been proposed. However, most existing approaches focus on problems with balanced data, and prediction performance is far from satisfactory for many real-world applications where the number of samples in different classes is highly imbalanced. To address this challenging problem, we developed a novel federated learning method for imbalanced data by directly optimizing the area under curve (AUC) score. In particular, we formulate the AUC maximization problem as a federated compositional minimax optimization problem, develop a local stochastic compositional gradient descent ascent with momentum algorithm, and provide bounds on the computational and communication complexities of our algorithm. To the best of our knowledge, this is the first work to achieve such favorable theoretical results. Finally, extensive experimental results confirm the effi
    
[^57]: 通过离线数据提升蒙特卡罗评估方法

    Improving Monte Carlo Evaluation with Offline Data. (arXiv:2301.13734v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13734](http://arxiv.org/abs/2301.13734)

    本论文介绍了通过使用离线数据来提升蒙特卡罗评估方法，实现在保持相同估计准确度的前提下，减少在线样本数量的目的。通过使用一个定制的行为策略，可以比普通的 MC 估计器产生更小的方差。该行为策略可以从现有的离线数据中高效学习，我们的实验表明，相对于现有的最先进方法，我们的方法只需使用小部分在线样本就能实现相同的估计精度。

    

    蒙特卡罗 (MC) 方法是估计策略表现最广泛使用的方法。给定一个感兴趣的策略，MC 方法通过重复运行该策略以收集样本并取出结果平均值来给出估计值。在此过程中收集的样本称为在线样本。为了获得准确的估计值，MC 方法需要消耗大量在线样本。当在线样本昂贵时，例如在线推荐和库存管理，我们希望在实现相同的估计准确度的同时减少在线样本数量。为此，我们使用离线 MC 方法，通过运行不同的策略（称为行为策略）评估感兴趣的策略。我们设计了一个定制的行为策略，使离线 MC 估计器的方差明显小于普通 MC 估计器。重要的是，该定制行为策略可以从现有的离线数据，即先前记录的数据中高效学习，这比在线样本要便宜得多。我们的实验表明，与现有的最先进方法相比，我们的方法只需使用小部分在线样本就能实现相同的估计精度。

    Monte Carlo (MC) methods are the most widely used methods to estimate the performance of a policy. Given an interested policy, MC methods give estimates by repeatedly running this policy to collect samples and taking the average of the outcomes. Samples collected during this process are called online samples. To get an accurate estimate, MC methods consume massive online samples. When online samples are expensive, e.g., online recommendations and inventory management, we want to reduce the number of online samples while achieving the same estimate accuracy. To this end, we use off-policy MC methods that evaluate the interested policy by running a different policy called behavior policy. We design a tailored behavior policy such that the variance of the off-policy MC estimator is provably smaller than the ordinary MC estimator. Importantly, this tailored behavior policy can be efficiently learned from existing offline data, i,e., previously logged data, which are much cheaper than onlin
    
[^58]: RFold：基于解耦优化方法的RNA二级结构预测

    RFold: RNA Secondary Structure Prediction with Decoupled Optimization. (arXiv:2212.14041v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2212.14041](http://arxiv.org/abs/2212.14041)

    所提出的RFold方法采用解耦优化过程和注意力机制进行简单又有效的RNA二级结构预测，具有较高的准确性和速度。

    

    核糖核酸（RNA）的二级结构比三级结构更稳定和更易于在细胞中访问，因此对于功能预测至关重要。尽管深度学习在这个领域中显示出了很好的结果，但当前的方法存在泛化性差和复杂性高的问题。在这项工作中，我们提出了一种简单而有效的RNA二级结构预测方法RFold。RFold引入了一种解耦优化的过程，将传统的约束满足问题分解为逐行和逐列优化，简化了求解过程，同时保证了输出的有效性。此外，RFold采用注意力地图作为信息表示，而不是设计手工特征。广泛的实验表明，RFold具有竞争性能，并且比现有最先进的方法具有约8倍的推理效率。代码和Colab演示可在\href{this http URL}{this http UR}上找到。

    The secondary structure of ribonucleic acid (RNA) is more stable and accessible in the cell than its tertiary structure, making it essential for functional prediction. Although deep learning has shown promising results in this field, current methods suffer from poor generalization and high complexity. In this work, we present RFold, a simple yet effective RNA secondary structure prediction in an end-to-end manner. RFold introduces a decoupled optimization process that decomposes the vanilla constraint satisfaction problem into row-wise and column-wise optimization, simplifying the solving process while guaranteeing the validity of the output. Moreover, RFold adopts attention maps as informative representations instead of designing hand-crafted features. Extensive experiments demonstrate that RFold achieves competitive performance and about eight times faster inference efficiency than the state-of-the-art method. The code and Colab demo are available in \href{this http URL}{this http UR
    
[^59]: 主动推理和强化学习：在部分可观测的连续状态和动作空间下的统一推理

    Active Inference and Reinforcement Learning: A unified inference on continuous state and action spaces under partially observability. (arXiv:2212.07946v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.07946](http://arxiv.org/abs/2212.07946)

    本论文提出了一种能够在部分可观测的连续状态和动作空间下进行统一推理的框架，通过最小化期望自由能函数指导代理选择动作，以实现最大化奖励的决策制定。

    

    强化学习在完全可观测环境中开发决策制定代理以最大化由外部监督员指定的奖励引起了极大关注。然而，许多现实世界的问题涉及部分观测，形式化为部分可观测的马尔可夫决策过程（POMDP）。以往的研究通过将过去的行动和观测记忆或通过从观测数据中推断环境的真实状态来解决POMDP中的强化学习问题。然而，在连续空间中随时间聚合观测数据变得不可行。此外，基于推理的强化学习方法通常需要许多样本才能表现良好，因为它们仅关注奖励最大化，忽视了推断状态的不确定性。主动推理（AIF）是在POMDP中制定的一种框架，通过最小化一个称为期望自由能（EFE）的函数指导代理选择动作。这提供了最大化奖励（富有开发性）的行为。

    Reinforcement learning (RL) has garnered significant attention for developing decision-making agents that aim to maximize rewards, specified by an external supervisor, within fully observable environments. However, many real-world problems involve partial observations, formulated as partially observable Markov decision processes (POMDPs). Previous studies have tackled RL in POMDPs by either incorporating the memory of past actions and observations or by inferring the true state of the environment from observed data. However, aggregating observed data over time becomes impractical in continuous spaces. Moreover, inference-based RL approaches often require many samples to perform well, as they focus solely on reward maximization and neglect uncertainty in the inferred state. Active inference (AIF) is a framework formulated in POMDPs and directs agents to select actions by minimizing a function called expected free energy (EFE). This supplies reward-maximizing (exploitative) behaviour, as
    
[^60]: 提高准确性: 基于时代驱动的混合尾数块浮点方法用于DNN训练

    Accuracy Boosters: Epoch-Driven Mixed-Mantissa Block Floating-Point for DNN Training. (arXiv:2211.10737v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.10737](http://arxiv.org/abs/2211.10737)

    本文提出了一种基于时代驱动的混合尾数HBFP技术，通过对不同参数的探索和优化，实现了对DNN训练中算术操作的更小编码。使用分析模型表明，该方法能够将HBFP训练加速器的算术密度增加高达$21.3\times$。

    

    DNN模型复杂性、规模和训练数据量的前所未有增长导致了对计算的巨大需求和对最小编码的搜索。最近的研究主张使用混合块浮点(HBFP)来最小化加速器中的硅配备，通过将大部分训练中的算术操作转换为8位定点。本文通过数学工具对HBFP设计空间进行了全面的探索，研究了各种参数之间的相互作用，并确定了在各层和各个时代中更小编码的机会。基于我们的发现，我们提出了Accuracy Boosters，一种基于时代驱动的混合尾数HBFP技术，只在最后一个时代和第一个/最后一层中使用6位尾数，在训练中的其他算术操作中使用4位尾数达到$99.7\%$。使用分析模型，我们展示了Accuracy Boosters可以使HBFP训练加速器的算术密度增加高达$21.3\times$。

    The unprecedented growth in DNN model complexity, size, and amount of training data has led to a commensurate increase in demand for computing and a search for minimal encoding. Recent research advocates Hybrid Block Floating Point (HBFP) to minimize silicon provisioning in accelerators by converting the majority of arithmetic operations in training to 8-bit fixed point. In this paper, we perform a full-scale exploration of the HBFP design space using mathematical tools to study the interplay among various parameters and identify opportunities for even smaller encodings across layers and epochs. Based on our findings, we propose Accuracy Boosters, an epoch-driven mixed-mantissa HBFP technique that uses 6-bit mantissas only in the last epoch and first/last layers, and 4-bit mantissas for $99.7\%$ of all other arithmetic operations in training. Using analytic models, we show Accuracy Boosters enable increasing arithmetic density for an HBFP training accelerator by up to $21.3\times$ comp
    
[^61]: 运动感知标记选择实现高效视频表征学习

    Efficient Video Representation Learning via Motion-Aware Token Selection. (arXiv:2211.10636v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.10636](http://arxiv.org/abs/2211.10636)

    该论文提出了一种新的运动感知标记选择方法，针对视频中不同补丁的信息密度，选择包含丰富动态特性的标记，放弃无效的标记，从而大大降低计算和存储需求，实现了在单台机器上进行预训练和微调而不影响性能。

    

    最近出现的蒙版视频建模技术通过在视频的自我监督学习中获得了显着的优势。然而，由于随机蒙版策略导致预测无效的标记/帧，这些技术需要大量的计算和存储，需要昂贵的计算机和大量显卡进行训练。我们利用视频补丁中的不均匀信息密度，并提出一种新的标记选择方法：MATS：运动感知标记选择，在自监督预训练和微调过程中找到包含丰富动态特性的标记，并放弃无效的标记，我们还提出了自适应帧选择策略，使模型能够关注最重要和因果性的帧，并使计算和存储需求得到显着降低，使得在单台机器上进行预训练和微调而不影响性能。

    Recently emerged Masked Video Modeling techniques demonstrated their potential by significantly outperforming previous methods in self-supervised learning for video. However, they require an excessive amount of computations and memory while predicting uninformative tokens/frames due to random masking strategies, requiring excessive computing power for training. (e.g., over 16 nodes with 128 NVIDIA A100 GPUs). To resolve this issue, we exploit the unequal information density among the patches in videos and propose a new token selection method, MATS: Motion-Aware Token Selection, that finds tokens containing rich motion features and drops uninformative ones during both self-supervised pre-training and fine-tuning. We further present an adaptive frame selection strategy that allows the model to focus on informative and causal frames with minimal redundancy. Our method significantly reduces computation and memory requirements, enabling the pre-training and fine-tuning on a single machine w
    
[^62]: 传输学习的共同直觉可以带来胜利或失败: 线性回归案例研究

    The Common Intuition to Transfer Learning Can Win or Lose: Case Studies for Linear Regression. (arXiv:2103.05621v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.05621](http://arxiv.org/abs/2103.05621)

    本论文研究了传输学习的基本过程，针对线性回归任务，通过利用源任务参数和目标任务训练数据，提出了一种传输学习方法。我们分析了该方法的泛化性能，并展示了其在解决线性回归中的泛化误差峰值方面的能力。此外，我们证明了在足够相关的任务中，该传输学习方法可以优于岭回归方法。

    

    我们研究了从源回归任务到目标回归任务的基本传输学习过程，包括在有比数据样本更多的学习参数的过参数化设置中。目标任务的学习通过使用其训练数据和先前计算的源任务参数来解决。我们将目标任务的传输学习方法定义为带有正则化的线性回归优化，其中正则化项是待学习的目标参数与已学习的源参数之间的距离。我们分析地表征了我们的传输学习方法的泛化性能，并展示了它解决线性回归的最小L2范数解中的泛化误差峰值的能力。此外，我们证明了对于足够相关的任务来说，经过最佳调优的传输学习方法可以胜过最佳调优的岭回归方法，即使真实参数向量符合最小L2范数解。

    We study a fundamental transfer learning process from source to target linear regression tasks, including overparameterized settings where there are more learned parameters than data samples. The target task learning is addressed by using its training data together with the parameters previously computed for the source task. We define a transfer learning approach to the target task as a linear regression optimization with a regularization on the distance between the to-be-learned target parameters and the already-learned source parameters. We analytically characterize the generalization performance of our transfer learning approach and demonstrate its ability to resolve the peak in generalization errors in double descent phenomena of the minimum L2-norm solution to linear regression. Moreover, we show that for sufficiently related tasks, the optimally tuned transfer learning approach can outperform the optimally tuned ridge regression method, even when the true parameter vector conform
    

