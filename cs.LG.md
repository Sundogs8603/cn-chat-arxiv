# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Input-Aware Dynamic Timestep Spiking Neural Networks for Efficient In-Memory Computing.](http://arxiv.org/abs/2305.17346) | DT-SNN是一种输入感知的动态时间步长脉冲神经网络算法解决方案， 可以根据数据特性动态确定时间步长的数量，从而将SNN的效率最大化。 |
| [^2] | [Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in Multi-Agent RL.](http://arxiv.org/abs/2305.17342) | 本文介绍了另一种常见、现实的多智能体RL攻击设置，提出了一种模拟攻击者对代理$\alpha$控制的更一般化攻击形式。并解决了先前攻击模型中缺乏可证明防御的问题。 |
| [^3] | [Improved Privacy-Preserving PCA Using Space-optimized Homomorphic Matrix Multiplication.](http://arxiv.org/abs/2305.17341) | 本文提出一种使用近似数值算术同态加密方案进行隐私保护PCA的新方法，相对以往方法，其在效率、准确性和可扩展性上均有提升，实现了同态矩阵乘法和高效同态电路，计算特征值和特征向量时具有良好的效果。 |
| [^4] | [Fine-Tuning Language Models with Just Forward Passes.](http://arxiv.org/abs/2305.17333) | 本论文提出了一种内存高效的零阶优化器，可以使用与推理相同的存储空间微调语言模型，其可以在大规模模型下更快地优化，具有更好的实验结果。 |
| [^5] | [Learning Capacity: A Measure of the Effective Dimensionality of a Model.](http://arxiv.org/abs/2305.17332) | 学习能力是一种度量模型有效维度的方法，它可以帮助我们判断是否需要获取更多数据或者寻找新的体系结构以提高性能。 |
| [^6] | [Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In.](http://arxiv.org/abs/2305.17331) | 本文提出了增强适应检索器(AAR)的方案，通过从已知的源LM中学习LM的偏好，能够以通用插件的形式帮助目标LM在不进行微调的情况下显著提高零样本泛化能力。 |
| [^7] | [MADiff: Offline Multi-agent Learning with Diffusion Models.](http://arxiv.org/abs/2305.17330) | 本论文提出了基于注意力的扩散模型MADiff，解决了多智能体问题，是第一个扩散模型应用于多智能体离线RL的框架。 |
| [^8] | [Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers.](http://arxiv.org/abs/2305.17328) | 《Zero-TPrune》是一个考虑到令牌的重要性和相似性的零射击方法，它利用预训练Transformer模型的注意图来进行令牌剪枝，以求解在边缘设备上Transformer模型即插即用的难题。 |
| [^9] | [Kernel-SSL: Kernel KL Divergence for Self-supervised Learning.](http://arxiv.org/abs/2305.17326) | 本文提出了一种名为Kernel-SSL的自监督学习方法，将多种现有非对比学习方法建立在了再生核希尔伯特空间（RKHS）理解之上并优化了其中的均值嵌入和协方差算子，实验结果显示，在ImageNet数据集下表现显著超越最先进的方法，提高了4.6%。 |
| [^10] | [Some Primal-Dual Theory for Subgradient Methods for Strongly Convex Optimization.](http://arxiv.org/abs/2305.17323) | 本文提出了一种强凸优化的次梯度法原始对偶理论，可以实现简单的、最佳的停止准则和优化证明，同时可以适用于各种步长的选择和非Lipschitz病态问题，保证了这些方法次线性收敛速度。 |
| [^11] | [Radar Enlighten the Dark: Enhancing Low-Visibility Perception for Automated Vehicles with Camera-Radar Fusion.](http://arxiv.org/abs/2305.17318) | 针对低能见度的自动驾驶车辆感知问题，本文提出了一种新的基于Transformer的3D目标检测模型“REDFormer”，通过鸟瞰相机-雷达融合进行实现。该模型在nuScenes数据集上表现出优异的分类和检测准确性，且相较于现有模型更经济实用。 |
| [^12] | [Automatic Roof Type Classification Through Machine Learning for Regional Wind Risk Assessment.](http://arxiv.org/abs/2305.17315) | 本论文使用机器学习技术开发了一个可以自动识别屋顶类型的分类框架，从卫星图像上提取建筑级别的高分辨率屋顶类型数据，填补了公开数据库缺失的数据，为区域风险评估提供了有效的工具。 |
| [^13] | [Beyond Positive Scaling: How Negation Impacts Scaling Trends of Language Models.](http://arxiv.org/abs/2305.17311) | 本研究介绍了一个包含否定问题的数据集NeQA，其中语言模型表现出反向缩放、U型缩放或正向缩放，解决NeQA依赖于问答和否定理解两个子任务，其缩放趋势由这两个子任务的缩放趋势组合形成。 |
| [^14] | [Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance.](http://arxiv.org/abs/2305.17306) | 本文介绍了一个名为 Chain-of-Thought Hub 的开源评估套件，目的是评估大型语言模型的多步推理能力。它是为了追踪LLMs进展而编制的具有挑战性的推理基准。目前的研究结果表明，模型规模与推理能力相关，而 Claude-v1.3 是迄今为止推理能力最强的LLM。 |
| [^15] | [Distilling BlackBox to Interpretable models for Efficient Transfer Learning.](http://arxiv.org/abs/2305.17303) | 本论文提出了一种方法可以将黑盒模型转化成为可解释模型并在目标领域成功进行迁移学习，从而实现高效迁移学习。 |
| [^16] | [Stability-penalty-adaptive Follow-the-regularized-leader: Sparsity, Game-dependency, and Best-of-both-worlds.](http://arxiv.org/abs/2305.17301) | 本文开发了一种稳定性惩罚自适应（SPA）学习率，该学习率使FTRL具有稀疏性、游戏依赖性和最佳世界（BOBW）三种适应性类型，其中SPA-sparse算法可适应于未知的稀疏级别，SPA-game-dependency算法可根据所玩的游戏自适应地改变其行为，BOBW算法则是既具有稀疏性又具有游戏依赖性的适应性算法。 |
| [^17] | [Exploiting Large Neuroimaging Datasets to Create Connectome-Constrained Approaches for more Robust, Efficient, and Adaptable Artificial Intelligence.](http://arxiv.org/abs/2305.17300) | 该论文讨论了如何利用大型神经影像数据集改进机器学习方法，以创造更加稳健、高效和适应性强的人工智能。通过发现重复子电路和分析果蝇的航向方向电路，该论文提出了新的连接模式和模型，以探索如何进一步扩展现有的计算模型。 |
| [^18] | [Improving Stability in Decision Tree Models.](http://arxiv.org/abs/2305.17299) | 本文通过医疗应用的视角，提出了一种新的决策树距离度量，并用它来确定树的稳定水平。我们提出了一种新的培训稳定决策树的方法，并探究稳定性、预测能力和可解释性之间不可避免的权衡。 |
| [^19] | [Generalization Error without Independence: Denoising, Linear Regression, and Transfer Learning.](http://arxiv.org/abs/2305.17297) | 本论文研究了具有低秩结构但非独立同分布数据的情况，在分离训练和测试分布的假设下，解决了分布偏移问题，实验结果表明，在分布偏移的情况下，本方法显著提高了泛化误差的性能。 |
| [^20] | [Fourier-DeepONet: Fourier-enhanced deep operator networks for full waveform inversion with improved accuracy, generalizability, and robustness.](http://arxiv.org/abs/2305.17289) | 该论文提出了Fourier-DeepONet算法，可用于完全波形反演，具有对地震源的泛化能力，可提高准确性和鲁棒性，适用于具有可变源的FWI。 |
| [^21] | [GC-Flow: A Graph-Based Flow Network for Effective Clustering.](http://arxiv.org/abs/2305.17284) | GC-Flow是一种生成模型，可以同时建模类别条件概率和类别先验，通过配备高斯混合表示空间，保持预测能力的同时实现了良好分离的聚类。 |
| [^22] | [Sharpened Lazy Incremental Quasi-Newton Method.](http://arxiv.org/abs/2305.17283) | 本文提出了一种新算法—— Sharpened Lazy Incremental Quasi-Newton (SLIQN) 方法，其具有显式的超线性收敛速率和$O(d^2)$的迭代复杂度。 |
| [^23] | [Universal consistency of the $k$-NN rule in metric spaces and Nagata dimension. II.](http://arxiv.org/abs/2305.17282) | 本文研究了k近邻学习规则中的普遍一致性，发现在可分度量空间中，该规则在Nagata维度下的sigma有限维度的空间中是普遍一致的，在非阿基米德度量空间中是强普遍一致的，此规则在具有de Groot有限维度意义下的度量空间和Heisenberg群中也是普遍一致的。 |
| [^24] | [Optimizing NOTEARS Objectives via Topological Swaps.](http://arxiv.org/abs/2305.17277) | 本文提出了一种双层算法来解决学习DAGs中的非凸优化问题，其中外层利用拓扑交换优化拓扑顺序，通过开发一种候选交换对的方法，算法在学习高质量DAGs方面具有高效和稳定的优势。 |
| [^25] | [Local Convergence of Gradient Methods for Min-Max Games under Partial Curvature.](http://arxiv.org/abs/2305.17275) | 该研究探讨了博弈理论中梯度方法的局部收敛性，在部分曲率条件下保证了局部收敛，同时平均特征值比最小特征值更能体现其收敛速度。 |
| [^26] | [Robust Lane Detection through Self Pre-training with Masked Sequential Autoencoders and Fine-tuning with Customized PolyLoss.](http://arxiv.org/abs/2305.17271) | 本论文提出了一种鲁棒车道检测流水线，该流水线包括自预训练掩模序列自编码器和使用定制PolyLoss微调的端到端神经网络模型。掩模序列自编码器被采用以通过重构随机掩膜图像中的丢失像素为目标来预训练神经网络模型，提升了车道检测性能。 |
| [^27] | [Closing the Gap in High-Risk Pregnancy Care Using Machine Learning and Human-AI Collaboration.](http://arxiv.org/abs/2305.17261) | 本论文介绍了一种基于机器学习和人工智能协作的高危孕产妇计划，提出了早期妊娠检测、准确识别高风险会员和提供可解释指标等三个挑战的解决方案，提高了孕期风险的预测准确率。 |
| [^28] | [Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning.](http://arxiv.org/abs/2305.17256) | 本文探讨了大型语言模型在上下文学习中利用提示中的捷径的依赖性，发现大型模型更有可能在推理过程中利用提示中的捷径，这为评估上下文学习的稳健性和检测和缓解提示中捷径的使用提供了新的视角和挑战。 |
| [^29] | [FineMorphs: Affine-diffeomorphic sequences for regression.](http://arxiv.org/abs/2305.17255) | FineMorphs是一种多元回归模型，通过形状分析中的微分同胚概念对模型状态进行优化，能够自然地减少（或增加）维度并适应大数据集。 |
| [^30] | [Duality in Multi-View Restricted Kernel Machines.](http://arxiv.org/abs/2305.17251) | 该论文提出了一个统一的框架，将现有的受限制内核机器方法结合成一个单一的原始-对偶多角度框架，可用于核主成分分析，实现了原始和对偶公式的完全等价性，并最终在时间序列数据集上验证了其等价性和提供的洞察。 |
| [^31] | [Self-Supervised Reinforcement Learning that Transfers using Random Features.](http://arxiv.org/abs/2305.17250) | 该论文提出了一种自监督增强学习方法，能够在不同奖励的任务间进行行为迁移，同时避免有模型强化学习的挑战。使用一些随机特征作为奖励，进行自监督预训练能够暗含长期环境动态模型，然后使用这些隐式模型的规划技术能够在短时间内适应问题。 |
| [^32] | [NASimEmu: Network Attack Simulator & Emulator for Training Agents Generalizing to Novel Scenarios.](http://arxiv.org/abs/2305.17246) | 该论文提出了一个名为NASimEmu的新框架，旨在提高智能体在现实世界中表现良好的能力。该框架使用模拟器和仿真器的结合，使智能体能够在模拟中进行训练，并在仿真器中部署，从而验证所使用的抽象的真实性。该框架的设计旨在培训通用的智能体，能够在训练期间未见过的新场景中进行转移。 |
| [^33] | [Causal Component Analysis.](http://arxiv.org/abs/2305.17225) | 本文介绍了一个中间问题：因果成分分析(CauCA)，它是独立成分分析(ICA)和因果表示学习(CRL)的泛化和特例，其目标是学习解混函数和因果机制，预设了因果图的知识。 |
| [^34] | [Fast and Minimax Optimal Estimation of Low-Rank Matrices via Non-Convex Gradient Descent.](http://arxiv.org/abs/2305.17224) | 本文提出一种针对低秩矩阵估计的方法，在保证极小极值优化性能的同时，解决了非凸梯度下降收敛缓慢的问题。 |
| [^35] | [Federated Learning for Semantic Parsing: Task Formulation, Evaluation Setup, New Algorithms.](http://arxiv.org/abs/2305.17221) | 本文研究了基于联邦学习的语义解析任务，提出了评估设置和新算法。实验表明，新算法FedSQL和Lorar优于现有的FL算法和我们提出的设置的强基线。 |
| [^36] | [GVdoc: Graph-based Visual Document Classification.](http://arxiv.org/abs/2305.17219) | GVdoc 是一个基于图的文档分类模型，能够通过生成文档图并训练图神经网络来学习节点和图嵌入，有效解决视觉文档分类器在领域内外样本分类和区分中所遇到的挑战。 |
| [^37] | [Generating Images with Multimodal Language Models.](http://arxiv.org/abs/2305.17216) | 该论文提出了一种方法，将大型语言模型与预训练的图像编码器和解码器模型进行融合，能生成具有连贯性的图像输出，同时也能进行图像检索和多模态对话。 |
| [^38] | [Rotational Optimizers: Simple & Robust DNN Training.](http://arxiv.org/abs/2305.17212) | 该论文提出了旋转优化器，这些优化器可以简化深度神经网络训练过程，甚至在几乎不需调整基线超参数的情况下与原始优化器的性能相匹配。 |
| [^39] | [Functional Flow Matching.](http://arxiv.org/abs/2305.17209) | 本文介绍了一种名为功能流匹配（FFM）的函数空间生成模型，该模型利用概率测度插值和学习底层函数空间上生成测度的向量场来生成数据分布。这种无需似然或模拟的方法在合成和真实世界基准数据集上表现优异，优于最近提出的几种函数空间生成模型。 |
| [^40] | [Ghost Noise for Regularizing Deep Neural Networks.](http://arxiv.org/abs/2305.17205) | 本文研究了幽灵批量归一化（GBN）中的“幽灵噪声”，提出了一种新的正则化技术Ghost Noise Injection (GNI)，该方法能够避免小批量训练带来的训练-测试差异效应，并在深度神经网络中提供更好的泛化效果。 |
| [^41] | [Improved Sales Forecasting using Trend and Seasonality Decomposition with LightGBM.](http://arxiv.org/abs/2305.17201) | 本文提出了一种根据趋势和季节性分量在时间序列上的独特影响指标进行时间序列分组的新方法，并采用 LightGBM 模型进行预测，在沃尔玛销售数据上实现了较高的预测精度。 |
| [^42] | [A Model-Based Solution to the Offline Multi-Agent Reinforcement Learning Coordination Problem.](http://arxiv.org/abs/2305.17198) | 提出了一个基于模型的离线多智能体强化学习方法MOMA-PPO，通过生成合成交互数据并优化智能体的政策，解决了策略一致性和策略微调两个协调问题，在具有挑战性的离线MARL场景中胜过主流的学习方法，提供了实际应用中的可行解决方案。 |
| [^43] | [AI-based analysis of super-resolution microscopy: Biological discovery in the absence of ground truth.](http://arxiv.org/abs/2305.17193) | 利用弱监督学习范例对超分辨率显微镜进行分析具有发现新生物学的巨大潜力，并且可以加速探索亚细胞大分子和细胞器分子结构。 |
| [^44] | [MT-SLVR: Multi-Task Self-Supervised Learning for Transformation In(Variant) Representations.](http://arxiv.org/abs/2305.17191) | 该论文提出了一种名为 MT-SLVR 的框架，用于解决自监督学习中的不变性问题，以改善不同的下游任务的分类性能。 |
| [^45] | [Hardware-Efficient Transformer Training via Piecewise Affine Operations.](http://arxiv.org/abs/2305.17190) | 本文提出一种使用分段仿射操作代替传统乘法的高效Transformer训练方法，不需要更改训练超参数即可在视觉和语言任务中成功实现训练，同时消除了整个训练过程中的所有乘法操作。 |
| [^46] | [Error Bounds for Learning with Vector-Valued Random Features.](http://arxiv.org/abs/2305.17170) | 本文提供了对向量值随机特征学习的完整误差分析，包括在模型错误说明下向量值RF估计器的强一致性和在良好规定的情况下极小化最优收敛速率。 |
| [^47] | [Flow Matching for Scalable Simulation-Based Inference.](http://arxiv.org/abs/2305.17161) | 这篇论文提出了一种基于流匹配技术的后验估计方法，用于模拟推理，通过提供灵活性和可扩展性解决高维问题的挑战，并在引力波推断上取得了可比离散流方法更好的结果。 |
| [^48] | [An Improved Model Ensembled of Different Hyper-parameter Tuned Machine Learning Algorithms for Fetal Health Prediction.](http://arxiv.org/abs/2305.17156) | 本研究提出了一种名为ETSE的机器学习集成模型，用于预测胎儿健康。该模型通过采用多种数据预处理技术和7种不同的机器学习分类器，能够提高预测准确性和性能。 |
| [^49] | [Stability of implicit neural networks for long-term forecasting in dynamical systems.](http://arxiv.org/abs/2305.17155) | 本文提出了一种基于隐式数值方案稳定性特性的神经网络，加入了硬性约束来保证其权重稳定性，取得了较好的长期预测结果。 |
| [^50] | [On convex conceptual regions in deep network representations.](http://arxiv.org/abs/2305.17154) | 本文研究了深度网络表示中概念空间的凸性对泛化能力、小样本学习和主观一致性的影响，发现近似凸性在多个应用领域中广泛存在。 |
| [^51] | [mldr.resampling: Efficient Reference Implementations of Multilabel Resampling Algorithms.](http://arxiv.org/abs/2305.17152) | mldr.resampling是一个软件包，提供11种多标签重采样方法的参考实现，旨在应对多标签学习中的不平衡情况，并具有高效性。 |
| [^52] | [Diagnostic Spatio-temporal Transformer with Faithful Encoding.](http://arxiv.org/abs/2305.17149) | 本文提出了一种诊断时空变换器（DFStrans），其利用新的位置编码和时空依赖性发现框架，能够在具有复杂时空依赖性的多元时间序列分类任务中提取可操作见解。 |
| [^53] | [Differentially private low-dimensional representation of high-dimensional data.](http://arxiv.org/abs/2305.17148) | 本文提出了一种在保护个人敏感信息的情况下，生成高效低维合成数据的算法，并在Wasserstein距离方面具有效用保证；与标准扰动分析不同，使用私有主成分分析过程避免了维度诅咒的影响。 |
| [^54] | [Heterogeneous Value Evaluation for Large Language Models.](http://arxiv.org/abs/2305.17147) | 本文提出了一种自动对齐评估方法A2EHV，采用异质价值系统，并基于价值合理性和社会价值定向框架评估代理人行为的社会偏好，结果表明比传统对齐方法更合理。 |
| [^55] | [Type Prediction With Program Decomposition and Fill-in-the-Type Training.](http://arxiv.org/abs/2305.17145) | 该论文提出了一种基于大型语言模型的搜索方法OpenTau，采用树形程序分解技术和类型填充微调方法解决类型预测中的一些挑战。在TypeScript类型预测方面，该方法在类型检查方面表现良好，平均精度为0.707，优于现有最先进方法。 |
| [^56] | [Ghost in the Minecraft: Generally Capable Agents for Open-World Enviroments via Large Language Models with Text-based Knowledge and Memory.](http://arxiv.org/abs/2305.17144) | 本文提出了Ghost in the Minecraft (GITM)框架，利用大型语言模型与基于文本的知识和记忆，创造了一种在Minecraft中具备通用能力的智能体，可在以文本为基础的复杂编程环境中熟练导航。 |
| [^57] | [Research on Multi-Agent Communication and Collaborative Decision-Making Based on Deep Reinforcement Learning.](http://arxiv.org/abs/2305.17141) | 本研究基于CTDE框架，提出了基于MAPPO算法的多智能体合作决策模型，并引入了基于权重调度和注意力模块的多智能体通信机制，能够有效缓解多智能体环境的不稳定性，提高多智能体在复杂环境中的协作决策能力。 |
| [^58] | [Integrating Generative Artificial Intelligence in Intelligent Vehicle Systems.](http://arxiv.org/abs/2305.17137) | 本文提供了关于在智能车辆系统中集成生成人工智能的全面指南，重点强调了其对语音、音频、视觉和多模态交互的应用，并提出了未来研究领域和与伦理道德相关的挑战和风险。 |
| [^59] | [Three Towers: Flexible Contrastive Learning with Pretrained Image Models.](http://arxiv.org/abs/2305.16999) | 本文提出了 3T 方法，即在视觉语言模型中引入预训练的图像分类器，从而提高对比学习的灵活性。3T 同时受益于预训练嵌入和对比训练，并在实验证明对检索任务和分类问题均取得了有竞争力的性能。 |
| [^60] | [Universal approximation with complex-valued deep narrow neural networks.](http://arxiv.org/abs/2305.16910) | 本文研究了具有有界宽度和任意深度的复值神经网络的普适性，发现当且仅当激活函数既不是全纯的，也不是反全纯的，也不是 $\mathbb{R}$-仿射的时，深窄的复值网络具有普适逼近能力。我们还发现足够的宽度依赖于考虑的激活函数，对于一类可允许的激活函数，宽度为 $n+m+4$ 是足够的。 |
| [^61] | [Can You Solve Closest String Faster than Exhaustive Search?.](http://arxiv.org/abs/2305.16878) | 本文研究最近字符串问题是否存在比平凡的穷举搜索算法更快的算法，针对不同版本的问题的自然版本，得出以下结果：对于连续的问题版本，不存在比平凡的穷举搜索算法更快的解法，对于离散版本，如果$d$是常数，则以$2^{O(\sqrt{n})}$次比较可以解决它。 |
| [^62] | [On the Generalization Capacities of Neural Controlled Differential Equations.](http://arxiv.org/abs/2305.16791) | 本文研究了使用神经控制微分方程进行监督学习的泛化能力问题，通过量化离散化偏差和利普希茨函数逼近误差，得到了经验风险最小化器与贝叶斯最优风险的泛化差距上界。 |
| [^63] | [Confidence-Based Feature Imputation for Graphs with Partially Known Features.](http://arxiv.org/abs/2305.16618) | 本论文提出了一种新颖的信道置信度与伪置信度的特征插值方法，解决了高缺失特征率的图像学习任务中的性能下降问题。 |
| [^64] | [Exploring Weight Balancing on Long-Tailed Recognition Problem.](http://arxiv.org/abs/2305.16573) | 研究分析了新提出的权重平衡方法在长尾识别问题中有效的原因，发现其能够缓解神经崩溃和圆锥效应，从而提高识别性能。 |
| [^65] | [Which Features are Learnt by Contrastive Learning? On the Role of Simplicity Bias in Class Collapse and Feature Suppression.](http://arxiv.org/abs/2305.16536) | 对比学习是一种表示学习技术，对于有监督的情况易于产生类坍塌，无监督情况下易于抑制类别相关的复杂特征；随机梯度下降方法偏向于寻找更简单的解决方案是导致这种现象的关键因素。 |
| [^66] | [Fast Online Node Labeling for Very Large Graphs.](http://arxiv.org/abs/2305.16257) | 本文提出了一种适用于超大规模图的在线节点分类算法FastONL，它基于广义局部推送方法，能有效近似逆矩阵列并应用于一系列流行的图核，具有较低的遗憾值和每个预测的较低成本。 |
| [^67] | [Exploiting Noise as a Resource for Computation and Learning in Spiking Neural Networks.](http://arxiv.org/abs/2305.16044) | 本文提出了噪声脉冲神经元网络（NSNN）和噪声驱动学习规则（NDL），展示了噪声可以作为计算和学习的资源，并为一般脉冲神经元网络提供了一个框架。研究还展示了NSNNs在图像分类和语音识别等实际任务中的适用性，表明它们是未来神经形态计算系统的潜在有力工具。 |
| [^68] | [Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers.](http://arxiv.org/abs/2305.15805) | 本研究提出了一种动态上下文剪枝方法，可以在保持模型表现力的同时，动态减少无效信息，提高模型的效率和可解释性。该技术可以应用于现有的预训练模型，并且可以通过简单的微调过程实现。 |
| [^69] | [Characterizing Out-of-Distribution Error via Optimal Transport.](http://arxiv.org/abs/2305.15640) | 本论文提出了一种基于最优输运理论的新方法 - 置信最优输运(COT)，并且引入了基于经验的变体 - 带门限的置信最优输运(COTT)，它们能够更精确地估计模型的性能，特别是在面对伪标签转移误差时。 |
| [^70] | [Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image Super-Resolution.](http://arxiv.org/abs/2305.15357) | 本文提出了一种求解最优边界条件解决扩散ODE问题的有效采样方法，以稳定地从预训练的基于扩散的超分辨率模型中采样高质量的超分辨率图像。 |
| [^71] | [Replicable Reinforcement Learning.](http://arxiv.org/abs/2305.15284) | 本篇论文提供了可复制的强化学习算法，是控制问题的第一个正式的可复制性结果 |
| [^72] | [Multi-modal Machine Learning for Vehicle Rating Predictions Using Image, Text, and Parametric Data.](http://arxiv.org/abs/2305.15218) | 本文提出了一种多模式机器学习模型，它可以同时使用图像、文本和参数数据对车辆进行评分预测，增加了数据的完整性和准确性。 |
| [^73] | [Knowledge-Design: Pushing the Limit of Protein Deign via Knowledge Refinement.](http://arxiv.org/abs/2305.15151) | 本文提出了一种知识感知模块来提炼低质量残基，引入记忆检索机制实现了超过50%的训练时间节省，并取得了较好的性能表现，是蛋白质设计领域的一次创新。 |
| [^74] | [Multi-State RNA Design with Geometric Multi-Graph Neural Networks.](http://arxiv.org/abs/2305.14749) | 本论文提出了一种基于几何多图神经网络的多状态RNA设计方法，可以明确考虑和反映RNA构象多样性在其设计中。其能够显著提高原生序列的恢复，尤其适用于多状态和结构多样化的RNA。 |
| [^75] | [Graphy Analysis Using a GPU-based Parallel Algorithm: Quantum Clustering.](http://arxiv.org/abs/2305.14641) | 本文介绍了一种新方法将量子聚类应用于图结构中，使用基于GPU的并行算法来计算潜在值。实验结果表明该方法具有优越性能。 |
| [^76] | [Patch-Mix Contrastive Learning with Audio Spectrogram Transformer on Respiratory Sound Classification.](http://arxiv.org/abs/2305.14032) | 本研究提出了一种新的通过在音频数据上进行对比学习的方法，在呼吸音分类任务中取得了最先进的性能表现。 |
| [^77] | [Distribution-aware Fairness Test Generation.](http://arxiv.org/abs/2305.13935) | 本文介绍了一种名为DistroFair的分布感知的公平性测试方法，可以从图像分类器中检测到类级别的公平性违规。 |
| [^78] | [Development of Non-Linear Equations for Predicting Electrical Conductivity in Silicates.](http://arxiv.org/abs/2305.13519) | 本文发展了一种通过人工神经网络预测电弧炉熔渣电导率的方法，并获得了最佳的人工神经网络模型，对该模型进行了平均绝对误差和标准偏差计算及敏感性分析。 |
| [^79] | [Text-to-SQL Error Correction with Language Models of Code.](http://arxiv.org/abs/2305.13073) | 本论文提出了一种基于从句编辑模型的文本到SQL的语言模型纠错方法，并通过新的SQL查询表示改进了语言模型的精确匹配准确率，提高了2.4-6.5，最多提高4.3个百分点。 |
| [^80] | [Road Planning for Slums via Deep Reinforcement Learning.](http://arxiv.org/abs/2305.13060) | 本文介绍了一种基于深度强化学习的方法，用于自动布局贫民窟道路。通过掩码策略优化，可使可达性提高14.3％，对现有基线方法具有明显改进。 |
| [^81] | [Adaptive action supervision in reinforcement learning from real-world multi-agent demonstrations.](http://arxiv.org/abs/2305.13030) | 本文提出了一种从多智能体场景真实世界展示中进行强化学习的自适应行动监督方法，实现了在复制和推广之间平衡的 RL 模型。 |
| [^82] | [Understanding Multi-phase Optimization Dynamics and Rich Nonlinear Behaviors of ReLU Networks.](http://arxiv.org/abs/2305.12467) | 本文对于ReLU神经网络通过Gradient Flow训练的两层模型在线性可分数据上进行了完整的理论分析，揭示了整个训练过程中的四个不同阶段，显示出一个从简化到复杂的学习趋势。 |
| [^83] | [Learning Large Graph Property Prediction via Graph Segment Training.](http://arxiv.org/abs/2305.12322) | 本文提出了一种名为Graph Segment Training的新方法，通过分治法允许使用恒定的内存消耗来学习大型图形属性预测。该方法被评估在几项大型图形属性预测任务上，表现出优于几个最先进基准的出色性能。 |
| [^84] | [Make Transformer Great Again for Time Series Forecasting: Channel Aligned Robust Dual Transformer.](http://arxiv.org/abs/2305.12095) | 本文提出了一种通道对齐鲁棒双Transformer模型，通过双Transformer结构和鲁棒损失函数的引入，解决了Transformer在时间序列预测中的关键缺点，显著提高了预测精度和效率。 |
| [^85] | [SneakyPrompt: Evaluating Robustness of Text-to-image Generative Models' Safety Filters.](http://arxiv.org/abs/2305.12082) | 本文提出了第一个自动攻击框架SneakyPrompt，以评估最先进的文本生成图像模型中的安全过滤器的鲁棒性，该框架的关键洞见是搜索备选令牌来绕过安全过滤器。 |
| [^86] | [Bayesian approach to Gaussian process regression with uncertain inputs.](http://arxiv.org/abs/2305.11586) | 本文提出了一种新的高斯过程回归技术，通过贝叶斯方法将输入数据的不确定性纳入回归模型预测中。在数值实验中展示了该方法具有普适性和不错的表现。 |
| [^87] | [Characterizing Long-Tail Categories on Graphs.](http://arxiv.org/abs/2305.09938) | 该研究提出了图上长尾分类的第一个泛化边界，并提出了一种可表征长尾类别的行为并提高机器学习模型在现实世界网络中的泛化性能的新图表示学习框架。 |
| [^88] | [Time delay multi-feature correlation analysis to extract subtle dependencies from EEG signals.](http://arxiv.org/abs/2305.09478) | 本文针对脑电信号的复杂性，提出使用多特征相关分析方法来自动分解和提取多种类型的统计依赖关系。其中，PCA降维技术用于找到占主导的依赖关系方向，从而提取脑电信号中微小依赖性。 |
| [^89] | [Double-Weighting for Covariate Shift Adaptation.](http://arxiv.org/abs/2305.08637) | 本文提出了一种双重加权的最小极大风险分类方法，可以有效避免协变量漂移对监督学习的影响。 |
| [^90] | [Convergence Analysis of Mean Shift.](http://arxiv.org/abs/2305.08463) | 本研究提出了均值漂移算法的模估计序列的收敛保证，并扩展了现有的涵盖解析核和Epanechnikov核的发现，意义在于涵盖了在基于KDE的模估计的渐近统计效率方面最优的非负核——双重核。 |
| [^91] | [Local Convergence of Gradient Descent-Ascent for Training Generative Adversarial Networks.](http://arxiv.org/abs/2305.08277) | 本论文研究了使用基于核函数的鉴别器训练GAN的梯度下降-上升算法的局部收敛性，揭示了学习率、正则化和带宽对其影响，同时展示了收敛、振荡或发散的相变现象。 |
| [^92] | [Provable Multi-instance Deep AUC Maximization with Stochastic Pooling.](http://arxiv.org/abs/2305.08040) | 本文提出了在多实例学习中使用深度AUC最大化（DAM）的方法，并根据包含大量实例的情况下训练的计算挑战，提出了一种基于方差减少的随机池化方法，使得只需对每个包进行少量采样即可计算MIDAM模型，提高了效率和准确性。 |
| [^93] | [A Category-theoretical Meta-analysis of Definitions of Disentanglement.](http://arxiv.org/abs/2305.06886) | 本文提出了一个存在的去卷积定义的范畴论元分析，将笛卡儿积和幺模积的概念应该构成去卷积的核心，并展现了处理函数、等变映射、关系和随机映射的相似性和关键区别。 |
| [^94] | [Robust Detection of Lead-Lag Relationships in Lagged Multi-Factor Models.](http://arxiv.org/abs/2305.06704) | 该论文提出了一种基于聚类的鲁棒检测滞后多因子模型中的领先滞后关系方法，并使用各种聚类技术和相似度度量方法实现了对领先滞后估计的聚合，从而强化了对原始宇宙中的一致关系的识别。 |
| [^95] | [Supervised learning with probabilistic morphisms and kernel mean embeddings.](http://arxiv.org/abs/2305.06348) | 本文提出了监督学习中正确损失函数的概念，其通过概率测度的条件正则概率测度解决线性算子方程的问题得到定义，适用于可测空间的输入空间和标签空间。 |
| [^96] | [Feature Expansion for Graph Neural Networks.](http://arxiv.org/abs/2305.06142) | 本文通过分析图神经网络中的特征空间，提出了特征子空间展开和结构主成分两种方法来扩展特征空间，从而获得更好的结果。 |
| [^97] | [Vcc: Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens.](http://arxiv.org/abs/2305.04241) | 这篇论文提出了一种新的方法 VCC，通过优先处理最重要的VIP标记，一定程度上压缩序列，从而使Transformer模型可处理长度更长的序列。 |
| [^98] | [A Comprehensive Study on Dataset Distillation: Performance, Privacy, Robustness and Fairness.](http://arxiv.org/abs/2305.03355) | 本研究对当前最先进的数据集压缩方法进行了全面评估，发现其存在隐私风险并可能放大模型的不公平性，提供了大规模的基准测试框架。 |
| [^99] | [Object-Centric Voxelization of Dynamic Scenes via Inverse Neural Rendering.](http://arxiv.org/abs/2305.00393) | 本文提出了一个逆向神经渲染框架DynaVol，可以在多实体动态场景中学习时间变化的体积表示，通过维护一个时间依赖的3D格点和联合学习格点级局部动态、物体级全局动态和组合神经辐射场来增强物体中心场景体素化的时空一致性。 |
| [^100] | [On the existence of solutions to adversarial training in multiclass classification.](http://arxiv.org/abs/2305.00075) | 本文研究了多类分类中敌对训练的鲁棒解存在性问题，证明了每个模型中存在 Borel 可测的鲁棒分类器，并与最优传输和总变差正则化建立了联系。在二元分类问题中，对不可知分类器的敌对训练问题存在 Borel 可测的解。 |
| [^101] | [When Do Graph Neural Networks Help with Node Classification: Investigating the Homophily Principle on Node Distinguishability.](http://arxiv.org/abs/2304.14274) | 同源性原则不一定是影响图神经网络优越性的唯一原因；本文提出Contextual Stochastic Block Model for Homophily (CSBM-H)以深入研究同源性对节点可区分性的影响。 |
| [^102] | [FedVS: Straggler-Resilient and Privacy-Preserving Vertical Federated Learning for Split Models.](http://arxiv.org/abs/2304.13407) | 该论文提出FedVS，一种同时解决垂直联邦学习中滞后客户端和数据泄露问题的方法，通过设计本地数据和模型的秘密共享方案，以保证信息理论隐私，并通过解密计算股份，无损重构所有客户端的嵌入的汇总。 |
| [^103] | [Alternating Local Enumeration (TnALE): Solving Tensor Network Structure Search with Fewer Evaluations.](http://arxiv.org/abs/2304.12875) | 提出了TnALE算法，通过交替局部枚举更新每个与结构相关的变量，大大减少了评估次数，用于解决张量网络结构搜索问题。在理论上证明，如果在每个邻域中达到了足够的目标函数降低，TnALE和TNLS都可以实现线性收敛度，直到一个常数。同时，与TNLS相比， TnALE需要更少的评估次数。 |
| [^104] | [PowerGAN: A Machine Learning Approach for Power Side-Channel Attack on Compute-in-Memory Accelerators.](http://arxiv.org/abs/2304.11056) | 本文提出了一种基于机器学习的计算内存加速器功耗副信道攻击技术，可以在大噪声和对抗措施存在的情况下重构用户的私有输入数据。 |
| [^105] | [Matching-based Data Valuation for Generative Model.](http://arxiv.org/abs/2304.10701) | 本论文提出了基于匹配的生成模型数据估值方法，这是一个针对任何生成模型的模型无关方法，可以对数据实例进行估值，而无需重新训练模型，并在估值效果上表现出色。 |
| [^106] | [Attributing Image Generative Models using Latent Fingerprints.](http://arxiv.org/abs/2304.09752) | 本文研究了一种使用潜在语义维度作为指纹的追溯方法，可以分析设计变量对于准确性-质量权衡的影响，在保证准确性的同时最小化计算量，更适用于大规模模型。 |
| [^107] | [Benchmarking Actor-Critic Deep Reinforcement Learning Algorithms for Robotics Control with Action Constraints.](http://arxiv.org/abs/2304.08743) | 本研究提出基准测试评估带操作约束的强化学习算法在多种机器人控制环境中的表现，并公开GitHub代码，为未来的研究和开发提供参考。 |
| [^108] | [In-Context Operator Learning for Differential Equation Problems.](http://arxiv.org/abs/2304.07993) | 本文提出了一种新的神经网络方法INDEED，它可以同时学习不同微分方程问题的操作符，而无需重新训练，且只需要极少的演示。 |
| [^109] | [Neural Operator Learning for Ultrasound Tomography Inversion.](http://arxiv.org/abs/2304.03297) | 本文首次将神经操作符学习应用于超声断层成像反演，通过学习时间飞行数据和异质声速场之间的映射，实现了可避免计算密集型反演过程的预测异质声场模型。该模型有潜在的在乳腺成像中进行软组织分布预测和肿瘤识别的实时应用。 |
| [^110] | [Re-IQA: Unsupervised Learning for Image Quality Assessment in the Wild.](http://arxiv.org/abs/2304.00451) | 该研究提出了一种名为 Re-IQA 的无监督学习方法，利用混合专家方法训练两个编码器，学习图像的高级内容和低级质量特征，以生成互补的低和高级图像表示，从而实现在野外自动化的感知图像质量评估，且在多个大型图像质量评估数据库上实现了最先进的性能。 |
| [^111] | [Predictive Context-Awareness for Full-Immersive Multiuser Virtual Reality with Redirected Walking.](http://arxiv.org/abs/2303.17907) | 本文提出了利用预测性上下文感知来优化发射端和接收端的波束成形和波束导向，实现面向全沉浸多用户虚拟现实技术的高效通信。 |
| [^112] | [HARFLOW3D: A Latency-Oriented 3D-CNN Accelerator Toolflow for HAR on FPGA Devices.](http://arxiv.org/abs/2303.17218) | 本研究提出了一种面向FPGA设备的基于延迟的3D-CNN加速器工具链HARFLOW3D，它以机器学习模型和FPGA的特性描述为输入，生成最小化计算延迟的设计。实验证明HARFLOW3D相比其他方案能够实现更低的延迟。 |
| [^113] | [Diffusion Schr\"odinger Bridge Matching.](http://arxiv.org/abs/2303.16852) | 本文介绍了一种新的方法 Iterative Markovian Fitting，用于解决高维度 Schr\"odinger桥（SBs）问题，该方法的数值实验表现出在准确性和性能方面的显著优势。 |
| [^114] | [A Closer Look at Scoring Functions and Generalization Prediction.](http://arxiv.org/abs/2303.13589) | 本文研究了广义误差预测器的有效性，探讨了置信度、局部流形平滑度和模型一致性评分函数的优缺点，发现在复杂机制缺失的情况下，最先进的评分无法在分布转移和损坏下超越简单的模型一致性。同时，在受损训练数据的情况下，模型一致性打分仍然表现良好，并且集成多样性有助于提高泛化性能。 |
| [^115] | [Posthoc Interpretation via Quantization.](http://arxiv.org/abs/2303.12659) | 本文提出了一种新的方法 PIQ，通过对分类器进行向量量化，将其表示转换为离散类特定的潜空间，从而解释分类器所做出的决策，并且通过研究发现该方法相比其他方法更容易让人理解。 |
| [^116] | [CB2: Collaborative Natural Language Interaction Research Platform.](http://arxiv.org/abs/2303.08127) | CB2是一个用于研究基于任务的合作自然语言交互的平台，在3D游戏环境中提供了后端服务器和各种工具和流程。它在可扩展的研究中展示了学习的指令跟随模型。 |
| [^117] | [An Improved Data Augmentation Scheme for Model Predictive Control Policy Approximation.](http://arxiv.org/abs/2303.05607) | 基于敏感度有效生成MPC策略逼近的数据增强框架被提出，本文在此基础上引入了基于预测校正步骤的改进方案，以提高数据准确性。 |
| [^118] | [Mastering Strategy Card Game (Hearthstone) with Improved Techniques.](http://arxiv.org/abs/2303.05197) | 本文将端到端策略函数和乐观平滑虚拟博弈算法应用于更加复杂的商业游戏爐石戰記，提出改进技术并在人机对战中表现出较强决策能力。 |
| [^119] | [Learning to Backdoor Federated Learning.](http://arxiv.org/abs/2303.03320) | 本文提出了一种基于强化学习的后门攻击框架，能够在联邦学习中成功设置后门，在现有防御措施下也具有强大的攻击性能和耐用性。 |
| [^120] | [Metaheuristic conditional neural network for harvesting skyrmionic metastable states.](http://arxiv.org/abs/2303.02876) | 提出了一种基于元启发式条件神经网络的方法，收集高不规则潜能能量面上的物理有趣的稳态， 并在Pd / Fe / Ir（111）系统中应用于识别自旋结构，观察了其中一些结构的有限温度自旋动力学特性和拓扑电荷与结构之间的关系。 |
| [^121] | [The Point to Which Soft Actor-Critic Converges.](http://arxiv.org/abs/2303.01240) | 本文证明了在极限情况下，Soft Actor-Critic算法和Soft Q-learning算法在最大熵框架下收敛于同一解，这一结论对优化算法具有较大的意义。 |
| [^122] | [Inseq: An Interpretability Toolkit for Sequence Generation Models.](http://arxiv.org/abs/2302.13942) | 本文介绍了Inseq，这是一个Python工具包，旨在推广可解释性序列生成模型的分析。它为常见的解码器和编码器-解码器Transformers架构提供了提取模型内部信息和特征重要性得分的直观优化方法。作者还在机器翻译模型和GPT-2中展示了Inseq的潜力，证明其有助于推动可解释性自然语言生成的未来发展。 |
| [^123] | [A Self-Supervised Learning-based Approach to Clustering Multivariate Time-Series Data with Missing Values (SLAC-Time): An Application to TBI Phenotyping.](http://arxiv.org/abs/2302.13457) | 本文提出了一种基于自监督学习的多元时间序列数据聚类方法(SLAC-Time)，采用时间序列预测作为代理任务，不需要填补缺失值，具有更健壮的时间序列表示。 |
| [^124] | [MDF-Net for Abnormality Detection by Fusing X-Rays with Clinical Data.](http://arxiv.org/abs/2302.13390) | 本研究提出了一种多模态融合方法MDF-Net，将临床数据与胸部X射线图像相结合，成功地提高了疾病定位的性能表现。 |
| [^125] | [Learning Physical Models that Can Respect Conservation Laws.](http://arxiv.org/abs/2302.11002) | 这项工作提出了ProbConserv框架，通过将守恒约束与贝叶斯更新相结合，将守恒约束纳入通用科学机器学习体系结构中，以便在学习高难度的PDE运算中应用。 |
| [^126] | [A Novel Noise Injection-based Training Scheme for Better Model Robustness.](http://arxiv.org/abs/2302.10802) | 本研究提出了一种基于噪声注入训练方案的模型鲁棒性改进方法，通过设计特殊的梯度估计方法和简化传统噪声注入方法，提高了对抗鲁棒性，并在MNIST和Fashion-MNIST数据集上得到了验证。 |
| [^127] | [NeuralStagger: Accelerating Physics-constrained Neural PDE Solver with Spatial-temporal Decomposition.](http://arxiv.org/abs/2302.10255) | 本文提出了一种新的名为NeuralStagger的通用加速方法，通过将物理约束下的神经偏微分方程解法器进行时空分解得到多个粗分辨率的子任务，并使用物理约束的标准损失联合训练，从而极大地减少了计算资源使用。 |
| [^128] | [HyFL: A Hybrid Framework For Private Federated Learning.](http://arxiv.org/abs/2302.09904) | HyFL是一种混合框架，它结合了安全多方计算技术和分层联合学习，并能够在分布式环境中保证数据和全局模型的隐私安全，有助于大规模部署。 |
| [^129] | [Depth Degeneracy in Neural Networks: Vanishing Angles in Fully Connected ReLU Networks on Initialization.](http://arxiv.org/abs/2302.09712) | 本文研究了深度神经网络中的深度退化现象，在全连接ReLU网络初始化时，两个输入之间的角度会趋近于0。通过使用组合展开，得到了其趋向于0的速度的精确公式，并验证了这些结果。 |
| [^130] | [Understanding Expertise through Demonstrations: A Maximum Likelihood Framework for Offline Inverse Reinforcement Learning.](http://arxiv.org/abs/2302.07457) | 通过提出的双层优化公式，我们提供了一个离线逆向强化学习的最大似然框架，该框架通过最大化奖励来估计专家的保守模型以及专家的环境动态，能够更准确地推断专业技能。 |
| [^131] | [Geometric Clifford Algebra Networks.](http://arxiv.org/abs/2302.06594) | 本文提出了基于几何代数的几何 Clifford 代数网络（GCANs），采用对称群变换建模动态系统，通过群作用层、激活和归一化方案，可以优化几何模板，提高三维刚体变换和流体动力学模拟的性能。 |
| [^132] | [On the Privacy-Robustness-Utility Trilemma in Distributed Learning.](http://arxiv.org/abs/2302.04787) | 本文研究了分布式学习中隐私、鲁棒性和实用性之间的权衡关系，并证明了它们之间存在一种根本的平衡，同时提出了一种新的高维度鲁棒聚合规则的分布式机器学习算法，可以优化效用-隐私权衡。 |
| [^133] | [Black Box Adversarial Prompting for Foundation Models.](http://arxiv.org/abs/2302.04237) | 本文提出了一个黑盒框架，用于生成对抗性提示，以在基础图像和文本生成模型中引入特定的生成行为。 |
| [^134] | [Rover: An online Spark SQL tuning service via generalized transfer learning.](http://arxiv.org/abs/2302.04046) | Rover 是一种在线 Spark SQL 调优服务，通过应用广义转移学习，结合专家知识和历史任务，来加速调优过程，提高性能并避免不良影响。 |
| [^135] | [Network-Aided Intelligent Traffic Steering in 6G O-RAN: A Multi-Layer Optimization Framework.](http://arxiv.org/abs/2302.02711) | 本论文提出了一个多层优化框架，利用流量分割分布，拥塞控制和调度优化O-RAN应用中的智能流量转发，能够显著降低延迟，取得了良好的实验结果和全面的性能分析。 |
| [^136] | [Revisiting Discriminative vs. Generative Classifiers: Theory and Implications.](http://arxiv.org/abs/2302.02334) | 本文重新审视关于判别式与生成式分类器的经典主题，利用多类$\mathcal{H}$-一致性下界，证明了在温和的假设下，多类朴素贝叶斯分类器的样本要求比逻辑回归分类器多了$O(\log n)$。 |
| [^137] | [IoT Botnet Detection Using an Economic Deep Learning Model.](http://arxiv.org/abs/2302.02013) | 本文提出了一种经济深度学习模型来检测物联网僵尸网络攻击以及不同类型的攻击。该模型能够在较小的预算下加速训练和检测过程，并且具有比最先进的检测模型更高的准确性。 |
| [^138] | [Beyond the Universal Law of Robustness: Sharper Laws for Random Features and Neural Tangent Kernels.](http://arxiv.org/abs/2302.01629) | 本文通过研究随机特征和神经切向核（NTK）的经验风险最小化，证明了在随机特征中，即使满足稳健性的通用定律所需的必要条件，模型也不具有任何过度参数化程度的稳健性。相对地，对于偶激活情况，NTK模型满足普遍下限，只要满足过参数条件就能稳健。这为机器学习中的稳健性提供了更尖锐的法则，超越了先前建立的普适定律。 |
| [^139] | [On the Robustness of Randomized Ensembles to Adversarial Perturbations.](http://arxiv.org/abs/2302.01375) | 本文对随机集成算法在对抗攻击环境中的鲁棒性进行了研究，提出了新的训练算法 BARRE，能够有效地防御强大的 $\ell_\infty$ 范围内的攻击。 |
| [^140] | [Double Permutation Equivariance for Knowledge Graph Completion.](http://arxiv.org/abs/2302.01313) | 本研究提出了双排列等变性的KG表示方法，可以使神经网络在KG中执行复杂的逻辑推理任务，并在多个归纳KG完成任务中实现了最先进的Hits@10测试准确率。双排列等变性在KG中开辟了新的研究方向。 |
| [^141] | [The contextual lasso: Sparse linear models via deep neural networks.](http://arxiv.org/abs/2302.00878) | 本论文提出了一种新的统计估计器——上下文套索，可以通过深度神经网络的方法解决解释性和拟合能力的矛盾问题，实现对可解释特征的稀疏拟合，并且稀疏模式和系数会随着上下文特征的变化而发生变化。 |
| [^142] | [Multi-Fidelity Covariance Estimation in the Log-Euclidean Geometry.](http://arxiv.org/abs/2301.13749) | 该论文提出了一种新颖的协方差矩阵估计方法，采用对数欧氏几何，并融合了具有不同保真度和成本的数据源层次结构的样本，保证了方差的降低和确定性的保持，使得在仿真或数据收集昂贵的应用中，协方差估计成为可行的。该方法在度量学习、数据同化和其他下游任务中具有重要的应用价值。 |
| [^143] | [Unconstrained Dynamic Regret via Sparse Coding.](http://arxiv.org/abs/2301.13349) | 本文探讨了在线线性优化（OLO）涉及无约束问题和动态遗憾问题的复杂性，提出了一种通过重新构造问题为稀疏编码的复杂度度量方式，在适应性和应用上有较好的应用价值。 |
| [^144] | [Variational sparse inverse Cholesky approximation for latent Gaussian processes via double Kullback-Leibler minimization.](http://arxiv.org/abs/2301.13303) | 本文提出了一种基于稀疏逆Cholesky因子的高斯分布的变分逼近方法，结合同样高效的SIC约束的Kullback-Leibler最优先验逼近，并在特定SIC排序和稀疏模式下，实现对潜在高斯过程的高度准确先验和后验逼近。与其他方法相比，该方法可以在类似计算复杂度下更准确地预测平稳核函数。 |
| [^145] | [Hierarchical Imitation Learning with Vector Quantized Models.](http://arxiv.org/abs/2301.12962) | 本文提出带有矢量量化模型的分层模仿学习方法，通过强化学习识别专家轨迹的子目标并建立矢量量化生成模型实现子目标级别的规划，该算法在解决复杂、长远的决策问题方面优于现有最先进方法。 |
| [^146] | [Extremal Domain Translation with Neural Optimal Transport.](http://arxiv.org/abs/2301.12874) | 该论文提出了一种称为“极值传输(ET)”的方法，可用于进行给定相似性函数下的一对域之间的最佳可能的非配对翻译，并且提出了一种可扩展的基于神经最优输运(OT)的算法来逼近ET映射。 |
| [^147] | [Are Random Decompositions all we need in High Dimensional Bayesian Optimisation?.](http://arxiv.org/abs/2301.12844) | 本文研究了数据独立分解采样规则，证明了随机树分解采样器有利的理论保证，促进了随机分解上置信度算法（RDUCB）的发展。 |
| [^148] | [Solving High-Dimensional PDEs with Latent Spectral Models.](http://arxiv.org/abs/2301.12664) | 本文提出一种基于隐变量谱模型的高维PDEs高效精确求解器，它使用基于注意力的分层投影网络将高维数据在线性时间内缩小到一个紧凑的潜空间，并利用谱方法在潜空间中学习算子解决维度诅咒。效果和可扩展性在Navier-Stokes和Schrödinger方程中得到验证。 |
| [^149] | [A Closer Look at Few-shot Classification Again.](http://arxiv.org/abs/2301.12246) | 本文研究了Few-shot分类问题中的训练算法和适应算法，并实证证明这两个算法是可以完全分离的。此外，本文的元分析揭示出了关于Few-shot分类的关键方面和与其他领域（如视觉表示学习和迁移学习）的联系的有趣见解。 |
| [^150] | [Certifiably Robust Reinforcement Learning through Model-Based Abstract Interpretation.](http://arxiv.org/abs/2301.11374) | CAROL是一个强化学习框架，它基于模型和抽象解释方法，学习出的策略具有机器可证明的对抗鲁棒性证书，在实验上表现出更好的认证性能和可比较的对抗性能。 |
| [^151] | [Estimating Causal Effects using a Multi-task Deep Ensemble.](http://arxiv.org/abs/2301.11351) | 通过学习研究群体中的共享和特定于组的信息，使用Causal Multi-task Deep Ensemble（CMDE）的方法可以有效地处理高维和多模态协变量，并提供因果效应的点估计不确定性。 |
| [^152] | [Maximum Optimality Margin: A Unified Approach for Contextual Linear Programming and Inverse Linear Programming.](http://arxiv.org/abs/2301.11260) | 本论文提出了一种名为“最大最优性边际”的新方法来解决预测-优化问题，通过下游优化的最优性条件设计机器学习损失函数，兼具计算效率和较好的理论性质，而且只需要训练数据中最优解的观测值。 |
| [^153] | [Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning.](http://arxiv.org/abs/2301.10886) | 本文提出了一种名为AIRS的自动内在奖励塑造探索方法，可以提供高质量的内在激励以增强强化学习中的探索性能；并开发了高效可靠的内在奖励工具包。实验表明，AIRS性能卓越，能够胜过基准方案。 |
| [^154] | [Banker Online Mirror Descent: A Universal Approach for Delayed Online Bandit Learning.](http://arxiv.org/abs/2301.10500) | 银行家在线镜像下降为在线劫匪学习任务提供了一种可靠的处理反馈延迟的通用方法，具有高效的遗憾界限。 |
| [^155] | [DIFFormer: Scalable (Graph) Transformers Induced by Energy Constrained Diffusion.](http://arxiv.org/abs/2301.09474) | DIFFormer是一种能量受限扩散模型，通过逐渐融合其他实例信息的演化状态，导出了一类新的神经编码器，称为DIFFormer（基于扩散的Transformer），能够揭示真实世界中复杂的数据生成过程。 |
| [^156] | [Prompt Federated Learning for Weather Forecasting: Toward Foundation Models on Meteorological Data.](http://arxiv.org/abs/2301.09152) | 本研究提出了一种跨地区的基础模型，能够理解复杂的气象数据并提供天气预报。采用联邦学习方法解决了地区间数据曝光问题，同时采用新颖的提示学习机制满足低资源传感器的通信和计算约束。 |
| [^157] | [DiME: Maximizing Mutual Information by a Difference of Matrix-Based Entropies.](http://arxiv.org/abs/2301.08164) | 本文提出了一种称为DiME的信息理论量，可以估计随机变量之间的互信息最大化，避免了平凡解，适用于多个实际应用。 |
| [^158] | [Curvilinear object segmentation in medical images based on ODoS filter and deep learning network.](http://arxiv.org/abs/2301.07475) | 该论文提出了一种基于 ODoS 滤波器和深度学习网络的曲线结构分割框架，用于解决医学图像中曲线对象的自动分割问题。 |
| [^159] | [Learning Deformation Trajectories of Boltzmann Densities.](http://arxiv.org/abs/2301.07388) | 本文介绍了一种学习Boltzmann密度变形轨迹的方法，其中通过插值能量函数等实现Boltzmann密度的变形，然后找到一个时间依赖向量场，将样本从一个分布转移到另一个分布，其表现在高斯混合和量子力学粒子的Boltzmann密度上比KL-反散度更具优势。 |
| [^160] | [Mean-Field Control based Approximation of Multi-Agent Reinforcement Learning in Presence of a Non-decomposable Shared Global State.](http://arxiv.org/abs/2301.06889) | 本文提出了一种基于均场控制的多智能体强化学习的近似方法，即使智能体共享一个非可分全局状态，也能具有较好的适用性和近似效果。 |
| [^161] | [Define, Evaluate, and Improve Task-Oriented Cognitive Capabilities for Instruction Generation Models.](http://arxiv.org/abs/2301.05149) | 本文提出了基于任务的认知能力，设计了评估方案来比较语言模型和人类的这些能力，通过在导航指令生成问题中的应用，发现模型的语用能力仍需改进。 |
| [^162] | [Black-box language model explanation by context length probing.](http://arxiv.org/abs/2212.14815) | 该论文提出了一个模型不可知的新颖解释技术：上下文长度探测，通过跟踪模型预测与可用上下文长度的关系来对不同上下文分配不同的重要性得分。该方法适用于大型预训练语言模型，并有利于研究远距离依赖性。 |
| [^163] | [Relative Probability on Finite Outcome Spaces: A Systematic Examination of its Axiomatization, Properties, and Applications.](http://arxiv.org/abs/2212.14555) | 本文提出了将概率看作相对度量的观点，建立了有限结果空间上相对概率函数的公理化，提供了其实例和组合系统，并讨论了相对贝叶斯推断及其数字实现，证明了相对概率空间的拓扑闭包，突显了其在极限下保留信息的能力。 |
| [^164] | [Doubly Smoothed GDA: Global Convergent Algorithm for Constrained Nonconvex-Nonconcave Minimax Optimization.](http://arxiv.org/abs/2212.12978) | 本文提出了一种双重平滑梯度下降上升法 (DSGDA)，该算法可以应用于非凸-非凹极小极大优化，并且能够全局收敛并消除极限环。在一定条件下，DSGDA 的迭代复杂度达到了文献中单循环算法的最佳结果。 |
| [^165] | [When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories.](http://arxiv.org/abs/2212.10511) | 本文通过对10个模型和4种增强方法的实验，发现语言模型在记忆不太流行的实际知识方面存在困难，而检索增强的语言模型表现较好，提出了一种检索增强语言模型的简单有效方法。 |
| [^166] | [Reasoning with Language Model Prompting: A Survey.](http://arxiv.org/abs/2212.09597) | 本文提供了使用语言模型提示进行推理的前沿研究综合调查。讨论了新兴推理能力出现的潜在原因，并提供系统资源帮助初学者。 |
| [^167] | [BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting.](http://arxiv.org/abs/2212.09535) | 本文在BLOOM模型中应用语言适应策略，将其适应到新语言上，并在八种新语言的零样本提示表现中提升了性能。适配器微调比大模型的持续预训练更有效，提示性能主要由语言适应数据的大小确定。 |
| [^168] | [Efficient Relation-aware Neighborhood Aggregation in Graph Neural Networks via Tensor Decomposition.](http://arxiv.org/abs/2212.05581) | 本文提出了一个张量分解的知识图编码器，将邻居实体使用由关系类型定义的低秩张量的投影矩阵进行转换，从而产生具有表达能力和关系感知性的表示，并使用对比学习的方法进行训练，从而提高了基于图的神经网络模型的效率和表现。 |
| [^169] | [ISAACS: Iterative Soft Adversarial Actor-Critic for Safety.](http://arxiv.org/abs/2212.03228) | 本文提出了一种新的算法，ISAACS，通过将博弈论安全分析与对抗强化学习相结合，使机器人系统能够进行可扩展的鲁棒安全控制。实验结果表明，该算法可以有效地学习并避免碰撞，并在安全问题上超越标准的深度强化学习算法。 |
| [^170] | [Understanding the Impact of Adversarial Robustness on Accuracy Disparity.](http://arxiv.org/abs/2211.15762) | 本文通过研究高斯混合模型下的线性分类器，分析了对抗鲁棒性对准确性不平衡的影响，并证明了在稳定分布的一般家族中也存在类似影响。 |
| [^171] | [Ensemble Multi-Quantiles: Adaptively Flexible Distribution Prediction for Uncertainty Quantification.](http://arxiv.org/abs/2211.14545) | 本文提出了一种自适应灵活的分布预测方法EMQ，用于量化机器学习中的不确定性。该方法逐步偏离高斯分布并在提升中发现最优条件分布，因此具有较好的实用性。 |
| [^172] | [Strategyproof Decision-Making in Panel Data Settings and Beyond.](http://arxiv.org/abs/2211.14236) | 本文研究了使用面板数据做决策制定时，如何应对生成数据的单位采取策略的情况，提出了一个可以对单位进行正确干预的无欺诈干预策略。 |
| [^173] | [Learning Large Causal Structures from Inverse Covariance Matrix via Matrix Decomposition.](http://arxiv.org/abs/2211.14221) | 本文提出了一种基于逆协方差矩阵的$\mathcal{O}$-ICID方法，该方法是通过连续优化一种矩阵分解来学习因果结构的，适用于变量数量庞大的情况。该方法可以在噪声方差已知时识别真实DAG, 也可以在较弱的先验信息下给出有用的定向图解 |
| [^174] | [Zeroth-Order Alternating Gradient Descent Ascent Algorithms for a Class of Nonconvex-Nonconcave Minimax Problems.](http://arxiv.org/abs/2211.13668) | 本文提出了零阶交替梯度下降算法和零阶方差减少交替梯度下降算法，用于解决一类非凸非凹的极小极大问题，分别在确定性和随机环境下。它们是解决这类问题的第一和第二个迭代复杂度保证的零阶算法。 |
| [^175] | [Masked Autoencoding for Scalable and Generalizable Decision Making.](http://arxiv.org/abs/2211.12740) | 本文提出了一种遮盖式决策预测 (MaskDP) 的简单可扩展的自监督预训练方法，在可扩展的增强学习和行为克隆中能够有效地从大规模多样的序列数据中学习，并且零样本转移至新任务。 |
| [^176] | [Learning Heterogeneous Agent Cooperation via Multiagent League Training.](http://arxiv.org/abs/2211.11616) | 本文提出了一种名为异质联赛训练（HLT）的通用强化学习算法，为解决异质多智能体问题，使用策略池和超网络，提高了异质智能体的合作效率。 |
| [^177] | [Exhaustive Symbolic Regression.](http://arxiv.org/abs/2211.11461) | 本文介绍了一种全面的符号回归方法，可以有效地解决传统方法中随机搜索和标准主观性等问题，保证找到最佳函数。 |
| [^178] | [Learning from Long-Tailed Noisy Data with Sample Selection and Balanced Loss.](http://arxiv.org/abs/2211.10906) | 本文提出了一种用样本选择和平衡损失的鲁棒方法，能够更好的处理长尾嘈杂数据的学习。 |
| [^179] | [Introduction to Online Nonstochastic Control.](http://arxiv.org/abs/2211.09619) | 介绍了一种新兴的在线非随机控制方法，通过在一组策略中寻找低后悔，获得对最优策略的近似。 |
| [^180] | [Scalar Invariant Networks with Zero Bias.](http://arxiv.org/abs/2211.08486) | 本文证明了在解决许多图像任务(例如图像分类)时可以忽略偏置，并且零偏置神经网络在实际图像分类任务中表现良好，同时具有标量 (乘法) 不变性，从而在改变对比度时仍能保持预测不变。 |
| [^181] | [OverFlow: Putting flows on top of neural transducers for better TTS.](http://arxiv.org/abs/2211.06892) | 本论文将神经HMM TTS与归一化流相结合，提出了一种强大而全面的概率模型，需要的数据更少，训练更新更少，发音准确性高，主观语音质量接近自然语音。 |
| [^182] | [GCondNet: A Novel Method for Improving Neural Networks on Small High-Dimensional Tabular Data.](http://arxiv.org/abs/2211.06302) | GCondNet利用高维表格数据的隐含结构，通过创建图形并利用图神经网络以及条件训练，提高了潜在预测网络的性能。 |
| [^183] | [Semi-supervised learning for continuous emotional intensity controllable speech synthesis with disentangled representations.](http://arxiv.org/abs/2211.06160) | 本文提出了一种基于半监督学习的方法来控制连续情感强度的语音合成，使用伪标签和基于情感的统一网格几何嵌入空间，其可控性和自然度都较好。 |
| [^184] | [A noise based novel strategy for faster SNN training.](http://arxiv.org/abs/2211.05453) | 本篇论文提出了一种引入噪声的新的SNN训练方法，它结合了ANN到SNN转换和基于脉冲的反向传播，通过训练单步SNN并将其转换为多步SNN来显著提高精度。 |
| [^185] | [lilGym: Natural Language Visual Reasoning with Reinforcement Learning.](http://arxiv.org/abs/2211.01994) | 本文提出了一个基于自然语言视觉推理的强化学习基准测试——lilGym，它由2661个高度组合的人类编写自然语言语句和交互式视觉环境组成，并通过注释可执行Python程序来实现精确的奖励计算。本文的实验结果和分析表明，lilGym是一个具有挑战性的开放性问题。 |
| [^186] | [Crosslingual Generalization through Multitask Finetuning.](http://arxiv.org/abs/2211.01786) | 该论文通过多任务微调实现跨语言泛化。研究表明，在英语提示下，对大型多语言模型进行英语任务的微调，可以实现对仅出现在预训练语料库中的非英语语言的任务泛化，并且使用英语提示进行多语言任务的微调进一步提高了在英语和非英语任务上的表现，从而实现了各种零-shot结果的最新水平。 |
| [^187] | [Perturbation Analysis of Neural Collapse.](http://arxiv.org/abs/2210.16658) | 本文提出了一种能够捕捉神经崩溃现象的模型，通过强制特征留在预定义的特征矩阵的附近来实现。通过扰动分析，在小邻域情况下建立了系数范数不变性和特征抖动的联系。 |
| [^188] | [Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming.](http://arxiv.org/abs/2210.14306) | 该论文研究了代码推荐系统GitHub Copilot，发现了程序员与该系统交互的一些常见活动，揭示了其低效性和时间成本，从而为改进界面设计和度量方法提供了动力。 |
| [^189] | [WaveBound: Dynamic Error Bounds for Stable Time Series Forecasting.](http://arxiv.org/abs/2210.14303) | 本文提出了一种名为WaveBound的正则化方法，通过引入动态误差界限来解决时间序列预测中的过度拟合问题，以显著提高泛化能力。 |
| [^190] | [Motif-Backdoor: Rethinking the Backdoor Attack on Graph Neural Networks via Motifs.](http://arxiv.org/abs/2210.13710) | 本文提出了一种基于motif的GNN后门攻击方法，设计了一种基于motif的触发器生成器，能够提高后门攻击的隐蔽性和效果，在各种基准数据集上进行实验证明其性能优于现有方法。 |
| [^191] | [PAC-Bayesian Offline Contextual Bandits With Guarantees.](http://arxiv.org/abs/2210.13132) | 本文提出了一种通过PAC-Bayesian方法分析离线情境强化学习问题的新算法，该算法通过优化新的泛化界限提供了保证，并在实际情境中得到了验证。 |
| [^192] | [Correlating sparse sensing for large-scale traffic speed estimation: A Laplacian-enhanced low-rank tensor kriging approach.](http://arxiv.org/abs/2210.11780) | 本文提出了一种基于拉普拉斯增强的低秩张量样条克里金方法，用于在有限观测下进行大规模交通速度估计，以从不完整的数据中恢复可信的估计值。 |
| [^193] | [Intra-Source Style Augmentation for Improved Domain Generalization.](http://arxiv.org/abs/2210.10175) | 本论文提出了一种基于源内样式增强（ISSA）的方法，用于改进语义分割中的领域泛化。通过使用掩模噪声编码器随机化样式和内容组合，ISSA有效地增加了训练数据的多样性并减少了虚假相关性，从而在不同类型的驾驶场景语义分割中获得了高达12.4％的mIoU改进。 |
| [^194] | [STaSy: Score-based Tabular data Synthesis.](http://arxiv.org/abs/2210.04018) | 提出了一种新的基于分值的表格数据生成模型（STaSy），并采用自适应学习技术和微调策略进一步提高采样质量和多样性。 |
| [^195] | [STSyn: Speeding Up Local SGD with Straggler-Tolerant Synchronization.](http://arxiv.org/abs/2210.03521) | 本文提出了一种名为STSyn的本地SGD策略，通过容错同步技术，等待最快的工人，充分利用每个工人的有效本地更新，成功地解决了同步本地随机梯度下降中因落后工人和通信效率低下而引起的问题。 |
| [^196] | [VIMA: General Robot Manipulation with Multimodal Prompts.](http://arxiv.org/abs/2210.03094) | 本研究提出了一种新的机器人操作方式——多模态提示实现通用机器人操作。通过设计一个基于变压器的机器人代理VIMA，可以处理提示并自回归地输出电机动作，实现了各种任务类型的最新结果，并能够零样本泛化到新的对象类别，这对实现通用机器人操作具有前景。 |
| [^197] | [Homotopy-based training of NeuralODEs for accurate dynamics discovery.](http://arxiv.org/abs/2210.01407) | 本论文提出了一种新的神经常微分方程训练方法，基于同步和同伦优化，可以用于从时间序列数据中提取动力学规律，而无需对模型架构进行修改。 |
| [^198] | [CRISP: Curriculum based Sequential Neural Decoders for Polar Code Family.](http://arxiv.org/abs/2210.00313) | 该研究提出了一种新颖的基于课程的序列神经解码器CRISP，可以用于极化编码族，相比连续取消(SC)译码器，CRISP具有更高的准确性和可靠性，并可轻松地拓展至极化调整卷积（PAC）代码。 |
| [^199] | [Augmentation Invariant Discrete Representation for Generative Spoken Language Modeling.](http://arxiv.org/abs/2209.15483) | 本研究提出了一种增强不变的离散语音表示方法，以提高其在生成式语音语言建模中的鲁棒性。该方法利用了transformer-based模型，并通过一种非线性量化方法来学习增强不变表示。实验证明，该方法相对于现有最先进方法具有显著的鲁棒性改进，并在语音生成任务上表现出了竞争性的表现。 |
| [^200] | [Many-body Approximation for Non-negative Tensors.](http://arxiv.org/abs/2209.15338) | 提出了一种名为多体逼近的方法来分解非负张量，通过能量建模来避免全局优化和目标秩选择的困难，可通过考虑模式之间的交互进行全局优化; 在许多任务中都展示了其有效性。 |
| [^201] | [Likelihood Adjusted Semidefinite Programs for Clustering Heterogeneous Data.](http://arxiv.org/abs/2209.15097) | 本论文提出了一种基于似然函数修正的半定规划方法用于异质数据聚类。经过实验表明，本方法在处理聚类形状不同的数据异质性时表现优异。 |
| [^202] | [Unsupervised Reward Shaping for a Robotic Sequential Picking Task from Visual Observations in a Logistics Scenario.](http://arxiv.org/abs/2209.12350) | 本研究针对物流领域典型的卸货问题，提出了一种无监督奖励塑造算法，能够减轻对代理人的监督级别，并改善RL性能。 |
| [^203] | [Global Convergence Rate Analysis of Nonsmooth Nonconvex-Nonconcave Minimax Optimization.](http://arxiv.org/abs/2209.10825) | 本论文提出了一种名为smoothed PLDA的算法来有效处理广泛的结构化非光滑非凸非凹极小极大问题，并证明了其具有全局收敛性，复杂度为O(epsilon^(-2/3))。 |
| [^204] | [MGG: Accelerating Graph Neural Networks with Fine-grained intra-kernel Communication-Computation Pipelining on Multi-GPU Platforms.](http://arxiv.org/abs/2209.06800) | MGG是一种软件流水线设计，可在多GPU平台上加速GNNs，通过采用GNN特殊的流水线构建和GPU感知的流水线映射，实现精细计算通信重叠以提高性能。 |
| [^205] | [AnaMeta: A Table Understanding Dataset of Field Metadata Knowledge Shared by Multi-dimensional Data Analysis Tasks.](http://arxiv.org/abs/2209.00946) | AnaMeta是一个包含467k张表格的数据集，它提供了常用字段元数据的四种衍生监督标签，同时推出了一种新的多编码器框架（KDF）提高了表格模型的元数据理解能力，并且提出了四个接口将元数据纳入下游分析任务中。 |
| [^206] | [Joint Optimization of Ranking and Calibration with Contextualized Hybrid Model.](http://arxiv.org/abs/2208.06164) | 本论文提出了一个联合优化排名和校准能力的方法JRC，通过对比输出logit值来提高排名能力和校准能力。 |
| [^207] | [Robust Methods for High-Dimensional Linear Learning.](http://arxiv.org/abs/2208.05447) | 该论文提出了一种在高维批量训练中具有高度鲁棒性和计算效率的线性学习方法，在多个应用程序中均能达到接近最优的估计速率，并提供了一个开源的Python库进行实现。 |
| [^208] | [Convergence of denoising diffusion models under the manifold hypothesis.](http://arxiv.org/abs/2208.05314) | 本文提供了基于流模型的去噪在流形假设下的收敛性分析，首次拓展到了目标分布受流形约束或通过经验分布给出的情况。 |
| [^209] | [ME-GAN: Learning Panoptic Electrocardio Representations for Multi-view ECG Synthesis Conditioned on Heart Diseases.](http://arxiv.org/abs/2207.10670) | 本文提出一种基于心脏疾病条件学习全景心电图表示的多视角ECG合成方法ME-GAN，利用新的混合归一化方法将疾病信息注入到合适位置，具有较好的生成效果和疾病识别能力。 |
| [^210] | [COVID-19 Detection from Respiratory Sounds with Hierarchical Spectrogram Transformers.](http://arxiv.org/abs/2207.09529) | 本研究提出了一种新的深度学习方法，使用谱图层级转换器对咳嗽或呼吸声的音频记录进行COVID-19检测。 |
| [^211] | [Deep Learning and Symbolic Regression for Discovering Parametric Equations.](http://arxiv.org/abs/2207.00529) | 该论文提出了一种神经网络体系结构，将符号回归扩展到参数系统中，通过分析多种不同类型的数学表达式，ODE 和 PDE，证明其在训练域之外的外推能力。这种体系结构还与其他深度学习体系结构集成，实现了对高维数据进行端对端训练，并可用于分析图像等数据。 |
| [^212] | [Masked World Models for Visual Control.](http://arxiv.org/abs/2206.14244) | 本研究提出了一种视觉模型驱动的RL框架，将视觉表示学习和动力学学习分离，使用自编码器和潜在动力学模型来准确建模机器人控制策略。 |
| [^213] | [Differentially Private Federated Combinatorial Bandits with Constraints.](http://arxiv.org/abs/2206.13192) | 本文研究了差分隐私联合组合赌博机问题，探讨了代理在共同学习时如何保持数据的隐私，并提出了在后悔和隐私之间实现平衡的重要性。 |
| [^214] | [Anchor Sampling for Federated Learning with Partial Client Participation.](http://arxiv.org/abs/2206.05891) | 提出一种针对部分客户端参与的联邦学习框架 FedAMD，其中核心思想是锚定抽样，将参与者分为锚定组和矿工组，以解决数据异构性。 |
| [^215] | [Mining Multi-Label Samples from Single Positive Labels.](http://arxiv.org/abs/2206.05764) | 本文提出了一种基于单一正标签的采样方法S2M，以实现对多标签样本的生成，从而避免了多标签数据集制作时高昂的注释成本。 |
| [^216] | [Utilising the CLT Structure in Stochastic Gradient based Sampling : Improved Analysis and Faster Algorithms.](http://arxiv.org/abs/2206.03792) | 本文研究了基于随机近似的采样算法，利用中心极限定理结构吸收扩散过程中的随机逼近误差并获得了改进的收敛保证。此外，对于SGLD和RBM，我们分别证明了不同的假设条件下较优的收敛率和参数范围。 |
| [^217] | [Generalization Error Bounds for Deep Neural Networks Trained by SGD.](http://arxiv.org/abs/2206.03299) | 通过对于适当参数规范的动态控制结合基于参数规范的 Rademacher 复杂度估计导出了深度神经网络的泛化误差界，适用于包括 MLP 和 CNN 在内的广泛网络架构，结果表明这个方法能够适应优化器和网络超参数的变化。 |
| [^218] | [Early Stage Convergence and Global Convergence of Training Mildly Parameterized Neural Networks.](http://arxiv.org/abs/2206.02139) | 本文证明了对于大部分的轻度参数化神经网络和损失函数，包括平方损失和交叉熵损失，在早期阶段会有很快的下降。对于指数型损失函数，在一些对于数据的假设下，本文证明了GD会全局收敛。本文的研究基于对神经元激活模式的微观分析，得出了“神经元划分”的结果，有助于理解神经网络训练动态的行为，并可能具有独立的兴趣。 |
| [^219] | [MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging.](http://arxiv.org/abs/2206.01408) | 提出了一种基于元学习的学习率调整器MetaLR，可以使不同层次的层次根据它们的可转移性自动协同适应下游任务，在多个医学图像数据集上实验表明MetaLR在微调方面的效果优于多种领先的微调方法。 |
| [^220] | [Detecting hidden confounding in observational data using multiple environments.](http://arxiv.org/abs/2205.13935) | 使用独立数据生成过程下的多环境方法，可以检测观测数据中的未观察到的混淆因素，并提出了测试独立性的程序。 |
| [^221] | [Personalized Algorithmic Recourse with Preference Elicitation.](http://arxiv.org/abs/2205.13743) | 研究提出了PEAR方法，这是一个首个能够针对最终用户需求提供个性化算法补救成本的人机交互方法。该方法利用贝叶斯偏好引导的见解，通过最大化原则性信息增益度量来计算目标用户选择的预期效用，然后将偏好引导整合到强化学习框架中。该方法显著提高了算法干预的经济实用性和用户友好性。 |
| [^222] | [What's Behind the Mask: Understanding Masked Graph Modeling for Graph Autoencoders.](http://arxiv.org/abs/2205.10053) | 本文探究了 Masked Graph Modeling 在图自编码器中的有效应用，并提出了基于蒙版的图建模（MGM）作为自监督学习的一个原先的任务，证明了它的优势作用。在获得了理论及实证证据的支持后，我们提出了 MaskGAE，一种可扩展有效的自监督学习方法。 |
| [^223] | [Visual Acuity Prediction on Real-Life Patient Data Using a Machine Learning Based Multistage System.](http://arxiv.org/abs/2204.11970) | 本研究提供了一种使用机器学习技术开发预测模型的多阶段系统，可高精度预测三种眼疾患者的视力变化，并辅助眼科医生进行临床决策和患者咨询。 |
| [^224] | [Compressed Empirical Measures (in finite dimensions).](http://arxiv.org/abs/2204.08847) | 本论文探讨了在有限维再生核希尔伯特空间中压缩经验测度的方法，导出了关于这样一个近似的核心集必须有的大小的高概率下限，并开发了一些技术以将压缩方法应用于具体的推断问题。 |
| [^225] | [Dual-Path Style Learning for End-to-End Noise-Robust Speech Recognition.](http://arxiv.org/abs/2203.14838) | 本文提出了用于端到端的噪声抗干扰语音识别的双路径式学习方法，包括样式学习和融合特征以从干净特征中学习潜在的语音信息，最终实现了10.6％和8.6％的词错误率（WER）降低。 |
| [^226] | [On-the-Fly Feature Based Rapid Speaker Adaptation for Dysarthric and Elderly Speech Recognition.](http://arxiv.org/abs/2203.14593) | 本文提出了两种新形式的数据有效、基于特征的动态说话者适应方法，能够显著提高言语障碍和老年人语音识别的准确率。 |
| [^227] | [Sampling Ex-Post Group-Fair Rankings.](http://arxiv.org/abs/2203.00887) | 本文提出了一组自然的公理来定义随机群体公平排名，证明了仅有事后群体公平排名的分布的存在，并提出了两种算法来采样随机的群体公平排名。 |
| [^228] | [Sion's Minimax Theorem in Geodesic Metric Spaces and a Riemannian Extragradient Algorithm.](http://arxiv.org/abs/2202.06950) | 该论文提出了在测地度量空间中的Sion极小极大定理和黎曼外推算法，在保持问题可处理的同时，为非凸-非凹极小极大问题提供了一个广泛的推广。 |
| [^229] | [Exploring Self-Attention Mechanisms for Speech Separation.](http://arxiv.org/abs/2202.02884) | 本论文探究了Transformers用于语音分离的应用，提出了SepFormer模型并在包括含噪和含噪回声的数据集上得到了最新成果。同时，还做了语音增强方面的研究，并首次将Linformers、Lonformers和ReFormers等高效的自注意力机制应用于语音分离，发现它们可以显著减少内存需求。 |
| [^230] | [HeterPS: Distributed Deep Learning With Reinforcement Learning Based Scheduling in Heterogeneous Environments.](http://arxiv.org/abs/2111.10635) | 这篇论文介绍了一个名为Paddle-HeterPS的分布式框架，基于强化学习的调度方法可以高效地利用多种类型的计算资源，解决了分布式深度学习训练中多层次分配计算资源的问题。 |
| [^231] | [Composite Goodness-of-fit Tests with Kernels.](http://arxiv.org/abs/2111.10275) | 本文提出了一种基于核的假设检验方法，可以解决具有挑战性的复合检验问题，其核心思想是在正确的模型规范的零假设下，非参数地估计参数（或模拟器）分布。 |
| [^232] | [MMD Aggregated Two-Sample Test.](http://arxiv.org/abs/2110.15073) | 本文提出了两种新颖的基于最大均值差异（MMD）的非参数双样本核检验，并构造了一种自适应平均测试，称为MMDAgg，以解决平滑参数未知的问题。 |
| [^233] | [Sinkhorn Distributionally Robust Optimization.](http://arxiv.org/abs/2109.11926) | 本文通过使用Sinkhorn距离进行分布鲁棒优化，推导出更容易处理且在实际中更合理的最坏情况分布，提出了解决方案，并展示了其优越性能。 |
| [^234] | [Stochastic Optimization under Distributional Drift.](http://arxiv.org/abs/2108.07356) | 本文提供了一种在分布漂移下优化凸函数的新方法，经数值实验证明在低漂移-噪声比的情况下，近端随机梯度方法采用步长衰减策略可显著提升跟踪效率。 |
| [^235] | [SGD with a Constant Large Learning Rate Can Converge to Local Maxima.](http://arxiv.org/abs/2107.11774) | 本研究构建了最坏情况下的优化问题，证明了带有定常大学习率的SGD可能表现出许多奇怪且潜在的不良行为，包括：收敛于局部最大值、缓慢越过鞍点和更喜欢尖锐的最小值。这强调了深入分析SGD在深度学习中作用的重要性。 |
| [^236] | [Fingerprinting Generative Adversarial Networks.](http://arxiv.org/abs/2106.11760) | 本文提出了一种保护GAN知识产权的指纹识别方案，通过生成指纹样本并嵌入到分类器中进行版权验证，解决了前一种对分类模型的指纹识别方法在简单转移至GAN时遇到的隐蔽性和鲁棒性瓶颈，具有实际保护现代GAN模型的可行性。 |
| [^237] | [Variance-Dependent Best Arm Identification.](http://arxiv.org/abs/2106.10417) | 本文研究了在多臂老虎机游戏中识别最优臂的问题。提出了一种自适应算法，该算法探索臂的奖励差距和方差，并使用一种称为分组中位数淘汰的新方法根据收集的信息做出未来决策。所提出的算法保证以概率(1-δ)输出最优臂，并使用最多的O（Σ(i=1)^n (σi²/Δi²+1/Δi)(lnδ-1+ln lnΔi-1)）个样本，这比方差独立算法获得了明显的优势。 |
| [^238] | [FairCanary: Rapid Continuous Explainable Fairness.](http://arxiv.org/abs/2106.07057) | FairCanary是一个持续模型监控系统，使用量化人口漂移(QDD)度量模型偏差，避免传统阈值偏差度量的统计限制，可以提供可解释的公平性分析，并在多个实际数据集上实现了可操作性见解。 |
| [^239] | [Diverse Gaussian Noise Consistency Regularization for Robustness and Uncertainty Calibration.](http://arxiv.org/abs/2104.01231) | 本研究提出了一种多样化高斯噪声一致性正则化方法，用于同时提高图像分类器的鲁棒性和准确性，与其他强大的多样化数据增强基础相比，可以将鲁棒性提高4.2-18.4％以应对未预见的噪声污染。 |
| [^240] | [The Gradient Convergence Bound of Federated Multi-Agent Reinforcement Learning with Efficient Communication.](http://arxiv.org/abs/2103.13026) | 本文提出了两种优化策略, 一种是逐步衰减本地梯度权重的衰减模式, 另一种是基于代价最小化设计的共识算法来减少模型之间的通信量, 有效解决了独立学习环境异质性和联邦学习的通信开销问题。 |
| [^241] | [Personalized Federated Learning: A Unified Framework and Universal Optimization Techniques.](http://arxiv.org/abs/2102.09743) | 本文探讨了个性化联邦学习的优化问题，并提出了适用于多种现有个性化FL目标的通用优化器。同时，提出了一个适用于广泛一类强凸性个性化FL模型的全面优化理论。其方法在通信和本地计算方面具有实用性和优越性，并能够恢复应对特定个性化FL目标的最佳计算和通信保证。 |
| [^242] | [On the Nature and Types of Anomalies: A Review of Deviations in Data.](http://arxiv.org/abs/2007.15634) | 本文通过广泛文献综述，提供了第一个理论上基于原则且领域无关的数据异常分类法，并呈现了异常类型和子类型的全面概述。 |
| [^243] | [A Modern Introduction to Online Learning.](http://arxiv.org/abs/1912.13213) | 这本专著介绍了在线学习的基本概念以及凸优化背景下的一阶和二阶算法, 包括欧几里得和非欧几里得设置中的在线镜像下降或遵循正则化领导者等算法。 |
| [^244] | [Fast MLE Computation for the Dirichlet Multinomial.](http://arxiv.org/abs/1405.0099) | 本文提出了一种修改的方法来快速计算Dirichlet分布的MLE参数，相较于现有实现方法，只需要一遍遍历数据集就可以大大减少运行时间。 |
| [^245] | [On the Dual Formulation of Boosting Algorithms.](http://arxiv.org/abs/0901.3590) | 本文从对偶问题的角度研究了Boosting算法，证明了AdaBoost等算法通过最大化间隔和控制间隔方差来维护更好的间隔分布，提出了基于列生成的优化算法，收敛速度更快。 |

# 详细

[^1]: 输入感知的动态时间步长脉冲神经网络用于高效的内存计算

    Input-Aware Dynamic Timestep Spiking Neural Networks for Efficient In-Memory Computing. (arXiv:2305.17346v1 [cs.NE])

    [http://arxiv.org/abs/2305.17346](http://arxiv.org/abs/2305.17346)

    DT-SNN是一种输入感知的动态时间步长脉冲神经网络算法解决方案， 可以根据数据特性动态确定时间步长的数量，从而将SNN的效率最大化。

    

    最近，由于脉冲神经网络（SNN）能够处理稀疏和二进制脉冲信息，并避免昂贵的乘法操作，因此已经引起了广泛的研究兴趣。虽然SNN在内存计算（IMC）架构上的效率可以实现，但我们发现SNN的能量成本和延迟随着在IMC硬件上使用的时间步长数量呈线性增长。因此，为了最大化SNN的效率，我们提出了一种输入感知的动态时间步长SNN（DT-SNN）算法解决方案，该解决方案可以在推理过程中动态确定时间步长的数量，而且还根据输入数据的特性进行调整。具体地，通过在每个时间步长后计算输出的熵，并将其与预定义的阈值进行比较，可以决定当前时间步长处理的信息是否足以进行有信心的预测。我们将DT-SNN部署在IMC架构上，并展示了...

    Spiking Neural Networks (SNNs) have recently attracted widespread research interest as an efficient alternative to traditional Artificial Neural Networks (ANNs) because of their capability to process sparse and binary spike information and avoid expensive multiplication operations. Although the efficiency of SNNs can be realized on the In-Memory Computing (IMC) architecture, we show that the energy cost and latency of SNNs scale linearly with the number of timesteps used on IMC hardware. Therefore, in order to maximize the efficiency of SNNs, we propose input-aware Dynamic Timestep SNN (DT-SNN), a novel algorithmic solution to dynamically determine the number of timesteps during inference on an input-dependent basis. By calculating the entropy of the accumulated output after each timestep, we can compare it to a predefined threshold and decide if the information processed at the current timestep is sufficient for a confident prediction. We deploy DT-SNN on an IMC architecture and show 
    
[^2]: 重新思考对抗策略：多智能体强化学习中的广义攻击形式和可证明的防御

    Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in Multi-Agent RL. (arXiv:2305.17342v1 [cs.LG])

    [http://arxiv.org/abs/2305.17342](http://arxiv.org/abs/2305.17342)

    本文介绍了另一种常见、现实的多智能体RL攻击设置，提出了一种模拟攻击者对代理$\alpha$控制的更一般化攻击形式。并解决了先前攻击模型中缺乏可证明防御的问题。

    

    大多数现有的研究研究直接扰动受害者的状态/动作或基础转移动态以展示强化学习智能体在对抗攻击下的脆弱性。然而，这样的直接操纵在实践中并不总是可行的。在本文中，我们考虑另一种常见且现实的攻击设置：在经过训练的多智能体RL的设置中，在部署期间，受害代理$\nu$被攻击者控制另一个代理$\alpha$以敌对方式行动，使用“对抗策略”对受害代理进行攻击。尽管之前的攻击模型考虑了这种设置，但他们没有考虑到攻击者可以遇到抵抗，因此只能部分控制代理$\alpha$，同时引入可察觉的“异常”行为，这些行为很容易被检测到。并且缺乏针对这些对抗策略的可证明的防御。为了解决这些问题，我们引入了一个更一般化的攻击形式，模拟了攻击者在何种程度上可以控制代理$\alpha$。

    Most existing works consider direct perturbations of victim's state/action or the underlying transition dynamics to show vulnerability of reinforcement learning agents under adversarial attacks. However, such direct manipulation may not always be feasible in practice. In this paper, we consider another common and realistic attack setup: in a multi-agent RL setting with well-trained agents, during deployment time, the victim agent $\nu$ is exploited by an attacker who controls another agent $\alpha$ to act adversarially against the victim using an \textit{adversarial policy}. Prior attack models under such setup do not consider that the attacker can confront resistance and thus can only take partial control of the agent $\alpha$, as well as introducing perceivable ``abnormal'' behaviors that are easily detectable. A provable defense against these adversarial policies is also lacking. To resolve these issues, we introduce a more general attack formulation that models to what extent the a
    
[^3]: 使用优化空间的同态矩阵乘法来改进隐私保护PCA

    Improved Privacy-Preserving PCA Using Space-optimized Homomorphic Matrix Multiplication. (arXiv:2305.17341v1 [cs.CR])

    [http://arxiv.org/abs/2305.17341](http://arxiv.org/abs/2305.17341)

    本文提出一种使用近似数值算术同态加密方案进行隐私保护PCA的新方法，相对以往方法，其在效率、准确性和可扩展性上均有提升，实现了同态矩阵乘法和高效同态电路，计算特征值和特征向量时具有良好的效果。

    

    主成分分析（PCA）是机器学习和数据分析领域中的重要技术。本研究提出了一种新的方法，使用近似数值算术同态加密方案进行隐私保护PCA。我们基于一种被称为PowerMethod的PCA常规方法，该方法以协方差矩阵作为输入，并产生与数据集的第一主成分对应的近似特征向量。我们的方法在效率、准确性和可扩展性方面优于以前的方法（如Pandas CSCML 21）。为了实现这样的效率和准确性，我们实现了以下优化：（i）优化了同态矩阵乘法技术（Jiang等人SIGSAC 2018），该技术在协方差矩阵的计算中起着关键作用；（ii）设计了一个有效的同态电路来同态计算协方差矩阵；（iii）设计了一种新颖且高效的同态加密方案用于特征值和特征向量的计算。

    Principal Component Analysis (PCA) is a pivotal technique in the fields of machine learning and data analysis. In this study, we present a novel approach for privacy-preserving PCA using an approximate numerical arithmetic homomorphic encryption scheme. We build our method upon a proposed PCA routine known as the PowerMethod, which takes the covariance matrix as input and produces an approximate eigenvector corresponding to the first principal component of the dataset. Our method surpasses previous approaches (e.g., Pandas CSCML 21) in terms of efficiency, accuracy, and scalability.  To achieve such efficiency and accuracy, we have implemented the following optimizations: (i) We optimized a homomorphic matrix multiplication technique (Jiang et al. SIGSAC 2018) that will play a crucial role in the computation of the covariance matrix. (ii) We devised an efficient homomorphic circuit for computing the covariance matrix homomorphically. (iii) We designed a novel and efficient homomorphic 
    
[^4]: 只使用前向传递微调语言模型

    Fine-Tuning Language Models with Just Forward Passes. (arXiv:2305.17333v1 [cs.LG])

    [http://arxiv.org/abs/2305.17333](http://arxiv.org/abs/2305.17333)

    本论文提出了一种内存高效的零阶优化器，可以使用与推理相同的存储空间微调语言模型，其可以在大规模模型下更快地优化，具有更好的实验结果。

    

    微调语言模型已经在各种下游任务中取得了成功，但随着语言模型的增大，反向传播需要的存储空间数量变得过高。零阶（ZO）方法理论上仅使用两次前向传递就可以估计梯度，但通常情况下对大型模型进行优化的速度非常慢。在本文中，我们提出了一种内存高效的零阶优化器（MeZO），将经典的ZO-SGD方法适应于原地操作，从而使用与推理相同的存储空间微调语言模型。例如，只使用一张A100 80GB GPU，MeZO就可以训练一个300亿参数的模型，而使用反向传播可以在相同的预算下仅训练一个27亿个参数的语言模型。我们在各种模型类型（掩码和自回归语言模型）、模型规模（高达66B）和下游任务（分类、多项选择和生成）进行了全面的实验。我们的结果表明，（1）MeZO明显优于上下文学习和线性PR模型。

    Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear pr
    
[^5]: 学习能力：模型有效维度的度量方式

    Learning Capacity: A Measure of the Effective Dimensionality of a Model. (arXiv:2305.17332v1 [cs.LG])

    [http://arxiv.org/abs/2305.17332](http://arxiv.org/abs/2305.17332)

    学习能力是一种度量模型有效维度的方法，它可以帮助我们判断是否需要获取更多数据或者寻找新的体系结构以提高性能。

    

    我们利用热力学和推理之间的正式对应关系，将样本数量视为反温度，定义了一种“学习能力”，这是模型有效维度的度量方式。我们发现，对于许多在典型数据集上训练的深度网络，学习能力仅占参数数量的一小部分，取决于用于训练的样本数量，并且在数值上与从PAC-Bayesian框架获得的能力概念一致。学习能力作为测试误差的函数不会出现双峰下降。我们展示了模型的学习能力在非常小和非常大的样本大小处饱和，这提供了指导，说明是否应该获取更多数据或者寻找新的体系结构以提高性能。我们展示了如何使用学习能力来理解有效维数，即使是非参数模型，如随机森林。

    We exploit a formal correspondence between thermodynamics and inference, where the number of samples can be thought of as the inverse temperature, to define a "learning capacity'' which is a measure of the effective dimensionality of a model. We show that the learning capacity is a tiny fraction of the number of parameters for many deep networks trained on typical datasets, depends upon the number of samples used for training, and is numerically consistent with notions of capacity obtained from the PAC-Bayesian framework. The test error as a function of the learning capacity does not exhibit double descent. We show that the learning capacity of a model saturates at very small and very large sample sizes; this provides guidelines, as to whether one should procure more data or whether one should search for new architectures, to improve performance. We show how the learning capacity can be used to understand the effective dimensionality, even for non-parametric models such as random fores
    
[^6]: 基于增强的适应性检索器以通用插件的形式提高了语言模型的泛化能力

    Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In. (arXiv:2305.17331v1 [cs.CL])

    [http://arxiv.org/abs/2305.17331](http://arxiv.org/abs/2305.17331)

    本文提出了增强适应检索器(AAR)的方案，通过从已知的源LM中学习LM的偏好，能够以通用插件的形式帮助目标LM在不进行微调的情况下显著提高零样本泛化能力。

    

    检索增强可以通过提供外部信息帮助语言模型(LMs)执行知识密集的任务。检索增强的先前工作通常联合微调检索器和LM，使它们紧密耦合。在本文中，我们探索了通用检索插件的方案：检索器是要帮助目标LM的，这些LM可能事先不知道或无法一起微调。为了检索出对未见过的目标LM有用的文档，我们提出了增强适应检索器(AAR)，它从已知的源LM中学习LM的偏好。在MMLU和PopQA数据集上的实验证明，我们用小型源LM训练的AAR能够显着提高从250M Flan-T5到175B InstructGPT范围内的更大目标LM的零样本泛化。进一步的分析表明，不同LM的偏好重叠，使得以单个源LM训练的AAR能够作为各种目标LM的通用插件。我们的代码...

    Retrieval augmentation can aid language models (LMs) in knowledge-intensive tasks by supplying them with external information. Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target LMs that may not be known beforehand or are unable to be fine-tuned together. To retrieve useful documents for unseen target LMs, we propose augmentation-adapted retriever (AAR), which learns LM's preferences obtained from a known source LM. Experiments on the MMLU and PopQA datasets demonstrate that our AAR trained with a small source LM is able to significantly improve the zero-shot generalization of larger target LMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates that the preferences of different LMs overlap, enabling AAR trained with a single source LM to serve as a generic plug-in for various target LMs. Our code 
    
[^7]: MADiff：离线多智能体学习与扩散模型

    MADiff: Offline Multi-agent Learning with Diffusion Models. (arXiv:2305.17330v1 [cs.AI])

    [http://arxiv.org/abs/2305.17330](http://arxiv.org/abs/2305.17330)

    本论文提出了基于注意力的扩散模型MADiff，解决了多智能体问题，是第一个扩散模型应用于多智能体离线RL的框架。

    

    扩散模型（DM）是一种强大的生成模型，最近在包括离线强化学习在内的各种场景中取得了巨大成功，其中策略通过在在线评估中产生轨迹来进行规划学习。然而，尽管单智能体学习显示了其有效性，但仍不清楚DM如何在多智能体问题中操作，其中代理商很难在独立建模每个代理商轨迹的情况下完成团队合作。在本文中，我们提出MADiff，一种新的生成式多智能体学习框架，以解决这个问题。MADiff是通过基于注意力的扩散模型来实现对多个扩散智能体行为的复杂协调建模。据我们所知，MADiff是第一个基于扩散的多智能体离线RL框架，它既可以行为为分散的政策，又可以为集中控制器，其中包括对手建模，并可用于多智能体轨迹预测。

    Diffusion model (DM), as a powerful generative model, recently achieved huge success in various scenarios including offline reinforcement learning, where the policy learns to conduct planning by generating trajectory in the online evaluation. However, despite the effectiveness shown for single-agent learning, it remains unclear how DMs can operate in multi-agent problems, where agents can hardly complete teamwork without good coordination by independently modeling each agent's trajectories. In this paper, we propose MADiff, a novel generative multi-agent learning framework to tackle this problem. MADiff is realized with an attention-based diffusion model to model the complex coordination among behaviors of multiple diffusion agents. To the best of our knowledge, MADiff is the first diffusion-based multi-agent offline RL framework, which behaves as both a decentralized policy and a centralized controller, which includes opponent modeling and can be used for multi-agent trajectory predic
    
[^8]: 《Zero-TPrune: 基于预训练Transformers关注图的零射击令牌剪枝方法》

    Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers. (arXiv:2305.17328v1 [cs.CV])

    [http://arxiv.org/abs/2305.17328](http://arxiv.org/abs/2305.17328)

    《Zero-TPrune》是一个考虑到令牌的重要性和相似性的零射击方法，它利用预训练Transformer模型的注意图来进行令牌剪枝，以求解在边缘设备上Transformer模型即插即用的难题。

    

    最近在边缘设备上部署Transformer模型变得越来越具有挑战性，原因是模型的体积呈指数级增长，而推理成本则随输入序列中令牌数量的平方提高。令牌剪枝是解决这一挑战的新兴解决方法之一，由于其易于在各种Transformer支持的模型上部署。然而，大多数令牌剪枝方法需要在剪枝后或期间进行计算密集型的微调过程，在许多情况下这是不可取的。最近的一些研究探讨了没有微调的即插即用的预训练Transformer的剪枝方法。但是，它们只考虑了令牌的重要性。在这项工作中，我们提出了Zero-TPrune，这是一种零射击方法，它既考虑令牌的重要性又考虑相似性来执行令牌剪枝。Zero-TPrune利用预训练Transformer模型的注意图来为令牌生成一个重要性排名并移除信息较少的令牌。注意矩阵可用于推断即插即用的模型。

    Deployment of Transformer models on the edge is increasingly challenging due to the exponentially growing model size and inference cost that scales quadratically with the number of tokens in the input sequence. Token pruning is an emerging solution to address this challenge due to its ease of deployment on various Transformer backbones. However, most token pruning methods require a computationally-expensive fine-tuning process after or during pruning, which is not desirable in many cases. Some recent works explore pruning of off-the-shelf pre-trained Transformers without fine-tuning. However, they only take the importance of tokens into consideration. In this work, we propose Zero-TPrune, the first zero-shot method that considers both the importance and similarity of tokens in performing token pruning. Zero-TPrune leverages the attention graph of pre-trained Transformer models to produce an importance rank for tokens and removes the less informative tokens. The attention matrix can be 
    
[^9]: 基于内核KL散度的自监督学习方法Kernel-SSL

    Kernel-SSL: Kernel KL Divergence for Self-supervised Learning. (arXiv:2305.17326v1 [cs.LG])

    [http://arxiv.org/abs/2305.17326](http://arxiv.org/abs/2305.17326)

    本文提出了一种名为Kernel-SSL的自监督学习方法，将多种现有非对比学习方法建立在了再生核希尔伯特空间（RKHS）理解之上并优化了其中的均值嵌入和协方差算子，实验结果显示，在ImageNet数据集下表现显著超越最先进的方法，提高了4.6%。

    

    对比学习通常将一个正锚点样本与许多负样本进行比较，来完成自监督学习（SSL）。相反，非对比学习，例如BYOL、SimSiam和Barlow Twins等方法，在没有显式使用负样本的情况下完成SSL。受对比学习现有分析的启发，我们提供了多种现有非对比学习方法的再生核希尔伯特空间（RKHS）理解。随后，我们提出了一种新的损失函数Kernel-SSL，直接优化RKHS中的均值嵌入和协方差算子。实验中，我们的方法Kernel-SSL在线性评估设置下在ImageNet数据集上大幅优于最先进的方法。具体来说，在进行100个epoch的预训练时，我们的方法比SimCLR表现提高了4.6%。

    Contrastive learning usually compares one positive anchor sample with lots of negative samples to perform Self-Supervised Learning (SSL). Alternatively, non-contrastive learning, as exemplified by methods like BYOL, SimSiam, and Barlow Twins, accomplishes SSL without the explicit use of negative samples. Inspired by the existing analysis for contrastive learning, we provide a reproducing kernel Hilbert space (RKHS) understanding of many existing non-contrastive learning methods. Subsequently, we propose a novel loss function, Kernel-SSL, which directly optimizes the mean embedding and the covariance operator within the RKHS. In experiments, our method Kernel-SSL outperforms state-of-the-art methods by a large margin on ImageNet datasets under the linear evaluation settings. Specifically, when performing 100 epochs pre-training, our method outperforms SimCLR by 4.6%.
    
[^10]: 强凸优化的次梯度法的原始对偶理论

    Some Primal-Dual Theory for Subgradient Methods for Strongly Convex Optimization. (arXiv:2305.17323v1 [math.OC])

    [http://arxiv.org/abs/2305.17323](http://arxiv.org/abs/2305.17323)

    本文提出了一种强凸优化的次梯度法原始对偶理论，可以实现简单的、最佳的停止准则和优化证明，同时可以适用于各种步长的选择和非Lipschitz病态问题，保证了这些方法次线性收敛速度。

    

    本文考虑强凸但潜在非光滑非Lipschitz优化的（随机）次梯度法。我们提供了新的等价对偶描述（类似于对偶平均）来描述经典的次梯度法，近端次梯度法和切换次梯度法。这些等价性能够以 $O(1/T)$ 的速度收敛，同时能够在强凸优化问题上分别还提供了经典原始间隙和前人未曾分析的对偶间隙保证。因此，我们的理论为这些经典方法提供了简单的、最佳的停止准则和优化证明，而不需要额外的计算成本。我们的结果适用于近乎所有的步长选择和一系列的非Lipschitz病态问题，对于在这些情况下，次梯度法的早期迭代可能会出现指数级的发散，而之前的研究没有处理过这种问题。即使在这种不良操作的情况下，我们的理论仍然确保和 bounds 了这些方法的次线性收敛速度。

    We consider (stochastic) subgradient methods for strongly convex but potentially nonsmooth non-Lipschitz optimization. We provide new equivalent dual descriptions (in the style of dual averaging) for the classic subgradient method, the proximal subgradient method, and the switching subgradient method. These equivalences enable $O(1/T)$ convergence guarantees in terms of both their classic primal gap and a not previously analyzed dual gap for strongly convex optimization. Consequently, our theory provides these classic methods with simple, optimal stopping criteria and optimality certificates at no added computational cost. Our results apply under nearly any stepsize selection and for a range of non-Lipschitz ill-conditioned problems where the early iterations of the subgradient method may diverge exponentially quickly (a phenomenon which, to the best of our knowledge, no prior works address). Even in the presence of such undesirable behaviors, our theory still ensures and bounds eventu
    
[^11]: 雷达照亮黑暗：通过相机-雷达融合增强自动驾驶车辆的低能见度感知

    Radar Enlighten the Dark: Enhancing Low-Visibility Perception for Automated Vehicles with Camera-Radar Fusion. (arXiv:2305.17318v1 [cs.CV])

    [http://arxiv.org/abs/2305.17318](http://arxiv.org/abs/2305.17318)

    针对低能见度的自动驾驶车辆感知问题，本文提出了一种新的基于Transformer的3D目标检测模型“REDFormer”，通过鸟瞰相机-雷达融合进行实现。该模型在nuScenes数据集上表现出优异的分类和检测准确性，且相较于现有模型更经济实用。

    

    传感器融合是一种关键的增强技术，用于在不同的驾驶条件下提高自动驾驶车辆感知系统的准确性和可靠性。然而，恶劣的天气和低光照条件仍然是一个挑战，在这些条件下，传感器性能会显著下降，从而使车辆安全面临潜在风险。本文提出了一种新颖的基于Transformer的3D目标检测模型“REDFormer”，利用鸟瞰相机-雷达融合的便利和经济实用性来解决低能见度问题。在使用多雷达点云、天气信息和时间数据的nuScenes数据集上，我们的模型在分类和检测准确性方面优于最先进的模型。最后，我们对每个模型组件进行了广泛的消融研究，以了解它们对应对上述问题的贡献。

    Sensor fusion is a crucial augmentation technique for improving the accuracy and reliability of perception systems for automated vehicles under diverse driving conditions. However, adverse weather and low-light conditions remain challenging, where sensor performance degrades significantly, exposing vehicle safety to potential risks. Advanced sensors such as LiDARs can help mitigate the issue but with extremely high marginal costs. In this paper, we propose a novel transformer-based 3D object detection model "REDFormer" to tackle low visibility conditions, exploiting the power of a more practical and cost-effective solution by leveraging bird's-eye-view camera-radar fusion. Using the nuScenes dataset with multi-radar point clouds, weather information, and time-of-day data, our model outperforms state-of-the-art (SOTA) models on classification and detection accuracy. Finally, we provide extensive ablation studies of each model component on their contributions to address the above-mention
    
[^12]: 基于机器学习的风险评估自动屋顶类型分类

    Automatic Roof Type Classification Through Machine Learning for Regional Wind Risk Assessment. (arXiv:2305.17315v1 [cs.LG])

    [http://arxiv.org/abs/2305.17315](http://arxiv.org/abs/2305.17315)

    本论文使用机器学习技术开发了一个可以自动识别屋顶类型的分类框架，从卫星图像上提取建筑级别的高分辨率屋顶类型数据，填补了公开数据库缺失的数据，为区域风险评估提供了有效的工具。

    

    屋顶类型是风险模型中最关键的建筑特征之一，然而公开数据库中最常缺失这一特征。本文开发了一个自动屋顶分类框架，基于机器学习生成高分辨率的屋顶类型数据。使用卷积神经网络在建筑级别卫星图像上对屋顶类型进行分类，模型在1000个测试建筑上的F1分数达到0.96。然后，利用CNN模型预测美国北卡罗来纳和佛罗里达州的161,772个单户住宅的屋顶类型，并展示了城市和人口普查区尺度的屋顶类型分布。普查区之间占主导地位的屋顶类型存在很大的差异。为了改善缺失屋顶数据的完整性，开发了填补算法，使用关键建筑属性和邻居数据填充低质量图像中缺少的屋顶数据。

    Roof type is one of the most critical building characteristics for wind vulnerability modeling. It is also the most frequently missing building feature from publicly available databases. An automatic roof classification framework is developed herein to generate high-resolution roof-type data using machine learning. A Convolutional Neural Network (CNN) was trained to classify roof types using building-level satellite images. The model achieved an F1 score of 0.96 on predicting roof types for 1,000 test buildings. The CNN model was then used to predict roof types for 161,772 single-family houses in New Hanover County, NC, and Miami-Dade County, FL. The distribution of roof type in city and census tract scales was presented. A high variance was observed in the dominant roof type among census tracts. To improve the completeness of the roof-type data, imputation algorithms were developed to populate missing roof data due to low-quality images, using critical building attributes and neighbor
    
[^13]: 超越正向缩放：否定语对语言模型缩放趋势的影响。

    Beyond Positive Scaling: How Negation Impacts Scaling Trends of Language Models. (arXiv:2305.17311v1 [cs.CL])

    [http://arxiv.org/abs/2305.17311](http://arxiv.org/abs/2305.17311)

    本研究介绍了一个包含否定问题的数据集NeQA，其中语言模型表现出反向缩放、U型缩放或正向缩放，解决NeQA依赖于问答和否定理解两个子任务，其缩放趋势由这两个子任务的缩放趋势组合形成。

    

    已经证明，语言模型表现出正向缩放，在大小、计算或数据方面扩展模型会提高性能。在本研究中，我们引入了一个包含否定问句的数据集NeQA，其中语言模型不会表现出简单的正向缩放。我们展示了这个任务可以表现出反向缩放、U形缩放或正向缩放，并且在使用更强大的提示方法或模型族群时，这三种缩放趋势会按照这个顺序发生转变。我们假设解决NeQA依赖于两个子任务：问答（任务1）和否定理解（任务2）。我们发现任务1具有线性缩放，而任务2具有S形缩放，并具有一个紧急的转折点，将这两个缩放趋势组合起来即可得出最终的NeQA缩放趋势。我们的研究揭示并提供了一种分析语言模型复杂缩放趋势的方法。

    Language models have been shown to exhibit positive scaling, where performance improves as models are scaled up in terms of size, compute, or data. In this work, we introduce NeQA, a dataset consisting of questions with negation in which language models do not exhibit straightforward positive scaling. We show that this task can exhibit inverse scaling, U-shaped scaling, or positive scaling, and the three scaling trends shift in this order as we use more powerful prompting methods or model families. We hypothesize that solving NeQA depends on two subtasks: question answering (task 1) and negation understanding (task 2). We find that task 1 has linear scaling, while task 2 has sigmoid-shaped scaling with an emergent transition point, and composing these two scaling trends yields the final scaling trend of NeQA. Our work reveals and provides a way to analyze the complex scaling trends of language models.
    
[^14]: “Chain-of-Thought Hub: 连续测量大型语言模型推理表现的努力”

    Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance. (arXiv:2305.17306v1 [cs.CL])

    [http://arxiv.org/abs/2305.17306](http://arxiv.org/abs/2305.17306)

    本文介绍了一个名为 Chain-of-Thought Hub 的开源评估套件，目的是评估大型语言模型的多步推理能力。它是为了追踪LLMs进展而编制的具有挑战性的推理基准。目前的研究结果表明，模型规模与推理能力相关，而 Claude-v1.3 是迄今为止推理能力最强的LLM。

    

    “随着大型语言模型（LLMs）的不断发展，它们的评估变得越来越重要但也更具挑战性。本文提出了 Chain-of-Thought Hub，这是一个开源的评估套件，旨在评估大型语言模型的多步推理能力。我们之所以对这个设置感兴趣，是因为 (1) 从 GPT 和 PaLM 模型家族的行为中，我们观察到复杂的推理很可能是一个更弱和更强的LLMs之间的关键区别； (2) 我们预见大型语言模型将成为下一代计算平台，并促进基于LLM的新应用的生态系统，这自然需要基础模型执行常常涉及语言和逻辑操作组合的复杂任务。我们的方法是编制一系列具有挑战性的推理基准，以跟踪LLMs的进展。我们目前的结果表明：(1) 模型规模显然与推理能力相关；(2) 截至2023年5月，Claude-v1.3 是迄今为止推理能力最强的LLM 。”

    As large language models (LLMs) are continuously being developed, their evaluation becomes increasingly important yet challenging. This work proposes Chain-of-Thought Hub, an open-source evaluation suite on the multi-step reasoning capabilities of large language models. We are interested in this setting for two reasons: (1) from the behavior of GPT and PaLM model family, we observe that complex reasoning is likely to be a key differentiator between weaker and stronger LLMs; (2) we envisage large language models to become the next-generation computational platform and foster an ecosystem of LLM-based new applications, this naturally requires the foundation models to perform complex tasks that often involve the composition of linguistic and logical operations. Our approach is to compile a suite of challenging reasoning benchmarks to track the progress of LLMs. Our current results show that: (1) model scale clearly correlates with reasoning capabilities; (2) As of May 2023, Claude-v1.3 an
    
[^15]: 从黑盒模型到可解释模型的转化，用于高效的迁移学习

    Distilling BlackBox to Interpretable models for Efficient Transfer Learning. (arXiv:2305.17303v1 [cs.CV])

    [http://arxiv.org/abs/2305.17303](http://arxiv.org/abs/2305.17303)

    本论文提出了一种方法可以将黑盒模型转化成为可解释模型并在目标领域成功进行迁移学习，从而实现高效迁移学习。

    

    建立具有普适性的AI模型是医疗领域面临的主要挑战之一。神经网络（NN）模型即使输入分布轻微移位（例如扫描仪类型），也会受到影响，而放射科医生则依赖于异常性的通用描述性规则。微调模型以将知识从一个领域转移到另一个领域需要大量标记数据。在本文中，我们开发了一种可解释的模型，它可以在计算成本最小的情况下，高效地针对未知的目标域进行微调。我们认为NN的可解释组件大致是域不变的。然而，可解释模型通常表现不及它们的BB变体。在源域中我们先使用人类理解的概念从BB开始，将其提炼成一组浅显易懂的interpretable模型。由于每个interpretable模型都覆盖了数据的一个子集，具有一组interpretable模型的混合可以实现与BB相当的性能。

    Building generalizable AI models is one of the primary challenges in the healthcare domain. While radiologists rely on generalizable descriptive rules of abnormality, Neural Network (NN) models suffer even with a slight shift in input distribution (\eg scanner type). Fine-tuning a model to transfer knowledge from one domain to another requires a significant amount of labeled data in the target domain. In this paper, we develop an interpretable model that can be efficiently fine-tuned to an unseen target domain with minimal computational cost. We assume the interpretable component of NN to be approximately domain-invariant. However, interpretable models typically underperform compared to their Blackbox (BB) variants. We start with a BB in the source domain and distill it into a \emph{mixture} of shallow interpretable models using human-understandable concepts. As each interpretable model covers a subset of data, a mixture of interpretable models achieves comparable performance as BB. Fu
    
[^16]: 稳定性惩罚自适应跟随正则化领袖：稀疏性、游戏依赖性和最佳世界的并存

    Stability-penalty-adaptive Follow-the-regularized-leader: Sparsity, Game-dependency, and Best-of-both-worlds. (arXiv:2305.17301v1 [cs.LG])

    [http://arxiv.org/abs/2305.17301](http://arxiv.org/abs/2305.17301)

    本文开发了一种稳定性惩罚自适应（SPA）学习率，该学习率使FTRL具有稀疏性、游戏依赖性和最佳世界（BOBW）三种适应性类型，其中SPA-sparse算法可适应于未知的稀疏级别，SPA-game-dependency算法可根据所玩的游戏自适应地改变其行为，BOBW算法则是既具有稀疏性又具有游戏依赖性的适应性算法。

    

    在顺序决策问题中，适应问题的困难程度是扩展算法适用性的关键属性。跟随正则化领袖近年来成为获取淘汰法中各种类型适应性的最有前途的方法之一。为了进一步推广这种适应性，我们为FTRL开发了一个通用的自适应学习率，称为稳定性惩罚自适应（SPA）学习率。该学习率产生的遗憾界共同取决于算法的稳定性和惩罚，其中FTRL的遗憾通常被分解。凭借这个结果，我们建立了几个具有三种适应性类型的算法：稀疏性、游戏依赖性和最佳世界（BOBW）。稀疏性经常出现在真实世界的问题中，但是，现有的稀疏多臂赌博算法$k$-arms假定事先已知稀疏级别$s \leq k$，而这在真实世界的情况下通常不是情况。为了适应未知的稀疏级别，我们提出了一种新算法SPA-sparse，该算法显示比现有稀疏算法的性能提高了。游戏依赖性是另一种适应性类型，当用于生成数据的游戏发生变化时，即必需的。我们提出了一种新算法SPA-game-dependency，该算法根据所玩的游戏自适应地改变其行为，并表明它比非自适应算法的性能更好。最后，我们提出了一个既具有稀疏性又具有游戏依赖性适应性的BOBW算法，并显示它比仅集中于一种适应性类型的算法表现更好。

    Adaptivity to the difficulties of a problem is a key property in sequential decision-making problems to broaden the applicability of algorithms. Follow-the-Regularized-Leader (FTRL) has recently emerged as one of the most promising approaches for obtaining various types of adaptivity in bandit problems. Aiming to further generalize this adaptivity, we develop a generic adaptive learning rate, called Stability-Penalty-Adaptive (SPA) learning rate for FTRL. This learning rate yields a regret bound jointly depending on stability and penalty of the algorithm, into which the regret of FTRL is typically decomposed. With this result, we establish several algorithms with three types of adaptivity: sparsity, game-dependency, and Best-of-Both-Worlds (BOBW). Sparsity frequently appears in real-world problems. However, existing sparse multi-armed bandit algorithms with $k$-arms assume that the sparsity level $s \leq k$ is known in advance, which is often not the case in real-world scenarios. To ad
    
[^17]: 利用大型神经影像数据集创造连接组限制方法，以实现更强大、更高效、更适应性强人工智能

    Exploiting Large Neuroimaging Datasets to Create Connectome-Constrained Approaches for more Robust, Efficient, and Adaptable Artificial Intelligence. (arXiv:2305.17300v1 [cs.NE])

    [http://arxiv.org/abs/2305.17300](http://arxiv.org/abs/2305.17300)

    该论文讨论了如何利用大型神经影像数据集改进机器学习方法，以创造更加稳健、高效和适应性强的人工智能。通过发现重复子电路和分析果蝇的航向方向电路，该论文提出了新的连接模式和模型，以探索如何进一步扩展现有的计算模型。

    

    尽管深度学习网络取得了进展，但边缘上的高效学习（实现适应性强、低复杂度的机器学习解决方案）仍然是国防和商业应用的关键需求。我们构想了使用大型神经影像数据集，包括捕获神经元和突触连接的大脑地图，来改善机器学习方法的流程。我们在该流程结构内追求了不同的方法。

    Despite the progress in deep learning networks, efficient learning at the edge (enabling adaptable, low-complexity machine learning solutions) remains a critical need for defense and commercial applications. We envision a pipeline to utilize large neuroimaging datasets, including maps of the brain which capture neuron and synapse connectivity, to improve machine learning approaches. We have pursued different approaches within this pipeline structure. First, as a demonstration of data-driven discovery, the team has developed a technique for discovery of repeated subcircuits, or motifs. These were incorporated into a neural architecture search approach to evolve network architectures. Second, we have conducted analysis of the heading direction circuit in the fruit fly, which performs fusion of visual and angular velocity features, to explore augmenting existing computational models with new insight. Our team discovered a novel pattern of connectivity, implemented a new model, and demonst
    
[^18]: 提高决策树模型的稳定性

    Improving Stability in Decision Tree Models. (arXiv:2305.17299v1 [stat.ML])

    [http://arxiv.org/abs/2305.17299](http://arxiv.org/abs/2305.17299)

    本文通过医疗应用的视角，提出了一种新的决策树距离度量，并用它来确定树的稳定水平。我们提出了一种新的培训稳定决策树的方法，并探究稳定性、预测能力和可解释性之间不可避免的权衡。

    

    由于其结构易于理解，决策树通常在需要可解释性的应用中被广泛使用。近期的工作集中于改进决策树的各个方面，包括预测能力和鲁棒性；然而，其不稳定性虽然有充分的记录，但却得到了较少的关注。本文通过实际的医疗应用的视角，提出了稳定化决策树模型的一小步。由于稳定性和可解释性在医疗领域具有重要性，我们介绍了一种新的决策树距离度量，并将其用于确定树的稳定水平。我们提出了一种新的培训稳定决策树的方法，并调查了决策树模型之间不可避免的权衡，包括在稳定性、预测能力和可解释性之间。我们通过对六个数据集的广泛定量和定性分析展示了所提议方法的价值。

    Owing to their inherently interpretable structure, decision trees are commonly used in applications where interpretability is essential. Recent work has focused on improving various aspects of decision trees, including their predictive power and robustness; however, their instability, albeit well-documented, has been addressed to a lesser extent. In this paper, we take a step towards the stabilization of decision tree models through the lens of real-world health care applications due to the relevance of stability and interpretability in this space. We introduce a new distance metric for decision trees and use it to determine a tree's level of stability. We propose a novel methodology to train stable decision trees and investigate the existence of trade-offs that are inherent to decision tree models - including between stability, predictive power, and interpretability. We demonstrate the value of the proposed methodology through an extensive quantitative and qualitative analysis of six 
    
[^19]: 无独立性的泛化误差：去噪、线性回归和迁移学习

    Generalization Error without Independence: Denoising, Linear Regression, and Transfer Learning. (arXiv:2305.17297v1 [cs.LG])

    [http://arxiv.org/abs/2305.17297](http://arxiv.org/abs/2305.17297)

    本论文研究了具有低秩结构但非独立同分布数据的情况，在分离训练和测试分布的假设下，解决了分布偏移问题，实验结果表明，在分布偏移的情况下，本方法显著提高了泛化误差的性能。

    

    研究线性模型在真实数据中的泛化能力是统计学习中的一个核心问题。先前的一些重要工作验证了理论工作与真实数据的相关性，但这些工作由于技术假设存在限制，这些假设包括具有良好条件的协方差矩阵以及具有独立同分布数据，这些假设在真实数据中并不一定成立。此外，以前的一些关于分布偏移的工作通常对训练和测试数据的联合分布进行技术假设，并且不在真实数据上进行测试。为了解决这些问题并更好地对真实数据进行建模，我们研究了具有低秩结构但非独立同分布数据的情况，同时通过分离训练和测试分布的假设来解决分布偏移问题。我们还在这些松弛的假设下，研究了去噪问题、线性回归和迁移学习。我们的实验结果表明，相比以前的方法，在分布偏移的情况下，我们的方法显著提高了泛化误差的性能。

    Studying the generalization abilities of linear models with real data is a central question in statistical learning. While there exist a limited number of prior important works (Loureiro et al. (2021A, 2021B), Wei et al. 2022) that do validate theoretical work with real data, these works have limitations due to technical assumptions. These assumptions include having a well-conditioned covariance matrix and having independent and identically distributed data. These assumptions are not necessarily valid for real data. Additionally, prior works that do address distributional shifts usually make technical assumptions on the joint distribution of the train and test data (Tripuraneni et al. 2021, Wu and Xu 2020), and do not test on real data.  In an attempt to address these issues and better model real data, we look at data that is not I.I.D. but has a low-rank structure. Further, we address distributional shift by decoupling assumptions on the training and test distribution. We provide anal
    
[^20]: Fourier-DeepONet: 增强傅里叶算子深度网络的完全波形反演，提高了精度，通用性和鲁棒性。

    Fourier-DeepONet: Fourier-enhanced deep operator networks for full waveform inversion with improved accuracy, generalizability, and robustness. (arXiv:2305.17289v1 [cs.LG])

    [http://arxiv.org/abs/2305.17289](http://arxiv.org/abs/2305.17289)

    该论文提出了Fourier-DeepONet算法，可用于完全波形反演，具有对地震源的泛化能力，可提高准确性和鲁棒性，适用于具有可变源的FWI。

    

    完全波形反演（FWI）是通过解决一个非凸优化问题从地震波形数据中推断地下结构信息的方法。数据驱动的FWI已经越来越被研究，使用不同的神经网络架构来提高准确性和计算效率。然而，预训练神经网络的适用性受到潜在的调查源函数和训练过程中使用的源函数之间的差异的严重限制。在这里，我们开发了一种傅里叶增强的深度算子网络（Fourier-DeepONet）用于全波形反演，并且具有地震源的泛化能力，包括地震源的频率和位置。具体而言，我们将傅里叶神经算子作为DeepONet的解码器，并利用源参数作为Fourier-DeepONet的其中一个输入，以实现对具有可变源的FWI的分辨率。为了测试Fourier-DeepONet，我们开发了两个新的逼真的FWI基准数据集（FWI-F和FWI-L），它们具有不同的源频率。

    Full waveform inversion (FWI) infers the subsurface structure information from seismic waveform data by solving a non-convex optimization problem. Data-driven FWI has been increasingly studied with various neural network architectures to improve accuracy and computational efficiency. Nevertheless, the applicability of pre-trained neural networks is severely restricted by potential discrepancies between the source function used in the field survey and the one utilized during training. Here, we develop a Fourier-enhanced deep operator network (Fourier-DeepONet) for FWI with the generalization of seismic sources, including the frequencies and locations of sources. Specifically, we employ the Fourier neural operator as the decoder of DeepONet, and we utilize source parameters as one input of Fourier-DeepONet, facilitating the resolution of FWI with variable sources. To test Fourier-DeepONet, we develop two new and realistic FWI benchmark datasets (FWI-F and FWI-L) with varying source frequ
    
[^21]: GC-Flow: 一种基于图的流网络用于有效聚类

    GC-Flow: A Graph-Based Flow Network for Effective Clustering. (arXiv:2305.17284v1 [cs.LG])

    [http://arxiv.org/abs/2305.17284](http://arxiv.org/abs/2305.17284)

    GC-Flow是一种生成模型，可以同时建模类别条件概率和类别先验，通过配备高斯混合表示空间，保持预测能力的同时实现了良好分离的聚类。

    

    图卷积网络（GCN）是直接建模半监督分类图数据类后验概率$p（y|\mathbf{x}）$的判别模型。虽然作为一种表示学习方法非常有效，但是从GCN中提取的节点表征常缺少有效聚类所需的有用信息，因为它们的目标不同。本研究设计了归一化流，用于替换GCN层，构建一种生成模型，同时建模类别条件概率$p(\mathbf{x}|y)$和类别先验$p(y)$。由此产生的神经网络GC-Flow保留了图卷积操作，同时配备了高斯混合表示空间。这有两个好处：它不仅保持了GCN的预测能力，还由于表示空间的结构而产生了良好分离的聚类。我们在各种基准数据集上展示了这些优势。此外，我们还展示了额外的参数化正则化优势和通用性。

    Graph convolutional networks (GCNs) are \emph{discriminative models} that directly model the class posterior $p(y|\mathbf{x})$ for semi-supervised classification of graph data. While being effective, as a representation learning approach, the node representations extracted from a GCN often miss useful information for effective clustering, because the objectives are different. In this work, we design normalizing flows that replace GCN layers, leading to a \emph{generative model} that models both the class conditional likelihood $p(\mathbf{x}|y)$ and the class prior $p(y)$. The resulting neural network, GC-Flow, retains the graph convolution operations while being equipped with a Gaussian mixture representation space. It enjoys two benefits: it not only maintains the predictive power of GCN, but also produces well-separated clusters, due to the structuring of the representation space. We demonstrate these benefits on a variety of benchmark data sets. Moreover, we show that additional par
    
[^22]: 优化退火算法的误差界和局部搜索策略

    Sharpened Lazy Incremental Quasi-Newton Method. (arXiv:2305.17283v1 [math.OC])

    [http://arxiv.org/abs/2305.17283](http://arxiv.org/abs/2305.17283)

    本文提出了一种新算法—— Sharpened Lazy Incremental Quasi-Newton (SLIQN) 方法，其具有显式的超线性收敛速率和$O(d^2)$的迭代复杂度。

    

    本文考虑了具有$Lipschitz$连续Hessian矩阵在$d$维空间中，$n$个强凸光滑函数的有限和最小化问题。在许多应用中，$n$的数量很大，因此必须使用每迭代一次与$n$无关的增量式或随机算法。本文提出了一种新算法—— Sharpened Lazy Incremental Quasi-Newton (SLIQN) 方法，其具有显式的超线性收敛速率和$O(d^2)$的迭代复杂度。

    We consider the finite sum minimization of $n$ strongly convex and smooth functions with Lipschitz continuous Hessians in $d$ dimensions. In many applications where such problems arise, including maximum likelihood estimation, empirical risk minimization, and unsupervised learning, the number of observations $n$ is large, and it becomes necessary to use incremental or stochastic algorithms whose per-iteration complexity is independent of $n$. Of these, the incremental/stochastic variants of the Newton method exhibit superlinear convergence, but incur a per-iteration complexity of $O(d^3)$, which may be prohibitive in large-scale settings. On the other hand, the incremental Quasi-Newton method incurs a per-iteration complexity of $O(d^2)$ but its superlinear convergence rate has only been characterized asymptotically. This work puts forth the Sharpened Lazy Incremental Quasi-Newton (SLIQN) method that achieves the best of both worlds: an explicit superlinear convergence rate with a per-
    
[^23]: 度量空间和Nagata维度中k-NN规则的普遍一致性(II)

    Universal consistency of the $k$-NN rule in metric spaces and Nagata dimension. II. (arXiv:2305.17282v1 [cs.LG])

    [http://arxiv.org/abs/2305.17282](http://arxiv.org/abs/2305.17282)

    本文研究了k近邻学习规则中的普遍一致性，发现在可分度量空间中，该规则在Nagata维度下的sigma有限维度的空间中是普遍一致的，在非阿基米德度量空间中是强普遍一致的，此规则在具有de Groot有限维度意义下的度量空间和Heisenberg群中也是普遍一致的。

    

    我们继续在可分度量空间中研究k近邻学习规则。由于C\'erou和Guyader(2006)以及Preiss(1983)的结果，已知该规则在每个Nagata意义下的sigma有限维度的度量空间X中是普遍一致的。在此，我们展示了在无平局情况下此规则在这样的空间中是强普遍一致的。在Devroye，Gy\"{o}rfi，Krzy\.{z}ak和Lugosi（1994）在欧几里得设置中应用的打破平局策略下，我们设法在非阿基米德度量空间（即Nagata维度为零的空间）中展示了强普遍一致性。结合C\'erou和Guyader的定理和Assouad和Quentin de Gromard (2006)的结果，可以推出$k$-NN规则在具有de Groot有限维度意义下的度量空间中是普遍一致的。特别地，$k$-NN规则在Heisenberg群中是普遍一致的，而该群并非sigma有限维度的。

    We continue to investigate the $k$ nearest neighbour learning rule in separable metric spaces. Thanks to the results of C\'erou and Guyader (2006) and Preiss (1983), this rule is known to be universally consistent in every metric space $X$ that is sigma-finite dimensional in the sense of Nagata. Here we show that the rule is strongly universally consistent in such spaces in the absence of ties. Under the tie-breaking strategy applied by Devroye, Gy\"{o}rfi, Krzy\.{z}ak, and Lugosi (1994) in the Euclidean setting, we manage to show the strong universal consistency in non-Archimedian metric spaces (that is, those of Nagata dimension zero). Combining the theorem of C\'erou and Guyader with results of Assouad and Quentin de Gromard (2006), one deduces that the $k$-NN rule is universally consistent in metric spaces having finite dimension in the sense of de Groot. In particular, the $k$-NN rule is universally consistent in the Heisenberg group which is not sigma-finite dimensional in the se
    
[^24]: 通过拓扑交换优化NOTEARS目标

    Optimizing NOTEARS Objectives via Topological Swaps. (arXiv:2305.17277v1 [stat.ML])

    [http://arxiv.org/abs/2305.17277](http://arxiv.org/abs/2305.17277)

    本文提出了一种双层算法来解决学习DAGs中的非凸优化问题，其中外层利用拓扑交换优化拓扑顺序，通过开发一种候选交换对的方法，算法在学习高质量DAGs方面具有高效和稳定的优势。

    

    最近，在学习有向无环图（DAGs）的背景下，出现了一类有趣的非凸优化问题。这些问题涉及到在给定损失或得分函数的情况下，最小化一个惩罚图中存在循环的非凸连续约束。在这项工作中，我们探讨了与这类非凸程序相关的优化挑战。为了解决这些问题，我们提出了一种双层算法，以新颖的方式利用非凸约束。算法的外层通过迭代地交换DAG的拓扑顺序中的节点对来优化拓扑顺序。我们方法的一个关键创新是，开发了一种有效的方法来为每次迭代生成一组候选交换对。在内层中，给定拓扑顺序，我们利用能够处理线性约束的现成求解器。我们所提出算法的主要优势是，它保证收敛到优化问题的一个稳定点，而现有方法可能会陷入亚最优解中。我们在合成和真实世界数据集上的实验证明了我们算法在学习高质量DAGs方面的有效性和效率。

    Recently, an intriguing class of non-convex optimization problems has emerged in the context of learning directed acyclic graphs (DAGs). These problems involve minimizing a given loss or score function, subject to a non-convex continuous constraint that penalizes the presence of cycles in a graph. In this work, we delve into the optimization challenges associated with this class of non-convex programs. To address these challenges, we propose a bi-level algorithm that leverages the non-convex constraint in a novel way. The outer level of the algorithm optimizes over topological orders by iteratively swapping pairs of nodes within the topological order of a DAG. A key innovation of our approach is the development of an effective method for generating a set of candidate swapping pairs for each iteration. At the inner level, given a topological order, we utilize off-the-shelf solvers that can handle linear constraints. The key advantage of our proposed algorithm is that it is guaranteed to
    
[^25]: 沿着部分曲率，梯度方法在零和博弈中局部收敛

    Local Convergence of Gradient Methods for Min-Max Games under Partial Curvature. (arXiv:2305.17275v1 [math.OC])

    [http://arxiv.org/abs/2305.17275](http://arxiv.org/abs/2305.17275)

    该研究探讨了博弈理论中梯度方法的局部收敛性，在部分曲率条件下保证了局部收敛，同时平均特征值比最小特征值更能体现其收敛速度。

    

    我们研究了零和可微分博弈的梯度方法对局部纳什均衡的收敛性。众所周知，当 $S \succ 0$ 时，这种动态会在局部收敛，而当 $S = 0$ 时可能会发散，其中 $S \succeq 0$ 是均衡时雅可比矩阵的对称部分，它占据了游戏的 “势能” 组件。我们表明，只要 $S$ 不为零（部分曲率），并且反对称部分 $A$ 的特征向量与 $S$ 的核相对位置良好，这些动态也会收敛。然后，我们研究了当 $S \ll A$ 时的收敛速度，并证明它们通常取决于 $S$ 的特征值的平均值，而不是最小值，这与最小化问题的类比所建议的相反。为了说明我们的结果，我们考虑计算连续博弈的混合纳什均衡问题。我们表明，由于部分曲率，锥形粒子方法——它在混合策略的权重和支持上进行优化——呈现出一致的局部收敛性，而标准的对偶梯度方法则可能会发散。

    We study the convergence to local Nash equilibria of gradient methods for two-player zero-sum differentiable games. It is well-known that such dynamics converge locally when $S \succ 0$ and may diverge when $S=0$, where $S\succeq 0$ is the symmetric part of the Jacobian at equilibrium that accounts for the "potential" component of the game. We show that these dynamics also converge as soon as $S$ is nonzero (partial curvature) and the eigenvectors of the antisymmetric part $A$ are in general position with respect to the kernel of $S$. We then study the convergence rates when $S \ll A$ and prove that they typically depend on the average of the eigenvalues of $S$, instead of the minimum as an analogy with minimization problems would suggest. To illustrate our results, we consider the problem of computing mixed Nash equilibria of continuous games. We show that, thanks to partial curvature, conic particle methods -- which optimize over both weights and supports of the mixed strategies -- g
    
[^26]: 通过自预训练掩模序列自编码器和使用定制PolyLoss微调的方法实现鲁棒车道检测

    Robust Lane Detection through Self Pre-training with Masked Sequential Autoencoders and Fine-tuning with Customized PolyLoss. (arXiv:2305.17271v1 [cs.CV])

    [http://arxiv.org/abs/2305.17271](http://arxiv.org/abs/2305.17271)

    本论文提出了一种鲁棒车道检测流水线，该流水线包括自预训练掩模序列自编码器和使用定制PolyLoss微调的端到端神经网络模型。掩模序列自编码器被采用以通过重构随机掩膜图像中的丢失像素为目标来预训练神经网络模型，提升了车道检测性能。

    

    车道检测是车辆定位的关键，是实现自动驾驶和许多智能高级驾驶辅助系统的基础。现有的基于视觉的车道检测方法未充分利用有价值的特征和聚合的上下文信息，尤其是车道线和图像中其他区域之间的相互关系。为填补这一研究空白并提升车道检测性能，本文提出了一种流水线，其中包括使用自预训练掩模序列自编码器和使用定制PolyLoss微调的端到端神经网络模型。掩模序列自编码器被采用以通过重构随机掩模图像中的丢失像素为目标来预训练神经网络模型。然后，在细调分割阶段中，连续的图像帧被用作输入，

    Lane detection is crucial for vehicle localization which makes it the foundation for automated driving and many intelligent and advanced driving assistant systems. Available vision-based lane detection methods do not make full use of the valuable features and aggregate contextual information, especially the interrelationships between lane lines and other regions of the images in continuous frames. To fill this research gap and upgrade lane detection performance, this paper proposes a pipeline consisting of self pre-training with masked sequential autoencoders and fine-tuning with customized PolyLoss for the end-to-end neural network models using multi-continuous image frames. The masked sequential autoencoders are adopted to pre-train the neural network models with reconstructing the missing pixels from a random masked image as the objective. Then, in the fine-tuning segmentation phase where lane detection segmentation is performed, the continuous image frames are served as the inputs,
    
[^27]: 机器学习和人工智能协作关闭高危孕产妇护理差距

    Closing the Gap in High-Risk Pregnancy Care Using Machine Learning and Human-AI Collaboration. (arXiv:2305.17261v1 [cs.LG])

    [http://arxiv.org/abs/2305.17261](http://arxiv.org/abs/2305.17261)

    本论文介绍了一种基于机器学习和人工智能协作的高危孕产妇计划，提出了早期妊娠检测、准确识别高风险会员和提供可解释指标等三个挑战的解决方案，提高了孕期风险的预测准确率。

    

    健康保险公司通常使用算法来识别会受益于护理和状况管理计划的会员，该计划提供个性化的高端临床支持。算法识别与临床干预之间的及时、准确和无缝集成取决于系统设计师和护理管理员之间的有效协作。我们关注了一个旨在减少孕产妇不良产前、产期和产后事件的高危孕产妇计划，并描述了我们如何克服护理管理员所述的三个HRP计划的挑战：（1）早期检测妊娠，（2）准确识别有影响力的高风险会员，以及（3）提供可解释的指标来补充预测。我们提出了一种新的孕期识别算法，在回顾性研究中比之前基于代码的模型提前了57天识别妊娠。然后我们建立了一个模型来预测会影响孕期的并发症。

    Health insurers often use algorithms to identify members who would benefit from care and condition management programs, which provide personalized, high-touch clinical support. Timely, accurate, and seamless integration between algorithmic identification and clinical intervention depends on effective collaboration between the system designers and nurse care managers. We focus on a high-risk pregnancy (HRP) program designed to reduce the likelihood of adverse prenatal, perinatal, and postnatal events and describe how we overcome three challenges of HRP programs as articulated by nurse care managers; (1) early detection of pregnancy, (2) accurate identification of impactable high-risk members, and (3) provision of explainable indicators to supplement predictions. We propose a novel algorithm for pregnancy identification that identifies pregnancies 57 days earlier than previous code-based models in a retrospective study. We then build a model to predict impactable pregnancy complications 
    
[^28]: 大型语言模型可能是懒惰的学习者：分析上下文学习中的捷径

    Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning. (arXiv:2305.17256v1 [cs.CL])

    [http://arxiv.org/abs/2305.17256](http://arxiv.org/abs/2305.17256)

    本文探讨了大型语言模型在上下文学习中利用提示中的捷径的依赖性，发现大型模型更有可能在推理过程中利用提示中的捷径，这为评估上下文学习的稳健性和检测和缓解提示中捷径的使用提供了新的视角和挑战。

    

    最近，大型语言模型（LLM）在上下文学习中展现出巨大潜力，其中LLM通过几个输入-标签对（提示）的条件来学习新任务。尽管其潜力巨大，但我们对影响最终任务性能和上下文学习稳健性的因素的理解仍然有限。本文旨在通过研究LLM对提示内捷径或假相关的依赖关系来弥补这一知识差距。通过分类和抽取任务的全面实验，我们揭示了LLM是“懒惰学习者”的事实，它往往利用提示中的捷径来获取下游任务的性能提升。此外，我们还发现一个令人惊讶的发现，即较大的模型更有可能在推理过程中利用提示中的捷径。我们的发现为评估上下文学习的稳健性和检测和缓解提示中捷径的使用提供了新的视角和挑战。

    Large language models (LLMs) have recently shown great potential for in-context learning, where LLMs learn a new task simply by conditioning on a few input-label pairs (prompts). Despite their potential, our understanding of the factors influencing end-task performance and the robustness of in-context learning remains limited. This paper aims to bridge this knowledge gap by investigating the reliance of LLMs on shortcuts or spurious correlations within prompts. Through comprehensive experiments on classification and extraction tasks, we reveal that LLMs are "lazy learners" that tend to exploit shortcuts in prompts for downstream tasks. Additionally, we uncover a surprising finding that larger models are more likely to utilize shortcuts in prompts during inference. Our findings provide a new perspective on evaluating robustness in in-context learning and pose new challenges for detecting and mitigating the use of shortcuts in prompts.
    
[^29]: FineMorphs:用于回归的仿射-微分同胚序列模型

    FineMorphs: Affine-diffeomorphic sequences for regression. (arXiv:2305.17255v1 [stat.ML])

    [http://arxiv.org/abs/2305.17255](http://arxiv.org/abs/2305.17255)

    FineMorphs是一种多元回归模型，通过形状分析中的微分同胚概念对模型状态进行优化，能够自然地减少（或增加）维度并适应大数据集。

    

    本文提出了一种仿射和微分同胚变换序列的多元回归模型FineMorphs。该模型利用形状分析的概念，在学习期间通过由光滑向量场生成的微分同胚优化地“重塑”模型状态。仿射变换和向量场在最优控制环境中进行优化，该模型可以通过次优向量场自然地减少（或增加）维度并适应大数据集。我们推导了该模型的解存在性证明和最优性的必要条件。在真实数据集上进行的实验结果表明，FineMorphs在与文献中最先进和基于TensorFlow的稠密连接神经网络的比较中，取得了有利的结果。

    A multivariate regression model of affine and diffeomorphic transformation sequences - FineMorphs - is presented. Leveraging concepts from shape analysis, model states are optimally "reshaped" by diffeomorphisms generated by smooth vector fields during learning. Affine transformations and vector fields are optimized within an optimal control setting, and the model can naturally reduce (or increase) dimensionality and adapt to large datasets via suboptimal vector fields. An existence proof of solution and necessary conditions for optimality for the model are derived. Experimental results on real datasets from the UCI repository are presented, with favorable results in comparison with state-of-the-art in the literature and densely-connected neural networks in TensorFlow.
    
[^30]: 多角度受限制内核机器的对偶性

    Duality in Multi-View Restricted Kernel Machines. (arXiv:2305.17251v1 [cs.LG])

    [http://arxiv.org/abs/2305.17251](http://arxiv.org/abs/2305.17251)

    该论文提出了一个统一的框架，将现有的受限制内核机器方法结合成一个单一的原始-对偶多角度框架，可用于核主成分分析，实现了原始和对偶公式的完全等价性，并最终在时间序列数据集上验证了其等价性和提供的洞察。

    

    我们提出了一个统一的框架，将现有的受限制内核机器方法结合成一个单一的原始-对偶多角度框架，用于核主成分分析，可以用于有监督和无监督学习。我们导出了该框架的原始和对偶表示，并从理论角度关联了不同的训练和推理算法。我们展示了如何通过重新调整原始变量来实现原始和对偶公式的完全等价性。最后，我们通过递归预测未看到测试数据并可视化所学特征的方式，在许多时间序列数据集上实验验证了等价性并提供了对不同方法之间关系的洞察。

    We propose a unifying setting that combines existing restricted kernel machine methods into a single primal-dual multi-view framework for kernel principal component analysis in both supervised and unsupervised settings. We derive the primal and dual representations of the framework and relate different training and inference algorithms from a theoretical perspective. We show how to achieve full equivalence in primal and dual formulations by rescaling primal variables. Finally, we experimentally validate the equivalence and provide insight into the relationships between different methods on a number of time series data sets by recursively forecasting unseen test data and visualizing the learned features.
    
[^31]: 基于随机特征的自监督增强学习实现迁移

    Self-Supervised Reinforcement Learning that Transfers using Random Features. (arXiv:2305.17250v1 [cs.LG])

    [http://arxiv.org/abs/2305.17250](http://arxiv.org/abs/2305.17250)

    该论文提出了一种自监督增强学习方法，能够在不同奖励的任务间进行行为迁移，同时避免有模型强化学习的挑战。使用一些随机特征作为奖励，进行自监督预训练能够暗含长期环境动态模型，然后使用这些隐式模型的规划技术能够在短时间内适应问题。

    

    无模型强化学习算法在解决具有高维观测和长期决策方案的单任务顺序决策问题方面表现出巨大潜力，但难以横跨任务进行泛化。相比之下，有模型强化学习则学习与任务无关的世界模型，自然地实现了不同奖励函数的迁移，但由于误差的累积而难以适应复杂的环境。为了实现两者兼顾，我们提出了一种自监督增强学习方法，能够实现在具有不同奖励的任务间进行行为迁移，同时避开有模型强化学习的挑战。特别地，我们展示了模型自由强化学习的自监督预训练，用一些随机特征作为奖励，能够暗含长期环境动态模型。然后，使用这些隐式模型的规划技术（如模型预测控制）能够在短时间内适应问题。

    Model-free reinforcement learning algorithms have exhibited great potential in solving single-task sequential decision-making problems with high-dimensional observations and long horizons, but are known to be hard to generalize across tasks. Model-based RL, on the other hand, learns task-agnostic models of the world that naturally enables transfer across different reward functions, but struggles to scale to complex environments due to the compounding error. To get the best of both worlds, we propose a self-supervised reinforcement learning method that enables the transfer of behaviors across tasks with different rewards, while circumventing the challenges of model-based RL. In particular, we show self-supervised pre-training of model-free reinforcement learning with a number of random features as rewards allows implicit modeling of long-horizon environment dynamics. Then, planning techniques like model-predictive control using these implicit models enable fast adaptation to problems wi
    
[^32]: NASimEmu: 用于训练能够推广到新场景的智能体的网络攻击模拟器与仿真器

    NASimEmu: Network Attack Simulator & Emulator for Training Agents Generalizing to Novel Scenarios. (arXiv:2305.17246v1 [cs.CR])

    [http://arxiv.org/abs/2305.17246](http://arxiv.org/abs/2305.17246)

    该论文提出了一个名为NASimEmu的新框架，旨在提高智能体在现实世界中表现良好的能力。该框架使用模拟器和仿真器的结合，使智能体能够在模拟中进行训练，并在仿真器中部署，从而验证所使用的抽象的真实性。该框架的设计旨在培训通用的智能体，能够在训练期间未见过的新场景中进行转移。

    

    当前用于使用深度强化学习培训攻击型渗透测试智能体的框架往往难以使智能体在现实世界中表现良好，原因在于基于模拟的框架中存在现实差距，而基于仿真的框架缺乏可扩展性。此外，现有的框架通常使用不现实的度量标准来衡量智能体在训练数据上的表现。本文介绍了一个名为NASimEmu的新框架，通过提供具有共享接口的模拟器和仿真器来解决这些问题。这种方法允许智能体在模拟中进行训练，并在仿真器中部署，从而验证所使用的抽象的真实性。我们的框架促进了能够在训练期间未见过的新场景中进行转移的通用智能体的开发。对于模拟部分，我们采用了一个现有的模拟器NASim并增强了其实现。仿真器使用行业级工具实施，如Vagrant、VirtualBox和Metasp。

    Current frameworks for training offensive penetration testing agents with deep reinforcement learning struggle to produce agents that perform well in real-world scenarios, due to the reality gap in simulation-based frameworks and the lack of scalability in emulation-based frameworks. Additionally, existing frameworks often use an unrealistic metric that measures the agents' performance on the training data. NASimEmu, a new framework introduced in this paper, addresses these issues by providing both a simulator and an emulator with a shared interface. This approach allows agents to be trained in simulation and deployed in the emulator, thus verifying the realism of the used abstraction. Our framework promotes the development of general agents that can transfer to novel scenarios unseen during their training. For the simulation part, we adopt an existing simulator NASim and enhance its realism. The emulator is implemented with industry-level tools, such as Vagrant, VirtualBox, and Metasp
    
[^33]: 因果成分分析

    Causal Component Analysis. (arXiv:2305.17225v1 [stat.ML])

    [http://arxiv.org/abs/2305.17225](http://arxiv.org/abs/2305.17225)

    本文介绍了一个中间问题：因果成分分析(CauCA)，它是独立成分分析(ICA)和因果表示学习(CRL)的泛化和特例，其目标是学习解混函数和因果机制，预设了因果图的知识。

    

    独立成分分析(ICA)的目标是从混合观测到的变量中恢复独立的潜在变量。而因果表示学习(CRL)的目标是推断因果关系强相关性的潜在变量，以及编码它们的因果关系的未知图。我们引入了一个中间问题，称为因果成分分析(CauCA)。CauCA可以被看作是ICA的一种推广，对潜在成分之间的因果依赖建模，也是CRL的一个特例。与CRL不同的是，它预设了因果图的知识，仅关注于学习解混函数和因果机制。所有关于CauCA回收基础真相的不可能结果也适用于CRL，而可能性结果可以作为扩展CRL的基础。我们将从对潜在因果变量实施不同类型干预的多个数据集中表征CauCA的可识别性。

    Independent Component Analysis (ICA) aims to recover independent latent variables from observed mixtures thereof. Causal Representation Learning (CRL) aims instead to infer causally related (thus often statistically dependent) latent variables, together with the unknown graph encoding their causal relationships. We introduce an intermediate problem termed Causal Component Analysis (CauCA). CauCA can be viewed as a generalization of ICA, modelling the causal dependence among the latent components, and as a special case of CRL. In contrast to CRL, it presupposes knowledge of the causal graph, focusing solely on learning the unmixing function and the causal mechanisms. Any impossibility results regarding the recovery of the ground truth in CauCA also apply for CRL, while possibility results may serve as a stepping stone for extensions to CRL. We characterize CauCA identifiability from multiple datasets generated through different types of interventions on the latent causal variables. As a
    
[^34]: 非凸梯度下降法快速极小化低秩矩阵估计

    Fast and Minimax Optimal Estimation of Low-Rank Matrices via Non-Convex Gradient Descent. (arXiv:2305.17224v1 [math.OC])

    [http://arxiv.org/abs/2305.17224](http://arxiv.org/abs/2305.17224)

    本文提出一种针对低秩矩阵估计的方法，在保证极小极值优化性能的同时，解决了非凸梯度下降收敛缓慢的问题。

    

    本文研究了从噪声测量中估计低秩矩阵的问题，特别是旨在实现极小极值误差。在实践中，由于非凸梯度下降的能力可以扩展到大规模真实世界数据集，这个问题通常使用非凸梯度下降来解决。理论上，非凸梯度下降能够实现极小极值误差。但在实践中，它经常收敛得非常缓慢，以至于甚至无法在合理的时间内提供适度准确的估计值。另一方面，通过重新缩放或预处理改进非凸梯度下降的收敛方法也会大大放大测量误差，导致得到的估计比理论上可实现的极小极值误差少几个数量级的准确性。在本文中，我们提出了一种对通常的非凸梯度下降方法进行轻微修改的方法，解决了收敛缓慢的问题，同时可证明保留其极小极值优化性能。

    We study the problem of estimating a low-rank matrix from noisy measurements, with the specific goal of achieving minimax optimal error. In practice, the problem is commonly solved using non-convex gradient descent, due to its ability to scale to large-scale real-world datasets. In theory, non-convex gradient descent is capable of achieving minimax error. But in practice, it often converges extremely slowly, such that it cannot even deliver estimations of modest accuracy within reasonable time. On the other hand, methods that improve the convergence of non-convex gradient descent, through rescaling or preconditioning, also greatly amplify the measurement noise, resulting in estimations that are orders of magnitude less accurate than what is theoretically achievable with minimax optimal error. In this paper, we propose a slight modification to the usual non-convex gradient descent method that remedies the issue of slow convergence, while provably preserving its minimax optimality. Our p
    
[^35]: 基于联邦学习的语义解析任务：任务形式，评估设置及新算法

    Federated Learning for Semantic Parsing: Task Formulation, Evaluation Setup, New Algorithms. (arXiv:2305.17221v1 [cs.CL])

    [http://arxiv.org/abs/2305.17221](http://arxiv.org/abs/2305.17221)

    本文研究了基于联邦学习的语义解析任务，提出了评估设置和新算法。实验表明，新算法FedSQL和Lorar优于现有的FL算法和我们提出的设置的强基线。

    

    本文研究了一种新的联邦学习任务，即针对语义解析的联邦学习，多个客户端共同训练一个全局模型，而无需共享其语义分析数据。通过利用多个客户端的数据，联邦学习模式对于那些没有足够训练数据来开发一个数据饥饿的神经语义分析器的客户端尤其有益。我们提出了一种评估设置来研究这个任务，将广泛使用的单域文本到SQL数据集作为客户端来形成一个现实的异构联邦学习设置，并协同训练一个全局模型。由于我们的现实设置中客户群的异质性很高，标准的联邦学习算法会受到影响，所以我们进一步提出了一种新的机制LOss Reduction Adjusted Re-weighting (Lorar)来缓解性能下降，该机制基于客户端每轮训练损失的减少情况来调节每个客户端对于全局模型更新的贡献。我们的直觉是，损失减少的越多，客户端离全局最优解就越远，其对模型更新的贡献就应该越高。同时，我们还提出了一个针对异构文本到SQL FL设置的新的FL算法FedSQL。我们的实验表明，FedSQL和Lorar显著优于现有的FL算法和我们提出的FL设置中的强基线。

    This paper studies a new task of federated learning (FL) for semantic parsing, where multiple clients collaboratively train one global model without sharing their semantic parsing data. By leveraging data from multiple clients, the FL paradigm can be especially beneficial for clients that have little training data to develop a data-hungry neural semantic parser on their own. We propose an evaluation setup to study this task, where we re-purpose widely-used single-domain text-to-SQL datasets as clients to form a realistic heterogeneous FL setting and collaboratively train a global model. As standard FL algorithms suffer from the high client heterogeneity in our realistic setup, we further propose a novel LOss Reduction Adjusted Re-weighting (Lorar) mechanism to mitigate the performance degradation, which adjusts each client's contribution to the global model update based on its training loss reduction during each round. Our intuition is that the larger the loss reduction, the further aw
    
[^36]: GVdoc: 基于图的视觉文档分类

    GVdoc: Graph-based Visual Document Classification. (arXiv:2305.17219v1 [cs.CV])

    [http://arxiv.org/abs/2305.17219](http://arxiv.org/abs/2305.17219)

    GVdoc 是一个基于图的文档分类模型，能够通过生成文档图并训练图神经网络来学习节点和图嵌入，有效解决视觉文档分类器在领域内外样本分类和区分中所遇到的挑战。

    

    模型在实际部署中的鲁棒性取决于其在未见过的数据上的表现和对领域内外样本的区分能力。视觉文档分类器在分布测试集上表现出色。然而，它们在正确分类和区分领域外例子方面往往遇到困难。基于图的文档分类模型通过生成基于布局的文档图，然后训练图神经网络来学习节点和图嵌入，解决了这两个挑战。我们的模型即使参数更少，在领域外的样本上也优于现有模型，在领域内基准上实现了最新的性能。

    The robustness of a model for real-world deployment is decided by how well it performs on unseen data and distinguishes between in-domain and out-of-domain samples. Visual document classifiers have shown impressive performance on in-distribution test sets. However, they tend to have a hard time correctly classifying and differentiating out-of-distribution examples. Image-based classifiers lack the text component, whereas multi-modality transformer-based models face the token serialization problem in visual documents due to their diverse layouts. They also require a lot of computing power during inference, making them impractical for many real-world applications. We propose, GVdoc, a graph-based document classification model that addresses both of these challenges. Our approach generates a document graph based on its layout, and then trains a graph neural network to learn node and graph embeddings. Through experiments, we show that our model, even with fewer parameters, outperforms stat
    
[^37]: 用多模态语言模型生成图片

    Generating Images with Multimodal Language Models. (arXiv:2305.17216v1 [cs.CL])

    [http://arxiv.org/abs/2305.17216](http://arxiv.org/abs/2305.17216)

    该论文提出了一种方法，将大型语言模型与预训练的图像编码器和解码器模型进行融合，能生成具有连贯性的图像输出，同时也能进行图像检索和多模态对话。

    

    我们提出了一种方法，将仅包含文本的大型语言模型（LLMs）与预训练的图像编码器和解码器模型进行融合，通过映射它们的嵌入空间。我们的模型展示了广泛的多模态能力：图像检索、新颖图像生成和多模态对话。这是第一种能够在任意交错的图像和文本输入之间进行条件调节，生成连贯图像（和文本）输出的方法。为了在图像生成任务中取得强大的性能，我们提出了一种有效的映射网络，将LLM基于现成的文本到图像生成模型，将文本的隐藏表示转换为视觉模型的嵌入空间，利用LLM强大的文本表示来生成视觉输出。我们的方法在长且复杂语言的任务上优于基准生成模型。除了新颖图像生成之外，我们的模型还能够从文本描述中检索图像，并进行多模态对话。

    We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespe
    
[^38]: 旋转优化器：简单而强健的深度神经网络训练。

    Rotational Optimizers: Simple & Robust DNN Training. (arXiv:2305.17212v1 [cs.LG])

    [http://arxiv.org/abs/2305.17212](http://arxiv.org/abs/2305.17212)

    该论文提出了旋转优化器，这些优化器可以简化深度神经网络训练过程，甚至在几乎不需调整基线超参数的情况下与原始优化器的性能相匹配。

    

    现代深度神经网络的训练动态取决于学习率、权重衰减、初始化等超参数之间的复杂交互作用。这些交互作用可以在尺度不变层（如归一化层）中产生球面运动动态，这些动态收敛到平衡状态，其中权重范数和预期旋转更新大小是固定的。我们对AdamW、带动量的SGD和Lion中的这个平衡进行了分析，提供了关于不同超参数及其相互作用对训练过程的影响的新见解。我们提出了这些优化器的旋转变体（RVs），强制预期角度更新大小与整个训练期间的平衡值相匹配。这简化了训练动态，通过消除收敛到平衡状态的瞬态相应。我们的旋转优化器可以匹配原始变体的性能，通常需要对基线超参数进行最少或不调整。

    The training dynamics of modern deep neural networks depend on complex interactions between the learning rate, weight decay, initialization, and other hyperparameters. These interactions can give rise to Spherical Motion Dynamics in scale-invariant layers (e.g., normalized layers), which converge to an equilibrium state, where the weight norm and the expected rotational update size are fixed. Our analysis of this equilibrium in AdamW, SGD with momentum, and Lion provides new insights into the effects of different hyperparameters and their interactions on the training process. We propose rotational variants (RVs) of these optimizers that force the expected angular update size to match the equilibrium value throughout training. This simplifies the training dynamics by removing the transient phase corresponding to the convergence to an equilibrium. Our rotational optimizers can match the performance of the original variants, often with minimal or no tuning of the baseline hyperparameters,
    
[^39]: 功能流匹配

    Functional Flow Matching. (arXiv:2305.17209v1 [cs.LG])

    [http://arxiv.org/abs/2305.17209](http://arxiv.org/abs/2305.17209)

    本文介绍了一种名为功能流匹配（FFM）的函数空间生成模型，该模型利用概率测度插值和学习底层函数空间上生成测度的向量场来生成数据分布。这种无需似然或模拟的方法在合成和真实世界基准数据集上表现优异，优于最近提出的几种函数空间生成模型。

    

    本文提出了一种名为功能流匹配（Functional Flow Matching, FFM）的函数空间生成模型，该模型将最近引入的流匹配（Flow Matching）直接推广到无限维空间中进行。我们的方法首先定义了一组概率测度路径，在固定的高斯测度和数据分布之间进行插值，然后学习函数的底层空间上生成此测度路径的向量场。我们的方法不依赖于似然或模拟，因此非常适合函数空间的设置。我们不仅提供构建这种模型的理论框架，还对我们的技术进行了经验评估。通过对合成和真实世界基准数据集的实验，我们证明了我们提出的FFM方法优于最近提出的几种函数空间生成模型。

    In this work, we propose Functional Flow Matching (FFM), a function-space generative model that generalizes the recently-introduced Flow Matching model to operate directly in infinite-dimensional spaces. Our approach works by first defining a path of probability measures that interpolates between a fixed Gaussian measure and the data distribution, followed by learning a vector field on the underlying space of functions that generates this path of measures. Our method does not rely on likelihoods or simulations, making it well-suited to the function space setting. We provide both a theoretical framework for building such models and an empirical evaluation of our techniques. We demonstrate through experiments on synthetic and real-world benchmarks that our proposed FFM method outperforms several recently proposed function-space generative models.
    
[^40]: 正则化深度神经网络的幽灵噪声

    Ghost Noise for Regularizing Deep Neural Networks. (arXiv:2305.17205v1 [cs.LG])

    [http://arxiv.org/abs/2305.17205](http://arxiv.org/abs/2305.17205)

    本文研究了幽灵批量归一化（GBN）中的“幽灵噪声”，提出了一种新的正则化技术Ghost Noise Injection (GNI)，该方法能够避免小批量训练带来的训练-测试差异效应，并在深度神经网络中提供更好的泛化效果。

    

    批量归一化（BN）被广泛用于稳定深度神经网络的优化过程并提高测试性能。BN的正则化效果取决于批量大小，显式使用较小的批量大小会通过批量归一化提高泛化性，在许多设置中都具有很好的应用。本文通过将引入的“幽灵噪声”与归一化进行分离，并定量分析噪声的分布及其对模型性能的影响来研究GBN的有效性。受我们分析的启发，我们提出了一种新的正则化技术称为Ghost Noise Injection (GNI)，模仿了GBN中的噪声，而避免了小批量训练带来的有害的训练-测试差异效应。实验证明GNI可以比GBN提供更好的泛化效益。Ghost Noise Injection在其他非噪声设置中，例如层归一化网络也能够产生积极作用。

    Batch Normalization (BN) is widely used to stabilize the optimization process and improve the test performance of deep neural networks. The regularization effect of BN depends on the batch size and explicitly using smaller batch sizes with Batch Normalization, a method known as Ghost Batch Normalization (GBN), has been found to improve generalization in many settings. We investigate the effectiveness of GBN by disentangling the induced "Ghost Noise" from normalization and quantitatively analyzing the distribution of noise as well as its impact on model performance. Inspired by our analysis, we propose a new regularization technique called Ghost Noise Injection (GNI) that imitates the noise in GBN without incurring the detrimental train-test discrepancy effects of small batch training. We experimentally show that GNI can provide a greater generalization benefit than GBN. Ghost Noise Injection can also be beneficial in otherwise non-noisy settings such as layer-normalized networks, provi
    
[^41]: 基于 Trend 和 Seasonality 分解和 LightGBM 的销售预测改进

    Improved Sales Forecasting using Trend and Seasonality Decomposition with LightGBM. (arXiv:2305.17201v1 [cs.LG])

    [http://arxiv.org/abs/2305.17201](http://arxiv.org/abs/2305.17201)

    本文提出了一种根据趋势和季节性分量在时间序列上的独特影响指标进行时间序列分组的新方法，并采用 LightGBM 模型进行预测，在沃尔玛销售数据上实现了较高的预测精度。

    

    针对沃尔玛和亚马逊等大型零售商销售预测的难点，本文提出了一种新的方法，即根据趋势和季节性分量在时间序列上的独特影响指标进行时间序列分组，并采用 LightGBM 模型进行预测。实验结果表明，该分组方法可以提高预测精度，相较于传统时间序列模型和其他机器学习模型，MAPE（平均绝对百分比误差）在测试集上可达 4.49%。

    Retail sales forecasting presents a significant challenge for large retailers such as Walmart and Amazon, due to the vast assortment of products, geographical location heterogeneity, seasonality, and external factors including weather, local economic conditions, and geopolitical events. Various methods have been employed to tackle this challenge, including traditional time series models, machine learning models, and neural network mechanisms, but the difficulty persists. Categorizing data into relevant groups has been shown to improve sales forecast accuracy as time series from different categories may exhibit distinct patterns. In this paper, we propose a new measure to indicate the unique impacts of the trend and seasonality components on a time series and suggest grouping time series based on this measure. We apply this approach to Walmart sales data from 01/29/2011 to 05/22/2016 and generate sales forecasts from 05/23/2016 to 06/19/2016. Our experiments show that the proposed strat
    
[^42]: 离线多智能体强化学习协调问题的基于模型的解决方案

    A Model-Based Solution to the Offline Multi-Agent Reinforcement Learning Coordination Problem. (arXiv:2305.17198v1 [cs.LG])

    [http://arxiv.org/abs/2305.17198](http://arxiv.org/abs/2305.17198)

    提出了一个基于模型的离线多智能体强化学习方法MOMA-PPO，通过生成合成交互数据并优化智能体的政策，解决了策略一致性和策略微调两个协调问题，在具有挑战性的离线MARL场景中胜过主流的学习方法，提供了实际应用中的可行解决方案。

    

    训练多个智能体进行协调是一项重要问题，具有机器人技术、博弈论、经济学和社会科学等领域的应用。然而，大多数现有的多智能体强化学习方法是在线的，因此在收集新的交互数据成本高昂或危险的实际应用中不可行。虽然这些算法应该利用离线数据，但这样做会引起离线协调问题。具体而言，我们确定并形式化了策略一致性（SA）和策略微调（SFT）两个协调问题，这是当前离线多智能体强化学习算法失败的原因。为解决这个问题，我们提出了一种简单的基于模型的方法，生成合成交互数据，使智能体能够在微调策略的同时收敛于一个策略。我们提出的方法，Model-based Offline Multi-Agent Proximal Policy Optimization（MOMA-PPO），在具有挑战性的离线MARL场景中胜过主流的学习方法，证明了基于模型的方法提供了一个可行的解决方案。

    Training multiple agents to coordinate is an important problem with applications in robotics, game theory, economics, and social sciences. However, most existing Multi-Agent Reinforcement Learning (MARL) methods are online and thus impractical for real-world applications in which collecting new interactions is costly or dangerous. While these algorithms should leverage offline data when available, doing so gives rise to the offline coordination problem. Specifically, we identify and formalize the strategy agreement (SA) and the strategy fine-tuning (SFT) challenges, two coordination issues at which current offline MARL algorithms fail. To address this setback, we propose a simple model-based approach that generates synthetic interaction data and enables agents to converge on a strategy while fine-tuning their policies accordingly. Our resulting method, Model-based Offline Multi-Agent Proximal Policy Optimization (MOMA-PPO), outperforms the prevalent learning methods in challenging offl
    
[^43]: 基于AI的超分辨显微镜分析：在没有基准的情况下进行生物发现

    AI-based analysis of super-resolution microscopy: Biological discovery in the absence of ground truth. (arXiv:2305.17193v1 [q-bio.SC])

    [http://arxiv.org/abs/2305.17193](http://arxiv.org/abs/2305.17193)

    利用弱监督学习范例对超分辨率显微镜进行分析具有发现新生物学的巨大潜力，并且可以加速探索亚细胞大分子和细胞器分子结构。

    

    超分辨显微镜的纳米级分辨率现已使荧光分子定位工具能够用于研究整个细胞结构生物学。基于机器学习的超分辨数据分析具有发现新生物学的巨大潜力，而新生物学本身没有被发现过，也没有基准。在这里，我们描述了弱监督学习范例在超分辨显微镜中的应用以及其潜力，以实现对亚细胞大分子和细胞器分子结构的加速探索。

    The nanoscale resolution of super-resolution microscopy has now enabled the use of fluorescent based molecular localization tools to study whole cell structural biology. Machine learning based analysis of super-resolution data offers tremendous potential for discovery of new biology, that by definition is not known and lacks ground truth. Herein, we describe the application of weakly supervised learning paradigms to super-resolution microscopy and its potential to enable the accelerated exploration of the molecular architecture of subcellular macromolecules and organelles.
    
[^44]: MT-SLVR: 多任务自监督学习用于变换表示中的特征学习

    MT-SLVR: Multi-Task Self-Supervised Learning for Transformation In(Variant) Representations. (arXiv:2305.17191v1 [cs.LG])

    [http://arxiv.org/abs/2305.17191](http://arxiv.org/abs/2305.17191)

    该论文提出了一种名为 MT-SLVR 的框架，用于解决自监督学习中的不变性问题，以改善不同的下游任务的分类性能。

    

    对比自监督学习因其能从大型未标记数据集中创建高质量表示而受到关注。这些强大的特征为下游任务的数据高效学习提供了扩充不变性，这通常是一种有用的归纳偏差。然而，从先验上不知道所需的不变性数量和类型，并且在不同的下游任务中变化。因此，我们提出了一种多任务自监督框架(MT-SLVR)，以一种参数高效的方式学习变体和不变特征。我们的多任务表示提供了强大和灵活的特征，可使多样的下游任务受益。我们在来自各种音频领域的少样本分类任务上评估了我们的方法，并证明了在所有任务上均有改善的分类性能。

    Contrastive self-supervised learning has gained attention for its ability to create high-quality representations from large unlabelled data sets. A key reason that these powerful features enable data-efficient learning of downstream tasks is that they provide augmentation invariance, which is often a useful inductive bias. However, the amount and type of invariances preferred is not known apriori, and varies across different downstream tasks. We therefore propose a multi-task self-supervised framework (MT-SLVR) that learns both variant and invariant features in a parameter-efficient manner. Our multi-task representation provides a strong and flexible feature that benefits diverse downstream tasks. We evaluate our approach on few-shot classification tasks drawn from a variety of audio domains and demonstrate improved classification performance on all of them
    
[^45]: 利用分段仿射操作实现高效Transformer训练

    Hardware-Efficient Transformer Training via Piecewise Affine Operations. (arXiv:2305.17190v1 [cs.LG])

    [http://arxiv.org/abs/2305.17190](http://arxiv.org/abs/2305.17190)

    本文提出一种使用分段仿射操作代替传统乘法的高效Transformer训练方法，不需要更改训练超参数即可在视觉和语言任务中成功实现训练，同时消除了整个训练过程中的所有乘法操作。

    

    在神经网络训练和推理中，大多数计算成本都是由乘法所贡献的。近期的研究致力于减少由此带来的成本。本文受Mogami（2020）启发，用一种廉价的分段仿射逼近替换乘法，它通过将浮点数的位表示作为整数相加来实现。我们证明了在视觉和语言任务中，可以使用所得到的修改后的矩阵乘法训练Transformer，几乎没有性能影响，并且不需要更改训练超参数。我们进一步用分段仿射替换了网络中的所有非线性，使它们在输入和权重方面都成为完全联合的分段仿射函数。最后，我们展示了如何消除整个训练过程中的所有乘法操作，包括前向传播、反向传播和优化器更新的操作，展示了现代神经网络架构的首次成功训练。

    Multiplications are responsible for most of the computational cost involved in neural network training and inference. Recent research has thus looked for ways to reduce the cost associated with them. Inspired by Mogami (2020), we replace multiplication with a cheap piecewise affine approximation that is achieved by adding the bit representation of the floating point numbers together as integers. We show that transformers can be trained with the resulting modified matrix multiplications on both vision and language tasks with little to no performance impact, and without changes to the training hyperparameters. We further replace all non-linearities in the networks making them fully and jointly piecewise affine in both inputs and weights. Finally, we show that we can eliminate all multiplications in the entire training process, including operations in the forward pass, backward pass and optimizer update, demonstrating the first successful training of modern neural network architectures in
    
[^46]: 向量值随机特征学习的误差界分析

    Error Bounds for Learning with Vector-Valued Random Features. (arXiv:2305.17170v1 [stat.ML])

    [http://arxiv.org/abs/2305.17170](http://arxiv.org/abs/2305.17170)

    本文提供了对向量值随机特征学习的完整误差分析，包括在模型错误说明下向量值RF估计器的强一致性和在良好规定的情况下极小化最优收敛速率。

    

    本文提供了对于向量值随机特征学习的完整误差分析。该理论是针对完全通用的无限维度输入-输出设定中的RF Ridge回归而开发的，但仍适用于并改进了现有的有限维度分析。与文献中其他类似的工作相比，本文提出的方法依赖于底层风险函数的直接分析，完全避免了基于随机矩阵的显式RF Ridge回归解决方案公式的使用。这消除了随机矩阵理论中的浓度结果或其对随机算子的推广的需求。本文建立的主要结果包括在模型错误说明下向量值RF估计器的强一致性和在良好规定的情况下极小化最优收敛速率。实现这些收敛速率所需的参数复杂度(随机特征数量)和样本复杂度(标记数据数量)与

    This paper provides a comprehensive error analysis of learning with vector-valued random features (RF). The theory is developed for RF ridge regression in a fully general infinite-dimensional input-output setting, but nonetheless applies to and improves existing finite-dimensional analyses. In contrast to comparable work in the literature, the approach proposed here relies on a direct analysis of the underlying risk functional and completely avoids the explicit RF ridge regression solution formula in terms of random matrices. This removes the need for concentration results in random matrix theory or their generalizations to random operators. The main results established in this paper include strong consistency of vector-valued RF estimators under model misspecification and minimax optimal convergence rates in the well-specified setting. The parameter complexity (number of random features) and sample complexity (number of labeled data) required to achieve such rates are comparable with 
    
[^47]: 可扩展模拟推理的流匹配技术

    Flow Matching for Scalable Simulation-Based Inference. (arXiv:2305.17161v1 [cs.LG])

    [http://arxiv.org/abs/2305.17161](http://arxiv.org/abs/2305.17161)

    这篇论文提出了一种基于流匹配技术的后验估计方法，用于模拟推理，通过提供灵活性和可扩展性解决高维问题的挑战，并在引力波推断上取得了可比离散流方法更好的结果。

    

    基于离散规范化流的神经后验估计方法已成为模拟推理（SBI）的成熟工具，但将其扩展到高维问题可能具有挑战性。建立在最近生成建模方面的进展基础上，我们提出了基于流匹配的后验估计（FMPE），一种使用连续规范化流进行SBI的技术。与离散流不同，像扩散模型一样，流匹配允许无约束的架构，提供了用于复杂数据模态的增强灵活性。因此，流匹配实现了精确的密度评估、快速的训练和无缝的可扩展性，使其成为SBI的理想选择。我们展示了FMPE在SBI基准测试上实现了有竞争力的性能，然后在一个具有挑战性的科学问题上展示了其改进的可扩展性：对于引力波推断，FMPE胜过基于相似离散流的方法，在减少训练时间30%的同时，提供了有说服力的结果。

    Neural posterior estimation methods based on discrete normalizing flows have become established tools for simulation-based inference (SBI), but scaling them to high-dimensional problems can be challenging. Building on recent advances in generative modeling, we here present flow matching posterior estimation (FMPE), a technique for SBI using continuous normalizing flows. Like diffusion models, and in contrast to discrete flows, flow matching allows for unconstrained architectures, providing enhanced flexibility for complex data modalities. Flow matching, therefore, enables exact density evaluation, fast training, and seamless scalability to large architectures--making it ideal for SBI. We show that FMPE achieves competitive performance on an established SBI benchmark, and then demonstrate its improved scalability on a challenging scientific problem: for gravitational-wave inference, FMPE outperforms methods based on comparable discrete flows, reducing training time by 30% with substanti
    
[^48]: 一种改进的机器学习算法超参数调整集成模型，用于胎儿健康预测

    An Improved Model Ensembled of Different Hyper-parameter Tuned Machine Learning Algorithms for Fetal Health Prediction. (arXiv:2305.17156v1 [cs.LG])

    [http://arxiv.org/abs/2305.17156](http://arxiv.org/abs/2305.17156)

    本研究提出了一种名为ETSE的机器学习集成模型，用于预测胎儿健康。该模型通过采用多种数据预处理技术和7种不同的机器学习分类器，能够提高预测准确性和性能。

    

    胎儿健康对于孕期非常重要，因为它会影响到母亲和胎儿的健康。监测胎儿健康通常需要使用人工智能技术，可以提高诊断的准确性、效率和速度。本研究提出了一种名为ETSE的机器学习定制支持向量机和ExtraTrees集成模型，用于预测胎儿健康。该模型采用了离群值剔除、缺失值补全、数据标准化和数据抽样等预处理技术，并实现了7种机器学习分类器，包括支持向量机、XGBoost、LGBM、决策树、随机森林、ExtraTrees和K近邻。这些模型经过评估和优化，采用超参数调整进行模型性能的最优化。

    Fetal health is a critical concern during pregnancy as it can impact the well-being of both the mother and the baby. Regular monitoring and timely interventions are necessary to ensure the best possible outcomes. While there are various methods to monitor fetal health in the mother's womb, the use of artificial intelligence (AI) can improve the accuracy, efficiency, and speed of diagnosis. In this study, we propose a robust ensemble model called ensemble of tuned Support Vector Machine and ExtraTrees (ETSE) for predicting fetal health. Initially, we employed various data preprocessing techniques such as outlier rejection, missing value imputation, data standardization, and data sampling. Then, seven machine learning (ML) classifiers including Support Vector Machine (SVM), XGBoost (XGB), Light Gradient Boosting Machine (LGBM), Decision Tree (DT), Random Forest (RF), ExtraTrees (ET), and K-Neighbors were implemented. These models were evaluated and then optimized by hyperparameter tuning
    
[^49]: 动力学系统中隐式神经网络的长期预测稳定性

    Stability of implicit neural networks for long-term forecasting in dynamical systems. (arXiv:2305.17155v1 [cs.LG])

    [http://arxiv.org/abs/2305.17155](http://arxiv.org/abs/2305.17155)

    本文提出了一种基于隐式数值方案稳定性特性的神经网络，加入了硬性约束来保证其权重稳定性，取得了较好的长期预测结果。

    

    预测长时间范围内的物理信号是偏微分方程研究中最具挑战性的任务之一。为了规避传统求解器的限制，提出了许多不同的深度学习方法。它们都基于自回归方法并展示出稳定性问题。受隐式数值方案的稳定性特性启发，我们引入了一个稳定的自回归隐式神经网络。我们根据数值方案的稳定性定义，开发了一种理论来保证网络预测的稳定性。它导致我们对其权重添加了硬性约束，并在潜空间中传播动态。我们的实验结果验证了我们的稳定性，展示了在两个输运偏微分方程的长期预测上改进的结果。

    Forecasting physical signals in long time range is among the most challenging tasks in Partial Differential Equations (PDEs) research. To circumvent limitations of traditional solvers, many different Deep Learning methods have been proposed. They are all based on auto-regressive methods and exhibit stability issues. Drawing inspiration from the stability property of implicit numerical schemes, we introduce a stable auto-regressive implicit neural network. We develop a theory based on the stability definition of schemes to ensure the stability in forecasting of this network. It leads us to introduce hard constraints on its weights and propagate the dynamics in the latent space. Our experimental results validate our stability property, and show improved results at long-term forecasting for two transports PDEs.
    
[^50]: 关于深度网络表示中概念空间的凸性研究

    On convex conceptual regions in deep network representations. (arXiv:2305.17154v1 [cs.LG])

    [http://arxiv.org/abs/2305.17154](http://arxiv.org/abs/2305.17154)

    本文研究了深度网络表示中概念空间的凸性对泛化能力、小样本学习和主观一致性的影响，发现近似凸性在多个应用领域中广泛存在。

    

    人机对齐的研究旨在理解潜在空间的几何结构和与人类表征的对应关系。Gardenfors的概念空间是理解人类表征的一个重要框架。在概念空间中，对象区域的凸性被认为是促进泛化能力、小样本学习和主观一致性的重要机制。基于这些洞见，本文研究了机器学习中学习的潜在空间中概念区域的凸性。作者开发了一组用于测量采样数据中凸性的工具，并评估了最先进深度网络中的层表示中的凸性。结果表明，凸性对于基本的重新参数化是稳健的，因此作为机器学习潜在空间质量的一个重要特征是有意义的。作者发现，近似凸性在神经表示中广泛存在于多个应用领域，包括图像、音频、人类活动、文本和脑数据。

    The current study of human-machine alignment aims at understanding the geometry of latent spaces and the correspondence to human representations. G\"ardenfors' conceptual spaces is a prominent framework for understanding human representations. Convexity of object regions in conceptual spaces is argued to promote generalizability, few-shot learning, and intersubject alignment. Based on these insights, we investigate the notion of convexity of concept regions in machine-learned latent spaces. We develop a set of tools for measuring convexity in sampled data and evaluate emergent convexity in layered representations of state-of-the-art deep networks. We show that convexity is robust to basic re-parametrization, hence, meaningful as a quality of machine-learned latent spaces. We find that approximate convexity is pervasive in neural representations in multiple application domains, including models of images, audio, human activity, text, and brain data. We measure convexity separately for l
    
[^51]: mldr.resampling: 多标签重采样算法有效的参考实现

    mldr.resampling: Efficient Reference Implementations of Multilabel Resampling Algorithms. (arXiv:2305.17152v1 [cs.LG])

    [http://arxiv.org/abs/2305.17152](http://arxiv.org/abs/2305.17152)

    mldr.resampling是一个软件包，提供11种多标签重采样方法的参考实现，旨在应对多标签学习中的不平衡情况，并具有高效性。

    

    重采样算法是应对多标签学习中不平衡情况的有用方法。这些方法必须处理多标签数据中的奇异性，例如同一实例中频繁和不频繁标签的出现。这篇原创软件发表介绍了 mldr.resampling，这是一个软件包，提供了11种多标签重采样方法的参考实现，强调效率，因为这些算法通常耗时。

    Resampling algorithms are a useful approach to deal with imbalanced learning in multilabel scenarios. These methods have to deal with singularities in the multilabel data, such as the occurrence of frequent and infrequent labels in the same instance. Implementations of these methods are sometimes limited to the pseudocode provided by their authors in a paper. This Original Software Publication presents mldr.resampling, a software package that provides reference implementations for eleven multilabel resampling methods, with an emphasis on efficiency since these algorithms are usually time-consuming.
    
[^52]: 具有精确编码的诊断时空变换器

    Diagnostic Spatio-temporal Transformer with Faithful Encoding. (arXiv:2305.17149v1 [cs.LG])

    [http://arxiv.org/abs/2305.17149](http://arxiv.org/abs/2305.17149)

    本文提出了一种诊断时空变换器（DFStrans），其利用新的位置编码和时空依赖性发现框架，能够在具有复杂时空依赖性的多元时间序列分类任务中提取可操作见解。

    

    本文讨论了当基本数据生成过程具有复杂的时空依赖性时的异常诊断任务。其中关键技术挑战是从描述时间和空间指数之间高阶交互的依赖张量中提取可操作见解。我们将问题形式化为监督依赖发现，其中时空依赖性被作为多元时间序列分类的副产品进行学习。我们发现现有ST变换器中使用的时间位置编码在捕捉更高频率（短时间尺度）方面存在严重限制。我们提出了一种具有离散傅里叶变换理论保证的新的位置编码。我们还提出了一种新的时空依赖性发现框架，可以在空间和时间方向上提供易于消化的诊断信息。最后，我们在合成和真实世界数据集上证明了所提出的模型DFStrans（基于傅里叶变换的诊断时空变换器）的实用性。

    This paper addresses the task of anomaly diagnosis when the underlying data generation process has a complex spatio-temporal (ST) dependency. The key technical challenge is to extract actionable insights from the dependency tensor characterizing high-order interactions among temporal and spatial indices. We formalize the problem as supervised dependency discovery, where the ST dependency is learned as a side product of multivariate time-series classification. We show that temporal positional encoding used in existing ST transformer works has a serious limitation in capturing higher frequencies (short time scales). We propose a new positional encoding with a theoretical guarantee, based on discrete Fourier transform. We also propose a new ST dependency discovery framework, which can provide readily consumable diagnostic information in both spatial and temporal directions. Finally, we demonstrate the utility of the proposed model, DFStrans (Diagnostic Fourier-based Spatio-temporal Transf
    
[^53]: 高维数据的差分隐私低维表示

    Differentially private low-dimensional representation of high-dimensional data. (arXiv:2305.17148v1 [cs.LG])

    [http://arxiv.org/abs/2305.17148](http://arxiv.org/abs/2305.17148)

    本文提出了一种在保护个人敏感信息的情况下，生成高效低维合成数据的算法，并在Wasserstein距离方面具有效用保证；与标准扰动分析不同，使用私有主成分分析过程避免了维度诅咒的影响。

    

    差分隐私合成数据提供了一种有效的机制，可以在保护个人敏感信息的同时进行数据分析。然而，当数据处于高维空间中时，合成数据的准确性会受到维度诅咒的影响。在本文中，我们提出了一种差分隐私算法，可以从高维数据集中高效地生成低维合成数据，并在Wasserstein距离方面具有效用保证。我们算法的一个关键步骤是使用具有近乎最优精度界限的私有主成分分析（PCA）过程，从而规避了维度诅咒的影响。与使用Davis-Kahan定理进行标准扰动分析不同，我们的私有PCA分析不需要假设样本协方差矩阵的谱间隙。

    Differentially private synthetic data provide a powerful mechanism to enable data analysis while protecting sensitive information about individuals. However, when the data lie in a high-dimensional space, the accuracy of the synthetic data suffers from the curse of dimensionality. In this paper, we propose a differentially private algorithm to generate low-dimensional synthetic data efficiently from a high-dimensional dataset with a utility guarantee with respect to the Wasserstein distance. A key step of our algorithm is a private principal component analysis (PCA) procedure with a near-optimal accuracy bound that circumvents the curse of dimensionality. Different from the standard perturbation analysis using the Davis-Kahan theorem, our analysis of private PCA works without assuming the spectral gap for the sample covariance matrix.
    
[^54]: 大型语言模型的异质价值评估

    Heterogeneous Value Evaluation for Large Language Models. (arXiv:2305.17147v1 [cs.CL])

    [http://arxiv.org/abs/2305.17147](http://arxiv.org/abs/2305.17147)

    本文提出了一种自动对齐评估方法A2EHV，采用异质价值系统，并基于价值合理性和社会价值定向框架评估代理人行为的社会偏好，结果表明比传统对齐方法更合理。

    

    大型语言模型（LLM）的出现使得将它们的价值与人类价值对齐变得至关重要。当前的方法通常尝试将其与一种同质的人类价值对齐，并需要人类验证，但缺乏对对齐所需方面和深度的共识以及造成的人类偏见。在本文中，我们提出了一种自动对齐评估方法A2EHV，该方法采用异质价值系统，（1）是自动化的，以最小化单个人类偏见，并且（2）允许评估针对各种目标值的异质代理人。我们的方法基于价值合理性的概念，它代表了代理人执行最能满足目标价值行为的能力。价值合理性的量化是通过社会心理学中的社会价值定向框架进行的，该框架将价值空间分为四个类别，以评估代理人行为的社会偏好。我们评估了三个模型的价值合理性，结果表明A2EHV方法比传统对齐方法更合理。

    The emergent capabilities of Large Language Models (LLMs) have made it crucial to align their values with those of humans. Current methodologies typically attempt alignment with a homogeneous human value and requires human verification, yet lack consensus on the desired aspect and depth of alignment and resulting human biases. In this paper, we propose A2EHV, an Automated Alignment Evaluation with a Heterogeneous Value system that (1) is automated to minimize individual human biases, and (2) allows assessments against various target values to foster heterogeneous agents. Our approach pivots on the concept of value rationality, which represents the ability for agents to execute behaviors that satisfy a target value the most. The quantification of value rationality is facilitated by the Social Value Orientation framework from social psychology, which partitions the value space into four categories to assess social preferences from agents' behaviors. We evaluate the value rationality of e
    
[^55]: 程序分解与类型填充训练的类型预测

    Type Prediction With Program Decomposition and Fill-in-the-Type Training. (arXiv:2305.17145v1 [cs.SE])

    [http://arxiv.org/abs/2305.17145](http://arxiv.org/abs/2305.17145)

    该论文提出了一种基于大型语言模型的搜索方法OpenTau，采用树形程序分解技术和类型填充微调方法解决类型预测中的一些挑战。在TypeScript类型预测方面，该方法在类型检查方面表现良好，平均精度为0.707，优于现有最先进方法。

    

    TypeScript和Python是支持可选类型注释的编程语言，很有用但引入和维护起来很麻烦。这激发了自动类型预测：给定一个未标记类型的程序，生成一个类型正确的输出程序。大型语言模型（LLMs）对于类型预测非常有希望，但存在挑战：中间填充的表现差，程序可能不适合上下文窗口，生成的类型可能无法通过类型检查，而且很难测量输出程序的类型正确性。我们通过构建OpenTau来解决这些问题，这是一种基于搜索的类型预测方法，利用大型语言模型。我们提出了一种新的类型预测质量度量方法，提供了一种树形程序分解技术来搜索生成的类型空间，并提出了用于LLMs的类型填充微调方法。我们使用新的TypeScript类型预测数据集进行评估，并展示了47.4％的文件通过类型检查（14.5％的绝对改进），平均精度达到0.707，超过了最先进的方法。

    TypeScript and Python are two programming languages that support optional type annotations, which are useful but tedious to introduce and maintain. This has motivated automated type prediction: given an untyped program, produce a well-typed output program. Large language models (LLMs) are promising for type prediction, but there are challenges: fill-in-the-middle performs poorly, programs may not fit into the context window, generated types may not type check, and it is difficult to measure how well-typed the output program is. We address these challenges by building OpenTau, a search-based approach for type prediction that leverages large language models. We propose a new metric for type prediction quality, give a tree-based program decomposition that searches a space of generated types, and present fill-in-the-type fine-tuning for LLMs. We evaluate our work with a new dataset for TypeScript type prediction, and show that 47.4% of files type check (14.5% absolute improvement) with an 
    
[^56]: Minecraft中的幽灵：利用基于文本知识和记忆的大型语言模型实现开放世界环境中的通用能力智能体。

    Ghost in the Minecraft: Generally Capable Agents for Open-World Enviroments via Large Language Models with Text-based Knowledge and Memory. (arXiv:2305.17144v1 [cs.AI])

    [http://arxiv.org/abs/2305.17144](http://arxiv.org/abs/2305.17144)

    本文提出了Ghost in the Minecraft (GITM)框架，利用大型语言模型与基于文本的知识和记忆，创造了一种在Minecraft中具备通用能力的智能体，可在以文本为基础的复杂编程环境中熟练导航。

    

    近年来，Minecraft玩法吸引了大量的研究关注，成为开发能够在开放世界环境中运行的智能体的丰富平台。然而，当前的研究主要集中在特定的目标上，例如流行的“ObtainDiamond”任务，并且还没有显示出有效地推广到更广泛任务的能力。此外，“ObtainDiamond”任务的目前最高成功率只有约20％，凸显了现有方法中使用强化学习（RL）控制器的局限性。为了解决这些挑战，我们引入了Ghost in the Minecraft (GITM)，一个新颖的框架，将大型语言模型与基于文本的知识和记忆相结合，旨在创建Minecraft中的通用能力智能体。这些具备LLM中的逻辑和常识能力的智能体可以熟练地在以文本为基础的复杂编程环境中导航。

    The captivating realm of Minecraft has attracted substantial research interest in recent years, serving as a rich platform for developing intelligent agents capable of functioning in open-world environments. However, the current research landscape predominantly focuses on specific objectives, such as the popular "ObtainDiamond" task, and has not yet shown effective generalization to a broader spectrum of tasks. Furthermore, the current leading success rate for the "ObtainDiamond" task stands at around 20%, highlighting the limitations of Reinforcement Learning (RL) based controllers used in existing methods. To tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel framework integrates Large Language Models (LLMs) with text-based knowledge and memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These agents, equipped with the logic and common sense capabilities of LLMs, can skillfully navigate complex, sparse-reward environments with text-based 
    
[^57]: 基于深度强化学习的多智能体通信与协作决策研究

    Research on Multi-Agent Communication and Collaborative Decision-Making Based on Deep Reinforcement Learning. (arXiv:2305.17141v1 [cs.MA])

    [http://arxiv.org/abs/2305.17141](http://arxiv.org/abs/2305.17141)

    本研究基于CTDE框架，提出了基于MAPPO算法的多智能体合作决策模型，并引入了基于权重调度和注意力模块的多智能体通信机制，能够有效缓解多智能体环境的不稳定性，提高多智能体在复杂环境中的协作决策能力。

    

    在多智能体环境中，为了克服和缓解环境的不稳定性，主流方法是采用集中式训练分散式执行（CTDE）框架。本文基于CTDE框架，研究了基于多智能体近端策略优化（MAPPO）算法的多智能体合作决策问题。为了缓解多智能体环境的不稳定性，引入了基于权重调度和注意力模块的多智能体通信机制。不同的智能体可以通过智能体之间的信息交换来缓解由本地观测引起的不稳定性，协助智能体的协作决策。具体方法是在策略网络部分引入一个通信模块。通信模块由权重生成器、权重调度器、信息编码器、信息解码器和注意力模块组成。经过实验证明，所提出的方法可以有效地提高多智能体在复杂环境中的协作决策能力。

    In a multi-agent environment, In order to overcome and alleviate the non-stationarity of the multi-agent environment, the mainstream method is to adopt the framework of Centralized Training Decentralized Execution (CTDE). This thesis is based on the framework of CTDE, and studies the cooperative decision-making of multi-agent based on the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm for multi-agent proximal policy optimization. In order to alleviate the non-stationarity of the multi-agent environment, a multi-agent communication mechanism based on weight scheduling and attention module is introduced. Different agents can alleviate the non-stationarity caused by local observations through information exchange between agents, assisting in the collaborative decision-making of agents. The specific method is to introduce a communication module in the policy network part. The communication module is composed of a weight generator, a weight scheduler, a message encoder, a messag
    
[^58]: 智能车辆系统中的生成人工智能集成

    Integrating Generative Artificial Intelligence in Intelligent Vehicle Systems. (arXiv:2305.17137v1 [cs.AI])

    [http://arxiv.org/abs/2305.17137](http://arxiv.org/abs/2305.17137)

    本文提供了关于在智能车辆系统中集成生成人工智能的全面指南，重点强调了其对语音、音频、视觉和多模态交互的应用，并提出了未来研究领域和与伦理道德相关的挑战和风险。

    

    本文旨在为研究人员和实践者提供全面指南，为生成人工智能和基础模型在智能车辆环境中的当前状态、潜在应用和未来研究方向提供洞见。随着汽车行业逐渐整合人工智能，生成人工智能技术有潜力在用户交互方面革命性地改变，提供更沉浸、直观、个性化的车内体验。我们提供生成人工智能在汽车领域中的当前应用概述，重点强调语音、音频、视觉和多模态交互。随后我们概述了关键未来研究领域，包括领域适应性、对齐、多模态集成等，以及解决与伦理道德相关的挑战和风险。通过促进协作和解决这些研究领域，生成人工智能可以成为智能车辆系统的一个重要组成部分，增强驾驶员和乘客的安全、舒适和便利性。

    This paper aims to serve as a comprehensive guide for researchers and practitioners, offering insights into the current state, potential applications, and future research directions for generative artificial intelligence and foundation models within the context of intelligent vehicles. As the automotive industry progressively integrates AI, generative artificial intelligence technologies hold the potential to revolutionize user interactions, delivering more immersive, intuitive, and personalised in-car experiences. We provide an overview of current applications of generative artificial intelligence in the automotive domain, emphasizing speech, audio, vision, and multimodal interactions. We subsequently outline critical future research areas, including domain adaptability, alignment, multimodal integration and others, as well as, address the challenges and risks associated with ethics. By fostering collaboration and addressing these research areas, generative artificial intelligence can
    
[^59]: 三塔：利用预训练图像模型进行灵活的对比学习

    Three Towers: Flexible Contrastive Learning with Pretrained Image Models. (arXiv:2305.16999v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.16999](http://arxiv.org/abs/2305.16999)

    本文提出了 3T 方法，即在视觉语言模型中引入预训练的图像分类器，从而提高对比学习的灵活性。3T 同时受益于预训练嵌入和对比训练，并在实验证明对检索任务和分类问题均取得了有竞争力的性能。

    

    本文提出了一种名为“三塔（3T）”的灵活方法，通过将预先训练的图像分类器纳入对比学习，改进了视觉语言模型的对比学习。与通常从头开始训练对比模型不同，最近的 LiT（Zhai 等人，2022）表明了使用预训练分类器嵌入的性能提升。但是，LiT 直接用冻结的嵌入替换图像塔，排除了对图像塔进行对比训练的任何潜在好处。通过 3T，我们提出了一种更灵活的策略，允许图像塔同时受益于预训练嵌入和对比训练。为了实现这一点，我们引入了第三个塔，其中包含冻结的预训练嵌入，并鼓励该第三个塔与主要的图像-文本塔之间的对齐。在实验证明，3T 在检索任务上始终优于 LiT 和 CLIP 风格的从头开始对比学习基线。对于分类问题，3T 在从头开始基线的基础上可靠地改善，虽然在某些数据集上表现不及 LiT，但仍然实现了有竞争力的性能。总的来说，本方法凸显了将预训练分类器注入到视觉语言模型中的有效性，并提供了一种更灵活的利用它们的方法。

    We introduce Three Towers (3T), a flexible method to improve the contrastive learning of vision-language models by incorporating pretrained image classifiers. While contrastive models are usually trained from scratch, LiT (Zhai et al., 2022) has recently shown performance gains from using pretrained classifier embeddings. However, LiT directly replaces the image tower with the frozen embeddings, excluding any potential benefits of contrastively training the image tower. With 3T, we propose a more flexible strategy that allows the image tower to benefit from both pretrained embeddings and contrastive training. To achieve this, we introduce a third tower that contains the frozen pretrained embeddings, and we encourage alignment between this third tower and the main image-text towers. Empirically, 3T consistently improves over LiT and the CLIP-style from-scratch baseline for retrieval tasks. For classification, 3T reliably improves over the from-scratch baseline, and while it underperform
    
[^60]: 带有复值的深窄神经网络的普适逼近

    Universal approximation with complex-valued deep narrow neural networks. (arXiv:2305.16910v1 [math.FA])

    [http://arxiv.org/abs/2305.16910](http://arxiv.org/abs/2305.16910)

    本文研究了具有有界宽度和任意深度的复值神经网络的普适性，发现当且仅当激活函数既不是全纯的，也不是反全纯的，也不是 $\mathbb{R}$-仿射的时，深窄的复值网络具有普适逼近能力。我们还发现足够的宽度依赖于考虑的激活函数，对于一类可允许的激活函数，宽度为 $n+m+4$ 是足够的。

    

    我们研究了具有有界宽度和任意深度的复值神经网络的普适性。在温和的假设下，我们给出了那些激活函数 $\varrho:\mathbb{CC}\to \mathbb{C}$ 的完整描述，这些函数具有这样一个属性：它们关联的网络是普适的，即能够在紧致域上逼近连续函数至任意精度。准确地说，我们表明了当且仅当它们的激活函数既不是全纯的，也不是反全纯的，也不是 $\mathbb{R}$-仿射的，深窄的复值网络是普适的。这是一个比宽度任意、深度固定的对偶设置中更大的函数类。与实值情况不同的是，足够的宽度依赖于考虑的激活函数。我们表明，宽度为 $2n+2m+5$ 总是足够的，并且通常 $\max\{2n,2m\}$ 是必要的。然而，我们证明了对于一类可允许的激活函数，宽度为 $n+m+4$ 是足够的。

    We study the universality of complex-valued neural networks with bounded widths and arbitrary depths. Under mild assumptions, we give a full description of those activation functions $\varrho:\mathbb{CC}\to \mathbb{C}$ that have the property that their associated networks are universal, i.e., are capable of approximating continuous functions to arbitrary accuracy on compact domains. Precisely, we show that deep narrow complex-valued networks are universal if and only if their activation function is neither holomorphic, nor antiholomorphic, nor $\mathbb{R}$-affine. This is a much larger class of functions than in the dual setting of arbitrary width and fixed depth. Unlike in the real case, the sufficient width differs significantly depending on the considered activation function. We show that a width of $2n+2m+5$ is always sufficient and that in general a width of $\max\{2n,2m\}$ is necessary. We prove, however, that a width of $n+m+4$ suffices for a rich subclass of the admissible acti
    
[^61]: 你能比穷举搜索更快地解决最近字符串问题吗？

    Can You Solve Closest String Faster than Exhaustive Search?. (arXiv:2305.16878v2 [cs.CC] UPDATED)

    [http://arxiv.org/abs/2305.16878](http://arxiv.org/abs/2305.16878)

    本文研究最近字符串问题是否存在比平凡的穷举搜索算法更快的算法，针对不同版本的问题的自然版本，得出以下结果：对于连续的问题版本，不存在比平凡的穷举搜索算法更快的解法，对于离散版本，如果$d$是常数，则以$2^{O(\sqrt{n})}$次比较可以解决它。

    

    我们研究了寻找用最佳字符串表示给定集合的基本问题，并提出了最近字符串问题：给定一个大小为$n$，长度为$d$，由字符串$X \subseteq \Sigma^d$组成的集合，找到字符串$x^*$，使得最小的Hamming球的半径囊括了$X$中的所有字符串。在本文中，我们探讨了最近字符串问题是否存在比平凡的穷举搜索算法更快的算法。对于问题的两个自然版本，我们得到了以下结果：在连续的最近字符串问题中，目标是在$\Sigma^d$中找到解决方案字符串$x^*$。对于二进制字符串，穷举搜索算法的运行时间为$O(2^d poly(nd))$，我们证明它不能改进为时间复杂度为$O(2^{(1-\epsilon) d} poly(nd))$，任何$\epsilon > 0$都不成立，除非强指数时间假设失败。在离散的最近字符串问题中，要求$x^*$在输入集合$X$中。虽然这将问题转换为NP-完全问题，但我们证明，如果$d$是常数，则以$2^{O(\sqrt{n})}$次比较的形式可以解决它。

    We study the fundamental problem of finding the best string to represent a given set, in the form of the Closest String problem: Given a set $X \subseteq \Sigma^d$ of $n$ strings, find the string $x^*$ minimizing the radius of the smallest Hamming ball around $x^*$ that encloses all the strings in $X$. In this paper, we investigate whether the Closest String problem admits algorithms that are faster than the trivial exhaustive search algorithm. We obtain the following results for the two natural versions of the problem:  $\bullet$ In the continuous Closest String problem, the goal is to find the solution string $x^*$ anywhere in $\Sigma^d$. For binary strings, the exhaustive search algorithm runs in time $O(2^d poly(nd))$ and we prove that it cannot be improved to time $O(2^{(1-\epsilon) d} poly(nd))$, for any $\epsilon > 0$, unless the Strong Exponential Time Hypothesis fails.  $\bullet$ In the discrete Closest String problem, $x^*$ is required to be in the input set $X$. While this p
    
[^62]: 神经控制微分方程的泛化能力研究

    On the Generalization Capacities of Neural Controlled Differential Equations. (arXiv:2305.16791v1 [stat.ML])

    [http://arxiv.org/abs/2305.16791](http://arxiv.org/abs/2305.16791)

    本文研究了使用神经控制微分方程进行监督学习的泛化能力问题，通过量化离散化偏差和利普希茨函数逼近误差，得到了经验风险最小化器与贝叶斯最优风险的泛化差距上界。

    

    本文研究了使用神经控制微分方程（Kidger，Morrill等，2020）从不规则采样的时间序列样本中预测结果的监督学习设置。在我们的框架中，时间序列是一个未观察到的连续路径的离散化，结果通过一个具有未知向量场的控制微分方程依赖于这个路径。使用离散数据进行学习会引入离散偏差，我们精确地量化了这种偏差。通过使用关于控制微分方程流的连续性的理论结果，我们展示了逼近偏差直接与由浅层神经网络定义生成模型的利普希茨函数的逼近误差相关。通过结合最近的工作将神经网络的利普希茨常数与其泛化能力联系起来，我们上界了经验风险最小化器达到的期望损失与贝叶斯最优风险之间的泛化差距。

    We consider a supervised learning setup in which the goal is to predicts an outcome from a sample of irregularly sampled time series using Neural Controlled Differential Equations (Kidger, Morrill, et al. 2020). In our framework, the time series is a discretization of an unobserved continuous path, and the outcome depends on this path through a controlled differential equation with unknown vector field. Learning with discrete data thus induces a discretization bias, which we precisely quantify. Using theoretical results on the continuity of the flow of controlled differential equations, we show that the approximation bias is directly related to the approximation error of a Lipschitz function defining the generative model by a shallow neural network. By combining these result with recent work linking the Lipschitz constant of neural networks to their generalization capacities, we upper bound the generalization gap between the expected loss attained by the empirical risk minimizer and th
    
[^63]: 带部分知识特征的图像缺失特征补全中的置信度特性辅助特征提取

    Confidence-Based Feature Imputation for Graphs with Partially Known Features. (arXiv:2305.16618v1 [cs.LG])

    [http://arxiv.org/abs/2305.16618](http://arxiv.org/abs/2305.16618)

    本论文提出了一种新颖的信道置信度与伪置信度的特征插值方法，解决了高缺失特征率的图像学习任务中的性能下降问题。

    

    本文探讨了在图学习任务中的缺失特征补全问题。过去有几种方法已经解决了具有缺失特征的图形学习任务。然而，对于高缺失特征率的情况，它们无法避免显著的性能下降。为了克服这个限制，我们引入了一个新的概念，即节点特征中的信道置信度，它被指定给每个节点的填充信道特征，以反映填充的确定性。然后，我们设计了伪置信度，使用缺失特征节点与其最近的已知特征节点之间的信道最短路径距离来替换实际学习过程中缺失的真实置信度。基于伪置信度，我们提出了一种新的特征插值方案，它执行信道内节点扩散和节点内信道传播。该方案可以在非常高的缺失率下（例如，99.5％）持久存在，并且实现了最先进的准确性。

    This paper investigates a missing feature imputation problem for graph learning tasks. Several methods have previously addressed learning tasks on graphs with missing features. However, in cases of high rates of missing features, they were unable to avoid significant performance degradation. To overcome this limitation, we introduce a novel concept of channel-wise confidence in a node feature, which is assigned to each imputed channel feature of a node for reflecting certainty of the imputation. We then design pseudo-confidence using the channel-wise shortest path distance between a missing-feature node and its nearest known-feature node to replace unavailable true confidence in an actual learning process. Based on the pseudo-confidence, we propose a novel feature imputation scheme that performs channel-wise inter-node diffusion and node-wise inter-channel propagation. The scheme can endure even at an exceedingly high missing rate (e.g., 99.5\%) and it achieves state-of-the-art accurac
    
[^64]: 探索长尾识别问题中的权重平衡

    Exploring Weight Balancing on Long-Tailed Recognition Problem. (arXiv:2305.16573v1 [cs.LG])

    [http://arxiv.org/abs/2305.16573](http://arxiv.org/abs/2305.16573)

    研究分析了新提出的权重平衡方法在长尾识别问题中有效的原因，发现其能够缓解神经崩溃和圆锥效应，从而提高识别性能。

    

    长尾数据中的识别问题最近变得越来越重要，因为数据集中每个类别的样本数量分布通常是指数分布，除非有意地调整样本数量。针对这些问题已经提出了各种方法。最近，提出了权重平衡方法，它结合了著名的经典正则化技术和两阶段训练。尽管其简单性，但已知其对现有各种不同方法具有高性能。然而，我们缺乏为什么这种方法对长尾数据有效的理解。在这项研究中，我们分析了该方法，并关注了神经崩溃和每个训练阶段的圆锥效应，并发现它可以分解为由权值衰减和交叉熵损失引起的特征提取器中Fisher判别比的增加以及由权重衰减和类平衡正则化引起的隐式逻辑调整。我们还证明了权重平衡方法成功缓解了神经崩溃和圆锥效应，从而提高了长尾数据的识别性能。

    Recognition problems in long-tailed data, where the sample size per class is heavily skewed, have recently gained importance because the distribution of the sample size per class in a dataset is generally exponential unless the sample size is intentionally adjusted. Various approaches have been devised to address these problems. Recently, weight balancing, which combines well-known classical regularization techniques with two-stage training, has been proposed. Despite its simplicity, it is known for its high performance against existing methods devised in various ways. However, there is a lack of understanding as to why this approach is effective for long-tailed data. In this study, we analyze the method focusing on neural collapse and cone effect at each training stage and find that it can be decomposed into the increase in Fisher's discriminant ratio of the feature extractor caused by weight decay and cross entropy loss and implicit logit adjustment caused by weight decay and class-b
    
[^65]: 对比学习学到了哪些特征？关于简易偏差在类坍塌和特征抑制中的作用

    Which Features are Learnt by Contrastive Learning? On the Role of Simplicity Bias in Class Collapse and Feature Suppression. (arXiv:2305.16536v1 [cs.LG])

    [http://arxiv.org/abs/2305.16536](http://arxiv.org/abs/2305.16536)

    对比学习是一种表示学习技术，对于有监督的情况易于产生类坍塌，无监督情况下易于抑制类别相关的复杂特征；随机梯度下降方法偏向于寻找更简单的解决方案是导致这种现象的关键因素。

    

    对比学习具备无监督和有监督学习的表示学习技术，在有监督场景下易于坍塌同一类别内的子类表示，丢失一部分特征信息；而无监督学习则可能通过学习易于处理的类别无关特征而无视一些类别相关的复杂特征信息，这两种方法都会显著地降低表征的质量。本文提出了第一个统一严谨的框架来理解测试时的类坍塌和特征抑制产生的原因，相关分析表明，（随机）梯度下降方法偏向于寻找更简单的解决方案是导致子类表示坍塌和类别相关的复杂特征被抑制的关键因素。此外，我们利用提高嵌入维度和改进数据增强的方法来提供有效的预防措施。

    Contrastive learning (CL) has emerged as a powerful technique for representation learning, with or without label supervision. However, supervised CL is prone to collapsing representations of subclasses within a class by not capturing all their features, and unsupervised CL may suppress harder class-relevant features by focusing on learning easy class-irrelevant features; both significantly compromise representation quality. Yet, there is no theoretical understanding of \textit{class collapse} or \textit{feature suppression} at \textit{test} time. We provide the first unified theoretically rigorous framework to determine \textit{which} features are learnt by CL. Our analysis indicate that, perhaps surprisingly, bias of (stochastic) gradient descent towards finding simpler solutions is a key factor in collapsing subclass representations and suppressing harder class-relevant features. Moreover, we present increasing embedding dimensionality and improving the quality of data augmentations 
    
[^66]: 面向超大规模图的快速在线节点分类算法

    Fast Online Node Labeling for Very Large Graphs. (arXiv:2305.16257v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.16257](http://arxiv.org/abs/2305.16257)

    本文提出了一种适用于超大规模图的在线节点分类算法FastONL，它基于广义局部推送方法，能有效近似逆矩阵列并应用于一系列流行的图核，具有较低的遗憾值和每个预测的较低成本。

    

    本文研究了转导学习背景下的在线节点分类问题。当前方法要么需要在$\mathcal{O}(n^3)$的时间复杂度和$\mathcal{O}(n^2)$的空间复杂度内求解图核矩阵的逆，要么需要采样大量的随机生成树，这使得这些方法难以处理大规模图。我们提出了一种基于在线松弛技术的改进算法。当适当选择参数化的图核时，我们首先证明了有效的遗憾值为$\mathcal{O}(\sqrt{n^{1+\gamma}})$。然后，我们基于该松弛提出了一种近似算法FastONL，其遗憾值为$\mathcal{O}(k\sqrt{n^{1+\gamma}})$。FastONL的关键是一种广义局部推送方法，它能有效地近似逆矩阵列并应用于一系列流行的图核。此外，每个预测的成本为$\mathcal{O}(\text{vol}({\mathcal{S}})\log 1/\epsilon)$

    This paper studies the online node classification problem under a transductive learning setting. Current methods either invert a graph kernel matrix with $\mathcal{O}(n^3)$ runtime and $\mathcal{O}(n^2)$ space complexity or sample a large volume of random spanning trees, thus are difficult to scale to large graphs. In this work, we propose an improvement based on the \textit{online relaxation} technique introduced by a series of works (Rakhlin et al.,2012; Rakhlin and Sridharan, 2015; 2017). We first prove an effective regret $\mathcal{O}(\sqrt{n^{1+\gamma}})$ when suitable parameterized graph kernels are chosen, then propose an approximate algorithm FastONL enjoying $\mathcal{O}(k\sqrt{n^{1+\gamma}})$ regret based on this relaxation. The key of FastONL is a \textit{generalized local push} method that effectively approximates inverse matrix columns and applies to a series of popular kernels. Furthermore, the per-prediction cost is $\mathcal{O}(\text{vol}({\mathcal{S}})\log 1/\epsilon)$
    
[^67]: 在脉冲神经网络中将噪声作为计算和学习资源

    Exploiting Noise as a Resource for Computation and Learning in Spiking Neural Networks. (arXiv:2305.16044v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2305.16044](http://arxiv.org/abs/2305.16044)

    本文提出了噪声脉冲神经元网络（NSNN）和噪声驱动学习规则（NDL），展示了噪声可以作为计算和学习的资源，并为一般脉冲神经元网络提供了一个框架。研究还展示了NSNNs在图像分类和语音识别等实际任务中的适用性，表明它们是未来神经形态计算系统的潜在有力工具。

    

    脉冲神经元网络是大脑非凡信息处理能力的基础，并已成为神经形态智能的支柱模型。本文介绍了噪声脉冲神经元网络（NSNN）和噪声驱动学习规则（NDL），采用带有噪声神经元动力学的脉冲神经元模型。该方法显示噪声可以作为计算和学习的资源，并理论上为一般脉冲神经元网络提供了一个框架。此外，NDL为代理梯度提供了深入的生物学合理性。通过将各种SNN架构和算法结合起来，我们展示了我们的方法表现出竞争性能，并且比确定性SNNs表现出更好的鲁棒性。此外，本文还展示了NSNNs在图像分类和语音识别等实际任务中的适用性，表明它们是未来神经形态计算系统的潜在有力工具。

    Networks of spiking neurons underpin the extraordinary information-processing capabilities of the brain and have emerged as pillar models in neuromorphic intelligence. Despite extensive research on spiking neural networks (SNNs), most are established on deterministic models. Integrating noise into SNNs leads to biophysically more realistic neural dynamics and may benefit model performance. This work presents the noisy spiking neural network (NSNN) and the noise-driven learning rule (NDL) by introducing a spiking neuron model incorporating noisy neuronal dynamics. Our approach shows how noise may act as a resource for computation and learning and theoretically provides a framework for general SNNs. Moreover, NDL provides an insightful biological rationale for surrogate gradients. By incorporating various SNN architectures and algorithms, we show that our approach exhibits competitive performance and improved robustness against challenging perturbations than deterministic SNNs. Additiona
    
[^68]: 动态上下文剪枝用于高效和可解释的自回归变换器

    Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers. (arXiv:2305.15805v1 [cs.CL])

    [http://arxiv.org/abs/2305.15805](http://arxiv.org/abs/2305.15805)

    本研究提出了一种动态上下文剪枝方法，可以在保持模型表现力的同时，动态减少无效信息，提高模型的效率和可解释性。该技术可以应用于现有的预训练模型，并且可以通过简单的微调过程实现。

    

    大型语言模型中采用的自回归变换器难以扩展到长序列。尽管有几项工作试图减少它们的计算成本，但大多数LLM仍然在所有标记对之间采用注意层，从而产生二次成本。本研究提出了一种新方法，通过保留模型的表现力来动态修剪上下文信息，从而在推理过程中减少内存和计算要求。我们的方法使用可学习机制，在生成过程中确定哪些无关的标记可以从上下文中删除。通过这样做，我们的方法不仅解决了性能问题，而且增强了可解释性，为模型的决策过程提供了宝贵的洞察力。我们的技术可以通过简单的微调过程应用于现有的预训练模型，并且剪枝强度可以由稀疏度参数指定。

    Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity para
    
[^69]: 通过最优输运表征区分于分布误差

    Characterizing Out-of-Distribution Error via Optimal Transport. (arXiv:2305.15640v1 [cs.LG])

    [http://arxiv.org/abs/2305.15640](http://arxiv.org/abs/2305.15640)

    本论文提出了一种基于最优输运理论的新方法 - 置信最优输运(COT)，并且引入了基于经验的变体 - 带门限的置信最优输运(COTT)，它们能够更精确地估计模型的性能，特别是在面对伪标签转移误差时。

    

    在机器学习部署中，没在分布(out-of-distribution)的数据对模型提出了严峻的挑战，因此预测模型在没标签的o

    Out-of-distribution (OOD) data poses serious challenges in deployed machine learning models, so methods of predicting a model's performance on OOD data without labels are important for machine learning safety. While a number of methods have been proposed by prior work, they often underestimate the actual error, sometimes by a large margin, which greatly impacts their applicability to real tasks. In this work, we identify pseudo-label shift, or the difference between the predicted and true OOD label distributions, as a key indicator to this underestimation. Based on this observation, we introduce a novel method for estimating model performance by leveraging optimal transport theory, Confidence Optimal Transport (COT), and show that it provably provides more robust error estimates in the presence of pseudo-label shift. Additionally, we introduce an empirically-motivated variant of COT, Confidence Optimal Transport with Thresholding (COTT), which applies thresholding to the individual tra
    
[^70]: 通过求解最优边界条件解决扩散ODE问题以实现更好的图像超分辨率

    Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image Super-Resolution. (arXiv:2305.15357v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2305.15357](http://arxiv.org/abs/2305.15357)

    本文提出了一种求解最优边界条件解决扩散ODE问题的有效采样方法，以稳定地从预训练的基于扩散的超分辨率模型中采样高质量的超分辨率图像。

    

    扩散模型作为一种强大的生成模型，已经在图像超分辨率任务中取得了令人印象深刻的结果。然而，由于扩散模型反向过程中引入的随机性，基于扩散的超分辨率模型在每次采样时性能波动很大，特别是对于具有少量重新采样步骤的采样器。扩散模型的这种固有随机性导致其无效和不稳定，使用户难以保证超分辨结果的质量。然而，我们的工作将这种随机性视为一种机遇：全面分析和利用它导致了构建一种有效的即插即用采样方法，具有潜力使一系列基于扩散的超分辨率方法受益。更详细地说，我们建议通过求解扩散普通微分方程（扩散ODE）和最优边界条件（BC），稳定地从预训练的基于扩散的超分辨率模型中采样高质量的超分辨率图像，并分析其特性。

    Diffusion models, as a kind of powerful generative model, have given impressive results on image super-resolution (SR) tasks. However, due to the randomness introduced in the reverse process of diffusion models, the performances of diffusion-based SR models are fluctuating at every time of sampling, especially for samplers with few resampled steps. This inherent randomness of diffusion models results in ineffectiveness and instability, making it challenging for users to guarantee the quality of SR results. However, our work takes this randomness as an opportunity: fully analyzing and leveraging it leads to the construction of an effective plug-and-play sampling method that owns the potential to benefit a series of diffusion-based SR methods. More in detail, we propose to steadily sample high-quality SR images from pretrained diffusion-based SR models by solving diffusion ordinary differential equations (diffusion ODEs) with optimal boundary conditions (BCs) and analyze the characterist
    
[^71]: 可复现强化学习

    Replicable Reinforcement Learning. (arXiv:2305.15284v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15284](http://arxiv.org/abs/2305.15284)

    本篇论文提供了可复制的强化学习算法，是控制问题的第一个正式的可复制性结果

    

    在社会、行为和数据科学中，可重复性危机导致了算法框架的形成，即要求算法在从相同的底层分布提取的两个不同样本上运行时产生相同的输出（概率高）。虽然仍处于初期阶段，但在机器学习和统计学中的许多基本任务，包括统计查询学习、重要项问题和分布测试，都已经开发出了可证明可复现算法。在这项工作中，我们开始研究可复现强化学习，并提供了并行值迭代的可证复制算法以及一个在连续设置中可证复制的R-max。这是控制问题的第一个正式可复制性结果，这些问题在批量学习环境中提出了不同的复制挑战。

    The replicability crisis in the social, behavioral, and data sciences has led to the formulation of algorithm frameworks for replicability -- i.e., a requirement that an algorithm produce identical outputs (with high probability) when run on two different samples from the same underlying distribution. While still in its infancy, provably replicable algorithms have been developed for many fundamental tasks in machine learning and statistics, including statistical query learning, the heavy hitters problem, and distribution testing. In this work we initiate the study of replicable reinforcement learning, providing a provably replicable algorithm for parallel value iteration, and a provably replicable version of R-max in the episodic setting. These are the first formal replicability results for control problems, which present different challenges for replication than batch learning settings.
    
[^72]: 多模式机器学习在车辆评分预测中的应用：基于图像、文本和参数数据。

    Multi-modal Machine Learning for Vehicle Rating Predictions Using Image, Text, and Parametric Data. (arXiv:2305.15218v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15218](http://arxiv.org/abs/2305.15218)

    本文提出了一种多模式机器学习模型，它可以同时使用图像、文本和参数数据对车辆进行评分预测，增加了数据的完整性和准确性。

    

    准确的车辆评分预测可以帮助设计和配置好的车辆。本论文提出了一种多模式学习模型，它可以同时从车辆参数、文本描述和图像中学习特征，并预测五种车辆评分，包括总分和评价分数等。

    Accurate vehicle rating prediction can facilitate designing and configuring good vehicles. This prediction allows vehicle designers and manufacturers to optimize and improve their designs in a timely manner, enhance their product performance, and effectively attract consumers. However, most of the existing data-driven methods rely on data from a single mode, e.g., text, image, or parametric data, which results in a limited and incomplete exploration of the available information. These methods lack comprehensive analyses and exploration of data from multiple modes, which probably leads to inaccurate conclusions and hinders progress in this field. To overcome this limitation, we propose a multi-modal learning model for more comprehensive and accurate vehicle rating predictions. Specifically, the model simultaneously learns features from the parametric specifications, text descriptions, and images of vehicles to predict five vehicle rating scores, including the total score, critics score,
    
[^73]: 知识设计：通过知识提炼推动蛋白质设计的极限

    Knowledge-Design: Pushing the Limit of Protein Deign via Knowledge Refinement. (arXiv:2305.15151v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2305.15151](http://arxiv.org/abs/2305.15151)

    本文提出了一种知识感知模块来提炼低质量残基，引入记忆检索机制实现了超过50%的训练时间节省，并取得了较好的性能表现，是蛋白质设计领域的一次创新。

    

    最近的研究表明，在蛋白质设计中，寻找折叠为所期望结构的氨基酸序列已经取得了竞争优势。然而，大多数研究忽略了预测置信度的重要性，未能覆盖广泛的蛋白质空间，并且没有融入常见的蛋白质知识。本文提出了一种知识感知模块来提炼低质量残基，并引入了一种记忆检索机制来节省超过50%的训练时间。我们在CATH、TS50和TS500数据集上对所提出的方法进行了广泛评估，结果显示我们的知识设计方法在CATH数据集上的性能超过了先前的PiFold方法约9％。具体来说，知识设计是第一个实现了...

    Recent studies have shown competitive performance in protein design that aims to find the amino acid sequence folding into the desired structure. However, most of them disregard the importance of predictive confidence, fail to cover the vast protein space, and do not incorporate common protein knowledge. After witnessing the great success of pretrained models on diverse protein-related tasks and the fact that recovery is highly correlated with confidence, we wonder whether this knowledge can push the limits of protein design further. As a solution, we propose a knowledge-aware module that refines low-quality residues. We also introduce a memory-retrieval mechanism to save more than 50\% of the training time. We extensively evaluate our proposed method on the CATH, TS50, and TS500 datasets and our results show that our Knowledge-Design method outperforms the previous PiFold method by approximately 9\% on the CATH dataset. Specifically, Knowledge-Design is the first method that achieves 
    
[^74]: 基于几何多图神经网络的多状态RNA设计

    Multi-State RNA Design with Geometric Multi-Graph Neural Networks. (arXiv:2305.14749v1 [cs.LG])

    [http://arxiv.org/abs/2305.14749](http://arxiv.org/abs/2305.14749)

    本论文提出了一种基于几何多图神经网络的多状态RNA设计方法，可以明确考虑和反映RNA构象多样性在其设计中。其能够显著提高原生序列的恢复，尤其适用于多状态和结构多样化的RNA。

    

    计算RNA设计在合成生物学和治疗开发方面具有广泛的应用。RNA多样的生物学功能的基础是它的构象灵活性，使单一序列能够采用多种不同的三维结构状态。目前，计算生物分子设计任务经常被提出为逆问题，即基于采用单一预期结构构象来设计序列。在这项工作中，我们提出了gRNAde，这是一个基于一组三维RNA骨架结构操作的几何RNA设计流程，以明确考虑和反映RNA构象多样性在其设计中。我们在一个新的大规模三维RNA设计数据集上演示了gRNAde的效用，特别适用于多状态和结构多样化的RNA，能够显著提高原生序列的恢复。我们的代码可在https://github.com/chaitjo/geometric-rna-design上获得。

    Computational RNA design has broad applications across synthetic biology and therapeutic development. Fundamental to the diverse biological functions of RNA is its conformational flexibility, enabling single sequences to adopt a variety of distinct 3D states. Currently, computational biomolecule design tasks are often posed as inverse problems, where sequences are designed based on adopting a single desired structural conformation. In this work, we propose gRNAde, a geometric RNA design pipeline that operates on sets of 3D RNA backbone structures to explicitly account for and reflect RNA conformational diversity in its designs. We demonstrate the utility of gRNAde for improving native sequence recovery over single-state approaches on a new large-scale 3D RNA design dataset, especially for multi-state and structurally diverse RNAs. Our code is available at https://github.com/chaitjo/geometric-rna-design
    
[^75]: 使用基于GPU的并行算法进行图分析：量子聚类

    Graphy Analysis Using a GPU-based Parallel Algorithm: Quantum Clustering. (arXiv:2305.14641v1 [cs.LG])

    [http://arxiv.org/abs/2305.14641](http://arxiv.org/abs/2305.14641)

    本文介绍了一种新方法将量子聚类应用于图结构中，使用基于GPU的并行算法来计算潜在值。实验结果表明该方法具有优越性能。

    

    本文介绍了一种将量子聚类应用于图结构的新方法。量子聚类（QC）是一种新的基于密度的无监督学习方法，通过构建潜在函数来确定聚类中心。在该方法中，我们使用图梯度下降算法来找到聚类中心。GPU并行化用于计算潜在值。我们还对五个广泛使用的数据集进行了实验，并使用四个指标进行了评估。结果显示该方法具有优越的性能。最后，我们讨论了$\sigma$对实验结果的影响。

    The article introduces a new method for applying Quantum Clustering to graph structures. Quantum Clustering (QC) is a novel density-based unsupervised learning method that determines cluster centers by constructing a potential function. In this method, we use the Graph Gradient Descent algorithm to find the centers of clusters. GPU parallelization is utilized for computing potential values. We also conducted experiments on five widely used datasets and evaluated using four indicators. The results show superior performance of the method. Finally, we discuss the influence of $\sigma$ on the experimental results.
    
[^76]: 带有音频光谱变换器的 Patch-Mix 对比学习在呼吸音分类中的应用

    Patch-Mix Contrastive Learning with Audio Spectrogram Transformer on Respiratory Sound Classification. (arXiv:2305.14032v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2305.14032](http://arxiv.org/abs/2305.14032)

    本研究提出了一种新的通过在音频数据上进行对比学习的方法，在呼吸音分类任务中取得了最先进的性能表现。

    

    呼吸声包含早期诊断致命肺部疾病的重要信息。自 COVID-19 疫情以来，基于电子听诊器的无接触医疗越来越受关注。为此，开发了先进的深度学习模型来诊断肺部疾病；然而，由于医学数据的稀缺，仍然存在挑战。本研究证明了在大规模视觉和音频数据集上预训练的模型可以推广到呼吸音分类任务。此外，我们引入了一种简单的 Patch-Mix 数据增强方法，通过随机混合不同样本之间的补丁，与 Audio Spectrogram Transformer (AST) 相结合。我们进一步提出了一种新颖而有效的 Patch-Mix 对比学习方法，以区分潜在空间中的混合表示。我们的方法在 ICBHI 数据集上取得了最先进的性能，优于先前的最高得分 4.08%。

    Respiratory sound contains crucial information for the early diagnosis of fatal lung diseases. Since the COVID-19 pandemic, there has been a growing interest in contact-free medical care based on electronic stethoscopes. To this end, cutting-edge deep learning models have been developed to diagnose lung diseases; however, it is still challenging due to the scarcity of medical data. In this study, we demonstrate that the pretrained model on large-scale visual and audio datasets can be generalized to the respiratory sound classification task. In addition, we introduce a straightforward Patch-Mix augmentation, which randomly mixes patches between different samples, with Audio Spectrogram Transformer (AST). We further propose a novel and effective Patch-Mix Contrastive Learning to distinguish the mixed representations in the latent space. Our method achieves state-of-the-art performance on the ICBHI dataset, outperforming the prior leading score by an improvement of 4.08%.
    
[^77]: 分布感知的公平性测试生成

    Distribution-aware Fairness Test Generation. (arXiv:2305.13935v1 [cs.CV])

    [http://arxiv.org/abs/2305.13935](http://arxiv.org/abs/2305.13935)

    本文介绍了一种名为DistroFair的分布感知的公平性测试方法，可以从图像分类器中检测到类级别的公平性违规。

    

    本文探讨如何验证图像识别软件中的组公平性。我们提出了一种分布感知的公平性测试方法（称为DistroFair），通过将超出分布范围的对象引入到图像识别器中，通过三种语义保留图像变换 - 对象删除，对象插入和对象旋转来系统性地暴露图像分类器中的类级别公平性违规。我们使用两个知名数据集（CityScapes和MS-COCO）和三个主要的商业图像识别软件（即Amazon Rekognition，Google Cloud Vision和Azure计算机视觉）对DistroFair进行评估。结果显示，DistroFair生成的图像中，约有21％通过真实标准或元测试标准显露出了类级别的公平性违规。

    This work addresses how to validate group fairness in image recognition software. We propose a distribution-aware fairness testing approach (called DistroFair) that systematically exposes class-level fairness violations in image classifiers via a synergistic combination of out-of-distribution (OOD) testing and semantic-preserving image mutation. DistroFair automatically learns the distribution (e.g., number/orientation) of objects in a set of images. Then it systematically mutates objects in the images to become OOD using three semantic-preserving image mutations -- object deletion, object insertion and object rotation. We evaluate DistroFair using two well-known datasets (CityScapes and MS-COCO) and three major, commercial image recognition software (namely, Amazon Rekognition, Google Cloud Vision and Azure Computer Vision). Results show that about 21% of images generated by DistroFair reveal class-level fairness violations using either ground truth or metamorphic oracles. DistroFair 
    
[^78]: 发展用于预测硅酸盐电导率的非线性方程

    Development of Non-Linear Equations for Predicting Electrical Conductivity in Silicates. (arXiv:2305.13519v1 [stat.AP])

    [http://arxiv.org/abs/2305.13519](http://arxiv.org/abs/2305.13519)

    本文发展了一种通过人工神经网络预测电弧炉熔渣电导率的方法，并获得了最佳的人工神经网络模型，对该模型进行了平均绝对误差和标准偏差计算及敏感性分析。

    

    电导率在电弧炉(EAF)中非常重要，而它与熔渣的相互作用导致能量损失和低效率。数学建模有助于理解这种现象的行为，研究者们使用人工神经网络来预测EAF熔渣的电导率。最佳的人工神经网络在隐藏层有100个神经元，使用6个预测变量和一个预测变量电导率。计算了平均绝对误差和绝对误差标准偏差，并进行了敏感性分析来对每个预测变量的影响与预测变量进行相关。

    Electrical conductivity is of fundamental importance in electric arc furnaces (EAF) and the interaction of this phenomenon with the process slag results in energy losses and low optimization. As mathematical modeling helps in understanding the behavior of phenomena and it was used to predict the electrical conductivity of EAF slags through artificial neural networks. The best artificial neural network had 100 neurons in the hidden layer, with 6 predictor variables and the predicted variable, electrical conductivity. Mean absolute error and standard deviation of absolute error were calculated, and sensitivity analysis was performed to correlate the effect of each predictor variable with the predicted variable.
    
[^79]: 文本到SQL的语言模型纠错

    Text-to-SQL Error Correction with Language Models of Code. (arXiv:2305.13073v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13073](http://arxiv.org/abs/2305.13073)

    本论文提出了一种基于从句编辑模型的文本到SQL的语言模型纠错方法，并通过新的SQL查询表示改进了语言模型的精确匹配准确率，提高了2.4-6.5，最多提高4.3个百分点。

    

    尽管文本到SQL解析取得了进展，但当前的语义解析器仍不够准确以实际应用。本文研究如何构建自动文本到SQL纠错模型。我们注意到单词层面的编辑缺乏上下文并且有时不明确，因此提出构建从句编辑模型。此外，虽然大多数代码语言模型没有专门预训练SQL，但它们熟悉Python等编程语言中的常见数据结构和其操作。因此，我们提出了一种新的SQL查询表示及其编辑方法，更符合代码语言模型的预训练语料库。我们的错误纠错模型提高了不同解析器的精确匹配准确率，提高了2.4-6.5，并获得了两个强基线的绝对改进最多4.3个百分点。我们的代码和数据可在https://github.com/OSU-NLP-Group/Auto-SQL-Correction 上找到。

    Despite recent progress in text-to-SQL parsing, current semantic parsers are still not accurate enough for practical use. In this paper, we investigate how to build automatic text-to-SQL error correction models. Noticing that token-level edits are out of context and sometimes ambiguous, we propose building clause-level edit models instead. Besides, while most language models of code are not specifically pre-trained for SQL, they know common data structures and their operations in programming languages such as Python. Thus, we propose a novel representation for SQL queries and their edits that adheres more closely to the pre-training corpora of language models of code. Our error correction model improves the exact set match accuracy of different parsers by 2.4-6.5 and obtains up to 4.3 point absolute improvement over two strong baselines. Our code and data are available at https://github.com/OSU-NLP-Group/Auto-SQL-Correction.
    
[^80]: 借助深度强化学习的贫民窟道路规划

    Road Planning for Slums via Deep Reinforcement Learning. (arXiv:2305.13060v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.13060](http://arxiv.org/abs/2305.13060)

    本文介绍了一种基于深度强化学习的方法，用于自动布局贫民窟道路。通过掩码策略优化，可使可达性提高14.3％，对现有基线方法具有明显改进。

    

    数百万贫民窟居民由于贫民窟内不足的道路基础设施而遭受城市服务无法访问的困境，而贫民窟道路规划对城市的可持续发展至关重要。现有的重组或启发式方法要么耗时，不能推广到不同的贫民窟，要么在可达性和建设成本方面产生次优的道路规划。本文提出了一种基于深度强化学习的方法，用于自动布局贫民窟道路。我们提出了一个通用图模型，用于捕获贫民窟的拓扑结构，并设计了一种新颖的图神经网络，用于选择计划道路的位置。通过掩码策略优化，我们的模型可以生成连接贫民窟地点的道路规划，以最小的建设成本。对不同国家的真实贫民窟进行大量实验验证了我们模型的有效性，可使可达性提高14.3％，对现有基线方法具有明显改进。

    Millions of slum dwellers suffer from poor accessibility to urban services due to inadequate road infrastructure within slums, and road planning for slums is critical to the sustainable development of cities. Existing re-blocking or heuristic methods are either time-consuming which cannot generalize to different slums, or yield sub-optimal road plans in terms of accessibility and construction costs. In this paper, we present a deep reinforcement learning based approach to automatically layout roads for slums. We propose a generic graph model to capture the topological structure of a slum, and devise a novel graph neural network to select locations for the planned roads. Through masked policy optimization, our model can generate road plans that connect places in a slum at minimal construction costs. Extensive experiments on real-world slums in different countries verify the effectiveness of our model, which can significantly improve accessibility by 14.3% against existing baseline metho
    
[^81]: 多智能体真实世界展示中强化学习的自适应行动监督

    Adaptive action supervision in reinforcement learning from real-world multi-agent demonstrations. (arXiv:2305.13030v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.13030](http://arxiv.org/abs/2305.13030)

    本文提出了一种从多智能体场景真实世界展示中进行强化学习的自适应行动监督方法，实现了在复制和推广之间平衡的 RL 模型。

    

    在各种科学和工程领域中，对真实世界生物多智能体进行建模是一个基本问题。强化学习（RL）是在网络空间中生成灵活和多样化行为的强大框架；然而，在建模真实世界生物多智能体时，在源（即真实世界数据）和目标（即 RL 的网络空间）之间存在域差异，并且源环境参数通常是未知的。在本文中，我们提出了一种从多智能体场景的真实世界展示中进行 RL 的自适应行动监督的方法。我们采用结合 RL 和监督学习的方法，通过选择基于动态时间扭曲的演示动作来在 RL 中利用未知源动态的信息。这种方法可以轻松应用于许多现有的神经网络架构，并为我们提供一个在复制和推广之间平衡的 RL 模型。

    Modeling of real-world biological multi-agents is a fundamental problem in various scientific and engineering fields. Reinforcement learning (RL) is a powerful framework to generate flexible and diverse behaviors in cyberspace; however, when modeling real-world biological multi-agents, there is a domain gap between behaviors in the source (i.e., real-world data) and the target (i.e., cyberspace for RL), and the source environment parameters are usually unknown. In this paper, we propose a method for adaptive action supervision in RL from real-world demonstrations in multi-agent scenarios. We adopt an approach that combines RL and supervised learning by selecting actions of demonstrations in RL based on the minimum distance of dynamic time warping for utilizing the information of the unknown source dynamics. This approach can be easily applied to many existing neural network architectures and provide us with an RL model balanced between reproducibility as imitation and generalization ab
    
[^82]: 理解ReLU网络的多阶段优化动态和丰富的非线性行为

    Understanding Multi-phase Optimization Dynamics and Rich Nonlinear Behaviors of ReLU Networks. (arXiv:2305.12467v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.12467](http://arxiv.org/abs/2305.12467)

    本文对于ReLU神经网络通过Gradient Flow训练的两层模型在线性可分数据上进行了完整的理论分析，揭示了整个训练过程中的四个不同阶段，显示出一个从简化到复杂的学习趋势。

    

    ReLU神经网络的训练过程经常表现出复杂的非线性现象。模型的非线性和损失的非凸性为理论分析带来了重大挑战。本文对通过Gradient Flow训练的二层ReLU网络在线性可分数据上进行了完整的理论描述。在这种特定的设置下，我们的分析捕获了从随机初始化到最终收敛的整个优化过程。尽管我们研究的模型和数据相对简单，但我们揭示了整个训练过程中的四个不同阶段，显示出一个从简化到复杂的学习趋势。特定的非线性行为也可以被精确地识别和理论上捕获，例如...

    The training process of ReLU neural networks often exhibits complicated nonlinear phenomena. The nonlinearity of models and non-convexity of loss pose significant challenges for theoretical analysis. Therefore, most previous theoretical works on the optimization dynamics of neural networks focus either on local analysis (like the end of training) or approximate linear models (like Neural Tangent Kernel). In this work, we conduct a complete theoretical characterization of the training process of a two-layer ReLU network trained by Gradient Flow on a linearly separable data. In this specific setting, our analysis captures the whole optimization process starting from random initialization to final convergence. Despite the relatively simple model and data that we studied, we reveal four different phases from the whole training process showing a general simplifying-to-complicating learning trend. Specific nonlinear behaviors can also be precisely identified and captured theoretically, such 
    
[^83]: 通过图形段训练学习大型图形属性预测

    Learning Large Graph Property Prediction via Graph Segment Training. (arXiv:2305.12322v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.12322](http://arxiv.org/abs/2305.12322)

    本文提出了一种名为Graph Segment Training的新方法，通过分治法允许使用恒定的内存消耗来学习大型图形属性预测。该方法被评估在几项大型图形属性预测任务上，表现出优于几个最先进基准的出色性能。

    

    学习预测大型图的属性具有挑战性，因为每个预测都需要整个图的知识，而在训练期间可用的内存量是有限的。在这里，我们提出了一种名为Graph Segment Training（GST）的通用框架，利用分治方法允许使用恒定的内存占用量来学习大型图形属性预测。 GST首先将大型图形划分为段，然后通过训练迭代中仅对几个段进行反向传播。我们通过引入历史嵌入表来改进GST范例，以有效地获取未进行反向传播的段的嵌入。为了减轻历史嵌入的过时性，我们设计了两种新技术。首先，我们微调预测头以修复输入分布移位。其次，我们引入“Stale Embedding Dropout”来在训练期间降低偏差，从而丢弃一些过时的嵌入。我们对大型图形属性预测任务进行了评估，包括化合物分类和蛋白质相互作用预测。我们提出的方法GST-EFD（包含所有技术）优于几个最先进的基准，并在准确性，存储器消耗和运行时效率方面实现了出色的性能。

    Learning to predict properties of large graphs is challenging because each prediction requires the knowledge of an entire graph, while the amount of memory available during training is bounded. Here we propose Graph Segment Training (GST), a general framework that utilizes a divide-and-conquer approach to allow learning large graph property prediction with a constant memory footprint. GST first divides a large graph into segments and then backpropagates through only a few segments sampled per training iteration. We refine the GST paradigm by introducing a historical embedding table to efficiently obtain embeddings for segments not sampled for backpropagation. To mitigate the staleness of historical embeddings, we design two novel techniques. First, we finetune the prediction head to fix the input distribution shift. Second, we introduce Stale Embedding Dropout to drop some stale embeddings during training to reduce bias. We evaluate our complete method GST-EFD (with all the techniques 
    
[^84]: 使Transformer在时间序列预测中再次卓越：通道对齐鲁棒双Transformer

    Make Transformer Great Again for Time Series Forecasting: Channel Aligned Robust Dual Transformer. (arXiv:2305.12095v1 [cs.LG])

    [http://arxiv.org/abs/2305.12095](http://arxiv.org/abs/2305.12095)

    本文提出了一种通道对齐鲁棒双Transformer模型，通过双Transformer结构和鲁棒损失函数的引入，解决了Transformer在时间序列预测中的关键缺点，显著提高了预测精度和效率。

    

    最近的研究表明，深度学习方法，尤其是Transformer和MLP，在时间序列预测方面具有巨大的优势。尽管在NLP和CV方面获得了成功，但许多研究发现，与MLP相比，Transformer在时间序列预测方面的效果不佳。在本文中，我们设计了一种特殊的Transformer，即通道对齐鲁棒双Transformer（CARD），以解决Transformer在时间序列预测中的关键缺点。首先，CARD引入了双Transformer结构，使其能够捕捉信号之间的时间相关性和多个变量在时间上的动态依赖。其次，我们引入了一种用于时间序列预测的鲁棒损失函数，以减轻潜在的过度拟合问题。这种新的损失函数基于预测不确定性加权预测在有限时间内的重要性。我们对多个长期和短期预测数据集进行的评估表明，CARD在精度和效率方面显著优于现有的方法。

    Recent studies have demonstrated the great power of deep learning methods, particularly Transformer and MLP, for time series forecasting. Despite its success in NLP and CV, many studies found that Transformer is less effective than MLP for time series forecasting. In this work, we design a special Transformer, i.e., channel-aligned robust dual Transformer (CARD for short), that addresses key shortcomings of Transformer in time series forecasting. First, CARD introduces a dual Transformer structure that allows it to capture both temporal correlations among signals and dynamical dependence among multiple variables over time. Second, we introduce a robust loss function for time series forecasting to alleviate the potential overfitting issue. This new loss function weights the importance of forecasting over a finite horizon based on prediction uncertainties. Our evaluation of multiple long-term and short-term forecasting datasets demonstrates that CARD significantly outperforms state-of-th
    
[^85]: SneakyPrompt：评估文本生成图像模型安全过滤器的鲁棒性

    SneakyPrompt: Evaluating Robustness of Text-to-image Generative Models' Safety Filters. (arXiv:2305.12082v1 [cs.LG])

    [http://arxiv.org/abs/2305.12082](http://arxiv.org/abs/2305.12082)

    本文提出了第一个自动攻击框架SneakyPrompt，以评估最先进的文本生成图像模型中的安全过滤器的鲁棒性，该框架的关键洞见是搜索备选令牌来绕过安全过滤器。

    

    文本生成图像模型，如Stable Diffusion和DALL$\cdot$E 2等，由于它们在现实世界中的广泛应用而受到广泛关注。文本生成图像模型面临的一个挑战性问题是生成不安全内容，例如与暴力和成人相关的内容。因此，常见做法是部署所谓的安全过滤器，基于文本或图像特征阻止不安全内容。先前的工作研究了此类安全过滤器的可能绕过方式。然而，现有的工作在很大程度上是手动完成并专门针对Stable Diffusion官方的安全过滤器。此外，基于我们的评估，Stable Diffusion的安全过滤器的绕过比率仅为23.51％。在本文中，我们提出了第一个自动攻击框架SneakyPrompt，以评估最先进的文本生成图像模型中现实世界安全过滤器的鲁棒性。我们的关键洞见是搜索备选令牌来绕过安全过滤器。

    Text-to-image generative models such as Stable Diffusion and DALL$\cdot$E 2 have attracted much attention since their publication due to their wide application in the real world. One challenging problem of text-to-image generative models is the generation of Not-Safe-for-Work (NSFW) content, e.g., those related to violence and adult. Therefore, a common practice is to deploy a so-called safety filter, which blocks NSFW content based on either text or image features. Prior works have studied the possible bypass of such safety filters. However, existing works are largely manual and specific to Stable Diffusion's official safety filter. Moreover, the bypass ratio of Stable Diffusion's safety filter is as low as 23.51% based on our evaluation.  In this paper, we propose the first automated attack framework, called SneakyPrompt, to evaluate the robustness of real-world safety filters in state-of-the-art text-to-image generative models. Our key insight is to search for alternative tokens in 
    
[^86]: 高斯过程回归的贝叶斯方法中融入不确定输入

    Bayesian approach to Gaussian process regression with uncertain inputs. (arXiv:2305.11586v1 [cs.LG])

    [http://arxiv.org/abs/2305.11586](http://arxiv.org/abs/2305.11586)

    本文提出了一种新的高斯过程回归技术，通过贝叶斯方法将输入数据的不确定性纳入回归模型预测中。在数值实验中展示了该方法具有普适性和不错的表现。

    

    传统高斯过程回归仅假设模型观测数据的输出具有噪声。然而，在许多科学和工程应用中，由于建模假设、测量误差等因素，观测数据的输入位置可能也存在不确定性。在本文中，我们提出了一种贝叶斯方法，将输入数据的可变性融入到高斯过程回归中。考虑两种可观测量——具有固定输入的噪声污染输出和具有先验分布定义的不确定输入，通过贝叶斯框架估计后验分布以推断不确定的数据位置。然后，利用边际化方法将这些输入的量化不确定性纳入高斯过程预测中。通过几个数值实验，展示了这种新回归技术的有效性，在其中观察到不同水平输入数据不确定性下的普适良好表现。

    Conventional Gaussian process regression exclusively assumes the existence of noise in the output data of model observations. In many scientific and engineering applications, however, the input locations of observational data may also be compromised with uncertainties owing to modeling assumptions, measurement errors, etc. In this work, we propose a Bayesian method that integrates the variability of input data into Gaussian process regression. Considering two types of observables -- noise-corrupted outputs with fixed inputs and those with prior-distribution-defined uncertain inputs, a posterior distribution is estimated via a Bayesian framework to infer the uncertain data locations. Thereafter, such quantified uncertainties of inputs are incorporated into Gaussian process predictions by means of marginalization. The effectiveness of this new regression technique is demonstrated through several numerical examples, in which a consistently good performance of generalization is observed, w
    
[^87]: 图中长尾类别的特征化

    Characterizing Long-Tail Categories on Graphs. (arXiv:2305.09938v1 [cs.LG])

    [http://arxiv.org/abs/2305.09938](http://arxiv.org/abs/2305.09938)

    该研究提出了图上长尾分类的第一个泛化边界，并提出了一种可表征长尾类别的行为并提高机器学习模型在现实世界网络中的泛化性能的新图表示学习框架。

    

    长尾数据分布在许多现实世界的网络中普遍存在，包括金融交易网络、电子商务网络和合作网络。尽管最近取得了成功，但现有的作品主要集中于通过图增强或目标重新加权消除机器学习模型的偏见。然而，目前有限的文献提供理论工具来表征图上长尾类别的行为，并理解实际情况下的泛化性能。为填补这一空白，我们通过将问题形式化为多任务学习的方式，即每个任务对应于预测一个特定的类别，提出了图上长尾分类的第一个泛化边界。我们的理论结果表明，长尾分类的泛化性能受所有任务中的损失范围和任务总数的支配。在理论发现的基础上，我们提出了一种新的图表示学习框架，可表征长尾类别的行为，并提高机器学习模型在现实世界网络中的泛化性能。

    Long-tail data distributions are prevalent in many real-world networks, including financial transaction networks, e-commerce networks, and collaboration networks. Despite the success of recent developments, the existing works mainly focus on debiasing the machine learning models via graph augmentation or objective reweighting. However, there is limited literature that provides a theoretical tool to characterize the behaviors of long-tail categories on graphs and understand the generalization performance in real scenarios. To bridge this gap, we propose the first generalization bound for long-tail classification on graphs by formulating the problem in the fashion of multi-task learning, i.e., each task corresponds to the prediction of one particular category. Our theoretical results show that the generalization performance of long-tail classification is dominated by the range of losses across all tasks and the total number of tasks. Building upon the theoretical findings, we propose a n
    
[^88]: 时间延迟多特征相关分析以提取脑电信号中微小依赖性

    Time delay multi-feature correlation analysis to extract subtle dependencies from EEG signals. (arXiv:2305.09478v1 [eess.SP])

    [http://arxiv.org/abs/2305.09478](http://arxiv.org/abs/2305.09478)

    本文针对脑电信号的复杂性，提出使用多特征相关分析方法来自动分解和提取多种类型的统计依赖关系。其中，PCA降维技术用于找到占主导的依赖关系方向，从而提取脑电信号中微小依赖性。

    

    脑电图（EEG）信号是极其复杂的脑活动的结果。通过例如一对电极信号在不同时间延迟（滞后$\Delta$t）下的联合分布（$\rho_{\Delta t}$）可以访问这种隐藏动态的某些细节。标准方法是监视这样的联合分布的单一评估，例如Pearson相关（或互信息），然而这种方法结果通常相对不太有趣。相比之下，这样的复杂信号可能由多种类型的统计依赖关系构成。本文提出了一种自动分解和提取这些依赖关系的方法。 具体来说，我们将这样的联合分布建模为所有考虑的滞后依赖关系的多项式估计，然后通过PCA降维找到占主导的依赖关系方向$f_v$。这样我们得到一些滞后依赖特征$a_i(\Delta t)$，用于描述各个滞后的统计依赖关系及其重要性。

    Electroencephalography (EEG) signals are resultants of extremely complex brain activity. Some details of this hidden dynamics might be accessible through e.g. joint distributions $\rho_{\Delta t}$ of signals of pairs of electrodes shifted by various time delays (lag $\Delta t$). A standard approach is monitoring a single evaluation of such joint distributions, like Pearson correlation (or mutual information), which turns out relatively uninteresting as expected, there is usually a small peak for zero delay and nearly symmetric drop with delay. In contrast, such a complex signal might be composed of multiple types of statistical dependencies - this article proposes approach to automatically decompose and extract them. Specifically, we model such joint distributions as polynomials estimated for all considered lag dependencies, then with PCA dimensionality reduction find dominant dependency directions $f_v$. This way we get a few lag dependent features $a_i(\Delta t)$ describing separat
    
[^89]: 为协变量漂移自适应引入双重加权方法

    Double-Weighting for Covariate Shift Adaptation. (arXiv:2305.08637v1 [stat.ML])

    [http://arxiv.org/abs/2305.08637](http://arxiv.org/abs/2305.08637)

    本文提出了一种双重加权的最小极大风险分类方法，可以有效避免协变量漂移对监督学习的影响。

    

    监督学习中常常受到协变量漂移影响，即训练样本和测试样本的实例边缘分布不同但标签条件相同。现有方法通过使用比率p_te（x）/p_tr（x）对训练样本进行加权（重新加权方法），或者使用比率p_tr（x）/p_te（x）对测试样本进行加权（鲁棒方法）来解决这种协变量漂移。然而，在支持不匹配或上述比率取大值时，这些方法的性能可能很差。我们提出了一种最小极大风险分类(MRC)方法，通过对训练样本和测试样本进行加权来避免这种限制。此外，我们开发了有效的技术来获得两组加权，并推广了传统的核均值匹配方法。我们提供了新的生成模型和实际数据集上的实验结果来证明我们方法的优越性。

    Supervised learning is often affected by a covariate shift in which the marginal distributions of instances (covariates $x$) of training and testing samples $\mathrm{p}_\text{tr}(x)$ and $\mathrm{p}_\text{te}(x)$ are different but the label conditionals coincide. Existing approaches address such covariate shift by either using the ratio $\mathrm{p}_\text{te}(x)/\mathrm{p}_\text{tr}(x)$ to weight training samples (reweighting methods) or using the ratio $\mathrm{p}_\text{tr}(x)/\mathrm{p}_\text{te}(x)$ to weight testing samples (robust methods). However, the performance of such approaches can be poor under support mismatch or when the above ratios take large values. We propose a minimax risk classification (MRC) approach for covariate shift adaptation that avoids such limitations by weighting both training and testing samples. In addition, we develop effective techniques that obtain both sets of weights and generalize the conventional kernel mean matching method. We provide novel genera
    
[^90]: 均值漂移的收敛性分析

    Convergence Analysis of Mean Shift. (arXiv:2305.08463v1 [stat.ML])

    [http://arxiv.org/abs/2305.08463](http://arxiv.org/abs/2305.08463)

    本研究提出了均值漂移算法的模估计序列的收敛保证，并扩展了现有的涵盖解析核和Epanechnikov核的发现，意义在于涵盖了在基于KDE的模估计的渐近统计效率方面最优的非负核——双重核。

    

    均值漂移（MS）算法寻找核密度估计（KDE）的模。本研究提出了一种由MS算法产生的模估计序列的收敛保证，并在相当温和的条件下，借助于关于{\L}ojasiewicz不等式的论证，评估了收敛速度。我们的发现扩展了现有的涵盖解析核和Epanechnikov核的发现，意义在于涵盖了在基于KDE的模估计的渐近统计效率方面最优的非负核——双重核。

    The mean shift (MS) algorithm seeks a mode of the kernel density estimate (KDE). This study presents a convergence guarantee of the mode estimate sequence generated by the MS algorithm and an evaluation of the convergence rate, under fairly mild conditions, with the help of the argument concerning the {\L}ojasiewicz inequality. Our findings, which extend existing ones covering analytic kernels and the Epanechnikov kernel, are significant in that they cover the biweight kernel that is optimal among non-negative kernels in terms of the asymptotic statistical efficiency for the KDE-based mode estimation.
    
[^91]: 训练生成对抗网络的梯度下降-上升算法的局部收敛性研究

    Local Convergence of Gradient Descent-Ascent for Training Generative Adversarial Networks. (arXiv:2305.08277v1 [cs.LG])

    [http://arxiv.org/abs/2305.08277](http://arxiv.org/abs/2305.08277)

    本论文研究了使用基于核函数的鉴别器训练GAN的梯度下降-上升算法的局部收敛性，揭示了学习率、正则化和带宽对其影响，同时展示了收敛、振荡或发散的相变现象。

    

    生成对抗网络（GAN）是一种流行的复杂高维数据生成模型的训练方法。训练GAN的标准方法涉及对极小-极大优化问题进行梯度下降-上升（GDA）过程。由于动态的非线性性质，该过程通常很难分析。本研究重点研究了使用基于核函数的鉴别器训练GAN时的GDA局部动态。该收敛性分析是在[Becker et al. 2022]的“孤立点模型”假设下，对描述GDA迭代的非线性动力学系统进行线性化得到的。我们的分析揭示了学习率、正则化和核判别器的带宽对GDA局部收敛速度的影响。重要的是，我们展示了相变现象，表明系统何时收敛、振荡或发散。我们还提供了验证我们结论的数值模拟。

    Generative Adversarial Networks (GANs) are a popular formulation to train generative models for complex high dimensional data. The standard method for training GANs involves a gradient descent-ascent (GDA) procedure on a minimax optimization problem. This procedure is hard to analyze in general due to the nonlinear nature of the dynamics. We study the local dynamics of GDA for training a GAN with a kernel-based discriminator. This convergence analysis is based on a linearization of a non-linear dynamical system that describes the GDA iterations, under an \textit{isolated points model} assumption from [Becker et al. 2022]. Our analysis brings out the effect of the learning rates, regularization, and the bandwidth of the kernel discriminator, on the local convergence rate of GDA. Importantly, we show phase transitions that indicate when the system converges, oscillates, or diverges. We also provide numerical simulations that verify our claims.
    
[^92]: 基于随机池化的可证明多实例深度AUC最大化方法

    Provable Multi-instance Deep AUC Maximization with Stochastic Pooling. (arXiv:2305.08040v1 [cs.LG])

    [http://arxiv.org/abs/2305.08040](http://arxiv.org/abs/2305.08040)

    本文提出了在多实例学习中使用深度AUC最大化（DAM）的方法，并根据包含大量实例的情况下训练的计算挑战，提出了一种基于方差减少的随机池化方法，使得只需对每个包进行少量采样即可计算MIDAM模型，提高了效率和准确性。

    

    本文提出了一种深度AUC最大化（DAM）的新型应用，用于多实例学习（MIL），其中将单个类标签分配给一组实例（例如，患者的多个CT扫描的多个2D切片）。我们在DAM的背景下解决了MIL中被忽略但非常重要的计算挑战，即包大小过大，无法在反向传播时加载到GPU内存中，这是MIL标准池化方法所必需的。为了解决这个问题，我们提出了一种基于方差减少的随机池化方法，这种方法可以将关于汇聚预测的损失函数构造为多级组合函数。通过综合随机组合优化和非凸极小最大优化技术，我们提出了一种统一且可证明的多实例DAM（MIDAM）算法，其使用随机平滑最大池化或随机注意力池化，仅对每个包对应的实例进行少量采样来计算 sto。

    This paper considers a novel application of deep AUC maximization (DAM) for multi-instance learning (MIL), in which a single class label is assigned to a bag of instances (e.g., multiple 2D slices of a CT scan for a patient). We address a neglected yet non-negligible computational challenge of MIL in the context of DAM, i.e., bag size is too large to be loaded into {GPU} memory for backpropagation, which is required by the standard pooling methods of MIL. To tackle this challenge, we propose variance-reduced stochastic pooling methods in the spirit of stochastic optimization by formulating the loss function over the pooled prediction as a multi-level compositional function. By synthesizing techniques from stochastic compositional optimization and non-convex min-max optimization, we propose a unified and provable muli-instance DAM (MIDAM) algorithm with stochastic smoothed-max pooling or stochastic attention-based pooling, which only samples a few instances for each bag to compute a sto
    
[^93]: 对“去卷积”定义的范畴论元分析

    A Category-theoretical Meta-analysis of Definitions of Disentanglement. (arXiv:2305.06886v1 [cs.LG])

    [http://arxiv.org/abs/2305.06886](http://arxiv.org/abs/2305.06886)

    本文提出了一个存在的去卷积定义的范畴论元分析，将笛卡儿积和幺模积的概念应该构成去卷积的核心，并展现了处理函数、等变映射、关系和随机映射的相似性和关键区别。

    

    在机器学习中，将数据的变化因素分离是一个基本概念，并且不同的研究人员以各种方式研究它，导致了众多的定义。尽管有许多经验研究，但我们仍需要更多的理论研究来充分理解去卷积的定义属性，以及不同的定义之间的关系。本文提出了一个存在的去卷积定义的范畴论元分析，将范畴论作为一个统一而严谨的框架。我们提出笛卡儿积和幺模积的概念应该构成去卷积的核心。有了这些核心概念，我们展示了处理（i）函数，（ii）等变映射，（iii）关系和（iv）随机映射的相似性和关键区别。总的来说，我们的范畴论元分析深化了我们对去卷积及其不同制定的理解，可以帮助研究人员在不同的定义之间进行导航，并选择最合适的定义。

    Disentangling the factors of variation in data is a fundamental concept in machine learning and has been studied in various ways by different researchers, leading to a multitude of definitions. Despite the numerous empirical studies, more theoretical research is needed to fully understand the defining properties of disentanglement and how different definitions relate to each other. This paper presents a meta-analysis of existing definitions of disentanglement, using category theory as a unifying and rigorous framework. We propose that the concepts of the cartesian and monoidal products should serve as the core of disentanglement. With these core concepts, we show the similarities and crucial differences in dealing with (i) functions, (ii) equivariant maps, (iii) relations, and (iv) stochastic maps. Overall, our meta-analysis deepens our understanding of disentanglement and its various formulations and can help researchers navigate different definitions and choose the most appropriate o
    
[^94]: 滞后多因子模型中领先滞后关系的鲁棒检测

    Robust Detection of Lead-Lag Relationships in Lagged Multi-Factor Models. (arXiv:2305.06704v1 [stat.ML])

    [http://arxiv.org/abs/2305.06704](http://arxiv.org/abs/2305.06704)

    该论文提出了一种基于聚类的鲁棒检测滞后多因子模型中的领先滞后关系方法，并使用各种聚类技术和相似度度量方法实现了对领先滞后估计的聚合，从而强化了对原始宇宙中的一致关系的识别。

    

    在多元时间序列系统中，通过发现数据中固有的领先滞后关系，可以获得关键信息，这指的是两个相对时间互移的时间序列之间的依赖关系，可以用于控制、预测或聚类。我们开发了一种基于聚类的方法，用于鲁棒检测滞后多因子模型中的领先滞后关系。在我们的框架中，所设想的管道接收一组时间序列作为输入，并使用滑动窗口方法从每个输入时间序列中提取一组子序列时间序列。然后，我们应用各种聚类技术（例如K-means++和谱聚类），采用各种成对相似性度量，包括非线性的相似性度量。一旦聚类被提取出来，跨聚类的领先滞后估计被聚合起来，以增强对原始宇宙中一致关系的识别。由于多

    In multivariate time series systems, key insights can be obtained by discovering lead-lag relationships inherent in the data, which refer to the dependence between two time series shifted in time relative to one another, and which can be leveraged for the purposes of control, forecasting or clustering. We develop a clustering-driven methodology for the robust detection of lead-lag relationships in lagged multi-factor models. Within our framework, the envisioned pipeline takes as input a set of time series, and creates an enlarged universe of extracted subsequence time series from each input time series, by using a sliding window approach. We then apply various clustering techniques (e.g, K-means++ and spectral clustering), employing a variety of pairwise similarity measures, including nonlinear ones. Once the clusters have been extracted, lead-lag estimates across clusters are aggregated to enhance the identification of the consistent relationships in the original universe. Since multi
    
[^95]: 带概率态射和核平均嵌入的监督学习

    Supervised learning with probabilistic morphisms and kernel mean embeddings. (arXiv:2305.06348v1 [math.ST])

    [http://arxiv.org/abs/2305.06348](http://arxiv.org/abs/2305.06348)

    本文提出了监督学习中正确损失函数的概念，其通过概率测度的条件正则概率测度解决线性算子方程的问题得到定义，适用于可测空间的输入空间和标签空间。

    

    本文提出了一个监督学习的生成模型中正确损失函数的概念，适用于可测空间的输入空间X和标签空间Y。 生成模型中的正确损失函数必须正确地度量可能预测器的假设空间H中的元素与监管运算符之间的差异，而监管运算符可能不属于H。 为了定义正确的损失函数，本文提出了一个关于概率测度μ在投影ΠX：X×Y→X相对于概率测度μ𝑋×𝑌的条件正则概率测度μY| X的特殊性质的表征方法，作为线性算子方程的解决方案。 如果Y是一个具有Borel σ-代数 BY的可分的可度量化拓扑空间，则提出了关于概率测度μ相对于投影ΠX的条件正则概率测度μY| X的另一种特殊性质的表征方法。

    In this paper I propose a concept of a correct loss function in a generative model of supervised learning for an input space $\mathcal{X}$ and a label space $\mathcal{Y}$, which are measurable spaces. A correct loss function in a generative model of supervised learning must correctly measure the discrepancy between elements of a hypothesis space $\mathcal{H}$ of possible predictors and the supervisor operator, which may not belong to $\mathcal{H}$. To define correct loss functions, I propose a characterization of a regular conditional probability measure $\mu_{\mathcal{Y}|\mathcal{X}}$ for a probability measure $\mu$ on $\mathcal{X} \times \mathcal{Y}$ relative to the projection $\Pi_{\mathcal{X}}: \mathcal{X}\times\mathcal{Y}\to \mathcal{X}$ as a solution of a linear operator equation. If $\mathcal{Y}$ is a separable metrizable topological space with the Borel $\sigma$-algebra $ \mathcal{B} (\mathcal{Y})$, I propose another characterization of a regular conditional probability measure
    
[^96]: 图神经网络的特征扩展

    Feature Expansion for Graph Neural Networks. (arXiv:2305.06142v1 [cs.LG])

    [http://arxiv.org/abs/2305.06142](http://arxiv.org/abs/2305.06142)

    本文通过分析图神经网络中的特征空间，提出了特征子空间展开和结构主成分两种方法来扩展特征空间，从而获得更好的结果。

    

    图神经网络旨在学习图结构数据的表示，并展现出令人瞩目的性能，尤其在节点分类方面。然而，支配表示学习的特征空间在图神经网络中尚未被系统地研究。本文提出通过分析空间模型和谱模型的特征空间来填补这一空白。我们将图神经网络分解为确定的特征空间和可训练的权重，从而通过矩阵空间分析明确地研究特征空间。特别地，我们在理论上发现，由于重复聚合，特征空间倾向于线性相关。基于这些发现，我们提出1）特征子空间展开和2）结构主成分来扩展特征空间。广泛的实验验证了我们的方法，取得了更好的结果。

    Graph neural networks aim to learn representations for graph-structured data and show impressive performance, particularly in node classification. Recently, many methods have studied the representations of GNNs from the perspective of optimization goals and spectral graph theory. However, the feature space that dominates representation learning has not been systematically studied in graph neural networks. In this paper, we propose to fill this gap by analyzing the feature space of both spatial and spectral models. We decompose graph neural networks into determined feature spaces and trainable weights, providing the convenience of studying the feature space explicitly using matrix space analysis. In particular, we theoretically find that the feature space tends to be linearly correlated due to repeated aggregations. Motivated by these findings, we propose 1) feature subspaces flattening and 2) structural principal components to expand the feature space. Extensive experiments verify the 
    
[^97]: Vcc: 通过优先处理重要标记将Transformer扩展到128K令牌或更多

    Vcc: Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens. (arXiv:2305.04241v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.04241](http://arxiv.org/abs/2305.04241)

    这篇论文提出了一种新的方法 VCC，通过优先处理最重要的VIP标记，一定程度上压缩序列，从而使Transformer模型可处理长度更长的序列。

    

    Transformer模型在现代自然语言处理和计算机视觉应用中扮演关键角色。尽管近年来有人致力于降低这些模型的二次成本（作为序列长度的函数），但处理超长序列（如超过16K标记）仍然具有挑战性。本文提出了一种将Ultra long sequences的Transformer模型的效率显着提高的方法，即在每层将序列压缩为更小的表示，通过利用许多任务中仅有的少数的特殊标记（我们称其为VIP标记）与最终预测结果最相关的这个事实，我们提出了基于VIP标记的压缩方法，即VIP标记中心压缩（VCC）方案，该方案根据其对近似VIP标记表示的影响有选择地压缩序列。

    Transformers are central in modern natural language processing and computer vision applications. Despite recent works devoted to reducing the quadratic cost of such models (as a function of the sequence length), dealing with ultra long sequences (e.g., with more than 16K tokens) remains challenging. Applications such as answering questions based on a book or summarizing a scientific article are inefficient or infeasible. Here, we propose to significantly improve the efficiency of Transformers for ultra long sequences, by compressing the sequence into a much smaller representation at each layer. Specifically, by exploiting the fact that in many tasks, only a small subset of special tokens (we call VIP-tokens) are most relevant to the final prediction, we propose a VIP-token centric compression (VCC) scheme which selectively compresses the sequence based on their impact on approximating the representation of the VIP-tokens. Compared with competitive baselines, our algorithm is not only e
    
[^98]: 数据集压缩综合研究：性能、隐私、鲁棒性以及公平性

    A Comprehensive Study on Dataset Distillation: Performance, Privacy, Robustness and Fairness. (arXiv:2305.03355v1 [cs.LG])

    [http://arxiv.org/abs/2305.03355](http://arxiv.org/abs/2305.03355)

    本研究对当前最先进的数据集压缩方法进行了全面评估，发现其存在隐私风险并可能放大模型的不公平性，提供了大规模的基准测试框架。

    

    数据集压缩旨在将原始数据集的丰富特征编码成小型数据集，是一种加速神经网络训练和相关研究的有前途的方法。已经提出了不同的方法来改善压缩图像的信息性和泛化性能。然而，目前还没有从安全性角度全面分析这一技术的工作，并且对潜在风险缺乏系统理解。在本文中，我们进行了大量实验，评估了当前最先进的数据集压缩方法。我们成功使用成员推理攻击来显示仍然存在隐私风险。本文还表明，数据集压缩在模型鲁棒性方面可能会产生不同程度的影响，并在进行预测时放大类别间的模型不公平性。本研究为数据集压缩评估提供了大规模的基准测试框架。

    The aim of dataset distillation is to encode the rich features of an original dataset into a tiny dataset. It is a promising approach to accelerate neural network training and related studies. Different approaches have been proposed to improve the informativeness and generalization performance of distilled images. However, no work has comprehensively analyzed this technique from a security perspective and there is a lack of systematic understanding of potential risks. In this work, we conduct extensive experiments to evaluate current state-of-the-art dataset distillation methods. We successfully use membership inference attacks to show that privacy risks still remain. Our work also demonstrates that dataset distillation can cause varying degrees of impact on model robustness and amplify model unfairness across classes when making predictions. This work offers a large-scale benchmarking framework for dataset distillation evaluation.
    
[^99]: 通过逆向神经渲染对动态场景进行物体中心体素化

    Object-Centric Voxelization of Dynamic Scenes via Inverse Neural Rendering. (arXiv:2305.00393v1 [cs.CV])

    [http://arxiv.org/abs/2305.00393](http://arxiv.org/abs/2305.00393)

    本文提出了一个逆向神经渲染框架DynaVol，可以在多实体动态场景中学习时间变化的体积表示，通过维护一个时间依赖的3D格点和联合学习格点级局部动态、物体级全局动态和组合神经辐射场来增强物体中心场景体素化的时空一致性。

    

    在无监督的3D场景中理解世界的组成动态非常具有挑战性。现有的方法要么未能有效利用时间线索，要么忽略了场景分解的多视角一致性。本文提出了DynaVol，一种逆向神经渲染框架，为多实体（如物体）的动态场景学习时间变化的体积表示提供了一个学习方法。它的主要贡献有两个。首先，它维护一个时间依赖的3D格点，动态而灵活地将空间位置绑定到不同的实体，从而在代表性水平上鼓励信息的分离。其次，我们的方法在端到端架构中联合学习格点级局部动态、物体级全局动态和组合神经辐射场，从而增强了物体中心场景体素化的时空一致性。我们提出了一个两阶段的DynaVol训练方案，并在合成和真实世界数据集上验证了它的有效性。

    Understanding the compositional dynamics of the world in unsupervised 3D scenarios is challenging. Existing approaches either fail to make effective use of time cues or ignore the multi-view consistency of scene decomposition. In this paper, we propose DynaVol, an inverse neural rendering framework that provides a pilot study for learning time-varying volumetric representations for dynamic scenes with multiple entities (like objects). It has two main contributions. First, it maintains a time-dependent 3D grid, which dynamically and flexibly binds the spatial locations to different entities, thus encouraging the separation of information at a representational level. Second, our approach jointly learns grid-level local dynamics, object-level global dynamics, and the compositional neural radiance fields in an end-to-end architecture, thereby enhancing the spatiotemporal consistency of object-centric scene voxelization. We present a two-stage training scheme for DynaVol and validate its ef
    
[^100]: 多类分类中敌对训练解的存在性研究

    On the existence of solutions to adversarial training in multiclass classification. (arXiv:2305.00075v1 [cs.LG])

    [http://arxiv.org/abs/2305.00075](http://arxiv.org/abs/2305.00075)

    本文研究了多类分类中敌对训练的鲁棒解存在性问题，证明了每个模型中存在 Borel 可测的鲁棒分类器，并与最优传输和总变差正则化建立了联系。在二元分类问题中，对不可知分类器的敌对训练问题存在 Borel 可测的解。

    

    本文研究了敌对训练在多类分类问题中的三种模型，旨在构建对抗扰动下鲁棒的分类器。我们证明了每个模型中存在 Borel 可测的鲁棒分类器，并提供了敌对训练问题的统一视角，拓展了作者之前的最优传输联系，并在多类情况下敌对训练和总变差正则化之间建立了新的联系。作为我们结果的推论，我们证明了在二元分类设置中，对不可知分类器的敌对训练问题存在 Borel 可测的解，这一结果改进了关于敌对训练的文献，文献中仅已知只有在特征空间的扩大通用 $σ$-代数内存在鲁棒的分类器。

    We study three models of the problem of adversarial training in multiclass classification designed to construct robust classifiers against adversarial perturbations of data in the agnostic-classifier setting. We prove the existence of Borel measurable robust classifiers in each model and provide a unified perspective of the adversarial training problem, expanding the connections with optimal transport initiated by the authors in previous work and developing new connections between adversarial training in the multiclass setting and total variation regularization. As a corollary of our results, we prove the existence of Borel measurable solutions to the agnostic adversarial training problem in the binary classification setting, a result that improves results in the literature of adversarial training, where robust classifiers were only known to exist within the enlarged universal $\sigma$-algebra of the feature space.
    
[^101]: 图神经网络何时对节点分类有帮助：研究同源性原则对节点可区分性的影响

    When Do Graph Neural Networks Help with Node Classification: Investigating the Homophily Principle on Node Distinguishability. (arXiv:2304.14274v1 [cs.SI])

    [http://arxiv.org/abs/2304.14274](http://arxiv.org/abs/2304.14274)

    同源性原则不一定是影响图神经网络优越性的唯一原因；本文提出Contextual Stochastic Block Model for Homophily (CSBM-H)以深入研究同源性对节点可区分性的影响。

    

    同源性原则指相同类别的节点更有可能连接在一起，一直被认为是图神经网络（GNN）在节点分类（NC）任务上性能优越的主要原因。最近，人们提出理论结果认为，即使同源性原则被打破，只要来自同一类别的节点分享相似的邻居模式，GNN的优势仍然存在，这对同源性的有效性提出了质疑。然而，这个论点仅考虑了同类节点的可区分性，忽略了跨类别的可区分性，这是研究同源性效应的不足之处。在本文中，我们首先通过例子证明了上述不足，并认为可区分性的理想情况是同类节点的可区分性小于跨类别节点的可区分性。为了形式化这个想法，更好地理解同源性，我们提出了Contextual Stochastic Block Model for Homophily (CSBM-H)，并进行了全面的实验分析。

    Homophily principle, i.e. nodes with the same labels are more likely to be connected, was believed to be the main reason for the performance superiority of Graph Neural Networks (GNNs) over Neural Networks (NNs) on Node Classification (NC) tasks. Recently, people have developed theoretical results arguing that, even though the homophily principle is broken, the advantage of GNNs can still hold as long as nodes from the same class share similar neighborhood patterns, which questions the validity of homophily. However, this argument only considers intra-class Node Distinguishability (ND) and ignores inter-class ND, which is insufficient to study the effect of homophily. In this paper, we first demonstrate the aforementioned insufficiency with examples and argue that an ideal situation for ND is to have smaller intra-class ND than inter-class ND. To formulate this idea and have a better understanding of homophily, we propose Contextual Stochastic Block Model for Homophily (CSBM-H) and def
    
[^102]: FedVS: 面向分割模型的容错和隐私保护垂直联邦学习

    FedVS: Straggler-Resilient and Privacy-Preserving Vertical Federated Learning for Split Models. (arXiv:2304.13407v1 [cs.LG])

    [http://arxiv.org/abs/2304.13407](http://arxiv.org/abs/2304.13407)

    该论文提出FedVS，一种同时解决垂直联邦学习中滞后客户端和数据泄露问题的方法，通过设计本地数据和模型的秘密共享方案，以保证信息理论隐私，并通过解密计算股份，无损重构所有客户端的嵌入的汇总。

    

    在一个由中央服务器和许多分布式客户端组成的垂直联邦学习系统中，训练数据被垂直分割，不同的特征存储在不同的客户端上。分割垂直联邦学习的问题是训练一个在服务器和客户端之间划分的模型。本文旨在解决分割垂直联邦学习中的两个主要挑战：1）由于训练过程中存在迟滞的客户端造成的性能下降；2）客户端上传数据嵌入导致的数据和模型隐私泄露。我们提出了FedVS来同时解决这两个挑战。FedVS的关键思想是设计本地数据和模型的秘密共享方案，从而保证针对勾结客户和好奇服务器的信息理论隐私，并且通过解密计算股份，无损重构所有客户端的嵌入的汇总。在各种类型的VFL数据集（包括表格，CV，图像，NLP）上进行了广泛的实验，证明了FedVS的有效性。

    In a vertical federated learning (VFL) system consisting of a central server and many distributed clients, the training data are vertically partitioned such that different features are privately stored on different clients. The problem of split VFL is to train a model split between the server and the clients. This paper aims to address two major challenges in split VFL: 1) performance degradation due to straggling clients during training; and 2) data and model privacy leakage from clients' uploaded data embeddings. We propose FedVS to simultaneously address these two challenges. The key idea of FedVS is to design secret sharing schemes for the local data and models, such that information-theoretical privacy against colluding clients and curious server is guaranteed, and the aggregation of all clients' embeddings is reconstructed losslessly, via decrypting computation shares from the non-straggling clients. Extensive experiments on various types of VFL datasets (including tabular, CV, a
    
[^103]: 交替局部枚举(TnALE): 用较少的评估解决张量网络结构搜索问题

    Alternating Local Enumeration (TnALE): Solving Tensor Network Structure Search with Fewer Evaluations. (arXiv:2304.12875v1 [cs.LG])

    [http://arxiv.org/abs/2304.12875](http://arxiv.org/abs/2304.12875)

    提出了TnALE算法，通过交替局部枚举更新每个与结构相关的变量，大大减少了评估次数，用于解决张量网络结构搜索问题。在理论上证明，如果在每个邻域中达到了足够的目标函数降低，TnALE和TNLS都可以实现线性收敛度，直到一个常数。同时，与TNLS相比， TnALE需要更少的评估次数。

    

    张量网络(TN)是机器学习中强大的框架，但选择一个好的TN模型，即TN结构搜索(TN-SS)，是一项具有挑战性和计算密集型的任务。最近的方法TNLS ~ \cite {li2022permutation} 在这个任务中显示出了有希望的结果，但它的计算效率仍然是无法承受的，需要太多评估目标函数的次数。我们提出了TnALE，一种新的算法，通过局部枚举交替更新每个与结构相关的变量，与TNLS相比，大大减少了评估次数。我们从理论上研究了TNLS和TnALE的下降步骤，证明如果在每个邻域中达到了足够的目标函数降低，那么两种算法都可以实现线性收敛度，直到一个常数。我们还比较了TNLS和TnALE的评估效率，揭示了在TNLS中通常需要Ω(2 ^ N)个评估才能在邻域内达到目标降低。

    Tensor network (TN) is a powerful framework in machine learning, but selecting a good TN model, known as TN structure search (TN-SS), is a challenging and computationally intensive task. The recent approach TNLS~\cite{li2022permutation} showed promising results for this task, however, its computational efficiency is still unaffordable, requiring too many evaluations of the objective function. We propose TnALE, a new algorithm that updates each structure-related variable alternately by local enumeration, \emph{greatly} reducing the number of evaluations compared to TNLS. We theoretically investigate the descent steps for TNLS and TnALE, proving that both algorithms can achieve linear convergence up to a constant if a sufficient reduction of the objective is \emph{reached} in each neighborhood. We also compare the evaluation efficiency of TNLS and TnALE, revealing that $\Omega(2^N)$ evaluations are typically required in TNLS for \emph{reaching} the objective reduction in the neighborhood
    
[^104]: PowerGAN: 一种基于机器学习的计算内存加速器功耗副信道攻击技术

    PowerGAN: A Machine Learning Approach for Power Side-Channel Attack on Compute-in-Memory Accelerators. (arXiv:2304.11056v1 [cs.CR])

    [http://arxiv.org/abs/2304.11056](http://arxiv.org/abs/2304.11056)

    本文提出了一种基于机器学习的计算内存加速器功耗副信道攻击技术，可以在大噪声和对抗措施存在的情况下重构用户的私有输入数据。

    

    随着深度神经网络（DNN）的不断发展，使用模拟计算内存（CIM）加速器进行DNN推理的能源效率和区块向量乘法能力变得越来越受欢迎。然而，保护用户输入隐私越来越重要。在本文中，我们发现了一种安全漏洞，即在一个适当的数据采集和预处理下，即使没有DNN模型的知识，攻击者也可以从功耗侧信道攻击重构用户的私有输入数据。我们进一步展示了一种基于机器学习的攻击方法，使用生成对抗网络（GAN）来增强重构。我们的结果表明，即使在大噪声水平和对抗措施被应用的情况下，该攻击方法在从模拟CIM加速器功耗泄漏重构用户输入方面是有效的。具体而言，我们展示了我们的方法在磁共振成像（MRI）数据中用于脑肿瘤检测的U-Net上的功效。

    Analog compute-in-memory (CIM) accelerators are becoming increasingly popular for deep neural network (DNN) inference due to their energy efficiency and in-situ vector-matrix multiplication (VMM) capabilities. However, as the use of DNNs expands, protecting user input privacy has become increasingly important. In this paper, we identify a security vulnerability wherein an adversary can reconstruct the user's private input data from a power side-channel attack, under proper data acquisition and pre-processing, even without knowledge of the DNN model. We further demonstrate a machine learning-based attack approach using a generative adversarial network (GAN) to enhance the reconstruction. Our results show that the attack methodology is effective in reconstructing user inputs from analog CIM accelerator power leakage, even when at large noise levels and countermeasures are applied. Specifically, we demonstrate the efficacy of our approach on the U-Net for brain tumor detection in magnetic
    
[^105]: 基于匹配的生成模型数据估值方法

    Matching-based Data Valuation for Generative Model. (arXiv:2304.10701v1 [cs.CV])

    [http://arxiv.org/abs/2304.10701](http://arxiv.org/abs/2304.10701)

    本论文提出了基于匹配的生成模型数据估值方法，这是一个针对任何生成模型的模型无关方法，可以对数据实例进行估值，而无需重新训练模型，并在估值效果上表现出色。

    

    数据估值对于机器学习非常重要，因为它有助于增强模型的透明度并保护数据特性。现有的数据估值方法主要集中在判别模型上，忽略了最近吸引了大量关注的深度生成模型。与判别模型类似，需要评估深度生成模型中数据贡献的紧迫需求也存在。然而，以往的数据估值方法主要依赖于判别模型性能指标，并需要对模型进行重新训练。因此，它们不能在实际中直接高效地应用于近期的深度生成模型，例如生成对抗网络和扩散模型。为了弥补这一差距，我们从相似性匹配的角度对生成模型中的数据估值问题进行了构建。具体地，我们引入了“Generative Model Valuator”（GMValuator）——第一个针对任何生成模型的模型无关方法，旨在为生成模型提供数据估值而无需重新训练模型。我们的方法利用数据实例及由生成模型生成的相应合成实例之间的相似度来估计原始数据的价值。大量实验证明了我们的方法在为不同的生成模型（包括GAN和扩散模型）评估数据实例方面的优越性。

    Data valuation is critical in machine learning, as it helps enhance model transparency and protect data properties. Existing data valuation methods have primarily focused on discriminative models, neglecting deep generative models that have recently gained considerable attention. Similar to discriminative models, there is an urgent need to assess data contributions in deep generative models as well. However, previous data valuation approaches mainly relied on discriminative model performance metrics and required model retraining. Consequently, they cannot be applied directly and efficiently to recent deep generative models, such as generative adversarial networks and diffusion models, in practice. To bridge this gap, we formulate the data valuation problem in generative models from a similarity-matching perspective. Specifically, we introduce Generative Model Valuator (GMValuator), the first model-agnostic approach for any generative models, designed to provide data valuation for gener
    
[^106]: 使用潜在指纹追溯图像生成模型

    Attributing Image Generative Models using Latent Fingerprints. (arXiv:2304.09752v1 [cs.CV])

    [http://arxiv.org/abs/2304.09752](http://arxiv.org/abs/2304.09752)

    本文研究了一种使用潜在语义维度作为指纹的追溯方法，可以分析设计变量对于准确性-质量权衡的影响，在保证准确性的同时最小化计算量，更适用于大规模模型。

    

    生成模型使得产生的内容难以区分是否源于自然环境。这种模型的开源开发引起了对于其被恶意利用的担忧。其中一个潜在的风险缓解策略是通过指纹追溯生成模型。然而，现有的指纹追溯方法在追溯准确性与生成质量之间存在明显的权衡，并且缺乏改善这种权衡的设计原则。本文研究了使用潜在语义维度作为指纹的方法，通过该方法可以分析设计变量对于准确性-质量权衡的影响，包括指纹维度的选择、强度和容量。相比之前的 SOTA，我们的方法需要最少的计算，并且更适用于大规模模型。我们使用 StyleGAN2 和潜在扩散模型展示了我们方法的功效。

    Generative models have enabled the creation of contents that are indistinguishable from those taken from the nature. Open-source development of such models raised concerns about the risks in their misuse for malicious purposes. One potential risk mitigation strategy is to attribute generative models via fingerprinting. Current fingerprinting methods exhibit significant tradeoff between robust attribution accuracy and generation quality, and also lack designing principles to improve this tradeoff. This paper investigates the use of latent semantic dimensions as fingerprints, from where we can analyze the effects of design variables, including the choice of fingerprinting dimensions, strength, and capacity, on the accuracy-quality tradeoff. Compared with previous SOTA, our method requires minimum computation and is more applicable to large-scale models. We use StyleGAN2 and the latent diffusion model to demonstrate the efficacy of our method.
    
[^107]: 带操作约束的机器人控制下的演员-评论家深度强化学习算法基准测试

    Benchmarking Actor-Critic Deep Reinforcement Learning Algorithms for Robotics Control with Action Constraints. (arXiv:2304.08743v1 [cs.LG])

    [http://arxiv.org/abs/2304.08743](http://arxiv.org/abs/2304.08743)

    本研究提出基准测试评估带操作约束的强化学习算法在多种机器人控制环境中的表现，并公开GitHub代码，为未来的研究和开发提供参考。

    

    本研究提出了一个用于评估操作约束的强化学习（RL）算法的基准测试。在操作受限的RL中，学习系统采取的每个操作都必须符合某些约束条件。这些约束条件对于确保实际系统中的操作的可行性和安全性至关重要。我们在多个机器人控制环境中评估了现有算法及其新颖的变体，涵盖多种操作限制类型。我们的评估提供了该领域的第一个深入视角，揭示了一些出人意料的见解，包括基线方法的有效性。我们的实验中使用的基准问题和相关代码可在github.com/omron-sinicx/action-constrained-RL-benchmark上在线获取，以进一步开展研究和开发。

    This study presents a benchmark for evaluating action-constrained reinforcement learning (RL) algorithms. In action-constrained RL, each action taken by the learning system must comply with certain constraints. These constraints are crucial for ensuring the feasibility and safety of actions in real-world systems. We evaluate existing algorithms and their novel variants across multiple robotics control environments, encompassing multiple action constraint types. Our evaluation provides the first in-depth perspective of the field, revealing surprising insights, including the effectiveness of a straightforward baseline approach. The benchmark problems and associated code utilized in our experiments are made available online at github.com/omron-sinicx/action-constrained-RL-benchmark for further research and development.
    
[^108]: 内在上下文算子学习用于微分方程问题

    In-Context Operator Learning for Differential Equation Problems. (arXiv:2304.07993v1 [cs.LG])

    [http://arxiv.org/abs/2304.07993](http://arxiv.org/abs/2304.07993)

    本文提出了一种新的神经网络方法INDEED，它可以同时学习不同微分方程问题的操作符，而无需重新训练，且只需要极少的演示。

    

    本文介绍了一种新的基于神经网络的方法——IN-context Differential Equation Encoder-Decoder（INDEED），用于从数据中同时学习操作符并在推理阶段将其应用于新问题，而无需进行任何权重更新。现有方法局限于使用神经网络来逼近特定的方程解或特定的操作符，需要重新训练来处理具有不同方程的新问题。通过训练单个神经网络作为操作符学习器，我们不仅可以摆脱为新问题重新训练（甚至微调）神经网络的困扰，还可以利用操作符之间共享的共同点，这样在学习新的操作符时只需要极少的演示即可。我们的数值结果显示了神经网络作为少样本学习器的能力，用于各种不同类型的微分方程问题，包括ODE和PDE的正向和反向问题，同时显示它可以推广学习能力。

    This paper introduces a new neural-network-based approach, namely IN-context Differential Equation Encoder-Decoder (INDEED), to simultaneously learn operators from data and apply it to new questions during the inference stage, without any weight update. Existing methods are limited to using a neural network to approximate a specific equation solution or a specific operator, requiring retraining when switching to a new problem with different equations. By training a single neural network as an operator learner, we can not only get rid of retraining (even fine-tuning) the neural network for new problems, but also leverage the commonalities shared across operators so that only a few demos are needed when learning a new operator. Our numerical results show the neural network's capability as a few-shot operator learner for a diversified type of differential equation problems, including forward and inverse problems of ODEs and PDEs, and also show that it can generalize its learning capabilit
    
[^109]: 超声断层成像反演的神经操作符学习

    Neural Operator Learning for Ultrasound Tomography Inversion. (arXiv:2304.03297v1 [eess.IV])

    [http://arxiv.org/abs/2304.03297](http://arxiv.org/abs/2304.03297)

    本文首次将神经操作符学习应用于超声断层成像反演，通过学习时间飞行数据和异质声速场之间的映射，实现了可避免计算密集型反演过程的预测异质声场模型。该模型有潜在的在乳腺成像中进行软组织分布预测和肿瘤识别的实时应用。

    

    神经操作符学习作为在计算科学和工程领域中进行复杂函数空间映射的一种手段已经引起了重视。在本文中，我们将神经操作符学习应用于飞行时间超声计算层析成像问题。我们使用全波求解器生成训练数据来学习时间飞行（TOF）数据和异质声速场之间的映射。该操作符学习的新颖应用规避了需要解决计算密集型迭代反问题的需求。该操作符学习在离线模式下学习非线性映射，并通过模型的单次前向通过来预测异质声场。这是第一次将操作符学习用于超声断层成像，也是潜在的实时预测乳腺成像中的软组织分布以进行肿瘤识别的第一步。

    Neural operator learning as a means of mapping between complex function spaces has garnered significant attention in the field of computational science and engineering (CS&E). In this paper, we apply Neural operator learning to the time-of-flight ultrasound computed tomography (USCT) problem. We learn the mapping between time-of-flight (TOF) data and the heterogeneous sound speed field using a full-wave solver to generate the training data. This novel application of operator learning circumnavigates the need to solve the computationally intensive iterative inverse problem. The operator learns the non-linear mapping offline and predicts the heterogeneous sound field with a single forward pass through the model. This is the first time operator learning has been used for ultrasound tomography and is the first step in potential real-time predictions of soft tissue distribution for tumor identification in beast imaging.
    
[^110]: 无监督学习用于野外图像质量评估的 Re-IQA 方法

    Re-IQA: Unsupervised Learning for Image Quality Assessment in the Wild. (arXiv:2304.00451v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2304.00451](http://arxiv.org/abs/2304.00451)

    该研究提出了一种名为 Re-IQA 的无监督学习方法，利用混合专家方法训练两个编码器，学习图像的高级内容和低级质量特征，以生成互补的低和高级图像表示，从而实现在野外自动化的感知图像质量评估，且在多个大型图像质量评估数据库上实现了最先进的性能。

    

    自动化的感知图像质量评估是一个具有挑战性的问题，每天影响着数十亿互联网和社交媒体用户。为了推进这一领域的研究，我们提出了一种混合专家方法，训练两个独立的编码器在无监督的环境下学习高层次的内容和低层次的图像质量特征。我们方法的独特之处在于它能够生成与表示图像内容的高级特征互补的图像质量的低级表示。我们称用于训练这两个编码器的框架为 Re-IQA。为了在野外评估图像质量，我们利用从 Re-IQA 框架获得的互补的低和高级图像表示来训练线性回归模型，该模型将图像表示映射到质量得分，具体细节见图1。我们的方法在包含真实和合成图像的多个大型图像质量评估数据库上取得了最先进的性能。

    Automatic Perceptual Image Quality Assessment is a challenging problem that impacts billions of internet, and social media users daily. To advance research in this field, we propose a Mixture of Experts approach to train two separate encoders to learn high-level content and low-level image quality features in an unsupervised setting. The unique novelty of our approach is its ability to generate low-level representations of image quality that are complementary to high-level features representing image content. We refer to the framework used to train the two encoders as Re-IQA. For Image Quality Assessment in the Wild, we deploy the complementary low and high-level image representations obtained from the Re-IQA framework to train a linear regression model, which is used to map the image representations to the ground truth quality scores, refer Figure 1. Our method achieves state-of-the-art performance on multiple large-scale image quality assessment databases containing both real and syn
    
[^111]: 面向全沉浸多用户虚拟现实技术的预测上下文感知和重定向步行

    Predictive Context-Awareness for Full-Immersive Multiuser Virtual Reality with Redirected Walking. (arXiv:2303.17907v1 [cs.NI])

    [http://arxiv.org/abs/2303.17907](http://arxiv.org/abs/2303.17907)

    本文提出了利用预测性上下文感知来优化发射端和接收端的波束成形和波束导向，实现面向全沉浸多用户虚拟现实技术的高效通信。

    

    虚拟现实技术正朝着增强沉浸感、支持多用户体验和在虚拟体验中支持无限制的移动，而通过重定向步行将用户限制在专门的VR设置内。为了满足未来VR系统的极端数据速率和延迟要求，支持无线网络基础设施将在毫米波（mmWave）频率上运行，并通过波束成形和波束导向实现高度定向的通信。我们提出了利用预测性上下文感知来优化发射端和接收端的波束成形和波束导向。具体而言，我们认为通过短期预测多用户VR设置中用户的横向移动，可以利用用户方向上的直线视距（LoS）“跟踪”来优化发射端的波束成形和波束导向。

    Virtual Reality (VR) technology is being advanced along the lines of enhancing its immersiveness, enabling multiuser Virtual Experiences (VEs), and supporting unconstrained mobility of the users in their VEs, while constraining them within specialized VR setups through Redirected Walking (RDW). For meeting the extreme data-rate and latency requirements of future VR systems, supporting wireless networking infrastructures will operate in millimeter Wave (mmWave) frequencies and leverage highly directional communication in both transmission and reception through beamforming and beamsteering. We propose to leverage predictive context-awareness for optimizing transmitter and receiver-side beamforming and beamsteering. In particular, we argue that short-term prediction of users' lateral movements in multiuser VR setups with RDW can be utilized for optimizing transmitter-side beamforming and beamsteering through Line-of-Sight (LoS) "tracking" in the users' directions. At the same time, short-
    
[^112]: HARFLOW3D：一种面向FPGA设备的基于延迟的3D-CNN加速器工具链

    HARFLOW3D: A Latency-Oriented 3D-CNN Accelerator Toolflow for HAR on FPGA Devices. (arXiv:2303.17218v1 [cs.AR])

    [http://arxiv.org/abs/2303.17218](http://arxiv.org/abs/2303.17218)

    本研究提出了一种面向FPGA设备的基于延迟的3D-CNN加速器工具链HARFLOW3D，它以机器学习模型和FPGA的特性描述为输入，生成最小化计算延迟的设计。实验证明HARFLOW3D相比其他方案能够实现更低的延迟。

    

    3D卷积神经网络已被证明在人体动作识别任务中具有高效性和最先进的结果。本研究引入一种新的基于流式架构的工具链，将此类模型映射到FPGA上，考虑模型固有特性和目标FPGA设备的特征。HARFLOW3D工具链以ONNX格式的3D卷积神经网络和FPGA特性描述为输入，生成最小化计算延迟的设计。该工具链由多个部分组成，包括i) 3D CNN解析器，ii) 性能和资源模型，iii) 用于在生成的硬件上执行3D模型的调度算法，iv) 针对3D模型量身定制的资源感知优化引擎，v) 自动映射到可合成的FPGA代码。通过对各种3D CNN和FPGA系统配对进行多个实验，展示了工具链支持广泛模型和设备的能力。此外，与其他最先进的3D CNN加速器设计方法相比，该工具链实现了更低的延迟。

    For Human Action Recognition tasks (HAR), 3D Convolutional Neural Networks have proven to be highly effective, achieving state-of-the-art results. This study introduces a novel streaming architecture based toolflow for mapping such models onto FPGAs considering the model's inherent characteristics and the features of the targeted FPGA device. The HARFLOW3D toolflow takes as input a 3D CNN in ONNX format and a description of the FPGA characteristics, generating a design that minimizes the latency of the computation. The toolflow is comprised of a number of parts, including i) a 3D CNN parser, ii) a performance and resource model, iii) a scheduling algorithm for executing 3D models on the generated hardware, iv) a resource-aware optimization engine tailored for 3D models, v) an automated mapping to synthesizable code for FPGAs. The ability of the toolflow to support a broad range of models and devices is shown through a number of experiments on various 3D CNN and FPGA system pairs. Furth
    
[^113]: 扩散Schr\"odinger桥匹配

    Diffusion Schr\"odinger Bridge Matching. (arXiv:2303.16852v1 [stat.ML])

    [http://arxiv.org/abs/2303.16852](http://arxiv.org/abs/2303.16852)

    本文介绍了一种新的方法 Iterative Markovian Fitting，用于解决高维度 Schr\"odinger桥（SBs）问题，该方法的数值实验表现出在准确性和性能方面的显著优势。

    

    解决运输问题，在机器学习中有着许多应用，例如新型的质量传输方法，如去噪扩散模型（DDMs）和流匹配模型（FMMs），通过随机微分方程（SDE）或常微分方程（ODE）实现这样的传输。然而，虽然在许多应用中，近似确定性动态最优传输（OT）映射是可取的，因为具有吸引人的性质，但 DDMs 和 FMMs 并不能保证提供接近 OT 映射的传输。相反，Schr\"odinger桥（SBs）计算随机动态映射，可以恢复正则熵版本的 OT。不幸的是，现有的数值方法近似 SBs 的维度缩放差或在迭代中积累误差。在这项工作中，我们介绍了迭代马尔科夫拟合，一种解决高维度 SB 问题的新方法。我们将这个方法设计为一个迭代过程，将置信传播扩展到 KL 散度，利用条件独立性降低计算复杂度，并确保一致性和收敛性质。我们的数值实验证明了相对于现有成果方法，在准确性和性能方面都有显著优势。

    Solving transport problems, i.e. finding a map transporting one given distribution to another, has numerous applications in machine learning. Novel mass transport methods motivated by generative modeling have recently been proposed, e.g. Denoising Diffusion Models (DDMs) and Flow Matching Models (FMMs) implement such a transport through a Stochastic Differential Equation (SDE) or an Ordinary Differential Equation (ODE). However, while it is desirable in many applications to approximate the deterministic dynamic Optimal Transport (OT) map which admits attractive properties, DDMs and FMMs are not guaranteed to provide transports close to the OT map. In contrast, Schr\"odinger bridges (SBs) compute stochastic dynamic mappings which recover entropy-regularized versions of OT. Unfortunately, existing numerical methods approximating SBs either scale poorly with dimension or accumulate errors across iterations. In this work, we introduce Iterative Markovian Fitting, a new methodology for solv
    
[^114]: Scoring Functions 和 Generalization Prediction 的详细研究

    A Closer Look at Scoring Functions and Generalization Prediction. (arXiv:2303.13589v1 [cs.LG])

    [http://arxiv.org/abs/2303.13589](http://arxiv.org/abs/2303.13589)

    本文研究了广义误差预测器的有效性，探讨了置信度、局部流形平滑度和模型一致性评分函数的优缺点，发现在复杂机制缺失的情况下，最先进的评分无法在分布转移和损坏下超越简单的模型一致性。同时，在受损训练数据的情况下，模型一致性打分仍然表现良好，并且集成多样性有助于提高泛化性能。

    

    本文研究了广义误差预测器（GEPs）的效果，这些 GEPs 旨在通过从样本级分数中推导出数据集级误差估计值，从而预测模型在未见分布上的表现。然而，GEPs 常常利用不同的机制（例如，回归器、阈值函数、校准数据集等），来推导这种误差估计值，这会混淆特定评分函数的优点。因此，本文在机制选择独立的情况下，深入研究了流行的评分函数的有效性（置信度、局部流形平滑度、模型一致性）。我们发现，在复杂机制缺失的情况下，当估计分布转移和损坏下的误差时，最先进的置信度和平滑度基础评分无法超越简单的模型一致性。此外，在实际情况下，当训练数据受到损害时（例如标签噪声、测量噪声、欠采样），我们发现模型一致性打分仍然表现良好，并且集成多样性有助于提高泛化性能。

    Generalization error predictors (GEPs) aim to predict model performance on unseen distributions by deriving dataset-level error estimates from sample-level scores. However, GEPs often utilize disparate mechanisms (e.g., regressors, thresholding functions, calibration datasets, etc), to derive such error estimates, which can obfuscate the benefits of a particular scoring function. Therefore, in this work, we rigorously study the effectiveness of popular scoring functions (confidence, local manifold smoothness, model agreement), independent of mechanism choice. We find, absent complex mechanisms, that state-of-the-art confidence- and smoothness- based scores fail to outperform simple model-agreement scores when estimating error under distribution shifts and corruptions. Furthermore, on realistic settings where the training data has been compromised (e.g., label noise, measurement noise, undersampling), we find that model-agreement scores continue to perform well and that ensemble diversi
    
[^115]: 通过量化进行的事后解释

    Posthoc Interpretation via Quantization. (arXiv:2303.12659v1 [cs.AI])

    [http://arxiv.org/abs/2303.12659](http://arxiv.org/abs/2303.12659)

    本文提出了一种新的方法 PIQ，通过对分类器进行向量量化，将其表示转换为离散类特定的潜空间，从而解释分类器所做出的决策，并且通过研究发现该方法相比其他方法更容易让人理解。

    

    本文提出了一种新的方法，称为“通过量化实现的事后解释（PIQ）”，用于解释训练分类器所做出的决策。我们的方法利用向量量化将分类器的表示转换为离散，类特定的潜空间。类特定的码本作为瓶颈，迫使解释者专注于分类器认为用于进行预测的输入数据的相关部分。我们通过定量和定性研究评估了我们的方法，并发现与文献中的几种其他解释方法相比，PIQ生成的解释更容易被参与我们用户研究的人所理解。

    In this paper, we introduce a new approach, called "Posthoc Interpretation via Quantization (PIQ)", for interpreting decisions made by trained classifiers. Our method utilizes vector quantization to transform the representations of a classifier into a discrete, class-specific latent space. The class-specific codebooks act as a bottleneck that forces the interpreter to focus on the parts of the input data deemed relevant by the classifier for making a prediction. We evaluated our method through quantitative and qualitative studies and found that PIQ generates interpretations that are more easily understood by participants to our user studies when compared to several other interpretation methods in the literature.
    
[^116]: CB2：合作自然语言交互研究平台

    CB2: Collaborative Natural Language Interaction Research Platform. (arXiv:2303.08127v1 [cs.LG])

    [http://arxiv.org/abs/2303.08127](http://arxiv.org/abs/2303.08127)

    CB2是一个用于研究基于任务的合作自然语言交互的平台，在3D游戏环境中提供了后端服务器和各种工具和流程。它在可扩展的研究中展示了学习的指令跟随模型。

    

    CB2 是一个多智能体平台，用于研究基于任务的情境下的合作自然语言交互。它包括一个 3D 游戏环境、一个后端服务器，可为人类智能体提供训练模型，以及各种工具和流程，以实现可扩展性的研究。我们在 https://cb2.ai 上展示了一个具有学习指令跟随模型的系统演示。

    CB2 is a multi-agent platform to study collaborative natural language interaction in a grounded task-oriented scenario. It includes a 3D game environment, a backend server designed to serve trained models to human agents, and various tools and processes to enable scalable studies. We deploy CB2 at https://cb2.ai as a system demonstration with a learned instruction following model.
    
[^117]: MPC模型预测控制策略逼近的改进数据增强方案

    An Improved Data Augmentation Scheme for Model Predictive Control Policy Approximation. (arXiv:2303.05607v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2303.05607](http://arxiv.org/abs/2303.05607)

    基于敏感度有效生成MPC策略逼近的数据增强框架被提出，本文在此基础上引入了基于预测校正步骤的改进方案，以提高数据准确性。

    

    本文研究了MPC策略逼近的数据生成问题。从专家演示中学习一个近似的MPC策略需要一个大型数据集，其中包含跨可行状态空间采样的最优状态-动作对。然而，有效地生成训练样本的关键挑战尚未得到广泛研究。最近，提出了一种基于敏感度的MPC策略逼近数据增强框架，利用参数敏感度从单个离线MPC计算中廉价地生成多个额外样本。通过使用不精确样本增强训练数据集的误差被证明随着数据增强的邻域大小的增加而增加。在此基础上，本文提出了一种基于预测校正步骤的改进数据增强方案，可以强制执行用户定义的精度级别，并且显示出增强样本的误差界独立于邻域大小。

    This paper considers the problem of data generation for MPC policy approximation. Learning an approximate MPC policy from expert demonstrations requires a large data set consisting of optimal state-action pairs, sampled across the feasible state space. Yet, the key challenge of efficiently generating the training samples has not been studied widely. Recently, a sensitivity-based data augmentation framework for MPC policy approximation was proposed, where the parametric sensitivities are exploited to cheaply generate several additional samples from a single offline MPC computation. The error due to augmenting the training data set with inexact samples was shown to increase with the size of the neighborhood around each sample used for data augmentation. Building upon this work, this letter paper presents an improved data augmentation scheme based on predictor-corrector steps that enforces a user-defined level of accuracy, and shows that the error bound of the augmented samples are indepe
    
[^118]: 改进技术掌握策略卡牌游戏（爐石戰記）

    Mastering Strategy Card Game (Hearthstone) with Improved Techniques. (arXiv:2303.05197v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.05197](http://arxiv.org/abs/2303.05197)

    本文将端到端策略函数和乐观平滑虚拟博弈算法应用于更加复杂的商业游戏爐石戰記，提出改进技术并在人机对战中表现出较强决策能力。

    

    策略卡牌游戏是一种著名的游戏类型，要求智能游戏玩法，并可以作为人工智能的理想测试平台。以往的研究合并了端到端策略函数和乐观平滑虚拟博弈，在《魔法与魅力：代码传说》等策略卡牌游戏中表现出有希望的性能。在本文中，我们将这些算法应用到爐石戰記上，并进一步提出了几种改进的技术，从而取得了显着的进展。为了进行机器对人的测试，我们邀请了一位在中国官方联赛中排名前十的爐石戰記播主，该地区的玩家数量估计在数百万人以上。我们的模型在所有完整游戏（包括构建卡组和对战）的五局比赛中击败了人类玩家，展现了强大的决策能力。

    Strategy card game is a well-known genre that is demanding on the intelligent game-play and can be an ideal test-bench for AI. Previous work combines an end-to-end policy function and an optimistic smooth fictitious play, which shows promising performances on the strategy card game Legend of Code and Magic. In this work, we apply such algorithms to Hearthstone, a famous commercial game that is more complicated in game rules and mechanisms. We further propose several improved techniques and consequently achieve significant progress. For a machine-vs-human test we invite a Hearthstone streamer whose best rank was top 10 of the official league in China region that is estimated to be of millions of players. Our models defeat the human player in all Best-of-5 tournaments of full games (including both deck building and battle), showing a strong capability of decision making.
    
[^119]: 学习如何在联邦学习中设置后门

    Learning to Backdoor Federated Learning. (arXiv:2303.03320v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03320](http://arxiv.org/abs/2303.03320)

    本文提出了一种基于强化学习的后门攻击框架，能够在联邦学习中成功设置后门，在现有防御措施下也具有强大的攻击性能和耐用性。

    

    在联邦学习（FL）系统中，恶意参与者可以轻易地将后门嵌入到聚合模型中，同时保持模型在主任务上的性能。为此，近期提出了各种防御措施，包括训练阶段的基于聚合的防御和后期防御措施。虽然这些防御措施在现有的基于启发式的后门攻击中可以获得合理的性能，但我们发现它们在面对更高级的攻击时不足以应对。特别地，我们提出了一种基于强化学习（RL）的后门攻击框架，攻击者首先使用基于其本地数据和FL系统的共同知识建立的仿真器训练一个（非近视）攻击策略，然后在实际的FL训练中应用它。我们的攻击框架既是适应性又是灵活的，并且即使在最先进的防御措施下也能实现强大的攻击性能和耐用性。

    In a federated learning (FL) system, malicious participants can easily embed backdoors into the aggregated model while maintaining the model's performance on the main task. To this end, various defenses, including training stage aggregation-based defenses and post-training mitigation defenses, have been proposed recently. While these defenses obtain reasonable performance against existing backdoor attacks, which are mainly heuristics based, we show that they are insufficient in the face of more advanced attacks. In particular, we propose a general reinforcement learning-based backdoor attack framework where the attacker first trains a (non-myopic) attack policy using a simulator built upon its local data and common knowledge on the FL system, which is then applied during actual FL training. Our attack framework is both adaptive and flexible and achieves strong attack performance and durability even under state-of-the-art defenses.
    
[^120]: 用元启发式条件神经网络收集天然抑制态的天空rmion状态

    Metaheuristic conditional neural network for harvesting skyrmionic metastable states. (arXiv:2303.02876v2 [physics.comp-ph] UPDATED)

    [http://arxiv.org/abs/2303.02876](http://arxiv.org/abs/2303.02876)

    提出了一种基于元启发式条件神经网络的方法，收集高不规则潜能能量面上的物理有趣的稳态， 并在Pd / Fe / Ir（111）系统中应用于识别自旋结构，观察了其中一些结构的有限温度自旋动力学特性和拓扑电荷与结构之间的关系。

    

    我们提出了一种基于元启发式条件神经网络的方法，旨在识别高不规则潜能能量面上的物理有趣的稳态。为了展示这种方法的工作原理，我们使用从密度泛函理论计算的参数基础上建立的古典微观旋转哈密顿量对Pd / Fe / Ir（111）系统中的拓扑电荷$Q$值从1到$-13$的自旋结构进行了分析 。为了促进相关自旋结构的收集，我们利用了最新开发的“任意分段模型”（SAM）。并使用有限温度自旋动力学模拟进一步分析了Q值范围从$-3$到$-6$的自旋结构。我们发现，对于高达20K的温度，预测寿命长达200ps以上，当这些结构衰减时，新的拓扑旋转结构就会形成。我们还发现，自旋结构的相对稳定性与其拓扑相关。

    We present a metaheuristic conditional neural-network-based method aimed at identifying physically interesting metastable states in a potential energy surface of high rugosity. To demonstrate how this method works, we identify and analyze spin textures with topological charge $Q$ ranging from 1 to $-13$ (where antiskyrmions have $Q<0$) in the Pd/Fe/Ir(111) system, which we model using a classical atomistic spin Hamiltonian based on parameters computed from density functional theory. To facilitate the harvest of relevant spin textures, we make use of the newly developed Segment Anything Model (SAM). Spin textures with $Q$ ranging from $-3$ to $-6$ are further analyzed using finite-temperature spin-dynamics simulations. We observe that for temperatures up to around 20\,K, lifetimes longer than 200\,ps are predicted, and that when these textures decay, new topological spin textures are formed. We also find that the relative stability of the spin textures depend linearly on the topological
    
[^121]: Soft Actor-Critic算法的收敛点

    The Point to Which Soft Actor-Critic Converges. (arXiv:2303.01240v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01240](http://arxiv.org/abs/2303.01240)

    本文证明了在极限情况下，Soft Actor-Critic算法和Soft Q-learning算法在最大熵框架下收敛于同一解，这一结论对优化算法具有较大的意义。

    

    Soft Actor-Critic是Soft Q-learning的成功后继者，尽管它们都处于最大熵框架下，但它们之间的关系仍不清楚。本文证明了它们在极限情况下收敛于相同的解，这一结果非常有吸引力，因为它将优化从困难的方式转化为了简单的方式。同样的证明也适用于其他正则项，例如KL散度。

    Soft actor-critic is a successful successor over soft Q-learning. While lived under maximum entropy framework, their relationship is still unclear. In this paper, we prove that in the limit they converge to the same solution. This is appealing since it translates the optimization from an arduous to an easier way. The same justification can also be applied to other regularizers such as KL divergence.
    
[^122]: Inseq：一个用于序列生成模型的可解释性工具包

    Inseq: An Interpretability Toolkit for Sequence Generation Models. (arXiv:2302.13942v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.13942](http://arxiv.org/abs/2302.13942)

    本文介绍了Inseq，这是一个Python工具包，旨在推广可解释性序列生成模型的分析。它为常见的解码器和编码器-解码器Transformers架构提供了提取模型内部信息和特征重要性得分的直观优化方法。作者还在机器翻译模型和GPT-2中展示了Inseq的潜力，证明其有助于推动可解释性自然语言生成的未来发展。

    

    自然语言处理领域的过去的可解释性研究主要集中在流行的分类任务上，而在生成任务中往往被忽视，部分原因是缺乏专门的工具。在本文中，我们介绍了Inseq，一个Python库，用于使序列生成模型的可解释性分析普及化。Inseq能够直观且优化地提取流行的仅解码器和编码器解码器Transformers架构的模型内部信息和特征重要性分数。我们还展示了它的潜力，通过使用它来突出机器翻译模型中的性别偏见并在GPT-2中定位事实知识。由于其支持对比特征归因等前沿技术的可扩展接口，因此Inseq可以推动可解释性自然语言生成的未来发展，集中优良实践，并实现公正和可重复的模型评估。

    Past work in natural language processing interpretability focused mainly on popular classification tasks while largely overlooking generation settings, partly due to a lack of dedicated tools. In this work, we introduce Inseq, a Python library to democratize access to interpretability analyses of sequence generation models. Inseq enables intuitive and optimized extraction of models' internal information and feature importance scores for popular decoder-only and encoder-decoder Transformers architectures. We showcase its potential by adopting it to highlight gender biases in machine translation models and locate factual knowledge inside GPT-2. Thanks to its extensible interface supporting cutting-edge techniques such as contrastive feature attribution, Inseq can drive future advances in explainable natural language generation, centralizing good practices and enabling fair and reproducible model evaluations.
    
[^123]: 基于自监督学习的方法用于聚类包含缺失值的多元时间序列数据 (SLAC-Time): 应用于TBI表型化

    A Self-Supervised Learning-based Approach to Clustering Multivariate Time-Series Data with Missing Values (SLAC-Time): An Application to TBI Phenotyping. (arXiv:2302.13457v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13457](http://arxiv.org/abs/2302.13457)

    本文提出了一种基于自监督学习的多元时间序列数据聚类方法(SLAC-Time)，采用时间序列预测作为代理任务，不需要填补缺失值，具有更健壮的时间序列表示。

    

    自监督学习方法为聚类多元时间序列数据提供了一个很有前途的方向。然而，现实世界中的时间序列数据通常包含缺失值，而现有的方法要求在聚类之前填补缺失值，这可能会导致大量计算和噪声，从而导致无效的解释。为了解决这些挑战，我们提出了一种基于自监督学习的用于聚类包含缺失值的多元时间序列数据的方法(SLAC-Time)。SLAC-Time是一种基于Transformer的聚类方法，它使用时间序列预测作为代理任务，利用无标签数据来学习更健壮的时间序列表示。该方法同时学习神经网络参数和所学表示的聚类分配。它使用K-means方法迭代地对学习表示进行聚类，然后利用随后的聚类分配作为伪标签来更新模型参数。

    Self-supervised learning approaches provide a promising direction for clustering multivariate time-series data. However, real-world time-series data often include missing values, and the existing approaches require imputing missing values before clustering, which may cause extensive computations and noise and result in invalid interpretations. To address these challenges, we present a Self-supervised Learning-based Approach to Clustering multivariate Time-series data with missing values (SLAC-Time). SLAC-Time is a Transformer-based clustering method that uses time-series forecasting as a proxy task for leveraging unlabeled data and learning more robust time-series representations. This method jointly learns the neural network parameters and the cluster assignments of the learned representations. It iteratively clusters the learned representations with the K-means method and then utilizes the subsequent cluster assignments as pseudo-labels to update the model parameters. To evaluate our
    
[^124]: MDF-Net：结合X射线与临床数据用于异常检测的方法

    MDF-Net for Abnormality Detection by Fusing X-Rays with Clinical Data. (arXiv:2302.13390v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2302.13390](http://arxiv.org/abs/2302.13390)

    本研究提出了一种多模态融合方法MDF-Net，将临床数据与胸部X射线图像相结合，成功地提高了疾病定位的性能表现。

    

    本研究探讨将患者的临床信息加入深度学习分类器以提高胸部X射线图像疾病定位性能的影响。当前的分类器在使用胸部X射线图像时具有高性能，但我们对放射科医生的访谈表明，临床数据对于解释图像和做出正确的诊断来说非常重要，并具有很高的信息量。我们提出了一种新颖的架构，该架构由两种融合方法组成，使得模型能够同时处理患者的临床数据（结构化数据）和胸部X射线（图像数据）。由于这两种数据模态在不同的维度空间中，因此我们提出了一种空间排列策略空间化，以便在Mask R-CNN模型中促进多模态学习过程。我们使用包含多种数据模态的MIMIC-Eye数据集进行了广泛的实验评估：MIMIC-CXR（胸部X射线图像）、MIMIC IV-ED（患者的临床数据）和REFLACX（评估注释）。实验结果表明，我们提出的MDF-Net相比当前最先进方法具有更优异的性能。

    This study investigates the effects of including patients' clinical information on the performance of deep learning (DL) classifiers for disease location in chest X-ray images. Although current classifiers achieve high performance using chest X-ray images alone, our interviews with radiologists indicate that clinical data is highly informative and essential for interpreting images and making proper diagnoses.  In this work, we propose a novel architecture consisting of two fusion methods that enable the model to simultaneously process patients' clinical data (structured data) and chest X-rays (image data). Since these data modalities are in different dimensional spaces, we propose a spatial arrangement strategy, spatialization, to facilitate the multimodal learning process in a Mask R-CNN model. We performed an extensive experimental evaluation using MIMIC-Eye, a dataset comprising modalities: MIMIC-CXR (chest X-ray images), MIMIC IV-ED (patients' clinical data), and REFLACX (annotatio
    
[^125]: 学习可以遵守守恒定律的物理模型

    Learning Physical Models that Can Respect Conservation Laws. (arXiv:2302.11002v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11002](http://arxiv.org/abs/2302.11002)

    这项工作提出了ProbConserv框架，通过将守恒约束与贝叶斯更新相结合，将守恒约束纳入通用科学机器学习体系结构中，以便在学习高难度的PDE运算中应用。

    

    科学机器学习的最近一些工作集中在将偏微分方程（PDE）信息融入学习过程中。其中许多工作集中在相对“容易”的PDE算子（例如椭圆和抛物线）上，而相对“困难”的PDE算子（例如双曲线）则较少。在数值PDE方面，后一种问题类需要控制一种体积元素或守恒约束类型，这被视为具有挑战性的问题。为了实现科学机器学习的承诺，需要无缝地将这两种类型的问题融入学习过程中。

    Recent work in scientific machine learning (SciML) has focused on incorporating partial differential equation (PDE) information into the learning process. Much of this work has focused on relatively ``easy'' PDE operators (e.g., elliptic and parabolic), with less emphasis on relatively ``hard'' PDE operators (e.g., hyperbolic). Within numerical PDEs, the latter problem class requires control of a type of volume element or conservation constraint, which is known to be challenging. Delivering on the promise of SciML requires seamlessly incorporating both types of problems into the learning process. To address this issue, we propose ProbConserv, a framework for incorporating conservation constraints into a generic SciML architecture. To do so, ProbConserv combines the integral form of a conservation law with a Bayesian update. We provide a detailed analysis of ProbConserv on learning with the Generalized Porous Medium Equation (GPME), a widely-applicable parameterized family of PDEs that 
    
[^126]: 一种基于噪声注入训练方案的模型鲁棒性改进方法

    A Novel Noise Injection-based Training Scheme for Better Model Robustness. (arXiv:2302.10802v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10802](http://arxiv.org/abs/2302.10802)

    本研究提出了一种基于噪声注入训练方案的模型鲁棒性改进方法，通过设计特殊的梯度估计方法和简化传统噪声注入方法，提高了对抗鲁棒性，并在MNIST和Fashion-MNIST数据集上得到了验证。

    

    先前的工作表明，噪音注入方法可以改进人工神经网络的鲁棒性。在本文中，我们提出了一种基于噪声注入训练方案的模型鲁棒性改进方法。具体地，我们首先开发了一个似然比方法来估计随机梯度下降训练下的突触权重和噪声水平的梯度。接着，我们设计了一种逼近的传统噪声注入训练方法，以减少内存并提高计算效率。然后，我们将我们提出的方案应用于尖峰神经网络，并在MNIST和Fashion-MNIST数据集上评估分类准确性和鲁棒性性能。实验结果表明，与传统的基于梯度的训练方法相比，我们提出的方法在对抗鲁棒性上表现更好，在原始准确性上略微更好。

    Noise injection-based method has been shown to be able to improve the robustness of artificial neural networks in previous work. In this work, we propose a novel noise injection-based training scheme for better model robustness. Specifically, we first develop a likelihood ratio method to estimate the gradient with respect to both synaptic weights and noise levels for stochastic gradient descent training. Then, we design an approximation for the vanilla noise injection-based training method to reduce memory and improve computational efficiency. Next, we apply our proposed scheme to spiking neural networks and evaluate the performance of classification accuracy and robustness on MNIST and Fashion-MNIST datasets. Experiment results show that our proposed method achieves a much better performance on adversarial robustness and slightly better performance on original accuracy, compared with the conventional gradient-based training method.
    
[^127]: NeuralStagger: 利用时空分解加速物理约束下的神经偏微分方程解法器

    NeuralStagger: Accelerating Physics-constrained Neural PDE Solver with Spatial-temporal Decomposition. (arXiv:2302.10255v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10255](http://arxiv.org/abs/2302.10255)

    本文提出了一种新的名为NeuralStagger的通用加速方法，通过将物理约束下的神经偏微分方程解法器进行时空分解得到多个粗分辨率的子任务，并使用物理约束的标准损失联合训练，从而极大地减少了计算资源使用。

    

    神经网络在加速偏微分方程的求解方面表现出了很大的潜力。最近，越来越多的研究关注于在训练神经偏微分方程解法器中引入物理约束来减少数据使用和提高模型的泛化能力。然而，这些基于有限维度近似的物理约束为了保证模拟的准确性和稳定性，必须解决最小尺度的物理问题，从而导致大量的计算资源使用，包括输入、输出和神经网络。本文提出了一种名为NeuralStagger的通用加速方法，通过将原始的学习任务空间和时间上进行分解，形成几个较粗分辨率的子任务。我们为每个子任务定义了一个粗分辨率的神经求解器，其所需的计算资源更少，并使用具有物理约束的标准损失联合训练它们，只需简单地排列其输出以重构完整的模拟结果。

    Neural networks have shown great potential in accelerating the solution of partial differential equations (PDEs). Recently, there has been a growing interest in introducing physics constraints into training neural PDE solvers to reduce the use of costly data and improve the generalization ability. However, these physics constraints, based on certain finite dimensional approximations over the function space, must resolve the smallest scaled physics to ensure the accuracy and stability of the simulation, resulting in high computational costs from large input, output, and neural networks. This paper proposes a general acceleration methodology called NeuralStagger by spatially and temporally decomposing the original learning tasks into several coarser-resolution subtasks. We define a coarse-resolution neural solver for each subtask, which requires fewer computational resources, and jointly train them with the vanilla physics-constrained loss by simply arranging their outputs to reconstruct
    
[^128]: HyFL:一种用于私有联合学习的混合框架

    HyFL: A Hybrid Framework For Private Federated Learning. (arXiv:2302.09904v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09904](http://arxiv.org/abs/2302.09904)

    HyFL是一种混合框架，它结合了安全多方计算技术和分层联合学习，并能够在分布式环境中保证数据和全局模型的隐私安全，有助于大规模部署。

    

    联合学习已经成为分布式机器学习的有效方法，通过在客户端设备上保留训练数据来确保数据隐私。然而，最近的研究强调了FL中的漏洞，包括通过单个模型更新甚至整个全局模型泄漏敏感信息。虽然关注点已放在客户端数据隐私上，但有限的研究解决了全局模型隐私问题。此外，客户端本地训练为恶意客户端启动强大的模型污染攻击开辟了途径。不幸的是，目前没有现有工作提供全面解决所有这些问题的解决方案。因此，我们介绍了HyFL，这是一种混合框架，可实现数据和全局模型隐私，并促进大规模部署。HyFL的基础是安全多方计算技术和分层联合学习的独特组合。在HyFL的训练过程中，客户端模型在多个抽象层次上进行安全聚合，以在分布式环境中提供隐私保护。实验证明，HyFL在大规模数据集上实现良好性能，同时确保客户端数据和全局模型的强大隐私和安全保障。

    Federated learning (FL) has emerged as an efficient approach for large-scale distributed machine learning, ensuring data privacy by keeping training data on client devices. However, recent research has highlighted vulnerabilities in FL, including the potential disclosure of sensitive information through individual model updates and even the aggregated global model. While much attention has been given to clients' data privacy, limited research has addressed the issue of global model privacy. Furthermore, local training at the client's side has opened avenues for malicious clients to launch powerful model poisoning attacks. Unfortunately, no existing work has provided a comprehensive solution that tackles all these issues. Therefore, we introduce HyFL, a hybrid framework that enables data and global model privacy while facilitating large-scale deployments. The foundation of HyFL is a unique combination of secure multi-party computation (MPC) techniques with hierarchical federated learnin
    
[^129]: 神经网络中的深度退化：全连接ReLU网络初始化时，消失角度的现象 (arXiv:2302.09712v2 [stat.ML] 更新版)

    Depth Degeneracy in Neural Networks: Vanishing Angles in Fully Connected ReLU Networks on Initialization. (arXiv:2302.09712v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.09712](http://arxiv.org/abs/2302.09712)

    本文研究了深度神经网络中的深度退化现象，在全连接ReLU网络初始化时，两个输入之间的角度会趋近于0。通过使用组合展开，得到了其趋向于0的速度的精确公式，并验证了这些结果。

    

    尽管深度神经网络在各种任务上表现出色，但许多其性质仍未被理论上理解，其中一个谜团是深度退化现象：网络层数越深，初始化时网络越接近于常数函数。在本文中，我们研究了ReLU神经网络两个输入之间随着层数变化的角度演变情况。通过使用组合展开，我们找到了它随深度增加趋向于0的速度的精确公式，这些公式捕捉了微观波动。我们用Monte Carlo实验验证了我们的理论结果，并证明了结果准确地近似了有限网络的行为。这些公式以通过ReLU函数的相关高斯变量的混合矩形式给出。我们还发现了一个令人惊讶的组合现象。

    Despite remarkable performance on a variety of tasks, many properties of deep neural networks are not yet theoretically understood. One such mystery is the depth degeneracy phenomenon: the deeper you make your network, the closer your network is to a constant function on initialization. In this paper, we examine the evolution of the angle between two inputs to a ReLU neural network as a function of the number of layers. By using combinatorial expansions, we find precise formulas for how fast this angle goes to zero as depth increases. These formulas capture microscopic fluctuations that are not visible in the popular framework of infinite width limits, and leads to qualitatively different predictions. We validate our theoretical results with Monte Carlo experiments and show that our results accurately approximate finite network behaviour. The formulas are given in terms of the mixed moments of correlated Gaussians passed through the ReLU function. We also find a surprising combinatoria
    
[^130]: 通过演示来理解专业技能：离线逆向强化学习的最大似然框架

    Understanding Expertise through Demonstrations: A Maximum Likelihood Framework for Offline Inverse Reinforcement Learning. (arXiv:2302.07457v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07457](http://arxiv.org/abs/2302.07457)

    通过提出的双层优化公式，我们提供了一个离线逆向强化学习的最大似然框架，该框架通过最大化奖励来估计专家的保守模型以及专家的环境动态，能够更准确地推断专业技能。

    

    离线逆向强化学习（Offline IRL）旨在从专家代理的固定有限演示中恢复支撑观察到的操作的奖励和环境动态的结构。准确的专业执行任务的模型在安全敏感的应用中具有重要应用，例如临床决策和自动驾驶。然而，专家喜好隐含在观察到的操作中的结构与专家对环境动态的模型（即“世界”）密切相关。因此，从具有有限覆盖范围的有限数据中获得的不准确世界模型可能会导致估计的奖励的不准确性变得更加严重。为了解决这个问题，我们提出了一个双层优化公式的估计任务，其中上层是基于专家策略的保守模型的最大似然估计（下层）。策略模型是保守的，因为它在惩罚（惩罚会随着专家对世界模型的不确定性而增加）下最大化奖励。我们的实验表明，我们的方法在各种基准测试任务中提高了离线IRL方法的准确性。

    Offline inverse reinforcement learning (Offline IRL) aims to recover the structure of rewards and environment dynamics that underlie observed actions in a fixed, finite set of demonstrations from an expert agent. Accurate models of expertise in executing a task has applications in safety-sensitive applications such as clinical decision making and autonomous driving. However, the structure of an expert's preferences implicit in observed actions is closely linked to the expert's model of the environment dynamics (i.e. the ``world''). Thus, inaccurate models of the world obtained from finite data with limited coverage could compound inaccuracy in estimated rewards. To address this issue, we propose a bi-level optimization formulation of the estimation task wherein the upper level is likelihood maximization based upon a conservative model of the expert's policy (lower level). The policy model is conservative in that it maximizes reward subject to a penalty that is increasing in the uncerta
    
[^131]: 几何 Clifford 代数网络

    Geometric Clifford Algebra Networks. (arXiv:2302.06594v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06594](http://arxiv.org/abs/2302.06594)

    本文提出了基于几何代数的几何 Clifford 代数网络（GCANs），采用对称群变换建模动态系统，通过群作用层、激活和归一化方案，可以优化几何模板，提高三维刚体变换和流体动力学模拟的性能。

    

    我们提出了基于几何（Clifford）代数的对称群变换的几何 Clifford 代数网络（GCANs）用于建模动态系统。我们首先回顾了现代（基于平面的）几何代数的精髓，该代数建立在作为 $\mathrm{Pin}(p,q,r)$ 群元素的等距映射之上。然后我们提出了群作用层的概念，它使用预定义的群作用线性组合物体变换，配合新的激活和归一化方案，这些层作为可调整的“几何模板”，可以通过梯度下降来优化。理论上的优势在三维刚体变换和大规模流体动力学模拟的建模中得到了强烈体现，表现出比传统方法显着提高的性能。

    We propose Geometric Clifford Algebra Networks (GCANs) for modeling dynamical systems. GCANs are based on symmetry group transformations using geometric (Clifford) algebras. We first review the quintessence of modern (plane-based) geometric algebra, which builds on isometries encoded as elements of the $\mathrm{Pin}(p,q,r)$ group. We then propose the concept of group action layers, which linearly combine object transformations using pre-specified group actions. Together with a new activation and normalization scheme, these layers serve as adjustable $\textit{geometric templates}$ that can be refined via gradient descent. Theoretical advantages are strongly reflected in the modeling of three-dimensional rigid body transformations as well as large-scale fluid dynamics simulations, showing significantly improved performance over traditional methods.
    
[^132]: 关于分布式学习中隐私、鲁棒性和实用性三难题的探讨

    On the Privacy-Robustness-Utility Trilemma in Distributed Learning. (arXiv:2302.04787v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04787](http://arxiv.org/abs/2302.04787)

    本文研究了分布式学习中隐私、鲁棒性和实用性之间的权衡关系，并证明了它们之间存在一种根本的平衡，同时提出了一种新的高维度鲁棒聚合规则的分布式机器学习算法，可以优化效用-隐私权衡。

    

    分布式机器学习在敏感公共领域应用中的普及需要保护数据隐私的算法，同时对故障和敌对行为具有鲁棒性。虽然隐私和鲁棒性在分布式机器学习中都有广泛的研究，但它们的综合仍然不为人所理解。本文提出了第一个紧密分析了任何算法在确保对抗性机器的鲁棒性和诚实机器的差分隐私方面会产生误差的模型，展示了隐私、鲁棒性和实用性之间的基本权衡。我们通过平均估计的案例来证明我们的下限，并将其受到分布式差分隐私和鲁棒性约束的影响，使用了一些减少单向边际中心化估计的降低方法。我们通过提出一种新的分布式机器学习算法，使用高维度的鲁棒聚合规则证明了我们的上限匹配。后面所述的方法将计算成本分摊化，并获得了改进的效用-隐私权衡。我们的结果阐明了在隐私-鲁棒性-效用三难问题的背景下出现的交叉学科挑战，并可以指导大规模分布式机器学习系统的实际设计。

    The ubiquity of distributed machine learning (ML) in sensitive public domain applications calls for algorithms that protect data privacy, while being robust to faults and adversarial behaviors. Although privacy and robustness have been extensively studied independently in distributed ML, their synthesis remains poorly understood. We present the first tight analysis of the error incurred by any algorithm ensuring robustness against a fraction of adversarial machines, as well as differential privacy (DP) for honest machines' data against any other curious entity. Our analysis exhibits a fundamental trade-off between privacy, robustness, and utility. To prove our lower bound, we consider the case of mean estimation, subject to distributed DP and robustness constraints, and devise reductions to centralized estimation of one-way marginals. We prove our matching upper bound by presenting a new distributed ML algorithm using a high-dimensional robust aggregation rule. The latter amortizes the
    
[^133]: 基础模型的黑盒对抗提示

    Black Box Adversarial Prompting for Foundation Models. (arXiv:2302.04237v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04237](http://arxiv.org/abs/2302.04237)

    本文提出了一个黑盒框架，用于生成对抗性提示，以在基础图像和文本生成模型中引入特定的生成行为。

    

    提示界面允许用户在视觉和语言方面快速调整生成模型的输出。 然而，提示中的小改变和设计选择可能导致输出中的显着差异。 在本文中，我们开发了一个针对非结构化图像和文本生成的黑盒框架，用于生成对抗性提示。 这些提示可以独立存在，也可以附加到良性提示之前，从而导致特定的生成过程行为，例如生成特定对象的图像或生成高困惑度文本。

    Prompting interfaces allow users to quickly adjust the output of generative models in both vision and language. However, small changes and design choices in the prompt can lead to significant differences in the output. In this work, we develop a black-box framework for generating adversarial prompts for unstructured image and text generation. These prompts, which can be standalone or prepended to benign prompts, induce specific behaviors into the generative process, such as generating images of a particular object or generating high perplexity text.
    
[^134]: Rover：一种通过广义转移学习提供在线 Spark SQL 调优服务的方法

    Rover: An online Spark SQL tuning service via generalized transfer learning. (arXiv:2302.04046v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04046](http://arxiv.org/abs/2302.04046)

    Rover 是一种在线 Spark SQL 调优服务，通过应用广义转移学习，结合专家知识和历史任务，来加速调优过程，提高性能并避免不良影响。

    

    分布式数据分析引擎（如Spark）是工业界处理海量数据的常用选择。然而，Spark SQL 的性能高度依赖于配置的选择，其中最优配置随执行的工作负载而变化。在众多的 Spark SQL 调优方案中，贝叶斯优化（BO）是一种流行的框架，可以在给定足够预算的情况下找到接近最优的配置，但它受到重新优化问题的影响，在实际生产中不实用。当应用转移学习来加速调优过程时，我们注意到两个领域特定的挑战：1）大多数之前的工作都集中在传递调优历史记录方面，而来自 Spark 工程师的专业知识具有极大的潜力来改善调优性能，但迄今为止尚未得到充分研究；2）应该仔细利用历史任务，其中使用不相似的任务会导致生产中的性能下降。在本文中，我们介绍了 Rover，一种已部署的在线 Spark SQL 调优服务，

    Distributed data analytic engines like Spark are common choices to process massive data in industry. However, the performance of Spark SQL highly depends on the choice of configurations, where the optimal ones vary with the executed workloads. Among various alternatives for Spark SQL tuning, Bayesian optimization (BO) is a popular framework that finds near-optimal configurations given sufficient budget, but it suffers from the re-optimization issue and is not practical in real production. When applying transfer learning to accelerate the tuning process, we notice two domain-specific challenges: 1) most previous work focus on transferring tuning history, while expert knowledge from Spark engineers is of great potential to improve the tuning performance but is not well studied so far; 2) history tasks should be carefully utilized, where using dissimilar ones lead to a deteriorated performance in production. In this paper, we present Rover, a deployed online Spark SQL tuning service for e
    
[^135]: 6G O-RAN中的网络辅助智能流量转发: 一个多层优化框架

    Network-Aided Intelligent Traffic Steering in 6G O-RAN: A Multi-Layer Optimization Framework. (arXiv:2302.02711v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02711](http://arxiv.org/abs/2302.02711)

    本论文提出了一个多层优化框架，利用流量分割分布，拥塞控制和调度优化O-RAN应用中的智能流量转发，能够显著降低延迟，取得了良好的实验结果和全面的性能分析。

    

    为了为6G网络实现智能、可编程和多厂商的无线电接入网络（RAN），在标准化和开放式RAN（O-RAN）的发展中做出了相当的努力。然而，迄今为止，O-RAN在控制和优化RAN功能方面的适用性尚未得到广泛研究。本文同时优化流量分割分布、拥塞控制和调度（JFCS），以实现O-RAN中的智能流量转发应用。结合网络效用最大化和随机优化的工具，我们引入了一个多层优化框架，提供快速收敛、长期效用最优化和显著的延迟降低，与现有技术和基准RAN方法相比。我们的主要贡献有三个: i) 我们提出了新颖的JFCS框架，可高效且自适应地将流量引导到适当的无线电单元; ii) 我们基于强化学习开发了低复杂度算法，inne

    To enable an intelligent, programmable and multi-vendor radio access network (RAN) for 6G networks, considerable efforts have been made in standardization and development of open RAN (O-RAN). So far, however, the applicability of O-RAN in controlling and optimizing RAN functions has not been widely investigated. In this paper, we jointly optimize the flow-split distribution, congestion control and scheduling (JFCS) to enable an intelligent traffic steering application in O-RAN. Combining tools from network utility maximization and stochastic optimization, we introduce a multi-layer optimization framework that provides fast convergence, long-term utility-optimality and significant delay reduction compared to the state-of-the-art and baseline RAN approaches. Our main contributions are three-fold: i) we propose the novel JFCS framework to efficiently and adaptively direct traffic to appropriate radio units; ii) we develop low-complexity algorithms based on the reinforcement learning, inne
    
[^136]: 重新审视判别式分类器与生成式分类器：理论与应用

    Revisiting Discriminative vs. Generative Classifiers: Theory and Implications. (arXiv:2302.02334v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02334](http://arxiv.org/abs/2302.02334)

    本文重新审视关于判别式与生成式分类器的经典主题，利用多类$\mathcal{H}$-一致性下界，证明了在温和的假设下，多类朴素贝叶斯分类器的样本要求比逻辑回归分类器多了$O(\log n)$。

    

    大规模深度模型预先在大规模标记或未标记数据上进行训练，可以有效地转移到下游任务。线性评估将预先训练的模型中的参数冻结，并单独训练一个线性分类器，这是一种有效且有吸引力的转移方法。然而，目前很少有研究线性评估中的分类器，除了默认的逻辑回归分类器。本文受到朴素贝叶斯的统计效率启发，重新审视了关于判别式与生成式分类器的经典主题。理论上，本文考虑使用代理损失而不是0-1损失进行分析，并将经典结果从二元情况推广到多类情况。我们表明，在温和的假设下，多类朴素贝叶斯需要$O(\log n)$个样本来接近其渐近误差，而相应的多类逻辑回归需要$O(n)$个样本，其中$n$是特征维度。为了证明这一点，我们提出了一个多类$\mathcal{H}$-一致性下界。

    A large-scale deep model pre-trained on massive labeled or unlabeled data transfers well to downstream tasks. Linear evaluation freezes parameters in the pre-trained model and trains a linear classifier separately, which is efficient and attractive for transfer. However, little work has investigated the classifier in linear evaluation except for the default logistic regression. Inspired by the statistical efficiency of naive Bayes, the paper revisits the classical topic on discriminative vs. generative classifiers. Theoretically, the paper considers the surrogate loss instead of the zero-one loss in analyses and generalizes the classical results from binary cases to multiclass ones. We show that, under mild assumptions, multiclass naive Bayes requires $O(\log n)$ samples to approach its asymptotic error while the corresponding multiclass logistic regression requires $O(n)$ samples, where $n$ is the feature dimension. To establish it, we present a multiclass $\mathcal{H}$-consistency bo
    
[^137]: 基于经济深度学习模型的IoT僵尸网络检测

    IoT Botnet Detection Using an Economic Deep Learning Model. (arXiv:2302.02013v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2302.02013](http://arxiv.org/abs/2302.02013)

    本文提出了一种经济深度学习模型来检测物联网僵尸网络攻击以及不同类型的攻击。该模型能够在较小的预算下加速训练和检测过程，并且具有比最先进的检测模型更高的准确性。

    

    技术创新的快速进步增加了过去十年的使用和分发。全球物联网系统的快速增长增加了由恶意第三方创建的网络安全挑战。因此，考虑安全问题和物联网系统限制的可靠入侵检测和网络取证系统对于保护这些系统至关重要。物联网僵尸网络攻击是企业和个人面临的重大威胁之一。因此，本文提出了一种基于经济的深度学习模型来检测物联网僵尸网络攻击以及不同类型的攻击。所提出的模型在较小的实现预算下，加速了训练和检测过程，并获得了比最先进的检测模型更高的准确性。

    The rapid progress in technology innovation usage and distribution has increased in the last decade. The rapid growth of the Internet of Things (IoT) systems worldwide has increased network security challenges created by malicious third parties. Thus, reliable intrusion detection and network forensics systems that consider security concerns and IoT systems limitations are essential to protect such systems. IoT botnet attacks are one of the significant threats to enterprises and individuals. Thus, this paper proposed an economic deep learning-based model for detecting IoT botnet attacks along with different types of attacks. The proposed model achieved higher accuracy than the state-of-the-art detection models using a smaller implementation budget and accelerating the training and detecting processes.
    
[^138]: 超越稳健性的普适定律：随机特征和神经切向核的更尖锐法则

    Beyond the Universal Law of Robustness: Sharper Laws for Random Features and Neural Tangent Kernels. (arXiv:2302.01629v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.01629](http://arxiv.org/abs/2302.01629)

    本文通过研究随机特征和神经切向核（NTK）的经验风险最小化，证明了在随机特征中，即使满足稳健性的通用定律所需的必要条件，模型也不具有任何过度参数化程度的稳健性。相对地，对于偶激活情况，NTK模型满足普遍下限，只要满足过参数条件就能稳健。这为机器学习中的稳健性提供了更尖锐的法则，超越了先前建立的普适定律。

    

    机器学习模型容易受到对抗性干扰，Bubeck和Sellke的一个有思想启示的文章通过过度参数化的视角分析了这一现象：平滑地插值数据需要的参数显著多于简单地记忆数据。然而，“普适”的法则仅为稳健性提供了必要条件，无法区分模型。本文通过专注于随机特征和神经切向核（NTK）的两个典型设置中的经验风险最小化来解决这些差距。我们证明，在随机特征中，即使满足稳健性的通用定律所需的必要条件，模型也不具有任何过度参数化程度的稳健性。相反，对于偶激活情况，NTK模型满足普遍下限，只要满足过参数条件就能稳健。这也解决了先前在NTK架构的最优性上的猜想。我们的结果为机器学习中的稳健性提供了更尖锐的法则，超越了先前建立的普适定律。

    Machine learning models are vulnerable to adversarial perturbations, and a thought-provoking paper by Bubeck and Sellke has analyzed this phenomenon through the lens of over-parameterization: interpolating smoothly the data requires significantly more parameters than simply memorizing it. However, this "universal" law provides only a necessary condition for robustness, and it is unable to discriminate between models. In this paper, we address these gaps by focusing on empirical risk minimization in two prototypical settings, namely, random features and the neural tangent kernel (NTK). We prove that, for random features, the model is not robust for any degree of over-parameterization, even when the necessary condition coming from the universal law of robustness is satisfied. In contrast, for even activations, the NTK model meets the universal lower bound, and it is robust as soon as the necessary condition on over-parameterization is fulfilled. This also addresses a conjecture in prior 
    
[^139]: 针对对抗扰动的随机集成算法的鲁棒性研究

    On the Robustness of Randomized Ensembles to Adversarial Perturbations. (arXiv:2302.01375v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01375](http://arxiv.org/abs/2302.01375)

    本文对随机集成算法在对抗攻击环境中的鲁棒性进行了研究，提出了新的训练算法 BARRE，能够有效地防御强大的 $\ell_\infty$ 范围内的攻击。

    

    随机集成分类器 (Randomized ensemble classifiers, RECs) 已被证明是一种吸引人的替代传统集成方法的分类器，具有较小的计算需求。然而，最近的研究表明，现有的构造 RECs 的方法比最初声称的更脆弱，对其功效产生了重大怀疑，并引发了一些基本问题，例如：“RECs 何时有效？”，“它们的局限性是什么？”，“我们如何训练它们？”。在本文中，我们首先从理论上探索 RECs 并得出了一些基本结果，例如 RECs 的理论限制、使用它们必要且充分的条件等。在此基础上，我们提出了一种新的增强算法 (BARRE) 用于训练强鲁棒的 RECs，并在各种网络结构和数据集上进行了实证，证明了其对抗强 $\ell_\infty$ 范围内的攻击的有效性。

    Randomized ensemble classifiers (RECs), where one classifier is randomly selected during inference, have emerged as an attractive alternative to traditional ensembling methods for realizing adversarially robust classifiers with limited compute requirements. However, recent works have shown that existing methods for constructing RECs are more vulnerable than initially claimed, casting major doubts on their efficacy and prompting fundamental questions such as: "When are RECs useful?", "What are their limits?", and "How do we train them?". In this work, we first demystify RECs as we derive fundamental results regarding their theoretical limits, necessary and sufficient conditions for them to be useful, and more. Leveraging this new understanding, we propose a new boosting algorithm (BARRE) for training robust RECs, and empirically demonstrate its effectiveness at defending against strong $\ell_\infty$ norm-bounded adversaries across various network architectures and datasets. Our code can
    
[^140]: 双排列等变性在知识图谱补全中的应用

    Double Permutation Equivariance for Knowledge Graph Completion. (arXiv:2302.01313v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01313](http://arxiv.org/abs/2302.01313)

    本研究提出了双排列等变性的KG表示方法，可以使神经网络在KG中执行复杂的逻辑推理任务，并在多个归纳KG完成任务中实现了最先进的Hits@10测试准确率。双排列等变性在KG中开辟了新的研究方向。

    

    本研究将知识图谱(KGs)形式化为一种新型的图，并称之为双交换属性图，其中节点和二元（两个节点之间的）表示必须对节点号和边（及节点）属性（关系和节点特征）的排列等变。双重排列等变的KG表示在KG中开辟了新的研究方向。我们展示了这种等变性对关系的结构表示产生的影响，从而使神经网络能够在KG中执行复杂的逻辑推理任务。最后，我们介绍了一种通用的等变表示蓝图，并测试了一种简单的基于GNN的双排列等变神经结构，在WN18RR、FB237和NELL995归纳KG完成任务中实现了最先进的Hits@10测试准确率，并能够准确执行现有方法无法执行的逻辑推理任务。

    This work provides a formalization of Knowledge Graphs (KGs) as a new class of graphs that we denote doubly exchangeable attributed graphs, where node and pairwise (joint 2-node) representations must be equivariant to permutations of both node ids and edge (& node) attributes (relations & node features). Double-permutation equivariant KG representations open a new research direction in KGs. We show that this equivariance imposes a structural representation of relations that allows neural networks to perform complex logical reasoning tasks in KGs. Finally, we introduce a general blueprint for such equivariant representations and test a simple GNN-based double-permutation equivariant neural architecture that achieve state-of-the-art Hits@10 test accuracy in the WN18RR, FB237 and NELL995 inductive KG completion tasks, and can accurately perform logical reasoning tasks that no existing methods can perform, to the best of our knowledge.
    
[^141]: 上下文套索：通过深度神经网络的方法实现稀疏线性模型

    The contextual lasso: Sparse linear models via deep neural networks. (arXiv:2302.00878v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.00878](http://arxiv.org/abs/2302.00878)

    本论文提出了一种新的统计估计器——上下文套索，可以通过深度神经网络的方法解决解释性和拟合能力的矛盾问题，实现对可解释特征的稀疏拟合，并且稀疏模式和系数会随着上下文特征的变化而发生变化。

    

    稀疏线性模型是可解释机器学习的黄金标准工具，本论文通过使用深度神经网络对稀疏线性模型进行改进，实现了可解释性和强大的拟合能力。上下文套索是一种新的统计估计器，它将输入特征分成可解释特征和上下文特征两组，并对可解释特征进行稀疏拟合，同时其稀疏模式和系数会随着上下文特征的变化而发生变化，这个过程通过深度神经网络无需参数地进行学习。

    Sparse linear models are a gold standard tool for interpretable machine learning, a field of emerging importance as predictive models permeate decision-making in many domains. Unfortunately, sparse linear models are far less flexible as functions of their input features than black-box models like deep neural networks. With this capability gap in mind, we study a not-uncommon situation where the input features dichotomize into two groups: explanatory features, which are candidates for inclusion as variables in an interpretable model, and contextual features, which select from the candidate variables and determine their effects. This dichotomy leads us to the contextual lasso, a new statistical estimator that fits a sparse linear model to the explanatory features such that the sparsity pattern and coefficients vary as a function of the contextual features. The fitting process learns this function nonparametrically via a deep neural network. To attain sparse coefficients, we train the net
    
[^142]: 对数欧氏几何中的多保真度协方差估计

    Multi-Fidelity Covariance Estimation in the Log-Euclidean Geometry. (arXiv:2301.13749v2 [stat.CO] UPDATED)

    [http://arxiv.org/abs/2301.13749](http://arxiv.org/abs/2301.13749)

    该论文提出了一种新颖的协方差矩阵估计方法，采用对数欧氏几何，并融合了具有不同保真度和成本的数据源层次结构的样本，保证了方差的降低和确定性的保持，使得在仿真或数据收集昂贵的应用中，协方差估计成为可行的。该方法在度量学习、数据同化和其他下游任务中具有重要的应用价值。

    

    我们引入了一种利用对数欧氏几何的对称正定流形的多保真度协方差矩阵估计器。该估计器融合了具有不同保真度和成本的数据源层次结构的样本，以实现方差的降低，同时保证确定性，与以前的方法相比。新的估计器使得在仿真或数据收集昂贵的应用中，协方差估计成为可行的。为此，我们开发了一种最佳样本分配方案，以在固定预算的情况下最小化估计器的均方误差。保证确定性对于度量学习、数据同化和其他下游任务至关重要。我们使用物理应用（热传导、流体动力学）的数据对我们的方法进行评估，结果显示了比基准测试更准确的度量学习和超过一个数量级的加速比。

    We introduce a multi-fidelity estimator of covariance matrices that employs the log-Euclidean geometry of the symmetric positive-definite manifold. The estimator fuses samples from a hierarchy of data sources of differing fidelities and costs for variance reduction while guaranteeing definiteness, in contrast with previous approaches. The new estimator makes covariance estimation tractable in applications where simulation or data collection is expensive; to that end, we develop an optimal sample allocation scheme that minimizes the mean-squared error of the estimator given a fixed budget. Guaranteed definiteness is crucial to metric learning, data assimilation, and other downstream tasks. Evaluations of our approach using data from physical applications (heat conduction, fluid dynamics) demonstrate more accurate metric learning and speedups of more than one order of magnitude compared to benchmarks.
    
[^143]: 通过稀疏编码实现无约束动态遗憾

    Unconstrained Dynamic Regret via Sparse Coding. (arXiv:2301.13349v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13349](http://arxiv.org/abs/2301.13349)

    本文探讨了在线线性优化（OLO）涉及无约束问题和动态遗憾问题的复杂性，提出了一种通过重新构造问题为稀疏编码的复杂度度量方式，在适应性和应用上有较好的应用价值。

    

    受时间序列预测的影响，本研究探讨了在线线性优化（OLO）在两个问题结构的耦合下的情况：域无界，而算法的性能是通过动态遗憾来衡量的。处理任一问题都要求遗憾界限依赖于比较序列的某些复杂度量度 - 特别是无约束OLO中的比较器范数，以及动态遗憾中的路径长度。与最近一篇文章(Jacobsen& Cutkosky，2022)适应这两个复杂度量度相比，我们提出了一种通过重新构造问题为稀疏编码的复杂度度量方式。可以通过一个简单的模块化框架实现适应性，这个框架自然地利用了环境更复杂的前置知识。同时，我们还提出了一种新的静态无约束OLO梯度自适应算法，使用了新颖的连续时间机制设计。这可能是具有独立兴趣的。

    Motivated by time series forecasting, we study Online Linear Optimization (OLO) under the coupling of two problem structures: the domain is unbounded, and the performance of an algorithm is measured by its dynamic regret. Handling either of them requires the regret bound to depend on certain complexity measure of the comparator sequence -- specifically, the comparator norm in unconstrained OLO, and the path length in dynamic regret. In contrast to a recent work (Jacobsen & Cutkosky, 2022) that adapts to the combination of these two complexity measures, we propose an alternative complexity measure by recasting the problem into sparse coding. Adaptivity can be achieved by a simple modular framework, which naturally exploits more intricate prior knowledge of the environment. Along the way, we also present a new gradient adaptive algorithm for static unconstrained OLO, designed using novel continuous time machinery. This could be of independent interest.
    
[^144]: 双Kullback-Leibler最小化的变分稀疏逆Cholesky近似用于潜在高斯过程

    Variational sparse inverse Cholesky approximation for latent Gaussian processes via double Kullback-Leibler minimization. (arXiv:2301.13303v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.13303](http://arxiv.org/abs/2301.13303)

    本文提出了一种基于稀疏逆Cholesky因子的高斯分布的变分逼近方法，结合同样高效的SIC约束的Kullback-Leibler最优先验逼近，并在特定SIC排序和稀疏模式下，实现对潜在高斯过程的高度准确先验和后验逼近。与其他方法相比，该方法可以在类似计算复杂度下更准确地预测平稳核函数。

    

    为了实现可扩展和准确的潜在高斯过程推断，我们提出了一种基于一族具有稀疏逆Cholesky（SIC）因子的高斯分布的变分逼近。我们将该变分逼近的后验与类似的高效SIC约束的Kullback-Leibler最优先验逼近相结合。然后，我们重点研究了特定的SIC排序和基于最近邻的稀疏模式，从而产生了高度准确的先验和后验逼近。对于这种设置，我们的变分逼近可以通过每次迭代的对数多项式时间的随机梯度下降来计算。我们提供了数字比较，表明所提出的双Kullback-Leibler最优高斯过程逼近（DKLGP）有时可以比诸如诱导点和均值场逼近等在类似计算复杂度下更准确地预测平稳核函数。

    To achieve scalable and accurate inference for latent Gaussian processes, we propose a variational approximation based on a family of Gaussian distributions whose covariance matrices have sparse inverse Cholesky (SIC) factors. We combine this variational approximation of the posterior with a similar and efficient SIC-restricted Kullback-Leibler-optimal approximation of the prior. We then focus on a particular SIC ordering and nearest-neighbor-based sparsity pattern resulting in highly accurate prior and posterior approximations. For this setting, our variational approximation can be computed via stochastic gradient descent in polylogarithmic time per iteration. We provide numerical comparisons showing that the proposed double-Kullback-Leibler-optimal Gaussian-process approximation (DKLGP) can sometimes be vastly more accurate for stationary kernels than alternative approaches such as inducing-point and mean-field approximations at similar computational complexity.
    
[^145]: 带有矢量量化模型的分层模仿学习

    Hierarchical Imitation Learning with Vector Quantized Models. (arXiv:2301.12962v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.12962](http://arxiv.org/abs/2301.12962)

    本文提出带有矢量量化模型的分层模仿学习方法，通过强化学习识别专家轨迹的子目标并建立矢量量化生成模型实现子目标级别的规划，该算法在解决复杂、长远的决策问题方面优于现有最先进方法。

    

    实现多级抽象的行动规划能力可以使智能体有效地解决复杂任务。然而，学习低级规划模型和高级规划模型并建立它们之间的关联具有挑战性，特别是在高维输入情况下。为了解决这个问题，我们提出使用强化学习来从专家轨迹中识别子目标，通过将奖励的大小与在给定状态和选择的子目标下可预测的低级行动相联系来实现识别。我们针对所识别的子目标建立矢量量化生成模型，以执行子目标级别的规划。在实验中，该算法优于最先进方法，能够解决复杂、长远的决策问题，因为它能够规划，所以在训练集中比现有轨迹找到了更好的轨迹。

    The ability to plan actions on multiple levels of abstraction enables intelligent agents to solve complex tasks effectively. However, learning the models for both low and high-level planning from demonstrations has proven challenging, especially with higher-dimensional inputs. To address this issue, we propose to use reinforcement learning to identify subgoals in expert trajectories by associating the magnitude of the rewards with the predictability of low-level actions given the state and the chosen subgoal. We build a vector-quantized generative model for the identified subgoals to perform subgoal-level planning. In experiments, the algorithm excels at solving complex, long-horizon decision-making problems outperforming state-of-the-art. Because of its ability to plan, our algorithm can find better trajectories than the ones in the training set
    
[^146]: 神经最优输运下的极值域翻译

    Extremal Domain Translation with Neural Optimal Transport. (arXiv:2301.12874v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12874](http://arxiv.org/abs/2301.12874)

    该论文提出了一种称为“极值传输(ET)”的方法，可用于进行给定相似性函数下的一对域之间的最佳可能的非配对翻译，并且提出了一种可扩展的基于神经最优输运(OT)的算法来逼近ET映射。

    

    我们提出了一种称为“极值传输(ET)”的方法，这是对于给定相似性函数的一对域之间的最佳可能的非配对翻译的数学形式化。受到神经最优输运(OT)近期发展的启发，我们提出了一种可扩展的算法，以OT的部分映射极限来逼近ET映射。我们在玩具实例和非配对图像到图像的翻译任务上测试了我们的算法。

    We propose the extremal transport (ET) which is a mathematical formalization of the theoretically best possible unpaired translation between a pair of domains w.r.t. the given similarity function. Inspired by the recent advances in neural optimal transport (OT), we propose a scalable algorithm to approximate ET maps as a limit of partial OT maps. We test our algorithm on toy examples and on the unpaired image-to-image translation task.
    
[^147]: 在高维贝叶斯优化中，随机分解是否足够？

    Are Random Decompositions all we need in High Dimensional Bayesian Optimisation?. (arXiv:2301.12844v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12844](http://arxiv.org/abs/2301.12844)

    本文研究了数据独立分解采样规则，证明了随机树分解采样器有利的理论保证，促进了随机分解上置信度算法（RDUCB）的发展。

    

    学习昂贵的黑盒函数分解有望将贝叶斯优化扩展到高维问题。然而，这些技术的成功取决于找到准确表示黑盒函数的适当分解。我们研究本文中关于数据独立分解采样规则的方法。我们发现，基于数据学习分解可以很容易地被误导到局部分解上，而这些分解在整个搜索空间中并不准确。然后，我们正式证明了基于随机树的分解采样器展现了有利的理论保证，可以有效权衡最大信息增益和实际黑盒函数及其分解之间的函数失配。这些结果促进了随机分解上置信度算法（RDUCB）的发展，该算法易于实现，几乎是即插即用的。

    Learning decompositions of expensive-to-evaluate black-box functions promises to scale Bayesian optimisation (BO) to high-dimensional problems. However, the success of these techniques depends on finding proper decompositions that accurately represent the black-box. While previous works learn those decompositions based on data, we investigate data-independent decomposition sampling rules in this paper. We find that data-driven learners of decompositions can be easily misled towards local decompositions that do not hold globally across the search space. Then, we formally show that a random tree-based decomposition sampler exhibits favourable theoretical guarantees that effectively trade off maximal information gain and functional mismatch between the actual black-box and its surrogate as provided by the decomposition. Those results motivate the development of the random decomposition upper-confidence bound algorithm (RDUCB) that is straightforward to implement - (almost) plug-and-play -
    
[^148]: 基于隐变量谱模型求解高维偏微分方程

    Solving High-Dimensional PDEs with Latent Spectral Models. (arXiv:2301.12664v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12664](http://arxiv.org/abs/2301.12664)

    本文提出一种基于隐变量谱模型的高维PDEs高效精确求解器，它使用基于注意力的分层投影网络将高维数据在线性时间内缩小到一个紧凑的潜空间，并利用谱方法在潜空间中学习算子解决维度诅咒。效果和可扩展性在Navier-Stokes和Schrödinger方程中得到验证。

    

    深度模型在解决偏微分方程（PDEs）方面取得了令人瞩目的进展。一种新兴的范例是学习神经算子来近似PDEs的输入输出映射。虽然以前的深度模型已经探索了多尺度的体系结构和各种算子设计，但它们仅限于在坐标空间中整体学习算子。在实际物理科学问题中，PDEs是具有复杂耦合方程的，数值求解器依赖于高维坐标空间的离散化，这不能被单个算子准确地近似，也不能由于维度诅咒而有效地学习。本文提出了一种基于隐变量谱模型（LSM）的高维PDEs高效精确求解器。LSM不仅超出了坐标空间，而且还利用基于注意力的分层投影网络将高维数据在线性时间内缩小到一个紧凑的潜空间。受数值分析中经典的谱方法的启示，LSM利用谱方法在潜空间中学习算子，可以捕捉复杂的非线性交互并有效地解决维度诅咒。我们在多个高维PDEs上展示了LSM的有效性和可扩展性，如Navier-Stokes方程和Schrödinger方程。

    Deep models have achieved impressive progress in solving partial differential equations (PDEs). A burgeoning paradigm is learning neural operators to approximate the input-output mappings of PDEs. While previous deep models have explored the multiscale architectures and various operator designs, they are limited to learning the operators as a whole in the coordinate space. In real physical science problems, PDEs are complex coupled equations with numerical solvers relying on discretization into high-dimensional coordinate space, which cannot be precisely approximated by a single operator nor efficiently learned due to the curse of dimensionality. We present Latent Spectral Models (LSM) toward an efficient and precise solver for high-dimensional PDEs. Going beyond the coordinate space, LSM enables an attention-based hierarchical projection network to reduce the high-dimensional data into a compact latent space in linear time. Inspired by classical spectral methods in numerical analysis,
    
[^149]: 再次深入探究Few-shot分类问题

    A Closer Look at Few-shot Classification Again. (arXiv:2301.12246v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12246](http://arxiv.org/abs/2301.12246)

    本文研究了Few-shot分类问题中的训练算法和适应算法，并实证证明这两个算法是可以完全分离的。此外，本文的元分析揭示出了关于Few-shot分类的关键方面和与其他领域（如视觉表示学习和迁移学习）的联系的有趣见解。

    

    Few-shot分类算法由训练阶段和适应阶段组成，在训练阶段，模型在一个相对大的数据集上进行学习，在适应阶段，已学习的模型被调整以适应之前从未见过的仅有有限标注样本的任务。本文通过实证证明，训练算法和适应算法是完全独立的，这使得可以分别为每个阶段进行算法分析和设计。针对每个阶段的元分析揭示了一些有趣的见解，有助于更好地理解Few-shot分类的关键方面和与其他领域（如视觉表示学习和迁移学习）的联系。我们希望本文揭示的见解和研究挑战能激发相关方向的未来工作。论文代码和预训练模型（使用PyTorch）可在 https://github.com/Frankluox/CloserLookAgainFewShot 找到。

    Few-shot classification consists of a training phase where a model is learned on a relatively large dataset and an adaptation phase where the learned model is adapted to previously-unseen tasks with limited labeled samples. In this paper, we empirically prove that the training algorithm and the adaptation algorithm can be completely disentangled, which allows algorithm analysis and design to be done individually for each phase. Our meta-analysis for each phase reveals several interesting insights that may help better understand key aspects of few-shot classification and connections with other fields such as visual representation learning and transfer learning. We hope the insights and research challenges revealed in this paper can inspire future work in related directions. Code and pre-trained models (in PyTorch) are available at https://github.com/Frankluox/CloserLookAgainFewShot.
    
[^150]: 可证明鲁棒性强化学习：基于模型的抽象解释方法

    Certifiably Robust Reinforcement Learning through Model-Based Abstract Interpretation. (arXiv:2301.11374v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11374](http://arxiv.org/abs/2301.11374)

    CAROL是一个强化学习框架，它基于模型和抽象解释方法，学习出的策略具有机器可证明的对抗鲁棒性证书，在实验上表现出更好的认证性能和可比较的对抗性能。

    

    我们提出了一个强化学习框架 CAROL，其学习出的策略带有可证明的对抗鲁棒性证书。我们的方法学习了环境的模型，并在每个学习迭代中使用该模型和外部抽象解释器构建可微分的证明鲁棒性信号以引导学习，直到收敛时，该抽象解释就直接导致了可靠性证书。我们给出了一个理论分析，界定了 CAROL 的最坏情况下累积奖励。我们还在四个连续状态和动作空间的 MuJoCo 环境上对 CAROL 进行了实验评估。在这些任务中，CAROL 学习出的策略与现有强化学习算法相比，展示出显著增强的认证性能下限和可比较的对抗性能。

    We present a reinforcement learning (RL) framework in which the learned policy comes with a machine-checkable certificate of provable adversarial robustness. Our approach, called CAROL, learns a model of the environment. In each learning iteration, it uses the current version of this model and an external abstract interpreter to construct a differentiable signal for provable robustness. This signal is used to guide learning, and the abstract interpretation used to construct it directly leads to the robustness certificate returned at convergence. We give a theoretical analysis that bounds the worst-case accumulative reward of CAROL. We also experimentally evaluate CAROL on four MuJoCo environments with continuous state and action spaces. On these tasks, CAROL learns policies that, when contrasted with policies from the state-of-the-art robust RL algorithms, exhibit: (i) markedly enhanced certified performance lower bounds; and (ii) comparable performance under empirical adversarial atta
    
[^151]: 使用多任务深度集合估计因果效应

    Estimating Causal Effects using a Multi-task Deep Ensemble. (arXiv:2301.11351v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11351](http://arxiv.org/abs/2301.11351)

    通过学习研究群体中的共享和特定于组的信息，使用Causal Multi-task Deep Ensemble（CMDE）的方法可以有效地处理高维和多模态协变量，并提供因果效应的点估计不确定性。

    

    已有许多方法被提出来用于因果效应估计，然而很少有方法可以处理像图像等具有复杂结构的数据。为了填补这一空白，我们提出了Causal Multi-task Deep Ensemble (CMDE)，这是一个新颖的框架，能够学习研究群体中的共享和特定于组的信息。我们提供了证明，证明了CMDE与先验中的多任务高斯过程（GP）具有同等效果。与多任务GP相比，CMDE可以有效地处理高维和多模态协变量，并提供因果效应的点估计不确定性。我们评估了我们的方法，并在各种类型的数据集和任务中发现CMDE在大多数任务上优于现有的最先进方法。

    A number of methods have been proposed for causal effect estimation, yet few have demonstrated efficacy in handling data with complex structures, such as images. To fill this gap, we propose Causal Multi-task Deep Ensemble (CMDE), a novel framework that learns both shared and group-specific information from the study population. We provide proofs demonstrating equivalency of CDME to a multi-task Gaussian process (GP) with a coregionalization kernel a priori. Compared to multi-task GP, CMDE efficiently handles high-dimensional and multi-modal covariates and provides pointwise uncertainty estimates of causal effects. We evaluate our method across various types of datasets and tasks and find that CMDE outperforms state-of-the-art methods on a majority of these tasks.
    
[^152]: 最大最优性边际：上下文线性规划和逆线性规划的统一方法

    Maximum Optimality Margin: A Unified Approach for Contextual Linear Programming and Inverse Linear Programming. (arXiv:2301.11260v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11260](http://arxiv.org/abs/2301.11260)

    本论文提出了一种名为“最大最优性边际”的新方法来解决预测-优化问题，通过下游优化的最优性条件设计机器学习损失函数，兼具计算效率和较好的理论性质，而且只需要训练数据中最优解的观测值。

    

    本文研究了预测-优化问题，其中机器学习预测任务的输出用作某个下游优化问题（例如线性规划的目标系数向量）的输入。该问题也被称为预测分析或上下文线性规划。现有方法在很大程度上要么受到（i）优化不可解性（非凸目标函数）/统计效率低下（子优一般化界限）的困扰，要么要求强条件（例如没有约束或损失校准）。我们开发了一种名为“最大最优性边际”的新方法，通过下游优化的最优性条件设计机器学习损失函数。最大边际公式既具有计算效率，又具有好的学习程序的理论性质。更重要的是，我们的新方法只需要训练数据中最优解的观测值。

    In this paper, we study the predict-then-optimize problem where the output of a machine learning prediction task is used as the input of some downstream optimization problem, say, the objective coefficient vector of a linear program. The problem is also known as predictive analytics or contextual linear programming. The existing approaches largely suffer from either (i) optimization intractability (a non-convex objective function)/statistical inefficiency (a suboptimal generalization bound) or (ii) requiring strong condition(s) such as no constraint or loss calibration. We develop a new approach to the problem called \textit{maximum optimality margin} which designs the machine learning loss function by the optimality condition of the downstream optimization. The max-margin formulation enjoys both computational efficiency and good theoretical properties for the learning procedure. More importantly, our new approach only needs the observations of the optimal solution in the training data
    
[^153]: 深度强化学习中的自动内在奖励塑造探索方法研究

    Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning. (arXiv:2301.10886v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10886](http://arxiv.org/abs/2301.10886)

    本文提出了一种名为AIRS的自动内在奖励塑造探索方法，可以提供高质量的内在激励以增强强化学习中的探索性能；并开发了高效可靠的内在奖励工具包。实验表明，AIRS性能卓越，能够胜过基准方案。

    

    本文提出了一种名为AIRS的自动内在奖励塑造方法，通过智能和适应性的塑造函数，提供高质量的内在激励以增强强化学习中的探索性能。AIRS可以根据实时估计的任务回报从预定义的函数集中选择塑造函数，提供可靠的探索激励并解决偏置目标问题。此外，我们开发了一个内在奖励工具包，提供多种内在奖励方法的高效可靠实现方式。我们将AIRS应用在MiniGrid、Procgen和DeepMind控制套件的多项任务中进行测试。大量仿真结果表明，AIRS可以胜过基准方案，并具有简单的架构和卓越的性能。

    We present AIRS: Automatic Intrinsic Reward Shaping that intelligently and adaptively provides high-quality intrinsic rewards to enhance exploration in reinforcement learning (RL). More specifically, AIRS selects shaping function from a predefined set based on the estimated task return in real-time, providing reliable exploration incentives and alleviating the biased objective problem. Moreover, we develop an intrinsic reward toolkit to provide efficient and reliable implementations of diverse intrinsic reward approaches. We test AIRS on various tasks of MiniGrid, Procgen, and DeepMind Control Suite. Extensive simulation demonstrates that AIRS can outperform the benchmarking schemes and achieve superior performance with simple architecture.
    
[^154]: 银行家在线镜像下降：一种用于延迟在线劫匪学习的通用方法

    Banker Online Mirror Descent: A Universal Approach for Delayed Online Bandit Learning. (arXiv:2301.10500v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10500](http://arxiv.org/abs/2301.10500)

    银行家在线镜像下降为在线劫匪学习任务提供了一种可靠的处理反馈延迟的通用方法，具有高效的遗憾界限。

    

    我们提出了一种新的框架银行家在线镜像下降（Banker-OMD），它推广了在线学习文献中的经典在线镜像下降（OMD）技术。Banker-OMD 框架几乎完全解耦了反馈延迟处理和任务特定 OMD 算法设计，从而有助于设计新算法以有效和稳健地处理反馈延迟。具体而言，它提供了一种通用方法，用于在延迟反馈的在线劫匪学习任务中实现$\widetilde{\mathcal O}(\sqrt{T} + \sqrt{D})$风格的遗憾界限，其中$T$是回合数，$D$是总反馈延迟。我们通过应用于两个重要的带延迟反馈的劫匪学习场景（包括延迟的无标度对抗多臂赌博机（MAB）和延迟的对抗线性赌博机）来展示Banker-OMD 的能力。Banker-OMD 导致了第一个延迟的无标度对抗 MAB 算法，其实现了$\widetilde{\ma

    We propose Banker Online Mirror Descent (Banker-OMD), a novel framework generalizing the classical Online Mirror Descent (OMD) technique in the online learning literature. The Banker-OMD framework almost completely decouples feedback delay handling and the task-specific OMD algorithm design, thus facilitating the design of new algorithms capable of efficiently and robustly handling feedback delays. Specifically, it offers a general methodology for achieving $\widetilde{\mathcal O}(\sqrt{T} + \sqrt{D})$-style regret bounds in online bandit learning tasks with delayed feedback, where $T$ is the number of rounds and $D$ is the total feedback delay. We demonstrate the power of \texttt{Banker-OMD} by applications to two important bandit learning scenarios with delayed feedback, including delayed scale-free adversarial Multi-Armed Bandits (MAB) and delayed adversarial linear bandits. \texttt{Banker-OMD} leads to the first delayed scale-free adversarial MAB algorithm achieving $\widetilde{\ma
    
[^155]: DIFFormer：通过受能量限制的扩散引出的可扩展（图形）Transformer

    DIFFormer: Scalable (Graph) Transformers Induced by Energy Constrained Diffusion. (arXiv:2301.09474v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.09474](http://arxiv.org/abs/2301.09474)

    DIFFormer是一种能量受限扩散模型，通过逐渐融合其他实例信息的演化状态，导出了一类新的神经编码器，称为DIFFormer（基于扩散的Transformer），能够揭示真实世界中复杂的数据生成过程。

    

    真实世界的数据生成常常涉及实例之间的复杂相互依赖，违反了标准学习范式的IID数据假设，从而对揭示几何结构以学习所需要的实例表示形成了挑战。为此，我们引入了一种能量受限扩散模型，将一批数据集中的实例编码为逐渐融合了其他实例信息的演化状态。扩散过程受限于基于合理能量函数的下降标准，该函数表征了潜在结构上实例表示的全局一致性。我们提供了严谨的理论，该理论暗示了任意实例对之间的最优扩散强度的闭合形式估计，这导致了一类新的神经编码器的产生：DIFFormer（基于扩散的Transformer），其中包含两个版本：一个简单版本具有线性复杂度，面临着禁忌的实例。

    Real-world data generation often involves complex inter-dependencies among instances, violating the IID-data hypothesis of standard learning paradigms and posing a challenge for uncovering the geometric structures for learning desired instance representations. To this end, we introduce an energy constrained diffusion model which encodes a batch of instances from a dataset into evolutionary states that progressively incorporate other instances' information by their interactions. The diffusion process is constrained by descent criteria w.r.t.~a principled energy function that characterizes the global consistency of instance representations over latent structures. We provide rigorous theory that implies closed-form optimal estimates for the pairwise diffusion strength among arbitrary instance pairs, which gives rise to a new class of neural encoders, dubbed as DIFFormer (diffusion-based Transformers), with two instantiations: a simple version with linear complexity for prohibitive instanc
    
[^156]: 天气预报的Prompt联邦学习：构建气象数据基础模型

    Prompt Federated Learning for Weather Forecasting: Toward Foundation Models on Meteorological Data. (arXiv:2301.09152v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.09152](http://arxiv.org/abs/2301.09152)

    本研究提出了一种跨地区的基础模型，能够理解复杂的气象数据并提供天气预报。采用联邦学习方法解决了地区间数据曝光问题，同时采用新颖的提示学习机制满足低资源传感器的通信和计算约束。

    

    为了应对全球气候挑战，迫切需要开发一个协作平台，对大规模气象数据进行全面的天气预报。尽管很紧迫，但国家和地区之间异构的气象传感器不可避免地导致多变量异质性和数据曝光成为主要障碍。本文开发了一个跨地区的基础模型，能够理解复杂的气象数据并提供天气预报。为了减轻地区间的数据曝光问题，提出了一种新颖的联邦学习方法，通过参与者之间异构的气象数据协作学习全新的时空基于Transformer的基础模型。此外，采用了新颖的提示学习机制，以满足低资源传感器的通信和计算约束。采用三个气象数据集对所提出的方法进行了测试, 结果表明该方法的有效性。

    To tackle the global climate challenge, it urgently needs to develop a collaborative platform for comprehensive weather forecasting on large-scale meteorological data. Despite urgency, heterogeneous meteorological sensors across countries and regions, inevitably causing multivariate heterogeneity and data exposure, become the main barrier. This paper develops a foundation model across regions capable of understanding complex meteorological data and providing weather forecasting. To relieve the data exposure concern across regions, a novel federated learning approach has been proposed to collaboratively learn a brand-new spatio-temporal Transformer-based foundation model across participants with heterogeneous meteorological data. Moreover, a novel prompt learning mechanism has been adopted to satisfy low-resourced sensors' communication and computational constraints. The effectiveness of the proposed method has been demonstrated on classical weather forecasting tasks using three meteoro
    
[^157]: DiME：通过熵矩阵的差异最大化互信息

    DiME: Maximizing Mutual Information by a Difference of Matrix-Based Entropies. (arXiv:2301.08164v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08164](http://arxiv.org/abs/2301.08164)

    本文提出了一种称为DiME的信息理论量，可以估计随机变量之间的互信息最大化，避免了平凡解，适用于多个实际应用。

    

    我们引入了一种信息理论量，具有与相互信息类似的性质，并可从数据中估计，而不需要对潜在分布进行明确假设。该数量基于最近提出的基于矩阵的熵，该熵利用规范化 Gram 矩阵的特征值来计算再生核 Hilbert 空间中未集中协方差运算符的特征值的估计值。我们展示了一种差异矩阵熵（DiME）对于涉及随机变量之间相互信息最大化的问题非常适用。虽然许多此类任务的方法可能会导致平凡解，但 DiME 自然会对这样的结果进行惩罚。我们将 DiME 与多个相互信息基准估计器在一个玩具高斯数据集上进行比较。我们提供了 DiME 的用例示例，例如潜在因子分解和多视图表示学习问题，其中 DiME 被用于学习视图之间的共享表示，该表示具有高互信息量。

    We introduce an information-theoretic quantity with similar properties to mutual information that can be estimated from data without making explicit assumptions on the underlying distribution. This quantity is based on a recently proposed matrix-based entropy that uses the eigenvalues of a normalized Gram matrix to compute an estimate of the eigenvalues of an uncentered covariance operator in a reproducing kernel Hilbert space. We show that a difference of matrix-based entropies (DiME) is well suited for problems involving the maximization of mutual information between random variables. While many methods for such tasks can lead to trivial solutions, DiME naturally penalizes such outcomes. We compare DiME to several baseline estimators of mutual information on a toy Gaussian dataset. We provide examples of use cases for DiME, such as latent factor disentanglement and a multiview representation learning problem where DiME is used to learn a shared representation among views with high mu
    
[^158]: 基于 ODoS 滤波器和深度学习网络的医学图像曲线对象分割

    Curvilinear object segmentation in medical images based on ODoS filter and deep learning network. (arXiv:2301.07475v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2301.07475](http://arxiv.org/abs/2301.07475)

    该论文提出了一种基于 ODoS 滤波器和深度学习网络的曲线结构分割框架，用于解决医学图像中曲线对象的自动分割问题。

    

    在医学图像中，曲线对象的自动分割对于人类疾病的诊断和评估起着重要作用。然而，由于不同的图像外观，曲线对象与其周围背景之间的低对比度，薄而不均匀的曲线结构以及不适当的背景光照条件等问题，这是一项具有挑战性的任务。为了克服这些挑战，我们提出了一个基于ODoS滤波器和深度学习网络的独特曲线结构分割框架，用于在医学图像中进行曲线对象分割。目前，许多深度学习模型强调开发深度体系结构，并忽略捕获曲线对象的结构特征，这可能导致不理想的结果。因此，我们提出了一种将ODoS滤波器作为深度学习网络一部分的新方法，以改进空间信息的提取。

    Automatic segmentation of curvilinear objects in medical images plays an important role in the diagnosis and evaluation of human diseases, yet it is a challenging uncertainty in the complex segmentation tasks due to different issues such as various image appearances, low contrast between curvilinear objects and their surrounding backgrounds, thin and uneven curvilinear structures, and improper background illumination conditions. To overcome these challenges, we present a unique curvilinear structure segmentation framework based on an oriented derivative of stick (ODoS) filter and a deep learning network for curvilinear object segmentation in medical images. Currently, a large number of deep learning models emphasize developing deep architectures and ignore capturing the structural features of curvilinear objects, which may lead to unsatisfactory results. Consequently, a new approach that incorporates an ODoS filter as part of a deep learning network is presented to improve the spatial 
    
[^159]: 学习Boltzmann密度的变形轨迹

    Learning Deformation Trajectories of Boltzmann Densities. (arXiv:2301.07388v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.07388](http://arxiv.org/abs/2301.07388)

    本文介绍了一种学习Boltzmann密度变形轨迹的方法，其中通过插值能量函数等实现Boltzmann密度的变形，然后找到一个时间依赖向量场，将样本从一个分布转移到另一个分布，其表现在高斯混合和量子力学粒子的Boltzmann密度上比KL-反散度更具优势。

    

    我们提出了一种连续标准化流的训练方法，可以在没有样本但存在能量函数的情况下使用。我们的方法依赖于能量函数$f_1$和广义高斯函数$f_0$之间的预定或学习插值$f_t$。能量函数的插值引起Boltzmann密度$p_t\propto e^{-f_t}$的插值，我们旨在找到一个沿着族$p_t$的时间依赖向量场$V_t$，将样本从一个分布转移到另一个分布。将样本沿着族$p_t$从一个分布转移到另一个分布的条件可以转化为$V_t$和$f_t$之间的PDE，我们优化$V_t$和$f_t$以满足此PDE。我们在高斯混合和双井势的量子力学粒子的Boltzmann密度上实验比较了所提出的训练目标与KL-反散度的差异。

    We introduce a training objective for continuous normalizing flows that can be used in the absence of samples but in the presence of an energy function. Our method relies on either a prescribed or a learnt interpolation $f_t$ of energy functions between the target energy $f_1$ and the energy function of a generalized Gaussian $f_0(x) = ||x/\sigma||_p^p$. The interpolation of energy functions induces an interpolation of Boltzmann densities $p_t \propto e^{-f_t}$ and we aim to find a time-dependent vector field $V_t$ that transports samples along the family $p_t$ of densities. The condition of transporting samples along the family $p_t$ can be translated to a PDE between $V_t$ and $f_t$ and we optimize $V_t$ and $f_t$ to satisfy this PDE. We experimentally compare the proposed training objective to the reverse KL-divergence on Gaussian mixtures and on the Boltzmann density of a quantum mechanical particle in a double-well potential.
    
[^160]: 基于均场控制的非可分共享全局状态下多智能体强化学习的近似方法

    Mean-Field Control based Approximation of Multi-Agent Reinforcement Learning in Presence of a Non-decomposable Shared Global State. (arXiv:2301.06889v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.06889](http://arxiv.org/abs/2301.06889)

    本文提出了一种基于均场控制的多智能体强化学习的近似方法，即使智能体共享一个非可分全局状态，也能具有较好的适用性和近似效果。

    

    均场控制是解决大规模多智能体强化学习问题的一种强大的近似工具。然而，均场控制的成功取决于一个假设，即在给定所有智能体的局部状态和行为的情况下，智能体的下一个（局部）状态会互相独立地演变。本文证明了即使在智能体共享一个全局状态的MARL场景中，MFC仍然可以作为一个好的近似工具，前提是智能体的局部状态仍具有条件独立性。我们假设全局状态是非可分的，即不能将它表示为智能体的局部状态的集合。我们将近似误差计算为$\mathcal{O}(e)$，其中$e=\frac{1}{\sqrt{N}}\left[\sqrt{|\mathcal{X}|} +\sqrt{|\mathcal{U}|}\right]$，代表智能体数量的术语为$N$，$|\mathcal{X}|, |\mathcal{U}|$ 表示状态和动作空间的大小。

    Mean Field Control (MFC) is a powerful approximation tool to solve large-scale Multi-Agent Reinforcement Learning (MARL) problems. However, the success of MFC relies on the presumption that given the local states and actions of all the agents, the next (local) states of the agents evolve conditionally independent of each other. Here we demonstrate that even in a MARL setting where agents share a common global state in addition to their local states evolving conditionally independently (thus introducing a correlation between the state transition processes of individual agents), the MFC can still be applied as a good approximation tool. The global state is assumed to be non-decomposable i.e., it cannot be expressed as a collection of local states of the agents. We compute the approximation error as $\mathcal{O}(e)$ where $e=\frac{1}{\sqrt{N}}\left[\sqrt{|\mathcal{X}|} +\sqrt{|\mathcal{U}|}\right]$. The size of the agent population is denoted by the term $N$, and $|\mathcal{X}|, |\mathcal
    
[^161]: 定义、评估和改进基于任务的认知能力对生成模型的影响

    Define, Evaluate, and Improve Task-Oriented Cognitive Capabilities for Instruction Generation Models. (arXiv:2301.05149v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.05149](http://arxiv.org/abs/2301.05149)

    本文提出了基于任务的认知能力，设计了评估方案来比较语言模型和人类的这些能力，通过在导航指令生成问题中的应用，发现模型的语用能力仍需改进。

    

    近期的工作通过为人类设计的心理测试研究了语言模型的认知能力。尽管这些研究有助于了解这些模型的一般能力，但并不能保证一个拥有足够能力通过这些测试的模型实际上会在执行实际任务时使用这些能力。在这项工作中，我们制定了基于任务的认知能力，这是一种人类式认知能力，语言模型可以利用这种能力来执行任务。这些能力包括：(i) 快速生成良好的候选话语的能力 (搜索能力)；(ii) 预测听者如何理解这些话语，并选择最合适的话语 (语用能力)。我们设计了一个评估方案，以比较语言模型与人类的这些能力。通过将此方案应用于导航指令生成问题中的各种模型的比较，我们发现它们的语用能力。

    Recent work studies the cognitive capabilities of language models through psychological tests designed for humans. While these studies are helpful for understanding the general capabilities of these models, there is no guarantee that a model possessing sufficient capabilities to pass those tests would actually use those capabilities in performing real-life tasks. In this work, we formulate task-oriented cognitive capabilities, which are human-like cognitive capabilities that language models leverage to perform tasks. These capabilities are (i) the ability to quickly generate good candidate utterances (the search capability) (ii) the ability to predict how a listener interprets those utterances and choose the most appropriate one (the pragmatic capability). We design an evaluation scheme for comparing these capabilities of a language model with those of a human. Applying this scheme to examine various models in a navigation instruction generation problem, we find that their pragmatic ca
    
[^162]: 通过上下文长度探究黑匣子语言模型解释

    Black-box language model explanation by context length probing. (arXiv:2212.14815v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.14815](http://arxiv.org/abs/2212.14815)

    该论文提出了一个模型不可知的新颖解释技术：上下文长度探测，通过跟踪模型预测与可用上下文长度的关系来对不同上下文分配不同的重要性得分。该方法适用于大型预训练语言模型，并有利于研究远距离依赖性。

    

    大型语言模型的广泛采用强调了改善其可解释性的必要性。我们提出了一种新颖的解释技术：上下文长度探测，它基于跟踪模型预测作为可用上下文长度的函数，并允许对不同上下文分配不同的重要性得分。该技术是模型不可知的，不依赖于除计算token级概率之外的模型内部访问。我们将上下文长度探测应用于大型预训练语言模型，并提供了一些初始的分析和见解，包括研究远距离依赖性的潜力。方法的源代码和交互式演示可用。

    The increasingly widespread adoption of large language models has highlighted the need for improving their explainability. We present context length probing, a novel explanation technique for causal language models, based on tracking the predictions of a model as a function of the length of available context, and allowing to assign differential importance scores to different contexts. The technique is model-agnostic and does not rely on access to model internals beyond computing token-level probabilities. We apply context length probing to large pre-trained language models and offer some initial analyses and insights, including the potential for studying long-range dependencies. The source code and an interactive demo of the method are available.
    
[^163]: 有限结果空间上的相对概率：其公理化、属性和应用的系统考察

    Relative Probability on Finite Outcome Spaces: A Systematic Examination of its Axiomatization, Properties, and Applications. (arXiv:2212.14555v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2212.14555](http://arxiv.org/abs/2212.14555)

    本文提出了将概率看作相对度量的观点，建立了有限结果空间上相对概率函数的公理化，提供了其实例和组合系统，并讨论了相对贝叶斯推断及其数字实现，证明了相对概率空间的拓扑闭包，突显了其在极限下保留信息的能力。

    

    本文提出了将概率看作相对度量而非绝对度量的观点。为了证明这一概念，我们将焦点放在有限结果空间上，并建立了三个基本公理，以确立相对概率函数的要求。我们提供了一组这些函数的实例库和一个组合系统。此外，我们讨论了相对贝叶斯推断及其数字实现。最后，我们证明了相对概率空间的拓扑闭包，突显了其在极限下保留信息的能力。

    This work proposes a view of probability as a relative measure rather than an absolute one. To demonstrate this concept, we focus on finite outcome spaces and develop three fundamental axioms that establish requirements for relative probability functions. We then provide a library of examples of these functions and a system for composing them. Additionally, we discuss a relative version of Bayesian inference and its digital implementation. Finally, we prove the topological closure of the relative probability space, highlighting its ability to preserve information under limits.
    
[^164]: 双重平滑GDA：用于非凸-非凹极小极大优化的全局收敛算法

    Doubly Smoothed GDA: Global Convergent Algorithm for Constrained Nonconvex-Nonconcave Minimax Optimization. (arXiv:2212.12978v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2212.12978](http://arxiv.org/abs/2212.12978)

    本文提出了一种双重平滑梯度下降上升法 (DSGDA)，该算法可以应用于非凸-非凹极小极大优化，并且能够全局收敛并消除极限环。在一定条件下，DSGDA 的迭代复杂度达到了文献中单循环算法的最佳结果。

    

    非凸-非凹极小极大优化近年来受到了广泛的关注，其在机器学习中具有广泛的应用。然而，大多数现有算法不能保证全局收敛，甚至会遭受极限环的困扰。为了解决这个问题，我们提出了一种新颖的单循环算法，称为双重平滑梯度下降上升法 (DSGDA)，它能够自然地平衡原始与对偶更新，并且将极其具有挑战性的非凸-非凹例子中的极限环消除，包括 Forsaken，Bilinearly-coupled minimax，Sixth-order polynomial 和 PolarGame。我们进一步证明，在一个单侧的 $\theta\in(0,1)$ Kurdyka-\L{}ojasiewicz条件（或凸原始/凹对偶函数）下，DSGDA 可以找到一个游戏平衡点，并且具有迭代复杂度 $\mathcal{O}(\epsilon^{-2\max\{2\theta,1\}})$（或 $\mathcal{O}(\epsilon^{-4})$），这些与文献中单循环算法的最佳结果相匹配。

    Nonconvex-nonconcave minimax optimization has received intense attention over the last decade due to its broad applications in machine learning. Unfortunately, most existing algorithms cannot be guaranteed to converge globally and even suffer from limit cycles. To address this issue, we propose a novel single-loop algorithm called doubly smoothed gradient descent ascent method (DSGDA), which naturally balances the primal and dual updates. The proposed DSGDA can get rid of limit cycles in various challenging nonconvex-nonconcave examples in the literature, including Forsaken, Bilinearly-coupled minimax, Sixth-order polynomial, and PolarGame. We further show that under an one-sided Kurdyka-\L{}ojasiewicz condition with exponent $\theta\in(0,1)$ (resp. convex primal/concave dual function), DSGDA can find a game-stationary point with an iteration complexity of $\mathcal{O}(\epsilon^{-2\max\{2\theta,1\}})$ (resp. $\mathcal{O}(\epsilon^{-4})$). These match the best results for single-loop al
    
[^165]: 何时不信任语言模型：探索参数和非参数记忆的有效性和限制。

    When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories. (arXiv:2212.10511v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10511](http://arxiv.org/abs/2212.10511)

    本文通过对10个模型和4种增强方法的实验，发现语言模型在记忆不太流行的实际知识方面存在困难，而检索增强的语言模型表现较好，提出了一种检索增强语言模型的简单有效方法。

    

    尽管大型语言模型在各种任务上表现出色，但仍然难以处理需要丰富世界知识的任务，这暗示了仅依靠其参数来编码丰富的世界知识的局限性。本文旨在通过对10个模型和4种增强方法在PopQA上进行大规模知识探测实验，以了解语言模型在记忆事实知识方面的优点和局限性。我们发现，语言模型难以记忆不太流行的实际知识，并且在长尾中，扩展规模无法明显改善记忆实际知识。然后，我们展示了检索增强的语言模型在很大程度上胜过级别大得多的语言模型，而未经协助的语言模型在涉及高流行实体的问题上仍然具有竞争力。基于这些发现，我们设计了一种简单而有效的强大和高效的检索增强语言模型方法，该方法仅在需要时检索非参数记忆。

    Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the limitations of relying solely on their parameters to encode a wealth of world knowledge. This paper aims to understand LMs' strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments of 10 models and 4 augmentation methods on PopQA, our new open-domain QA dataset with 14k questions. We find that LMs struggle with less popular factual knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the long tail. We then show that retrieval-augmented LMs largely outperform orders of magnitude larger LMs, while unassisted LMs remain competitive in questions about high-popularity entities. Based on those findings, we devise a simple, yet effective, method for powerful and efficient retrieval-augmented LMs, which retrieves non-parametric memories only whe
    
[^166]: 使用语言模型提示进行推理：一项调查

    Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09597](http://arxiv.org/abs/2212.09597)

    本文提供了使用语言模型提示进行推理的前沿研究综合调查。讨论了新兴推理能力出现的潜在原因，并提供系统资源帮助初学者。

    

    推理作为复杂问题解决的重要能力，可以为医疗诊断、谈判等各种实际应用提供后端支持。本文对使用语言模型提示进行推理的前沿研究进行了综合调查。我们介绍了研究成果的比较和总结，并提供了系统资源以帮助初学者。我们还讨论了新兴推理能力出现的潜在原因，并突出了未来的研究方向。资源可在 https://github.com/zjunlp/Prompt4ReasoningPapers 上获取（定期更新）。

    Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
    
[^167]: BLOOM+1：为零样本提示添加语言支持

    BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting. (arXiv:2212.09535v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09535](http://arxiv.org/abs/2212.09535)

    本文在BLOOM模型中应用语言适应策略，将其适应到新语言上，并在八种新语言的零样本提示表现中提升了性能。适配器微调比大模型的持续预训练更有效，提示性能主要由语言适应数据的大小确定。

    

    BLOOM模型是一个大型公开的多语言语言模型，但其预训练仅限于46种语言。为了将BLOOM的好处扩展到其他语言，而不会产生过高的成本，有必要将BLOOM适应到新的语言上。本文将现有的语言适应策略应用于BLOOM，并在资源受限的情况下对其在八种新语言的零样本提示表现进行基准测试。我们发现，语言适应对于提高新语言的零样本性能是有效的。令人惊讶的是，我们发现适配器微调比大模型的持续预训练更有效。此外，我们发现提示性能不会受到语言特定性的显着影响，如书写系统。它主要由语言适应数据的大小确定。我们还向BLOOMZ添加了新语言，这是BLOOM的多任务微调版本，能够跟随提示。

    The BLOOM model is a large publicly available multilingual language model, but its pretraining was limited to 46 languages. To extend the benefits of BLOOM to other languages without incurring prohibitively large costs, it is desirable to adapt BLOOM to new languages not seen during pretraining. In this work, we apply existing language adaptation strategies to BLOOM and benchmark its zero-shot prompting performance on eight new languages in a resource-constrained setting. We find language adaptation to be effective at improving zero-shot performance in new languages. Surprisingly, we find that adapter-based finetuning is more effective than continued pretraining for large models. In addition, we discover that prompting performance is not significantly affected by language specifics, such as the writing system. It is primarily determined by the size of the language adaptation data. We also add new languages to BLOOMZ, which is a multitask finetuned version of BLOOM capable of following 
    
[^168]: 基于张量分解的图神经网络中高效的关系感知邻域聚合

    Efficient Relation-aware Neighborhood Aggregation in Graph Neural Networks via Tensor Decomposition. (arXiv:2212.05581v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.05581](http://arxiv.org/abs/2212.05581)

    本文提出了一个张量分解的知识图编码器，将邻居实体使用由关系类型定义的低秩张量的投影矩阵进行转换，从而产生具有表达能力和关系感知性的表示，并使用对比学习的方法进行训练，从而提高了基于图的神经网络模型的效率和表现。

    

    许多面向知识图谱嵌入的图神经网络(GNN)被提出。然而，大量这种方法忽略了关系信息的重要性，将其与实体信息组合使用效率低下，导致表达能力低。为了解决这个问题，我们在关系图卷积网络(R-GCN)的聚合函数中引入了张量分解，提出了一个通用的知识图编码器。在我们的模型中，使用由关系类型定义的低秩张量的投影矩阵，将邻居实体进行转换，以获得多任务学习的好处，并生成具有表达能力的关系感知表示。此外，我们还提出使用CP分解来估计核心张量的低秩估计，从而压缩和规范我们的模型。我们采用了对比学习的训练方法，以缓解基于1-N方法在大型图上的训练限制。我们使用低维度嵌入，在FB15k-237和WN18RR数据集上取得了有竞争力的结果，说明了我们的模型的效率和有效性。

    Many Graph Neural Networks (GNNs) are proposed for Knowledge Graph Embedding (KGE). However, lots of these methods neglect the importance of the information of relations and combine it with the information of entities inefficiently, leading to low expressiveness. To address this issue, we introduce a general knowledge graph encoder incorporating tensor decomposition in the aggregation function of Relational Graph Convolutional Network (R-GCN). In our model, neighbor entities are transformed using projection matrices of a low-rank tensor which are defined by relation types to benefit from multi-task learning and produce expressive relation-aware representations. Besides, we propose a low-rank estimation of the core tensor using CP decomposition to compress and regularize our model. We use a training method inspired by contrastive learning, which relieves the training limitation of the 1-N method on huge graphs. We achieve favorably competitive results on FB15k-237 and WN18RR with embedd
    
[^169]: ISAACS：安全的迭代软对抗 Actor-Critic 算法

    ISAACS: Iterative Soft Adversarial Actor-Critic for Safety. (arXiv:2212.03228v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.03228](http://arxiv.org/abs/2212.03228)

    本文提出了一种新的算法，ISAACS，通过将博弈论安全分析与对抗强化学习相结合，使机器人系统能够进行可扩展的鲁棒安全控制。实验结果表明，该算法可以有效地学习并避免碰撞，并在安全问题上超越标准的深度强化学习算法。

    

    在不受控制的环境中部署机器人需要它们能够在之前未见过的情况下稳健地运行，例如不规则的地形和风力条件。由于优化控制理论的严格安全框架难以扩展到高维非线性动态系统，而更易处理的“深度”方法计算出的控制策略缺乏保证，并且往往在不确定的操作条件下表现出很少的稳健性。本文提出了一种新的方法，将博弈论安全分析与仿真中的对抗式强化学习相结合，使具有一般非线性动态的机器人系统能够进行可扩展的鲁棒安全控制器的综合合成，受到有界建模误差的限制。采用软性 actor-critic 方法，同时进行一个安全的回退策略和一个称为“干扰”的对抗智能体的协同训练，该对抗智能体致力于唤起设计者不确定性假设下允许的模型误差和训练-部署偏差的最坏情况实现。同时，一个评价网络被训练来评估主策略和回退策略的安全性，提供一个自适应的软约束来指导探索和限制不良行为。在各种任务的实验中表明，我们的框架可以有效地学习避免碰撞的策略，并超越了标准深度强化学习算法在安全问题上的表现，即使存在显著的建模不确定性。

    The deployment of robots in uncontrolled environments requires them to operate robustly under previously unseen scenarios, like irregular terrain and wind conditions. Unfortunately, while rigorous safety frameworks from robust optimal control theory scale poorly to high-dimensional nonlinear dynamics, control policies computed by more tractable "deep" methods lack guarantees and tend to exhibit little robustness to uncertain operating conditions. This work introduces a novel approach enabling scalable synthesis of robust safety-preserving controllers for robotic systems with general nonlinear dynamics subject to bounded modeling error by combining game-theoretic safety analysis with adversarial reinforcement learning in simulation. Following a soft actor-critic scheme, a safety-seeking fallback policy is co-trained with an adversarial "disturbance" agent that aims to invoke the worst-case realization of model error and training-to-deployment discrepancy allowed by the designer's uncert
    
[^170]: 理解对准确性不平衡影响的对抗鲁棒性问题

    Understanding the Impact of Adversarial Robustness on Accuracy Disparity. (arXiv:2211.15762v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15762](http://arxiv.org/abs/2211.15762)

    本文通过研究高斯混合模型下的线性分类器，分析了对抗鲁棒性对准确性不平衡的影响，并证明了在稳定分布的一般家族中也存在类似影响。

    

    尽管长期以来已经从经验上观察到对抗鲁棒性可能与标准准确性存在一些矛盾，并且可能对不同类别产生不平等影响，但它仍然是一个未解决的问题，即这些观察有多大程度的保持，以及类别不平衡在其中扮演什么样的角色。在本文中，我们试图通过更深入地研究高斯混合模型下的线性分类器来理解这个准确性不平衡问题。我们将对抗鲁棒性的影响分解成两部分：一部分是因为鲁棒性约束而会降低所有类别的标准准确性而固有的影响，另一部分是由于类别不平衡比率引起的，这将增加与标准训练相比的准确性差异。此外，我们还表明这些影响超越了高斯混合模型，通过将数据模型推广到稳定分布的一般家族。具体而言，我们证明了，虽然对抗鲁棒性的约束一致会减少所有类别的标准准确性，但通常会增加对少数类别的准确性不平衡。

    While it has long been empirically observed that adversarial robustness may be at odds with standard accuracy and may have further disparate impacts on different classes, it remains an open question to what extent such observations hold and how the class imbalance plays a role within. In this paper, we attempt to understand this question of accuracy disparity by taking a closer look at linear classifiers under a Gaussian mixture model. We decompose the impact of adversarial robustness into two parts: an inherent effect that will degrade the standard accuracy on all classes due to the robustness constraint, and the other caused by the class imbalance ratio, which will increase the accuracy disparity compared to standard training. Furthermore, we also show that such effects extend beyond the Gaussian mixture model, by generalizing our data model to the general family of stable distributions. More specifically, we demonstrate that while the constraint of adversarial robustness consistentl
    
[^171]: 集成多分位数算法：自适应灵活的分布预测用于不确定性量化

    Ensemble Multi-Quantiles: Adaptively Flexible Distribution Prediction for Uncertainty Quantification. (arXiv:2211.14545v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14545](http://arxiv.org/abs/2211.14545)

    本文提出了一种自适应灵活的分布预测方法EMQ，用于量化机器学习中的不确定性。该方法逐步偏离高斯分布并在提升中发现最优条件分布，因此具有较好的实用性。

    

    本文提出了一种新颖、简洁而有效的方法来量化机器学习中的不确定性。它结合了自适应灵活的分布预测，用于回归任务中的条件分布$\mathbb{P}(\mathbf{y}|\mathbf{X}=x)$预测。通过将概率水平的分位数（覆盖区间$(0,1)$）用由我们设计的加法模型提升，来预测这个条件分布。我们寻求$\mathbb{P}(\mathbf{y}|\mathbf{X}=x)$的结构完整性和灵活性之间的自适应平衡，而高斯假设对于真实数据的灵活性不足，高度灵活的方法（例如在没有分布结构的情况下分别估计分位数）不可避免地具有缺陷，并且可能导致无法很好地概括。我们提出的集成多分位数方法EMQ完全是数据驱动的，可以逐步偏离高斯分布并在提升中发现最优条件分布。

    We propose a novel, succinct, and effective approach to quantify uncertainty in machine learning. It incorporates adaptively flexible distribution prediction for $\mathbb{P}(\mathbf{y}|\mathbf{X}=x)$ in regression tasks. For predicting this conditional distribution, its quantiles of probability levels spreading the interval $(0,1)$ are boosted by additive models which are designed by us with intuitions and interpretability. We seek an adaptive balance between the structural integrity and the flexibility for $\mathbb{P}(\mathbf{y}|\mathbf{X}=x)$, while Gaussian assumption results in a lack of flexibility for real data and highly flexible approaches (e.g., estimating the quantiles separately without a distribution structure) inevitably have drawbacks and may not lead to good generalization. This ensemble multi-quantiles approach called EMQ proposed by us is totally data-driven, and can gradually depart from Gaussian and discover the optimal conditional distribution in the boosting. On ex
    
[^172]: 面板数据中无欺诈决策制定的研究及拓展

    Strategyproof Decision-Making in Panel Data Settings and Beyond. (arXiv:2211.14236v3 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2211.14236](http://arxiv.org/abs/2211.14236)

    本文研究了使用面板数据做决策制定时，如何应对生成数据的单位采取策略的情况，提出了一个可以对单位进行正确干预的无欺诈干预策略。

    

    本文研究使用面板数据的决策制定问题，其中决策者得到了多个单位（或代理人）的有噪声、重复的测量结果。我们考虑了一个设置，其中存在一个干预前期，当决策者观察每个单位的结果后，会根据这些观察结果为每个单位分配一个处理。与传统的设置不同的是，我们允许生成面板数据的单位采取策略，即单位可能会修改其干预前的结果以获得更理想的干预。本文旨在设计一个无欺诈的干预策略，也就是一个能够对单位进行正确干预的策略，无论单位是否进行了策略。我们首先确定了一个必要且充分的条件来判断无欺诈的干预策略是否存在，并提供了一个具有简单闭合形式的无欺诈机制。在此过程中，我们证明了具有应用于劳动力市场信号的战略性多类分类设置的不可能性结果。

    We consider the classical problem of decision-making using panel data, in which a decision-maker gets noisy, repeated measurements of multiple units (or agents). We consider a setup where there is a pre-intervention period, when the principal observes the outcomes of each unit, after which the principal uses these observations to assign a treatment to each unit. Unlike this classical setting, we permit the units generating the panel data to be strategic, i.e. units may modify their pre-intervention outcomes in order to receive a more desirable intervention. The principal's goal is to design a strategyproof intervention policy, i.e. a policy that assigns units to their correct interventions despite their potential strategizing. We first identify a necessary and sufficient condition under which a strategyproof intervention policy exists, and provide a strategyproof mechanism with a simple closed form when one does exist. Along the way, we prove impossibility results for strategic multicl
    
[^173]: 通过矩阵分解从逆协方差矩阵中学习大型因果结构

    Learning Large Causal Structures from Inverse Covariance Matrix via Matrix Decomposition. (arXiv:2211.14221v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14221](http://arxiv.org/abs/2211.14221)

    本文提出了一种基于逆协方差矩阵的$\mathcal{O}$-ICID方法，该方法是通过连续优化一种矩阵分解来学习因果结构的，适用于变量数量庞大的情况。该方法可以在噪声方差已知时识别真实DAG, 也可以在较弱的先验信息下给出有用的定向图解

    

    当变量数量庞大时，从观测数据中学习因果结构是一个基本但高度复杂的问题。本文从线性结构方程模型(SEMs)出发，研究了从逆协方差矩阵中学习因果结构的方法。所提出的方法称为$\mathcal{O}$-ICID(来自Oracle 逆协方差矩阵的独立保持分解)，基于一种矩阵分解的连续优化，该分解保留了逆协方差矩阵的非零模式。我们证明了在噪声方差已知的情况下，$\mathcal{O}$-ICID为识别真实有向无环图(DAG)提供了一种高效的方式。在较弱的先验信息下，所提出的方法可以给出有用的定向图解，用于进行更精细的因果发现。当真实DAG具有有限的节点度数时，所提出的方法具有低复杂度，在实验中表现出良好的时间效率。

    Learning causal structures from observational data is a fundamental yet highly complex problem when the number of variables is large. In this paper, we start from linear structural equation models (SEMs) and investigate ways of learning causal structures from the inverse covariance matrix. The proposed method, called $\mathcal{O}$-ICID (for {\it Independence-preserving} Decomposition from Oracle Inverse Covariance matrix), is based on continuous optimization of a type of matrix decomposition that preserves the nonzero patterns of the inverse covariance matrix. We show that $\mathcal{O}$-ICID provides an efficient way for identifying the true directed acyclic graph (DAG) under the knowledge of noise variances. With weaker prior information, the proposed method gives directed graph solutions that are useful for making more refined causal discovery. The proposed method enjoys a low complexity when the true DAG has bounded node degrees, as reflected by its time efficiency in experiments in
    
[^174]: 一类非凸非凹极小极大问题的零阶交替梯度下降算法研究

    Zeroth-Order Alternating Gradient Descent Ascent Algorithms for a Class of Nonconvex-Nonconcave Minimax Problems. (arXiv:2211.13668v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2211.13668](http://arxiv.org/abs/2211.13668)

    本文提出了零阶交替梯度下降算法和零阶方差减少交替梯度下降算法，用于解决一类非凸非凹的极小极大问题，分别在确定性和随机环境下。它们是解决这类问题的第一和第二个迭代复杂度保证的零阶算法。

    

    本文考虑一类非凸非凹的极小极大问题，即NC-PL极小极大问题，其目标函数针对内部变量满足Polyak-Lôjasiewicz（PL）条件。我们提出了零阶交替梯度下降上升（ZO-AGDA）算法和零阶方差减少交替梯度下降上升（ZO-VRAGDA）算法，分别用于确定性和随机环境下解决NC-PL极小极大问题。使用ZO-AGDA和ZO-VRAGDA算法得到NC-PL极小极大问题的ε-稳定点所需的总函数值查询次数上界分别为O(ε^(-2))和O(ε^(-3))。据我们所知，它们是解决NC-PL极小极大问题的第一和第二个迭代复杂度保证的零阶算法。

    In this paper, we consider a class of nonconvex-nonconcave minimax problems, i.e., NC-PL minimax problems, whose objective functions satisfy the Polyak-\L ojasiewicz (PL) condition with respect to the inner variable. We propose a zeroth-order alternating gradient descent ascent (ZO-AGDA) algorithm and a zeroth-order variance reduced alternating gradient descent ascent (ZO-VRAGDA) algorithm for solving NC-PL minimax problem under the deterministic and the stochastic setting, respectively. The total number of function value queries to obtain an $\epsilon$-stationary point of ZO-AGDA and ZO-VRAGDA algorithm for solving NC-PL minimax problem is upper bounded by $\mathcal{O}(\varepsilon^{-2})$ and $\mathcal{O}(\varepsilon^{-3})$, respectively. To the best of our knowledge, they are the first two zeroth-order algorithms with the iteration complexity gurantee for solving NC-PL minimax problems.
    
[^175]: 可扩展和可推广决策制定的遮盖自编码

    Masked Autoencoding for Scalable and Generalizable Decision Making. (arXiv:2211.12740v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.12740](http://arxiv.org/abs/2211.12740)

    本文提出了一种遮盖式决策预测 (MaskDP) 的简单可扩展的自监督预训练方法，在可扩展的增强学习和行为克隆中能够有效地从大规模多样的序列数据中学习，并且零样本转移至新任务。

    

    本文关注于学习可扩展的增强学习代理，使其能够从类似于当前大规模视觉和语言模型的大规模多样的序列数据中学习。为了实现这一目标，本文提出了一种遮盖式决策预测 (MaskDP) 的简单可扩展的自监督预训练方法，用于增强学习和行为克隆。在 MaskDP 方法中，我们利用遮挡自编码器 (MAE) 处理状态-动作轨迹，随机遮盖状态和操作标记并重建缺失的数据。通过这样做，模型需要推断出遮挡的状态和操作，并提取关于动态的信息。我们发现，遮盖不同比例的输入序列显著有助于学习一个更好的模型，能够推广到多个后续任务。在实证研究中，我们发现 MaskDP 模型获得了零样本转移到新的 BC 任务的能力，例如单一和多个目标到达任务，并且可以零样本进行连续控制。

    We are interested in learning scalable agents for reinforcement learning that can learn from large-scale, diverse sequential data similar to current large vision and language models. To this end, this paper presents masked decision prediction (MaskDP), a simple and scalable self-supervised pretraining method for reinforcement learning (RL) and behavioral cloning (BC). In our MaskDP approach, we employ a masked autoencoder (MAE) to state-action trajectories, wherein we randomly mask state and action tokens and reconstruct the missing data. By doing so, the model is required to infer masked-out states and actions and extract information about dynamics. We find that masking different proportions of the input sequence significantly helps with learning a better model that generalizes well to multiple downstream tasks. In our empirical study, we find that a MaskDP model gains the capability of zero-shot transfer to new BC tasks, such as single and multiple goal reaching, and it can zero-shot
    
[^176]: 通过多智能体联赛训练实现异质智能体的协作学习

    Learning Heterogeneous Agent Cooperation via Multiagent League Training. (arXiv:2211.11616v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11616](http://arxiv.org/abs/2211.11616)

    本文提出了一种名为异质联赛训练（HLT）的通用强化学习算法，为解决异质多智能体问题，使用策略池和超网络，提高了异质智能体的合作效率。

    

    现实世界中的许多多智能体系统包括多种能力和功能不同的智能体。这样的异质多智能体系统具有重要的实用优势。然而，与同质系统相比，它们也带来了多智能体强化学习面临的一些挑战，如非稳态问题和策略版本迭代问题。本文提出了一种名为异质联赛训练（HLT）的通用强化学习算法，以解决异质多智能体问题。HLT跟踪代理人在训练期间探索的一组策略，收集异质策略联盟以促进未来的策略优化。此外，引入了超网络，以增加代理人的行为多样性，从而在与具有不同级别的合作技能的队友合作时进行协作。我们使用异质基准任务来证明HLT能够提高协作异质任务的成功率。

    Many multiagent systems in the real world include multiple types of agents with different abilities and functionality. Such heterogeneous multiagent systems have significant practical advantages. However, they also come with challenges compared with homogeneous systems for multiagent reinforcement learning, such as the non-stationary problem and the policy version iteration issue. This work proposes a general-purpose reinforcement learning algorithm named Heterogeneous League Training (HLT) to address heterogeneous multiagent problems. HLT keeps track of a pool of policies that agents have explored during training, gathering a league of heterogeneous policies to facilitate future policy optimization. Moreover, a hyper-network is introduced to increase the diversity of agent behaviors when collaborating with teammates having different levels of cooperation skills. We use heterogeneous benchmark tasks to demonstrate that (1) HLT promotes the success rate in cooperative heterogeneous task
    
[^177]: 全面的符号回归

    Exhaustive Symbolic Regression. (arXiv:2211.11461v2 [astro-ph.CO] UPDATED)

    [http://arxiv.org/abs/2211.11461](http://arxiv.org/abs/2211.11461)

    本文介绍了一种全面的符号回归方法，可以有效地解决传统方法中随机搜索和标准主观性等问题，保证找到最佳函数。

    

    符号回归（SR）算法尝试学习精确适合数据的解析表达式，并以高度可解释的方式呈现。传统SR存在两个基本问题，我们在此处加以解决。首先，这些方法随机地搜索空间（通常使用遗传编程），因此不一定找到最佳函数。其次，用于选择最优方程的标准在精度和简洁性之间平衡时是可变和主观的。为了解决这些问题，我们介绍了全面的符号回归（ESR），它系统地和高效地考虑所有可能的方程——使用给定的算子基础集和最大复杂度——因此，如果参数完美优化，则保证找到真正的最优解和在这些约束条件下的完整函数排名。我们实现了最小描述长度原则作为将这些偏好合并为单个目标的严格方法。

    Symbolic Regression (SR) algorithms attempt to learn analytic expressions which fit data accurately and in a highly interpretable manner. Conventional SR suffers from two fundamental issues which we address here. First, these methods search the space stochastically (typically using genetic programming) and hence do not necessarily find the best function. Second, the criteria used to select the equation optimally balancing accuracy with simplicity have been variable and subjective. To address these issues we introduce Exhaustive Symbolic Regression (ESR), which systematically and efficiently considers all possible equations -- made with a given basis set of operators and up to a specified maximum complexity -- and is therefore guaranteed to find the true optimum (if parameters are perfectly optimised) and a complete function ranking subject to these constraints. We implement the minimum description length principle as a rigorous method for combining these preferences into a single objec
    
[^178]: 用样本选择和平衡损失从长尾嘈杂数据中学习

    Learning from Long-Tailed Noisy Data with Sample Selection and Balanced Loss. (arXiv:2211.10906v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.10906](http://arxiv.org/abs/2211.10906)

    本文提出了一种用样本选择和平衡损失的鲁棒方法，能够更好的处理长尾嘈杂数据的学习。

    

    深度学习的成功依赖于大规模且精心策划的训练数据，然而实际应用中的数据通常是长尾的和嘈杂的。许多方法已被提出来处理长尾数据或嘈杂数据，而很少的方法被开发用于解决长尾嘈杂数据。为了解决这个问题，我们提出了一种用样本选择和平衡损失从长尾嘈杂数据中学习的鲁棒方法。具体来说，我们使用样本选择将嘈杂训练数据分为有标签的干净集合和无标签集合，并以基于模型偏差的平衡损失的半监督方式训练深度神经网络。在基准测试上的广泛实验表明，我们的方法优于现有的最先进的方法。

    The success of deep learning depends on large-scale and well-curated training data, while data in real-world applications are commonly long-tailed and noisy. Many methods have been proposed to deal with long-tailed data or noisy data, while a few methods are developed to tackle long-tailed noisy data. To solve this, we propose a robust method for learning from long-tailed noisy data with sample selection and balanced loss. Specifically, we separate the noisy training data into clean labeled set and unlabeled set with sample selection, and train the deep neural network in a semi-supervised manner with a balanced loss based on model bias. Extensive experiments on benchmarks demonstrate that our method outperforms existing state-of-the-art methods.
    
[^179]: 在线非随机控制简介

    Introduction to Online Nonstochastic Control. (arXiv:2211.09619v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.09619](http://arxiv.org/abs/2211.09619)

    介绍了一种新兴的在线非随机控制方法，通过在一组策略中寻找低后悔，获得对最优策略的近似。

    

    本文介绍了一种新兴的动态系统控制与可微强化学习范式——在线非随机控制，并应用在线凸优化和凸松弛技术得到了具有可证明保证的新方法，在最佳和鲁棒控制方面取得了显著成果。与其他框架不同，该方法的目标是对抗性攻击，在无法预测扰动模型的情况下，通过在一组策略中寻找低后悔，获得对最优策略的近似。

    This text presents an introduction to an emerging paradigm in control of dynamical systems and differentiable reinforcement learning called online nonstochastic control. The new approach applies techniques from online convex optimization and convex relaxations to obtain new methods with provable guarantees for classical settings in optimal and robust control.  The primary distinction between online nonstochastic control and other frameworks is the objective. In optimal control, robust control, and other control methodologies that assume stochastic noise, the goal is to perform comparably to an offline optimal strategy. In online nonstochastic control, both the cost functions as well as the perturbations from the assumed dynamical model are chosen by an adversary. Thus the optimal policy is not defined a priori. Rather, the target is to attain low regret against the best policy in hindsight from a benchmark class of policies.  This objective suggests the use of the decision making frame
    
[^180]: 零偏置标量不变网络

    Scalar Invariant Networks with Zero Bias. (arXiv:2211.08486v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.08486](http://arxiv.org/abs/2211.08486)

    本文证明了在解决许多图像任务(例如图像分类)时可以忽略偏置，并且零偏置神经网络在实际图像分类任务中表现良好，同时具有标量 (乘法) 不变性，从而在改变对比度时仍能保持预测不变。

    

    与权重一样，偏置项也是许多流行的机器学习模型(包括神经网络)可学习的参数。人们认为偏差能有效地增加神经网络表示能力来解决计算机视觉中的各种任务。然而，我们认为，如果我们从第一原理考虑图像在输入空间中的内在分布以及模型应具有的一些期望特性，则偏差可以完全忽略，以解决许多与图像相关的任务，例如图像分类任务。我们的观察结果表明，零偏置神经网络在实际图像分类任务上可能与带偏置的神经网络表现相当。此外，我们证明零偏置神经网络具有称为标量(乘法)不变性的良好属性，这使得当改变输入图像的对比度时，神经网络的预测保持不变。然后，我们将标量不变性扩展到更一般的情况…

    Just like weights, bias terms are the learnable parameters of many popular machine learning models, including neural networks. Biases are believed to effectively increase the representational power of neural networks to solve a wide range of tasks in computer vision. However, we argue that if we consider the intrinsic distribution of images in the input space as well as some desired properties a model should have from the first principles, biases can be completely ignored in addressing many image-related tasks, such as image classification. Our observation indicates that zero-bias neural networks could perform comparably to neural networks with bias at least on practical image classification tasks. In addition, we prove that zero-bias neural networks possess a nice property called scalar (multiplication) invariance, which allows the prediction of neural networks remains the same when altering the contrast of the input image. We then extend scalar invariance to more general cases that a
    
[^181]: OverFlow：在神经转录器上叠加流来提高TTS效果

    OverFlow: Putting flows on top of neural transducers for better TTS. (arXiv:2211.06892v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2211.06892](http://arxiv.org/abs/2211.06892)

    本论文将神经HMM TTS与归一化流相结合，提出了一种强大而全面的概率模型，需要的数据更少，训练更新更少，发音准确性高，主观语音质量接近自然语音。

    

    神经HMM是一种最近提出的神经转录器，用于文本到语音的序列模型。它们结合了经典的统计语音合成和现代的神经TTS的最佳特性，需要更少的数据和较少的训练更新，并且不容易出现由于神经注意力失败导致的无意义输出。在本文中，我们将神经HMM TTS与归一化流相结合，用于描述语音声学的高度非高斯分布。结果是一个强大而全面的概率模型，可以使用准确的最大似然进行训练。实验表明，基于我们的提议的系统需要比可比方法更少的更新，以产生准确的发音和接近自然语音的主观语音质量。请参见https://shivammehta25.github.io/OverFlow/获取音频示例和代码。

    Neural HMMs are a type of neural transducer recently proposed for sequence-to-sequence modelling in text-to-speech. They combine the best features of classic statistical speech synthesis and modern neural TTS, requiring less data and fewer training updates, and are less prone to gibberish output caused by neural attention failures. In this paper, we combine neural HMM TTS with normalising flows for describing the highly non-Gaussian distribution of speech acoustics. The result is a powerful, fully probabilistic model of durations and acoustics that can be trained using exact maximum likelihood. Experiments show that a system based on our proposal needs fewer updates than comparable methods to produce accurate pronunciations and a subjective speech quality close to natural speech. Please see https://shivammehta25.github.io/OverFlow/ for audio examples and code.
    
[^182]: GCondNet: 一种改进小型高维表格数据神经网络的新方法

    GCondNet: A Novel Method for Improving Neural Networks on Small High-Dimensional Tabular Data. (arXiv:2211.06302v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.06302](http://arxiv.org/abs/2211.06302)

    GCondNet利用高维表格数据的隐含结构，通过创建图形并利用图神经网络以及条件训练，提高了潜在预测网络的性能。

    

    神经网络模型在处理高维但样本数量较小的表格数据集时经常遇到困难。其中一个原因是当前的权重初始化方法假定权重之间相互独立，当样本不足以准确估计模型参数时，这可能会产生问题。在这种小数据场景下，利用其他结构可以提高模型的训练稳定性和性能。为解决这个问题，我们提出了GCondNet，一种通过利用表格数据中的隐含结构来增强神经网络的通用方法。我们针对每个数据维度在样本之间创建一个图形，并利用图神经网络 (GNN) 提取这种隐含结构，以及调整潜在预测 MLP 网络的第一层参数进行条件训练。通过创建许多小图，GCondNet 利用了数据的高维特性，从而提高了潜在预测网络的性能。我们通过实验证明了我们的方法的有效性。

    Neural network models often struggle with high-dimensional but small sample-size tabular datasets. One reason is that current weight initialisation methods assume independence between weights, which can be problematic when there are insufficient samples to estimate the model's parameters accurately. In such small data scenarios, leveraging additional structures can improve the model's training stability and performance. To address this, we propose GCondNet, a general approach to enhance neural networks by leveraging implicit structures present in tabular data. We create a graph between samples for each data dimension, and utilise Graph Neural Networks (GNNs) for extracting this implicit structure, and for conditioning the parameters of the first layer of an underlying predictor MLP network. By creating many small graphs, GCondNet exploits the data's high-dimensionality, and thus improves the performance of an underlying predictor network. We demonstrate the effectiveness of our method 
    
[^183]: 基于解耦表示的连续情感强度可控语音合成半监督学习

    Semi-supervised learning for continuous emotional intensity controllable speech synthesis with disentangled representations. (arXiv:2211.06160v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2211.06160](http://arxiv.org/abs/2211.06160)

    本文提出了一种基于半监督学习的方法来控制连续情感强度的语音合成，使用伪标签和基于情感的统一网格几何嵌入空间，其可控性和自然度都较好。

    

    最近的文本到语音模型已经达到了生成类似于人类说话的自然语音的水平。但在表达能力方面还存在一些限制。现有的情感语音合成模型使用情感潜在空间中的缩放参数进行插值特征控制已经显示出了可控性。然而，由于包含情感、说话者等特征的融合，现有模型生成的情感潜在空间难以控制连续的情感强度。在本文中，我们提出了一种新的半监督学习方法，通过学习使用基于音素级语音信息的伪标签产生的中间强度的情感。从提出的模型中构建的嵌入空间满足带有情感基础的统一网格几何。实验结果表明，所提出的方法在可控性和自然度方面具有优越性。

    Recent text-to-speech models have reached the level of generating natural speech similar to what humans say. But there still have limitations in terms of expressiveness. The existing emotional speech synthesis models have shown controllability using interpolated features with scaling parameters in emotional latent space. However, the emotional latent space generated from the existing models is difficult to control the continuous emotional intensity because of the entanglement of features like emotions, speakers, etc. In this paper, we propose a novel method to control the continuous intensity of emotions using semi-supervised learning. The model learns emotions of intermediate intensity using pseudo-labels generated from phoneme-level sequences of speech information. An embedding space built from the proposed model satisfies the uniform grid geometry with an emotional basis. The experimental results showed that the proposed method was superior in controllability and naturalness.
    
[^184]: 一种基于噪声的快速SNN训练新策略

    A noise based novel strategy for faster SNN training. (arXiv:2211.05453v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2211.05453](http://arxiv.org/abs/2211.05453)

    本篇论文提出了一种引入噪声的新的SNN训练方法，它结合了ANN到SNN转换和基于脉冲的反向传播，通过训练单步SNN并将其转换为多步SNN来显著提高精度。

    

    脉冲神经网络（SNN）因其低功耗和强的生物合理性而受到越来越多的关注。SNN的优化是一个具有挑战性的任务。两种主要方法，即人工神经网络（ANN）到SNN的转换和基于脉冲的反向传播（BP），都有其优点和局限性。对于ANN到SNN的转换，它需要长时间的推理来逼近ANN的准确性，从而降低了SNN的优势。使用基于脉冲的BP来训练高精度SNN通常需要比它们的ANN相对应地消耗几十倍的计算资源和时间。在本文中，我们提出了一种结合了两种方法优点的新的SNN训练方法。我们首先通过将神经电位分布近似为随机噪声来训练单步SNN(T=1)，然后无损地将单步SNN(T=1)转换为多步SNN(T=N)。引入高斯分布噪声可以显著提高转换后的精度。

    Spiking neural networks (SNNs) are receiving increasing attention due to their low power consumption and strong bio-plausibility. Optimization of SNNs is a challenging task. Two main methods, artificial neural network (ANN)-to-SNN conversion and spike-based backpropagation (BP), both have their advantages and limitations. For ANN-to-SNN conversion, it requires a long inference time to approximate the accuracy of ANN, thus diminishing the benefits of SNN. With spike-based BP, training high-precision SNNs typically consumes dozens of times more computational resources and time than their ANN counterparts. In this paper, we propose a novel SNN training approach that combines the benefits of the two methods. We first train a single-step SNN(T=1) by approximating the neural potential distribution with random noise, then convert the single-step SNN(T=1) to a multi-step SNN(T=N) losslessly. The introduction of Gaussian distributed noise leads to a significant gain in accuracy after conversion
    
[^185]: 基于强化学习的自然语言视觉推理：lilGym

    lilGym: Natural Language Visual Reasoning with Reinforcement Learning. (arXiv:2211.01994v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01994](http://arxiv.org/abs/2211.01994)

    本文提出了一个基于自然语言视觉推理的强化学习基准测试——lilGym，它由2661个高度组合的人类编写自然语言语句和交互式视觉环境组成，并通过注释可执行Python程序来实现精确的奖励计算。本文的实验结果和分析表明，lilGym是一个具有挑战性的开放性问题。

    

    本文介绍了一种新的有关语言条件下强化学习在视觉环境下的基准测试——lilGym。lilGym基于2661个高度组合的人类编写的自然语言陈述，这些陈述是基于一个交互式视觉环境的。我们采用了一种新的方法，在每种可能的世界状态下，通过为所有语句注释可执行的Python程序，实现了精确的奖励计算。每个语句都与多个起始状态和奖励函数配对，以形成数千个不同难度的马尔可夫决策过程。我们使用不同的模型和学习机制进行了lilGym实验。我们的实验结果和分析表明，虽然现有的方法能够实现较高的性能，但是lilGym形成了一个具有挑战性的开放性问题。lilGym可以在 https://lil.nlp.cornell.edu/lilgym/ 上获得。

    We present lilGym, a new benchmark for language-conditioned reinforcement learning in visual environments. lilGym is based on 2,661 highly-compositional human-written natural language statements grounded in an interactive visual environment. We introduce a new approach for exact reward computation in every possible world state by annotating all statements with executable Python programs. Each statement is paired with multiple start states and reward functions to form thousands of distinct Markov Decision Processes of varying difficulty. We experiment with lilGym with different models and learning regimes. Our results and analysis show that while existing methods are able to achieve non-trivial performance, lilGym forms a challenging open problem. lilGym is available at https://lil.nlp.cornell.edu/lilgym/.
    
[^186]: 通过多任务微调实现跨语言泛化

    Crosslingual Generalization through Multitask Finetuning. (arXiv:2211.01786v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.01786](http://arxiv.org/abs/2211.01786)

    该论文通过多任务微调实现跨语言泛化。研究表明，在英语提示下，对大型多语言模型进行英语任务的微调，可以实现对仅出现在预训练语料库中的非英语语言的任务泛化，并且使用英语提示进行多语言任务的微调进一步提高了在英语和非英语任务上的表现，从而实现了各种零-shot结果的最新水平。

    

    已经证明，多任务微调可以帮助大型语言模型在零-shot场景下推广到新的任务，但目前MTF的研究集中在英语数据和模型上。我们将MTF应用于预训练的多语言BLOOM和mT5模型系列，生成了经过微调的变体BLOOMZ和mT0。我们发现，在英语提示下，对大型多语言语言模型进行英语任务的微调，可以实现对仅出现在预训练语料库中的非英语语言的任务泛化。使用英语提示进行多语言任务的微调进一步提高了在英语和非英语任务上的表现，从而实现了各种零-shot结果的最新水平。我们还研究了在英语翻译为每个数据集的语言的情况下进行多语言任务微调。我们发现，在这些机器翻译提示上训练可以在各自语言中更好地完成人写的提示。令人惊讶的是，我们发现m

    Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find m
    
[^187]: 神经崩溃的扰动分析

    Perturbation Analysis of Neural Collapse. (arXiv:2210.16658v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.16658](http://arxiv.org/abs/2210.16658)

    本文提出了一种能够捕捉神经崩溃现象的模型，通过强制特征留在预定义的特征矩阵的附近来实现。通过扰动分析，在小邻域情况下建立了系数范数不变性和特征抖动的联系。

    

    对于分类任务的深度神经网络训练，在零训练误差点之后，通常涉及最小化训练损失。在这一阶段的训练中，观察到了一种“神经崩溃”行为：同一类别样本的特征（倒数第二层的输出）的变异性减少，不同类别的平均特征趋向于某个紧密的框架结构。最近的研究通过理想化的无约束特征模型分析了这种行为，在所有最小化器都出现完全崩溃的情况下。然而，在实际网络和数据集中，特征通常不会达到完全崩溃的状态，例如，因为深层无法任意修改远离崩溃状态的中间特征。在本文中，我们提出了一种更丰富的模型，可以通过强制特征留在预定义的特征矩阵（例如中间特征）的附近来捕捉这种现象。我们通过扰动分析探索了小邻域情况下的模型，并建立了系数范数不变性和特征抖动的联系。

    Training deep neural networks for classification often includes minimizing the training loss beyond the zero training error point. In this phase of training, a "neural collapse" behavior has been observed: the variability of features (outputs of the penultimate layer) of within-class samples decreases and the mean features of different classes approach a certain tight frame structure. Recent works analyze this behavior via idealized unconstrained features models where all the minimizers exhibit exact collapse. However, with practical networks and datasets, the features typically do not reach exact collapse, e.g., because deep layers cannot arbitrarily modify intermediate features that are far from being collapsed. In this paper, we propose a richer model that can capture this phenomenon by forcing the features to stay in the vicinity of a predefined features matrix (e.g., intermediate features). We explore the model in the small vicinity case via perturbation analysis and establish res
    
[^188]: 读懂代码背后：模拟AI辅助编程中的用户行为和成本

    Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming. (arXiv:2210.14306v3 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2210.14306](http://arxiv.org/abs/2210.14306)

    该论文研究了代码推荐系统GitHub Copilot，发现了程序员与该系统交互的一些常见活动，揭示了其低效性和时间成本，从而为改进界面设计和度量方法提供了动力。

    

    代码推荐系统，如Copilot和CodeWhisperer，通过自动建议和自动完成代码，有潜力提高程序员的生产率。然而，要充分发挥它们的潜力，我们必须了解程序员如何与这些系统交互，并确定改进交互的方法。为了取得进展，我们研究了每天由数百万程序员使用的代码推荐系统GitHub Copilot。我们开发了一个常见程序员活动的分类系统CUPS，以便模拟用户与Copilot的交互。我们对21名完成编码任务并回顾性地使用CUPS标记其会话的程序员的研究表明，CUPS可以帮助我们了解程序员如何与代码推荐系统交互，揭示了效率低下和时间成本。我们的洞见揭示了程序员如何与Copilot交互，并激发了新的界面设计和度量方法。

    Code-recommendation systems, such as Copilot and CodeWhisperer, have the potential to improve programmer productivity by suggesting and auto-completing code. However, to fully realize their potential, we must understand how programmers interact with these systems and identify ways to improve that interaction. To make progress, we studied GitHub Copilot, a code-recommendation system used by millions of programmers daily. We developed CUPS, a taxonomy of common programmer activities when interacting with Copilot. Our study of 21 programmers, who completed coding tasks and retrospectively labeled their sessions with CUPS, showed that CUPS can help us understand how programmers interact with code-recommendation systems, revealing inefficiencies and time costs. Our insights reveal how programmers interact with Copilot and motivate new interface designs and metrics.
    
[^189]: WaveBound：稳定时间序列预测的动态误差界限

    WaveBound: Dynamic Error Bounds for Stable Time Series Forecasting. (arXiv:2210.14303v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14303](http://arxiv.org/abs/2210.14303)

    本文提出了一种名为WaveBound的正则化方法，通过引入动态误差界限来解决时间序列预测中的过度拟合问题，以显著提高泛化能力。

    

    时间序列预测已成为一项关键任务，因为它在实际应用中具有高实用性，如交通、能源消耗、经济和金融、疾病分析等。最近的基于深度学习的方法在时间序列预测方面取得了显着的成功。然而，由于时间序列数据的动态性，深度网络仍然面临不稳定的训练和过度拟合问题。在实际数据中出现的不一致模式导致模型对特定模式存在偏差，从而限制了泛化性能。在本文中，我们引入了训练损失的动态误差界限，以解决时间序列预测中的过度拟合问题。因此，我们提出了一种名为WaveBound的正则化方法，该方法估计每个时间步长和每个特征的训练损失的适当误差界限。通过使模型专注于不可预测的数据较少，WaveBound稳定了训练过程，从而显着提高了泛化能力。

    Time series forecasting has become a critical task due to its high practicality in real-world applications such as traffic, energy consumption, economics and finance, and disease analysis. Recent deep-learning-based approaches have shown remarkable success in time series forecasting. Nonetheless, due to the dynamics of time series data, deep networks still suffer from unstable training and overfitting. Inconsistent patterns appearing in real-world data lead the model to be biased to a particular pattern, thus limiting the generalization. In this work, we introduce the dynamic error bounds on training loss to address the overfitting issue in time series forecasting. Consequently, we propose a regularization method called WaveBound which estimates the adequate error bounds of training loss for each time step and feature at each iteration. By allowing the model to focus less on unpredictable data, WaveBound stabilizes the training process, thus significantly improving generalization. With
    
[^190]: 基于子图的GNN后门攻击研究

    Motif-Backdoor: Rethinking the Backdoor Attack on Graph Neural Networks via Motifs. (arXiv:2210.13710v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13710](http://arxiv.org/abs/2210.13710)

    本文提出了一种基于motif的GNN后门攻击方法，设计了一种基于motif的触发器生成器，能够提高后门攻击的隐蔽性和效果，在各种基准数据集上进行实验证明其性能优于现有方法。

    

    具有强大表示能力的图神经网络（GNN）已经被广泛应用于生物基因预测、社交推荐等领域。最近的研究表明，GNN容易受到后门攻击的影响，即使用恶意样本训练的模型很容易被修补后的样本欺骗。大多数的后门攻击研究使用的触发器要么是随机生成的子图（例如 erd\H{o}s-r\'enyi 后门），以减轻计算负担，要么是基于梯度的生成子图（例如图Trojan攻击），以使攻击更加有效。然而，如何理解触发器结构与后门攻击效果之间的关系却在当前文献中被忽略了。在图中，重复性和具有统计显著性的子图（motif）包含了丰富的结构信息。因此，本文从motif的角度重新思考触发器的设计，并提出一种基于motif的GNN后门攻击方法。我们设计了一种基于motif的触发器生成器，利用motif信息提高了后门攻击的隐蔽性和效果。在各种基准数据集上进行了大量实验，结果表明我们的方法在攻击成功率和隐蔽性方面均优于现有的后门攻击方法。

    Graph neural network (GNN) with a powerful representation capability has been widely applied to various areas, such as biological gene prediction, social recommendation, etc. Recent works have exposed that GNN is vulnerable to the backdoor attack, i.e., models trained with maliciously crafted training samples are easily fooled by patched samples. Most of the proposed studies launch the backdoor attack using a trigger that either is the randomly generated subgraph (e.g., erd\H{o}s-r\'enyi backdoor) for less computational burden, or the gradient-based generative subgraph (e.g., graph trojaning attack) to enable a more effective attack. However, the interpretation of how is the trigger structure and the effect of the backdoor attack related has been overlooked in the current literature. Motifs, recurrent and statistically significant sub-graphs in graphs, contain rich structure information. In this paper, we are rethinking the trigger from the perspective of motifs, and propose a motif-ba
    
[^191]: 具有保证的PAC-Bayesian离线情境强化学习算法

    PAC-Bayesian Offline Contextual Bandits With Guarantees. (arXiv:2210.13132v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.13132](http://arxiv.org/abs/2210.13132)

    本文提出了一种通过PAC-Bayesian方法分析离线情境强化学习问题的新算法，该算法通过优化新的泛化界限提供了保证，并在实际情境中得到了验证。

    

    本文提出了一种新的基于PAC-Bayesian方法的离线情境强化学习算法。与之前的方法不同，该方法不是从难以处理或不准确的界限推导学习原则。我们通过PAC-Bayesian方法分析问题，将策略解释为决策规则的混合物。这使我们能够提出新的泛化界限，并提供可解算法来优化它们。我们证明所得界限比竞争对手更紧，可以直接优化以在离线情况下自信地改进记录策略。我们的方法学习带保证的策略，使用所有可用数据，并不需要在保留集上调整更多的超参数。通过广泛的实验，我们展示了该方法在实际情景中提供性能保证的有效性。

    This paper introduces a new principled approach for off-policy learning in contextual bandits. Unlike previous work, our approach does not derive learning principles from intractable or loose bounds. We analyse the problem through the PAC-Bayesian lens, interpreting policies as mixtures of decision rules. This allows us to propose novel generalization bounds and provide tractable algorithms to optimize them. We prove that the derived bounds are tighter than their competitors, and can be optimized directly to confidently improve upon the logging policy offline. Our approach learns policies with guarantees, uses all available data and does not require tuning additional hyperparameters on held-out sets. We demonstrate through extensive experiments the effectiveness of our approach in providing performance guarantees in practical scenarios.
    
[^192]: 基于拉普拉斯增强的低秩张量样条克里金方法的稀疏传感大规模交通速度估计

    Correlating sparse sensing for large-scale traffic speed estimation: A Laplacian-enhanced low-rank tensor kriging approach. (arXiv:2210.11780v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.11780](http://arxiv.org/abs/2210.11780)

    本文提出了一种基于拉普拉斯增强的低秩张量样条克里金方法，用于在有限观测下进行大规模交通速度估计，以从不完整的数据中恢复可信的估计值。

    

    交通速度是表征道路网络流动性的核心因素，许多交通应用程序都依赖于它，如实时导航、动态路线规划和拥堵管理。传感和通信技术的快速进展使交通速度检测比以往任何时候都更加容易。然而，由于静态传感器的稀疏部署或移动传感器的低渗透，检测到的速度是不完整的，并且远离全网使用。此外，由于各种原因传感器容易出现误差或缺失数据，这些传感器检测到的速度会变得非常嘈杂。因此，需要有效的技术从不完整的数据中恢复可信的估计值。在本研究中，我们首先将问题确定为一个时空克里金问题，并提出了一种拉普拉斯增强的低秩张量完成（LETC）框架，其具有低秩性和多维相关性，用于在有限观测下进行大规模交通速度克里金。

    Traffic speed is central to characterizing the fluidity of the road network. Many transportation applications rely on it, such as real-time navigation, dynamic route planning, and congestion management. Rapid advances in sensing and communication techniques make traffic speed detection easier than ever. However, due to sparse deployment of static sensors or low penetration of mobile sensors, speeds detected are incomplete and far from network-wide use. In addition, sensors are prone to error or missing data due to various kinds of reasons, speeds from these sensors can become highly noisy. These drawbacks call for effective techniques to recover credible estimates from the incomplete data. In this work, we first identify the issue as a spatiotemporal kriging problem and propose a Laplacian enhanced low-rank tensor completion (LETC) framework featuring both lowrankness and multi-dimensional correlations for large-scale traffic speed kriging under limited observations. To be specific, th
    
[^193]: 基于源内样式增强的领域泛化改进方法

    Intra-Source Style Augmentation for Improved Domain Generalization. (arXiv:2210.10175v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.10175](http://arxiv.org/abs/2210.10175)

    本论文提出了一种基于源内样式增强（ISSA）的方法，用于改进语义分割中的领域泛化。通过使用掩模噪声编码器随机化样式和内容组合，ISSA有效地增加了训练数据的多样性并减少了虚假相关性，从而在不同类型的驾驶场景语义分割中获得了高达12.4％的mIoU改进。

    

    深度学习模型在应用中经常出现领域偏移，比如在自动驾驶中的泛化仍然是一个重大挑战。因此，我们提出了一种基于源内样式增强（ISSA）的方法，用于改进语义分割中的领域泛化。我们的方法基于一种新型的StyleGAN2反演掩模噪声编码器。该模型通过噪声预测学习忠实重建图像并保留其语义布局。估计噪声的随机掩蔽使我们的模型具有样式混合能力，即它可以在不影响图像语义布局的情况下改变全局外观。使用所提出的掩模噪声编码器来随机化训练集中的样式和内容组合，ISSA有效地增加了训练数据的多样性并减少了虚假相关性。结果，在不同类型的驾驶场景语义分割中，我们取得了高达12.4％的mIoU改进。

    The generalization with respect to domain shifts, as they frequently appear in applications such as autonomous driving, is one of the remaining big challenges for deep learning models. Therefore, we propose an intra-source style augmentation (ISSA) method to improve domain generalization in semantic segmentation. Our method is based on a novel masked noise encoder for StyleGAN2 inversion. The model learns to faithfully reconstruct the image preserving its semantic layout through noise prediction. Random masking of the estimated noise enables the style mixing capability of our model, i.e. it allows to alter the global appearance without affecting the semantic layout of an image. Using the proposed masked noise encoder to randomize style and content combinations in the training set, ISSA effectively increases the diversity of training data and reduces spurious correlation. As a result, we achieve up to $12.4\%$ mIoU improvements on driving-scene semantic segmentation under different type
    
[^194]: STaSy：基于分值的表格数据生成

    STaSy: Score-based Tabular data Synthesis. (arXiv:2210.04018v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.04018](http://arxiv.org/abs/2210.04018)

    提出了一种新的基于分值的表格数据生成模型（STaSy），并采用自适应学习技术和微调策略进一步提高采样质量和多样性。

    

    表格数据生成是机器学习中一个长期存在的研究主题。过去几十年中提出了许多不同的方法，从统计方法到深度生成式方法。然而，由于实际表格数据的复杂性，它并不总是成功的。在本文中，我们提出了一种名为基于分值的表格数据生成（STaSy）的新模型及其基于分值生成建模范例的训练策略。尽管基于分值的生成模型已经解决了许多生成模型中的问题，但表格数据生成仍有改进的空间。我们提出的训练策略包括自适应学习技术和微调策略，通过稳定去噪分值匹配训练进一步增加采样质量和多样性。此外，我们还从采样质量、多样性和时间三个方面进行了严格的实验研究。在o

    Tabular data synthesis is a long-standing research topic in machine learning. Many different methods have been proposed over the past decades, ranging from statistical methods to deep generative methods. However, it has not always been successful due to the complicated nature of real-world tabular data. In this paper, we present a new model named Score-based Tabular data Synthesis (STaSy) and its training strategy based on the paradigm of score-based generative modeling. Despite the fact that score-based generative models have resolved many issues in generative models, there still exists room for improvement in tabular data synthesis. Our proposed training strategy includes a self-paced learning technique and a fine-tuning strategy, which further increases the sampling quality and diversity by stabilizing the denoising score matching training. Furthermore, we also conduct rigorous experimental studies in terms of the generative task trilemma: sampling quality, diversity, and time. In o
    
[^195]: STSyn：通过容错同步加速本地SGD

    STSyn: Speeding Up Local SGD with Straggler-Tolerant Synchronization. (arXiv:2210.03521v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.03521](http://arxiv.org/abs/2210.03521)

    本文提出了一种名为STSyn的本地SGD策略，通过容错同步技术，等待最快的工人，充分利用每个工人的有效本地更新，成功地解决了同步本地随机梯度下降中因落后工人和通信效率低下而引起的问题。

    

    同步本地随机梯度下降（local SGD）会因为一些工作单元空闲并因慢工或落后工人而存在一些随机延迟，因为它在等待工人完成相同数量的本地更新。本文提出了一种新的本地SGD策略——STSyn，以减轻落后者和提高通信效率。关键是等待$K$个最快的工人，同时让所有工人在每个同步循环中始终计算，并充分利用每个工人的有效（已完成）本地更新，无论是否存在落后者。文章提供了关于STSyn性能的平均挂钟时间、本地更新的平均数量和每轮上传工人的平均数量的分析。即使目标函数是非凸的，STSyn的收敛性也得到了严格的证明。实验结果显示，通过使用容错同步技术，所提出的STSyn优于现有的最先进方案。

    Synchronous local stochastic gradient descent (local SGD) suffers from some workers being idle and random delays due to slow and straggling workers, as it waits for the workers to complete the same amount of local updates. In this paper, to mitigate stragglers and improve communication efficiency, a novel local SGD strategy, named STSyn, is developed. The key point is to wait for the $K$ fastest workers, while keeping all the workers computing continually at each synchronization round, and making full use of any effective (completed) local update of each worker regardless of stragglers. An analysis of the average wall-clock time, average number of local updates and average number of uploading workers per round is provided to gauge the performance of STSyn. The convergence of STSyn is also rigorously established even when the objective function is nonconvex. Experimental results show the superiority of the proposed STSyn against state-of-the-art schemes through utilization of the stragg
    
[^196]: VIMA：多模态提示实现通用机器人操作

    VIMA: General Robot Manipulation with Multimodal Prompts. (arXiv:2210.03094v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.03094](http://arxiv.org/abs/2210.03094)

    本研究提出了一种新的机器人操作方式——多模态提示实现通用机器人操作。通过设计一个基于变压器的机器人代理VIMA，可以处理提示并自回归地输出电机动作，实现了各种任务类型的最新结果，并能够零样本泛化到新的对象类别，这对实现通用机器人操作具有前景。

    

    基于提示的学习模式已经成为自然语言处理中的成功范例，在此模式下，单个通用语言模型可以按照输入提示执行任何任务。然而，在机器人工程中，任务规范的形式多种多样，例如，模仿单次演示、遵循语言指令和达到视觉目标等。这些任务通常被认为是不同的任务，并由专门的模型来处理。我们展示了一种广泛的机器人操作任务可以通过多模态提示来表达，交错文本和视觉令牌。因此，我们开发了一个新的仿真基准，其中包含数千个程序生成的桌面任务，具有多模态提示，60万个专家轨迹以进行模仿学习，并采用四级评估协议进行系统化的广义化。我们设计了一个基于变压器的机器人代理，VIMA，该代理处理这些提示并自回归地输出电机动作。VIMA具有一套配方，实现了各种任务类型的最新结果，包括未见过的模态组合，甚至可以零样本泛化到新的对象类别。总体而言，我们的工作提出了一种有前途的方法，采用统一的基于提示的框架实现通用机器人操作。

    Prompt-based learning has emerged as a successful paradigm in natural language processing, where a single general-purpose language model can be instructed to perform any task specified by input prompts. Yet task specification in robotics comes in various forms, such as imitating one-shot demonstrations, following language instructions, and reaching visual goals. They are often considered different tasks and tackled by specialized models. We show that a wide spectrum of robot manipulation tasks can be expressed with multimodal prompts, interleaving textual and visual tokens. Accordingly, we develop a new simulation benchmark that consists of thousands of procedurally-generated tabletop tasks with multimodal prompts, 600K+ expert trajectories for imitation learning, and a four-level evaluation protocol for systematic generalization. We design a transformer-based robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively. VIMA features a recipe that achieve
    
[^197]: 基于同步和同伦优化的神经常微分方程训练方法用于精确动力学发现

    Homotopy-based training of NeuralODEs for accurate dynamics discovery. (arXiv:2210.01407v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01407](http://arxiv.org/abs/2210.01407)

    本论文提出了一种新的神经常微分方程训练方法，基于同步和同伦优化，可以用于从时间序列数据中提取动力学规律，而无需对模型架构进行修改。

    

    神经常微分方程（NeuralODEs）作为神经网络和物理科学基于微分方程的建模范式之间的桥梁，是从时间序列数据中提取动力学规律的一种有吸引力的方式。然而，这些模型通常表现出长时间的训练和次优的结果，特别是对于更长时间段的数据。本文提出了一种基于同步和同伦优化的神经常微分方程训练方法，不需要对模型架构进行改变。我们展示了将模型动力学和训练数据同步可以驯服原本不规则的损失和的景象，同伦优化可以利用这一点来增强训练。

    Neural Ordinary Differential Equations (NeuralODEs) present an attractive way to extract dynamical laws from time series data, as they bridge neural networks with the differential equation-based modeling paradigm of the physical sciences. However, these models often display long training times and suboptimal results, especially for longer duration data. While a common strategy in the literature imposes strong constraints to the NeuralODE architecture to inherently promote stable model dynamics, such methods are ill-suited for dynamics discovery as the unknown governing equation is not guaranteed to satisfy the assumed constraints. In this paper, we develop a new training method for NeuralODEs, based on synchronization and homotopy optimization, that does not require changes to the model architecture. We show that synchronizing the model dynamics and the training data tames the originally irregular loss landscape, which homotopy optimization can then leverage to enhance training. Throug
    
[^198]: 基于课程的序列神经解码器用于极化编码族的研究

    CRISP: Curriculum based Sequential Neural Decoders for Polar Code Family. (arXiv:2210.00313v3 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2210.00313](http://arxiv.org/abs/2210.00313)

    该研究提出了一种新颖的基于课程的序列神经解码器CRISP，可以用于极化编码族，相比连续取消(SC)译码器，CRISP具有更高的准确性和可靠性，并可轻松地拓展至极化调整卷积（PAC）代码。

    

    极化编码是可靠通信的最新规范（5G）中广泛使用的最先进的编码，但在短块长度范围内设计既高效又可靠的极化译码器仍有空间。受数据驱动信道译码成功的启发，我们引入了一种新颖的基于课程的序列神经解码器，用于极化编码（CRISP）。我们设计了一种受信息理论启发的有原则的课程来训练CRISP，并表明它在Polar（32,16）和Polar（64,22）代码上优于连续取消(SC)译码器并达到接近最优的 可靠性能。所提议的课程选择对于实现CRISP的准确性增益至关重要，正如我们通过与其他课程的比较所示。值得注意的是，CRISP可以轻松地扩展到极化调整卷积（PAC）代码，其中现有SC解码器性能损失很大。

    Polar codes are widely used state-of-the-art codes for reliable communication that have recently been included in the 5th generation wireless standards (5G). However, there remains room for the design of polar decoders that are both efficient and reliable in the short blocklength regime. Motivated by recent successes of data-driven channel decoders, we introduce a novel $\textbf{C}$ur$\textbf{RI}$culum based $\textbf{S}$equential neural decoder for $\textbf{P}$olar codes (CRISP). We design a principled curriculum, guided by information-theoretic insights, to train CRISP and show that it outperforms the successive-cancellation (SC) decoder and attains near-optimal reliability performance on the Polar(32,16) and Polar(64,22) codes. The choice of the proposed curriculum is critical in achieving the accuracy gains of CRISP, as we show by comparing against other curricula. More notably, CRISP can be readily extended to Polarization-Adjusted-Convolutional (PAC) codes, where existing SC decod
    
[^199]: 面向生成式语音语言建模的增强不变离散表示方法

    Augmentation Invariant Discrete Representation for Generative Spoken Language Modeling. (arXiv:2209.15483v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.15483](http://arxiv.org/abs/2209.15483)

    本研究提出了一种增强不变的离散语音表示方法，以提高其在生成式语音语言建模中的鲁棒性。该方法利用了transformer-based模型，并通过一种非线性量化方法来学习增强不变表示。实验证明，该方法相对于现有最先进方法具有显著的鲁棒性改进，并在语音生成任务上表现出了竞争性的表现。

    

    生成式语音语言建模的研究关注于使用原始音频记录优化语言模型，而不使用任何文本监督。这种语言模型通常使用从自监督模型的内部表示量化得到的离散单位进行操作。本研究旨在改善离散输入表示对生成式语音语言建模的鲁棒性。我们定义了如何测量这些表示对各种不会改变语音信息（例如时间拉伸）的信号变化的鲁棒性，并通过实验证明了目前最先进的表示模型缺乏对此类变化的鲁棒性。为了克服这一问题，我们提出了一种有效且高效的方法来学习面向生成式语音语言建模的鲁棒离散语音表示。该方法利用基于transformer的模型的最新进展，针对数据增强的不变性，提出了一种非线性量化方法，以学习增强不变表示。该方法在鲁棒性上表现出了显著的改进，并在语音生成任务上取得了竞争性的表现。

    Generative Spoken Language Modeling research focuses on optimizing speech Language Models (LMs) using raw audio recordings without accessing any textual supervision. Such speech LMs usually operate over discrete units obtained from quantizing internal representations of self-supervised models. Although such units show impressive modeling results, their robustness capabilities have not been extensively investigated. This work focuses on improving the robustness of discrete input representations for generative spoken language modeling. First, we formally define how to measure the robustness of such representations to various signal variations that do not alter the spoken information (e.g., time-stretch). Next, we empirically demonstrate how current state-of-the-art representation models lack robustness to such variations. To overcome this, we propose an effective and efficient method to learn robust discrete speech representation for generative spoken language modeling. The proposed appr
    
[^200]: 非负张量的多体逼近

    Many-body Approximation for Non-negative Tensors. (arXiv:2209.15338v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.15338](http://arxiv.org/abs/2209.15338)

    提出了一种名为多体逼近的方法来分解非负张量，通过能量建模来避免全局优化和目标秩选择的困难，可通过考虑模式之间的交互进行全局优化; 在许多任务中都展示了其有效性。

    

    我们提出了一种替代方法来分解非负张量，称为多体逼近。传统的分解方法假设表示具有低秩性，导致全局优化和目标秩选择的困难。我们通过张量的能量建模避免了这些问题，其中张量和其模式分别对应于概率分布和随机变量。我们的模型可以通过考虑模式之间的交互来进行全局优化，可以比秩更直观地进行调整。此外，我们将模式之间的相互作用可视化为张量网络，揭示了多体逼近和低秩逼近之间的非平凡关系。我们在张量完成和逼近中展示了我们方法的有效性。

    We present an alternative approach to decompose non-negative tensors, called many-body approximation. Traditional decomposition methods assume low-rankness in the representation, resulting in difficulties in global optimization and target rank selection. We avoid these problems by energy-based modeling of tensors, where a tensor and its mode correspond to a probability distribution and a random variable, respectively. Our model can be globally optimized in terms of the KL divergence minimization by taking the interaction between variables, i.e. modes, into account that can be tuned more intuitively than ranks. Furthermore, we visualize interactions between modes as tensor networks and reveal a nontrivial relationship between many-body approximation and low-rank approximation. We demonstrate the effectiveness of our approach in tensor completion and approximation.
    
[^201]: 基于似然函数修正的半定规划方法用于异质数据聚类

    Likelihood Adjusted Semidefinite Programs for Clustering Heterogeneous Data. (arXiv:2209.15097v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.15097](http://arxiv.org/abs/2209.15097)

    本论文提出了一种基于似然函数修正的半定规划方法用于异质数据聚类。经过实验表明，本方法在处理聚类形状不同的数据异质性时表现优异。

    

    聚类是一种广泛使用的无监督学习工具。基于模型的聚类是一种灵活的框架，用来处理聚类具有不同形状的数据的异质性。对于混合分布的基于似然的推断通常涉及非凸和高维的目标函数，带来了复杂的计算和统计挑战。在本文中，我们将基于似然函数修正的半定规划（LA-SDP）方法应用于异质数据聚类。我们的方法通过一组新的矩阵不等式实现了似然函数调整的凸松弛。我们证明，在混合组分的一些温和的前提条件下，LA-SDP 可以一致而有效地计算出最大似然估计值。我们的实验表明，与现有的方法相比，尤其是当聚类显著异质时，我们的方法在合成数据和真实数据的实验中表现优异。

    Clustering is a widely deployed unsupervised learning tool. Model-based clustering is a flexible framework to tackle data heterogeneity when the clusters have different shapes. Likelihood-based inference for mixture distributions often involves non-convex and high-dimensional objective functions, imposing difficult computational and statistical challenges. The classic expectation-maximization (EM) algorithm is a computationally thrifty iterative method that maximizes a surrogate function minorizing the log-likelihood of observed data in each iteration, which however suffers from bad local maxima even in the special case of the standard Gaussian mixture model with common isotropic covariance matrices. On the other hand, recent studies reveal that the unique global solution of a semidefinite programming (SDP) relaxed $K$-means achieves the information-theoretically sharp threshold for perfectly recovering the cluster labels under the standard Gaussian mixture model. In this paper, we ext
    
[^202]: 基于视觉观察的物流场景下机器人顺序取货任务的无监督奖励塑造

    Unsupervised Reward Shaping for a Robotic Sequential Picking Task from Visual Observations in a Logistics Scenario. (arXiv:2209.12350v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2209.12350](http://arxiv.org/abs/2209.12350)

    本研究针对物流领域典型的卸货问题，提出了一种无监督奖励塑造算法，能够减轻对代理人的监督级别，并改善RL性能。

    

    我们针对物流领域典型的卸货问题，建立了一个顺序取货任务模型。在这类任务中，现代机器学习技术表现得比传统系统更好，因为它们更适应随机性，更能应对大量不确定性。尤其是，受监督和模仿学习在这方面取得了杰出的成果，但缺点是需要某种形式的监督，而这在所有设置中并不总是可获得的。另一方面，强化学习（RL）需要更轻微的监督，但仍然因其低效而难以应用。本文提出并在理论上证明了一种新颖的无监督奖励塑造算法，该算法利用专家的观察来减轻对代理人所需监督级别，并改善我们任务的RL性能。

    We focus on an unloading problem, typical of the logistics sector, modeled as a sequential pick-and-place task. In this type of task, modern machine learning techniques have shown to work better than classic systems since they are more adaptable to stochasticity and better able to cope with large uncertainties. More specifically, supervised and imitation learning have achieved outstanding results in this regard, with the shortcoming of requiring some form of supervision which is not always obtainable for all settings. On the other hand, reinforcement learning (RL) requires much milder form of supervision but still remains impracticable due to its inefficiency. In this paper, we propose and theoretically motivate a novel Unsupervised Reward Shaping algorithm from expert's observations which relaxes the level of supervision required by the agent and works on improving RL performance in our task.
    
[^203]: 非光滑非凸非凹极小极大优化的全局收敛率分析

    Global Convergence Rate Analysis of Nonsmooth Nonconvex-Nonconcave Minimax Optimization. (arXiv:2209.10825v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2209.10825](http://arxiv.org/abs/2209.10825)

    本论文提出了一种名为smoothed PLDA的算法来有效处理广泛的结构化非光滑非凸非凹极小极大问题，并证明了其具有全局收敛性，复杂度为O(epsilon^(-2/3))。

    

    在过去的十年中，非凸非凹极小极大优化引起了广泛关注。然而，大多数现有的工作集中在梯度下降-上升（GDA）算法的各种变体上，这些算法仅适用于平滑的非凸凹场景。为了解决这个局限性，我们提出了一种新算法，名为平滑的近端线性下降上升（smoothed PLDA），可以有效地处理广泛的结构化非光滑非凸非凹极小极大问题。具体而言，我们考虑原始函数具有非光滑复合结构，对偶函数具有Kurdyka-L{o}jasiewicz（K\L{}）性质的情况。我们引入了一种新的收敛分析框架来分析smoothed PLDA算法，其中关键组件是我们最新开发的非光滑原始误差界和对偶误差界属性。利用这个框架，我们证明了smoothed PLDA可以在具有非光滑复合原始函数和KL对偶函数的广泛极小极大问题中找到$\varepsilon$-game-stationary点和$\varepsilon$-最优化稳定点，其复杂度为$\mathcal{O}(\varepsilon^{-2/3})$。

    Nonconvex-nonconcave minimax optimization has gained widespread interest over the last decade. However, most existing work focuses on variants of gradient descent-ascent (GDA) algorithms, which are only applicable in smooth nonconvex-concave settings. To address this limitation, we propose a novel algorithm named smoothed proximal linear descent-ascent (smoothed PLDA), which can effectively handle a broad range of structured nonsmooth nonconvex-nonconcave minimax problems. Specifically, we consider the setting where the primal function has a nonsmooth composite structure and the dual function possesses the Kurdyka-\L{}ojasiewicz (K\L{}) property with exponent $\theta \in [0,1)$. We introduce a novel convergence analysis framework for smoothed PLDA, the key components of which are our newly developed nonsmooth primal error bound and dual error bound properties. Using this framework, we show that smoothed PLDA can find both $\epsilon$-game-stationary points and $\epsilon$-optimization-st
    
[^204]: MGG: 多GPU平台上通过精细的内核通信计算流水线加速图神经网络

    MGG: Accelerating Graph Neural Networks with Fine-grained intra-kernel Communication-Computation Pipelining on Multi-GPU Platforms. (arXiv:2209.06800v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2209.06800](http://arxiv.org/abs/2209.06800)

    MGG是一种软件流水线设计，可在多GPU平台上加速GNNs，通过采用GNN特殊的流水线构建和GPU感知的流水线映射，实现精细计算通信重叠以提高性能。

    

    图神经网络（GNNs）输入图的大小越来越大，需要使用多GPU平台。然而，现有的多GPU GNN系统仅基于传统做法缩放稠密DNN，优化计算和通信操作，对于不规则稀疏的GNN工作负载，缺失同时调度优化计算和通信操作以提高性能的机会。因此，我们提出了MGG，这是一种新的系统设计，可以在多GPU平台上加速全图GNN。MGG的核心是其新颖的动态软件流水线，以促进GPU内部精细的计算通信重叠。具体而言，MGG引入了适用于GNN的流水线构建和GPU感知的流水线映射，以促进工作负载平衡和操作重叠。MGG还结合了智能的运行时设计和分析建模和优化启发式方法，以动态改进性能。

    The increasing size of input graphs for graph neural networks (GNNs) highlights the demand for using multi-GPU platforms. However, existing multi-GPU GNN systems optimize the computation and communication individually based on the conventional practice of scaling dense DNNs. For irregularly sparse and fine-grained GNN workloads, such solutions miss the opportunity to jointly schedule/optimize the computation and communication operations for high-performance delivery. To this end, we propose MGG, a novel system design to accelerate full-graph GNNs on multi-GPU platforms. The core of MGG is its novel dynamic software pipeline to facilitate fine-grained computation-communication overlapping within a GPU kernel. Specifically, MGG introduces GNN-tailored pipeline construction and GPU-aware pipeline mapping to facilitate workload balancing and operation overlapping. MGG also incorporates an intelligent runtime design with analytical modeling and optimization heuristics to dynamically improve
    
[^205]: AnaMeta：多维数据分析任务共享的表格元数据知识的理解数据集

    AnaMeta: A Table Understanding Dataset of Field Metadata Knowledge Shared by Multi-dimensional Data Analysis Tasks. (arXiv:2209.00946v2 [cs.DB] UPDATED)

    [http://arxiv.org/abs/2209.00946](http://arxiv.org/abs/2209.00946)

    AnaMeta是一个包含467k张表格的数据集，它提供了常用字段元数据的四种衍生监督标签，同时推出了一种新的多编码器框架（KDF）提高了表格模型的元数据理解能力，并且提出了四个接口将元数据纳入下游分析任务中。

    

    表格数据分析在各个领域每天都在进行，它需要准确地理解字段语义才能正确地操作表格字段并在日常分析中找到共同模式。本文介绍了AnaMeta数据集，其中包含467k张表格以及常用字段元数据的四种衍生监督标签：衡量/维度二分法、常见字段角色、语义字段类型和默认聚合函数。我们评估了一系列模型来推断元数据作为基准。我们还提出了一种名为KDF的多编码器框架，通过结合分布和知识信息来提高表格模型的元数据理解能力。此外，我们提出了四个接口，将字段元数据纳入下游分析任务中。

    Tabular data analysis is performed every day across various domains. It requires an accurate understanding of field semantics to correctly operate on table fields and find common patterns in daily analysis. In this paper, we introduce the AnaMeta dataset, a collection of 467k tables with derived supervision labels for four types of commonly used field metadata: measure/dimension dichotomy, common field roles, semantic field type, and default aggregation function. We evaluate a wide range of models for inferring metadata as the benchmark. We also propose a multi-encoder framework, called KDF, which improves the metadata understanding capability of tabular models by incorporating distribution and knowledge information. Furthermore, we propose four interfaces for incorporating field metadata into downstream analysis tasks.
    
[^206]: 融合背景的混合模型下的排名和校准联合优化研究

    Joint Optimization of Ranking and Calibration with Contextualized Hybrid Model. (arXiv:2208.06164v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2208.06164](http://arxiv.org/abs/2208.06164)

    本论文提出了一个联合优化排名和校准能力的方法JRC，通过对比输出logit值来提高排名能力和校准能力。

    

    尽管排名优化技术得到了发展，点对点损失仍然是点击率预测的主要方法。这可以归因于点对点损失的校准能力，因为预测可以被视为点击概率。实际上，CTR预测模型也通常通过排名能力进行评估。为了优化排名能力，可以采用排名损失（例如成对或列表损失），因为它们通常比点对点损失实现更好的排名。之前的研究尝试直接将两种损失组合起来以获得两种损失的益处，并观察到了改进的性能。然而，之前的研究打破了输出logit作为点击率的含义，这可能会导致次优解。为了解决这个问题，我们提出了一种方法，可以联合优化排名和校准能力（简称JRC）。JRC通过对输出logit值进行对比来提高排名能力。

    Despite the development of ranking optimization techniques, pointwise loss remains the dominating approach for click-through rate prediction. It can be attributed to the calibration ability of the pointwise loss since the prediction can be viewed as the click probability. In practice, a CTR prediction model is also commonly assessed with the ranking ability. To optimize the ranking ability, ranking loss (e.g., pairwise or listwise loss) can be adopted as they usually achieve better rankings than pointwise loss. Previous studies have experimented with a direct combination of the two losses to obtain the benefit from both losses and observed an improved performance. However, previous studies break the meaning of output logit as the click-through rate, which may lead to sub-optimal solutions. To address this issue, we propose an approach that can Jointly optimize the Ranking and Calibration abilities (JRC for short). JRC improves the ranking ability by contrasting the logit value for the 
    
[^207]: 模型训练中鲁棒性高的高维线性学习方法

    Robust Methods for High-Dimensional Linear Learning. (arXiv:2208.05447v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2208.05447](http://arxiv.org/abs/2208.05447)

    该论文提出了一种在高维批量训练中具有高度鲁棒性和计算效率的线性学习方法，在多个应用程序中均能达到接近最优的估计速率，并提供了一个开源的Python库进行实现。

    

    我们提出了一种高维批处理中具有统计鲁棒性和计算有效性的线性学习方法，其中特征数d可能超过样本数n。我们在通用学习设置中采用了两种算法，取决于所考虑的损失函数是否是梯度Lipschitz的。然后，我们将我们的框架实例化到几个应用程序上，包括香草稀疏，组稀疏和低秩矩阵恢复。这导致了每个应用程序的高效和鲁棒的学习算法，在重尾分布和异常值的情况下，达到接近最优的估计速率。对于香草$s$-稀疏，我们能够在重尾和$\eta$-污染下达到$s\log(d)/n$的速率，计算成本与非鲁棒模拟相当。我们提供了一个开源的$\mathtt{Python}$库$\mathtt{linlearn}$来实现我们的算法，通过这个库进行数值实验，证明了我们方法的有效性和可扩展性。

    We propose statistically robust and computationally efficient linear learning methods in the high-dimensional batch setting, where the number of features $d$ may exceed the sample size $n$. We employ, in a generic learning setting, two algorithms depending on whether the considered loss function is gradient-Lipschitz or not. Then, we instantiate our framework on several applications including vanilla sparse, group-sparse and low-rank matrix recovery. This leads, for each application, to efficient and robust learning algorithms, that reach near-optimal estimation rates under heavy-tailed distributions and the presence of outliers. For vanilla $s$-sparsity, we are able to reach the $s\log (d)/n$ rate under heavy-tails and $\eta$-corruption, at a computational cost comparable to that of non-robust analogs. We provide an efficient implementation of our algorithms in an open-source $\mathtt{Python}$ library called $\mathtt{linlearn}$, by means of which we carry out numerical experiments whi
    
[^208]: 基于流模型的去噪在流形假设下的收敛性分析

    Convergence of denoising diffusion models under the manifold hypothesis. (arXiv:2208.05314v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2208.05314](http://arxiv.org/abs/2208.05314)

    本文提供了基于流模型的去噪在流形假设下的收敛性分析，首次拓展到了目标分布受流形约束或通过经验分布给出的情况。

    

    去噪流模型是一类生成模型，在图像和音频合成方面表现出最先进的性能。这样的模型近似于从目标分布到参考密度（通常为高斯分布）的正向噪声过程的时间反演。尽管它们具有强大的实证结果，但对这些模型的理论分析仍然有限。特别地，所有当前的方法都关键地假设目标分布相对于勒贝格测度存在密度。这不涵盖目标分布受低维流形约束或通过某些经验分布给出的情况。我们提供了第一个针对流模型在这种更加普遍的情况下的收敛性结果并提供一阶Wasserstein距离量化界限。

    Denoising diffusion models are a recent class of generative models exhibiting state-of-the-art performance in image and audio synthesis. Such models approximate the time-reversal of a forward noising process from a target distribution to a reference density, which is usually Gaussian. Despite their strong empirical results, the theoretical analysis of such models remains limited. In particular, all current approaches crucially assume that the target density admits a density w.r.t. the Lebesgue measure. This does not cover settings where the target distribution is supported on a lower-dimensional manifold or is given by some empirical distribution. In this paper, we bridge this gap by providing the first convergence results for diffusion models in this more general setting. In particular, we provide quantitative bounds on the Wasserstein distance of order one between the target data distribution and the generative distribution of the diffusion model.
    
[^209]: ME-GAN：基于心脏疾病条件学习全景心电图表示的多视角ECG合成

    ME-GAN: Learning Panoptic Electrocardio Representations for Multi-view ECG Synthesis Conditioned on Heart Diseases. (arXiv:2207.10670v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.10670](http://arxiv.org/abs/2207.10670)

    本文提出一种基于心脏疾病条件学习全景心电图表示的多视角ECG合成方法ME-GAN，利用新的混合归一化方法将疾病信息注入到合适位置，具有较好的生成效果和疾病识别能力。

    

    心电图( ECG)是一种广泛使用的非侵入性诊断工具，用于检测心脏疾病。许多研究已经设计了ECG分析模型(如分类器)来辅助诊断。作为一个上游任务，研究人员建立生成模型来合成ECG数据，这有利于提供训练样本、隐私保护和注释减少。然而，以前的ECG生成方法往往既没有合成多视图数据，也没有处理心脏疾病情况。在本文中，我们提出了一种新的以疾病为导向的多视角ECG合成生成对抗网络ME-GAN，它获得了基于心脏疾病条件的全景心电图表示，并将这些表示投影到多个标准视图上以产生ECG信号。由于心脏疾病的ECG表现通常局部化在特定的波形中，我们提出了一种新的"混合归一化"方法，以将疾病信息精确地注入到合适的位置。

    Electrocardiogram (ECG) is a widely used non-invasive diagnostic tool for heart diseases. Many studies have devised ECG analysis models (e.g., classifiers) to assist diagnosis. As an upstream task, researches have built generative models to synthesize ECG data, which are beneficial to providing training samples, privacy protection, and annotation reduction. However, previous generative methods for ECG often neither synthesized multi-view data, nor dealt with heart disease conditions. In this paper, we propose a novel disease-aware generative adversarial network for multi-view ECG synthesis called ME-GAN, which attains panoptic electrocardio representations conditioned on heart diseases and projects the representations onto multiple standard views to yield ECG signals. Since ECG manifestations of heart diseases are often localized in specific waveforms, we propose a new "mixup normalization" to inject disease information precisely into suitable locations. In addition, we propose a view 
    
[^210]: 基于谱图层级转换器的呼吸声COVID-19检测

    COVID-19 Detection from Respiratory Sounds with Hierarchical Spectrogram Transformers. (arXiv:2207.09529v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2207.09529](http://arxiv.org/abs/2207.09529)

    本研究提出了一种新的深度学习方法，使用谱图层级转换器对咳嗽或呼吸声的音频记录进行COVID-19检测。

    

    流行性空气传播疾病，如COVID-19的监测通常涉及呼吸评估。虽然听诊是初步筛查疾病症状的主流方法，但需要专门的医院访问，限制了其效用。基于便携式设备上呼吸声记录的远程监测是一种有前途的替代方案，它可以协助早期评估首要影响下呼吸道的COVID-19。在这项研究中，我们介绍了一种新的深度学习方法，可以通过咳嗽或呼吸声的音频记录区分COVID-19患者和健康对照组。所提出的方法利用呼吸声的谱图表示中的一种新型谱图层级转换器（HST）。 HST在谱图中的局部窗口上具有自我关注机制，并且窗口大小随着模型阶段的逐渐增长而逐渐增加，以捕获本地到全局上下文。 HST与最先进的卷积神经网络模型进行了比较。

    Monitoring of prevalent airborne diseases such as COVID-19 characteristically involves respiratory assessments. While auscultation is a mainstream method for preliminary screening of disease symptoms, its utility is hampered by the need for dedicated hospital visits. Remote monitoring based on recordings of respiratory sounds on portable devices is a promising alternative, which can assist in early assessment of COVID-19 that primarily affects the lower respiratory tract. In this study, we introduce a novel deep learning approach to distinguish patients with COVID-19 from healthy controls given audio recordings of cough or breathing sounds. The proposed approach leverages a novel hierarchical spectrogram transformer (HST) on spectrogram representations of respiratory sounds. HST embodies self-attention mechanisms over local windows in spectrograms, and window size is progressively grown over model stages to capture local to global context. HST is compared against state-of-the-art conve
    
[^211]: 使用深度学习与符号回归发现参数方程

    Deep Learning and Symbolic Regression for Discovering Parametric Equations. (arXiv:2207.00529v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.00529](http://arxiv.org/abs/2207.00529)

    该论文提出了一种神经网络体系结构，将符号回归扩展到参数系统中，通过分析多种不同类型的数学表达式，ODE 和 PDE，证明其在训练域之外的外推能力。这种体系结构还与其他深度学习体系结构集成，实现了对高维数据进行端对端训练，并可用于分析图像等数据。

    

    符号回归是一种机器学习技术，它可以学习数据的控制公式，从而有可能改变科学发现。然而，符号回归在分析复杂度和维度方面仍然存在局限性。另一方面，深度学习已经通过其分析极其复杂和高维数据集的能力改变了机器学习。我们提出了一种神经网络体系结构，将符号回归扩展到参数系统，其中一些系数可能会变化，但潜在的控制方程的结构保持不变。我们在各种解析表达式，ODE 和 PDE 上展示了我们的方法，其中系数变化不同，并展示了它在训练域之外的外推能力。基于神经网络的体系结构还可以与其他深度学习体系结构集成，从而能够在进行端对端训练的同时分析高维数据。为此，我们将我们的体系结构与深度卷积神经网络集成，以分析来自流体动力学模拟的一组图像，并发现相应的控制方程。

    Symbolic regression is a machine learning technique that can learn the governing formulas of data and thus has the potential to transform scientific discovery. However, symbolic regression is still limited in the complexity and dimensionality of the systems that it can analyze. Deep learning on the other hand has transformed machine learning in its ability to analyze extremely complex and high-dimensional datasets. We propose a neural network architecture to extend symbolic regression to parametric systems where some coefficient may vary but the structure of the underlying governing equation remains constant. We demonstrate our method on various analytic expressions, ODEs, and PDEs with varying coefficients and show that it extrapolates well outside of the training domain. The neural network-based architecture can also integrate with other deep learning architectures so that it can analyze high-dimensional data while being trained end-to-end. To this end we integrate our architecture w
    
[^212]: 可掩蔽世界模型用于视觉控制

    Masked World Models for Visual Control. (arXiv:2206.14244v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2206.14244](http://arxiv.org/abs/2206.14244)

    本研究提出了一种视觉模型驱动的RL框架，将视觉表示学习和动力学学习分离，使用自编码器和潜在动力学模型来准确建模机器人控制策略。

    

    视觉模型驱动的强化学习（RL）有潜力使机器人能够从视觉观测中实现样本有效的学习。但目前的方法通常训练一个端到端的单一模型来学习视觉表示和动力学，使得准确建模机器人与小物体之间的相互作用变得困难。本研究介绍了一种视觉模型驱动的RL框架，将视觉表示学习和动力学学习分离。具体来说，我们使用卷积层和视觉变换器（ViT）来训练自编码器，以在给定掩蔽卷积特征的情况下重构像素，并学习一个操作自编码器表示的潜在动力学模型。此外，为了编码任务相关信息，我们引入了一个辅助奖励预测目标来训练自编码器。我们使用从环境交互中收集的在线样本不断更新自编码器和动力学模型。我们证明了我们的分离策略可以有效地学习机器人控制策略。

    Visual model-based reinforcement learning (RL) has the potential to enable sample-efficient robot learning from visual observations. Yet the current approaches typically train a single model end-to-end for learning both visual representations and dynamics, making it difficult to accurately model the interaction between robots and small objects. In this work, we introduce a visual model-based RL framework that decouples visual representation learning and dynamics learning. Specifically, we train an autoencoder with convolutional layers and vision transformers (ViT) to reconstruct pixels given masked convolutional features, and learn a latent dynamics model that operates on the representations from the autoencoder. Moreover, to encode task-relevant information, we introduce an auxiliary reward prediction objective for the autoencoder. We continually update both autoencoder and dynamics model using online samples collected from environment interaction. We demonstrate that our decoupling a
    
[^213]: 带约束条件的差分隐私联合组合赌博机研究

    Differentially Private Federated Combinatorial Bandits with Constraints. (arXiv:2206.13192v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.13192](http://arxiv.org/abs/2206.13192)

    本文研究了差分隐私联合组合赌博机问题，探讨了代理在共同学习时如何保持数据的隐私，并提出了在后悔和隐私之间实现平衡的重要性。

    

    在在线学习模式中，合作学习范式（即联邦学习）快速增长。与大多数联邦学习情景不同的是，有很多情况下代理是竞争的。每个代理都想从其他人那里学习，但它分享给其他人学习的信息有可能是敏感的，因此它需要隐私。本文研究了一组代理同时解决类似的组合赌博机问题，同时保持质量约束。这些代理是否可以通过采用差分隐私来保持机密性，集体学习？我们观察到通信可以降低后悔。但是，保护敏感信息的差分隐私技术使数据变得很嘈杂，可能会恶化而不是有帮助地提高后悔。因此，我们指出决定何时通信以及学习哪些共享数据来在后悔和隐私之间实现功能平衡至关重要。

    There is a rapid increase in the cooperative learning paradigm in online learning settings, i.e., federated learning (FL). Unlike most FL settings, there are many situations where the agents are competitive. Each agent would like to learn from others, but the part of the information it shares for others to learn from could be sensitive; thus, it desires its privacy. This work investigates a group of agents working concurrently to solve similar combinatorial bandit problems while maintaining quality constraints. Can these agents collectively learn while keeping their sensitive information confidential by employing differential privacy? We observe that communicating can reduce the regret. However, differential privacy techniques for protecting sensitive information makes the data noisy and may deteriorate than help to improve regret. Hence, we note that it is essential to decide when to communicate and what shared data to learn to strike a functional balance between regret and privacy. F
    
[^214]: 针对部分客户端参与的联邦学习的锚定抽样方法

    Anchor Sampling for Federated Learning with Partial Client Participation. (arXiv:2206.05891v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.05891](http://arxiv.org/abs/2206.05891)

    提出一种针对部分客户端参与的联邦学习框架 FedAMD，其中核心思想是锚定抽样，将参与者分为锚定组和矿工组，以解决数据异构性。

    

    相较于全客户端参与，部分客户端参与是联邦学习中更常见的场景，但是会加重一些挑战，例如数据异构性。在部分客户端参与的情况下缺少非活动客户端的更新，可能会导致模型聚合偏离基于全客户端参与的聚合。通常提出采用在个体客户端上使用大批量来进行训练以解决数据异构性，但其在部分客户端参与的情况下的有效性不明确。在考虑这些挑战的基础上，我们提出了一种新的针对部分客户端参与的联邦学习框架，称为FedAMD，其核心思想是锚定抽样，将部分参与者分为锚定组和矿工组。

    Compared with full client participation, partial client participation is a more practical scenario in federated learning, but it may amplify some challenges in federated learning, such as data heterogeneity. The lack of inactive clients' updates in partial client participation makes it more likely for the model aggregation to deviate from the aggregation based on full client participation. Training with large batches on individual clients is proposed to address data heterogeneity in general, but their effectiveness under partial client participation is not clear. Motivated by these challenges, we propose to develop a novel federated learning framework, referred to as FedAMD, for partial client participation. The core idea is anchor sampling, which separates partial participants into anchor and miner groups. Each client in the anchor group aims at the local bullseye with the gradient computation using a large batch. Guided by the bullseyes, clients in the miner group steer multiple near
    
[^215]: 从单一正标签中挖掘多标签样本

    Mining Multi-Label Samples from Single Positive Labels. (arXiv:2206.05764v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.05764](http://arxiv.org/abs/2206.05764)

    本文提出了一种基于单一正标签的采样方法S2M，以实现对多标签样本的生成，从而避免了多标签数据集制作时高昂的注释成本。

    

    条件生成对抗网络（cGAN）在类别有条件的生成任务中已经证明具有出色的结果。为了同时控制多个条件，cGAN需要多标签训练数据集，其中可以为每个数据实例分配多个标签。然而，巨大的注释成本限制了多标签数据集在现实场景中的可访问性。因此，在本研究中，我们探索了实际设置的称为单正标签设置，其中每个数据实例仅由一个正标签注释，没有明确的负标签。为了在单正标签设置中生成多标签数据，我们提出了一种基于马尔可夫链蒙特卡罗方法的新型采样方法，称为单到多标签（S2M）采样。作为广泛适用的“附加”方法，我们提出的S2M采样方法使现有的无条件和条件GAN能够以最小的注释成本绘制高质量的多标签数据。在实际图像和文本数据集上进行的大量实验证明了我们提出的方法在各种情况下的有效性。

    Conditional generative adversarial networks (cGANs) have shown superior results in class-conditional generation tasks. To simultaneously control multiple conditions, cGANs require multi-label training datasets, where multiple labels can be assigned to each data instance. Nevertheless, the tremendous annotation cost limits the accessibility of multi-label datasets in real-world scenarios. Therefore, in this study we explore the practical setting called the single positive setting, where each data instance is annotated by only one positive label with no explicit negative labels. To generate multi-label data in the single positive setting, we propose a novel sampling approach called single-to-multi-label (S2M) sampling, based on the Markov chain Monte Carlo method. As a widely applicable "add-on" method, our proposed S2M sampling method enables existing unconditional and conditional GANs to draw high-quality multi-label data with a minimal annotation cost. Extensive experiments on real im
    
[^216]: 利用中心极限定理结构的随机梯度采样方法：改进的分析和更快的算法

    Utilising the CLT Structure in Stochastic Gradient based Sampling : Improved Analysis and Faster Algorithms. (arXiv:2206.03792v3 [math.PR] UPDATED)

    [http://arxiv.org/abs/2206.03792](http://arxiv.org/abs/2206.03792)

    本文研究了基于随机近似的采样算法，利用中心极限定理结构吸收扩散过程中的随机逼近误差并获得了改进的收敛保证。此外，对于SGLD和RBM，我们分别证明了不同的假设条件下较优的收敛率和参数范围。

    

    本文研究了基于随机近似的采样算法，如随机梯度 langevin 动力学（SGLD）和随机批处理方法（RBM）用于相互作用粒子动力学（IPD）。我们观察到，由于中心极限定理（CLT），随机逼近引入的噪声几乎是高斯分布，而驱动布朗运动则是确切的高斯分布。我们利用这种结构来吸收扩散过程中的随机逼近误差，并获得了这些算法的改进收敛保证。对于 SGLD，我们证明了在不需要统一温暖启动的情况下KL散度的第一个稳定收敛率，假设目标密度满足一个对数 Sobolev 不等式。我们的结果意味着在显著较轻的假设条件下，相对于先前的工作，我们具有更优异的一阶 oracle 复杂性。我们还证明了 SGLD 的第一个保证，对于更弱的条件，如 H\''{o}lder 平滑性和 Poincare不等式，从而填补了现有技术和实际应用之间的差距。对于 RBM，我们在 IPD 的弱混合条件下获得了第一次收敛分析和最佳参数范围，这在统计物理和学习理论中具有几个含义。

    We consider stochastic approximations of sampling algorithms, such as Stochastic Gradient Langevin Dynamics (SGLD) and the Random Batch Method (RBM) for Interacting Particle Dynamcs (IPD). We observe that the noise introduced by the stochastic approximation is nearly Gaussian due to the Central Limit Theorem (CLT) while the driving Brownian motion is exactly Gaussian. We harness this structure to absorb the stochastic approximation error inside the diffusion process, and obtain improved convergence guarantees for these algorithms. For SGLD, we prove the first stable convergence rate in KL divergence without requiring uniform warm start, assuming the target density satisfies a Log-Sobolev Inequality. Our result implies superior first-order oracle complexity compared to prior works, under significantly milder assumptions. We also prove the first guarantees for SGLD under even weaker conditions such as H\"{o}lder smoothness and Poincare Inequality, thus bridging the gap between the state-
    
[^217]: 由 SGD 训练的深度神经网络的泛化误差界

    Generalization Error Bounds for Deep Neural Networks Trained by SGD. (arXiv:2206.03299v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.03299](http://arxiv.org/abs/2206.03299)

    通过对于适当参数规范的动态控制结合基于参数规范的 Rademacher 复杂度估计导出了深度神经网络的泛化误差界，适用于包括 MLP 和 CNN 在内的广泛网络架构，结果表明这个方法能够适应优化器和网络超参数的变化。

    

    本文通过将适当参数规范的动态控制和基于参数规范的 Rademacher 复杂度估计相结合，导出了由随机梯度下降（SGD）训练的深度神经网络的泛化误差界。这些界明确取决于沿训练轨迹的损失，并适用于包括多层感知机（MLP）和卷积神经网络（CNN）在内的广泛网络架构。与其他算法依赖的泛化估计（如基于全局稳定性的界）相比，我们的界不需要非凸损失函数的 $L$-平滑性，并且直接适用于 SGD，而不是随机 Langevin 梯度下降（SGLD）。数值结果表明，我们的界是非虚假和强健的，能够适应优化器和网络超参数的变化。

    Generalization error bounds for deep neural networks trained by stochastic gradient descent (SGD) are derived by combining a dynamical control of an appropriate parameter norm and the Rademacher complexity estimate based on parameter norms. The bounds explicitly depend on the loss along the training trajectory, and work for a wide range of network architectures including multilayer perceptron (MLP) and convolutional neural networks (CNN). Compared with other algorithm-depending generalization estimates such as uniform stability-based bounds, our bounds do not require $L$-smoothness of the nonconvex loss function, and apply directly to SGD instead of Stochastic Langevin gradient descent (SGLD). Numerical results show that our bounds are non-vacuous and robust with the change of optimizer and network hyperparameters.
    
[^218]: 对轻度参数化神经网络训练的早期收敛和全局收敛的研究

    Early Stage Convergence and Global Convergence of Training Mildly Parameterized Neural Networks. (arXiv:2206.02139v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.02139](http://arxiv.org/abs/2206.02139)

    本文证明了对于大部分的轻度参数化神经网络和损失函数，包括平方损失和交叉熵损失，在早期阶段会有很快的下降。对于指数型损失函数，在一些对于数据的假设下，本文证明了GD会全局收敛。本文的研究基于对神经元激活模式的微观分析，得出了“神经元划分”的结果，有助于理解神经网络训练动态的行为，并可能具有独立的兴趣。

    

    本文研究了在随机初始化情况下训练轻度参数化神经网络时，GD和SGD的收敛性。对于包括最常用的平方损失和交叉熵损失在内的广泛模型和损失函数，我们证明了“早期收敛”结果。我们表明在训练的早期阶段损失函数会有较大程度的下降，这种下降是很快的。此外，对于指数型损失函数以及对于训练数据的一些假设，我们证明了GD的全局收敛。与依靠极端过度参数化不同的是，我们的研究是基于对神经元激活模式的微观分析，这有助于我们推导出更强大的梯度下界。我们称这种激活模式的结果为“神经元划分”，这有助于理解神经网络训练动态的行为，并可能具有独立的兴趣。

    The convergence of GD and SGD when training mildly parameterized neural networks starting from random initialization is studied. For a broad range of models and loss functions, including the most commonly used square loss and cross entropy loss, we prove an ``early stage convergence'' result. We show that the loss is decreased by a significant amount in the early stage of the training, and this decrease is fast. Furthurmore, for exponential type loss functions, and under some assumptions on the training data, we show global convergence of GD. Instead of relying on extreme over-parameterization, our study is based on a microscopic analysis of the activation patterns for the neurons, which helps us derive more powerful lower bounds for the gradient. The results on activation patterns, which we call ``neuron partition'', help build intuitions for understanding the behavior of neural networks' training dynamics, and may be of independent interest.
    
[^219]: MetaLR: 医学图像迁移学习中学习速率的元调整方法

    MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging. (arXiv:2206.01408v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.01408](http://arxiv.org/abs/2206.01408)

    提出了一种基于元学习的学习率调整器MetaLR，可以使不同层次的层次根据它们的可转移性自动协同适应下游任务，在多个医学图像数据集上实验表明MetaLR在微调方面的效果优于多种领先的微调方法。

    

    在医学图像分析中，迁移学习是深度神经网络（DNN）能够对有限的医学数据进行泛化的强大方法。先前的研究集中于开发针对肺部超声，胸部X射线和肝脏CT等领域的预训练算法，以弥合领域差距。然而，我们发现模型微调在适应目标任务中也扮演着至关重要的角色。常见的微调方法是手动选择可转移层（例如最后几层）进行更新，这是非常耗时的。在这项工作中，我们提出了一种基于元学习的学习速率调整器，名为MetaLR，使不同层次的层次在跨域任务中根据它们在领域之间的可转移性自动协同适应下游任务。MetaLR在线学习不同层次的适当学习率，防止高度可转移层忘记其医学表示能力，并促使其他可转移层主动适应新领域。在各种医学图像数据集上的大量实验表明，我们提出的MetaLR可以显着改善几种领先的微调方法的性能。

    In medical image analysis, transfer learning is a powerful method for deep neural networks (DNNs) to generalize well on limited medical data. Prior efforts have focused on developing pre-training algorithms on domains such as lung ultrasound, chest X-ray, and liver CT to bridge domain gaps. However, we find that model fine-tuning also plays a crucial role in adapting medical knowledge to target tasks. The common fine-tuning method is manually picking transferable layers (e.g., the last few layers) to update, which is labor-expensive. In this work, we propose a meta-learning-based LR tuner, named MetaLR, to make different layers automatically co-adapt to downstream tasks based on their transferabilities across domains. MetaLR learns appropriate LRs for different layers in an online manner, preventing highly transferable layers from forgetting their medical representation abilities and driving less transferable layers to adapt actively to new domains. Extensive experiments on various med
    
[^220]: 使用多环境方法检测观测数据中的隐式混淆

    Detecting hidden confounding in observational data using multiple environments. (arXiv:2205.13935v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2205.13935](http://arxiv.org/abs/2205.13935)

    使用独立数据生成过程下的多环境方法，可以检测观测数据中的未观察到的混淆因素，并提出了测试独立性的程序。

    

    在因果推断中，常见的假设是没有隐式混淆。然而，在单个数据集中不能确定这个假设通常是不可能的。在独立的数据生成过程下，我们展示了一种方法来在多个来自不同环境的观测数据集中检测未观察到的混淆因素。我们提出了一种测试可验证的条件独立性的理论，这种独立性仅当存在混淆因素时才不存在，并检查了违反其假设的情况：退化和依赖机制以及忠实度违反。此外，我们提出了一种程序来测试这些独立性，并使用基于真实世界数据的半合成数据和模拟研究研究其经验有限样本行为。在大多数情况下，提出的程序能够正确预测存在隐式混淆，特别是当混淆偏差很大时。

    A common assumption in causal inference from observational data is that there is no hidden confounding. Yet it is, in general, impossible to verify this assumption from a single dataset. Under the assumption of independent causal mechanisms underlying the data-generating process, we demonstrate a way to detect unobserved confounders when having multiple observational datasets coming from different environments. We present a theory for testable conditional independencies that are only absent when there is hidden confounding and examine cases where we violate its assumptions: degenerate & dependent mechanisms, and faithfulness violations. Additionally, we propose a procedure to test these independencies and study its empirical finite-sample behavior using simulation studies and semi-synthetic data based on a real-world dataset. In most cases, the proposed procedure correctly predicts the presence of hidden confounding, particularly when the confounding bias is large.
    
[^221]: 带有偏好引导的个性化算法干预研究

    Personalized Algorithmic Recourse with Preference Elicitation. (arXiv:2205.13743v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.13743](http://arxiv.org/abs/2205.13743)

    研究提出了PEAR方法，这是一个首个能够针对最终用户需求提供个性化算法补救成本的人机交互方法。该方法利用贝叶斯偏好引导的见解，通过最大化原则性信息增益度量来计算目标用户选择的预期效用，然后将偏好引导整合到强化学习框架中。该方法显著提高了算法干预的经济实用性和用户友好性。

    

    算法干预（AR）的问题是计算用户执行一系列操作以颠覆不良机器决策的过程。该过程的操作序列不应该对用户的实施提出过高的要求。然而，大多数AR方法都假设所有用户的操作成本相同，因此可能会向某些用户推荐昂贵的补救计划。为了解决这个问题，我们提出了PEAR，这是一种首个可提供个性化算法补救成本的人机交互方法，以满足任何最终用户的需求。PEAR利用贝叶斯偏好引导的见解，通过向目标用户发出选择集查询来迭代地改善对操作成本的估计值。这些查询的计算是通过最大化选择的预期效用来计算的，这是一种能够考虑成本估计和用户响应不确定性的原则性信息增益度量。PEAR将偏好引导整合到强化学习框架中，同时考虑用户实现AR任务所需达成目标的偏好，以及执行每个操作所涉及的成本。我们通过引入更具挑战性的AR任务来评估PEAR，并显示其比现有的方法找到了更为经济实用且用户友好的补救计划。

    Algorithmic Recourse (AR) is the problem of computing a sequence of actions that -- once performed by a user -- overturns an undesirable machine decision. It is paramount that the sequence of actions does not require too much effort for users to implement. Yet, most approaches to AR assume that actions cost the same for all users, and thus may recommend unfairly expensive recourse plans to certain users. Prompted by this observation, we introduce PEAR, the first human-in-the-loop approach capable of providing personalized algorithmic recourse tailored to the needs of any end-user. PEAR builds on insights from Bayesian Preference Elicitation to iteratively refine an estimate of the costs of actions by asking choice set queries to the target user. The queries themselves are computed by maximizing the Expected Utility of Selection, a principled measure of information gain accounting for uncertainty on both the cost estimate and the user's responses. PEAR integrates elicitation into a Rein
    
[^222]: 探究图自编码器中 Masked Graph Modeling 的作用

    What's Behind the Mask: Understanding Masked Graph Modeling for Graph Autoencoders. (arXiv:2205.10053v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.10053](http://arxiv.org/abs/2205.10053)

    本文探究了 Masked Graph Modeling 在图自编码器中的有效应用，并提出了基于蒙版的图建模（MGM）作为自监督学习的一个原先的任务，证明了它的优势作用。在获得了理论及实证证据的支持后，我们提出了 MaskGAE，一种可扩展有效的自监督学习方法。

    

    近年来，自监督学习的一种颇具潜力的策略被称为 Masked Autoencoding。然而，对于在图自编码器中如何实现 Masking，理论上仍然存在缺失。本文提出了 Masked Graph Autoencoder （MaskGAE），它是用于图结构数据的自监督学习框架。相较于其它普通的 GAEs，MaskGAE 采用基于蒙版的图建模（Masked Graph Modeling，MGM）作为一个原先的任务。在这个任务中，“掩蔽”一个部分边缘，以部分可视、非掩蔽的图结构，试图重构缺失的部分。我们提供了理论及实证证据，全面证明了此预测任务优势的作用，以探究 MGM 对 GAEs 的输出表示的改善作用。在理论上，我们建立了 GAEs 与对比学习的紧密关系，表明 MGM 明显改善了 GAEs 的自监督学习方案。在经验方面，我们在各种基准数据集上进行了广泛实验，并展示了 MaskGAE 在各种评估指标下始终优于最先进的 GAEs。本研究为探究 Masked Graph Modeling 在图自编码器中的重要性提供了启示，并为图表示学习提供了一种可扩展和有效的自监督学习方法。

    The last years have witnessed the emergence of a promising self-supervised learning strategy, referred to as masked autoencoding. However, there is a lack of theoretical understanding of how masking matters on graph autoencoders (GAEs). In this work, we present masked graph autoencoder (MaskGAE), a self-supervised learning framework for graph-structured data. Different from standard GAEs, MaskGAE adopts masked graph modeling (MGM) as a principled pretext task - masking a portion of edges and attempting to reconstruct the missing part with partially visible, unmasked graph structure. To understand whether MGM can help GAEs learn better representations, we provide both theoretical and empirical evidence to comprehensively justify the benefits of this pretext task. Theoretically, we establish close connections between GAEs and contrastive learning, showing that MGM significantly improves the self-supervised learning scheme of GAEs. Empirically, we conduct extensive experiments on a variet
    
[^223]: 基于机器学习的多阶段系统对真实患者数据进行视力预测

    Visual Acuity Prediction on Real-Life Patient Data Using a Machine Learning Based Multistage System. (arXiv:2204.11970v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2204.11970](http://arxiv.org/abs/2204.11970)

    本研究提供了一种使用机器学习技术开发预测模型的多阶段系统，可高精度预测三种眼疾患者的视力变化，并辅助眼科医生进行临床决策和患者咨询。

    

    现实生活中，眼科学中的玻璃体手术药物治疗是治疗年龄相关性黄斑变性（AMD）、糖尿病性黄斑水肿（DME）和视网膜静脉阻塞（RVO）相关疾病的一种普遍治疗方法。然而，在真实世界的情况下，由于数据的异质性和不完整性，患者往往会在多年时间内失去视力，尽管接受治疗。本文采用多种IT系统，提出了一种用于研究的数据集成流程，该流程融合了德国一家最佳医疗保健医院的眼科部门的不同IT系统。经过使用机器学习技术开发预测模型，我们实现了对患者视力的预测。我们的结果表明，我们的系统可以为三种疾病的预测提供高准确性。此外，我们还展示了我们的系统可以作为工具，辅助眼科医生进行临床决策和患者咨询。

    In ophthalmology, intravitreal operative medication therapy (IVOM) is a widespread treatment for diseases related to the age-related macular degeneration (AMD), the diabetic macular edema (DME), as well as the retinal vein occlusion (RVO). However, in real-world settings, patients often suffer from loss of vision on time scales of years despite therapy, whereas the prediction of the visual acuity (VA) and the earliest possible detection of deterioration under real-life conditions is challenging due to heterogeneous and incomplete data. In this contribution, we present a workflow for the development of a research-compatible data corpus fusing different IT systems of the department of ophthalmology of a German maximum care hospital. The extensive data corpus allows predictive statements of the expected progression of a patient and his or her VA in each of the three diseases. We found out for the disease AMD a significant deterioration of the visual acuity over time. Within our proposed m
    
[^224]: 有限维下的压缩经验测度

    Compressed Empirical Measures (in finite dimensions). (arXiv:2204.08847v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2204.08847](http://arxiv.org/abs/2204.08847)

    本论文探讨了在有限维再生核希尔伯特空间中压缩经验测度的方法，导出了关于这样一个近似的核心集必须有的大小的高概率下限，并开发了一些技术以将压缩方法应用于具体的推断问题。

    

    我们研究了有限维再生核希尔伯特空间（RKHSs）中压缩经验测度的方法。在这种情况下，经验测度包含在一个自然的凸集中，并且可以使用凸优化方法来近似。在某些条件下，这种近似会导致数据点的coreset。控制这样一个coreset必须有多大的一个关键数量是包含在经验凸集中的经验测量周围的最大球的大小。我们的大部分工作是在各种条件下导出关于这样一个球的大小的高概率下限。我们通过开发技术，使得我们能够将压缩方法应用于具体的推断问题，如核岭回归，来补充这种下限的派生。我们最后介绍了一种无限维RKHS的构造，其中压缩很差，突出了我们面临的一些困难。

    We study approaches for compressing the empirical measure in the context of finite dimensional reproducing kernel Hilbert spaces (RKHSs).In this context, the empirical measure is contained within a natural convex set and can be approximated using convex optimization methods.Such an approximation gives under certain conditions rise to a coreset of data points. A key quantity that controls how large such a coreset has to be is the size of the largest ball around the empirical measure that is contained within the empirical convex set. The bulk of our work is concerned with deriving high probability lower bounds on the size of such a ball under various conditions. We complement this derivation of the lower bound by developing techniques that allow us to apply the compression approach to concrete inference problems such as kernel ridge regression. We conclude with a construction of an infinite dimensional RKHS for which the compression is poor, highlighting some of the difficulties one face
    
[^225]: 双路径式学习用于端到端的噪声抗干扰语音识别

    Dual-Path Style Learning for End-to-End Noise-Robust Speech Recognition. (arXiv:2203.14838v3 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2203.14838](http://arxiv.org/abs/2203.14838)

    本文提出了用于端到端的噪声抗干扰语音识别的双路径式学习方法，包括样式学习和融合特征以从干净特征中学习潜在的语音信息，最终实现了10.6％和8.6％的词错误率（WER）降低。

    

    在嘈杂环境下，自动语音识别（ASR）系统会显著降低。最近，引入了语音增强（SE）作为前端，以减少ASR的噪声，但是它也会压制一些重要的语音信息，即过度抑制。为了缓解这种情况，我们提出了一种双路径式学习方法，用于端到端的噪声抗干扰语音识别（DPSL-ASR）。具体而言，我们首先介绍了干净语音特征，以及来自IFF-Net的融合特征作为双路径输入，以恢复被压制的信息。然后，我们提出了样式学习，将融合特征映射到接近干净特征，以从后者中学习潜在的语音信息，即干净的“语音风格”。此外，我们还通过最小化两个路径的最终ASR输出之间的距离来提高噪声鲁棒性。实验结果表明，所提出的方法相对于最佳IFF-Net基线，在RATS和CHiME-4数据集上分别实现了10.6％和8.6％的词错误率（WER）降低。

    Automatic speech recognition (ASR) systems degrade significantly under noisy conditions. Recently, speech enhancement (SE) is introduced as front-end to reduce noise for ASR, but it also suppresses some important speech information, i.e., over-suppression. To alleviate this, we propose a dual-path style learning approach for end-to-end noise-robust speech recognition (DPSL-ASR). Specifically, we first introduce clean speech feature along with the fused feature from IFF-Net as dual-path inputs to recover the suppressed information. Then, we propose style learning to map the fused feature close to clean feature, in order to learn latent speech information from the latter, i.e., clean "speech style". Furthermore, we also minimize the distance of final ASR outputs in two paths to improve noise-robustness. Experiments show that the proposed approach achieves relative word error rate (WER) reductions of 10.6% and 8.6% over the best IFF-Net baseline, on RATS and CHiME-4 datasets respectively.
    
[^226]: 动态特征快速说话者适应在言语障碍和老年人语音识别中的应用

    On-the-Fly Feature Based Rapid Speaker Adaptation for Dysarthric and Elderly Speech Recognition. (arXiv:2203.14593v3 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2203.14593](http://arxiv.org/abs/2203.14593)

    本文提出了两种新形式的数据有效、基于特征的动态说话者适应方法，能够显著提高言语障碍和老年人语音识别的准确率。

    

    目前，准确识别言语障碍和老年人的语音仍然是一个具有挑战性的任务。由口音或性别引起的说话者级别的异质性，与年龄和言语障碍相结合，使这些说话者之间的差异很大。说话者级别数据的稀缺限制了基于数据密集型模型的说话者适应方法的实际应用。因此，本文提出了两种新形式的数据有效、基于特征的动态说话者适应方法：方差正则化频谱基准嵌入（SVR）和谱特征驱动的f-LHUC转换。在UASpeech言语障碍和DementiaBank Pitt老年人语音语料库上进行的实验证明，所提出的动态说话者适应方法能够持续地优于基线iVector自适应混合DNN/TDNN和E2E Conformer系统，统计显著地减少WER 2.48%-2.85%（绝对值）（7.92%-8.06%相关），与离线模型基础LHUC适应分别减小1.82%（相对值）。

    Accurate recognition of dysarthric and elderly speech remain challenging tasks to date. Speaker-level heterogeneity attributed to accent or gender, when aggregated with age and speech impairment, create large diversity among these speakers. Scarcity of speaker-level data limits the practical use of data-intensive model based speaker adaptation methods. To this end, this paper proposes two novel forms of data-efficient, feature-based on-the-fly speaker adaptation methods: variance-regularized spectral basis embedding (SVR) and spectral feature driven f-LHUC transforms. Experiments conducted on UASpeech dysarthric and DementiaBank Pitt elderly speech corpora suggest the proposed on-the-fly speaker adaptation approaches consistently outperform baseline iVector adapted hybrid DNN/TDNN and E2E Conformer systems by statistically significant WER reduction of 2.48%-2.85% absolute (7.92%-8.06% relative), and offline model based LHUC adaptation by 1.82% absolute (5.63% relative) respectively.
    
[^227]: 采样后事后群体公平排名

    Sampling Ex-Post Group-Fair Rankings. (arXiv:2203.00887v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.00887](http://arxiv.org/abs/2203.00887)

    本文提出了一组自然的公理来定义随机群体公平排名，证明了仅有事后群体公平排名的分布的存在，并提出了两种算法来采样随机的群体公平排名。

    

    最近随机排名受到关注，旨在实现比确定性排名更公平的曝光和更好的鲁棒性。我们提出了一组自然的公理来定义随机群体公平排名，并证明存在一个满足我们公理的唯一分布 $D$，该分布仅被支持于事后群体公平排名，即在前 $k$ 名中满足特定下限和上限的群体代表性。我们的问题形式即使存在隐含偏见、不完整的相关信息或者仅有顺序排名而无相关得分或效用值也可以处理。我们提出了两种算法来从上述分布 $D$ 中采样随机的群体公平排名。我们的第一个基于动态规划的算法可以在 $O(k^2\ell)$ 的时间内均匀随机采样事后群体公平排名，其中 $\ell$ 是组的数量。我们的第二个基于随机游走的算法可以从分布 $\de$ 采样事后群体公平排名。

    Randomized rankings have been of recent interest to achieve ex-ante fairer exposure and better robustness than deterministic rankings. We propose a set of natural axioms for randomized group-fair rankings and prove that there exists a unique distribution $D$ that satisfies our axioms and is supported only over ex-post group-fair rankings, i.e., rankings that satisfy given lower and upper bounds on group-wise representation in the top-$k$ ranks. Our problem formulation works even when there is implicit bias, incomplete relevance information, or only ordinal ranking is available instead of relevance scores or utility values.  We propose two algorithms to sample a random group-fair ranking from the distribution $D$ mentioned above. Our first dynamic programming-based algorithm samples ex-post group-fair rankings uniformly at random in time $O(k^2\ell)$, where $\ell$ is the number of groups. Our second random walk-based algorithm samples ex-post group-fair rankings from a distribution $\de
    
[^228]: 在测地度量空间中的Sion极小极大定理和黎曼外推算法

    Sion's Minimax Theorem in Geodesic Metric Spaces and a Riemannian Extragradient Algorithm. (arXiv:2202.06950v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2202.06950](http://arxiv.org/abs/2202.06950)

    该论文提出了在测地度量空间中的Sion极小极大定理和黎曼外推算法，在保持问题可处理的同时，为非凸-非凹极小极大问题提供了一个广泛的推广。

    

    判断非凸-非凹问题是否存在鞍点通常是难以处理的。该论文向理解一类保持可处理的非凸-非凹极小极大问题迈出了一步。具体而言，它研究了测地度量空间上的极小极大问题，这提供了通常的凸-凹鞍点问题的广泛推广。论文的第一个主要结果是Sion极小极大定理的测地度量空间版本; 我们认为我们的证明是新颖且广泛可用的，因为它仅基于有限交叉性质。第二个主要结果是针对完整测地黎曼流形的专业化：在这里，我们设计和分析了光滑极小极大问题的一阶方法的复杂性。

    Deciding whether saddle points exist or are approximable for nonconvex-nonconcave problems is usually intractable. This paper takes a step towards understanding a broad class of nonconvex-nonconcave minimax problems that do remain tractable. Specifically, it studies minimax problems over geodesic metric spaces, which provide a vast generalization of the usual convex-concave saddle point problems. The first main result of the paper is a geodesic metric space version of Sion's minimax theorem; we believe our proof is novel and broadly accessible as it relies on the finite intersection property alone. The second main result is a specialization to geodesically complete Riemannian manifolds: here, we devise and analyze the complexity of first-order methods for smooth minimax problems.
    
[^229]: 探究自注意力机制用于语音分离

    Exploring Self-Attention Mechanisms for Speech Separation. (arXiv:2202.02884v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2202.02884](http://arxiv.org/abs/2202.02884)

    本论文探究了Transformers用于语音分离的应用，提出了SepFormer模型并在包括含噪和含噪回声的数据集上得到了最新成果。同时，还做了语音增强方面的研究，并首次将Linformers、Lonformers和ReFormers等高效的自注意力机制应用于语音分离，发现它们可以显著减少内存需求。

    

    Transformers在深度学习中取得了令人瞩目的成就。它们往往在许多任务中优于循环和卷积模型，同时利用并行处理的优势。最近，我们提出了SepFormer，在WSJ0-2/3 Mix数据集上获得了语音分离方面的最新成果。本文深入研究了用于语音分离的Transformers。特别地，我们通过在更具挑战性的含噪和含噪回声的数据集（如LibriMix、WHAM！和WHAMR！）上提供结果，扩展了我们之前关于SepFormer的发现。此外，我们将我们的模型扩展到执行语音增强，并提供了消除噪声和混响任务的实验证据。最后，我们首次在语音分离中研究了使用高效的自注意力机制，如Linformers、Lonformers和ReFormers。我们发现，它们显著减少了内存需求。例如，我们展示了基于Reformer的注意力机制优于流行的ConvS2s注意力机制，同时仅使用约四分之一的内存。

    Transformers have enabled impressive improvements in deep learning. They often outperform recurrent and convolutional models in many tasks while taking advantage of parallel processing. Recently, we proposed the SepFormer, which obtains state-of-the-art performance in speech separation with the WSJ0-2/3 Mix datasets. This paper studies in-depth Transformers for speech separation. In particular, we extend our previous findings on the SepFormer by providing results on more challenging noisy and noisy-reverberant datasets, such as LibriMix, WHAM!, and WHAMR!. Moreover, we extend our model to perform speech enhancement and provide experimental evidence on denoising and dereverberation tasks. Finally, we investigate, for the first time in speech separation, the use of efficient self-attention mechanisms such as Linformers, Lonformers, and ReFormers. We found that they reduce memory requirements significantly. For example, we show that the Reformer-based attention outperforms the popular Con
    
[^230]: HeterPS：基于强化学习调度的异构环境下分布式深度学习

    HeterPS: Distributed Deep Learning With Reinforcement Learning Based Scheduling in Heterogeneous Environments. (arXiv:2111.10635v3 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2111.10635](http://arxiv.org/abs/2111.10635)

    这篇论文介绍了一个名为Paddle-HeterPS的分布式框架，基于强化学习的调度方法可以高效地利用多种类型的计算资源，解决了分布式深度学习训练中多层次分配计算资源的问题。

    

    深度神经网络利用许多层和大量参数实现了优秀的性能。DNN模型的训练过程通常处理具有许多稀疏特征的大规模输入数据，这会产生高延迟和I/O成本，而某些层的计算成本很高。训练过程通常利用分布式计算资源来减少训练时间。此外，多种类型的计算资源，如CPU和GPU等，也可用于分布式训练过程。因此，多层次地分配计算资源对训练过程至关重要。为了通过异构计算资源高效地训练DNN模型，我们提出了一种分布式框架Paddle-Heterogeneous Parameter Server（Paddle-HeterPS），由分布式架构和基于强化学习的调度方法组成。与现有框架相比，Paddle-HeterPS的优点有三个。

    Deep neural networks (DNNs) exploit many layers and a large number of parameters to achieve excellent performance. The training process of DNN models generally handles large-scale input data with many sparse features, which incurs high Input/Output (IO) cost, while some layers are compute-intensive. The training process generally exploits distributed computing resources to reduce training time. In addition, heterogeneous computing resources, e.g., CPUs, GPUs of multiple types, are available for the distributed training process. Thus, the scheduling of multiple layers to diverse computing resources is critical for the training process. To efficiently train a DNN model using the heterogeneous computing resources, we propose a distributed framework, i.e., Paddle-Heterogeneous Parameter Server (Paddle-HeterPS), composed of a distributed architecture and a Reinforcement Learning (RL)-based scheduling method. The advantages of Paddle-HeterPS are three-fold compared with existing frameworks. 
    
[^231]: 带有核的复合适合性检验方法

    Composite Goodness-of-fit Tests with Kernels. (arXiv:2111.10275v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2111.10275](http://arxiv.org/abs/2111.10275)

    本文提出了一种基于核的假设检验方法，可以解决具有挑战性的复合检验问题，其核心思想是在正确的模型规范的零假设下，非参数地估计参数（或模拟器）分布。

    

    模型错误说明可能会对概率模型的实现造成重大挑战，这促使开发出一些直接解决此问题的鲁棒方法。但是，这些更为复杂的方法是否需要取决于模型是否真的错误，目前缺乏通用的方法回答这个问题。在本文中，我们提出了一种方法。更具体地说，我们提出了基于核的假设检验方法，用于具有挑战性的复合检验问题，即我们是否感兴趣的数据来自某些参数模型族中的任何分布。我们的测试利用基于最大均值差异和核Stein差异的最小距离估计器。它们具有广泛的适用性，包括当参数模型的密度已知除标准化常数外，或者如果模型采用模拟器形式。作为我们的主要结果，我们展示了在正确的模型规范的零假设下，我们能够非参数地估计参数（或模拟器）分布。我们提供了建立我们方法有效性的理论，并通过模拟和异常检测应用案例演示了其性能。

    Model misspecification can create significant challenges for the implementation of probabilistic models, and this has led to development of a range of robust methods which directly account for this issue. However, whether these more involved methods are required will depend on whether the model is really misspecified, and there is a lack of generally applicable methods to answer this question. In this paper, we propose one such method. More precisely, we propose kernel-based hypothesis tests for the challenging composite testing problem, where we are interested in whether the data comes from any distribution in some parametric family. Our tests make use of minimum distance estimators based on the maximum mean discrepancy and the kernel Stein discrepancy. They are widely applicable, including whenever the density of the parametric model is known up to normalisation constant, or if the model takes the form of a simulator. As our main result, we show that we are able to estimate the param
    
[^232]: MMD聚合双样本检验

    MMD Aggregated Two-Sample Test. (arXiv:2110.15073v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2110.15073](http://arxiv.org/abs/2110.15073)

    本文提出了两种新颖的基于最大均值差异（MMD）的非参数双样本核检验，并构造了一种自适应平均测试，称为MMDAgg，以解决平滑参数未知的问题。

    

    我们提出了两种新颖的基于最大均值差异（MMD）的非参数双样本核检验。首先，对于固定的核，我们使用排列或野蛮自举（wild bootstrap）构造了一个MMD检验，这两种流行的数值程序可确定测试阈值。我们证明这个测试可以在非渐近情况下控制I型错误的概率。因此，即使在小样本情况下，它仍然保持良好的校准性，这与以前的MMD测试不同，前者只能在渐近意义下保证正确的测试水平。当密度差异在Sobolev球中时，我们证明了我们的MMD检验在特定的核函数下是最优的，该核函数依赖于Sobolev球的平滑参数。在实践中，这个参数是未知的，因此不能使用具有特定核的最优MMD检验。为了解决这个问题，我们构造了一个自适应平均测试，称为MMDAgg。测试功率在Sobolev球的平滑参数上最大化。

    We propose two novel nonparametric two-sample kernel tests based on the Maximum Mean Discrepancy (MMD). First, for a fixed kernel, we construct an MMD test using either permutations or a wild bootstrap, two popular numerical procedures to determine the test threshold. We prove that this test controls the probability of type I error non-asymptotically. Hence, it can be used reliably even in settings with small sample sizes as it remains well-calibrated, which differs from previous MMD tests which only guarantee correct test level asymptotically. When the difference in densities lies in a Sobolev ball, we prove minimax optimality of our MMD test with a specific kernel depending on the smoothness parameter of the Sobolev ball. In practice, this parameter is unknown and, hence, the optimal MMD test with this particular kernel cannot be used. To overcome this issue, we construct an aggregated test, called MMDAgg, which is adaptive to the smoothness parameter. The test power is maximised ove
    
[^233]: Sinkhorn分布鲁棒优化

    Sinkhorn Distributionally Robust Optimization. (arXiv:2109.11926v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2109.11926](http://arxiv.org/abs/2109.11926)

    本文通过使用Sinkhorn距离进行分布鲁棒优化，推导出更容易处理且在实际中更合理的最坏情况分布，提出了解决方案，并展示了其优越性能。

    

    我们研究了使用Sinkhorn距离 -一种基于熵正则化的Wasserstein距离变体- 的分布鲁棒优化（DRO）。我们为一般名义分布推导了凸规划对偶重构。相比于Wasserstein DRO，对于更大类的损失函数，它在计算上更容易处理，它的最坏情况分布对实际应用更合理。为了解决对偶重构，我们开发了一种使用有偏梯度神经元的随机镜像下降算法，并分析了其收敛速度。最后，我们提供了使用合成和真实数据的数值实例，以证明其优越性能。

    We study distributionally robust optimization (DRO) with Sinkhorn distance -a variant of Wasserstein distance based on entropic regularization. We derive convex programming dual reformulation for a general nominal distribution. Compared with Wasserstein DRO, it is computationally tractable for a larger class of loss functions, and its worst-case distribution is more reasonable for practical applications. To solve the dual reformulation, we develop a stochastic mirror descent algorithm using biased gradient oracles and analyze its convergence rate. Finally, we provide numerical examples using synthetic and real data to demonstrate its superior performance.
    
[^234]: 分布漂移下的随机优化

    Stochastic Optimization under Distributional Drift. (arXiv:2108.07356v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2108.07356](http://arxiv.org/abs/2108.07356)

    本文提供了一种在分布漂移下优化凸函数的新方法，经数值实验证明在低漂移-噪声比的情况下，近端随机梯度方法采用步长衰减策略可显著提升跟踪效率。

    

    本文考虑了最小化随机演化凸函数的问题，这个演化过程是未知的，并且可能依赖于时间和决策变量本身。这类问题在机器学习和信号处理领域中广泛存在，称为概念漂移、随机跟踪和执行预测。我们提供了新的非渐近收敛保证，重点关注在期望值和高概率下成立的界限。我们获得的效率估计明确地解耦了优化误差、梯度噪声和时间漂移的影响。值得注意的是，我们确定了一个低漂移-噪声比的区域，在这个区域里，近端随机梯度方法的跟踪效率因步长衰减策略而受益显著。数值实验证明了我们的结果。

    We consider the problem of minimizing a convex function that is evolving according to unknown and possibly stochastic dynamics, which may depend jointly on time and on the decision variable itself. Such problems abound in the machine learning and signal processing literature, under the names of concept drift, stochastic tracking, and performative prediction. We provide novel non-asymptotic convergence guarantees for stochastic algorithms with iterate averaging, focusing on bounds valid both in expectation and with high probability. The efficiency estimates we obtain clearly decouple the contributions of optimization error, gradient noise, and time drift. Notably, we identify a low drift-to-noise regime in which the tracking efficiency of the proximal stochastic gradient method benefits significantly from a step decay schedule. Numerical experiments illustrate our results.
    
[^235]: 带有定常大学习率的SGD可能收敛于局部最大值

    SGD with a Constant Large Learning Rate Can Converge to Local Maxima. (arXiv:2107.11774v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.11774](http://arxiv.org/abs/2107.11774)

    本研究构建了最坏情况下的优化问题，证明了带有定常大学习率的SGD可能表现出许多奇怪且潜在的不良行为，包括：收敛于局部最大值、缓慢越过鞍点和更喜欢尖锐的最小值。这强调了深入分析SGD在深度学习中作用的重要性。

    

    先前关于随机梯度下降（SGD）的研究通常着眼于其成功，本研究构建了最坏情况下的优化问题，证明了在先前研究通常假设不成立的情况下，SGD可能表现出许多奇怪且潜在的不良行为。具体来说，我们构建了景观和数据分布，使得（1）SGD收敛于局部最大值，（2）SGD缓慢越过鞍点，(3) SGD更喜欢尖锐的最小值而非平坦的最小值，(4) AMSGrad收敛于局部最大值。我们还通过极简的神经网络示例进行了实现。我们的研究强调了同时分析小批量采样、离散时间更新规则和现实景观以了解SGD在深度学习中的作用的重要性。

    Previous works on stochastic gradient descent (SGD) often focus on its success. In this work, we construct worst-case optimization problems illustrating that, when not in the regimes that the previous works often assume, SGD can exhibit many strange and potentially undesirable behaviors. Specifically, we construct landscapes and data distributions such that (1) SGD converges to local maxima, (2) SGD escapes saddle points arbitrarily slowly, (3) SGD prefers sharp minima over flat ones, and (4) AMSGrad converges to local maxima. We also realize results in a minimal neural network-like example. Our results highlight the importance of simultaneously analyzing the minibatch sampling, discrete-time updates rules, and realistic landscapes to understand the role of SGD in deep learning.
    
[^236]: 生成对抗网络的指纹识别技术

    Fingerprinting Generative Adversarial Networks. (arXiv:2106.11760v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2106.11760](http://arxiv.org/abs/2106.11760)

    本文提出了一种保护GAN知识产权的指纹识别方案，通过生成指纹样本并嵌入到分类器中进行版权验证，解决了前一种对分类模型的指纹识别方法在简单转移至GAN时遇到的隐蔽性和鲁棒性瓶颈，具有实际保护现代GAN模型的可行性。

    

    生成对抗网络（GANs）已经广泛应用于各种应用场景。由于商业GAN的生产需要大量的计算和人力资源，因此迫切需要版权保护。本文提出了一种用于保护GAN知识产权的指纹识别方案。我们突破了前一种对分类模型的指纹识别方法在简单转移至GAN时所遇到的隐蔽性和鲁棒性瓶颈。具体来说，我们创造性地从目标GAN和分类器构建一个复合深度学习模型。然后，我们从这个复合模型中产生指纹样本，并将其嵌入到分类器中，以进行有效的版权验证。这种方案启发了一些具体的方法，以实际保护现代GAN模型。理论分析证明了这些方法可以满足知识产权保护所需要的不同安全要求。我们还进行了实验来证明该方案的功效。

    Generative Adversarial Networks (GANs) have been widely used in various application scenarios. Since the production of a commercial GAN requires substantial computational and human resources, the copyright protection of GANs is urgently needed. In this paper, we present the first fingerprinting scheme for the Intellectual Property (IP) protection of GANs. We break through the stealthiness and robustness bottlenecks suffered by previous fingerprinting methods for classification models being naively transferred to GANs. Specifically, we innovatively construct a composite deep learning model from the target GAN and a classifier. Then we generate fingerprint samples from this composite model, and embed them in the classifier for effective ownership verification. This scheme inspires some concrete methodologies to practically protect the modern GAN models. Theoretical analysis proves that these methods can satisfy different security requirements necessary for IP protection. We also conduct 
    
[^237]: 方差相关的最优臂识别

    Variance-Dependent Best Arm Identification. (arXiv:2106.10417v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.10417](http://arxiv.org/abs/2106.10417)

    本文研究了在多臂老虎机游戏中识别最优臂的问题。提出了一种自适应算法，该算法探索臂的奖励差距和方差，并使用一种称为分组中位数淘汰的新方法根据收集的信息做出未来决策。所提出的算法保证以概率(1-δ)输出最优臂，并使用最多的O（Σ(i=1)^n (σi²/Δi²+1/Δi)(lnδ-1+ln lnΔi-1)）个样本，这比方差独立算法获得了明显的优势。

    

    本文研究了在随机多臂老虎机游戏中识别最优臂的问题。给定一个从1到n标号的臂的集合，每个臂i都与一个支持[0,1]上平均值为θi和方差为σi²的未知奖励分布相关联。假设θ1>θ2≥...≥θn。我们提出了一种自适应算法，该算法探索臂的奖励差距和方差，并使用一种称为分组中位数淘汰的新方法根据收集的信息做出未来决策。所提出的算法保证以概率(1-δ)输出最优臂，并使用最多的O（Σ(i=1)^n (σi²/Δi²+1/Δi)(lnδ-1+ln lnΔi-1)）个样本，其中 Δi (i≥2)表示臂i与最优臂之间的奖励差距，我们定义 Δ1 = Δ2。这比方差独立算法获得了明显的优势。

    We study the problem of identifying the best arm in a stochastic multi-armed bandit game. Given a set of $n$ arms indexed from $1$ to $n$, each arm $i$ is associated with an unknown reward distribution supported on $[0,1]$ with mean $\theta_i$ and variance $\sigma_i^2$. Assume $\theta_1 > \theta_2 \geq \cdots \geq\theta_n$. We propose an adaptive algorithm which explores the gaps and variances of the rewards of the arms and makes future decisions based on the gathered information using a novel approach called \textit{grouped median elimination}. The proposed algorithm guarantees to output the best arm with probability $(1-\delta)$ and uses at most $O \left(\sum_{i = 1}^n \left(\frac{\sigma_i^2}{\Delta_i^2} + \frac{1}{\Delta_i}\right)(\ln \delta^{-1} + \ln \ln \Delta_i^{-1})\right)$ samples, where $\Delta_i$ ($i \geq 2$) denotes the reward gap between arm $i$ and the best arm and we define $\Delta_1 = \Delta_2$. This achieves a significant advantage over the variance-independent algorit
    
[^238]: FairCanary: 快速持续的可解释公平性监控系统

    FairCanary: Rapid Continuous Explainable Fairness. (arXiv:2106.07057v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.07057](http://arxiv.org/abs/2106.07057)

    FairCanary是一个持续模型监控系统，使用量化人口漂移(QDD)度量模型偏差，避免传统阈值偏差度量的统计限制，可以提供可解释的公平性分析，并在多个实际数据集上实现了可操作性见解。

    

    针对已部署的机器学习(ML)和人工智能(AI)模型的失败和新的法规要求，出现了提供持续模型监控的系统。现有的监控系统持续跟踪已部署的ML模型的性能，并为每个预测计算特征重要性(即解释)，以帮助开发人员确定新出现的模型性能问题的根本原因。我们提出量化人口漂移(QDD)，一种新的模型偏差量化度量，使用分位数分组来度量子组之间整体预测分布的差异。QDD非常适合于连续监控场景，不会受到传统基于阈值偏差度量的统计限制，并且不需要结果标签(可能无法在运行时获得)。我们将QDD纳入连续模型监控系统FairCanary中，该系统重用现有的实验基础设施，利用最先进的可解释性方法提供快速、持续且可解释的公平性分析。FairCanary在几个实际数据集上进行了评估，并向开发人员提供可操作性见解，以提高其ML模型的公平性。

    Systems that offer continuous model monitoring have emerged in response to (1) well-documented failures of deployed Machine Learning (ML) and Artificial Intelligence (AI) models and (2) new regulatory requirements impacting these models. Existing monitoring systems continuously track the performance of deployed ML models and compute feature importance (a.k.a. explanations) for each prediction to help developers identify the root causes of emergent model performance problems.  We present Quantile Demographic Drift (QDD), a novel model bias quantification metric that uses quantile binning to measure differences in the overall prediction distributions over subgroups. QDD is ideal for continuous monitoring scenarios, does not suffer from the statistical limitations of conventional threshold-based bias metrics, and does not require outcome labels (which may not be available at runtime). We incorporate QDD into a continuous model monitoring system, called FairCanary, that reuses existing exp
    
[^239]: 多样化高斯噪声一致性正则化用于鲁棒性和不确定性校准

    Diverse Gaussian Noise Consistency Regularization for Robustness and Uncertainty Calibration. (arXiv:2104.01231v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2104.01231](http://arxiv.org/abs/2104.01231)

    本研究提出了一种多样化高斯噪声一致性正则化方法，用于同时提高图像分类器的鲁棒性和准确性，与其他强大的多样化数据增强基础相比，可以将鲁棒性提高4.2-18.4％以应对未预见的噪声污染。

    

    当训练和测试数据分布一致时，深度神经网络可以实现高水平的预测精度。然而，在实际情况中，各种类型的损坏会导致表现严重下降，这与预期的情况有所偏差。目前只有少数方法可以在出现未预见到的领域偏移时提高泛化能力。特别是，在图像获取阶段，数字噪声污染经常出现。我们提出了一种多样化高斯噪声一致性正则化方法，用于改进图像分类器在各种污染情况下的鲁棒性，同时仍保持较高的准确性。通过本地损失景观分析，我们导出界限，以激励和理解我们的高斯噪声一致性正则化的行为。相比于对抗性训练和其他强大的多样化数据增强基础，我们的方法可以将鲁棒性提高4.2-18.4％以应对未预见的噪声污染。

    Deep neural networks achieve high prediction accuracy when the train and test distributions coincide. In practice though, various types of corruptions occur which deviate from this setup and cause severe performance degradations. Few methods have been proposed to address generalization in the presence of unforeseen domain shifts. In particular, digital noise corruptions arise commonly in practice during the image acquisition stage and present a significant challenge for current methods. In this paper, we propose a diverse Gaussian noise consistency regularization method for improving robustness of image classifiers under a variety of corruptions while still maintaining high clean accuracy. We derive bounds to motivate and understand the behavior of our Gaussian noise consistency regularization using a local loss landscape analysis. Our approach improves robustness against unforeseen noise corruptions by 4.2-18.4% over adversarial training and other strong diverse data augmentation base
    
[^240]: 有效通信下联邦多智能体强化学习的梯度收敛界限

    The Gradient Convergence Bound of Federated Multi-Agent Reinforcement Learning with Efficient Communication. (arXiv:2103.13026v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.13026](http://arxiv.org/abs/2103.13026)

    本文提出了两种优化策略, 一种是逐步衰减本地梯度权重的衰减模式, 另一种是基于代价最小化设计的共识算法来减少模型之间的通信量, 有效解决了独立学习环境异质性和联邦学习的通信开销问题。

    

    本文考虑联邦学习范式中多智能体协作决策的独立强化学习，但是由于独立学习环境异质性和联邦学习的通信开销问题，现有方法在训练过程中存在收敛问题。因此本文提出了两种优化策略，一种是逐步衰减本地梯度权重的衰减模式，另一种是基于代价最小化设计的共识算法来减少模型之间的通信量，理论分析和实验证明了这两种方法的有效性和优越性。

    The paper considers independent reinforcement learning (IRL) for multi-agent collaborative decision-making in the paradigm of federated learning (FL). However, FL generates excessive communication overheads between agents and a remote central server, especially when it involves a large number of agents or iterations. Besides, due to the heterogeneity of independent learning environments, multiple agents may undergo asynchronous Markov decision processes (MDPs), which will affect the training samples and the model's convergence performance. On top of the variation-aware periodic averaging (VPA) method and the policy-based deep reinforcement learning (DRL) algorithm (i.e., proximal policy optimization (PPO)), this paper proposes two advanced optimization schemes orienting to stochastic gradient descent (SGD): 1) A decay-based scheme gradually decays the weights of a model's local gradients with the progress of successive local updates, and 2) By representing the agents as a graph, a cons
    
[^241]: 个性化联邦学习：统一框架和通用优化技术

    Personalized Federated Learning: A Unified Framework and Universal Optimization Techniques. (arXiv:2102.09743v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2102.09743](http://arxiv.org/abs/2102.09743)

    本文探讨了个性化联邦学习的优化问题，并提出了适用于多种现有个性化FL目标的通用优化器。同时，提出了一个适用于广泛一类强凸性个性化FL模型的全面优化理论。其方法在通信和本地计算方面具有实用性和优越性，并能够恢复应对特定个性化FL目标的最佳计算和通信保证。

    

    本研究探讨了个性化联邦学习（FL）的优化方面。我们提出了通用优化器，可应用于许多现有的个性化FL目标，特别是一种定制的本地SGD变体和加速坐标下降/加速SVRCD的变体。通过研究一般的个性化目标，能够将许多现有的个性化FL目标作为特殊情况恢复，我们开发了一种应用于文献中广泛的一类强凸性个性化FL模型的全面优化理论。我们展示了我们的方法在通信和本地计算方面的实用性和/或优越性。值得注意的是，我们的通用优化求解器和理论可以恢复用于特定个性化FL目标的最佳已知通信和计算保证。因此，我们提出的方法可以作为通用优化器，在许多情况下可以免去设计特定任务优化器的需要。

    We investigate the optimization aspects of personalized Federated Learning (FL). We propose general optimizers that can be applied to numerous existing personalized FL objectives, specifically a tailored variant of Local SGD and variants of accelerated coordinate descent/accelerated SVRCD. By examining a general personalized objective capable of recovering many existing personalized FL objectives as special cases, we develop a comprehensive optimization theory applicable to a wide range of strongly convex personalized FL models in the literature. We showcase the practicality and/or optimality of our methods in terms of communication and local computation. Remarkably, our general optimization solvers and theory can recover the best-known communication and computation guarantees for addressing specific personalized FL objectives. Consequently, our proposed methods can serve as universal optimizers, rendering the design of task-specific optimizers unnecessary in many instances.
    
[^242]: 关于异常的性质和类型：数据偏差的回顾

    On the Nature and Types of Anomalies: A Review of Deviations in Data. (arXiv:2007.15634v5 [cs.DB] UPDATED)

    [http://arxiv.org/abs/2007.15634](http://arxiv.org/abs/2007.15634)

    本文通过广泛文献综述，提供了第一个理论上基于原则且领域无关的数据异常分类法，并呈现了异常类型和子类型的全面概述。

    

    异常是指数据集中以某种方式不寻常且不符合一般模式的事件。异常的概念通常没有明确定义，并被认为是模糊的和依赖于具体领域的。此外，尽管在这个主题上已有约250年的出版物，但迄今尚未发表过关于不同类型的异常的综合和具体概述。通过广泛文献综述，本研究因此提供了第一个理论上基于原则且领域无关的数据异常分类法，并呈现了异常类型和子类型的全面概述。为了明确定义异常的概念及其不同的表现，该分类法运用了五个维度：数据类型、关系的基数、异常级别、数据结构和数据分布。这些基本且以数据为中心的维度自然地产生了3个大组、9种基本类型和63个异常子类型。该分类法有助于评估某个数据处理系统对异常的功能能力。

    Anomalies are occurrences in a dataset that are in some way unusual and do not fit the general patterns. The concept of the anomaly is typically ill-defined and perceived as vague and domain-dependent. Moreover, despite some 250 years of publications on the topic, no comprehensive and concrete overviews of the different types of anomalies have hitherto been published. By means of an extensive literature review this study therefore offers the first theoretically principled and domain-independent typology of data anomalies and presents a full overview of anomaly types and subtypes. To concretely define the concept of the anomaly and its different manifestations, the typology employs five dimensions: data type, cardinality of relationship, anomaly level, data structure, and data distribution. These fundamental and data-centric dimensions naturally yield 3 broad groups, 9 basic types, and 63 subtypes of anomalies. The typology facilitates the evaluation of the functional capabilities of an
    
[^243]: 在线学习的现代介绍

    A Modern Introduction to Online Learning. (arXiv:1912.13213v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1912.13213](http://arxiv.org/abs/1912.13213)

    这本专著介绍了在线学习的基本概念以及凸优化背景下的一阶和二阶算法, 包括欧几里得和非欧几里得设置中的在线镜像下降或遵循正则化领导者等算法。

    

    在这本专著中，我通过现代的在线凸优化视角介绍了在线学习的基本概念。在这里，在线学习指的是在最坏情况下的遗憾最小化框架。我介绍了一阶和二阶具有凸损失的在线学习算法，以欧几里得和非欧几里得设置为基础，所有算法都清晰地呈现为在线镜像下降或遵循正则化领导者及其变形的实例。特别关注算法参数的调整问题和通过自适应和无参数在线学习算法在无界域中的学习。 凸损失通过凸代理损失和随机化处理，来处理非凸损失。同时还简要讨论了赌博设置，涉及敌对和随机多臂赌博问题。这些笔记不需要先前对凸分析的了解，并且所有所需的数学工具都已严谨解释。

    In this monograph, I introduce the basic concepts of Online Learning through a modern view of Online Convex Optimization. Here, online learning refers to the framework of regret minimization under worst-case assumptions. I present first-order and second-order algorithms for online learning with convex losses, in Euclidean and non-Euclidean settings. All the algorithms are clearly presented as instantiation of Online Mirror Descent or Follow-The-Regularized-Leader and their variants. Particular attention is given to the issue of tuning the parameters of the algorithms and learning in unbounded domains, through adaptive and parameter-free online learning algorithms. Non-convex losses are dealt through convex surrogate losses and through randomization. The bandit setting is also briefly discussed, touching on the problem of adversarial and stochastic multi-armed bandits. These notes do not require prior knowledge of convex analysis and all the required mathematical tools are rigorously ex
    
[^244]: Dirichlet多项式的快速MLE计算

    Fast MLE Computation for the Dirichlet Multinomial. (arXiv:1405.0099v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1405.0099](http://arxiv.org/abs/1405.0099)

    本文提出了一种修改的方法来快速计算Dirichlet分布的MLE参数，相较于现有实现方法，只需要一遍遍历数据集就可以大大减少运行时间。

    

    给定一个分类数据集，我们希望找到一个Dirichlet分布的参数，使得在该分布下，这个数据集的似然函数最大化。通常利用牛顿迭代法来求解，但目前的实现需要每次迭代都读取整个数据集。在本文中，我们提出了一种修改方法，只需要一次通过数据集，并大大减少了运行时间。此外，我们还从理论和实证的角度分析了该算法的性能，并提供了开源实现。

    Given a collection of categorical data, we want to find the parameters of a Dirichlet distribution which maximizes the likelihood of that data. Newton's method is typically used for this purpose but current implementations require reading through the entire dataset on each iteration. In this paper, we propose a modification which requires only a single pass through the dataset and substantially decreases running time. Furthermore we analyze both theoretically and empirically the performance of the proposed algorithm, and provide an open source implementation.
    
[^245]: 关于Boosting算法的对偶形式

    On the Dual Formulation of Boosting Algorithms. (arXiv:0901.3590v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/0901.3590](http://arxiv.org/abs/0901.3590)

    本文从对偶问题的角度研究了Boosting算法，证明了AdaBoost等算法通过最大化间隔和控制间隔方差来维护更好的间隔分布，提出了基于列生成的优化算法，收敛速度更快。

    

    本文从新的角度研究了Boosting算法。我们展示了AdaBoost、LogitBoost和带广义铰链损失的软间隔LPBoost算法的拉格朗日对偶问题都是熵最大化问题。通过研究这些Boosting算法的对偶问题，我们展示了Boosting算法的成功可以通过最大化间隔和控制间隔方差来维护更好的间隔分布来理解。我们还理论上证明了，近似地，AdaBoost最大化的是平均间隔而非最小间隔。对偶形式还使我们能够开发基于列生成的优化算法，这些算法是完全校正的。我们展示了它们与标准的逐步加性Boosting算法几乎完全相同的分类结果，但收敛速度更快。因此，使用我们提出的优化技术构建集合只需要较少的弱分类器。

    We study boosting algorithms from a new perspective. We show that the Lagrange dual problems of AdaBoost, LogitBoost and soft-margin LPBoost with generalized hinge loss are all entropy maximization problems. By looking at the dual problems of these boosting algorithms, we show that the success of boosting algorithms can be understood in terms of maintaining a better margin distribution by maximizing margins and at the same time controlling the margin variance.We also theoretically prove that, approximately, AdaBoost maximizes the average margin, instead of the minimum margin. The duality formulation also enables us to develop column generation based optimization algorithms, which are totally corrective. We show that they exhibit almost identical classification results to that of standard stage-wise additive boosting algorithms but with much faster convergence rates. Therefore fewer weak classifiers are needed to build the ensemble using our proposed optimization technique.
    

