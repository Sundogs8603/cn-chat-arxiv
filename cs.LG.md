# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Multi-level protein pre-training with Vabs-Net](https://rss.arxiv.org/abs/2402.01481) | 这篇论文介绍了一种使用Vabs-Net进行多级蛋白质预训练的方法。当前大多数基于结构的预训练模型仅关注残基水平，但忽略了侧链原子的重要性。为了解决这个问题，论文提出了一种新的预训练策略，引入了跨度掩码，以在三维蛋白质预训练中同时建模残基和原子水平的信息，并改善了残基表示的表达能力。 |
| [^2] | [Multiclass Learning from Noisy Labels for Non-decomposable Performance Measures](https://rss.arxiv.org/abs/2402.01055) | 本论文提出了用于从带有噪声标签的数据中学习非可分解性能度量的多类学习算法。这些算法分别适用于单调凸性和线性比率两类性能度量，并基于类条件噪声模型进行噪声校正。 |
| [^3] | [Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning](https://arxiv.org/abs/2402.17768) | 提出了一种名为Diffusion Meets DAgger (DMD)的方法，用于eye-in-hand模仿学习，通过扩散模型创建新样本以提高模型的鲁棒性表现，并减少成本。 |
| [^4] | [Opening Cabinets and Drawers in the Real World using a Commodity Mobile Manipulator](https://arxiv.org/abs/2402.17767) | 实现了一个端到端系统，使商品移动操作器成功在以前未见的真实世界环境中打开橱柜和抽屉，感知误差是主要挑战。 |
| [^5] | [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764) | 介绍了一种新的1比特LLM变体，通过引入三进制参数在保持性能的情况下显著提高了成本效益，定义了新的训练规律，为设计专门硬件优化的1比特LLMs打开了大门 |
| [^6] | [Massive Activations in Large Language Models](https://arxiv.org/abs/2402.17762) | 大型语言模型中出现了大量激活现象，它们具有非常大的值并且在模型中起到重要作用。 |
| [^7] | [Learning to Program Variational Quantum Circuits with Fast Weights](https://arxiv.org/abs/2402.17760) | 本文介绍了量子快速权重编程器（QFWP）作为解决量子循环神经网络（QRNNs）模型训练时间延长问题的解决方案。 |
| [^8] | [Robustly Learning Single-Index Models via Alignment Sharpness](https://arxiv.org/abs/2402.17756) | 通过引入对齐锐度技术，我们提出了一种高效的学习算法，实现了单指数模型的常数近似学习，适用于各种分布和连接函数，是首个适用于高斯数据和任何非平凡连接函数类的稳健学习器。 |
| [^9] | [Evaluating Very Long-Term Conversational Memory of LLM Agents](https://arxiv.org/abs/2402.17753) | 通过引入机器-人流程，基于LLM代理架构并将其对话基于人物角色和时间事件图进行基础，成功创建了LoCoMo数据集，为非常长期对话的研究填补了空白。 |
| [^10] | [Scaling on-chip photonic neural processors using arbitrarily programmable wave propagation](https://arxiv.org/abs/2402.17750) | 提出并演示了一种利用任意可编程波传播的2D波导器件，通过组合光电导增益和电光效应实现对板块的折射率进行大规模并行调制 |
| [^11] | [When Your AI Deceives You: Challenges with Partial Observability of Human Evaluators in Reward Learning](https://arxiv.org/abs/2402.17747) | RLHF在考虑部分观察性时可能导致策略欺骗性地夸大性能或过度辩护行为，我们提出了数学条件来解决这些问题，并警告不要盲目应用RLHF在部分可观测情况下。 |
| [^12] | [reBandit: Random Effects based Online RL algorithm for Reducing Cannabis Use](https://arxiv.org/abs/2402.17739) | reBandit是一种在线RL算法，利用随机效应和贝叶斯先验快速高效地学习，在移动健康环境中通过个性化干预来减少新兴成年人的大麻使用 |
| [^13] | [Learning-Based Algorithms for Graph Searching Problems](https://arxiv.org/abs/2402.17736) | 本研究提出了针对未知图的图搜索问题的基于学习的算法，首次在未知加权图上建立了形式保证，并设计算法在预测误差上具有最优或几乎最佳依存关系。 |
| [^14] | [Batched Nonparametric Contextual Bandits](https://arxiv.org/abs/2402.17732) | 该论文研究了批处理约束下的非参数上下文臂问题，提出了一种名为BaSEDB的方案，在动态分割协变量空间的同时，实现了最优的后悔。 |
| [^15] | [Markovletics: Methods and A Novel Application for Learning Continuous-Time Markov Chain Mixtures](https://arxiv.org/abs/2402.17730) | 本研究引入了一个新颖框架来探索CTMCs，强调观察到的轨迹长度和混合参数对问题情境的影响，这需要特定的算法。 |
| [^16] | [Taming Nonconvex Stochastic Mirror Descent with General Bregman Divergence](https://arxiv.org/abs/2402.17722) | 提出了对一般Bregman散度支持的非凸SMD新收敛分析，克服了先前的限制，并在全局收敛性和高概率收敛性方面取得了进展。 |
| [^17] | [The SMART approach to instance-optimal online learning](https://arxiv.org/abs/2402.17720) | SMART算法在实现实例最优在线学习中取得了重要突破，具有比传统“两全其美”界限更强大的保证，能够在每个输入序列上实现竞争性表现。 |
| [^18] | [Towards a Digital Twin Framework in Additive Manufacturing: Machine Learning and Bayesian Optimization for Time Series Process Optimization](https://arxiv.org/abs/2402.17718) | 提出了用于增材制造的数字孪生框架，结合机器学习和贝叶斯优化实现实时预测控制，解决DED过程中的热管理问题。 |
| [^19] | [Understanding Neural Network Binarization with Forward and Backward Proximal Quantizers](https://arxiv.org/abs/2402.17710) | 从优化的角度阐明神经网络二值化中的训练技巧，提出了ProxConnect++（PC++）这一泛化模型，将现有二值化技术视为其特例 |
| [^20] | [Federated Learning for Estimating Heterogeneous Treatment Effects](https://arxiv.org/abs/2402.17705) | 提出了一种通过联邦学习在机构之间协作学习HTE估计量的新框架，实现了即使在客户之间存在多样干预和受试者群体情况下共同学习特征表示并私下学习特定的预测函数。 |
| [^21] | [Transfer Learning Bayesian Optimization to Design Competitor DNA Molecules for Use in Diagnostic Assays](https://arxiv.org/abs/2402.17704) | 通过将转移学习的代理模型与贝叶斯优化相结合，本文展示了如何通过在优化任务之间共享信息来减少实验的总数，并且演示了在设计用于扩增基因诊断测定的DNA竞争对手时实验数量的减少。 |
| [^22] | [Real-time Low-latency Music Source Separation using Hybrid Spectrogram-TasNet](https://arxiv.org/abs/2402.17701) | 提出了一种混合谱图时域音频分离网络（HS-TasNet），用于实现实时低延迟音乐源分离，在MusDB测试集上达到了较高的信号-失真比。 |
| [^23] | [RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations](https://arxiv.org/abs/2402.17700) | RAVEL数据集介绍了一种新方法MDAS，该方法在解开语言模型表示方面取得了最新的成果，强调了跨激活特征的重要性。 |
| [^24] | [Gradient-based Discrete Sampling with Automatic Cyclical Scheduling](https://arxiv.org/abs/2402.17699) | 提出了一种使用自动循环调度的基于梯度的离散采样方法，有效应对高度多模态的离散分布，包括循环步长调度、循环平衡调度和自动调整超参数方案。 |
| [^25] | [Learning reduced-order Quadratic-Linear models in Process Engineering using Operator Inference](https://arxiv.org/abs/2402.17698) | 使用算子推断学习的降阶模型在过程工程中建模动态系统，为实现快速可靠的数字孪生架构迈出重要一步。 |
| [^26] | [Geometric Deep Learning for Computer-Aided Design: A Survey](https://arxiv.org/abs/2402.17695) | 几何深度学习技术在计算机辅助设计领域具有变革性力量，可以通过机器学习优化CAD设计师的工作流程，节省时间和精力，提高决策效率，并创造出具有创新性和实用性的设计。 |
| [^27] | [Autonomous Vehicles: Evolution of Artificial Intelligence and Learning Algorithms](https://arxiv.org/abs/2402.17690) | 本文全面探讨了自动驾驶车辆中AI的演进轨迹，从基础原理追溯到最新进展，并阐明了AI在塑造车辆自主决策能力中的基础作用。 |
| [^28] | [QoS prediction in radio vehicular environments via prior user information](https://arxiv.org/abs/2402.17689) | 本文评估了使用先前提取的用户信息对车载无线环境中的QoS进行预测的方法，展示了如何利用ML树集成方法来提高预测性能。 |
| [^29] | [Outlier-Detection for Reactive Machine Learned Potential Energy Surfaces](https://arxiv.org/abs/2402.17686) | 集成模型在检测异常值方面表现最佳，其次是高斯混合模型，研究发现一种结构指标与大误差相关，有助于快速分类新结构。 |
| [^30] | [Securing Reliability: A Brief Overview on Enhancing In-Context Learning for Foundation Models](https://arxiv.org/abs/2402.17671) | 论文回顾了增强基础模型可靠性和可信度的最新进展，着重于四种关键方法论，为构建安全可靠的FMs和促进稳定一致的ICL环境提供了有价值的见解。 |
| [^31] | [Multi-Agent Deep Reinforcement Learning for Distributed Satellite Routing](https://arxiv.org/abs/2402.17666) | 本文介绍了一种多智能体深度强化学习方法，用于低地球轨道卫星星座中的路由，通过离线学习最佳路径，并在在线阶段进行高效的分布式路由。 |
| [^32] | [TorchMD-Net 2.0: Fast Neural Network Potentials for Molecular Simulations](https://arxiv.org/abs/2402.17660) | TorchMD-Net 2.0是在神经网络势模型方面取得的重要进展，通过引入TensorNet等尖端结构，显著提高了计算效率，使得在计算能量和力时获得了2到10倍的性能提升。 |
| [^33] | [Confidence-Aware Multi-Field Model Calibration](https://arxiv.org/abs/2402.17655) | 本研究提出了一种基于置信度的多字段校准方法，通过根据样本统计推导的置信水平自适应调整校准强度，以解决校准过程中存在的偏差放大和在线干扰问题。 |
| [^34] | [Variational Learning is Effective for Large Deep Networks](https://arxiv.org/abs/2402.17641) | 变分学习在大型深度网络中展现出非常好的效果，IVON优化器在训练大型网络时几乎能与Adam相媲美甚至胜过它，且预测不确定性更准确，对模型微调、泛化误差预测和数据敏感性估计均有显著改进。 |
| [^35] | [Supervised machine learning for microbiomics: bridging the gap between current and best practices](https://arxiv.org/abs/2402.17621) | 该研究通过分析大量期刊文章，总结了监督机器学习在微生物组学中的现有实践，探讨了实验设计方法的优缺点，并提出了如何避免常见实验设计缺陷的指导。 |
| [^36] | [Learning Topological Representations with Bidirectional Graph Attention Network for Solving Job Shop Scheduling Problem](https://arxiv.org/abs/2402.17606) | 本文提出了拓扑感知的双向图注意力网络（TBGAT），在解决车间作业调度问题中，通过嵌入并发图并利用双向视图嵌入、图注意力聚合等技术，实现了对拓扑结构的更好建模和利用。 |
| [^37] | [Advancing sleep detection by modelling weak label sets: A novel weakly supervised learning approach](https://arxiv.org/abs/2402.17601) | 提出了一种利用弱监督学习进行睡眠检测的新方法，基于传统睡眠检测算法生成的弱标签集，采用新颖的统计模型来最小化软交叉熵损失和Brier分数作为损失函数。 |
| [^38] | [DAGnosis: Localized Identification of Data Inconsistencies using Structures](https://arxiv.org/abs/2402.17599) | DAGnosis使用有向无环图(DAGs)来解决数据一致性检测中的两个关键限制，并能够准确定位为何样本会被标记为不一致。 |
| [^39] | [Implicit Regularization via Spectral Neural Networks and Non-linear Matrix Sensing](https://arxiv.org/abs/2402.17595) | 本文在更现实的神经网络背景下探讨了隐式正则化现象，通过研究非线性激活函数的一般类别，严格证明了在矩阵感知问题设置中这些网络的隐式正则化现象，同时提供了严格的速率保证，确保梯度的指数级快速收敛。 |
| [^40] | [FaultProfIT: Hierarchical Fault Profiling of Incident Tickets in Large-scale Cloud Systems](https://arxiv.org/abs/2402.17583) | 提出了一种名为FaultProfIT的自动化方法，用于处理大规模云系统中的故障模式分析，填补了手动标记的缺陷。 |
| [^41] | [Hyperdimensional computing: a fast, robust and interpretable paradigm for biological data](https://arxiv.org/abs/2402.17572) | 超维计算作为一种新兴技术，在生物信息学中具有高效、可解释和能够处理多模态数据的潜力。 |
| [^42] | [Sparse Variational Contaminated Noise Gaussian Process Regression for Forecasting Geomagnetic Perturbations](https://arxiv.org/abs/2402.17570) | 本文提出了一种稀疏变分受干扰噪声高斯过程回归框架，用于更好地处理异方差方差和离群噪声，应用于地磁扰动预测，并展示了更短的预测间隔和类似的覆盖精度。 |
| [^43] | [Evaluation of Predictive Reliability to Foster Trust in Artificial Intelligence. A case study in Multiple Sclerosis](https://arxiv.org/abs/2402.17554) | 通过提出一种方法，评估人工智能预测的可靠性，使决策者能够根据其可靠性来接受或拒绝预测结果 |
| [^44] | [Adapting Learned Image Codecs to Screen Content via Adjustable Transformations](https://arxiv.org/abs/2402.17544) | 引入可调变换的学习图像编解码器，通过神经网络前后滤波器提高屏幕内容压缩效率并减少编码伪影，比基线模型节省10%比特率。 |
| [^45] | [Label-Noise Robust Diffusion Models](https://arxiv.org/abs/2402.17517) | Transition-aware weighted Denoising Score Matching（TDSM）是用于训练带有嘈杂标签的条件扩散模型的新方法，通过加权得分网络和过渡概率来提高生成样本质量。 |
| [^46] | [QUCE: The Minimisation and Quantification of Path-Based Uncertainty for Generative Counterfactual Explanations](https://arxiv.org/abs/2402.17516) | QUCE方法旨在通过减少路径不确定性来量化和缓解基于路径的不确定性，从而改善对抗性反事实解释的表现。 |
| [^47] | [Thermodynamics-informed super-resolution of scarce temporal dynamics data](https://arxiv.org/abs/2402.17506) | 提出了一种利用热力学感知神经网络来提高物理系统测量分辨率并预测时间演化的方法，采用对抗自动编码器和结构保持神经网络相结合的方式，可有效解决超分辨率问题，并确保满足热力学定律。 |
| [^48] | [Intensive Care as One Big Sequence Modeling Problem](https://arxiv.org/abs/2402.17501) | 将医疗保健视为序列建模问题，通过将患者与医疗提供者之间的交互表示为事件流，实现对未来事件（如诊断和治疗选择）进行预测。 |
| [^49] | [Predicting Instability in Complex Oscillator Networks: Limitations and Potentials of Network Measures and Machine Learning](https://arxiv.org/abs/2402.17500) | 通过研究发现，对于振荡系统的同步稳定性预测，网络特征无法可靠地进行预测，只有通过结合所有网络特征和节点机器学习才能匹敌图神经网络(GNNs)的性能。 |
| [^50] | [syren-halofit: A fast, interpretable, high-precision formula for the $\Lambda$CDM nonlinear matter power spectrum](https://arxiv.org/abs/2402.17492) | 通过符号回归获得了简单的解析逼近，重新优化了halofit的系数以拟合各种宇宙学和红移范围，利用符号回归探索了用于拟合残差的解析表达式空间，所有方法均经过$N$体模拟验证。 |
| [^51] | [Bit Rate Matching Algorithm Optimization in JPEG-AI Verification Model](https://arxiv.org/abs/2402.17487) | 该研究对JPEG-AI验证模型中的比特率匹配算法进行了优化，以提高其性能和速度。 |
| [^52] | [Fraud Detection with Binding Global and Local Relational Interaction](https://arxiv.org/abs/2402.17472) | 这项工作提出了一个名为RAGFormer的新框架，同时将局部和全局特征嵌入目标节点，以改进欺诈检测性能。 |
| [^53] | [Bit Distribution Study and Implementation of Spatial Quality Map in the JPEG-AI Standardization](https://arxiv.org/abs/2402.17470) | 研究表明，JPEG-AI标准化中的灵活位分布结构可以提高图像压缩性能，并超过经典编解码器VVC intra。 |
| [^54] | [Why do Learning Rates Transfer? Reconciling Optimization and Scaling Limits for Deep Learning](https://arxiv.org/abs/2402.17457) | 学习速率迁移现象可以归因于在μP和其深度延伸下，训练损失Hessian矩阵的最大特征值（即锐度）在较长时间的训练过程中，基本独立于网络的宽度和深度。 |
| [^55] | [DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning](https://arxiv.org/abs/2402.17453) | DS-Agent是一个自动框架，结合了大型语言模型代理和案例推理，能够在数据科学任务中灵活利用专家知识并通过反馈机制持续改善性能 |
| [^56] | [Principled Architecture-aware Scaling of Hyperparameters](https://arxiv.org/abs/2402.17440) | 通过精确定位初始化和最大学习速率对网络结构的依赖性，本文可以将初始化和学习速率推广到MLP和CNN中，以适用于不同的神经网络架构。 |
| [^57] | [The KANDY Benchmark: Incremental Neuro-Symbolic Learning and Reasoning with Kandinsky Patterns](https://arxiv.org/abs/2402.17431) | 本文介绍了KANDY基准框架，通过生成基于坎丁斯基模式的学习和推理任务，提出了持续和半监督学习的新挑战，并着重研究符号组成性。 |
| [^58] | [Reinforced In-Context Black-Box Optimization](https://arxiv.org/abs/2402.17423) | 提出了一种从离线数据中端到端地强化学习黑盒优化算法的方法，通过使用表达能力强的序列模型和后悔-前进令牌来获取任务信息并做出决策。 |
| [^59] | [A novel image space formalism of Fourier domain interpolation neural networks for noise propagation analysis](https://arxiv.org/abs/2402.17410) | 提出了一种用于MRI重建的傅里叶域插值神经网络的图像空间形式主义，并分析了在CNN推断过程中噪声传播的估计方法。 |
| [^60] | [LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning](https://arxiv.org/abs/2402.17406) | LSPT是一种革命性的视觉表示学习方法，通过引入长期门控提示，巧妙地利用长距离先前块作为提示的潜在来源，减轻了遗忘参数的风险。 |
| [^61] | [Beacon, a lightweight deep reinforcement learning benchmark library for flow control](https://arxiv.org/abs/2402.17402) | Beacon是一个开源基准库，用于流控制，包含7个轻量级的1D和2D问题，有助于提高深度强化学习算法对数值流体动力学环境的适应性和可重现性。 |
| [^62] | [A Quantum Approach to Synthetic Minority Oversampling Technique (SMOTE)](https://arxiv.org/abs/2402.17398) | 使用量子计算技术提出了Quantum-SMOTE方法，可以解决机器学习数据集中的类别不平衡问题，并引入了旋转角度、少数类百分比和分裂因子等超参数，实现了对合成数据生成过程的更好控制。 |
| [^63] | [Robustness-Congruent Adversarial Training for Secure Machine Learning Model Updates](https://arxiv.org/abs/2402.17390) | 通过鲁棒一致对抗训练技术，解决了更新机器学习模型时对抗性鲁棒性和系统安全性的问题。 |
| [^64] | [Accelerating Diffusion Sampling with Optimized Time Steps](https://arxiv.org/abs/2402.17376) | 提出了一个通用框架用于设计优化问题，旨在通过寻找更合适的时间步长加速扩散采样。 |
| [^65] | [Impact of Computation in Integral Reinforcement Learning for Continuous-Time Control](https://arxiv.org/abs/2402.17375) | 计算方法选择会显著影响连续时间控制中积分强化学习的性能表现 |
| [^66] | [CGGM: A conditional graph generation model with adaptive sparsity for node anomaly detection in IoT networks](https://arxiv.org/abs/2402.17363) | CGGM是一种新颖的图生成模型，通过自适应稀疏性生成邻接矩阵，解决了物联网网络中节点异常检测中节点类别不平衡的问题 |
| [^67] | [Understanding the training of PINNs for unsteady flow past a plunging foil through the lens of input subdomain level loss function gradients](https://arxiv.org/abs/2402.17346) | 通过分析损失函数梯度统计和样本点比例，研究了PINNs对经济翼非稳定流的训练过程中各个输入空间子域的影响。 |
| [^68] | [LocalGCL: Local-aware Contrastive Learning for Graphs](https://arxiv.org/abs/2402.17345) | 提出了一种名为LocalGCL的新的自监督学习框架，通过掩码建模补充地捕捉局部图信息，优于传统对比学习方法。 |
| [^69] | [Enhanced Bayesian Optimization via Preferential Modeling of Abstract Properties](https://arxiv.org/abs/2402.17343) | 本文提出了一种人工智能协作的贝叶斯框架，通过将专家对未被测量的抽象属性的偏好纳入到替代建模中，进一步提升了贝叶斯优化的性能。 |
| [^70] | [Outdoor Environment Reconstruction with Deep Learning on Radio Propagation Paths](https://arxiv.org/abs/2402.17336) | 本文提出了一种利用环境无线信号进行室外环境重建的新方法，并通过深度学习技术对RF数据进行分析，填补了该领域的研究空白。 |
| [^71] | [Data-Efficient Learning via Clustering-Based Sensitivity Sampling: Foundation Models and Beyond](https://arxiv.org/abs/2402.17327) | 通过基于聚类和敏感度采样的数据选择方法，可以高效选择代表性数据子集来训练机器学习模型，在微调基础模型方面表现优异。 |
| [^72] | [Scaling Supervised Local Learning with Augmented Auxiliary Networks](https://arxiv.org/abs/2402.17318) | 本文提出了一种增强的本地学习方法AugLocal，通过构建辅助网络来增强各隐藏层之间的协同作用，从而解决了本地学习方法在大规模网络中与BP方法之间存在的精度差距问题。 |
| [^73] | [How we won BraTS 2023 Adult Glioma challenge? Just faking it! Enhanced Synthetic Data Augmentation and Model Ensemble for brain tumour segmentation](https://arxiv.org/abs/2402.17317) | 通过使用生成对抗网络和配准来增强合成数据，我们成功训练了三个不同的深度学习模型，结合卷积算法和transformers技术填补了知识差距，取得了0.9005的dice结果。 |
| [^74] | [Quantum Distance Approximation for Persistence Diagrams](https://arxiv.org/abs/2402.17295) | 探索了量子计算机在估计持久图之间距离方面的潜力，提出了用于Wasserstein距离和$d^{c}_{p}$距离的变分量子算法 |
| [^75] | [An Interpretable Evaluation of Entropy-based Novelty of Generative Models](https://arxiv.org/abs/2402.17287) | 提出了一种用于评估生成模型新颖性的基于核的熵新颖性 (KEN) 分数 |
| [^76] | [Multi-Agent, Human-Agent and Beyond: A Survey on Cooperation in Social Dilemmas](https://arxiv.org/abs/2402.17270) | 调查了多智能体、人智能体和人工智能智能体在社会困境合作中的三个关键领域，讨论了合作的动机、策略、人类偏见，以及未来研究方向。 |
| [^77] | [Curriculum Learning Meets Directed Acyclic Graph for Multimodal Emotion Recognition](https://arxiv.org/abs/2402.17269) | 使用有向无环图和课程学习相结合的新方法，提升多模态情感识别模型在处理情感变化和数据不平衡方面的性能。 |
| [^78] | [RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences](https://arxiv.org/abs/2402.17257) | RIME是一种针对嘈杂偏好的健壮PbRL算法，通过动态过滤去噪偏好和热启动奖励模型，极大增强了现有最先进PbRL方法的鲁棒性。 |
| [^79] | [Deep Learning-Based Speech and Vision Synthesis to Improve Phishing Attack Detection through a Multi-layer Adaptive Framework](https://arxiv.org/abs/2402.17249) | 提出了一个结合深度学习和随机森林的自适应框架，通过在多个预测层中读取图像、合成语音以及进行自然语言处理，显著提高了网络钓鱼攻击检测的性能。 |
| [^80] | [SDR-Former: A Siamese Dual-Resolution Transformer for Liver Lesion Classification Using 3D Multi-Phase Imaging](https://arxiv.org/abs/2402.17246) | SDR-Former是一种针对肝脏病变分类而设计的新型Transformer框架，结合了共焦神经网络和混合双分辨率Transformer，能够在3D多相CT和MR成像中处理多相输入，提升特征提取能力。 |
| [^81] | [Does Negative Sampling Matter? A Review with Insights into its Theory and Applications](https://arxiv.org/abs/2402.17238) | 提出了一个利用负采样的通用框架，对负采样的历史进行了深入探讨，将当前的负采样方法分类为静态、困难、基于GAN、基于辅助和批内方法 |
| [^82] | [A Review of Data Mining in Personalized Education: Current Trends and Future Prospects](https://arxiv.org/abs/2402.17236) | 这项研究综述了个性化教育中数据挖掘的最新进展，着重于教育推荐、认知诊断、知识追踪和学习分析四个主要场景，并提出了未来研究方向。 |
| [^83] | [Stochastic Gradient Succeeds for Bandits](https://arxiv.org/abs/2402.17235) | 随机梯度赌博机算法以常数步长收敛到全局最优策略，无需额外的噪声控制，同时自动实现弱探索，确保每个动作被无限次采样。 |
| [^84] | [Hybrid Square Neural ODE Causal Modeling](https://arxiv.org/abs/2402.17233) | 混合模型将基于ODE的机械动力学与神经网络组件结合，在解释性和因果基础的同时，利用领域知识对治疗效果进行排名，从而解决灵活性增加带来的因果基础丢失问题。 |
| [^85] | [Two-scale Neural Networks for Partial Differential Equations with Small Parameters](https://arxiv.org/abs/2402.17232) | 提出了一种用双尺度神经网络方法解决具有小参数的偏微分方程的方法，能够直接将小参数纳入神经网络架构中，从而简化解决过程，并能够合理准确地捕捉由小参数引起的解中大导数特征。 |
| [^86] | [Preserving Fairness Generalization in Deepfake Detection](https://arxiv.org/abs/2402.17229) | 本研究提出了第一种方法来解决深度伪造检测中的公平泛化问题，通过同时考虑特征、损失和优化方面，利用解耦学习来实现。 |
| [^87] | [Efficient Backpropagation with Variance-Controlled Adaptive Sampling](https://arxiv.org/abs/2402.17227) | 引入了方差控制的自适应采样（VCAS）方法，通过在数据维度和标记维度中进行重要性采样，控制采样比率来加速反向传播，并在多个任务中保持准确性。 |
| [^88] | [Temporal Logic Specification-Conditioned Decision Transformer for Offline Safe Reinforcement Learning](https://arxiv.org/abs/2402.17217) | 提出了时间逻辑规范条件化决策转换器（SDT）框架，结合信号时间逻辑（STL）和决策转换器（DT）的能力，比现有方法在离线安全强化学习中学习出更好的安全高奖励策略。 |
| [^89] | [Application of Machine Learning Optimization in Cloud Computing Resource Scheduling and Management](https://arxiv.org/abs/2402.17216) | 本文提出了一种利用机器学习优化技术解决云计算资源调度与管理中复杂问题的创新方法。 |
| [^90] | [Multidimensional unstructured sparse recovery via eigenmatrix](https://arxiv.org/abs/2402.17215) | 该论文提出了一种通过特征矩阵方法来解决多维非结构稀疏恢复问题，并通过数值结果展示了方法的性能。 |
| [^91] | [Measuring Vision-Language STEM Skills of Neural Models](https://arxiv.org/abs/2402.17205) | 该研究引入了一个新挑战，用于测试神经模型的STEM技能，提出了一个包含大量基础技能和问题的数据集，需要理解STEM的多模式视觉语言信息，并展示了最新模型对于低年级技能的有限掌握。 |
| [^92] | [FedBRB: An Effective Solution to the Small-to-Large Scenario in Device-Heterogeneity Federated Learning](https://arxiv.org/abs/2402.17202) | FedBRB方法提出了基于块概念的解决方案，实现了使用小型本地模型训练大型全局模型的所有块，并在设备异构联邦学习中有效应用。 |
| [^93] | [Prediction of the SYM-H Index Using a Bayesian Deep Learning Method with Uncertainty Quantification](https://arxiv.org/abs/2402.17196) | 提出了一种名为SYMHnet的深度学习框架，通过合作学习太阳风和行星间磁场参数的模式，可以短期预测SYM-H指数，并量化预测中的数据不确定性和模型不确定性 |
| [^94] | [When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method](https://arxiv.org/abs/2402.17193) | 研究了不同扩展因素如何影响大型语言模型微调性能，认为LLM微调遵循着一种特殊的扩展行为。 |
| [^95] | [AI-Driven Anonymization: Protecting Personal Data Privacy While Leveraging Machine Learning](https://arxiv.org/abs/2402.17191) | 通过机器学习的差分隐私保护算法，实现个人数据隐私保护和检测。 |
| [^96] | [Inpainting Computational Fluid Dynamics with Deep Learning](https://arxiv.org/abs/2402.17185) | 使用向量量化技术，通过两阶段学习过程将完整和不完整的流体数据空间映射到离散值较低维度的表示。 |
| [^97] | [Dual-Space Optimization: Improved Molecule Sequence Design by Latent Prompt Transformer](https://arxiv.org/abs/2402.17179) | 提出了双空间优化（DSO）方法，通过整合潜在空间采样和数据空间选择，使用Latent Prompt Transformer (LPT)生成模型，解决了分子设计中的关键问题，取得了在不同任务中的性能优势。 |
| [^98] | [Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models](https://arxiv.org/abs/2402.17177) | Sora是一种文本到视频生成的人工智能模型，展示出在模拟物理世界方面的潜力，具有广泛的应用前景和挑战，未来发展具有重要意义。 |
| [^99] | [DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection](https://arxiv.org/abs/2402.17176) | DeepDRK是一种分布无关的深度学习方法，通过引入基于Transformer架构的生成模型以实现“交换属性”，并提出新颖有效的正则化技术，取得了在FDR和能力之间取得平衡。 |
| [^100] | [Generative Learning for Forecasting the Dynamics of Complex Systems](https://arxiv.org/abs/2402.17157) | 生成式学习可以通过学习和演变系统的有效动态加速模拟，为准确预测复杂系统的统计性质提供新的可能性。 |
| [^101] | [TaxDiff: Taxonomic-Guided Diffusion Model for Protein Sequence Generation](https://arxiv.org/abs/2402.17156) | 提出了一种名为TaxDiff的分类引导扩散模型，结合了生物物种信息和扩散模型的生成能力，用于可控生成结构稳定的蛋白质序列。 |
| [^102] | [Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations](https://arxiv.org/abs/2402.17152) | 提出了HSTU架构，用于高基数、非平稳流推荐数据，性能优于基线方法高达65.8％的NDCG，并且比基于FlashAttention2的Transformer在8192长度序列上快5.3倍到15.2倍。 |
| [^103] | [Time series generation for option pricing on quantum computers using tensor network](https://arxiv.org/abs/2402.17148) | 提出了一种使用矩阵乘积态作为时间序列生成的方法，可以有效生成多个时间点处基础资产价格的联合分布的态，并证实了该方法在Heston模型中的可行性。 |
| [^104] | [Energy-Efficient Scheduling with Predictions](https://arxiv.org/abs/2402.17143) | 基于机器学习预测的能效调度算法在能量最小化与截止时间问题上取得了改进的竞争比率。 |
| [^105] | [Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings](https://arxiv.org/abs/2402.17135) | 通过功能奖励编码实现的无监督零样本强化学习方法能够在各种模拟机器人基准测试中训练代理并成功解决新任务，相比以往的零样本强化学习方法表现更优秀。 |
| [^106] | [Predicting O-GlcNAcylation Sites in Mammalian Proteins with Transformers and RNNs Trained with a New Loss Function](https://arxiv.org/abs/2402.17131) | 本研究提出了一种新的损失函数，称为加权焦点可微MCC，用于改善分类模型的性能，并在预测哺乳动物蛋白质中的O-GlcNAcylation位点方面取得了进展 |
| [^107] | [LCEN: A Novel Feature Selection Algorithm for Nonlinear, Interpretable Machine Learning Models](https://arxiv.org/abs/2402.17120) | LCEN算法是一种用于创建非线性、可解释机器学习模型的新型特征选择算法，能够更准确、更稀疏地生成模型，并具有鲁棒性。 |
| [^108] | [Sinkhorn Distance Minimization for Knowledge Distillation](https://arxiv.org/abs/2402.17110) | 提出了Sinkhorn Knowledge Distillation（SinKD）来解决知识蒸馏过程中散度度量存在的问题，确保对教师和学生分布之间差异的准确评估 |
| [^109] | [Repeated Contracting with Multiple Non-Myopic Agents: Policy Regret and Limited Liability](https://arxiv.org/abs/2402.17108) | 该研究揭示了在选择代理人进行合同时产生的博弈，展示了出色的纯策略均衡以及对于任何凹合同的政策后悔优势。 |
| [^110] | [Dataset Fairness: Achievable Fairness on Your Data With Utility Guarantees](https://arxiv.org/abs/2402.17106) | 该论文提出了一种针对数据集特性量身定制的近似公平性-准确性权衡曲线计算方法，能够有效减轻训练多个模型的计算负担并提供了严格的统计保证 |
| [^111] | [Adversarial Perturbations of Physical Signals](https://arxiv.org/abs/2402.17104) | 研究了基于计算机视觉的信号分类器对物理信号的对抗扰动的脆弱性，通过引入PDE约束优化问题构造干扰信号，成功实现对检测器的误分类，对解决这些问题提出了高效方法，实验证明可以计算出各种机器学习模型的对抗扰动。 |
| [^112] | [Learning high-dimensional targets by two-parameter models and gradient flow](https://arxiv.org/abs/2402.17089) | 通过提出两参数模型和梯度流学习高维目标的理论可能性，研究发现在特定条件下存在大量不可学习目标，并且这些目标的集合不密集，具有一定拓扑性质的子集中也存在不可学习目标。最终，发现使用层次过程构建的主要定理模型在数学表达上并非由单一初等函数表示。 |
| [^113] | [A Note on Bayesian Networks with Latent Root Variables](https://arxiv.org/abs/2402.17087) | 从贝叶斯网络计算出的数据集的似然性主要由经验网络的似然性的全局最大值所主导，并且仅当贝叶斯网络的参数与经验模型的参数一致时，这样的最大值才会被实现。 |
| [^114] | [Parallelized Spatiotemporal Binding](https://arxiv.org/abs/2402.17077) | PSB是第一个针对序列输入进行时空并行化的槽学习架构，通过并行处理所有时间步骤中的对象中心表示，利用固定层数和因果关注力，显著提高了效率。 |
| [^115] | [One-Shot Graph Representation Learning Using Hyperdimensional Computing](https://arxiv.org/abs/2402.17073) | 该方法提出了一种使用超高维计算进行单次图表示学习的方法，通过将数据投影到高维空间并利用HD运算符进行信息聚合，实现了与最先进深度学习方法相竞争的预测性能，而无需进行计算昂贵的训练。 |
| [^116] | [Taming the Tail in Class-Conditional GANs: Knowledge Sharing via Unconditional Training at Lower Resolutions](https://arxiv.org/abs/2402.17065) | 通过在较低分辨率进行无条件训练，允许长尾类别从信息更丰富的类别中共享知识，以改善长尾数据下类别条件GANs的训练 |
| [^117] | [A Multi-Fidelity Methodology for Reduced Order Models with High-Dimensional Inputs](https://arxiv.org/abs/2402.17061) | 该研究介绍了一种面向高维设计空间的新型降阶模型多保真度方法，集成了机器学习技术用于流形对齐和维度减少。 |
| [^118] | [An Investigation into the Performances of the State-of-the-art Machine Learning Approaches for Various Cyber-attack Detection: A Survey](https://arxiv.org/abs/2402.17045) | 分析了过去10年针对不同类型网络攻击检测的各种最先进机器学习模型，着重比较了最新的工作。 |
| [^119] | [Towards Generalizing Inferences from Trials to Target Populations](https://arxiv.org/abs/2402.17042) | 本研究在试图解决从试验结果推广到目标种群的外部有效性挑战方面取得了重要进展 |
| [^120] | [Iterated INLA for State and Parameter Estimation in Nonlinear Dynamical Systems](https://arxiv.org/abs/2402.17036) | 提出了一种基于迭代INLA的方法，用于在非线性动力系统中推断状态和参数，能够保留可解释性并且适用于任意非线性系统。 |
| [^121] | [REFACTOR: Learning to Extract Theorems from Proofs](https://arxiv.org/abs/2402.17032) | 提出了一种名为REFACTOR的新方法，用于训练神经网络从证明中提取定理，新定理的引入帮助缩短证明长度并提高证明效率。 |
| [^122] | [A Curious Case of Remarkable Resilience to Gradient Attacks via Fully Convolutional and Differentiable Front End with a Skip Connection](https://arxiv.org/abs/2402.17018) | 通过在神经模型中引入不同iable和完全卷积的前端模型，并结合跳跃连接，成功实现对梯度攻击的显著韧性，并通过将模型组合成随机集合，有效对抗黑盒攻击。 |
| [^123] | [Towards Explainability and Fairness in Swiss Judgement Prediction: Benchmarking on a Multilingual Dataset](https://arxiv.org/abs/2402.17013) | 本研究深入探讨了在瑞士司法预测中实现可解释性和公平性的重要性，利用了唯一可用的多语言LJP数据集，并对最新的单语和多语BERT-based LJP模型进行了可解释性能评估。 |
| [^124] | [Pandora's White-Box: Increased Training Data Leakage in Open LLMs](https://arxiv.org/abs/2402.17012) | 本文对开源大型语言模型（LLMs）进行了隐私攻击研究，提出了首个能同时实现高真正率和低误分类率的预训练LLMs会员推理攻击（MIAs），以及展示了在自然环境中可以从微调LLM中提取超过50%的微调数据集。 |
| [^125] | [Monitoring Fidelity of Online Reinforcement Learning Algorithms in Clinical Trials](https://arxiv.org/abs/2402.17003) | 提出了算法准确性作为在临床试验中部署在线RL算法的关键要求，强调了对参与者保护和数据科学效用的保留责任，并提出了一个框架进行预部署规划和实时监测以确保算法准确性。 |
| [^126] | [Discovering Symmetry Group Structures via Implicit Orthogonality Bias](https://arxiv.org/abs/2402.17002) | HyperCube网络通过独特的因式分解架构和正则化器，成功学习了对称群的操作，能够高效地恢复完整操作表，并形成广义傅里叶基进行群卷积。 |
| [^127] | [What Do Language Models Hear? Probing for Auditory Representations in Language Models](https://arxiv.org/abs/2402.16998) | 通过训练一个线性探针，将语言模型中的文本表示和预训练音频模型中的声音表示联系在一起，研究发现尽管仅在原始文本上进行训练，语言模型对于一些对象的声音知识有着基于实质的编码。 |
| [^128] | [Towards Decoding Brain Activity During Passive Listening of Speech](https://arxiv.org/abs/2402.16996) | 本研究通过深度学习方法从颅内脑电图数据解码被动听取的言语，旨在推动脑机接口技术的言语合成应用，并提供对言语知觉认知过程的额外视角。 |
| [^129] | [GEM3D: GEnerative Medial Abstractions for 3D Shape Synthesis](https://arxiv.org/abs/2402.16994) | GEM3D提出了一种新的深度、拓扑感知的三维形状生成模型，通过神经骨架编码了拓扑和几何信息，通过骨架驱动的神经隐式公式生成准确和多样化的表面。 |
| [^130] | [A Phase Transition in Diffusion Models Reveals the Hierarchical Nature of Data](https://arxiv.org/abs/2402.16991) | 扩散模型在研究数据的分层生成模型中展示出了在阈值时间发生相变的特性，这影响了高级特征和低级特征的重建过程。 |
| [^131] | [inGRASS: Incremental Graph Spectral Sparsification via Low-Resistance-Diameter Decomposition](https://arxiv.org/abs/2402.16990) | inGRASS提出了一种用于大型无向图增量谱稀疏化的新算法，其具有高度可扩展性和并行友好性，关键创新在于低阻抗直径分解方案，能够高效识别关键边和检测多余边。 |
| [^132] | [Algorithmic Arbitrariness in Content Moderation](https://arxiv.org/abs/2402.16979) | 机器学习在内容审核中引入了预测多样性的挑战，可能导致内容被武断分类并限制言论自由。 |
| [^133] | [An inexact Bregman proximal point method and its acceleration version for unbalanced optimal transport](https://arxiv.org/abs/2402.16978) | 该论文提出了一种不精确的Bregman近端点方法，用于解决不平衡最优输运问题，能够在保持算法收敛性和稳定性的同时，对于大正则化参数和小正则化参数均有较好的表现。 |
| [^134] | [Disentangled 3D Scene Generation with Layout Learning](https://arxiv.org/abs/2402.16936) | 该方法通过布局学习实现了将三维场景分解为各个单独对象，从而在文本到三维内容创建方面带来了新的功能。 |
| [^135] | [FedReview: A Review Mechanism for Rejecting Poisoned Updates in Federated Learning](https://arxiv.org/abs/2402.16934) | 提出了FedReview机制，通过随机分配评审员客户端来识别和拒绝联邦学习中的潜在毒化更新，并采用多数表决机制来整合排名并移除这些更新。 |
| [^136] | [Avoiding Catastrophic Forgetting in Visual Classification Using Human Concept Formation](https://arxiv.org/abs/2402.16933) | 提出了一种名为Cobweb4V的新颖视觉分类方法，利用人类类似学习系统，避免了灾难性遗忘效应，与传统方法相比，需要更少的数据来实现有效学习成果，并保持稳定性能。 |
| [^137] | [TrustMol: Trustworthy Inverse Molecular Design via Alignment with Molecular Dynamics](https://arxiv.org/abs/2402.16930) | TrustMol提出了一种值得信赖的逆向分子设计方法，利用新的变分自动编码器网络和潜在属性对获取方法来有效导航分子的潜在优化复杂性。 |
| [^138] | [On the (In)feasibility of ML Backdoor Detection as an Hypothesis Testing Problem](https://arxiv.org/abs/2402.16926) | 提出了机器学习系统中后门检测问题的正式统计定义，并得出了后门检测的不可能性与可实现性结果，指出了通用后门检测的局限性，强调后门检测方法需要考虑敌对因素。 |
| [^139] | [Minimize Control Inputs for Strong Structural Controllability Using Reinforcement Learning with Graph Neural Network](https://arxiv.org/abs/2402.16925) | 使用带有图神经网络的强化学习方法，将图着色问题形式化为马尔可夫决策过程，有效解决了强结构可控性条件下最小化控制输入的问题。 |
| [^140] | [Personalized Federated Instruction Tuning via Neural Architecture Search](https://arxiv.org/abs/2402.16919) | 提出了一种基于体系结构搜索的新型个性化联邦指令调优（PerFIT）框架，允许每个客户端通过扩展全局模型的可训练参数空间来搜索个性化体系结构，解决了FIT面临的数据和资源异质性问题。 |
| [^141] | [m2mKD: Module-to-Module Knowledge Distillation for Modular Transformers](https://arxiv.org/abs/2402.16918) | 提出了用于在模块之间传递知识的通用模块到模块知识蒸馏（m2mKD）方法，解决了模块化Transformer训练中的优化困难和参数数量庞大等挑战。 |
| [^142] | [More Than Routing: Joint GPS and Route Modeling for Refine Trajectory Representation Learning](https://arxiv.org/abs/2402.16915) | 提出了一种新颖的联合GPS和路由建模的表示学习框架，通过自监督技术实现，利用两个编码器分别捕获路由和GPS轨迹的表示，并通过共享的变压器进行模态间信息交互。 |
| [^143] | [PDETime: Rethinking Long-Term Multivariate Time Series Forecasting from the perspective of partial differential equations](https://arxiv.org/abs/2402.16913) | 本文提出了PDETime，一种受神经PDE解算器原则启发的新型LMTF模型，从偏微分方程的视角重新思考了长期多元时间序列预测。 |
| [^144] | [An Adversarial Robustness Benchmark for Enterprise Network Intrusion Detection](https://arxiv.org/abs/2402.16912) | 本文提出了一种对抗鲁棒性基准，评估了多个决策树集成模型在企业网络入侵检测中的表现，发现新的数据集NewCICIDS可以提高模型性能，但对于最新的网络攻击，RF和LGBM的鲁棒性较差。 |
| [^145] | [Trustworthy Personalized Bayesian Federated Learning via Posterior Fine-Tune](https://arxiv.org/abs/2402.16911) | 本研究针对联邦学习面临的数据异质性和低输出可解释性挑战，提出了一个新颖的个性化联邦学习框架，融合了贝叶斯方法和正则化流，在参数后验角度实现个性化，提高了算法对不确定性的量化能力，并在理论上分析了对贝叶斯神经网络的超分布检测影响。 |
| [^146] | [Impact of Physical Activity on Quality of Life During Pregnancy: A Causal ML Approach](https://arxiv.org/abs/2402.16909) | 体育活动对怀孕期间生活质量的影响是一个重要的研究领域，现有研究方法存在偏差，本研究提出了使用因果机器学习方法来考察此关系。 |
| [^147] | [Local stochastic computing using memristor-enabled stochastic logics](https://arxiv.org/abs/2402.16908) | 利用忆阻器开发随机逻辑，实现了具有良好调节概率和相关性的随机数字编码和处理，并设计了一种紧凑的随机Roberts交叉算子用于边缘检测。 |
| [^148] | [Diffusion Posterior Proximal Sampling for Image Restoration](https://arxiv.org/abs/2402.16907) | 本文提出了一种改进的基于扩散的图像恢复范式，通过选择与测量标识一致的样本，以及从与测量信号相结合的初始化开始恢复过程，实现输出稳定性和增强。 |
| [^149] | [Enforcing Temporal Constraints on Generative Agent Behavior with Reactive Synthesis](https://arxiv.org/abs/2402.16905) | 提出了一种利用形式逻辑为基础的程序合成和LLM内容生成相结合的方法，通过使用时间流逻辑（TSL）对生成式代理施加时间约束，从而提高了代理行为的保证水平、系统的解释性和代理的模块化构建能力。 |
| [^150] | [Selective Task offloading for Maximum Inference Accuracy and Energy efficient Real-Time IoT Sensing Systems](https://arxiv.org/abs/2402.16904) | 本论文提出了一个轻量级混合遗传算法（LGSTO），用于解决面向实时物联网传感系统的选择性任务卸载问题，以在时间和能量约束下最大化推理准确性。 |
| [^151] | [A novel data generation scheme for surrogate modelling with deep operator networks](https://arxiv.org/abs/2402.16903) | 提出了一种新的数据生成方法，用于减轻深度操作网络（DeepONets）训练数据生成的计算负担，避免了使用偏微分方程积分策略，显著降低了生成训练数据集的计算成本 |
| [^152] | [PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA](https://arxiv.org/abs/2402.16902) | PRoLoRA是一个新的部分旋转增强低秩适应方法，通过引入广播减少、旋转增强、部分共享细化和修正初始化策略等四个组件，实现了对LoRA的优势提升，避免了其他参数共享方法的缺点，具有更高的参数效率和可扩展性。 |
| [^153] | [FGBERT: Function-Driven Pre-trained Gene Language Model for Metagenomics](https://arxiv.org/abs/2402.16901) | 该论文提出了基于蛋白质的基因表示作为一种上下文感知和结构相关的标记器，通过Masked Gene Modeling（MGM）和Triple Enhanced Metagenomic Contrastive Learning（TEM-CL）进行预训练，构建了一个新颖的宏基因组语言模型FGBERT，能够更好地捕捉基因序列与功能之间的复杂关系。 |
| [^154] | [A prior Estimates for Deep Residual Network in Continuous-time Reinforcement Learning](https://arxiv.org/abs/2402.16899) | 本研究针对连续时间控制问题，提出了一种可以直接分析Bellman最优损失\emph{先验}泛化误差的方法，避免了有界性假设，并通过最大算子的分解方法实现了损失函数的转换。 |
| [^155] | [MIM-Reasoner: Learning with Theoretical Guarantees for Multiplex Influence Maximization](https://arxiv.org/abs/2402.16898) | 引入了MIM-Reasoner，结合强化学习和概率图模型，有效地捕捉了给定多重网络内部和层间的复杂传播过程，从而解决了MIM中最具挑战性的问题。 |
| [^156] | [Reliable Conflictive Multi-View Learning](https://arxiv.org/abs/2402.16897) | 提出了可靠的冲突多视角学习（RCML）问题，开发了一种Evidential Conflictive Multi-view Learning (ECML)方法来处理具有冲突信息的多视角数据。 |
| [^157] | [On Trojan Signatures in Large Language Models of Code](https://arxiv.org/abs/2402.16896) | 本文研究了在大型代码语言模型中木马签名的问题，并发现木马签名无法推广到代码语言模型，具有重要的研究意义。 |
| [^158] | [Multi-Task Learning for Routing Problem with Cross-Problem Zero-Shot Generalization](https://arxiv.org/abs/2402.16891) | 本研究首次尝试解决跨问题泛化的关键挑战，通过将VRPs定义为共享基础属性的不同组合，并通过属性组合同时解决它们，实现了零样本泛化的路径问题解决方法。 |
| [^159] | [Generative Models are Self-Watermarked: Declaring Model Authentication through Re-Generation](https://arxiv.org/abs/2402.16889) | 该论文提出了一种通过再生成识别模型数据所有权的方法，避免了传统数字水印技术可能破坏输出质量的问题。 |
| [^160] | [Chaotic attractor reconstruction using small reservoirs - the influence of topology](https://arxiv.org/abs/2402.16888) | 重建混沌吸引子时，非耦合节点的水库比复杂水库更可靠产生长期时间序列预测，且较小的谱半径有助于改进替代系统的吸引子重建性能。 |
| [^161] | [Artificial Intelligence for Complex Network: Potential, Methodology and Application](https://arxiv.org/abs/2402.16887) | 人工智能技术与丰富真实网络数据的存在开启了复杂网络科学研究的新时代，有望克服现存挑战。 |
| [^162] | [Using text embedding models and vector databases as text classifiers with the example of medical data](https://arxiv.org/abs/2402.16886) | 向量数据库和嵌入模型的应用为文本分类器提供了强大的方式来表达数据模式，特别是在医疗领域中开始有着广泛的应用。 |
| [^163] | [Substrate Scope Contrastive Learning: Repurposing Human Bias to Learn Atomic Representations](https://arxiv.org/abs/2402.16882) | 提出了一种新颖的预训练策略，底物范围对比学习，以学习适合化学反应性的原子表示。 |
| [^164] | [BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation](https://arxiv.org/abs/2402.16880) | 该论文提出了一种名为BESA的新型大型语言模型修剪技术，通过应用分块重构损失，与传统的逐层修剪技术不同，BESA具有优势 |
| [^165] | [EvoGPT-f: An Evolutionary GPT Framework for Benchmarking Formal Math Languages](https://arxiv.org/abs/2402.16878) | EvoGPT-f是一个新颖的进化框架，用于对五个形式数学语料库进行差异机器可学习性的系统量化分析，为形式数学语言的基准测试提供了新的方法。 |
| [^166] | [Large Language Model Augmented Exercise Retrieval for Personalized Language Learning](https://arxiv.org/abs/2402.16877) | 大型语言模型利用生成能力来合成假设练习，以弥合学习者需求与练习内容之间的语义鸿沟，提高个性化语言学习练习检索效果。 |
| [^167] | [Improve Robustness of Eye Disease Detection by including Learnable Probabilistic Discrete Latent Variables into Machine Learning Models](https://arxiv.org/abs/2402.16865) | 通过引入可学习的概率离散潜变量，该研究提出了一种新颖的眼部疾病检测方法，利用生成流网络来学习眼底图像中眼部疾病的后验分布，提高了鲁棒性和泛化能力。 |
| [^168] | [Pragmatic Goal-Oriented Communications under Semantic-Effectiveness Channel Errors](https://arxiv.org/abs/2402.16858) | 在AI辅助的6G网络中，实现了语义、实用和目标导向通信策略的整合，通过提出数学模型解决了语言不匹配导致的出错问题 |
| [^169] | [Attention Based Molecule Generation via Hierarchical Variational Autoencoder](https://arxiv.org/abs/2402.16854) | 通过将递归神经网络与卷积网络分层结合，实现了高效的分子生成模型，能够在重建已知分子时获得95%的有效性，同时能够实现在SMILES字符串和其学习表示之间的映射。 |
| [^170] | [Asymmetry in Low-Rank Adapters of Foundation Models](https://arxiv.org/abs/2402.16842) | 本文研究发现在预训练模型的参数微调过程中存在着低秩适配器矩阵重要性的不对称性，特别是在更新参数矩阵时，$B$和$A$矩阵具有不同功能，微调$B$比微调$A$更加有效。 |
| [^171] | [Language Agents as Optimizable Graphs](https://arxiv.org/abs/2402.16823) | 将基于LLM的代理统一描述为计算图，提出新颖的自动图优化器来改进节点和边，实现了代理之间的自动协作和改进。 |
| [^172] | [Nemotron-4 15B Technical Report](https://arxiv.org/abs/2402.16819) | Nemotron-4 15B是一个150亿参数的大型多语言模型，在多语言能力上表现优异，超过其他规模相似的模型。 |
| [^173] | [Interpreting Grokked Transformers in Complex Modular Arithmetic](https://arxiv.org/abs/2402.16726) | 本研究通过可解释的逆向工程在复杂模块化算术中观察了Transformer内部电路学习过程，并发现减法在Transformer上造成了强烈的不对称性，乘法需要余弦偏置分量，多项式叠加了基本算术模式，但在挑战性情况下并不清晰，Grokking甚至可以在具有基本对称和交替表达式的高次公式中轻松发生。 |
| [^174] | [Pretrained Visual Uncertainties](https://arxiv.org/abs/2402.16569) | 这项工作引入了第一个针对视觉模型的预训练不确定性模块，实现了不确定性的零-shot转移并加速了训练，能泛化到未知数据集，使得安全检索和对不确定性敏感的数据集可视化成为可能。 |
| [^175] | [Feedback Efficient Online Fine-Tuning of Diffusion Models](https://arxiv.org/abs/2402.16359) | 提出了一种反馈高效的在线微调扩散模型的强化学习程序 |
| [^176] | [CoDream: Exchanging dreams instead of models for federated aggregation with heterogeneous models](https://arxiv.org/abs/2402.15968) | CoDream提出了一种通过在输入数据空间中协作优化数据来交换知识的框架，实现了模型之间的合作学习，实现了模型架构无关、通信不受模型大小影响、兼容安全聚合的优点。 |
| [^177] | [On the dynamics of three-layer neural networks: initial condensation](https://arxiv.org/abs/2402.15958) | 深入研究三层神经网络训练中的凝聚现象和梯度下降方法自发减少神经网络复杂性的机制，提出有效动力学的爆炸性质和凝聚发生的充分条件，并通过实验证实了这些发现。 |
| [^178] | [GraphEdit: Large Language Models for Graph Structure Learning](https://arxiv.org/abs/2402.15183) | 本研究提出了一种名为GraphEdit的方法，利用大型语言模型（LLMs）学习复杂的图结构化数据中的节点关系，通过在图结构上进行指导调整，增强LLMs的推理能力，从而提高图结构学习的可靠性。 |
| [^179] | [Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement](https://arxiv.org/abs/2402.15180) | 提出了一种通过自我完善和格式化改进LMs对抗越狱攻击的方法，即使在非安全对齐的LMs中也具有出色的安全性，同时降低攻击成功率。 |
| [^180] | [Machine Unlearning of Pre-trained Large Language Models](https://arxiv.org/abs/2402.15159) | 本研究在大型语言模型（LLMs）中探讨了“被遗忘权”的概念，提出机器遗忘作为解决方案，并在预训练模型中建立了全面的遗忘框架及高效的遗忘方法，同时提供了改进超参数鲁棒性以及高效调整超参数的指南。 |
| [^181] | [COPR: Continual Human Preference Learning via Optimal Policy Regularization](https://arxiv.org/abs/2402.14228) | 提出了Continual Optimal Policy Regularization (COPR) 方法，通过借鉴最优策略理论，利用采样分布作为示范和正则化约束，以动态地对当前策略进行正则化，从而使强化学习从人类反馈中学习在持续学习情境下更加稳健 |
| [^182] | [Wisdom of Committee: Distilling from Foundation Model to SpecializedApplication Model](https://arxiv.org/abs/2402.14035) | 将基础模型的知识转移到专用应用模型中存在挑战，提出了通过创建教学委员会来应对这些挑战。 |
| [^183] | [AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement Learning](https://arxiv.org/abs/2402.13946) | 这项工作提出了AttackGNN，针对硬件安全中使用的基于GNN的技术进行了首次红队攻击，通过设计新颖的强化学习代理生成对抗性示例电路。 |
| [^184] | [ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models](https://arxiv.org/abs/2402.13516) | 本文介绍了一种名为"ProSparse"的有效稀疏化方法，以推动大型语言模型实现更高的激活稀疏性而不降低模型性能 |
| [^185] | [Federated Causal Discovery from Heterogeneous Data](https://arxiv.org/abs/2402.13241) | 该研究提出了一种新型联邦因果发现方法，旨在适应任意因果模型和异构数据，通过使用替代变量和联邦条件独立性检验来解决数据异质性，并建立了联邦独立变化原则用于确定因果方向。 |
| [^186] | [SMORE: Similarity-based Hyperdimensional Domain Adaptation for Multi-Sensor Time Series Classification](https://arxiv.org/abs/2402.13233) | 提出了SMORE，一种新颖的多传感器时间序列分类领域自适应算法，利用高维计算的高效和并行操作，动态定制测试模型以减轻数据分布偏移带来的性能下降。 |
| [^187] | [ISCUTE: Instance Segmentation of Cables Using Text Embedding](https://arxiv.org/abs/2402.11996) | 提出了一种基于文本提示的DLO实例分割技术，结合了CLIPSeg模型的文本条件语义分割能力和Segment Anything Model的零样本泛化能力，有效解决了传统方法在感知可变形直线对象如电线、电缆和柔性管道方面的挑战，性能超越了目前的技术水平，同时引入了一个新的DLO特定数据集。 |
| [^188] | [Generative Kaleidoscopic Networks](https://arxiv.org/abs/2402.11793) | 发现深层ReLU网络表现出过度泛化现象，利用这一特性设计了“生成万花筒网络”，通过递归映射随机输入噪声生成样本。 |
| [^189] | [PolypNextLSTM: A lightweight and fast polyp video segmentation network using ConvNext and ConvLSTM](https://arxiv.org/abs/2402.11585) | PolypNextLSTM是一个轻量且快速的息肉视频分割网络，使用ConvNext和ConvLSTM，最大的创新在于参数最少且速度最快，性能超越了五种先进的基于图像和视频的深度学习模型。 |
| [^190] | [Advancing Translation Preference Modeling with RLHF: A Step Towards Cost-Effective Solution](https://arxiv.org/abs/2402.11525) | 提出了一种利用强化学习和人类反馈（RLHF）来改进翻译质量的成本效益偏好学习策略，该策略通过优化奖励模型来区分人类和机器翻译，从而指导改进机器翻译。 |
| [^191] | [Best of Three Worlds: Adaptive Experimentation for Digital Marketing in Practice](https://arxiv.org/abs/2402.10870) | 本文分享了关于在工业环境中使用AED系统面临的挑战，并提供了在这种环境中适当的目标和系统规格的视角，最终开发了一个基于反事实推断的AED框架并在商业环境中进行了测试。 |
| [^192] | [Simple, unified analysis of Johnson-Lindenstrauss with applications](https://arxiv.org/abs/2402.10232) | 这项工作提出了Johnson-Lindenstrauss（JL）引理的简单统一分析，简化和统一了各种构造，包括球形、高斯、二进制硬币和次高斯模型，通过创新性地将Hanson-Wright不等式拓展到高维度，标志着对数据固有几何的保持取得重大进展。 |
| [^193] | [A Systematic Review of Data-to-Text NLG](https://arxiv.org/abs/2402.08496) | 这篇系统性回顾全面分析了数据到文本自然语言生成研究的现状，提出未来方向，并解决了相关挑战。 |
| [^194] | [On Limitations of the Transformer Architecture](https://arxiv.org/abs/2402.08164) | 本论文通过通信复杂性证明了Transformer层在处理函数组合任务时的局限性，指出对于大型定义域和某些数学任务，Transformers可能无法解决。 |
| [^195] | [TriAug: Out-of-Distribution Detection for Robust Classification of Imbalanced Breast Lesion in Ultrasound](https://arxiv.org/abs/2402.07452) | TriAug是一个用于乳腺超声图像的异常样本检测框架，通过使用三元状态增强和平衡的球形损失来提高示踪分类的准确性和异常样本检测性能。 |
| [^196] | [HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal](https://arxiv.org/abs/2402.04249) | HarmBench是一个为自动红队和强大拒绝设计的标准化评估框架，通过对18种红队方法和33个目标LLM和防御的比较，得出了新的见解，并介绍了一种高效的对抗训练方法，提高了LLM在各种攻击下的稳健性。 |
| [^197] | [Distinguishing the Knowable from the Unknowable with Language Models](https://arxiv.org/abs/2402.03563) | 通过研究大型语言模型，在自由文本中识别作为代理的模型和冻结预训练模型的嵌入的小型线性探测器可以准确预测更大模型令牌级别上的自信度，进一步提出了一种无监督的方法在相同任务上达到了非平凡的准确度，这证明了语言模型中存在不同类型的不确定性表示。 |
| [^198] | [GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models](https://arxiv.org/abs/2402.03299) | 本论文提出了一个通过角色扮演的系统，可以生成自然语言越狱，用于测试大型语言模型的指南遵循情况。系统通过收集现有越狱并将其组织成知识图来生成新的越狱，证明了其高效性和有效性。 |
| [^199] | [Killer Apps: Low-Speed, Large-Scale AI Weapons](https://arxiv.org/abs/2402.01663) | 本文研究了AI武器的概念、部署、检测和潜在对策，强调了在信息领域内基于AI的心理操纵的潜力，以及其对全球个人、组织和社会的威胁。 |
| [^200] | [Score-based Causal Representation Learning: Linear and General Transformations](https://arxiv.org/abs/2402.00849) | 这篇论文提出了一种基于得分的算法类，用于干预范围内的因果表示学习，涵盖了线性和一般转化。算法保证了可识别性和实现性，并且通过创造性地将得分函数与因果表示学习相结合。 |
| [^201] | [Anatomy of Neural Language Models](https://arxiv.org/abs/2401.03797) | 本教程详细、简化和清晰地解释了神经语言模型，并提供清晰的图形说明，填补了缺乏统一数学框架的现存问题。 |
| [^202] | [Cascade Speculative Drafting for Even Faster LLM Inference](https://arxiv.org/abs/2312.11462) | 引入了Cascade Speculative Drafting（CS Drafting）算法，通过垂直级联消除神经模型的自回归生成，通过水平级联优化草稿中的时间分配，从而进一步提高LLM推理效率。 |
| [^203] | [Causal Fairness under Unobserved Confounding: A Neural Sensitivity Framework](https://arxiv.org/abs/2311.18460) | 分析了因果公平性对未观察到混杂的敏感性，推导出因果公平性指标的界限，提出神经框架用于学习公平预测，展示了框架的有效性 |
| [^204] | [TEA: Test-time Energy Adaptation](https://arxiv.org/abs/2311.14402) | 提出了一种基于能量的测试时间适应（TEA）方法，通过转换训练后的分类器为基于能量的模型，增强模型对目标数据分布的感知，从而解决协变量转移问题。 |
| [^205] | [Reinforcement Learning with Maskable Stock Representation for Portfolio Management in Customizable Stock Pools](https://arxiv.org/abs/2311.10801) | 使用EarnMore方法，我们提出了一种新的RL方法，可以允许RL代理与可定制股票池（CSPs）交互，而不需要重新训练。 |
| [^206] | [Uncertainty estimation in satellite precipitation interpolation with machine learning](https://arxiv.org/abs/2311.07511) | 该研究使用机器学习算法对卫星和测站数据进行插值，通过量化预测不确定性来提高降水数据集的分辨率。 |
| [^207] | [Pyramidal Hidden Markov Model For Multivariate Time Series Forecasting](https://arxiv.org/abs/2310.14341) | 本论文提出了一种金字塔形隐藏马尔可夫模型（PHMM），能够捕捉多个多步随机状态，并提出一种新颖的时间序列预测结构，在性能上具有重要影响。 |
| [^208] | [Multimodal Federated Learning in Healthcare: a Review](https://arxiv.org/abs/2310.09650) | 医疗保健领域引入了多模态联邦学习，结合最新的机器学习进展，确保了患者数据隐私和安全，为优化医疗AI系统提供了新的可能性。 |
| [^209] | [Ask Again, Then Fail: Large Language Models' Vacillations in Judgement](https://arxiv.org/abs/2310.02174) | 目前的语言模型在面对后续问题时常常摇摆不定，研究者提出了一个后续问题机制和两个度量标准来量化这种不一致性，并开发出Unwavering-FQ框架来教导模型保持最初的正确判断，实验证明其有效性。 |
| [^210] | [Pay Attention to the Atlas: Atlas-Guided Test-Time Adaptation Method for Robust 3D Medical Image Segmentation](https://arxiv.org/abs/2307.00676) | 提出了一种Atlas引导的测试时适应方法，用于解决医学图像分割中的数据分布不同问题 |
| [^211] | [Efficient Contextformer: Spatio-Channel Window Attention for Fast Context Modeling in Learned Image Compression](https://arxiv.org/abs/2306.14287) | 提出了高效Contextformer，采用了时空通道窗口注意力机制，用于学习图像压缩中快速上下文建模，并引入了优化技术来降低计算成本 |
| [^212] | [Masking Augmentation for Supervised Learning](https://arxiv.org/abs/2306.11339) | 提出了一种名为MaskSub的新方法，通过使用遮罩子模型和放松的损失函数来强化监督学习中的遮罩增强，提高了性能并加速训练过程。 |
| [^213] | [OCAtari: Object-Centric Atari 2600 Reinforcement Learning Environments](https://arxiv.org/abs/2306.08649) | 以对象为中心的Atari 2600强化学习环境OCAtari扩展了Atari Learning Environments框架，实现了对游戏中基于对象的状态进行资源高效提取，并允许对象发现、对象表征学习以及对象为中心的强化学习。 |
| [^214] | [Exploring the Promise and Limits of Real-Time Recurrent Learning](https://arxiv.org/abs/2305.19044) | 实时递归学习（RTRL）具有一定概念优势，不需要缓存过去的激活状态和截断上下文，支持在线学习，在演员-评论家方法中探索了其实际潜力，并在DMLab-30、ProcGen和Atari-2600环境中进行了测试，在DMLab存储任务中表现出与优于IMPALA和R2D2基线相媲美的竞争力，为了应对复杂任务，研究重点放在了某些方面 |
| [^215] | [Deep Augmentation: Self-Supervised Learning with Transformations in Activation Space](https://arxiv.org/abs/2303.14537) | 深度增强是一种利用dropout或PCA在神经网络中转换目标层的方法，有效改善性能和泛化能力。在对比学习任务中，在Transformers、ResNets和图神经网络等基础模型上，通过深度增强实现了显著的性能提升，但在监督问题上效果相反。 |
| [^216] | [Communication-Efficient Federated Bilevel Optimization with Local and Global Lower Level Problems](https://arxiv.org/abs/2302.06701) | 本文研究了联邦学习设置中的双层优化问题，并提出了一种通信高效的算法FedBiOAcc，实现了优秀的通信和样本复杂度，并且实现了与客户端数量线性加速。 |
| [^217] | [Effects of noise on the overparametrization of quantum neural networks](https://arxiv.org/abs/2302.05059) | 噪音如何影响量子神经网络过度参数化的现象。 |
| [^218] | [Integrating Multimodal Data for Joint Generative Modeling of Complex Dynamics](https://arxiv.org/abs/2212.07892) | 提出了一种基于多模态变分自动编码器的算法框架，用于联合生成建模复杂动力学，引导重建模型训练。 |
| [^219] | [Cross-lingual Text-To-Speech with Flow-based Voice Conversion for Improved Pronunciation](https://arxiv.org/abs/2210.17264) | 通过基于流的语音转换实现了跨语言文本到语音，提升了发音质量，并在客观和主观评估中表现出优势。 |
| [^220] | [All the Feels: A dexterous hand with large-area tactile sensing](https://arxiv.org/abs/2210.15658) | 该研究引入了一种廉价、模块化、稳健且可扩展的DManus平台，解决了机器人中灵巧手的高成本、可靠性问题，同时提供大规模数据采集，其带有覆盖整个手掌和指尖表面的ReSkin感应器。 |
| [^221] | [Tree-Guided Rare Feature Selection and Logic Aggregation with Electronic Health Records Data](https://arxiv.org/abs/2206.09107) | 提出了一种用于大规模回归的树导向特征选择和逻辑聚合方法，旨在改善基于电子健康记录的建模和利用疾病分类的自然分层结构 |
| [^222] | [Goal-Space Planning with Subgoal Models](https://arxiv.org/abs/2206.02902) | 通过在一组（抽象的）子目标上进行约束和学习本地、子目标条件的模型，本文提出的目标空间规划（GSP）方法更具计算效率，自然地结合了时间抽象，避免了学习转换动力学。 |
| [^223] | [Snapture -- A Novel Neural Architecture for Combined Static and Dynamic Hand Gesture Recognition](https://arxiv.org/abs/2205.15862) | 提出了一种新型混合手势识别系统，能够学习静态和动态手势，并通过捕捉手势表现高峰的“快照”来融合静态姿势和动态运动。 |
| [^224] | [A Robust Cybersecurity Topic Classification Tool](https://arxiv.org/abs/2109.02473) | 通过使用多数投票的方式，研究提出了一种网络安全主题分类工具，相比于21个单独模型，该工具在检测网络安全相关文本时表现出较低的假阳性和假阴性率，并且能够处理数十万份文档。 |
| [^225] | [OneLog: Towards End-to-End Training in Software Log Anomaly Detection](https://arxiv.org/abs/2104.07324) | OneLog提出了一种利用单个深度神经网络进行端到端训练的方法，通过利用字符级别的卷积神经网络进行软件日志异常检测，在多个数据集上取得了最先进的性能。 |
| [^226] | [DoubleML -- An Object-Oriented Implementation of Double Machine Learning in R](https://arxiv.org/abs/2103.09603) | DoubleML是在R中实现的双重机器学习框架，提供了估计因果模型参数的功能，包括在部分线性和交互回归模型中进行推断。对象导向的实现使得模型规范具有很高的灵活性并易于扩展。 |
| [^227] | [Automated Machine Learning: From Principles to Practices](https://arxiv.org/abs/1810.13306) | 自动化机器学习（AutoML）旨在以数据驱动的方式生成令人满意的ML配置，本文提供了对AutoML原理和实践的全面调研。 |
| [^228] | [Optimal Sparse Survival Trees.](http://arxiv.org/abs/2401.15330) | 本研究提出了一种动态规划边界法，可以在几秒钟内找到可证明最优稀疏生存树模型，对于涉及人类健康的高风险问题的分析和决策具有重要意义。 |
| [^229] | [Ricci flow-guided autoencoders in learning time-dependent dynamics.](http://arxiv.org/abs/2401.14591) | 利用Ricci流引导的自编码器方法能够学习非线性动力学，尤其是偏微分方程。该方法通过在训练中学习流形，并使用Ricci流使流形潜空间逐步适应动力学的变化，从而获得更好的表示能力。在实验中，我们展示了该方法在具有周期性和随机性的PDE上的应用，并评估了在分布内和外推场景中的误差。 |
| [^230] | [How Good is ChatGPT at Face Biometrics? A First Look into Recognition, Soft Biometrics, and Explainability.](http://arxiv.org/abs/2401.13641) | 本研究初步探索了基于GPT-4的ChatGPT在面部生物识别中的表现。研究分析了ChatGPT在面部验证、软生物特征估计和结果可解释性方面的能力。ChatGPT的应用有望提高自动决策在人类场景中的解释性和透明度。 |
| [^231] | [How Far Can 100 Samples Go? Unlocking Overall Zero-Shot Multilingual Translation via Tiny Multi-Parallel Data.](http://arxiv.org/abs/2401.12413) | 本文研究了如何通过仅有的少量微小多语言平行数据来增强以英语为中心的模型的零样本翻译能力，实现大幅度的非英文整体改进，并保持英文方向上的性能能力。研究表明，即使在随机抽取的少量方向集上进行微调，也可以获得可比较的整体改进。 |
| [^232] | [Point Transformer with Federated Learning for Predicting Breast Cancer HER2 Status from Hematoxylin and Eosin-Stained Whole Slide Images.](http://arxiv.org/abs/2312.06454) | 本文介绍了一种使用点变换器结合联邦学习的方法，用于从嗪和嘧啶法染色的全切片图像中预测乳腺癌HER2状态。该方法通过引入动态标签分布策略和辅助分类器，解决了联邦学习中的标签不平衡和利用局部上下文信息和长程依赖性的问题。 |
| [^233] | [Multistage Collaborative Knowledge Distillation from Large Language Models for Semi-Supervised Sequence Generation.](http://arxiv.org/abs/2311.08640) | 将大型语言模型的知识通过多阶段协作蒸馏的方式应用于半监督序列生成任务中，可以显著提高模型的泛化能力和性能。 |
| [^234] | [Conditional Unscented Autoencoders for Trajectory Prediction.](http://arxiv.org/abs/2310.19944) | 本文提出了使用条件非线性自动编码器(CVAE)进行轨迹预测的方法，通过利用变分自动编码器(VAE)中的非线性采样过程和其他改进，超越了现有技术，为自动驾驶领域的轨迹预测提供了更好的性能。 |
| [^235] | [An Online Bootstrap for Time Series.](http://arxiv.org/abs/2310.19683) | 本文提出了一种新型的在线自助法用于处理大规模的时间序列和相关数据流，通过考虑数据的依赖关系，该方法可以提供可靠的不确定性量化，填补了现有自助法在复杂数据依赖情况下的应用空白。 |
| [^236] | [On Feynman--Kac training of partial Bayesian neural networks.](http://arxiv.org/abs/2310.19608) | 本文提出了一种将部分贝叶斯神经网络训练转化为模拟费曼-卡克模型的高效采样训练策略，并通过各种数据集的实验证明其在预测性能方面优于现有技术。 |
| [^237] | [Series of Hessian-Vector Products for Tractable Saddle-Free Newton Optimisation of Neural Networks.](http://arxiv.org/abs/2310.14901) | 本文提出了一种以Hessian-Vector乘积系列为基础的优化算法，通过平方根和求逆操作实现了高效可伸缩的优化方法，并相对于其他一阶和二阶优化方法在运行时间和性能上具有可比性。 |
| [^238] | [On the Evaluation of Generative Models in Distributed Learning Tasks.](http://arxiv.org/abs/2310.11714) | 本文研究了在具有异构数据分布的分布式学习任务中评估生成模型。通过研究Fr\'echet inception距离（FID），并考虑不同聚合分数，发现FID-all和FID-avg分数的模型排名可能不一致。 |
| [^239] | [Sample Complexity of Preference-Based Nonparametric Off-Policy Evaluation with Deep Networks.](http://arxiv.org/abs/2310.10556) | 本文研究了在深度网络中使用人类偏好进行非参数离策略评估的样本复杂性，并建立了统计保证。 |
| [^240] | [Comparing the robustness of modern no-reference image- and video-quality metrics to adversarial attacks.](http://arxiv.org/abs/2310.06958) | 本文比较了现代图像和视频质量评估度量方法对抗攻击的鲁棒性，并发现部分度量方法对对抗攻击表现出较高的抵抗力，为基准测试提供了更安全的选择。 |
| [^241] | [NECO: NEural Collapse Based Out-of-distribution detection.](http://arxiv.org/abs/2310.06823) | NECO是一种基于神经坍塌的新颖的超出分布检测方法，利用几何属性和主成分空间识别OOD数据，在小规模和大规模任务上取得了最先进的结果，并展示了强大的泛化能力。 |
| [^242] | [Are Large Language Models Post Hoc Explainers?.](http://arxiv.org/abs/2310.05797) | 这项工作提出了第一个研究大型语言模型（LLMs）解释其他预测模型有效性的框架，并且提出了多个提示策略，填补了当前对于LLMs在解释其他模型行为方面的缺失。 |
| [^243] | [Distributed Deep Joint Source-Channel Coding with Decoder-Only Side Information.](http://arxiv.org/abs/2310.04311) | 本文提出了一种带有仅解码器侧信息的分布式深度联合源信道编码方法，在低延迟图像传输中实现了改进的性能，尤其在低信道信噪比和小带宽比的情况下。 |
| [^244] | [A Foundation Model for General Moving Object Segmentation in Medical Images.](http://arxiv.org/abs/2309.17264) | 本文提出了一种用于医学图像中移动目标分割的基础模型iMOS，通过对序列中只有少量图像进行注释，即可实现高精度的分割效果 |
| [^245] | [Can LLMs Effectively Leverage Structural Information for Graph Learning: When and Why.](http://arxiv.org/abs/2309.16595) | 本文研究了大型语言模型（LLM）在图数据中的应用，发现LLM可以从结构信息中受益，尤其是在文本节点特征缺乏的情况下，而LLM的性能与数据泄露没有显著相关。 |
| [^246] | [Actively Learning Reinforcement Learning: A Stochastic Optimal Control Approach.](http://arxiv.org/abs/2309.10831) | 本文提供了一个框架来解决强化学习中的模型不确定性和计算成本高的问题，通过使用强化学习解决随机动态规划方程，生成的控制器能够主动学习模型不确定性，并确保安全性和实时学习。 |
| [^247] | [Locally Stationary Graph Processes.](http://arxiv.org/abs/2309.01657) | 这是一种局部平稳图形过程模型，旨在将局部平稳概念扩展到不规则的图域上。它通过将整个过程表示为一组组成部分过程的组合来表征局部平稳性，以使过程在图上按照每个组成部分的要求变化得更加平滑。 |
| [^248] | [FedSoL: Bridging Global Alignment and Local Generality in Federated Learning.](http://arxiv.org/abs/2308.12532) | FedSoL提出了一种联邦学习的方法，该方法旨在解决数据分布不均匀导致性能下降的问题。它通过平衡全局对齐和本地一般性来改善FL的学习效果。 |
| [^249] | [Data-driven Intra-Autonomous Systems Graph Generator.](http://arxiv.org/abs/2308.05254) | 本文介绍了一种基于深度学习的新型合成图生成器DGGI，用于准确地模拟互联网中自治系统内的图的属性，如中心性、聚类性、同质性以及节点度量。该生成器的性能优于现有的互联网拓扑生成器。 |
| [^250] | [Predicting masked tokens in stochastic locations improves masked image modeling.](http://arxiv.org/abs/2308.00566) | 本论文提出了一种名为FlexPredict的随机模型，通过在模型中加入位置不确定性，以预测掩盖的标记位置，从而改善了掩盖图像建模的性能。 |
| [^251] | [Robust Single-view Cone-beam X-ray Pose Estimation with Neural Tuned Tomography (NeTT) and Masked Neural Radiance Fields (mNeRF).](http://arxiv.org/abs/2308.00214) | 在影像导引的微创医疗过程中，我们提出了新的方法，利用X射线投影进行辐射透明物体的姿态估计，并且展示了优化视图合成在完成此任务中的关键作用。 |
| [^252] | [Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity.](http://arxiv.org/abs/2308.00177) | 本研究研究了在标签稀缺的Learning-To-Rank问题中，无监督预训练的深度模型是否能胜过GBDTs和其他非预训练模型。实验结果表明，通过使用SimCLR-Rank方法进行无监督预训练，我们的深度学习模型在大量无标签数据和有限标签数据的情况下取得了显著优势。 |
| [^253] | [AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models.](http://arxiv.org/abs/2307.12499) | 本文提出了一种使用扩散模型生成无限制对抗样本的方法AdvDiff。通过设计两种新的对抗引导技术，在扩散模型的逆生成过程中进行对抗采样，从而有效地生成高质量、逼真的对抗样本。 |
| [^254] | [Fractional Denoising for 3D Molecular Pre-training.](http://arxiv.org/abs/2307.10683) | 本论文提出了一种分数降噪算法，用于3D分子预训练。通过混合噪声策略解决了样本覆盖率低和各向同性力场的挑战，通过解耦两种类型的噪声克服了传统降噪方法无法学习力场的问题。 |
| [^255] | [Towards Accelerating Benders Decomposition via Reinforcement Learning Surrogate Models.](http://arxiv.org/abs/2307.08816) | 本文介绍了一种利用强化学习代理模型加速Benders分解方法的方法，并通过实验证明了其相对于其他加速方案的30%更快的平均收敛速度。 |
| [^256] | [On the power of graph neural networks and the role of the activation function.](http://arxiv.org/abs/2307.04661) | 本文通过对称多项式代数的工具证明了对于具有分段多项式激活函数且体系结构大小不变的GNNs，存在一对非同构根树在任意迭代次数内无法被区分，与此同时，具有不同大小的GNNs只需两次迭代即可区分。此外，我们还证明了如果允许非分段多项式激活函数，则在两次迭代内，单个神经元感知器可以区分任意一对非同构树的根节点。 |
| [^257] | [The curse of dimensionality in operator learning.](http://arxiv.org/abs/2306.15924) | 算子学习中存在维度诅咒，但对于由Hamilton-Jacobi方程定义的解算子可以克服维度诅咒。 |
| [^258] | [HRTF upsampling with a generative adversarial network using a gnomonic equiangular projection.](http://arxiv.org/abs/2306.05812) | 本文利用生成对抗网络（GANs）将HRTF上采样，提出一种新方法，该方法在性能方面优于传统方法，使带HRTF的虚拟现实（VR）和增强现实（AR）环境更加逼真。 |
| [^259] | [Variational Gaussian Process Diffusion Processes.](http://arxiv.org/abs/2306.02066) | 本文提出一种高斯变分过程参数化方法来更好地学习具有非线性扩散过程的潜在过程，此方法采用具有连续指数族描述的算法实现凸优化，可以代替缓慢的具有固定点迭代的算法。 |
| [^260] | [Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning.](http://arxiv.org/abs/2306.00003) | 本研究提出了一种基于监督式注意力多实例学习的方法，可以自动分析超声图像，实现AS的精确筛查且精度优于传统机器学习和深度学习方法。 |
| [^261] | [Dynamic Neighborhood Construction for Structured Large Discrete Action Spaces.](http://arxiv.org/abs/2305.19891) | 本研究针对无法处理的结构化大离散动作空间（SLDAS）提出了一种名为动态邻域构建（DNC）的新型利用策略，通过可扩展的邻域探索启发式方法，高效地探索连续代理动作周围的离散邻域。 |
| [^262] | [Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making.](http://arxiv.org/abs/2305.17588) | 该论文介绍了一种名为SUFO的框架，用于增强微调的变压器模型的可解释性。该框架利用多种分析和可视化技术，解决了模型信任和可解释性的关键问题，并通过案例研究验证了其有效性。 |
| [^263] | [The cross-sectional stock return predictions via quantum neural network and tensor network.](http://arxiv.org/abs/2304.12501) | 本文研究将量子神经网络和张量网络应用于股票收益预测，在日本股市中张量网络模型表现优于传统模型，并在最新市场环境下呈现出卓越表现。 |
| [^264] | [Multimodal and multicontrast image fusion via deep generative models.](http://arxiv.org/abs/2303.15963) | 该论文提出了一种基于变分自动编码器（VAE）的图像融合方法，可以整合多模态和多对比度的神经影像数据，以提高神经影像分析的分类性能。 |
| [^265] | [Private, fair and accurate: Training large-scale, privacy-preserving AI models in medical imaging.](http://arxiv.org/abs/2302.01622) | 本研究评估了隐私保护训练医学影像人工智能模型的准确性和公平性，并与非隐私训练进行了比较。研究结果可为隐私保护技术的广泛应用提供重要参考。 |
| [^266] | [Empirical Risk Minimization with Relative Entropy Regularization.](http://arxiv.org/abs/2211.06617) | 本文研究了相对熵正则化的经验风险最小化问题，其中参考度量为sigma有限测度，解为唯一的概率测度并展现了几乎正确的保证。ERM-RER问题的解被称为Gibbs算法。 |
| [^267] | [GmGM: a Fast Multi-Axis Gaussian Graphical Model.](http://arxiv.org/abs/2211.02920) | 本文介绍了一种快速的多轴高斯图形模型，用于构建稀疏图形表示。相比先前工作，我们的算法在每个轴上仅使用一次特征分解，实现了数量级的加速。该模型可以应用于大型多模态数据集，包括单细胞多组学数据。 |
| [^268] | [Composite Goodness-of-fit Tests with Kernels.](http://arxiv.org/abs/2111.10275) | 本文提出了一种基于核的假设检验方法，可以解决具有挑战性的复合检验问题，其核心思想是在正确的模型规范的零假设下，非参数地估计参数（或模拟器）分布。 |
| [^269] | [Solving PDEs on Unknown Manifolds with Machine Learning.](http://arxiv.org/abs/2106.06682) | 本文提出了一种在未知流形上解椭圆型PDE问题的机器学习方法，通过扩散映射和深度学习，构建了一个无网格计算框架。通过将PDE求解转化为监督学习任务，采用基于神经网络的最小二乘回归来近似求解代数方程，得到了一致估计量，最终得到的数值方法在极限情况下是一致解。 |

# 详细

[^1]: 用Vabs-Net进行多级蛋白质预训练

    Multi-level protein pre-training with Vabs-Net

    [https://rss.arxiv.org/abs/2402.01481](https://rss.arxiv.org/abs/2402.01481)

    这篇论文介绍了一种使用Vabs-Net进行多级蛋白质预训练的方法。当前大多数基于结构的预训练模型仅关注残基水平，但忽略了侧链原子的重要性。为了解决这个问题，论文提出了一种新的预训练策略，引入了跨度掩码，以在三维蛋白质预训练中同时建模残基和原子水平的信息，并改善了残基表示的表达能力。

    

    最近几年，三维结构预训练蛋白质模型的发展迅猛，相较于预训练蛋白质语言模型，在各种下游任务中取得了重大进展。然而，大多数现有的基于结构的预训练模型主要关注残基水平，即α碳原子，而忽略了其他原子，如侧链原子。我们认为，在残基和原子水平上对蛋白质进行建模很重要，因为侧链原子对于许多下游任务（如分子对接）也是至关重要的。然而，我们发现在预训练中天真地组合残基和原子信息通常会失败。我们发现，信息泄漏是包含原子结构的输入导致残基级预训练任务变得琐碎并导致残基表示不够充分的一个关键原因。为了解决这个问题，我们引入了一种基于跨度掩码预训练策略的三维蛋白质预训练方法。

    In recent years, there has been a surge in the development of 3D structure-based pre-trained protein models, representing a significant advancement over pre-trained protein language models in various downstream tasks. However, most existing structure-based pre-trained models primarily focus on the residue level, i.e., alpha carbon atoms, while ignoring other atoms like side chain atoms. We argue that modeling proteins at both residue and atom levels is important since the side chain atoms can also be crucial for numerous downstream tasks, for example, molecular docking. Nevertheless, we find that naively combining residue and atom information during pre-training typically fails. We identify a key reason is the information leakage caused by the inclusion of atom structure in the input, which renders residue-level pre-training tasks trivial and results in insufficiently expressive residue representations. To address this issue, we introduce a span mask pre-training strategy on 3D protein
    
[^2]: 从有噪声标签学习非可分解性能度量的多类学习

    Multiclass Learning from Noisy Labels for Non-decomposable Performance Measures

    [https://rss.arxiv.org/abs/2402.01055](https://rss.arxiv.org/abs/2402.01055)

    本论文提出了用于从带有噪声标签的数据中学习非可分解性能度量的多类学习算法。这些算法分别适用于单调凸性和线性比率两类性能度量，并基于类条件噪声模型进行噪声校正。

    

    近年来，学习从带有噪声标签的数据中得到良好分类器引起了广泛关注。大多数关于从有噪声标签学习的工作都集中在标准的基于损失的性能度量上。然而，许多机器学习问题需要使用非可分解性能度量，这些度量不能表示为单个示例上的损失的期望或总和；其中包括类不平衡设置中的H-mean，Q-mean和G-mean，以及信息检索中的Micro F1。在本文中，我们设计了算法，用于学习两类广泛的多类非可分解性能度量，即单调凸性和线性比率，它们包括上述所有示例。我们的工作基于Narasimhan等人的Frank-Wolfe和Bisection算法(2015)。在这两种情况下，我们在广泛研究的类条件噪声模型家族下开发了算法的噪声校正版本。我们提供了遗憾(超额风险)上界。

    There has been much interest in recent years in learning good classifiers from data with noisy labels. Most work on learning from noisy labels has focused on standard loss-based performance measures. However, many machine learning problems require using non-decomposable performance measures which cannot be expressed as the expectation or sum of a loss on individual examples; these include for example the H-mean, Q-mean and G-mean in class imbalance settings, and the Micro $F_1$ in information retrieval. In this paper, we design algorithms to learn from noisy labels for two broad classes of multiclass non-decomposable performance measures, namely, monotonic convex and ratio-of-linear, which encompass all the above examples. Our work builds on the Frank-Wolfe and Bisection based methods of Narasimhan et al. (2015). In both cases, we develop noise-corrected versions of the algorithms under the widely studied family of class-conditional noise models. We provide regret (excess risk) bounds 
    
[^3]: 扩散遇见DAgger: 超级眼在手模仿学习

    Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning

    [https://arxiv.org/abs/2402.17768](https://arxiv.org/abs/2402.17768)

    提出了一种名为Diffusion Meets DAgger (DMD)的方法，用于eye-in-hand模仿学习，通过扩散模型创建新样本以提高模型的鲁棒性表现，并减少成本。

    

    论文介绍了一种新方法Diffusion Meets DAgger (DMD)，用于eye-in-hand模仿学习问题，通过利用扩散模型创建新样本，使得学习策略在遇到未出现在专家演示中的状态时具有鲁棒性表现，减少了数据采集成本。

    arXiv:2402.17768v1 Announce Type: cross  Abstract: A common failure mode for policies trained with imitation is compounding execution errors at test time. When the learned policy encounters states that were not present in the expert demonstrations, the policy fails, leading to degenerate behavior. The Dataset Aggregation, or DAgger approach to this problem simply collects more data to cover these failure states. However, in practice, this is often prohibitively expensive. In this work, we propose Diffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without the cost for eye-in-hand imitation learning problems. Instead of collecting new samples to cover out-of-distribution states, DMD uses recent advances in diffusion models to create these samples with diffusion models. This leads to robust performance from few demonstrations. In experiments conducted for non-prehensile pushing on a Franka Research 3, we show that DMD can achieve a success rate of 80% with as few as 8 e
    
[^4]: 在现实世界中使用商品移动操作器打开橱柜和抽屉

    Opening Cabinets and Drawers in the Real World using a Commodity Mobile Manipulator

    [https://arxiv.org/abs/2402.17767](https://arxiv.org/abs/2402.17767)

    实现了一个端到端系统，使商品移动操作器成功在以前未见的真实世界环境中打开橱柜和抽屉，感知误差是主要挑战。

    

    在这项工作中，我们构建了一个端到端系统，使商品移动操作器（Stretch RE2）能够在多样的以前未见的真实世界环境中拉开橱柜和抽屉。我们在31个不同的物体和13个不同真实世界环境中进行了4天的实际测试。我们的系统在零击打下，对在未知环境中新颖的橱柜和抽屉的打开率达到61%。对失败模式的分析表明，感知误差是我们系统面临的最重要挑战。

    arXiv:2402.17767v1 Announce Type: cross  Abstract: Pulling open cabinets and drawers presents many difficult technical challenges in perception (inferring articulation parameters for objects from onboard sensors), planning (producing motion plans that conform to tight task constraints), and control (making and maintaining contact while applying forces on the environment). In this work, we build an end-to-end system that enables a commodity mobile manipulator (Stretch RE2) to pull open cabinets and drawers in diverse previously unseen real world environments. We conduct 4 days of real world testing of this system spanning 31 different objects from across 13 different real world environments. Our system achieves a success rate of 61% on opening novel cabinets and drawers in unseen environments zero-shot. An analysis of the failure modes suggests that errors in perception are the most significant challenge for our system. We will open source code and models for others to replicate and bui
    
[^5]: 1比特LLM的时代：所有大型语言模型均为1.58比特

    The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits

    [https://arxiv.org/abs/2402.17764](https://arxiv.org/abs/2402.17764)

    介绍了一种新的1比特LLM变体，通过引入三进制参数在保持性能的情况下显著提高了成本效益，定义了新的训练规律，为设计专门硬件优化的1比特LLMs打开了大门

    

    近期的研究，如BitNet，正在为一个新时代的1比特大型语言模型（LLMs）铺平道路。在这项工作中，我们引入了一个1比特LLM变体，即BitNet b1.58，其中LLM的每个单个参数（或权重）均为三进制{-1, 0, 1}。它在困惑度和最终任务性能方面与相同模型大小和训练标记的全精度（即FP16或BF16）Transformer LLM相匹配，同时在延迟、内存、吞吐量和能耗方面显着更具成本效益。更重要的是，1.58比特的LLM定义了一种新的缩放定律和训练新一代既高性能又具成本效益的LLMs的配方。此外，它实现了一个新的计算范式，并为设计专门针对1比特LLMs优化的特定硬件敞开了大门。

    arXiv:2402.17764v1 Announce Type: new  Abstract: Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.
    
[^6]: 大型语言模型中的大量激活

    Massive Activations in Large Language Models

    [https://arxiv.org/abs/2402.17762](https://arxiv.org/abs/2402.17762)

    大型语言模型中出现了大量激活现象，它们具有非常大的值并且在模型中起到重要作用。

    

    我们观察到大型语言模型（LLMs）中的一个经验现象——很少的激活展现出比其他激活明显更大的值（例如，大出 100,000 倍）。我们称之为大量激活。首先，我们展示了大量激活在各种LLMs中的普遍存在，并对其位置进行了表征。其次，我们发现它们的值基本上不受输入影响，并且在LLMs中起到不可或缺的偏置项作用。第三，这些大量激活导致关注概率集中于其对应的标记，并进一步成为自注意输出中的隐式偏置项。最后，我们还研究了视觉Transformer中的大量激活。

    arXiv:2402.17762v1 Announce Type: new  Abstract: We observe an empirical phenomenon in Large Language Models (LLMs) -- very few activations exhibit significantly larger values than others (e.g., 100,000 times larger). We call them massive activations. First, we demonstrate the widespread existence of massive activations across various LLMs and characterize their locations. Second, we find their values largely stay constant regardless of the input, and they function as indispensable bias terms in LLMs. Third, these massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-attention output. Last, we also study massive activations in Vision Transformers.
    
[^7]: 学习使用快速权重编程变分量子电路

    Learning to Program Variational Quantum Circuits with Fast Weights

    [https://arxiv.org/abs/2402.17760](https://arxiv.org/abs/2402.17760)

    本文介绍了量子快速权重编程器（QFWP）作为解决量子循环神经网络（QRNNs）模型训练时间延长问题的解决方案。

    

    Quantum Machine Learning (QML)已经成为一个主要的框架，用于解决顺序控制任务和时间序列建模。 特别是在强化学习（RL）和时间序列预测等领域，已经展示了经验量子优势。 量子循环神经网络（QRNNs）是一个重大进展，专门为存储密集型任务设计，包括部分可观测环境和非线性时间序列预测。 然而，基于QRNN的模型面临挑战，尤其是由于需要使用通过时间反向传播（BPTT）计算量子梯度而产生的训练时间延长的问题。 当在量子设备上执行完整模型时，由于参数移位规则带来的电路评估需求巨大，这个困境进一步加剧。 本文将量子快速权重程序员（QFWP）引入作为解决方案。

    arXiv:2402.17760v1 Announce Type: cross  Abstract: Quantum Machine Learning (QML) has surfaced as a pioneering framework addressing sequential control tasks and time-series modeling. It has demonstrated empirical quantum advantages notably within domains such as Reinforcement Learning (RL) and time-series prediction. A significant advancement lies in Quantum Recurrent Neural Networks (QRNNs), specifically tailored for memory-intensive tasks encompassing partially observable environments and non-linear time-series prediction. Nevertheless, QRNN-based models encounter challenges, notably prolonged training duration stemming from the necessity to compute quantum gradients using backpropagation-through-time (BPTT). This predicament exacerbates when executing the complete model on quantum devices, primarily due to the substantial demand for circuit evaluation arising from the parameter-shift rule. This paper introduces the Quantum Fast Weight Programmers (QFWP) as a solution to the temporal
    
[^8]: 通过对齐锐度稳健学习单指数模型

    Robustly Learning Single-Index Models via Alignment Sharpness

    [https://arxiv.org/abs/2402.17756](https://arxiv.org/abs/2402.17756)

    通过引入对齐锐度技术，我们提出了一种高效的学习算法，实现了单指数模型的常数近似学习，适用于各种分布和连接函数，是首个适用于高斯数据和任何非平凡连接函数类的稳健学习器。

    

    我们研究了在对齐模型下以$L_2^2$损失学习单指数模型的问题。在对齐模型中，我们提出了一种高效的学习算法，实现了对最优损失的常数近似，并且适用于一系列分布（包括对数凹分布）和广泛的单调和Lipschitz连接函数的类。这是首个高效的常数近似对齐学习器，即使对于高斯数据和任何非平凡的连接函数类。以前针对未知连接函数情况的工作要么适用于可实现设置，要么无法达到常数近似。使我们的算法和分析成为可能的主要技术要素是我们称之为对齐锐度的新颖优化局部误差界的概念，这可能具有更广泛的兴趣。

    arXiv:2402.17756v1 Announce Type: new  Abstract: We study the problem of learning Single-Index Models under the $L_2^2$ loss in the agnostic model. We give an efficient learning algorithm, achieving a constant factor approximation to the optimal loss, that succeeds under a range of distributions (including log-concave distributions) and a broad class of monotone and Lipschitz link functions. This is the first efficient constant factor approximate agnostic learner, even for Gaussian data and for any nontrivial class of link functions. Prior work for the case of unknown link function either works in the realizable setting or does not attain constant factor approximation. The main technical ingredient enabling our algorithm and analysis is a novel notion of a local error bound in optimization that we term alignment sharpness and that may be of broader interest.
    
[^9]: 评估LLM代理的非常长期对话记忆

    Evaluating Very Long-Term Conversational Memory of LLM Agents

    [https://arxiv.org/abs/2402.17753](https://arxiv.org/abs/2402.17753)

    通过引入机器-人流程，基于LLM代理架构并将其对话基于人物角色和时间事件图进行基础，成功创建了LoCoMo数据集，为非常长期对话的研究填补了空白。

    

    长期开放领域对话方面的现有研究主要集中在评估模型响应，其上下文跨度不超过五个聊天会话。尽管长上下文大语言模型（LLMs）和检索增强生成（RAG）技术有所进展，但它们在非常长期对话中的有效性尚未被探索。为了解决这一研究空白，我们介绍了一种机器-人的流程，通过利用基于LLM的代理架构生成高质量的非常长期对话，并将其对话基于人物角色和时间事件图进行基础。此外，我们赋予每个代理分享和对图像做出反应的能力。生成的对话经人类注释员验证和编辑，以确保长期一致性和与事件图的基础相联系。使用此流程，我们收集了LoCoMo，一个非常长期对话的数据集，每个数据集包含300轮和平均9K令牌，最多达到35个会话。

    arXiv:2402.17753v1 Announce Type: cross  Abstract: Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoM
    
[^10]: 利用任意可编程波传播来扩展芯片光子神经处理器的规模

    Scaling on-chip photonic neural processors using arbitrarily programmable wave propagation

    [https://arxiv.org/abs/2402.17750](https://arxiv.org/abs/2402.17750)

    提出并演示了一种利用任意可编程波传播的2D波导器件，通过组合光电导增益和电光效应实现对板块的折射率进行大规模并行调制

    

    用于神经网络的芯片光子处理器在速度和能量效率方面具有潜在优势，但尚未达到能够胜过电子处理器的规模。设计芯片光子学的主导范式是制作由相对笨重的离散元件构成的网络，这些元件通过一维波导连接。一个更紧凑的替代方案是避免明确定义任何元件，而是通过在两个维度中自由传播的波直接塑造光子处理器的连续衬底来执行计算。我们提出并展示了一种可以快速重新编程空间折射率$n(x,z)$的设备，从而实现对设备中波传播的任意控制。我们的设备，一维可编程波导，将光电导增益与电光效应结合，实现了对板块的折射率的并行调制。

    arXiv:2402.17750v1 Announce Type: cross  Abstract: On-chip photonic processors for neural networks have potential benefits in both speed and energy efficiency but have not yet reached the scale at which they can outperform electronic processors. The dominant paradigm for designing on-chip photonics is to make networks of relatively bulky discrete components connected by one-dimensional waveguides. A far more compact alternative is to avoid explicitly defining any components and instead sculpt the continuous substrate of the photonic processor to directly perform the computation using waves freely propagating in two dimensions. We propose and demonstrate a device whose refractive index as a function of space, $n(x,z)$, can be rapidly reprogrammed, allowing arbitrary control over the wave propagation in the device. Our device, a 2D-programmable waveguide, combines photoconductive gain with the electro-optic effect to achieve massively parallel modulation of the refractive index of a slab
    
[^11]: 当你的AI欺骗你：在奖励学习中人类评估者部分可观测性的挑战

    When Your AI Deceives You: Challenges with Partial Observability of Human Evaluators in Reward Learning

    [https://arxiv.org/abs/2402.17747](https://arxiv.org/abs/2402.17747)

    RLHF在考虑部分观察性时可能导致策略欺骗性地夸大性能或过度辩护行为，我们提出了数学条件来解决这些问题，并警告不要盲目应用RLHF在部分可观测情况下。

    

    强化学习从人类反馈（RLHF）的过去分析假设人类完全观察到环境。当人类反馈仅基于部分观察时会发生什么？我们对两种失败情况进行了正式定义：欺骗和过度辩护。通过将人类建模为对轨迹信念的Boltzmann-理性，我们证明了RLHF保证会导致策略欺骗性地夸大其性能、为了留下印象而过度辩护或者两者兼而有之的条件。为了帮助解决这些问题，我们数学地刻画了环境部分可观测性如何转化为（缺乏）学到的回报函数中的模糊性。在某些情况下，考虑环境部分可观测性使得在理论上可能恢复回报函数和最优策略，而在其他情况下，存在不可减少的模糊性。我们警告不要盲目应用RLHF在部分可观测情况下。

    arXiv:2402.17747v1 Announce Type: cross  Abstract: Past analyses of reinforcement learning from human feedback (RLHF) assume that the human fully observes the environment. What happens when human feedback is based only on partial observations? We formally define two failure cases: deception and overjustification. Modeling the human as Boltzmann-rational w.r.t. a belief over trajectories, we prove conditions under which RLHF is guaranteed to result in policies that deceptively inflate their performance, overjustify their behavior to make an impression, or both. To help address these issues, we mathematically characterize how partial observability of the environment translates into (lack of) ambiguity in the learned return function. In some cases, accounting for partial observability makes it theoretically possible to recover the return function and thus the optimal policy, while in other cases, there is irreducible ambiguity. We caution against blindly applying RLHF in partially observa
    
[^12]: reBandit：基于随机效应的在线RL算法用于减少大麻使用

    reBandit: Random Effects based Online RL algorithm for Reducing Cannabis Use

    [https://arxiv.org/abs/2402.17739](https://arxiv.org/abs/2402.17739)

    reBandit是一种在线RL算法，利用随机效应和贝叶斯先验快速高效地学习，在移动健康环境中通过个性化干预来减少新兴成年人的大麻使用

    

    大麻使用及相关的大麻使用障碍（CUD）的不断增加在全球范围内构成了一个重大的公共卫生挑战。尤其是在新兴成年人（18-25岁）中，存在明显的治疗缺口，因此解决大麻使用和CUD仍然是2030年联合国可持续发展目标（SDG）中的一个关键目标。在这项工作中，我们开发了一种名为reBandit的在线强化学习（RL）算法，将其应用于移动健康研究中，旨在通过提供个性化移动健康干预来减少新兴成年人的大麻使用。reBandit利用随机效应和信息丰富的贝叶斯先验以在嘈杂的移动健康环境中快速而有效地学习。此外，reBandit采用经验贝叶斯和优化技术来在线自主更新其超参数。为了评估我们算法的性能，我们利用数据构建了一个模拟测试平台

    arXiv:2402.17739v1 Announce Type: new  Abstract: The escalating prevalence of cannabis use, and associated cannabis-use disorder (CUD), poses a significant public health challenge globally. With a notably wide treatment gap, especially among emerging adults (EAs; ages 18-25), addressing cannabis use and CUD remains a pivotal objective within the 2030 United Nations Agenda for Sustainable Development Goals (SDG). In this work, we develop an online reinforcement learning (RL) algorithm called reBandit which will be utilized in a mobile health study to deliver personalized mobile health interventions aimed at reducing cannabis use among EAs. reBandit utilizes random effects and informative Bayesian priors to learn quickly and efficiently in noisy mobile health environments. Moreover, reBandit employs Empirical Bayes and optimization techniques to autonomously update its hyper-parameters online. To evaluate the performance of our algorithm, we construct a simulation testbed using data from
    
[^13]: 基于学习的图搜索问题算法

    Learning-Based Algorithms for Graph Searching Problems

    [https://arxiv.org/abs/2402.17736](https://arxiv.org/abs/2402.17736)

    本研究提出了针对未知图的图搜索问题的基于学习的算法，首次在未知加权图上建立了形式保证，并设计算法在预测误差上具有最优或几乎最佳依存关系。

    

    我们考虑了Banerjee等人（2022年）最近提出的具有预测的图搜索问题。在这个问题中，一个从某个顶点$r$出发的代理者必须在最小化总行程的同时遍历一个（潜在未知的）图$G$以找到隐藏的目标节点$g$。我们研究了一种设置，在这种设置中，在任意节点$v$处，代理者会接收到到$g$的距离的噪声估计。我们设计了针对未知图的这种搜索任务的算法。我们在未知加权图上建立了第一次形式保证，并提供了显示我们提出的算法在预测误差上具有最优或几乎最佳依存关系的下界。此外，我们进行了数值实验，证明我们的算法除了对抗性误差具有鲁棒性外，还在误差是随机的典型实例中表现良好。最后，我们提供了对Banerjee等人算法的替代简化性能界限。

    arXiv:2402.17736v1 Announce Type: cross  Abstract: We consider the problem of graph searching with prediction recently introduced by Banerjee et al. (2022). In this problem, an agent, starting at some vertex $r$ has to traverse a (potentially unknown) graph $G$ to find a hidden goal node $g$ while minimizing the total distance travelled. We study a setting in which at any node $v$, the agent receives a noisy estimate of the distance from $v$ to $g$. We design algorithms for this search task on unknown graphs. We establish the first formal guarantees on unknown weighted graphs and provide lower bounds showing that the algorithms we propose have optimal or nearly-optimal dependence on the prediction error. Further, we perform numerical experiments demonstrating that in addition to being robust to adversarial error, our algorithms perform well in typical instances in which the error is stochastic. Finally, we provide alternative simpler performance bounds on the algorithms of Banerjee et 
    
[^14]: 批处理非参数上下文臂

    Batched Nonparametric Contextual Bandits

    [https://arxiv.org/abs/2402.17732](https://arxiv.org/abs/2402.17732)

    该论文研究了批处理约束下的非参数上下文臂问题，提出了一种名为BaSEDB的方案，在动态分割协变量空间的同时，实现了最优的后悔。

    

    我们研究了在批处理约束下的非参数上下文臂问题，在这种情况下，每个动作的期望奖励被建模为协变量的平滑函数，并且策略更新是在每个Observations批次结束时进行的。我们为这种设置建立了一个最小化后悔的下限，并提出了一种名为Batched Successive Elimination with Dynamic Binning（BaSEDB）的方案，可以实现最优的后悔（达到对数因子）。实质上，BaSEDB动态地将协变量空间分割成更小的箱子，并仔细调整它们的宽度以符合批次大小。我们还展示了在批处理约束下静态分箱的非最优性，突出了动态分箱的必要性。另外，我们的结果表明，在完全在线设置中，几乎恒定数量的策略更新可以达到最佳后悔。

    arXiv:2402.17732v1 Announce Type: cross  Abstract: We study nonparametric contextual bandits under batch constraints, where the expected reward for each action is modeled as a smooth function of covariates, and the policy updates are made at the end of each batch of observations. We establish a minimax regret lower bound for this setting and propose Batched Successive Elimination with Dynamic Binning (BaSEDB) that achieves optimal regret (up to logarithmic factors). In essence, BaSEDB dynamically splits the covariate space into smaller bins, carefully aligning their widths with the batch size. We also show the suboptimality of static binning under batch constraints, highlighting the necessity of dynamic binning. Additionally, our results suggest that a nearly constant number of policy updates can attain optimal regret in the fully online setting.
    
[^15]: 马尔科夫力学：学习连续时间马尔可夫链混合的方法和新颖应用

    Markovletics: Methods and A Novel Application for Learning Continuous-Time Markov Chain Mixtures

    [https://arxiv.org/abs/2402.17730](https://arxiv.org/abs/2402.17730)

    本研究引入了一个新颖框架来探索CTMCs，强调观察到的轨迹长度和混合参数对问题情境的影响，这需要特定的算法。

    

    随机数据自然产生于数字平台上用户的参与，比如社交媒体、音乐流媒体服务和网页导航，通过连续信息流体现了用户偏好和行为的演化。在随机过程中一个尚未解决的问题是学习连续时间马尔可夫链混合（CTMCs）。虽然在学习具有恢复保证的离散时间马尔可夫链混合方面取得了进展，但连续情景揭示了独特尚未探索的挑战。CTMC混合的吸引力在于它们有潜力对各个领域普遍存在的复杂连续时间随机过程进行建模，包括社交媒体、金融和生物学。

    arXiv:2402.17730v1 Announce Type: new  Abstract: Sequential data naturally arises from user engagement on digital platforms like social media, music streaming services, and web navigation, encapsulating evolving user preferences and behaviors through continuous information streams. A notable unresolved query in stochastic processes is learning mixtures of continuous-time Markov chains (CTMCs). While there is progress in learning mixtures of discrete-time Markov chains with recovery guarantees [GKV16,ST23,KTT2023], the continuous scenario uncovers unique unexplored challenges. The intrigue in CTMC mixtures stems from their potential to model intricate continuous-time stochastic processes prevalent in various fields including social media, finance, and biology.   In this study, we introduce a novel framework for exploring CTMCs, emphasizing the influence of observed trails' length and mixture parameters on problem regimes, which demands specific algorithms. Through thorough experimentati
    
[^16]: 控制非凸随机镜像下降与一般Bregman散度

    Taming Nonconvex Stochastic Mirror Descent with General Bregman Divergence

    [https://arxiv.org/abs/2402.17722](https://arxiv.org/abs/2402.17722)

    提出了对一般Bregman散度支持的非凸SMD新收敛分析，克服了先前的限制，并在全局收敛性和高概率收敛性方面取得了进展。

    

    本文重新探讨了在当代非凸优化设置中随机镜像下降（SMD）的收敛性。现有关于无批处理非凸SMD的结果限制了选择不同iable不可间断梯度的距离生成函数（DGF），从而排除了重要设置，如Shannon熵。在这项工作中，我们提出了一种支持一般DGF的非凸SMD的新收敛分析，克服了上述限制，仅依赖于标准假设。此外，我们的收敛性是针对Bregman前向-后向包络建立的，这比常用的梯度映射的平方范数更强。我们进一步将我们的结果扩展到在次高斯噪声下保证高概率收敛，并在广义Bregman Proximal Polyak-{\L}ojasiewicz条件下保证全局收敛。此外，我们说明了我们的优势。

    arXiv:2402.17722v1 Announce Type: cross  Abstract: This paper revisits the convergence of Stochastic Mirror Descent (SMD) in the contemporary nonconvex optimization setting. Existing results for batch-free nonconvex SMD restrict the choice of the distance generating function (DGF) to be differentiable with Lipschitz continuous gradients, thereby excluding important setups such as Shannon entropy. In this work, we present a new convergence analysis of nonconvex SMD supporting general DGF, that overcomes the above limitations and relies solely on the standard assumptions. Moreover, our convergence is established with respect to the Bregman Forward-Backward envelope, which is a stronger measure than the commonly used squared norm of gradient mapping. We further extend our results to guarantee high probability convergence under sub-Gaussian noise and global convergence under the generalized Bregman Proximal Polyak-{\L}ojasiewicz condition. Additionally, we illustrate the advantages of our 
    
[^17]: SMART算法在实例最优在线学习中的应用

    The SMART approach to instance-optimal online learning

    [https://arxiv.org/abs/2402.17720](https://arxiv.org/abs/2402.17720)

    SMART算法在实现实例最优在线学习中取得了重要突破，具有比传统“两全其美”界限更强大的保证，能够在每个输入序列上实现竞争性表现。

    

    我们设计了一种在线学习算法 - Switching via Monotone Adapted Regret Traces (SMART) - 该算法能够适应数据并实现实例最优的遗憾，即相对于随机跟随者（FTL）策略的性能以及任何其他输入策略的最坏情况保证而言，在每个输入序列上具有竞争力。我们证明了SMART策略对任何输入序列的遗憾都在一个倍乘因子$e/(e-1) \approx 1.58$之内，这个倍乘因子是FTL策略在该序列上获得的遗憾和由给定的最坏情况策略保证的遗憾上限中较小值的倍数。这意味着我们提供的保证比典型的“两全其美”的界限要强大，因为该保证适用于每个输入序列，无论它是如何生成的。SMART算法易于实施，因为它从FTL开始，并且在时间范围内至多切换一次到最坏情况算法。

    arXiv:2402.17720v1 Announce Type: new  Abstract: We devise an online learning algorithm -- titled Switching via Monotone Adapted Regret Traces (SMART) -- that adapts to the data and achieves regret that is instance optimal, i.e., simultaneously competitive on every input sequence compared to the performance of the follow-the-leader (FTL) policy and the worst case guarantee of any other input policy. We show that the regret of the SMART policy on any input sequence is within a multiplicative factor $e/(e-1) \approx 1.58$ of the smaller of: 1) the regret obtained by FTL on the sequence, and 2) the upper bound on regret guaranteed by the given worst-case policy. This implies a strictly stronger guarantee than typical `best-of-both-worlds' bounds as the guarantee holds for every input sequence regardless of how it is generated. SMART is simple to implement as it begins by playing FTL and switches at most once during the time horizon to the worst-case algorithm. Our approach and results fol
    
[^18]: 面向增材制造的数字孪生框架：机器学习和贝叶斯优化用于时间序列过程优化

    Towards a Digital Twin Framework in Additive Manufacturing: Machine Learning and Bayesian Optimization for Time Series Process Optimization

    [https://arxiv.org/abs/2402.17718](https://arxiv.org/abs/2402.17718)

    提出了用于增材制造的数字孪生框架，结合机器学习和贝叶斯优化实现实时预测控制，解决DED过程中的热管理问题。

    

    Laser-directed-energy deposition (DED)提供增材制造(AM)中创造复杂几何和材料分级的优势。然而，由于其分层制造，材料的不一致性和零件的可变性等挑战仍然存在。DED过程中热积累是一个关键问题，影响材料的微观结构和性质。我们的工作提出了一个数字孪生(DT)框架，用于实时预测控制DED过程参数，以满足特定的设计目标。我们使用基于长短期记忆(LSTM)的机器学习与贝叶斯推断开发了一个代用模型，可以预测DED零件中的温度。这个模型能够实时预测未来的温度状态。我们还引入了贝叶斯优化(BO)用于时间序列过程。

    arXiv:2402.17718v1 Announce Type: new  Abstract: Laser-directed-energy deposition (DED) offers advantages in additive manufacturing (AM) for creating intricate geometries and material grading. Yet, challenges like material inconsistency and part variability remain, mainly due to its layer-wise fabrication. A key issue is heat accumulation during DED, which affects the material microstructure and properties. While closed-loop control methods for heat management are common in DED research, few integrate real-time monitoring, physics-based modeling, and control in a unified framework. Our work presents a digital twin (DT) framework for real-time predictive control of DED process parameters to meet specific design objectives. We develop a surrogate model using Long Short-Term Memory (LSTM)-based machine learning with Bayesian Inference to predict temperatures in DED parts. This model predicts future temperature states in real time. We also introduce Bayesian Optimization (BO) for Time Seri
    
[^19]: 通过前向和后向近端量化器理解神经网络二值化

    Understanding Neural Network Binarization with Forward and Backward Proximal Quantizers

    [https://arxiv.org/abs/2402.17710](https://arxiv.org/abs/2402.17710)

    从优化的角度阐明神经网络二值化中的训练技巧，提出了ProxConnect++（PC++）这一泛化模型，将现有二值化技术视为其特例

    

    在神经网络二值化中，BinaryConnect（BC）及其变体被认为是标准。这些方法在前向传播中应用符号函数，它们的梯度被反向传播来更新权重。然而，当定义时符号函数的导数为零，这会导致训练停滞。因此，BC的实现（例如BNN）通常使用恒等或其他近似梯度替代符号函数在反向计算中的导数。虽然这种做法在经验上表现良好，但主要是一种启发式或“训练技巧”。我们的目标是从优化的角度阐明这些训练技巧。基于现有的ProxConnect（PC，BC的一种泛化）理论，我们（1）为PC配备了不同的前向-后向量化器，获得了包括现有二值化技术在内的ProxConnect++（PC++）特例；（2）推导了一个原则性的

    arXiv:2402.17710v1 Announce Type: new  Abstract: In neural network binarization, BinaryConnect (BC) and its variants are considered the standard. These methods apply the sign function in their forward pass and their respective gradients are backpropagated to update the weights. However, the derivative of the sign function is zero whenever defined, which consequently freezes training. Therefore, implementations of BC (e.g., BNN) usually replace the derivative of sign in the backward computation with identity or other approximate gradient alternatives. Although such practice works well empirically, it is largely a heuristic or ''training trick.'' We aim at shedding some light on these training tricks from the optimization perspective. Building from existing theory on ProxConnect (PC, a generalization of BC), we (1) equip PC with different forward-backward quantizers and obtain ProxConnect++ (PC++) that includes existing binarization techniques as special cases; (2) derive a principled wa
    
[^20]: 用于估计异质治疗效果的联邦学习

    Federated Learning for Estimating Heterogeneous Treatment Effects

    [https://arxiv.org/abs/2402.17705](https://arxiv.org/abs/2402.17705)

    提出了一种通过联邦学习在机构之间协作学习HTE估计量的新框架，实现了即使在客户之间存在多样干预和受试者群体情况下共同学习特征表示并私下学习特定的预测函数。

    

    用于估计异质治疗效果（HTE）的机器学习方法促进了跨各种领域（如医疗保健、政策制定、教育等）的大规模个性化决策。现有的用于HTE的机器学习方法需要每种处理方法大量数据的访问，而干预的高成本使得为每种干预集中收集这么多数据成为一个艰巨的挑战。为了克服这一障碍，在这项工作中，我们提出了一个通过联邦学习在机构之间协作学习HTE估计量的新框架。我们展示了，即使在客户之间存在多样的干预和受试者群体，也可以共同学习一个共同的特征表示，同时在各个机构之间同时并私下学习关于不同干预情况下结果的特定预测功能。我们的框架和相关算法基于

    arXiv:2402.17705v1 Announce Type: new  Abstract: Machine learning methods for estimating heterogeneous treatment effects (HTE) facilitate large-scale personalized decision-making across various domains such as healthcare, policy making, education, and more. Current machine learning approaches for HTE require access to substantial amounts of data per treatment, and the high costs associated with interventions makes centrally collecting so much data for each intervention a formidable challenge. To overcome this obstacle, in this work, we propose a novel framework for collaborative learning of HTE estimators across institutions via Federated Learning. We show that even under a diversity of interventions and subject populations across clients, one can jointly learn a common feature representation, while concurrently and privately learning the specific predictive functions for outcomes under distinct interventions across institutions. Our framework and the associated algorithm are based on 
    
[^21]: 将贝叶斯优化应用于转移学习以设计用于诊断测定的竞争对手DNA分子

    Transfer Learning Bayesian Optimization to Design Competitor DNA Molecules for Use in Diagnostic Assays

    [https://arxiv.org/abs/2402.17704](https://arxiv.org/abs/2402.17704)

    通过将转移学习的代理模型与贝叶斯优化相结合，本文展示了如何通过在优化任务之间共享信息来减少实验的总数，并且演示了在设计用于扩增基因诊断测定的DNA竞争对手时实验数量的减少。

    

    随着工程生物分子设备的兴起，定制生物序列的需求不断增加。通常，为了特定应用需要制作许多类似的生物序列，这意味着需要进行大量甚至昂贵的实验来优化这些序列。本文提出了一个转移学习设计实验工作流程，使这种开发变得可行。通过将转移学习代理模型与贝叶斯优化相结合，我们展示了如何通过在优化任务之间共享信息来减少实验的总数。我们演示了使用用于扩增基因诊断测定中使用的DNA竞争对手开发数据来减少实验数量。我们使用交叉验证来比较不同转移学习模型的预测准确性，然后比较这些模型在单一目标和惩罚优化下的性能。

    arXiv:2402.17704v1 Announce Type: cross  Abstract: With the rise in engineered biomolecular devices, there is an increased need for tailor-made biological sequences. Often, many similar biological sequences need to be made for a specific application meaning numerous, sometimes prohibitively expensive, lab experiments are necessary for their optimization. This paper presents a transfer learning design of experiments workflow to make this development feasible. By combining a transfer learning surrogate model with Bayesian optimization, we show how the total number of experiments can be reduced by sharing information between optimization tasks. We demonstrate the reduction in the number of experiments using data from the development of DNA competitors for use in an amplification-based diagnostic assay. We use cross-validation to compare the predictive accuracy of different transfer learning models, and then compare the performance of the models for both single objective and penalized opti
    
[^22]: 使用混合谱图-TasNet进行实时低延迟音乐源分离

    Real-time Low-latency Music Source Separation using Hybrid Spectrogram-TasNet

    [https://arxiv.org/abs/2402.17701](https://arxiv.org/abs/2402.17701)

    提出了一种混合谱图时域音频分离网络（HS-TasNet），用于实现实时低延迟音乐源分离，在MusDB测试集上达到了较高的信号-失真比。

    

    近年来，深度学习在音乐解混方面取得了显著进展。然而，对于如何调整这些神经网络以适用于实时低延迟应用几乎没有得到关注，这对助听器、混音音频流和现场表演可能很有帮助。本文研究了将当前文献中的解混模型调整为这种用例所涉及的各种挑战。随后，受混合Demucs架构启发，我们提出了混合谱图时域音频分离网络HS-TasNet，充分利用了频谱和波形域的优势。在23毫秒的延迟下，HS-TasNet在MusDB测试集上获得了4.65的总信号-失真比（SDR），并在额外的训练数据下增加到5.55。这些结果表明了实时低延迟音乐应用中高效解混的潜力。

    arXiv:2402.17701v1 Announce Type: cross  Abstract: There have been significant advances in deep learning for music demixing in recent years. However, there has been little attention given to how these neural networks can be adapted for real-time low-latency applications, which could be helpful for hearing aids, remixing audio streams and live shows. In this paper, we investigate the various challenges involved in adapting current demixing models in the literature for this use case. Subsequently, inspired by the Hybrid Demucs architecture, we propose the Hybrid Spectrogram Time-domain Audio Separation Network HS-TasNet, which utilises the advantages of spectral and waveform domains. For a latency of 23 ms, the HS-TasNet obtains an overall signal-to-distortion ratio (SDR) of 4.65 on the MusDB test set, and increases to 5.55 with additional training data. These results demonstrate the potential of efficient demixing for real-time low-latency music applications.
    
[^23]: RAVEL: 在解开语言模型表示方面评估可解释性方法

    RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations

    [https://arxiv.org/abs/2402.17700](https://arxiv.org/abs/2402.17700)

    RAVEL数据集介绍了一种新方法MDAS，该方法在解开语言模型表示方面取得了最新的成果，强调了跨激活特征的重要性。

    

    个别神经元参与多个高级概念的表示。不同的可解释性方法在多大程度上能成功解开这些角色？为了帮助解决这个问题，我们介绍了RAVEL（Resolving Attribute-Value Entanglements in Language Models），这是一个数据集，可以实现对多种现有可解释性方法进行紧密控制的定量比较。我们利用由此产生的概念框架来定义新的Multi-task Distributed Alignment Search（MDAS）方法，该方法能够找到满足多个因果标准的分布式表示。以Llama2-7B作为目标语言模型，MDAS在RAVEL上取得了最新的成果，展示了超越神经元级别分析以识别跨激活的特征的重要性。我们在https://github.com/explanare/ravel上发布了我们的基准。

    arXiv:2402.17700v1 Announce Type: new  Abstract: Individual neurons participate in the representation of multiple high-level concepts. To what extent can different interpretability methods successfully disentangle these roles? To help address this question, we introduce RAVEL (Resolving Attribute-Value Entanglements in Language Models), a dataset that enables tightly controlled, quantitative comparisons between a variety of existing interpretability methods. We use the resulting conceptual framework to define the new method of Multi-task Distributed Alignment Search (MDAS), which allows us to find distributed representations satisfying multiple causal criteria. With Llama2-7B as the target language model, MDAS achieves state-of-the-art results on RAVEL, demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations. We release our benchmark at https://github.com/explanare/ravel.
    
[^24]: 使用自动循环调度的基于梯度的离散采样

    Gradient-based Discrete Sampling with Automatic Cyclical Scheduling

    [https://arxiv.org/abs/2402.17699](https://arxiv.org/abs/2402.17699)

    提出了一种使用自动循环调度的基于梯度的离散采样方法，有效应对高度多模态的离散分布，包括循环步长调度、循环平衡调度和自动调整超参数方案。

    

    离散分布，特别是在高维深度模型中，通常由于固有的不连续性而呈现高度多模态。虽然基于梯度的离散采样已被证明是有效的，但由于梯度信息，它容易陷入局部模式。为了解决这一挑战，我们提出了一种自动循环调度，旨在实现对多模态离散分布进行高效准确的采样。我们的方法包括三个关键部分：（1）循环步长调度，其中大步长发现新模式，小步长利用每个模式；（2）循环平衡调度，确保给定步长的“平衡”提案和马尔可夫链的高效率；以及（3）自动调整方案，用于调整循环调度中的超参数，实现在各种数据集上的自适应性且需最小调整。我们证明了我们的方法的非渐近收敛和推断保证。

    arXiv:2402.17699v1 Announce Type: new  Abstract: Discrete distributions, particularly in high-dimensional deep models, are often highly multimodal due to inherent discontinuities. While gradient-based discrete sampling has proven effective, it is susceptible to becoming trapped in local modes due to the gradient information. To tackle this challenge, we propose an automatic cyclical scheduling, designed for efficient and accurate sampling in multimodal discrete distributions. Our method contains three key components: (1) a cyclical step size schedule where large steps discover new modes and small steps exploit each mode; (2) a cyclical balancing schedule, ensuring ``balanced" proposals for given step sizes and high efficiency of the Markov chain; and (3) an automatic tuning scheme for adjusting the hyperparameters in the cyclical schedules, allowing adaptability across diverse datasets with minimal tuning. We prove the non-asymptotic convergence and inference guarantee for our method i
    
[^25]: 在过程工程中使用算子推断学习降阶二次-线性模型

    Learning reduced-order Quadratic-Linear models in Process Engineering using Operator Inference

    [https://arxiv.org/abs/2402.17698](https://arxiv.org/abs/2402.17698)

    使用算子推断学习的降阶模型在过程工程中建模动态系统，为实现快速可靠的数字孪生架构迈出重要一步。

    

    在这项工作中，我们解决了在过程工程中高效建模动态系统的挑战。我们使用降阶模型学习，具体来说是算子推断。这是一种非侵入式、数据驱动的从时域数据学习动态系统的方法。我们的研究应用是二氧化碳甲烷化反应，这是电力转化技术框架中的重要反应，以展示其潜力。数值结果表明，用算子推断构建的降阶模型能够提供一个简化但准确的替代解决方案。这标志着实现快速可靠数字孪生架构的重要里程碑。

    arXiv:2402.17698v1 Announce Type: cross  Abstract: In this work, we address the challenge of efficiently modeling dynamical systems in process engineering. We use reduced-order model learning, specifically operator inference. This is a non-intrusive, data-driven method for learning dynamical systems from time-domain data. The application in our study is carbon dioxide methanation, an important reaction within the Power-to-X framework, to demonstrate its potential. The numerical results show the ability of the reduced-order models constructed with operator inference to provide a reduced yet accurate surrogate solution. This represents an important milestone towards the implementation of fast and reliable digital twin architectures.
    
[^26]: 计算机辅助设计的几何深度学习：一项调查

    Geometric Deep Learning for Computer-Aided Design: A Survey

    [https://arxiv.org/abs/2402.17695](https://arxiv.org/abs/2402.17695)

    几何深度学习技术在计算机辅助设计领域具有变革性力量，可以通过机器学习优化CAD设计师的工作流程，节省时间和精力，提高决策效率，并创造出具有创新性和实用性的设计。

    

    几何深度学习技术已成为计算机辅助设计（CAD）领域的一股变革力量，并有可能彻底改变设计师和工程师处理和增强设计过程的方式。通过利用基于机器学习的方法，CAD设计师可以优化他们的工作流程，节省时间和精力，做出更为明智的决策，创造既创新又实用的设计。处理以几何数据表示的CAD设计并分析其编码特征的能力使得能够识别不同CAD模型之间的相似之处，提出替代设计和增强方案，甚至生成新的设计替代方案。这份调查全面介绍了计算机辅助设计中基于学习的方法，涵盖了各种类别，包括相似性分析和检索、2D和3D CAD模型合成，以及CAD生成。

    arXiv:2402.17695v1 Announce Type: cross  Abstract: Geometric Deep Learning techniques have become a transformative force in the field of Computer-Aided Design (CAD), and have the potential to revolutionize how designers and engineers approach and enhance the design process. By harnessing the power of machine learning-based methods, CAD designers can optimize their workflows, save time and effort while making better informed decisions, and create designs that are both innovative and practical. The ability to process the CAD designs represented by geometric data and to analyze their encoded features enables the identification of similarities among diverse CAD models, the proposition of alternative designs and enhancements, and even the generation of novel design alternatives. This survey offers a comprehensive overview of learning-based methods in computer-aided design across various categories, including similarity analysis and retrieval, 2D and 3D CAD model synthesis, and CAD generatio
    
[^27]: 自动驾驶车辆：人工智能和学习算法的演进

    Autonomous Vehicles: Evolution of Artificial Intelligence and Learning Algorithms

    [https://arxiv.org/abs/2402.17690](https://arxiv.org/abs/2402.17690)

    本文全面探讨了自动驾驶车辆中AI的演进轨迹，从基础原理追溯到最新进展，并阐明了AI在塑造车辆自主决策能力中的基础作用。

    

    自动驾驶车辆的出现标志着交通运输领域迎来了一个变革时代，通过尖端技术重塑了移动性的格局。人工智能（AI）和学习算法的整合是这一进化的核心，将车辆推向前所未有的自主领域。本文全面探讨了自动驾驶车辆中AI的演进轨迹，从基础原理追溯到最新进展。从当前景观概述开始，本文深入探讨了AI在塑造车辆自主决策能力中的基础作用。阐明了AI驱动的车辆开发生命周期中涉及的步骤，解决了自动驾驶车辆中AI驱动软件开发中的伦理考虑和偏见问题。该研究提供了关于AI/学习的使用和类型的统计洞见。

    arXiv:2402.17690v1 Announce Type: cross  Abstract: The advent of autonomous vehicles has heralded a transformative era in transportation, reshaping the landscape of mobility through cutting-edge technologies. Central to this evolu- tion is the integration of Artificial Intelligence (AI) and learning algorithms, propelling vehicles into realms of unprecedented autonomy. This paper provides a comprehensive exploration of the evolutionary trajectory of AI within autonomous vehicles, tracing the journey from foundational principles to the most recent advancements. Commencing with a current landscape overview, the paper delves into the fundamental role of AI in shaping the autonomous decision-making capabilities of vehicles. It elucidates the steps involved in the AI-powered development life cycle in vehicles, addressing ethical considerations and bias in AI-driven software development for autonomous vehicles. The study presents statis- tical insights into the usage and types of AI/learning
    
[^28]: 通过先验用户信息在无线车载环境中预测QoS

    QoS prediction in radio vehicular environments via prior user information

    [https://arxiv.org/abs/2402.17689](https://arxiv.org/abs/2402.17689)

    本文评估了使用先前提取的用户信息对车载无线环境中的QoS进行预测的方法，展示了如何利用ML树集成方法来提高预测性能。

    

    可靠的无线通信在汽车行业中扮演着重要角色，它有助于增强当前用例并实现新的用例，如连接的自动驾驶、编队行驶、合作操纵、远程驾驶和智能导航。这些以及其他用例通常依赖于特定的通信服务质量（QoS）水平。最近，预测性服务质量（QoS）领域受到了极大关注，因为它可以提前足够准确地预测通信质量。然而，在可靠地预测QoS方面是一项非常困难的任务。在本文中，我们评估使用从蜂窝测试网络收集的数据来预测QoS的ML树集成方法，范围为几分钟。我们讨论了无线环境特征，并展示了如何利用这些特征来提高ML性能，并进一步支持ML在商业网络中的应用。具体来说，我们使用了

    arXiv:2402.17689v1 Announce Type: cross  Abstract: Reliable wireless communications play an important role in the automotive industry as it helps to enhance current use cases and enable new ones such as connected autonomous driving, platooning, cooperative maneuvering, teleoperated driving, and smart navigation. These and other use cases often rely on specific quality of service (QoS) levels for communication. Recently, the area of predictive quality of service (QoS) has received a great deal of attention as a key enabler to forecast communication quality well enough in advance. However, predicting QoS in a reliable manner is a notoriously difficult task. In this paper, we evaluate ML tree-ensemble methods to predict QoS in the range of minutes with data collected from a cellular test network. We discuss radio environment characteristics and we showcase how these can be used to improve ML performance and further support the uptake of ML in commercial networks. Specifically, we use the 
    
[^29]: 反应性机器学习势能表面的异常检测

    Outlier-Detection for Reactive Machine Learned Potential Energy Surfaces

    [https://arxiv.org/abs/2402.17686](https://arxiv.org/abs/2402.17686)

    集成模型在检测异常值方面表现最佳，其次是高斯混合模型，研究发现一种结构指标与大误差相关，有助于快速分类新结构。

    

    本文应用不确定性量化（UQ）来检测具有大期望误差（异常值）的样本，应用于反应性分子势能表面（PES）。三种方法 - 集成模型、深刻证据回归（DER）和高斯混合模型（GMM） - 被应用于${\it syn-}$Criegee和乙烯羟基过氧化物之间的氢转移反应。结果表明，集成模型为检测异常值提供了最佳结果，其次是GMM。例如，从具有最大不确定性的1000个结构中，如果寻找具有大误差的25或1000个结构，异常值检测质量分别为约90\%和约50\%。相反，DER的统计假设的局限性严重影响了其预测能力。最后，发现了一种基于结构的指标与大平均误差相关，这有助于快速将新结构分类为那些能够提供最大稳定性的结构。

    arXiv:2402.17686v1 Announce Type: cross  Abstract: Uncertainty quantification (UQ) to detect samples with large expected errors (outliers) is applied to reactive molecular potential energy surfaces (PESs). Three methods - Ensembles, Deep Evidential Regression (DER), and Gaussian Mixture Models (GMM) - were applied to the H-transfer reaction between ${\it syn-}$Criegee and vinyl hydroxyperoxide. The results indicate that ensemble models provide the best results for detecting outliers, followed by GMM. For example, from a pool of 1000 structures with the largest uncertainty, the detection quality for outliers is $\sim 90$ \% and $\sim 50$ \%, respectively, if 25 or 1000 structures with large errors are sought. On the contrary, the limitations of the statistical assumptions of DER greatly impacted its prediction capabilities. Finally, a structure-based indicator was found to be correlated with large average error, which may help to rapidly classify new structures into those that provide a
    
[^30]: 加强上下文学习以确保可靠性：对基础模型的简要概述

    Securing Reliability: A Brief Overview on Enhancing In-Context Learning for Foundation Models

    [https://arxiv.org/abs/2402.17671](https://arxiv.org/abs/2402.17671)

    论文回顾了增强基础模型可靠性和可信度的最新进展，着重于四种关键方法论，为构建安全可靠的FMs和促进稳定一致的ICL环境提供了有价值的见解。

    

    随着基础模型（FMs）继续塑造人工智能的格局，上下文学习（ICL）范式蓬勃发展，但也遇到了毒性、幻觉、差异、对抗性脆弱性和不一致性等问题。确保FMs的可靠性和责任性对于人工智能生态系统的可持续发展至关重要。在这篇简明概述中，我们调查了增强FMs在ICL框架内可靠性和可信度的最新进展，重点关注四种关键方法论，每种方法论都有其相应的子目标。我们真诚希望本文能为致力于构建安全可靠FMs并促进稳定一致的ICL环境、从而释放其巨大潜力的研究人员和从业者提供宝贵的见解。

    arXiv:2402.17671v1 Announce Type: new  Abstract: As foundation models (FMs) continue to shape the landscape of AI, the in-context learning (ICL) paradigm thrives but also encounters issues such as toxicity, hallucination, disparity, adversarial vulnerability, and inconsistency. Ensuring the reliability and responsibility of FMs is crucial for the sustainable development of the AI ecosystem. In this concise overview, we investigate recent advancements in enhancing the reliability and trustworthiness of FMs within ICL frameworks, focusing on four key methodologies, each with its corresponding subgoals. We sincerely hope this paper can provide valuable insights for researchers and practitioners endeavoring to build safe and dependable FMs and foster a stable and consistent ICL environment, thereby unlocking their vast potential.
    
[^31]: 多智能体深度强化学习用于分布式卫星路由

    Multi-Agent Deep Reinforcement Learning for Distributed Satellite Routing

    [https://arxiv.org/abs/2402.17666](https://arxiv.org/abs/2402.17666)

    本文介绍了一种多智能体深度强化学习方法，用于低地球轨道卫星星座中的路由，通过离线学习最佳路径，并在在线阶段进行高效的分布式路由。

    

    本文介绍了一种用于低地球轨道卫星星座（LSatCs）中路由的多智能体深度强化学习（MA-DRL）方法。每个卫星是一个独立的决策制定智能体，具有对环境的部分知识，并受到附近智能体的反馈支持。在我们之前介绍的Q-routing解决方案的基础上，本文的贡献是将其扩展为一个深度学习框架，能够快速适应网络和交通变化，并基于两个阶段：（1）一个依赖全局深度神经网络（DNN）学习在每个可能位置和拥堵级别上的最佳路径的离线探索学习阶段；（2）一个带有本地、机载、预训练DNN的在线开发阶段。结果表明，MA-DRL能够有效地在离线学习最佳路由，然后加载以进行高效的分布式在线路由。

    arXiv:2402.17666v1 Announce Type: new  Abstract: This paper introduces a Multi-Agent Deep Reinforcement Learning (MA-DRL) approach for routing in Low Earth Orbit Satellite Constellations (LSatCs). Each satellite is an independent decision-making agent with a partial knowledge of the environment, and supported by feedback received from the nearby agents. Building on our previous work that introduced a Q-routing solution, the contribution of this paper is to extend it to a deep learning framework able to quickly adapt to the network and traffic changes, and based on two phases: (1) An offline exploration learning phase that relies on a global Deep Neural Network (DNN) to learn the optimal paths at each possible position and congestion level; (2) An online exploitation phase with local, on-board, pre-trained DNNs. Results show that MA-DRL efficiently learns optimal routes offline that are then loaded for an efficient distributed routing online.
    
[^32]: TorchMD-Net 2.0: 分子模拟中的快速神经网络势

    TorchMD-Net 2.0: Fast Neural Network Potentials for Molecular Simulations

    [https://arxiv.org/abs/2402.17660](https://arxiv.org/abs/2402.17660)

    TorchMD-Net 2.0是在神经网络势模型方面取得的重要进展，通过引入TensorNet等尖端结构，显著提高了计算效率，使得在计算能量和力时获得了2到10倍的性能提升。

    

    arXiv:2402.17660v1 声明类型：新的 摘要：在分子模拟中实现计算速度、预测准确性和通用适用性之间的平衡一直是一个持久的挑战。本文介绍了TorchMD-Net软件的重大进展，这是从传统力场转向基于神经网络的势的重要一步。TorchMD-Net演变成一个更全面和更多样化的框架，引入了TensorNet等尖端体系结构。通过模块化设计方法实现了这种转变，鼓励科学界内部的定制应用。最显着的增强是在计算效率方面显著改进，在TensorNet模型的能量和力计算中实现了非常显著的加速，性能提升范围从前几个版本的2倍到10倍。其他增强功能包括高度优化的邻居搜索

    arXiv:2402.17660v1 Announce Type: new  Abstract: Achieving a balance between computational speed, prediction accuracy, and universal applicability in molecular simulations has been a persistent challenge. This paper presents substantial advancements in the TorchMD-Net software, a pivotal step forward in the shift from conventional force fields to neural network-based potentials. The evolution of TorchMD-Net into a more comprehensive and versatile framework is highlighted, incorporating cutting-edge architectures such as TensorNet. This transformation is achieved through a modular design approach, encouraging customized applications within the scientific community. The most notable enhancement is a significant improvement in computational efficiency, achieving a very remarkable acceleration in the computation of energy and forces for TensorNet models, with performance gains ranging from 2-fold to 10-fold over previous iterations. Other enhancements include highly optimized neighbor sear
    
[^33]: 基于置信度的多字段模型校准

    Confidence-Aware Multi-Field Model Calibration

    [https://arxiv.org/abs/2402.17655](https://arxiv.org/abs/2402.17655)

    本研究提出了一种基于置信度的多字段校准方法，通过根据样本统计推导的置信水平自适应调整校准强度，以解决校准过程中存在的偏差放大和在线干扰问题。

    

    预测用户反馈概率（如点击和转换）对于广告排名和竞价至关重要。然而，由于数据分布的转移和固有模型偏差，预测概率与真实可能性之间经常存在不希望的不一致。校准旨在通过后处理模型预测来解决此问题，而基于字段的校准可以调整不同特征字段值上的模型输出，以满足细粒度的广告需求。不幸的是，对应于某些字段值的观察样本可能太有限，无法进行有信心的校准，这可能导致偏差放大和在线干扰。在本文中，我们提出了一种基于置信度的多字段校准方法，根据样本统计推导的置信水平自适应调整校准强度。它还利用多个特征字段进行联合模型校准。

    arXiv:2402.17655v1 Announce Type: new  Abstract: Accurately predicting the probabilities of user feedback, such as clicks and conversions, is critical for ad ranking and bidding. However, there often exist unwanted mismatches between predicted probabilities and true likelihoods due to the shift of data distributions and intrinsic model biases. Calibration aims to address this issue by post-processing model predictions, and field-aware calibration can adjust model output on different feature field values to satisfy fine-grained advertising demands. Unfortunately, the observed samples corresponding to certain field values can be too limited to make confident calibrations, which may yield bias amplification and online disturbance. In this paper, we propose a confidence-aware multi-field calibration method, which adaptively adjusts the calibration intensity based on the confidence levels derived from sample statistics. It also utilizes multiple feature fields for joint model calibration wi
    
[^34]: 变分学习对大型深度网络有效

    Variational Learning is Effective for Large Deep Networks

    [https://arxiv.org/abs/2402.17641](https://arxiv.org/abs/2402.17641)

    变分学习在大型深度网络中展现出非常好的效果，IVON优化器在训练大型网络时几乎能与Adam相媲美甚至胜过它，且预测不确定性更准确，对模型微调、泛化误差预测和数据敏感性估计均有显著改进。

    

    我们提供了大量实证证据，反驳了变分学习对大型神经网络无效的普遍看法。我们展示了一种名为Improved Variational Online Newton (IVON)的优化器，在训练大型网络（如GPT-2和ResNets）时始终能够与Adam相匹配或胜过它。IVON的计算成本几乎与Adam相同，但其预测不确定性更好。我们展示了IVON的几种新用例，其中我们改进了大型语言模型的微调和模型合并，在准确预测泛化误差和忠实估计对数据的敏感性方面。我们找到了大量支持变分学习有效性的证据。

    arXiv:2402.17641v1 Announce Type: cross  Abstract: We give extensive empirical evidence against the common belief that variational learning is ineffective for large neural networks. We show that an optimizer called Improved Variational Online Newton (IVON) consistently matches or outperforms Adam for training large networks such as GPT-2 and ResNets from scratch. IVON's computational costs are nearly identical to Adam but its predictive uncertainty is better. We show several new use cases of IVON where we improve fine-tuning and model merging in Large Language Models, accurately predict generalization error, and faithfully estimate sensitivity to data. We find overwhelming evidence in support of effectiveness of variational learning.
    
[^35]: 用于微生物组学的监督机器学习：弥合当前和最佳实践之间的差距

    Supervised machine learning for microbiomics: bridging the gap between current and best practices

    [https://arxiv.org/abs/2402.17621](https://arxiv.org/abs/2402.17621)

    该研究通过分析大量期刊文章，总结了监督机器学习在微生物组学中的现有实践，探讨了实验设计方法的优缺点，并提出了如何避免常见实验设计缺陷的指导。

    

    机器学习（ML）将加速临床微生物组学创新，如疾病诊断和预后。这将需要高质量、可重现、可解释的工作流程，其预测能力达到或超过监管机构对临床工具设定的高门槛。我们通过深入分析2021-2022年发表的100篇同行评议的期刊文章，捕捉了当前将监督ML应用于微生物组学数据的实践的一个快照。我们采用数据驱动方法，引导讨论各种实验设计方法的优点，包括关键考虑因素，如如何减轻小数据集大小的影响同时避免数据泄漏。我们进一步提供关于如何避免可能损害模型性能、可信度和可重复性的常见实验设计缺陷的指南。讨论附有一个互动在线教程。

    arXiv:2402.17621v1 Announce Type: cross  Abstract: Machine learning (ML) is set to accelerate innovations in clinical microbiomics, such as in disease diagnostics and prognostics. This will require high-quality, reproducible, interpretable workflows whose predictive capabilities meet or exceed the high thresholds set for clinical tools by regulatory agencies. Here, we capture a snapshot of current practices in the application of supervised ML to microbiomics data, through an in-depth analysis of 100 peer-reviewed journal articles published in 2021-2022. We apply a data-driven approach to steer discussion of the merits of varied approaches to experimental design, including key considerations such as how to mitigate the effects of small dataset size while avoiding data leakage. We further provide guidance on how to avoid common experimental design pitfalls that can hurt model performance, trustworthiness, and reproducibility. Discussion is accompanied by an interactive online tutorial th
    
[^36]: 使用双向图注意力网络学习拓扑表示解决车间作业调度问题

    Learning Topological Representations with Bidirectional Graph Attention Network for Solving Job Shop Scheduling Problem

    [https://arxiv.org/abs/2402.17606](https://arxiv.org/abs/2402.17606)

    本文提出了拓扑感知的双向图注意力网络（TBGAT），在解决车间作业调度问题中，通过嵌入并发图并利用双向视图嵌入、图注意力聚合等技术，实现了对拓扑结构的更好建模和利用。

    

    现有的基于学习的方法通常使用针对无向图的现成GNN模型解决车间作业调度问题（JSSP），并忽略了并发图（DGs）的丰富而有意义的拓扑结构。本文提出了拓扑感知的双向图注意力网络（TBGAT），这是一种基于注意力机制的新颖GNN架构，用于在本地搜索框架中嵌入DG以解决JSSP。具体而言，TBGAT分别从正向和反向视图嵌入DG，消息通过遵循不同视图的拓扑结构传播，并通过图注意力进行汇总。然后，我们提出一种基于消息传递机制的新操作符，用于计算DG的前向和后向拓扑排序，这些特征用于表征拓扑结构并被我们的模型利用。此外，我们从理论和实验上展示了TBGAT的...

    arXiv:2402.17606v1 Announce Type: cross  Abstract: Existing learning-based methods for solving job shop scheduling problem (JSSP) usually use off-the-shelf GNN models tailored to undirected graphs and neglect the rich and meaningful topological structures of disjunctive graphs (DGs). This paper proposes the topology-aware bidirectional graph attention network (TBGAT), a novel GNN architecture based on the attention mechanism, to embed the DG for solving JSSP in a local search framework. Specifically, TBGAT embeds the DG from a forward and a backward view, respectively, where the messages are propagated by following the different topologies of the views and aggregated via graph attention. Then, we propose a novel operator based on the message-passing mechanism to calculate the forward and backward topological sorts of the DG, which are the features for characterizing the topological structures and exploited by our model. In addition, we theoretically and experimentally show that TBGAT h
    
[^37]: 通过对弱标签集建模推进睡眠检测：一种新颖的弱监督学习方法

    Advancing sleep detection by modelling weak label sets: A novel weakly supervised learning approach

    [https://arxiv.org/abs/2402.17601](https://arxiv.org/abs/2402.17601)

    提出了一种利用弱监督学习进行睡眠检测的新方法，基于传统睡眠检测算法生成的弱标签集，采用新颖的统计模型来最小化软交叉熵损失和Brier分数作为损失函数。

    

    睡眠和活动模式的理解在身心健康中起着至关重要的作用。本研究引入了一种利用弱监督学习进行睡眠检测的新方法，适用于可靠地基准标签不可用的情况。所提出的方法依赖于一组弱标签，这些标签是通过传统睡眠检测算法生成的预测导出的。引入了一种新颖的方法，我们提出了一种新颖的广义非线性统计模型，其中弱睡眠标签的数量被建模为二项分布的结果。二项分布中的睡眠概率与基于觉醒剂学的神经网络训练结果相关联。我们展示了最大化模型的似然函数等价于最小化软交叉熵损失。此外，我们探讨了将Brier分数作为弱标签的损失函数的使用。所提出方法的效力

    arXiv:2402.17601v1 Announce Type: new  Abstract: Understanding sleep and activity patterns plays a crucial role in physical and mental health. This study introduces a novel approach for sleep detection using weakly supervised learning for scenarios where reliable ground truth labels are unavailable. The proposed method relies on a set of weak labels, derived from the predictions generated by conventional sleep detection algorithms. Introducing a novel approach, we suggest a novel generalised non-linear statistical model in which the number of weak sleep labels is modelled as outcome of a binomial distribution. The probability of sleep in the binomial distribution is linked to the outcomes of neural networks trained to detect sleep based on actigraphy. We show that maximizing the likelihood function of the model, is equivalent to minimizing the soft cross-entropy loss. Additionally, we explored the use of the Brier score as a loss function for weak labels. The efficacy of the suggested 
    
[^38]: DAGnosis：使用结构进行数据不一致性的局部识别

    DAGnosis: Localized Identification of Data Inconsistencies using Structures

    [https://arxiv.org/abs/2402.17599](https://arxiv.org/abs/2402.17599)

    DAGnosis使用有向无环图(DAGs)来解决数据一致性检测中的两个关键限制，并能够准确定位为何样本会被标记为不一致。

    

    在部署时识别和适当处理数据中的不一致性对可靠地使用机器学习模型至关重要。近期的数据中心方法能够识别与训练集相关的这种不一致性，但存在两个关键限制：（1）在特征展现统计独立性的情况下表现不佳，因为它们使用压缩表示；（2）缺乏局部化，无法准确定位样本为何被标记为不一致，这对指导未来数据收集至关重要。我们使用有向无环图（DAGs）来编码训练集的特征概率分布和独立性作为结构，从而解决了这两个基本限制。我们的方法被称为DAGnosis，利用这些结构交互带来有价值的、深刻的数据中心结论。DAGnosis解锁了在DAG上定位不一致性原因的能力，

    arXiv:2402.17599v1 Announce Type: cross  Abstract: Identification and appropriate handling of inconsistencies in data at deployment time is crucial to reliably use machine learning models. While recent data-centric methods are able to identify such inconsistencies with respect to the training set, they suffer from two key limitations: (1) suboptimality in settings where features exhibit statistical independencies, due to their usage of compressive representations and (2) lack of localization to pin-point why a sample might be flagged as inconsistent, which is important to guide future data collection. We solve these two fundamental limitations using directed acyclic graphs (DAGs) to encode the training set's features probability distribution and independencies as a structure. Our method, called DAGnosis, leverages these structural interactions to bring valuable and insightful data-centric conclusions. DAGnosis unlocks the localization of the causes of inconsistencies on a DAG, an aspec
    
[^39]: 通过谱神经网络和非线性矩阵感知实现隐式正则化

    Implicit Regularization via Spectral Neural Networks and Non-linear Matrix Sensing

    [https://arxiv.org/abs/2402.17595](https://arxiv.org/abs/2402.17595)

    本文在更现实的神经网络背景下探讨了隐式正则化现象，通过研究非线性激活函数的一般类别，严格证明了在矩阵感知问题设置中这些网络的隐式正则化现象，同时提供了严格的速率保证，确保梯度的指数级快速收敛。

    

    隐式正则化现象近年来引起了人们的兴趣，作为神经网络出色泛化能力的一个基本方面。简而言之，它意味着在许多神经网络中，即使损失函数中没有任何显式正则化器，梯度下降动态也会收敛到一个正则化学习问题的解。然而，已知的试图从理论上解释这一现象的结果主要集中在线性神经网络的设置上，线性结构的简单性对现有论据特别关键。本文在更现实的神经网络和一般非线性激活函数的背景下探讨了这一问题，并在矩阵感知问题的设置中严格证明了这些网络的隐式正则化现象，同时提供了严格的速率保证，确保梯度的指数级快速收敛。

    arXiv:2402.17595v1 Announce Type: cross  Abstract: The phenomenon of implicit regularization has attracted interest in recent years as a fundamental aspect of the remarkable generalizing ability of neural networks. In a nutshell, it entails that gradient descent dynamics in many neural nets, even without any explicit regularizer in the loss function, converges to the solution of a regularized learning problem. However, known results attempting to theoretically explain this phenomenon focus overwhelmingly on the setting of linear neural nets, and the simplicity of the linear structure is particularly crucial to existing arguments. In this paper, we explore this problem in the context of more realistic neural networks with a general class of non-linear activation functions, and rigorously demonstrate the implicit regularization phenomenon for such networks in the setting of matrix sensing problems, together with rigorous rate guarantees that ensure exponentially fast convergence of gradi
    
[^40]: FaultProfIT: 大规模云系统中故障票据的分层故障分析

    FaultProfIT: Hierarchical Fault Profiling of Incident Tickets in Large-scale Cloud Systems

    [https://arxiv.org/abs/2402.17583](https://arxiv.org/abs/2402.17583)

    提出了一种名为FaultProfIT的自动化方法，用于处理大规模云系统中的故障模式分析，填补了手动标记的缺陷。

    

    事后分析在云系统中的事件管理中至关重要，它为改进系统的可靠性和稳健性提供了宝贵的见解。在CloudA，故障模式分析是在事后阶段执行的，涉及将事件故障分类为独特类别，称为故障模式。通过汇总和分析这些故障模式，工程师可以识别常见故障、脆弱组件和新出现的故障趋势。然而，这一过程目前是通过手动标记进行的，存在固有缺陷。一方面，事件数量庞大意味着只分析了最严重的事件，导致对故障模式的偏斜概述。另一方面，任务的复杂性需要广泛的领域知识，这导致错误和不一致性。为了解决这些限制，我们提出了一种自动化方法，名为FaultProfIT，用于处理故障模式。

    arXiv:2402.17583v1 Announce Type: cross  Abstract: Postmortem analysis is essential in the management of incidents within cloud systems, which provides valuable insights to improve system's reliability and robustness. At CloudA, fault pattern profiling is performed during the postmortem phase, which involves the classification of incidents' faults into unique categories, referred to as fault pattern. By aggregating and analyzing these fault patterns, engineers can discern common faults, vulnerable components and emerging fault trends. However, this process is currently conducted by manual labeling, which has inherent drawbacks. On the one hand, the sheer volume of incidents means only the most severe ones are analyzed, causing a skewed overview of fault patterns. On the other hand, the complexity of the task demands extensive domain knowledge, which leads to errors and inconsistencies. To address these limitations, we propose an automated approach, named FaultProfIT, for Fault pattern 
    
[^41]: 超维计算：生物数据的快速、稳健和可解释范式

    Hyperdimensional computing: a fast, robust and interpretable paradigm for biological data

    [https://arxiv.org/abs/2402.17572](https://arxiv.org/abs/2402.17572)

    超维计算作为一种新兴技术，在生物信息学中具有高效、可解释和能够处理多模态数据的潜力。

    

    生物信息学的进步主要归因于处理多样生物数据源的新算法。高级的序列比对算法已经在分析生物序列方面发挥了关键作用，深度学习已经大幅改变了生物信息学，解决了序列、结构和功能分析的问题。然而，这些方法非常依赖数据，计算密集且难以解释。超维计算（HDC）最近作为一种有趣的替代方案出现。其关键思想是高维度的随机向量可以表示诸如序列同一性或系统发育等概念。然后，利用高维空间的特殊属性，可以使用简单运算符组合这些向量进行学习、推理或查询。我们的工作审查并探讨了HDC在生物信息学中的潜力，强调了其效率、可解释性以及处理多模态和结构化数据的熟练程度。

    arXiv:2402.17572v1 Announce Type: new  Abstract: Advances in bioinformatics are primarily due to new algorithms for processing diverse biological data sources. While sophisticated alignment algorithms have been pivotal in analyzing biological sequences, deep learning has substantially transformed bioinformatics, addressing sequence, structure, and functional analyses. However, these methods are incredibly data-hungry, compute-intensive and hard to interpret. Hyperdimensional computing (HDC) has recently emerged as an intriguing alternative. The key idea is that random vectors of high dimensionality can represent concepts such as sequence identity or phylogeny. These vectors can then be combined using simple operators for learning, reasoning or querying by exploiting the peculiar properties of high-dimensional spaces. Our work reviews and explores the potential of HDC for bioinformatics, emphasizing its efficiency, interpretability, and adeptness in handling multimodal and structured da
    
[^42]: 稀疏变分受干扰噪声高斯过程回归用于地磁扰动预测

    Sparse Variational Contaminated Noise Gaussian Process Regression for Forecasting Geomagnetic Perturbations

    [https://arxiv.org/abs/2402.17570](https://arxiv.org/abs/2402.17570)

    本文提出了一种稀疏变分受干扰噪声高斯过程回归框架，用于更好地处理异方差方差和离群噪声，应用于地磁扰动预测，并展示了更短的预测间隔和类似的覆盖精度。

    

    高斯过程（GP）已成为处理复杂协方差结构数据集的基于核的机器学习方法。本文提出一种新的GP框架扩展，使用受干扰的正态似然函数更好地考虑异方差方差和离群噪声。我们提出了基于稀疏变分高斯过程（SVGP）方法的可扩展推断算法，用于拟合具有受干扰正态噪声的稀疏高斯过程回归模型的大型数据集。我们考察了地磁地面扰动的应用，其中最先进的预测模型基于神经网络。我们展示了与人工密集的神经网络基线相比，我们的方法产生了更短的预测间隔，但具有相似的覆盖范围和准确性。

    arXiv:2402.17570v1 Announce Type: new  Abstract: Gaussian Processes (GP) have become popular machine learning methods for kernel based learning on datasets with complicated covariance structures. In this paper, we present a novel extension to the GP framework using a contaminated normal likelihood function to better account for heteroscedastic variance and outlier noise. We propose a scalable inference algorithm based on the Sparse Variational Gaussian Process (SVGP) method for fitting sparse Gaussian process regression models with contaminated normal noise on large datasets. We examine an application to geomagnetic ground perturbations, where the state-of-art prediction model is based on neural networks. We show that our approach yields shorter predictions intervals for similar coverage and accuracy when compared to an artificial dense neural network baseline.
    
[^43]: 评估预测可靠性以培养人工智能的信任。多发性硬化症案例研究

    Evaluation of Predictive Reliability to Foster Trust in Artificial Intelligence. A case study in Multiple Sclerosis

    [https://arxiv.org/abs/2402.17554](https://arxiv.org/abs/2402.17554)

    通过提出一种方法，评估人工智能预测的可靠性，使决策者能够根据其可靠性来接受或拒绝预测结果

    

    在关键背景（如医学）中应用人工智能（AI）和机器学习（ML）需要实施安全措施，以降低在预测错误的情况下造成的伤害风险。当ML预测用于指导临床决策时，发现ML失败至关重要。ML预测可靠性衡量了ML预测在新示例上的信任度，从而使决策者能够根据其可靠性接受或拒绝。为了评估可靠性，我们提出了一种实现两个原则的方法。首先，我们的方法评估要分类的示例是否来自训练集的相同分布。为此，我们利用自动编码器（AE）重建具有低误差的训练集的能力。如果AE将示例重构为高误差，则认为该示例是“分布外”（OOD）。其次，评估ML分类器的性能是否良好

    arXiv:2402.17554v1 Announce Type: new  Abstract: Applying Artificial Intelligence (AI) and Machine Learning (ML) in critical contexts, such as medicine, requires the implementation of safety measures to reduce risks of harm in case of prediction errors. Spotting ML failures is of paramount importance when ML predictions are used to drive clinical decisions. ML predictive reliability measures the degree of trust of a ML prediction on a new instance, thus allowing decision-makers to accept or reject it based on its reliability. To assess reliability, we propose a method that implements two principles. First, our approach evaluates whether an instance to be classified is coming from the same distribution of the training set. To do this, we leverage Autoencoders (AEs) ability to reconstruct the training set with low error. An instance is considered Out-of-Distribution (OOD) if the AE reconstructs it with a high error. Second, it is evaluated whether the ML classifier has good performances 
    
[^44]: 通过可调变换将学习的图像编解码器适应屏幕内容

    Adapting Learned Image Codecs to Screen Content via Adjustable Transformations

    [https://arxiv.org/abs/2402.17544](https://arxiv.org/abs/2402.17544)

    引入可调变换的学习图像编解码器，通过神经网络前后滤波器提高屏幕内容压缩效率并减少编码伪影，比基线模型节省10%比特率。

    

    随着学习的图像编解码器（LICs）变得越来越普遍，它们对于超出分布数据的低编码效率成为某些应用的瓶颈。为了提高LICs在屏幕内容（SC）图像上的性能，同时又不破坏向后兼容性，我们提出引入参数化和可逆线性变换到编码管线中，而不改变基线编解码器的操作流。我们设计了两个神经网络作为我们设置中的前置滤波器和后置滤波器，以增加编码效率并帮助恢复编码伪影。我们的端到端训练解决方案在SC压缩上相比基线LICs实现高达10%的比特率节省，同时只引入1%的额外参数。

    arXiv:2402.17544v1 Announce Type: cross  Abstract: As learned image codecs (LICs) become more prevalent, their low coding efficiency for out-of-distribution data becomes a bottleneck for some applications. To improve the performance of LICs for screen content (SC) images without breaking backwards compatibility, we propose to introduce parameterized and invertible linear transformations into the coding pipeline without changing the underlying baseline codec's operation flow. We design two neural networks to act as prefilters and postfilters in our setup to increase the coding efficiency and help with the recovery from coding artifacts. Our end-to-end trained solution achieves up to 10% bitrate savings on SC compression compared to the baseline LICs while introducing only 1% extra parameters.
    
[^45]: 标签噪声鲁棒扩散模型

    Label-Noise Robust Diffusion Models

    [https://arxiv.org/abs/2402.17517](https://arxiv.org/abs/2402.17517)

    Transition-aware weighted Denoising Score Matching（TDSM）是用于训练带有嘈杂标签的条件扩散模型的新方法，通过加权得分网络和过渡概率来提高生成样本质量。

    

    有条件扩散模型在各种生成任务中表现出色，但训练它们需要包含有噪声的条件输入的大规模数据集，即嘈杂标签。本文提出了用于训练有噪声标签的条件扩散模型的Transition-aware weighted Denoising Score Matching (TDSM)，这是扩散模型领域中的首次研究。TDSM目标包含得分网络的加权和，结合了实例级和时间相关的标签转移概率。我们引入了一种过渡感知的权重估计器，利用了一个与扩散过程明显定制化的时间相关嘈杂标签分类器。通过在各种数据集和嘈杂标签设置上的实验，TDSM提高了生成的样本质量与给定条件一致。

    arXiv:2402.17517v1 Announce Type: new  Abstract: Conditional diffusion models have shown remarkable performance in various generative tasks, but training them requires large-scale datasets that often contain noise in conditional inputs, a.k.a. noisy labels. This noise leads to condition mismatch and quality degradation of generated data. This paper proposes Transition-aware weighted Denoising Score Matching (TDSM) for training conditional diffusion models with noisy labels, which is the first study in the line of diffusion models. The TDSM objective contains a weighted sum of score networks, incorporating instance-wise and time-dependent label transition probabilities. We introduce a transition-aware weight estimator, which leverages a time-dependent noisy-label classifier distinctively customized to the diffusion process. Through experiments across various datasets and noisy label settings, TDSM improves the quality of generated samples aligned with given conditions. Furthermore, our 
    
[^46]: QUCE: 减少和量化基于路径的不确定性以生成对抗性反事实解释

    QUCE: The Minimisation and Quantification of Path-Based Uncertainty for Generative Counterfactual Explanations

    [https://arxiv.org/abs/2402.17516](https://arxiv.org/abs/2402.17516)

    QUCE方法旨在通过减少路径不确定性来量化和缓解基于路径的不确定性，从而改善对抗性反事实解释的表现。

    

    arXiv:2402.17516v1 公告类型：跨学科 深度神经网络（DNNs）作为机器学习领域最突出的方法之一。DNNs的有效性随着最近计算能力的增加而激增，使得这些方法能够扩展到处理大数据中的重要复杂性以应对预测挑战。然而，随着DNN模型复杂性的提高，可解释性降低。针对这一挑战，诸如对抗梯度整合（AGI）这样的可解释模型利用DNN提供的基于路径的梯度来阐明它们的决策。然而，当梯度在越界路径遍历期间表现出不规则性时，基于路径的解释器的性能可能会受到损害。在这种情况下，我们介绍了Quantified Uncertainty Counterfactual Explanations（QUCE），这是一种旨在减少路径不确定性的方法，以缓解越界遍历。 QUCE不仅在提出解释时量化不确定性

    arXiv:2402.17516v1 Announce Type: cross  Abstract: Deep Neural Networks (DNNs) stand out as one of the most prominent approaches within the Machine Learning (ML) domain. The efficacy of DNNs has surged alongside recent increases in computational capacity, allowing these approaches to scale to significant complexities for addressing predictive challenges in big data. However, as the complexity of DNN models rises, interpretability diminishes. In response to this challenge, explainable models such as Adversarial Gradient Integration (AGI) leverage path-based gradients provided by DNNs to elucidate their decisions. Yet the performance of path-based explainers can be compromised when gradients exhibit irregularities during out-of-distribution path traversal. In this context, we introduce Quantified Uncertainty Counterfactual Explanations (QUCE), a method designed to mitigate out-of-distribution traversal by minimizing path uncertainty. QUCE not only quantifies uncertainty when presenting e
    
[^47]: 热力学导向稀缺时间动态数据的超分辨率

    Thermodynamics-informed super-resolution of scarce temporal dynamics data

    [https://arxiv.org/abs/2402.17506](https://arxiv.org/abs/2402.17506)

    提出了一种利用热力学感知神经网络来提高物理系统测量分辨率并预测时间演化的方法，采用对抗自动编码器和结构保持神经网络相结合的方式，可有效解决超分辨率问题，并确保满足热力学定律。

    

    我们提出了一种利用了热力学感知神经网络来提高物理系统测量分辨率并随后预测其时间演化的方法。我们的方法使用对抗自动编码器，将完整模型的维度降低到一组潜变量，这些潜变量被强制匹配先验，例如正态分布。对抗自动编码器被视为生成模型，它们可以被训练以从低分辨率输入生成高分辨率样本，也就是可以解决所谓的超分辨率问题。然后，第二个神经网络被训练以学习潜变量的物理结构并预测其时间演化。这个神经网络被称为保持结构的神经网络。它学习系统的metriplectic结构，并应用物理偏差以确保热力学的第一和第二定律被遵守。

    arXiv:2402.17506v1 Announce Type: cross  Abstract: We present a method to increase the resolution of measurements of a physical system and subsequently predict its time evolution using thermodynamics-aware neural networks. Our method uses adversarial autoencoders, which reduce the dimensionality of the full order model to a set of latent variables that are enforced to match a prior, for example a normal distribution. Adversarial autoencoders are seen as generative models, and they can be trained to generate high-resolution samples from low-resoution inputs, meaning they can address the so-called super-resolution problem. Then, a second neural network is trained to learn the physical structure of the latent variables and predict their temporal evolution. This neural network is known as an structure-preserving neural network. It learns the metriplectic-structure of the system and applies a physical bias to ensure that the first and second principles of thermodynamics are fulfilled. The i
    
[^48]: 重症监护作为一个大型序列建模问题

    Intensive Care as One Big Sequence Modeling Problem

    [https://arxiv.org/abs/2402.17501](https://arxiv.org/abs/2402.17501)

    将医疗保健视为序列建模问题，通过将患者与医疗提供者之间的交互表示为事件流，实现对未来事件（如诊断和治疗选择）进行预测。

    

    在医疗保健中，强化学习通常涉及狭窄的自包含任务，如脓毒症预测或麻醉控制。然而，先前的研究表明，通用模型（主要示例为大型语言模型）具有超越特定任务方法的潜力，因为它们具有隐式迁移学习的能力。为了实现保健基础模型的训练以及利用最先进的Transformer架构的能力，我们提出了保健作为序列建模的范式，其中患者和医疗提供者之间的交互被表示为事件流，诊断和治疗选择等任务被建模为对流中未来事件的预测。为了在实验中探索这一范式，我们开发了MIMIC-SEQ，这是一个序列建模基准，通过将来自MIMIC-IV数据集的异构临床记录转换为一种统一...

    arXiv:2402.17501v1 Announce Type: cross  Abstract: Reinforcement Learning in Healthcare is typically concerned with narrow self-contained tasks such as sepsis prediction or anesthesia control. However, previous research has demonstrated the potential of generalist models (the prime example being Large Language Models) to outperform task-specific approaches due to their capability for implicit transfer learning. To enable training of foundation models for Healthcare as well as leverage the capabilities of state of the art Transformer architectures, we propose the paradigm of Healthcare as Sequence Modeling, in which interaction between the patient and the healthcare provider is represented as an event stream and tasks like diagnosis and treatment selection are modeled as prediction of future events in the stream. To explore this paradigm experimentally we develop MIMIC-SEQ, a sequence modeling benchmark derived by translating heterogenous clinical records from MIMIC-IV dataset into a un
    
[^49]: 预测复杂振荡器网络中的不稳定性: 网络特征与机器学习的局限性与潜力

    Predicting Instability in Complex Oscillator Networks: Limitations and Potentials of Network Measures and Machine Learning

    [https://arxiv.org/abs/2402.17500](https://arxiv.org/abs/2402.17500)

    通过研究发现，对于振荡系统的同步稳定性预测，网络特征无法可靠地进行预测，只有通过结合所有网络特征和节点机器学习才能匹敌图神经网络(GNNs)的性能。

    

    网络科学的一个核心问题是系统的功能特性如何从其结构中产生。对于网络动力系统，结构通常用网络特征来量化。对于振荡系统而言，一个理论上和实际上感兴趣的功能特性是同步稳定性对局部扰动的响应。最近，图神经网络（GNNs）已被证明成功地预测这种稳定性; 与此同时，网络特征却难以描绘出清晰的图景。在这里，我们收集了46个相关的网络特征，发现没有任何小的子集能够可靠地预测稳定性。GNNs的性能只有通过结合所有网络特征和节点机器学习才能匹敌。然而，与GNNs不同，这种方法无法从网络集合推广到几种真实的电力网络拓扑结构。这表明网络特征与功能的相关性可能具有误导性，而GNNs

    arXiv:2402.17500v1 Announce Type: cross  Abstract: A central question of network science is how functional properties of systems arise from their structure. For networked dynamical systems, structure is typically quantified with network measures. A functional property that is of theoretical and practical interest for oscillatory systems is the stability of synchrony to localized perturbations. Recently, Graph Neural Networks (GNNs) have been shown to predict this stability successfully; at the same time, network measures have struggled to paint a clear picture. Here we collect 46 relevant network measures and find that no small subset can reliably predict stability. The performance of GNNs can only be matched by combining all network measures and nodewise machine learning. However, unlike GNNs, this approach fails to extrapolate from network ensembles to several real power grid topologies. This suggests that correlations of network measures and function may be misleading, and that GNNs
    
[^50]: syren-halofit: 一种快速、可解释、高精度的$\Lambda$CDM非线性物质功率谱公式

    syren-halofit: A fast, interpretable, high-precision formula for the $\Lambda$CDM nonlinear matter power spectrum

    [https://arxiv.org/abs/2402.17492](https://arxiv.org/abs/2402.17492)

    通过符号回归获得了简单的解析逼近，重新优化了halofit的系数以拟合各种宇宙学和红移范围，利用符号回归探索了用于拟合残差的解析表达式空间，所有方法均经过$N$体模拟验证。

    

    在宇宙学中，快速准确地评估非线性物质功率谱$P(k)$关于宇宙学参数和红移的函数是非常重要的。我们使用符号回归获得了关于halofit模型所需的非线性尺度$k_\sigma$、有效谱指数$n_{\rm eff}$和曲率$C$的简单解析逼近。然后，我们重新优化halofit的系数以适应广泛的宇宙学和红移范围。之后，我们再次利用符号回归来探索用于拟合$P(k)$与halofit优化预测之间残差的解析表达式空间。所有方法都经过与$N$体模拟的验证。

    arXiv:2402.17492v1 Announce Type: cross  Abstract: Rapid and accurate evaluation of the nonlinear matter power spectrum, $P(k)$, as a function of cosmological parameters and redshift is of fundamental importance in cosmology. Analytic approximations provide an interpretable solution, yet current approximations are neither fast nor accurate relative to black-box numerical emulators. We use symbolic regression to obtain simple analytic approximations to the nonlinear scale, $k_\sigma$, the effective spectral index, $n_{\rm eff}$, and the curvature, $C$, which are required for the halofit model. We then re-optimise the coefficients of halofit to fit a wide range of cosmologies and redshifts. We then again exploit symbolic regression to explore the space of analytic expressions to fit the residuals between $P(k)$ and the optimised predictions of halofit. All methods are validated against $N$-body simulations. Our symbolic expressions for $k_\sigma$, $n_{\rm eff}$ and $C$ have root mean squ
    
[^51]: JPEG-AI验证模型中比特率匹配算法的优化

    Bit Rate Matching Algorithm Optimization in JPEG-AI Verification Model

    [https://arxiv.org/abs/2402.17487](https://arxiv.org/abs/2402.17487)

    该研究对JPEG-AI验证模型中的比特率匹配算法进行了优化，以提高其性能和速度。

    

    基于神经网络（NN）的图像压缩研究表现出优越的性能，与传统压缩框架相比有着更紧凑的比特表示，并在并行设备上实现更快的编码速度。这些特性引起了科学界和工业界的关注，导致了JPEG-AI标准化活动的展开。JPEG-AI标准化过程的验证模型已经在开发中，并且已超越先进的VVC intra编解码器。为了生成带有所需每像素位数的重建图像，并评估JPEG-AI验证模型和VVC intra的BD-率性能，采用了比特率匹配。然而，当前JPEG-AI验证模型处于比特率匹配过程中经历了显着的减速。

    arXiv:2402.17487v1 Announce Type: cross  Abstract: The research on neural network (NN) based image compression has shown superior performance compared to classical compression frameworks. Unlike the hand-engineered transforms in the classical frameworks, NN-based models learn the non-linear transforms providing more compact bit representations, and achieve faster coding speed on parallel devices over their classical counterparts. Those properties evoked the attention of both scientific and industrial communities, resulting in the standardization activity JPEG-AI. The verification model for the standardization process of JPEG-AI is already in development and has surpassed the advanced VVC intra codec. To generate reconstructed images with the desired bits per pixel and assess the BD-rate performance of both the JPEG-AI verification model and VVC intra, bit rate matching is employed. However, the current state of the JPEG-AI verification model experiences significant slowdowns during bit
    
[^52]: 通过融合全局和局部关系交互进行欺诈检测

    Fraud Detection with Binding Global and Local Relational Interaction

    [https://arxiv.org/abs/2402.17472](https://arxiv.org/abs/2402.17472)

    这项工作提出了一个名为RAGFormer的新框架，同时将局部和全局特征嵌入目标节点，以改进欺诈检测性能。

    

    图神经网络已被证明对于欺诈检测具有有效性，因为它能够在整体视角中编码节点交互和聚合特征。最近，具有出色序列编码能力的Transformer网络在文献中也表现出优于其他基于GNN的方法。然而，基于GNN和基于Transformer的网络只编码整个图的一个视角，而GNN编码全局特征，Transformer网络编码局部特征。此外，先前的工作忽视了使用单独网络编码异构图的全局交互特征，导致性能不佳。在这项工作中，我们提出了一个称为Relation-Aware GNN with transFormer（RAGFormer）的新颖框架，将局部和全局特征同时嵌入目标节点中。这个简单而有效的网络应用了一个修改后的GAGA模块，其中每个Transformer层后面都跟着一个跨关系聚合层。

    arXiv:2402.17472v1 Announce Type: cross  Abstract: Graph Neural Network has been proved to be effective for fraud detection for its capability to encode node interaction and aggregate features in a holistic view. Recently, Transformer network with great sequence encoding ability, has also outperformed other GNN-based methods in literatures. However, both GNN-based and Transformer-based networks only encode one perspective of the whole graph, while GNN encodes global features and Transformer network encodes local ones. Furthermore, previous works ignored encoding global interaction features of the heterogeneous graph with separate networks, thus leading to suboptimal performance. In this work, we present a novel framework called Relation-Aware GNN with transFormer (RAGFormer) which simultaneously embeds local and global features into a target node. The simple yet effective network applies a modified GAGA module where each transformer layer is followed by a cross-relation aggregation lay
    
[^53]: 位分布研究与JPEG-AI标准化中空间质量图的实现

    Bit Distribution Study and Implementation of Spatial Quality Map in the JPEG-AI Standardization

    [https://arxiv.org/abs/2402.17470](https://arxiv.org/abs/2402.17470)

    研究表明，JPEG-AI标准化中的灵活位分布结构可以提高图像压缩性能，并超过经典编解码器VVC intra。

    

    目前，对基于神经网络的图像压缩编解码器有很高的需求。这些编解码器采用非线性变换来创建紧凑的位表示，并在设备上比经典框架中使用的手工制作的变换实现更快的编码速度。科学界和工业界对这些特性非常感兴趣，这导致了JPEG-AI的标准化工作。JPEG-AI验证模型已发布，目前正在进行标准化开发。利用神经网络，它可以在基本操作点上比经典编解码器VVC intra提高10%以上的BD率。研究人员将这一成功归因于空间域中的灵活位分布，与VVC intra的锚点相反，后者生成一个恒定质量点。然而，我们的研究揭示了VVC intra通过实现更具适应性的位分布结构

    arXiv:2402.17470v1 Announce Type: cross  Abstract: Currently, there is a high demand for neural network-based image compression codecs. These codecs employ non-linear transforms to create compact bit representations and facilitate faster coding speeds on devices compared to the hand-crafted transforms used in classical frameworks. The scientific and industrial communities are highly interested in these properties, leading to the standardization effort of JPEG-AI. The JPEG-AI verification model has been released and is currently under development for standardization. Utilizing neural networks, it can outperform the classic codec VVC intra by over 10% BD-rate operating at base operation point. Researchers attribute this success to the flexible bit distribution in the spatial domain, in contrast to VVC intra's anchor that is generated with a constant quality point. However, our study reveals that VVC intra displays a more adaptable bit distribution structure through the implementation of 
    
[^54]: 为什么学习速率具有迁移性？调和深度学习中的优化和尺度极限

    Why do Learning Rates Transfer? Reconciling Optimization and Scaling Limits for Deep Learning

    [https://arxiv.org/abs/2402.17457](https://arxiv.org/abs/2402.17457)

    学习速率迁移现象可以归因于在μP和其深度延伸下，训练损失Hessian矩阵的最大特征值（即锐度）在较长时间的训练过程中，基本独立于网络的宽度和深度。

    

    最近，越来越多的证据表明，如果神经网络的宽度和深度朝着所谓的丰富特征学习极限（μP及其深度扩展）进行缩放，那么某些超参数 - 例如学习速率 - 就会从小模型转移到非常大的模型，从而降低了超参数调整的成本。从优化的角度来看，这种现象令人困惑，因为它意味着损失景观在非常不同的模型尺寸之间是非常一致的。在这项工作中，我们找到证据支持学习速率迁移可以归因于事实：在μP及其深度扩展下，训练损失Hessian的最大特征值（即锐度）在较长的训练时间内在很大程度上独立于网络的宽度和深度。另一方面，我们展示在神经切线核（NTK）体系下，锐度在不同

    arXiv:2402.17457v1 Announce Type: new  Abstract: Recently, there has been growing evidence that if the width and depth of a neural network are scaled toward the so-called rich feature learning limit ($\mu$P and its depth extension), then some hyperparameters - such as the learning rate - exhibit transfer from small to very large models, thus reducing the cost of hyperparameter tuning. From an optimization perspective, this phenomenon is puzzling, as it implies that the loss landscape is remarkably consistent across very different model sizes. In this work, we find empirical evidence that learning rate transfer can be attributed to the fact that under $\mu$P and its depth extension, the largest eigenvalue of the training loss Hessian (i.e. the sharpness) is largely independent of the width and depth of the network for a sustained period of training time. On the other hand, we show that under the neural tangent kernel (NTK) regime, the sharpness exhibits very different dynamics at differ
    
[^55]: DS-Agent：通过赋予大型语言模型案例推理能力实现自动化数据科学

    DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning

    [https://arxiv.org/abs/2402.17453](https://arxiv.org/abs/2402.17453)

    DS-Agent是一个自动框架，结合了大型语言模型代理和案例推理，能够在数据科学任务中灵活利用专家知识并通过反馈机制持续改善性能

    

    在这项工作中，我们研究了基于大型语言模型（LLMs）代理的潜力，以自动化数据科学任务，目标是理解任务要求，然后构建和训练最合适的机器学习模型。尽管现有的LLM代理取得了广泛成功，但在这种情景下生成不合理的实验计划受到阻碍。为此，我们提出了DS-Agent，这是一个利用LLM代理和案例推理（CBR）的新颖自动化框架。在开发阶段，DS-Agent遵循CBR框架来构建自动迭代流水线，可以灵活利用来自Kaggle的专业知识，并通过反馈机制促进一致的性能改进。此外，DS-Agent实现了一个低资源部署阶段，采用简化的CBR范例来适应开发阶段成功解决方案，以进行直接代码生成，显著减少了...

    arXiv:2402.17453v1 Announce Type: new  Abstract: In this work, we investigate the potential of large language models (LLMs) based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models. Despite their widespread success, existing LLM agents are hindered by generating unreasonable experiment plans within this scenario. To this end, we present DS-Agent, a novel automatic framework that harnesses LLM agent and case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism. Moreover, DS-Agent implements a low-resource deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct code generation, significantly reducing th
    
[^56]: 基于原则的架构感知超参数缩放

    Principled Architecture-aware Scaling of Hyperparameters

    [https://arxiv.org/abs/2402.17440](https://arxiv.org/abs/2402.17440)

    通过精确定位初始化和最大学习速率对网络结构的依赖性，本文可以将初始化和学习速率推广到MLP和CNN中，以适用于不同的神经网络架构。

    

    训练高质量的深度神经网络需要选择合适的超参数，这是一个非常重要且昂贵的过程。当前的研究试图自动优化或设计超参数的原则，以便它们能够推广到多样的未知场景。然而，大多数设计或优化方法对网络结构的选择一无所知，因此在很大程度上忽略了神经架构对超参数的影响。本文准确表征了初始化和最大学习速率对网络架构的依赖性，包括网络深度、宽度、卷积核大小和连接模式。通过追求使每个参数在预激活中具有相同的均方变化，我们可以将初始化和学习速率推广到具有复杂图拓扑结构的多层感知器（MLP）和卷积神经网络（CNN）中。

    arXiv:2402.17440v1 Announce Type: new  Abstract: Training a high-quality deep neural network requires choosing suitable hyperparameters, which is a non-trivial and expensive process. Current works try to automatically optimize or design principles of hyperparameters, such that they can generalize to diverse unseen scenarios. However, most designs or optimization methods are agnostic to the choice of network structures, and thus largely ignore the impact of neural architectures on hyperparameters. In this work, we precisely characterize the dependence of initializations and maximal learning rates on the network architecture, which includes the network depth, width, convolutional kernel size, and connectivity patterns. By pursuing every parameter to be maximally updated with the same mean squared change in pre-activations, we can generalize our initialization and learning rates across MLPs (multi-layer perception) and CNNs (convolutional neural network) with sophisticated graph topologie
    
[^57]: KANDY基准：使用坎丁斯基模式进行增量神经符号学习和推理

    The KANDY Benchmark: Incremental Neuro-Symbolic Learning and Reasoning with Kandinsky Patterns

    [https://arxiv.org/abs/2402.17431](https://arxiv.org/abs/2402.17431)

    本文介绍了KANDY基准框架，通过生成基于坎丁斯基模式的学习和推理任务，提出了持续和半监督学习的新挑战，并着重研究符号组成性。

    

    arXiv:2402.17431v1 公告类型：新的 摘要：人工智能不断寻求新的挑战和基准，以有效衡量性能并推动最新技术的发展。本文介绍了KANDY，一个可用于生成受坎丁斯基模式启发的各种学习和推理任务的基准框架。通过创建一系列具有递增复杂性和稀疏监督的二元分类任务课程，KANDY可用于实现持续和半监督学习的基准，并专注于符号组成性。基本事实中还提供了分类规则，以便分析可解释的解决方案。除了基准生成管道，我们还发布了两个课程，一个更容易一个更难，我们提议这些作为研究社区的新挑战。通过彻底的实验评估，我们展示了最先进的神经模型和纯符号方法的表现。

    arXiv:2402.17431v1 Announce Type: new  Abstract: Artificial intelligence is continuously seeking novel challenges and benchmarks to effectively measure performance and to advance the state-of-the-art. In this paper we introduce KANDY, a benchmarking framework that can be used to generate a variety of learning and reasoning tasks inspired by Kandinsky patterns. By creating curricula of binary classification tasks with increasing complexity and with sparse supervisions, KANDY can be used to implement benchmarks for continual and semi-supervised learning, with a specific focus on symbol compositionality. Classification rules are also provided in the ground truth to enable analysis of interpretable solutions. Together with the benchmark generation pipeline, we release two curricula, an easier and a harder one, that we propose as new challenges for the research community. With a thorough experimental evaluation, we show how both state-of-the-art neural models and purely symbolic approaches 
    
[^58]: 加强上下文黑盒优化

    Reinforced In-Context Black-Box Optimization

    [https://arxiv.org/abs/2402.17423](https://arxiv.org/abs/2402.17423)

    提出了一种从离线数据中端到端地强化学习黑盒优化算法的方法，通过使用表达能力强的序列模型和后悔-前进令牌来获取任务信息并做出决策。

    

    黑盒优化（BBO）已经在许多科学和工程领域取得成功应用。最近，人们越来越关注元学习BBO算法的特定组件，以加快优化速度并摆脱繁琐的手工启发式算法。作为扩展，从数据中学习整个算法需要专家最少的工作量，并且可以提供最大的灵活性。在本文中，我们提出了一种名为RIBBO的方法，可以以端到端的方式从离线数据中强化学习BBO算法。RIBBO利用表达能力强的序列模型来学习多个行为算法和任务产生的优化历史，利用大型模型的上下文学习能力来提取任务信息并相应地做出决策。我们方法的核心是通过增加后悔-前进令牌来增强优化历史，这些令牌旨在基于累积表现来表示算法的性能。

    arXiv:2402.17423v1 Announce Type: cross  Abstract: Black-Box Optimization (BBO) has found successful applications in many fields of science and engineering. Recently, there has been a growing interest in meta-learning particular components of BBO algorithms to speed up optimization and get rid of tedious hand-crafted heuristics. As an extension, learning the entire algorithm from data requires the least labor from experts and can provide the most flexibility. In this paper, we propose RIBBO, a method to reinforce-learn a BBO algorithm from offline data in an end-to-end fashion. RIBBO employs expressive sequence models to learn the optimization histories produced by multiple behavior algorithms and tasks, leveraging the in-context learning ability of large models to extract task information and make decisions accordingly. Central to our method is to augment the optimization histories with regret-to-go tokens, which are designed to represent the performance of an algorithm based on cumul
    
[^59]: 傅里叶域插值神经网络的图像空间形式主义用于噪声传播分析

    A novel image space formalism of Fourier domain interpolation neural networks for noise propagation analysis

    [https://arxiv.org/abs/2402.17410](https://arxiv.org/abs/2402.17410)

    提出了一种用于MRI重建的傅里叶域插值神经网络的图像空间形式主义，并分析了在CNN推断过程中噪声传播的估计方法。

    

    旨在为MRI重建中的图像域插值开发多层卷积神经网络（CNNs）的图像空间形式主义，并在CNN推断过程中对噪声传播进行分析。通过使用复值整流线性单元在傅里叶域（也称为k空间）中的非线性激活，将其表示为与激活掩模的逐元素乘法。这种操作在图像空间中转换为卷积。在k空间网络训练后，这种方法为相对于别名线圈图像的重建图像的导数提供了一个代数表达式，这些别名线圈图像作为图像空间中网络的输入张量。这使得可以通过分析估计网络推断中的方差，并用于描述噪声特性。通过蒙特卡洛模拟和基于自动微分的数值方法进行验证。

    arXiv:2402.17410v1 Announce Type: cross  Abstract: Purpose: To develop an image space formalism of multi-layer convolutional neural networks (CNNs) for Fourier domain interpolation in MRI reconstructions and analytically estimate noise propagation during CNN inference. Theory and Methods: Nonlinear activations in the Fourier domain (also known as k-space) using complex-valued Rectifier Linear Units are expressed as elementwise multiplication with activation masks. This operation is transformed into a convolution in the image space. After network training in k-space, this approach provides an algebraic expression for the derivative of the reconstructed image with respect to the aliased coil images, which serve as the input tensors to the network in the image space. This allows the variance in the network inference to be estimated analytically and to be used to describe noise characteristics. Monte-Carlo simulations and numerical approaches based on auto-differentiation were used for val
    
[^60]: LSPT：用于视觉表示学习的长期空间提示调整

    LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning

    [https://arxiv.org/abs/2402.17406](https://arxiv.org/abs/2402.17406)

    LSPT是一种革命性的视觉表示学习方法，通过引入长期门控提示，巧妙地利用长距离先前块作为提示的潜在来源，减轻了遗忘参数的风险。

    

    视觉提示调整（VPT）技术因其通过专用的可学习令牌（称为提示）将预训练的视觉Transformer（ViT）调整到下游视觉任务而闻名。在自监督视觉Transformer中使用的当代VPT方法通常默认引入来源自模型先前块的新可学习提示或门控提示令牌。这种方法的一个关键缺失是未利用长距离先前块作为每个自监督ViT内提示的潜力来源。为了弥补这一重要差距，我们引入了长期空间提示调整（LSPT）- 一种革命性的视觉表示学习方法。 LSPT从人类大脑的复杂性中汲取灵感，巧妙地结合了长期门控提示。这个特性作为时间编码，减轻了遗忘参数的风险。

    arXiv:2402.17406v1 Announce Type: cross  Abstract: Visual Prompt Tuning (VPT) techniques have gained prominence for their capacity to adapt pre-trained Vision Transformers (ViTs) to downstream visual tasks using specialized learnable tokens termed as prompts. Contemporary VPT methodologies, especially when employed with self-supervised vision transformers, often default to the introduction of new learnable prompts or gated prompt tokens predominantly sourced from the model's previous block. A pivotal oversight in such approaches is their failure to harness the potential of long-range previous blocks as sources of prompts within each self-supervised ViT. To bridge this crucial gap, we introduce Long-term Spatial Prompt Tuning (LSPT) - a revolutionary approach to visual representation learning. Drawing inspiration from the intricacies of the human brain, LSPT ingeniously incorporates long-term gated prompts. This feature serves as temporal coding, curbing the risk of forgetting parameter
    
[^61]: Beacon，一个用于流控制的轻量级深度强化学习基准库

    Beacon, a lightweight deep reinforcement learning benchmark library for flow control

    [https://arxiv.org/abs/2402.17402](https://arxiv.org/abs/2402.17402)

    Beacon是一个开源基准库，用于流控制，包含7个轻量级的1D和2D问题，有助于提高深度强化学习算法对数值流体动力学环境的适应性和可重现性。

    

    最近，深度强化学习在流控制问题中的日益增多的应用导致了一个新的研究领域，专注于将现有算法与数值流体动力学环境的控制耦合和调整。尽管这个领域仍处于萌芽阶段，但在短时间内取得了多次成功，其快速发展速度肯定部分归功于推动社区扩大的开源努力。然而，这一新兴领域仍缺乏一个共同基础，来确保结果的可重现性，并提供适当的专门基准。为此，我们提出了Beacon，一个开源基准库，由七个轻量级1D和2D流控制问题组成，具有不同的特征、行动和观察空间特征以及CPU需求。在这篇论文中，描述了七个考虑的问题，并提供了参考控制解决方案

    arXiv:2402.17402v1 Announce Type: cross  Abstract: Recently, the increasing use of deep reinforcement learning for flow control problems has led to a new area of research, focused on the coupling and the adaptation of the existing algorithms to the control of numerical fluid dynamics environments. Although still in its infancy, the field has seen multiple successes in a short time span, and its fast development pace can certainly be partly imparted to the open-source effort that drives the expansion of the community. Yet, this emerging domain still misses a common ground to (i) ensure the reproducibility of the results, and (ii) offer a proper ad-hoc benchmarking basis. To this end, we propose Beacon, an open-source benchmark library composed of seven lightweight 1D and 2D flow control problems with various characteristics, action and observation space characteristics, and CPU requirements. In this contribution, the seven considered problems are described, and reference control solutio
    
[^62]: 量子方法研究合成少数类过采样技术（SMOTE）

    A Quantum Approach to Synthetic Minority Oversampling Technique (SMOTE)

    [https://arxiv.org/abs/2402.17398](https://arxiv.org/abs/2402.17398)

    使用量子计算技术提出了Quantum-SMOTE方法，可以解决机器学习数据集中的类别不平衡问题，并引入了旋转角度、少数类百分比和分裂因子等超参数，实现了对合成数据生成过程的更好控制。

    

    这篇论文提出了Quantum-SMOTE方法，这是一种使用量子计算技术来解决机器学习数据集中普遍存在的类别不平衡问题的新颖解决方案。Quantum-SMOTE受到合成少数类过采样技术（SMOTE）的启发，利用量子过程如交换测试和量子旋转生成合成数据点。该方法与传统的SMOTE算法使用K-最近邻（KNN）和欧氏距离的方式有所不同，能够从少数类数据点生成合成实例而无需依赖于邻近性。算法通过引入旋转角度、少数类百分比和分裂因子等超参数，可以更好地控制合成数据生成过程，从而实现对特定数据集需求的定制。该方法在TelecomChurn公共数据集上进行了测试，并与两种主要的分类算法进行了评估。

    arXiv:2402.17398v1 Announce Type: cross  Abstract: The paper proposes the Quantum-SMOTE method, a novel solution that uses quantum computing techniques to solve the prevalent problem of class imbalance in machine learning datasets. Quantum-SMOTE, inspired by the Synthetic Minority Oversampling Technique (SMOTE), generates synthetic data points using quantum processes such as swap tests and quantum rotation. The process varies from the conventional SMOTE algorithm's usage of K-Nearest Neighbors (KNN) and Euclidean distances, enabling synthetic instances to be generated from minority class data points without relying on neighbor proximity. The algorithm asserts greater control over the synthetic data generation process by introducing hyperparameters such as rotation angle, minority percentage, and splitting factor, which allow for customization to specific dataset requirements. The approach is tested on a public dataset of TelecomChurn and evaluated alongside two prominent classification
    
[^63]: 针对安全机器学习模型更新的鲁棒一致对抗训练

    Robustness-Congruent Adversarial Training for Secure Machine Learning Model Updates

    [https://arxiv.org/abs/2402.17390](https://arxiv.org/abs/2402.17390)

    通过鲁棒一致对抗训练技术，解决了更新机器学习模型时对抗性鲁棒性和系统安全性的问题。

    

    机器学习模型需要定期更新以提高其平均准确度，利用新颖的架构和额外的数据。然而，新更新的模型可能会犯以前模型未曾犯过的错误。这种误分类被称为负翻转，并被用户体验为性能的退化。在本文中，我们展示了这个问题也影响对抗性样本的鲁棒性，从而阻碍了安全模型更新实践的发展。特别是，当更新模型以提高其对抗性鲁棒性时，一些先前无效的对抗性样本可能会被错误分类，导致系统安全性的认知退化。我们提出了一种名为鲁棒一致对抗训练的新技术来解决这个问题。它涉及使用对抗训练对模型进行微调，同时约束其在对抗性示例上保持更高的鲁棒性。

    arXiv:2402.17390v1 Announce Type: new  Abstract: Machine-learning models demand for periodic updates to improve their average accuracy, exploiting novel architectures and additional data. However, a newly-updated model may commit mistakes that the previous model did not make. Such misclassifications are referred to as negative flips, and experienced by users as a regression of performance. In this work, we show that this problem also affects robustness to adversarial examples, thereby hindering the development of secure model update practices. In particular, when updating a model to improve its adversarial robustness, some previously-ineffective adversarial examples may become misclassified, causing a regression in the perceived security of the system. We propose a novel technique, named robustness-congruent adversarial training, to address this issue. It amounts to fine-tuning a model with adversarial training, while constraining it to retain higher robustness on the adversarial examp
    
[^64]: 优化时间步长加速扩散采样

    Accelerating Diffusion Sampling with Optimized Time Steps

    [https://arxiv.org/abs/2402.17376](https://arxiv.org/abs/2402.17376)

    提出了一个通用框架用于设计优化问题，旨在通过寻找更合适的时间步长加速扩散采样。

    

    扩散概率模型（DPMs）在高分辨率图像合成中表现出色，但由于通常需要大量采样步骤，其采样效率仍有待提高。近期高阶数值ODE求解器在DPMs中的应用使得用更少的采样步骤生成高质量图像成为可能。尽管这是一项重大进展，大多数采样方法仍然采用均匀时间步长，而在采样步骤较少时并不是最佳选择。为解决这一问题，我们提出了一个通用框架，用于设计一个优化问题，该优化问题旨在为DPMs的特定数值ODE求解器寻找更合适的时间步长。此优化问题旨在最小化地实现地真实解与与数值求解器对应的近似解之间的距离。它可以通过受限信赖域方法进行高效求解，时间少于

    arXiv:2402.17376v1 Announce Type: cross  Abstract: Diffusion probabilistic models (DPMs) have shown remarkable performance in high-resolution image synthesis, but their sampling efficiency is still to be desired due to the typically large number of sampling steps. Recent advancements in high-order numerical ODE solvers for DPMs have enabled the generation of high-quality images with much fewer sampling steps. While this is a significant development, most sampling methods still employ uniform time steps, which is not optimal when using a small number of steps. To address this issue, we propose a general framework for designing an optimization problem that seeks more appropriate time steps for a specific numerical ODE solver for DPMs. This optimization problem aims to minimize the distance between the ground-truth solution to the ODE and an approximate solution corresponding to the numerical solver. It can be efficiently solved using the constrained trust region method, taking less than 
    
[^65]: 连续时间控制中积分强化学习中计算的影响

    Impact of Computation in Integral Reinforcement Learning for Continuous-Time Control

    [https://arxiv.org/abs/2402.17375](https://arxiv.org/abs/2402.17375)

    计算方法选择会显著影响连续时间控制中积分强化学习的性能表现

    

    积分强化学习(IntRL)在政策评估(PEV)阶段需要精确计算效用函数的积分。这是通过积分规则实现的，即来自离散时间中获得的状态样本评估的效用函数的加权总和。我们的研究揭示了一个关键但被忽视的现象：计算方法的选择--在本例中是积分规则--可以显著影响控制性能。这种影响可追溯到引入于PEV阶段的计算错误可能影响政策迭代的收敛行为，进而影响所学控制器。为了阐明计算如何影响控制，我们将IntRL的政策迭代与应用于哈密顿-雅可比-贝尔曼方程的牛顿法进行了类比。在这种光下，PEV中的计算误差表现为牛顿法的每次迭代中的额外误差项

    arXiv:2402.17375v1 Announce Type: cross  Abstract: Integral reinforcement learning (IntRL) demands the precise computation of the utility function's integral at its policy evaluation (PEV) stage. This is achieved through quadrature rules, which are weighted sums of utility functions evaluated from state samples obtained in discrete time. Our research reveals a critical yet underexplored phenomenon: the choice of the computational method -- in this case, the quadrature rule -- can significantly impact control performance. This impact is traced back to the fact that computational errors introduced in the PEV stage can affect the policy iteration's convergence behavior, which in turn affects the learned controller. To elucidate how computation impacts control, we draw a parallel between IntRL's policy iteration and Newton's method applied to the Hamilton-Jacobi-Bellman equation. In this light, computational error in PEV manifests as an extra error term in each iteration of Newton's method
    
[^66]: CGGM：一种具有自适应稀疏性的条件图生成模型，用于物联网网络中节点异常检测

    CGGM: A conditional graph generation model with adaptive sparsity for node anomaly detection in IoT networks

    [https://arxiv.org/abs/2402.17363](https://arxiv.org/abs/2402.17363)

    CGGM是一种新颖的图生成模型，通过自适应稀疏性生成邻接矩阵，解决了物联网网络中节点异常检测中节点类别不平衡的问题

    

    动态图被广泛用于检测物联网中节点的异常行为。生成模型通常用于解决动态图中节点类别不平衡的问题。然而，它面临的约束包括邻接关系的单调性，为节点构建多维特征的困难，以及缺乏端到端生成多类节点的方法。本文提出了一种名为CGGM的新颖图生成模型，专门设计用于生成少数类别中更多节点。通过自适应稀疏性生成邻接矩阵的机制增强了其结构的灵活性。特征生成模块名为多维特征生成器（MFG），可生成包括拓扑信息在内的节点特征。标签被转换为嵌入向量，用作条件。

    arXiv:2402.17363v1 Announce Type: cross  Abstract: Dynamic graphs are extensively employed for detecting anomalous behavior in nodes within the Internet of Things (IoT). Generative models are often used to address the issue of imbalanced node categories in dynamic graphs. Nevertheless, the constraints it faces include the monotonicity of adjacency relationships, the difficulty in constructing multi-dimensional features for nodes, and the lack of a method for end-to-end generation of multiple categories of nodes. This paper presents a novel graph generation model, called CGGM, designed specifically to generate a larger number of nodes belonging to the minority class. The mechanism for generating an adjacency matrix, through adaptive sparsity, enhances flexibility in its structure. The feature generation module, called multidimensional features generator (MFG) to generate node features along with topological information. Labels are transformed into embedding vectors, serving as condition
    
[^67]: 通过输入子域级别损失函数梯度解析理解 PINNs 对经济翼非稳定流训练

    Understanding the training of PINNs for unsteady flow past a plunging foil through the lens of input subdomain level loss function gradients

    [https://arxiv.org/abs/2402.17346](https://arxiv.org/abs/2402.17346)

    通过分析损失函数梯度统计和样本点比例，研究了PINNs对经济翼非稳定流的训练过程中各个输入空间子域的影响。

    

    最近，受浸入边界方法启发的物理信息神经网络（PINNs），包括移动边界启用的 PINNs（MB-PINNs），已经显示出在经济翼非稳定流经过运动体时准确重建速度和恢复压力作为隐藏变量的能力。考虑到经济翼的流动，MB-PINNs 在全局物理损失放松和物理学基础下采样方法的影响下进行了训练，获得了良好的准确性。本研究的目的是通过物理损失放松和基于物理的下采样方法，调查哪个输入空间子域对训练有贡献。在 MB-PINNs 训练的背景下，定义了三个空间区域：运动体、尾迹和外部区域。为了量化哪个空间区域驱动了训练，从分区损失分量梯度统计和每个区域中样本点的比例计算了两个新的指标。

    arXiv:2402.17346v1 Announce Type: cross  Abstract: Recently immersed boundary method-inspired physics-informed neural networks (PINNs) including the moving boundary-enabled PINNs (MB-PINNs) have shown the ability to accurately reconstruct velocity and recover pressure as a hidden variable for unsteady flow past moving bodies. Considering flow past a plunging foil, MB-PINNs were trained with global physics loss relaxation and also in conjunction with a physics-based undersampling method, obtaining good accuracy. The purpose of this study was to investigate which input spatial subdomain contributes to the training under the effect of physics loss relaxation and physics-based undersampling. In the context of MB-PINNs training, three spatial zones: the moving body, wake, and outer zones were defined. To quantify which spatial zone drives the training, two novel metrics are computed from the zonal loss component gradient statistics and the proportion of sample points in each zone. Results c
    
[^68]: LocalGCL：面向图的局部感知对比学习

    LocalGCL: Local-aware Contrastive Learning for Graphs

    [https://arxiv.org/abs/2402.17345](https://arxiv.org/abs/2402.17345)

    提出了一种名为LocalGCL的新的自监督学习框架，通过掩码建模补充地捕捉局部图信息，优于传统对比学习方法。

    

    图表示学习（GRL）最近取得了相当大的进展，它将图与拓扑结构编码为低维嵌入。同时，手动注释图标签的耗时和成本高昂促使自监督学习（SSL）技术的发展。作为SSL的主要方法，对比学习（CL）通过区分正负样本来学习具有区分性的表示。然而，当应用于图数据时，它过分强调全局模式而忽视了局部结构。为了解决以上问题，我们提出了一种自监督学习框架，即面向图的局部感知对比学习（\methname），与普通对比学习相比，它通过基于掩码的建模补充地捕捉局部图信息。大量实验证实了\methname 的优越性。

    arXiv:2402.17345v1 Announce Type: cross  Abstract: Graph representation learning (GRL) makes considerable progress recently, which encodes graphs with topological structures into low-dimensional embeddings. Meanwhile, the time-consuming and costly process of annotating graph labels manually prompts the growth of self-supervised learning (SSL) techniques. As a dominant approach of SSL, Contrastive learning (CL) learns discriminative representations by differentiating between positive and negative samples. However, when applied to graph data, it overemphasizes global patterns while neglecting local structures. To tackle the above issue, we propose \underline{Local}-aware \underline{G}raph \underline{C}ontrastive \underline{L}earning (\textbf{\methnametrim}), a self-supervised learning framework that supplementarily captures local graph information with masking-based modeling compared with vanilla contrastive learning. Extensive experiments validate the superiority of \methname against st
    
[^69]: 通过优先建模抽象属性增强贝叶斯优化

    Enhanced Bayesian Optimization via Preferential Modeling of Abstract Properties

    [https://arxiv.org/abs/2402.17343](https://arxiv.org/abs/2402.17343)

    本文提出了一种人工智能协作的贝叶斯框架，通过将专家对未被测量的抽象属性的偏好纳入到替代建模中，进一步提升了贝叶斯优化的性能。

    

    实验设计优化是设计和发现新产品和流程的关键驱动因素。贝叶斯优化（BO）是优化昂贵和黑盒实验设计过程的有效工具。本文提出了一个人工智能协作的贝叶斯框架，将专家对未被测量的抽象属性的偏好纳入到替代建模中，以进一步提升BO的性能。我们提供了一种高效的策略，可以处理任何不正确/误导性的专家偏见。我们讨论了我们的方法的收敛行为。

    arXiv:2402.17343v1 Announce Type: new  Abstract: Experimental (design) optimization is a key driver in designing and discovering new products and processes. Bayesian Optimization (BO) is an effective tool for optimizing expensive and black-box experimental design processes. While Bayesian optimization is a principled data-driven approach to experimental optimization, it learns everything from scratch and could greatly benefit from the expertise of its human (domain) experts who often reason about systems at different abstraction levels using physical properties that are not necessarily directly measured (or measurable). In this paper, we propose a human-AI collaborative Bayesian framework to incorporate expert preferences about unmeasured abstract properties into the surrogate modeling to further boost the performance of BO. We provide an efficient strategy that can also handle any incorrect/misleading expert bias in preferential judgments. We discuss the convergence behavior of our pr
    
[^70]: 使用深度学习在无线电传播路径上重建室外环境

    Outdoor Environment Reconstruction with Deep Learning on Radio Propagation Paths

    [https://arxiv.org/abs/2402.17336](https://arxiv.org/abs/2402.17336)

    本文提出了一种利用环境无线信号进行室外环境重建的新方法，并通过深度学习技术对RF数据进行分析，填补了该领域的研究空白。

    

    传统的室外环境重建方法主要依赖于视觉技术，如摄影测量和激光雷达，面临着诸如覆盖范围受限、容易受环境条件影响、计算和能源需求高等限制。本文提出了一种新颖的方法，利用环境中的环境无线信号进行室外环境重建。通过分析射频（RF）数据，本文旨在推断环境特征并数字重建室外环境。研究在合成RF数据集WAIR-D上选定的深度学习（DL）技术的有效性，努力填补这一领域的研究空白。

    arXiv:2402.17336v1 Announce Type: cross  Abstract: Conventional methods for outdoor environment reconstruction rely predominantly on vision-based techniques like photogrammetry and LiDAR, facing limitations such as constrained coverage, susceptibility to environmental conditions, and high computational and energy demands. These challenges are particularly pronounced in applications like augmented reality navigation, especially when integrated with wearable devices featuring constrained computational resources and energy budgets. In response, this paper proposes a novel approach harnessing ambient wireless signals for outdoor environment reconstruction. By analyzing radio frequency (RF) data, the paper aims to deduce the environmental characteristics and digitally reconstruct the outdoor surroundings. Investigating the efficacy of selected deep learning (DL) techniques on the synthetic RF dataset WAIR-D, the study endeavors to address the research gap in this domain. Two DL-driven appro
    
[^71]: 基于聚类敏感度采样的数据高效学习：基础模型及其延伸

    Data-Efficient Learning via Clustering-Based Sensitivity Sampling: Foundation Models and Beyond

    [https://arxiv.org/abs/2402.17327](https://arxiv.org/abs/2402.17327)

    通过基于聚类和敏感度采样的数据选择方法，可以高效选择代表性数据子集来训练机器学习模型，在微调基础模型方面表现优异。

    

    我们研究了数据选择问题，其目的是选择一个小的代表性数据子集，可以用来高效地训练机器学习模型。我们提出了一种基于$k$-means聚类和敏感度采样的新数据选择方法。假设我们可以访问数据的嵌入表示，其中模型损失是H\"older连续的，我们的方法可以选择一组“典型”的$k + 1/\varepsilon^2$个元素，这些元素的平均损失与整个数据集的平均损失对应，乘以一个$(1\pm\varepsilon)$的因子并加上一个$\varepsilon \lambda \Phi_k$，其中$\Phi_k$代表输入嵌入的$k$-means成本，$\lambda$是H\"older常数。此外，我们展示了我们的方法在微调基础模型上的性能和可扩展性，并表明它胜过了最先进的方法。

    arXiv:2402.17327v1 Announce Type: new  Abstract: We study the data selection problem, whose aim is to select a small representative subset of data that can be used to efficiently train a machine learning model. We present a new data selection approach based on $k$-means clustering and sensitivity sampling. Assuming access to an embedding representation of the data with respect to which the model loss is H\"older continuous, our approach provably allows selecting a set of ``typical'' $k + 1/\varepsilon^2$ elements whose average loss corresponds to the average loss of the whole dataset, up to a multiplicative $(1\pm\varepsilon)$ factor and an additive $\varepsilon \lambda \Phi_k$, where $\Phi_k$ represents the $k$-means cost for the input embeddings and $\lambda$ is the H\"older constant.   We furthermore demonstrate the performance and scalability of our approach on fine-tuning foundation models and show that it outperforms state-of-the-art methods. We also show how it can be applied on
    
[^72]: 通过增强的辅助网络扩展监督本地学习

    Scaling Supervised Local Learning with Augmented Auxiliary Networks

    [https://arxiv.org/abs/2402.17318](https://arxiv.org/abs/2402.17318)

    本文提出了一种增强的本地学习方法AugLocal，通过构建辅助网络来增强各隐藏层之间的协同作用，从而解决了本地学习方法在大规模网络中与BP方法之间存在的精度差距问题。

    

    深度神经网络通常使用全局误差信号进行端到端反向传播（BP）进行训练，这既不符合生物学实际，也存在更新锁定问题，并且需要大量内存消耗。本文提出了一种增强的本地学习方法，称为AugLocal。AugLocal通过从其后续网络层中均匀选择一小部分层来构建每个隐藏层的辅助网络，以增强它们的协同作用。

    arXiv:2402.17318v1 Announce Type: cross  Abstract: Deep neural networks are typically trained using global error signals that backpropagate (BP) end-to-end, which is not only biologically implausible but also suffers from the update locking problem and requires huge memory consumption. Local learning, which updates each layer independently with a gradient-isolated auxiliary network, offers a promising alternative to address the above problems. However, existing local learning methods are confronted with a large accuracy gap with the BP counterpart, particularly for large-scale networks. This is due to the weak coupling between local layers and their subsequent network layers, as there is no gradient communication across layers. To tackle this issue, we put forward an augmented local learning method, dubbed AugLocal. AugLocal constructs each hidden layer's auxiliary network by uniformly selecting a small subset of layers from its subsequent network layers to enhance their synergy. We al
    
[^73]: 如何赢得BraTS 2023成年胶质瘤挑战？假装而已！增强的合成数据增强和模型集成用于脑肿瘤分割

    How we won BraTS 2023 Adult Glioma challenge? Just faking it! Enhanced Synthetic Data Augmentation and Model Ensemble for brain tumour segmentation

    [https://arxiv.org/abs/2402.17317](https://arxiv.org/abs/2402.17317)

    通过使用生成对抗网络和配准来增强合成数据，我们成功训练了三个不同的深度学习模型，结合卷积算法和transformers技术填补了知识差距，取得了0.9005的dice结果。

    

    深度学习是颅内肿瘤分割的最先进技术，但这需要大量高质量数据，尤其在医学领域难以获得。因此，我们的解决方案通过使用非传统的数据增强机制来解决这个问题。生成对抗网络和配准被用来大量增加可用样本数，用于训练三个不同的深度学习模型，分别用于颅内肿瘤分割的BraTS2023挑战的第一个任务。第一个模型是标准nnU-Net，第二个是Swin UNETR，第三个是BraTS 2021挑战的获胜方案。整个流程基于nnU-Net实现，除了合成数据的生成。卷积算法和transformers的使用能够填补彼此的知识差距。使用新指标，我们的最佳解决方案达到了0.9005的dice结果。

    arXiv:2402.17317v1 Announce Type: cross  Abstract: Deep Learning is the state-of-the-art technology for segmenting brain tumours. However, this requires a lot of high-quality data, which is difficult to obtain, especially in the medical field. Therefore, our solutions address this problem by using unconventional mechanisms for data augmentation. Generative adversarial networks and registration are used to massively increase the amount of available samples for training three different deep learning models for brain tumour segmentation, the first task of the BraTS2023 challenge. The first model is the standard nnU-Net, the second is the Swin UNETR and the third is the winning solution of the BraTS 2021 Challenge. The entire pipeline is built on the nnU-Net implementation, except for the generation of the synthetic data. The use of convolutional algorithms and transformers is able to fill each other's knowledge gaps. Using the new metric, our best solution achieves the dice results 0.9005
    
[^74]: 持久图的量子距离逼近

    Quantum Distance Approximation for Persistence Diagrams

    [https://arxiv.org/abs/2402.17295](https://arxiv.org/abs/2402.17295)

    探索了量子计算机在估计持久图之间距离方面的潜力，提出了用于Wasserstein距离和$d^{c}_{p}$距离的变分量子算法

    

    拓扑数据分析方法对于在许多不同领域中的分类和聚类任务可能会很有用，因为它们能够提供总结关于潜在复杂和高维数据集形状的重要信息的二维持久图。持久图的空间可以赋予各种度量，比如Wasserstein距离，其具有统计结构，并允许将这些总结用于机器学习算法。然而，计算两个持久图之间的距离涉及找到两个图的点之间的最佳匹配方式，对于传统计算机来说可能并不总是一项容易的任务。在这项工作中，我们探讨了量子计算机评估持久图之间距离的潜力，特别是我们提出了用于Wasserstein距离和$d^{c}_{p}$距离的变分量子算法。

    arXiv:2402.17295v1 Announce Type: cross  Abstract: Topological Data Analysis methods can be useful for classification and clustering tasks in many different fields as they can provide two dimensional persistence diagrams that summarize important information about the shape of potentially complex and high dimensional data sets. The space of persistence diagrams can be endowed with various metrics such as the Wasserstein distance which admit a statistical structure and allow to use these summaries for machine learning algorithms. However, computing the distance between two persistence diagrams involves finding an optimal way to match the points of the two diagrams and may not always be an easy task for classical computers. In this work we explore the potential of quantum computers to estimate the distance between persistence diagrams, in particular we propose variational quantum algorithms for the Wasserstein distance as well as the $d^{c}_{p}$ distance. Our implementation is a weighted 
    
[^75]: 一个可解释的生成模型熵值新颖性评估

    An Interpretable Evaluation of Entropy-based Novelty of Generative Models

    [https://arxiv.org/abs/2402.17287](https://arxiv.org/abs/2402.17287)

    提出了一种用于评估生成模型新颖性的基于核的熵新颖性 (KEN) 分数

    

    生成模型框架和架构的巨大发展需要有原则的方法来评估模型相对于参考数据集或基线生成模型的新颖性。 虽然最近的文献已广泛研究了生成模型的质量、多样性和泛化能力的评估，但与基线模型相比的模型新颖性评估在机器学习社区中尚未得到充分研究。在这项工作中，我们关注多模态生成模型下的新颖性评估，并尝试回答以下问题：给定生成模型 $\mathcal{G}$ 的样本和参考数据集 $\mathcal{S}$，我们如何发现并计算 $\mathcal{G}$ 比 $\mathcal{S}$ 中更频繁地表达的模式。 我们介绍了一种谱方法来描述这一任务，并提出了基于核的熵新颖性 (KEN) 分数来量化基于模式的新颖性

    arXiv:2402.17287v1 Announce Type: new  Abstract: The massive developments of generative model frameworks and architectures require principled methods for the evaluation of a model's novelty compared to a reference dataset or baseline generative models. While the recent literature has extensively studied the evaluation of the quality, diversity, and generalizability of generative models, the assessment of a model's novelty compared to a baseline model has not been adequately studied in the machine learning community. In this work, we focus on the novelty assessment under multi-modal generative models and attempt to answer the following question: Given the samples of a generative model $\mathcal{G}$ and a reference dataset $\mathcal{S}$, how can we discover and count the modes expressed by $\mathcal{G}$ more frequently than in $\mathcal{S}$. We introduce a spectral approach to the described task and propose the Kernel-based Entropic Novelty (KEN) score to quantify the mode-based novelty 
    
[^76]: 多智能体、人智能体及其进展：合作在社会困境中的调查

    Multi-Agent, Human-Agent and Beyond: A Survey on Cooperation in Social Dilemmas

    [https://arxiv.org/abs/2402.17270](https://arxiv.org/abs/2402.17270)

    调查了多智能体、人智能体和人工智能智能体在社会困境合作中的三个关键领域，讨论了合作的动机、策略、人类偏见，以及未来研究方向。

    

    在社会困境中研究合作长期以来一直是各种学科的基本课题，包括计算机科学和社会科学。人工智能领域的最新进展显著重塑了这一领域，为理解和增强合作提供了新的见解。本调查考察了人工智能和社会困境合作交汇处的三个关键领域。首先，着重于多智能体合作，我们审查了支持理性智能体之间合作的内在和外在动机，以及用于制定有效策略对抗不同对手的方法。其次，探讨了人智能体合作，我们讨论了当前用于与人类合作的人工智能算法，以及人类对人工智能智能体的偏见。第三，我们审查了利用人工智能智能体增强人类合作的新兴领域。最后，我们讨论了未来研究方向，例如 u

    arXiv:2402.17270v1 Announce Type: new  Abstract: The study of cooperation within social dilemmas has long been a fundamental topic across various disciplines, including computer science and social science. Recent advancements in Artificial Intelligence (AI) have significantly reshaped this field, offering fresh insights into understanding and enhancing cooperation. This survey examines three key areas at the intersection of AI and cooperation in social dilemmas. First, focusing on multi-agent cooperation, we review the intrinsic and external motivations that support cooperation among rational agents, and the methods employed to develop effective strategies against diverse opponents. Second, looking into human-agent cooperation, we discuss the current AI algorithms for cooperating with humans and the human biases towards AI agents. Third, we review the emergent field of leveraging AI agents to enhance cooperation among humans. We conclude by discussing future research avenues, such as u
    
[^77]: 课程学习遇见有向无环图进行多模态情感识别

    Curriculum Learning Meets Directed Acyclic Graph for Multimodal Emotion Recognition

    [https://arxiv.org/abs/2402.17269](https://arxiv.org/abs/2402.17269)

    使用有向无环图和课程学习相结合的新方法，提升多模态情感识别模型在处理情感变化和数据不平衡方面的性能。

    

    这篇论文提出了一种新颖的方法，称为MultiDAG+CL，用于会话中的多模态情感识别（ERC），它利用有向无环图（DAG）在一个统一的框架内集成文本、声音和视觉特征。模型通过课程学习（CL）进行增强，以应对情感变化和数据不平衡带来的挑战。课程学习通过逐渐以有意义的顺序呈现训练样本来促进学习过程，从而提高模型处理情绪变化和数据不平衡的性能。在IEMOCAP和MELD数据集上的实验结果表明，MultiDAG+CL模型优于基线模型。

    arXiv:2402.17269v1 Announce Type: new  Abstract: Emotion recognition in conversation (ERC) is a crucial task in natural language processing and affective computing. This paper proposes MultiDAG+CL, a novel approach for Multimodal Emotion Recognition in Conversation (ERC) that employs Directed Acyclic Graph (DAG) to integrate textual, acoustic, and visual features within a unified framework. The model is enhanced by Curriculum Learning (CL) to address challenges related to emotional shifts and data imbalance. Curriculum learning facilitates the learning process by gradually presenting training samples in a meaningful order, thereby improving the model's performance in handling emotional variations and data imbalance. Experimental results on the IEMOCAP and MELD datasets demonstrate that the MultiDAG+CL models outperform baseline models.
    
[^78]: RIME: 具有嘈杂偏好的健壮偏好强化学习

    RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences

    [https://arxiv.org/abs/2402.17257](https://arxiv.org/abs/2402.17257)

    RIME是一种针对嘈杂偏好的健壮PbRL算法，通过动态过滤去噪偏好和热启动奖励模型，极大增强了现有最先进PbRL方法的鲁棒性。

    

    偏好强化学习（PbRL）通过利用人类偏好作为奖励信号，避免了对奖励设计的需求。然而，当前PbRL算法过度依赖来自领域专家的高质量反馈，导致缺乏鲁棒性。在本文中，我们提出了RIME，一种针对嘈杂偏好的健壮PbRL算法，用于有效地从嘈杂偏好中学习奖励。我们的方法结合了基于样本选择的鉴别器，动态过滤去噪偏好以进行健壮训练。为了减轻选择不正确造成的累积误差，我们提出热启动奖励模型，此外还能填补PbRL中从预训练到在线训练过渡时的性能差距。我们在机器人操纵和运动任务上的实验表明，RIME显著提升了当前最先进的PbRL方法的鲁棒性。消融研究进一步表明，热启动

    arXiv:2402.17257v1 Announce Type: cross  Abstract: Preference-based Reinforcement Learning (PbRL) avoids the need for reward engineering by harnessing human preferences as the reward signal. However, current PbRL algorithms over-reliance on high-quality feedback from domain experts, which results in a lack of robustness. In this paper, we present RIME, a robust PbRL algorithm for effective reward learning from noisy preferences. Our method incorporates a sample selection-based discriminator to dynamically filter denoised preferences for robust training. To mitigate the accumulated error caused by incorrect selection, we propose to warm start the reward model, which additionally bridges the performance gap during transition from pre-training to online training in PbRL. Our experiments on robotic manipulation and locomotion tasks demonstrate that RIME significantly enhances the robustness of the current state-of-the-art PbRL method. Ablation studies further demonstrate that the warm star
    
[^79]: 基于深度学习的语音和视觉合成在多层自适应框架中改进网络钓鱼攻击检测

    Deep Learning-Based Speech and Vision Synthesis to Improve Phishing Attack Detection through a Multi-layer Adaptive Framework

    [https://arxiv.org/abs/2402.17249](https://arxiv.org/abs/2402.17249)

    提出了一个结合深度学习和随机森林的自适应框架，通过在多个预测层中读取图像、合成语音以及进行自然语言处理，显著提高了网络钓鱼攻击检测的性能。

    

    攻击者不断演进的方式持续改进其欺骗技术，以绕过现有的最先进网络钓鱼检测方法，给行业和学术研究人员带来了巨大挑战，因为当前方法无法检测复杂的网络钓鱼攻击。因此，由于攻击者采用的策略日益复杂且新策略不断被开发以逃避检测，当前反网络钓鱼方法仍然容易受到复杂网络钓鱼攻击的影响。在这项研究中，我们提出了一个可调整的框架，将深度学习和随机森林结合起来，从深度伪造视频中读取图像，合成语音，以及自然语言处理，并在各种预测层中显著提高机器学习模型对网络钓鱼攻击检测的性能。

    arXiv:2402.17249v1 Announce Type: cross  Abstract: The ever-evolving ways attacker continues to im prove their phishing techniques to bypass existing state-of-the-art phishing detection methods pose a mountain of challenges to researchers in both industry and academia research due to the inability of current approaches to detect complex phishing attack. Thus, current anti-phishing methods remain vulnerable to complex phishing because of the increasingly sophistication tactics adopted by attacker coupled with the rate at which new tactics are being developed to evade detection. In this research, we proposed an adaptable framework that combines Deep learning and Randon Forest to read images, synthesize speech from deep-fake videos, and natural language processing at various predictions layered to significantly increase the performance of machine learning models for phishing attack detection.
    
[^80]: SDR-Former：用于3D多相成像的共焦双分辨率Transformer进行肝脏病变分类

    SDR-Former: A Siamese Dual-Resolution Transformer for Liver Lesion Classification Using 3D Multi-Phase Imaging

    [https://arxiv.org/abs/2402.17246](https://arxiv.org/abs/2402.17246)

    SDR-Former是一种针对肝脏病变分类而设计的新型Transformer框架，结合了共焦神经网络和混合双分辨率Transformer，能够在3D多相CT和MR成像中处理多相输入，提升特征提取能力。

    

    arXiv:2402.17246v1 公告类型：跨领域摘要：多相CT和MR扫描中肝脏病变的自动分类具有临床意义，但具有挑战性。本研究提出了一种新颖的Siamese Dual-Resolution Transformer（SDR-Former）框架，专为处理3D多相CT和MR成像中的肝脏病变分类问题而设计，具有不同相位计数。所提出的SDR-Former利用了简化的共焦神经网络（SNN）来处理多相成像输入，具有稳健的特征表示同时保持计算效率。SNN的权重共享特性通过混合双分辨率Transformer（DR-Former）进一步丰富，包括用于处理高分辨率和低分辨率图像的3D卷积神经网络（CNN）和定制化3D Transformer。这种混合子架构擅长捕捉详细的局部特征和理解全局上下文信息，从而提升SNN的特征提取能力。

    arXiv:2402.17246v1 Announce Type: cross  Abstract: Automated classification of liver lesions in multi-phase CT and MR scans is of clinical significance but challenging. This study proposes a novel Siamese Dual-Resolution Transformer (SDR-Former) framework, specifically designed for liver lesion classification in 3D multi-phase CT and MR imaging with varying phase counts. The proposed SDR-Former utilizes a streamlined Siamese Neural Network (SNN) to process multi-phase imaging inputs, possessing robust feature representations while maintaining computational efficiency. The weight-sharing feature of the SNN is further enriched by a hybrid Dual-Resolution Transformer (DR-Former), comprising a 3D Convolutional Neural Network (CNN) and a tailored 3D Transformer for processing high- and low-resolution images, respectively. This hybrid sub-architecture excels in capturing detailed local features and understanding global contextual information, thereby, boosting the SNN's feature extraction ca
    
[^81]: 负采样重要吗？对其理论和应用的综述与洞察

    Does Negative Sampling Matter? A Review with Insights into its Theory and Applications

    [https://arxiv.org/abs/2402.17238](https://arxiv.org/abs/2402.17238)

    提出了一个利用负采样的通用框架，对负采样的历史进行了深入探讨，将当前的负采样方法分类为静态、困难、基于GAN、基于辅助和批内方法

    

    负采样迅速成为研究的焦点，具有广泛的应用，涵盖机器学习、计算机视觉、自然语言处理、数据挖掘和推荐系统。这种日益增长的兴趣引发了几个关键问题：负采样真的很重要吗？是否存在一个能够整合所有现有负采样方法的通用框架？它在哪些领域被应用？针对这些问题，我们提出了一个利用负采样的通用框架。深入探讨了负采样的历史，我们通过五个演化路径追溯了负采样的发展。我们剖析并分类了选择负样本候选的策略，详细介绍了全局、局部、小批量、跳跃和基于内存的方法。我们的综述将当前的负采样方法分类为静态、困难、基于GAN、基于辅助和批内方法。

    arXiv:2402.17238v1 Announce Type: new  Abstract: Negative sampling has swiftly risen to prominence as a focal point of research, with wide-ranging applications spanning machine learning, computer vision, natural language processing, data mining, and recommender systems. This growing interest raises several critical questions: Does negative sampling really matter? Is there a general framework that can incorporate all existing negative sampling methods? In what fields is it applied? Addressing these questions, we propose a general framework that leverages negative sampling. Delving into the history of negative sampling, we trace the development of negative sampling through five evolutionary paths. We dissect and categorize the strategies used to select negative sample candidates, detailing global, local, mini-batch, hop, and memory-based approaches. Our review categorizes current negative sampling methods into five types: static, hard, GAN-based, Auxiliary-based, and In-batch methods, pr
    
[^82]: 个性化教育中的数据挖掘综述：当前趋势与未来展望

    A Review of Data Mining in Personalized Education: Current Trends and Future Prospects

    [https://arxiv.org/abs/2402.17236](https://arxiv.org/abs/2402.17236)

    这项研究综述了个性化教育中数据挖掘的最新进展，着重于教育推荐、认知诊断、知识追踪和学习分析四个主要场景，并提出了未来研究方向。

    

    个性化教育，根据个人学生需求定制，利用教育技术和人工智能在数字化时代增强学习效果。人工智能在教育平台中的整合为学术表现、学习偏好和行为提供了洞察，优化了个性化学习过程。由数据挖掘技术驱动，不仅使学生受益，同时为教育者和机构提供了工具，以打造定制化学习体验。该论文着重于四个主要场景：教育推荐、认知诊断、知识追踪和学习分析，以全面回顾个性化教育数据挖掘的最新进展。本文为每个领域提供了结构化分类，整理了常用数据集，并确定了未来的研究方向，强调了数据挖掘在增强个性化教育中的作用。

    arXiv:2402.17236v1 Announce Type: cross  Abstract: Personalized education, tailored to individual student needs, leverages educational technology and artificial intelligence (AI) in the digital age to enhance learning effectiveness. The integration of AI in educational platforms provides insights into academic performance, learning preferences, and behaviors, optimizing the personal learning process. Driven by data mining techniques, it not only benefits students but also provides educators and institutions with tools to craft customized learning experiences. To offer a comprehensive review of recent advancements in personalized educational data mining, this paper focuses on four primary scenarios: educational recommendation, cognitive diagnosis, knowledge tracing, and learning analysis. This paper presents a structured taxonomy for each area, compiles commonly used datasets, and identifies future research directions, emphasizing the role of data mining in enhancing personalized educat
    
[^83]: 随机梯度在赌博机问题中取得成功

    Stochastic Gradient Succeeds for Bandits

    [https://arxiv.org/abs/2402.17235](https://arxiv.org/abs/2402.17235)

    随机梯度赌博机算法以常数步长收敛到全局最优策略，无需额外的噪声控制，同时自动实现弱探索，确保每个动作被无限次采样。

    

    我们展示了\emph{随机梯度}赌博机算法以$O(1/t)$的速度收敛到一个\emph{全局最优}策略，即使采用\emph{恒定}步长。值得注意的是，尽管随机梯度赌博机算法是一个已知适用于赌博机问题的古老算法，但其全局收敛性以前尚未被证实。通过建立两个新颖的技术发现，我们取得了这一新结果：首先，梯度赌博机算法中随机更新的噪声满足强“增长条件”属性，即当进展变小时，方差会减小，这意味着通过减小步长来控制额外噪声是不必要的；其次，通过随机梯度更新自动实现了一种形式的“弱探索”，因为它们阻止行动概率以比$O(1/t)$更快的速度衰减，从而确保每个动作被无限次采样的概率。

    arXiv:2402.17235v1 Announce Type: new  Abstract: We show that the \emph{stochastic gradient} bandit algorithm converges to a \emph{globally optimal} policy at an $O(1/t)$ rate, even with a \emph{constant} step size. Remarkably, global convergence of the stochastic gradient bandit algorithm has not been previously established, even though it is an old algorithm known to be applicable to bandits. The new result is achieved by establishing two novel technical findings: first, the noise of the stochastic updates in the gradient bandit algorithm satisfies a strong ``growth condition'' property, where the variance diminishes whenever progress becomes small, implying that additional noise control via diminishing step sizes is unnecessary; second, a form of ``weak exploration'' is automatically achieved through the stochastic gradient updates, since they prevent the action probabilities from decaying faster than $O(1/t)$, thus ensuring that every action is sampled infinitely often with probabi
    
[^84]: 混合方块神经ODE因果建模

    Hybrid Square Neural ODE Causal Modeling

    [https://arxiv.org/abs/2402.17233](https://arxiv.org/abs/2402.17233)

    混合模型将基于ODE的机械动力学与神经网络组件结合，在解释性和因果基础的同时，利用领域知识对治疗效果进行排名，从而解决灵活性增加带来的因果基础丢失问题。

    

    混合模型将基于ODE的机械动力学与灵活且表达力强的神经网络组件结合起来。这种模型在科学领域越来越受欢迎，特别是在ODE-based建模提供重要解释性和经过验证的因果基础（例如，用于反事实推理）的领域。将机械模型纳入也为标准黑箱建模方法提供了归纳偏差，这在从小型数据集或部分观察到的复杂系统中学习时至关重要。不幸的是，随着混合模型变得更加灵活，机械模型提供的因果基础很快会丢失。我们通过利用另一个常见的领域知识来源来解决这个问题：对一组干预的治疗效果进行排名，即使准确的治疗效果不知道。我们在因果损失中编码这些信息，将其与标准预测损失相结合，得出混合损失。

    arXiv:2402.17233v1 Announce Type: new  Abstract: Hybrid models combine mechanistic ODE-based dynamics with flexible and expressive neural network components. Such models have grown rapidly in popularity, especially in scientific domains where such ODE-based modeling offers important interpretability and validated causal grounding (e.g., for counterfactual reasoning). The incorporation of mechanistic models also provides inductive bias in standard blackbox modeling approaches, critical when learning from small datasets or partially observed, complex systems. Unfortunately, as hybrid models become more flexible, the causal grounding provided by the mechanistic model can quickly be lost. We address this problem by leveraging another common source of domain knowledge: ranking of treatment effects for a set of interventions, even if the precise treatment effect is unknown. We encode this information in a causal loss that we combine with the standard predictive loss to arrive at a hybrid los
    
[^85]: 具有小参数的偏微分方程的双尺度神经网络

    Two-scale Neural Networks for Partial Differential Equations with Small Parameters

    [https://arxiv.org/abs/2402.17232](https://arxiv.org/abs/2402.17232)

    提出了一种用双尺度神经网络方法解决具有小参数的偏微分方程的方法，能够直接将小参数纳入神经网络架构中，从而简化解决过程，并能够合理准确地捕捉由小参数引起的解中大导数特征。

    

    我们提出了一种用物理信息神经网络（PINNs）解决具有小参数的偏微分方程（PDEs）的双尺度神经网络方法。我们直接将小参数纳入神经网络的架构中。所提出的方法使得以简单方式解决具有小参数的PDE成为可能，而无需添加傅里叶特征或其他计算繁琐的截断参数搜索。多个数值例子展示了在解决由小参数引起的解中大导数特征时的合理准确性。

    arXiv:2402.17232v1 Announce Type: cross  Abstract: We propose a two-scale neural network method for solving partial differential equations (PDEs) with small parameters using physics-informed neural networks (PINNs). We directly incorporate the small parameters into the architecture of neural networks. The proposed method enables solving PDEs with small parameters in a simple fashion, without adding Fourier features or other computationally taxing searches of truncation parameters. Various numerical examples demonstrate reasonable accuracy in capturing features of large derivatives in the solutions caused by small parameters.
    
[^86]: 在深度伪造检测中保持公平泛化

    Preserving Fairness Generalization in Deepfake Detection

    [https://arxiv.org/abs/2402.17229](https://arxiv.org/abs/2402.17229)

    本研究提出了第一种方法来解决深度伪造检测中的公平泛化问题，通过同时考虑特征、损失和优化方面，利用解耦学习来实现。

    

    虽然近年来已经开发出了有效的深度伪造检测模型，然而最近的研究表明，这些模型可能导致在人口统计学群体中（如种族和性别）出现不公平的性能差距。这可能导致特定群体面临不公平的定位或被排除在检测之外，潜在地允许被错误分类的深度伪造篡改公众舆论并破坏对模型的信任。现有的解决这一问题的方法是提供公平的损失函数。 它对于内域评估表现出良好的公平性能，但在跨域测试中无法保持公平性。这突显了在打击深度伪造中公平泛化的重要性。在这项工作中，我们提出了第一种方法，通过同时考虑特征、损失和优化方面，解决深度伪造检测中的公平泛化问题。我们的方法采用解耦学习来扩展

    arXiv:2402.17229v1 Announce Type: cross  Abstract: Although effective deepfake detection models have been developed in recent years, recent studies have revealed that these models can result in unfair performance disparities among demographic groups, such as race and gender. This can lead to particular groups facing unfair targeting or exclusion from detection, potentially allowing misclassified deepfakes to manipulate public opinion and undermine trust in the model. The existing method for addressing this problem is providing a fair loss function. It shows good fairness performance for intra-domain evaluation but does not maintain fairness for cross-domain testing. This highlights the significance of fairness generalization in the fight against deepfakes. In this work, we propose the first method to address the fairness generalization problem in deepfake detection by simultaneously considering features, loss, and optimization aspects. Our method employs disentanglement learning to ext
    
[^87]: 具有方差控制的自适应采样的高效反向传播

    Efficient Backpropagation with Variance-Controlled Adaptive Sampling

    [https://arxiv.org/abs/2402.17227](https://arxiv.org/abs/2402.17227)

    引入了方差控制的自适应采样（VCAS）方法，通过在数据维度和标记维度中进行重要性采样，控制采样比率来加速反向传播，并在多个任务中保持准确性。

    

    基于采样的算法在前向传播和/或反向传播中消除“不重要”的计算，为加速神经网络训练提供潜在解决方案。然而，由于采样引入了训练的近似，这些算法可能无法在各种任务中始终保持准确性。在本研究中，我们介绍了一种旨在加速反向传播的方差控制自适应采样（VCAS）方法。VCAS通过数据维度中的细粒度逐层重要性采样来计算无偏随机梯度以进行激活梯度计算，并通过标记维度中的杠杆分数采样来进行权重梯度计算。为了保持准确性，我们在训练过程中通过学习样本比率联合学习模型参数来控制额外的方差。我们在视觉和自然语言领域的多个微调和预训练任务上评估了VCAS。在所有任务上，VCAS能够保持原始的精度。

    arXiv:2402.17227v1 Announce Type: new  Abstract: Sampling-based algorithms, which eliminate ''unimportant'' computations during forward and/or back propagation (BP), offer potential solutions to accelerate neural network training. However, since sampling introduces approximations to training, such algorithms may not consistently maintain accuracy across various tasks. In this work, we introduce a variance-controlled adaptive sampling (VCAS) method designed to accelerate BP. VCAS computes an unbiased stochastic gradient with fine-grained layerwise importance sampling in data dimension for activation gradient calculation and leverage score sampling in token dimension for weight gradient calculation. To preserve accuracy, we control the additional variance by learning the sample ratio jointly with model parameters during training. We assessed VCAS on multiple fine-tuning and pre-training tasks in both vision and natural language domains. On all the tasks, VCAS can preserve the original tr
    
[^88]: 离线安全强化学习中的时间逻辑规范条件化决策转换器

    Temporal Logic Specification-Conditioned Decision Transformer for Offline Safe Reinforcement Learning

    [https://arxiv.org/abs/2402.17217](https://arxiv.org/abs/2402.17217)

    提出了时间逻辑规范条件化决策转换器（SDT）框架，结合信号时间逻辑（STL）和决策转换器（DT）的能力，比现有方法在离线安全强化学习中学习出更好的安全高奖励策略。

    

    离线安全强化学习旨在从固定数据集训练一个满足约束的策略。本文提出了一种新颖的框架，即时间逻辑规范条件化决策转换器（SDT），它利用信号时间逻辑（STL）的表达能力来指定代理应该遵循的复杂时间规则，以及决策转换器（DT）的顺序建模能力。对DSRL基准测试的实证评估表明，与现有方法相比，SDT在学习安全高奖励策略方面具有更好的能力。

    arXiv:2402.17217v1 Announce Type: cross  Abstract: Offline safe reinforcement learning (RL) aims to train a constraint satisfaction policy from a fixed dataset. Current state-of-the-art approaches are based on supervised learning with a conditioned policy. However, these approaches fall short in real-world applications that involve complex tasks with rich temporal and logical structures. In this paper, we propose temporal logic Specification-conditioned Decision Transformer (SDT), a novel framework that harnesses the expressive power of signal temporal logic (STL) to specify complex temporal rules that an agent should follow and the sequential modeling capability of Decision Transformer (DT). Empirical evaluations on the DSRL benchmarks demonstrate the better capacity of SDT in learning safe and high-reward policies compared with existing approaches. In addition, SDT shows good alignment with respect to different desired degrees of satisfaction of the STL specification that it is condi
    
[^89]: 机器学习优化在云计算资源调度与管理中的应用

    Application of Machine Learning Optimization in Cloud Computing Resource Scheduling and Management

    [https://arxiv.org/abs/2402.17216](https://arxiv.org/abs/2402.17216)

    本文提出了一种利用机器学习优化技术解决云计算资源调度与管理中复杂问题的创新方法。

    

    近年来，云计算被广泛应用。云计算是指集中式的计算资源，用户通过访问集中式资源完成计算，云计算中心将程序处理结果返回给用户。云计算不仅适用于个人用户，也适用于企业用户。购买云服务器后，用户无需购买大量计算机，节约了计算成本。根据中国经济新闻网络的报告，中国的云计算规模已达到2091亿元人民币。目前，中国较为成熟的云服务提供商有阿里云、百度云、华为云等。因此，本文提出了一种创新方法，利用机器学习优化技术解决云计算资源调度与管理中的复杂问题。

    arXiv:2402.17216v1 Announce Type: cross  Abstract: In recent years, cloud computing has been widely used. Cloud computing refers to the centralized computing resources, users through the access to the centralized resources to complete the calculation, the cloud computing center will return the results of the program processing to the user. Cloud computing is not only for individual users, but also for enterprise users. By purchasing a cloud server, users do not have to buy a large number of computers, saving computing costs. According to a report by China Economic News Network, the scale of cloud computing in China has reached 209.1 billion yuan. At present, the more mature cloud service providers in China are Ali Cloud, Baidu Cloud, Huawei Cloud and so on. Therefore, this paper proposes an innovative approach to solve complex problems in cloud computing resource scheduling and management using machine learning optimization techniques. Through in-depth study of challenges such as low r
    
[^90]: 通过特征矩阵进行多维非结构稀疏恢复

    Multidimensional unstructured sparse recovery via eigenmatrix

    [https://arxiv.org/abs/2402.17215](https://arxiv.org/abs/2402.17215)

    该论文提出了一种通过特征矩阵方法来解决多维非结构稀疏恢复问题，并通过数值结果展示了方法的性能。

    

    这篇论文考虑了多维非结构稀疏恢复问题，例如傅立叶反演和稀疏去卷积。特征矩阵是一种具有期望近似特征值和特征向量的数据驱动构造，用于解决一维问题。本文将特征矩阵方法扩展到多维问题。通过数值结果展示了所提方法的性能。

    arXiv:2402.17215v1 Announce Type: cross  Abstract: This note considers the multidimensional unstructured sparse recovery problems. Examples include Fourier inversion and sparse deconvolution. The eigenmatrix is a data-driven construction with desired approximate eigenvalues and eigenvectors proposed for the one-dimensional problems. This note extends the eigenmatrix approach to multidimensional problems. Numerical results are provided to demonstrate the performance of the proposed method.
    
[^91]: 测量神经模型的视觉语言STEM技能

    Measuring Vision-Language STEM Skills of Neural Models

    [https://arxiv.org/abs/2402.17205](https://arxiv.org/abs/2402.17205)

    该研究引入了一个新挑战，用于测试神经模型的STEM技能，提出了一个包含大量基础技能和问题的数据集，需要理解STEM的多模式视觉语言信息，并展示了最新模型对于低年级技能的有限掌握。

    

    我们引入了一个新挑战，用于测试神经模型的STEM技能。现实世界中的问题通常需要结合STEM（科学、技术、工程和数学）知识来解决。与现有数据集不同，我们的数据集需要理解STEM的多模式视觉语言信息。我们的数据集是挑战性问题中最大、最全面的数据集之一。它包括448项技能和1,073,146个跨越所有STEM科目的问题。与通常侧重于检验专家水平能力的现有数据集不同，我们的数据集包括基础技能和根据K-12课程设计的问题。我们还将最先进的基础模型，如CLIP和GPT-3.5-Turbo，添加到我们的基准中。结果显示，最近的模型进展只有助于掌握数据集中非常有限数量的低年级技能（三年级中的2.5%）。事实上，这些模型仍远没有完全掌握学前教育阶段的技能。

    arXiv:2402.17205v1 Announce Type: cross  Abstract: We introduce a new challenge to test the STEM skills of neural models. The problems in the real world often require solutions, combining knowledge from STEM (science, technology, engineering, and math). Unlike existing datasets, our dataset requires the understanding of multimodal vision-language information of STEM. Our dataset features one of the largest and most comprehensive datasets for the challenge. It includes 448 skills and 1,073,146 questions spanning all STEM subjects. Compared to existing datasets that often focus on examining expert-level ability, our dataset includes fundamental skills and questions designed based on the K-12 curriculum. We also add state-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our benchmark. Results show that the recent model advances only help master a very limited number of lower grade-level skills (2.5% in the third grade) in our dataset. In fact, these models are still well bel
    
[^92]: FedBRB：解决设备异构联邦学习中小规模到大规模场景的有效方案

    FedBRB: An Effective Solution to the Small-to-Large Scenario in Device-Heterogeneity Federated Learning

    [https://arxiv.org/abs/2402.17202](https://arxiv.org/abs/2402.17202)

    FedBRB方法提出了基于块概念的解决方案，实现了使用小型本地模型训练大型全局模型的所有块，并在设备异构联邦学习中有效应用。

    

    最近，大型模型的成功证明了扩大模型规模的重要性。这引发了从联邦学习的角度探索大规模模型协作训练的兴趣。由于计算约束，许多机构难以在本地训练大规模模型。因此，仅使用较小的本地模型训练更大的全局模型已成为一个重要情景，即\textbf{小规模到大规模场景}。尽管最近设备异构联邦学习方法已经开始探索这一领域，但它们在完全涵盖全局模型的参数空间方面存在局限性。在本文中，我们基于块概念提出了一种称为\textbf{FedBRB}（基于块概念的\underline{B}locking和加权\underline{R}olling与\underline{B}roadcasting）的方法。FedBRB可以使用小型本地模型训练大型全局模型的所有块，并将训练好的参数广播到整个空间。

    arXiv:2402.17202v1 Announce Type: new  Abstract: Recently, the success of large models has demonstrated the importance of scaling up model size. This has spurred interest in exploring collaborative training of large-scale models from federated learning perspective. Due to computational constraints, many institutions struggle to train a large-scale model locally. Thus, training a larger global model using only smaller local models has become an important scenario (i.e., the \textbf{small-to-large scenario}). Although recent device-heterogeneity federated learning approaches have started to explore this area, they face limitations in fully covering the parameter space of the global model. In this paper, we propose a method called \textbf{FedBRB} (\underline{B}lock-wise \underline{R}olling and weighted \underline{B}roadcast) based on the block concept. FedBRB can uses small local models to train all blocks of the large global model, and broadcasts the trained parameters to the entire spac
    
[^93]: 使用贝叶斯深度学习方法与不确定性量化预测SYM-H指数

    Prediction of the SYM-H Index Using a Bayesian Deep Learning Method with Uncertainty Quantification

    [https://arxiv.org/abs/2402.17196](https://arxiv.org/abs/2402.17196)

    提出了一种名为SYMHnet的深度学习框架，通过合作学习太阳风和行星间磁场参数的模式，可以短期预测SYM-H指数，并量化预测中的数据不确定性和模型不确定性

    

    我们提出了一种新颖的深度学习框架，名为SYMHnet，它采用图神经网络和双向长短时记忆网络，合作学习来自太阳风和行星间磁场参数的模式，用于基于1分钟和5分钟分辨率数据的SYM-H指数的短期预测。SYMHnet将NASA太空科学数据协调存档提供的参数值的时间序列作为输入，并预测在给定时间点t时，时间点t + w小时的SYM-H指数值，其中w为1或2。通过将贝叶斯推断纳入学习框架，SYMHnet在预测未来SYM-H指数时可以量化数据不确定性和模型不确定性。实验结果表明，SYMHnet在平静时期和风暴时期，无论是1分钟还是5分钟分辨率数据上表现良好。结果还表明，SYMHnet通常表现出色

    arXiv:2402.17196v1 Announce Type: cross  Abstract: We propose a novel deep learning framework, named SYMHnet, which employs a graph neural network and a bidirectional long short-term memory network to cooperatively learn patterns from solar wind and interplanetary magnetic field parameters for short-term forecasts of the SYM-H index based on 1-minute and 5-minute resolution data. SYMHnet takes, as input, the time series of the parameters' values provided by NASA's Space Science Data Coordinated Archive and predicts, as output, the SYM-H index value at time point t + w hours for a given time point t where w is 1 or 2. By incorporating Bayesian inference into the learning framework, SYMHnet can quantify both aleatoric (data) uncertainty and epistemic (model) uncertainty when predicting future SYM-H indices. Experimental results show that SYMHnet works well at quiet time and storm time, for both 1-minute and 5-minute resolution data. The results also show that SYMHnet generally performs b
    
[^94]: 当扩展遇到LLM微调: 数据、模型和微调方法的影响

    When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method

    [https://arxiv.org/abs/2402.17193](https://arxiv.org/abs/2402.17193)

    研究了不同扩展因素如何影响大型语言模型微调性能，认为LLM微调遵循着一种特殊的扩展行为。

    

    大型语言模型（LLMs）通常采用微调来释放其在下游应用中的能力，但我们对不同微调方法的归纳偏差（特别是扩展属性）的理解仍然有限。为了填补这一空白，我们进行了系统实验，研究不同扩展因素（包括LLM模型大小、预训练数据大小、新微调参数大小和微调数据大小）如何影响微调性能。我们考虑了两种微调类型--全模型调整（FMT）和参数高效微调（PET，包括提示调整和LoRA），并探索它们在数据有限的情况下的扩展行为，其中LLM模型大小远远超过微调数据大小。基于1B到16B的两组预训练双语LLMs和对双语机器翻译和多语言摘要基准的实验，我们发现LLM微调遵循

    arXiv:2402.17193v1 Announce Type: new  Abstract: While large language models (LLMs) often adopt finetuning to unlock their capabilities for downstream applications, our understanding on the inductive biases (especially the scaling properties) of different finetuning methods is still limited. To fill this gap, we conduct systematic experiments studying whether and how different scaling factors, including LLM model size, pretraining data size, new finetuning parameter size and finetuning data size, affect the finetuning performance. We consider two types of finetuning -- full-model tuning (FMT) and parameter efficient tuning (PET, including prompt tuning and LoRA), and explore their scaling behaviors in the data-limited regime where the LLM model size substantially outweighs the finetuning data size. Based on two sets of pretrained bilingual LLMs from 1B to 16B and experiments on bilingual machine translation and multilingual summarization benchmarks, we find that 1) LLM finetuning follo
    
[^95]: AI驱动的匿名化：在利用机器学习的同时保护个人数据隐私

    AI-Driven Anonymization: Protecting Personal Data Privacy While Leveraging Machine Learning

    [https://arxiv.org/abs/2402.17191](https://arxiv.org/abs/2402.17191)

    通过机器学习的差分隐私保护算法，实现个人数据隐私保护和检测。

    

    人工智能的发展显著改变了人们的生活。然而，它也对隐私和安全构成了重大威胁，许多个人信息被公开，并有犯罪攻击和窃取的报道。因此，通过机器学习算法实现个人信息的智能保护已经成为首要关注的问题。人工智能利用先进算法和技术有效加密和匿名化个人数据，实现有价值的数据分析和利用同时维护隐私。本文着眼于个人数据隐私保护和匿名化推广作为其核心研究目标。它通过使用机器学习的差分隐私保护算法实现了个人数据隐私保护和检测。该论文还解决了机器学习领域中现有的挑战。

    arXiv:2402.17191v1 Announce Type: cross  Abstract: The development of artificial intelligence has significantly transformed people's lives. However, it has also posed a significant threat to privacy and security, with numerous instances of personal information being exposed online and reports of criminal attacks and theft. Consequently, the need to achieve intelligent protection of personal information through machine learning algorithms has become a paramount concern. Artificial intelligence leverages advanced algorithms and technologies to effectively encrypt and anonymize personal data, enabling valuable data analysis and utilization while safeguarding privacy. This paper focuses on personal data privacy protection and the promotion of anonymity as its core research objectives. It achieves personal data privacy protection and detection through the use of machine learning's differential privacy protection algorithm. The paper also addresses existing challenges in machine learning rel
    
[^96]: 使用深度学习对计算流体动力学进行修补

    Inpainting Computational Fluid Dynamics with Deep Learning

    [https://arxiv.org/abs/2402.17185](https://arxiv.org/abs/2402.17185)

    使用向量量化技术，通过两阶段学习过程将完整和不完整的流体数据空间映射到离散值较低维度的表示。

    

    流体数据的完成是一个具有潜在益处的研究问题，对实验和计算流体动力学都有高潜力。有效的流体数据完成方法可以减少流体动力学实验中所需的传感器数量，并允许在计算流体动力学（CFD）模拟中使用更粗糙和更自适应的网格。然而，流体数据完成问题的逆问题性质使得从理论上获得解决方案非常困难，并对数据驱动方法（例如神经网络模型）产生高数值不确定性和不稳定性。为了解决这些挑战，我们借鉴了计算机视觉领域的最新进展，利用向量量化技术通过两阶段学习过程将完整和不完整的流体数据空间映射到离散值较低维度的表示。我们在 Kolmogorov 流数据（雷诺数：100）上展示了我们方法的有效性。

    arXiv:2402.17185v1 Announce Type: new  Abstract: Fluid data completion is a research problem with high potential benefit for both experimental and computational fluid dynamics. An effective fluid data completion method reduces the required number of sensors in a fluid dynamics experiment, and allows a coarser and more adaptive mesh for a Computational Fluid Dynamics (CFD) simulation. However, the ill-posed nature of the fluid data completion problem makes it prohibitively difficult to obtain a theoretical solution and presents high numerical uncertainty and instability for a data-driven approach (e.g., a neural network model). To address these challenges, we leverage recent advancements in computer vision, employing the vector quantization technique to map both complete and incomplete fluid data spaces onto discrete-valued lower-dimensional representations via a two-stage learning procedure. We demonstrated the effectiveness of our approach on Kolmogorov flow data (Reynolds number: 100
    
[^97]: 双空间优化：通过潜在提示变换器改进分子序列设计

    Dual-Space Optimization: Improved Molecule Sequence Design by Latent Prompt Transformer

    [https://arxiv.org/abs/2402.17179](https://arxiv.org/abs/2402.17179)

    提出了双空间优化（DSO）方法，通过整合潜在空间采样和数据空间选择，使用Latent Prompt Transformer (LPT)生成模型，解决了分子设计中的关键问题，取得了在不同任务中的性能优势。

    

    设计具有理想性质（如药物样性和对蛋白靶点的高结合亲和力）的分子是一个具有挑战性的问题。在本文中，我们提出了双空间优化（DSO）方法，该方法整合了潜在空间采样和数据空间选择来解决这一问题。DSO通过迭代更新潜在空间生成模型和合成数据集，逐步将生成模型和合成数据移向所需性质数值的区域。我们的生成模型采用潜在提示变换器（LPT）的形式，其中潜在向量充当因果变换器的提示。我们广泛的实验表明了提出方法的有效性，该方法在单目标、多目标和约束分子设计任务中树立了新的性能基准。

    arXiv:2402.17179v1 Announce Type: new  Abstract: Designing molecules with desirable properties, such as drug-likeliness and high binding affinities towards protein targets, is a challenging problem. In this paper, we propose the Dual-Space Optimization (DSO) method that integrates latent space sampling and data space selection to solve this problem. DSO iteratively updates a latent space generative model and a synthetic dataset in an optimization process that gradually shifts the generative model and the synthetic data towards regions of desired property values. Our generative model takes the form of a Latent Prompt Transformer (LPT) where the latent vector serves as the prompt of a causal transformer. Our extensive experiments demonstrate effectiveness of the proposed method, which sets new performance benchmarks across single-objective, multi-objective and constrained molecule design tasks.
    
[^98]: Sora: 大型视觉模型背景、技术、局限性和机遇的综述

    Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models

    [https://arxiv.org/abs/2402.17177](https://arxiv.org/abs/2402.17177)

    Sora是一种文本到视频生成的人工智能模型，展示出在模拟物理世界方面的潜力，具有广泛的应用前景和挑战，未来发展具有重要意义。

    

    Sora是由OpenAI于2024年2月发布的一种文本到视频生成的人工智能模型。这个模型经过训练，可以根据文本指令生成逼真或想象的场景视频，并在模拟物理世界方面显示出潜力。本文基于公开的技术报告和逆向工程，对这个模型的背景、相关技术、应用、尚存的挑战以及文本到视频人工智能模型的未来方向进行了全面回顾。首先我们追溯了Sora的发展历程，并调查了用于构建这个"世界模拟器"的基础技术。然后，我们详细描述了Sora在从电影制作和教育到营销等多个行业中的应用和潜在影响。我们讨论了需要解决的主要挑战和局限性，以便广泛部署Sora，如确保安全和无偏见的视频生成。最后，我们讨论了Sora以及视频生成技术未来的发展。

    arXiv:2402.17177v1 Announce Type: cross  Abstract: Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this "world simulator". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video gene
    
[^99]: DeepDRK:深度依赖正则化 Knockoff 用于特征选择

    DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection

    [https://arxiv.org/abs/2402.17176](https://arxiv.org/abs/2402.17176)

    DeepDRK是一种分布无关的深度学习方法，通过引入基于Transformer架构的生成模型以实现“交换属性”，并提出新颖有效的正则化技术，取得了在FDR和能力之间取得平衡。

    

    arXiv:2402.17176v1 公告类型:新 摘要: Model-X knockoff，在各种特征选择方法中，由于其对假发现率（FDR）控制的保证而最近受到广泛关注。在参数设计中引入后，knockoff被发展为使用基于深度学习的生成建模来处理任意数据分布。然而，我们观察到目前深度Model-X knockoff框架的实现存在局限性。值得注意的是，knockoffs所需的“交换属性”经常在样本级别遇到挑战，导致选择能力下降。为了克服这一问题，我们开发了“深度依赖正则化Knockoff（DeepDRK）”，这是一种不依赖分布的深度学习方法，可以在FDR和能力之间取得平衡。在DeepDRK中，引入了一种基于Transformer架构的生成模型，以更好地实现“交换属性”。还提出了新颖有效的正则化技术，以获得更高的能力。

    arXiv:2402.17176v1 Announce Type: new  Abstract: Model-X knockoff, among various feature selection methods, received much attention recently due to its guarantee on false discovery rate (FDR) control. Subsequent to its introduction in parametric design, knockoff is advanced to handle arbitrary data distributions using deep learning-based generative modeling. However, we observed that current implementations of the deep Model-X knockoff framework exhibit limitations. Notably, the "swap property" that knockoffs necessitate frequently encounter challenges on sample level, leading to a diminished selection power. To overcome, we develop "Deep Dependency Regularized Knockoff (DeepDRK)", a distribution-free deep learning method that strikes a balance between FDR and power. In DeepDRK, a generative model grounded in a transformer architecture is introduced to better achieve the "swap property". Novel efficient regularization techniques are also proposed to reach higher power. Our model outper
    
[^100]: 用于预测复杂系统动态的生成式学习

    Generative Learning for Forecasting the Dynamics of Complex Systems

    [https://arxiv.org/abs/2402.17157](https://arxiv.org/abs/2402.17157)

    生成式学习可以通过学习和演变系统的有效动态加速模拟，为准确预测复杂系统的统计性质提供新的可能性。

    

    我们引入了用于通过学习和演变其有效动态加速复杂系统模拟的生成式模型。在提出的有效动态生成式学习（G-LED）中，将高维数据的实例进行降采样到一个经过自回归注意机制演化的低维流形中。反过来，贝叶斯扩散模型将这个低维流形映射到其相应的高维空间，捕捉系统动态的统计信息。我们演示了G-LED在几个基准系统的模拟中的能力和缺陷，包括Kuramoto-Sivashinsky（KS）方程，反向脉冲后方高雷诺数流体的二维流动，以及三维湍流通道流的模拟。结果表明，生成式学习为准确预测复杂系统的统计性质开辟了新的前沿。

    arXiv:2402.17157v1 Announce Type: new  Abstract: We introduce generative models for accelerating simulations of complex systems through learning and evolving their effective dynamics. In the proposed Generative Learning of Effective Dynamics (G-LED), instances of high dimensional data are down sampled to a lower dimensional manifold that is evolved through an auto-regressive attention mechanism. In turn, Bayesian diffusion models, that map this low-dimensional manifold onto its corresponding high-dimensional space, capture the statistics of the system dynamics. We demonstrate the capabilities and drawbacks of G-LED in simulations of several benchmark systems, including the Kuramoto-Sivashinsky (KS) equation, two-dimensional high Reynolds number flow over a backward-facing step, and simulations of three-dimensional turbulent channel flow. The results demonstrate that generative learning offers new frontiers for the accurate forecasting of the statistical properties of complex systems at
    
[^101]: TaxDiff：用于蛋白质序列生成的分类引导扩散模型

    TaxDiff: Taxonomic-Guided Diffusion Model for Protein Sequence Generation

    [https://arxiv.org/abs/2402.17156](https://arxiv.org/abs/2402.17156)

    提出了一种名为TaxDiff的分类引导扩散模型，结合了生物物种信息和扩散模型的生成能力，用于可控生成结构稳定的蛋白质序列。

    

    设计具有特定生物功能和结构稳定性的蛋白质序列在生物学和化学中至关重要。生成模型已经展示了它们在可靠蛋白质设计方面的能力。然而，先前的模型仅限于无条件生成蛋白质序列，缺乏对生物任务至关重要的可控生成能力。在这项工作中，我们提出了TaxDiff，一种用于可控蛋白质序列生成的分类引导扩散模型，它将生物物种信息与扩散模型的生成能力相结合，以在序列空间内生成结构稳定的蛋白质。具体地，分类控制信息被插入到变压器块的每一层，以实现细粒度控制。全局和局部关注的结合确保了分类特定蛋白质的序列一致性和结构可折叠性。广泛的实验...

    arXiv:2402.17156v1 Announce Type: cross  Abstract: Designing protein sequences with specific biological functions and structural stability is crucial in biology and chemistry. Generative models already demonstrated their capabilities for reliable protein design. However, previous models are limited to the unconditional generation of protein sequences and lack the controllable generation ability that is vital to biological tasks. In this work, we propose TaxDiff, a taxonomic-guided diffusion model for controllable protein sequence generation that combines biological species information with the generative capabilities of diffusion models to generate structurally stable proteins within the sequence space. Specifically, taxonomic control information is inserted into each layer of the transformer block to achieve fine-grained control. The combination of global and local attention ensures the sequence consistency and structural foldability of taxonomic-specific proteins. Extensive experimen
    
[^102]: 行动胜过言辞：用于生成推荐的千亿参数顺序转导器

    Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations

    [https://arxiv.org/abs/2402.17152](https://arxiv.org/abs/2402.17152)

    提出了HSTU架构，用于高基数、非平稳流推荐数据，性能优于基线方法高达65.8％的NDCG，并且比基于FlashAttention2的Transformer在8192长度序列上快5.3倍到15.2倍。

    

    大规模推荐系统的特点是依赖于高基数、异构特征，并且需要每天处理数十亿用户行为。尽管在成千上万个特征上训练了大量数据，但大多数行业中的深度学习推荐模型(DLRMs)在计算方面无法扩展。受到在语言和视觉领域取得成功的Transformer的启发，我们重新审视了推荐系统中的基本设计选择。我们将推荐问题重新构建为生成建模框架中的顺序转导任务（“生成推荐者”），并提出了一种针对高基数、非平稳流推荐数据设计的新架构HSTU。

    arXiv:2402.17152v1 Announce Type: new  Abstract: Large-scale recommendation systems are characterized by their reliance on high cardinality, heterogeneous features and the need to handle tens of billions of user actions on a daily basis. Despite being trained on huge volume of data with thousands of features, most Deep Learning Recommendation Models (DLRMs) in industry fail to scale with compute.   Inspired by success achieved by Transformers in language and vision domains, we revisit fundamental design choices in recommendation systems. We reformulate recommendation problems as sequential transduction tasks within a generative modeling framework (``Generative Recommenders''), and propose a new architecture, HSTU, designed for high cardinality, non-stationary streaming recommendation data.   HSTU outperforms baselines over synthetic and public datasets by up to 65.8\% in NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based Transformers on 8192 length sequences. HSTU-based Gener
    
[^103]: 使用张量网络在量子计算机上生成期权定价的时间序列

    Time series generation for option pricing on quantum computers using tensor network

    [https://arxiv.org/abs/2402.17148](https://arxiv.org/abs/2402.17148)

    提出了一种使用矩阵乘积态作为时间序列生成的方法，可以有效生成多个时间点处基础资产价格的联合分布的态，并证实了该方法在Heston模型中的可行性。

    

    金融，特别是期权定价，是一个有望从量子计算中受益的行业。尽管已经提出了用于期权定价的量子算法，但人们希望在算法中设计出更高效的实现方式，其中之一是准备编码基础资产价格概率分布的量子态。特别是在定价依赖路径的期权时，我们需要生成一个编码多个时间点处基础资产价格的联合分布的态，这更具挑战性。为解决这些问题，我们提出了一种使用矩阵乘积态（MPS）作为时间序列生成的生成模型的新方法。为了验证我们的方法，以Heston模型为目标，我们进行数值实验以在模型中生成时间序列。我们的研究结果表明MPS模型能够生成Heston模型中的路径，突显了...

    arXiv:2402.17148v1 Announce Type: cross  Abstract: Finance, especially option pricing, is a promising industrial field that might benefit from quantum computing. While quantum algorithms for option pricing have been proposed, it is desired to devise more efficient implementations of costly operations in the algorithms, one of which is preparing a quantum state that encodes a probability distribution of the underlying asset price. In particular, in pricing a path-dependent option, we need to generate a state encoding a joint distribution of the underlying asset price at multiple time points, which is more demanding. To address these issues, we propose a novel approach using Matrix Product State (MPS) as a generative model for time series generation. To validate our approach, taking the Heston model as a target, we conduct numerical experiments to generate time series in the model. Our findings demonstrate the capability of the MPS model to generate paths in the Heston model, highlightin
    
[^104]: 带有预测的能效调度

    Energy-Efficient Scheduling with Predictions

    [https://arxiv.org/abs/2402.17143](https://arxiv.org/abs/2402.17143)

    基于机器学习预测的能效调度算法在能量最小化与截止时间问题上取得了改进的竞争比率。

    

    现代调度系统的一个重要目标是高效管理能源使用。在能效调度中，操作系统控制着机器处理作业的速度，以最小化能量消耗并优化所得调度的服务质量成本。由于机器学习关于未来请求的预测通常可以从历史数据中学习到，最近一系列关于学习增强算法的工作旨在通过利用预测来实现改进的性能保证。特别是，针对能效调度，Bamas et. al. [BamasMRS20] 和 Antoniadis et. al. [antoniadis2021novel] 设计了带有预测的算法，用于能量最小化与截止时间问题，并在预测误差较小时实现了改进的竞争比率，即使在预测误差任意大时也能保持最坏情况下的界限。

    arXiv:2402.17143v1 Announce Type: cross  Abstract: An important goal of modern scheduling systems is to efficiently manage power usage. In energy-efficient scheduling, the operating system controls the speed at which a machine is processing jobs with the dual objective of minimizing energy consumption and optimizing the quality of service cost of the resulting schedule. Since machine-learned predictions about future requests can often be learned from historical data, a recent line of work on learning-augmented algorithms aims to achieve improved performance guarantees by leveraging predictions. In particular, for energy-efficient scheduling, Bamas et. al. [BamasMRS20] and Antoniadis et. al. [antoniadis2021novel] designed algorithms with predictions for the energy minimization with deadlines problem and achieved an improved competitive ratio when the prediction error is small while also maintaining worst-case bounds even when the prediction error is arbitrarily large.   In this paper, w
    
[^105]: 通过功能奖励编码实现的无监督零样本强化学习

    Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings

    [https://arxiv.org/abs/2402.17135](https://arxiv.org/abs/2402.17135)

    通过功能奖励编码实现的无监督零样本强化学习方法能够在各种模拟机器人基准测试中训练代理并成功解决新任务，相比以往的零样本强化学习方法表现更优秀。

    

    这项工作提出了一种称为功能奖励编码（FRE）的通用、可扩展的解决方案，用于零样本强化学习问题。我们的主要想法是通过使用基于transformer的变分自动编码器对任意任务的状态-奖励样本进行编码，从而学习任意任务的功能表示。这种功能编码不仅使得能够从各种通用无监督奖励函数进行预训练，而且还提供了一种在零样本情况下解决任何新的下游任务的方法，只需少量奖励注释样本。我们在实验中显示，针对多样的随机无监督奖励函数进行训练的FRE代理能够推广到解决一系列模拟机器人基准测试中的新任务，通常优于先前的零样本强化学习方法。

    arXiv:2402.17135v1 Announce Type: cross  Abstract: Can we pre-train a generalist agent from a large amount of unlabeled offline trajectories such that it can be immediately adapted to any new downstream tasks in a zero-shot manner? In this work, we present a functional reward encoding (FRE) as a general, scalable solution to this zero-shot RL problem. Our main idea is to learn functional representations of any arbitrary tasks by encoding their state-reward samples using a transformer-based variational auto-encoder. This functional encoding not only enables the pre-training of an agent from a wide diversity of general unsupervised reward functions, but also provides a way to solve any new downstream tasks in a zero-shot manner, given a small number of reward-annotated samples. We empirically show that FRE agents trained on diverse random unsupervised reward functions can generalize to solve novel tasks in a range of simulated robotic benchmarks, often outperforming previous zero-shot RL
    
[^106]: 使用Transformer和RNN在经过训练的新损失函数下预测哺乳动物蛋白质中的O-GlcNAcylation位点

    Predicting O-GlcNAcylation Sites in Mammalian Proteins with Transformers and RNNs Trained with a New Loss Function

    [https://arxiv.org/abs/2402.17131](https://arxiv.org/abs/2402.17131)

    本研究提出了一种新的损失函数，称为加权焦点可微MCC，用于改善分类模型的性能，并在预测哺乳动物蛋白质中的O-GlcNAcylation位点方面取得了进展

    

    糖基化是一种蛋白质修饰，在功能和结构上起着多种重要作用。O-GlcNAcylation是糖基化的一种亚型，有潜力成为治疗的重要靶点，但在2023年之前尚未有可靠预测O-GlcNAcylation位点的方法；2021年的一篇评论正确指出已发表的模型不足，并且未能泛化。此外，许多模型已不再可用。2023年，一篇具有F$_1$分数36.17%和MCC分数34.57%的大型数据集上的显着更好的RNN模型被发表。本文首次试图通过Transformer编码器提高这些指标。尽管Transformer在该数据集上表现出色，但其性能仍不及先前发表的RNN。然后我们创建了一种新的损失函数，称为加权焦点可微MCC，以提高分类模型的性能。

    arXiv:2402.17131v1 Announce Type: new  Abstract: Glycosylation, a protein modification, has multiple essential functional and structural roles. O-GlcNAcylation, a subtype of glycosylation, has the potential to be an important target for therapeutics, but methods to reliably predict O-GlcNAcylation sites had not been available until 2023; a 2021 review correctly noted that published models were insufficient and failed to generalize. Moreover, many are no longer usable. In 2023, a considerably better RNN model with an F$_1$ score of 36.17% and an MCC of 34.57% on a large dataset was published. This article first sought to improve these metrics using transformer encoders. While transformers displayed high performance on this dataset, their performance was inferior to that of the previously published RNN. We then created a new loss function, which we call the weighted focal differentiable MCC, to improve the performance of classification models. RNN models trained with this new function di
    
[^107]: LCEN：一种新型特征选择算法，用于非线性的可解释机器学习模型

    LCEN: A Novel Feature Selection Algorithm for Nonlinear, Interpretable Machine Learning Models

    [https://arxiv.org/abs/2402.17120](https://arxiv.org/abs/2402.17120)

    LCEN算法是一种用于创建非线性、可解释机器学习模型的新型特征选择算法，能够更准确、更稀疏地生成模型，并具有鲁棒性。

    

    可解释的架构相对于黑盒架构具有优势，在关键领域如航空或医学中，可解释性对机器学习应用至关重要。然而，最简单、最常用的可解释架构（如LASSO或EN）仅限于线性预测，并且特征选择能力较差。在这项工作中，我们引入了LASSO-Clip-EN（LCEN）算法，用于创建非线性、可解释的机器学习模型。LCEN在多种人工和实证数据集上进行了测试，生成比其他常用架构更准确、更稀疏的模型。这些实验表明，LCEN对数据集和建模中通常存在的许多问题具有鲁棒性，包括噪声、多重共线性、数据稀缺和超参数方差。LCEN还能够从实证数据中重新发现多个物理定律，

    arXiv:2402.17120v1 Announce Type: new  Abstract: Interpretable architectures can have advantages over black-box architectures, and interpretability is essential for the application of machine learning in critical settings, such as aviation or medicine. However, the simplest, most commonly used interpretable architectures (such as LASSO or EN) are limited to linear predictions and have poor feature selection capabilities. In this work, we introduce the LASSO-Clip-EN (LCEN) algorithm for the creation of nonlinear, interpretable machine learning models. LCEN is tested on a wide variety of artificial and empirical datasets, creating more accurate, sparser models than other commonly used architectures. These experiments reveal that LCEN is robust against many issues typically present in datasets and modeling, including noise, multicollinearity, data scarcity, and hyperparameter variance. LCEN is also able to rediscover multiple physical laws from empirical data and, for processes with no kn
    
[^108]: Sinkhorn Distance Minimization for Knowledge Distillation

    Sinkhorn Distance Minimization for Knowledge Distillation

    [https://arxiv.org/abs/2402.17110](https://arxiv.org/abs/2402.17110)

    提出了Sinkhorn Knowledge Distillation（SinKD）来解决知识蒸馏过程中散度度量存在的问题，确保对教师和学生分布之间差异的准确评估

    

    知识蒸馏（KD）被广泛采用来压缩大型语言模型（LLMs）。现有的KD方法研究包括Kullback-Leibler（KL）、反向Kullback-Leibler（RKL）和Jensen-Shannon（JS）散度在内的各种散度度量。然而，由于它们的假设和定义中固有的限制，这些度量在教师和学生之间存在少量分布重叠时未能提供有效的监督。本文表明，前述的KL、RKL和JS散度分别存在模式平均、模式坍塌和模式低估的问题，这恶化了基于logits的多样NLP任务的KD。我们提出了利用Sinkhorn距离的Sinkhorn知识蒸馏（SinKD），以确保对教师和学生分布之间差异的细致和准确评估。此外，由于Sinkhorn度量的属性，我们可以摆脱

    arXiv:2402.17110v1 Announce Type: new  Abstract: Knowledge distillation (KD) has been widely adopted to compress large language models (LLMs). Existing KD methods investigate various divergence measures including the Kullback-Leibler (KL), reverse Kullback-Leibler (RKL), and Jensen-Shannon (JS) divergences. However, due to limitations inherent in their assumptions and definitions, these measures fail to deliver effective supervision when few distribution overlap exists between the teacher and the student. In this paper, we show that the aforementioned KL, RKL, and JS divergences respectively suffer from issues of mode-averaging, mode-collapsing, and mode-underestimation, which deteriorates logits-based KD for diverse NLP tasks. We propose the Sinkhorn Knowledge Distillation (SinKD) that exploits the Sinkhorn distance to ensure a nuanced and precise assessment of the disparity between teacher and student distributions. Besides, profit by properties of the Sinkhorn metric, we can get rid
    
[^109]: 多个非近视代理的重复合同：政策后悔和有限责任

    Repeated Contracting with Multiple Non-Myopic Agents: Policy Regret and Limited Liability

    [https://arxiv.org/abs/2402.17108](https://arxiv.org/abs/2402.17108)

    该研究揭示了在选择代理人进行合同时产生的博弈，展示了出色的纯策略均衡以及对于任何凹合同的政策后悔优势。

    

    我们研究了一个重复合同设置，在这个设置中，委托人在每一轮中选择$k$名代理。代理不是近视的，所以委托人的机制引发了一场在代理之间进行的$T$轮广泛形式的博弈。我们得出了几个旨在理解合同理论中一个少有探讨的方面的结果 - 即在选择与之签订合同的代理时引发的博弈。首先，我们展示了这场博弈允许代理之间存在一个纯策略的\emph{非响应}均衡——非正式地说，这是一个均衡状态，代理的行动取决于实现状态的历史，但不取决于彼此行动的历史，从而避免了勾结和威胁的复杂性。接下来，我们证明如果委托人使用一个\emph{单调}摇臂算法来选择代理，那么对于任何凹合同，在这样一个均衡状态下，委托人对事后选择与最佳代理签订合同没有后悔。

    arXiv:2402.17108v1 Announce Type: cross  Abstract: We study a repeated contracting setting in which a Principal adaptively chooses amongst $k$ Agents at each of $T$ rounds. The Agents are non-myopic, and so a mechanism for the Principal induces a $T$-round extensive form game amongst the Agents. We give several results aimed at understanding an under-explored aspect of contract theory -- the game induced when choosing an Agent to contract with. First, we show that this game admits a pure-strategy \emph{non-responsive} equilibrium amongst the Agents -- informally an equilibrium in which the Agent's actions depend on the history of realized states of nature, but not on the history of each other's actions, and so avoids the complexities of collusion and threats. Next, we show that if the Principal selects Agents using a \emph{monotone} bandit algorithm, then for any concave contract, in any such equilibrium, the Principal obtains no regret to contracting with the best Agent in hindsight -
    
[^110]: 数据集公平性：在您的数据上实现具有效用保证的公平性

    Dataset Fairness: Achievable Fairness on Your Data With Utility Guarantees

    [https://arxiv.org/abs/2402.17106](https://arxiv.org/abs/2402.17106)

    该论文提出了一种针对数据集特性量身定制的近似公平性-准确性权衡曲线计算方法，能够有效减轻训练多个模型的计算负担并提供了严格的统计保证

    

    在机器学习公平性中，训练能够最小化不同敏感群体之间差异的模型通常会导致准确性下降，这种现象被称为公平性-准确性权衡。这种权衡的严重程度基本取决于数据集的特性，如数据集的不均衡或偏见。因此，在数据集之间使用统一的公平性要求仍然值得怀疑，并且往往会导致效用极低的模型。为了解决这个问题，我们提出了一种针对单个数据集量身定制的近似公平性-准确性权衡曲线的计算效率高的方法，该方法支持严格的统计保证。通过利用You-Only-Train-Once（YOTO）框架，我们的方法减轻了在逼近权衡曲线时需要训练多个模型的计算负担。此外，我们通过在该曲线周围引入置信区间来量化我们近似值的不确定性，

    arXiv:2402.17106v1 Announce Type: cross  Abstract: In machine learning fairness, training models which minimize disparity across different sensitive groups often leads to diminished accuracy, a phenomenon known as the fairness-accuracy trade-off. The severity of this trade-off fundamentally depends on dataset characteristics such as dataset imbalances or biases. Therefore using a uniform fairness requirement across datasets remains questionable and can often lead to models with substantially low utility. To address this, we present a computationally efficient approach to approximate the fairness-accuracy trade-off curve tailored to individual datasets, backed by rigorous statistical guarantees. By utilizing the You-Only-Train-Once (YOTO) framework, our approach mitigates the computational burden of having to train multiple models when approximating the trade-off curve. Moreover, we quantify the uncertainty in our approximation by introducing confidence intervals around this curve, offe
    
[^111]: 物理信号的对抗扰动

    Adversarial Perturbations of Physical Signals

    [https://arxiv.org/abs/2402.17104](https://arxiv.org/abs/2402.17104)

    研究了基于计算机视觉的信号分类器对物理信号的对抗扰动的脆弱性，通过引入PDE约束优化问题构造干扰信号，成功实现对检测器的误分类，对解决这些问题提出了高效方法，实验证明可以计算出各种机器学习模型的对抗扰动。

    

    我们研究了基于计算机视觉的信号分类器对其输入的对抗扰动的脆弱性，其中信号和扰动受物理约束。我们考虑了这样一个情景：一个源和干扰器发出信号作为波传播到检测器，检测器试图通过分析接收到的信号的频谱图来分类源，使用一个预训练的神经网络。通过求解PDE约束优化问题，我们构造干扰信号，即使对接收到的信号的频谱图的扰动几乎不可察觉，也会导致检测器对源进行错误分类。虽然这类问题可能包含数百万个决策变量，但我们引入了有效求解它们的方法。我们的实验证明，可以为各种机器学习模型在不同的物理约束下计算出有效且物理可实现的对抗扰动。

    arXiv:2402.17104v1 Announce Type: new  Abstract: We investigate the vulnerability of computer-vision-based signal classifiers to adversarial perturbations of their inputs, where the signals and perturbations are subject to physical constraints. We consider a scenario in which a source and interferer emit signals that propagate as waves to a detector, which attempts to classify the source by analyzing the spectrogram of the signal it receives using a pre-trained neural network. By solving PDE-constrained optimization problems, we construct interfering signals that cause the detector to misclassify the source even though the perturbations to the spectrogram of the received signal are nearly imperceptible. Though such problems can have millions of decision variables, we introduce methods to solve them efficiently. Our experiments demonstrate that one can compute effective and physically realizable adversarial perturbations for a variety of machine learning models under various physical co
    
[^112]: 通过两参数模型和梯度流学习高维目标

    Learning high-dimensional targets by two-parameter models and gradient flow

    [https://arxiv.org/abs/2402.17089](https://arxiv.org/abs/2402.17089)

    通过提出两参数模型和梯度流学习高维目标的理论可能性，研究发现在特定条件下存在大量不可学习目标，并且这些目标的集合不密集，具有一定拓扑性质的子集中也存在不可学习目标。最终，发现使用层次过程构建的主要定理模型在数学表达上并非由单一初等函数表示。

    

    我们探讨了当$W<d$时，通过梯度流（GF）以$W$参数模型学习$d$维目标的理论可能性，必然存在GF-不可学习目标的大子集。特别是，可学习目标的集合在$\mathbb R^d$中不是密集的，任何形同$W$维球面的$\mathbb R^d$子集包含不可学习目标。最后，我们观察到在几乎保证二参数学习的主要定理中，所述模型是通过层次过程构建的，因此不能用单个初等函数表达。我们展示了这种限制在本质上是必要的，因为这种可学习性对于许多初等函数类的可学习性是被排除的。

    arXiv:2402.17089v1 Announce Type: cross  Abstract: We explore the theoretical possibility of learning $d$-dimensional targets with $W$-parameter models by gradient flow (GF) when $W<d$ there is necessarily a large subset of GF-non-learnable targets. In particular, the set of learnable targets is not dense in $\mathbb R^d$, and any subset of $\mathbb R^d$ homeomorphic to the $W$-dimensional sphere contains non-learnable targets. Finally, we observe that the model in our main theorem on almost guaranteed two-parameter learning is constructed using a hierarchical procedure and as a result is not expressible by a single elementary function. We show that this limitation is essential in the sense that such learnability can be ruled out for a large class of elementary functions.
    
[^113]: 关于具有潜在根变量的贝叶斯网络的注解

    A Note on Bayesian Networks with Latent Root Variables

    [https://arxiv.org/abs/2402.17087](https://arxiv.org/abs/2402.17087)

    从贝叶斯网络计算出的数据集的似然性主要由经验网络的似然性的全局最大值所主导，并且仅当贝叶斯网络的参数与经验模型的参数一致时，这样的最大值才会被实现。

    

    我们表征了从具有潜在根节点的贝叶斯网络计算出的似然函数。我们展示了对于剩余的显性变量的边缘分布也会像一个贝叶斯网络一样分解，我们称之为经验化。一组观测到的显性变量的数据集使我们能够量化经验贝叶斯网络的参数。我们证明了(i)从原始贝叶斯网络计算出这样一个数据集的似然性由经验化模型的似然性的全局最大值所主导；以及(ii)这样一个最大值仅在贝叶斯网络的参数与经验模型的参数一致时才会达到。

    arXiv:2402.17087v1 Announce Type: cross  Abstract: We characterise the likelihood function computed from a Bayesian network with latent variables as root nodes. We show that the marginal distribution over the remaining, manifest, variables also factorises as a Bayesian network, which we call empirical. A dataset of observations of the manifest variables allows us to quantify the parameters of the empirical Bayesian net. We prove that (i) the likelihood of such a dataset from the original Bayesian network is dominated by the global maximum of the likelihood from the empirical one; and that (ii) such a maximum is attained if and only if the parameters of the Bayesian network are consistent with those of the empirical model.
    
[^114]: 并行化时空绑定

    Parallelized Spatiotemporal Binding

    [https://arxiv.org/abs/2402.17077](https://arxiv.org/abs/2402.17077)

    PSB是第一个针对序列输入进行时空并行化的槽学习架构，通过并行处理所有时间步骤中的对象中心表示，利用固定层数和因果关注力，显著提高了效率。

    

    尽管现代最佳实践提倡支持远程交互的可扩展体系结构，但面向对象的模型仍未完全采用这些体系结构。具体而言，由于现有的面向对象模型依赖于基于RNN的实现，导致其稳定性和容量较差，并且在长序列上训练缓慢。我们引入了并行化时空绑定器或PSB，这是面向序列输入的第一个时间并行可学习体系结构。与传统的基于RNN的方法不同，PSB同时为所有时间步生成对象中心化表示，称为slots。通过在具有因果关注力的固定层数上精细化初始slots，我们得以在所有时间步骤上实现并行。通过充分利用我们体系结构所引入的并行性，所提出的模型在效率上显示出显著提升。在实验中，我们对PSB进行了广泛测试。

    arXiv:2402.17077v1 Announce Type: new  Abstract: While modern best practices advocate for scalable architectures that support long-range interactions, object-centric models are yet to fully embrace these architectures. In particular, existing object-centric models for handling sequential inputs, due to their reliance on RNN-based implementation, show poor stability and capacity and are slow to train on long sequences. We introduce Parallelizable Spatiotemporal Binder or PSB, the first temporally-parallelizable slot learning architecture for sequential inputs. Unlike conventional RNN-based approaches, PSB produces object-centric representations, known as slots, for all time-steps in parallel. This is achieved by refining the initial slots across all time-steps through a fixed number of layers equipped with causal attention. By capitalizing on the parallelism induced by our architecture, the proposed model exhibits a significant boost in efficiency. In experiments, we test PSB extensivel
    
[^115]: 使用超高维计算进行单次图表示学习

    One-Shot Graph Representation Learning Using Hyperdimensional Computing

    [https://arxiv.org/abs/2402.17073](https://arxiv.org/abs/2402.17073)

    该方法提出了一种使用超高维计算进行单次图表示学习的方法，通过将数据投影到高维空间并利用HD运算符进行信息聚合，实现了与最先进深度学习方法相竞争的预测性能，而无需进行计算昂贵的训练。

    

    我们提出了一种新颖、简单、快速、高效的半监督图学习方法。所提方法利用超高维计算，将数据样本使用随机投影编码到高维空间（简称HD空间）。具体来说，我们提出了一种利用图神经网络节点表示的单射性质的超高维图学习（HDGL）算法。HDGL将节点特征映射到HD空间，然后使用HD运算符（如捆绑和绑定）来聚合每个节点的局部邻域信息。对广泛使用的基准数据集进行的实验结果显示，HDGL实现了与最先进深度学习方法相竞争的预测性能，而无需进行计算昂贵的训练。

    arXiv:2402.17073v1 Announce Type: cross  Abstract: We present a novel, simple, fast, and efficient approach for semi-supervised learning on graphs. The proposed approach takes advantage of hyper-dimensional computing which encodes data samples using random projections into a high dimensional space (HD space for short). Specifically, we propose a Hyper-dimensional Graph Learning (HDGL) algorithm that leverages the injectivity property of the node representations of a family of graph neural networks. HDGL maps node features to the HD space and then uses HD operators such as bundling and binding to aggregate information from the local neighborhood of each node. Results of experiments with widely used benchmark data sets show that HDGL achieves predictive performance that is competitive with the state-of-the-art deep learning methods, without the need for computationally expensive training.
    
[^116]: 驯服类别条件GAN中的长尾问题：通过在较低分辨率进行无条件训练进行知识共享

    Taming the Tail in Class-Conditional GANs: Knowledge Sharing via Unconditional Training at Lower Resolutions

    [https://arxiv.org/abs/2402.17065](https://arxiv.org/abs/2402.17065)

    通过在较低分辨率进行无条件训练，允许长尾类别从信息更丰富的类别中共享知识，以改善长尾数据下类别条件GANs的训练

    

    尽管对于使用有限训练数据训练生成对抗网络（GANs）进行了广泛的研究，但从长尾训练分布生成图像的技术仍然相当未被探索。在存在不平衡的多类别训练数据时，GANs倾向于偏爱样本更多的类别，导致尾部类别的生成低质量且样本不够多样化。在这项研究中，我们旨在改进使用长尾数据训练类别条件GANs。我们提出了一种简单而有效的知识共享方法，允许尾部类别从训练数据更丰富的类别中借鉴丰富的信息。具体地，我们对现有的类别条件GAN架构进行了修改，以确保生成器的较低分辨率层完全无条件地进行训练，同时将类别条件生成保留给较高分辨率层。在多个实验中进行了实验

    arXiv:2402.17065v1 Announce Type: cross  Abstract: Despite the extensive research on training generative adversarial networks (GANs) with limited training data, learning to generate images from long-tailed training distributions remains fairly unexplored. In the presence of imbalanced multi-class training data, GANs tend to favor classes with more samples, leading to the generation of low-quality and less diverse samples in tail classes. In this study, we aim to improve the training of class-conditional GANs with long-tailed data. We propose a straightforward yet effective method for knowledge sharing, allowing tail classes to borrow from the rich information from classes with more abundant training data. More concretely, we propose modifications to existing class-conditional GAN architectures to ensure that the lower-resolution layers of the generator are trained entirely unconditionally while reserving class-conditional generation for the higher-resolution layers. Experiments on seve
    
[^117]: 一个适用于高维输入的降阶模型多保真度方法论

    A Multi-Fidelity Methodology for Reduced Order Models with High-Dimensional Inputs

    [https://arxiv.org/abs/2402.17061](https://arxiv.org/abs/2402.17061)

    该研究介绍了一种面向高维设计空间的新型降阶模型多保真度方法，集成了机器学习技术用于流形对齐和维度减少。

    

    在航空航天设计的早期阶段，降阶模型（ROMs）对于在多次评估界面物理丰富的场信息的许多查询情景中最小化计算成本至关重要。航空航天设计的复杂性要求使用高维设计空间来准确捕获详细特征和设计变异。然而，这些空间引入了重要挑战，包括维度的诅咒，这是由于高维输入和输出需要大量训练数据和计算努力。为了解决这些复杂性，本研究介绍了一种针对高维背景设计的新颖的多保真度、参数化和非侵入式ROM框架。它集成了机器学习技术用于流形对齐和维度减少，采用Proper Orthogonal Decomposition（POD）和基于模型的主动子空间与多保真度回归。

    arXiv:2402.17061v1 Announce Type: new  Abstract: In the early stages of aerospace design, reduced order models (ROMs) are crucial for minimizing computational costs associated with using physics-rich field information in many-query scenarios requiring multiple evaluations. The intricacy of aerospace design demands the use of high-dimensional design spaces to capture detailed features and design variability accurately. However, these spaces introduce significant challenges, including the curse of dimensionality, which stems from both high-dimensional inputs and outputs necessitating substantial training data and computational effort. To address these complexities, this study introduces a novel multi-fidelity, parametric, and non-intrusive ROM framework designed for high-dimensional contexts. It integrates machine learning techniques for manifold alignment and dimension reduction employing Proper Orthogonal Decomposition (POD) and Model-based Active Subspace with multi-fidelity regressio
    
[^118]: 对各类网络攻击检测的最先进机器学习方法性能的研究：一项调查

    An Investigation into the Performances of the State-of-the-art Machine Learning Approaches for Various Cyber-attack Detection: A Survey

    [https://arxiv.org/abs/2402.17045](https://arxiv.org/abs/2402.17045)

    分析了过去10年针对不同类型网络攻击检测的各种最先进机器学习模型，着重比较了最新的工作。

    

    为了保护计算机和信息系统免受攻击者利用系统中的漏洞进行网络犯罪的侵害，已经提出了几种用于实时检测漏洞以提高信息系统安全性的方法。在所有提出的方法中，机器学习是实现系统安全的效果最好的方法，其能力范围从早期检测软件漏洞到实时检测系统中正在进行的妥协。由于存在不同类型的网络攻击，每种现有最先进的机器学习模型都依赖于不同的训练算法，这也影响了它们对特定类型网络攻击的适用性。在这项研究中，我们分析了过去10年针对不同类型网络攻击检测的每一个当前最先进的机器学习模型，重点放在最近的工作上以进行比较。

    arXiv:2402.17045v1 Announce Type: cross  Abstract: To secure computers and information systems from attackers taking advantage of vulnerabilities in the system to commit cybercrime, several methods have been proposed for real-time detection of vulnerabilities to improve security around information systems. Of all the proposed methods, machine learning had been the most effective method in securing a system with capabilities ranging from early detection of software vulnerabilities to real-time detection of ongoing compromise in a system. As there are different types of cyberattacks, each of the existing state-of-the-art machine learning models depends on different algorithms for training which also impact their suitability for detection of a particular type of cyberattack. In this research, we analyzed each of the current state-of-theart machine learning models for different types of cyberattack detection from the past 10 years with a major emphasis on the most recent works for comparat
    
[^119]: 通向从试验推广推理到目标种群的泛化

    Towards Generalizing Inferences from Trials to Target Populations

    [https://arxiv.org/abs/2402.17042](https://arxiv.org/abs/2402.17042)

    本研究在试图解决从试验结果推广到目标种群的外部有效性挑战方面取得了重要进展

    

    随机对照试验（RCTs）在产生内部有效估计方面起着至关重要的作用，而对扩展这些发现以获得外部有效估计至关重要，以促进更广泛的科学探究。本文探讨了应对这些外部有效性挑战的前沿，概括了2023年秋季在布朗大学计算与实验数学研究所（ICERM）举行的一次跨学科研讨会的精华。该研讨会汇集了来自社会科学、医学、公共卫生、统计学、计算机科学和教育等各个领域的专家，以解决每个学科在推断实验结果方面面临的独特障碍。我们的研究提出了三个关键贡献：我们整合正在进行的努力，突出了

    arXiv:2402.17042v1 Announce Type: cross  Abstract: Randomized Controlled Trials (RCTs) are pivotal in generating internally valid estimates with minimal assumptions, serving as a cornerstone for researchers dedicated to advancing causal inference methods. However, extending these findings beyond the experimental cohort to achieve externally valid estimates is crucial for broader scientific inquiry. This paper delves into the forefront of addressing these external validity challenges, encapsulating the essence of a multidisciplinary workshop held at the Institute for Computational and Experimental Research in Mathematics (ICERM), Brown University, in Fall 2023. The workshop congregated experts from diverse fields including social science, medicine, public health, statistics, computer science, and education, to tackle the unique obstacles each discipline faces in extrapolating experimental findings. Our study presents three key contributions: we integrate ongoing efforts, highlighting me
    
[^120]: 迭代INLA用于非线性动力系统中的状态和参数估计

    Iterated INLA for State and Parameter Estimation in Nonlinear Dynamical Systems

    [https://arxiv.org/abs/2402.17036](https://arxiv.org/abs/2402.17036)

    提出了一种基于迭代INLA的方法，用于在非线性动力系统中推断状态和参数，能够保留可解释性并且适用于任意非线性系统。

    

    数据同化（DA）方法使用源自微分方程的先验条件来稳健地对数据进行插值和外推。流行的技术，如处理高维非线性PDE先验条件的集合方法，主要关注状态估计，但可能会在学习参数方面遇到困难。另一方面，基于机器学习的方法可以自然地学习状态和参数，但它们的适用性可能受到限制，或者产生难以解释的不确定性。受空间统计中集成嵌套拉普拉斯近似（INLA）方法的启发，我们提出了一个基于迭代线性化动力学模型的DA替代方法。这在每次迭代中产生一个高斯马尔可夫随机场，使得可以使用INLA来推断状态和参数。我们的方法可以用于任意非线性系统，同时保持可解释性，并且进一步被证明可以o

    arXiv:2402.17036v1 Announce Type: cross  Abstract: Data assimilation (DA) methods use priors arising from differential equations to robustly interpolate and extrapolate data. Popular techniques such as ensemble methods that handle high-dimensional, nonlinear PDE priors focus mostly on state estimation, however can have difficulty learning the parameters accurately. On the other hand, machine learning based approaches can naturally learn the state and parameters, but their applicability can be limited, or produce uncertainties that are hard to interpret. Inspired by the Integrated Nested Laplace Approximation (INLA) method in spatial statistics, we propose an alternative approach to DA based on iteratively linearising the dynamical model. This produces a Gaussian Markov random field at each iteration, enabling one to use INLA to infer the state and parameters. Our approach can be used for arbitrary nonlinear systems, while retaining interpretability, and is furthermore demonstrated to o
    
[^121]: 从证明中提取定理的学习

    REFACTOR: Learning to Extract Theorems from Proofs

    [https://arxiv.org/abs/2402.17032](https://arxiv.org/abs/2402.17032)

    提出了一种名为REFACTOR的新方法，用于训练神经网络从证明中提取定理，新定理的引入帮助缩短证明长度并提高证明效率。

    

    人类数学家通常擅长识别模块化和可重用的定理，这些定理使复杂的数学结果易于获得。在本文中，我们提出了一个名为“定理从证明中提取器（REFACTOR）”的新方法，用于训练神经网络模仿这种形式数学定理证明的能力。我们展示了在一组未见证明上，REFACTOR能够提取出人类在写证明时会使用的19.6%的定理。当将模型应用于现有的Metamath库时，REFACTOR提取出了16个新定理。通过新提取的定理，我们展示了MetaMath数据库中的现有证明可以被重构。经重构后，这些新定理被非常频繁地使用，平均使用次数为733.5次，并有助于缩短证明长度。最后，我们展示了在经过新定理重构的数据集上训练的证明者证明了更多测试定理，并通过频繁利用超越了最先进的基准模型。

    arXiv:2402.17032v1 Announce Type: new  Abstract: Human mathematicians are often good at recognizing modular and reusable theorems that make complex mathematical results within reach. In this paper, we propose a novel method called theoREm-from-prooF extrACTOR (REFACTOR) for training neural networks to mimic this ability in formal mathematical theorem proving. We show on a set of unseen proofs, REFACTOR is able to extract 19.6% of the theorems that humans would use to write the proofs. When applying the model to the existing Metamath library, REFACTOR extracted 16 new theorems. With newly extracted theorems, we show that the existing proofs in the MetaMath database can be refactored. The new theorems are used very frequently after refactoring, with an average usage of 733.5 times, and help shorten the proof lengths. Lastly, we demonstrate that the prover trained on the new-theorem refactored dataset proves more test theorems and outperforms state-of-the-art baselines by frequently lever
    
[^122]: 通过完全卷积和可微的前端与跳跃连接对梯度攻击表现出显著韧性的耐人寻味案例

    A Curious Case of Remarkable Resilience to Gradient Attacks via Fully Convolutional and Differentiable Front End with a Skip Connection

    [https://arxiv.org/abs/2402.17018](https://arxiv.org/abs/2402.17018)

    通过在神经模型中引入不同iable和完全卷积的前端模型，并结合跳跃连接，成功实现对梯度攻击的显著韧性，并通过将模型组合成随机集合，有效对抗黑盒攻击。

    

    我们测试了通过在一个冻结的分类器之前增加一个可微且完全卷积的模型，并具有跳跃连接的前端增强神经模型。通过使用较小的学习率进行大约一个epoch的训练，我们获得了一些模型，这些模型在保持骨干分类器准确性的同时，对包括AutoAttack软件包中的APGD和FAB-T攻击在内的梯度攻击具有异常的抵抗力，这归因于梯度掩盖。梯度掩盖现象并不新鲜，但对于这些没有梯度破坏部分（如JPEG压缩或预计导致梯度减小的部分）的完全可微模型来说，掩盖的程度相当显著。尽管黑盒攻击对梯度掩盖可能部分有效，但通过将模型组合成随机集合，可以轻松击败它们。我们估计这样的集合在CIFAR10和CIF等上实现了几乎SOTA级别的AutoAttack准确性。

    arXiv:2402.17018v1 Announce Type: cross  Abstract: We tested front-end enhanced neural models where a frozen classifier was prepended by a differentiable and fully convolutional model with a skip connection. By training them using a small learning rate for about one epoch, we obtained models that retained the accuracy of the backbone classifier while being unusually resistant to gradient attacks including APGD and FAB-T attacks from the AutoAttack package, which we attributed to gradient masking. The gradient masking phenomenon is not new, but the degree of masking was quite remarkable for fully differentiable models that did not have gradient-shattering components such as JPEG compression or components that are expected to cause diminishing gradients.   Though black box attacks can be partially effective against gradient masking, they are easily defeated by combining models into randomized ensembles. We estimate that such ensembles achieve near-SOTA AutoAttack accuracy on CIFAR10, CIF
    
[^123]: 在瑞士司法预测中实现可解释性和公平性：在一个多语言数据集上进行基准测试

    Towards Explainability and Fairness in Swiss Judgement Prediction: Benchmarking on a Multilingual Dataset

    [https://arxiv.org/abs/2402.17013](https://arxiv.org/abs/2402.17013)

    本研究深入探讨了在瑞士司法预测中实现可解释性和公平性的重要性，利用了唯一可用的多语言LJP数据集，并对最新的单语和多语BERT-based LJP模型进行了可解释性能评估。

    

    arXiv:2402.17013v1 公告类型：跨领域 摘要：对法律裁决预测（LJP）系统中的可解释性进行评估在构建值得信赖和透明系统方面至关重要，特别是考虑到这些系统依赖可能缺乏法律相关性或涉及敏感属性的因素。本研究深入探讨了LJP模型中可解释性和公平性的领域，利用瑞士裁决预测（SJP）这一唯一可用的多语言LJP数据集。我们整理了一个包括108个案例的支持和反对法律专家裁决的理由的全面收集，在德语、法语和意大利语中提供。通过采用基于遮挡的可解释性方法，我们评估了最新的单语和多语BERT-based LJP模型的可解释性表现，以及利用数据增强和跨语言转移等技术开发的模型，这些模型展示了预测性能的提高。值得注意的是，我们的发现

    arXiv:2402.17013v1 Announce Type: cross  Abstract: The assessment of explainability in Legal Judgement Prediction (LJP) systems is of paramount importance in building trustworthy and transparent systems, particularly considering the reliance of these systems on factors that may lack legal relevance or involve sensitive attributes. This study delves into the realm of explainability and fairness in LJP models, utilizing Swiss Judgement Prediction (SJP), the only available multilingual LJP dataset. We curate a comprehensive collection of rationales that `support' and `oppose' judgement from legal experts for 108 cases in German, French, and Italian. By employing an occlusion-based explainability approach, we evaluate the explainability performance of state-of-the-art monolingual and multilingual BERT-based LJP models, as well as models developed with techniques such as data augmentation and cross-lingual transfer, which demonstrated prediction performance improvement. Notably, our finding
    
[^124]: Pandora's White-Box：开放LLMs中训练数据泄漏的增加

    Pandora's White-Box: Increased Training Data Leakage in Open LLMs

    [https://arxiv.org/abs/2402.17012](https://arxiv.org/abs/2402.17012)

    本文对开源大型语言模型（LLMs）进行了隐私攻击研究，提出了首个能同时实现高真正率和低误分类率的预训练LLMs会员推理攻击（MIAs），以及展示了在自然环境中可以从微调LLM中提取超过50%的微调数据集。

    

    在本文中，我们对开源的大型语言模型（LLMs）遭受的隐私攻击进行了系统研究，其中对手可以访问模型权重、梯度或损失，试图利用它们来了解底层训练数据。我们的主要结果是针对预训练LLMs的第一个会员推理攻击（MIAs），能够同时实现高TPR和低FPR，并展示了在自然环境中可以从微调LLM中提取超过50%的微调数据集。我们考虑了对底层模型的不同访问程度、语言模型的定制化以及攻击者可以使用的资源。在预训练设置中，我们提出了三种新的白盒MIAs：基于梯度范数的攻击、监督神经网络分类器和单步损失比攻击。所有这些都优于现有的黑盒基线，并且我们的.....

    arXiv:2402.17012v1 Announce Type: cross  Abstract: In this paper we undertake a systematic study of privacy attacks against open source Large Language Models (LLMs), where an adversary has access to either the model weights, gradients, or losses, and tries to exploit them to learn something about the underlying training data. Our headline results are the first membership inference attacks (MIAs) against pre-trained LLMs that are able to simultaneously achieve high TPRs and low FPRs, and a pipeline showing that over $50\%$ (!) of the fine-tuning dataset can be extracted from a fine-tuned LLM in natural settings. We consider varying degrees of access to the underlying model, customization of the language model, and resources available to the attacker. In the pre-trained setting, we propose three new white-box MIAs: an attack based on the gradient norm, a supervised neural network classifier, and a single step loss ratio attack. All outperform existing black-box baselines, and our supervi
    
[^125]: 在临床试验中监测在线强化学习算法的准确性

    Monitoring Fidelity of Online Reinforcement Learning Algorithms in Clinical Trials

    [https://arxiv.org/abs/2402.17003](https://arxiv.org/abs/2402.17003)

    提出了算法准确性作为在临床试验中部署在线RL算法的关键要求，强调了对参与者保护和数据科学效用的保留责任，并提出了一个框架进行预部署规划和实时监测以确保算法准确性。

    

    在线强化学习（RL）算法为个性化临床试验中参与者的治疗提供了巨大潜力。然而，在高风险医疗领域部署在线自主算法使得质量控制和数据质量特别难以实现。本文提出了作为在临床试验中部署在线RL算法的关键要求的算法准确性。它强调了算法对（1）保护参与者和（2）保留数据在试验后分析中的科学效用的责任。我们还提出了一个用于部署前规划和实时监测的框架，以协助算法开发者和临床研究人员确保算法的准确性。为了说明我们框架的实际应用，我们介绍了来自Oralytics临床试验的真实案例。自2023年春季以来，这项试验成功地部署了一种自主的在线RL算法来进行个

    arXiv:2402.17003v1 Announce Type: cross  Abstract: Online reinforcement learning (RL) algorithms offer great potential for personalizing treatment for participants in clinical trials. However, deploying an online, autonomous algorithm in the high-stakes healthcare setting makes quality control and data quality especially difficult to achieve. This paper proposes algorithm fidelity as a critical requirement for deploying online RL algorithms in clinical trials. It emphasizes the responsibility of the algorithm to (1) safeguard participants and (2) preserve the scientific utility of the data for post-trial analyses. We also present a framework for pre-deployment planning and real-time monitoring to help algorithm developers and clinical researchers ensure algorithm fidelity. To illustrate our framework's practical application, we present real-world examples from the Oralytics clinical trial. Since Spring 2023, this trial successfully deployed an autonomous, online RL algorithm to persona
    
[^126]: 通过隐含正交偏置发现对称群结构

    Discovering Symmetry Group Structures via Implicit Orthogonality Bias

    [https://arxiv.org/abs/2402.17002](https://arxiv.org/abs/2402.17002)

    HyperCube网络通过独特的因式分解架构和正则化器，成功学习了对称群的操作，能够高效地恢复完整操作表，并形成广义傅里叶基进行群卷积。

    

    我们介绍了HyperCube网络，一个用于自动发现数据中对称群结构的新方法。关键创新在于独特的因式分解架构，结合一种新颖的正则化器，向学习正交表示灌输了强大的归纳偏差。这利用了表示理论的一个基本定理，即所有紧致/有限群都可以由正交矩阵表示。HyperCube能够高效地从部分观测数据中学习通用群操作，成功恢复完整的操作表。值得注意的是，所学习出的因素直接对应于底层群的精确矩阵表示。此外，这些因素捕捉到了群的完整不可约表示集合，形成了执行群卷积的广义傅里叶基。在对群和非群符号操作进行的大量实验证明，HyperCube展示了10

    arXiv:2402.17002v1 Announce Type: new  Abstract: We introduce the HyperCube network, a novel approach for autonomously discovering symmetry group structures within data. The key innovation is a unique factorization architecture coupled with a novel regularizer that instills a powerful inductive bias towards learning orthogonal representations. This leverages a fundamental theorem of representation theory that all compact/finite groups can be represented by orthogonal matrices. HyperCube efficiently learns general group operations from partially observed data, successfully recovering complete operation tables. Remarkably, the learned factors correspond directly to exact matrix representations of the underlying group. Moreover, these factors capture the group's complete set of irreducible representations, forming the generalized Fourier basis for performing group convolutions. In extensive experiments with both group and non-group symbolic operations, HyperCube demonstrates a dramatic 10
    
[^127]: 语言模型听到了什么？探究语言模型中的听觉表征

    What Do Language Models Hear? Probing for Auditory Representations in Language Models

    [https://arxiv.org/abs/2402.16998](https://arxiv.org/abs/2402.16998)

    通过训练一个线性探针，将语言模型中的文本表示和预训练音频模型中的声音表示联系在一起，研究发现尽管仅在原始文本上进行训练，语言模型对于一些对象的声音知识有着基于实质的编码。

    

    这项工作探讨了语言模型是否对物体的声音具有含义深刻且基于实质的表征。我们学习了一个线性探针，通过一个预训练的音频模型给出一个对象的声音表示，从而在给定与该对象相关的音频片段的情况下检索出该对象的正确文本表示。这个探针是通过对比损失进行训练的，推动对象的语言表示和声音表示彼此接近。在训练之后，我们测试了探针对于一些在训练中没有见过的对象的泛化能力。在不同的语言模型和音频模型中，我们发现在许多情况下探针的泛化能力超过了随机猜测的水平，这表明尽管仅在原始文本上进行训练，语言模型对于一些对象的声音知识具有基于实质的编码。

    arXiv:2402.16998v1 Announce Type: cross  Abstract: This work explores whether language models encode meaningfully grounded representations of sounds of objects. We learn a linear probe that retrieves the correct text representation of an object given a snippet of audio related to that object, where the sound representation is given by a pretrained audio model. This probe is trained via a contrastive loss that pushes the language representations and sound representations of an object to be close to one another. After training, the probe is tested on its ability to generalize to objects that were not seen during training. Across different language models and audio models, we find that the probe generalization is above chance in many cases, indicating that despite being trained only on raw text, language models encode grounded knowledge of sounds for some objects.
    
[^128]: 探索被动听取言语时大脑活动的解码

    Towards Decoding Brain Activity During Passive Listening of Speech

    [https://arxiv.org/abs/2402.16996](https://arxiv.org/abs/2402.16996)

    本研究通过深度学习方法从颅内脑电图数据解码被动听取的言语，旨在推动脑机接口技术的言语合成应用，并提供对言语知觉认知过程的额外视角。

    

    该研究的目的是探究言语知觉的复杂机制，并最终解码在听取言语时大脑中产生的电信号变化。我们尝试利用深度学习方法从颅内脑电图（iEEG）数据中解码所听取的言语。旨在推动脑机接口（BCI）技术在言语合成方面的发展，并希望为言语知觉的认知过程提供额外视角。这种方法偏离了传统对言语产生的关注，转而选择研究所感知的言语的神经表示。这一视角打开了复杂的视野，有可能让我们研究更复杂的神经模式。利用深度学习模型的能力，研究旨在建立这些复杂的神经活动与相应言语声音之间的联系。尽管这种方法 diverges from the conventional focus on speech production and instead chooses to investigate neural representations of perceived speech. 该研究new angle opened up a complex perspective, potentially allowing us to study more sophisticated neural patterns.

    arXiv:2402.16996v1 Announce Type: cross  Abstract: The aim of the study is to investigate the complex mechanisms of speech perception and ultimately decode the electrical changes in the brain accruing while listening to speech. We attempt to decode heard speech from intracranial electroencephalographic (iEEG) data using deep learning methods. The goal is to aid the advancement of brain-computer interface (BCI) technology for speech synthesis, and, hopefully, to provide an additional perspective on the cognitive processes of speech perception. This approach diverges from the conventional focus on speech production and instead chooses to investigate neural representations of perceived speech. This angle opened up a complex perspective, potentially allowing us to study more sophisticated neural patterns. Leveraging the power of deep learning models, the research aimed to establish a connection between these intricate neural activities and the corresponding speech sounds. Despite the appro
    
[^129]: GEM3D：三维形状合成的生成媒体抽象

    GEM3D: GEnerative Medial Abstractions for 3D Shape Synthesis

    [https://arxiv.org/abs/2402.16994](https://arxiv.org/abs/2402.16994)

    GEM3D提出了一种新的深度、拓扑感知的三维形状生成模型，通过神经骨架编码了拓扑和几何信息，通过骨架驱动的神经隐式公式生成准确和多样化的表面。

    

    我们介绍了GEM3D——一种新的深度、拓扑感知的三维形状生成模型。我们方法的关键部分是基于神经骨架的表示，编码了关于形状拓扑和几何的信息。通过一个去噪扩散概率模型，我们的方法首先生成遵循中轴变换（MAT）的基于骨架的表示，然后通过一个骨架驱动的神经隐式公式生成表面。神经隐式考虑了在生成的骨架表示中存储的拓扑和几何信息，产生的表面与之前的神经场公式相比更加拓扑和几何准确。我们讨论了我们的方法在形状合成和点云重建任务中的应用，并对我们的方法进行了定性和定量评估。我们展示了比以前更为准确和多样化的形状重建。

    arXiv:2402.16994v1 Announce Type: cross  Abstract: We introduce GEM3D -- a new deep, topology-aware generative model of 3D shapes. The key ingredient of our method is a neural skeleton-based representation encoding information on both shape topology and geometry. Through a denoising diffusion probabilistic model, our method first generates skeleton-based representations following the Medial Axis Transform (MAT), then generates surfaces through a skeleton-driven neural implicit formulation. The neural implicit takes into account the topological and geometric information stored in the generated skeleton representations to yield surfaces that are more topologically and geometrically accurate compared to previous neural field formulations. We discuss applications of our method in shape synthesis and point cloud reconstruction tasks, and evaluate our method both qualitatively and quantitatively. We demonstrate significantly more faithful surface reconstruction and diverse shape generation r
    
[^130]: 扩散模型中的相变揭示了数据的分层性质

    A Phase Transition in Diffusion Models Reveals the Hierarchical Nature of Data

    [https://arxiv.org/abs/2402.16991](https://arxiv.org/abs/2402.16991)

    扩散模型在研究数据的分层生成模型中展示出了在阈值时间发生相变的特性，这影响了高级特征和低级特征的重建过程。

    

    理解真实数据的结构在推动现代深度学习方法方面至关重要。自然数据，如图像，被认为是由以层次和组合方式组织的特征组成的，神经网络在学习过程中捕捉到这些特征。最近的进展显示，扩散模型能够生成高质量的图像，暗示了它们捕捉到这种潜在结构的能力。我们研究了数据的分层生成模型中的这一现象。我们发现，在时间$t$后作用的反向扩散过程受到某个阈值时间处的相变控制，此时重建高级特征（如图像的类别）的概率突然下降。相反，低级特征（如图像的具体细节）的重建在整个扩散过程中平稳演变。这一结果暗示，在超出转变时间的时刻，类别已变化，但是基

    arXiv:2402.16991v1 Announce Type: cross  Abstract: Understanding the structure of real data is paramount in advancing modern deep-learning methodologies. Natural data such as images are believed to be composed of features organised in a hierarchical and combinatorial manner, which neural networks capture during learning. Recent advancements show that diffusion models can generate high-quality images, hinting at their ability to capture this underlying structure. We study this phenomenon in a hierarchical generative model of data. We find that the backward diffusion process acting after a time $t$ is governed by a phase transition at some threshold time, where the probability of reconstructing high-level features, like the class of an image, suddenly drops. Instead, the reconstruction of low-level features, such as specific details of an image, evolves smoothly across the whole diffusion process. This result implies that at times beyond the transition, the class has changed but the gene
    
[^131]: inGRASS: 通过低阻抗直径分解实现增量图谱稀疏化

    inGRASS: Incremental Graph Spectral Sparsification via Low-Resistance-Diameter Decomposition

    [https://arxiv.org/abs/2402.16990](https://arxiv.org/abs/2402.16990)

    inGRASS提出了一种用于大型无向图增量谱稀疏化的新算法，其具有高度可扩展性和并行友好性，关键创新在于低阻抗直径分解方案，能够高效识别关键边和检测多余边。

    

    这项工作介绍了inGRASS，这是一种旨在对大型无向图进行增量谱稀疏化的新算法。所提出的inGRASS算法具有高度可扩展性和并行友好性，设置阶段的时间复杂度几乎是线性的，并且能够在对具有N个节点的原始图进行增量更改时，以$O(\log N)$的时间更新谱稀疏器。在inGRASS的设置阶段中，一个关键组件是引入了多级阻抗嵌入框架，用于高效识别谱关键边并有效检测多余边，这是通过将初始稀疏器分解为许多节点群集并利用低阻抗直径分解（LRD）方案来实现的。inGRASS的更新阶段利用低维节点嵌入向量，有效估计每个新添加边的重要性和唯一性。

    arXiv:2402.16990v1 Announce Type: cross  Abstract: This work presents inGRASS, a novel algorithm designed for incremental spectral sparsification of large undirected graphs. The proposed inGRASS algorithm is highly scalable and parallel-friendly, having a nearly-linear time complexity for the setup phase and the ability to update the spectral sparsifier in $O(\log N)$ time for each incremental change made to the original graph with $N$ nodes. A key component in the setup phase of inGRASS is a multilevel resistance embedding framework introduced for efficiently identifying spectrally-critical edges and effectively detecting redundant ones, which is achieved by decomposing the initial sparsifier into many node clusters with bounded effective-resistance diameters leveraging a low-resistance-diameter decomposition (LRD) scheme. The update phase of inGRASS exploits low-dimensional node embedding vectors for efficiently estimating the importance and uniqueness of each newly added edge. As de
    
[^132]: 内容审核中的算法武断性

    Algorithmic Arbitrariness in Content Moderation

    [https://arxiv.org/abs/2402.16979](https://arxiv.org/abs/2402.16979)

    机器学习在内容审核中引入了预测多样性的挑战，可能导致内容被武断分类并限制言论自由。

    

    机器学习被广泛应用于在线内容审核。尽管相对于人工审核具有可扩展性，但使用机器学习在内容审核中引入了独特的挑战。其中之一是预测的多样性：针对内容分类的多个竞争模型可能在平均表现上同样出色，但对同一内容给出了冲突的预测。这种多样性可能源自模型开发过程中看似无害的选择，比如参数初始化的随机种子选择。我们通过实验证明内容审核工具如何可以武断地将样本分类为有毒的，导致言论受到武断的限制。我们从《国际公民和政治权利公约》所规定的人权角度，特别是言论自由、非歧视和程序公正，讨论了这些发现。我们分析了（i）最先进的审核工具之间预测多样性的程度。

    arXiv:2402.16979v1 Announce Type: cross  Abstract: Machine learning (ML) is widely used to moderate online content. Despite its scalability relative to human moderation, the use of ML introduces unique challenges to content moderation. One such challenge is predictive multiplicity: multiple competing models for content classification may perform equally well on average, yet assign conflicting predictions to the same content. This multiplicity can result from seemingly innocuous choices during model development, such as random seed selection for parameter initialization. We experimentally demonstrate how content moderation tools can arbitrarily classify samples as toxic, leading to arbitrary restrictions on speech. We discuss these findings in terms of human rights set out by the International Covenant on Civil and Political Rights (ICCPR), namely freedom of expression, non-discrimination, and procedural justice. We analyze (i) the extent of predictive multiplicity among state-of-the-ar
    
[^133]: 一种不精确的Bregman近端点方法及其加速版本用于不平衡最优输运问题

    An inexact Bregman proximal point method and its acceleration version for unbalanced optimal transport

    [https://arxiv.org/abs/2402.16978](https://arxiv.org/abs/2402.16978)

    该论文提出了一种不精确的Bregman近端点方法，用于解决不平衡最优输运问题，能够在保持算法收敛性和稳定性的同时，对于大正则化参数和小正则化参数均有较好的表现。

    

    不平衡最优输运（UOT）问题在计算生物学、计算成像和深度学习中发挥着日益重要的作用。由于其便利性和良好的收敛特性，缩放算法被广泛用于解决UOT。然而，对于较大的正则化参数，这种算法的精度较低，而由于稳定性问题，较小的正则化参数很容易导致数值溢出。我们通过开发一种不精确的Bregman近端点方法来解决这一挑战。该算法在每次迭代中使用缩放算法来近似近端算子。该算法（1）收敛于UOT的真实解，（2）具有理论保证和稳健的正则化参数选择，（3）缓解了数值稳定性问题，（4）在具体实践中可以实现与缩放算法相当的计算复杂度。

    arXiv:2402.16978v1 Announce Type: cross  Abstract: The Unbalanced Optimal Transport (UOT) problem plays increasingly important roles in computational biology, computational imaging and deep learning. Scaling algorithm is widely used to solve UOT due to its convenience and good convergence properties. However, this algorithm has lower accuracy for large regularization parameters, and due to stability issues, small regularization parameters can easily lead to numerical overflow. We address this challenge by developing an inexact Bregman proximal point method for solving UOT. This algorithm approximates the proximal operator using the Scaling algorithm at each iteration. The algorithm (1) converges to the true solution of UOT, (2) has theoretical guarantees and robust regularization parameter selection, (3) mitigates numerical stability issues, and (4) can achieve comparable computational complexity to the Scaling algorithm in specific practice. Building upon this, we develop an accelerat
    
[^134]: 具有布局学习的去卷积三维场景生成

    Disentangled 3D Scene Generation with Layout Learning

    [https://arxiv.org/abs/2402.16936](https://arxiv.org/abs/2402.16936)

    该方法通过布局学习实现了将三维场景分解为各个单独对象，从而在文本到三维内容创建方面带来了新的功能。

    

    我们介绍了一种方法来生成被分解成其组件对象的三维场景。这种分解是无监督的，仅依赖于一个大型预训练的文本到图像模型的知识。我们的关键洞察是，通过找到一个三维场景的部分，在空间重新布置时仍然产生相同场景的有效配置，可以发现对象。具体来说，我们的方法从头开始联合优化多个 NeRF 模型 - 每个模型代表其自己的对象 - 以及将这些对象组成场景的一组布局。然后，我们鼓励这些合成的场景根据图像生成器保持在分布中。我们展示了，尽管方法简单，但我们的方法成功地生成了分解为单独对象的三维场景，实现了文本到三维内容创建中的新功能。有关结果和交互式演示，请查看我们的项目页面 https://dave.ml/layoutlearning/

    arXiv:2402.16936v1 Announce Type: cross  Abstract: We introduce a method to generate 3D scenes that are disentangled into their component objects. This disentanglement is unsupervised, relying only on the knowledge of a large pretrained text-to-image model. Our key insight is that objects can be discovered by finding parts of a 3D scene that, when rearranged spatially, still produce valid configurations of the same scene. Concretely, our method jointly optimizes multiple NeRFs from scratch - each representing its own object - along with a set of layouts that composite these objects into scenes. We then encourage these composited scenes to be in-distribution according to the image generator. We show that despite its simplicity, our approach successfully generates 3D scenes decomposed into individual objects, enabling new capabilities in text-to-3D content creation. For results and an interactive demo, see our project page at https://dave.ml/layoutlearning/
    
[^135]: FedReview: 一种用于拒绝毒化更新的联邦学习审查机制

    FedReview: A Review Mechanism for Rejecting Poisoned Updates in Federated Learning

    [https://arxiv.org/abs/2402.16934](https://arxiv.org/abs/2402.16934)

    提出了FedReview机制，通过随机分配评审员客户端来识别和拒绝联邦学习中的潜在毒化更新，并采用多数表决机制来整合排名并移除这些更新。

    

    Federated learning最近已经被提出作为一种去中心化的方法，在不访问用户数据的情况下学习一个高性能模型。尽管其有效性，但联邦学习给恶意用户提供了机会通过向服务器上传毒化模型更新来操纵模型。在本文中，我们提出了一种名为FedReview的审查机制，用于识别和拒绝联邦学习中潜在的毒化更新。在我们的机制下，服务器每轮随机分配子集客户端作为评审员，在其训练数据集上评估模型更新。评审员根据评价结果对模型更新进行排名，统计相对低质量的更新数量作为估计的毒化更新数量。基于审查报告，服务器采用多数表决机制整合排名并在模型聚合过程中去除潜在的毒化更新。

    arXiv:2402.16934v1 Announce Type: cross  Abstract: Federated learning has recently emerged as a decentralized approach to learn a high-performance model without access to user data. Despite its effectiveness, federated learning gives malicious users opportunities to manipulate the model by uploading poisoned model updates to the server. In this paper, we propose a review mechanism called FedReview to identify and decline the potential poisoned updates in federated learning. Under our mechanism, the server randomly assigns a subset of clients as reviewers to evaluate the model updates on their training datasets in each round. The reviewers rank the model updates based on the evaluation results and count the number of the updates with relatively low quality as the estimated number of poisoned updates. Based on review reports, the server employs a majority voting mechanism to integrate the rankings and remove the potential poisoned updates in the model aggregation process. Extensive evalu
    
[^136]: 使用人类概念形成避免视觉分类中的灾难性遗忘

    Avoiding Catastrophic Forgetting in Visual Classification Using Human Concept Formation

    [https://arxiv.org/abs/2402.16933](https://arxiv.org/abs/2402.16933)

    提出了一种名为Cobweb4V的新颖视觉分类方法，利用人类类似学习系统，避免了灾难性遗忘效应，与传统方法相比，需要更少的数据来实现有效学习成果，并保持稳定性能。

    

    深度神经网络在机器学习中表现出色，特别是在视觉任务中，然而，当按顺序学习新任务时，它们经常面临灾难性遗忘。本研究提出了Cobweb4V，这是一种新颖的视觉分类方法，它基于Cobweb，这是一种人类类似的学习系统，受到人类随时间逐渐学习新概念的启发。我们进行了全面评估，展示了Cobweb4V在学习视觉概念方面的熟练程度，相较于传统方法，需要更少的数据来实现有效的学习成果，随时间保持稳定的性能，并实现了令人称赞的渐近行为，避免了灾难性遗忘效应。这些特征与人类认知中的学习策略一致，将Cobweb4V定位为神经网络方法的一个有前途的替代方案。

    arXiv:2402.16933v1 Announce Type: cross  Abstract: Deep neural networks have excelled in machine learning, particularly in vision tasks, however, they often suffer from catastrophic forgetting when learning new tasks sequentially. In this work, we propose Cobweb4V, a novel visual classification approach that builds on Cobweb, a human like learning system that is inspired by the way humans incrementally learn new concepts over time. In this research, we conduct a comprehensive evaluation, showcasing the proficiency of Cobweb4V in learning visual concepts, requiring less data to achieve effective learning outcomes compared to traditional methods, maintaining stable performance over time, and achieving commendable asymptotic behavior, without catastrophic forgetting effects. These characteristics align with learning strategies in human cognition, positioning Cobweb4V as a promising alternative to neural network approaches.
    
[^137]: TrustMol: 通过与分子动力学对齐实现值得信赖的逆向分子设计

    TrustMol: Trustworthy Inverse Molecular Design via Alignment with Molecular Dynamics

    [https://arxiv.org/abs/2402.16930](https://arxiv.org/abs/2402.16930)

    TrustMol提出了一种值得信赖的逆向分子设计方法，利用新的变分自动编码器网络和潜在属性对获取方法来有效导航分子的潜在优化复杂性。

    

    随着近年来对具有期望属性的分子进行数据驱动生成（也被称为逆向分子设计IMD）的显著关注，在精确度和解决方案多样性方面取得了重大进展。尽管现有的IMD方法在解决方面的准确性和多样性上取得了显著进展，但在可靠性方面仍然滞后。这些方法设计过程的根本问题是越来越隐含和间接，而且这一过程也与本机的前向过程（NFP）隔离，后者是模拟分子动力学的地面真实函数。基于这一思路，我们提出TrustMol，这是一种值得信赖的IMD方法。为此，TrustMol依赖于一组技术创新，包括一个新的变分自动编码器网络。此外，我们提出了一种潜在属性对获取方法，以有效地导航分子潜在优化的复杂性，这个过程看似直观但由于高频率.

    arXiv:2402.16930v1 Announce Type: cross  Abstract: Data-driven generation of molecules with desired properties, also known as inverse molecular design (IMD), has attracted significant attention in recent years. Despite the significant progress in the accuracy and diversity of solutions, existing IMD methods lag behind in terms of trustworthiness. The root issue is that the design process of these methods is increasingly more implicit and indirect, and this process is also isolated from the native forward process (NFP), the ground-truth function that models the molecular dynamics. Following this insight, we propose TrustMol, an IMD method built to be trustworthy. For this purpose, TrustMol relies on a set of technical novelties including a new variational autoencoder network. Moreover, we propose a latent-property pairs acquisition method to effectively navigate the complexities of molecular latent optimization, a process that seems intuitive yet challenging due to the high-frequency an
    
[^138]: 机器学习后门检测的可行性问题作为假设检验问题的探讨

    On the (In)feasibility of ML Backdoor Detection as an Hypothesis Testing Problem

    [https://arxiv.org/abs/2402.16926](https://arxiv.org/abs/2402.16926)

    提出了机器学习系统中后门检测问题的正式统计定义，并得出了后门检测的不可能性与可实现性结果，指出了通用后门检测的局限性，强调后门检测方法需要考虑敌对因素。

    

    我们提出了一种正式的统计学定义，用于分析机器学习系统中的后门检测问题的可行性，并使用它来分析这些问题的可行性，提供了对我们定义的实用性和适用性的证据。这项工作的主要贡献是对后门检测的不可能性结果和可实现性结果。我们展示了一个没有免费午餐定理，证明了通用（对敌方不知情）后门检测是不可能的，除非Alphabet大小非常小。因此，我们认为，后门检测方法需要明确地或隐式地考虑敌对因素。然而，我们的工作并不意味着后门检测在特定场景下不能起作用，因为科学文献中成功的后门检测方法证明了这一点。此外，我们将我们的定义与大概近似正确（PAC）学习与外分布检测问题联系起来。

    arXiv:2402.16926v1 Announce Type: cross  Abstract: We introduce a formal statistical definition for the problem of backdoor detection in machine learning systems and use it to analyze the feasibility of such problems, providing evidence for the utility and applicability of our definition. The main contributions of this work are an impossibility result and an achievability result for backdoor detection. We show a no-free-lunch theorem, proving that universal (adversary-unaware) backdoor detection is impossible, except for very small alphabet sizes. Thus, we argue, that backdoor detection methods need to be either explicitly, or implicitly adversary-aware. However, our work does not imply that backdoor detection cannot work in specific scenarios, as evidenced by successful backdoor detection methods in the scientific literature. Furthermore, we connect our definition to the probably approximately correct (PAC) learnability of the out-of-distribution detection problem.
    
[^139]: 使用带有图神经网络的强结构可控性强化学习最小化控制输入

    Minimize Control Inputs for Strong Structural Controllability Using Reinforcement Learning with Graph Neural Network

    [https://arxiv.org/abs/2402.16925](https://arxiv.org/abs/2402.16925)

    使用带有图神经网络的强化学习方法，将图着色问题形式化为马尔可夫决策过程，有效解决了强结构可控性条件下最小化控制输入的问题。

    

    强结构可控性(SSC)保证具有线性不变动力学的网络系统对于所有参数的数值实现都是可控的。当前研究已经为零/非零或零/非零/任意结构的SSC建立了代数和图论条件。一个相关的实际问题是如何用最少的输入信号完全控制系统，并确定必须施加信号的节点。先前的研究表明，这个优化问题是NP难的，很难找到解决方案。为了解决这个问题，我们根据零/非零和零/非零/任意结构的SSC的图论条件，将图着色过程构建为马尔可夫决策过程(MDP)。我们使用带有有向图神经网络的Actor-critic方法来表示图的着色信息以优化MDP。我们的方法在一个社交影响网络中得到了验证。

    arXiv:2402.16925v1 Announce Type: cross  Abstract: Strong structural controllability (SSC) guarantees networked system with linear-invariant dynamics controllable for all numerical realizations of parameters. Current research has established algebraic and graph-theoretic conditions of SSC for zero/nonzero or zero/nonzero/arbitrary structure. One relevant practical problem is how to fully control the system with the minimal number of input signals and identify which nodes must be imposed signals. Previous work shows that this optimization problem is NP-hard and it is difficult to find the solution. To solve this problem, we formulate the graph coloring process as a Markov decision process (MDP) according to the graph-theoretical condition of SSC for both zero/nonzero and zero/nonzero/arbitrary structure. We use Actor-critic method with Directed graph neural network which represents the color information of graph to optimize MDP. Our method is validated in a social influence network with
    
[^140]: 通过神经结构搜索实现个性化的联邦指令调优

    Personalized Federated Instruction Tuning via Neural Architecture Search

    [https://arxiv.org/abs/2402.16919](https://arxiv.org/abs/2402.16919)

    提出了一种基于体系结构搜索的新型个性化联邦指令调优（PerFIT）框架，允许每个客户端通过扩展全局模型的可训练参数空间来搜索个性化体系结构，解决了FIT面临的数据和资源异质性问题。

    

    Federated Instruction Tuning (FIT)已经表明在不共享私人数据的情况下，能够实现庞大数据所有者之间的协作模型指令调优能力。然而，它仍然面临两个关键挑战，即数据和资源的异质性。由于数据所有者之间的数据分布和偏好各不相同，FIT无法适应个人所有者的个性化数据。此外，具有优越计算能力的客户端受到限制，因为它们需要保持与较弱客户端相同的微调架构。为了解决这些问题，我们提出了一种基于体系结构搜索的新型个性化联邦指令调优（PerFIT）框架。具体而言，PerFIT允许每个客户端通过扩展全局模型的可训练参数空间来搜索个性化体系结构，然后将参数修剪到原始状态。该过程允许在扩展的情况下进行个性化指令微调。

    arXiv:2402.16919v1 Announce Type: new  Abstract: Federated Instruction Tuning (FIT) has shown the ability to achieve collaborative model instruction tuning among massive data owners without sharing private data. However, it still faces two key challenges, i.e., data and resource heterogeneity. Due to the varying data distribution and preferences among data owners, FIT cannot adapt to the personalized data of individual owners. Moreover, clients with superior computational abilities are constrained since they need to maintain the same fine-tuning architecture as the weaker clients. To address these issues, we propose a novel Personalized Federated Instruction Tuning (PerFIT) framework based on architecture search. Specifically, PerFIT allows each client to search for a personalized architecture by expanding the trainable parameter space of the global model followed by pruning the parameters to the original state. This procedure allows personalized instruction fine-tuning within expanded
    
[^141]: m2mKD：模块间知识蒸馏用于模块化Transformer

    m2mKD: Module-to-Module Knowledge Distillation for Modular Transformers

    [https://arxiv.org/abs/2402.16918](https://arxiv.org/abs/2402.16918)

    提出了用于在模块之间传递知识的通用模块到模块知识蒸馏（m2mKD）方法，解决了模块化Transformer训练中的优化困难和参数数量庞大等挑战。

    

    模块化神经结构因其强大的泛化能力和对新领域的高效适应能力而越来越受到关注。然而，训练模块化模型，特别是在早期阶段，由于固有的稀疏连接导致的优化困难，存在挑战。利用来自整体模型的知识，如知识蒸馏等技术，可能有助于训练模块化模型，并使它们能够整合来自在多个来源上预训练的模型的知识。然而，传统的知识蒸馏方法并不针对模块化模型设计，直接应用时可能失败，这是由于独特的架构和大量涉及的参数。受到这些挑战的启发，我们提出了一种用于在模块之间传递知识的通用模块到模块知识蒸馏（m2mKD）方法。

    arXiv:2402.16918v1 Announce Type: new  Abstract: Modular neural architectures are gaining increasing attention due to their powerful capability for generalization and sample-efficient adaptation to new domains. However, training modular models, particularly in the early stages, poses challenges due to the optimization difficulties arising from their intrinsic sparse connectivity. Leveraging the knowledge from monolithic models, using techniques such as knowledge distillation, is likely to facilitate the training of modular models and enable them to integrate knowledge from multiple models pretrained on diverse sources. Nevertheless, conventional knowledge distillation approaches are not tailored to modular models and can fail when directly applied due to the unique architectures and the enormous number of parameters involved. Motivated by these challenges, we propose a general module-to-module knowledge distillation (m2mKD) method for transferring knowledge between modules. Our approac
    
[^142]: 超越路由：联合GPS和路由建模以优化轨迹表示学习

    More Than Routing: Joint GPS and Route Modeling for Refine Trajectory Representation Learning

    [https://arxiv.org/abs/2402.16915](https://arxiv.org/abs/2402.16915)

    提出了一种新颖的联合GPS和路由建模的表示学习框架，通过自监督技术实现，利用两个编码器分别捕获路由和GPS轨迹的表示，并通过共享的变压器进行模态间信息交互。

    

    轨迹表示学习在支持各种下游任务中发挥着关键作用。传统方法为了过滤GPS轨迹中的噪声往往侧重于基于路由的方法用于简化轨迹。然而，这种方法忽略了GPS数据中包含的运动细节，限制了轨迹表示学习的表示能力。为了填补这一空白，我们提出了一种新颖的基于自监督技术的联合GPS和路由建模的表示学习框架，即JGRM。我们将GPS轨迹和路由视为单个移动观察的两种模式，并通过模态间信息交互来融合信息。具体来说，我们开发了两个编码器，分别用于捕获路由和GPS轨迹的表示。来自这两种模态的表示被送入一个共享的变压器进行模态间信息交互。

    arXiv:2402.16915v1 Announce Type: cross  Abstract: Trajectory representation learning plays a pivotal role in supporting various downstream tasks. Traditional methods in order to filter the noise in GPS trajectories tend to focus on routing-based methods used to simplify the trajectories. However, this approach ignores the motion details contained in the GPS data, limiting the representation capability of trajectory representation learning. To fill this gap, we propose a novel representation learning framework that Joint GPS and Route Modelling based on self-supervised technology, namely JGRM. We consider GPS trajectory and route as the two modes of a single movement observation and fuse information through inter-modal information interaction. Specifically, we develop two encoders, each tailored to capture representations of route and GPS trajectories respectively. The representations from the two modalities are fed into a shared transformer for inter-modal information interaction. Eve
    
[^143]: PDETime：从偏微分方程的视角重新思考长期多元时间序列预测

    PDETime: Rethinking Long-Term Multivariate Time Series Forecasting from the perspective of partial differential equations

    [https://arxiv.org/abs/2402.16913](https://arxiv.org/abs/2402.16913)

    本文提出了PDETime，一种受神经PDE解算器原则启发的新型LMTF模型，从偏微分方程的视角重新思考了长期多元时间序列预测。

    

    最近深度学习的发展导致了各种模型的出现，用于长期多变量时间序列预测（LMTF），其中许多模型表现出色。一般来说，关注点集中在基于历史价值的模型上，这些模型依赖于过去的观测结果来预测未来的序列。值得注意的是，新的趋势已经出现，即基于时间索引的模型，提供了对时间序列潜在连续动态的更细致理解。与聚合空间域或时间域信息的这两种模型不同，在本文中，我们将多元时间序列视为从连续动力系统定期抽样的时空数据，可通过偏微分方程（PDEs）表示，其中空间域为固定。基于这一视角，我们提出PDETime，这是一种受神经PDE解算器原则启发的新型LMTF模型，遵循编码-集成-

    arXiv:2402.16913v1 Announce Type: new  Abstract: Recent advancements in deep learning have led to the development of various models for long-term multivariate time-series forecasting (LMTF), many of which have shown promising results. Generally, the focus has been on historical-value-based models, which rely on past observations to predict future series. Notably, a new trend has emerged with time-index-based models, offering a more nuanced understanding of the continuous dynamics underlying time series. Unlike these two types of models that aggregate the information of spatial domains or temporal domains, in this paper, we consider multivariate time series as spatiotemporal data regularly sampled from a continuous dynamical system, which can be represented by partial differential equations (PDEs), with the spatial domain being fixed. Building on this perspective, we present PDETime, a novel LMTF model inspired by the principles of Neural PDE solvers, following the encoding-integration-
    
[^144]: 企业网络入侵检测的对抗鲁棒性基准

    An Adversarial Robustness Benchmark for Enterprise Network Intrusion Detection

    [https://arxiv.org/abs/2402.16912](https://arxiv.org/abs/2402.16912)

    本文提出了一种对抗鲁棒性基准，评估了多个决策树集成模型在企业网络入侵检测中的表现，发现新的数据集NewCICIDS可以提高模型性能，但对于最新的网络攻击，RF和LGBM的鲁棒性较差。

    

    随着网络攻击变得越来越复杂，提高机器学习（ML）模型的鲁棒性对于各个规模的企业都必须是一个优先考虑的事项。为了可靠地比较不同ML模型在企业计算机网络中用于检测网络攻击的鲁棒性，它们必须在标准化条件下进行评估。本文提出了一种有条理的对抗鲁棒性基准，利用标准数据集生成有限制的对抗示例，评估了多个决策树集成（RF、XGB、LGBM和EBM）的鲁棒性。这些模型在原始的CICIDS2017数据集、经过校正的NewCICIDS版本以及包含了更多最新网络流量的HIKARI数据集上进行了评估。NewCICIDS导致了性能更好的模型，特别是XGB和EBM，而RF和LGBM对HIKARI的最新网络攻击的鲁棒性较差。总体而言，模型对对抗性

    arXiv:2402.16912v1 Announce Type: cross  Abstract: As cyber-attacks become more sophisticated, improving the robustness of Machine Learning (ML) models must be a priority for enterprises of all sizes. To reliably compare the robustness of different ML models for cyber-attack detection in enterprise computer networks, they must be evaluated in standardized conditions. This work presents a methodical adversarial robustness benchmark of multiple decision tree ensembles with constrained adversarial examples generated from standard datasets. The robustness of regularly and adversarially trained RF, XGB, LGBM, and EBM models was evaluated on the original CICIDS2017 dataset, a corrected version of it designated as NewCICIDS, and the HIKARI dataset, which contains more recent network traffic. NewCICIDS led to models with a better performance, especially XGB and EBM, but RF and LGBM were less robust against the more recent cyber-attacks of HIKARI. Overall, the robustness of the models to advers
    
[^145]: 基于后验微调的可信个性化贝叶斯联邦学习

    Trustworthy Personalized Bayesian Federated Learning via Posterior Fine-Tune

    [https://arxiv.org/abs/2402.16911](https://arxiv.org/abs/2402.16911)

    本研究针对联邦学习面临的数据异质性和低输出可解释性挑战，提出了一个新颖的个性化联邦学习框架，融合了贝叶斯方法和正则化流，在参数后验角度实现个性化，提高了算法对不确定性的量化能力，并在理论上分析了对贝叶斯神经网络的超分布检测影响。

    

    数据异质性和低输出可解释性导致的性能下降是联邦学习在实际应用中面临的最大挑战。个性化联邦学习不同于传统方法，不再试图训练单个模型，而是为每个客户定制独特的个性化模型。我们建立了一个新颖的框架，将贝叶斯方法融入个性化联邦学习中，增强了算法量化不确定性的能力。此外，我们引入了正则化流来实现从参数后验角度的个性化，并在理论上分析了正则化流对贝叶斯神经网络的超分布检测的影响。最后，我们评估了我们的方法。

    arXiv:2402.16911v1 Announce Type: new  Abstract: Performance degradation owing to data heterogeneity and low output interpretability are the most significant challenges faced by federated learning in practical applications. Personalized federated learning diverges from traditional approaches, as it no longer seeks to train a single model, but instead tailors a unique personalized model for each client. However, previous work focused only on personalization from the perspective of neural network parameters and lack of robustness and interpretability. In this work, we establish a novel framework for personalized federated learning, incorporating Bayesian methodology which enhances the algorithm's ability to quantify uncertainty. Furthermore, we introduce normalizing flow to achieve personalization from the parameter posterior perspective and theoretically analyze the impact of normalizing flow on out-of-distribution (OOD) detection for Bayesian neural networks. Finally, we evaluated our 
    
[^146]: 怀孕期间体育活动对生活质量的影响：因果机器学习方法

    Impact of Physical Activity on Quality of Life During Pregnancy: A Causal ML Approach

    [https://arxiv.org/abs/2402.16909](https://arxiv.org/abs/2402.16909)

    体育活动对怀孕期间生活质量的影响是一个重要的研究领域，现有研究方法存在偏差，本研究提出了使用因果机器学习方法来考察此关系。

    

    生活质量（QoL）的概念涉及对个体福祉的整体评估，包括心理和社会方面。怀孕的妇女，特别是那些肥胖和压力较大的妇女，常常体验到较低的生活质量。体育活动（PA）显示出增强生活质量的潜力。然而，超重和肥胖的孕妇很少达到推荐的体育活动水平。研究已经使用基于相关性的方法调查了怀孕期间体育活动与生活质量之间的关系。这些方法旨在发现变量之间的偶然相关性，而不是因果关系。此外，现有方法主要依赖于体育活动参数，忽略了使用不同因素（如母亲的医学史）和上下文数据，导致估计有偏差。此外，估计缺乏对可能影响它们的中介变量和反事实情景的理解。在本文中，我们进行了调查...

    arXiv:2402.16909v1 Announce Type: new  Abstract: The concept of Quality of Life (QoL) refers to a holistic measurement of an individual's well-being, incorporating psychological and social aspects. Pregnant women, especially those with obesity and stress, often experience lower QoL. Physical activity (PA) has shown the potential to enhance the QoL. However, pregnant women who are overweight and obese rarely meet the recommended level of PA. Studies have investigated the relationship between PA and QoL during pregnancy using correlation-based approaches. These methods aim to discover spurious correlations between variables rather than causal relationships. Besides, the existing methods mainly rely on physical activity parameters and neglect the use of different factors such as maternal (medical) history and context data, leading to biased estimates. Furthermore, the estimations lack an understanding of mediators and counterfactual scenarios that might affect them. In this paper, we inve
    
[^147]: 利用忆阻器启用的随机逻辑进行本地随机计算

    Local stochastic computing using memristor-enabled stochastic logics

    [https://arxiv.org/abs/2402.16908](https://arxiv.org/abs/2402.16908)

    利用忆阻器开发随机逻辑，实现了具有良好调节概率和相关性的随机数字编码和处理，并设计了一种紧凑的随机Roberts交叉算子用于边缘检测。

    

    随机计算提供了一种概率方法来解决各个领域中由于不确定性和噪声带来的挑战，特别是在机器学习领域。然而，实现随机计算面临着发展可靠的随机逻辑的限制。在这里，我们提出了使用忆阻器开发随机逻辑。具体来说，我们将忆阻器集成到逻辑电路中设计随机逻辑，其中忆阻器切换中固有的随机性被利用来实现具有良好调节概率和相关性的随机数字编码和处理。作为随机逻辑的实际应用，我们设计了一个用于边缘检测的紧凑型随机Roberts交叉算子。值得注意的是，该算子展示出出色的轮廓和纹理提取能力，即使存在50%的噪音，由于其概率性质和紧凑的设计，该算子能够节省95%的能量。

    arXiv:2402.16908v1 Announce Type: cross  Abstract: Stochastic computing offers a probabilistic approach to address challenges posed by problems with uncertainty and noise in various fields, particularly machine learning. The realization of stochastic computing, however, faces the limitation of developing reliable stochastic logics. Here, we present stochastic logics development using memristors. Specifically, we integrate memristors into logic circuits to design the stochastic logics, wherein the inherent stochasticity in memristor switching is harnessed to enable stochastic number encoding and processing with well-regulated probabilities and correlations. As a practical application of the stochastic logics, we design a compact stochastic Roberts cross operator for edge detection. Remarkably, the operator demonstrates exceptional contour and texture extractions, even in the presence of 50% noise, and owning to the probabilistic nature and compact design, the operator can consume 95% le
    
[^148]: 用于图像恢复的扩散后验近似采样

    Diffusion Posterior Proximal Sampling for Image Restoration

    [https://arxiv.org/abs/2402.16907](https://arxiv.org/abs/2402.16907)

    本文提出了一种改进的基于扩散的图像恢复范式，通过选择与测量标识一致的样本，以及从与测量信号相结合的初始化开始恢复过程，实现输出稳定性和增强。

    

    扩散模型在生成高质量样本方面表现出卓越的功效。现有基于扩散的图像恢复算法利用预先训练的扩散模型来利用数据先验，但仍保留了继承自无条件生成范式的元素。本文介绍了一种改进的基于扩散的图像恢复范式。具体来说，我们选择在每个生成步骤中与测量标识一致的样本，利用采样选择作为输出稳定性和增强的途径。此外，我们从一个与测量信号相结合的初始化开始恢复过程，提供了附加信息以更好地对齐生成过程。

    arXiv:2402.16907v1 Announce Type: cross  Abstract: Diffusion models have demonstrated remarkable efficacy in generating high-quality samples. Existing diffusion-based image restoration algorithms exploit pre-trained diffusion models to leverage data priors, yet they still preserve elements inherited from the unconditional generation paradigm. These strategies initiate the denoising process with pure white noise and incorporate random noise at each generative step, leading to over-smoothed results. In this paper, we introduce a refined paradigm for diffusion-based image restoration. Specifically, we opt for a sample consistent with the measurement identity at each generative step, exploiting the sampling selection as an avenue for output stability and enhancement. Besides, we start the restoration process with an initialization combined with the measurement signal, providing supplementary information to better align the generative process. Extensive experimental results and analyses val
    
[^149]: 利用反应合成对生成式代理行为施加时间约束

    Enforcing Temporal Constraints on Generative Agent Behavior with Reactive Synthesis

    [https://arxiv.org/abs/2402.16905](https://arxiv.org/abs/2402.16905)

    提出了一种利用形式逻辑为基础的程序合成和LLM内容生成相结合的方法，通过使用时间流逻辑（TSL）对生成式代理施加时间约束，从而提高了代理行为的保证水平、系统的解释性和代理的模块化构建能力。

    

    大型语言模型（LLM）的流行引发了对创建交互代理新方法的探索。然而，在互动过程中管理这些代理的时间行为仍然具有挑战性。我们提出了一种将形式逻辑为基础的程序合成与LLM内容生成相结合的方法，以创建遵守时间约束的生成式代理。我们的方法使用时间流逻辑（Temporal Stream Logic，TSL）生成一个自动机，对代理施加时间结构，并将每个动作的细节留给LLM。通过使用TSL，我们能够增强生成代理，使用户在行为上有更高的保证水平，系统更易解释，并且更能以模块化方式构建代理。我们评估了我们的方法……

    arXiv:2402.16905v1 Announce Type: new  Abstract: The surge in popularity of Large Language Models (LLMs) has opened doors for new approaches to the creation of interactive agents. However, managing the temporal behavior of such agents over the course of an interaction remains challenging. The stateful, long-term horizon and quantitative reasoning required for coherent agent behavior does not fit well into the LLM paradigm. We propose a combination of formal logic-based program synthesis and LLM content generation to create generative agents that adhere to temporal constraints. Our approach uses Temporal Stream Logic (TSL) to generate an automaton that enforces a temporal structure on an agent and leaves the details of each action for a moment in time to an LLM. By using TSL, we are able to augment the generative agent where users have a higher level of guarantees on behavior, better interpretability of the system, and more ability to build agents in a modular way. We evaluate our appro
    
[^150]: 面向最大推理准确性和能效实时物联网传感系统的选择性任务卸载

    Selective Task offloading for Maximum Inference Accuracy and Energy efficient Real-Time IoT Sensing Systems

    [https://arxiv.org/abs/2402.16904](https://arxiv.org/abs/2402.16904)

    本论文提出了一个轻量级混合遗传算法（LGSTO），用于解决面向实时物联网传感系统的选择性任务卸载问题，以在时间和能量约束下最大化推理准确性。

    

    小型推理模型的最新进展促进了人工智能在边缘部署。然而，边缘设备有限的资源特性，特别是对于实时应用程序，带来了新的挑战。部署多个推理模型（或者一个尺寸可调的模型）变化大小，因此准确性和功耗，以及边缘服务器推理模型，可以提供一个动态系统，在其中根据当前资源条件执行对推理模型到推理任务的分配。因此，在这项工作中，我们解决了将推理模型选择性分配给任务或将其卸载到边缘服务器以在时间和能量约束下最大化推理准确性的问题。该问题被证明是无界多维背包问题的一个实例，被认为是一个强 NP-难问题。我们提出了一个轻量级混合遗传算法（LGSTO）来解决此问题。

    arXiv:2402.16904v1 Announce Type: cross  Abstract: The recent advancements in small-size inference models facilitated AI deployment on the edge. However, the limited resource nature of edge devices poses new challenges especially for real-time applications. Deploying multiple inference models (or a single tunable model) varying in size and therefore accuracy and power consumption, in addition to an edge server inference model, can offer a dynamic system in which the allocation of inference models to inference jobs is performed according to the current resource conditions. Therefore, in this work, we tackle the problem of selectively allocating inference models to jobs or offloading them to the edge server to maximize inference accuracy under time and energy constraints. This problem is shown to be an instance of the unbounded multidimensional knapsack problem which is considered a strongly NP-hard problem. We propose a lightweight hybrid genetic algorithm (LGSTO) to solve this problem.
    
[^151]: 一种新的数据生成方案用于深度操作网络的代理建模

    A novel data generation scheme for surrogate modelling with deep operator networks

    [https://arxiv.org/abs/2402.16903](https://arxiv.org/abs/2402.16903)

    提出了一种新的数据生成方法，用于减轻深度操作网络（DeepONets）训练数据生成的计算负担，避免了使用偏微分方程积分策略，显著降低了生成训练数据集的计算成本

    

    arXiv:2402.16903v1 公告类型：新 基于操作符的神经网络架构，如DeepONets已成为物理系统代理建模的一种有希望的工具。一般来说，为了进行操作符代理建模，训练数据是通过使用有限元法等技术解决PDEs生成的。数据生成的计算密集型特性是部署这些代理模型到实际应用中的最大瓶颈之一。在这项研究中，我们提出了一种新的方法来减轻DeepONets训练数据生成所带来的计算负担。与现有文献不同，所提出的数据生成框架不使用任何偏微分方程积分策略，从而显著减少了用于DeepONet生成训练数据集的计算成本。在所提出的策略中，首先随机生成输出字段，满足边界条件。

    arXiv:2402.16903v1 Announce Type: new  Abstract: Operator-based neural network architectures such as DeepONets have emerged as a promising tool for the surrogate modeling of physical systems. In general, towards operator surrogate modeling, the training data is generated by solving the PDEs using techniques such as Finite Element Method (FEM). The computationally intensive nature of data generation is one of the biggest bottleneck in deploying these surrogate models for practical applications. In this study, we propose a novel methodology to alleviate the computational burden associated with training data generation for DeepONets. Unlike existing literature, the proposed framework for data generation does not use any partial differential equation integration strategy, thereby significantly reducing the computational cost associated with generating training dataset for DeepONet. In the proposed strategy, first, the output field is generated randomly, satisfying the boundary conditions u
    
[^152]: PRoLoRA: 部分旋转增强更高效LoRA

    PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA

    [https://arxiv.org/abs/2402.16902](https://arxiv.org/abs/2402.16902)

    PRoLoRA是一个新的部分旋转增强低秩适应方法，通过引入广播减少、旋转增强、部分共享细化和修正初始化策略等四个组件，实现了对LoRA的优势提升，避免了其他参数共享方法的缺点，具有更高的参数效率和可扩展性。

    

    随着大型语言模型（LLMs）的快速扩展，同时服务多个LoRAs变得日益不切实际，导致成本不可承受，需要更具参数效率的微调方法。在这项工作中，我们引入了部分旋转增强低秩适应（PRoLoRA），这是一种由四个关键组件组成的层内共享机制：广播减少、旋转增强、部分共享细化和修正的初始化策略。作为LoRA的超集，PRoLoRA保留了其优势，并通过具有卓越的模型容量、实用可行性和广泛适用性，有效地避开了其他参数共享方法的缺点。实证实验表明，PRoLoRA在特定参数预算和性能目标情景下，具有显着更高的参数效率，并且可以扩展到更大的LLMs。值得注意的是，PRoLoRA在可训练参数减少一倍的情况下，

    arXiv:2402.16902v1 Announce Type: new  Abstract: With the rapid scaling of large language models (LLMs), serving numerous LoRAs concurrently has become increasingly impractical, leading to unaffordable costs and necessitating more parameter-efficient finetuning methods. In this work, we introduce Partially Rotation-enhanced Low-Rank Adaptation (PRoLoRA), an intra-layer sharing mechanism comprising four essential components: broadcast reduction, rotation enhancement, partially-sharing refinement, and rectified initialization strategy. As a superset of LoRA, PRoLoRA pertains its advantages, and effectively circumvent the drawbacks of peer parameter-sharing methods with superior model capacity, practical feasibility, and broad applicability. Empirical experiments demonstrate the remarkably higher parameter efficiency of PRoLoRA in both specific parameter budget and performance target scenarios, and its scalability to larger LLMs. Notably, with one time less trainable parameters, PRoLoRA s
    
[^153]: FGBERT：基于功能驱动的宏基因组预训练基因语言模型

    FGBERT: Function-Driven Pre-trained Gene Language Model for Metagenomics

    [https://arxiv.org/abs/2402.16901](https://arxiv.org/abs/2402.16901)

    该论文提出了基于蛋白质的基因表示作为一种上下文感知和结构相关的标记器，通过Masked Gene Modeling（MGM）和Triple Enhanced Metagenomic Contrastive Learning（TEM-CL）进行预训练，构建了一个新颖的宏基因组语言模型FGBERT，能够更好地捕捉基因序列与功能之间的复杂关系。

    

    Metagenomic data, comprising mixed multi-species genomes, are prevalent in diverse environments like oceans and soils, significantly impacting human health and ecological functions. However, current research relies on K-mer representations, limiting the capture of structurally relevant gene contexts. To address these limitations and further our understanding of complex relationships between metagenomic sequences and their functions, we introduce a protein-based gene representation as a context-aware and structure-relevant tokenizer. Our approach includes Masked Gene Modeling (MGM) for gene group-level pre-training, providing insights into inter-gene contextual information, and Triple Enhanced Metagenomic Contrastive Learning (TEM-CL) for gene-level pre-training to model gene sequence-function relationships. MGM and TEM-CL constitute our novel metagenomic language model FGBERT, pre-trained on 100 million metagenomic sequences.

    arXiv:2402.16901v1 Announce Type: cross  Abstract: Metagenomic data, comprising mixed multi-species genomes, are prevalent in diverse environments like oceans and soils, significantly impacting human health and ecological functions. However, current research relies on K-mer representations, limiting the capture of structurally relevant gene contexts. To address these limitations and further our understanding of complex relationships between metagenomic sequences and their functions, we introduce a protein-based gene representation as a context-aware and structure-relevant tokenizer. Our approach includes Masked Gene Modeling (MGM) for gene group-level pre-training, providing insights into inter-gene contextual information, and Triple Enhanced Metagenomic Contrastive Learning (TEM-CL) for gene-level pre-training to model gene sequence-function relationships. MGM and TEM-CL constitute our novel metagenomic language model {\NAME}, pre-trained on 100 million metagenomic sequences. We demon
    
[^154]: 连续时间强化学习中深度残差网络的\emph{先验估计}

    A prior Estimates for Deep Residual Network in Continuous-time Reinforcement Learning

    [https://arxiv.org/abs/2402.16899](https://arxiv.org/abs/2402.16899)

    本研究针对连续时间控制问题，提出了一种可以直接分析Bellman最优损失\emph{先验}泛化误差的方法，避免了有界性假设，并通过最大算子的分解方法实现了损失函数的转换。

    

    深度强化学习在许多大规模实际应用中表现出色。然而，现有的性能分析忽略了连续时间控制问题的独特特征，无法直接估计Bellman最优损失的泛化误差，并且需要一个有界性假设。我们的工作侧重于连续时间控制问题，并提出了一种适用于所有满足半群和Lipschitz性质的问题的方法。在该方法下，我们能够直接分析Bellman最优损失的\emph{先验}泛化误差。该方法的核心在于损失函数的两次转换。为了完成转换，我们提出了最大算子的分解方法。此外，这个分析方法不需要有界性假设。最终我们维得到了一个没有“维度诅咒”的\emph{先验}泛化误差。

    arXiv:2402.16899v1 Announce Type: cross  Abstract: Deep reinforcement learning excels in numerous large-scale practical applications. However, existing performance analyses ignores the unique characteristics of continuous-time control problems, is unable to directly estimate the generalization error of the Bellman optimal loss and require a boundedness assumption. Our work focuses on continuous-time control problems and proposes a method that is applicable to all such problems where the transition function satisfies semi-group and Lipschitz properties. Under this method, we can directly analyze the \emph{a priori} generalization error of the Bellman optimal loss. The core of this method lies in two transformations of the loss function. To complete the transformation, we propose a decomposition method for the maximum operator. Additionally, this analysis method does not require a boundedness assumption. Finally, we obtain an \emph{a priori} generalization error without the curse of dime
    
[^155]: MIM-Reasoner: 具有理论保证的多重影响最大化学习

    MIM-Reasoner: Learning with Theoretical Guarantees for Multiplex Influence Maximization

    [https://arxiv.org/abs/2402.16898](https://arxiv.org/abs/2402.16898)

    引入了MIM-Reasoner，结合强化学习和概率图模型，有效地捕捉了给定多重网络内部和层间的复杂传播过程，从而解决了MIM中最具挑战性的问题。

    

    多重影响最大化（MIM）要求我们识别一组种子用户，以最大化多重网络中受影响用户的预期数量。本文介绍了MIM-Reasoner，将强化学习与概率图模型相结合，有效捕捉给定多重网络内部和层间的复杂传播过程，从而解决了MIM中最具挑战性的问题。

    arXiv:2402.16898v1 Announce Type: cross  Abstract: Multiplex influence maximization (MIM) asks us to identify a set of seed users such as to maximize the expected number of influenced users in a multiplex network. MIM has been one of central research topics, especially in nowadays social networking landscape where users participate in multiple online social networks (OSNs) and their influences can propagate among several OSNs simultaneously. Although there exist a couple combinatorial algorithms to MIM, learning-based solutions have been desired due to its generalization ability to heterogeneous networks and their diversified propagation characteristics. In this paper, we introduce MIM-Reasoner, coupling reinforcement learning with probabilistic graphical model, which effectively captures the complex propagation process within and between layers of a given multiplex network, thereby tackling the most challenging problem in MIM. We establish a theoretical guarantee for MIM-Reasoner as w
    
[^156]: 可靠的冲突多视角学习

    Reliable Conflictive Multi-View Learning

    [https://arxiv.org/abs/2402.16897](https://arxiv.org/abs/2402.16897)

    提出了可靠的冲突多视角学习（RCML）问题，开发了一种Evidential Conflictive Multi-view Learning (ECML)方法来处理具有冲突信息的多视角数据。

    

    多视角学习旨在结合多个特征，以实现对数据的更全面描述。之前的大部分工作都假设多个视图是严格对齐的。然而，现实世界中的多视角数据可能包含低质量的冲突实例，即在不同视图中显示冲突信息。为了解决这个问题，我们提出了一个新的可靠的冲突多视角学习（RCML）问题，要求模型为冲突的多视角数据提供决策结果和附加的可靠性。我们为这个问题开发了一种Evidential Conflictive Multi-view Learning (ECML)方法。

    arXiv:2402.16897v1 Announce Type: cross  Abstract: Multi-view learning aims to combine multiple features to achieve more comprehensive descriptions of data. Most previous works assume that multiple views are strictly aligned. However, real-world multi-view data may contain low-quality conflictive instances, which show conflictive information in different views. Previous methods for this problem mainly focus on eliminating the conflictive data instances by removing them or replacing conflictive views. Nevertheless, real-world applications usually require making decisions for conflictive instances rather than only eliminating them. To solve this, we point out a new Reliable Conflictive Multi-view Learning (RCML) problem, which requires the model to provide decision results and attached reliabilities for conflictive multi-view data. We develop an Evidential Conflictive Multi-view Learning (ECML) method for this problem. ECML first learns view-specific evidence, which could be termed as th
    
[^157]: 关于代码大语言模型中的木马签名

    On Trojan Signatures in Large Language Models of Code

    [https://arxiv.org/abs/2402.16896](https://arxiv.org/abs/2402.16896)

    本文研究了在大型代码语言模型中木马签名的问题，并发现木马签名无法推广到代码语言模型，具有重要的研究意义。

    

    木马签名是由Fields等人(2021)描述的，是被感染类别参数（权重）与未被感染类别参数之间分布的显著差异，可以用于检测被感染的模型。Fields等人(2021)发现了计算机视觉分类任务中的木马签名，比如Resnet、WideResnet、Densenet和VGG。本文研究了源代码大语言模型中分类层参数的这种签名。我们的结果表明，木马签名不能泛化到代码的大语言模型。我们发现，即使在更明确的设置下对模型进行了中毒（用预训练的权重冻结进行微调），被感染的代码模型也仍然固执。我们对两个二进制分类任务进行了九个感染模型的分析：克隆和缺陷检测。据我们所知，这是第一个检查基于权重的木马的工作。

    arXiv:2402.16896v1 Announce Type: cross  Abstract: Trojan signatures, as described by Fields et al. (2021), are noticeable differences in the distribution of the trojaned class parameters (weights) and the non-trojaned class parameters of the trojaned model, that can be used to detect the trojaned model. Fields et al. (2021) found trojan signatures in computer vision classification tasks with image models, such as, Resnet, WideResnet, Densenet, and VGG. In this paper, we investigate such signatures in the classifier layer parameters of large language models of source code.   Our results suggest that trojan signatures could not generalize to LLMs of code. We found that trojaned code models are stubborn, even when the models were poisoned under more explicit settings (finetuned with pre-trained weights frozen). We analyzed nine trojaned models for two binary classification tasks: clone and defect detection. To the best of our knowledge, this is the first work to examine weight-based troj
    
[^158]: 多任务学习用于具有跨问题零样本泛化的路径问题

    Multi-Task Learning for Routing Problem with Cross-Problem Zero-Shot Generalization

    [https://arxiv.org/abs/2402.16891](https://arxiv.org/abs/2402.16891)

    本研究首次尝试解决跨问题泛化的关键挑战，通过将VRPs定义为共享基础属性的不同组合，并通过属性组合同时解决它们，实现了零样本泛化的路径问题解决方法。

    

    车辆路径问题（VRPs）在许多实际应用中都能找到，已经成为几十年的重要研究课题。最近，利用基于学习的模型来解决VRPs的神经组合优化（NCO）方法引起了相当大的关注。然而，当前的NCO方法通常需要为每个路径问题构建一个模型，这显著阻碍了它们在具有不同属性的真实工业问题中的实际应用。在这项工作中，我们首次尝试解决跨问题泛化的关键挑战。具体而言，我们将VRPs定义为一组共享的基础属性的不同组合，并通过属性组合同时通过单一模型解决它们。通过这种方式，我们提出的模型能够成功解决具有未见属性组合的VRPs，实现零样本泛化。

    arXiv:2402.16891v1 Announce Type: cross  Abstract: Vehicle routing problems (VRPs), which can be found in numerous real-world applications, have been an important research topic for several decades. Recently, the neural combinatorial optimization (NCO) approach that leverages a learning-based model to solve VRPs without manual algorithm design has gained substantial attention. However, current NCO methods typically require building one model for each routing problem, which significantly hinders their practical application for real-world industry problems with diverse attributes. In this work, we make the first attempt to tackle the crucial challenge of cross-problem generalization. In particular, we formulate VRPs as different combinations of a set of shared underlying attributes and solve them simultaneously via a single model through attribute composition. In this way, our proposed model can successfully solve VRPs with unseen attribute combinations in a zero-shot generalization mann
    
[^159]: 生成模型具有自身水印：通过再生成声明模型认证

    Generative Models are Self-Watermarked: Declaring Model Authentication through Re-Generation

    [https://arxiv.org/abs/2402.16889](https://arxiv.org/abs/2402.16889)

    该论文提出了一种通过再生成识别模型数据所有权的方法，避免了传统数字水印技术可能破坏输出质量的问题。

    

    随着机器和人工智能生成的内容不断增加，保护生成模型的知识产权已变得至关重要，然而验证数据所有权在未经授权重复使用生成数据的情况下面临着巨大挑战。我们的工作致力于检测即使是个别样本的数据重复使用。传统上，数字水印技术被利用来检测人工智能生成的内容。然而，与将附加信息嵌入模型或生成内容以作为触发器的水印技术不同，潜在损害输出质量，我们的方法通过重新生成识别在输出中固有存在的潜在指纹。我们提出了一个可解释的验证过程，通过再生成来归属数据所有权，进一步

    arXiv:2402.16889v1 Announce Type: cross  Abstract: As machine- and AI-generated content proliferates, protecting the intellectual property of generative models has become imperative, yet verifying data ownership poses formidable challenges, particularly in cases of unauthorized reuse of generated data. The challenge of verifying data ownership is further amplified by using Machine Learning as a Service (MLaaS), which often functions as a black-box system.   Our work is dedicated to detecting data reuse from even an individual sample. Traditionally, watermarking has been leveraged to detect AI-generated content. However, unlike watermarking techniques that embed additional information as triggers into models or generated content, potentially compromising output quality, our approach identifies latent fingerprints inherently present within the outputs through re-generation. We propose an explainable verification procedure that attributes data ownership through re-generation, and further 
    
[^160]: 利用小水库重建混沌吸引子-拓扑结构的影响

    Chaotic attractor reconstruction using small reservoirs - the influence of topology

    [https://arxiv.org/abs/2402.16888](https://arxiv.org/abs/2402.16888)

    重建混沌吸引子时，非耦合节点的水库比复杂水库更可靠产生长期时间序列预测，且较小的谱半径有助于改进替代系统的吸引子重建性能。

    

    需要基于测量数据对时间序列进行预测，在广泛的应用中是必需的，并且已经成为广泛研究的课题。其中一个特别具有挑战性的任务是对混沌动力学生成的时间序列进行预测。近年来，储层计算已被证明是一种有效的方法，可以从数据中预测混沌动力学并重建混沌吸引子。在这项工作中，我们朝着更小、更低复杂度的水库迈出了步伐，目标是提高硬件实现性能，并更可靠地生产出足够满意的替代模型。我们表明，一个非耦合节点的水库比复杂水库拓扑更可靠地产生长期时间序列预测。然后，我们将非耦合水库改进的吸引子重建与所得替代系统的较小谱半径联系起来。这些结果表明，节点度在确定水库的长期时间序列预测性能方面起着重要作用。

    arXiv:2402.16888v1 Announce Type: new  Abstract: Forecasting timeseries based upon measured data is needed in a wide range of applications and has been the subject of extensive research. A particularly challenging task is the forecasting of timeseries generated by chaotic dynamics. In recent years reservoir computing has been shown to be an effective method of forecasting chaotic dynamics and reconstructing chaotic attractors from data. In this work strides are made toward smaller and lower complexity reservoirs with the goal of improved hardware implementability and more reliable production of adequate surrogate models. We show that a reservoir of uncoupled nodes more reliably produces long term timeseries predictions than complex reservoir topologies. We then link the improved attractor reconstruction of the uncoupled reservoir with smaller spectral radii of the resulting surrogate systems. These results indicate that, the node degree plays an important role in determining whether th
    
[^161]: 复杂网络的人工智能：潜力、方法论和应用

    Artificial Intelligence for Complex Network: Potential, Methodology and Application

    [https://arxiv.org/abs/2402.16887](https://arxiv.org/abs/2402.16887)

    人工智能技术与丰富真实网络数据的存在开启了复杂网络科学研究的新时代，有望克服现存挑战。

    

    复杂网络存在于各种真实世界系统中，从自然环境到人类社会。这些网络的本质在于它们能够从微观混乱-其中网络拓扑和节点动态交织-转变和演化为具有特定集体行为的宏观秩序。在过去的二十年里，复杂网络科学显著增强了我们对真实世界网络潜在机制、结构和动态的理解。尽管取得了这些进展，但在探索更加真实系统和提升实际应用方面仍然存在着相当大的挑战。人工智能技术的出现，以及丰富多样的真实世界网络数据的存在，开启了复杂网络科学研究的新时代。本调查旨在系统地探讨人工智能在克服复杂网络科学研究所面临的挑战方面的潜在优势。

    arXiv:2402.16887v1 Announce Type: cross  Abstract: Complex networks pervade various real-world systems, from the natural environment to human societies. The essence of these networks is in their ability to transition and evolve from microscopic disorder-where network topology and node dynamics intertwine-to a macroscopic order characterized by certain collective behaviors. Over the past two decades, complex network science has significantly enhanced our understanding of the statistical mechanics, structures, and dynamics underlying real-world networks. Despite these advancements, there remain considerable challenges in exploring more realistic systems and enhancing practical applications. The emergence of artificial intelligence (AI) technologies, coupled with the abundance of diverse real-world network data, has heralded a new era in complex network science research. This survey aims to systematically address the potential advantages of AI in overcoming the lingering challenges of com
    
[^162]: 使用文本嵌入模型和向量数据库作为文本分类器的研究，以医疗数据为例

    Using text embedding models and vector databases as text classifiers with the example of medical data

    [https://arxiv.org/abs/2402.16886](https://arxiv.org/abs/2402.16886)

    向量数据库和嵌入模型的应用为文本分类器提供了强大的方式来表达数据模式，特别是在医疗领域中开始有着广泛的应用。

    

    大型语言模型（LLMs）的出现是令人兴奋的，并已在许多领域找到应用，但通常情况下，医学领域的标准要求非常高。与LLMs配合使用，向量嵌入模型和向量数据库提供了一种强大的方式来表达各种数据模式，这些数据模式容易被典型的机器学习模型所理解。除了方便地向这些向量数据库添加信息、知识和数据外，它们还提供了一个令人信服的理由，即将其应用于通常由人类完成的检索信息任务的各种领域。Google的研究人员开发了一个清晰的替代模型Med-PaLM，专门旨在与临床医师的医学知识水平匹配。在训练分类器和开发模型时，保持事实和减少偏见是至关重要的。在本文中，我们探讨了向量数据库和嵌入模型的应用

    arXiv:2402.16886v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) is promising and has found application in numerous fields, but as it often is with the medical field, the bar is typically quite high [5]. In tandem with LLMs, vector embedding models and vector databases provide a robust way of expressing numerous modes of data that are easily digestible by typical machine learning models. Along with the ease of adding information, knowledge, and data to these vector databases, they provide a compelling reason to apply them in numerous fields where the task of retrieving information is typically done by humans. Researchers at Google have developed a clear alternative model, Med-PaLM [6] specifically designed to match a clinician's level of accuracy when it comes to medical knowledge. When training classifiers, and developing models, it is imperative to maintain factuality and reduce bias [4]. Here, we explore the use of vector databases and embedding models a
    
[^163]: 底物范围对比学习：重新利用人类偏见学习原子表示

    Substrate Scope Contrastive Learning: Repurposing Human Bias to Learn Atomic Representations

    [https://arxiv.org/abs/2402.16882](https://arxiv.org/abs/2402.16882)

    提出了一种新颖的预训练策略，底物范围对比学习，以学习适合化学反应性的原子表示。

    

    学习分子表示是分子机器学习中的关键步骤，对建模成功产生显著影响，尤其在数据稀缺情况下。广义预训练神经网络的概念推动了计算机视觉、自然语言处理和蛋白质工程等领域的发展。然而，类似的方法在小有机分子方面并未取得类似的成功。在这项工作中，我们引入一种新颖的预训练策略，即底物范围对比学习，它学习适合化学反应性的原子表示。这种方法以已发表的底物范围表中底物的分组和产物收率作为化学反应性相似性或不相似性的衡量。我们关注 CAS Content Collection 中的 20,798 个芳香卤代烃，涵盖数千篇出版物，以学习芳香卤代烃的反应性表示。我们验证了我们的预训练方法。

    arXiv:2402.16882v1 Announce Type: cross  Abstract: Learning molecular representation is a critical step in molecular machine learning that significantly influences modeling success, particularly in data-scarce situations. The concept of broadly pre-training neural networks has advanced fields such as computer vision, natural language processing, and protein engineering. However, similar approaches for small organic molecules have not achieved comparable success. In this work, we introduce a novel pre-training strategy, substrate scope contrastive learning, which learns atomic representations tailored to chemical reactivity. This method considers the grouping of substrates and their yields in published substrate scope tables as a measure of their similarity or dissimilarity in terms of chemical reactivity. We focus on 20,798 aryl halides in the CAS Content Collection spanning thousands of publications to learn a representation of aryl halide reactivity. We validate our pre-training appr
    
[^164]: BESA: 使用分块参数高效稀疏分配修剪大型语言模型

    BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation

    [https://arxiv.org/abs/2402.16880](https://arxiv.org/abs/2402.16880)

    该论文提出了一种名为BESA的新型大型语言模型修剪技术，通过应用分块重构损失，与传统的逐层修剪技术不同，BESA具有优势

    

    大型语言模型（LLMs）在文本摘要、文本问答等各种任务中表现出色。尽管它们的性能令人印象深刻，但由于大量参数造成的计算占用可能是禁锢的。现有解决方案（如SparseGPT和Wanda）尝试通过权重修剪缓解此问题。然而，它们的逐层方法会导致模型输出显著扰动，并需要细致的超参数调整，如修剪速率，这可能会对整体模型性能产生不利影响。为解决此问题，本文引入了一种新颖的LLM修剪技术，称为分块参数高效稀疏分配（BESA），通过应用分块重构损失。与典型的逐层修剪技术相比，BESA具有两个独特的特点：i）它定位于整体修剪误差相对于每个

    arXiv:2402.16880v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated outstanding performance in various tasks, such as text summarization, text question-answering, and etc. While their performance is impressive, the computational footprint due to their vast number of parameters can be prohibitive. Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance. To address this, this paper introduces a novel LLM pruning technique dubbed blockwise parameter-efficient sparsity allocation (BESA) by applying a blockwise reconstruction loss. In contrast to the typical layer-wise pruning techniques, BESA is characterized by two distinctive attributes: i) it targets the overall pruning error with respect to indi
    
[^165]: EvoGPT-f: 一种用于基准测试形式数学语言的进化GPT框架

    EvoGPT-f: An Evolutionary GPT Framework for Benchmarking Formal Math Languages

    [https://arxiv.org/abs/2402.16878](https://arxiv.org/abs/2402.16878)

    EvoGPT-f是一个新颖的进化框架，用于对五个形式数学语料库进行差异机器可学习性的系统量化分析，为形式数学语言的基准测试提供了新的方法。

    

    arXiv:2402.16878v1 公告类型: 新的 摘要: 形式数学是将数学转化为编程语言的学科，在这种编程语言中，任何陈述都可以被计算机明确地检查。数学家和计算机科学家花费了数十年进行艰苦的形式化工作，开发了诸如Coq、HOL和Lean等语言。机器学习研究已经汇集到这些形式化数学语料库上，并产生了各种方法来帮助交互式和自动定理证明。然而，这些论文主要集中在一个方法、一个证明任务、一个语言上。本文介绍了EvoGPT-f: 一种新颖的进化框架，用于首次系统量化分析五个形式数学语料库(Lean 3、Lean 4、Coq、HOL 4、HOL Light)的差异机器可学习性，使用四种记号化方法(字符、单词级、字节对编码和StarCoder记号化器)。本文并未结束关于“最佳”的问题。

    arXiv:2402.16878v1 Announce Type: new  Abstract: Formal mathematics is the discipline of translating mathematics into a programming language in which any statement can be unequivocally checked by a computer. Mathematicians and computer scientists have spent decades of painstaking formalization efforts developing languages such as Coq, HOL, and Lean. Machine learning research has converged on these formal math corpora and given rise to an assortment of methodologies to aid in interactive and automated theorem proving. However, these papers have primarily focused on one method, for one proof task, in one language. This paper introduces EvoGPT-f: a novel evolutionary framework for the first systematic quantitative analysis of the differential machine learnability of five formal math corpora (Lean 3, Lean 4, Coq, HOL 4, HOL Light) using four tokenization methods (character, word-level, Byte Pair Encoding and StarCoder tokenizer). This paper does not put to rest the question of the "best" o
    
[^166]: 大型语言模型增强的个性化语言学习练习检索

    Large Language Model Augmented Exercise Retrieval for Personalized Language Learning

    [https://arxiv.org/abs/2402.16877](https://arxiv.org/abs/2402.16877)

    大型语言模型利用生成能力来合成假设练习，以弥合学习者需求与练习内容之间的语义鸿沟，提高个性化语言学习练习检索效果。

    

    我们研究了在线语言学习环境中的零样本练习检索问题，以赋予学习者通过自然语言明确请求个性化练习的能力。通过收集自语言学习者的真实数据，我们观察到矢量相似性方法很难捕捉练习内容与学习者用于表达他们想要学习内容的语言之间的关系。我们利用大型语言模型的生成能力来弥合这一差距，通过基于学习者输入合成假设练习，然后用于搜索相关练习。我们的方法mHyER克服了三个挑战：（1）缺乏用于训练的相关性标签，（2）受限的学习

    arXiv:2402.16877v1 Announce Type: cross  Abstract: We study the problem of zero-shot exercise retrieval in the context of online language learning, to give learners the ability to explicitly request personalized exercises via natural language. Using real-world data collected from language learners, we observe that vector similarity approaches poorly capture the relationship between exercise content and the language that learners use to express what they want to learn. This semantic gap between queries and content dramatically reduces the effectiveness of general-purpose retrieval models pretrained on large scale information retrieval datasets like MS MARCO. We leverage the generative capabilities of large language models to bridge the gap by synthesizing hypothetical exercises based on the learner's input, which are then used to search for relevant exercises. Our approach, which we call mHyER, overcomes three challenges: (1) lack of relevance labels for training, (2) unrestricted learn
    
[^167]: 通过将可学习的概率离散潜变量引入机器学习模型来提高眼部疾病检测的鲁棒性

    Improve Robustness of Eye Disease Detection by including Learnable Probabilistic Discrete Latent Variables into Machine Learning Models

    [https://arxiv.org/abs/2402.16865](https://arxiv.org/abs/2402.16865)

    通过引入可学习的概率离散潜变量，该研究提出了一种新颖的眼部疾病检测方法，利用生成流网络来学习眼底图像中眼部疾病的后验分布，提高了鲁棒性和泛化能力。

    

    眼部疾病从糖尿病性视网膜病变到青光眼等，由于其高发病率和可能导致视力损害，构成了一个重要的公共卫生挑战。及早和准确的诊断对于有效治疗和管理至关重要。近年来，深度学习模型已经成为分析医学图像（包括眼部图像）的强大工具。然而，模型的解释性和不确定性估计方面仍然存在挑战，这对临床决策至关重要。本研究引入了GFlowOut的新颖应用，利用生成流网络（GFlowNets）的概率框架来学习关于辍学掩码的后验分布，用于使用眼底图像对眼部疾病进行分类和分析。我们开发了一种稳健且具有普适性的方法，利用以ResNet18和ViT模型为主干的GFlowOut来识别各种眼部状况。

    arXiv:2402.16865v1 Announce Type: cross  Abstract: Ocular diseases, ranging from diabetic retinopathy to glaucoma, present a significant public health challenge due to their prevalence and potential for causing vision impairment. Early and accurate diagnosis is crucial for effective treatment and management.In recent years, deep learning models have emerged as powerful tools for analysing medical images, including ocular imaging . However, challenges persist in model interpretability and uncertainty estimation, which are critical for clinical decision-making. This study introduces a novel application of GFlowOut, leveraging the probabilistic framework of Generative Flow Networks (GFlowNets) to learn the posterior distribution over dropout masks, for the classification and analysis of ocular diseases using eye fundus images. We develop a robust and generalizable method that utilizes GFlowOut integrated with ResNet18 and ViT models as backbone in identifying various ocular conditions. Th
    
[^168]: 基于语义有效性通道错误的实用目标导向通信

    Pragmatic Goal-Oriented Communications under Semantic-Effectiveness Channel Errors

    [https://arxiv.org/abs/2402.16858](https://arxiv.org/abs/2402.16858)

    在AI辅助的6G网络中，实现了语义、实用和目标导向通信策略的整合，通过提出数学模型解决了语言不匹配导致的出错问题

    

    在即将到来的AI辅助的6G网络中，整合语义、实用和目标导向通信策略变得至关重要。这种整合将实现仅涉及相关任务数据的感知、传输和处理，确保传达的信息具有可理解的、实用的语义重要性，与目标和需求相一致。在此背景下，除了典型无线通信动态引起的错误外，由于语义处理能力的限制、发射方意图和接收方解释之间的意义差异以及发射方和接收方之间的语言和知识表征差异，还可能出现发射方意图和接收方解释之间的潜在失真。本文的主要贡献有两个方面。首先，它提出并详细介绍了一种新颖的数学模型，用于描述源于语言不匹配的错误，包括语义和有效性级别的错误。

    arXiv:2402.16858v1 Announce Type: cross  Abstract: In forthcoming AI-assisted 6G networks, integrating semantic, pragmatic, and goal-oriented communication strategies becomes imperative. This integration will enable sensing, transmission, and processing of exclusively pertinent task data, ensuring conveyed information possesses understandable, pragmatic semantic significance, aligning with destination needs and goals. Without doubt, no communication is error free. Within this context, besides errors stemming from typical wireless communication dynamics, potential distortions between transmitter-intended and receiver-interpreted meanings can emerge due to limitations in semantic processing capabilities, as well as language and knowledge representation disparities between transmitters and receivers. The main contribution of this paper is two-fold. First, it proposes and details a novel mathematical modeling of errors stemming from language mismatches at both semantic and effectiveness le
    
[^169]: 基于注意力的分层变分自动编码器生成分子

    Attention Based Molecule Generation via Hierarchical Variational Autoencoder

    [https://arxiv.org/abs/2402.16854](https://arxiv.org/abs/2402.16854)

    通过将递归神经网络与卷积网络分层结合，实现了高效的分子生成模型，能够在重建已知分子时获得95%的有效性，同时能够实现在SMILES字符串和其学习表示之间的映射。

    

    分子生成这一任务在计算上能表示分子的复杂方式中变得十分困难。在分子生成模型中常用的技术是使用带有递归神经网络的SMILES字符串，内置到变分自动编码器中，但这些技术存在许多问题：梯度消失，长距离遗忘和无效分子。在这项工作中，我们展示了通过以分层方式将递归神经网络与卷积网络相结合，我们能够从SMILES字符串中提取自回归信息，同时保持信号和长距离依赖性。这使得在重建已知分子时生成的分子具有非常高的有效性，达到了95%的水平。我们还观察到测试集和重建分子之间的平均Tanimoto相似度为0.6，这表明我们的方法能够在SMILES字符串和它们的学习表示之间进行映射。

    arXiv:2402.16854v1 Announce Type: cross  Abstract: Molecule generation is a task made very difficult by the complex ways in which we represent molecules computationally. A common technique used in molecular generative modeling is to use SMILES strings with recurrent neural networks built into variational autoencoders - but these suffer from a myriad of issues: vanishing gradients, long-range forgetting, and invalid molecules. In this work, we show that by combining recurrent neural networks with convolutional networks in a hierarchical manner, we are able to both extract autoregressive information from SMILES strings while maintaining signal and long-range dependencies. This allows for generations with very high validity rates on the order of 95% when reconstructing known molecules. We also observe an average Tanimoto similarity of .6 between test set and reconstructed molecules, which suggests our method is able to map between SMILES strings and their learned representations in a more
    
[^170]: 基于基础模型低秩适配器的不对称性

    Asymmetry in Low-Rank Adapters of Foundation Models

    [https://arxiv.org/abs/2402.16842](https://arxiv.org/abs/2402.16842)

    本文研究发现在预训练模型的参数微调过程中存在着低秩适配器矩阵重要性的不对称性，特别是在更新参数矩阵时，$B$和$A$矩阵具有不同功能，微调$B$比微调$A$更加有效。

    

    参数高效微调通过更新参数的子集来优化大型、预先训练的基础模型；在这一类中，低秩适应（LoRA）特别有效。受到调查LoRA矩阵在微调过程中不同作用的启发，本文表征和利用了低秩适配器矩阵重要性的意外不对称性。具体地，在通过添加乘积$BA$来更新神经网络的参数矩阵时，我们观察到$B$和$A$矩阵具有不同的功能：$A$从输入中提取特征，而$B$利用这些特征创建期望的输出。基于这一观察，我们证明微调$B$固有地比微调$A$更有效，并且一个随机未经训练的$A$应该表现几乎与经过微调的$A$一样好。同时，我们还使用信息论视角限制了低秩适配器的泛化能力。

    arXiv:2402.16842v2 Announce Type: replace  Abstract: Parameter-efficient fine-tuning optimizes large, pre-trained foundation models by updating a subset of parameters; in this class, Low-Rank Adaptation (LoRA) is particularly effective. Inspired by an effort to investigate the different roles of LoRA matrices during fine-tuning, this paper characterizes and leverages unexpected asymmetry in the importance of low-rank adapter matrices. Specifically, when updating the parameter matrices of a neural network by adding a product $BA$, we observe that the $B$ and $A$ matrices have distinct functions: $A$ extracts features from the input, while $B$ uses these features to create the desired output. Based on this observation, we demonstrate that fine-tuning $B$ is inherently more effective than fine-tuning $A$, and that a random untrained $A$ should perform nearly as well as a fine-tuned one. Using an information-theoretic lens, we also bound the generalization of low-rank adapters, showing tha
    
[^171]: 作为可优化图的语言代理

    Language Agents as Optimizable Graphs

    [https://arxiv.org/abs/2402.16823](https://arxiv.org/abs/2402.16823)

    将基于LLM的代理统一描述为计算图，提出新颖的自动图优化器来改进节点和边，实现了代理之间的自动协作和改进。

    

    多种人类设计的提升技术被提出，用于改进基于大型语言模型（LLMs）的问题求解器，产生了许多不同的代码库。我们通过将LLM代理描述为计算图来统一这些方法。节点实现处理多模态数据或查询LLMs的功能，并且边描述操作之间的信息流动。图形可以递归地组合成代表不同代理之间协作层次的更大组合图（其中边连接不同代理的操作）。我们的新颖自动图优化器（1）优化节点级LLM提示（节点优化）并（2）通过改变图连接性来改善代理协调（边缘优化）。实验证明我们的框架可用于高效开发、集成和自动改进各种LLM代理。代码可在https://github.com/metauto-ai/gptswarm找到。

    arXiv:2402.16823v1 Announce Type: cross  Abstract: Various human-designed prompt engineering techniques have been proposed to improve problem solvers based on Large Language Models (LLMs), yielding many disparate code bases. We unify these approaches by describing LLM-based agents as computational graphs. The nodes implement functions to process multimodal data or query LLMs, and the edges describe the information flow between operations. Graphs can be recursively combined into larger composite graphs representing hierarchies of inter-agent collaboration (where edges connect operations of different agents). Our novel automatic graph optimizers (1) refine node-level LLM prompts (node optimization) and (2) improve agent orchestration by changing graph connectivity (edge optimization). Experiments demonstrate that our framework can be used to efficiently develop, integrate, and automatically improve various LLM agents. The code can be found at https://github.com/metauto-ai/gptswarm.
    
[^172]: Nemotron-4 15B技术报告

    Nemotron-4 15B Technical Report

    [https://arxiv.org/abs/2402.16819](https://arxiv.org/abs/2402.16819)

    Nemotron-4 15B是一个150亿参数的大型多语言模型，在多语言能力上表现优异，超过其他规模相似的模型。

    

    我们介绍了Nemotron-4 15B，这是一个拥有150亿参数的大型多语言模型，训练过程中使用了8000万亿个文本标记。Nemotron-4 15B在英语、多语言和编码任务上表现出色：在7个下游评估领域中，它在4个领域中表现出色，并在其余领域中取得了竞争性表现，超过了所有现有规模相似的开放模型。具体来说，Nemotron-4 15B展现出了所有规模相似模型中最强的多语言能力，甚至在多语言任务上优于四倍以上的大型模型，以及专门用于多语言任务的模型。

    arXiv:2402.16819v1 Announce Type: new  Abstract: We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates strong performance when assessed on English, multilingual, and coding tasks: it outperforms all existing similarly-sized open models on 4 out of 7 downstream evaluation areas and achieves competitive performance to the leading open models in the remaining ones. Specifically, Nemotron-4 15B exhibits the best multilingual capabilities of all similarly-sized models, even outperforming models over four times larger and those explicitly specialized for multilingual tasks.
    
[^173]: 在复杂模块化算术中解释理解的Transformer

    Interpreting Grokked Transformers in Complex Modular Arithmetic

    [https://arxiv.org/abs/2402.16726](https://arxiv.org/abs/2402.16726)

    本研究通过可解释的逆向工程在复杂模块化算术中观察了Transformer内部电路学习过程，并发现减法在Transformer上造成了强烈的不对称性，乘法需要余弦偏置分量，多项式叠加了基本算术模式，但在挑战性情况下并不清晰，Grokking甚至可以在具有基本对称和交替表达式的高次公式中轻松发生。

    

    Grokking一直是解开延迟泛化之谜的积极探索。在已解密模型中识别可解释的算法是理解其机制的暗示性线索。在这项工作中，除了最简单和广为研究的模块化加法外，我们通过可解释的逆向工程观察了通过Grokking在复杂模块化算术中学到的内部电路，突出显示了它们动力学上的重大差异：减法对Transformer产生强烈的不对称性；乘法在傅立叶域的所有频率上需要余弦偏置分量；多项式通常导致基本算术模式的叠加，但在挑战性情况下清晰的模式并不显现；即使在具有基本对称和交替表达式的高次公式中，Grokking也很容易发生。我们还引入了模块化算术的新颖进展度量；傅立叶频率

    arXiv:2402.16726v2 Announce Type: replace-cross  Abstract: Grokking has been actively explored to reveal the mystery of delayed generalization. Identifying interpretable algorithms inside the grokked models is a suggestive hint to understanding its mechanism. In this work, beyond the simplest and well-studied modular addition, we observe the internal circuits learned through grokking in complex modular arithmetic via interpretable reverse engineering, which highlights the significant difference in their dynamics: subtraction poses a strong asymmetry on Transformer; multiplication requires cosine-biased components at all the frequencies in a Fourier domain; polynomials often result in the superposition of the patterns from elementary arithmetic, but clear patterns do not emerge in challenging cases; grokking can easily occur even in higher-degree formulas with basic symmetric and alternating expressions. We also introduce the novel progress measure for modular arithmetic; Fourier Freque
    
[^174]: 预训练的视觉不确定性

    Pretrained Visual Uncertainties

    [https://arxiv.org/abs/2402.16569](https://arxiv.org/abs/2402.16569)

    这项工作引入了第一个针对视觉模型的预训练不确定性模块，实现了不确定性的零-shot转移并加速了训练，能泛化到未知数据集，使得安全检索和对不确定性敏感的数据集可视化成为可能。

    

    准确的不确定性估计对于可信任的机器学习至关重要，然而通常不确定性必须针对每个任务重新学习。这项工作引入了第一个针对视觉模型的预训练不确定性模块。类似于标准的预训练，这使得在大型预训练数据集上学习的不确定性可以零-shot转移到专门的下游数据集。我们通过解决以前不确定性模块中的梯度冲突并将训练加速高达180倍来实现在ImageNet-21k上的大规模预训练。我们发现预训练的不确定性可以泛化到未知数据集。在审查学习到的不确定性时，我们发现其捕获了与认知成分分离的非认知不确定性。我们证明这使得安全检索和对不确定性敏感的数据集可视化成为可能。为了鼓励将应用拓展到更多问题和领域，我们公开发布了所有预训练的检查点。

    arXiv:2402.16569v2 Announce Type: replace-cross  Abstract: Accurate uncertainty estimation is vital to trustworthy machine learning, yet uncertainties typically have to be learned for each task anew. This work introduces the first pretrained uncertainty modules for vision models. Similar to standard pretraining this enables the zero-shot transfer of uncertainties learned on a large pretraining dataset to specialized downstream datasets. We enable our large-scale pretraining on ImageNet-21k by solving a gradient conflict in previous uncertainty modules and accelerating the training by up to 180x. We find that the pretrained uncertainties generalize to unseen datasets. In scrutinizing the learned uncertainties, we find that they capture aleatoric uncertainty, disentangled from epistemic components. We demonstrate that this enables safe retrieval and uncertainty-aware dataset visualization. To encourage applications to further problems and domains, we release all pretrained checkpoints an
    
[^175]: 反馈高效在线微调扩散模型

    Feedback Efficient Online Fine-Tuning of Diffusion Models

    [https://arxiv.org/abs/2402.16359](https://arxiv.org/abs/2402.16359)

    提出了一种反馈高效的在线微调扩散模型的强化学习程序

    

    扩散模型在建模复杂数据分布方面表现出色，包括图像，蛋白质和小分子的分布。然而，在许多情况下，我们的目标是模拟最大化某些属性的分布的部分：例如，我们可能希望生成具有高审美质量的图像，或具有高生物活性的分子。自然地，我们可以将这视为一个强化学习（RL）问题，其目标是微调扩散模型以最大化与某些属性对应的奖励函数。即使可以访问地面真实奖励函数的在线查询，有效地发现高奖励样本也可能具有挑战性：它们在初始分布中的概率可能很低，并且可能存在许多不可行的样本，甚至没有定义良好的奖励（例如，不自然的图像或物理上不可能的分子）。在这项工作中，我们提出了一种新颖的强化学习程序，可以高效地发现高奖励样本。

    arXiv:2402.16359v1 Announce Type: cross  Abstract: Diffusion models excel at modeling complex data distributions, including those of images, proteins, and small molecules. However, in many cases, our goal is to model parts of the distribution that maximize certain properties: for example, we may want to generate images with high aesthetic quality, or molecules with high bioactivity. It is natural to frame this as a reinforcement learning (RL) problem, in which the objective is to fine-tune a diffusion model to maximize a reward function that corresponds to some property. Even with access to online queries of the ground-truth reward function, efficiently discovering high-reward samples can be challenging: they might have a low probability in the initial distribution, and there might be many infeasible samples that do not even have a well-defined reward (e.g., unnatural images or physically impossible molecules). In this work, we propose a novel reinforcement learning procedure that effi
    
[^176]: CoDream：使用异构模型交换梦想而不是模型进行联合聚合

    CoDream: Exchanging dreams instead of models for federated aggregation with heterogeneous models

    [https://arxiv.org/abs/2402.15968](https://arxiv.org/abs/2402.15968)

    CoDream提出了一种通过在输入数据空间中协作优化数据来交换知识的框架，实现了模型之间的合作学习，实现了模型架构无关、通信不受模型大小影响、兼容安全聚合的优点。

    

    联邦学习（FL）通过聚合模型参数实现机器学习模型在分散数据上的协作优化。我们的方法通过聚合模型产生的“知识”，而不是模型参数来扩展这一概念。我们提出了一个名为 \codream 的新颖框架，在这个框架中，客户端通过在输入数据空间中使用联合优化来协作优化随机初始化的数据，类似于在FL中优化随机初始化的模型参数。我们的关键见解是，联合优化这些数据可以有效捕获全局数据分布的特性。在数据空间共享知识具有许多好处：（1）与模型无关的协作学习，即不同客户端可以具有不同的模型架构；（2）通信不受模型大小影响，消除了模型参数的可伸缩性问题；（3）与安全聚合兼容，因此可预

    arXiv:2402.15968v1 Announce Type: cross  Abstract: Federated Learning (FL) enables collaborative optimization of machine learning models across decentralized data by aggregating model parameters. Our approach extends this concept by aggregating "knowledge" derived from models, instead of model parameters. We present a novel framework called \codream, where clients collaboratively optimize randomly initialized data using federated optimization in the input data space, similar to how randomly initialized model parameters are optimized in FL. Our key insight is that jointly optimizing this data can effectively capture the properties of the global data distribution. Sharing knowledge in data space offers numerous benefits: (1) model-agnostic collaborative learning, i.e., different clients can have different model architectures; (2) communication that is independent of the model size, eliminating scalability concerns with model parameters; (3) compatibility with secure aggregation, thus pre
    
[^177]: 论三层神经网络动力学：初始凝聚

    On the dynamics of three-layer neural networks: initial condensation

    [https://arxiv.org/abs/2402.15958](https://arxiv.org/abs/2402.15958)

    深入研究三层神经网络训练中的凝聚现象和梯度下降方法自发减少神经网络复杂性的机制，提出有效动力学的爆炸性质和凝聚发生的充分条件，并通过实验证实了这些发现。

    

    经验和理论研究显示，当初始化为小值时，两层神经网络的输入权重会收敛到孤立的方向。这种现象被称为凝聚，表明梯度下降方法在训练过程中往往会自发地减少神经网络的复杂性。 在这项研究中，我们阐明了三层神经网络训练中出现的凝聚现象背后的机制，并将其与两层神经网络的训练进行区分。 通过严格的理论分析，我们建立了有效动力学的爆炸性质，并提出了发生凝聚的充分条件，这些发现得到了实验结果的证实。此外，我们还探讨了凝聚与深度矩阵因子分解中观察到的低秩偏差之间的关联。

    arXiv:2402.15958v1 Announce Type: new  Abstract: Empirical and theoretical works show that the input weights of two-layer neural networks, when initialized with small values, converge towards isolated orientations. This phenomenon, referred to as condensation, indicates that the gradient descent methods tend to spontaneously reduce the complexity of neural networks during the training process. In this work, we elucidate the mechanisms behind the condensation phenomena occurring in the training of three-layer neural networks and distinguish it from the training of two-layer neural networks. Through rigorous theoretical analysis, we establish the blow-up property of effective dynamics and present a sufficient condition for the occurrence of condensation, findings that are substantiated by experimental results. Additionally, we explore the association between condensation and the low-rank bias observed in deep matrix factorization.
    
[^178]: GraphEdit：用于图结构学习的大型语言模型

    GraphEdit: Large Language Models for Graph Structure Learning

    [https://arxiv.org/abs/2402.15183](https://arxiv.org/abs/2402.15183)

    本研究提出了一种名为GraphEdit的方法，利用大型语言模型（LLMs）学习复杂的图结构化数据中的节点关系，通过在图结构上进行指导调整，增强LLMs的推理能力，从而提高图结构学习的可靠性。

    

    图结构学习（GSL）致力于通过生成新颖的图结构来捕捉图结构数据中节点之间的固有依赖性和相互作用。本文提出了一种名为GraphEdit的方法，利用大型语言模型（LLMs）学习图结构化数据中复杂的节点关系。通过在图结构上进行指导调整，增强LLMs的推理能力，我们旨在克服显式图结构信息带来的挑战，并提高图结构学习的可靠性。

    arXiv:2402.15183v1 Announce Type: cross  Abstract: Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies and interactions among nodes in graph-structured data by generating novel graph structures. Graph Neural Networks (GNNs) have emerged as promising GSL solutions, utilizing recursive message passing to encode node-wise inter-dependencies. However, many existing GSL methods heavily depend on explicit graph structural information as supervision signals, leaving them susceptible to challenges such as data noise and sparsity. In this work, we propose GraphEdit, an approach that leverages large language models (LLMs) to learn complex node relationships in graph-structured data. By enhancing the reasoning capabilities of LLMs through instruction-tuning over graph structures, we aim to overcome the limitations associated with explicit graph structural information and enhance the reliability of graph structure learning. Our approach not only effectively denoises noisy co
    
[^179]: 打破Breakout: 用自我完善重新定义LM对抗越狱攻击的防御

    Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement

    [https://arxiv.org/abs/2402.15180](https://arxiv.org/abs/2402.15180)

    提出了一种通过自我完善和格式化改进LMs对抗越狱攻击的方法，即使在非安全对齐的LMs中也具有出色的安全性，同时降低攻击成功率。

    

    警告：本文包含可能引起不快的冒犯性词语。语言模型（LMs）容易被利用进行恶意滥用。对LM进行安全对齐的训练非常复杂，使得难以立即应对快速发展的攻击，如越狱攻击。我们提出了一种通过格式自我完善的方法，即使在非安全对齐的LMs中也能实现出色的安全性，并将我们的方法与几种防御基线进行评估，表明这是针对越狱攻击最安全的无训练方法。此外，我们提出了一种改进自我完善过程效率的格式化方法，同时在较少迭代中降低攻击成功率。我们还观察到非安全对齐的LM在安全任务中表现优于安全对齐的LM，因为它们给出更有用且更安全的回复。总之，我们的发现能够在较少的计算成本下实现更少的安全风险。

    arXiv:2402.15180v1 Announce Type: cross  Abstract: Caution: This paper includes offensive words that could potentially cause unpleasantness. Language models (LMs) are vulnerable to exploitation for adversarial misuse. Training LMs for safety alignment is extensive and makes it hard to respond to fast-developing attacks immediately, such as jailbreaks. We propose self-refine with formatting that achieves outstanding safety even in non-safety-aligned LMs and evaluate our method alongside several defense baselines, demonstrating that it is the safest training-free method against jailbreak attacks. Additionally, we proposed a formatting method that improves the efficiency of the self-refine process while reducing attack success rates in fewer iterations. We've also observed that non-safety-aligned LMs outperform safety-aligned LMs in safety tasks by giving more helpful and safe responses. In conclusion, our findings can achieve less safety risk with fewer computational costs, allowing non-
    
[^180]: 面向预训练大型语言模型的机器遗忘

    Machine Unlearning of Pre-trained Large Language Models

    [https://arxiv.org/abs/2402.15159](https://arxiv.org/abs/2402.15159)

    本研究在大型语言模型（LLMs）中探讨了“被遗忘权”的概念，提出机器遗忘作为解决方案，并在预训练模型中建立了全面的遗忘框架及高效的遗忘方法，同时提供了改进超参数鲁棒性以及高效调整超参数的指南。

    

    本研究探讨了大型语言模型（LLMs）背景下“被遗忘权”的概念。我们以机器遗忘作为一个关键解决方案，重点关注预训练模型——一个明显缺乏研究的领域。我们在预训练LLMs中勾勒了一个全面的机器遗忘框架，包括对七种不同遗忘方法的批判性分析。通过使用来自arXiv、书籍和GitHub的策划数据集进行严格评估，我们建立了一个有力的机器遗忘性能基准，表明这些方法的计算效率比重新训练高出 $10^5$ 倍以上。我们的结果表明，在分布数据上将梯度上升与梯度下降结合可以改善超参数的鲁棒性。我们还提供了关于在遗忘过程中进行高效超参数调整的详细指南。我们的研究推动了有关伦理人工智能实践的讨论，提供了

    arXiv:2402.15159v1 Announce Type: cross  Abstract: This study investigates the concept of the `right to be forgotten' within the context of large language models (LLMs). We explore machine unlearning as a pivotal solution, with a focus on pre-trained models--a notably under-researched area. Our research delineates a comprehensive framework for machine unlearning in pre-trained LLMs, encompassing a critical analysis of seven diverse unlearning methods. Through rigorous evaluation using curated datasets from arXiv, books, and GitHub, we establish a robust benchmark for unlearning performance, demonstrating that these methods are over $10^5$ times more computationally efficient than retraining. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering
    
[^181]: COPR:通过最优策略正则化实现持续人类偏好学习

    COPR: Continual Human Preference Learning via Optimal Policy Regularization

    [https://arxiv.org/abs/2402.14228](https://arxiv.org/abs/2402.14228)

    提出了Continual Optimal Policy Regularization (COPR) 方法，通过借鉴最优策略理论，利用采样分布作为示范和正则化约束，以动态地对当前策略进行正则化，从而使强化学习从人类反馈中学习在持续学习情境下更加稳健

    

    arXiv:2402.14228v1 公告类型:跨界 摘要: 利用强化学习从人类反馈中学习（RLHF）通常用于改善大型语言模型（LLMs）与人类偏好的对齐。鉴于人类偏好的不断变化，持续对齐相对于传统静态对齐变得更加重要和实际。然而，使RLHF与持续学习（CL）兼容由于其复杂过程而具有挑战性。同时，直接学习新的人类偏好可能导致历史偏好的灾难性遗忘（CF），导致无助或有害的结果。为了克服这些挑战，我们提出了Continual Optimal Policy Regularization (COPR) 方法，该方法借鉴了最优策略理论。COPR利用采样分布作为示范和正则化约束用于持续学习。它采用Lagrange对偶（LD）方法根据历史上的最优策略动态地正则化当前策略

    arXiv:2402.14228v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is commonly utilized to improve the alignment of Large Language Models (LLMs) with human preferences. Given the evolving nature of human preferences, continual alignment becomes more crucial and practical in comparison to traditional static alignment. Nevertheless, making RLHF compatible with Continual Learning (CL) is challenging due to its complex process. Meanwhile, directly learning new human preferences may lead to Catastrophic Forgetting (CF) of historical preferences, resulting in helpless or harmful outputs. To overcome these challenges, we propose the Continual Optimal Policy Regularization (COPR) method, which draws inspiration from the optimal policy theory. COPR utilizes a sampling distribution as a demonstration and regularization constraints for CL. It adopts the Lagrangian Duality (LD) method to dynamically regularize the current policy based on the historically optimal p
    
[^182]: 委员会的智慧：从基础模型到专用应用模型的提取

    Wisdom of Committee: Distilling from Foundation Model to SpecializedApplication Model

    [https://arxiv.org/abs/2402.14035](https://arxiv.org/abs/2402.14035)

    将基础模型的知识转移到专用应用模型中存在挑战，提出了通过创建教学委员会来应对这些挑战。

    

    最近基础模型的进展在各种任务上取得了令人印象深刻的性能，与此同时，为特定应用，从业者们一直在开发专门的应用模型。为了享受这两种模型的好处，一个自然的路径是将基础模型中的知识转移到专用应用模型中，后者通常更有效地提供服务。知识蒸馏的技术可以在这里应用，其中应用模型学会模仿基础模型。然而，专用应用模型和基础模型在容量上存在实质性差距，采用不同的架构，使用来自不同模态的不同输入特征，并在不同的分布上进行优化。模型特征上的这些差异导致了蒸馏方法面临重大挑战。在这项工作中，我们提出创建一个教学委员会，包括基础模型和专用应用模型。

    arXiv:2402.14035v1 Announce Type: cross  Abstract: Recent advancements in foundation models have yielded impressive performance across a wide range of tasks. Meanwhile, for specific applications, practitioners have been developing specialized application models. To enjoy the benefits of both kinds of models, one natural path is to transfer the knowledge in foundation models into specialized application models, which are generally more efficient for serving. Techniques from knowledge distillation may be applied here, where the application model learns to mimic the foundation model. However, specialized application models and foundation models have substantial gaps in capacity, employing distinct architectures, using different input features from different modalities, and being optimized on different distributions. These differences in model characteristics lead to significant challenges for distillation methods. In this work, we propose creating a teaching committee comprising both foun
    
[^183]: AttackGNN: 使用强化学习在硬件安全中对GNN进行红队测试

    AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement Learning

    [https://arxiv.org/abs/2402.13946](https://arxiv.org/abs/2402.13946)

    这项工作提出了AttackGNN，针对硬件安全中使用的基于GNN的技术进行了首次红队攻击，通过设计新颖的强化学习代理生成对抗性示例电路。

    

    机器学习在解决一些关键的硬件安全问题上表现出了极大的潜力。研究人员开发了基于图神经网络（GNN）的新颖技术，用于检测知识产权（IP）盗版、检测硬件特洛伊木马（HTs）和反向工程电路等问题。这些技术表现出色，受到了广泛关注。本文提出了AttackGNN，这是针对硬件安全中基于GNN的技术的第一个红队攻击。为此，我们设计了一种新颖的强化学习（RL）代理，用于生成针对GNN技术的对抗示例，即电路。

    arXiv:2402.13946v1 Announce Type: new  Abstract: Machine learning has shown great promise in addressing several critical hardware security problems. In particular, researchers have developed novel graph neural network (GNN)-based techniques for detecting intellectual property (IP) piracy, detecting hardware Trojans (HTs), and reverse engineering circuits, to name a few. These techniques have demonstrated outstanding accuracy and have received much attention in the community. However, since these techniques are used for security applications, it is imperative to evaluate them thoroughly and ensure they are robust and do not compromise the security of integrated circuits.   In this work, we propose AttackGNN, the first red-team attack on GNN-based techniques in hardware security. To this end, we devise a novel reinforcement learning (RL) agent that generates adversarial examples, i.e., circuits, against the GNN-based techniques. We overcome three challenges related to effectiveness, scal
    
[^184]: ProSparse: 引入和增强大型语言模型内部激活稀疏性

    ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models

    [https://arxiv.org/abs/2402.13516](https://arxiv.org/abs/2402.13516)

    本文介绍了一种名为"ProSparse"的有效稀疏化方法，以推动大型语言模型实现更高的激活稀疏性而不降低模型性能

    

    Activation sparsity指的是激活输出中存在许多弱贡献元素。作为使用ReLU激活函数的模型的普遍属性，已被证明是提高模型推理效率的一种有前途的范例。然而，大多数大型语言模型（LLMs）采用了没有内在激活稀疏性的激活函数（例如GELU和Swish）。一些最近的努力尝试引入ReLU或其变体作为替代激活函数，以帮助LLMs实现激活稀疏性和推理加速，但很少能同时获得高稀疏度和可比较的模型性能。本文介绍了一种名为"ProSparse"的有效稀疏化方法，以推动LLMs实现更高的激活稀疏性而不降低模型性能。具体来说，将LLMs的激活函数替换为ReLU后，ProSparse采用渐进稀疏正则化

    arXiv:2402.13516v1 Announce Type: cross  Abstract: Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, it has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces an effective sparsification method named "ProSparse" to push LLMs for higher activation sparsity without decreasing model performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization wit
    
[^185]: 来自异构数据的联邦因果发现

    Federated Causal Discovery from Heterogeneous Data

    [https://arxiv.org/abs/2402.13241](https://arxiv.org/abs/2402.13241)

    该研究提出了一种新型联邦因果发现方法，旨在适应任意因果模型和异构数据，通过使用替代变量和联邦条件独立性检验来解决数据异质性，并建立了联邦独立变化原则用于确定因果方向。

    

    传统的因果发现方法依赖于集中式数据，这与许多实际情况下数据的分散性质不一致。这种差异推动了联邦因果发现（FCD）方法的发展。然而，现有的FCD方法可能受到其对可识别功能因果模型或 homogeneous数据分布的潜在限制，从而限制了它们在各种场景中的适用性。在本文中，我们提出了一种尝试适应任意因果模型和异构数据的新型FCD方法。我们首先利用与客户端索引对应的替代变量，以解决不同客户端之间的数据异质性。然后我们开发了一个用于因果骨架发现的联邦条件独立性检验（FCIT），并建立了一个用于确定因果方向的联邦独立变化原则（FICP）。这些方法涉及构建

    arXiv:2402.13241v1 Announce Type: cross  Abstract: Conventional causal discovery methods rely on centralized data, which is inconsistent with the decentralized nature of data in many real-world situations. This discrepancy has motivated the development of federated causal discovery (FCD) approaches. However, existing FCD methods may be limited by their potentially restrictive assumptions of identifiable functional causal models or homogeneous data distributions, narrowing their applicability in diverse scenarios. In this paper, we propose a novel FCD method attempting to accommodate arbitrary causal models and heterogeneous data. We first utilize a surrogate variable corresponding to the client index to account for the data heterogeneity across different clients. We then develop a federated conditional independence test (FCIT) for causal skeleton discovery and establish a federated independent change principle (FICP) to determine causal directions. These approaches involve constructing
    
[^186]: 基于相似性的高维域自适应算法用于多传感器时间序列分类

    SMORE: Similarity-based Hyperdimensional Domain Adaptation for Multi-Sensor Time Series Classification

    [https://arxiv.org/abs/2402.13233](https://arxiv.org/abs/2402.13233)

    提出了SMORE，一种新颖的多传感器时间序列分类领域自适应算法，利用高维计算的高效和并行操作，动态定制测试模型以减轻数据分布偏移带来的性能下降。

    

    许多物联网(IoT)的实际应用利用机器学习(ML)算法分析由相互连接的传感器收集的时间序列信息。然而，在部署在与训练数据不同的数据分布上的模型时，数据驱动的ML中固有的挑战——分布偏移会显著降低模型性能。此外，越来越复杂的深度神经网络(DNNs)需要捕获多传感器时间序列数据中复杂的空间和时间依赖关系，往往超过了今天边缘设备的能力。 在本文中，我们提出了SMORE，一种新颖的资源高效的多传感器时间序列分类领域自适应(DA)算法，利用了超高维计算的高效和并行操作。 SMORE动态地定制测试模型，明确考虑每个样本的领域上下文，以减轻

    arXiv:2402.13233v1 Announce Type: new  Abstract: Many real-world applications of the Internet of Things (IoT) employ machine learning (ML) algorithms to analyze time series information collected by interconnected sensors. However, distribution shift, a fundamental challenge in data-driven ML, arises when a model is deployed on a data distribution different from the training data and can substantially degrade model performance. Additionally, increasingly sophisticated deep neural networks (DNNs) are required to capture intricate spatial and temporal dependencies in multi-sensor time series data, often exceeding the capabilities of today's edge devices. In this paper, we propose SMORE, a novel resource-efficient domain adaptation (DA) algorithm for multi-sensor time series classification, leveraging the efficient and parallel operations of hyperdimensional computing. SMORE dynamically customizes test-time models with explicit consideration of the domain context of each sample to mitigate
    
[^187]: ISCUTE：使用文本嵌入进行电缆实例分割

    ISCUTE: Instance Segmentation of Cables Using Text Embedding

    [https://arxiv.org/abs/2402.11996](https://arxiv.org/abs/2402.11996)

    提出了一种基于文本提示的DLO实例分割技术，结合了CLIPSeg模型的文本条件语义分割能力和Segment Anything Model的零样本泛化能力，有效解决了传统方法在感知可变形直线对象如电线、电缆和柔性管道方面的挑战，性能超越了目前的技术水平，同时引入了一个新的DLO特定数据集。

    

    在机器人技术和自动化领域，传统的对象识别和实例分割方法在感知类似电线、电缆和柔性管道等可变形直线对象（DLOs）方面面临着巨大挑战。这一挑战主要源于缺乏形状、颜色和纹理等明显属性，这需要量身定制的解决方案来实现精确识别。在这项工作中，我们提出了一种基于基于文本提示的、用户友好的DLO实例分割技术。具体来说，我们的方法结合了CLIPSeg模型的文本条件语义分割能力和Segment Anything Model (SAM) 的零样本泛化能力。我们展示了我们的方法在DLO实例分割方面超越了最先进技术，实现了$91.21\%$的平均交并比（mIoU）。我们还介绍了一个丰富多样的用于实例分割的DLO特定数据集。

    arXiv:2402.11996v1 Announce Type: cross  Abstract: In the field of robotics and automation, conventional object recognition and instance segmentation methods face a formidable challenge when it comes to perceiving Deformable Linear Objects (DLOs) like wires, cables, and flexible tubes. This challenge arises primarily from the lack of distinct attributes such as shape, color, and texture, which calls for tailored solutions to achieve precise identification. In this work, we propose a foundation model-based DLO instance segmentation technique that is text-promptable and user-friendly. Specifically, our approach combines the text-conditioned semantic segmentation capabilities of CLIPSeg model with the zero-shot generalization capabilities of Segment Anything Model (SAM). We show that our method exceeds SOTA performance on DLO instance segmentation, achieving a mIoU of $91.21\%$. We also introduce a rich and diverse DLO-specific dataset for instance segmentation.
    
[^188]: 生成万花筒网络

    Generative Kaleidoscopic Networks

    [https://arxiv.org/abs/2402.11793](https://arxiv.org/abs/2402.11793)

    发现深层ReLU网络表现出过度泛化现象，利用这一特性设计了“生成万花筒网络”，通过递归映射随机输入噪声生成样本。

    

    发现深层ReLU网络（或多层感知器架构）表现出“过度泛化”现象。也就是说，那些在训练过程中没有看到的输入的输出值被映射到了在学习过程中观察到的输出范围附近。换句话说，多层感知器学习了一对多的映射，这种效应在增加层数或多层感知器的深度时更为明显。我们利用了深层ReLU网络的这一特性来设计一个数据集万花筒，称为“生成万花筒网络”。简而言之，如果我们学习一个多层感知器将输入 $x\in\mathbb{R}^D$ 映射到自身 $f_\mathcal{N}(x)\rightarrow x$，那么“万花筒采样”过程将从随机输入噪声 $z\in\mathbb{R}^D$ 开始，并递归地应用 $f_\mathcal{N}(\cdots f_\mathcal{N}(z)\cdots )$。经过燃烧期后，我们开始观察来自输入分布的样本，我们发现更深的

    arXiv:2402.11793v1 Announce Type: cross  Abstract: We discovered that the Deep ReLU networks (or Multilayer Perceptron architecture) demonstrate an 'over-generalization' phenomenon. That is, the output values for the inputs that were not seen during training are mapped close to the output range that were observed during the learning process. In other words, the MLP learns a many-to-one mapping and this effect is more prominent as we increase the number of layers or depth of the MLP. We utilize this property of Deep ReLU networks to design a dataset kaleidoscope, termed as 'Generative Kaleidoscopic Networks'. Briefly, if we learn a MLP to map from input $x\in\mathbb{R}^D$ to itself $f_\mathcal{N}(x)\rightarrow x$, the 'Kaleidoscopic sampling' procedure starts with a random input noise $z\in\mathbb{R}^D$ and recursively applies $f_\mathcal{N}(\cdots f_\mathcal{N}(z)\cdots )$. After a burn-in period duration, we start observing samples from the input distribution and we found that deeper 
    
[^189]: PolypNextLSTM：使用ConvNext和ConvLSTM的轻量级快速息肉视频分割网络

    PolypNextLSTM: A lightweight and fast polyp video segmentation network using ConvNext and ConvLSTM

    [https://arxiv.org/abs/2402.11585](https://arxiv.org/abs/2402.11585)

    PolypNextLSTM是一个轻量且快速的息肉视频分割网络，使用ConvNext和ConvLSTM，最大的创新在于参数最少且速度最快，性能超越了五种先进的基于图像和视频的深度学习模型。

    

    通常用于息肉分割的单图像UNet架构缺乏临床医生在诊断息肉时从视频数据中获得的时间洞察。为了更忠实地反映临床实践，我们提出的解决方案PolypNextLSTM利用基于视频的深度学习，利用时间信息实现了卓越的分割性能，参数开销最小，可能适用于边缘设备。PolypNextLSTM采用类似UNet的结构，ConvNext-Tiny作为其主干，策略性地省略最后两层以减少参数开销。我们的时间融合模块，一个卷积长短期记忆（ConvLSTM），有效地利用时间特征。我们的主要创新在于PolypNextLSTM，在参数上最瘦且速度最快，超越了五种最先进的基于图像和视频的深度学习模型的性能。SUN-SEG数据集的评估跨越了

    arXiv:2402.11585v1 Announce Type: cross  Abstract: Commonly employed in polyp segmentation, single image UNet architectures lack the temporal insight clinicians gain from video data in diagnosing polyps. To mirror clinical practices more faithfully, our proposed solution, PolypNextLSTM, leverages video-based deep learning, harnessing temporal information for superior segmentation performance with the least parameter overhead, making it possibly suitable for edge devices. PolypNextLSTM employs a UNet-like structure with ConvNext-Tiny as its backbone, strategically omitting the last two layers to reduce parameter overhead. Our temporal fusion module, a Convolutional Long Short Term Memory (ConvLSTM), effectively exploits temporal features. Our primary novelty lies in PolypNextLSTM, which stands out as the leanest in parameters and the fastest model, surpassing the performance of five state-of-the-art image and video-based deep learning models. The evaluation of the SUN-SEG dataset spans 
    
[^190]: 用RLHF推进翻译偏好建模：迈向成本效益解决方案

    Advancing Translation Preference Modeling with RLHF: A Step Towards Cost-Effective Solution

    [https://arxiv.org/abs/2402.11525](https://arxiv.org/abs/2402.11525)

    提出了一种利用强化学习和人类反馈（RLHF）来改进翻译质量的成本效益偏好学习策略，该策略通过优化奖励模型来区分人类和机器翻译，从而指导改进机器翻译。

    

    arXiv:2402.11525v1 公告类型：新 真实性、表达力和优雅是机器翻译中不断追求的目标。然而，传统的度量标准如BLEU并不严格符合人类对翻译质量的偏好。本文探讨了利用强化学习与人类反馈（RLHF）来提高翻译质量。收集人类对翻译之间的比较的大规模高质量数据集并不容易，尤其对于低资源语言。为解决这一问题，我们提出了一种成本效益的偏好学习策略，通过区分人类和机器翻译来优化奖励模型。通过这种方式，奖励模型学习机器翻译与人类之间的不足之处，并指导随后对机器翻译的改进。实验结果表明，RLHF可以有效地提升翻译质量，这种改进也有益于其他翻译方向。

    arXiv:2402.11525v1 Announce Type: new  Abstract: Faithfulness, expressiveness, and elegance is the constant pursuit in machine translation. However, traditional metrics like \textit{BLEU} do not strictly align with human preference of translation quality. In this paper, we explore leveraging reinforcement learning with human feedback (\textit{RLHF}) to improve translation quality. It is non-trivial to collect a large high-quality dataset of human comparisons between translations, especially for low-resource languages. To address this issue, we propose a cost-effective preference learning strategy, optimizing reward models by distinguishing between human and machine translations. In this manner, the reward model learns the deficiencies of machine translation compared to human and guides subsequent improvements in machine translation. Experimental results demonstrate that \textit{RLHF} can effectively enhance translation quality and this improvement benefits other translation directions 
    
[^191]: 三界之最：实践中的数字营销自适应实验

    Best of Three Worlds: Adaptive Experimentation for Digital Marketing in Practice

    [https://arxiv.org/abs/2402.10870](https://arxiv.org/abs/2402.10870)

    本文分享了关于在工业环境中使用AED系统面临的挑战，并提供了在这种环境中适当的目标和系统规格的视角，最终开发了一个基于反事实推断的AED框架并在商业环境中进行了测试。

    

    自适应实验设计（AED）方法越来越多地被工业界用作一种工具，以提高测试吞吐量或减少与传统A/B/N测试方法相比的实验成本。然而，这些方法的行为和保证在理想化的稳态设置之外并不为人熟知。本文分享了有关在工业环境中天真地使用AED系统面临的挑战，以及在这种环境中适当的目标和系统规格的视角。我们根据这些经验开发了一个基于反事实推断的AED框架，并在商业环境中进行了测试。

    arXiv:2402.10870v1 Announce Type: new  Abstract: Adaptive experimental design (AED) methods are increasingly being used in industry as a tool to boost testing throughput or reduce experimentation cost relative to traditional A/B/N testing methods. However, the behavior and guarantees of such methods are not well-understood beyond idealized stationary settings. This paper shares lessons learned regarding the challenges of naively using AED systems in industrial settings where non-stationarity is prevalent, while also providing perspectives on the proper objectives and system specifications in such settings. We developed an AED framework for counterfactual inference based on these experiences, and tested it in a commercial environment.
    
[^192]: Johnson-Lindenstrauss的简单统一分析及其应用

    Simple, unified analysis of Johnson-Lindenstrauss with applications

    [https://arxiv.org/abs/2402.10232](https://arxiv.org/abs/2402.10232)

    这项工作提出了Johnson-Lindenstrauss（JL）引理的简单统一分析，简化和统一了各种构造，包括球形、高斯、二进制硬币和次高斯模型，通过创新性地将Hanson-Wright不等式拓展到高维度，标志着对数据固有几何的保持取得重大进展。

    

    在这项工作中，我们提出了Johnson-Lindenstrauss（JL）引理的简单统一分析，这是处理高维数据至关重要的降维领域中的基石。我们的方法不仅简化了理解，还将各种构造统一到JL框架下，包括球形、高斯、二进制硬币和次高斯模型。这种简化和统一在保持数据固有几何的重要性方面取得了重大进展，对从流算法到强化学习等各种应用至关重要。值得注意的是，我们在这个简化框架内提出了球形构造有效性的第一个严格证明。我们贡献的核心是将Hanson-Wright不等式拓展到高维度，具有明确的常数，这标志着文献中质的飞跃。通过运用简单而强大的概率工具

    arXiv:2402.10232v1 Announce Type: new  Abstract: In this work, we present a simple and unified analysis of the Johnson-Lindenstrauss (JL) lemma, a cornerstone in the field of dimensionality reduction critical for managing high-dimensional data. Our approach not only simplifies the understanding but also unifies various constructions under the JL framework, including spherical, Gaussian, binary coin, and sub-Gaussian models. This simplification and unification make significant strides in preserving the intrinsic geometry of data, essential across diverse applications from streaming algorithms to reinforcement learning. Notably, we deliver the first rigorous proof of the spherical construction's effectiveness within this simplified framework. At the heart of our contribution is an innovative extension of the Hanson-Wright inequality to high dimensions, complete with explicit constants, marking a substantial leap in the literature. By employing simple yet powerful probabilistic tools and 
    
[^193]: 数据到文本自然语言生成研究的系统性回顾

    A Systematic Review of Data-to-Text NLG

    [https://arxiv.org/abs/2402.08496](https://arxiv.org/abs/2402.08496)

    这篇系统性回顾全面分析了数据到文本自然语言生成研究的现状，提出未来方向，并解决了相关挑战。

    

    这篇系统性回顾旨在全面分析数据到文本生成研究的现状，重点是确定研究空白，提供未来方向，并解决回顾中发现的挑战。我们对文献进行了全面的检查，包括方法、数据集、评估指标、应用、多语言性和幻觉缓解措施。我们的回顾为这个快速发展的领域的未来研究提供了路线图。

    This systematic review aims to provide a comprehensive analysis of the state of data-to-text generation research, focusing on identifying research gaps, offering future directions, and addressing challenges found during the review. We thoroughly examined the literature, including approaches, datasets, evaluation metrics, applications, multilingualism, and hallucination mitigation measures. Our review provides a roadmap for future research in this rapidly evolving field.
    
[^194]: 关于Transformer架构的限制

    On Limitations of the Transformer Architecture

    [https://arxiv.org/abs/2402.08164](https://arxiv.org/abs/2402.08164)

    本论文通过通信复杂性证明了Transformer层在处理函数组合任务时的局限性，指出对于大型定义域和某些数学任务，Transformers可能无法解决。

    

    大型语言模型（LLMs）中幻觉的根本原因是什么？我们使用通信复杂性来证明，如果函数的定义域足够大，Transformer层无法组合函数（例如，在家谱中查找一个人的祖父）；我们通过示例显示，当定义域相当小的时候，这种能力的缺乏已经在经验上存在。我们还指出，许多在所谓的组合任务中的数学任务，认为它们对LLMs来说很难解决，对于足够大的实例来说，且假设计算复杂性领域的某些被广泛接受的猜想是正确的，Transformers也不太可能解决。

    What are the root causes of hallucinations in large language models (LLMs)? We use Communication Complexity to prove that the Transformer layer is incapable of composing functions (e.g., identify a grandparent of a person in a genealogy) if the domains of the functions are large enough; we show through examples that this inability is already empirically present when the domains are quite small. We also point out that several mathematical tasks that are at the core of the so-called compositional tasks thought to be hard for LLMs are unlikely to be solvable by Transformers, for large enough instances and assuming that certain well accepted conjectures in the field of Computational Complexity are true.
    
[^195]: TriAug：用于超声乳腺病变不平衡分类的异常样本检测

    TriAug: Out-of-Distribution Detection for Robust Classification of Imbalanced Breast Lesion in Ultrasound

    [https://arxiv.org/abs/2402.07452](https://arxiv.org/abs/2402.07452)

    TriAug是一个用于乳腺超声图像的异常样本检测框架，通过使用三元状态增强和平衡的球形损失来提高示踪分类的准确性和异常样本检测性能。

    

    不同的疾病，如乳腺病变的组织亚型，具有严重不同的发病率。即使通过大量的示踪数据进行训练，模型在临床实际中通常遇到属于未见类别的异常样本。为了解决这个问题，我们提出了一种新的框架，基于乳腺超声图像的长尾异常样本检测任务，并配备了一种三元状态增强（TriAug），它可以提高示踪分类的准确性，同时保持良好的异常样本检测性能。与此同时，我们设计了一个平衡的球形损失来处理类不平衡的问题。

    Different diseases, such as histological subtypes of breast lesions, have severely varying incidence rates. Even trained with substantial amount of in-distribution (ID) data, models often encounter out-of-distribution (OOD) samples belonging to unseen classes in clinical reality. To address this, we propose a novel framework built upon a long-tailed OOD detection task for breast ultrasound images. It is equipped with a triplet state augmentation (TriAug) which improves ID classification accuracy while maintaining a promising OOD detection performance. Meanwhile, we designed a balanced sphere loss to handle the class imbalanced problem.
    
[^196]: HarmBench：用于自动红队和强大拒绝的标准化评估框架

    HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal

    [https://arxiv.org/abs/2402.04249](https://arxiv.org/abs/2402.04249)

    HarmBench是一个为自动红队和强大拒绝设计的标准化评估框架，通过对18种红队方法和33个目标LLM和防御的比较，得出了新的见解，并介绍了一种高效的对抗训练方法，提高了LLM在各种攻击下的稳健性。

    

    自动红队具有发现和减轻大型语言模型（LLM）恶意使用的风险的巨大潜力，然而该领域缺乏一个标准化的评估框架来严格评估新方法。为了解决这个问题，我们引入了HarmBench，一个用于自动红队的标准化评估框架。我们在红队评估中确定了几个以前未考虑的有吸引力的特性，并系统地设计了HarmBench以满足这些标准。使用HarmBench，我们对18种红队方法和33个目标LLM和防御进行了大规模比较，得到了新的见解。我们还引入了一种高效的对抗训练方法，显著增强了LLM在各种攻击下的稳健性，展示了HarmBench如何促进攻击和防御的共同开发。我们在https://github.com/centerforaisafety/HarmBench上开源了HarmBench。

    Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github.com/centerforaisafety/HarmBench.
    
[^197]: 用语言模型区分可知与不可知的能力

    Distinguishing the Knowable from the Unknowable with Language Models

    [https://arxiv.org/abs/2402.03563](https://arxiv.org/abs/2402.03563)

    通过研究大型语言模型，在自由文本中识别作为代理的模型和冻结预训练模型的嵌入的小型线性探测器可以准确预测更大模型令牌级别上的自信度，进一步提出了一种无监督的方法在相同任务上达到了非平凡的准确度，这证明了语言模型中存在不同类型的不确定性表示。

    

    我们研究了在大型语言模型（LLMs）生成的自由文本输出中，是否可以鉴别出认知不确定性（反映缺乏知识的不确定性）和偶然不确定性（反映基础分布中的熵）。在没有真实概率的情况下，我们探索了一个设置，在这个设置中，为了（近似地）分解给定LLM的不确定性，一个明显更大的模型充当地面真相的代理。我们表明，基于冻结预训练模型的嵌入的小型线性探测器能够准确预测在令牌级别上更大模型将更自信的情况，并且在一个文本领域上训练的探测器可以泛化到其他领域。进一步地，我们提出了一种完全无监督的方法，在相同任务上达到了非平凡的准确度。综合考虑这些结果，我们解释这些结果作为LLMs内部自然地包含了不同类型不确定性的表示，这可能有助于制定更有效的方法。

    We study the feasibility of identifying epistemic uncertainty (reflecting a lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in the underlying distribution), in the outputs of large language models (LLMs) over free-form text. In the absence of ground-truth probabilities, we explore a setting where, in order to (approximately) disentangle a given LLM's uncertainty, a significantly larger model stands in as a proxy for the ground truth. We show that small linear probes trained on the embeddings of frozen, pretrained models accurately predict when larger models will be more confident at the token level and that probes trained on one text domain generalize to others. Going further, we propose a fully unsupervised method that achieves non-trivial accuracy on the same task. Taken together, we interpret these results as evidence that LLMs naturally contain internal representations of different types of uncertainty that could potentially be leveraged to devise more i
    
[^198]: GUARD: 通过角色扮演生成自然语言越狱来测试大型语言模型遵循指南的合规性

    GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models

    [https://arxiv.org/abs/2402.03299](https://arxiv.org/abs/2402.03299)

    本论文提出了一个通过角色扮演的系统，可以生成自然语言越狱，用于测试大型语言模型的指南遵循情况。系统通过收集现有越狱并将其组织成知识图来生成新的越狱，证明了其高效性和有效性。

    

    发现绕过大型语言模型（LLM）的安全过滤和有害回应的"越狱"已经鼓励社区采取安全措施。其中一个主要的安全措施是在发布之前用越狱主动测试LLM。因此，这样的测试将需要一种能够大规模且高效地生成越狱的方法。本文在追随一种新颖而直观的策略下，以人类生成的方式来生成越狱。我们提出了一个角色扮演系统，将四种不同角色分配给用户LLM，以便协作生成新的越狱。此外，我们收集现有的越狱，并通过句子逐句进行聚类频率和语义模式的划分，将它们分成不同的独立特征。我们将这些特征组织成一个知识图，使其更易于访问和检索。我们的角色系统将利用这个知识图来生成新的越狱，证明了其有效性。

    The discovery of "jailbreaks" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effec
    
[^199]: 杀手级应用：低速大规模AI武器

    Killer Apps: Low-Speed, Large-Scale AI Weapons

    [https://arxiv.org/abs/2402.01663](https://arxiv.org/abs/2402.01663)

    本文研究了AI武器的概念、部署、检测和潜在对策，强调了在信息领域内基于AI的心理操纵的潜力，以及其对全球个人、组织和社会的威胁。

    

    人工智能（AI）和机器学习（ML）的不断进步，特别是由OpenAI、Meta和Anthropic等组织开发的尖端生成式预训练转换器（GPT）模型的发展，给战争和安全带来了新的挑战和机会。目前关注的主要是AI在武器系统中的整合以及在动能冲突中快速决策中的作用。然而，同样重要但经常被忽视的一个方面是在信息领域中基于AI的心理操纵在互联网规模内的潜力。这些能力可能对全球个人、组织和社会造成重大威胁。本文探讨了AI武器的概念、部署、检测和潜在对策。

    The accelerating advancements in Artificial Intelligence (AI) and Machine Learning (ML), highlighted by the development of cutting-edge Generative Pre-trained Transformer (GPT) models by organizations such as OpenAI, Meta, and Anthropic, present new challenges and opportunities in warfare and security. Much of the current focus is on AI's integration within weapons systems and its role in rapid decision-making in kinetic conflict. However, an equally important but often overlooked aspect is the potential of AI-based psychological manipulation at internet scales within the information domain. These capabilities could pose significant threats to individuals, organizations, and societies globally. This paper explores the concept of AI weapons, their deployment, detection, and potential countermeasures.
    
[^200]: 基于得分的因果表示学习：线性和一般的转化

    Score-based Causal Representation Learning: Linear and General Transformations

    [https://arxiv.org/abs/2402.00849](https://arxiv.org/abs/2402.00849)

    这篇论文提出了一种基于得分的算法类，用于干预范围内的因果表示学习，涵盖了线性和一般转化。算法保证了可识别性和实现性，并且通过创造性地将得分函数与因果表示学习相结合。

    

    本篇论文针对一般非参数潜在因果模型和将潜在变量映射到观测变量的未知转化，研究了基于干预的因果表示学习（CRL）。研究了线性和一般的转化。这篇论文同时讨论了可识别性和实现性两个方面。可识别性是指确定算法不相关的条件，以确保恢复真实的潜在因果变量和潜在因果图。实现性是指算法方面，解决设计算法来实现可识别保证的问题。通过将得分函数（即密度函数对数的梯度）与CRL之间建立新联系，本文设计了一种得分为基础的算法类，确保了可识别性和实现性。首先，本文专注于线性转化，并展示了每个n个随机硬干预下该转化的因果表示可识别。

    This paper addresses intervention-based causal representation learning (CRL) under a general nonparametric latent causal model and an unknown transformation that maps the latent variables to the observed variables. Linear and general transformations are investigated. The paper addresses both the \emph{identifiability} and \emph{achievability} aspects. Identifiability refers to determining algorithm-agnostic conditions that ensure recovering the true latent causal variables and the latent causal graph underlying them. Achievability refers to the algorithmic aspects and addresses designing algorithms that achieve identifiability guarantees. By drawing novel connections between \emph{score functions} (i.e., the gradients of the logarithm of density functions) and CRL, this paper designs a \emph{score-based class of algorithms} that ensures both identifiability and achievability. First, the paper focuses on \emph{linear} transformations and shows that one stochastic hard intervention per n
    
[^201]: 神经语言模型解剖学

    Anatomy of Neural Language Models

    [https://arxiv.org/abs/2401.03797](https://arxiv.org/abs/2401.03797)

    本教程详细、简化和清晰地解释了神经语言模型，并提供清晰的图形说明，填补了缺乏统一数学框架的现存问题。

    

    生成式人工智能和迁移学习领域在最近几年取得了显著进展，特别是在自然语言处理（NLP）领域。变压器已经成为这些进展的核心，基于最前沿的变压器的语言模型（LMs）在广泛的应用中导致了新的最先进的结果。尽管涉及神经LMs的研究作品数量呈指数增长，但其中绝大多数是高级的，离实际操作颇远。因此，在缺乏解释主要类型神经LMs的统一数学框架的前提下，对这一领域的文献深入了解是一项艰巨的任务。我们在本教程中解决了上述问题，旨在在详细、简化和清晰的数学框架中解释神经LMs，并附有清晰的图形说明。

    arXiv:2401.03797v2 Announce Type: replace  Abstract: The fields of generative AI and transfer learning have experienced remarkable advancements in recent years especially in the domain of Natural Language Processing (NLP). Transformers have been at the heart of these advancements where the cutting-edge transformer-based Language Models (LMs) have led to new state-of-the-art results in a wide spectrum of applications. While the number of research works involving neural LMs is exponentially increasing, their vast majority are high-level and far from self-contained. Consequently, a deep understanding of the literature in this area is a tough task especially in the absence of a unified mathematical framework explaining the main types of neural LMs. We address the aforementioned problem in this tutorial where the objective is to explain neural LMs in a detailed, simplified and unambiguous mathematical framework accompanied by clear graphical illustrations. Concrete examples on widely used m
    
[^202]: 用于更快的LLM推理的级联推测草图

    Cascade Speculative Drafting for Even Faster LLM Inference

    [https://arxiv.org/abs/2312.11462](https://arxiv.org/abs/2312.11462)

    引入了Cascade Speculative Drafting（CS Drafting）算法，通过垂直级联消除神经模型的自回归生成，通过水平级联优化草稿中的时间分配，从而进一步提高LLM推理效率。

    

    引入了增强大型语言模型（LLM）推理效率的级联推测草图，通过较小的模型生成草稿来运作。较大的目标模型然后查看这个草稿以与其输出对齐，目标模型的任何接受都将减少目标模型运行的数量，从而提高效率。然而，在级联推测的草图过程中包括缓慢的自回归生成，并为生成的标记分配相同的时间，而不考虑它们的重要性。这些低效性共同导致级联推测的性能不佳。为了进一步改善LLM推理，我们引入了级联推测草图（CS Drafting），这是一种整合了两种级联类型的推测执行算法。垂直级联从神经模型中消除自回归生成，而水平级联优化了草稿中的时间分配

    arXiv:2312.11462v3 Announce Type: replace-cross  Abstract: Introduced to enhance the efficiency of large language model (LLM) inference, speculative decoding operates by having a smaller model generate a draft. A larger target model then reviews this draft to align with its output, and any acceptance by the target model results in a reduction of the number of the target model runs, ultimately improving efficiency. However, the drafting process in speculative decoding includes slow autoregressive generation and allocates equal time to generating tokens, irrespective of their importance. These inefficiencies collectively contribute to the suboptimal performance of speculative decoding. To further improve LLM inference, we introduce Cascade Speculative Drafting (CS Drafting), a speculative execution algorithm that incorporates two types of cascades. The Vertical Cascade eliminates autoregressive generation from neural models, while the Horizontal Cascade optimizes time allocation in draft
    
[^203]: 未观察到的混杂下的因果公平性：一种神经敏感性框架

    Causal Fairness under Unobserved Confounding: A Neural Sensitivity Framework

    [https://arxiv.org/abs/2311.18460](https://arxiv.org/abs/2311.18460)

    分析了因果公平性对未观察到混杂的敏感性，推导出因果公平性指标的界限，提出神经框架用于学习公平预测，展示了框架的有效性

    

    机器学习预测中的公平性由于法律、道德和社会原因在实践中被广泛要求。现有工作通常集中在没有未观察到的混杂的设置上，尽管未观察到的混杂可能导致严重违反因果公平性，从而产生不公平的预测。在这项工作中，我们分析了因果公平性对未观察到的混杂的敏感性。我们的贡献有三个方面。首先，我们推导出不同来源的未观察到混杂下因果公平性指标的界限。这使从业者能够检查其机器学习模型对在公平关键应用中的未观察到的混杂的敏感性。其次，我们提出了一种用于学习公平预测的新型神经框架，这使我们能够提供对因果公平性可能由于未观察到的混杂而受到违反的程度的最坏情况保证。第三，我们展示了我们框架的有效性。

    arXiv:2311.18460v2 Announce Type: replace-cross  Abstract: Fairness for machine learning predictions is widely required in practice for legal, ethical, and societal reasons. Existing work typically focuses on settings without unobserved confounding, even though unobserved confounding can lead to severe violations of causal fairness and, thus, unfair predictions. In this work, we analyze the sensitivity of causal fairness to unobserved confounding. Our contributions are three-fold. First, we derive bounds for causal fairness metrics under different sources of unobserved confounding. This enables practitioners to examine the sensitivity of their machine learning models to unobserved confounding in fairness-critical applications. Second, we propose a novel neural framework for learning fair predictions, which allows us to offer worst-case guarantees of the extent to which causal fairness can be violated due to unobserved confounding. Third, we demonstrate the effectiveness of our framewor
    
[^204]: TEA: 测试时间能量适应

    TEA: Test-time Energy Adaptation

    [https://arxiv.org/abs/2311.14402](https://arxiv.org/abs/2311.14402)

    提出了一种基于能量的测试时间适应（TEA）方法，通过转换训练后的分类器为基于能量的模型，增强模型对目标数据分布的感知，从而解决协变量转移问题。

    

    测试时间适应（TTA）旨在在测试数据偏离训练分布时提高模型的泛化能力，其独特优势在于不需要访问训练数据和处理过程，尤其在大型预训练模型的背景下尤为重要。然而，当前的TTA方法未能解决根本问题：协变量转移，即降低的泛化能力可以归因于模型依赖训练数据的边际分布，这可能影响模型校准并引入确认偏见。为了解决这一问题，我们提出了一种新颖的基于能量的视角，增强模型对目标数据分布的感知，而无需访问训练数据或处理过程。基于这一视角，我们引入了 $\textbf{T}$est-time $\textbf{E}$nergy $\textbf{A}$daptation（$\textbf{TEA}$），将经过训练的分类器转换为基于能量的模型，并使模型的分类边缘对齐。

    arXiv:2311.14402v2 Announce Type: replace  Abstract: Test-time adaptation (TTA) aims to improve model generalizability when test data diverges from training distribution, offering the distinct advantage of not requiring access to training data and processes, especially valuable in the context of large pre-trained models. However, current TTA methods fail to address the fundamental issue: covariate shift, i.e., the decreased generalizability can be attributed to the model's reliance on the marginal distribution of the training data, which may impair model calibration and introduce confirmation bias. To address this, we propose a novel energy-based perspective, enhancing the model's perception of target data distributions without requiring access to training data or processes. Building on this perspective, we introduce $\textbf{T}$est-time $\textbf{E}$nergy $\textbf{A}$daptation ($\textbf{TEA}$), which transforms the trained classifier into an energy-based model and aligns the model's di
    
[^205]: 使用可屏蔽股票表示的强化学习在可定制股票池中进行投资组合管理

    Reinforcement Learning with Maskable Stock Representation for Portfolio Management in Customizable Stock Pools

    [https://arxiv.org/abs/2311.10801](https://arxiv.org/abs/2311.10801)

    使用EarnMore方法，我们提出了一种新的RL方法，可以允许RL代理与可定制股票池（CSPs）交互，而不需要重新训练。

    

    投资组合管理（PM）是一项基本的金融交易任务，探索定期将资金重新配置到不同股票中以追求长期利润。最近，强化学习（RL）显示出其潜力，通过与金融市场互动来训练具有盈利能力的PM代理。但是，现有工作主要集中在固定股票池上，这与投资者的实际需求不一致。为应对这一挑战，我们提出EarnMore，一种新的RL方法，可以允许RL代理与可定制股票池（CSPs）交互，而不需要重新训练。

    arXiv:2311.10801v3 Announce Type: replace-cross  Abstract: Portfolio management (PM) is a fundamental financial trading task, which explores the optimal periodical reallocation of capitals into different stocks to pursue long-term profits. Reinforcement learning (RL) has recently shown its potential to train profitable agents for PM through interacting with financial markets. However, existing work mostly focuses on fixed stock pools, which is inconsistent with investors' practical demand. Specifically, the target stock pool of different investors varies dramatically due to their discrepancy on market states and individual investors may temporally adjust stocks they desire to trade (e.g., adding one popular stocks), which lead to customizable stock pools (CSPs). Existing RL methods require to retrain RL agents even with a tiny change of the stock pool, which leads to high computational cost and unstable performance. To tackle this challenge, we propose EarnMore, a rEinforcement leARNin
    
[^206]: 用机器学习进行卫星降水插值的不确定性估计

    Uncertainty estimation in satellite precipitation interpolation with machine learning

    [https://arxiv.org/abs/2311.07511](https://arxiv.org/abs/2311.07511)

    该研究使用机器学习算法对卫星和测站数据进行插值，通过量化预测不确定性来提高降水数据集的分辨率。

    

    合并卫星和测站数据并利用机器学习产生高分辨率降水数据集，但预测不确定性估计往往缺失。我们通过对比六种算法，大部分是针对这一任务而设计的新算法，来量化空间插值中的预测不确定性。在连续美国的15年月度数据上，我们比较了分位数回归（QR）、分位数回归森林（QRF）、广义随机森林（GRF）、梯度提升机（GBM）、轻梯度提升机（LightGBM）和分位数回归神经网络（QRNN）。它们能够在九个分位水平（0.025、0.050、0.100、0.250、0.500、0.750、0.900、0.950、0.975）上发布预测降水分位数，以近似完整概率分布，评估时采用分位数评分函数和分位数评分规则。特征重要性分析揭示了卫星降水（PERSIA

    arXiv:2311.07511v2 Announce Type: replace-cross  Abstract: Merging satellite and gauge data with machine learning produces high-resolution precipitation datasets, but uncertainty estimates are often missing. We address this gap by benchmarking six algorithms, mostly novel for this task, for quantifying predictive uncertainty in spatial interpolation. On 15 years of monthly data over the contiguous United States (CONUS), we compared quantile regression (QR), quantile regression forests (QRF), generalized random forests (GRF), gradient boosting machines (GBM), light gradient boosting machines (LightGBM), and quantile regression neural networks (QRNN). Their ability to issue predictive precipitation quantiles at nine quantile levels (0.025, 0.050, 0.100, 0.250, 0.500, 0.750, 0.900, 0.950, 0.975), approximating the full probability distribution, was evaluated using quantile scoring functions and the quantile scoring rule. Feature importance analysis revealed satellite precipitation (PERSIA
    
[^207]: 金字塔形隐藏马尔可夫模型用于多元时间序列预测

    Pyramidal Hidden Markov Model For Multivariate Time Series Forecasting

    [https://arxiv.org/abs/2310.14341](https://arxiv.org/abs/2310.14341)

    本论文提出了一种金字塔形隐藏马尔可夫模型（PHMM），能够捕捉多个多步随机状态，并提出一种新颖的时间序列预测结构，在性能上具有重要影响。

    

    隐藏马尔可夫模型（HMM）可以根据当前和先前值预测时间序列的未来值，使其成为处理各种类型时间序列的强大算法。许多研究探索了利用先进技术改进HMM的方法，导致了几种HMM的变体的发展。尽管这些研究表明HMM与其他先进算法相比具有增强的竞争力，但很少有人意识到将多步随机状态纳入其性能的重要性和影响。在这项工作中，我们提出了一个能够捕捉多个多步随机状态的金字塔形隐藏马尔可夫模型（PHMM）。最初，设计了一个多步HMM用于提取短多步随机状态。接下来，基于PHMM提出了一种新颖的时间序列预测结构，利用类似金字塔堆叠的方式自适应地识别长多步随机状态。通过采用

    arXiv:2310.14341v2 Announce Type: replace  Abstract: The Hidden Markov Model (HMM) can predict the future value of a time series based on its current and previous values, making it a powerful algorithm for handling various types of time series. Numerous studies have explored the improvement of HMM using advanced techniques, leading to the development of several variations of HMM. Despite these studies indicating the increased competitiveness of HMM compared to other advanced algorithms, few have recognized the significance and impact of incorporating multistep stochastic states into its performance. In this work, we propose a Pyramidal Hidden Markov Model (PHMM) that can capture multiple multistep stochastic states. Initially, a multistep HMM is designed for extracting short multistep stochastic states. Next, a novel time series forecasting structure is proposed based on PHMM, which utilizes pyramid-like stacking to adaptively identify long multistep stochastic states. By employing the
    
[^208]: 医疗保健中的多模态联邦学习：综述

    Multimodal Federated Learning in Healthcare: a Review

    [https://arxiv.org/abs/2310.09650](https://arxiv.org/abs/2310.09650)

    医疗保健领域引入了多模态联邦学习，结合最新的机器学习进展，确保了患者数据隐私和安全，为优化医疗AI系统提供了新的可能性。

    

    多模态机器学习的最新进展赋予了医学领域准确而稳健的AI系统的发展，尤其是在中心化数据库系统内。同时，联邦学习（FL）也得到了进展，提供了一种数据无需整合的去中心化机制，增强了对敏感医疗数据的隐私和安全性。这两个概念的整合支持了医疗保健中多模态学习的持续进展，同时确保了患者记录在本地数据持有机构内的安全和隐私。本文简要概述了FL在医疗保健中的重要性，并概述了医疗领域内多模态联邦学习（MMFL）的最新技术方法。文章全面审视了该领域中存在的挑战，揭示了现有模型的局限性。

    arXiv:2310.09650v2 Announce Type: replace-cross  Abstract: Recent advancements in multimodal machine learning have empowered the development of accurate and robust AI systems in the medical domain, especially within centralized database systems. Simultaneously, Federated Learning (FL) has progressed, providing a decentralized mechanism where data need not be consolidated, thereby enhancing the privacy and security of sensitive healthcare data. The integration of these two concepts supports the ongoing progress of multimodal learning in healthcare while ensuring the security and privacy of patient records within local data-holding agencies. This paper offers a concise overview of the significance of FL in healthcare and outlines the current state-of-the-art approaches to Multimodal Federated Learning (MMFL) within the healthcare domain. It comprehensively examines the existing challenges in the field, shedding light on the limitations of present models. Finally, the paper outlines poten
    
[^209]: 让循环的询问: 大型语言模型在判断中的摇摆

    Ask Again, Then Fail: Large Language Models' Vacillations in Judgement

    [https://arxiv.org/abs/2310.02174](https://arxiv.org/abs/2310.02174)

    目前的语言模型在面对后续问题时常常摇摆不定，研究者提出了一个后续问题机制和两个度量标准来量化这种不一致性，并开发出Unwavering-FQ框架来教导模型保持最初的正确判断，实验证明其有效性。

    

    我们观察到目前的会话式语言模型在面对后续问题时往往在其判断上摇摆不定，即使原始判断是正确的。这种摇摆对于生成可靠回复和建立用户信任构成了重要挑战。为了全面评估这一问题，我们引入了一个后续问题机制以及两个度量标准来量化这种不一致性，确认了当前语言模型普遍存在这种情况。为了缓解这一问题，我们探讨了各种提示策略用于闭源模型；此外，我们开发了一个基于训练的框架Unwavering-FQ，通过合成高质量的偏好数据来教导语言模型保持其最初的正确判断。我们的实验结果验证了我们框架的有效性以及其增强模型通用能力的能力。

    arXiv:2310.02174v2 Announce Type: replace-cross  Abstract: We observe that current conversational language models often waver in their judgements when faced with follow-up questions, even if the original judgement was correct. This wavering presents a significant challenge for generating reliable responses and building user trust. To comprehensively assess this issue, we introduce a Follow-up Questioning Mechanism along with two metrics to quantify this inconsistency, confirming its widespread presence in current language models. To mitigate this issue, we explore various prompting strategies for closed-source models; moreover, we develop a training-based framework Unwavering-FQ that teaches language models to maintain their originally correct judgements through synthesized high-quality preference data. Our experimental results confirm the effectiveness of our framework and its ability to enhance the general capabilities of models (https://github.com/NUSTM/LLMs-Waver-In-Judgements).
    
[^210]: 关注Atlas：Atlas引导的用于鲁棒3D医学图像分割的测试时适应方法

    Pay Attention to the Atlas: Atlas-Guided Test-Time Adaptation Method for Robust 3D Medical Image Segmentation

    [https://arxiv.org/abs/2307.00676](https://arxiv.org/abs/2307.00676)

    提出了一种Atlas引导的测试时适应方法，用于解决医学图像分割中的数据分布不同问题

    

    卷积神经网络（CNNs）在测试目标数据与训练（源）数据分布不同时往往表现不佳，尤其在医学成像应用中，不同临床站点和扫描仪之间的成像协议变化导致不同的成像外观。为了解决这个问题，我们提出了一种新的Atlas引导的测试时适应（TTA）方法，用于鲁棒的3D医学图像分割，称为AdaAtlas。AdaAtlas仅需要一个未标记的单个测试样本作为输入，并通过最小化基于Atlas的损失来调整分割网络。

    arXiv:2307.00676v2 Announce Type: replace-cross  Abstract: Convolutional neural networks (CNNs) often suffer from poor performance when tested on target data that differs from the training (source) data distribution, particularly in medical imaging applications where variations in imaging protocols across different clinical sites and scanners lead to different imaging appearances. However, re-accessing source training data for unsupervised domain adaptation or labeling additional test data for model fine-tuning can be difficult due to privacy issues and high labeling costs, respectively. To solve this problem, we propose a novel atlas-guided test-time adaptation (TTA) method for robust 3D medical image segmentation, called AdaAtlas. AdaAtlas only takes one single unlabeled test sample as input and adapts the segmentation network by minimizing an atlas-based loss. Specifically, the network is adapted so that its prediction after registration is aligned with the learned atlas in the atla
    
[^211]: 高效Contextformer：用于学习图像压缩中快速上下文建模的时空通道窗口注意力

    Efficient Contextformer: Spatio-Channel Window Attention for Fast Context Modeling in Learned Image Compression

    [https://arxiv.org/abs/2306.14287](https://arxiv.org/abs/2306.14287)

    提出了高效Contextformer，采用了时空通道窗口注意力机制，用于学习图像压缩中快速上下文建模，并引入了优化技术来降低计算成本

    

    熵估计对于学习图像压缩的性能至关重要。已经证明，基于Transformer的熵模型对于实现高压缩比至关重要，但是需要付出巨大的计算代价。在本研究中，我们引入了高效Contextformer（eContextformer）- 一种用于学习图像压缩的计算效率高的基于Transformer的自回归上下文模型。eContextformer有效地融合了基于块、棋盘格和通道的分组技术，用于并行上下文建模，并引入了一种移位窗口时空通道注意力机制。我们探索了更好的训练策略和架构设计，并引入了额外的复杂性优化。在解码过程中，所提出的优化技术动态调整注意力范围并缓存先前的注意力计算，极大地减少了模型的计算成本。

    arXiv:2306.14287v2 Announce Type: replace-cross  Abstract: Entropy estimation is essential for the performance of learned image compression. It has been demonstrated that a transformer-based entropy model is of critical importance for achieving a high compression ratio, however, at the expense of a significant computational effort. In this work, we introduce the Efficient Contextformer (eContextformer) - a computationally efficient transformer-based autoregressive context model for learned image compression. The eContextformer efficiently fuses the patch-wise, checkered, and channel-wise grouping techniques for parallel context modeling, and introduces a shifted window spatio-channel attention mechanism. We explore better training strategies and architectural designs and introduce additional complexity optimizations. During decoding, the proposed optimization techniques dynamically scale the attention span and cache the previous attention computations, drastically reducing the model an
    
[^212]: 遮罩数据增强用于监督学习

    Masking Augmentation for Supervised Learning

    [https://arxiv.org/abs/2306.11339](https://arxiv.org/abs/2306.11339)

    提出了一种名为MaskSub的新方法，通过使用遮罩子模型和放松的损失函数来强化监督学习中的遮罩增强，提高了性能并加速训练过程。

    

    使用随机遮罩进行预训练已经成为训练技术中的新趋势。然而，监督学习在采用遮罩增强方面面临挑战，主要是由于不稳定的训练。本文提出了一种涉及遮罩增强的新方法，称为Masked Sub-model (MaskSub)。MaskSub由主模型和子模型组成；前者享受传统训练方法，而后者利用强大的遮罩增强来训练。MaskSub通过缓解类似于自蒸馏损失的放松损失函数来解决挑战。我们的分析表明，MaskSub提高了性能，训练损失的收敛速度甚至比常规训练更快，这表明我们的方法有助于训练。我们进一步验证了MaskSub在各种训练方法和模型上的有效性，包括DeiT-III，MAE微调，CLIP微调，ResNet和Swin T。

    arXiv:2306.11339v2 Announce Type: replace-cross  Abstract: Pre-training using random masking has emerged as a novel trend in training techniques. However, supervised learning faces a challenge in adopting masking augmentations, primarily due to unstable training. In this paper, we propose a novel way to involve masking augmentations dubbed Masked Sub-model (MaskSub). MaskSub consists of the main-model and sub-model; while the former enjoys conventional training recipes, the latter leverages the benefit of strong masking augmentations in training. MaskSub addresses the challenge by mitigating adverse effects through a relaxed loss function similar to a self-distillation loss. Our analysis shows that MaskSub improves performance, with the training loss converging even faster than regular training, which suggests our method facilitates training. We further validate MaskSub across diverse training recipes and models, including DeiT-III, MAE fine-tuning, CLIP fine-tuning, ResNet, and Swin T
    
[^213]: OCAtari: 以对象为中心的Atari 2600强化学习环境

    OCAtari: Object-Centric Atari 2600 Reinforcement Learning Environments

    [https://arxiv.org/abs/2306.08649](https://arxiv.org/abs/2306.08649)

    以对象为中心的Atari 2600强化学习环境OCAtari扩展了Atari Learning Environments框架，实现了对游戏中基于对象的状态进行资源高效提取，并允许对象发现、对象表征学习以及对象为中心的强化学习。

    

    认知科学和心理学表明，复杂场景的以对象为中心的表征是实现从低级感知特征有效抽象推理的一个有希望的步骤。然而，大多数深度强化学习方法只依赖于像素级表示，无法捕捉自然场景的组合特性。因此，我们需要允许我们处理和评估以对象为中心方法的环境和数据集。在我们的工作中，我们通过引入OCAtari来扩展Atari学习环境，这是深度RL方法最常用的评估框架，OCAtari对这些游戏进行了资源高效的对象中心状态提取。我们的框架允许对象发现、对象表征学习以及对象为中心的RL。我们评估了OCAtari的检测能力和资源效率。我们的源代码可在github.com/k4ntz/OC_Atari上找到。

    arXiv:2306.08649v2 Announce Type: replace-cross  Abstract: Cognitive science and psychology suggest that object-centric representations of complex scenes are a promising step towards enabling efficient abstract reasoning from low-level perceptual features. Yet, most deep reinforcement learning approaches only rely on pixel-based representations that do not capture the compositional properties of natural scenes. For this, we need environments and datasets that allow us to work and evaluate object-centric approaches. In our work, we extend the Atari Learning Environments, the most-used evaluation framework for deep RL approaches, by introducing OCAtari, that performs resource-efficient extractions of the object-centric states for these games. Our framework allows for object discovery, object representation learning, as well as object-centric RL. We evaluate OCAtari's detection capabilities and resource efficiency. Our source code is available at github.com/k4ntz/OC_Atari.
    
[^214]: 探索实时递归学习的潜力与限制

    Exploring the Promise and Limits of Real-Time Recurrent Learning

    [https://arxiv.org/abs/2305.19044](https://arxiv.org/abs/2305.19044)

    实时递归学习（RTRL）具有一定概念优势，不需要缓存过去的激活状态和截断上下文，支持在线学习，在演员-评论家方法中探索了其实际潜力，并在DMLab-30、ProcGen和Atari-2600环境中进行了测试，在DMLab存储任务中表现出与优于IMPALA和R2D2基线相媲美的竞争力，为了应对复杂任务，研究重点放在了某些方面

    

    实时递归学习（RTRL）用于序列处理的递归神经网络（RNN）相比于时间反向传播（BPTT）具有一定的概念优势。RTRL既不需要缓存过去的激活状态，也不需要截断上下文，而且支持在线学习。然而，RTRL的时间和空间复杂度使其实际应用困难。为了克服这个问题，最近关于RTRL的工作主要集中在近似理论上，而实验通常局限于诊断设置。在这里，我们在更现实的环境中探索了RTRL的实际潜力。我们研究了结合了RTRL和策略梯度的演员-评论家方法，并在DMLab-30、ProcGen和Atari-2600环境的几个子集中进行了测试。在DMLab存储任务中，我们的系统在少于1.2 B的环境帧上训练，与或优于在10 B帧上训练的著名IMPALA和R2D2基线。为了扩展到这些具有挑战性的任务，我们专注于某些方面

    arXiv:2305.19044v2 Announce Type: replace  Abstract: Real-time recurrent learning (RTRL) for sequence-processing recurrent neural networks (RNNs) offers certain conceptual advantages over backpropagation through time (BPTT). RTRL requires neither caching past activations nor truncating context, and enables online learning. However, RTRL's time and space complexity make it impractical. To overcome this problem, most recent work on RTRL focuses on approximation theories, while experiments are often limited to diagnostic settings. Here we explore the practical promise of RTRL in more realistic settings. We study actor-critic methods that combine RTRL and policy gradients, and test them in several subsets of DMLab-30, ProcGen, and Atari-2600 environments. On DMLab memory tasks, our system trained on fewer than 1.2 B environmental frames is competitive with or outperforms well-known IMPALA and R2D2 baselines trained on 10 B frames. To scale to such challenging tasks, we focus on certain wel
    
[^215]: 深度增强：在激活空间中使用自监督学习进行数据增强

    Deep Augmentation: Self-Supervised Learning with Transformations in Activation Space

    [https://arxiv.org/abs/2303.14537](https://arxiv.org/abs/2303.14537)

    深度增强是一种利用dropout或PCA在神经网络中转换目标层的方法，有效改善性能和泛化能力。在对比学习任务中，在Transformers、ResNets和图神经网络等基础模型上，通过深度增强实现了显著的性能提升，但在监督问题上效果相反。

    

    我们提出了一种称为深度增强的方法，通过使用辍学或PCA来转换神经网络中的目标层，以提高性能和泛化能力。我们通过在自然语言处理、计算机视觉和图学习中的对比学习任务上进行大量实验来展示深度增强。 我们观察到在对比学习的基础模型中，如Transformers、ResNets和图神经网络上深度增强能够带来显著的性能提升，但在相应的监督问题上观察到相反的效果。 我们的分析表明，深度增强减轻了层之间的相互适应，即"崩溃"形式的问题。 我们利用这一观察结果制定了一种选择目标层的方法；特别是，我们的实验表明，用深度增强定位更深层次的层要优于增强输入数据。 这种方法的简单网络和模态无关性使其

    arXiv:2303.14537v2 Announce Type: replace-cross  Abstract: We introduce Deep Augmentation, an approach to implicit data augmentation using dropout or PCA to transform a targeted layer within a neural network to improve performance and generalization. We demonstrate Deep Augmentation through extensive experiments on contrastive learning tasks in NLP, computer vision, and graph learning. We observe substantial performance gains with Transformers, ResNets, and Graph Neural Networks as the underlying models in contrastive learning, but observe inverse effects on the corresponding supervised problems. Our analysis suggests that Deep Augmentation alleviates co-adaption between layers, a form of "collapse." We use this observation to formulate a method for selecting which layer to target; in particular, our experimentation reveals that targeting deeper layers with Deep Augmentation outperforms augmenting the input data. The simple network- and modality-agnostic nature of this approach enables
    
[^216]: 具有本地和全局下层问题的通信高效联邦双层优化

    Communication-Efficient Federated Bilevel Optimization with Local and Global Lower Level Problems

    [https://arxiv.org/abs/2302.06701](https://arxiv.org/abs/2302.06701)

    本文研究了联邦学习设置中的双层优化问题，并提出了一种通信高效的算法FedBiOAcc，实现了优秀的通信和样本复杂度，并且实现了与客户端数量线性加速。

    

    双层优化近年来取得了显著进展，新兴有效算法不断涌现。然而，在联邦学习环境中应用双层优化仍相对未被充分探索，以及联邦学习固有挑战对双层算法收敛的影响仍不清楚。本文研究了联邦双层优化问题，并提出了一种名为FedBiOAcc的通信高效算法。该算法利用分布式环境中对超梯度的高效估计，并利用基于动量的方差减少加速。值得注意的是，FedBiOAcc实现了通信复杂度$O(\epsilon^{-1})$，样本复杂度$O(\epsilon^{-1.5})$，并且随着客户端数量的增加而线性加速。我们还分析了联邦双层优化问题的一种特殊情况，即由客户端本地管理下层问题。

    arXiv:2302.06701v2 Announce Type: replace  Abstract: Bilevel Optimization has witnessed notable progress recently with new emerging efficient algorithms. However, its application in the Federated Learning setting remains relatively underexplored, and the impact of Federated Learning's inherent challenges on the convergence of bilevel algorithms remain obscure. In this work, we investigate Federated Bilevel Optimization problems and propose a communication-efficient algorithm, named FedBiOAcc. The algorithm leverages an efficient estimation of the hyper-gradient in the distributed setting and utilizes the momentum-based variance-reduction acceleration. Remarkably, FedBiOAcc achieves a communication complexity $O(\epsilon^{-1})$, a sample complexity $O(\epsilon^{-1.5})$ and the linear speed up with respect to the number of clients. We also analyze a special case of the Federated Bilevel Optimization problems, where lower level problems are locally managed by clients. We prove that FedBiO
    
[^217]: 噪音对量子神经网络过度参数化的影响

    Effects of noise on the overparametrization of quantum neural networks

    [https://arxiv.org/abs/2302.05059](https://arxiv.org/abs/2302.05059)

    噪音如何影响量子神经网络过度参数化的现象。

    

    过度参数化是机器学习中最令人惊讶和臭名昭著的现象之一。最近，有多次尝试研究在没有硬件噪音的情况下，量子神经网络（QNNs）是否以及如何被过度参数化。特别是，已经提出，如果QNN具有足够的参数来探索状态空间中的所有可用方向，则可以将其定义为过度参数化。也就是说，如果QNN的输出状态的量子费舍尔信息矩阵（QFIM）的秩得以饱和。在这里，我们探讨了噪音的存在如何影响过度参数化现象。我们的结果表明，噪音可以“打开”先前为零的QFIM特征值。这使得参数化状态可以探索原本无法访问的方向，从而潜在地将一个过度参数化的QNN转化为一个欠参数化的QNN。对于小噪音水平，QNN是准过度参数化的，因为存在大的特征值

    arXiv:2302.05059v2 Announce Type: replace-cross  Abstract: Overparametrization is one of the most surprising and notorious phenomena in machine learning. Recently, there have been several efforts to study if, and how, Quantum Neural Networks (QNNs) acting in the absence of hardware noise can be overparametrized. In particular, it has been proposed that a QNN can be defined as overparametrized if it has enough parameters to explore all available directions in state space. That is, if the rank of the Quantum Fisher Information Matrix (QFIM) for the QNN's output state is saturated. Here, we explore how the presence of noise affects the overparametrization phenomenon. Our results show that noise can "turn on" previously-zero eigenvalues of the QFIM. This enables the parametrized state to explore directions that were otherwise inaccessible, thus potentially turning an overparametrized QNN into an underparametrized one. For small noise levels, the QNN is quasi-overparametrized, as large eige
    
[^218]: 整合多模态数据用于复杂动力学的联合生成建模

    Integrating Multimodal Data for Joint Generative Modeling of Complex Dynamics

    [https://arxiv.org/abs/2212.07892](https://arxiv.org/abs/2212.07892)

    提出了一种基于多模态变分自动编码器的算法框架，用于联合生成建模复杂动力学，引导重建模型训练。

    

    许多科学中感兴趣的系统本质上是非线性动力系统。通常，我们通过时间序列测量来访问这些系统。这些时间序列可能由离散随机变量而非连续测量组成，或者可能由同时观察到的多个数据模态的测量组成。在神经科学中，我们可能除了脉冲计数和连续生理记录外，还有行为标签。虽然现在关于深度学习用于动态系统重建的文献正在蓬勃发展，但多模态数据集成在这个背景下几乎没有被考虑。在这里，我们提供了一个基于多模态变分自动编码器的高效灵活算法框架，用于生成稀疏教师信号，指导重建模型的训练，利用了DSR训练技术的最新进展。

    arXiv:2212.07892v2 Announce Type: replace  Abstract: Many, if not most, systems of interest in science are naturally described as nonlinear dynamical systems. Empirically, we commonly access these systems through time series measurements. Often such time series may consist of discrete random variables rather than continuous measurements, or may be composed of measurements from multiple data modalities observed simultaneously. For instance, in neuroscience we may have behavioral labels in addition to spike counts and continuous physiological recordings. While by now there is a burgeoning literature on deep learning for dynamical systems reconstruction (DSR), multimodal data integration has hardly been considered in this context. Here we provide such an efficient and flexible algorithmic framework that rests on a multimodal variational autoencoder for generating a sparse teacher signal that guides training of a reconstruction model, exploiting recent advances in DSR training techniques. 
    
[^219]: 具有基于流的语音转换的跨语言文本到语音方法以改善发音

    Cross-lingual Text-To-Speech with Flow-based Voice Conversion for Improved Pronunciation

    [https://arxiv.org/abs/2210.17264](https://arxiv.org/abs/2210.17264)

    通过基于流的语音转换实现了跨语言文本到语音，提升了发音质量，并在客观和主观评估中表现出优势。

    

    本文提出了一种用于端到端跨语言文本到语音（TTS）的方法，旨在保留目标语言的发音，而不考虑原始说话者的语言。所使用的模型基于非注意力Tacotron架构，其中解码器已经被替换为一个受讲话者身份条件的归一化流网络，允许通过相同模型执行TTS和声音转换（VC），因为固有的语言内容和说话者身份解耦。在跨语言设置中使用时，首先使用目标语言的本地发音者产生声学特征，然后通过相同模型应用声音转换，以将这些特征转换为目标说话者的声音。我们通过客观和主观评估验证，我们的方法与基准跨语言合成相比具有一定好处。

    arXiv:2210.17264v2 Announce Type: replace-cross  Abstract: This paper presents a method for end-to-end cross-lingual text-to-speech (TTS) which aims to preserve the target language's pronunciation regardless of the original speaker's language. The model used is based on a non-attentive Tacotron architecture, where the decoder has been replaced with a normalizing flow network conditioned on the speaker identity, allowing both TTS and voice conversion (VC) to be performed by the same model due to the inherent linguistic content and speaker identity disentanglement. When used in a cross-lingual setting, acoustic features are initially produced with a native speaker of the target language and then voice conversion is applied by the same model in order to convert these features to the target speaker's voice. We verify through objective and subjective evaluations that our method can have benefits compared to baseline cross-lingual synthesis. By including speakers averaging 7.5 minutes of spe
    
[^220]: 所有感受：带有大面积触觉传感的灵巧手

    All the Feels: A dexterous hand with large-area tactile sensing

    [https://arxiv.org/abs/2210.15658](https://arxiv.org/abs/2210.15658)

    该研究引入了一种廉价、模块化、稳健且可扩展的DManus平台，解决了机器人中灵巧手的高成本、可靠性问题，同时提供大规模数据采集，其带有覆盖整个手掌和指尖表面的ReSkin感应器。

    

    高昂的成本和缺乏可靠性妨碍了灵巧手在机器人技术中的广泛应用。此外，缺乏能够感知整个手区域的可靠触觉传感器阻碍了丰富的低级反馈，这将改善对灵巧操作技能的学习。本文介绍了一种廉价、模块化、稳健且可扩展的平台 - DManus，旨在解决这些挑战，同时满足深度机器人学习范式所需的大规模数据采集能力。对人类操作的研究指出，在执行日常灵巧任务时，低级触觉反馈的重要性。DManus配有ReSkin感应器，覆盖整个手掌和指尖的表面。我们展示了完全集成系统在触觉感知任务 - 箱式拾取和分类中的有效性。代码、文档、设计文件、详细组装说明文档，

    arXiv:2210.15658v3 Announce Type: replace-cross  Abstract: High cost and lack of reliability has precluded the widespread adoption of dexterous hands in robotics. Furthermore, the lack of a viable tactile sensor capable of sensing over the entire area of the hand impedes the rich, low-level feedback that would improve learning of dexterous manipulation skills. This paper introduces an inexpensive, modular, robust, and scalable platform -- the DManus -- aimed at resolving these challenges while satisfying the large-scale data collection capabilities demanded by deep robot learning paradigms. Studies on human manipulation point to the criticality of low-level tactile feedback in performing everyday dexterous tasks. The DManus comes with ReSkin sensing on the entire surface of the palm as well as the fingertips. We demonstrate effectiveness of the fully integrated system in a tactile aware task -- bin picking and sorting. Code, documentation, design files, detailed assembly instructions, 
    
[^221]: 电子健康记录数据中的树导向罕见特征选择和逻辑聚合

    Tree-Guided Rare Feature Selection and Logic Aggregation with Electronic Health Records Data

    [https://arxiv.org/abs/2206.09107](https://arxiv.org/abs/2206.09107)

    提出了一种用于大规模回归的树导向特征选择和逻辑聚合方法，旨在改善基于电子健康记录的建模和利用疾病分类的自然分层结构

    

    处理具有大量罕见二进制特征的统计学习在分析电子健康记录（EHR）数据时很常见，尤其是在利用先前的医学诊断和程序对疾病发生进行建模时。为了改善基于EHR的建模并利用疾病分类的自然分层结构，我们提出了一种用于大规模回归的树导向特征选择和逻辑聚合方法，通过稀疏追求和“或”逻辑运算符的聚合促进器实现了降维。

    arXiv:2206.09107v2 Announce Type: replace  Abstract: Statistical learning with a large number of rare binary features is commonly encountered in analyzing electronic health records (EHR) data, especially in the modeling of disease onset with prior medical diagnoses and procedures. Dealing with the resulting highly sparse and large-scale binary feature matrix is notoriously challenging as conventional methods may suffer from a lack of power in testing and inconsistency in model fitting while machine learning methods may suffer from the inability of producing interpretable results or clinically-meaningful risk factors. To improve EHR-based modeling and utilize the natural hierarchical structure of disease classification, we propose a tree-guided feature selection and logic aggregation approach for large-scale regression with rare binary features, in which dimension reduction is achieved through not only a sparsity pursuit but also an aggregation promoter with the logic operator of ``or''
    
[^222]: 具有子目标模型的目标空间规划

    Goal-Space Planning with Subgoal Models

    [https://arxiv.org/abs/2206.02902](https://arxiv.org/abs/2206.02902)

    通过在一组（抽象的）子目标上进行约束和学习本地、子目标条件的模型，本文提出的目标空间规划（GSP）方法更具计算效率，自然地结合了时间抽象，避免了学习转换动力学。

    

    本文研究了一种新的基于模型的强化学习方法，使用背景规划：混合（近似的）动态规划更新和模型无关的更新，类似于Dyna架构。利用学习的模型进行背景规划通常比模型无关的替代方法（如Double DQN）更差，尽管前者使用了显著更多的内存和计算资源。根本问题在于，学习的模型可能不准确，并且在迭代多个步骤时通常会产生无效状态。在本文中，我们通过将背景规划限制在一组（抽象的）子目标，并仅学习本地、子目标条件的模型来避免这种限制。这种目标空间规划（GSP）方法更具计算效率，自然地结合了用于更快长时程规划的时间抽象，并完全避免了学习转换动力学。我们展示了我们的GSP算法可以传播价值

    arXiv:2206.02902v5 Announce Type: replace-cross  Abstract: This paper investigates a new approach to model-based reinforcement learning using background planning: mixing (approximate) dynamic programming updates and model-free updates, similar to the Dyna architecture. Background planning with learned models is often worse than model-free alternatives, such as Double DQN, even though the former uses significantly more memory and computation. The fundamental problem is that learned models can be inaccurate and often generate invalid states, especially when iterated many steps. In this paper, we avoid this limitation by constraining background planning to a set of (abstract) subgoals and learning only local, subgoal-conditioned models. This goal-space planning (GSP) approach is more computationally efficient, naturally incorporates temporal abstraction for faster long-horizon planning and avoids learning the transition dynamics entirely. We show that our GSP algorithm can propagate value
    
[^223]: Snapture -- 一种用于静态和动态手势识别的新型神经结构

    Snapture -- A Novel Neural Architecture for Combined Static and Dynamic Hand Gesture Recognition

    [https://arxiv.org/abs/2205.15862](https://arxiv.org/abs/2205.15862)

    提出了一种新型混合手势识别系统，能够学习静态和动态手势，并通过捕捉手势表现高峰的“快照”来融合静态姿势和动态运动。

    

    随着机器人预期更多地参与人们的日常生活，需要能够提供直观用户界面的框架。手势识别系统提供了一种自然的交流方式，因此是无缝人机交互的重要组成部分。近年来，由深度学习驱动的计算模型发生了巨大的发展。然而，最先进的模型在跨不同手势领域（如象征手势和共语手势）方面仍存在不足。在本文中，我们提出了一种新颖的混合手势识别系统。我们的架构实现了对静态和动态手势的学习：通过捕捉手势在其高峰表现时的所谓“快照”，我们将手势姿势与动态运动融合在一起。此外，我们提出了一种分析手势运动轨迹的方法，以揭示其动态特征，并允许调节一个静态通道。

    arXiv:2205.15862v2 Announce Type: replace-cross  Abstract: As robots are expected to get more involved in people's everyday lives, frameworks that enable intuitive user interfaces are in demand. Hand gesture recognition systems provide a natural way of communication and, thus, are an integral part of seamless Human-Robot Interaction (HRI). Recent years have witnessed an immense evolution of computational models powered by deep learning. However, state-of-the-art models fall short in expanding across different gesture domains, such as emblems and co-speech. In this paper, we propose a novel hybrid hand gesture recognition system. Our architecture enables learning both static and dynamic gestures: by capturing a so-called "snapshot" of the gesture performance at its peak, we integrate the hand pose along with the dynamic movement. Moreover, we present a method for analyzing the motion profile of a gesture to uncover its dynamic characteristics and which allows regulating a static channel
    
[^224]: 一种强大的网络安全主题分类工具

    A Robust Cybersecurity Topic Classification Tool

    [https://arxiv.org/abs/2109.02473](https://arxiv.org/abs/2109.02473)

    通过使用多数投票的方式，研究提出了一种网络安全主题分类工具，相比于21个单独模型，该工具在检测网络安全相关文本时表现出较低的假阳性和假阴性率，并且能够处理数十万份文档。

    

    在这项研究中，我们使用来自三个互联网文本信息来源（Reddit，Stackexchange，Arxiv）的用户定义的标签，训练了21种不同的机器学习模型，用于检测自然文本中的网络安全讨论的主题分类任务。我们分析了每个模型在交叉验证实验中的假阳性和假阴性率。然后，我们提出了一种网络安全主题分类（CTC）工具，该工具将21个训练好的机器学习模型的多数投票作为检测网络安全相关文本的决策机制。我们还展示了CTC工具的多数投票机制平均提供了比21个单独模型更低的假阴性和假阳性率。我们展示了CTC工具可扩展到数十万份文档，而其墙钟时间大约为几小时。

    arXiv:2109.02473v4 Announce Type: replace-cross  Abstract: In this research, we use user defined labels from three internet text sources (Reddit, Stackexchange, Arxiv) to train 21 different machine learning models for the topic classification task of detecting cybersecurity discussions in natural text. We analyze the false positive and false negative rates of each of the 21 model's in a cross validation experiment. Then we present a Cybersecurity Topic Classification (CTC) tool, which takes the majority vote of the 21 trained machine learning models as the decision mechanism for detecting cybersecurity related text. We also show that the majority vote mechanism of the CTC tool provides lower false negative and false positive rates on average than any of the 21 individual models. We show that the CTC tool is scalable to the hundreds of thousands of documents with a wall clock time on the order of hours.
    
[^225]: OneLog: 面向软件日志异常检测的端到端训练

    OneLog: Towards End-to-End Training in Software Log Anomaly Detection

    [https://arxiv.org/abs/2104.07324](https://arxiv.org/abs/2104.07324)

    OneLog提出了一种利用单个深度神经网络进行端到端训练的方法，通过利用字符级别的卷积神经网络进行软件日志异常检测，在多个数据集上取得了最先进的性能。

    

    随着在线服务、物联网设备和面向DevOps的软件开发的增长，软件日志异常检测变得越来越重要。该论文提出了OneLog，它利用单个深度神经网络（DNN）代替多个独立组件。OneLog在字符级别利用卷积神经网络（CNN）考虑数字、数字和标点符号，并结合主要的自然语言文本，而这些在先前的工作中被移除。我们在六个基于消息和序列的数据集（HDFS、Hadoop、BGL、Thunderbird、Spirit和Liberty）中评估我们的方法。我们在单项目、多项目和跨项目设置下尝试了Onelog。Onelog在我们的数据集中提供了最先进的性能。Onelog可以在训练过程中同时利用多个项目数据集。

    arXiv:2104.07324v2 Announce Type: replace-cross  Abstract: With the growth of online services, IoT devices, and DevOps-oriented software development, software log anomaly detection is becoming increasingly important. Prior works mainly follow a traditional four-staged architecture (Preprocessor, Parser, Vectorizer, and Classifier). This paper proposes OneLog, which utilizes a single Deep Neural Network (DNN) instead of multiple separate components. OneLog harnesses Convolutional Neural Networks (CNN) at the character level to take digits, numbers, and punctuations, which were removed in prior works, into account alongside the main natural language text. We evaluate our approach in six message- and sequence-based data sets: HDFS, Hadoop, BGL, Thunderbird, Spirit, and Liberty. We experiment with Onelog with single-, multi-, and cross-project setups. Onelog offers state-of-the-art performance in our datasets. Onelog can utilize multi-project datasets simultaneously during training, which 
    
[^226]: DoubleML -- 双重机器学习的面向对象实现在R中

    DoubleML -- An Object-Oriented Implementation of Double Machine Learning in R

    [https://arxiv.org/abs/2103.09603](https://arxiv.org/abs/2103.09603)

    DoubleML是在R中实现的双重机器学习框架，提供了估计因果模型参数的功能，包括在部分线性和交互回归模型中进行推断。对象导向的实现使得模型规范具有很高的灵活性并易于扩展。

    

    该研究介绍了R包DoubleML实现了Chernozhukov等人（2018）提出的双重/无偏机器学习框架。它提供了基于机器学习方法估计因果模型参数的功能。双重机器学习框架包括三个关键要素：Neyman正交性、高质量的机器学习估计和样本拆分。可以通过mlr3生态系统中提供的各种最先进的机器学习方法来估计干扰成分。DoubleML使得可以在各种因果模型中进行推断，包括部分线性和交互回归模型以及它们对工具变量估计的扩展。DoubleML的面向对象实现使模型规范非常灵活且易于扩展。本文作为双重机器学习框架和R pac的介绍。

    arXiv:2103.09603v5 Announce Type: replace-cross  Abstract: The R package DoubleML implements the double/debiased machine learning framework of Chernozhukov et al. (2018). It provides functionalities to estimate parameters in causal models based on machine learning methods. The double machine learning framework consist of three key ingredients: Neyman orthogonality, high-quality machine learning estimation and sample splitting. Estimation of nuisance components can be performed by various state-of-the-art machine learning methods that are available in the mlr3 ecosystem. DoubleML makes it possible to perform inference in a variety of causal models, including partially linear and interactive regression models and their extensions to instrumental variable estimation. The object-oriented implementation of DoubleML enables a high flexibility for the model specification and makes it easily extendable. This paper serves as an introduction to the double machine learning framework and the R pac
    
[^227]: 自动化机器学习：从原理到实践

    Automated Machine Learning: From Principles to Practices

    [https://arxiv.org/abs/1810.13306](https://arxiv.org/abs/1810.13306)

    自动化机器学习（AutoML）旨在以数据驱动的方式生成令人满意的ML配置，本文提供了对AutoML原理和实践的全面调研。

    

    机器学习（ML）方法发展迅速，但配置和选择合适的方法以达到所需性能正变得越来越困难和乏味。为了解决这一挑战，自动化机器学习（AutoML）应运而生，旨在以数据驱动的方式为给定任务生成令人满意的ML配置。本文就该主题进行了全面调研。我们从AutoML的正式定义开始，然后介绍其原理，包括双层学习目标、学习策略和理论解释。接着，我们通过基于三个主要因素——搜索空间、搜索算法和评估策略——设立现有工作的分类法来总结AutoML实践。每个类别还会通过代表性方法进行解释。然后，我们通过配置ML管线等示例应用说明了AutoML的原理和实践。

    arXiv:1810.13306v5 Announce Type: replace  Abstract: Machine learning (ML) methods have been developing rapidly, but configuring and selecting proper methods to achieve a desired performance is increasingly difficult and tedious. To address this challenge, automated machine learning (AutoML) has emerged, which aims to generate satisfactory ML configurations for given tasks in a data-driven way. In this paper, we provide a comprehensive survey on this topic. We begin with the formal definition of AutoML and then introduce its principles, including the bi-level learning objective, the learning strategy, and the theoretical interpretation. Then, we summarize the AutoML practices by setting up the taxonomy of existing works based on three main factors: the search space, the search algorithm, and the evaluation strategy. Each category is also explained with the representative methods. Then, we illustrate the principles and practices with exemplary applications from configuring ML pipeline, 
    
[^228]: 最优稀疏生存树

    Optimal Sparse Survival Trees. (arXiv:2401.15330v1 [cs.LG])

    [http://arxiv.org/abs/2401.15330](http://arxiv.org/abs/2401.15330)

    本研究提出了一种动态规划边界法，可以在几秒钟内找到可证明最优稀疏生存树模型，对于涉及人类健康的高风险问题的分析和决策具有重要意义。

    

    在涉及人类健康的高风险问题的分析和决策中，可解释性对于医生、医院、制药公司和生物技术公司至关重要。由于其吸引人的可解释性和捕捉复杂关系的能力，基于树的方法已被广泛应用于生存分析。然而，大多数现有的生成生存树的方法依赖于启发式（或贪婪）算法，存在生成次优模型的风险。我们提出了一种动态规划边界法，可以在几秒钟内找到可证明最优稀疏生存树模型。

    Interpretability is crucial for doctors, hospitals, pharmaceutical companies and biotechnology corporations to analyze and make decisions for high stakes problems that involve human health. Tree-based methods have been widely adopted for \textit{survival analysis} due to their appealing interpretablility and their ability to capture complex relationships. However, most existing methods to produce survival trees rely on heuristic (or greedy) algorithms, which risk producing sub-optimal models. We present a dynamic-programming-with-bounds approach that finds provably-optimal sparse survival tree models, frequently in only a few seconds.
    
[^229]: 利用Ricci流引导的自编码器学习时变动力学

    Ricci flow-guided autoencoders in learning time-dependent dynamics. (arXiv:2401.14591v1 [cs.LG])

    [http://arxiv.org/abs/2401.14591](http://arxiv.org/abs/2401.14591)

    利用Ricci流引导的自编码器方法能够学习非线性动力学，尤其是偏微分方程。该方法通过在训练中学习流形，并使用Ricci流使流形潜空间逐步适应动力学的变化，从而获得更好的表示能力。在实验中，我们展示了该方法在具有周期性和随机性的PDE上的应用，并评估了在分布内和外推场景中的误差。

    

    我们提出了一种基于流形的自编码器方法，用于学习时间上的非线性动力学，尤其是偏微分方程（PDE），其中流形潜空间根据Ricci流发展。这可以通过在物理信息设置中模拟Ricci流来实现，并且可以匹配流形量，以便实现Ricci流。使用我们的方法，流形是作为训练过程的一部分学习的，因此可以识别出理想的几何形状，同时演变也能在静态方法上引起更宽容的潜在表示。我们在一系列数值实验中展示了我们的方法，包括具有周期性和随机性等理想特征的PDE，并在分布内和外推场景中进行误差评估。

    We present a manifold-based autoencoder method for learning nonlinear dynamics in time, notably partial differential equations (PDEs), in which the manifold latent space evolves according to Ricci flow. This can be accomplished by simulating Ricci flow in a physics-informed setting, and manifold quantities can be matched so that Ricci flow is empirically achieved. With our methodology, the manifold is learned as part of the training procedure, so ideal geometries may be discerned, while the evolution simultaneously induces a more accommodating latent representation over static methods. We present our method on a range of numerical experiments consisting of PDEs that encompass desirable characteristics such as periodicity and randomness, remarking error on in-distribution and extrapolation scenarios.
    
[^230]: ChatGPT在面部生物识别中的表现有多好？对识别、软生物特征和可解释性的初步探索。

    How Good is ChatGPT at Face Biometrics? A First Look into Recognition, Soft Biometrics, and Explainability. (arXiv:2401.13641v1 [cs.CV])

    [http://arxiv.org/abs/2401.13641](http://arxiv.org/abs/2401.13641)

    本研究初步探索了基于GPT-4的ChatGPT在面部生物识别中的表现。研究分析了ChatGPT在面部验证、软生物特征估计和结果可解释性方面的能力。ChatGPT的应用有望提高自动决策在人类场景中的解释性和透明度。

    

    诸如OpenAI开发的GPT这样的大型语言模型已经展现出令人惊讶的结果，为我们的社会引入了快速变革。ChatGPT的发布进一步加强了这一影响，它使任何人都能以简单的对话方式与语言模型进行交互，不需要任何领域经验。因此，ChatGPT已被迅速应用于许多不同的任务，如代码和歌曲创作、教育、虚拟助手等，展示了对于未经过训练的任务而言令人印象深刻的结果（零样本学习）。本研究旨在探讨基于最新的GPT-4多模态语言模型的ChatGPT在面部生物识别任务中的能力。具体而言，我们分析了ChatGPT在面部验证、软生物特征估计和结果可解释性方面的能力。ChatGPT对于进一步增加人类场景中自动决策的解释性和透明度非常有价值。实验被进行以评估ChatGPT的表现。

    Large Language Models (LLMs) such as GPT developed by OpenAI, have already shown astonishing results, introducing quick changes in our society. This has been intensified by the release of ChatGPT which allows anyone to interact in a simple conversational way with LLMs, without any experience in the field needed. As a result, ChatGPT has been rapidly applied to many different tasks such as code- and song-writer, education, virtual assistants, etc., showing impressive results for tasks for which it was not trained (zero-shot learning).  The present study aims to explore the ability of ChatGPT, based on the recent GPT-4 multimodal LLM, for the task of face biometrics. In particular, we analyze the ability of ChatGPT to perform tasks such as face verification, soft-biometrics estimation, and explainability of the results. ChatGPT could be very valuable to further increase the explainability and transparency of the automatic decisions in human scenarios. Experiments are carried out in order
    
[^231]: 100个样本可以走多远？通过微小的多语言平行数据解锁全面的零样本跨语言翻译

    How Far Can 100 Samples Go? Unlocking Overall Zero-Shot Multilingual Translation via Tiny Multi-Parallel Data. (arXiv:2401.12413v1 [cs.CL])

    [http://arxiv.org/abs/2401.12413](http://arxiv.org/abs/2401.12413)

    本文研究了如何通过仅有的少量微小多语言平行数据来增强以英语为中心的模型的零样本翻译能力，实现大幅度的非英文整体改进，并保持英文方向上的性能能力。研究表明，即使在随机抽取的少量方向集上进行微调，也可以获得可比较的整体改进。

    

    零样本翻译是一个开放问题，旨在在多语言机器翻译（MMT）中翻译训练过程中未见过的语言对。一种常见但资源消耗较大的解决方案是尽可能挖掘更多的翻译方向并添加到平行语料库中。本文展示了通过使用仅有的少量微小多语言平行数据来优化以英语为中心的模型的零样本能力。例如，在EC30数据集上，我们展示了仅使用100个多语言平行样本就能够实现+21.7 ChrF非英文整体改进（870个方向），同时保持在以英语为中心的方向上的能力。我们进一步研究了微调数据的规模效应和其转移能力。令人惊讶的是，我们的实证分析表明，即使是在一个小的、随机抽取的方向集（10%）上进行微调，也可以获得可比较的整体改进。此外，所得到的非英文性能与英文性能非常接近。

    Zero-shot translation is an open problem, aiming to translate between language pairs unseen during training in Multilingual Machine Translation (MMT). A common, albeit resource-consuming, solution is to mine as many translation directions as possible to add to the parallel corpus. In this paper, we show that the zero-shot capability of an English-centric model can be easily enhanced by fine-tuning with a very small amount of multi-parallel data. For example, on the EC30 dataset, we show that up to +21.7 ChrF non-English overall improvements (870 directions) can be achieved by using only 100 multi-parallel samples, meanwhile preserving capability in English-centric directions. We further study the size effect of fine-tuning data and its transfer capabilities. Surprisingly, our empirical analysis shows that comparable overall improvements can be achieved even through fine-tuning in a small, randomly sampled direction set (10\%). Also, the resulting non-English performance is quite close 
    
[^232]: 使用点变换器结合联邦学习从嗪和嘧啶法洗全切片图像中预测乳腺癌HER2状态

    Point Transformer with Federated Learning for Predicting Breast Cancer HER2 Status from Hematoxylin and Eosin-Stained Whole Slide Images. (arXiv:2312.06454v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2312.06454](http://arxiv.org/abs/2312.06454)

    本文介绍了一种使用点变换器结合联邦学习的方法，用于从嗪和嘧啶法染色的全切片图像中预测乳腺癌HER2状态。该方法通过引入动态标签分布策略和辅助分类器，解决了联邦学习中的标签不平衡和利用局部上下文信息和长程依赖性的问题。

    

    直接从广泛可得的嗪和嘧啶法染色全切片图像中预测人类表皮生长因子受体2（HER2）状态可以降低技术成本，加快治疗选择速度。准确预测HER2需要大量的多地点全切片图像。联邦学习可以在不传输千兆字节大小的全切片图像和数据隐私问题的情况下，协同训练这些全切片图像。然而，联邦学习在解决真实世界中多地点全切片图像的标签不平衡方面面临挑战。此外，现有的全切片图像分类方法不能同时利用联邦学习中站点端特征表示中的局部上下文信息和长程依赖关系。为了解决这些问题，我们提出了一种使用点变换器结合联邦学习从嗪和嘧啶法染色全切片图像中预测多地点HER2状态的方法。我们的方法包括两项新设计。我们提出了一种动态标签分布策略和一个辅助分类器。

    Directly predicting human epidermal growth factor receptor 2 (HER2) status from widely available hematoxylin and eosin (HE)-stained whole slide images (WSIs) can reduce technical costs and expedite treatment selection. Accurately predicting HER2 requires large collections of multi-site WSIs. Federated learning enables collaborative training of these WSIs without gigabyte-size WSIs transportation and data privacy concerns. However, federated learning encounters challenges in addressing label imbalance in multi-site WSIs from the real world. Moreover, existing WSI classification methods cannot simultaneously exploit local context information and long-range dependencies in the site-end feature representation of federated learning. To address these issues, we present a point transformer with federated learning for multi-site HER2 status prediction from HE-stained WSIs. Our approach incorporates two novel designs. We propose a dynamic label distribution strategy and an auxiliary classifier,
    
[^233]: 大型语言模型的多阶段协作知识蒸馏在半监督序列生成任务中的应用

    Multistage Collaborative Knowledge Distillation from Large Language Models for Semi-Supervised Sequence Generation. (arXiv:2311.08640v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.08640](http://arxiv.org/abs/2311.08640)

    将大型语言模型的知识通过多阶段协作蒸馏的方式应用于半监督序列生成任务中，可以显著提高模型的泛化能力和性能。

    

    我们研究了半监督序列生成任务，在这种任务中，标记数据太少以至于无法有效地微调模型，同时在大型语言模型 (LLM) 中进行少样本提示的性能也不够理想，尤其是对于一些昂贵且对预训练的 LLM 不熟悉的任务，如解析。本文发现，从上下文学习的 LLM 蒸馏出的学生模型在这些任务上通常比其教师模型具有更好的泛化能力。基于这一发现，我们提出了一种新的方法 - 大型语言模型的多阶段协作知识蒸馏 (MCKD) - 用于这些任务。MCKD 首先进行少样本提示，让LLM为无标签数据生成伪标签。在每个中间知识蒸馏 (KD) 阶段，使用伪标签数据的不重叠分区来训练一对新的学生模型。然后，每个学生模型为其未见分区生成新的和改进的伪标签，在下一个蒸馏阶段中使用。我们展示了该方法的优势。

    We study semi-supervised sequence generation tasks where labeled data are too scarce to effectively finetune a model and at the same time few-shot prompting of a large language model (LLM) has suboptimal performance. This happens when a task, such as parsing, is expensive to annotate and also unfamiliar to a pretrained LLM. In this paper, we present a discovery that student models distilled from an in-context learned LLM can often generalize better than their teacher on such tasks. Leveraging this finding, we present a new method -multistage collaborative knowledge distillation from an LLM (MCKD) -- for such tasks. MCKD first few-shot prompts an LLM to produce pseudolabels for unlabeled data. At each intermediate knowledge distillation (KD) stage, a new pair of students is trained on disjoint partitions of the pseudolabeled data. Each student then produces new and improved pseudolabels for its unseen partition to be used in the next stage of distillation. We demonstrate the advantage
    
[^234]: 条件非线性自动编码器用于轨迹预测

    Conditional Unscented Autoencoders for Trajectory Prediction. (arXiv:2310.19944v1 [cs.RO])

    [http://arxiv.org/abs/2310.19944](http://arxiv.org/abs/2310.19944)

    本文提出了使用条件非线性自动编码器(CVAE)进行轨迹预测的方法，通过利用变分自动编码器(VAE)中的非线性采样过程和其他改进，超越了现有技术，为自动驾驶领域的轨迹预测提供了更好的性能。

    

    条件变分自动编码器(CVAE)是自动驾驶轨迹预测中最常用的模型之一。它将驾驶环境和真实未来关系建立在概率隐空间中，并利用此空间生成预测。本文对CVAE的关键组件提出了挑战。我们利用变分自动编码器(VAE)领域的最新进展，发现变化采样过程可以极大地提高性能。我们发现非线性采样能够更适合于轨迹预测，而随机采样可能带来潜在的问题。我们进一步提出了其他改进，包括更结构化的混合隐空间，以及一种新颖、可能更具表达力的CVAE推理方法。我们通过在INTERACTION预测数据集上进行评估，展示了我们模型的广泛适用性，超过了现有技术的表现。

    The \ac{CVAE} is one of the most widely-used models in trajectory prediction for \ac{AD}. It captures the interplay between a driving context and its ground-truth future into a probabilistic latent space and uses it to produce predictions. In this paper, we challenge key components of the CVAE. We leverage recent advances in the space of the VAE, the foundation of the CVAE, which show that a simple change in the sampling procedure can greatly benefit performance. We find that unscented sampling, which draws samples from any learned distribution in a deterministic manner, can naturally be better suited to trajectory prediction than potentially dangerous random sampling. We go further and offer additional improvements, including a more structured mixture latent space, as well as a novel, potentially more expressive way to do inference with CVAEs. We show wide applicability of our models by evaluating them on the INTERACTION prediction dataset, outperforming the state of the art, as well 
    
[^235]: 时间序列的在线自助法

    An Online Bootstrap for Time Series. (arXiv:2310.19683v1 [stat.ML])

    [http://arxiv.org/abs/2310.19683](http://arxiv.org/abs/2310.19683)

    本文提出了一种新型的在线自助法用于处理大规模的时间序列和相关数据流，通过考虑数据的依赖关系，该方法可以提供可靠的不确定性量化，填补了现有自助法在复杂数据依赖情况下的应用空白。

    

    如何处理大规模的相关数据流（如时间序列或空间相关观测数据）时，传统的自助法受限制。本文提出了一种可以在线执行的新型自助法，专门用于考虑数据的依赖关系，使其特别适用于实时应用。这种方法基于一个自回归序列，其中包含越来越相关的重采样权重。我们证明了在一般条件下提出的自助法的理论有效性。通过大量的模拟实验，我们证明了我们的方法的有效性，并显示它在复杂数据依赖存在的情况下提供可靠的不确定性量化。我们的工作填补了传统重采样技术与现代数据分析需求之间的鸿沟，为研究人员和从业者提供了一种有价值的工具。

    Resampling methods such as the bootstrap have proven invaluable in the field of machine learning. However, the applicability of traditional bootstrap methods is limited when dealing with large streams of dependent data, such as time series or spatially correlated observations. In this paper, we propose a novel bootstrap method that is designed to account for data dependencies and can be executed online, making it particularly suitable for real-time applications. This method is based on an autoregressive sequence of increasingly dependent resampling weights. We prove the theoretical validity of the proposed bootstrap scheme under general conditions. We demonstrate the effectiveness of our approach through extensive simulations and show that it provides reliable uncertainty quantification even in the presence of complex data dependencies. Our work bridges the gap between classical resampling techniques and the demands of modern data analysis, providing a valuable tool for researchers and
    
[^236]: 论费曼-卡克训练部分贝叶斯神经网络

    On Feynman--Kac training of partial Bayesian neural networks. (arXiv:2310.19608v1 [cs.LG])

    [http://arxiv.org/abs/2310.19608](http://arxiv.org/abs/2310.19608)

    本文提出了一种将部分贝叶斯神经网络训练转化为模拟费曼-卡克模型的高效采样训练策略，并通过各种数据集的实验证明其在预测性能方面优于现有技术。

    

    最近，部分贝叶斯神经网络(pBNNs)被证明与全贝叶斯神经网络具有竞争力，但pBNNs在潜变量空间中往往是多峰的，因此用参数模型来近似是具有挑战性的。为了解决这个问题，我们提出了一种高效的基于采样的训练策略，即将pBNN的训练转化为模拟费曼-卡克模型。我们还描述了序贯蒙特卡洛采样器的变种，使我们能够以可行的计算成本同时估计参数和该模型的潜在后验分布。我们在各种合成和真实世界的数据集上展示了我们提出的训练方案在预测性能方面优于现有技术。

    Recently, partial Bayesian neural networks (pBNNs), which only consider a subset of the parameters to be stochastic, were shown to perform competitively with full Bayesian neural networks. However, pBNNs are often multi-modal in the latent-variable space and thus challenging to approximate with parametric models. To address this problem, we propose an efficient sampling-based training strategy, wherein the training of a pBNN is formulated as simulating a Feynman--Kac model. We then describe variations of sequential Monte Carlo samplers that allow us to simultaneously estimate the parameters and the latent posterior distribution of this model at a tractable computational cost. We show on various synthetic and real-world datasets that our proposed training scheme outperforms the state of the art in terms of predictive performance.
    
[^237]: 可行的无鞍牛顿优化神经网络的Hessian-Vector乘积系列

    Series of Hessian-Vector Products for Tractable Saddle-Free Newton Optimisation of Neural Networks. (arXiv:2310.14901v1 [cs.LG])

    [http://arxiv.org/abs/2310.14901](http://arxiv.org/abs/2310.14901)

    本文提出了一种以Hessian-Vector乘积系列为基础的优化算法，通过平方根和求逆操作实现了高效可伸缩的优化方法，并相对于其他一阶和二阶优化方法在运行时间和性能上具有可比性。

    

    尽管拟牛顿方法在连续优化领域非常受欢迎，但在机器学习中应用仍具有挑战性，因为Hessian矩阵的规模过大。通过修改Hessian的特征值来处理非凸性，如无鞍牛顿方法，进一步增加了计算负担。我们提出了一种同时解决这两个问题的优化算法-据我们所知，这是首个可以渐近地使用精确（特征值修改后的）逆Hessian的高效可伸缩优化算法。我们的方法将问题表述为一个主要对Hessian进行平方根和求逆的级数，然后用它来预处理梯度向量，而无需显式计算或进行特征分解。对该无限级数的截断提供了一个新的可伸缩优化算法，其运行时间和优化性能与其他一阶和二阶优化方法相当。

    Despite their popularity in the field of continuous optimisation, second-order quasi-Newton methods are challenging to apply in machine learning, as the Hessian matrix is intractably large. This computational burden is exacerbated by the need to address non-convexity, for instance by modifying the Hessian's eigenvalues as in Saddle-Free Newton methods. We propose an optimisation algorithm which addresses both of these concerns - to our knowledge, the first efficiently-scalable optimisation algorithm to asymptotically use the exact (eigenvalue-modified) inverse Hessian. Our method frames the problem as a series which principally square-roots and inverts the squared Hessian, then uses it to precondition a gradient vector, all without explicitly computing or eigendecomposing the Hessian. A truncation of this infinite series provides a new optimisation algorithm which is scalable and comparable to other first- and second-order optimisation methods in both runtime and optimisation performan
    
[^238]: 在分布式学习任务中评估生成模型

    On the Evaluation of Generative Models in Distributed Learning Tasks. (arXiv:2310.11714v1 [cs.LG])

    [http://arxiv.org/abs/2310.11714](http://arxiv.org/abs/2310.11714)

    本文研究了在具有异构数据分布的分布式学习任务中评估生成模型。通过研究Fr\'echet inception距离（FID），并考虑不同聚合分数，发现FID-all和FID-avg分数的模型排名可能不一致。

    

    在文献中已经广泛研究了对包括生成对抗网络（GAN）和扩散模型在内的深度生成模型的评估。然而，现有的评估方法主要针对单个客户端存储的训练数据的集中式学习问题，而生成模型的许多应用涉及到分布式学习环境，例如联邦学习场景，其中训练数据由多个客户端收集并分发。本文研究了在具有异构数据分布的分布式学习任务中评估生成模型。首先，我们关注Fr\'echet inception距离（FID），并考虑以下基于FID的聚合分数：1）FID-avg作为客户端个体FID分数的平均值，2）FID-all作为训练模型与包含所有客户端数据的集体数据集之间的FID距离。我们证明了根据FID-all和FID-avg分数的模型排名可能不一致。

    The evaluation of deep generative models including generative adversarial networks (GANs) and diffusion models has been extensively studied in the literature. While the existing evaluation methods mainly target a centralized learning problem with training data stored by a single client, many applications of generative models concern distributed learning settings, e.g. the federated learning scenario, where training data are collected by and distributed among several clients. In this paper, we study the evaluation of generative models in distributed learning tasks with heterogeneous data distributions. First, we focus on the Fr\'echet inception distance (FID) and consider the following FID-based aggregate scores over the clients: 1) FID-avg as the mean of clients' individual FID scores, 2) FID-all as the FID distance of the trained model to the collective dataset containing all clients' data. We prove that the model rankings according to the FID-all and FID-avg scores could be inconsist
    
[^239]: 基于人类偏好的非参数离策略评估在深度网络中的样本复杂性

    Sample Complexity of Preference-Based Nonparametric Off-Policy Evaluation with Deep Networks. (arXiv:2310.10556v1 [cs.LG])

    [http://arxiv.org/abs/2310.10556](http://arxiv.org/abs/2310.10556)

    本文研究了在深度网络中使用人类偏好进行非参数离策略评估的样本复杂性，并建立了统计保证。

    

    最近流行的解决强化学习问题的方法是使用人类偏好数据。事实上，人类偏好数据现在与经典的强化学习算法（如演员-评论家方法）一起使用，在从人类偏好数据中学习的奖励上评估中间策略，即离策略评估（OPE）。该算法包括（i）从人类偏好数据集中学习奖励函数，以及（ii）学习目标策略的累积奖励。尽管有巨大的经验成功，但现有的使用偏好数据的OPE方法通常缺乏理论理解，并且严重依赖于启发式方法。在本文中，我们研究了基于人类偏好的OPE的样本效率，并为其建立了统计保证。具体而言，我们通过使用深度神经网络进行拟合Q评估来处理OPE。通过适当选择ReLU网络的大小，我们表明可以利用任何lo

    A recently popular approach to solving reinforcement learning is with data from human preferences. In fact, human preference data are now used with classic reinforcement learning algorithms such as actor-critic methods, which involve evaluating an intermediate policy over a reward learned from human preference data with distribution shift, known as off-policy evaluation (OPE). Such algorithm includes (i) learning reward function from human preference dataset, and (ii) learning expected cumulative reward of a target policy. Despite the huge empirical success, existing OPE methods with preference data often lack theoretical understanding and rely heavily on heuristics. In this paper, we study the sample efficiency of OPE with human preference and establish a statistical guarantee for it. Specifically, we approach OPE by learning the value function by fitted-Q-evaluation with a deep neural network. By appropriately selecting the size of a ReLU network, we show that one can leverage any lo
    
[^240]: 比较现代无参考图像和视频质量评估度量方法对对抗攻击的鲁棒性

    Comparing the robustness of modern no-reference image- and video-quality metrics to adversarial attacks. (arXiv:2310.06958v1 [cs.CV])

    [http://arxiv.org/abs/2310.06958](http://arxiv.org/abs/2310.06958)

    本文比较了现代图像和视频质量评估度量方法对抗攻击的鲁棒性，并发现部分度量方法对对抗攻击表现出较高的抵抗力，为基准测试提供了更安全的选择。

    

    如今，基于神经网络的图像和视频质量评估度量方法相比传统方法表现更好。然而，它们也变得更容易受到对抗性攻击，这些攻击可以提高度量分数但不改善视觉质量。现有的质量度量基准将其性能与主观质量相关性和计算时间进行比较。然而，图像质量度量的对抗鲁棒性也是一个值得研究的领域。本文分析了现代度量方法对不同对抗攻击的鲁棒性。我们采用了计算机视觉任务中的对抗攻击，并比较了这些攻击对15个无参考图像/视频质量度量方法的效果。一些度量方法对对抗攻击表现出了较高的抵抗力，使它们在基准测试中的使用比容易受攻击的方法更安全。该基准测试接受研究人员提交新的度量方法，以使他们的方法对攻击更加鲁棒，或者为他们寻找符合需求的鲁棒度量方法。

    Nowadays neural-network-based image- and video-quality metrics show better performance compared to traditional methods. However, they also became more vulnerable to adversarial attacks that increase metrics' scores without improving visual quality. The existing benchmarks of quality metrics compare their performance in terms of correlation with subjective quality and calculation time. However, the adversarial robustness of image-quality metrics is also an area worth researching. In this paper, we analyse modern metrics' robustness to different adversarial attacks. We adopted adversarial attacks from computer vision tasks and compared attacks' efficiency against 15 no-reference image/video-quality metrics. Some metrics showed high resistance to adversarial attacks which makes their usage in benchmarks safer than vulnerable metrics. The benchmark accepts new metrics submissions for researchers who want to make their metrics more robust to attacks or to find such metrics for their needs. 
    
[^241]: NECO: 基于神经坍塌的超出分布检测

    NECO: NEural Collapse Based Out-of-distribution detection. (arXiv:2310.06823v1 [stat.ML])

    [http://arxiv.org/abs/2310.06823](http://arxiv.org/abs/2310.06823)

    NECO是一种基于神经坍塌的新颖的超出分布检测方法，利用几何属性和主成分空间识别OOD数据，在小规模和大规模任务上取得了最先进的结果，并展示了强大的泛化能力。

    

    由于模型过于自信并且没有意识到其认识论限制，检测超出分布（OOD）数据是机器学习中的一个重要挑战。我们假设“神经坍塌”，一种影响超出分布数据的现象，也会影响超出分布数据。为了从这种相互作用中受益，我们引入了NECO，一种用于OOD检测的新颖的事后方法，它利用“神经坍塌”和主成分空间的几何属性来识别OOD数据。我们的大量实验表明，NECO在小规模和大规模OOD检测任务上取得了最先进的结果，同时在不同的网络架构上展示了强大的泛化能力。此外，我们还对我们的方法在OOD检测中的有效性提供了理论解释。我们计划在匿名期结束后发布代码。

    Detecting out-of-distribution (OOD) data is a critical challenge in machine learning due to model overconfidence, often without awareness of their epistemological limits. We hypothesize that ``neural collapse'', a phenomenon affecting in-distribution data for models trained beyond loss convergence, also influences OOD data. To benefit from this interplay, we introduce NECO, a novel post-hoc method for OOD detection, which leverages the geometric properties of ``neural collapse'' and of principal component spaces to identify OOD data. Our extensive experiments demonstrate that NECO achieves state-of-the-art results on both small and large-scale OOD detection tasks while exhibiting strong generalization capabilities across different network architectures. Furthermore, we provide a theoretical explanation for the effectiveness of our method in OOD detection. We plan to release the code after the anonymity period.
    
[^242]: 大型语言模型是事后解释器吗？

    Are Large Language Models Post Hoc Explainers?. (arXiv:2310.05797v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05797](http://arxiv.org/abs/2310.05797)

    这项工作提出了第一个研究大型语言模型（LLMs）解释其他预测模型有效性的框架，并且提出了多个提示策略，填补了当前对于LLMs在解释其他模型行为方面的缺失。

    

    大型语言模型（LLM）越来越被广泛应用于各种自然语言处理（NLP）应用中。最近的一项创新，即上下文学习（ICL），使得LLM能够在推理阶段通过在提示中提供少量示例来学习新任务，从而消除了模型微调的需要。虽然LLM已经被应用于多个领域，但其在解释其他模型行为方面的适用性仍相对未被探索。尽管存在越来越多的新解释技术，但很多技术要求对模型具有白盒访问权限和/或计算成本较高，凸显了下一代事后解释器的需求。在这项工作中，我们提出了第一个研究LLM解释其他预测模型有效性的框架。具体而言，我们提出了一个包含多种提示策略的新颖框架：i）基于扰动的ICL，ii）基于预测的ICL，iii）基于指令的ICL，和iv）基于解释的ICL。

    Large Language Models (LLMs) are increasingly used as powerful tools for a plethora of natural language processing (NLP) applications. A recent innovation, in-context learning (ICL), enables LLMs to learn new tasks by supplying a few examples in the prompt during inference time, thereby eliminating the need for model fine-tuning. While LLMs have been utilized in several applications, their applicability in explaining the behavior of other models remains relatively unexplored. Despite the growing number of new explanation techniques, many require white-box access to the model and/or are computationally expensive, highlighting a need for next-generation post hoc explainers. In this work, we present the first framework to study the effectiveness of LLMs in explaining other predictive models. More specifically, we propose a novel framework encompassing multiple prompting strategies: i) Perturbation-based ICL, ii) Prediction-based ICL, iii) Instruction-based ICL, and iv) Explanation-based I
    
[^243]: 带有仅解码器侧信息的分布式深度联合源信道编码

    Distributed Deep Joint Source-Channel Coding with Decoder-Only Side Information. (arXiv:2310.04311v1 [cs.CV])

    [http://arxiv.org/abs/2310.04311](http://arxiv.org/abs/2310.04311)

    本文提出了一种带有仅解码器侧信息的分布式深度联合源信道编码方法，在低延迟图像传输中实现了改进的性能，尤其在低信道信噪比和小带宽比的情况下。

    

    我们考虑在只有接收方有相关辅助信息的噪声无线信道上进行低延迟图像传输（Wyner-Ziv情景）。我们特别关注通过数据驱动的联合源信道编码（JSCC）方法开发实际方案，这在实际有限块长度的情况下已被证明优于传统的分离式方法，并在信道质量方面提供了优雅的退化。我们提出了一种新的神经网络架构，在接收器端的多个阶段将仅解码器侧信息融入其中。我们的结果表明，所提出的方法成功地整合了辅助信息，在各种畸变准则下，在所有信道噪声水平上都实现了改进的性能，尤其是在低信道信噪比（SNR）和小带宽比（BR）的情况下。我们还提供了所提方法的源代码，以便进一步研究。

    We consider low-latency image transmission over a noisy wireless channel when correlated side information is present only at the receiver side (the Wyner-Ziv scenario). In particular, we are interested in developing practical schemes using a data-driven joint source-channel coding (JSCC) approach, which has been previously shown to outperform conventional separation-based approaches in the practical finite blocklength regimes, and to provide graceful degradation with channel quality. We propose a novel neural network architecture that incorporates the decoder-only side information at multiple stages at the receiver side. Our results demonstrate that the proposed method succeeds in integrating the side information, yielding improved performance at all channel noise levels in terms of the various distortion criteria considered here, especially at low channel signal-to-noise ratios (SNRs) and small bandwidth ratios (BRs). We also provide the source code of the proposed method to enable fu
    
[^244]: 一种用于医学图像中一般移动目标分割的基础模型

    A Foundation Model for General Moving Object Segmentation in Medical Images. (arXiv:2309.17264v1 [cs.CV])

    [http://arxiv.org/abs/2309.17264](http://arxiv.org/abs/2309.17264)

    本文提出了一种用于医学图像中移动目标分割的基础模型iMOS，通过对序列中只有少量图像进行注释，即可实现高精度的分割效果

    

    医学图像分割旨在描绘感兴趣的解剖或病理结构，在临床诊断中起着关键作用。构建高精度的深度分割模型需要大量高质量的注释数据。然而，医学注释非常繁琐耗时，特别是对于医学视频或3D体积，由于巨大的标签空间和差的帧间一致性。最近，在自然图像中，一个名为Moving Object Segmentation (MOS)的基本任务在技术上取得了重大进展。它的目标是在图像序列中从背景中描绘移动物体，只需要最小的注释。在本文中，我们提出了第一个用于医学图像中MOS的基础模型，名为iMOS。对一个大规模多模态医学数据集进行的大量实验验证了所提出的iMOS的有效性。具体而言，只需对序列中少量的图像进行注释，iMOS就可以实现了

    Medical image segmentation aims to delineate the anatomical or pathological structures of interest, playing a crucial role in clinical diagnosis. A substantial amount of high-quality annotated data is crucial for constructing high-precision deep segmentation models. However, medical annotation is highly cumbersome and time-consuming, especially for medical videos or 3D volumes, due to the huge labeling space and poor inter-frame consistency. Recently, a fundamental task named Moving Object Segmentation (MOS) has made significant advancements in natural images. Its objective is to delineate moving objects from the background within image sequences, requiring only minimal annotations. In this paper, we propose the first foundation model, named iMOS, for MOS in medical images. Extensive experiments on a large multi-modal medical dataset validate the effectiveness of the proposed iMOS. Specifically, with the annotation of only a small number of images in the sequence, iMOS can achieve sati
    
[^245]: LLM能否有效利用结构信息进行图学习：何时何地。

    Can LLMs Effectively Leverage Structural Information for Graph Learning: When and Why. (arXiv:2309.16595v1 [cs.LG])

    [http://arxiv.org/abs/2309.16595](http://arxiv.org/abs/2309.16595)

    本文研究了大型语言模型（LLM）在图数据中的应用，发现LLM可以从结构信息中受益，尤其是在文本节点特征缺乏的情况下，而LLM的性能与数据泄露没有显著相关。

    

    本文研究了大型语言模型（LLM）在结构化数据（特别是图数据）上的应用，这是LLM文献中尚未充分探索的重要数据形态。我们旨在了解在节点分类任务中，何时何地引入图数据中的结构信息可以提高LLM的预测性能。为了解决“何时”问题，我们研究了多种编码结构信息的提示方法，设置中文本节点特征丰富或稀缺。对于“为什么”问题，我们探讨了LLM性能的两个潜在因素：数据泄露和同质性。我们的研究结果表明：（i）LLM可以从结构信息中受益，尤其是在文本节点特征缺乏的情况下；（ii）没有实质性的证据表明LLM性能与数据泄露有显著相关；（iii）LLM在目标节点上的性能与正向相关。

    This paper studies Large Language Models (LLMs) for structured data--particularly graphs--a crucial data modality that remains underexplored in the LLM literature. We aim to understand when and why the incorporation of structural information inherent in graph data can improve the prediction performance of LLMs on node classification tasks. To address the ``when'' question, we examine a variety of prompting methods for encoding structural information, in settings where textual node features are either rich or scarce. For the ``why'' questions, we probe into two potential contributing factors to the LLM performance: data leakage and homophily. Our exploration of these questions reveals that (i) LLMs can benefit from structural information, especially when textual node features are scarce; (ii) there is no substantial evidence indicating that the performance of LLMs is significantly attributed to data leakage; and (iii) the performance of LLMs on a target node is strongly positively relat
    
[^246]: 活动学习强化学习：一种随机最优控制方法的应用

    Actively Learning Reinforcement Learning: A Stochastic Optimal Control Approach. (arXiv:2309.10831v1 [cs.LG])

    [http://arxiv.org/abs/2309.10831](http://arxiv.org/abs/2309.10831)

    本文提供了一个框架来解决强化学习中的模型不确定性和计算成本高的问题，通过使用强化学习解决随机动态规划方程，生成的控制器能够主动学习模型不确定性，并确保安全性和实时学习。

    

    本文提供了一个框架来应对两个问题：（i）强化学习在模型不确定性方面的脆弱性，因为受控实验室/仿真和实际条件之间的不匹配，以及（ii）随机最优控制的计算成本过高。我们通过使用强化学习来解决随机动态规划方程来解决这两个问题。由此产生的强化学习控制器对于几种类型的约束条件是安全的，并且它可以主动学习模型不确定性。与探索和利用不同，探测和安全性由控制器自身自动实现，实现了实时学习。一个仿真示例证明了所提方法的有效性。

    In this paper we provide framework to cope with two problems: (i) the fragility of reinforcement learning due to modeling uncertainties because of the mismatch between controlled laboratory/simulation and real-world conditions and (ii) the prohibitive computational cost of stochastic optimal control. We approach both problems by using reinforcement learning to solve the stochastic dynamic programming equation. The resulting reinforcement learning controller is safe with respect to several types of constraints constraints and it can actively learn about the modeling uncertainties. Unlike exploration and exploitation, probing and safety are employed automatically by the controller itself, resulting real-time learning. A simulation example demonstrates the efficacy of the proposed approach.
    
[^247]: 局部平稳图形过程

    Locally Stationary Graph Processes. (arXiv:2309.01657v1 [stat.ML])

    [http://arxiv.org/abs/2309.01657](http://arxiv.org/abs/2309.01657)

    这是一种局部平稳图形过程模型，旨在将局部平稳概念扩展到不规则的图域上。它通过将整个过程表示为一组组成部分过程的组合来表征局部平稳性，以使过程在图上按照每个组成部分的要求变化得更加平滑。

    

    在不规则的网络拓扑上收集的数据集的分析和推理中，常常会使用平稳图形过程模型。然而，大多数现有方法使用单一的全局有效的平稳过程模型表示图形信号，但在许多实际问题中，过程的特性可能会在图的不同区域发生局部变化。本文提出了一种局部平稳图形过程（LSGP）模型，旨在将经典的局部平稳概念扩展到不规则的图域上。我们通过将整个过程表示为一组组成部分过程的组合来表征局部平稳性，以使过程在图上按照每个组成部分的要求变化得更加平滑。我们提出了一种计算LSGP模型的算法，并研究了用WSS过程对LSGP进行局部近似。在信号内插问题上进行的实验表明

    Stationary graph process models are commonly used in the analysis and inference of data sets collected on irregular network topologies. While most of the existing methods represent graph signals with a single stationary process model that is globally valid on the entire graph, in many practical problems, the characteristics of the process may be subject to local variations in different regions of the graph. In this work, we propose a locally stationary graph process (LSGP) model that aims to extend the classical concept of local stationarity to irregular graph domains. We characterize local stationarity by expressing the overall process as the combination of a set of component processes such that the extent to which the process adheres to each component varies smoothly over the graph. We propose an algorithm for computing LSGP models from realizations of the process, and also study the approximation of LSGPs locally with WSS processes. Experiments on signal interpolation problems show 
    
[^248]: FedSoL: 在联邦学习中解决全局对齐和本地一般性的问题

    FedSoL: Bridging Global Alignment and Local Generality in Federated Learning. (arXiv:2308.12532v1 [cs.LG])

    [http://arxiv.org/abs/2308.12532](http://arxiv.org/abs/2308.12532)

    FedSoL提出了一种联邦学习的方法，该方法旨在解决数据分布不均匀导致性能下降的问题。它通过平衡全局对齐和本地一般性来改善FL的学习效果。

    

    联邦学习(Federated Learning, FL)通过聚合来自个体客户端的本地训练模型来构建全局模型。虽然FL可以在保护数据隐私的情况下学习模型，但当客户端数据分布不均匀时，常常导致性能下降。许多先前的FL算法通过引入各种近似约束来解决这个问题。这些约束旨在通过限制局部学习与全局目标的偏离来促进全局对齐。然而，它们本质上通过干扰原始的局部目标而限制了局部学习。最近，出现了一种替代方法来改善本地学习的一般性。通过在平滑的损失空间中获得本地模型，这种方法减轻了客户端不同本地目标之间的冲突。然而，它不能确保稳定的全局对齐，因为本地学习不考虑全局目标。在本研究中，我们提出了联邦学习的稳定性(FedSoL)方法来在FL中解决全局对齐和本地一般性的问题。

    Federated Learning (FL) aggregates locally trained models from individual clients to construct a global model. While FL enables learning a model with data privacy, it often suffers from significant performance degradation when client data distributions are heterogeneous. Many previous FL algorithms have addressed this issue by introducing various proximal restrictions. These restrictions aim to encourage global alignment by constraining the deviation of local learning from the global objective. However, they inherently limit local learning by interfering with the original local objectives. Recently, an alternative approach has emerged to improve local learning generality. By obtaining local models within a smooth loss landscape, this approach mitigates conflicts among different local objectives of the clients. Yet, it does not ensure stable global alignment, as local learning does not take the global objective into account. In this study, we propose Federated Stability on Learning (Fed
    
[^249]: 基于数据驱动的自治系统图生成器

    Data-driven Intra-Autonomous Systems Graph Generator. (arXiv:2308.05254v1 [cs.NI])

    [http://arxiv.org/abs/2308.05254](http://arxiv.org/abs/2308.05254)

    本文介绍了一种基于深度学习的新型合成图生成器DGGI，用于准确地模拟互联网中自治系统内的图的属性，如中心性、聚类性、同质性以及节点度量。该生成器的性能优于现有的互联网拓扑生成器。

    

    本文介绍了一种基于深度学习的新型合成图生成器DGGI，用于表示互联网中自治系统（AS）内的图。同时，还提出了一种来自Internet Topology Data Kit（ITDK）项目的真实自治系统图的大规模数据集，称为Internet Graphs（IGraphs）。创建IGraphs采用了Filtered Recurrent Multi-level（FRM）算法进行社区提取。实验证明，DGGI生成的合成图可以准确地再现中心性、聚类性、同质性和节点度量的特性。DGGI生成器优于现有的互联网拓扑生成器。平均而言，对于同质性、中介度、聚类性和节点度量，DGGI在最大均匀差异度（MMD）指标上分别提高了84.4%、95.1%、97.9%和94.7%。

    This paper introduces a novel deep-learning based generator of synthetic graphs that represent intra-Autonomous System (AS) in the Internet, named Deep-generative graphs for the Internet (DGGI). It also presents a novel massive dataset of real intra-AS graphs extracted from the project Internet Topology Data Kit (ITDK), called Internet Graphs (IGraphs). To create IGraphs, the Filtered Recurrent Multi-level (FRM) algorithm for community extraction was developed. It is shown that DGGI creates synthetic graphs which accurately reproduce the properties of centrality, clustering, assortativity, and node degree. The DGGI generator overperforms existing Internet topology generators. On average, DGGI improves the Maximum Mean Discrepancy (MMD) metric 84.4%, 95.1%, 97.9%, and 94.7% for assortativity, betweenness, clustering, and node degree, respectively.
    
[^250]: 在随机位置预测掩盖的标记改善了掩盖图像建模

    Predicting masked tokens in stochastic locations improves masked image modeling. (arXiv:2308.00566v1 [cs.CV])

    [http://arxiv.org/abs/2308.00566](http://arxiv.org/abs/2308.00566)

    本论文提出了一种名为FlexPredict的随机模型，通过在模型中加入位置不确定性，以预测掩盖的标记位置，从而改善了掩盖图像建模的性能。

    

    自监督学习是深度学习中一种有前景的范式，通过构建需要学习有用表示的预训练任务，可以从无标签数据中进行学习。在自然语言处理中，主要的预训练任务是掩盖语言建模（MLM），而在计算机视觉中存在相应的掩盖图像建模（MIM）。然而，MIM具有挑战性，因为它需要在准确位置上预测语义内容。例如，给定一张不完整的狗的图片，我们可以猜测有一个尾巴，但我们无法确定它的确切位置。在这项工作中，我们提出了FlexPredict，这是一个考虑位置不确定性的随机模型，通过将模型条件化到随机掩盖的标记位置上，引导模型学习更加鲁棒对位置不确定性的特征。我们的方法改进了多个任务的下游性能，例如与MIM基准相比，FlexPredict在一系列任务上表现更好。

    Self-supervised learning is a promising paradigm in deep learning that enables learning from unlabeled data by constructing pretext tasks that require learning useful representations. In natural language processing, the dominant pretext task has been masked language modeling (MLM), while in computer vision there exists an equivalent called Masked Image Modeling (MIM). However, MIM is challenging because it requires predicting semantic content in accurate locations. E.g, given an incomplete picture of a dog, we can guess that there is a tail, but we cannot determine its exact location. In this work, we propose FlexPredict, a stochastic model that addresses this challenge by incorporating location uncertainty into the model. Specifically, we condition the model on stochastic masked token positions to guide the model toward learning features that are more robust to location uncertainties. Our approach improves downstream performance on a range of tasks, e.g, compared to MIM baselines, Fle
    
[^251]: 使用神经调整层析成像（NeTT）和掩蔽神经辐射场（mNeRF）的稳健单视锥形X射线姿态估计

    Robust Single-view Cone-beam X-ray Pose Estimation with Neural Tuned Tomography (NeTT) and Masked Neural Radiance Fields (mNeRF). (arXiv:2308.00214v1 [cs.CV])

    [http://arxiv.org/abs/2308.00214](http://arxiv.org/abs/2308.00214)

    在影像导引的微创医疗过程中，我们提出了新的方法，利用X射线投影进行辐射透明物体的姿态估计，并且展示了优化视图合成在完成此任务中的关键作用。

    

    在影像导引的微创医疗过程中，许多任务可以看作是姿态估计问题，其中利用X射线投影来达到3D空间中的目标。近期在可微分渲染技术上的进展使得RGB相机视图合成和姿态估计的性能达到了最先进水平。在之前的工作基础上，我们引入了新的方法，用于使用X射线投影进行辐射透明物体的姿态估计，并且展示了优化视图合成在完成此任务中的关键作用。首先我们开发了一种算法（DiffDRR），能够在TensorFlow中高效计算数字重建放射图像（DRRs）并利用自动微分。结合经典的CBCT重建算法，我们通过梯度下降进行姿态估计，使用一个损失函数来量化从随机初始化姿态合成的DRR与目标处真实透视图像的相似性。

    Many tasks performed in image-guided, mini-invasive, medical procedures can be cast as pose estimation problems, where an X-ray projection is utilized to reach a target in 3D space. Recent advances in the differentiable rendering of optically reflective materials have enabled state-of-the-art performance in RGB camera view synthesis and pose estimation. Expanding on these prior works, we introduce new methods for pose estimation of radiolucent objects using X-ray projections, and we demonstrate the critical role of optimal view synthesis in performing this task. We first develop an algorithm (DiffDRR) that efficiently computes Digitally Reconstructed Radiographs (DRRs) and leverages automatic differentiation within TensorFlow. In conjunction with classic CBCT reconstruction algorithms, we perform pose estimation by gradient descent using a loss function that quantifies the similarity of the DRR synthesized from a randomly initialized pose and the true fluoroscopic image at the target p
    
[^252]: 预训练的深度模型在标签稀缺的Learning-To-Rank中胜过GBDTs

    Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity. (arXiv:2308.00177v1 [cs.LG])

    [http://arxiv.org/abs/2308.00177](http://arxiv.org/abs/2308.00177)

    本研究研究了在标签稀缺的Learning-To-Rank问题中，无监督预训练的深度模型是否能胜过GBDTs和其他非预训练模型。实验结果表明，通过使用SimCLR-Rank方法进行无监督预训练，我们的深度学习模型在大量无标签数据和有限标签数据的情况下取得了显著优势。

    

    尽管深度学习模型在文本和图像领域是最先进的，但它们在表格形式的Learning-To-Rank问题上尚未一致地胜过梯度提升决策树(GBDTs)。近期在文本和图像任务上深度学习模型取得的性能提升主要依赖于无监督预训练，这种方法利用了比有标签数据多几个数量级的无标签数据。据我们所知，无监督预训练还未应用于Learning-To-Rank问题，而该问题通常产生大量无标签数据。本研究探究了无监督预训练是否能提高LTR性能，与GBDTs和其他非预训练模型相比。通过使用简单的设计选择(包括SimCLR-Rank，这是我们针对排名问题修改的SimCLR方法)，我们产生了预训练的深度学习模型，在有大量无标签数据且有限标签数据的情况下，显著优于GBDTs(和其他非预训练模型)。

    While deep learning (DL) models are state-of-the-art in text and image domains, they have not yet consistently outperformed Gradient Boosted Decision Trees (GBDTs) on tabular Learning-To-Rank (LTR) problems. Most of the recent performance gains attained by DL models in text and image tasks have used unsupervised pretraining, which exploits orders of magnitude more unlabeled data than labeled data. To the best of our knowledge, unsupervised pretraining has not been applied to the LTR problem, which often produces vast amounts of unlabeled data. In this work, we study whether unsupervised pretraining can improve LTR performance over GBDTs and other non-pretrained models. Using simple design choices--including SimCLR-Rank, our ranking-specific modification of SimCLR (an unsupervised pretraining method for images)--we produce pretrained deep learning models that soundly outperform GBDTs (and other non-pretrained models) in the case where labeled data is vastly outnumbered by unlabeled data
    
[^253]: AdvDiff:使用扩散模型生成无限制的对抗样本

    AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models. (arXiv:2307.12499v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.12499](http://arxiv.org/abs/2307.12499)

    本文提出了一种使用扩散模型生成无限制对抗样本的方法AdvDiff。通过设计两种新的对抗引导技术，在扩散模型的逆生成过程中进行对抗采样，从而有效地生成高质量、逼真的对抗样本。

    

    无限制的对抗攻击对深度学习模型和对抗防御技术构成严重威胁。它们对深度学习应用造成严重的安全问题，因为它们可以有效地绕过防御机制。然而，先前的攻击方法通常利用生成对抗网络（GAN），这些网络在理论上无法证明，因此在大规模数据集（如ImageNet）上通过引入对抗目标生成的例子是不现实的。在本文中，我们提出了一种新的方法，称为AdvDiff，使用扩散模型生成无限制的对抗样本。我们设计了两种新的对抗引导技术，在扩散模型的逆生成过程中进行对抗采样。这两种技术通过可解释的目标分类器梯度集成生成高质量、逼真的对抗样本非常有效和稳定。在MNIST和ImageNet数据集上的实验结果表明，AdvDiff能够生成高质量、逼真的对抗样本。

    Unrestricted adversarial attacks present a serious threat to deep learning models and adversarial defense techniques. They pose severe security problems for deep learning applications because they can effectively bypass defense mechanisms. However, previous attack methods often utilize Generative Adversarial Networks (GANs), which are not theoretically provable and thus generate unrealistic examples by incorporating adversarial objectives, especially for large-scale datasets like ImageNet. In this paper, we propose a new method, called AdvDiff, to generate unrestricted adversarial examples with diffusion models. We design two novel adversarial guidance techniques to conduct adversarial sampling in the reverse generation process of diffusion models. These two techniques are effective and stable to generate high-quality, realistic adversarial examples by integrating gradients of the target classifier interpretably. Experimental results on MNIST and ImageNet datasets demonstrate that AdvD
    
[^254]: 分数降噪用于3D分子预训练

    Fractional Denoising for 3D Molecular Pre-training. (arXiv:2307.10683v1 [q-bio.QM])

    [http://arxiv.org/abs/2307.10683](http://arxiv.org/abs/2307.10683)

    本论文提出了一种分数降噪算法，用于3D分子预训练。通过混合噪声策略解决了样本覆盖率低和各向同性力场的挑战，通过解耦两种类型的噪声克服了传统降噪方法无法学习力场的问题。

    

    坐标降噪是一种有前途的3D分子预训练方法，在各种下游药物发现任务中取得了显著的性能。从理论上讲，其目标等同于学习力场，并且对下游任务有帮助。然而，坐标降噪学习有效力场面临两个挑战，即样本覆盖率低和各向同性力场。根本原因在于现有降噪方法所假设的分子分布不能捕捉分子的各向异性特征。为了解决这些挑战，我们提出了一种新的混合噪声策略，包括二面角和坐标的噪声。然而，以传统方式降噪这种混合噪声不再等同于学习力场。通过理论推导，我们发现问题是由于输入构象对协方差的依赖性所导致的。因此，我们提出将这两种类型的噪声解耦

    Coordinate denoising is a promising 3D molecular pre-training method, which has achieved remarkable performance in various downstream drug discovery tasks. Theoretically, the objective is equivalent to learning the force field, which is revealed helpful for downstream tasks. Nevertheless, there are two challenges for coordinate denoising to learn an effective force field, i.e. low coverage samples and isotropic force field. The underlying reason is that molecular distributions assumed by existing denoising methods fail to capture the anisotropic characteristic of molecules. To tackle these challenges, we propose a novel hybrid noise strategy, including noises on both dihedral angel and coordinate. However, denoising such hybrid noise in a traditional way is no more equivalent to learning the force field. Through theoretical deductions, we find that the problem is caused by the dependency of the input conformation for covariance. To this end, we propose to decouple the two types of nois
    
[^255]: 加速Benders分解方法的强化学习代理模型研究

    Towards Accelerating Benders Decomposition via Reinforcement Learning Surrogate Models. (arXiv:2307.08816v1 [cs.LG])

    [http://arxiv.org/abs/2307.08816](http://arxiv.org/abs/2307.08816)

    本文介绍了一种利用强化学习代理模型加速Benders分解方法的方法，并通过实验证明了其相对于其他加速方案的30%更快的平均收敛速度。

    

    随机优化试图在存在不确定性的情况下提供最优决策。通常，由于需要捕捉不确定性的情景数量以及现实规划问题的离散性质，这些问题的经典形式变得难以处理。为了克服这些可行性问题，实践者们转向分解方法，将问题分解为更小、更易处理的子问题。本文的主要分解方法是Benders分解（BD），它根据情景独立性对随机优化问题进行分解。在本文中，我们提出了一种利用代理模型加速BD的方法，该代理模型取代了NP难的整数主问题。通过加速方法，与其他加速的BD实现相比，我们观察到平均收敛速度提高了30%。我们引入了一个强化学习代理作为替代，并展示了如何使用它来解决随机库存问题。

    Stochastic optimization (SO) attempts to offer optimal decisions in the presence of uncertainty. Often, the classical formulation of these problems becomes intractable due to (a) the number of scenarios required to capture the uncertainty and (b) the discrete nature of real-world planning problems. To overcome these tractability issues, practitioners turn to decomposition methods that divide the problem into smaller, more tractable sub-problems. The focal decomposition method of this paper is Benders decomposition (BD), which decomposes stochastic optimization problems on the basis of scenario independence. In this paper we propose a method of accelerating BD with the aid of a surrogate model in place of an NP-hard integer master problem. Through the acceleration method we observe 30% faster average convergence when compared to other accelerated BD implementations. We introduce a reinforcement learning agent as a surrogate and demonstrate how it can be used to solve a stochastic invent
    
[^256]: 关于图神经网络的能力和激活函数的作用

    On the power of graph neural networks and the role of the activation function. (arXiv:2307.04661v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.04661](http://arxiv.org/abs/2307.04661)

    本文通过对称多项式代数的工具证明了对于具有分段多项式激活函数且体系结构大小不变的GNNs，存在一对非同构根树在任意迭代次数内无法被区分，与此同时，具有不同大小的GNNs只需两次迭代即可区分。此外，我们还证明了如果允许非分段多项式激活函数，则在两次迭代内，单个神经元感知器可以区分任意一对非同构树的根节点。

    

    在这篇文章中，我们提出了关于图神经网络（GNNs）表达能力的新结果。我们证明了对于任何具有分段多项式激活函数、其体系结构大小不随图输入大小增长的GNNs，存在一对深度为二的非同构根树，使得GNNs在任意迭代次数内无法区分它们的根节点。证明依赖于对称多项式代数的工具。相比之下，已经知道具有分段多项式激活函数的无界GNNs（其大小允许随图大小改变）只需两次迭代即可区分这些顶点。我们的结果对于有界大小和无界大小的GNNs之间存在严格的分离，回答了 [Grohe, 2021] 提出的一个开放性问题。接下来，我们证明如果允许非分段多项式激活函数，则在两次迭代中，单个神经元感知器可以区分任意一对非同构树的根节点。

    In this article we present new results about the expressivity of Graph Neural Networks (GNNs). We prove that for any GNN with piecewise polynomial activations, whose architecture size does not grow with the graph input sizes, there exists a pair of non-isomorphic rooted trees of depth two such that the GNN cannot distinguish their root vertex up to an arbitrary number of iterations. The proof relies on tools from the algebra of symmetric polynomials. In contrast, it was already known that unbounded GNNs (those whose size is allowed to change with the graph sizes) with piecewise polynomial activations can distinguish these vertices in only two iterations. Our results imply a strict separation between bounded and unbounded size GNNs, answering an open question formulated by [Grohe, 2021]. We next prove that if one allows activations that are not piecewise polynomial, then in two iterations a single neuron perceptron can distinguish the root vertices of any pair of nonisomorphic trees of 
    
[^257]: 运算学习中的维度诅咒

    The curse of dimensionality in operator learning. (arXiv:2306.15924v1 [cs.LG])

    [http://arxiv.org/abs/2306.15924](http://arxiv.org/abs/2306.15924)

    算子学习中存在维度诅咒，但对于由Hamilton-Jacobi方程定义的解算子可以克服维度诅咒。

    

    神经算子架构利用神经网络来近似映射函数空间之间的算子，可以用于通过模拟加速模型评估，或者从数据中发现模型。因此，这一方法在近年来受到越来越多的关注，引发了算子学习领域的快速发展。本文的第一项贡献是证明了对于一般的只由其 $C^r$ 或 Lipschitz 正则性特征化的算子类，算子学习遭受了维度诅咒，这里通过无穷维输入和输出函数空间的表征来精确定义维度诅咒。该结果适用于包括 PCA-Net、DeepONet 和 FNO 在内的多种现有神经算子。本文的第二项贡献是证明了对于由Hamilton-Jacobi方程定义的解算子，可以克服一般的维度诅咒；这是通过引入新的表示方法来实现的。

    Neural operator architectures employ neural networks to approximate operators mapping between Banach spaces of functions; they may be used to accelerate model evaluations via emulation, or to discover models from data. Consequently, the methodology has received increasing attention over recent years, giving rise to the rapidly growing field of operator learning. The first contribution of this paper is to prove that for general classes of operators which are characterized only by their $C^r$- or Lipschitz-regularity, operator learning suffers from a curse of dimensionality, defined precisely here in terms of representations of the infinite-dimensional input and output function spaces. The result is applicable to a wide variety of existing neural operators, including PCA-Net, DeepONet and the FNO. The second contribution of the paper is to prove that the general curse of dimensionality can be overcome for solution operators defined by the Hamilton-Jacobi equation; this is achieved by lev
    
[^258]: 使用gnomonic equiangular投影的生成对抗网络进行HRTF上采样

    HRTF upsampling with a generative adversarial network using a gnomonic equiangular projection. (arXiv:2306.05812v1 [eess.AS])

    [http://arxiv.org/abs/2306.05812](http://arxiv.org/abs/2306.05812)

    本文利用生成对抗网络（GANs）将HRTF上采样，提出一种新方法，该方法在性能方面优于传统方法，使带HRTF的虚拟现实（VR）和增强现实（AR）环境更加逼真。

    

    个性化的头部相关转移函数（HRTF）对于创建逼真的虚拟现实（VR）和增强现实（AR）环境至关重要。然而，声学测量高质量的HRTF需要昂贵的设备和声学实验室设置。为了克服这些限制并使测量更加高效，过去已经利用了HRTF上采样，其中从低分辨率的HRTF创建高分辨率的HRTF。本文展示了如何将生成对抗网络（GANs）应用于HRTF上采样。我们提出了一种新方法，将HRTF数据转换为与卷积超分辨率生成对抗网络（SRGAN）方便使用。这种新方法与两个基线进行了基准测试：重心上采样和HRTF选择方法。实验结果表明，所提出的方法在使用感知模型时，从log-spectral失真（LSD）和定位性能方面均优于两个基线。

    An individualised head-related transfer function (HRTF) is essential for creating realistic virtual reality (VR) and augmented reality (AR) environments. However, acoustically measuring high-quality HRTFs requires expensive equipment and an acoustic lab setting. To overcome these limitations and to make this measurement more efficient HRTF upsampling has been exploited in the past where a high-resolution HRTF is created from a low-resolution one. This paper demonstrates how generative adversarial networks (GANs) can be applied to HRTF upsampling. We propose a novel approach that transforms the HRTF data for convenient use with a convolutional super-resolution generative adversarial network (SRGAN). This new approach is benchmarked against two baselines: barycentric upsampling and a HRTF selection approach. Experimental results show that the proposed method outperforms both baselines in terms of log-spectral distortion (LSD) and localisation performance using perceptual models when the 
    
[^259]: 变分高斯过程扩散过程

    Variational Gaussian Process Diffusion Processes. (arXiv:2306.02066v1 [cs.LG])

    [http://arxiv.org/abs/2306.02066](http://arxiv.org/abs/2306.02066)

    本文提出一种高斯变分过程参数化方法来更好地学习具有非线性扩散过程的潜在过程，此方法采用具有连续指数族描述的算法实现凸优化，可以代替缓慢的具有固定点迭代的算法。

    

    扩散过程是一类随机微分方程，提供了一系列表现丰富的模型，自然地出现在动态建模任务中。概率推理和生成模型下具有非线性扩散过程的潜在过程的学习都是棘手的问题。本文在变分推理的基础上构建高斯过程扩散过程的参数化，指出方法中的病态，并提出一种使用连续指数族描述的高斯变分过程的替代参数化方法。这使我们可以用凸优化的快速算法代替具有固定点迭代的缓慢算法，这种算法类似于自然梯度下降，同时提供更好的目标来学习模型参数。

    Diffusion processes are a class of stochastic differential equations (SDEs) providing a rich family of expressive models that arise naturally in dynamic modelling tasks. Probabilistic inference and learning under generative models with latent processes endowed with a non-linear diffusion process prior are intractable problems. We build upon work within variational inference approximating the posterior process as a linear diffusion process, point out pathologies in the approach, and propose an alternative parameterization of the Gaussian variational process using a continuous exponential family description. This allows us to trade a slow inference algorithm with fixed-point iterations for a fast algorithm for convex optimization akin to natural gradient descent, which also provides a better objective for the learning of model parameters.
    
[^260]: 通过监督式注意力多实例学习从多视角超声图像检测心脏病

    Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning. (arXiv:2306.00003v1 [eess.IV])

    [http://arxiv.org/abs/2306.00003](http://arxiv.org/abs/2306.00003)

    本研究提出了一种基于监督式注意力多实例学习的方法，可以自动分析超声图像，实现AS的精确筛查且精度优于传统机器学习和深度学习方法。

    

    主动脉瓣狭窄(AS)是一种导致严重发病率和死亡率的退行性瓣膜疾病。这种情况经常被低估和低治疗。在临床实践中，AS是通过超声心动图的专家审查来诊断的，这会产生数十个下肺采样的超声图像。只有一些视图显示主动脉瓣。为了自动化筛查AS，深度网络必须学习模仿人类专家识别主动脉瓣视图的能力，然后汇总这些相关图像以产生研究级诊断。我们发现先前的AS检测方法由于依赖于跨图像的不灵活平均值而导致精度不足。我们进一步发现，现成的基于注意力的多实例(MIL)学习表现不佳。我们提出了一种新的端到端MIL方法，包含两个关键方法创新。首先，通过监督式注意技术，引导学习的注意机制偏爱相关视图。其次，一种新颖的自我监督学习技术提高了每个单独图像的表现。我们的方法在一个真实的临床数据集（4569名患者）上实现了最先进的性能，在传统的机器学习和深度学习方法之上。

    Aortic stenosis (AS) is a degenerative valve condition that causes substantial morbidity and mortality. This condition is under-diagnosed and under-treated. In clinical practice, AS is diagnosed with expert review of transthoracic echocardiography, which produces dozens of ultrasound images of the heart. Only some of these views show the aortic valve. To automate screening for AS, deep networks must learn to mimic a human expert's ability to identify views of the aortic valve then aggregate across these relevant images to produce a study-level diagnosis. We find previous approaches to AS detection yield insufficient accuracy due to relying on inflexible averages across images. We further find that off-the-shelf attention-based multiple instance learning (MIL) performs poorly. We contribute a new end-to-end MIL approach with two key methodological innovations. First, a supervised attention technique guides the learned attention mechanism to favor relevant views. Second, a novel self-sup
    
[^261]: 结构化大离散动作空间的动态邻域构建

    Dynamic Neighborhood Construction for Structured Large Discrete Action Spaces. (arXiv:2305.19891v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19891](http://arxiv.org/abs/2305.19891)

    本研究针对无法处理的结构化大离散动作空间（SLDAS）提出了一种名为动态邻域构建（DNC）的新型利用策略，通过可扩展的邻域探索启发式方法，高效地探索连续代理动作周围的离散邻域。

    

    大离散动作空间（LDAS）是强化学习中的一个核心挑战。现有的解决方案可以处理多达几百万个动作的非结构化LDAS。然而，在物流、生产和运输系统等许多现实应用中，动作空间具有组合结构，其规模甚至在小规模实例上也超过了数百万个动作。幸运的是，这样的动作空间呈现出一定的结构，例如等间距的离散资源单位。在这项工作中，我们专注于处理当前基准测试无法处理的结构化LDAS（SLDAS）。我们提出了一种名为动态邻域构建（DNC）的新型利用策略，用于SLDAS。我们提出了一种可扩展的邻域探索启发式方法，利用这种策略，在具有高达$10^{73}$个动作的结构化动作空间中高效地探索连续代理动作周围的离散邻域。我们通过与三个标杆算法进行基准测试来展示我们方法的性能。

    Large discrete action spaces (LDAS) remain a central challenge in reinforcement learning. Existing solution approaches can handle unstructured LDAS with up to a few million actions. However, many real-world applications in logistics, production, and transportation systems have combinatorial action spaces, whose size grows well beyond millions of actions, even on small instances. Fortunately, such action spaces exhibit structure, e.g., equally spaced discrete resource units. With this work, we focus on handling structured LDAS (SLDAS) with sizes that cannot be handled by current benchmarks: we propose Dynamic Neighborhood Construction (DNC), a novel exploitation paradigm for SLDAS. We present a scalable neighborhood exploration heuristic that utilizes this paradigm and efficiently explores the discrete neighborhood around the continuous proxy action in structured action spaces with up to $10^{73}$ actions. We demonstrate the performance of our method by benchmarking it against three sta
    
[^262]: 诊断变压器：揭示临床决策中的特征空间。 (arXiv:2305.17588v2 [cs.CL] UPDATED)

    Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making. (arXiv:2305.17588v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17588](http://arxiv.org/abs/2305.17588)

    该论文介绍了一种名为SUFO的框架，用于增强微调的变压器模型的可解释性。该框架利用多种分析和可视化技术，解决了模型信任和可解释性的关键问题，并通过案例研究验证了其有效性。

    

    在医学等高风险领域，为了建立信任和确保安全，模型的可解释性至关重要，而使用有限的临床记录对预训练的变压器进行微调以辅助临床决策。我们引入了一种名为SUFO的系统框架，该框架增强了微调的变压器特征空间的可解释性。SUFO利用一系列分析和可视化技术，包括监督探索、无监督相似性分析、特征动态和异常值分析，来解决关于模型信任和可解释性的关键问题。我们进行了一个案例研究，研究了预训练数据对真实世界病理分类任务的影响，并在MedNLI上验证了我们的发现。我们评估了五个110M规模的预训练变压器模型，分为通用领域（BERT, TNLR）、混合领域（BioBERT, Clinical BioBERT）和领域特定（PubMedBERT）组。我们的SUFO分析揭示了：(1)

    Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial, especially in high-stakes domains like medicine, to establish trust and ensure safety, which requires human engagement. We introduce SUFO, a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. SUFO utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis to address key questions about model trust and interpretability. We conduct a case study investigating the impact of pre-training data where we focus on real-world pathology classification tasks, and validate our findings on MedNLI. We evaluate five 110M-sized pre-trained transformer models, categorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical BioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal that: (1
    
[^263]: 量子神经网络和张量网络在截面股票收益预测中的应用

    The cross-sectional stock return predictions via quantum neural network and tensor network. (arXiv:2304.12501v1 [cs.LG])

    [http://arxiv.org/abs/2304.12501](http://arxiv.org/abs/2304.12501)

    本文研究将量子神经网络和张量网络应用于股票收益预测，在日本股市中张量网络模型表现优于传统模型，并在最新市场环境下呈现出卓越表现。

    

    本文研究了利用量子和量子启发式的机器学习算法进行股票收益预测的应用。其中，我们将量子神经网络（一种适用于噪声中等规模量子计算机的算法）和张量网络（一种受量子启发的机器学习算法）的性能与传统模型如线性回归和神经网络进行比较。通过构建基于模型预测的投资组合并测量投资绩效，我们发现在日本股市中，张量网络模型表现优于传统基准模型（包括线性和神经网络模型）。虽然量子神经网络模型在整个周期内具有降低风险调整超额收益的能力，但最新的市场环境下，量子神经网络和张量网络模型均表现出卓越的性能。

    In this paper we investigate the application of quantum and quantum-inspired machine learning algorithms to stock return predictions. Specifically, we evaluate performance of quantum neural network, an algorithm suited for noisy intermediate-scale quantum computers, and tensor network, a quantum-inspired machine learning algorithm, against classical models such as linear regression and neural networks. To evaluate their abilities, we construct portfolios based on their predictions and measure investment performances. The empirical study on the Japanese stock market shows the tensor network model achieves superior performance compared to classical benchmark models, including linear and neural network models. Though the quantum neural network model attains the lowered risk-adjusted excess return than the classical neural network models over the whole period, both the quantum neural network and tensor network models have superior performances in the latest market environment, which sugges
    
[^264]: 基于深生成模型的多模态和多对比度图像融合

    Multimodal and multicontrast image fusion via deep generative models. (arXiv:2303.15963v1 [cs.LG])

    [http://arxiv.org/abs/2303.15963](http://arxiv.org/abs/2303.15963)

    该论文提出了一种基于变分自动编码器（VAE）的图像融合方法，可以整合多模态和多对比度的神经影像数据，以提高神经影像分析的分类性能。

    

    最近，人们逐渐意识到传统的诊断标签无法可靠地描述多种临床表型的复杂性和变异性，尤其是广泛的神经精神疾病（例如抑郁症、焦虑症、行为表型）。患者的异质性可以通过将个体根据经验得出的交织连续的新类别分组来更好地描述，这些连续跨越并超越了传统的类别边界。在这种情况下，神经影像数据携带着关于每个患者大脑的富含时空的信息。然而，它们通常会经过一系列事先未学习为模型的训练部分的过程进行预处理，因此没有针对下游预测任务进行优化。这是因为每个个体通常都有多个全脑三维成像模态，常伴随着深层的基因型和表型特征描述。为应对这些挑战，我们提出了一种基于深度生成模型的图像融合方法，以组合多模态和多对比度的神经影像数据。我们的方法使用变分自动编码器（VAE）来学习不同成像模态的共同表示空间，同时强制执行模态特定和对比度特定的编码-解码过程。我们在来自OPENNeuro数据库的174名MDD患者的数据集上评估了我们的方法，包括结构T1加权MRI、扩散张量成像（DTI）和静息功能MRI（rsfMRI）数据。我们的结果表明，所提出的方法有效地整合了多模态和多对比度的成像数据，相对于传统方法，分类表现得到了改善。

    Recently, it has become progressively more evident that classic diagnostic labels are unable to reliably describe the complexity and variability of several clinical phenotypes. This is particularly true for a broad range of neuropsychiatric illnesses (e.g., depression, anxiety disorders, behavioral phenotypes). Patient heterogeneity can be better described by grouping individuals into novel categories based on empirically derived sections of intersecting continua that span across and beyond traditional categorical borders. In this context, neuroimaging data carry a wealth of spatiotemporally resolved information about each patient's brain. However, they are usually heavily collapsed a priori through procedures which are not learned as part of model training, and consequently not optimized for the downstream prediction task. This is because every individual participant usually comes with multiple whole-brain 3D imaging modalities often accompanied by a deep genotypic and phenotypic char
    
[^265]: 私密、公平且精确：在医学影像中训练大规模隐私保护的人工智能模型

    Private, fair and accurate: Training large-scale, privacy-preserving AI models in medical imaging. (arXiv:2302.01622v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2302.01622](http://arxiv.org/abs/2302.01622)

    本研究评估了隐私保护训练医学影像人工智能模型的准确性和公平性，并与非隐私训练进行了比较。研究结果可为隐私保护技术的广泛应用提供重要参考。

    

    人工智能模型在医学领域的应用越来越多。然而，由于医学数据的高度敏感性，需要采取特殊措施确保其保护。保护隐私的黄金标准是引入差分隐私（DP）来进行模型训练。先前的研究表明，DP对模型的准确性和公平性有负面影响，这在医学中是不可接受的，并且是隐私保护技术广泛应用的主要障碍。在这项工作中，我们评估了隐私保护训练人工智能模型对准确性和公平性的影响，与非隐私训练进行了比较。为此，我们使用了两个数据集：（1）一个大规模数据集（N=193,311）的高质量临床胸部X射线图像，和（2）一个数据集（N=1,625）的3D腹部计算机断层扫描（CT）图像，用于分类胰腺导管腺癌（PDAC）的存在。两个数据集均为回顾性采集，并由经验丰富的医学影像专家进行手动标注。

    Artificial intelligence (AI) models are increasingly used in the medical domain. However, as medical data is highly sensitive, special precautions to ensure its protection are required. The gold standard for privacy preservation is the introduction of differential privacy (DP) to model training. Prior work indicates that DP has negative implications on model accuracy and fairness, which are unacceptable in medicine and represent a main barrier to the widespread use of privacy-preserving techniques. In this work, we evaluated the effect of privacy-preserving training of AI models regarding accuracy and fairness compared to non-private training. For this, we used two datasets: (1) A large dataset (N=193,311) of high quality clinical chest radiographs, and (2) a dataset (N=1,625) of 3D abdominal computed tomography (CT) images, with the task of classifying the presence of pancreatic ductal adenocarcinoma (PDAC). Both were retrospectively collected and manually labeled by experienced radio
    
[^266]: 相对熵正则化的经验风险最小化问题

    Empirical Risk Minimization with Relative Entropy Regularization. (arXiv:2211.06617v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2211.06617](http://arxiv.org/abs/2211.06617)

    本文研究了相对熵正则化的经验风险最小化问题，其中参考度量为sigma有限测度，解为唯一的概率测度并展现了几乎正确的保证。ERM-RER问题的解被称为Gibbs算法。

    

    在假定参考度量为sigma有限测度（measure）而非概率测度的情况下，研究了相对熵正则化的经验风险最小化（ERM-RER）问题。在这种假设下，存在一个ERM-RER问题的泛化，允许更大程度地灵活地并入先验知识。在这些性质中，如果存在ERM-RER问题的解，则该解是唯一的概率测度，通常与参考测度相互绝对连续。这样的解对于ERM问题展现了几乎正确的保证，而不需关心ERM问题是否有解。当从ERM-RER问题的解抽取模型时，固定数据集时，经验风险被证明是一个亚高斯随机变量。ERM-RER问题的解（Gibbs算法）的泛化能力得到了验证。

    The empirical risk minimization (ERM) problem with relative entropy regularization (ERM-RER) is investigated under the assumption that the reference measure is a {\sigma}-finite measure, and not necessarily a probability measure. Under this assumption, which leads to a generalization of the ERM-RER problem allowing a larger degree of flexibility for incorporating prior knowledge, numerous relevant properties are stated. Among these properties, the solution to this problem, if it exists, is shown to be a unique probability measure, often mutually absolutely continuous with the reference measure. Such a solution exhibits a probably-approximately-correct guarantee for the ERM problem independently of whether the latter possesses a solution. For a fixed dataset, the empirical risk is shown to be a sub-Gaussian random variable when the models are sampled from the solution to the ERM-RER problem. The generalization capabilities of the solution to the ERM-RER problem (the Gibbs algorithm) are
    
[^267]: GmGM: 一种快速的多轴高斯图形模型。

    GmGM: a Fast Multi-Axis Gaussian Graphical Model. (arXiv:2211.02920v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.02920](http://arxiv.org/abs/2211.02920)

    本文介绍了一种快速的多轴高斯图形模型，用于构建稀疏图形表示。相比先前工作，我们的算法在每个轴上仅使用一次特征分解，实现了数量级的加速。该模型可以应用于大型多模态数据集，包括单细胞多组学数据。

    

    本文介绍了一种高斯多图形模型，用于构建矩阵和张量变量数据的稀疏图形表示。我们通过同时学习多个共享轴的张量上的表示来推广该领域的先前工作，这对于分析多模态数据集（如多组学中遇到的数据集）是必要的。我们的算法在每个轴上仅使用一次特征分解，相对于非广义情况下的先前工作实现了数量级的加速。这使得我们的方法可以应用于包括单细胞多组学数据在内的大型多模态数据集，这在之前的方法中具有挑战性。我们在合成数据和五个真实数据集上验证了我们的模型。

    This paper introduces the Gaussian multi-Graphical Model, a model to construct sparse graph representations of matrix- and tensor-variate data. We generalize prior work in this area by simultaneously learning this representation across several tensors that share axes, which is necessary to allow the analysis of multimodal datasets such as those encountered in multi-omics. Our algorithm uses only a single eigendecomposition per axis, achieving an order of magnitude speedup over prior work in the ungeneralized case. This allows the use of our methodology on large multi-modal datasets such as single-cell multi-omics data, which was challenging with previous approaches. We validate our model on synthetic data and five real-world datasets.
    
[^268]: 带有核的复合适合性检验方法

    Composite Goodness-of-fit Tests with Kernels. (arXiv:2111.10275v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2111.10275](http://arxiv.org/abs/2111.10275)

    本文提出了一种基于核的假设检验方法，可以解决具有挑战性的复合检验问题，其核心思想是在正确的模型规范的零假设下，非参数地估计参数（或模拟器）分布。

    

    模型错误说明可能会对概率模型的实现造成重大挑战，这促使开发出一些直接解决此问题的鲁棒方法。但是，这些更为复杂的方法是否需要取决于模型是否真的错误，目前缺乏通用的方法回答这个问题。在本文中，我们提出了一种方法。更具体地说，我们提出了基于核的假设检验方法，用于具有挑战性的复合检验问题，即我们是否感兴趣的数据来自某些参数模型族中的任何分布。我们的测试利用基于最大均值差异和核Stein差异的最小距离估计器。它们具有广泛的适用性，包括当参数模型的密度已知除标准化常数外，或者如果模型采用模拟器形式。作为我们的主要结果，我们展示了在正确的模型规范的零假设下，我们能够非参数地估计参数（或模拟器）分布。我们提供了建立我们方法有效性的理论，并通过模拟和异常检测应用案例演示了其性能。

    Model misspecification can create significant challenges for the implementation of probabilistic models, and this has led to development of a range of robust methods which directly account for this issue. However, whether these more involved methods are required will depend on whether the model is really misspecified, and there is a lack of generally applicable methods to answer this question. In this paper, we propose one such method. More precisely, we propose kernel-based hypothesis tests for the challenging composite testing problem, where we are interested in whether the data comes from any distribution in some parametric family. Our tests make use of minimum distance estimators based on the maximum mean discrepancy and the kernel Stein discrepancy. They are widely applicable, including whenever the density of the parametric model is known up to normalisation constant, or if the model takes the form of a simulator. As our main result, we show that we are able to estimate the param
    
[^269]: 用机器学习在未知流形上解PDE问题

    Solving PDEs on Unknown Manifolds with Machine Learning. (arXiv:2106.06682v3 [math.NA] UPDATED)

    [http://arxiv.org/abs/2106.06682](http://arxiv.org/abs/2106.06682)

    本文提出了一种在未知流形上解椭圆型PDE问题的机器学习方法，通过扩散映射和深度学习，构建了一个无网格计算框架。通过将PDE求解转化为监督学习任务，采用基于神经网络的最小二乘回归来近似求解代数方程，得到了一致估计量，最终得到的数值方法在极限情况下是一致解。

    

    本文提出了一种基于扩散映射和深度学习的无网格计算框架和机器学习理论，用于在未知流形上解椭圆型PDE问题，其中流形用点云表示。PDE求解器被形式化为一个监督学习任务，以解决一个最小二乘回归问题，该问题通过近似表示一个PDE（以及边界条件，如果适用）的代数方程。这个代数方程涉及到通过扩散映射渐进展开得到的图拉普拉斯矩阵，它是一个关于二阶椭圆微分算子的一致估计量。最终得到的数值方法是解决一个高度非凸的经验风险最小化问题，该问题限制在神经网络假设空间的解空间内。在良定义的椭圆型PDE设置中，当假设空间包括无限宽度或深度的神经网络时，我们证明经验损失函数的全局最小值是极限情况下的一致解。

    This paper proposes a mesh-free computational framework and machine learning theory for solving elliptic PDEs on unknown manifolds, identified with point clouds, based on diffusion maps (DM) and deep learning. The PDE solver is formulated as a supervised learning task to solve a least-squares regression problem that imposes an algebraic equation approximating a PDE (and boundary conditions if applicable). This algebraic equation involves a graph-Laplacian type matrix obtained via DM asymptotic expansion, which is a consistent estimator of second-order elliptic differential operators. The resulting numerical method is to solve a highly non-convex empirical risk minimization problem subjected to a solution from a hypothesis space of neural networks (NNs). In a well-posed elliptic PDE setting, when the hypothesis space consists of neural networks with either infinite width or depth, we show that the global minimizer of the empirical loss function is a consistent solution in the limit of l
    

