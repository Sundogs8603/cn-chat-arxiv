# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [LipsFormer: Introducing Lipschitz Continuity to Vision Transformers.](http://arxiv.org/abs/2304.09856) | 本文介绍了一种Lipschitz连续的Transformer模型LipsFormer，其中引入了多项Lipschitz连续的组件来保证训练的稳定性，并在实验证明其可以让深度Transformer架构稳定进行训练，无需过多调整学习率等参数。 |
| [^2] | [Bridging RL Theory and Practice with the Effective Horizon.](http://arxiv.org/abs/2304.09853) | 本论文通过对常见深度强化学习测试基准中155个MDP的数据集进行分析，发现当最高Q值的动作在随机策略下Q值最高时，深度强化学习往往会成功；反之，则失败的可能性较高。 |
| [^3] | [Points of non-linearity of functions generated by random neural networks.](http://arxiv.org/abs/2304.09837) | 论文从神经网络模型的参数分布出发，探究网络输出函数几何形状的简单性与信息熵复杂度的关系。 |
| [^4] | [Regions of Reliability in the Evaluation of Multivariate Probabilistic Forecasts.](http://arxiv.org/abs/2304.09836) | 本研究通过有限样本和功率分析确定了多元概率时间序列预测评分规则的可靠性区域，并在电力生产问题上评估了结果对真实世界任务的普适性。 |
| [^5] | [Towards transparent and robust data-driven wind turbine power curve models.](http://arxiv.org/abs/2304.09835) | 该研究利用可解释的人工智能框架研究了基于数据驱动的风力涡轮机功率曲线模型的稳健性。结果表明，学习到的策略可以更好地指示模型的稳健性。高度复杂的机器学习模型容易学习到物理上不合理的策略。 |
| [^6] | [Fairness in AI and Its Long-Term Implications on Society.](http://arxiv.org/abs/2304.09826) | 本文探讨了AI的公平性问题，指出缺乏AI公平性会加深偏见成为社会压力因素，可能对社会产生长期影响，因此需要寻求潜在解决方案。 |
| [^7] | [Using Offline Data to Speed-up Reinforcement Learning in Procedurally Generated Environments.](http://arxiv.org/abs/2304.09825) | 本研究旨在提高程序生成环境中强化学习的样本效率。研究证明，使用模仿学习进行预训练和同时进行模仿学习和在线强化学习的方法可以提高效率。 |
| [^8] | [Leveraging Deep Reinforcement Learning for Metacognitive Interventions across Intelligent Tutoring Systems.](http://arxiv.org/abs/2304.09821) | 本研究比较了两种方法来提供元认知干预，并发现基于深度强化学习的自适应干预能够缩小学生之间的元认知技能差距。 |
| [^9] | [Generalization and Estimation Error Bounds for Model-based Neural Networks.](http://arxiv.org/abs/2304.09802) | 基于模型的神经网络在稀疏恢复中表现出较高的泛化能力，复杂度量有助于提高其泛化和估计误差界限 |
| [^10] | [Advances on Concept Drift Detection in Regression Tasks using Social Networks Theory.](http://arxiv.org/abs/2304.09788) | 本文介绍了一种基于社交网络理论的动态集成型回归算法(SFNR)，使用自适应窗口(ADWIN)算法检测概念漂移，结果表明在概念漂移情况下SFNR的准确性更好且表现超过其他现有算法。 |
| [^11] | [Equalised Odds is not Equal Individual Odds: Post-processing for Group and Individual Fairness.](http://arxiv.org/abs/2304.09779) | 研究发现，通过平等化受保护子群体之间的预测分布来实现组公平和视相似个体同等对待实现个人公正是不兼容的。 并提出了一种构建连续概率函数的解决方法，来实现组和个人公平。 |
| [^12] | [Practical Differentially Private and Byzantine-resilient Federated Learning.](http://arxiv.org/abs/2304.09762) | 本文提出了一种结合了差分隐私和拜占庭容错的联邦学习方法，在保护隐私的同时，提高系统的鲁棒性，拒绝多种攻击方法，具有较高的隐私保护水平。 |
| [^13] | [An innovative Deep Learning Based Approach for Accurate Agricultural Crop Price Prediction.](http://arxiv.org/abs/2304.09761) | 本文提出了一种基于深度学习的方法，利用历史价格信息、气候条件、土壤类型、地理位置和其他关键决策因素，精确预测农产品价格。该方法使用图神经网络和卷积神经网络相结合，适用于有噪声的历史数据，并且表现至少比现有文献中的结果好20%。 |
| [^14] | [Amplifying Sine Unit: An Oscillatory Activation Function for Deep Neural Networks to Recover Nonlinear Oscillations Efficiently.](http://arxiv.org/abs/2304.09759) | 这篇论文介绍了一种能够处理微机电系统中非线性振荡的神经网络方法，使用一种名为增幅正弦单元（ASU）的新型脉冲函数明显优于其他激活函数。 |
| [^15] | [K-means Clustering Based Feature Consistency Alignment for Label-free Model Evaluation.](http://arxiv.org/abs/2304.09758) | 本文提出了一种名为KCFCA的新方法，该方法利用K-means算法来对标记的训练集和无标签的测试集进行聚类，并通过特征一致性来对齐聚类中心。另外，文章还设计了动态回归模型来研究分布偏移与模型准确性之间的关系以及排除异常模型。这些方法为无标签模型评估提供了更加全面准确的方法。 |
| [^16] | [Attributing Image Generative Models using Latent Fingerprints.](http://arxiv.org/abs/2304.09752) | 本文研究了一种使用潜在语义维度作为指纹的追溯方法，可以分析设计变量对于准确性-质量权衡的影响，在保证准确性的同时最小化计算量，更适用于大规模模型。 |
| [^17] | [Skeleton-based action analysis for ADHD diagnosis.](http://arxiv.org/abs/2304.09751) | 提出一种基于骨架的动作识别框架用于注意力缺陷多动障碍的诊断，其显示出成本效益和显著的性能改进，是一种低成本的初始ADHD诊断方法。 |
| [^18] | [Application of Tensor Neural Networks to Pricing Bermudan Swaptions.](http://arxiv.org/abs/2304.09750) | 本论文用张量神经网络(TNN)对百慕大掉期进行定价，相比于传统方法，TNN具有更快的收敛速度和减少参数敏感度的优点。 |
| [^19] | [Sample-efficient Model-based Reinforcement Learning for Quantum Control.](http://arxiv.org/abs/2304.09718) | 本论文提出了一种基于模型的强化学习方法，通过受到神经常微分方程进展的启发，这个方法采用自动微分的ODE表达由可学习的汉密尔顿安排参数化的模型来近似环境，在门控制和汉密尔顿参数的学习中通过系统交互解决问题。该方法在样本复杂度方面比标准基于模型自由的强化学习方法具有一个数量级的优势，适用于噪声时变门优化。 |
| [^20] | [Disentangling Neuron Representations with Concept Vectors.](http://arxiv.org/abs/2304.09707) | 本文提出了一种方法，可以将多义神经元解开为封装不同特征的概念向量，这些向量编码了连贯的、人类可理解的特征。 |
| [^21] | [Big-Little Adaptive Neural Networks on Low-Power Near-Subthreshold Processors.](http://arxiv.org/abs/2304.09695) | 本文研究了近阈值处理器在边缘人工智能应用中的能量节省，并提出大-小自适应神经网络策略以提高电池寿命和保持预测准确性。 |
| [^22] | [Generative Modeling of Time-Dependent Densities via Optimal Transport and Projection Pursuit.](http://arxiv.org/abs/2304.09663) | 本文提出了一种基于最优输运和投影追踪的方法，用于便宜、高效地生成时态密度建模，其最优映射与恒等映射接近，训练过程高度并行化。 |
| [^23] | [Quantum deep Q learning with distributed prioritized experience replay.](http://arxiv.org/abs/2304.09648) | 本论文提出了 QDQN-DPER 框架，它将分布式优先经验回放和异步训练纳入算法，以提高量子强化学习在解决顺序决策任务中的效率。 |
| [^24] | [AdapterGNN: Efficient Delta Tuning Improves Generalization Ability in Graph Neural Networks.](http://arxiv.org/abs/2304.09595) | 本文提出了一种专为图神经网络设计的δ调节方法——AdapterGNN，该方法保留了预训练模型的知识，利用高度表达的适配器能够在仅有少量参数的情况下有效地适应下游任务，并提高模型的泛化能力，实验结果表明其在多个基准数据集上取得了最优性能。 |
| [^25] | [Leveraging the two timescale regime to demonstrate convergence of neural networks.](http://arxiv.org/abs/2304.09576) | 研究了双时间尺度制度下浅层神经网络的训练动态，证明了梯度流收敛于全局最优解，无需神经元数量趋于无限，并提供了实验证明。 |
| [^26] | [Approximate non-linear model predictive control with safety-augmented neural networks.](http://arxiv.org/abs/2304.09575) | 本文提出了一种基于神经网络(NNs)的非线性模型预测控制(MPC)的近似方法，称为安全增强，可以使解决方案在线可行并具有收敛和约束条件的确定保证。 |
| [^27] | [The State-of-the-Art in Air Pollution Monitoring and Forecasting Systems using IoT, Big Data, and Machine Learning.](http://arxiv.org/abs/2304.09574) | 本文综述了基于物联网、大数据和机器学习的空气污染监测和预测系统的现状。最近的研究使用这些技术来提高空气质量数据的收集、分析和预测模型，以实现更高的效率和可靠性。 |
| [^28] | [Denoising Cosine Similarity: A Theory-Driven Approach for Efficient Representation Learning.](http://arxiv.org/abs/2304.09552) | 本文提出了一种去噪余弦相似度（dCS）损失函数， 可以用于在原始数据集中学习鲁棒的表示形式。 |
| [^29] | [SemEval 2023 Task 6: LegalEval -- Understanding Legal Texts.](http://arxiv.org/abs/2304.09548) | SemEval 2023举办了LegalEval共享任务，即理解法律文本，包括 自动结构化和语义连贯化的法律文件（Task-A），法律命名实体识别（Task-B）以及自动预测法律案件结果和提供预测解释（Task-C）。26个团队提交了系统论文并在所有子任务中优于基准线，但仍有改进空间。 |
| [^30] | [Graph Exploration for Effective Multi-agent Q-Learning.](http://arxiv.org/abs/2304.09547) | 本文提出了一种基于图通信的多智能体强化学习探索技术，在智能体的协作下估计状态行为空间不确定性，以实现更有效的探索行为，同时也不需要计数机制和复杂的转换技术，该方案允许智能体在完全分散的方式下进行通信。 |
| [^31] | [SelfAct: Personalized Activity Recognition based on Self-Supervised and Active Learning.](http://arxiv.org/abs/2304.09530) | SelfAct是一种基于自我监督和主动学习的人体活动识别框架，可以用大量未标记数据进行预训练，并通过新的无监督主动学习策略进行微调，从而实现对每个用户的个性化活动识别。 |
| [^32] | [Secure Split Learning against Property Inference, Data Reconstruction, and Feature Space Hijacking Attacks.](http://arxiv.org/abs/2304.09515) | 本论文研究了分布式学习面临的安全问题并提出了一种隐私保护通道，使用新的激活函数R3eLU避免属性推断、数据重组和特征劫持等攻击。实验表明，该方法在保护隐私的同时，性能相当于不保护隐私的最新方法。 |
| [^33] | [NetGPT: Generative Pretrained Transformer for Network Traffic.](http://arxiv.org/abs/2304.09513) | 本文提出了首个网络流量生成预训练变压器模型NetGPT，该模型可以优化网络任务的训练效率和有效性。 |
| [^34] | [Community Detection Using Revised Medoid-Shift Based on KNN.](http://arxiv.org/abs/2304.09512) | 在本文中，作者提出了一种名为修正Medoid-Shift（RMS）的新聚类算法，用于社区检测，该算法可以更好地解决社交网络中的问题。 |
| [^35] | [The Responsibility Problem in Neural Networks with Unordered Targets.](http://arxiv.org/abs/2304.09499) | 本文解决了无序目标下神经网络的不连续性问题，并鼓励进一步研究。 |
| [^36] | [Learning Resource Scheduling with High Priority Users using Deep Deterministic Policy Gradients.](http://arxiv.org/abs/2304.09488) | 本文探索了利用深度确定性策略梯度（\ddpg）方法学习通信资源调度算法的应用，特别是限制于优先用户。 |
| [^37] | [Security and Privacy Problems in Voice Assistant Applications: A Survey.](http://arxiv.org/abs/2304.09486) | 语音助手应用面临安全和隐私问题，包括攻击模型和私人信息泄露，所以需要进行全面的调查以便概览当前研究。 |
| [^38] | [DiFaReli : Diffusion Face Relighting.](http://arxiv.org/abs/2304.09479) | DiFaReli提出了一种新方法，通过利用条件扩散隐式模型解码解耦的光编码以及从现成的估算器推断出的与3D形状和面部身份相关的其他编码，能够处理单视角的野外环境下的人脸重照，无需光线舞台数据、多视图图像或光照基础事实，实验表明其效果优于现有方法。 |
| [^39] | [MAMAF-Net: Motion-Aware and Multi-Attention Fusion Network for Stroke Diagnosis.](http://arxiv.org/abs/2304.09466) | 本研究提出了一个名为MAMAF-Net的网络用于检测多模态检查视频中的中风情况，并提出了一个多数采样的数据集。这是第一个提供端到端解决方案的视频分析中的中风检测研究。 |
| [^40] | [EC^2: Emergent Communication for Embodied Control.](http://arxiv.org/abs/2304.09448) | EC^2 提出一种新的紧急通信方案，用于视频语言预训练以进行少样本身体控制，实现了在 EmbodiedAI 基准测试上的最先进的少样本性能。 |
| [^41] | [Martingale Posterior Neural Processes.](http://arxiv.org/abs/2304.09431) | 本文提出了一种基于鞅后验的神经过程方法，用于估计使用神经网络隐式定义的随机过程，并在 benchmark 数据集上表现出更高的精度和样本效率。 |
| [^42] | [Decoupled Training for Long-Tailed Classification With Stochastic Representations.](http://arxiv.org/abs/2304.09426) | 本文提出了解耦训练和利用随机表示法进行分类的方法来解决长尾分类问题，使用SWA优化技术得到更好的特征提取器，并提出一种基于随机表示进行分类器重新训练的算法。 |
| [^43] | [Loss minimization yields multicalibration for large neural networks.](http://arxiv.org/abs/2304.09424) | 本文展示了对于大型神经网络大小，最优地最小化损失会导致多校准，以提供公平的预测结果。 |
| [^44] | [TieFake: Title-Text Similarity and Emotion-Aware Fake News Detection.](http://arxiv.org/abs/2304.09421) | 该论文提出了一种新颖的假新闻检测方法TieFake，通过检测新闻标题和正文之间的相似度和情感信息，同时通过BERT和ResNeSt学习文本和图像的表示，实现对多模态文本的假新闻检测。 |
| [^45] | [Wavelets Beat Monkeys at Adversarial Robustness.](http://arxiv.org/abs/2304.09403) | 该研究旨在探讨如何开发出能够像人类视觉一样鲁棒地概括的神经网络，提高对抗性噪声的鲁棒性，并提出了一种模仿灵长类动物视觉皮层(V1)的神经网络模型，其在小扰动下具有非平凡的对抗鲁棒性，该模型使用小波方法。 |
| [^46] | [MixPro: Simple yet Effective Data Augmentation for Prompt-based Learning.](http://arxiv.org/abs/2304.09402) | MixPro是一种数据增强方法，通过对原始输入和模板进行混合来提高基于提示的学习性能，平均提高了5.08%的模型性能。 |
| [^47] | [Information Geometrically Generalized Covariate Shift Adaptation.](http://arxiv.org/abs/2304.09387) | 该论文基于信息几何框架统一了协变量漂移适应家族，提出了一种有效的几何广义协变量漂移适应方法，并在数值实验中展示了其优越性能。 |
| [^48] | [Physical Knowledge Enhanced Deep Neural Network for Sea Surface Temperature Prediction.](http://arxiv.org/abs/2304.09376) | 该论文提出了一种从历史观测数据中转移物理知识到数值模型的海表温度预测方法，旨在提高预测准确性。 |
| [^49] | [Shuffle & Divide: Contrastive Learning for Long Text.](http://arxiv.org/abs/2304.09374) | 本文介绍了一种基于对比学习的自监督学习方法，通过“洗牌和切割”算法对长文本进行预处理，提取BERT嵌入，在无监督的情况下对文本进行分类，比当前最先进的技术提高20.94%。 |
| [^50] | [ContraCluster: Learning to Classify without Labels by Contrastive Self-Supervision and Prototype-Based Semi-Supervision.](http://arxiv.org/abs/2304.09369) | ContraCluster是一种无监督的图像分类方法，结合了聚类和对比自监督学习。ContraCluster使用对比原型采样和基于原型的半监督微调来提高准确性，并且在CIFAR-10等标准基准数据集上实现了新的最先进结果。 |
| [^51] | [Graph Neural Network-Based Anomaly Detection for River Network Systems.](http://arxiv.org/abs/2304.09367) | 本研究采用图神经网络模型Graph Deviation Network (GDN)来捕捉河流传感器数据的复杂时空关系，并提出了备用异常阈值标准GDN+，以实现对水质的准确持续监测。 |
| [^52] | [Long-Term Fairness with Unknown Dynamics.](http://arxiv.org/abs/2304.09362) | 本文提出一种新方法，通过在未知动态下追求长期公平性，实现算法的动态适应和权衡，可为分类器-人群系统推向更理想的平衡。 |
| [^53] | [Investigating the Nature of 3D Generalization in Deep Neural Networks.](http://arxiv.org/abs/2304.09358) | 本论文研究了深度学习架构对新视图推广的能力，发现深度模型具有很好的推广能力，但它们的方式与所有现有模型不同。 |
| [^54] | [To Compress or Not to Compress -- Self-Supervised Learning and Information Theory: A Review.](http://arxiv.org/abs/2304.09355) | 本文从信息论的角度回顾了各种自监督学习方法，并提出了一个正式的“自监督信息理论学习问题”统一框架。此外，讨论了压缩性和压缩算法在自监督学习中的作用，并凸显了潜在的未来方向。 |
| [^55] | [Learning to Transmit with Provable Guarantees in Wireless Federated Learning.](http://arxiv.org/abs/2304.09329) | 该研究提出了一种数据驱动方法，用于分配联邦学习中的发射功率，以优化通信约束下FL过程中服务器端接收到的信息，并提高全局FL模型的准确性和效率。 |
| [^56] | [Federated Alternate Training (FAT): Leveraging Unannotated Data Silos in Federated Segmentation for Medical Imaging.](http://arxiv.org/abs/2304.09327) | 本文提出了一种联邦交替训练(FAT)框架，它可以在带有和不带有地面真值标签的数据库之间交替训练，利用未标注的数据来辅助模型学习，适用于医学图像领域。 |
| [^57] | [Multi-Modality Multi-Scale Cardiovascular Disease Subtypes Classification Using Raman Image and Medical History.](http://arxiv.org/abs/2304.09322) | 本研究提出了一种名为 M3S 的多模态多尺度模型，通过 Gramian Angular Summation Field 方法将 Raman 光谱数据转换为各种分辨率的图像丰富了其表示，并引入病史信息作为辅助输入，最终在四种 CVD 亚型的大型数据集上取得了95.71%的准确率。 |
| [^58] | [Deep Dynamic Cloud Lighting.](http://arxiv.org/abs/2304.09317) | 该论文提出一种深度云光照模型，能够模拟整个天空的云运动，提高云天空照明方法的动态性。 |
| [^59] | [The Adaptive $\tau$-Lasso: Its Robustness and Oracle Properties.](http://arxiv.org/abs/2304.09310) | 本文提出了一种新型鲁棒的自适应 $\tau$-Lasso 估计器，同时采用自适应 $\ell_1$-范数惩罚项以降低真实回归系数的偏差。它具有变量选择一致性和真实支持下回归向量的渐近正态性的最优性质，假定已知真实回归向量的支持。 |
| [^60] | [Searching for ribbons with machine learning.](http://arxiv.org/abs/2304.09304) | 用机器学习发现带状物，反驳四维平凡 Poincaré 猜想。 |
| [^61] | [Towards Spatio-temporal Sea Surface Temperature Forecasting via Static and Dynamic Learnable Personalized Graph Convolution Network.](http://arxiv.org/abs/2304.09290) | 该论文提出了基于静态和动态可学习个性化图卷积网络的时空海表温度预测方法，其中利用两个图学习层分别模型化了SST数据的固定网络和动态网络，并设计了个性化的图卷积网络层以精确预测时空变化。实验结果显示，该方法在预测准确度方面优于目前最先进的方法。 |
| [^62] | [Pelphix: Surgical Phase Recognition from X-ray Images in Percutaneous Pelvic Fixation.](http://arxiv.org/abs/2304.09285) | 本文提出了一种名为Pelphix的X光引导下的经皮盆骨折修复手术阶段识别方法，使用马尔科夫过程模拟过程并提供完全注释的训练数据，在四个粒度级别上回归手术阶段，并取得了很好的准确率。 |
| [^63] | [A Data Driven Sequential Learning Framework to Accelerate and Optimize Multi-Objective Manufacturing Decisions.](http://arxiv.org/abs/2304.09278) | 本文提出了一种利用序列学习来高效优化多个相互冲突目标的复杂系统的数据驱动贝叶斯优化框架。 |
| [^64] | [A Neural Lambda Calculus: Neurosymbolic AI meets the foundations of computing and functional programming.](http://arxiv.org/abs/2304.09276) | 本文提出了一种神经λ演算法，使用λ语言编程，研究神经网络在执行整个程序的能力，旨在拓展神经网络在符号人工智能领域的应用。 |
| [^65] | [Coarse race data conceals disparities in clinical risk score performance.](http://arxiv.org/abs/2304.09270) | 研究发现仅依赖粗糙的种族类别可能掩盖了临床风险评分表现中的重要差异，需要更精细的种族数据采集。 |
| [^66] | [Heterogeneous Integration of In-Memory Analog Computing Architectures with Tensor Processing Units.](http://arxiv.org/abs/2304.09258) | 本文介绍了一种将内存模拟计算（IMAC）单元与张量处理单元（TPUs）集成的新型、异构、混合信号和混合精度架构，以提高移动CNN性能。结合混合精度训练技术，充分利用TPUs在卷积层中的优点和IMAC电路在密集层中的优势，实现了高达...的性能提升。 |
| [^67] | [IMAC-Sim: A Circuit-level Simulator For In-Memory Analog Computing Architectures.](http://arxiv.org/abs/2304.09252) | IMAC-Sim 是一种针对内存模拟模拟计算架构的电路级模拟器，可以根据用户指定的超参数快速创建电路，并自动评估其准确性、功耗和延迟，同时还考虑了互连寄生电阻和电容问题。 |
| [^68] | [Early Detection of Parkinson's Disease using Motor Symptoms and Machine Learning.](http://arxiv.org/abs/2304.09245) | 本研究利用机器学习算法分析运动症状和步态相关参数，以筛选出有效的生物标志物，进而实现帕金森病的早期检测。该方法基于经济和稳健的可穿戴设备，模型准确率高达91.9％。 |
| [^69] | [A Framework for Analyzing Online Cross-correlators using Price's Theorem and Piecewise-Linear Decomposition.](http://arxiv.org/abs/2304.09242) | 本文提出了一种使用非线性函数构建交叉相关器的方法，并使用Price定理和分段线性分解提出了一个数学框架来分析这种交叉相关器。 |
| [^70] | [Quantum machine learning for image classification.](http://arxiv.org/abs/2304.09224) | 本论文提出了两种混合量子-经典的神经网络模型用于图像分类，其中包括一个具有并行量子层的神经网络和一个具有量子卷积层的神经网络。其中一个混合量子方法在MNIST数据集上展现了超过99%的惊人准确率。 |
| [^71] | [Convergence of stochastic gradient descent under a local Lajasiewicz condition for deep neural networks.](http://arxiv.org/abs/2304.09221) | 本文通过随机梯度下降算法研究了解析度函数为非凸的深度神经网络的全局收敛性，证明了当机器学习噪声的尺度与目标函数相等时，在局部区域内初始化后，以正的概率能够收敛到该区域内的全局最小值。 |
| [^72] | [A Deep Learning Framework for Traffic Data Imputation Considering Spatiotemporal Dependencies.](http://arxiv.org/abs/2304.09182) | 该论文提出了一种考虑时空依赖关系的交通数据填补深度学习框架，可用于解决缺失或不完整数据问题，以进一步应用该数据。 |
| [^73] | [Alzheimers Disease Diagnosis using Machine Learning: A Review.](http://arxiv.org/abs/2304.09178) | 本综述分析了机器学习在阿尔茨海默病诊断中的应用，深度学习和强化学习是研究热点，可用于早期病变检测和诊断。 |
| [^74] | [Enhancing Personalized Ranking With Differentiable Group AUC Optimization.](http://arxiv.org/abs/2304.09176) | 本文提出了一种个性化和可微分的AUC优化方法（PDAOM），可用于训练二元分类器并向其提供在独立用户组中紧密相关的正负样本对，以促进分类器关注不易区分的样本之间的关系，这些方法不仅提高了AUC和GAUC指标，还减少了训练目标的计算复杂度。 |
| [^75] | [Memento: Facilitating Effortless, Efficient, and Reliable ML Experiments.](http://arxiv.org/abs/2304.09175) | Memento是一个Python包，旨在协助研究人员高效地管理和执行计算密集型机器学习实验。它提供了简单明了的配置矩阵和并发运行实验的能力。 |
| [^76] | [CF-VAE: Causal Disentangled Representation Learning with VAE and Causal Flows.](http://arxiv.org/abs/2304.09010) | 本文提出了一种新的因果流以进行因果分离表示学习，设计了一个新模型CF-VAE，利用因果流增强了VAE编码器的分离能力，并展示了在合成和真实数据集上实现因果分离并进行干预实验的结果。 |
| [^77] | [Joint Age-based Client Selection and Resource Allocation for Communication-Efficient Federated Learning over NOMA Networks.](http://arxiv.org/abs/2304.08996) | 本文针对联邦学习在无线网络上通信受限、收敛速度慢和资源有限等问题，提出了一种基于年龄和资源分配的客户端选择方案，旨在最小化每轮联邦学习的总时间消耗，从而提高联邦学习的性能。 |
| [^78] | [Implicit representation priors meet Riemannian geometry for Bayesian robotic grasping.](http://arxiv.org/abs/2304.08805) | 该研究利用隐式表示构建了场景相关先验，从而在不规则环境中利用基于模拟的高效贝叶斯推理算法成功识别抓取姿态。 |
| [^79] | [EEGSN: Towards Efficient Low-latency Decoding of EEG with Graph Spiking Neural Networks.](http://arxiv.org/abs/2304.07655) | 该论文提出了一种名为 EEGSN 的图形脉冲神经网络（SNN）架构，面向多通道 EEG 分类任务，在学习分布式 EEG 传感器中的动态关系信息的同时，将推断计算复杂度降低了20倍，为低延迟和功耗效率的脑计算机接口的开发提供了一个可行的框架。 |
| [^80] | [Understanding Overfitting in Adversarial Training in Kernel Regression.](http://arxiv.org/abs/2304.06326) | 本文研究了核回归的对抗训练和带噪声的数据增强，发现如果没有适当的正则化，这两种方法可能会导致过拟合现象，但适当的正则化可以缓解这种现象，提高性能。 |
| [^81] | [A Comprehensive Survey on Deep Graph Representation Learning.](http://arxiv.org/abs/2304.05055) | 本文综述了深度图表示学习的研究现状和存在的问题，并指出利用深度学习已经显示出巨大的优势和潜力。 |
| [^82] | [Bayesian optimization for sparse neural networks with trainable activation functions.](http://arxiv.org/abs/2304.04455) | 本文提出了一种可训练的激活函数以提高神经网络性能，在此基础上开发了一个基于贝叶斯优化和MCMC采样的模型，能通过有效的采样和全局优化来解决过拟合并提高收敛速度。 |
| [^83] | [GPT detectors are biased against non-native English writers.](http://arxiv.org/abs/2304.02819) | 该研究发现，GPT检测器对非英语母语作者存在偏见，容易将其内容错误地分类为AI生成的内容。此外，简单的提示策略可以缓解这种偏见，同时规避GPT检测器，这表明GPT检测器可能会惩罚具有受限语言表达能力的作者。 |
| [^84] | [Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers.](http://arxiv.org/abs/2304.00215) | 本文提出了一种基于分层Transformer的方法，即REPORT，能够同时聚合关系路径和上下文，捕捉实体之间的联系和内在特性。它完全依赖于关系语义，并能自然地推广到完全归纳的设置中。在基准数据集上实现了最先进的性能。 |
| [^85] | [From Private to Public: Benchmarking GANs in the Context of Private Time Series Classification.](http://arxiv.org/abs/2303.15916) | 本论文在时间序列领域对两种GAN架构进行了评估，结果以GSWGAN表现最佳，可以私密地生成保护数据隐私的公共数据。 |
| [^86] | [Unknown Sniffer for Object Detection: Don't Turn a Blind Eye to Unknown Objects.](http://arxiv.org/abs/2303.13769) | 本文提出了未知嗅探器(UnSniffer)，用于同时寻找未知和已知的目标。通过引入广义物体置信度(GOC)分数和负能量抑制损失来提高未知对象在背景中的检测准确率，并解决了在推断过程中难以获得每个未知目标最佳框的问题。 |
| [^87] | [Enabling Calibration In The Zero-Shot Inference of Large Vision-Language Models.](http://arxiv.org/abs/2303.12748) | 本文研究了零样本推理中视觉语言模型的校准问题，发现CLIP存在误校准，并提出了一种修改版的温度缩放方法，可以适用于每个特定的CLIP模型。 |
| [^88] | [Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament.](http://arxiv.org/abs/2303.07925) | 本文采用 Numerai 数据科学竞赛的数据，探究了多变量时间序列建模中不同特征工程和降维方法的应用；提出了一种新的集成方法，用于高维时间序列建模，该方法在通用性、鲁棒性和效率上优于一些深度学习模型。 |
| [^89] | [TSMixer: An all-MLP Architecture for Time Series Forecasting.](http://arxiv.org/abs/2303.06053) | TSMixer是一种通过堆叠多层感知器（MLP）设计的新型结构，基于沿时间和特征维度的混合操作，能够在时间序列预测中表现出极好的性能。 |
| [^90] | [Ensemble Reinforcement Learning: A Survey.](http://arxiv.org/abs/2303.02618) | 集成强化学习（ERL）是一种将强化学习（RL）与集成学习（EL）相结合的有前途的方法，旨在利用多个模型或培训算法全面探索问题空间，并具有强大的泛化能力。 |
| [^91] | [RAFEN -- Regularized Alignment Framework for Embeddings of Nodes.](http://arxiv.org/abs/2303.01926) | RAFEN是一个节点嵌入的正则化对齐框架，可以在训练过程中学习对齐节点嵌入，而不需要额外的参数设置。RAFEN在现有方法上实现了与更好的性能，并在动态图的情况下可以保持节点嵌入的可比性。 |
| [^92] | [The In-Sample Softmax for Offline Reinforcement Learning.](http://arxiv.org/abs/2302.14372) | 本文研究离线强化学习中的In-Sample Softmax，通过使用只由数据集中的操作组成的In-Sample softmax来解决操作覆盖不足问题，并且In-Sample Actor-Critic与该方法相比在稳定性或性能上表现更好。 |
| [^93] | [A Privacy-Preserving Hybrid Federated Learning Framework for Financial Crime Detection.](http://arxiv.org/abs/2302.03654) | 本论文提出了一种隐私保护混合联合学习框架，它结合中心化和分布式学习的优点，可以用于检测金融犯罪并保护数据隐私。 |
| [^94] | [Fine-tuning Neural-Operator architectures for training and generalization.](http://arxiv.org/abs/2301.11509) | 本文全面分析了神经算符及其衍生结构的泛化特性并提出了改进方法，包括引入核积分算符来代替自关注机制和逐渐增加模型容量的训练课程，结果显著提高了性能和泛化能力。 |
| [^95] | [Toward Foundation Models for Earth Monitoring: Generalizable Deep Learning Models for Natural Hazard Segmentation.](http://arxiv.org/abs/2301.09318) | 该论文提出的基于深度学习的自然灾害分割模型在透明云，烟雾柱和洪水分割任务中实现了最先进的性能，并通过在适当的预训练任务上进行预训练，显着提高了模型的通用性。 |
| [^96] | [Understanding the Spectral Bias of Coordinate Based MLPs Via Training Dynamics.](http://arxiv.org/abs/2301.05816) | 该论文研究了基于坐标的MLPs的谱偏置对高频组件收敛的阻碍，并提出使用高频正弦波编码输入来克服这一限制。 |
| [^97] | [Graph Laplacian for Semi-Supervised Learning.](http://arxiv.org/abs/2301.04956) | 本文提出了一种新型图拉普拉斯算子，旨在平稳无监督聚类和低监督基于图的分类之间的过渡，并且无需使用Dirichlet正则化或者基于核的方法。 |
| [^98] | [BASiS: Batch Aligned Spectral Embedding Space.](http://arxiv.org/abs/2211.16960) | 该论文提出了BASiS，一种用于直接学习特征向量空间的对齐机制，可以同时处理批量和图度量变化，表现优于现有的最佳模型。 |
| [^99] | [Statistical inference for transfer learning with high-dimensional quantile regression.](http://arxiv.org/abs/2211.14578) | 本研究提出了一种高维分位数回归模型中的转移学习方法，以适应源域和目标域中的异质性和重尾分布。根据精心选择的可转移源域建立了转移学习估计量的误差界限，并提出了有效的置信区间和假设检验程序，以实现一步完成。 |
| [^100] | [A Self-Attention Ansatz for Ab-initio Quantum Chemistry.](http://arxiv.org/abs/2211.13672) | Psiformer是一种使用自注意力机制的新型神经网络架构，可显著提高从头计算量子化学中基态能量的准确性，特别是在大分子上。 |
| [^101] | [Real-Time Target Sound Extraction.](http://arxiv.org/abs/2211.02250) | 该论文提出了Waveformer模型，是第一个能够实时流目标声音提取的神经网络模型。这个模型使用混合体系结构，能够处理大接受域并具有较高的泛化性能，并在模型大小和运行时间上取得了显着改进。 |
| [^102] | [Automated Code Extraction from Discussion Board Text Dataset.](http://arxiv.org/abs/2210.17495) | 本研究探究了用潜在语义分析、潜在狄利克雷分配和聚类词向量三种不同的文本挖掘方法自动提取讨论版数据集中的代码的能力。结果表明，即使是较小的数据集，自动化方法也有助于提取讨论代码，并用于认识网络分析。 |
| [^103] | [Convergence Rates of Stochastic Zeroth-order Gradient Descent for \L ojasiewicz Functions.](http://arxiv.org/abs/2210.16997) | 该论文证明了在Lojasiewicz函数上，随机零阶梯度下降算法具有收敛速率，且比 $\{ \|\mathbf{x}_t-\mathbf{x}_\infty\| \}_{t \in \mathbb{N}}$更快，无论$f$是平滑还是非平滑的。 |
| [^104] | [Embedding-Assisted Attentional Deep Learning for Real-World RF Fingerprinting of Bluetooth.](http://arxiv.org/abs/2210.02897) | 本研究提出了一种嵌入式注意力深度学习框架，用于提取实际蓝牙设备的指纹。该模型具有较高的分类准确率，并在内存使用方面有所改进。 |
| [^105] | [Policy Gradients for Probabilistic Constrained Reinforcement Learning.](http://arxiv.org/abs/2210.00596) | 本文提出了处理概率安全约束的策略梯度方法，是首个给出概率约束梯度表达式的工作。 |
| [^106] | [On the Convergence of AdaGrad on $\R^{d}$: Beyond Convexity, Non-Asymptotic Rate and Acceleration.](http://arxiv.org/abs/2209.14827) | 本论文主要展示了AdaGrad在平滑凸函数和更一般的quasar凸函数的情况下的收敛性。具体地，我们提出了新的技术，明确限定了vanilla AdaGrad在无约束问题中的收敛速率，并提出了一种AdaGrad变种，可以实现更快的收敛。 |
| [^107] | [Differentially private partitioned variational inference.](http://arxiv.org/abs/2209.11595) | 本论文提出了差分隐私的分区变分推断算法，是第一种在联邦贝叶斯学习环境下实现差分隐私的方法。 |
| [^108] | [Constraining Representations Yields Models That Know What They Don't Know.](http://arxiv.org/abs/2208.14488) | 通过对模型内部激活模式施加类感知约束的方法，本文提出总激活分类器（TAC）可以让模型更加安全、可靠，并具有广泛的应用前景。 |
| [^109] | [A Universal Trade-off Between the Model Size, Test Loss, and Training Loss of Linear Predictors.](http://arxiv.org/abs/2207.11621) | 本文建立了一个与算法和分布无关的非渐进性权衡方法，来衡量线性预测器的模型大小、测试损失和训练损失之间的关系，发现测试数据上表现出色的模型要么是经典的，要么是现代的。同时提供了当白化特征的极限谱分布为Marchenko-Pastur时的更为精确的渐进分析，使得分析更加精确。 |
| [^110] | [Code Translation with Compiler Representations.](http://arxiv.org/abs/2207.03578) | 本文提出了一种将编译器中间表示与神经机器翻译相结合的方法，能够更好地捕捉代码的语义，从而提高了代码翻译的质量和实用性。 |
| [^111] | [Provably Efficient Offline Reinforcement Learning with Trajectory-Wise Reward.](http://arxiv.org/abs/2206.06426) | 本文提出了一种离线RL算法PARTED，它通过基于最小二乘的奖励重新分配将轨迹回报分解为每步代理奖励，然后基于学习的代理奖励执行悲观值迭代，用于解决轨迹奖励难以很好地利用的问题。 |
| [^112] | [Fast Vision Transformers with HiLo Attention.](http://arxiv.org/abs/2205.13213) | 摘要：本文提出了一种名为HiLo注意力的自注意机制，使用分而治之的策略在注意力转换器中分解高/低频模式，可以更加高效地运行视觉Transformer，并在不同模型大小的范围内胜过现有的最先进方法。 |
| [^113] | [Broad Recommender System: An Efficient Nonlinear Collaborative Filtering Approach.](http://arxiv.org/abs/2204.11602) | 本文提出了一种新的宽泛推荐系统(BroadCF)，使用宽泛学习系统(BLS)作为映射函数来学习用户和项目之间的复杂非线性关系，同时通过用户-项评级协同向量预处理程序将原始数据转换为更适合BLS学习的格式。BroadCF的实验结果表明，在用户推荐准确性和效率方面都优于几种最先进的CF方法。 |
| [^114] | [Dimensionality Expansion of Load Monitoring Time Series and Transfer Learning for EMS.](http://arxiv.org/abs/2204.02802) | 本文提出了一种维度扩展和迁移学习的负载监测方法，该方法在5个不同的低频数据集上表现良好，可以更高效地计算。研究表明在跨数据集内领域迁移学习方面也有良好的效果。 |
| [^115] | [A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems.](http://arxiv.org/abs/2203.01387) | 本文综述了离线强化学习中的分类与最新算法突破，离线RL算法具有更广泛的应用，尤其适用于教育、医疗保健和机器人等实际应用。 |
| [^116] | [Statistical Inference After Adaptive Sampling for Longitudinal Data.](http://arxiv.org/abs/2202.07098) | 本文针对经过自适应采样得到的纵向用户数据，通过 Z-估计器提出了新的方法进行统计分析，包括引入校正夹心方差估计器等。通过汇集用户数据，自适应采样算法能够潜在地更快地学习，但同时也引入了依赖关系导致方差估计低估，因此需要采用新的方法进行统计分析。 |
| [^117] | [Control of Dual-Sourcing Inventory Systems using Recurrent Neural Networks.](http://arxiv.org/abs/2201.06126) | 本论文采用循环神经网络令库存管理者能够更好地决策如何重新补充库存，特别是在考虑多个供应商订单和成本的情况下快速调整需求。 |
| [^118] | [Value Functions Factorization with Latent State Information Sharing in Decentralized Multi-Agent Policy Gradients.](http://arxiv.org/abs/2201.01247) | 在QMIX方法的基础上，提出了LSF-SAC框架，其中包括一个潜在信息共享机制，可显著扩展价值函数分解的能力，同时在完全分散执行中保持了有效性。 |
| [^119] | [Data-Efficient Deep Reinforcement Learning for Attitude Control of Fixed-Wing UAVs: Field Experiments.](http://arxiv.org/abs/2111.04153) | 本文使用深度强化学习算法成功实现了对固定翼无人机的姿态控制，只需三分钟的飞行数据。该算法可以直接操作原始非线性动力学，相较于现有技术具备了更好的性能和有效性。 |
| [^120] | [Sparse Plus Low Rank Matrix Decomposition: A Discrete Optimization Approach.](http://arxiv.org/abs/2109.12701) | 本文研究稀疏加低秩矩阵分解问题(SLR)，提出了一种新的离散模型和求解方法，适用于多种应用场景。 |
| [^121] | [Continuous Time Bandits With Sampling Costs.](http://arxiv.org/abs/2107.05289) | 本文研究了带采样成本的连续时间多臂赌博机问题，在连续时间里，学习者要在获得更高奖励和承担采样成本之间进行有效平衡。本文提出了一个达到下界的算法，并揭示了与传统多臂赌博机问题不同的特殊现象，具有广泛应用价值。 |
| [^122] | [An Analysis of Robustness of Non-Lipschitz Networks.](http://arxiv.org/abs/2010.06154) | 本文研究了深度非利普希茨网络的鲁棒性问题，定义了一个攻击模型帮助理解内在属性，证明了此类攻击者可以战胜所有必须对其输入进行分类的算法，但也提出了克服此类攻击者的方法，进一步提供了理论保证并为最近邻算法提供了新的鲁棒性保证。 |
| [^123] | [Smart Choices and the Selection Monad.](http://arxiv.org/abs/2007.08926) | 该论文提出了基于选择和成本收益的系统描述方式，并从编程语言的角度研究了此方法。研究者定义了两种支持决策抽象的小语言，并给出了它们的操作语义和底层语义，并将底层语义增强为选择和概率单子。该研究通过应用于两个简单例子展示了此方法的实用性。 |

# 详细

[^1]: LipsFormer：向Vision Transformer引入Lipschitz连续性

    LipsFormer: Introducing Lipschitz Continuity to Vision Transformers. (arXiv:2304.09856v1 [cs.CV])

    [http://arxiv.org/abs/2304.09856](http://arxiv.org/abs/2304.09856)

    本文介绍了一种Lipschitz连续的Transformer模型LipsFormer，其中引入了多项Lipschitz连续的组件来保证训练的稳定性，并在实验证明其可以让深度Transformer架构稳定进行训练，无需过多调整学习率等参数。

    

    我们提出了一个名为LipsFormer的Lipschitz连续Transformer，旨在从理论上和实践上追求Transformer-based模型的训练稳定性。相比于以前通过学习率warmup、层归一化、attention公式、权重初始化等方法解决训练不稳定性的实际技巧，我们证明了Lipschitz连续性是确保训练稳定性的更重要的属性。在LipsFormer中，我们用Lipschitz连续的中心归一化代替不稳定的Transformer组件模块：中心归一化代替层归一化，谱初始化代替Xavier初始化，缩放余弦相似度注意代替点积注意，并使用加权残差快捷方式。我们证明了这些引入的模块是Lipschitz连续的，并推导出LipsFormer的Lipschitz常数的上界。我们的实验表明，LipsFormer允许深度Transformer架构的稳定训练，无需仔细调整学习率。

    We present a Lipschitz continuous Transformer, called LipsFormer, to pursue training stability both theoretically and empirically for Transformer-based models. In contrast to previous practical tricks that address training instability by learning rate warmup, layer normalization, attention formulation, and weight initialization, we show that Lipschitz continuity is a more essential property to ensure training stability. In LipsFormer, we replace unstable Transformer component modules with Lipschitz continuous counterparts: CenterNorm instead of LayerNorm, spectral initialization instead of Xavier initialization, scaled cosine similarity attention instead of dot-product attention, and weighted residual shortcut. We prove that these introduced modules are Lipschitz continuous and derive an upper bound on the Lipschitz constant of LipsFormer. Our experiments show that LipsFormer allows stable training of deep Transformer architectures without the need of careful learning rate tuning such 
    
[^2]: 用有效的视野连接强化学习理论和实践

    Bridging RL Theory and Practice with the Effective Horizon. (arXiv:2304.09853v1 [cs.LG])

    [http://arxiv.org/abs/2304.09853](http://arxiv.org/abs/2304.09853)

    本论文通过对常见深度强化学习测试基准中155个MDP的数据集进行分析，发现当最高Q值的动作在随机策略下Q值最高时，深度强化学习往往会成功；反之，则失败的可能性较高。

    

    深度强化学习在某些环境中表现出色，但在其他环境中却失败得非常严重。理想情况下，强化学习理论应该能够解释这种现象，提供预测实际性能的界限。不幸的是，当前的理论还没有这种能力。本文通过引入包含155个MDP的新数据集BRIDGE，将标准的深度强化学习算法与之前的样本复杂度先前界进行比较，并发现了一个意想不到的性质：当最高Q值的动作在随机策略下的Q值也是最高的时，深度强化学习往往会成功；反之，失败的可能性较高。基于这一性质，我们将其概括为一个新的MDP复杂度度量，称为有效的视野。

    Deep reinforcement learning (RL) works impressively in some environments and fails catastrophically in others. Ideally, RL theory should be able to provide an understanding of why this is, i.e. bounds predictive of practical performance. Unfortunately, current theory does not quite have this ability. We compare standard deep RL algorithms to prior sample complexity prior bounds by introducing a new dataset, BRIDGE. It consists of 155 MDPs from common deep RL benchmarks, along with their corresponding tabular representations, which enables us to exactly compute instance-dependent bounds. We find that prior bounds do not correlate well with when deep RL succeeds vs. fails, but discover a surprising property that does. When actions with the highest Q-values under the random policy also have the highest Q-values under the optimal policy, deep RL tends to succeed; when they don't, deep RL tends to fail. We generalize this property into a new complexity measure of an MDP that we call the eff
    
[^3]: 随机神经网络生成的函数的非线性点

    Points of non-linearity of functions generated by random neural networks. (arXiv:2304.09837v1 [cs.LG])

    [http://arxiv.org/abs/2304.09837](http://arxiv.org/abs/2304.09837)

    论文从神经网络模型的参数分布出发，探究网络输出函数几何形状的简单性与信息熵复杂度的关系。

    

    我们考虑由具有1个隐藏激活层，任意宽度和ReLU激活函数的神经网络产生的从实数到实数的函数。我们假设神经网络的参数基于不同概率分布的均匀随机选择，并计算非线性点的期望分布。我们利用这些结果阐明为什么网络可能偏向于输出具有更简单几何形状的函数，以及为什么某些信息熵复杂度较低的函数仍然难以用神经网络近似。

    We consider functions from the real numbers to the real numbers, output by a neural network with 1 hidden activation layer, arbitrary width, and ReLU activation function. We assume that the parameters of the neural network are chosen uniformly at random with respect to various probability distributions, and compute the expected distribution of the points of non-linearity. We use these results to explain why the network may be biased towards outputting functions with simpler geometry, and why certain functions with low information-theoretic complexity are nonetheless hard for a neural network to approximate.
    
[^4]: 多元概率预测评估中的可靠性区域研究

    Regions of Reliability in the Evaluation of Multivariate Probabilistic Forecasts. (arXiv:2304.09836v1 [cs.LG])

    [http://arxiv.org/abs/2304.09836](http://arxiv.org/abs/2304.09836)

    本研究通过有限样本和功率分析确定了多元概率时间序列预测评分规则的可靠性区域，并在电力生产问题上评估了结果对真实世界任务的普适性。

    

    在多元概率时间序列预测的评估中，通常使用适当的评分规则进行评估，即对于基准分布期望最小的函数。然而，在非渐进情况下，这一属性不能保证具有良好的区分度。在本文中，我们提供了第一篇系统的有限样本适当评分规则研究，通过功率分析，我们确定了一个分数规则的“可靠性区域”，即它可以可靠地识别预测误差的一组实际条件。我们在一个全面的人造基准测试上进行了分析，该测试专门设计以测试基准分布与预测分布之间的几个关键差异，并通过在电力生产问题上应用来评估我们的结果对真实世界任务的普适性。我们的结果揭示了在多元概率预测的评估中的重大缺陷。

    Multivariate probabilistic time series forecasts are commonly evaluated via proper scoring rules, i.e., functions that are minimal in expectation for the ground-truth distribution. However, this property is not sufficient to guarantee good discrimination in the non-asymptotic regime. In this paper, we provide the first systematic finite-sample study of proper scoring rules for time-series forecasting evaluation. Through a power analysis, we identify the "region of reliability" of a scoring rule, i.e., the set of practical conditions where it can be relied on to identify forecasting errors. We carry out our analysis on a comprehensive synthetic benchmark, specifically designed to test several key discrepancies between ground-truth and forecast distributions, and we gauge the generalizability of our findings to real-world tasks with an application to an electricity production problem. Our results reveal critical shortcomings in the evaluation of multivariate probabilistic forecasts as co
    
[^5]: 透明且稳健的基于数据驱动的风力涡轮机功率曲线模型

    Towards transparent and robust data-driven wind turbine power curve models. (arXiv:2304.09835v1 [cs.LG])

    [http://arxiv.org/abs/2304.09835](http://arxiv.org/abs/2304.09835)

    该研究利用可解释的人工智能框架研究了基于数据驱动的风力涡轮机功率曲线模型的稳健性。结果表明，学习到的策略可以更好地指示模型的稳健性。高度复杂的机器学习模型容易学习到物理上不合理的策略。

    

    风力涡轮机功率曲线模型将环境条件转化为涡轮机的功率输出。它们对于能量产出预测和涡轮机性能监测至关重要。近年来，基于数据驱动的机器学习方法已经优于基于参数和物理知识的方法。然而，它们常常被批评为是不透明的“黑匣子”，这引起了人们对于它们在非稳态环境下的稳健性的担忧，例如风力涡轮机所面临的情况。因此，我们引入了可解释的人工智能（XAI）框架，根据运行中的SCADA数据来研究和验证数据驱动的功率曲线模型所学习的策略。它将领域特定的考虑与Shapley值和最新的XAI回归研究结果相结合。我们的结果表明，学习到的策略可以更好地指示模型的稳健性，而不是验证或测试集的误差。此外，我们观察到，高度复杂，最先进的机器学习模型很容易学习到物理上不合理的策略，这一点应引起注意。

    Wind turbine power curve models translate ambient conditions into turbine power output. They are essential for energy yield prediction and turbine performance monitoring. In recent years, data-driven machine learning methods have outperformed parametric, physics-informed approaches. However, they are often criticised for being opaque "black boxes" which raises concerns regarding their robustness in non-stationary environments, such as faced by wind turbines. We, therefore, introduce an explainable artificial intelligence (XAI) framework to investigate and validate strategies learned by data-driven power curve models from operational SCADA data. It combines domain-specific considerations with Shapley Values and the latest findings from XAI for regression. Our results suggest, that learned strategies can be better indicators for model robustness than validation or test set errors. Moreover, we observe that highly complex, state-of-the-art ML models are prone to learn physically implausib
    
[^6]: AI的公平性及其对社会的长期影响

    Fairness in AI and Its Long-Term Implications on Society. (arXiv:2304.09826v1 [cs.CY])

    [http://arxiv.org/abs/2304.09826](http://arxiv.org/abs/2304.09826)

    本文探讨了AI的公平性问题，指出缺乏AI公平性会加深偏见成为社会压力因素，可能对社会产生长期影响，因此需要寻求潜在解决方案。

    

    人工智能（AI）在各种设置中的成功部署已经为个人和社会带来了许多积极的成果。然而，由于预测的偏见，AI系统也被证明对部分人口造成了伤害。我们着眼于AI的公平性，分析了缺乏AI公平性时如何导致偏见随着时间的加深而成为社会压力因素。如果问题持续存在，可能会对社会产生不良的长期影响，并通过与其他风险的交互来加强。我们检查了提高AI公平性的当前策略，并评估它们在实际部署方面的限制，并探讨了确保我们在不损害社会重要部分的情况下获得AI的好处的潜在路径。

    Successful deployment of artificial intelligence (AI) in various settings has led to numerous positive outcomes for individuals and society. However, AI systems have also been shown to harm parts of the population due to biased predictions. We take a closer look at AI fairness and analyse how lack of AI fairness can lead to deepening of biases over time and act as a social stressor. If the issues persist, it could have undesirable long-term implications on society, reinforced by interactions with other risks. We examine current strategies for improving AI fairness, assess their limitations in terms of real-world deployment, and explore potential paths forward to ensure we reap AI's benefits without harming significant parts of the society.
    
[^7]: 利用离线数据加速程序生成环境中的强化学习

    Using Offline Data to Speed-up Reinforcement Learning in Procedurally Generated Environments. (arXiv:2304.09825v1 [cs.LG])

    [http://arxiv.org/abs/2304.09825](http://arxiv.org/abs/2304.09825)

    本研究旨在提高程序生成环境中强化学习的样本效率。研究证明，使用模仿学习进行预训练和同时进行模仿学习和在线强化学习的方法可以提高效率。

    

    强化学习面临的主要挑战之一是代理能够将其学习策略推广到未见过的环境中。此外，训练强化学习代理需要与环境进行大量交互。受离线强化学习和模仿学习的最近成功启发，我们进行了一项研究，以调查代理是否可以利用轨迹的离线数据来提高程序生成环境中的样本效率。我们考虑了两种使用离线数据的模仿学习方法：（1）在在线强化学习训练之前预训练策略和（2）同时训练在线强化学习和来自离线数据的模仿学习。我们分析了可用的离线轨迹的质量（轨迹的最佳性）和多样性（轨迹数量和覆盖级别）对两种方法有效性的影响。在MiniGrid环境中的四个知名稀疏奖励任务中，我们发现使用模仿学习进行预训练和同时进行模仿学习和在线强化学习的方法可以提供更高的样本效率。

    One of the key challenges of Reinforcement Learning (RL) is the ability of agents to generalise their learned policy to unseen settings. Moreover, training RL agents requires large numbers of interactions with the environment. Motivated by the recent success of Offline RL and Imitation Learning (IL), we conduct a study to investigate whether agents can leverage offline data in the form of trajectories to improve the sample-efficiency in procedurally generated environments. We consider two settings of using IL from offline data for RL: (1) pre-training a policy before online RL training and (2) concurrently training a policy with online RL and IL from offline data. We analyse the impact of the quality (optimality of trajectories) and diversity (number of trajectories and covered level) of available offline trajectories on the effectiveness of both approaches. Across four well-known sparse reward tasks in the MiniGrid environment, we find that using IL for pre-training and concurrently d
    
[^8]: 利用深度强化学习在智能辅导系统中提供元认知干预

    Leveraging Deep Reinforcement Learning for Metacognitive Interventions across Intelligent Tutoring Systems. (arXiv:2304.09821v1 [cs.CY])

    [http://arxiv.org/abs/2304.09821](http://arxiv.org/abs/2304.09821)

    本研究比较了两种方法来提供元认知干预，并发现基于深度强化学习的自适应干预能够缩小学生之间的元认知技能差距。

    

    本研究比较了两种不同方法来提供元认知干预，以及它们对智能辅导系统（ITSs）中学生未来学习准备的影响。我们在两个连续的学期中进行了两个课堂实验：实验1使用经典的人工智能方法将学生分类为不同的元认知组，并根据他们的分类组提供静态的干预。在实验2中，我们利用了深度强化学习（DRL）来提供自适应干预，考虑到学生元认知水平的动态变化。在两个实验中，学生接受了这些干预，学习如何和何时在逻辑辅导程序上使用向后链接（BC）策略，该程序支持默认的向前链接策略。六周后，我们在一个只支持BC而没有干预的概率辅导程序上对学生进行了培训。我们的结果表明，自适应的基于DRL的干预缩小了学生之间的元认知技能差距。

    This work compares two approaches to provide metacognitive interventions and their impact on preparing students for future learning across Intelligent Tutoring Systems (ITSs). In two consecutive semesters, we conducted two classroom experiments: Exp. 1 used a classic artificial intelligence approach to classify students into different metacognitive groups and provide static interventions based on their classified groups. In Exp. 2, we leveraged Deep Reinforcement Learning (DRL) to provide adaptive interventions that consider the dynamic changes in the student's metacognitive levels. In both experiments, students received these interventions that taught how and when to use a backward-chaining (BC) strategy on a logic tutor that supports a default forward-chaining strategy. Six weeks later, we trained students on a probability tutor that only supports BC without interventions. Our results show that adaptive DRL-based interventions closed the metacognitive skills gap between students. In 
    
[^9]: 基于模型的神经网络的泛化和估计误差界限

    Generalization and Estimation Error Bounds for Model-based Neural Networks. (arXiv:2304.09802v1 [cs.LG])

    [http://arxiv.org/abs/2304.09802](http://arxiv.org/abs/2304.09802)

    基于模型的神经网络在稀疏恢复中表现出较高的泛化能力，复杂度量有助于提高其泛化和估计误差界限

    

    基于模型的神经网络在各种任务（如稀疏编码和压缩感知问题）中提供了无与伦比的性能。由于与传感模型的强关联，这些网络是可解释的并继承了问题的先前结构。实践中，与ReLU神经网络相比，基于模型的神经网络表现出更高的泛化能力。然而，这种现象在理论上还没有得到解决。在本文中，我们利用复杂度量（包括全局和局部Rademacher复杂度）的方法，为基于模型的网络提供泛化和估计误差的上限。我们表明，基于模型的网络在稀疏恢复方面的泛化能力优于常规的ReLU网络，并导出允许构建具有高保证性的基于模型的网络的实际设计规则。我们通过一系列实验演示了我们的理论见解为深度学习实践者提供了一些启示。

    Model-based neural networks provide unparalleled performance for various tasks, such as sparse coding and compressed sensing problems. Due to the strong connection with the sensing model, these networks are interpretable and inherit prior structure of the problem. In practice, model-based neural networks exhibit higher generalization capability compared to ReLU neural networks. However, this phenomenon was not addressed theoretically. Here, we leverage complexity measures including the global and local Rademacher complexities, in order to provide upper bounds on the generalization and estimation errors of model-based networks. We show that the generalization abilities of model-based networks for sparse recovery outperform those of regular ReLU networks, and derive practical design rules that allow to construct model-based networks with guaranteed high generalization. We demonstrate through a series of experiments that our theoretical insights shed light on a few behaviours experienced 
    
[^10]: 基于社交网络理论的回归任务概念漂移检测的进展

    Advances on Concept Drift Detection in Regression Tasks using Social Networks Theory. (arXiv:2304.09788v1 [cs.LG])

    [http://arxiv.org/abs/2304.09788](http://arxiv.org/abs/2304.09788)

    本文介绍了一种基于社交网络理论的动态集成型回归算法(SFNR)，使用自适应窗口(ADWIN)算法检测概念漂移，结果表明在概念漂移情况下SFNR的准确性更好且表现超过其他现有算法。

    

    数据流挖掘是机器学习领域的主要研究之一，因其在许多知识领域中的应用而备受关注。概念漂移是挖掘数据流面临的主要挑战之一，需要学习者放弃当前概念并适应新概念。集成型漂移检测算法已成功用于分类任务，但通常会维护一个固定大小的学习器集合，从而冒着需要不必要地花费处理时间和内存的风险。本文在社交网络理论基础上对比例自由网络回归器(SFNR)进行了改进，它是一种用于回归的动态集成型方法。为了检测概念漂移，SFNR使用自适应窗口(ADWIN)算法。结果显示，在真实数据和合成数据中，SFNR在准确性方面表现出色，特别是在概念漂移情况下，相较于其他现有算法表现更佳。

    Mining data streams is one of the main studies in machine learning area due to its application in many knowledge areas. One of the major challenges on mining data streams is concept drift, which requires the learner to discard the current concept and adapt to a new one. Ensemble-based drift detection algorithms have been used successfully to the classification task but usually maintain a fixed size ensemble of learners running the risk of needlessly spending processing time and memory. In this paper we present improvements to the Scale-free Network Regressor (SFNR), a dynamic ensemble-based method for regression that employs social networks theory. In order to detect concept drifts SFNR uses the Adaptive Window (ADWIN) algorithm. Results show improvements in accuracy, especially in concept drift situations and better performance compared to other state-of-the-art algorithms in both real and synthetic data.
    
[^11]: 平等攸关不等于平等个人几率: 用于组和个人公平的后处理方法

    Equalised Odds is not Equal Individual Odds: Post-processing for Group and Individual Fairness. (arXiv:2304.09779v1 [cs.LG])

    [http://arxiv.org/abs/2304.09779](http://arxiv.org/abs/2304.09779)

    研究发现，通过平等化受保护子群体之间的预测分布来实现组公平和视相似个体同等对待实现个人公正是不兼容的。 并提出了一种构建连续概率函数的解决方法，来实现组和个人公平。

    

    组公平通过平衡受保护子群体之间的预测分布来实现；个人公平要求将相似的个体视为同等对待。然而，当评分模型通过不连续的概率函数进行校准时，这两个目标是不兼容的，其中个体可能会随机分配由固定概率确定的结果。这个过程可能会使来自同一受保护组的两个相似个体的分类几率差别明显不同，这是个人公平的明显违反。为每个受保护子群体分配唯一的几率也可能会阻止一个子群体的成员接到另一个子群体有正面结果的平等机会，我们认为这是另一种称为个人几率的不公平类型。我们通过构建受群体阈值约束的连续概率函数来解决所有这些问题。我们的解决方案保留了模型的预测能力。

    Group fairness is achieved by equalising prediction distributions between protected sub-populations; individual fairness requires treating similar individuals alike. These two objectives, however, are incompatible when a scoring model is calibrated through discontinuous probability functions, where individuals can be randomly assigned an outcome determined by a fixed probability. This procedure may provide two similar individuals from the same protected group with classification odds that are disparately different -- a clear violation of individual fairness. Assigning unique odds to each protected sub-population may also prevent members of one sub-population from ever receiving equal chances of a positive outcome to another, which we argue is another type of unfairness called individual odds. We reconcile all this by constructing continuous probability functions between group thresholds that are constrained by their Lipschitz constant. Our solution preserves the model's predictive powe
    
[^12]: 实用的差分隐私和拜占庭容错联邦学习

    Practical Differentially Private and Byzantine-resilient Federated Learning. (arXiv:2304.09762v1 [cs.LG])

    [http://arxiv.org/abs/2304.09762](http://arxiv.org/abs/2304.09762)

    本文提出了一种结合了差分隐私和拜占庭容错的联邦学习方法，在保护隐私的同时，提高系统的鲁棒性，拒绝多种攻击方法，具有较高的隐私保护水平。

    

    隐私和拜占庭容错是联邦学习系统不可或缺的要求。尽管隐私和拜占庭安全都有广泛的研究，但同时考虑这两个要求的解决方案仍然很少。这是由于协调隐私保护和拜占庭容错算法的困难。本文提出了解决这个问题的方法。我们使用差分隐私随机梯度下降算法来保护隐私，然后应用我们的拜占庭容错算法。我们注意到，尽管现有的工作遵循这种通用方法，但对 DP 和拜占庭容错之间的相互作用进行了深入分析，并提出了令人满意的性能，这已被忽略。具体来说，为了减少 DP 引入的随机噪声对 Byzantine 聚合的影响，先前的工作努力。相反，我们利用随机噪声构建一个聚合，有效地拒绝了许多现有的攻击方法，并在具有与当前最先进算法相当的隐私保护水平时，提升了整体 FL 系统的 Byzantine 鲁棒性。

    Privacy and Byzantine resilience are two indispensable requirements for a federated learning (FL) system. Although there have been extensive studies on privacy and Byzantine security in their own track, solutions that consider both remain sparse. This is due to difficulties in reconciling privacy-preserving and Byzantine-resilient algorithms.  In this work, we propose a solution to such a two-fold issue. We use our version of differentially private stochastic gradient descent (DP-SGD) algorithm to preserve privacy and then apply our Byzantine-resilient algorithms. We note that while existing works follow this general approach, an in-depth analysis on the interplay between DP and Byzantine resilience has been ignored, leading to unsatisfactory performance. Specifically, for the random noise introduced by DP, previous works strive to reduce its impact on the Byzantine aggregation. In contrast, we leverage the random noise to construct an aggregation that effectively rejects many existing
    
[^13]: 一种基于深度学习的创新方法用于准确的农产品价格预测

    An innovative Deep Learning Based Approach for Accurate Agricultural Crop Price Prediction. (arXiv:2304.09761v1 [cs.LG])

    [http://arxiv.org/abs/2304.09761](http://arxiv.org/abs/2304.09761)

    本文提出了一种基于深度学习的方法，利用历史价格信息、气候条件、土壤类型、地理位置和其他关键决策因素，精确预测农产品价格。该方法使用图神经网络和卷积神经网络相结合，适用于有噪声的历史数据，并且表现至少比现有文献中的结果好20%。

    

    农产品价格的准确预测对于农业各利益相关者（农民、消费者、零售商、批发商和政府）的决策至关重要，特别是对农民的经济福祉产生重要影响。本文的目标是利用历史价格信息、气候条件、土壤类型、地理位置和其他关键决策因素，精确预测农产品价格。这是一个技术挑战，以前也曾被尝试过。本文提出了一种创新的基于深度学习的方法，以实现在价格预测中的提高准确性。所提出的方法使用图神经网络（GNNs）与标准卷积神经网络（CNN）模型相结合，利用价格的地理空间相关性。我们的方法适用于有噪声的历史数据，并且表现至少比现有文献中的结果好20%。

    Accurate prediction of agricultural crop prices is a crucial input for decision-making by various stakeholders in agriculture: farmers, consumers, retailers, wholesalers, and the Government. These decisions have significant implications including, most importantly, the economic well-being of the farmers. In this paper, our objective is to accurately predict crop prices using historical price information, climate conditions, soil type, location, and other key determinants of crop prices. This is a technically challenging problem, which has been attempted before. In this paper, we propose an innovative deep learning based approach to achieve increased accuracy in price prediction. The proposed approach uses graph neural networks (GNNs) in conjunction with a standard convolutional neural network (CNN) model to exploit geospatial dependencies in prices. Our approach works well with noisy legacy data and produces a performance that is at least 20% better than the results available in the li
    
[^14]: 增幅正弦单元：一种用于恢复非线性振荡的深度神经网络震荡激活函数

    Amplifying Sine Unit: An Oscillatory Activation Function for Deep Neural Networks to Recover Nonlinear Oscillations Efficiently. (arXiv:2304.09759v1 [cs.LG])

    [http://arxiv.org/abs/2304.09759](http://arxiv.org/abs/2304.09759)

    这篇论文介绍了一种能够处理微机电系统中非线性振荡的神经网络方法，使用一种名为增幅正弦单元（ASU）的新型脉冲函数明显优于其他激活函数。

    

    许多工业和现实生活中的问题表现出高度非线性的周期行为，传统方法可能无法找到它们的解析或闭合形式解决方案。这些问题需求一些前沿的计算工具，具有增强的功能性和降低成本。近年来，由于其处理大数据和学习复杂函数的普适性，深度神经网络已经引起了大量的研究兴趣。在本文中，我们提出了一种基于响应性层结构的深度神经网络的方法，用于处理微机电系统中的非线性振荡。我们在设计的网络中结合了一些振荡和非振荡激活函数，如Growing Cosine Unit（GCU）、Sine、Mish和Tanh，以对它们在高度非线性和振动问题的性能进行全面分析。将振荡激活函数与深度神经网络集成，可以以高精度和效率预测周期性模式。我们提出的激活函数称为增幅正弦单元（ASU），在各种基准问题上进行测试时，显示出显着的改进。

    Many industrial and real life problems exhibit highly nonlinear periodic behaviors and the conventional methods may fall short of finding their analytical or closed form solutions. Such problems demand some cutting edge computational tools with increased functionality and reduced cost. Recently, deep neural networks have gained massive research interest due to their ability to handle large data and universality to learn complex functions. In this work, we put forward a methodology based on deep neural networks with responsive layers structure to deal nonlinear oscillations in microelectromechanical systems. We incorporated some oscillatory and non oscillatory activation functions such as growing cosine unit known as GCU, Sine, Mish and Tanh in our designed network to have a comprehensive analysis on their performance for highly nonlinear and vibrational problems. Integrating oscillatory activation functions with deep neural networks definitely outperform in predicting the periodic patt
    
[^15]: 基于K-means聚类的特征一致性对齐用于无标签模型评估

    K-means Clustering Based Feature Consistency Alignment for Label-free Model Evaluation. (arXiv:2304.09758v1 [cs.LG])

    [http://arxiv.org/abs/2304.09758](http://arxiv.org/abs/2304.09758)

    本文提出了一种名为KCFCA的新方法，该方法利用K-means算法来对标记的训练集和无标签的测试集进行聚类，并通过特征一致性来对齐聚类中心。另外，文章还设计了动态回归模型来研究分布偏移与模型准确性之间的关系以及排除异常模型。这些方法为无标签模型评估提供了更加全面准确的方法。

    

    无标签模型评估旨在在不依赖于真实标签的情况下预测模型在各种测试集上的性能。这篇文章介绍了我们针对CVPR 2023 Visual Dataset Understanding研讨会的第一个DataCV挑战的解决方案。首先，我们提出了一种名为K-means聚类特征一致性对齐（KCFCA）的新方法，该方法旨在处理各种数据集的分布偏移。KCFCA利用K-means算法来聚类标记的训练集和无标签的测试集，然后通过特征一致性来对齐聚类中心。其次，我们开发了一种动态回归模型来捕捉分布偏移和模型准确性之间的关系。第三，我们设计了一个算法来发现异常模型因素，排除异常模型，并结合多个自动评估模型的优点。

    The label-free model evaluation aims to predict the model performance on various test sets without relying on ground truths. The main challenge of this task is the absence of labels in the test data, unlike in classical supervised model evaluation. This paper presents our solutions for the 1st DataCV Challenge of the Visual Dataset Understanding workshop at CVPR 2023. Firstly, we propose a novel method called K-means Clustering Based Feature Consistency Alignment (KCFCA), which is tailored to handle the distribution shifts of various datasets. KCFCA utilizes the K-means algorithm to cluster labeled training sets and unlabeled test sets, and then aligns the cluster centers with feature consistency. Secondly, we develop a dynamic regression model to capture the relationship between the shifts in distribution and model accuracy. Thirdly, we design an algorithm to discover the outlier model factors, eliminate the outlier models, and combine the strengths of multiple autoeval models. On the
    
[^16]: 使用潜在指纹追溯图像生成模型

    Attributing Image Generative Models using Latent Fingerprints. (arXiv:2304.09752v1 [cs.CV])

    [http://arxiv.org/abs/2304.09752](http://arxiv.org/abs/2304.09752)

    本文研究了一种使用潜在语义维度作为指纹的追溯方法，可以分析设计变量对于准确性-质量权衡的影响，在保证准确性的同时最小化计算量，更适用于大规模模型。

    

    生成模型使得产生的内容难以区分是否源于自然环境。这种模型的开源开发引起了对于其被恶意利用的担忧。其中一个潜在的风险缓解策略是通过指纹追溯生成模型。然而，现有的指纹追溯方法在追溯准确性与生成质量之间存在明显的权衡，并且缺乏改善这种权衡的设计原则。本文研究了使用潜在语义维度作为指纹的方法，通过该方法可以分析设计变量对于准确性-质量权衡的影响，包括指纹维度的选择、强度和容量。相比之前的 SOTA，我们的方法需要最少的计算，并且更适用于大规模模型。我们使用 StyleGAN2 和潜在扩散模型展示了我们方法的功效。

    Generative models have enabled the creation of contents that are indistinguishable from those taken from the nature. Open-source development of such models raised concerns about the risks in their misuse for malicious purposes. One potential risk mitigation strategy is to attribute generative models via fingerprinting. Current fingerprinting methods exhibit significant tradeoff between robust attribution accuracy and generation quality, and also lack designing principles to improve this tradeoff. This paper investigates the use of latent semantic dimensions as fingerprints, from where we can analyze the effects of design variables, including the choice of fingerprinting dimensions, strength, and capacity, on the accuracy-quality tradeoff. Compared with previous SOTA, our method requires minimum computation and is more applicable to large-scale models. We use StyleGAN2 and the latent diffusion model to demonstrate the efficacy of our method.
    
[^17]: 基于骨架的行为分析用于注意力缺陷多动障碍的诊断

    Skeleton-based action analysis for ADHD diagnosis. (arXiv:2304.09751v1 [cs.CV])

    [http://arxiv.org/abs/2304.09751](http://arxiv.org/abs/2304.09751)

    提出一种基于骨架的动作识别框架用于注意力缺陷多动障碍的诊断，其显示出成本效益和显著的性能改进，是一种低成本的初始ADHD诊断方法。

    

    注意力缺陷多动障碍 (ADHD)是世界范围内常见的神经行为障碍。尽管对于机器学习方法在ADHD诊断上的研究非常广泛，但大多数研究都依赖于高成本设备，例如MRI机器和EEG贴片。因此，基于ADHD行为特征的低成本诊断方法备受期待。基于骨架的动作识别由于其聚焦于动作和鲁棒性而受到关注。在本文中，我们提出了一个新颖的ADHD诊断系统，其中包含基于骨架的动作识别框架，利用真实的多模态ADHD数据集和最先进的检测算法。与传统方法相比，所提出的方法显示出成本效益和显著的性能改进，使其更易于进行广泛的初始ADHD诊断。通过实验结果，所提出的方法在准确性和AUC方面优于传统方法。同时，我们的方法可广泛应用于大规模诊断和预测。

    Attention Deficit Hyperactivity Disorder (ADHD) is a common neurobehavioral disorder worldwide. While extensive research has focused on machine learning methods for ADHD diagnosis, most research relies on high-cost equipment, e.g., MRI machine and EEG patch. Therefore, low-cost diagnostic methods based on the action characteristics of ADHD are desired. Skeleton-based action recognition has gained attention due to the action-focused nature and robustness. In this work, we propose a novel ADHD diagnosis system with a skeleton-based action recognition framework, utilizing a real multi-modal ADHD dataset and state-of-the-art detection algorithms. Compared to conventional methods, the proposed method shows cost-efficiency and significant performance improvement, making it more accessible for a broad range of initial ADHD diagnoses. Through the experiment results, the proposed method outperforms the conventional methods in accuracy and AUC. Meanwhile, our method is widely applicable for mass
    
[^18]: 张量神经网络在百慕大掉期定价中的应用

    Application of Tensor Neural Networks to Pricing Bermudan Swaptions. (arXiv:2304.09750v1 [q-fin.CP])

    [http://arxiv.org/abs/2304.09750](http://arxiv.org/abs/2304.09750)

    本论文用张量神经网络(TNN)对百慕大掉期进行定价，相比于传统方法，TNN具有更快的收敛速度和减少参数敏感度的优点。

    

    Cheyette模型是一种准高斯波动率利率模型，广泛用于定价利率衍生品，例如欧式掉期和百慕大掉期，而蒙特卡罗模拟已成为行业标准。在低维度下，这些方法为欧式掉期提供了准确而稳健的价格，但即使在这种计算简单的情况下，当使用状态变量作为回归器时，它们也会低估百慕大掉期的价值。这主要是由于所用回归器中预先确定的基函数数量有限。此外，在高维环境中，这些方法也面临着维度灾难的问题。为了解决这些问题，研究者提出利用张量神经网络(TNN)来进行百慕大掉期的定价。研究结果表明，与传统方法相比，TNN具有更快的收敛速度，对于回归器中所用基函数的数量等参数，减少了敏感度。数值实验证实TNN能够在高维度情况下准确地定价欧式掉期和百慕大掉期。

    The Cheyette model is a quasi-Gaussian volatility interest rate model widely used to price interest rate derivatives such as European and Bermudan Swaptions for which Monte Carlo simulation has become the industry standard. In low dimensions, these approaches provide accurate and robust prices for European Swaptions but, even in this computationally simple setting, they are known to underestimate the value of Bermudan Swaptions when using the state variables as regressors. This is mainly due to the use of a finite number of predetermined basis functions in the regression. Moreover, in high-dimensional settings, these approaches succumb to the Curse of Dimensionality. To address these issues, Deep-learning techniques have been used to solve the backward Stochastic Differential Equation associated with the value process for European and Bermudan Swaptions; however, these methods are constrained by training time and memory. To overcome these limitations, we propose leveraging Tensor Neura
    
[^19]: 基于样本效率的模型驱动量子控制强化学习

    Sample-efficient Model-based Reinforcement Learning for Quantum Control. (arXiv:2304.09718v1 [quant-ph])

    [http://arxiv.org/abs/2304.09718](http://arxiv.org/abs/2304.09718)

    本论文提出了一种基于模型的强化学习方法，通过受到神经常微分方程进展的启发，这个方法采用自动微分的ODE表达由可学习的汉密尔顿安排参数化的模型来近似环境，在门控制和汉密尔顿参数的学习中通过系统交互解决问题。该方法在样本复杂度方面比标准基于模型自由的强化学习方法具有一个数量级的优势，适用于噪声时变门优化。

    

    我们提出了一种基于模型的强化学习方法，用于噪声时变门优化，其样本复杂度优于基于模型自由的强化学习。样本复杂度是控制器与物理系统交互的次数。借助一个归纳偏置，受最近神经常微分方程的进展启发，我们使用可微的ODE，其由可学习的汉密尔顿安排参数化，以表示模型近似环境，其时变部分（包括控制）完全已知。控制器和连续时域独立参数的汉密尔顿学习是通过与系统的交互来解决的。在真实数值实验中，我们展示了使用我们方法在准备一些标准单量子门的闭合和开放系统动态时，在样本复杂度方面与标准模型自由强化学习相比，具有一个数量级的优势，这包括单次测量、任意希尔伯特空间截断和不确定性等。

    We propose a model-based reinforcement learning (RL) approach for noisy time-dependent gate optimization with improved sample complexity over model-free RL. Sample complexity is the number of controller interactions with the physical system. Leveraging an inductive bias, inspired by recent advances in neural ordinary differential equations (ODEs), we use an auto-differentiable ODE parametrised by a learnable Hamiltonian ansatz to represent the model approximating the environment whose time-dependent part, including the control, is fully known. Control alongside Hamiltonian learning of continuous time-independent parameters is addressed through interactions with the system. We demonstrate an order of magnitude advantage in the sample complexity of our method over standard model-free RL in preparing some standard unitary gates with closed and open system dynamics, in realistic numerical experiments incorporating single shot measurements, arbitrary Hilbert space truncations and uncertaint
    
[^20]: 用概念向量解开神经元表示的纠缠

    Disentangling Neuron Representations with Concept Vectors. (arXiv:2304.09707v1 [cs.CV])

    [http://arxiv.org/abs/2304.09707](http://arxiv.org/abs/2304.09707)

    本文提出了一种方法，可以将多义神经元解开为封装不同特征的概念向量，这些向量编码了连贯的、人类可理解的特征。

    

    机制可解释性旨在通过将神经网络分解为可解释单元来理解模型存储表示的方式。然而，多义神经元的出现使得解释单个神经元变得具有挑战性。这导致了在激活空间中寻找有意义的向量，称为概念向量，而不是单个神经元。本文主要贡献是一种方法，可以将多义神经元解开为封装不同特征的概念向量。我们的方法可以根据用户所需的概念分离级别搜索细粒度概念。分析表明，多义神经元可以解开为由神经元的线性组合组成的方向。我们的评估表明，所找到的概念向量编码了连续的、人类可理解的特征。

    Mechanistic interpretability aims to understand how models store representations by breaking down neural networks into interpretable units. However, the occurrence of polysemantic neurons, or neurons that respond to multiple unrelated features, makes interpreting individual neurons challenging. This has led to the search for meaningful vectors, known as concept vectors, in activation space instead of individual neurons. The main contribution of this paper is a method to disentangle polysemantic neurons into concept vectors encapsulating distinct features. Our method can search for fine-grained concepts according to the user's desired level of concept separation. The analysis shows that polysemantic neurons can be disentangled into directions consisting of linear combinations of neurons. Our evaluations show that the concept vectors found encode coherent, human-understandable features.
    
[^21]: 低功耗近阈值处理器上的大-小自适应神经网络

    Big-Little Adaptive Neural Networks on Low-Power Near-Subthreshold Processors. (arXiv:2304.09695v1 [cs.LG])

    [http://arxiv.org/abs/2304.09695](http://arxiv.org/abs/2304.09695)

    本文研究了近阈值处理器在边缘人工智能应用中的能量节省，并提出大-小自适应神经网络策略以提高电池寿命和保持预测准确性。

    

    本文研究了近阈值处理器在边缘人工智能应用中可以获得的能量节省，并提出了策略来提高它们同时保持应用的准确性。选择的处理器采用自适应电压缩放技术，处理器核的频率和电压级别在运行时确定。在这些系统中，嵌入式RAM和Flash存储器的大小通常限制在不到1兆字节以节省电力。这种有限内存对可以映射到这些设备的神经网络模型的复杂性和精度与电池续航时间之间所需的权衡产生了限制。为了解决这些问题，我们提出并评估了替代的“大-小”神经网络策略，以提高电池寿命同时保持预测准确性。所提出的策略应用于人类活动识别应用程序作为示范，结果显示相比于原始网络，最佳配置

    This paper investigates the energy savings that near-subthreshold processors can obtain in edge AI applications and proposes strategies to improve them while maintaining the accuracy of the application. The selected processors deploy adaptive voltage scaling techniques in which the frequency and voltage levels of the processor core are determined at the run-time. In these systems, embedded RAM and flash memory size is typically limited to less than 1 megabyte to save power. This limited memory imposes restrictions on the complexity of the neural networks model that can be mapped to these devices and the required trade-offs between accuracy and battery life. To address these issues, we propose and evaluate alternative 'big-little' neural network strategies to improve battery life while maintaining prediction accuracy. The strategies are applied to a human activity recognition application selected as a demonstrator that shows that compared to the original network, the best configurations
    
[^22]: 通过最优输运和投影追踪的时变密度生成建模

    Generative Modeling of Time-Dependent Densities via Optimal Transport and Projection Pursuit. (arXiv:2304.09663v1 [stat.ML])

    [http://arxiv.org/abs/2304.09663](http://arxiv.org/abs/2304.09663)

    本文提出了一种基于最优输运和投影追踪的方法，用于便宜、高效地生成时态密度建模，其最优映射与恒等映射接近，训练过程高度并行化。

    

    受到流行的深度学习算法对于时态密度生成建模所带来的计算困难的启发，我们提出了一种便宜的替代方案，它需要最少的超参数调整，并且可以很好地扩展到高维问题。具体地，我们使用基于投影的最优输运求解器 [Meng等，2019] 来连接连续的样本，然后使用传输样条 [Chewi等，2020] 来插值演化的密度。当采样频率足够高时，最优映射接近于恒等映射，因此计算效率高。此外，训练过程可以高度并行化，因为所有最优映射是独立的，因此可以同时学习。最后，该方法仅基于数值线性代数而不是最小化非凸目标函数，这使我们能够轻松分析和控制算法。我们在合成和真实数据集上进行了几个数值实验。

    Motivated by the computational difficulties incurred by popular deep learning algorithms for the generative modeling of temporal densities, we propose a cheap alternative which requires minimal hyperparameter tuning and scales favorably to high dimensional problems. In particular, we use a projection-based optimal transport solver [Meng et al., 2019] to join successive samples and subsequently use transport splines [Chewi et al., 2020] to interpolate the evolving density. When the sampling frequency is sufficiently high, the optimal maps are close to the identity and are thus computationally efficient to compute. Moreover, the training process is highly parallelizable as all optimal maps are independent and can thus be learned simultaneously. Finally, the approach is based solely on numerical linear algebra rather than minimizing a nonconvex objective function, allowing us to easily analyze and control the algorithm. We present several numerical experiments on both synthetic and real-w
    
[^23]: 基于分布式优先经验回放的量子深度 Q 学习

    Quantum deep Q learning with distributed prioritized experience replay. (arXiv:2304.09648v1 [quant-ph])

    [http://arxiv.org/abs/2304.09648](http://arxiv.org/abs/2304.09648)

    本论文提出了 QDQN-DPER 框架，它将分布式优先经验回放和异步训练纳入算法，以提高量子强化学习在解决顺序决策任务中的效率。

    

    本研究介绍了 QDQN-DPER 框架，以提高量子强化学习 (QRL) 在解决顺序决策任务中的效率。该框架将优先经验回放和异步训练纳入训练算法，以减少高采样复杂性。数值模拟表明，QDQN-DPER 在具有相同模型架构的分布式量子 Q 学习的基础上表现更好。该框架可以在保持训练效率的同时适用于更复杂的任务。

    This paper introduces the QDQN-DPER framework to enhance the efficiency of quantum reinforcement learning (QRL) in solving sequential decision tasks. The framework incorporates prioritized experience replay and asynchronous training into the training algorithm to reduce the high sampling complexities. Numerical simulations demonstrate that QDQN-DPER outperforms the baseline distributed quantum Q learning with the same model architecture. The proposed framework holds potential for more complex tasks while maintaining training efficiency.
    
[^24]: AdapterGNN：高效的δ调节提高了图神经网络的泛化能力

    AdapterGNN: Efficient Delta Tuning Improves Generalization Ability in Graph Neural Networks. (arXiv:2304.09595v1 [cs.LG])

    [http://arxiv.org/abs/2304.09595](http://arxiv.org/abs/2304.09595)

    本文提出了一种专为图神经网络设计的δ调节方法——AdapterGNN，该方法保留了预训练模型的知识，利用高度表达的适配器能够在仅有少量参数的情况下有效地适应下游任务，并提高模型的泛化能力，实验结果表明其在多个基准数据集上取得了最优性能。

    

    最近，在图神经网络（GNNs）中微调预训练模型已经取得了显著的性能提升。除了预训练技术外，由于自然语言领域的最新工作的启示，更近期的研究转向应用有效的微调方法，例如参数有效的调节（δ调节）。然而，考虑到GNNs和基于transformer的模型之间存在重大差异，将这些方法直接应用于GNNs证明效果较弱。在本文中，我们对GNNs的δ调节技术进行了全面比较，并提出了一种专门为GNNs设计的新型δ调节方法——AdapterGNN。AdapterGNN保留了大型预训练模型的知识，并利用高度表达的GNN适配器，在仅有少量参数的情况下有效地适应下游任务，同时提高了模型的下游任务的泛化能力。广泛的实验表明，AdapterGNN在几个基准数据集上取得了最先进的性能，同时具有高效的特点。

    Fine-tuning pre-trained models has recently yielded remarkable performance gains in graph neural networks (GNNs). In addition to pre-training techniques, inspired by the latest work in the natural language fields, more recent work has shifted towards applying effective fine-tuning approaches, such as parameter-efficient tuning (delta tuning). However, given the substantial differences between GNNs and transformer-based models, applying such approaches directly to GNNs proved to be less effective. In this paper, we present a comprehensive comparison of delta tuning techniques for GNNs and propose a novel delta tuning method specifically designed for GNNs, called AdapterGNN. AdapterGNN preserves the knowledge of the large pre-trained model and leverages highly expressive adapters for GNNs, which can adapt to downstream tasks effectively with only a few parameters, while also improving the model's generalization ability on the downstream tasks. Extensive experiments show that AdapterGNN a
    
[^25]: 利用双时间尺度制度证明神经网络的收敛性研究

    Leveraging the two timescale regime to demonstrate convergence of neural networks. (arXiv:2304.09576v1 [math.OC])

    [http://arxiv.org/abs/2304.09576](http://arxiv.org/abs/2304.09576)

    研究了双时间尺度制度下浅层神经网络的训练动态，证明了梯度流收敛于全局最优解，无需神经元数量趋于无限，并提供了实验证明。

    

    我们研究了浅层神经网络的训练动态，在内层步长远小于外层步长的双时间尺度制度下。在这个制度下，在简单的单变量环境中，我们证明了梯度流收敛于非凸优化问题的全局最优解。我们的结果不需要神经元数量趋于无限，这使我们的结果不同于最近流行的方法，如神经切向核或平均场制度。我们提供实验说明，显示随机梯度下降按照我们对梯度流的描述进行行为，并因此在双时间尺度制度下收敛于全局最优解，但在此制度之外可能失败。

    We study the training dynamics of shallow neural networks, in a two-timescale regime in which the stepsizes for the inner layer are much smaller than those for the outer layer. In this regime, we prove convergence of the gradient flow to a global optimum of the non-convex optimization problem in a simple univariate setting. The number of neurons need not be asymptotically large for our result to hold, distinguishing our result from popular recent approaches such as the neural tangent kernel or mean-field regimes. Experimental illustration is provided, showing that the stochastic gradient descent behaves according to our description of the gradient flow and thus converges to a global optimum in the two-timescale regime, but can fail outside of this regime.
    
[^26]: 基于安全增强神经网络的非线性近似模型预测控制

    Approximate non-linear model predictive control with safety-augmented neural networks. (arXiv:2304.09575v1 [eess.SY])

    [http://arxiv.org/abs/2304.09575](http://arxiv.org/abs/2304.09575)

    本文提出了一种基于神经网络(NNs)的非线性模型预测控制(MPC)的近似方法，称为安全增强，可以使解决方案在线可行并具有收敛和约束条件的确定保证。

    

    模型预测控制(MPC)可以实现对于一般非线性系统的稳定性和约束条件的满足，但需要进行计算开销很大的在线优化。本文研究了通过神经网络(NNs)对这种MPC控制器的近似，以实现快速的在线评估。我们提出了安全增强，尽管存在近似不准确性，但可以获得收敛和约束条件的确定保证。我们使用NN近似MPC的整个输入序列，这使得我们在线验证它是否是MPC问题的可行解。当该解决方案不可行或成本更高时，我们基于标准MPC技术将NN解决方案替换为安全候选解。我们的方法仅需要对NN进行一次评估和对输入序列进行在线前向积分，这在资源受限系统上的计算速度很快。所提出的控制框架在三个不同复杂度的非线性MPC基准上进行了演示，展示了计算效率。

    Model predictive control (MPC) achieves stability and constraint satisfaction for general nonlinear systems, but requires computationally expensive online optimization. This paper studies approximations of such MPC controllers via neural networks (NNs) to achieve fast online evaluation. We propose safety augmentation that yields deterministic guarantees for convergence and constraint satisfaction despite approximation inaccuracies. We approximate the entire input sequence of the MPC with NNs, which allows us to verify online if it is a feasible solution to the MPC problem. We replace the NN solution by a safe candidate based on standard MPC techniques whenever it is infeasible or has worse cost. Our method requires a single evaluation of the NN and forward integration of the input sequence online, which is fast to compute on resource-constrained systems. The proposed control framework is illustrated on three non-linear MPC benchmarks of different complexity, demonstrating computational
    
[^27]: 基于物联网、大数据和机器学习的空气污染监测和预测系统的现状

    The State-of-the-Art in Air Pollution Monitoring and Forecasting Systems using IoT, Big Data, and Machine Learning. (arXiv:2304.09574v1 [cs.LG])

    [http://arxiv.org/abs/2304.09574](http://arxiv.org/abs/2304.09574)

    本文综述了基于物联网、大数据和机器学习的空气污染监测和预测系统的现状。最近的研究使用这些技术来提高空气质量数据的收集、分析和预测模型，以实现更高的效率和可靠性。

    

    空气质量与人类、植被和野生动物的生活质量密切相关。 因此，需要对其进行持续监测和保护。 交通，工业，建筑工地，发电机，烟花和焚烧废品等来源在降低空气质量方面占有重要比例。 这些来源需要以安全和可控的方式使用。 使用传统的实验室分析或每隔几英里就安装大型昂贵模型已不再高效。 需要智能设备来收集和分析空气数据。 空气质量取决于各种因素，包括位置，交通和时间。 最近的研究正在使用机器学习算法，大数据技术和物联网来提出稳定而有效的模型来实现上述目的。 本文重点研究和编译了该领域的最新研究，并强调数据源，监测和预测模型。 本文的主要目标是概述使用物联网，大数据和机器学习进行空气污染监测和预测的现状。

    The quality of air is closely linked with the life quality of humans, plantations, and wildlife. It needs to be monitored and preserved continuously. Transportations, industries, construction sites, generators, fireworks, and waste burning have a major percentage in degrading the air quality. These sources are required to be used in a safe and controlled manner. Using traditional laboratory analysis or installing bulk and expensive models every few miles is no longer efficient. Smart devices are needed for collecting and analyzing air data. The quality of air depends on various factors, including location, traffic, and time. Recent researches are using machine learning algorithms, big data technologies, and the Internet of Things to propose a stable and efficient model for the stated purpose. This review paper focuses on studying and compiling recent research in this field and emphasizes the Data sources, Monitoring, and Forecasting models. The main objective of this paper is to provid
    
[^28]: 去噪余弦相似度：一种理论驱动的有效表示学习方法

    Denoising Cosine Similarity: A Theory-Driven Approach for Efficient Representation Learning. (arXiv:2304.09552v1 [stat.ML])

    [http://arxiv.org/abs/2304.09552](http://arxiv.org/abs/2304.09552)

    本文提出了一种去噪余弦相似度（dCS）损失函数， 可以用于在原始数据集中学习鲁棒的表示形式。

    

    表示学习已经在机器学习的研究和实践中发挥着越来越大的影响，因为它能够学习出可以有效应用于各种下游任务的表示形式。然而，最近的研究很少关注到表示学习阶段中使用的真实数据集通常会受到噪声污染，这可能会降低学习出的表示形式的质量。本文解决了在原始数据集中学习鲁棒表示形式的问题。为此，受到最近去噪相关工作和基于余弦相似度目标函数在表示学习中的成功的启发，我们提出了去噪余弦相似度（dCS）损失函数。dCS损失是一种修改过的余弦相似度损失函数，具有去噪属性，这一点得到了我们的理论和实证研究的支持。为了使dCS损失可实现，我们还构建了具有统计保证的dCS损失估计器。

    Representation learning has been increasing its impact on the research and practice of machine learning, since it enables to learn representations that can apply to various downstream tasks efficiently. However, recent works pay little attention to the fact that real-world datasets used during the stage of representation learning are commonly contaminated by noise, which can degrade the quality of learned representations. This paper tackles the problem to learn robust representations against noise in a raw dataset. To this end, inspired by recent works on denoising and the success of the cosine-similarity-based objective functions in representation learning, we propose the denoising Cosine-Similarity (dCS) loss. The dCS loss is a modified cosine-similarity loss and incorporates a denoising property, which is supported by both our theoretical and empirical findings. To make the dCS loss implementable, we also construct the estimators of the dCS loss with statistical guarantees. Finally,
    
[^29]: SemEval 2023 任务6: LegalEval -- 理解法律文本

    SemEval 2023 Task 6: LegalEval -- Understanding Legal Texts. (arXiv:2304.09548v1 [cs.CL])

    [http://arxiv.org/abs/2304.09548](http://arxiv.org/abs/2304.09548)

    SemEval 2023举办了LegalEval共享任务，即理解法律文本，包括 自动结构化和语义连贯化的法律文件（Task-A），法律命名实体识别（Task-B）以及自动预测法律案件结果和提供预测解释（Task-C）。26个团队提交了系统论文并在所有子任务中优于基准线，但仍有改进空间。

    

    在人口众多的国家，待处理的法律案件呈指数增长。有必要开发基于自然语言处理的技术，对法律文件进行处理和自动理解。为了促进在法律自然语言处理领域的研究，我们在 SemEval 2023 上组织了共享任务 LegalEval - 理解法律文本。LegalEval 任务有三个子任务：Task-A（修辞角色标记）是自动将法律文件结构化为语义连贯的单元，Task-B（法律命名实体识别）处理在法律文件中识别相关实体，而 Task-C（法院判决预测与解释）探索了自动预测法律案件结果以及提供预测解释的可能性。共有26个团队（分布在全球的约100名参与者）提交了系统论文。在每个子任务中，所提出的系统都优于基准线；但是，仍然有很大的改进空间。本文介绍了 LegalEval 任务的组织和细节，并概述了参与系统及其性能。

    In populous countries, pending legal cases have been growing exponentially. There is a need for developing NLP-based techniques for processing and automatically understanding legal documents. To promote research in the area of Legal NLP we organized the shared task LegalEval - Understanding Legal Texts at SemEval 2023. LegalEval task has three sub-tasks: Task-A (Rhetorical Roles Labeling) is about automatically structuring legal documents into semantically coherent units, Task-B (Legal Named Entity Recognition) deals with identifying relevant entities in a legal document and Task-C (Court Judgement Prediction with Explanation) explores the possibility of automatically predicting the outcome of a legal case along with providing an explanation for the prediction. In total 26 teams (approx. 100 participants spread across the world) submitted systems paper. In each of the sub-tasks, the proposed systems outperformed the baselines; however, there is a lot of scope for improvement. This pape
    
[^30]: 有效的多智能体Q学习的图探索方法

    Graph Exploration for Effective Multi-agent Q-Learning. (arXiv:2304.09547v1 [cs.LG])

    [http://arxiv.org/abs/2304.09547](http://arxiv.org/abs/2304.09547)

    本文提出了一种基于图通信的多智能体强化学习探索技术，在智能体的协作下估计状态行为空间不确定性，以实现更有效的探索行为，同时也不需要计数机制和复杂的转换技术，该方案允许智能体在完全分散的方式下进行通信。

    

    本文为基于图通信的多智能体强化学习（MARL）提出了一种探索技术。我们假设智能体收到的个体奖励和其他智能体的动作无关，但策略相互耦合。在所提出的框架中，相邻的智能体合作估计状态行为空间不确定性，以执行更有效的探索行为。与现有方法不同，所提出的算法不需要计数机制，并可以应用于连续状态环境，而无需复杂的转换技术。此外，所提出的方案允许智能体以完全分散的方式进行通信，最小化信息交换。对于连续状态场景，每个智能体只需要交换一个参数向量。离散状态场景的理论结果以及连续状态的实验验证了算法的性能。

    This paper proposes an exploration technique for multi-agent reinforcement learning (MARL) with graph-based communication among agents. We assume the individual rewards received by the agents are independent of the actions by the other agents, while their policies are coupled. In the proposed framework, neighbouring agents collaborate to estimate the uncertainty about the state-action space in order to execute more efficient explorative behaviour. Different from existing works, the proposed algorithm does not require counting mechanisms and can be applied to continuous-state environments without requiring complex conversion techniques. Moreover, the proposed scheme allows agents to communicate in a fully decentralized manner with minimal information exchange. And for continuous-state scenarios, each agent needs to exchange only a single parameter vector. The performance of the algorithm is verified with theoretical results for discrete-state scenarios and with experiments for continuou
    
[^31]: SelfAct: 基于自监督和主动学习的个性化活动识别

    SelfAct: Personalized Activity Recognition based on Self-Supervised and Active Learning. (arXiv:2304.09530v1 [cs.LG])

    [http://arxiv.org/abs/2304.09530](http://arxiv.org/abs/2304.09530)

    SelfAct是一种基于自我监督和主动学习的人体活动识别框架，可以用大量未标记数据进行预训练，并通过新的无监督主动学习策略进行微调，从而实现对每个用户的个性化活动识别。

    

    目前，基于传感器的人体活动识别已经成为了穿戴设备和移动设备上应用广泛的方法，监督深度学习模型是目前领先的方法。然而，训练这些模型需要大量标记数据，采集这些数据通常是耗时、昂贵且容易出错的。与此同时，由于活动执行的内部和外部可变性，活动模型应该为每个用户个性化设计。本文提出了SelfAct：一种新的用于缓解这些问题的人体活动识别框架，它结合自监督和主动学习。SelfAct利用从许多用户收集的大量未标记数据进行自我监督预训练深度学习模型，以学习有意义、高效的传感器数据的潜在表示。由此得出的预训练模型可以被新用户当地使用，他们将通过一种新的无监督主动学习策略来微调这个模型。我们在两个公开可用的人体活动识别数据集上的实验表明，SelfAct在准确性和模型效率方面都取得了优异的表现。

    Supervised Deep Learning (DL) models are currently the leading approach for sensor-based Human Activity Recognition (HAR) on wearable and mobile devices. However, training them requires large amounts of labeled data whose collection is often time-consuming, expensive, and error-prone. At the same time, due to the intra- and inter-variability of activity execution, activity models should be personalized for each user. In this work, we propose SelfAct: a novel framework for HAR combining self-supervised and active learning to mitigate these problems. SelfAct leverages a large pool of unlabeled data collected from many users to pre-train through self-supervision a DL model, with the goal of learning a meaningful and efficient latent representation of sensor data. The resulting pre-trained model can be locally used by new users, which will fine-tune it thanks to a novel unsupervised active learning strategy. Our experiments on two publicly available HAR datasets demonstrate that SelfAct ac
    
[^32]: 防止属性推断、数据重构和特征空间劫持攻击的安全分布式学习算法

    Secure Split Learning against Property Inference, Data Reconstruction, and Feature Space Hijacking Attacks. (arXiv:2304.09515v1 [cs.LG])

    [http://arxiv.org/abs/2304.09515](http://arxiv.org/abs/2304.09515)

    本论文研究了分布式学习面临的安全问题并提出了一种隐私保护通道，使用新的激活函数R3eLU避免属性推断、数据重组和特征劫持等攻击。实验表明，该方法在保护隐私的同时，性能相当于不保护隐私的最新方法。

    

    分布式学习已经成为一种解决来自不同背景的客户和主机共同学习的有希望的方法。然而，这种方法可能为恶意攻击者创造新的攻击面，使其在实际使用中具有局限性。本文针对分布式学习中可能遭受的属性推断、数据重组和特征劫持等威胁进行研究，并提出了解决方案，以确保分布式学习的学习保障。我们设计了一个隐私保护通道，用于客户和主机之间的信息交换，防止潜在的威胁，并保证了系统的安全稳定性。

    Split learning of deep neural networks (SplitNN) has provided a promising solution to learning jointly for the mutual interest of a guest and a host, which may come from different backgrounds, holding features partitioned vertically. However, SplitNN creates a new attack surface for the adversarial participant, holding back its practical use in the real world. By investigating the adversarial effects of highly threatening attacks, including property inference, data reconstruction, and feature hijacking attacks, we identify the underlying vulnerability of SplitNN and propose a countermeasure. To prevent potential threats and ensure the learning guarantees of SplitNN, we design a privacy-preserving tunnel for information exchange between the guest and the host. The intuition is to perturb the propagation of knowledge in each direction with a controllable unified solution. To this end, we propose a new activation function named R3eLU, transferring private smashed data and partial loss int
    
[^33]: NetGPT：网络流量生成预训练变压器模型

    NetGPT: Generative Pretrained Transformer for Network Traffic. (arXiv:2304.09513v1 [cs.NI])

    [http://arxiv.org/abs/2304.09513](http://arxiv.org/abs/2304.09513)

    本文提出了首个网络流量生成预训练变压器模型NetGPT，该模型可以优化网络任务的训练效率和有效性。

    

    预训练模型可以利用大规模的原始数据学习网络流量的基本特征，并为输入流量生成可区分的结果，而不考虑特定的下游任务。有效的预训练模型可以显著优化下游任务的训练效率和有效性，例如流量分类、攻击检测、资源调度、协议分析和流量生成。本文提出了NetGPT，旨在为网络流量构建预训练模型并解决多样的挑战。

    Pretrained models for network traffic can utilize large-scale raw data to learn the essential characteristics of network traffic, and generate distinguishable results for input traffic without considering specific downstream tasks. Effective pretrained models can significantly optimize the training efficiency and effectiveness of downstream tasks, such as traffic classification, attack detection, resource scheduling, protocol analysis, and traffic generation. Despite the great success of pretraining in natural language processing, there is no work in the network field. Considering the diverse demands and characteristics of network traffic and network tasks, it is non-trivial to build a pretrained model for network traffic and we face various challenges, especially the heterogeneous headers and payloads in the multi-pattern network traffic and the different dependencies for contexts of diverse downstream network tasks.  To tackle these challenges, in this paper, we make the first attemp
    
[^34]: 基于KNN的修正Medoid-Shift的社区检测

    Community Detection Using Revised Medoid-Shift Based on KNN. (arXiv:2304.09512v1 [cs.SI])

    [http://arxiv.org/abs/2304.09512](http://arxiv.org/abs/2304.09512)

    在本文中，作者提出了一种名为修正Medoid-Shift（RMS）的新聚类算法，用于社区检测，该算法可以更好地解决社交网络中的问题。

    

    随着社交网络的兴起，社区检测成为了一个重要的问题。虽然均值漂移是一种出色的聚类算法，但由于均值漂移算法只能处理具有坐标的数据，而社区检测问题中的数据大多以图的形式表示，可以视为具有距离矩阵（或相似度矩阵）的数据，因此均值漂移算法不能直接用于社区检测。幸运的是，提出了一种新的聚类算法 Medoid-Shift，该算法保留了均值漂移的优点，并可以应用于基于距离矩阵的问题，如社区检测。Medoid-Shift算法的一个缺点是可能在由距离参数定义的邻域区域内没有数据点。 为了更好地处理社区检测问题，因此在本文中提出了一种名为修正Medoid-Shift（RMS）的新算法。 在寻找下一个中心点的过程中，RMS算法基于邻域的定义。

    Community detection becomes an important problem with the booming of social networks. As an excellent clustering algorithm, Mean-Shift can not be applied directly to community detection, since Mean-Shift can only handle data with coordinates, while the data in the community detection problem is mostly represented by a graph that can be treated as data with a distance matrix (or similarity matrix). Fortunately, a new clustering algorithm called Medoid-Shift is proposed. The Medoid-Shift algorithm preserves the benefits of Mean-Shift and can be applied to problems based on distance matrix, such as community detection. One drawback of the Medoid-Shift algorithm is that there may be no data points within the neighborhood region defined by a distance parameter. To deal with the community detection problem better, a new algorithm called Revised Medoid-Shift (RMS) in this work is thus proposed. During the process of finding the next medoid, the RMS algorithm is based on a neighborhood defined
    
[^35]: 无序目标下神经网络的责任问题

    The Responsibility Problem in Neural Networks with Unordered Targets. (arXiv:2304.09499v1 [cs.LG])

    [http://arxiv.org/abs/2304.09499](http://arxiv.org/abs/2304.09499)

    本文解决了无序目标下神经网络的不连续性问题，并鼓励进一步研究。

    

    本文讨论将无序对象映射到固定排列神经网络输出时出现的不连续性，称之为责任问题。以前的工作已经通过识别单个不连续性证明了该问题的存在。本文证明，在这种模型下不连续性是不可数无限的，鼓励进一步研究适用于无序数据的神经网络。

    We discuss the discontinuities that arise when mapping unordered objects to neural network outputs of fixed permutation, referred to as the responsibility problem. Prior work has proved the existence of the issue by identifying a single discontinuity. Here, we show that discontinuities under such models are uncountably infinite, motivating further research into neural networks for unordered data.
    
[^36]: 利用深度确定性策略梯度进行高优先级用户的学习资源调度

    Learning Resource Scheduling with High Priority Users using Deep Deterministic Policy Gradients. (arXiv:2304.09488v1 [eess.SY])

    [http://arxiv.org/abs/2304.09488](http://arxiv.org/abs/2304.09488)

    本文探索了利用深度确定性策略梯度（\ddpg）方法学习通信资源调度算法的应用，特别是限制于优先用户。

    

    移动通信功能的进步为院前和院内护理过程的更紧密整合打开了大门。例如，医学专家可以指导现场医护人员，并且可以提供实时生命体征或视觉。将这种对性能至关重要的应用与移动通信的高度复杂的工作结合起来，需要可靠且高效的解决方案，同时易于与现有系统集成。本文探讨了利用深度确定性策略梯度（\ddpg）方法学习通信资源调度算法的应用，特别是限制于优先用户。与普通的深度Q网络不同，\ddpg 能够产生连续值的输出。经过轻微的后处理，得到的调度器能够在灵活的总和效用目标上实现高性能。

    Advances in mobile communication capabilities open the door for closer integration of pre-hospital and in-hospital care processes. For example, medical specialists can be enabled to guide on-site paramedics and can, in turn, be supplied with live vitals or visuals. Consolidating such performance-critical applications with the highly complex workings of mobile communications requires solutions both reliable and efficient, yet easy to integrate with existing systems. This paper explores the application of Deep Deterministic Policy Gradient~(\ddpg) methods for learning a communications resource scheduling algorithm with special regards to priority users. Unlike the popular Deep-Q-Network methods, the \ddpg is able to produce continuous-valued output. With light post-processing, the resulting scheduler is able to achieve high performance on a flexible sum-utility goal.
    
[^37]: 语音助手应用中的安全和隐私问题：一项调查

    Security and Privacy Problems in Voice Assistant Applications: A Survey. (arXiv:2304.09486v1 [cs.CR])

    [http://arxiv.org/abs/2304.09486](http://arxiv.org/abs/2304.09486)

    语音助手应用面临安全和隐私问题，包括攻击模型和私人信息泄露，所以需要进行全面的调查以便概览当前研究。

    

    现在，语音助手应用已经无处不在。提供现实生活中两个最重要功能（如谷歌家庭、亚马逊Alexa、Siri等）的两个主要模型是自动语音识别（ASR）模型和说话人识别（SI）模型。根据最近的研究，随着物联网（IoT）的快速发展，安全和隐私威胁也出现了。研究的安全问题包括针对语音助手应用程序中广泛使用的机器学习模型和其他硬件组件的攻击技术。隐私问题包括技术上的信息窃取和政策上的隐私泄露。语音助手应用程序每年占据着不断增长的市场份额，但它们的隐私和安全问题一直在造成巨大的经济损失并危及用户的个人敏感信息。因此，有必要进行全面的调查，以概述当前研究的分类。

    Voice assistant applications have become omniscient nowadays. Two models that provide the two most important functions for real-life applications (i.e., Google Home, Amazon Alexa, Siri, etc.) are Automatic Speech Recognition (ASR) models and Speaker Identification (SI) models. According to recent studies, security and privacy threats have also emerged with the rapid development of the Internet of Things (IoT). The security issues researched include attack techniques toward machine learning models and other hardware components widely used in voice assistant applications. The privacy issues include technical-wise information stealing and policy-wise privacy breaches. The voice assistant application takes a steadily growing market share every year, but their privacy and security issues never stopped causing huge economic losses and endangering users' personal sensitive information. Thus, it is important to have a comprehensive survey to outline the categorization of the current research r
    
[^38]: DiFaReli: 扩散人脸重照技术

    DiFaReli : Diffusion Face Relighting. (arXiv:2304.09479v1 [cs.CV])

    [http://arxiv.org/abs/2304.09479](http://arxiv.org/abs/2304.09479)

    DiFaReli提出了一种新方法，通过利用条件扩散隐式模型解码解耦的光编码以及从现成的估算器推断出的与3D形状和面部身份相关的其他编码，能够处理单视角的野外环境下的人脸重照，无需光线舞台数据、多视图图像或光照基础事实，实验表明其效果优于现有方法。

    

    我们提出了一种新方法，用于处理野外环境下的单视角人脸重照。处理全局照明或投影阴影等非漫反射效应一直是人脸重照领域的难点。以往的研究通常假定兰伯特反射表面，简化光照模型，或者需要估计三维形状、反射率或阴影图。然而，这种估计是容易出错的，需要许多具有光照基础事实的训练样本才能很好地推广。我们的研究绕过了准确估计固有组件的需要，可以仅通过2D图像训练而不需要任何光线舞台数据、多视图图像或光照基础事实。我们的关键思想是利用条件扩散隐式模型（DDIM）解码解耦的光编码以及从现成的估算器推断出的与3D形状和面部身份相关的其他编码。我们还提出了一种新的调节技术，通过使用归一化方案，简化光与几何之间复杂互动的建模。在多个基准数据集上的实验表明，我们的方法优于现有方法。

    We present a novel approach to single-view face relighting in the wild. Handling non-diffuse effects, such as global illumination or cast shadows, has long been a challenge in face relighting. Prior work often assumes Lambertian surfaces, simplified lighting models or involves estimating 3D shape, albedo, or a shadow map. This estimation, however, is error-prone and requires many training examples with lighting ground truth to generalize well. Our work bypasses the need for accurate estimation of intrinsic components and can be trained solely on 2D images without any light stage data, multi-view images, or lighting ground truth. Our key idea is to leverage a conditional diffusion implicit model (DDIM) for decoding a disentangled light encoding along with other encodings related to 3D shape and facial identity inferred from off-the-shelf estimators. We also propose a novel conditioning technique that eases the modeling of the complex interaction between light and geometry by using a ren
    
[^39]: MAMAF-Net：用于中风诊断的运动感知和多关注融合网络

    MAMAF-Net: Motion-Aware and Multi-Attention Fusion Network for Stroke Diagnosis. (arXiv:2304.09466v1 [eess.IV])

    [http://arxiv.org/abs/2304.09466](http://arxiv.org/abs/2304.09466)

    本研究提出了一个名为MAMAF-Net的网络用于检测多模态检查视频中的中风情况，并提出了一个多数采样的数据集。这是第一个提供端到端解决方案的视频分析中的中风检测研究。

    

    中风是全球死亡率和残疾率的主要原因，每四人中就有一人有中风的危险。预先医院中风评估在准确识别中风患者以加速进一步检查和治疗方面发挥着关键作用。本文提出了一种名为MAMAF-Net的运动感知和多关注融合网络，可以检测多模式检查视频中的中风情况。与其他视频分析中的中风检测研究不同，我们提出了一个多数采样的数据集，用于从每个被试者的多个视频记录中进行端到端的解决方案，包括中风、短暂性缺血性发作（TIA）和健康对照。

    Stroke is a major cause of mortality and disability worldwide from which one in four people are in danger of incurring in their lifetime. The pre-hospital stroke assessment plays a vital role in identifying stroke patients accurately to accelerate further examination and treatment in hospitals. Accordingly, the National Institutes of Health Stroke Scale (NIHSS), Cincinnati Pre-hospital Stroke Scale (CPSS) and Face Arm Speed Time (F.A.S.T.) are globally known tests for stroke assessment. However, the validity of these tests is skeptical in the absence of neurologists. Therefore, in this study, we propose a motion-aware and multi-attention fusion network (MAMAF-Net) that can detect stroke from multimodal examination videos. Contrary to other studies on stroke detection from video analysis, our study for the first time proposes an end-to-end solution from multiple video recordings of each subject with a dataset encapsulating stroke, transient ischemic attack (TIA), and healthy controls. T
    
[^40]: EC^2: 基于身体控制的新型紧急交流方案

    EC^2: Emergent Communication for Embodied Control. (arXiv:2304.09448v1 [cs.LG])

    [http://arxiv.org/abs/2304.09448](http://arxiv.org/abs/2304.09448)

    EC^2 提出一种新的紧急通信方案，用于视频语言预训练以进行少样本身体控制，实现了在 EmbodiedAI 基准测试上的最先进的少样本性能。

    

    身体控制需要代理通过多模态预训练快速学习如何在新环境中行动，其中视频演示包含所需的视觉和运动细节以进行低级别知觉和控制，语言指令支持通过抽象符号结构进行泛化。虽然最近的方法应用对比学习来强制两种模式之间的对齐，但我们假设更好地建模它们之间的互补差异可以带来更全面的表示以进行下游适应。为此，我们提出了 Emergent Communication for Embodied Control (EC^2)，这是一种新的方案，用于预训练视频语言表示以进行少样本身体控制。关键思想是通过紧急通信学习视频的“语言”，它桥接了视频细节的语义和自然语言的结构。我们使用语言模型学习视频轨迹，紧急语言和自然语言的身体表示，然后对它们进行微调以进行身体控制任务。我们在 EmbodiedAI 基准测试上评估 EC^2，在三个任务上实现了最先进的少样本性能。

    Embodied control requires agents to leverage multi-modal pre-training to quickly learn how to act in new environments, where video demonstrations contain visual and motion details needed for low-level perception and control, and language instructions support generalization with abstract, symbolic structures. While recent approaches apply contrastive learning to force alignment between the two modalities, we hypothesize better modeling their complementary differences can lead to more holistic representations for downstream adaption. To this end, we propose Emergent Communication for Embodied Control (EC^2), a novel scheme to pre-train video-language representations for few-shot embodied control. The key idea is to learn an unsupervised "language" of videos via emergent communication, which bridges the semantics of video details and structures of natural language. We learn embodied representations of video trajectories, emergent language, and natural language using a language model, whic
    
[^41]: 鞅后验神经过程

    Martingale Posterior Neural Processes. (arXiv:2304.09431v1 [cs.LG])

    [http://arxiv.org/abs/2304.09431](http://arxiv.org/abs/2304.09431)

    本文提出了一种基于鞅后验的神经过程方法，用于估计使用神经网络隐式定义的随机过程，并在 benchmark 数据集上表现出更高的精度和样本效率。

    

    神经过程(NP)可用于估计使用神经网络隐式定义的随机过程，而不是预先规定已知先验的过程，例如高斯过程。理想的NP将从数据中学习一切而没有任何归纳偏差，但在实践中，我们常常为了方便估计而限制了随机过程的类别。本文提出了一种基于鞅后验的方法，为未来数据指定了一种预测分布，从而减小了生成函数时的不确定性，并提出一个新的MPNP框架，表现出比现有NP方法更高的精度和样本效率。

    A Neural Process (NP) estimates a stochastic process implicitly defined with neural networks given a stream of data, rather than pre-specifying priors already known, such as Gaussian processes. An ideal NP would learn everything from data without any inductive biases, but in practice, we often restrict the class of stochastic processes for the ease of estimation. One such restriction is the use of a finite-dimensional latent variable accounting for the uncertainty in the functions drawn from NPs. Some recent works show that this can be improved with more "data-driven" source of uncertainty such as bootstrapping. In this work, we take a different approach based on the martingale posterior, a recently developed alternative to Bayesian inference. For the martingale posterior, instead of specifying prior-likelihood pairs, a predictive distribution for future data is specified. Under specific conditions on the predictive distribution, it can be shown that the uncertainty in the generated fu
    
[^42]: 长尾分类的解耦训练和随机表示法

    Decoupled Training for Long-Tailed Classification With Stochastic Representations. (arXiv:2304.09426v1 [cs.LG])

    [http://arxiv.org/abs/2304.09426](http://arxiv.org/abs/2304.09426)

    本文提出了解耦训练和利用随机表示法进行分类的方法来解决长尾分类问题，使用SWA优化技术得到更好的特征提取器，并提出一种基于随机表示进行分类器重新训练的算法。

    

    在长尾数据分类中，解耦表示学习和分类器学习已经被证明是有效的。构建解耦式学习方案有两个主要因素：1）如何训练特征提取器进行表示学习，以便提供可泛化的表示；2）如何重新训练分类器，通过处理长尾数据中的类别不平衡来构建适当的决策边界。 本文首先应用随机权重平均（SWA）优化技术来获得更好的泛化特征提取器，用于长尾分类。然后，我们基于SWA-Gaussian提出了一种新的分类器重新训练算法，该算法利用基于不确定性估计的多样随机表示来构建更强大的分类器。在多个基准数据集上进行的广泛实验表明，所提出的方法有效地解决了长尾分类问题。

    Decoupling representation learning and classifier learning has been shown to be effective in classification with long-tailed data. There are two main ingredients in constructing a decoupled learning scheme; 1) how to train the feature extractor for representation learning so that it provides generalizable representations and 2) how to re-train the classifier that constructs proper decision boundaries by handling class imbalances in long-tailed data. In this work, we first apply Stochastic Weight Averaging (SWA), an optimization technique for improving the generalization of deep neural networks, to obtain better generalizing feature extractors for long-tailed classification. We then propose a novel classifier re-training algorithm based on stochastic representation obtained from the SWA-Gaussian, a Gaussian perturbed SWA, and a self-distillation strategy that can harness the diverse stochastic representations based on uncertainty estimates to build more robust classifiers. Extensive exp
    
[^43]: 大型神经网络的多校准可最小化损失

    Loss minimization yields multicalibration for large neural networks. (arXiv:2304.09424v1 [cs.LG])

    [http://arxiv.org/abs/2304.09424](http://arxiv.org/abs/2304.09424)

    本文展示了对于大型神经网络大小，最优地最小化损失会导致多校准，以提供公平的预测结果。

    

    多校准是一种公平性概念，旨在提供跨大量团体的准确预测。即使对于简单的预测器，如线性函数，多校准也被认为是与最小化损失不同的目标。在本文中，我们展示了对于（几乎所有的）大型神经网络大小，最优地最小化平方误差会导致多校准。我们的结果关于神经网络的表征方面，而不是关于算法或样本复杂性考虑。以前的这样的结果仅适用于几乎贝叶斯最优的预测器，因此是表征无关的。我们强调，我们的结果不适用于优化神经网络的特定算法，如 SGD，并且不应解释为“公平性从优化神经网络中获得免费的好处”。

    Multicalibration is a notion of fairness that aims to provide accurate predictions across a large set of groups. Multicalibration is known to be a different goal than loss minimization, even for simple predictors such as linear functions. In this note, we show that for (almost all) large neural network sizes, optimally minimizing squared error leads to multicalibration. Our results are about representational aspects of neural networks, and not about algorithmic or sample complexity considerations. Previous such results were known only for predictors that were nearly Bayes-optimal and were therefore representation independent. We emphasize that our results do not apply to specific algorithms for optimizing neural networks, such as SGD, and they should not be interpreted as "fairness comes for free from optimizing neural networks".
    
[^44]: TieFake: 标题-正文相似度和情感感知的假新闻检测

    TieFake: Title-Text Similarity and Emotion-Aware Fake News Detection. (arXiv:2304.09421v1 [cs.CL])

    [http://arxiv.org/abs/2304.09421](http://arxiv.org/abs/2304.09421)

    该论文提出了一种新颖的假新闻检测方法TieFake，通过检测新闻标题和正文之间的相似度和情感信息，同时通过BERT和ResNeSt学习文本和图像的表示，实现对多模态文本的假新闻检测。

    

    假新闻检测旨在检测在社交媒体平台上广泛传播的虚假新闻，这可能会对公众和政府产生负面影响。许多方法已经开发出来，以从新闻图像、文本或视频中获取相关信息。然而，这些方法可能会面临以下限制：（1）忽略新闻中固有的情感信息，这可能有益，因为它包含作者的主观意图；（2）对于新闻文章中标题与正文之间的关系（相似度）给予很少关注，这经常使用无关标题来吸引读者的注意力。为此，我们提出一种新颖的标题-正文相似度和情感感知假新闻检测（TieFake）方法，通过在统一框架中共同建模多模态上下文信息和作者情感来实现。具体而言，我们分别使用BERT和ResNeSt来学习文本和图像的表示，并利用发行方情感提取器。

    Fake news detection aims to detect fake news widely spreading on social media platforms, which can negatively influence the public and the government. Many approaches have been developed to exploit relevant information from news images, text, or videos. However, these methods may suffer from the following limitations: (1) ignore the inherent emotional information of the news, which could be beneficial since it contains the subjective intentions of the authors; (2) pay little attention to the relation (similarity) between the title and textual information in news articles, which often use irrelevant title to attract reader' attention. To this end, we propose a novel Title-Text similarity and emotion-aware Fake news detection (TieFake) method by jointly modeling the multi-modal context information and the author sentiment in a unified framework. Specifically, we respectively employ BERT and ResNeSt to learn the representations for text and images, and utilize publisher emotion extractor 
    
[^45]: 小波胜过了猴子——对抗鲁棒性的研究

    Wavelets Beat Monkeys at Adversarial Robustness. (arXiv:2304.09403v1 [cs.LG])

    [http://arxiv.org/abs/2304.09403](http://arxiv.org/abs/2304.09403)

    该研究旨在探讨如何开发出能够像人类视觉一样鲁棒地概括的神经网络，提高对抗性噪声的鲁棒性，并提出了一种模仿灵长类动物视觉皮层(V1)的神经网络模型，其在小扰动下具有非平凡的对抗鲁棒性，该模型使用小波方法。

    

    研究人员一直致力于提高神经网络对对抗性噪声的鲁棒性——即对数据中微不可见的恶意扰动的抵御能力。目前，获取强大神经网络鲁棒性的无可争议的最先进防御策略是对抗性训练(AT)，但它比标准训练耗费更多资源，同时在鲁棒性和准确性之间有所折衷。最近的一项鼓舞人心的工作[Dapello等人]旨在将神经生物学工具应用于问题：我们如何开发能够像人类视觉一样鲁棒地概括的神经网络？[Dapello等人]设计了一个神经隐藏第一层的网络结构，该层模仿了灵长类动物的视觉皮层(V1)，其后是从当前CNN视觉模型调整而来的后端结构。当在小扰动条件下在标准视觉基准测试中测试时，似乎实现了非平凡的对抗鲁棒性。在这里，我们重新审视这项受生物启发的研究，并问一个问题：是否存在一个基于原则的无参数表示方法?

    Research on improving the robustness of neural networks to adversarial noise - imperceptible malicious perturbations of the data - has received significant attention. The currently uncontested state-of-the-art defense to obtain robust deep neural networks is Adversarial Training (AT), but it consumes significantly more resources compared to standard training and trades off accuracy for robustness. An inspiring recent work [Dapello et al.] aims to bring neurobiological tools to the question: How can we develop Neural Nets that robustly generalize like human vision? [Dapello et al.] design a network structure with a neural hidden first layer that mimics the primate primary visual cortex (V1), followed by a back-end structure adapted from current CNN vision models. It seems to achieve non-trivial adversarial robustness on standard vision benchmarks when tested on small perturbations. Here we revisit this biologically inspired work, and ask whether a principled parameter-free representatio
    
[^46]: MixPro：基于提示学习的简单有效的数据增强方式

    MixPro: Simple yet Effective Data Augmentation for Prompt-based Learning. (arXiv:2304.09402v1 [cs.CL])

    [http://arxiv.org/abs/2304.09402](http://arxiv.org/abs/2304.09402)

    MixPro是一种数据增强方法，通过对原始输入和模板进行混合来提高基于提示的学习性能，平均提高了5.08%的模型性能。

    

    基于提示的学习通过将输入与模板组合起来，将下游任务重构为填空问题。这种技术在少样本学习中特别有用，然而，使用有限的模板和文本仍然存在显着的性能改进空间。此外，现有的使用模型集成的方法可以限制模型的效率。为解决这些问题，我们提出了一种称为MixPro的增强方法，它通过标记级、句子级和时代级的混合策略来增强原始输入文本和模板。我们在五个少样本数据集上进行了实验，结果表明MixPro优于其他增强基线，相比增强前，平均提高了5.08%的模型性能。

    Prompt-based learning reformulates downstream tasks as cloze problems by combining the original input with a template. This technique is particularly useful in few-shot learning, where a model is trained on a limited amount of data. However, the limited templates and text used in few-shot prompt-based learning still leave significant room for performance improvement. Additionally, existing methods using model ensembles can constrain the model efficiency. To address these issues, we propose an augmentation method called MixPro, which augments both the vanilla input text and the templates through token-level, sentence-level, and epoch-level Mixup strategies. We conduct experiments on five few-shot datasets, and the results show that MixPro outperforms other augmentation baselines, improving model performance by an average of 5.08% compared to before augmentation.
    
[^47]: 信息几何广义协变量漂移适应

    Information Geometrically Generalized Covariate Shift Adaptation. (arXiv:2304.09387v1 [cs.LG])

    [http://arxiv.org/abs/2304.09387](http://arxiv.org/abs/2304.09387)

    该论文基于信息几何框架统一了协变量漂移适应家族，提出了一种有效的几何广义协变量漂移适应方法，并在数值实验中展示了其优越性能。

    

    许多机器学习方法假设训练和测试数据遵循相同的分布。然而，在现实世界中，这个假设经常被违反。特别地，数据的边缘分布发生变化的现象称为协变量漂移，是机器学习中最重要的研究主题之一。我们表明，众所周知的协变量漂移适应方法家族在信息几何框架下是统一的。此外，我们表明，可以有效地实现几何广义协变量漂移适应方法的参数搜索。数值实验表明，我们的广义方法可以比现有的方法实现更好的性能。

    Many machine learning methods assume that the training and test data follow the same distribution. However, in the real world, this assumption is very often violated. In particular, the phenomenon that the marginal distribution of the data changes is called covariate shift, one of the most important research topics in machine learning. We show that the well-known family of covariate shift adaptation methods is unified in the framework of information geometry. Furthermore, we show that parameter search for geometrically generalized covariate shift adaptation method can be achieved efficiently. Numerical experiments show that our generalization can achieve better performance than the existing methods it encompasses.
    
[^48]: 增强物理知识的深度神经网络在海表温度预测中的应用

    Physical Knowledge Enhanced Deep Neural Network for Sea Surface Temperature Prediction. (arXiv:2304.09376v1 [cs.LG])

    [http://arxiv.org/abs/2304.09376](http://arxiv.org/abs/2304.09376)

    该论文提出了一种从历史观测数据中转移物理知识到数值模型的海表温度预测方法，旨在提高预测准确性。

    

    传统上，数值模型在海洋学研究中被用来模拟海洋动力学，代表了物理方程。然而，许多与海洋动力学有关的因素似乎是不确定的。我们认为，将从观测数据中提取的物理知识转移可以进一步提高数值模型在预测海表温度（SST）时的准确性。最近，地球观测技术的进步产生了巨大的数据增长。因此，探索利用越来越多的历史观测数据来改进和补充数值模型的方法至关重要。为此，我们介绍了一种从历史观测数据中转移物理知识到数值模型的SST预测方法。具体而言，我们使用一个编码器和生成对抗网络（GAN）的组合来捕获从观测数据中提取的物理知识。然后，将数值模型数据馈入预训练模型以进行预测。

    Traditionally, numerical models have been deployed in oceanography studies to simulate ocean dynamics by representing physical equations. However, many factors pertaining to ocean dynamics seem to be ill-defined. We argue that transferring physical knowledge from observed data could further improve the accuracy of numerical models when predicting Sea Surface Temperature (SST). Recently, the advances in earth observation technologies have yielded a monumental growth of data. Consequently, it is imperative to explore ways in which to improve and supplement numerical models utilizing the ever-increasing amounts of historical observational data. To this end, we introduce a method for SST prediction that transfers physical knowledge from historical observations to numerical models. Specifically, we use a combination of an encoder and a generative adversarial network (GAN) to capture physical knowledge from the observed data. The numerical model data is then fed into the pre-trained model to
    
[^49]: 洗牌和切割：长文本的对比学习

    Shuffle & Divide: Contrastive Learning for Long Text. (arXiv:2304.09374v1 [cs.CL])

    [http://arxiv.org/abs/2304.09374](http://arxiv.org/abs/2304.09374)

    本文介绍了一种基于对比学习的自监督学习方法，通过“洗牌和切割”算法对长文本进行预处理，提取BERT嵌入，在无监督的情况下对文本进行分类，比当前最先进的技术提高20.94%。

    

    我们提出了一种基于对比学习的自监督学习方法，用于长文本文档。我们的方法的关键在于“洗牌和切割”（SaD），这是一种简单的文本增广算法，可为基于BERT的文档嵌入所需的对比更新设置一个前置任务。SaD将文档拆分为两个子文档，其中包含随机洗牌的单词。这些子文档被视为正样本，将所有其他文档视为负样本。在SaD之后，我们重复对比更新和聚类阶段，直至收敛。我们的方法可以帮助减轻人力资源的工作量，从而节省昂贵的AI资源。我们通过对20 Newsgroups、Reuters-21578、BBC和BBCSport数据集进行无监督文本分类的经验评估。特别是，我们的方法在20 Newsgroups上将当前的最新技术SS-SB-MT提高了20.94％。

    We propose a self-supervised learning method for long text documents based on contrastive learning. A key to our method is Shuffle and Divide (SaD), a simple text augmentation algorithm that sets up a pretext task required for contrastive updates to BERT-based document embedding. SaD splits a document into two sub-documents containing randomly shuffled words in the entire documents. The sub-documents are considered positive examples, leaving all other documents in the corpus as negatives. After SaD, we repeat the contrastive update and clustering phases until convergence. It is naturally a time-consuming, cumbersome task to label text documents, and our method can help alleviate human efforts, which are most expensive resources in AI. We have empirically evaluated our method by performing unsupervised text classification on the 20 Newsgroups, Reuters-21578, BBC, and BBCSport datasets. In particular, our method pushes the current state-of-the-art, SS-SB-MT, on 20 Newsgroups by 20.94% in
    
[^50]: ContraCluster：通过对比自监督和基于原型的半监督学习学习无标签分类

    ContraCluster: Learning to Classify without Labels by Contrastive Self-Supervision and Prototype-Based Semi-Supervision. (arXiv:2304.09369v1 [cs.CV])

    [http://arxiv.org/abs/2304.09369](http://arxiv.org/abs/2304.09369)

    ContraCluster是一种无监督的图像分类方法，结合了聚类和对比自监督学习。ContraCluster使用对比原型采样和基于原型的半监督微调来提高准确性，并且在CIFAR-10等标准基准数据集上实现了新的最先进结果。

    

    最近表示学习的进展启发我们采用一种有原则的方式应对无监督的图像分类任务。我们提出了ContraCluster，这是一种无监督的图像分类方法，将聚类与对比自监督学习的能力相结合。ContraCluster包括三个阶段：(1) 对比自监督预训练(CPT)，(2) 对比原型采样(CPS)，(3) 基于原型的半监督微调(PB-SFT)。CPS可以在由对比学习学习的嵌入空间中选择高准确性的类别原型图像。我们使用采样的原型作为带噪声的标记数据来执行半监督微调(PB-SFT)，利用小原型和大规模未标记数据进一步提高准确性。我们凭经验证明，ContraCluster在标准基准数据集包括CIFAR-10，STL-10和ImageNet-10中实现了新的最先进结果。例如，ContraCluster在仅有10%标记数据的情况下实现了87.9％的CIFAR-10准确率，超过了先前最先进技术水平3.8％。

    The recent advances in representation learning inspire us to take on the challenging problem of unsupervised image classification tasks in a principled way. We propose ContraCluster, an unsupervised image classification method that combines clustering with the power of contrastive self-supervised learning. ContraCluster consists of three stages: (1) contrastive self-supervised pre-training (CPT), (2) contrastive prototype sampling (CPS), and (3) prototype-based semi-supervised fine-tuning (PB-SFT). CPS can select highly accurate, categorically prototypical images in an embedding space learned by contrastive learning. We use sampled prototypes as noisy labeled data to perform semi-supervised fine-tuning (PB-SFT), leveraging small prototypes and large unlabeled data to further enhance the accuracy. We demonstrate empirically that ContraCluster achieves new state-of-the-art results for standard benchmark datasets including CIFAR-10, STL-10, and ImageNet-10. For example, ContraCluster achi
    
[^51]: 基于图神经网络的河流系统异常检测

    Graph Neural Network-Based Anomaly Detection for River Network Systems. (arXiv:2304.09367v1 [cs.LG])

    [http://arxiv.org/abs/2304.09367](http://arxiv.org/abs/2304.09367)

    本研究采用图神经网络模型Graph Deviation Network (GDN)来捕捉河流传感器数据的复杂时空关系，并提出了备用异常阈值标准GDN+，以实现对水质的准确持续监测。

    

    水是河流网络的生命线，其质量对维护水生态系统和人类社会都有着至关重要的作用。现场传感器技术越来越依赖实时监测水质。异常检测是识别传感器数据中错误模式的关键步骤，但由于数据复杂性和变异性，即使在正常情况下也是一项具有挑战性的任务。本文提出了一种针对河流网络传感器数据异常检测的解决方案，这对于精确持续监测水质非常重要。我们使用图神经网络模型——最近提出的Graph Deviation Network (GDN)，它利用基于图注意力的预测来捕捉传感器之间复杂的时空关系。我们根据所学图形提出了模型GDN+的备用异常阈值标准。为了评估模型的有效性，我们引入了新的基准仿真实验。

    Water is the lifeblood of river networks, and its quality plays a crucial role in sustaining both aquatic ecosystems and human societies. Real-time monitoring of water quality is increasingly reliant on in-situ sensor technology. Anomaly detection is crucial for identifying erroneous patterns in sensor data, but can be a challenging task due to the complexity and variability of the data, even under normal conditions. This paper presents a solution to the challenging task of anomaly detection for river network sensor data, which is essential for the accurate and continuous monitoring of water quality. We use a graph neural network model, the recently proposed Graph Deviation Network (GDN), which employs graph attention-based forecasting to capture the complex spatio-temporal relationships between sensors. We propose an alternate anomaly threshold criteria for the model, GDN+, based on the learned graph. To evaluate the model's efficacy, we introduce new benchmarking simulation experimen
    
[^52]: 未知动态下的长期公平性

    Long-Term Fairness with Unknown Dynamics. (arXiv:2304.09362v1 [cs.LG])

    [http://arxiv.org/abs/2304.09362](http://arxiv.org/abs/2304.09362)

    本文提出一种新方法，通过在未知动态下追求长期公平性，实现算法的动态适应和权衡，可为分类器-人群系统推向更理想的平衡。

    

    尽管机器学习可能会强化社会不平等，但它也可以用于动态地寻求公平的结果。在本文中，我们在在线强化学习的上下文中规范了长期公正性。该公式可以容纳动态控制目标，例如推动人群状态中固有的平等，这些目标不能被纳入到公平性的静态公式中。我们证明这种方法允许算法通过牺牲短期激励，将分类器-人群系统推向更理想的平衡，以适应未知的动态系统。针对这种情况，我们开发了一种算法，该算法适应了最近的在线学习工作。我们证明，这种算法在累积损失和公平性违规的累积性（作为人群之间的统计规律）上实现了同时的概率界限。我们将我们的算法与基准的细微分类器的重复训练以及一个解题器进行比较，在两个协调目标之间实现了更好的权衡。

    While machine learning can myopically reinforce social inequalities, it may also be used to dynamically seek equitable outcomes. In this paper, we formalize long-term fairness in the context of online reinforcement learning. This formulation can accommodate dynamical control objectives, such as driving equity inherent in the state of a population, that cannot be incorporated into static formulations of fairness. We demonstrate that this framing allows an algorithm to adapt to unknown dynamics by sacrificing short-term incentives to drive a classifier-population system towards more desirable equilibria. For the proposed setting, we develop an algorithm that adapts recent work in online learning. We prove that this algorithm achieves simultaneous probabilistic bounds on cumulative loss and cumulative violations of fairness (as statistical regularities between demographic groups). We compare our proposed algorithm to the repeated retraining of myopic classifiers, as a baseline, and to a d
    
[^53]: 探究深度神经网络中三维泛化的本质。

    Investigating the Nature of 3D Generalization in Deep Neural Networks. (arXiv:2304.09358v1 [cs.CV])

    [http://arxiv.org/abs/2304.09358](http://arxiv.org/abs/2304.09358)

    本论文研究了深度学习架构对新视图推广的能力，发现深度模型具有很好的推广能力，但它们的方式与所有现有模型不同。

    

    视觉对象识别系统需要从一组二维训练视图推广到新视图。如何使人类视觉系统可以推广到新视图的问题已经在心理学、计算机视觉和神经科学中进行了研究和建模。现代深度学习架构用于对象识别对新视图具有很好的推广能力，但机制尚未得到很好的理解。在本文中，我们表征了常见深度学习架构对新视图推广的能力。我们将其制定为一个监督分类任务，其中标签对应于唯一的三维物体，示例对应于物体在不同三维方向上的二维视图。我们考虑了三种常见的推广到新视图的模型：(i)完全的三维泛化，(ii)纯二维匹配，(iii)基于视图的线性组合匹配。我们发现，深度模型具有很好的推广能力，但它们的方式与所有这些现有模型不同。外推到vi

    Visual object recognition systems need to generalize from a set of 2D training views to novel views. The question of how the human visual system can generalize to novel views has been studied and modeled in psychology, computer vision, and neuroscience. Modern deep learning architectures for object recognition generalize well to novel views, but the mechanisms are not well understood. In this paper, we characterize the ability of common deep learning architectures to generalize to novel views. We formulate this as a supervised classification task where labels correspond to unique 3D objects and examples correspond to 2D views of the objects at different 3D orientations. We consider three common models of generalization to novel views: (i) full 3D generalization, (ii) pure 2D matching, and (iii) matching based on a linear combination of views. We find that deep models generalize well to novel views, but they do so in a way that differs from all these existing models. Extrapolation to vi
    
[^54]: 压缩与否——自监督学习与信息论:一篇综述

    To Compress or Not to Compress -- Self-Supervised Learning and Information Theory: A Review. (arXiv:2304.09355v1 [cs.LG])

    [http://arxiv.org/abs/2304.09355](http://arxiv.org/abs/2304.09355)

    本文从信息论的角度回顾了各种自监督学习方法，并提出了一个正式的“自监督信息理论学习问题”统一框架。此外，讨论了压缩性和压缩算法在自监督学习中的作用，并凸显了潜在的未来方向。

    

    深度神经网络在监督学习任务中表现出了卓越的性能，但需要大量的标注数据。自监督学习提供了一个替代范例，使得模型可以在没有明确标签的情况下学习。信息论在理解和优化深度神经网络方面起着关键作用。特别地，信息瓶颈原则被应用于在监督设置中优化压缩和相关信息保存之间的权衡。然而，自监督学习中的最佳信息目标仍然不清楚。在本文中，我们从信息论的角度回顾了各种自监督学习方法，并提出了一个正式的“自监督信息理论学习问题”统一框架。我们将现有研究融合成一个一致的框架，研究了最近的自监督方法，并确定了研究机会和挑战。此外，我们还讨论了压缩性和压缩算法在自监督学习中的作用，并凸显了潜在的未来方向。

    Deep neural networks have demonstrated remarkable performance in supervised learning tasks but require large amounts of labeled data. Self-supervised learning offers an alternative paradigm, enabling the model to learn from data without explicit labels. Information theory has been instrumental in understanding and optimizing deep neural networks. Specifically, the information bottleneck principle has been applied to optimize the trade-off between compression and relevant information preservation in supervised settings. However, the optimal information objective in self-supervised learning remains unclear. In this paper, we review various approaches to self-supervised learning from an information-theoretic standpoint and present a unified framework that formalizes the \textit{self-supervised information-theoretic learning problem}. We integrate existing research into a coherent framework, examine recent self-supervised methods, and identify research opportunities and challenges. Moreove
    
[^55]: 学习在无线联邦学习中具有可证明保障的发射

    Learning to Transmit with Provable Guarantees in Wireless Federated Learning. (arXiv:2304.09329v1 [cs.LG])

    [http://arxiv.org/abs/2304.09329](http://arxiv.org/abs/2304.09329)

    该研究提出了一种数据驱动方法，用于分配联邦学习中的发射功率，以优化通信约束下FL过程中服务器端接收到的信息，并提高全局FL模型的准确性和效率。

    

    我们提出了一种新的数据驱动方法，用于分配联邦学习中的发射功率，适用于干扰受限无线网络中的挑战性情景，如FL训练过程中无线信道正在变化以及本地设备上的训练数据不是独立且同分布的情况下。直观来说，功率策略旨在优化通信约束下FL过程中服务器端接收到的信息。我们的目标是提高全局FL模型的准确性和效率。该策略使用图卷积网络进行参数化，并通过原始-对偶算法求解相关的约束优化问题。从理论上讲，我们表明了制定问题具有零对偶间隙，并且一旦参数化功率策略，则最优性取决于此参数化的表达能力。数值上，我们使用实际的无线数据集评估了方法的性能，并证明了其优于传统的功率分配方法。

    We propose a novel data-driven approach to allocate transmit power for federated learning (FL) over interference-limited wireless networks. The proposed method is useful in challenging scenarios where the wireless channel is changing during the FL training process and when the training data are not independent and identically distributed (non-i.i.d.) on the local devices. Intuitively, the power policy is designed to optimize the information received at the server end during the FL process under communication constraints. Ultimately, our goal is to improve the accuracy and efficiency of the global FL model being trained. The proposed power allocation policy is parameterized using a graph convolutional network and the associated constrained optimization problem is solved through a primal-dual (PD) algorithm. Theoretically, we show that the formulated problem has zero duality gap and, once the power policy is parameterized, optimality depends on how expressive this parameterization is. Nu
    
[^56]: 联邦交替训练：在联邦分割中利用未注释数据库(arXiv:2304.09327v1 [cs.CV])

    Federated Alternate Training (FAT): Leveraging Unannotated Data Silos in Federated Segmentation for Medical Imaging. (arXiv:2304.09327v1 [cs.CV])

    [http://arxiv.org/abs/2304.09327](http://arxiv.org/abs/2304.09327)

    本文提出了一种联邦交替训练(FAT)框架，它可以在带有和不带有地面真值标签的数据库之间交替训练，利用未标注的数据来辅助模型学习，适用于医学图像领域。

    

    联邦学习(Fl)旨在分布式地训练机器学习(ML)模型，以加强数据隐私，同时减少数据迁移成本。它是一种适用于隐私敏感的医学图像数据集的分布式学习框架。然而，目前大多数基于FL的医学图像研究都假定数据库具有地面真值标签来进行训练。在实践中，医学领域的标签获取是具有挑战性的，因为它经常需要大量的精力和时间成本。为解决这一挑战并利用未注释数据库来改进建模，我们提出了一种基于替代训练的框架——联邦交替训练(FAT)，它在带注释数据库和未带注释数据库之间交替训练。带注释的数据库利用注释来学习一个合理的全局分割模型。与此同时，未注释的数据库使用全局分割模型作为目标模型，为自我监督学习生成伪标签。我们评估了FAT在数据保护和性能方面的有效性。

    Federated Learning (FL) aims to train a machine learning (ML) model in a distributed fashion to strengthen data privacy with limited data migration costs. It is a distributed learning framework naturally suitable for privacy-sensitive medical imaging datasets. However, most current FL-based medical imaging works assume silos have ground truth labels for training. In practice, label acquisition in the medical field is challenging as it often requires extensive labor and time costs. To address this challenge and leverage the unannotated data silos to improve modeling, we propose an alternate training-based framework, Federated Alternate Training (FAT), that alters training between annotated data silos and unannotated data silos. Annotated data silos exploit annotations to learn a reasonable global segmentation model. Meanwhile, unannotated data silos use the global segmentation model as a target model to generate pseudo labels for self-supervised learning. We evaluate the performance of 
    
[^57]: 基于 Raman 成像和病史的多模态多尺度心血管疾病亚型分类

    Multi-Modality Multi-Scale Cardiovascular Disease Subtypes Classification Using Raman Image and Medical History. (arXiv:2304.09322v1 [eess.IV])

    [http://arxiv.org/abs/2304.09322](http://arxiv.org/abs/2304.09322)

    本研究提出了一种名为 M3S 的多模态多尺度模型，通过 Gramian Angular Summation Field 方法将 Raman 光谱数据转换为各种分辨率的图像丰富了其表示，并引入病史信息作为辅助输入，最终在四种 CVD 亚型的大型数据集上取得了95.71%的准确率。

    

    基于 Raman 光谱学的诊断可靠性和成分特异性测试能力，其被广泛应用于疾病诊断，例如心血管疾病(CVD)。最近引入了一系列流行的深度学习方法，以学习 RS 的细微特征进行二元分类，并取得了优异的表现。然而，这些现有的深度学习方法在分类 CVD 子类型方面仍然面临一些挑战。为了解决这些问题，我们提出了一种名为 M3S 的多模态多尺度模型，该模型具有两个核心模块，通过 Gramian Angular Summation Field 方法将 RS 数据转换为各种分辨率的图像来丰富 RS 序列的表示，并将病史信息引入我们的模型作为辅助输入，并增强了我们模型的区分能力。我们对一个拥有四个 CVD 亚型的大型数据集进行了评估，并取得了95.71% 的准确率，优于现有的最先进方法。

    Raman spectroscopy (RS) has been widely used for disease diagnosis, e.g., cardiovascular disease (CVD), owing to its efficiency and component-specific testing capabilities. A series of popular deep learning methods have recently been introduced to learn nuance features from RS for binary classifications and achieved outstanding performance than conventional machine learning methods. However, these existing deep learning methods still confront some challenges in classifying subtypes of CVD. For example, the nuance between subtypes is quite hard to capture and represent by intelligent models due to the chillingly similar shape of RS sequences. Moreover, medical history information is an essential resource for distinguishing subtypes, but they are underutilized. In light of this, we propose a multi-modality multi-scale model called M3S, which is a novel deep learning method with two core modules to address these issues. First, we convert RS data to various resolution images by the Gramian
    
[^58]: 深入动态云光照

    Deep Dynamic Cloud Lighting. (arXiv:2304.09317v1 [cs.GR])

    [http://arxiv.org/abs/2304.09317](http://arxiv.org/abs/2304.09317)

    该论文提出一种深度云光照模型，能够模拟整个天空的云运动，提高云天空照明方法的动态性。

    

    天空照明是渲染中的核心光源，已经有很多工作开展了模拟晴空照明。然而，在现实中，云朵会显著改变天空的外观，进而改变场景的照明。尽管最近在开发包括云的天空模型方面取得了进展，但这些都忽略了云运动，而云运动是云天空外观的关键组成部分。在任何视频或交互环境中，可以预期云会移动，在短时间内有时会有相当大的移动。我们的工作提出了一种解决方案，首次实现了整个天空的动态云合成。我们通过提出一个多时间尺度天空外观模型，学习预测不同时间尺度上的天空照明，从而能够增加先前静态、云天空照明方法的动态性。

    Sky illumination is a core source of lighting in rendering, and a substantial amount of work has been developed to simulate lighting from clear skies. However, in reality, clouds substantially alter the appearance of the sky and subsequently change the scene's illumination. While there have been recent advances in developing sky models which include clouds, these all neglect cloud movement which is a crucial component of cloudy sky appearance. In any sort of video or interactive environment, it can be expected that clouds will move, sometimes quite substantially in a short period of time. Our work proposes a solution to this which enables whole-sky dynamic cloud synthesis for the first time. We achieve this by proposing a multi-timescale sky appearance model which learns to predict the sky illumination over various timescales, and can be used to add dynamism to previous static, cloudy sky lighting approaches.
    
[^59]: 自适应 $\tau$-Lasso：其健壮性和最优性质。

    The Adaptive $\tau$-Lasso: Its Robustness and Oracle Properties. (arXiv:2304.09310v1 [stat.ML])

    [http://arxiv.org/abs/2304.09310](http://arxiv.org/abs/2304.09310)

    本文提出了一种新型鲁棒的自适应 $\tau$-Lasso 估计器，同时采用自适应 $\ell_1$-范数惩罚项以降低真实回归系数的偏差。它具有变量选择一致性和真实支持下回归向量的渐近正态性的最优性质，假定已知真实回归向量的支持。

    

    本文介绍了一种用于分析高维数据集的新型正则化鲁棒 $\tau$-回归估计器，以应对响应变量和协变量的严重污染。我们称这种估计器为自适应 $\tau$-Lasso，它对异常值和高杠杆点具有鲁棒性，同时采用自适应 $\ell_1$-范数惩罚项来减少真实回归系数的偏差。具体而言，该自适应 $\ell_1$-范数惩罚项为每个回归系数分配一个权重。对于固定数量的预测变量 $p$，我们显示出自适应 $\tau$-Lasso 具有变量选择一致性和真实支持下回归向量的渐近正态性的最优性质，假定已知真实回归向量的支持。然后我们通过有限样本断点和影响函数来表征其健壮性。我们进行了广泛的模拟来比较不同的估计器的性能。

    This paper introduces a new regularized version of the robust $\tau$-regression estimator for analyzing high-dimensional data sets subject to gross contamination in the response variables and covariates. We call the resulting estimator adaptive $\tau$-Lasso that is robust to outliers and high-leverage points and simultaneously employs adaptive $\ell_1$-norm penalty term to reduce the bias associated with large true regression coefficients. More specifically, this adaptive $\ell_1$-norm penalty term assigns a weight to each regression coefficient. For a fixed number of predictors $p$, we show that the adaptive $\tau$-Lasso has the oracle property with respect to variable-selection consistency and asymptotic normality for the regression vector corresponding to the true support, assuming knowledge of the true regression vector support. We then characterize its robustness via the finite-sample breakdown point and the influence function. We carry-out extensive simulations to compare the per
    
[^60]: 用机器学习搜索带状物

    Searching for ribbons with machine learning. (arXiv:2304.09304v1 [math.GT])

    [http://arxiv.org/abs/2304.09304](http://arxiv.org/abs/2304.09304)

    用机器学习发现带状物，反驳四维平凡 Poincaré 猜想。

    

    我们将贝叶斯优化和强化学习应用于拓扑学中的一个问题：如何确定一个结不能限定一个带状物。该问题在反驳四维平凡 Poincaré 猜想的方法中是相关的；利用我们的程序，我们排除了许多猜想的反例。我们还展示了这些程序成功检测了范围在70节点内的许多带状物。

    We apply Bayesian optimization and reinforcement learning to a problem in topology: the question of when a knot bounds a ribbon disk. This question is relevant in an approach to disproving the four-dimensional smooth Poincar\'e conjecture; using our programs, we rule out many potential counterexamples to the conjecture. We also show that the programs are successful in detecting many ribbon knots in the range of up to 70 crossings.
    
[^61]: 基于静态和动态可学习个性化图卷积网络的时空海表温度预测研究

    Towards Spatio-temporal Sea Surface Temperature Forecasting via Static and Dynamic Learnable Personalized Graph Convolution Network. (arXiv:2304.09290v1 [cs.LG])

    [http://arxiv.org/abs/2304.09290](http://arxiv.org/abs/2304.09290)

    该论文提出了基于静态和动态可学习个性化图卷积网络的时空海表温度预测方法，其中利用两个图学习层分别模型化了SST数据的固定网络和动态网络，并设计了个性化的图卷积网络层以精确预测时空变化。实验结果显示，该方法在预测准确度方面优于目前最先进的方法。

    

    海表温度对于地球大气非常重要，因为其动力学对于塑造本地和全球气候有很大的作用，并且深刻地影响着我们的生态系统。准确预测海表温度能够带来重大的经济和社会影响，例如提前数月更好地准备极端天气，如严重干旱或热带气旋。然而，由于海洋系统的内在复杂性和不确定性，这项任务面临着独特的挑战。为了解决这个问题，本文提出了一个新颖的基于静态和动态可学习个性化图卷积网络 (SD-LPGC)。实验结果表明，SD-LPGC方法在预测精度方面优于目前最先进的方法。

    Sea surface temperature (SST) is uniquely important to the Earth's atmosphere since its dynamics are a major force in shaping local and global climate and profoundly affect our ecosystems. Accurate forecasting of SST brings significant economic and social implications, for example, better preparation for extreme weather such as severe droughts or tropical cyclones months ahead. However, such a task faces unique challenges due to the intrinsic complexity and uncertainty of ocean systems. Recently, deep learning techniques, such as graphical neural networks (GNN), have been applied to address this task. Even though these methods have some success, they frequently have serious drawbacks when it comes to investigating dynamic spatiotemporal dependencies between signals. To solve this problem, this paper proposes a novel static and dynamic learnable personalized graph convolution network (SD-LPGC). Specifically, two graph learning layers are first constructed to respectively model the stabl
    
[^62]: Pelphix：经皮盆骨固定术中基于X光图像的手术阶段识别

    Pelphix: Surgical Phase Recognition from X-ray Images in Percutaneous Pelvic Fixation. (arXiv:2304.09285v1 [cs.LG])

    [http://arxiv.org/abs/2304.09285](http://arxiv.org/abs/2304.09285)

    本文提出了一种名为Pelphix的X光引导下的经皮盆骨折修复手术阶段识别方法，使用马尔科夫过程模拟过程并提供完全注释的训练数据，在四个粒度级别上回归手术阶段，并取得了很好的准确率。

    

    手术阶段识别(SPR)是现代手术室数字化转型中至关重要的元素。虽然基于视频源的SPR已经很成熟，但插管X光序列的整合尚未被探索。本文提出了Pelphix，这是一种针对X光引导下的经皮盆骨折修复的SPR方法，该方法将该过程建模为四个粒度水平——走廊、活动、视图和帧值——将盆骨折修复工作流程模拟为马尔科夫过程，从而提供完全注释的训练数据。 使用从骨走廊、工具和解剖学检测中添加的监督学习，我们学习图像表示，并将其馈送到Transformer模型中，以在四个粒度级别上回归手术阶段。我们的方法展示了基于X光的SPR的可行性，在模拟序列中实现了93.8％的平均准确率，在尸体中实现了67.57％的所有粒度级别，并针对目标走廊的准确率高达88％。

    Surgical phase recognition (SPR) is a crucial element in the digital transformation of the modern operating theater. While SPR based on video sources is well-established, incorporation of interventional X-ray sequences has not yet been explored. This paper presents Pelphix, a first approach to SPR for X-ray-guided percutaneous pelvic fracture fixation, which models the procedure at four levels of granularity -- corridor, activity, view, and frame value -- simulating the pelvic fracture fixation workflow as a Markov process to provide fully annotated training data. Using added supervision from detection of bony corridors, tools, and anatomy, we learn image representations that are fed into a transformer model to regress surgical phases at the four granularity levels. Our approach demonstrates the feasibility of X-ray-based SPR, achieving an average accuracy of 93.8% on simulated sequences and 67.57% in cadaver across all granularity levels, with up to 88% accuracy for the target corrido
    
[^63]: 一种数据驱动的序列学习框架，用于加速和优化多目标制造决策

    A Data Driven Sequential Learning Framework to Accelerate and Optimize Multi-Objective Manufacturing Decisions. (arXiv:2304.09278v1 [cs.LG])

    [http://arxiv.org/abs/2304.09278](http://arxiv.org/abs/2304.09278)

    本文提出了一种利用序列学习来高效优化多个相互冲突目标的复杂系统的数据驱动贝叶斯优化框架。

    

    制造具有特定性质或性质组合的先进材料和产品通常是必要的。为了实现这一点，找到能够生成这些性质理想组合的最佳配方或处理条件至关重要。大多数时候，需要进行足够数量的实验才能生成Pareto前沿。然而，制造实验通常很昂贵，甚至进行一次实验也可能是一个耗时的过程。因此，确定最佳数据收集位置以获得对过程的最全面理解非常关键。序列学习是一种有前途的方法，可以从进行中的实验中主动学习，迭代更新基础优化例程，并随时调整数据收集过程。本文提出了一种新颖的基于数据驱动的贝叶斯优化框架，利用序列学习来高效优化具有多个相互冲突目标的复杂系统。

    Manufacturing advanced materials and products with a specific property or combination of properties is often warranted. To achieve that it is crucial to find out the optimum recipe or processing conditions that can generate the ideal combination of these properties. Most of the time, a sufficient number of experiments are needed to generate a Pareto front. However, manufacturing experiments are usually costly and even conducting a single experiment can be a time-consuming process. So, it's critical to determine the optimal location for data collection to gain the most comprehensive understanding of the process. Sequential learning is a promising approach to actively learn from the ongoing experiments, iteratively update the underlying optimization routine, and adapt the data collection process on the go. This paper presents a novel data-driven Bayesian optimization framework that utilizes sequential learning to efficiently optimize complex systems with multiple conflicting objectives. 
    
[^64]: 一种神经λ演算法：神经符号人工智能遇见计算和函数式编程的基础。

    A Neural Lambda Calculus: Neurosymbolic AI meets the foundations of computing and functional programming. (arXiv:2304.09276v1 [cs.LG])

    [http://arxiv.org/abs/2304.09276](http://arxiv.org/abs/2304.09276)

    本文提出了一种神经λ演算法，使用λ语言编程，研究神经网络在执行整个程序的能力，旨在拓展神经网络在符号人工智能领域的应用。

    

    在过去几十年中，基于深度神经网络的模型成为了机器学习中的主导范式。最近，人们越来越认为在符号学习中使用人工神经网络是越来越相关的。为了研究神经网络在符号人工智能领域的能力，研究人员已经探索了深度神经网络学习数学构造（如加法和乘法）、逻辑推理（如定理证明器）甚至执行计算机程序的能力。然而，后者对于神经网络来说是太复杂的任务，结果并不总是成功的，并且往往需要在学习过程中引入有偏见的元素，以限制可能要执行的程序的范围。在这项工作中，我们将分析神经网络学习如何执行整个程序的能力。为此，我们提出了一种不同的方法。我们不使用命令式编程语言，而是采用λ语言进行编程。

    Over the last decades, deep neural networks based-models became the dominant paradigm in machine learning. Further, the use of artificial neural networks in symbolic learning has been seen as increasingly relevant recently. To study the capabilities of neural networks in the symbolic AI domain, researchers have explored the ability of deep neural networks to learn mathematical constructions, such as addition and multiplication, logic inference, such as theorem provers, and even the execution of computer programs. The latter is known to be too complex a task for neural networks. Therefore, the results were not always successful, and often required the introduction of biased elements in the learning process, in addition to restricting the scope of possible programs to be executed. In this work, we will analyze the ability of neural networks to learn how to execute programs as a whole. To do so, we propose a different approach. Instead of using an imperative programming language, with com
    
[^65]: 粗糙的种族数据掩盖了临床风险评分表现的差异

    Coarse race data conceals disparities in clinical risk score performance. (arXiv:2304.09270v1 [cs.CY])

    [http://arxiv.org/abs/2304.09270](http://arxiv.org/abs/2304.09270)

    研究发现仅依赖粗糙的种族类别可能掩盖了临床风险评分表现中的重要差异，需要更精细的种族数据采集。

    

    美国的医疗保健数据通常只记录病人的粗略种族组：例如，印度和中国病人通常都被编码为“亚洲人”。然而，目前还不清楚这种粗略编码是否掩盖了精细种族组之间的临床风险评分表现的显著差异。本文利用418K紧急科室就诊的数据，评估了三种结局、五种风险评分和四种表现指标的精细种族组之间的临床风险评分表现差异。在各种结局和指标中，我们表明，粗略种族类别内存在重要的表现差异。事实上，在粗略类别内，性能指标的变异常常超过粗略类别之间的变异。我们探讨了这些差异的原因，发现结局率、特征分布以及特征与结果之间的关系在不同精细种族类别之间都有显着差异。我们的结果表明，仅依赖粗糙的种族类别可能掩盖了临床风险评分表现中的重要差异，并强调了医疗保健环境中需要收集更精细的种族数据。

    Healthcare data in the United States often records only a patient's coarse race group: for example, both Indian and Chinese patients are typically coded as ``Asian.'' It is unknown, however, whether this coarse coding conceals meaningful disparities in the performance of clinical risk scores across granular race groups. Here we show that it does. Using data from 418K emergency department visits, we assess clinical risk score performance disparities across granular race groups for three outcomes, five risk scores, and four performance metrics. Across outcomes and metrics, we show that there are significant granular disparities in performance within coarse race categories. In fact, variation in performance metrics within coarse groups often exceeds the variation between coarse groups. We explore why these disparities arise, finding that outcome rates, feature distributions, and the relationships between features and outcomes all vary significantly across granular race categories. Our res
    
[^66]: 基于张量处理单元的内存模拟模拟计算异构集成

    Heterogeneous Integration of In-Memory Analog Computing Architectures with Tensor Processing Units. (arXiv:2304.09258v1 [cs.AR])

    [http://arxiv.org/abs/2304.09258](http://arxiv.org/abs/2304.09258)

    本文介绍了一种将内存模拟计算（IMAC）单元与张量处理单元（TPUs）集成的新型、异构、混合信号和混合精度架构，以提高移动CNN性能。结合混合精度训练技术，充分利用TPUs在卷积层中的优点和IMAC电路在密集层中的优势，实现了高达...的性能提升。

    

    张量处理单元（TPUs）是专门加速机器学习任务的硬件加速器，在执行卷积神经网络（CNNs）中的卷积层时显示出显着的性能提升。然而，在全连接（FC）层中，它们很难保持相同的效率，导致硬件利用率不佳。另一方面，内存模拟计算（IMAC）架构在执行FC层时表现出了显著的加速效果。本文介绍了一种新颖、异构、混合信号和混合精度架构，将IMAC单元与边缘TPU集成，以提高移动CNN性能。为了充分利用TPUs在卷积层中的优点和IMAC电路在密集层中的优势，我们提出了一种统一的学习算法，结合混合精度训练技术来减轻在TPU-IMAC架构上部署模型时可能出现的精度降低问题。模拟结果表明，TPU-IMAC配置实现了高达...

    Tensor processing units (TPUs), specialized hardware accelerators for machine learning tasks, have shown significant performance improvements when executing convolutional layers in convolutional neural networks (CNNs). However, they struggle to maintain the same efficiency in fully connected (FC) layers, leading to suboptimal hardware utilization. In-memory analog computing (IMAC) architectures, on the other hand, have demonstrated notable speedup in executing FC layers. This paper introduces a novel, heterogeneous, mixed-signal, and mixed-precision architecture that integrates an IMAC unit with an edge TPU to enhance mobile CNN performance. To leverage the strengths of TPUs for convolutional layers and IMAC circuits for dense layers, we propose a unified learning algorithm that incorporates mixed-precision training techniques to mitigate potential accuracy drops when deploying models on the TPU-IMAC architecture. The simulations demonstrate that the TPU-IMAC configuration achieves up 
    
[^67]: IMAC-Sim：一种面向内存模拟模拟计算架构的电路级模拟器。

    IMAC-Sim: A Circuit-level Simulator For In-Memory Analog Computing Architectures. (arXiv:2304.09252v1 [cs.ET])

    [http://arxiv.org/abs/2304.09252](http://arxiv.org/abs/2304.09252)

    IMAC-Sim 是一种针对内存模拟模拟计算架构的电路级模拟器，可以根据用户指定的超参数快速创建电路，并自动评估其准确性、功耗和延迟，同时还考虑了互连寄生电阻和电容问题。

    

    随着越来越多的人把基于忆阻器的内存模拟模拟计算 (IMAC) 架构作为替代高能耗计算机系统的选择，特别是在机器学习领域中，因此，一种可以探索IMAC架构中的设备和电路设计空间的工具可以显著促进该领域的研究和发展。本文提出了IMAC-Sim，这是一种针对IMAC架构设计空间探索的电路级模拟器。IMAC-Sim是基于Python的模拟框架，它根据用户选择的各种设备和电路级超参数创建IMAC电路的SPICE网表，并自动评估所开发电路的准确性、功耗和延迟，使用用户指定的数据集。此外，IMAC-Sim模拟了IMAC架构中的互连寄生电阻和电容，并配备了水平和垂直分区技术以克服这些可靠性挑战。

    With the increased attention to memristive-based in-memory analog computing (IMAC) architectures as an alternative for energy-hungry computer systems for machine learning applications, a tool that enables exploring their device- and circuit-level design space can significantly boost the research and development in this area. Thus, in this paper, we develop IMAC-Sim, a circuit-level simulator for the design space exploration of IMAC architectures. IMAC-Sim is a Python-based simulation framework, which creates the SPICE netlist of the IMAC circuit based on various device- and circuit-level hyperparameters selected by the user, and automatically evaluates the accuracy, power consumption, and latency of the developed circuit using a user-specified dataset. Moreover, IMAC-Sim simulates the interconnect parasitic resistance and capacitance in the IMAC architectures and is also equipped with horizontal and vertical partitioning techniques to surmount these reliability challenges. IMAC-Sim is 
    
[^68]: 利用运动症状和机器学习进行帕金森病的早期检测

    Early Detection of Parkinson's Disease using Motor Symptoms and Machine Learning. (arXiv:2304.09245v1 [cs.LG])

    [http://arxiv.org/abs/2304.09245](http://arxiv.org/abs/2304.09245)

    本研究利用机器学习算法分析运动症状和步态相关参数，以筛选出有效的生物标志物，进而实现帕金森病的早期检测。该方法基于经济和稳健的可穿戴设备，模型准确率高达91.9％。

    

    帕金森病(PD)已经发现影响到每1000人中的1个人，更倾向于60岁以上的人群。利用可穿戴设备寻找准确的诊断生物标志物已成为时代的需求，特别是对于像帕金森这样的神经退行性疾病。本文旨在关注早期发生的常见症状，如运动和步态相关参数，以便对经济和稳健的可穿戴设备的可行性进行定量分析。经过各种机器学习算法的彻底分析，利用帕金森疾病进展标志倡议 (PPMI) 的一个子集PPMI步态数据集进行特征选择。确认的重要特征已被用于测试实时数据以早期检测帕金森综合症，模型准确率达91.9％。

    Parkinson's disease (PD) has been found to affect 1 out of every 1000 people, being more inclined towards the population above 60 years. Leveraging wearable-systems to find accurate biomarkers for diagnosis has become the need of the hour, especially for a neurodegenerative condition like Parkinson's. This work aims at focusing on early-occurring, common symptoms, such as motor and gait related parameters to arrive at a quantitative analysis on the feasibility of an economical and a robust wearable device. A subset of the Parkinson's Progression Markers Initiative (PPMI), PPMI Gait dataset has been utilised for feature-selection after a thorough analysis with various Machine Learning algorithms. Identified influential features has then been used to test real-time data for early detection of Parkinson Syndrome, with a model accuracy of 91.9%
    
[^69]: 使用Price定理和分段线性分解分析在线交叉相关器的框架。

    A Framework for Analyzing Online Cross-correlators using Price's Theorem and Piecewise-Linear Decomposition. (arXiv:2304.09242v1 [cs.LG])

    [http://arxiv.org/abs/2304.09242](http://arxiv.org/abs/2304.09242)

    本文提出了一种使用非线性函数构建交叉相关器的方法，并使用Price定理和分段线性分解提出了一个数学框架来分析这种交叉相关器。

    

    精确估计两个随机变量之间的交叉相关或相似度是信号检测、高维计算、联想记忆和神经网络的核心问题。本文提出了一种能够构建具有更高信噪比（SNR）的交叉相关器的大量简单非线性函数的方法，并使用Price定理和分段线性分解提出了一个数学框架，以分析使用混合分段线性函数构建的交叉相关器。

    Precise estimation of cross-correlation or similarity between two random variables lies at the heart of signal detection, hyperdimensional computing, associative memories, and neural networks. Although a vast literature exists on different methods for estimating cross-correlations, the question what is the best and simplest method to estimate cross-correlations using finite samples ? is still not clear. In this paper, we first argue that the standard empirical approach might not be the optimal method even though the estimator exhibits uniform convergence to the true cross-correlation. Instead, we show that there exists a large class of simple non-linear functions that can be used to construct cross-correlators with a higher signal-to-noise ratio (SNR). To demonstrate this, we first present a general mathematical framework using Price's Theorem that allows us to analyze cross-correlators constructed using a mixture of piece-wise linear functions. Using this framework and high-dimensiona
    
[^70]: 图像分类的量子机器学习方法

    Quantum machine learning for image classification. (arXiv:2304.09224v1 [quant-ph])

    [http://arxiv.org/abs/2304.09224](http://arxiv.org/abs/2304.09224)

    本论文提出了两种混合量子-经典的神经网络模型用于图像分类，其中包括一个具有并行量子层的神经网络和一个具有量子卷积层的神经网络。其中一个混合量子方法在MNIST数据集上展现了超过99%的惊人准确率。

    

    图像识别和分类是各行各业中多种实际应用的基本任务，是现代世界中至关重要的领域。近年来，尤其是神经网络，机器学习模型已成为解决这些问题的强大工具。然而，通过混合量子-经典方法利用量子效应可以进一步增强传统经典模型的能力。在这里，我们提出两种混合量子-经典模型：一个具有并行量子层的神经网络和一个具有量子卷积层的神经网络，来解决图像分类问题。我们其中一个混合量子方法在MNIST数据集上展现了超过99%的惊人准确率。值得注意的是，在我们提出的量子电路中，所有可变参数都是可训练的，并且我们将量子部分分成多个并行可变量子电路以提高神经网络学习的效率。

    Image recognition and classification are fundamental tasks with diverse practical applications across various industries, making them critical in the modern world. Recently, machine learning models, particularly neural networks, have emerged as powerful tools for solving these problems. However, the utilization of quantum effects through hybrid quantum-classical approaches can further enhance the capabilities of traditional classical models. Here, we propose two hybrid quantum-classical models: a neural network with parallel quantum layers and a neural network with a quanvolutional layer, which address image classification problems. One of our hybrid quantum approaches demonstrates remarkable accuracy of more than 99% on the MNIST dataset. Notably, in the proposed quantum circuits all variational parameters are trainable, and we divide the quantum part into multiple parallel variational quantum circuits for efficient neural network learning. In summary, our study contributes to the ong
    
[^71]: 基于局部Lajasiewicz条件的随机梯度下降在深度神经网络中的收敛性研究

    Convergence of stochastic gradient descent under a local Lajasiewicz condition for deep neural networks. (arXiv:2304.09221v1 [cs.LG])

    [http://arxiv.org/abs/2304.09221](http://arxiv.org/abs/2304.09221)

    本文通过随机梯度下降算法研究了解析度函数为非凸的深度神经网络的全局收敛性，证明了当机器学习噪声的尺度与目标函数相等时，在局部区域内初始化后，以正的概率能够收敛到该区域内的全局最小值。

    

    本文考虑了解析度函数为非凸的情况下，通过随机梯度下降算法对深度神经网络的全局收敛性进行了研究。在有限宽的神经网络中，通过加入最小的额外假设并保证机器学习噪声的尺度与目标函数相等，证明了在局部区域内初始化时，以正的概率随机梯度下降迭代收敛到该区域内的全局最小值。本文的关键是确保随机梯度下降的整个轨迹以正的概率保留在局部区域内。文章提供了负面分析，表明使用Robbins-Monro类型的步长之间具有有界噪声的假设不足以保持该关键部分的有效性。

    We extend the global convergence result of Chatterjee \cite{chatterjee2022convergence} by considering the stochastic gradient descent (SGD) for non-convex objective functions. With minimal additional assumptions that can be realized by finitely wide neural networks, we prove that if we initialize inside a local region where the \L{}ajasiewicz condition holds, with a positive probability, the stochastic gradient iterates converge to a global minimum inside this region. A key component of our proof is to ensure that the whole trajectories of SGD stay inside the local region with a positive probability. For that, we assume the SGD noise scales with the objective function, which is called machine learning noise and achievable in many real examples. Furthermore, we provide a negative argument to show why using the boundedness of noise with Robbins-Monro type step sizes is not enough to keep the key component valid.
    
[^72]: 考虑时空依赖关系的交通数据填补的深度学习框架

    A Deep Learning Framework for Traffic Data Imputation Considering Spatiotemporal Dependencies. (arXiv:2304.09182v1 [cs.LG])

    [http://arxiv.org/abs/2304.09182](http://arxiv.org/abs/2304.09182)

    该论文提出了一种考虑时空依赖关系的交通数据填补深度学习框架，可用于解决缺失或不完整数据问题，以进一步应用该数据。

    

    传感器收集的时空（ST）数据可以表示为多变量时间序列，这是按时间顺序列出的数据点序列。尽管存在大量有用信息，但ST数据通常存在缺失或不完整数据的问题，这也限制了它的应用。数据填补是一个可行的解决方案，经常用于预处理数据以进行进一步的应用。然而，在实践中，由于交通网络中时空依赖关系变化的复杂性，时空数据填补非常困难，是进一步应用的关键前提任务。现有的方法大多只捕捉时间序列中的时间依赖性或静态空间依赖性。他们无法直接建模时空依赖关系，并且模型的表示能力相对有限。

    Spatiotemporal (ST) data collected by sensors can be represented as multi-variate time series, which is a sequence of data points listed in an order of time. Despite the vast amount of useful information, the ST data usually suffer from the issue of missing or incomplete data, which also limits its applications. Imputation is one viable solution and is often used to prepossess the data for further applications. However, in practice, n practice, spatiotemporal data imputation is quite difficult due to the complexity of spatiotemporal dependencies with dynamic changes in the traffic network and is a crucial prepossessing task for further applications. Existing approaches mostly only capture the temporal dependencies in time series or static spatial dependencies. They fail to directly model the spatiotemporal dependencies, and the representation ability of the models is relatively limited.
    
[^73]: 机器学习在阿尔茨海默病诊断中的应用：一篇综述

    Alzheimers Disease Diagnosis using Machine Learning: A Review. (arXiv:2304.09178v1 [cs.LG])

    [http://arxiv.org/abs/2304.09178](http://arxiv.org/abs/2304.09178)

    本综述分析了机器学习在阿尔茨海默病诊断中的应用，深度学习和强化学习是研究热点，可用于早期病变检测和诊断。

    

    阿尔茨海默病是一种严重的神经退行性疾病，会逐渐导致记忆力丧失。这种致命性脑病主要影响老年人，导致认知和生物学功能衰退并逐渐引起脑萎缩。为了准确诊断阿尔茨海默病，需要采用先进的机器学习方法。近年来，机器学习在医疗行业中受到了广泛关注和应用。通过机器学习方法可以判断一个人是否有早期阿尔茨海默病的可能性。本文综述了2008年至2023年间通过谷歌学术发现的基于深度学习技术和强化学习的阿尔茨海默病诊断论文。

    Alzheimers Disease AD is an acute neuro disease that degenerates the brain cells and thus leads to memory loss progressively. It is a fatal brain disease that mostly affects the elderly. It steers the decline of cognitive and biological functions of the brain and shrinks the brain successively, which in turn is known as Atrophy. For an accurate diagnosis of Alzheimers disease, cutting edge methods like machine learning are essential. Recently, machine learning has gained a lot of attention and popularity in the medical industry. As the illness progresses, those with Alzheimers have a far more difficult time doing even the most basic tasks, and in the worst case, their brain completely stops functioning. A persons likelihood of having early-stage Alzheimers disease may be determined using the ML method. In this analysis, papers on Alzheimers disease diagnosis based on deep learning techniques and reinforcement learning between 2008 and 2023 found in google scholar were studied. Sixty re
    
[^74]: 利用不可微分的群组 AUC 优化提升个性化排序

    Enhancing Personalized Ranking With Differentiable Group AUC Optimization. (arXiv:2304.09176v1 [cs.LG])

    [http://arxiv.org/abs/2304.09176](http://arxiv.org/abs/2304.09176)

    本文提出了一种个性化和可微分的AUC优化方法（PDAOM），可用于训练二元分类器并向其提供在独立用户组中紧密相关的正负样本对，以促进分类器关注不易区分的样本之间的关系，这些方法不仅提高了AUC和GAUC指标，还减少了训练目标的计算复杂度。

    

    AUC是评估分类器性能的常见指标。然而，大多数分类器是使用交叉熵训练的，它并不直接优化AUC指标，这在训练和评估阶段之间存在差距。本文提出了PDAOM损失，一种具有最大违规规定的个性化和可微分AUC优化方法，可直接应用于训练二元分类器并用梯度优化。具体地，我们构造了成对指数损失函数，将用户ID分组的子批次中的难分辨正负样本对拆分出来，旨在指导分类器从独立用户的角度关注相反样本之间的难以区分的关系。与成对指数损失函数的原始形式相比，所提出的PDAOM损失函数不仅在离线评估中提高了AUC和GAUC指标，而且减少了训练目标的计算复杂度。

    AUC is a common metric for evaluating the performance of a classifier. However, most classifiers are trained with cross entropy, and it does not optimize the AUC metric directly, which leaves a gap between the training and evaluation stage. In this paper, we propose the PDAOM loss, a Personalized and Differentiable AUC Optimization method with Maximum violation, which can be directly applied when training a binary classifier and optimized with gradient-based methods. Specifically, we construct the pairwise exponential loss with difficult pair of positive and negative samples within sub-batches grouped by user ID, aiming to guide the classifier to pay attention to the relation between hard-distinguished pairs of opposite samples from the perspective of independent users. Compared to the origin form of pairwise exponential loss, the proposed PDAOM loss not only improves the AUC and GAUC metrics in the offline evaluation, but also reduces the computation complexity of the training objecti
    
[^75]: Memento: 实现机器学习实验的轻松、高效和可靠的框架

    Memento: Facilitating Effortless, Efficient, and Reliable ML Experiments. (arXiv:2304.09175v1 [cs.LG])

    [http://arxiv.org/abs/2304.09175](http://arxiv.org/abs/2304.09175)

    Memento是一个Python包，旨在协助研究人员高效地管理和执行计算密集型机器学习实验。它提供了简单明了的配置矩阵和并发运行实验的能力。

    

    由于缺乏统一框架，运行复杂的机器学习实验集非常具挑战性和耗时。这迫使研究人员自己花费时间实现必要的功能，如并行化、缓存和检查点，而不是集中精力于他们的项目。为了简化这个过程，本文介绍了 Memento，一个旨在帮助研究人员和数据科学家高效管理和执行计算密集型实验的 Python 包。Memento 通过提供一个简单明了的配置矩阵和能够同时运行多个线程的实验来优化任何实验流程。Memento 的演示可在以下网站查看：https://wickerlab.org/publication/memento。

    Running complex sets of machine learning experiments is challenging and time-consuming due to the lack of a unified framework. This leaves researchers forced to spend time implementing necessary features such as parallelization, caching, and checkpointing themselves instead of focussing on their project. To simplify the process, in this paper, we introduce Memento, a Python package that is designed to aid researchers and data scientists in the efficient management and execution of computationally intensive experiments. Memento has the capacity to streamline any experimental pipeline by providing a straightforward configuration matrix and the ability to concurrently run experiments across multiple threads. A demonstration of Memento is available at: https://wickerlab.org/publication/memento.
    
[^76]: CF-VAE：基于VAE和因果流的因果分离表示学习

    CF-VAE: Causal Disentangled Representation Learning with VAE and Causal Flows. (arXiv:2304.09010v1 [cs.LG])

    [http://arxiv.org/abs/2304.09010](http://arxiv.org/abs/2304.09010)

    本文提出了一种新的因果流以进行因果分离表示学习，设计了一个新模型CF-VAE，利用因果流增强了VAE编码器的分离能力，并展示了在合成和真实数据集上实现因果分离并进行干预实验的结果。

    

    学习分离表示在表示学习中至关重要，旨在学习数据的低维表示，其中每个维度对应一个潜在的生成因素。由于生成因素之间可能存在因果关系，因果分离表示学习已经受到广泛关注。本文首先提出了一种新的可以将因果结构信息引入模型中的流，称为因果流。基于广泛用于分离表示学习的变分自编码器（VAE），我们设计了一个新模型CF-VAE，利用因果流增强了VAE编码器的分离能力。通过进一步引入基准因素的监督，我们展示了我们模型的分离可识别性。在合成和真实数据集上的实验结果表明，CF-VAE可以实现因果分离并进行干预实验。

    Learning disentangled representations is important in representation learning, aiming to learn a low dimensional representation of data where each dimension corresponds to one underlying generative factor. Due to the possibility of causal relationships between generative factors, causal disentangled representation learning has received widespread attention. In this paper, we first propose a new flows that can incorporate causal structure information into the model, called causal flows. Based on the variational autoencoders(VAE) commonly used in disentangled representation learning, we design a new model, CF-VAE, which enhances the disentanglement ability of the VAE encoder by utilizing the causal flows. By further introducing the supervision of ground-truth factors, we demonstrate the disentanglement identifiability of our model. Experimental results on both synthetic and real datasets show that CF-VAE can achieve causal disentanglement and perform intervention experiments. Moreover, C
    
[^77]: 基于年龄和资源分配的NOMA网络下通信高效联邦学习的客户端选择方案

    Joint Age-based Client Selection and Resource Allocation for Communication-Efficient Federated Learning over NOMA Networks. (arXiv:2304.08996v1 [cs.LG])

    [http://arxiv.org/abs/2304.08996](http://arxiv.org/abs/2304.08996)

    本文针对联邦学习在无线网络上通信受限、收敛速度慢和资源有限等问题，提出了一种基于年龄和资源分配的客户端选择方案，旨在最小化每轮联邦学习的总时间消耗，从而提高联邦学习的性能。

    

    联邦学习（FL）是一种有效的分布式学习范式，它可以使得客户端使用本地数据协同训练全局模型。然而，当FL部署在无线网络上时，由于通信链路差和收敛速度慢，FL的性能常常受到限制。此外，由于无线资源受限，准确选取客户端和控制资源分配对于提高FL性能至关重要。鉴于这些挑战，在本文中提出了客户端选择和资源分配的联合优化问题，旨在最小化在非正交多址（NOMA）无线网络上每轮FL的总时间消耗。具体而言，我们首先提出了一种新的客户端选择方案，通过考虑收到的本地FL模型的新旧程度来设计，在此基础上，通过年龄更新（AoU）指标获得资源分配的闭合式解。

    Federated learning (FL) is a promising paradigm that enables distributed clients to collaboratively train a shared global model while keeping the training data locally. However, the performance of FL is often limited by poor communication links and slow convergence when FL is deployed over wireless networks. Besides, due to the limited radio resources, it is crucial to select clients and control resource allocation accurately for improved FL performance. Motivated by these challenges, a joint optimization problem of client selection and resource allocation is formulated in this paper, aiming to minimize the total time consumption of each round in FL over non-orthogonal multiple access (NOMA) enabled wireless network. Specifically, based on a metric termed the age of update (AoU), we first propose a novel client selection scheme by accounting for the staleness of the received local FL models. After that, the closed-form solutions of resource allocation are obtained by monotonicity analy
    
[^78]: 基于黎曼几何与隐式表示先验的贝叶斯机器人抓取方法

    Implicit representation priors meet Riemannian geometry for Bayesian robotic grasping. (arXiv:2304.08805v1 [cs.RO])

    [http://arxiv.org/abs/2304.08805](http://arxiv.org/abs/2304.08805)

    该研究利用隐式表示构建了场景相关先验，从而在不规则环境中利用基于模拟的高效贝叶斯推理算法成功识别抓取姿态。

    

    在高噪声环境下进行机器人抓取面临着复杂的挑战，尤其是在缺乏场景先验知识的情况下。具体来说，由于两个原因，用贝叶斯推理来识别良好的抓取姿态变得困难：i）从无信息的先验生成数据效率低下，ii）后验通常在黎曼流形上定义一个复杂分布。在本研究中，我们探讨了使用隐式表示构建场景相关性先验的方法，从而使得在不规则环境中应用基于模拟的高效贝叶斯推理算法来确定成功的抓取姿态成为可能。模拟和物理基准测试的结果展示了这种方法的高成功率和良好潜力。

    Robotic grasping in highly noisy environments presents complex challenges, especially with limited prior knowledge about the scene. In particular, identifying good grasping poses with Bayesian inference becomes difficult due to two reasons: i) generating data from uninformative priors proves to be inefficient, and ii) the posterior often entails a complex distribution defined on a Riemannian manifold. In this study, we explore the use of implicit representations to construct scene-dependent priors, thereby enabling the application of efficient simulation-based Bayesian inference algorithms for determining successful grasp poses in unstructured environments. Results from both simulation and physical benchmarks showcase the high success rate and promising potential of this approach.
    
[^79]: EEG SN：面向 EEG 的图形脉冲神经网络的高效低延迟解码

    EEGSN: Towards Efficient Low-latency Decoding of EEG with Graph Spiking Neural Networks. (arXiv:2304.07655v1 [cs.NE])

    [http://arxiv.org/abs/2304.07655](http://arxiv.org/abs/2304.07655)

    该论文提出了一种名为 EEGSN 的图形脉冲神经网络（SNN）架构，面向多通道 EEG 分类任务，在学习分布式 EEG 传感器中的动态关系信息的同时，将推断计算复杂度降低了20倍，为低延迟和功耗效率的脑计算机接口的开发提供了一个可行的框架。

    

    目前大多数脉冲神经网络（SNN）的训练依赖于归纳偏差，这并不一定适用于多个需要低延迟和功耗效率的关键任务。 基于相关的脑电图（EEG）信号推断大脑行为就是一个这样的例子，学习时空依赖关系会严重影响网络的训练和推断效率。目前，SNN仅仅依靠一般归纳偏差来模拟不同数据流之间的动态关系。在这里，我们提出了一种用于多通道 EEG 分类的图形脉冲神经网络架构（EEGSN），它能够学习分布在 EEG 传感器中的动态关系信息。与现有技术相比，我们的方法将推断计算复杂度降低了20倍，同时在运动执行分类任务上达到了可比较的准确性。总体而言，我们的工作为可解释和高效训练 EEG 数据的图形SNN提供了一个框架，从而实现了低延迟和功耗效率的脑-计算机界面的开发。

    A vast majority of spiking neural networks (SNNs) are trained based on inductive biases that are not necessarily a good fit for several critical tasks that require low-latency and power efficiency. Inferring brain behavior based on the associated electroenchephalography (EEG) signals is an example of how networks training and inference efficiency can be heavily impacted by learning spatio-temporal dependencies. Up to now, SNNs rely solely on general inductive biases to model the dynamic relations between different data streams. Here, we propose a graph spiking neural network architecture for multi-channel EEG classification (EEGSN) that learns the dynamic relational information present in the distributed EEG sensors. Our method reduced the inference computational complexity by $\times 20$ compared to the state-of-the-art SNNs, while achieved comparable accuracy on motor execution classification tasks. Overall, our work provides a framework for interpretable and efficient training of gr
    
[^80]: 理解核回归对抗训练中的过拟合现象

    Understanding Overfitting in Adversarial Training in Kernel Regression. (arXiv:2304.06326v1 [stat.ML])

    [http://arxiv.org/abs/2304.06326](http://arxiv.org/abs/2304.06326)

    本文研究了核回归的对抗训练和带噪声的数据增强，发现如果没有适当的正则化，这两种方法可能会导致过拟合现象，但适当的正则化可以缓解这种现象，提高性能。

    

    对抗训练和带噪声的数据增强是提高神经网络性能的常见方法。本文研究了在再生希尔伯特空间（RKHS）中正则化回归的对抗训练和带噪声的数据增强。当攻击和噪声大小以及正则化参数趋向于零时，建立了这些技术的极限公式。根据该极限公式，分析了特定情况并证明了，如果没有适当的正则化，这两种方法可能具有大于标准核回归的广义误差和Lipschitz常数。然而，通过选择适当的正则化参数，这两种方法可以优于标准核回归，达到更小的广义误差和Lipschitz常数。这些发现支持对抗训练可能导致过拟合的经验观察，以及适当的正则化方法能够缓解这种过拟合现象。

    Adversarial training and data augmentation with noise are widely adopted techniques to enhance the performance of neural networks. This paper investigates adversarial training and data augmentation with noise in the context of regularized regression in a reproducing kernel Hilbert space (RKHS). We establish the limiting formula for these techniques as the attack and noise size, as well as the regularization parameter, tend to zero. Based on this limiting formula, we analyze specific scenarios and demonstrate that, without appropriate regularization, these two methods may have larger generalization error and Lipschitz constant than standard kernel regression. However, by selecting the appropriate regularization parameter, these two methods can outperform standard kernel regression and achieve smaller generalization error and Lipschitz constant. These findings support the empirical observations that adversarial training can lead to overfitting, and appropriate regularization methods, suc
    
[^81]: 深度图表示学习综述

    A Comprehensive Survey on Deep Graph Representation Learning. (arXiv:2304.05055v1 [cs.LG])

    [http://arxiv.org/abs/2304.05055](http://arxiv.org/abs/2304.05055)

    本文综述了深度图表示学习的研究现状和存在的问题，并指出利用深度学习已经显示出巨大的优势和潜力。

    

    图表示学习旨在将高维稀疏的图结构数据有效地编码成低维密集向量，这是一个基本任务，在包括机器学习和数据挖掘在内的一系列领域都得到了广泛的研究。传统图嵌入方法遵循这样一种基本思想，即图中相互连接的节点的嵌入矢量仍然能够保持相对接近的距离，从而保留了图中节点之间的结构信息。然而，这种方法存在以下问题：（i）传统方法的模型容量受限，限制了学习性能; （ii）现有技术通常依赖于无监督学习策略，无法与最新的学习范式相结合；（iii）表示学习和下游任务相互依存，应共同加强。随着深度学习的显着成功，深度图表示学习已经显示出巨大的潜力和优势。

    Graph representation learning aims to effectively encode high-dimensional sparse graph-structured data into low-dimensional dense vectors, which is a fundamental task that has been widely studied in a range of fields, including machine learning and data mining. Classic graph embedding methods follow the basic idea that the embedding vectors of interconnected nodes in the graph can still maintain a relatively close distance, thereby preserving the structural information between the nodes in the graph. However, this is sub-optimal due to: (i) traditional methods have limited model capacity which limits the learning performance; (ii) existing techniques typically rely on unsupervised learning strategies and fail to couple with the latest learning paradigms; (iii) representation learning and downstream tasks are dependent on each other which should be jointly enhanced. With the remarkable success of deep learning, deep graph representation learning has shown great potential and advantages 
    
[^82]: 可训练激活函数的稀疏神经网络贝叶斯优化

    Bayesian optimization for sparse neural networks with trainable activation functions. (arXiv:2304.04455v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.04455](http://arxiv.org/abs/2304.04455)

    本文提出了一种可训练的激活函数以提高神经网络性能，在此基础上开发了一个基于贝叶斯优化和MCMC采样的模型，能通过有效的采样和全局优化来解决过拟合并提高收敛速度。

    

    在深度神经网络的文献中，人们对开发能增强神经网络性能的激活函数非常感兴趣。最近，科学界提出了可以在学习过程中进行训练的激活函数，因为它们似乎可以提高网络性能，特别是通过减少过拟合。本文提出了一种可训练的激活函数，需要估计其参数。开发了一个完全贝叶斯模型，自动从学习数据中估计出模型权重和激活函数参数。开发了一个基于MCMC的优化方案来构建推理。提出的方法旨在通过使用有效的采样方案来保证收敛到全局最大值，从而解决上述问题并改善收敛时间。在三个不同CNN数据集上测试了所提出的方案。有希望的结果证明了它的有效性。

    In the literature on deep neural networks, there is considerable interest in developing activation functions that can enhance neural network performance. In recent years, there has been renewed scientific interest in proposing activation functions that can be trained throughout the learning process, as they appear to improve network performance, especially by reducing overfitting. In this paper, we propose a trainable activation function whose parameters need to be estimated. A fully Bayesian model is developed to automatically estimate from the learning data both the model weights and activation function parameters. An MCMC-based optimization scheme is developed to build the inference. The proposed method aims to solve the aforementioned problems and improve convergence time by using an efficient sampling scheme that guarantees convergence to the global maximum. The proposed scheme is tested on three datasets with three different CNNs. Promising results demonstrate the usefulness of o
    
[^83]: GPT检测器对非英语母语的作者存在偏见。

    GPT detectors are biased against non-native English writers. (arXiv:2304.02819v1 [cs.CL])

    [http://arxiv.org/abs/2304.02819](http://arxiv.org/abs/2304.02819)

    该研究发现，GPT检测器对非英语母语作者存在偏见，容易将其内容错误地分类为AI生成的内容。此外，简单的提示策略可以缓解这种偏见，同时规避GPT检测器，这表明GPT检测器可能会惩罚具有受限语言表达能力的作者。

    

    生成语言模型的快速推广带来了数字通信方面的实质性进展，同时也引发了AI生成内容潜在误用的担忧。虽然已经提出了许多检测方法来区分AI和人类生成的内容，但这些检测器的公平性和鲁棒性仍未得到充分探讨。在这项研究中，我们使用来自英语母语和非英语母语作者的写作样本评估了几种广泛使用的GPT检测器的性能表现。我们的研究发现，这些检测器持续将非英语母语的写作样本错误地分类为AI生成的内容，而原生写作样本则能够被准确识别。此外，我们证明了简单的提示策略不仅可以缓解这种偏见，而且还可以有效地规避GPT检测器，这表明GPT检测器可能无意中惩罚具有受限语言表达能力的作者。我们的研究结果呼吁进行更广泛的讨论。

    The rapid adoption of generative language models has brought about substantial advancements in digital communication, while simultaneously raising concerns regarding the potential misuse of AI-generated content. Although numerous detection methods have been proposed to differentiate between AI and human-generated content, the fairness and robustness of these detectors remain underexplored. In this study, we evaluate the performance of several widely-used GPT detectors using writing samples from native and non-native English writers. Our findings reveal that these detectors consistently misclassify non-native English writing samples as AI-generated, whereas native writing samples are accurately identified. Furthermore, we demonstrate that simple prompting strategies can not only mitigate this bias but also effectively bypass GPT detectors, suggesting that GPT detectors may unintentionally penalize writers with constrained linguistic expressions. Our results call for a broader conversati
    
[^84]: 基于分层Transformer的关系路径和上下文归纳关系预测方法

    Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers. (arXiv:2304.00215v1 [cs.CL])

    [http://arxiv.org/abs/2304.00215](http://arxiv.org/abs/2304.00215)

    本文提出了一种基于分层Transformer的方法，即REPORT，能够同时聚合关系路径和上下文，捕捉实体之间的联系和内在特性。它完全依赖于关系语义，并能自然地推广到完全归纳的设置中。在基准数据集上实现了最先进的性能。

    

    在知识图谱中进行关系预测是一个重要的研究课题。现有的嵌入式方法主要依赖于转导设置，缺乏归纳能力，无法推广到新的实体上进行推理。本文提出了一种新方法，通过使用统一的分层Transformer框架，即REPORT，同时聚合关系路径和上下文，捕捉实体之间的联系和内在特性，这种方法完全依赖于关系语义，并能自然地推广到完全归纳的设置中。在实验中，REPORT表现优于所有基线方法，甚至在两个完全归纳的数据集的八个版本子集上也是如此。此外，REPORT能够将推理推广到训练和推理中没有公共实体的新实体上，并在基准数据集上实现了最先进的性能。

    Relation prediction on knowledge graphs (KGs) is a key research topic. Dominant embedding-based methods mainly focus on the transductive setting and lack the inductive ability to generalize to new entities for inference. Existing methods for inductive reasoning mostly mine the connections between entities, i.e., relational paths, without considering the nature of head and tail entities contained in the relational context. This paper proposes a novel method that captures both connections between entities and the intrinsic nature of entities, by simultaneously aggregating RElational Paths and cOntext with a unified hieRarchical Transformer framework, namely REPORT. REPORT relies solely on relation semantics and can naturally generalize to the fully-inductive setting, where KGs for training and inference have no common entities. In the experiments, REPORT performs consistently better than all baselines on almost all the eight version subsets of two fully-inductive datasets. Moreover. REPO
    
[^85]: 从私有到公有：在私有时间序列分类情境下对GAN进行基准测试

    From Private to Public: Benchmarking GANs in the Context of Private Time Series Classification. (arXiv:2303.15916v1 [cs.LG])

    [http://arxiv.org/abs/2303.15916](http://arxiv.org/abs/2303.15916)

    本论文在时间序列领域对两种GAN架构进行了评估，结果以GSWGAN表现最佳，可以私密地生成保护数据隐私的公共数据。

    

    深度学习已被证明在各个领域和任务中都很成功。然而，当涉及到私人数据时，几个限制使得难以在这些应用领域中使用深度学习方法。最近的方法尝试私密地生成数据，而不是在分类器之上直接应用隐私保护机制。解决方案是以一种保护数据隐私的方式从私有数据创建公共数据。在这项工作中，针对私有时间序列分类情境，评估了两种非常突出的基于GAN的架构。与先前主要局限于图像领域的工作相比，这个基准测试的范围是时间序列领域。实验表明，尤其是GSWGAN在多种公共数据集上表现良好，优于竞争对手DPWGAN。生成数据集的分析进一步验证了GSWGAN在时间序列生成的情境下的优越性。

    Deep learning has proven to be successful in various domains and for different tasks. However, when it comes to private data several restrictions are making it difficult to use deep learning approaches in these application fields. Recent approaches try to generate data privately instead of applying a privacy-preserving mechanism directly, on top of the classifier. The solution is to create public data from private data in a manner that preserves the privacy of the data. In this work, two very prominent GAN-based architectures were evaluated in the context of private time series classification. In contrast to previous work, mostly limited to the image domain, the scope of this benchmark was the time series domain. The experiments show that especially GSWGAN performs well across a variety of public datasets outperforming the competitor DPWGAN. An analysis of the generated datasets further validates the superiority of GSWGAN in the context of time series generation.
    
[^86]: 未知嗅探器用于目标检测：不要对未知对象视而不见

    Unknown Sniffer for Object Detection: Don't Turn a Blind Eye to Unknown Objects. (arXiv:2303.13769v1 [cs.CV])

    [http://arxiv.org/abs/2303.13769](http://arxiv.org/abs/2303.13769)

    本文提出了未知嗅探器(UnSniffer)，用于同时寻找未知和已知的目标。通过引入广义物体置信度(GOC)分数和负能量抑制损失来提高未知对象在背景中的检测准确率，并解决了在推断过程中难以获得每个未知目标最佳框的问题。

    

    近期提出的开放世界目标和开放集检测在寻找从未见过的物体并将其与已知类别区分开方面取得了突破。然而，他们对从已知类别向未知类别的知识传递的研究需要更深入，从而导致探测隐藏在背景中的未知物体的能力不足。本文中，我们提出了未知嗅探器(UnSniffer)来寻找未知和已知的目标。首先，引入广义物体置信度(GOC)分数，仅使用已知类别样本进行监督和避免在背景中不适当地压制未知物体。值得注意的是，从已知物体学习到的这种置信度分数可以推广到未知物体。此外，我们提出了负能量抑制损失来进一步限制背景中非物体样本。接下来，在推断过程中由于缺乏它们在训练中的语义信息，难以获得每个未知目标的最佳框。为了解决这个问题，

    The recently proposed open-world object and open-set detection achieve a breakthrough in finding never-seen-before objects and distinguishing them from class-known ones. However, their studies on knowledge transfer from known classes to unknown ones need to be deeper, leading to the scanty capability for detecting unknowns hidden in the background. In this paper, we propose the unknown sniffer (UnSniffer) to find both unknown and known objects. Firstly, the generalized object confidence (GOC) score is introduced, which only uses class-known samples for supervision and avoids improper suppression of unknowns in the background. Significantly, such confidence score learned from class-known objects can be generalized to unknown ones. Additionally, we propose a negative energy suppression loss to further limit the non-object samples in the background. Next, the best box of each unknown is hard to obtain during inference due to lacking their semantic information in training. To solve this is
    
[^87]: 大型视觉语言模型零样本推理中的校准方法研究

    Enabling Calibration In The Zero-Shot Inference of Large Vision-Language Models. (arXiv:2303.12748v1 [cs.CV])

    [http://arxiv.org/abs/2303.12748](http://arxiv.org/abs/2303.12748)

    本文研究了零样本推理中视觉语言模型的校准问题，发现CLIP存在误校准，并提出了一种修改版的温度缩放方法，可以适用于每个特定的CLIP模型。

    

    深度学习模型的校准对于保证其可靠性和安全使用是至关重要的，因此在监督分类模型中对其进行了广泛研究，提出了降低误校准的方法。然而，视觉语言模型在进行零样本推理时的校准尚未得到全面的研究，例如CLIP。本研究衡量了跨相关变量（如提示，数据集和架构）的校准情况，并发现CLIP的零样本推理存在误校准。此外，我们提出了一种修改版的温度缩放方法，与CLIP作为零样本推理模型的常见用例相一致，并展示出单个学习的温度值可以广泛适用于每个特定的CLIP模型（由选定的预训练数据集和架构定义），跨不同的推理数据集和提示选择。

    Calibration of deep learning models is crucial to their trustworthiness and safe usage, and as such, has been extensively studied in supervised classification models, with methods crafted to decrease miscalibration. However, there has yet to be a comprehensive study of the calibration of vision-language models that are used for zero-shot inference, like CLIP. We measure calibration across relevant variables like prompt, dataset, and architecture, and find that zero-shot inference with CLIP is miscalibrated. Furthermore, we propose a modified version of temperature scaling that is aligned with the common use cases of CLIP as a zero-shot inference model, and show that a single learned temperature generalizes for each specific CLIP model (defined by a chosen pre-training dataset and architecture) across inference dataset and prompt choice.
    
[^88]: 通过 Numerai 数据科学竞赛案例，理解时间表格和多变量时间序列的模型复杂度

    Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament. (arXiv:2303.07925v1 [cs.LG])

    [http://arxiv.org/abs/2303.07925](http://arxiv.org/abs/2303.07925)

    本文采用 Numerai 数据科学竞赛的数据，探究了多变量时间序列建模中不同特征工程和降维方法的应用；提出了一种新的集成方法，用于高维时间序列建模，该方法在通用性、鲁棒性和效率上优于一些深度学习模型。

    

    本文探究了在多变量时间序列建模中使用不同特征工程和降维方法的应用。利用从 Numerai 数据竞赛创建的特征目标交叉相关时间序列数据集，我们证明在过度参数化的情况下，不同特征工程方法的性能与预测会收敛到可由再生核希尔伯特空间刻画的相同平衡态。我们提出了一种新的集成方法，该方法结合了不同的随机非线性变换，随后采用岭回归模型进行高维时间序列建模。与一些常用的用于序列建模的深度学习模型（如 LSTM 和 transformer）相比，我们的方法更加鲁棒（在不同的随机种子下具有较低的模型方差，且对架构的选择不太敏感），并且更有效率。我们方法的另一个优势在于模型的简单性，因为没有必要使用复杂的深度学习框架。

    In this paper, we explore the use of different feature engineering and dimensionality reduction methods in multi-variate time-series modelling. Using a feature-target cross correlation time series dataset created from Numerai tournament, we demonstrate under over-parameterised regime, both the performance and predictions from different feature engineering methods converge to the same equilibrium, which can be characterised by the reproducing kernel Hilbert space. We suggest a new Ensemble method, which combines different random non-linear transforms followed by ridge regression for modelling high dimensional time-series. Compared to some commonly used deep learning models for sequence modelling, such as LSTM and transformers, our method is more robust (lower model variance over different random seeds and less sensitive to the choice of architecture) and more efficient. An additional advantage of our method is model simplicity as there is no need to use sophisticated deep learning frame
    
[^89]: TSMixer：一种全MLP架构用于时间序列预测

    TSMixer: An all-MLP Architecture for Time Series Forecasting. (arXiv:2303.06053v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06053](http://arxiv.org/abs/2303.06053)

    TSMixer是一种通过堆叠多层感知器（MLP）设计的新型结构，基于沿时间和特征维度的混合操作，能够在时间序列预测中表现出极好的性能。

    

    实际时间序列数据集通常是多变量且具有复杂的动态。为了捕获这种复杂性，像循环或基于注意力的顺序深度学习模型这样的高容量结构变得受欢迎。然而，最近的研究表明，简单的单变量线性模型可以在几个常用的学术基准测试中胜过这样的深度学习模型。扩展它们，本文研究线性模型在时间序列预测中的能力，并提出了时序混合器（TSMixer），这是一种通过堆叠多层感知器（MLP）设计的新型结构。 TSMixer基于沿时间和特征维度的混合操作，以有效地提取信息。在流行的学术基准测试上，简单易行的TSMixer与利用特定基准的归纳偏差的专业先进模型相媲美。在具有挑战性和大规模的M5基准测试中，即一个实际的零售数据集上，TSMixer表现出非常出色的性能。

    Real-world time-series datasets are often multivariate with complex dynamics. To capture this complexity, high capacity architectures like recurrent- or attention-based sequential deep learning models have become popular. However, recent work demonstrates that simple univariate linear models can outperform such deep learning models on several commonly used academic benchmarks. Extending them, in this paper, we investigate the capabilities of linear models for time-series forecasting and present Time-Series Mixer (TSMixer), a novel architecture designed by stacking multi-layer perceptrons (MLPs). TSMixer is based on mixing operations along both the time and feature dimensions to extract information efficiently. On popular academic benchmarks, the simple-to-implement TSMixer is comparable to specialized state-of-the-art models that leverage the inductive biases of specific benchmarks. On the challenging and large scale M5 benchmark, a real-world retail dataset, TSMixer demonstrates super
    
[^90]: 集成强化学习综述

    Ensemble Reinforcement Learning: A Survey. (arXiv:2303.02618v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02618](http://arxiv.org/abs/2303.02618)

    集成强化学习（ERL）是一种将强化学习（RL）与集成学习（EL）相结合的有前途的方法，旨在利用多个模型或培训算法全面探索问题空间，并具有强大的泛化能力。

    

    强化学习（RL）已成为解决各种科学和应用问题的高效技术。尽管其取得了成功，但某些复杂任务仍难以仅使用单个模型和算法解决。作为响应，集成强化学习（ERL）作为一种有前途的方法，结合了RL和集成学习（EL）的优点，已经广泛受到欢迎。ERL利用多个模型或培训算法全面探索问题空间，并具有强大的泛化能力。本研究旨在提供ERL的综合调查，以便为读者提供该领域的最新进展和挑战概述。首先，我们介绍ERL的背景和动机。其次，我们详细分析了成功应用于ERL中的策略，包括模型平均、模型选择和模型组合。随后，我们总结了相关数据集并分析了所使用的算法。

    Reinforcement Learning (RL) has emerged as a highly effective technique for addressing various scientific and applied problems. Despite its success, certain complex tasks remain challenging to be addressed solely with a single model and algorithm. In response, ensemble reinforcement learning (ERL), a promising approach that combines the benefits of both RL and ensemble learning (EL), has gained widespread popularity. ERL leverages multiple models or training algorithms to comprehensively explore the problem space and possesses strong generalization capabilities. In this study, we present a comprehensive survey on ERL to provide readers with an overview of recent advances and challenges in the field. First, we introduce the background and motivation for ERL. Second, we analyze in detail the strategies that have been successfully applied in ERL, including model averaging, model selection, and model combination. Subsequently, we summarize the datasets and analyze algorithms used in releva
    
[^91]: RAFEN -- 节点嵌入的正则化对齐框架

    RAFEN -- Regularized Alignment Framework for Embeddings of Nodes. (arXiv:2303.01926v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01926](http://arxiv.org/abs/2303.01926)

    RAFEN是一个节点嵌入的正则化对齐框架，可以在训练过程中学习对齐节点嵌入，而不需要额外的参数设置。RAFEN在现有方法上实现了与更好的性能，并在动态图的情况下可以保持节点嵌入的可比性。

    

    节点嵌入学习是图机器学习领域的一个关键研究领域。一个明确定义的节点嵌入模型应该在最终嵌入中反映出节点特征和图结构。在动态图的情况下，由于特征和结构随时间而变化，这个问题变得更加复杂。在图的演化过程中，某些节点的嵌入应该保持可比性，这可以通过应用对齐过程来实现。在现有的工作中，这一步经常是在节点嵌入已经计算出来后进行的。在本文中，我们介绍了一个框架--RAFEN，它允许使用上述对齐项来丰富任何现有的节点嵌入方法，并在训练过程中学习对齐节点嵌入。我们提出了几个变体的框架，并在六个真实数据集上展示了其性能。RAFEN 实现了与现有方法相当或更好的性能，而不需要额外的参数设置。

    Learning representations of nodes has been a crucial area of the graph machine learning research area. A well-defined node embedding model should reflect both node features and the graph structure in the final embedding. In the case of dynamic graphs, this problem becomes even more complex as both features and structure may change over time. The embeddings of particular nodes should remain comparable during the evolution of the graph, what can be achieved by applying an alignment procedure. This step was often applied in existing works after the node embedding was already computed. In this paper, we introduce a framework -- RAFEN -- that allows to enrich any existing node embedding method using the aforementioned alignment term and learning aligned node embedding during training time. We propose several variants of our framework and demonstrate its performance on six real-world datasets. RAFEN achieves on-par or better performance than existing approaches without requiring additional p
    
[^92]: 离线强化学习中的In-Sample Softmax

    The In-Sample Softmax for Offline Reinforcement Learning. (arXiv:2302.14372v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14372](http://arxiv.org/abs/2302.14372)

    本文研究离线强化学习中的In-Sample Softmax，通过使用只由数据集中的操作组成的In-Sample softmax来解决操作覆盖不足问题，并且In-Sample Actor-Critic与该方法相比在稳定性或性能上表现更好。

    

    强化学习RL代理可以利用以前收集的数据的批次来提取合理的控制策略。然而，在这种离线RL设置中，一个不断出现的问题是，许多方法下的bootstrapping更新受到行动覆盖不足的影响：标准max运算符可能会选择在数据集中没有出现过的最大动作。从这些不准确的值进行bootstrapping更新会导致高估甚至发散。有越来越多的方法尝试近似一个仅使用数据集中涵盖良好的操作的in-sample max。本文强调一个简单的事实：使用仅由数据集中的动作近似In-Sample softmax更加直观。我们展示了在In-Sample softmax基础上的策略迭代的收敛性，并且对于温度的下降，它会接近In-Sample max。我们使用这种In-Sample softmax推导出一个In-Sample Actor-Critic（AC），并且证明其在稳定性或性能上更好。

    Reinforcement learning (RL) agents can leverage batches of previously collected data to extract a reasonable control policy. An emerging issue in this offline RL setting, however, is that the bootstrapping update underlying many of our methods suffers from insufficient action-coverage: standard max operator may select a maximal action that has not been seen in the dataset. Bootstrapping from these inaccurate values can lead to overestimation and even divergence. There are a growing number of methods that attempt to approximate an \emph{in-sample} max, that only uses actions well-covered by the dataset. We highlight a simple fact: it is more straightforward to approximate an in-sample \emph{softmax} using only actions in the dataset. We show that policy iteration based on the in-sample softmax converges, and that for decreasing temperatures it approaches the in-sample max. We derive an In-Sample Actor-Critic (AC), using this in-sample softmax, and show that it is consistently better or 
    
[^93]: 面向金融犯罪检测的隐私保护混合联合学习框架

    A Privacy-Preserving Hybrid Federated Learning Framework for Financial Crime Detection. (arXiv:2302.03654v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03654](http://arxiv.org/abs/2302.03654)

    本论文提出了一种隐私保护混合联合学习框架，它结合中心化和分布式学习的优点，可以用于检测金融犯罪并保护数据隐私。

    

    近十年来，公共和私人部门的金融犯罪数量激增，2022年金融机构的诈骗平均成本为1.02亿美元。开发一种打击金融犯罪的机制是一个紧迫的任务，需要多个机构深度合作，然而这样的合作由于分布式金融数据的隐私和安全要求而带来了重大的技术挑战。本文收集并预处理了多家银行的大规模金融数据集，提出了一种混合联合学习框架，结合中心化和分布式学习的优点，在保护隐私的同时检测金融犯罪。所提出的框架利用隐私保护聚合器，并通过利用中心模型作为分布式学习过程热身来增强联合学习的效率。我们在实际数据集上展示了我们框架的有效性，并证明了我们的方法在保护数据隐私的同时，精确度优于中心化和联合学习方法。

    The recent decade witnessed a surge of increase in financial crimes across the public and private sectors, with an average cost of scams of $102m to financial institutions in 2022. Developing a mechanism for battling financial crimes is an impending task that requires in-depth collaboration from multiple institutions, and yet such collaboration imposed significant technical challenges due to the privacy and security requirements of distributed financial data. For example, consider the modern payment network systems, which can generate millions of transactions per day across a large number of global institutions. Training a detection model of fraudulent transactions requires not only secured transactions but also the private account activities of those involved in each transaction from corresponding bank systems. The distributed nature of both samples and features prevents most existing learning systems from being directly adopted to handle the data mining task. In this paper, we collec
    
[^94]: 细调神经算符结构以提高训练和泛化能力

    Fine-tuning Neural-Operator architectures for training and generalization. (arXiv:2301.11509v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11509](http://arxiv.org/abs/2301.11509)

    本文全面分析了神经算符及其衍生结构的泛化特性并提出了改进方法，包括引入核积分算符来代替自关注机制和逐渐增加模型容量的训练课程，结果显著提高了性能和泛化能力。

    

    本篇论文全面分析了神经算符（NOs）及其衍生结构的泛化特性。通过对测试损失的经验评估、基于复杂性的泛化界限的分析以及对损失景观可视化的定性评估，我们研究了旨在提高NOs泛化能力的修改。受Transformer的成功启发，我们提出了${\textit{s}}{\text{NO}}+\varepsilon$，该方法引入了一个核积分算符来代替自关注机制。我们的结果显示，伴随着损失景观可视化的定性变化，性能显著提高了，适用于各种数据集和初始化。我们猜测，Transformer的布局使优化算法能够找到更好的极小值，并且随机深度可以提高泛化性能。由于训练动态的严格分析是深度学习最突出的未解决问题之一，因此我们还推出了一个新的训练课程，重点是逐渐增加模型容量，从而显著提高了泛化能力。

    This work provides a comprehensive analysis of the generalization properties of Neural Operators (NOs) and their derived architectures. Through empirical evaluation of the test loss, analysis of the complexity-based generalization bounds, and qualitative assessments of the visualization of the loss landscape, we investigate modifications aimed at enhancing the generalization capabilities of NOs. Inspired by the success of Transformers, we propose ${\textit{s}}{\text{NO}}+\varepsilon$, which introduces a kernel integral operator in lieu of self-Attention. Our results reveal significantly improved performance across datasets and initializations, accompanied by qualitative changes in the visualization of the loss landscape. We conjecture that the layout of Transformers enables the optimization algorithm to find better minima, and stochastic depth, improve the generalization performance. As a rigorous analysis of training dynamics is one of the most prominent unsolved problems in deep lear
    
[^95]: 基于深度学习的自然灾害分割模型的通用性

    Toward Foundation Models for Earth Monitoring: Generalizable Deep Learning Models for Natural Hazard Segmentation. (arXiv:2301.09318v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.09318](http://arxiv.org/abs/2301.09318)

    该论文提出的基于深度学习的自然灾害分割模型在透明云，烟雾柱和洪水分割任务中实现了最先进的性能，并通过在适当的预训练任务上进行预训练，显着提高了模型的通用性。

    

    气候变化导致极端天气事件的概率增加，这对全球范围内的社会和企业构成风险。因此，近实时地图制作已成为支持自然灾害救援、风险管理和政府政策决策的新优先事项。最近，实现近实时制图的方法越来越多地利用深度学习（DL）。但是，基于DL的方法仅针对单个地理区域中特定频段的卫星数据的一个特定任务。因此，用于制图特定自然灾害的DL模型难以将其推广到未见过的其他类型的自然灾害上。 在这项工作中，我们提出了一种方法，通过在适当的预训练任务上进行预训练，显着提高DL自然灾害映射器的通用性。在没有任何目标领域的数据的情况下，我们展示了该改进的通用性跨越了四个透明云，烟雾柱和洪水分段任务的多个地理区域。通过合并预训练步骤，我们的模型在所有三个自然灾害分割任务中实现了最先进的性能，同时与其他DL体系结构相比，保持更快的计算速度。我们的方法代表着迈向能够在多个领域通用的近实时自然灾害制图基本模型的有希望的一步。

    Climate change results in an increased probability of extreme weather events that put societies and businesses at risk on a global scale. Therefore, near real-time mapping of natural hazards is an emerging priority for the support of natural disaster relief, risk management, and informing governmental policy decisions. Recent methods to achieve near real-time mapping increasingly leverage deep learning (DL). However, DL-based approaches are designed for one specific task in a single geographic region based on specific frequency bands of satellite data. Therefore, DL models used to map specific natural hazards struggle with their generalization to other types of natural hazards in unseen regions. In this work, we propose a methodology to significantly improve the generalizability of DL natural hazards mappers based on pre-training on a suitable pre-task. Without access to any data from the target domain, we demonstrate this improved generalizability across four U-Net architectures for t
    
[^96]: 通过训练动态理解基于坐标的MLPs的谱偏置

    Understanding the Spectral Bias of Coordinate Based MLPs Via Training Dynamics. (arXiv:2301.05816v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05816](http://arxiv.org/abs/2301.05816)

    该论文研究了基于坐标的MLPs的谱偏置对高频组件收敛的阻碍，并提出使用高频正弦波编码输入来克服这一限制。

    

    谱偏置是神经网络训练的重要观察结果，它表示网络在收敛到更高频率组件前，会学习目标函数的低频表示。这一属性与超参数网络的良好泛化能力有关，但在应用于场景渲染时，采用具有ReLU激活的多层感知器(MLPs)利用密集的低维坐标输入会导致严重的谱偏差，完全阻碍了收敛到高频组件。为了克服这个限制，可以使用高频正弦波编码输入。以前的研究试图使用神经切向核(NTK)和傅里叶分析来解释坐标系中的谱偏差及其严重性。然而，这种方法存在各种限制，因为NTK不能捕捉到真正的网络动态，而傅里叶分析只能提供对频率组件的全局视角。

    Spectral bias is an important observation of neural network training, stating that the network will learn a low frequency representation of the target function before converging to higher frequency components. This property is interesting due to its link to good generalization in over-parameterized networks. However, in applications to scene rendering, where multi-layer perceptrons (MLPs) with ReLU activations utilize dense, low dimensional coordinate based inputs, a severe spectral bias occurs that obstructs convergence to high freqeuncy components entirely. In order to overcome this limitation, one can encode the inputs using high frequency sinusoids. Previous works attempted to explain both spectral bias and its severity in the coordinate based regime using Neural Tangent Kernel (NTK) and Fourier analysis. However, such methods come with various limitations, since NTK does not capture real network dynamics, and Fourier analysis only offers a global perspective on the frequency compo
    
[^97]: 半监督学习的图拉普拉斯算子

    Graph Laplacian for Semi-Supervised Learning. (arXiv:2301.04956v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.04956](http://arxiv.org/abs/2301.04956)

    本文提出了一种新型图拉普拉斯算子，旨在平稳无监督聚类和低监督基于图的分类之间的过渡，并且无需使用Dirichlet正则化或者基于核的方法。

    

    半监督学习在标记数据稀缺但未标记数据丰富的常见场景中非常有用。图（或非本地）拉普拉斯算子是解决各种学习任务的基本平滑算子。对于无监督聚类，通常使用基于图拉普拉斯特征向量的谱嵌入。对于半监督问题，常见的方法是通过图拉普拉斯算子基于Dirichlet能量来解决约束优化问题。然而，随着监督减少，Dirichlet优化变得次优。因此，我们希望在无监督聚类和低监督基于图的分类之间获得平滑的过渡。在本文中，我们提出了一种适用于半监督学习问题的新型图拉普拉斯算子。它基于密度和对比度度量，并允许直接在运算符中对标记数据进行编码。 因此，我们可以成功地通过最小化所提出的平滑SSL图拉普拉斯算子，而无需单独使用Dirichlet正则化或基于核的方法，来进行半监督学习。

    Semi-supervised learning is highly useful in common scenarios where labeled data is scarce but unlabeled data is abundant. The graph (or nonlocal) Laplacian is a fundamental smoothing operator for solving various learning tasks. For unsupervised clustering, a spectral embedding is often used, based on graph-Laplacian eigenvectors. For semi-supervised problems, the common approach is to solve a constrained optimization problem, regularized by a Dirichlet energy, based on the graph-Laplacian. However, as supervision decreases, Dirichlet optimization becomes suboptimal. We therefore would like to obtain a smooth transition between unsupervised clustering and low-supervised graph-based classification. In this paper, we propose a new type of graph-Laplacian which is adapted for Semi-Supervised Learning (SSL) problems. It is based on both density and contrastive measures and allows the encoding of the labeled data directly in the operator. Thus, we can perform successfully semi-supervised le
    
[^98]: BASiS：批量对齐谱嵌入空间

    BASiS: Batch Aligned Spectral Embedding Space. (arXiv:2211.16960v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.16960](http://arxiv.org/abs/2211.16960)

    该论文提出了BASiS，一种用于直接学习特征向量空间的对齐机制，可以同时处理批量和图度量变化，表现优于现有的最佳模型。

    

    图是一种高度通用和多样化的表示形式，适用于几乎任何数据处理问题。谱图理论已被证明提供了强大的算法，并得到了坚实的线性代数理论支持。因此，设计具有谱图特性的深度网络建模块可能会非常有用。例如，这样的网络允许为某些任务设计最佳图形或获得数据的规范正交低维嵌入。最近解决这个问题的尝试是基于最小化瑞利商损失的方式。我们提出了一种直接学习特征向量空间的不同方法。应用于批量学习的直接方法的一个严重问题是将特征不一致地映射到不同批次中的特征向量坐标。我们分析了使用批量学习这项任务的自由度，并提出了一个稳定的对齐机制，可以同时处理批量变化和图度量变化。我们展示了我们的方法比已有的最佳模型表现更好。

    Graph is a highly generic and diverse representation, suitable for almost any data processing problem. Spectral graph theory has been shown to provide powerful algorithms, backed by solid linear algebra theory. It thus can be extremely instrumental to design deep network building blocks with spectral graph characteristics. For instance, such a network allows the design of optimal graphs for certain tasks or obtaining a canonical orthogonal low-dimensional embedding of the data. Recent attempts to solve this problem were based on minimizing Rayleigh-quotient type losses. We propose a different approach of directly learning the eigensapce. A severe problem of the direct approach, applied in batch-learning, is the inconsistent mapping of features to eigenspace coordinates in different batches. We analyze the degrees of freedom of learning this task using batches and propose a stable alignment mechanism that can work both with batch changes and with graph-metric changes. We show that our l
    
[^99]: 高维分位数回归中的转移学习统计推断

    Statistical inference for transfer learning with high-dimensional quantile regression. (arXiv:2211.14578v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.14578](http://arxiv.org/abs/2211.14578)

    本研究提出了一种高维分位数回归模型中的转移学习方法，以适应源域和目标域中的异质性和重尾分布。根据精心选择的可转移源域建立了转移学习估计量的误差界限，并提出了有效的置信区间和假设检验程序，以实现一步完成。

    

    转移学习已经成为一种重要的技术，用于利用源域中的信息来提高目标任务的性能。尽管高维数据普遍存在异质性和/或重尾分布，但目前的转移学习方法未能充分考虑这些问题，可能会影响结果的性能。我们在高维分位数回归模型框架下提出了一种转移学习过程，以适应源域和目标域中的异质性和重尾分布。我们根据精心选择的可转移源域建立了转移学习估计量的误差界限，显示在关键选择标准和较大的源任务样本量下可以实现更低的误差界限。我们进一步提出了一个有效的置信区间和假设检验程序，用于高维分位数回归系数的各个分量，通过倡导双重转移学习估计量，实现一步完成。

    Transfer learning has become an essential technique to exploit information from the source domain to boost performance of the target task. Despite the prevalence in high-dimensional data, heterogeneity and/or heavy tails are insufficiently accounted for by current transfer learning approaches and thus may undermine the resulting performance. We propose a transfer learning procedure in the framework of high-dimensional quantile regression models to accommodate the heterogeneity and heavy tails in the source and target domains. We establish error bounds of the transfer learning estimator based on delicately selected transferable source domains, showing that lower error bounds can be achieved for critical selection criterion and larger sample size of source tasks. We further propose valid confidence interval and hypothesis test procedures for individual component of high-dimensional quantile regression coefficients by advocating a double transfer learning estimator, which is the one-step 
    
[^100]: 自注意力框架在从头计算量子化学中的应用

    A Self-Attention Ansatz for Ab-initio Quantum Chemistry. (arXiv:2211.13672v2 [physics.chem-ph] UPDATED)

    [http://arxiv.org/abs/2211.13672](http://arxiv.org/abs/2211.13672)

    Psiformer是一种使用自注意力机制的新型神经网络架构，可显著提高从头计算量子化学中基态能量的准确性，特别是在大分子上。

    

    我们提出了一种新的神经网络架构-波函数变压器（Psiformer），它使用自注意力作为近似方法（或Ansatz）来解决许多电子薛定谔方程，这是量子化学和材料科学的基本方程。这个方程可以从第一原理中解决，不需要外部训练数据。近年来，像FermiNet和PauliNet这样的深度神经网络已被用于显著提高这些第一原理计算的准确性，但它们缺乏控制电子之间相互作用的注意力机制。在这里，我们展示了Psiformer可以作为这些其他神经网络的可替换品，经常显著提高计算的准确性。尤其是在大分子上，基态能量的提高可以达到几十kcal/mol，比以前的方法有了质的飞跃。这表明，自注意力网络可以学习复杂的量子力学。

    We present a novel neural network architecture using self-attention, the Wavefunction Transformer (Psiformer), which can be used as an approximation (or Ansatz) for solving the many-electron Schr\"odinger equation, the fundamental equation for quantum chemistry and material science. This equation can be solved from first principles, requiring no external training data. In recent years, deep neural networks like the FermiNet and PauliNet have been used to significantly improve the accuracy of these first-principle calculations, but they lack an attention-like mechanism for gating interactions between electrons. Here we show that the Psiformer can be used as a drop-in replacement for these other neural networks, often dramatically improving the accuracy of the calculations. On larger molecules especially, the ground state energy can be improved by dozens of kcal/mol, a qualitative leap over previous methods. This demonstrates that self-attention networks can learn complex quantum mechani
    
[^101]: 实时目标声音提取

    Real-Time Target Sound Extraction. (arXiv:2211.02250v3 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2211.02250](http://arxiv.org/abs/2211.02250)

    该论文提出了Waveformer模型，是第一个能够实时流目标声音提取的神经网络模型。这个模型使用混合体系结构，能够处理大接受域并具有较高的泛化性能，并在模型大小和运行时间上取得了显着改进。

    

    我们提出了第一个能够实现实时流目标声音提取的神经网络模型。为了实现此目标，我们提出了Waveformer，它是一个编码器-解码器体系结构，其中编码器是堆叠的膨胀因果卷积层，解码器则是变压器层。该混合体系结构使用膨胀因果卷积以计算高效性地处理大接受域，并利用变压器体系结构的泛化性能。我们的评估表明，在与该任务的先前模型相比，SI-SNRi的性能提高了2.2-3.3 dB，同时模型大小减小了1.2-4倍，并且运行时间降低了1.5-2倍。我们提供了代码、数据集和音频样本：https://waveformer.cs.washington.edu/。

    We present the first neural network model to achieve real-time and streaming target sound extraction. To accomplish this, we propose Waveformer, an encoder-decoder architecture with a stack of dilated causal convolution layers as the encoder, and a transformer decoder layer as the decoder. This hybrid architecture uses dilated causal convolutions for processing large receptive fields in a computationally efficient manner while also leveraging the generalization performance of transformer-based architectures. Our evaluations show as much as 2.2-3.3 dB improvement in SI-SNRi compared to the prior models for this task while having a 1.2-4x smaller model size and a 1.5-2x lower runtime. We provide code, dataset, and audio samples: https://waveformer.cs.washington.edu/.
    
[^102]: 讨论版文本数据集中的自动化代码提取

    Automated Code Extraction from Discussion Board Text Dataset. (arXiv:2210.17495v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17495](http://arxiv.org/abs/2210.17495)

    本研究探究了用潜在语义分析、潜在狄利克雷分配和聚类词向量三种不同的文本挖掘方法自动提取讨论版数据集中的代码的能力。结果表明，即使是较小的数据集，自动化方法也有助于提取讨论代码，并用于认识网络分析。

    

    本研究介绍并探究了三种不同的文本挖掘方法，即潜在语义分析、潜在狄利克雷分配和聚类词向量，用于自动提取相对较小的讨论版数据集中的代码。我们将每种算法的输出与先前由两个人手动编码的数据集进行比较。结果表明，即使是相对较小的数据集，自动化方法也可以成为课程教师的有益工具，通过提取一些讨论代码，可用于认识网络分析。

    This study introduces and investigates the capabilities of three different text mining approaches, namely Latent Semantic Analysis, Latent Dirichlet Analysis, and Clustering Word Vectors, for automating code extraction from a relatively small discussion board dataset. We compare the outputs of each algorithm with a previous dataset that was manually coded by two human raters. The results show that even with a relatively small dataset, automated approaches can be an asset to course instructors by extracting some of the discussion codes, which can be used in Epistemic Network Analysis.
    
[^103]: 随机零阶梯度下降在L-Lojasiewicz函数上的收敛速率

    Convergence Rates of Stochastic Zeroth-order Gradient Descent for \L ojasiewicz Functions. (arXiv:2210.16997v5 [math.OC] UPDATED)

    [http://arxiv.org/abs/2210.16997](http://arxiv.org/abs/2210.16997)

    该论文证明了在Lojasiewicz函数上，随机零阶梯度下降算法具有收敛速率，且比 $\{ \|\mathbf{x}_t-\mathbf{x}_\infty\| \}_{t \in \mathbb{N}}$更快，无论$f$是平滑还是非平滑的。

    

    我们证明了随机零阶梯度下降（SZGD）算法在Lojasiewicz函数上的收敛速率。SZGD算法迭代如下：$ \mathbf{x}_{t+1} = \mathbf{x}_t - \eta_t \widehat{\nabla} f (\mathbf{x}_t) $，其中$f$是满足Lojasiewicz不等式的目标函数，具有Lojasiewicz指数$\theta$，$\eta_t$是步长（学习率），$ \widehat{\nabla} f (\mathbf{x}_t)$是使用零阶信息估计的近似梯度。我们的结果表明，$ \{ f (\mathbf{x}_t) - f (\mathbf{x}_\infty) \}_{t \in \mathbb{N} }$的收敛速度可以比 $ \{ \| \mathbf{x}_t \mathbf{x}_\infty \| \}_{t \in \mathbb{N} } $更快，无论目标$f$是平滑还是非平滑的。

    We prove convergence rates of Stochastic Zeroth-order Gradient Descent (SZGD) algorithms for Lojasiewicz functions. The SZGD algorithm iterates as \begin{align*}  \mathbf{x}_{t+1} = \mathbf{x}_t - \eta_t \widehat{\nabla} f (\mathbf{x}_t), \qquad t = 0,1,2,3,\cdots , \end{align*} where $f$ is the objective function that satisfies the \L ojasiewicz inequality with \L ojasiewicz exponent $\theta$, $\eta_t$ is the step size (learning rate), and $ \widehat{\nabla} f (\mathbf{x}_t) $ is the approximate gradient estimated using zeroth-order information only.  Our results show that $ \{ f (\mathbf{x}_t) - f (\mathbf{x}_\infty) \}_{t \in \mathbb{N} } $ can converge faster than $ \{ \| \mathbf{x}_t \mathbf{x}_\infty \| \}_{t \in \mathbb{N} }$, regardless of whether the objective $f$ is smooth or nonsmooth.
    
[^104]: 嵌入式注意力深度学习用于实际蓝牙射频指纹的提取

    Embedding-Assisted Attentional Deep Learning for Real-World RF Fingerprinting of Bluetooth. (arXiv:2210.02897v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2210.02897](http://arxiv.org/abs/2210.02897)

    本研究提出了一种嵌入式注意力深度学习框架，用于提取实际蓝牙设备的指纹。该模型具有较高的分类准确率，并在内存使用方面有所改进。

    

    本研究设计了一个可扩展且计算效率高的框架，用于提取实际蓝牙设备的指纹。我们提出了一种嵌入式注意力框架（Mbed-ATN），适用于获取实际蓝牙设备的指纹。我们分析了其在不同环境中的泛化能力，并演示了样本长度和抗混叠抽取的效果。嵌入式模块作为降维单元，将高维的三维输入张量映射为一维特征向量，供ATN模块进一步处理。此外，我们对该模型的复杂性进行了密切评估，并在训练后使用另一个在不同时段和实验设置下收集的真实蓝牙数据集测试了其指纹能力，这是该领域先前研究所没有做过的。研究表明，在样本长度为100 kS时，与基准——GRU和Oracle模型相比，我们的模型内存使用量分别降低了9.17倍和65.2倍。此外，所提出的Mbed-ATN在室内和室外环境中均实现了较高的分类准确率，分别为99.4％和97.4％。

    A scalable and computationally efficient framework is designed to fingerprint real-world Bluetooth devices. We propose an embedding-assisted attentional framework (Mbed-ATN) suitable for fingerprinting actual Bluetooth devices. Its generalization capability is analyzed in different settings and the effect of sample length and anti-aliasing decimation is demonstrated. The embedding module serves as a dimensionality reduction unit that maps the high dimensional 3D input tensor to a 1D feature vector for further processing by the ATN module. Furthermore, unlike the prior research in this field, we closely evaluate the complexity of the model and test its fingerprinting capability with real-world Bluetooth dataset collected under a different time frame and experimental setting while being trained on another. Our study reveals a 9.17x and 65.2x lesser memory usage at a sample length of 100 kS when compared to the benchmark - GRU and Oracle models respectively. Further, the proposed Mbed-ATN
    
[^105]: 概率约束强化学习的策略梯度方法

    Policy Gradients for Probabilistic Constrained Reinforcement Learning. (arXiv:2210.00596v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00596](http://arxiv.org/abs/2210.00596)

    本文提出了处理概率安全约束的策略梯度方法，是首个给出概率约束梯度表达式的工作。

    

    本文考虑在强化学习（RL）的情境下学习安全策略的问题。具体而言，我们考虑了概率安全的概念。也就是说，我们的目标是设计能够在高概率下将系统状态保持在安全集合中的策略。这一概念不同于文献中常考虑的累积约束。处理概率安全的挑战在于缺乏其梯度的表达式。实际上，策略优化算法依赖于目标函数和约束的梯度。据我们所知，本文提供了首个明确给出概率约束梯度表达式的工作。值得注意的是，这种约束的梯度可以应用于各种基于策略的算法中。我们通过实验证明，在一项连续导航问题中处理概率约束是可行的。

    This paper considers the problem of learning safe policies in the context of reinforcement learning (RL). In particular, we consider the notion of probabilistic safety. This is, we aim to design policies that maintain the state of the system in a safe set with high probability. This notion differs from cumulative constraints often considered in the literature. The challenge of working with probabilistic safety is the lack of expressions for their gradients. Indeed, policy optimization algorithms rely on gradients of the objective function and the constraints. To the best of our knowledge, this work is the first one providing such explicit gradient expressions for probabilistic constraints. It is worth noting that the gradient of this family of constraints can be applied to various policy-based algorithms. We demonstrate empirically that it is possible to handle probabilistic constraints in a continuous navigation problem.
    
[^106]: 论AdaGrad在$\R^{d}$上的收敛性：超越凸性、非渐近速率和加速（arXiv：2209.14827v2 [cs.LG] UPDATED）

    On the Convergence of AdaGrad on $\R^{d}$: Beyond Convexity, Non-Asymptotic Rate and Acceleration. (arXiv:2209.14827v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.14827](http://arxiv.org/abs/2209.14827)

    本论文主要展示了AdaGrad在平滑凸函数和更一般的quasar凸函数的情况下的收敛性。具体地，我们提出了新的技术，明确限定了vanilla AdaGrad在无约束问题中的收敛速率，并提出了一种AdaGrad变种，可以实现更快的收敛。

    

    现有的关于平滑凸优化的AdaGrad和其他自适应方法的分析通常是针对具有有界定义域直径的函数。在无约束问题中，以前的研究保证了渐近收敛速率，但没有明确的恒定因子，这适用于整个函数类。此外，在随机环境中，只分析了一个修改版本的AdaGrad，与通常实践中使用的版本不同，在这个回归中不使用最新的梯度来更新步幅。我们的论文旨在弥合这些差距，并在平滑凸函数的标准情况下以及更一般的quasar凸函数的情况下深入理解AdaGrad及其变种。首先，我们展示了新技术，明确地限定了vanilla AdaGrad在无约束问题中的收敛速率，无论是确定性的还是随机的情况下。其次，我们提出了一种AdaGrad变种，我们可以展示l的收敛

    Existing analysis of AdaGrad and other adaptive methods for smooth convex optimization is typically for functions with bounded domain diameter. In unconstrained problems, previous works guarantee an asymptotic convergence rate without an explicit constant factor that holds true for the entire function class. Furthermore, in the stochastic setting, only a modified version of AdaGrad, different from the one commonly used in practice, in which the latest gradient is not used to update the stepsize, has been analyzed. Our paper aims at bridging these gaps and developing a deeper understanding of AdaGrad and its variants in the standard setting of smooth convex functions as well as the more general setting of quasar convex functions. First, we demonstrate new techniques to explicitly bound the convergence rate of the vanilla AdaGrad for unconstrained problems in both deterministic and stochastic settings. Second, we propose a variant of AdaGrad for which we can show the convergence of the l
    
[^107]: 差分隐私的分区变分推断方法

    Differentially private partitioned variational inference. (arXiv:2209.11595v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.11595](http://arxiv.org/abs/2209.11595)

    本论文提出了差分隐私的分区变分推断算法，是第一种在联邦贝叶斯学习环境下实现差分隐私的方法。

    

    从分布在多个设备上的敏感数据中学习隐私保护模型是一个日益重要的问题。该问题通常在联邦学习背景下进行规划，目标是在保持数据分布的同时学习单个全局模型。此外，贝叶斯学习是一种流行的建模方法，因为它自然地支持可靠的不确定性估计。然而，即使对于集中的非隐私数据，贝叶斯学习也通常是不可操作的，因此变分推断等近似技术是必需的。近期，通过分区变分推断算法，变分推断已经扩展到非隐私联邦学习的情况。对于隐私保护，目前的黄金标准被称为差分隐私。差分隐私在数学上定义了一个强的隐私保护概念。本文提出了差分隐私的分区变分推断方法，是第一种通过分区变分推断算法在联邦贝叶斯学习环境下实现差分隐私的方法。我们在人造和真实基准测试中展示了我们提出的方法的有效性和效率。

    Learning a privacy-preserving model from sensitive data which are distributed across multiple devices is an increasingly important problem. The problem is often formulated in the federated learning context, with the aim of learning a single global model while keeping the data distributed. Moreover, Bayesian learning is a popular approach for modelling, since it naturally supports reliable uncertainty estimates. However, Bayesian learning is generally intractable even with centralised non-private data and so approximation techniques such as variational inference are a necessity. Variational inference has recently been extended to the non-private federated learning setting via the partitioned variational inference algorithm. For privacy protection, the current gold standard is called differential privacy. Differential privacy guarantees privacy in a strong, mathematically clearly defined sense.  In this paper, we present differentially private partitioned variational inference, the first
    
[^108]: 限制网络表示，让模型知道自己的不足

    Constraining Representations Yields Models That Know What They Don't Know. (arXiv:2208.14488v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.14488](http://arxiv.org/abs/2208.14488)

    通过对模型内部激活模式施加类感知约束的方法，本文提出总激活分类器（TAC）可以让模型更加安全、可靠，并具有广泛的应用前景。

    

    神经网络的一个已知失败模式是它们可能自信地返回错误的预测。这种不安全的行为在使用案例略有不同的情况下特别频繁，或者在面对敌手时。本文介绍了一种新的方法来以一种广泛且一般的方式解决这些问题：对模型内部的激活模式施加类感知约束。具体而言，我们为每个类分配一个独特的、固定的、随机生成的二进制向量（后文称为类编码），并训练模型，使其通过交叉深度的激活模式根据输入样本的类别预测相应的类编码。结果预测器被称为总激活分类器（TAC），TAC可以从头开始训练，也可以在冻结的预训练神经网络顶部用极小的代价作为薄层附加使用。TAC的激活模式与最接近的有效编码之间的距离作为额外的置信度评分。

    A well-known failure mode of neural networks is that they may confidently return erroneous predictions. Such unsafe behaviour is particularly frequent when the use case slightly differs from the training context, and/or in the presence of an adversary. This work presents a novel direction to address these issues in a broad, general manner: imposing class-aware constraints on a model's internal activation patterns. Specifically, we assign to each class a unique, fixed, randomly-generated binary vector - hereafter called class code and train the model so that its cross-depths activation patterns predict the appropriate class code according to the input sample's class. The resulting predictors are dubbed Total Activation Classifiers (TAC), and TACs may either be trained from scratch, or used with negligible cost as a thin add-on on top of a frozen, pre-trained neural network. The distance between a TAC's activation pattern and the closest valid code acts as an additional confidence scor
    
[^109]: 线性预测器的模型大小、测试损失和训练损失之间的通用权衡

    A Universal Trade-off Between the Model Size, Test Loss, and Training Loss of Linear Predictors. (arXiv:2207.11621v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.11621](http://arxiv.org/abs/2207.11621)

    本文建立了一个与算法和分布无关的非渐进性权衡方法，来衡量线性预测器的模型大小、测试损失和训练损失之间的关系，发现测试数据上表现出色的模型要么是经典的，要么是现代的。同时提供了当白化特征的极限谱分布为Marchenko-Pastur时的更为精确的渐进分析，使得分析更加精确。

    

    本文建立了一个与算法和分布无关的非渐进性权衡方法，来衡量线性预测器的模型大小、超额测试损失和训练损失之间的关系。我们发现，在测试数据上表现出色的模型要么是“经典”的——其训练损失接近噪声水平，要么是“现代的”——其参数数量远远超过仅仅能精确拟合训练数据所需的最小参数数。同时，我们还提供了当白化特征的极限谱分布为Marchenko-Pastur时的更为精确的渐进分析。值得注意的是，在插值顶点附近，即参数数量刚好足以拟合训练数据时，Marchenko-Pastur分析更加精确，而随着过度参数化程度的增加，它恰好与无分布限制的理论上界相一致。

    In this work we establish an algorithm and distribution independent non-asymptotic trade-off between the model size, excess test loss, and training loss of linear predictors. Specifically, we show that models that perform well on the test data (have low excess loss) are either "classical" -- have training loss close to the noise level, or are "modern" -- have a much larger number of parameters compared to the minimum needed to fit the training data exactly.  We also provide a more precise asymptotic analysis when the limiting spectral distribution of the whitened features is Marchenko-Pastur. Remarkably, while the Marchenko-Pastur analysis is far more precise near the interpolation peak, where the number of parameters is just enough to fit the training data, it coincides exactly with the distribution independent bound as the level of overparametrization increases.
    
[^110]: 利用编译器中间表示进行代码翻译

    Code Translation with Compiler Representations. (arXiv:2207.03578v4 [cs.PL] UPDATED)

    [http://arxiv.org/abs/2207.03578](http://arxiv.org/abs/2207.03578)

    本文提出了一种将编译器中间表示与神经机器翻译相结合的方法，能够更好地捕捉代码的语义，从而提高了代码翻译的质量和实用性。

    

    本文提出了一种利用低级别的编译器中间表示（IR）来改进代码翻译的方法。我们的方法结合了神经机器翻译（NMT）和IR，能够更好地捕捉代码的语义，避免以往方法存在的常见错误，从而提高了翻译的质量和实用性。

    In this paper, we leverage low-level compiler intermediate representations (IR) to improve code translation. Traditional transpilers rely on syntactic information and handcrafted rules, which limits their applicability and produces unnatural-looking code. Applying neural machine translation (NMT) approaches to code has successfully broadened the set of programs on which one can get a natural-looking translation. However, they treat the code as sequences of text tokens, and still do not differentiate well enough between similar pieces of code which have different semantics in different languages. The consequence is low quality translation, reducing the practicality of NMT, and stressing the need for an approach significantly increasing its accuracy. Here we propose to augment code translation with IRs, specifically LLVM IR, with results on the C++, Java, Rust, and Go languages. Our method improves upon the state of the art for unsupervised code translation, increasing the number of corr
    
[^111]: 带轨迹奖励的离线强化学习的可证效率性

    Provably Efficient Offline Reinforcement Learning with Trajectory-Wise Reward. (arXiv:2206.06426v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.06426](http://arxiv.org/abs/2206.06426)

    本文提出了一种离线RL算法PARTED，它通过基于最小二乘的奖励重新分配将轨迹回报分解为每步代理奖励，然后基于学习的代理奖励执行悲观值迭代，用于解决轨迹奖励难以很好地利用的问题。

    

    强化学习（RL）的显著成功严重依赖于观测每个访问的状态-动作对的奖励。然而在许多真实世界应用中，代理只能观察表示整个轨迹质量的得分，称为"轨迹奖励"。在这种情况下，标准的RL方法很难很好地利用轨迹奖励，并且可能会在策略评估中产生大的偏差和方差误差。在本文中，我们提出一种新的离线RL算法PARTED，它通过基于最小二乘的奖励重新分配将轨迹回报分解为每步代理奖励，然后基于学习的代理奖励执行悲观值迭代。为了确保PARTED构建的值函数始终对最优值函数悲观，我们设计了一个新的惩罚项来抵消代理奖励的不确定性。

    The remarkable success of reinforcement learning (RL) heavily relies on observing the reward of every visited state-action pair. In many real world applications, however, an agent can observe only a score that represents the quality of the whole trajectory, which is referred to as the {\em trajectory-wise reward}. In such a situation, it is difficult for standard RL methods to well utilize trajectory-wise reward, and large bias and variance errors can be incurred in policy evaluation. In this work, we propose a novel offline RL algorithm, called Pessimistic vAlue iteRaTion with rEward Decomposition (PARTED), which decomposes the trajectory return into per-step proxy rewards via least-squares-based reward redistribution, and then performs pessimistic value iteration based on the learned proxy reward. To ensure the value functions constructed by PARTED are always pessimistic with respect to the optimal ones, we design a new penalty term to offset the uncertainty of the proxy reward. For 
    
[^112]: 高低注意力的快速视觉Transformer

    Fast Vision Transformers with HiLo Attention. (arXiv:2205.13213v5 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2205.13213](http://arxiv.org/abs/2205.13213)

    摘要：本文提出了一种名为HiLo注意力的自注意机制，使用分而治之的策略在注意力转换器中分解高/低频模式，可以更加高效地运行视觉Transformer，并在不同模型大小的范围内胜过现有的最先进方法。

    

    视觉Transformer (ViT) 已经在计算机视觉领域引起了重大突破。它们的高效设计大多由计算复杂性之间的间接指标（即 FLOP）指导，而与直接指标（如吞吐量）存在明显差距。因此，我们提出使用目标平台上的直接速度评估作为高效ViTs的设计原则。特别是，我们引入了LITv2，这是一个简单而有效的ViT，它在不同模型大小的范围内与现有的最先进方法相比表现优异，速度更快。LITv2的核心是一种新颖的自注意机制，我们称之为“高低注意力”。高低注意力的灵感来自于图像中的高频捕捉局部细节，低频专注于全局结构，而多头自注意层忽略了不同频率的特征。因此，我们提出使用分而治之的策略在注意力转换器中分解高/低频模式，其中自注意层分为两个分支，每个分支专门捕捉局部或全局信息。

    Vision Transformers (ViTs) have triggered the most recent and significant breakthroughs in computer vision. Their efficient designs are mostly guided by the indirect metric of computational complexity, i.e., FLOPs, which however has a clear gap with the direct metric such as throughput. Thus, we propose to use the direct speed evaluation on the target platform as the design principle for efficient ViTs. Particularly, we introduce LITv2, a simple and effective ViT which performs favourably against the existing state-of-the-art methods across a spectrum of different model sizes with faster speed. At the core of LITv2 is a novel self-attention mechanism, which we dub HiLo. HiLo is inspired by the insight that high frequencies in an image capture local fine details and low frequencies focus on global structures, whereas a multi-head self-attention layer neglects the characteristic of different frequencies. Therefore, we propose to disentangle the high/low frequency patterns in an attention
    
[^113]: 宽泛推荐系统：一种高效的非线性协同过滤方法

    Broad Recommender System: An Efficient Nonlinear Collaborative Filtering Approach. (arXiv:2204.11602v4 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2204.11602](http://arxiv.org/abs/2204.11602)

    本文提出了一种新的宽泛推荐系统(BroadCF)，使用宽泛学习系统(BLS)作为映射函数来学习用户和项目之间的复杂非线性关系，同时通过用户-项评级协同向量预处理程序将原始数据转换为更适合BLS学习的格式。BroadCF的实验结果表明，在用户推荐准确性和效率方面都优于几种最先进的CF方法。

    

    最近，深度神经网络（DNN）被广泛引入到协同过滤（CF）中，以产生更准确的推荐结果，因为它们具有捕获项目和用户之间复杂非线性关系的能力。然而，基于DNN的模型通常遭受高计算复杂性的问题，即消耗非常长的训练时间并存储大量可训练参数。为了解决这些问题，我们提出了一种新的宽泛推荐系统，称为宽泛协同过滤（BroadCF），它是一种高效的非线性协同过滤方法。宽泛学习系统（BLS）被用作映射函数，以学习用户和项目之间的复杂非线性关系，可以避免上述问题，同时实现非常令人满意的推荐性能。但是，将原始评分数据直接馈送到BLS中并不可行。为此，我们提出了一种用户-项评级协同向量预处理程序，将原始数据转换为更适合BLS学习的格式。三个公共数据集上的实验结果表明，我们提出的BroadCF在推荐准确性和效率方面均优于几种最先进的CF方法。

    Recently, Deep Neural Networks (DNNs) have been widely introduced into Collaborative Filtering (CF) to produce more accurate recommendation results due to their capability of capturing the complex nonlinear relationships between items and users.However, the DNNs-based models usually suffer from high computational complexity, i.e., consuming very long training time and storing huge amount of trainable parameters. To address these problems, we propose a new broad recommender system called Broad Collaborative Filtering (BroadCF), which is an efficient nonlinear collaborative filtering approach. Instead of DNNs, Broad Learning System (BLS) is used as a mapping function to learn the complex nonlinear relationships between users and items, which can avoid the above issues while achieving very satisfactory recommendation performance. However, it is not feasible to directly feed the original rating data into BLS. To this end, we propose a user-item rating collaborative vector preprocessing pro
    
[^114]: 应用维度扩展和迁移学习于EMS负荷监测中

    Dimensionality Expansion of Load Monitoring Time Series and Transfer Learning for EMS. (arXiv:2204.02802v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.02802](http://arxiv.org/abs/2204.02802)

    本文提出了一种维度扩展和迁移学习的负载监测方法，该方法在5个不同的低频数据集上表现良好，可以更高效地计算。研究表明在跨数据集内领域迁移学习方面也有良好的效果。

    

    能源管理系统（EMS）通过（非）侵入式负载监测（(N)ILM）监控和管理电器，帮助居民提高能源效率以及节省用电费用。本文提出了一种新的基于时间序列维度扩展和迁移学习的负载监测方法。我们在5个不同的低频数据集上进行了广泛的评估，并使用类似视频的转换和资源感知深度学习架构提出了特征维度扩展的方法，方法在29种电器的数据集上获得了平均加权F1得分0.88，相较于最先进的成像方法，具有更高的计算效率。此外，我们还研究了该方法在跨数据集内领域迁移学习方面的可行性。

    Energy management systems (EMS) rely on (non)-intrusive load monitoring (N)ILM to monitor and manage appliances and help residents be more energy efficient and thus more frugal. The robustness as well as the transfer potential of the most promising machine learning solutions for (N)ILM is not yet fully understood as they are trained and evaluated on relatively limited data. In this paper, we propose a new approach for load monitoring in building EMS based on dimensionality expansion of time series and transfer learning. We perform an extensive evaluation on 5 different low-frequency datasets. The proposed feature dimensionality expansion using video-like transformation and resource-aware deep learning architecture achieves an average weighted F1 score of 0.88 across the datasets with 29 appliances and is computationally more efficient compared to the state-of-the-art imaging methods. Investigating the proposed method for cross-dataset intra-domain transfer learning, we find that 1) our
    
[^115]: 离线强化学习综述：分类、回顾和未解决问题

    A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems. (arXiv:2203.01387v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.01387](http://arxiv.org/abs/2203.01387)

    本文综述了离线强化学习中的分类与最新算法突破，离线RL算法具有更广泛的应用，尤其适用于教育、医疗保健和机器人等实际应用。

    

    随着深度学习的广泛应用，强化学习（RL）在解决以往无法处理的问题方面取得了显著进展，如从像素观察中玩复杂游戏、与人类进行对话以及控制机器人智能体。然而，仍有许多领域由于与环境互动的高成本和危险而无法用RL解决。离线RL是一种范式，它仅从以前收集的交互的静态数据集中学习，因此可以从大型和多样化的培训数据集中提取策略。有效的离线RL算法比在线RL算法具有更广泛的应用，尤其适用于教育、医疗保健和机器人等实际应用。在本文中，我们提出了一个统一的分类法，对离线RL方法进行分类。此外，我们还对该领域最新的算法突破进行了全面回顾。

    With the widespread adoption of deep learning, reinforcement learning (RL) has experienced a dramatic increase in popularity, scaling to previously intractable problems, such as playing complex games from pixel observations, sustaining conversations with humans, and controlling robotic agents. However, there is still a wide range of domains inaccessible to RL due to the high cost and danger of interacting with the environment. Offline RL is a paradigm that learns exclusively from static datasets of previously collected interactions, making it feasible to extract policies from large and diverse training datasets. Effective offline RL algorithms have a much wider range of applications than online RL, being particularly appealing for real-world applications, such as education, healthcare, and robotics. In this work, we contribute with a unifying taxonomy to classify offline RL methods. Furthermore, we provide a comprehensive review of the latest algorithmic breakthroughs in the field usin
    
[^116]: 经过自适应采样的纵向数据的统计推断

    Statistical Inference After Adaptive Sampling for Longitudinal Data. (arXiv:2202.07098v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.07098](http://arxiv.org/abs/2202.07098)

    本文针对经过自适应采样得到的纵向用户数据，通过 Z-估计器提出了新的方法进行统计分析，包括引入校正夹心方差估计器等。通过汇集用户数据，自适应采样算法能够潜在地更快地学习，但同时也引入了依赖关系导致方差估计低估，因此需要采用新的方法进行统计分析。

    

    在数字干预实验中，在线强化学习和其他自适应采样算法被越来越多地用于优化用户的治疗效果。本文关注于由大量自适应采样算法设计用于利用多个用户的不断累加数据在线优化治疗决策的纵向用户数据。通过合并或"汇集"用户数据，自适应采样算法能够潜在地更快地学习。然而，通过汇集，这些算法在样本用户数据轨迹之间引入了依赖关系；我们表明这会导致对于独立同分布数据的标准方差估计低估该数据类型上常见估计量的真实方差。我们通过Z-估计器开发出新的方法来对这种自适应采样数据进行各种统计分析。具体而言，我们引入了"自适应"夹心方差估计器——一种校正夹心估计器，可以得到一致的方差估计。

    Online reinforcement learning and other adaptive sampling algorithms are increasingly used in digital intervention experiments to optimize treatment delivery for users over time. In this work, we focus on longitudinal user data collected by a large class of adaptive sampling algorithms that are designed to optimize treatment decisions online using accruing data from multiple users. Combining or "pooling" data across users allows adaptive sampling algorithms to potentially learn faster. However, by pooling, these algorithms induce dependence between the sampled user data trajectories; we show that this can cause standard variance estimators for i.i.d. data to underestimate the true variance of common estimators on this data type. We develop novel methods to perform a variety of statistical analyses on such adaptively sampled data via Z-estimation. Specifically, we introduce the \textit{adaptive} sandwich variance estimator, a corrected sandwich estimator that leads to consistent varianc
    
[^117]: 采用循环神经网络控制双重供应链库存系统

    Control of Dual-Sourcing Inventory Systems using Recurrent Neural Networks. (arXiv:2201.06126v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.06126](http://arxiv.org/abs/2201.06126)

    本论文采用循环神经网络令库存管理者能够更好地决策如何重新补充库存，特别是在考虑多个供应商订单和成本的情况下快速调整需求。

    

    库存管理中一个关键的挑战是识别出最优的从多个供应商重新补充库存的策略。为了解决这类优化问题，库存管理者需要决定从每个供应商订购哪些数量的产品，考虑到净库存和待处理订单，以便同时最小化预期的滞后、持有和调度成本。库存管理问题已经被广泛研究超过60年，然而即使是基本的双重采购问题，在其中一个供应商的订单比另一个供应商的订单要快的情况下，在其一般形式中依然难以解决。此外，还有一个新兴的需求，即开发主动的可扩展优化算法，能够及时地根据动态的需求变化调整其推荐结果。在这项工作中，我们将双重采购问题视为神经网络优化问题，并将库存动态及其补充（即控制）策略的信息融入其中。

    A key challenge in inventory management is to identify policies that optimally replenish inventory from multiple suppliers. To solve such optimization problems, inventory managers need to decide what quantities to order from each supplier, given the net inventory and outstanding orders, so that the expected backlogging, holding, and sourcing costs are jointly minimized. Inventory management problems have been studied extensively for over 60 years, and yet even basic dual-sourcing problems, in which orders from an expensive supplier arrive faster than orders from a regular supplier, remain intractable in their general form. In addition, there is an emerging need to develop proactive, scalable optimization algorithms that can adjust their recommendations to dynamic demand shifts in a timely fashion. In this work, we approach dual sourcing from a neural network--based optimization lens and incorporate information on inventory dynamics and its replenishment (i.e., control) policies into th
    
[^118]: 带有潜在状态信息共享的分散式多智能体策略梯度中的价值函数分解

    Value Functions Factorization with Latent State Information Sharing in Decentralized Multi-Agent Policy Gradients. (arXiv:2201.01247v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2201.01247](http://arxiv.org/abs/2201.01247)

    在QMIX方法的基础上，提出了LSF-SAC框架，其中包括一个潜在信息共享机制，可显著扩展价值函数分解的能力，同时在完全分散执行中保持了有效性。

    

    通过集中训练和分散执行的价值函数分解方法有望解决合作多智能体强化学习任务。在这个领域中，QMIX是一种方法，已经成为最先进的技术，并在StarCraft II微观管理基准测试中取得了最佳性能。然而，QMIX中的单个智能体估计的单调混合被认为限制了它能表示的联合动作Q值的范围，同时全局状态信息不足以进行单个智能体值函数估计，通常会导致结果次优。为此，我们提出了LSF-SAC，这是一个新颖的框架，具有基于变分推理的信息共享机制作为额外的状态信息，以辅助个体智能体在价值函数分解中。我们证明了这种潜在的个体状态信息共享可以显著扩展值函数分解的能力，同时在LSF-SAC中仍然可以通过软限制实现完全分散执行。

    Value function factorization via centralized training and decentralized execution is promising for solving cooperative multi-agent reinforcement tasks. One of the approaches in this area, QMIX, has become state-of-the-art and achieved the best performance on the StarCraft II micromanagement benchmark. However, the monotonic-mixing of per agent estimates in QMIX is known to restrict the joint action Q-values it can represent, as well as the insufficient global state information for single agent value function estimation, often resulting in suboptimality. To this end, we present LSF-SAC, a novel framework that features a variational inference-based information-sharing mechanism as extra state information to assist individual agents in the value function factorization. We demonstrate that such latent individual state information sharing can significantly expand the power of value function factorization, while fully decentralized execution can still be maintained in LSF-SAC through a soft-
    
[^119]: 固定翼无人机姿态控制的数据高效深度强化学习：现场实验

    Data-Efficient Deep Reinforcement Learning for Attitude Control of Fixed-Wing UAVs: Field Experiments. (arXiv:2111.04153v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2111.04153](http://arxiv.org/abs/2111.04153)

    本文使用深度强化学习算法成功实现了对固定翼无人机的姿态控制，只需三分钟的飞行数据。该算法可以直接操作原始非线性动力学，相较于现有技术具备了更好的性能和有效性。

    

    由于存在不确定的非线性动力学、执行机构约束以及纵向和横向运动的耦合，固定翼无人机的姿态控制是一个难题。目前最先进的自动驾驶仍基于线性控制，其有效性和性能受到限制。本文研究表明，深度强化学习（DRL）可以成功学习直接操作原始非线性动力学的固定翼无人机的姿态控制，只需要三分钟的飞行数据。我们首先在仿真环境中训练模型，然后在飞行测试中将学习的控制器应用于无人机上，其表现与现有的ArduPlane比例积分微分（PID）姿态控制器相当。

    Attitude control of fixed-wing unmanned aerial vehicles (UAVs) is a difficult control problem in part due to uncertain nonlinear dynamics, actuator constraints, and coupled longitudinal and lateral motions. Current state-of-the-art autopilots are based on linear control and are thus limited in their effectiveness and performance. Deep reinforcement learning (DRL) is a machine learning method to automatically discover optimal control laws through interaction with the controlled system, which can handle complex nonlinear dynamics. We show in this paper that DRL can successfully learn to perform attitude control of a fixed-wing UAV operating directly on the original nonlinear dynamics, requiring as little as three minutes of flight data. We initially train our model in a simulation environment and then deploy the learned controller on the UAV in flight tests, demonstrating comparable performance to the state-of-the-art ArduPlane proportional-integral-derivative (PID) attitude controller w
    
[^120]: 稀疏加低秩矩阵分解: 一种离散优化方法

    Sparse Plus Low Rank Matrix Decomposition: A Discrete Optimization Approach. (arXiv:2109.12701v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2109.12701](http://arxiv.org/abs/2109.12701)

    本文研究稀疏加低秩矩阵分解问题(SLR)，提出了一种新的离散模型和求解方法，适用于多种应用场景。

    

    本文研究稀疏加低秩分解问题(SLR)，即将损坏的数据矩阵分解为包含基本真值的低秩矩阵和包含扰动的稀疏矩阵。 SLR是运筹学和机器学习领域的基础问题，在数据压缩、潜在语义索引、协同过滤和医学成像等各种应用中出现。我们提出了一种新的离散模型，并设计了交替最小化启发式算法以及新的半定松弛算法来解决这个问题。此外，我们还开发了一个自定义分支定界算法，利用我们的启发式算法和凸松弛来解决小规模的SLR问题。我们的启发式算法可以解决 $n=10000$ 的问题规模。

    We study the Sparse Plus Low-Rank decomposition problem (SLR), which is the problem of decomposing a corrupted data matrix into a sparse matrix of perturbations plus a low-rank matrix containing the ground truth. SLR is a fundamental problem in Operations Research and Machine Learning which arises in various applications, including data compression, latent semantic indexing, collaborative filtering, and medical imaging. We introduce a novel formulation for SLR that directly models its underlying discreteness. For this formulation, we develop an alternating minimization heuristic that computes high-quality solutions and a novel semidefinite relaxation that provides meaningful bounds for the solutions returned by our heuristic. We also develop a custom branch-and-bound algorithm that leverages our heuristic and convex relaxations to solve small instances of SLR to certifiable (near) optimality. Given an input $n$-by-$n$ matrix, our heuristic scales to solve instances where $n=10000$ in m
    
[^121]: 带采样成本的连续时间多臂赌博机问题研究

    Continuous Time Bandits With Sampling Costs. (arXiv:2107.05289v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.05289](http://arxiv.org/abs/2107.05289)

    本文研究了带采样成本的连续时间多臂赌博机问题，在连续时间里，学习者要在获得更高奖励和承担采样成本之间进行有效平衡。本文提出了一个达到下界的算法，并揭示了与传统多臂赌博机问题不同的特殊现象，具有广泛应用价值。

    

    本文研究了连续时间下的多臂赌博机问题(Continuous Time Multi-arm Bandit Problem，CTMAB)。在给定时间段内，学习者可以对臂进行任意次采样，每次采样都能获得随机奖励，但采样频率的提高会带来额外的惩罚/成本。因此，存在获得更高奖励与承担采样成本之间的平衡。本文旨在设计一种学习算法，使遗憾（regret，定义为学习算法与理论最优策略收益之间的差值）达到最小。CTMAB与通常的多臂赌博机问题(Multi-armed Bandit Problem，MAB)有根本的区别，例如，在CTMAB中，单臂情况都不是微不足道的，因为最优采样频率取决于臂的均值，而该均值需要被估计。本文首先建立了所有算法可达到的遗憾下界，然后提出了一种在对数因子上达到下界的算法。对于单臂情况，我们证明了下限和上限大致符合，并提出了一个计算效率高的算法。我们的结果揭示了在经典MAB问题中不存在的令人惊讶的现象，并在各个领域的顺序决策问题中具有广泛的应用。

    We consider a continuous-time multi-arm bandit problem (CTMAB), where the learner can sample arms any number of times in a given interval and obtain a random reward from each sample, however, increasing the frequency of sampling incurs an additive penalty/cost. Thus, there is a tradeoff between obtaining large reward and incurring sampling cost as a function of the sampling frequency. The goal is to design a learning algorithm that minimizes regret, that is defined as the difference of the payoff of the oracle policy and that of the learning algorithm. CTMAB is fundamentally different than the usual multi-arm bandit problem (MAB), e.g., even the single-arm case is non-trivial in CTMAB, since the optimal sampling frequency depends on the mean of the arm, which needs to be estimated. We first establish lower bounds on the regret achievable with any algorithm and then propose algorithms that achieve the lower bound up to logarithmic factors. For the single-arm case, we show that the lower
    
[^122]: 非利普希茨网络的鲁棒性分析

    An Analysis of Robustness of Non-Lipschitz Networks. (arXiv:2010.06154v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2010.06154](http://arxiv.org/abs/2010.06154)

    本文研究了深度非利普希茨网络的鲁棒性问题，定义了一个攻击模型帮助理解内在属性，证明了此类攻击者可以战胜所有必须对其输入进行分类的算法，但也提出了克服此类攻击者的方法，进一步提供了理论保证并为最近邻算法提供了新的鲁棒性保证。

    

    尽管已经取得了显著进展，但深度网络仍然极易受到对抗攻击的影响。其中一个根本性的挑战是：即使输入略微扰动，也可能会产生网络最终层特征空间中的大幅移动。在本文中，我们定义了一个攻击模型来抽象这个挑战，以帮助理解它的内在属性。在我们的模型中，对手可以在特征空间中的任意距离上移动数据，但只能在随机的低维子空间内进行操作。我们证明了这种攻击者可以非常强大：它们可以战胜任何必须对其收到的所有输入进行分类的算法。然而，通过允许算法放弃处理不寻常的输入，我们表明当类在特征空间中相对分离得很好时，这种攻击者是可以被克服的。我们进一步提供了强有力的理论保证，以使用数据驱动方法设置算法参数以优化精度-放弃权衡。我们的结果为最近邻算法提供了新的鲁棒性保证。

    Despite significant advances, deep networks remain highly susceptible to adversarial attack. One fundamental challenge is that small input perturbations can often produce large movements in the network's final-layer feature space. In this paper, we define an attack model that abstracts this challenge, to help understand its intrinsic properties. In our model, the adversary may move data an arbitrary distance in feature space but only in random low-dimensional subspaces. We prove such adversaries can be quite powerful: defeating any algorithm that must classify any input it is given. However, by allowing the algorithm to abstain on unusual inputs, we show such adversaries can be overcome when classes are reasonably well-separated in feature space. We further provide strong theoretical guarantees for setting algorithm parameters to optimize over accuracy-abstention trade-offs using data-driven methods. Our results provide new robustness guarantees for nearest-neighbor style algorithms, a
    
[^123]: 智能选择和选择单子

    Smart Choices and the Selection Monad. (arXiv:2007.08926v8 [cs.LO] UPDATED)

    [http://arxiv.org/abs/2007.08926](http://arxiv.org/abs/2007.08926)

    该论文提出了基于选择和成本收益的系统描述方式，并从编程语言的角度研究了此方法。研究者定义了两种支持决策抽象的小语言，并给出了它们的操作语义和底层语义，并将底层语义增强为选择和概率单子。该研究通过应用于两个简单例子展示了此方法的实用性。

    

    用选择和相应的成本和收益来描述系统，有望使算法设计人员和程序员从指定如何进行选择中解放出来；在实际实现中，可以通过优化技术和越来越多的机器学习方法来实现这些选择。我们从编程语言的角度研究了这种方法。我们定义了两种支持决策抽象的小语言：一个具有选择和收益，另一个则额外加入了概率。我们给出了操作语义和底层语义。针对第二种语言，我们考虑了三种底层语义，它们在可能的程序值和预期回报之间具有不同程度的相关性。操作语义将标准构造的通常语义与可能的执行策略空间的优化相结合。基于选择单子的组合底层语义，增加第二种语言中的概率单子。我们展示了选择单子可以被理解为一种特殊类型的 continuation 单子，而语言的操作语义和底层语义则通过单子翻译相关。最后，我们通过将其应用于两个简单的例子，展示了我们方法的有用性。

    Describing systems in terms of choices and their resulting costs and rewards offers the promise of freeing algorithm designers and programmers from specifying how those choices should be made; in implementations, the choices can be realized by optimization techniques and, increasingly, by machine-learning methods. We study this approach from a programming-language perspective. We define two small languages that support decision-making abstractions: one with choices and rewards, and the other additionally with probabilities. We give both operational and denotational semantics.  In the case of the second language we consider three denotational semantics, with varying degrees of correlation between possible program values and expected rewards. The operational semantics combine the usual semantics of standard constructs with optimization over spaces of possible execution strategies. The denotational semantics, which are compositional, rely on the selection monad, to handle choice, augmente
    

